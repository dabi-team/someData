0
2
0
2

b
e
F
7
2

]
L
M

.
t
a
t
s
[

1
v
5
7
4
2
1
.
2
0
0
2
:
v
i
X
r
a

Journal of Machine Learning Research 1 (2020) pp

Submitted mm/dd; Published mm/dd

Cautious Reinforcement Learning
via Distributional Risk in the Dual Domain

Junyu Zhang∗
Department of Industrial and Systems Engineering
University of Minnesota
Minneapolis, Minnesota, 55455

Amrit Singh Bedi∗
Computational and Information Sciences Directorate
US Army Research Laboratory
Adelphi, MD, USA 20783

Mengdi Wang
Department of Electrical Engineering
Center for Statistics and Machine Learning
Princeton University
Princeton, NJ 08544

Alec Koppel
Computational and Information Sciences Directorate
US Army Research Laboratory
Adelphi, MD 20783

Editor:

zhan4393@umn.edu

amrit0714@gmail.com

mengdiw@princeton.edu

alec.e.koppel.civ@mail.mil

Abstract
We study the estimation of risk-sensitive policies in reinforcement learning problems deﬁned by a
Markov Decision Process (MDPs) whose state and action spaces are countably ﬁnite. Prior eﬀorts
are predominately aﬄicted by computational challenges associated with the fact that risk-sensitive
MDPs are time-inconsistent. To ameliorate this issue, we propose a new deﬁnition of risk, which
we call caution, as a penalty function added to the dual objective of the linear programming
(LP) formulation of reinforcement learning. The caution measures the distributional risk of a
policy, which is a function of the policy’s long-term state occupancy distribution. To solve this
problem in an online model-free manner, we propose a stochastic variant of primal-dual method
that uses Kullback-Lieber (KL) divergence as its proximal term. We establish that the number of
iterations/samples required to attain approximately optimal solutions of this scheme matches tight
dependencies on the cardinality of the state and action spaces, but diﬀers in its dependence on
the inﬁnity norm of the gradient of the risk measure. Experiments demonstrate the merits of this
approach for improving the reliability of reward accumulation without additional computational
burdens.

1. Introduction

In reinforcement learning (RL) (Sutton and Barto, 2018), an autonomous agent in a given state
selects an action and then transitions to a new state randomly depending on its current state and
action, at which point the environment reveals a reward. This framework for sequential decision
making has gained traction in recent years due to its ability to eﬀectively describe problems where
the long-term merit of decisions does not have an analytical form and is instead observed only in

∗. Denotes equal contribution.

c(cid:13)2020 Junyu Zhang, Amrit Singh Bedi, Alec Koppel, and Mengdi Wang.

 
 
 
 
 
 
Zhang, Bedi, Koppel, and Wang

increments, as in recommender systems (Karatzoglou et al., 2013), videogames (Mnih et al., 2013;
Vinyals et al., 2019), control amidst complicated physics (Schulman et al., 2015), and management
applications (Peidro et al., 2009).

The canonical performance metric for RL is the expected value of long-term accumulation
of rewards. Unfortunately, restricting focus to expected returns fails to encapsulate many well-
documented aspects of reasoning under uncertainty such as anticipation (Roca et al., 2011), inattention
(Sims, 2003), and risk-aversion (Tom et al., 2007). In this work, we focus on risk beyond expected
rewards, both due to its inherent value in behavioral science and in pursuit of improving the reliability
of RL in safety-critical applications (Achiam et al., 2017).

Risk-awareness broadens the focus of decision making from expected outcomes to other quantiﬁers
of uncertainty. Risk, originally quantiﬁed using the variance in portfolio management (Markowitz,
1952), has broaden to higher-order moments or quantiles (Rockafellar and Uryasev, 2002), and gave
rise to a rich theory of coherent risk (Artzner et al., 1999), which has gained attention in RL in recent
years (Chow et al., 2017; Jiang and Powell, 2018) as a frequentist way to deﬁne uncertainty-aware
decision-making.

Incorporating risk gives rise to computational challenges in RL. In particular, if one replaces the
expectation in the value function by a risk measure, the MDP becomes time-inconsistent (Bjork and
Murgoci, 2010), that is, Bellman’s principle of optimality does not hold. This issue has necessitated
modiﬁed Bellman equations (Ruszczy´nski, 2010), multi-stage schemes (Jiang and Powell, 2018), or
policy search (Tamar et al., 2015), all of which do not attain near-optimal solutions in polynomial time,
even for ﬁnite MDPs. Alternatively, one may impose risk as a probabilistic constraint (Krishnamurthy
et al., 2003; Prashanth, 2014; Chow et al., 2017; Paternain et al., 2019; Yu et al., 2019) in the spirit
of chance-constrained programming (Nemirovski and Shapiro, 2007) common in model predictive
control.

An additional approach is Bayesian (Ghavamzadeh et al., 2015) and distributional RL (Bellemare
et al., 2017), which seeks to track a full posterior over returns. These approaches beneﬁt from the
fact that with access to a full distribution, one may deﬁne risk speciﬁcally, with, e.g., conditional
value at risk (CVaR)(Keramati et al., 2019). One limitation is that succinctly parameterizing the
value distribution intersects with approximate Bayesian computation, an active area of research
(Yang et al., 2019).

In this paper, we seek to deﬁne risk in sequential decision making that (1) provides a tunable
tradeoﬀ between the mean return and uncertainty of a decision; (2) captures long-term behaviors of
policies that cannot be modeled using cumulative functions; (3) can be solved eﬃciently in polynomial
time, depending on the choice of risk. To do so, we formulate a class of distributional risk-averse
policy optimization problems to address risks involving the long-term behaviors that permit the
derivation of eﬃcient algorithms. More speciﬁcally, we:

• propose a new deﬁnition of the risk of a policy, which we call caution, as a function of the
policy’s long-term state-action occupancy distribution. We formulate a caution-sensitive policy
optimization problem by adding the caution risk as a penalty function to the dual objective of
the linear programming (LP) formulation of RL. The caution-sensitive optimization problem is
often convex, allowing us to directly design the policy’s long-term occupancy distribution (Sec.
3).

• derive an online model-free algorithm based on a stochastic variant of primal-dual policy
gradient method that uses Kullback-Lieber (KL) divergence as its proximal term, and extend
the method to nonconvex caution risks by using a block coordinate ascent (BCA) scheme (Sec.
4).

• establish that the number of sample transitions required to attain approximately optimal
solutions of this scheme matches tight dependencies on the cardinality of the state and action
spaces, as compared to the typical risk-neutral setting (Sec. 5).

2

Distributional Risk in the Dual Domain

Further, we demonstrate the experimental merits of this approach for improving the reliability of
reward accumulation without additional computational burdens (Sec. 6)

2. Preliminaries

2.1 Discounted Markov Decision Process

We consider the problem of reinforcement learning (RL) with ﬁnitely many states and actions as
mathematically described by a Markov Decision Process (MDP) (S, A, P, r, γ). For each state i ∈ S,
a transition to state j ∈ S occurs when selecting action a ∈ A according to a conditional probability
distribution j ∼ P(·|a, i), for which we deﬁne the short-hand notation Pa(i, j). Moreover, a reward
ˆr : S ×S ×A (cid:55)→ R is revealed and is denoted as ˆrija. Without loss of generality, we assume ˆrija ∈ [0, 1]
with probability 1 for ∀i, j ∈ S and ∀a ∈ A throughout the paper. For future reference, we denote
the expected reward with respect to transition dynamics as ria := E [ˆrija|i, a] = (cid:80)
Pa(i, j) · ˆrija and

j∈S

the vector of rewards for each action a as ra = [r1a, · · · , r|S|a]T ∈ R|S|.

In standard (risk-neutral) RL, the goal is to ﬁnd the action sequence which yields the most

long-term reward, or value:

v∗(s) := max
{at∈A}

E

(cid:34) ∞
(cid:88)

t=0

γtˆritit+1at

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:35)

i0 = s

,

∀s ∈ S.

(2.1)

2.2 Bellman Equation and Duality

The optimal value function v∗ (2.1) satisﬁes Bellman’s optimality principle Bertsekas and Shreve
(2004):

v∗(i) = max
a∈A

(cid:110)

γ

(cid:88)

j∈S

Pa(i, j)v∗(i)+

Pa(i, j)ˆrija

(cid:111)

(cid:88)

j∈S

(2.2)

for all i ∈ S. Then, due to De Farias and Van Roy (2003), the Bellman optimality equation (2.2)
may be reformulated as a linear program (LP)

minv≥0
s.t.

(cid:104)ξ, v(cid:105)
(I − γPa)v − ra ≥ 0, ∀a ∈ A

where ξ is an arbitrary positive vector. The dual of (2.3) is given as

maxλ≥0

s.t.

(cid:88)

a∈A
(cid:88)

a∈A

(cid:104)λa, ra(cid:105)

(I − γP (cid:62)

a )λa = ξ,

∀a ∈ A

(2.3)

(2.4)

where λa = [λ1a, · · · , λ|S|a](cid:62) ∈ R|A| is the a-th column of λ. Essential to the subsequent development
is the fact that λ is an unnormalized state-action occupancy measure and

(cid:104)λa, ra(cid:105) = E

(cid:88)

a∈A

(cid:34) ∞
(cid:88)

t=0

γtritit+1at

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:35)

i0 ∼ ξ, at ∼ π(·|it)

when ξ belongs to the probability simplex. Moreover, one can recover the policy parameters through
normalization of the dual variable as π(a|s) = λsa/(cid:80)
a(cid:48)∈A λsa(cid:48) for all a ∈ A and s ∈ S, as detailed in
Proposition A.1.

3

Zhang, Bedi, Koppel, and Wang

3. Caution-Sensitive Policy Optimization

In this work, we prioritize deﬁnitions of risk in MDPs that capture long-term behavior of the policy
and permit the derivation of computationally eﬃcient algorithms. We focus on optimizing the policy’s
long-run behaviors that cannot be described by any cumulative sum of rewards, for examples the
peak risk and variance.

3.1 Problem Formulation

We focus on directly designing the long-term state-action occupancy distribution, whose unnormalized
version is the dual variable λ := {λa}a∈A. Rather than only maximizing the expected cumulative
return, i.e., the typical objective in risk-neutral MDP (e.g., (2.4)), we seek policies that incorporate
risk functions concerning the full distribution λ.
We propose a non-standard notion of risk:

in standard deﬁnitions, such as those previously
mentioned, they are typically risk measures of the cumulative rewards; by contrast, here we augment
the risk to be deﬁned over the long-term state-action occupancy distributions, which we dub caution
measures. Speciﬁcally, denote as ρ(λ) a caution function that takes as input dual variables λ
(unnormalized state-action distributions) feasible to (2.4) and maps to the reals R. The caution risk
measures the ﬁtness of the entire state path, rather than just a cumulative sum over the path.

In pursuit of computationally eﬃcient solutions, we hone in on properties of the dual LP

formulation of RL. The caution-sensitive variant of (2.4) then takes the form:

maxλ≥0

s.t.

(cid:104)λ, r(cid:105) − cρ(λ)
(cid:88)
(I − γP (cid:62)

a )λa = ξ,

a∈A
(cid:107)λ(cid:107)1 = (1 − γ)−1,

(3.1)

where c is a positive penalty parameter and we take ξ to be the vector of uniform distribution without
loss of generality, i.e., ξ = 1
s,a |λsa|. The constraints require that λ be the
unnormalized state-action distribution corresponding to some policy. The last constraint is implied
by (cid:80)
a )λa = ξ, but we include it for clarity. When ρ is convex, problem (3.1) is a convex
a∈A

|S||A| · 1; and (cid:107)λ(cid:107)1 := (cid:80)

(I − γP (cid:62)

optimization problem that facilitates computationally eﬃcient solutions.

Denote by λ∗ the optimal solution to the cautious policy optimization problem (3.1). This λ∗
gives the optimal long-term state-action occupancy distribution under the caution risk. Let π∗ be
the mixed policy given by

π∗(a|s) =

λ∗(s, a)
a(cid:48) λ∗(s, a(cid:48))

.

(cid:80)

We call this π∗ the optimal caution-sensitive policy. We remark that with the introduction of the risk
measure into the dual form (3.1), the corresponding primal is no longer the LP problem (2.3) but
changes to one that incorporates risk. The optimal caution-sensitive policy π∗ diﬀers from the optimal
policy in the typical risk-neutral setting. Since the LP structure is lost, the optimal risk-sensitive
policy π∗ is not guaranteed to be deterministic. Moreover, the Lagrangian multipliers, denoted by
v∗, for the risk-sensitive problem (3.1) is no longer the risk-neutral value vector, meaning that we
are solving a diﬀerent problem than (2.1). Indeed, by deﬁning caution in this way, we incorporate
long-term distributional risk into the dual domain of Bellman equation, while sidestepping the
computational challenges of time-inconsistency.

3.2 Examples of Caution Risk

Next, we discuss several examples of the caution risk ρ to clarify the problem setting (3.1).

4

Distributional Risk in the Dual Domain

Example 3.1 (Barrier risks). Caution risk can take the form of barriers to guarantee that a
policy’s long-term behavior meets certain expectations. Two examples follow:

• Staying in safety set. Suppose we want to keep the state trajectory within a safety set ¯S ⊂ S
for more than 1 − δ fraction of all time. In light of the typical barrier risk used in constrained
optimization, we deﬁne

ρ(λ) = − log (cid:0)λ( ¯S) − (1 − δ)(cid:1) ,

where λ( ¯S) = (1 − γ) (cid:80)
risk ρ is convex.

s,a λ(s, a)1s∈ ¯S. Since λ( ¯S) is linear in λ, we can verify that the log barrier

• Meeting multiple job requirements. Further, suppose there are multiple tasks with strict
requirements on their expected returns (cid:104)λ, rj(cid:105) ≥ bj, j = 1, . . . , m. One can transform these return
constraints into a log barrier given by

ρ(λ) = −

m
(cid:88)

j=1

log ((cid:104)λ, rj(cid:105) − bj) .

In this way, the optimal caution-sensitive policy will meet all the job requirements for large enough
penalty c.

Example 3.2 (Peak risk). Let f1, . . . , fm be risk functions of λ. Consider the peak risk deﬁned as

ρ(λ) = sup
j∈[m]

fj(λ).

• Worst-case exposure to danger areas. For example, let S1, . . . Sm be known “danger” sets. If
we let fj(λ) = λ(Sj) quantify the long-term exposure to Sj, the peak risk ρ measures the long-run
exposure to the most acute danger.

• Worst-case multitask performance. For another example, suppose there are m diﬀerent tasks
deﬁned in the same environment with reward functions r1, . . . , rm. Let fj(λ) = −(cid:104)λ, rj(cid:105) be the
negative cumulative return for task j, and an agent has to do well in all the tasks and be evaluated
based on her worst performance. Then the objective is a peak risk:

−ρ(λ) = sup
j∈[m]

−(cid:104)λ, rj(cid:105).

In the preceding examples, fj’s are linear, therefore ρ is always convex.

Example 3.3 (Variance risk). In ﬁnance applications, one canonical risk concern is the variance
of return. To formulate risk as variance, we ﬁrst note that λ is an unnormalized distribution, whose
normalized counterpart is denoted as ˆλ := (1 − γ)λ. Then it holds that (cid:104)ˆλ, r(cid:105) is the expected reward
accumulation. Then, the variance of return per timestep takes the form

ρ(λ) = V ar(ˆrss(cid:48)a|λ) = Eˆλ (cid:104)(cid:0)Eˆλ [ˆrss(cid:48)a] − ˆrss(cid:48)a

(cid:1)2(cid:105)

(3.2)

where Eˆλ := E
Es(cid:48)∼P(·|a,s)[ˆr2

(s,a,s(cid:48))∼ˆλ×P(·|a,s). For ease of notation, denote R ∈ R|S|×|A| with R(s, a) =

ss(cid:48)a]. Substituting in these deﬁnitions, we may write

ρ(λ) = (cid:104)ˆλ, R(cid:105) − (cid:104)ˆλ, r(cid:105)2,

(3.3)

which is a quadratic function of the variable λ. Note that the variance risk ρ(λ) is non-convex with
respect to λ.

Example 3.4 (Divergence for incorporating priors). Often in applications, we have access to
demonstrations, which can be used to learn a prior on the policy for ensuring baseline performance. Let

5

Zhang, Bedi, Koppel, and Wang

¯λ be a prior state-action distribution learned from demonstrations. Maintaining baseline performance
with respect to this prior, or demonstration distribution, then can be encoded as the Kullback-Liebler
(KL) divergence between the normalized distribution ˆλ = (1 − γ)λ and the prior ¯λ stated as

ρ(λ) = KL (cid:0)(1 − γ)λ||¯λ(cid:1)

(3.4)

which is substituted into (3.1) to obtain a framework for eﬃciently incorporating a baseline policy.
In some scenarios, existing demonstrations are only state trajectories without revealing the actions
taken. Then one may estimate the long-term state-only distribution µ and deﬁne the risk as

(cid:32)

ρ(λ) = KL

(1 − γ)

(cid:33)

λa||µ

,

(cid:88)

a

which measures the divergence between the marginalized state occupancy distribution and the prior.
In addition to KL, one can also use other convex distances such as Wasserstein, total variation, or
even a simple quadratic.

4. Stochastic Primal-Dual Policy Gradient

We shift focus to developing an algorithmic solution to the caution-sensitive policy optimization
problem (3.1). While the problem upon ﬁrst glance appears deterministic, the transition matrices Pa
are a priori unknown and we assume the presence of a generative model. Such a generative model is
fairly common in control/RL applications where a system simulator is available. For a given state
action pair (s, a), the generative model provides the next state s(cid:48) and the stochastic reward ˆrss(cid:48)a
according to the unknown transition dynamics.

Thus, we propose methodologies based on Lagrangian duality together with stochastic approxi-
mation. Given the convexity of ρ, by virtue of duality, (3.1) admits an equivalent formulation as a
saddle point problem:

max
λ∈L

min
v∈V

L(v,λ) = (cid:104)λ,r(cid:105) − cρ(λ) + (cid:104)ξ,v(cid:105) +

λ(cid:62)
a(γPa −I)v,

(cid:88)

a∈A

(4.1)

where V should be R|S| in principle. However, we can later on ﬁnd a large enough compact set to
replace the whole space without loss of optimality. By choosing ξ to satisfy ξ ≥ 0 and (cid:107)ξ(cid:107)1 = 1, we
deﬁne the dual feasible set L as

L := {λ : λ ≥ 0, (cid:107)λ(cid:107)1 = (1 − γ)−1}.

(4.2)

Given distribution ζ over S × A, deﬁne the stochastic approximation of the risk-neutral component
of the Lagrangian:

Lζ

(s,a,s(cid:48)),¯s(v, λ) := v¯s + 1{ζsa>0} ·

λsa(ˆrss(cid:48)a + γvs(cid:48) − vs)
ζsa

(4.3)

where ¯s ∼ P(ξ) is a sample from the discrete distribution deﬁned by probability vector ξ. Then by
direct computation, when the support of ζ contains that of λ, i.e., supp(λ) ⊂ supp(ζ), we may write

(cid:104)
Lζ
L(v, λ) = E(s,a,s(cid:48))∼ζ×P(·|a,s),¯s∼ξ

(cid:105)
−cρ(λ).
(s,a,s(cid:48)),¯s(v, λ)

(4.4)

Thus, we view (4.1) as a stochastic saddle point problem.

We propose variants of stochastic primal-dual method applied to (4.1). To obtain the primal
descent direction, we note that if ζ is chosen such that supp(λ) ⊂ supp(ζ), an unbiased estimator of

6

|S| · 1. Stepsizes α, β > 0. Discount γ ∈ (0, 1). Constants

Distributional Risk in the Dual Domain

Algorithm 1 Stochastic Risk-Averse (Cautious) RL
Input: Sample size T . Parameter ξ = 1
M1,M2 > 0, δ ∈ (0, 1).
Initialize: Arbitrary v1 ∈ V and λ1 :=
for t = 1, 2, · · · , T
Set ζ t := (1 − δ)(1 − γ)λt + δ
Sample (st, at) ∼ ζ t and ¯st ∼ ξ.
Generate s(cid:48)
tat from generative model.
t ∼ P(·|at, st) & ˆrsts(cid:48)
Construct ˆ∇vL(vt, λt) [cf. (4.8)] and ˆ∂λL(vt, λt) [cf. (4.9)] .
Update v and λ as

|S||A|(1−γ) · 1 ∈ L.

|S||A| 1.

1

vt+1 = ΠV (vt − α ˆ∇vL(vt, λt))

and

λt+1

2 =argmax

(cid:104) ˆ∂λL(vt,λt),λ−λt(cid:105)

λ

KL(cid:0)(1 − γ)λ||(1 − γ)λt(cid:1).

−

1
(1−γ)β
λt+ 1
(1 − γ)(cid:107)λt+ 1

2

.

2 (cid:107)1

λt+1 =

Output: ¯λ := 1
T

(cid:80)T

t=1 λt and ¯v := 1
T

(cid:80)T

t=1 vt.

the gradient of L w.r.t. v ∈ V is

ˆ∇vL(v, λ)

:= ∇vLζ

(s,a,s(cid:48)),¯s(v, λ)
λsa
ζsa

= e¯s + 1{ζsa>0} ·

(γes(cid:48) − es),

(4.5)

(4.6)

(4.7)

(4.8)

where es ∈ R|S| is a column vector with only the s-th entry equaling to 1 and all other entries being
0. Moreover, a dual subgradient of the instantaneous Lagrangian is given as

ˆ∂λL(v, λ)

:= 1{ζsa>0} ·

ˆrss(cid:48)a + γvs(cid:48) − vs − M1
ζsa

· Es,a

−c ˆ∂ρ(λ) − M2 · 1,

(4.9)

where Es,a ∈ R|S|×|A| is a matrix with (s, a)-th entry equal to 1 and all other entries equal to 0.
ˆ∂ρ(λ) is an unbiased subgradient estimate of the convex but possibly non-smooth function ρ, i.e.
E[ ˆ∂ρ(λ)] ∈ ∂ρ(λ). In (4.9), M1 and M2 are the “shift” parameters speciﬁed in Theorem 5.3 by
the convergence analysis in Section 5. Note that since the function ρ is often known in practice,
a full subgradient u ∈ ∂ρ(λ) may be used instead of an instantaneous approximate ˆ∂ρ(λ). With
appropriately deﬁned shift parameters M1, M2 in the subgradient estimator, if ζ > 0, then the dual
subgradient is biased with a constant shift:

E[ ˆ∂λL(v, λ)] ∈ ∂λL(v, λ) − (M1 + M2) · 1.

With these estimates for the primal gradient and dual subgradient of the Lagrangian (4.4), we
propose executing primal-dual stochastic subgradient iteration (Chen and Wang, 2016; Chen et al.,
2018) with the KL divergence in the dual domain. The detailed steps are summarized in Algorithm
1. Employing KL divergence in deﬁning the dual update permits us to leverage the structure of λ as
a distribution to derive tighter convergence rates, as detailed in Section 5.

7

Zhang, Bedi, Koppel, and Wang

Algorithm 1 provides a model-free method for learning cautious-optimal policies from transition
samples. Each primal and dual update can be computed easily based on a single observation.
Although Algorithm 1 is given in the tabular form, its spirit of primal-dual stochastic approximation
can be generalized to work with function approximations in the primal and dual spaces as the subject
of future work.

5. Convergence Analysis

In this section, we establish the convergence of Algorithm 1 when the caution (risk) ρ in (3.1) is
convex in λ, after which we present extensions of Algorithm 1 to address the non-convex variance
risk, with its associated convergence presented thereafter. We provide sample complexity results for
ﬁnding near-optimal solutions whose dependence on the size of the state and action spaces is tight.
Before delving into these details, we state a technical condition on the caution function ρ required
for the subsequent analysis, which is that we have access to a ﬁrst-order oracle providing noisy
samples of its subgradient, and that the inﬁnity norm of these samples is bounded.

Assumption 5.1. The caution function ρ(λ) is convex but possibly non-smooth, and it has bounded
subgradients as

sup
λ∈L

sup
u∈∂ρ(λ)

(cid:107)u(cid:107)∞ ≤ σ < ∞.

Further, samples ˆ∂ρ(λ) of its subgradients are unbiased and have ﬁnite inﬁnity norm:

E[ ˆ∂ρ(λ)] ∈ ∂ρ(λ) ,

(cid:107) ˆ∂ρ(λ)(cid:107)∞ ≤ σ.

sup
λ∈L

(5.1)

(5.2)

In our subsequent analysis, we treat σ as a known constant. In all of Examples 3.1-3.4, the
caution function ρ is explicitly known, which yields ˆ∂ρ(λ) ∈ ∂ρ(λ). For an instance, in Example 3.2,
if we let ρ(λ) = supj∈[m](cid:104)cj, λ(cid:105), then any subgradient is bounded by | ˆ∂ρ(λ)| ≤ supj (cid:107)cj(cid:107)∞ = O(1).
For another instance, in Example 3.4, ρ(λ) = KL(ˆλ || µ) for some ﬁxed µ [cf. (3.4)], the gradient
takes the form

|∇λsa ρ(λ)| =

(cid:12)
(cid:12)
(cid:12)(1 − γ)

(cid:16)

1 + log

(cid:16)ˆλsa/µsa

(cid:17)(cid:17)(cid:12)
(cid:12)
(cid:12)

for any s ∈ S and a ∈ A. Then, we can ensure Assumption 5.1 by imposing an elementwise lower
bound δ0 on µ and λ s.t. µ ≥ δ0 · 1 and λ ≥ δ0 · 1. The constant δ0 may be chosen extremely small,
for instance, δ0 = min{10−15, |S|−1|A|−1}. Consequently, we have

σ ≤ O (cid:0)(1 − γ) (cid:0)1 + log (cid:0)δ−1

0

(cid:1)(cid:1)(cid:1) = O(1).

5.1 The Case of Convex Caution Risk

In this subsection, we characterize the performance of Algorithm 1 when the caution ρ is convex.
We begin by noting that the saddle point problem (4.1) does not specify the feasible region V for
the variable v. However, the convergence necessitates V to be a compact set rather than the entire
R|S|. To disambiguate the domain of v, next we derive a bounded region that contains the primal
optimizer v∗.

Lemma 5.2. If ξ > 0, then the primal optimizer v∗ satisﬁes

(cid:107)v∗(cid:107)∞ ≤ (1 − γ)−1(1 + cσ).

Therefore, we can deﬁne the feasible region V to be the compact set
(cid:27)

(cid:26)

V :=

v ∈ R|S| : (cid:107)v(cid:107)∞ ≤ 2

1 + cσ
1 − γ

(5.3)

(5.4)

.

8

Distributional Risk in the Dual Domain

The proof of Lemma 5.2 is provided in Appendix B. We note that the factor of 2 is incorporated

to simplify the analysis.

Subsequently, we analyze the primal-dual convergence of Algorithm 1 for solving (4.1) (and
the equivalently (3.1)). Before providing the main theorem, we introduce a technical result which
deﬁnes convergence in terms of a form of duality gap. The duality gap measures the distance of the
Lagrangian evaluations to a saddle point as deﬁned by (4.1).

Theorem 5.3 (Convergence of duality gap). For Algorithm 1, select shift parameters M1 =
(cid:113) log(|S||A|)
T (1 + cσ). Let ¯λ and ¯v be the
4(1+cσ)
T |S||A|
1−γ

and M2 = cσ, δ ∈ (0, 1

2 ), β = 1−γ

, and α =

(cid:113) |S|

1+cσ

output of Algorithm 1 and let λ∗ be the optimum. Then for the output of Algorithm 1, we have

E[L(¯v,λ∗) − min
v∈V
(cid:32)(cid:114)

L(v, ¯λ)]

≤ O

|S||A| log(|S||A|)
T

·

1 + 2cσ
(1 − γ)2

(cid:33)

.

As a result, to guarantee E[L(¯v, λ∗) − minv∈V L(v, ¯λ)] ≤ (cid:15), the amount of samples needed is

T = Θ

(cid:18) |S||A| log(|S||A|)(1 + 2cσ)2
(1 − γ)4(cid:15)2

(cid:19)

.

(5.5)

(5.6)

The proof of this Theorem is provided in Appendix C.
We may then use the convergence of duality gap to characterize the sub-optimality and constraint

violation attained by the output of Algorithm 1 for the problem (3.1).

Theorem 5.4 (Convergence to optimal caution-sensitive policies). Let the parameters M1,
M2, δ, β, and α, as deﬁned in Theorem 5.3, if ¯λ is the output of Algorithm 1 after T iterations, then
the constraint violation of the original problem (3.1) satisﬁes




¯λ ≥ 0,

(cid:13)¯λ(cid:13)
(cid:13)

(cid:13)1 = (1 − γ)−1
a )¯λa − ξ(cid:13)



(cid:13)
(cid:13)(cid:80)

a∈A(I − γP (cid:62)

(cid:13)1 ≤ (1−γ)(cid:15)

1+cσ ≤ (1 − γ)(cid:15).

Moreover, the sub-optimality of (3.1) is given as

E[((cid:104)λ∗, r(cid:105) − cρ(λ∗)) − ((cid:104)¯λ, r(cid:105) − cρ(¯λ))] ≤ (cid:15)

(5.7)

(5.8)

Eqs. (5.7) and (5.8) showed the output solution is (cid:15)-feasible and (cid:15)-optimal. Note that (cid:15) determines

the number of samples T as given in (5.6). The proof is provided in Appendix D.

Theorem 5.4 suggests that to get (cid:15)-optimal policy and its corresponding state-action distribution,
the sample complexity has near-linear dependence (up to logarithmic factors) on the sizes of S and
A. This matches the optimal dependence in the risk-neutral case, see e.g. (Chen et al., 2018; Wang,
2017a,b) which proves that Algorithm 1 is sample-eﬃcient.

Further, consider the case where ρ is a KL divergence as in Example 3.4. This ρ acts as a
regularization term to keep λ close to a prior long-term behavior. In this case, we can show that
the primal-dual algorithm enjoys better convergence rates. In particular, we show a tighter KL
divergence bound between the estimated ¯λ and the optimal state-action distribution.
Corollary 5.5 (The case when ρ is a KL divergence). If ¯λ is the output of Algorithm 1, with
parameters M1, M2, δ, β, α and T chosen as in Theorem 5.3, with ρ(λ) := KL(cid:0)(1 − γ)λ || µ(cid:1) is the
KL divergence from given prior µ, we have E (cid:2)KL (cid:0)(1 − γ)¯λ || (1 − γ)λ∗(cid:1)(cid:3) ≤ (cid:15)
c .

Corollary 5.5 explicitly gives a dependence of the risk in terms of penalty parameter c, which may
be made small as the penalty parameter grows large. Next, we discuss the case where the caution ρ
is not necessarily convex.

9

Zhang, Bedi, Koppel, and Wang

Algorithm 2 A Block Coordinate Ascent (PCA) framework for Policy Optimization with Nonconvex
Caution
Initialize: λ0, µ0.
for k = 0, 1, ..., K − 1
Update µk+1 by solving

µk+1= arg max Φ(λk, µ)

s.t. µ ≥ 0, (cid:107)µ(cid:107)1 = 1

with a known closed form solution.
Update λk+1 by solving the following to (cid:15)-sub-optimality

max
λ

Φ(λ, µk−1) s.t.

(cid:88)

(I − γP (cid:62)

a )λa = ξ, λ ≥ 0

a∈A

(5.9)

(5.10)

using Algorithm 1.
Output: Select (λk∗

, µk∗

) randomly from (λ1, µ1), ..., (λK, µK).

5.2 Extension to Nonconvex Variance Risk

In this subsection, we specify the caution as variance as in Example 3.3, which is nonconvex unlike
other examples. For this instance, our strategy of addressing the constraints of problem (3.1) via
Lagrangian relaxation fails here due to the nonconvexity of the variance in terms of λ. Therefore, we
propose to approximately solve the nonconvex saddle point problem by solving a blockwise convex
surrogate problem. Consider the following surrogate problem for some M > 0:

max
λ∈Λ

max
µ∈U

Φ(λ, µ) :=(cid:104)λ, r(cid:105)−cρ(λ, µ)−

M
2

(cid:107)µ−ˆλ(cid:107)2

(5.11)

where Λ := (cid:8)λ : (cid:80)

a∈A

(I − γP (cid:62)

a )λa = ξ, λa ≥ 0, a ∈ A(cid:9),

U : = (cid:8)µ : µ ≥ 0, (cid:107)µ(cid:107)1 = 1(cid:9) , ˆλ = (1 − γ)λ,

ρ(λ, µ) = (cid:104)ˆλ, r(cid:105)2 − 2(cid:104)µ, r(cid:105)(cid:104)ˆλ, r(cid:105) + (cid:104)µ, R(cid:105).

Note that the surrogate problem (5.11) is not equivalent to the original problem (3.1) when risk ρ is
chosen to be variance. However, in this alternative formulation a quadratic penalty is applied to push
distribution µ towards ˆλ. Observe that when µ = ˆλ, we have ρ(λ, µ) = (cid:104)ˆλ, R(cid:105) − (cid:104)ˆλ, r(cid:105)2, which equals
exactly to the variance function. Therefore, the problem (5.11) will be close to the original problem
(3.1) when the penalty parameter M is reasonably large. In what follows we propose algorithmic
solution to the surrogate problem, assuming that a suﬃciently large M is chosen.

The surrogate objective Φ is strongly concave in λ for any ﬁxed µ and is strongly concave in µ
for any ﬁxed λ. But Φ is not jointly concave in λ and µ. Therefore, we can employ block-coordinate
ascent (BCA) to solve problem (5.11). The BCA method alternates between the two steps: First we
ﬁx λ and optimize the problem over µ - this subproblem is a projection onto a simplex and has a
closed form solution (see (Wang and Carreira-Perpin´an, 2013)); Second we ﬁx µ and optimize over λ,
which is a convex problem and can be solved by using Algorithm 1. The full scheme is presented
in Algorithm 2. Finally, we establish the sample complexity of Algorithm 2 to ﬁnd a ﬁrst-order
stationary point of (5.11).

Theorem 5.6 (Convergence to approximate stationarity). Suppose we apply Algorithm 1 to
solve the subproblem (5.10) with T satisfying

T = Θ

(cid:18) |S||A| log(|S||A|)
(1 − γ)4(cid:15)2

(cid:0)1 + (1 − γ)2(c2 + M 2)(cid:1)

(cid:19)

.

10

Distributional Risk in the Dual Domain

(a) Reward dist.

(b) Risk neutral

(c) Risk averse

Figure 1: Experiment on grid world with variance as the risk. (a) Reward distribution for the Maze
environment; (b) Risk neutral and (c) Risk averse trajectories, respectively, from start to
goal. The trajectory resulting from greedily following the risk-averse policy avoids negative
reward states.

And we solve the subproblem (5.9) with a closed form solution (Wang and Carreira-Perpin´an, 2013).
Let the number of outer iterations be

K ≥

maxλ,µ Φ(λ, µ) − Φ(λ0, µ0)
(cid:15)

.

Then the output (λk∗
which satisﬁes

, µk∗

) of Algorithm 2 is an approximate-stationary solution to problem (5.11),

E(cid:2)(cid:107)ΠΛ(∇λΦ(λ, µ))(cid:107)2 + (cid:107)ΠU (∇µΦ(λ, µ))(cid:107)2(cid:3)

(cid:18)

(1 − γ)2

≤ O

(cid:18) c2|S|2|A|2
M

(cid:19)

(cid:19)

+ M

(cid:15)

and






(I − γP (cid:62)

E[(cid:107) (cid:80)
a∈A
≥ 0, (cid:107)λk∗
≥ 0, (cid:107)µk∗

λk∗
µk∗

(cid:107)1 = (1 − γ)−1,
(cid:107)1 = 1.

a )λk∗

a − ξ(cid:107)1] ≤ (1 − γ)(cid:15),

(5.12)

(5.13)

Eqs. (5.12), (5.13) suggest that both the projected gradient norm and the level of constraint
violation are O((cid:15)) small. They imply that the output solution is nearly feasible and nearly stationary.
See Appendix F for the proof of theorem.

6. Experimental Results

In this section, we experimentally evaluate the proposed technique for incorporating risk or other
sources of exogenous information into RL training. In particular, we consider a setting in which
an agent originally learns in the risk-neutral sense of (2.2), i.e., focusing on expected returns. The
MDP we focus on is a 10 × 10 grid with each state permitting for four possible actions (moving
A := {up, down, left, and right}). For the transition model, given the direction of the previous
action selection, the agent movies in the same direction with probability p and moves in the diﬀerent
direction with probability 1 − p, and moves backwards with null probability. For instance, in a given
state action pair (s, a), suppose the action a selected is up. Then, the next action will be up with
prob p and {left, or right} with prob 1 − p, and down with null probability. Overall, this means

11

Zhang, Bedi, Koppel, and Wang

(a) Objective

(b) Reward mean

(c) Reward Variance

Figure 2: (a) Convergence of the dual objective [cf. (3.1)]; Sample mean return (b) and variance (c)
over 100 simulated trajectories. Observe the expected reward return is comparable while
the risk-averse policy attains lower variance, and is thus more reliable.

(a) µ

(b) Risk neutral

(c) Risk averse

Figure 3: Results for the learning with demonstration µ. We have used KL divergence as the risk
function for these results. (a) The given demonstration, (b) Risk neutral solution, (c)
Risk averse solution. Note that incorporating KL divergence yields a policy that avoids
unrewarding states (red block in (b) and (c)).

that the transition matrix has four nonzero sequences of likelihoods along the main diagonal, i.e., it
is quad-diagonal. For the experiments, we consider the caution-sensitive formulation presented in
Examples 3.3 and 3.4 which respectively correspond to quantifying risk via the variance and the KL
divergence to a previously learned policy which serves as a prior. We append videos (links in the
footnote12) to the submission which visualize the safety of risk-awareness during training.

6.1 Variance-Sensitive Policy Optimization

The variance risk given in Example 3.3 characterizes the statistical robustness of the rewards from
a policy. To evaluate the merit of this deﬁnition, consider the maze example with the rewards
distribution as described in Fig. 1(a). There are two ways to go from start to destination. The
reward of dark green areas is more negative than lighter shades of green, and thus it is riskier to be
near darker green in terms of the returns of a trajectory. We display a sample path of the Markov
chain obtained by solving the variance-sensitive policy optimization problem as Fig. 1(c), whereas
the one based on the risk-neutral (classical) formulation is shown in Fig. 1(b). Clearly, the risk-averse
one avoids the dark green areas and collects a sequence of more robust rewards, yet still reaches

1. https://tinyurl.com/sk4lddb
2. https://tinyurl.com/tlcl3m2

12

0100200300400500Iterations × 1050.250.30.350.4Objective0100200300400500Iterations × 10511.522.53Mean (cumu. reward)Risk neutralRisk averse0100200300400500Iterations × 10511.522.533.5Var (cumu. reward)Risk neutralRisk averseDistributional Risk in the Dual Domain

(a) Comparison

(b) Time in unrewarding states

Figure 4: We plot the running average of (a) Expected reward return, (b) percentage of time we visit
the unrewarding states. Note that the prior demonstration helps in the faster convergence
as clear from (a). Further, the KL divergence based risk helps to avoid the visitation of
the unrewarding states as clear from the result in (b).

the goal. The convergence of objective is plotted in Fig. 2(a) for the proposed algorithm. Further,
we plot the associated sample mean and variance of the discounted return over number of training
indices in Figs. 2(b) and 2(c), respectively. Observe that the risk-averse policy yields comparable
mean reward accumulation with reduced variance, meaning it more reliably reaches the goal without
visiting unwanted states whose rewards are negative.

6.2 Caution as Proximity to a Prior

When a prior is available in the form of some baseline state-action distribution µ , KL divergence to
the baseline makes sense as a measure of caution [cf. (3.4)] as stated in Example 3.4. To evaluate
this deﬁnition, consider the setting where the baseline µ is a risk-neutral policy (shown in Fig. 3(a))
learned by solving (2.4) with a reward that is highly negative r = −5 in the dark green area, strictly
positive r = 0.3 in the light green area, and r = 1 at the goal in the bottom right denoted by G in
Fig. 3(a). The transition probabilities are deﬁned by p = 0.4. Then, the resulting risk-neutral policy
is used as a baseline policy for a drifted MDP whose reward is r = 0 for the dark green area while
identical elsewhere, and whose transition dynamics are deﬁned by likelihood parameter p = 0.6. The
overarching purpose is that although the reward landscape and transition dynamics changed, the
“lessons” of past learning may still be incorporated.

The resulting policy learned from this procedure, as compared with the risk-neutral policy, are
visualized in Figures 3(b) and 3(c), respectively. Observe that the policy associated with incorporating
past experience in the form of policy µ has explicitly pushed avoidance of the dark green region,
whereas the risk-neutral policy resulting from (2.4) does not. Thus, past (negative) experiences may
be incorporated into the learned policy. This hearkens back to psychological experiments on mice: if
its food supply is electriﬁed, then a mouse will refuse to eat, even after the electricity is shut oﬀ, a
form of fear conditioning. Further, we plot the associated discounted return and empirical occupancy
of negative reward states with the iteration index of the optimization procedure in Algorithm 1 in
Fig. 4. Overall, then, the incorporation of prior demonstrations results in the faster learning (see Fig.
4(a)) and reduces the proportion of time spent in unrewarding states as evidenced by Fig. 4(b).

7. Conclusions

In this work, we proposed a new deﬁnition of risk named caution which takes as input unnormalized
state-action occupancy distributions, motivated by the dual of the LP formulation of the MDP. To

13

0246810Iterations×1040.511.522.5Mean rewardRisk neutralRisk averse0246810Iterations×1040246PercentageRisk neutralRisk averseZhang, Bedi, Koppel, and Wang

solve the resulting risk-aware RL in an online model-free manner, we proposed a variant of stochastic
primal-dual method to solve it, whose sample complexity matches optimal dependencies of risk-neutral
problem. Experiments illuminated the usefulness of this deﬁnition in practice. Future work includes
deriving the Bellman equations associated with cautious policy optimization (3.1), generalizations to
continuous spaces, and broadening caution to encapsulate other aspects of decision-making such as
inattention and anticipation.

References

Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In
Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 22–31.
JMLR. org, 2017.

Philippe Artzner, Freddy Delbaen, Jean-Marc Eber, and David Heath. Coherent measures of risk.

Mathematical ﬁnance, 9(3):203–228, 1999.

Francis Bach and Kﬁr Y Levy. A universal algorithm for variational inequalities adaptive to

smoothness and noise. arXiv preprint arXiv:1902.01637, 2019.

Marc G Bellemare, Will Dabney, and R´emi Munos. A distributional perspective on reinforcement
learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70,
pages 449–458. JMLR. org, 2017.

Dimitir P Bertsekas and Steven Shreve. Stochastic optimal control: the discrete-time case. 2004.

Tomas Bjork and Agatha Murgoci. A general theory of markovian time inconsistent stochastic

control problems. Available at SSRN 1694759, 2010.

Yichen Chen and Mengdi Wang. Stochastic primal-dual methods and sample complexity of reinforce-

ment learning. arXiv preprint arXiv:1612.02516, 2016.

Yichen Chen, Lihong Li, and Mengdi Wang. Scalable bilinear pi learning using state and action

features. arXiv preprint arXiv:1804.10328, 2018.

Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone. Risk-constrained
reinforcement learning with percentile risk criteria. The Journal of Machine Learning Research, 18
(1):6070–6120, 2017.

Daniela Pucci De Farias and Benjamin Van Roy. The linear programming approach to approximate

dynamic programming. Operations research, 51(6):850–865, 2003.

Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, Aviv Tamar, et al. Bayesian reinforcement

learning: A survey. Foundations and Trends R(cid:13) in Machine Learning, 8(5-6):359–483, 2015.

Daniel R Jiang and Warren B Powell. Risk-averse approximate dynamic programming with quantile-

based risk measures. Mathematics of Operations Research, 43(2):554–579, 2018.

Alexandros Karatzoglou, Linas Baltrunas, and Yue Shi. Learning to rank for recommender systems.

In Proceedings of the 7th ACM conference on Recommender systems, pages 493–494, 2013.

Ramtin Keramati, Christoph Dann, Alex Tamkin, and Emma Brunskill. Being optimistic to be

conservative: Quickly learning a cvar policy. arXiv preprint arXiv:1911.01546, 2019.

Vikram Krishnamurthy, K Martin, and F Vasquez Abad. Implementation of gradient estimation to a
constrained markov decision problem. In 42nd IEEE International Conference on Decision and
Control (IEEE Cat. No. 03CH37475), volume 5, pages 4841–4846. IEEE, 2003.

14

Distributional Risk in the Dual Domain

Harry Markowitz. Portfolio selection. The journal of ﬁnance, 7(1):77–91, 1952.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.

Arkadi Nemirovski and Alexander Shapiro. Convex approximations of chance constrained programs.

SIAM Journal on Optimization, 17(4):969–996, 2007.

Santiago Paternain, Miguel Calvo-Fullana, Luiz FO Chamon, and Alejandro Ribeiro. Learning safe
policies via primal-dual methods. In Proceedings of the 58th IEEE Conference on Decision and
Control, IEEE, 2019.

David Peidro, Josefa Mula, Ra´ul Poler, and Francisco-Cruz Lario. Quantitative models for supply
chain planning under uncertainty: a review. The International Journal of Advanced Manufacturing
Technology, 43(3-4):400–420, 2009.

LA Prashanth. Policy gradients for cvar-constrained mdps. In International Conference on Algorithmic

Learning Theory, pages 155–169. Springer, 2014.

Andr´e Roca, Paul R Ford, Allistair P McRobert, and A Mark Williams. Identifying the processes
underpinning anticipation and decision-making in a dynamic time-constrained task. Cognitive
processing, 12(3):301–310, 2011.

R Tyrrell Rockafellar and Stanislav Uryasev. Conditional value-at-risk for general loss distributions.

Journal of banking & ﬁnance, 26(7):1443–1471, 2002.

Andrzej Ruszczy´nski. Risk-averse dynamic programming for markov decision processes. Mathematical

programming, 125(2):235–261, 2010.

John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional
continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015.

Christopher A Sims. Implications of rational inattention. Journal of monetary Economics, 50(3):

665–690, 2003.

Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. 2018.

Aviv Tamar, Yinlam Chow, Mohammad Ghavamzadeh, and Shie Mannor. Policy gradient for
coherent risk measures. In Advances in Neural Information Processing Systems, pages 1468–1476,
2015.

Sabrina M Tom, Craig R Fox, Christopher Trepel, and Russell A Poldrack. The neural basis of loss

aversion in decision-making under risk. Science, 315(5811):515–518, 2007.

Oriol Vinyals, Igor Babuschkin, Junyoung Chung, Michael Mathieu, Max Jaderberg, Wojciech M
Czarnecki, Andrew Dudzik, Aja Huang, Petko Georgiev, Richard Powell, et al. Alphastar: Mastering
the real-time strategy game starcraft ii. DeepMind blog, page 2, 2019.

Mengdi Wang. Primal-dual pi learning: Sample complexity and sublinear run time for ergodic markov

decision problems. arXiv preprint arXiv:1710.06100, 2017a.

Mengdi Wang. Randomized linear programming solves the discounted markov decision problem in

nearly-linear (sometimes sublinear) running time. arXiv preprint arXiv:1704.01869, 2017b.

Weiran Wang and Miguel A Carreira-Perpin´an. Projection onto the probability simplex: An eﬃcient

algorithm with a simple proof, and an application. arXiv preprint arXiv:1309.1541, 2013.

15

Zhang, Bedi, Koppel, and Wang

Derek Yang, Li Zhao, Zichuan Lin, Tao Qin, Jiang Bian, and Tie-Yan Liu. Fully parameterized
quantile function for distributional reinforcement learning. In Advances in Neural Information
Processing Systems, pages 6190–6199, 2019.

Ming Yu, Zhuoran Yang, Mladen Kolar, and Zhaoran Wang. Convergent policy optimization for safe
reinforcement learning. In Advances in Neural Information Processing Systems, pages 3121–3133,
2019.

16

Distributional Risk in the Dual Domain

Supplementary Material for “Cautious Reinforcement Learning via
Distributional Risk in the Dual Domain”

Appendix A. The Physical Meaning of Dual LP

The dual LP formulation (2.4) has a clear physical meaning. Suppose ξ ≥ 0 and (cid:107)ξ(cid:107)1 = 1 is a
distribution over the state space S. Then the following proposition explains the meaning of the dual
problem.

Proposition A.1. Suppose the variable λ ∈ R|S|×|A|

+

satisﬁes the conditions

λ ≥ 0 and

(I − γP (cid:62)

a )λa = ξ,

(cid:88)

a∈A

(A.1)

Then λ is an unnormalized distribution, or ﬂux, under the randomized policy π:

π(a|s) =

λsa
a(cid:48)∈A λsa(cid:48)

(cid:80)

,

for ∀a ∈ A, ∀s ∈ S.

(A.2)

Furthermore, it satisﬁes

and

λsa =

(cid:18)

γt · P

∞
(cid:88)

t=0

it = s, at = a

i0 ∼ ξ, at ∼ π(·|it)

(cid:19)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:104)λ, r(cid:105) = E

(cid:34) ∞
(cid:88)

t=0

γtritit+1at

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:35)

i0 ∼ ξ, at ∼ π(·|it)

.

(A.3)

(A.4)

Proof. Under the initial distribution ξ and the randomized policy π : S (cid:55)→ ∆|A| deﬁned in (A.2), we
deﬁne a new initial distribution ˆξ as

ˆξsa = ξs · π(a|s)

for ∀s ∈ S, a ∈ A

as the distribution of the initial state-action pair (s0, a0). Therefore the dynamics of the state-action
pairs (st, at) form another Markov chain with transition matrix ˆP ∈ R|S||A|×|A||S|

deﬁned as

+

ˆPπ(s, a; s(cid:48), a(cid:48)) = Pa(s, s(cid:48)) · π(a(cid:48)|s(cid:48)).

First, let us prove that (A.1) is equivalent to (A.3). For the ease of notation, we used the multi-indices.
Let us view both r and λ as vectors with s, a being a multi-index. Note that (A.1) implies that for
all s ∈ S

(cid:88)

ξs =

λsa(cid:48) − γ

(cid:88)

(cid:88)

Pa(cid:48)(s(cid:48), s)λs(cid:48)a(cid:48).

Multiplying both sides by π(a|s) =

(cid:80)

λsa
a(cid:48) ∈A λsa(cid:48)

, we get

a(cid:48)∈A

a(cid:48)∈A

s(cid:48)∈S

ˆξsa = λsa − γ

= λsa − γ

(cid:88)

(cid:88)

a(cid:48)∈A
(cid:88)

s(cid:48)∈S
(cid:88)

a(cid:48)∈A

s(cid:48)∈S

Pa(cid:48)(s(cid:48), s) · π(a|s) · λs(cid:48)a(cid:48)

ˆPπ(s(cid:48), a(cid:48); s, a)λs(cid:48)a(cid:48)

for any s ∈ S, a ∈ A. If we write this equation in a compact matrix form, we get

ˆξ = (I − γ ˆP (cid:62)

π )λ.

17

Zhang, Bedi, Koppel, and Wang

Note that (cid:107)γ ˆP (cid:62)

π (cid:107)2 ≤ γ < 1, we know (I − γ ˆP (cid:62)

π )−1 = (cid:80)∞

i=0 γi( ˆP i

π)(cid:62). Consequently,

λ(cid:62) = ˆξ(cid:62)(I − γ ˆPπ)−1 = ˆξ(cid:62) + γ ˆξ(cid:62) ˆPπ + γ2 ˆξ(cid:62) ˆP 2

π + · · ·

If we write the above equation in an elementwise way, we get (A.3). Consequently, we also have

λ(cid:62)r = ˆξ(cid:62)r + γ ˆξ(cid:62) ˆPπr + γ2 ˆξ(cid:62) ˆP 2

π r + · · ·

(cid:35)

i0 ∼ ξ, at ∼ π(·|it)

,

= E

(cid:34) ∞
(cid:88)

t=0

γtˆritit+1at

(cid:12)
(cid:12)
(cid:12)
(cid:12)

which is as stated in (A.4)

Appendix B. Proof of Lemma 5.2

Proof. Consider the min-max saddle point problem,

max
λ≥0

min
v∈R|S|

L(v,λ) = (cid:104)λ,r(cid:105) − cρ(λ) + (cid:104)ξ,v(cid:105) +

λ(cid:62)
a(γPa −I)v,

(cid:88)

a∈A

Then (λ∗, v∗) solves this saddle point problem if and only if

λ∗ = argmax

L(v∗, λ)

and

λ≥0

(cid:88)

a∈A

(I − γP (cid:62)

a )λ∗

a − ξ = 0.

(B.1)

(B.2)

A remark is that, this is also the KKT condition for the original convex problem (3.1). Due to
L(v∗, λ) is equivalent to the
the concavity of L(v∗, λ) for any ﬁxed v∗, the condition λ∗ = argmax

existence of a subgradient w∗ ∈ ∂λL(v∗, λ∗) s.t.

λ≥0

(cid:104)w∗, λ − λ∗(cid:105) ≤ 0

for ∀λ ≥ 0.

(B.3)

If we use u∗ to denote the speciﬁc subgradient in ∂ρ(λ∗) that consists w∗. For any ﬁxed s, a, we know
w∗
s(cid:48)a(cid:48) for ∀(s(cid:48), a(cid:48)) (cid:54)= (s, a), (B.3) further
sa. If we choose λs(cid:48)a(cid:48) = λ∗
implies

sa = −(es − γPas)(cid:62)v∗ + rsa − cu∗

(cid:0)(es − γPas)(cid:62)v∗ − rsa + cu∗

(cid:1)(λsa − λ∗

sa

sa) ≥ 0,

where Pas is a column vector, with Pas(s(cid:48)) = P(s(cid:48)|a, s). Combine this inequality with (B.2), we can
formally write the ﬁnal optimality condition as follows.

∃u∗ ∈ ∂ρ(λ∗) s.t.






(cid:80)
a∈A

(I − γP (cid:62)

a )λ∗
(cid:0)(es − γPas)(cid:62)v∗ − rsa + cu∗

a = ξ,

λ∗ ≥ 0,

sa

(cid:1)(λsa − λ∗

sa) ≥ 0, ∀s ∈ S, ∀a ∈ A, ∀λsa ≥ 0.

(B.4)

By (A.3) of Proposition A.1, we know that

(cid:88)

a∈A

λ∗
sa ≥

(cid:88)

a∈A

Prob(cid:0)i0 = s, a0 = a|i0 ∼ ξ, a0 ∼ π(·|i0)(cid:1) = ξs > 0

for

∀s ∈ S.

Therefore, for any s ∈ S, there exists an as such that λ∗
the optimality condition (B.4) implies that,

sas

> 0. Therefore, the second inequality of

(es − γPass)(cid:62)v∗ − rsas + cu∗

sas
Let us denote ˜r := [r1a1, · · · , r|S|a|S| ](cid:62) ∈ R|S|, ˜u := [u∗
R|S|×|S|. Then we can write

1a1

(cid:16)

I − γ ˜P (cid:62)(cid:17)

v∗ = ˜r − c˜u.

= 0

for

∀s ∈ S.

, · · · , u∗

|S|a|S|

](cid:62) ∈ R|S| and ˜P := [Pa11, · · · , Pa|S||S|] ∈

18

Distributional Risk in the Dual Domain

As a result,

1 + cσ ≥ (cid:107)˜r − c˜u(cid:107)∞ = (cid:107)(I − γ ˜P (cid:62))v∗(cid:107)∞ ≥ (cid:107)v∗(cid:107)∞ − (cid:107)γ ˜P (cid:62)v∗(cid:107)∞ ≥ (1 − γ)(cid:107)v∗(cid:107)∞,

which implies the statement of Lemma 5.2.

Appendix C. Proof of Theorem 5.3

Proof. To make the proof of this result clearer, we will separate part of the major steps into several
diﬀerent lemmas.

Lemma C.1. Suppose the iterate sequence {vt} is updated according to the rule (4.5) in Algorithm
1. Then for any t,

(cid:104)∇vL(vt, λt), vt − v(cid:105) ≤

((cid:107)vt − v(cid:107)2 − (cid:107)vt+1 − v(cid:107)2) +

1
2α
+ (cid:104)∇vL(vt, λt) − ˆ∇vL(vt, λt), vt − v(cid:105).

α
2

(cid:107) ˆ∇vL(vt, λt)(cid:107)2

(C.1)

The proof of this lemma is provided in Appendix C.1.

Lemma C.2. Suppose the iterate sequence {λt} is updated according to the rule (4.6) and (4.7) in
Algorithm 1. For ∀t,

−(cid:104)wt, λt − λ(cid:105) ≤

1
(1 − γ)β
β
(cid:88)
2

+

s,a

(cid:0)KL(cid:0)(1 − γ)λ || (1 − γ)λt(cid:1) − KL(cid:0)(1 − γ)λ || (1 − γ)λt+1(cid:1)(cid:1)

sa(∆t
λt

sa)2 + (cid:104) ˆ∂λL(vt, λt) − wt, λt − λ(cid:105),

(C.2)

where wt := E

(cid:104) ˆ∂λL(vt, λt)(cid:12)

(cid:12)λt, vt(cid:105)

+ (M1 + M2) · 1 ∈ ∂λL(vt, λt) is a subgradient vector.

The proof of this lemma is provided in Appendix C.2. Based on these two lemmas, we start
t=1 λt. Deﬁne

the proof of Theorem 5.3. Note that by deﬁnition, ¯v = 1
T
¯v∗ := argminv∈V L(v, ¯λ). Then by the convex-concave structure of L we have

t=1 vt and ¯λ = 1

(cid:80)T

(cid:80)T

T

L(¯v, λ∗) − L(¯v∗, ¯λ) ≤

=

≤

1
T

1
T

1
T

T
(cid:88)

t=1

T
(cid:88)

t=1

T
(cid:88)

t=1

(cid:0)L(vt, λ∗) − L(¯v∗, λt)(cid:1)

(C.3)

(cid:0)L(vt, λ∗) − L(vt, λt) + L(vt, λt) − L(¯v∗, λt)(cid:1)

(cid:0)−(cid:104)wt, λt − λ∗(cid:105) + (cid:104)∇vL(vt, λt), vt − ¯v∗(cid:105)(cid:1) ,

where the ﬁrst line applies Jensen’s inequality and last line is due to the convexity of L(·, λt) and the
concavity of L(vt, ·). Note that by specifying v = ¯v∗ in (C.1) and λ = λ∗ in (C.2), we can sum up

19

Zhang, Bedi, Koppel, and Wang

the inequlities (C.1) and (C.2) for t = 1, ..., T to yield

(cid:0)−(cid:104)wt, λt − λ∗(cid:105) + (cid:104)∇vL(vt, λt), vt − ¯v∗(cid:105)(cid:1)

T
(cid:88)

t=1

1
T
KL(cid:0)(1 −γ)λ∗||(1− γ)λ1(cid:1)
T (1− γ)β
(cid:123)(cid:122)
T1

(cid:125)

(cid:124)

≤

+

β
2T

T
(cid:88)

(cid:88)

t=1

s,a

sa(∆t
λt

sa)2

+

(cid:124)

(cid:123)(cid:122)
T2

1
T
(cid:124)

T
(cid:88)

(cid:104) ˆ∂λL(vt, λt)−wt, λt −λ∗(cid:105)

t=1

(cid:123)(cid:122)
T3

(cid:125)

+

(cid:107)v1 − ¯v∗(cid:107)2
2T α
(cid:123)(cid:122)
T4

(cid:124)

(cid:125)

T
(cid:88)

t=1

+

α
2T
(cid:124)

(cid:107) ˆ∇vL(vt, λt)(cid:107)2

+

(cid:123)(cid:122)
T5

(cid:125)

(cid:104)∇vL(vt, λt) − ˆ∇vL(vt, λt), vt − ¯v∗(cid:105)

.

(cid:123)(cid:122)
T6

(cid:125)

(cid:125)

T
(cid:88)

t=1

1
T
(cid:124)

Substitute this inequality into (C.3) and take the expectation on both sides, we get

E[L(¯v, λ∗) − min
v∈V

L(v, ¯λ)] ≤

6
(cid:88)

i=1

E[Ti].

(C.4)

For the E[Ti]’s, the following bounds hold with detailed derivation provided in Appendix C.3:

E[T1] ≤

log(|S||A|)
T (1 − γ)β

,

E[T2] ≤

4βc2σ2
1 − γ

+

128β|S||A|(1 + cσ)2
(1 − γ)3

,

E[T3] = 0,

E[T4] ≤

8|S|(1 + cσ)2
T α(1 − γ)2 ,

E[T5] ≤

27α
2(1 − γ)2 ,

E[T6] ≤

3(cid:112)3|S|(1 + cσ)
T (1 − γ)2

√

.

Substitute these bounds for E[Ti]’s into inequality (C.4) we get

E[L(¯v, λ∗) − min
v∈V

L(v, ¯λ)] ≤

log(|S||A|)
T (1 − γ)β

+

4βc2σ2
1 − γ

+

128β|S||A|(1 + cσ)2
(1 − γ)3
3(cid:112)3|S|(1 + cσ)
T (1 − γ)2

√

27α
2(1 − γ)2 +

.

(C.5)

+

8|S|(1 + cσ)2
T α(1 − γ)2 +
(cid:113) |S|

If we choose β = 1−γ
1+cσ

T |S||A| and α =

T (1 + cσ), we have

(cid:113) log(|S||A|)

E[L(¯v, λ∗) − min
v∈V

L(v, ¯λ)] ≤ O

(cid:32)(cid:114)

|S||A| log(|S||A|)
T

·

1 + cσ
(1 − γ)2

(cid:33)

,

which completes the proof.

C.1 Proof of Lemma C.1

Proof. Consider the update rule of v provided in (4.5). For any v ∈ V, it holds that
(cid:107)vt+1 − v(cid:107)2 = (cid:107)ΠV (vt − α ˆ∇vL(vt, λt)) − v(cid:107)2

≤ (cid:107)vt − α ˆ∇vL(vt, λt) − v(cid:107)2
= (cid:107)vt − v(cid:107)2 + α2(cid:107) ˆ∇vL(vt, λt)(cid:107)2 − 2α(cid:104) ˆ∇vL(vt, λt), vt − v(cid:105)
= (cid:107)vt − v(cid:107)2 + α2(cid:107) ˆ∇vL(vt, λt)(cid:107)2 − 2α(cid:104) ˆ∇vL(vt, λt) − ∇vL(vt, λt) + ∇vL(vt, λt), vt − v(cid:105).

Rearranging the above inequality yields
2α(cid:104)∇vL(vt, λt), vt−v(cid:105) ≤ (cid:107)vt−v(cid:107)2−(cid:107)vt+1−v(cid:107)2+α2(cid:107) ˆ∇vL(vt, λt)(cid:107)2−2α(cid:104) ˆ∇vL(vt, λt)−∇vL(vt, λt), vt−v(cid:105).

Deviding both sides by 2α proves lemma.

20

Distributional Risk in the Dual Domain

C.2 Proof of Lemma C.2

Proof. Now let us consider the update rule of λ given by (4.6) and (4.7). Note that in the subproblem
(4.6), the problem is separable for each component of λ and allows for a closed form solution, i.e.,

λt+ 1
sa =argmax

2

∆t

saλsa −

λsa
sa · exp{β∆t

sa},

=λt

1
(1 − γ)β

(1 − γ)λsa log

(cid:19)

(cid:18) (1 − γ)λsa
(1 − γ)λt
sa

(C.6)

where we denote ∆t
as

sa to be the (s, a)-th component of ˆ∂λL(vt, λt). Then the next iterate is constructed

λt+1 =

Or in a more elementary way, we deﬁne

λt+ 1
(1 − γ)(cid:107)λt+ 1

2

2 (cid:107)1

.

λt+1
sa =

sa · exp{β∆t
λt
(1 − γ) (cid:80)
s(cid:48),a(cid:48) λt

sa}
s(cid:48)a(cid:48) · exp{β∆t

s(cid:48)a(cid:48)}

.

It is straightforward that λt+1 ∈ L. As a result, for any λ ∈ L,

KL(cid:0)(1 − γ)λ || (1 − γ)λt+1(cid:1) − KL(cid:0)(1 − γ)λ || (1 − γ)λt(cid:1)
(cid:18) λsa
λt+1
sa
(cid:19)

(cid:18) λsa
λt
sa

− λsa log

λsa log

(cid:19)(cid:19)

(cid:88)

(cid:88)

a∈A

s∈S

(cid:18)

(cid:19)

=(1 − γ)

(cid:88)

(cid:88)

s∈S

a∈A

(cid:88)

(cid:88)

s∈S

a∈A

=(1 − γ)

=(1 − γ)



λsa log



(cid:18) λt
sa
λt+1
sa


λsa

log

(1 − γ)

(cid:88)

s(cid:48),a(cid:48)


s(cid:48)a(cid:48) · exp{β∆t
λt

s(cid:48)a(cid:48)}


 − β∆t
sa

(C.7)

(C.8)





(C.9)

= log

(1 − γ)

(cid:32)

= log

(1 − γ)

(cid:88)

s(cid:48),a(cid:48)

s(cid:48)a(cid:48) · exp{β∆t
λt

s(cid:48)a(cid:48)}

 − (1 − γ)β

(cid:88)

(cid:88)

s∈S

a∈A

λsa∆t
sa

(cid:33)

sa · exp{β∆t
λt

sa}

− (1 − γ)β(cid:104) ˆ∂λL(vt, λt), λ(cid:105).

(C.10)

(cid:88)

s,a

The equality in (C.9) is obtained by using the elementary deﬁnition of λt+1
of (C.10) is obtained by applying the deﬁnition of ∆t

sa. Note that

sa in (C.7); The last equality

∆t

sa =




ˆrsts(cid:48)

tat

+γvs(cid:48)
t
ζt
stat
(cid:17)

(cid:16) ˆ∂ρ(λt)



−c

−vst −M1

(cid:16) ˆ∂ρ(λt)

− c

(cid:17)

stat

− M2,

if (s, a) = (st, at),

− M2,

if (s, a) (cid:54)= (st, at).

stat

When we choose M1 = 4(1 − γ)−1(1 + cσ) and M2 = cσ, we can guarantee that ∆t
s ∈ S, a ∈ A. Therefore, by the fact that ex ≤ 1 + x + x2

sa ≤ 0 for all
2 for all x ≤ 0 and log(1 + x) ≤ x for all

21

Zhang, Bedi, Koppel, and Wang

x > −1, we have

(cid:32)

log

(1 − γ)

(cid:88)

s,a

(cid:33)

(cid:32)

sa · exp{β∆t
λt

sa}

≤ log

(1 − γ)

sa · (cid:0)1 + β∆t
λt

sa +

(cid:88)

s,a

(cid:33)

β2
2

(∆t

sa)2(cid:1)

(cid:32)

= log

1 + (1 − γ)β(cid:104) ˆ∂λL(vt, λt), λt(cid:105) +

(1 − γ)β2
2

(cid:88)

s,a

(cid:33)

sa(∆t
λt

sa)2

≤(1 − γ)β(cid:104) ˆ∂λL(vt, λt), λt(cid:105) +

(1 − γ)β2
2

(cid:88)

s,a

sa(∆t
λt

sa)2.

(C.11)

Utilizing the upper bound of (C.11) into the right hand side of (C.8) results in

KL(cid:0)(1 − γ)λ || (1 − γ)λt+1(cid:1) − KL(cid:0)(1 − γ)λ || (1 − γ)λt(cid:1)
(1 − γ)β2
2

sa)2 + (1 − γ)β(cid:104) ˆ∂λL(vt, λt) − wt + wt, λt − λ(cid:105).

sa(∆t
λt

(cid:88)

s,a

≤

Rearranging the terms and deviding both sides by (1 − γ)β proves this lemma.

C.3 Bounding the E[Ti]’s
Step 1. Bounding E[T1]. Note that λ1 =

1

(1−γ)|S||A| , we know

E[T1] =

≤

=

1
T (1 − γ)β

1
T (1 − γ)β

(cid:88)

(1 − γ)λ∗
sa

(cid:0)log(λ∗

sa) − log(|S|−1|A|−1)(cid:1)

(C.12)

s,a
(cid:88)

(1 − γ)λ∗

sa log(|S||A|)

s,a

log(|S||A|)
T (1 − γ)β

.

Step 2. Bounding E[T2]. For each t, we have

(cid:34)

(cid:88)

E

s,a

sa(∆t
λt

sa)2(cid:12)

(cid:12)vt, λt

(cid:35)

= Est,at

(cid:34)

(cid:88)

λt
sa

(cid:18) ˆrss(cid:48)a + γvs(cid:48) − vs − M1
ζ t
sa

· 1(s,a)=(st,at) − c

(cid:17)
(cid:16) ˆ∂ρ(λt)

− M2

sa

(cid:35)

(cid:19)2 (cid:12)
(cid:12)
(cid:12)
(cid:12)

vt, λt

≤ 2Est,at

s,a
(cid:34)

(cid:88)

s,a

(cid:16)

(cid:17)
(cid:16) ˆ∂ρ(λt)

c

λt
sa

+ M2

sa

(cid:17)2

+ λt

st,at

(cid:18) ˆrsts(cid:48)

tat + γvs(cid:48)
ζ t
stat

t

− vst − M1

(cid:35)

(cid:19)2 (cid:12)
(cid:12)
(cid:12)
(cid:12)

vt, λt

≤ 8(1 − γ)−1c2σ2 + 2

≤ 8(1 − γ)−1c2σ2 + 2

≤ 8(1 − γ)−1c2σ2 + 2

(cid:88)

s,a

(cid:88)

s,a

(cid:88)

s,a

saζ t
λt
sa

(cid:18) ˆrss(cid:48)a + γvs(cid:48) − vs − M1
ζ t
sa

(cid:19)2

sa (ˆrss(cid:48)a + γvs(cid:48) − vs − M1)2
λt
(1 − δ)(1 − γ)λt

sa + δ
sa(1 − γ)−2(1 + cσ)2

64λt
(1 − δ)(1 − γ)λt

sa + δ

|S||A|

|S||A|

≤ 8(1 − γ)−1c2σ2 +

≤ 8(1 − γ)−1c2σ2 +

128|S||A|(1 + cσ)2
(1 − δ)(1 − γ)3
256|S||A|(1 + cσ)2
(1 − γ)3

.

22

Distributional Risk in the Dual Domain

The second row follows the deﬁnition of ∆t
In the 5-th we substitute the deﬁnition of ζ t
the detailed value of M1; The 8-th row is because δ ∈ (0, 1

sa; The 4-th row is due to the assumption that (cid:107) ˆ∂ρ(cid:107)∞ ≤ σ;
sa provided in Algorithm 1; In the 6-th row we substitute
2 ). As a result, we have

E[T2] =

β
2T

T
(cid:88)

E

(cid:34)

(cid:88)

t=1

s,a

(cid:35)

sa(∆t
λt

sa)2

≤

4βc2σ2
1 − γ

+

128β|S||A|(1 + cσ)2
(1 − γ)3

.

(C.13)

Step 3. Bounding E[T3], because λ∗ is a constant, for each t, we have

E[(cid:104) ˆ∂λL(vt, λt) − wt, λt − λ(cid:105)|vt, λt] = −(cid:104)(M1 + M2) · 1, λt − λ∗(cid:105) = 0,

where we have applied the fact that (cid:80)
when ζ t > 0. As a result,

s,a λt

sa = (cid:80)

s,a λ∗

sa, and wt = E[ ˆ∂λL(vt, λt)|vt, λt]+(M1+M2)·1

E[T3] =

1
T

T
(cid:88)

t=1

E

(cid:104)

(cid:105)
(cid:104) ˆ∂λL(vt, λt) − wt, λt − λ∗(cid:105)

= 0.

Step 4. Bounding E[T4], we have

E[T4] =

1
2T α

E (cid:2)(cid:107)v1 − ¯v∗(cid:107)2(cid:3) ≤

8|S|(1 + cσ)2
T α(1 − γ)2 .

Step 5. Bounding E[T5], applying the expression (4.8) yields

(C.14)

(C.15)

E

(cid:104)
(cid:107) ˆ∇vL(vt, λt)(cid:107)2(cid:12)

(cid:12)vt, λt(cid:105)

= Est,at,s(cid:48)

t,¯st

(cid:20)
(cid:13)
(cid:13)e¯st +

λt
stat
ζ t
stat

(γes(cid:48)

t

− est )(cid:13)
2
(cid:13)

(cid:21)

vt, λt

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:34)

(cid:20)

(cid:13)
(cid:13)e¯st +

λt

stat
(1 − δ)(1 − γ)λt

3 +

3γ2 + 3
(1 − δ)2(1 − γ)2

(cid:12)
(cid:12)
vt, λt
(cid:12)
(cid:12)

stat

+ δ
(cid:21)

|S||A|

(γes(cid:48)

t

− est)(cid:13)
2
(cid:13)

(cid:35)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

vt, λt

= Est,at,s(cid:48)

t,¯st

≤ Est,at,s(cid:48)

t,¯st

≤

27
(1 − γ)2 .

Consequently,

E[T5] =

α
2T

T
(cid:88)

t=1

E

(cid:104)

(cid:107) ˆ∇vL(vt, λt)(cid:107)2(cid:105)

≤

27α
2(1 − γ)2 .

(C.16)

Step 6. Bounding E[T6]. Because ¯v∗ is a random variable dependent on ˆ∇vL(vt, λt) we will need

the following proposition.

Proposition C.3 ((Bach and Levy, 2019)). Let Z ⊆ Rd be a convex set and w : Z → R be a 1
strongly convex function with respect to norm (cid:107)·(cid:107) over Z. With the assumption that for all x ∈ Z we
k=1 ∈ Rd and
have w(x) − minx∈Z w(x) ≤ 1
any random vector z ∈ Z, it holds that

2 D2, then for any martingale diﬀerence sequence {Zk}K

(cid:34) K
(cid:88)

E

(cid:35)

(cid:104)Zk, x(cid:105)

≤

k=1

where (cid:107)·(cid:107)∗ denotes the dual norm of (cid:107)·(cid:107).

(cid:118)
(cid:117)
(cid:117)
(cid:116)

K
(cid:88)

(cid:104)

E

(cid:107)Zk(cid:107)2
∗

(cid:105)

,

k=1

D
2

23

Zhang, Bedi, Koppel, and Wang

With this proposition, and note that E

(cid:104)

(cid:104) ˆ∇vL(vt, λt)(cid:12)

(cid:12)vt, λt(cid:105)

= ∇vL(vt, λt), we have

E[T6] =

=

≤

≤

≤

≤

1
T

1
T

T
(cid:88)

t=1

T
(cid:88)

t=1

E

(cid:104)

(cid:105)
(cid:104)∇vL(vt, λt) − ˆ∇vL(vt, λt), vt − ¯v∗(cid:105)

(C.17)

E

(cid:104)

(cid:105)
(cid:104)∇vL(vt, λt) − ˆ∇vL(vt, λt), ¯v∗(cid:105)

(cid:112)|S|(1 + cσ)
T (1 − γ)

(cid:118)
(cid:117)
(cid:117)
(cid:116)

T
(cid:88)

t=1

(cid:104)

E

(cid:107)∇vL(vt, λt) − ˆ∇vL(vt, λt)(cid:107)2

(cid:105)

T
(cid:88)

(cid:104)

E

(cid:107) ˆ∇vL(vt, λt)(cid:107)2

(cid:105)

(cid:118)
(cid:117)
(cid:117)
(cid:116)

(cid:112)|S|(1 + cσ)
T (1 − γ)
(cid:112)|S|(1 + cσ)
T (1 − γ)
3(cid:112)3|S|(1 + cσ)
T (1 − γ)2

√

(cid:114)

E[T5]

t=1

2T
α

.

Appendix D. Proof of Theorem 5.4
Proof. The ﬁrst row of (5.7) is directly satisﬁed due to the feasibility of ¯λ ∈ L. Now we prove the
second row of (5.7). When the parameters are chosen according to Theorem 5.3, we know

(cid:15) ≥ E[L(¯v, λ∗) − min
v∈V

L(v, ¯λ)].

(D.1)

For the ease of notation, denote C := (1 − γ)−1(1 + cσ). Then substitute the details of L we get

L(v, ¯λ) =

min
v∈V

min
(cid:107)v(cid:107)∞≤2C

(cid:104)¯λ, r(cid:105) − cρ(¯λ) + (cid:104)ξ, v(cid:105) +

(cid:88)

a∈A

¯λa(γPa − I)v

(D.2)

= (cid:104)¯λ, r(cid:105) − cρ(¯λ) − 2C

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

a∈A

(I − γP (cid:62)

a )¯λa − ξ

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

.

By the feasibility of λ∗, namely, (cid:80)

a∈A(I − γP (cid:62)

a )λ∗

a − ξ = 0, we have

L(¯v, λ∗) = (cid:104)λ∗, r(cid:105) − cρ(λ∗) + (cid:104)ξ, ¯v(cid:105) +

(cid:88)

a∈A

(λ∗

a)(cid:62)(γPa − I)¯v = (cid:104)λ∗, r(cid:105) − cρ(λ∗).

(D.3)

Substituting (D.2) and (D.3) into (D.1) yields

(cid:34)
((cid:104)λ∗, r(cid:105) − cρ(λ∗)) − (cid:0)(cid:104)¯λ, r(cid:105) − cρ(¯λ)(cid:1) + 2C

E

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

a∈A

(I − γP (cid:62)

a )¯λa − ξ

(cid:35)

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

≤ (cid:15).

(D.4)

24

Distributional Risk in the Dual Domain

Actually, this inequality has already proved the bound (5.8) in terms of the objective value of problem
(3.1). Also, by the feasibility of λ∗, the convexity of ρ and the optimality condition (B.4), we have

((cid:104)λ∗, r(cid:105) − cρ(λ∗)) − (cid:0)(cid:104)¯λ, r(cid:105) − cρ(¯λ)(cid:1) + (cid:10)v∗,

= ((cid:104)λ∗, r(cid:105) − cρ(λ∗)) − (cid:0)(cid:104)¯λ, r(cid:105) − cρ(¯λ)(cid:1) + (cid:10)v∗,

(cid:88)

a∈A
(cid:88)

a∈A

(I − γP (cid:62)

a )¯λa − ξ(cid:11)

(I − γP (cid:62)

a )¯λa −

(I − γP (cid:62)

a )λ∗
a

(cid:11)

(cid:88)

a∈A

(cid:10)(I − γPa)v∗ − ra + cu∗

a, ¯λa − λ∗

a

(cid:11)

≥

(cid:88)

a∈A

≥ 0,

where u∗ ∈ ∂ρ(λ∗) is deﬁned in (B.4), and u∗
implies

a := [u∗

1a, ..., u∗

|S|a](cid:62) is column vector. Immediately, this

((cid:104)λ∗, r(cid:105) − cρ(λ∗)) − (cid:0)(cid:104)¯λ, r(cid:105) − cρ(¯λ)(cid:1) ≥ −(cid:10)v∗,

(cid:88)

(I − γP (cid:62)

a )¯λa − ξ(cid:11)

(D.5)

≥ −(cid:107)v∗(cid:107)∞

a∈A
(cid:13)
(cid:13)

(cid:88)

(I − γP (cid:62)

a )¯λa − ξ(cid:13)
(cid:13)1

≥ −C(cid:13)
(cid:13)

(cid:88)

a∈A

a∈A
(I − γP (cid:62)

a )¯λa − ξ(cid:13)
(cid:13)1.

where we used the fact that (cid:107)v∗(cid:107)∞ ≤ C proved in Lemma 5.2. Substitute (D.5) into (D.4) gives

E(cid:2)C(cid:13)
(cid:13)

(cid:88)

(I − γP (cid:62)

a )¯λa − ξ(cid:13)
(cid:13)1

(cid:3) ≤ (cid:15).

a∈A

Divide both sides by C = (1 − γ)−1(1 + cσ) proves inequality (5.7).

Appendix E. Proof of Proposition 5.5

Proof. Let v∗ be the optimal for the saddle point problem (4.1). Then the duality gap also guarantees
that

E[L(¯v, λ∗) − L(v∗, ¯λ)] ≤ (cid:15).

Substituting the detailed form of L(·, ·) we get E[T1 + T2] ≤ (cid:15) where

T1 = (cid:104)ξ, ¯v(cid:105) − cρ(λ∗, r) − [(cid:104)ξ, v∗(cid:105) − cρ(¯λ, r)]

(E.1)

and

(cid:88)

T2 =

(cid:104)¯λa, (I − γPa)¯v − ra(cid:105) −

Note that for λ∗, (cid:80)

a )λ∗

a = ξ. Consequently,

a∈A
a∈A(I − γP (cid:62)

(cid:104)λ∗

a, (I − γPa)v∗ − ra(cid:105).

(cid:88)

a∈A

T2 =

=

(cid:88)

a∈A
(cid:88)

a∈A

(cid:104)(I − γPa)v∗ − ra, ¯λa − λ∗

a(cid:105) +

(cid:88)

(cid:104)λ∗

a, (I − γPa)(v∗ − ¯v)(cid:105)

(cid:104)(I − γPa)v∗ − ra, ¯λa − λ∗

a∈A
a(cid:105) + ξ(cid:62)(v∗ − ¯v).

25

Zhang, Bedi, Koppel, and Wang

Let us deﬁne D(¯λ, λ∗) := ρ(¯λ) − ρ(λ∗) − (cid:104)∇ρ(λ∗), ¯λ − λ∗(cid:105) ≥ 0. Combine this equality for T2 and the
deﬁnition of T1 in (E.1), we get

(cid:15) ≥ E[T1 + T2]

(cid:20) (cid:88)

= E

a∈A
(cid:124)

(cid:104)(I − γPa)v∗ − ra + c∇λa ρ(λ∗), ¯λa − λ∗
a(cid:105)

(cid:21)

+ cE[D(¯λ, λ∗)]

(cid:123)(cid:122)
≥0 due to optimality condition (B.4)

(cid:125)

≥ cE[D(¯λ, λ∗)].

Now let us denote θ = (1 − γ) for the ease of notation. By direct calculation, we compute the function
D as follows

D(¯λ, λ∗)

= ρ(¯λ) − ρ(λ∗) − (cid:104)∇ρ(λ∗), ¯λ − λ∗(cid:105)

(cid:88)

(cid:88)

s∈S

a∈A

(cid:88)

(cid:88)

(cid:18)

(cid:18)

θ¯λsa log

θ¯λsa log

=

=

s∈S

a∈A
= KL(θ¯λ||θλ∗),

(cid:19)

(cid:19)

(cid:18) θ¯λsa
µsa
(cid:18) θ¯λsa
µsa

− θλ∗

sa log

− θ¯λsa log

(cid:18) θλ∗
sa
µsa
(cid:18) θλ∗
sa
µsa

(cid:19)

(cid:18)

−

θ log

(cid:18) θλ∗
sa
µsa

(cid:19)

(cid:19)

+ θ

(cid:19)

− θ(¯λsa − λ∗

sa)

(cid:19)

(cid:19)

(¯λsa − λ∗

sa)

where the last inequality use the fact that (cid:80)
the proof.

s∈S

(cid:80)

a∈A θ¯λsa = 1 = (cid:80)

s∈S

(cid:80)

a∈A θλ∗

sa. This completes

Appendix F. Proof of Theorem 5.6

Proof. For the ease of presentation, let us ﬁrst prove two lemmas.

Lemma F.1. For each subproblem (5.9), there is a closed form solution. For (5.10), we apply
Algorithm 1 with the number of iterations T set according to Theorem 5.6. Then for all λk and µk
with k ∈ {1, ..., K}, the approximate feasibility condition (5.13) is satisﬁed. Furthermore we have

E

(cid:20)
Φ(λk+1, µk+1) − max
λ∈Λ

(cid:21)
Φ(λ, µk+1)

≥ −(cid:15).

The proof of this lemma is provided in Appendix F.1.

Lemma F.2. Suppose the sequence {(λk, µk)} is generated by Algorithm 2 with the parameters set
by Theorem 5.6. For each iteration of Algorithm 2, let us deﬁne

Then the output (λk∗

, µk∗

) solution satisﬁes

λk+1
∗ = arg max
λ∈Λ

Φ(λ, µk+1).

E[(cid:107)λk∗

− λk∗

∗ (cid:107)2] ≤

2(cid:15)
M

and

E

(cid:104)
(cid:107)λk∗

∗ − λk∗−1(cid:107)2(cid:105)

≤

4(cid:15)
M

.

The proof of this lemma is provided in Appendix F.2. Based on this result, let us bound the

expected squared projected gradients. By the optimality of µk∗

for subproblem (5.9),

ΠU (∇µΦ(λk∗−1, µk∗

)) = 0.

26

Distributional Risk in the Dual Domain

Consequently,

))(cid:107)2
)) − ΠU (∇µΦ(λk∗−1, µk∗

(cid:107)ΠU (∇µΦ(λk∗
, µk∗
= (cid:107)ΠU (∇µΦ(λk∗
, µk∗
≤ (cid:107)∇µΦ(λk∗
, µk∗
) − ∇µΦ(λk∗−1, µk∗
− λk∗−1, r(cid:105)r + 2M (λk∗
= (1 − γ)2(cid:107)4c(cid:104)λk∗
− λk∗−1(cid:107)2
2(cid:107)λk∗
≤ (1 − γ)2(cid:107)4crr(cid:62) + 2M I(cid:107)2
− λk∗−1(cid:107)2
= 4(1 − γ)2(2c|S||A| + M )2(cid:107)λk∗
≤ 8(1 − γ)2(2c|S||A| + M )2 (cid:16)
∗ − λk∗−1(cid:107)2 + (cid:107)λk∗

(cid:107)λk∗

))(cid:107)2

)(cid:107)2

− λk∗−1)(cid:107)2

− λk∗

∗ (cid:107)2(cid:17)

.

In the above arguments, the fourth row is yielded by directly substituting the formulas of ∇µΦ(λk∗
and ∇µΦ(λk∗−1, µk∗
provided in Lemma F.2 yields

, µk∗
)
); the sixth row is due to (cid:107)rr(cid:62)(cid:107)2 = (cid:107)r(cid:107)2 ≤ |S||A|. Substituting the bounds

(cid:104)

E

(cid:107)ΠU (∇µΦ(λk∗

, µk∗

))(cid:107)2(cid:105)

≤

48(cid:15)
M

(1 − γ)2(2c|S||A| + M )2.

(F.1)

Similarly,

, µk∗
, µk∗
) − ∇λΦ(λk∗
∗ − λk∗

(cid:107)ΠΛ(∇λΦ(λk∗
))(cid:107)2
)) − ΠΛ(∇λΦ(λk∗
= (cid:107)ΠΛ(∇λΦ(λk∗
∗ , µk∗
= (cid:107)∇λΦ(λk∗
, µk∗
)(cid:107)2
∗ − λk∗
, r(cid:105) · r + M (λk∗
= (1 − γ)4(cid:107)2c(cid:104)λk∗
∗ − λk∗
2(cid:107)λk∗
≤ (1 − γ)4(cid:107)2crr(cid:62) + M I(cid:107)2
(cid:107)2
≤ (1 − γ)4(2c|S||A| + M )2(cid:107)λk∗
∗ − λk∗
(cid:107)2

∗ , µk∗

))(cid:107)2

)(cid:107)2

≤

16(cid:15)
M

(1 − γ)4(2c|S||A| + M )2.

Combine the above inequality with (F.1), we get

E(cid:2)(cid:107)ΠΛ(∇λΦ(λ, µ))(cid:107)2 + (cid:107)ΠU (∇µΦ(λ, µ))(cid:107)2(cid:3) ≤ O

(cid:18)

(1 − γ)2

(cid:18) c2|S|2|A|2
M

(cid:19)

(cid:19)

+ M

(cid:15)

.

F.1 Proof of Lemma F.1

Proof. First, let us consider the subproblem (5.9) for updating µ. Note that for any ﬁxed λ, we
rewrite the subproblem as follows (constants omitted)

max
µ

2c(cid:104)ˆλ, r(cid:105)(cid:104)µ, r(cid:105) − c(cid:104)µ, R(cid:105) −

M
2

(cid:107)µ − ˆλ(cid:107)2

s.t. µ ≥ 0, (cid:107)µ(cid:107)1 = 1.

With a few more reformulation, this is actually a projection problem:

(cid:13)
(cid:13)
(cid:13)
(cid:13)

µ −

(cid:18)

ˆλ +

2
M

min
µ

(2(cid:104)ˆλ, r(cid:105) · r − R)

(cid:19)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

s.t. µ ≥ 0, (cid:107)µ(cid:107)1 = 1.

This problem has a closed form solution and can be implemented within O(|S||A| log(|S||A|)) cost,
see (Wang and Carreira-Perpin´an, 2013).

27

Zhang, Bedi, Koppel, and Wang

Second, we consider the subproblem (5.10). As a special case of problem (3.1) whose sample
complexity is fully characterized by Theorem 5.3 and Theorem 5.4, all we need to do here is to specify
the constant σ with the following “ρ function”:

ρ(λ) = (cid:104)ˆλ, r(cid:105)2 − 2(cid:104)µ, r(cid:105)(cid:104)ˆλ, r(cid:105) + (cid:104)µ, R(cid:105) +

M
2c

(cid:107)ˆλ − µ(cid:107)2,

where ˆλ = (1 − γ)λ. Note that ∇ρ(λ) = (1 − γ) · (2((cid:104)ˆλ, r(cid:105) − (cid:104)µ, r(cid:105)) · r + M
directly set ˆ∂ρ(λ) := ∇ρ(λ), we have

c (ˆλ − µ)). Then, if we

σ = sup
λ∈L

(cid:107)∇ρ(λ)(cid:107)∞ ≤ (1 − γ)(1 + M/c).

Therefore, Theorem 5.3 and Theorem 5.4 tell us that the required sample complexity is

T = Θ

(cid:18) |S||A| log(|S||A|)(1 + cσ)2
(1 − γ)4(cid:15)2

(cid:19)

= Θ

(cid:18) |S||A| log(|S||A|)
(1 − γ)4(cid:15)2

(cid:0)1 + (1 − γ)2(c2 + M 2)(cid:1)

(cid:19)

.

Thus we complete the proof.

F.2 Proof of Lemma F.2

Proof. By Lemma F.1, we have

E[Φ(λk+1, µk+1) − Φ(λk+1

∗

, µk+1)] ≥ −(cid:15).

(F.2)

By M -strongly concavity of Φ(·, µk), we know

M
2

(cid:107)λk+1 − λk+1

∗

(cid:107)2 ≤ E[Φ(λk+1

∗

, µk+1) − Φ(λk+1, µk+1)] ≤ (cid:15).

Dividing both sides by M/2 proves the ﬁrst inequality of the lemma. Again, by M -strongly concavity
of Φ(·, µk), we also have

Φ(λk+1
∗

, µk+1) ≥ Φ(λk, µk+1) +

M
2

(cid:107)λk+1

∗ − λk(cid:107)2.

Because µk+1 = arg maxµ Φ(λk, µ) s.t. µ ≥ 0, (cid:107)µ(cid:107)1 = 1. We know

Φ(λk, µk+1) ≥ Φ(λk, µk).

(F.3)

(F.4)

Combining the above three inequalities, we have
E(cid:2)Φ(λk+1, µk+1) − Φ(λk, µk)(cid:3)
≥ E(cid:2)Φ(λk+1, µk+1) − Φ(λk, µk+1)(cid:3)
= E(cid:2)Φ(λk+1, µk+1) − Φ(λk+1

, µk+1)(cid:3) + E(cid:2)Φ(λk+1

, µk+1) − Φ(λk, µk+1)(cid:3)

∗

∗

≥

M
2

(cid:107)λk+1

∗ − λk(cid:107)2 − (cid:15),

where the second row is due to (F.4), the last row is due to (F.2) and (F.3). Summing up the above
inequalities over all k and then take the average, then we know

M
2K

K
(cid:88)

k=1

E (cid:2)(cid:107)λk

∗ − λk−1(cid:107)2(cid:3) ≤

maxλ,µ Φ(λ, µ) − Φ(λ0, µ0)
K

+(cid:15) ≤ 2(cid:15).

Because k∗ is randomly chosen from {1, ..., K}, we get

E

(cid:104)

(cid:107)λk∗

∗ − λk∗−1(cid:107)2(cid:105)

=

1
K

K
(cid:88)

k=1

E (cid:2)(cid:107)λk

∗ − λk−1(cid:107)2(cid:3) ≤

4(cid:15)
M

.

This proves the second inequality of this lemma.

28

