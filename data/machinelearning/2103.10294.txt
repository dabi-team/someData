1
2
0
2

r
a

M
8
1

]

G
L
.
s
c
[

1
v
4
9
2
0
1
.
3
0
1
2
:
v
i
X
r
a

Learning to Schedule Heuristics in Branch-and-Bound∗

Antonia Chmiela†,1, Elias B. Khalil2, Ambros Gleixner1,3, Andrea Lodi4, and Sebastian
Pokutta1,5

1Zuse Institute Berlin
2University of Toronto
3HTW Berlin
4Polytechnique Montr´eal
5TU Berlin

Abstract

Primal heuristics play a crucial role in exact solvers for Mixed Integer Programming (MIP). While solvers
are guaranteed to ﬁnd optimal solutions given suﬃcient time, real-world applications typically require
ﬁnding good solutions early on in the search to enable fast decision-making. While much of MIP research
focuses on designing eﬀective heuristics, the question of how to manage multiple MIP heuristics in a solver
has not received equal attention. Generally, solvers follow hard-coded rules derived from empirical testing
on broad sets of instances. Since the performance of heuristics is instance-dependent, using these general
rules for a particular problem might not yield the best performance. In this work, we propose the ﬁrst
data-driven framework for scheduling heuristics in an exact MIP solver. By learning from data describing
the performance of primal heuristics, we obtain a problem-speciﬁc schedule of heuristics that collectively
ﬁnd many solutions at minimal cost. We provide a formal description of the problem and propose an
eﬃcient algorithm for computing such a schedule. Compared to the default settings of a state-of-the-art
academic MIP solver, we are able to reduce the average primal integral by up to 49% on a class of
challenging instances.

1 Introduction

Many decision-making problems arising from real-world applications can be formulated using Mixed Integer
Programming (MIP). The Branch-and-Bound (B&B) framework is a general approach to solving MIPs to
global optimality. Over the recent years, the idea of using machine learning (ML) to improve optimization
techniques has gained renewed interest. There exist various approaches to tackle diﬀerent aspects of the
solving process using classical ML techniques. For instance, ML has been used to ﬁnd good parameter
conﬁgurations for a solver [Hutter et al., 2009, Hutter et al., 2011], improve node [He et al., 2014], variable
[Khalil et al., 2016, Nair et al., 2020] or cut [Baltean-Lugojan et al., 2019] selection strategies, and detect
decomposable structures [Kruber et al., 2017].

Even though exact MIP solvers aim for global optimality, ﬁnding good feasible solutions fast is at least as
important, especially in the presence of a time limit. The use of primal heuristics is crucial in ensuring

∗This work was partially funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under
Germany’s Excellence Strategy – The Berlin Mathematics Research Center MATH+ (EXC-2046/1, project ID: 390685689),
and by the German Federal Ministry of Education and Research (BMBF) within the Research Campus MODAL (grant numbers
05M14ZAM, 05M20ZBM).

†Corresponding author. Email: chmiela@zib.de

1

 
 
 
 
 
 
Learning to Schedule Heuristics in Branch-and-Bound

Figure 1 – Average solution success rates
of ten heuristics for two problem classes.
Heuristic success is problem-dependent: each pair
of blue-yellow bars belongs to one heuristic, and
the heuristics are sorted in descending order w.r.t.
the solution success rates for GISP (blue). The yel-
low bars representing the success rates for FCMNF
are far from being sorted, implying that the perfor-
mance of a heuristic is strongly problem-dependent.

Figure 2 – Primal gap for an exem-
plary GISP instance. Our method’s heuris-
tic schedule (orange) obtains better solutions
earlier than SCIP’s default (blue).

good primal performance in modern solvers. For instance, [Berthold, 2013a] showed that the primal bound–
the objective value of the best solution–improved on average by around 80% when primal heuristics were
used. Generally, a solver has a variety of primal heuristics implemented, where each class exploits a diﬀerent
idea to ﬁnd good solutions. During B&B, these heuristics are executed successively at each node of the
search tree, and improved solutions are reported back to the solver if found. Extensive overviews of diﬀerent
primal heuristics, their computational costs, and their impact in MIP solving can be found in [Lodi, 2013,
Berthold, 2013b, Berthold, 2018].

Since most heuristics can be very costly, it is necessary to be strategic about the order in which the heuristics
are executed and the number of iterations allocated to each, with the ultimate goal of obtaining good
primal performance overall. Such decisions are often made by following hard-coded rules derived from
testing on broad benchmark test sets. While these static settings yield good performance on average, their
performance can be far from optimal when considering speciﬁc families of instances. To illustrate this fact,
Figure 1 compares the success rates of diﬀerent primal heuristics for two problem classes: the Generalized
Independent Set Problem (GISP) [Hochbaum and Pathria, 1997, Colombi et al., 2017] and the Fixed-Charge
Multicommodity Network Flow Problem (FCMNF) [Hewitt et al., 2010].

In this paper, we propose a data-driven approach to systematically improve the use of primal heuristics
in B&B. By learning from data about the duration and success of every heuristic call for a set of training
instances, we construct a schedule of heuristics deciding when and for how long a certain heuristic should
be executed to obtain good primal solutions early on. As a result, we are able to signiﬁcantly improve the
use of primal heuristics which is shown in Figure 2 for one MIP instance.

Even through we will focus on improving primal performance of MIP solving, it is important to note that
ﬁnding good solutions faster also improves the overall running time of the solver. The B&B procedure
generates a search tree in which the nodes correspond to diﬀerent subproblems. To determine whether a
certain part of the tree should be further explored or pruned, we keep track of the incumbent, i.e., the best
feasible solution seen thus far. Hence, when good incumbents are found early on, the size of the search tree
may be signiﬁcantly reduced, leading to the problem being solved faster. On a standardized test set, primal
heuristics reduced the solving time by up to 30% [Berthold, 2013a].

1. We formalize the learning task of ﬁnding an eﬀective, cost-eﬃcient heuristic schedule on a training

dataset as a Mixed Integer Quadratic Program (see Section 3);

2. We propose an eﬃcient heuristic for solving the training (scheduling) problem and a scalable data

2

heuristics0.00.10.20.30.40.50.60.7solution success rateGISPFCMNF100101102103104time (in seconds)0.00.20.40.60.81.0primal gapscheduledefault SCIPLearning to Schedule Heuristics in Branch-and-Bound

collection strategy (see Section 4 and 5);

3. We perform extensive computational experiments on a class of challenging instances and demon-

strate the beneﬁts of our approach (see Section 6).

Since primal heuristics have such a signiﬁcant inﬂuence on the solving process, optimizing their usage is a
topic of ongoing research. For instance, by characterizing nodes with diﬀerent features, [Khalil et al., 2017]
propose an ML method to decide at which nodes heuristics should run to improve primal performance.
After that decision, all heuristics are executed according to the predeﬁned rules set by the solver. The
authors in [Hendel, 2018] and [Hendel et al., 2018] use bandit algorithms to learn from previous calls which
heuristics to execute ﬁrst. In contrast to the method proposed in this paper, their procedure only adapts
the order in which heuristics are executed. Furthermore, primal performance can also be improved by using
hyperparameter tuning [Hutter et al., 2009, Hutter et al., 2011], but generally come with extremely high
computational cost, since they do not exploit information about the structure of the problem.

2 Preliminaries

Let us consider a MIP of the form

cTx,

min
x∈Rn
s.t.

Ax ≤ b,
xi ∈ Z,
with matrix A ∈ Rm×n, vectors c ∈ Rn, b ∈ Rm, and index set I ⊆ [n]. A MIP can be solved using Branch-
and-Bound, a tree search algorithm that ﬁnds an optimal solution to (PMIP) by recursively partitioning
the original problem into linear subproblems. The nodes in the resulting search tree correspond to these
subproblems. Throughout this work, we assume that each node has a unique index that identiﬁes the node
even across branch-and-bound trees obtained for diﬀerent MIP instances. For a set of instances X , we denote
the union of the corresponding node indices by NX .

∀i ∈ I

(PMIP)

Primal Performance Metrics. Since we are interested in ﬁnding good solutions fast, we consider a
collection of diﬀerent metrics for primal performance. Beside statistics like the time to the ﬁrst/best solution
[Berthold, 2013a] as a
and the solution/incumbent success rate, we mainly focus on the primal integral
comprehensive measure of primal performance. Intuitively, this metric can be interpreted as a normalized
average of the incumbent value over time. Formally, if x is feasible and x∗ is an optimal (or best known)
solution to (PMIP), the primal gap of x is deﬁned as

γ(x) :=





0,
1,

if |cTx| = |cTx∗|,
if cTx · cTx∗ < 0,

|cTx − cTx∗|
max{|cTx|, |cTx∗|}

,

otherwise.

With xt denoting the incumbent at time t, the primal gap function p : R≥0 → [0, 1] is then deﬁned as

p(t) :=

(cid:40)

1,
γ(xt),

if no incumbent is found until time t,
otherwise.

For a time limit T ∈ R≥0, the primal integral P (T ) is then given by the area underneath the primal gap
function p up to time T ,

P (T ) :=

K
(cid:88)

i=1

p(ti−1)(ti − ti−1),

3

Learning to Schedule Heuristics in Branch-and-Bound

where (K − 1) incumbents have been found until time T , t0 = 0, tK = T , and t1, . . . , tK−1 are the points in
time at which new incumbents are found.

Figure 2 gives an example for the primal gap function. The primal integrals are the areas under each of the
curves. It is easy to see that ﬁnding near-optimal incumbents earlier shrinks the area under the graph of p,
resulting in a smaller primal integral.

3 Data-Driven Heuristic Scheduling

The performance of heuristics strongly depends on the set of problem instances they are applied to. Hence, it
is natural to consider data-driven approaches for optimizing the use of primal heuristics for the instances of
interest. Concretely, we consider the following practically relevant setting. We are given a set of heuristics H
and a homogeneous set of training instances X from the same problem class. In a data collection phase, we
are allowed to execute the B&B algorithm on the training instances, observing how each heuristic performs
at each node of each search tree. At a high level, our goal is then to leverage this data to obtain a schedule
of heuristics that minimizes a primal performance metric.

The speciﬁcs of how such data collection is carried out will be discussed later on in the paper. First, let us
examine the decisions that could potentially beneﬁt from a data-driven approach. Our discussion is inspired
by an in-depth analysis of how the source-open academic MIP solver SCIP [Gamrath et al., 2020] manages
primal heuristics. However, our approach is generic and is likely to apply to other MIP solvers.

3.1 Controlling the Order

One important degree of freedom in scheduling heuristics is the order in which a set of applicable heuristics
H is executed by the solver at a given node. This can be controlled by assigning a priority for each heuristic.
In a heuristic loop, the solver then iterates over the heuristics in decreasing priority. The loop is terminated
if a heuristic ﬁnds a new incumbent solution. As such, an ordering (cid:104)h1, . . . , hk(cid:105) that prioritizes eﬀective
heuristics can lead to time savings without sacriﬁcing primal performance.

3.2 Controlling the Duration

Furthermore, solvers use working limits to control the computational eﬀort spent on heuristics. Consider
Increasing the maximal diving depth increases the likelihood of ﬁnding
diving heuristics as an example.
an integer feasible solution. At the same time, this increases the overall running time. Figure 3 visualizes
this cost-beneﬁt trade-oﬀ empirically for three diﬀerent diving heuristics, highlighting the need for a careful
“balancing act”. For a heuristic h ∈ H, let τ ∈ R>0 denote h’s time budget. Then, we are interested in
ﬁnding a schedule S deﬁned by

S := (cid:104)(h1, τ1), . . . , (hk, τk)(cid:105), hi ∈ H.

Since controlling the time budget directly can be unreliable and lead to nondeterministic behavior in practice
(see Appendix B for details), a deterministic proxy measure is preferable. For diving heuristics, the maximal
diving depth provides a suitable measure as demonstrated by Figure 3. Similar measures can be used for
other types of heuristics, as we will demonstrate with Large Neighborhood Search heuristics in Section 6. In
general, we will refer to τi as the maximal number of iterations that is alloted to a heuristic hi in schedule S.

4

Learning to Schedule Heuristics in Branch-and-Bound

Figure 3 – Number of solutions found (in percent) and cost of diﬀerent diving heuristics depending on the the
maximal diving depth: This ﬁgure shows the average number of solutions found by a heuristic (left) and average duration
in seconds (right) of three diving heuristics when limiting the maximal depth of a dive. Hereby, the baseline for the values on
the vertical axis of the left ﬁgure is the number of found solutions by the heuristics with no limitations on the diving depth.
The likelihood of ﬁnding a solution increases with the maximal diving depth. At the same time, an average call to all three
heuristics becomes more expensive as the diving depth increases.

3.3 Deriving the Scheduling Problem

Having argued for order and duration as suitable control decisions, we will now formalize our heuristic
Ideally, we would like to construct a single schedule S that minimizes the primal
scheduling problem.
integral, as deﬁned in Section 2, averaged over the training instances X . Unfortunately, it is very diﬃcult
to optimize the primal integral directly, as it depends on the sequence of incumbents found over time during
B&B. The primal integral also depends on the way the search tree is explored, which is aﬀected by pruning,
further complicating any attempt at directly optimizing this primal metric.

We address this diﬃculty by considering a more practical surrogate objective. Recall that NX denotes the
collection of search tree nodes of the set of training instances X . We will construct a schedule S that ﬁnds
feasible solutions for a large fraction of the nodes in NX , while also minimizing the number of iterations
spent by schedule S. Note that we consider feasible solutions instead of incumbents here: This way, we are
able to obtain more data faster since a heuristic ﬁnds a feasible solution more often than a new incumbent.
The framework we propose in the following can handle incumbents instead, but we have found no beneﬁt in
that in preliminary experiments.

For a heuristic h ∈ H and node N , denote by t(h, N ) the iterations necessary for h to ﬁnd a solution at node
N , and set t(h, N ) = ∞ if h does not succeed at N . Now suppose a schedule S is successful at node N , i.e.,
some heuristic ﬁnds a solution within the budget allocated to it in S. Let

jS = min{j = 1, . . . , |H| | t(hj, N ) ≤ τj}

be the index of the ﬁrst successful heuristic. Following the (successful) execution of hjS , the heuristic loop
is terminated, and the time spent by schedule S at node N is given by

T (S, N ) :=

jS −1
(cid:88)

i=1

τi + t(hjS , N ).

Otherwise, set T (S, N ) := (cid:80)k

i=1 τi + 11.

Furthermore, let NS denote the set of nodes at which schedule S is successful in ﬁnding a solution. Then,
we consider the heuristic scheduling problem given by

min
S∈S

(cid:88)

N ∈NX

T (S, N ) s.t.

|NS| ≥ α|NX |.

(PS )

1We add 1 to penalize unsolved nodes.

5

050100150200250300350maximal diving depth0.00.20.40.60.81.0number of solutions found  (in percent)050100150200250300350maximal diving depth0.000.010.020.030.040.050.060.07time per call (in seconds)Learning to Schedule Heuristics in Branch-and-Bound

Here α ∈ [0, 1] denotes a minimum fraction of nodes at which we want the schedule to ﬁnd a solution.
Problem (PS ) can be formulated as a Mixed-Integer Quadratic Program (MIQP); the exact formulation can
be found in Appendix A.

To ﬁnd such a schedule, we need to know t(h, N ) for every heuristic h and node N . Hence, when collecting
data for the instances in the training set X , we track for every B&B node N at which a heuristic h was
called, the number of iterations τ h
N = ∞ if h does not succeed
at N . Formally, we require a training dataset

N it took h to ﬁnd a feasible solution; we set τ h

D := {(h, N, τ h

N )) | h ∈ H, N ∈ NX , τ h

N ∈ R+ ∪ {∞}}.

Section 5 describes a computationally eﬃcient approach for building D using a single B&B run per training
instance.

4 Solving the Scheduling Problem

Problem (PS ) is a generalization of the Pipelined Set Cover Problem which is known to be N P-hard
[Munagala et al., 2005]. As for the MIQP in Appendix A, tackling it using a non-linear integer programming
solver is challenging: the MIQP has O(|H||NX |) variables and constraints, and a single training instance
may involve thousands of search tree nodes, leading to an MIQP with hundreds of thousands of variables
and constraints even with a handful of heuristics and tens of training instances.

As already mentioned in the beginning, one approach to ﬁnding a schedule that heuristically solves (PS )
is using a hyperparameter tuning software like SMAC [Hutter et al., 2011]. Since SMAC is a sequential
algorithm that searches for a good parameter conﬁguration by successively adapting and re-testing its best
settings, training a SMAC schedule can get very expensive quickly.
In the following, we present a more
eﬃcient approach.

We now direct our attention towards designing an eﬃcient heuristic algorithm for (PS ). A similar problem
was studied by [Streeter, 2007] in the context of decision problems. Among other things, the author discusses
how to ﬁnd a schedule of (randomized) heuristics that minimizes the expected time necessary to solve a set
of training instances X of a decision problem. Although this setting is somewhat similar to ours, there exist
multiple aspects in which they diﬀer signiﬁcantly:

1. Decision problems are considered instead of MIPs: Solving a MIP is generally much more challenging
than solving a decision problem. When solving a MIP with B&B, we normally have to solve many
linear subproblems. Since in theory, every such LP is an opportunity for a heuristic to ﬁnd a new
incumbent, we consider the set of nodes NX instead of X as the “instances” we want to solve.

2. A heuristic call can be suspended and resumed:

In the work of [Streeter, 2007], a heuristic can be
executed in a “suspend-and-resume model”: If h was executed before, the action (h, τ ) represents
continuing a heuristic run for an additional τ iterations. When h reaches the iteration limit, the run
is suspended and its state kept in memory such that it can be resumed later in the schedule. The
“suspend-and-resume” model is not used in MIP solving due to challenges in maintaining the states of
heuristics in memory. As such, we allow every heuristic to be included in the schedule at most once.

3. Time is used to control the duration of a heuristic run: Controlling time directly is unreliable in
practice and can lead to nondeterministic behavior of the solver. Instead, we rely on diﬀerent proxy
measures for diﬀerent classes of heuristics. Thus, when building a schedule that contains heuristics of
distinct types (e.g., diving and LNS heuristics), we need to ensure that these measures are comparable.

Despite these diﬀerences, it is useful to examine the greedy scheduling approach proposed by [Streeter, 2007].
A schedule is built by successively adding the action (h, τ ) to G that maximizes the ratio of the marginal

6

Learning to Schedule Heuristics in Branch-and-Bound

Figure 4 – Comparison of average cost of iterations for diﬀerent primal heuristics: While the cost of an iteration is
relatively similar among heuristics of the same type, they diﬀer signiﬁcantly when comparing diving and LNS with each other.
On average, an iteration for LNS heuristics (number of nodes in sub-MIP) is much more expensive than for diving heuristics
(maximal diving depth).

increase in the number of instances solved to the cost (i.e., τ ) of including (h, τ ). As shown in Corollary 2 of
[Streeter, 2007], the greedy schedule G yields a 4-approximation of that version of the scheduling problem.
In an attempt to leverage this elegant heuristic in our problem (PS ), we will describe it formally.

Let us denote the greedy schedule by G := (cid:104)g1, . . . , gk(cid:105). Then, G is deﬁned inductively by setting G0 = (cid:104)(cid:105)
and Gj = (cid:104)g1, . . . , gj(cid:105) with

gj = argmax

(h,τ )∈Hj−1×T

|{N ∈ N j−1
τ

X

| τ h

N ≤ τ }|

.

Here, Hi denotes the set of heuristics that are not yet in Gi, N i
X denotes the subset of nodes where Gi is not
yet successful in ﬁnding a solution, and T is the interval generated by all possible iteration limits in D, i.e.,

T := [min{τ h

N | (N, h, τ h

N ) ∈ D}, max{τ h

N | (N, h, τ h

N ) ∈ D}].

We stop adding actions gj when Gj ﬁnds a solution at all nodes in NX or all heuristics are contained in the
schedule, i.e., Hj = ∅.

Unfortunately, we can show that the resulting schedule can perform arbitrarily bad in our setting. Consider
the following situation. We assume that there are 100 nodes in NX and only one heuristic h. This heuristic
solves one node in just one iteration and takes 100 iterations each for the other 99 nodes. Following the
greedy approach, the resulting schedule would be G = (cid:104)(h, 1)(cid:105) since 1
100 . Whenever α > 0.01, G would
be infeasible for our constrained problem (PS ). Since we are not allowed to add a heuristic more than once,
this cannot be ﬁxed with the current algorithm.

1 > 99

To avoid this situation, we propose the following modiﬁcation. Instead of only considering the heuristics
that are not in Gj−1 when choosing the next action gj, we also consider the option to run the last heuristic
hj−1 of Gj−1 for longer. That is, we allow to choose (hj−1, τ ) with τ > τj−1. Note that the cost of adding
(hj−1, τ ) to the schedule is not τ , but τ − τj−1, since we decide to run hj−1 for τ − τj−1 iterations longer
and not to rerun hj−1 for τ iterations.

Furthermore, when including diﬀerent classes of heuristics in the schedule, the respective time measures are
not necessarily comparable (see Figure 4). To circumvent this problem, we use the average time per iteration
to normalize diﬀerent notions of iterations. In the following, we denote the average cost of an iteration by
th
avg for heuristic h. Note that th
avg can be easily computed by also tracking the duration (w.r.t. time) of a
heuristic run in data collection. Hence, we redeﬁne gj and obtain

gj = argmax
(h,τ )∈Aj−1

|{N ∈ N j−1

X

| τ h
cj−1(h, τ )

N ≤ τ }|

,

7

heuristics10-410-310-210-1time per iteration (in seconds)LNSdivingLearning to Schedule Heuristics in Branch-and-Bound

Algorithm 1 Greedy algorithm to obtain a schedule

Input: Nodes NX , heuristics H, data D, time frame T
Output: Greedy Schedule G
G ← (cid:104)(cid:105)
Nunsol ← NX
improve ← TRUE
repeat

(h∗, τ ∗) ← argmax
(h,τ )∈A

(cid:104) |{N ∈Nunsol|τ N

h ≤τ }|

(cid:105)

c(h,τ )

h∗ ≤τ ∗}|

if |{N ∈Nunsol|τ N
c(h,τ ∗)
G ← G ⊕ (cid:104)(h∗, τ ∗)(cid:105)
Nunsol ← Nunsol \ {N ∈ Nunsol | τ N

> 0 then

h∗ ≤ τ ∗}

else

improve ← FALSE

end if

until improve == FALSE

with Ai := (Hi × T ) ∪ {(hi, τ ) | τ > τi, τ ∈ T } and

ci(h, τ ) :=

(cid:40)

th
avgτ,
th
avg(τ − τi),

if h (cid:54)= hi
otherwise.

We set A0 := H×T and c0(h, τ ) = th
(which solves all 100 nodes) in the above example.

avgτ . With this modiﬁcation, we would obtain the schedule G = (cid:104)(h, 100)(cid:105)

Finally, note that this greedy procedure still does not explicitly enforce that the schedule is successful at a
fraction of at least α nodes. In our experiments, however, we observe that the resultings schedules reach a
success rate of α = 98% or above. The ﬁnal formulation of the algorithm can be found in Algorithm 1.

Example. Figure 5 shows an example of how we obtain a schedule with three heurisitcs and nodes. As the
left ﬁgure indicates, the data set is given by

D = {(h1, N1, 1), (h1, N2, ∞), (h1, N3, ∞), (h2, N1, 4), (h2, N2, 3),

(h2, N3, 3), (h3, N1, ∞), (h3, N2, 4), (h3, N3, 2)}.

Let us now assume that an iterations of each heuristic has the same average costs, i.e., th1
avg,
we build an schedule G as follows. First, we add the action (h1, 1), since h1 solves one node with only one
iteration yielding a ratio that cannot be bet by the other heuristics. No other node can be solved by h1,
hence it does not have to be considered anymore, as well as node N1. Among the remaining possibilities,
the action (h2, 3) is the best, since h2 solves both nodes in three iterations yielding a ratio of 2
3 . In contrast,
executing h3 for two and four iterations, respectively, would yield a ratio of 1
2 . Since this is smaller, we add
(h2, 3) to the G and obtain the schedule G = (cid:104)(h1, 1), (h2, 3)(cid:105) which solves all three nodes as shown on the
right of Figure 5. It is easy to see that this schedule is an optimal solution of (PS ) for α > 1
3 .

avg = th2

avg = th3

8

Learning to Schedule Heuristics in Branch-and-Bound

Figure 5 – Example of how to obtain a heuristic schedule from data: The data is shown on the left for three heuristics
and nodes and the (optimal) schedule obtained by following Algorithm 1 is illustrated on the right.

5 Data Collection

The scheduling approach described thus far rests on the availability of a training data set of the form

D := {(h, N, τ h

N ) | h ∈ H, N ∈ NX , τ h

N ∈ R+ ∪ {∞}}.

In words, each entry in data set D is a triplet containing: the index of a heuristic h; the index of a B&B
node N coming from one of the training instances in X ; the number of iterations required by h to ﬁnd a
feasible solution at the node N . The latter piece of information, τ h
N , must be collected by executing the
heuristic and observing its performance. Two main challenges arise in collecting such a data set for multiple
heuristics:

1. Eﬃcient data collection: Solving N P-hard MIPs by B&B remains computationally expensive, even
given the sophisticated techniques implemented in today’s solvers. This poses diﬃculties to ML ap-
proaches that create a single reward signal from one MIP evaluation, which may take several minutes
up to hours. This holds in particular for challenging problem classes that are the focus of this work.
In other words, even with a handful of heuristics, i.e., a small set H, it is prohibitive to run B&B once
for each heuristic-training instance pair in order to construct the data set D.

2. Obtaining unbiased data: On the other hand, executing multiple heuristics at each node of the search
tree during data collection can have dangerous side eﬀects: if a heuristic ﬁnds an incumbent, subsequent
heuristics are no longer executed at the same node, as described in Section 3.1.

We address the ﬁrst point by using a specially crafted version of the MIP solver for collecting multiple reward
signals for the execution of multiple heuristics per single MIP evaluation during the training phase. As a
result, we obtain a large amount of data points that scales with the running time of the MIP solves. This
has the clear advantage that the eﬃciency of our data collection does not automatically decrease when the
time to evaluate a single MIP increases for more challenging problems.

To address the second point and prevent bias from mutual interaction of diﬀerent heuristic calls during
training, we engineered the MIP solver to be executed in a special shadow mode, where heuristics are called
in a sandbox environment and interaction with the main solving path is maximally reduced. In particular
this means that new incumbents and primal bounds are not communicated back, but only recorded for
training data. This setting is an improved version of the shadow mode introduced in [Khalil et al., 2017].

As a result of these measures, we have instrumented the SCIP solver in a way that allows for the collection
of a proper data set D with a single run of the Branch-and-Bound algorithm per training instance.

9

N1N2N31432∞∞5h1h2h3nodesiterationsN131optimal scheduleN2,N3∞∞Learning to Schedule Heuristics in Branch-and-Bound

6 Computational Results

We will now detail our computational results. The code we used for data collection and scheduling is publicly
available 2.

6.1 Heuristics

We can build a schedule containing arbitrary heuristics as long as they have some type of time measure. In
this work, we focus on two broad groups of heuristics: Diving and Large Neighborhood Search (LNS). Both
classes are much more computationally expensive than simpler heuristics like rounding, but are generally also
more likely to ﬁnd (good) solutions [Berthold, 2006]. That is why it is particularly important to schedule
these heuristics most economically.

Diving Heuristics. Diving heuristics examine a single probing path by successively ﬁxing variables ac-
cording to a speciﬁc rule. There are multiple ways of controlling the duration of a dive. After careful
consideration of the diﬀerent options, we decided on using the maximum diving depth to limit the cost of a
call to a diving heuristic: It is both related to the eﬀort spent by the heuristic and its likelihood of success.

LNS Heuristics. This class of heuristics ﬁrst builds a neighborhood of some reference point which is then
searched for improving solutions by solving a sub-MIP. To control the duration of a call to a LNS heuristic,
we choose to limit the number of nodes in the sub-MIP. The idea behind this measure is similar to limiting
the diving depth of diving heuristics: In both cases, we control the number of subproblems that a heuristic
considers within its execution. Nevertheless, the two measures are not directly comparable, as shown in
Figure 4.

To summarize, we use 16 primal heuristics in our schedule: ten diving and six LNS heuristics. By controlling
this set, we cover the majority of the more complex heuristics implemented in SCIP. All other heuristics are
executed after the schedule according to their default settings.

6.2 Instances

Since our goal is to improve the primal performance of a solver, we focus on a primally challenging problem
class: The Generalized Independent Set Problem (GISP) [Hochbaum and Pathria, 1997, Colombi et al., 2017].
In the following, we will brieﬂy explain how we generate and partition the instances.

Let G = (V, E) be a graph and E(cid:48) ⊆ E a subset of removable edges. Each vertex has a revenue and every
edge has a cost associated with it. Then, GISP asks to select a subset of vertices and removable edges that
maximizes the proﬁt, i.e., the diﬀerence of vertex revenue and edge costs. Thereby, no edge should exist
between two selected vertices v, u ∈ V , i.e., either we have that (v, u) /∈ E or (v, u) ∈ E(cid:48) is removed.

We generate GISP instances in the following way. Given a graph, we randomize the set of removable edges
by setting the probability that an edge is in E(cid:48) to α = 0.75. Furthermore, we choose the revenue for each
node to be 100 and the cost of every edge as 1. This results in a conﬁguration for which it is diﬃcult to ﬁnd
good feasible solutions as shown in [Colombi et al., 2017].

We use this scheme to generate two types of instances. The ﬁrst one takes graphs from the 1993 DIMACS
Challenge which is also used by [Khalil et al., 2017, Colombi et al., 2017]. Thereby, we focus on the same
twelve dense graphs as well as the same train/test partition as in [Khalil et al., 2017]. The training set
consists of six graphs with 125–300 nodes and 6963–20864 edges, whereas the testing graphs are considerably
larger with 250–400 nodes and 21928–71819 edges. We generate 20 instances for every graph by using diﬀerent

2https://github.com/antoniach/heuristic-scheduling

10

Learning to Schedule Heuristics in Branch-and-Bound

test

train

[150,160]

[200,210]

[250,260]

[300,310]

[350,360]

[400,410]

[450,460]

[500,510]

[550,560]

[150,160]

[200,210]

[250,260]

[300,310]

[350,360]

[400,410]

[450,460]

[500,510]

[550,560]

0.89 ± 0.23

0.76 ± 0.22

0.87 ± 0.37

0.95 ± 0.40

0.87 ± 0.28

0.86 ± 0.23

0.78 ± 0.24

0.80 ± 0.25

0.65 ± 0.24

0.94 ± 0.28

0.75 ± 0.25

0.82 ± 0.30

0.91 ± 0.34

0.93 ± 0.23

0.90 ± 0.28

0.83 ± 0.22

0.79 ± 0.20

0.66 ± 0.20

0.89 ± 0.28

0.69 ± 0.23

0.81 ± 0.34

0.94 ± 0.40

0.92 ± 0.23

0.96 ± 0.39

0.81 ± 0.24

0.76 ± 0.22

0.66 ± 0.20

0.87 ± 0.25

0.71 ± 0.26

0.83 ± 0.36

0.97 ± 0.39

0.92 ± 0.28

0.90 ± 0.35

0.81 ± 0.24

0.75 ± 0.24

0.61 ± 0.24

0.84 ± 0.24

0.70 ± 0.23

0.82 ± 0.36

0.91 ± 0.37

0.81 ± 0.26

0.86 ± 0.31

0.80 ± 0.21

0.75 ± 0.19

0.59 ± 0.20

0.90 ± 0.27

0.70 ± 0.23

0.83 ± 0.36

0.88 ± 0.32

0.77 ± 0.23

0.88 ± 0.30

0.81 ± 0.21

0.74 ± 0.20

0.58 ± 0.20

0.89 ± 0.25

0.70 ± 0.23

0.83 ± 0.36

0.88 ± 0.32

0.77 ± 0.23

0.88 ± 0.30

0.81 ± 0.21

0.74 ± 0.20

0.58 ± 0.20

0.89 ± 0.26

0.72 ± 0.22

0.84 ± 0.30

0.99 ± 0.42

0.92 ± 0.24

0.95 ± 0.46

0.81 ± 0.23

0.80 ± 0.25

0.61 ± 0.20

0.88 ± 0.26

0.72 ± 0.24

0.89 ± 0.42

0.95 ± 0.37

0.86 ± 0.27

0.90 ± 0.28

0.81 ± 0.20

0.78 ± 0.23

0.63 ± 0.21

SCIP TUNED

0.89 ± 0.28

0.77 ± 0.23

0.99 ± 0.31

1.08 ± 0.45

1.05 ± 0.28

1.03 ± 0.38

0.94 ± 0.23

0.91 ± 0.28

0.76 ± 0.25

Table 1 – Average relative primal integral (mean ± std.) of schedule (with diving heuristics) w.r.t. default SCIP over GISP
instances derived from random graphs. The ﬁrst nine rows correspond to schedules that were trained on instances of size [l,u]

random seeds, leaving us with 120 instances for training as well as testing. For the second group of GISP
instances, we use randomly generated graphs where the number of nodes is uniformly chosen from {L, . . . , U }
for bounds L, U ∈ N. An edge is added to the resulting graph with probability ¯α = 0.1, giving us slightly less
dense graphs than the previous case. We denote these sets by [L,U]. For each set, we generate 25 instances
for training and 10 for testing. The smallest set of graphs then has 150–160 nodes and 1099–1268 edges
whereas the largest set consists of graphs with 550–560 nodes and 14932–15660 edges.

6.3 Results

To study the performance of our approach, we used the state-of-the-art solver SCIP 7.0 [Gamrath et al., 2020]
with CPLEX 12.10.0.0 3 as the underlying LP solver. Thereby, we needed to modify SCIP’s source code to
collect data as described in Section 5, as well as control heuristic parameters that are not already implemented
by default. For our experiments, we used a Linux cluster of Intel Xeon CPU E5-2660 v3 2.60GHz with 25MB
cache and 128GB main memory. The time limit in all experiments was set to two hours; for data collection,
we used a time limit of four hours. Since the primal integral depends on time, we ran one process at a time
on every machine, allowing for accurate on time measurements.

MIP solver performance can be highly sensitive to even small and seemingly performance-neutral pertur-
bations during the solving process [Lodi and Tramontani, 2013], a phenomenon referred to as performance
variability. We implemented a more exhaustive testing framework than the commonly used benchmark
methodology in MIP that uses extensive cross-validation in addition to multiple random seeds.

In addition to comparing our scheduling method against default SCIP, we also compare against scip tuned,
a hand-tuned version of SCIP’s default settings for GISP4. Since in practice, a MIP expert would try to
manually optimize some parameters when dealing with a homogeneous set of instances, we emulated that
process to create an even stronger baseline to compare our method against.

Random graph instances. Table 1 shows the results of the cross-validation experiments for schedules
with diving heuristics. Our scheduling framework yields a signiﬁcant improvement w.r.t. primal integral on
all test sets. Since this improvement is consistent over all schedules and test sets, we are able to validate
that the behavior actually comes from our procedure. Especially remarkable is the fact that the schedules
trained on smaller instances also perform well on much larger instances.

Note that the instances in the ﬁrst three test sets were solved to optimality by all settings whereas the
remaining ones terminated after two hours without a provably optimal solution. When looking at the
instances that were not solved to optimality, we can see that the schedules perform especially well on
instances of increasing diﬃculty. This behavior is intuitive: Since our method aims to improve the primal
performance of a solver, it performs better when an instance is very challenging on the primal side.

3https://www.ibm.com/products/ilog-cplex-optimization-studio
4We set the frequency oﬀset to 0 for all diving heuristics.

11

Learning to Schedule Heuristics in Branch-and-Bound

schedule

better primal integral

better primal bound

[150,160]

[200,210]

[250,260]

[300,310]

[350,360]

[400,410]

[450,460]

[500,510]

[550,560]

0.69

0.69

0.68

0.72

0.76

0.75

0.75

0.68

0.70

0.70

0.65

0.55

0.58

0.62

0.61

0.61

0.58

0.59

Table 2 – Fraction of instances for which our method’s schedule (with diving heuristics) has a better primal integral/bound
at termination w.r.t. scip tuned. Only instances that were not solved to optimality by both scip tuned and the schedule are
considered in the second column.

test

train

[150,160]

[200,210]

[250,260]

[300,310]

[350,360]

[150,160]

[200,210]

[250,260]

[300,310]

[350,360]

0.84 ± 0.19

0.65 ± 0.29

0.89 ± 0.35

0.91 ± 0.32

0.88 ± 0.28

0.87 ± 0.16

0.76 ± 0.33

0.91 ± 0.32

0.93 ± 0.34

0.89 ± 0.29

0.83 ± 0.18

0.71 ± 0.34

0.89 ± 0.31

0.90 ± 0.29

0.87 ± 0.32

0.81 ± 0.19

0.62 ± 0.24

0.91 ± 0.42

0.92 ± 0.32

0.94 ± 0.32

0.82 ± 0.19

0.61 ± 0.25

0.84 ± 0.42

0.86 ± 0.23

0.86 ± 0.28

SCIP TUNED

0.89 ± 0.28

0.77 ± 0.23

0.99 ± 0.31

1.08 ± 0.45

1.05 ± 0.28

Table 3 – Average relative primal integral (mean ± std.) of schedule (with diving and LNS heuristics) w.r.t. default SCIP
over GISP instances derived from random graphs. The ﬁrst ﬁve rows correspond to schedules that were trained on instances of
size [l,u]. On highlighted entries, a schedule controling both diving and LNS performs better than its diving counterpart (see
Table 1). More intense colors denote higher improvement.

Over all test sets, the schedules terminated with a strictly better primal integral on 69–76% and with a
strictly better primal bound on 59–70% of the instances compared to scip tuned (see Table 2).

Table 3 shows a part of the cross-validation experiments for schedules containing diving and LNS heuristics.
As expected, including both classes of heuristics improves the overall performance of the schedule.
In
this case, the improvement is only marginal since on the instances we consider, diving seems to perform
signiﬁcantly better than LNS.

Finding a schedule with SMAC. As mentioned before, we can also ﬁnd a schedule by using the hyper-
parameter tuning tool SMAC. To test SMAC’s performance on the random graph instances, we trained ten
SMAC schedules on a selection of the nine training sets. To make it easier for SMAC, we only considered
diving heuristics in this case. For the sake of comparability, we gave SMAC the same total computational
time for training as we did in data collection: With 25 training instances per set and a time limit of four
hours, this comes to 100 hours per training set and schedule. Note that since SMAC runs sequentially,
training the SMAC schedules took over four days per schedule, whereas training a schedule following the
greedy algorithm only took four hours with enough machines. To pick the best performing SMAC schedule
for each training set, we ran all ten schedules on the test set of same size as the corresponding training set
and chose the best performing one to also run on the other test sets.

The results can be found in Table 4. As we can see, on all test sets, all schedules are signiﬁcantly better than
default SCIP. However, when comparing these results to the performance of the greedy schedules (see Table
1), we can see that SMAC performs worse on average. Over all ﬁve test sets, the SMAC schedules terminated
with a strictly better primal integral on 36 – 54% and with a strictly better primal bound on 37 – 55% of
the instances. DIMACS graph instances. Table 5 summarizes the results on the instances derived from
DIMACS graphs. To stay consistent with [Khalil et al., 2017], we only schedule diving heuristics. As we can
see, the schedule setting dominates default SCIP in all metrics, but an especially drastic improvement can
be obtained w.r.t. the primal integral: the schedule reduces the primal integral by 49%.

When looking at the total time spent in heuristics, we see that heuristics run signiﬁcantly shorter but with
more success: On average, the incumbent success rate is higher compared to default SCIP.

12

Learning to Schedule Heuristics in Branch-and-Bound

test

train

[150,160]

[250,260]

[350,360]

[450,460]

[550,560]

[150,160]

[250,260]

[350,360]

[450,460]

[550,560]

0.81 ± 0.23

0.77 ± 0.34

0.90 ± 0.27

0.85 ± 0.24

0.65 ± 0.19

0.87 ± 0.26

0.88 ± 0.42

0.87 ± 0.25

0.83 ± 0.24

0.59 ± 0.22

0.86 ± 0.24

0.80 ± 0.37

0.86 ± 0.25

0.80 ± 0.24

0.68 ± 0.18

0.93 ± 0.26

0.87 ± 0.32

0.90 ± 0.19

0.85 ± 0.25

0.69 ± 0.23

0.87 ± 0.22

0.83 ± 0.31

0.92 ± 0.29

0.84 ± 0.26

0.58 ± 0.21

scip tuned

0.89 ± 0.28

0.77 ± 0.23

0.99 ± 0.31

1.08 ± 0.45

1.05 ± 0.28

compared to schedule

better primal integral

better primal bound

0.49

0.52

0.47

0.36

0.54

-

0.37

0.53

0.42

0.44

0.55

-

Table 4 – Average relative primal integral (mean ± std.) of SMAC schedule w.r.t. default SCIP and the fraction of instances
for which the SMAC schedule has a better primal integral/bound at termination w.r.t. the greedy schedule over GISP instances
derived from random graphs. The ﬁrst ﬁve rows correspond to schedules (with diving heuristics) that were trained with SMAC
on instances of size [l,u]. On highlighted entries, a SMAC schedule performs better than its greedy counterpart (see Table 1).
More intense colors denote higher improvement. Only instances that were not solved to optimality by both SMAC and the
greedy schedule are considered in the last column.

DEFAULT

SCIP TUNED

SCHEDULE

Primal Integral

Time to ﬁrst Incumbent

Time to best Incumbent

Best Incumbent

Total heuristic calls*

Total heuristic time*

Number of Incumbents found*

Incumbent Success Rate*

934.48

1.33

4266.68

2382.03

138.57

258.88

2.72

0.01

555.75

1.33

2642.46

2385.73

137.38

304.96

3.08

0.02

Gap

144.59

144.03

470.73

1.26

2803.38

2404.63

140.03

190.10

3.33

0.02

141.70

Primal-dual Integral

450 148.72

435 321.67

430 882.04

Table 5 – Summary of results on GISP instances derived from DIMACS graphs. Values shown are aggregates over instances;
geometric means are used. Statictics with * refer only to the heuristics used in the schedule.

7 Conclusion and Discussion

In this work, we propose a data-driven framework for scheduling primal heuristics in a MIP solver such that
the primal performance is optimized. Central to our approach is a novel formulation of the learning task
as a scheduling problem, an eﬃcient data collection procedure, and a fast, eﬀective heuristic for solving the
learning problem on a training dataset. A comprehensive experimental evaluation shows that our approach
consistently learns heuristic schedules with better primal performance than SCIP’s default settings. Further-
emore, by replacing our heuristic algorithm with the hyperparameter tuning tool SMAC in our scheduling
framework, we are able to obtain a worse but still signiﬁcant performance improvement w.r.t. SCIP’s default.
Together with the prohibitive computational costs of SMAC, we conclude that for our heuristic scheduling
problem, the proposed heuristic algorithm constitutes an eﬃcient alternative to existing methods.

A possible limitation of our approach is that it produces a single, “one-size-ﬁts-all” schedule for a class of
training instances. It is thus natural to wonder whether alternative formulations of the learning problem
that leverage additional contextual data about an input MIP instance and/or a heuristic can be useful. We
note that learning a mapping from the space of MIP instances to the space of possible schedules is not trivial.
The space of possible schedules is a highly structured output space that involves both the permutation over
heuristics and their respective iteration limits. The approach proposed here is much simpler in nature, which
makes it easy to implement and incorporate into a sophisticated MIP solver.

Although we have framed the heuristic scheduling problem in machine learning terms, we are yet to analyze
the learning-theoretic aspects of the problem. More speciﬁcally, our approach is justiﬁed on empirical grounds
in Section 6, but we are yet to attempt to analyze potential generalization guarantees. We view the recent
foundational results by [Balcan et al., 2019] as a promising framework that may apply to our setting, as it
has been used for the branching problem in MIP [Balcan et al., 2018].

13

Learning to Schedule Heuristics in Branch-and-Bound

References

[Balcan et al., 2019] Balcan, M.-F., DeBlasio, D., Dick, T., Kingsford, C., Sandholm, T., and Vitercik, E.
(2019). How much data is suﬃcient to learn high-performing algorithms? arXiv preprint arXiv:1908.02894.

[Balcan et al., 2018] Balcan, M.-F., Dick, T., Sandholm, T., and Vitercik, E. (2018). Learning to branch.

In International conference on machine learning, pages 344–353. PMLR.

[Baltean-Lugojan et al., 2019] Baltean-Lugojan, R., Bonami, P., Misener, R., and Tramontani, A. (2019).

Scoring positive semideﬁnite cutting planes for quadratic optimization via trained neural networks.

[Berthold, 2006] Berthold, T. (2006). Primal heuristics for mixed integer programs. Master’s thesis.

[Berthold, 2013a] Berthold, T. (2013a). Measuring the impact of primal heuristics. Operations Research

Letters, 41(6):611 – 614.

[Berthold, 2013b] Berthold, T. (2013b). Primal minlp heuristics in a nutshell. In OR.

[Berthold, 2018] Berthold, T. (2018). A computational study of primal heuristics inside an mi(nl)p solver.

Journal of Global Optimization, 70:189–206.

[Colombi et al., 2017] Colombi, M., Mansini, R., and Savelsbergh, M. (2017). The generalized independent
set problem: Polyhedral analysis and solution approaches. European Journal of Operational Research,
260:41–55.

[Gamrath et al., 2020] Gamrath, G., Anderson, D., Bestuzheva, K., Chen, W.-K., Eiﬂer, L., Gasse, M.,
Gemander, P., Gleixner, A., Gottwald, L., Halbig, K., Hendel, G., Hojny, C., Koch, T., Bodic, P. L.,
Maher, S. J., Matter, F., Miltenberger, M., M¨uhmer, E., M¨uller, B., Pfetsch, M., Schl¨osser, F., Serrano,
F., Shinano, Y., Tawﬁk, C., Vigerske, S., Wegscheider, F., Weninger, D., and Witzig, J. (2020). The SCIP
Optimization Suite 7.0. ZIB-Report 20-10, Zuse Institute Berlin.

[He et al., 2014] He, H., III, H. D., and Eisner, J. M. (2014). Learning to search in branch and bound
algorithms. In Advances in Neural Information Processing Systems, volume 27, pages 3293–3301. Curran
Associates, Inc.

[Hendel, 2018] Hendel, G. (2018). Adaptive large neighborhood search for mixed integer programming.

Mathematical Programming Computation. under review.

[Hendel et al., 2018] Hendel, G., Miltenberger, M., and Witzig, J. (2018). Adaptive algorithmic behavior
for solving mixed integer programs using bandit algorithms. In OR 2018: International Conference on
Operations Research.

[Hewitt et al., 2010] Hewitt, M., Nemhauser, G., and Savelsbergh, M. (2010). Combining exact and heuristic
approaches for the capacitated ﬁxed-charge network ﬂow problem. INFORMS Journal on Computing,
22:314–325.

[Hochbaum and Pathria, 1997] Hochbaum, D. S. and Pathria, A. (1997). Forest harvesting and minimum

cuts: A new approach to handling spatial constraints. Forest Science, 43:544–554.

[Hutter et al., 2009] Hutter, F., Hoos, H., Leyton-Brown, K., and St¨utzle, T. (2009). Paramils: An automatic

algorithm conﬁguration framework. J. Artif. Intell. Res. (JAIR), 36:267–306.

[Hutter et al., 2011] Hutter, F., Hoos, H. H., and Leyton-Brown, K. (2011). Sequential model-based op-
In Coello, C. A. C., editor, Learning and Intelligent

timization for general algorithm conﬁguration.
Optimization, pages 507–523, Berlin, Heidelberg. Springer Berlin Heidelberg.

[Khalil et al., 2016] Khalil, E. B., Bodic, P. L., Song, L., Nemhauser, G., and Dilkina, B. (2016). Learning
In Proceedings of the 30th AAAI Conference on Artiﬁcial

to branch in mixed integer programming.
Intelligence.

14

Learning to Schedule Heuristics in Branch-and-Bound

[Khalil et al., 2017] Khalil, E. B., Dilkina, B., Nemhauser, G., Ahmed, S., and Shao, Y. (2017). Learning
to run heuristics in tree search. In 26th International Joint Conference on Artiﬁcial Intelligence (IJCAI),
pages 659–666.

[Kruber et al., 2017] Kruber, M., L¨ubbecke, M., and Parmentier, A. (2017). Learning when to use a decom-

position. In Lecture Notes in Computer Science, pages 202–210.

[Lodi, 2013] Lodi, A. (2013). The heuristic (dark) side of mip solvers. Hybrid Metaheuristics, 434:273–284.

[Lodi and Tramontani, 2013] Lodi, A. and Tramontani, A. (2013). Performance variability in mixed-integer

programming. Tutorials in Operations Research, Vol. 10, pages 1–12.

[Munagala et al., 2005] Munagala, K., Babu, S., Motwani, R., and Widom, J. (2005). The pipelined set

cover problem. In International Conference on Database Theory, volume 3363, pages 83–98.

[Nair et al., 2020] Nair, V., Bartunov, S., Gimeno, F., von Glehn, I., Lichocki, P., Lobov, I., O’Donoghue,
B., Sonnerat, N., Tjandraatmadja, C., Wang, P., Addanki, R., Hapuarachchi, T., Keck, T., Keeling, J.,
Kohli, P., Ktena, I., Li, Y., Vinyals, O., and Zwols, Y. (2020). Solving mixed integer programs using
neural networks.

[Streeter, 2007] Streeter, M. (2007). Using Online Algorithms to Solve NP-Hard Problems More Eﬃciently

in Practice. PhD thesis, Carnegie Mellon University.

15

Learning to Schedule Heuristics in Branch-and-Bound

A Formulating the Scheduling Problem as a MIQP

In this section, we present the exact formulation of Problem (PS ) as a MIQP. First, we describe the param-
eters and variables we need to formulate the problem. Then, we state the problem and explain shortly what
every constraint represents.

A.1 Parameters

– D, set of data points coming from a set of MIP instances X . Each data point is of the form (h, N, τ h

where h ∈ H is a heuristic, N ∈ NX indexes a node of the B&B tree of X , and τ h
iterations h needed to ﬁnd a solution to N (if h could not solve N , we set τ h
N = ∞).

N ),
N is the number of

– T h, ∀h ∈ H, the maximum number of iterations h needed to ﬁnd a solution, i.e., T h := max{τ h

N < ∞ |

N ∈ NX }.

– α ∈ [0, 1], minimal fraction of nodes the resulting schedule should solve.

A.2 Variables Describing the Schedule

– xh

p ∈ {0, 1}, ∀h ∈ H, ∀p ∈ {0, . . . , |H|}: The variable is set to 1 if h is executed at position p in the
schedule (if xh

0 = 1 then h is not in the schedule).

– th ∈ [0, . . . , T h], ∀h ∈ H: This integer variable is equal to the maximal number of iterations h can use

in the schedule (we set th = 0 if h is not in the schedule).

A.3 Auxiliary Variables

– ph ∈ {0, . . . , |H|}, ∀h ∈ H: This integer variable is equal to the position of h in the schedule.

– sh

N ∈ {0, 1}, ∀h ∈ H, ∀N ∈ NX : The variable is set to 1 if heuristic h solves node N in the schedule.

– sN ∈ {0, 1}, ∀N ∈ NX : This variable is set to 1 if the schedule solves node N .

– pmin

N ∈ {1, . . . , |H|}, ∀N ∈ NX : This integer variable is equal to the position of the heuristic that ﬁrst
solves node N in the schedule (if the schedule does not solve N , we set it to |H|).

– zh

N ∈ {0, 1}, ∀h ∈ H, ∀N ∈ NX : This variable is set to 1 if h is executed before position pmin
N .

– f h

N ∈ {0, 1}, ∀h ∈ H, ∀N ∈ NX : The variable is set to 1 if h is the heuristic that solves N ﬁrst, i.e., if
ph = pmin
N .
– tN ∈ {1, . . . , 1 + (cid:80)

h T h}, ∀N ∈ NX : This integer variable is equal to the total number of iterations
the schedule needs to solve node N (if N is not solved by the schedule, we set it to 1 plus the total
length of the schedule, i.e., 1 + (cid:80)
p

p th).

h xh

(cid:80)

A.4 Formulation

In the following, we give an explicit formulation of (PS ) as a MIQP. Note that some constraints use nonlinear
functions like the maximum/minimum of a ﬁnite set or the indicator function 1. These can be easily linearized
by introducing additional variables and constraints, thus the following formulation is indeed a MIQP. For
the sake of readability, we omit stating all the linearizations explicitly.

16

Learning to Schedule Heuristics in Branch-and-Bound

min

s.t.

(cid:88)

N
(cid:88)

h
(cid:88)

tN

xh
p ≤ 1, ∀p ∈ {1, . . . , |H|}

xh
p = 1, ∀h ∈ H

p
ph =

(cid:88)

p

pxh

p , ∀h ∈ H

0 ) ≥ th, ∀h ∈ H

T h(1 − xh
N = max{0, min{1, th − τ h
sh
(cid:88)
sN = min{1,

sh
N }, ∀N ∈ NX

N + 1}}, ∀h ∈ H, ∀N ∈ NX

h

N

(cid:88)

sN ≥ α

1
|NX |
pmin
N = min{phsh
N = 1
zh
N = 1
f h

{ph<pmin

tN = sN (

h

N + (1 − sh

N ) | H|) | h ∈ H}, ∀N ∈ NX

N }, ∀h ∈ H, ∀N ∈ NX
N }, ∀h ∈ H, ∀N ∈ NX
{ph=pmin
(cid:88)
N τ h
N th + f h
zh

N ) + (1 − sn)(1 +

p th), ∀N ∈ NX
xh

(cid:88)

h,p

(1)

(2)

(3)

(4)

(5)

(6)

(7)

(8)

(9)

(10)

(11)

(12)

(1) calculates the total number of iterations the schedule needs to solve all nodes.

(2) and (3) guarantee that only one copy of each heuristic is run, and that every non-zero position is occupied
by at most one heuristic.

(4) calculates the position of a heuristic in the schedule.

(5) ensures that th = 0 if h is not in the schedule.

(6) forces sh

N to 1 if h solves node N in the schedule.

(7) forces sN to 1 if the schedule solves node N .

(8) guarantees that the schedules solves enough nodes.

(9) calculates the position of the ﬁrst heuristic that solves N in the schedule.

(10) forces zh

N to 1 if h is executed before position pmin
N .

(11) forces f h

N to 1 if h is executed at position pmin
N .

(12) calculates the number of iterations necessary for the schedule to solve N .

B Implementation Details

Not every idea that works in theory can be directly translated to also work in practice. Hence, it is sometimes
inevitable to adapt ideas and make compromises when implementing a new method. In this section, we touch

17

Learning to Schedule Heuristics in Branch-and-Bound

upon aspects we needed to consider to ensure a reliable implementation of the framework proposed in this
paper.

Time as a measure of duration. A heuristic schedule controls two general aspects: The order and the
duration for which the diﬀerent heuristics are executed. Even though it might seem intuitive to use time
to control the duration of a heuristic run, we use a suitable proxy measure for every class of heuristics
instead, as discussed in Section 3. There are two main problems that hinder us from directly controlling
time. First, time is generally not stable enough to use for decision-making within an optimization solver.
To make it somewhat reliable, we would need to solve instances exclusively on a single machine at a time.
Hence, it would not be possible to run instances in parallel which would cause the solving process to be
very expensive in practice. The second, even more important problem is the following. Since the behavior
of heuristics signiﬁcantly depends on diﬀerent parameters, allowing the heuristic to run for a longer time
does not necessarily translate to a increase in success probability if crucial parameters are set to be very
restrictive by default. That is why we use a suitable proxy measure for time instead of time itself.

Limitations of the shadow mode. To make sure we obtain data that is as independent as possible,
the heuristics run in shadow mode during data collection. This setting aims to ensure that the heuristics
only run in the background and do not report anything back to the solver. However, it is not possible to
hide all of the heuristic’s actions from SCIP. Since SCIP is not designed to have heuristics running in the
background, it is almost impossible to locate and adjust the lines of code that inﬂuence the solving process
globally without limiting the heuristic’s behavior too much. For instance, one way of hiding all actions of
diving heuristics would be turning oﬀ propagation while diving. Since this would inﬂuence the performance
of the heuristics considerably, the resulting data would not represent how the heuristics behave in practice.
That is why we settled with a shadow mode that hides most (and the most inﬂuential) of the heuristic’s
activities from SCIP.

18

