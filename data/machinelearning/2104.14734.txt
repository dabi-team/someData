1
2
0
2

r
p
A
0
3

]

G
L
.
s
c
[

1
v
4
3
7
4
1
.
4
0
1
2
:
v
i
X
r
a

Flattening Multiparameter Hierarchical
Clustering Functors

Dan Shiebler

University of Oxford

Abstract. We bring together topological data analysis, applied cate-
gory theory, and machine learning to study multiparameter hierarchical
clustering. We begin by introducing a procedure for ﬂattening multipa-
rameter hierarchical clusterings. We demonstrate that this procedure is
a functor from a category of multiparameter hierarchical partitions to a
category of binary integer programs. We also include empirical results
demonstrating its eﬀectiveness. Next, we introduce a Bayesian update
algorithm for learning clustering parameters from data. We demonstrate
that the composition of this algorithm with our ﬂattening procedure
satisﬁes a consistency property.

1

Introduction

One of the most useful ways to process a dataset represented as a ﬁnite met-
ric space is to cluster the dataset, or break it into groups. An important step
is choosing the desired granularity of the clustering, and multiparameter hi-
erarchical clustering algorithms accept a hyperparameter vector to control
this.

Since applying a multiparameter hierarchical clustering algorithm with dif-
ferent hyperparameter values may produce diﬀerent partitions, we can view such
algorithms as mapping a ﬁnite metric space (X, dX ) to a function from the hy-
perparameter space to the space of partitions of X. A ﬂattening procedure then
maps this function to a single partition.

Many popular clustering algorithms, such as HDBSCAN [7] and ToMATo
[3], include a ﬂattening step that operates on the same intuition that drives the
analysis of persistence diagrams in TDA. That is, the most important clusters
(homological structures) of a dataset are those which exist at multiple scales
(have large diﬀerences between their birth and death times).

In this paper we will characterize and study clustering algorithms as functors,
similarly to [1,4,12]. We will particularly focus on multiparameter hierarchical
clustering algorithms with partially ordered hyperparameter spaces. This per-
spective allows us to guarantee that the clustering algorithms we study preserve
both non-expansive maps between metric spaces and the ordering of the hyper-
parameter space. Our contributions are:

– We describe an algorithm for ﬂattening multiparameter hierarchical cluster-
ings, which we demonstrate is a functorial map from a category of multipa-
rameter hierarchical partitions to a category of binary integer programs.

 
 
 
 
 
 
D. Shiebler

– We introduce a Bayesian update algorithm for learning a distribution over

clustering hyperparameters from data.

– We prove that the composition of the Bayesian update algorithm and the

ﬂattening procedure is consistent.

2 Multiparameter Hierarchical Clustering
In this work we will deﬁne ﬂat clustering algorithms to map a ﬁnite metric space
(X, dX ) to a partition of X. We will primarily work with the following categories:

Deﬁnition 1. In the category Met objects are ﬁnite metric spaces (X, dX ) and
morphisms are non-expansive maps, or functions f : X → Y such that
dY (f (x1), f (x2)) ≤ dX (x1, x2).

Deﬁnition 2. In the category Part objects are tuples (X, PX ) where PX is a
partition of the set X. Morphisms in Part are functions f : X → Y such that
if S ∈ PX then ∃S(cid:48) ∈ Y, f (S) ⊆ S(cid:48).
We will also work in the subcategories Metbij, Partbij of Met, Part respectively
in which morphisms are further restricted to be bijective.

Deﬁnition 3. Given a subcategory D of Met, a ﬂat clustering functor on
D is a functor F : D → Part that is the identity on the underlying set X. In
the case that D is unspeciﬁed we simply call F a ﬂat clustering functor.

Now recall that the set of connected components of the δ-Vietoris-Rips complex
of (X, dX ) is the partioning of X into subsets with maximum pairwise distance
no greater than δ. Given a ∈ (0, 1], an example of a ﬂat clustering functor
on Met is the a-single linkage functor SL(a), which maps a metric space to
the connected components of its −log(a)-Vietoris-Rips complex [12,4,1]. Given
a1, a2 ∈ (0, 1], an example of a ﬂat clustering functor on Metbij is the robust
single linkage functor SLR(a1, a2) which maps a metric space (X, dX ) to the
connected components of the −log(a2)-Vietoris-Rips complex of (X, da1
X ) where:

da1
X (x1, x2) = max(dX (x1, x2), µXa1

(x1), µXa1

(x2))

and µXa1
(x1) is the distance from x1 to its (cid:98)a1 ∗ |X|(cid:99)th nearest neighbor [2].
Intuitively, robust single linkage reduces the impact of dataset noise by increas-
ing distances in sparse regions of the space. Note that robust single linkage is
not a ﬂat clustering functor on Met because it includes a k-nearest neighbor
computation that is sensitive to |X|.

Like single linkage and robust single linkage, many ﬂat clustering algorithms
are conﬁgured by a hyperparameter vector that governs their behavior. In the
case that this hyperparameter vector is an element of a partial order O we can
represent the output of such an algorithm with a functor O → Part.

Recall that PartO is the category of functors from O to Part and natural
transformations between them. We will write PartO to represent the subcategory
of such functors that commute with the forgetful functor U : Part → Set.
Given F : O → Part in PartO we will call the image of U ◦ F the underlying

Flattening Multiparameter Hierarchical Clustering Functors

set of F . Note that there also exists a forgetful functor PartO → Set that
maps F : O → Part to its underlying set and that any natural transformation
in PartO between the functors FX : O → Part and FY : O → Part with
underlying sets X and Y is fully speciﬁed by a function f : X → Y .

Deﬁnition 4. Given a partial order O and a subcategory D of Met, an O-
clustering functor on D is a functor H : D → PartO that commutes with the
forgetful functors from D and PartO into Set. In the case that D is unspeciﬁed
we simply call H an O-clustering functor.

For example, single linkage SL : Met → Part(0,1]op is a (0, 1]op-clustering
is

functor on Met and robust single linkage SLR : Metbij → Part(0,1]op×(0,1]op
a (0, 1]op × (0, 1]op-clustering functor on Metbij.

bij

Deﬁnition 5. Given the functor FX ∈ PartO with underlying set X, its par-
tition collection is the set SX of all subsets S ⊆ X such that there exists some
a ∈ O where S ∈ FX (a).

We will write the elements of SX (subsets of X) with the notation SX =
{s1X , s2X , · · · , snX }.

In practice it is often convenient to “ﬂatten” a functor FX ∈ PartO to a
single partition of X by selecting a non-overlapping collection of sets from its
partition collection SX . Since we will express this selection problem as a binary
integer program we will work in the following category:

Deﬁnition 6. The objects in BIP are tuples (n, m, c, A, B, u) where n, m ∈ N,
c, u are m-element real-valued vectors, A is an n×m real-valued matrix and B is
an n × m {0, 1}-valued matrix. Intuitively, the tuple (n, m, c, A, B, u) represents
the following binary integer program: ﬁnd an m-element {0, 1}-valued vector v
that maximizes cT v subject to Av + Bv ≤ u.

The morphisms between (n, m, c, A, B, u) and (n(cid:48), m(cid:48), c(cid:48), A(cid:48), B(cid:48), u(cid:48)) are tuples
(Pc, Pu, PA, PA∗ , PB, PB∗ ) where Pu, PA are n(cid:48) × n real-valued matrices, PA∗ , Pc
are m × m(cid:48) real-valued matrices, PB is an n(cid:48) × n {0, 1}-valued matrix and PB∗
is an m × m(cid:48) {0, 1}-valued matrix such that:

Pcc(cid:48) = c

Puu = u(cid:48)

PAAPA∗ = A(cid:48)

PBBPB∗ = B(cid:48)

where the operation PBBPB∗ is performed with logical matrix multiplication.
Intuitively, a morphism (Pc, Pu, PA, PA∗ , PB, PB∗ ) maps the binary integer pro-
gram “ﬁnd an m-element {0, 1}-valued vector v that maximizes (Pcc(cid:48))T v subject
to Av +Bv ≤ u” to the binary integer program “ﬁnd an m(cid:48)-element {0, 1}-valued
vector v that maximizes c(cid:48)T v subject to PAAPA∗ v + PBBPB∗ v ≤ Puu”.

When we construct a binary integer program from an object FX in PartO
with underlying set X, we weight the elements of its partition collection SX
according to a model of the importance of diﬀerent regions of O. In this work
we will only consider O that are Borel measurable, so we can represent this

D. Shiebler

model with a probability measure µ over O. This probabilistic interpretation
will be useful in Section 2.2 when we update this model with labeled data. We
can then view the ﬂattening algorithm as choosing a non-overlapping subset
PX ⊆ SX (the elements of PX are subsets of X where no element of X belongs
to more than a single set in PX ) that maximizes the expectation of the function
that maps a to the number of siX ∈ PX that are also in FX (a). Formally, the
algorithm maximizes (cid:82)
a∈O |{siX | siX ∈ PX , siX ∈ FX (a)}| dµ . If µ is uniform
this is similar to the Topologically Motivated HDBSCAN described in [7].

Proposition 1. Given a probability measure µ over O, there exists a functor
F lattenµ : PartO
bij → BIP that maps FX : O → Partbij with partition col-
lection SX to a tuple (|SX |, |SX |, c, A, B, u) such that any solution to the prob-
lem “ﬁnd an m-element {0, 1}-valued vector v that maximizes cT v subject to
Av + Bv ≤ u” speciﬁes a non-overlapping subset PX ⊆ SX that maximizes
(cid:82)
a∈O |{siX | siX ∈ PX , siX ∈ FX (a)}| dµ.

Proof. Given a probability measure µ over O, F lattenµ : PartO
bij → BIP maps
the functor FX : O → Partbij with partition collection SX to the binary integer
program (|SX |, |SX |, c, A, B, u) where c, u are |SX |-element real-valued vectors
and A, B are respectively real-valued and {0, 1}-valued |SX | × |SX | matrices
where:

ui = |SX |

ci =

(cid:90)

dµ

Ai,j =

(cid:40)

|SX | − 1
0

i = j
else

{a | siX ∈FX (a)}
(cid:40)

Bi,j =

1
0

siX ∩ sjX (cid:54)= ∅
else

A natural transformation between FX , FY : O → Partbij with underlying sets
X, Y and partition collections SX , SY that is speciﬁed by the surjective function
f : X → Y is sent to the tuple (Pc, Pu, PA, P T
B ) where Pc is an |SX |×|SY |
real-valued matrix, PA, Pu are |SY | × |SX | real-valued matrices, and PB is an
|SY | × |SX | {0, 1}-valued matrix such that:

A , PB, P T

Pci,j =

(cid:82)






0

{a | siX

∈FX (a), sjY

(cid:82)
{a | sjY

∈FY (a)} dµ

∈FY (a)} dµ

f (siX ) ⊆ sjY

else

Puj,i =

(cid:40) |SY |
|SX |
0

i = j

else

PAj,i =

(cid:40)(cid:113) |SY |−1
|SX |−1

0

i = j

else

PBj,i =

(cid:40)
1
0

f (siX ) ⊆ sjY
else

First we will show that any feasible solution to the integer program F lattenµFX
corresponds to a selection of elements from SX with no overlaps. If siX ∩sjX (cid:54)= ∅,
then the ith row of A + B will have |SX | in position i and 1 in position j. This
implies that if vi = 1 then (A + B)T
i v ≤ |SX | if and only if vj = 0. Note also that
by the deﬁnition of binary integer programming this is the selection of elements

Flattening Multiparameter Hierarchical Clustering Functors

that maximizes:

(cid:88)

ci =

(cid:90)

(cid:88)

(cid:90)

dµ =

siX ∈PX

siX ∈PX

{a | siX ∈FX (a)}

a∈O

|{siX | siX ∈ PX , siX ∈ FX (a)}| dµ

Next, we will show that F lattenµ is a functor. Consider FX , FY : O → Partbij
with underlying sets X, Y and partition collections SX , SY and suppose:

F lattenµFX = (|SX |, |SX |, cX , AX , BX , uX )

F lattenµFY = (|SY |, |SY |, cY , AY , BY , uY )

Consider also a natural transformation speciﬁed by the function f : X → Y and
deﬁne the image of F lattenµ on this natural transformation to be (Pc, Pu, PA, P T
We ﬁrst need to show that:

A , PB, P T

B ).

PccY = cX

PuuX = uY

PAAX P T

A = AY

PBBX P T

B = BY

Where PBBX P T
to see that PccY = cX , note that:

B = BY is performed with logical matrix multiplication. In order

(PccY )i =

(cid:88)

(cid:32)(cid:90)

{j | f (siX )⊆sjY }
(cid:88)

(cid:90)

{a | sjY ∈FY (a)}

(cid:33) (cid:32) (cid:82)

dµ

{a | siX ∈FX (a), sjY ∈FY (a)} dµ
{a | sjY ∈FY (a)} dµ

(cid:82)

(cid:33)

=

(cid:90)

dµ =

dµ = cXi

{j | f (siX )⊆sjY }

{a | siX ∈FX (a), sjY ∈FY (a)}

{a | siX ∈FX (a)}

Next, to see that PuuX = uY , note that (PuuX )j = |SY |
|SX | |SX | = |SY | = uYj .
Next, to see that PAAX P T
A = AY , recall that PA is an |SY | × |SX | matrix and
note that since f is surjective it must be that SY ≤ SX . Therefore both the
left |SY | × |SY | submatrix of PA and the top |SY | × |SY | submatrix of P T
A are
diagonal, so the product PAAX P T

A is a diagonal |SY | × |SY | matrix with:

(PAAX P T

A )jj = PAjj AXjj P T
Ajj

=

|SY | − 1
|SX | − 1

(|SX | − 1) = |SY | − 1 = AYjj

Next, to see that PBBX P T
|SY | × |SX | matrix where:

B = BY , note ﬁrst that PBBX is a {0, 1}-valued

(PBBX )ji = ∃k=1...|SX |PBj,k ∧ BXk,i = ∃ skX ∈ SX , f (skX ) ⊆ sjY ∧ skX ∩ siX (cid:54)= ∅

And therefore that:

(PBBX P T
∃ slX ∈ SX (∃ skX ∈ SX , f (skX ) ⊆ sjY ∧ skX ∩ slX (cid:54)= ∅) ∧ (cid:0)f (slX ) ⊆ sj(cid:48)

∃ slX , skX ∈ SX , f (skX ) ⊆ sjY ∧ f (slX ) ⊆ sj(cid:48)

B )jj(cid:48) =
(cid:1) =
Y ∧ skX ∩ slX (cid:54)= ∅ =
Y (cid:54)= ∅
sjY ∩ sj(cid:48)

Y

Finally, note that F lattenµ preserves the identity since PB = PA = I when
SX = SY and it preserves composition by the laws of matrix multiplication. (cid:117)(cid:116)

D. Shiebler

For example, if µ is uniform then the connected components of the Vietoris-
Rips ﬁltration of (X, dX ) that have the largest diﬀerences between their birth
and death times will be a solution to (F lattenµ ◦ SL)(X, dX ). Note also that
bij, and not all of PartO. Intuitively, this is
F lattenµ is only functorial over PartO
because F lattenµ maps natural transformations between elements of PartO
bij to
linear maps that only exist when the functions underlying these natural trans-
formations are bijective.
2.1 The Multiparameter Flattening Algorithm

Initialize an empty list L
Repeat n times:

Sample the hyperparameter vector a according to µ
Add each set in H(X, dX )(a) to L

Given an O-clustering functor H and a distribution µ over O we can use Monte
Carlo integration and F lattenµ to implement the following algorithm:
1: procedure MultiparameterFlattening(H, µ, (X, dX ), n)
2:
3:
4:
5:
Deﬁne SX to be the list of unique elements of L
6:
Deﬁne c such that ci is the number of times that siX appears in L
7:
Set A, B, u with F lattenµ
8:
Return the solution to the binary integer program (|SX |, |SX |, c, A, B, u)
9:
We include an example of this procedure on Github 1 that builds on McInnes et.
al.’s [7]’s HDBSCAN implementation. In Table 1 we demonstrate that applying
this procedure and solving the resulting binary integer program can perform
better than choosing an optimal parameter value.

Algorithm

HDBSCAN With Optimal
Distance Scaling Parameter α
HDBSCAN With Flattening Over
Distance Scaling Parameter α

Adjusted Rand Score on
Fashion MNIST Dataset
0.217

Adjusted Rand Score on
20 Newsgroups Dataset
0.181

0.262

0.231

Table 1: We compare the performance of applying F lattenµ to HDBSCAN with
simply running HDBSCAN with the value of the distance scaling parameter
α that achieves the best Adjusted Rand Score. We evaluate over the Fashion
MNIST [13] and 20 Newsgroups [6] datasets by using the scikit-learn imple-
mentation of the Adjusted Rand Score [9]. We see that the F lattenµ procedure
performs consistently better, which suggests that it may be a good option for
unsupervised learning applications such as data visualization or pre-processing.

2.2 Multiparameter Flattening with Supervision

One of the most important components in the Multiparameter Flattening Al-
gorithm is the choice of distribution µ over O. For example, if O = (0, 1]op
and µ is the Dirac distribution at a then SL(X, dX )(a) will be a solution to
(F lattenµ ◦ SL)(X, dX ).

1 https://github.com/dshieble/FunctorialHyperparameters

Flattening Multiparameter Hierarchical Clustering Functors

Fig. 1: If we deﬁne H = SL and apply Equation 2 to learn µn, then samples
from µn are distance thresholds for single linkage. We see that the samples tend
to be drawn from the region where the Adjusted Rand Score is highest (code on
Github).

We can leverage the importance of µ to enable our ﬂattening procedure to
learn from labeled data. Formally given an O-clustering functor H, a metric space
(X, dX ), and an observed partition PX of X (the labels) we can use Bayes rule to
a∈σ γX (PX | a) dµ
deﬁne the probability measure µPX over O where µPX (σ) =
a∈O γX (PX | a) dµ if
σ is an element of the Borel algebra of O and for each a ∈ O the map γX ( | a)
is a probability mass function over the ﬁnite set of all partitions of X. There are
several possible choices of γX . Intuitively, we want γX (PX | a) to be large when
H(X, dX )(a) and PX are similar. The simplest choice would be γX (PX | a) =
(cid:40)

(cid:82)
(cid:82)

, but a more robust strategy would be to use the Rand

1 H(X, dX )(a) = PX
0

else

index, which measures how well two partitions agree on each pair of distinct
points [10]. That is:

γX (PX | a) =

(cid:80)

|both(PX )| + |neither(PX )|

P (cid:48)

X ∈PX

|both(P (cid:48)

X )| + |neither(P (cid:48)

X )|

(1)

where PX is the set of all partitions of X and:

(cid:54) ∃sX ∈ PX , xi, xj ∈ sX ∧ (cid:54) ∃s(cid:48)

X ∈ H(X, dX )(a), xi, xj ∈ s(cid:48)

both(PX ) = {xi, xj | ∃sX ∈ PX , xi, xj ∈ sX ∧ ∃s(cid:48)
neither(PX ) = {xi, xj |
Note that by deﬁnition (cid:80)
γX (PX | a) = 1. Now suppose we have a
set of tuples {(X1, dX1, PX1), (X2, dX2, PX2 ), · · · , (Xn, dXn , PXn )} where each
(Xi, dXi ) is a metric space and each PXi
is a partition of Xi. Given an ini-
tial probability measure µ0 over O we can use Bayesian updating to learn a
posterior distribution over O by iteratively setting:

X ∈ H(X, dX )(a), xi, xj ∈ s(cid:48)

PX ∈PX

X }

X }

µi(σ) =

(cid:82)
a∈σ γXi(PXi | a) dµi−1
(cid:82)
a∈O γXi(PXi | a) dµi−1

(2)

D. Shiebler

In Figure 1 we show the results of this procedure. Under mild conditions the
functor F lattenµn ◦H maps (X, dX ) to an optimization problem with an optimal
solution that is consistent with these n observations. Formally:

Proposition 2. Given d1, d2 ∈ N suppose we have a compact region R ⊆ Rd1
and an O-clustering functor H where O is a subset of Rd2. Deﬁne dR to be
Euclidean distance and assume that:

– H is R-identiﬁable: for each a ∈ O there exists some pair of k-element

subsets X1, X2 ⊂ R with H(X1, dR)(a) (cid:54)= H(X2, dR)(a)

– H is R-smooth: for any k-element X ⊂ R and a ∈ O there exists a neigh-
borhood Ba of a where for a(cid:48) ∈ Ba we have that H(X, dR)(a) = H(X, dR)(a(cid:48))
Now suppose we sample a∗ ∈ O according to the uniform distribution µ0 over
O and for each i > 0 uniformly select a k-element subset Xi from R, set
(Xi, PXi) = H(Xi, dR)(a∗) and set µi as in Equation 2.

Then for any k-element X ⊂ R there µ0-a.s. (almost surely) exists some
m such that for n ≥ m, the partition H(X, dR)(a∗) is an optimal solution to
(F lattenµn ◦ H)(X, dR).

Proof. We will use Doob’s theorem [5] to prove this. Suppose PR is the set of
pairs (X, PX ) of ﬁnite k-element subsets X ∈ R and partitions PX of X. Deﬁne
λR to be the uniform distribution on the set of ﬁnite k-element subsets of R, and
for each a ∈ O deﬁne ηa(σ) = (cid:82)
(X,PX )∈σ γX (PX | a) dλR where σ is an element of
the Borel algebra of PR. Now since H is R-identiﬁable, a (cid:54)= a(cid:48) =⇒ ηa (cid:54)= ηa(cid:48) and
Theorem 2.4 in [8] holds. Therefore for any k-element X ⊂ R, ball Ba∗ centered
at a∗, and (cid:15) > 0, there µ0-a.s. (almost surely) exists some m such that for n ≥ m
we have that µn(Ba∗ ) ≥ 1 − (cid:15). Therefore, no solution to (F lattenµn ◦ H)(X, dR)
can be improved by including sets that only exist in partitions of X formed
from H(X, dR)(a(cid:48)) where a(cid:48) (cid:54)∈ Ba∗ . Since H is R-smooth, this implies that the
(cid:117)(cid:116)
partition H(X, dR)(a∗) is an optimal solution to (F lattenµn ◦ H)(X, dR)

3 Discussion and Next Steps

One issue with the F lattenµ procedure is that it reduces to a binary integer
program, which can be very slow to solve. In the case that the hyperparameters
of a multiparameter hierarchical clustering algorithm form a total order, we
can organize its outputs in a merge tree or dendrogram, and then solve the
optimization problem with a tree traversal [3,7].

However, it is not always possible to use this strategy on hierarchical cluster-
ing algorithms that accept multiple real-valued hyperparameters (e.g. the hyper-
parameter space is a general subset of Rd where d > 1). In this case it is possible
that there exist clusterings c ∈ H(a) and c(cid:48) ∈ H(a(cid:48)) that are neither disjoint nor
in a containment relationship. There are some ways to get around this limita-
tion, however. For example, Rolle and Scoccola [11] index hyperparameters by
a curve γ in Rd, rather than all of Rd. In this way the hyperparameter space
remains a total order. In the future we plan to explore other strategies to solve
the ﬂattening optimization problem eﬃciently.

Flattening Multiparameter Hierarchical Clustering Functors

References

1. Carlsson, G., M´emoli, F.: Persistent clustering and a theorem of J. Kleinberg. arXiv

preprint arXiv:0808.2241 (2008)

2. Chaudhuri, K., Dasgupta, S.: Rates of convergence for the cluster tree. In: Advances

in Neural Information Processing Systems. pp. 343–351 (2010)

3. Chazal, F., Guibas, L.J., Oudot, S.Y., Skraba, P.: Persistence-based clustering in

Riemannian manifolds. Journal of the ACM (JACM) 60(6), 1–38 (2013)

4. Culbertson, J., Guralnik, D.P., Hansen, J., Stiller, P.F.: Consistency constraints

for overlapping data clustering. arXiv preprint arXiv:1608.04331 (2016)

5. Doob, J.L.: Application of the theory of martingales. In: Actes du Colloque Inter-

national Le Calcul des Probabilit´es et ses applications. pp. 23–27 (1949)

6. Lang, K.: Newsweeder: Learning to ﬁlter netnews. Machine Learning Proceedings

(1995)

7. McInnes, L., Healy, J.: Accelerated hierarchical density clustering. arXiv preprint

arXiv:1705.07321 (2017)

8. Miller, J.W.: A detailed treatment of Doob’s

theorem. arXiv preprint

arXiv:1801.03122 (2018)

9. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O.,
Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A.,
Cournapeau, D., Brucher, M., Perrot, M., Duchesnay, E.: Scikit-learn: Machine
learning in Python. Journal of Machine Learning Research 12, 2825–2830 (2011)
10. Rand, W.M.: Objective criteria for the evaluation of clustering methods. Journal

of the American Statistical Association 66(336), 846–850 (1971)

11. Rolle, A., Scoccola, L.: Stable and consistent density-based clustering. arXiv

preprint arXiv:2005.09048 (2020)

12. Shiebler, D.: Functorial clustering via simplicial complexes. NeurIPS Workshop on

Topological Data Analysis in ML (2020)

13. Xiao, H., Rasul, K., Vollgraf, R.: Fashion-MNIST: a novel image dataset for bench-
marking machine learning algorithms. arXiv preprint arXiv:1708.07747 (2017)

