On using distributed representations of source code for the
detection of C security vulnerabilities

David Coimbra¸, Soﬁa Reis¸, Rui Abreu˝, Corina P˘as˘areanu(cid:204), Hakan Erdogmus(cid:204)
¸INESC-ID & IST/U.Lisbon, Portugal
˝INESC-ID & FEUP, Portugal
(cid:204)Carnegie Mellon University, USA

1
2
0
2

n
u
J

1

]

R
C
.
s
c
[

1
v
7
6
3
1
0
.
6
0
1
2
:
v
i
X
r
a

Abstract

This paper presents an evaluation of the code
representation model Code2vec when trained on
the task of detecting security vulnerabilities in
C source code. We leverage the open-source li-
brary astminer to extract path-contexts from the
abstract syntax trees of a corpus of labeled C
functions. Code2vec is trained on the resulting
path-contexts with the task of classifying a func-
tion as vulnerable or non-vulnerable. Using the
CodeXGLUE benchmark, we show that the ac-
curacy of Code2vec for this task is comparable
to simple transformer-based methods such as pre-
trained RoBERTa, and outperforms more naive
NLP-based methods. We achieved an accuracy
of 61.43% while maintaining low computational
requirements relative to larger models.

1 Introduction
Security vulnerabilities are a major concern in software de-
velopment, as even the simplest mistakes can be turned into
attack vectors by a malicious party. In continuous integra-
tion (CI), it is common to introduce static analysers into the
build pipeline to verify code against known patterns [1–3].
For example, Brakeman1 and SonarQube2 are static analy-
sers capable of detecting software vulnerabilities in source
code that can be used for this purpose.

Static analyzers can reduce security concerns in the soft-
ware development lifecycle when introduced in the imple-
mentation phase, to assist developers to produce safer soft-
ware. These analyzers are also useful tools for code review.
Nowadays, one common practice to provide feedback is us-
ing GitHub pull requests. Warnings are usually added au-
tomatically to pull requests as comments, which can be re-
viewed manually before merging. Such pipelines stream-
line the quality assurance process and increase productivity.
However, current techniques in static analysis are limited:
they are prone to false positives (wasting developer effort),
and false negatives (making the analysis less reliable) [4, 5].
In recent years, there has been an increase in the applica-
tion of statistical models, namely neural networks, to a vari-
ety of code intelligence tasks, including vulnerability detec-
tion [6, 7]. This research has mainly focused on the appli-

1https://brakemanscanner.org/
2https://docs.sonarqube.org/latest/

analysis/github-integration/

cation of pre-trained models that capture knowledge partic-
ular to a speciﬁc programming language, and has been in-
spired by transformer-based models [8] such as BERT [9]
and GPT [10], both developed in the context of natural
language processing (NLP). For example, models such as
CodeBERT [11], C-BERT [12] and PLBART [13] produce
distributed representations from source code that have been
applied to many code tasks, such as code search, code trans-
lation and vulnerability detection. The recent CodeXGLUE
benchmark [14] aims to facilitate the comparison and eval-
uation of these recent models in a large variety of tasks.
This benchmark is publicly available and open for for fur-
ther contributions 3.

Although these models are demonstrably effective, they
also have their limitations: mainly, large models with hun-
dreds of millions of parameters need a relatively large
amount of computational resources, including both mem-
ory and CPU time [15]. The requirement for large amounts
of computational resources is a signiﬁcant limitation for re-
searchers and developers, and it can make the usage of large
pre-trained models impractical. There is a noticeable trade-
off between model representativeness and convenience of
use. As a result, often smaller models are preferred for de-
tection performance.

In this work, we investigated the applicability of a code
representation model, Code2vec [16], to the vulnerability
detection task. Code2vec is built on a simple attention-
based feed-forward neural network that learns and combines
semantic knowledge extracted from AST path-contexts. To
the best of our knowledge, Code2vec has not been evaluated
for vulnerability detection. We evaluated Code2vec on a la-
beled corpus of C functions. We found that Code2vec out-
performed traditional NLP-based approaches for vulnerabil-
ity detection at an accuracy of 61.43%, comparable to sim-
ple transformer models such as pre-trained RoBERTa [17].
We show that these results are achievable at a fraction of
the computational resources and training time necessary for
transformer-based models; Code2vec ran a full training ses-
sion in approximately 5 minutes on a consumer-grade GPU,
handling 1024 samples of data at each training step with-
out exhausting its memory. We submitted our results to the
CodeXGLUE leaderboard for the defect detection task for
comparison with the state-of-the-art.
Our contributions are as follows:

• An evaluation of Code2vec on the task of vulnerabil-
ity detection using a dataset of labeled C functions

3https://github.com/microsoft/CodeXGLUE

 
 
 
 
 
 
void scsi_req_abort(SCSIRequest *req, int status) {

if (!req->enqueued) {

return;

}
scsi_req_ref(req);
scsi_req_dequeue(req);
req->io_canceled = true;
if (req->ops->cancel_io) {

req->ops->cancel_io(req);

}
scsi_req_complete(req, status);
scsi_req_unref(req);

}

Figure 1: Example of non-vulnerable function.

uint32_t HELPER(shr_cc)(CPUM68KState *env, uint32_t val, uint32_t shift) {

uint64_t temp;
uint32_t result;
shift &= 63;
temp = (uint64_t)val << 32 >> shift;
result = temp >> 32;
env->cc_c = (temp >> 31) & 1;
env->cc_n = result;
env->cc_z = result;
env->cc_v = 0;
env->cc_x = shift ? env->cc_c : env->cc_x;
return result;

}

Figure 2: Example of vulnerable function.

Vulnerable Non-Vulnerable

Train
Test
Validation

10018
1255
1187

11836
1477
1545

Table 1: Distribution of vulnerable and non-vulnerable
functions in Devign.

both regarding accuracy and computational require-
ments (training time and memory).

• A replication package with the scripts and data to
train and test the model, for reproducibility. Avail-
able online: https://github.com/dcoimbra/
dx2021.

The paper is organized as follows:

in section 2, we
present our approach for applying Code2vec to vulnerability
detection. In section 3, we describe the implementation de-
tails related to our extraction of path-contexts and Code2vec
conﬁguration. In section 4, we describe the evaluation met-
rics employed and present our results alongside previous
studies, and discuss them in section 5.
In section 6, we
give a brief summary of the related work in deep learning
for vulnerability detection. Finally, section 7 presents our
conclusions and lays discusses future work.

2 Approach

In this section, we describe our approach for using path-
context representations for the detection of security vulner-
abilities using Code2vec. We describe our procedure for the
extraction of path-contexts from a corpus of labeled func-
tions and provide a summary of Code2vec and how we
adapted it for the vulnerability detection task.

2.1 Dataset
We used the public dataset Devign4 [18], provided as
part of the CodeXGLUE benchmark. Devign includes
27318 manually-labeled functions collected from QEMU
and FFmpeg, two large C open-source projects. These
functions were extracted by collecting security-related com-
mits and selecting vulnerable and non-vulnerable versions
of functions from the labeled commits. Each function was
manually labeled by a group of three professional security
experts. Functions are labeled as 1 (vulnerable) and 0 (non-
vulnerable), with no distinction made with regard to the type
of vulnerability. Examples of a non-vulnerable and vulner-
able functions extracted from Devign are displayed in Fig-
ures 1 and 2 respectively. We used the dataset splits as pre-
pared by CodeXGLUE5: 80%/10%/10% for training, val-
idation, and testing, respectively. The distribution of vul-
nerable and non-vulnerable functions for the dataset is pre-
sented in Table 1.

2.2 Representing code snippets as bags of

path-contexts

The path-attention model on which Code2vec is built takes
a source code function as represented by a set of path-
contexts, extracted from its abstract syntax tree (AST). We
describe the deﬁnitions of AST, AST path and path-context
as presented in the Code2vec paper:
Deﬁnition 2.1 (Abstract Syntax Tree). An abstract syn-
for a code snippet C is a tuple <
tax tree (AST)
N, T, X, s, δ, φ > where N is a set of non-terminal nodes,
T is a set of terminal nodes, X is a set of values, s ∈ N is
the root node, δ : N −→ (N ∪ T )∗ is a function that maps a
non-terminal node to a list of its children, and φ : T −→ X is

4https://sites.google.com/view/devign
5https://github.com/microsoft/CodeXGLUE/

tree/main/Code-Code/Defect-detection

Figure 3: Code2vec architecture. Adapted from original image by [16]

a function that maps a terminal node to its associated value.
Every node except the root appears exactly once in all the
lists of children; that is, each node has exactly one parent.

Deﬁnition 2.2 (AST path). An AST path of length k is a se-
quence of the form: n1d1 . . . nkdknk+1, where n1, nk+1 ∈
T are terminal nodes, ∀i ∈ [2..k] : ni ∈ N are non-terminal
nodes and ∀i ∈ [1..k] : di ∈ {↑, ↓} are movement direc-
tions (up or down in the tree). If di =↑, then ni ∈ δ(ni+1);
if di =↓, then ni+1 ∈ δ(ni).
Deﬁnition 2.3 (Path-context). Given an AST path p, a path-
context is a triplet < xs, p, xt > where p is a syntactic path
in the AST and xs and xt correspond to the values associ-
ated with the start and end terminals of p, respectively. A
possible path-context for the statement x = 7 would be:

<x,(N ameExpr↑AssignExpr↓IntegerLiteralExpr),7>

It is possible to limit the paths by a maximum length and
a maximum width (distance between two child nodes of the
same intermediate node). A code snippet C is represented as
a bag of path-contexts consisting of path-contexts extracted
from the AST for C. We kept the Code2vec defaults of max-
imum length 8 and maximum width 3 and, for each function
in the corpus, extracted a bag of at most 200 path-contexts.

2.3 The Code2vec model
Code2vec learns embedding matrices for paths, values and
labels (path_vocab, value_vocab, tags_vocab, re-
spectively), a fully-connected layer, and an attention vector
a. An illustration of the Code2vec architecture is shown in
Figure 3. An embedding for a single path-context bi =<
xs, pj, xt > is a context vector ci which corresponds to the
concatenation of the embeddings of the start and end tokens
and of their connecting paths:

ci = embedding(< xs, pj, xt >) =
[value_vocabs;path_vocabj ;value_vocabt]∈R3d

(1)

In the previous equation, the operator ; is the concatena-
tion operator and d is an empirically-determined hyperpa-
rameter deﬁning the length of the internal representation.

A fully connected layer of Code2vec learns to com-
bine each component of the embedding of a path-context,
through a simple linear combination with a learned weights
matrix W passed through a hyperbolic tangent function:

˜ci = tanh(W · ci) ∈ Rd

(2)

In the previous equation, W ∈ R3d×d.

Finally,
Code2vec’s attention mechanism aggregates all combined
context vectors {˜c1, . . . , ˜cn} into a single representation.
The attention weight αi of each ˜ci is obtained through the
normalized inner product between ˜ci and the global atten-
tion vector a.

(cid:124)
exp(˜c
i · a)
(cid:124)
j · a)
j=1 exp(˜c
The ﬁnal code vector v is a weighted average of the com-

αi =

(cid:80)n

(3)

bined context vectors factored by their attention weights:

v =

n
(cid:88)

i=1

αi · ˜ci

(4)

For prediction, the probability that a speciﬁc label yi is
assigned to a code snippet C is the normalized inner product
between the embedding for yi and the code vector v:

∀yi ∈ Y : q(yi) =

exp(v(cid:124) · tags_vocabi)
yj ∈Y exp(v(cid:124) · tags_vocabj)

(cid:80)

(5)

In the previous equation, Y is the set of label values found
in the training corpus. Training is performed by minimizing
cross-entropy loss using the Adam optimization algorithm.
For inference, we take the target label to which Code2vec
assigned the highest probability.

Implementation

3
In this section, we describe the implementation details for
our approach. We describe the open-source library astminer,
which we leveraged for extracting bags of path-contexts, as
well as the parameters used for extraction and training. Our
ﬁnal pipeline is illustrated in Figure 4.

3.1 Extracting bags of path-contexts
To extract a set of path-contexts for each code snippet in
Devign, we use the open-source library astminer6 [19]. We
wrote a custom script that visits each function in the corpus,

6https://github.com/JetBrains-Research/

astminer

Figure 4: Experiments pipeline. The vulnerable and non-vulnerable raw source code functions are independently passed
to astminer. For each function, astminer extracts the respective AST, from which it computes an appropriate bag of path-
contexts. After labeling each bag of path-contexts corresponding to each function, the result is passed to Code2vec for
training. The trained model is then used for inference.

Vulnerable Non-Vulnerable

Train
Test
Validation

9987
1253
1185

11809
1472
1541

Table 2: Distribution of vulnerable and non-vulnerable
functions in Devign after applying astminer.

and computes its path-contexts. As Code2vec is designed
to predict the names of functions rather than numeric la-
bels, our script converts each label of 0 or 1 in the original
dataset to the string tokens “‘safe”’ or “‘vuln”’ respectively.
Following Code2vec defaults, we limit the length and width
of each path-context to 8 and 3 respectively, and extract at
most 200 path-contexts per function.

By default, astminer replaces each value and path in a
path-context with a corresponding ID number in order to re-
duce memory consumption and training time when passing
the samples to Code2vec. This is implemented by maintain-
ing tables of < id, value > pairs for tokens, node types,
and paths throughout the entire run. However, this was con-
siderably memory-intensive on the machine we used. As
such, we bypassed this feature and directly extracted path-
contexts in their original format. We computed the MD5
hash of the string representation of the path component of
each path-context instead of extracting the entire path as-is.
This process is identical to the one used in the Code2vec
paper for function name prediction [16].

The astminer library was unable to extract path-contexts
from 71 functions of the Devign dataset. Therefore, these
samples were not included in our study. The ﬁnal dis-
tribution of functions after applying astminer is described
in Table 2: 9987 vulnerable functions and 11809 non-
vulnerable functions for the training dataset; 1253 vulner-
able functions and 1473 non-vulnerable functions for the
testing dataset; and, ﬁnally, 1185 vulnerable functions and
1541 non-vulnerable functions for the validation dataset.

Model
CoTexT
C-BERT
PLBART
CodeBERT
Code2vec
RoBERTa
TextCNN
BiLSTM

Accuracy
66.62
65.45
63.18
62.08
61.43
61.05
60.69
59.37

Precision Recall

-
-
-
-
57.50
-
-
-

-
-
-
-
61.77
-
-
-

F1
-
-
-
-
59.56
-
-
-

Table 3: Results for the Devign dataset alongside the
CodeXGLUE leaderboard. Our contributions are in bold.

Model
Code2vec
CodeBERT
CodeBERT

Train Time
5 minutes
7 hours
1 hour

#Epochs
20
5
5

Hardware
1050Ti x1
1050Ti x1
Tesla P100 x2

Table 4: Training time on the Devign dataset alongside the
CodeXGLUE measurement, for a complete training session.
Our contributions are in bold.

3.2 Code2vec
We used the ofﬁcial implementation of Code2vec7. We
trained for 20 epochs and performed inference on the epoch
with the highest F1-score on the validation dataset. The per-
formance measures of the model were adapted to the vulner-
ability detection task: we considered a prediction of “‘safe”’
to be a negative prediction while a prediction of “‘vuln”’ to
be a positive prediction. The training hyperparameters fol-
lowed Code2vec defaults: batch size of 1024, embedding
size of 128, and dropout rate of 0.25.

4 Evaluation
The CodeXGLUE benchmark for defect detection reports
only accuracy as an evaluation metric. As Devign is a bal-

7https://github.com/tech-srl/Code2vec

Model
Code2vec
CodeBERT

#Params Embedding size Memory (MB)

31M
125M

128
400

600
2484

Table 5: Parameter count and memory consumption for each
model. Memory consumption is deﬁned as the amount oc-
cupied in RAM by the model and a single sample of data
during a training step, with all gradients loaded.

anced dataset, accuracy is an appropriate metric for assess-
ing performance. Nevertheless, we also computed preci-
sion, recall and F1-score in addition to accuracy. These
metrics help us assess the model’s ability to distinguish
between vulnerable and non-vulnerable samples. Our re-
sults are shown in Table 3 alongside the current entries
in the CodeXGLUE leaderboard. We note that CoTexT
had not been released at the time of writing.
In addition
to these scores, we compared the training time and mem-
ory consumption for Code2vec and CodeBERT on this task
on our hardware. We computed training time for a com-
plete training session for each model, which corresponds to
20 epochs on Code2vec and 5 epochs on CodeBERT. We
chose 5 epochs for CodeBERT as this is the value used in
CodeXGLUE to obtain the original results. Training times
for each model are presented in Table 4 while memory con-
sumption is shown in Table 5.

5 Results and Discussion
Based on accuracy alone, Code2vec outperformed the tradi-
tional NLP-based methods BiLSTM [20] (61.43 > 59.37)
and TextCNN [21] (61.43 > 60.69). The obtained accu-
racy score for Code2vec was slightly higher than pre-trained
RoBERTa (61.43 > 61.05):
the two methods have sim-
ilar performance. This is to be expected as these mod-
els were not designed for code intelligence tasks, nor pre-
trained on source code data. In the same vein, Code2vec
was outperformed by PLBART (61.43 < 63.18), C-BERT
(61.43 < 65.45) and CodeBERT (61.43 < 62.08), which
uses the transformer architecture to learn source-code fea-
tures through pre-training on large amounts of source code
data. As an advantage, transformer-based models do not re-
quire an intermediate representation and can be ﬁne-tuned
on source code directly.

Regarding training time, as shown in Table 4, Code2vec
completed a 20-epoch training session in approximately
5 minutes on our hardware. Conversely, on the same
hardware, CodeBERT completes a 5-epoch training ses-
sion in approximately 7 hours. This is to be expected,
as transformer-based models need a larger amount of pa-
rameters, in turn requiring much more computational re-
sources to process the large amounts of data. Addition-
ally, transformer-based models create internal representa-
tions that occupy large amounts of memory, which typically
are not available on consumer-grade hardware:
therefore,
training has to be carried out in very small batches of data,
increasing training time. A large advantage of Code2vec
compared with transformer-based models is its relatively
low memory footprint: as shown in Table 5, a single train-
ing step with CodeBERT requires approximately 2.5GB of
memory to complete one training step, an amount mostly
represented by saved gradients during back-propagation.
Conversely, a single training step with Code2vec was per-
formed with just 600MB of GPU memory. This is due

to Code2vec’s simpler architecture, allowing for a lower
amount of gradients to be saved during training, as well as
a lower number of parameters and smaller embedding sizes.
Additionally, Code2vec’s lower memory footprint allows us
to load larger batches of data into memory at each training
step, increasing training performance.

6 Related Work
Recent research on machine learning for security vulnera-
bility detection has used both token-based and graph-based
approaches [22–24].

Token-based models consider the code as a sequence
of tokens. Several models have been proposed using dif-
ferent neural network architectures such as Bidirectional
Long-Short-Term Memory (BSLTM) [22], Convolutional
Neural Networks (CNN) [24], and Recurrent Neural Net-
works (RNN). However, simple token-based models strug-
gle to reason about the long sequences produced from trans-
forming source code into token sequences. To help ad-
dress this problem, newer approaches using code slices in-
stead of the entire code sequences were proposed, see for
example VulDeePecker [22] and SySeVR [23]. The hy-
pothesis behind slicing is that different parts of the code
are not equally important for the model to learn vulner-
ability patterns. Therefore, these newer approaches con-
sider only slices extracted from interesting points (e.g., API
calls)—points considered important for vulnerability pre-
diction. The rest of the code elements are ignored. Token-
based approaches usually fail to maintain the dependencies
between the tokens that are the root of the problem. Thus,
learning those dependencies (or semantic relationships) is at
best difﬁcult and at worst impossible.

Graph-based models incorporate syntactic and semantic
dependencies between different code elements. Source code
can be transformed to syntactic graphs (Abstract Syntax
Trees) and semantic graphs (Control Flow Graphs, Data
Flow Graphs, Program Dependency Graphs, and so on).
Devign [18] uses Code Property Graphs (CPGs) to build a
vulnerability detection model as proposed by Yamaguchi et
al. [25]. Chakraborty et al.
[26] also generate code prop-
erty graphs from source code to consider the syntax and the
semantics of the code. CPGs consider succinct information
from the control-ﬂow and data-ﬂow graphs in addition to the
AST and the program dependency graphs. Each of these el-
ements offer additional context about the semantic structure
of the code that may be relevant for vulnerability detection.
Both token and graph-based models suffer from vocab-
ulary explosion—the number of possible identiﬁers (e.g.,
variable and function names) in the code can be inﬁnite.
Some approaches replace tokens with abstract names [22,
23]. Other techniques use word embedding tools (e.g.,
word2vec) to create vector representations of every token.
For instance, VulDeePecker [22] and SySeVR [23] use
word2vec to transform symbolic tokens into vectors. In con-
trast, Devign [18] uses word2vec to transform pure code
tokens to real vectors. Alon et al. [16] proposed contin-
uous distributed vector (or code embeddings) to represent
code. Code2vec aggregates an arbitrary sized snippet of
code into a ﬁxed-size vector in a way that captures its se-
mantics. Code functions are transformed in groups of path-
contexts, where each path-context represents a semantic re-
lationship between two code elements in a function.

CodeBERT [11] is a transformer-based model that repre-

sents given snippets of source code in a distributed repre-
sentation vector [8]. The non-sequential nature of the archi-
tecture of the transformer encoder, being based on a sim-
ple attention mechanism, is designed to address the prob-
lem of reasoning about long sequences: each token is pro-
cessed in parallel throughout the model. CodeBERT was
pre-trained on pairs of programming language and natural
language data. A pre-trained model produces distributed
representations that can be used in a variety of downstream
tasks, on which the model itself can be further ﬁne-tuned.

This study evaluated how Code2vec—a model that con-
siders syntactic and semantic relationships in the code—
fairs compared to other non-token based and token-based
models (speciﬁcally, CodeBERT) for vulnerability detection
in C/C++.

7 Conclusions
We applied Code2vec, a model for distributed code repre-
sentations using AST path-contexts, to the task of binary
vulnerability detection. We evaluated Code2vec on the De-
vign dataset as part of the CodeXGLUE benchmark. Our
experiments achieved an accuracy score that outperformed
naive NLP-based approaches and was equivalent to a sim-
ple transformer-based model that had not been pre-trained
on source code data. However, as expected, it was out-
performed by more expressive models that directly lever-
age features unique to the source code. Additionally, we
showed that smaller models such as Code2vec have modest
computational resource requirements compared to other al-
ternatives: when computational resources are scarce, the re-
duced training time requirement and memory consumption
of Code2vec on a labeled dataset of source code functions
may outweigh the loss in accuracy that results from its lower
expressiveness.

The performance of Code2vec for vulnerability detec-
tion could potentially be improved through hyperparameter
tuning, for example, by optimally choosing the maximum
length and width of path-contexts as well as the learning rate
and batch size. We plan to investigate these improvements
in future work. Although Code2vec is limited compared to
state-of-the-art, it remains an attractive choice for develop-
ers for code intelligence tasks such as vulnerability detec-
tion, as it demonstrates comparable performance and can be
more easily integrated in CI pipelines than static analyzers
or larger models.

Acknowledgments
This work was supported in part by Fundação para a Ciencia
e a Tecnologia (FCT) under Grants CMU/TIC/0064/2019 (a
project funded by the Carnegie Mellon Portugal Program)
and UIDB/50021/2020.

References
[1]

Jason Bau, Elie Bursztein, Divij Gupta, and John
Mitchell. State of the art: Automated black-box web
application vulnerability testing. In 2010 IEEE Sympo-
sium on Security and Privacy, pages 332–345, 2010.
Juha Kuusela. Security testing in continuous inte-
gration processes. Master’s thesis, Aalto University.
School of Science, 2017.

[2]

[3] Fiorella Zampetti, Simone Scalabrino, Rocco Oliveto,
Gerardo Canfora, and Massimiliano Di Penta. How

open source projects use static code analysis tools in
continuous integration pipelines. In 2017 IEEE/ACM
14th International Conference on Mining Software
Repositories (MSR), pages 334–344, 2017.
J. Zheng, L. Williams, N. Nagappan, W. Snipes, J.P.
Hudepohl, and M.A. Vouk. On the value of static anal-
ysis for fault detection in software. IEEE Transactions
on Software Engineering, 32(4):240–253, 2006.

[4]

[6]

[5] Paul Anderson.

The use and limitations of
static-analysis tools to improve software quality.
CrossTalk: The Journal of Defense Software Engineer-
ing, 21(6):18–21, 2008.
Jacob A. Harer, Louis Y. Kim, Rebecca L. Russell,
Onur Ozdemir, Leonard R. Kosta, Akshay Rangamani,
Lei H. Hamilton, Gabriel I. Centeno, Jonathan R. Key,
and Paul M. Ellingwood. Automated software vulner-
ability detection with machine learning. arXiv preprint
arXiv:1803.04497, 2018.

[7] Guanjun Lin, Sheng Wen, Qing-Long Han, Jun Zhang,
and Yang Xiang. Software vulnerability detection us-
ing deep neural networks: A survey. Proceedings of
the IEEE, 108(10):1825–1848, 2020.

[9]

[8] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need.
In Advances in Neural Information Processing Sys-
tems, volume 30. Curran Associates, Inc., 2017.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. BERT: Pre-training of deep bidi-
rectional transformers for language understanding. In
Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long and Short Papers), pages 4171–4186,
Minneapolis, Minnesota, June 2019. Association for
Computational Linguistics.

[10] Alec Radford, Karthik Narasimhan, Tim Salimans,
and Ilya Sutskever. Improving language understand-
ing by generative pre-training. 2018.

[11] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-
aocheng Feng, Ming Gong, Linjun Shou, Bing Qin,
Ting Liu, and Daxin Jiang. CodeBERT: A pre-trained
model for programming and natural languages. arXiv
preprint arXiv:2002.08155, 2020.

[12] Luca Buratti, Saurabh Pujar, Mihaela Bornea, Scott
McCarley, Yunhui Zheng, Gaetano Rossiello, Alessan-
dro Morari,
and
Exploring software naturalness
Yufan Zhuang.
arXiv preprint
through neural
arXiv:2006.12641, 2020.

Jim Laredo, Veronika Thost,

language models.

[13] Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi
Ray, and Kai-Wei Chang. Uniﬁed pre-training for pro-
gram understanding and generation. arXiv preprint
arXiv:2103.06333, 2021.

[14] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang,
Alexey Svyatkovskiy, Ambrosio Blanco, Colin
Clement, Dawn Drain, Daxin Jiang, and Duyu Tang.
CodeXGLUE: A machine learning benchmark dataset
for code understanding and generation. arXiv preprint
arXiv:2102.04664, 2021.

[15] Nimit Sharad Sohoni, Christopher Richard Aberger,
Megan Leszczynski, Jian Zhang, and Christopher Ré.
Low-memory neural network training: A technical re-
port. arXiv preprint arXiv:1904.10631, 2019.

[16] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Ya-
hav. code2vec: Learning distributed representations of
code. Proceedings of the ACM on Programming Lan-
guages, 3, January 2019.

[17] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,
Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa:
A robustly optimized BERT pretraining approach.
arXiv preprint arXiv:1907.11692, 2019.

[18] Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning
Du, and Yang Liu. Devign: Effective vulnerability
identiﬁcation by learning comprehensive program se-
In Advances in
mantics via graph neural networks.
Neural Information Processing Systems, volume 32.
Curran Associates, Inc., 2019.

[19] Vladimir Kovalenko, Egor Bogomolov, Timofey
Bryksin, and Alberto Bacchelli.
PathMiner: a li-
brary for mining of path-based representations of code.
In Proceedings of the 16th International Conference
on Mining Software Repositories, pages 13–17. IEEE
Press, 2019.

[20] Alex Graves and Jürgen Schmidhuber. Framewise
phoneme classiﬁcation with bidirectional LSTM and
other neural network architectures. Neural Networks,
18(5):602–610, 2005. IJCNN 2005.

[21] Yoon Kim. Convolutional neural networks for sen-
tence classiﬁcation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1746–1751, Doha, Qatar,
October 2014. Association for Computational Linguis-
tics.

[22] Zhen Li, Deqing Zou, Shouhuai Xu, Xinyu Ou, Hai
Jin, Sujuan Wang, Zhijun Deng, and Yuyi Zhong.
Vuldeepecker: A deep learning-based system for vul-
nerability detection. Proceedings 2018 Network and
Distributed System Security Symposium, 2018.
[23] Zhen Li, Deqing Zou, Shouhuai Xu, Hai Jin, Yawei
Zhu, and Zhaoxuan Chen. Sysevr: A framework for
using deep learning to detect software vulnerabilities.
IEEE Transactions on Dependable and Secure Com-
puting, page 1, 2021.

[24] Rebecca Russell, Louis Kim, Lei Hamilton, Tomo La-
zovich, Jacob Harer, Onur Ozdemir, Paul Ellingwood,
and Marc McConley. Automated vulnerability detec-
tion in source code using deep representation learn-
ing. In 2018 17th IEEE International Conference on
Machine Learning and Applications (ICMLA), pages
757–762, 2018.

[25] Fabian Yamaguchi, Nico Golde, Daniel Arp, and Kon-
rad Rieck. Modeling and discovering vulnerabilities
with code property graphs. In 2014 IEEE Symposium
on Security and Privacy, pages 590–604, 2014.
[26] Saikat Chakraborty, Rahul Krishna, Yangruibo Ding,
and Baishakhi Ray. Deep learning based vulnera-
arXiv preprint
bility detection: Are we there yet?
arXiv:2009.07235, 2020.

