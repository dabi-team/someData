0
2
0
2

p
e
S
8
1

]

G
L
.
s
c
[

2
v
0
7
5
0
0
.
4
0
0
2
:
v
i
X
r
a

Tightened Convex Relaxations for Neural Network Robustness Certiﬁcation

Brendon G. Anderson, Ziye Ma, Jingqi Li, and Somayeh Sojoudi

Abstract— In this paper, we consider the problem of cer-
tifying the robustness of neural networks to perturbed and
adversarial
input data. Such certiﬁcation is imperative for
the application of neural networks in safety-critical decision-
making and control systems. Certiﬁcation techniques using
convex optimization have been proposed, but they often suffer
from relaxation errors that void the certiﬁcate. Our work
exploits the structure of ReLU networks to improve relaxation
errors through a novel partition-based certiﬁcation procedure.
The proposed method is proven to tighten existing linear
programming relaxations, and asymptotically achieves zero
relaxation error as the partition is made ﬁner. We develop a
ﬁnite partition that attains zero relaxation error and use the
result to derive a tractable partitioning scheme that minimizes
the worst-case relaxation error. Experiments using real data
show that the partitioning procedure is able to issue robustness
certiﬁcates in cases where prior methods fail. Consequently,
partition-based certiﬁcation procedures are found to provide
an intuitive, effective, and theoretically justiﬁed method for
tightening existing convex relaxation techniques.

I. INTRODUCTION

Recent successes of neural networks can be found in
nearly all forms of data-driven decision making problems.
In particular, both classical and modern problems within
control theory have been addressed using neural networks,
e.g., control of nonlinear systems [1], [2], data-driven system
identiﬁcation [3]–[5], and adaptive and self-learning control
[6], [7]. With their increasing prevalence, neural networks
have begun to ﬁnd applications in highly sensitive data-
driven decision-making problems involving the control of
safety-critical systems, such as autonomous vehicles [8],
[9] and the power grid [10]–[12]. The common underlying
principle among these systems is that decisions and control
actions must be robust against ﬂuctuations in the measure-
ments or inputs to the decision making algorithm. As a
result, much effort has been placed on developing methods to
certify the robustness of neural networks to perturbations in
their input data [13]–[20]. Due to the vast range of network
architectures, their inherent nonconvexity, and computational
burdens arising with large-scale networks, the development
of efﬁcient and reliable certiﬁcation methods remains an
ongoing effort.

A common deterministic certiﬁcation procedure is to ver-
ify that all possible unknown inputs are mapped to outputs
that the network operator classiﬁes as safe [13], [19]. From
this perspective, certiﬁcation amounts to proving that the
image of an input uncertainty set
is contained within a
prescribed safe set. Such a worst-case analysis is naturally

The authors are with the University of California, Berkeley. Somayeh
Sojoudi is also with the Tsinghua-Berkeley Shenzhen Institute. Emails:
{bganderson,ziyema,jingqili,sojoudi}@berkeley.edu.
This work was supported by grants from AFOSR, ONR, and NSF.

formulated as a robust optimization (RO) problem [21],
[22], however, approaching the problem from a general RO
framework neglects the informative structure of the network
architecture. Furthermore, even when the input uncertainty
set is convex, its output set may be nonconvex, which renders
the certiﬁcation an NP-complete nonconvex optimization
problem [16], [23]. To overcome these issues, researchers
have proposed various relaxations to over-approximate the
output set of the network by a convex one and perform the
certiﬁcation on the easier-to-analyze convex set. One of the
simplest and most popular approximation classes is based on
linear program (LP) relaxations [13].

In the case that a convex outer approximation of the
original nonconvex output set is contained in the safe set, a
certiﬁcate of robustness for the true network can be obtained.
An immediate problem arises with these convex relaxations:
if the convex outer approximation of the output set is too
loose, the relaxation may not issue a certiﬁcate even in the
case the true network is robust. To tighten the outer approx-
imation, more sophisticated and computationally demanding
convex relaxations have been proposed in the literature,
such as the semideﬁnite programming and quadratically-
constrained semideﬁnite programming techniques [14], [18].

A. Partition-Based Certiﬁcation

In this paper, we focus on feedforward ReLU networks,
which are popular due to their simplicity, fast training speeds,
and non-vanishing gradient property [15]. Our approach to
certifying these networks is based on partitioning the input
uncertainty set and solving simple linear programs over
each input part. Partitioning heuristics have been applied
in areas such as robust optimization [24] and deep learning
[25], and are often found to tighten bounds on optimization
objectives. Furthermore, partitioning naturally allows for
parallelization of the optimization, resulting in computational
advantages over centralized methods. Our proposed method
is closely related to solving mixed integer programming
(MIP) problems, as ReLU robustness certiﬁcation can be
expressed as an MIP. However, mixed integer reformulations
of ReLU constraints introduce new integer variables for each
neuron in the network, making MIP approaches, such as the
outer approximation algorithm in [26], unnecessarily large
in dimension. Instead, our partition-based approach directly
encodes the integral nature of the ReLU constraints without
adding extra variables.

Previous works that apply partitioning to network certi-
ﬁcation include [15], where the authors perform a reacha-
bility analysis for the safety veriﬁcation of neural network
controllers. However, that method is restricted to hyperrect-
angular partitions of both the input uncertainty set and the re-

 
 
 
 
 
 
sulting outer approximations. The authors of [19] use duality
arguments to propose a novel partitioning scheme; however,
the designed algorithm only considers splitting box-shaped
uncertainty sets in half along coordinate axes. Not only are
the current partition-based methods too restrictive in their
partition structure and accordingly produce unnecessarily
loose outer approximations, but they also lack mathematical
support for the effectiveness of the partitioning in tightening
the relaxations.

B. Contributions

In an effort to improve relaxation errors, we exploit the

nature of ReLU networks to achieve the following goals:

1) Prove that partitioning tightens existing linear program
relaxations, and deﬁne the notion of Lipschitz relax-
ations to show that relaxation error converges to zero
as partitions become ﬁner;

2) Show that an intelligently designed ﬁnite partition
attains zero relaxation error, and use this insight to
derive a computationally tractable partitioning scheme
that minimizes worst-case relaxation error;

3) Demonstrate on real data that the optimal partitioning
scheme sufﬁciently reduces relaxation error to certify
robustness where prior methods fail.

The contributions of this paper culminate into a theoretically
justiﬁed and empirically validated robustness certiﬁcation
procedure that combines simple and efﬁcient linear program
models, computational parallelizability, and optimal relax-
ation tightening.

C. Organization

In Section II, we deﬁne some mathematical notations.
Section III introduces the robustness certiﬁcation problem
and linear program relaxation. In Section IV, we introduce
the notion of partitioning and analyze its properties when
applied to the robustness certiﬁcation of ReLU networks.
In Section V, we further develop the theory to study the
optimality of partitions and propose an optimal partitioning
strategy. We provide illustrative examples in Section VI and
conclude in Section VII.

II. NOTATIONS

We write the sets of n-vectors and m × n matrices with
real-valued elements as Rn and Rm×n, respectively. For
X, Y ∈ Rm×n, we write X ≤ Y to mean Xij ≤ Yij for
all i ∈ {1, 2, . . . , m} and all j ∈ {1, 2, . . . , n}. We write
the Hadamard (element-wise) product between X and Y as
X ⊙ Y and the Hadamard division of X by Y as X ⊘ Y .
Furthermore, for f : R → R, we deﬁne f (X) to be an
m × n matrix whose (i, j) element is equal to f (Xij) for all
i ∈ {1, 2, . . . , m} and all j ∈ {1, 2, . . . , n}. In particular, let
the ReLU function be denoted as ReLU(·) = max{0, ·}.

III. PROBLEM STATEMENT

A. Network Description

Consider a K-layer ReLU neural network deﬁned by

x[0] = x,
ˆz[k] = W [k−1]x[k−1] + b[k−1],
x[k] = ReLU(ˆz[k]),

z = x[K],

(1)

for all k ∈ {1, 2, . . . , K}, where x ∈ Rnx is the input to the
neural network, z ∈ Rnz is the output, and ˆz[k] ∈ Rnk is
the preactivation of the kth layer. The parameters W [k] ∈
Rnk+1×nk and b[k] ∈ Rnk+1 are the weight matrix and
bias vector applied to the kth layer’s activation x[k] ∈ Rnk ,
respectively. Without loss of generality, assume that the bias
terms are accounted for in the activations x[k], thereby setting
b[k] = 0 for all layers k. Let the function f : Rnx → Rnz
denote the map x 7→ z deﬁned by (1).

B. Input Uncertainty, Relaxed Network Constraint, and Safe
Sets

We consider the scenario in which the network inputs are
unknown but contained in a compact set X ⊆ Rnx . We call
X the input uncertainty set, which is assumed to be a convex
polytope. In the literature of neural network robustness
certiﬁcation, the input uncertainty set is commonly modeled
as X = {x ∈ Rnx : kx − ¯xk∞ ≤ ǫ}, where ¯x ∈ Rnx is a
nominal input to the network and ǫ > 0 [13], [14].

The bounds on the input, as deﬁned by X , implicitly
deﬁne bounds on the preactivation at each layer. That is,
x ∈ X implies that there exist bounds l[k], u[k] ∈ Rnk such
that l[k] ≤ ˆz[k] ≤ u[k] for all k ∈ {1, 2, . . . , K}. Although
one can create an outer approximation of these bounds, we
consider the true bounds l[k] and u[k] as tight, i.e., z[k] = l[k]
for some x ∈ X and similarly for the upper bound u[k]. From
these bounds, we relax the kth ReLU constraint in (1) to its
convex envelope, which leads to a relaxed ReLU constraint
set associated with the kth layer:

N [k] = {(x[k−1], x[k]) ∈ Rnk−1 × Rnk :

x[k] ≤ u[k] ⊙ (ˆz[k] − l[k]) ⊘ (u[k] − l[k]),
x[k] ≥ 0, x[k] ≥ ˆz[k], ˆz[k] = W [k−1]x[k−1]}.

(2)

Deﬁne the relaxed network constraint set as

N = {(x, z) ∈ Rnx × Rnz : (x, x[1]) ∈ N [1],

(x[1], x[2]) ∈ N [2], . . . , (x[K−1], z) ∈ N [K]}.

(3)

In essence, N is the set of all feasible input-output pairs of
the network satisfying the relaxed ReLU constraint at each
layer. Since the bounds l[k] and u[k] are determined by the
input uncertainty set X , the set N [k] is also determined by
X for all layers k.
Remark 1. In the context of one-layer networks (i.e., K = 1),
the single relaxed ReLU constraint set coincides with the
relaxed network constraint set: N [1] = N . Therefore, for
K = 1 we drop the k-notation from z, ˆz, x, W , l, u, and
N . A visualization of N is given in Fig. 1 for this case.

z

ReLU(ˆz)

l

0

ˆz

u

Fig. 1. Relaxed ReLU constraint set at a single neuron for a one-layer
network. The convex envelope N is shaded.

Remark 2. Consider a one-layer ReLU constraint relaxed
according to (2). Suppose that l < u < 0. A simple
calculation shows that N = {(x, 0) ∈ Rnx × Rnz : W x = l}
due to the inequalities l ≤ W x ≤ u. That is, the set of
input-output pairs that are feasible for the relaxed network
constraints exclude many possible inputs that are feasible for
the input uncertainty constraints. The same problem occurs
when 0 < l < u. To overcome this issue, we impose the
conditions that l ≤ 0 ≤ u and l < u so that the certiﬁcation
procedure considers all possible inputs in X .

Now, consider a set S ⊆ Rnz , termed the safe set. As is
common in the adversarial machine learning literature, we
consider (possibly unbounded) polyhedral safe sets deﬁned
as the intersection of a ﬁnite number of half-spaces: S =
{z ∈ Rnz : Cz ≤ d}, where C ∈ RnS ×nz and d ∈ RnS are
given. An output z ∈ S is said to be safe.

C. Robustness Certiﬁcation

The goal is to certify that all inputs in X map to safe out-
puts in S. If this is successfully accomplished, the network
is said to be certiﬁably robust. Formally, this certiﬁcate is
written as f (X ) ⊆ S, or equivalently

c⊤
i f (x) ≤ di for all i ∈ {1, 2, . . . , nS},

sup
x∈X

is the ith row of C. Thus, the certiﬁcation procedure
where c⊤
i
amounts to solving an optimization problem corresponding
to each ci. In the sequel, we focus on a single optimization
problem, namely supx∈X c⊤f (x), since the generalization
to the case nS > 1 is straightforward. With no loss of
generality, assume that d = 0 (if d 6= 0, one can ﬁrst solve
the optimization for d = 0 and then shift the corresponding
result). Note that
the proposed mathematical framework
encapsulates the popular certiﬁcation that a classiﬁcation
network will not misclassify any adversarial inputs within
a bounded uncertainty set.

In general, the optimization supx∈X c⊤f (x) is a noncon-
vex problem and f (X ) is a nonconvex set, and therefore
computing a robustness certiﬁcate is intractable. To circum-
vent this issue, one can instead certify that a convex outer
approximation of f (X ) is safe, as this inherently certiﬁes the
safety of the true nonconvex set f (X ), and hence certiﬁes
the robustness of the network. This process is illustrated in
Fig. 2.

The robustness certiﬁcation problem can be written as

f

X

f (X )

ˆf (X )

The convex outer approximation of the nonconvex set f (X ) is
Fig. 2.
ˆf (X ). If the outer approximation is safe, i.e., ˆf (X ) ⊆ S, then so is f (X ).

The nonconvexity of (4) comes from the nonlinear equality
constraint z = f (x). Note that for all x ∈ X , the equality
z = f (x) implies that (x, z) ∈ N . Therefore, to avoid
the nonconvex equality constraint, one can use the relaxed
network constraint set to solve the following surrogate LP
relaxation [13]:

ˆf ∗(X ) = sup{c⊤z : (x, z) ∈ N , x ∈ X }.

(5)

The suprema in (4) and (5) are assumed to be attained.
Due to the relaxation introduced in (5), it holds that

f ∗(X ) ≤ ˆf ∗(X ).

(6)

Therefore, a sufﬁcient condition for the network to be
certiﬁably robust is that ˆf ∗(X ) ≤ 0. In the case ˆf ∗(X ) > 0,
the relaxation cannot certify whether or not the true network
is robust, since it may still hold that f ∗(X ) ≤ 0. The
remainder of this paper is dedicated to optimally tightening
the bound (6) while maintaining the advantageous convexity
and computational properties of the LP relaxation.

IV. PROPERTIES OF PARTITIONED RELAXATIONS

In this section, we investigate the notion of input parti-
tioning and rigorously derive guarantees on the effectiveness
of partitioned relaxations for the robustness certiﬁcation
problem. We will ﬁrst show that by partitioning the input
uncertainty set and solving separate LP relaxations over each
part, a useful upper bound for the unrelaxed problem (4) can
be obtained. In particular, the partitioning method yields a
valid relaxation of (4).

A. Validation of Partitioned Relaxations
Deﬁnition 1 (Partition). The collection {X (j) ⊆ X : j ∈
{1, 2, . . . , p}} is said to be a partition of the input uncertainty
set X if X = ∪p
j=1X (j) and X (j) ∩ X (k) = ∅ for all j 6= k.
The set X (j) is called the jth input part.

Proposition 1 (Partitioned relaxation bound). Let {X (j) ⊆
X : j ∈ {1, 2, . . . , p}} be a partition of X . Then, it holds
that

f ∗(X ) ≤ max

j∈{1,2,...,p}

ˆf ∗(X (j)).

(7)

Proof. Assume that f ∗(X ) > maxj∈{1,2,...,p}
Then,

ˆf ∗(X (j)).

f ∗(X ) > ˆf ∗(X (j)) for all j ∈ {1, 2, . . . , p}.

(8)

Let (x∗, z∗) denote an optimal solution to the unrelaxed
problem (4), i.e., x∗ ∈ X , z∗ = f (x∗), and

f ∗(X ) = sup{c⊤z : z = f (x), x ∈ X }.

(4)

c⊤z∗ = f ∗(X ).

(9)

j=1X (j) = X , there exists j∗ ∈ {1, 2, . . . , p} such
) and z∗ = f (x∗), it holds
) is the relaxed network

Since ∪p
that x∗ ∈ X (j∗
that (x∗, z∗) ∈ N (j∗
constraint set deﬁned by X (j∗

). Since x∗ ∈ X (j∗

), where N (j∗

). Therefore,

c⊤z∗ ≤ sup{c⊤z : x ∈ X (j∗

), (x, z) ∈ N (j∗
= ˆf ∗(X (j∗

)}

)) < f ∗(X ),

where the ﬁrst
inequality comes from the feasibility of
(x∗, z∗) over the j∗th subproblem and the ﬁnal inequality is
due to (8). This contradicts the optimality of (x∗, z∗) given
in (9). Hence, (7) must hold.

B. Tightening of the LP Relaxation

The objective is to show that by partitioning the input
uncertainty set, the linear program relaxation bound in (6)
is improved. The result will be presented for one-layer net-
works for simplicity, but the conclusion naturally generalizes
to multi-layer networks.

Proposition 2 (Improving the relaxation bound). Consider
a one-layer feedforward neural network. Let {X (j) ⊆ X :
j ∈ {1, 2, . . . , p}} be a partition of X . For the jth input part
X (j), denote the corresponding preactivation bounds by l(j)
and u(j), where l ≤ l(j) ≤ W x ≤ u(j) ≤ u for all x ∈ X (j).
Then, it holds that

max
j∈{1,2,...,p}

ˆf ∗(X (j)) ≤ ˆf ∗(X ).

(10)

Proof. Let j ∈ {1, 2, . . . , p}. It will be shown that N (j) ⊆
N . Let (x, z) ∈ N (j). Deﬁne u′ = u(j), l′ = l(j), and

g(x) = u ⊙ (W x − l) ⊘ (u − l),
g′(x) = u′ ⊙ (W x − l′) ⊘ (u′ − l′).

Then, by letting ∆g(x) = g(x) − g′(x) = a ⊙ (W x) + b,
where

a = u ⊘ (u − l) − u′ ⊘ (u′ − l′),
b = u′ ⊙ l′ ⊘ (u′ − l′) − u ⊙ l ⊘ (u − l),

that l(j) = l′ ≤ W x ≤ u′ = u(j). Hence, since (x, z) ∈
N (j), it holds that z ≥ 0, z ≥ W x, and

z ≤ g′(x) ≤ g(x) = u ⊙ (W x − l) ⊘ (u − l).

Therefore, we have that (x, z) ∈ N .

Since X (j) ⊆ X (by deﬁnition) and N (j) ⊆ N , it holds
that the solution to the problem over the smaller feasible set
gives a lower bound to the original solution: ˆf ∗(X (j)) ≤
ˆf ∗(X ). Finally, since j was chosen arbitrarily, this implies
the desired inequality (10).

C. Asymptotic Exactness of Partitioned Relaxations

In this section, we deﬁne the notion of Lipschitz continuity
of a relaxation. We use this property to show that, under ap-
propriate conditions, LP relaxations asymptotically approach
the true problem as the partition becomes ﬁner.

Deﬁnition 2 (L-Lipschitz relaxation). A neural network f
is said to have an L-Lipschitz continuous relaxation (with
respect to N on X ) if there exists a ﬁnite constant L ∈ R
such that

|c⊤z∗

2| ≤ Lkx1 − x2k2 for all x1, x2 ∈ X ,

1 − c⊤z∗
i = sup{c⊤z : (xi, z) ∈ N } for all i ∈ {1, 2}.

where c⊤z∗

Remark 3. In the case the relaxed network constraint set
is exact (i.e., (x[k−1], x[k]) ∈ N [k] if and only if x[k] =
ReLU(W [k−1]x[k−1]) for all layers k ∈ {1, 2, . . . , K}), the
i = c⊤f (xi), and so the L-
relation xi ∈ X implies that c⊤z∗
Lipschitz continuity of the relaxation reduces to the classical
L-Lipschitz continuity of the function c⊤f over the set X .

Lemma 1 (Lipschitz LP relaxations). All K-layer neural
networks deﬁned by (1) have L-Lipschitz continuous LP
relaxations.

Proof. Let x1, x2 ∈ X . By Theorem 2.4 in [27], there exists
a ﬁnite constant β ∈ R such that for all z∗
1 ∈ arg max{c⊤z :
(x1, z) ∈ N } there exists z∗
2 ∈ arg max{c⊤z : (x2, z) ∈ N }
satisfying

kz∗

1 − z∗

2 k∞ ≤ βkx1 − x2k2.

the following relations are derived for all i ∈ {1, 2, . . . , nz}:

By the Cauchy-Schwarz inequality, this yields that

g∗
i :=

inf
{x:l′≤W x≤u′}

(∆g(x))i ≥

inf
{ˆz:l′≤ˆz≤u′}

(a ⊙ ˆz + b)i

=

inf
i≤ˆzi≤u′
i}

{ˆzi:l′

(ai ˆzi + bi) =

ail′
aiu′

i + bi
i + bi

(

if ai ≥ 0,
if ai < 0.

In the case that ai ≥ 0, we have that

i ≥ ail′
g∗

i + bi =

ui
ui − li

(l′

i − li) ≥ 0,

where the ﬁnal inequality comes from the fact that u ≥ 0,
l′ ≥ l, and u > l. On the other hand, if ai < 0, it holds that

i ≥ aiu′
g∗

i + bi =

u′
i − ui
ui − li

li ≥ 0,

where the ﬁnal inequality comes from the fact that u′ ≤ u,
l ≤ 0, and u > l. Therefore, g∗ = (g∗
nz ) ≥ 0,
which implies that ∆g(x) = g(x) − g′(x) ≥ 0 for all x such

2, . . . , g∗

1, g∗

|c⊤(z∗

1 − z∗

2 )| ≤ kck1kz∗
Deﬁning L = βkck1 completes the proof.

1 − z∗

2 k∞ ≤ βkck1kx1 − x2k2.

Lemma 1 shows that a partitioned LP relaxation is a
Lipschitz relaxation over any chosen input part. This property
will be used to derive a bound on the difference between the
partitioned LP relaxation and the unrelaxed problem for one-
layer networks based on the diameters of the input parts.
Deﬁnition 3 (Diameter). For a set X ⊆ Rn, the diameter of
X is deﬁned as d(X ) = sup{kx − yk2 : x, y ∈ X }.

Proposition 3 (Diameter bound). Consider a one-layer
feedforward neural network over the input uncertainty set
X and the relaxed network constraint set N . Let {X (j) ⊆
X : j ∈ {1, 2, . . . , p}} be a partition of X . Denote the
largest diameter among the input parts by d∗, i.e., d∗ =

max{d(X (j)) : j ∈ {1, 2, . . . , p}}. Then, there exists a ﬁnite
constant L ∈ R such that

ˆf ∗(X (j))
(cid:12)
(cid:12)
(cid:12)
(cid:12)

f ∗(X ) − max

j∈{1,2,...,p}

≤ Ld∗.

(11)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Proof. First, let j∗ be the index corresponding to the partition
subproblem with the highest objective value: ˆf ∗(X (j∗
)) =
ˆf ∗(X (j)). By Lemma 1, the network has an
maxj∈{1,2,...,p}
L-Lipschitz relaxation with respect to N (j∗
). Thus,
there exists a ﬁnite constant L ∈ R such that

) on X (j∗

L ≥

|c⊤z∗

1 − c⊤z∗
2 |
kx1 − x2k2

for all x1, x2 ∈ X (j∗
N (j∗
)} and c⊤z∗
more, by the deﬁnition of d∗ and j∗, we have that

2 = sup{c⊤z : (x2, z) ∈ N (j∗

1 = sup{c⊤z : (x1, z) ∈
)}. Further-

), where c⊤z∗

).

)) ≥ kx1 − x2k2 for all x1, x2 ∈ X (j∗
) be such that W ¯x = l(j∗

d∗ ≥ d(X (j∗
Let ¯x ∈ X (j∗
) and ¯z =
ReLU(W ¯x), so that c⊤ ¯z = sup{c⊤z : (¯x, z) ∈ N (j∗
)}.
Furthermore, let (˜x∗, ˜z∗) denote a solution to the relaxed
problem (5) over the j∗th input part, i.e., corresponding to
ˆf ∗(X (j∗
) and c⊤ ˜z∗ = sup{c⊤z :
(˜x∗, z) ∈ N (j∗

)). Then, since ¯x, ˜x∗ ∈ X (j∗
)}, it holds that

Ld∗ ≥

|c⊤ ¯z − c⊤ ˜z∗|
k¯x − ˜x∗k2

k¯x − ˜x∗k2 = |c⊤ ¯z − ˆf ∗(X (j∗

))|.

Now, since (¯x, ¯z) is feasible for the unrelaxed problem (4)
and the relaxation ˆf ∗(X (j∗
)) provides an upper bound on
the unrelaxed problem by Proposition 1, it holds that

c⊤ ¯z ≤ f ∗(X ) ≤ ˆf ∗(X (j∗

)).

This implies that |f ∗(X ) − ˆf ∗(X (j∗
Therefore, Ld∗ ≥ |f ∗(X ) − ˆf ∗(X (j∗
desired.

))| ≤ |c⊤ ¯z − ˆf ∗(X (j∗

))|.
))|, proving (11), as

In the case that the network has a Lipschitz relaxation
that is uniform over all possible input parts, Proposition 3
shows that as the partition becomes ﬁner, namely d∗ → 0, the
gap between the partitioned relaxation and the true solution
converges to zero. As a result, partitioned relaxations are
asymptotically exact.

V. OPTIMAL PARTITIONING

In this section, we construct a partition with a ﬁnite num-
ber of input parts under which LP relaxations exactly recover
the nonconvex robustness certiﬁcation problem. Motivated
by this optimal partition, we develop a simple and compu-
tationally tractable partitioning procedure that minimizes the
worst-case relaxation error.

A. Exact Partitioned Relaxation

The goal is to show that by meticulously selecting the
partition of the input uncertainty set based on the rows of the
weight matrix W , the relaxation introduced by the resulting
linear programs becomes exact.

Proposition 4 (Motivating partition). Consider a one-layer
feedforward neural network and denote the ith row of W by
i ∈ R1×nx for all i ∈ {1, 2, . . . , nz}. Deﬁne J = {0, 1}nz
w⊤
and take the partition of X to be indexed by J . That is,
{X (j) ⊆ X : j ∈ J }, where for a given j ∈ J we deﬁne

X (j) = {x ∈ X : w⊤
w⊤

i x < 0 for all i such that ji = 0}.

i x ≥ 0 for all i such that ji = 1,

(12)

(13)

Then, the partitioned relaxation is exact, i.e.,

f ∗(X ) = max
j∈J

ˆf ∗(X (j)).

Proof. We ﬁrst show that {X (j) ⊆ X : j ∈ J } is a
valid partition. Since X (j) ⊆ X for all j ∈ J , the relation
∪j∈J X (j) ⊆ X is satisﬁed. Now, suppose that x ∈ X . Then,
for all i ∈ {1, 2, . . . , nz}, it holds that either w⊤
i x ≥ 0 or
w⊤

i x < 0. Deﬁne j ∈ {0, 1}nz as follows:

ji =

1
0

(

if w⊤
if w⊤

i x ≥ 0,
i x < 0,

for all i ∈ {1, 2, . . . , nz}. Then, by the deﬁnition of X (j) in
(12), it holds that x ∈ X (j). Therefore, the relation x ∈ X
implies that x ∈ X (j) for some j ∈ {0, 1}nz = J . Hence,
X ⊆ ∪j∈J X (j), and therefore ∪j∈J X (j) = X .

We now show that X (j) ∩ X (k) = ∅ for all j 6= k. Let
j, k ∈ J with the property that j 6= k. Then there exists
i ∈ {1, 2, . . . , nz} such that ji 6= ki. Let x ∈ X (j). In the
case that w⊤
i x ≥ 0, it holds that ji = 1 and therefore ki = 0.
Hence, for all y ∈ X (k), it holds that w⊤
i y < 0, and therefore
x /∈ X (k). An analogous reasoning shows that x /∈ X (k)
i x < 0. Therefore, one concludes that x ∈ X (j) and
when w⊤
j 6= k implies that x /∈ X (k), i.e., that X (j) ∩ X (k) = ∅.
Hence, {X (j) ⊆ X : j ∈ J } is a valid partition.
We now prove (13). Let j ∈ J . Since w⊤

i x ≥ 0 for all
i such that ji = 1, the preactivation lower bound becomes
l(j)
i = 0 for all such i. On the other hand, since w⊤
i x < 0
for all i such that ji = 0, the preactivation upper bound
becomes u(j)
i = 0 for all such i. Therefore, the relaxed
network constraint set (3) for the jth input part reduces to

N (j) = {(x, z) ∈ Rnx × Rnz :

zi = 0 for all i such that ji = 0,
zi = w⊤

i x = (W x)i for all i such that ji = 1}.

That is, the relaxed ReLU constraint envelope collapses to
the exact ReLU constraint through the a priori knowledge
of each preactivation coordinate’s sign. Therefore, we ﬁnd
that for all x ∈ X (j) it holds that (x, z) ∈ N (j) if and only
if z = ReLU(W x). Hence, the LP over the jth input part
yields that

ˆf ∗(X (j)) = sup{c⊤z : (x, z) ∈ N (j), x ∈ X (j)}

= sup{c⊤z : z = ReLU(W x), x ∈ X (j)}
≤ sup{c⊤z : z = ReLU(W x), x ∈ X }
= f ∗(X ).

Therefore, maxj∈J ˆf ∗(X (j)) ≤ f ∗(X ), since j was chosen
arbitrarily. Since f ∗(X ) ≤ maxj∈J ˆf ∗(X (j)) by the relax-
ation bound (7), the equality (13) holds, as desired.

The partition introduced in Proposition 4 requires solving
2nz linear programs, which may quickly become computa-
tionally intractable in practice. Despite this limitation, the
result provides two major theoretical implications. First, it
shows that, using the input partitioning methodology pre-
sented in this paper, the robustness certiﬁcation problem
can be solved exactly via a ﬁnite number of linear program
subproblems. Second, Proposition 4 provides a starting point
to answer the following question: If the input uncertainty set
is to be partitioned into only two parts, what is the optimal
partition to choose? The proposition gives insight into the
structure of an optimal partition, namely that it is deﬁned
by intersections of the half-spaces generated by the rows of
W (see Fig. 3). Motivated by this structure, we develop an
optimal two-part partitioning scheme in the next section.

wi

zi

ReLU(ˆzi)

X (1)
i

X (2)
i

li

0

ˆzi

ui

Fig. 3. Partitioning based on row w⊤
i of the weight matrix. This partition
results in an exact ReLU constraint in coordinate i over the two resulting
input parts X (1)

i = {x ∈ X : w⊤

i x ≥ 0} and X (2)

i = X \ X (1)

i

.

B. Optimal Partitioning Scheme

To derive an optimal partitioning scheme, we ﬁrst bound

the relaxation error in the worst-case sense.

Theorem 1 (Worst-case relaxation bound). Consider a one-
layer feedforward neural network with the input uncertainty
set X and preactivation bounds l, u ∈ Rnz . Consider also
the relaxation error ∆f ∗(X ) := ˆf ∗(X ) − f ∗(X ). Assume
that there exists x∗ ∈ X such that (x∗, ˜z∗) and (x∗, z∗) are
optimal solutions for the relaxation ˆf ∗(X ) and the unrelaxed
problem f ∗(X ), respectively. Then, it holds that

nz

∆f ∗(X ) ≤ −

ReLU(ci)

uili
ui − li

.

(14)

i=1
X
Proof. The deﬁnitions of (x∗, ˜z∗) and (x∗, z∗) give that

∆f ∗(X ) =

nz

i=1
X

where

ci(˜z∗

i − z∗

i ) ≤

∆f ∗
i ,

(15)

nz

i=1
X

∆f ∗

i = sup{ci(˜zi − zi) : zi = ReLU(w⊤

i x), ˜zi ≥ 0,

˜zi ≥ w⊤

i x, ˜zi ≤

(w⊤

i x − li), x ∈ X }

ui
ui − li

for all i ∈ {1, 2, . . . , nz}. For ci ≥ 0, it holds that

∆f ∗

i = ci sup{˜zi − zi : zi = ReLU(w⊤

i x), ˜zi ≥ 0,

ui
ui − li

(w⊤

i x − li), x ∈ X }

˜zi ≥ w⊤

i x, ˜zi ≤

= − ci

uili
ui − li

,

where the ﬁnal equality is readily shown by computing the
maximum difference between the line ˜zi = ui
(ˆzi − li)
ui−li
and the function zi = ReLU(ˆzi) over ˆzi ∈ [li, ui]. On the
other hand, for ci < 0, we have that

∆f ∗

i = ci inf{˜zi − zi : zi = ReLU(w⊤
i x, ˜zi ≤

˜zi ≥ w⊤

(w⊤

ui
ui − li

i x), ˜zi ≥ 0,

i x − li), x ∈ X }

= 0,

where the ﬁnal equality is due to the fact that ˜zi ≥ zi on the
above feasible set and ˜zi = zi = 0 is feasible. Substituting
these results into (15) gives the desired bound (14).

The value ∆f ∗

i can be interpreted as the worst-case relax-
ation error in coordinate i. From this perspective, Theorem 1
provides an upper bound on the overall worst-case relaxation
error. The error bound scales linearly as the input uncertainty
set is made smaller, as shown in Corollary 1 that follows.

Corollary 1 (Linear scaling of relaxation error). Under the
settings of Theorem 1, consider an input uncertainty subset
˜X ⊆ X such that its associated preactivation bounds are
scaled inward by a factor of α ∈ (0, 1), namely ˜u = αu and
˜l = αl. Then, the worst-case relaxation bound over ˜X also
scales by α, i.e.,

ˆf ∗( ˜X ) − f ∗(X ) ≤ −α

ReLU(ci)

nz

uili
ui − li

.

(16)

Proof. Since ˜X ⊆ X ,
Therefore, by Theorem 1 on ˜X we have that

it holds that f ∗( ˜X ) ≤ f ∗(X ).

ˆf ∗( ˜X ) − f ∗(X ) ≤ ˆf ∗( ˜X ) − f ∗( ˜X )

nz

≤ −

ReLU(ci)

˜ui˜li
˜ui − ˜li

.

i=1
X

i=1
X

Substituting ˜ui = αui and ˜li = αli completes the proof.

We now focus on developing an optimal two-part parti-
tioning scheme based on the worst-case relaxation bound of
Theorem 1. We start with the following lemma.

Lemma 2 (Two-part partition bound). Under the settings
of Theorem 1, let i ∈ {1, 2, . . . , nz} and consider a two-
, X (2)
part partition of X given by {X (1)
i =
i = X \ X (1)
i x ≥ 0} and X (2)
{x ∈ X : w⊤
. Consider
, X (2)
also the partitioned relaxation error ∆f ∗({X (1)
i }) :=
maxj∈{1,2}

) − f ∗(X ). It holds that

i }, where X (1)

ˆf ∗(X (j)
i

i

i

i

∆f ∗({X (1)

i

, X (2)

i }) ≤ −

ReLU(ck)

uklk
uk − lk

.

(17)

nz

k=1
X
k6=i

i

Proof. Consider the relaxation solved over the ﬁrst input
part, X (1)
, and denote by l(1), u(1) ∈ Rnz the corresponding
preactivation bounds. Since w⊤
i x ≥ 0 on this input part,
the preactivation bounds for the ﬁrst subproblem ˆf ∗(X (1)
)
can be taken as l(1) = (l1, l2, . . . , li−1, 0, li+1, . . . , lnz ) and
u(1) = u. It follows from Theorem 1 that

i

ˆf ∗(X (1)
i

) − f ∗(X (1)

i

) ≤ −

ReLU(ck)

nz

k=1
X
nz

= −

ReLU(ck)

k=1
X
k6=i

k

u(1)
k l(1)
k − l(1)
u(1)
uklk
uk − lk

k

.

i

Similarly, over the second input part, X (2)
, we have that
w⊤
i x < 0, and so the preactivation bounds for the second
subproblem ˆf ∗(X (2)
) can be taken as l(2) = l and u(2) =
(u1, u2, . . . , ui−1, 0, ui+1, . . . , unz ), resulting in the same
bound: ˆf ∗(X (2)
ReLU(ck) uklk
.
uk−lk
Putting these two bounds together and using the fact that
f ∗(X (j)
) ≤ f ∗(X ) for all j ∈ {1, 2}, we ﬁnd that
i

) − f ∗(X (2)

nz
k=1
k6=i

) ≤ −

P

i

i

i

∆f ∗({X (1)

i

, X (2)

i }) = max
j∈{1,2}

ˆf ∗(X (j)
i

) − f ∗(X )

(cid:16)

ˆf ∗(X (j)
i

(cid:17)
) − f ∗(X (j)

i

)

(cid:16)
ReLU(ck)

(cid:17)

uklk
uk − lk

,

≤ max
j∈{1,2}
nz

≤ −

k=1
X
k6=i

as desired.

With the partitioned relaxation error bound of Lemma 2
established, we now present the optimal two-part partition.

Theorem 2 (Optimal partition). Consider the two-part parti-
tions deﬁned by the rows of W : {X (1)
i =
i x ≥ 0} and X (2)
{x ∈ X : w⊤
for all
i ∈ {1, 2, . . . , nz} =: I. The optimal partition that minimizes
the worst-case relaxation error in (17) is given by

, X (2)
= X \ X (1)

i }, where X (1)

i

i

i

i∗ ∈ arg min
i∈I

ReLU(ci)

uili
ui − li

.

(18)

Proof. Minimizing the bound in (17) of Lemma 2 over the
partition i gives rise to

min
i∈I 


= −

nz

−

ReLU(ck)

k=1
X
k6=i

nz

ReLU(ck)

uklk
uk − lk 


+ min
i∈I

uklk
uk − lk

k=1
X

as desired.

ReLU(ci)

uili
ui − li

,

Theorem 2 provides a methodical way of selecting the
optimal two-part partition based on the rows of W in a worst-
case sense. To understand the efﬁciency of this result, notice
that the optimization over i scales linearly with the dimension
nz, and the resulting LP subproblems require the addition

of only one extra linear constraint. Finally, we note that
Theorem 2 can be immediately extended in two ways. First,
by ordering the values ReLU(ci) uili
in (18), we can choose
ui−li
the best np > 1 rows to partition along in order to perform
a 2np-part partition. The other application of Theorem 2 is
the following recursion: solve the two-part partitioned LP
using Theorem 2 to partition X . If X (j∗
is the input part
i∗
containing the solution x∗, then partition X (j∗
into two
i∗
smaller sub-parts again according to Theorem 2 and solve
the reﬁned partitioned LP over the sub-parts. Continuing
this nested procedure results in tightened localization of a
true worst-case input (i.e., a solution to f ∗(X )) and further
reduces conservatism of the partitioned LP relaxation.

)

)

VI. SIMULATION RESULTS

Consider a one-layer classiﬁcation network with four
inputs and three outputs, trained on the celebrated Iris data
set [28] with a test accuracy of 97%. A negative optimal ob-
jective value in the robustness certiﬁcation problem indicates
that any perturbation in X = {x ∈ Rnx : kx − ¯xk∞ ≤ ǫ} of
the nominal input ¯x will not change the input’s classiﬁcation.
For this experiment, we solve the certiﬁcation problem for
10 different nominal inputs using MATLAB and CVX on a
Windows 7 laptop with a 2.9 GHz quad-core i7 processor.
We ﬁrst solve the problem in its nonconvex form (using
multistart and MATLAB’s fmincon function). We then solve
the problem using an unpartitioned LP relaxation, and then
using partitioned LP relaxations, one per row of W . We
restrict our experiments to two-part partitions to explore the
effect of changing the row of W along which we partition.
The average time taken to solve the nonconvex, unpartitioned
LP, and partitioned LPs are 0.21, 0.26, and 0.48 seconds,
respectively. As expected, the computational burden of the
two-part partitioned LP is twice that of the unpartitioned LP,
both of which are very fast for this network.

The optimal objective values for each nominal input are
shown in Fig. 4a. As seen, the optimally partitioned LP
developed in Theorem 2 yields the best convex upper bound
on the true problem. Furthermore, ordering the rows w⊤
i
by their suboptimality in (18) corresponds to the order of
relaxation tightening. For instance, in Fig. 4, we compare
the LP partitioned via i1 ∈ arg mini∈I\{i∗} ReLU(ci) uili
ui−li
(suboptimally partitioned LP 1) and that partitioned via i2 ∈
arg mini∈I\{i∗,i1} ReLU(ci) uili
(suboptimally partitioned
ui−li
LP 2). In this example, the suboptimally partitioned LP
2 (partitioned along worst row w⊤
i2 ) coincides with the
unpartitioned LP, suggesting that none of the relaxation error
is attributed to the ith
2 coordinate of the ReLU layer. The
suboptimally partitioned LPs do not certify robustness, as the
objective values are positive for each nominal input tested.
On the other hand, the developed optimal partitioning scheme
tightens the relaxation enough to provide a certiﬁcate of
robustness for the one-layer network corresponding to every
nominal input tested here. For the same experiment on a two-
layer network (with an added ﬁve-neuron ReLU layer), we
ﬁnd that the optimal partitioning scheme maintains the best
convex upper bound, albeit without guaranteed relaxation

error bounds (see Fig. 4b). The average computation times
for the nonconvex, unpartitioned LP, and partitioned LPs rise
to 0.94, 0.68, and 1.48 seconds, respectively. For general
network sizes,
the two-part partitioned LP maintains the
polynomial-time complexity of linear programming with
respect to the number of neurons, since it requires solving
two instances of the same LP structure [29].

e
u
l
a
V
e
v
i
t
c
e
j
b
O

l
a
m

i
t
p
O

1

0

−1

2

1

0

−1

e
u
l
a
V
e
v
i
t
c
e
j
b
O

l
a
m

i
t
p
O

10

4

2
Nominal Input Number

6

8

10

4

2
Nominal Input Number

8

6

(a) One-layer network.

(b) Two-layer network.

Nonconvex problem via multistart
Unpartitioned LP
Upper bound for optimally partitioned LP
Optimally partitioned LP
Suboptimally partitioned LP 1
Suboptimally partitioned LP 2

Fig. 4. Optimal values of robustness certiﬁcation for ReLU Iris classiﬁer.
For the two-layer network, the optimal partitioning scheme is applied to
only the ReLU constraints of the ﬁnal layer.

VII. CONCLUSIONS

In this work, we develop a partition-based method for
ReLU neural network robustness certiﬁcation that system-
atically reduces relaxation error while maintaining the efﬁ-
ciency of linear programming. We theoretically justify the ef-
fectiveness of partitioning and derive an optimal partitioning
scheme. A case study on real data shows that the proposed
method is able to certify the robustness of a network while
the existing methods fail. Our results demonstrate, both theo-
retically and experimentally, that partition-based certiﬁcation
procedures are capable of tightening relaxation errors with
remarkable simplicity.

REFERENCES

[1] S. N. Kumpati, P. Kannan et al., “Identiﬁcation and control of
dynamical systems using neural networks,” IEEE Transactions on
neural networks, vol. 1, no. 1, pp. 4–27, 1990.

[2] F. Lewis, S. Jagannathan, and A. Yesildirak, Neural network control
of robot manipulators and non-linear systems. CRC Press, 1998.
[3] G. P. Liu, Nonlinear identiﬁcation and control: a neural network

approach. Springer Science & Business Media, 2012.

[4] S. Bansal, A. K. Akametalu, F. J. Jiang, F. Laine, and C. J. Tomlin,
“Learning quadrotor dynamics using neural network for ﬂight control,”
in 2016 IEEE 55th Conference on Decision and Control (CDC).
IEEE, 2016, pp. 4653–4660.

[5] E. Yeung, S. Kundu, and N. Hodas, “Learning deep neural network
representations for koopman operators of nonlinear dynamical sys-
tems,” in 2019 American Control Conference (ACC).
IEEE, 2019,
pp. 4832–4839.

[6] D. H. Nguyen and B. Widrow, “Neural networks for self-learning
control systems,” IEEE Control systems magazine, vol. 10, no. 3, pp.
18–23, 1990.

[7] E. N. Johnson and A. J. Calise, “Neural network adaptive control of
systems with input saturation,” in Proceedings of the 2001 American
Control Conference.(Cat. No. 01CH37148), vol. 5.
IEEE, 2001, pp.
3527–3532.

[8] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp,
P. Goyal, L. D. Jackel, M. Monfort, U. Muller, J. Zhang et al., “End to
end learning for self-driving cars,” arXiv preprint arXiv:1604.07316,
2016.

[9] B. Wu, F. Iandola, P. H. Jin, and K. Keutzer, “Squeezedet: Uniﬁed,
small, low power fully convolutional neural networks for real-time
object detection for autonomous driving,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition Workshops,
2017, pp. 129–137.

[10] W. Kong, Z. Y. Dong, Y. Jia, D. J. Hill, Y. Xu, and Y. Zhang, “Short-
term residential
load forecasting based on LSTM recurrent neural
network,” IEEE Transactions on Smart Grid, vol. 10, no. 1, pp. 841–
851, 2017.

[11] K. Muralitharan, R. Sakthivel, and R. Vishnuvarthan, “Neural network
based optimization approach for energy demand prediction in smart
grid,” Neurocomputing, vol. 273, pp. 199–208, 2018.

[12] X. Pan, T. Zhao, and M. Chen, “Deepopf: Deep neural network for
dc optimal power ﬂow,” in 2019 IEEE International Conference on
Communications, Control, and Computing Technologies for Smart
Grids (SmartGridComm).

IEEE, 2019, pp. 1–6.

[13] E. Wong and J. Z. Kolter, “Provable defenses against adversarial
examples via the convex outer adversarial polytope,” arXiv preprint
arXiv:1711.00851, 2017.

[14] A. Raghunathan, J. Steinhardt, and P. S. Liang, “Semideﬁnite relax-
ations for certifying robustness to adversarial examples,” in Advances
in Neural Information Processing Systems, 2018, pp. 10 877–10 887.
[15] W. Xiang and T. T. Johnson, “Reachability analysis and safety
veriﬁcation for neural network control systems,” arXiv preprint
arXiv:1805.09944, 2018.

[16] T.-W. Weng, H. Zhang, H. Chen, Z. Song, C.-J. Hsieh, D. Boning,
I. S. Dhillon, and L. Daniel, “Towards fast computation of certiﬁed
robustness for relu networks,” arXiv preprint arXiv:1804.09699, 2018.
[17] H. Zhang, T.-W. Weng, P.-Y. Chen, C.-J. Hsieh, and L. Daniel, “Efﬁ-
cient neural network robustness certiﬁcation with general activation
functions,” in Advances in neural information processing systems,
2018, pp. 4939–4948.

[18] M. Fazlyab, M. Morari, and G. J. Pappas, “Safety veriﬁcation and
robustness analysis of neural networks via quadratic constraints and
semideﬁnite programming,” arXiv preprint arXiv:1903.01287, 2019.
[19] V. R. Royo, R. Calandra, D. M. Stipanovic, and C. Tomlin,
“Fast neural network veriﬁcation via shadow prices,” arXiv preprint
arXiv:1902.07247, 2019.

[20] M. Jin, H. Chang, W. Zhu, and S. Sojoudi, “Power up! robust
graph convolutional network against evasion attacks based on graph
powering,” arXiv preprint arXiv:1905.10029, 2019.

[21] A. Ben-Tal, L. El Ghaoui, and A. Nemirovski, Robust optimization.

Princeton University Press, 2009, vol. 28.

[22] D. Bertsimas, D. B. Brown, and C. Caramanis, “Theory and applica-
tions of robust optimization,” SIAM review, vol. 53, no. 3, pp. 464–501,
2011.

[23] G. Katz, C. Barrett, D. L. Dill, K. Julian, and M. J. Kochenderfer, “Re-
luplex: An efﬁcient smt solver for verifying deep neural networks,” in
International Conference on Computer Aided Veriﬁcation. Springer,
2017, pp. 97–117.

[24] D. Bertsimas and I. Dunning, “Multistage robust mixed-integer opti-
mization with adaptive partitions,” Operations Research, vol. 64, no. 4,
pp. 980–998, 2016.

[25] G. F. Montufar, R. Pascanu, K. Cho, and Y. Bengio, “On the number
of linear regions of deep neural networks,” in Advances in neural
information processing systems, 2014, pp. 2924–2932.

[26] M. A. Duran and I. E. Grossmann, “An outer-approximation algo-
rithm for a class of mixed-integer nonlinear programs,” Mathematical
programming, vol. 36, no. 3, pp. 307–339, 1986.

[27] O. L. Mangasarian and T.-H. Shiau, “Lipschitz continuity of solutions
of linear inequalities, programs and complementarity problems,” SIAM
Journal on Control and Optimization, vol. 25, no. 3, pp. 583–595,
1987.

[28] R. A. Fisher, “The use of multiple measurements in taxonomic
problems,” Annals of eugenics, vol. 7, no. 2, pp. 179–188, 1936.
[29] N. Karmarkar, “A new polynomial-time algorithm for linear program-
ming,” in Proceedings of the sixteenth annual ACM symposium on
Theory of computing, 1984, pp. 302–311.

