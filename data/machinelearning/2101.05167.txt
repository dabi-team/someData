1
2
0
2

n
a
J

3
1

]

C
O
.
h
t
a
m

[

1
v
7
6
1
5
0
.
1
0
1
2
:
v
i
X
r
a

A Sublevel Moment-SOS Hierarchy for Polynomial
Optimization

CHEN, Tong ∗
tchen@laas.fr

LASSERRE, Jean-Bernard ∗†
lasserre@laas.fr

MAGRON, Victor ∗†
vmagron@laas.fr

PAUWELS, Edouard ‡ †
edouard.pauwels@irit.fr

January 14, 2021

Abstract

We introduce a sublevel Moment-SOS hierarchy where each SDP relaxation can be viewed
as an intermediate (or interpolation) between the d-th and (d + 1)-th order SDP relaxations of
the Moment-SOS hierarchy (dense or sparse version).

With the ﬂexible choice of determining the size (level) and number (depth) of subsets in the
SDP relaxation, one is able to obtain diﬀerent improvements compared to the d-th order relax-
ation, based on the machine memory capacity. In particular, we provide numerical experiments
for d = 1 and various types of problems both in combinatorial optimization (Max-Cut, Mixed
Integer Programming) and deep learning (robustness certiﬁcation, Lipschitz constant of neural
networks), where the standard Lasserre’s relaxation (or its sparse variant) is computationally
intractable. In our numerical results, the lower bounds from the sublevel relaxations improve
the bound from Shor’s relaxation (ﬁrst order Lasserre’s relaxation) and are signiﬁcantly closer
to the optimal value or to the best-known lower/upper bounds.

1

Introduction

Consider the polynomial optimization problem (POP) of the following form:

f ∗ := inf
x∈Rn

{f (x) : gi(x) ≥ 0, i = 1, . . . , p} ,

(POP)

where f and gi are polynomials in variable x for all i = 1, . . . , p. Lasserre’s hierarchy [9] is a
well-known method based on semideﬁnite programming (SDP) to approximate the optimal value of
(POP), by solving a sequence of SDPs that provide a series of lower bounds and converges to the
optimal value of the original problem. Under certain assumptions, such convergence is shown to be
ﬁnite [18].

∗LAAS-CNRS, BP 54200, 7 avenue du Colonel Roche, 31031 Toulouse, C´edex 4, France.
†IMT, Universit´e Toulouse 3 Paul Sabatier.
‡IRIT, Universit´e de Toulouse, CNRS.

1

 
 
 
 
 
 
Related works Other related frameworks of relaxations, including DSOS [17] based on linear
programming (LP), SDSOS [17] based on second-order cone programming (SOCP), and the hybrid
BSOS [11] combining the features of LP and SDP hierarchies, also provide lower bounds converging
to the optimal value of a POP. Generally speaking, when comparing LP and SDP solvers, the former
can handle problems of much larger size. On the other hand, the bounds from LP relaxations
are signiﬁcantly weaker than those obtained by SDP relaxations, in particular for combinatorial
problems [13]. Based on the standard Lasserre’s hierarchy, several further works have explored
various types of sparsity patterns inside POPs to compute lower bounds more eﬃciently and handle
larger-scale POPs. The ﬁrst such extension can be traced back to Waki [27] and Lasserre [10]
where the authors consider the so-called correlative sparsity pattern (CSP) with associated CSP
graph whose nodes consist of the POP’s variables. Two nodes in the CSP graph are connected
via an edge if the two corresponding variables appear in the same constraint or in same monomial
of the objective. The standard sparse Lasserre’s hierarchy splits the full moment and localizing
matrices into several smaller submatrices, according to subsets of nodes (maximal cliques) in a
chordal extension of the CSP graph associated with the POP. When the size of the largest clique
(a crucial parameter of the sparsity pattern) is reasonable the resulting SDP relaxations become
tractable. There are many successful applications of the resulting sparse moment-SOS hierarchy,
including certiﬁed roundoﬀ error bounds [15, 14], optimal power ﬂow [6], volume computation of
sparse semialgebraic sets [26], approximating regions of attractions of sparse polynomial systems
[24, 25], noncommutative POPs [7], sparse positive deﬁnite functions [16]. Similarly, the sparse
BSOS hierarchy [33] is a sparse version of BSOS for large scale polynomial optimization.

Besides correlative sparsity, recent developments [32, 31] exploit the so-called term sparsity
(TSSOS) or combine correlative sparsity and term sparsity (CS-TSSOS) [30] to handle large scale
polynomial optimization problems. The TSSOS framework relies on a term sparsity pattern (TSP)
graph whose nodes consist of monomials of some monomial basis. Two nodes in a TSP graph are
connected via an edge when the product of the corresponding monomials appears in the supports of
polynomials involved in the POP or is a monomial of even degree. Extensions have been provided to
compute more eﬃciently approximations of joint spectral radii [28] and minimal traces or eigenvalue
of noncommutative polynomials [29]. More variants of the sparse moment-SOS hierarchy have been
built for quantum bounds on Bell inequalities [21], condensed-matter ground-state problems [1],
quantum many-body problems [5], where one selects a certain subset of words (noncommutative
monomials) to decrease the number of SDP variables.

Recently, in [2] the authors proposed a partial and augmented partial relaxation tailored to
the Max-Cut problem.
It strengthens Shor relaxation by adding some (and not all) constraints
from the second-order Lasserre’s hierarchy. The same idea was already used in the multi-order
SDP relaxation of [6] for solving large-scale optimal power ﬂow (OPF) problems. The authors set a
threshold for the maximal cliques and include the second-order relaxation constraints for the cliques
with size under the threshold and the ﬁrst-order relaxation constraints for the cliques with size over
the threshold.

Contribution This work is in the line of research concerned with extensions and/or variants of
the Moment-SOS hierarchy so as to handle large-scale POPs out of reach by the standard hierarchy.
We provide a principled way to obtain intermediate alternative SDP relaxations between the ﬁrst-
and second-order SDP relaxations of the Moment-SOS hierarchy for general POPs. It encompasses
It can also be
the above cited works [6, 2] as special cases for MAX-Cut and OPF problems.
generalized to provide intermediate alternative SDP relaxations between (arbitrary) order-d and

2

order-d + 1 relaxations of the Moment-SOS hierarchy when the order-d + 1 relaxation is too costly
to implement.

We develop what we call the sublevel hierarchy based on the standard Moment-SOS hierarchy.
Compared with existing sparse variants of the latter, we propose several possible SDP relaxations
to improve lower bounds for general POPs.

The basic principle is quite simple. In the sublevel hierarchy concerned with d-th and (d+ 1)-th
orders of the sparse Moment-SOS hierarchy, from the maximal cliques of a chordal extension of the
csp graph, we further select several subsets of nodes (variables). Then in the d-th sparse SDP
relaxation we also include (d + 1)-th order moment and localizing matrices w.r.t. these subsets only.
This methodology reveals helpful if the bound obtained by the d-th order relaxation of a POP is
not satisfactory and if one is not able to solve the (d + 1)-th order relaxation.

One important distinguished feature of the sublevel hierarchy is to not be restricted to POPs
with a correlative sparsity pattern. Indeed it can also be applied to dense POPs or nearly-dense
POPs where the problem is sparse except that there are a few dense constraints. As a result we are
able to improve bounds obtained at the ﬁrst-order relaxation (also called Shor’s relaxation). In [3]
we proposed a heuristic method to deal with general nearly-dense POPs as a trade-oﬀ between the
ﬁrst-order and second-order relaxations of the Moment-SOS hierarchy. As we will see the heuristic
[3] is also a special case of the sublevel hierarchy.

Another feature of the sublevel hierarchy is that we have more ﬂexible ways to tune the resulting
relaxation instead of simply increasing the relaxation order (a rigid and costly strategy). More
speciﬁcally, there are two hyper-parameters in the sublevel hierarchy: (i) the size of the selected
subsets, and (ii) the number of such subsets. Suppose m is the size of a maximal clique of a chordal
extension of the csp graph. Then we can choose q (called the depth) many subsets of size l (called
the level) with 1 ≤ l ≤ m and q ≤
. For each maximal clique, we have a wide range of choices for
the level and depth, yielding a good trade-oﬀ between the solution accuracy and the computational
eﬃciency.

m
l

(cid:0)

(cid:1)

The outline of the paper is as follows: Section 2 introduces some preliminaries of dense and sparse
Lasserre’s hierarchy; Section 3 is the theoretical part of the sublevel hierarchy and the sublevel
relaxation; Section 4 explicitly illustrates the sublevel relaxation for several type of optimization
problems; Section 5 shows the results of sublevel relaxation applied on the problems discussed in
Section 4.

2 Preliminary background on Lasserre’s hierarchy

In this section we brieﬂy introduce the Lasserre’s hierarchy [9] which has already many successful
applications in and outside optimization [12]. First let us recall some notations in polynomial
optimization. Given a positive integer n ∈ N, let x = [x1, . . . , xn]T be a vector of decision variables
and R[x] be the space of real polynomials in variable x. For a set I ⊆ {1, 2, . . . , n}, let xI := [xi]i∈I
and let R[xI ] be the space of real polynomials in variable xI . Denote by R[x] (resp. R[x]d) the vector
space of polynomials (resp. of degree at most d) in variable x; P[x] ⊆ R[x] (resp. Pd[x] ⊆ R[x]2d)
the convex cone of nonnegative polynomials (resp. nonnegative polynomials of degree at most 2d)
in variable x; Σ[x] ⊆ P[x] (resp. Σ[x]d ⊆ Pd[x]) the convex cone of SOS polynomials (resp. SOS
polynomials of degree at most 2d) in variable x.

3

In the context of optimization, Lasserre’s hierarchy allows one to approximate the global op-
timum of (POP), by solving a hierarchy of SDPs of increasing size. Each SDP is a semideﬁnite
relaxation of (POP) in the form:

ρdense
d

= inf
y

{ Ly(f ) : Ly(1) = 1, Md(y) (cid:23) 0,

Md−ωi(giy) (cid:23) 0, i = 1, . . . , p} ,

(Mom-d)

where ωi = ⌈deg(gj)/2⌉, y = (yα)α∈Nn

2d , Ly : R[x] → R is the so-called Riesz linear functional :

f =

fα xα 7→ Ly(f ) :=

fα yα,

f ∈ R[x],

Xα

Xα

and Md(y), Md−ωi(giy) are moment matrix and localizing matrix respectively; see [12] for precise
deﬁnitions and more details. The semideﬁnite program (Mom-d) is the d-th order moment relaxation
of problem (POP). As a result, when the semialgebraic set K := {x : gi(x) ≥ 0, i = 1, . . . , p} is
compact, one obtains a monotone sequence of lower bounds (ρd)d∈N with the property ρd ↑ f ∗ as
d → ∞ under a certain technical Archimedean condition; the latter is easily satisﬁed by including
a redundant quadratic constraint M − kxk2 ≥ 0 for some well-chosen M > 0 in the deﬁnition of
K (redundant as K is compact and M is large enough). At last but not least and interestingly,
Ideally, one expects an optimal solution y∗ of
generically the latter convergence is ﬁnite [18].
(Mom-d) to be the vector of moments up to order 2d of the Dirac measure δx∗ at a global minimizer
x∗ of (POP).

The hierarchy (Mom-d) is often referred to as dense Lasserre’s hierarchy since we do not exploit
any possible sparsity pattern of the POP. Therefore, if one solves (Mom-d) with interior point
methods (as current SDP solvers usually do), then the dense hierarchy is limited to POPs of
modest size.
variables
= O(nd) at ﬁxed d. Fortunately, large-scale POPs
and a moment matrix Md(y) of size
often exhibit some structured sparsity patterns which can be exploited to yield a sparse version of
(Mom-d), as initially demonstrated in [27]. As a result, wider applications of Lasserre’s hierarchy
have been possible.

Indeed the d-th order dense moment relaxation (Mom-d) involves

n+2d
2d

n+d
d

(cid:0)

(cid:1)

(cid:0)

(cid:1)

Assume that the set of variables in (POP) can be divided into r several subsets indexed by Ik,

for k ∈ {1, . . . , r}, i.e., {1, . . . , n} = ∪r

k=1Ik. Suppose that the following assumptions hold:

A1: The function f is a sum of polynomials, each summand involving variables of only one

subset, i.e., f (x) =

r
k=1 fk(xIk );

k(i) ∈ {1, · · · , r};

Ik+1 ∩

k
j=1 Ij ⊆ Is, for some s ≤ k.

A2: Each constraint also involves variables of only one subset, i.e., gi ∈ R[xIk(i) ] for some

P

A3: The subsets Ik satisfy the Running Intersection Property (RIP): for every k ∈ {1, · · · , r−1},

S

It turns out that the maximal cliques in the chordal extension of the csp graph induced by the
POP satisfy the RIP [27]. From now on, we will call the these subsets cliques, in order to distinguish
from the subsets in the sublevel hierarchy that will be discussed in the next section. A POP with
a sparsity pattern is of the form:

inf
x∈Rn

{f (x) : gi(xIk ) ≥ 0, i = 1, . . . , p; i ∈ Ik} ,

(SpPOP)

4

and its associated sparse Lasserre’s hierarchy reads:

ρsparse
d

= inf
y

{Ly(f ) : Ly(1) = 1, Md(y, Ik) (cid:23) 0, k ∈ {1, · · · , r},

Md−ωi(gi y, Ik) (cid:23) 0 , i ∈ {1, · · · , p}; i ∈ Ik } ,

(SpMom-d)

where d, ωi, y, Ly are deﬁned as in (Mom-d) but with a crucial diﬀerence. The matrix Md(y, Ik)
localizing matrix
(resp. Md−ωi(gi y, Ik)) is a submatrix of the moment matrix Md(y) (resp.
τk+d
Md−ωi(giy)) with respect to the clique Ik, and hence of much smaller size
if |Ik| =: τk ≪ n.
τk
Finally, ρsparse
≤ f ∗ for all d and moreover, if the cliques Ik satisfy the RIP, then we still obtain
d
the convergence ρsparse

(cid:0)
↑ f ∗ as d → ∞, as for the dense relaxation (Mom-d).

(cid:1)

d

Finally, for each ﬁxed d, the dual of (Mom-d) reads:

{t : f − t = θ +

sup
t∈R

p

Xi=1

σigi} ,

(SOS-d)

where θ is a sum-of-squares (SOS) polynomial in R[x] of degree at most 2d, and σj are SOS
polynomials in R[x] of degree at most 2(d − ωi) with ωi = ⌈deg(gj)/2⌉. The right-hand-side of
the identity in (SOS-d) is nothing less than Putinar’s positivity certiﬁcate [20] for the polynomial
x 7→ f (x) − t on the compact semialgebraic set K.

Similarly, the dual problem of (SpMom-d) reads:

{t : f − t =

sup
t∈R

m

θk +

σi,kgi

} ,
(cid:1)

(SpSOS-d)

Xk=1 (cid:0)
where θk is an SOS in R[xIk ] of degree at most 2d, and σi,k is an SOS in R[xIk ] of degree at most
2(d − ωi) with ωi = ⌈deg(gi)/2⌉, for each k = 1, . . . , p. Then (SpSOS-d) implements the sparse
Putinar’s positivity certiﬁcate [10, 27].

Xi∈Ik

Example 1 Let x ∈ R6, x1:4 := [xi]4
semialgebraic set deﬁned by g1(x) = 1 − ||x1:4||2
second-order dense Lasserre’s relaxation reads

i=1, x3:6 := [xi]6

i=3. We minimize f (x) = −||x||2

2 ≥ 0 and g2(x) = 1 − ||x3:6||2

2, under the
2 ≥ 0. Then, the

{t : f (x) − t = θ(x) + σ1(x)g1(x) + σ2(x)g2(x)}

sup
t∈R

where θ is a degree-4 SOS polynomial in variable x, σ1, σ2 are degree-2 SOS polynomials in variable
x. Deﬁne I1 = {1, 2, 3, 4} and I2 = {3, 4, 5, 6}, then g1 ∈ R[xI1 ] and g2 ∈ R[xI2 ]. The second-order
sparse Lasserre’s relaxation reads

{t : f (x) − t =

sup
t∈R

θ1(xI1 ) + σ1(xI1 )g1(x)
(cid:1)
(cid:0)

+

θ2(xI2 ) + σ2(xI2 )g2(x)
(cid:1)
(cid:0)

}

where θk is a degree-4 SOS polynomials in variable xIk , and σk is a degree-2 SOS polynomials in
variable xIk , for eac k = 1, 2.

3 Sublevel hierarchy

As seen in Section 2, the way to reduce the size of the moment and localizing matrices in (Mom-d) is
either by reducing the relaxation order or the number of variables/terms in the SOS weights involved

5

in the Putinar’s representation. The authors from [6] propose the multi-order Lasserre’s hierarchy
to deal with large-scale optimal power ﬂow problems. In this hierarchy, one reduces the relaxation
order with respect to the constraints with large number of variables. This approach is reused as
the so-called partial relaxation to solve Max-Cut problems in [2]. The authors in [2] also proposed
the augmented partial relaxation as an extended version of the partial relaxation, to improve the
bounds further.
In this section, we develop the sublevel hierarchy which is a generalization of
several existing frameworks for both sparse and non-sparse POPs, and show that in the case of
Max-Cut problems, the partial and augmented partial relaxation can be cast as special instances
of the sublevel relaxation.

3.1 Deriving the sublevel hierarchy

p

For problem (POP), the d-th order dense Lasserre’s relaxation relates to the Putinar’s certiﬁcate
i=1 σigi where σ0 is an SOS in R[x] of degree at most 2d and σi are SOS in R[x]
f − t = σ0 +
of degree at most 2(d − ωi) with ωi = ⌈deg(gi)/2⌉. In this section, we are going to choose some
subsets of the variable x to decrease the number of terms involved in the SOS multipliers σ0 and σi,
and deﬁne the intermediate sublevel hierarchies between the d-th and (d + 1)-th order relaxations.
Note that in the dense variant of Lasserre’s hierarchy, one approximates the cone of positive

P

polynomials from the inside with the following hierarchy of SOS cones:

R = Σ[x]0 ⊆ Σ[x]1 ⊆ . . . ⊆ Σ[x]

+∞
d=0 Σ[x]d = Σ[x]. Similarly, in the sparse variant, one relies on the following hierarchy of

with
direct sums of SOS cones:

S

R = ⊕kΣ[xIk ]0 ⊆ ⊕kΣ[xIk ]1 ⊆ . . . ⊆ ⊕Σ[xIk ]

with

+∞
d=0(⊕kΣ[xIk ]d) = ⊕kΣ[xIk ].

S

Deﬁnition 1 (Sublevel hierarchy of SOS cones) Let n be the number of variables in (POP).
For d ≥ 1 and 0 ≤ l ≤ n, the l-th level SOS cone associated to Σ[x]d, denoted by Σ[x]l
d, is an SOS
cone lying between Σ[x]d and Σ[x]d+1, which is deﬁned as

Σ[x]d ⊆ Σ[x]l

d := Σ[x]d + ˜Σ[x]l

d+1 ⊆ Σ[x]d+1

where ˜Σ[x]l

d+1 :=

σI (xI ) : I ⊆ {1, . . . , n}, σI (xI ) ∈ Σ[xI ]d+1(cid:27)

(cid:26) X|I|=l
polynomials in ˜Σ[x]l
d+1 are the elements in Σ[x]d+1 which can be decomposed into several components
where each component is an SOS polynomial in l variables. Let us use the convention Σ[x]0
d := Σ[x]d.
Then, for the dense case, we rely on the sublevel hierarchy of inner approximations of the cone of
positive polynomials:

⊆ Σ[x]d+1, i.e., the SOS

Σ[x]d = Σ[x]0

d ⊆ Σ[x]1

d ⊆ . . . ⊆ Σ[x]n

d = Σ[x]d+1

Similarly, suppose that {Ik}1≤k≤r are the cliques of the sparse problem (SpPOP). For l ≤ τk :=

|Ik|, we deﬁne the l-th level SOS cone of Σ[xIk ]d, denoted by Σ[xIk ]l

d, as

Σ[xIk ]d ⊆ Σ[xIk ]l

d := Σ[xIk ]d + ˜Σ[xIk ]l

d+1 ⊆ Σ[xIk ]d+1

6

where ˜Σ[xIk ]l

d+1 :=

(cid:26) X|I|=l

σI (xI ) : I ⊆ Ik, σI (xI ) ∈ Σ[xI ]d+1(cid:27)

⊆ Σ[xIk ]d+1, i.e., the SOS polyno-

mials in ˜Σ[xIk ]l
d+1 are the elements in Σ[xIk ]d+1 which can be decomposed into several components
where each component is an SOS polynomial in l variables indexed by Ik. Then, for the sparse case,
we rely on the sublevel hierarchy of inner approximations of the cone of positive polynomials:

Σ[xIk ]d = Σ[xIk ]0

d ⊆ Σ[xIk ]1

d ⊆ . . . ⊆ Σ[xIk ]τk

d = Σ[xIk ]d+1

Remark 1 Lasserre’s hierarchy relies on a hierarchy of SOS cones, while the sublevel hierarchy
relies on a hierarchy of sublevel SOS cones. Take the sparse case for illustration, solving the d-th
order relaxation of the standard sparse Lasserre’s hierarchy boils down to ﬁnding SOS multipli-
k(Σ[xIk ]d ⊕ Σ[xIk ]d−ωi). Solv-
ers in the cone Σ[xIk ]d ⊕ Σ[xIk ]d−ωi for each clique Ik, i.e.,
ing the d-th order sublevel hierarchy boils down to ﬁnding SOS multipliers in the intermediate
d−ωi) for some 0 ≤ lk ≤ τk. This cone approximates the standard
cones
k(Σ[xIk ]d ⊕
cone
Σ[xIk ]d−ωi). We will see in the next deﬁnition that this is the so-called sublevel relaxation, and
we call the vector {lk} the vector of sublevels of the relaxation. Each lk determines the size of the
subsets in the clique Ik and is called a sublevel.

k(Σ[xIk ]lk
k(Σ[xIk ]d ⊕ Σ[xIk ]d−1) as lk gets larger since
L

d ⊕ Σ[xIk ]τk

d ⊕ Σ[xIk ]lk

k(Σ[xIk ]τk

d−ωi) =

L

L

L

L

Deﬁnition 2 (Sublevel hierarchy of moment-SOS relaxations) Let n be the number of vari-
ables in (POP). For each constraint gi ≥ 0 in (POP), we deﬁne a sublevel 0 ≤ li ≤ n and a depth
0 ≤ qi ≤ n. Denote by l = [li]p
i=1 the vector of sublevels and q = [qi]p
i=1 the vector of depths. Then,
the (l, q)-sublevel relaxation of the d-th order dense SOS problem (SOS-d) reads

sup
t∈R (cid:26)

t : f − t = θ0 +

p

˜θi + (σi + ˜σi)gi

Xi=1 (cid:0)

,

(cid:27)
(cid:1)

(SubSOS-[d, l, q])

d+1 (resp. ˜Σ[x]li

where θ0 (resp. σi) are SOS polynomials in Σ[x]d (resp. Σ[x]d−ωi), and ˜θi (resp. ˜σi) are SOS
polynomials in ˜Σ[x]li
d−ωi+1) with ωi = ⌈deg(gi)/2⌉). Moreover, each ˜σi is a sum of
qi SOS polynomials where each sum term involves variables in a certain subset Γi,j ⊆ {1, 2, . . . , n}
j=1 ˜σi,j where ˜σi,j ∈ Σ[xΓi,j ]d−ωi+1. Each ˜θi is also a sum of qi SOS
with |Γi,j| = li, i.e., ˜σi =
polynomials where the sum terms share the same variable sets Γi,j as ˜σi,j, i.e., ˜θi =
˜θi,j where
˜θi,j ∈ Σ[xΓi,j ]d+1. The equation (SubSOS-[d, l, q]) can be compressed as an analogical form of the
standard dense Lasserre’s relaxation:

qi
j=1

P

P

qi

sup
t∈R (cid:26)

t : f − t =

p

Xi=1

(˜θi + ˜σigi)

(cid:27)

,

where ˜θi (resp. ˜σi) are SOS polynomials in Σ[x]l

d+1 (resp. Σ[x]l

d−ωi+1).

Similarly, suppose that (Ik)1≤k≤p are the cliques of the sparse problem (SpPOP) with τk = |Ik|.
For each constraint gi ≥ 0 in (SpPOP), denote by k(i) the set of indices s such that i ∈ Is.
For each i and s ∈ k(i), deﬁne a sublevel 0 ≤ li,s ≤ τs and a depth 0 ≤ qi,s ≤ τs. Denote by
l = [li,s]i=1,...,p; s∈k(i) the vector of sublevels and q = [qi,s]i=1,...,p; si∈k(i) the vector of depths. Then,
the (l, q)-sublevel relaxation of the d-th order sparse SOS problem (SpSOS-d) reads

sup
t∈R (cid:26)

t : f − t =

m

Xk=1

θ0,k +

(cid:18)

˜θi,k + (σi,k + ˜σi,k)gi

Xi∈Ik (cid:0)

,

(cid:19)(cid:27)
(cid:1)

(SubSpSOS-[d, l, q])

7

d+1 (resp. ˜Σ[xIk ]li,k

where θ0,k (resp. σi,k) are SOS polynomials in Σ[xIk ]d (resp. Σ[xIk ]d−ωi), and ˜θ0,k (resp. ˜σi,k)
are SOS polynomials in ˜Σ[xIk ]li,k
d−ωi+1) with ωi = ⌈deg(gi)/2⌉. Moreover, each ˜σi,k
with i ∈ Ik is a sum of qi,k SOS polynomials where each sum term involves variables in a certain
qi,k
j=1 ˜σi,k,j where ˜σi,k,j ∈ Σ[xΓi,k,j ]d−ωi+1. Each
subset Γi,k,j ⊆ Ik with |Γi,k,j | = li,k, i.e., ˜σi,k =
˜θi,k is also a sum of qi,k SOS polynomials where the sum terms share the same variable sets Γi,k,j
P
˜θi,k,j where ˜θi,k,j ∈ Σ[xΓi,k,j ]d+1. The equation (SubSpSOS-[d, l, q]) can
as ˜σi,k,j , i.e., ˜θi,k =
also be compressed as an analogical form of the standard sparse Lasserre’s relaxation:

qi,k
j=1

P

sup
t∈R (cid:26)

t : f − t =

m

˜θi,k + ˜σi,kgi

Xk=1 Xi∈Ik (cid:0)

,

(cid:27)
(cid:1)

where ˜θi,k (resp. ˜σi,k) are SOS polynomials in Σ[xIk ]l

d+1 (resp. Σ[xIk ]l

d−ωi+1).

Remark 2 (i). If one of the sublevel li (resp. li,k) in the dense (resp. sparse) sublevel relaxation
is such that li = n (resp. li,k = τk), then the depth qi (resp. qi,k) should automatically be 1.

(ii). The heuristics to determine the subsets (Γi,j for the dense case and Γi,k,j for the sparse

case) in the sublevel relaxation will be discussed in the next section.

(iii). The size of the SDP Gram matrix associated to an SOS polynomial in Σ[x]l
,
(cid:1)

d (resp. Σ[xIk ]l
d)
l+d+1
is max{
}). If the lower bound obtained by solving the
d+1
SOS problem over Σ[x]d+1 (resp. Σ[xIk ]d+1) is not satisfactory enough, then we may try to ﬁnd
(cid:1)
d (resp. Σ[xIk ]l
more accurate solutions in one of the cones of Σ[x]l

} (resp. max{
(cid:1)

|Ik|+d
d

l+d+1
d+1

n+d
d

,
(cid:1)

d).

(cid:0)

(cid:0)

(cid:0)

(cid:0)

Example 2 Take the polynomials f, gk and the cliques Ik as in Example 1. Deﬁne l = [2, 2] and
q = [1, 1]. We select subsets w.r.t. g1 and g2 respectively as Γ1,1 = {1, 2}, Γ2,1 = {5, 6}. Then, the
second-order dense (l, q)-sublevel relaxation reads

{t : f (x) − t = θ0(x) +

sup
t∈R

˜θ1(xΓ1,1 ) + ˜σ1(xΓ1,1 )g1(x)
(cid:1)
(cid:0)

+

˜θ2(xΓ2,1 ) + ˜σ2(xΓ2,1 )g2(x)
(cid:1)

(cid:0)

}

where θ0 is a degree-2 SOS polynomial in variable x, ˜θk are degree-4 SOS polynomials in vari-
able xΓk,1 , ˜σk are degree-2 SOS polynomials in variable xΓk,1 . In other words, θ0 ∈ Σ[x]1, ˜θk ∈
Σ[xΓk,1 ]2 ⊆ ˜Σ[x]2

2, ˜σk ∈ Σ[xΓk,1 ]1 ⊆ ˜Σ[x]2
1.

Similarly, deﬁne Γ1,1,1 = {1, 2} ⊆ I1 and Γ2,2,1 = {5, 6} ⊆ I2, then the second-order sparse

(l, q)-sublevel relaxation reads

(cid:0)

sup
t∈R

{t : f (x)−t =

θ0,1(xI1 )+ ˜θ1(xΓ1,1,1 )+ ˜σ1(xΓ1,1,1 )g1(x)
(cid:1)

θ0,2(xI2 )+ ˜θ2(xΓ2,2,1 )+ ˜σ2(xΓ2,2,1 )g2(x)
(cid:1)
(cid:0)
where θ0,k are degree-2 SOS polynomials in variable xIk , ˜θk are degree-4 SOS polynomials in variable
xΓk,k,1 , ˜σk are degree-2 SOS polynomials in variable xΓk,k,1 . In other words, θ0,k ∈ Σ[xIk ]1, ˜θk ∈
Σ[xΓk,k,1 ]2 ⊆ ˜Σ[xIk ]2

2, ˜σk ∈ Σ[xΓk,k,1 ]1 ⊆ ˜Σ[xIk ]2
1.

+

}

The standard Lasserre’s hierarchy and many of its variants are contained in the framework of

sublevel hierarchy:

Example 3 (Dense Lasserre’s Relaxation [9]) The dense version of the d-th order Lasserre’s
relaxation is the dense (d − 1)-th order sublevel relaxation with l = [n, n, . . . , n] and q = 1p, where
1p denotes the p-dimensional vector with all ones.

8

Example 4 (Sparse Lasserre’s Relaxation [10]) The sparse version of the (d − 1)-th order
Lasserre’s relaxation is the sparse d-th order sublevel relaxation with l = [[τs]s∈k(1); . . . ; [τs]s∈k(n)]
and q = 1|k(1)|+...+|k(n)|.

Example 5 (Multi-Order/Partial Relaxation) The multi-order relaxation (used to solve the
Optimal Power Flow problem in [6]), also named as partial relaxation (used to solve the Max-
Cut problem in [2]), is a variant of the second-order sparse Lasserre’s relaxation. We ﬁrst preset
a value r, then compute the maximal cliques in the chordal extension of the CSP graph of the
POP. For those cliques of size larger than r, we consider the ﬁrst-order moment matrices; for
those of size smaller or equal than r, we consider the second-order moment matrices. Denote
by S the set of indices such that τk > r for k ∈ S, and T the set of indices such that τk ≤ r
for k ∈ T . Then the multi-order/partial relaxation is the second-order sublevel relaxation with l =
[[0]s∈k(1)∩S, [τs]s∈k(1)∪T ; . . . ; [0]s∈k(n)∩S, [τs]s∈k(n)∪T ] and q = [[0]s∈k(1)∩S, [1]s∈k(1)∪T ; . . . ; [0]s∈k(n)∩S, [1]s∈k(n)∪T ].

Example 6 (Augmented Partial Relaxation) This relaxation is the strengthened version of the
partial relaxation used by the authors in [2] to solve Max-Cut problems. It is exactly the second-order
sublevel relaxation restricted to Max-Cut problem.

Example 7 (Heuristic Relaxation) The heuristic relaxation proposed by the authors in [3] to
compute the upper bound of the Lipschitz constant of ReLU networks, is a variant of the second-order
dense Lasserre’s relaxation. The intuition is that some constraints in the POP are sparse, so let
us denote by S the set of their indices, while their corresponding cliques are large, thus one cannot
solve the second-order relaxation of the standard sparse Lasserre’s hierarchy. We then consider the
dense ﬁrst-order relaxation (Shor’s relaxation), and choose subsets of moderate sizes (size 2 in [3])
that contain the variable sets of these sparse constraints. For other constraints with larger variable
sets, let us denote by T the set of their indices and let us consider the ﬁrst-order moment matrices.
Then the heuristic relaxation is the second-order sublevel relaxation with l = [[0]i∈T , [2]i∈S] and
q = [[0]i∈T , [1]i∈S].

Summarizing the above discussion, we have the following proposition:

Proposition 1 For the dense case, if l = [n, n, . . . , n], then the d-th order (l, q)-sublevel relaxation
is exactly the dense (d + 1)-th order Lasserre’s relaxation.

For the sparse case, if l = [[τs]s∈k(1); . . . ; [τs]s∈k(n)], then the d-th order (l, q)-sublevel relaxation

is exactly the sparse (d + 1)-th order Lasserre’s relaxation.

3.2 Determining the subsets of cliques

There are diﬀerent ways to determine the subsets Γi,j (or Γi,k,j) of the sublevel relaxation described
in Deﬁnition 2. Generically, we are not aware of any algorithm that would guarantee that the
selected subsets are optimal at a given level of relaxation.
In this section, we propose several
heuristics to select the subsets. Suppose that {Ik}1≤k≤r is the sequence of maximal cliques in the
chordal extension of the CSP graph of the sparse problem (SpPOP) and that the level of relaxation
is l ≤ |Ik| =: τk. We need to select the “best” candidate among the
many subsets of size l.
τk
However, in practice, the number
l

might be very large since
In order to make this selection procedure tractable, we reduce the number of sample subsets to
τk. Precisely, suppose Ik := {i1, i2, . . . , iτk }, deﬁne Ik,j := {ij, ij+1, . . . , ij+l} for j = 1, 2, . . . , τk
and 1 ≤ l ≤ τk. By convention, ij = ik if j ≡ k mod τk. Denote by p the depth of the relaxation.

τk
l
≈ τ l
k when l is ﬁxed.
(cid:0)
(cid:1)

τk
l

(cid:0)

(cid:1)

(cid:1)

(cid:0)

9

Then we use the following heuristics to choose p subsets among the candidates Ik,j . Without loss
of generality, we assume that l < τk (otherwise one has l ≥ τk, then we only need to select one
subset I = Ik).

• H1 (Random Heuristic). For each i and clique Ik, we randomly select p subsets Γi,k,j ⊆ Ik

for j = 1, . . . , p, such that |Γi,k,j | = l for all j.

• H2 (Ordered Heuristic). For each i and clique Ik, we select one after another Γi,k,j = Ik,j ⊆ Ik

for j = 1, . . . , p. For p = τk, we also call this heuristic the cyclic heuristic.

The heurisics H1 and H2 do not depend on the problem, thus they might not fully explore
the speciﬁc structure hidden in the POPs. We can also try the heuristic that selects the subsets
according to the value of the moments in the ﬁrst-order moment relaxation (Shor’s relaxation).

• H3 (Moment Heuristic). First of all, we solve the ﬁrst-order sparse relaxation. For each i
and clique Ik, suppose Mk is the ﬁrst-order moment matrix indexed by 1 and the monomials in
xIk . Denote by Mk(Ik,j ) the submatrix whose rows and columns are indexed by 1 and xIk,j for
j = 1, 2, . . . , τk. We reorder the subsets Ik,j w.r.t. the inﬁnity norm of the submatrices Mk(Ik,j ),
i.e.,

||Mk(Ik,1)||∞ ≥ ||Mk(Ik,2)||∞ ≥ . . . ≥ k|Mk(Ik,τk )||∞ .

Then we pick the ﬁrst p subsets Γi,k,1 = Ik,1, Γi,k,2 = Ik,2, . . . , Γi,k,p = Ik,p after reordering.

In particular, for Max-Cut problem, the authors in [2] proposed the following heuristics that
take the weights in the graph or the maximal cliques in the chordal graph into account. We brieﬂy
introduce the idea of these heuristics, readers can refer to [2] for details. For heuristic H4 to H4-6,
denote by L the Laplacian matrix of the graph.

• H4 (Laplacian Heuristic). For each clique Ik, denote by L(Ik,j ) the submatrix of the moment
matrix Mk whose rows and columns are indexed by 1 and xIk,j for j = 1, 2, . . . , τk. We reorder the
subsets (Ik,j ) w.r.t. the inﬁnity norm of the submatrices (L(Ik,j )), i.e.,

||L(Ik,1)||∞ ≥ ||L(Ik,2)||∞ ≥ . . . ≥ k|L(Ik,τk )||∞

Then we pick the ﬁrst p subsets Γi,k,1 = Ik,1, Γi,k,2 = Ik,2, . . . , Γi,k,p = Ik,p after reordering.
• H5 (Max-Repeated Heuristic). We select subsets contained in many maximal cliques.
• H6 (Min-Repeated Heuristic). We select subsets contained in few maximal cliques.
• H4-5. We combine heuristic H4 and H5 to select the subsets that are not repeated in other

maximal cliques and contain variables with large weights.

In the spirit of the heuristic H4-5, we can also combine H5 with the moment heuristic H3:
• H3-5. We combine H3 and H5 to select the subsets that are not repeated in other maximal

cliques and contain variables with large moments.

Table 1: Comparison of diﬀerent heuristics for Max-Cut instances g 20 and w01 100.

Heuristics

lv=4, p=1

g20

w01

lv=4, p=2

g20

w01

lv=6, p=1

g20

w01

lv=6, p=2

g20

w01

Count

H1
H2
H3
H4
H5
H6
H3-5
H4-5

548.4
546.4
550.6
549.7
553.5
553.3
550.5
549.8

725.6
728.0
728.8
723.4
731.0
731.2
729.5
726.6

539.0
539.9
541.8
542.0
543.1
543.2
541.8
542.0

720.0
721.1
723.2
718.6
725.8
726.6
726.6
719.3

526.6
526.9
528.5
526.9
529.3
529.3
528.5
526.9

709.5
705.7
713.9
710.5
715.6
717.2
713.8
710.4

522.0
523.1
524.2
523.6
525.2
525.2
524.2
523.6

700.4
701.7
705.6
701.5
708.4
710.3
704.8
700.4

4
2
0
2
0
0
0
1

10

There is no general guarantee that one of the heuristics always performs better than the others.
In Table 1, we show the upper bounds obtained by the above heuristics for two Max-Cut instances
g 20 and w01 100 (the detail of the numerical settings and the results is referred to Section 5), for
level 4, 6, and depth 1, 2, respectively. For each heuristic, we count the number of times that the
heuristic performs the best. We see that, suprisingly, the random heuristic H1 performs the best
among other heuristics. The ordered heuristic H2 and Laplacian heuristic H4 also performs well.
For the sake of simplicity, we will only consider the ordered heuristic H2 and its variants for the
forthcoming examples.

4 Applications of sublevel hierarchy

In this section, we explicitly build diﬀerent sublevel relaxations for diﬀerent classes of polyno-
mial optimization problems: Maximum Cut (Max-Cut), Maximum Clique (Max-Cliq), Mixed Inte-
ger Quadratically Constrained Programming (MIQCP) and Quadratically Constrained Quadratic
Problem (QCQP). We also consider two classes of problems arising from deep learning: robustness
certiﬁcation and Lipschitz constant estimation of neural networks. For many deep learning applica-
tions, the targeted optimization problems are often dense or nearly-dense, due to the composition
of aﬃne maps and non-linear activation functions such as ReLU(Ax) = max{Ax, 0}. In this case,
the sublevel hierarchy is indeed helpful. A simple application for Lipschitz constant estimation was
previously considered by the authors in [3].

For simplicity, unless stated explicitly, we always assume that all the levels (li) (resp. (li,k))
and depths (qi) (resp. (qi,k)) are identical, i.e., li = l, qi = q for all i (resp. li,k = l, qi,k = q for all
i, k). We say that this simpliﬁed sublevel relaxation is of level l and depth q. Note that the sublevel
relaxation of level 0 and depth 0 is equivalent to Shor’s relaxation. By convention, if li,k ≥ τk, then
this sublevel li,k should automatically be τk and the depth p should be 1. For all the examples, we
consider the ordered heuristic H2 or its variants to select the subsets in the sublevel relaxation.

4.1 Examples from optimization

The examples listed in this section are typical in optimization.

Maximum cut (Max-Cut) problem

Given an undirected graph G(V, E) where V is a set of vertices and E is a set of edges, a cut is a
partition of the vertices into two disjoint subsets. The Max-Cut problem consists of ﬁnding a cut
in a graph such that the number of edges between the two subsets is as large as possible. It can be
formulated as follows:

{xT Lx : x ∈ {−1, 1}n} ,

max
x

(Max-Cut)

where L is the Laplace matrix of the given graph of n vertices, i.e., L := diag(W1n) − W where
W is the weight matrix of the graph. The constraints x ∈ {−1, 1}n are equivalent to (xi)2 = 1
for all i. Suppose that (Ik) are the maximum cliques in the chordal extension of the given graph.
For i = 1, 2, . . . , n, denote by k(i) the set of indices s such that i ∈ Is. For s ∈ k(i), suppose that
Is = {i1, . . . , iτs} so that ij(i) = i for 1 ≤ j(i) ≤ τs. Then we select the q subsets of size l by order as:
Is,t = {ij(i), ij(i)+t, . . . , ij(i)+t+l−2} for t = 1, 2, . . . , q. If we consider the dense sublevel hierarchy,
then we directly select the subsets by order as It = {i, i + t, . . . , i + t + l − 2} for t = 1, 2, . . . , q.

11

Maximum clique (Max-Cliq) problem

Given an undirected graph G(V, E) where V is a set of vertices and E is a set of edges, a clique
is deﬁned to be a set of vertices that is completely interconnected. The Max-Cliq problem con-
sists of determining a clique of maximum cardinality. It can be stated as a nonconvex quadratic
programming problem over the unit simplex [19] and its general formulation is:

{xT Ax :

max
x

n

Xi=1

xi = 1, x ∈ [0, 1]n} ,

(Max-Cliq)

where A is the adjacency matrix of the given graph of n vertices. The constraints x ∈ [0, 1]n
are equivalent to xi(xi − 1) ≤ 0 for i = 1, 2, . . . , n. The Max-Cliq problem is dense since we
n
i=1 xi = 1 involving all the variables. Therefore, we apply the dense sublevel
have a constraint
n
hierarchy. To handle the constraint
i=1 xi = 1, we select the q subsets of size l by order as
P
It = {t, t + 1, . . . , t + l − 1} for t = 1, 2, . . . , q. For the constraints xi(xi − 1) ≤ 0, we select the
subsets by order as It = {i, i + t, . . . , i + t + l − 2} for t = 1, 2, . . . , q.

P

Mixed integer quadratically constrained programming (MIQCP)

The MIQCP problem is of the following form:

min
x

{xT Q0x + bT

0 x : xT Qix + bT

i x ≤ ci, i = 1, . . . , p,

Ax = b, l ≤ x ≤ u, xI ∈ Z} ,

(MIQCP)

where each Qi is a symmetric matrix of size n × n, A is a matrix of size n × n, b, bi, l, u are
n-dimensional vectors, and each ci is a real number. The constraints xT Qix + bT
i x ≤ ci are
called quadratic constraints, the constraints Ax = b are called linear constraints. The constraints
l ≤ x ≤ u and xI ∈ Z bound the variables and restrict some of them to be integers.
In our
benchmarks, we only consider the case where x ∈ {0, 1}n, which is also equivalent to xi(xi − 1) = 0
for i = 1, 2, . . . , n. If we only have bound constraints, then we use the same ordered heuristic as
for the Max-Cut problem to select the subsets. If in addition we also have quadratic constraints or
linear constraints, then the problem is dense and therefore we consider the dense sublevel hierarchy.
For quadratic constraints, we don’t apply the sublevel relaxation to them, i.e., l = q = 0. However,
if Qi equals the identity matrix, then we use the same heuristic as the linear constraints: we select
the subsets by order as It = {t, t + 1, . . . , t + l − 1} for t = 1, 2, . . . , q.

Quadratically constrained quadratic problems (QCQP)

A QCQP can be cast as follows:

min
x

{xT Q0x + bT

0 x : xT Qix + bT

i x ≤ ci, i = 1, . . . , p,

Ax = b, l ≤ x ≤ u} ,

(QCQP)

where each Qi is a symmetric matrix of size n × n, A is a matrix of size n × n, b, bi, l, u are
n-dimensional vectors, and each ci is a real number. This is very similar to the MIQCP except that
we drop out the integer constraints. Therefore, we use the same strategy to select the subsets in
the sublevel relaxation.

12

4.2 Examples from deep learning

The following examples are picked from the recent deep learning topics.

Upper bounds of lipschitz constants of deep neural networks [3]

We only consider the 1-hidden layer neural network with ReLU activation function, the upper bound
of whose Lipschitz constant results in a QCQP as follows:

{tT AT diag(u)c : u(u − 1) = 0, (u − 1/2)(Ax + b) ≥ 0;

max
x,u,t

t2 ≤ 1, (x − ¯x + ε)(x − ¯x − ε) ≤ 0 .}

(Lip)

where A is a matrix of size p2 × p1, ¯x is a p1-dimensional vector, b, c are p2-dimensional vectors,
and ǫ is a positive real number. When ǫ = 10 (resp. ǫ = 0.1), we compute the upper bounds
of the global (resp.
local ) Lipschitz constant of the neural network. Assume the matrix A is
dense, then the maximal cliques in the chordal extension of (Lip) are I = {x1, . . . , xp1 ; u1, . . . , up2}
and Ik = {u1, . . . , up2, tk} for k = 1, . . . , p1. Therefore, we consider the sparse sublevel relax-
ation. For the constraints t2
k ≤ 1, we choose the subsets by order as Ik,i = {ui, . . . , ui+l−2; tk} for
i = 1, . . . , q. For the constraints (xi − ¯xk + ε)(x − ¯xk − ε) ≤ 0, we choose the subsets by order as Ii =
{xk, xk+i, . . . , xk+i+l/2−2; ui, . . . , ui+l/2−1} for i = 1, . . . , q. For the constraints uj(uj − 1) = 0 and
(uj−1/2)(Aj,:x+bj) ≥ 0, we choose the subsets by order as Ii = {xi, . . . , xi+l/2−1; uj, uj+i, . . . , uj+i+l/2−2}
for i = 1, . . . , q.

Robustness certiﬁcation of deep neural networks [22]

We also consider the 1-hidden layer neural network with ReLU activation function. Then the
robustness certiﬁcation problem can be formulated as a QCQP as follows:

{cT u : u(u − Ax − b) = 0, u ≥ Ax + b, u ≥ 0,

max
x,u

(x − ¯x + ε)(x − ¯x − ε) ≤ 0 .}

(Cert)

where A is a matrix of size p2 × p1, ¯x is a p1-dimensional vector, b, c are p2-dimensional vectors,
and ǫ is a positive real number. Assume the matrix A is dense, then the maximal cliques in the
chordal extension of (Cert) are Ik = {x1, . . . , xp1 ; uk} for k = 1, . . . , p2. Similarly to the Lipschitz
problem (Lip), we consider the sparse sublevel relaxation. For all the constraints, we choose the
subsets by order as Ik,i = {xi, . . . , xi+l−2; uk} for i = 1, . . . , q.

5 Numerical results

In this section, we apply the sublevel relaxation to diﬀerent type of POPs both in optimization
and deep learning, as discussed in the previous section. Most of the instances in optimization are
taken from the Biq-Mac library [23] and the QPLIB library [4], others are generated randomly.
We calculate the ratio of improvements (RI) of each sublevel relaxation, compared with Shor’s
relaxation, namely RI = Shor−sublevel
Shor−solution × 100%. We also compute the relative gap (RG) between
the sublevel relaxation and the optimal solution, given by RG = sublevel−solution
× 100%. For each
instance, we only show the ratio of improvements and relative gap corresponding to the results of

|solution|

13

the last sublevel relaxation. The larger the ratio of improvements or the smaller the relative gap,
the better the bounds. If the optimal solution is not known so far, it is replaced by the (best-known)
valid upper bounds (UB) or lower bounds (LB). We implement all the programs on Julia, and use
Mosek as back-end to solve SDP relaxations. The running time (with second as unit) displayed
in all tables refers to the time spent by Mosek to solve the SDP relaxation. All experiments are
performed with an Intel 8-Core i7-8665U CPU @ 1.90GHz Ubuntu 18.04.5 LTS, 32GB RAM.

5.1 Examples from optimization

Max-Cut instances

The following classes of problems and their solutions are from the Biq-Mac library. For each class
of problem, we choose the ﬁrst instance, i.e., i = 0, and drop the suﬃx “.i” in Table 3:

• g05 n.i, unweighted graphs with edge probability 0.5, n = 60, 80, 100.
• pm1s n.i, pm1d n.i, weighted graph with edge weights chosen uniformly from {−1, 0, 1} and

density 10% and 99% respectively, n = 80, 100.

• wd n.i, pwd n.i, graph with integer edge weights chosen from [−10, 10] and [0, 10] respectively,

density d = 0.1, 0.5, 0.9, n = 100.

The instances named g n and the corresponding upper bounds are from the CS-TSSOS paper

[30].

The instances named Gn are from the G-set library by Y.Y. Ye 1, and their best-known solutions

are taken from [8].

In Table 2, we give a summary of basic information and the graph structure of each instance:
nVar denotes the number of variables, Density denotes the percentage of non-zero elements in
the adjacency matrix, nCliques denotes the number of cliques in the chordal extension, MaxClique
denotes the maximum size of the cliques, MinClique denotes the minimum size of the cliques.

1http://web.stanford.edu/~yyye/yyye/Gset/

14

Table 2: Summary of the basic information and graph structure of the Max-Cut instances.

nVar Density

g05 60
g05 80
g05 100
pm1d 80
pm1d 100
pm1s 80
pm1s 100
pw01 100
pw05 100
pw09 100
w01 100
w05 100
w09 100
g 20
g 40
g 60
g 80
g 100
g 120
g 140
g 160
g 180
g 200
G11
G12
G13
G32
G33
G34

60
80
100
80
100
80
100
100
100
100
100
100
100
505
1005
1505
2005
2505
3005
3505
4005
4505
5005
800
800
800
2000
2000
2000

50%
50%
50%
99%
99%
10%
10%
10%
50%
90%
10%
50%
90%
1.6%
0.68%
0.43%
0.30%
0.23%
0.19%
0.16%
0.13%
0.12%
0.11%
0.25%
0.25%
0.25%
0.1%
0.1%
0.1%

nCliques MaxClique MinClique
50
69
88
79
99
37
54
54
89
97
54
89
97
15
15
15
15
16
15
15
15
15
15
24
48
90
76
99
141

11
12
13
2
2
44
47
47
12
4
47
12
4
369
756
756
1556
1930
2383
2762
3131
3429
3886
598
598
598
1498
1498
1498

19
28
37
76
95
4
4
4
40
83
4
40
83
1
1
1
1
1
1
1
1
1
1
5
5
5
5
5
5

In Table 3, we display the upper bounds and running times corresponding to the sublevel
relaxations of depth 1, and level 0, 4, 6, 8, respectively. Notice that the authors in [2] use the
partial relaxation to compute upper bounds for instances g 20 to g 200. The sublevel relaxation we
consider here is actually what they call the augmented partial relaxation, which is a strengthened
relaxation based on partial relaxation. From the ratio of improvement, we see that the more sparse
structure the graph has, the better the sublevel relaxation performs. Notice that if we obtain better
upper/lower bounds than the current best-known bounds, the ratio of improvements will be larger
than 100% and the relative gap will become nagative. Particularly, our method provides better
bounds for all the instances g n in the CS-TSSOS paper [30], and computes upper bounds very
close to the best-known solution for the instances Gn in G-set.

Moreover, if the number of variables is of moderate size, the dense sublevel relaxation might
performs faster than the sparse one. For example, the instance g05 100 has 13 maximal cliques with
maximum size 88 and minimum size 37. The sparse sublevel relaxation consists of 13 ﬁrst-order
moment matrices of size from 37 to 88. However, the dense version only consists of 1 ﬁrst-order
moment matrix of size 100. In fact, the dense sublevel relaxation gives an upper bound of 1463.5
at level 0 in 10 seconds, yielding the same bound as the sparse case at level 0 but with much less
computing time, and 1458.1 at level 8 in 178.1 seconds, providing better bounds than the sparse
case at level 6, with less computing time.

15

Table 3: Results obtained with sublevel relaxations of Max-Cut problems.

Sol./UB

nVar Density

g05 60
g05 80
g05 100
pm1d 80
pm1d 100
pm1s 80
pm1s 100
pw01 100
pw05 100
pw09 100
w01 100
w05 100
w09 100
g 20
g 40
g 60
g 80
g 100
g 120
g 140
g 160
g 180
g 200
G11
G12
G13
G32
G33
G34

536
929
1430
227
340
79
127
2019
8190
13585
651
1646
2121
537.4
992.2
1387.2
1838.1
2328.3
2655.4
3027.2
3589.0
3953.1
4472.3
564
556
580
1398
1376
1372

60
80
100
80
100
80
100
100
100
100
100
100
100
505
1005
1505
2005
2505
3005
3505
4005
4505
5005
800
800
800
2000
2000
2000

50%
50%
50%
99%
99%
10%
10%
10%
50%
90%
10%
50%
90%
1.6%
0.68%
0.43%
0.3%
0.23%
0.19%
0.16%
0.13%
0.12%
0.11%
0.25%
0.25%
0.25%
0.1%
0.1%
0.1%

MIQCP instances

Sublevel relaxation, l = 0/4/6/8, q = 1 (level 0 = Shor)

upper bounds (RI, RG)

550.1
950.9
1463.5
270.0
405.4
90.3
143.2
2125.4
8427.7
13806.0
740.9
1918.0
2500.3
570.8
1032.6
1439.9
1899.2
2398.7
2731.7
3115.8
3670.7
4054.7
4584.6
629.2
623.9
647.1
1567.6
1544.3
1546.7

548.1
949.0
1462.0
265.9
402.2
86.7
141.4
2107.8
8416.6
13797.1
728.3
1902.6
2478.2
547.1
982.4
1368.4
1803.8
2282.9
2588.5
2947.9
3487.1
3855.9
4353.3
581.3
572.5
594.2
1433.4
1415.3
1407.9

546.0
946.6
1459.2
262.0
397.9
83.6
137.6
2088.1
8403.6
13781.1
710.3
1885.5
2447.3
526.7
950.8
1317.8
1744.9
2205.1
2507.3
2856.5
3380.7
3736.9
4228.1
564.6
559.6
585.1
1415.9
1392.7
1388.2

544.6
944.6
1456.8
258.8
393.7
82.8
135.3
2075.0
8388.1
13766.5
696.2
1869.7
2422.8
513.4
927.6
1281.9
1698.8
2149.3
2439.8
2782.6
3310.9
3653.5
4132.2
564.6
559.6
584.1
1415.9
1387.4
1388.2

(39.0%, 1.6%)
(28.8%, 1.7%)
(20.0%, 1.9%)
(26.0%, 14.0%)
(19.0%, 15.8%)
(66.4%, 4.8%)
(48.8%, 6.5%)
(47.4%, 2.8%)
(16.7%, 2.4%)
(17.9%, 1.3%)
(49.7%, 6.9%)
(17.8%, 13.6%)
(20.4%, 14.2%)
(171.9%, -4.5%)
(260.0%, -6.5%)
(300.4%, -7.6%)
(328.0%, -7.6%)
(354.3%, -7.7%)
(382.6%, -8.1%)
(376.1%, -8.1%)
(440.4%, -7.7%)
(394.9%, -7.6%)
(402.8%, -7.6%)
(99.1%, 0.1%)
(94.7%, 0.6%)
(93.9%, 0.7%)
(89.4%, 1.3%)
(93.2%, 0.8%)
(90.7%, 1.2%)

4.5
33.8
138.7
15.0
47.6
1.4
11.1
13.0
136.8
141.6
10.5
138.1
124.3
0.7
1.2
2.8
6.0
3.4
3.8
3.8
8.2
8.8
5.4
4.0
17.8
159.2
622.0
1956.6
3613.5

solving time (s)
17.6
10.6
61.8
56.2
328.7
303.7
39.2
29.4
110.2
69.4
13.4
4.9
28.6
24.3
29.7
20.5
272.9
223.0
268.7
218.4
35.0
22.4
272.2
265.8
280.8
255.0
46.1
15.1
47.9
18.6
74.7
26.0
76.0
23.8
117.4
30.1
113.2
33.3
138.4
46.3
198.2
56.5
277.0
51.5
203.2
52.7
32.6
15.8
54.3
57.8
340.2
241.7
630.8
736.3
1221.5
2115.8
6327.9
6580.9

65.7
137.4
460.3
128.1
225.1
37.7
180.3
285.8
400.3
442.4
224.7
403.2
451.7
102.2
102.5
431.1
290.7
428.6
434.5
522.1
506.6
693.4
839.2
36.5
51.9
321.6
628.0
1486.8
6147.4

The following classes of problems and their solutions are from the Biq-Mac library, where there are
neither quadratic constraints xT Qix + bT
i x ≤ ci nor linear constraints Ax = b. We only have
integer bound constraints x ∈ {0, 1}n.

• bqpn-i, with 10% density. All the coeﬃcients have uniformly chosen integer values in [−100, 100],

n = 50, 100, 250, 500.

• gkaia, with dimensions in [30, 100] and densities in [0.0625, 0.5]. The diagonal coeﬃcients lie

in [−100, 100] and the oﬀ-diagonal coeﬃcients belong to [−100, 100].

• gkaib, with dimensions in [20, 125] and density 1. The diagonal coeﬃcients lie in [−63, 0] and

the oﬀ-diagonal coeﬃcients belong to [0, 100].

• gkaic, dimensions in [40, 100] and densities in [0.1, 0.8]. Diagonal coeﬃcients in [−100, 100],

oﬀ-diagonal coeﬃcients in [−50, 50].

• gkaid, with dimension 100 and densities in [0.1, 1]. The diagonal coeﬃcients lie in [−75, 75]

and the oﬀ-diagonal coeﬃcients belong to [−50, 50].

We also select some instances and their solutions from the QPLIB library with ID 0032, 0067,
0633, 2512, 3762, 5935 and 5944, in which we have additional linear constraints Ax = b. For the
instance 0032, there are 50 continuous variables and 50 integer variables. For the two instances
5935 and 5944, we maximize the objective, the others are minimization problems.

Similarly to the Max-Cut instances, Table 4 summarizes the basic information and cliques
structure of each instance. Table 5 is a summary of basic information and the number of quadratic,
linear, bound constraints of the instances from the QPLIB library.

16

Table 4: Summary of the basic information and sparse structure of the MIQCP instances.

nVar Density

bqp50-1
bqp100-1
gka1a
gka2a
gka3a
gka4a
gka5a
gka6a
gka7a
gka8a
gka1b
gka2b
gka3b
gka4b
gka5b
gka6b
gka7b
gka8b
gka9b
gka10b
gka1c
gka2c
gka3c
gka4c
gka5c
gka6c
gka7c
gka1d
gka2d
gka3d
gka4d
gka5d
gka6d
gka7d
gka8d
gka9d
gka10d

50
100
50
60
70
80
50
30
30
100
20
30
40
50
60
70
80
90
100
125
40
50
60
70
80
90
100
100
100
100
100
100
100
100
100
100
100

10%
10%
10%
10%
10%
10%
20%
40%
50%
62.5%
100%
100%
100%
100%
100%
100%
100%
100%
100%
100%
80%
60%
40%
30%
20%
10%
10%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%

nCliques MaxClique MinClique
15
49
15
20
27
33
26
20
21
37
19
29
39
49
59
69
79
89
99
124
37
45
47
49
54
45
50
51
71
78
86
88
91
94
95
96
99

3
4
1
3
3
4
4
7
10
2
19
29
38
47
56
67
77
87
97
124
25
26
17
12
11
4
3
4
11
18
31
36
47
57
68
79
95

36
52
36
41
44
48
25
11
10
64
2
2
2
2
2
2
2
2
2
2
4
6
14
22
27
46
51
50
30
23
15
13
10
7
6
5
2

nQuad
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

nLin
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

nBound
50
100
50
60
70
80
50
30
30
100
20
30
40
50
60
70
80
90
100
125
40
50
60
70
80
90
100
100
100
100
100
100
100
100
100
100
100

Table 5: Summary of the basic information and constraint structure of the MIQCP instances from
QPLIB library.

qplib0032
qplib0067
qplib0633
qplib2512
qplib3762
qplib5935
qplib5944

nVar Density
100
80
75
100
90
100
100

89%
89%
99%
28%
28%
28%
28%

nQuad
0
0
0
0
0
0
0

nLin
52
1
1
20
480
1237
2475

nBound
100
80
75
100
90
100
100

In Table 6, we show the lower bounds and running time obtained by solving the sublevel relax-
ations with depth 1 and level 0, 4, 6, 8, respectively. We see that when the problem has a good
sparsity structure or is of low dimension, the sublevel relaxation performs very well and provides
the exact solution, in particular for the two instances gka2a and gka7a. For dense problems, we are
not able to ﬁnd the exact solution, but still have improvements between 20% and 40% compared
to Shor’s relaxation. Notice that for the instances gka1b to gka10b, even though we have an im-

17

provement ratio ranging from 24.0% to 77.9%, the relative gap is very high, varying from 38.2%
to 947.2%. This means that these problems themselves are very hard to solve, so that the gap
between the results of Shor’s relaxation and the exact optimal solution is very large. Even though
the sublevel relaxation yields substantial improvement compared to Shor’s relaxation, it’s still far
away from the true optimum.

Table 6: Results obtained with sublevel relaxations of MIQCP problems.

Sol.

nVar Density

bqp50-1
bqp100-1
gka1a
gka2a
gka3a
gka4a
gka5a
gka6a
gka7a
gka8a
gka1b
gka2b
gka3b
gka4b
gka5b
gka6b
gka7b
gka8b
gka9b
gka10b
gka1c
gka2c
gka3c
gka4c
gka5c
gka6c
gka7c
gka1d
gka2d
gka3d
gka4d
gka5d
gka6d
gka7d
gka8d
gka9d
gka10d
qplib0032
qplib0067
qplib0633
qplib2512
qplib3762
qplib5935
qplib5944

-2098
-7970
-3414
-6063
-6037
-8598
-5737
-3980
-4541
-11109
-133
-121
-118
-129
-150
-146
-160
-145
-137
-154
-5058
-6213
-6665
-7398
-7362
-5824
-7225
-6333
-6579
-9261
-10727
-11626
-14207
-14476
-16352
-15656
-19102
10.1
-110942
79.6
135028
-296
4758
1829

50
100
50
60
70
80
50
30
30
100
20
30
40
50
60
70
80
90
100
125
40
50
60
70
80
90
100
100
100
100
100
100
100
100
100
100
100
100
80
75
100
90
100
100

10%
10%
10%
10%
10%
10%
20%
40%
50%
62.5%
100%
100%
100%
100%
100%
100%
100%
100%
100%
100%
80%
60%
40%
30%
20%
10%
10%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
99%
89%
99%
77%
28%
99%
99%

Max-Cliq instances

Sublevel relaxation, l = 0/4/6/8, q = 1 (level 0 = Shor)

lower/upper bounds (RI, RG)

-2136.3
-8358.2
-3453.2
-6076.3
-6291.5
-8767.3
-5789.9
-4008.9
-4566.8
-11148.0
-295.1
-425.3
-535.6
-670.9
-820.9
-972.2
-1138.1
-1269.8
-1385.4
-1782.1
-5102.9
-6291.3
-6730.7
-7527.7
-7543.7
-5932.2
-7297.8
-6475.3
-6980.5
-9686.2
-11303.3
-12381.6
-14938.2
-15413.2
-17011.5
-16652.0
-20121.7
-16491
-112923
74.0
-125060
-330.8
40148
27437

-2116.3
-8215.1
-3432.6
-6063.0
-6182.6
-8713.7
-5760.3
-3986.0
-4541.1
-11124.8
-253.6
-325.4
-483.4
-614.2
-736.8
-894.8
-1031.0
-1190.2
-1298.9
-1707.1
-5077.9
-6263.1
-6703.1
-7494.9
-7474.6
-5869.7
-7264.3
-6403.1
-6897.9
-9591.7
-11175.4
-12274.7
-14834.9
-15267.6
-16887.6
-16513.3
-19974.1
-15962
-112615
75.1
27898
-319.9
36842
23142

-2105.4
-8101.8
-3428.5
-6063.0
-6106.3
-8676.0
-5750.0
-3982.5
-4541.1
-11114.0
-183.8
-282.5
-437.7
-571.5
-705.5
-833.5
-982.6
-1120.9
-1212.6
-1612.7
-5073.7
-6246.2
-6688.1
-7462.8
-7412.8
-5847.4
-7248.7
-6369.6
-6811.6
-9523.6
-11096.5
-12185.0
-14720.2
-15173.6
-16794.3
-16409.6
-19863.8
-15440
-112478
75.7
82909
-309.5
28812
19784

(97.0%, 0.4%)
(82.5%, 1.7%)
(93.1%, 0.4%)
(100%, 0%)
(86.4%, 1.1%)
(76.9%, 0.9%)
(94.6%, 0.2%)
(98.8%, 0.1%)
(100%, 0%)
(97.1%, 0.05%)
(77.9%, 38.2%)
(58.0%, 133.5%)
(46.7%, 270.9%)
(35.0%, 343.0%)
(37.2%, 370.3%)
(39.3%, 470.9%)
(31.6%, 514.1%)
(26.8%, 673.0%)
(29.5%, 785.1%)
(24.0%, 947.2%)
(84.8%, 0.3%)
(81.5%, 0.5%)
(87.5%, 0.3%)
(74.0%, 0.9%)
(84.2%, 0.7%)
(90.3%, 0.4%)
(88.0%, 0.3%)
(85.9%, 0.6%)
(64.5%, 3.5%)
(62.6%, 2.8%)
(57.3%, 3.4%)
(44.4%, 4.8%)
(50.1%, 3.6%)
(41.7%, 4.8%)
(55.8%, 2.7%)
(44.4%, 4.8%)
(44.3%, 4.0%)
(21.8%, 152971.3%)
(72.3%, 1.4%)
(55.2%, 4.9%)
(91.0%, 38.6%)
(72.8%, 4.6%)
(61.7%, 505.5%)
(72.4%, 981.7%)

0.1
8.8
0.1
0.3
0.7
2.1
0.7
0.2
0.3
2.3
0.1
0.2
0.7
1.9
3.2
9.1
26.1
40.5
65.9
285.8
0.8
1.9
6.1
13.1
15.1
10.0
12.4
11.4
42.3
164.8
302.2
324.3
236.6
138.8
271.5
390.5
77.8
18.1
6.2
2.9
18.6
6.3
12.8
15.6

solving time (s)
0.5
16.7
0.6
1.0
1.6
3.4
1.4
0.6
0.8
2.7
0.5
0.7
1.4
3.3
8.4
11.6
31.2
60.1
92.3
413.3
1.6
2.8
9.3
18.4
27.7
11.0
13.9
13.4
70.8
200.4
259.1
256.3
239.7
225.9
277.9
419.8
83.4
19.4
11.1
10.1
19.9
18.1
39.4
182.1

1.3
21.8
0.9
4.6
6.1
10.1
6.2
3.9
4.9
7.5
2.4
4.0
6.5
14.3
15.5
26.6
50.8
102.3
111.2
452.2
5.3
7.8
15.9
24.6
40.3
19.0
22.1
29.1
70.6
262.7
191.8
294.3
221.9
150.0
291.6
367.0
130.2
37.0
21.9
27.1
53.4
50.7
259.0
2304.3

3.4
87.9
1.8
8.5
31.0
30.0
31.0
23.6
23.1
19.5
25.0
29.9
45.9
65.2
76.1
86.5
136.1
187.0
256.3
700.9
41.9
50.3
62.1
88.1
112.8
57.4
55.6
71.3
193.7
330.0
387.7
380.2
437.9
314.6
408.6
513.5
244.8
94.7
158.3
140.0
278.6
183.4
1745.3
13204.6

-2345.5
-8721.1
-3623.3
-6204.3
-6546.2
-8935.1
-5979.9
-4190.2
-4696.6
-11283.8
-362.9
-505.7
-718.0
-809.8
-1034.8
-1279.0
-1362.5
-1479.1
-1663.6
-2073.1
-5161.1
-6392.6
-6849.9
-7647.1
-7684.5
-6065.8
-7422.7
-6592.7
-7234.2
-9963.0
-11592.5
-12632.1
-15235.3
-15672.0
-17353.3
-17010.9
-20421.4
-19751
-116480
70.9
-441284
-345.6
67494
66934

We take the same graphs as the ones considered in the Max-Cut instances. Some instances share the
same adjacency matrix with diﬀerent weights, in which case we delete these repeated graphs. LB
denotes the lower bound of a given instance, computed by 106 random samples. By contrast with

18

the strategy used for the Max-Cut instances, we use sublevel relaxations with level 2 and depth 0,
20, 40, 60, respectively. From Table 7 we see that the sublevel relaxation yields large improvement
compared to Shor’s relaxation. The Max-Cliq problem remains hard to solve as emphasized by the
large relative gap, ranging from 662.5% to 3660%.

Table 7: Results obtained with sublevel relaxations of Max-Cliq problems.

Sublevel relaxation, l = 2, q = 0/20/40/60 (depth 0 = Shor)

g05 60
g05 80
g05 100
pm1d 80
pm1d 100
pm1s 80
pw01 100
pw05 100
pw09 100

LB

0.8
0.9
0.8
1.0
1.0
0.7
0.6
0.8
1.0

nVar Density

60
80
100
80
100
80
100
100
100

50%
50%
50%
99%
99%
10%
10%
50%
90%

29.9
39.9
50.0
78.2
98.0
8.9
10.6
49.8
89.2

19.3
29.1
39.1
57.5
77.2
6.2
8.2
39.7
70.2

upper bounds (RI, RG)
6.1
8.9
18.4
17.9
37.6
4.6
5.4
18.9
34.0

(81.8%, 662.5%)
(79.5%, 888.9%)
(64.2%, 2200.0%)
(78.1%, 1690.0%)
(62.3%, 3660%)
(52.1%, 557.1%)
(51.8%, 800.0%)
(63.0%, 2262.5%)
(62.5%, 3300%)

8.3
20.1
28.9
37.6
57.5
4.6
5.9
28.9
51.9

solving time (s)

0.6
2.8
6.5
2.3
5.6
2.6
7.5
7.6
8.5

1.8
7.4
30.9
5.4
12.5
6.1
30.7
21.9
15.0

3.6
7.5
19.1
7.6
22.8
6.4
20.0
24.0
33.2

2.4
8.3
33.0
4.4
17.3
9.0
29.6
26.5
28.9

QCQP instances

We take the MIQCP instances from the Biq-Mac library with size larger or equal than 50, then
2 = 1, and relax the integer bound constraints x ∈ {0, 1}n
add one dense quadratic constraint ||x||2
to linear bound constraints x ∈ [0, 1]n. UB denotes the upper bound obtained by selecting the
minimum value over 106 random evaluations.

We also select some instances and their solutions from the QPLIB library with ID 1535, 1661,
1675, 1703 and 1773. These instances have more than one quadratic constraint and involve linear
constraints.

Table 8 is a summary of basic information as well as the number of quadratic, linear, and bound

constraints of the instances from the QPLIB library.

Table 8: Summary of the basic information and constraint structure of the QCQP instances from
the QPLIB library.

nVar Density

qplib1535
qplib1661
qplib1675
qplib1703
qplib1773

60
60
60
60
60

94%
95%
49%
98%
95%

nQuad
60
1
1
30
1

nLin
6
12
12
6
6

nBound
60
60
60
60
60

In Table 9, we show the lower bounds and running time obtained by the sublevel relaxation
with depth 1 for the instances from the QPLIB library, 10 for the instances adapted from the Biq-
Mac library, and level 0, 4, 6, 8, respectively. We see that the sublevel relaxation yields a uniform
improvement compared to Shor’s relaxation. However, for the QCQP problems adapted from the
MIQCP instances, it is very hard to ﬁnd the exact optimal solution as the relative gap varies from
60.5% to 77.0%. This is in deep contrast with the instances from the QPLIB library which are
relatively easier to solve as the relative gap varies from 9.4% to 13.8%.

19

Table 9: Results obtained with sublevel relaxations of QCQP problems.

Sol./UB

nVar Density

Sublevel relaxation, l = 0, 4, 6, 8, q = 1, 10 (level 0 = Shor)

lower bounds (RI, RG)

solving time (s)

bqp50-1
bqp100-1
gka1a
gka2a
gka3a
gka4a
gka5a
gka8a
gka4b
gka5b
gka6b
gka7b
gka8b
gka9b
gka10b
gka2c
gka3c
gka4c
gka5c
gka6c
gka7c
gka1d
gka2d
gka3d
gka4d
gka5d
gka6d
gka7d
gka8d
gka9d
gka10d
qplib1535
qplib1661
qplib1675
qplib1703
qplib1773

-99
-67.2
-109.5
-140.7
-143.2
-126.2
-180.2
-122.5
-63
-63
-63
-63
-63
-63
-63
-159.1
-126.3
-123.0
-114.0
-100
-100
-75
-87.2
-88.1
-105.5
-131.9
-137.7
-156.3
-147.6
-179.6
-187.0
-11.6
-16.0
-75.7
-132.8
-14.6

50
100
50
60
70
80
50
100
50
60
70
80
90
100
125
50
60
70
80
90
100
100
100
100
100
100
100
100
100
100
100
60
60
60
60
60

10%
10%
10%
10%
10%
10%
20%
62.5%
100%
100%
100%
100%
100%
100%
100%
60%
40%
30%
20%
10%
10%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
94%
95%
49%
98%
95%

-215.7
-323.1
-241.8
-275.3
-300.0
-311.0
-351.8
-320.1
-381.4
-446.8
-496.6
-518.3
-534.5
-573.0
-639.4
-290.0
-271.2
-292.7
-239.1
-198.8
-225.8
-197.9
-259.6
-304.0
-375.2
-383.6
-443.1
-453.9
-488.0
-539.7
-552.4
-13.9
-18.4
-93.1
-152.8
-17.3

-195.4
-304.7
-224.1
-260.9
-284.6
-288.3
-319.3
-306.8
-326.2
-377.2
-409.9
-447.1
-472.7
-501.3
-569.7
-269.3
-240.2
-263.7
-225.9
-190.8
-213.7
-182.5
-242.2
-281.6
-340.1
-351.5
-400.0
-421.4
-441.1
-487.7
-505.7
-13.5
-18.1
-87.0
-147.0
-16.8

-180.6
-296.1
-219.8
-258.7
-278.8
-282.9
-306.4
-302.1
-302.4
-348.9
-385.9
-421.6
-449.0
-477.0
-553.7
-261.6
-235.4
-254.4
-223.2
-186.7
-210.5
-177.0
-233.8
-274.1
-326.0
-341.5
-391.0
-406.6
-423.3
-469.2
-491.8
-13.3
-17.8
-85.2
-145.2
-16.6

-172.5
-290.0
-213.8
-251.6
-275.0
-280.0
-299.1
-299.5
-280.8
-327.4
-366.8
-404.3
-430.1
-455.8
-533.6
-255.4
-231.3
-247.9
-220.4
-182.4
-208.5
-174.5
-229.5
-267.5
-317.5
-332.3
-378.9
-397.4
-414.2
-456.8
-478.4
-13.2
-17.5
-83.8
-143.5
-16.4

(37.0%, 74.2%)
(12.9%, 331.5%)
(21.2%, 95.3%)
(17.6%, 78.8%)
(15.9%, 92.0%)
(16.8%, 121.9%)
(30.7%, 66.0%)
(10.4%, 144.5%)
(31.6%, 345.7%)
(31.1%, 419.7%)
(29.9%, 482.2%)
(25.0%, 541.7%)
(22.1%, 582.7%)
(23.0%, 623.5%)
(18.4%, 747.0%)
(26.4%, 60.5%)
(27.5%, 83.1%)
(26.4%, 101.5%)
(14.9%, 93.3%)
(16.6%, 82.4%)
(13.8%, 108.5%)
(19.0%, 132.7%)
(17.5%, 163.2%)
(16.9%, 203.6%)
(21.4%, 201.0%)
(20.4%, 152.0%)
(21.0%, 175.2%)
(19.0%, 154.3%)
(21.7%, 180.6%)
(23.0%, 154.3%)
(20.3%, 155.8%)
(30.4%, 13.8%)
(37.5%, 9.4%)
(53.4%, 10.7%)
(46.5%, 8.06%)
(33.3%, 12.3%)

0.8
21.4
0.8
1.7
3.6
6.8
0.7
21.3
0.7
1.1
3.2
5.9
10.9
19.7
80.1
0.8
1.7
3.0
10.6
12.3
21.4
19.3
23.5
26.2
21.6
20.4
23.8
21.4
21.7
20.6
23.2
1.4
1.4
1.0
1.2
1.1

2.4
22.7
1.9
3.8
9.1
7.9
2.8
23.5
1.8
4.2
5.3
9.5
16.0
36.6
82.1
2.5
4.4
6.5
9.6
15.9
28.2
25.9
28.2
28.9
21.9
22.7
23.3
22.5
25.0
21.5
24.6
4.3
3.0
4.2
4.2
4.0

12.3
56.9
10.0
16.8
23.1
25.4
15.1
72.5
17.4
19.5
17.4
21.6
36.0
42.5
110.9
11.2
16.0
19.7
32.3
33.1
63.4
64.8
61.1
49.2
53.2
41.3
48.6
71.7
47.0
45.1
56.9
13.7
14.7
19.2
20.8
-14.0

88.5
249.6
65.6
126.9
121.1
160.0
75.6
232.0
79.2
117.2
118.1
170.3
148.9
191.4
410.1
80.8
103.1
155.6
166.5
216.5
323.1
243.9
254.3
278.4
270.7
257.1
254.2
217.8
232.8
222.2
196.5
99.2
96.4
147.8
109.3
89.3

5.2 Examples from deep learning

Lipschitz constant estimation

We generate random 1-hidden layer neural networks with parameters A, b, c. We denote by net 1 n
the instances of 1-hidden layer networks of size n, and compute the upper bounds corresponding to
ǫ = 0.1, 10, by the sublevel relaxations of depth 1 and level 0, 4, 6, 8, respectively.

We see in Table 10 that we have a relatively high improvement ratio and low gap for the global
case, while Table 11 ,dedicated to the local case, shows that the improvement ratio is decreasing and
the gap is increasing. The underlying rationale is that local Lipschitz constants of neural networks
are harder to estimate than the global ones.

20

Table 10: Results obtained with sublevel relaxation of Lipschitz constant problems, ǫ = 10.

Sol./LB

nVar

net 1 5
net 1 10
net 1 15
net 1 20
net 1 25
net 1 30
net 1 35
net 1 40

0.38
0.69
1.72
2.68
3.56
5.60
7.77
7.40

15
30
45
60
75
90
105
120

0.44
0.72
1.86
2.88
3.83
6.16
8.92
9.07

0.39
0.70
1.81
2.83
3.74
6.11
8.79
8.97

Sublevel relaxation, lv = 0/4/6/8, p = 1 (level 0 = Shor)

upper bounds (RI, RG)
0.38
0.69
1.73
2.75
3.68
6.06
8.66
8.78

0.38
0.69
1.76
2.77
3.69
6.08
8.73
8.86

(100%, 0%)
(100%, 0%)
(92.86%, 0.58%)
(65.00%, 2.61%)
(55.56%, 3.37%)
(17.86%, 8.21%)
(22.61%, 11.455%)
(17.37%, 18.65%)

0.02
0.10
0.35
1.17
2.79
8.45
16.69
33.19

solving time (s)
0.61
0.90
2.19
4.24
8.72
11.68
26.55
56.15

1.77
4.18
10.55
15.56
29.38
33.29
74.28
116.37

7.01
26.05
69.66
60.59
166.35
220.39
267.19
333.65

Table 11: Results obtained with sublevel relaxations of Lipschitz constant problems, ǫ = 0.1.

Sol./LB

nVar

Sublevel relaxation, lv = 0/4/6/8, p = 1 (level 0 = Shor)

upper bounds (RI, RG)

solving time (s)

net 1 5
net 1 10
net 1 15
net 1 20
net 1 25
net 1 30
net 1 35
net 1 40

0.247
0.581
1.384
1.73
2.03
4.10
5.84
5.02

15
30
45
60
75
90
105
120

0.251
0.610
1.449
2.23
2.73
5.09
7.12
7.30

0.251
0.608
1.441
2.22
2.67
5.07
7.08
7.21

0.247
0.606
1.441
2.20
2.65
5.06
7.07
7.15

0.247
0.605
1.435
2.19
2.64
5.04
7.03
7.07

(100%, 0%)
(17.2%, 4.13%)
(21.54%, 3.68%)
(8.00%, 26.59%)
(12.86%, 30.05%)
(5.05%, 22.93%)
(7.03%, 20.38%)
(10.09%, 40.84%)

0.04
0.18
0.42
4.21
4.79
19.10
56.46
144.28

0.34
0.84
1.43
3.82
7.08
13.60
28.95
58.01

1.25
4.65
7.29
13.11
23.31
28.29
47.06
80.27

6.72
38.56
60.27
83.14
134.50
146.17
192.31
254.22

Certiﬁcation instances

We use the same network net 1 n as the one generated for the above Lipschitz problems, and
compute the upper bounds corresponding to ǫ = 0.1, 10, by the sublevel relaxations of depth 1 and
level 0, 4, 6, 8, respectively.

As for the Lipschitz problem, Table 12 and 13 indicate that in the local case it is much harder
to improve and ﬁnd the exact optimal solution than in the global case. Furthermore, the diﬃculty
of the problem also increases with the dimension. When the number of variables gets larger, the
improvement ratio decreases while the relative gap increases.

Table 12: Results obtained with sublevel relaxations of certiﬁcation problems, ǫ = 10.

Sol./LB

nVar

net 1 5
net 1 10
net 1 15
net 1 20
net 1 25
net 1 30
net 1 35
net 1 40
net 1 45
net 1 50

2.63
3.49
5.61
9.24
14.40
17.22
26.71
22.94
22.57
27.34

10
20
30
40
50
60
70
80
90
100

Sublevel relaxation, lv = 0/4/6/8, p = 1 (level 0 = Shor)

upper bounds (RI, RG)

3.51
4.88
8.20
16.48
26.68
38.06
59.18
57.59
57.56
73.59

3.00
4.69
8.10
16.03
26.28
37.72
58.64
56.08
56.34
72.10

2.74
4.60
7.84
15.75
25.89
36.82
57.78
54.69
55.57
71.19

2.74
4.48
7.41
15.48
25.57
35.89
57.39
54.18
54.68
69.92

(87.50%, 4.18%)
(28.78%, 28.37%)
(30.50%, 32.09%)
(13.81%, 67.53%)
(9.04%, 77.57%)
(10.41%, 108.42%)
(5.51%, 114.86%)
(9.84%, 136.18%)
(8.23%, 142.27%)
(7.94%, 155.74%)

0.01
0.06
0.19
0.60
2.24
5.08
10.69
23.22
44.67
81.61

solving time (s)
0.26
0.99
1.02
2.35
5.67
14.32
25.79
44.74
85.38
144.61

1.79
4.23
6.93
9.69
17.40
24.02
40.96
65.04
107.12
165.18

1.83
30.36
40.31
67.31
66.99
102.93
136.35
146.91
186.94
333.42

21

Table 13: Results obtained with sublevel relaxations of certiﬁcation problems, ǫ = 0.1.

Sol./LB

nVar

Sublevel relaxation, lv = 0/4/6/8, p = 1 (level 0 = Shor)

upper bounds (RI, RG)

net 1 5
net 1 10
net 1 15
net 1 20
net 1 25
net 1 30
net 1 35
net 1 40
net 1 45
net 1 50

0.190
0.021
0.027
0.269
-0.104
0.669
0.825
0.741
0.265
0.614

10
20
30
40
50
60
70
80
90
100

0.191
0.025
0.053
0.299
-0.025
0.810
1.107
0.949
0.603
0.920

0.191
0.025
0.053
0.299
-0.028
0.807
1.107
0.943
0.602
0.919

0.191
0.025
0.053
0.299
-0.031
0.806
1.107
0.942
0.600
0.916

0.191
0.024
0.053
0.298
-0.031
0.803
1.107
0.940
0.599
0.914

(0.00%, 0.53%)
(25.00%, 14.29%)
(0.00%, 96.30%)
(3.33%, 10.78%)
(7.59%, 70.19%)
(4.96%, 20.03%)
(0.00%, 34.18%)
(4.33%, 26.86%)
(1.18%, 126.04%)
(1.96%, 48.86%)

0.01
0.15
0.17
0.79
2.37
6.34
12.61
34.70
55.69
105.68

solving time (s)
0.74
0.13
3.16
0.51
2.82
0.55
7.83
2.36
13.22
5.77
21.85
10.06
34.07
18.44
56.64
52.44
115.78
89.39
179.53
177.41

0.73
17.68
17.01
46.08
54.81
84.82
102.26
163.67
200.94
205.58

The two latter examples show us that ﬁnding the guaranteed bounds for optimization problems
arising from deep learning is much harder than the usual sparse problems coming from the classical
optimization literature. Hence, it remains a big challenge to adapt our approach to large real net-
works, involving a large number of variables and more complicated structures such as convolutional
or max-pooling layers.

6 Conclusion

In this paper, we propose a new semideﬁnite programming hierarchy based on the standard dense
and sparse Lasserre’s hierarchies. This hierarchy provides a wider choice of intermediate relaxation
levels, lying between the d-th and (d + 1)-th order relaxations in Lasserre’s hierarchy. With this
technique, we are able to solve problems where the standard relaxations are untractable. Our
experimental results demonstrate that the sublevel relaxation often allows one to compute more
accurate bounds by comparison with existing frameworks such as Shor’s relaxation or term sparsity,
in particular for dense problems.

Sublevel relaxations oﬀer a large choice of parameters tuning, as one can select the level, depth,
and subsets for each relaxation. We can beneﬁt from this to potentially perform better that state-of-
the-art methods. However, the ﬂexibility of our approach also comes together with a drawback since
the more ﬂexible it is, the more diﬃcult for the users it is to tune the parameters. One important
and interesting future topic would be to design an algorithm that searches for the optimal level,
depth and subsets in sublevel relaxations.

Acknowledgement

This work has beneﬁted from the Tremplin ERC Stg Grant ANR-18-ERC2-0004-01 (T-COPS
project), the European Union’s Horizon 2020 research and innovation programme under the Marie
Sklodowska-Curie Actions, grant agreement 813211 (POEMA) as well as from the AI Interdisci-
plinary Institute ANITI funding, through the French “Investing for the Future PIA3” program
under the Grant agreement n◦ANR-19-PI3A-0004. The third author was supported by the FMJH
Program PGMO (EPICS project) and EDF, Thales, Orange et Criteo. The fourth author acknowl-
edge the support of Air Force Oﬃce of Scientiﬁc Research, Air Force Material Command, USAF,
under grant numbers FA9550-19-1-7026, FA9550-18-1-0226, and ANR MasDol.

22

References

[1] Thomas Barthel and Robert H¨ubener. Solving condensed-matter ground-state problems by

semideﬁnite relaxations. Physical Review Letters, 108(20), May 2012.

[2] Juan S Campos, Ruth Misener, and Panos Parpas. Partial Lasserre relaxation for sparse

Max-Cut. 2020.

[3] Tong Chen, Jean B Lasserre, Victor Magron, and Edouard Pauwels. Semialgebraic Optimiza-
tion for Lipschitz Constants of ReLU Networks. Advances in Neural Information Processing
Systems, 33, 2020.

[4] Fabio Furini, Emiliano Traversi, Pietro Belotti, Antonio Frangioni, Ambros Gleixner, Nick
Gould, Leo Liberti, Andrea Lodi, Ruth Misener, Hans Mittelmann, et al. Qplib: a library of
quadratic programming instances. Mathematical Programming Computation, 11(2):237–265,
2019.

[5] Arbel Haim, Richard Kueng, and Gil Refael. Variational-correlations approach to quantum

many-body problems, 2020.

[6] C´edric Josz and Daniel K Molzahn. Lasserre hierarchy for large scale polynomial optimization

in real and complex variables. SIAM Journal on Optimization, 28(2):1017–1048, 2018.

[7] Igor Klep, Victor Magron, and Janez Povh. Sparse noncommutative polynomial optimization,

2019.

[8] Gary A Kochenberger, Jin-Kao Hao, Zhipeng L¨u, Haibo Wang, and Fred Glover. Solving large

scale max cut problems via tabu search. Journal of Heuristics, 19(4):565–571, 2013.

[9] Jean B Lasserre. Global optimization with polynomials and the problem of moments. SIAM

Journal on optimization, 11(3):796–817, 2001.

[10] Jean B Lasserre. Convergent sdp-relaxations in polynomial optimization with sparsity. SIAM

Journal on Optimization, 17(3):822–843, 2006.

[11] Jean B Lasserre, Kim-Chuan Toh, and Shouguang Yang. A bounded degree sos hierarchy for
polynomial optimization. EURO Journal on Computational Optimization, 5(1-2):87–117, 2017.

[12] Jean Bernard Lasserre. An introduction to polynomial and semi-algebraic optimization, vol-

ume 52. Cambridge University Press, 2015.

[13] Monique Laurent. A comparison of the sherali-adams, lov´asz-schrijver, and lasserre relaxations

for 0–1 programming. Mathematics of Operations Research, 28(3):470–496, 2003.

[14] Victor Magron.

Interval enclosures of upper bounds of roundoﬀ errors using semideﬁnite

programming. ACM Transactions on Mathematical Software (TOMS), 44(4):1–18, 2018.

[15] Victor Magron, George Constantinides, and Alastair Donaldson. Certiﬁed roundoﬀ er-
ror bounds using semideﬁnite programming. ACM Transactions on Mathematical Software
(TOMS), 43(4):1–31, 2017.

23

[16] Ngoc Hoang Anh Mai, Victor Magron, and Jean-Bernard Lasserre. A sparse version of

Reznick’s Positivstellensatz, 2020.

[17] A. Majumdar, A. A. Ahmadi, and R. Tedrake. Control and veriﬁcation of high-dimensional
systems with DSOS and SDSOS programming. In Proceedings of the 53rd IEEE Conference
on Decision and Control, pages 394–401. IEEE, 2014.

[18] Jiawang Nie. Optimality conditions and ﬁnite convergence of Lasserre’s hierarchy. Mathemat-

ical programming, 146(1-2):97–121, 2014.

[19] Panos M. Pardalos and A. T. Phillips. A global optimization approach for solving the maximum
clique problem. International Journal of Computer Mathematics, 33(3-4):209–216, 1990.

[20] Mihai Putinar. Positive polynomials on compact semi-algebraic sets. Indiana University Math-

ematics Journal, 42(3):969–984, 1993.

[21] K´aroly F. P´al and Tam´as V´ertesi. Quantum bounds on Bell inequalities. Physical Review A,

79(2), Feb 2009.

[22] Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Semideﬁnite relaxations for certifying
robustness to adversarial examples. In Advances in Neural Information Processing Systems,
pages 10877–10887, 2018.

[23] Franz Rendl, Giovanni Rinaldi, and Angelika Wiegele. A branch and bound algorithm for Max-
Cut based on combining semideﬁnite and polyhedral relaxations. In International Conference
on Integer Programming and Combinatorial Optimization, pages 295–309. Springer, 2007.

[24] Corbinian Schlosser and Milan Korda. Sparse moment-sum-of-squares relaxations for nonlinear
dynamical systems with guaranteed convergence. arXiv preprint arXiv:2012.05572, 2020.

[25] Matteo Tacchi, Carmen Cardozo, Didier Henrion, and Jean Lasserre. Approximating regions of
attraction of a sparse polynomial diﬀerential system. arXiv preprint arXiv:1911.09500, 2019.

[26] Matteo Tacchi, Tillmann Weisser, Jean-Bernard Lasserre, and Didier Henrion. Exploiting

sparsity for semi-algebraic set volume computation, 2019.

[27] Hayato Waki, Sunyoung Kim, Masakazu Kojima, and Masakazu Muramatsu. Sums of squares
and semideﬁnite program relaxations for polynomial optimization problems with structured
sparsity. SIAM Journal on Optimization, 17(1):218–242, 2006.

[28] Jie Wang, Martina Maggio, and Victor Magron. SparseJSR: A Fast Algorithm to Compute
Joint Spectral Radius via Sparse SOS Decompositions. arXiv preprint arXiv:2008.11441, 2020.

[29] Jie Wang and Victor Magron. Exploiting term sparsity in noncommutative polynomial opti-

mization. arXiv preprint arXiv:2010.06956, 2020.

[30] Jie Wang, Victor Magron, Jean B Lasserre, and Ngoc Hoang Anh Mai. CS-TSSOS: Correlative
and term sparsity for large-scale polynomial optimization. arXiv preprint arXiv:2005.02828,
2020.

24

[31] Jie Wang, Victor Magron, and Jean-Bernard Lasserre. Chordal-TSSOS: a moment-SOS hi-
erarchy that exploits term sparsity with chordal extension. SIAM Journal on Optimization,
31(1):114–141, 2021.

[32] Jie Wang, Victor Magron, and Jean-Bernard Lasserre. TSSOS: A Moment-SOS hierarchy that

exploits term sparsity. SIAM Journal on Optimization, 31(1):30–58, 2021.

[33] Tillmann Weisser, Jean B Lasserre, and Kim-Chuan Toh. Sparse-BSOS: a bounded degree SOS
hierarchy for large scale polynomial optimization with sparsity. Mathematical Programming
Computation, 10(1):1–32, 2018.

25

