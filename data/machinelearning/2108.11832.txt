Subgradient methods near active manifolds: saddle
point avoidance, local convergence, and asymptotic
normality

Damek Davis1

Dmitriy Drusvyatskiy2

Liwei Jiang3

Last updated: August 27, 2021

1
2
0
2

g
u
A
6
2

]

C
O
.
h
t
a
m

[

1
v
2
3
8
1
1
.
8
0
1
2
:
v
i
X
r
a

1School

of

ORIE,

Cornell

people.orie.cornell.edu/dsd95/.
research fellowship and NSF DMS award 2047637.

University,

USA;
Research of Davis supported by an Alfred P. Sloan

Ithaca,

14850,

NY

2Department

of

Mathematics,

U. Washington,

Seattle,

www.math.washington.edu/∼ddrusv.
DMS-1651851 and CCF-2023166 awards.

Research of Drusvyatskiy was

WA

98195;
supported by NSF

3School

of

ORIE,

Cornell

University.

Ithaca,

NY

14850,

USA;

orie.cornell.edu/research/grad-students/liwei-jiang

 
 
 
 
 
 
Abstract

Nonsmooth optimization problems arising in practice, whether in signal processing, statisti-
cal estimation, or modern machine learning, tend to exhibit beneﬁcial smooth substructure:
their domains stratify into “active manifolds” of smooth variation, which common proximal
algorithms “identify” in ﬁnite time. Identiﬁcation then entails a transition to smooth dynam-
ics, and permits the use of second-order information for acceleration. While identiﬁcation
is clearly useful algorithmically, empirical evidence suggests that even those algorithms that
do not identify the active manifold in ﬁnite time—notably the subgradient method—are
nonetheless aﬀected by it. This work seeks to explain this phenomenon, asking: how do
active manifolds impact the subgradient method in nonsmooth optimization?

To answer this question, our approach posits two algorithmically useful properties that
link the behavior of the function on and oﬀ the active manifold. The ﬁrst, which we call
aiming, asserts that subgradients point towards the manifold. This property ensures that,
though identiﬁcation fails, the subgradient iterates steadily approach the manifold. The sec-
ond property states that subgradients on and oﬀ the manifold are close in tangent directions
up to a linear error. This property ensures that the nonsmooth dynamics of the subgradient
method are well-approximated by their smooth shadow along the manifold, with a controlled
error. We show that these properties, while not automatic, hold for a wide class of problems,
including cone reducible/decomposable functions and generic semialgebraic problems. More-
over, we develop a thorough calculus, proving such properties are preserved under smooth
deformations and spectral lifts.

We then turn to algorithmic consequences. Here, the two pillars—aiming and subgradient
approximation—fully expose the smooth substructure of the problem, implying that the
shadow of the (stochastic) subgradient method along the active manifold is precisely an
inexact Riemannian gradient method with an implicit retraction. This viewpoint leads to
several consequences that parallel results in smooth optimization, despite the nonsmoothness
of the problem: local rates of convergence, asymptotic normality, and saddle point avoidance.
The asymptotic normality results appear to be new even in the most classical setting of
stochastic nonlinear programming. The results culminate in the following observation: the
perturbed subgradient method on generic, Clarke regular semialgebraic problems, converges
only to local minimizers.

Contents

1 Introduction

1.1 Chapter 2: Strong (a) regularity and (b) regularity.
. . . . . . . . . . . . . .
1.2 Chapter 3: Algorithmic consequences . . . . . . . . . . . . . . . . . . . . . .
1.2.1 Local rates of convergence . . . . . . . . . . . . . . . . . . . . . . . .
1.2.2 Asymptotic normality . . . . . . . . . . . . . . . . . . . . . . . . . .
Saddle point avoidance . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2.3
1.3 Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
1.4 Notation and basic constructions
1.4.1 Manifolds
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.4.2 Normal cones . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Subdiﬀerentials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.4.3
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.4.4 Active manifolds

2 Regularity along a manifold: conditions of Whitney and Verdier in nons-

mooth optimization
2.1 The four fundamental regularity conditions . . . . . . . . . . . . . . . . . . .
2.2 Relation between the four conditions
. . . . . . . . . . . . . . . . . . . . . .
2.3 Basic examples
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.4 Preservation of regularity under preimages by smooth transversal maps . . .
2.5 Preservation of regularity under spectral lifts . . . . . . . . . . . . . . . . . .
2.6 Regularity of functions along manifolds . . . . . . . . . . . . . . . . . . . . .
2.7 Generic regularity along active manifolds . . . . . . . . . . . . . . . . . . . .

3 Algorithmic Consequences: local rates of convergence, asymptotic normal-

ity, and saddle point avoidance
3.1 Algorithm and main assumptions . . . . . . . . . . . . . . . . . . . . . . . .
3.2 Example algorithms satisfying Assumption A . . . . . . . . . . . . . . . . . .
3.2.1 Projected subgradient method . . . . . . . . . . . . . . . . . . . . . .
3.2.2 Proximal gradient method . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.3.1 Pillar I: Aiming towards the manifold . . . . . . . . . . . . . . . . . .
3.3.2 Pillar II: The shadow iteration . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
3.4 Local convergence guarantees
3.5 Asymptotic normality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.5.1 Asymptotic normality in nonlinear programming . . . . . . . . . . . .

3.3 The two pillars

1

3
6
6
7
8
9
10
10
11
12
13
13

17
17
23
26
28
30
33
35

38
38
40
41
42
44
45
46
47
48
50

3.6.1 Consequences for generic semialgebraic functions

52
3.6 Avoiding saddle points . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
. . . . . . . . . . .
56
3.7 Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.8 Proofs of the two pillars
56
3.8.1 Proof of Proposition 3.3.1: aiming towards the manifold . . . . . . .
58
3.8.2 Proof of Proposition 3.3.2: the shadow iteration . . . . . . . . . . . .
61
3.9 Proofs of the main theorems . . . . . . . . . . . . . . . . . . . . . . . . . . .
62
3.9.1 Proof of Theorem 3.4.1: general rates . . . . . . . . . . . . . . . . . .
63
3.9.2 Proof of Theorem 3.4.2: rates near strong local minimizers . . . . . .
66
3.9.3 Proof of Theorem 3.5.1: asymptotic normality . . . . . . . . . . . . .
73
. . . .
3.9.4 Proof of Theorem 3.6.1: nonconvergence of stochastic process
80
3.9.5 Proof of Theorem 3.6.2: nonconvergence to saddle points . . . . . . .
88
3.10 Appendix to Part 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
88
3.10.1 Proof of Proposition 2.3.3 . . . . . . . . . . . . . . . . . . . . . . . .
89
3.11 Appendix to Part 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
89
3.11.1 Proof of Proposition 3.2.3: the projected gradient method . . . . . .
3.11.2 Proof of Proposition 3.2.5: the proximal gradient method . . . . . . .
92
3.11.3 Proof of Corollary 3.5.2: asymptotic normality in nonlinear programming 96
3.11.4 Proof of Corollary 3.6.3: avoiding active strict saddle via projected
subgradient method . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.11.5 Proof of Corollary 3.6.4: avoiding active strict saddle via proximal
gradient method . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.11.6 Proofs of Corollaries 3.6.5, 3.6.6, and 3.6.7: saddle point avoidance for
97
. . . . . . . . . . . . . . . . . . . . .
97
3.11.7 Sequences and Stochastic Processes . . . . . . . . . . . . . . . . . . .
3.11.8 Proof of Lemma 3.11.6 . . . . . . . . . . . . . . . . . . . . . . . . . . 100

generic semialgebraic problems.

96

97

2

Chapter 1

Introduction

The notion of a “smooth active manifold” appears throughout the nonsmooth optimization
literature, leading to transparent views of optimality conditions, sensitivity analysis, and
algorithmic behavior. Formal models of active manifolds include identiﬁable surfaces [95],
partly smooth manifolds [57], UV-structures [54, 67], decomposable functions [82], and min-
imal identiﬁable sets [32]. Active manifolds pervade modern problems in signal recovery, ro-
bust statistics, and deep learning, and classically feature in “active set” methods of nonlinear
programming. In these problems nonsmoothness is not pathological, but is highly-structured:
domains stratify into manifolds of smooth variation, and under strict complementarity con-
ditions, the manifolds become “active,” that is, quickly identiﬁable by common proximal
methods. Once identiﬁed, smooth dynamics of proximal methods automatically ensue along
the “active manifold,” and accommodate second-order acceleration techniques if the mani-
fold is known [60, 67]. While identiﬁcation has clear utility, empirical evidence suggests that
non-identifying algorithms, such as the subgradient method, continue to beneﬁt from the
mere existence of active manifolds. This work seeks to explain this phenomenon, asking:

How do active manifolds impact subgradient methods?

Setting the stage, consider a locally Lipschitz function f on Rd and let ¯x be a ﬁrst-order
critical point of f in the sense that the directional derivative of f at ¯x is nonnegative in
every direction. Following [32], a smooth submanifold M of Rd is called active for f at ¯x if
(i) the restriction of f to M is smooth near ¯x, and (ii) all points x near ¯x with small Clarke
subgradients w ∈ ∂cf (x) must lie in M (see Figure 1.1).

The algorithmic importance of active manifolds stems from the following observation:
any algorithm that generates a sequence of points xk converging to ¯x, along with “dual
certiﬁcates” wk ∈ ∂cf (xk) tending to zero, must eventually identify the manifold M in the
sense that all the iterates xk eventually lie in M. Once M is identiﬁed, the nonsmoothness
of the problem is largely irrelevant, since all future iterates lie on a smooth manifold along
which f is smooth. Not all algorithms generate vanishing dual certiﬁcates, but several
important examples exist, such as the proximal gradient method [42], the dual averaging
procedure [53], and various operator-splitting schemes [58,62]. An important requirement of
all such methods, however, is that there exist an explicit decomposition of the problem into
smooth and proximable terms.

3

(a) The function f (x, y)

Figure 1.1: The y-axis is an active manifold for the function f (x, y) = |x| − y2 at the origin.

(b) Subgradient ﬂow ˙γ ∈ −∂f (γ)

The subgradient method, on the other hand, is an important algorithm that does not
generate asymptotically vanishing dual certiﬁcates. Given a control sequence αk > 0 the
method repeats the steps:

xk+1 = xk − αkwk

where wk ∈ ∂cf (xk).

(1.0.1)

Though the subgradient method does not identify the active manifold in ﬁnite time, it is
empirically strongly inﬂuenced by it. As an illustration, Figure 1.1 depicts a nonsmooth
function, having the x-axis as its active manifold. The function has a unique critical point
at the origin and directions of second-order negative curvature along the active manifold—a
so-called strict saddle point. As one would hope for, Figure 1.1b shows the continuous time
analogue of the subgradient method, when randomly initialized, almost surely avoids the
origin, and moreover, one can check that similar behavior persists in discrete time. Thus,
although the subgradient method never reaches the active manifold, it nonetheless inherits
desirable properties from the function along the manifold, e.g., saddle point avoidance.

The purpose of this work is to oﬀer a revealing explanation of the dynamics of the
subgradient method near active manifolds. Our central observation is that under two mild
regularity conditions, which we will describe shortly, the following is true:

the shadow of the (stochastic) subgradient method along the active manifold is
precisely an inexact Riemannian gradient method with an implicit retraction.

More formally, we will ﬁnd that the shadow sequence yk = PM(xk), satisﬁes a recursion

yk+1 = yk − αk∇Mf (yk) + O(αkdist(xk, M) + α2

k),

(1.0.2)

near ¯x, where ∇Mf denotes the covariant gradient of f along M.1 This observation allows
us to infer desirable properties for the subgradient sequence xk from those of its shadow yk.

1The covariant gradient ∇Mf (y) is the projection onto TM(y) of ∇ ˆf (y) where ˆf is any C 1 smooth

function deﬁned on a neighborhood U of ¯x and that agrees with f on U ∩ M.

4

-3-2-10123-3-2-10123Concretely, such desirable properties include local rates of convergence, asymptotic normality
of the iterates, and saddle point avoidance.

To link the iterates of the subgradient method with a Riemannian gradient sequence

along the active manifold, we require two regularity properties that we now describe.

Regularity property I: aiming towards the manifold. Although the subgradient
method fails to identify the active manifold, the iterates it generates often steadily approach
the manifold. In this work, we will show that a suﬃcient condition for this behavior is the
following proximal aiming inequality:

(cid:104)v, x − PM(x)(cid:105) ≥ c · dist(x, M)

for all x near ¯x and v ∈ ∂fc(x).

(1.0.3)

In words, the proximal aiming inequality asserts that subgradients are well aligned with
directions pointing towards the nearest point on the manifold. A natural question is when
should we expect (1.0.3) to hold. For example, it is not hard to see that (1.0.3) holds if f
is weakly convex;2 indeed, this follows directly from [25, Theorem D.2]. Weak convexity is
not essential, however; instead (1.0.3) holds under the following property that is unrelated
to convexity:

f (x) ≥ f (PM(x)) + (cid:104)v, x − PM(x)(cid:105) + o(dist(x, M))

as x → ¯x with v ∈ ∂cf (x). (1.0.4)

We call this condition (b)-regularity of f along M at ¯x, for reasons that will become apparent
shortly. In the simplest setting when M is the singleton {¯x}, this condition is precisely (one-
sided) semi-smoothness of the function f in the sense of [66]. More generally, the condition
(1.0.4) is a uniformization of semi-smoothness relative to M. Summarizing, (b)-regularity
along the active manifold implies the key aiming condition (1.0.3).

Regularity property II: subgradients on and oﬀ the manifold. The second regular-
ity property posits that subgradients on and oﬀ the manifold are aligned in tangent directions
up to a linear error, that is, there exists C > 0 satisfying

(cid:107)PTM(y)(∂cf (x) − ∇Mf (y))(cid:107) ≤ C · (cid:107)x − y(cid:107)

for all x ∈ Rd and y ∈ M near ¯x.

(1.0.5)

Whenever (1.0.5) holds, we say that f is strongly (a)-regular along M, for reasons that will
become apparent shortly. An illustrative example is shown in Figure 1.1: there the function
f is smooth in tangent directions, implying the relation between subgradients on and oﬀ the
manifold. In general, strong (a) regularity requires f to vary smoothly in tangent directions
to the manifold only up to a linear error.

The rest of this introduction discusses the main contents of this work, which are broken
down into two chapters. Chapter 2 develops the strong (a) and (b) regularity properties
including basic examples, calculus, and genericity guarantees. Chapter 3 of the paper focuses
on algorithmic consequences. We now describe these parts in detail.

2A function f is weakly convex if the perturbed function x (cid:55)→ f (x) + ρ

2 (cid:107)x(cid:107)2 is convex for some ρ > 0.

5

1.1 Chapter 2: Strong (a) regularity and (b) regular-

ity.

Conditions (b) and strong (a) play a central role in our work and are explored in detail
in Chapter 2. The two properties, both in content and name, are entirely motivated by
classical regularity conditions in stratiﬁcation theory due to Whitney [92–94], Kuo [46],
and Verdier [90]. There is an important distinction, however, that is worth emphasizing.
Regularity conditions in stratiﬁcation theory deal with compatibility between two smooth
manifolds; the central goal is to prove that reasonable sets (e.g.
semi-algebraic) can be
partitioned into smooth manifolds so that the regularity condition holds for every adjacent
pair of manifolds. In contrast, we will be concerned with compatibility between a speciﬁc
nonsmooth set—the epigraph of f —and the speciﬁc manifold—the graph of the restriction
of f to the active manifold M.

The broad goal of the chapter is to convince the reader that conditions (b) and strong (a)
are common in optimization problems and are easy to work with. To that end, we begin by
deﬁning regularity conditions for sets, focusing on building geometric intuition, and then pass
to functions by means of epigraphs. A basic question arises immediately: are conditions (b)
and strong (a) related? We provide a satisfying answer generalizing the results of Kuo [45],
Verdier [90], and Ta Le Loi [48] in stratiﬁcation theory. We will show that if f and M are
deﬁnable in an o-minimal structure, then strong (a) regularity implies condition (b). Thus,
in most interesting examples, condition (b) follows automatically from strong (a).

Having developed basic deﬁnitions and explored their interplay, we present a number
of examples that are relevant to optimization.
In particular, we show that a sublinear
function is both (b) and strongly (a) regular along its lineality space. We then develop a
thorough calculus, which in particular implies that the regularity conditions are preserved
under a transversal pre-composition with a smooth map. Thus, decomposable functions
of [82], under a transversality condition, are both (b) and strongly (a) regular along their
active manifolds. We moreover argue that the two conditions are common in eigenvalue
problems because they satisfy the so-called transfer principle. Namely, any orthogonally
invariant function of symmetric matrices will satisfy the regularity condition, as long as
its restriction to diagonal matrices satisﬁes the analogous property. We end Chapter 2 of
the paper by showing that if f is a deﬁnable function, then for almost all perturbations
v ∈ Rd, the tilted function fv(x) = f (x) − (cid:104)v, x(cid:105) admits at most ﬁnitely many ﬁrst-order
critical points, each lying on an active manifold along which fv is both (b) and strongly (a)
regular. Summarizing the chapter, typical functions, whether built from concrete structured
examples or from unstructured linear perturbations, admit an active manifold around each
critical point along which the objective function is both (b) and strongly (a) regular.

1.2 Chapter 3: Algorithmic consequences

Chapter 3 of the paper develops algorithmic consequences of strong (a)-regularity and (b)
regularity along the active manifold: local rates of convergence, asymptotic normality, and
saddle-point avoidance. Throughout the paper we consider a broad family of algorithms,

6

but our guarantees are easiest to state for the constrained minimization problem

min
x∈X

f (x),

(1.2.1)

where X ⊆ Rd is closed and f is locally Lipschitz. Active manifolds M ⊆ X and the two
regularity conditions naturally extend to this setting by adding to f the indicator function of
X ; see section 1.4.4 for details. For this problem class, we consider the stochastic projected
subgradient method, which repeats the steps:

xk+1 = PX (xk − αk(wk + νk))

where wk ∈ ∂cf (xk)

(1.2.2)

and νk is zero mean stochastic error and αk = 1/kγ for γ ∈ (1/2, 1). For this algorithm, we
will show that the shadow sequence

yk = PM(xk)

is still locally an inexact stochastic Riemannian gradient sequence with implicit retraction as
in (1.0.2). Building on this observation, we extend several classical properties of (stochastic)
gradient method sequences to our setting.

1.2.1 Local rates of convergence

The most immediate consequence of (1.0.2) is that the covariant gradient tends to zero
at a controlled rate along the shadow of the iterate sequence: around every critical point ¯x
contained in an active manifold, if xk enters and remains indeﬁnitely in a small neighborhood
of ¯x, then

min
i=1,...,k

dist(xi, M) = O

(cid:19)

(cid:18) 1
kγ−(cid:15)

and

min
i=1,...,k

(cid:107)∇Mf (yi)(cid:107) = O

(cid:18) 1

k(1−γ)/2

(cid:19)

.

for every (cid:15) > 0.3
If the problem (1.2.1) satisﬁes further regularity properties, we prove
stronger rates of convergence for the distance (cid:107)xk − ¯x(cid:107) itself. For example, the standard
second-order suﬃcient condition for optimality asks that ∇2
Mf (¯x) is positive deﬁnite on the
tangent space TM(¯x): there exists σ > 0 satisfying

u(cid:62)∇2

Mf (¯x)u ≥ σ(cid:107)u(cid:107)2

for all u ∈ TM(¯x).

(1.2.3)

Here ∇2
and remains indeﬁnitely in a small neighborhood of ¯x, then

Mf denotes the covariant Hessian of f .4 In this setting, we show that if xk enters

min
i=1,...,k

(cid:107)xi − ¯x(cid:107)2 = O

(cid:18) 1
kγ−(cid:15)

(cid:19)

.

for all (cid:15) > 0. Intriguingly, this rate is near optimal, even for stochastic gradient methods in
smooth and strongly convex optimization problems [71].

3The constants in the O term also depend on the sample path throughout.
4When f is C 3-smooth, ∇2

Mf (¯x) is simply the compression of ∇2(f ◦PM)(¯x) to the tangent space TM(¯x).

7

1.2.2 Asymptotic normality

k(¯xk − ¯x), where ¯xk := 1
k

Polyak and Juditsky [78] famously showed that the stochastic gradient method for minimizing
smooth and strongly convex functions enjoys a central limit theorem: the error sequence
√
i=1 xi is an average iterate, converges in distribution to a
normal random vector. Moreover, the covariance matrix of the limiting distribution depends
on the Hessian of f and the covariance of the stochastic gradient at the minimizer.
In
this section, we show that an analogous property holds under reasonable assumptions in
nonsmooth optimizations, with the Hessian of f replaced by its covariant counterpart.

(cid:80)k

While we prove a more general result in Chapter 3, the guarantees are already interesting

in the case of constrained smooth minimization:

f (x) = Ez∼P [f (x; z)]

min
x∈X

(1.2.4)

where P is a ﬁxed, unknown probability distribution, and for each z the function x (cid:55)→ f (x; z)
is C 1. Algorithm 1.2.2 then becomes the stochastic projected gradient method:

Sample: zk ∼ P
Update: xk+1 ∈ PX (xk − αk∇f (xk; zk)).

(1.2.5)

For the class of problems (1.2.4), we show that under classical second order suﬃcient opti-
mality condition (1.2.3) and other mild technical assumptions, the following holds:

If xk converges to ¯x with probability 1, the average iterate ¯xk = 1
k

(cid:80)k

i=1 xi satisﬁes

√

k(¯xk − ¯x) d−→ N (cid:0)0, ∇2

Mf (¯x)†Cov(∇f (¯x, z))∇2

Mf (¯x)†(cid:1) .

Convergence with probability 1 in turn is automatic if f has a unique critical point ¯x and
supk (cid:107)xk(cid:107) < ∞ almost surely. To the best of our knowledge, this is the ﬁrst asymptotic
normality guarantee for the standard stochastic projected gradient method even in the non-
smooth convex setting.

The result appears to be new and interesting for classical nonlinear programming prob-
lems under linear independence of the active constraints, strict complementarity, and strong
second order suﬃcient conditions for optimality. Note that in this case, the covariant Hes-
sian coincides with the Hessian of the Lagrangian pre and post multiplied by the projection
In this context, according to [33, Theorem 1] the result is
onto the tangent space at ¯x.
essentially unimprovable and matches the estimation quality of empirical risk minimization
methods [81, Theorem 3.3].
Interestingly, it is known that even for the problem of min-
imizing a the expectation of a linear function over a ball, dual averaging [72] procedures
may achieve suboptimal covariance [33, Section 5.2]. This is surprising to us since (stochas-
tic) dual averaging procedures provably identify the active manifold in ﬁnite time [52] (also
see [33, Section 4.1]), which intuitively suggests a transition to smooth dynamics, a setting
where asymptotic normality traditionally holds with the optimal covariance. In contrast, the
projected stochastic gradient method does not identify constraints, and yet exhibits optimal
behavior.

8

1.2.3 Saddle point avoidance

The seminal papers [49, 50] prove that simple iterative methods (e.g., gradient descent) for
C 2 optimization avoid all strict saddle points (critical points that have negative curvature),
when randomly initialized. If all saddle points are strict, such methods converge to local
minimizers. Leveraging this result, further works (e.g., [4,37,38,85,86]) veriﬁed that a wealth
of concrete statistical estimation and learning problems possessed this strict-saddle property
and also had no spurious local minimizers, implying that simple randomly initialized methods
converge to global minimizers. Recent work extended these results to C 2 smooth manifold
constrained optimization [17, 36, 87]. Other extensions to nonsmooth convex constraint sets
are second-order, requiring at every step to minimize a nonconvex quadratic over a convex
set, which is NP hard in general [41, 69, 73].

A long-standing open question is how to extend these results to simple iterative methods
for nonsmooth optimization. In general, this is a diﬃcult task, since even gradient methods
on C 1 functions may converge to saddle points from a positive measure set of initializations,
as shown in [24]. To overcome this issue, the recent work [24] provided a new approach,
introducing a novel saddle-point concept called an active strict saddle. These are critical
points ¯x which admit an active manifold M for which ∇2
Mf (¯x) has a negative eigenvalue.
The work [24] then proved that several randomly initialized proximal algorithms avoid active
strict saddles of weakly convex functions with probability 1. Following [24], the works [21,44]
showed how to escape active strict saddles of weakly convex functions at a controlled rate,
using randomly perturbed proximal methods. The work [21] in addition treats a variant of
the subgradient method that is based on inexact, high-accuracy evaluations of the proximal
operator of f , which is computationally expensive.

While interesting, these works leave open the question

Do standard projected subgradient methods avoid active strict saddle points?

To understand why this question is diﬃcult, let us recall the argument of [24]: Near active
strict saddle points of weakly convex functions, the update mapping of common proximal
methods inherit the smoothness of f along the active manifold and have an “unstable ﬁxed-
point.” Consequently, the classical center-stable manifold [43, 83, 84] may be applied and
ensures nonconvergence.
In contrast, the update mapping of the projected subgradient
method does not inherit the smoothness of f along the manifold, rendering the argument
of [24] inapplicable. One may hope that the existence of the shadow iteration (3.3.2) might
allow one to apply a “center-stable manifold theorem with errors,” but to the best of our
knowledge, such theorems (e.g., [91]) require errors to behave in a Lipschitz fashion, which
appears untrue of the error in the shadow iteration (1.0.2). Thus, we resort to another
technique from the stochastic approximation literature: random perturbations [2, 3, 13, 74].
A central consequence of the results [2, 3, 13, 74] is that gradient descent with uniform
random perturbations converge to strict saddle points only with probability zero. In this
work, we extend these results to the shadow sequence yk, showing that, under reasonable
assumptions (e.g., Clarke regularity)

for problems (1.2.2) that are (b) regular and strongly (a)-regular, the perturbed
projected subgradient method with noise νk uniformly distributed in the ball,
converges to an active strict saddle point only with probability zero.

9

Moreover, based on the techniques of Part I, we are able to show that for generic semialgebraic
Clarke regular problems, every (composite) critical point is either an active strict saddle point
or a local minimizer. As a consequence, the paper culminates in the observation:

The perturbed projected subgradient method, on generic Clarke regular semial-
gebraic problems, converges only to local minimizers.

This result is striking when contrasted with the state-of-the-art results on the projected
subgradient method, which only ensure convergence to critical points [26, 64].

In the ﬁnal stages of completing this manuscript we became aware of the concurrent and
independent work [5], which proves a similar result. The two papers, in large part, share the
same core ideas, rooted in strong (a) regularity and proximal aiming.

1.3 Outline

The remainder of the paper is organized as follows. In the rest of this chapter, we introduce all
the necessary preliminaries that will be used in the the rest of the paper. These preliminaries
include sections on manifolds 1.4.1, normal cones 1.4.2, subdiﬀerentials 1.4.3, and active
manifolds 1.4.4. The results in these sections will be routinely used in the rest of the paper.
Chapter 2 discusses regularity properties of sets and functions along active manifolds. The
goal of the section is to connect strong (a) and (b) regularity to classical “compatibility”
conditions of stratiﬁcation theory, as well as to develop a robust calculus. The section closes
with a theorem asserting that conditions (b) and strong (a) hold along the active manifold
around any limiting critical of generic semialgebraic problems.

Chapter 3 presents algorithmic consequences of (b) and strong (a) regularity: local rates
of convergence, asymptotic normality, and saddle-point avoidance. The ﬁrst several sections
of the chapter outlines the main results, while the remaining sections consist of detailed
proofs.

1.4 Notation and basic constructions

We follow standard terminology and notation of variational analysis, following mostly closely
the monograph of Rockafellar-Wets [80]. Other inﬂuential treatments of the subject include
[10, 15, 70, 75]. Throughout, we let E and Y denote Euclidean spaces with inner products
denoted by (cid:104)·, ·(cid:105) and the induced norm (cid:107)x(cid:107) = (cid:112)(cid:104)x, x(cid:105). The symbol B will stand for the
closed unit ball in E, while Br(x) will denote the closed ball of radius r around a point x.
The closure of any set Q ⊂ E will be denoted by cl Q, while its convex hull will be denoted
by conv Q. The relative interior of a convex set Q will be written as ri Q. The lineality space
of any convex cone is the linear subspace Lin(Q) := Q ∩ −Q.

For any function f : E → R ∪ {+∞}, the domain, graph, and epigraph are deﬁned as

dom f := {x ∈ E : f (x) < ∞},
gph f := {(x, f (x)) ∈ E × R : x ∈ dom f },
epi f := {(x, r) ∈ E × R : r ≥ f (x)},

10

respectively. If M is some subset of E, the symbol f (cid:12)
(cid:12)M denotes the function obtained by
restricting f to M and we set gph f (cid:12)
(cid:12)M := (gph f ) ∩ (M × R). We say that f sublinear if its
epigraph is a convex cone, and we then deﬁne the lineality space of h to be Lin(h) := {x :
h(x) = −h(−x)}. The graph of h restricted to Lin(h) is precisely the lineality space of epi h.

The distance and the projection of a point x ∈ E onto a set Q ⊂ E are

d(x, Q) := inf
y∈Q

(cid:107)y − x(cid:107)

and

PQ(x) := argmin

y∈Q

(cid:107)y − x(cid:107),

respectively. The indicator function of a set Q, denoted by δQ : E → R ∪ {∞}, is deﬁned to
be zero on Q and +∞ oﬀ it.

For any closed two cones U, V ⊂ E, we deﬁne the gap of U to V as

∆(U, V ) := sup{dist(u, V ) : u ∈ U, (cid:107)u(cid:107) = 1}.

In particular, the containment U ⊂ V holds if and only if the gap ∆(U, V ) is zero. If U and
V are linear subspaces, one may equivalently write:

∆(U, V ) = (cid:107)PV ⊥PU (cid:107)op = (cid:107)PU PV ⊥(cid:107)op = ∆(V ⊥, U ⊥),

where (cid:107) · (cid:107)op denotes the operator norm and ⊥ denotes the orthogonal complement.

A set-valued map F : E ⇒ Y is an assignment of points x ∈ E to subsets F (x) ⊂ Y.
The map F is called inner-semicontinuous at a point ¯x ∈ E if for any vector ¯y ∈ F (¯x) and
any sequence xi → ¯x, there exists a sequence yi ∈ F (xi) converging to ¯y.

1.4.1 Manifolds

We next recall a few basic properties of smooth embedded submanifolds of E. For details,
we refer the reader to the recent monograph on manifold optimization [11] and the classical
text on smooth manifolds [51]. A set M ⊂ E is called a C p manifold (with p ≥ 1) if
around any point x ∈ M there exists an open neighborhood U ⊂ E and a C p-smooth
map F from U to some Euclidean space Y such that the Jacobian ∇F (x) is surjective and
equality M ∩ U = F −1(0) holds. Then the tangent and normal spaces to M at x are
deﬁned as TM (x) := Null (∇F (x)) and NM (x) := (TM (x))⊥, respectively. Note that for
C p manifolds M with p ≥ 1, the projection PM is C p−1-smooth on a neighborhood of
each point x in Q, and is C p smooth on the tangent space TM (x) [68]. Moreover, we have
range(∇PM(x)) ⊆ TM(x) for all x near M and the equality ∇PM(x) = PTM(x) holds for all
x ∈ M. That is, the Jacobian of the projection onto a manifold at a point on the manifold
is simply the orthogonal projection onto the tangent space.

Let M ⊂ E be a C p-manifold for some p ≥ 1. Then a function f : M → R is called
C p-smooth around a point x ∈ M if there exists a C p function ˆf : U → R deﬁned on an
open neighborhood U of x and that agrees with f on U ∩ M . Then the covariant gradient
of f at x is deﬁned to be the vector

∇M f (x) := PTM (x)(∇ ˆf (x)).

11

When f and M are C 2-smooth, the covariant Hessian of f at x is deﬁned to be the unique
self-adjoint bilinear form ∇2

M f (x) : TM (x) × TM (x) → R satisfying

(cid:104)∇2

M f (x)u, u(cid:105) =

d2
dt2 f (PM (x + tu)) |t=0

for all u ∈ TM (x).

If M is a C 3-smooth manifold, then the composition F := f ◦ PM is C 2-smooth near x and
we can identify ∇2

M f (x) with the matrix PTM(x)∇2F (x)PTM(x).

1.4.2 Normal cones

The symbol “o(h) as h → 0” stands for any univariate function o(·) satisfying o(h)/h → 0 as
h (cid:38) 0. The Fr´echet normal cone to a set Q ⊂ E at a point x ∈ E, denoted ˆNQ(x), consists
of all vectors v ∈ E satisfying

(cid:104)v, y − x(cid:105) ≤ o((cid:107)y − x(cid:107)) as

y → x in Q.

(1.4.1)

The set-valued map x (cid:55)→ ˆNQ does not have a closed graph, in general. With this in mind,
the limiting normal cone to Q at x ∈ Q, denoted by NQ(x), is deﬁned to consist of all vectors
v ∈ E for which there exist sequences xi ∈ Q and vi ∈ ˆNQ(xi) satisfying (xi, vi) → (x, v). The
Clarke normal cone is the closed convex hull N c

Q(x) = cl conv NQ(x). Thus the inclusions

ˆNQ(x) ⊂ NQ(x) ⊂ N c

Q(x),

(1.4.2)

hold for all points x ∈ Q. The set Q is called Clarke regular at ¯x ∈ Q if Q is locally closed
Q(x) = ˆNQ(¯x) holds. In this case, all the inclusions in (1.4.2) hold
around ¯x and equality N c
with equalities.

A particular large class of Clarke regular sets consists of those called prox-regular. Fol-
lowing [16, 76], a locally closed set Q ⊂ E is called prox-regular at ¯x ∈ Q if the projection
PQ(x) is a singleton set for all points x near ¯x. The result [76, Theorem 1.3] shows that
a locally closed set Q is prox-regular at ¯x ∈ Q if and only if there exist constants (cid:15), ρ > 0
satisfying

(cid:104)v, y − x(cid:105) ≤

(cid:107)y − x(cid:107)2,

ρ
2

for all y, x ∈ Q∩B(cid:15)(¯x) and all normal vectors v ∈ NQ(x)∩(cid:15)B. Consequently, prox-regularity
amounts to a possibility of replacing the little-o term in (1.4.1) with a simple quadratic,
whose amplitude is independent of the base-point x and the normal vector v. If Q is prox-
regular at ¯x, then the projection PQ(·) is automatically locally Lipschitz continuous around
¯x [76, Theorem 1.3]. Prox-regularity admits a convenient calculus. In particular, common
examples of prox-regular sets are convex sets and C 2 manifolds, as well as sets cut out by
ﬁnitely many C 2 inequalities under the Mangasarian-Fromovitz constraint qualiﬁcation [77].
Prox-regular sets are closely related to proximally smooth sets of [16] and sets with positive
reach of [35].

12

1.4.3 Subdiﬀerentials

Generalized gradients of functions can be deﬁned through the normal cones to epigraphs.
Namely, consider a function f : E → R ∪ {∞} and a point x ∈ dom f . The Fr´echet, limiting,
and Clarke subdiﬀerentials of f at x are deﬁned, respectively, as

ˆ∂f (x) := {v ∈ E : (v, −1) ∈ ˆNepi f (x, f (x))},
∂f (x) := {v ∈ E : (v, −1) ∈ Nepi f (x, f (x))},
∂cf (x) := {v ∈ E : (v, −1) ∈ N c
epi f (x, f (x))}.

(1.4.3)

Explicitly, the inclusion v ∈ ˆ∂f (x) amounts to requiring the lower-approximation property:

f (y) ≥ f (x) + (cid:104)v, y − x(cid:105) + o((cid:107)y − x(cid:107)) as

y → x.

Moreover, a vector v lies in ∂f (x) if and only if there exist sequences xi ∈ E and Fr´echet
subgradients vi ∈ ˆ∂f (xi) satisfying (xi, f (xi), vi) → (x, f (x), v) as i → ∞. If f is locally Lip-
schitz continuous around x, then equality ∂cf (x) = conv ∂f (x) holds. A point ¯x satisfying
0 ∈ ∂f (x) is called critical for f , while a point satisfying 0 ∈ ∂cf (x) is called Clarke critical.
Clearly, the latter requirement may be much weaker than the former. The distinction dis-
appears for subdiﬀerentially regular functions. We say that f is subdiﬀerentially regular at
x ∈ dom f if the set epi f is Clarke regular at (x, f (x)).

The three subdiﬀerentials deﬁned in (1.4.3) fail to capture the horizontal normals to the
epigraph—meaning those of the form (v, 0). Such horizontal normals play an important role
in variational analysis, in particular for subdiﬀerential calculus. Consequently, we deﬁne the
limiting and Clarke horizon subdiﬀerentials, respectively, by:

∂∞f (x) := {v ∈ E : (v, 0) ∈ Nepi f (x, f (x))},
c f (x) := {v ∈ E : (v, 0) ∈ N c
∂∞
epi f (x, f (x))}.

A function f : E → R ∪ {∞} is called ρ-weakly convex if the quadratically perturbed
function x (cid:55)→ f (x) + ρ
2 (cid:107)x(cid:107)2 is convex. Weakly convex functions are subdiﬀerentially regular.
Indeed, the subgradients of a ρ-weakly convex function yield quadratic minorants, meaning

f (y) ≥ f (x) + (cid:104)v, y − x(cid:105) −

ρ
2

(cid:107)y − x(cid:107)2

all points x, y ∈ dom f and all subgradients v ∈ ∂f (x). The epigraph of any weakly convex
function is a prox-regular set at each of its points. Weakly convex functions are widespread
in applications, with the primary example being compositions of Lipschitz continuous convex
functions with C 1 maps. We refer the reader to [23, 30] for an extensive discussion of this
function class in contemporary applications.

1.4.4 Active manifolds

Critical points of typical nonsmooth functions lie on a certain manifold that captures the
activity of the problem in the sense that critical points of slight linear tilts of the function
do not leave the manifold. Such active manifolds have been modeled in a variety of ways,

13

including identiﬁable surfaces [95], partly smooth manifolds [57], UV-structures [54,67], g ◦F
decomposable functions [82], and minimal identiﬁable sets [32].

In this work, we adopt the following formal model of activity, explicitly used in [32], where
the only diﬀerence is that we focus on the Clarke subdiﬀerential instead of the limiting one.

Deﬁnition 1.4.1 (Active manifold). Consider a closed function f : Rd → R ∪ {∞} and
ﬁx a set M ⊆ Rd containing a point ¯x satisfying 0 ∈ ∂cf (¯x). Then M is called an active
C p-manifold around ¯x if there exists a constant (cid:15) > 0 satisfying the following.

• (smoothness) The set M ∩ U is a C p-smooth manifold and the restriction of f to

M ∩ U is C p-smooth.

• (sharpness) The lower bound holds:

inf{(cid:107)v(cid:107) : v ∈ ∂cf (x), x ∈ U \ M} > 0,

where we set U = {x ∈ B(cid:15)(¯x) : |f (x) − f (¯x)| < (cid:15)}.

The sharpness condition simply means that the subgradients of f must be uniformly
bounded away from zero at points oﬀ the manifold that are suﬃciently close to ¯x in distance
and in function value. The localization in function value can be omitted for example if f is
weakly convex or if f is continuous on its domain; see [32] for details.

Intuitively, the active manifold has the distinctive feature that the the function grows
linearly in normal directions to the manifold; see Figure 1.1a for an illustration. This is
summarized by the following theorem from [25, Theorem D.2].

Proposition 1.4.2 (Identiﬁcation implies sharpness). Suppose that a closed function f : E →
R ∪ {∞} admits an active manifold M at a point ¯x satisfying 0 ∈ ˆ∂f (¯x). Then there exist
constants c, (cid:15) > 0 such that

f (x) − f (PM(x)) ≥ c · dist(x, M),

∀x ∈ B(cid:15)(¯x).

(1.4.4)

Notice that there is a nontrivial assumption 0 ∈ ˆ∂f (¯x) at play in Proposition 1.4.2.
Indeed, under the weaker inclusion 0 ∈ ∂cf (¯x) the growth condition (1.4.4) may easily
fail, as the univariate example f (x) = −|x| shows. It is worthwhile to note that under the
assumption 0 ∈ ˆ∂f (¯x), the active manifold is locally unique around ¯x. The following theorem
is proved in [32, Proposition 8.2].

Theorem 1.4.3 (Local uniqueness of the active manifold). Suppose that a closed function
f : E → R∪{∞} admits two C 1 active manifolds M and L at a point ¯x satisfying 0 ∈ ˆ∂f (¯x).
Then there exists a neighborhood U of ¯x such that the equality holds:

M ∩ U = L ∩ U.

Active manifolds are useful in optimization because they allow to reduce many questions
about nonsmooth functions to a smooth setting. In particular, the notion of a strict saddle
point of smooth functions naturally extends to a nonsmooth setting. The following deﬁnition
is taken from [22].

14

Deﬁnition 1.4.4 (Active strict saddle). Fix an integer p ≥ 2 and consider a closed function
f : E → R ∪ {∞} and a point ¯x satisfying 0 ∈ ∂cf (¯x). We say that ¯x is a C p strict
active saddle point of f if f admits a C p active manifold M at ¯x such that the inequality
(cid:104)∇2

Mf (¯x)u, u(cid:105) < 0 holds for some u ∈ TM(¯x).

Figure 1.1 presents an example of an active strict saddle point of a nonsmooth function
and its subgradient ﬂow. Notice that the subgradient ﬂow, when initialized uniformly at
random, almost surely escapes the strict saddle point. The recent work [22] established this
phenomenon for discrete proximal type methods, whereas in this work we investigate such
behavior for the pure subgradient algorithm—a much more nuanced task.

It is often convenient to think about active manifolds of slightly tilted functions. There-
fore, we say that M is an active C p manifold of f at ¯x for v ∈ ∂cf (¯x) if M is an active C p
manifold for the tilted function x (cid:55)→ f (x) − (cid:104)v, x(cid:105) at ¯x. Active manifolds for sets are deﬁned
through their indicator functions. Namely a set M ⊂ Q is an active C p manifold of Q at
¯x ∈ Q for v ∈ N c

Q(¯x) if it is an active C p manifold of the indicator function δQ at ¯x for v.

A large class of sets admitting active manifolds is comprised of cone-reducible sets, in-
troduced in [9]. Roughly speaking, these sets are smooth deformations of closed convex
cones.

Deﬁnition 1.4.5 (Cone reducible sets). A set Q ⊂ E is C p cone-reducible at ¯x ∈ Q if there
exists a neighborhood U of ¯x, a C p map F from U to some Euclidean space Y with surjective
Jacobian ∇F (¯x) and F (¯x) = 0, and a pointed closed convex cone C ⊂ Y satisfying

We then say that Q is C p cone reducible to C by F at ¯x.

Q ∩ U = {x ∈ U : F (x) ∈ C}.

We refer the reader to [9] for the numerous examples of cone reducible sets in optimization.
Active manifolds of cone reducible sets admit the following simple description [57, Theorem
4.2].

Theorem 1.4.6 (Active manifolds of cone reducible sets). Suppose that a set Q is C 1
cone-reducible to C by F at ¯x. Then the pair F −1(0) is an active manifold at ¯x for any
v ∈ ri NQ(x).

A functional analogue of cone reducible sets consists of decomposable functions, intro-
duced in [82]. Roughly speaking; these functions are smooth deformations of sublinear
functions.

Deﬁnition 1.4.7 (Decomposable functions). A function f : E → R ∪ {∞} is called properly
C p decomposable at ¯x as h ◦ c if on a neighborhood of ¯x it can be written as

f (x) = f (¯x) + h(c(x))

for some C p-smooth mapping c : E → Y satisfying c(¯x) = 0 and some proper, closed sublinear
function h : Y → R satisfying the transversality condition:

Lin(h) + Range(∇c(¯x)) = Y.

15

Any C 1 decomposable function admits active manifolds in the following sense [82, p 683].

Lemma 1.4.8 (Decomposable functions admit active manifolds). Suppose that f : E →
R ∪ {∞} is properly C p decomposable at ¯x as h ◦ c. Then the set M = c−1(Lin(h)) is a
C p-active manifold around ¯x for any subgradient v ∈ ri ∂f (¯x).

Cone reducible sets and properly decomposable functions form widespread examples for

which all of our techniques apply.

16

Chapter 2

Regularity along a manifold:
conditions of Whitney and Verdier in
nonsmooth optimization

2.1 The four fundamental regularity conditions

This section introduces compatibility conditions between two sets, motivated by the pioneer-
ing works of Whitney [92–94], Kuo [46], and Verdier [90]. Our discussion builds on the recent
survey of Trotman [88]. We illustrate the deﬁnitions with examples and prove basic relations
between them. It is important to note that these classical works focused on compatibility
conditions between smooth manifolds, wherein primal (tangent) and dual (normal) based
characterizations are equivalent. In contrast, it will be more expedient for us to base deﬁni-
tions on normal vectors instead of tangents. The reason is that when applied to epigraphs,
such conditions naturally imply some regularity properties for the subgradients, which in
turn underpin all algorithmic consequences in Chapter 3.

Throughout this section, we ﬁx two sets X and Y that are locally closed around a point
¯x ∈ Y. The reader should keep in my mind the most important setting when Y is a
smooth manifold contained in the closure of X . We begin with the two classical conditions,
introduced by Whitney in [93, 94].

Deﬁnition 2.1.1 (Whitney conditions). Deﬁne the following two properties.

1. The pair (X , Y) is (a)-regular at ¯x if for any sequence xi ∈ X converging to ¯x and any

convergent sequence of normals vi ∈ NX (xi) the limit limi→∞ vi lies in NY(¯x).

2. The pair (X , Y) is (b)-regular at ¯x if for any sequence xi ∈ X converging to ¯x,
any sequence yi ∈ Y converging to ¯x, and any unit vectors vi ∈ NX (xi), we have
limi→∞(cid:104)vi, xi−yi

(cid:107)xi−yi(cid:107)(cid:105) = 0.

Both conditions (a) and (b) are geometrically transparent. Condition (a) simply asserts
that “limits of normals to X are normal to Y”—clearly a desirable property. Figure 2.1a
illustrates how condition (a) may fail using the classical example of the Cartan umbrella

17

(a) x3 = z(x2 + y2)

(b) y2 = x2z2 − z3

Figure 2.1: Illustrations of conditions (a) and (b).

X = {(x, y, z) : z(x2 + y2) = x3}. If Y is the z-axis, a subset of X , then condition (a) fails
for the pair (X , Y) at the origin.

Condition (b) is more subtle, and is in essense a “restricted smoothness condition”.

Namely, any C 1-smooth manifold M containing a point ¯x satisﬁes the estimate:

(cid:104)v(x), y − x(cid:105) = o((cid:107)y − x(cid:107))

as y, x → ¯x in M,

where v(x) ∈ NM(x) is any selection of unit normal vectors. Condition (b) asserts exactly
this estimate but only along points x ∈ X and y ∈ Y near ¯x. This “restricted smoothness”
viewpoint will become even more clear at the end of the section, when we interpret conditions
(a) and (b) for epigraphs of functions. Optimization experts might recognize condition (b)
in the setting Y = {¯x} as semismoothness of the distance function distX (·) at ¯x in the sense
of [39, 66]. In this vain, when Y is any subset of X , we can interpret condition (b) for the
pair (X , Y) as a kind of uniform semismoothness of X relative to Y.

A useful consequence of condition (a) is the following inclusion of normal cones. This
observation in the smooth category played a fundamental role in the work [8], underpinning
their projection formula and its numerous consequences for subgradient dynamics (e.g. [6,
26, 31]).

Lemma 2.1.1. Suppose that the inclusion Y ⊂ X holds and that the pair (X , Y) is (a)-
regular at a point ¯x ∈ Y. Then the inclusion NX (¯x) ⊂ NY(¯x) holds.

Proof. This is almost tautological. The inclusion ˆNX (¯x) ⊂ ˆNY(¯x) holds since Y is contained
in X . For any vector v ∈ NX (¯x), we may ﬁnd a sequence xi ∈ X converging to ¯x and vectors
vi ∈ ˆNX (xi) converging to ¯v. Condition (a) therefore guarantees ¯v ∈ NY(¯x), as claimed.

John Mather in his lecture notes [65] pointed out that condition (b) for two smooth
manifolds X and Y implies condition (a). The following simple lemma shows that this is
true for general locally closed sets X and Y.

Lemma 2.1.2. The implication (b) ⇒ (a) holds.

18

Proof. Suppose that the pair (X , Y) is (b)-regular at ¯x. Let xi ∈ X be a sequence converging
to ¯x and vi ∈ NX (xi) be a sequence converging to some vector v. It suﬃces to argue that
the inclusion v ∈ ˆNY(¯x) holds. To this end, consider an arbitrary sequence yj ∈ Y \ {¯x}
converging to ¯x. Choosing a subsequence ij, we may ensure (cid:107)xij − ¯x(cid:107) ≤ (cid:107)yj −¯x(cid:107)
. We therefore
deduce

j

lim sup
j→∞

(cid:104)v, yj − ¯x(cid:105)
(cid:107)yj − ¯x(cid:107)

= lim sup

j→∞

(cid:104)vij , yij − xij (cid:105)
(cid:107)yj − xij (cid:107)

= 0,

where the last inequality follows from condition (b). Therefore the inclusion v ∈ ˆNY(¯x) holds
as claimed.

Example 2.1.1 (Condition (b) is strictly stronger than condition (a)). Whitney showed
that condition (b) may be strictly stronger than condition (a) even for smooth manifolds
X and Y. This distinction is worth highlighting with Whitney’s original example. Namely,
deﬁne the set Q = {(x, y, z) : y2 = x2z2 − z3}, depicted in Figure 2.1b. Deﬁne now two
smooth manifolds X = Q ∩ {z > 0} and Y = Q ∩ {z = 0}; note that Y is just the x-
axis. It is straightforward to see that the the pair (X , Y) satisﬁes condition (a) at the origin.
Condition (b), on the other hand, fails at the origin. To see this, we may deﬁne the sequences
ui = (i−1, 0, 0) lying in Y and wi = (i−1, 0, i−2); in the ﬁgure, the points wi ∈ X are lying
on the parabola above the points ui ∈ Y. It is clear from the picture, and can be formally
veriﬁed, that the secant line joining ui with wi and the normal line to X at wi become
collinear in the limit as i → ∞.

Notice that condition (a) does not specify the rate at which the gap ∆(NX (xi), NY(¯x))
tends to zero as xi ∈ X tends to ¯x. A natural strengthening of the condition, introduced by
Verdier [90], requires the gap to be linearly bounded by (cid:107)xi − ¯x(cid:107), with a coeﬃcient that is
uniform over all small perturbation of the base point ¯x ∈ Y.1 Condition (b) can be similarly
strengthened. The following deﬁnition records the resulting two properties.

Deﬁnition 2.1.2 (Strong (a) and strong (b) conditions). Deﬁne the following two properties.

1. The pair (X , Y) is strongly (a)-regular at ¯x if there exists a constant C > 0 satisfying

∆(NX (x), NY(y)) ≤ C · (cid:107)x − y(cid:107),

(2.1.1)

for all x ∈ X and y ∈ Y suﬃciently close to ¯x.

2. The pair (X , Y) is strongly (b)-regular at ¯x if there exists a constant C > 0 satisfying

|(cid:104)v, x − y(cid:105)| ≤ C(cid:107)x − y(cid:107)2,

(2.1.2)

for all x ∈ X and y ∈ Y suﬃciently close to ¯x and all unit vectors v ∈ NX (x).

Summarizing, we have deﬁned four fundamental regularity conditions quantifying the
compatibility of two sets X and Y near a point ¯x. The most important situation for our
purposes is when Y is a smooth manifold contained in X . The algorithmic importance of
these conditions becomes clear when we interpret what they mean for epigraphs of functions.
With this in mind, we introduce the following deﬁnition.

1What we call strong (a) is often called condition (w), the Verdier condition, or the Kuo-Verdier (kw)

condition in the stratiﬁcation literature.

19

Deﬁnition 2.1.3 (Regularity of a function along a set). Consider a function f : E → R ∪
{∞} and a set M ⊂ dom f . We say that f is (a)-regular along M at a point ¯x ∈ M if
the pair (epi f, gph f (cid:12)
(cid:12)M) is (a)-regular at (¯x, f (¯x)). Regularity of type (b), strong (a), and
strong (b) are deﬁned similarly.

The following theorem reinterprets the four geometric regularity conditions for the epi-

graph in analytic terms.

Theorem 2.1.4 (From geometry to analysis). Consider a function f : E → R ∪ {∞} that
is locally Lipschitz continuous on its domain and ﬁx a set M ⊂ dom f . Let ¯x ∈ M be a
point such that near ¯x, the set M is a C 1-smooth manifold and the restriction of f to M is
C 1-smooth. Then the following claims are true.

1.

(condition (a)) If f is (a)-regular along M at ¯x, then the inclusions hold:

PTM(¯x)(∂f (¯x)) ⊂ {∇Mf (¯x)}

and

PTM(¯x)(∂∞f (¯x)) = {0}.

2.

(strong (a)) If f is strongly (a)-regular along M at ¯x, then there exists a constant

C > 0 such that the conditions:

(cid:107)PTM(y)(v − ∇Mf (y))(cid:107) ≤ C(cid:112)1 + (cid:107)v(cid:107)2(cid:107)x − y(cid:107),

(cid:107)PTM(y)(w)(cid:107) ≤ C(cid:107)w(cid:107)(cid:107)x − y(cid:107),

(2.1.3)
(2.1.4)

hold for x ∈ dom f and y ∈ M close to ¯x, and all v ∈ ∂f (x) and all w ∈ ∂∞f (x).

3.

(condition (b)) If f is (b)-regular along M at ¯x, then for every δ > 0, there exists

(cid:15) > 0 such that the estimates

|f (y) − f (x) − (cid:104)v, y − x(cid:105)| ≤ δ(cid:112)1 + (cid:107)v(cid:107)2 · (cid:107)x − y(cid:107),

|(cid:104)w, y − x(cid:105)| ≤ δ(cid:107)w(cid:107)(cid:107)y − x(cid:107),

(2.1.5)
(2.1.6)

hold for all x ∈ (dom f ) ∩ B(cid:15)(¯x), y ∈ M ∩ B(cid:15)(¯x), v ∈ ∂f (x), and w ∈ ∂∞f (x).

4.

(strong (b)) If f is strongly (b)-regular along M at ¯x, then there exists a constant

C > 0 such that the estimates

|f (y) − f (x) − (cid:104)v, y − x(cid:105)| ≤ C(cid:112)1 + (cid:107)v(cid:107)2 · (cid:107)x − y(cid:107)2,

|(cid:104)w, y − x(cid:105)| ≤ C(cid:107)w(cid:107)(cid:107)y − x(cid:107)2,

hold for all x ∈ dom f and y ∈ M suﬃciently close to ¯x, and for all v ∈ ∂f (x) and
w ∈ ∂∞f (x).

Proof. The proof of the ﬁrst claim is identical to the proof of the projection formula in [8].
We prove the rest of the claims in order. Without loss of generarlity, we suppose ¯x = 0.
Throughout we ﬁx points x ∈ dom f and y ∈ M near ¯x, deﬁne the lifted points X = (x, f (x))
and Y = (y, f (y)), and set X = epi f and Y = gph f (cid:12)
(cid:12)M. We let L be a Lipschitz constant
of the restriction of f to dom f on some neighborhood of ¯x.

20

Next, suppose that f is strongly (a)-regular along M at ¯x. Clearly the inclusion

∂f (x) × {−1} ⊂ NX (X),

holds. Therefore, for any vector v ∈ ∂f (x), strong (a) regularity and Lipschitz continuity of
f on dom f imply

(v, −1) ⊂ NY(Y ) + C(cid:112)1 + (cid:107)v(cid:107)2(cid:107)x − y(cid:107)B.

(2.1.7)

Classical arguments yield the description of the tangent space

TY(Y ) = {(u, (cid:104)∇Mf (y), u(cid:105)) : u ∈ TM(y)}.

Therefore taking dot products of the inclusion (2.1.7) with such tangent vectors, we deduce
(cid:104)v − ∇Mf (y), u(cid:105) ≤ (cid:112)1 + (cid:107)v(cid:107)2C(cid:107)x − y(cid:107)(cid:107)(u, (cid:104)∇Mf (y), u(cid:105))(cid:107).

Note that if u ∈ TM(y) has unit length, then we may upper bound (cid:107)(cid:104)u, ∇Mf (y), u(cid:105))(cid:107) by
√

1 + L2. Therefore for such vectors u we deduce

(cid:104)v − ∇Mf (y), u(cid:105) ≤ (cid:112)(1 + (cid:107)v(cid:107)2)(1 + L2)C(cid:107)x − y(cid:107).

Setting u to be the normalized projection of v − ∇Mf (y) onto TM(y) immediately yields
(cid:107)PTM(y)(v − ∇Mf (y))(cid:107) ≤ (cid:112)(1 + (cid:107)v(cid:107)2)(1 + L2)C(cid:107)x − y(cid:107).

Thus 2.1.3 holds. Next, note the inclusion

∂∞f (x) × {0} ⊂ NX (X).

By the same argument as above, we deduce that for any w ∈ ∂∞f (x) we have

(cid:104)w, u(cid:105) ≤ (1 + L2)C(cid:107)w(cid:107)(cid:107)x − y(cid:107).

for all unit vectors u ∈ TM(y). Choosing u to be the normalized projection of w onto TM(y)
completes the proof of (2.1.4).

Next, suppose that f is (b)-regular along M at ¯x. Then the exists a function C(t, s) that

tends to zero as (t, x) → 0 such that for any vector v ∈ ∂f (x), we have

|f (y) − f (x) − (cid:104)v, y − x(cid:105)| = |(cid:104)(v, −1), X − Y (cid:105)|

≤ C((cid:107)x(cid:107), (cid:107)y(cid:107))(cid:112)(cid:107)x − y(cid:107)2 + (f (x) − f (y))2(cid:112)1 + (cid:107)v(cid:107)2
≤ C((cid:107)x(cid:107), (cid:107)y(cid:107))

1 + L2(cid:112)1 + (cid:107)v(cid:107)2(cid:107)x − y(cid:107)

√

Thus (2.1.5) holds. For any vector w ∈ ∂∞f (x), we similarly compute

|(cid:104)w, y − x(cid:105)| = |(cid:104)(w, 0), X − Y (cid:105)|

≤ C((cid:107)x(cid:107), (cid:107)y(cid:107))(cid:112)(cid:107)x − y(cid:107)2 + (f (x) − f (y))2(cid:107)w(cid:107)
≤ C((cid:107)x(cid:107), (cid:107)y(cid:107))

1 + L2(cid:107)w(cid:107)(cid:107)x − y(cid:107),

√

thereby establishing (2.1.6). The ﬁnal claim for strong (b)-regularity is completely analogous
to (b)-regularity.

21

The conditions in Theorem 2.1.4 are particularly transparent when f is Lipschitz con-
tinuous near ¯x. Then ∂f (¯x) is nonempty and condition (a) implies that the projection
PTM(¯x)(∂f (¯x)) is a single point—the covariant gradient ∇Mf (¯x). This guarantee is called
the projection formula in [8]. Strong (a) provides a “stable improvement” over the projec-
tion formula wherein the deviation ∂f (x) − ∇Mf (y) in tangent directions TM(y) is linearly
bounded by (cid:107)x − y(cid:107), for points x ∈ E and y ∈ M near ¯x.

Condition (b) has a diﬀerent ﬂavor: it ensures a restricted Taylor approximation property

f (y) − f (x) − (cid:104)v, y − x(cid:105) = o((cid:107)x − y(cid:107))

as x ∈ E and y ∈ M tend to ¯x and v ∈ ∂f (x) are arbitrary. In particular, in the setting
M = {¯x}, this is is exactly the semismoothness condition of [].2 Strong (b)-regularity, in
turn, replaces the little-o term with the squared norm:

f (y) − f (x) − (cid:104)v, y − x(cid:105) = O((cid:107)x − y(cid:107)2)

as x ∈ E and y ∈ M tend to ¯x and v ∈ ∂f (x) are arbitrary. In the setting M = {¯x}, this
condition is called strong semi-smoothness in the optimization literature.

Condition (b) becomes particularly useful algorithmically when the inclusion 0 ∈ ˆ∂f (¯x)
holds and M is a C 1 active manifold of f around ¯x. Indeed, condition (b) along with the
sharp growth guarantee of Theorem 1.4.2 then imply that there exists a constant c > 0 such
that the estimate

(cid:104)v, x − PM(x)(cid:105) ≥ c · dist(x, M),

(2.1.8)

holds for all y ∈ M near ¯x and for all v ∈ ∂f (x).
In words, this means that negative
subgradients of f at x always point towards the active manifold. The angle condition (2.1.8)
together with strong (a) regularity will form the core of the algorithmic development in
Chapter 3. For ease of reference, we slight generalization of the angle condition (2.1.8) when
f is not necessarily locally Lipschitz around ¯x and can even be inﬁnite-valued.

Corollary 2.1.5 (Proximal aiming). Consider a closed function f : E → R ∪ {∞} that
admits an active C 1-manifold M at a point ¯x and suppose that f is prox-regular at ¯x or
(b)-regular along M at ¯x. Then, there exists a constant c > 0 such that for any δ > 0, there
exists (cid:15) > 0 satisfying

(cid:104)v, x − PM(x)(cid:105) ≥ (c − δ(cid:112)1 + (cid:107)v(cid:107)2) · dist(x, M),

(2.1.9)

for all x ∈ B(cid:15)(¯x) and all v ∈ ∂f (x). Moreover, if f is locally Lipschitz around ¯x, the same
statement holds with ∂f (x) replaced by ∂cf (x) and with δ = 0.3

The rest of the chapter is devoted to exploring the relationship between the four ba-
sic regularity conditions, presenting examples, proving calculus rules, and justifying that
these conditions hold “generically” along active manifolds. Chapter 3 will in turn use these
conditions to analyze subgradient type algorithms.

2ignoring a directional diﬀerentiability requirement
3The last claim follows immediately from (3.11.5) by choosing δ <

constant of f near ¯x, and taking convex combinations of limiting subgradients.

√

c
1+L2 , where L is a local Lipschitz

2

22

2.2 Relation between the four conditions

The goal of this section is to explore the relationship between the four regularity conditions.
Recall that Lemma 2.1.2 already established the implication (b) ⇒ (a). More generally, the
goal of this section is to show in reasonable settings the string of implications:

(a) ⇐ (b) ⇐ strong (a) ⇐ strong (b) .

(2.2.1)

Before passing to formal statements, we require some preparation. Namely, the task of
verifying conditions (b), strong (a), and strong (b) requires considering arbitrary points
x ∈ X and y ∈ Y, which are a priori unrelated. We now show that it essentially suﬃces to
set y to be the projection of x onto Y. In this way, we may remove one degree of ﬂexibility
for the question of veriﬁcation. We begin by deﬁning the projected variants of conditions
(b), strong (a), and strong (b).

Deﬁnition 2.2.1 (Projected conditions). Fix two sets X , Y ⊂ E and a point ¯x ∈ Y around
which Y is prox-regular. The pair (X , Y) is called (bπ)-regular if it satisﬁes condition (b)
in the restricted setting yi = PY(xi). Conditions strong (aπ) and strong (bπ) are deﬁned
analogously.

The following theorem allows one to reduce the question of verifying regularity conditions

to the setting y ∈ PY(x).

Theorem 2.2.2. Fix two sets X , Y ⊂ E and a point ¯x ∈ Y. Suppose in addition that Y is
a C 2-smooth manifold around ¯x. Then the following equivalences hold:

1. strong (a) ⇔ strong (aπ)

2. (a) and (bπ) ⇔ (b)

Moreover, if Y is C 3-smooth, then the equivalence holds:

(a) and strong (bπ) ⇔ strong (b).

Proof. Suppose that the pair (X , Y) is strongly (aπ)-regular at ¯x. Thus there exists a con-
stant C1 > 0 such that

∆ (NX (x), NY(PY(x))) ≤ C1dist(x, Y) ≤ C1(cid:107)x − y(cid:107),

(2.2.2)

for all x ∈ X and y ∈ Y suﬃciently close to ¯x. On the other hand, since Y is a C 2-manifold,
there exists some constant C2 such that for any x ∈ X , y ∈ Y suﬃciently close to ¯x we have

∆ (NY(PY(x)), NY(y)) ≤ C2(cid:107)PY(x) − y(cid:107)

≤ C2 ((cid:107)PY(x) − x(cid:107) + (cid:107)x − y(cid:107))
≤ 2C2(cid:107)x − y(cid:107).

(2.2.3)

Combining (2.2.2) and (2.2.3), and using the triangle inequality, we conclude ∆(NX (x), NY(y)) ≤
(C1 + 2C2) (cid:107)x − y(cid:107), for all x ∈ X , y ∈ Y suﬃciently close to ¯x. Thus the pair (X , Y) is
strongly (a)-regular at ¯x as claimed.

23

Next, suppose that the pair (X , Y) satisﬁes both conditions (a) and (bπ) at ¯x. Let xi ∈ X

and yi ∈ Y be sequences converging to ¯x and let vi ∈ NX (xi) be arbitrary. Let us write

(cid:104)vi, xi − yi(cid:105) = (cid:104)vi, xi − PY(xi)(cid:105) + (cid:104)vi, PY(xi) − yi(cid:105) ,

We analyze each term on the right side separately. To this end, observe

(cid:28)

vi,

xi − PY(xi)
(cid:107)xi − yi(cid:107)

(cid:29)

(cid:28)

=

vi,

(cid:29)

xi − PY(xi)
(cid:107)xi − PY(xi)(cid:107)

·

(cid:107)xi − PY(xi)(cid:107)
(cid:107)xi − yi(cid:107)

.

Taking into account condition (bπ) and the inequality (cid:107)xi − PY(xi)(cid:107) ≤ (cid:107)yi − xi(cid:107), we deduce
limi→∞(cid:104)vi, xi−PY (xi)

(cid:107)xi−yi(cid:107) (cid:105) = 0.

Next since the projection PY is C 1-smooth near ¯x, it holds:

lim
i→∞

(cid:107)PY(yi) − PY(xi) − ∇PY(xi)(yi − xi)(cid:107)
(cid:107)yi − xi(cid:107)

= 0.

Therefore taking into account yi = PY(yi) we deduce

lim sup
i→∞

(cid:28)

vi,

(cid:12)
(cid:12)
(cid:12)
(cid:12)

PY(xi) − yi
(cid:107)xi − yi(cid:107)

(cid:29)(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ lim sup

i→∞

(cid:28)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

vi,

∇PY(xi)(yi − xi)
(cid:107)yi − xi(cid:107)

(cid:29)(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

(2.2.4)

(2.2.5)

yi−xi
Passing to a subsequence, we may assume
(cid:107)yi−xi(cid:107) tends to some vector w ∈ E and that
vi converge to some vector v. Taking into account the equality ∇PY(¯x) = PTY (¯x) and that
condition (a) guarantees v ∈ NY(¯x) implies that the right-side of (2.2.5) tends to zero. Thus
condition (b) holds as claimed.

Lastly, suppose that Y is a C 3 smooth manifold and that the pair (X , Y) satisﬁes both
conditions (a) and strong (bπ) at ¯x. Note that the projection PY is C 2-smooth. The
proof proceeds exactly in the same way as the previous implication with (2.2.4) replaced
by lim supi→∞

(cid:107)PY (yi)−PY (xi)−∇PY (xi)(yi−xi)(cid:107)
(cid:107)yi−xi(cid:107)2

< ∞.

With Theorem 2.2.2 at hand, we may now establish the remaining implications in (2.2.1),

beginning with strong (b) implies strong (a).

Proposition 2.2.3 (Strong (b) implies strong (a)). Consider a C 3 manifold Y that is con-
tained in a set X ⊂ E. Suppose that X is prox-regular at a point ¯x ∈ Y. Then the following
implication holds for the pair (X , Y) and any point ¯x ∈ Y:

strong (b) ⇒ strong (a).

Proof. Suppose that the pair (X , Y) is strongly (b)-regular at ¯x. In light of Theorem 2.2.2,
it suﬃces to prove that the strong (aπ) condition holds. Since X is a C 3 manifold, the
projection PY is C 2-smooth. Therefore, there exist constants (cid:15), L > 0 satisfying

(cid:107)PY(y + h) − PY(y) − ∇PY(y)h(cid:107) ≤ L(cid:107)h(cid:107)2

(2.2.6)

for all y ∈ B(cid:15)(¯x) and h ∈ (cid:15)B. Fix now two points x ∈ X and y ∈ Y and a unit vector
v ∈ NX (x). Clearly, we may suppose v /∈ NY(y), since otherwise the claim is trivially true.

24

Deﬁne the normalized vector w := −

appealing to (3.10.1), we deduce the estimate

PTY (y)(v)
(cid:107)PTY (y)(v)(cid:107). Noting the equality ∇PY(y) = PTY (y) and

(cid:107)PY(y − αw) − (y − αw)(cid:107) ≤ L(cid:107)αw(cid:107)2 = Lα2,

for all y ∈ B(cid:15)(¯x) and α ∈ (0, (cid:15)). Shrinking (cid:15) > 0, prox-regularity yields the estimate

(cid:104)v, PY(y − αw) − x(cid:105) ≤

ρ
2

(cid:107)x − PY(y − αw)(cid:107)2,

for some constant ρ > 0. Therefore, we conclude

α(cid:107)PTY (y)v(cid:107) = −α (cid:104)v, w(cid:105) = (cid:104)v, x − y(cid:105) + (cid:104)v, PY(y − αw) − x(cid:105) + (cid:104)v, (y − αw) − PY(y − αw)(cid:105)

≤ C(cid:107)x − y(cid:107)2 +

(cid:107)x − PY(x − αw)(cid:107)2 + Lα2,

ρ
2

where the last inequality follows from the strong (b) condition. Note that the middle term
is small:

(cid:107)PY(y − αw) − x(cid:107)2 ≤ 2(cid:107)PY(y − αw) − (y − αw)(cid:107)2 + 2(cid:107)y − αw − x(cid:107)2 ≤ 2L2α4 + 4(cid:107)y − x(cid:107)2 + 4α2.

Thus, we have

α(cid:107)PTY (y)v(cid:107) ≤ C(cid:107)x − y(cid:107)2 + ρL2α4 + 2ρ(cid:107)x − y(cid:107)2 + 2ρα2 + Lα2.

Dividing both sides by α and setting α = (cid:107)x − y(cid:107) completes the proof.

Next we prove the last implication, strong (a) ⇒ (b), in the deﬁnable category. This
result thus generalizes the theorems of Kuo [45], Verdier [90], and Ta Le Loi [48]. The proof
technique we present is diﬀerent from those in the earlier works on the subject and will be
based on an application of the Kurdyka-(cid:32)Lojasiewicz inequality [8].

Theorem 2.2.4 (Strong (a) implies (b)). Fix two deﬁnable sets X , Y ⊂ E and a point ¯x ∈ Y.
Suppose in addition that Y is a C 2-smooth manifold around ¯x and that X is a locally closed
set. Then the following implication holds at ¯x for the pair (X , Y):

strong (a) ⇒ (b).

We note that the theorem may easily fail for general C ∞-manifolds X and Y, without
some extra “tameness” assumption such as deﬁnability. See the discussion in [48] for details.

Proof. Suppose that a pair (X , Y) is strongly (a)-regular at ¯x. In light of Theorem 2.2.2, it
suﬃces to show that (X , Y) is (bπ)-regular at ¯x. To this end, deﬁne the function

g(x, v) = | (cid:104)v, x − PY(x)(cid:105) | + δcl X (x).

Fix a compact neighborhood U of {¯x} × B. Then the KL-inequality [8, Theorem 11] ensures
that there exists η > 0 and a continuous function ψ : [0, η) → R satisfying ψ(0) = 0 and
ψ(cid:48)(0) = 0 such that

g(x, v) ≤ ψ(dist(0, ∂g(x, v)))).

(2.2.7)

25

for any (x, v) ∈ U with g(x, v) ≤ η. It suﬃces now to show that dist(0, ∂g(x, v)) is linearly
upper bounded by dist(x, Y) for all x ∈ X near ¯x and all unit vectors v ∈ NX (x). To this
end, ﬁx any point (x, v). Clearly, we may assume g(x, v) (cid:54)= 0, since otherwise there is nothing
to prove. We compute

∂g(x, v) = {(I − ∇PY (x))v + Ncl X (x)} × {x − PY(x)}.

Therefore as long as v ∈ NX (x) we have

dist(0, ∂g(x, v)) ≤ (cid:107)∇PY(x)v(cid:107) + dist(x, Y).

(2.2.8)

Since Y is a C 2-manifold near ¯x, there exists a constant L > 0 such that the inequality
(cid:107)∇PY(x)(cid:107) ≤ L holds for all x near ¯x. Further, let C > 0 be the constant from the deﬁning
property (2.1.1) of strong (a) regularity. Thus, as long as x ∈ X is suﬃciently close to ¯x,
there exists a vector w ∈ NY(PY(x)) satisfying (cid:107)v − w(cid:107) ≤ Cdist(x, Y). Therefore, continuing
with 2.2.8 we deduce

dist(0, ∂g(x, v)) ≤ (cid:107)∇PY(x)w(cid:107) + (1 + CL)dist(x, Y).

To complete the proof, note that ∇PY(x)w = 0 since range(∇PY(x)) ⊆ TY(PY(x)).

2.3 Basic examples

Having a clear understanding of how the four regularity conditions are related, we now
present a few interesting examples of sets that are regular along a distinguished submanifold.
More interesting examples can be constructed with the help of calculus rule, discussed at
the end of the section. We begin with the following simple example showing that any convex
cone is regular along its lineality space.

Proposition 2.3.1 (Cones along the lineality space). Let X ⊂ E be a convex cone and let
Y = X ∩ (−X ) denote its lineality space. Then the pair (X , Y) is both strongly (a) and
strongly (b) regular at any point ¯x ∈ Y.

Proof. For any point x ∈ X , the inclusion x + Y ⊂ X holds. Therefore we compute NX (x) ⊂
Nx+Y(x) = Y ⊥ = NY(y), for any points x ∈ X and y ∈ Y. Thus the pair (X , Y) is strongly
(a)-regular at all points ¯x ∈ Y. Next, ﬁx any points x ∈ X and y ∈ Y and a vector
v ∈ NX (x). Translating by y, we see that v is a normal vector to X − y at x − y. Taking into
account X − y = X , we deduce (cid:104)v, x − y(cid:105) = 0. Thus the pair (X , Y) is strongly (b)-regular
at all points ¯x ∈ Y.

More generally, any convex set is strongly (a)-regular along any aﬃne space contained in

it.

Proposition 2.3.2 (Aﬃne subsets of convex sets). Consider a convex set X ⊂ E and a
subset Y ⊂ X that is locally aﬃne around a point ¯x ∈ Y. Then the pair (X , Y) is strongly
(a)-regular at ¯x.

26

Proof. Translating the sets we may suppose ¯x = 0 and therefore that Y coincides with
a linear subspace near the origin. Fix now points x ∈ X and y ∈ Y and a unit vector
v ∈ NX (x). Clearly, we may suppose v /∈ NY(y), since otherwise the claim is trivially true.
Deﬁne the normalized vector w := − PY (v)
(cid:107)PY (v)(cid:107). The for all y ∈ Y near ¯x and all small α > 0,
using the linearity of the projection PY we compute

α(cid:107)PTY (y)v(cid:107) = α(cid:107)PYv(cid:107) = −α (cid:104)v, w(cid:105) = (cid:104)v, x − y(cid:105) + (cid:104)v, PY(y − αw) − x(cid:105)

≤ (cid:107)x − y(cid:107),

where the last inequality follows from convexity of X . This completes the proof.

Not surprisingly, the conclusion of Theorem 2.3.2 can easily fail if X is prox-regular
(instead of convex) or if Y is a smooth manifold (instead of aﬃne). This is the content of
the following example.

Example 2.3.1 (Failure of strong (a)-regularity). Deﬁne X to be the epigraph of the func-
tion f (x, y) = max{0, y − x2} and set Y to be the x-axis Y = R × {0} × {0}. See Figure 2.2
for an illustration. Consider the sequence yk = (1/k, 0, 0)) in Y and xk = (1/k, 1/k2, 0) in X
converging to the origin. Fix the sequence of normal vectors vk = (−2/k, 1, −1) ∈ NX(xk)
and note NY(yk) = {0} × R × R. A quick computation shows

∆

(cid:18) vk
(cid:107)vk(cid:107)

(cid:19)

, NY(yk)

=

2/k
(cid:112)2 + 4/k2

≥

2
√

6

k

=

2
√
6

(cid:112)(cid:107)xk − yk(cid:107).

Therefore the pari (X , Y) is not strongly (a)-regular at ¯x.

Figure 2.2: Graph of the function f (x, y) = max{0, y − x2}.

Strong (a)-regularity fails in the above example “by a square root factor in the distance
to Y.” The following theorem shows a surprising fact: the estimate (2.1.1) is guaranteed to
hold up to a square root for any proximal smooth set along a smooth submanifold. Since
we will not use this result and the proof is very similar to that of Proposition 2.2.3, we have
placed the argument in the appendix.

27

Proposition 2.3.3 (Strong (a) up to square root). Consider a C 3 manifold Y that is con-
tained in a set X ⊂ E. Suppose that X is prox-regular around a point ¯x ∈ Y. Then there
exists a constant C > 0 satisfying

∆(NX (x), NY(y)) ≤ C · (cid:112)(cid:107)x − y(cid:107),

(2.3.1)

for all x ∈ X and y ∈ Y suﬃciently close to ¯x.

The following example connects (b)-regularity to inner-semicontinuity of the normal cone
map. In particular, any proximally smooth set is (b)-regular along any of its partly smooth
submanifolds in the sense of Lewis [57].

Proposition 2.3.4 (Condition (b) and inner semicontinuity). Consider a proximally smooth
set X and a set Y ⊂ X such that the normal cone map NX is inner-semicontinuous on Y.
Then the pair (X , Y) satisﬁes the (b)-condition.

Proof. Consider sequences xi ∈ X \ Y and yi ∈ Y converging to a point ¯x ∈ Y. Let
vi ∈ NX (xi) be arbitrary unit normal vectors. Passing to a subsequence we may assume
that vi converge to some unit normal vector ¯v ∈ NX (¯x). By inner semicontinuity, there exist
unit vectors wi ∈ NX (yi) converging to ¯v. Deﬁne the unit vectors ui := xi−yi
(cid:107)xi−yi(cid:107). Proximal
smoothness of X therefore guarantees (cid:104)vi, ui(cid:105) ≥ − ρ
2 (cid:107)xi − yi(cid:107) and (cid:104)wi, ui(cid:105) ≤ ρ
2 (cid:107)xi − yi(cid:107). We
conclude

−

ρ
2

(cid:107)xi − yi(cid:107) ≤ (cid:104)vi, ui(cid:105) = (cid:104)wi, ui(cid:105) + (cid:104)vi − wi, ui(cid:105) ≤

ρ
2

(cid:107)xi − yi(cid:107) + (cid:107)vi − wi(cid:107).

Noting that the left and right sides both tend to zero completes the proof.

2.4 Preservation of regularity under preimages by smooth

transversal maps

More interesting examples may be constructed through calculus rules. The next theorem
shows that the four regularity conditions are preserved by taking preimages of smooth maps
under a transversality condition.

Theorem 2.4.1 (Smooth preimages). Consider a C 1-map F : Y → E and an arbitrary point
¯x ∈ Y. Let X , Y ⊂ E be two locally closed sets with Y Clarke regular. Suppose that (X , Y)
is (a)-regular (respectively (b)-regular) at the point F (¯x) ∈ Y and that the transversality
condition holds:

NY(F (¯x)) ∩ Null (∇F (¯x)∗) = {0}.
Then the pair (F −1(X ), F −1(Y)) is (a)-regular (respectively (b)-regular) at ¯x. Analogous
statements hold for strong (a) and strong (b) conditions, provided F is in addition C 2-smooth.

(2.4.1)

Proof. Transversality ensures that there exists a constant τ > 0 and a neighborhood U of ¯x
satisfying

(cid:107)∇F (y)∗v(cid:107) ≥ τ (cid:107)v(cid:107)

for all y ∈ F −1(Y) ∩ U, v ∈ NY(F (y)).

28

Moreover, shrinking U , we may assume that F is (cid:96)-Lipschitz continuous on U . We prove the
theorem in the order: (a), strong (a), (b), strong (b).

Condition (a): Suppose that the pair (X , Y) is (a)-regular at F (¯x). Then, shrinking η, τ > 0
and U , we may ensure:

(cid:107)∇F (x)∗v(cid:107) ≥ τ (cid:107)v(cid:107)

for all x ∈ F −1(X ) ∩ U, v ∈ NX (F (x)).

(2.4.2)

Transversality and Clarke regularity of Y imply [80, Theorem 10.6]

NF −1(Y)(y) = ∇F (y)∗NY(F (y))

and

NF −1(X )(x) ⊂ ∇F (x)∗NX (F (x))

(2.4.3)

for all y ∈ F −1(Y) and x ∈ F −1(X ) suﬃciently close to ¯x.

Consider now a sequence xi ∈ F −1(X ) converging to ¯x and a sequence of unit normal
vectors wi ∈ NF −1(X )(xi) converging to some vector w. Using (2.4.3), we may write wi =
∇F (xi)∗vi for some vectors vi ∈ NX (F (xi)). Note that due to (2.4.2), the sequence vi is
bounded. Indeed, the norm of vi is upper bounded by a constant that is independent of xi
and yi. Therefore passing to a subsequence we may suppose vi converges to some vector v.
Since the pair (X , Y) is (a)-regular at F (¯x), the inclusion v ∈ NY(F (¯x)) holds. Therefore
using (2.4.3) we deduce w = limi→∞ ∇F (xi)∗vi = ∇F (¯x)∗v ∈ NF −1(Y)(F (¯x)). Thus the pair
(F −1(X ), F −1(Y)) is (a)-regular at ¯x.

Before moving on to the next three regularity conditions, note that each of them implies
condition (a) (Lemma 2.1.2) and therefore we can be sure that the expressions (2.4.2) and
(2.4.3) hold. Therefore, ﬁx for the rest of the proof the sequence xi, vi, and wi as above, and
let yi ∈ Y be an arbitrary sequence converging to ¯x.

Condition strong (a): Suppose that F is C 2-smooth and the pair (X , Y) is strongly (a)-
regular at F (¯x) and let C > 0 be the corresponding constant in (2.1.1). Shrinking U we may
assume ∇F is L-Lipschitz continuous on U . We successively compute

dist(wi, NF −1(Y)(yi)) = dist(∇F (xi)∗vi, NF −1(Y)(yi))

≤ (cid:107)∇F (xi) − ∇F (yi)(cid:107)op(cid:107)vi(cid:107) + dist(∇F (yi)∗vi, NF −1(Y)(yi))
= (cid:107)∇F (xi) − ∇F (yi)(cid:107)op(cid:107)vi(cid:107) + dist(vi, NY(F (yi)))
≤ L(cid:107)vi(cid:107)(cid:107)xi − yi(cid:107) + C(cid:107)vi(cid:107)(cid:107)F (xi) − F (yi)(cid:107)
≤ (L + C(cid:96))(cid:107)vi(cid:107)(cid:107)xi − yi(cid:107)
≤ (L + C(cid:96))τ −1(cid:107)∇F (xi)∗vi(cid:107)(cid:107)xi − yi(cid:107)
= (L + C(cid:96))τ −1(cid:107)wi(cid:107)(cid:107)xi − yi(cid:107),

(2.4.4)
(2.4.5)
(2.4.6)

(2.4.7)

where (2.4.4) follows from the triangle inequality, (2.4.5) follows from (2.4.3), the estimate
(2.4.6) follows from strong (a)-regularity, and (2.4.7) follows from (2.4.2).

Setting the stage for the remainder of the proof, we compute

29

(cid:28)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

wi,

yi − xi
(cid:107)yi − xi(cid:107)

(cid:29)(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

≤

(cid:28)

vi,

(cid:28)

vi,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∇F (xi)(yi − xi)
(cid:107)yi − xi(cid:107)
F (yi) − F (xi)
(cid:107)yi − xi(cid:107)

(cid:29)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:29)(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ (cid:107)vi(cid:107) ·

(cid:107)F (yi) − F (xi) − ∇F (xi)(yi − xi)(cid:107)
(cid:107)xi − yi(cid:107)

.

(2.4.8)

Condition (b): Suppose that the pair (X , Y) is (b)-regular at F (¯x). Let us look at the two
terms on the right side of (2.4.8). Local Lipschitz continuity of F and (b)-regularity imply
that the ﬁrst term tends to zero while C 1-smoothness of f ensures that the second term also
tends to zero.

Condition strong (b): Suppose that F is C 2-smooth and the pair (X , Y) is strongly (b)-
regular at F (¯x). Shrinking U we may assume ∇F is L-Lipschitz continuous on U . Let us
look again at the two terms on the right side of (2.4.8). Local Lipschitz continuity of F and
strong (b)-regularity imply that the ﬁrst term can be upper bounded as

(cid:28)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

vi,

F (yi) − F (xi)
(cid:107)yi − xi(cid:107)

(cid:29)(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ C

(cid:107)F (yi) − F (xi)(cid:107)2
(cid:107)yi − xi(cid:107)

≤ C(cid:96)2(cid:107)yi − xi(cid:107),

where C is the constant in (2.1.2). The second term may be upper bounded as (cid:107)vi(cid:107) ·
(cid:107)F (yi)−F (xi)−∇F (xi)(yi−xi)(cid:107)
≤ L(cid:107)vi(cid:107)(cid:107)yi − xi(cid:107). Noting that the vectors vi are bounded in norm
(cid:107)xi−yi(cid:107)

by a constant that is independent of xi and yi completes the proof.

The calculus rule just developed along with basic examples can be used to justify the
four regularity conditions for many examples. In particular, cone reducible sets, introduced
by [82], form a large class of sets are regular along their active manifolds. We refer the reader
to [82] for numerous examples of cone reducible sets in optimization.

Corollary 2.4.2 (Cone reducible sets are regular along the active manifold). Suppose that
a set X is C 2 cone reducible to C by F at ¯x. Then the pair (X , F −1(0)) is strongly (a) and
strongly (b)-regular at ¯x.

2.5 Preservation of regularity under spectral lifts

In this section, we study the prevalence of the four regularity conditions in eigenvalue prob-
lems. We begin with some notation. The symbol Sn will denote the Euclidean space of
symmetric matrices, endowed with the trace inner product (cid:104)A, B(cid:105) = tr(AB) and the in-
duced Frobenius norm (cid:107)A(cid:107) = (cid:112)tr(A2). The symbol O(n) will denote the set of n × n
orthogonal matrices. The eigenvalue map λ : Sn → Rn assigns to every matrix X its ordered
list of eigenvalues

The following class of sets will be the subject of the study.

λ1(X) ≥ λ2(X) ≥ . . . ≥ λn(X).

30

Deﬁnition 2.5.1. A set X ⊂ Rn → R is called symmetric if it satisﬁes

πX ⊂ X

for all π ∈ Π(n).

Deﬁnition 2.5.2. A set Q ⊂ Sn is called spectral if it satisﬁes

U QU T ⊂ Q

for all U ∈ O(n).

Thus a set in Rn is symmetric if it is invariant under reordering of the coordinates. For
example, all (cid:96)p-norm balls, the nonnegative orthant, and the unit simplex are symmetric.
A set in Sn is spectral if it is invariant under conjugation of its argument by orthogonal
matrices. Spectral sets are precisely those that can be written as λ−1(X ) for some symmetric
set X ⊂ Rn. See ﬁgure 2.3 for an illustration.

(a) p = 1

(b) p = 1.5

(c) p = 2

(d) p = 5

(e) p = ∞

Figure 2.3: Unit (cid:96)p balls in R2 (top row) and unit balls of Schatten (cid:96)p-norms (cid:107)A(cid:107)p = (cid:107)λ(A)(cid:107)p
over S2 (bottom row).

A prevalent theme in variational analysis is that a variety of geometric properties of a
symmetric set X and those of its induced spectral set λ−1(X ) are in one-to-one correspon-
dence. Notable examples include convexity [20, 55], smoothness [59, 61], prox-regularity [19],
and partial smoothness [18]. In this section, we add to this list the four regularity conditions.
The key idea of the arguments is to pass through the projected conditions (Deﬁnition 2.2.1)
and then invoke Theorem 2.2.2.

We will use the following expressions for the normal cone and the projection map to

spectral sets X :

Pλ−1(X )(X) = (cid:8)U Diag(w)U T : w ∈ PX (λ(X)), U ∈ OX
Nλ−1(X )(X) = (cid:8)U Diag(y)U T : y ∈ NX (λ(X)), U ∈ OX

(cid:9)
(cid:9) .

(2.5.1)

where for any matrix X, we deﬁne the set of diagonalizing matrices

OX := {U ∈ O(n) : X = U Diag(λ(X))U T }.

31

The expression for the proximal map was established in [29] while the subdiﬀerential formula
was proved in [56]. A short and elementary proof of the subdiﬀerential formula appears
in [29].

Theorem 2.5.3 (Spectral preservation of projected regularity). Let ¯X ∈ Sn be a symmetric
matrix and consider two locally closed symmetric sets X , Y ⊆ Rn such that Y is prox-regular
at λ( ¯X). If the pair (X , Y) satisﬁes condition # at λ( ¯X) ∈ X , then the pair (λ−1(X ), λ−1(Y))
satisﬁes condition # at ¯X, where # can stand for strong (aπ), (bπ), and strong (bπ).

Proof. The result for (a)-regularity holds trivially from (2.5.1). For the rest of the proof
suppose that Y is prox-regular at λ( ¯X). The result [19] therefore guarantees that λ−1(Y) is
prox-regular at ¯X.

Consider now an arbitrary matrix X ∈ λ−1(X ) near ¯X and a normal vector V ∈

Nλ−1(X )(X) with unit Frobenius length. We may then write

V = U Diag(v)U T ,

for some unit vector v ∈ NX (λ(X)) and orthogonal matrix U ∈ OX. Deﬁne the projection
Y = Pλ−1(Y)(X). Using (2.5.1), we may write

Y = U Diag(PY(λ(X)))U T .

Notice that because the coordinates of λ(X) are decreasing and Y is symmetric, the coor-
dinates of PY(λ(X)) are also decreasing; otherwise, one may reorder PY(λ(X)) and ﬁnd a
vector closer to λ(X) in Y. Consequently, we have

λ(Y ) = PY(λ(X))

and

U ∈ OY .

(2.5.2)

Suppose now that the pair (X, Y ) is strongly (aπ) regular at λ( ¯X) and let C be the
corresponding constant in (2.1.1). Condition strong (aπ) ensures that there exists w ∈
NY(PY(λ(X))) satisfying

(cid:107)v − w(cid:107) = dist(v, NY(PY(λ(X))) ≤ C(cid:107)λ(X) − PY(λ(X))(cid:107) = C(cid:107)X − Y (cid:107),

where the last equation follows X and Y being simultaneously diagonalizable. Taking into
account (2.5.1) and (2.5.2), we deduce that W := U Diag(w)U T lies in Nλ−1(Y)(Y ). Therefore
we compute

dist(V, Nλ−1(Y)(Y )) ≤ (cid:107)V − W (cid:107) = (cid:107)v − w(cid:107) ≤ C(cid:107)X − Y (cid:107).

We therefore conclude that the pair (λ−1(X), λ−1(Y )) is strongly (aπ) regular at ¯X as
claimed.

Next moving onto conditions (bπ) and strong (bπ), we compute

(cid:104)V, X − Y (cid:105) = (cid:104)v, λ(X) − PY(λ(X))(cid:105).

The claimed results now follow immediately by noting (cid:107)λ(X) − PY(λ(X))(cid:107) = (cid:107)X − Y (cid:107).

32

Combining Theorems 2.5.3, 2.2.2, and spectral preservation of smoothness [18] yields the

main result of the section.

Proposition 2.5.4 (Spectral Lifts). Let ¯X ∈ Sn be a symmetric matrix. Let X , Y ⊆ Rn be
locally closed symmetric sets with Y a C 2-smooth manifold around λ( ¯X). If the pair (X , Y)
satisﬁes condition (cid:93) at λ( ¯X), then the pair (λ−1(X ), λ−1(Y)) satisﬁes condition (cid:93) at ¯X, where
(cid:93) can stand for (a), (b), or strong (a). Moreover, if Y is C 3-smooth and X is prox-regular,
then we may set (cid:93) to be the strong (b) condition.

2.6 Regularity of functions along manifolds

The previous sections developed basic examples and calculus rules for the four basic regularity
conditions. In this section we interpret these results for functions through their epigraphs.
We begin with the following lemma, which is the direct analogue of Propositions 2.3.1, 2.3.2,
and 2.3.4.

Lemma 2.6.1 (Basic examples). Consider a function f : E → R ∪ {∞}, a set M ⊂ dom f ,
and a point ¯x ∈ M. The following statements are true.

1. If f is a sublinear function and M = {x : f (x) = −f (−x)} is its lineality space, then

f is both strongly (a) and strongly (b) regular along M at ¯x.

2. If f is convex, M is locally aﬃne near ¯x, and f restricted to M is a linear function

near ¯x, then f is strongly (a)-regular along M near ¯x.

3. If f is weakly convex and locally Lipschitz near ¯x and the subdiﬀerential map x (cid:55)→ ∂f (x)

is inner-semicontinuous on M, then f is (b)-regular along M at ¯x.

Proof. Let f be a sublinear function. Then epi f is a convex cone whose lineality space is
precisely gph f (cid:12)
(cid:12)M. Applying proposition 2.3.1 we deduce that f is both strongly (a) and
strongly (b) regular along M at ¯x, as claimed. The second and third claims are immediate
from Propositions 2.3.2 and 2.3.4.

Theorem 2.4.1 takes the form of the following chain rule.

Theorem 2.6.2 (Chain rule). Consider a C p-smooth map c : Y → E and a closed function
h : E → R ∪ {∞} that is ﬁnite at some point c(¯x). Deﬁne the composite function f (x) =
h(c(x)), ﬁx a set M ⊂ E, and deﬁne L := c−1(M). Suppose the following:

1. h is (a)-regular (respectively (b)-regular) along M at c(¯x),
2. M is a C p manifold and the restriction h(cid:12)

(cid:12)M is C p-smooth near ¯x,

3. the transversality condition holds:

NM(c(¯x)) ∩ Null (∇c(¯x)∗) = {0}.

(2.6.1)

33

Then L is a C p manifold around ¯x, the restriction of f to L is C p-smooth, and f is (a)-
regular (respectively (b)-regular) along L at ¯x. Analogous statements hold for strong (a) and
strong (b) conditions, provided c is in addition C 2-smooth.

Proof. First, the transversality condition (2.6.2) classically guarantees that L is a C p man-
ifold around ¯x. Moreover, for any x ∈ L, we may write f (x) = h(c(x)) = (h(cid:12)
(cid:12)M ◦ c)(x).
Therefore the restriction of f to L is indeed C p-smooth.

Next, observe that we may write epi f = {(x, r) : (c(x), r) ∈ epi h}. Thus in the notation

of Theorem 2.4.1, setting X = epi h, Y = gph h(cid:12)

(cid:12)M, and F (x, r) = (c(x), r)), we may write

epi f = F −1(X )

and gph f (cid:12)

(cid:12)L = F −1(Y).

An application of Theorem 2.4.1 will then complete the proof as soon as we verify the
required transversality condition (2.4.1). To this end, consider a vector (v, s) ∈ NY(F (¯x)) ∩
Null (∇F (¯x)∗). We compute

(0, 0) = ∇F (¯x)∗(v, s) = (∇c(¯x)∗v, s).

Therefore, the inclusion (v, 0) ∈ NY(F (¯x)) holds and we have v ∈ Null ∇(c(¯x)∗). The
former immediately implies that v lies in NM(c(¯x)) and therefore (2.6.2) implies v = 0, as
claimed.

In particular, decomposble functions of [82] are regular along their active manifolds.

Corollary 2.6.3 (Decomposable functions are regular). Suppose that a function f is properly
C 1 decomposable as h ◦ c around ¯x. Then f is (b)-regular along c−1(Lin(h)) at ¯x. If f is
C 2-smooth, then f is strongly (a) and strongly (b) regular along c−1(Lin(h)) at ¯x.

As usual, the chain rule immediately implies a sum rule—the ﬁnal result of the section.

Theorem 2.6.4 (Sum rule). Consider closed functions fi : R → R∪{∞} indexed by a ﬁnite
set i ∈ I, and which are ﬁnite at a point ¯x. Deﬁne the function f (x) = (cid:80)
i∈I fi(x), ﬁx some
set Mi for each i ∈ I, and deﬁne the intersection M = ∩i∈IMi. Suppose that that the
following hold for each i ∈ I:

1. fi is (a)-regular (respectively (b)-regular) along Mi at ¯x,

2. Mi is a C p manifold and the restriction of fi to Mi is C p-smooth near ¯x,

3. the implication holds:

(cid:88)

i∈I

vi = 0 and vi ∈ NMi(¯x) ∀i ∈ I

=⇒

vi = 0 ∀i ∈ I.

(2.6.2)

Then M is a C p manifold around ¯x, the restriction of f to M is C p-smooth, and f is (a)-
regular (respectively (b)-regular) along L at ¯x. Analogous statements hold for strong (a) and
strong (b) conditions, provided fi are in addition C 2-smooth for each i ∈ I.

Proof. Enumerate the indices as I = {1, . . . , k}. The result follows from Theorem 2.6.2 by
setting c(x) = (x, . . . , x) and h(x1, . . . , xk) = x1 + . . . + xk.

34

2.7 Generic regularity along active manifolds

How can one justify the use of a particular regularity condition? One approach, highlighted
in the previous sections, is to verify the conditions for certain basic examples and then
show that they are preserved under transverse smooth deformations. Stratiﬁcation theory
adapts another viewpoint, wherein a regularity condition between two manifolds is considered
acceptable if reasonable sets (e.g. semi-algebraic, subanalytic or deﬁnable) can always be
partitioned into ﬁnitely many smooth manifolds so that the regularity condition holds along
any two “adjacent” manifolds. See the survey [88] for an extensive discussion.
To formalize this viewpoint, we begin with a deﬁnition of a stratiﬁcation.

Deﬁnition 2.7.1 (Stratiﬁcation). A C p-stratiﬁcation (p ≥ 1) of a set Q ⊂ E is a partition
of Q into ﬁnitely many C p manifolds, called strata, such that any two strata X and Y satisfy
the implication:

Y ∩ cl X (cid:54)= ∅ =⇒ Y ⊂ cl X .

A stratum Y is said to be adjacent to a stratum X if the inclusion Y ⊂ cl X holds. If each
stratum is a deﬁnable manifold, the stratiﬁcation is called deﬁnable.

Thus a stratiﬁcation of Q is simply a partition of Q into smooth manifolds so that the
closure of any stratum is a union of strata. Stratiﬁcations such that any pair of adjacent
strata are strongly (a)-regular are called Verdier stratiﬁcations.

Deﬁnition 2.7.2. A C p Verdier stratiﬁcation (p ≥ 1) of a set Q ⊂ E is a C p stratiﬁcation
of Q such that whenever a stratum Y is adjacent to a stratum X , the pair (X , Y) is strongly
(a)-regular at any point x ∈ Y.

It is often useful to reﬁne stratiﬁcations. To this end, a stratiﬁcation is compatible with
a collection of sets Q1, . . . , Qk if for every index i, every stratum M is either contained in
Qi or is disjoint from it. The following theorem, due to Ta Le Loi [48], shows that deﬁnable
sets admit a Verdier stratiﬁcation, which is compatible with any ﬁnite collection of deﬁnable
sets.

Theorem 2.7.3 (Verdier stratiﬁcation). For any p ≥ 1, any deﬁnable set Q ⊂ E ad-
mits a deﬁnable C p Verdier stratiﬁcation. Moreover, given ﬁnitely many deﬁnable subsets
Q1, . . . , Qk, we may ensure that the Verdier stratiﬁcation of Q is compatible with Q1, . . . , Qk.

The analogous theorem for condition (b) (and therefore condition (a)) was proved earlier;
see the discussion in [89]. The strong (b) condition does not satisfy such decomposition
properties. It can fail even relative to a single point of a deﬁnable set in R2, as Example 2.7.1
shows. Nonetheless, as we have seen in previous sections, it does hold in a number of
interesting settings in optimization (e.g. for cone reducible sets along the active manifold).

Example 2.7.1 (Strong (b) is not generic). Deﬁne the curve γ(t) = (t, t3/2) in R2. Let X
be the graph of γ and let Y be the origin in R2. Then a quick computation shows that a
√
unit normal u(t) ∈ NX (γ(t)) is given by (− 2
3

9t and therefore

1 + 4

t, 1)/

(cid:113)

(cid:28)

u(t),

γ(t)
(cid:107)γ(t)(cid:107)2

(cid:29)

=

t3/2
(cid:113)

3(t2 + t3)

1 + 4
9t

→ ∞ as

t → 0.

35

Therefore, the strong condition (b) fails for the pair (X, Y ) at the origin.

Applying Theorem 2.7.3, to epigraphs immediately yields the following.

Theorem 2.7.4 (Verdier stratiﬁcation of a function). Consider a deﬁnable function f : E →
R ∪ {∞} that is continuous on its domain. Then for any p > 0, there exists a partition of
dom f into ﬁnitely many C p-smooth manifolds such that f is strongly (a) regular and (b)-
regular along any manifold M at any point x ∈ M.

Proof. We ﬁrst form a nonvertical stratiﬁcation {Mi} of gph f , guaranteed to exist by [8].
Choose any integer p ≥ 2. Restratifying using Theorem 2.7.3 yields a nonvertical C p-Verdier
stratiﬁcation {Kj} of gph f . Let Xj denote the image of Kj under the canonical projection
(x, r) (cid:55)→ x. As explained in [8], each set Xj is a C p-smooth manifolds, the function f
restricted to Xj is C p-smooth, and equality gph f (cid:12)
(cid:12)Xj

Consider now an arbitrary point ¯X ∈ Kj. It remains to verify that the pair (epi f, Kj)
is strongly (a)-regular at ¯X. To this end, since Kj is a C p manifold, there exists C > 0
satisfying

= Kj holds.

∆(NKj (X)), NKj (Y )) ≤ C(cid:107)X − Y (cid:107)
for all X, Y ∈ Kj near ¯X. Moreover since there are only ﬁnitely many strata and using the
deﬁnition of the Verdier stratiﬁcation, shrinking C > 0 we may be sure the estimate

∆(NKl(X)), NKj (Y )) ≤ C(cid:107)X − Y (cid:107)
holds for all Y ∈ Kj near ¯X and for all X near ¯X lying in any stratum Kl that is adjacent
to Kj. The projection formula [8, Proposition 4] applied to the indicator function of the
epigraph implies the inclusion N c
epi f (X) ⊂ NKl(X) for any index l and any point X ∈ Kl.
We therefore immediately deduce that the pair (epi f, Kj) is strongly (a)-regular at ¯X, as
claimed.

In this work, we will be interested in sets that are regular along a particular manifold—
the active one. Theorem 2.7.3 quickly implies that critical points of “generic” deﬁnable
functions lie on an active manifold along which the objective function is strongly (a) regular.

Theorem 2.7.5 (Regularity at critical points of generic functions). Consider a closed de-
ﬁnable function f : E → R ∪ {∞}. Then for almost every direction v ∈ E in the sense
of Lebesgue measure, the perturbed function fv := f (x) − (cid:104)v, x(cid:105) has at most ﬁnitely many
limiting critical points, each lying on a unique C p-smooth active manifold and along which
the function fv is strongly (a)-regular.

This theorem is a special case of a more general result that applies to structured problems

of the form

min
x

g(x) + h(x)

(2.7.1)

for deﬁnable functions g and h. Algorithms that utilize this structure, such as the proximal
subgradient method, generate a sequence that may convergence to composite Clarke critical
points ¯x, meaning those satisfying

0 ∈ ∂cg(¯x) + ∂ch(¯x).

36

This condition is typically weaker than 0 ∈ ∂c(g + h)(¯x). Points ¯x satisfying the stronger
inclusion 0 ∈ ∂g(¯x) + ∂h(¯x) will be called composite limiting critical.

The following theorem shows that under a reasonably rich class of perturbations, the
problem (2.7.1) admits no extraneous composite limiting critical points. Moreover each of
the functions involved admits an active manifold along which the function is strongly (a)-
regular. The proof is a small modiﬁcation of [28, Theorem 5.2].

Theorem 2.7.6 (Regularity at critical points of generic functions). Consider closed deﬁnable
functions g : E → R∪{∞} and h : E → R∪{∞} and deﬁne the parametric family of problems

min
x

fy,v(x) = g(x) − (cid:104)v, x(cid:105) + h(x + y)

(2.7.2)

Deﬁne the tilted function gv(x) = g(x) − (cid:104)v, x(cid:105). Then there exists an integer N > 0 such that
for almost all parameters (v, y) in the sense of Lebesgue measure, the problem (2.7.2) has at
most N composite Clarke critical points. Moreover, for any limiting composite critical point
¯x, there exists a unique vector

¯λ ∈ ∂h(¯x + y)

satisfying − ¯λ ∈ ∂gv(¯x),

and the following properties are true.

1. The inclusions ¯λ ∈ ˆ∂h(¯x + y) and −¯λ ∈ ˆ∂gv(¯x) hold.

2. gv admits a C p active manifold M at ¯x for −¯λ and h admits a C p active manifold K

at ¯x + y for ¯λ, and the two manifolds intersect transversally:

NK(¯x) ∩ NM(¯x) = {0}.

3. ¯x is either a local minimizer of fy,v or a C p strict active saddle point of ϕy,v.

4. gv is strongly (a)-regular along M at ¯x and h is strongly (a)-regular along K at ¯x + y.

Proof. All the claims, except for 3 and 4, are proved in [28]; note, that in that work, active
manifolds are deﬁned using the limiting subdiﬀerential, but exactly the same arguments
apply under the more restrictive Deﬁnition 1.4.1. Claim 3 is proved in [24, Theorem 5.2]4;
it is a direct consequence of the classical Sard’s theorem and existence of stratiﬁcations.
Claim 4 follows from a small modiﬁcation to the proof of [28]. Namely, the ﬁrst-bullet point
in the proof may be replaced by “g is C p-smooth and strongly (a) regular on X j
i ( (cid:98)Ui) and h
is C p-smooth and strongly (a)-regular on F j

i ( (cid:98)Ui)”.

4weak convexity is invoked in the theorem statement but is not necessary for the result.

37

Chapter 3

Algorithmic Consequences: local rates
of convergence, asymptotic normality,
and saddle point avoidance

3.1 Algorithm and main assumptions

In this chapter, we introduce our main algorithmic consequences of the strong (a) and (b)
regularity properties developed in Chapter 2. Setting the stage, throughout we consider a
minimization problem

f (x),

min
x∈Rd

(3.1.1)

where f : Rd → R ∪ {+∞} is a closed function. The function f may enforce constraints
or regularization; it may also be the population loss of a stochastic optimization problem.
In order to simultaneously model algorithms which exploit such structure, we take a fairly
abstract approach, assuming access to a generalized gradient mapping for f :

We then consider the following stochastic method: given x0 ∈ Rd, we iterate

G : R++ × dom f × Rd → Rd

xk+1 = xk − αkGαk(xk, νk),

(3.1.2)

where αk is a control sequence and νk is stochastic noise, satisfying Assumption E, which we
defer to Section 3.3. A ﬁrst classical example of (3.1.2), valid for locally Lipschitz f , is the
stochastic subgradient method:

xk+1 = xk − αk(wk + νk)

where wk ∈ ∂cf (xk),

In this case, mapping G satisﬁes

Gα(x, ν) ∈ ∂cf (x) + ν

for all x, ν ∈ Rd and α > 0.

(3.1.3)

38

More generally, G may represent a stochastic projected gradient method or a stochastic
proximal gradient method—two algorithms we examine in detail in Section 3.2.

The purpose of this chapter is to understand how iteration (3.1.2) is aﬀected by the
existence of “active manifolds” M contained within the domain of f . For this, we posit a tight
interaction between G and the active manifold, which we ﬁrst motivate by the subgradient
mapping (3.1.3).

Indeed, let us suppose that f is locally Lipschitz and has a critical point ¯x contained in
an active manifold M. Then we posit the following two properties that describe how f and
M interact: we assume there exists C, µ > 0 such that for all x near ¯x and v ∈ ∂cf (x),

Strong (a):
Proximal aiming:

(cid:107)PTM(y)(v − ∇Mf (y))(cid:107) ≤ C(cid:107)x − y(cid:107);

(cid:104)v, x − PM(x)(cid:105) ≥ µ · dist(x, M).

Recalling the terminology and results of Chapter 2, we ﬁnd that the ﬁrst condition is simply
strong (a) regularity along M and ¯x, while the proximal aiming condition is an immediate
consequence of (b) regularity of f along M at ¯x (see Corollary 2.1.5). Moreover, these prop-
erties have natural algorithmic consequences: we will later show that strong (a) regularity
ensures that the shadow sequence

yk = PM(xk)

is locally an inexact stochastic Riemannian gradient sequence with implicit retraction:

Shadow Iteration:

yk+1 = yk − αk(∇Mf (yk) + PTM(yk)(νk)) + αkEk,

with error sequence Ek that decays as xk approaches the manifold. On the other hand, the
proximal aiming condition ensures that the subgradient method aims towards the manifold.
As we will later show, this property implies that dist(xk, M) and the error sequence Ek
tend to zero at a controlled rate. These two properties in turn are suﬃcient to establish all
of our claimed results for the subgradient method:
local rates of convergence, asymptotic
normality, and saddle-point avoidance.

Motivated by the strong (a) and proximal aiming conditions, we propose the following
analogous conditions for generalized gradient mapping G, which models the tight interaction
between f , G, and an “active manifold” M containing a given point ¯x ∈ M.

Assumption A (Strong (a) and Aiming at ¯x). Fix ¯x ∈ dom f . Then there exists constants
C, µ > 0, a neighborhood U of ¯x, and a C 3 manifold M ⊆ dom f containing ¯x such that the
following hold on Uf := U ∩ dom f : for all ν ∈ Rd and α > 0,

(A1) (Local Boundedness) We have

(cid:107)Gα(x, ν)(cid:107) ≤ C(1 + (cid:107)ν(cid:107)).

sup
x∈Uf

(A2) (Strong (a)) The function f is C 2 on M and for all x ∈ Uf , we have

(cid:107)PTM(PM(x))(Gα(x, ν) − ∇Mf (PM(x)) − ν)(cid:107) ≤ C(1 + (cid:107)ν(cid:107))2(dist(x, M) + α).

39

(A3) (Proximal Aiming) For x ∈ Uf tending to ¯x, we have

(cid:104)Gα(x, ν) − ν, x − PM(x)(cid:105) ≥ µ · dist(x, M) − (1 + (cid:107)ν(cid:107))2(o(dist(x, M)) + Cα).

Some comments are in order. Assumption (A1) is similar to classical Lipschitz assump-
tions and ensures the steplength can only scale linearly in (cid:107)ν(cid:107). Assumption (A2) is the
natural analogue of strong (a) regularity for the operator Gα(x, ν).
It ensures that the
shadow sequence yk = PM(xk) locally remains an inexact stochastic Riemannian gradient
sequence with implicit retraction. Assumption (A3) ensures that after subtracting the noise
from Gαk(xk, νk), the update direction xk − xk+1 locally points towards the manifold M. We
will later show that this ensures the iterates xk approach the manifold M at a controlled
rate. Finally we note in passing that the power of (1 + (cid:107)ν(cid:107)) in the above expressions must
be at least 2 for common iterative algorithms to satisfy Assumption A; one may also take
higher powers, but this requires higher moment bounds on (cid:107)νk(cid:107). Before making these results
precise in Section 3.3, we ﬁrst formalize our statements about the subgradient method and
introduce several examples.

3.2 Example algorithms satisfying Assumption A

The most immediate example of operator G arises from the subgradient method applied to
a locally Lipschitz function f . In this setting, any selection s : Rd → R of ∂cf (x) gives rise
to a mapping

Gα(x, ν) = s(x) + ν,

(3.2.1)

which is independent of α. Then Algorithm (3.1.2) gives rise to the classical stochastic
subgradient method:

xk+1 = xk − αk(s(xk) + νk).

(3.2.2)

For this mapping G, Assumption A holds under the following conditions:

Assumption B (Assumptions for the subgradient mapping). Let f : Rd → R be a function.
Fix ¯x ∈ Rd and let M ⊆ X be a C 3 manifold containing ¯x and suppose that f is C 2 on M
near x.

(B1) (Local Lipschitz) The function f is locally Lipschitz.

(B2) (Strong (a)) The function f is strong (a)-regular along M at ¯x.

(B3) (Proximal aiming) There exists µ > 0 such that the inequality holds

(cid:104)v, x − PM(x)(cid:105) ≥ µ · dist(x, M)

for all x near ¯x and v ∈ ∂cf (x).

(3.2.3)

The following proposition is then immediate.

Proposition 3.2.1 (Subgradient method). If assumption B holds at ¯x ∈ Rd, then f and G
satisfy Assumption A at ¯x.

40

Thus, all three properties arise from reasonable assumptions on the function f , as we have
already seen in Chapter 1. Moreover, for deﬁnable functions, they are in fact automatic, as
the following corollary shows:

Corollary 3.2.2. Suppose that f : Rd → R is locally Lipschitz and semialgebraic. Then there
exists a ﬁnite N such that for a generic set of v ∈ Rd the tilted function fv(x) := f (x)−(cid:104)v, x(cid:105)
has at most N Clarke critical points ¯x. Moreover, each limiting critical point ¯x is in fact
Fr´echet critical and satisﬁes

1. The function f and the subgradient mapping (3.2.2) satisfy Assumption A at ¯x with

respect to an active manifold M.

2. Critical point ¯x is either a local minimizer or an active strict saddle point of f .

The proof of this Corollary follows from a more general result on the projected subgradient

method. This is the topic of the next section.

3.2.1 Projected subgradient method

Throughout this section let g : Rd → R be a locally Lipschitz function and let X be a closed
set and consider the constrained minimization problem

min f (x) := g(x) + δX .

A classical algorithm for solving this problem is known as the stochastic projected subgra-
dient method. Each iteration of the method updates

xk+1 ∈ PX (xk − αk(vk + νk))

where vk ∈ ∂cg(xk)

(3.2.4)

This algorithm can be reformulated as an instance of (3.1.2). Indeed, let sX : Rd → Rd be
a measurable selection of PX , let sg : Rd → Rd be a measurable selection of ∂cg, and deﬁne
the generalized gradient mapping

Gα(x, ν) :=

x − sX (x − α(sg(x) + ν))
α

Evidently, the update rule (3.1.2) becomes

for all x ∈ Rd, ν ∈ Rd, α > 0.

(3.2.5)

xk+1 = xk − αkGαk(xk, νk) ∈ PX (xk − αk(sg(xk) + νk)),

an instance of the classical stochastic projected subgradient method.

To understand Assumption A for the stochastic projected subgradient method, we intro-

duce the following assumptions on g and X .

Assumption C (Assumptions for the projected gradient mapping). Let f := g + δX , where
X is a closed set and let g : Rd → R be a function. Fix ¯x ∈ Rd and let M ⊆ X be a C 3
manifold containing ¯x and suppose that f is C 2 on M near x.

(C1) (Local Lipschitz) The function g is locally Lipschitz.

41

(C2) (Strong (a)) The function g and set X are strong (a)-regular along M at ¯x.

(C3) (Proximal aiming) There exists µ > 0 such that the inequality holds

(cid:104)v, x − PM(x)(cid:105) ≥ µ · dist(x, M)

for all x ∈ X near ¯x and v ∈ ∂cg(x).

(3.2.6)

(C4) (Prox regularity/condition (b)) The set X is either prox-regular at ¯x or is (b)-

regular along M at ¯x.

The following proposition shows that Assumption C is suﬃcient to ensure Assumption A.

The proof of the Proposition appears in Appendix 3.11.1.

Proposition 3.2.3 (Projected subgradient method). If assumption C holds at ¯x ∈ dom f ,
then f and G satisfy Assumption A at ¯x.

Given this proposition, an immediate question is whether Assumption C is automatically
satisﬁed for some reasonably large class of functions. The following corollary, which is an
immediate consequence of Proposition 3.2.3, Theorem 2.7.6 and Corollary 2.1.5, shows that
the answer is yes.

Corollary 3.2.4. Suppose that f = g + δX , where X ⊆ Rd semialgebraic and closed, and
g : Rd → R is locally Lipschitz and semialgebraic. Then there exists a ﬁnite N such that for
a generic set of v, w ∈ Rd the tilted function fv,w(x) := g(x + w) + δX (x) − (cid:104)v, x(cid:105) has at most
N composite Clarke critical points ¯x. Moreover, each composite limiting critical point ¯x is
in fact Fr´echet critical and satisﬁes

1. The function f and the projected subgradient mapping G (3.2.5) satisfy Assumption A

at ¯x with respect to an active manifold M.

2. Critical point ¯x is either a local minimizer or an active strict saddle point of f .

In the above corollary, the qualiﬁcation composite critical points, as deﬁned in Theo-
rem 2.7.6, is important, since the projected subgradient method is only known to converge
to such points.

3.2.2 Proximal gradient method

Throughout this section let g : Rd → R be a C 1 function and let h : Rd → R ∪ {+∞} be a
closed function. We then consider the minimization problem

f (x) := g(x) + h(x).

min
x∈Rd

A classical algorithm for solving this problem is known as the stochastic proximal gradient
method. Each iteration of the method solves the proximal problem:

(cid:26)

xk+1 ∈ argmin

x∈Rd

h(x) + (cid:104)∇g(xk) + νk, x − xk(cid:105) +

(cid:107)x − xk(cid:107)2

(cid:27)

.

1
2αk

42

This algorithm can be reformulated as an instance of (3.1.2). Indeed, let s : R++ × Rd → Rd
be a measurable selection of the proximal problem and consider the mapping G deﬁned by

Gα(x, ν) =

x − sα(x − α(∇g(x) + ν))
α

Evidently, the update rule (3.1.2) becomes

for all x ∈ Rd, ν ∈ Rd and α > 0.

(3.2.7)

xk+1 = xk − αkGαk(xk, νk) = sαk(xk − α(∇g(x) + ν)),

an instance of the stochastic proximal gradient method.

To understand Assumption A for the stochastic proximal gradient method, we introduce

the following assumptions on g and h.

Assumption D (Assumptions for the proximal gradient mapping). Let f := g + h, where
g : Rd → R is C 1 and h : Rd → R ∪ {+∞} is closed. Denote X := dom h and let M ⊆ X be
a C 3 manifold containing some point ¯x and suppose that f is C 2 on M near ¯x.

(D1) (Lipschitz gradient/boundedness) The gradient ∇g |X Lipschitz near ¯x. Moreover,

there exists C > 0 such that (cid:107)∇g(x)(cid:107) ≤ C(1 + (cid:107)x(cid:107)) for all x ∈ X .

(D2) (Lipschitz proximal term) The function h is Lipschitz on X .

(D3) (Strong (a)) The function f is strong (a)-regular along M at ¯x.

(D4) (Proximal Aiming) There exists µ > 0 such that the inequality holds

(cid:104)∇g(x) + v, x − PM(x)(cid:105) ≥ µ · dist(x, M) − (1 + (cid:107)v(cid:107))o(dist(x, M))

(3.2.8)

as x ∈ dom h tends to ¯x and v ∈ ˆ∂h(x).

The following proposition shows that Assumption D is suﬃcient to ensure Assumption A.

The proof of the Proposition appears in Appendix 3.11.2

Proposition 3.2.5 (Proximal gradient method). If assumption D holds at ¯x ∈ dom f , then
f and G satisfy Assumption A at ¯x.

The following corollary, which is an immediate consequence of Proposition 3.2.5 and
Theorem 2.7.5, shows that assumption D is automatically true for generic semialgebraic
functions.

Corollary 3.2.6. Suppose that f = g + h0 + δX , where X ⊆ Rd, g is a C 1 function with
Lipschitz gradient on X , the function h0 : Rd → R is Lipschitz on X , and we deﬁne h :=
h0 + δX . Suppose that g, h0, and X are semialgebraic. Then there exists a ﬁnite N such that
for a full measure set of v, w ∈ Rd, the tilted function fv,w := g(x+w)+h0(x+w)+δ(x)−(cid:104)v, x(cid:105)
has at most N composite Clarke critical points ¯x. Moreover, each composite limiting critical
point ¯x is in fact composite Fr´echet critical and satisﬁes

1. The function f and the proximal gradient mapping (3.2.7) satisfy Assumption A at ¯x

with respect to an active manifold M.

43

2. Critical point ¯x is either a local minimizer or an active strict saddle point of f .

Thus, we ﬁnd that Assumption A is satisﬁed for common iterative mappings, under
reasonable assumptions, and is even automatic for certain generic classes of functions. In
the next several sections, we turn our attention to the statements of the main results of this
chapter.

3.3 The two pillars

Assumption A at a point ¯x guarantees two useful behaviors, provided the iterates {xk} of
iteration (3.1.2) remain in a small ball around ¯x. First xk must approach the manifold M
containing ¯x at a controlled rate, a consequence of the proximal aiming condition. Second
the shadow yk = PM(xk) of the iterates along the manifold form an approximate Riemannian
stochastic gradient sequence with an implicit retraction. Moreover, the approximation error
of the sequence decays with dist(xk, M) and αk, quantities that quickly tend to zero. These
two properties together are suﬃcient to establish the results outlined in Section 1.2.

The statements of the results in this section are necessarily highly technical and can be
skipped on ﬁrst reading. However, the following technical notation and assumptions cannot
be skipped, since they will appear in our results on local rates of convergence. First, the
formal statements of our results crucially require local arguments and frequently refer to the
following stopping time: given an index k ≥ 1 and a δ > 0, deﬁne

τk,δ := inf{j ≥ k : xj /∈ Bδ(¯x)}.

(3.3.1)

Note that the stopping time implicitly depends on ¯x, a point at which Assumption A is
satisﬁed. In the statements of our result, the point ¯x will always be clear from the context.
Second, we make the following standing assumption on the stepsizes αk and νk. We assume
they are in force throughout the rest of the chapter.

Assumption E (Standing assumptions).

(E1) We assume that G is measurable.

(E2) There exists constants c1, c2 > 0 and γ ∈ (1/2, 1] such that

c1
kγ ≤ αk ≤

c2
kγ .

(E3) {νk} is a martingale diﬀerence sequence w.r.t. to the increasing sequence of σ-ﬁelds

Fk = σ(xj : j ≤ k and νj : j < k)

That is, there exists a function q : Rd → R+ that is bounded on bounded sets with

E[νk | Fk] = 0

and

E[(cid:107)νk(cid:107)4 | Fk] < q(xk).

We let Ek[·] = E[· | Fk] denote the conditional expectation.

44

(E4) The inclusion xk ∈ dom f for all k ≥ 1.

All items in Assumption E are standard in the literature on stochastic approximation
methods and mirror those found in [26, Assumption C]. The only exception is the fourth
moment bound on (cid:107)νk(cid:107), which stipulates that νk has slightly lighter tails. This bound
appears to be necessary for the setting we consider. We now turn to the ﬁrst pillar.

3.3.1 Pillar I: Aiming towards the manifold

The following proposition ensures the sequence xk approaches the manifold. Our results are
separated into two cases: small νk and arbitrary νk. When νk is small, we can establish
deterministic bounds with faster O(k−γ) convergence rates. Obtaining this rate is crucial for
fast local rates of convergence near strong local minimizers of f . When νk is arbitrary, the
corresponding rates of convergence degrade, but are suﬃcient to establish all of our other
results, including asymptotic normality and saddle point avoidance. The proof appears in
Section 3.8.1.

Proposition 3.3.1. Suppose that f satisﬁes Assumption A at ¯x. Let γ ∈ (1/2, 1] and assume
c1 ≥ 32/µ if γ = 1. Then for all k0 ≥ 1 and suﬃciently small δ > 0, there exists a constant
C, such that the following hold with stopping time τk0,δ deﬁned in (3.3.1):

1. Then there exists a random variable Vk0,δ such that

(a) The limit holds:

k2γ−1

log(k + 1)2 dist2(xk, M)1τk0,δ>k

a.s.−−→ Vk0,δ

(b) The sum is almost surely ﬁnite:

∞
(cid:88)

k=1

kγ−1

log(k + 1)2 dist(xk, M)1τk0,δ>k < +∞

2. We have

(a) The expected squared distance satisﬁes:

E[dist2(xk, M)1τk0,δ>k] ≤ Cαk

for all k ≥ 1.

(b) The tail sum is bounded:

(cid:34) ∞
(cid:88)

E

i=k

(cid:35)

αidist(xi, M)1τk0,δ>i

≤ C

∞
(cid:88)

i=k

α2
i

for all k ≥ 1.

3. If (cid:107)νk(cid:107) ≤ µ/4 for all k ≥ 1, we have

dist(xk, M)1τk0,δ>k ≤ Cαk

for all k ≥ k0.

In Section 3.4 we will comment further on how to interpret these conditional convergence
results. For now we remark that Part 1b of the proposition holds not only almost surely, but
also in expectation, which is a stronger statement in general. Now we turn our attention to
Pillar II: the shadow iteration.

45

3.3.2 Pillar II: The shadow iteration

Next we study the evolution of the shadow yk = PM(xk) along the manifold, showing that
yk is locally an inexact Riemannian stochastic gradient sequence with error that asymptot-
ically decays as xk approaches the manifold. Consequently, we may control the error using
Proposition 3.3.1. As before, we prove that the error decays at a rate O(k−γ) when νk is
small, but degrades when νk is arbitrary. The proof appears in Section 3.8.2
Proposition 3.3.2. Suppose that f satisﬁes Assumption A at ¯x. Then for all k0 ≥ 1 and
suﬃciently small δ > 0, there exists a constant C, such that the following hold with stopping
time τk0,δ deﬁned in (3.3.1): there exists a sequence Fk+1 measurable random vectors Ek ∈ Rd
such that

1. The shadow sequence

yk =

(cid:40)

PM(xk)
¯x

if xk ∈ B2δ(¯x)
otherwise.

satisﬁes yk ∈ B4δ(¯x) ∩ M for all k and the recursion holds:

yk+1 = yk − αk∇Mf (yk) − αkPTM(yk)(νk) + αkEk

for all k ≥ 1.

(3.3.2)

Moreover, for such k, we have Ek[PTM(yk)(νk)] = 0.

2. Let γ ∈ (1/2, 1] and assume that c1 ≥ 32/µ if γ = 1.

(a) We have the following bounds for k0 ≤ k ≤ τk0,δ − 1:

i. (cid:107)Ek(cid:107)1τk0,δ>k ≤ C(1 + (cid:107)νk(cid:107))2(dist(xk, M) + αk)1τk0,δ>k
ii. max{Ek[(cid:107)Ek(cid:107)]1τk0,δ>k, Ek[(cid:107)Ek(cid:107)2]1τk0,δ>k} ≤ C.
iii. E[(cid:107)Ek(cid:107)2]1τk0,δ>k ≤ Cαk
(b) The following sums are ﬁnite

i. (cid:80)∞
ii. (cid:80)∞

k=1

k=1

kγ−1

log(k+1)2 max{(cid:107)Ek(cid:107)1τk0,δ>k, Ek[(cid:107)Ek(cid:107)]1τk0,δ>k} < +∞
log(k+1)2 max{(cid:107)Ek(cid:107)21τk0,δ>k, Ek[(cid:107)Ek(cid:107)2]1τk0,δ>k} < +∞

kγ−1

(c) The tail sum is bounded

(cid:34)

E

1τk0,δ=∞

∞
(cid:88)

i=k

(cid:35)

αi(cid:107)Ek(cid:107)

≤ C

∞
(cid:88)

i=k

α2
i

for all k ≥ 1.

(d) If (cid:107)νk(cid:107) ≤ µ/2 for all k0 ≤ k ≤ τk0,δ − 1, we have

(cid:107)Ek(cid:107)1τk0,δ>k ≤ Cαk

for all k ≥ 1.

With the two pillars we separate our study of the sequence xk into two orthogonal com-
ponents: In the tangent/smooth directions, we study the sequence yk, which arises from an
inexact gradient method with rapidly decaying errors and is amenable to the techniques of
smooth optimization. In the normal/nonsmooth directions, we steadily approach the man-
ifold, allowing us to infer strong properties of xk from corresponding properties for yk, e.g.,
asymptotic normality and saddle point avoidance. In the following three sections, we for-
mally state our main results: local rates of convergence, asymptotic normality, and saddle
point avoidance.

46

3.4 Local convergence guarantees

Our ﬁrst application of the two pillars states:
if {xk} remains in Bδ(¯x) for some small δ
and all suﬃciently large k, then dist(xk, M) and (cid:107)∇Mf (PM(xk))(cid:107) decay at a controlled
rate. Given the properties of the shadow sequence, the reason for this behavior is fairly
straightforward: the function fM := f ◦ PM is C 2 near ¯x, its gradient agrees with ∇Mf
along the manifold, and the shadow iteration is an inexact gradient descent sequence for fM,
yielding the conditional controlled decrease:

Ek[fM(yk+1)] ≤ fM(yk) − C(cid:107)∇Mf (yk)(cid:107)2 + O(αkEk(cid:107)Ek(cid:107) + α2

k(Ek(cid:107)Ek(cid:107)2 + Ek(cid:107)νk(cid:107)2)).

Then, the summability properties of (cid:107)Ek(cid:107) outlined in Proposition 3.3.2 yield the following
proposition. We place the proof in Section 3.9.1.

Theorem 3.4.1 (Local rates near critical points). Suppose that f satisﬁes Assumption A at
¯x. Let γ ∈ (1/2, 1] and assume that c1 ≥ 32/µ if γ = 1. Then for all k0 ≥ 1 and suﬃciently
small δ > 0, the following hold with stopping time τk0,δ deﬁned in (3.3.1):

∞
(cid:88)

k=1

αk(cid:107)∇Mf (yk)(cid:107)21τk0,δ>k < ∞.

and

∞
(cid:88)

k=1

kγ−1

log(k + 1)2 dist(xk, M)1τk0,δ>k < ∞.

If moreover, (cid:107)νk(cid:107) ≤ µ/2 for all k ≥ 1, we have
(cid:26) dist(xk, M)
αk

sup
k

1τk0,δ>k

(cid:27)

< +∞.

An immediate consequence of Theorem 3.4.1 is following: on the event {τk0,δ = ∞}, there

exists a ﬁnite nonnegative random variable Wk0,δ such that for any (cid:15) > 0, and any k ≥ 1

min
i=1,...,k

dist(xi, M) ≤

Wk0,δ
kγ−(cid:15)

and

min
i=1,...,k

(cid:107)∇Mf (PM(xi))(cid:107) ≤

Wk0,δ
k1/2−γ/2 .

(3.4.1)

Additionally, when (cid:107)νk(cid:107) ≤ µ/2, the rates of convergence for dist(xk, M) improve. Indeed,
in the event {τk0,δ = ∞} there exists a ﬁnite nonnegative random variable W (cid:48)
k0,δ such that

dist(xk, M) ≤

W (cid:48)
k0,δ
kγ

for all k ≥ 1.

How should we interpret these results? This question is most easily understood when
the covariant gradient is H¨older metrically subregular to a subset Y ⊆ M of the manifold,
a property connected to the Kurdyka-(cid:32)Lojasiewicz inequality [8, 47, 63]:

(cid:107)∇Mf (y)(cid:107) ≥ ˆµ · distη(y, Y)

for all y ∈ B4δ(¯x) ∩ M,

where ˆµ > 0, η > 0. In this case, we ﬁnd that

dist(xk, Y) ≤ dist(xk, M) + dist(yk, Y) ≤ dist(xk, M) + ˆµ−1/η(cid:107)∇Mf (yk)(cid:107)1/η

47

Recalling (3.4.1) immediately yields local O(k− min{γ−(cid:15),1/2η−γ/2η}) rates of convergence for
how quickly xk approaches Y.

A natural question is whether we observe improved behavior when f satisﬁes further
regularity conditions. Our next theorem shows that xk converges to ¯x if ¯x is critical for the
covariant gradient and covariant Hessian is positive deﬁnite on tangent space at ¯x. We place
the proof in Section 3.9.2

Theorem 3.4.2 (Local rates near strong local minimizers). Suppose that f satisﬁes As-
sumption A at ¯x. Suppose that ∇Mf (¯x) = 0 and ∇2
Mf (¯x) is σ-positive deﬁnite on TM(¯x).
Finally, suppose that c1 ≥ max{16/µ, 64/σ} if γ = 1. Then for all k0 ≥ 1 and suﬃciently
small δ > 0, the following hold with stopping time τk0,δ deﬁned in (3.3.1):

1. There exists a ﬁnite random variable V such that the following holds:

(cid:26) k2γ−1
log(k + 1)2 (cid:107)xk − ¯x(cid:107)2 1τk0,δ>k

(cid:27)

≤ V

lim sup
k≥1

(3.4.2)

Moreover, we have (cid:80)∞

k=1

kγ−1

log(k+1)2 (cid:107)xk − ¯x(cid:107)2 1τk0,δ>k < +∞.

2. If νk = 0 for all k ≥ 1, then the following holds:

(cid:107)xk − ¯x(cid:107) 1τk0,δ>k ≤

C
kγ ,

∀k ≥ k0.

The most interesting setting of this theorem is the case γ = 1. In this case, on the event

{τk0,δ = ∞}, the following bound holds:

(cid:107)xk − ¯x(cid:107) = O

(cid:32)√

V log(1 + k)
k

√

(cid:33)

.

This rate nearly matches that of the stochastic gradient method for smooth and strongly
convex problems, which is known to be optimal [71].
In the next section, we prove that
positive deﬁniteness of ∇2
Mf (¯x) on TM(¯x) not only guarantees improves rates of convergence,
but leads to asymptotic normality of the iterate sequence.

3.5 Asymptotic normality

k(¯xk − ¯x), where xk := 1
k

Polyak and Juditsky [78] famously showed that the stochastic gradient method for minimizing
smooth and strongly convex functions enjoys a central limit theorem: the error sequence
√
i=1 xi, converges in distribution to a normal random vector with
a problem-dependent covariance matrix. In this section we ask whether a similar property
is available in nonsmooth optimization. To provide context for our result, for the moment
assume that

(cid:80)k

min f (x) = Ez∼P [f (x; z)],

48

where P is a ﬁxed, unknown distribution, the map x (cid:55)→ f (x; z) is C 2, and f is strongly
convex. For this class of problems, a popular algorithm is the stochastic gradient method

Sample: zk ∼ P

xk+1 = xk − αk∇f (xk; zk),

for an iid sequence of samples zk. Then the result of [78] states that

√

k(¯xk − ¯x) d−→ N (cid:0)0, ∇2f (¯x)−1Cov(∇f (¯x, z))∇2f (¯x)−1(cid:1) ,

where ¯x is the unique minimizer of the f . Hence, the covariance structure of the error
In the nonsmooth setting, we will
depends on second-order smoothness properties of f .
shortly prove a similar result, showing that the optimal covariance depends instead on the
covariant Hessian ∇2

Mf (¯x) at the solution.

To prove our results, we must make the following more restrictive assumption on the
noise sequence νk, which appears in [33, Assumption D’]. The assumption is modeled on
stochastic gradient noise, which in the setting of Section 1.2.2 decomposes into stationary
k and “small” noise ν(2)
ν(1)

k (x) components, as follows:

νk = ∇f (xk; zk) − ∇f (xk) = ∇f (¯x; zk)
(cid:125)

(cid:124)

(cid:123)(cid:122)
=:ν(1)
k

+ (∇f (xk; zk) − ∇f (¯x; zk)) + (∇f (¯x) − ∇f (xk))
(cid:123)(cid:122)
(cid:125)
=:ν(2)
k (x)

(cid:124)

.

Assumption F. Fix a point ¯x ∈ dom f at which Assumption A holds and let U be a matrix
whose column vectors form an orthogonal basis of TM(¯x). We suppose the noise sequence has
k (xk), where ν(2)
decomposable structure νk = ν(1)
: dom f → Rd is a random function
satisfying

k + ν(2)

k

Ek[(cid:107)U (cid:62)ν(2)

k (x)(cid:107)2] ≤ C(cid:107)x − ¯x(cid:107)2

for all x ∈ dom f near ¯x,

and some C > 0.
Ek[ν(2)

k (x)] = 0 and the following limit holds:

In addition, we suppose that for all x ∈ dom f , we have Ek[ν(1)

k ] =

1
√
k

k
(cid:88)

i=1

U (cid:62)ν(1)
i

d−→ N (0, Σ).

for some symmetric positive semideﬁnite matrix Σ.

Based on this assumption, we establish the following asymptotic normality guarantee,
which is reminiscent of the classical result of Polyak and Juditsky for smooth problems [78].
The proof appears in Section 3.9.3.

Theorem 3.5.1 (Asymptotic Normality). Suppose that f satisﬁes Assumption A at ¯x and
νk satisﬁes assumption F. Suppose that γ ∈ ( 1
2, 1) and xk converges to ¯x with probability one.
Suppose ∇Mf (¯x) = 0 and that ∇2
Mf (¯x) is positive deﬁnite on TM(¯x). Then the average
iterate ¯xk = 1
k

i=1 xi satisﬁes

(cid:80)k

√

k(¯xk − ¯x) d−→ N (cid:0)0, ∇2

Mf (¯x)†Σ∇2

Mf (¯x)†(cid:1) .

49

The conclusion of this theorem is surprising: although the sequence xk never reaches the
k(¯xk − ¯x) is supported on the tangent space TM(¯x). Thus
manifold, the distribution of
asymptotically, the “directions of nonsmoothness,” which are normal to the M, are quickly
“averaged out.” When (cid:107)Gαk(xk, νk)(cid:107) is bounded away from 0 for all k, this means that xk
must oscillate across the manifold, instead of approaching it from one direction.

√

The proof of the theorem works as follows. First, we show that it suﬃces to under-
stand the limit distribution of yk, since dist(xk, M) decreases suﬃciently rapidly. While the
shadow iteration (3.3.2) looks like a standard stochastic gradient sequence with quickly de-
creasing errors, it does not live within the ambient space ¯x + TM(¯x), where the distribution
is supported. Thus, we prove that it suﬃces to study the sequence zk = P¯x+TM(¯x)(yk), which
due to the smoothness of M, closely approximates yk. Then we ﬁnd that zk is amenable to
existing proof techniques for stochastic gradient methods with errors, e.g., [1, Theorem 2].

3.5.1 Asymptotic normality in nonlinear programming

To illustrate this result, we consider the classical nonlinear programming problem

min
x

f0(x) := Ez∼P [f0(x; z)]

subject to: x ∈ X := {x ∈ Rd : fi(x) = 0, i ∈ I1; fi(x) ≤ 0, i ∈ I2}.
where I1, I2 ⊆ N are ﬁnite index sets, each fi : Rd → R is smooth, P is a ﬁxed, unknown
probability distribution, and for each z the function x (cid:55)→ f (x; z) is C 1. Consider the following
stochastic projected gradient method for solving (3.5.1):

(3.5.1)

Sample: zk ∼ P
Update: xk+1 ∈ PX (xk − αk∇f0(xk; zk)).

(3.5.2)

Now ﬁx a point ¯x ∈ Rd and suppose the following conditions hold: For a given point ¯x ∈ Rd,
we assume that

(G1) (Local Smoothness) The functions f0 and fi are C 3 smooth near ¯x for all i ∈ I1 ∪ I2.

(G2) (Constraint Qualiﬁcation/Activity) The vector ∇f (¯x) satisﬁes

−∇f0(¯x) ∈ relint NX (¯x)

(3.5.3)

Moreover, let I = {i ∈ I1 ∪ I2 : fi(¯x) = 0} denote the active constraints. Then the
gradients {∇if (¯x) : i ∈ I} are linearly independent.

Two consequences of (G2) follow: First there exists unique ¯λi ∈ R++ (i ∈ I) such that

∇f0(¯x) +

(cid:88)

i∈I

¯λi∇fi(¯x) = 0.

Second, the set

M = {x ∈ Rd : fi(x) = 0 ∀i ∈ I},
is a C 3 manifold with tangent space TM(¯x) = {w : Rd : ∇fi(x)(cid:62)w = 0 ∀i ∈ I}. Moreover,
due to local smoothness, f0 is C 3 along M near ¯x. Consequently, we may examine the second
order optimality conditions through the covariant Hessian:

50

(G3) (Second Order Suﬃcient Condition) There exists σ > 0 such that

(cid:34)

w(cid:62)

∇2f0(¯x) +

(cid:35)
¯λi∇2fi(¯x)

(cid:88)

i∈I

w ≥ σ(cid:107)w(cid:107)2

for all w ∈ TM(¯x).

Note that this condition is simply the requirement that the covariant Hessian of f := f0 +δX ,
computed in [68],

(cid:34)

∇2

Mf (¯x) = PTM(¯x)

∇2f0(¯x) +

(cid:35)
¯λi∇2fi(¯x)

PTM(¯x)

(cid:88)

i∈I

is positive deﬁnite on TM(¯x).

Finally, to ensure our noise sequence

νk = ∇f0(xk; zk) − ∇f0(xk)
= ∇f0(¯x; zk) − ∇f0(¯x)
(cid:123)(cid:122)
(cid:125)
=:ν(1)
k

(cid:124)

,
+ (∇f0(xk; zk) − ∇f0(¯x; zk) + ∇f0(¯x) − ∇f0(xk))
(cid:123)(cid:122)
(cid:125)
=:ν(2)
k (xk)

(cid:124)

satisﬁes Assumption F, we assume the stochasticity is suﬃciently well-behaved:

(G5) (Stochastic Gradients) As a function of x, the fourth moment

x ∈ X (cid:55)→ Ez∼P [(cid:107)∇f0(x; z) − ∇f0(x)(cid:107)4]

is bounded on bounded sets. Moreover, there exists C > 0 such that
Ez∼P [(cid:107)∇f0(x; z) − ∇f0(¯x; z)(cid:107)2] ≤ C(cid:107)x − ¯x(cid:107)2

for all x ∈ X .

Finally, the gradients PTM(¯x)∇f0(¯x; z) have ﬁnite covariance Σ = Cov(PTM(¯x)∇f0(¯x; z)).
With these assumptions in hand, we have the following asymptotic normality result for

nonlinear programming. The proof appears in Appendix 3.11.3.

Corollary 3.5.2 (Asymptotic normality in nonlinear programming). Suppose that Assump-
tions (G1)-(G5) hold and denote f = f0 + δX . Suppose that γ ∈ ( 1
2, 1) and consider the
iterates xk generated by the stochastic projected gradient method (3.5.2). Then if xk con-
verges to ¯x with probability 1, the average iterate ¯xk = 1
k
k(¯xk − ¯x) d−→ N (cid:0)0, ∇2

Mf (¯x)†Cov(PTM(¯x)∇f0(¯x; z))∇2

i=1 xi satisﬁes

Mf (¯x)†(cid:1) .

(cid:80)k

√

Moreover, if ¯x is the unique critical point of (3.5.1) and xk remains bounded with probability
one, then xk converges to ¯x with probability 1.

As stated in introduction (Section 1.2.2), this appears to be the ﬁrst asymptotic nor-
mality guarantee for the standard stochastic projected gradient method in general nonlin-
ear programming problems with C 3 data, even in the convex setting. Moreover, as shown
in [33, Theorem 1] the covariance matrix ∇2
Mf (¯x)† is in a well-deﬁned sense op-
timal, matching the estimation quality of classical empirical risk minimization methods [81,
Theorem 3.3]. Finally we note that even for simple optimization problems dual averaging
procedures can achieve suboptimal convergence [33]. This is surprising since such meth-
ods identify the active manifold [52] (also see [33, Section 4.1]), while projected stochastic
gradient methods do not.

Mf (¯x)†Σ∇2

51

3.6 Avoiding saddle points

In this section, we ask whether xk can converge to points ¯x at which ∇2
Mf (¯x) has at least
one strictly negative eigenvalue. We call such points strict saddle points, and when M is
in addition an active manifold for f , then we call such points active strict saddle points,
following [24]. As outlined in the introduction (Section 1.2.3), the recent work [24] proved
that almost surely several proximal methods cannot converge to active strict saddles of
weakly convex functions, when randomly initialization. Since the subgradient method does
not identify the active manifold, we cannot apply the same strategy. Instead we resort to
another technique, well-known in the stochastic approximation literature: random pertur-
bations [2, 3, 13, 74].

Let us brieﬂy describe this technique. Fix a point p ∈ Rd, consider a C 2 mapping
Fp : Rd → Rd with “unstable zero” at p, meaning ∇Fp(p) has an eigenvalue with positive
real part. Then a well-known result of Pemantle [74] states that, with probability 1, the
following perturbed iteration cannot converge to p:

(cid:40)

Sample ξk ∼ Unif(B1(0))
Set Yk+1 = Yk + αkFp(Yk) + αkξk

(cid:41)

.

(3.6.1)

As stated, the result of [74] does not shed light on the iteration (1.2.2). Nevertheless, the
shadow iteration yk does satisfy an iteration similar to (3.6.1) with mapping

Fp(y) = −∇(f ◦ PM)(PM(y)),

which under reasonable assumptions is locally C 2 near p and satisﬁes Fp(y) = −∇Mf (y)
and ∇Fp(y) = −∇2
Mf (y) for all y ∈ M near p. Moreover, if p is an active strict saddle of
f , then ∇2
Mf (p) has a strictly negative eigenvalue, so p is an “unstable zero” of Fp. Thus,
we might reasonably expect yk to converge to p only with probability zero. If this is the
case, we can then lift the argument to xk, showing that if xk converges to p, then so does
yk—a probability zero event. This is the strategy we will apply in what follows, taking into
account the additional error term Ek in the shadow iteration (1.0.2), a key technical issue
that we have so far ignored.

In order to formalize the above plan, we prove the following extension of the main result
of [74] which models the relationship between xk and yk described above. The proof, which
we defer Section 3.9.4 draws on and the techniques of [2, 3, 12, 13, 74].

Theorem 3.6.1 (Nonconvergence). Fix c1, c2 > 0 and let S ⊆ Rd. Suppose for any p ∈ S,
there exists a ball B(cid:15)p(p) centered at p and a C 2 mapping Fp : B(cid:15)p(p) → Rd that vanishes at
p and has a symmetric Jacobian ∇Fp(p) that has at least one positive eigenvalue. Suppose
{Xk}∞
k=1 is a stochastic process and for any k0, p ∈ S, and δ > 0 deﬁne the stopping time:

τk0,δ(p) = inf {k ≥ k0 : Xk /∈ Bδ(p)} .

Suppose that for any p ∈ S, k0 ≥ 1, and all suﬃciently small δp ≤ (cid:15)p the following hold:
there exists c3, c4 > 0 possibly depending on p, but not on δp and (cid:15)p, such that on the event
Ω0 = {τk0,δp(p) = ∞}, we have

52

1. (Local iteration.) There exists a process {Yk : k ≥ k0} ⊆ B(cid:15)p/2(p) satisfying

Yk+1 = Yk + αkFp(Yk) + αkξk + αkEk

(3.6.2)

for error sequence {Ek}, noise sequence {ξk}, and deterministic stepsize sequence {αk}
that is square summable, but not summable.

2. (Noise Conditions.) Let Fk be the sigma algebra generated by Xk0, . . . , Xk and
Yk0, . . . , Yk. Deﬁne Wp to be the subspace of eigenvectors of ∇Fp(p) with positive eigen-
values. Then we have

(a) E [ξk | Fk] = 0.
(b) lim supk
(c) E [| (cid:104)ξk, w(cid:105) | | Fk] ≥ c4

E[(cid:107)ξk(cid:107)4 | Fk] ≤ c3.

for k ≥ k0 and all unit norm w ∈ Wp.

3. (Error Conditions.)

(a) We have lim supk
(b) For all n ≥ k0, we have E [1Ω0

E[(cid:107)Ek(cid:107)4 | Fk] < ∞.

(cid:80)∞

k=n αk(cid:107)Ek(cid:107)] = Ok0 ((cid:80)∞

k=n α2

k).

Then P (limk→∞ Xk ∈ S) = 0.

Looking at the theorem, recursion condition (3.6.2) is clearly modeled on the shadow
sequence of Proposition 3.3.2. Moreover, the error condition 3b on Ek precisely matches 2c.
Finally, the noise ξk is modeled on PTM(yk)(νk) in the shadow iteration, which is mean zero
and has bounded fourth moment. Condition 2c is not automatic for all noise distributions
and requires that νk has nontrivial mass in all directions of negative curvature for f .
Given Theorem 3.6.1, we now ask: can xk converge to critical points ¯x at which ∇2
Mf (¯x)
has a strict negative eigenvalue? In the following theorem we show that the answer is no,
provided that we choose the noise νk according to the following assumption:

Assumption G (Uniform noise). There exists r > 0 such that νk ∼ Unif(Br(0)) for all k.

The proof of the theorem appears in Section 3.9.5.

Theorem 3.6.2 (Nonconvergence to strict saddle point). Let S ⊆ Rd and suppose that
Assumption A holds at each point ¯x ∈ S, where each manifold is C 4. Let M be the manifold
associated to ¯x and suppose the ∇2
Mf (¯x) has a strictly negative eigenvalue. Suppose that νk
satisﬁes Assumption G. In addition, suppose that γ ∈ ( 1

2, 1). Then

P

(cid:16)

lim
k→∞

xk ∈ S

(cid:17)

= 0.

(3.6.3)

Some comments are in order. First note that the theorem applies to arbitrary sets S,
making no assumptions on countability/isolatedness. Second the result does not preclude
the limit points of xk from lying in S. Thus, the result is useful only when xk is known
to converge. Third, the noise sequence νk may be chosen from diﬀerent distributions. The
main requirement is that it be suﬃciently well-spread in all directions of negative curvature
for f and that it have a bounded eighth moment.

53

We now examine two applications of the above theorem. The following corollary provides
suﬃcient conditions for the projected subgradient method to avoid active strict saddle points.
We place the proof in Appendix 3.11.4.
Corollary 3.6.3 (Projected subgradient methods). Suppose that f = g +δX , where g : Rd →
R is locally Lipschitz and X ⊆ Rd is closed. Let S be a set of Fr´echet C 4 active strict saddle
points of f . Suppose the following hold for all x ∈ S with associated active manifold Mx:

1. The function g and the set X are strong (a)-regular along Mx at x.

2. The function g is prox-regular at x or (b)-regular along Mx at x.

3. The set X is prox-regular at x or (b)-regular along Mx at x.

Suppose that νk satisﬁes Assumption G. Then the iterates of the stochastic projected subgra-
dient method (3.2.4) satisfy

(cid:16)

P

lim
k→∞

(cid:17)

xk ∈ S

= 0.

Next we analyze the the proximal gradient method. Recall that the paper [24] showed
that randomly initialized proximal gradient methods avoid active strict saddles of weakly
convex functions. The following Corollary shows that the same behavior holds for per-
turbed proximal gradient methods beyond the weakly convex class. We place the proof in
Appendix 3.11.5.

Corollary 3.6.4 (Proximal gradient methods). Suppose that f = g + h, where h : Rd →
R ∪ {∞} is closed that is Lipschitz on its domain X := dom h and g : Rd → R is C 1 with
Lipschitz continuous gradient on X . Let S be a set of Fr´echet C 4 active strict saddle points
of f . Suppose that for all x ∈ S with associated active manifold Mx, the function f is strong
(a)-regular and (b)-regular along Mx at x. Suppose that νk satisﬁes Assumption G. Then
the iterates of the stochastic proximal gradient method (3.2.4) satisfy

P

(cid:16)

lim
k→∞

xk ∈ S

(cid:17)

= 0.

3.6.1 Consequences for generic semialgebraic functions

The above results show that the perturbed projected subgradient and the proximal gradient
method cannot converge to Fr´echet active strict saddle points, provided that xk converges and
various regularity properties hold. Although the convergence of xk and the required regular-
ity properties may seem stringent, they are in a precise sense generic. Indeed, the genericity
of the regularity properties was already addressed in Chapter 2 and Section 3.2. Conver-
gence also holds generically: it is known that all limit points of the stochastic subgradient
method, the stochastic projected subgradient method, and the stochastic proximal method
are (composite) Clarke critical points, as long as f is a semialgebraic function [26, Corol-
lary 6.4.]. Thus, since generic semialgebraic functions have only ﬁnitely many (composite)
Clarke critical points and one can show (with small eﬀort) that the set of limit points of
each algorithm is connected, it follows that the entire sequence xk must converge on generic
problems (if the sequence remains bounded). Thus we have the following three corollaries,
whose proofs we place in Appendix 3.11.6.

54

Corollary 3.6.5 (Subgradient method on generic semialgebraic functions). Let f : Rd → R
be a locally Lipschitz semialgebraic function. Then for a full measure set of v the following is
true for the tilted function fv(x) := f (x)−(cid:104)v, x(cid:105): Let {xk}k∈N be generated by the subgradient
method 3.2.2 on fv. Suppose that νk satisﬁes Assumption G. Then on the event {xk}k∈N is
bounded, almost surely we have only two possibilities

1. xk converges to a local minimizer ¯x of fv.

2. xk converges to a Clarke critical point of fv

Thus, if f is Clarke regular, the sequence xk must converge to a local minimizer of fv.

Corollary 3.6.6 (Projected subgradient method on generic semialgebraic functions). Let
f = g + δX , where X ⊆ Rd semialgebraic and closed and g : Rd → R is locally Lipschitz
and semialgebraic. Then for a full measure set of v, w ∈ Rd the following is true for the
tilted function fv,w(x) := g(x + w) + δX (x) − (cid:104)v, x(cid:105). Let {xk}k∈N be generated by the projected
subgradient method 3.2.5. Suppose that νk satisﬁes Assumption G. Then on the event {xk}k∈N
is bounded, almost surely we have only two possibilities

1. xk converges to a local minimizer ¯x of fv,w.

2. xk converges to a composite Clarke critical point of fv,w.

Thus, if g and X are Clarke regular, the sequence xk converges to a local minimizer of fv,w.

Corollary 3.6.7 (Proximal gradient method on generic semialgebraic functions). Suppose
that f = g + h0 + δX , where X ⊆ Rd, g is a C 1 function with Lipschitz gradient on X , the
function h0 : Rd → R is Lipschitz on X , and we deﬁne h := h0 + δX . Then for a full measure
set of v, w ∈ Rd the following is true for the tilted function fv,w := g(x + w) + h0(x + w) +
δ(x) − (cid:104)v, x(cid:105). Let {xk}k∈N be generated by the proximal gradient method 3.2.7. Suppose that
νk satisﬁes Assumption G. Then on the event {xk}k∈N is bounded, almost surely we have
only two possibilities

1. xk converges to a local minimizer ¯x of fv,w.

2. xk converges to a composite Clarke critical point of fv,w.

Thus, if h0 and X are Clarke regular, the sequence xk converges to a local minimizer of fv,w.

In short, the main conclusion of the above three theorems is

On generic regular semialgebraic functions, perturbed subgradient/proximal
methods converge only to local minimizers

We note in passing that the results hold verbatim if one replaces the word “semialgebraic”
with “deﬁnable in an o-minimal structure,” throughout.

55

3.7 Outline

This concludes our statement of main results for Chapter 3. The remaining portion of this
paper consists of proofs. In Section 3.8 we prove the two pillars. This section forms the core
of the arguments for the remainder of the paper. In Section 3.9, we prove the remaining theo-
rems: local rates of converges (Section 3.9.1 and 3.9.2), asymptotic normality (Section 3.9.3),
and saddle point avoidance (Sections 3.9.4 and 3.9.5). Finally the appendix of the paper in-
cludes several secondary results: proofs that Assumption A holds for subgradient, projected
subgradient (Section 3.11.1), and proximal gradient (Section 3.11.2) algorithms; asymptotic
normality in nonlinear programming (Section 3.11.3); saddle avoidance for projected sub-
gradient (Section 3.11.4) and proximal gradient methods (Section 3.11.4); saddle avoidance
for generic semialgebraic problems (Section 3.11.6); and a small appendix on sequences and
stochastic processes 3.11.7.

3.8 Proofs of the two pillars

Throughout this work we let Ek[·] = E[· | Fk] denote the conditional expectation. We now
present the proofs of the two pillars.

3.8.1 Proof of Proposition 3.3.1: aiming towards the manifold

Throughout the proof we let C denote a constant depending on k0 and δ, which may change
from line to line. Choose δ ≤ min{1, c1µ
12γ }, satisfying Bδ(¯x) ⊆ U where U is the neighborhood
in which Assumption A holds. Deﬁne Q := max{supx∈Bδ
q(x), 1}. By shrinking δ slightly,
we can assume that the little o term in (A3) satisﬁes

o(dist(x, M)) ≤

µ
4(1 + Q)

dist(x, M)

for all x ∈ Bδ(¯x).

Now deﬁne: Dk := dist(xk, M) for all k ≥ 0. We prove a recurrence relation satisﬁed
by the sequence Dk. To that end, denote vk = Gαk(xk, νk) and observe that in the event
Ak := {τk0,δ > k}, we have

D2

k+1 ≤ (cid:107)xk+1 − PM(xk)(cid:107)2

= (cid:107)xk − αkvk − PM(xk)(cid:107)2
= (cid:107)xk − PM(xk)(cid:107)2 − 2αk (cid:104)vk, xk − PM(xk)(cid:105) + α2
≤ D2

k − 2αkµDk + 2αk(1 + (cid:107)νk(cid:107))2o(Dk)
− 2αk (cid:104)νk, xk − PM(xk)(cid:105) + C(1 + (cid:107)νk(cid:107))2
(cid:125)

(cid:124)

(cid:123)(cid:122)
:=Bk

k (cid:107)vk(cid:107)2

α2
k,

(3.8.1)

where the second inequality follows from the proximal aiming and local boundedness prop-
erties of G; see Assumption A. This inequality will allow us to prove all parts of the result.
Indeed, let us prove Part 1. To that end, ﬁrst note that the bound Ek[(cid:107)νk(cid:107)2]1Ak ≤

q(xk)1Ak ≤ Q implies that there exists C > 0 such that

Ek[Bk]1Ak ≤ C,

56

meaning the conditional expectation is bounded for all k. Moreover, by our choice of δ,

Ek[(1 + (cid:107)ν(cid:107))2o(Dk)1Ak] ≤

µ
2

Dk1Ak.

Thus, for each k, we have
k+11Ak+1] ≤ Ek[D2

Ek[D2

k+11Ak]

k1Ak − αkµDk1Ak + Ek[Bk]1Akα2
k1Ak − αkµDk1Ak + Cα2
k

≤ D2
≤ D2
≤ (1 − (αk/2)µ)D2

k1Ak − (αk/2)µDk1Ak + Cα2
k,

k − 2αk (cid:104)Ek[νk], xk − PM(xk)(cid:105) 1Ak

(3.8.2)

where the ﬁrst inequality follows from 1Ak+1 ≤ 1Ak; the second inequality follows from Fk-
measurability of Ak; and the fourth inequality follows since Dk1Ak ≥ D2
k1Ak (recall δ ≤ 1).
k1Ak, Yk := αkµDk1Ak, and Zk := Cα2
Now apply Lemma 3.11.6 with the sequences Xk := D2
k
and deduce that (k2γ−1/ log(k + 1)2)D2
k almost surely converges to a ﬁnite valued random
variable and the following sum is ﬁnite:

∞
(cid:88)

k2γ−1αk
log(k + 1)2 µDk1Ak < +∞.

k=1
Recalling that αk ≥ c1/kγ, we get the claimed summability result.

Next we prove Part 2. To that end, take expectation of (3.8.2) and use the law of total

expectation to deduce that for some C > 0, we have

E[D2

k+11Ak] ≤ (1 − µαk/2)E[D2

k1Ak] − (αk/2)µE[Dk1Ak] + Cα2

k

≤ (1 − µc1k−γ/2)E[D2

k1Ak] − (αk/2)µE[Dk1Ak] + Ck−2γ

To prove part 2a, simply apply Lemma 3.11.8 applied with sequence sk = E[D2
k1Ak] and
constants c = µc1/2 and C. To prove part 2b, sum the above inequality from n to inﬁnity
to get

∞
(cid:88)

k=n

(αk/2)µE[Dk1Ak] ≤ E[D2

n1An] + C

∞
(cid:88)

k=n

k ≤ Cn−γ + C
α2

∞
(cid:88)

k=n

α2
k,

where the second inequality follows from Part 2a. Noting that n−γ = O((cid:80)∞
the result.

k=n α2

k) proves

Now we prove Part 3. We ﬁrst slightly shrink δ so that

o(dist(x, M)) ≤

µ
4(1 + µ/4)

dist(x, M)

for all x ∈ Bδ(¯x).

Next, based on our assumptions on (cid:107)νk(cid:107), we have

| (cid:104)νk, xk − PM(xk)(cid:105) | ≤ (µ/4)Dk.

In addition, we have that for some constant C > 0, that (cid:107)Bk(cid:107) ≤ C. Consequently, inequal-
ity (3.8.1) shows that in the event Ak, we have

D2

k+1 ≤ D2
≤ D2

k − 2αkµDk − 2αk(1 + (cid:107)νk(cid:107))o(Dk) + Bkα2
k − αkµDk + Cα2
k.

k − 2αk (cid:104)νk, xk − PM(xk)(cid:105)

Therefore, the result follows from Lemma 3.11.7 applied with the sequence sk = Dk1Ak and
constants c = µc1, and C, as desired.

57

3.8.2 Proof of Proposition 3.3.2: the shadow iteration

Throughout the proof we let C denote a constant depending on k0 and δ, but not on k,
which may change from line to line. We assume δ is small enough that the conclusions of
Proposition 3.3.1 hold; that B4δ(¯x) ⊆ U where U is the neighborhood in which Assumption A
holds; and that PM and ∇PM are Lipschitz continuous on B4δ(¯x). Write τ = τk0,δ and ﬁx
index k ≥ 1. Finally, recall that PM is C 2 on U and ∇PM(x) = PTM(x) for all x ∈ M.

Let us ﬁrst prove that yk ∈ B4δ(¯x). Clearly, we need only consider the case x ∈ B2δ(¯x).

In this case,

(cid:107)yk − ¯x(cid:107) ≤ (cid:107)yk − xk(cid:107) + (cid:107)xk − ¯x(cid:107) ≤ 2(cid:107)xk − ¯x(cid:107) ≤ 4δ,

where the ﬁnal inequality follows since ¯x ∈ M. Therefore, we always have (cid:107)yk − ¯x(cid:107) ≤ 4δ.

Next, let us deﬁne the error sequence Ek in the shadow iteration. To that end, denote

Tk := TM(yk) and

wk := yk − αk∇Mf (yk) − αkPTk(νk)
Then with error sequence Ek := (yk+1 −wk)/αk, the claimed recursion is trivially true. Thus,
in the remainder of the proof, we bound Ek.

Turning to the bound, we ﬁrst note that throughout the proof, we must separate the
analysis into two cases: xk+1 ∈ B2δ(¯x) and xk+1 /∈ B2δ(¯x). In the second case, the following
preliminary observation will be useful:

Claim 1. Suppose that in the event {τ > k} it holds that xk+1 /∈ B2δ(¯x). Then there exists
C > 0 such that

(cid:107)yk+1 − yk(cid:107) ≤ 4δ ≤ C(cid:107)xk+1 − xk(cid:107).

(3.8.3)

Proof. First notice that

(cid:107)xk+1 − xk(cid:107) ≥ (cid:107)xk+1 − ¯x(cid:107) − (cid:107)xk − ¯x(cid:107) ≥ 2δ − δ ≥ δ.

Therefore, the result trivially holds since (cid:107)yk+1 − yk(cid:107) ≤ 4δ.

With the preliminaries set, we now bound (cid:107)Ek(cid:107). To that end, in what follows we assume
we are in the event {τ > k} where k ≥ k0. In this event, our strategy will be to bound the
terms R1 and R2 in the following decomposition:

(cid:107)Ek(cid:107) = (cid:107)(yk+1 − wk)/αk(cid:107)

≤ (cid:107)yk+1 − yk − PTk(yk+1 − yk)(cid:107)/αk
(cid:125)
(cid:123)(cid:122)
:=R1

(cid:124)

+ (cid:107)PTk(yk+1 − yk)/αk + ∇fM(yk) + PTk(νk)(cid:107)
(cid:123)(cid:122)
(cid:125)
:=R2

(cid:124)

.

(3.8.4)

In our bounds of these terms, we frequently use the following bound: there exists C > 0
such that

(cid:107)xk+1 − xk(cid:107) ≤ αk(cid:107)Gαk(xk, νk)(cid:107) ≤ C(1 + (cid:107)νk(cid:107))αk.

(3.8.5)

We now bound R1 and R2 separately.
The following claim bounds R1.

58

Claim 2. There exists C > 0 such that

R11τ >k ≤ C(1 + (cid:107)νk(cid:107))2αk1τ >k.

(3.8.6)

Proof. We consider two cases. First suppose xk+1 ∈ B2δ(¯x). Let C > 0 be a local Lipschitz
constant of ∇PM and PM. Then it follows that vector yk+1 − yk = PM(xk+1) − PM(xk) is
nearly tangent to the manifold at yk:

(cid:107)yk+1 − yk − PTk(yk+1 − yk)(cid:107) ≤ C(cid:107)yk+1 − yk(cid:107)2 ≤ C 2(cid:107)xk+1 − xk(cid:107)2.

Thus, taking into account (3.8.5), we have for some C > 0, the bound:

R1 ≤ C(1 + (cid:107)νk(cid:107))2αk,

as desired.

Now suppose that xk+1 /∈ B2δ(¯x). Therefore, there exists C > 0 such that

(cid:107)yk+1 − yk − PTk(yk+1 − yk)(cid:107) ≤ 2(cid:107)yk+1 − yk(cid:107) ≤ C(cid:107)xk+1 − xk(cid:107) ≤

C 2
δ

(cid:107)xk+1 − xk(cid:107)2,

where the ﬁrst inequality follows since (cid:107)PTk(cid:107) ≤ 1 and the second and third inequalities follow
from Claim 1. Thus taking into account (3.8.5), we again have for some C > 0, the bound:

R1 ≤ C(1 + (cid:107)νk(cid:107))2αk,

Thus, putting together both bounds on R1, the result follows.

The following claim bounds R2.
Claim 3. There exists C > 0 such that

R21τ >k ≤ C(1 + (cid:107)νk(cid:107))2(dist(xk, M) + αk)1τ >k.

(3.8.7)

Proof. To bound R2, we ﬁrst simplify:

R2 = (cid:107)PTk(yk+1 − yk)/αk + ∇Mf (yk) + PTk(νk)(cid:107)

≤ (cid:107)PTk(yk+1 − xk+1)/αk(cid:107) + (cid:107)PTk(xk − yk)/αk(cid:107) + (cid:107)PTk(xk+1 − xk)/αk + ∇Mf (yk) + PTk(νk)(cid:107)
≤ (cid:107)PTk(yk+1 − xk+1)/αk(cid:107) + C(1 + (cid:107)νk(cid:107))2(dist(xk, M) + α),

(3.8.8)

where the second equality follows from by Assumption A and the inclusion xk −yk ∈ NM(yk),
which implies that PTk(xk − yk) = 0. We now bound the term (cid:107)PTk(yk+1 − xk+1)/αk(cid:107).

First suppose that xk+1 ∈ B2δ(¯x) and note that yk+1 ∈ B4δ(¯x) ∩ M ⊆ U ∩ M. Let C (cid:48) > 0
be a local Lipschitz constant of ∇PM and PM. Then for some C > 0 larger than C (cid:48), we have

(cid:107)PTk(yk+1 − xk+1)/αk(cid:107) ≤ (cid:107)(PTk+1 − PTk)(yk+1 − xk+1)/αk(cid:107)

≤ C (cid:48)(cid:107)yk+1 − yk(cid:107)dist(xk+1, M)/αk
≤ (C (cid:48))2(cid:107)xk+1 − xk(cid:107)(dist(xk, M) + (cid:107)xk+1 − xk(cid:107))/αk
≤ C 3(1 + (cid:107)νk(cid:107))dist(xk, M) + C 4(1 + (cid:107)νk(cid:107))2αk,

59

where the ﬁrst inequality follows from xk+1 − yk+1 ∈ NM(yk+1), which implies PTk+1(yk+1 −
xk+1) = 0; the second inequality follows from Lipschitz continuity of ∇PM(y) = PTM(y) in
y; the third inequality follows from Lipschitz continuity of PM and Lipschitz continuity of
dist(·, M); and the fourth inequality follows from (3.8.5). Plugging this bound into (3.8.8),
yields that for some C > 0, we have

R2 ≤ C(1 + (cid:107)νk(cid:107))2(dist(xk, M) + αk),

as desired.

Now suppose that xk+1 /∈ B2δ(¯x). Then, there exists C > 0 such that

(cid:107)PTk(yk+1 − xk+1)/αk(cid:107) ≤ (cid:107)PTk(yk+1 − xk)(cid:107)/αk + (cid:107)PTk(xk − xk+1)(cid:107)/αk

≤ 2δ/αk + (cid:107)xk − xk+1(cid:107)/αk
≤ (1 + C)(cid:107)xk − xk+1(cid:107)/αk

≤

≤

(1 + C)C
δαk
(1 + C)C 3
δ

(cid:107)xk − xk+1(cid:107)2

(1 + (cid:107)νk(cid:107))2αk

where ﬁrst inequality follows from the triangle inequality; the second inequality follows since
xk ∈ B2δ(¯x) and yk+1 = ¯x; the third and fourth third inequalities follow from Claim 1; and
the ﬁfth follows from (3.8.5). Thus, in this case, we ﬁnd that there exists C > 0 with

R2 ≤ C(1 + (cid:107)νk(cid:107))2(dist(xk, M) + αk).

Therefore, putting together both bounds on R2, the result follows.

Now we prove Part 2a. Beginning with subpart 2(a)i, we ﬁnd that by Claim 2 and 3, we

have that for some C > 0, the bound

(cid:107)Ek(cid:107)1τ >k ≤ R11τ >k + R21τ >k ≤ C(1 + (cid:107)νk(cid:107))2(dist(xk, M) + αk)1τ >k,

(3.8.9)

as desired. Turning to Part 2(a)ii, ﬁrst note that that dist(xk, M)1τ >k ≤ δ. Thus, the bound
will follow if the conditional expectation of (1 + (cid:107)νk(cid:107))4 is bounded whenever xk ∈ Bδ(¯x).
This holds by assumption, since

Ek[(cid:107)νk(cid:107)4]1τ >k ≤ sup
x∈Bδ(¯x)

q(x) < ∞.

Finally, we prove Part subpart 2(a)iii. Again using the boundedness of the conditional fourth
moment of (cid:107)νk(cid:107)1τ >k, we ﬁnd that there exists a C > 0 such that

Ek[(cid:107)Ek(cid:107)21τ >k] ≤ Cdist2(xk, M)1τ >k + Cα2

k1τ >k,

(3.8.10)

where the ﬁrst inequality follows from Jensen’s inequality and the second inequality follows
from (3.8.9). Consequently, there exists C (cid:48) > 0 such that

E[(cid:107)Ek(cid:107)21τ >k] = E[Ek(cid:107)Ek(cid:107)21τ >k] ≤ CE[dist2(xk, M)1τ >k] + Cα2

k ≤ C (cid:48)αk,

60

where the third inequality follows from Part 3 of Proposition 3.3.1. This prove Part 2a.
Now we prove Part 2b, beginning with Part 2(b)i. To that end, deﬁne Fk = kγ−1

log(k+1)2 (cid:107)Ek(cid:107)1τ >k.

Recall that by the conditional Borel-Cantelli theorem (Lemma 3.11.2), the sequence Fk is
summable whenever Ek[Fk] is summable. Thus, we ﬁrst upper bound Ek[Fk] by a summable
sequence: there exists C > 0 such that

Ek[Fk] ≤ C

kγ−1

log(k + 1)2 (dist(xk, M) + αk)1τ >k

≤ C

kγ−1

log(k + 1)2 dist(xk, M)1τ >k + C

c2
k log(k + 1)2 ,

where the ﬁrst inequality follows from (3.8.10) and the second inequality follows by deﬁnition
of αk. By Part 1 of Proposition 3.3.1, it follows that we have upper bounded Ek[Fk] by a
summable sequence. Therefore, it follows that Fk is summable, as desired. This proves
part 2(b)i.

Now we prove part 2(b)ii. The conditional expectation is summable by Part 2(a)iii, since

∞
(cid:88)

k=k0

kγ−1
log(k + 1)2

E[(cid:107)Ek(cid:107)21τ >k] ≤ C

∞
(cid:88)

k=k0

k−1

log(k + 1)2 < +∞.

By conditional Borel-Cantelli theorem (Lemma 3.11.2), we also have that

∞
(cid:88)

k=k0

kγ−1

log(k + 1)2 (cid:107)Ek(cid:107)21τ >k < +∞,

as desired.

Now we prove Part 2c. To that end, note that there exists C > 0 such that

E[αk(cid:107)Ek(cid:107)1τ >k] = E[αkEk[(cid:107)Ek(cid:107)1τ >k]] ≤ CE[αkdist(xk, M)1τ >k + α2

k1τ >k].

where the inequality from (3.8.10). Thus, the result follows by Part 2b of Proposition 3.3.1.
Finally, we prove Part 2d. To that end, Part 3 of Proposition 3.3.1 ensures that for all

k ≥ k0, we have

dist(xk, M)1τ >k ≤ Ck−γ,
for some C > 0. Therefore, since (cid:107)νk(cid:107) ≤ µ/2, it follows from 2(a)i that there exists C (cid:48) > 0
with

(cid:107)Ek(cid:107)1τ >k ≤ C (cid:48)k−γ,

as desired.

3.9 Proofs of the main theorems

In this section, we prove the remaining theorems stated above: local rates of convergence,
asymptotic normality, and saddle avoidance. We now outline common notation and conven-
tions used in all of the sections. Common to all proofs below is that Assumption A holds at

61

a point ¯x with associated neighborhood U. When this assumption holds, there exists (cid:15) > 0
such that the function fM : B2(cid:15)(¯x) → R, deﬁned as the composition

fM := f ◦ PM

(3.9.1)

is C 2 and satisﬁes

∇fM(x) = ∇Mf (x)

and

∇2fM(x) = ∇2

Mf (x)

for all x ∈ B2(cid:15)(¯x)∩M. Moreover, we may also assume that the projection map PM : B2(cid:15)(¯x) →
Rd is C 2, in particular, Lipschitz with Lipschitz Jacobian. Throughout the proofs, we assume
that δ ≤ (cid:15)/4 is small enough that conclusions of Propositions 3.3.1 and 3.3.2 are valid; we
shrink δ several further times throughout the proofs. In addition, we let C denote a constant
depending on k0 and δ, which may change from line to line.

Now, denote stopping time (3.3.1) by τ := τk0,δ and the noise bound by Q := supx∈Bδ(¯x) q(x).

Observe that by Proposition 3.3.2, the shadow sequence yk satisﬁes yk ∈ B4δ(xk) ∩ M ⊆
B(cid:15)(¯x) ∩ M and recursion (3.3.2) holds. Due to the identity ∇fM(yk) = ∇Mf (yk), we use
the equivalent recursion throughout:

yk+1 = yk − αk∇fM(yk) − αkPTM(yk)(νk) + αkEk.

In addition, deﬁning

f ∗ := inf

x∈B(cid:15)(¯x)

fM,

we have the bound f ∗1τ >k ≤ f (yk)1τ >k for all k. We now turn to the proofs.

3.9.1 Proof of Theorem 3.4.1: general rates

We ﬁrst note that the summability and claimed bounds on the distance immediately follow
from Proposition 3.3.1. Thus, the remainder of the proof proves the desired summability
properties of the gradient.

To that end, since ∇fM is Lipschitz on B4δ(¯x), there exists C > 0 such that

Ek[(fM(yk+1) − f ∗)1τ >k]
≤ (fM(yk) − f ∗)1τ >k + (cid:104)∇fM(yk), Ek[yk+1 − yk](cid:105) 1τ >k + CEk(cid:107)yk+1 − yk(cid:107)21τ >k
≤ (fM(yk) − f ∗)1τ >k − αk(cid:107)∇fM(yk)(cid:107)21τ >k

+ αkEk[(cid:104)∇fM(yk), Ek(cid:105)]1τ >k
(cid:125)
(cid:123)(cid:122)
=:Zk,1

(cid:124)

+C Ek(cid:107)yk+1 − yk(cid:107)21τ >k
(cid:125)

(cid:124)

(cid:123)(cid:122)
=:Zk,2

.

(3.9.2)

We now prove that Zk,1 and Zk,2 are summable.

Let us begin with Zk,1. To that end, we ﬁrst observe that αk ≤ C kγ−1

log(k+1)2 for some C > 0.
Consequently, Parts 2(b)i and 2d of Proposition 3.3.2 both imply the sequence αkEk(cid:107)Ek(cid:107)1τ >k
is summable. Thus, since (cid:107)∇fM(y)(cid:107) is bounded in B4δ(¯x) by continuity, there exists C > 0
such that

|Zk,1| ≤ CαkEk[(cid:107)Ek(cid:107)]1τ >k

Thus, Zk,1 is summable.

62

We now turn to Zk,2. To that end, we ﬁrst note that Parts 2(b)i and 2d of Propo-
Ek[(cid:107)Ek(cid:107)2]1τ >k is summable. Moreover, there

sition 3.3.2 both imply that the sequence α2
k
exists C > 0 such that

|Zk,2| ≤ Ek(3α2

k(cid:107)∇fM(yk)(cid:107)2 + 3α2

k(cid:107)Ek(cid:107)2 + 3α2

k(cid:107)νk(cid:107)2)1τ >k ≤ C(1 + Ek[(cid:107)Ek(cid:107)2]1τ >k + Q)α2
k.

Therefore it follows that Zk,2 is also summable as desired.

Now deﬁne nonnegative Fk adapted random variables Xk := (fM(yk) − f ∗)1τ >k, Yk :=
αk(cid:107)∇fM(yk)(cid:107)2, and Zk := Zk,1 + Zk,2. It follows from (3.9.2) and the bound 1τ >k+1 ≤ 1τ >k,
that

Ek[Xk+1] ≤ Xk − Yk + Zk.

Thus, by Robbins-Siegmund Lemma (Lemma 3.11.1), we ﬁnd that Yk is almost surely
summable, as claimed.

3.9.2 Proof of Theorem 3.4.2: rates near strong local minimizers

We ﬁrst begin with a Lemma ensuring ∇2fM(x) has suﬃcient curvature in B2δ(¯x).

Lemma 3.9.1. For all suﬃciently small δ and all x ∈ B2δ(¯x), the following hold. First, the
Hessian satisﬁes:

(y − ¯x)(cid:62)∇2fM(x)(y − ¯x) ≥

σ
4

(cid:107)y − ¯x(cid:107)2 ,

Second, the negative gradient points towards ¯x:

for all x ∈ B2δ(¯x), y ∈ B4δ(¯x) ∩ M.

(cid:104)∇fM(y), y − ¯x(cid:105) ≥

σ
4

(cid:107)y − ¯x(cid:107)2

for all y ∈ B4δ(¯x) ∩ M.

Proof. Observe that by continuity of ∇2fM(x) near ¯x, the following holds for all small δ:

u(cid:62)∇2fM(x)u ≥ σ/2,

for all u ∈ Sd−1 ∩ TM(¯x) and x ∈ B2δ(¯x).

In addition, for all y ∈ M near ¯x, there exists some constant C that

(cid:13)y − ¯x − PTM(¯x)(y − ¯x)(cid:13)
(cid:13)

(cid:13) ≤ C (cid:107)y − ¯x(cid:107)2 .

y−¯x
(cid:107)y−¯x(cid:107) is in the tangent space up to error at the order of (cid:107)y − ¯x(cid:107), which
Hence, when y (cid:54)= ¯x,
yields the second statement of the theorem, possibly after shrinking δ. To complete the
proof, note that by Newton-Leibniz rule,

(cid:104)∇fM(y), y − ¯x(cid:105) = (cid:104)∇fM(y) − ∇fM(¯x), y − ¯x(cid:105)

=

≥

as desired.

(cid:90) 1

(y − ¯x)(cid:62)∇2fM(¯x + t(y − ¯x))(y − ¯x)dt

0
σ
4

(cid:107)y − ¯x(cid:107)2 ,

63

Turning to the proof, we begin a preliminary bound, which will also be used in our proof

of asymptotic normality.

Lemma 3.9.2. For all suﬃciently small δ, there exists a constant C such that for any
k ≥ k0, we have

Ek[(cid:107)yk+1 − ¯x(cid:107)2 1τ >k] ≤ (1 − αkσ/2) (cid:107)yk − ¯x(cid:107)2 1τ >k + Cα2

k + α2
k
+ 2αkEk[(cid:104)yk − αk∇fM(yk) − ¯x, Ek(cid:105) 1τ >k].

Ek[(cid:107)Ek(cid:107)2 1τ >k]

(3.9.3)

Proof. Expanding (cid:107)yk+1 − ¯x(cid:107)2, we obtain

(cid:13)yk − αk∇fM(yk) − αkPTM(yk)(νk) + αkEk − ¯x(cid:13)
2 1τ >k
(cid:13)
(cid:13)PTM(yk)(νk)(cid:13)
(cid:13)
2 1τ >k
(cid:13)

(cid:107)yk+1 − ¯x(cid:107)2 1τ >k
= (cid:13)
= (cid:107)yk − αk∇fM(yk) + αkEk − ¯x(cid:107)2 1τ >k + α2
k
+ 2αk
= (cid:107)yk − αk∇fM(yk) − ¯x(cid:107)2 1τ >k + α2
+ α2
k

(cid:13)PTM(yk)(νk)(cid:13)
(cid:13)
2 1τ >k + 2αk
(cid:13)

(cid:10)yk − αk∇fM(yk) + αkEk − ¯x, PTM(yk)(νk)(cid:11) 1τ >k

k (cid:107)Ek(cid:107)2 1τ >k + 2αk (cid:104)yk − αk∇fM(yk) − ¯x, Ek(cid:105) 1τ >k

(cid:10)yk − αk∇fM(yk) + αkEk − ¯x, PTM(yk)(νk)(cid:11) 1τ >k.

(3.9.4)

In addition, Lemma 3.9.1, ensures that for some C > 0, we have

(cid:107)yk − αk∇fM(yk) − ¯x(cid:107)2 1τ >k = (cid:0)(cid:107)yk − ¯x(cid:107)2 − 2αk (cid:104)∇fM(yk), yk − ¯x(cid:105) + α2

k (cid:107)∇fM(yk)(cid:107)2(cid:1) 1τ >k

≤ (cid:0)(1 − αkσ/2) (cid:107)yk − ¯x(cid:107)2 + Cα2

k

(cid:1) 1τ >k

To complete the proof, plug the above bound into (3.9.4), take the conditional expectation
of both sides, recall that Ek[(cid:107)PTM(yk)(νk)(cid:107)2] ≤ Q1/2, and apply Cauchy-Schwarz to the dot
product:

2αkEk[(cid:10)yk − αk∇fM(yk) + αkEk − ¯x, PTM(yk)(νk)(cid:11) 1˜τ >k] = 2α2
≤ α2
k

Ek[(cid:10)Ek, PTM(yk)(νk)(cid:11)]
k
Ek(cid:107)Ek(cid:107)21τ >k + Q1/2α2
k,

as desired.

We now prove Part 1. To that end, Deﬁne Xk := (cid:107)yk − ¯x(cid:107)2 1τ >k and Yk = αkσ
4 Xk.
Observe that C > 0, satisfying C > (cid:107)(yk − αk∇fM(yk) − ¯x)(cid:107) 1τ >k for all k ≥ k0. Thus, by
this bound and Lemma 3.9.2, there exists C > 0 such that

Ek[Xk+1] ≤ (1 − σαk/4)Xk − Yk

k + α2
k

+ Cα2
≤ (1 − σαk/4)Xk − Yk + Cα2

Ek[(cid:107)Ek(cid:107)2 1˜τ >k] + 2αkEk[(cid:104)yk − αk∇fM(yk) − ¯x, Ek(cid:105) 1˜τ >k].

k + α2
k

Ek[(cid:107)Ek(cid:107)2 1˜τ >k] + CαkEk[(cid:107)Ek(cid:107) 1˜τ >k],

where the second inequality follows from Cauchy-Schwarz. Now deﬁne

Zk := Cα2

k + α2
k

Ek[(cid:107)Ek(cid:107)2 1˜τ >k] + CαkEk[(cid:107)Ek(cid:107) 1˜τ >k]

64

and note that by Proposition 3.3.2, we have

∞
(cid:88)

k=1

(k + 1)2γ−1
log(k + 2)2 Zk < +∞.

Moreover, we have

σc1
4kγ
Applying Lemma 3.11.6, we therefore deduce that there exists a ﬁnite random variable Vy

Xk − Yk + Zk.

EkXk+1 ≤

1 −

(cid:16)

(cid:17)

k2γ−1
log2(k + 1)

(cid:107)yk − ¯x(cid:107)2 1τ >k

a.s.−−→ Vy

and

∞
(cid:88)

k=1

kγ−1

log(k + 1)2 (cid:107)yk − ¯x(cid:107)2 1τ >k < +∞

(3.9.5)

Similarly by Proposition 3.3.1 there exists a ﬁnite random variable Vx such that

k2γ−1
log2(k + 1)

dist2(xk, M)1τ >k

a.s.−−→ Vx

and

∞
(cid:88)

k=1

kγ−1

log(k + 1)2 dist2(xk, M)1τ >k < +∞
(3.9.6)

To complete the proof, note the decomposition

(cid:107)xk − ¯x(cid:107)2 ≤ 2dist2(xk, M) + 2(cid:107)yk − ¯x(cid:107)2

and apply simply apply (3.9.5) and (3.9.6). This completes the proof of Part 1

Now we prove Part 2: Applying Proposition 3.3.2 to the case when νk = 0, we ﬁnd that

yk satisﬁes

yk+1 = yk − αk∇fM(yk) + αkEk

for k ≥ k0,

where the error sequence {Ek} satisﬁes

(cid:107)Ek(cid:107)1τ >k ≤ Ck−γ

for k ≥ k0.

By Newton-Leibniz rule and Lemma 3.9.1, we obtain in the event {τ > k}, that

(cid:107)yk+1 − ¯x(cid:107) = (cid:107)(yk − αk∇fM(yk)) − (¯x − αk∇fM(¯x)) + αkEk(cid:107)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:16)

(cid:90) 1

0

1 −

(cid:16)

1 −

(cid:17)

(cid:13)
(cid:13)
(cid:0)I − αk∇2fM(¯x + t(yk − ¯x))(cid:1) (yk − ¯x)dt
(cid:13)
(cid:13)
αkσ
4
c1σ
4

(cid:107)yk − ¯x(cid:107) + Cαkk−γ

(cid:107)yk − ¯x(cid:107) + Ck−2γ

k−γ(cid:17)

+ αk (cid:107)Ek(cid:107)

≤

≤

≤

Applying Lemma 3.11.8 to the sequence sk = (cid:107)yk − ¯x(cid:107) 1τ >k, it follows that there exists a
constant C such that

(cid:107)yk − ¯x(cid:107) 1τ >k ≤ Ck−γ

for k ≥ k0.

The result therefore follows by (i) bounding (cid:107)xk − ¯x(cid:107) ≤ dist(xk, M) + (cid:107)yk − ¯x(cid:107) and (ii)
applying Proposition 3.3.1, which ensures that for some C > 0, we have dist(xk, M) ≤ Ck−γ
for all k ≥ k0.

65

3.9.3 Proof of Theorem 3.5.1: asymptotic normality

Throughout we adopt the same conventions that were outlined at the start of Section 3.9,
except that we write τk0 = τk0,δ, since we will consider several values of k0. Under these
conventions, we have the following Proposition, which will be useful in ensuring summability
of certain sequences.

Lemma 3.9.3. Under the condition of Theorem 3.5.1, there exists C > 0 such that

1. E[(cid:107)yk − ¯x(cid:107)2 1τk0 >k] ≤ C/kγ for all k ≥ 1.
2. (cid:80)∞

(cid:107)yk − ¯x(cid:107)2 < ∞ almost surely.

1√
k

k=1

3.

1√
n

(cid:80)n

k=1 (cid:107)yk − ¯x(cid:107)2 → 0 almost surely.

4. (cid:80)∞

k=1

1√
k

(cid:107)Ek(cid:107) < +∞.

Proof. We ﬁrst prove Part 1. Recall by Proposition 3.3.2 that there exists C > 0 such that
E[(cid:107)Ek(cid:107)21τk0 >k] ≤ Cαk for all k ≥ 1. In addition, we may also assume by enlarging C that
(cid:107)∇f (yk)(cid:107)1τk0 >k ≤ C for all k ≥ 1. Therefore, there exists C (cid:48) > 0 such that

E[(cid:104)yk − αk∇fM(yk) − ¯x, Ek(cid:105) 1τk0 >k]
(cid:16)
≤ CαkE[(cid:107)Ek(cid:107)] +

E[(cid:107)yk − ¯x(cid:107)2 1τk0 >k]

(cid:17) 1

2 (cid:16)

E[(cid:107)Ek(cid:107)2 1τk0 >k]

(cid:17) 1

2

E[(cid:107)yk − ¯x(cid:107)2 1τk0 >k]

(cid:17) 1

2 α1/2
k

k + C 1/2 (cid:16)
σ
4

≤ C 3/2α3/2

≤ C 3/2α3/2
k +
σ
4

≤ C (cid:48)αk +

E[(cid:107)yk − ¯x(cid:107)2 1τk0 >k] +

E[(cid:107)yk − ¯x(cid:107)2 1τk0 >k],

4Cαk
σ

(3.9.7)

where the ﬁrst inequality follows by Cauchy-Schwarz and the third inequality follows by
Young’s inequality. Therefore, by inequality (3.9.3) of Lemma 3.9.2, we obtain there exists
C (cid:48)(cid:48) > 0 such that

E[(cid:107)yk+1 − ¯x(cid:107)2 1τk0 >k+1] ≤ E[(cid:107)yk+1 − ¯x(cid:107)2 1τk0 >k]

= E[Ek[(cid:107)yk+1 − ¯x(cid:107)2 1τk0 >k]]
≤ (1 − αkσ/2)E[(cid:107)yk − ¯x(cid:107)2 1τk0 >k] + Cα2
+ 2αkE[(cid:104)yk − αk∇fM(yk) − ¯x, Ek(cid:105) 1τk0 >k]
≤ (1 − αkσ/4)E[(cid:107)yk − ¯x(cid:107)2 1τk0 >k] + C (cid:48)(cid:48)α2
k,
where the ﬁrst inequality follows from Proposition 3.3.2 and the ﬁnal inequality follows
from (3.9.7). To complete the proof apply Lemma 3.11.8 to the sequence sk = E[(cid:107)yk − ¯x(cid:107)2 1τk0 >k].

E[[(cid:107)Ek(cid:107)2 1τk0 >k]

k + α2
k

We now prove Part 2. By Part 1, we have

(cid:34) ∞
(cid:88)

E

k=1

1
√
k

(cid:35)

(cid:107)yk − ¯x(cid:107)2 1τk0 >k

≤

∞
(cid:88)

k=1

C
kγ+ 1

2

< ∞.

66

k=1

1√
k

Therefore, (cid:80)∞
(cid:107)yk − ¯x(cid:107)2 1τk0 >k is ﬁnite almost surely. Since xk → ¯x almost surely, for
almost every sample path, we can ﬁnd a k0 such that τk0 = ∞. Therefore, (cid:80)∞
(cid:107)yk − ¯x(cid:107)2
is ﬁnite almost surely. Note that Part 3 now immediately follows from Kronecker lemma 3.11.5
Finally, we prove Part 4. By Proposition 3.3.2, we know that the error sequence Ek

1√
k

k=1

almost surely satisﬁes

∞
(cid:88)

k=1

1
√
k

(cid:107)Ek(cid:107)1τk0 >k < +∞.

Since xk → ¯x almost surely, for almost every sample path, we can ﬁnd a k0 such that τk0 = ∞.
Therefore, almost surely we have (cid:80)∞

(cid:107)Ek(cid:107) < +∞, as desired.

1√
k

k=1

Turning to the proof, we introduce an additional shadow sequence

zk = P¯x+TM(¯x)(yk).

(3.9.8)

Evidently, for all δ suﬃciently small, zk closely approximates yk. Indeed, due to the smooth-
ness of M, there exists C > 0 such that

(cid:107)yk − zk(cid:107)1τk0 >k ≤ C(cid:107)yk − ¯x(cid:107)21τk0 >k.

(3.9.9)

Our ﬁrst result states that it suﬃces to study the distribution of

1√
n

(cid:80)n

k=1(zk − ¯x).

Lemma 3.9.4. If
1√
n

(cid:80)n

k=1(xk − ¯x) converges in distribution to D.

1√
n

(cid:80)n

k=1(zk − ¯x) converges in distribution to some distribution D, then

Proof. Note that

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
√
n

n
(cid:88)

k=1

(xk − ¯x)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

=

1
√
n

n
(cid:88)

k=1

(zk − ¯x) +

1
√
n

n
(cid:88)

(xk − yk) +

k=1

1
√
n

n
(cid:88)

(yk − zk).

k=1

By the [34, Exercise 3.2.13], the result will follow if the following two limits hold:

1
√
n

n
(cid:88)

k=1

(xk − yk) → 0

and

1
√
n

n
(cid:88)

(yk − zk) → 0.

k=1

almost surely. To that end, we recall that Proposition 3.3.1 guarantees that almost surely
we have

∞
(cid:88)

k=1

1
√
k

(cid:107)xk − yk(cid:107) 1τk0 >k < +∞

Since xk → ¯x almost surely, for almost every sample path, we can ﬁnd a k0 such that τk0 = ∞.
Therefore, almost surely we have (cid:80)∞
(cid:107)xk−yk(cid:107)
< ∞. Applying Kronecker lemma 3.11.5,
√
k
almost surely we have

k=1

n
(cid:88)

1
√
n

(cid:107)xk − yk(cid:107) → 0,

k=1

67

which implies
inequality (3.9.9), that

1√
n

(cid:80)n

k=1(xk − yk) → 0. On the other hand, we have by Lemma 3.9.3 and

∞
(cid:88)

k=1

1
√
k

(cid:107)yk − zk(cid:107) 1τk0 >k ≤

∞
(cid:88)

k=1

C
√
k

(cid:107)yk − ¯x(cid:107)2 1τk0 >k < +∞

Again since for almost every sample path we may ﬁnd k0 such that τk0 = ∞, we have that
(cid:80)∞

(cid:107)yk − zk(cid:107) < +∞, as desired.

1√
k

k=1

In the remainder of the proof, we study the limit distribution of

k=1(zk − ¯x). In the
following Lemma, notice that we state the covariance matrix in a diﬀerent, equivalent form
that is more convenient for computation.

(cid:80)n

1√
n

Lemma 3.9.5. Under the conditions of Theorem 3.5.1,
tribution to

N (cid:0)0, U (U (cid:62)∇2fM(¯x)U )−1Σ(U (cid:62)∇2fM(¯x)U )−1U (cid:62)(cid:1) ,

1√
n

(cid:80)n

k=1(zk − ¯x) converges in dis-

where that U is a matrix whose column vectors form an orthogonal basis of TM(¯x)

Proof. Clearly, zk = ¯x + U U (cid:62)(yk − ¯x). Moreover, multiplying both sides of (3.3.2) by U (cid:62),
we have

U (cid:62)(yk+1 − ¯x) = U (cid:62)(yk − ¯x) − αkU (cid:62)∇fM(yk) − αkU (cid:62)PTM(yk)(νk) + αkU (cid:62)Ek

= U (cid:62)(yk − ¯x) − αkU (cid:62)∇2fM(¯x)(yk − ¯x) − αk(U (cid:62)∇fM(yk) − U (cid:62)∇2fM(¯x)(yk − ¯x))

− αkU (cid:62)PTM(¯x)(νk) − αk(U (cid:62)PTM(yk)(νk) − U (cid:62)PTM(¯x)(νk)) + αkU (cid:62)Ek

= U (cid:62)(yk − ¯x) − αkU (cid:62)∇2fM(¯x)U U (cid:62)(yk − ¯x) − αkU (cid:62)∇2fM(¯x)(I − U U (cid:62))(yk − ¯x)

− αk(U (cid:62)∇fM(yk) − U (cid:62)∇2fM(¯x)(yk − ¯x)) − αkU (cid:62)PTM(¯x)(νk)
− αk(U (cid:62)PTM(yk)(νk) − U (cid:62)PTM(¯x)(νk)) + αkU (cid:62)Ek

Deﬁne ∆k = U (cid:62)(yk − ¯x), H = U (cid:62)∇2fM(¯x)U , ζk = U (cid:62)PTM(yk)(νk) − U (cid:62)PTM(¯x)(νk), and

R(y) = U (cid:62)∇2fM(¯x)(I − U U (cid:62))(y − ¯x) + U (cid:62)∇fM(yk) − U (cid:62)∇2fM(¯x)(y − ¯x).

By our assumption, H is positive deﬁnite and U (cid:62)PTM(¯x)(νk) = U (cid:62)U U (cid:62)νk = U (cid:62)νk. Thus we
can rewrite the update of ∆k as

∆k+1 = ∆k − αkH∆k − αkU (cid:62)νk − αk

(cid:0)R(yk) + ζk − U (cid:62)Ek

(cid:1) .

In the remainder of the proof, we prove that

N (cid:0)0, (U (cid:62)∇2fM(¯x)U )−1Σ(U (cid:62)∇2fM(¯x)U )−1(cid:1) . This implies that result since 1√
1√
n

k=1 U ∆k. We note that our proof closely mirrors [1, Theorem 2].

(cid:80)n

(cid:80)n

1√
n

k=1 ∆n converges in distribution to
n (zk − ¯x) =

To prove this claim, deﬁne matrices

n
(cid:88)

i
(cid:89)

Bn

k = αk

(I − αjH) and An

k = Bn

k − H −1.

i=k

j=k+1

68

Polyak and Juditsky [78, Lemma 2] show that ¯∆n = 1

n

(cid:80)n

k=1 ∆k satisﬁes the equality

√

n ¯∆n =

1
√
n

+

=

1
√
n

+

n
(cid:88)

k=1
1
√
n

n
(cid:88)

k=1
1
√
n

H −1U (cid:62)νk

n
(cid:88)

k=1

An

k U (cid:62)νk +

1
√
n

n
(cid:88)

k=1

H −1U (cid:62)ν(1)
k

Bn

k [R(yk) + ζk − U (cid:62)Ek] + O

(cid:19)

(cid:18) 1
√
n

n
(cid:88)

k=1

An

k U (cid:62)ν(1)

k +

1
√
n

n
(cid:88)

k=1

Bn

k [R(yk) + ζk − U (cid:62)Ek + ν(2)

k (xk)] + O

(cid:18) 1
√
n

(cid:19)

,

where supk,n max{(cid:107)Bn
sumption, the sum 1√
n

k (cid:107) , (cid:107)An
(cid:80)n

k (cid:107)} < +∞ and limn→∞

1
n
converges in distribution to

k=1 (cid:107)An

(cid:80)n

k=1 H −1U (cid:62)ν(1)

k

k (cid:107) = 0. Notice that by as-

N (cid:0)0, (U (cid:62)∇2fM(¯x)U )−1Σ(U (cid:62)∇2fM(¯x)U )−1(cid:1) .

Thus the claim holds if we can show that the other sums in our expression for
to 0 almost surely. To complete the proof, we now prove these limits.
Claim 4. We have that

1
√
n

n
(cid:88)

k=1

An

k U (cid:62)ν(1)

k

a.s.−−→ 0.

√

n ¯∆n converge

Proof. Observe that (cid:107)An

k (cid:107) is bounded, so

E





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
√
n

n
(cid:88)

k=1

An

k U (cid:62)vk



1τk0 >k

 =

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤

1
n

C
n

n
(cid:88)

E

(cid:20)(cid:13)
(cid:13)An
(cid:13)

k U (cid:62)ν(1)

k 1τk0 >k

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

k=1

n
(cid:88)

k=1

(cid:107)An
k (cid:107)

→ 0.

(cid:80)n

1√
n
1√
n

As a result,

k=1 An
(cid:80)n

k U (cid:62)νk1τk0 >k is a L2-bounded martingale. By [34, Theorem 4.4.6],
L2
−→ 0. On the other hand, by [34, Theorem 4.2.11],
k U (cid:62)νk1τk0 >k converges almost surely. Therefore, since for almost every sample

we know that
1√
n
path there exists k0 such that τk0 = ∞, we have 1√
n

a.s.−−→ 0, as desired.

k U (cid:62)νk1τk0 >k

k=1 An

k=1 An

k=1 An

k U (cid:62)νk

(cid:80)n

(cid:80)n

Claim 5. We have that

1
√
n

n
(cid:88)

k=1

Bn

k U (cid:62)R(yk) a.s.−−→ 0.

69

Proof. Recall that

R(y) = U (cid:62)∇2fM(¯x)(I − U U (cid:62))(y − ¯x) + U (cid:62)∇fM(yk) − U (cid:62)∇2fM(¯x)(y − ¯x).

By smoothness of M and the fact that fM is C 2, for y near ¯x, we have (cid:107)R(y)(cid:107) = O((cid:107)y − ¯x(cid:107)2).
a.s.−−→ ¯x. Consequently, there exists
In addition, by our assumption that xk
a constant C depending on sample path such that (cid:107)R(yk)(cid:107) ≤ C (cid:107)yk − ¯x(cid:107)2 almost surely. By
n (cid:107)R(yk)(cid:107) a.s.−−→ 0. Therefore,
Lemma 3.9.3, we know that 1√

k U (cid:62)R(yk) a.s.−−→ 0.

a.s.−−→ ¯x, we have yk

k=1 Bn

(cid:80)n

1√
n

Claim 6. We have that

1
√
n

n
(cid:88)

k=1

Bn

k ζk

a.s.−−→ 0.

Proof. For k ≥ 1, deﬁne truncated variables ζ (k0)

k = ζk1τk0 >k. Note that suﬃces to show that

1
√
n

n
(cid:88)

k=1

Bn

k ζ (k0)

k

a.s.−−→ 0,

since on every sample path there exists a k0 such that τk0 = ∞. Thus, we will work with
these truncated variables throughout.

Turning to the proof, we ﬁrst show that

k=1 ζ (k0)
Recall that ζk = U (cid:62)PTM(yk)(νk) − U (cid:62)PTM(¯x)(νk), so we have
(cid:104)
ζ (k0)
k

= E [ζk | Fk] 1τk0 >k = 0.

| Fk

1√
n

E

(cid:105)

k

(cid:80)n

P−→ 0 and 1√
n

(cid:80)n

k=1 An

k ζ (k0)

k

P−→ 0.

Since x (cid:55)→ PTM(x) is locally Lipschitz on a neighborhood of ¯x in M, we have the following
bound for some C > 0 and all suﬃciently small δ:

(cid:13)
(cid:13)ζ (k0)
(cid:13)

k

(cid:13)
(cid:13)
(cid:13) ≤ C (cid:107)yk − ¯x(cid:107) 1τk0 >k,

In particular, it holds that

E

(cid:20)(cid:13)
(cid:13)ζ (k0)
(cid:13)

k

(cid:13)
2
(cid:13)
(cid:13)

(cid:21)

| Fk

≤ C 2 (cid:107)yk − ¯x(cid:107)2 1τk0 >k.

Combining with Lemma 3.9.3, we know that ζ (k0)
almost surely,

k

is a martingale diﬀerence sequence and

∞
(cid:88)

k=1

E

1
k

(cid:20)(cid:13)
(cid:13)ζ (k0)
(cid:13)

k

(cid:13)
2
(cid:13)
(cid:13)

| Fk

(cid:21)

≤ C 2

∞
(cid:88)

k=1

1
k

(cid:107)yk − ¯x(cid:107)2 < ∞.

Therefore, by Lemma 3.11.4, we have

1
√
n

n
(cid:88)

k=1

ζ (k0)
k

a.s.−−→ 0.

In particular, it holds that 1√
n

(cid:80)n

k=1 ζ (k0)

k

P−→ 0.

70

Next we show that for any k0 < ∞, we have n−1/2 (cid:80)n

that by Lemma 3.9.3, there exists C (cid:48) > 0 such that
(cid:20)(cid:13)
(cid:13)ζ (k0)
(cid:13)

≤ CE

(cid:104)
(cid:107)yk − ¯x(cid:107)2 1τk0 >k

(cid:13)
(cid:13)
(cid:13)

2(cid:21)

E

k

k=1 An

k ζ (k0)

k

P−→ 0. To see this, note

(cid:105)

≤ C (cid:48)αk.

(3.9.10)

Hence, the following limit holds

E





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
√
n

n
(cid:88)

k=1

An

k ζ (k0)

k

2
 =

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n

n
(cid:88)

E

k=1

(cid:20)(cid:13)
(cid:13)An
(cid:13)

k ζ (k0)

k

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

≤

C (cid:48)αk
n

n
(cid:88)

k=1

(cid:107)An

k (cid:107)2 ≤

C (cid:48)αk supk,n (cid:107)An
k (cid:107)
n

∞
(cid:88)

k=1

(cid:107)An

k (cid:107) → 0,

where the ﬁrst equality follows from the martingale diﬀerence property, the second inequality
follows from the boundedness of moments of ζ (k0)
k (cid:107) is bounded.
Consequently, we have shown that

, and the limit holds since (cid:107)An

k

1
√
n

n
(cid:88)

k=1

An

k ζ (k0)

k

L2
−→ 0,

which implies that 1√
n

k ζ (k0)
We have therefore proved that 1√
n

k=1 An

k

(cid:80)n

P−→ 0.
(cid:80)n

k ζ (k0)
k ζ (k0)
converges almost surely. Since the almost sure limits and limits in probability agree when
both exist, this will complete the proof.
To this end, deﬁne the sequence

P−→ 0. We now show that 1√
n

k=1 Bn

k=1 Bn

k

k

(cid:80)n

Zn,k0 =

n
(cid:88)

k=1

Bn

k ζ (k0)

k

.

The result follows if we can prove that for any ﬁnite k0, the sequence n−1/2Zn,k0 almost surely
converges. To that end, note that Bn+1

i=k+1(I − αiH). Thus, deﬁning

k − Bn

k = αk

(cid:81)n

W n

k =

n
(cid:89)

(I − αiH),

i=k

Vn,k0 =

n
(cid:88)

k=1

αkW n+1

k+1 ζ (k0)

k

,

we deduce that Vn,k0 is Fn+1 measurable and Zn,k0 admits the decomposition:

Zn,k0 = Zn−1,k0 + Vn−1,k0 + αnζ (k0)

n =

n−1
(cid:88)

k=1

Vk,k0 +

n
(cid:88)

k=1

αkζ (k0)
k

.

k=1 αkζ (k0)

Note that the sum (cid:80)n
is a square-integrable martingale with summable squared
increments, so it converges almost surely [34, Theorem 4.2.11]. As a result, we have the
following limit n−1/2 (cid:80)n
k=1 Vk,k0
converges almost surely.

It thus suﬃces to show that n−1/2 (cid:80)n−1

k=1 αkζ (k0)

a.s.−−→ 0.

k

k

To that end, let λ denote the smallest eigenvalue of H. Then we have

E (cid:2)(cid:107)Vn,k0(cid:107)2(cid:3) =

n
(cid:88)

k=1

α2
k

(cid:13)
(cid:13)W n+1
k+1

(cid:13)
2 E
(cid:13)

(cid:20)(cid:13)
(cid:13)ζ (k0)
(cid:13)

k

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

≤ C (cid:48)

n
(cid:88)

α3
k

n+1
(cid:89)

k=1

i=k+1

|1 − λαi| ,

71

where the inequality follows from the bound E

(cid:20)(cid:13)
(cid:13)ζ (k0)
(cid:13)

k

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

≤ C (cid:48)αk (see Equation (3.9.10))

By [1, Lemma A.7], there thus exists some constant C such that

Hence, for any (cid:15) > 0, we can ﬁnd some C such that

E (cid:2)(cid:107)Vn,k0(cid:107)2(cid:3) ≤

C log n
n2γ

.

E (cid:2)(cid:107)Vn,k0(cid:107)2(cid:3) ≤

C
n2γ−(cid:15) .

Now deﬁne Tn,k0 = 1√
n
for any (cid:15) > 0 there exists C, C (cid:48) > 0 such that

(cid:80)n

k=1 Vk,k0. We claim that Tn,k0 almost surely has ﬁnite length. Indeed,

E [(cid:107)Tn,k0 − Tn+1,k0(cid:107)] ≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)

√

1
n + 1

−

1
√
n

(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

E [(cid:107)Vk,k0(cid:107)] +

√

1
n + 1

E [(cid:107)Vn+1,k0(cid:107)]

k=1
1
√
n

1
nγ−(cid:15)

≤

≤

n
(cid:88)

1
kγ−(cid:15) +

C
n 3

2

k=1
C (cid:48)
nγ+1/2−(cid:15) .

2, 1), we therefore have (cid:80)

E [(cid:107)Tn,k0 − Tn+1,k0(cid:107)] < ∞. Consequently, the sum is
Since γ ∈ ( 1
ﬁnite almost surely: (cid:80)
k=1 Vk,k0
converges almost surely. Recalling the deﬁnition of Vk,k0, we ﬁnd that n−1/2Zn,k0 almost
surely converges, which completes the proof.

n (cid:107)Tn,k0 − Tn+1,k0(cid:107) < +∞. This implies that Tn,k0 = n−1/2 (cid:80)n

n

Claim 7. We have that

1
√
n

n
(cid:88)

k=1

Bn

k ν(2)

k (xk) a.s.−−→ 0.

Proof. This may be proved by argument that mirrors Claim 6.
sequence ξk = ν(2)
C > 0

Indeed, observe that the
k (xk)1τ >k0 is a martingale diﬀerence sequence, the bounds hold for some

Ek[(cid:107)ξk(cid:107)2] ≤ C(cid:107)xk − ¯x(cid:107)21τ >k0

and

E[(cid:107)ξk(cid:107)2] ≤ Cαk,

and (cid:80)∞
k=1
to prove Claim 6.

1
k

Ek[(cid:107)ξk(cid:107)2] ≤ (cid:80)∞

k=1

1

k (cid:107)xk − ¯x(cid:107)21τ >k0 < +∞. Only these facts for ζ (k0)

k were used

Claim 8. We have that

1
√
n

n
(cid:88)

k=1

Bn

k U (cid:62)Ek

a.s.−−→ 0.

Proof. Recall that (cid:80)∞
1√
k
there exists C (cid:48) > 0 such that almost surely we have

k=1

(cid:107)Ek(cid:107) < ∞ almost surely and that supk,n (cid:107)Bn

k (cid:107) < ∞. Therefore,

∞
(cid:88)

k=1

1
√
k

(cid:13)
(cid:13)Bn

k U (cid:62)Ek

(cid:13)
(cid:13) ≤ C (cid:48)

∞
(cid:88)

k=1

1
√
k

(cid:107)Ek(cid:107) < ∞.

72

Therefore, by Kronecker lemma, we have
n
(cid:88)

1
√
n

k=1

Bn

k U (cid:62)Ek

a.s.−−→ 0,

as desired.

Taking these claims into account, the proof is complete.

3.9.4 Proof of Theorem 3.6.1: nonconvergence of stochastic pro-

cess

We begin by recalling and slightly reframing Proposition 3 in [74]. This result provides a
Lyapunov function, which we will use to show that each local process Yk escapes a local
neighborhood of each p ∈ S.
Proposition 3.9.6 (Lyapunov Function). Fix p ∈ Rd and suppose F : Rd → Rd is a C 2
mapping that vanishes at p and has symmetric Jacobian ∇F (p). Suppose that ∇F (p) has at
least one positive eigenvalue and let W denote the subspace of eigenvectors of ∇F (p) with
positive eigenvalues. Then, there exists a matrix A ∈ Rd×d with range(AT ) = W , a ball B
centered at p, and a C 2 mapping Φ : B → Rd with Φ(p) = p and ∇Φ(p) = Id such that the
weakly convex function η : B → R deﬁned as

satisﬁes the following condition: There exists c, c(cid:48) > 0 such that

η(v) = (cid:107)A(Φ(v) − p)(cid:107)2

η(v + (cid:15)F (v)) ≥ (1 + c(cid:15))η(v) − c(cid:48)(cid:15)2

for v in B and all suﬃciently small (cid:15).

In particular, we have

η(cid:48)(v; F (v)) ≥ cη(v)

for all v ∈ B.

Turning to the proof of Theorem 3.6.1, we begin with a covering argument: For any p ∈ S,
choose (cid:15)p small enough that both the conditions of Theorem 3.6.1 and Proposition 3.9.6
hold in B(cid:15)p(p). Let δp ≤ (cid:15)p and c3, c4, c5 > 0 be the associated constants. Clearly, the union
∪p∈SBδp(p) is an open cover of set S, so by second countability of Rd, there exists a countable
index set Λ such that S ⊂ ∪p∈ΛBδp(p). Therefore, to prove Theorem 3.6.1, it suﬃces to show
that

P (cid:0)Xk ∈ Bδp(p), ∀k ≥ k0

(cid:1) = 0
To this end, ﬁx p ∈ Λ and k0 ≥ Kp. Let F = Fp denote the local mapping in Condition 1
of Theorem 3.6.1. In addition, let η = ηp, denote the mapping associated to F , guaranteed
to exist by Theorem 3.9.6.1 Furthermore, recall the stopping time τk0 = τk0,δp(p), deﬁned as
τk0,δp(p) = inf{k ≥ k0 : Xk /∈ Bδp(p)}. Note that (3.9.11) holds if P (τk0 = ∞) = 0.

for all k0 ≥ Kp.

(3.9.11)

Our strategy is as follows. We ﬁrst prove that on the event that on the event {τk0 = ∞},
we have η(Yk) → 0 almost surely. Then we show that P ({τk0 = ∞} ∩ {η(Yk) → 0}) = 0.
This will imply that P ({τk0 = ∞}) = 0 and the proof will be complete. These two claims
are subjects of the following two subsections.

1Note that strictly speaking we should extend F to all Rd, for example, by a partition of unity [53, Lemma

2.26]. Since the argument that follows is local, we omit this discussion for simplicity.

73

Claim: On the event {τk0 = ∞}, we have η(Yk) → 0

To prove this claim, note that the following hold for almost all sample paths in the event
{τk0 = ∞}:

1. The sequence Yk is bounded.
2. Deﬁne βk = (cid:80)k−1

i=0 αi. Then for each T > 0, the limit holds:

(cid:32)

lim
n→∞

sup
k : 0≤βk−βn≤T

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

k−1
(cid:88)

i=n

αi · (ξi + Ek)

(cid:33)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

= 0.

(3.9.12)

Indeed, note that by Condition 3b of the Theorem, it suﬃces to show Mk = (cid:80)k
i=0 αiξi
converges almost surely, since then it is a Cauchy sequence. To prove that Mk con-
verges, note that (cid:80)
i < ∞ and lim sup E[(cid:107)ξi(cid:107)2 | Fk] < ∞, so Mk is a martingale.
Moreover,

i α2

sup
k≥0

E [(cid:107)Mk(cid:107)]2 ≤ sup
k≥0

E (cid:2)(cid:107)Mk(cid:107)2(cid:3) ≤ c2

4

(cid:88)

i≥0

α2

i < ∞.

(3.9.13)

Standard martingale theory then shows that Mk converges almost surely (Theorem
4.2.11 in [34]). Therefore, (3.9.12) holds almost surely.

These conditions match those of [2, Theorem 1.2]. Consequently, by this result it holds
that the set of limit points of Yk is almost surely invariant under the mapping Θt : B(cid:15)p/2(p) →
Rd, deﬁned as the time-t map of the ODE ˙γ(t) = F (γ(t)). Thus, for any x(cid:48) in limit set of
Yk, we have Θt(x(cid:48)) ∈ B(cid:15)p/2(p) for all t ≥ 0. Consequently, by Proposition 3.9.6, we have

η(cid:48)(Θt(x(cid:48)); F (Θt(x(cid:48)))) ≥ cη(Θt(x(cid:48)))

for all t ≥ 0.

(3.9.14)

Therefore, by integrating η(cid:48) integrating with respect to t, we have for all t ≥ 0, the bound

η(Θt(x(cid:48))) = η(Θ0(x(cid:48))) +

(cid:90) t

0

η(cid:48)(Θs(x(cid:48)); F (Θs(x(cid:48))))ds ≥ η(Θ0(x(cid:48))) +

(cid:90) t

0

cη(Θs(x(cid:48)))ds.

Thus, by Gronwall’s inequality [40] it holds that

η(Θt(x(cid:48))) ≥ ectη(Θ0(x(cid:48))) = ectη(x(cid:48))

for all t ≥ 0.

Now observe that since Θt(x(cid:48)) ∈ B(cid:15)p/2(p), the quantity η(Θt(x(cid:48))) is bounded for all t ≥ 0.
Consequently, we must have η(x(cid:48)) = 0. Thus, we have shown that for all limits points x(cid:48) of
Yk, we have η(x(cid:48)) = 0. Since η is continuous in B(cid:15)p/2(p), we must therefore have η(Yk) → 0.

Claim: We have P ({τk0 = ∞} ∩ {η(Yk) → 0}) = 0.

We begin by stating the following straightforward extension of [12, Theorem 4.1].

74

Lemma 3.9.1. Let {ζk}k be a nonnegative sequence of random variables adapted to a ﬁltra-
tion {Fk} satisfying the following recurrence almost surely on an F∞-measurable set Ω0:

ζk+1 ≥ ζk + αk(ek+1 + rk+1 + ˆrk+1)

for all k ≥ k0.

where {αk} is a square-summable, but not summable sequence. Assume that {ek}k, {rk},
and {ˆrk}k are Fk measurable and satisfy

E[ek+1 | Fk] = 0;

lim sup
k

E[e2

k+1 | Fk] < ∞

∞
(cid:88)

k=1

r2
k < +∞

lim inf
k

E[|ek+1| | Fk] > 0,

almost surely on Ω0. Assume that for n ≥ k0, we have

(cid:34)

E

1Ω0

∞
(cid:88)

k=n

(cid:35)

αk|ˆrk+1|

= O

(cid:32) ∞
(cid:88)

(cid:33)

α2
k

.

k=n

Then we have P (Ω0 ∩ {ζk → 0}) = 0.

Proof. Without loss of generality we may assume k0 = 0. Following [12, Theorem 4.1] (itself
based on [13, Page 401]) it suﬃces to work in the case where there exist ﬁxed constants µ
and C > 0 such that almost surely on the whole probability space, we have

E[ek+1 | Fk] = 0

and

lim sup
k

E[e2

k+1 | Fk] < C

lim inf
k

E[|ek+1| | Fk] > µ > 0

and

∞
(cid:88)

k=1

r2
k < C.

Now deﬁne the nonnegative residual sequence:

αkUk+1 = ζk+1 − ζk − αk(ek+1 + rk+1 + ˆrk+1)

Notice that for all k ≥ 0, we have

(cid:34)

ζk =

ζ0 +

k
(cid:88)

j=0

αj(ej+1 + rj+1 + ˆrj+1 + Uj+1)

on G := Ω0 ∩ {ζk → 0}.

(cid:35)

Therefore, on G, we have

−ζ0 =

(cid:34) ∞
(cid:88)

j=0

αj(ej+1 + rj+1 + ˆrj+1 + Uj+1)

.

(cid:35)

Then as argued the proof of [12, Theorem 4.1] it suﬃces by Theorem A of [13] (included as
Lemma 3.11.3 in the Appendix) to show that

(cid:34)

E

1G

∞
(cid:88)

k=n

(cid:35)



αk|Uk+1 + ˆrj+1|

= o



(cid:32) ∞
(cid:88)

(cid:33)1/2
 ,

α2
k

k=n

75

Clearly, it suﬃces to bound the series E
terms), since by assumption, we have

(cid:104)
1G

(cid:80)∞

j=K αjUj+1

(cid:105)

(which consists of nonnegative

(cid:34)

E

1G

∞
(cid:88)

k=n

(cid:35)

αk|ˆrj+1|

= O

(cid:32) ∞
(cid:88)

j=n

(cid:33)

α2
j

= o

(cid:32) ∞
(cid:88)

j=n

α2
j

(cid:33)1/2

.

To that end, note that for all k, n ≥ 0, we have

(cid:34)

ζn+k =

ζn +

n+k
(cid:88)

j=n

αj(ej+1 + rj+1 + ˆrj+1 + Uj+1)

(cid:35)

Hence on G, we may let k tend to inﬁnity, yielding:

−ζn =

∞
(cid:88)

j=n

αj(ej+1 + rj+1 + ˆrj+1 + Uj+1).

Thus, on the event G, we have

∞
(cid:88)

j=n

αjUj+1 = −ζn −

∞
(cid:88)

j=n

αj(ej+1 + rj+1 + ˆrj+1)

Therefore, we ﬁnd that

(cid:34)

E

1G

∞
(cid:88)

j=n

(cid:35)

(cid:34)

αjUj+1

≤ −ζn − E

1G

∞
(cid:88)

j=n

αj(ej+1 + rj+1 + ˆrj+1)

(cid:35)

≤

(cid:34)

E

1G

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∞
(cid:88)

j=n

αj(ej+1 + rj+1)

(cid:35)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ o

(cid:32) ∞
(cid:88)

j=n

α2
j

(cid:33)1/2

.

where the second inequality follows from nonnegativity of ζn and our assumptions on ˆrj+1.
Thus, to complete the bound of E[1G

j=K αjUj+1] we must show that

(cid:80)∞

(cid:34)

E

1G

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∞
(cid:88)

j=n

αj(ej+1 + rj+1)

(cid:35)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= o

(cid:32) ∞
(cid:88)

j=n

α2
j

(cid:33)1/2

.

The above bound follows by the exact same argument as [12, Theorem 4.1], which we
reproduce for completeness: First let Gn = E[1G | Fn], recall that G is F∞ measurable and
that Gn converges to 1G almost surely in Lp for every p ≥ 1, e.g., E[(Gn −1G)2] → 0. Turning

76

to the bound, we have

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:34)

E

1G

∞
(cid:88)

j=n

αj(ej+1 + rj+1)

(cid:35)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:34)

E

(1G − Gn)

∞
(cid:88)

j=n

αj(ej+1 + rj+1)

(cid:35)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ E[(1G − Gn)2]1/2


E





(cid:32) ∞
(cid:88)

j=n

(cid:124)

αj(ej+1 + rj+1)

(cid:123)(cid:122)
=:R1

(cid:34)

E

Gn

∞
(cid:88)

αj(ej+1 + rj+1)

(cid:35)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

j=n
(cid:33)2






1/2

+ E

(cid:34) ∞
(cid:88)

j=n

(cid:35)

αj|rj+1|

.

(cid:125)

(cid:124)

(cid:123)(cid:122)
=:R2

(cid:125)

The proof will be complete if R1 = O
bound R2:

(cid:16)(cid:80)∞

j=n α2
j

(cid:17)1/2

and R2 = o

(cid:16)(cid:80)∞

j=n α2
j

(cid:17)1/2

. Let us ﬁrst

R2 ≤

(cid:32) ∞
(cid:88)

j=n

α2
j

(cid:33)1/2

(cid:34) ∞
(cid:88)

E

(cid:35)1/2



r2
j+1

= o



j=n

(cid:33)1/2
 ,

α2
j

(cid:32) ∞
(cid:88)

j=n

where the last inequality follows from the bound (cid:80)∞

k=1 r2

k+1 < C. Now we bound R1:


E

R1 ≤

(cid:32) ∞
(cid:88)





αjej+1

(cid:33)2






1/2


E





+

(cid:32) ∞
(cid:88)

αjrj+1

(cid:33)2






1/2

j=n

j=n

(cid:32)

≤

E

(cid:34) ∞
(cid:88)

j=n

(cid:35)(cid:33)1/2

α2
j

E[e2

j+1 | Fk]

(cid:32) ∞
(cid:88)

+

α2
j

j=n

(cid:33)1/2 (cid:32)

E

(cid:34) ∞
(cid:88)

j=n

r2
j+1

(cid:35)(cid:33)1/2

= O

(cid:32) ∞
(cid:88)

j=n

α2
j

(cid:33)1/2

.

Therefore, the proof is complete.

kγ ≤ αk ≤ c2

Now we apply the above Lemma. To that end, we state a few simpliﬁcations and facts
to be used below. First, throughout the proof, we let C be a positive constant that changes
from line to line. Second, we simplify notation and let τ denote τk0,δ. Third, we recall the
bound c1
kγ . Fourth, the function η is weakly convex and Lipschitz continuous
on B(cid:15)p(p). Fifth, the Jacobian ∇Φ is Lip∇Φ-Lipschitz in B(cid:15)p(p). Sixth, we note that for
suﬃciently large k, we have the following on {τ = ∞}: Yk + αkF (Yk) ∈ B(cid:15)p(p). We may
assume without loss of generality that these assertions hold for all k ≥ 1. Finally, we note
that by shrinking (cid:15)p, if necessary, we can assume that on the event {τ = ∞}, we have

smin(A) lim inf

k

inf
w∈W ∩Sd−1

E[| (cid:104)w, ξk(cid:105) | | Fk] − (cid:15)p lim sup

E[(cid:107)ξk(cid:107) | Fk](cid:107)A(cid:107)Lip∇Φ

k
≥ c4smin(A) − (cid:15)pc1/4

3 (cid:107)A(cid:107)Lip∇Φ > 0

(3.9.15)

where c4 and c3 are independent of (cid:15)p and δp, A is deﬁned in Proposition 3.9.6, and smin(A)
denotes the minimal nonzero singular value of A.

Now let s : B(cid:15)p(p) → Rd be a selection of ∂η deﬁned as follows: for all y ∈ B(cid:15)p(p),

77

• If η(y) (cid:54)= 0, then η is diﬀerentiable at Y , so set s(y) = ∇η(y).

• If η(y) = 0, then η is nondiﬀerentiable, so we choose subgradient

s(Y ) = ∇Φ(y)(cid:62)A(cid:62)u ∈ ∂η(y)

where u ∈ Sd−1 satisﬁes (cid:107)A(cid:62)u(cid:107) = (cid:107)A(cid:107) > 0.

Next, consider the event Ω0 = {τ = ∞}. Then by the boundedness of s(Yk + αkF (Yk)) and
the weak convexity of η on B(cid:15)p(p), there exists C > 0 such that

η(Yk+1) ≥ η(Yk + αkF (Yk)) + (cid:104)s(Yk + αkF (Yk)), αkEk + αkξk(cid:105) − C(cid:107)αkEk + αkξk(cid:107)2

≥ η(Yk + αkF (Yk)) + (cid:104)s(Yk + αkF (Yk)), αkξk(cid:105) − C(cid:107)αkEk + αkξk(cid:107)2 − Cαk(cid:107)Ek(cid:107)
≥ (1 + cαk)η(Yk) + (cid:104)s(Yk + αkF (Yk)), αkξk(cid:105) − C(cid:107)αkEk + αkξk(cid:107)2 − Cαk(cid:107)Ek(cid:107) − Cα2
k.
(3.9.16)

Now deﬁne four sequences:

ζk := η(Yk);

ek+1 := (cid:104)s(Yk + αkF (Yk)), ξk(cid:105) ;

rk+1 := −Cαk

(cid:0)1 + (cid:107)Ek + ξk(cid:107)2(cid:1) ;

ˆrk+1 := −C(cid:107)Ek(cid:107)

and observe that on Ω0, we have

ζk+1 ≥ ζk + αk(ek+1 + rk+1 + ˆrk+1).

Now we must verify the assumptions of the Lemma. We begin with ˆrk+1. To that end,

observe that

(cid:34)

E

1Ω0

∞
(cid:88)

k=n

(cid:35)

αk ˆrk+1

= O

(cid:32) ∞
(cid:88)

(cid:33)

α2
k

,

k=n

by our assumption on (cid:107)Ek(cid:107). Next we prove square summability of rk+1 on Ω0: Indeed,
observe

k+1 ≤ Cα2
r2

k((cid:107)ξk(cid:107)4 + (cid:107)Ek(cid:107)4 + 1).

Moreover both lim supk
on Ω0. Therefore, by conditional Borel-Cantelli Lemma 3.11.2, we have

Ek[(cid:107)ξk(cid:107)4 | Fk] < ∞ and lim supk

Ek[(cid:107)Ek(cid:107)4 | Fk] < ∞ are bounded

∞
(cid:88)

k=1

r2
k+1 < +∞.

almost surely on Ω0.

Finally we prove that ek has the desired properties. First note that we have

E[ek+1 | Fk] = 0

and

lim sup
k

E[e2

k+1 | Fk] < ∞.

E[(cid:107)ξk(cid:107)4 | Fk] < ∞ almost surely and and Yk +
on Ω0. Indeed, this follows since lim supk
αkF (Yk) ∈ B(cid:15)p(p) on Ω0. Next, since η is globally Lipschitz on B(cid:15)p(p), we have that s(Yk +
αkF (Yk)) is uniformly bounded. Thus,

lim sup
k

E[e2

k+1 | Fk] ≤ lim sup

k

E[(cid:107)s(Yk + αkF (Yk))(cid:107)2(cid:107)ξk(cid:107)2 | Fk] < ∞,

78

on Ω0, as desired.

Now we prove that lim inf E[|ek+1| | Fk] is positive on Ω0. To that end, recall that the
mapping Φ satisﬁes ∇Φ(p) = Id. Turning to the proof, there are two cases to consider. First
suppose that η(Yk + αkF (Yk)) (cid:54)= 0. Then η is diﬀerentiable at Yk + αkF (Yk). Now deﬁne
uk := A(Φ(Yk+αkF (Yk))−p)

(cid:107)A(Φ(Yk+αkF (Yk))−p)(cid:107) and note that

s(Yk + αkF (Yk)) = ∇η(Yk + αkF (Yk)) = ∇Φ(Yk + αkF (Yk))(cid:62)A(cid:62)uk

= A(cid:62)uk + (∇Φ(Yk + αkF (Yk)) − ∇Φ(p))(cid:62)A(cid:62)uk
∈ A(cid:62)uk + (cid:15)p(cid:107)A(cid:107)Lip∇ΦB1(0),

where the inclusion follows since Yk + αkF (Yk) ∈ B(cid:15)p(p). Let smin(A) denote the minimal
nonzero singular value of A and notice that since uk ∈ Sd−1 ∩ range(A), we have that
wk := AT uk satisﬁes and

wk ∈ W

and

(cid:107)wk(cid:107) ≥ smin(A) > 0.

Therefore, it follows that on the event Ω0, we have

E[|ek+1| | Fk] = E[| (cid:104)s(Yk + αkF (Yk)), ξk(cid:105) | | Fk]

≥ E[| (cid:104)wk, ξk(cid:105) | | Fk] − (cid:15)pE[(cid:107)ξk(cid:107) | Fk](cid:107)A(cid:107)Lip∇Φ
≥ smin(A)

E[| (cid:104)w, ξk(cid:105) | | Fk] − (cid:15)pE[(cid:107)ξk(cid:107) | Fk](cid:107)A(cid:107)Lip∇Φ

inf
w∈W ∩Sd−1

We now consider the case η(Yk + αkF (Yk)) = 0. In this case, there exists uk ∈ Sd−1 such
that (cid:107)A(cid:62)uk(cid:107) = (cid:107)A(cid:107) and

s(Yk + αkF (Yk)) = ∇Φ(Yk + αkF (Yk))(cid:62)A(cid:62)uk ∈ A(cid:62)uk + (cid:15)p(cid:107)A(cid:107)Lip∇ΦB1(0),

Recall range(A(cid:62)) = W . Thus, we have that the vector wk := A(cid:62)uk is in W and (cid:107)wk(cid:107) =
(cid:107)A(cid:107) > 0. Thus, for all v ∈ Rd, we have

| (cid:104)s(Yk + αkF (Yk)), v(cid:105) | = (cid:10)∇Φ(Yk + αkF (Yk))(cid:62)A(cid:62)uk, v(cid:11) ≥ (cid:104)wk, v(cid:105) − (cid:15)pLip∇Φ (cid:107)A(cid:107) (cid:107)v(cid:107) .

Taking v = ξk, we obtain

E[| (cid:104)s(Yk + αkF (Yk)), ξk(cid:105) | | Fk] ≥ (cid:107)A(cid:107)

inf
w∈W ∩Sd−1

E[| (cid:104)w, ξk(cid:105) | | Fk] − (cid:15)pE[(cid:107)ξk(cid:107) | Fk](cid:107)A(cid:107)Lip∇Φ

Thus, putting both cases together, we ﬁnd that on the event Ω0, we have

lim inf
k

E[|ek+1| | Fk] ≥ smin(A) lim inf

k

inf
w∈W ∩Sd−1

E[| (cid:104)w, ξk(cid:105) | | Fk]−(cid:15)p lim sup

k

E[(cid:107)ξk(cid:107) | Fk](cid:107)A(cid:107)Lip∇Φ > 0,

where the last inequality follows from (3.9.15).

Therefore, we have veriﬁed that all the conditions of Lemma 3.9.1. It follows that

as desired.

P ({τ = ∞} ∩ {η(Yk) → 0}) = 0,

79

3.9.5 Proof of Theorem 3.6.2: nonconvergence to saddle points

In this section, prove Theorem 3.6.2 by verifying that the iterates {xk}k∈N satisfy the con-
ditions of Theorem 3.6.1. To that end, ﬁx a point p ∈ S with associated manifold M
and neighborhood U. Let (cid:15)p be small enough that B(cid:15)p(¯x) ⊆ U and deﬁne the C 2 mapping
Fp : B(cid:15)p(p) → Rd by:

Fp(y) = −∇fM(y),
where fM := f ◦ PM. Note that the mapping F is indeed C 2, since M is a C 4 manifold,
and hence, fM is C 3. Moreover, since ∇F (p) = −∇2
Mf (p), the mapping Fp has at least
one eigenvector with positive eigenvalue.
In addition, the subspace Wp spanned by such
eigenvectors is contained in TM(p).

Turning to the proof, deﬁne Xk = xk for all k ≥ 1. We now construct the sequences
Yk, ξk, and Ek and show they satisfy the assumptions of the theorem. Beginning with Yk,
recall that by Proposition 3.3.2, for all k ≥ 1 and all suﬃciently small δ > 0, the sequence

Yk :=

(cid:40)

PM(Xk)
p

if xk ∈ B2δ(¯x)
otherwise.

,

(3.9.17)

satisﬁes Yk ∈ B4δ(¯x) ∩ M and the recursion

Yk+1 = Yk − αk∇fM(yk) − αkξk + αkEk

for all k ≥ 1.

where ξk := PTM(Yk)(νk) and Ek is an error sequence. Moving to Ek, let us show that the
error sequence satisﬁes the assumptions of theorem. To that end, Proposition 3.3.2 shows
that for δ suﬃciently small, there exists C > 0 such that for all n ≥ k0, we have

(cid:34)
1τk0,δ=∞

E

∞
(cid:88)

k=n

(cid:35)

αk(cid:107)Ek(cid:107)

≤ C

∞
(cid:88)

k=n

α2
k.

Moreover, by the inequality from Proposition 3.3.2, the sequence (cid:107)Ek(cid:107)1τk0,δ>k is bounded
above by a bounded sequence that almost surely converges to zero:

(cid:107)Ek(cid:107)1τk0,δ>k ≤ C(1 + (cid:107)νk(cid:107))2(dist(xk, M) + αk)1τk0,δ>k ≤ C(1 + r)2(δ + αk),

Thus, on the event {τk0,δ = ∞}, we have

lim sup
k

1Ω0

E[(cid:107)Ek(cid:107)4 | Fk] ≤ lim sup

k

E[(cid:107)Ek(cid:107)41τk0,δ>k | Fk] ≤ C(1 + r)2(δ + αk).

Therefore, Yk and Ek satisfy the conditions 1 and 3 of Theorem 3.6.1 for all suﬃciently small
δp satisfying δp ≤ (cid:15)p/8.

To conclude the proof, we now show that Condition 2 of Theorem 3.6.1 is satisﬁed. To

that end, clearly (cid:107)ξk(cid:107) = (cid:107)PTk(νk)(cid:107) ≤ r =: c3 for all k ≥ k0. In addition, we have that

E [ξk | Fk] = PTk(E [νk | Xk0, . . . , Xk]) = 0.

Indeed, this follows from two facts: ﬁrst Yk is a measurable function of Xk; and second the
noise sequence νk is mean zero and independent of Xk0, . . . , Xk. Finally, we must show that
ξk has positive correlation with the unstable subspace Wp.

80

To prove correlation with the unstable subspace, recall that there exists C (cid:48) > 0 such that
the mapping x (cid:55)→ PTM(x) is C (cid:48)-Lipschitz mapping on M ∩ B(cid:15)p(p). In addition, we have that
Wp ⊆ TM(p). Therefore, since Yk ∈ M ∩ B(cid:15)p(p) for all k ≥ k0, we have the following bound
for all w ∈ W ∩ Sd−1:

E[| (cid:104)ξk, w(cid:105) | | Fk] = E[| (cid:10)νk, PTM(Yk)w(cid:11) | | Fk]

≥ E[| (cid:104)νk, w(cid:105) | | Fk] − r(cid:107)(PTM(Yk) − PTM(p))w(cid:107)
≥ rcd − rC (cid:48)(cid:107)Yk − p(cid:107),

where cd is a constant dependent only on d since νk ∼ Unif(Br(0)). By slightly shrinking (cid:15)p
if needed, we can ensure that inf x∈B(cid:15)p (p){rcd − rC (cid:48)(cid:107)x − p(cid:107)} > (1/2)rcd =: c4, as desired.

81

Bibliography

[1] Hilal Asi and John C Duchi. Stochastic (approximate) proximal point methods: Con-
vergence, optimality, and adaptivity. SIAM Journal on Optimization, 29(3):2257–2290,
2019.

[2] Michel Benaim. A dynamical system approach to stochastic approximations. SIAM

Journal on Control and Optimization, 34(2):437–472, 1996.

[3] Michel Bena¨ım. Dynamics of stochastic approximation algorithms.

In Seminaire de

probabilites XXXIII, pages 1–68. Springer, 1999.

[4] S. Bhojanapalli, B. Neyshabur, and N. Srebro. Global optimality of local search for low
In Advances in Neural Information Processing Systems, pages

rank matrix recovery.
3873–3881, 2016.

[5] Pascal Bianchi, Walid Hachem, and Sholom Schechtman. Stochastic subgradient descent

escapes active strict saddles. arXiv preprint arXiv:2108.02072, 2021.

[6] J´erˆome Bolte and Edouard Pauwels. Conservative set valued ﬁelds, automatic diﬀer-
entiation, stochastic gradient methods and deep learning. Mathematical Programming,
188(1):19–51, 2021.

[7] J´erˆome Bolte, Shoham Sabach, and Marc Teboulle. Proximal alternating linearized
minimization for nonconvex and nonsmooth problems. Mathematical Programming,
146(1):459–494, 2014.

[8] J´erˆome Bolte, Aris Daniilidis, Adrian Lewis, and Masahiro Shiota. Clarke subgradients

of stratiﬁable functions. SIAM Journal on Optimization, 18(2):556–572, 2007.

[9] J Fr´ed´eric Bonnans and Alexander Shapiro. Perturbation analysis of optimization prob-

lems. Springer Science & Business Media, 2013.

[10] Jonathan Borwein and Adrian S Lewis. Convex analysis and nonlinear optimization:

theory and examples. Springer Science & Business Media, 2010.

[11] Nicolas Boumal. An introduction to optimization on smooth manifolds. Available online,

Aug, 2020.

[12] Odile Brandiere. Some pathological traps for stochastic approximation. SIAM journal

on control and optimization, 36(4):1293–1314, 1998.

82

[13] Odile Brandi`ere and Marie Duﬂo. Les algorithmes stochastiques contournent-ils les

pi`eges ? Annales de l’I.H.P. Probabilit´es et statistiques, 32(3):395–427, 1996.

[14] Louis H. Y. Chen. A Short Note on the Conditional Borel-Cantelli Lemma. The Annals

of Probability, 6(4):699 – 700, 1978.

[15] Francis H Clarke, Yuri S Ledyaev, Ronald J Stern, and Peter R Wolenski. Nonsmooth
analysis and control theory, volume 178. Springer Science & Business Media, 2008.

[16] Francis H Clarke, RJ Stern, and PR Wolenski. Proximal smoothness and the lower-c2

property. J. Convex Anal, 2(1-2):117–144, 1995.

[17] C. Criscitiello and N. Boumal. Eﬃciently escaping saddle points on manifolds. arXiv

preprint arXiv:1906.04321, 2019.

[18] Aris Daniilidis, Dmitriy Drusvyatskiy, and Adrian S Lewis. Orthogonal invariance and
identiﬁability. SIAM Journal on Matrix Analysis and Applications, 35(2):580–598, 2014.

[19] Aris Daniilidis, Adrian Lewis, J´erˆome Malick, and Hristo Sendov. Prox-regularity of
spectral functions and spectral sets. Journal of Convex Analysis, 15(3):547–560, 2008.

[20] Chandler Davis. All convex invariant functions of hermitian matrices. Archiv der Math-

ematik, 8(4):276–278, 1957.

[21] Damek Davis, Mateo D´ıaz, and Dmitriy Drusvyatskiy. Escaping strict saddle points
of the moreau envelope in nonsmooth optimization. arXiv preprint arXiv:2106.09815,
2021.

[22] Damek Davis and Dmitriy Drusvyatskiy. Active strict saddles in nonsmooth optimiza-

tion. arXiv preprint arXiv:1912.07146, 2019.

[23] Damek Davis and Dmitriy Drusvyatskiy. Stochastic model-based minimization of

weakly convex functions. SIAM Journal on Optimization, 29(1):207–239, 2019.

[24] Damek Davis and Dmitriy Drusvyatskiy. Proximal methods avoid active strict saddles
of weakly convex functions. Foundations of Computational Mathematics, pages 1–46,
2021.

[25] Damek Davis, Dmitriy Drusvyatskiy, and Vasileios Charisopoulos. Stochastic algo-
rithms with geometric step decay converge linearly on sharp functions. arXiv preprint
arXiv:1907.09547, 2019.

[26] Damek Davis, Dmitriy Drusvyatskiy, Sham Kakade, and Jason D Lee. Stochastic sub-
gradient method converges on tame functions. Foundations of computational mathe-
matics, 20(1):119–154, 2020.

[27] A Dembo. Lecture notes on probability theory: Stanford statistics 310. Accessed Octo-

ber, 1:2016, 2016.

83

[28] D. Drusvyatskiy, A. D. Ioﬀe, and A. S. Lewis. Generic minimizing behavior in semial-

gebraic optimization. SIAM J. Optim., 26(1):513–534, 2016.

[29] D. Drusvyatskiy and C. Paquette. Variational analysis of spectral functions simpliﬁed.

J. Convex Anal., 25(1):119–134, 2018.

[30] Dmitriy Drusvyatskiy.
arXiv:1712.06038, 2017.

The proximal point method revisited.

arXiv preprint

[31] Dmitriy Drusvyatskiy, Alexander D Ioﬀe, and Adrian S Lewis. Curves of descent. SIAM

Journal on Control and Optimization, 53(1):114–138, 2015.

[32] Dmitriy Drusvyatskiy and Adrian S Lewis. Optimality, identiﬁability, and sensitivity.

Mathematical Programming, 147(1):467–498, 2014.

[33] John C Duchi and Feng Ruan. Asymptotic optimality in stochastic optimization. The

Annals of Statistics, 49(1):21–48, 2021.

[34] Rick Durrett. Probability: theory and examples, volume 49. Cambridge university press,

2019.

[35] Herbert Federer. Curvature measures. Transactions of the American Mathematical

Society, 93(3):418–491, 1959.

[36] R. Ge, F. Huang, C. Jin, and Y. Yuan. Escaping from saddle points—online stochastic
gradient for tensor decomposition. In Conference on Learning Theory, pages 797–842,
2015.

[37] R. Ge, C. Jin, and Y. Zheng. No spurious local minima in nonconvex low rank problems:
A uniﬁed geometric analysis. In Proceedings of the 34th International Conference on
Machine Learning-Volume 70, pages 1233–1242. JMLR. org, 2017.

[38] R. Ge, J.D. Lee, and T. Ma. Matrix completion has no spurious local minimum. In

Advances in Neural Information Processing Systems, pages 2973–2981, 2016.

[39] H. Gfrerer and V. Outrata. On a semismooth* newton method for solving generalized

equations. SIAM Journal on Optimization, 31(1):489–517, 2021.

[40] Thomas Hakon Gronwall. Note on the derivatives with respect to a parameter of the
solutions of a system of diﬀerential equations. Annals of Mathematics, pages 292–296,
1919.

[41] Nadav Hallak and Marc Teboulle. Finding second-order stationary points in constrained
minimization: A feasible direction approach. Journal of Optimization Theory and Ap-
plications, 186(2):480–503, 2020.

[42] Warren L Hare and Adrian S Lewis. Identifying active constraints via partial smoothness

and prox-regularity. Journal of Convex Analysis, 11(2):251–266, 2004.

84

[43] Morris W Hirsch, Charles Chapman Pugh, and Michael Shub.

Invariant manifolds,

volume 583. Springer, 2006.

[44] Minhui Huang. Escaping saddle points for nonsmooth weakly convex functions via

perturbed proximal algorithms. arXiv preprint arXiv:2102.02837, 2021.

[45] T-C Kuo. The ratio test for analytic whitney stratiﬁcations. In Proceedings of Liverpool

Singularities—Symposium I, pages 141–149. Springer, 1971.

[46] Tzee-Char Kuo. Characterizations of v-suﬃciency of jets. Topology, 11(1):115–131,

1972.

[47] Krzysztof Kurdyka. On gradients of functions deﬁnable in o-minimal structures.

In

Annales de l’institut Fourier, volume 48, pages 769–783, 1998.

[48] Ta Lˆe Loi. Verdier and strict thom stratiﬁcations in o-minimal structures.

Illinois

Journal of Mathematics, 42(2):347–356, 1998.

[49] J.D. Lee, I. Panageas, G. Piliouras, M. Simchowitz, M.I. Jordan, and B. Recht. First-
order methods almost always avoid strict saddle points. Math. Program., 176(1-2):311–
337, July 2019.

[50] J.D. Lee, M. Simchowitz, M.I. Jordan, and B. Recht. Gradient descent only converges

to minimizers. In Conference on learning theory, pages 1246–1257, 2016a.

[51] John M Lee. Smooth manifolds.

In Introduction to Smooth Manifolds, pages 1–31.

Springer, 2013.

[52] Sangkyun Lee and Stephen J. Wright. Manifold identiﬁcation in dual averaging for reg-
ularized stochastic online learning. Journal of Machine Learning Research, 13(55):1705–
1744, 2012.

[53] Sangkyun Lee, Stephen J Wright, and L´eon Bottou. Manifold identiﬁcation in dual aver-
aging for regularized stochastic online learning. Journal of Machine Learning Research,
13(6), 2012.

[54] Claude Lemar´echal, Fran¸cois Oustry, and Claudia Sagastiz´abal. The U-lagrangian of a
convex function. Transactions of the American mathematical Society, 352(2):711–729,
2000.

[55] Adrian S Lewis. Convex analysis on the hermitian matrices. SIAM Journal on Opti-

mization, 6(1):164–177, 1996.

[56] Adrian S Lewis. Nonsmooth analysis of eigenvalues. Mathematical Programming,

84(1):1–24, 1999.

[57] Adrian S Lewis. Active sets, nonsmoothness, and sensitivity. SIAM Journal on Opti-

mization, 13(3):702–725, 2002.

85

[58] Adrian S Lewis and Jingwei Liang. Partial smoothness and constant rank. arXiv

preprint arXiv:1807.03134, 2018.

[59] Adrian S Lewis and Hristo S Sendov. Nonsmooth analysis of singular values. part i:

Theory. Set-Valued Analysis, 13(3):213–241, 2005.

[60] Adrian S Lewis and Stephen J Wright. A proximal method for composite minimization.

Mathematical Programming, 158(1):501–546, 2016.

[61] Adrian Stephen Lewis. Derivatives of spectral functions. Mathematics of Operations

Research, 21(3):576–588, 1996.

[62] Jingwei Liang, Jalal Fadili, Gabriel Peyr´e, and Russell Luke. Activity identiﬁcation
and local linear convergence of douglas–rachford/admm under partial smoothness. In
International Conference on Scale Space and Variational Methods in Computer Vision,
pages 642–653. Springer, 2015.

[63] Stanislaw Lojasiewicz. Une propri´et´e topologique des sous-ensembles analytiques r´eels.

Les ´equations aux d´eriv´ees partielles, 117:87–89, 1963.

[64] Szymon Majewski, B(cid:32)la˙zej Miasojedow, and Eric Moulines.

Analysis of nons-
mooth stochastic approximation: the diﬀerential inclusion approach. arXiv preprint
arXiv:1805.01916, 2018.

[65] John Mather. Notes on topological stability. Citeseer, 1970.

[66] Robert Miﬄin. Semismooth and semiconvex functions in constrained optimization.

SIAM Journal on Control and Optimization, 15(6):959–972, 1977.

[67] Robert Miﬄin and Claudia Sagastiz´abal. A V U -algorithm for convex minimization.

Mathematical programming, 104(2):583–608, 2005.

[68] Scott A Miller and J´erˆome Malick. Newton methods for nonsmooth convex minimiza-
tion: connections among-lagrangian, riemannian newton and sqp methods. Mathemat-
ical programming, 104(2):609–633, 2005.

[69] A. Mokhtari, A. Ozdaglar, and A. Jadbabaie. Escaping saddle points in constrained
optimization. In Advances in Neural Information Processing Systems, pages 3629–3639,
2018.

[70] Boris S Mordukhovich. Variational analysis and generalized diﬀerentiation I: Basic

theory, volume 330. Springer Science & Business Media, 2006.

[71] Arkadij Semenoviˇc Nemirovskij and David Borisovich Yudin. Problem complexity and

method eﬃciency in optimization. 1983.

[72] Yurii Nesterov. Primal-dual subgradient methods for convex problems. Mathematical

Programming, 120(1):221–259, 2009.

86

[73] Maher Nouiehed, Jason D Lee, and Meisam Razaviyayn. Convergence to second-order
stationarity for constrained non-convex optimization. arXiv preprint arXiv:1810.02024,
2018.

[74] Robin Pemantle et al. Nonconvergence to unstable points in urn models and stochastic

approximations. The Annals of Probability, 18(2):698–712, 1990.

[75] Jean-Paul Penot. Calculus without derivatives, volume 266. Springer Science & Business

Media, 2012.

[76] Ren´e Poliquin, R Rockafellar, and Lionel Thibault. Local diﬀerentiability of distance
functions. Transactions of the American mathematical Society, 352(11):5231–5249, 2000.

[77] Ren´e A Poliquin and Ralph Tyrell Rockafellar. A calculus of prox-regularity. J. Convex

Anal, 17(1):203–210, 2010.

[78] Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by

averaging. SIAM journal on control and optimization, 30(4):838–855, 1992.

[79] Herbert Robbins and David Siegmund. A convergence theorem for non negative almost
In Optimizing methods in statistics, pages

supermartingales and some applications.
233–257. Elsevier, 1971.

[80] R Tyrrell Rockafellar and Roger J-B Wets. Variational analysis, volume 317. Springer

Science & Business Media, 2009.

[81] Alexander Shapiro. Asymptotic Properties of Statistical Estimators in Stochastic Pro-

gramming. The Annals of Statistics, 17(2):841 – 858, 1989.

[82] Alexander Shapiro. On a class of nonsmooth composite functions. Mathematics of

Operations Research, 28(4):677–692, 2003.

[83] Michael Shub. Global stability of dynamical systems. Springer Science & Business Media,

2013.

[84] Stephen Smale. Diﬀerentiable dynamical systems. Bulletin of the American mathemat-

ical Society, 73(6):747–817, 1967.

[85] J. Sun, Q. Qu, and J. Wright. When are nonconvex problems not scary? arXiv preprint

arXiv:1510.06096, 2015.

[86] J. Sun, Q. Qu, and J. Wright. A geometric analysis of phase retrieval. Foundations of

Computational Mathematics, 18(5):1131–1198, 2018.

[87] Y. Sun, N. Flammarion, and M. Fazel. Escaping from saddle points on riemannian

manifolds. arXiv preprint arXiv:1906.07355, 2019.

[88] David Trotman. Stratiﬁcation theory. In Handbook of Geometry and Topology of Sin-

gularities I, pages 243–273. Springer, 2020.

87

[89] Lou van den Dries and Chris Miller. Geometric categories and o-minimal structures.

Duke Math. J., 84(2):497–540, 1996.

[90] Jean-Louis Verdier. Stratiﬁcations de whitney et th´eoreme de bertini-sard. Inventiones

mathematicae, 36(1):295–312, 1976.

[91] John Wells. Invariant manifolds on non-linear operators. Paciﬁc Journal of Mathemat-

ics, 62(1):285–293, 1976.

[92] Hassler Whitney. Elementary structure of real algebraic varieties. In Hassler Whitney

Collected Papers, pages 456–467. Springer, 1992.

[93] Hassler Whitney. Local properties of analytic varieties. In Hassler Whitney Collected

Papers, pages 497–536. Springer, 1992.

[94] Hassler Whitney. Tangents to an analytic variety. In Hassler Whitney Collected Papers,

pages 537–590. Springer, 1992.

[95] Stephen J Wright. Identiﬁable surfaces in constrained optimization. SIAM Journal on

Control and Optimization, 31(4):1063–1079, 1993.

3.10 Appendix to Part 1

3.10.1 Proof of Proposition 2.3.3

Since X is a C 3 manifold, the projection PY is C 2-smooth. Therefore, there exist constants
(cid:15), L > 0 satisfying

(cid:107)PY (y + h) − PY (y) − ∇PY (y)h(cid:107) ≤ L(cid:107)h(cid:107)2
(3.10.1)
for all y ∈ B(cid:15)(¯x) and h ∈ (cid:15)B. Fix now two points x ∈ X and y ∈ Y and a unit vector
v ∈ NX(x). Clearly, we may suppose v /∈ NY (y), since otherwise the claim is trivially true.
PTY (y)(v)
(cid:107)PTY (y)(v)(cid:107). Noting the equality ∇PY (y) = PTY (y) and
Deﬁne the normalized vector w := −
appealing to (3.10.1), we deduce the estimate

(cid:107)PY (y − αw) − (y − αw)(cid:107) ≤ L(cid:107)αw(cid:107)2 = Lα2,

for all y ∈ B(cid:15)(¯x) and α ∈ (0, (cid:15)). Shrinking (cid:15) > 0, prox-regularity yields the estimate

(cid:104)v, PY (y − αw) − x(cid:105) ≤

ρ
2

(cid:107)x − PY (y − αw)(cid:107)2,

for some constant ρ > 0. Therefore, we conclude

α(cid:107)PTY (y)v(cid:107) = −α (cid:104)v, w(cid:105) = (cid:104)v, x − y(cid:105) + (cid:104)v, PY (y − αw) − x(cid:105) + (cid:104)v, (y − αw) − PY (y − αw)(cid:105)

≤ (cid:107)x − y(cid:107) +

Note that the middle term is small:

ρ
2

(cid:107)x − PY (x − αw)(cid:107)2 + Lα2.

(cid:107)PY (y −αw)−x(cid:107)2 ≤ 2(cid:107)PY (y −αw)−(y −αw)(cid:107)2 +2(cid:107)y −αw −x(cid:107)2 ≤ 2L2α4 +4(cid:107)y −x(cid:107)2 +4α2.

88

Thus, we have

α(cid:107)PTY (y)v(cid:107) ≤ (cid:107)x − y(cid:107) + ρL2α4 + 2ρ(cid:107)x − y(cid:107)2 + 2ρα2 + Lα2.

Dividing both sides by α and setting α = (cid:112)(cid:107)x − y(cid:107) completes the proof of (2.3.1).

3.11 Appendix to Part 2

3.11.1 Proof of Proposition 3.2.3: the projected gradient method

Let (cid:15) > 0 be a neighborhood small enough that the following hold for all x ∈ B(cid:15)(¯x) ∩ X :
ﬁrst (3.2.6) holds; second we require that for some C > 0, we have

(cid:107)PTM(PM(x))(sg(x) − ∇Mf (PM(x))(cid:107) ≤ Cdist(x, M);
(cid:107)PTM(PM(x))(w)(cid:107) ≤ Cdist(x, M),

for all w ∈ NX (x) of unit norm, a consequence of strong (a);2 third we require that

(cid:104)w, x − PM(x)(cid:105) ≥ −

µ
2

dist(x, M)

for all w ∈ NX (x) of unit norm, a consequence of prox-regularity or (b). Finally, we assume
that PM is C 3, in particular, C Lipschitz with C-Lipschitz Jacobian.

Verifying Assumption (A1)

Turning to the proof, throughout we let C be a constant that varies from line to line. We
also ﬁx x ∈ B(cid:15)/2(¯x) ∩ X , α > 0 and ν ∈ Rd. Deﬁne

w = Gα(x, ν) − sg(x) − ν

and

x+ = sX (x − α(sg(x) + ν)).

We begin with the following bound, which veriﬁes Assumption (A1):

Claim 9. We have w ∈ NX (x+) and there exists C > 0 such that the following bounds hold:

(cid:107)w(cid:107) ≤ C + (cid:107)ν(cid:107);

(cid:107)Gα(x, ν)(cid:107) ≤ 2(C + (cid:107)ν(cid:107));

(cid:107)x+ − x(cid:107) ≤ 2(C + (cid:107)ν(cid:107))α

Proof. Beginning with the inclusion, ﬁrst-order optimality conditions imply that:

w =

x − α(sg(x) + ν) − x+
α

∈ NX (x+),

as desired. Next we bound (cid:107)w(cid:107): there exists C > 0 such that

(cid:107)w(cid:107) = (cid:107)x − α(sg(x) + ν) − x+(cid:107)/α ≤ (cid:107)x − α(sg(x) + ν) − x(cid:107)/α ≤ C + (cid:107)ν(cid:107),

2The strong (a) regularity property for g is a consequence of Equation (2.1.3), which is stated with respect
to the limiting subdiﬀerential. However, for locally Lipschitz functions, the relationship extends to Clarke
subdiﬀerentials through convexiﬁcation: ∂cg(x) = Conv ∂g(x).

89

where the ﬁrst inequality follows from the deﬁnition of the projection and the second in-
equality follows from our local Lipschitz assumptions on g. Next, we bound (cid:107)x+ − x(cid:107):

(cid:107)x+ − x(cid:107) ≤ (cid:107)x+ − (x − α(sg(x) + ν))(cid:107) + α(cid:107)sg(x) + ν(cid:107) = α(cid:107)w(cid:107) + α(cid:107)sg(x) + ν(cid:107) ≤ 2α(C + (cid:107)ν(cid:107)),

as desired. Finally, we bound (cid:107)Gα(x, ν)(cid:107):

(cid:107)Gα(x, ν)(cid:107) = (cid:107)x+ − x(cid:107)/α ≤ 2(C + (cid:107)ν(cid:107)),

as desired.

In the remainder of the proof, we will make use of the following claim:

Claim 10. Suppose that y ∈ B(cid:15)/2(¯x) and z /∈ B(cid:15)(¯x). Then

dist(y, M) ≤

1
2

(cid:15) ≤ (cid:107)z − y(cid:107)

Proof. We have

(cid:107)z − y(cid:107) ≥ (cid:107)z − ¯x(cid:107) − (cid:107)y − ¯x(cid:107) ≥

1
2

(cid:15) ≥ (cid:107)y − ¯x(cid:107) ≥ dist(y, M)

where the ﬁnal inequality follows from the inclusion ¯x ∈ M.

Verifying Assumption (A3)

Now we verify Assumption (A3). Observe the following bound:

(cid:104)Gα(x, ν) − ν, x − PM(x)(cid:105) = (cid:104)w + sg(x), x − PM(x)(cid:105)

≥ (cid:104)w, x − PM(x)(cid:105) + µdist(x, M)
= µdist(x, M)

+ (cid:104)w, x+ − PM(x+)(cid:105)
(cid:123)(cid:122)
(cid:125)
=:R1

(cid:124)

− (cid:107)w(cid:107)(cid:107)(x − PM(x)) − (x+ − PM(x+))(cid:107)
(cid:123)(cid:122)
(cid:125)
=:R2

(cid:124)

.

(3.11.1)

In what follows we will bound R1 and R2.
Claim 11. There exists C > 0 such that

R1 ≥ −

µ
2

dist(x, M) − µ(C + (cid:107)ν(cid:107))α − 4(C + (cid:107)ν(cid:107))2α

Proof. First suppose that x+ ∈ B(cid:15)(¯x). Then

R1(x) = (cid:104)w, x+(α, ν) − PM(x+(α, ν))(cid:105)

≥ −

≥ −

≥ −

µ
2
µ
2
µ
2

dist(x+(α, ν), M)

dist(x, M) −

dist(x, M) −

µ
2
µ
2

(cid:107)x+ − x(cid:107)

2(C + (cid:107)ν(cid:107))α,

90

as desired. Now suppose that x+ /∈ B(cid:15)(¯x). Then by Claim 10, we have dist(x, M) ≤ (cid:107)x−x+(cid:107).
Consequently,

R1 ≥ −(cid:107)w(cid:107)dist(x+, M)

≥ −(cid:107)w(cid:107)dist(x, M) − (cid:107)w(cid:107)(cid:107)x+ − x(cid:107)
≥ −2(cid:107)w(cid:107)(cid:107)x+ − x(cid:107)
≥ −4(C + (cid:107)ν(cid:107))2α

Claim 12. There exists C > 0 such that

R2(x) ≤ 6(C + (cid:107)ν(cid:107))2α

Proof. First suppose that x+ ∈ B(cid:15)(¯x). Then the result follows from the bound:

(cid:107)w(cid:107)(cid:107)x − PM(x) − (x+ − PM(x+))(cid:107) ≤ (1 + C)(cid:107)w(cid:107)(cid:107)x+ − x(cid:107)

≤ 2(C + (cid:107)ν(cid:107))2α,

as desired. Next suppose that x+ /∈ B(cid:15)(¯x). Then by Claim 10, we have dist(x, M) ≤ (cid:107)x−x+(cid:107).
Consequently,

(cid:107)x − PM(x) − (x+ − PM(x+))(cid:107) ≤ dist(x, M) + dist(x+, M) ≤ 3(cid:107)x+ − x(cid:107).

Therefore, we have

(cid:107)w(cid:107)(cid:107)x − PM(x) − (x+ − PM(x+))(cid:107) ≤ 3(cid:107)w(cid:107)(cid:107)x+ − x(cid:107) ≤ 6(C + (cid:107)ν(cid:107))2α,

as desired.

Therefore, plugging these bounds in (3.11.1), it follows that there exists C (cid:48) > 0 such that

(cid:104)Gα(x, ν) − ν, x − PM(x)(cid:105) ≥

µ
2

dist(x, M) − C (cid:48)(1 + (cid:107)ν(cid:107))2α,

as desired.

Verifying Assumption (A2)

Now we verify Assumption (A2). To that end, suppose that x+ ∈ Bε(¯x). Then

(cid:107)PTM(PM(x))(Gα(x, ν) − sg(x) − ν)(cid:107) = (cid:107)PTM(PM(x))w(cid:107)

≤ C(cid:107)w(cid:107)(cid:107)x+ − PM(x)(cid:107)
≤ C(cid:107)w(cid:107)((cid:107)x+ − x(cid:107) + dist(x, M))
≤ C(2(C + (cid:107)ν(cid:107))2α + (C + (cid:107)ν(cid:107))dist(x, M)),

(3.11.2)

91

where the forst inequality follows from strong (a) regularity of X along M. Now suppose
that x+ /∈ Bε(¯x). Then by Claim 10, we have

(cid:107)PTM(PM(x))(Gα(x, ν) − sg(x) − ν)(cid:107) ≤ (cid:107)PTM(PM(x))w(cid:107) ≤ (cid:107)w(cid:107) ≤

2(cid:107)w(cid:107)
(cid:15)

(cid:107)x+ − x(cid:107) ≤

4
(cid:15)

(C + (cid:107)ν(cid:107))2α

(3.11.3)

Therefore, putting together (3.11.2) and (3.11.2), we ﬁnd that there exists C (cid:48) > 0 such that

(cid:107)PTM(PM(x))(Gα(x, ν) − sg(x) − ν)(cid:107) ≤ C (cid:48)(1 + (cid:107)ν(cid:107))2(α + dist(x, M)),

as desired.

3.11.2 Proof of Proposition 3.2.5: the proximal gradient method

Let (cid:15) > 0 be a neighborhood small enough that the following hold for all x ∈ B(cid:15)(¯x) ∩ X :
ﬁrst (3.2.8) holds; second we require that for some C > 0, we have

(cid:107)PTM(PM(x))(∇g(x) + u − ∇Mf (PM(x))(cid:107) ≤ C(cid:112)1 + (cid:107)u(cid:107)2dist(x, M)
for all u ∈ ∂h(x), a consequence of strong (a);3 third, we assume that ∇Mf is C-Lipschitz
on B(cid:15)(¯x) ∩ M; fourth, we assume that ∇g is C-Lipschitz on B(cid:15)(¯x) ∩ X ; Finally, we assume
that PM is C 3, in particular, C-Lipschitz with C-Lipschitz Jacobian on B(cid:15)(¯x).

Turning to the proof, ﬁx x ∈ B(cid:15)/2(¯x) and ν ∈ Rd. We also deﬁne

w = Gα(x, ν) − ∇g(x) − ν

and

x+ = sα(x − α(∇g(x) + ν)).

Finally, we let C be a constant independent of x, α and ν, which changes from line to line.

Verifying Assumption (A1)

We begin with the following bound, which veriﬁes Assumption (A1).
Claim 13. We have w ∈ ˆ∂h(x+) and there exists a constant C independent of x, ν, α, such
that the following bounds hold:

max{(cid:107)Gα(x, ν)(cid:107), (cid:107)w(cid:107)} ≤ C(1 + (cid:107)ν(cid:107));

and

(cid:107)x+ − x(cid:107) ≤ C(1 + (cid:107)ν(cid:107))α.

Proof. Beginning with the inclusion, ﬁrst-order optimality conditions imply that w is a
Fr´echet subgradient:

w =

x − α(∇g(x) + ν) − x+
α

∈ ˆ∂h(x+),

as desired. First, we bound (cid:107)x+ − x(cid:107): Let v = ∇g(x) + ν and observe that there exists
C > 0 such that

1
2α

(cid:107)x+ − x(cid:107)2 ≤ h(x) − h(x+) − (cid:104)v, x+ − x(cid:105)

≤ C(cid:107)x+ − x(cid:107) + (cid:107)v(cid:107)(cid:107)x+ − x(cid:107).

3The strong (a) regularity property for f is a consequence of Equation (2.1.3) and the fact that ∇g(x) is

bounded near ¯x.

92

Consequently, we have (cid:107)x+ − x(cid:107) ≤ (2C + 2(cid:107)v(cid:107))α ≤ 2(2C + (cid:107)ν(cid:107))α, as desired. Second, the
bound Gα(x, ν) follows trivially

(cid:107)Gα(x, ν)(cid:107) = (cid:107)x+ − x(cid:107)/α ≤ 2(2C + (cid:107)ν(cid:107)),

as desired. Finally, we bound (cid:107)w(cid:107):

(cid:107)w(cid:107) = (cid:107)x − x+(cid:107)/α + (cid:107)∇g(x) + ν(cid:107) ≤ 4(2C + (cid:107)ν(cid:107)),

as desired.

Verifying Assumption (A3)

We now verify Assumption (A3). Let us ﬁrst assume that x+ /∈ B(cid:15)(¯x). Then using Claim 10,
we ﬁnd that there exists C > 0 such that

Consequently, there exists C (cid:48) > 0 such that

dist(x, M) ≤ C(cid:107)x − x+(cid:107).

(cid:104)Gα(x, ν) − ν, x − PM(x)(cid:105) ≥ dist(x, M) − dist(x, M)(1 + (cid:107)Gα(x, ν) − ν(cid:107))

≥ dist(x, M) − C(cid:107)x − x+(cid:107)(1 + C(1 + (cid:107)ν(cid:107)) + (cid:107)ν(cid:107))
≥ dist(x, M) − C 2(1 + (cid:107)ν(cid:107))(1 + C(1 + (cid:107)ν(cid:107)) + (cid:107)ν(cid:107))α
≥ dist(x, M) − C (cid:48)(1 + (cid:107)ν(cid:107))2α,

as desired.

Now let us consider the case where x+ ∈ B(cid:15)(¯x). To that end, let v = ∇g(x) + w and

observe that

(cid:104)Gα(x, ν) − ν, x − PM(x)(cid:105)
= (cid:104)v, x − PM(x)(cid:105)
≥ (cid:104)v, x+ − PM(x+)(cid:105) − (cid:107)v(cid:107)(cid:107)(x − PM(x)) − (x+ − PM(x+))(cid:107)
≥ (cid:104)∇g(x+) + w, x+ − PM(x+)(cid:105)

− (cid:107)∇g(x) − ∇g(x+)(cid:107)(cid:107)x+ − PM(x+)(cid:107)
(cid:123)(cid:122)
(cid:125)
=:R1(x)

(cid:124)

− (cid:107)v(cid:107)(cid:107)(x − PM(x)) − (x+ − PM(x+))(cid:107)
(cid:123)(cid:122)
(cid:125)
=:R2(x)

(cid:124)

(3.11.4)

In what follows we will bound R1 and R2.
Claim 14. There exists a constant C independent of x, ν, and α such that

Proof. By Lipschitz continuity, there exists C > 0 with

R1(x) ≤ C(1 + (cid:107)ν(cid:107))2α

(cid:107)∇g(x+) − ∇g(x)(cid:107) ≤ C (cid:107)x − x+(cid:107) ≤ C 2(1 + (cid:107)ν(cid:107))α.

Therefore, the result follows from the bound (cid:107)x+ − PM(x+)(cid:107) ≤ (cid:107)x+ − ¯x(cid:107) ≤ (cid:15).

93

Claim 15. There exists a constant C independent of x, ν, and α such that

Proof. By local Lipschitz continuity of PM, we have

R2(x) ≤ C(1 + (cid:107)ν(cid:107))2α

(cid:107)(x − PM(x)) − (x+ − PM(x+))(cid:107) ≤ (1 + C)(cid:107)x − x+(cid:107) ≤ (1 + C)C(1 + (cid:107)ν(cid:107))α.

Therefore, to complete the proof, we must only note that there exists C > 0 such that

(cid:107)v(cid:107) ≤ (cid:107)∇g(x) + w(cid:107) ≤ C + C(1 + (cid:107)ν(cid:107)),

as desired.

Now we lower bound the dot product in 3.11.4.

Claim 16. There there exists C > 0 such that

(cid:104)∇g(x+) + w, x+ − PM(x+)(cid:105) ≥ µdist(x, M) − (1 + (cid:107)ν(cid:107))o(dist(x, M)) − C(1 + (cid:107)ν(cid:107))2α.

Proof. Recall that by the proximal aiming (3.2.8) property, we have

(cid:104)∇g(x+) + w, x+ − PM(x+)(cid:105) ≥ µdist(x+, M) − (1 + (cid:107)w(cid:107))o(dist(x+, M))

≥ µdist(x, M) − µ(cid:107)x − x+(cid:107) − (1 + (cid:107)w(cid:107))o(dist(x+, M))
≥ µdist(x, M) − µC(1 + (cid:107)ν(cid:107))α − (1 + (cid:107)w(cid:107))o(dist(x+, M))

To complete the proof, combine the bound (1 + (cid:107)w(cid:107)) ≤ C(1 + (cid:107)ν(cid:107)) and the following
calculation:

(1 + (cid:107)ν(cid:107))o(dist(x+, M)) ≤ (1 + (cid:107)ν(cid:107))o(dist(x, M)) + (1 + (cid:107)ν(cid:107))(cid:107)x − x+(cid:107)

≤ (1 + (cid:107)ν(cid:107))o(dist(x, M)) + (1 + (cid:107)ν(cid:107))2α,

which holds as long as (cid:15) is suﬃciently small.

Thus, to complete the proof of assumption Assumption (A3), note that Equation (3.11.4),

together with the claims implies the existence of a constant C > 0 such that

(cid:104)Gα(x, ν) − ν, x − PM(x)(cid:105) ≥ µdist(x, M) − (1 + (cid:107)ν(cid:107))o(dist(x, M)) − C(1 + (cid:107)ν(cid:107))2α,

as desired.

Verifying Assumption (A2)

We now verify Assumption (A2). First suppose that x+ ∈ B(cid:15)(¯x). To that end, notice that

PTM(PM(x))(Gα(x, ν) − ν − ∇fM(PM(x)))
= (PTM(PM(x)) − PTM(PM(x+)))(Gα(x, ν) − ν − ∇fM(PM(x)))
(cid:125)
(cid:123)(cid:122)
=:R3

(cid:124)

+

+ PTM(PM(x+))(Gα(x, ν) − ν − ∇fM(PM(x)))
(cid:123)(cid:122)
(cid:125)
=:R4

(cid:124)

.

94

Let us ﬁrst bound R3: there exists C, C (cid:48) > 0 such that

(cid:107)R3(cid:107) ≤ C(cid:107)PM(x+) − PM(x)(cid:107)(cid:107)Gα(x, ν) − ν − ∇fM(PM(x))(cid:107)

≤ C 2(cid:107)x+ − x(cid:107)(cid:107)Gα(x, ν) − ν − ∇fM(PM(x))(cid:107)
≤ C (cid:48)(1 + (cid:107)ν(cid:107))2α.

where the ﬁrst and second inequalities follow from Lipschitz continuity of PM and ∇PM,
and the third inequality follows from Claim 13 and the boundedness of ∇fM(PM(x)) on
B(cid:15)(¯x). Turning to R4, we ﬁrst use the following decomposition, which is a consequence of
the expression Gα(x, ν) − ν = ∇g(x) + w:

R4 = PTM(PM(x+))(Gα(x, ν) − ν − ∇fM(PM(x)))
= PTM(PM(x+))(∇g(x+) + w − ∇fM(PM(x+)))
(cid:123)(cid:122)
(cid:125)
=:R5

(cid:124)

+

PTM(PM(x+))(∇g(x) − ∇g(x+))
(cid:123)(cid:122)
(cid:125)
(cid:124)
=:R7

.

+ PTM(PM(x+))(∇fM(PM(x+)) − ∇fM(PM(x)))
(cid:123)(cid:122)
(cid:125)
=:R6

(cid:124)

We now bound each term in turn, beginning with (cid:107)R5(cid:107): by the strong (a) regularity for f
and the boundedness of w, There exists C > 0 such that

(cid:107)R5(cid:107) ≤ Cdist(x+, M) ≤ C(dist(x, M) + (cid:107)x+ − x(cid:107)) ≤ Cdist(x, M) + C 2(1 + (cid:107)ν(cid:107))α.

Now we bound (cid:107)R6(cid:107): by local Lipschitz continuity of ∇fM and PM, there exists C > 0 such
that

(cid:107)R6(cid:107) ≤ C(cid:107)PM(x+) − PM(x)(cid:107) ≤ C 2(cid:107)x − x+(cid:107) ≤ C 3(1 + (cid:107)ν(cid:107))α.

Finally we bound (cid:107)R7(cid:107): by local Lipschitz continuity of ∇g, there exists C > 0 such that

(cid:107)R7(cid:107) ≤ (cid:107)∇g(x) − ∇g(x+)(cid:107) ≤ C(cid:107)x − x+(cid:107) ≤ C 2(1 + (cid:107)ν(cid:107))α.

Putting these bounds together, we ﬁnd that there exists C > 0 such that

(cid:107)PTM(PM(x))(Gα(x, ν) − ν − ∇fM(PM(x))(cid:107) ≤ (cid:107)R3(cid:107) + (cid:107)R5(cid:107) + (cid:107)R6(cid:107) + (cid:107)R5(cid:107) + (cid:107)R7(cid:107)

≤ C(1 + (cid:107)ν(cid:107))2(dist(x, M) + α),

as desired.

Now suppose that x+ /∈ B(cid:15)(¯x). Then we have (cid:107)x+ − x(cid:107) ≥ (cid:15)/2 since x ∈ B(cid:15)/2(¯x).

Therefore, there exists C > 0 such that

(cid:107)PTM(PM(x))(Gα(x, ν) − ν − ∇fM(PM(x)))(cid:107) ≤ (cid:107)Gα(x, ν)(cid:107) + (cid:107)ν(cid:107) + (cid:107)∇fM(PM(x))(cid:107)

≤

2
(cid:15)

((cid:107)Gα(x, ν)(cid:107) + (cid:107)ν(cid:107) + (cid:107)∇fM(PM(x))(cid:107))(cid:107)x − x+(cid:107)

≤ C(1 + (cid:107)ν(cid:107))2α,

as desired.

95

3.11.3 Proof of Corollary 3.5.2: asymptotic normality in nonlinear

programming

To prove the result, we verify that f := f0(x) + δX and Gα(x, ν) = (x − PX (x − α(∇f (x) +
ν))/α satisﬁes Assumption A at ¯x. By Proposition 3.2.3, this will follow if we can verify
verify Assumption C. To that end, we ﬁrst note that Assumption (C1) holds by assumption.
Second, notice X is prox-regular at ¯x, so Condition (C4) holds. Third, g := f0 satisﬁes
strong (a) along M at ¯x, since it is smooth along M. Moreover, by Corollary 2.4.2, X
satisﬁes strong (a) along M at ¯x since it is cone-reducible at ¯x. Finally, by Corollary 2.1.5,
the aiming condition holds for f , since f is weakly convex. Then, there exists a constant
c > 0 such that for any δ > 0, there exists (cid:15) > 0 satisfying

(cid:104)v, x − PM(x)(cid:105) ≥ (c − δ(cid:112)1 + (cid:107)v(cid:107)2) · dist(x, M),

(3.11.5)

for all x ∈ B(cid:15)(¯x) and all v ∈ ∂f (x). Then since f0 is C 1 smooth near ¯x, we have
∂f (x) = ∇f0(x) + NX (x). Consequently, plugging in v = ∇f0(x) ∈ ∂f (x) and using the
local boundedness of ∇f0(x), we may ﬁnd a µ > 0 such that

(cid:104)∇f0(x), x − PM(x)(cid:105) ≥ µdist(x, M),

for all x ∈ X near ¯x. Recalling that g := f0, completes the proof of the aiming assumption.
Finally we deal with convergence in the case that supk (cid:107)xk(cid:107) < +∞ with probability 1.
In this case, it is well-known that with probability 1, every limit point of the sequence is
ﬁrst-order stationary, i.e., equal to ¯x (see eg; [26, Corollary 6.4.]4). Consequently, xk almost
surely converges to ¯x.

3.11.4 Proof of Corollary 3.6.3: avoiding active strict saddle via

projected subgradient method

By Proposition 3.2.3 we need only show that Assumption C holds. To that end, note that
Assumptions (C1), (C2), and (C4) hold by assumption. Next we prove (C3). Note that if
g satisﬁes (b) along M, then (C3) holds by Corollary 2.1.5. Next, suppose that g is prox-
regular at x. In this case, since each x ∈ S is Fr´echet critical and Mx is an active manifold,
it follows by Proposition 1.4.2 that for some µ > 0, we have

g(y) − g(PMx(y)) ≥ µdist(y, M),

near x. Consequently, for all v ∈ ∂cg(x), we have

(cid:104)v, y − PMx(y)(cid:105) ≥ g(y) − g(PMx(y)) − O((cid:107)x − y(cid:107)2) ≥ (µ/2)dist(x, M),

for all y near x, verifying (C3).

4Although this corollary is stated for deﬁnable functions, its conclusion holds as long as the problem X
“admits a chain rule.” This easily follows for nonlinear programing, due to Clarke regularity as outlined
in [26, Lemma 5.4.].

96

3.11.5 Proof of Corollary 3.6.4: avoiding active strict saddle via

proximal gradient method

By Proposition 3.2.5, we need only show that Assumption D holds. Note that (D1), (D2),
and (D3) hold by assumption. Thus, we need only verify (D4), which is immediate from
(b)-regularity and Corollary 2.1.5.

3.11.6 Proofs of Corollaries 3.6.5, 3.6.6, and 3.6.7: saddle point

avoidance for generic semialgebraic problems.

We ﬁrst claim that the collection of limit points for all three methods is a connected set of
composite Clarke critical points. To that end, note that by [26, Theorem 6.2/Corollary 6.4],
we know that for each method, on the event the sequence xk is bounded, all limit points are
composite Clarke critical. We claim that the set of limit points is in fact connected. Indeed,
by [7, Lemma 5(iii)], this will follow if

lim
k→0

(cid:107)xk+1 − xk(cid:107) = lim
k→0

(cid:107)αkGαk(xk, νk)(cid:107) = 0.

This in turn follows from [26, Lemma A.4, A.5, and A.6], which shows that Gαk(xk, νk) =
wk + ξk, where wk is bounded and (cid:80)∞
k=1 αkξk exists almost surely. Consequently, we have
(cid:107)αkGαk(xk, νk)(cid:107) = αk(cid:107)wk + ξk(cid:107) → 0 almost surely, as desired.

Next we claim that the sequence xk converges for all three methods. Indeed, by Corol-
laries 3.2.2, 3.2.4, and 3.2.6, it follows that each of the set of composite Clarke critical points
for all three problems is ﬁnite for generic semialgebraic problems. Therefore, since the set
of limit points of xk is connected and discrete, it follows that on the event the sequence xk
is bounded, it must converge to a composite Clarke critical point.

To wrap up the proof, suppose that xk converges to a composite limiting critical point.
Then by Corollaries 3.2.2, 3.2.4, and 3.2.6 for any of the three methods, every composite lim-
iting critical point of f is a composite Fr´echet critical point which is either a local minimizer
or an active strict saddle point at which Assumption A holds along the active manifold. By
Theorem 3.6.2, the sequence xk can converge to the such active strict saddle points only
with probability zero. Therefore, the limit point must be a local minimizer, as desired.

3.11.7 Sequences and Stochastic Processes

Lemmas from other works.

Lemma 3.11.1 (Robbins-Siegmund [79]). Let Ak, Bk, Ck, Dk ≥ 0 be non-negative random
variables adapted to the ﬁltration Fk and satisfying

Ek[Ak+1] ≤ (1 + Bk)Ak + Ck − Dk.

Then on the event {(cid:80)
a.s.−−→ A∞ and (cid:80)
that Ak

k Bk < ∞, (cid:80)
k Dk < ∞ almost surely.

k Ck < ∞}, there is a random variable A∞ < ∞ such

97

Lemma 3.11.2 (Conditional Borel-Cantelli [14]). Let {Xn : n ≥ 1} be a sequence of non-
negative random variables deﬁned on the probability space (Ω, F, P) and {Fn : n ≥ 0} be a
sequence of sub-σ-algebras of F. Let Mn = E [Xn | Fn−1] for n ≥ 1. If {Fn : n ≥ 0} is
nondecreasing, i.e., it is a ﬁltration, then (cid:80)∞
n=1 Mn < ∞}.

n=1 Xn < ∞ almost surely on {(cid:80)∞

Lemma 3.11.3 ( [13, Theorem A]). Let {Fk} be a ﬁltration and let {(cid:15)k} be a sequence of
random variables adapted to F satisfying for all k the bound

E((cid:15)2

k+1 | Fk) < ∞

and

E[(cid:15)k+1 | Fk] = 0.

Let {Φk}k be another sequence of random variables adapted to {Fk}. Let {ck} be a determin-
istic sequence that is square summable but not summable. Suppose that the following hold
almost surely on an event H:

• We have that Marcinkiewick-Zygmund conditions:

lim sup
k

E[(cid:15)2

k+1 | Fk] < ∞

and

lim inf
k

E[|(cid:15)k+1| | Fk] > 0.

• There exists sequences of random variables {rk} and {Rk}, adapted to Fk such that

Φk = rk + Rk and

(cid:107)ri(cid:107)2 < ∞

and

(cid:88)

k

(cid:34)

E

1H

∞
(cid:88)

k=K

(cid:35)



ck|Rk|

= o



(cid:33)1/2
 .

c2
k

(cid:32) ∞
(cid:88)

k=K

Then on H the series (cid:80)∞
L. Moreover, for any p ∈ N and any Fp-measurable random variable Y we have

k=1 ck(Φk + (cid:15)k) converges almost surely to a ﬁnite random variable

P (H ∩ (L = Y )) = 0.

Lemma 3.11.4 ( [27, Exercise 5.3.35]). Let Mk be an L2 martingale adapted to a ﬁltration
Fk and let bk ↑ ∞ is a positive deterministic sequence. Then if

(cid:88)

k≥1

b−2
k

E (cid:2)(Mk − Mk−1)2 | Fk−1

(cid:3) < +∞,

we have b−1

n Mn

a.s.−−→ 0.

Lemma 3.11.5 (Kronecker Lemma). Suppose {xk}k is an inﬁnite sequence of real number
such that

∞
(cid:88)

xk = s

k=1
exists and ﬁnite. Then for any divergent positive nondecreasing sequence {bk}, we have

lim
K→∞

1
bK

K
(cid:88)

k=1

bkxk = 0.

98

Lemmas proved in this work

We will use the following two Lemmas on sequences. The proof of the following Lemma may
be found in Appendix 3.11.8.

Lemma 3.11.6. Fix k0 ∈ N, c > 0, and γ ∈ (1/2, 1]. Suppose that {Xk}, {Yk}, and {Zk}
are nonnegative random variables adapted to a ﬁltration Fk. Suppose the relationship holds:

Ek[Xk+1] ≤ (1 − ck−γ)Xk − Yk + Zk

for all k ≥ k0.

Assume furthermore that c ≥ 6 if γ = 1. Then on the event {(cid:80)∞
there exists a random variable V < ∞ satisfying the following:

k=1

(k+1)2γ−1
log(k+2)2 Zk < +∞},

1. The limit holds

2. The sum is ﬁnite

k2γ−1
log(k + 1)2 Xk

a.s.−−→ V.

∞
(cid:88)

k=1

(k + 1)2γ−1
log(k + 2)2 Yk < +∞.

The proof of the following Lemma may be found in Appendix 3.11.8.

Lemma 3.11.7. Fix k0 ∈ N, c, C > 0, and γ ∈ (1/2, 1]. Suppose that {sk}k is a nonnegative
sequence satisfying

sk ≤

c
12γ

and

k+1 ≤ s2
s2

k − ck−γsk + Ck2γ,

for all k ≥ k0,

Then, there exists a constant Cub depending only on c, C, γ and k0 such that

sk ≤ Cubk−γ,

∀k ≥ 1.

The proof of the following Lemma may be found in Appendix 3.11.8.

Lemma 3.11.8. Fix k0 ∈ N, c, C > 0, and γ ∈ (1/2, 1]. Suppose that {sk}k is a nonnegative
sequence satisfying

sk+1 ≤ (1 − ck−γ)sk + Ck−2γ,

for all k ≥ k0,

Assume furthermore that c ≥ 16 if γ = 1. Then, there exists a constant Cub depending only
on c, C, γ and k0 such that

sk ≤ Cubk−γ,

∀k ≥ 1.

99

3.11.8 Proof of Lemma 3.11.6
Proof. For all k ≥ 0, deﬁne zk := k2γ−1

log(k+1)2 and observe that

Ek[zk+1Xk+1] ≤ zk+1(1 − ck−γ)Xk − zk+1Yk + zk+1Zk

for all k ≥ k0.

Thus, the result will follow from Robbins-Siegmund Lemma 3.11.1 if zk+1(1 − ck−γ) ≤ zk for
all suﬃciently large k. To that end, notice that for suﬃciently large k, we have

(cid:19)2γ−1

(cid:18)k + 1
k

≤ 1 +

2(2γ − 1)
k

.

Therefore,

zk+1
zk

≤ 1 +

2(2γ − 1)
k

for all suﬃciently large k.

Now we deal separately with the cases γ < 1 and γ = 1. First suppose ﬁrst that γ < 1.
Then there exists a constant C (cid:48) > 0 such that

C (cid:48)
kγ ,
Consequently, zk+1/zk ≤ (1 − ck−γ)−1 for all suﬃciently large k, as desired.

1 − ck−γ ≥ 1 +

for all suﬃciently large k.

1

Now assume that γ = 1.

1

1 − ck−1 ≥ 1 +

c
2kγ ,

for all suﬃciently large k.

Consequently, zk+1/zk ≤ (1 − ck−1)−1 for all large k, provided that whenever c ≥ 6.

Proof of Lemma 3.11.7

Proof. It suﬃces to exhibit C (cid:48)

ub > 0 such that

sk ≤ C (cid:48)

ubk−γ

for all suﬃciently large k ≥ k0.

To that end, choose k1 large enough that the following two bounds hold:
= ckγ

1. C (cid:48)

(cid:110) ckγ

√

(cid:111)

ub := max

C, 4C
c

1

12γ , 2

1
12γ

2.

(cid:17)2γ

(cid:16) k1+1
k1

≤ min

(cid:110)

2, 1 + 3γ
k1

(cid:111)
.

Then by assumption, we have

k2γs2

k+1 ≤ k2γs2

k − ckγsk + C

for all k ≥ k1.

Denoting tk := kγsk, we obtain the following bound for all k ≥ k1:

t2
k+1 ≤

(cid:19)2γ

(cid:18) k + 1
k

(t2

k − ctk + C) ≤

(cid:19)2γ

(cid:18) k1 + 1
k1

(t2

k − ctk + C)

(3.11.6)

100

Thus the claim will follow if tk ≤ C (cid:48)
the case k = k1 holds by deﬁnition of C (cid:48)
consider two cases

ub for all k ≥ k1. We prove the claim by induction. First
ub for some k ≥ k1 and

ub. Now suppose tk ≤ C (cid:48)

First suppose tk ∈ [0, 1

2C (cid:48)

ub]. By (3.11.6) and deﬁnition of C (cid:48)

ub, we have

t2
k+1 ≤

≤

(cid:18) k1 + 1
k1
(cid:18) k1 + 1
k1

≤ C (cid:48)2
ub.

(cid:19)2γ

(t2

k + C)

(cid:19)2γ (cid:18) 1
4

C (cid:48)2

ub +

(cid:19)

1
4

C (cid:48)2
ub

Second, suppose tk ∈ [ 1

2C (cid:48)

ub, C (cid:48)

ub]. By (3.11.6) and deﬁnition of C (cid:48)

ub, we have

t2
k+1 ≤

≤

≤

(cid:19)2γ (cid:18)

(cid:19)2γ (cid:18)

(cid:19)2γ

(cid:18)k1 + 1
k1
(cid:18) k1 + 1
k1
(cid:18) k1 + 1
k1
(cid:18) k1 + 1
k1

= C (cid:48)
ub

(t2

k − ctk + C)

C (cid:48)2

ub −

C (cid:48)2

ub −

(cid:19)

+ C

(cid:19)

cC (cid:48)
ub
2
cC (cid:48)
ub
4

(cid:19)2γ (cid:16)

C (cid:48)

ub −

(cid:17)

c
4

We claim that

(cid:16) k1+1
k1

(cid:17)2γ (cid:0)C (cid:48)

ub − c
4

(cid:1) ≤ C (cid:48)

ub. Indeed, we have

(cid:19)2γ (cid:16)

C (cid:48)

ub −

(cid:17)

c
4

C (cid:48)

ub −

(cid:17)

c
4

(cid:19) (cid:16)

3γ
k1
3γC (cid:48)
ub
k1
c
4k1−γ
1

−

−

c
4
c
4

(cid:18)k1 + 1
k1
(cid:18)

≤

1 +

≤ C (cid:48)

ub +

≤ C (cid:48)

ub +

≤ C (cid:48)

ub,

as desired. This completes the induction.

Proof of Lemma 3.11.8

Proof. It suﬃces to exhibit Cub > 0 such that

sk ≤ Cubk−γ

for all suﬃciently large k ≥ k0.

To that end, choose k1 large enough that the following two bounds hold:

101

1. (cid:0) k+1

k

(cid:1)γ ≤ 1 + 2γ

k ≤ 2 for all k ≥ k1.

2. k1−γ

1 ≥ 16γ

c

if γ ∈ ( 1

2, 1).

Now let tk = skkγ, then we rewrite the above inequality as

tk+1 ≤

(cid:18)k + 1
k

(cid:19)γ (cid:20)

(1 − ck−γ)tk +

(cid:21)

,

C
kγ

for all k ≥ k0.

(3.11.7)

Let Cub = max{sk1kγ

1 , 4C, 8C

c }. By deﬁnition of Cub, we know that

tk1 = sk1kγ

1 ≤ Cub.

For the induction step, we consider two cases.
First suppose tk ∈ [0, 1

4Cub]. By (3.11.7) and deﬁnition of Cub, we have
(cid:19)γ

tk+1 ≤

≤

(cid:18) k0 + 1
k0
(cid:18) k0 + 1
k0

(tk + C)

(cid:19)γ (cid:18) 1
4

Cub +

(cid:19)

1
4

Cub

≤ Cub.

Second, suppose tk ∈ [ 1

4Cub,k0,¯x, Cub,k0,¯x]. By (3.11.7) and deﬁnition of ˜Cub,k0,¯x, we have

(cid:19)

(cid:19)

C
kγ

tk+1 ≤

≤

≤

(cid:19)γ (cid:18)

(cid:19)γ (cid:18)

(cid:18) k + 1
k
(cid:18) k + 1
k
(cid:18) k + 1
k
(cid:18) k + 1
k

(cid:19)γ (cid:18)

tk −

Cub −

Cub −

C
kγ

ctk
kγ +
cCub
4kγ +
(cid:19)
cCub
8kγ

(cid:19)2γ (cid:16)

1 −

(cid:17)

c
8kγ

= Cub

We claim that (cid:0) k+1

k

(cid:1)γ (cid:0)1 − c
8kγ

(cid:1) ≤ 1. Indeed, we have

(cid:18) k + 1
k
(cid:18)

≤

1 +

(cid:19)γ (cid:16)

1 −

(cid:17)

c
8kγ

(cid:19) (cid:16)

1 −

2γ
k

(cid:17)

c
8kγ

≤ 1 +

2γ
k

−

c
8kγ

When γ = 1, 1 + 2γ
k − c
choice of k0. This completes the induction.

8kγ by our assumption on c. When γ ∈ ( 1

2, 1), 1 + 2γ

k − c

8kγ ≤ 1 by our

102

