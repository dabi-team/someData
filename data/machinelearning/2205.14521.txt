2
2
0
2

y
a
M
8
2

]
L
C
.
s
c
[

1
v
1
2
5
4
1
.
5
0
2
2
:
v
i
X
r
a

Learning Non-Autoregressive Models from Search
for Unsupervised Sentence Summarization

Puyuan Liu, Chenyang Huang, Lili Mou
Dept. Computing Science, Alberta Machine Intelligence Institute (Amii)
University of Alberta, Canada
{puyuan, chuang8}@ualberta.ca, doublepower.mou@gmail.com

Abstract

Text summarization aims to generate a short
summary for an input text. In this work, we
propose a Non-Autoregressive Unsupervised
Summarization (NAUS) approach, which does
not require parallel data for training. Our
NAUS ﬁrst performs edit-based search to-
wards a heuristically deﬁned score, and gener-
ates a summary as pseudo-groundtruth. Then,
we train an encoder-only non-autoregressive
Transformer based on the search result. We
also propose a dynamic programming ap-
proach for length-control decoding, which is
important for the summarization task. Ex-
periments on two datasets show that NAUS
achieves state-of-the-art performance for unsu-
pervised summarization, yet largely improving
inference efﬁciency. Further, our algorithm is
able to perform explicit length-transfer sum-
mary generation.1

1

Introduction

Text summarization is an important natural lan-
guage processing (NLP) task, aiming at generating
concise summaries for given texts while preserving
the key information. It has extensive real-world
applications such as headline generation (Nenkova
et al., 2011). In this paper, we focus on the set-
ting of sentence summarization (Rush et al., 2015;
Filippova et al., 2015).

State-of-the-art text summarization models are
typically trained in a supervised way with large
training corpora, comprising pairs of long texts and
their summaries (Zhang et al., 2020; Aghajanyan
et al., 2020, 2021). However, such parallel data are
expensive to obtain, preventing the applications to
less popular domains and less spoken languages.

Unsupervised text generation has been attracting
increasing interest, because it does not require par-
allel data for training. One widely used approach

1Our code, model, and output are released at: https:

//github.com/MANGA-UOFA/NAUS

is to compress a long text into a short one, and to
reconstruct it to the long text by a cycle consis-
tency loss (Miao and Blunsom, 2016; Wang and
Lee, 2018; Baziotis et al., 2019). Due to the in-
differentiability of the compressed sentence space,
such an approach requires reinforcement learning
(or its variants), which makes the training difﬁcult
(Kreutzer et al., 2021).

Recently, Schumann et al. (2020) propose an
edit-based approach for unsupervised summariza-
tion. Their model maximizes a heuristically deﬁned
scoring function that evaluates the quality (ﬂuency
and semantics) of the generated summary, achiev-
ing higher performance than cycle-consistency
methods. However, the search approach is slow
in inference because hundreds of search steps are
needed for each data sample. Moreover, their ap-
proach can only select words from the input sen-
tence with the word order preserved. Thus, it is
restricted and may generate noisy summaries due
to the local optimality of search algorithms.

To address the above drawbacks, we propose
a Non-Autoregressive approach to Unsupervised
Summarization (NAUS). The idea is to perform
search as in Schumann et al. (2020) and, inspired
by Li et al. (2020), to train a machine learning
model to smooth out such noise and to speed up the
inference process. Different from Li et al. (2020),
we propose to utilize non-autoregressive decoders,
which generate all output tokens in parallel due to
our following observations:

• Non-autoregressive models are several times
faster than autoregressive generation, which is im-
portant when the system is deployed.

• The input and output of the summarization task
have a strong correspondence. Non-autoregressive
generation supports encoder-only architectures,
which can better utilize such input–output cor-
respondence and even outperform autoregressive
models for summarization.

• For non-autoregressive models, we can de-

In ACL, pages 7916-7929, 2022

 
 
 
 
 
 
sign a length-control algorithm based on dynamic
programming to satisfy the constraint of output
lengths, which is typical in summarization applica-
tions but cannot be easily achieved with autoregres-
sive models.

We conducted experiments on Gigaword head-
line generation (Graff et al., 2003) and DUC2004
(Over and Yen, 2004) datasets. Experiments show
that our NAUS achieves state-of-the-art perfor-
mance on unsupervised summarization; especially,
it outperforms its teacher (i.e., the search approach),
conﬁrming that NAUS can indeed smooth out the
search noise. Regarding inference efﬁciency, our
NAUS with truncating is 1000 times more efﬁcient
than the search approach; even with dynamic pro-
gramming for length control, NAUS is still 100
times more efﬁcient than search and several times
more efﬁcient than autoregressive models. Our
NAUS is also able to perform length-transfer sum-
mary generation, i.e., generating summaries of dif-
ferent lengths from training.

2 Approach

In our approach, we ﬁrst follow Schumann et al.
(2020) and obtain a summary by discrete search
towards a heuristically deﬁned objective function
(§2.1). Then, we propose a non-autoregressive
model for the summarization task (§2.2). We
present the training strategy and the proposed
length-control algorithm in §2.3.

2.1 Search-Based Summarization

Consider a given source text x = (x1, x2, . . . , xn).
The goal of summarization is to ﬁnd a shorter text
y = (y1, y2, . . . , ym) as the summary.

Our work on unsupervised summarization fol-
lows the recent progress of search-based text gener-
ation (Liu et al., 2020, 2021a; Kumar et al., 2020).
Schumann et al. (2020) formulate summarization
as word-level extraction (with order preserved), and
apply edit-based discrete local search to maximize
a heuristically designed objective.

Speciﬁcally, the objective function considers
two aspects: (1) a language ﬂuency score fLM(y),
given by the reciprocal of a language model’s
perplexity; and (2) a semantic similarity score
fSIM(y; x), given by the cosine embeddings. The
overall objective combines the two aspects as

f (y; x) = fLM(y) · fSIM(y; x)γ

(1)

where γ is a weighting hyperparameter. Interested

readers are referred to Schumann et al. (2020) for
the details of the scoring function.

Further, the desired summary length can be spec-
iﬁed as a hard constraint, achieved by searching
only among sentences of the correct length. Sup-
pose the desired summary length is T , the approach
selects T random words from the input, and max-
imizes the scoring function (1) by changing the
selection and non-selection of two words.

A greedy hill-climbing algorithm determines
whether the change is accepted or not. In other
words, a change is accepted if the score improves,
or rejected otherwise. Such a process continues
until a (possibly local) optimum is found.

A pilot analysis in Schumann et al. (2020) shows
that words largely overlap between a source text
and its reference summary. This explains the high
performance of such a word extraction approach,
being a state-of-the-art unsupervised summariza-
tion system and outperforming strong competitors,
e.g., cycle consistency (Wang and Lee, 2018; Bazi-
otis et al., 2019).

2.2 Non-Autoregressive Model for

Summarization

Despite the high performance, such edit-based
search has several drawbacks. First, the search
process is slow because hundreds of local search
steps are needed to obtain a high-quality summary.
Second, their approach only extracts the original
words with order preserved. Therefore, the gener-
ated summary is restricted and may be noisy.

To this end, we propose a Non-Autoregressive
approach to Unsupervised Summarization (NAUS)
by learning from the search results. In this way,
the machine learning model can smooth out the
search noise and is much faster, largely alleviat-
ing the drawbacks of search-based summarization.
Compared with training an autoregressive model
from search (Li et al., 2020), non-autoregressive
generation predicts all the words in parallel, further
improving inference efﬁciency by several times.

Moreover, a non-autoregressive model enables
us to design an encoder-only architecture, which is
more suited to the summarization task due to the
strong correspondence between input and output,
which cannot be fully utilized by encoder–decoder
models, especially autoregressive ones.

Speciﬁcally, we propose to use multi-layer
Transformer (Vaswani et al., 2017) as the non-
autoregressive architecture for summarization.

Figure 1: The overview of our NAUS approach. In each search step, input words corresponding to grey cells are
selected. Besides, the blue arrow refers to the training process, and the green arrow refers to inference.

Each Transformer layer is composed of a multi-
head attention sublayer and a feed-forward sub-
layer. Additionally, there is a residual connection
in each sublayer, followed by layer normalization.
Let X (n) ∈ RT ×d be the representation at the
nth layer, where T is the number of words and d
is the dimension. Specially, the input layer X (0) is
the embeddings of words. Suppose we have h at-
tention heads. The output of the ith head in the nth
attention sublayer is A(n)
Vi,
where Qi, Ki, and Vi are matrices calculated by
three distinct multi-layer perceptrons (MLPs) from
X (n−1); dk is the attention dimension.

i = softmax

(cid:16) QiK(cid:62)
i√
dk

(cid:17)

Multiple attention heads are then concatenated:

A(n) = Concat (cid:0)A(n)

1 , . . . , A(n)
h
where WO ∈ Rd×d is a weight matrix.

(cid:1)WO

Then, we have a residual connection and layer

normalization by

¯A(n) = LayerNorm (cid:0)X (n−1) + A(n)(cid:1)

(2)

Further, an MLP sublayer processes ¯A(n), followed
by residual connection and layer normalization,
yielding the nth layer’s representation

X (n) = LayerNorm (cid:0) ¯A(n) + MLP( ¯A(n))(cid:1) (3)

The last Transformer layer X (N ) is fed to
softmax to predict the words of the summary in a
non-autoregressive manner, that is, the probability
at the tth step is given by softmax(W x(N )
), where
x(N )
is the tth row of the matrix X (N ) and W is
t
the weight matrix.

t

It is emphasized that, in the vocabulary, we in-
clude a special blank token (cid:15), which is handled by
dynamic programming during both training and in-
ference (§2.3). This enables us to generate a shorter
summary than the input with such a multi-layer

Transformer.

Our model can be thought of as an encoder-
only architecture, differing from a typical encoder–
decoder model with cross attention (Vaswani et al.,
2017; Baziotis et al., 2019; Zhou and Rush, 2019).
Previously, Su et al. (2021) propose a seemingly
similar model to us, but put multiple end-of-
sequence (EOS) tokens at the end of the generation;
thus, they are unable to maintain the correspon-
dence between input and output. Instead, we allow
blank tokens scattering over the entire sentence;
the residual connections in Eqns (2) and (3) can
better utilize such input–output correspondence for
summarization.

2.3 Training and Inference

In this section, we ﬁrst introduce the Connectionist
Temporal Classiﬁcation (CTC) training. Then, we
propose a length-control decoding approach for
summary generation.

CTC Training. The Connectionist Temporal
Classiﬁcation (CTC, Graves et al., 2006) algorithm
allows a special blank token (cid:15) in the vocabulary,
and uses dynamic programming to marginalize out
such blank tokens, known as latent alignment (Sa-
haria et al., 2020). In addition, non-autoregressive
generation suffers from a common problem that
words may be repeated in consecutive steps (Gu
et al., 2018; Lee et al., 2018); thus, CTC merges
repeated words unless separated by (cid:15). For example,
the sequence of tokens a(cid:15)(cid:15)aabb(cid:15) is reduced to the
text aab, denoted by Γ(a(cid:15)(cid:15)aabb(cid:15)) = aab.

Concretely, the predicted likelihood is marginal-
ized over all possible ﬁllings of (cid:15), i.e., all possible
token sequences that are reduced to the groundtruth
text:

P (y|x) =

(cid:88)

P (w|x)

(4)

w:Γ(w)=y

Output:    high productivity doesn't in the auto industryhighproductivityInput:correspondn'talwaysprofitsdoeswithhighintheautoindustryCTC training from search resultsLength-control decoding w/ dynamic programmingInput:Hill-climbing search . . .Output: high productivity doesn't always profit auto industry …………………highproductivitytheautoindustrydoes𝑤!𝑤"where P (w|x) is the probability of generating a
sequence of tokens w. Although enumerating every
candidate in {w : Γ(w) = y} is intractable, such
marginalization fortunately can be computed by
dynamic programming in an efﬁcient way.

Let αs,t = (cid:80)

P (w1:s|x) be the
w1:s:Γ(w1:s)=y1:t
marginal probability of generating y1:t up to the
sth decoding slot. Moreover, αs,0 is deﬁned to be
the probability that w1:s is all (cid:15), thus not having
matched any word in y. The αs,t variable can be
further decomposed into two terms αs,t = α(cid:15)
s,t +
α¬(cid:15)
s,t, where the ﬁrst term is such probability with
ws = (cid:15), and the second term ws (cid:54)= (cid:15). Apparently,
the initialization of α variables is

α(cid:15)
α¬(cid:15)
α(cid:15)
α¬(cid:15)

1,0 = P (w1 = (cid:15)|x)
1,1 = P (w1 = y1|x)
1,t = 0, ∀t ≥ 1
1,t = 0, ∀t > 1 or t = 0

(5)

(6)

(7)

(8)

Eqn. (7) is because, at the ﬁrst prediction slot, the
empty token (cid:15) does not match any target words;
Eqn. (8) is because the predicted non-(cid:15) ﬁrst token
must match exactly the ﬁrst target word.

The recursion formula for α(cid:15)

s,t is

α(cid:15)

s,t = αs−1,tP (wt = (cid:15)|x)

since the newly predicted token (cid:15) with probabil-
ity P (wt = (cid:15)|x) does not match any target word,
inheriting αs−1,t.

The recursion formula for α¬(cid:15)

α¬(cid:15)

s,t =






(cid:0)α(cid:15)

s−1,t−1 + α¬(cid:15)

s−1,t

(cid:0)αs−1,t−1 + α¬(cid:15)

s−1,t

s,t is
(cid:1) P (ws = yt|x),
if yt = yt−1
(cid:1) P (ws = yt|x),
otherwise

Here, ws is not (cid:15), so we must have ws = yt, having
the predicted probability P (ws = yt|x).

If yt = yt−1, then we have two sub-cases: ﬁrst,
w1:s−1 is reduced to y1:t−1 with ws−1 = (cid:15) separat-
ing two repeating words in y, having probability
α(cid:15)
s−1,t−1; or second, w1:s−1 is reduced to y1:t with
ws−1 = yt (cid:54)= (cid:15), having probability α¬(cid:15)
s−1, which
implies we are merging ws−1 and ws.

If yt (cid:54)= yt−1, w1:s−1 is reduced to either y1:t−1
or y1:t. In the ﬁrst case, ws−1 can be either (cid:15) or
non-(cid:15), given by αs−1,t−1 = α(cid:15)
s−1,t−1.
In the second case, we must have ws−1 (cid:54)= (cid:15), which
has a probability of α¬(cid:15)
s−1,t.

s−1,t−1 + α¬(cid:15)

Figure 2: Illustration of our length-control algorithm.

erated sequence matches the entire target text.

The CTC maximum likelihood estimation is to
maximize the marginal probability, which is equiv-
alent to minimizing the loss −α|w|,|y|. Since the
dynamic programming formulas are differentiable,
the entire model can be trained by backpropagation
in an end-to-end manner with auto-differentiation
tools (such as PyTorch).

Length-Control Inference. Controlling output
length is the nature of the summarization task, for
example, displaying a short news headline on a mo-
bile device. Moreover, Schumann et al. (2020)
show that the main evaluation metric ROUGE
(Lin, 2004) is sensitive to the summary length, and
longer summaries tend to achieve higher ROUGE
scores. Thus, it is crucial to control the summary
length for fair comparison.

We propose a length-control algorithm by dy-
namic programming (DP), following the nature of
CTC training. However, our DP is an approximate
algorithm because of the dependencies introduced
by removing consecutive repeated tokens. Thus,
we equip our DP with a beam search mechanism.
We deﬁne Bs,t to be a set of top-B sequences
with s predicted tokens that are reduced to t words.
Bs,t is constructed by three scenarios.

First, the blank token (cid:15) is predicted for the sth
generation slot, and thus the summary length t re-
mains the same, shown by the blue arrow in Fig-
ure 2. This yields a set of candidates

B(1)

s,t = (cid:8)b ⊕ (cid:15) : b ∈ Bs−1,t

(cid:9)

(9)

where ⊕ refers to string/token concatenation.

Second, a repeated word is predicted for the sth
generation slot, i.e., bs−1 for a subsequence b of
length s−1. In this case, the summary length t also
remains the same, also shown in the blue arrow in
Figure 2. This gives a candidate set

B(2)

s,t = (cid:8)b ⊕ bs−1 : b ∈ Bs−1,t

(cid:9)

(10)

Finally, α|w|,|y| is the marginal probability in
Eqn. (4), as it is the probability that the entire gen-

Third, a non-(cid:15), non-repeating word ws is gener-
ated, increasing the summary length from t − 1 to

𝑏!"#or 𝜖non-𝜖or non-𝑏!"#Generation slot 𝑠Partial sentence length 𝑡𝑠−1,𝑡−1𝑠−1,𝑡𝑠,𝑡B(3)

t, shown by the red arrow in Figure 2. This gives
(cid:8)b ⊕ w : b ∈ Bs−1,t−1, ws (cid:54)= (cid:15),
ws (cid:54)= bs−1
(11)

s,t = topB

(cid:9)

where topB selects the best B elements by the
probability P (ws|x).

Based on the three candidates sets, we select

top-B sequences to keep the beam size ﬁxed:

Bs,t = topB(B(1)

s,t ∪ B(2)

s,t ∪B(3)
s,t )

(12)

where the sequences are ranked by their predicted
joint probabilities.

Theorem 1. (1) If repeating tokens are not merged,
then the proposed length-control algorithm with
beam size B = 1 ﬁnds the exact optimum BS,T
being the most probable length-T sentence given
by S prediction slots. (2) If we merge repeating
tokens predicted by CTC-trained models, the above
algorithm may not be exact.

Appendix A presents the proof of the theorem
and provides a more detailed analysis, showing
that our length-control algorithm, although being
approximate inference, can generate a summary of
the desired length properly. Compared with trun-
cating an overlength output, our approach is able
to generate more ﬂuent and complete sentences.
Also, our length-control algorithm is different from
conventional beam search, shown in Appendix C.

3 Experiments

3.1 Setup

Datasets. We evaluated our NAUS model on Giga-
word headline generation and DUC2004 datasets.
The headline generation dataset (Rush et al.,
2015) is constructed from the Gigaword news cor-
pus (Graff et al., 2003), where the ﬁrst sentence
of a news article is considered as input text and
the news title is considered as the summary. The
dataset contains 3.8M/198K/1951 samples for train-
ing/validation/test. Based on the analysis of the
training size in Appendix B, we used 3M samples
for training NAUS.

It should be emphasized that, when NAUS learns
from search, we only use the input of the training
corpus: we perform search (Schumann et al., 2020)
for each input, and train our NAUS from the search
results. Therefore, we do not utilize any labeled
parallel data, and our approach is unsupervised.

Moreover, we considered two settings with de-
sired summary lengths of 8 and 10, following Schu-

mann et al. (2020). Our NAUS is trained from
respective search results.

The DUC2004 dataset (Over and Yen, 2004) is
designed for testing only with 500 samples, where
we also take the ﬁrst sentence of an article as the
input text. Our NAUS is transferred from the above
headline generation corpus. Based on the length
of DUC2004 summaries, we trained NAUS from
search results with 13 words, also following Schu-
mann et al. (2020) for fair comparison.

Evaluation Metrics. We evaluated the quality
of predicted summaries by ROUGE scores 2 (Lin,
2004), which are the most widely used metrics
in previous work (Wang and Lee, 2018; Baziotis
et al., 2019; Zhou and Rush, 2019). Speciﬁcally,
ROUGE-n evaluates n-gram overlap between a
predicted summary and its reference summary;
ROUGE-L, instead, measures the longest common
sequence between the predicted and reference sum-
maries.

Different ROUGE variants are adopted in previ-
ous work, depending on the dataset. We followed
the standard evaluation scripts and evaluated head-
line generation by ROUGE F1 (Wang and Lee,
2018; Baziotis et al., 2019; Schumann et al., 2020)
and DUC2004 by Truncate ROUGE Recall (Dorr
et al., 2003; West et al., 2019).

In addition to summary quality, we also eval-
uated the inference efﬁciency of different meth-
ods, as it is important for the deployment of deep
learning models in real-time applications. We
report the average inference time in seconds for
each data sample, and compare the speedup with
Schumann et al. (2020)’s search approach, which
achieves (previous) state-of-the-art ROUGE scores.
Our experiments were conducted on an i9-9940X
CPU and an RTX6000 graphic card. Appendix B
presents additional implementation details.

3.2 Results and Analyses

Main Results. Table 1 presents the performance of
our model and baselines on the Gigaword headline
test set. For a fair comparison, we categorize all
approaches by average summary lengths of ~8 and
~10 into Groups A and B, respectively.

The Lead baseline extracts the ﬁrst several words
of the input sentence. Despite its simplicity, the
Lead approach is a strong summarization baseline
adopted in most previous work (Févry and Phang,
2018; Baziotis et al., 2019).

2https://github.com/tagucci/pythonrouge

Group

A
(desired
length 8)

B
(desired
length 10)

#

1
2
3
4
5
6
7
8
9
10
11
12
13
14

Baseline

Search

Learn from
search

Baseline

Search

Learn from
search

Approach

Lead (8 words)†
Schumann et al. (2020)†
Our replication
Su et al. (2021)
NAUS (truncate)
NAUS (length control)
Lead (10 words)†
Wang and Lee (2018)†
Zhou and Rush (2019)†
Schumann et al. (2020)†
Our replication
Su et al. (2021)
NAUS (truncate)
NAUS (length control)

Len

7.9
7.9
7.9
7.7
7.8
7.8
9.8
10.8
9.3
9.8
9.8
9.4
9.8
9.8

R-1
21.39
26.32
26.17
26.88
27.27
27.94
23.03
27.29
26.48
27.52
27.35
27.86
28.24
28.55

ROUGE F1
R-L
R-2
20.03
7.42
24.19
9.63
9.69
24.10
24.54
9.37
24.96
9.49
25.51
9.24
21.29
7.95
24.59
10.01
24.41
10.05
10.27
24.91
24.87
10.25
25.51
9.88
25.40
10.04
25.78
9.97

∆R
-11.12
0.18
0
0.83
1.76
2.73
-10.2
-0.58
-1.53
0.23
0
0.78
1.21
1.83

Inf.Time Speedup

–
–
6.846
0.017
0.005
0.041
–
–
–
–
9.217
0.020
0.005
0.044

–
–
1x
403x
1369x
167x
–
–
–
–
1x
461x
1843x
210x

Table 1: Results on the Gigaword headline generation test set. Len: Average length of predicted summaries. R-1,
R-2, R-L: ROUGE-1, ROUGE-2, ROUGE-L. ∆R: The difference of total ROUGE (sum of R-1, R-2, and R-L)
in comparison with the (previous) state-of-the-art search method under replication. Inf.Time: Average inference
time in seconds for one sample on an i9-9940X CPU and a RTX6000 GPU. Speedup: Relative to Schumann et al.
(2020). †Results quoted from previous papers; others are given by our experiments.

Time Speedup

Model

Lead (75 characters)†
Zajic et al. (2004)†
Baziotis et al. (2019)†
West et al. (2019)†

R-1
22.50
25.12
22.13
22.85
Schumann et al. (2020)† 26.04
26.14
26.25
26.52
26.71

Our replication
Su et al. (2021)
NAUS (truncate)
NAUS (length control)

ROUGE Recall

R-2
6.49
6.46
6.18
5.71
8.06
8.03
7.66
7.88
7.68

R-L
19.72
20.12
19.30
19.87
22.90
22.88
22.83
22.91
23.06

∆R
-8.34
-5.35
-9.44
-8.62
-0.05

-0.31
0.26
0.40

–
–
–
–
–
0 12.314
0.022
0.005
0.048

–
–
–
–
–
1x
559x
2463x
257x

Table 2: Results on the DUC2004 dataset.
from previous papers.

†Quoted

Wang and Lee (2018) utilize cycle consis-
tency (Miao and Blunsom, 2016) for unsupervised
summarization; the performance is relatively low,
because the cycle consistency loss cannot ensure
the generated text is a valid summary. Zhou and
Rush (2019) perform beam search towards a step-
by-step decomposable score of ﬂuency and contex-
tual matching. Both are unable to explicitly control
the summary length: in a fair comparison of length
10 (Group B, Table 1), their performance is worse
than the (previous) state-of-the-art approach (Schu-
mann et al., 2020),3 which performs edit-based
local search.

Our NAUS approach follows Schumann et al.
(2020), but trains a non-autoregressive model from

3Schumann et al. (2020) present a few variants that use
additional datasets for training language models (in an unsu-
pervised way). In our study, we focus on the setting without
data augmentation, i.e., the language model is trained on non-
parallel the Gigawords corpus.

search results. We consider two settings for con-
trolling the summary length:
truncating longer
summaries and decoding with our proposed length-
control algorithm. Both of our variants outperform
Schumann et al. (2020) by 1.21–2.73 in terms of the
total ROUGE score (Rows 5–6 & 13–14, Table 1).
As mentioned, Schumann et al. (2020) only extract
original words with order preserved, yielding noisy
sentences. Our NAUS, as a student, learns from the
search-based teacher model and is able to smooth
out its noise. This is a compelling result, as our
student model outperforms its teacher.

Regarding inference efﬁciency, our NAUS
method with truncating is more than 1300 times
faster than Schumann et al. (2020), because we
do not need iterative search. Even with dynamic
programming and beam search for length control,
NAUS is still over 100 times faster. This shows our
NAUS is extremely efﬁcient in inference, which is
important for real-time applications.

Although the efﬁciency of Wang and Lee (2018)
and Zhou and Rush (2019) is not available, we
still expect our approach to be a few times faster
(despite our higher ROUGE scores) because their
models are autoregressive. By contrast, our NAUS
is non-autoregressive, meaning that it predicts all
words simultaneously. We will provide a con-
trolled comparison between autoregressive and non-
autoregressive models in Table 3.

Table 2 shows the results on the DUC2004
dataset. The cycle-consistency approach (Bazio-

#

1
2
3
4
5
6
7
8
9

10
11
12
13
14
15
16
17
18

Search

AR

NAR
enc-dec

NAR
enc-only

Search

AR

NAR
enc-dec

NAR
enc-only

Approach

ROUGE Recall

Schumann et al.
Our replication
Transformer (T)
Vanilla
CTC (T)
CTC (LC)
Su et al. (2021)
Our NAUS (T)
Our NAUS (LC)

R-2

R-1
Group A (desired length 8)
9.63
26.32
9.69
26.17
9.51
26.65
8.33
24.87
9.20
27.30
9.13
27.76
9.37
26.88
9.49
27.27
27.94
9.24
Group B (desired length 10)
27.52
27.35
27.06
25.77
28.14
28.45
27.86
28.24
28.55

10.27
10.25
9.63
8.69
10.07
9.81
9.88
10.04
9.97

Schumann et al.
Our replication
Transformer (T)
Vanilla
CTC (T)
CTC (LC)
Su et al. (2021)
Our NAUS (T)
Our NAUS (LC)

R-L ∆R

0.18
24.19
0
24.10
24.67
0.87
22.74 -4.02
1.5
24.96
2.26
25.33
0.83
24.54
1.76
24.96
25.51
2.73

0.23
24.91
24.87
0
24.55 -1.23
23.52 -4.49
1.11
25.37
1.42
25.63
0.78
25.51
1.21
25.40
1.83
25.78

Speedup

–
1x
58x
571x
571x
149x
403x
1396x
167x

–
1x
66x
709x
709x
192x
461x
1843x
210x

Table 3: Model analysis on headline generation.
AR: Autoregressive models. NAR enc-dec: Non-
autoregressive encoder–decoder. NAR enc-only: Non-
autoregressive encoder-only. T: Truncating. LC:
Length control. All AR and NAR models use the Trans-
former architecture.

tis et al., 2019; West et al., 2019) does not per-
form well on this dataset, outperformed by an
early rule-based syntax tree trimming approach (Za-
jic et al., 2004) and the state-of-the-art edit-based
search (Schumann et al., 2020).

The performance of our NAUS model is con-
sistent with Table 1, outperforming all previous
methods in terms of the total ROUGE score, and
being 100–1000 times faster than the search ap-
proach (Schumann et al., 2020).

In general, the proposed NAUS not only achieves
state-of-the-art ROUGE scores for unsupervised
summarization, but also is more efﬁcient when de-
ployed. Results are consistent on both datasets,
demonstrating the generality of our NAUS.

In-Depth Analyses. We conduct in-depth anal-
yses on the proposed NAUS model in Table 3. Due
to the limit of time and space, we chose the Giga-
word headline generation as our testbed. All the
autoregressive (AR) and non-autoregressive (NAR)
variants learn from the search output of our replica-
tion (Rows 2 & 11), where we achieve very close
results to those reported in Schumann et al. (2020).
We ﬁrst tried vanilla encoder–decoder NAR
Transformer (Rows 4 & 13, Gu et al., 2018), where
we set the number of decoding slots as the de-
sired summary length; thus, the blank token and
the length-control algorithm are not needed. As
seen, a vanilla NAR model does not perform well,

and CTC largely outperforms vanilla NAR in both
groups (Rows 5–6 & 14–15). Such results are
highly consistent with the translation literature (Sa-
haria et al., 2020; Chan et al., 2020; Gu and Kong,
2021; Qian et al., 2021; Huang et al., 2022).

The proposed encoder-only NAUS model out-
performs encoder–decoder ones in both groups in
terms of the total ROUGE score, when the sum-
mary length is controlled by either truncating or
length-control decoding (Rows 8–9 & 17–18). Pro-
foundly, our non-autoregressive NAUS is even bet-
ter than the autoregressive Transformer (Rows 3
& 12). We also experimented with previous non-
autoregressive work for supervised summariza-
tion (Su et al., 2021)4 in our learning-from-search
setting. Although their approach appears to be
encoder-only, it adds end-of-sequence (EOS) to-
kens at the end of the generation, and thus is unable
to utilize the input–output correspondence. Their
performance is higher than vanilla NAR models,
but lower than ours. By contrast, NAUS is able to
capture such correspondence with the residual con-
nections, i.e., Eqns. (2) and (3), in its encoder-only
architecture.

Generally, the efﬁciency of encoder-only NAR5
(without length-control decoding) is ~2 times faster
than encoder–decoder NAR and ~20 times faster
than the AR Transformer.

Further, our length-control decoding improves
the total ROUGE score, compared with truncating,
for both encoder–decoder CTC and encoder-only
NAUS models (Rows 6, 9, 15, & 18), although its
dynamic programming is slower. Nevertheless, our
non-autoregressive NAUS with length control is
~200 times faster than search and ~3 times faster
than the AR Transformer.

Additional Results. We present additional re-

sults in our appendices:

C. Analysis of Beam Search

D. Case Study

E. Human Evaluation

F. Length-Transfer Summarization

4To the best of our knowledge,

the other two non-
autoregressive supervised summarization models are Yang
et al. (2021) and Qi et al. (2021). Their code and pretrained
models are not available, making replication difﬁcult.

5The standard minimal encoder–decoder NAR model has
6 layers for the encoder and another 6 layers for the de-
coder (Vaswani et al., 2017). Our NAUS only has a 6-layer
encoder. Our pilot study shows that more layers do not further
improve performance in our encoder-only architecture.

4 Related Work

Summarization systems can be generally catego-
rized into two paradigms: extractive and abstrac-
tive. Extractive systems extract certain sentences
and clauses from input, for example, based on
salient features (Zhou and Rush, 2019) or feature
construction (He et al., 2012). Abstraction systems
generate new utterances as the summary, e.g., by
sequence-to-sequence models trained in a super-
vised way (Zhang et al., 2020; Liu et al., 2021b).

Recently, unsupervised abstractive summariza-
tion is attracting increasing attention. Yang et al.
(2020) propose to use the Lead baseline (ﬁrst sev-
eral sentences) as the pseudo-groundtruth. How-
ever, such an approach only works with well-
structured articles (such as CNN/DailyMail). Wang
and Lee (2018) and Baziotis et al. (2019) use cycle
consistency for unsupervised summarization. Zhou
and Rush (2019) propose a step-by-step decompos-
able scoring function and perform beam search for
summary generation. Schumann et al. (2020) pro-
pose an edit-based local search approach, which
allows a more comprehensive scoring function and
outperforms cycle consistency and beam search.

Our paper follows Schumann et al. (2020) but
trains a machine learning model to improve efﬁ-
ciency and smooth out search noise. Previously,
Li et al. (2020) ﬁne-tune a GPT-2 model based
on search results for unsupervised paraphrasing;
Jolly et al. (2022) adopt the search-and-learning
framework to improve the semantic coverage for
few-shot data-to-text generation. We extend pre-
vious work in a non-trivial way by designing a
non-autoregressive generator and further proposing
a length-control decoding algorithm.

The importance of controlling the output length
is recently realized in the summarization commu-
nity. Baziotis et al. (2019) and Su et al. (2021)
adopt soft penalty to encourage shorter sentences;
Yang et al. (2021) and Qi et al. (2021) control the
summary length through POS tag and EOS predic-
tions. None of these studies can control the length
explicitly. Song et al. (2021) is able to precisely
control the length by progressively ﬁlling a pre-
determined number of decoding slots, analogous to
the vanilla NAR model in our non-autoregressive
setting.

Non-autoregressive generation is originally pro-
posed for machine translation (Gu et al., 2018; Guo
et al., 2020; Saharia et al., 2020), which is later
extended to other text generation tasks. Wiseman

et al. (2018) address the table-to-text generation
task, and model output segments by a hidden semi-
Markov model (Ostendorf et al., 1996), simulta-
neously generating tokens for all segments. Jia
et al. (2021) apply non-autoregressive models to
extractive document-level summarization. Su et al.
(2021) stack a non-autoregressive BERT model
with a conditional random ﬁeld (CRF) for abstrac-
tive summarization; since the summary is shorter
than the input text, their approach puts multiple
end-to-sequence (EOS) tokens at the end of the
sentence, and thus is unable to utilize the strong
input–output correspondence in the summarization
task. Yang et al. (2021) apply auxiliary part-of-
speech (POS) loss and Qi et al. (2021) explore
pretraining strategies for encoder–decoder non-
autoregressive summarization. All these studies
concern supervised summarization, while our pa-
per focuses on unsupervised summarization. We
adopt CTC training in our encoder-only architec-
ture, allowing blank tokens to better align input
and output words, which is more appropriate for
summarization.

5 Conclusion

In this work, we propose a non-autoregressive un-
supervised summarization model (NAUS), where
we further propose a length-control decoding al-
gorithm based on dynamic programming. Exper-
iments show that NAUS not only archives state-
of-the-art unsupervised performance on Gigaword
headline generation and DUC2004 datasets, but
also is much more efﬁcient than search methods
and autoregressive models. Appendices present ad-
ditional analyses and length-transfer experiments.
Limitation and Future Work. Our paper fo-
cuses on unsupervised summarization due to the
importance of low-data applications. One limita-
tion is that we have not obtained rigorous empirical
results for supervised summarization, where the
developed model may also work. This is because
previous supervised summarization studies lack
explicit categorization of summary lengths (Yang
et al., 2020; Qi et al., 2021), making comparisons
unfair and problematic (Schumann et al., 2020).
Such an observation is also evidenced by Su et al.
(2021), where the same model may differ by a few
ROUGE points when generating summaries of dif-
ferent lengths. Nevertheless, we have compared
with Su et al. (2021) in our setting and show the su-
periority of the NAUS under fair comparison. We

plan to explore supervised summarization in future
work after we establish a rigorous experimental
setup, which is beyond the scope of this paper.

Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK
Li, and Richard Socher. 2018. Non-autoregressive
neural machine translation. In ICLR.

6 Acknowledgments

We thank Raphael Schumann for providing valu-
able suggestions on the work. We also thank the
Action Editor and reviewers for their comments dur-
ing ACL Rolling Review. The research is supported
in part by the Natural Sciences and Engineering
Research Council of Canada (NSERC) under grant
No. RGPIN2020-04465, the Amii Fellow Program,
the Canada CIFAR AI Chair Program, a UAHJIC
project, a donation from DeepMind, and Compute
Canada (www.computecanada.ca).

References

Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava,
Xilun Chen, Luke Zettlemoyer, and Sonal Gupta.
2021. Muppet: Massive multi-task representations
with pre-ﬁnetuning. In EMNLP, page 5799–5811.

Armen Aghajanyan, Akshat Shrivastava, Anchit Gupta,
Naman Goyal, Luke Zettlemoyer, and Sonal Gupta.
2020. Better ﬁne-tuning by reducing representa-
tional collapse. In ICLR.

Christos Baziotis, Ion Androutsopoulos, Ioannis Kon-
stas, and Alexandros Potamianos. 2019. SEQ3: Dif-
ferentiable sequence-to-sequence-to-sequence au-
toencoder
for unsupervised abstractive sentence
compression. In NAACL-HLT, pages 673–681.

William Chan, Chitwan Saharia, Geoffrey Hinton, Mo-
hammad Norouzi, and Navdeep Jaitly. 2020.
Im-
puter: Sequence modelling via imputation and dy-
namic programming. In ICML, pages 1403–1413.

Bonnie Dorr, David Zajic, and Richard Schwartz. 2003.
Hedge trimmer: A parse-and-trim approach to head-
line generation. In Proc. HLT-NAACL 03 Text Sum-
marization Workshop, pages 1–8.

Thibault Févry and Jason Phang. 2018. Unsuper-
vised sentence compression using denoising auto-
encoders. In CoNLL, pages 413–422.

Katja Filippova, Enrique Alfonseca, Carlos A. Col-
menares, Lukasz Kaiser, and Oriol Vinyals. 2015.
Sentence compression by deletion with LSTMs. In
EMNLP, pages 360–368.

David Graff, Junbo Kong, Ke Chen, and Kazuaki
Maeda. 2003. English Gigaword. Linguistic Data
Consortium, Philadelphia.

Alex Graves, Santiago Fernández, Faustino Gomez,
Connectionist
and Jürgen Schmidhuber. 2006.
temporal classiﬁcation: Labelling unsegmented se-
quence data with recurrent neural networks.
In
ICML, page 369–376.

Jiatao Gu and Xiang Kong. 2021.

Fully non-
autoregressive neural machine translation: tricks of
the trade. In Findings of ACL-IJCNLP, pages 120–
133.

Junliang Guo, Xu Tan, Linli Xu, Tao Qin, Enhong
Chen, and Tie-Yan Liu. 2020. Fine-tuning by cur-
riculum learning for non-autoregressive neural ma-
chine translation. In AAAI, pages 7839–7846.

Zhanying He, Chun Chen, Jiajun Bu, Can Wang, Li-
jun Zhang, Deng Cai, and Xiaofei He. 2012. Doc-
ument summarization based on data reconstruction.
In AAAI, pages 620–626.

Chenyang Huang, Hao Zhou, Osmar R Zaïane, Lili
Mou, and Lei Li. 2022. Non-autoregressive transla-
tion with layer-wise prediction and deep supervision.
In AAAI.

Ruipeng Jia, Yanan Cao, Haichao Shi, Fang Fang,
Pengfei Yin, and Shi Wang. 2021. Flexible non-
autoregressive extractive summarization with thresh-
old: How to extract a non-ﬁxed number of summary
sentences. In AAAI, pages 13134–13142.

Shailza Jolly, Zi Xuan Zhang, Andreas Dengel, and Lili
Mou. 2022. Search and learn: Improving semantic
coverage for data-to-text generation. In AAAI.

Julia Kreutzer, Stefan Riezler, and Carolin Lawrence.
2021. Ofﬂine reinforcement learning from human
feedback in real-world sequence-to-sequence tasks.
In Proc. Workshop on Structured Prediction for NLP,
pages 37–43.

Dhruv Kumar, Lili Mou, Lukasz Golab, and Olga Vech-
tomova. 2020. Iterative edit-based unsupervised sen-
tence simpliﬁcation. In ACL, pages 7918–7928.

Jason Lee, Elman Mansimov, and Kyunghyun Cho.
Deterministic non-autoregressive neural
In

2018.
sequence modeling by iterative reﬁnement.
EMNLP, pages 1173–1182.

Jingjing Li, Zichao Li, Lili Mou, Xin Jiang, Michael
Lyu, and Irwin King. 2020. Unsupervised text gen-
eration by learning from search. In NeurIPS, pages
10820–10831.

Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Text Summariza-
tion Branches Out, pages 74–81.

Xianggen Liu, Pengyong Li, Fandong Meng, Hao
Zhou, Huasong Zhong, Jie Zhou, Lili Mou, and
Sen Song. 2021a. Simulated annealing for opti-
mization of graphs and sequences. Neurocomputing,
465:310–324.

Xianggen Liu, Lili Mou, Fandong Meng, Hao Zhou, Jie
Zhou, and Sen Song. 2020. Unsupervised paraphras-
ing by simulated annealing. In ACL, pages 302–312.

Yixin Liu, Zi-Yi Dou, and Pengfei Liu. 2021b. Ref-
In ACL,

Sum: Refactoring neural summarization.
pages 1437–1448.

Clara Meister, Ryan Cotterell, and Tim Vieira. 2020. If
beam search is the answer, what was the question?
In EMNLP, pages 2173–2185.

Yishu Miao and Phil Blunsom. 2016. Language as a
latent variable: Discrete generative models for sen-
tence compression. In EMNLP, pages 319–328.

Ani Nenkova, Sameer Maskey, and Yang Liu. 2011.
Automatic summarization. In ACL, pages 1–86.

Yaushian Wang and Hung-Yi Lee. 2018. Learning
to encode text as human-readable summaries using
generative adversarial networks. In EMNLP, pages
4187–4195.

Peter West, Ari Holtzman, Jan Buys, and Yejin
Choi. 2019. BottleSum: Unsupervised and self-
supervised sentence summarization using the infor-
In EMNLP-IJCNLP,
mation bottleneck principle.
pages 3752–3761.

Sam Wiseman, Stuart Shieber, and Alexander Rush.
2018. Learning neural templates for text generation.
In EMNLP, pages 3174–3187.

Mari Ostendorf, Vassilios V Digalakis, and Owen A
Kimball. 1996. From hmm’s to segment models:
A uniﬁed view of stochastic modeling for speech
recognition. IEEE TASLP, 4(5):360–378.

Kexin Yang, Wenqiang Lei, Dayiheng Liu, Weizhen Qi,
and Jiancheng Lv. 2021. POS-constrained parallel
decoding for non-autoregressive generation. In ACL-
IJCNLP, pages 5990–6000.

Paul Over and James Yen. 2004. An introduction to
DUC-2004: Intrinsic evaluation of generic news text
summarization systems. In Proc. the Document Un-
derstanding Conference.

Weizhen Qi, Yeyun Gong, Jian Jiao, Yu Yan, Weizhu
Chen, Dayiheng Liu, Kewen Tang, Houqiang Li,
Jiusheng Chen, Ruofei Zhang, Ming Zhou, and Nan
Duan. 2021. Bang: Bridging autoregressive and
non-autoregressive generation with large scale pre-
training. In ICML, pages 8630–8639.

Lihua Qian, Hao Zhou, Yu Bao, Mingxuan Wang, Lin
Qiu, Weinan Zhang, Yong Yu, and Lei Li. 2021.
Glancing transformer for non-autoregressive neural
machine translation. In ACL-IJCNLP, pages 1993–
2003.

Alexander M. Rush, Sumit Chopra, and Jason Weston.
2015. A neural attention model for abstractive sen-
tence summarization. In EMNLP, pages 379–389.

Ziyi Yang, Chenguang Zhu, Robert Gmyr, Michael
Zeng, Xuedong Huang, and Eric Darve. 2020. TED:
A pretrained unsupervised summarization model
In EMNLP,
with theme modeling and denoising.
pages 1865–1874.

David Zajic, Bonnie Dorr, and Richard Schwartz. 2004.
BBN/UMD at DUC-2004: Topiary. In Proc. HLT-
NAACL Document Understanding Workshop, pages
112–119.

Jingqing Zhang, Yao Zhao, Mohammad Saleh, and
Peter Liu. 2020. PEGASUS: Pre-training with ex-
tracted gap-sentences for abstractive summarization.
In ICML, pages 11328–11339.

Jiawei Zhou and Alexander Rush. 2019. Simple unsu-
pervised summarization by contextual matching. In
ACL, pages 5101–5106.

A Proof of Theorem 1

Chitwan Saharia, William Chan, Saurabh Saxena, and
Mohammad Norouzi. 2020. Non-autoregressive ma-
chine translation with latent alignments. In EMNLP,
pages 1098–1108.

Raphael Schumann, Lili Mou, Yao Lu, Olga Vechto-
mova, and Katja Markert. 2020. Discrete optimiza-
tion for unsupervised sentence summarization with
word-level extraction. In ACL, pages 5032–5042.

Theorem 1. (1) If repeating tokens are not merged,
then the proposed length-control algorithm with
beam size B = 1 ﬁnds the exact optimum BS,T
being the most probable length-T sentence given
by S prediction slots. (2) If we merge repeating
tokens predicted by CTC-trained models, the above
algorithm may not be exact.

Kaiqiang Song, Bingqing Wang, Zhe Feng, and Fei Liu.
2021. A new approach to overgenerating and scor-
In NAACL-HLT, pages
ing abstractive summaries.
1392–1404.

Yixuan Su, Deng Cai, Yan Wang, David Vandyke, Si-
mon Baker, Piji Li, and Nigel Collier. 2021. Non-
autoregressive text generation with pre-trained lan-
guage models. In EACL, pages 234–243.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In NIPS, pages 5998–6008.

Proof. [Part (1)] This part concerns a variant of our
decoding algorithm, which only removes the blank
token (cid:15) but does not merge consecutive repeated
tokens to a single word, i.e., Eqn. (10) is removed.
We denote this by Γ(cid:48), for example, Γ(cid:48)(a(cid:15)(cid:15)aabb(cid:15)) =
aaabb, as opposed to Γ(a(cid:15)(cid:15)aabb(cid:15)) = aab in our
algorithm. We now show that, based on Γ(cid:48), our
dynamic programming algorithm in §2.3 with beam
size B = 1 is an exact inference algorithm.

We deﬁne βs,t = maxb:|b|=s,|Γ(cid:48)(b)|=t P (b|x),
where | · | denotes the length of a sequence. In

other words, βs,t is the maximum probability of s
tokens that are reduced to t words.

According to the deﬁnition, we have

β1,0 = P (w1 = (cid:15)|x)
β1,1 = maxw1(cid:54)=(cid:15) P (w1|x)
βs,t = 0 for s > t

(13)

(14)

(15)

In (13), β1,0 refers to the probability of one to-
ken that is reduced to zero words, in which case
the ﬁrst predicted token can only be the blank to-
ken (cid:15), corresponding to Eqn. (9) with s = 1 and
t = 0. Likewise, β1,1 is the maximum probability
of one token that is reduced to one word. Thus,
it is the probability of the most probable non-(cid:15) to-
ken, corresponding to Eqn. (11) with s = 1 and
t = 0. Eqn. (15) asserts that fewer tokens cannot
be reduced to more words; it is used for mathe-
matical derivations, but need not to be explicitly
implemented in our algorithm in §2.3.

The recursion variable βs,t is computed by

βs,t = max

(cid:110)

βs−1,t · P (ws = (cid:15)|x),

βs−1,t−1 · maxws(cid:54)=(cid:15) P (ws|x)

(cid:111)

(16)
In other words, the variable βs,t can inherit βs−1,t
with a predicted blank token (cid:15), corresponding to
Eqn. (9); or it can inherit βs−1,t−1 with a predicted
non-(cid:15) token, corresponding to Eqn. (11). Specially,
if t = 0, then the second term has βs−1,−1 unde-
ﬁned, and thus is ignored in the max operation.

We need the max operator to take the higher
probability in the two cases, since βs,t is the max-
imum probability of s tokens being reduced to t
words. This corresponds to Eqn. (12) with beam
size B = 1.

To sum up, our inductive calculation guaran-
tees that βS,T is the exact maximum probability of
maxb:|b|=S,|Γ(cid:48)(b)|=T P (b|x) for the desired length
T with S generation slots; our algorithm (if not
merging repeating tokens) gives the correspond-
ing BS,T as argmax P (b|x) under the same con-
straints, concluding the proof of Part (1).

[Part (2)] CTC training merges consecutive re-
peated tokens to a single word, unless separated by
the blank token (cid:15) (Graves et al., 2006). Since our
model is trained by CTC, we should adopt this rule
in inference as well. We show in this part that our
algorithm, with beam size B = 1, may not yield
the exact optimum with an example in Table 4.

We consider generating a sentence of two words

Word
I
like
coding
(cid:15)

P (w1|x) P (w2|x)

0.39
0.4
0.1
0.11

0.1
0.9
0
0

Table 4: An example of predicted probabilities of two
generation slots, where we have a vocabulary of three
words and a blank token (cid:15).

from the two prediction slots, i.e., S = T = 2.
Apparently, the optimal sequence is “I like” with
probability 0.39 · 0.9 = 0.351. However, the al-
gorithm would predict B1,1 = {“like”} because
“like” is the most probably token in the ﬁrst slot.
Then, our algorithm will give B2,2 = {“like I”},
because it has to select a non-repeating token based
on Γ, yielding a non-optimal solution.

It is noted that, if we do not merge repeating
tokens as in Γ(cid:48), our algorithm will give the exact
optimum “like like” in the above example. This
shows that merging consecutive repeated tokens
requires the decoding algorithm to correct early
predictions, and thus, our dynamic programming
becomes an approximate inference. Nevertheless,
our algorithm is able to generate a sequence of
the desired length properly; its approximation hap-
pens only when the algorithm compares more rep-
etitions with fewer (cid:15)s versus more (cid:15)s with fewer
repetitions. Such approximation is further allevi-
ated by beam search in our dynamic programming.
Therefore, the proposed length-control algorithm is
better than truncating a longer sentence; especially,
our approach generates more ﬂuent and complete
sentences.

B Implementation Details

Our NAUS had a Transformer encoder as the ba-
sic structure, generally following the settings in
Vaswani et al. (2017): 6 encoder layers, each hav-
ing 8 attention heads. The dimension was 512 for
attention and 2048 for feed-forward modules.

Our training used a batch size of 4K tokens,
with a maximum of 200K updates. We used Adam
with β = (0.9, 0.98). In general, the learning rate
warmed up to 5e-4 in the ﬁrst 10K steps, and then
decayed to 1e-9 with the inverse square-root sched-
ule, except that we ﬁnd the maximum learning rate
of 1e-4 worked better for headline generation with
the summary length of 8. We set the (cid:96)2 weight de-
cay to 0.01. Our length-control decoding algorithm

Figure 3: Performance versus the number of training
samples in the setting of Group B, Table 1. Notice that
NAUS is trained by pseudo-groundtruth given by un-
supervised edit-based search (Schumann et al., 2020).
Thus, our approach is indeed unsupervised.

had a beam size of 6. More details can be found in
our repository (Footnote 1).

Our NAUS training is based on Schumann et al.
(2020)’s prediction on the input of the Gigaword
headline generation training set. We show perfor-
mance against the number of training samples in
Figure 3. As seen, NAUS outperforms its search
teacher even with a small set of 0.1 million sam-
ples. The performance saturates as the number of
samples increases. Based on this analysis, we used
3 million samples from the 3.8 million Gigaword
training set to train our NAUS models.

Each reported number in Tables 1–3 were aver-
aged over 10 independent runs, whereas the results
in Table 7 (Appendix F) were based on a single run
due to the limited time.

C Analysis of Beam Search

As mentioned, our length-control decoding algo-
rithm involves beam search within its dynamic pro-
gramming, because the algorithm does not ﬁnd
the exact optimum when it merges repeating words.
We analyze the effect of the beam size in our length-
control algorithm.

In addition, we compare our approach with CTC
beam search (Graves et al., 2006).6 Typically, a
CTC-trained non-autoregressive model can be de-
coded either greedily or by beam search. The
greedy decoding ﬁnds the most probable token at
each step, i.e., w∗
i = argmaxwi P (wi|x), and re-
duces the tokens to a sentence by Γ(w1, · · · , wT ),
where T is the number of decoding steps.
The CTC beam search algorithm searches for
the most likely sentence by marginalizing all
token sequences that are reduced to y,
i.e.,
w:Γ(w)=y P (w|x).
argmaxy

(cid:80)

6Our implementation of CTC beam search is based on

https://github.com/parlance/ctcdecode

Figure 4: Comparing our length-control NAUS and the
truncated CTC beam search on the Gigaward headline
generation test set.

We show results in Figure 4, where we chose 10-
word Gigaword headline generation as the testbed
with our NAUS model (Group B, Table 1). Notice
that CTC beam search does not control the output
length, and for fair comparison, we truncated its
generated summaries. This also shows that our
novel decoding approach and CTC beam search
are distinct algorithms.

As seen in Figure 4a, the beam search does play
a role in our length-control algorithm. When the
beam enlarges from 1 to 6, the performance (or-
ange solid line) increases by 1.2 points in ∆R, the
difference of total ROUGE in comparison with
Schumann et al. (2020) under our replication (Row
10, Table 1). However, further increasing the beam
size does not yield additional performance gain.
This is consistent with previous literature in autore-
gressive generation (Meister et al., 2020), which
also suggests a beam size of 5–7 is the best in
their applications. In terms of the efﬁciency (Fig-
ure 4b), a larger beam size monotonically increases
the inference time. However, the overhead of beam
search is relatively small in our dynamic program-
ming, and thus we chose a beam size of 6 in our
experiments.

Our length-control algorithm signiﬁcantly out-
performs CTC beam search (dashed blue lines) in
terms of both ∆R and efﬁciency. Especially, CTC
beam search is three times slower, and degrades
more signiﬁcantly than our length-control decoding
when the beam size increases.

D Case Study

We show in Table 6 example summaries generated
by our NAUS with truncating and length-control
decoding, as well as the previous state-of-the-art
method (Schumann et al., 2020). We observe that
NAUS without length control generates slightly
longer summaries, and if truncated, the output may
be incomplete; by contrast, our length-control algo-

0.00.51.01.52.02.53.0Training Samples (million)3.01.50.01.53.0R11020Beam Size10123R(a)CTC Beam SearchLength Control11020Beam Size0.000.050.150.25Inference Time (s)(b)CTC Beam SearchLength ControlOverall quality

Completeness
& ﬂuency

Decoding
Truncate

Wins
Loses
Ties
18.67% 40.67% 40.67%
Length control 40.67% 40.67% 18.67%
24.67% 26.67% 48.67%
Length control 48.67% 26.67% 24.67%

Truncate

p-val

0.0004

0.0005

Table 5: Human evaluation comparing truncating and
length control for our NAUS model on 50 samples in
the Gigaword headline generation task. The results are
statistically signiﬁcant, where the p-value is given by a
one-sided binomial test.

rithm can generate a ﬂuent and complete sentence
of the desired length by dynamic programming.
Compared with Schumann et al. (2020), our NAUS
(length control) generates a more informative sum-
mary that includes the main clause (united nations
condemned), which also appears in the reference
summary.

E Human Evaluation

We conducted human evaluation with a focus on
truncating and length-control decodings. This is
because truncating may generate incomplete sen-
tences, which cannot be adequately evaluated by
automatic metrics as their ROUGE scores are close.
Speciﬁcally, we invited three human annotators
to compare the two decoding algorithms for NAUS
on 50 randomly selected samples, in the setting of
Group B, Table 1 (Gigaword headline generation
with a target length of 10). The annotation was
conducted in a pairwise manner in terms of overall
quality and ﬂuency/completeness; average results
(wins/loses/ties) are shown in Table 4. It should be
mentioned that our annotation was strictly blind:
the samples of two systems were presented in ran-
dom order and annotators did not know which sys-
tem generated a sample.

As seen, our length-control decoding algo-
rithm largely outperforms the truncating approach
in terms of both the overall quality and ﬂu-
ency/completeness. The results are statistically
signiﬁcant (p-values < 0.01) in a one-sided bino-
mial test. This veriﬁes that length-control decoding
is important for summarization, as truncating yields
incomplete sentences, which are inadequately re-
ﬂected by ROUGE scores.

F Length-Transfer Summary Generation

Input: the united nations condemned saturday an attack on
russian embassy employees in baghdad that claimed the life
of one russian and resulted in the kidnapping of four others
Reference: un condemns murder of russians in iraq with
annan comment
Schumann et al. (2020): attack on russian embassy in
baghdad claimed one in four
NAUS (truncate): an attack on russian embassy employees
in baghdad claimed in kidnapping of four others
NAUS (length control): united nations condemned attack
on russian embassy employees in baghdad

Table 6: Example summaries for Gigaword headline
generation. The gray words are truncated for fair com-
parison.

in machine learning that training and test samples
are independently identically distributed.

In this appendix, we show the performance of
length-transfer summary generation, where the pre-
diction has a different length from that of training.
We denote such a model by NAUSi→j, referring to
training with i words and testing for j words.

As seen in Groups A & B in Table 7, NAUS
with length transfer is slightly worse than NAUS
trained on the correct length, which is understand-
able. Nevertheless, length-transfer decoding still
outperforms the search teacher and other baselines.
Moreover, we consider the third setting in Schu-
mann et al. (2020), where the target length is 50%
of the input. Since it takes time to obtain pseudo-
groundtruths given by the edit-based search, we
would directly transfer already trained NAUS mod-
els to this setting by our length-control decoding.
Results are shown in Group C, Table 7. We ob-
serve NASU10→50% is better than NASU8→50%,
which makes much sense because the latter has
a larger gap during transfer. Remarkably, both
NASU8→50% and NASU10→50% outperform Schu-
mann et al. (2020) and other baselines, achieving
new state-of-the-art unsupervised performance on
this setting as well.

We further compare with Su et al. (2021), who
use a length penalty to encourage short summaries.
However, their length control works in the statisti-
cal sense but may fail for individual samples. More-
over, such a soft length penalty cannot generate
longer summaries than trained. Even in the setting
of 10 → 8, their generates summaries are slightly
longer than required, while the performance de-
grades much more considerably than NAUS.

In the main paper, we present results where our
NAUS is trained on search outputs (Schumann
et al., 2020) that have the same length as the infer-
ence target. This follows the common assumption

These results show that our novel length-control
decoding algorithm is not only effective when gen-
erating summaries of similar length to the train-
ing targets, but also generalizes well to different

Group

Group A
(desired length 8)

Group B
(desired length 10)

Group C
(desired length
50% of the input)

#

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27

Baseline

Search

Learn from
search

Baseline

Search

Learn from
search

Baseline

Search

Learn from
search

Approach

Lead (8 words)†
Schumann et al. (2020)†
Our replication
Su et al. (2021)8→8
Su et al. (2021)10→8
NAUS (truncate)
NAUS8→8
NAUS10→8
Lead (10 words)†
Wang and Lee (2018)†
Zhou and Rush (2019)†
Schumann et al. (2020)†
Our replication
Su et al. (2021)8→10
Su et al. (2021)10→10
NAUS (truncate)
NAUS8→10
NAUS10→10
Lead (50% words)†
Févry and Phang (2018)†
Baziotis et al. (2019)†
Schumann et al. (2020)†
Our replication
Su et al. (2021)8→50%
Su et al. (2021)10→50%
NAUS8→50%
NAUS10→50%

Len

7.9
7.9
7.9
7.7
8.4
7.8
7.8
7.9
9.8
10.8
9.3
9.8
9.8
–
9.4
9.8
9.9
9.8
14.6
14.8
15.1
14.9
14.9
–
–
14.9
14.9

R-1
21.39
26.32
26.17
26.88
25.71
27.27
27.94
27.12
23.03
27.29
26.48
27.52
27.35
–
27.86
28.24
28.32
28.55
24.97
23.16
24.70
27.05
27.03
–
–
28.39
28.53

ROUGE F1
R-L
R-2
20.03
7.42
24.19
9.63
9.69
24.10
24.54
9.37
23.65
8.94
24.96
9.49
25.50
9.24
24.86
9.08
21.29
7.95
24.59
10.01
24.41
10.05
10.27
24.91
24.87
10.25
–
–
25.51
9.88
25.40
10.04
25.46
9.58
25.78
9.97
22.43
8.65
20.11
5.93
22.41
7.97
23.89
9.75
23.79
9.81
–
–
–
–
24.94
9.78
25.10
9.88

∆R
-11.12
0.18
0
0.83
-1.84
1.76
2.73
1.10
-10.2
-0.58
-1.53
0.23
0
–
0.78
1.21
0.89
1.83
-4.58
-11.43
-5.55
0.06
0
–
–
2.48
2.88

Inf.Time Speedup

–
–
6.846
0.017
0.018
0.005

0.041

–
–
–
–
9.217
–
0.020
0.005

0.044

–
–
–
–
17.462
–
–

–
–
1x
403x
380x
1369x

167x

–
–
–
–
1x
–
461x
1843x

210x

–
–
–
–
1x
–
–

0.052

336x

Table 7: Analysis of length-transfer summary generation. A subscript i → j (or j%) refers to a model trained with
i words and tested for j (or j%) words. Len: Average length of predicted summaries. R-1, R-2, R-L: ROUGE-1,
ROUGE-2, ROUGE-L. ∆R: The difference of total ROUGE (sum of R-1, R-2, and R-L) in comparison with the
(previous) state-of-the-art model (Schumann et al., 2020) under replication. Inf.Time: Average inference time in
seconds for one sample on an i9-9940X CPU and a RTX6000 GPU. Speedup: Relative to Schumann et al. (2020).
†Results quoted from previous papers; others are given by our experiments. Su et al. (2021)’s approach has a soft
length penalty to encourage short output, but cannot generate longer summaries than trained.

desired summary lengths without re-training. In
general, our NAUS is an effective and efﬁcient un-
supervised summarization system with the ability
of explicit length control.

