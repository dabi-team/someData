9
1
0
2

g
u
A
8
1

]

O
C

.
t
a
t
s
[

1
v
5
1
5
6
0
.
8
0
9
1
:
v
i
X
r
a

Computing Estimators of Dantzig Selector type via Column and
Constraint Generation

Rahul Mazumder∗ Stephen Wright† Andrew Zheng‡

August 20, 2019

Abstract

We consider a class of linear-programming based estimators in reconstructing a sparse signal
from linear measurements. Speciﬁc formulations of the reconstruction problem considered here
include Dantzig selector, basis pursuit (for the case in which the measurements contain no
errors), and the fused Dantzig selector (for the case in which the underlying signal is piecewise
constant). In spite of being estimators central to sparse signal processing and machine learning,
solving these linear programming problems for large scale instances remains a challenging task,
thereby limiting their usage in practice. We show that classic constraint- and column-generation
techniques from large scale linear programming, when used in conjunction with a commercial
implementation of the simplex method, and initialized with the solution from a closely-related
Lasso formulation, yields solutions with high eﬃciency in many settings.

1 Introduction

We consider the prototypical problem of sparse signal recovery from linear measurements [9, 28, 10]:
given a model matrix X ∈ Rn×p with n samples and p features, response y ∈ Rn generated via
the model y = Xβ0 + (cid:15), where, β0 ∈ Rp is sparse (that is, has few nonzero entries) and the errors
are i.i.d. Gaussian with mean zero and variance σ2 (i.e., (cid:15) ∼ N (0, σ2I)). We consider the case in
which the number of variables is much larger than the number of samples (p (cid:29) n) and our task is
to estimate β0 from (y, X), exploiting the knowledge that β0 is sparse.

We assume throughout that the columns of X have been standardized to have mean zero and unit
(cid:96)2-norm. The (cid:96)1-norm (cid:107)β(cid:107)1 is often used as a convex surrogate to the cardinality of β, which is a
count of the number of nonzero elements in β. The celebrated Dantzig Selector [9] approximates
β0 by minimizing (cid:107)β(cid:107)1 subject to a constraint on the maximal absolute correlation between the
features and the vector of residuals (given by r := y − Xβ). The optimization problem of this

∗MIT Sloan School of Management, Operations Research Center and Center for Statistics, MIT.
†Department of Computer Sciences, University of Wisconsin-Madison.
‡Operations Research Center, MIT.

1

 
 
 
 
 
 
recovery problem is as follows:

((cid:96)1-DS)

minimize
β

(cid:107)β(cid:107)1

s.t. (cid:107)X T (y − Xβ)(cid:107)∞ ≤ λ,

(1)

where λ > 0 controls the data-ﬁdelity term. Ideally, the value of λ should be such that the unknown
signal vector β0 is feasible, that is, (cid:107)X T (cid:15)(cid:107)∞ ≤ λ holds (with high probability, say). The constraint
in (1) can be interpreted as the (cid:96)∞-norm of the gradient of the least squares loss 1
2 (cid:107)y − Xβ(cid:107)2
2.
An appealing property of the Dantzig Selector estimator is that it is invariant under orthogonal
transformations of (y, X).

Problem (1) can be reformulated as a linear program (LP), and thus solved via standard LP
algorithms and software (for example, commercial solvers like Gurobi and Cplex) for instances of
moderate size. As pointed out in [4], eﬃcient algorithms for (cid:96)1-DS are scarce:

“...Typical modern solvers rely on interior-point methods which are somewhat problem-
atic for large scale problems, since they do not scale well with size.”

Although important progress has been made on algorithms for (cid:96)1-DS in subsequent years (see,
for example, [4, 23, 30, 25, 22]), large-scale instances of (1) (with p of a million or more) still
cannot be solved. The main goal of our work is to improve our current toolkit for solving (cid:96)1-DS
and related problems, bringing to bear some underutilized classical tools from large scale linear
programming.

The Dantzig Selector is closely related to the Lasso [28], which combines a least squares data-
ﬁdelity term with an (cid:96)1-norm penalty on β. While the Lasso and Dantzig Selectors yield diﬀerent
solutions [15], for suitably chosen regularization parameters, they both lead to estimators with
similar statistical properties in terms of estimation error, under suitable assumptions on X, β0,
and σ (see [6]). A version of the Lasso that has the same objective as (1) is

minimize
β

(cid:107)β(cid:107)1 s.t. (cid:107)y − Xβ(cid:107)2 ≤ δ,

(2)

where δ ≥ 0 is a parameter that places a budget on the data ﬁdelity, deﬁned here as the (cid:96)2-norm of
the residuals. The explicit constraint on data ﬁdelity makes this formulation appealing, but it poses
computational challenges [4] because of the diﬃculty of projecting onto the constraint. The follow-
ing alternative, unconstrained version has become the most popular formulation of Lasso:

(Lasso) minimize

β

1
2

(cid:107)y − Xβ(cid:107)2

2 + λ(cid:107)β(cid:107)1

(3)

where λ ≥ 0 is a regularization parameter that controls the (cid:96)1-norm of β. (This is the formulation
that we refer to as “Lasso” in the remainder of the paper.) There are several highly eﬃcient
algorithms for (3) (see, for example, [3, 17, 31]), making it an extremely eﬀective tool in the
context of sparse learning based on (cid:96)1-minimization.

2

1.1 Algorithms for Lasso and Dantzig Selector: Fixing the Gap in Perfor-

mance.

Common algorithms for solving (3) are based on proximal gradient methods [3, 31], coordinate
descent [17], or homotopy methods [14]. Several eﬃcient implementations of these methods are
available publicly. There are key diﬀerences between the Lasso and (cid:96)1-DS in terms of computational
properties and associated solvers: (cid:96)1-DS is essentially an LP whereas Lasso is a convex quadratic
program (QP). Although LPs are generally thought to be easier to solve than QPs of a similar size,
the Lasso QP can be solved with remarkable eﬃciency, at least when β is quite sparse and the
matrix X has felicitous properties.

While ﬁrst order optimization algorithms [4, 7] have also led to good algorithms to solve (cid:96)1-DS, they
are still much slower than Lasso. To illustrate, to compute a path of 100 solutions for a problem
with n = 200, p = 12, 000, glmnet [17] takes 0.24 seconds with minimal memory requirement on a
modest desktop computer. On the other hand, for the same dataset (and machine), solving (cid:96)1-DS
for a path of 100 λ values by the parametric simplex method of [25] takes several minutes and
requires at least 10GB of memory. The software package flare [22], based on the Alternating
Direction Method of Multipliers (ADMM) [7] has prohibitive memory requirements and would not
run on a modest desktop machine. The diﬀerences between solvers grow with problem size. As
a consequence of the diﬃculties of solving the LP formulation, (cid:96)1-DS remains somewhat under-
utilized, in spite of its excellent statistical properties. This paper seeks to address the striking
diﬀerence in computational performance between the Lasso and the Dantzig Selector by proposing
eﬃcient methods for the latter. We make use of classical techniques from optimization: column
generation and constraint generation. These techniques were ﬁrst proposed as early as 1958 [16, 11]
in the context of solving large scale LPs but, to our knowledge, have not been applied to (cid:96)1-DS or
its relatives discussed below.

Our approach exploits the sparsity that is typically present in the solution of (cid:96)1-DS: at optimality,
an optimal β will have few nonzeros. If we can identify the nonzero components eﬃciently, we may
avoid having to solve a full LP formulation that includes all p components of β. Column generation
starts by selecting a subset of components in β and solving a reduced version of (1) that includes
only these components (that is, it ﬁxes the components of β that are not selected to zero). If the
optimality conditions for the full problem are not satisﬁed by the solution of the reduced LP, more
components of β are added to the formulation in a controlled way, and a new reduced LP is solved,
using the previous solution as a warm start. The process is repeated until optimality for the full
LP is obtained. Whenever new components are added to the reduced LP, we add new columns to
the constraint matrix, hence the name column generation.

We make use too of another key property of (cid:96)1-DS: redundancy of the constraints in (1). Typically,
the number of components of X T (y − Xβ) that are at their bounds of −λ and λ at the solution is
small, of the same order as the number of nonzero components of β at the solution. This observation
suggests a procedure in which we solve a reduced LP with just a subset of constraints enforced.
We then check the constraints that were not included in this formulation to see if they are violated
by the solution of the reduced LP. If so, we add (some of) these violated constraints to the LP
formulation, and solve the modiﬁed reduced LP. This process is repeated until a solution of the

3

original problem is obtained. This procedure is known as constraint generation.

While column generation and constraint generation are commonly used as separate entities to solve
large scale LPs, it makes sense to use them jointly in solving (cid:96)1-DS, and a combination of the
two strategies can be implemented with little complication. The procedures can beneﬁt from good
initial guesses of the nonzero components of β and the active constraint set for (1). We use eﬃcient
Lasso solvers to obtain these initial guesses.

1.2 Other Examples

Several other examples of sparse linear models are also amenable to column and constraint gen-
eration techniques. These include basis pursuit denoising [10] and a Dantzig selector version of
one-dimensional total variation denoising (also known as fused Lasso) [24, 29]. Each of these
problems can be formulated as a linear program task and, like (cid:96)1-DS, they are computationally
challenging.

Basis Pursuit. The noiseless version of (cid:96)1 sparse approximation, popularly known as Basis Pur-
suit [10], is given by the following optimization problem:

minimize
β

(cid:107)β(cid:107)1

s.t.

y = Xβ,

(4)

which can be formulated as an LP. This problem can be interpreted as a limiting version of (3)
as λ → 0+. It may be tempting to solve (3) for a small value of λ ≈ 0 (possibly with warm-start
continuation) to obtain a solution to (4). However, this approach is often ineﬃcient in practice,
because obtaining accurate solutions to the Lasso becomes increasingly expensive as λ ↓ 0. It has
been pointed out in [13] that solving (4) to high accuracy using existing convex optimization solvers
or specialized iterative algorithms is a daunting task, for large instances. Our own experiments
show that current solvers based on ADMM fail to solve (4) for p ≥ 104 (with n < 1000), while our
proposed approach, described below, solves problems with p ≈ 4 × 105 within 3-4 minutes. Our
approach relies on column generation, exploiting the familiar observation that the solution β of (4)
is sparse.

Fused Dantzig Selector. The Fused Lasso [29] or the total variation penalty [27] is a commonly
used (cid:96)1-based penalty that encourages the solution to be (approximately) piecewise constant. The
unconstrained formulation of this problem is

minimize
β

1
2

(cid:107)y − Xβ(cid:107)2

2 + λ(cid:107)D(0)β(cid:107)1,

(5)

where λ ≥ 0 is a regularization parameter and D(0) ∈ R(p−1)×p is the ﬁrst order diﬀerence operator
matrix, deﬁned by

D(0)β = (β2 − β1, β3 − β2, . . . , βp − βp−1)T ,

(6)

4

which represents diﬀerences between successive components of β. As we show in Section 2.4, (5)
can be expressed as a Lasso problem of the standard form (3), with a modiﬁed model matrix
˜X ∈ Rn×p−1 and response ˜y ∈ Rn 1. This suggests a natural Dantzig Selector analog of the fused
Lasso problem:

minimize
α∈Rp−1

(cid:107)α(cid:107)1 s.t. (cid:107) ˜X T (˜y − ˜Xα)(cid:107)∞ ≤ λ,

(7)

a formulation that is amenable to the column and constraint generation methods developed in this
paper. In the special case of X = I, the problem has additional structure that we can exploit to
solve instances with p ≈ 5 × 105, well beyond the capabilities of alternative methods.

1.3 Related work and Contributions

The Dantzig Selector formulations presented here — (1), (4), and (7) — can all be expressed as LPs
and solved with interior point methods or simplex-based methods, as implemented in commercial
solvers 2 (Gurobi, Cplex, Mosek, XPress, etc) or open-source codes (GLKP, lpsolve, among others).
Specialized implementations for (cid:96)1-DS and Basis Pursuit have been investigated for several years.
An interior point method was used in [9], a ﬁrst-order method for a regularized version of (cid:96)1-DS was
described in [4]3, and methods based on ADMM were discussed in [23, 30, 7]. Using a homotopy
continuation approach, [20] extend the framework of LARS [14] to ﬁnd the solution path to (cid:96)1-DS,
which is piecewise linear in λ. Homotopy continuation methods applicable to (cid:96)1-DS have also been
proposed by [2, 8, 26], but these works do not appear to use column and constraint generation
methods, which are the main focus of our work. For the (cid:96)1-DS problem, the algorithms of [2, 8, 26]
compute the full p × p matrix X T X at the outset. This operation is memory-intensive, so these
approaches can handle values of p only up to a few thousands on a modest desktop computer.

Our methods solve the problems (1), (4), and (7) at a single value of the regularization parameter,
but they can be extended to solve these problems on a grid of regularization parameters via a warm
start continuation strategy. We show that the classical tools of column and constraint generation
can be eﬀective in solving large-scale instances of these problems. Our work is related to the
proposal of [12] who explored column and constraint generation to solve regularized linear SVM
problems (with a hinge loss) that can be expressed as LPs. (Regularizers considered in [12] include
the (cid:96)1-norm, group (cid:96)1-norm, and the Slope penalty.) Our Dantzig Selector problems have structural
properties diﬀerent from the SVM problems. They also have the unique advantage that they can be
initialized using Lasso. This fact plays an important role in the practical computational eﬃciency
of our approaches.

Our methods are based on the simplex algorithm, which is better at making use of the available
warm-start information than interior-point methods. A memory-friendly version of Gurobi’s sim-
plex solver, applied to an LP formulation of (1) that avoids formation of X T X by using auxiliary

1We deﬁne (˜y, ˜X) as follows. Deﬁne D = [eT

1 ; D(0)] ∈ Rp×p, where e1 = (1, 0, 0, . . . , 0)T , and deﬁne H = D−1. For
any A ⊂ {1, 2, . . . , p}, let HA be the submatrix of H containing the columns indexed by A, and let PA denote the
projection operator onto the column space of HA. We set ˜y := (I − PA)y and ˜X := (I − PA)XHB, with A = {1} and
B = {2, . . . , p}.

2Commercial solvers such as Gurobi, Cplex, Mosek are free for academic use.
3The authors add a small ridge penalty to the objective and optimize the dual via gradient methods

5

variables, works well for (cid:96)1-DS with n in the hundreds and p in the thousands. In fact, this approach
can be faster than some specialized algorithms [26, 4, 23]. We show simplex performance can be
improved substantially by using column and constraint generation when p (cid:29) n, for problems in
which the underlying solution is suﬃciently sparse. We refer to our framework as Dantzig-Linear
Programming (DantzigLP for short). Because we use a simplex engine as the underlying solution,
a primal-dual solution is available at optimality.
If we decide to terminate the algorithm early
due to computational budget constraints, our framework delivers a certiﬁcate of suboptimality.
DantzigLP can solve instances of the (cid:96)1-DS problem with n ≈ 103 and p ≈ 106; Basis Pursuit
with n ≈ 103 and p ≈ 105; and Fused Lasso with n = p ≈ 106; all within a few minutes and
with reasonable memory requirements. To our knowledge, problems of this size are beyond the
capabilities of current solvers. A Julia implementations of our DantzigLP framework can be found
at https://github.com/atzheng/DantzigLP.

Notation. We denote [n] := {1, 2, . . . n}. The identity matrix is denoted by I (with dimension
understood from the context). When operating on vector-valued operands u, v ∈ Rn, the inequality
u ≤ v denotes elementwise comparison. For any matrix X ∈ Rn×p and index sets I ⊂ [n] and
J ⊂ [p], we denote by XI,J the |I| × |J| submatrix of X that consists of the rows of X indexed by
I and the columns of X indexed by J. The notation X∗,J denotes a submatrix consisting of all
rows of X but only the columns indexed by J (A similar convention applies to XI,∗). For a vector
v ∈ Rn and a set S ⊂ [n], vS denotes the subvector of v restricted to the indices in S. The notation
1 denotes the vector (1, 1, . . . , 1)T , whose length is deﬁned by the context.

2 Methodology

2.1 Column and Constraint Generation for Large-Scale LP

Column generation [16, 11, 5] is a classical tool to solve large scale LPs with a large number of
variables and a relatively small number of constraints, when we anticipate an optimal solution with
few nonzero coeﬃcients. The basic idea is simple: we solve a small LP involving just a subset of
columns, and incrementally add columns into the model, re-solving the LP after each addition, until
optimality conditions for the original problem are satisﬁed. Constraint generation is used when the
number of constraints is large relative to the number of variables, when we expect a relatively small
subset of the constraints to be active at optimality.

For the sake of completeness, we provide an overview of these techniques in this section, referring
the reader to [5] for a more detailed treatment.

Given problem data A ∈ Rm×n, b ∈ Rm and c ∈ Rn with decision variable x ∈ Rn, we consider the
LP (Primal-Full), whose dual (Dual-Full) has decision variable v ∈ Rm. We assume that A has
full rank.

6

minimize
x

cT x

s.t.

Ax ≥ b

x ≥ 0

maximize
v

bT v

(Primal-Full)

s.t.

AT v ≤ c
v ≥ 0.

(Dual-Full)

We will assume that (Primal-Full) has a ﬁnite optimal solution. By LP duality theory, the dual
also has a solution with the same optimal objective value.

The solutions of (Primal-Full) and (Dual-Full) can be derived from the solutions to reduced
problems of the following form, for some index sets I ⊂ [m] and J ⊂ [n]:

minimize
xJ

cT
J xJ

s.t.

(AI,J )xJ ≥ bI
xJ ≥ 0,

maximize
vI

bT
I vI

(Primal(I, J))

s.t.

I,J vI ≤ cJ

AT
vI ≥ 0.

(Dual(I, J))

The subsets I and J are not known in advance; the simplex method can be viewed as a search
for these subsets in which typically one element is changed at each iteration. Suﬃcient condi-
tions for the solutions xJ of (Primal(I, J)) and vI of (Dual(I, J)) to be extendible to solutions of
(Primal-Full) and (Dual-Full) are that:

Ai,J xJ ≥ bi,

for all i ∈ [m] \ I; AT

I,jvI ≤ cj,

for all j ∈ [n] \ J.

(8)

If these conditions are satisﬁed, we obtain solutions x∗ and v∗ of (Primal-Full) and (Dual-Full),
respectively, by setting x∗

J c = 0, and v∗

I = vI and v∗

J = xJ and x∗

I c = 0.

Column and constraint generation are techniques for systematically expanding the sets I and J
until the optimality conditions (8) are satisﬁed by the solutions of (Primal(I, J)) and (Dual(I, J)).
At each iteration, we solve a reduced problem of this form, then seek indices i and j for which
the conditions (8) are violated. Some of these indices are added to the sets I and J, and the new
(slightly larger) versions of (Primal(I, J)) and (Dual(I, J)) are solved using a variant of the simplex
method, typically warm-started from the previous reduced problem. An outline of the approach is
shown in Algorithm 1.

Algorithm 1 Constraint and Column Generation to solve (Primal-Full) and (Dual-Full)

Initialize I ⊂ [m], J ⊂ [n];
repeat

Solve (Primal(I, J)) and (Dual(I, J)) to obtain xJ and vI ;
Choose Is ⊂ Iv := {i ∈ [m] \ I : Ai,J xJ < bi};
Choose Js ⊂ Jv := {j ∈ [n] \ J : AT
I,jvI > cj};
Set I ← I ∪ Is and J ← J ∪ Js;

until Iv = ∅ and Jv = ∅;
Set xJ c = 0 and vI c = 0 and terminate.

Many variants are possible within this framework. One could deﬁne the sets Is and Js to contain

7

only the smallest valid index, or the most-violated index. More commonly, Is and Js are chosen
to have cardinality greater than 1 where possible. In large-scale problems (analogous to partial
pricing in the simplex method), not all checks in (8) are even performed. When A is too large to
store in memory, for example, we can stream columns of A to calculate the quantities AT
I,jvI − cj
until enough have been calculated to deﬁne Jv.

When Algorithm 1 is implemented with I = [m] but J a strict subset of [n], it reduces to column
generation. (In this case, Is and Iv are null at every iteration.) Similarly, when I is a strict subset
of [m] but J = [n], Algorithm 1 reduces to constraint generation.

The success of constraint and column generation hinges on the ability to generate initial guesses
for I and J that requires few additional iterations of Algorithm 1 to identify the solutions of
(Primal-Full) and (Dual-Full).

The DantzigLP Framework. Combining column and constraint generation LP techniques with
methods for ﬁnding good initializations for the initial column and constraint sets I and J, we develop
DantzigLP, a general framework for solving large-scale versions of the the Dantzig Selector-type
problems described in Section 1. Initializations for I and/or J are obtained typically by solving the
Lasso variant of a given problem. This basic approach can be tailored to a large range of problems,
and in many cases the problem structure admits fast algorithms for both initialization and column
and constraint generation.

All of the Dantzig-type problems described in Section 1 (except for Basis Pursuit) have a tunable
regularization parameter λ. Practitioners often wish to compute the estimator for a grid of λ values
speciﬁed a-priori. DantzigLP makes this process eﬃcient by leveraging a simplex-based solver’s
warm start capabilities. Given the solution for one value of λ, DantzigLP can eﬃciently ﬁnd the
solution for its neighboring value of λ (within the column and constraint generation framework).
Repeating this process yields a path of solutions.

2.2 The Dantzig Selector

We show how the DantzigLP framework applies to (cid:96)1-DS. We present an LP formulation for
(cid:96)1-DS—the primal (9) and its corresponding dual (10) are as follows:

minimize
β+,β−,r

(β+

i + β−
i )

(cid:88)

i∈[p]

s.t. − 1λ ≤ X T r ≤ λ1
r = y − X(β+ − β−)
β+, β− ≥ 0

(9)

maximize
ν+,ν−,α

(cid:88)

−

λ(ν+

i + ν−

i ) − αT y

i∈[p]
s.t. − 1 ≤ X T α ≤ 1

(10)

X(ν+ − ν−) + α = 0
ν+, ν− ≥ 0.

The primal problem (9) has decision variables β+, β− ∈ Rp denoting the positive and negative parts
of β, and r ∈ Rn corresponding to the residual vector. The dual variables ν+, ν− ∈ Rp correspond
to the inequality constraints −λ ≤ X T r and X T r ≤ λ respectively, and α corresponds to the

8

equality constraint r = y − X(β+ − β−). At optimality, the following complementarity conditions
hold:

ν+ ◦ (X T α − λ1) = 0

and ν− ◦ (X (cid:48)α + λ1) = 0,

where, “◦” denotes componentwise multiplication. Therefore, X T
∗,ir <
λ =⇒ ν−
i = 0. Formulation (9) does not require computation and storage of the memory-intensive
p × p matrix X T X; this is avoided by introducing auxiliary variable r. Moreover, the related Lasso
problem (3) gives us reason to expect that this problem is a good candidate for both constraint and
column generation. Optimality conditions for solution βL of (3) can be written as follows:

i = 0 and −X T

∗,ir < λ =⇒ ν+

rL := y − XβL, X T

∗,jrL ∈






{λ}

[−λ, λ]

{−λ}

if βL
if βL
if βL

j > 0
j = 0
j < 0.

(11)

This suggests that, for the Lasso solution βL at least, the number of active constraints in (9) is
similar to the number of nonzero components in βL, which is typically small. If the solution of the
Dantzig selector has similar properties to the Lasso solution, then we would expect both the number
of nonzero components in the solution of (9) and the number of active constraints to be small relative
to the dimensions of the problem. (The papers [20] and [1] demonstrate conditions under which
the Dantzig and Lasso solution paths, traced as functions of the regularization parameter λ, are in
fact identical.)

For a subset J ⊂ [p] of columns of X and I ⊂ [p] of rows of X (note that I and J need not be the
same), we deﬁne a reduced column and constraint version of (9) as follows:

minimize
β+, β−, r

(cid:88)

(cid:16)

(cid:17)

j + β−
β+
j

J − β−

J ) = r

j∈J
s.t. y − X∗,J (β+
(X∗,I )T r ≤ λ1
(X∗,I )T r ≥ −λ1
β+, β− ≥ 0.

(DS(I, J))

Our constraint and column generation strategy for solving (cid:96)1-DS solves problems of this form at
each iteration, initializing I and J from the Lasso solution, and alternately expanding I and J
until a solution of (cid:96)1-DS is identiﬁed. We initialize J to be the subset of components j ∈ [p] for
which βj (cid:54)= 0, while I is the set of indices i ∈ [p] such that |X T
∗,ir| = λ. The strategy is speciﬁed as
Algorithm 2.

Note that each time we solve DS(I, J), we warm-start from the previous instance. The violation
checks that deﬁne Iv and Jv can be replaced by relaxed versions involving a tolerance (cid:15). That is,
we can deﬁne

Iv ← {i ∈ [p] \ I : |X T

∗,ir| > λ + (cid:15)},

Jv ← {j ∈ [p] \ J : |X T

∗,jα| > 1 + (cid:15)}.

(12)

9

Algorithm 2 Constraint and Column Generation to solve (cid:96)1-DS

Solve (3) to obtain initial I and J;
loop

Calculate Iv ← {i ∈ [p] \ I : |X T
if Iv (cid:54)= ∅ then

∗,ir| > λ};

Choose ∅ (cid:54)= Is ⊂ Iv; Set I ← I ∪ Is; Solve DS(I, J);

else

Using α from the dual solution of DS(I, J), calculate Jv ← {j ∈ [p] \ J : |X T
if Jv (cid:54)= ∅ then

∗,jα| > 1};

Choose ∅ (cid:54)= Js ⊂ Jv; Set J ← J ∪ Js; Solve DS(I, J);

else

terminate.

end if

end if
end loop

(We use (cid:15) = 10−4 in our experiments.)

Computing a path of solutions. We can extend Algorithm 3 to solve (cid:96)1-DS for a path of λ
values of the form Λ = {λi, i ∈ [k]} in decreasing order. We ﬁrst obtain the Lasso solution for
the smallest λ value, which typically corresponds to the densest solution in the Lasso path. (This
strategy, which is the opposite of that used in solvers for Lasso — see for example [31] — reduces
the overhead of continuously updating our LP model with new columns and constraints as we move
across the λ-path.) The Lasso solution can be used to supply initial guesses of index sets I0 and J0
for the ﬁrst value λ1. The ﬁnal index sets I1 and J1 for λ1 can then be used as initial guesses for
λ2, and so on. Optimal basis information (basis matrices and their factorizations) for each value of
λ can also be carried over to the next value.

Existing approaches. Many interesting approaches have been presented to solve the (cid:96)1-DS
problem. [4] presents a general framework for by solving a regularized version of the problem with
[23, 30] use ADMM for
ﬁrst order gradient based methods, but these methods do not scale well.
(cid:96)1-DS, which may lead to large feasibility violations. Homotopy methods were presented in [26]
and were shown to outperforms existing algorithms for (cid:96)1-DS. Other homotopy based methods of
appear in [2, 8]. All the homotopy algorithms [2, 8, 26] compute the matrix X T X at the outset;
this memory intensive computation precludes the possibility of solving large scale instances p ≈ 106
with p (cid:29) n. Column and constraint generation has not been considered in these aforementioned
papers.

Computational results of our methods are presented in Section 3.

10

2.3 Basis Pursuit

We study how our DantzigLP framework can be applied to (4), which admits the following LP
representation. (Recall that we assume that the feasible set is nonempty.)

minimize
β+,β−

(β+

j + β−
j )

(cid:88)

j∈[p]

s.t.

y = X(β+ − β−), β+ ≥ 0, β− ≥ 0.

(BP-Full)

Consider a subset of features J ⊂ [p] and a restriction of (BP-Full) to the indices in J. We obtain
the following reduced primal (BPP(J)) and dual (BPD(J)):

minimize
J ,β−
β+

J

s.t.

(cid:88)

(β+

j + β−
j )

j∈J
y = X∗,J (β+
J , β−
β+
J ≥ 0

J − β−
J )

maximize
v

vT y

(BPP(J))

s.t.

∗,J v ≤ 1

X T
− X T

∗,J v ≤ 1.

(BPD(J))

For the column generation procedure, we need a subset J (preferably of small size) for which (BPP(J))
is feasible. Accordingly, we seek an approximation to the largest value of λ for which the Lasso
yields a a feasible solution for (BPP(J)). We ﬁnd such a value by solving the Lasso for a sequence
of decreasing values of λ, checking after each solution whether the resulting solution is feasible
for (BPP(J)), and if so, deﬁning J to be the support obtained of this solution.

If we cannot ﬁnd a set J for which (BPP(J)) is feasible, we append the current J with an additional
n − |J| columns to obtain a feasible solution4. As before, the Lasso continuation approach is used
just to obtain a good initialization for J for our column generation framework, not to obtain a
solution for BP.

The approach is summarized in Algorithm 3; and computational results are presented in Sec-
tion 3.

Algorithm 3 Column Generation for Basis Pursuit (BP-Full)

Solve a sequence of Lasso problems (3) to obtain initial J;
loop

Solve (BPP(J)), with v ∈ Rn as the dual solution;
Calculate Jv := {j ∈ [p] \ J : |X T
if Jv = ∅ then
terminate;

∗,J v| > 1};

else

Choose ∅ (cid:54)= Js ⊂ Jv and set J ← J ∪ Js;

end if
end loop

4If the entries of X are drawn from a continuous distribution, then |J| = n will lead to a feasible solution for
(BPP(J)). In all our experiments, the Lasso continuation approach did lead to a J for which (BPP(J)) was feasible.
Note that the Lasso path often leads to solutions for which the number of nonzeros exceeds n.

11

2.4 The Fused Dantzig Selector

2.4.1 Signal estimation

We discuss how the DantzigLP framework can be extended to the Dantzig analog of (5) with X = I
(the identity matrix), which is

1
2

(cid:107)y − β(cid:107)2

2 + λ(cid:107)D(0)β(cid:107)1,

(13)

where D(0) is deﬁned in (6). To express this problem in Lasso form, we deﬁne the n × n matrix
D = [eT

1 ; D(0)] (where e1 = (1, 0, 0, . . . , 0)T ), which has full rank. Its inverse H = D−1 is

Hi,j =


1


0

i − j

if

if

j = 1

i > j

otherwise.

(14)

We can now rewrite (5) in terms of the variables α := Dβ, and recover the solution β of (13) by
setting β = Hα. Deﬁning A := {1} and B := {2, . . . , n}, we write (13) as follows:

minimize
αA,αB

1
2

(cid:107)y − HAαA − HBαB(cid:107)2

2 + λ(cid:107)αB(cid:107)1.

(15)

This formulation diﬀers slightly from the standard Lasso problem in that the (cid:96)1 penalty term
excludes αA. The Dantzig analog of (15) is

minimize
αA,αB

(cid:107)αB(cid:107)1 s.t. (cid:107)H T

B (y − HAαA − HBαB)(cid:107)∞ ≤ λ, H T

A (y − HAαA − HBαB) = 0.

(16)

(Note the constraint H T
A (y − HAαA − HBαB) = 0, which arises as an optimality condition for αA
in (15).) Recalling that H −1 = D, and introducing auxiliary variables β = Hα, r = y − β, and
g = DT r, we rewrite (16) as follows:

minimize
r,α+,α−,β,∇

s.t.

(cid:0)α+

i + α−
i

(cid:1)

(cid:88)

i∈B

β = D(α+ − α−)
r = y − β
g = DT r
gA = 0
gB ≤ λ1
gB ≥ −λ1

α+, α− ≥ 0.

(17)

We apply column and constraint generation to formulation (17): Column generation because of
sparsity in αB, and constraint generation because few of the constraints gB ∈ [−λ1, λ1] are expected

12

to be active at optimality. The formulation (17) exploits the following key characteristics:

• H −1 = D is banded. Writing constraints in terms of D rather than H yields a constraint
matrix with O(n) nonzero entries. (A direct LP representation for (cid:96)1-DS with n = p as in (9)
would lead to a constraint matrix with O(n2) nonzeros.)

• Each component of α+ and α− appears in exactly one equality constraint, reducing the cost
j , and ¯cj = 1 + vj for
j ) — much cheaper than the corresponding cost for a general (cid:96)1-DS problem, which

of computing each reduced cost to O(1) time (¯cj = 1 − vj for each α+
each α−
requires the O(n) operation vT Xj.

• We can check each constraint violation gi ∈ [−λ, λ] in O(1) time, compared to a general

(cid:96)1-DS problem which requires the O(n) operation X T

∗,ir for each constraint.

To obtain a good initialization for (17), we use the solution of (13), which can be computed
eﬃciently via dynamic programming [21] at O(n) cost.

2.4.2 Beyond signal estimation: Regression

We now consider problem (5) with general design matrix X.

As in (15), we introduce a new variable α = Hβ. We let HA be the submatrix of H containing the
columns indexed by A, and PA denote the projection operator onto the column space of HA. With
this notation in place, we rewrite (5) in standard Lasso form with model matrix ˜X = (I − PA)XH
and response ˜y = (I − PA)y. The corresponding Dantzig Selector problem (7) is an instance of
(cid:96)1-DS problem with problem data (˜y, ˜X). Since (7) lacks the structure as the signal estimation
problem in Section 2.4.1 (where X = I), we apply the DantzigLP procedure (Algorithm 3) to this
problem, with a special initialization. The initial point is obtained by solving (5) using a proximal
gradient method [3]. Each step of this method requires calculation of the proximal map deﬁned
by

Θ(uk) := arg min

β

L
2

(cid:18)

β −

(cid:13)
(cid:13)
(cid:13)
(cid:13)

uk −

1
L

∇f (uk)

(cid:19)(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
2

+ λ(cid:107)D(0)β(cid:107)1,

where L is the largest eigenvalue5 of X T X and f (u) = 1/2(cid:107)y − Xu(cid:107)2
2. The operator Θ(uk) is
computed via dynamic programming [21], which is highly eﬃcient. If we set uk = βk and βk+1 =
Θ(βk) we get the usual (unaccelerated) proximal gradient algorithm. We use the accelerated variant
which enjoys an improved convergence rate. It sets ˜uk+1 = Θ(uk) where uk+1 = ˜uk + qk−1(˜uk −
k)/2. The sequence is initialized with u1 = ˜u0 = 0 and
˜uk−1)/qk+1 and qk+1 = (1 +
q1 = 1.

1 + 4q2

(cid:113)

This proximal gradient approach to solving (5) is much faster than a coordinate descent procedure
[17] applied to the Lasso reformulation of (5) with problem data (˜y, ˜X).

5This can be computed via the power method or by computing the largest singular value of X with cost

O(min{n, p}2 max{n, p}).

13

3 Computational Results

This section presents computational results showing the performance of our proposed DantzigLP
framework for the (cid:96)1-DS problem (Section 3.1), Basis Pursuit (Section 3.2), and the Fused Dantzig
Selector (Section 3.3).

3.1 Computational experience with Dantzig Selector

We implement DantzigLP in Julia6, using Gurobi’s dual simplex solver7 as the LP solver and
Lasso.jl (a Julia implementation of glmnet [17]) as the Lasso solver. At each iteration of column
and constraint generation, we add up to 30 columns with the most negative reduced costs, and
up to 50 of the most violated constraints. Unless stated otherwise, we solve problems to within a
tolerance of 10−4 for both column and constraint generation violations.

3.1.1 Experiments on synthetic datasets

Data Generation. Our ﬁrst set of experiments were performed on synthetic data. The rows of
X are drawn from a multivariate Gaussian distribution MVN(0, Σ) with mean zero and covariance
Σ where, Σij = ρ for all i (cid:54)= j and Σii = 1. We then sparsify8 X by setting its entries to 0
independently with probability π; and ﬁnally normalize so that the columns of X to have unit
(cid:96)2-norm. To generate the true β0, we choose a set S ⊂ [p], |S| = n/5 and set entries of β0
S as i.i.d.
draws from a standard Gaussian distribution. The remaining components β0
Sc are set to zero. We
iid∼ N (0, σ2); and σ2 is chosen so as to achieve a signal-to-noise
then generate y = Xβ0 + e with ei
ratio (SNR) of 10. (Note that we deﬁne SNR as the ratio Var(Xβ)/σ2.)

Comparison with ADMM. A popular method for the (cid:96)1-DS problem is based on ADMM [23,
30, 7]. In Figure 1, we compare DantzigLP with flare [23], a publically available implementation
of ADMM, plotting the violation in feasibility (max{(cid:107)X T (y − Xβ)(cid:107)∞ − λ, 0}) and the diﬀerence
between the objective function from its optimal value (|(cid:107)β(cid:107)1 −(cid:107)β∗(cid:107)1|) as a function of runtime. (We
use absolute value in the objective measure because infeasibility can result in a β with smaller norm
than the solution β∗.) We ﬁnd that ADMM is slow by both measures (an observation made also
by [26]) and that DantzigLP is much faster. Each path in Figure 1 represents a single simulated
problem instance with data generated by the means above, where ρ = 0, π = 0, n = 200, and
p = 1000. We set λ = (cid:107)X T e0(cid:107)∞ where, e0 = (y − Xβ0). Since ADMM has diﬃculty ﬁnding a
solution with an absolute feasibility violation of less than 0.01 in many cases, we do not consider
it further in the experiments below.

6Our Julia/JuMP implementation can be found at https://github.com/atzheng/DantzigLP.
7We use Gurobi version 7.5 in our experiments. All computations were performed on a Mac Pro desktop machine

with specs: 2.7GHz 12-Core Intel Xeon E5. Unless otherwise speciﬁed the memory budget was 64GB of RAM.

8Sparsiﬁcation may destroy the correlation structure among columns of X.

14

Figure 1: Feasibility and objective violations as a function of runtime for DantzigLP (including time
required for Lasso initialization) and the ADMM implementation Flare, for the (cid:96)1-DS problem.
In all
instances, DantzigLP reaches an optimal solution with zero feasibility violation within a few iterations. In
comparison, Flare takes much longer to improve the objective and feasibility violations, often failing to
converge even after hundreds of seconds.

Comparison with PSM. The recently proposed parametric simplex-based solver PSM described
in [26] can solve the (cid:96)1-DS problem and is a state-of-the-art solver for (cid:96)1-DS. In the next set of
tests, we compare the following approaches.

1. PSM, as implemented in the companion R package fastclime, for a path of 50 λ values

logarithmically spaced between λmin = 2(cid:107)X T e0(cid:107)∞ and λmax = (cid:107)X T y(cid:107)∞.

2. Gurobi (dual simplex method) applied to the full LP model (9) for λ = λmin. We denote

these results by “Full LP (Single).”

3. DantzigLP applied to (9) for the same 50 λ values as in the PSM tests. We denote these

results by “DantzigLP (Path).”

4. DantzigLP applied to (9) for λ = λmin. We denote these results by “DantzigLP (Single).”

Note that PSM always computes a full path of solutions via homotopy, even if the solution is
required at just one value of λ. The times shown for our DantzigLP methods include the times
required to compute a Lasso path, usually between 0.1–1 seconds9.

Table 1 shows that when computing a path of 50 λ-values, PSM is usually outperformed by
In computing a solution to (cid:96)1-DS at a single λ, PSM is seen to be outper-
DantzigLP (Path).
formed by solving the full LP model with Gurobi (denoted by “Full LP”), with DantzigLP (Path)
still faster.

We observe that PSM works well on instances with small p (in the hundreds), but its performance
deteriorates with increasing p values. PSM computes the whole matrix X T X, leading to large
memory requirements by comparison with “Full LP” (which in our formulation does not require

9Solving the Lasso for n = 200, p = 5000 is the fastest with n = 1000, p = 104 being the slowest.

15

n
200
200
500
500
1000
1000

p
5000
10000
5000
10000
5000
10000

DantzigLP (Path) DantzigLP (Single) Full LP (Single) PSM
12.1
49.0
16.0
86.5
22.3
92.3

0.81
1.1
4.5
5.9
24.8
27.2

5.1
10.3
13.9
28.9
31.9
65.1

0.12
0.14
1.1
1.3
7.9
7.6

Table 1: Runtime comparison for (cid:96)1-DS (synthetic instances) with ρ = 0 and π = 0. For each n and p,
we show the mean runtime (in seconds) of 20 problem instances. DantzigLP (with column and constraint
generation) is usually faster than Gurobi’s Full LP simplex solver (without column and constraint generation)
and also faster than the state-of-the-art simplex-based homotopy solver PSM.

π

DantzigLP DantzigLP Full LP PSM

(Path)
8.2
6.1
2.4
0.79

0
0.4
0.8
0.95

(Single)
2.4
2.0
0.86
0.27

(Single)
16.7
10.2
3.5
1.0

57.3
55.4
57.0
56.3

ρ DantzigLP DantzigLP Full LP PSM
(Single)
2.4
0.57
0.08

(Single)
16.7
12.7
11.7

(Path)
8.2
3.2
1.1

57.3
15.4
10.2

0
0.4
0.8

Table 2: Runtimes (in seconds, averaged over 20 replications) for solving (cid:96)1-DS (synthetic instances) with
n = 500, p = 5000 for varying sparsity and correlations in X. [Left] We vary sparsity in X, with ρ = 0 and
π ∈ {0, 0.4, 0.8, 0.95} (larger π values correspond to more zeroes in X). PSM has similar runtimes across
all sparsity levels, whereas DantzigLP and Gurobi (Full LP) both see substantial reductions in runtime for
sparse instances. [Right] We vary correlations in columns of X: X is dense with π = 0 and ρ ∈ {0, 0.4, 0.8}.
Runtimes of all algorithms diminish with increasing correlations, with DantzigLP again fastest.

computation of X T X) and also DantzigLP. Of all the methods, DantzigLP has the lowest memory
requirements, as it generates new columns and constraints only as necessary.

Varying sparsity and correlation in X. We next explore sensitivity of runtimes of the various
approaches to sparsity in X and correlations between columns of X, which are captured by the
parameters π and ρ, respectively. Table 2 (Left) shows that the DantzigLP and Full LP approaches
exploit sparsity well (runtimes decrease with increasing sparsity), while PSM does not beneﬁt from
sparsity, possibly becase the product X T X (which it computes) remains dense. For the case of
dense X, Table 2 (Right) shows that increasing correlations between the columns lead to improved
runtimes for all algorithms. (The solutions β tend to be sparser in these cases.) DantzigLP remains
the clear winner.

Dependence on sparsity in solution β.
In the experiments above, we considered a sequence
of λ-values in the range [λmin, λmax] with λmin = 2(cid:107)X T e0(cid:107)∞. It is well known that smaller values
of λ correspond to denser solutions in β. To understand better the dependence of runtime of the
various algorithms on sparsity of β, we tried the values λ = τ (cid:107)X T e0(cid:107)∞, where τ ∈ {0.1, 0.4, 0.7, 1},
showing the results in Table 3. Runtimes for DantzigLP increase with density in β, as expected,
mostly because the Lasso solution has a very diﬀerent support from the solution of (cid:96)1-DS, requiring

16

DantzigLP to generate many more columns than it would for larger λ values. For smaller values of
p (results not shown), DantzigLP can be even slower than the vanilla Gurobi implementation for
the Full LP, due to the overhead of column generation, although memory consumption is still much
smaller (O(n2) as opposed to O(np)). Computation time for PSM also increases as λ decreases,
though not as much as DantzigLP in relative terms. Still, DantzigLP performance remains superior
for most interesting values of λ.

τ

Avg. L0 DantzigLP DantzigLP Full LP PSM

0.1
0.4
0.7
1.0

453.
320.
234.
182.

(Path)
209.
79.2
39.6
13.6

(Single)
96.9
30.7
15.2
4.7

(Single)
77.7
74.5
55.6
24.1

334.
368.
351.
125.

Table 3: Runtimes (in secs, averaged over 20 replications) for solving (cid:96)1-DS (synthetic instances) with
n = 500, p = 5000, ρ = 0, π = 0 for λτ = τ (cid:107)X T e0(cid:107)∞, where τ ∈ {0.1, 0.4, 0.7, 1.0}. DantzigLP (Path) solves
for a path of 50 λ values equally spaced between λmax and λτ ; Full LP and DantzigLP (Single) solve only
for λτ ; PSM solves for the whole path of values from λmax to λτ . The column labelled “Avg. L0” shows
average support size of the optimal solution to (cid:96)1-DS at λτ . As τ decreases, DantzigLP takes longer to solve
the problem, although it still outperforms PSM.

Importance of the components of DantzigLP. The DantzigLP framework consists of several
components: initialization provided by the Lasso, column generation and constraint generation, and
the simplex LP solver. To understand the roles played by these components, we compare several
variants in which some or other of them are omitted. Figure 2 compares the following ﬁve variants
of DantzigLP.

(i) DantzigLP: The complete framework described above.

(ii) Random Init.: Rather than using a Lasso initialization, we initialize I, J to a random subset

of [p] of size (cid:107)β0(cid:107).

(iii) Constraint Gen.: Obtain I from Lasso initialization, but J = [p]. That is, only constraint

generation is enabled.

(iv) Column Gen.: Here, J is obtained from Lasso initialization, with I = [p]. That is, only

column generation is enabled.

(v) Full LP: The full LP model solved with Gurobi, without column or constraint generation.

The boxplots of Figure 2 show the runtime distribution over 20 randomly generated (cid:96)1-DS instances
with n = 1, 000, p = 10, 000, ρ = 0 and π = 0, solved for λ = 2(cid:107)X T e0(cid:107)∞. The ﬁgure highlights
the importance of all elements of Dantzig LP. We note that column generation alone provides no
performance gains over the Full LP approach, due to the overhead of generating new columns
(and restarting our simplex-based LP solvers). However, we see improvements when constraint
generation is also introduced.

17

Figure 2: Comparison of 5 variants of DantzigLP (n = 1, 000, p = 10, 000), showing the importance of all
components of the approach: Lasso initialization, column generation, constraint generation, and the simplex
solver.

3.1.2 Experiments on real datasets

We demonstrate the performance of DantzigLP on real-datasets with p ≈ 106 and n ≈ 103. Due
to memory constraints (our maximum was 64GB), only DantzigLP could solve these problems
among our tested algorithms. We consider a path of 100 λ values log-spaced in the interval
[λmax, 10−3λmax]. Results are displayed in Table 4.

The “AmazonLarge” dataset from [19] has as its goal the prediction of helpfulness scores for product
reviews based on data from Amazon’s Grocery and Gourmet Food dataset. The “Boston1M”
dataset consists of 104 covariates obtained from polynomial expansions of features in the Boston
House Prices dataset ([18]), augmented with 1000 random permutations of each column.

Dataset
AmazonLarge
Boston1M

max ||β||0
178
56

n
2,500
200

p
174,755
1,000,103

DantzigLP Lasso
70.7
44.3

248.
3,702.

Table 4: DantzigLP (and associated Lasso) runtimes in seconds on real datasets. We solve for a path of
100 λ values. The column “max (cid:107)β(cid:107)0” indicates the size of the support of the densest solution obtained.
The DantzigLP runtime includes the runtime of the Lasso initialization step.

3.2 Results for Basis Pursuit

We present some numerical results illustrating performance of the DantzigLP procedure for the
basis pursuit problem (4). The matrix X is generated as in Section 3.1.1, with ρ = 0. We set
y = Xβ0, where β0 is 0.2n-sparse with nonzero elements chosen i.i.d. from N (0, 1). We compare
DantzigLP against two other algorithms: the ADMM approach of [7] as implemented in the ADMM
R package, and solving the full LP model (BP-Full). In this example, we set a memory cap of
16GB of RAM for all experiments.

Table 5 shows runtimes in seconds for a number of instances. We see that DantzigLP performs
much better than competing algorithms in terms of runtime, particularly when p (cid:29) n. This also

18

n

p

DantzigLP Lasso

200
200
200
500
500
500
1000
1000
1000

103
105
4 × 105
103
105
4 × 105
103
105
4 × 105

1.9
8.8
31.3
16.8
25.3
72.7
27.1
88.7
209.

0.38
7.8
28.8
1.4
19.1
62.7
3.0
42.1
116.

Full LP ADMM
(Gurobi)
1.3
NA
NA
4.3
NA
NA
10.9
NA
NA

> 1.1
NA
NA
6.3
NA
NA
26.5
NA
NA

ADMM
(% Converged)
90
NA
NA
100
NA
NA
100
NA
NA

Table 5: Mean runtimes in seconds for simulated basis pursuit instances (20 instances per setting of n and
p), comparing DantzigLP, the Lasso initialization step, the Full LP implementation in Gurobi, and ADMM.
The “NA” values indicate cases where the algorithm was terminated due to memory constraints, with a
maximum memory allocation of 16GB. “ADMM (% Converged)” indicates the percentage of instances in
which ADMM was able to converge. The “ADMM” column reports the minimum of time to termination
and time to convergence, with a “>” symbol indicating cases where fewer than 100% of instances converged.

comes with large savings in memory; the other algorithms were unable to solve problems of size
p ≥ 105 due to violation of the 16GB memory bound. DantzigLP can solve problems an order
of magnitude larger than this. For instances with smaller p, the overhead of generating columns
can cause DantzigLP to underperform the baselines in certain cases. The DantzigLP runtimes
in Table 5 include the runtime of the Lasso initialization step, which is the main performance
bottleneck; the Lasso accounts for 80% of the runtime for these instances.

While DantzigLP can obtain solutions of high accuracy, ADMM often has diﬃculty in doing so.
The ADMM column in Table 5 reports the minimum of time to converge to within 10−4 of the true
objective and time to complete 10000 ADMM iterations (after which we terminate the algorithm).
As the “ADMM (% Converged)” column shows, for larger problem sizes none of the instances
converge to that tolerance within the allotted iteration bound.

3.3 Computational experience with Fused Dantzig Selector

Signal estimation. We ﬁrst consider the Fused Dantzig Selector with X = I, that is, the signal
estimation case of Section 2.4.1. We generate a piecewise constant signal, with discontinuities /
knots chosen at random from [n]. At each knot, the jump is chosen from N (0, 1). We add noise
with SNR=10 to the signal. We solve (17) at a single value λ = (cid:107)H T
B)(cid:107)∞, where
(α0

B) corresponds to the true signal.

B (y − HAα0

A − HBα0

A, α0

In Table 6, we compare our DantzigLP framework10 to directly solving the full version (17).
Gurobi’s dual simplex solver is used in both cases. The formulation (17) allows solution of problems
several orders of magnitude larger than the Dantzig Selector problem (9), when column/constraint

10We solve each instance to within a tolerance of (cid:15) = 10−4 and add up to 40 columns (constraints) per iteration of

column (constraint) generation.

19

n
5 × 104

105

# knots DantzigLP Full LP
3.5
3.5
4.2
5.7
6.2
8.2

100
200
1,000
100
200
1,000

3.3
3.2
3.1
7.9
8.0
7.9

n
2 × 105

5 × 105

# knots DantzigLP Full LP
10.8
11.9
16.4
48.7
54.1
80.9

24.0
24.1
23.6
120.3
121.4
120.6

100
200
1,000
100
200
1,000

[Signal Estimation] Runtimes (seconds) for the Fused Dantzig Selector, comparing DantzigLP
Table 6:
with solution of the Full LP formulation (17) using Gurobi. Because of memory requirements, it would not
be possible directly solve (16) without using our proposed reformulation (17) for the instances considered
here.

n
500
500
1,000
1,000

p
5,000
10,000
5,000
10,000

DantzigLP Gurobi

33.3
117.3
67.0
207.7

82.8
341.7
221.3
925.2

Table 7: [Regression] Runtimes (in seconds, averaged over 20 replications) for the fused Dantzig Selector
with X (cid:54)= I. We compare our DantzigLP framework with Full LP (Gurobi).

generation is not used. The DantzigLP framework improves modestly on this enhancement, by
factors of 2-3 for problems with large n and a small number of knots. The runtime for the full LP
formulation is insensitive to the number of knots, while the runtime of DantzigLP increases with
the number of knots with a large number of knots. This is not surprising, as more knots implies
a denser solution. Solving the Fused Lasso (required for initialization) takes less than one second
across all instances.

Regression. We illustrate the performance of our DantzigLP framework for (7) for a general
model matrix X (cid:54)= I. Here, entries of X are drawn from a standard Gaussian ensemble:
i.e.,
iid∼ N (0, 1). The underlying β0 is piecewise constant and is drawn as before with 20 knots, with
Xij
iid∼ N (0, σ2) and σ chosen to
jumps chosen i.i.d. from N (0, 1). We generate y = Xβ0 + e, with ei
produce an SNR of 10.

Table 7 shows that the Fused Dantzig Selector modestly outperforms solving the full LP with
Gurobi. The runtime improvements are not as pronounced as in the case of (cid:96)1-DS because
the initial solution obtained from the Lasso problem (5) is not as accurate as for (cid:96)1-DS. Thus,
additional columns/constraints need to be generated to achieve optimality, leading to increased
runtimes.

20

4 Acknowledgements

Rahul Mazumder acknowledges research support from the Oﬃce Naval Research ONR-N000141512342,
ONR-N000141812298 (Young Investigator Award) and the National Science Foundation (NSF-IIS-
1718258).

References

[1] M. S. Asif and J. Romberg. On the lasso and dantzig selector equivalence. In 2010 44th Annual

Conference on Information Sciences and Systems (CISS), pages 1–6, 3 2010.

[2] M. S. Asif and J. Romberg. Dantzig selector homotopy with dynamic measurements. Proc.

SPIE 7246, Computational Imaging VII, 72460E, 7246, 2009.

[3] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse

problems. SIAM Journal on Imaging Sciences, 2(1):183–202, 2009.

[4] S. R. Becker, E. J. Cand`es, and M. C. Grant. Templates for convex cone problems with
applications to sparse signal recovery. Mathematical Programming Computation, 3(3):165, 7
2011.

[5] D. Bertsimas and J. N. Tsitsiklis. Introduction to Linear Optimization. Athena, 1997.
[6] P. J. Bickel, Y. Ritov, and A. B. Tsybakov. Simultaneous analysis of lasso and dantzig selector.

The Annals of Statistics, 37(4):1705–1732, 2009.

[7] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statisti-
cal learning via the alternating direction method of multipliers. Found. Trends Mach. Learn.,
3(1):1–122, January 2011.

[8] C. Brauer, D. A. Lorenz, and A. M. Tillmann. A primal-dual homotopy algorithm for (cid:96)1-
minimization with (cid:96)∞-constraints. Computational Optimization and Applications, 70(2):443–
478, 2018.

[9] E. Candes and T. Tao. The dantzig selector: Statistical estimation when p is much larger than

n. Ann. Statist., 35(6):2313–2351, 12 2006.

[10] S. Chen and D. Donoho. Basis pursuit. In Proceedings of 1994 28th Asilomar Conference on

Signals, Systems and Computers, volume 1, pages 41–44 vol.1, 10 1994.

[11] G. B. Dantzig and P. Wolfe. Decomposition principle for linear programs. Operations research,

8(1):101–111, 1960.

[12] A. Dedieu and R. Mazumder. Solving large-scale l1-regularized svms and cousins: the sur-
prising eﬀectiveness of column and constraint generation. arXiv preprint arXiv:1901.01585,
2019.

[13] D. L. Donoho, A. Maleki, and A. Montanari. Message-passing algorithms for compressed

sensing. Proceedings of the National Academy of Sciences, 106(45):18914–18919, 2009.

[14] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. The Annals of

statistics, 32(2):407–499, 2004.

[15] B. Efron, T. Hastie, and R. Tibshirani. Discussion: The dantzig selector: Statistical estimation

when p is much larger than n. The Annals of Statistics, 35(6):2358–2364, 2007.

21

[16] L. R. Ford Jr and D. R. Fulkerson. A suggested computation for maximal multi-commodity

network ﬂows. Management Science, 5(1):97–101, 1958.

[17] J. Friedman, T. Hastie, and R. Tibshirani. Regularization paths for generalized linear models

via coordinate descent. Journal of Statistical Software, 33(1):1–22, 2010.

[18] D. Harrison and D. L. Rubinfeld. Hedonic housing prices and the demand for clean air. Journal

of Environmental Economics and Management, 5(1):81 – 102, 1978.

[19] H. Hazimeh and R. Mazumder. Fast Best Subset Selection: Coordinate Descent and Local

Combinatorial Optimization Algorithms. ArXiv e-prints, March 2018.

[20] G. M. James, P. Radchenko, and J. Lv. Dasso: connections between the dantzig selector
and lasso. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 71(1):
127–142, 2009.

[21] N. A. Johnson. A dynamic programming algorithm for the fused lasso and l 0-segmentation.

Journal of Computational and Graphical Statistics, 22(2):246–260, 2013.

[22] X. Li, T. Zhao, X. Yuan, and H. Liu. The ﬂare package for high dimensional linear regression
and precision matrix estimation in r. Journal of Machine Learning Research, 16:553–557, 2015.
[23] Z. Lu, T. K. Pong, and Y. Zhang. An alternating direction method for ﬁnding dantzig selectors.

Computational Statistics & Data Analysis, 56(12):4037 – 4046, 2012.

[24] E. Mammen and S. Geer. Locally adaptive regression splines. The Annals of Statistics, 25(1):

387–413, 1997.

[25] H. Pang, H. Liu, and R. Vanderbei. The fastclime package for linear programming and large-
scale precision matrix estimation in r. Journal of Machine Learning Research, 15(1):489–493,
January 2014.

[26] H. Pang, H. Liu, R. J. Vanderbei, and T. Zhao. Parametric simplex method for sparse learning.

In Advances in Neural Information Processing Systems, pages 188–197, 2017.

[27] L. I. Rudin, S. Osher, and E. Fatemi. Nonlinear total variation based noise removal algorithms.

Physica D: Nonlinear Phenomena, 60(1):259 – 268, 1992.

[28] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical

Society: Series B (Methodological), 58(1):267–288, 1996.

[29] R. Tibshirani, M. Saunders, S. Rosset, J. Zhu, and K. Knight. Sparsity and smoothness via

the fused lasso. Journal of the Royal Statistical Society Series B, pages 91–108, 2005.

[30] X. Wang and X. Yuan. The linearized alternating direction method of multipliers for dantzig

selector. SIAM Journal on Scientiﬁc Computing, 34(5):A2792–A2811, 2012.

[31] S. J. Wright, R. D. Nowak, and M. A. Figueiredo. Sparse reconstruction by separable approx-

imation. IEEE Transactions on Signal Processing, 57(7):2479–2493, 2009.

22

