AutoDS: Towards Human-Centered Automation of Data Science

Dakuo Wang
dakuo.wang@ibm.com
IBM Research
USA

Josh Andres
IBM Research Australia
Australia

Justin Weisz
IBM Research
USA

Erick Oduor
IBM Research Africa
Kenya

Casey Dugan
IBM Research
USA

1
2
0
2

n
a
J

3
1

]

C
H
.
s
c
[

1
v
3
7
2
5
0
.
1
0
1
2
:
v
i
X
r
a

ABSTRACT
Data science (DS) projects often follow a lifecycle that consists of
laborious tasks for data scientists and domain experts (e.g., data
exploration, model training, etc.). Only till recently, machine learn-
ing(ML) researchers have developed promising automation tech-
niques to aid data workers in these tasks. This paper introduces Au-
toDS, an automated machine learning (AutoML) system that aims
to leverage the latest ML automation techniques to support data sci-
ence projects. Data workers only need to upload their dataset, then
the system can automatically suggest ML configurations, prepro-
cess data, select algorithm, and train the model. These suggestions
are presented to the user via a web-based graphical user interface
and a notebook-based programming user interface. We studied Au-
toDS with 30 professional data scientists, where one group used
AutoDS, and the other did not, to complete a data science project.
As expected, AutoDS improves productivity; Yet surprisingly, we
find that the models produced by the AutoDS group have higher
quality and less errors, but lower human confidence scores.
We reflect on the findings by presenting design implications for
incorporating automation techniques into human work in the data
science lifecycle.

CCS CONCEPTS
• Human-centered computing → User studies; Empirical studies
in HCI ; • Computing methodologies → Artificial intelligence.

KEYWORDS
Data science, automated data science, automated machine learning,
AutoML, AutoDS, model building, human-in-the-loop, AI, human-
AI collaboration, XAI, collaborative AI

ACM Reference Format:
Dakuo Wang, Josh Andres, Justin Weisz, Erick Oduor, and Casey Dugan.
2021. AutoDS: Towards Human-Centered Automation of Data Science. In
CHI Conference on Human Factors in Computing Systems (CHI ’21), May
8–13, 2021, Yokohama, Japan. ACM, New York, NY, USA, 12 pages. https:
//doi.org/10.1145/3411764.3445526

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CHI ’21, May 8–13, 2021, Yokohama, Japan
© 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-8096-6/21/05. . . $15.00
https://doi.org/10.1145/3411764.3445526

1 INTRODUCTION AND BACKGROUND
Data Science (DS) refers to the practice of applying statistical and
machine learning approaches to analyze data and generate insights
for decision making or knowledge discovery [11, 44, 48]. It involves
a wide range of tasks from understanding a technical problem to
coding and training a machine learning model [48, 65]. Together,
these steps constitute a data science project’s lifecycle [63]. As illus-
trated in Figure 1, data science literature often use a circle diagram
to represent the entire data science life cycle. In one version of the
DS lifecycle model, [63] synthesizes multiple papers and suggests
a 10 Stages view of the DS work. Because the complex natural of
a DS lifecycle, a DS project often requires an interdisciplinary DS
team (e.g., domain experts and data scientists) [29, 52, 70]. Previous
literature suggests that as much as 80% of a DS team’s time [55] is
spent on low-level activities, such as manually tweaking data [25]
or trying to select various candidate algorithms [71] with python
and Jupyter notebooks [30]; thus, they do not have enough time
for valuable knowledge discovery activities to create better models.
To cope with this challenge, researchers have started exploring the
use of ML algorithms (i.e. Bayesian optimization) to automate the
low-level activities, such as automatically training and selecting the
best algorithm from all the candidates [35, 38, 40, 71]; this group of
work is called Automated Data Science (or AutoDS for short) 1.

Many research communities, universities, and companies have
recently made significant investments in AutoDS research, under
the belief that the world has ample data and, therefore, an unprece-
dented demand for data scientists [24]. For example, Google released
AutoML in 2018 [19]. Startups like H2O [23] and Data Robot [10]
both have their branded products. There are also Auto-sklearn [16]
and TPOT [14, 50] from the open source communities.

While ML researchers and practitioners keep investing and ad-
vancing the AutoDS technologies, HCI researchers have begun to
investigate how these AI systems may change the future of data
scientists’ work. For example, a recent qualitative study revealed
data scientists’ attitudes and perceived interactions with AutoDS
systems is beginning to shift to a collaborative mindset, rather than
a competitive one, where the relationship between data scientists
and AutoDS systems, cautiously opens the “inevitable future of
automated data science” [65].

However, little is known about how data scientists would ac-
tually interact with an AutoDS system to solve data science
problems. To fill this gap, we designed the AutoDS system and

1Some researchers also refer to Automated Artificial Intelligence (AutoAI) or Auto-
mated Machine Learning (AutoML). In this paper, we use AutoDS to refer to the
collection of all these technologies.

 
 
 
 
 
 
CHI ’21, May 8–13, 2021, Yokohama, Japan

Wang and et al.

In summary, our work makes the following contributions to the

CHI community:

• We present an automated data science prototype system with
various novel feature designs (e.g., end-to-end, human-in-
the-loop, and automatically exporting models to notebooks);
• We offer a systematic investigation of user interaction and
perceptions of using an AutoDS system in solving a data
science task, which yields many expected (e.g., higher pro-
ductivity) and novel findings (e.g., performance is not equal
to confidence, and shift of focus); and

• Based on these novel findings, we present design implica-
tions for AutoDS systems to better fit into data science work-
ers’ workflow.

2 RELATED WORK
2.1 Human-Centered Machine Learning
Many researchers have studied how data scientists work with data
and models. For example, it was suggested that 80 percent of the
time spent on a data science project is spent in data preparation [22,
48, 70, 71]. As a result, data scientists often do not have enough
time to complete a comprehensive data analysis [61].

A popular research topic in this domain is interactive machine
learning, which aims to design better user experiences for human
users to interact with machine learning tool [1]. These human
users are often labelers of a data sample or domain experts who
have better domain knowledge but not so much machine learning
expertise.

Based on these the findings from these empirical studies, many
tools have been built to support data science workers’ work [34,
44, 53]. For example, Jupyter Notebook [30] and its variations such
as Google Colab [20] and Jupyter-Lab [31] are widely adopted by
the data science community. These systems provide an easy code-
and-test environment with a graphical user interface so that data
scientists can quickly iterate their model crafting and testing pro-
cess [37, 48]. Another group of tools includes the Data Voyager [26]
and TensorBoard [9] systems that provide visual analytic support
to data scientists to explore their data, but they often stop in the
data preparation stage and thus do not provide automated support
for model building tasks in the lifecycle (as shown in Figure 1).

2.2 Human-in-the-Loop AutoDS
AutoDS refers to a group of technologies that can automate the man-
ual processes of data pre-processing, feature engineering, model
selection, etc. [71]. Several technologies have been developed with
different specializations. For example, Google has developed a set
of AutoDS products under the umbrella of Cloud AutoML, such
that even non-technical users can build models for visual, text, and
tabular data [19]. H2O is java-based software for data modelling
that provides a python module, which data scientists can import
into their code file in order to use the automation capability [23].
Despite the extensive work in building AutoDS systems and
algorithms, only a few recent efforts have focused on the interac-
tion between humans and AutoDS. Gil and collaborators propose a
guideline for designing AutoDS systems [18]. However, they envi-
sion new design features for AutoDS based on their understanding

Figure 1: A 10 Stages, 43 Sub-tasks (not shown) DS/ML Life-
cycle. This is a synthesized version from reviewing multiple
scholarly publications and marketing reports [16, 18, 39, 41,
65].

conducted a between-subject user study to learn how data work-
ers use it in practice. We recruited 30 professional data scientists
and assigned them into one of two groups to complete the same
task — using up to 30 minutes to build the best performing model
for the given dataset. Half of the participants used AutoDS (ex-
periment group), and the other half built models using Python
in a Jupyter notebook, which aims to replicate their daily model
building work practice (control group). We collected various mea-
surements to quantify 1) the participant’s productivity (e.g., how
long a participant spent in building the model or improving the
model), 2) the final model’s performance (e.g., its accuracy), and 3)
the participant’s confidence score in the final model (i.e., how much
confidence a participant has in the final model, if they they are
asked to deploy their model). Before the experiment, we also collect
measurements on 4) each participant’s data science expertise level
and 5) their prior experience and general attitude towards AutoDS
as control variables.

Our results show that, as expected, the participants with AutoDS
support can create more models (on average 8 models v.s. 3.13
models) and much faster (on average 5 minutes v.s. 15 minutes),
than the participants with python and notebook. More interestingly,
the final models from the AutoDS group have higher quality (.919
ROC AUC v.s. .899) and less human errors (0 out of 15 v.s 7 out
of 15), than the models from the control group with python and
notebooks. The most intriguing finding is that despite participants
acknowledged the AutoDS models were at better quality, they had
lower confidence scores in these models than in the manually-
crafted models, if they were to deploy the model (2.4 v.s. 3.3 out of
a 5-point Likert scale). This result indicates that “better” models
are not equal to “confident” models, and the “trustworthiness” of
AutoDS is critical for user adoption in the future. We discuss the
potential explanations for this seemingly conflicted result, and
design implications stemming from it.

AutoDS: Towards Human-Centered Automation of Data Science

CHI ’21, May 8–13, 2021, Yokohama, Japan

of how data scientists manually build models, and their understand-
ings arise from surveying the previous literature and the authors’
personal experience. In our study, we aim to fill in the empirical
understanding gap by conduct an experiment to systematically
examine how data scientists actually use an AutoDS system.

Another recent work studies data scientists’ perceptions and
attitudes towards AutoDS systems through an interview with 30
data scientists [65]. The interviewees, who have never interacted
with AutoDS before, believe that AutoDS could potentially change
their work practice. They also believe automation in data science
work is the future, but they certainly hope that such automation
could support their jobs instead of sabotaging them. We follow this
line of research. We aim to provide an account for the actual user
behaviors when a data scientist uses an AutoDS system to build a
model.

There is one more research strand worth mentioning is the in-
formation visualization designs for AutoDS systems [67, 68]. For
example, ATMSeer enables users to browse AutoDS processes at the
algorithm, hyperpartition, and hyperparameter levels [71]. Their
results indicate that users with a higher level of expertise in ma-
chine learning are more willing to interact with ATMSeer [67].
We reference these existing visualization work’s findings while
designing and implementing our prototype system’s user interface.
But these papers only focus on the visualization aspect and reveal
information on existing AutoDS pipelines, and they do not measure
the data scientists’ behaviors. Thus, the feedback from these studies
are limited to the AutoDS visualization user interface design, but
not much for the AutoDS’s functionality improvement.

2.3 Human-Centered AI Design and

“Cooperative” AI

The recent "AI Summer" [21] is remarked with machine learning
system demonstrations such as IBM DeepBlue [6] and Watson
Jeopardy [45], and Google’s AlphaGo [66]. In these user scenarios,
AI has largely been portrayed as a competitor to humans. The
general public and news media began to worry about when the
“singularity” will arrive, with AI replacing humans [3].

More recently, a few researchers have started to argue that in-
stead of worrying about the singularity, why do not we work to-
gether to design AI systems that can collaborate with humans? [42,
59] Following this trend, HCI projects have reported various case
studies of designing AI systems to work together with humans in-
stead of replacing them. For example, Cranshaw and collaborators
developed a calendar scheduling assistant system that combines
the complementary capabilities of humans and AI systems for tasks
such as scheduling meetings [8]. They coined this architecture a
“human-in-the-loop AI system”. More towards the hardware side of
the AI spectrum, a group of researchers from Cornell University
have designed a robot that can work together with humans as a
team and complete the tasks such as distributing resources to hu-
man collaborators [7]. IBM researchers also experimented the use
case of putting an embodied conversational agent into a recruit-
ing team of two human participants, with the agent and humans
working together to complete a CV review task [57].

There was a seminal debate 20 years ago on “Agency v.s. Direct
Manipulation User Interfaces Design” [58]. With more and more

deep neural network AI technologies, we, as human users, find
it harder and harder to understand what happens inside the AI
“black box”. In parallel, we designate more and more agency and
proactiveness to many of today’s AI system user interfaces, such as
the conversational systems and autonomous cars. Some researchers
have reported that users already look at these AI systems differ-
ently than the traditional computer systems but more like human
partners [69], where anthropomorphism effect plays a critical role
in user perceptions (e.g. [49, 62]. Researchers and designers are
actively asking: what are the updated frameworks and theories that
we can leverage to help us design better AI systems to work with
human?

Various human-centered design guidelines for AI have been
published in the past year from big companies such as Google, Mi-
crosoft, and IBM [2, 28, 56, 60]. But these guidelines often focus
only on usability of AI system’s interface design, but fall short in
discussing the integration of AI systems into human workflows
as a collaborative partner. We argue that with an AI system being
perceived more like a collaborator teaming up with humans, we
may be able to reference classical theories from human-human
collaborations to guide our design of an cooperative AI system.
For example, we may learn from the “collaboration awareness” con-
cept [12] to guide our design of AI system’s “transparency”. The
“awareness” is a bi-directional concept, thus maybe “transparent
AI” should not only have human better understand AI’s runtime
state and logic, but also have AI keep track of human’s states and
intention. Thus, in this study, we are also interested in exploring
the human-AutoDS interaction through a collaborative work per-
spective.

3 AUTODS SYSTEM DESIGN
Based on machine learning technique’s advances and inspired by
design insights from related literature, we implement an AutoDS
prototype system, shown in Figure 2, to support data science work-
ers on data science tasks.

From the user perceptive, a user is only required to upload their
dataset to trigger the AutoDS execution, and then they can wait
for the final model result. They interact with the system primarily
from two graphical user interface screens: a configuration screen
(Figure 2a) and a result screen(Figure 2b). At the configuration screen,
after the user uploads a dataset, AutoDS suggests a specific the ML
task configuration for users to approve or adjust (e.g., classification
or regression); then at the result screen, the users can monitor Au-
toDS’s execution progress visualization in real time and eventually
see the final results in a model leaderboard view.

When looking closer at the result screen in Figure 2b, we design
a tree-based progress visualization at the top. Each leave dot in
the tree-based visualization represents one of the candidate model
pipeline; and the path represents its composition flow. The screen-
shot illustrates an AutoDS execution is in progress. Two algorithms
are being tested (XGB Classifier (blue) and Gradient Boosting Clas-
sifier (purple) ). For the XGB Classifier, four models (P1 to P4) are
generated; and for Gradient Boosting Classifier, only two models
(P5 and P6) are generated, and two more (P7 and P8) are in training.
Some models use the same algorithm but their composition steps
are different. For example, P2 has an extra step of hyperparameter

CHI ’21, May 8–13, 2021, Yokohama, Japan

Wang and et al.

automatic generation of human-readable python notebooks) are
also novel and practical contributions to the AutoDS system designs.
From the system and algorithm perspective, AutoDS reads in
the user uploaded dataset, suggests a problem configuration (e.g.,
classification or regression) based on the data structure. Then, Au-
toDS jointly optimizes2 the sequence of the model pipeline, which
includes selecting the appropriate methods for data preprocessing,
feature engineering, algorithm selection, and hyperparameter opti-
mization. AutoDS can choose a sequence of transformation steps
before training an estimator, or it can simply decide not to use
any transformation to generate new features. Once the sequence is
decided, AutoDS can search and decide on which particular trans-
formation to be used inside each of the transformation step, and
which estimator to be used inside the modeling step. Finaly, AutoDS
will fine-tune the hyperparameters of those chosen estimators and
transformations simultaneously.

By design, our AutoDS system can provide automation support
for the end-to-end data science lifecycle as shown in Fig. 1: from
Requirement Gathering and Problem Formulation (i.e., suggesting
configurations), to Model Building and Training (i.e., selecting algo-
rithms), and to Decision Making and Optimization (i.e., deploying
models).

4 METHOD
4.1 Experiment Design
Following a lab experiment study guideline [17], we conducted a
between-subject user evaluation to understand how people interact
with AutoDS in a data science project. Participants were asked to
try their best at building a machine learning model for the same
UCI Heart Disease dataset [5], either by writing Python code in a
Jupyter Notebook (Notebook Condition in Figure 3) or by using
the AutoDS prototype system (AutoDS Condition in Figure 2).
The experiment design follows a time-constrained clinical trial
setting, each participants was given 30 minutes to build their best
model. The reason why we decided on 30 minutes was because that
from our three pilot study sessions, all the pilot participants (data
scientists) finished the task within 12 minutes, and they needed
to write no more than 10 lines of code to get the expected result:
split train and test data subsets, define a model variable, and run a
cross-validation function to train the model and report the accuracy
score. We considered a participant having completed the task if
s/he got at least one model.

Their final model’s performance is evaluated based on the ROC
AUC score3. During the 30 minutes time, participants were allowed
to try out one or more models, but they understood that their per-
formance and productivity was not measured by the quantity of
the mode, rather the quality of the model. They were allowed to
submit early if they were satisfied with the model. All experiment
sessions were conducted remotely using a video conferencing sys-
tem, and participants used their own laptops during the experiment,
as both the notebook and AutoDS were hosted on our experiment
cloud server. These requirement was specified in the experiment

2For more technical details about the AutoDS joint optimization algorithm, readers
can refer to [38, 43] to replicate the backend.
3One of the widely-used model performance metrics [51]. For simplicity, we refer to
this metric as “accuracy” in the rest of paper.

(a) Configuration UI Screen

(b) Result UI Screen

Figure 2: The two steps of AutoDS’s graphical user interface.

optimization, in comparison to P1, and P3 has an additional feature
engineering step, despite these models are all using the same XGB
algorithm.

At the bottom of the result screen in Figure 2b, we included a
model result leaderboard. Each row in the leaderboard represents a
candidate model pipeline, which corresponds to a model node in the
tree visualization at the top. It displays following model information
Rank, Pipeline ID, Algorithm, Accuracy Score, Enhancement Steps
such as Transformation and HyperparameterOptimization, Training
Runtime, and ROC AUC model performance score on training or
holdout data splits.

We design three user functions for each model result, and they
all can be triggered through interacting with the model leaderboard.
1) a user can further examine the details of a model (such as the
features included or excluded in the model, the prediction plot,
etc) through clicking on a row in the leaderboard (not shown in
Figure 2b); 2) if a user is satisfied with a model, they can click on the
“Save as” button to save the “Model” (shown in Figure 2b), which
will be automatically deployed as a Cloud API endpoint; and 3) if
a user is interested in checking the details of a model via python
notebook, they can click on “Save as Notebook” button (shown
in Figure 2b) and download the AutoDS-generated notebook for
further improvement. Many of these design consideration (e.g.,

AutoDS: Towards Human-Centered Automation of Data Science

CHI ’21, May 8–13, 2021, Yokohama, Japan

instruction and all the participants verbally acknowledged this
requirement.

Worthy noting that this data science task is a simplified version
of their daily data science works, we selected the UCI Heart Disease
dataset, which contains only 303 patients’ basic medical record in-
formation and 13 features (both continuous and categorical values)
to predict whether the patients have heart disease (binary target
feature with 0 represents no disease). It was a widely used bench-
mark dataset in research papers as well as in machine learning
competitions (e.g., Kaggle [32]). Data cleaning, preprocessing, and
feature engineering steps are not critical to build a valid model, but
in order to achieve better model performance, participants do need
to try out further model improvement steps. In both conditions,
the dataset and required libraries in Notebook were pre-loaded so
participants did not need to find the data or set up the network.

We asked the participants to share their screens, and with their

consent, we recorded each session for further analysis.

4.2 AutoDS Condition
Participants in the AutoDS Condition used our AutoDS prototype,
shown in Figure 2. As a default problem configuration, AutoDS
can generate a list of the four model pipelines with one single
algorithm (shown in Fig. 2). A model pipeline consists of all of the
composition steps in generating the model, from reading data to
optimizing hyperparameters. As aforementioned, not all pipelines
had the hyperparameter optimization step or feature engineering
step, leading to the variety of generated models and their accuracy
scores. Participants could modify the AutoDS’s configuration and
get up to 8 model pipelines.

As shown in the AutoDS system design section, participants
have a user function to inspect the generated model results with
a visualization and a leaderboard showing information such as
the confusion matrix, a feature importance chart, and tables with
various evaluation metrics (e.g. ROC AUC, precision, F1, etc.) and
descriptions of feature transformations. In addition, through the
automated notebook generation feature in AutoDS systems, partic-
ipants were also able to download, execute, or edit the generated
code and to further improve the AutoDS model through coding.

4.3 Notebook Condition
Participants in the Notebook Condition were provided with an
online Jupyter Notebook environment as a replication to their cur-
rent work environment. To simplify the task, we provided note-
books with pre-scripted code sections and a skeleton of instructions.
The notebooks were preloaded with data and necessary libraries,
and we provided instructions in the markdown cells in different
sections. For example, we listed “(Optional) Feature Engineering
section”, “(Required) Modeling Training and Selection section”, and
blank code cells for users to fill in their code. In the model training
and selection section, we suggested five commonly used algorithms
for this particular task: Logistic Classifier, KNN, SVM, Decision
Tree, and Random Forest [51]. Noted that these pre-scripted code
skeletons were meant to help participants to easily write code, and
we specifically described in the task sheet that they were not en-
forced to use these pre-scripted codes. From our observation of

Figure 3: Screenshot from the Jupyter Notebook used in par-
ticipants group with python and notebook (Control Condi-
tion). The Notebook contained skeleton code to load rele-
vant data science libraries (numpy, pandas, skikit-learn), as
well as the data set. Instructional sections in the Notebook
were provided to remind participants to perform the follow-
ing tasks: data preprocessing (optional), feature engineering
(optional), model training (with 5 different algorithms rec-
ommended), and model comparison (optional).

the experiment, some participants in the Notebook Condition did
choose to not use the skeleton but wrote all codes in one cell.

4.4 Measures
Our research goal was to explore how different the participants (i.e.
data scientists) may complete the data science tasks in these two
conditions. To that goal, we manually coded the 30 video recording
sessions (each is at 45 minutes to one hour long) to extract various
measurements, such as how many models each participant gen-
erated, what final algorithm they selected, and what its accuracy
was. We also captured time-related measures, such as how long
they spent on the model building task, to evaluate how participants
allocate their time differently.

In both AutoDS Condition and Notebook Condition, we collected
participants background information and their attitudes and per-
ception towards AutoDS before the session. We also collected their
confidence score of the final submitted model after the session.

In the AutoDS Condition alone, we also collected participants’
perceptions towards the AutoDS prototype they just used through
a XAI questionnaire [27], as its four dimensions of predictable,
reliable, efficient, and believable of an AI system is applicable in
our context. In addition, we also collected participants’ ratings
of trust in AutoDS technologies for a second time in the AutoDS
condition, as their scores may change after just experiencing the
system. Thus, we can compare the two pre-study trust scores across
the two conditions, and compare the pre-study and post-study trust
scores in the AutoDS Condition.

It is worth noting that we collect ROC AUC score among other
scores as an indicator of the model performance from both con-
ditions, as we simply need a standardized metric to reflect each

CHI ’21, May 8–13, 2021, Yokohama, Japan

Wang and et al.

participant’s task performance. We acknowledge that the accuracy
score of a model should not be overstated [36, 54], as in an actual
data science project, there are various other considerations (e.g.,
runtime efficiency) in evaluating a model’s performance than the
model accuracy score.

From our observation, some participants in the Notebook Con-
dition made human errors in the submitted model solutions. For
example, they forgot to split the data, and thus reported a very high
but incorrect accuracy score because they used the training data
split. Other participants reported a F1-score despite the instruction
asked for a ROC AUC score. Thus, the scores from these error so-
lutions are not comparable to the scores from other participants’
solutions. The participants that made these mistakes acknowledged
they were indeed human errors in the post-study interview, and
they said they forgot to follow the instruction sheet. To capture
this human error, we created one additional measurement to indi-
cate whether a participant successfully complete the task with no
human error.

In summary, we have four groups of quantitative measurements:

• Participant’s background information, collected via a pre-

study survey (P1, P2 for both conditions);

• Participant’s perceptions toward general AutoDS type of
technologies, collected via a pre-study survey (A1, A2, A3
for both conditions) and post-study survey (AA2, AA3, AA5
for AutoDS condition only)

• Participant’s behavioral measurements in the task (B3, BNx,
BAx) and the final model performance (B1, B2), collected via
coding the video recordings (both conditions)

• Whether the participant successfully completed the task
without human error, collected by examining the final model’s
code (F1 for both conditions)

A summary of all the collected quantitative measurements are
listed in Table 1. We also conducted a semi-structured interview
at the end of each session to gather users’ qualitative feedback to
enrich our results.

4.5 Participants
We recruited 30 professional data scientists in a multinational IT
company, and randomly assigned them to one of the two conditions
(AutoDS v.s. Notebook). These participants come from different
locations in the U.S., South Africa, and Australia. Seven participants
out of 30 were female (23%), which was similar to the 16.8% ratio
reported in the Kaggle 2018 survey of data science [33].

Our recruitment criteria was that the participants were pro-
fessional data scientists and that they practiced data science or
machine learning works in their day-to-day work. Participants re-
ported practicing data science for an average of 3.5 years (SD =
2.7). Six participants (20%) rated themselves as beginners, 17 (57%)
as intermediates, and 7 (23%) as experts in data science. Partici-
pants rated themselves at a moderate amount of experience (3.2
SD=.97 out of 5) with python scikit-learn lib, which is used in our
study. Later, in the modeling section in result section 5.5.2, we test
whether these background factors (e.g., years of expertise, and prior
experience with scikit-learn) may influence the model performance,
and the result is not significant.

Background Measures
P1. Years spent practicing data science
P2. Prior experience with scikit-learn (1=low, 5=high)
Attitudinal Measures

“A” questions used in both conditions, “AA” used only in AutoDS)

A1. Previous familiarity with general AutoDS (1=low, 5=high)
A2. Trust in general AutoDS (first time)
AA2. Trust in general AutoDS (second time)
A3. Belief in general AutoDS replace human (first time)
AA3. Belief in general AutoDS replace human (second time)
A4. Participant’s confidence in the selected final model
AA5. XAI Scale [27]
Behavioral Measures

“B” questions used in both conditions, “BA” used only in AutoDS, “BN”

used only in Notebook
B1. Accuracy of final model (ROC AUC)
B2. Type of final model (e.g. logistic regression, random forest, etc.)
B3. Total time spent searching for information on the web
BN1. Amount of time spent preparing data
BN2. Amount of time spent until the first model was produced
BN3. Number of different models tried
BN4. Did participant perform exploratory data analysis (EDA)?
BN5. Did participant perform feature engineering?
BA1. Total time spent configuring AutoDS
BA2. Total time spent running AutoDS
BA3. Total time spent examining the leaderboard
BA4. Total time spent examining pipeline details
BA5. Number of times AutoDS was run
BA6. Number of pipelines examined (i.e. viewed pipeline details)
BA7. Number of pipeline notebooks viewed
BA8. Number of pipeline notebooks edited
BA9. Which AutoDS pipeline was chosen at the end?
BA10. Did participant change the code of the final selected pipeline?
BA11. Accuracy of the modified pipeline
Successful Completion
F1. (Binary) Did participant successfully complete the task without human
errors (e.g., mistakenly reporting accuracy score from the training data split
instead of the holdout)?

Table 1: Summary of background, behavioral, and attitudi-
nal measures captured in the study.

5 RESULTS
We first describe the overall results, then we list results in the
order of: attitudes toward general AutoDS systems; behavioral
measures from Notebook conditions and from AutoDS conditions
respectively; and lastly, an extensive comparison of the two con-
ditions in terms of participant behaviors, model outcomes, and
participant attitudes.

5.1 Overall Results
All 15 participants (100%) in the AutoDS condition and eight partic-
ipants (53%) in the Notebook condition finished the model building
task without any human error. We refereed to these participants as
the “successful” participants. In contrast, seven participants (47%)
from the Notebook condition made one or more mistakes in the
process. Common human errors included the participant forgetting
to split the dataset into training and testing subsets, or reporting a
different accuracy metric other than the required ROC AUC score.

AutoDS: Towards Human-Centered Automation of Data Science

CHI ’21, May 8–13, 2021, Yokohama, Japan

The experiment time was limited up to 30 minutes for both the
conditions, out of all 30 participants, the fastest participant to reach
a state of completion was P23 (Notebook), who finished the task
early in 17.9 minutes with an accuracy score of 0.923. This was
an extraordinarily good performance of human data scientists in
the Notebook condition, because, on average, participants spent 15
minutes (SD = 6.8 minutes) generating 1 model (BN2). For compar-
ison, AutoDS condition participants generated 4 model pipelines
in about 5 minutes. If the productivity of building a model were to
be measured by the time and the quantity, this result could have
been seen as “AutoDS brings a 300% increase in productivity”.
But this result was not the goal of our study and it was kinds of
expected. Also, it was not a fair comparison, thus we would caution
readers not to use this number bluntly.

As for final submitted models, 18 participants chose the Random
Forest algorithm (also the best one according to our own experi-
ment), 9 chose Logistic Regression, 2 used SVM and 1 chose Extra-
Tree. The AutoDS condition outcome was dominated by Random
Forest (13 out of 15) with 2 participants chose Logistic regression.
In the Notebook condition, participants’ choices were more diverse
with only 5 out of 15 chose Random Forest.

5.2 Attitudes Toward AutoDS
Before the task, participants in both conditions were asked their
familarity and attitudes toward general AutoDS technologies (A1,
A2, A3); at the end of the AutoDS condition, participants were again
asked about the attitudes questions (AA2, AA3). In this section, we
report the comparison between A1, A2, A3 measurements between
the two conditions. We reserve the comparison result between
pre-study trust and post-study trust inside the AutoDS condition
in Sec 5.5.4. In general, participants had some familiarity with
AutoDS/AutoML technologies (2.27 SD = 1.08 out of 5), but not
much. They may have heard about it before or even used it once,
but they had not been using it frequently (A1).

About the “Trust in General AutoDS/AutoML” question (A2), we
found participants had conflicting opinions: 13 participants (43%)
agreed with this statement, 13 remained neutral, and 4 disagreed
(13%).

And lastly, participants did not believe AutoDS would replace
human data scientists (A3): 15 (50%) disagreed with this statement
and only 3 (10%) agreed, with the rest remaining neutral.

5.3 How Data Scientists Worked with AutoDS
In this subsection, we report how participants built models using
AutoDS.

Participants needed to explore the data to understand the prob-
lem; they did so in the AutoDS configuration step. Thus, despite
AutoDS automatically and instantly suggested default configura-
tions (such as prediction type), participants still spent some time
in this step to further examine the dataset by checking the data
distributions etc. (2.3 SD=.96 min) (BA1).

Feature engineering, model training, and model selection
steps were fully automated by AutoDS; we tracked time as par-
ticipants waited for AutoDS to complete runs (2.1 SD=.59 min)
(BA2).4

“It’s fast and it gives you visualizations to compare the
models, this saves me a lot of time” (P8, M, AutoDS)

Because it was fast for data scientists in the AutoDS condition
to generate models (in total less than 5 minutes), participants had
some extra time to spend on other activities (BA5). For example,
they spent more time to examine or further refine the model
pipeline results generated by AutoDS. They spent most of their
time in examining the leaderboard and visualization as shown in
Figure ?? (9.7 SD=3.4 min) (BA3). Then, they also went into particu-
lar model’s detailed information page for fine-grained information
(3.9 SD=2.2 min) (BA4),

Participants examined the details of an average of 3.8 (SD=1.2)
pipelines (BA6), and some of them downloaded the generated note-
book, examined the code to further understand the model (7 out
15 participants, spent an average of 2.3 (SD=.70) minutes) (BA7).
Some participants even further revised those codes (7 out 15), and
most of these participants edited only one Notebook (BA8). Partici-
pants who chose to edit code did so with mixed outcome results.
Out of 7, two were able to improve accuracy scores to the pipeline
produced by AutoDS, and two submitted final models that actually
had slightly worse accuracy. Three abandoned their changes and
submitted the original scores (BA10).

The path of selecting the best model in AutoDS condition seemed
straightforward: a participant ran AutoDS and selected the model
with the “highest performance AUC ROC score”. But some partici-
pants did not select the top performance model as their final choice.
Of the 8 possible models that could be produced with AutoDS, 8
participants chose the highest-scoring one (called “P7”) with a Ran-
dom Forest algorithm, and 7 chose a different model with a lower
ROC AUC score than P7. It appears that even though P7 with the
highest ROC AUC score was available to the participant, in some
cases they had already invested time and effort in inspecting and
tweaking parameters within an earlier generated pipeline, in the
end selecting a pipeline with a lower ROC AUC score than P7 (BA9).
The mean ROC AUC score for the final selected model was 0.919
(SD=.01) (BA11).

5.4 How Data Scientists Work with Notebooks
Now we shift our discussion from user perception to user behavior.
Participants’ workflow in the Notebook condition was very similar
to prior findings, e.g. [48]. They went through data exploration,
feature generation, model building, hyperparamenter tuning, and
modeling selection steps. As this is a lab study centered around
model building, participants were not asked to perform deployment
steps of the model.

About the time-related features, as aforementioned, participants
spent 15 minutes (SD=6.8) in generating the first model (BN2). Note
that this BN2 measure also includes the time participants spent
in pre-processing and splitting data (BN1 9.4 minutes, SD=5.5),

4In our study, the dataset was simple, so it took AutoDS only a few minutes to get the
first model generated; with a more complicated dataset (e.g., hundreds of MB data file),
this training step could take hours.

CHI ’21, May 8–13, 2021, Yokohama, Japan

Wang and et al.

so the actual model building task does not take that long. A few
participants (N = 2) were marked “un-successfull” as they did not
perform the necessary data splitting step (F1).

In investigating the dataset, seven participants (47%) performed
some form of exploratory data analysis, either by looking at tables
that summarized descriptive statistics of the data, or by producing
graphs and charts (BN4). But none of the participants wrote new
code to explore the data distribution or generate visualizations to
support their findings. From the post-study interview, almost all
participants mentioned that if they had had more time, they would
have loved to conduct more data exploration and understanding of
the domain, and consulting with domain experts. By doing so, they
believe they could have built better models, and increased their
confidence in the model they generated.

“... I need to talk with domain experts or doctors to
understand the data and feature ... whether my current
model makes sense .. Then, I need to think about how
to create new features to improve the model.” (P29, M,
Notebook)

Only four participants (27%) performed some form of feature en-
gineering, by transforming, scaling, or combining existing features
to create new features (BN5). Some argued that they did not have
time, while a few others stated they did not do so for this simple,
small dataset.

“I know SVM’s and Random Forest, in problems like
this you don’t need to pre-normalize [the data], RF
can handle it.” (P28, M, Notebook)

About the quantity of the outcome, Notebook participants tried
on average 3.13 (SD = 1.69) models (BN3). Three participants fo-
cused all of their attention on just one model, and four participants
tried all five suggested models. Despite finishing the task minutes
early, participants who finished only one model argued they did not
have enough time to try more models, or so they believed. All of the
participants used only the recommended models in the Notebook
skeleton, with them justifying:

“I would [still] try KNN, RF, Logistic Regression [with-
out your recommendation], because it’s a simple bi-
nary classification.” (P18, M, Notebook)

In summary, the results from the Notebook condition are not
surprising. Data scientists followed the general pattern of data
science workflow, as reported previously. This is a good result for us,
as it means that we have successfully replicated data scientists’ day-
to-day data science jobs in the lab environment. And by tracking
various behavioral and perceptional measures, we now have a solid
baseline condition to compare with data scientists’ new ways of
working with AutoDS.

5.5 Human-Crafted Models v.s. AutoDS-Built

Models

We use this section to delve deeply into the comparison of the two
ways of working for data scientists in building models. We start
with various comparison analyses [46] on the behavioral patterns
from participants, the outcomes of the task, and finally participant
attitudes.

5.5.1 AutoDS Creates More Models Faster and With Fewer Errors.
AutoDS easily generated 8 models with various algorithm and
hyperparameter combinations, whereas participants in Notebook
condition generated 3.13 (SD = 1.69) models (BN3). To provide a fair
comparison, participants in AutoDS examined in detail 3.8 (SD =
1.2) models out of the 8 models (BA6), which served as a candidate
set for participants to select the final model. That count (BA6) is
still a bit higher than those in Notebook condition (BN3), though
the 𝑡-test shows the difference is not signification 𝑡 (28) = 1.25,
𝑝 = .223.

About the accuracy of the final selected model, we first reiterate
that 7 participants in the Notebook condition reported either an
accuracy score other than the ROC AUC metric, or reported the
score from the same training data set; as a result, their scores can
not be considered valid model scores. We conducted an analysis of
variance (ANOVA) to compare the ROC AUC scores (B1) between
participants with AutoDS and only the successful Notebook par-
ticipants. When reporting ANOVA results, we included effect size
as partial 𝜂2, which is the proportion of variance accounted for by
each factor in the model, controlling for all other factors. To inter-
pret effect sizes, Miles & Shevlin [47] gives guidance that partial
𝜂2 ≥ .14 is a large effect, ≥ .06 is a medium-sized effect, and ≥ .01
is a small effect. Participants in AutoDS conndition generated and
selected the final model with higher ROC AUC scores (.919 SD=.01)
than participants with Notebooks (.899 SD=.02), 𝐹 [1, 21] = 7.61,
𝑝 = .01, partial 𝜂2 = .27. (B1)

As aforementioned, participants spent much more time in creat-
ing models in Notebook condition than in AutoDS condition. Even
if we compare the time participants spent on the first model in Note-
book (time to first model 15.0 SD=6.8 min) (BN2) v.s. the time they
generate 8 models in the AutoDS condition (config + run time 4.3
SD=1.0 min) (BA1+BA2), that difference is significant 𝑡 (14.7) = 6.0,
𝑝 < .001. One factor that contributes to this time difference is that
participants in the Notebook condition spent a significantly greater
amount of time searching for SK-learn library documentation on
the web (6.4 SD=2.6 min) than participants in the AutoDS condition
(0.93 SD=1.4) (B3), 𝑡 (21.3) = −7.2, 𝑝 < .001.

Given that participants had limited time in this study, we found
that many Notebook participants stopped when they felt they had
created a “good enough” model and the amount of time they needed
to continue to refine their model (e.g. by doing feature engineering)
was more than the remaining amount of time in the study. Partici-
pants with AutoDS had an easier time understanding which model
had the highest accuracy, but spent much of their time trying to
understand why. They leveraged the generated code to interpret
the decisions that AutoDS made in the process (BA7), and to evalu-
ate the legitimacy of the AutoDS system. After modifying a couple
hyperparameters of the model and seeing the expected changes
in accuracy score (BA8), one participant believed they understood
why AutoDS selected that particular set of hyperparameters and
decided:

“Ok, I get an explanation and break down as to what
decisions it made, I’ll tweak the cross validation then.”
(P4, F, AutoDS)

5.5.2
Factors That Led to More Accurate Models in AutoDS Con-
dition. To understand what factors led to participants choosing a

AutoDS: Towards Human-Centered Automation of Data Science

CHI ’21, May 8–13, 2021, Yokohama, Japan

partial 𝜂2 Direction

Factor
Years spent practicing data science (P1)
Prior experience with scikit-learn (P2)†
Prior experience with AutoDS (A1)†
Time on task**
Time spent searching for information (B3)*
Time configuring AutoDS (BA1)
Time examining the leaderboard (BA3)*
Time examining pipeline details (BA4)*
Number of pipelines examined (BA6)**
Number of pipeline’s code inspected (BA7)**
Number of pipeline’s code edited (BA8)

M
3.5
3.6
1.7
29.0 min
.93 min
2.3 min
9.7 min
3.9 min
3.8
2.3
.8
Table 2: Factors that predicted higher ROC AUC scores for AutoDS participants. Model adjusted 𝑅2 = .85. (* and **) Indicates
effects that are both significant (𝑝 ≤ .05, ≤ .01) and sizable (partial 𝜂2 ≥ .06). (†) Prior experience with scikit-learn and AutoDS
were rated on a 5-point scale (1=low, 5=high).

SD
2.7
1.1
.81
3.6 min
1.4 min
.96 min
3.4 min
2.2 min
1.2
.70
.94

t
.955
.798
-0.55
6.96
-3.19
2.17
3.51
4.34
7.89
5.33
-1.26

.47
.43
.28
.73
.77
< .01
.27
.15
.93
.87
.65

p
n.s.
n.s.
n.s.
.006
.05
n.s.
.04
.02
.004
.01
n.s.

+
+
-
+
-
+
+
+
+
+
-

more accurate model while working with AutoDS, we created a
regression model to predict ROC AUC scores in AutoDS condition
(B1). We included a variety of behavioral measures (B3, BA1, BA3,
BA4, BA6, BA7, BA8)5 and three self-reported background mea-
sures of data scientists (P1, P2, A1) to control for expertise effects:
years practicing data science, prior experience with scikit-learn
experience, and prior experience with AutDS. We also control how
long the participants took to complete the task. For AutoDS partici-
pants, we found a number of significant predictors of ROC AUC
score, detailed in Table 2.

We found that the longer participants searched online for docu-
mentation and task-related information in the AutoDS condition
(B3), the lower their accuracy score was in the selected model. We
observed a similar trend in the Notebook condition, where the more
experienced data scientists seemed to spend less time in searching
for documentation.

The longer participants spent in examining the leaderboard and
the pipeline details, the better the model they obtained (BA3, BA4).
This suggests that the more time and effort participants dedicated to
understanding the AutoDS model results, the better their outcome.
There were quite a few models generated by AutoDS, and partic-
ipants only needed to inspect a number of them in detail (BA6).
They may have been interested in viewing the pipeline’s code in
Notebook as well (BA7). Both behaviors led to higher accuracy in
the model they selected.

The behavior of changing pipeline code did not necessarily in-
crease the accuracy score (BA8), partially because participants
changed the code to make sure they could see corresponding changes
in model score, even if the score went down. That gave participants
reassurance of the AutoDS generated models.

5.5.3 Confidence in Human-Crafted Models. Other than the accu-
racy score of the final model, we also collected how confident each
participant was in their final selected model (A4) at the end of the
study. Overall ratings of confidence landed in the middle of the
scale (2.9 SD=.97 out of 5), suggesting that participants felt they
could have produced even better models if they had had more time.

5We added all the behavioral measures except BA5. Because BA5 is the AutoDS
computation time, which is a constant number in seconds, thus the model would treat
it as a constant and omit it.

Participants in AutoDS condition were significantly less confi-
dent (2.4 SD=.98) in their models than participants in the Notebook
condition (3.3 SD=.72), 𝑡 (25.7) = −2.9, 𝑝 < .01. An important con-
sideration here is that more confidence did not equate to better
models between the two conditions. This may suggest that while
the AutoDS users had more options to compare and study, their
counterparts in the Notebook condition were invested in writing
code from the start. This investment of time and effort into a model
could influence users’ perspectives when it came to how confident
they were. This is backed by the qualitative interview:

“I’m pretty confident that this is the best model I can
get with this dataset in 30 mins” (P18, M, Notebook)

5.5.4 Trust of AutoDS Increases After Use. To form a deeper un-
derstanding of people’s trust in AutoDS, we decomposed the trust
into various fine-grain dimensions. We asked AutoDS condition
participants to complete an survey developed by Hoffman et al. [27]
at the end of the study (AA5). The survey was intended to evaluate
trust in Explainable AI systems (XAI), and contains 8-items, each
with 5-point Likert scale responses ranging from Strongly Disagree
to Strongly Agree (Table 3). Factor analysis indicated that removing
item Q2 increased reliability of the scale, so we aggregated the final
scale by omitting this item.

Q1. I am confident in the AutoDS. I feel that it works well.
Q2. The outputs of the AutoDS are very predictable.
Q3. The AutoDS is very reliable. I can count on it to be correct all the
time.
Q4. I feel safe that when I rely on the [AutoDS] I will get the right
answers.
Q5. AutoDS is efficient in that it works very quickly.
Q6. I am wary of the AutoDS.
Q7. AutoDS can perform the task better than a novice human user.
Q8. I like using the system for decision making.

Table 3: XAI survey from [27]. With the removal of Q2, this
scale has acceptable reliability (Cronbach’s 𝛼 = 0.75).

Previous literature [27] suggests to use the XAI scales as an
overall score. The overall XAI score was quite positive (3.6 SD=.41
out of 5) (AA5). Participants found the AutoDS efficient, predictable,
and liked it for decision making.

CHI ’21, May 8–13, 2021, Yokohama, Japan

Wang and et al.

“It’s fast and it gives you visualisations to compare the
models, this saves me a lot of time” (P8, M, AutoDS)

However, participants wondered about the reliability of AutoDS,

trying to understand the rationale behind AutoDS’s decisions.

“I’m not sure about the features it created for me” (P5,
M, AutoDS), “It selected in one case random forest and
in another logistic regression, but why, can i choose
the classifier?” (P6, F, AutoDS)

In order to gauge how participants felt about AutoDS after having
used it, we measured their attitude toward AutoDS twice, once at
the beginning of the study (A2, A3) and once at the end (AA2, AA3).
Participants’ opinions generally did shift in a positive direction
after experiencing AutoDS. Participants had significantly higher
levels of trust in AutoDS after using it ( 3.5 SD=.74) than before
(2.7 SD=1.1), 𝑡 (24.4) = −2.3, 𝑝 = .03. This finding is consistent
with the finding that all 15 participants reported that they want
to adopt AutoDS in their day-to-day model building work after
the experiment. As to whether our participants felt that AutoDS
would one day replace data scientists, there was a small shift in
opinion toward feeling that it would, but it still falls in the negative
direction, and this difference was not significant (A3) (pre-study
2.4 SD=.83, post-study 2.9 SD=1.2, 𝑡 (25.3) = −1.4, 𝑝 = 𝑛.𝑠.).

6 DISCUSSION
Motivated by literature, we began the investigation on how data
scientists use an AutoDS system in DS tasks. Through our analyses,
we found that participants working with AutoDS not only can cre-
ate more models and faster, but more importantly these models are
at a higher accuracy and with fewer human-errors, comparing to
participants working with a Jupyter Notebook coding environment.
With AutoDS’s help, data scientists can afford to spend more time
in understanding the results and inspecting the codes generated by
AutoDS, whereas in the baseline condition participants had to spent
lots of their efforts and time in searching for library documenta-
tions and writing code from scratch. However, despite participants
acknowledge all these benefits from using AutoDS, participants still
trust their manually-crafted models more than the AutoDS gener-
ated model. These results suggest design improvement for AutoDS
technology, and enlighten us about the future of human-centered
automated data science work.

6.1 Design Implications
Our results suggested that AutoDS prototype’s code generation fea-
ture, which automatically translate a resulting model into human-
readable Python code, is very welcomed by the participants. They
could inspect the codes to see the decisions made by AutoDS. They
can rationalize some of those decisions, and they could even change
the code and see if that changes the result as expected. Furthermore,
we observed there are a couple participants who kept working on
the code generated by AutoDS to achieve a better model, that use
case again supports our claim that data scientists and AutoDS to-
gether could deliver better results. Other than the possible use of
the entire code, data scientists could also easily migrate or replicate
a segment of the generated code, maybe with a transformation

function, some hyperparameters, or other decisions made by Au-
toDS into a different task and context. Thus, we highly recommend
AutoDS systems to provide such code-generation feature.

We also learned that participants have trouble in trusting AutoDS
system, and they have more confidence in their worse-performing
manually-created models than in the higher quality AutoDS-generated
models, thus the explainable and trustworthy AutoDS research
agenda should be prioritized. The fact that people loved the code-
generation feature could also attribute to the needs of a transparent
and trusted AutoDS. Participants wanted to understand not only
what the decisions are (code-generation satisfied that), but also why
AutoDS made such decisions. However, only the generated code
for the result model was not enough. We need to design AutoDS
systems to provide better explanation for why certain decisions
were made. For example, when showing users the top performed al-
gorithm and the optimized set of hyperparameter values, we could
also reveal the information about all the other candidates consid-
ered, and why AutoDS eliminated those options. Another example
is that for the new features engineered by AutoDS, users should
have a better view of how those features are generated and chosen
in the middle of the feature engineering process, because users may
have some domain knowledge to prevent the nonsense transforma-
tions (e.g., absolute value of gender). We are glad to see that recently
there are some researchers moving toward this direction [13]. We
should also caution readers that we are not arguing to show a user
all the information AutoDS has. Full transparency causes informa-
tion overload and is no better than no transparency. This links back
to a well-established CSCW theory: “social translucent” [15], where
researchers argue to show the information at the right moment and
at the right level of details, and we should learn from our past.

6.2 Human-Centered Automated Data Science
Our result showed that some participants worked over a wider
range of AutoDS results, some others focused on improving the
code of one particular model (BN6 and BA7). The more successful
participants were the ones who tried a wider range of approaches
rather than focusing all of their attention on just one model. This
finding echoed wisdom in the artistic domain, where the most cre-
ative works emerge when favoring quantity over quality [4]. And
this was not isolated that the data science work appears linked
with a practice in the art design domain, as [48] argued that “Data
science workers work is a new type of crafting”, while in the Au-
toDS context, users are crafting the AutoDS generated results as a
material.

This collaborative step that emerged over practice, between data
scientist and the AutoDS, appears to us as a demythification of
the AutoDS replacing the job of the data scientist, and instead,
offers a perspective on how the relationship between this parties
towards a collaborative partnership is maturing. We argue that the
Human-AI Collaboration [65] paradigm is also emerging in the
data science domain, as in many other domains (e.g., education [69],
healthcare [64], human resource [57]). In the data science context,
human data scientists can work on the understanding of domain
knowledge and context tasks, while AutoDS systems work on the
computation tasks. Then human data scientists can work on result
interpretation tasks, while AutoDS can work on deployment tasks.

AutoDS: Towards Human-Centered Automation of Data Science

CHI ’21, May 8–13, 2021, Yokohama, Japan

They hand over information to each other and share responsibility.
Together, AutoDS plays a partner role in the future of human-
centered automated data science practice.

We believe AutoDS technologies will become available to more
and more data scientists, affording them a chance to craft models
together with AI. We agree with the majority of our participants
that AutoDS will not replace data scientists’ job. Instead, a human-
centered AutoDS will play a more important assistant role to the
human data scientists in new paradigm of data scietist’s work.

In this new paradigm, data scientists’ role will shift, as suggested
by our findings: they can spend less time in the tedious model
training and hyperparameter tuning tasks, but spend more time in
understanding the data, communicating with the domain experts,
and selecting the model to best fit the domain and context. In
addition, they will need to spend a big proportion of their time to
rationalize the AutoDS decisions and results, and translate those
decisions to present to other human stakeholders.

In this new paradigm, not only professional data scientists but
also end-customers and domain experts can also build ML solutions
to answer their own simple questions with their data. Our system
provide a GUI and an automatically export human-readable note-
book in automatically models building. The future in which various
human stakeholders can work together with AutoDS is promising,
but there are works needed to be done to achieve that vision. This
work is just the first step.

6.3 Limitations
We acknowledge our works limitations as follows: this is a lab ex-
periment study, thus it has the common limitations as any other
lab experiments [17]. For example, in the Notebook Condition, we
provided participants a code skeleton, which may also bias their
behaviors in that condition. Another example is that the task and
data set is tailored as simple and specific tasks to emphasize model
building, thus the reported behaviors of participants interacting
with AutoDS may be different from when they actually adopt Au-
toDS in their daily DS projects. In an actual data science project,
data acquisition and curation are more significant problems for data
scientists during the ML lifecycle. The AutoDS system presented in
this paper is just the starting point of a long-term project. We have
another paper in submission that discusses human-AI collaboration
in automated data preparation and curation process. In that work,
we used reinforcement learning instead of AutoML, because it is
better suited for the task.

Another limitation is that all the participants are professional
data scientists coming from a technology company, there may be
some selection bias in their background. For example, on average,
they rated themselves as moderate level of data science expertise.
This participant population may also impose limitations on the
generalizability of our paper’s findings. We call out these external
validity limitations to the reader who plan to apply the findings to
other contexts.

7 CONCLUSION
Our work presents an AutoDS prototype system, and a between-
subject user experiment with with 30 professional data scientists to
use the AutoDS system, or use python in notebooks, to complete

a data science model building task. Grounded in the results, we
present design implications for building AutoDS systems to better
incorporate into human workflows. We also discuss future research
directions for human-centered automated data science.

ACKNOWLEDGMENTS
We thank all the participants who contributed their time participate
our experiment. We want to specifically thank Arunima Chaudhary,
Abel Valente, and Carolina Spina for implementing the presented
AutoDS system. We also want to thank Alex Swain, Dillon Evers-
man, Voranouth Supadulya, and Daniel Karl for their inspiring
UX design sketches, and Gregory Bramble, Theodoros Salonidis,
Peter Kirchner, and Alex Gray for their input on the algorithm
implementation.

REFERENCES
[1] Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza. 2014.
Power to the people: The role of humans in interactive machine learning. AI
Magazine 35, 4 (2014), 105–120.

[2] Saleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi,
Penny Collisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen, et al. 2019.
Guidelines for human-AI interaction. In Proceedings of the 2019 CHI Conference
on Human Factors in Computing Systems. ACM, 3.

[3] Stuart Armstrong and Kaj Sotala. 2015. How we’re predicting AI–or failing to.

In Beyond artificial intelligence. Springer, 11–29.

[4] David Bayles and Ted Orland. 2001. Art & fear: Observations on the perils (and

rewards) of artmaking. Image Continuum Press.

[5] Catherine L Blake and Christopher J Merz. 1998. UCI repository of machine

learning databases, 1998.

[6] Murray Campbell, A Joseph Hoane Jr, and Feng-hsiung Hsu. 2002. Deep blue.

Artificial intelligence 134, 1-2 (2002), 57–83.

[7] Houston Claure, Yifang Chen, Jignesh Modi, Malte Jung, and Stefanos Niko-
laidis. 2019. Reinforcement Learning with Fairness Constraints for Resource
Distribution in Human-Robot Teams. arXiv preprint arXiv:1907.00313 (2019).
[8] Justin Cranshaw, Emad Elwany, Todd Newman, Rafal Kocielnik, Bowen Yu,
Sandeep Soni, Jaime Teevan, and Andrés Monroy-Hernández. 2017. Calendar.
help: Designing a workflow-based scheduling agent with humans in the loop.
In Proceedings of the 2017 CHI Conference on Human Factors in Computing
Systems. ACM, 2382–2393.

[9] Tommy Dang, Fang Jin, et al. 2018. Predict saturated thickness using tensorboard
visualization. In Proceedings of the Workshop on Visualisation in Environmental
Sciences. Eurographics Association, 35–39.

[10] DataRobot. [n.d.]. Automated Machine Learning for Predictive Modeling. Re-

trieved 3-April-2019 from https://www.datarobot.com/

[11] David Donoho. 2017. 50 years of data science. Journal of Computational and

Graphical Statistics 26, 4 (2017), 745–766.

[12] Paul Dourish and Victoria Bellotti. 1992. Awareness and coordination in shared

workspaces.. In CSCW, Vol. 92. 107–114.

[13] Jaimie et al. Drozdal. 2020. Exploring Information Needs for Establishing Trust

in Automated Data Science Systems. In IUI’20. ACM, in press.

[14] EpistasisLab. [n.d.].

tpot. Retrieved 3-April-2019 from https://github.com/

EpistasisLab/tpot

[15] Thomas Erickson and Wendy A Kellogg. 2000. Social translucence: an ap-
proach to designing systems that support social processes. ACM transactions on
computer-human interaction (TOCHI) 7, 1 (2000), 59–83.

[16] Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Springenberg, Manuel
Blum, and Frank Hutter. 2015. Efficient and robust automated machine learning.
In Advances in Neural Information Processing Systems. 2962–2970.

[17] Darren Gergle and Desney S Tan. 2014. Experimental research in HCI. In Ways

of Knowing in HCI. Springer, 191–227.

[18] Yolanda Gil, James Honaker, Shikhar Gupta, Yibo Ma, Vito D’Orazio, Daniel
Garijo, Shruti Gadewar, Qifan Yang, and Neda Jahanshad. 2019. Towards human-
guided machine learning. In Proceedings of the 24th International Conference
on Intelligent User Interfaces. ACM, 614–624.

[19] Google. [n.d.]. Cloud AutoML. Retrieved 3-April-2019 from https://cloud.google.

com/automl/

[20] Google. [n.d.]. Colaboratory. Retrieved 3-April-2019 from https://colab.research.

google.com

[21] Jonathan Grudin. 2009. AI and HCI: Two fields divided by a common focus. Ai

Magazine 30, 4 (2009), 48–48.

CHI ’21, May 8–13, 2021, Yokohama, Japan

Wang and et al.

[22] Philip J Guo, Sean Kandel, Joseph M Hellerstein, and Jeffrey Heer. 2011. Proactive
wrangling: mixed-initiative end-user programming of data transformation scripts.
In Proceedings of the 24th annual ACM symposium on User interface software
and technology. ACM, 65–74.

[23] H2O. [n.d.]. H2O. Retrieved 3-April-2019 from https://h2o.ai
[24] Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han. 2018. Amc: Au-
toml for model compression and acceleration on mobile devices. In Proceedings
of the European Conference on Computer Vision (ECCV). 784–800.

[25] Jeffrey Heer and Ben Shneiderman. 2012. Interactive dynamics for visual analysis.

Queue 10, 2 (2012), 30.

[26] Jeffrey Heer, Fernanda B Viégas, and Martin Wattenberg. 2007. Voyagers and
voyeurs: supporting asynchronous collaborative information visualization. In
Proceedings of the SIGCHI conference on Human factors in computing systems.
ACM, 1029–1038.

[27] Robert R Hoffman, Shane T Mueller, Gary Klein, and Jordan Litman. 2018. Metrics
for explainable AI: Challenges and prospects. arXiv preprint arXiv:1812.04608
(2018).

[28] Eric Horvitz. 1999. Principles of mixed-initiative user interfaces. In Proceedings
of the SIGCHI conference on Human Factors in Computing Systems. 159–166.

[29] Youyang Hou and Dakuo Wang. 2017. Hacking with NPOs: collaborative an-
alytics and broker roles in civic data hackathons. Proceedings of the ACM on
Human-Computer Interaction 1, CSCW (2017), 53.

[30] Project Jupyter. [n.d.]. Jupyter Notebook. Retrieved 3-April-2019 from https:

//jupyter.org

[31] Project Jupyter. [n.d.].

JupyterLab.

https://www.github.com/jupyterlab/

jupyterlab

[32] Kaggle. 2017. The State of Data Science & Machine Learning. https://www.

kaggle.com/kaggle/kaggle-survey-2017

[33] Kaggle. 2018. Kaggle Data Science Survey 2018. Retrieved 17-September-2019

from https://www.kaggle.com/sudhirnl7/data-science-survey-2018/

[34] Sean Kandel, Andreas Paepcke, Joseph Hellerstein, and Jeffrey Heer. 2011. Wran-
gler: Interactive visual specification of data transformation scripts. In Proceedings
of the SIGCHI Conference on Human Factors in Computing Systems. ACM,
3363–3372.

[35] James Max Kanter and Kalyan Veeramachaneni. 2015. Deep feature synthesis: To-
wards automating data science endeavors. In 2015 IEEE International Conference
on Data Science and Advanced Analytics (DSAA). IEEE, 1–10.

[36] Matthew Kay, Shwetak N Patel, and Julie A Kientz. 2015. How good is 85%?
A survey tool to connect classifier evaluation to acceptability of accuracy.
In Proceedings of the 33rd Annual ACM Conference on Human Factors in
Computing Systems. 347–356.

[37] Mary Beth Kery, Marissa Radensky, Mahima Arya, Bonnie E John, and Brad A
Myers. 2018. The story in the notebook: Exploratory data science using a liter-
ate programming tool. In Proceedings of the 2018 CHI Conference on Human
Factors in Computing Systems. ACM, 174.

[38] Udayan Khurana, Deepak Turaga, Horst Samulowitz, and Srinivasan Parthasrathy.
2016. Cognito: Automated feature engineering for supervised learning. In 2016
IEEE 16th International Conference on Data Mining Workshops (ICDMW). IEEE,
1304–1307.

[39] Peter Krensky, Pieter den Harner, Erick Brethenoux, Jim Hare, Svetlana Sicular,
and Shubhangi Vashisth. 2020. Magic Quadrant for data science and machine-
learning platforms. Gartner, Inc (2020).

[40] Hoang Thanh Lam, Johann-Michael Thiebaut, Mathieu Sinn, Bei Chen, Tiep Mai,
and Oznur Alkan. 2017. One button machine for automating feature engineering
in relational databases. arXiv preprint arXiv:1706.00327 (2017).

[41] Doris Jung-Lin Lee, Stephen Macke, Doris Xin, Angela Lee, Silu Huang, and
Aditya Parameswaran. 2019. A Human-in-the-loop Perspective on AutoML:
Milestones and the Road Ahead. Data Engineering (2019), 58.

[42] Fei-Fei Li. 2018. How to Make A.I. That’s Good for People. The New York Times
(7 March 2018). Retrieved 3-April-2019 from https://www.nytimes.com/2018/03/
07/opinion/artificial-intelligence-human.html

[43] Sijia Liu, Parikshit Ram, Deepak Vijaykeerthy, Djallel Bouneffouf, Gregory Bram-
ble, Horst Samulowitz, Dakuo Wang, Andrew Conn, and Alexander G Gray. 2020.
An ADMM Based Framework for AutoML Pipeline Configuration.. In AAAI.
4892–4899.

[44] Yaoli Mao, Dakuo Wang, Michael Muller, Kush Varshney, Ioana Baldini, Casey
Dugan, and Aleksandra Mojsilovic. 2020. How Data Scientists Work Together
With Domain Experts in Scientific Collaborations. In Proceedings of the 2020
ACM conference on GROUP. ACM.

[45] John Markoff. 2011. Computer wins on jeopardy!: trivial, it is not. New York

Times 16 (2011).

[46] John H McDonald. 2009. Handbook of biological statistics. Vol. 2. sparky house

publishing Baltimore, MD.

[47] Jeremy Miles and Mark Shevlin. 2001. Applying regression and correlation: A

guide for students and researchers. Sage.

[48] Michael Muller, Ingrid Lange, Dakuo Wang, David Piorkowski, Jason Tsay, Q. Vera
Liao, Casey Dugan, and Thomas Erickson. 2019. How Data Science Workers
Work with Data: Discovery, Capture, Curation, Design, Creation. In Proceedings

of the 2019 CHI Conference on Human Factors in Computing Systems (Glasgow,
UK) (CHI ’19). ACM, New York, NY, USA, Forthcoming.

[49] Clifford Nass, Jonathan Steuer, and Ellen R Tauber. 1994. Computers are social
actors. In Proceedings of the SIGCHI conference on Human factors in computing
systems. ACM, 72–78.

[50] Randal S Olson and Jason H Moore. 2016. TPOT: A tree-based pipeline optimiza-
tion tool for automating machine learning. In Workshop on Automatic Machine
Learning. 66–74.

[51] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel,
Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss,
Vincent Dubourg, et al. 2011. Scikit-learn: Machine learning in Python. Journal
of machine learning research 12, Oct (2011), 2825–2830.

[52] David Piorkowski, Soya Park, April Yi Wang, Dakuo Wang, Michael Muller, and
Felix Portnoy. 2021. How AI Developers Overcome Communication Challenges
in a Multidisciplinary Team: A Case Study. In Proceedings of the CSCW 2021.

[53] Tye Rattenbury, Joseph M Hellerstein, Jeffrey Heer, Sean Kandel, and Con-
nor Carreras. 2017. Principles of data wrangling: Practical techniques for data
preparation. " O’Reilly Media, Inc.".

[54] Quentin Roy, Futian Zhang, and Daniel Vogel. 2019. Automation accuracy is
good, but high controllability may be better. In Proceedings of the 2019 CHI
Conference on Human Factors in Computing Systems. 1–8.

[55] Armand Ruiz. 2017. The 80/20 data science dilemma. InfoWorld (2017).
[56] Katharine Schwab. 2018.

Google’s Rules For Designers Working With
AI. https://www.fastcompany.com/90132700/googles-rules-for-designing-ai-
that-isnt-evil

[57] Ameneh Shamekhi, Q Vera Liao, Dakuo Wang, Rachel KE Bellamy, and Thomas
Erickson. 2018. Face Value? Exploring the effects of embodiment for a group
facilitation agent. In Proceedings of the 2018 CHI Conference on Human Factors
in Computing Systems. ACM, 391.

[58] Ben Shneiderman and Pattie Maes. 1997. Direct manipulation vs. interface agents.

interactions 4, 6 (1997), 42–61.

[59] Matt Simon. 2018. Forget the Robot Singularity Apocalypse. Let’s Talk About
the Multiplicity. https://www.wired.com/story/forget-the-robot-singularity-
apocalypse-lets-talk-about-the-multiplicity/

[60] Jennifer Sukis. 2019. AI Design & Practices Guidelines. https://medium.com/

design-ibm/ai-design-guidelines-e06f7e92d864

[61] Charles Sutton, Timothy Hobson, James Geddes, and Rich Caruana. 2018. Data
diff: Interpretable, executable summaries of changes in distributions for data
wrangling. In Proceedings of the 24th ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining. ACM, 2279–2288.

[62] Haodan Tan, Dakuo Wang, and Selma Sabanovic. 2018. Projecting Life Onto
Robots: The Effects of Cultural Factors and Design Type on Multi-Level Evalua-
tions of Robot Anthropomorphism. In 2018 27th IEEE International Symposium
on Robot and Human Interactive Communication (RO-MAN). IEEE, 129–136.

[63] Dakuo Wang, Q. Vera Liao, Yunfeng Zhang, Udayan Khurana, Horst Samulowitz,
Soya Park, Michael Muller, and Lisa Amini. 2021. How Much Automation Does a
Data Scientist Want?. In pre-print.

[64] Dakuo Wang, Liuping Wang, Zhan Zhang, Ding Wang, Haiyi Zhu, Yvonne Gao,
Xiangmin Fan, and Feng Tian. 2021. Brilliant AI Doctor in Rural China: Tensions
and Challenges in AI-Powered CDSS Deployment. In Proceedings of the CHI
2021.

[65] Dakuo Wang, Justin D. Weisz, Michael Muller, Parikshit Ram, Werner Geyer,
Casey Dugan, Yla Tausczik, Horst Samulowitz, and Alexander Gray. 2019. Human-
AI Collaboration in Data Science: Exploring Data Scientists’ Perceptions of Auto-
mated AI. To appear in Computer Supported Cooperative Work (CSCW) (2019).
[66] Fei-Yue Wang, Jun Jason Zhang, Xinhu Zheng, Xiao Wang, Yong Yuan, Xiaoxiao
Dai, Jie Zhang, and Liuqing Yang. 2016. Where does AlphaGo go: From church-
turing thesis to AlphaGo thesis and beyond. IEEE/CAA Journal of Automatica
Sinica 3, 2 (2016), 113–120.

[67] Qianwen Wang, Yao Ming, Zhihua Jin, Qiaomu Shen, Dongyu Liu, Micah J Smith,
Kalyan Veeramachaneni, and Huamin Qu. 2019. Atmseer: Increasing transparency
and controllability in automated machine learning. In Proceedings of the 2019
CHI Conference on Human Factors in Computing Systems. ACM, 681.

[68] Daniel Karl I Weidele, Justin D Weisz, Erick Oduor, Michael Muller, Josh Andres,
Alexander Gray, and Dakuo Wang. 2020. AutoAIViz: opening the blackbox of auto-
mated artificial intelligence with conditional parallel coordinates. In Proceedings
of the 25th International Conference on Intelligent User Interfaces. 308–312.
[69] Ying Xu, Dakuo Wang, Penelope Collins, Hyelim Lee, and Mark Warschauer.
[n.d.]. Same benefits, different communication patterns: Comparing Children’s
reading with a conversational agent vs. a human partner. Computers & Education
161 ([n. d.]), 104059.

[70] Amy X Zhang, Michael Muller, and Dakuo Wang. 2020. How do data science
workers collaborate? roles, workflows, and tools. Proceedings of the ACM on
Human-Computer Interaction 4, CSCW1 (2020), 1–23.

[71] Marc-André Zöller and Marco F Huber. 2019. Survey on Automated Machine

Learning. arXiv preprint arXiv:1904.12054 (2019).

