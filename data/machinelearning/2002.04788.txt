2
2
0
2

r
p
A
4
1

]

G
L
.
s
c
[

4
v
8
8
7
4
0
.
2
0
0
2
:
v
i
X
r
a

To Split or Not to Split:
The Impact of Disparate Treatment in Classiﬁcation

Hao Wang, Hsiang Hsu, Mario Diaz, Flavio P. Calmon∗

Abstract

Disparate treatment occurs when a machine learning model yields diﬀerent decisions for individuals
In domains where prediction accuracy is paramount,
based on a sensitive attribute (e.g., age, sex).
it could potentially be acceptable to ﬁt a model which exhibits disparate treatment. To evaluate the
eﬀect of disparate treatment, we compare the performance of split classiﬁers (i.e., classiﬁers trained
and deployed separately on each group) with group-blind classiﬁers (i.e., classiﬁers which do not use a
sensitive attribute). We introduce the beneﬁt-of-splitting for quantifying the performance improvement
by splitting classiﬁers. Computing the beneﬁt-of-splitting directly from its deﬁnition could be intractable
since it involves solving optimization problems over an inﬁnite-dimensional functional space. Under
diﬀerent performance measures, we (i) prove an equivalent expression for the beneﬁt-of-splitting which
can be eﬃciently computed by solving small-scale convex programs; (ii) provide sharp upper and lower
bounds for the beneﬁt-of-splitting which reveal precise conditions where a group-blind classiﬁer will
always suﬀer from a non-trivial performance gap from the split classiﬁers. In the ﬁnite sample regime,
splitting is not necessarily beneﬁcial and we provide data-dependent bounds to understand this eﬀect.
Finally, we validate our theoretical results through numerical experiments on both synthetic and real-
world datasets.

Index Terms— Trustworthy machine learning, fairness, domain adaptation, f -divergence, converse bounds.

1

Introduction

A machine learning (ML) model exhibits disparate treatment [1] if it treats similar data points from distinct
individuals diﬀerently based on a sensitive attribute (e.g., age, sex).
In applications such as hiring, the
existence of disparate treatment can be illegal [2]. However, in settings such as healthcare, it can be legal
and ethical to ﬁt a model which presents disparate treatment in order to improve prediction accuracy [3–5].
For example, the Equal Credit Opportunity Act (ECOA) permits a creditor to use an applicant’s age and
income for analyzing credit, as long as such information is used in a fair manner (see 12 CFR §1002.6(b)(2)
in [6]).

The role of a sensitive attribute in fair classiﬁcation can be understood through several metrics and
principles. When a ML model is deployed in practice, fairness can be quantiﬁed in terms of the performance
disparity conditioned on a sensitive attribute, such as statistical parity [7] and equalized odds [8]. In domains
where the goal is to predict accurately (e.g., medical diagnostics), non-maleﬁcence (i.e., “do no harm”) and
beneﬁcence (i.e., “do good”) [9] become more appropriate moral principles for fairness [10–12]. Accordingly,
a ML model should avoid the causation of harm and be as accurate as possible on each protected group.

The relationship between achieving the above-mentioned principles and allowing a classiﬁer to exhibit
disparate treatment is complex. On the one hand, using a group-blind classiﬁer (i.e., a classiﬁer that does
not use the sensitive attribute as an input feature) may cause harm unintentionally since model performance
relies on the distribution of the input data [10, 13–15]. This probability distribution can vary signiﬁcantly
conditioned on a sensitive attribute due to, for example, inherent diﬀerences between groups [13], diﬀerences
in labeling [16], and diﬀerences in sampling [17]. On the other hand, training a separate classiﬁer for each
protected group—a setting we refer to as splitting classiﬁers—does not necessarily guarantee non-maleﬁcence

∗H. Wang, H. Hsu, and F. P. Calmon are with Harvard University (e-mail: {hao wang,hsianghsu}@g.harvard.edu;

ﬂavio@seas.harvard.edu).
M. Diaz is with the Instituto de Investigaciones en Matem´aticas Aplicadas y en Sistemas (IIMAS), Universidad Nacional
Aut´onoma de M´exico (e-mail: mario.diaz@sigma.iimas.unam.mx).

1

 
 
 
 
 
 
Figure 1: The taxonomy of splitting based on two diﬀerent factors. Samples from two groups are depicted in red
and blue, respectively, and their labels are represented by +, −. Each group’s labeling function is shown with the
corresponding color and the arrows indicate the regions where the points are labeled as +. Information-theoretically,
splitting classiﬁers beneﬁts model performance the most if the labeling functions are diﬀerent and the unlabeled
distributions are similar (yellow region).

when sample size is limited [18]: groups with insuﬃcient samples may incur a high generalization error and
suﬀer from overﬁtting.

We consider two questions that are central to understanding non-maleﬁcence and beneﬁcence through

the use of a sensitive attribute by a ML model:

(i) When is it beneﬁcial to split classiﬁers in terms of model performance?

(ii) When splitting is beneﬁcial, how much do the split classiﬁers outperform a group-blind classiﬁer?

First, we show that in the information-theoretic regime where the underlying distribution is known—or,
equivalently, an arbitrarily large number of samples are available—splitting never harms any group in terms of
average performance metrics. Thus, splitting will naturally follow the non-maleﬁcence principle in the large-
sample regime. Second, we introduce a notion called the beneﬁt-of-splitting which measures the performance
improvement by splitting classiﬁers compared to using a group-blind classiﬁer across all groups. The beneﬁt-
of-splitting is also an information-theoretic quantity as it only relies on the underlying data distribution
rather than number of samples or hypothesis class.

The deﬁnition of the beneﬁt-of-splitting involves a model performance measure and, hence, we divide
our analyses into two parts based on diﬀerent choices of this measure.
In Section 3, we quantify model
performance in terms of standard loss functions (e.g., (cid:96)1 and cross entropy loss). For the beneﬁt-of-splitting
under these loss functions, we provide sharp upper and lower bounds (Theorem 1) that capture when
splitting classiﬁers beneﬁts model performance the most. These bounds indicate two factors (see Figure 1
for an illustration) which are central to the beneﬁt-of-splitting: (i) disagreement between labeling functions1,
(ii) similarity between unlabeled distributions1. Based on these two factors, our upper bounds in Theorem 1
indicate that splitting does not produce much beneﬁt if the labeling functions are similar or the unlabeled
distributions are diﬀerent; our lower bounds in Theorem 1 indicate that splitting beneﬁts the most if two
groups’ labeling functions are diﬀerent and unlabeled distributions are similar. Furthermore, our lower
bounds in Theorem 1 lead to an impossibility (i.e., converse) result for group-blind classiﬁers: under certain

1 We borrow the terms “labeling function” and “unlabeled distribution” from the domain adaptation literature [19, 20].
The labeling function takes a data point as an input and produces a probability of its binary label being 1 and the unlabeled
distribution is a (marginal) probability distribution of the unlabeled data. Furthermore, the labeling function can be viewed as
a “channel” (i.e., conditional distribution) in the information theory parlance. The formal deﬁnitions are given in Section 1.2.

2

Similarunlabeleddistributions(P0⇡P1)Di↵erentunlabeleddistributions(P06⇡P1)Similarlabelingfunctions(y0⇡y1)Di↵erentlabelingfunctions(y06⇡y1)precise conditions, using a group-blind classiﬁer will always suﬀer from an inherent accuracy trade-oﬀ between
diﬀerent groups and splitting classiﬁers can reconcile this issue. This converse result is information-theoretic:
a data scientist cannot overcome this limit by using more samples or altering the hypothesis class.

In Section 4, we consider false error rate as a performance measure since in applications such as medical
diagnostics, high false error rate could result in unintentional harm [21]. Under this metric, computing the
beneﬁt-of-splitting directly from its deﬁnition may at ﬁrst seem intractable since it involves an optimization
over an inﬁnite-dimensional functional space. Nonetheless, we prove that the beneﬁt-of-splitting under false
error rate has an equivalent, dual expression (Theorem 2) which only requires solving two small-scale convex
programs. Furthermore, the objective functions of these convex programs have closed-form supergradients
(Proposition 2). Combining these two results leads to an eﬃcient procedure (Algorithm 1) for computing
the beneﬁt-of-splitting. We validate our procedure through numerical experiments on synthetic datasets in
Section 6.1. When the underlying data distribution is known, our procedure has a provable convergence
guarantee and returns the precise values of the beneﬁt-of-splitting. When the underlying data distribution
is unknown, our procedure may suﬀer from approximation errors but still outperforms more naive empirical
approaches.

The aforementioned results capture the beneﬁt-of-splitting from an information-theoretic perspective
where the underlying data distributions are assumed to be known and the space of potential classiﬁers is
unrestricted. In Section 5, we consider the eﬀect of splitting classiﬁers in a more practical setting where
group-blind and split classiﬁers are restricted over the same hypothesis class (e.g., logistic regressions) and the
underlying distribution is accessed only through ﬁnitely many i.i.d. samples. In this case, splitting classiﬁers
is not necessarily beneﬁcial since the group with less samples may suﬀer from overﬁtting. To quantify the
eﬀect of splitting classiﬁers, we analyze the sample-limited beneﬁt-of-splitting. We derive upper and lower
bounds for the beneﬁt-of-splitting in this regime in Theorem 4. These bounds disentangle three factors which
determine the eﬀect of splitting classiﬁers in practice: (i) disagreement between optimal (split) classiﬁers
and training error associated with these optimal classiﬁers; (ii) similarity between (empirical) unlabeled
distributions; and (iii) model complexity and number of samples. The ﬁrst two factors are analogous to
the ones that aﬀect the beneﬁt-of-splitting in the information-theoretic regime: when the hypothesis class
is complex enough and the sample size tends to inﬁnity, the optimal classiﬁers approximate the labeling
functions and the empirical unlabeled distributions converge to the true unlabeled distributions. Finally, we
illustrate how these factors determine the performance impact of splitting classiﬁers through experiments on
40 datasets downloaded from OpenML [22].

The proof techniques of this paper are based on fundamental tools found in statistics, such as Brown-
Low’s two-points lower bound [23], and methods in convex analysis, such as Ky Fan’s min-max theorem
[24]. These tools are widely used in applications such as non-parametric estimation [25], and are useful
for analyzing the min-max risk in statistical settings [26–30]. Furthermore, the factors that we provide
for understanding the eﬀect of splitting classiﬁers are inspired by the necessary and suﬃcient conditions of
domain adaptation learnability in Ben-David et al.
[31].
The rest of this paper is organized as follows. In the remainder of this section, we review related works
and present notation adopted in this paper. In Section 2, we introduce the main object of interest: the
beneﬁt-of-splitting. Under diﬀerent performance measures, we provide upper and lower bounds for the
beneﬁt-of-splitting in Section 3 and present an eﬃcient procedure for computing the beneﬁt-of-splitting in
Section 4. The eﬀect of splitting classiﬁers in the ﬁnite sample regime is studied in Section 5. Finally, we
illustrate our results through numerical experiments in Section 6 and provide conclusion remarks and future
works in Section 7.

1.1 Related Work

Privacy. Fairness and privacy are closely connected and central to trustworthy machine learning. In this
paper, we study the impact of disparate treatment from an information-theoretic perspective: we assume
the underlying data distribution is known and analyze how the diﬀerent distributions between groups aﬀect
the performance improvement by splitting classiﬁers. In this regard, our present work relates with studies
on information-theoretic privacy [see e.g., 32–38]. These eﬀorts explore the fundamental limits of privacy-
utility trade-oﬀs by also assuming the underlying data distribution is known. For example, Makhdoumi et
al.
[39] introduce privacy funnel method for solving the privacy-utility trade-oﬀs and connect it with the

3

information bottleneck [40], and this connection is further studied in [41]. Kairouz et al.
[42] study the
trade-oﬀs between local diﬀerential privacy [30, 43, 44] and utility functions measured by f -divergence [45].
Besides analyzing the fundamental limits, there are works [see e.g., 46–49] on designing privacy mechanisms
which enable a certain level of utility to be obtained from the disclosed datasets while controlling private
information leakage. The robustness of the privacy mechanisms is analyzed in [50, 51] when these privacy
mechanisms are constructed by using ﬁnitely many samples. We follow a similar line of analysis in order to
understand the eﬀect of splitting classiﬁers in the ﬁnite sample regime and complement our bounds for the
beneﬁt-of-splitting by incorporating additional factors such as sample size and model complexity.

Domain adaptation. A standard assumption in ML is that the training and testing data are drawn
from the same underlying probability distribution. Domain adaptation [19, 20, 52] and transfer learning
[53, 54] consider a more general setting where models are trained on a source domain and deployed on
a (diﬀerent) target domain. A common assumption therein is known as covariate shift, which requires
the source and target domain share the same labeling function. In this paper, we prove (see Theorem 1)
that if the covariate shift assumption is violated and two groups’ unlabeled distributions are similar, then
In this regard, our work is connected to Ben-David et
no classiﬁer can perform well on both groups.
al. [31] which present impossibility results on domain adaptation learnability. Compared to [31], Theorem 1
characterizes an information-theoretic fundamental limit which cannot be circumvented by using a large
number of samples or a carefully designed hypothesis class. Furthermore, the lower bound in Theorem 1
serves as a complementary statement to the upper bounds in domain adaptation [cf. 19, 20]. These bounds
jointly describe the range of the loss a data scientist may incur by training a model on the source domain
and deploying on the target domain.

Fair ML. ML models have been increasingly used in applications of individual-level consequences, ranging
from recidivism prediction [55] and lending [56] to healthcare [57]. A number of works in fair ML aim
at understanding why discrimination happens [58–66]; how it can be quantiﬁed [67–69]; and how it can
be reduced [70–79]. There are also an increasing number of studies that take causality into account for
understanding and mitigating discrimination [80–84]. We build on a line of recent results on decoupling
predictive models for improving accuracy-fairness trade-oﬀs [see e.g., 10, 12, 13, 85, 86]. For example, Ustun et
al. [10] introduce a tree structure to recursively choose sensitive attributes for decoupling. Lipton et al. [85]
show that using group-blind classiﬁers could be suboptimal in terms of trading oﬀ accuracy and fairness.
The work closest to ours is Dwork et al.
[13] which present a decoupling technique to learn separate models
for diﬀerent groups. A detailed comparison with [13] is given in Section 5.2.

1.2 Notation and Deﬁnitions

Consider a binary classiﬁcation task (e.g., detecting pneumonia from X-rays) where the goal is to learn a
probabilistic classiﬁer h :
using
. We assume there is an additional binary2 sensitive attribute
input features (e.g., chest X-rays) X
(e.g., sex) S
that does not belong to the input features X. We denote the unlabeled probability
0, 1
}
distributions of input features conditioned on the sensitive attribute by

[0, 1] that predicts a label (e.g., presence of pneumonia) Y

X →

∈ X

∈ {

∈ {

0, 1

}

The labeling functions of the two groups are denoted by

P0 (cid:44) PX|S=0, P1 (cid:44) PX|S=1.

y0(x) (cid:44) PY |X,S(1
|

x, 0),

y1(x) (cid:44) PY |X,S(1

x, 1).
|

In order to measure the diﬀerence between two unlabeled distributions (i.e., P0 and P1), we recall Csisz´ar’s
R be a convex function with f (1) = 0 and P , Q be two probability
f -divergence [45]. Let f : (0,

)
∞

→

2For the sake of illustration, we assume that the sensitive attribute S is binary but our results can be extended to a setting
of multi-groups. Furthermore, split classiﬁers can be applied to a scenario where multiple subgroups overlap [74, 87] since
individuals belonging to both groups can opt for either one of the split classiﬁers.

4

distributions over

X

. The f -divergence between P and Q is deﬁned by

Df (P

Q) (cid:44)

(cid:107)

(cid:90)

f

X

(cid:19)

(cid:18) dP
dQ

dQ.

Some examples of f -divergence are included in Appendix A.

(1)

The proofs of some of our main results (Lemma 2 and Theorem 2) rely on Ky Fan’s min-max theorem [24].
R is said to be concave-like on
if, for any two elements x1, x2 ∈ X
such that for all y

As a reminder, a function f :
and λ

X × Y →

X

[0, 1], there exists an element x0 ∈ X

∈

Similarly, f is said to be convex-like on
element y0 ∈ Y

such that for all x

and λ

∈

[0, 1], there exists an

∈ Y
λ)f (x2, y).

f (x0, y)

λf (x1, y) + (1

≥
−
, if for any two elements y1, y2 ∈ Y

Y

∈ X
f (x, y0)

λf (x, y1) + (1

λ)f (x, y2).

−

≤

R is called upper semicontinuous on a metric space
g(x0). Next, we recall3 Ky Fan’s min-max theorem [24].

if for every point x0 ∈ X

,

X

be a compact Hausdorﬀ space and
Y
, y) is upper semicontinuous on
, f (
·

an arbitrary set (not topologized).
.

such that, for every y

X
X × Y

∈ Y

X

and convex-like on

, then

Y
f (x, y) = max
x∈X

inf
y∈Y

f (x, y).

(2)

A function g :
lim supx→x0 g(x)

X →
≤
Lemma 1 ([24, Theorem 2]). Let
Let f be a real-valued function on
If f is concave-like on

X

inf
y∈Y

max
x∈X

2 The Beneﬁt-of-Splitting

We study the impact of disparate treatment by comparing the performance between optimal group-blind and
split classiﬁers. Recall that a ML model exhibits disparate treatment if it explicitly uses a sensitive attribute
to produce an output. We illustrate the diﬀerence between group-blind and split classiﬁers through the
example of logistic regressions:

• a group-blind classiﬁer does not use a sensitive attribute as an input: h(x) = logistic(wT x) where

logistic(t) (cid:44) 1/(1 + exp(

t)) for t

−

∈

R;

• split classiﬁers are a set of classiﬁers trained and deployed separately on each group: hs(x) = logistic(wT

s x)

for s

0, 1
}

∈ {

.

We measure the performance of both group-blind and split classiﬁers in terms of the disadvantaged group
) (higher values indicate a
(i.e., the group with worst performance). For a given performance measure Ls(
·
worse performance), the performance of a group-blind classiﬁer h and a set of split classiﬁers
}s∈{0,1},
respectively, is measured by

hs
{

max
s∈{0,1}

Ls(h)

and max
s∈{0,1}

Ls(hs).

Consequently, the optimal group-blind and split classiﬁers (across all measurable functions from
achieve the performance

X

to [0, 1])

inf
h:X →[0,1]

max
s∈{0,1}

Ls(h)

and max
s∈{0,1}

inf
h:X →[0,1]

Ls(h).

Next, we introduce the beneﬁt-of-splitting to quantify the eﬀect of splitting classiﬁers compared to using a
group-blind classiﬁer.

3We apply Ky Fan’s min-max theorem to the function −f instead of f .

5

Deﬁnition 1. For each s
measure, we deﬁne the beneﬁt-of-splitting as

∈ {

0, 1

) be a performance
, let PX,Y |S=s be a ﬁxed probability distribution and Ls(
·
}

(cid:15)split (cid:44) inf

h:X →[0,1]

max
s∈{0,1}

Ls(h)

max
s∈{0,1}

inf
h:X →[0,1]

−

Ls(h),

(3)

where the inﬁmum is taken over all (measurable) functions.

The beneﬁt-of-splitting is the diﬀerence between the performance of the optimal group-blind and split
h∗
s}s∈{0,1} are optimal group-blind and split classiﬁers respectively,
{

classiﬁers. In other words, if h∗ and
i.e.,

h∗

argmin
h:X →[0,1]

max
s∈{0,1}

∈

Ls(h),

h∗
s ∈

argmin
h:X →[0,1]

Ls(h) s

0, 1
}

∈ {

,

the beneﬁt-of-splitting can be equivalently expressed as

(cid:15)split = max
s∈{0,1}

Ls(h∗)

max
s∈{0,1}

−

Ls(h∗

s).

(4)

In practice, a data scientist may restrict the type of classiﬁers by ﬁxing a hypothesis class (e.g., logistic
regressions). The beneﬁt-of-splitting can be adapted for capturing the eﬀect of splitting classiﬁers in this
case (see Deﬁnition 5).

By the optimality of h∗

s and the max-min inequality, we have Ls(h∗)

0
and (cid:15)split ≥
which implies that, information-theoretically, using a separate classiﬁer on each group will never diminish
model performance compared to using a group-blind classiﬁer. A natural question is: how much perfor-
mance improvement does splitting classiﬁers bring? Before answering this question, we specify performance
measures of interest and present the beneﬁt-of-splitting under these performance measures.

s) for s

Ls(h∗

∈ {

0, 1

≥

}

2.1 Loss Reduction by Splitting

The ﬁrst type of performance measures contains standard loss functions which have been widely used in
fair ML [see e.g., 13] and domain adaptation [see e.g., 19]. These loss functions quantify the disagreement
between the labeling function ys and the probabilistic classiﬁer h. We recast the beneﬁt-of-splitting under
these loss functions below.

Deﬁnition 2. The (cid:96)1-beneﬁt-of-splitting (cid:15)split,1 is the beneﬁt-of-splitting in Deﬁnition 1 with the perfor-
mance measure:

The (cid:96)2-beneﬁt-of-splitting (cid:15)split,2 is the beneﬁt-of-splitting in Deﬁnition 1 with the performance measure:

Ls(h) = E [

h(X)
|

−

ys(X)

| |

S = s] .

−
The KL-beneﬁt-of-splitting (cid:15)split,KL is the beneﬁt-of-splitting in Deﬁnition 1 with the performance measure:

|

Ls(h) = E (cid:2)(h(X)

ys(X))2

S = s(cid:3) .

where for p, q

q) (cid:44) p log(p/q) + (1
[0, 1], DKL(p
(cid:107)

−

∈

p) log((1

p)/(1

q)).

−

−

Ls(h) = E [DKL(ys(X)
h(X))
(cid:107)

|

S = s] ,

Remark 1. Another widely used loss function is cross entropy Ls(h) = E [H(ys(X), h(X))
S = s] where
|
q) + H(p), the analysis of the
q). Since H(p, q) = DKL(p
for p, q
p) log(1
(cid:107)
beneﬁt-of-splitting under cross entropy is essentially the same as the analysis of (cid:15)split,KL (see Appendix B.3).

[0, 1], H(p, q) (cid:44)

p log q

(1

−

−

−

−

∈

6

2.2 False Error Rate Reduction by Splitting

Now we use the false error rate (FER) as a performance measure. The false error rate of a classiﬁer
is the maximum4 between (generalized) false positive rate and (generalized) false negative rate [67].
In
healthcare, assuring low false error rate is as important as guaranteeing high accuracy since patients could
suﬀer from harm due to a classiﬁer’s false error rate [21]. For example, the false negative diagnosis may
delay treatment in patients who are critically ill; the false positive diagnosis could lead to an unnecessary
treatment. Furthermore, a classiﬁer with high accuracy does not necessarily mean it has low false error rate.
Hence, we consider how split classiﬁers reduce the false error rate by recasting the beneﬁt-of-splitting under
this performance measure.

Deﬁnition 3. The FER-beneﬁt-of-splitting (cid:15)split,FER is the beneﬁt-of-splitting in Deﬁnition 1 with the per-
formance measure:

Ls(h) = max

E [h(X)

{

|

Y = 0, S = s] , E [1

h(X)

Y = 1, S = s]
}

.

|

−

(5)

Connection with equalized odds. Equalized odds, discussed by Hardt et al.
[8], is a commonly used
group fairness measure that requires diﬀerent groups to have (approximately) the same false positive and
false negative rates. Speciﬁcally, a probabilistic classiﬁer h :

[0, 1] satisﬁes equalized odds [8, 67] if

X →

E [h(X)
h(X)

−

|

|

E [1

Y = 0, S = 0] = E [h(X)
Y = 1, S = 0] = E [1

|
h(X)

Y = 0, S = 1]

(equal false positive rate),

−

Y = 1, S = 1]

(equal false negative rate).

|

Under this deﬁnition, classiﬁers are considered “unfair” if their false positive rate or false negative rate vary
across diﬀerent groups. However, imposing equalized odds constraints may lead to a signiﬁcant performance
reduction in classiﬁcation [60, 88–90].
In contrast, the beneﬁt-of-splitting deﬁnition studied in this work
aims to capture the principles of non-maleﬁcence and beneﬁcence [9]: classiﬁers should avoid the causation
of harm and achieve the best performance on each group. By taking the optimal group-blind classiﬁer as
a baseline approach, this may allow split classiﬁers to potentially exhibit performance disparities between
groups—as long as the split classiﬁers do not perform worse than the baseline approach and are as accurate
as possible.

3 The Taxonomy of Splitting

In this section, we analyze the loss reduction by splitting classiﬁers compared to using a group-blind classiﬁer.
We achieve this goal by upper and lower bounding the beneﬁt-of-splitting under diﬀerent loss functions (see
Deﬁnition 2). These bounds reveal factors which could impact the eﬀect of splitting classiﬁers and lead to
a taxonomy of splitting, i.e., a characterization of when splitting beneﬁts model performance the most or
splitting does not bring much beneﬁt.

Before stating the main result (i.e., bounds for the beneﬁt-of-splitting), we prove a lemma ﬁrst which
converts the deﬁnition of the beneﬁt-of-splitting into a single variable optimization problem. This lemma
will be used in the proof of our lower bounds.

Lemma 2. The beneﬁt-of-splitting under diﬀerent loss functions in Deﬁnition 2 have equivalent expressions

(cid:15)split,1 = sup
ω∈[0,1]

(1

−

ω)

(cid:90)

(cid:90)

y1(x)

Aω |
−
(cid:90) (y1(x)

y0(x)

dP1(x) + ω
|

ω |
Ac
y0(x))2dP0(x)dP1(x)

(cid:15)split,2 = sup
ω∈[0,1]

ω(1

−

ω)

−
ωdP0(x) + (1

(cid:15)split,KL = sup

ω∈[0,1]

JSω(PX,Y |S=0(cid:107)

PX,Y |S=1)

−

,

ω)dP1(x)
−
JSω(P0(cid:107)

P1),

y1(x)

dP0(x),
y0(x)
|

−

where

(cid:110)

x

ω (cid:44)

A

∈ X |

dP0(x)
dP1(x) ≥

(cid:111)

1−ω
ω

and JSω(

·(cid:107)·

) is the Jensen-Shannon divergence.

4Our analysis can be extended to any convex combination of false positive rate and false negative rate.

7

Proof. See Appendix B.1.

Next, we provide upper and lower bounds for (cid:15)split,1, (cid:15)split,2, and (cid:15)split,KL, respectively. These bounds rely
on two main factors: (i) disagreement between diﬀerent groups’ labeling functions and (ii) similarity between
their unlabeled distributions. In particular, the second factor is captured by a certain f -divergence [45] (see
Appendix A for some examples of f -divergence).

Theorem 1. The (cid:96)1-beneﬁt-of-splitting can be upper and lower bounded

(cid:26)

(cid:15)split,1 ≤

min

min
s∈{0,1}

(cid:112)E [(y1(X)

1
2

(cid:15)split,1 ≥

1
2

max
s∈{0,1}

max
s∈{0,1}
(cid:110)

E [
|

E [
|

y1(X)

y0(X))2

|

(cid:112)

1

S = s]

·
(cid:27)

y0(X)

| |

S = s]

,

−

−

DTV(P0(cid:107)

P1),

−

y1(X)

y0(X)

| |

−

S = s]

−

(cid:112)E [(y1(X)

y0(X))2

−

S = s]

·

|

d2(P1−s

(cid:111)

,

Ps)

(cid:107)

where DTV(P0(cid:107)

P1) is the total variation distance and d2(P1−s

E [
|

y1(X)

y0(X)

| |

−

S = 1]

E [
|

≥

Ps) is Marton’s divergence. Suppose that
(cid:107)
y1(X)

S = 0] .

y0(X)

(6)

−

| |

Then the (cid:96)2-beneﬁt-of-splitting can be upper and lower bounded

(cid:26)

(cid:15)split,2 ≤

min

min
s∈{0,1}

(cid:112)E [(y1(X)

−
E (cid:2)(y1(X)

y0(X))4

|

y0(X))2

|

−

(cid:112)

S = s]

·
(cid:27)
S = s(cid:3)

,

1
4

max
s∈{0,1}

1

DTV(P0(cid:107)

P1),

−

(cid:15)split,2 ≥

(cid:32) E [
|

y1(X)

−

(cid:112)Dχ2(P1(cid:107)

S = 0]

y0(X)
| |
P0) + 1 + 1

(cid:33)2

,

P0) is the chi-square divergence. The KL-beneﬁt-of-splitting can be upper and lower bounded

P1), max
s∈{0,1}

E

(cid:20)

(cid:18)

DKL

ys(X)
(cid:107)

y0(X) + y1(X)
2

(cid:19)

|

(cid:21)(cid:27)

S = s

,

where Dχ2 (P1(cid:107)
(cid:26)

(cid:15)split,KL

min

≤

2JS(PX,Y |S=0(cid:107)

PX,Y |S=1)

(cid:15)split,KL

≥
where JS(

PX,Y |S=1)

JS(PX,Y |S=0(cid:107)
) is the Jensen–Shannon divergence.

−

·(cid:107)·

2JS(P0(cid:107)
P1),

−
JS(P0(cid:107)

Proof. See Appendix B.2.

Now we consider extreme scenarios to verify the sharpness of the bounds and to understand when splitting

classiﬁers beneﬁts model performance the most (see Figure 1 for an illustration).

• Consider the setting where two groups share the same labeling function (i.e., y0 = y1). All the upper
and lower bounds in Theorem 1 for the beneﬁt-of-splitting under diﬀerent loss functions become zero
and, hence, the bounds are sharp. This is quite intuitive as one can use the labeling function y0 as a
group-blind classiﬁer and it achieves perfect performance on both groups. Hence, there is no beneﬁt
of splitting classiﬁers.

• Consider the setting where two groups share the same unlabeled distribution (i.e., P0 = P1). The upper
] /2, which is equal to (cid:15)split,1. The bounds of
|

and lower bounds of (cid:15)split,1 are both E [
|
(cid:15)split,2 become

y1(X)

y0(X)

−

1
4

E [
y1(X)
|

y0(X)
|

−

]2

(cid:15)split,2 ≤

≤

E (cid:2)(y1(X)

1
4

−

y0(X))2(cid:3) .

8

If, in addition,
y1(x)
|
and, hence, are sharp. Finally, the bounds of (cid:15)split,KL become

y0(x)
|

−

is the same across all x, the upper and lower bounds become the same

E [JS(y0(X)
y1(X))]
(cid:107)

≤

(cid:15)split,KL

max
s∈{0,1}

≤

E

(cid:20)

(cid:18)

DKL

ys(X)

y0(X) + y1(X)
2

(cid:107)

(cid:19)(cid:21)

.

If, in addition, E [DKL (y0(X)
(y0(X) + y1(X))/2)] = E [DKL (y1(X)
(y0(X) + y1(X))/2)], then the
(cid:107)
(cid:107)
upper and lower bounds are equal. This extreme case indicates that when diﬀerent groups have the
same unlabeled distribution (i.e., P0 = P1), the beneﬁt-of-splitting is determined by the disagreement
between their labeling functions (i.e., large disagreement leads to high beneﬁt).

• Consider the setting where two groups have unlabeled distributions lying on disjoint support sets. In
P1) = log 2. Hence, the upper bounds of (cid:15)split,1 and (cid:15)split,2

this case, DTV(P0(cid:107)
become zero. Furthermore,

P1) = 1 and JS(P0(cid:107)

0

≤

JS(PX,Y |S=0(cid:107)

PX,Y |S=1)

−

JS(P0(cid:107)

P1) = JS(PX,Y |S=0(cid:107)

PX,Y |S=1)

log 2

0.

≤

−

where the last step is because the Jensen–Shannon divergence is always upper bounded by log 2.
Therefore, the upper bound of (cid:15)split,KL is zero as well. In other words, there is no beneﬁt of splitting
classiﬁers when the unlabeled distributions are mutually singular. One can interpret this fact by
considering a special group-blind classiﬁer which mimics the labeling function of each group in the
region where its unlabeled distribution lies. This classiﬁer achieves perfect performance for each group.
Note that such a group-blind classiﬁer exists since we do not restrict the space of potential classiﬁers
and, hence, any (measurable) function could become a classiﬁer.

To summarize, from an information-theoretic perspective, splitting classiﬁers beneﬁts the most if two groups
have similar unlabeled distributions and diﬀerent labeling functions. This taxonomy of splitting appears for
all the commonly used loss functions (i.e., (cid:96)1, (cid:96)2, and KL loss).

Recall that the beneﬁt-of-splitting (see Deﬁnition 1) measures the performance improvement by using the
optimal split classiﬁers compared to deploying the optimal group-blind classiﬁer across all groups. Here model
performance is quantiﬁed in terms of the disadvantaged group (i.e., the group with the worst performance).
We end this section by considering the Bayes risk as an alternative way of measuring model performance5.
Speciﬁcally, the performance of a group-blind classiﬁer h and a set of split classiﬁers
}s∈{0,1}, respectively,
is measured by

hs

{

group-blind :

split :

Pr(S = 0)

Pr(S = 0)

E [
|
E [
|

·

·

They can be equivalently written as

h(X)
h0(X)

−

y0(X)
| |
y0(X)

S = 0] + Pr(S = 1)

·
S = 0] + Pr(S = 1)

−

| |

E [
h(X)
−
|
E [
h1(X)
|

·

y1(X)

| |
y1(X)

−

S = 1] ,

S = 1] .

| |

E [

h(X)
|

−

yS(X)
|

]

and E [
|

hS(X)

yS(X)
|

] .

−

The performance diﬀerence between the optimal group-blind and split classiﬁers leads to the following
deﬁnition.

Deﬁnition 4. We deﬁne the population-beneﬁt-of-splitting as

(cid:15)split,pop (cid:44) inf

h:X →[0,1]

E [

h(X)
|

−

yS(X)
|

]

−

inf
hs:X →[0,1]
for s∈{0,1}

E [
|

hS(X)

] .
yS(X)
|

−

The population-beneﬁt-of-splitting is upper bounded by the beneﬁt-of-splitting (i.e., (cid:15)split,pop ≤

(cid:15)split,1)
since the Bayes risk is upper bounded by the worst-case risk and the split classiﬁers
}s∈{0,1} composed
by the labeling functions can achieve zero risk. Hence, the upper bound of (cid:15)split,1 in Theorem 1 naturally
translates into an upper bound of (cid:15)split,pop. Next, we provide alternative bounds for (cid:15)split,pop which reveal
an additional factor inﬂuencing (cid:15)split,pop.

ys
{

5For the sake of illustration, in what follows we only consider the (cid:96)1 loss.

9

Proposition 1. Assume Pr(S = 0)
bounded

≤

0.5. The population-beneﬁt-of-splitting can be upper and lower

(cid:15)split,pop

(cid:15)split,pop

≤

≥

Pr(S = 0)

Pr(S = 0)

y1(X)

E [
|
·
(cid:18)
E [
y1(X)
|

−

−

y0(X)

| |

S = 0] ,

(cid:19)

y0(X)

| |

S = 0]

E Pr(S=1)
Pr(S=0)

(P0(cid:107)

P1)

−

,

where E Pr(S=1)

Pr(S=0)

(P0(cid:107)

P1) is the Eγ-divergence with γ = Pr(S = 1)/Pr(S = 0).

Proof. See Appendix B.4.

Remark 2. The Eγ-divergence plays an important role in Bayesian statistical hypothesis testing [91, 92].
Since γ
Q) [92], we can further lower bound (cid:15)split,pop
(cid:107)
by using the total variation distance

Q) is non-increasing and E1(P
(cid:107)

Q) = DTV(P
(cid:107)

Eγ(P

→

−
The Eγ-divergence relates with the DeGroot statistical information [93] through (see Equation (421) in [91])

| |

−

y1(X)

y0(X)

S = 0]

DTV(P0(cid:107)

P1)) .

(cid:15)split,pop ≥

Pr(S = 0) (E [
|

p(P

I

Q) =
(cid:107)

(cid:40)

(P

pE 1−p
Q)
(cid:107)
p
p)E p
1−p

(1

−

P )
(Q
(cid:107)

(0, 1
2 ]
[ 1
2 , 1).

p

p

∈

∈

Hence, we can write our lower bound of (cid:15)split,pop equivalently as

(cid:15)split,pop ≥

Pr(S = 0)

E [

·

y1(X)
|

−

y0(X)

| |

S = 0]

− IPr(S=0)(P0(cid:107)

P1).

As shown in Proposition 1, the population-beneﬁt-of-splitting is aﬀected not only by the above-mentioned
two factors (i.e., disagreement between labeling functions and similarity between unlabeled distributions)
but also by the percentage of the minority group over the whole population. This reveals a caveat of
the population-beneﬁt-of-splitting: the minority group can be underrepresented when one designs a group-
blind classiﬁer by minimizing the loss over the whole population. In contrast, the beneﬁt-of-splitting (see
Deﬁnition 2) does not rely on the probability of the sensitive attribute and, hence, represents each group
equally.

4 An Eﬃcient Procedure for Computing the Eﬀect of Splitting

In the last section, we provide upper and lower bounds for the beneﬁt-of-splitting under diﬀerent kinds of
loss functions. Here, we consider a diﬀerent performance measure: false error rate. It turns out that the
beneﬁt-of-splitting under false error rate, denoted by (cid:15)split,FER (see Deﬁnition 3), has an equivalent expression
which leads to an eﬃcient procedure of computing (cid:15)split,FER.

Even with the knowledge of the underlying data distribution, computing the beneﬁt-of-splitting directly
from its deﬁnition is challenging. This is because the space of potential classiﬁers is unrestricted (i.e., any
measurable function could be used as group-blind or split classiﬁers) and solving optimization problems
over this inﬁnite-dimensional functional space could be intractable. One may attempt to circumvent this
issue by restricting the classiﬁers over a hypothesis class. However, this naive approach has two limitations.
First, it is unclear how to choose a hypothesis class in order to compute the beneﬁt-of-splitting reliably. We
will show in Example 1 that diﬀerent hypothesis classes could result in completely diﬀerent values of the
beneﬁt-of-splitting. Second, as evidenced in [94], training the optimal group-blind or split classiﬁers may
suﬀer from a non-convexity issue.

We leverage the special form of the false error rate in (5) and prove an equivalent expression of (cid:15)split,FER
below which can be computed by solving two small-scale convex programs. The objective functions of these
convex programs have closed-form supergradients. Hence, they can be solved eﬃciently via standard solvers,
such as (stochastic) mirror descent [95, 96]. When the data distribution is known, our procedure returns
the precise values of (cid:15)split,FER without the need of training optimal group-blind and split classiﬁers. The
equivalent expression of (cid:15)split,FER is given in the following theorem.

10

Theorem 2. Assume Pr(Y = i, S = s) > 0 for any i, s
be equivalently written as

0, 1

∈ {

. The FER-beneﬁt-of-splitting (cid:15)split,FER can
}






max
µµµ∈∆4







(cid:88)

µs,1 + E





(cid:88)

µs,iφs,i(X)



s∈{0,1}

s,i∈{0,1}

−







 −

max
ννν(s)∈∆2
for s∈{0,1}






ν(s)
1 + E









(cid:88)

i∈{0,1}



ν(s)
i φs,i(X)



−










.

Here for a positive integer d, ∆d (cid:44)
µµµ (cid:44) (µ0,0, µ0,1, µ1,0, µ1,1), ννν(s) (cid:44) (ν(s)

z
∈
{
|
0 , ν(s)
1 ), and

Rd

(cid:80)d

i=1 zi = 1, zi

, for any a
0
}

∈

≥

R, (a)− (cid:44) min

a, 0
}
{

,

φs,i(x) (cid:44) (1

−

i

−

ys(x)) Pr(S = s
|
Pr(Y = i, S = s)

X = x)

,

s, i

0, 1

∈ {

.
}

(7)

Proof. See Appendix C.1.

Remark 3. We demonstrate a proof sketch of Theorem 2. The FER-beneﬁt-of-splitting (cid:15)split,FER is composed
by inf h:X →[0,1] maxs∈{0,1} Ls(h) and maxs∈{0,1} inf h:X →[0,1] Ls(h). The ﬁrst term can be equivalent written
as

inf
h:X →[0,1]

max
µµµ∈∆4






(cid:88)

s∈{0,1}

µs,0E [h(X)

|

Y = 0, S = s] + µs,1E [1

h(X)

|

−




Y = 1, S = s]



.

(8)

The key step in our proof is to swap maximum and inﬁmum in (8) by using Ky Fan’s min-max theorem
[24] (see Lemma 1). Then for a ﬁxed µµµ, the optimal classiﬁer owns a closed-form expression. After some
algebraic manipulations, (8) becomes the ﬁrst convex program in the equivalent expression of (cid:15)split,FER. In
the same vein, the another term maxs∈{0,1} inf h:X →[0,1] Ls(h) becomes the second convex program.

Next, we show that the objective functions of the convex programs in Theorem 2 have closed-form

supergradients.

Proposition 2. Under the same notations and assumptions in Theorem 2, functions g : ∆4 →
gs : ∆2 →

R with s

deﬁned as

0, 1
}

∈ {







g(µµµ) (cid:44) (cid:88)

µs,1 + E





(cid:88)

µs,iφs,i(X)



s∈{0,1}

s,i∈{0,1}

−


 gs(ννν) (cid:44) ν1 + E









(cid:88)



νiφs,i(X)







i∈{0,1}

−

R and

have a closed-form supergradient, respectively:


i + E



ψs,i(X)

·

∂g(µµµ)

(cid:51)

(cid:104) (cid:88)
I

s(cid:48),i(cid:48)∈{0,1}

µs(cid:48),i(cid:48)φs(cid:48),i(cid:48)(X) < 0

(cid:105)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂gs(ννν)

(cid:51)


i + E



ψs,i(X)

I

·

(cid:104) (cid:88)

i(cid:48)∈{0,1}

νi(cid:48)φs,i(cid:48)(X) < 0

(cid:105)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

where I[
·

] is the indicator function and





S = s





s,i∈{0,1}





S = s





,

i∈{0,1}

ψs,i(x) (cid:44) 1

i

−
−
Pr(Y = i

ys(x)
S = s)

|

,

s, i

0, 1
}

∈ {

.

,

(9)

(10)

(11)

Proof. See Appendix C.2.

When the underlying data distribution is known, one can compute (cid:15)split,FER by solving the convex pro-
grams in Theorem 2 via standard tools, such as mirror descent, with convergence guarantees [96]. This is
non-trivial because, as stated before, computing (cid:15)split,FER directly from its deﬁnition could be intractable.

11

In practice, when the underlying data distribution is unknown, one can ﬁrst approximate the conditional
distribution Pr(S = 1
X = x) and the labeling functions y0(x), y1(x) by training three well-calibrated
binary classiﬁers. These classiﬁers will be called when computing the supergradient of the objective functions
(see Proposition 2). We summarize our procedure of computing (cid:15)split,FER in Algorithm 1 where stochastic
mirror descent is used for solving the convex programs in Theorem 2. The numerical results are deferred to
Section 6.1.

|

Algorithm 1 Computing (cid:15)split,FER using stochastic mirror descent.

Input:

dataset: D = {(xi, yi, si)}n

i=1, maximum number of iterations: T ,

step size: {ηt}T

t=1

Initialize

I0 ← {i = 1, · · · , n | si = 0}
D0 ← (xi, yi) for i ∈ I0
D1 ← (xi, yi) for i (cid:54)∈ I0
approximate Pr(S = 1 | X = x)
approximate y0(x) and y1(x)
µµµ ← (0.25, 0.25, 0.25, 0.25) and ννν(s) ← (0.5, 0.5)

for t = 1,2, · · ·,T do

draw unlabeled sample x0,t, x1,t
pick w ∈ ∂g(µµµ) and w(s) ∈ ∂gs(ννν(s))
µj ← µj exp(ηtwj)/(cid:80)
exp(ηtw(s)
j ← ν(s)
ν(s)
end for
return: g(µµµ) − maxs∈{0,1} gs(ννν(s))

j(cid:48) µj(cid:48) exp(ηtwj(cid:48) )
j(cid:48) ν(s)

j )/(cid:80)

j

j(cid:48) exp(ηtw(s)
j(cid:48) )

(cid:46) indices of points in D with si = 0
(cid:46) points with si = 0
(cid:46) points with si = 1

(cid:46) train a classiﬁer using {(xi, si)}n

i=1
(cid:46) train two classiﬁers using D0 and D1, respectively
(cid:46) initialize values

(cid:46) randomly draw sample from D0, D1
(cid:46) approximate supergradient using x0,t, x1,t
(cid:46) update variable via entropic descent

(cid:46) update variable via entropic descent

(cid:46) the FER-beneﬁt-of-splitting: (cid:15)split,FER

Our procedure can be understood through the following two steps:

• training a classiﬁer to identify the sensitive attribute using input features and a classiﬁer for each group

to predict label using input features;

• solving (convex) programs with these classiﬁers in hand.

We remark that this two-step approach has also appeared in [e.g., 14, 72] for designing “fair” classiﬁers.

5 Splitting in Practice

So far we have studied the beneﬁt-of-splitting from an information-theoretic view as we assume the underlying
data distribution is known and do not restrict the space of potential classiﬁers. In this section, we study
the eﬀect of splitting classiﬁers from a more practical perspective. First, we restrict the classiﬁers over a
hypothesis class (e.g., logistic regressions) and analyze the hypothesis class dependent splitting. Second, we
consider splitting classiﬁers in a ﬁnite sample regime and study the sample limited splitting.

5.1 Hypothesis Class Dependent Splitting

We restrict both group-blind and split classiﬁers over the same hypothesis class and introduce a hypothesis
class dependent beneﬁt-of-splitting for quantifying the loss reduction by splitting classiﬁers.

Deﬁnition 5. For a ﬁxed probability distribution PX,Y |S=s with s
the

-beneﬁt-of-splitting is deﬁned as

H

0, 1

∈ {

}

and a given hypothesis class

,

H

(cid:15)H
split

(cid:44) inf
h∈H

max
s∈{0,1}

E [
h(X)
|

−

ys(X)

| |

S = s]

max
s∈{0,1}

inf
h∈H

−

E [
h(X)
|

−

ys(X)

| |

S = s] .

(12)

Clearly, the

-beneﬁt-of-splitting maintains the non-maleﬁcence principle (cid:15)H

0, i.e., given suﬃcient
samples, splitting classiﬁers will never diminish model accuracy compared to using a group-blind classiﬁer.
Next, we provide upper and lower bounds for (cid:15)H
split in order to understand when splitting classiﬁers brings

split ≥

H

12

the most beneﬁt. As before, these bounds rely on three major factors: (i) disagreement between optimal
(split) classiﬁers; (ii) similarity between unlabeled distributions; and (iii) approximation error deﬁned as the
smallest loss achieved by split classiﬁers. In particular, we assume that the last factor is small. This is a
common assumption in, e.g., the domain adaptation literature [19] since when the hypothesis class is complex
enough, this term will be negligible. Furthermore, one central notion of fairness we follow is non-maleﬁcence
(i.e., classiﬁers should avoid the causation of harm on any group). When the approximation error is large,
neither group-blind classiﬁers nor splitting classiﬁers are accurate and “harm” is inevitable. Hence, one
should change the hypothesis class ﬁrst instead of splitting.

Theorem 3. Let h∗

s be an optimal classiﬁer for group s

h∗
s ∈

argmin
h∈H

E [

h(X)
|

−

:
0, 1
∈ {
}
ys(X)

| |

S = s] .

Then we have the following upper and lower bounds for the

-beneﬁt-of-splitting

H

(cid:15)H
split ≤

min
s∈{0,1}

E [
h∗
1(X)
|

−

h∗
0(X)

| |

S = s]

(cid:15)H
split ≥

1
2

max
s∈{0,1}

E [

h∗
1(X)
|

−

h∗
0(X)

| |

S = s]

DTV(P0(cid:107)

P1)

−

−

3 (cid:80)

s∈{0,1}

E [
|

h∗
s(X)
2

−

ys(X)

| |

S = s]

.

Proof. See Appendix D.1.

Analogous to our discussions in Section 3, the bounds in Theorem 3 delineate a taxonomy of splitting
when both group-blind and split classiﬁers are restricted over the same hypothesis class: splitting classiﬁers
does not bring much beneﬁt when two groups have similar optimal classiﬁers; splitting classiﬁers beneﬁts
the most when two groups have similar unlabeled distributions and diﬀerent optimal classiﬁers. We further
demonstrate this taxonomy of splitting and show how these factors inﬂuence the eﬀect of splitting through
numerical experiments in Section 6.2.

In contrast to the upper bound for (cid:15)split (see Theorem 1), the upper bound for (cid:15)H

split does not involve the
similarity between the unlabeled distributions. Consequently, when the optimal classiﬁers are diﬀerent and
the unlabeled distributions are diﬀerent as well, it is unclear how much beneﬁt splitting classiﬁers brings. We
provide the following example which shows that diﬀerent hypothesis classes may result in largely diﬀerent
values of the
-beneﬁt-of-splitting. Hence, one must study the eﬀect of splitting on a case-by-case basis for
H
diﬀerent hypothesis classes.

Example 1. Let two groups’ unlabeled distributions and labeling functions be P0 ∼ N
I[x >
µ] and P1 ∼ N
unlabeled distributions P0 and P1 increases (i.e., DTV(P0(cid:107)
two hypothesis classes:

µ, 1), y0(x) =
(µ, 1), y1(x) = I[x < µ], respectively. As µ grows larger, the distance between the
). Now we consider the following
1 as µ

→ ∞

(
−

P1)

→

−

•

•

Hthreshold is the class of threshold functions over R: I[x > a] or I[x < b].
Hinterval is the class of intervals over R: I[x

(a, b)].

∈

, respectively. In both cases, the labeling functions are included in
Here, a, b are allowed to be
the hypothesis classes and, hence, are optimal classiﬁers. The disagreement between these optimal classiﬁers
is at least 1/2:

and +

−∞

∞

E [

y1(X)
|

y0(X)

S = s]

1/2,

s

.

0, 1
}

The beneﬁt-of-splitting under
disadvantaged group. On the other hand, as µ becomes larger, the beneﬁt-of-splitting under
0 since a group-blind classiﬁer with the form h∗(x) = I[x

∈ {
Hthreshold is 1/2 as any group-blind classiﬁer incurs at least 1/2 loss on the
Hinterval is nearly

µ, µ)] can achieve almost perfect accuracy.

| |

≥

−

(
−

∈

The previous example shows that using a threshold function as a group-blind classiﬁer will always incur
an inevitable accuracy trade-oﬀ between two groups. On the other hand, if we enrich the hypothesis class to
include interval functions, this trade-oﬀ can be reconciled. Motivated by this observation, when two groups

13

-beneﬁt-
have diﬀerent unlabeled distributions and diﬀerent labeling functions, we conjecture that the
of-splitting is determined by the “richness” of the hypothesis class: a more complex hypothesis class can
produce a group-blind classiﬁer which mimics the labeling function of each group in the region they lie in,
and, hence, this classiﬁer guarantees high accuracy for both groups. We formalize this intuition through
the example of feedforward neural networks. Recall that a sigmoidal function [97] (e.g., logistic function)
S : R
.
→ −∞
The hypothesis class associated to feedforward neural networks with one layer of sigmoidal functions has the
form

R is a bounded measurable function which satisﬁes S(z)

and S(z)

1 as z

0 as z

→

→

→

∞

→

H

+

(cid:40) k

(cid:88)

i=1

=

H

ciS(ai

x + bi) + c0 |

·

ai

∈

Rd, bi, ci

(cid:41)

R

.

∈

(13)

In this case, Barron’s approximation bounds [97] guarantee that these neural networks can approximate a
large class of functions reliably.

Proposition 3. Consider the hypothesis class

in (13). If

H

X ⊂

Rd is compact, we have

(cid:15)H
split ≤

min
s∈{0,1}

(cid:112)E [(y1(X)

y0(X))2

−

S = s]

|

(cid:112)

1

·

DTV(P0(cid:107)

P1) +

−

2diam(
X
√k

)C

,

where diam(

X

x
) = supx,x(cid:48)∈X (cid:107)

−

x(cid:48)

(cid:107)2,

h∗(x) (cid:44) y0(x)dP0(x) + y1(x)dP1(x)

dP0(x) + dP1(x)

=

(cid:90)

Rd

exp(iwx)(cid:102)h∗(w)dw

(14)

for some complex-valued function (cid:102)h∗, and C (cid:44) (cid:82)

w

Rd (cid:107)

(cid:102)h∗(w)
|

(cid:107)2|

dw.

Proof. See Appendix D.2.

Remark 4. The condition in (14) goes back to the seminal work of Barron [97]. By the Fourier inversion
theorem, if both h∗ and its Fourier transform are integrable, this condition is satisﬁed. Further situations
where (14) holds are discussed in [97, Section IX].

In contrast to Theorem 3, the upper bound for the

-beneﬁt-of-splitting above involves the similarity
P1)) at the cost of having an additional term which
between the unlabeled distributions (i.e., DTV(P0(cid:107)
is inversely proportional to the hypothesis class complexity. The intuition behind our proof is that if a
data scientist is able to train a neural network with enough neurons, a group-blind classiﬁer is capable of
guaranteeing high accuracy for both groups when their unlabeled distributions are diﬀerent. Consequently,
there is no much room for accuracy improvement by splitting classiﬁers.

H

5.2 Comparison with the Cost-of-Coupling

-beneﬁt-of-splitting with the cost-of-coupling introduced by Dwork et al. [13].
We compare our notion of the
We ﬁrst illustrate the diﬀerence between group blind, coupled, split classiﬁers through the example of logistic
regressions:

H

• a group blind classiﬁer never uses a sensitive attribute as an input: h(x) = logistic(wT x);

• a coupled classiﬁer uses a sensitive attribute while sharing other parameters: h(s, x) = logistic(wT x +

w0s);

• split classiﬁers are a set of classiﬁers applied to each separate group: hs(x) = logistic(wT

s x).

Now we recast the deﬁnition of the cost-of-coupling [13] using our notation.

14

Deﬁnition 6 ([13]). Let
to [0, 1]. For a given loss function (cid:96)(
·

HC be a hypothesis class which contains coupled classiﬁers from a ﬁnite set

S × X

,

), the cost-of-coupling is deﬁned as
·

(cid:110)

max
PS,X,Y

min
h∈HC

L(h)

−

min
hs∈HC
for s∈S

L(

hs
{

s∈S )
}

(cid:111)
,

where the maximum is over all distributions on
E [(cid:96)(Y, hS(S, X))].

S × X × {

0, 1
}

and L(h) (cid:44) E [(cid:96)(Y, h(S, X))], L(
hs
{

s∈S ) (cid:44)
}

There are two important diﬀerences between the

-beneﬁt-of-splitting (see Deﬁnition 5) and the cost-
H
of-coupling [13]. First, our notion quantiﬁes the gain in accuracy by using split classiﬁers rather than a
group-blind classiﬁer.
In contrast, the cost-of-coupling compares coupled classiﬁers with split classiﬁers
which both take a sensitive attribute as an input. Second, the cost-of-coupling is a worst-case quantity as
it maximizes over all distributions. By allowing our notion to rely on the data distribution, Deﬁnition 5
captures more intricate scenarios for characterizing the beneﬁt of splitting classiﬁers. Furthermore, by taking
the maximum over all distributions, we recover an analogous result of Theorem 2 in [13].

Corollary 1. There exists a probability distribution QS,X,Y whose

-beneﬁt-of-splitting is at least 1/2 under

H

1. Linear predictors:

2. Decision trees:

H

=

I[wT x
{

0]

w

Rd

;
}

H
is the set of binary decision trees.

≥

∈

|

Furthermore, under this hypothetical distribution QS,X,Y , no matter which group-blind classiﬁer h
used, there is always a group s

S = s]

h(X)

1/2.

0, 1

ys(X)

is

∈ H

∈ {

such that E [
|

}

−

| |

≥

The proof technique used for this corollary can be extended to many other models (e.g., kernel methods

or neural networks) and we defer its proof to Appendix D.3.

5.3 Sample Limited Splitting

Consider the following scenario. A data scientist has access to ﬁnitely many samples and she/he solves
an empirical risk optimization in order to obtain an optimal group-blind classiﬁer or a set of optimal split
classiﬁers. When these classiﬁers are deployed on new fresh samples, a natural question is whether the
optimal split classiﬁers still outperform the group-blind classiﬁer. We introduce the sample-limited-splitting
which quantiﬁes the eﬀect of splitting classiﬁers within this ﬁnite sample regime.

{

Deﬁnition 7. For a given hypothesis class
let ˆh∗ and

and ns i.i.d. samples
∈ {
ˆh∗
s}s∈{0,1} be optimal group-blind and split classiﬁers for the empirical (cid:96)1 loss, respectively:
h(xs,i)
ns
ys,i

(xs,i, ys,i)
}
{

max
s∈{0,1}
(cid:80)ns

argmin
h∈H

i=1|

(cid:80)ns

ns
i=1 from group s

ys,i

ˆh∗

H

−

∈

|

,

ˆh∗
s ∈

argmin
h∈H

i=1|

h(xs,i)
ns

−

|

,

s

.

0, 1
}

∈ {

,

0, 1
}

(15)

(16)

The sample-limited-splitting is deﬁned as

ˆ(cid:15)split (cid:44) max
s∈{0,1}

(cid:104)

E

ˆh∗(X)
|

−

ys(X)

| |

(cid:105)
S = s

(cid:104)

E

max
s∈{0,1}

−

ˆh∗
s(X)
|

−

ys(X)

| |

(cid:105)
S = s

.

(17)

Unlike the beneﬁt-of-splitting or the

-beneﬁt-of-splitting, the sample-limited-splitting is not necessarily
non-negative. In other words, with limited amount of samples available, splitting classiﬁers may not improve
accuracy for both groups.
In what follows, we provide data-dependent upper and lower bounds for the
sample-limited-splitting in order to understand the eﬀect of splitting classiﬁers in the ﬁnite sample regime.

H

15

Theorem 4. Let
of the empirical (cid:96)1 loss (cid:80)ns
probability at least 1

H

δ,

i=1|

be a hypothesis class from

h(xs,i)

ys,i

−

to

/ns computed via ns i.i.d. samples
|

with VC dimension D. If ˆh∗
(xs,i, ys,i)
}
{

0, 1
}

X

{

s is a minimizer
ns
i=1, then, with

−

ˆ(cid:15)split

min
s∈{0,1}

≤

(cid:80)ns

i=1|

ˆh∗
1(xs,i)
ns

ˆh∗
0(xs,i)

−

|

+ Ω,

ˆ(cid:15)split

1
2

≥

max
s∈{0,1}

(cid:80)ns

i=1|

ˆh∗
1(xs,i)
ns

ˆh∗
0(xs,i)
|

−

DTV( ˆP0(cid:107)

ˆP1)

−

−

3λ

−

Ω,

where ˆPs is the empirical unlabeled distribution and

(18)

(19)

λ (cid:44) 1
2

(cid:32) (cid:80)n0

i=1|

ˆh∗
0(x0,i)
n0

y0,i

−

|

+

Proof. See Appendix D.4.

(cid:80)n1

i=1|

ˆh∗
1(x1,i)
n1

(cid:33)

y1,i

|

−

, Ω (cid:44) 4 max
s∈{0,1}

(cid:115)

2D log(6ns) + 2 log(8/δ)
ns

.

(cid:112)

n0, n1}

Here, the term λ is the (average) training loss and Ω is the complexity term, which is approximately
. As shown, the upper and lower bounds for ˆ(cid:15)split rely on four factors. The ﬁrst three
D/min
{
factors, which also appear in our bounds of the
-beneﬁt-of-splitting (see Theorem 3), are the disagreement
between the (empirically) optimal classiﬁers, the similarity of the (empirically) unlabeled distributions, and
the (empirically) training error. In addition to these factors, our bounds for ˆ(cid:15)split also depend on the number
of samples from each group, especially minority group with less samples, and model complexity (measured
by the VC dimension [98]).

H

6 Numerical Experiments

We illustrate the theoretical results presented in this paper through experiments. In Section 4, we presented
an algorithm (Algorithm 1) for computing the beneﬁt-of-splitting. In particular, when the data distribution
is known, this algorithm provably converges to the exact value of the beneﬁt-of-splitting. To evaluate
Algorithm 1, we conduct experiments on a synthetic example where both the data distribution and the values
of the beneﬁt-of-splitting are known. In Section 5, we characterized a taxonomy of splitting when classiﬁers
are restricted over a hypothesis class. We demonstrate this taxonomy of splitting through experiments on
40 real-world datasets.

6.1 Synthetic Datasets

We introduced the FER-beneﬁt-of-splitting (cid:15)split,FER in Section 2.2 and proposed an eﬃcient procedure for
computing this quantity (Algorithm 1). Here, we validate Algorithm 1 through experiments on synthetic
datasets. For a ﬁxed parameter θ
[0, π/2], let two groups’ unlabeled distributions be zero-mean Gaussian
distributions with diﬀerent covariance matrices: P0 ∼ N

(0, Σ1) where

∈

Σ0 =

(cid:18) 0.5 cos(θ)2 + 1
0.5 sin(θ) cos(θ)

0.5 sin(θ) cos(θ)
0.5 sin(θ)2 + 1

(cid:19)

, Σ1 =

−

0.5 sin(θ) cos(θ)
0.5 sin(θ)2 + 1

(cid:19)

.

(0, Σ0) and P1 ∼ N
(cid:18) 0.5 cos(θ)2 + 1
0.5 sin(θ) cos(θ)

−

The distributions P0 and P1 correspond to θ counterclockwise and clockwise rotation of the Gaussian distri-
bution

(0, diag(1.5, 1)). Furthermore, let the labeling functions be

N

y0(x) =

(cid:40)
1
0

sin(θ), cos(θ))

if (
otherwise,

−

x > 0

·

y1(x) =

(cid:40)

1
0

if (sin(θ), cos(θ))
otherwise.

·

x > 0

The left-hand side of Figure 2 displays the level sets of P0 as well as its labeling function.
In this synthetic example, (cid:15)split,FER has a closed-form expression: (cid:15)split,FER = 2 Pr(X
(cid:44)

S = 0) where
(see Appendix E.1 for a proof). When θ = 0, two groups
A
share the same unlabeled distribution (i.e., P0 = P1) and the same labeling function (i.e., y0 = y1). Hence,

x = (x1, x2)
{

y1(x) = 1, x2 < 0

∈ A |

R2

∈

}

|

16

Figure 2: We demonstrate the performance of Algorithm 1 for computing the FER-beneﬁt-of-splitting (cid:15)split,FER on
synthetic datasets. Left: the ellipses are the level sets of the unlabeled distribution P0 and the dash line is the labeling
function y0 with a arrow indicating the region where points are labeled as +. Right: (cid:15)split,FER computed by diﬀerent
approaches along with its true values. As shown, when the underlying data distribution is known, the approximation
of (cid:15)split,FER produced by Algorithm 1 (orange curve) recovers its true values (blue dash curve). When the underlying
distribution is unknown, we train binary classiﬁers and feed them into Algorithm 1. The approximation of (cid:15)split,FER
produced by Algorithm 1 is depicted as red curve. Finally, we compute (cid:15)split,FER empirically by training optimal
group-blind and split classiﬁers via (i) logistic regression (gray curve), (ii) linear SVM (green curve), (iii) Naive Bayes
classiﬁer (purple curve). We use 5-fold cross validation for training these optimal classiﬁers and plot the standard
deviation as shaded region. As shown, the approximations of (cid:15)split,FER produced by Algorithm 1 outperform all three
empirical approximations of (cid:15)split,FER.

there is no beneﬁt of splitting classiﬁers: (cid:15)split,FER = 0. On the other hand, when θ = π/2, two groups have
the same unlabeled distribution but completely diﬀerent labeling functions. Splitting classiﬁers achieves the
most beneﬁt: (cid:15)split,FER = 0.5.

By varying the values of θ and drawing 10k samples from each group, we compare the true values
of (cid:15)split,FER with the outputs from Algorithm 1 as well as other empirical approximations. Recall that
Algorithm 1 requires a conditional distribution Pr(S = s
X = x) and the labeling functions y0 and y1.
Since the conditional distribution and labeling functions are known in this synthetic example, we feed their
explicit forms into Algorithm 1 for computing (cid:15)split,FER (orange curve in Figure 2 Right). In practice, the
conditional distribution and labeling functions are unknown, so we also train a Naive Bayes classiﬁer [99] to
approximate Pr(S = s
X = x) and two linear support-vector machine (SVM) classiﬁers [99] to approximate
the labeling functions. By feeding these binary classiﬁers into Algorithm 1, another approximation of (cid:15)split,FER
is output (red curve in Figure 2 Right). Furthermore, we compute (cid:15)split,FER empirically by training optimal
group-blind and split classiﬁers via logistic regression, linear SVM, or Naive Bayes classiﬁer. Computing the
false error rate reduction leads to three empirical approximations of (cid:15)split,FER.

|

|

As shown in Figure 2, when Algorithm 1 has access to the explicit forms of Pr(S = s

X = x), y0,
and y1, it accurately recovers (cid:15)split,FER. This is remarkable since even with the knowledge of the under-
lying distributions, it is unclear how to compute (cid:15)split,FER directly from its deﬁnition. We also observe
that Algorithm 1 applied to binary classiﬁers outputs more accurate approximation of (cid:15)split,FER than the
approximations produced by using logistic regression, linear SVM, or Naive Bayes classiﬁer.

|

To summarize, we conclude that (i) when the underlying distribution is known, Algorithm 1 can produce
the precise values of (cid:15)split,FER and has convergence guarantees; (ii) when Algorithm 1 is fed with binary classi-
ﬁers, it produces reliable approximation of (cid:15)split,FER; (iii) computing the FER-beneﬁt-of-splitting empirically
by training optimal classiﬁers could incur high approximation errors.

17

Figure 3: We demonstrate how the eﬀect of splitting classiﬁers is determined by the two factors: disagreement
between optimal classiﬁers (y-axis) and total variation distance between unlabeled distributions (x-axis). We restrict
both group-blind and split classiﬁers to logistic regression classiﬁers (left) or decision tree classiﬁers (right). Each
dot represents a dataset in OpenML [22] with color indicating the eﬀect of splitting classiﬁers compared to using a
group-blind classiﬁer and texts indicating dataset ID. Our upper and lower bounds in Theorem 3 reveal a taxonomy of
splitting where splitting does not bring much beneﬁt (white region); splitting brings the most beneﬁt (yellow region);
or splitting has undetermined eﬀect (grey region).

6.2 Real-world Datasets

In Section 5, we analyzed the eﬀect of splitting classiﬁers when both group-blind and split classiﬁers are
restricted over the same hypothesis class. The bounds in Theorem 3 reveal two main factors that could
determine this eﬀect: disagreement between optimal classiﬁers and similarity between unlabeled distribu-
tions. Here we demonstrate how these two factors inﬂuence the eﬀect of splitting through experiments on
40 real-world datasets, collected from OpenML [22].

Setup. We preprocess all 40 datasets by adopting the procedure described in [13]. All categorical features
are transformed into binary by assigning the most frequent object to 1 and the rest of the objects to 0. The
ﬁrst binary feature is selected as the sensitive attribute and, hence, these datasets are “semi-synthetic”. We
truncate the datasets so that each group contains at most 10k data points. In each dataset, there are at
least 8k data points per group, minimizing the eﬀect of potential lack of samples per group.

∈ {

Implementation. We obtain optimal split classiﬁers via training a logistic regression model with the
LIBLINEAR solver [100], ﬁtting the model by drawing samples from each group. Since an optimal group-
blind classiﬁer is a minimizer of minh∈H maxw∈[0,1] wL0(h) + (1
w)L1(h) where Ls(h) is the loss of a
, we solve this optimization approximately by considering its dual formula
classiﬁer h on group s
0, 1
}
maxw∈[0,1] minh∈H wL0(h) + (1
w)L1(h) and use 5-fold cross validation to tune the parameter w therein.
Although this procedure of training group-blind classiﬁer needs access to data points’ sensitive attribute,
it does not violate group-blindness [85] because the output classiﬁer does not use the sensitive attribute as
an input when deploying on new data.
In addition to logistic regressions, we repeat this experiment by
training decision tree classiﬁers with depth 7. The disagreement between optimal classiﬁers is calculated by
applying the optimal split classiﬁers on each data point and computing the discrepancy. We estimate the
total variation distance between unlabeled distributions by applying the procedures introduced in [101] (see
Appendix E.2 for more details).

−

−

18

Result.
In Figure 3, we illustrate the taxonomy of splitting delineated by our bounds in Theorem 3. We
restrict the hypothesis class to be logistic regression (Figure 3 Left) or to be decision trees with depth 7
(Figure 3 Right). Each dot in the ﬁgures represents a dataset with its corresponding ID number in the
OpenML dataset. The color captures the loss reduction by using the optimal split classiﬁers compared to
deploying the optimal group-blind classiﬁer (red means splitting has more beneﬁt and blue means splitting
does not bring much beneﬁt). The location of each dot is determined by the two factors: disagreement
between optimal classiﬁers (y-axis) and total variation distance between unlabeled distributions (x-axis).

• The upper bound in Theorem 3 indicates that splitting does not bring much beneﬁt when the optimal
classiﬁers are similar. As shown in Figure 3, all datasets which are below the horizontal dash line have
small beneﬁt by splitting classiﬁers (i.e., dots are blue).

• The lower bound in Theorem 3 indicates that splitting beneﬁts model performance when the optimal
classiﬁers are diﬀerent and the unlabeled distributions are similar. As shown in Figure 3, there are
two datasets (ID 122 and 1169) which are in the yellow region and they all achieve large beneﬁt from
splitting classiﬁers.

• When both the optimal classiﬁers and the unlabeled distributions are diﬀerent, the eﬀect of splitting
classiﬁers can not be determined by the bounds in Theorem 3. As shown in Figure 3, the datasets in
the grey region could have either large beneﬁt by splitting classiﬁers or limited beneﬁt. Furthermore,
we have conjectured (see Section 5.1) that in this case a more complex hypothesis class leads to less
beneﬁt from splitting classiﬁers. This is further evidenced in the experiments: when both group-blind
and split classiﬁers are logistic regressions (Figure 3 Left), the datasets which are in the grey region all
achieve non-trivial beneﬁt by splitting classiﬁers. In contrast, when decision trees are used (Figure 3
Right), there are datasets (e.g., ID 1240) in the grey region which achieve a limited amount of beneﬁt
by splitting.

7 Conclusion and Future Work

Split classiﬁers should only be considered when it is ethical and legal to do so, and when it does not result
in harm to any underlying group. Eliminating disparate treatment does not necessarily lead to a group-fair
classiﬁer. On the one hand, a sensitive attribute could correlate with other proxy variables which are used
for decision making [14, 56]. On the other hand, the sensitive attribute can be an important feature for the
prediction task [86, 89]. In the latter case, using a group-blind classiﬁer for achieving treatment parity may
lead to an unfavorable accuracy trade-oﬀ.

Motivated by the above discussion, we investigated the following fundamental question: when disparate
treatment is allowed, is it beneﬁcial to incorporate the sensitive attribute as an input feature in order to
improve a classiﬁer’s performance? Due to the bias-variance trade-oﬀ, in practice, the answer will depend
In this paper, we focused
on the number of training data and the complexity of the hypothesis class.
on an information-theoretic regime where the underlying data distribution is known—or inﬁnitely many
data points are available—and the hypothesis class is unrestricted. To evaluate the potential gain in average
performance from allowing a classiﬁer to exhibit disparate treatment, we compare split classiﬁers with group-
blind classiﬁers and characterize precise conditions where splitting classiﬁers achieves the most beneﬁt.
Our results show that—in this narrow information-theoretic regime—splitting classiﬁers follows the non-
maleﬁcence principle and allows a data scientist to deploy more accurate and suitable models for each group.
There are two open questions that deserve further exploration. First, our bounds indicate that the
diﬀerence in underlying data distributions between groups, the number of samples, and the hypothesis class
can all inﬂuence the eﬀect of splitting classiﬁers. Nonetheless, we believe that there are more factors that
play an important role in determining such an eﬀect. For example, a group-blind classiﬁer may perform
worse on minority groups due to unbalanced samples in the training process and using split classiﬁers could
potentially reconcile this issue. In a similar vein, the lack of sample diversity (i.e., training datasets do not
contain enough samples from minority groups) could aﬀect the performance and generalization of ML models
for minority groups. Hence, it is crucial to characterize the impact of sample size and diversity on detecting
and reducing discrimination. Second, we introduce the sample-limited-splitting ˆ(cid:15)split for quantifying the

19

eﬀect of splitting classiﬁers in the ﬁnite sample regime and provide its upper and lower bounds. It would be
interesting to characterize precise conditions under which ˆ(cid:15)split ≥

0 (or ˆ(cid:15)split ≤

0).

Acknowledgements

The authors would like to thank Dr. Berk Ustun for his valuable input. The authors would also like to
thank the anonymous reviewers and the associate editor for their constructive feedback.

References

[1] S. Barocas and A. D. Selbst, “Big data’s disparate impact,” Calif. L. Rev., vol. 104, p. 671, 2016.

[2] EEOC, “Uniform guidelines on employee

selection procedures,”

1979.

[Online]. Available:

https://www.eeoc.gov/policy/docs/qanda clarify procedures.html

[3] M. K. Cho, “Racial and ethnic categories in biomedical research: there is no baby in the bathwater,”

J. Law Med. Ethics, vol. 34, no. 3, pp. 497–499, 2006.

[4] J. N. Cohn, “The use of race and ethnicity in medicine: lessons from the african-american heart failure

trial,” J. Law Med. Ethics, vol. 34, no. 3, pp. 552–554, 2006.

[5] J. Perez-Rodriguez and A. de la Fuente, “Now is the time for a postracial medicine: Biomedical
research, the national institutes of health, and the perpetuation of scientiﬁc racism,” Am. J. Bioeth.,
vol. 17, no. 9, pp. 36–47, 2017.

[6] Federal
2020.
consumer-compliance-examination-manual/documents/5/v-7-1.pdf

Commission
Available:

(ECOA),”
https://www.fdic.gov/resources/supervision-and-examinations/

Trade
[Online].

Opportunity

(FTC),

“Equal

Credit

Act

[7] M. Feldman, S. A. Friedler, J. Moeller, C. Scheidegger, and S. Venkatasubramanian, “Certifying and
removing disparate impact,” in Proc. 21th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, 2015, pp. 259–268.

[8] M. Hardt, E. Price, and N. Srebro, “Equality of opportunity in supervised learning,” in Advances in

Neural Information Processing Systems, 2016, pp. 3315–3323.

[9] T. L. Beauchamp and J. F. Childress, Principles of biomedical ethics. Oxford University Press, USA,

2001.

[10] B. Ustun, Y. Liu, and D. Parkes, “Fairness without harm: Decoupled classiﬁers with preference guar-

antees,” in Proc. 36th International Conference on Machine Learning, 2019, pp. 6373–6382.

[11] N. Martinez, M. Bertran, and G. Sapiro, “Fairness with minimal harm: A pareto-optimal approach

for healthcare,” arXiv preprint arXiv:1911.06935, 2019.

[12] M. B. Zafar, I. Valera, M. Rodriguez, K. Gummadi, and A. Weller, “From parity to preference-based
notions of fairness in classiﬁcation,” in Advances in Neural Information Processing Systems, 2017, pp.
229–239.

[13] C. Dwork, N. Immorlica, A. T. Kalai, and M. Leiserson, “Decoupled classiﬁers for group-fair and
eﬃcient machine learning,” in Proc. 1st Conference on Fairness, Accountability and Transparency,
2018, pp. 119–133.

[14] H. Wang, B. Ustun, and F. P. Calmon, “Repairing without retraining: Avoiding disparate impact with
counterfactual distributions,” in Proc. 36th International Conference on Machine Learning, 2019, pp.
6618–6627.

20

[15] ——, “On the direction of discrimination: An information-theoretic analysis of disparate impact in

machine learning,” in Proc. 2018 IEEE Int. Symp. on Inf. Theory, 2018, pp. 126–130.

[16] A. Blum and K. Stangl, “Recovering from biased data: Can fairness constraints improve accuracy?”

arXiv preprint arXiv:1912.01094, 2019.

[17] H. Suresh and J. V. Guttag, “A framework for understanding unintended consequences of machine

learning,” arXiv preprint arXiv:1901.10002, 2019.

[18] H. H. Zhou, Y. Zhang, V. K. Ithapu, S. C. Johnson, G. Wahba, and V. Singh, “When can multi-site
datasets be pooled for regression? hypothesis tests, l2-consistency and neuroscience applications,” in
Proc. 34th International Conference on Machine Learning, 2017, pp. 4170–4179.

[19] S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. W. Vaughan, “A theory of

learning from diﬀerent domains,” Machine learning, vol. 79, no. 1-2, pp. 151–175, 2010.

[20] Y. Mansour, M. Mohri, and A. Rostamizadeh, “Domain adaptation: Learning bounds and algorithms,”

in 22nd Conference on Learning Theory, 2009.

[21] A. G. Lalkhen and A. McCluskey, “Clinical tests: sensitivity and speciﬁcity,” Continuing Education

in Anaesthesia Critical Care & Pain, vol. 8, no. 6, pp. 221–223, 2008.

[22] J. Vanschoren, J. N. van Rijn, B. Bischl, and L. Torgo, “Openml: Networked science in machine

learning,” SIGKDD Explorations, vol. 15, no. 2, pp. 49–60, 2013.

[23] L. D. Brown and M. G. Low, “A constrained risk inequality with applications to nonparametric func-

tional estimation,” The Annals of Statistics, vol. 24, no. 6, pp. 2524–2535, 1996.

[24] K. Fan, “Minimax theorems,” Proc. National Academy of Sciences of the United States of America,

vol. 39, no. 1, p. 42, 1953.

[25] A. B. Tsybakov, Introduction to nonparametric estimation. Springer Science & Business Media, 2008.

[26] Y. Wu and P. Yang, “Minimax rates of entropy estimation on large alphabets via best polynomial

approximation,” IEEE Trans. Inf. Theory, vol. 62, no. 6, pp. 3702–3720, 2016.

[27] J. Jiao, Y. Han, and T. Weissman, “Minimax estimation of the l1 distance,” IEEE Trans. Inf. Theory,

vol. 64, no. 10, pp. 6672–6706, 2018.

[28] Y. Polyanskiy and Y. Wu, “Dualizing le cam’s method, with applications to estimating the unseens,”

arXiv preprint arXiv:1902.05616, 2019.

[29] A. Xu and M. Raginsky, “Information-theoretic lower bounds on bayes risk in decentralized estimation,”

IEEE Trans. Inf. Theory, vol. 63, no. 3, pp. 1580–1600, 2016.

[30] J. Duchi, M. J. Wainwright, and M. I. Jordan, “Local privacy and minimax bounds: Sharp rates for
probability estimation,” in Advances in Neural Information Processing Systems, 2013, pp. 1529–1537.

[31] S. B. David, T. Lu, T. Luu, and D. Pal, “Impossibility theorems for domain adaptation,” in Proc.
Thirteenth International Conference on Artiﬁcial Intelligence and Statistics, 2010, pp. 129–136.

[32] S. Li, A. Khisti, and A. Mahajan, “Information-theoretic privacy for smart metering systems with a

rechargeable battery,” IEEE Trans. Inf. Theory, vol. 64, no. 5, pp. 3679–3695, 2018.

[33] B. Rassouli and D. G¨und¨uz, “Optimal utility-privacy trade-oﬀ with total variation distance as a privacy

measure,” IEEE Trans. Inf. Forensics Security, vol. 15, pp. 594–603, 2019.

[34] D. Rebollo-Monedero, J. Forne, and J. Domingo-Ferrer, “From t-closeness-like privacy to postran-
domization via information theory,” IEEE Trans. Knowl. Data Eng., vol. 22, no. 11, pp. 1623–1636,
2009.

21

[35] Y. O. Basciftci, Y. Wang, and P. Ishwar, “On privacy-utility tradeoﬀs for constrained data release
IEEE, 2016, pp. 1–6.

mechanisms,” in 2016 Information Theory and Applications Workshop (ITA).

[36] N. Takbiri, A. Houmansadr, D. L. Goeckel, and H. Pishro-Nik, “Privacy against statistical matching:

Inter-user correlation,” in Proc. 2018 IEEE Int. Symp. on Inf. Theory, 2018, pp. 1036–1040.

[37] A. Nageswaran and P. Narayan, “Data privacy for a ρ-recoverable function,” IEEE Trans. Inf. Theory,

vol. 65, no. 6, pp. 3470–3488, 2019.

[38] J. Liao, L. Sankar, F. P. Calmon, and V. Y. Tan, “Hypothesis testing under maximal leakage privacy

constraints,” in Proc. 2017 IEEE Int. Symp. on Inf. Theory, 2017, pp. 779–783.

[39] A. Makhdoumi, S. Salamatian, N. Fawaz, and M. M´edard, “From the information bottleneck to the

privacy funnel,” in IEEE Inf. Theory Workshop (ITW), 2014, pp. 501–505.

[40] N. Tishby, F. C. Pereira, and W. Bialek, “The information bottleneck method,” in Proc. 37th Annu.

Allerton Conf. Commun. Control Comput., 1999, pp. 368–377.

[41] H. Hsu, S. Asoodeh, S. Salamatian, and F. P. Calmon, “Generalizing bottleneck problems,” in Proc.

2018 IEEE Int. Symp. on Inf. Theory, 2018, pp. 531–535.

[42] P. Kairouz, S. Oh, and P. Viswanath, “Extremal mechanisms for local diﬀerential privacy,” in Advances

in Neural Information Processing Systems, 2014, pp. 2879–2887.

[43] S. L. Warner, “Randomized response: A survey technique for eliminating evasive answer bias,” J.

Amer. Statist. Assoc., vol. 60, no. 309, pp. 63–69, 1965.

[44] J. Geumlek and K. Chaudhuri, “Proﬁle-based privacy for locally private computations,” in Proc. 2019

IEEE Int. Symp. on Inf. Theory, 2019, pp. 537–541.

[45] I. Csisz´ar, “Information-type measures of diﬀerence of probability distributions and indirect observa-

tion,” studia scientiarum Mathematicarum Hungarica, vol. 2, pp. 229–318, 1967.

[46] F. P. Calmon and N. Fawaz, “Privacy against statistical inference,” in Proc. 50th Annu. Allerton Conf.

Commun. Control Comput., 2012, pp. 1401–1408.

[47] M. Bertran, N. Martinez, A. Papadaki, Q. Qiu, M. Rodrigues, G. Reeves, and G. Sapiro, “Adversar-
ially learned representations for information obfuscation and inference,” in Proc. 36th International
Conference on Machine Learning, 2019, pp. 614–623.

[48] H. Wang, L. Vo, F. P. Calmon, M. M´edard, K. R. Duﬀy, and M. Varia, “Privacy with estimation

guarantees,” IEEE Trans. Inf. Theory, vol. 65, no. 12, pp. 8025–8042, 2019.

[49] H. Hsu, S. Asoodeh, and F. P. Calmon, “Information-theoretic privacy watchdogs,” in Proc. 2019

IEEE Int. Symp. on Inf. Theory, 2019, pp. 552–556.

[50] I. Issa, A. B. Wagner, and S. Kamath, “An operational approach to information leakage,” IEEE Trans.

Inf. Theory, vol. 66, no. 3, pp. 1625–1657, 2020.

[51] M. Diaz, H. Wang, F. P. Calmon, and L. Sankar, “On the robustness of information-theoretic privacy

measures and mechanisms,” IEEE Trans. Inf. Theory, vol. 66, no. 4, pp. 1949–1978, 2020.

[52] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. Marchand, and
V. Lempitsky, “Domain-adversarial training of neural networks,” The Journal of Machine Learning
Research, vol. 17, no. 1, pp. 2096–2030, 2016.

[53] I. Kuzborskij and F. Orabona, “Stability and hypothesis transfer learning,” in Proc. 30th International

Conference on Machine Learning, 2013, pp. 942–950.

[54] S. Kpotufe and G. Martinet, “Marginal singularity, and the beneﬁts of labels in covariate-shift,” in

Proc. 31st Conference On Learning Theory, 2018, pp. 1882–1886.

22

[55] J. Angwin, J. Larson, S. Mattu, and L. Kirchner, “Machine bias,” ProPublica, May, vol. 23, p. 2016,

2016.

[56] D. B. Hunt, “Redlining,” Encyclopedia of Chicago, 2005.

[57] N. M. Kinyanjui, T. Odonga, C. Cintas, N. C. Codella, R. Panda, P. Sattigeri, and K. R. Varsh-
ney, “Estimating skin tone and eﬀects on classiﬁcation performance in dermatology datasets,” arXiv
preprint arXiv:1910.13268, 2019.

[58] J. Kleinberg, S. Mullainathan, and M. Raghavan, “Inherent trade-oﬀs in the fair determination of risk

scores,” arXiv preprint arXiv:1609.05807, 2016.

[59] A. Datta, S. Sen, and Y. Zick, “Algorithmic transparency via quantitative input inﬂuence: Theory and
experiments with learning systems,” in 2016 IEEE Symp. on Security and Privacy, 2016, pp. 598–617.

[60] A. Chouldechova, “Fair prediction with disparate impact: A study of bias in recidivism prediction

instruments,” Big data, vol. 5, no. 2, pp. 153–163, 2017.

[61] I. Chen, F. D. Johansson, and D. Sontag, “Why is my classiﬁer discriminatory?” in Advances in Neural

Information Processing Systems, 2018, pp. 3539–3550.

[62] H. Jiang, B. Kim, M. Guan, and M. Gupta, “To trust or not to trust a classiﬁer,” in Advances in

Neural Information Processing Systems, 2018, pp. 5541–5552.

[63] N. Kallus and A. Zhou, “Residual unfairness in fair machine learning from prejudiced data,” in Proc.

35th International Conference on Machine Learning, 2018, pp. 2439–2448.

[64] P. Adler, C. Falk, S. A. Friedler, T. Nix, G. Rybeck, C. Scheidegger, B. Smith, and S. Venkatasub-
ramanian, “Auditing black-box models for indirect inﬂuence,” Knowledge and Information Systems,
vol. 54, no. 1, pp. 95–122, 2018.

[65] S. Dutta, P. Venkatesh, P. Mardziel, A. Datta, and P. Grover, “An information-theoretic quantiﬁca-
tion of discrimination with exempt features,” in Proc. Thirty-Fourth AAAI Conference on Artiﬁcial
Intelligence, 2020.

[66] A. Cotter, M. Gupta, and H. Narasimhan, “On making stochastic classiﬁers deterministic,” in Advances

in Neural Information Processing Systems, 2019, pp. 10 910–10 920.

[67] G. Pleiss, M. Raghavan, F. Wu, J. Kleinberg, and K. Q. Weinberger, “On fairness and calibration,” in

Advances in Neural Information Processing Systems, 2017, pp. 5680–5689.

[68] R. Berk, H. Heidari, S. Jabbari, M. Kearns, and A. Roth, “Fairness in criminal justice risk assessments:

The state of the art,” Sociological Methods & Research, p. 0049124118782533, 2018.

[69] J. Chen, N. Kallus, X. Mao, G. Svacha, and M. Udell, “Fairness under unawareness: Assessing dis-
parity when protected class is unobserved,” in Proc. Conference on Fairness, Accountability, and
Transparency, 2019, p. 339–348.

[70] F. Kamiran and T. Calders, “Data preprocessing techniques for classiﬁcation without discrimination,”

Knowledge and Information Systems, vol. 33, no. 1, pp. 1–33, 2012.

[71] F. P. Calmon, D. Wei, B. Vinzamuri, K. N. Ramamurthy, and K. R. Varshney, “Optimized pre-
processing for discrimination prevention,” in Advances in Neural Information Processing Systems,
2017, pp. 3992–4001.

[72] A. K. Menon and R. C. Williamson, “The cost of fairness in binary classiﬁcation,” in Proc. 1st Con-

ference on Fairness, Accountability and Transparency, 2018, pp. 107–118.

[73] A. Agarwal, A. Beygelzimer, M. Dudik, J. Langford, and H. Wallach, “A reductions approach to fair

classiﬁcation,” in Proc. 35th International Conference on Machine Learning, 2018, pp. 60–69.

23

[74] M. Kearns, S. Neel, A. Roth, and Z. S. Wu, “Preventing fairness gerrymandering: Auditing and
learning for subgroup fairness,” in Proc. of the 35th International Conference on Machine Learning,
2018, pp. 2564–2572.

[75] A. Ghassami, S. Khodadadian, and N. Kiyavash, “Fairness in supervised learning: An information

theoretic approach,” in Proc. 2018 IEEE Int. Symp. on Inf. Theory, 2018, pp. 176–180.

[76] T. Hashimoto, M. Srivastava, H. Namkoong, and P. Liang, “Fairness without demographics in repeated
loss minimization,” in Proc. 35th International Conference on Machine Learning, 2018, pp. 1929–1938.

[77] M. P. Kim, A. Ghorbani, and J. Zou, “Multiaccuracy: Black-box post-processing for fairness in clas-
siﬁcation,” in Proc. 2019 AAAI/ACM Conference on AI, Ethics, and Society, 2019, pp. 247–254.

[78] L. E. Celis, L. Huang, V. Keswani, and N. K. Vishnoi, “Classiﬁcation with fairness constraints: A
meta-algorithm with provable guarantees,” in Proc. Conference on Fairness, Accountability, and Trans-
parency. ACM, 2019, pp. 319–328.

[79] W. Alghamdi, S. Asoodeh, H. Wang, F. P. Calmon, D. Wei, and K. N. Ramamurthy, “Model projection:
Theory and applications to fair machine learning,” in Proc. 2020 IEEE Int. Symp. on Inf. Theory, 2020.

[80] M. J. Kusner, J. Loftus, C. Russell, and R. Silva, “Counterfactual fairness,” in Advances in Neural

Information Processing Systems, 2017, pp. 4066–4076.

[81] N. Kilbertus, M. R. Carulla, G. Parascandolo, M. Hardt, D. Janzing, and B. Sch¨olkopf, “Avoiding
discrimination through causal reasoning,” in Advances in Neural Information Processing Systems,
2017, pp. 656–666.

[82] R. Nabi and I. Shpitser, “Fair inference on outcomes,” in Proc. Thirty-Second AAAI Conference on

Artiﬁcial Intelligence, 2018, pp. 1931–1940.

[83] L. Hu and Y. Chen, “Welfare and distributional

impacts of fair classiﬁcation,” arXiv preprint

arXiv:1807.01134, 2018.

[84] S. Chiappa, “Path-speciﬁc counterfactual fairness,” in Proc. Thirty-Third AAAI Conference on Arti-

ﬁcial Intelligence, vol. 33, 2019, pp. 7801–7808.

[85] Z. Lipton, J. McAuley, and A. Chouldechova, “Does mitigating ml’s impact disparity require treatment

disparity?” in Advances in Neural Information Processing Systems, 2018, pp. 8125–8135.

[86] J. Kleinberg, J. Ludwig, S. Mullainathan, and A. Rambachan, “Algorithmic fairness,” in Aea papers

and proceedings, vol. 108, 2018, pp. 22–27.

[87] A. Blum and T. Lykouris, “Advancing subgroup fairness via sleeping experts,” arXiv preprint

arXiv:1909.08375, 2019.

[88] S. A. Friedler, C. Scheidegger, and S. Venkatasubramanian, “On the (im) possibility of fairness,” arXiv

preprint arXiv:1609.07236, 2016.

[89] S. Corbett-Davies and S. Goel, “The measure and mismeasure of fairness: A critical review of fair

machine learning,” arXiv preprint arXiv:1808.00023, 2018.

[90] H. Zhao and G. J. Gordon, “Inherent tradeoﬀs in learning fair representations,” arXiv preprint

arXiv:1906.08386, 2019.

[91] I. Sason and S. Verdu, “f -divergence inequalities,” IEEE Trans. Inf. Theory, vol. 62, no. 11, pp.

5973–6006, 2016.

[92] J. Liu, P. Cuﬀ, and S. Verd´u, “Eγ-resolvability,” IEEE Trans. Inf. Theory, vol. 63, no. 5, pp. 2629–2658,

2016.

24

[93] M. H. DeGroot, “Uncertainty, information, and sequential experiments,” The Annals of Mathematical

Statistics, vol. 33, no. 2, pp. 404–419, 1962.

[94] M. B. Zafar, I. Valera, M. G. Rogriguez, and K. P. Gummadi, “Fairness constraints: Mechanisms for

fair classiﬁcation,” in Artiﬁcial Intelligence and Statistics, 2017, pp. 962–970.

[95] A. Nemirovsky and D. Yudin, Problem complexity and method eﬃciency in optimization. Wiley, 1983.

[96] A. Beck and M. Teboulle, “Mirror descent and nonlinear projected subgradient methods for convex

optimization,” Operations Research Letters, vol. 31, no. 3, pp. 167–175, 2003.

[97] A. R. Barron, “Universal approximation bounds for superpositions of a sigmoidal function,” IEEE

Trans. Inf. Theory, vol. 39, no. 3, pp. 930–945, 1993.

[98] M. Anthony and P. L. Bartlett, Neural network learning: Theoretical foundations. Cambridge Uni-

versity press, 2009.

[99] J. Friedman, T. Hastie, and R. Tibshirani, The elements of statistical learning.

Springer series in

statistics New York, 2001, vol. 1, no. 10.

[100] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin, “Liblinear: A library for large linear

classiﬁcation,” Journal of machine learning research, vol. 9, no. Aug, pp. 1871–1874, 2008.

[101] T. Kanamori, T. Suzuki, and M. Sugiyama, “f -divergence estimation and two-sample homogeneity
test under semiparametric density-ratio models,” IEEE Trans. Inf. Theory, vol. 58, no. 2, pp. 708–720,
2011.

[102] S. Kullback and R. A. Leibler, “On information and suﬃciency,” The Annals of Mathematical Statistics,

vol. 22, no. 1, pp. 79–86, 1951.

[103] K. Pearson, “On the criterion that a given system of deviations from the probable in the case of a
correlated system of variables is such that it can be reasonably supposed to have arisen from random
sampling,” The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, vol. 50,
no. 302, pp. 157–175, 1900.

[104] J. Lin, “Divergence measures based on the shannon entropy,” IEEE Trans. Inf. Theory, vol. 37, no. 1,

pp. 145–151, 1991.

[105] Y. Polyanskiy, H. V. Poor, and S. Verd´u, “Channel coding rate in the ﬁnite blocklength regime,” IEEE

Trans. Inf. Theory, vol. 56, no. 5, pp. 2307–2359, 2010.

[106] N. Sharma and N. A. Warsi, “Fundamental bound on the reliability of quantum information transmis-

sion,” Physical review letters, vol. 110, no. 8, p. 080501, 2013.

[107] K. Marton, “A measure concentration inequality for contracting markov chains,” Geometric & Func-

tional Analysis GAFA, vol. 6, no. 3, pp. 556–571, 1996.

[108] M. Raginsky, “Strong data processing inequalities and Φ-Sobolev inequalities for discrete channels,”

IEEE Trans. Inf. Theory, vol. 62, no. 6, pp. 3355–3389, 2016.

[109] C. D. Aliprantis and K. C. Border, Inﬁnite Dimensional Analysis: A Hitchhiker’s Guide. Springer,

2006.

[110] A. J. Kurdila and M. Zabarankin, Convex functional analysis.

Springer Science & Business Media,

2006.

[111] R. T. Rockafellar and R. J. Wets, “On the interchange of subdiﬀerentiation and conditional expectation
for convex functionals,” Stochastics: An International Journal of Probability and Stochastic Processes,
vol. 7, no. 3, pp. 173–182, 1982.

25

[112] X. Nguyen, M. J. Wainwright, and M. I. Jordan, “Estimating divergence functionals and the likelihood

ratio by convex risk minimization,” IEEE Trans. Inf. Theory, vol. 56, no. 11, pp. 5847–5861, 2010.

[113] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” arXiv preprint

arXiv:1412.6980, 2014.

[114] M. I. Belghazi, A. Baratin, S. Rajeswar, S. Ozair, Y. Bengio, A. Courville, and R. D. Hjelm, “Mine:

mutual information neural estimation,” arXiv preprint arXiv:1801.04062, 2018.

[115] H. Hsu, S. Asoodeh, and F. P. Calmon, “Obfuscation via information density estimation,” in Interna-

tional Conference on Artiﬁcial Intelligence and Statistics. PMLR, 2020, pp. 906–917.

26

(20)

(21)

(22)

(23)

(24)

Appendix A Examples of f -divergence

We recall some examples of f -divergence [45] here.

• KL-divergence [102]: f (x) = x log(x),

• Total variation distance: f (x) =

DKL(P

Q) =
(cid:107)

(cid:90)

log

(cid:19)

(cid:18) dP
dQ

dP.

x
|

/2,
1
|

−

DTV(P

Q) =
(cid:107)

(cid:90) (cid:12)
(cid:12)
(cid:12)
(cid:12)

1
2

dP
dQ −

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1

dQ.

• Chi-square divergence [103]: f (x) = (x

1)2 or f (x) = x2

−
(cid:90) (cid:18) dP

(cid:19)2

1

dQ =

1,

−
(cid:90) (cid:18) dP
dQ

(cid:19)2

dQ

1.

−

Dχ2(P

Q) =

(cid:107)

dQ −

• Jensen-Shannon divergence [104]: f (x) = x log(x)/2

(1 + x) log((1 + x)/2)/2,

JS(P

Q) =
(cid:107)

(cid:18)

DKL

P

1
2

(cid:107)

P + Q
2

+

1
2

DKL

(cid:18)

Q
(cid:107)

(cid:19)

.

P + Q
2

−
(cid:19)

Note that the Jensen-Shannon divergence is deﬁned in a general form in [104] for ω

[0, 1]

JSω(P

(cid:107)

Q) = ωDKL (P

ωP + (1
(cid:107)

−

ω)Q) + (1

ω)DKL (Q
(cid:107)

−

ωP + (1

• Eγ-divergence (also called hockey-stick divergence) [91, 92, 105, 106]: f (x) = (x

(a)+ (cid:44) max

a, 0
}
{

,

Eγ(P

Q) =
(cid:107)

(cid:90) (cid:18) dP

dQ −

(cid:19)

γ

+

dQ.

−

−

∈
ω)Q) .

γ)+ for γ

1 where

≥

(25)

• DeGroot statistical information [93] of order p: f (x) = min
p, 1
{
(cid:26)

(cid:90)

p(P

I

(cid:107)

Q) = min

p, 1
{

−

p

} −

min

p, 1

−

−

• Marton’s divergence [107]: f (x) = (x

1)2I[x < 1],

−

p

} −
dP
dQ

p

p, 1
{

−

px

}

for p

∈

(0, 1),

min

(cid:27)

dQ.

(26)

Q)2 = inf E (cid:2)Pr(X

= Y

d2(P

(cid:107)

Y )2(cid:3) =

|

(cid:90) (cid:18) dP

dQ −

(cid:19)2

1

I[

dP
dQ

< 1]dQ,

(27)

where the inﬁmum is taken over all couplings, i.e., joint distributions PX,Y which have marginals
PX = P and PY = Q, respectively.

We refer the readers to [91, 108] for more examples of f -divergence and their properties.

Appendix B Proofs for Section 3

B.1 Proof of Lemma 2

Proof. We ﬁrst introduce a (measurable) loss function (cid:96) : [0, 1]
function satisﬁes: (i) (cid:96)(a, a) = 0 for any a
The beneﬁt-of-splitting in Deﬁnition 2 can be written as

[0, 1] and (ii) for any a

×

∈

[0, 1]

∈

R+
→
[0, 1], (cid:96)(a,

∪ {∞}

and assume that this loss
) is convex and continuous.
·

inf
h:X →[0,1]

max
s∈{0,1}

E [(cid:96)(ys(X), h(X))

S = s]

max
s∈{0,1}

inf
h:X →[0,1]

−

|

E [(cid:96)(ys(X), h(X))

|

S = s] .

(28)

27

(cid:54)
a) log((1

b) (cid:44) a log(a/b) +
b)2, and KL loss (cid:96)(a, b) = DKL(a
By taking the (cid:96)1 loss (cid:96)(a, b) =
(cid:107)
(1
b)), respectively, the above quantity becomes (cid:15)split,1, (cid:15)split,2, and (cid:15)split,KL. These
loss functions all satisfy our above two assumptions. In particular, by our ﬁrst assumption, one can choose
h(x) = ys(x) which leads to

, (cid:96)2 loss (cid:96)(a, b) = (a
|

a)/(1

−

−

−

−

−

a

b

|

max
s∈{0,1}

inf
h:X →[0,1]

E [(cid:96)(ys(X), h(X))

S = s] = 0.

|

(29)

Hence, the problem remains providing equivalent expression for the inf-max term

inf
h:X →[0,1]

max
s∈{0,1}

E [(cid:96)(ys(X), h(X))

S = s]

|

= inf

h:X →[0,1]

sup
ω∈[0,1]

ω

·

E [(cid:96)(y0(X), h(X))

S = 0] + (1

ω)

·

−

|

E [(cid:96)(y1(X), h(X))

S = 1] .

|

(30)

Next, we use Ky Fan’s min-max theorem [24] (see Lemma 1) to swap the positions of inﬁmum and supremum
in (30). We start with verifying the assumptions in Ky Fan’s min-max theorem. We denote the set of all
R
measurable functions from

[0, 1]) and introduce a function F : [0, 1]

to [0, 1] by

[0, 1])

(
L

X

X →
E [(cid:96)(y0(X), h(X))

F (ω, h) (cid:44) ω

·
, h) is a linear function. Consequently, F (
For every ﬁxed h
[0, 1]), F (
·
·
[0, 1]), λ
and F is concave-like on [0, 1]. Furthermore, for any h1, h1 ∈ L
have λh1 + (1

(
X →

[0, 1]) and

−

|

·

(
∈ L
X →
(
λ)h2 ∈ L

−

∈

|

S = 0] + (1

ω)

E [(cid:96)(y1(X), h(X))

(
X →

×L
S = 1] .

→

, h) is upper semicontinuous
[0, 1], we

[0, 1], and ω

∈

X →
F (ω, λh1 + (1

λ)h2)

−

≤

λF (ω, h1) + (1

λ)F (ω, h2)

−

by the convexity of (cid:96)(a,
Fan’s min-max theorem, (30) is equal to

) for any a

∈

·

[0, 1]. Hence, F is convex-like on

(
X →

L

[0, 1]). Therefore, by Ky

sup
ω∈[0,1]

inf
h:X →[0,1]

ω

·

E [(cid:96)(y0(X), h(X))

S = 0] + (1

ω)

·

−

|

E [(cid:96)(y1(X), h(X))

S = 1] .

|

(31)

Now we take any probability distribution P over
respect to P . For example, one can simply choose dP = (dP0 + dP1)/2. Then (31) can be written as

such that P0 and P1 are absolutely continuous with

X

(cid:90) (cid:18)

sup
ω∈[0,1]

inf
h:X →[0,1]

ω

·

(cid:96)(y0(x), h(x))

dP0(x)
dP (x)

+ (1

ω)

·

−

(cid:96)(y1(x), h(x))

(cid:19)

dP1(x)
dP (x)

dP (x).

(32)

Next, we prove that the inﬁmum and the integer in (32) can be interchanged. For a ﬁxed ω
introduce a function f :

[0, 1]

R

X ×

→

[0, 1], we

∈

f (x, ¯h) (cid:44) ω

(cid:96)(y0(x), ¯h)

·

dP0(x)
dP (x)

+ (1

ω)

·

−

(cid:96)(y1(x), ¯h)

dP1(x)
dP (x)

and aim at proving

(cid:90)

inf
h:X →[0,1]

f (x, h(x))dP (x) =

(cid:90)

f (x, ¯h)dP (x).

inf
¯h∈[0,1]

(33)

, ¯h) is measurable and f (x,
Since f (
·
Hence, by the measurable maximum theorem [see Theorem 18.19 in 109], the mapping

) is continuous, f is a Carath´eodory function [see Section 4.10 in 109].

·

x

inf
¯h∈[0,1]

→

f (x, ¯h)

is measurable and the argmin correspondence (i.e., set-valued function)

(cid:26)

¯h∗

∗(x) (cid:44)

H

[0, 1]

|

∈

f (x, ¯h∗) = inf

¯h∈[0,1]

(cid:27)

f (x, ¯h)

28

is also measurable and admits a measurable selector. We denote this selector by h∗ :
by deﬁnition, it satisﬁes h∗(x)
LHS

[0, 1] and,
. Now we are ready to prove (33). One direction

∗(x) for all x

X →

[0, 1]

∈ H

∈ X

≥

RHS can be obtained directly since for any h :
(cid:90)

(cid:90)

X →

By the deﬁnition of h∗(x),

f (x, h(x))dP (x)

inf
¯h∈[0,1]

≥

f (x, ¯h)dP (x).

(cid:90)

RHS =

f (x, h∗(x))dP (x)

(cid:90)

inf
h:X →[0,1]

≥

f (x, h(x))dP (x) = LHS.

Therefore, the equality in (33) holds and (32) becomes

(cid:90) (cid:18)

sup
ω∈[0,1]

ω

·

(cid:96)(y0(x), h∗(x))

dP0(x)
dP (x)

+ (1

ω)

·

−

(cid:96)(y1(x), h∗(x))

(cid:19)

dP1(x)
dP (x)

dP (x).

(34)

Hence, our last step is to compute the function h∗ for the loss functions of interest. If the loss function is
(cid:96)1, then

argmin
¯h∈[0,1]

f (x, ¯h) = argmin
¯h∈[0,1]

(cid:26)

ω

dP0(x)
dP (x) · |

¯h

+(1
y0(x)
|

−

−

ω)

dP1(x)
dP (x) · |

¯h

(cid:27)

y1(x)
|

−

.

For a ﬁxed ω

∈

[0, 1], the optimal classiﬁer is

h∗(x) =

(cid:40)

y0(x)

y1(x)

if dP0(x)
dP1(x) ≥
otherwise.

1−ω
ω

By substituting the optimal classiﬁer and (cid:96)1 loss into (34), we get the desired equivalent expression of (cid:15)split,1:

(cid:90)

(cid:90)

sup
ω∈[0,1]

(1

−

ω)

y1(x)

Aω |

y0(x)

dP1(x) + ω
|

−

y1(x)

ω |
Ac

y0(x)

dP0(x),
|

−

where

(cid:110)

x

ω (cid:44)

A

dP0(x)
dP1(x) ≥

|

1−ω
ω

(cid:111)

. Similarly, when (cid:96)2 loss is used, the optimal classiﬁer becomes

h∗(x) =

ωy0(x)dP0(x) + (1
ωdP0(x) + (1

which leads to the equivalent expression of (cid:15)split,2:

ω)y1(x)dP1(x)
ω)dP1(x)

,

−
−

(35)

(cid:90) (y1(x)

y0(x))2dP0(x)dP1(x)

sup
ω∈[0,1]

ω(1

−

ω)

−

ωdP0(x) + (1

.

ω)dP1(x)

−

When the KL-loss is used, the optimal classiﬁer h∗ has expression in (35) as well. Consequently, we have
the equivalent expression of (cid:15)split,KL:
ωE [DKL(y0(X)

S = 0] + (1

h∗(X))

S = 1] .

(36)

ω)E [DKL(y1(X)
(cid:107)

−

|

h∗(X))
(cid:107)

|

sup
ω∈[0,1]

This expression can be further simpliﬁed by using the chain rule of KL-divergence:

DKL(QX,Y

(cid:107)

RX,Y ) = DKL(QY |X (cid:107)
−
S = s]

RY |X |
w)dP1, QY |X (1
|

QX ) + DKL(QX

RX ).

(cid:107)

x) = ys(x), and RY |X (1
|

By taking dQX = dPs, dRX = wdP0 + (1
E [DKL(ys(X)
h∗(X))
(cid:107)
= DKL

|

(cid:0)PX,Y |S=s(cid:107)

ωPX,Y |S=0 + (1

ω)PX,Y |S=1

−

(cid:1)

−

DKL (Ps

ωP0 + (1
(cid:107)

−

ω)P1) .

x) = h∗(x), we obtain

Substituting this into (36) gives

(cid:15)split,KL = sup
ω∈[0,1]

JSω(PX,Y |S=0(cid:107)

PX,Y |S=1)

JSω(P0(cid:107)

P1),

−

where JSω(

) is the Jensen-Shannon divergence (see (24) for its deﬁnition).

·(cid:107)·

29

B.2 Proof of Theorem 1

We divide the proof of Theorem 1 into three independent steps. First, we prove the upper bounds for (cid:15)split,1,
(cid:15)split,2, and (cid:15)split,KL in a uniﬁed way. Then we prove the lower bounds for (cid:15)split,1 and (cid:15)split,KL using Lemma 2.
Finally, we prove the lower bound for (cid:15)split,2 by leveraging the proof techniques of Brown-Low’s two-points
lower bound [23].

Proof. Note that (29) implies in the information-theoretic regime, optimal split classiﬁers can always achieve
perfect performance. Speciﬁcally, one can select labeling functions y0 and y1 as split classiﬁers which have
zero loss on each group. Hence, the problem remains upper bounding the performance of the optimal
group-blind classiﬁer. To achieve this goal, we consider two special group-blind classiﬁers:

h∗(x) =

y0(x) +

dP1(x)
2dP (x)

y1(x),

h∗∗(x) =

(y0(x) + y1(x)),

dP0(x)
2dP (x)
1
2

(37)

(38)

where dP = (dP0 + dP1)/2. In what follows, we upper bound the performance of the group-blind classiﬁers
in (37) and (38) and these bounds will be naturally translated into the upper bounds of (cid:15)split,1, (cid:15)split,2, and
(cid:15)split,KL, respectively.

We upper bound (cid:15)split,1 by using the group-blind classiﬁer h∗ in (37).

(cid:15)split,1 = inf

h:X →[0,1]

max
s∈{0,1}

E [
h(X)
|

−

ys(X)

| |

S = s]

≤

=

E [

h∗(X)
|

max
s∈{0,1}
(cid:90)

y1(x)
|

−

y0(x)

By the Cauchy-Schwarz inequality, we can further upper bound (39) by

| |

ys(X)

−
dP1(x)
2dP (x)

|

S = s]

dP0(x).

(39)

(cid:115)

E [(y1(X)

y0(X))2

S = 0]

|

−

(cid:90) (cid:18) dP1(x)
2dP (x)

·

(cid:19)2

dP0(x).

(40)

Furthermore, we have

(cid:90) (cid:18) dP1(x)
2dP (x)

(cid:19)2

dP0(x) =

(cid:90) (cid:18) dP1(x)
dP (x)

(cid:19)2 dP0(x)
dP (x)

1
4

dP (x) =

(cid:90) (cid:18) dP1(x)
dP (x)

1
4

(cid:19)2 (cid:18)
2

dP1(x)
dP (x)

−

(cid:19)

dP (x).

(41)

Since 1

4 x2(2

x)

−

≤

holds for any x

0, the RHS of (41) can be upper bounded by

≥

(cid:12)
(cid:12)
1
(cid:12)
(cid:12)

dP (x) = 1

(cid:90)

1
2

|

−

dP1(x)

dP0(x)
|

−

= 1

−

DTV(P0(cid:107)

P1).

(42)

1

x

1
−
|
dP1(x)
dP (x) −

− |
(cid:90) (cid:12)
(cid:12)
(cid:12)
(cid:12)

1

−

Combining (39–42) gives

(cid:15)split,1 ≤
By symmetry, we can further tighten the upper bound

y0(X))2

−

(cid:112)E [(y1(X)

S = 0]

|

(cid:112)

1

·

DTV(P0(cid:107)

P1).

−

(cid:15)split,1 ≤

min
s∈{0,1}

(cid:112)E [(y1(X)

y0(X))2

−

S = s]

|

(cid:112)

1

·

DTV(P0(cid:107)

P1).

−

On the other hand, using the classiﬁer h∗∗ in (38) leads to an alternative upper bound

(cid:15)split,1 ≤

1
2

max
s∈{0,1}

E [
y1(X)
|

−

y0(X)

| |

S = s] .

30

≤

=

(cid:90)

(y1(x)

(y1(x)

−

−

Similarly, we can upper bound (cid:15)split,2 by using the classiﬁer h∗ in (37)

(cid:15)split,2 ≤

max
s∈{0,1}
(cid:90)

E (cid:2)(h∗(X)

ys(X))2

−
(cid:18) dP1(x)
2dP (x)

S = s(cid:3)

|
(cid:19)2

(cid:90)

dP0(x) +

(y1(x)

y0(x))2

−

(cid:19)2

(cid:18) dP0(x)
2dP (x)

dP1(x)

y0(x))2

y0(x))2 dP1(x)
2dP (x)

dP0(x),

(43)

where the second inequality uses the fact that max
(41), (42), we can further upper bound (43) by

a, b
{

} ≤

a + b. By the Cauchy-Schwarz inequality and

(cid:112)E [(y1(X)

DTV(P0(cid:107)
By symmetry, we can further tighten this upper bound by replacing it with
(cid:112)E [(y1(X)

y0(X))4

S = 0]

S = s]

(cid:112)
1

y0(X))4

−

−

1

|

·

P1).

(cid:112)

−

|

·

DTV(P0(cid:107)

−

P1).

min
s∈{0,1}

On the other hand, using the classiﬁer h∗∗ in (38) leads to an alternative upper bound for (cid:15)split,2.

We repeat the same strategy and upper bound (cid:15)split,KL by using the classiﬁer h∗ in (37)

(cid:15)split,KL

≤

≤

max
s∈{0,1}
E [DKL(y0(X)
(cid:107)

E [DKL(ys(X)
h∗(X))
(cid:107)
h∗(X))

|

S = s]

|

S = 0] + E [DKL(y1(X)
(cid:107)

h∗(X))

S = 1] .

|

Recall the chain rule of KL-divergence

DKL(QX,Y

RX,Y ) = DKL(QY |X (cid:107)
(cid:107)

RY |X |

QX ) + DKL(QX

RX ).
(cid:107)

By taking dQX = dPs, dRX = dP , QY |X (1
|
of h∗ in (37), we obtain

x) = ys(x), and RY |X (1
|

x) = h∗(x) and noticing the deﬁnition

E [DKL(ys(X)
h∗(X))
(cid:107)

|

S = s] = DKL

(cid:18)

PX,Y |S=s(cid:107)

PX,Y |S=0 + PX,Y |S=1
2

(cid:19)

(cid:18)

DKL

Ps

−

P0 + P1
2

(cid:107)

(cid:19)

.

(44)

Hence,

(cid:15)split,KL

2JS(PX,Y |S=0(cid:107)

PX,Y |S=1)

2JS(P0(cid:107)

P1),

−

≤

where JS(
an alternative upper bound for (cid:15)split,KL.

·(cid:107)·

) is the Jensen-Shannon divergence. On the other hand, taking the classiﬁer h∗∗ in (38) gives

We proceed to prove the lower bounds of (cid:15)split,1 and (cid:15)split,KL.

Proof. Recall that

(cid:110)

x

A0.5 (cid:44)

∈ X |

dP0(x)
dP1(x) ≥
(cid:90)

(cid:111)
1

. By Lemma 2, we have

(cid:33)

y1(x)

A0.5 |

y0(x)

dP1(x) +
|

−

y1(x)

dP0(x)
y0(x)
|

−

|

Ac

0.5

(cid:32)(cid:90)

1
2

(cid:15)split,1 ≥
(cid:32)

S = 1]

(cid:90)

−

Ac

0.5

y1(x)
|

−

y0(x)

(dP1(x)
|

−

dP0(x))

(cid:33)

E [

y1(X)
|

−

y0(X)

(cid:32)

E [

y1(X)
|

−

y0(X)

| |

| |

=

=

≥

=

1
2

1
2

1
2

1
2


E [

(cid:16)

E [
|

y1(X)
|

−

y0(X)

| |

S = 1]

−

(y1(x)

y1(X)

y0(X)

| |

−

S = 1]

−

(cid:112)E [(y1(X)

−

−

S = 1]

−

(cid:90)

y1(x)
|
(cid:115)

(cid:90)

(cid:18)

y0(x)

1

−

|

−

dP0(x)
dP1(x)

(cid:19)

+

(cid:33)

dP1(x)

(cid:21)
dP1(x)
1





y0(x))2dP1(x)

(cid:90) (cid:18)
1

dP0(x)
dP1(x)

−

(cid:19)2

(cid:20) dP0(x)
I
dP1(x) ≤

y0(X))2

S = 1]

d2(P0(cid:107)

·

P1)

|

(cid:17)

,

31

where d2(P0(cid:107)
1
(cid:15)split,1 ≥
2

P1) is Marton’s divergence. By symmetry, one can obtain

(cid:16)

E [

max
s∈{0,1}

y1(X)
|

−

y0(X)

| |

S = s]

−

(cid:112)E [(y1(X)

y0(X))2

−

S = s]

·

|

d2(P1−s

(cid:17)

Ps)
(cid:107)

.

(45)

Finally, the lower bound of (cid:15)split,KL follows directly from Lemma 2.

Before getting to the lower bound of (cid:15)split,2, we prove a useful lemma. Here we denote

As (cid:44) E [
(cid:113)

B (cid:44)

y0(X)

y1(X)
|
−
| |
P0) + 1.
Dχ2(P1(cid:107)

S = s]

for s

,
0, 1
}

∈ {

Lemma 3. Assume that A0 ≤
S = 0(cid:3)
if E (cid:2)(h(X)

y0(X))2

A1. For any measurable classiﬁer h :

(cid:15), then E (cid:2)(h(X)

y1(X))2

−

S = 1(cid:3)

X →
≥

|

[0, 1] and constant 0
B√(cid:15))2.
(A1 −

|
Proof. Consider a convex optimization problem

≤

−

(46a)

(46b)

(cid:15) < A2

0/B2,

≤

min
h:X →[0,1]

(h(x)

(cid:90)

(cid:90)

s.t.

(h(x)

−

−

y1(x))2dP1(x),

y0(x))2dP0(x)

(cid:15).

≤

Computing the Gateaux derivative of the Lagrange multiplier gives the following optimal conditions [110,
Theorem 6.6.1],

(h(x)

−

y1(x))dP1(x) + λ(h(x)

y0(x))dP0(x) = 0,

(cid:18)(cid:90)

λ

(h(x)

−

y0(x))2dP0(x)

−

(cid:19)

(cid:15)

= 0,

−

0,

λ

≥

which provides the optimal classiﬁer

h∗(x) =

y1(x)dP1(x) + λy0(x)dP0(x)
dP1(x) + λdP0(x)

.

We denote r(x) (cid:44) dP1(x)
dP0(x)

and simplify the expression of the optimal classiﬁer

If λ = 0, then h∗(x) = y1(x) and, consequently,

h∗(x) =

y1(x)r(x) + λy0(x)
r(x) + λ

.

(47)

(48)

(49)

(50)

E (cid:2)(h∗(X)
However, this contradicts our assumptions E (cid:2)(h∗(X)

y0(X))2

−

|

S = 0(cid:3) = E (cid:2)(y1(X)

y0(X))2
−
S = 0(cid:3)

S = 0(cid:3) .

|
(cid:15) and

E [
y1(X)
|

(cid:15) <

y0(X)

−
Dχ2(P1(cid:107)

| |
P0) + 1

S = 0]2

y0(X))2

|
E (cid:2)(y1(X)

−

≤

≤

y0(X))2

|

−

S = 0(cid:3) .

Hence, we have λ > 0. In this case, (48) and (50) imply

(cid:90) (cid:18) y1(x)r(x) + λy0(x)

r(x) + λ

−

(cid:19)2

y0(x)

dP0(x) = (cid:15).

We simplify the expression and obtain

(cid:90)

r(x)2

(cid:18) y1(x)

y0(x)

−
r(x) + λ

(cid:19)2

dP0(x) = (cid:15).

(51)

32

Now we consider lower bounding E (cid:2)(h∗(X)
optimal classiﬁer (50), we have

−

y1(X))2

|

S = 1(cid:3). By its deﬁnition and the expression of the

E (cid:2)(h∗(X)

y1(X))2

|

−

S = 1(cid:3) =

=

≥

=

=

(cid:90) (cid:18) y1(x)r(x) + λy0(x)

r(x) + λ

(cid:90) (cid:18) λ(y1(x)

y0(x))

−
r(x) + λ

−

(cid:19)2

dP1(x)

(cid:19)2

y1(x)

dP1(x)

y1(x)
|

−
r(x) + λ

y0(x)
|

(cid:19)2

dP1(x)

(cid:18)(cid:90) λ

(cid:18)(cid:90)

y1(x)

|

y0(x)
|

−

dP1(x)

−

(cid:90) r(x)
y1(x)
|
r(x) + λ

−

y0(x)
|

(cid:19)2

dP1(x)

(cid:18)

E [
|

y1(X)

y0(X)

| |

−

S = 1]

−

(cid:90) r(x)

y0(x)

y1(x)
|
r(x) + λ

−

(cid:19)2

|

dP1(x)

,

(52)

where the only inequality is due to the Cauchy-Schwarz inequality. Furthermore, by the Cauchy-Schwarz
inequality again and (51), we have

(cid:90) r(x)

y0(x)

y1(x)
|
r(x) + λ

−

|

dP1(x)

≤

=

(cid:115)

(cid:90)

r(x)2

(cid:18) y1(x)

y0(x)

−
r(x) + λ

(cid:19)2

(cid:90)

dP0(x)

r(x)dP1(x)

(cid:112)

(cid:15)E [r(X)

S = 1].

|

(53)

Recall that r(x) = dP1(x)
dP0(x)

. Hence,

E [r(X)

|

S = 1] =

(cid:90) dP1(x)
dP0(x)

dP1(x) =

(cid:90) (cid:34)(cid:18) dP1(x)
dP0(x)

(cid:19)2

−

(cid:35)
1

dP0(x) + 1 = Dχ2 (P1(cid:107)

P0) + 1.

(54)

By our assumptions,

E [

(cid:15)(Dχ2(P1(cid:107)
E [
y1(X)
|
−
Combining (52), (53), and (54) together, we conclude that

y1(X)
|
E [
|

−
S = 1]

−
y1(X)

| |
y0(X)

y0(X)

S = 1]

| |

≥

−

(cid:113)

P0) + 1)

y0(X)

| |

−

S = 0]

0.

≥

E (cid:2)(h∗(X)

y1(X))2

−

S = 1(cid:3)

|

≥

=

(E [
|
(cid:18)
E [
|

y1(X)

y0(X)

| |

−

S = 1]

−

y1(X)

y0(X)

| |

−

S = 1]

−

(cid:112)

|

(cid:15)E [r(X)
(cid:15) (cid:0)Dχ2 (P1(cid:107)

(cid:113)

S = 1])2

(cid:1) + 1)

(cid:19)2

.

P0

Now we are in a position to prove the lower bound for (cid:15)split,2.

Proof. By Lemma 3, for any classiﬁer h :

[0, 1] and

X →

0

≤
(cid:15), then E (cid:2)(h(X)

(cid:15) <

A2
0
B2 ,
y1(X))2

if E (cid:2)(h(X)
B√(cid:15))2, where A0, A1, and B
are deﬁned in (46). Now we take (cid:15) = (A0/(B + 1))2, which naturally satisﬁes the assumptions in Lemma 3.
As a result, if

(A1 −

S = 1(cid:3)

S = 0(cid:3)

y0(X))2

≥

−

≤

−

|

|

E (cid:2)(h(X)

y0(X))2

−

S = 0(cid:3)

|

(cid:18) A0
B + 1

(cid:19)2

,

≤

33

then

E (cid:2)(h(X)

y1(X))2

−

S = 1(cid:3)

|

(cid:18)
A1 −

B

A0
B + 1

≥

(cid:19)2

(cid:18) A0
B + 1

≥

(cid:19)2

where the second inequality is because of the assumption A1 ≥

A0. Consequently, for any h :

[0, 1],

X →

E (cid:2)(h(X)

max
s∈{0,1}

ys(X))2

−

S = s(cid:3)

|

(cid:19)2

(cid:18) A0
B + 1

≥

=

(cid:32) E [

y1(X)
|

(cid:112)Dχ2 (P1(cid:107)

−

S = 0]

y0(X)
| |
P0) + 1 + 1

(cid:33)2

.

B.3 Extension to Cross Entropy Loss

As discussed in Remark 1, one can deﬁne the beneﬁt-of-splitting under the cross entropy loss. We provide
upper and lower bounds of the corresponding beneﬁt-of-splitting as an extension of Theorem 1.

Proposition 4. The cross-entropy-beneﬁt-of-splitting can be upper and lower bounded

(cid:26)

(cid:15)split,H

min

≤

2JS(PX,Y |S=0(cid:107)

PX,Y |S=1)

−

P1), max
s∈{0,1}

(cid:20)

(cid:18)

DKL

E

ys(X)
(cid:107)

y0(X) + y1(X)
2

(cid:21)(cid:27)

S = s

,

(cid:19)

|

≥

(cid:15)split,H

JS(PX,Y |S=0(cid:107)
Proof. Recall that H(p, q) = DKL(p
(cid:107)

PX,Y |S=1)

−

2JS(P0(cid:107)
1
2 |

−

P1)

JS(P0(cid:107)
q) + H(p). Hence,

E [H(y0(X))

S = 0]

|

−

E [H(y1(X))

S = 1]
|

.

|

E [H(ys(X), h(X))

S = s] = E [DKL(ys(X)
h(X))
(cid:107)

|

|

S = s] + E [H(ys(X))

S = s] ,

|

which leads to

max
s∈{0,1}

inf
h:X →[0,1]

E [H(ys(X), h(X))

S = s] = max
s∈{0,1}

|

E [H(ys(X))

S = s] .

|

Since max

{

ai + bi

} ≤
inf
h:X →[0,1]

max

ai
{
}
max
s∈{0,1}

bi
{

+ max

,
}
E [H(ys(X), h(X))

inf
h:X →[0,1]

max
s∈{0,1}

≤

S = s]

|
E [KL(ys(X)
h(X))
(cid:107)

S = s] + max
s∈{0,1}

|

E [H(ys(X))

S = s] .

|

(55)

(56)

Combining (55) and (56) implies (cid:15)split,H
an upper bound of (cid:15)split,H as well. Now we proceed to prove the lower bound.

≤

(cid:15)split,KL. Therefore, the upper bound of (cid:15)split,KL in Theorem 1 is

max
s∈{0,1}

E [H(ys(X), h(X))

S = s]

|

(E [H(y0(X), h(X))

|

S = 0] + E [H(y1(X), h(X))

S = 1])

|

(E [DKL(y0(X)
(cid:107)

h(X))

S = 0] + E [DKL(y1(X)
(cid:107)

|

h(X))

|

S = 1])

inf
h:X →[0,1]
1
2
1
2

≥

=

inf
h:X →[0,1]

inf
h:X →[0,1]
1
2

+

(E [H(y0(X))

S = 0] + E [H(y1(X))

|

S = 1]) .

|

By the proof of Lemma 2, we have

inf
h:X →[0,1]

1
2
= JS(PX,Y |S=0(cid:107)

(E [DKL(y0(X)
(cid:107)
PX,Y |S=1)

|
JS(P0(cid:107)

−

P1).

h(X))

S = 0] + E [DKL(y1(X)

h(X))
(cid:107)

|

S = 1])

34

(57)

(58)

Combining (55), (57), (58) gives

(cid:15)split,H

≥

JS(PX,Y |S=0(cid:107)
+

(E [H(y0(X))

PX,Y |S=1)

JS(P0(cid:107)
S = 0] + E [H(y1(X))

P1)

−

1
2

|

S = 1])

max
s∈{0,1}

−

E [H(ys(X))

S = s]

|

= JS(PX,Y |S=0(cid:107)

PX,Y |S=1)

JS(P0(cid:107)

P1)

−

−

E [H(y0(X))

S = 0]

|

−

E [H(y1(X))

S = 1]
|

.

|

|
1
2 |

B.4 Proof of Proposition 1
E [
hS(X)
|

Proof. First, note that inf hs:X →[0,1]
for s∈{0,1}

focus is on upper and lower bounding

] = 0 as one can choose hs(x) = ys(x). Hence, our
yS(X)
|

−

(cid:15)split,pop = inf

h:X →[0,1]

E [
h(X)
|

yS(X)

−
(cid:90)

]
|

(cid:90)

= inf

h:X →[0,1]

Pr(S = 0)

h(x)
|

−

y0(x)

dP0(x) + Pr(S = 1)
|

h(x)
|

−

y1(x)

dP1(x).
|

(59)

By the proof of Lemma 2, the optimal classiﬁer of (59) is

h∗(x) =

(cid:40)

y0(x)

y1(x)

if Pr(S=0)·dP0(x)
otherwise.

Pr(S=1)·dP1(x) ≥

1

By plugging the optimal classiﬁer into (59), we can write (cid:15)split,pop equivalently as

Pr(S = 0)

E [

·

y1(X)
|

−

y0(X)

| |

(cid:90)

S = 0]

y1(x)
|

−

(Pr(S = 0)
y0(x)
|

·

dP0(x)

−

−

Pr(S = 1)

dP1(x))+ .

·

The desired upper bound can be obtained by dropping the negative term. Now we proceed to prove the
lower bound. Since

(cid:90)

≤

y1(x)
|

−

y0(x)
|

(Pr(S = 0)

(cid:90) (cid:18) dP0(x)

Pr(S = 0)

dP0(x)

·
−
Pr(S = 1)
Pr(S = 0)

(cid:19)

= Pr(S = 0)

·

dP1(x) −
(P0(cid:107)

E Pr(S=1)
Pr(S=0)

P1),

Pr(S = 1)

dP1(x))+

·

dP1(x)

+

where E Pr(S=1)
Pr(S=0)

(P0(cid:107)

P1) is the Eγ-divergence with γ = Pr(S = 1)/Pr(S = 0), we have

(cid:15)split,pop ≥

Pr(S = 0)

(cid:18)

E [

y1(X)
|

−

y0(X)

| |

S = 0]

E Pr(S=1)
Pr(S=0)

(P0(cid:107)

P1)

−

(cid:19)

.

Appendix C Proofs for Section 4

C.1 Proof of Theorem 2

Recall that the false error rate is the maximum between false positive rate and false negative rate

FERs(h) (cid:44) max

E [h(X)
{

|

Y = 0, S = s] , E [1

h(X)

Y = 1, S = s]
}

.

|

−

(60)

We prove the following lemma which will be used in the proof of Theorem 2.

35

Lemma 4. The false error rate has the following equivalent expressions

FERs(h) = max

= max

(cid:26) E [h(X)(1

Pr(Y = 0

(cid:26) E [h(X)(1

, 1

−

−

ys(X))

S = s]

|

|
S = s)
ys(X))fs(X)]
S = s)

, 1

−

S = s]

E [h(X)ys(X)
Pr(Y = 1

|
S = s)
−
(cid:27)
E [h(X)ys(X)fs(X)]
S = s)
Pr(Y = 1

|

.

(cid:27)

Pr(Y = 0

|

|

where fs(x) (cid:44) Pr(S=s|X=x)

Pr(S=s)

.

Proof. The proof follows directly from Bayes’s rule,

dPX|Y =0,S=s =

ys(x)

1
Pr(Y = 0

−

S = s)

|

dPX|S=s =

ys(x)

1
Pr(Y = 0

−

S = s) ·

|

fs(x)dPX .

Now we are in a position to prove Theorem 2.

Proof. By Lemma 4, the quantity inf h:X →[0,1] maxs∈{0,1} FERs(h) can be equivalently written as

inf
h:X →[0,1]

max
s∈{0,1}

(cid:26) E [h(X)(1

−

ys(X))fs(X)]
S = s)

Pr(Y = 0

|
where µµµ (cid:44) (µ0,0, µ0,1, µ1,0, µ1,1) and

, 1

−

E [h(X)ys(X)fs(X)]
S = s)
Pr(Y = 1

|

(cid:27)

= inf

h:X →[0,1]

max
µµµ∈∆4

G(µµµ, h),

(61)

G(µµµ, h) (cid:44) (cid:88)

(cid:18)

µs,0

s∈{0,1}

E [h(X)(1

−

ys(X))fs(X)]
S = s)

Pr(Y = 0

(cid:18)

+ µs,1

1

−

E [h(X)ys(X)fs(X)]
S = s)
Pr(Y = 1

(cid:19)(cid:19)

(cid:88)

=

µs,1 + E





(cid:88)

s∈{0,1}

s∈{0,1}

|
(cid:18) µs,0(1

−
Pr(Y = 0

ys(X))fs(X)

S = s) −

|

µs,1ys(X)fs(X)
S = s)
Pr(Y = 1

|



(cid:19)

h(X)

 .

By denoting

we can write

φs,0(x) (cid:44) (1

−
Pr(Y = 0

ys(x))fs(x)
S = s)

|

φs,1(x) (cid:44)

G(µµµ, h) =

(cid:88)

µs,1 + E





(cid:88)

s∈{0,1}

s,i∈{0,1}

ys(x)fs(x)

−
Pr(Y = 1

S = s)

|



µs,iφs,i(X)h(X)

 .

|

,

We next use Ky Fan’s min-max theorem [24] (see Lemma 1) to swap the positions of inﬁmum and maximum.
, h) is continuous on ∆4. Furthermore, for any
[0, 1], G(
First, ∆4 is a compact set and for any h :
·
∆4, G(µµµ,
h :
) is convex-like over all (measurable) classiﬁers
·
from

X →
, h) is linear over ∆4; for any µµµ
·

[0, 1], G(
to [0, 1]. Hence, we have

∈

X →
X

inf
h:X →[0,1]

max
µµµ∈∆4

G(µµµ, h) = max
µµµ∈∆4

inf
h:X →[0,1]

G(µµµ, h).

(62)

Next, we prove that, for any ﬁxed µµµ

∆4,

∈

inf
h:X →[0,1]





E

(cid:88)

s,i∈{0,1}

µs,iφs,i(X)h(X)


 = E



 inf

h:X →[0,1]

(cid:88)

s,i∈{0,1}



µs,iφs,i(X)h(X)

 .

(63)

One direction LHS

≥


RHS can be obtained directly since for any h :

µs,iφs,i(X)h(X)







(cid:88)

E



s,i∈{0,1}

[0, 1]

X →



E

≥

 inf

h:X →[0,1]

(cid:88)

s,i∈{0,1}

µs,iφs,i(X)h(X)

 .

36

Note that the inﬁmum in the RHS of (63) is point-wise. For any ﬁxed x
problem

∈ X

, the following optimization

inf
¯h∈[0,1]

(cid:88)

µs,iφs,i(x)¯h

s,i∈{0,1}

has an optimal solution ¯h∗ = I[(cid:80)
s,i∈{0,1} µs,iφs,i(x)
achieve the point-wise inﬁmum inside the expectation of the RHS in (63): h∗(x) = I[(cid:80)
Consequently, the RHS of (63) can be simpliﬁed as

0]. Hence, there is a measurable classiﬁer which can
0].

s,i∈{0,1} µs,iφs,i(x)

≤

≤

RHS = E









(cid:88)

s,i∈{0,1}





µs,iφs,i(X)



 ,

−

(64)

where for a
∈
the classiﬁer h∗ leads to

R, (a)− (cid:44) min
{

a, 0

. Since the LHS of (63) is an inﬁmum over all measurable classiﬁers, using
}





E

(cid:88)

s,i∈{0,1}

LHS

≤

µs,iφs,i(X)h∗(X)


 = E









(cid:88)





µs,iφs,i(X)



 = RHS.

s,i∈{0,1}

−

Combining (61–64) together implies

inf
h:X →[0,1]

max
s∈{0,1}

FERs(h) = max
µµµ∈∆4






Similarly, one can prove that







(cid:88)

µs,1 + E





(cid:88)

µs,iφs,i(X)



s∈{0,1}

s,i∈{0,1}



















.

.

−



ν(s)
i φs,i(X)



−

max
s∈{0,1}

inf
h:X →[0,1]

FERs(h) = max
s∈{0,1}

max
ννν(s)∈∆2






ν(s)
1 + E









(cid:88)

i∈{0,1}

C.2 Proof of Proposition 2

We start with a useful lemma which will be used in the proof of Proposition 2.

Lemma 5. Let f :
supergradient of f (x,

Rk
X ×
→
) at w0:
·

R be a bounded measurable function. For a ﬁxed x

, if v(x, w0)

∈ X

Rk is a

∈

f (x, w)

−

f (x, w0)

≤

v(x, w0)T (w

w0),

−

(65)

then E [v(X, w0)] is a supergradient of E [f (X,

)] at w0:

E [f (X, w)]

·
E [f (X, w0)]

E [v(X, w0)]T (w

w0).

(66)

≤
The proof of Lemma 5 follows directly by taking expectation on both sides of (65). We refer the readers to
[111] for a more general result on the interchangeability of subdiﬀerentiation and (conditional) expectation.
Now we are in a position to prove Proposition 2.

−

−

Proof. Consider a function g(x, µµµ) (cid:44) (cid:80)
supergradient at µµµ (cid:44) (µ0,0, µ0,1, µ1,0, µ1,1):

s∈{0,1} µs,1 +

(cid:16)(cid:80)

(cid:17)
s,i∈{0,1} µs,iφs,i(x)

−

. For a ﬁxed x, g(x,

) has a

·


i + φs,i(x)I

(cid:104) (cid:88)


(cid:105)
µs(cid:48),i(cid:48)φs(cid:48),i(cid:48)(x) < 0


s(cid:48),i(cid:48)∈{0,1}

.

s,i∈{0,1}

37

Therefore, by Lemma 5, g has a supergradient at µµµ:


i + E



φs,i(X)

·

∂g(µµµ)

(cid:51)

(cid:104) (cid:88)
I

(cid:105)
µs(cid:48),i(cid:48)φs(cid:48),i(cid:48)(X) < 0

s(cid:48),i(cid:48)∈{0,1}









.

s,i∈{0,1}

Now we introduce auxiliary functions

ψs,i(x) (cid:44) 1

i

−
−
Pr(Y = i

ys(x)
S = s)

|

,

s, i

0, 1

∈ {

.
}

By Bayes’s rule and the deﬁnition of φs,i (see (7)), we have

ψs,i(x)

·

dPX|S=s(x) = φs,i(x)

dPX (x).

·

Hence,


i + E



ψs,i(X)

·

∂g(µµµ)

(cid:51)

(cid:104) (cid:88)
I

(cid:105)
µs(cid:48),i(cid:48)φs(cid:48),i(cid:48)(X) < 0

s(cid:48),i(cid:48)∈{0,1}

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Similarly, one can obtain a closed-form supergradient of gs(ννν).

Appendix D Proofs for Section 5

D.1 Proof of Theorem 3





S = s





.

s,i∈{0,1}

We ﬁrst recall a useful lemma which can be proved by the variational representation [112] of total variation
distance.

Lemma 6. For any measurable and non-negative function f :

E [f (X0)]

|

−

E [f (X1)]

f
|≤ (cid:107)

where X0 ∼

P0 and X1 ∼

P1.

Now we are in a position to prove Theorem 3.

R+,

X →
∞DTV(P0(cid:107)
(cid:107)

P1),

argminh∈H

Proof. We prove the upper bound ﬁrst. Let h∗
h∗
S = s]. Then
s ∈
inf
h∈H

| |
S = s]

E [
|
h(X)
|

−
ys(X)

max
s∈{0,1}

ys(X)

E [
{

h(X)

max

E [

| |

−

≤

s be an optimal classiﬁer for the group s

0, 1
}

∈ {

, i.e.,

h∗
0(X)
|

−

y1(X)

| |

S = 1] , E [

h∗
0(X)
|

−

y0(X)

| |

.
S = 0]
}

By the triangle inequality,

h∗
0(X)

E [
|

−

y1(X)

| |

S = 1]

E [

h∗
1(X)
|

−

≤

h∗
0(X)

| |

S = 1] + E [
|

h∗
1(X)

−

y1(X)

| |

S = 1] .

Therefore,

inf
h∈H

max
s∈{0,1}

E [

h(X)
|

−

ys(X)

| |

S = s]

E [

h∗
1(X)
|

−

≤

h∗
0(X)

| |

S = 1] + max
s∈{0,1}

h∗
s(X)

E [
|

−

ys(X)

| |

S = s] ,

which implies that

(cid:15)H
split = inf
h∈H
E [

max
s∈{0,1}
h∗
1(X)
|

−

E [

h(X)
|
h∗
0(X)

| |

≤

ys(X)

−
S = 1] .

| |

S = s]

max
s∈{0,1}

inf
h∈H

−

E [
h(X)
|

−

ys(X)

| |

S = s]

38

By Lemma 6,

Therefore,

E [

By symmetry, we obtain the desired upper bound for (cid:15)H
(cid:15)H
split. By the triangle inequality, we have

split. Now we proceed to prove the lower bound for

E [
|

y1(X)

y0(X)

| |

−

S = 0]

≥

E [
h∗
1(X)
−
|
E [
h∗
1(X)
|

−

h∗
0(X)
| |
y1(X)

−

| |

S = 0]

−
S = 0] .

E [
h∗
0(X)
|

y0(X)

| |

−

S = 0]

h∗
1(X)

E [
|

−

y1(X)

| |

S = 0]

E [

h∗
1(X)
|

−

≤

y1(X)

| |

S = 1] + DTV(P0(cid:107)

P1).

y1(X)
|

y0(X)

S = 0]

E [

h∗
1(X)
|

−

≥

h∗
0(X)

| |

S = 0]

2λ

DTV(P0(cid:107)

−

P1).

−

| |

where λ (cid:44) (cid:80)

s∈{0,1}

E [
|

ys(X)

| |

−

S = s] /2. Hence,

−
h∗
s(X)

max
s∈{0,1}

E [
|

y1(X)

y0(X)

| |

−

S = s]

max
s∈{0,1}

E [
h∗
1(X)
|

−

≥

h∗
0(X)

| |

S = s]

2λ

DTV(P0(cid:107)

−

P1).

−

(67)

By a slight modiﬁcation of the proof of Theorem 1, we have

inf
h:X →[0,1]

max
s∈{0,1}

E [
|

h(X)

−

ys(X)

| |

S = s]

(cid:18)

1
2

≥

max
s∈{0,1}

E [
|

y1(X)

y0(X)

| |

−

S = s]

DTV(P0(cid:107)

P1)

−

.

(68)

(cid:19)

Substituting (67) into (68) leads to

inf
h∈H

max
s∈{0,1}

E [

h(X)
|

−

ys(X)

| |

S = s]

inf
h:X →[0,1]

max
s∈{0,1}

E [
|

h(X)

−

ys(X)

| |

S = s]

(cid:18)

max
s∈{0,1}

1
2

E [

h∗
1(X)
|

−

h∗
0(X)

| |

S = s]

2λ

2DTV(P0(cid:107)

−

P1)

−

≥

≥

(cid:19)

.

(69)

Finally, since max

a + b and

a, b
{
max
s∈{0,1}

inf
h∈H

} ≤
E [

h(X)
|

−

h∗
s}s∈{0,1} is the set of optimal split classiﬁers, then
{
h∗
s(X)
ys(X)

S = s]

ys(X)

S = s] = max
s∈{0,1}

E [
|

| |

| |

−

≤

2λ.

(70)

Combining (69) with (70) gives

(cid:15)H
split ≥

1
2

max
s∈{0,1}

E [

h∗
1(X)
|

−

h∗
0(X)

| |

S = s]

DTV(P0(cid:107)

−

P1)

−

3λ.

D.2 Proof of Proposition 3
Proof. By the triangle inequality, inf h∈H maxs∈{0,1} E [
h(X)
|

ys(X)

S = s]

≤

I + II where

| |

I (cid:44) inf
h∈H
II (cid:44) max
s∈{0,1}

max
s∈{0,1}
E [
|

h(X)

E [
|
h∗(X)

−
h∗(X)

−

| |

S = s] ,

ys(X)

| |

−

S = s] ,

(cid:3) where the
and h∗ is deﬁned in (37). Since max
random variable ¯X follows the probability distribution (P0 + P1)/2. By Barron’s approximation bounds [97],

2 inf h∈H E (cid:2)
|

a + b, we have I

h∗( ¯X)
|

h( ¯X)

} ≤

a, b

−

≤

{

inf
h∈H

E (cid:2)
h( ¯X)
|

−

h∗( ¯X)

(cid:3)
|

≤

diam(
X
√k

)C

,

(71)

39

where C (cid:44) (cid:82)
Rd (cid:107)
(see Appendix B.2), we have

(cid:107)2|

dw, (cid:102)h∗(w) (cid:44) 1
(cid:102)h∗(w)
|

w

(2π)d

(cid:82)
X h∗(x) exp(

−

iwx)dx. Moreover, by the proof of Theorem 1

II

min
s∈{0,1}

≤

(cid:112)E [(y1(X)

y0(X))2

−

S = s]

|

(cid:112)

1

·

DTV(P0(cid:107)

P1).

−

To summarize, if the hypothesis class contains feedforward neural network models with one layer of sigmoidal
functions, the

-beneﬁt-of-splitting has an upper bound below.

H

(cid:15)H
split = inf
h∈H

E [

max
s∈{0,1}

h(X)
|
(cid:112)E [(y1(X)

ys(X)

| |

−

S = s]

max
s∈{0,1}

inf
h∈H

−

E [

h(X)
|

y0(X))2

S = s]

|

(cid:112)

1

·

−

−

DTV(P0(cid:107)

P1) +

S = s]

| |

ys(X)

−
2diam(
X
√k

)C

.

min
s∈{0,1}

≤

D.3 Proof of Corollary 1

We approach Corollary 1 by proving a more general result.

Lemma 7. For any hypothesis
is at least

H

, there exists a probability distribution QS,X,Y whose

-beneﬁt-of-splitting

H

1
2

sup
|
h1,h0∈H
x∈X

h1(x)

h0(x)
|

.

−

Proof. For any (cid:15) > 0, there exist two classiﬁers h∗

1(x∗)
h∗

|

−

0(x∗)
h∗

|≥

1, h∗

0 ∈ H
sup
h1,h0∈H
x∈X

|

and x∗

h1(x)

−

such that

∈ X
h0(x)

(cid:15).

|−

(72)

Now we construct a probability distribution QS,X,Y with QY |X,S(1
0, 1
QS(s) = 0.5 for s
∈ {
}
(cid:15)H
0(x∗)
h∗
1(x∗)
h∗
1
split ≥
2 |
−
(cid:15)). Since this lower bound of (cid:15)H
h0(x)
|−
to the desired conclusion.

x∗),
) is the Dirac delta function. Our lower bound in Theorem 3 implies that
·

where δ(
which, due to (72), can be further lower bounded by 1

−
split holds for any (cid:15) > 0, one can let (cid:15) be suﬃciently small which leads

2 (suph1,h0∈H,x∈X |

s(x), QX|S=s(x) = δ(x

x, s) = h∗
|

h1(x)

−

|

D.4 Proof of Theorem 4

We introduce the empirical beneﬁt-of-splitting and bound its diﬀerence from the sample-limited-splitting
(see Deﬁnition 7).

Deﬁnition 8. For a given hypothesis class
the empirical-splitting is deﬁned as

H

and ns i.i.d. samples

(xs,i, ys,i)
}
{

ns
i=1 from group s

0, 1
}

∈ {

,

ˆ(cid:15)split,emp (cid:44) inf
h∈H

max
s∈{0,1}

(cid:80)ns

i=1|

h(xs,i)
ns

ys,i

|

−

max
s∈{0,1}

inf
h∈H

−

(cid:80)ns

i=1|

h(xs,i)
ns

ys,i

|

.

−

(73)

Lemma 8. Let
least 1

δ,

−

be a hypothesis class from

to

0, 1
}

{

X

H

with VC dimension D. Then with probability at

ˆ(cid:15)split

|

−

ˆ(cid:15)split,emp

4 max
s∈{0,1}

| ≤

(cid:115)

2D log(6ns) + 2 log(16/δ)
ns

,

(74)

where ns is the number of samples from group s

0, 1

∈ {

.
}

40

Proof. Corollary 3.8 and Theorem 4.3 in [98] together imply that with probability at least 1
s

and h

,

δ, for any

−

0, 1
}

∈ {

∈ H

(cid:80)ns

i=1|

(cid:12)
(cid:12)
(cid:12)
(cid:12)

h(xs,i)
ns

ys,i

−

|

E [
|

−

h(X)

−

ys(X)

| |

(cid:12)
(cid:12)
S = s]
(cid:12)
(cid:12) ≤

(cid:115)
2

2D log(6ns) + 2 log(8/δ)
ns

.

Therefore, for any hs

∈ H

with s

0, 1

∈ {

}

(cid:12)
(cid:12)
(cid:12)
(cid:12)

max
s∈{0,1}

(cid:80)ns

i=1|

hs(xs,i)
ns

ys,i

−

|

max
s∈{0,1}

−

E [
|

hs(X)

ys(X)

| |

−

(cid:12)
(cid:12)
S = s]
(cid:12)
(cid:12) ≤

2 max
s∈{0,1}

(cid:115)

2D log(6ns) + 2 log(8/δ)
ns

.

(75)

Recall that

ˆ(cid:15)split = max
s∈{0,1}

(cid:104)

E

ˆh∗(X)
|

−

ys(X)

| |

(cid:105)
S = s

(cid:104)

E

max
s∈{0,1}

−

ˆh∗
s(X)
|

−

ys(X)

| |

(cid:105)
S = s

.

Now by (75), we conclude that

ˆ(cid:15)split −
|

ˆ(cid:15)split,emp| ≤

4 max
s∈{0,1}

(cid:115)

2D log(6ns) + 2 log(8/δ)
ns

.

Since the upper and lower bounds of (cid:15)H

split (see Theorem 3) hold for any underlying distribution PS,X,Y .
One can plug in the empirical distribution and obtain the corresponding bounds for ˆ(cid:15)split,emp. Then we
obtain the desired bounds for ˆ(cid:15)split by using Lemma 8 for bounding the diﬀerence between ˆ(cid:15)split,emp and
ˆ(cid:15)split.

Appendix E Supporting Results for Experiments

E.1 Closed-form Expression of (cid:15)split,FER

Proof. For the distributions we construct, one can choose
}s∈{0,1} as the split classiﬁers which lead to
zero false error rate. Therefore, the problem remains computing the false error rate of the optimal group-blind
classiﬁer. First, the labeling functions naturally divide R2 into four parts: I (cid:44)
,
y0(x) = 1, y1(x) = 1
}
II (cid:44)
. Clearly, in
x
x
}
{
{
|
order to be an optimal group-blind classiﬁer, h must satisfy h(x) = 1 on I and h(x) = 0 on III. We deﬁne
|II and h
h

|
y0(x) = 0, y1(x) = 1

y0(x) = 0, y1(x) = 0

y0(x) = 1, y1(x) = 0

, III (cid:44)
}

, IV (cid:44)
}

ys(x)
{

|IV as

x
{

x

{

|

|

(cid:40)

|II(x) (cid:44)
h

|IV(x) (cid:44)
h
Due to our construction of the labeling functions and Lemma 4,
(cid:26) E [h(X)(1

S = 0]

y0(X))

II

if x
∈
otherwise,

h(x)
0

(cid:15)split,FER = inf

max

h:X →[0,1]

(cid:40)

h(x)
0

IV

if x
∈
otherwise.

−

−

|
S = 0)

|
y1(X))

|
S = 1)

|

S = 0] , 1

|
S = 1] , 1

Pr(Y = 0

E [h(X)(1

2E [h
{
2E [h

Pr(Y = 0
|IV(X)
|II(X)
1

|

{

−

E [h(X)y0(X)
Pr(Y = 1
|
E [h(X)y1(X)
Pr(Y = 1

, 1

, 1

−

−

S = 1]

2 Pr(X

−

2 Pr(X

−

I

∈
I

S = 0)

|
S = 1)

∈
|
2E [h

−
|II(X)

|

S = 0]

|
S = 0)

S = 1]

,

(cid:27)

|

−

|
S = 1)
2E [h

|

2E [h

|II(X)
|IV(X)
|
S = 0] , 2E [h

max

2 Pr(X

I

|

∈

S = 0)

−

S = 0] ,

S = 1]
}

|II(X)

= inf

h:X →[0,1]

max

(cid:26)

= max

inf
h|II:X →[0,1]

S = 1]
}

,

|

(cid:27)

inf
h|IV:X →[0,1]

max

2E [h

{
2E [h
|IV(X)
{

|

|IV(X)

|
S = 0] , 1

−

2 Pr(X

I

|

∈

−

|

∈
S = 1)

2E [h

−
2E [h
|IV(X)

|IV(X)

|
S = 1]
}

|

S = 1]
}
(76)

,

−

S = 0] , 1

2 Pr(X

I

S = 1)

=

inf
h|IV:X →[0,1]

max

41

a, b
where the last step is because of symmetry. Since 2 max
{
} ≥
E [h
|IV(X)

inf
h|IV:X →[0,1]

S = 1) +

Pr(X

∈

I

|

|

a + b, (cid:15)split,FER can be lower bounded by

S = 0]

E [h

|IV(X)

|

−

S = 1]

1
2 −
1
2 −
1
2 −

=

=

(cid:90)

Pr(X

Pr(X

I

I

|

|

∈

∈

S = 1) +

inf
h|IV:X →[0,1]
(cid:90)

|IV(x)(dP0(x)
h

−

IV

dP1(x))

S = 1)

(dP1(x)

−

−

IV

dP0(x))+.

Since P0 ∼ N

(0, Σ0) and P0 ∼ N
(cid:44)
IV

dP1(x) > dP0(x)
}
Therefore, (cid:15)split,FER can be lower bounded by

x
{

A

∈

|

=

(0, Σ1), by comparing their probability density functions, we have

x
{

∈

IV

x2 < 0
}

|

=

x

{

∈

R2

|

y1(x) = 1, x2 < 0

.
}

1
2

(1

−

By symmetry, we have

2 Pr(X

I

|

∈

S = 1)

−

2 Pr(X

∈ A |

S = 1) + 2 Pr(X

S = 0)) .

∈ A |

(77)

Pr(X

I

|

∈

S = 1) = Pr(X

III

|

∈

S = 1), Pr(X

II

|

∈

S = 1) = Pr(X

IV

|

∈

S = 1),

which leads to

2 Pr(X

1

−

I

|

∈

S = 1)

−

2 Pr(X

∈ A |

S = 1) = 2 Pr(X

IV

∈

\A |

S = 1) = 2 Pr(X

∈ A |

S = 0),

(78)

where the last step is by symmetry again. Therefore, the lower bound of (cid:15)split,FER in (77) can be simpliﬁed
∗
as 2 Pr(X
IV(x) = 0
; h
|
∈ A |
otherwise. By (76), we have

S = 0). On the other hand, we can design a classiﬁer h
|

∗
IV(x) = 1 if x

∈ A

(cid:15)split,FER

max

≤
= max

{

{

2E [h
∗
IV(X)
|
2 Pr(X

|

∈ A |

S = 0] , 1

S = 0), 1

2 Pr(X

2 Pr(X

I

I

|

|

∈

∈

S = 1)

S = 1)

−

−

−

2E [h
∗
IV(X)
|
2 Pr(X

|

S = 1]
}

S = 1)
}

.

−
S = 0) as well. Hence, (cid:15)split,FER =

∈ A |

Due to (78), the above upper bound of (cid:15)split,FER is equal to 2 Pr(X
2 Pr(X

S = 0).

∈ A |

E.2 Total Variation Distance Estimation

∈ A |

We provide details on how we estimate the total variation distance DTV(P0(cid:107)
data
xs,i
{
equivalently as

P1) by using ns i.i.d. unlabeled
. By applying Baye’s rule, we can write the density ratio
0, 1
}

ns
i=1 drawn from each group s

∈ {

}

dP1(x)
dP0(x)

=

dPX|S=1(x)
dPX|S=0(x)

=

Pr(S = 1

X = x)

|
Pr(S = 1

1

−

X = x) ·

|

1

Pr(S = 1)

−
Pr(S = 1)

,

which leads to:

(cid:90)

DTV(P0(cid:107)

P1) =

(cid:20) Pr(S = 1
I
1

|
Pr(S = 1

−

X = x)

X = x) ≥

1

|

Pr(S = 1)

Pr(S = 1)

−

(cid:21)
dP1(x)

(cid:90)

−

(cid:20) Pr(S = 1
I
1

|
Pr(S = 1

−

X = x)

X = x) ≥

1

|

(cid:21)
dP0(x).

Pr(S = 1)

Pr(S = 1)

−
(79)

This expression gives rise to the following procedure of estimating the total variation distance.

• Compute a constant α = n1

n0+n1

to estimate the marginal probability Pr(S = 1) and train a classi-
ﬁer s(x) to approximate the conditional distribution Pr(S = 1
In particular, we use a
feed-forward neural network for s(x), which consists of one hidden layer with 100 neurons and ReLU
activation, and a soft-max readout layer. We adopt cross entropy as the loss function, set learning rate
to be 0.001, and use AdamOptimizer [113] to train the datasets with batch size 200. To avoid overﬁt-
ting, we hold 10% of the samples as a validation set, and terminate training once the validation loss
is not improving by 10−4 for the next 10 consecutive epochs (i.e., early stopping), and the maximum
number of epochs is set to be 200.

X = x).

|

42

• By plugging α and s(x) into (79) and using i.i.d. samples to estimate the integrals (i.e., expectations),

we obtain the following approximation of DTV(P0(cid:107)
n1(cid:88)
(cid:21)

α

P1):

1
n1

(cid:20) s(x1,i)
I
1

s(x1,i) ≥

i=1

−

1
n0

n0(cid:88)

I

i=1

(cid:20) s(x0,i)
1

s(x0,i) ≥

−

(cid:21)
.

α

−

1

α

(80)

α

−

1

−

We remark that estimating information-theoretic measures has been studied in [e.g., 26, 27, 101, 114, 115].

43

