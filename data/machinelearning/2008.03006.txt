Noname manuscript No.
(will be inserted by the editor)

Polynomial-time algorithms for Multimarginal Optimal Transport
problems with structure

Jason M. Altschuler · Enric Boix-Adser`a

2
2
0
2

l
u
J

6
1

]

C
O
.
h
t
a
m

[

4
v
6
0
0
3
0
.
8
0
0
2
:
v
i
X
r
a

Abstract Multimarginal Optimal Transport (MOT) has attracted signiﬁcant interest due to applications in
machine learning, statistics, and the sciences. However, in most applications, the success of MOT is severely
limited by a lack of eﬃcient algorithms. Indeed, MOT in general requires exponential time in the number of
marginals k and their support sizes n. This paper develops a general theory about what “structure” makes
MOT solvable in poly(n, k) time.

We develop a uniﬁed algorithmic framework for solving MOT in poly(n, k) time by characterizing the struc-
ture that diﬀerent algorithms require in terms of simple variants of the dual feasibility oracle. This framework
has several beneﬁts. First, it enables us to show that the Sinkhorn algorithm, which is currently the most
popular MOT algorithm, requires strictly more structure than other algorithms do to solve MOT in poly(n, k)
time. Second, our framework makes it much simpler to develop poly(n, k) time algorithms for a given MOT
problem. In particular, it is necessary and suﬃcient to (approximately) solve the dual feasibility oracle—which
is much more amenable to standard algorithmic techniques.

We illustrate this ease-of-use by developing poly(n, k)-time algorithms for three general classes of MOT cost
structures: (1) graphical structure; (2) set-optimization structure; and (3) low-rank plus sparse structure. For
structure (1), we recover the known result that Sinkhorn has poly(n, k) runtime [49, 82]; moreover, we provide
the ﬁrst poly(n, k) time algorithms for computing solutions that are exact and sparse. For structures (2)-(3), we
give the ﬁrst poly(n, k) time algorithms, even for approximate computation. Together, these three structures
encompass many—if not most—current applications of MOT.

Keywords Multimarginal optimal transport · polynomial-time algorithms · implicit linear programming ·
structured linear programs

Mathematics Subject Classiﬁcation (2000) 90C08 · 90C06

Contents

2
1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.1 Contribution 1: uniﬁed algorithmic framework for MOT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
1.2 Contribution 2: applications to general classes of structured MOT problems . . . . . . . . . . . . . . . . . . . . .
1.3 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
1.4 Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.1 Multimarginal Optimal Transport
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
2.2 Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
3 Oracles
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
4 Algorithms to oracles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
4.1 The Ellipsoid algorithm and the MIN oracle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
4.2 The Multiplicative Weights Update algorithm and the AMIN oracle . . . . . . . . . . . . . . . . . . . . . . . . . . 19

Work partially supported by NSF GRFP 1122374, a TwoSigma PhD fellowship, and a Siebel Scholarship.

J.M. Altschuler · E. Boix-Adser`a
Laboratory for Information and Decision Systems (LIDS), Massachusetts Institute of Technology, Cambridge MA 02139.
E-mail: jasonalt@mit.edu
E-mail: eboix@mit.edu

 
 
 
 
 
 
2

Jason M. Altschuler, Enric Boix-Adser`a

4.3 The Sinkhorn algorithm and the SMIN oracle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
5 Application: MOT problems with graphical structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
5.1 Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
5.2 Polynomial-time algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
5.3 Application vignette: ﬂuid dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
6 Application: MOT problems with set-optimization structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
6.1 Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
6.2 Polynomial-time algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
6.3 Application vignette: network reliability with correlations
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
7 Application: MOT problems with low-rank plus sparse structure
7.1 Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
7.2 Polynomial-time algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
7.3 Application vignette: risk estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
7.4 Application vignette: projection to the transportation polytope . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
8 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
A Deferred proof details
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
B Additional numerical experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45

1 Introduction

Multimarginal Optimal Transport (MOT) is the problem of linear programming over joint probability distri-
butions with ﬁxed marginal distributions. In this way, MOT generalizes the classical Kantorovich formulation
of Optimal Transport from 2 marginal distributions to an arbitrary number k (cid:62) 2 of them.

More precisely, an MOT problem is speciﬁed by a cost tensor C in the k-fold tensor product space (Rn)⊗k =
i=1 vi = 1}.1 The

Rn ⊗ · · · ⊗ Rn, and k marginal distributions µ1, . . . , µk in the simplex ∆n = {v ∈ Rn
MOT problem is to compute

(cid:62)0 : (cid:80)n

min
P ∈M(µ1,...,µk)

(cid:104)P, C(cid:105)

(MOT)

where M(µ1, . . . , µk) is the “transportation polytope” consisting of all entrywise non-negative tensors P ∈
(Rn)⊗k satisfying the marginal constraints (cid:80)
Pj1,...,ji−1,j,ji+1,...,jk = [µi]j for all i ∈ {1, . . . , k}
and j ∈ {1, . . . , n}.

j1,...,ji−1,ji+1,...,jk

This MOT problem has many applications throughout machine learning, computer science, and the nat-
ural sciences since it arises in tasks that require “stitching” together aggregate measurements. For instance,
applications of MOT include inference from collective dynamics [38, 49], information fusion for Bayesian learn-
ing [80], averaging point clouds [2, 36], the n-coupling problem [76], quantile aggregation [60, 75], matching
for teams [28, 31], image processing [74, 79], random combinatorial optimization [1, 50, 61, 65, 68, 89, 93],
Distributionally Robust Optimization [30, 62, 66], simulation of incompressible ﬂuids [17, 26], and Density
Functional Theory [16, 27, 32].

However, in most applications, the success of MOT is severely limited by the lack of eﬃcient algorithms.
Indeed, in general, MOT requires exponential time in the number of marginals k and their support sizes n. For
instance, applying a linear program solver out-of-the-box takes nΘ(k) time because MOT is a linear program
with nk variables, nk non-negativity constraints, and nk equality constraints. Specialized algorithms in the
literature such as the Sinkhorn algorithm yield similar nΘ(k) runtimes. Such runtimes currently limit the
applicability of MOT to tiny-scale problems (e.g., n = k = 10).

Polynomial-time algorithms for MOT. This paper develops polynomial-time algorithms for MOT, where here
and henceforth “polynomial” means in the number of marginals k and their support sizes n—and possibly also
Cmax/ε for ε-additive approximation, where Cmax is a bound on the entries of C.

At ﬁrst glance, this may seem impossible for at least two “trivial” reasons. One is that it takes exponential
time to read the input cost C since it has nk entries. We circumvent this issue by considering costs C with
poly(n, k)-size implicit representations, which encompasses essentially all MOT applications.2 A second obvious

1 For simplicity, all µi are assumed to have the same support size n. Everything in this paper extends in a straightforward

way to non-uniform sizes ni where nk is replaced by (cid:81)k

i=1 ni, and poly(n, k) is replaced by poly(maxi ni, k).

2 E.g., in the MOT problems of Wasserstein barycenters, generalized Euler ﬂows, and Density Functional Theory, C has

entries Cj1,...,jk = (cid:80)k

i,i(cid:48)=1 gi,i(cid:48) (ji, ji(cid:48) ) and thus can be implicitly input via the k2 functions gi,i(cid:48) : {1, . . . , n}2 → R.

Polynomial-time algorithms for Multimarginal Optimal Transport problems with structure

3

Algorithm Oracle
ELLIPSOID
MWU
SINKHORN

MIN
AMIN
SMIN

Runtime
Theorem 4.1
Theorem 4.7
Theorem 4.18

Always applicable? Exact solution?

Sparse solution? Practical?

Yes
Yes
No

Yes
No
No

Yes
Yes
No

No
Yes
Yes

Table 1: These MOT algorithms have polynomial runtime except for a bottleneck “oracle”. Each oracle is a simple variant of
the dual feasibility oracle for MOT. The number of oracle computations is poly(n, k) for ELLIPSOID, and poly(n, k, Cmax/ε) for
both MWU and SINKHORN. From a theoretical perspective, the most important aspect of an algorithm is whether it can solve MOT
in polynomial time if and only if any algorithm can. We show that ELLIPSOID and MWU satisfy this (Theorem 1.1), but SINKHORN
does not (Theorem 1.3). From a practical perspective, SINKHORN is the most scalable when applicable.3

issue is that it takes exponential time to write the output variable P since it has nk entries. We circumvent
this issue by returning solutions P with poly(n, k)-size implicit representations, for instance sparse solutions.
But, of course, circumventing these issues of input/output size is not enough to actually solve MOT in
polynomial time. See [6] for examples of NP-hard MOT problems with costs that have poly(n, k)-size implicit
representations.

Remarkably, for several MOT problems, there are specially-tailored algorithms that run in polynomial
time—notably, for MOT problems with graphically-structured costs of constant treewidth [47, 49, 82], varia-
tional mean-ﬁeld games [15], computing generalized Euler ﬂows [14], computing low-dimensional Wasserstein
barycenters [7, 14, 29], and ﬁltering and estimation tasks based on target tracking [38, 46, 47, 49, 77]. However,
the number of MOT problems that are known to be solvable in polynomial time is small, and it is unknown if
these techniques can be extended to the many other MOT problems arising in applications. This motivates the
central question driving this paper:

Are there general “structural properties” that make MOT solvable in poly(n, k) time?

This paper is conceptually divided into two parts. In the ﬁrst part of the paper, we develop a uniﬁed
algorithmic framework for MOT that characterizes the structure required for diﬀerent algorithms to solve MOT
in poly(n, k) time, in terms of simple variants of the dual feasibility oracle. This enables us to prove that some
algorithms can solve MOT problems in polynomial time whenever any algorithm can; whereas the popular
SINKHORN algorithm cannot. Moreover, this algorithmic framework makes it signiﬁcantly easier to design a
poly(n, k) time algorithm for a given MOT problem (when possible) because it now suﬃces to solve the dual
feasibility oracle—and this is much more amenable to standard algorithmic techniques. In the second part of
the paper, we demonstrate the ease-of-use of our algorithmic framework by applying it to three general classes
of MOT cost structures.

Below, we detail these two parts of the paper in §1.1 and §1.2, respectively.

1.1 Contribution 1: uniﬁed algorithmic framework for MOT

In order to understand what structural properties make MOT solvable in polynomial time, we ﬁrst lay a more
general groundwork. The purpose of this is to understand the following fundamental questions:

Q1 What are reasonable candidate algorithms for solving structured MOT problems in polynomial time?
Q2 What structure must an MOT problem have for these algorithms to have polynomial runtimes?
Q3 Is the structure required by a given algorithm more restrictive than the structure required by a diﬀerent

algorithm (or any algorithm)?

Q4 How to check if this structure occurs for a given MOT problem?
We detail our answers to these four questions below in §1.1.1 to §1.1.4, and then brieﬂy discuss practical
tradeoﬀs beyond polynomial-time solvability in §1.1.5; see Table 1 for a summary. We expect that this general
groundwork will prove useful in future investigations of tractable MOT problems.

1.1.1 Answer to Q1: candidate poly(n, k)-time algorithms

We consider three algorithms for MOT whose exponential runtimes can be isolated into a single bottleneck—
and thus can be implemented in polynomial time whenever that bottleneck can. These algorithms are the

3 Code for implementing these algorithms and reproducing all numerical simulations in this paper is provided at https:

//github.com/eboix/mot.

4

Jason M. Altschuler, Enric Boix-Adser`a

Ellipsoid algorithm ELLIPSOID [44], the Multiplicative Weights Update algorithm MWU [91], and the natural
multidimensional analog of Sinkhorn’s scaling algorithm SINKHORN [14, 70]. SINKHORN is specially tailored to
MOT and is currently the predominant algorithm for it. To foreshadow our answer to Q3, the reason that
we restrict to these candidate algorithms is: we show that ELLIPSOID and MWU can solve an MOT problem in
polynomial time if and only if any algorithm can.

1.1.2 Answer to Q2: structure necessary to run candidate algorithms

These three algorithms only access the cost tensor C through polynomially many calls of their respective bot-
tlenecks. Thus the structure required to implement these candidate algorithms in polynomial time is equivalent
to the structure required to implement their respective bottlenecks in polynomial time.

In §4, we show that the bottlenecks of these three algorithms are polynomial-time equivalent to natural

analogs of the feasibility oracle for the dual LP to MOT. Namely, given weights p1, . . . , pk ∈ Rn, compute

min
(j1,...,jk)∈{1,...,n}k

Cj1,...,jk −

k
(cid:88)

i=1

[pi]ji

(1.1)

either exactly for ELLIPSOID, approximately for MWU, or with the “min” replaced by a “softmin” for SINKHORN.
We call these three tasks the MIN, AMIN, and SMIN oracles, respectively. See Remark 3.4 for the interpretation
of these oracles as variants of the dual feasibility oracle.

These three oracles take nk time to implement in general. However, for a wide range of structured cost
tensors C they can be implemented in poly(n, k) time, see §1.2 below. For such structured costs C, our oracle
abstraction immediately implies that the MOT problem with cost C and any input marginals µ1, . . . , µk can
be (approximately) solved in polynomial time by any of the three respective algorithms.

Our characterization of the algorithms’ bottlenecks as variations of the dual feasibility oracle has two key

beneﬁts—which are the answers to Q3 and Q4, described below.

1.1.3 Answer to Q3: characterizing what MOT problems each algorithm can solve

A key beneﬁt of our characterization of the algorithms’ bottlenecks as variations of the dual feasibility oracles
is that it enables us to establish whether the structure required by a given MOT algorithm is more restrictive
than the structure required by a diﬀerent algorithm (or by any algorithm).

In particular, this enables us to answer the natural question: why restrict to just the three algorithms
described above? Can other algorithms solve MOT in poly(n, k) time in situations when these algorithms
cannot? Critically, the answer is no: restricting ourselves to the ELLIPSOID and MWU algorithms is at no loss of
generality.

Theorem 1.1 (Informal statement of part of Theorems 4.1 and 4.7). For any family of costs C ∈ (Rn)⊗k:

– ELLIPSOID computes an exact solution for MOT in poly(n, k) time if and only if any algorithm can.
– MWU computes an ε-approximate solution for MOT in poly(n, k, Cmax/ε) time if and only if any algorithm

can.

The statement for ELLIPSOID is implicit from classical results about LP [44] combined with arguments

from [7], see the previous work section §1.3. The statement for MWU is new to this paper.

The oracle abstraction helps us show Theorem 1.1 because it reduces this question of what structure is
needed for the algorithms to solve MOT in polynomial time, to the question of what structure is needed to
solve their respective bottlenecks in polynomial time. Thus Theorem 1.1 is a consequence of the following result.
(The “if” part of this result is a contribution of this paper; the “only if” part was shown in [6].)

Theorem 1.2 (Informal statement of part of Theorems 4.1 and 4.7). For any family of costs C ∈ (Rn)⊗k:

– MOT can be exactly solved in poly(n, k) time if and only if MIN can.
– MOT can be ε-approximately solved in poly(n, k, Cmax/ε) time if and only if AMIN can.

Interestingly, a further consequence of our uniﬁed algorithm-to-oracle abstraction is that it enables us to
show that SINKHORN—which is currently the most popular algorithm for MOT by far—requires strictly more
structure to solve an MOT problem than other algorithms require. This is in sharp contrast to the complete
generality of the other two algorithms shown in Theorem 1.1.

Polynomial-time algorithms for Multimarginal Optimal Transport problems with structure

5

Theorem 1.3 (Informal statement of Theorem 4.19). Under standard complexity-theoretic assumptions, there
exists a family of MOT problems that can be solved exactly in poly(n, k) time using ELLIPSOID, however it is
impossible to implement a single iteration of SINKHORN (even approximately) in poly(n, k) time.

The reason that our uniﬁed algorithm-to-oracle abstraction helps us show Theorem 1.3 is that it puts
SINKHORN on equal footing with the other two classical algorithms in terms of their reliance on variants of the
dual feasibility oracle. This reduces proving Theorem 1.3 to showing the following separation between the SMIN
oracle and the other two oracles.

Theorem 1.4 (Informal statement of Lemma 3.7). Under standard complexity-theoretic assumptions, there
exists a family of cost tensors C ∈ (Rn)⊗k such that there are poly(n, k)-time algorithms for MIN and AMIN,
however it is impossible to solve SMIN (even approximately) in poly(n, k) time.

1.1.4 Answer to Q4: ease-of-use for checking if MOT is solvable in polynomial time

The second key beneﬁt of this oracle abstraction is that it is helpful for showing that a given MOT problem
(whose cost C is input implicitly through some concise representation) is solvable in polynomial time as it
without loss of generality reduces MOT to solving any of the three corresponding oracles in polynomial time.
The upshot is that these oracles are more directly amenable to standard algorithmic techniques since they
are phrased as more conventional combinatorial-optimization problems. In the second part of the paper, we
illustrate this ease-of-use via applications to three general classes of structured MOT problems; for an overview
see §1.2.

1.1.5 Practical algorithmic tradeoﬀs beyond polynomial-time solvability

From a theoretical perspective, the most important aspect of an algorithm is whether it can solve MOT in
polynomial time if and only if any algorithm can. As we have discussed, this is true for ELLIPSOID and MWU
(Theorem 1.1) but not for SINKHORN (Theorem 1.3). Nevertheless, for a wide range of MOT cost structures, all
three oracles can be implemented in polynomial time, which means that all three algorithms ELLIPSOID, MWU,
and SINKHORN can be implemented in polynomial time. Which algorithm is best in practice depends on the
relative importance of the following considerations for the particular application.

– Error. ELLIPSOID computes exact solutions, whereas MWU and SINKHORN only compute low-precision solutions

due to poly(1/ε) runtime dependence.

– Solution sparsity. ELLIPSOID and MWU output solutions with polynomially many non-zero entries (roughly
nk), whereas SINKHORN outputs fully dense solutions with nk non-zero entries (through a polynomial-size
implicit representation, see §4.3). Solution sparsity enables interpretability, visualization, and eﬃcient down-
stream computation—beneﬁts which are helpful in diverse applications, for example ranging from computer
graphics [20, 71, 79] to facility location problems [10] to machine learning [7, 33] to ecological inference [64]
to ﬂuid dynamics (see §5.3), and more. Furthermore, in §7.4, we show that sparse solutions for MOT (a.k.a.
linear optimization over the transportation polytope) enable eﬃciently solving certain non-linear optimiza-
tion problems over the transportation polytope.

– Practical runtime. Although all three algorithms enjoy polynomial runtime guarantees, the polynomials are
smaller for some algorithms than for others. In particular, SINKHORN has remarkably good scalability in
practice as long the error ε is not too small and its bottleneck oracle SMIN is practically implementable.
By Theorems 1.1 and 1.3, MWU can solve strictly more MOT problems in polynomial time than SINKHORN;
however, it is less scalable in practice when both MWU and SINKHORN can be implemented. ELLIPSOID is not
practical and is used solely as a proof of concept that problems are tractable to solve exactly; in practice, we
use Column Generation (see, e.g., [18, §6.1]) rather than ELLIPSOID as it has better empirical performance,
yet still has the same bottleneck oracle MIN, see §4.1.3. Column Generation is not as practically scalable as
SINKHORN in n and k but has the beneﬁt of computing exact, sparse solutions.

To summarize: which algorithm is best in practice depends on the application. For example, Column Generation
produces the qualitatively best solutions for the ﬂuid dynamics application in §5.3, SINKHORN is the most scalable
for the risk estimation application in §7.3, and MWU is the most scalable for the network reliability application
in §6.3 (for that application there is no known implementation of SINKHORN that is practically eﬃcient).

6

Jason M. Altschuler, Enric Boix-Adser`a

Structure

Deﬁnition

Complexity measure

Polynomial-time algorithm?

Approximate

Exact

Graphical (§5)
Set-optimization (§6)
Low-rank + sparse (§7)

C(cid:126)j = (cid:80)

S∈S fS ((cid:126)jS )

C(cid:126)j = 1[(cid:126)j /∈ S]
C = R + S

treewidth

Known [49, 82]

Corollary 5.6

optimization oracle over S

Corollary 6.9

Corollary 6.9

rank of R, sparsity of S

Corollary 7.5

Unknown

Table 2: In the second part of the paper, we illustrate the ease-of-use of our algorithmic framework by applying it to three
general classes of MOT cost structures. These structures encompass many—if not most—current applications of MOT.

1.2 Contribution 2: applications to general classes of structured MOT problems

In the second part of the paper, we illustrate the algorithmic framework developed in the ﬁrst part of the paper
by applying it to three general classes of MOT cost structures:

1. Graphical structure (in §5).
2. Set-optimization structure (in §6).
3. Low-rank plus sparse structure (in §7).

Speciﬁcally, if the cost C is structured in any of these three ways, then MOT can be (approximately) solved in
poly(n, k) time for any input marginals µ1, . . . , µk.

Previously, it was known how to solve MOT problems with structure (1) using SINKHORN [49, 82], but this
only computes solutions that are dense (with nk non-zero entries) and low-precision (due to poly(1/ε) runtime
dependence). We therefore provide the ﬁrst solutions that are sparse and exact for structure (1). For structures
(2) and (3), we provide the ﬁrst polynomial-time algorithms, even for approximate computation. These three
structures are incomparable: it is in general not possible to model a problem falling under any of the three
structures in a non-trivial way using any of the others, for details see Remarks 6.7 and 7.3. This means that
the new structures (2) and (3) enable capturing a wide range of new applications.

Below, we detail these structures individually in §1.2.1, §1.2.2, and §1.2.3. See Table 2 for a summary.

1.2.1 Graphical structure

In §5, we apply our algorithmic framework to MOT problems with graphical structure, a broad class of MOT
problems that have been previously studied [47, 49, 82]. Brieﬂy, an MOT problem has graphical structure if its
cost tensor C decomposes as

Cj1,...,jk =

(cid:88)

fS((cid:126)jS),

S∈S

where fS((cid:126)jS) are arbitrary “local interactions” that depend only on tuples (cid:126)jS := {ji}i∈S of the k variables.

In order to derive eﬃcient algorithms, it is necessary to restrict how local the interactions are because
otherwise MOT is NP-hard (even if all interaction sets S ∈ S have size 2) [6]. We measure the locality of
the interactions via the standard complexity measure of the “treewidth” of the associated graphical model.
See §5.1 for formal deﬁnitions. While the runtimes of our algorithms (and all previous algorithms) depend
exponentially on the treewidth, we emphasize that the treewidth is a very small constant (either 1 or 2) in all
current applications of MOT falling under this framework; see the related work section.

We show that for MOT cost tensors that have graphical structure of constant treewidth, all three oracles can
be implemented in poly(n, k) time. We accomplish this by leveraging the known connection between graphically
structured MOT and graphical models shown in [49]. In particular, the MIN, AMIN, and SMIN oracles are
respectively equivalent to the mode, approximate mode, and log-partition function of an associated graphical
model. Thus we can implement all oracles in poly(n, k) time by simply applying classical algorithms from the
graphical models community [55, 88].

Theorem 1.5 (Informal statement of Theorem 5.5). Let C ∈ (Rn)⊗k have graphical structure of constant
treewidth. Then the MIN, AMIN, and SMIN oracles can be computed in poly(n, k) time.

It is an immediate corollary of Theorem 1.5 and our algorithms-to-oracles reduction described in §1.1
that one can implement ELLIPSOID, MWU, and SINKHORN in polynomial time. Below, we record the theoretical
guarantee of ELLIPSOID since it is the best of the three algorithms as it computes exact, sparse solutions.

Polynomial-time algorithms for Multimarginal Optimal Transport problems with structure

7

Theorem 1.6 (Informal statement of Corollary 5.6). Let C ∈ (Rn)⊗k have graphical structure of constant
treewidth. Then an exact, sparse solution for MOT can be computed in poly(n, k) time.

Previously, it was known how to solve such MOT problems [49, 82] using SINKHORN, but this only computes a
solution that is fully dense (with nk non-zero entries) and low-precision (due to poly(1/ε) runtime dependence).
Details in the related work section. Our result improves over this state-of-the-art algorithm by producing
solutions that are exact and sparse in poly(n, k) time.

In §5.3, we demonstrate the beneﬁt of Theorem 1.6 on the application of computing generalized Euler ﬂows,
which was historically the motivation of MOT and has received signiﬁcant attention, e.g., [14, 17, 23, 24, 25, 26].
While there is a specially-tailored version of the SINKHORN algorithm for this problem that runs in polynomial
time [14, 17], it produces solutions that are approximate and fully dense. Our algorithm produces exact, sparse
solutions which lead to sharp visualizations rather than blurry ones (see Figure 4).

1.2.2 Set-optimization structure

In §6, we apply our algorithmic framework to MOT problems whose cost tensors C take value 0 or 1 in each
entry. That, is costs C of the form

Cj1,...,jk = 1[(j1, . . . , jk) /∈ S],
for some subset S ⊆ [n]k. Such MOT problems arise naturally in applications where one seeks to minimize the
probability that some event S occurs, given marginal probabilities on each variable ji, see Example 6.1.

In order to derive eﬃcient algorithms, it is necessary to restrict the (otherwise arbitrary) set S. We
parametrize the complexity of such MOT problems via the complexity of ﬁnding the minimum-weight object
in S. This opens the door to combinatorial applications of MOT because ﬁnding the minimum-weight object in
S is well-known to be polynomial-time solvable for many “combinatorially-structured” sets S of interest—e.g.,
the set S of cuts in a graph, or the set S of independent sets in a matroid.

We show that for MOT cost tensors with this structure, all three oracles can be implemented eﬃciently.
Theorem 1.7 (Informal statement of Theorem 6.8). Let C ∈ (Rn)⊗k have set-optimization structure. Then
the MIN, AMIN, and SMIN oracles can be computed in poly(n, k) time.

It is an immediate corollary of Theorem 1.7 and our algorithms-to-oracles reduction described in §1.1
that one can implement ELLIPSOID, MWU, and SINKHORN in polynomial time. Below, we record the theoretical
guarantee for ELLIPSOID since it is the best of these three algorithms as it computes exact, sparse solutions.
Theorem 1.8 (Informal statement of Corollary 6.9). Let C ∈ (Rn)⊗k have set-optimization structure. Then
an exact, sparse solution for MOT can be computed in poly(n, k) time.

This is the ﬁrst polynomial-time algorithm for this class of MOT problems. We note that a more restrictive

class of MOT problems was studied in [93] under the additional restriction that S is upwards-closed.

In §6.3, we show how this general class of set-optimization structure captures, for example, the classical
application of computing the extremal reliability of a network with stochastic edge failures. Network reliability
is a fundamental topic in network science and engineering [12, 13, 41] which is often studied in an average-
case setting where each edge fails independently with some given probability [52, 63, 72, 85]. The application
investigated here is a robust notion of network reliability in which edge failures may be maximally correlated
(e.g., by an adversary) or minimally correlated (e.g., by a network maintainer) subject to a marginal constraint
on each edge’s failure probability, a setting that dates back to the 1980s [89, 93]. We show how to express both
the minimally and maximally correlated network reliability problems as MOT problems with set-optimization
structure, recovering as a special case of our general framework the known polynomial-time algorithms in [89, 93]
as well as more practical polynomial-time algorithms that scale to input sizes that are an order-of-magnitude
larger.

1.2.3 Low-rank and sparse structure

In §7, we apply our algorithmic framework to MOT problems whose cost tensors C decompose as

C = R + S,

where R is a constant-rank tensor, and S is a polynomially-sparse tensor. We assume that R is represented
in factored form, and that S is represented through its non-zero entries, which overall yields a poly(n, k)-size
representation of C.

8

Jason M. Altschuler, Enric Boix-Adser`a

We show that for MOT cost tensors with low-rank plus sparse structure, the AMIN and SMIN oracles can
be implemented in polynomial time.4 This may be of independent interest because, by taking all oracle inputs
pi = 0 in (1.1), this generalizes the previously open problem of approximately computing the smallest entry of
a constant-rank tensor with nk entries in poly(n, k) time.

Theorem 1.9 (Informal statement of Theorem 7.4). Let C ∈ (Rn)⊗k have low-rank plus sparse structure.
Then the AMIN and SMIN oracles can be computed in poly(n, k, Cmax/ε) time.

It is an immediate corollary of Theorem 1.9 and our algorithms-to-oracles reduction described in §1.1
that one can implement MWU and SINKHORN in polynomial time. Of these two algorithms, MWU computes sparse
solutions, yielding the following theorem.

Theorem 1.10 (Informal statement of Corollary 7.5). Let C ∈ (Rn)⊗k have low-rank plus sparse structure.
Then a sparse, ε-approximate solution for MOT can be computed in poly(n, k, Cmax/ε) time.

This is the ﬁrst polynomial-time result for this class of MOT problems. We note that the runtime of our
MOT algorithm depends exponentially on the rank r of R, hence why we take r to be constant. Nevertheless,
such a restriction on the rank is unavoidable since unless P = NP, there does not exist an algorithm with
runtime that is jointly polynomial in n, k, and the rank r [6].

We demonstrate this polynomial-time algorithm concretely on two applications. First, in §7.3 we consider
the risk estimation problem of computing an investor’s expected proﬁt in the worst-case over all future prices
that are consistent with given marginal distributions. We show that this is equivalent to an MOT problem with
a low-rank tensor and thereby provide the ﬁrst eﬃcient algorithm for it.

Second, in §7.4, we consider the fundamental problem of projecting a joint distribution Q onto the trans-
portation polytope. We provide the ﬁrst polynomial-time algorithm for solving this when Q decomposes into
a constant-rank and sparse component, which models mixtures of product distributions with polynomially
many corruptions. This application illustrates the versatility of our algorithmic results beyond polynomial-time
solvability of MOT, since this projection problem is a quadratic optimization over the transportation polytope
rather than linear optimization (a.k.a. MOT). In order to achieve this, we develop a simple quadratic-to-linear
reduction tailored to this problem that crucially exploits the sparsity of the MOT solutions enabled by the MWU
algorithm.

1.3 Related work

1.3.1 MOT algorithms

MOT algorithms fall into two categories. One category consists of general-purpose algorithms that do not
depend on the speciﬁc MOT cost. For example, this includes running an LP solver out-of-the-box, or running
the Sinkhorn algorithm where in each iteration one sums over all nk entries of the cost tensor to implement
the marginalization bottleneck [40, 58, 84]. These approaches are robust in the sense that they do not need to
be changed based on the speciﬁc MOT problem. However, they are impractical beyond tiny input sizes (e.g.,
n = k = 10) because their runtimes scale as nΩ(k).

The second category consists of algorithms that are much more scalable but require extra structure of the
MOT problem. Speciﬁcally, these are algorithms that somehow exploit the structure of the relevant cost tensor
C in order to (approximately) solve an MOT problem in poly(n, k) time [1, 7, 14, 15, 17, 29, 30, 38, 46, 47,
49, 50, 61, 62, 65, 66, 67, 68, 77, 82, 89, 93]. Such a poly(n, k) runtime is far more tractable—but it is not well
understood for which MOT problems such a runtime is possible. The purpose of this paper is to clarify this
question.

To contextualize our answer to this question with the rapidly growing literature requires further splitting

this second category of algorithms.

Sinkhorn algorithm. Currently, the predominant approach in the second category is to solve an entropically
regularized version of MOT with the Sinkhorn algorithm, a.k.a. Iterative Proportional Fitting or Iterative
Bregman Projections or RAS algorithm or Iterative Scaling algorithm, see e.g., [15, 16, 17, 47, 49, 67, 82].

4 It is an interesting open question if the MIN oracle can similarly be implemented in poly(n, k) time. This would enable
implementing ELLIPSOID in poly(n, k) time by our algorithms-to-oracles reduction, and thus would enable computing exact
solutions for this class of MOT problems (cf., Theorem 1.10).

Polynomial-time algorithms for Multimarginal Optimal Transport problems with structure

9

Recent work has shown that a polynomial number of iterations of this algorithm suﬃces [40, 58, 84]. However,
the bottleneck is that each iteration requires nk operations in general because it requires marginalizing a tensor
with nk entries. The critical question is therefore: what structure of an MOT problem enables implementing
this marginalization bottleneck in polynomial time.

This paper makes two contributions to this question. First, we identify new broad classes of MOT problems
for which this bottleneck can be implemented in polynomial time, and thus SINKHORN can be implemented
in polynomial time (see §1.2). Second, we propose other algorithms that require strictly less structure than
SINKHORN does in order to solve an MOT problem in polynomial time (Theorem 4.19).

Ellipsoid algorithm. The Ellipsoid algorithm is among the most classical algorithms for implicit LP [43, 44, 53],
however it has taken a back seat to the SINKHORN algorithm in the vast majority of the MOT literature.

In §4.1, we make explicit the fact that the variant of ELLIPSOID from [7] can solve MOT exactly in poly(n, k)
time if and only if any algorithm can (Theorem 4.1). This is implicit from combining several known results [6, 7,
44]. In the process of making this result explicit, we exploit the special structure of the MOT LP to signiﬁcantly
simplify the reduction from the dual violation oracle to the dual feasibility oracle. The previously known
reduction is highly impractical as it requires an indirect “back-and-forth” use of the Ellipsoid algorithm [44, page
107]. In contrast, our reduction is direct and simple; this is critical for implementing our practical alternative
to ELLIPSOID, namely COLGEN, with the dual feasibility oracle.

Multiplicative Weights Update algorithm. This algorithm, ﬁrst introduced by [91], has been studied in the
context of optimal transport when k = 2 [19, 73], in which case implicit LP is not necessary for a polynomial
runtime. MWU lends itself to implicit LP [91], but is notably absent from the MOT literature.

In §4.2, we show that MWU can be applied to MOT in polynomial time if and only if the approximate dual
feasibility oracle can be solved in polynomial time. To do this, we show that in the special case of MOT,
the well-known “softmax-derivative” bottleneck of MWU is polynomial-time equivalent to the approximate dual
feasibility oracle. Since it is known that the approximate dual feasibility oracle is polynomial-time reducible to
approximate MOT [6], we therefore establish that MWU can solve MOT approximately in polynomial time if and
only if any algorithm can (Theorem 4.7).

1.3.2 Graphically structured MOT problems with constant treewidth

We isolate here graphically structured costs with constant treewidth because this framework encompasses all
MOT problems that were previously known to be tractable in polynomial time [49, 82], with the exceptions of the
ﬁxed-dimensional Wasserstein barycenter problem and MOT problems related to combinatorial optimization—
both of which are described below in §1.3.3. This family of graphical structured costs with treewidth 1 (a.k.a.
“tree-structured costs” [47]) includes applications in economics such as variational mean-ﬁeld games [15], in-
terpolating histograms on trees [3], matching for teams [29, 67]; as well as encompasses applications in ﬁltering
and estimation for collective dynamics such as target tracking [38, 46, 47, 49, 77] and Wasserstein barycen-
ters in the case of ﬁxed support [14, 29, 38, 67]. With treewidth 2, this family of costs also includes dynamic
multi-commodity ﬂow problems [48], as well as the application of computing generalized Euler ﬂows in ﬂuid
dynamics [14, 17, 67], which was historically the original motivation of MOT [23, 24, 25, 26].

Previous polynomial-time algorithms for graphically structured MOT compute approximate, dense solutions.
Implementing SINKHORN for graphically structured MOT problems by using belief propagation to eﬃciently
implement the marginalization bottleneck was ﬁrst proposed twenty years ago in [82]. There have been recent
advancements in understanding connections of this algorithm to the Schr¨odinger bridge problem in the case of
trees [47], as well as developing more practically eﬃcient single-loop variations [49].

All of these works prove theoretical runtime guarantees only in the case of tree structure (i.e., treewidth
1). However, this graphical model perspective for eﬃciently implementing SINKHORN readily extends to any
constant treewidth: simply implement the marginalization bottleneck using junction trees. This, combined
with the iteration complexity of SINKHORN which is known to be polynomial [40, 58, 84], immediately yields an
overall polynomial runtime. This is why we cite [49, 82] throughout this paper regarding the fact that SINKHORN
can be implemented in polynomial time for graphical structure with any constant treewidth.

While the use of SINKHORN for graphically structured MOT is mathematically elegant and can be impressively
scalable in practice, it has two drawbacks. The ﬁrst drawback of this algorithm is that it computes (implicit
representations of) solutions that are fully dense with nk non-zero entries. Indeed, it is well-known that SINKHORN

10

Jason M. Altschuler, Enric Boix-Adser`a

ﬁnds the unique optimal solution to the entropically regularized MOT problem minP ∈M(µ1,...,µk)(cid:104)P, C(cid:105) −
η−1H(P ), and that this solution is fully dense [70]. For example, in the simple case of cost C = 0, uniform
marginals µi, and any strictly positive regularization parameter η > 0, this solution P has value 1/nk in each
entry.

The second drawback of this algorithm is that it only computes solutions that are low-precision due to
poly(1/ε) runtime dependence on the accuracy ε. This is because the number of SINKHORN iterations is known
to scale polynomially in the entropic regularization parameter η even in the matrix case k = 2 [59, §1.2], and
it is known that η = Ω(ε−1k log n) is necessary for the converged solution of SINKHORN to be an ε-approximate
solution to the (unregularized) original MOT problem [58].

Improved algorithms for graphically structured MOT problems. The contribution of this paper to the study of
graphically structured MOT problems is that we give the ﬁrst poly(n, k) time algorithms that can compute
solutions which are exact and sparse (Corollary 5.6). Our framework also directly recovers all known results
about SINKHORN for graphically structured MOT problems—namely that it can be implemented in polynomial
time for trees [47, 82] and for constant treewidth [49, 82].

1.3.3 Tractable MOT problems beyond graphically structured costs

The two new classes of MOT problems studied in this paper—namely, set-optimization structure and low-
rank plus sparse structure—are incomparable to each other as well as to graphical structure. Details in Re-
marks 6.7 and 7.3. This lets us handle a wide range of new MOT problems that could not be handled before.
There are two other classes of MOT problems studied in the literature which do not fall under the three

structures studied in this paper. We elaborate on both below.
Remark 1.11 (Low-dimensional Wasserstein barycenter). This MOT problem has cost Cj1,...,jk = (cid:80)k
xi(cid:48),ji(cid:48) (cid:107)2 where xi,j ∈ Rd denotes the j-th atom in the distribution µi. Clearly this cost is not a graphically
structured cost of constant treewidth—indeed, representing it through the lens of graphical structure requires
the complete graph of interactions, which means a maximal treewidth of k − 1.5 This problem also does not
fall under the set-optimization or constant-rank structures. Nevertheless, this MOT problem can be solved in
poly(n, k) time for any ﬁxed dimension d by exploiting the low-dimensional geometric structure of the points
{xi,j} that implicitly deﬁne the cost [7].

i,i(cid:48)=1 (cid:107)xi,ji −

Remark 1.12 (Random combinatorial optimization). MOT problems also appear in the random combinato-
rial optimization literature since the 1970s, see e.g., [50, 61, 65, 89, 93], although under a diﬀerent name and
in a diﬀerent community. These papers consider MOT problems with costs of the form C(x) = minv∈V (cid:104)x, v(cid:105)
for polytopes V ⊆ {0, 1}k given through a list of their extreme points. Applications include PERT (Program
Evaluation and Review Technique), extremal network reliability, and scheduling. Recently, applications to Dis-
tributionally Robust Optimization were investigated in [30, 62, 66] which considered general polytopes V ⊂ Rk,
as well as in [68] which considered MOT costs of the related form C(x) = 1[minv∈V (cid:104)x, v(cid:105) (cid:62) t], and in [1]
which considers other combinatorial costs C such as sub/supermodular functions. These papers show that these
random combinatorial optimization problems are in general intractable, and give suﬃcient conditions on when
they can be solved in polynomial time. In general, these families of MOT problems are diﬀerent from the three
structures studied in this paper, although some MOT applications fall under multiple umbrellas (e.g., extremal
network reliability). It is an interesting question to understand to what extent these structures can be reconciled
(as well as the algorithms, which sometimes use extended formulations in these papers).

1.3.4 Intractable MOT problems

These algorithmic results beg the question: what are the fundamental limitations of this line of work on
polynomial-time algorithms for structured MOT problems? To this end, the recent paper [6] provides a system-
atic investigation of NP-hardness results for structured MOT problems, including converses to several results in

5 We remark that the related but diﬀerent problem of ﬁxed-support Wasserstein barycenters has graphical structure with
treewidth 1 [14, 29, 38, 67]. However, it should be emphasized that the ﬁxed-support Wasserstein barycenter problem is diﬀerent
from the Wasserstein barycenter problem: it only approximates the latter to ε accuracy if the ﬁxed support is restricted to an
O(ε)-net which requires n = 1/εΩ(d) discretization size for the barycenter’s support, and thus (i) even in constant dimension,
does not lead to high-precision algorithms due to poly(1/ε) runtime; and (ii) scales exponentially in the dimension d. See [8,
§1.3] for further details about the complexity of Wasserstein barycenters.

Polynomial-time algorithms for Multimarginal Optimal Transport problems with structure

11

this paper. In particular, [6, Propositions 4.1 and 4.2] justify the constant-rank regime studied in §7 by showing
that unless P = NP, there does not exist an algorithm with runtime that is jointly polynomially in the rank r
and the input parameters n and k. Similarly, [6, Propositions 5.1 and 5.2] justify the constant-treewidth regime
for graphically structured costs studied in §5 and all previous work by showing that unless P = NP, there
does not exist an algorithm with polynomial runtime even in the seemingly simple class of MOT costs that
decompose into pairwise interactions Cj1,...,jk = (cid:80)
i). The paper [6] also shows NP-hardness
for several MOT problems with repulsive costs, including for example the MOT formulation of Density Func-
tional Theory with Coulomb-Buckingham potential. It is an problem whether the Coulomb potential, studied
in [16, 27, 32], also leads to an NP-hard MOT problem [6, Conjecture 6.4].

i(cid:54)=i(cid:48)∈[k] ci,i(cid:48) (ji, j(cid:48)

1.3.5 Variants of MOT

The literature has studied several other variants of the MOT problem, notably with entropic regularization
and/or with constraints on a subset of the k marginals, see, e.g., [14, 15, 16, 17, 38, 46, 47, 48, 49, 58, 77].
Our techniques readily apply with little change. Brieﬂy, to handle entropic regularization, simply use the SMIN
oracle and SINKHORN algorithm with ﬁxed regularization parameter 1/η > 0 (rather than 1/η of vanishing size
Θ(ε/ log n)) as described in §4.3. And to handle partial marginal constraints, essentially the only change is
that in the MIN, AMIN, and SMIN oracles, the potentials pi are zero for all indices i ∈ [k] corresponding to
unconstrained marginals mi(P ). Full details are omitted for brevity since they are straightforward modiﬁcations
of our main results.

1.3.6 Optimization over joint distributions

Optimization problems over exponential-size joint distributions appear in many domains. For instance, they
arise in game theory when computing correlated equilibria [69]; however, in that case the optimization has
diﬀerent constraints which lead to diﬀerent algorithms. Such problems also arise in variational inference [87];
however, the optimization there typically constrains this distribution to ensure tractability (e.g., mean-ﬁeld
approximation restricts to product distributions). The diﬀerent constraints in these optimization problems
over joint distributions versus MOT lead to signiﬁcant diﬀerences in computational complexity, and thus also
necessitate diﬀerent algorithmic techniques.

1.4 Organization

In §2 we recall preliminaries about MOT and establish notation. The ﬁrst part of the paper then establishes
our uniﬁed algorithmic framework for MOT. Speciﬁcally, in §3 we deﬁne and compare three variants of the
dual feasibility oracle; and in §4 we characterize the structure that MOT algorithms require for polynomial-
time implementation in terms of these three oracles. For an overview of these results, see §1.1. The second
part of the paper applies this algorithmic framework to three general classes of MOT cost structures: graphical
structure (§5), set-optimization structure (§6), and low-rank plus sparse structure (§7). For an overview of these
results, see §1.2. These three application sections are independent of each other and can be read separately. We
conclude in §8.

2 Preliminaries

General notation. The set {1, . . . , n} is denoted by [n]. For shorthand, we write poly(t1, . . . , tm) to denote a
function that grows at most polynomially fast in those parameters. Throughout, we assume for simplicity of
exposition that all entries of the input C and µ1, . . . , µk have bit complexity at most poly(n, k), and same
with the components deﬁning C in structured settings. As such, throughout runtimes refer to the number of
arithmetic operations. The set R ∪ {−∞} is denoted by ¯R, and note that the value −∞ can be represented
eﬃciently by adding a single ﬂag bit. We use the standard O(·) and Ω(·) notation, and use ˜O(·) and ˜Ω(·) to
denote that polylogarithmic factors may be omitted.

12

Jason M. Altschuler, Enric Boix-Adser`a

j1,...,ji−1,ji+1,...,jk

Tensor notation. The k-fold tensor product space Rn ⊗ · · · ⊗ Rn is denoted by (Rn)⊗k, and similarly for
(cid:62)0)⊗k. Let P ∈ (Rn)⊗k. Its i-th marginal, i ∈ [k], is denoted by mi(P ) ∈ Rn and has entries [mi(P )]j :=
(Rn
(cid:80)
Pj1,...,ji−1,j,ji+1,...,jk . For shorthand, we often denote an index (j1, . . . , jk) by (cid:126)j. The sum of
P ’s entries is denoted by m(P ) = (cid:80)
(cid:126)j P(cid:126)j. The maximum absolute value of P ’s entries is denoted by (cid:107)P (cid:107)max :=
max(cid:126)j |P(cid:126)j|, or simply Pmax for short. For (cid:126)j ∈ [n]k, we write δ(cid:126)j to denote the tensor with value 1 at entry (cid:126)j, and
0 elswewhere. The operations (cid:12) and ⊗ respectively denote the entrywise product and the Kronecker product.
The notation ⊗k
i=1di is shorthand for d1 ⊗ · · · ⊗ dk. A non-standard notation we use throughout is that f [P ]
denotes a function f : R → R (typically exp, log, or a polynomial) applied entrywise to a tensor P .

2.1 Multimarginal Optimal Transport

The transportation polytope between measures µ1, . . . , µk ∈ ∆n is

M(µ1, . . . , µk) :=

(cid:110)

P ∈ (Rn

(cid:111)
(cid:62)0)⊗k : mi(P ) = µi, ∀i ∈ [k]

.

(2.1)

For a ﬁxed cost C ∈ (Rn)⊗k, the MOTC problem is to solve the following linear program, given input measures
µ = (µ1, . . . , µk) ∈ (∆n)k:

min
P ∈M(µ1,...,µk)

(cid:104)P, C(cid:105).

(MOT)

In the k = 2 matrix case, (MOT) is the Kantorovich formulation of OT [86]. Its dual LP is

max
p1,...,pk∈Rn

k
(cid:88)

i=1

(cid:104)pi, µi(cid:105)

subject to Cj1,...,jk −

k
(cid:88)

i=1

[pi]ji

(cid:62) 0, ∀(j1, . . . , jk) ∈ [n]k.

(MOT-D)

A basic, folklore fact about MOT is that it always has a sparse optimal solution (e.g., [10, Lemma 3]). This

follows from elementary facts about standard-form LP; we provide a short proof for completeness.
Lemma 2.1 (Sparse solutions for MOT). For any cost C ∈ (Rn)⊗k and any marginals µ1, . . . , µk ∈ ∆n, there
exists an optimal solution P to MOTC (µ) that has at most nk − k + 1 non-zero entries.

Proof. Since (MOT) is an LP over a compact domain, it has an optimal solution at a vertex [18, Theorem
2.7]. Since (MOT) is a standard-form LP, these vertices are in correspondence with basic solutions, thus their
sparsity is bounded by the number of linearly dependent constraints deﬁning M(µ1, . . . , µk) [18, Theorem 2.4].
We bound this quantity by nk − k + 1 via two observations. First, M(µ1, . . . , µk) is deﬁned by nk equality
constraints [mi(P )]j = [µi]j in (2.1), one for each coordinate j ∈ [n] of each marginal constraint i ∈ [k].
Second, at least k − 1 of these constraints are linearly dependent because we can construct k distinct linear
combinations of them, namely (cid:80)
j∈[n][µi]j for each marginal i ∈ [k], which all simplify to
the same constraint m(P ) = 1, and thus are redundant with each other.

j∈[n][mi(P )]j = (cid:80)

Deﬁnition 2.2 (ε-approximate MOT solution). P is an ε-approximate solution to MOTC (µ) if P is feasible
(i.e., P ∈ M(µ1, . . . , µk)) and (cid:104)C, P (cid:105) is at most ε more than the optimal value.

2.2 Regularization

We introduce two standard regularization operators. First is the Shannon entropy H(P ) := − (cid:80)
a tensor P ∈ (Rn
0 log 0 = 0. Second is the softmin operator, which is deﬁned for parameter η > 0 as

(cid:126)j P(cid:126)j log P(cid:126)j of
(cid:62)0)⊗k with entries summing to m(P ) = 1. We adopt the standard notational convention that

sminη
i∈[m]

ai := −

1
η

log

(cid:32) m
(cid:88)

i=1

(cid:33)

e−ηai

.

(2.2)

This softmin operator naturally extends to ai ∈ R ∪ {∞} by adopting the standard notational conventions that
e−∞ = 0 and log 0 = −∞.

We make use of the following folklore fact, which bounds the error between the min and smin operators

based on the regularization and the number of points. For completeness, we provide a short proof.

Polynomial-time algorithms for Multimarginal Optimal Transport problems with structure

13

Lemma 2.3 (Softmin approximation bound). For any a1, . . . , am ∈ R ∪ {∞} and η > 0,

min
i∈[m]

ai (cid:62) sminη
i∈[m]

ai (cid:62) min
i∈[m]

ai −

log m
η

.

Proof. Assume without loss of generality that all ai are ﬁnite, else ai can be dropped (if all ai = ∞ then the
claim is trivial). For shorthand, denote mini∈[m] ai by amin. For the ﬁrst inequality, use the non-negativity of
the exponential function to bound

sminη
i∈[m]

ai = −

1
η

log

(cid:32) m
(cid:88)

i=1

(cid:33)

e−ηai

(cid:54) −

1
η

(cid:16)

e−ηamin (cid:17)

log

= amin.

For the second inequality, use the fact that each ai (cid:62) amin to bound

sminη
i∈[m]

ai = −

1
η

log

(cid:32) m
(cid:88)

i=1

(cid:33)

e−ηai

(cid:62) −

1
η

me−ηamin (cid:17)
(cid:16)

log

= amin −

log m
η

.

The entropically regularized MOT problem (RMOT for short) is the convex optimization problem

min
P ∈M(µ1,...,µk)

(cid:104)P, C(cid:105) − η−1H(P ).

(RMOT)

This is the natural multidimensional analog of entropically regularized OT, which has a rich literature in
statistics [57] and transportation theory [90], and has recently attracted signiﬁcant interest in machine learn-
ing [35, 70]. The convex dual of (RMOT) is the convex optimization problem

max
p1,...,pk∈Rn

k
(cid:88)

i=1

(cid:32)

(cid:104)pi, µi(cid:105) + sminη
(cid:126)j∈[n]k

C(cid:126)j −

(cid:33)

[pi]ji

.

k
(cid:88)

i=1

(RMOT-D)

In contrast to MOT, there is no analog of Lemma 2.1 for RMOT: the unique optimal solution to RMOT is
dense. Further, this solution may not even be “approximately” sparse. For example, when C = 0, all µi are
uniform, and η > 0 is any positive number, the solution is fully dense with all entries equal to 1/nk.

We deﬁne P to be an ε-approximate RMOT solution in the analogous way as in Deﬁnition 2.2. A basic,
folklore fact about RMOT is that if the regularization η is suﬃciently large, then RMOT and MOT are equivalent
in terms of approximate solutions.

Lemma 2.4 (MOT and RMOT are close for large regularization η). Let P ∈ M(µ1, . . . , µk), ε > 0, and
η (cid:62) ε−1k log n. If P is an ε-approximate solution to (RMOT), then P is also a (2ε)-approximate solution to
(MOT); and vice versa.

Proof. Since a discrete distribution supported on nk atoms has entropy at most k log n [34], the objectives of
(MOT) and (RMOT) diﬀer pointwise by at most η−1k log n (cid:54) ε. Since (MOT) and (RMOT) also have the
same feasible sets, their optimal values therefore diﬀer by at most ε.

3 Oracles

Here we deﬁne the three oracle variants described in the introduction and discuss their relations. In the below
deﬁnitions, let C ∈ (Rn)⊗k be a cost tensor.

Deﬁnition 3.1 (MIN oracle). For weights p = (p1, . . . , pk) ∈ Rn×k, MINC (p) returns

min
(cid:126)j∈[n]k

C(cid:126)j −

k
(cid:88)

i=1

[pi]ji .

Deﬁnition 3.2 (AMIN oracle). For weights p = (p1, . . . , pk) ∈ Rn×k and accuracy ε > 0, AMINC (p, ε) returns
MINC (p) up to additive error ε.

14

Jason M. Altschuler, Enric Boix-Adser`a

Deﬁnition 3.3 (SMIN oracle). For weights p = (p1, . . . , pk) ∈ ¯Rn×k and regularization parameter η > 0,
SMINC (p, η) returns

sminη
(cid:126)j∈[n]k

C(cid:126)j −

k
(cid:88)

i=1

[pi]ji .

An algorithm is said to “solve” or “implement” MINC if given input p, it outputs MINC (p). Similarly for
AMINC and SMINC . Note that the weights p that are input to SMIN have values inside ¯R = R ∪ {−∞};
this simpliﬁes the notation in the treatment of the SINKHORN algorithm below and does not increase the bit-
complexity by more than 1 bit by adding a ﬂag for the value −∞.

Remark 3.4 (Interpretation as variants of the dual feasibility oracle). These three oracles can be viewed as
variants of the feasibility oracle for (MOT-D). For MINC (p), this relationship is exact: p ∈ Rn×k is feasible
for (MOT-D) if and only if MINC (p) is non-negative. For AMINC and SMINC , this relationship is approximate,
with the approximation depending on how small ε is and how large η is, respectively.

Since these oracles form the respective bottlenecks of all algorithms from the MOT and implicit linear
programming literatures (see the overview in the introduction §1.1), an important question is: if one oracle can
be implemented in poly(n, k) time, does this imply that the other can be too?

Two reductions are straightforward: the AMIN oracle can be implemented in poly(n, k) time whenever either
the MIN oracle or the SMIN oracle can be implemented in poly(n, k) time. We record these simple observations
in remarks for easy recall.

Remark 3.5 (MIN implies AMIN). For any accuracy ε > 0, the MINC (p) oracle provides a valid answer to the
AMINC (p, ε) oracle by deﬁnition.
Remark 3.6 (SMIN implies AMIN). For any p ∈ Rn×k and regularization η (cid:62) ε−1k log n, the SMINC (p, η)
oracle provides a valid answer to the AMINC (p, ε) oracle due to the approximation property of the smin operator
(Lemma 2.3).

In the remainder of this section, we show a separation between the SMIN oracle and both the MIN and
AMIN oracles by exhibiting a family of cost tensors C for which there exist polynomial-time algorithms for MIN
and AMIN, however there is no polynomial-time algorithm for SMIN. The non-existence of a polynomial-time
algorithm of course requires a complexity theoretic assumption; our result holds under #BIS-hardness—which
is a by-now standard complexity theory assumption introduced in [37], and in words is the statement that there
does not exist a polynomial-time algorithm for counting the number of independent sets in a bipartite graph.

Lemma 3.7 (Restrictiveness of the SMIN oracle). There exists a family of costs C ∈ (Rn)⊗k for which MINC
and AMINC can be solved in poly(n, k) time, however SMINC is #BIS-hard.

Proof. In order to prove hardness for general n, it suﬃces to exhibit such a family of cost tensors when n = 2.
Since n = 2, it is convenient to abuse notation slightly by indexing a cost tensor C ∈ (Rn)⊗k by (cid:126)j ∈ {−1, 1}k
rather than by (cid:126)j ∈ {1, 2}k. The family we exhibit is {C(A, b) : A ∈ Rk×k
(cid:62)0 , b ∈ Rk}, where the cost tensors
C(A, b) are parameterized by a non-negative square matrix A and a vector b, and have entries of the form

C(cid:126)j(A, b) := −(cid:104)(cid:126)j, A(cid:126)j(cid:105) − (cid:104)b,(cid:126)j(cid:105),

(cid:126)j ∈ {±1}k.

Polynomial-time algorithm for MIN and AMIN. We show that given a matrix A ∈ Rk×k

(cid:62)0 , vector b ∈ Rk, and
weights p ∈ R2×k, it is possible to compute MINC (p) on the cost tensor C(A, b) in poly(k) time. Clearly this
also implies a poly(k) time algorithm for AMINC (p, ε) for any ε > 0, see Remark 3.5.

To this end, we ﬁrst re-write the MINC (p) problem on C(A, b) in a more convenient form that enables us

to “ignore” the weights p. Recall that MINC (p) is the problem of

MINC (p) = min

(cid:126)j∈{±1}k

−(cid:104)(cid:126)j, A(cid:126)j(cid:105) − (cid:104)b,(cid:126)j(cid:105) −

k
(cid:88)

i=1

[pi]ji .

Note that the linear part of the cost is equal to

(cid:104)b,(cid:126)j(cid:105) +

k
(cid:88)

i=1

[pi]ji = (cid:104)(cid:96),(cid:126)j(cid:105) + d,

(3.1)

Polynomial-time algorithms for Multimarginal Optimal Transport problems with structure

15

where (cid:96) ∈ Rk is the vector with entries (cid:96)i = bi + 1
Thus, since d is clearly computable in O(k) time, the MINC problem is equivalent to solving

2 ((pi)1 −(pi)−1), and d is the scalar d = 1

2

(cid:80)k

i=1([pi]1 +[pi]−1).

min
(cid:126)j∈{±1}k

−(cid:104)(cid:126)j, A(cid:126)j(cid:105) − (cid:104)(cid:96),(cid:126)j(cid:105),

(3.2)

when given as input a non-negative matrix A ∈ Rk×k

(cid:62)0 and a vector (cid:96) ∈ Rk.

To show that this task is solvable in poly(k) time, note that the objective in (3.2) is a submodular func-
tion because it is a quadratic whose Hessian −A has non-positive oﬀ-diagonal terms [11, Proposition 6.3].
Therefore (3.2) is a submodular optimization problem, and thus can be solved in poly(k) time using classical
algorithms from combinatorial optimization [44, Chapter 10.2].

SMIN oracle is #BIS-hard. On the other hand, by using the deﬁnition of the SMIN oracle, the re-parameterization (3.1),

and then the deﬁnition of the softmin operator,

SMINC (p, η) = sminη
(cid:126)j∈{±1}k

−(cid:104)(cid:126)j, A(cid:126)j(cid:105) − (cid:104)b,(cid:126)j(cid:105) −

k
(cid:88)

i=1

[pi]ji = sminη
(cid:126)j∈{±1}k

−(cid:104)(cid:126)j, A(cid:126)j(cid:105) − (cid:104)(cid:96),(cid:126)j(cid:105) − d = −

log Z
η

− d,

where Z = (cid:80)
ﬁelds given by

(cid:126)j∈{±1}k Q((cid:126)j) is the partition function of the ferromagnetic Ising model with inconsistent external

Q((cid:126)j) = exp

(cid:16)

(cid:17)
η(cid:104)(cid:126)j, A(cid:126)j(cid:105) + η(cid:104)(cid:96),(cid:126)j(cid:105)

.

Because it is #BIS hard to compute the partition function Z of a ferromagnetic Ising model with inconsistent
external ﬁelds [42], it is #BIS hard to compute the value −η−1 log Z − d of the oracle SMINC (p, η).

Remark 3.8 (The restrictiveness of SMIN extends to approximate computation). The separation between
the oracles shown in Lemma 3.7 further extends to approximate computation of the SMIN oracle under the
assumption that #BIS is hard to approximate, since under this assumption it is hard to approximate the partition
function of a ferromagnetic Ising model with inconsistent external ﬁelds [42].

4 Algorithms to oracles

In this section, we consider three algorithms for MOT. Each is iterative and requires only polynomially many
iterations. The key issue for each algorithm is the per-iteration runtime, which is in general exponential (roughly
nk). We isolate the respective bottlenecks of these three algorithms into the three variants of the dual feasibility
oracle deﬁned in §3. See §1.1 and Table 1 for a high-level overview of this section’s results.

4.1 The Ellipsoid algorithm and the MIN oracle

Among the most classical algorithms for implicit LP is the Ellipsoid algorithm [43, 44, 53]. However it has taken
a back seat to the SINKHORN algorithm in the vast majority of the MOT literature. The very recent paper [7],
which focuses on the speciﬁc MOT application of computing low-dimensional Wasserstein barycenters, develops
a variant of the classical Ellipsoid algorithm specialized to MOT; henceforth this is called ELLIPSOID, see §4.1.1
for a description of this algorithm. The objective of this section is to analyze ELLIPSOID in the context of
general MOT problems in order to prove the following.

Theorem 4.1. For any family of cost tensors C ∈ (Rn)⊗k, the following are equivalent:

(i) ELLIPSOID takes poly(n, k) time to solve the MOTC problem. (Moreover, it outputs a vertex solution repre-

sented as a sparse tensor with at most nk − k + 1 non-zeros.)

(ii) There exists a poly(n, k) time algorithm that solves the MOTC problem.
(iii) There exists a poly(n, k) time algorithm that solves the MINC problem.

16

Jason M. Altschuler, Enric Boix-Adser`a

Interpretation of results. In words, the equivalence “(i) ⇐⇒ (ii)” establishes that ELLIPSOID can solve any MOT
problem in polynomial time that any other algorithm can. Thus from a theoretical perspective, this paper’s
restriction to ELLIPSOID is at no loss of generality for developing polynomial-time algorithms that exactly solve
MOT. In words, the equivalence “(ii) ⇐⇒ (iii)” establishes that the MOT and MIN problems are polynomial-
time equivalent. Thus we may investigate when MOT is tractable by instead investigating the more amenable
question of when MIN is tractable (see §1.1.4) at no loss of generality.

As stated in the related work section, Theorem 4.1 is implicit from combining several known results [6, 7, 44].
Our contribution here is to make this result explicit, since this allows us to unify algorithms from the implicit
LP literature with the SINKHORN algorithm. We also signiﬁcantly simplify part of the implication “(iii) =⇒
(i)”, which is crucial for making an algorithm that relies on the MIN oracle practical—namely, the Column
Generation algorithm discussed below.

Organization of §4.1. In §4.1.1, we recall this ELLIPSOID algorithm and how it depends on the violation oracle
for (MOT-D). In §4.1.2, we give a signiﬁcantly simpler proof that the violation and feasibility oracles are
polynomial-time equivalent in the case of (MOT-D), and use this to prove Theorem 4.1. In §4.1.3, we describe
a practical implementation that replaces the ELLIPSOID outer loop with Column Generation.

4.1.1 Algorithm

A key component of the proof of Theorem 4.1 is the ELLIPSOID algorithm introduced in [7] for MOT, which we
describe below. In order to present this, we ﬁrst deﬁne a variant of the MIN oracle that returns a minimizing
tuple rather than the minimizing value.
Deﬁnition 4.2 (Violation oracle for (MOT-D)). Given weights p = (p1, . . . , pk) ∈ Rn×k, ARGMINC returns
the minimizing solution (cid:126)j and value of min(cid:126)j∈[n]k C(cid:126)j − (cid:80)k

i=1[pi]ji .

ARGMINC can be viewed as a violation oracle6 for the decision set to (MOT-D). This is because, given
p = (p1, . . . , pk) ∈ Rn×k, the tuple (cid:126)j output by ARGMINC (p) either provides a violated constraint if C(cid:126)j −
(cid:80)k
i=1[pi]ji < 0, or otherwise certiﬁes p is feasible. In [7] it is proved that MOT can be solved with polynomially

many calls to the ARGMINC oracle.

Theorem 4.3 (ELLIPSOID guarantee; Proposition 12 of [7]). Algorithm 1 ﬁnds an optimal vertex solution
for MOTC (µ) using poly(n, k) calls to the ARGMINC oracle and poly(n, k) additional time. The solution is
returned as a sparse tensor with at most nk − k + 1 non-zero entries.

Algorithm 1 ELLIPSOID: specialization of the classical Ellipsoid algorithm to MOT

Input: Cost C ∈ (Rn)⊗k, marginals µ1, . . . , µk ∈ ∆n
Output: Vertex solution to MOTC (µ)
\\ Solve dual

1: Solve (MOT-D) using the Ellipsoid algorithm, with ARGMINC as the violation oracle. Let S denote the set of tuples returned

by all calls to ARGMINC .

\\ Solve primal

2: Solve (4.1) using the Ellipsoid algorithm.

Sketch of algorithm. Full details and a proof are in [7]. We give a brief overview here for completeness. First,
recall from the implicit LP literature that the classical Ellipsoid algorithm can be implemented in polynomial
time for an LP with arbitrarily many constraints so long as it has polynomially many variables and the violation
oracle for its decision set is solvable in polynomial time [44]. This does not directly apply to the LP (MOT)
because that LP has nk variables. However, it can apply to the dual LP (MOT-D) because that LP only has
nk variables.

This suggests a natural two-step algorithm for MOT. First, compute an optimal dual solution by directly
applying the Ellipsoid algorithm to (MOT-D). Second, use this dual solution to construct a sparse primal

6 Recall that a violation oracle for a polytope K = {x : (cid:104)ai, x(cid:105) (cid:54) bi, ∀i ∈ [N ]} is an algorithm that given a point p, either

asserts p is in K, or otherwise outputs the index i of a violated constraint (cid:104)ai, p(cid:105) > bi.

Polynomial-time algorithms for Multimarginal Optimal Transport problems with structure

17

solution. Although this dual-to-primal conversion does not extend to arbitrary LP [18, Exercise 4.17], the
paper [7] provides a solution by exploiting the standard-form structure of MOT. The procedure is to solve

min
P ∈M(µ1,...,µk)
s.t. P(cid:126)j =0, ∀(cid:126)j /∈S

(cid:104)C, P (cid:105)

(4.1)

which is the MOT problem restricted to sparsity pattern S, where S is the set of tuples (cid:126)j returned by the violation
oracle during the execution of step one of Algorithm 1. This second step takes poly(n, k) time using a standard
LP solver, because running the Ellipsoid algorithm in the ﬁrst step only calls the violation oracle poly(n, k)
times, and thus S has poly(n, k) size, and therefore the LP (4.1) has poly(n, k) variables and constraints. In
[7] it is proved that this produces a primal vertex solution to the original MOT problem.

4.1.2 Equivalence of bottleneck to MIN

Although Theorem 4.3 shows that ELLIPSOID can solve MOT in poly(n, k) time using the ARGMIN oracle, this
is not suﬃcient to prove the implication “(iii) =⇒ (i)” in Theorem 4.1. In order to prove that implication
requires showing the polynomial-time equivalence between MIN and ARGMIN.

Lemma 4.4 (Equivalence of MIN and ARGMIN). Each of the oracles MINC and ARGMINC can be implemented
using poly(n, k) calls of the other oracle and poly(n, k) additional time.

This equivalence follows from classical results about the equivalence of violation and feasibility oracles [92].
However, the known proof of that general result requires an involved and indirect argument based on “back-
and-forth” applications of the Ellipsoid algorithm [44, §4.3]. Here we exploit the special structure of MOT to
give a direct and elementary proof. This is essential to practical implementations (see §4.1.3).

Proof. It is obvious how the MINC oracle can be implemented via a single call of the ARGMINC oracle; we now
show the converse. Speciﬁcally, given p1, . . . , pk ∈ Rn, we show how to compute a solution (cid:126)j = (j1, . . . , jk) ∈ [n]k
for ARGMINC ([p1, . . . , pk]) using nk calls to the MINC oracle and polynomial additional time. We use the ﬁrst
n calls to compute the ﬁrst index j1 of the solution, the next n calls to compute the next index j2, and so on.
s ) ∈ [n]s is a “partial solution” of size s if there exists a
for all i ∈ [s]. Then it suﬃces to show
1 , . . . , j∗
s ) of size s from a partial solution

solution j ∈ [n]k for ARGMINC ([p1, . . . , pk]) that satisﬁes ji = j∗
i
that for every s ∈ [k], it is possible to compute a partial solution (j∗
(j∗

s−1) of size s − 1 using n calls to the MINC oracle and polynomial additional time.

Formally, for s ∈ [k], let us say that (j∗

1 , . . . , j∗

1 , . . . , j∗

The simple but key observation enabling this is the following. Below, for i ∈ [k] and j ∈ [n], deﬁne qi,j to
be the vector in Rn with value [pi]j on entry j, and value −M on all other entries. In words, the following
observation states that if the constant M is suﬃciently large, then for any indices j(cid:48)
i, replacing the vectors
in a MIN oracle query eﬀectively performs a MIN oracle query on the original input
pi with the vectors qi,j(cid:48)
p1, . . . , pk except that now the minimization is only over (cid:126)j ∈ [n]k satisfying ji = j(cid:48)
i.
Observation 4.5. Set M := 2Cmax + 2 (cid:80)k

i=1 (cid:107)pi(cid:107)max + 1. Then for any s ∈ [k] and any (j(cid:48)

s) ∈ [n]s,

1, . . . , j(cid:48)

i

MINC ([q1,j(cid:48)

1

, . . . , qs,j(cid:48)

s

, ps+1, . . . , pk]) =

min
(cid:126)j∈[n]k

s.t. j1=j(cid:48)

1,...,js=j(cid:48)

s

C(cid:126)j −

k
(cid:88)

i=1

[pi]ji .

Proof. By deﬁnition of the MIN oracle,

MINC ([q1,j(cid:48)

1

, . . . , qs,j(cid:48)

s

, ps+1, . . . , pk]) = min
(cid:126)j∈[n]k

C(cid:126)j −

s
(cid:88)

i=1

[qi,j(cid:48)

i

]ji −

k
(cid:88)

i=s+1

[pi]ji

It suﬃces to prove that every minimizing tuple (cid:126)j ∈ [n]k for the right hand side satisﬁes ji = j(cid:48)
Suppose not for sake of contradiction. Then there exists a minimizing tuple (cid:126)j ∈ [n]k for which j(cid:96) (cid:54)= j(cid:48)
(cid:96) ∈ [s]. But then [q(cid:96),j(cid:48)

]j(cid:96) = −M , so the objective value of (cid:126)j is at least

i for all i ∈ [s].
(cid:96) for some

(cid:96)

C(cid:126)j −

s
(cid:88)

i=1

[qi,j(cid:48)

i

]ji −

k
(cid:88)

i=s+1

[pi]ji

(cid:62) M − Cmax −

k
(cid:88)

i=1

(cid:107)pi(cid:107)max = Cmax +

k
(cid:88)

i=1

(cid:107)pi(cid:107)max + 1.

But this is strictly larger (by at least 1) than the value of any tuple with preﬁx (j(cid:48)
optimality of (cid:126)j.

1, . . . , j(cid:48)

s), contradicting the

18

Jason M. Altschuler, Enric Boix-Adser`a

Thus, given a partial solution (j∗

1 , . . . , j∗

s−1) of length s − 1, we construct a partial solution (j∗

1 , . . . , j∗

s ) of

length s by setting j∗

s to be a minimizer of

min
j(cid:48)
s∈[n]

MINC ([q1,j∗

1 , . . . , qs−1,j∗

s−1

, qs,j(cid:48)

s

, ps+1, . . . , pk]).

(4.2)

The runtime bound is clear; it remains to show correctness. To this end, note that

min
(cid:126)j∈[n]k

s.t. j1=j∗

1 ,...,js=j∗

s

C(cid:126)j −

k
(cid:88)

i=1

[pi]ji = MINC ([q1,j∗

1 , . . . , qs,j∗

s

, ps+1, . . . , pk])

MINC ([q1,j∗

1 , . . . , qs−1,j∗

s−1

min
(cid:126)j∈[n]k
1 ,...,js−1=j∗

s.t. j1=j∗

s−1,js=j(cid:48)

s

, qs,j(cid:48)

s

, ps+1, . . . , pk])

C(cid:126)j −

k
(cid:88)

i=1

[pi]ji

= min
j(cid:48)
s∈[n]

= min
j(cid:48)
s∈[n]

=

min
(cid:126)j∈[n]k
1 ,...,js−1=j∗

s−1

s.t. j1=j∗

C(cid:126)j −

k
(cid:88)

i=1

[pi]ji

= MINC ([p1, . . . , pk]),

where above the ﬁrst and third steps are by Observation 4.5, the second step is by construction of j∗
step is by simplifying, and the ﬁnal step is by deﬁnition of (j∗
We conclude that (j∗

1 , . . . , j∗
s ) is a partial solution of size s, as desired.

s , the fourth
s−1) being a partial solution of size s − 1.

1 , . . . , j∗

We can now conclude the proof of the main result of §4.1.

Proof of Theorem 4.1. The implication “(i) =⇒ (ii)” is trivial, and the implication “(ii) =⇒ (iii)” is shown
in [6]. It therefore suﬃces to show the implication “(iii) =⇒ (i)”. This follows from combining the fact that
ELLIPSOID solves MOTC in polynomial time given an eﬃcient implementation of ARGMINC (Theorem 4.3),
with the fact that the MINC and ARGMINC oracles are polynomial-time equivalent (Lemma 4.4).

4.1.3 Practical implementation via Column Generation

Although ELLIPSOID enjoys powerful theoretical runtime guarantees, it is slow in practice because the classical
Ellipsoid algorithm is. Nevertheless, whenever ELLIPSOID is applicable (i.e., whenever the MINC oracle can be
eﬃciently implemented), we can use an alternative practical algorithm, namely the delayed Column Generation
method COLGEN, to compute exact, sparse solutions to MOT.

For completeness, we brieﬂy recall the idea behind COLGEN; for further details see the standard textbook [18,
§6.1]. COLGEN runs the Simplex method, keeping only basic variables in the tableau. Each time that COLGEN
needs to ﬁnd a Simplex variable on which to pivot, it solves the “pricing problem” of ﬁnding a variable with
negative reduced cost. This is the key subroutine in COLGEN. In the present context of the MOT LP, this pricing
problem is equivalent to a call to the ARGMIN violation oracle (see [18, Deﬁnition 3.2] for the deﬁnition of
reduced costs). By the polynomial-time equivalence of the ARGMIN and MIN oracles shown in Lemma 4.4, this
bottleneck subroutine in COLGEN can be computed in polynomial time whenever the MIN oracle can. For easy
recall, we summarize this discussion as follows.

Theorem 4.6 (Standard guarantee for COLGEN; Section 6.1 of [18]). For any T > 0, one can implement T
iterations of COLGEN in poly(n, k, T ) time and calls to the MINC oracle. When COLGEN terminates, it returns an
optimal vertex solution, which is given as a sparse tensor with at most nk − k + 1 non-zero entries.

Note that COLGEN does not have a theoretical guarantee stating that it terminates after a polynomial number
of iterations. But it often performs well in practice and terminates after a small number of iterations, leading
to much better empirical performance than ELLIPSOID.

Polynomial-time algorithms for Multimarginal Optimal Transport problems with structure

19

4.2 The Multiplicative Weights Update algorithm and the AMIN oracle

The second classical algorithm for solving implicitly-structured LPs that we study in the context of MOT is
the Multiplicative Weights Update algorithm MWU [91]. The objective of this section is to prove the following
guarantees for its specialization to MOT.

Theorem 4.7. For any family of cost tensors C ∈ (Rn)⊗k, the following are equivalent:

(i) For any ε > 0, MWU takes poly(n, k, Cmax/ε) time to solve the MOTC problem ε-approximately. (Moreover,

it outputs a sparse solution with at most poly(n, k, Cmax/ε) non-zero entries.)

(ii) There exists a poly(n, k, Cmax/ε)-time algorithm that solves the MOTC problem ε-approximately for any

ε > 0.

(iii) There exists a poly(n, k, Cmax/ε)-time algorithm that solves the AMINC problem ε-approximately for any

ε > 0.

Interpretation of results. Similarly to the analogous Theorem 4.1 for ELLIPSOID, the equivalence “(i) ⇐⇒ (ii)”
establishes that MWU can approximately solve any MOT problem in polynomial time that any other algorithm
can. Thus, from a theoretical perspective, restricting to MWU for approximately solving MOT problems is at no
loss of generality. In words, the equivalence “(ii) ⇐⇒ (iii)” establishes that approximating MOT and approxi-
mating MIN are polynomial-time equivalent. Thus we may investigate when MOT is tractable to approximate by
instead investigating the more amenable question of when MIN is tractable (see §1.1.4) at no loss of generality.
Theorem 4.7 is new to this work. In particular, equivalences between problems with polynomially small
error do not fall under the purview of classical LP theory, which deals with exponentially small error [44]. Our
use of the MWU algorithm exploits a simple reduction of MOT to a mixed packing-covering LP that has appeared
in the k = 2 matrix case of Optimal Transport in [19, 73], where implicit LP is not necessary for polynomial
runtime.

Organization of §4.2. In §4.2.1 we present the specialization of Multiplicative Weights Update to MOT, and
recall how it runs in polynomial time and calls to a certain bottleneck oracle. In §4.2.2, we show that this
bottleneck oracle is equivalent to the AMIN oracle, and then use this to prove Theorem 4.7.

4.2.1 Algorithm

Here we present the MWU algorithm, which combines the generic Multiplicative Weights Update algorithm of
[91] specialized to MOT, along with a ﬁnal rounding step that ensures feasibility of the solution.

In order to present MWU, it is convenient to assume that the cost C has entries in the range [1, 2] ⊂ R, which
is at no loss of generality by simply translating and rescaling the cost (see §4.2.2), and can be done implicitly
given a bound on Cmax. This is why in the rest of this subsection, every runtime dependence on ε is polynomial
in 1/ε for costs in the range [1, 2]; after transformation back to [−Cmax, Cmax], this is polynomial dependence
in the standard scale-invariant quantity Cmax/ε.

Since the cost C is assumed to have non-negative entries, for any λ ∈ [1, 2], the polytope

K(λ) = {P ∈ M(µ1, . . . , µk) : (cid:104)C, P (cid:105) (cid:54) λ}

of couplings with cost at most λ is a mixed packing-covering polytope (i.e., all variables are non-negative and
all constraints have non-negative coeﬃcients). Note that K(λ) is non-empty if and only if MOTC (µ) has value
at most λ. Thus, modulo a binary search on λ, this reduces computing the value of MOTC (µ) to the task of
detecting whether K(λ) is empty. Since K(λ) is a mixed packing-covering polytope, the Multiplicative Weights
Update algorithm of [91] determines whether K(λ) is empty, and runs in polynomial time apart from one
bottleneck, which we now deﬁne.

In order to deﬁne the bottleneck, we ﬁrst deﬁne a potential function. For this, we deﬁne the softmax

analogously to the softmin as

smax(a1, . . . , at) = − smin(−a1, . . . , −at) = log

(cid:32) t

(cid:88)

(cid:33)

eai

.

i=1

Here we use regularization parameter η = 1 for simplicity, since this suﬃces for analyzing MWU, and thus we
have dropped this index η for shorthand.

20

Jason M. Altschuler, Enric Boix-Adser`a

Deﬁnition 4.8 (Potential function for MWU). Fix a cost C ∈ (Rn)⊗k, target marginals µ ∈ (∆n)k, and target
value λ ∈ R. Deﬁne the potential function Φ := ΦC,µ,λ : (Rn

(cid:62)0)⊗k → R by

Φ(P ) = smax

(cid:18) (cid:104)C, P (cid:105)
λ

,

m1(P )
µ1

, . . . ,

(cid:19)

.

mk(P )
µk

The softmax in the above expression is interpreted as a softmax over the nk + 1 values in the concatenation of
vectors and scalars in its input. (This slight abuse of notation signiﬁcantly reduces notational overhead.)

Given this potential function, we now deﬁne the bottleneck operation for MWU: ﬁnd a direction (cid:126)j ∈ [n]k in

which P can be increased such that the potential is increased as little as possible.

Deﬁnition 4.9 (Bottleneck oracle for MWU). Given iterate P ∈ (Rn
value λ ∈ R, and accuracy ε > 0, MWU BOTTLENECKC (P, µ, λ, ε) either:

(cid:62)0)⊗k, target marginals µ ∈ (∆n)k, target

– Outputs “null”, certifying that min(cid:126)j∈[n]k
– Outputs (cid:126)j ∈ [n]k such that ∂

∂h Φ(P + h · δ(cid:126)j) |h=0(cid:54) 1 + ε.

∂
∂h Φ(P + h · δ(cid:126)j) |h=0> 1, or

(If min(cid:126)j∈[n]k

∂
∂h Φ(P + h · δ(cid:126)j) |h=0 is within (1, 1 + ε], then either return behavior is a valid output.)

Pseudocode for the MWU algorithm is given in Algorithm 2. We prove that MWU runs in polynomial time given

access to this bottleneck oracle.

Theorem 4.10. Let the entries of the cost C lie in the range [1, 2]. Given λ ∈ R and accuracy parameter
ε > 0, MWU either certiﬁes that MOTC (µ) (cid:54) λ, or returns a poly(n, k, 1/ε)-sparse P ∈ M(µ1, . . . , µk) satisfying
(cid:104)C, P (cid:105) (cid:54) λ + 8ε.

Furthermore, the loop in Step 1 runs in ˜O(nk/ε2) iterations, and Step 2 runs in poly(n, k, 1/ε) time.

The MWU algorithm can be used to output a O(ε)-approximate solution for MOT time via an outer loop that

performs binary search over λ; this only incurs O(log(1/ε))-multiplicative overhead in runtime.

Algorithm 2 MWU: specialization of Multiplicative Weights Update [91] to MOT

Input: Accuracy ε > 0, marginals µ1, . . . , µk ∈ ∆n, target value λ > 0
Output: Either certiﬁes MOTC (µ) > λ by returning “infeasible”, or returns a solution P with (cid:104)C, P (cid:105) (cid:54) λ + 8ε

\\ Assume cost C satisﬁes C(cid:126)j ∈ [1, 2] for all (cid:126)j ∈ [n]k (without loss of generality by rescaling)
\\ Step 1: Multiplicative Weights Update

(cid:62)0)⊗k, η ← 2(log(nk + 1))/ε

1: P ← 0 ∈ (Rn
2: while m(P ) < η do
3:
4:
5:
6: P ← P/(η(1 + ε)4)

(cid:126)j ← MWU BOTTLENECKC (P, µ, λ, ε)
if (cid:126)j =“null” then return “infeasible”
else P ← P + δ(cid:126)j · ε · min(λ/C(cid:126)j , mini[µi]ji )

\\ Step 2: round to transportation polytope

7: while m(P ) < 1 do
8:
9:
10:

ji ← argmaxj∈[n]([µi]j − [mi(P )]j ) for each i ∈ [k]
α ← mini∈[k]([µi]ji − [mi(P )]ji )
P ← P + α · δ(cid:126)j

(cid:46) While total mass is small
(cid:46) Bottleneck: ﬁnd direction with small potential increase
(cid:46) Infeasible if no good direction
(cid:46) Else, increase in good direction

(cid:46) Rescale

(cid:46) Until all constraints are satisﬁed
(cid:46) For each marginal, ﬁnd unsatisﬁed constraint
(cid:46) Maximal mass to add
(cid:46) Add mass to saturate at least one constraint

Proof. We analyze Step 1 (Multiplicative Weights Update) and Step 2 (rounding) of MWU separately.

Lemma 4.11 (Correctness of Step 1). Step 1 of Algorithm 2 runs in ˜O(nk/ε2) iterations. It either returns
(i) “infeasible”, certifying that K(λ) is empty; or (ii) ﬁnds a poly(n, k, 1/ε)-sparse tensor P ∈ (Rn
(cid:62)0)⊗k that is
approximately in K(λ), i.e., P satisﬁes:

m(P ) (cid:62) 1 − 4ε,

(cid:104)C, P (cid:105) (cid:54) λ,

and mi(P ) (cid:54) µi for all i ∈ [k]

Polynomial-time algorithms for Multimarginal Optimal Transport problems with structure

21

Step 1 is the Multiplicative Weights Update algorithm of [91] applied to the polytope K(λ), so correctness
follows from the analysis of [91]. We brieﬂy recall the main idea behind this algorithm for the convenience of the
reader, and provide a full proof in Appendix A.1 for completeness. The main idea behind the algorithm is that
on each iteration, (cid:126)j ∈ [n]k is chosen so that the increase in the potential Φ(P ) is approximately bounded by
the increase in the total mass m(P ). If this is impossible, then the bottleneck oracle returns null, which means
K(λ) is empty. So assume otherwise. Then once the total mass has increased to m(P ) = η + O(ε), the potential
Φ(P ) must be bounded by η(1 + O(ε)). By exploiting the inequality between the max and the softmax, this
means that max((cid:104)C, P (cid:105)/λ, maxi∈[n],j∈[k][mi(P )]j/[µi]j) (cid:54) Φ(P ) (cid:54) η(1 + O(ε)) as well. Thus, rescaling P by
1/(η(1 + O(ε))) in Line 6 satisﬁes m(P ) (cid:62) 1 − O(ε), (cid:104)C, P (cid:105)/λ (cid:54) 1, and mi(P )/µi (cid:54) 1. See Appendix A.1 for
full details and a proof of the runtime and sparsity claims.

Lemma 4.12 (Correctness of Step 2). Step 2 of Algorithm 2 runs in poly(n, k, 1/ε) time and returns P ∈
M(µ1, . . . , µk) satisfying (cid:104)C, P (cid:105) (cid:54) λ + 8ε. Furthermore, P only has poly(n, k, 1/ε) non-zero entries.

Proof of Lemma 4.12. By Lemma 4.11, P satisﬁes mi(P ) (cid:54) µi for all i ∈ [k]. Observe that this is an invariant
that holds throughout the execution of Step 2. This, along with the fact that (cid:80)n
j=1[mi(P )]j = m(P ) is equal
for all i, implies that the indices (j1, . . . , jk) found in Line 8 satisfy [µi]ji − [mi(P )]ji > 0 for each i ∈ [k]. Thus
in particular α > 0 in Line 9. It follows that Line 10 makes at least one more constraint satisﬁed (in particular
the constraint “[µi]ji = [mi(P )]ji ” where i is the minimizer in Line 9). Since there are nk constraints total
to be satisﬁed, Step 2 terminates in at most nk iterations. Each iteration increases the number of non-zero
entries in P by at most one, thus P is poly(n, k, 1/ε) sparse throughout. That P is sparse also implies that
each iteration can be performed in poly(n, k, 1/ε) time, thus Step 2 takes poly(n, k, 1/ε) time overall.

Finally, we establish the quality guarantee on (cid:104)C, P (cid:105). By Lemma 4.11, this is at most λ before starting Step
2. During Step 2, the total mass added to P is equal to 1 − m(P ). This is upper bounded by 4ε by Lemma 4.11.
Since Cmax (cid:54) 2, we conclude that the value of (cid:104)C, P (cid:105) is increased by at most 8ε in Step 2.

Combining Lemmas 4.11 and 4.12 concludes the proof of Theorem 4.10.

4.2.2 Equivalence of bottleneck to AMIN

In order to prove Theorem 4.7, we show that the MWU algorithm can be implemented in polynomial time and
calls to the AMIN oracle. First, we prove this fact for the ARGAMIN oracle, which diﬀers from the AMIN oracle
in that it also returns a tuple (cid:126)j ∈ [n]k that is an approximate minimizer.

Deﬁnition 4.13 (Approximate violation oracle for (MOT-D)). Given weights p = (p1, . . . , pk) ∈ Rn×k and
accuracy ε > 0, ARGAMINC returns (cid:126)j ∈ [n]k that minimizes min(cid:126)j∈[n]k C(cid:126)j − (cid:80)k
i=1[pi]ji up to additive error ε,
and its value up to additive error ε.

Lemma 4.14. Let the entries of the cost C lie in the range [1, 2]. The MWU algorithm (Algorithm 2), can be im-
plemented by poly(n, k, 1/ε) time and calls to the ARGAMINC oracle with accuracy parameter ε(cid:48) = Θ(ε2/(nk)).

Proof. We show that on each iteration of Step 1 of Algorithm 2 we can emulate the call to the MWU BOTTLENECK
oracle with one call to the ARGAMIN oracle. Recall that MWU BOTTLENECKC (P, µ, λ, ε) seeks to ﬁnd (cid:126)j ∈ [n]k
such that

V(cid:126)j :=

Φ(P + hδ(cid:126)j)

∂
∂h

(cid:12)
(cid:12)
(cid:12)h=0

is at most 1 + ε, or to certify that for all (cid:126)j it is greater than 1. By explicit computation,

V(cid:126)j =

∂
∂h

(cid:32)

(cid:32)

log

exp

(cid:32)

(cid:16)
(cid:104)C, P (cid:105) + hC(cid:126)j

(cid:17)

/λ +

k
(cid:88)

n
(cid:88)

s=1

t=1

exp (([ms(P )]t + δt,js ) /[µs]t)

(cid:33)(cid:33) (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)h=0

=

C(cid:126)j −

(cid:33)

[pi]ji

k
(cid:88)

i=1

exp((cid:104)C, P (cid:105)/λ) + (cid:80)k

s=1

t=1 exp([ms(P )]t/[µs]t)

exp((cid:104)C, P (cid:105)/λ)/λ
(cid:80)n

,

(4.3)

where the weights p = (p1, . . . , pk) ∈ Rk×n in the last line are deﬁned as

[pi]j = −

λ
exp((cid:104)C, P (cid:105)/λ)

·

exp([mi(P )]j/[µi]j)
[µi]j

,

∀i ∈ [k], j ∈ [n].

22

Jason M. Altschuler, Enric Boix-Adser`a

Note that the second term in the product in (4.3) is positive and does not depend on (cid:126)j. This suggests that in
order to minimize (4.3), it suﬃces to compute (cid:126)j ← ARGAMINC (p, ε(cid:48)) for some accuracy parameter ε(cid:48) > 0.

The main technical diﬃculty with formalizing this intuitive approach is that the weights p are not necessarily
eﬃciently computable. Nevertheless, using poly(n, k) extra time on each iteration, we can compute the marginals
m1(P ), . . . , mk(P ). Since the ARGAMIN oracle returns an ε(cid:48)-additive approximation of the cost, we can also
compute a running estimate ˜c of the cost such that, on iteration T ,

Therefore, we deﬁne weights ˜p ∈ Rn×k, which approximate p and which can be computed in poly(n, k) time

˜c − T ε(cid:48) (cid:54) (cid:104)C, P (cid:105) (cid:54) ˜c + T ε(cid:48).

on each iteration:

[˜pi]j = −

λ
exp(˜c/λ)

·

exp([mi(P )]j/[µi]j)
[µi]j

,

∀i ∈ [k], j ∈ [n].

We also deﬁne the approximate value for any (cid:126)j ∈ [n]k:

(cid:32)

˜V(cid:126)j :=

C(cid:126)j −

(cid:33)

[˜pi]ji

k
(cid:88)

i=1

exp(˜c/λ)/λ

exp(˜c/λ) + (cid:80)k

s=1

(cid:80)n

t=1 exp([ms(P )]t/[µs]t)

It holds that ARGAMINC (˜p, ε(cid:48)) returns a (cid:126)j ∈ [n]k that minimizes C(cid:126)j − (cid:80)k
i=1[˜pi]j up to multiplicative error
1/(1 − ε(cid:48)), because the entries of the cost C are lower-bounded by 1, and [˜pi]j (cid:54) 0 for all i ∈ [n], j ∈ [k]. In
particular, ARGAMINC (˜p, ε(cid:48)) minimizes ˜V(cid:126)j up to multiplicative error 1/(1 − ε(cid:48)). We prove the following claim
relating V(cid:126)j and ˜V(cid:126)j:
Claim 4.15. For any (cid:126)j ∈ [n]k, on iteration T , we have V(cid:126)j/ ˜V(cid:126)j ∈ [exp(−2T ε(cid:48)/λ), exp(2T ε(cid:48)/λ)].

By the above claim, therefore ARGAMINC (˜p, ε(cid:48)) minimizes V(cid:126)j up to multiplicative error exp(4T ε(cid:48)/λ)/(1 −
ε(cid:48)) (cid:54) (1 + ε/3) if we choose ε(cid:48) = Ω(λε/T ). Thus one can implement MWU BOTTLENECKC (p, µ, λ, ε) by
returning the value of ARGAMINC (˜p, ε(cid:48)) if its value is estimated to be at most 1 + ε/3, and returning “null“
otherwise. The bound on the accuracy ε(cid:48) = ˜Ω(ε2/(nk)) follows since λ ∈ [1, 2] follows since λ ∈ [1, 2] and
T = ˜O(nk/ε2) by Theorem 4.10.

Proof of Claim. We compare the expressions for V(cid:126)j and ˜V(cid:126)j. Each of these is a product of two terms. Since
C(cid:126)j

(cid:54) 0 for all i, the ratio of the ﬁrst terms is

(cid:62) 0, and [˜pi]ji , [pi]ji

C(cid:126)j − (cid:80)k
C(cid:126)j − (cid:80)k

i=1[˜pi]ji
i=1[pi]ji

∈ [min

[˜pi]ji /[pi]ji , max

[˜pi]ji /[pi]ji ] ⊂ [exp(−T ε(cid:48)/λ), exp(T ε(cid:48)/λ)],

i

i

where we have used that, for all i ∈ [k],

[˜pi]ji /[pi]ji = exp((cid:104)C, P (cid:105)/λ)/ exp(˜c/λ) ∈ [exp(−T ε(cid:48)/λ), exp(T ε(cid:48)/λ)].
Similarly the ratio of the second terms in the expression for V(cid:126)j and ˜V(cid:126)j is also in the range [exp(−T ε(cid:48)/λ), exp(T ε(cid:48)/λ)].
This concludes the proof of the claim.

Finally, we show that the ARGAMIN oracle can be reduced to the AMIN oracle, which completes the proof

that MWU can be run with AMIN.

Lemma 4.16 (Equivalence of AMIN and ARGAMIN). Each of the oracles AMINC and ARGAMINC with ac-
curacy parameter ε > 0 can be implemented using poly(n, k) calls of the other oracle with accuracy parameter
Θ(ε/k) and poly(n, k) additional time.

It is worth remarking that the equivalence that we show between AMIN and ARGAMIN is not known to
hold for the feasibility and separation oracles of general LPs, since the known result for general LPs requires
exponentially small error in nk [44, §4.3]. However, in the case of MOT the equivalence follows from a direct
and practical reduction, similar to the proof of the equivalence of the exact oracles (Lemma 4.4). The main
diﬀerence is that some care is needed to bound the propagation of the errors of the approximate oracles. For
completeness, we provide the full proof of Lemma 4.16 in Appendix A.

We conclude by proving Theorem 4.7.

Polynomial-time algorithms for Multimarginal Optimal Transport problems with structure

23

Proof of Theorem 4.7. The implication “(i) =⇒ (ii)” is trivial, and the implication “(ii) =⇒ (iii)” is shown
in [6]. It therefore suﬃces to show the implication “(iii) =⇒ (i)”. For costs C with entries in the range [1, 2],
this follows from combining the fact that MWU can be implemented to solve MOTC in poly(n, k, 1/ε) time given
an eﬃcient implementation of ARGAMINC with polynomially-sized accuracy parameter ε(cid:48) = poly(1/n, 1/k, ε)
(Lemma 4.14), along with the fact that the AMINC and ARGAMINC oracles are polynomially-time equivalent
with polynomial-sized accuracy parameter (Lemma 4.16).

The assumption that C has entries within the range [1, 2] can be removed with no loss by translating and
rescaling the original cost C (cid:48) ← (C + 3Cmax)/(2Cmax) and running Algorithm 2 on C (cid:48) with approximation pa-
rameter ε(cid:48) ← ε/(2Cmax). Each τ (cid:48)-approximate query to the AMINC(cid:48) oracle can be simulated by a τ -approximate
query to the AMINC oracle, where τ = 2Cmaxτ (cid:48).

Remark 4.17 (Practical optimizations). Our numerical implementation of MWU has two modiﬁcations that
provide practical speedups. One is maintaining a cached list of the tuples (cid:126)j ∈ [n]k previously returned by calls
to MWU BOTTLENECK. Whenever MWU BOTTLENECK is called, we ﬁrst check whether any tuple (cid:126)j in the
∂h Φ(P + h · δ(cid:126)j) |h=0(cid:54) 1 + ε, in which case we use this (cid:126)j to answer the oracle
cache satisﬁes the desiderata ∂
query. Otherwise, we answer the oracle query using AMIN as explained above. In practice, this cache allows
us to avoid many calls to the potentially expensive AMIN bottleneck. Our second optimization is that, at each
iteration of MWU, we check whether the current iterate P can be rescaled in order to satisfy the guarantees in
Lemma 4.11 required from Step 1. If so, we stop Step 1 early and use this rescaled P .

4.3 The Sinkhorn algorithm and the SMIN oracle

The Sinkhorn algorithm (SINKHORN) is specially tailored to MOT, and does not apply to general exponential-size
LP. Currently it is by far the most popular algorithm in the MOT literature (see §1.3). However, in general each
iteration of SINKHORN takes exponential time nΘ(k), and it is not well-understood when it can be implemented
in polynomial-time. The objective of this section is to show that this bottleneck is polynomial-time equivalent
to the SMIN oracle, and in doing so put SINKHORN on equal footing with classical implicit LP algorithms in
terms of their reliance on variants of the dual feasibility oracle for MOT. Concretely, this lets us establish the
following two results.

First, SINKHORN can solve MOT in polynomial time whenever SMIN can be solved in polynomial time.
Theorem 4.18. For any family of cost tensors C ∈ (Rn)⊗k and accuracy ε > 0, SINKHORN solves MOTC
to ε accuracy in poly(n, k, Cmax/ε) time and poly(n, k, Cmax/ε) calls to the SMINC oracle with regularization
η = (2k log n)/ε. (The solution is output through a polynomial-size implicit representation, see §4.3.1.)

Second, we show that SINKHORN requires strictly more structure than other algorithms do to solve an MOT
problem. This is why the results about ELLIPSOID (Theorem 4.1) and MWU (Theorem 4.7) state that those
algorithms solve an MOT problem whenever possible, whereas Theorem 4.18 cannot be analogously extended
to such an “if and only if” characterization.
Theorem 4.19. There is a family of cost tensors C ∈ (Rn)⊗k for which ELLIPSOID solves MOTC exactly in
poly(n, k) time, however it is #BIS-hard to implement a single iteration of SINKHORN in poly(n, k) time.

Organization of §4.3. In §4.3.1, we recall this SINKHORN algorithm and how it depends on a certain marginal-
ization oracle. In §4.3.2, we show that this marginalization oracle is polynomial-time equivalent to the SMIN
oracle, and use this to prove Theorems 4.18 and 4.19.

4.3.1 Algorithm

Here we recall SINKHORN and its known guarantees. To do this, we ﬁrst deﬁne the following oracle. While this
oracle does not have an interpretation as a dual feasibility oracle, we show below that it is polynomial-time
equivalent to SMIN, which is a speciﬁc type of approximate dual feasibility oracle (Remark 3.6).
Deﬁnition 4.20 (MARG). Given scalings d = (d1, . . . , dk) ∈ Rn×k
the marginalization oracle MARGC (d, η, i) returns the vector mi((⊗k

(cid:62)0 , regularization η > 0, and an index i ∈ [k],

i(cid:48)=1di(cid:48) ) (cid:12) exp[−ηC]) ∈ Rn

(cid:62)0.

It is known that SINKHORN can solve MOT with only polynomially many calls to this oracle [58]. The
approximate solution that SINKHORN computes is a fully dense tensor with nk non-zero entries, but it is output
implicitly in O(nk) space through “scaling vectors” and “rounding vectors”, described below.

24

Jason M. Altschuler, Enric Boix-Adser`a

Theorem 4.21 (SINKHORN guarantee, [58]). Algorithm 3 computes an ε-approximate solution to MOTC (µ)
using poly(n, k, Cmax/ε) calls to the MARGC oracle with parameter η = (2k log n)/ε, and poly(n, k, Cmax/ε)
additional time. The solution is of the form

P =

(cid:16)

⊗k

i=1di

(cid:17)

(cid:12) exp[−ηC] +

(cid:16)
⊗k

i=1vi

(cid:17)

,

(4.4)

and is output implicitly via the scaling vectors d1, . . . , dk ∈ Rn

(cid:62)0 and rounding vectors v1, . . . , vk ∈ Rn

(cid:62)0.

Algorithm 3 SINKHORN: multidimensional analog of classical Sinkhorn scaling

Input: Cost C ∈ (Rn)⊗k, marginals µ1, . . . , µk ∈ ∆n
Output: Implicit representation of tensor (4.4) that is an ε-approximate solution to MOTC (µ)
\\ Step 1: scale

1: d1, . . . , dk ← 1 and η ← (2k log n)/ε
2: for poly(n, k, Cmax/ε) iterations do
3:
4:
5:

Choose i ∈ [k]
˜µi ← MARGC (d, η, i)
di ← di (cid:12) (µi/˜µi)

\\ Step 2: round to transportation polytope

˜µi ← MARGC (d, η, i)
di ← di (cid:12) min[1, µi/˜µi]

6: for i = 1, . . . , k do
7:
8:
9: vi ← µi − MARGC (d, η, i) for each i ∈ [k]
10: v1 ← v1/(cid:107)v(cid:107)k−1
1
11: Return d1, . . . , dk and v1, . . . , vk

(cid:46) Initialize (no scaling)

(cid:46) Round-robin, greedily, or randomly
(cid:46) Bottleneck: compute i-th marginal
(cid:46) Rescale i-th marginal (division is entrywise)

(cid:46) Rescale each marginal to be below marginal constraints
(cid:46) Bottleneck: compute i-th marginal
(cid:46) Rescale i-th marginal (operations are entrywise)
(cid:46) Add back mass
(cid:46) Rescale so that (4.4) satisﬁes marginal constraints
(cid:46) Implicit representation of solution (4.4)

Sketch of algorithm. Full details and a proof are in [58]. We give a brief overview here for completeness. The
main idea of SINKHORN is to solve RMOT, the entropically regularized variant of MOT described in §2.2. On one
hand, this provides an ε-approximate solution to MOT by taking the regularization parameter η = Θ(ε−1k log n)
suﬃciently high (Lemma 2.4). On the other hand, solving RMOT rather than MOT enables exploiting the ﬁrst-
order optimality conditions of RMOT, which imply that the unique solution to RMOT is the unique tensor in
M(µ1, . . . , µk) of the form

P ∗ = (⊗k

i=1d∗

i ) (cid:12) K,

(4.5)

where K denotes the entrywise exponentiated tensor exp[−ηC], and d∗
1, . . . , d∗
The SINKHORN algorithm approximately computes this solution in two steps.

k ∈ Rn

(cid:62)0 are non-negative vectors.

The ﬁrst and main step of Algorithm 3 is the natural multimarginal analog of the Sinkhorn scaling algo-
rithm [78]. It computes an approximate solution P = (⊗k
i=1di) (cid:12) K by ﬁnding d1, . . . , dk such that P is nearly
feasible in the sense that mi(P ) ≈ µi for each i ∈ [k]. Brieﬂy, it does this via alternating optimization: initialize
di to the all-ones vector 1 ∈ Rn, and then iteratively update one di so that the i-th marginal mi(P ) of the
current scaled iterate P = (⊗k
i=1di) (cid:12) K is µi. Although correcting one marginal can detrimentally aﬀect the
others, this algorithm nevertheless converges—in fact, in a polynomial number of iterations [58].

The second step of Algorithm 3 is the natural multimarginal analog of the rounding algorithm [5, Algorithm
2]. It rounds the solution P = (⊗k
i=1di) (cid:12) K found in step one to the transportation polytope M(µ1, . . . , µk).
Brieﬂy, it performs this by scaling each marginal mi(P ) to be entrywise less than the desired µi, and then
adding mass back to P so that all marginals constraints are exactly satisﬁed. The former adjustment is done by
adjusting the diagonal scalings d1, . . . , dk, and the latter adjustment is done by adding a rank-1 term ⊗k
i=1vi.
Critically, note that Algorithm 3 takes polynomial time except for possibly the calls to the MARGC oracle.
In the absence of structure in the cost tensor C, evaluating this MARGC oracle takes exponential time because
it requires computing marginals of a tensor with nk entries.

We conclude this discussion with several remarks about SINKHORN.

Remark 4.22 (Choice of update index in SINKHORN). In line 3 there are several ways to choose update indices,
all of which lead to the polynomial iteration complexity we desire. Iteration-complexity bounds are shown for
a greedy choice in [40, 58]. Similar bounds can be shown for random and round-robin choices by adapting the

Polynomial-time algorithms for Multimarginal Optimal Transport problems with structure

25

techniques of [9]. These latter two choices do not incur the overhead of k MARG computations per iteration
required by the greedy choice, which is helpful in practice. Empirically, we observe that round-robin works quite
well, and we use this in our experiments.

Remark 4.23 (Alternative implementations of SINKHORN). For simplicity, Algorithm 3 provides pseudocode
for the “vanilla” version of SINKHORN as it performs well in practice and it achieves the polynomial itera-
tion complexity we desire. There are several variants in the literature, including accelerated versions and ﬁrst
rounding small entries of the marginals—these variants have iteration-complexity bounds with better polynomial
dependence on ε and k, albeit sometimes at the expense of larger polynomial factors in n [58, 84].

Remark 4.24 (Output of SINKHORN and eﬃcient downstream tasks). While the output P of SINKHORN is
fully dense with nk non-zero entries, its speciﬁc form (4.4) enables performing downstream tasks in polynomial
time. This is conditional on a polynomial-time MARGC oracle, which is at no loss of generality since that is
required for running SINKHORN in polynomial time in the ﬁrst place. The basic idea is that P is a mixture of
two simple distributions (modulo normalization). The ﬁrst is (cid:0)⊗k
(cid:1) (cid:12) exp[−ηC], which is marginalizable
using MARGC . The second is ⊗k
i=1vi, which is easily marginalizable since it is a product distribution (as the
vi are non-negative). Together, this enables eﬃcient marginalization of P . By recursively marginalizing on
conditional distributions, this enables eﬃciently sampling from P . This in turn enables eﬃcient estimation of
bounded statistics of P (e.g., the cost (cid:104)C, P (cid:105)) by Hoeﬀding’s inequality.

i=1di

4.3.2 Equivalence of bottleneck to SMIN

Although Theorem 4.21 shows that SINKHORN solves MOT in polynomial time using the MARG oracle, this is
neither suﬃcient to prove the implication “(ii) =⇒ (i)” in Theorem 4.18, nor to prove Theorem 4.19. In order
to prove these results, we show that SMIN and MARG are polynomial-time equivalent.
Lemma 4.25 (Equivalence of MARG and SMIN). For any regularization η > 0, each of the oracles MARGC
and SMINC can be implemented using poly(n) calls of the other oracle and poly(n, k) additional time.

Proof. Reduction from SMIN to MARG. First, we show how to compute SMINC (p, η) using one call to the
marginalization oracle and O(n) additional time. Consider the entrywise exponentiated matrix d = exp[ηp] ∈
Rn×k

i=1di) (cid:12) exp[−ηC]) be the answer to MARGC (d, η, 1). Observe that

(cid:62)0 , and let µ1 = m1((⊗k

−η−1 log





n
(cid:88)

j1=1


 = −η−1 log





n
(cid:88)

[µ1]j1

(cid:88)

k
(cid:89)

[di]ji e−ηC(cid:126)j





j1=1

j2,...,jk∈[n]

i=1

= −η−1 log





(cid:88)

e−η(C(cid:126)j −(cid:80)k

i=1[pi]ji )





(cid:126)j∈[n]k

(cid:32)

= sminη
(cid:126)j∈[n]k

C(cid:126)j −

(cid:33)

[pi]ji

,

k
(cid:88)

i=1

where above the ﬁrst step is by deﬁnition of µ1, the second step is by deﬁnition of d and combining the sums, and
the third step is by deﬁnition of smin. We conclude that −η−1 log (cid:80)n
j1=1[µ1]j1 is a valid answer to SMINC (p, η).
Since this is clearly computable from µ1 in O(n) time, this establishes the claimed reduction.

Reduction from MARG to SMIN. Next, we show that for any marginalization index i ∈ [k] and entry (cid:96) ∈ [n],
it is possible to compute the (cid:96)-th entry of the vector MARGC (d, η, i) using one call to the SMINC oracle and
poly(n, k) additional time. Deﬁne v ∈ Rn to be the vector with (cid:96)-th entry equal to [di](cid:96), and all other entries
0. Deﬁne the matrix p = η−1 log[d1, . . . , di−1, v, di+1, . . . , dk] ∈ ¯Rn×k, where recall that log 0 = −∞ (see §2).
Let s ∈ R denote the answer to SMINC (p, η). Observe that

e−ηs =

(cid:88)

e−η(C(cid:126)j −(cid:80)k

i=1[pi]ji ) =

(cid:88)

k
(cid:89)

[di]ji e−ηC(cid:126)j =

(cid:16)

(cid:104)

mi

(cid:126)j∈[n]k

(cid:126)j∈[n]k : (cid:126)ji=(cid:96)

i=1

(⊗k

i=1di) (cid:12) exp[−ηC]

(cid:17)(cid:105)

,

(cid:96)

where above the ﬁrst step is by deﬁnition of s, the second step is by deﬁnition of p and v, and the third step is
by deﬁnition of the marginalization notation mi(·). We conclude that exp(−ηs) is a valid answer for the (cid:96)-th
entry of the vector MARGC (d, η, i). This establishes the claimed reduction since we may repeat this procedure
n times to compute all n entries of the the vector MARGC (d, η, i).

26

Jason M. Altschuler, Enric Boix-Adser`a

We can now conclude the proofs of the main results of §4.3.

Proof of Theorem 4.18. This follows from the fact that SINKHORN approximates MOTC in polynomial time
given a eﬃcient implementation of MARGC (Theorem 4.21), combined with the fact that the MARGC and
SMINC oracles are polynomial-time equivalent (Lemma 4.25).

Proof of Theorem 4.19. Consider the family of cost tensors in Lemma 3.7 for which the MINC oracle admits a
polynomial-time algorithm, but for which the SMINC oracle is #BIS-hard. Then on one hand, the ELLIPSOID
algorithm solves MOTC in polynomial time by Theorem 4.1. And on the other hand, it is #BIS-hard to
implement a single iteration of SINKHORN because that requires implementing the MARGC oracle, which is
polynomial-time equivalent to the SMINC oracle by Lemma 4.25.

5 Application: MOT problems with graphical structure

In this section, we illustrate our algorithmic framework on MOT problems with graphical structure. Although
a polynomial-time algorithm is already known for this particular structure [49, 82], that algorithm computes
solutions that are approximate and dense; see the related work section for details. By combining our algorithmic
framework developed above with classical facts about graphical models, we show that it is possible to compute
solutions that are exact and sparse in polynomial time.

The section is organized as follows. In §5.1, we recall the deﬁnition of graphical structure. In §5.2, we show
that the MIN, AMIN, and SMIN oracles can be implemented in polynomial time for cost tensors with graphical
structure; from this it immediately follows that all of the MOT algorithms discussed in part 1 of this paper can
be implemented in polynomial time. Finally, in §5.3, we demonstrate our results on the popular application of
computing generalized Euler ﬂows, which was the original motivation of MOT. Numerical simulations demon-
strate how the exact, sparse solutions produced by our new algorithms provide qualitatively better solutions
than previously possible in polynomial time.

5.1 Setup

We begin by recalling preliminaries about undirected graphical models, a.k.a., Markov Random Fields. We
recall only the relevant background; for further details we refer the reader to the textbooks [55, 88].

In words, graphical models provide a way of encoding the independence structure of a collection of random
variables in terms of a graph. The formal deﬁnition is as follows. Below, all graphs are undirected, and the
notation 2V means the power set of V (i.e., the set of all subsets of V ).
Deﬁnition 5.1 (Graphical model structure). Let S ⊂ 2[k]. The graphical model structure corresponding to S
is the graph GS = (V, E) with vertices V = [k] and edges E = {(i, j) : i, j ∈ S, for some S ∈ S}.
Deﬁnition 5.2 (Graphical model). Let S ⊂ 2[k]. A probability distribution P over {Xi}i∈[k] is a graphical
model with structure S if there exist functions {ψS}S∈S and normalizing constant Z such that

(cid:16)

P

{xi}i∈[k]

(cid:17)

=

1
Z

(cid:89)

S∈S

ψS

(cid:16)

{xi}i∈S

(cid:17)
.

A standard measure of complexity for graphical models is the treewidth of the underlying graphical model
structure GS because this captures not just the storage complexity, but also the algorithmic complexity of
performing fundamental tasks such as computing the mode, log-partition function, and marginal distribu-
tions [55, 88]. There are a number of equivalent deﬁnitions of treewidth [22]. Each requires deﬁning intermediate
combinatorial concepts. We recall here the deﬁnition that is based on the concept of a junction tree because
this is perhaps the most standard deﬁnition in the graphical models community.

Deﬁnition 5.3 (Junction tree, treewidth). A junction tree T = (VT , ET , {Bu}u∈VT ) for a graph G = (V, E)
consists of a tree (VT , ET ) and a set of bags {Bu ⊆ V }u∈VT satisfying:

– For each variable i ∈ V , the set of nodes Ui = {u ∈ VT : i ∈ Bu} induces a subtree of T .
– For each edge e ∈ E, there is some bag Bu containing both endpoints of e.

The width of the junction tree is one less than the size of the largest bag, i.e., is maxu∈VT |Bu|−1. The treewidth
of a graph is the width of its minimum-width junction tree.

Polynomial-time algorithms for Multimarginal Optimal Transport problems with structure

27

(a) Path graph.

(b) Junction tree.

Fig. 1: The path graph (left) has treewidth 1 because the corresponding junction tree (right) has bags of size at most 2.

(a) Window graph with window size 2.

(b) Junction tree.

Fig. 2: The graph that has an edges between all vertices of distance at most two when ordered sequentially (left) has treewidth
2 because the corresponding junction tree (right) has bags of size at most 3.

(a) Cycle graph.

(b) Junction tree.

Fig. 3: The cycle graph (left) has treewidth 2 because the corresponding junction tree (right) has bags of size at most 3.

See Figures 1, 2, and 3 for illustrated examples.
We now formally recall the deﬁnition of graphical structure for MOT.

Deﬁnition 5.4 (Graphical structure for MOT). An MOT cost tensor C ∈ (Rn)⊗k has graphical structure with
treewidth ω if there is a graphical model structure S ⊂ 2[k] and functions {fS}S∈S such that

C(cid:126)j =

(cid:88)

S∈S

fS

(cid:16)

{ji}i∈S

(cid:17)

,

∀(cid:126)j := (j1, . . . , jk) ∈ [n]k,

(5.1)

and such that the graph GS has treewidth ω.

We make three remarks about this structure. First, note that the functions {fS}S∈S can be arbitrary so

long as the corresponding graphical model structure has treewidth at most ω.

Second, if Deﬁnition 5.4 did not constrain the treewidth ω, then every tensor C would trivially have graph-
ical structure with maximal treewidth ω = k − 1 (take S to be the singleton containing [k], GS to be the
complete graph, and f[k] to be C). Just like all previous algorithms, our algorithms have runtimes that depend
exponentially (only) on the treewidth of GS . This is optimal in the sense that unless P = NP, there is no
algorithm with jointly polynomial runtime in the input size and treewidth [6]. We also point out that in all
current applications of graphically structured MOT, the treewidth is either 1 or 2, see §1.3.

Third, as in all previous work on graphically structured MOT, we make the natural assumptions that the
cost C is input implicitly through the functions {fS}S∈S , and that each function fS can be evaluated in
polynomial time, since otherwise graphical structure is useless for designing polynomial-time algorithms. In all
applications in the literature, these two basic assumptions are always satisﬁed. Note also that if the treewidth
of the graphical structure is constant, then there is a linear-time algorithm to compute the treewidth and a
corresponding minimum-width junction tree [21].

5.2 Polynomial-time algorithms

By our oracle reductions in §4, in order to design polynomial-time algorithms for MOT with graphical structure,
it suﬃces to design polynomial-time algorithms for the MIN, AMIN, or SMIN oracles. This follows directly from
classical algorithmic results in the graphical models literature [55].

28

Jason M. Altschuler, Enric Boix-Adser`a

Theorem 5.5 (Polynomial-time algorithms for the MIN, AMIN, and SMIN oracles for costs with graphical
structure). Let C ∈ (Rn)⊗k be a cost tensor that has graphical structure with constant treewidth ω (see Deﬁni-
tion 5.4). Then the MINC , AMINC , and SMINC oracles can be computed in poly(n, k) time.

Algorithm 4 Polynomial-time algorithm for MIN for graphically structured costs (Theorem 5.5).

Input: Cost C with graphical structure, matrix p ∈ Rn×k
Output: Solution to MINC (p)

1: (cid:126)j ← mode of the graphical model P in (5.2)
2: Return C(cid:126)j − (cid:80)k
i=1[pi]ji

(cid:46) Using the classical max-product algorithm [55, §13.3]
(cid:46) Value of the MINC (p) oracle

Algorithm 5 Polynomial-time algorithm for SMIN for graphically structured costs (Theorem 5.5).

Input: Cost C with graphical structure, matrix p ∈ ¯Rn×k, regularization η > 0
Output: Solution to SMINC (p, η)

1: Z ← partition function of the graphical model P in (5.2)
2: Return −η−1 log Z

(cid:46) Using the classical sum-product algorithm [55, §10.2]
(cid:46) Value of the SMINC (p, η) oracle

Proof. Consider input p for the oracles. Let P denote the probability distribution on [n]k given by

P ((cid:126)j) =

1
Z

(cid:32)

(cid:32)

exp

−η

C(cid:126)j −

(cid:33)(cid:33)

[pi]ji

,

k
(cid:88)

i=1

∀(cid:126)j ∈ [n]k,

(5.2)

(cid:126)j∈[n][k] exp(−η(C(cid:126)j −(cid:80)k

where Z = (cid:80)
i=1[pi]ji )) ensures P is normalized. Observe that the MINC oracle amounts7
to computing the mode of the distribution P because MINC (p) = C(cid:126)j −(cid:80)k
i=1[pi]ji , where (cid:126)j ∈ [n]k is a maximizer
of P(cid:126)j. Also, the SMINC oracle amounts to computing the partition function Z because SMINC (p) = −η−1 log Z.
Thus it suﬃces to compute the mode and partition function of P in polynomial time. (The AMINC oracle follows
from the MINC oracle by Remark 3.5).

To this end, observe that by assumption on C, there is a graphical model structure S ∈ 2[k] and functions

{fS}S∈S such that the corresponding graph GS has treewidth ω and the distribution P factors as

(cid:32)

(cid:32)

P ((cid:126)j) = exp

−η

(cid:88)

fS ({ji}i∈S) −

S∈S

(cid:33)(cid:33)

[pi]ji

.

k
(cid:88)

i=1

It follows that P is a graphical model with respect to the same graphical model structure S because the
“vertex potentials” exp(η[pi]ji ) do not aﬀect the underlying graphical model structure. Thus P is a graphical
model with constant treewidth ω, so we may compute the mode and partition function of P in poly(n, k) time
using, respectively, the classical max-product and sum-product algorithms [55, Chapters 13.3 and 10.2]. For
convenience, pseudocode summarizing this discussion is provided in Algorithms 4 and 5.

An immediate consequence of Theorem 5.5 combined with our oracle reductions is that all candidate MOT
algorithms in §4 can be eﬃciently implemented for MOT problems with graphical structure. From a theoretical
perspective, ELLIPSOID gives the best guarantee since it produces an exact, sparse solution.

Corollary 5.6 (Polynomial-time algorithms for MOT problems with graphical structure). Let C ∈ (Rn)⊗k be
a cost tensor that has graphical structure with constant treewidth ω (see Deﬁnition 5.4). Then:

– The ELLIPSOID algorithm in §4.1 computes an exact solution to MOTC in poly(n, k) time.
– The MWU algorithm in §4.2 computes an ε-approximate solution to MOTC in poly(n, k, Cmax/ε) time.
– The SINKHORN algorithm in §4.3 computes an ε-approximate solution to MOTC in poly(n, k, Cmax/ε) time.
– The COLGEN algorithm in §4.1.3 can be run for T iterations in poly(n, k, T ) time.

7 In fact, for the purpose of computing MINC , the distribution P ((cid:126)j) can be deﬁned using any η > 0.

Polynomial-time algorithms for Multimarginal Optimal Transport problems with structure

29

Moreover, ELLIPSOID, MWU, and COLGEN output a polynomially sparse tensor, whereas SINKHORN outputs a fully
dense tensor through the implicit representation described in §4.3.1.

Proof. Combine the polynomial-time implementations of the oracles in Theorem 5.5 with the polynomial-time
algorithm-to-oracle reductions in Theorems 4.1, 4.7, 4.18, and 4.6, respectively.

5.3 Application vignette: ﬂuid dynamics

In this section, we numerically demonstrate our new results for graphically structured MOT—namely the ability
to compute exact, sparse solutions in polynomial time (Corollary 5.6). We illustrate this on the problem of
computing generalized Euler ﬂows—an MOT application which has received signiﬁcant interest and which was
historically the motivation of MOT, see e.g., [14, 16, 23, 24, 25, 26]. This MOT problem is already known
to be tractable via a popular, specially-tailored modiﬁcation of SINKHORN [14]—which can be interpreted as
implementing SINKHORN using graphical structure [49, 82]. However, that algorithm is based on SINKHORN and
thus unavoidably produces solutions that are low-precision (due to poly(1/ε) runtime dependence), fully dense
(with nk non-zero entries), and have well-documented numerical precision issues. We oﬀer the ﬁrst polynomial-
time algorithm for computing exact and/or sparse solutions.

We brieﬂy recall the premise of this MOT problem; for further background see [14, 26]. An incompressible
ﬂuid (e.g., water) is modeled by n particles which are uniformly distributed in space (due to incompressibility)
at all times t ∈ {1, . . . , k + 1}. We observe each particle’s location at initial time t = 1 and ﬁnal time t = k + 1.
The task is to infer the particles’ locations at all intermediate times t ∈ {2, . . . , k}, and this is modeled by an
MOT problem as follows.

Speciﬁcally, the locations of the ﬂuid particles are discretized to points {xj}j∈[n] ⊂ Rd, and σ is a known
permutation on this set that encodes the relation between each initial location xj at time t = 1 and ﬁnal location
σ(xj) at time t = k + 1. The total movement of a particle that takes the trajectory xj1 , xj2 , . . . , xjk , σ(xj1 ) is
given by

Cj1,...,jk = (cid:107)σ(xj1 ) − xjk (cid:107)2 +

k−1
(cid:88)

t=1

(cid:107)xjt+1 − xjt (cid:107)2,

(5.3)

By the principle of least action, the generalized Euler ﬂow problem of inferring the most likely trajectories
of the ﬂuid particles is given by the solution to the MOT problem with this cost C and uniform marginals
µt = 1n/n ∈ ∆n which impose the constraint that the ﬂuid is incompressible.

Corollary 5.7 (Exact, sparse solutions for generalized Euler ﬂows). The MOT problem with cost (5.3) can be
solved in d · poly(n, k) time. The solution is returned as a sparse tensor with at most nk − k + 1 non-zeros.

Proof. This cost tensor C can be expressed in graphical form C(cid:126)j = (cid:80)
S∈S fS({ji}) where S consists of the sets
{1, 2}, . . . , {k − 1, k} of adjacent time points as well as the set {1, k}. Moreover, each function fS : [n]2 → R can
be computed in O(dn2) time since this simply requires computing (cid:107)xj −xj(cid:48) (cid:107)2 for n2 pairs of points xj, xj(cid:48) ∈ Rd.
Once this graphical representation is computed, Corollary 5.6 implies a poly(n, k) time algorithm for this MOT
problem because the graphical model structure S is a cycle graph and thus has treewidth 2 (cf., Figure 3).

Figure 4 illustrates how the exact, sparse solutions found by our new algorithm provide visually sharper
estimates than the popular modiﬁcation of SINKHORN in [14], which blurs the trajectories. The latter is the state-
of-the-art algorithm in the literature and in particular is the only previously known non-heuristic algorithm that
has polynomial-time guarantees. Note that this algorithm is identical to implementing SINKHORN by exploiting
the graphical structure to perform exact marginalization eﬃciently [49, 82].

The numerical simulation is on a standard benchmark problem used in the literature (see e.g., [14, Figure
9] and [26, Figure 2]) in which the particle at initial location x ∈ [0, 1] moves to ﬁnal location σ(x) = x + 1
2
(mod 1). This is run with k = 6 and marginals µ1 = · · · = µk uniformly supported on n = 51 positions in [0, 1].
See Appendix B for numerics on other standard benchmark instances. Note that this amounts to solving an
MOT LP with nk = 516 ≈ 1.8 × 1010 variables, which is infeasible for standard LP solvers. Our algorithm is
the ﬁrst to compute exact solutions for problem instances of this scale.

8 All experiments in this paper are run on a standard-issue Apple MacBook Pro 2020 laptop with an M1 Chip.

30

Jason M. Altschuler, Enric Boix-Adser`a

t = 1

t = 2

t = 3

t = 4

t = 5

t = 6

t = 7

t = 1

t = 2

t = 3

t = 4

t = 5

t = 6

t = 7

SINKHORN

COLGEN

Fig. 4: Transport maps computed by the fast implementation of SINKHORN [14] (left) and our COLGEN implementation (right)
on a standard ﬂuid dynamics benchmark problem in dimension d = 1 [26]. The pairwise transport maps between successive
timesteps are plotted with opacity proportional to the mass. The SINKHORN algorithm is run at the highest precision (i.e.,
smallest regularization) before serious numerical precision issues (NaNs). It returns a dense, approximate solution in 2.25
seconds.8 COLGEN returns an exact, sparse solution in 9.52 seconds. Furthermore, in this particular problem instance, the COLGEN
method returns a Monge solution, i.e., the sparsity is n so that the particles never split in the computed trajectories.

Two important remarks. First, since this MOT problem is a discretization of the underlying PDE, an
exact solution is of course not necessary; however, there is an important—even qualitative—diﬀerence be-
tween low-precision solutions (computable with poly(1/ε) runtime) and high-precision solutions (computable
with polylog(1/ε) runtime) for the discretized problem. Second, a desirable feature of SINKHORN that should
be emphasized is its practical scalability, which might make it advantageous for problems where very ﬁne
discretization is required. It is an interesting direction of practical relevance to develop algorithms that can
compute high-precision solutions at a similarly large scale in practice (see the discussion in §8).

6 Application: MOT problems with set-optimization structure

In this section, we consider MOT problems whose cost tensors C take values 0 and 1—or more generally any
two values, by a straightforward reduction9. Such MOT problems arise naturally in applications where one
seeks to minimize or maximize the probability that some event occurs given marginal probabilities on each
variable (see Example 6.1). We establish that this general class of MOT problems can be solved in polynomial
time under a condition on the sparsity pattern of C that is often simple to check due its connection to classical
combinatorial optimization problems.

The section is organized as follows. In §6.1 we formally describe this setup and discuss why it is incomparable
to all other structures discussed in this paper. In §6.2, we show that for costs with this structure, the MIN,
AMIN, and SMIN oracles can be implemented in polynomial time; from this it immediately follows that the
ELLIPSOID, MWU, SINKHORN, and COLGEN algorithms discussed in part 1 of this paper can be implemented in
polynomial time. In §6.3, we illustrate our results via a case study on network reliability.

6.1 Setup

Example 6.1 (Motivation for binary-valued MOT costs: minimizing/maximizing probability of an event). Let
S ⊂ [n]k. If C(cid:126)j = 1[(cid:126)j ∈ S], then the MOTC problem amounts to minimizing the probability that event S occurs,
given marginals on each variable. On the other hand, if C(cid:126)j = 1[(cid:126)j /∈ S], then the MOTC problem amounts to
maximizing the probability that event S occurs since

MOTC (µ1, . . . , µk) =

min
P ∈M(µ1,...,µk)

P(cid:126)j∼P [(cid:126)j /∈ S] = 1 −

max
P ∈M(µ1,...,µk)

P(cid:126)j∼P [(cid:126)j ∈ S].

Even if the cost is binary-valued, there is no hope to solve MOT in polynomial time without further
assumptions—essentially because in the worst case, any algorithm must query all nk entries if C is a completely
arbitrary {0, 1}-valued tensor.

9 If C takes two values a < b, then deﬁne the tensor ˜C with {0, 1}-entries by ˜C(cid:126)j = (C(cid:126)j − a)/(b − a). It is straightforward to

see that the MOT problems with costs C and ˜C have identical solutions.

Polynomial-time algorithms for Multimarginal Optimal Transport problems with structure

31

We show that MOT is polynomial-time solvable under the general and often simple-to-check condition that
the MIN, AMIN, and SMIN oracles introduced in §3 are polynomial-time solvable when restricted to the set S
of indices (cid:126)j ∈ [n]k for which C(cid:126)j = 0. For simplicity, our deﬁnition of these set oracles removes the cost C(cid:126)j as it
is constant on S. Of course it is also possible to remove the negative sign in −p by re-parameterizing the inputs
as w = −p; however, we keep this notation in order to parallel the original oracles.
Deﬁnition 6.2 (MIN set oracle). Let S ⊂ [n]k. For weights p = (p1, . . . , pk) ∈ Rn×k, MINC,S(p) returns

−

min
(cid:126)j∈S

k
(cid:88)

i=1

[pi]ji .

Deﬁnition 6.3 (AMIN set oracle). Let S ⊂ [n]k. For weights p = (p1, . . . , pk) ∈ Rn×k and accuracy ε > 0,
AMINC,S(p, ε) returns MINC,S(p) up to additive error ε.
Deﬁnition 6.4 (SMIN set oracle). Let S ⊂ [n]k. For weights p = (p1, . . . , pk) ∈ ¯Rn×k and regularization
parameter η > 0, SMINC,S(p, η) returns

−

sminη
(cid:126)j∈S

k
(cid:88)

i=1

[pi]ji .

The key motivation behind these set oracle deﬁnitions (aside from the syntactic similarity to the original
oracles) is that they encode the problem of (approximately) ﬁnding the min-weight object in S. This opens
the door to combinatorial applications of MOT because ﬁnding the min-weight object in S is well-known to be
polynomial-time solvable for many “combinatorial-structured” sets S of interest—e.g., the set S of cuts in a
graph, or the set S of independent sets in a matroid. See §6.3 for fully-detailed applications.
Deﬁnition 6.5 (Set-optimization structure for MOT). An MOT cost tensor C ∈ (Rn)⊗k has exact, approxi-
mate, or soft set-optimization structure of complexity β if

C(cid:126)j = 1[(cid:126)j /∈ S]

for a set S ⊂ [n]k for which there is an algorithm solving MINC,S, AMINC,S, or SMINC,S, respectively, in β
time.

We make two remarks about this structure.

Remark 6.6 (Only require set oracle for C −1(0), not for C −1(1)). Note that Deﬁnition 6.5 only requires the
set oracles for the set S of entries where C is 0, and does not need the set oracles for the set [n]k \ S where
C is 1. The fact that both set oracles are not needed makes set-optimization structure easier to check than the
original oracles in §3, because those eﬀectively require optimization over both S and [n]k \ S.

Remark 6.7 (Set-optimization structure is incomparable to graphical and low-rank plus sparse structure).
Costs C that satisfy Deﬁnition 6.5 in general do not have non-trivial graphical structure or low-rank plus
sparse structure. Speciﬁcally, there are costs C that satisfy Deﬁnition 6.5, yet require maximal k − 1 treewidth
to model via graphical structure, and super-constant rank or exponential sparsity to model via low-rank plus
sparse structure. (A concrete example is the network reliability application in §6.3.) Because of the NP-hardness
of MOT problems with (k − 1)-treewidth graphical structure or super-constant rank [6], simply modeling such
problems with graphical structure or low-rank plus rank structure is therefore useless for the purpose of designing
polynomial-time MOT algorithms.

6.2 Polynomial-time algorithms

By our oracle reductions in part 1 of this paper, in order to design polynomial-time algorithms for MOT
with set-optimization structure, it suﬃces to design polynomial-time algorithms for the MIN, AMIN, or SMIN
oracles. We show how to do this for all three oracles in a straightforward way by exploiting the set-optimization
structure.

Theorem 6.8 (Polynomial-time algorithms for the MIN, AMIN, and SMIN oracles for costs with set-optimiza-
tion structure). If C ∈ (Rn)⊗k is a cost tensor with exact, approximate, or soft set-optimization structure of
complexity β (see Deﬁnition 6.5), then the MINC , AMINC , and SMINC oracles, respectively, can be computed
in β + poly(n, k) time.

32

Jason M. Altschuler, Enric Boix-Adser`a

Algorithm 6 Polynomial-time algorithm for MIN for costs with exact set-optimization structure (Theorem 6.8).

Input: Access to C via MINC,S oracle, matrix p ∈ Rn×k
Output: Solution to MINC (p)

1: a ← MINC,S (p)
2: x ← − (cid:80)k
3: Return a if a (cid:54) x, or min(a, 1 + x) otherwise

i=1 maxj∈[n][pi]j

(cid:46) One oracle call
(cid:46) Takes O(nk) time
(cid:46) Takes O(1) time

Algorithm 7 Polynomial-time algorithm for SMIN for costs with soft set-optimization structure (Theorem 6.8).

Input: Access to C via SMINC,S oracle, matrix p ∈ ¯Rn×k, regularization η
Output: Solution to SMINC (p, η)

1: a ← exp(−η · SMINC,S (p))
2: x ← (cid:81)k
(cid:80)n
3: Return −η−1 log(e−ηx + (1 − e−η)a)

j=1 exp(η[pi]ji )

i=1

(cid:46) One oracle call
(cid:46) Takes O(nk) time
(cid:46) Takes O(1) time

Proof. Polynomial-time algorithm for MIN. We ﬁrst claim that Algorithm 6 implements the MINC (p) oracle.
To this end, deﬁne

a := MINC,S(p) = min
(cid:126)j∈[n]k
s.t. C(cid:126)j =0

−

k
(cid:88)

i=1

[pi]ji

and

b := min
(cid:126)j∈[n]k
s.t. C(cid:126)j =1

−

k
(cid:88)

i=1

[pi]ji .

(6.1)

By re-arranging the sum and max, it follows that

x := −

k
(cid:88)

i=1

max
j∈[n]

[pi]j = − max
(cid:126)j∈[n]k

k
(cid:88)

i=1

[pi]ji = min
(cid:126)j∈[n]k

k
(cid:88)

i=1

−[pi]ji = min(a, b).

(6.2)

Therefore

MINC (p) = min
(cid:126)j∈[n]k

C(cid:126)j −

k
(cid:88)

i=1

[pi]ji = min(a, 1 + b) =

(cid:40)

a
min(a, 1 + min(a, b))

if a (cid:54) b
if a > b

(cid:40)

a
min(a, 1 + x)

=

if a (cid:54) x
if a > x

,

(6.3)

where above the ﬁrst step is by deﬁnition of MINC ; the second step is by partitioning the minimization over
(cid:126)j ∈ [n]k into (cid:126)j such that C(cid:126)j = 0 or C(cid:126)j = 1, and then plugging in the deﬁnitions of a and b; the third step is
by manipulating min(a, 1 + b) in both cases; and the last step is because x = min(a, b) as shown above. We
conclude that Algorithm 6 correctly outputs MINC (p). Since the algorithm uses one call to the MINC,S oracle
and O(nk) additional time, the claim is proven.

Polynomial-time algorithm for AMIN. Next, we claim that the same Algorithm 6, now run with the approx-
imate oracle AMINC,S(p, ε) in the ﬁrst step instead of the exact oracle MINC,S(p), computes a valid solution
to AMINC (p, ε). To prove this, let a, b, and x be as deﬁned in (6.1) and (6.2) for the MIN analysis, and let
˜a = AMINC,S(p, ε). By the same logic as in (6.3), except now reversed, the output

(cid:40)

˜a
min(˜a, 1 + x)

if ˜a (cid:54) x
if ˜a > x

is equal to min(˜a, 1 + b). Now because ˜a is within additive ε error of a (by deﬁnition of the AMINC,S oracle),
it follows that the above output is within ε additive error of

min(a, 1 + b) = min
(cid:126)j∈[n]k

C(cid:126)j −

k
(cid:88)

i=1

[pi]ji = MINC (p).

Thus the output is a valid answer to AMINC (p, ε), establishing correctness. The runtime claim is obvious.

Polynomial-time algorithms for Multimarginal Optimal Transport problems with structure

33

Polynomial-time algorithm for SMIN. Finally, we claim that Algorithm 7 implements the SMINC (p, η) ora-

cle. To this end, deﬁne

a := e−η·SMINC,S (p,η) =

(cid:88)

eη (cid:80)k

i=1[pi]ji

and

b :=

(cid:88)

eη (cid:80)k

i=1[pi]ji .

(cid:126)j∈[n]k
s.t. C(cid:126)j=0

(cid:126)j∈[n]k
s.t. C(cid:126)j=1

By re-arranging products and sums, it follows that

x :=

k
(cid:89)

n
(cid:88)

i=1

j=1

eη[pi]ji =

(cid:88)

k
(cid:89)

(cid:126)j∈[n]k

i=1

eη[pi]ji = a + b.

Therefore

SMINC (p, η) = −

1
η



log



(cid:88)

e−η(C(cid:126)j −(cid:80)k

i=1[pi]ji )

 = −



(cid:126)j∈[n]k

1
η

(cid:17)
(cid:16)
a + e−ηb

log

= −

1
η

(cid:17)
(cid:16)
e−ηx + (1 − e−η)a

,

log

where above the ﬁrst step is by deﬁnition of SMINC ; the second step is by partitioning the sum over (cid:126)j ∈ [n]k
into (cid:126)j such that C(cid:126)j = 0 or C(cid:126)j = 1, and then plugging in the deﬁnitions of a and b; and the third step is because
x = a + b as shown above. We conclude that Algorithm 7 correctly outputs SMINC (p, η). Since the algorithm
uses one call to the SMINC,S oracle and O(nk) additional time, the claim is proven.

An immediate consequence of Theorem 6.8 combined with our oracle reductions is that all of the candi-
date MOT algorithms described in §4 can be eﬃciently implemented for MOT problems with set-optimization
structure. From a theoretical perspective, the ELLIPSOID algorithm gives the best guarantee since it produces
an exact, sparse solution in polynomial time.

Corollary 6.9 (Polynomial-time algorithms for MOT problems with set-optimization structure). Let C ∈
(Rn)⊗k be a cost tensor that has set-optimization structure with poly(n, k) complexity (see Deﬁnition 6.5).
– Exact set-optimization structure. The ELLIPSOID algorithm in §4.1 computes an exact solution to MOTC in
poly(n, k) time. Also, the COLGEN algorithm in §4.1.3 can be run for T iterations in poly(n, k, T ) time.
– Approximate set-optimization structure. The MWU algorithm in §4.2 computes an ε-approximate solution to

MOTC in poly(n, k, Cmax/ε) time.

– Soft set-optimization structure. The SINKHORN algorithm in §4.3 computes an ε-approximate solution to

MOTC in poly(n, k, Cmax/ε) time.

Moreover, ELLIPSOID, MWU, and COLGEN output a polynomially sparse tensor, whereas SINKHORN outputs a fully
dense tensor through the implicit representation described in §4.3.1.

Proof. Combine the polynomial-time implementations of the oracles in Theorem 7.4 with the polynomial-time
algorithm-to-oracle reductions in Theorems 4.1, 4.6, 4.7, and 4.18, respectively.

6.3 Application vignette: network reliability with correlations

In this section, we illustrate this class of MOT structures via an application to network reliability, a central
topic in network science, engineering, and operations research, see e.g., the textbooks [12, 13, 41]. The basic
network reliability question is: given an undirected graph G = (V, E) where each edge e ∈ E is reliable with
some probability qe and fails with probability 1 − qe, what is the probability that all vertices are reachable
from all others? This connectivity is desirable in applications, e.g., if G is a computer cluster, the vertices are
the machines, and the edges are communication links, then connectivity corresponds to the reachability of all
machines. See the aforementioned textbooks for many other applications.

Of course, the above network reliability question is not yet well-deﬁned since the edge failures are only

prescribed up to their marginal distributions. Which joint distribution greatly impacts the answer.

The most classical setup posits that edge failures are independent [63]. Denote the network reliability proba-
bility for this setting by ρind. This quantity ρind is #P-complete [72, 85] and thus NP-hard to compute, but there
exist fully polynomial randomized approximation schemes (a.k.a. FPRAS) for multiplicatively approximating
both the connection probability ρind [52] and the failure probability 1 − ρind [45].

34

Jason M. Altschuler, Enric Boix-Adser`a

Fig. 5: Optimal decompositions for the the worst-case (top) and best-case (bottom) reliability problems on the same graph
G and edge reliability probabilities qe (left). Coordinating edge failures yields signiﬁcantly diﬀerent connection probabilities:
ρmin = 40%, ρind ≈ 60%, and ρmax = 90%.

Here we investigate the setting of coordinated edge failures, which dates back to the 1980s [89, 93]. This
coordination may optimize for disconnection (e.g., by an adversary), or for connection (e.g., maximize the time
a network is connected while performing maintenance on each edge e during 1 − qe fraction of the time). We
deﬁne these notions below; see also Figure 5 for an illustration. Below, Ber(qe) denotes the Bernoulli distribution
with parameter qe.

Deﬁnition 6.10 (Network reliability with correlations). For an undirected graph G = (V, E) and edge reliability
probabilities {qe}e∈E:

– The worst-case network reliability is

ρmin :=

min
P ∈M({Ber(qe)}e∈E )

PH∼P [H is a connected subgraph of G] .

– The best-case network reliability is

ρmax :=

max
P ∈M({Ber(qe)}e∈E )

PH∼P [H is a connected subgraph of G].

Clearly ρmin (cid:54) ρind (cid:54) ρmax. These gaps can be large (e.g., see Figure 5), which promises large opportunities
for applications in which coordination is possible. However, in order to realize such an opportunity requires
being able to compute ρmin and ρmax, and both of these problems require solving an exponentially large LP with
2|E| variables. Below we show how to use set-optimization structure to compute these quantities in poly(|E|)
time, thereby recovering as a special case of our general framework the known polynomial-time algorithms for
this particular problem in [89, 93], as well as more practical polynomial-time algorithms that scale to input
sizes that are an order-of-magnitude larger.

Corollary 6.11 (Polynomial-time algorithm for network reliability with correlations). The worst-case and
best-case network reliability can both be computed in poly(|E|) time.

Proof. By the observation in Example 6.1, the optimization problems deﬁning ρmin and 1 − ρmax are instances
of MOT in which k = |E|, n = 2, µe = Ber(qe), and each entry of the cost C ∈ {0, 1}|E| is the indicator of
whether that subset of edges is a connected or disconnected subgraph of G, respectively. It therefore suﬃces to
show that both of these MOT cost tensors satisfy exact set-optimization structure (Deﬁnition 6.5) since that
implies a polynomial-time algorithm for exactly solving MOT (Corollary 6.9).

Set-optimization structure for 1−ρmax. In this case, S is the set of connected subgraphs of G. Thus the MINC,S
problem is: given weights p ∈ R2×|E|, compute

min
connected subgraph H of G

−

(cid:88)

e∈H

p2,e −

(cid:88)

e /∈H

p1,e.

Polynomial-time algorithms for Multimarginal Optimal Transport problems with structure

35

Note that this objective is equal to (cid:80)
e∈E p1,e where xe := p1,e − p2,e. Since the latter sum is
independent of H, the MINC,S problem therefore reduces to the problem of ﬁnding a minimum-weight connected
subgraph in G; that is, given edge weights x ∈ R|E|, compute

e∈H xe − (cid:80)

min
connected subgraph H of G

(cid:88)

e∈H

xe.

(6.4)

We ﬁrst show how to solve this in polynomial time in the case that all edge weights xe are positive. In this case,
the optimal solution H is a minimum-weight spanning tree of G. This can be found by Kruskal’s algorithm in
O(|E| log |E|) time [56].

For the general case of arbitrary edge weights, note that the edges e with non-positive weight x (cid:54) 0 can be
added to any solution without worsening the cost or feasibility. Thus these edges are without loss of generality
in every solution H, and so it suﬃces to solve the same problem (6.4) on the graph G(cid:48) obtained by contracting
these non-positively-weighted edges in G. This reduces (6.4) to the same problem of ﬁnding a minimum-weight
connected subgraph, except now in the special case that all edge weights are positive. Since we have already
shown how to solve this case in polynomial time, the proof is complete.

Set-optimization structure for ρmin. In this case, S is the set of disconnected subgraphs of G. We may simplify
the MINC,S problem for ρmin by re-parameterizing the input p ∈ R2×|E| to edge weights x ∈ R|E| as done
above in (6.4) for 1 − ρmax. Thus the MINC,S problem for ρmin is: given weights x ∈ R|E|, compute

min
disconnected subgraph H of G

(cid:88)

e∈H

xe.

(6.5)

We ﬁrst show how to solve this in the case that all edge weights xe are negative. In that case, the optimal
solution is of the form H = E \ C, where C is a maximum-weight cut of the graph G with weights xe.
Equivalently, by negating all edge weights, C is a minimum-weight cut of the graph G with weights −xe. Since
a minimum-weight cut of a graph with positive weights can be found in polynomial time [81], the problem (6.5)
can be solved in polynomial time when all xe are negative.

Now in the general case of arbitrary edge weights, note that the edges e with non-negative weight x (cid:62) 0 can
be removed from any solution without worsening the cost or feasibility. Thus these edges are without loss of
generality not in every solution H, and so it suﬃces to solve the same problem (6.5) on the graph G(cid:48) obtained
by deleting these non-negatively-weighted edges in G. This reduces (6.5) to the same problem of ﬁnding a
minimum-weight disconnected subgraph, except now in the special case that all edge weights are negative.
Since we have already shown how to solve this case in polynomial time, the proof is complete.

In Figure 6, we compare the numerical performance of the algorithms in Corollary 6.11—COLGEN and MWU
with polynomial-time implementation of their bottlenecks—with the fastest previous algorithms for both best-
case and worst-case network reliability. Previously, the fastest algorithms that apply to this problem are (1)
out-of-the-box LP solvers run on MOT, (2) the brute-force implementation of SINKHORN which marginalizes
over all nk = 2|E| entries in each iteration, and (3) this COLGEN algorithm that we recover [89, 93]. It is
unknown if there is a practically eﬃcient implementation of the SMINC,S oracle (and thus of SINKHORN) for
both best-case or worst-case reliability. Since the previous algorithms (1) and (2) have exponential runtime that
scales as nΩ(k) = 2Ω(|E|), they do not scale past tiny input sizes. In contrast, the algorithms in Corollary 6.11
scale to much larger inputs. Indeed, the COLGEN algorithm that our framework recovers can compute exact
solutions roughly an order-of-magnitude faster than the other algorithms, and the new MWU algorithm computes
reasonably approximate solutions beyond k = 400, which amounts to an MOT LP with nk = 2400 ≈ 2.6 × 10120
variables.

7 Application: MOT problems with low-rank plus sparse structure

In this section, we consider MOT problems whose cost tensors C decompose into low-rank and sparse compo-
nents. We propose the ﬁrst polynomial-time algorithms for this general class of MOT problems.

The section is organized as follows. In §7.1 we formally describe this setup and discuss why it is incomparable
to all other structures discussed in this paper. In §7.2, we show that for costs with this structure, the AMIN and
SMIN oracles can be implemented in polynomial time; from this it immediately follows that MWU and SINKHORN
can be implemented in polynomial time. Finally, in §7.3 and §7.4, we provide two illustrative applications of

36

Jason M. Altschuler, Enric Boix-Adser`a

Fig. 6: Top: comparison of the runtime (left) and accuracy (right) of the algorithms described in the main text, for the worst-
case reliability of a clique graph on t vertices and k = (cid:0)t
(cid:1) edges with reliability probabilities qe = 0.99. Bottom: same, but for
2
best-case reliability and reliability probabilities qe = 0.01. For worst-case reliability, the algorithms compute an upper bound, so
a smaller value is better; reverse for best-case reliability. The algorithms are cut oﬀ at 2 minutes, denoted by an “x”. SINKHORN
is run at the highest precision (i.e., highest η) before numerical precision issues. The COLGEN algorithm that our framework
recovers computes exact solutions an order-of-magnitude faster than the other algorithms, and the new MWU algorithm computes
reasonably approximate solutions for k = 400, which amounts to an MOT LP with nk = 2400 ≈ 2.6 × 10120 variables.

these algorithms. The former regards portfolio risk management and is a direct application of our result for
MOT with low-rank cost tensors. The latter regards projecting mixture distributions to the transportation
polytope and illustrates the versality of our algorithmic results since this problem is quadratic optimization
over the transportation polytope rather than linear (a.k.a. MOT).

7.1 Setup

We begin by recalling the deﬁnition of tensor rank. It is the direct analog of the standard concept of matrix
rank. See the survey [54] for further background.
Deﬁnition 7.1 (Tensor rank). A rank-r factorization of a tensor R ∈ (Rn)⊗k is a collection of rk vectors
{ui,(cid:96)}i∈[k],(cid:96)∈[r] ⊂ Rn satisfying

R =

r
(cid:88)

k
(cid:79)

(cid:96)=1

i=1

ui,(cid:96).

The rank of a tensor is the minimal r for which there exists a rank-r factorization.

In this section we consider MOT problems with the following “low-rank plus sparse” structure.

Deﬁnition 7.2 (Low-rank plus sparse structure for MOT). An MOT cost tensor C ∈ (Rn)⊗k has low-rank
plus sparse structure of rank r and sparsity s if it decomposes as

C = R + S,

(7.1)

where R is a rank-r tensor and S is an s-sparse tensor.

050100150200250300350400edges k10-210-1100101102time (s)Worst-case reliability: runtime comparisonNaive LP solverNaive SINKHORN, η=500COLGENMWU, ε=0.02MWU, ε=0.01MWU, ε=0.005050100150200250300350400edges k0.880.900.920.940.960.98worst-case reliability boundWorst-case reliability: accuracy comparisonNaive LP solverNaive SINKHORN, η=500COLGENMWU, ε=0.02MWU, ε=0.01MWU, ε=0.005050100150200250300350400edges k10-310-210-1100101102time (s)Best-case reliability: runtime comparisonNaive LP solverNaive SINKHORN, η=500COLGENMWU, ε=0.02MWU, ε=0.01MWU, ε=0.005050100150200250300350400edges k0.020.040.060.080.100.12best-case reliability boundBest-case reliability: accuracy comparisonNaive LP solverNaive SINKHORN, η=500COLGENMWU, ε=0.02MWU, ε=0.01MWU, ε=0.005Polynomial-time algorithms for Multimarginal Optimal Transport problems with structure

37

Throughout, we make the natural assumption that S is input through its s non-zero entries, and that R is
input through a rank-r factorization. We also make the natural assumption that the entries of both R and S
are of size O(Cmax)—this rules out the case of having extremely large entries of R and S, one positive and one
negative, which cancel to yield a small entry of C = R + S.

Remark 7.3 (Neither low-rank structure nor sparse structure can be modeled by graphical structure or
set-optimization structure). In general, both rank-1 costs and polynomially sparse costs do not have non-trivial
graphical structure. Speciﬁcally, modeling these costs with graphical structure requires the complete graph (a.k.a.,
maximal treewidth of k − 1)—and because MOT problems with graphical structure of treewidth k − 1 are NP-hard
to solve in the absence of further structure [6], modeling such problems with graphical structure is useless for
the purpose of designing polynomial-time MOT algorithms. It is also clear that neither low-rank structure nor
sparse structure can be modeled by set-optimization structure because in general, neither R nor S nor R + S
has binary-valued entries.

7.2 Polynomial-time algorithms

From a technical perspective, the main result of this section is that there is a polynomial-time algorithm for
approximating the minimum entry of a tensor that decomposes into constant-rank and sparse components.
Previously, this was not known even for constant-rank tensors. This result may be of independent interest. We
remark that this result is optimal in the sense that unless P = NP, there does not exist an algorithm with
runtime that is jointly polynomial in the input size and the rank r [6].

Theorem 7.4 (Polynomial-time algorithm solving AMIN and SMIN for low-rank + sparse costs). Consider
cost tensors C ∈ (Rn)⊗k that have low-rank plus sparse structure of rank r and sparsity s (see Deﬁnition 7.2).
For any ﬁxed r, Algorithm 8 runs in poly(n, k, s, Cmax/ε) time and solves the ε-approximate AMINC oracle.
Furthermore, it also solves the SMIN ˜C oracle for η = (2k log n)/ε on some cost tensor ˜C ∈ (Rn)⊗k satisfying
(cid:107)C − ˜C(cid:107)max (cid:54) ε/2.

We make three remarks about Theorem 7.4. First, we are unaware of any polynomial-time implementation
of SMINC for the cost C. Instead, Theorem 7.4 solves the SMIN ˜C oracle for an O(ε)-approximate cost tensor
˜C since this is suﬃcient for implementing SINKHORN on the original cost tensor C (see Corollary 7.5 below).
Second, it is an interesting open question if the poly(n, k, Cmax/ε) runtime for the ε-approximate AMINC oracle
can be improved to poly(n, k, log(Cmax/ε)), as this would imply a poly(n, k) runtime for the MINC oracle and
thus for this class of MOT problems (see also Footnote 4 in the introduction). Third, we remark about practical
eﬃciency: the runtime of Algorithm 8 is not just polynomially small in s and n, but in fact linear in s and
near-linear in n. However, since this improved runtime is not needed for the theoretical results in the sequel,
we do not pursue this further.

Combining the eﬃcient oracle implementations in Theorem 7.4 with our algorithm-to-oracles reductions
in §4 implies the ﬁrst polynomial-time algorithms for MOT problems with costs that have constant-rank plus
sparse structure. This is optimal in the sense that unless P = NP, there does not exist an algorithm with
runtime that is jointly polynomial in the input size and the rank r [6].

Corollary 7.5 (Polynomial-time algorithms solving MOT for low-rank + sparse costs). Consider cost ten-
sors C ∈ (Rn)⊗k that have low-rank plus sparse structure of constant rank r and poly(n, k) sparsity s (see
Deﬁnition 7.2). For any ε > 0:
– The MWU algorithm in §4.2 computes an ε-approximate solution to MOTC in poly(n, k, Cmax/ε) time.
– The SINKHORN algorithm in §4.3 computes an ε-approximate solution to MOTC in poly(n, k, Cmax/ε) time.
Moreover, MWU outputs a polynomially sparse tensor, whereas SINKHORN outputs a fully dense tensor through the
implicit representation described in §4.3.1.

Proof. For MWU, simply combine the polynomial-time reduction to the AMINC oracle (Theorem 4.7) with the
polynomial-time algorithm for the AMIN oracle (Theorem 7.4). For SINKHORN, combining the polynomial-time
reduction to the SMIN ˜C oracle (Theorem 4.18) with the polynomial-time algorithm for the SMIN ˜C oracle
(Theorem 7.4) yields a poly(n, k, Cmax/ε) algorithm for ε/2-approximating the MOT problem with cost tensor
˜C. It therefore suﬃces to show that the values of the MOT problems with cost tensors C and ˜C diﬀer by at
most ε/2, that is,

(cid:12)
(cid:12)
(cid:12)
(cid:12)

min
P ∈M(µ1,...,µk)

(cid:104)P, C(cid:105) −

min
P ∈M(µ1,...,µk)

(cid:104)P, ˜C(cid:105)

(cid:54) ε/2.

(cid:12)
(cid:12)
(cid:12)
(cid:12)

38

Jason M. Altschuler, Enric Boix-Adser`a

But this holds because both MOT problems have the same feasible set, and for any feasible P ∈ M(µ1, . . . , µk)
it follows from H¨older’s inequality that the objectives of the two MOT problems diﬀer by at most

(cid:12)
(cid:12)(cid:104)P, C(cid:105) − (cid:104)P, ˜C(cid:105)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:54) (cid:107)P (cid:107)1(cid:107)C − ˜C(cid:107)max (cid:54) ε/2.

Below, we describe the algorithm in Theorem 7.4. Speciﬁcally, in §7.2.1, we give four helper lemmas which
form the technical core of our algorithm; and then in §7.2.2, we combine these ingredients to design the
algorithm and prove its correctness. Throughout, recall that we use the bracket notation f [A] to denote the
entrwise application of a univariate function f (e.g., exp, log, or a polynomial) to A.

7.2.1 Technical ingredients

At a high level, our approach to designing the algorithm in Theorem 7.4 is to approximately compute the SMIN
oracle in polynomial time by synthesizing four facts:

1. By expanding the softmin and performing simple operations, it suﬃces to compute the total sum of all nk

entries of the entrywise exponentiated tensor exp[−ηR] (modulo simple transforms).

2. Although exp[−ηR] is in general a full-rank tensor, we can exploit the fact that R is a low-rank tensor in
order to approximate exp[−ηR] by a low-rank tensor L. (Moreover, we can eﬃciently compute a low-rank
factorization of L in closed form.)

3. There is a simple algorithm for computing the sum of all nk entries of L in polynomial time because L is
low-rank. (And thus we may approximate the sum of all nk entries of exp[−ηR] as desired in step 1.)

4. This approximation is suﬃcient for computing both the AMIN and SMIN oracle in Theorem 7.4.

Of these four steps, the main technical step is the low-rank approximation in step two. Below, we formalize
these four steps individually in Lemmas 7.6, 7.7, 7.8, and 7.9. Further detail on how to synthesize these four
steps is then provided afterwards, in the proof of Theorem 7.4.

It is convenient to write the ﬁrst lemma in terms of an approximate tensor ˜C = ˜R + S rather than the

original cost C = R + S.

Lemma 7.6 (Softmin for cost with sparse component). Let ˜C = ˜R + S and p1, . . . , pk ∈ Rn. Then

where di := exp[ηpi] ∈ Rn

(cid:62)0,

and

˜C(cid:126)j −

sminη
(cid:126)j∈[n]k

k
(cid:88)

i=1

[pi]ji = −η−1 log(a + b),

(cid:88)

a :=

(cid:126)j∈[n]k
s.t. S(cid:126)j (cid:54)=0

k
(cid:89)

i=1

[di]ji · e−η ˜R(cid:126)j · (e−ηS(cid:126)j − 1)

b :=

(cid:88)

k
(cid:89)

(cid:126)j∈[n]k

i=1

[di]ji · e−η ˜R(cid:126)j .

(7.2)

(7.3)

Proof. By expanding the deﬁnition of softmin, and then substituting pi with di and ˜C with ˜R + S,

˜C(cid:126)j −

sminη
(cid:126)j∈[n]k

k
(cid:88)

i=1

[pi]ji = −

1
η



log



(cid:88)

eη (cid:80)k

i=1[pi]ji e−η ˜C(cid:126)j

 = −



(cid:126)j∈[n]k



log



(cid:88)

k
(cid:89)

(cid:126)j∈[n]k

i=1

1
η



[di]ji · e−η ˜R(cid:126)j e−ηS(cid:126)j

 .

Polynomial-time algorithms for Multimarginal Optimal Transport problems with structure

39

By simple manipulations, we conclude that the above quantity is equal to the desired quantity:

(cid:88)

k
(cid:89)

[di]ji · e−η ˜R(cid:126)j e−ηS(cid:126)j +

(cid:88)

k
(cid:89)

[di]ji · e−η ˜R(cid:126)j

(cid:126)j∈[n]k
s.t. S(cid:126)j (cid:54)=0

i=1

(cid:126)j∈[n]k
s.t. S(cid:126)j =0

i=1

(cid:88)

k
(cid:89)

[di]ji · e−η ˜R(cid:126)j

(cid:17)
(cid:16)
e−ηS(cid:126)j − 1

+

(cid:88)

k
(cid:89)

[di]ji · e−η ˜R(cid:126)j

i=1

(cid:126)j∈[n]k

i=1





























log

log

· · · = −

= −

= −

1
η

1
η

1
η

(cid:126)j∈[n]k
s.t. S(cid:126)j (cid:54)=0

log(a + b).

Above, the ﬁrst step is by partitioning the sum over (cid:126)j ∈ [n]k based on if S(cid:126)j = 0, the second step is by adding
and subtracting (cid:80)

i=1[di]ji · e−η ˜R(cid:126)j , and the last step is by deﬁnition of a and b.

(cid:81)k

(cid:126)j∈[n]k s.t. S(cid:126)j (cid:54)=0

Lemma 7.7 (Low-rank approximation of the exponential of a low-rank tensor). There is an algorithm that
given R ∈ (Rn)⊗k in rank-r factored form, η > 0, and a precision ˜ε < e−ηRmax , takes n · poly(k, ˜r) time to
compute a rank-˜r tensor L ∈ (Rn)⊗k in factored form satisfying (cid:107)L − exp[−ηR](cid:107)max (cid:54) ˜ε, where

˜r (cid:54)

(cid:32)

r + O(log 1
˜ε )
r

(cid:33)
.

(7.4)

Proof. By classical results from approximation theory (see, e.g., [83]), there exists a polynomial q of degree
m = O(log 1/˜ε) satisfying

|exp(−ηx) − q(x)| (cid:54) ˜ε,

∀x ∈ [−Rmax, Rmax].

For instance, the Taylor or Chebyshev expansion of x (cid:55)→ exp(−ηx) suﬃces. Thus the tensor L with entries

approximates exp[−ηR] to error

L(cid:126)j = q(R(cid:126)j)

(cid:107)L − exp[−ηR](cid:107)max (cid:54) ˜ε.

We now show that L has rank ˜r (cid:54) (cid:0)r+m

n · poly(k, ˜r) time. Denote q(x) = (cid:80)m
R, and then the Multinomial Theorem,

(cid:1), and moreover that a rank-˜r factorization can be computed in
i=1ui,(cid:96). By deﬁnition of L, deﬁnition of q and

t=0 atxt and R = (cid:80)r

(cid:96)=1 ⊗k

r

L(cid:126)j = q(R(cid:126)j) =

m
(cid:88)

t=0

at

(cid:32) r

(cid:88)

k
(cid:89)

(cid:33)t

[ui,(cid:96)]ji

=

(cid:88)

(cid:96)=1

i=1

α∈Nr

0 : |α|(cid:54)m

(cid:32)

(cid:33)

|α|
α

a|α|

r
(cid:89)

k
(cid:89)

[ui,(cid:96)]αi
ji

,

(cid:96)=1

i=1

where the sum is over r-tuples α with non-negative entries summing to at most m. Thus

L =

(cid:88)

k
(cid:79)

vi,α,

0 : |α|(cid:54)m
(cid:1)a|α|
where vi,α ∈ Rn denotes the vector with j-th entry (cid:0)|α|
This yields the desired low-rank factorization of L because

α∈Nr

α

i=1

(cid:81)r

(cid:96)=1[ui,(cid:96)]αi

j

for i = 1, and (cid:81)r

(cid:96)=1[ui,(cid:96)]αi

j

for i > 1.

˜r (cid:54) #{α ∈ Nr

0 : |α| (cid:54) m} =

(cid:32)

(cid:33)
.

r + m
r

Finally, since each of the k˜r vectors vi,α in the factorization of L can be computed eﬃciently from the closed-
form expression above, the desired runtime follows.

Lemma 7.8 (Marginalizing a scaled low-rank tensor). Given vectors d1, . . . , dk ∈ Rn and a tensor L ∈ (Rn)⊗k
through a rank ˜r factorization, we can compute m((⊗k

i=1di) (cid:12) L) in O(nk˜r) time.

40

Jason M. Altschuler, Enric Boix-Adser`a

Proof. Denote the factorization of L by L = (cid:80)˜r

(cid:96)=1 ⊗k

i=1vi,(cid:96). Then

m((⊗k

i=1di) (cid:12) L) =

(cid:88)

(cid:126)j∈[n]k

(cid:104)
(⊗k

(cid:105)
i=1di) (cid:12) L
(cid:126)j

=

(cid:88)

˜r
(cid:88)

k
(cid:89)

[di]ji [vi,(cid:96)]ji =

˜r
(cid:88)

k
(cid:89)

n
(cid:88)

[di]j[vi,(cid:96)]j =

˜r
(cid:88)

k
(cid:89)

(cid:104)di, vi,(cid:96)(cid:105),

(cid:126)j∈[n]k

(cid:96)=1

i=1

(cid:96)=1

i=1

j=1

(cid:96)=1

i=1

where the ﬁrst step is by deﬁnition of the m(·) operation that sums over all entries, the second step is by
deﬁnition of L, and the third step is by swapping products and sums. Thus computing the desired quantity
amounts to computing ˜rk inner products of n-dimensional vectors. This can be done in O(nr˜k) time.

Lemma 7.9 (Precision of the low-rank approximation). Let ε (cid:54) 1. Suppose L ∈ (Rn)⊗k satisﬁes (cid:107)L −
exp[−ηR](cid:107)max (cid:54) ε

3 e−ηRmax . Then the matrix ˜C := − 1

η log[L] + S satisﬁes

Proof. Observe that the minimum entry of L is at least

(cid:107) ˜C − C(cid:107)max (cid:54) ε
2

.

e−ηRmax − ε

3 e−ηRmax (cid:62) 2

3 e−ηRmax .

(7.5)

(7.6)

Since this is strictly positive, the tensor ˜R := −η−1 log[L] is well deﬁned. Furthermore,

(cid:107)η ˜R − ηR(cid:107)max = max
(cid:126)j∈[n]k

(cid:12)
(cid:12)η ˜R(cid:126)j − ηR(cid:126)j
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:54) max
(cid:126)j∈[n]k

(cid:12)
(cid:12)
(cid:12)L(cid:126)j − e−ηR(cid:126)j
(cid:12)
(cid:12)
(cid:12)
min(L(cid:126)j, e−ηR(cid:126)j )

(cid:54)

ε

3 e−ηRmax
2
3 e−ηRmax

=

ε
2

,

where above the ﬁrst step is by deﬁnition of the max norm; the second step is by the elementary inequality
| log x − log y| (cid:54) |x − y|/ min(x, y) which holds for positive scalars x and y [4, Lemma K]; and the third step
is by (7.6) and the approximation bound of L. Since η (cid:62) 1, we therefore conclude that (cid:107) ˜R − R(cid:107)max (cid:54) ε/2. By
adding and subtracting S, this implies (cid:107) ˜C − C(cid:107)max = (cid:107) ˜R − R(cid:107)max (cid:54) ε/2.

7.2.2 Proof of Theorem 7.4

We are now ready to state the algorithm in Theorem 7.4. Pseudocode is in Algorithm 8. Note that ˜R =
−η−1 log[L] and ˜C = ˜R + S are never explicitly computed because in both Lines 3 and 4, the algorithm
performs the relevant operations only through the low-rank tensor L and the sparse tensor S.

Algorithm 8 Polynomial-time algorithm for AMIN and SMIN for low-rank + sparse costs (Theorem 7.4).

Input: Low-rank tensor R, sparse tensor S, matrix p ∈ ¯Rn×k, accuracy ε > 0
Output: Solution to both AMINC (p, ε) on cost tensor C = R + S, and also SMIN ˜C (p, (2k log n)/ε) on some approximate

cost tensor ˜C satisfying (cid:107)C − ˜C(cid:107)max (cid:54) ε/2
1: η ← (2k log n)/ε
2: Compute low-rank approximation L of exp[−ηR] via Lemma 7.7, for precision ˜ε = ε
3: Compute a in (7.2) directly by enumerating over the polynomially many non-zero entries of S, where ˜R = −η−1 log[L]
4: Compute b in (7.3) via Lemma 7.8, where ˜R = −η−1 log[L]
5: Return −η−1 log(a + b)

3 e−ηRmax

Proof of Theorem 7.4. Proof of correctness for SMIN. Consider any oracle inputs p = (p1, . . . , pk) ∈ ¯Rn×k. By
Lemma 7.9, the tensor ˜C = ˜R + S = −η−1 log L + S satisﬁes (cid:107) ˜C − C(cid:107)max (cid:54) ε/2. Therefore it suﬃces to show
that Algorithm 8 correctly computes SMIN ˜C (p, η). This is true because that quantity is equal to −η−1 log(a+b)
by Lemma 7.6.

Proof of correctness for AMIN. We have just established that Algorithm 8 computes SMIN ˜C (p, η). Because
η = (2k log n)/ε and the fact that SMIN is a special case of AMIN (Remark 3.6), it follows that SMIN ˜C (p, η)
is within additive accuracy ε/2 of MIN ˜C (p, η). Therefore, by the triangle inequality, it suﬃces to show that
MIN ˜C (p) is within ε/2 additive accuracy of MINC (p). That is, it suﬃces to show that

(cid:12)
(cid:12)
(cid:12)
min
(cid:12)
(cid:126)j∈[n]k
(cid:12)

C(cid:126)j −

k
(cid:88)

i=1

[pi]ji − min
(cid:126)j∈[n]k

˜C(cid:126)j −

k
(cid:88)

i=1

[pi]ji

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:54) ε/2.

Polynomial-time algorithms for Multimarginal Optimal Transport problems with structure

41

But this is true because (cid:107)C − ˜C(cid:107)max (cid:54) ε/2 by Lemma 7.9, and thus the quantities C(cid:126)j − (cid:80)k
˜C(cid:126)j − (cid:80)k

i=1[pi]ji are within additive accuracy ε/2 for each (cid:126)j ∈ [n]k.

i=1[pi]ji and

Proof of runtime. We prove the claimed runtime bound simultaneously for the AMIN and SMIN computation
because we use the same algorithm for both. To this end, we ﬁrst bound the rank ˜r of the low-rank approximation
3 e−ηRmax and since it is assumed that Rmax = O(Cmax), we
L computed in Lemma 7.7. Note that since ˜ε = ε
have log 1/˜ε = O( Cmax

ε k log n). Therefore
(cid:32)

(cid:33)

˜r (cid:54)

r + O(log 1/˜ε)
r

= O(log 1/˜ε)r = O( Cmax

ε k log n)r = poly(log n, k, Cmax/ε).

Above, the ﬁrst step is by Lemma 7.7, and the ﬁnal step is because r is assumed constant.

Therefore Line 2 in Algorithm 8 takes polynomial time by Lemma 7.7, Line 3 takes polynomial time by

simply enumerating over the s non-zero entries of S, and Line 4 takes polynomial time by Lemma 7.8.

7.3 Application vignette: risk estimation

Here we consider an application to portfolio risk management. For simplicity of exposition, let us ﬁrst de-
scribe the setting of 1 ﬁnancial instrument (“stock”). Consider investing in one unit of a stock for k years. For
i ∈ {0, . . . , k}, let Xi denote the price of the stock at year i. Suppose that the return ρi = Xi/Xi−1 of the
stock between years i − 1 and i is believed to follow some distribution ρi ∼ µi. A fundamental question about
the riskiness of this stock is to compute the investor’s expected proﬁt in the worst-case over all joint proba-
bility distributions on future returns (ρ1, . . . , ρk) that are consistent with the modeled marginal distributions
(µ1, . . . , µk). This is an MOT problem with cost C given by

C(ρ1, . . . , ρk) =

(cid:89)

ρi,

i∈[k]

where here we view C as a function rather than a tensor for notational simplicity. If each return ρi has n possible
values (e.g., after quantization), then the cost C is equivalently represented as a rank-1 tensor in (Rn)⊗k (by
assigning an index to each of the n possible values of each ρi). Therefore our result Corollary 7.5 provides a
polynomial-time algorithm for solving this MOT problem deﬁning the investor’s worst-case proﬁt.

Rather than formalize this proof for 1 stock, we directly generalize to the general case of investing in r

stocks, r (cid:62) 1. This is essentially identical to the simple case of r = 1 stock, modulo additional notation.

Corollary 7.10 (Polynomial-time algorithm for expected proﬁt given marginals on the returns). Suppose an
investor holds 1 unit of r stocks for k years. For each stock (cid:96) ∈ [r] and each year i ∈ [k], let ρi,(cid:96) denote the
relative price of stock (cid:96) between years i and i − 1. Suppose ρi,(cid:96) has distribution µi,(cid:96), and that each µi,(cid:96) has
i=1 ρi,(cid:96) denote the maximal possible return. For any constant
at most n atoms. Let Rmax = max{ρi,(cid:96)}
number of stocks r, there is a poly(n, k, Rmax/ε) time algorithm for ε-approximating the expected proﬁt in the
worst-case over all futures that are consistent with the returns’ marginal distributions.

(cid:80)r

(cid:81)k

(cid:96)=1

Proof. This is the optimization problem

min
P ∈M({µi,(cid:96)}i∈[k],(cid:96)∈[r])

E{ρi,(cid:96)}i∈[k],(cid:96)∈[r]∼P

(cid:34) r

(cid:88)

k
(cid:89)

(cid:35)

ρi,(cid:96)

(cid:96)=1

i=1

over all joint distributions P on the returns {ρi,(cid:96)}i∈[k],(cid:96)∈[k] that are consistent with the marginal distibutions
{µi,(cid:96)}i∈[k],(cid:96)∈[k]. This is an MOT problem with k(cid:48) = rk marginals, each over n atoms, with cost function

(cid:16)

C

{ρi,(cid:96)}i∈[k],(cid:96)∈[r]

(cid:17)

=

(cid:88)

(cid:89)

(cid:0)ρi,(cid:96) · 1[(cid:96) = (cid:96)(cid:48)] + 1[(cid:96) (cid:54)= (cid:96)(cid:48)](cid:1).

(7.7)

(cid:96)(cid:48)∈[r]

(i,(cid:96))∈[k]×[r]∼=[k(cid:48)]

By viewing this cost function C as a cost tensor in the natural way (i.e., assigning an index to each of the
n possible values of ρi,(cid:96)), this representation (7.7) shows that the corresponding cost tensor C ∈ (Rn)⊗k(cid:48)
has rank r. Moreover, observe that the maximum entry of the cost is Rmax. Therefore we may appeal to our
polynomial-time MOT algorithms in Corollary 7.5 for costs with constant rank.

42

Jason M. Altschuler, Enric Boix-Adser`a

Fig. 7: Comparison of the runtime (left) and accuracy (right) of the fastest existing algorithms (naive LP solver and naive
SINKHORN which both have exponential runtimes that scale as nΩ(k)) with our algorithms (SINKHORN, MWU, and COLGEN and
MWU with polynomial-time implementations of their bottlenecks) for the risk estimation problem described in the main text.
The algorithms are cut oﬀ at 2 minutes, denoted by an “x”. Our new polynomial-time implementation of SINKHORN returns
high-quality solutions for problems that are orders-of-magnitude larger than previously possible: e.g., it takes less than a second
to solve the problem for k = 30, which amounts to an MOT LP with 1030 variables.

The algorithm is readily generalized, e.g., if the investor has diﬀerent units of a stock, or if a stock is held
for a diﬀerent number of years. The former is modeled simply by adding an extra year in which the return of
stock (cid:96) is equal to the number of units, with probability 1. The latter is modeled simply by setting the return
of stock (cid:96) to be 1 for all years after it is held, with probability 1.

In Figure 7, we provide a numerical illustration comparing our new polynomial-time algorithms for this
risk estimation task with the previous fastest algorithms. Previously, the fastest algorithms that apply to this
problem are out-of-the-box LP solvers run on MOT, and the brute-force implementation of SINKHORN which
marginalizes over all nk entries in each iteration. Since both of these previous algorithms have exponential
runtime that scales as nΩ(k), they do not scale beyond tiny input sizes of n = 10 and k = 8 even with two
minutes of computation time. In contrast, our new polynomial-time algorithms compute high-quality solutions
for problems that are orders-of-magnitude larger. For example, our polynomial-time implementation of SINKHORN
takes less than a second to solve an MOT LP with nk = 1030 variables.

Details for this numerical experiment: we consider r = 1 stock over k timesteps, where each marginal
distribution µi is uniform on [1, 1 + 1/k], discretized with n = 10. We implement the AMIN and SMIN oracle
eﬃciently by using our above algorithm to exploit the rank-one structure of the cost tensor. In particular, the
polynomial approximation we use here to approximate exp[−ηC] is the degree-5 Taylor approximation (cf.,
Lemma 7.7). This lets us run SINKHORN and MWU in polynomial time, as described above. In the numerical
experiment, we also implement an approximate version of COLGEN using our polynomial-time implementation
of the approximate violation oracle AMIN. Since the algorithms compute an upper bound, lower value is better
in the right plot of Figure 7. We observe that MWU yields the loosest approximation for this application, whereas
our implementations of SINKHORN and COLGEN produce high-quality approximations, as is evident by comparing
to the exact LP solver in the regime that the latter is tractable to run.

7.4 Application vignette: projection to the transportation polytope

Here we consider the fundamental problem of projecting a joint probability distribution Q onto the transporta-
tion polytope M(µ1, . . . , µk), i.e.,

argmin
P ∈M(µ1,...,µk)

(cid:88)

(cid:126)j

(P(cid:126)j − Q(cid:126)j)2.

(7.8)

We provide the ﬁrst polynomial-time algorithm for solving this problem in the case where Q is a distribution that
decomposes into a low-rank component plus a sparse component. The low-rank component enables modeling
mixtures of product distributions (e.g., mixtures of isotropic Gaussians), which arise frequently in statistics
and machine learning; see, e.g., [39]. In such applications, the number of product distributions in the mixture

51015202530time step discretization k10-210-1100101102time (s)Risk estimation: runtime comparisonNaive LP solverNaive SINKHORN, η=200SINKHORN, η=200COLGEN, with AMINMWU, ε=0.02MWU, ε=0.0151015202530time step discretization k1.481.501.521.541.561.581.601.62estimated worst-case returnRisk estimation: accuracy comparisonNaive LP solverNaive SINKHORN, η=200SINKHORN, η=200COLGEN, with AMINMWU, ε=0.02MWU, ε=0.01Polynomial-time algorithms for Multimarginal Optimal Transport problems with structure

43

corresponds to the tensor rank. The sparse component further enables modeling arbitrary corruptions to the
distribution in polynomially many entries.

We emphasize that this projection problem (7.8) is not an MOT problem since the objective is quadratic
rather than linear. This illustrates the versatility of our algorithmic results. Our algorithm is based on a
reduction from quadratic optimization to linear optimization over M(µ1, . . . , µk) that is tailored to this problem.
Crucial to this reduction is the fact that the MOT algorithms in §4 can compute sparse solutions. In particular,
this reduction does not work with SINKHORN because SINKHORN cannot compute sparse solutions.
Corollary 7.11 (Eﬃcient projection to the transportation polytope). Let Q = R + S ∈ (Rn
(cid:62)0)⊗k, where R has
constant rank and S is polynomially sparse. Suppose that Rmax and Smax are O(1). Given R in factored form, S
through its non-zero entries, measures µ1, . . . , µk ∈ ∆n, and accuracy ε > 0, we can compute in poly(n, k, 1/ε)
time a feasible P ∈ M(µ1, . . . , µk) that has ε-suboptimal cost for the projection problem (7.8). This solution P
is a sparse tensor output through its poly(n, k, 1/ε) non-zero entries.

Proof. We apply the Frank-Wolfe algorithm (a.k.a., Conditional Gradient Descent) to solve (7.8), speciﬁcally
using approximate LP solutions for the descent direction as in [51, Algorithm 2]. By the known convergence
guarantee of this algorithm [51, Theorem 1.1], if each LP is solved to ε(cid:48) = O(ε) accuracy, then T = O(1/ε)
Frank-Wolfe iterations suﬃce to obtain an ε-suboptimal solution to (7.8).

The crux, therefore, is to show that each Frank-Wolfe iteration can be computed eﬃciently, and that the
ﬁnal solution is sparse. Initialize P (0) to be an arbitrary vertex of M(µ1, . . . , µk). Then P (0) is feasible and is
polynomially sparse (see §2.1). Let P (t) ∈ (Rn
(cid:62)0)⊗k denote the t-th Frank-Wolfe iterate. Performing the next
iteration requires two computations:
1. Approximately solve the following LP to ε(cid:48) accuracy:

D(t) ←

min
P ∈M(µ1,...,µk)

(cid:104)P, P (t) − Q(cid:105).

(7.9)

2. Update P (t+1) ← (1 − γt)P (t) + γtD(t), where γt = 2/(t + 2) is the current stepsize.
For the ﬁrst iteration t = 0, note that the LP (7.9) is an MOT problem with cost C (0) = P (0) −Q = P (0) −R−S
which decomposes into a polynomially sparse tensor P (0) − S plus a constant-rank tensor −R. Therefore the
algorithm in Corollary 7.5 can solve the LP (7.9) to ε(cid:48) = O(ε) additive accuracy in poly(n, k, 1/ε) time, and it
outputs a solution D(0) that is poly(n, k, 1/ε) sparse. It follows that P (1) can be computed in poly(n, k, 1/ε)
time and moreover is poly(n, k, 1/ε) sparse since it is a convex combination of the similarly sparse tensors P (0)
and D(0). By repeating this argument identically for T = O(1/ε) iterations, it follows that each iteration takes
poly(n, k, 1/ε) time, and that each iterate P (t) is poly(n, k, 1/ε) sparse.

8 Discussion

In this paper, we investigated what structure enables MOT—an LP with nk variables—to be solved in poly(n, k)
time. We developed a uniﬁed algorithmic framework for MOT by characterizing what “structure” is required to
solve MOT in polynomial time by diﬀerent algorithms in terms of simple variants of the dual feasibility oracle.
On one hand, this enabled us to show that ELLIPSOID and MWU solve MOT in polynomial time whenever any
algorithm can, whereas SINKHORN requires strictly more structure. And on the other hand, this made the design
of polynomial-time algorithms for MOT much simpler, as we illustrated on three general classes of MOT cost
structures.

Our results suggest several natural directions for future research. One exciting direction is to identify further
tractable classes of MOT cost structures beyond the three studied in this paper, since this may enable new
applications of MOT. Our results help guide this search because they make it signiﬁcantly easier to identify if
an MOT problem is polynomial-time solvable (see §1.1.4).

Another important direction is practicality. While the focus of this paper is to characterize when MOT
problems can be solved in polynomial time, in practice there is of course a diﬀerence between small and large
polynomial runtimes. It is therefore a question of practical signiﬁcance to improve our “proof of concept”
polynomial-time algorithms by designing algorithms with smaller polynomial runtimes. Our theoretical results
help guide this search for practical algorithms because they make it signiﬁcantly easier to identify if an MOT
problem is polynomial-time solvable in the ﬁrst place.

In order to develop more practical algorithms, recall that, roughly speaking, our approach for designing

MOT algorithms consisted of three parts:

44

Jason M. Altschuler, Enric Boix-Adser`a

– An “outer loop” algorithm such as ELLIPSOID, MWU, or SINKHORN that solves MOT in polynomial time

conditionally on a polynomial-time implementation of a certain bottleneck oracle.

– An “intermediate” algorithm that reduces this bottleneck oracle to polynomial calls of a variant of the dual

feasibility oracle.

– An “inner loop” algorithm that solves the relevant variant of the dual feasibility oracle for the structured

MOT problem at hand.

Obtaining a smaller polynomial runtime for any of these three parts immediately implies smaller polynomial
runtimes for the overall MOT algorithm. Another approach is to design altogether diﬀerent algorithms that
avoid the polynomial blow-up of the runtime that arises from composing these three parts. Understanding how
to solve an MOT problem more “directly” in this way is an interesting question.

Acknowledgements We are grateful to Jonathan Niles-Weed, Pablo Parrilo, and Philippe Rigollet for insightful conversations;
to Frederic Koehler for suggesting a simpler proof of Lemma 3.7; to Ben Edelman and Siddhartha Jayanti who were involved in
the brainstorming stages and provided helpful references; and to Karthik Natarajan for references to the random combinatorial
optimization literature.

A Deferred proof details

A.1 Proof of Lemma 4.11

Our proof is based on two helper claims.

Claim A.1 (Lemma 3 of [91]). If MWU BOTTLENECK(P, µ, λ, ε) returns “null”, then MOTC (µ) > λ.

Proof. We prove the contrapositive. Let P ∗ ∈ M(µ1, . . . , µk) such that (cid:104)C, P ∗(cid:105) (cid:54) λ. Then

(cid:88)

(cid:126)j∈[n]k

P ∗
(cid:126)j

∂
∂h

Φ(P + hδ(cid:126)j ) |h=0 =

=

(cid:54)

(cid:80)

(cid:126)j∈[n]k P ∗
(cid:126)j
exp((cid:104)C, P (cid:105)/λ) + (cid:80)k

C(cid:126)j
λ exp((cid:104)C, P (cid:105)/λ) + (cid:80)k
(cid:80)n

(

i=1

1
[µi]ji

exp([mi(P )]ji /[µi]ji ))

s=1

t=1 exp([ms(P )]t/[µs]t)

(cid:104)C,P ∗(cid:105)
λ

exp((cid:104)C, P (cid:105)/λ) + (cid:80)k
s=1
exp((cid:104)C, P (cid:105)/λ) + (cid:80)k

(cid:80)n

[ms(P ∗)]t
[µs]t

t=1
(cid:80)n
t=1 exp([ms(P )]t/[µs]t)

s=1

exp([ms(P )]t/[µs]t)

exp((cid:104)C, P (cid:105)/λ) + (cid:80)k
exp((cid:104)C, P (cid:105)/λ) + (cid:80)k

s=1

s=1

(cid:80)n

(cid:80)n

t=1 exp([ms(P )]t/[µs]t)
t=1 exp([ms(P )]t/[µs]t)

= 1.

Since P ∗ is non-negative and (cid:80)

(cid:126)j∈[n]k P ∗
(cid:126)j

= m(P ∗) = 1, there must exist a (cid:126)j ∈ [n]k satisfying ∂

∂h Φ(P + hδ(cid:126)j ) |h=0(cid:54) 1.

This ﬁrst claim ensures that if the algorithm returns “infeasible”, then indeed MOTC (µ) > λ. This proves the ﬁrst part of
the lemma. We now prove a second claim, useful for bounding the running time and the quality of the returned solution when
the algorithm does not return “infeasible”.

Claim A.2 (Lemmas 1 and 4 of [91]). In Lines 1 to 5, we maintain the invariant that Φ(P ) − (1 + ε)2m(P ) (cid:54) log(nk + 1).

Proof. Note that initially P = 0 and Φ(0) − (1 + ε)2m(0) = Φ(0) = log(nk + 1). So it suﬃces to prove that Φ(P ) − (1 + ε)2m(P )
does not increase on each iteration. Indeed, if (cid:126)j is returned by MWU BOTTLENECK, then ∂
∂h Φ(P + hδ(cid:126)j ) |h=0(cid:54) (1 + ε).
Furthermore, let ε(cid:48) = ε · min(λ/C(cid:126)j , mini[µi]ji ). By the smoothness of the softmax, it holds that

Φ(P + ε(cid:48)δ(cid:126)j ) (cid:54) Φ(P ) + ε(cid:48)(1 + ε)

∂
∂h

Φ(P + hδ(cid:126)j ) |h=0(cid:54) ε(cid:48)(1 + ε)2.

So, in Line 5, (1 + ε)2m(P ) increases by (1 + ε)2ε(cid:48), and Φ(P ) increases by at most (1 + ε)2ε(cid:48). This proves the claim.

We use this second claim to bound the running time of Step 1. Each iteration of the loop from Line 1 to Line 5 of Algorithm 2,

increases the value of

Ψ (P ) := (cid:104)C, P (cid:105)/λ +

(cid:88)

i

(cid:107)mi(P )/µi(cid:107)1

by at least ε. So after T iterations we must have

Φ(P ) (cid:62) Ψ (P )/(nk + 1) (cid:62) T ε/(nk + 1),

Polynomial-time algorithms for Multimarginal Optimal Transport problems with structure

45

where we have used Jensen’s inequality to relate Φ and Ψ . By the claim, this means that on any iteration T (cid:62) (η + log(nk +
1))(nk + 1)(1 + ε)2/ε = ˜O(nk/ε2), we must have

m(P ) (cid:62) (Φ(P ) − log(nk + 1))/(1 + ε)2 (cid:62) η,

so the loop must terminate after at most ˜O(nk/ε2) iterations. Since each iteration can increase the number of non-zero entries
by at most 1, this also proves the sparsity bound on P .

We now prove the bounds on the marginals and cost, again using the claim. When Line 6 is reached, we must have

m(P ) ∈ [η, η + ε], because each iteration increases m(P ) by at most ε. Therefore,

max((cid:104)C, P (cid:105)/λ, m1(P )/µ1, . . . , mk(P )/µk) (cid:54) Φ(P )

(cid:54) log(nk + 1) + (1 + ε)2m(P )
(cid:54) log(nk + 1) + (1 + ε)2(η + ε)
(cid:54) (1 + ε)4η

by Lemma 2.3 for softmax

by the claim

by η (cid:62) 2 log(nk + 1) (cid:62) 1

Therefore, the rescaling in Line 6 yields P satisfying the guarantees of Lemma 4.11.

A.2 Proof of Lemma 4.16

It is obvious how the AMINC oracle can be implemented via a single call of the ARGAMINC oracle; we now show the converse.
Speciﬁcally, given p1, . . . , pk ∈ Rn, we show how to compute a solution (cid:126)j = (j1, . . . , jk) ∈ [n]k for ARGAMINC ([p1, . . . , pk], ε)
using nk calls to the AMINC oracle with accuracy ε/(2k). As in the proof of Lemma 4.4, we use the ﬁrst n calls to compute the
ﬁrst index j1 of the solution, the next n calls to compute the next index j2, and so on.

Formally, for s ∈ [k], let us say that (j∗

1 , . . . , j∗

solution j ∈ [n]k for ARGAMINC ([p1, . . . , pk], δ) that satisﬁes ji = j∗
i
s ∈ [k], it is possible to compute an (sε/k)-approximate partial solution (j∗
partial solution (j∗

s−1) of size s − 1 using n calls to AMINC and polynomial additional time.

s ) ∈ [n]s is a δ-approximate “partial solution” of size s if there exists a
for all i ∈ [s]. Then it suﬃces to show that for every
s ) of size s from an ((s − 1)ε/k)-approximate

1 , . . . , j∗

1 , . . . , j∗
Do this by setting j∗

s to be a minimizer of

min
j(cid:48)
s∈[n]

AMINC

(cid:16)(cid:104)

q1,j∗
1

, . . . , qs−1,j∗

s−1

, qs,j(cid:48)
s

, ps+1, . . . , pk

(cid:105)
,

(cid:17)

,

ε
2k

(A.1)

where the q vectors are deﬁned as in the proof of Lemma 4.4. The runtime claim is obvious; it suﬃces to prove correctness. To
this end, observe that

min
(cid:126)j∈[n]k

s.t. j1=j∗

1 ,...,js=j∗
s

C(cid:126)j −

k
(cid:88)

i=1

[pi]ji = MINC

(cid:16)(cid:104)

q1,j∗
1

, . . . , qs,j∗
s

, ps+1, . . . , pk

(cid:105)(cid:17)

+ min
j(cid:48)
s∈[n]

+ min
j(cid:48)
s∈[n]

+ min
j(cid:48)
s∈[n]

(cid:54)

=

(cid:54)

=

=

ε
2k
ε
2k
ε
k

ε
k

ε
k

=

sε
k

(cid:16)(cid:104)

+ AMINC

, . . . , qs,j∗
s

q1,j∗
1
(cid:16)(cid:104)

, ps+1, . . . , pk

(cid:105)
,

(cid:17)

ε
2k

AMINC

q1,j∗
1

, . . . , qs−1,j∗

s−1

, qs,j(cid:48)
s

, ps+1, . . . , pk

(cid:105)

,

(cid:17)

ε
2k

MINC

(cid:16)(cid:104)

q1,j∗
1

, . . . , qs−1,j∗

s−1

, qs,j(cid:48)
s

, ps+1, . . . , pk

(cid:105)(cid:17)

min
(cid:126)j∈[n]k
1 ,...js−1=j∗

s.t. j1=j∗

s−1,js=j(cid:48)
s

C(cid:126)j −

k
(cid:88)

i=1

[pi]ji

+

min
(cid:126)j∈[n]k
s.t. j1=j∗
1 ,...js−1=j∗
(cid:16)(cid:104)

+ MINC

p1, . . . , pk

s−1
(cid:105)(cid:17)

C(cid:126)j −

k
(cid:88)

i=1

[pi]ji

.

Above, the ﬁrst and ﬁfth steps are by Observation 4.5, the second and fourth steps are by deﬁnition of the AMIN oracle, the
third step is by construction of j∗
s−1)
being an ((s − 1)ε/k)-approximate partial solution of size s − 1. We conclude that (j∗
s ) is an (sε/k)-approximate partial
solution of size s, as desired.

s , the penultimate step is by simplifying, and the ﬁnal step is by deﬁnition of (j∗

1 , . . . , j∗

1 , . . . , j∗

B Additional numerical experiments

Here, we provide additional numerics for the generalized Euler ﬂow application in §5.3 in order to demonstrate that similar
behavior is observed on other standard benchmark inputs in the literature [26]. These instances are identical to Figure 4, except
with diﬀerent input permutations σ between the initial and ﬁnal positions of the particles. Note that our algorithm COLGEN
computes an exact, sparse solution with at most nk − k + 1 non-zero entries (Theorem 4.6). In contrast, the SINKHORN algorithm
of [14] computes approximate, fully dense solutions with nk non-zero entries, which leads to blurry visualizations.

46

Jason M. Altschuler, Enric Boix-Adser`a

t = 1

t = 2

t = 3

t = 4

t = 5

t = 6

t = 7

t = 1

t = 2

t = 3

t = 4

t = 5

t = 6

t = 7

SINKHORN

COLGEN

Fig. 8: Same as Figure 4, but now with the permutation σ that sends the particle at initial location x ∈ [0, 1] to ﬁnal location
σ(x) = min(2x, 2 − 2x). COLGEN runs in 7.88 seconds, while SINKHORN with regularization η = 2000 runs in 6.97 seconds. In the
COLGEN solution, roughly half the particles have trajectories that never split.

t = 1

t = 2

t = 3

t = 4

t = 5

t = 6

t = 7

t = 1

t = 2

t = 3

t = 4

t = 5

t = 6

t = 7

SINKHORN

COLGEN

Fig. 9: Same as Figure 4, but now with the permutation σ that sends the particle at initial location x ∈ [0, 1] to ﬁnal location
σ(x) = 1 − x. COLGEN runs in 10.53 seconds, while SINKHORN with regularization η = 1500 runs in 2.10 seconds.

References

1. S. Agrawal, Y. Ding, A. Saberi, and Y. Ye. Price of correlations in stochastic optimization. Operations Research, 60(1):150–

162, 2012.

2. M. Agueh and G. Carlier. Barycenters in the Wasserstein space. SIAM Journal on Mathematical Analysis, 43(2):904–924,

2011.

3. Y. Akagi, Y. Tanaka, T. Iwata, T. Kurashima, and H. Toda. Probabilistic optimal transport based on collective graphical

models. Preprint at arXiv:2006.08866, 2020.

4. J. Altschuler, F. Bach, A. Rudi, and J. Niles-Weed. Massively scalable Sinkhorn distances via the Nystr¨om method. In

Advances in Neural Information Processing Systems, pages 4429–4439, 2019.

5. J. Altschuler, J. Weed, and P. Rigollet. Near-linear time approximation algorithms for optimal transport via Sinkhorn

iteration. In Advances in Neural Information Processing Systems, 2017.

6. J. M. Altschuler and E. Boix-Adser`a. Hardness results for Multimarginal Optimal Transport problems. Discrete Optimiza-

tion, 42:100669, 2021.

7. J. M. Altschuler and E. Boix-Adser`a. Wasserstein barycenters can be computed in polynomial time in ﬁxed dimension.

Journal of Machine Learning Research, 22:44–1, 2021.

8. J. M. Altschuler and E. Boix-Adser`a. Wasserstein barycenters are NP-hard to compute. SIAM Journal on Mathematics

of Data Science, 2022.

9. J. M. Altschuler and P. A. Parrilo. Near-linear convergence of the Random Osborne algorithm for Matrix Balancing.

Mathematical Programming, pages 1–35, 2022.

10. E. Anderes, S. Borgwardt, and J. Miller. Discrete Wasserstein barycenters: Optimal transport for discrete data. Mathe-

matical Methods of Operations Research, 84(2):389–409, 2016.

11. F. Bach. Learning with submodular functions: a convex optimization perspective. Foundations and Trends in Machine

Learning, 6(2-3):145–373, 2013.

12. M. O. Ball. Computational complexity of network reliability analysis: An overview. IEEE Transactions on Reliability,

35(3):230–239, 1986.

13. M. O. Ball, C. J. Colbourn, and J. S. Provan. Network reliability. Handbooks in Operations Research and Management

Science, 7:673–762, 1995.

14. J.-D. Benamou, G. Carlier, M. Cuturi, L. Nenna, and G. Peyr´e. Iterative Bregman projections for regularized transportation

problems. SIAM Journal on Scientiﬁc Computing, 37(2):A1111–A1138, 2015.

15. J.-D. Benamou, G. Carlier, S. Di Marino, and L. Nenna. An entropy minimization approach to second-order variational

mean-ﬁeld games. Mathematical Models and Methods in Applied Sciences, 29(08):1553–1583, 2019.

Polynomial-time algorithms for Multimarginal Optimal Transport problems with structure

47

16. J.-D. Benamou, G. Carlier, and L. Nenna. A numerical method to solve multi-marginal optimal transport problems with
Coulomb cost. In Splitting Methods in Communication, Imaging, Science, and Engineering, pages 577–601. Springer, 2016.
17. J.-D. Benamou, G. Carlier, and L. Nenna. Generalized incompressible ﬂows, multi-marginal transport and Sinkhorn algo-

rithm. Numerische Mathematik, 142(1):33–54, 2019.

18. D. Bertsimas and J. N. Tsitsiklis. Introduction to linear optimization, volume 6. Athena Scientiﬁc Belmont, MA, 1997.
19. J. Blanchet, A. Jambulapati, C. Kent, and A. Sidford. Towards optimal running times for optimal transport. Preprint at

arXiv:1810.07717, 2018.

20. M. Blondel, V. Seguy, and A. Rolet. Smooth and sparse optimal transport.

In International conference on Artiﬁcial

Intelligence and Statistics, pages 880–889. PMLR, 2018.

21. H. L. Bodlaender. A linear-time algorithm for ﬁnding tree-decompositions of small treewidth. SIAM Journal on computing,

25(6):1305–1317, 1996.

22. H. L. Bodlaender. Treewidth: Structure and algorithms.
Communication Complexity, pages 11–25. Springer, 2007.

In International Colloquium on Structural Information and

23. Y. Brenier. The least action principle and the related concept of generalized ﬂows for incompressible perfect ﬂuids. Journal

of the American Mathematical Society, 2(2):225–255, 1989.

24. Y. Brenier. The dual least action problem for an ideal, incompressible ﬂuid. Archive for Rational Mechanics and Analysis,

122(4):323–351, 1993.

25. Y. Brenier. Minimal geodesics on groups of volume-preserving maps and generalized solutions of the Euler equations.

Communications on Pure and Applied Mathematics, 52(4):411–452, 1999.

26. Y. Brenier. Generalized solutions and hydrostatic approximation of the Euler equations. Physica D: Nonlinear Phenomena,

237(14-17):1982–1988, 2008.

27. G. Buttazzo, L. De Pascale, and P. Gori-Giorgi. Optimal-transport formulation of electronic density-functional theory.

Physical Review A, 85(6):062502, 2012.

28. G. Carlier and I. Ekeland. Matching for teams. Economic Theory, 42(2):397–418, 2010.
29. G. Carlier, A. Oberman, and E. Oudet. Numerical methods for matching for teams and Wasserstein barycenters. ESAIM:

Mathematical Modelling and Numerical Analysis, 49(6):1621–1642, 2015.

30. L. Chen, W. Ma, K. Natarajan, D. Simchi-Levi, and Z. Yan. Distributionally robust linear and discrete optimization with

marginals. Operations Research, 2022.

31. P.-A. Chiappori, R. J. McCann, and L. P. Nesheim. Hedonic price equilibria, stable matching, and optimal transport:

equivalence, topology, and uniqueness. Economic Theory, 42(2):317–354, 2010.

32. C. Cotar, G. Friesecke, and C. Kl¨uppelberg. Density functional theory and optimal transportation with Coulomb cost.

Communications on Pure and Applied Mathematics, 66(4):548–599, 2013.

33. N. Courty, R. Flamary, D. Tuia, and A. Rakotomamonjy. Optimal transport for domain adaptation. IEEE Transactions

on Pattern Analysis and Machine Intelligence, 1, 2016.

34. T. M. Cover and J. A. Thomas. Elements of information theory. John Wiley & Sons, 2012.
35. M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in Neural Information Processing

Systems, 2013.

36. M. Cuturi and A. Doucet. Fast computation of Wasserstein barycenters. In International Conference on Machine Learning,

2014.

37. M. Dyer, L. A. Goldberg, C. Greenhill, and M. Jerrum. On the relative complexity of approximate counting problems. In
International Workshop on Approximation Algorithms for Combinatorial Optimization, pages 108–119. Springer, 2000.
38. F. Elvander, I. Haasler, A. Jakobsson, and J. Karlsson. Multi-marginal optimal transport using partial information with

applications in robust localization and sensor fusion. Signal Processing, 171:107474, 2020.

39. J. Feldman, R. O’Donnell, and R. A. Servedio. Learning mixtures of product distributions over discrete domains. SIAM

Journal on Computing, 37(5):1536–1564, 2008.

40. S. Friedland. Optimal transport, distance between sets of measures and tensor scaling. Preprint at arXiv:2005.00945, 2020.
41. I. Gertsbakh and Y. Shpungin. Network reliability and resilience. Springer Science & Business Media, 2011.
42. L. A. Goldberg and M. Jerrum. The complexity of ferromagnetic Ising with local ﬁelds. Combinatorics, Probability and

Computing, 16(1):43–61, 2007.

43. M. Gr¨otschel, L. Lov´asz, and A. Schrijver. The ellipsoid method and its consequences in combinatorial optimization.

Combinatorica, 1(2):169–197, 1981.

44. M. Gr¨otschel, L. Lov´asz, and A. Schrijver. Geometric algorithms and combinatorial optimization, volume 2. Springer

Science & Business Media, 2012.

45. H. Guo and M. Jerrum. A polynomial-time approximation algorithm for all-terminal network reliability. SIAM Journal on

Computing, 48(3):964–978, 2019.

46. I. Haasler, A. Ringh, Y. Chen, and J. Karlsson. Estimating ensemble ﬂows on a hidden Markov chain. In IEEE Conference

on Decision and Control, pages 1331–1338. IEEE, 2019.

47. I. Haasler, A. Ringh, Y. Chen, and J. Karlsson. Multi-marginal optimal transport and Schr¨odinger bridges on trees. Preprint

at arXiv:2004.06909, 2020.

48. I. Haasler, A. Ringh, Y. Chen, and J. Karlsson. Scalable computation of dynamic ﬂow problems via multi-marginal graph-

structured optimal transport. Preprint at arXiv:2106.14485, 2021.

49. I. Haasler, R. Singh, Q. Zhang, J. Karlsson, and Y. Chen. Multi-marginal optimal transport and probabilistic graphical

models. IEEE Transactions on Information Theory, 2021.

50. W. K. Haneveld. Robustness against dependence in PERT: An application of duality and distributions with known

marginals. In Stochastic Programming, pages 153–182. Springer, 1986.

51. M. Jaggi. Revisiting Frank-Wolfe: Projection-free sparse convex optimization. In International Conference on Machine

Learning, pages 427–435, 2013.

52. D. R. Karger. A randomized fully polynomial time approximation scheme for the all-terminal network reliability problem.

SIAM Review, 43(3):499–522, 2001.

48

Jason M. Altschuler, Enric Boix-Adser`a

53. L. G. Khachiyan. Polynomial algorithms in linear programming. USSR Computational Mathematics and Mathematical

Physics, 20(1):53–72, 1980.

54. T. G. Kolda and B. W. Bader. Tensor decompositions and applications. SIAM Review, 51(3):455–500, 2009.
55. D. Koller and N. Friedman. Probabilistic graphical models: principles and techniques. MIT press, 2009.
56. J. B. Kruskal. On the shortest spanning subtree of a graph and the traveling salesman problem. Proceedings of the American

Mathematical society, 7(1):48–50, 1956.

57. C. L´eonard. A survey of the Sch¨rodinger problem and some of its connections with optimal transport. Preprint at

arXiv:1308.0215, 2013.

58. T. Lin, N. Ho, M. Cuturi, and M. I. Jordan. On the complexity of approximating multimarginal optimal transport. Preprint

at arXiv:1910.00152, 2019.

59. N. Linial, A. Samorodnitsky, and A. Wigderson. A deterministic strongly polynomial algorithm for matrix scaling and

approximate permanents. In Symposium on Theory of Computing, pages 644–652, 1998.

60. G. Makarov. Estimates for the distribution function of a sum of two random variables when the marginal distributions are

ﬁxed. Theory of Probability and its Applications, 26(4):803–806, 1982.

61. I. Meilijson and A. N´adas. Convex majorization with an application to the length of critical paths. Journal of Applied

Probability, 16(3):671–677, 1979.

62. V. K. Mishra, K. Natarajan, D. Padmanabhan, C.-P. Teo, and X. Li. On theoretical and empirical aspects of marginal

distribution choice models. Management Science, 60(6):1511–1531, 2014.

63. E. F. Moore and C. E. Shannon. Reliable circuits using less reliable relays. Journal of the Franklin Institute, 262(3):191–208,

1956.

64. B. Muzellec, R. Nock, G. Patrini, and F. Nielsen. Tsallis regularized optimal transport and ecological inference. In AAAI

Conference on Artiﬁcial Intelligence, volume 31, 2017.

65. A. Nadas. Probabilistic PERT. IBM Journal of Research and Development, 23(3):339–347, 1979.
66. K. Natarajan, M. Song, and C.-P. Teo. Persistency model and its applications in choice modeling. Management Science,

55(3):453–469, 2009.

67. L. Nenna. Numerical methods for multi-marginal optimal transportation. PhD thesis, 2016.
68. D. Padmanabhan, S. Damla Ahipasaoglu, A. Ramachandra, and K. Natarajan. Extremal probability bounds in combina-

torial optimization. Preprint at arXiv:2109.01591, 2021.

69. C. H. Papadimitriou and T. Roughgarden. Computing correlated equilibria in multi-player games. Journal of the ACM,

55(3):1–29, 2008.

70. G. Peyr´e and M. Cuturi. Computational optimal transport. Foundations and Trends in Machine Learning, 2017.
71. F. Piti´e, A. C. Kokaram, and R. Dahyot. Automated colour grading using colour distribution transfer. Computer Vision

and Image Understanding, 107(1-2):123–137, 2007.

72. J. S. Provan and M. O. Ball. The complexity of counting cuts and of computing the probability that a graph is connected.

SIAM Journal on Computing, 12(4):777–788, 1983.

73. K. Quanrud. Approximating optimal transport with linear programs. In Symposium on Simplicity in Algorithms, 2018.
74. J. Rabin, G. Peyr´e, J. Delon, and M. Bernot. Wasserstein barycenter and its application to texture mixing. In International

Conference on Scale Space and Variational Methods in Computer Vision, pages 435–446. Springer, 2011.

75. L. R¨uschendorf. Random variables with maximum sums. Advances in Applied Probability, pages 623–632, 1982.
76. L. R¨uschendorf and L. Uckelmann. On the n-coupling problem. Journal of Multivariate Analysis, 81(2):242–258, 2002.
77. R. Singh, I. Haasler, Q. Zhang, J. Karlsson, and Y. Chen. Incremental inference of collective graphical models. IEEE

Control Systems Letters, 2020.

78. R. Sinkhorn. Diagonal equivalence to matrices with prescribed row and column sums. The American Mathematical Monthly,

74(4):402–405, 1967.

79. J. Solomon, F. De Goes, G. Peyr´e, M. Cuturi, A. Butscher, A. Nguyen, T. Du, and L. Guibas. Convolutional Wasserstein
distances: Eﬃcient optimal transportation on geometric domains. ACM Transactions on Graphics, 34(4):1–11, 2015.
80. S. Srivastava, C. Li, and D. B. Dunson. Scalable Bayes via barycenter in Wasserstein space. Journal of Machine Learning

Research, 19(1):312–346, 2018.

81. M. Stoer and F. Wagner. A simple min-cut algorithm. Journal of the ACM, 44(4):585–591, 1997.
82. Y. W. Teh and M. Welling. The uniﬁed propagation and scaling algorithm. In Advances in Neural Information Processing

Systems, pages 953–960, 2002.

83. L. N. Trefethen. Approximation theory and approximation practice, volume 164. SIAM, 2019.
84. N. Tupitsa, P. Dvurechensky, A. Gasnikov, and C. A. Uribe. Multimarginal optimal transport by accelerated alternating

minimization. In 2020 IEEE Conference on Decision and Control, pages 6132–6137. IEEE, 2020.

85. L. G. Valiant. The complexity of enumeration and reliability problems. SIAM Journal on Computing, 8(3):410–421, 1979.
86. C. Villani. Topics in optimal transportation. Number 58. American Mathematical Society, 2003.
87. M. J. Wainwright and M. I. Jordan. Variational inference in graphical models: The view from the marginal polytope. In

Allerton Conference on Communication, Control, and Computing, volume 41, pages 961–971, 2003.

88. M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational inference. Now Publishers

Inc, 2008.

89. G. Weiss. Stochastic bounds on distributions of optimal value functions with applications to PERT, network ﬂows and

reliability. Operations Research, 34(4):595–605, 1986.

90. A. G. Wilson. The use of entropy maximising models, in the theory of trip distribution, mode split and route split. Journal

of Transport Economics and Policy, pages 108–126, 1969.

91. N. E. Young. Sequential and parallel algorithms for mixed packing and covering. In Symposium on Foundations of Computer

Science, pages 538–546. IEEE, 2001.

92. D. B. Yudin and A. S. Nemirovskii. Informational complexity and eﬃcient methods for the solution of convex extremal

problems. Matekon, 13(2):22–45, 1976.

93. E. Zemel. Polynomial algorithms for estimating network reliability. Networks, 12(4):439–452, 1982.

