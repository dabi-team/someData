1
2
0
2

r
a

M
3

]
h
p
-
t
n
a
u
q
[

1
v
8
9
4
2
0
.
3
0
1
2
:
v
i
X
r
a

Variational learning for
quantum artiï¬cial neural networks

Francesco Tacchinoâˆ—Â§Â¶, Stefano Manginiâ€ (cid:107)Â¶, Panagiotis Kl. Barkoutsosâˆ—,
Chiara Macchiavelloâ€ (cid:107)âˆ—âˆ—, Dario Geraceâ€ , Ivano Tavernelliâˆ— and Daniele Bajoniâ€¡
âˆ—IBM Quantum, IBM Research â€“ Zurich, 8803 RÂ¨uschlikon, Switzerland
â€ University of Pavia, Department of Physics, via Bassi 6, 27100 Pavia, Italy
â€¡University of Pavia, Department of Industrial and Information Engineering, via Ferrata 1, 27100 Pavia, Italy
(cid:107)INFN Sezione di Pavia, Via Bassi 6, I-27100, Pavia, Italy
âˆ—âˆ—CNR-INO - Largo E. Fermi 6, I-50125, Firenze, Italy
Â§Email: fta@zurich.ibm.com
Â¶These authors contributed equally to this work.

Abstractâ€”In the last few years, quantum computing and
machine learning fostered rapid developments in their respec-
tive areas of application, introducing new perspectives on how
information processing systems can be realized and programmed.
The rapidly growing ï¬eld of Quantum Machine Learning aims
at bringing together these two ongoing revolutions. Here we ï¬rst
review a series of recent works describing the implementation of
artiï¬cial neurons and feed-forward neural networks on quantum
processors. We then present an original realization of efï¬cient
individual quantum nodes based on variational unsampling
protocols. We investigate different learning strategies involving
global and local layer-wise cost functions, and we assess their
performances also in the presence of statistical measurement
noise. While keeping full compatibility with the overall memory-
efï¬cient feed-forward architecture, our constructions effectively
reduce the quantum circuit depth required to determine the
activation probability of single neurons upon input of the relevant
data-encoding quantum states. This suggests a viable approach
towards the use of quantum neural networks for pattern classi-
ï¬cation on near-term quantum hardware.

I. INTRODUCTION

In classical machine learning, artiï¬cial neurons and neural
networks were originally proposed, more than a half century
ago, as trainable algorithms for classiï¬cation and pattern
recognition [1], [2]. A few milestone results obtained in
subsequent years, such as the backpropagation algorithm [3]
and the Universal Approximation Theorem [4], [5], certiï¬ed
the potential of deep feed-forward neural networks as a com-
putational model which nowadays constitutes the cornerstone
of many artiï¬cial intelligence protocols [6], [7].

In recent years, several attempts were made to link these
powerful but computationally intensive applications to the
rapidly growing ï¬eld of quantum computing, see also Ref. [8]
for a useful review. The latter holds the promise to achieve
relevant advantages with respect
to classical machines al-
ready in the near term, at least on selected tasks including
e.g. chemistry calculations [9], [10], classiï¬cation and op-
timization problems [11]. Among the most relevant results
obtained in Quantum Machine Learning it is worth mentioning
the use of trainable parametrized digital and continuous-
variable quantum circuits as a model for quantum neural

networks [12]â€“[21], the realization of quantum Support Vector
Machines (qSVMs) [22] working in quantum-enhanced feature
spaces [23], [24] and the introduction of quantum versions of
artiï¬cial neuron models [25]â€“[32]. However, it is true that
very few clear statements have been made concerning the
concrete and quantitative achievement of quantum advantage
in machine learning applications, and many challenges still
need to be addressed [8], [33], [34].

In this work, we review a recently proposed quantum
algorithm implementing the activity of binary-valued artiï¬cial
neurons for classiï¬cation purposes. Although formally exact,
this algorithm in general requires quite large circuit depth
for the analysis of the input classical data. To mitigate for
this effect we introduce a variational
learning procedure,
based on quantum unsampling techniques, aimed at critically
reducing the quantum resources required for its realization. By
combining memory-efï¬cient encoding schemes and low-depth
quantum circuits for the manipulation and analysis of quantum
states,
the proposed methods, currently at an early stage
of investigation, suggest a practical route towards problem-
speciï¬c instances of quantum computational advantage in
machine learning applications.

II. A MODEL OF QUANTUM ARTIFICIAL NEURONS

The simplest formalization of an artiï¬cial neuron can be
given following the classical model proposed by McCulloch
and Pitts [1]. In this scheme, a single node receives a set of
binary inputs {i0, . . . , imâˆ’1} âˆˆ {âˆ’1, 1}m, which can either be
signals from other neurons in the network or external data. The
computational operation carried out by the artiï¬cial neuron
consists in ï¬rst weighting each input by a synapse coefï¬cient
wj âˆˆ {âˆ’1, 1} and then providing a binary output O âˆˆ {âˆ’1, 1}
denoting either an active or rest state of the node determined
by an integrate-and-ï¬re response

O =

(cid:40)

1
âˆ’1

j wjij â‰¥ Î¸

if (cid:80)
otherwise

(1)

where Î¸ represents some predeï¬ned threshold.

 
 
 
 
 
 
A quantum procedure closely mimicking the functionality
of a binary valued McCulloch-Pitts artiï¬cial neuron can be
designed by exploiting, on one hand, the superposition of com-
putational basis states in quantum registers, and on the other
hand the natural non-linear activation behavior provided by
quantum measurements. In this section, we will brieï¬‚y outline
a device-independent algorithmic procedure [28] designed to
implement such a computational model on a gate-based quan-
tum processor. More explicitly, we show how classical input
and weight vectors of size m can be encoded on a quantum
hardware by using only N = log2 m qubits [28], [35], [36].
For loading and manipulation of data, we describe a protocol
based on the generation of quantum hypergraph states [37].
This exact approach to artiï¬cial neuron operations will be used
in the main body of this work as a benchmark to assess the
performances of approximate variational techniques designed
to achieve more favorable scaling properties in the number of
logical operations with respect to classical counterparts.

Let (cid:126)i and (cid:126)w be binary input and weight vectors of the form

(cid:126)i =

ï£¶

ï£·
ï£·
ï£·
ï£¸

(cid:126)w =

ï£«

ï£¬
ï£¬
ï£¬
ï£­

i0
i1
...
imâˆ’1

ï£«

ï£¬
ï£¬
ï£¬
ï£­

w0
w1
...
wmâˆ’1

ï£¶

ï£·
ï£·
ï£·
ï£¸

(2)

with ij, wj âˆˆ {âˆ’1, 1} and m = 2N . A simple and qubit-
effective way of encoding such collections of classical data
can be given by making use of the relative quantum phases
(i.e. factors Â±1 in our binary case) in equally weighted
superpositions of computational basis states. We then deï¬ne
the states

|Ïˆi(cid:105) =

1
âˆš
m

mâˆ’1
(cid:88)

j=0

ij|j(cid:105)

|Ïˆw(cid:105) =

1
âˆš
m

mâˆ’1
(cid:88)

j=0

wj|j(cid:105)

(3)

where, as usual, we label computational basis states with
integers j âˆˆ {0, . . . , m âˆ’ 1} corresponding to the decimal
representation of the respective binary string. The set of all
possible states which can be expressed in the form above is
known as the class of hypergraph states [37].

According to Eq. (1), the quantum algorithm must ï¬rst
perform the inner product (cid:126)i Â· (cid:126)w. It is not difï¬cult to see
that, under the encoding scheme of Eq. (3), the inner product
between inputs and weights is contained in the overlap [28]

(cid:104)Ïˆw|Ïˆi(cid:105) =

(cid:126)w Â·(cid:126)i
m

(4)

We can explicitly compute such overlap on a quantum register
through a sequence of (cid:126)i- and (cid:126)w-controlled unitary operations.
First, assuming that we operate on a N-qubit quantum register
starting in the blank state |0(cid:105)âŠ—N , we can load the input-
encoding quantum state |Ïˆi(cid:105) by performing a unitary trans-
formation Ui such that

Ui|0(cid:105)âŠ—N = |Ïˆi(cid:105)

(5)

It is important to mention that this preparation step would most
effectively be replaced by, e.g., a direct call to a quantum
memory [38], or with the supply of data encoding states
readily generated in quantum form by quantum sensing devices
to be analyzed or classiï¬ed. It is indeed well known that
the interface between classical data and their representation
on quantum registers currently constitutes one of the major
bottlenecks for Quantum Machine Learning applications [8].
Let now Uw be a unitary operator such that

Uw|Ïˆw(cid:105) = |1(cid:105)âŠ—N = |m âˆ’ 1(cid:105)

(6)

In principle, any m Ã— m unitary matrix having the elements of
(cid:126)w appearing in the last row satisï¬es this condition. If we apply
Uw after Ui, the overall N -qubits quantum state becomes

Uw|Ïˆi(cid:105) =

mâˆ’1
(cid:88)

j=0

cj|j(cid:105) â‰¡ |Ï†i,w(cid:105)

Using Eq. (6), we then have

(cid:104)Ïˆw|Ïˆi(cid:105) = (cid:104)Ïˆw|U â€ 

wUw|Ïˆi(cid:105) =

= (cid:104)m âˆ’ 1|Ï†i,w(cid:105) = cmâˆ’1

(7)

(8)

We thus see that, as a consequence of the constraints imposed
to Ui and Uw, the desired result (cid:126)i Â· (cid:126)w âˆ (cid:104)Ïˆw|Ïˆi(cid:105) is contained
up to a normalization factor in the coefï¬cient cmâˆ’1 of the ï¬nal
state |Ï†i,w(cid:105).

The ï¬nal step of the algorithm must access the computed
input-weight scalar product and determine the activation state
of the artiï¬cial neuron. In view of constructing a general
architecture for feed-forward neural networks [30], it is useful
to introduce an ancilla qubit a, initially set in the state |0(cid:105), on
which the cmâˆ’1 âˆ (cid:104)Ïˆw|Ïˆi(cid:105) coefï¬cient can be written through
a multi-controlled NOT gate, where the role of controls is
assigned to the N encoding qubits [28]:

|Ï†i,w(cid:105)|0(cid:105)a â†’

mâˆ’2
(cid:88)

j=0

cj|j(cid:105)|0(cid:105)a + cmâˆ’1|m âˆ’ 1(cid:105)|1(cid:105)a

(9)

At this stage, a measurement of qubit a in the computational
basis provides a probabilistic non-linear threshold activation
behavior, producing the output |1(cid:105)a state, interpreted as an
active state of the neuron, with probability |cmâˆ’1|2. Although
this form of the activation function is already sufï¬cient to
carry out elementary classiï¬cation tasks and to realize a logical
XOR operation [28], more complex threshold behaviors can in
principle be engineered once the information about the inner
product is stored on the ancilla [27], [29]. Equivalently, the
ancilla can be used, via quantum controlled operations, to
pass on the information to other quantum registers encoding
successive layers in a feed-forward network architecture [30].
It is worth noticing that directing all the relevant information
into the state of a single qubit, besides enabling effective quan-
tum synapses, can be advantageous when implementing the
procedure on real hardware on which readout errors constitute
a major source of inaccuracy. Nevertheless, multi-controlled
NOT operations, which are inherently non-local, can lead to

complex decompositions into hardware-native gates especially
in the presence of constraints in qubit-qubit connectivity.
When operating a single node to carry out simple classiï¬cation
tasks or, as we will do in the following sections, to assess the
performances of individual portions of the proposed algorithm,
the activation probability of the artiï¬cial neuron can then
equivalently be extracted directly from the register of N
encoding qubits by performing a direct measurement of |Ï†i,w(cid:105)
targeting the |m âˆ’ 1(cid:105) â‰¡ |1(cid:105)âŠ—N computational basis state.

A. Exact implementation with quantum hypergraph states

A general and exact realization of the unitary transforma-
tions Ui and Uw can be designed by using the generation
algorithm for quantum hypergraph states [28]. The latter have
been extensively studied as useful quantum resources [37],
[39], and are formally deï¬ned as follows. Given a collection
of N vertices V , we call a k-hyper-edge any subset of exactly
k vertices. A hypergraph gâ‰¤N = {V, E} is then composed of
a set V of vertices together with a set E of hyper-edges of
any order k, not necessarily uniform. Notice that this deï¬nition
includes the usual notion of a mathematical graph if k = 2
for all (hyper)-edges. To any hypergraph gâ‰¤N we associate a
N -qubit quantum hypergraph state via the deï¬nition

N
(cid:89)

(cid:89)

|gâ‰¤N (cid:105) =

k=1

{qv1 ,...,qvk }âˆˆE

CkZqv1 ,...,qvk

|+(cid:105)âŠ—N

(10)

where qv1 , . . . , qvk are the qubits connected by a k-hyper-edge
in E and, with a little abuse of notation, we assume C2Z â‰¡ CZ
and C1Z â‰¡ Z = Rz(Ï€). For N qubits there are exactly N =
22N âˆ’1 different hypergraph states. We can make use of well
known preparation strategies for hypergraph states to realize
the unitaries Ui and Uw with at most a single N -controlled
CN Z and a collection of p-controlled CpZ gates with p < N .
It is worth pointing out already here that such an approach,
while optimizing the number of multi-qubit logic gates to be
employed, implies a circuit depth which scales linearly in the
size of the classical input, i.e. O(m) â‰¡ O(2N ), in the worst
case corresponding to a fully connected hypergraph [28].

To describe a possible implementation of Ui, assume once
again that the quantum register of N encoding qubits is ini-
tially in the blank state |0(cid:105)âŠ—N . By applying parallel Hadamard
gates (HâŠ—N ) we obtain the state |+(cid:105)âŠ—N , corresponding to
a hypergraph with no edges. We can then use the target
collection of classical inputs (cid:126)i as a control for the following
iterative procedure:

for P = 1 to N do

for j = 0 to m âˆ’ 1 do

if (|j(cid:105) has exactly P qubits in |1(cid:105) and ij = âˆ’1) then

Apply CP Z to those qubits
Flip the sign of ik in (cid:126)i âˆ€k such that |k(cid:105) has the
same P qubits in |1(cid:105)

end if
end for

end for

Similarly, Uw can be obtained by ï¬rst performing the routine
outlined obove (without the initial parallel Hadamard gates)
tailored according to the classical control (cid:126)w: since all the
gates involved in the construction are the inverse of themselves
and commute with each other, this step produces a unitary
transformation bringing |Ïˆw(cid:105) back to |+(cid:105)âŠ—N . The desired
transformation Uw is completed by adding parallel HâŠ—N and
NOTâŠ—N gates [28].

III. VARIATIONAL REALIZATION OF A QUANTUM
ARTIFICIAL NEURON

Although the implementation of the unitary transformations
Ui and Uw outlined above is formally exact and optimizes the
number of multi-qubit operations to be performed by lever-
aging on the correlations between the Â±1 phase factors, the
overall requirements in terms of circuit depth pose in general
severe limitations to their applicability in non error-corrected
quantum devices. Moreover, although with such an approach
the encoding and manipulation of classical data is performed
in an efï¬cient way with respect to memory resources, the
computational cost needed to control the execution of the
unitary transformations and to actually perform the sequences
of quantum logic gates remains bounded by the corresponding
classical limits. Therefore, the aim of this section is to explore
conditions under which some of the operations introduced in
our quantum model of artiï¬cial neurons can be obtained in
more efï¬cient ways by exploiting the natural capabilities of
quantum processors.

In the following, we will mostly concentrate on the task of
realizing approximate versions of the weight unitary Uw with
signiï¬cantly lower implementation requirements in terms of
circuit depth. Although most of the techniques that we will
introduce below could in principle work equally well for the
preparation of encoding states |Ïˆi(cid:105), it is important to stress
already at this stage that such approaches cannot be interpreted
as a way of solving the long standing issue represented by the
loading of classical data into a quantum register. Instead, they
are pursued here as an efï¬cient way of analyzing classical
or quantum data presented in the form of a quantum state.
Indeed, the variational approach proposed here requires ad-
hoc training for every choice of the target vector (cid:126)w whose Uw
needs to be realized. To this purpose, we require access to
many copies of the desired |Ïˆw(cid:105) state, essentially representing
a quantum training set for our artiï¬cial neuron. As in our
formulation a single node characterized by weight connections
(cid:126)w can be used as an elementary classiï¬er recognizing input
data sufï¬ciently close to (cid:126)w itself [28], the variational procedure
presented here essentially serves the double purpose of training
the classiï¬er upon input of positive examples |Ïˆw(cid:105) and of
ï¬nding an efï¬cient quantum realization of such state analyzer.

A. Global variational training

According to Eq. (6), the purpose of the transformation
Uw within the quantum artiï¬cial neuron implementation is
essentially to reverse the preparation of a non-trivial quantum
state |Ïˆw(cid:105) back to the relatively simple product state |1(cid:105)âŠ—N .

a

b

Fig. 1. Variational learning via unsampling. (a) Global strategy, with optimization targeting all qubits simultaneously. (b) Local qubit-by-qubit approach, in
which each layer is used to optimize the operation for one qubit at a time.

in general

the qubits in the state |Ïˆw(cid:105) share
Notice that
multipartite entanglement [39]. Here we discuss a promis-
ing strategy for the efï¬cient approximation of the desired
transformation satisfying the necessary constraints based on
variational techniques. Inspired by the well known variational
quantum eigensolver (VQE) algorithm [40], and in line with
a recently introduced unsampling protocol [41], we deï¬ne
the following optimization problem: given access to indepen-
dent copies of |Ïˆw(cid:105) and to a variational quantum circuit,
characterized by a unitary operation V ((cid:126)Î¸) and parametrized
by a set of angles (cid:126)Î¸, we wish to ï¬nd a set of values (cid:126)Î¸opt
that guarantees a good approximation of Uw. The heuristic
circuit implementation typically consists of sequential blocks
of single-qubit rotations followed by entangling gates, repeated
up to a certain number that guarantees enough freedom for the
convergence to the desired unitary [9].

Once the solution V ((cid:126)Î¸opt) is found, which in our setup
corresponds to a fully trained artiï¬cial neuron, it would then
provide a form of quantum advantage in the analysis of
arbitrary input states |Ïˆi(cid:105) as long as the circuit depth for
the implementation of the variational ansatz is sub-linear in
the dimension of the classical data, i.e. sub-exponential in the
size of the qubit register. As it is customarily done in near-
term VQE applications, the optimization landscape is explored
by combining executions of quantum circuits with classical
feedback mechanisms for the update of the (cid:126)Î¸ angles. In the
most general scenario, and according to Eq. (6), a cost function
can be deï¬ned as

F((cid:126)Î¸) = 1 âˆ’ |(cid:104)11 . . . 1|V ((cid:126)Î¸)|Ïˆw(cid:105)|2

The solution (cid:126)Î¸opt is then represented by

(cid:126)Î¸opt = arg min
(cid:126)Î¸

F((cid:126)Î¸)

(11)

(12)

and leads to V ((cid:126)Î¸opt) (cid:39) Uw. We call this approach a global
variational unsampling as the cost function in Eq. (11) requires
all qubits to be simultaneously found as close as possible to
their respective target state |1(cid:105), without making explicit use
of the product structure of the desired output state |1(cid:105)âŠ—N .
It is indeed well known that VQE can lead in general to
exponentially difï¬cult optimization problems [14], however

the characteristic feature of the problem under evaluation may
actually allow for a less complex implementation of the VQE
for unsampling purposes [41], as outlined in the following
section. A schematic representation of the global variational
training is provided in Fig. 1a.

B. Local variational training

An alternative approach to the global unsampling task,
particularly suited for the case we are considering in which the
desired ï¬nal state of the quantum register is fully unentangled,
makes use of a local, qubit-by-qubit procedure. This tech-
nique, which was recently proposed and tested on a photonic
platform as a route towards efï¬cient certiï¬cation of quantum
processors [41], is highlighted here as an additional useful tool
within a general quantum machine learning setting.
In the local variational unsampling scheme,

the global
transformation V ((cid:126)Î¸) is divided into successive layers Vj((cid:126)Î¸j)
of decreasing complexity and size. Each layer is trained
separately, in a serial fashion, according to a cost function
which only involves the ï¬delity of a single qubit to its desired
ï¬nal state. More explicitly, every Vj((cid:126)Î¸j) operates on qubits
j, . . . , N and has an associated cost function

Fj((cid:126)Î¸j) = 1 âˆ’ (cid:104)1| Trj+1,...,N [Ïj]|1(cid:105)

(13)

where the partial trace leaves only the degrees of freedom
associated to the j-th qubit and, recursively, we deï¬ne

(cid:40)

Ïj =

Vj((cid:126)Î¸j)Ïjâˆ’1V â€ 
|Ïˆw(cid:105)(cid:104)Ïˆw|

j ((cid:126)Î¸j)

j > 1
j = 0

(14)

At step j, it is implicitly assumed that all the parameters (cid:126)Î¸k
for k = 1, . . . , j âˆ’ 1 are ï¬xed to the optimal values obtained
by the minimization of the cost functions in the previous steps.
Notice that, operationally, the evaluation of the cost function
Fj can be automatically carried out by measuring the j-th
qubit in the computational basis while ignoring the rest of the
quantum register, as shown in Fig. 1b.

The beneï¬ts of local variational unsampling with respect
to the global strategy are mainly associated to the reduced
complexity of the optimization landscape per step. Indeed,
the local version always operates on the overlap between

|ğœ“!âŸ©ğ‘‰âƒ—ğœƒâ„±âƒ—ğœƒ|ğœ“!âŸ©ğ‘‰"âƒ—ğœƒ"â„±!âƒ—ğœƒ!ğ‘‰#âƒ—ğœƒ#â„±"âƒ—ğœƒ"â€¦â€¦â€¦nâˆ’timesnâ€²âˆ’timesnâ€²âˆ’timesâ„°â„›âƒ—ğœƒ)â„°â„›(âƒ—ğœƒ)â„›âƒ—ğœƒ*â„°a

b

c

Fig. 2. Comparison of output activation pout = |(cid:104)Ïˆw|Ïˆi(cid:105)|2 among the exact (hypergraph states routine), global (n = 3) and local (n(cid:48) = 2) approximate
implementations of Uw. The inset shows the general mapping of any 16-dimensional binary vector (cid:126)b onto the 4 Ã— 4 binary image (b) and the cross-shaped
(cid:126)w used in this example (c). The selected inputs on which the approximations are tested were chosen to cover all the possible cases for pout, and are labeled
with their corresponding integer ki (see main text).

single-qubit states, at the relatively modest cost of adding
N âˆ’ 1 smaller and smaller variational ansatzes. In the speciï¬c
problem at study, we thus envision the local approach to
become particularly effective, and more advantageous than the
global one, in the limit of large enough number of qubits, i.e.
for the most interesting regime where the size of the quantum
register, and therefore of the quantum computation, exceeds
the current classical simulation capabilities.

C. Case study: pattern recognition

To show an explicit example of the proposed construction,
let us ï¬x m = 16, N = 4. Following Ref. [28], we can
visualize a 16-bit binary vector (cid:126)b, see Eq. (2), as a 4 Ã— 4
binary pattern of black (bj = âˆ’1) and white (bj = 1) pixels.
Moreover, we can assign to every possible pattern an integer
label kb corresponding to the conversion of the binary string
kb = b0 . . . b15, where bj = (âˆ’1)bj . We choose as our target
(cid:126)w the vector corresponding to kw = 20032, which represents
a black cross on white background at the north-west corner of
the 16-bit image, see Fig 2.

Starting from the global variational strategy, we construct
a parametrized ansatz for V ((cid:126)Î¸) as a series of entangling (E)
and rotation (R((cid:126)Î¸)) cycles:

V ((cid:126)Î¸) =

(cid:32) n
(cid:89)

c=1

(cid:33)

R(Î¸c,1 . . . Î¸c,4)E

R(Î¸0,1 . . . Î¸0,4)

(15)

increasing its total depth. Rotations are assumed to be acting
independently on the N = 4 qubits according to

R(Î¸c,1 . . . Î¸c,4) =

4
(cid:79)

q=1

(cid:18)

exp

âˆ’i

(cid:19)

Î¸c,q
2

Ïƒ(q)
y

(16)

where Ïƒ(q)
is the Pauli y-matrix acting on qubit q. At the
y
same time, the entangling parts promote all-to-all interactions
between the qubits according to

(cid:89)

4
(cid:89)

E =

q

q(cid:48)=q+1

CNOTqq(cid:48)

(17)

where CNOTqq(cid:48) is the usual controlled NOT operation be-
tween control qubit q and target q(cid:48) acting on the space of all
4-qubits. For n cycles, the total number nÎ¸ of Î¸-parameters,
including the initial rotation layer R(Î¸0,1 . . . Î¸0,4), is therefore
nÎ¸ = 4 + 4n.

A qubit-by-qubit version of the ansatz can be constructed
in a similar way by using the same structure of entangling
and rotation cycles, decreasing the total number of qubits by
one after each layer of the optimization. Here we choose a
uniform number n(cid:48) of cycles per qubit (this condition will be
relaxed afterwards, see Sec. III-D), thus setting âˆ€j (cid:54)= 4

ï£¶

R(Î¸c,j . . . Î¸c,4)E

ï£¸ R(Î¸0,j . . . Î¸0,4)

(18)

ï£«

ï£­

n(cid:48)
(cid:89)

c=1

where n is the total number of cycles which in principle
can be varied to increase the expressibility of the ansatz by

Vj((cid:126)Î¸j) =

ğ’˜For j = 4, we add a single general single-qubit rotation with
three parameters

V4(Î±, Î², Î³) = exp

âˆ’i

(cid:18)

(Î±, Î², Î³) Â· (cid:126)Ïƒ(4)
2

(cid:19)

(19)

where (cid:126)Ïƒ = (Ïƒx, Ïƒy, Ïƒz) are again the usual Pauli matrices.

We implemented both versions of the variational training
in Qiskit [42], combining exact simulation of the quantum
circuits required to evaluate the cost function with classical
Nelder-Mead [43] and Cobyla [44] optimizers from the scipy
Python library. We ï¬nd that the values n = 3 and n(cid:48) = 2 allow
the routine to reach total ï¬delities to the target state |1(cid:105)âŠ—N well
above 99.99%. As shown in Fig 2, this in turn guarantees a
correct reproduction of the exact activation probabilities of the
quantum artiï¬cial neuron with a quantum circuit depth of 19
(29) for the global (qubit-by-qubit) strategy, as compared to
the total depth equal to 49 for the exact implementation of
Uw using hypergraph states. This counting does not include
the gate operations required to prepare the input state, i.e.
it only evidences the different realizations of the Uw imple-
mentation assuming that each |Ïˆi(cid:105) is provided already in the
form of a wavefunction. Moreover, the multi-controlled CP Z
operations appearing in the exact version were decomposed
into single-qubit rotations and CNOTs without the use of
additional working qubits. Notice that these conditions are the
ones usually met in real near-term superconducting hardware
endowed with a ï¬xed set of universal operations.

D. Structure of the ansatz and scaling properties

In many practical applications, the implementation of the
entangling block E could prove technically challenging, in
particular for near term quantum devices based, e.g., on
superconducting wiring technology, for which the available
connectivity between qubits is limited. For this reason,
it
is useful
to consider a more hardware-friendly entangling
scheme, which we refer to as nearest neighbours. In this case,
each qubit is entangled only with at most two other qubits,
essentially assuming the topology of a linear chain

Enn =

3
(cid:89)

q=1

CNOTq,q+1

(20)

This scheme may require even fewer two-qubit gates to be
implemented with respect to the all-to-all scheme presented
above. Moreover, this entangling unitary ï¬ts perfectly well on
those quantum processors consisting of linear chains of qubits
or heavy hexagonal layouts.

We implemented both global and local variational learning
procedures with nearest neighbours entanglers in Qiskit [42],
using exact simulation of the quantum circuits with classical
optimizers to drive the learning procedure. In the following,
we report an extensive analysis of the performances and a
comparison with the all-to-all strategy introduced in Sec. III-C
above. All the simulations are performed by assuming the
same cross-shaped target weight vector (cid:126)w depicted in Fig. 2.
In Figure 3 we show an example of the typical optimization
procedure for three different choices of the ansatz depth (i.e.

Fig. 3. Optimization of the global unitary with nearest neighbours entan-
glement for three different structures differing in the numbers of entangling
blocks n. The cost function is |(cid:104)11 . . . 1|V ((cid:126)Î¸)|Ïˆw(cid:105)|2 = 1 âˆ’ F((cid:126)Î¸), see
Eq. (11). Only for n = 3 the learning model has enough expressibility to
reach a good ï¬nal ï¬delity. The classical optimizer used in this case was
COBYLA [44].

to implement

Fig. 4. Final ï¬delity obtained for the local variational training and using both
the all-to-all entangler E (17) and nearest neighbour Enn (20). On top of each
rectangle, in light blue, it is reported the depth of the corresponding quantum
circuit
that given structure with that particular entangling
scheme. For clarity, a structure â€˜211â€™ corresponds to a variational model
having two repetitions (n(cid:48)
1 = 2) for the ï¬rst layer acting on all 4 qubits,
and 1 cycle (n(cid:48)
2 = 1) for the remaining two layers acting on 3 and 2
qubits respectively. Each bar was obtained executing the optimization process
10 times, and then evaluating the means and standard deviations (shown as
error bars). The optimization procedure was performed using COBYLA [44].

1 = n(cid:48)

number of entangling cycles) n = 1, 2, 3, assuming a global
cost function. Here we ï¬nd that n = 3 allows the routine to
reach a ï¬delity F((cid:126)Î¸) to the target state |1(cid:105)âŠ—N above 99%.

In the local qubit-by-qubit variational scheme, we can
actually introduce an additional degree of freedom by allowing
the number of cycles per qubit, n(cid:48), to vary between successive
layers corresponding to the different stages of the optimization
procedure. For example, we may want to use a deeper ansatz
for the ï¬rst unitary acting on all the qubits, and shallower
ones for smaller subsystems. We thus introduce a different n(cid:48)
j
for each Vj((cid:126)Î¸j) in Eq. (18) and we name structure the string

050100150200250Number of function evaluations (COBYLA)0.00.20.40.60.81.0Cost Function1 layer2 layers3 layers111211221222321322333Structure0.00.20.40.60.81.0Fidelity 1316192122242916212527303238Nearest neighbourAll to allFig. 5. Final ï¬delities for different structures of the local variational learning
model with a nearest neighbour entangler, for the case of N = 5 qubits.
Similarly to the case with N = 4 qubits portrayed in Figure 4, the most
depth-efï¬cient structure is the one consisting of constantly decreasing number
of cycles.

Fig. 6. Number of iterations of the classical optimizer to reach a ï¬delity of
F = 95%. Each point in the plot is obtained by running the optimization
procedure 10 times and then evaluating the mean and standard deviation
(shown as error bars in the plot). All results refer to exact simulations of
the quantum circuits in the absence of statistical measurement sampling or
device noise, performed with Qiskit statevector_simulator.

â€˜n1n2n3â€™. The latter denotes a learning model consisting of
three optimization layers: V1((cid:126)Î¸1) with n1 entangling cycles,
V2((cid:126)Î¸2) with n2 and V3((cid:126)Î¸3) with n3, respectively. In the last
step of the local optimization procedure, i.e. when a single
qubit is involved, we always assume a single 3-parameter
rotation, see Eq. (19). A similar notation will be also applied
in the following when scaling up to N > 4 qubits.

The effectiveness of different structures is explored in Fig-
ure 4. We see that, while the all-to-all entangling scheme typ-
ically performs better in comparison to the nearest neighbour
one, this increase in performance comes at the cost of deeper
circuits. Moreover, the stepwise decreasing structure â€˜321â€™
for the nearest neighbour entangler proves to be an effective
solution to problem, achieving a good ï¬nal accuracy (above
99%) with a low circuit depth. This trend is also conï¬rmed
for the higher dimensional case of N = 5 qubits, which
we report in Fig. 5. Here, the dimension of the underlying
pattern recognition task is increased by extending the original
16-bit weight vector (cid:126)w with extra 0s in front of the binary
representation kw. In fact, it can easily be seen that, assuming
directly nearest neighbours entangling blocks, the decreasing
structure â€˜4321â€™ gives the best performance-depth tradeoff.
Such empirical fact, namely that the most efï¬cient structure
is typically the one consisting of decreasing depths, can be
heuristically interpreted by recalling again that, in general, the
optimization of a function depending on the state of a large
number of qubits is a hard training problem [14]. Although
we employ local cost functions, to complete our particular
task each variational layer needs to successfully disentangle a
single qubit from all the others still present in the register. It is
therefore not totally surprising that the optimization carried out
in larger subsystems requires more repetitions and parameters
(i.e. larger n(cid:48)
j) in order to make the ansatz more expressive.
By assuming that the stepwise decreasing structure remains
sufï¬ciently good also for larger numbers of qubits, we studied

the optimization landscape of global, Eq. (11), and local,
Eq. (13), cost functions by investigating how the hardness
of the training procedure scales with increasing N . As com-
mented above for N = 5, we keep the same underlying target
(cid:126)w, which we expand by appending extra 0s in the binary
representation. To account for the stochastic nature of the
optimization procedure, we run many simulations of the same
learning task and report the mean number of iterations needed
for the classical optimizer to reach a given target ï¬delity
F = 95%. Results are shown in Figure 6. The most signiï¬cant
the use of the aforementioned local cost
message is that
function seems to require higher classical resources to reach
a given target ï¬delity when the number of qubits increases.
This actually should not come as a surprise, since the number
of parameters to be optimized in the two cases is different.
In fact, in the global scenarios there are N + N Â· n (the ï¬rst
N is due to the initial layer of rotations) to be optimized,
while in the local case there are N + N Â· n(cid:48)
1 for the ï¬rst layer,
(N âˆ’ 1) + (N âˆ’ 1) Â· n(cid:48)

2 for the second, ...; for a total of

#local =

N
(cid:88)

q=2

q + qn(cid:48)

q + 3

(21)

where the ï¬nal 3 is due to the fact that the last layer always
consist of a rotation on the Bloch sphere with three parameters,
see Eq. (19). Using the stepwise decreasing structure, that
q = q âˆ’ 1, we eventually obtain (cid:80)N
is n(cid:48)
q=2 q + q(q âˆ’ 1) =
(cid:80)N
q=2 q2 âˆ¼ O(N 3), compared to #global âˆ¼ O(N 2). Here we
are assuming a number of layers n = N âˆ’ 1, consistently with
the N = 4 qubits case (see Figure 3). While in the global case
the optimization makes full use of the available parameters to
globally optimize the state towards |1(cid:105)âŠ—N , the local unitary
has to go through multiple disentangling stages, requiring (at
least for the cases presented here) more classical iteration
steps. At the same time, it would probably be interesting to

1111222242213322432133334332Structure0.00.20.40.60.81.0Fidelity 19303436374142Depth45678Number of qubits025050075010001250150017502000Mean Iteration for =0.95LocalGlobala

b

Fig. 7. Optimization of cost functions for the local (a) and global (b) case in
the presence of measurement noise for N = 5 qubits. In each ï¬gure we plot
the mean values averaged on 5 runs of the simulation. The shaded colored
areas denote one standard deviation. The number of measurement repetitions
in each simulation was 1024. The ï¬nal ï¬delity at the end of the training
procedure in this case were Flocal = 0.87 Â± 0.02 and Fglobal = 0.89 Â± 0.02.
Notice the difference in the horizontal axes bounds. (a) Optimization of the
local cost functions Vj ((cid:126)Î¸j ) (see Eq. (13)), plotted with different colors for
clarity. The vertical dashed lines denotes the end of the optimization of one
layer, and the start of the optimization for the following one. (b) Optimization
of the global cost function V ((cid:126)Î¸) in Eq. (11)
.

investigate other examples in which the number of parameters
between the two alternative schemes remains ï¬xed, as this
would most likely narrow the differences and provide a more
direct comparison.

In agreement with similar investigations [45], we can ac-
tually conclude that only modest differences between global
and local layer-wise optimization approaches are present when
dealing with exact simulations (i.e. free from statistical and
hardware noise) of the quantum circuit. Indeed, both strategies
achieve good results and a ï¬nal ï¬delity F((cid:126)Î¸) > 99%. At
the same time,
it becomes interesting to investigate how
the different approaches behave in the presence of noise,
and speciï¬cally statistical noise coming from measurements
operations. For this reason, we implemented the measurement
sampling using the Qiskit qasm_simulator and employed
a stochastic gradient descent (SPSA) classical optimization
method. Each benchmark circuit is executed nshots = 1024
times in order to reconstruct the statistics of the outcomes.
Moreover, we repeat the stochastic optimization routine multi-
ple times to analyze the average behaviour of the cost function.
In Figure 7 we show the optimization procedure for the local
and global cost functions in the presence of measurement
noise, with both of them reaching acceptable and identical ï¬nal
ï¬delities Flocal = 0.87Â±0.02 and Fglobal = 0.89Â±0.02. Notice
that for the local case (Figure 7(a)) each colored line indicates
the optimization of a Vj((cid:126)Î¸j) from Eq. (13). We observe that the
training for the local model generally requires fewer iterations,
with an effective optimization of each single layer. On the
contrary, in the presence of measurement noise the global

Fig. 8. Scaling of circuit depth for the implementation of Uw computed with
Qiskit. The labels locals and global refer to the local and global variational
approaches, while a2a and nn refer to the all-to-all and nearest-neighbour
entangling schemes respectively. The number of ansatz cycles used for both
the global (n) and local/qubit-by-qubit (n(cid:48)) variational constructions and for
each entangling sctructure are increased with the number of qubits up to the
minimum value guaranteeing a ï¬delity of the approximations above 98%.

variational training struggles to ï¬nd a good direction for the
optimization and eventually follows a slowly decreasing path
to the minimum. These ï¬ndings look to be in agreement, e.g.,
with results from Refs. [45], [46]: with the introduction of
statistical shot noise, the performances of the global model are
heavily affected, while the local approach proves to be more
resilient and capable of ï¬nding a good gradient direction in the
parameters space [46]. In all these simulations, the parameters
in the global unitary and in the ï¬rst layer of the local unitary
were initialized with a random distribution in [0, 2Ï€). All
subsequent layers in the local model were initialized with all
parameters set to zero in order to allow for smooth transitions
from one optimization layer to the following. This strategy was
actually suggested as a possible way to mitigate the occurence
Barren plateaus [45], [47].

We conclude the scaling analysis by reporting in Fig. 8 a
summary of the quantum circuit depths required to implement
the target unitary transformation with different strategies and
for increasing sizes of the qubit register up to N = 7. As it can
be seen, all the variational approaches scale much better when
compared to the exact implementation of the target Uw, with
the global ones requiring shallower depths in the speciï¬c case.
In addition, we recall that the use of an all-to-all entangling
scheme requires longer circuits due to the implementation of
all the CNOTs, but generally needs less ansatz cycles (see
Figure 4). At last, while the global procedures seem to provide
a better alternative compared to local ones in terms of circuit
depth, they might be more prone to suffering from classical
optimization issues [14], [45] when trained and executed on
real hardware, as suggested by the data reported in Fig. 7.
The overall promising results conï¬rm the signiï¬cant advantage
brought by variational strategies compared to the exponential
increase of complexity required by the exact formulation of
the algorithm.

01000200030004000500060000.00.20.40.6CostFunctionsLayer0Layer1Layer2Layer3Layer40200040006000800010000SPSAIteration0.20.40.60.81.0CostFunctionMeancostfunction1-Ïƒrange4567Number of qubits0100200300400500600Circuit depthLocal, a2aLocal, nnGlobal, a2aGlobal, nnExactIV. CONCLUSIONS
In this work, we reviewed an exact model for the im-
plementation of artiï¬cial neurons on a quantum processor
and we introduced variational training methods for efï¬ciently
handling the manipulation of classical and quantum input
data. Through extensive numerical analysis, we compared
the effectiveness of different circuit structures and learning
strategies, highlighting potential beneï¬ts brought by hardware-
compatible entangling operations and by layerwise training
routines. Our work suggests that quantum unsampling tech-
niques represent a useful resource, upon input of quantum
training sets, to be integrated in quantum machine learning
applications. From a theoretical perspective, our proposed
procedure allows for an explicit and direct quantiï¬cation of
possible quantum computational advantages for classiï¬cation
tasks. It is also worth pointing out that such a scheme re-
mains fully compatible with recently introduced architectures
for quantum feed-forward neural networks [30], which are
needed in general to deploy e.g. complex convolutional ï¬lters.
Moreover, although the interpretation of quantum hypergraph
states as memory-efï¬cient carriers of classical information
guarantees an optimal use of the available dimension of a N -
qubit Hilbert space, the variational techniques introduced here
can in principle be used to learn different encoding schemes
designed, e.g., to include continuous-valued features or to
improve the separability of the data to be classiï¬ed [23], [24],
[48]. In all envisioned applications, our proposed protocols are
intended as an effective method for the analysis of quantum
states as provided, e.g., by external devices or sensors, while
it is worth stressing that the general problem of efï¬ciently
loading classical data into quantum registers still stands open.
Finally, on a more practical level, a successful implementation
on near-term quantum hardware of the variational learning
algorithm introduced in this work will necessarily rely on a
deeper analysis of the impact of realistic noise effects both
on the training procedure and on the ï¬nal optimized circuit.
In particular, we anticipate that
the reduced circuit depth
produced via the proposed method could critically lessen the
quality requirements for quantum hardware, eventually leading
to meaningful implementation of quantum neural networks
within the near-term regime.

ACKNOWLEDGMENTS
A preliminary version of this work was presented at the
2020 IEEE International Conference on Quantum Computing
and Engineering [49]. We acknowledge support from SNF
grant 200021 179312. IBM,
the IBM logo, and ibm.com
are trademarks of International Business Machines Corp.,
registered in many jurisdictions worldwide. Other product
and service names might be trademarks of IBM or other
companies. The current list of IBM trademarks is available
at https://www.ibm.com/legal/copytrade.

REFERENCES

[1] W. S. McCulloch and W. Pitts, â€œA logical calculus of the ideas immanent
in nervous activity,â€ The bulletin of mathematical biophysics, vol. 5,
no. 4, pp. 115â€“133, 1943.

[2] F. Rosenblatt, â€œThe Perceptron: A perceiving and recognizing automa-
ton,â€ Cornell Areonautical Laboratory, Inc., Tech. Rep. 85-460-1, 1957.
[3] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, â€œLearning repre-
sentations by back-propagating errors,â€ Nature, vol. 323, no. 6088, pp.
533â€“536, 1986.

[4] G. Cybenko, â€œApproximation by superpositions of a sigmoidal function,â€
Mathematics of Control, Signals and Systems, vol. 2, no. 4, pp. 303â€“314,
1989.

[5] K. Hornik, â€œApproximation capabilities of multilayer feedforward net-

works,â€ Neural Networks, vol. 4, no. 2, pp. 251â€“257, 1991.

[6] J. M. Zurada, Introduction to Artiï¬cial Neural Systems. West Group,

1992.

[7] R. Rojas, Neural Networks: A Systematic Introduction. Springer, 1996.
[8] J. Biamonte, P. Wittek, N. Pancotti, P. Rebentrost, N. Wiebe, and
S. Lloyd, â€œQuantum machine learning,â€ Nature, vol. 549, no. 7671, pp.
195â€“202, 2017.

[9] A. Kandala, A. Mezzacapo, K. Temme, M. Takita, M. Brink, J. M.
Chow, and J. M. Gambetta, â€œHardware-efï¬cient variational quantum
eigensolver for small molecules and quantum magnets,â€ Nature, vol.
549, no. 7671, pp. 242â€“246, 2017.

[10] Y. Cao, J. Romero, J. P. Olson, M. Degroote, P. D. Johnson,
M. KieferovÂ´a, I. D. Kivlichan, T. Menke, B. Peropadre, N. P. D. Sawaya,
S. Sim, L. Veis, and A. Aspuru-Guzik, â€œQuantum Chemistry in the
Age of Quantum Computing,â€ Chemical Reviews, vol. 119, no. 19, pp.
10 856â€“10 915, 2019.

[11] N. Moll, P. Barkoutsos, L. S. Bishop, J. M. Chow, A. Cross, D. J.
Egger, S. Filipp, A. Fuhrer, J. M. Gambetta, M. Ganzhorn, A. Kandala,
A. Mezzacapo, P. MÂ¨uller, W. Riess, G. Salis, J. Smolin, I. Tavernelli,
and K. Temme, â€œQuantum optimization using variational algorithms on
near-term quantum devices,â€ Quantum Science and Technology, vol. 3,
no. 3, p. 030503, 2018.

[12] E. Farhi and H. Neven, â€œClassiï¬cation with Quantum Neural Networks

on Near Term Processors,â€ arXiv:1802.06002, 2018.

[13] E. Grant, M. Benedetti, S. Cao, A. Hallam, J. Lockhart, V. Stojevic,
A. G. Green, and S. Severini, â€œHierarchical quantum classiï¬ers,â€ npj
Quantum Information, vol. 4, no. 1, p. 65, 2018.

[14] J. R. McClean, S. Boixo, V. N. Smelyanskiy, R. Babbush, and H. Neven,
â€œBarren plateaus in quantum neural network training landscapes,â€ Nature
Communications, vol. 9, no. 1, p. 4812, 2018.

[15] N. Killoran, T. R. Bromley, J. M. Arrazola, M. Schuld, N. Quesada, and
S. Lloyd, â€œContinuous-variable quantum neural networks,â€ Phys. Rev.
Research, vol. 1, p. 033063, 2019.

[16] M. Benedetti, E. Lloyd, S. Sack, and M. Fiorentini, â€œParameterized
quantum circuits as machine learning models,â€ Quantum Science and
Technology, vol. 4, no. 4, p. 043001, 2019.

[17] A. Mari, T. R. Bromley, J. Izaac, M. Schuld, and N. Killoran, â€œTransfer
learning in hybrid classical-quantum neural networks,â€ Quantum, vol. 4,
p. 340, 2020.

[18] I. Cong, S. Choi, and M. D. Lukin, â€œQuantum convolutional neural

networks,â€ Nature Physics, vol. 15, pp. 1273â€“1278, 2019.

[19] M. Schuld, A. Bocharov, K. M. Svore, and N. Wiebe, â€œCircuit-centric

quantum classiï¬ers,â€ Phys. Rev. A, vol. 101, p. 032308, 2020.

[20] M. Henderson, S. Shakya, S. Pradhan, and T. Cook, â€œQuanvolutional
neural networks: powering image recognition with quantum circuits,â€
Quantum Machine Intelligence, vol. 2, no. 2, 2020.

[21] M. Broughton, G. Verdon, T. McCourt, A. J. Martinez, J. H. Yoo, S. V.
Isakov, P. Massey, M. Y. Niu, R. Halavati, E. Peters, M. Leib, A. Skolik,
M. Streif, D. Von Dollen, J. R. McClean, S. Boixo, D. Bacon, A. K.
Ho, H. Neven, and M. Mohseni, â€œTensorFlow Quantum: A Software
Framework for Quantum Machine Learning,â€ arXiv:2003.02989, 2020.
[22] P. Rebentrost, M. Mohseni, and S. Lloyd, â€œQuantum Support Vector
Machine for Big Data Classiï¬cation,â€ Physical Review Letters, vol. 113,
no. 13, p. 130503, 2014.

[23] V. HavlÂ´Ä±Ë‡cek, A. D. CÂ´orcoles, K. Temme, A. W. Harrow, A. Kandala,
J. M. Chow, and J. M. Gambetta, â€œSupervised learning with quantum-
enhanced feature spaces,â€ Nature, vol. 567, no. 7747, pp. 209â€“212, 2019.
[24] M. Schuld and N. Killoran, â€œQuantum Machine Learning in Feature
Hilbert Spaces,â€ Physical Review Letters, vol. 122, no. 4, p. 040504,
2019.

[25] M. Schuld, I. Sinayskiy, and F. Petruccione, â€œSimulating a perceptron on
a quantum computer,â€ Physics Letters A, vol. 379, no. 7, pp. 660â€“663,
2015.

[26] N. Wiebe, A. Kapoor, and K. M. Svore, â€œQuantum Perceptron Models,â€

arXiv:1602.04799, 2016.

[27] Y. Cao, G. G. Guerreschi, and A. Aspuru-Guzik, â€œQuantum Neuron: an
elementary building block for machine learning on quantum computers,â€
arXiv:1711.11240, 2017.

[28] F. Tacchino, C. Macchiavello, D. Gerace, and D. Bajoni, â€œAn artiï¬cial
neuron implemented on an actual quantum processor,â€ npj Quantum
Information, vol. 5, p. 26, 2019.

[29] E. Torrontegui and J. J. Garcia-Ripoll, â€œUnitary quantum perceptron as
efï¬cient universal approximator,â€ EPL (Europhysics Letters), vol. 125,
no. 3, p. 30004, 2019.

[30] F. Tacchino, P. Barkoutsos, C. Macchiavello, I. Tavernelli, D. Gerace,
and D. Bajoni, â€œQuantum implementation of an artiï¬cial feed-forward
neural network,â€ Quantum Science and Technology, vol. 5, no. 4, p.
044010, 2020.

[31] L. B. Kristensen, M. Degroote, P. Wittek, A. Aspuru-Guzik, and N. T.
Zinner, â€œAn Artiï¬cial Spiking Quantum Neuron,â€ arXiv:1907.06269,
2019.

[32] S. Mangini, F. Tacchino, D. Gerace, C. Macchiavello, and D. Bajoni,
â€œQuantum computing model of an artiï¬cial neuron with continuously
valued input data,â€ Machine Learning: Science and Technology, vol. 1,
no. 4, p. 045008, 2020.

[33] L. G. Wright and P. L. McMahon, â€œThe Capacity of Quantum Neural

Networks,â€ arXiv:1908.01364, 2019.

[34] N.-H. Chia, A. GilyÂ´en, T. Li, H.-H. Lin, E. Tang, and C. Wang,
â€œSampling-based sublinear low-rank matrix arithmetic framework for
dequantizing quantum machine learning,â€ STOC 2020: Proceedings of
the 52nd Annual ACM SIGACT Symposium on Theory of Computing,
pp. 387â€“400, 2020.

[35] M. Schuld, M. Fingerhuth, and F. Petruccione, â€œImplementing a
distance-based classiï¬er with a quantum interference circuit,â€ EPL
(Europhysics Letters), vol. 119, no. 6, 2017.

[36] P. Rebentrost, T. R. Bromley, C. Weedbrook, and S. Lloyd, â€œQuantum
Hopï¬eld neural network,â€ Physical Review A, vol. 98, no. 4, p. 042308,
2018.

[37] M. Rossi, M. Huber, D. BruÃŸ, and C. Macchiavello, â€œQuantum hyper-
graph states,â€ New Journal of Physics, vol. 15, no. 11, p. 113022, 2013.
[38] V. Giovannetti, S. Lloyd, and L. Maccone, â€œQuantum Random Access
Memory,â€ Physical Review Letters, vol. 100, no. 16, p. 160501, 2008.
[39] M. Ghio, D. Malpetti, M. Rossi, D. BruÃŸ, and C. Macchiavello,
â€œMultipartite entanglement detection for hypergraph states,â€ Journal of
Physics A: Mathematical and Theoretical, vol. 51, no. 4, p. 045302,
2018.

[40] A. Peruzzo, J. McClean, P. Shadbolt, M.-H. Yung, X.-Q. Zhou, P. J.
Love, A. Aspuru-Guzik, and J. L. Oâ€™Brien, â€œA variational eigenvalue
solver on a photonic quantum processor,â€ Nature Communications,
vol. 5, no. 1, p. 4213, 2014.

[41] J. Carolan, M. Mohseni, J. P. Olson, M. Prabhu, C. Chen, D. Bunandar,
M. Y. Niu, N. C. Harris, F. N. C. Wong, M. Hochberg, S. Lloyd, and
D. Englund, â€œVariational quantum unsampling on a quantum photonic
processor,â€ Nature Physics, vol. 16, p. 322, 2020.

[42] G. Aleksandrowicz and al., â€œQiskit: An open-source framework for

quantum computing,â€ 2019.

[43] J. A. Nelder and R. Mead, â€œA Simplex Method for Function Minimiza-
tion,â€ The Computer Journal, vol. 7, no. 4, pp. 308â€“313, 1965.
[44] M. J. Powell, â€œA direct search optimization method that models the
objective and constraint functions by linear interpolation,â€ in Advances
in Optimization and Numerical Analysis, S. Gomez and J.-P. Hennart,
Eds. Dordrecht: Kluwer Academic, 1994, pp. 51â€“67.

[45] A. Skolik, J. R. McClean, M. Mohseni, P. van der Smagt, and M. Leib,
â€œLayerwise learning for quantum neural networks,â€ Quantum Machine
Intelligence vol. 3, no. 5, 2021.

[46] M. Cerezo, A. Sone, T. Volkoff, L. Cincio, and P. J. Coles, â€œCost-
function-dependent barren plateaus in shallow quantum neural net-
works,â€ arXiv:2001.00550, 2020.

[47] E. Grant, L. Wossnig, M. Ostaszewski, and M. Benedetti, â€œAn
initialization strategy for addressing barren plateaus in parametrized
quantum circuits,â€ Quantum, vol. 3, p. 214, 2019.

[48] H. Buhrman, R. Cleve, J. Watrous, and R. de Wolf, â€œQuantum Finger-

printing,â€ Physical Review Letters, vol. 87, no. 16, p. 167902, 2001.

[49] F. Tacchino, P. Barkoutsos, C. Macchiavello, D. Gerace, I. Tavernelli
and D. Bajoni, â€œVariational learning for quantum artiï¬cial neural net-
works,â€ Proceedings of the IEEE International Conference on Quantum
Computing and Engineering , 2020.

