1
2
0
2

r
a

M
3

]
h
p
-
t
n
a
u
q
[

1
v
8
9
4
2
0
.
3
0
1
2
:
v
i
X
r
a

Variational learning for
quantum artiﬁcial neural networks

Francesco Tacchino∗§¶, Stefano Mangini†(cid:107)¶, Panagiotis Kl. Barkoutsos∗,
Chiara Macchiavello†(cid:107)∗∗, Dario Gerace†, Ivano Tavernelli∗ and Daniele Bajoni‡
∗IBM Quantum, IBM Research – Zurich, 8803 R¨uschlikon, Switzerland
†University of Pavia, Department of Physics, via Bassi 6, 27100 Pavia, Italy
‡University of Pavia, Department of Industrial and Information Engineering, via Ferrata 1, 27100 Pavia, Italy
(cid:107)INFN Sezione di Pavia, Via Bassi 6, I-27100, Pavia, Italy
∗∗CNR-INO - Largo E. Fermi 6, I-50125, Firenze, Italy
§Email: fta@zurich.ibm.com
¶These authors contributed equally to this work.

Abstract—In the last few years, quantum computing and
machine learning fostered rapid developments in their respec-
tive areas of application, introducing new perspectives on how
information processing systems can be realized and programmed.
The rapidly growing ﬁeld of Quantum Machine Learning aims
at bringing together these two ongoing revolutions. Here we ﬁrst
review a series of recent works describing the implementation of
artiﬁcial neurons and feed-forward neural networks on quantum
processors. We then present an original realization of efﬁcient
individual quantum nodes based on variational unsampling
protocols. We investigate different learning strategies involving
global and local layer-wise cost functions, and we assess their
performances also in the presence of statistical measurement
noise. While keeping full compatibility with the overall memory-
efﬁcient feed-forward architecture, our constructions effectively
reduce the quantum circuit depth required to determine the
activation probability of single neurons upon input of the relevant
data-encoding quantum states. This suggests a viable approach
towards the use of quantum neural networks for pattern classi-
ﬁcation on near-term quantum hardware.

I. INTRODUCTION

In classical machine learning, artiﬁcial neurons and neural
networks were originally proposed, more than a half century
ago, as trainable algorithms for classiﬁcation and pattern
recognition [1], [2]. A few milestone results obtained in
subsequent years, such as the backpropagation algorithm [3]
and the Universal Approximation Theorem [4], [5], certiﬁed
the potential of deep feed-forward neural networks as a com-
putational model which nowadays constitutes the cornerstone
of many artiﬁcial intelligence protocols [6], [7].

In recent years, several attempts were made to link these
powerful but computationally intensive applications to the
rapidly growing ﬁeld of quantum computing, see also Ref. [8]
for a useful review. The latter holds the promise to achieve
relevant advantages with respect
to classical machines al-
ready in the near term, at least on selected tasks including
e.g. chemistry calculations [9], [10], classiﬁcation and op-
timization problems [11]. Among the most relevant results
obtained in Quantum Machine Learning it is worth mentioning
the use of trainable parametrized digital and continuous-
variable quantum circuits as a model for quantum neural

networks [12]–[21], the realization of quantum Support Vector
Machines (qSVMs) [22] working in quantum-enhanced feature
spaces [23], [24] and the introduction of quantum versions of
artiﬁcial neuron models [25]–[32]. However, it is true that
very few clear statements have been made concerning the
concrete and quantitative achievement of quantum advantage
in machine learning applications, and many challenges still
need to be addressed [8], [33], [34].

In this work, we review a recently proposed quantum
algorithm implementing the activity of binary-valued artiﬁcial
neurons for classiﬁcation purposes. Although formally exact,
this algorithm in general requires quite large circuit depth
for the analysis of the input classical data. To mitigate for
this effect we introduce a variational
learning procedure,
based on quantum unsampling techniques, aimed at critically
reducing the quantum resources required for its realization. By
combining memory-efﬁcient encoding schemes and low-depth
quantum circuits for the manipulation and analysis of quantum
states,
the proposed methods, currently at an early stage
of investigation, suggest a practical route towards problem-
speciﬁc instances of quantum computational advantage in
machine learning applications.

II. A MODEL OF QUANTUM ARTIFICIAL NEURONS

The simplest formalization of an artiﬁcial neuron can be
given following the classical model proposed by McCulloch
and Pitts [1]. In this scheme, a single node receives a set of
binary inputs {i0, . . . , im−1} ∈ {−1, 1}m, which can either be
signals from other neurons in the network or external data. The
computational operation carried out by the artiﬁcial neuron
consists in ﬁrst weighting each input by a synapse coefﬁcient
wj ∈ {−1, 1} and then providing a binary output O ∈ {−1, 1}
denoting either an active or rest state of the node determined
by an integrate-and-ﬁre response

O =

(cid:40)

1
−1

j wjij ≥ θ

if (cid:80)
otherwise

(1)

where θ represents some predeﬁned threshold.

 
 
 
 
 
 
A quantum procedure closely mimicking the functionality
of a binary valued McCulloch-Pitts artiﬁcial neuron can be
designed by exploiting, on one hand, the superposition of com-
putational basis states in quantum registers, and on the other
hand the natural non-linear activation behavior provided by
quantum measurements. In this section, we will brieﬂy outline
a device-independent algorithmic procedure [28] designed to
implement such a computational model on a gate-based quan-
tum processor. More explicitly, we show how classical input
and weight vectors of size m can be encoded on a quantum
hardware by using only N = log2 m qubits [28], [35], [36].
For loading and manipulation of data, we describe a protocol
based on the generation of quantum hypergraph states [37].
This exact approach to artiﬁcial neuron operations will be used
in the main body of this work as a benchmark to assess the
performances of approximate variational techniques designed
to achieve more favorable scaling properties in the number of
logical operations with respect to classical counterparts.

Let (cid:126)i and (cid:126)w be binary input and weight vectors of the form

(cid:126)i =








(cid:126)w =








i0
i1
...
im−1








w0
w1
...
wm−1








(2)

with ij, wj ∈ {−1, 1} and m = 2N . A simple and qubit-
effective way of encoding such collections of classical data
can be given by making use of the relative quantum phases
(i.e. factors ±1 in our binary case) in equally weighted
superpositions of computational basis states. We then deﬁne
the states

|ψi(cid:105) =

1
√
m

m−1
(cid:88)

j=0

ij|j(cid:105)

|ψw(cid:105) =

1
√
m

m−1
(cid:88)

j=0

wj|j(cid:105)

(3)

where, as usual, we label computational basis states with
integers j ∈ {0, . . . , m − 1} corresponding to the decimal
representation of the respective binary string. The set of all
possible states which can be expressed in the form above is
known as the class of hypergraph states [37].

According to Eq. (1), the quantum algorithm must ﬁrst
perform the inner product (cid:126)i · (cid:126)w. It is not difﬁcult to see
that, under the encoding scheme of Eq. (3), the inner product
between inputs and weights is contained in the overlap [28]

(cid:104)ψw|ψi(cid:105) =

(cid:126)w ·(cid:126)i
m

(4)

We can explicitly compute such overlap on a quantum register
through a sequence of (cid:126)i- and (cid:126)w-controlled unitary operations.
First, assuming that we operate on a N-qubit quantum register
starting in the blank state |0(cid:105)⊗N , we can load the input-
encoding quantum state |ψi(cid:105) by performing a unitary trans-
formation Ui such that

Ui|0(cid:105)⊗N = |ψi(cid:105)

(5)

It is important to mention that this preparation step would most
effectively be replaced by, e.g., a direct call to a quantum
memory [38], or with the supply of data encoding states
readily generated in quantum form by quantum sensing devices
to be analyzed or classiﬁed. It is indeed well known that
the interface between classical data and their representation
on quantum registers currently constitutes one of the major
bottlenecks for Quantum Machine Learning applications [8].
Let now Uw be a unitary operator such that

Uw|ψw(cid:105) = |1(cid:105)⊗N = |m − 1(cid:105)

(6)

In principle, any m × m unitary matrix having the elements of
(cid:126)w appearing in the last row satisﬁes this condition. If we apply
Uw after Ui, the overall N -qubits quantum state becomes

Uw|ψi(cid:105) =

m−1
(cid:88)

j=0

cj|j(cid:105) ≡ |φi,w(cid:105)

Using Eq. (6), we then have

(cid:104)ψw|ψi(cid:105) = (cid:104)ψw|U †

wUw|ψi(cid:105) =

= (cid:104)m − 1|φi,w(cid:105) = cm−1

(7)

(8)

We thus see that, as a consequence of the constraints imposed
to Ui and Uw, the desired result (cid:126)i · (cid:126)w ∝ (cid:104)ψw|ψi(cid:105) is contained
up to a normalization factor in the coefﬁcient cm−1 of the ﬁnal
state |φi,w(cid:105).

The ﬁnal step of the algorithm must access the computed
input-weight scalar product and determine the activation state
of the artiﬁcial neuron. In view of constructing a general
architecture for feed-forward neural networks [30], it is useful
to introduce an ancilla qubit a, initially set in the state |0(cid:105), on
which the cm−1 ∝ (cid:104)ψw|ψi(cid:105) coefﬁcient can be written through
a multi-controlled NOT gate, where the role of controls is
assigned to the N encoding qubits [28]:

|φi,w(cid:105)|0(cid:105)a →

m−2
(cid:88)

j=0

cj|j(cid:105)|0(cid:105)a + cm−1|m − 1(cid:105)|1(cid:105)a

(9)

At this stage, a measurement of qubit a in the computational
basis provides a probabilistic non-linear threshold activation
behavior, producing the output |1(cid:105)a state, interpreted as an
active state of the neuron, with probability |cm−1|2. Although
this form of the activation function is already sufﬁcient to
carry out elementary classiﬁcation tasks and to realize a logical
XOR operation [28], more complex threshold behaviors can in
principle be engineered once the information about the inner
product is stored on the ancilla [27], [29]. Equivalently, the
ancilla can be used, via quantum controlled operations, to
pass on the information to other quantum registers encoding
successive layers in a feed-forward network architecture [30].
It is worth noticing that directing all the relevant information
into the state of a single qubit, besides enabling effective quan-
tum synapses, can be advantageous when implementing the
procedure on real hardware on which readout errors constitute
a major source of inaccuracy. Nevertheless, multi-controlled
NOT operations, which are inherently non-local, can lead to

complex decompositions into hardware-native gates especially
in the presence of constraints in qubit-qubit connectivity.
When operating a single node to carry out simple classiﬁcation
tasks or, as we will do in the following sections, to assess the
performances of individual portions of the proposed algorithm,
the activation probability of the artiﬁcial neuron can then
equivalently be extracted directly from the register of N
encoding qubits by performing a direct measurement of |φi,w(cid:105)
targeting the |m − 1(cid:105) ≡ |1(cid:105)⊗N computational basis state.

A. Exact implementation with quantum hypergraph states

A general and exact realization of the unitary transforma-
tions Ui and Uw can be designed by using the generation
algorithm for quantum hypergraph states [28]. The latter have
been extensively studied as useful quantum resources [37],
[39], and are formally deﬁned as follows. Given a collection
of N vertices V , we call a k-hyper-edge any subset of exactly
k vertices. A hypergraph g≤N = {V, E} is then composed of
a set V of vertices together with a set E of hyper-edges of
any order k, not necessarily uniform. Notice that this deﬁnition
includes the usual notion of a mathematical graph if k = 2
for all (hyper)-edges. To any hypergraph g≤N we associate a
N -qubit quantum hypergraph state via the deﬁnition

N
(cid:89)

(cid:89)

|g≤N (cid:105) =

k=1

{qv1 ,...,qvk }∈E

CkZqv1 ,...,qvk

|+(cid:105)⊗N

(10)

where qv1 , . . . , qvk are the qubits connected by a k-hyper-edge
in E and, with a little abuse of notation, we assume C2Z ≡ CZ
and C1Z ≡ Z = Rz(π). For N qubits there are exactly N =
22N −1 different hypergraph states. We can make use of well
known preparation strategies for hypergraph states to realize
the unitaries Ui and Uw with at most a single N -controlled
CN Z and a collection of p-controlled CpZ gates with p < N .
It is worth pointing out already here that such an approach,
while optimizing the number of multi-qubit logic gates to be
employed, implies a circuit depth which scales linearly in the
size of the classical input, i.e. O(m) ≡ O(2N ), in the worst
case corresponding to a fully connected hypergraph [28].

To describe a possible implementation of Ui, assume once
again that the quantum register of N encoding qubits is ini-
tially in the blank state |0(cid:105)⊗N . By applying parallel Hadamard
gates (H⊗N ) we obtain the state |+(cid:105)⊗N , corresponding to
a hypergraph with no edges. We can then use the target
collection of classical inputs (cid:126)i as a control for the following
iterative procedure:

for P = 1 to N do

for j = 0 to m − 1 do

if (|j(cid:105) has exactly P qubits in |1(cid:105) and ij = −1) then

Apply CP Z to those qubits
Flip the sign of ik in (cid:126)i ∀k such that |k(cid:105) has the
same P qubits in |1(cid:105)

end if
end for

end for

Similarly, Uw can be obtained by ﬁrst performing the routine
outlined obove (without the initial parallel Hadamard gates)
tailored according to the classical control (cid:126)w: since all the
gates involved in the construction are the inverse of themselves
and commute with each other, this step produces a unitary
transformation bringing |ψw(cid:105) back to |+(cid:105)⊗N . The desired
transformation Uw is completed by adding parallel H⊗N and
NOT⊗N gates [28].

III. VARIATIONAL REALIZATION OF A QUANTUM
ARTIFICIAL NEURON

Although the implementation of the unitary transformations
Ui and Uw outlined above is formally exact and optimizes the
number of multi-qubit operations to be performed by lever-
aging on the correlations between the ±1 phase factors, the
overall requirements in terms of circuit depth pose in general
severe limitations to their applicability in non error-corrected
quantum devices. Moreover, although with such an approach
the encoding and manipulation of classical data is performed
in an efﬁcient way with respect to memory resources, the
computational cost needed to control the execution of the
unitary transformations and to actually perform the sequences
of quantum logic gates remains bounded by the corresponding
classical limits. Therefore, the aim of this section is to explore
conditions under which some of the operations introduced in
our quantum model of artiﬁcial neurons can be obtained in
more efﬁcient ways by exploiting the natural capabilities of
quantum processors.

In the following, we will mostly concentrate on the task of
realizing approximate versions of the weight unitary Uw with
signiﬁcantly lower implementation requirements in terms of
circuit depth. Although most of the techniques that we will
introduce below could in principle work equally well for the
preparation of encoding states |ψi(cid:105), it is important to stress
already at this stage that such approaches cannot be interpreted
as a way of solving the long standing issue represented by the
loading of classical data into a quantum register. Instead, they
are pursued here as an efﬁcient way of analyzing classical
or quantum data presented in the form of a quantum state.
Indeed, the variational approach proposed here requires ad-
hoc training for every choice of the target vector (cid:126)w whose Uw
needs to be realized. To this purpose, we require access to
many copies of the desired |ψw(cid:105) state, essentially representing
a quantum training set for our artiﬁcial neuron. As in our
formulation a single node characterized by weight connections
(cid:126)w can be used as an elementary classiﬁer recognizing input
data sufﬁciently close to (cid:126)w itself [28], the variational procedure
presented here essentially serves the double purpose of training
the classiﬁer upon input of positive examples |ψw(cid:105) and of
ﬁnding an efﬁcient quantum realization of such state analyzer.

A. Global variational training

According to Eq. (6), the purpose of the transformation
Uw within the quantum artiﬁcial neuron implementation is
essentially to reverse the preparation of a non-trivial quantum
state |ψw(cid:105) back to the relatively simple product state |1(cid:105)⊗N .

a

b

Fig. 1. Variational learning via unsampling. (a) Global strategy, with optimization targeting all qubits simultaneously. (b) Local qubit-by-qubit approach, in
which each layer is used to optimize the operation for one qubit at a time.

in general

the qubits in the state |ψw(cid:105) share
Notice that
multipartite entanglement [39]. Here we discuss a promis-
ing strategy for the efﬁcient approximation of the desired
transformation satisfying the necessary constraints based on
variational techniques. Inspired by the well known variational
quantum eigensolver (VQE) algorithm [40], and in line with
a recently introduced unsampling protocol [41], we deﬁne
the following optimization problem: given access to indepen-
dent copies of |ψw(cid:105) and to a variational quantum circuit,
characterized by a unitary operation V ((cid:126)θ) and parametrized
by a set of angles (cid:126)θ, we wish to ﬁnd a set of values (cid:126)θopt
that guarantees a good approximation of Uw. The heuristic
circuit implementation typically consists of sequential blocks
of single-qubit rotations followed by entangling gates, repeated
up to a certain number that guarantees enough freedom for the
convergence to the desired unitary [9].

Once the solution V ((cid:126)θopt) is found, which in our setup
corresponds to a fully trained artiﬁcial neuron, it would then
provide a form of quantum advantage in the analysis of
arbitrary input states |ψi(cid:105) as long as the circuit depth for
the implementation of the variational ansatz is sub-linear in
the dimension of the classical data, i.e. sub-exponential in the
size of the qubit register. As it is customarily done in near-
term VQE applications, the optimization landscape is explored
by combining executions of quantum circuits with classical
feedback mechanisms for the update of the (cid:126)θ angles. In the
most general scenario, and according to Eq. (6), a cost function
can be deﬁned as

F((cid:126)θ) = 1 − |(cid:104)11 . . . 1|V ((cid:126)θ)|ψw(cid:105)|2

The solution (cid:126)θopt is then represented by

(cid:126)θopt = arg min
(cid:126)θ

F((cid:126)θ)

(11)

(12)

and leads to V ((cid:126)θopt) (cid:39) Uw. We call this approach a global
variational unsampling as the cost function in Eq. (11) requires
all qubits to be simultaneously found as close as possible to
their respective target state |1(cid:105), without making explicit use
of the product structure of the desired output state |1(cid:105)⊗N .
It is indeed well known that VQE can lead in general to
exponentially difﬁcult optimization problems [14], however

the characteristic feature of the problem under evaluation may
actually allow for a less complex implementation of the VQE
for unsampling purposes [41], as outlined in the following
section. A schematic representation of the global variational
training is provided in Fig. 1a.

B. Local variational training

An alternative approach to the global unsampling task,
particularly suited for the case we are considering in which the
desired ﬁnal state of the quantum register is fully unentangled,
makes use of a local, qubit-by-qubit procedure. This tech-
nique, which was recently proposed and tested on a photonic
platform as a route towards efﬁcient certiﬁcation of quantum
processors [41], is highlighted here as an additional useful tool
within a general quantum machine learning setting.
In the local variational unsampling scheme,

the global
transformation V ((cid:126)θ) is divided into successive layers Vj((cid:126)θj)
of decreasing complexity and size. Each layer is trained
separately, in a serial fashion, according to a cost function
which only involves the ﬁdelity of a single qubit to its desired
ﬁnal state. More explicitly, every Vj((cid:126)θj) operates on qubits
j, . . . , N and has an associated cost function

Fj((cid:126)θj) = 1 − (cid:104)1| Trj+1,...,N [ρj]|1(cid:105)

(13)

where the partial trace leaves only the degrees of freedom
associated to the j-th qubit and, recursively, we deﬁne

(cid:40)

ρj =

Vj((cid:126)θj)ρj−1V †
|ψw(cid:105)(cid:104)ψw|

j ((cid:126)θj)

j > 1
j = 0

(14)

At step j, it is implicitly assumed that all the parameters (cid:126)θk
for k = 1, . . . , j − 1 are ﬁxed to the optimal values obtained
by the minimization of the cost functions in the previous steps.
Notice that, operationally, the evaluation of the cost function
Fj can be automatically carried out by measuring the j-th
qubit in the computational basis while ignoring the rest of the
quantum register, as shown in Fig. 1b.

The beneﬁts of local variational unsampling with respect
to the global strategy are mainly associated to the reduced
complexity of the optimization landscape per step. Indeed,
the local version always operates on the overlap between

|𝜓!⟩𝑉⃗𝜃ℱ⃗𝜃|𝜓!⟩𝑉"⃗𝜃"ℱ!⃗𝜃!𝑉#⃗𝜃#ℱ"⃗𝜃"………n−timesn′−timesn′−timesℰℛ⃗𝜃)ℰℛ(⃗𝜃)ℛ⃗𝜃*ℰa

b

c

Fig. 2. Comparison of output activation pout = |(cid:104)ψw|ψi(cid:105)|2 among the exact (hypergraph states routine), global (n = 3) and local (n(cid:48) = 2) approximate
implementations of Uw. The inset shows the general mapping of any 16-dimensional binary vector (cid:126)b onto the 4 × 4 binary image (b) and the cross-shaped
(cid:126)w used in this example (c). The selected inputs on which the approximations are tested were chosen to cover all the possible cases for pout, and are labeled
with their corresponding integer ki (see main text).

single-qubit states, at the relatively modest cost of adding
N − 1 smaller and smaller variational ansatzes. In the speciﬁc
problem at study, we thus envision the local approach to
become particularly effective, and more advantageous than the
global one, in the limit of large enough number of qubits, i.e.
for the most interesting regime where the size of the quantum
register, and therefore of the quantum computation, exceeds
the current classical simulation capabilities.

C. Case study: pattern recognition

To show an explicit example of the proposed construction,
let us ﬁx m = 16, N = 4. Following Ref. [28], we can
visualize a 16-bit binary vector (cid:126)b, see Eq. (2), as a 4 × 4
binary pattern of black (bj = −1) and white (bj = 1) pixels.
Moreover, we can assign to every possible pattern an integer
label kb corresponding to the conversion of the binary string
kb = b0 . . . b15, where bj = (−1)bj . We choose as our target
(cid:126)w the vector corresponding to kw = 20032, which represents
a black cross on white background at the north-west corner of
the 16-bit image, see Fig 2.

Starting from the global variational strategy, we construct
a parametrized ansatz for V ((cid:126)θ) as a series of entangling (E)
and rotation (R((cid:126)θ)) cycles:

V ((cid:126)θ) =

(cid:32) n
(cid:89)

c=1

(cid:33)

R(θc,1 . . . θc,4)E

R(θ0,1 . . . θ0,4)

(15)

increasing its total depth. Rotations are assumed to be acting
independently on the N = 4 qubits according to

R(θc,1 . . . θc,4) =

4
(cid:79)

q=1

(cid:18)

exp

−i

(cid:19)

θc,q
2

σ(q)
y

(16)

where σ(q)
is the Pauli y-matrix acting on qubit q. At the
y
same time, the entangling parts promote all-to-all interactions
between the qubits according to

(cid:89)

4
(cid:89)

E =

q

q(cid:48)=q+1

CNOTqq(cid:48)

(17)

where CNOTqq(cid:48) is the usual controlled NOT operation be-
tween control qubit q and target q(cid:48) acting on the space of all
4-qubits. For n cycles, the total number nθ of θ-parameters,
including the initial rotation layer R(θ0,1 . . . θ0,4), is therefore
nθ = 4 + 4n.

A qubit-by-qubit version of the ansatz can be constructed
in a similar way by using the same structure of entangling
and rotation cycles, decreasing the total number of qubits by
one after each layer of the optimization. Here we choose a
uniform number n(cid:48) of cycles per qubit (this condition will be
relaxed afterwards, see Sec. III-D), thus setting ∀j (cid:54)= 4



R(θc,j . . . θc,4)E

 R(θ0,j . . . θ0,4)

(18)





n(cid:48)
(cid:89)

c=1

where n is the total number of cycles which in principle
can be varied to increase the expressibility of the ansatz by

Vj((cid:126)θj) =

𝒘For j = 4, we add a single general single-qubit rotation with
three parameters

V4(α, β, γ) = exp

−i

(cid:18)

(α, β, γ) · (cid:126)σ(4)
2

(cid:19)

(19)

where (cid:126)σ = (σx, σy, σz) are again the usual Pauli matrices.

We implemented both versions of the variational training
in Qiskit [42], combining exact simulation of the quantum
circuits required to evaluate the cost function with classical
Nelder-Mead [43] and Cobyla [44] optimizers from the scipy
Python library. We ﬁnd that the values n = 3 and n(cid:48) = 2 allow
the routine to reach total ﬁdelities to the target state |1(cid:105)⊗N well
above 99.99%. As shown in Fig 2, this in turn guarantees a
correct reproduction of the exact activation probabilities of the
quantum artiﬁcial neuron with a quantum circuit depth of 19
(29) for the global (qubit-by-qubit) strategy, as compared to
the total depth equal to 49 for the exact implementation of
Uw using hypergraph states. This counting does not include
the gate operations required to prepare the input state, i.e.
it only evidences the different realizations of the Uw imple-
mentation assuming that each |ψi(cid:105) is provided already in the
form of a wavefunction. Moreover, the multi-controlled CP Z
operations appearing in the exact version were decomposed
into single-qubit rotations and CNOTs without the use of
additional working qubits. Notice that these conditions are the
ones usually met in real near-term superconducting hardware
endowed with a ﬁxed set of universal operations.

D. Structure of the ansatz and scaling properties

In many practical applications, the implementation of the
entangling block E could prove technically challenging, in
particular for near term quantum devices based, e.g., on
superconducting wiring technology, for which the available
connectivity between qubits is limited. For this reason,
it
is useful
to consider a more hardware-friendly entangling
scheme, which we refer to as nearest neighbours. In this case,
each qubit is entangled only with at most two other qubits,
essentially assuming the topology of a linear chain

Enn =

3
(cid:89)

q=1

CNOTq,q+1

(20)

This scheme may require even fewer two-qubit gates to be
implemented with respect to the all-to-all scheme presented
above. Moreover, this entangling unitary ﬁts perfectly well on
those quantum processors consisting of linear chains of qubits
or heavy hexagonal layouts.

We implemented both global and local variational learning
procedures with nearest neighbours entanglers in Qiskit [42],
using exact simulation of the quantum circuits with classical
optimizers to drive the learning procedure. In the following,
we report an extensive analysis of the performances and a
comparison with the all-to-all strategy introduced in Sec. III-C
above. All the simulations are performed by assuming the
same cross-shaped target weight vector (cid:126)w depicted in Fig. 2.
In Figure 3 we show an example of the typical optimization
procedure for three different choices of the ansatz depth (i.e.

Fig. 3. Optimization of the global unitary with nearest neighbours entan-
glement for three different structures differing in the numbers of entangling
blocks n. The cost function is |(cid:104)11 . . . 1|V ((cid:126)θ)|ψw(cid:105)|2 = 1 − F((cid:126)θ), see
Eq. (11). Only for n = 3 the learning model has enough expressibility to
reach a good ﬁnal ﬁdelity. The classical optimizer used in this case was
COBYLA [44].

to implement

Fig. 4. Final ﬁdelity obtained for the local variational training and using both
the all-to-all entangler E (17) and nearest neighbour Enn (20). On top of each
rectangle, in light blue, it is reported the depth of the corresponding quantum
circuit
that given structure with that particular entangling
scheme. For clarity, a structure ‘211’ corresponds to a variational model
having two repetitions (n(cid:48)
1 = 2) for the ﬁrst layer acting on all 4 qubits,
and 1 cycle (n(cid:48)
2 = 1) for the remaining two layers acting on 3 and 2
qubits respectively. Each bar was obtained executing the optimization process
10 times, and then evaluating the means and standard deviations (shown as
error bars). The optimization procedure was performed using COBYLA [44].

1 = n(cid:48)

number of entangling cycles) n = 1, 2, 3, assuming a global
cost function. Here we ﬁnd that n = 3 allows the routine to
reach a ﬁdelity F((cid:126)θ) to the target state |1(cid:105)⊗N above 99%.

In the local qubit-by-qubit variational scheme, we can
actually introduce an additional degree of freedom by allowing
the number of cycles per qubit, n(cid:48), to vary between successive
layers corresponding to the different stages of the optimization
procedure. For example, we may want to use a deeper ansatz
for the ﬁrst unitary acting on all the qubits, and shallower
ones for smaller subsystems. We thus introduce a different n(cid:48)
j
for each Vj((cid:126)θj) in Eq. (18) and we name structure the string

050100150200250Number of function evaluations (COBYLA)0.00.20.40.60.81.0Cost Function1 layer2 layers3 layers111211221222321322333Structure0.00.20.40.60.81.0Fidelity 1316192122242916212527303238Nearest neighbourAll to allFig. 5. Final ﬁdelities for different structures of the local variational learning
model with a nearest neighbour entangler, for the case of N = 5 qubits.
Similarly to the case with N = 4 qubits portrayed in Figure 4, the most
depth-efﬁcient structure is the one consisting of constantly decreasing number
of cycles.

Fig. 6. Number of iterations of the classical optimizer to reach a ﬁdelity of
F = 95%. Each point in the plot is obtained by running the optimization
procedure 10 times and then evaluating the mean and standard deviation
(shown as error bars in the plot). All results refer to exact simulations of
the quantum circuits in the absence of statistical measurement sampling or
device noise, performed with Qiskit statevector_simulator.

‘n1n2n3’. The latter denotes a learning model consisting of
three optimization layers: V1((cid:126)θ1) with n1 entangling cycles,
V2((cid:126)θ2) with n2 and V3((cid:126)θ3) with n3, respectively. In the last
step of the local optimization procedure, i.e. when a single
qubit is involved, we always assume a single 3-parameter
rotation, see Eq. (19). A similar notation will be also applied
in the following when scaling up to N > 4 qubits.

The effectiveness of different structures is explored in Fig-
ure 4. We see that, while the all-to-all entangling scheme typ-
ically performs better in comparison to the nearest neighbour
one, this increase in performance comes at the cost of deeper
circuits. Moreover, the stepwise decreasing structure ‘321’
for the nearest neighbour entangler proves to be an effective
solution to problem, achieving a good ﬁnal accuracy (above
99%) with a low circuit depth. This trend is also conﬁrmed
for the higher dimensional case of N = 5 qubits, which
we report in Fig. 5. Here, the dimension of the underlying
pattern recognition task is increased by extending the original
16-bit weight vector (cid:126)w with extra 0s in front of the binary
representation kw. In fact, it can easily be seen that, assuming
directly nearest neighbours entangling blocks, the decreasing
structure ‘4321’ gives the best performance-depth tradeoff.
Such empirical fact, namely that the most efﬁcient structure
is typically the one consisting of decreasing depths, can be
heuristically interpreted by recalling again that, in general, the
optimization of a function depending on the state of a large
number of qubits is a hard training problem [14]. Although
we employ local cost functions, to complete our particular
task each variational layer needs to successfully disentangle a
single qubit from all the others still present in the register. It is
therefore not totally surprising that the optimization carried out
in larger subsystems requires more repetitions and parameters
(i.e. larger n(cid:48)
j) in order to make the ansatz more expressive.
By assuming that the stepwise decreasing structure remains
sufﬁciently good also for larger numbers of qubits, we studied

the optimization landscape of global, Eq. (11), and local,
Eq. (13), cost functions by investigating how the hardness
of the training procedure scales with increasing N . As com-
mented above for N = 5, we keep the same underlying target
(cid:126)w, which we expand by appending extra 0s in the binary
representation. To account for the stochastic nature of the
optimization procedure, we run many simulations of the same
learning task and report the mean number of iterations needed
for the classical optimizer to reach a given target ﬁdelity
F = 95%. Results are shown in Figure 6. The most signiﬁcant
the use of the aforementioned local cost
message is that
function seems to require higher classical resources to reach
a given target ﬁdelity when the number of qubits increases.
This actually should not come as a surprise, since the number
of parameters to be optimized in the two cases is different.
In fact, in the global scenarios there are N + N · n (the ﬁrst
N is due to the initial layer of rotations) to be optimized,
while in the local case there are N + N · n(cid:48)
1 for the ﬁrst layer,
(N − 1) + (N − 1) · n(cid:48)

2 for the second, ...; for a total of

#local =

N
(cid:88)

q=2

q + qn(cid:48)

q + 3

(21)

where the ﬁnal 3 is due to the fact that the last layer always
consist of a rotation on the Bloch sphere with three parameters,
see Eq. (19). Using the stepwise decreasing structure, that
q = q − 1, we eventually obtain (cid:80)N
is n(cid:48)
q=2 q + q(q − 1) =
(cid:80)N
q=2 q2 ∼ O(N 3), compared to #global ∼ O(N 2). Here we
are assuming a number of layers n = N − 1, consistently with
the N = 4 qubits case (see Figure 3). While in the global case
the optimization makes full use of the available parameters to
globally optimize the state towards |1(cid:105)⊗N , the local unitary
has to go through multiple disentangling stages, requiring (at
least for the cases presented here) more classical iteration
steps. At the same time, it would probably be interesting to

1111222242213322432133334332Structure0.00.20.40.60.81.0Fidelity 19303436374142Depth45678Number of qubits025050075010001250150017502000Mean Iteration for =0.95LocalGlobala

b

Fig. 7. Optimization of cost functions for the local (a) and global (b) case in
the presence of measurement noise for N = 5 qubits. In each ﬁgure we plot
the mean values averaged on 5 runs of the simulation. The shaded colored
areas denote one standard deviation. The number of measurement repetitions
in each simulation was 1024. The ﬁnal ﬁdelity at the end of the training
procedure in this case were Flocal = 0.87 ± 0.02 and Fglobal = 0.89 ± 0.02.
Notice the difference in the horizontal axes bounds. (a) Optimization of the
local cost functions Vj ((cid:126)θj ) (see Eq. (13)), plotted with different colors for
clarity. The vertical dashed lines denotes the end of the optimization of one
layer, and the start of the optimization for the following one. (b) Optimization
of the global cost function V ((cid:126)θ) in Eq. (11)
.

investigate other examples in which the number of parameters
between the two alternative schemes remains ﬁxed, as this
would most likely narrow the differences and provide a more
direct comparison.

In agreement with similar investigations [45], we can ac-
tually conclude that only modest differences between global
and local layer-wise optimization approaches are present when
dealing with exact simulations (i.e. free from statistical and
hardware noise) of the quantum circuit. Indeed, both strategies
achieve good results and a ﬁnal ﬁdelity F((cid:126)θ) > 99%. At
the same time,
it becomes interesting to investigate how
the different approaches behave in the presence of noise,
and speciﬁcally statistical noise coming from measurements
operations. For this reason, we implemented the measurement
sampling using the Qiskit qasm_simulator and employed
a stochastic gradient descent (SPSA) classical optimization
method. Each benchmark circuit is executed nshots = 1024
times in order to reconstruct the statistics of the outcomes.
Moreover, we repeat the stochastic optimization routine multi-
ple times to analyze the average behaviour of the cost function.
In Figure 7 we show the optimization procedure for the local
and global cost functions in the presence of measurement
noise, with both of them reaching acceptable and identical ﬁnal
ﬁdelities Flocal = 0.87±0.02 and Fglobal = 0.89±0.02. Notice
that for the local case (Figure 7(a)) each colored line indicates
the optimization of a Vj((cid:126)θj) from Eq. (13). We observe that the
training for the local model generally requires fewer iterations,
with an effective optimization of each single layer. On the
contrary, in the presence of measurement noise the global

Fig. 8. Scaling of circuit depth for the implementation of Uw computed with
Qiskit. The labels locals and global refer to the local and global variational
approaches, while a2a and nn refer to the all-to-all and nearest-neighbour
entangling schemes respectively. The number of ansatz cycles used for both
the global (n) and local/qubit-by-qubit (n(cid:48)) variational constructions and for
each entangling sctructure are increased with the number of qubits up to the
minimum value guaranteeing a ﬁdelity of the approximations above 98%.

variational training struggles to ﬁnd a good direction for the
optimization and eventually follows a slowly decreasing path
to the minimum. These ﬁndings look to be in agreement, e.g.,
with results from Refs. [45], [46]: with the introduction of
statistical shot noise, the performances of the global model are
heavily affected, while the local approach proves to be more
resilient and capable of ﬁnding a good gradient direction in the
parameters space [46]. In all these simulations, the parameters
in the global unitary and in the ﬁrst layer of the local unitary
were initialized with a random distribution in [0, 2π). All
subsequent layers in the local model were initialized with all
parameters set to zero in order to allow for smooth transitions
from one optimization layer to the following. This strategy was
actually suggested as a possible way to mitigate the occurence
Barren plateaus [45], [47].

We conclude the scaling analysis by reporting in Fig. 8 a
summary of the quantum circuit depths required to implement
the target unitary transformation with different strategies and
for increasing sizes of the qubit register up to N = 7. As it can
be seen, all the variational approaches scale much better when
compared to the exact implementation of the target Uw, with
the global ones requiring shallower depths in the speciﬁc case.
In addition, we recall that the use of an all-to-all entangling
scheme requires longer circuits due to the implementation of
all the CNOTs, but generally needs less ansatz cycles (see
Figure 4). At last, while the global procedures seem to provide
a better alternative compared to local ones in terms of circuit
depth, they might be more prone to suffering from classical
optimization issues [14], [45] when trained and executed on
real hardware, as suggested by the data reported in Fig. 7.
The overall promising results conﬁrm the signiﬁcant advantage
brought by variational strategies compared to the exponential
increase of complexity required by the exact formulation of
the algorithm.

01000200030004000500060000.00.20.40.6CostFunctionsLayer0Layer1Layer2Layer3Layer40200040006000800010000SPSAIteration0.20.40.60.81.0CostFunctionMeancostfunction1-σrange4567Number of qubits0100200300400500600Circuit depthLocal, a2aLocal, nnGlobal, a2aGlobal, nnExactIV. CONCLUSIONS
In this work, we reviewed an exact model for the im-
plementation of artiﬁcial neurons on a quantum processor
and we introduced variational training methods for efﬁciently
handling the manipulation of classical and quantum input
data. Through extensive numerical analysis, we compared
the effectiveness of different circuit structures and learning
strategies, highlighting potential beneﬁts brought by hardware-
compatible entangling operations and by layerwise training
routines. Our work suggests that quantum unsampling tech-
niques represent a useful resource, upon input of quantum
training sets, to be integrated in quantum machine learning
applications. From a theoretical perspective, our proposed
procedure allows for an explicit and direct quantiﬁcation of
possible quantum computational advantages for classiﬁcation
tasks. It is also worth pointing out that such a scheme re-
mains fully compatible with recently introduced architectures
for quantum feed-forward neural networks [30], which are
needed in general to deploy e.g. complex convolutional ﬁlters.
Moreover, although the interpretation of quantum hypergraph
states as memory-efﬁcient carriers of classical information
guarantees an optimal use of the available dimension of a N -
qubit Hilbert space, the variational techniques introduced here
can in principle be used to learn different encoding schemes
designed, e.g., to include continuous-valued features or to
improve the separability of the data to be classiﬁed [23], [24],
[48]. In all envisioned applications, our proposed protocols are
intended as an effective method for the analysis of quantum
states as provided, e.g., by external devices or sensors, while
it is worth stressing that the general problem of efﬁciently
loading classical data into quantum registers still stands open.
Finally, on a more practical level, a successful implementation
on near-term quantum hardware of the variational learning
algorithm introduced in this work will necessarily rely on a
deeper analysis of the impact of realistic noise effects both
on the training procedure and on the ﬁnal optimized circuit.
In particular, we anticipate that
the reduced circuit depth
produced via the proposed method could critically lessen the
quality requirements for quantum hardware, eventually leading
to meaningful implementation of quantum neural networks
within the near-term regime.

ACKNOWLEDGMENTS
A preliminary version of this work was presented at the
2020 IEEE International Conference on Quantum Computing
and Engineering [49]. We acknowledge support from SNF
grant 200021 179312. IBM,
the IBM logo, and ibm.com
are trademarks of International Business Machines Corp.,
registered in many jurisdictions worldwide. Other product
and service names might be trademarks of IBM or other
companies. The current list of IBM trademarks is available
at https://www.ibm.com/legal/copytrade.

REFERENCES

[1] W. S. McCulloch and W. Pitts, “A logical calculus of the ideas immanent
in nervous activity,” The bulletin of mathematical biophysics, vol. 5,
no. 4, pp. 115–133, 1943.

[2] F. Rosenblatt, “The Perceptron: A perceiving and recognizing automa-
ton,” Cornell Areonautical Laboratory, Inc., Tech. Rep. 85-460-1, 1957.
[3] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning repre-
sentations by back-propagating errors,” Nature, vol. 323, no. 6088, pp.
533–536, 1986.

[4] G. Cybenko, “Approximation by superpositions of a sigmoidal function,”
Mathematics of Control, Signals and Systems, vol. 2, no. 4, pp. 303–314,
1989.

[5] K. Hornik, “Approximation capabilities of multilayer feedforward net-

works,” Neural Networks, vol. 4, no. 2, pp. 251–257, 1991.

[6] J. M. Zurada, Introduction to Artiﬁcial Neural Systems. West Group,

1992.

[7] R. Rojas, Neural Networks: A Systematic Introduction. Springer, 1996.
[8] J. Biamonte, P. Wittek, N. Pancotti, P. Rebentrost, N. Wiebe, and
S. Lloyd, “Quantum machine learning,” Nature, vol. 549, no. 7671, pp.
195–202, 2017.

[9] A. Kandala, A. Mezzacapo, K. Temme, M. Takita, M. Brink, J. M.
Chow, and J. M. Gambetta, “Hardware-efﬁcient variational quantum
eigensolver for small molecules and quantum magnets,” Nature, vol.
549, no. 7671, pp. 242–246, 2017.

[10] Y. Cao, J. Romero, J. P. Olson, M. Degroote, P. D. Johnson,
M. Kieferov´a, I. D. Kivlichan, T. Menke, B. Peropadre, N. P. D. Sawaya,
S. Sim, L. Veis, and A. Aspuru-Guzik, “Quantum Chemistry in the
Age of Quantum Computing,” Chemical Reviews, vol. 119, no. 19, pp.
10 856–10 915, 2019.

[11] N. Moll, P. Barkoutsos, L. S. Bishop, J. M. Chow, A. Cross, D. J.
Egger, S. Filipp, A. Fuhrer, J. M. Gambetta, M. Ganzhorn, A. Kandala,
A. Mezzacapo, P. M¨uller, W. Riess, G. Salis, J. Smolin, I. Tavernelli,
and K. Temme, “Quantum optimization using variational algorithms on
near-term quantum devices,” Quantum Science and Technology, vol. 3,
no. 3, p. 030503, 2018.

[12] E. Farhi and H. Neven, “Classiﬁcation with Quantum Neural Networks

on Near Term Processors,” arXiv:1802.06002, 2018.

[13] E. Grant, M. Benedetti, S. Cao, A. Hallam, J. Lockhart, V. Stojevic,
A. G. Green, and S. Severini, “Hierarchical quantum classiﬁers,” npj
Quantum Information, vol. 4, no. 1, p. 65, 2018.

[14] J. R. McClean, S. Boixo, V. N. Smelyanskiy, R. Babbush, and H. Neven,
“Barren plateaus in quantum neural network training landscapes,” Nature
Communications, vol. 9, no. 1, p. 4812, 2018.

[15] N. Killoran, T. R. Bromley, J. M. Arrazola, M. Schuld, N. Quesada, and
S. Lloyd, “Continuous-variable quantum neural networks,” Phys. Rev.
Research, vol. 1, p. 033063, 2019.

[16] M. Benedetti, E. Lloyd, S. Sack, and M. Fiorentini, “Parameterized
quantum circuits as machine learning models,” Quantum Science and
Technology, vol. 4, no. 4, p. 043001, 2019.

[17] A. Mari, T. R. Bromley, J. Izaac, M. Schuld, and N. Killoran, “Transfer
learning in hybrid classical-quantum neural networks,” Quantum, vol. 4,
p. 340, 2020.

[18] I. Cong, S. Choi, and M. D. Lukin, “Quantum convolutional neural

networks,” Nature Physics, vol. 15, pp. 1273–1278, 2019.

[19] M. Schuld, A. Bocharov, K. M. Svore, and N. Wiebe, “Circuit-centric

quantum classiﬁers,” Phys. Rev. A, vol. 101, p. 032308, 2020.

[20] M. Henderson, S. Shakya, S. Pradhan, and T. Cook, “Quanvolutional
neural networks: powering image recognition with quantum circuits,”
Quantum Machine Intelligence, vol. 2, no. 2, 2020.

[21] M. Broughton, G. Verdon, T. McCourt, A. J. Martinez, J. H. Yoo, S. V.
Isakov, P. Massey, M. Y. Niu, R. Halavati, E. Peters, M. Leib, A. Skolik,
M. Streif, D. Von Dollen, J. R. McClean, S. Boixo, D. Bacon, A. K.
Ho, H. Neven, and M. Mohseni, “TensorFlow Quantum: A Software
Framework for Quantum Machine Learning,” arXiv:2003.02989, 2020.
[22] P. Rebentrost, M. Mohseni, and S. Lloyd, “Quantum Support Vector
Machine for Big Data Classiﬁcation,” Physical Review Letters, vol. 113,
no. 13, p. 130503, 2014.

[23] V. Havl´ıˇcek, A. D. C´orcoles, K. Temme, A. W. Harrow, A. Kandala,
J. M. Chow, and J. M. Gambetta, “Supervised learning with quantum-
enhanced feature spaces,” Nature, vol. 567, no. 7747, pp. 209–212, 2019.
[24] M. Schuld and N. Killoran, “Quantum Machine Learning in Feature
Hilbert Spaces,” Physical Review Letters, vol. 122, no. 4, p. 040504,
2019.

[25] M. Schuld, I. Sinayskiy, and F. Petruccione, “Simulating a perceptron on
a quantum computer,” Physics Letters A, vol. 379, no. 7, pp. 660–663,
2015.

[26] N. Wiebe, A. Kapoor, and K. M. Svore, “Quantum Perceptron Models,”

arXiv:1602.04799, 2016.

[27] Y. Cao, G. G. Guerreschi, and A. Aspuru-Guzik, “Quantum Neuron: an
elementary building block for machine learning on quantum computers,”
arXiv:1711.11240, 2017.

[28] F. Tacchino, C. Macchiavello, D. Gerace, and D. Bajoni, “An artiﬁcial
neuron implemented on an actual quantum processor,” npj Quantum
Information, vol. 5, p. 26, 2019.

[29] E. Torrontegui and J. J. Garcia-Ripoll, “Unitary quantum perceptron as
efﬁcient universal approximator,” EPL (Europhysics Letters), vol. 125,
no. 3, p. 30004, 2019.

[30] F. Tacchino, P. Barkoutsos, C. Macchiavello, I. Tavernelli, D. Gerace,
and D. Bajoni, “Quantum implementation of an artiﬁcial feed-forward
neural network,” Quantum Science and Technology, vol. 5, no. 4, p.
044010, 2020.

[31] L. B. Kristensen, M. Degroote, P. Wittek, A. Aspuru-Guzik, and N. T.
Zinner, “An Artiﬁcial Spiking Quantum Neuron,” arXiv:1907.06269,
2019.

[32] S. Mangini, F. Tacchino, D. Gerace, C. Macchiavello, and D. Bajoni,
“Quantum computing model of an artiﬁcial neuron with continuously
valued input data,” Machine Learning: Science and Technology, vol. 1,
no. 4, p. 045008, 2020.

[33] L. G. Wright and P. L. McMahon, “The Capacity of Quantum Neural

Networks,” arXiv:1908.01364, 2019.

[34] N.-H. Chia, A. Gily´en, T. Li, H.-H. Lin, E. Tang, and C. Wang,
“Sampling-based sublinear low-rank matrix arithmetic framework for
dequantizing quantum machine learning,” STOC 2020: Proceedings of
the 52nd Annual ACM SIGACT Symposium on Theory of Computing,
pp. 387–400, 2020.

[35] M. Schuld, M. Fingerhuth, and F. Petruccione, “Implementing a
distance-based classiﬁer with a quantum interference circuit,” EPL
(Europhysics Letters), vol. 119, no. 6, 2017.

[36] P. Rebentrost, T. R. Bromley, C. Weedbrook, and S. Lloyd, “Quantum
Hopﬁeld neural network,” Physical Review A, vol. 98, no. 4, p. 042308,
2018.

[37] M. Rossi, M. Huber, D. Bruß, and C. Macchiavello, “Quantum hyper-
graph states,” New Journal of Physics, vol. 15, no. 11, p. 113022, 2013.
[38] V. Giovannetti, S. Lloyd, and L. Maccone, “Quantum Random Access
Memory,” Physical Review Letters, vol. 100, no. 16, p. 160501, 2008.
[39] M. Ghio, D. Malpetti, M. Rossi, D. Bruß, and C. Macchiavello,
“Multipartite entanglement detection for hypergraph states,” Journal of
Physics A: Mathematical and Theoretical, vol. 51, no. 4, p. 045302,
2018.

[40] A. Peruzzo, J. McClean, P. Shadbolt, M.-H. Yung, X.-Q. Zhou, P. J.
Love, A. Aspuru-Guzik, and J. L. O’Brien, “A variational eigenvalue
solver on a photonic quantum processor,” Nature Communications,
vol. 5, no. 1, p. 4213, 2014.

[41] J. Carolan, M. Mohseni, J. P. Olson, M. Prabhu, C. Chen, D. Bunandar,
M. Y. Niu, N. C. Harris, F. N. C. Wong, M. Hochberg, S. Lloyd, and
D. Englund, “Variational quantum unsampling on a quantum photonic
processor,” Nature Physics, vol. 16, p. 322, 2020.

[42] G. Aleksandrowicz and al., “Qiskit: An open-source framework for

quantum computing,” 2019.

[43] J. A. Nelder and R. Mead, “A Simplex Method for Function Minimiza-
tion,” The Computer Journal, vol. 7, no. 4, pp. 308–313, 1965.
[44] M. J. Powell, “A direct search optimization method that models the
objective and constraint functions by linear interpolation,” in Advances
in Optimization and Numerical Analysis, S. Gomez and J.-P. Hennart,
Eds. Dordrecht: Kluwer Academic, 1994, pp. 51–67.

[45] A. Skolik, J. R. McClean, M. Mohseni, P. van der Smagt, and M. Leib,
“Layerwise learning for quantum neural networks,” Quantum Machine
Intelligence vol. 3, no. 5, 2021.

[46] M. Cerezo, A. Sone, T. Volkoff, L. Cincio, and P. J. Coles, “Cost-
function-dependent barren plateaus in shallow quantum neural net-
works,” arXiv:2001.00550, 2020.

[47] E. Grant, L. Wossnig, M. Ostaszewski, and M. Benedetti, “An
initialization strategy for addressing barren plateaus in parametrized
quantum circuits,” Quantum, vol. 3, p. 214, 2019.

[48] H. Buhrman, R. Cleve, J. Watrous, and R. de Wolf, “Quantum Finger-

printing,” Physical Review Letters, vol. 87, no. 16, p. 167902, 2001.

[49] F. Tacchino, P. Barkoutsos, C. Macchiavello, D. Gerace, I. Tavernelli
and D. Bajoni, “Variational learning for quantum artiﬁcial neural net-
works,” Proceedings of the IEEE International Conference on Quantum
Computing and Engineering , 2020.

