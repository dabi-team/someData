Combinatory Adjoints and Differentiation

Martin Elsman
DIKU, U. Copenhagen

Fritz Henglein
DIKU, U. Copenhagen

Robin Kaarsgaard
U. Edinburgh

mael@diku.dk

henglein@diku.dk

Robin.Kaarsgaard@ed.ac.uk

Mikkel Kragh Mathiesen
DIKU, U. Copenhagen

mkm@di.ku.dk

Robert Schenck
DIKU, U. Copenhagen

rschenck@di.ku.dk

We develop a compositional approach for automatic and symbolic differentiation based on categor-
ical constructions in functional analysis where derivatives are linear functions on abstract vectors
rather than being limited to scalars, vectors, matrices or tensors represented as multi-dimensional
arrays.

We show that both symbolic and automatic differentiation can be performed using a differential
calculus for generating linear functions representing Fr´echet derivatives based on rules for primitive,
constant, linear and bilinear functions as well as their sequential and parallel composition. Linear
functions are represented in a combinatory domain-speciﬁc language.

Finally, we provide a calculus for symbolically computing the adjoint of a derivative without
using matrices, which are too inefﬁcient to use on high-dimensional spaces. The resulting symbolic
representation of a derivative retains the data-parallel operations from the input program. The com-
bination of combinatory differentiation and computing formal adjoints turns out to be behaviorally
equivalent to reverse-mode automatic differentiation. In particular, it provides opportunities for opti-
mizations where matrices are too inefﬁcient to represent linear functions.

1 Introduction

Automatic differentiation (AD) [21] is the discipline of computing derivatives for functions given by
programs. It is used in gradient-based optimization, neural networks, probabilistic inference [3, sec. 4]
and has numerous applications in computer vision, natural language processing, computational science,
bioinformatics, quantitative ﬁnance, computational economics, and in many other areas. For example,
backpropagation, which is used in machine learning to train neural networks, is an instance of reverse
mode AD. Building tools to implement and compute derivatives from programs automatically, efﬁciently,
and precisely, has far-reaching impact potential.

1.1 Contributions

In this paper we develop a general framework for expressing and reasoning about functions, their deriva-
tives and the adjoints of these in combinatory form.
We make the following novel contributions:

• We present a general framework for constructing Hilbert spaces. The constructions freely combine
tensor products and direct sums. Direct sums generalize both homogeneous data types, such as
order-k tensors (scalars, vectors, matrices, and so on) and inhomogeneous types such as tuple
and record types. Abstract tensor products express tensor decomposition of matrices, which are
asymptotically more efﬁcient than using matrices for low-rank matrices.

Jeremy Gibbons and Max S. New (Eds.): Ninth Workshop on
Mathematically Structured Functional Programming (MSFP 2022)
EPTCS 360, 2022, pp. 1–26, doi:10.4204/EPTCS.360.1

© Elsman, Henglein, Kaarsgaard, Mathiesen, Schenck
This work is licensed under the
Creative Commons Attribution License.

2

Combinatory Adjoints and Differentiation

• We identify ﬁve general differentiation rules for calculating Fr´echet derivatives, which represent
derivatives as linear functions, by structural recursion on functions given in combinatory form. The
combinatory form of a function thus distills its differential properties. Intuitively, differentiating a
function in point-full notation consists mostly of (implicitly) turning it into combinatory form.

• We exhibit the generalized product rule, which is applicable to arbitrary bilinear functions oper-
ating on spaces of any dimension as a general rule not previously exploited. Bilinear functions
on high-dimensional data are common, including matrix multiplication, outer product, dot prod-
uct, zip (Hadamard) product and any composition of a linear function with a bilinear function.
To differentiate a bilinear function we only need to know that it is bilinear since its derivative is
expressed in terms of itself.

• We provide afﬁne interpretation of a function in combinatory form, which computes both the
output value of a function at a given input and returns a symbolic (term) representation of its
Fr´echet derivative. Symbolic rather than functional representations facilitate optimization using
(multi)linear and tensor algebra equalities.

• We further demystify reverse-mode automatic differentiation by identifying its essence as sym-
bolically computing the adjoint of the Fr´echet derivative in combinatory form. The adjoint of a
linear function f is a representation of its transpose, the continuation passing style version of f .
In adjoints linear continuations are represented by their duals, ordinary ﬁrst-order vectors, which
facilitates and explains how a linear function can be executed efﬁciently in reverse.

• We provide an adjoint calculus for symbolically calculating the adjoints of linear functions in
combinatory form. We identify relational reduction and tensor contraction as natural parallel
linear operations since they provide their own adjoints.

• We illustrate how combinatory differentiation and combinatory adjoint calculation can be used to
derive the backpropagation code for neural networks such that all data parallelism is preserved.

More speculatively, we believe our combinatory setting is useful for a differential and adjoint calculus on
functions and linear functions. The Hilbert space setting seems to provide a promising setting in which
both database and analytic functions can be speciﬁed, differentiated and reversed by taking adjoints.

1.2 Outline

We assume basic familiarity with functional analysis, which, as a framework, generalizes both mul-
tivariate and tensor calculus by operating on arbitrary elements of structured vector spaces instead of
restricting them to tuples of scalars or multi-dimensional arrays that represent tensors. The relevant
notions are introduced in the remainder of this and the next section.

In Section 3 we informally present a list of primitive, constant, linear and bilinear analytic functions
that can be combined freely by sequential and parallel composition to complex analytic functions in
point-free notation. In Section 4 we then formulate a calculus for symbolically differentiating functions
in combinatory form such that the derivatives of parallel functions are rendered in point-free notation, as
parallel linear functions. In Section 5 we show how this gives rise to afﬁne interpretation of an analytic
function in combinatory form: The interpreter returns not only the value of a function on its input, but
also a compact combinatory representation of its derivative whose size is largely independent of the
dimensionality of the vector spaces involved.
In Section 6 we show how the inner product operator
can be used to uniquely represent linear continuations by ordinary ﬁrst-order vectors. This gives rise
to symbolically computed adjoints, which run a linear function efﬁciently “in reverse” and thus provide

Elsman, Henglein, Kaarsgaard, Mathiesen, Schenck

3

reverse-mode AD. We illustrate combinatory differentiation on neural networks in Section 7 and discuss
related work in Section 8.

1.3 Background

Deﬁnition 1.1 (Fr´echet derivative). For a function f : V → W on Banach spaces V,W , the linear function
A ∈ V (cid:40) W is the Fr´echet derivative of f at v if it satisﬁes

that is

f (v + dv) ≈ f (v) + A(dv),

lim
||dv||V →0

|| f (v + dv) − ( f (v) + A(dv))||W
||dv||V

= 0

where || . . . ||U is the norm that comes with the Banach space U.

The Fr´echet derivative of f : V → W is the partial function f (cid:48) : V → (V (cid:40) W ) that maps a vector

v ∈ V to the Fr´echet derivative of f at v.

See Appendix A for other notions of derivatives, including Gateaux derivatives.

2 Sets and spaces

We provide general constructions for deﬁning inner product spaces over R and their implicit completions
to real Hilbert spaces. These provide a model of symbolic derivatives as Fr´echet derivatives.
An inner product space over R is a vector space V over R equipped with an inner product

(cid:12) : V ×V →2 R

that is symmetric, v1 (cid:12) v2 = v2 (cid:12) v1, and positive deﬁnite, v (cid:12) v > 0 for all v (cid:54)= 0. A real Hilbert space is
an inner product space over R that is also a complete metric space with respect to the distance function
d(v, w) = (cid:107)v − w(cid:107) where (cid:107)v(cid:107) =

v (cid:12) v.

√

A continuous function f : V → W on real Hilbert spaces V,W is linear if f (u + v) = f (u) + f (v) and

f (k · v) = k · f (v); we write f : V (cid:40) W if f is continuous and linear.

A continuous binary function (cid:5) : U ×V → W is bilinear if (u(cid:5)) : V (cid:40) W and ((cid:5)v) : U (cid:40) W are
linear for all u ∈ U, v ∈ V where (u(cid:5)) and ((cid:5)v) are deﬁned by (u(cid:5))(v) = u (cid:5) v = ((cid:5)v)(u). We write
f : U ×V →2 W if f is continuous and bilinear.

Proviso: Henceforth all functions will implicitly be continuous.

2.1 Sets

We provide a language for deﬁning index sets. These are used to construct direct sum spaces.

X,Y ::= n | X ×Y | X +Y

where n ∈ N, n is the initial segment {1, . . . , n} of natural numbers; S ×T and S +T the Cartesian product,
respectively disjoint union of S and T .

The constructible index sets are ﬁnite, which ensure that the constructions are metrically complete.
We believe the theory, being essentially algebraic, can be extended to inﬁnite denumerable sets. We stick
to ﬁnite index sets and thus ﬁnite-dimensional Hilbert spaces in this paper, however.

4

2.2 Spaces

Combinatory Adjoints and Differentiation

Below we provide constructions for Hilbert spaces generated by the following terms:

U,V,W ::= 0 | K | ⊕x∈XVx | V ⊗W

where Vx may depend on x ∈ X.

2.2.1 Atomic spaces

The trivial vector space 0 consists of the single element 0.

K stands for the underlying ﬁeld of our vector spaces, here R. Its elements are the elements of R as

a ﬁeld. Its operations as a vector space are the corresponding ﬁeld operations.

2.2.2 Direct sum space

The Hilbert space V = ⊕x∈XVx for denumerable X is the (external) direct sum of a family of Hilbert
spaces Vx indexed by x ∈ X. Its elements are maps m from X such that m(x) ∈ Vx and ∑x∈X (m(x) (cid:12)Vx
m(x)) < ∞. We write mx for the result of applying the map to highlight that x is an element of an index
set, not a vector. Its operations are deﬁned by component-wise lifting, where the inner product is
(v (cid:12)V v(cid:48)) = ∑

(vx (cid:12)Vx v(cid:48)
x)

x∈X

The summation is deﬁned since ∑x∈X (vx (cid:12)Vx v(cid:48)
trivially well-deﬁned for ﬁnite X.

x) ≤ ∑x∈X (vx (cid:12)Vx vx) + ∑x∈X (v(cid:48)

x (cid:12)Vx v(cid:48)

x) < ∞. Note it is

V comes with linear injection and projection functions

ι X
y
π X
y

: Vy (cid:40) ⊕x∈XVx
: ⊕x∈XVx (cid:40) Vy

for y ∈ X, and the zipped apply operator

Πx∈X fx

: ⊕x∈XVx → ⊕x∈XWx

for a family of functions fx ∈ Vx → Wx indexed by x ∈ X. They satisfy
π X
y ◦ Πx∈X fx ◦ ι X
z ◦ Πx∈X fx ◦ ι X
π X

y = fy
y = 0yz

if y (cid:54)= z

where 0yz : Vy (cid:40) Wz maps all vx ∈ Vx to 0 ∈ Wz. The zipped apply operator preserves linearity, that is

for fx : Vx (cid:40) Wx. A special case of this is

Πx∈X fx

: ⊕x∈XVx (cid:40) ⊕x∈XWx

∆ : ⊕x∈X (Vx (cid:40) Wx) (cid:40) (⊕x∈XVx (cid:40) ⊕x∈XWx)

deﬁned by

∆(⊕x∈X fx)(⊕x∈X vx) = ⊕x∈X ( fx(vx))

which will later play the role of gathering derivatives acting on the individual differentials of a collection
into a derivative that acts on all differentials in parallel.

We write V1 × . . . × Vn or V1 ⊕ . . . ⊕ Vn for ⊕i∈nVi. In particular, V1 × V2 = V1 ⊕ V2 = ⊕i∈2Vi is the

direct sum of V1 and V2, whose elements are the pairs (v1, v2) such that v1 ∈ V1 and v2 ∈ V2.

Elsman, Henglein, Kaarsgaard, Mathiesen, Schenck

5

2.2.3 Copower space

For set X and space V , the copower V X is the direct sum, where each space in the family is the same
Vx = V :

V X = ⊕x∈XV.

As special cases we have Rn as the space of n-ary vectors of scalars. In particular, V × V = V 2. Note
that the exponents are sets, not numbers. This is reﬂected in the notation Rn: an element is a ﬁnite
map m from n to R, which can conveniently be written using tuple notation (m1, . . . , mn). For example
(5, 8, 22) ∈ R3 is syntactic sugar for {1 (cid:55)→ 5, 2 (cid:55)→ 8, 3 (cid:55)→ 22}.

For relation R ⊆ X ×Y where X,Y are ﬁnite we deﬁne relational reduction

redR

: V X (cid:40) V Y

(redR(v))y = ∑
(x,y)∈R

vx

Many useful functions can be deﬁned in terms of relational reduction. Let Y ⊆ X be ﬁnite. The

functions

if f : V → W

: V X → W X
f X
: V (cid:40) V X
repY
: V X (cid:40) V
∑Y
+ : V 2 (cid:40) V
dup : V (cid:40) V 2
: V n (cid:40) V n
: U (cid:40) ⊕y∈YVy
if fy : U (cid:40) Vy
: ⊕x∈XVx (cid:40) W if gx : Vx (cid:40) W

scann
(cid:104) fy(cid:105)y∈Y
[gx]x∈X

are deﬁned by

f X = Πx∈X f
repY = red1×Y •ι 1
1
= π 1
1 • redY ×1

∑
Y
+ = ∑
2

dup = rep2

scann = red{(i, j)|1≤i≤ j≤n}

(cid:104) fy(cid:105)y∈Y = Πy∈Y fy • repY
[gx]x∈X = ∑
X

•Πx∈X gx

2.2.4 Tensor product space

W = U ⊗V is the tensor product space of U and V . Its ﬁnite elements are the formal terms generated by

w ::= 0 | k · w | w1 + w2 | u ⊗ v

6

Combinatory Adjoints and Differentiation

where k ∈ R, u ∈ U, v ∈ V that are identiﬁed modulo the vector space axioms and the equalities

(k · v) ⊗ w = k · (v ⊗ w) = v ⊗ (k · w)
(v1 + v2) ⊗ w = (v1 ⊗ w) + (v2 ⊗ w)
v ⊗ (w1 + w2) = (v ⊗ w1) + (v ⊗ w3).

We write [w]⊗ for the equivalence class of w under these equalities and deﬁne

0W = [0]⊗
v1 +W v2 = [v1 + v2]⊗
k ·W v = [k · v]⊗

W is metrically complete for ﬁnite-dimensional U,V ; otherwise metric completion of the equivalence
classes [w]⊗ is required. The equalities guarantee that the functions are well-deﬁned and (W, 0W , +W , ·W )
forms a Hilbert space such that

⊗ : U ×V →2 W

is bilinear, that is pointwise linear in each of its arguments. Indeed, the operation ⊗ and the space U ⊗V
are constructed to be universal: For every bilinear function (cid:5) : U ×V →2 T there exists a unique linear
function ¯(cid:5) : U ⊗V (cid:40) T such that (cid:5) = ¯(cid:5) ◦ ⊗.
Furthermore, we deﬁne the inner product

to be the unique bilinear function that satisﬁes

(cid:12) : W ×W →2 R

(u1 ⊗ v1) (cid:12) (u2 ⊗ v2) = (u1 (cid:12) u2) · (v1 (cid:12) v2).

3 Functions in combinatory form

We provide a domain-speciﬁc language for specifying analytic functions on Hilbert spaces in combi-
natory form, that is in point-free notation. In combinatory form, all subterms are closed functions; in
particular, a subterm does not have implicit dependencies on an environment. This facilitates formulation
of a compositional differential calculus for calculating Fr´echet derivatives.

3.1 Tensor contraction

It would be sufﬁcient to provide the
We provide a language constant for a single bilinear function.
tensor product ⊗ as sole bilinear function since it is universal in the sense that all bilinear functions
f : U ×V →2 W factor into f = ¯f • (⊗) for a unique ¯f : U ⊗V (cid:40) W , the characteristic universal property
of ⊗. For reasons to become clear later, we provide tensor contraction

∗ :

(W ⊗V ) × (V ⊗U) →2 (W ⊗U)

instead. It is deﬁned as the unique bilinear function satisfying

(w ⊗ v) ∗ (v(cid:48) ⊗ u) = (v (cid:12) v(cid:48)) · (w ⊗ u)

Elsman, Henglein, Kaarsgaard, Mathiesen, Schenck

7

3.2 Unitary operators

We have a large number of useful natural unitary operators; these are natural linear isomorphisms that
are isometric, i.e. preserve norms. We list a few of them here.

(cid:104)(cid:104)(cid:104)

(cid:105)(cid:105)(cid:105)
T

:
:
:
assoc
:
distrib :
zip :

They are deﬁned by

V ←→1 R ⊗V
V ←→1 V ⊗ R
(V ⊗W ) ←→1 (W ⊗V )
((U ⊗V ) ⊗W ) ←→1 (U ⊗ (V ⊗W ))
(⊕x∈XVx) ⊗W ←→1 ⊕x∈X (Vx ⊗W )
(⊕x∈XVx) ⊕ (⊕x∈XWx) ←→1 ⊕x∈X (Vx ⊕Wx)

| |
:
| |
:
T
:
: assoc−1
: distrib−1
: unzip

(cid:104)(cid:104)(cid:104) v = 1 ⊗ v

|k ⊗ v| = k · v

v (cid:105)(cid:105)(cid:105) = v ⊗ 1

|v ⊗ k| = k · v
(v ⊗ w)T = w ⊗ v

assoc((u ⊗ v) ⊗ w) = u ⊗ (v ⊗ w)
assoc−1(u ⊗ (v ⊗ w)) = (u ⊗ v) ⊗ w
distrib((⊕x∈X vx) ⊗ w) = ⊕x∈X (vx ⊗ w)

(zip(v, w))x = (vx, wx)

where ⊕x∈X vx is notation for the element of ⊕x∈XVx that maps x to the value vx ∈ Vx. It turns out that the
inverse of a unitary operator is also its adjoint; this will be useful later.

A derived isometric isomorphism is

V X ⊗W Y ←→1 (V ⊗W )X×Y

and in particular

Rm ⊗ Rn ←→1 Rm×n.
In other words, all the elements of the tensor product of Rm and Rn can be represented by m × n matrices.
Our construction of Rm ⊗ Rn using symbolic operators 0, ·, + and ⊗ provides more space efﬁcient
representations for low-rank matrices, however. For example, every rank-1 m × n matrix corresponds to
v ⊗ w, its tensor decomposition, for some v ∈ Rm, w ∈ Rn. This term representation is of size O(m +
n) rather than requiring m · n entries in a matrix. (Note that a rank-1 matrix may have no 0-entries.)
Matrix/vector multiplication can be performed with only n multiplications instead of m · n multiplications
when using the matrix representation.

The tensor and inner product operators are special cases of tensor contraction via the (cid:104)(cid:104)(cid:104) and (cid:105)(cid:105)(cid:105) unitary

operators:

v ⊗ w = v (cid:105)(cid:105)(cid:105) ∗ (cid:104)(cid:104)(cid:104) w
v1 (cid:12) v2 = |(cid:104)(cid:104)(cid:104) v1 ∗ v2 (cid:105)(cid:105)(cid:105)|

Note that these are parsed as (v (cid:105)(cid:105)(cid:105)) ∗ ((cid:104)(cid:104)(cid:104) w) and |((cid:104)(cid:104)(cid:104) v) ∗ (w (cid:105)(cid:105)(cid:105))|, respectively.

8

Combinatory Adjoints and Differentiation

3.3 Linear functions

In addition to the unitary operators, the following are linear functions:

: V ⊗U (cid:40) W ⊗U
: W ⊗V (cid:40) W ⊗U

(v∗)
(∗w)
0V,W : V (cid:40) W

ι X
y
π X
y
Πx∈X fx
∆ f
(cid:104) fx(cid:105)x∈X
redR
f X
idV
g • f

: Vy (cid:40) ⊕x∈XVx
: ⊕x∈XVx (cid:40) Vy
: ⊕x∈XVx (cid:40) ⊕x∈XWx
: ⊕x∈XVx (cid:40) ⊕x∈XWx
: V (cid:40) ⊕x∈XWx
: V X (cid:40) V Y
: U X (cid:40) V X
: V (cid:40) V
: U (cid:40) W

if v ∈ W ⊗V
if w ∈ V ⊗U

if y ∈ X
if y ∈ X
if fx : Vx (cid:40) Wx
if f : ⊕x∈X (Vx (cid:40) Wx)
if fx : V (cid:40) Wx
if R ⊆ X ×Y is compact
if f : U (cid:40) V

if f : U (cid:40) V, g : V (cid:40) W

3.4 Constant functions

We have the constant functions

deﬁned by Kw(v) = w.

3.5 Primitive functions

Kw : V → W if w ∈ W

We furthermore assume we have named primitive functions p1, . . . , pn denoting analytic functions with
associated derivative functions that are expressible as combinator expressions. For example, for each
(cid:48)
k ∈ Z/{0} we have the function k : R → R with associated derivative ( k)
(x) = ((k · xk−1)·). Note
the · at the end; it is there since the Fr´echet derivative at x is not a value from R, but an element of
R (cid:40) R, which is isomorphic with, but not the same as, R. Similarly, we have ln : R → R with associated
ln(cid:48)(x) = (x−1·); sin : R → R with sin(cid:48)(x) = ((cos x)·); cos : R → R with cos(cid:48)(x) = ((− sin x)·) and so on.
Note that ln is only deﬁned on R+ = {x ∈ R | x > 0} and is thus, in particular, not analytic on all of R.
We defer the subtleties of handling partially deﬁned and not-everywhere differentiable functions in
this paper to future work and assume henceforth for simplicity that our primitive functions are analytic
on their entire domain.

In practice almost all primitive functions are functions on scalars and returning scalars. Primitive

operators and functions on high-dimensional spaces are typically linear or bilinear.

3.6 Function composition

Every constant, linear, bilinear and primitive function constructed so far is an analytical function.

Finally we have sequential and parallel composition of analytical functions:

g ◦ f
Πx∈X fx

: U → W
: ⊕x∈XVx → ⊕x∈XWx

if f : U → V, g : V → W
if fx : Vx → Wx for all x ∈ X, X ﬁnite

Elsman, Henglein, Kaarsgaard, Mathiesen, Schenck

9

4 Fr´echet differential calculus

Recall that f (cid:48) : V → (V (cid:40) W ) denotes the Fr´echet derivative of f : V → W . The linear function f (cid:48)(v) is
the tangent of f at v. We provide differentiation rules for functions in combinatory form.

Theorem 4.1. The following differentiation rules are valid for analytic functions on Hilbert spaces:

(g ◦ f )(cid:48)(v) = g(cid:48)( f (v)) • f (cid:48)(v)

Kw

(cid:48)(v) = 0
h(cid:48)(v) = h

(cid:5)(cid:48)(u, v) = (u(cid:5)) • π2 + ((cid:5)v) • π1

(Πx∈X fx)(cid:48)(v) = ∆((Πx∈X fx

(cid:48))(v))

if h : V (cid:40) W
if (cid:5) : U ×V →2 W
if fx : Vx → Wx

(1)

(2)

(3)

(4)

(5)

Rule 1 is the chain rule for sequential composition. It expresses that the derivatives of g at f (v) and

of f at v are combined by composition • of linear functions.

Rules 2, 3 and 4 are for constant, linear and bilinear functions, respectively. Note in particular
Rule 4, the generalized product rule. It is applicable to any bilinear function. The derivative of any
bilinear function can be written in terms of the function itself; we do not need access to its deﬁnition,
only its name. The same is true for linear functions; they are their own derivatives. All we need to know
is that a function is linear to differentiate it. We will see that adjoint differentiation, which underlies
reverse-mnode AD, requires processing its deﬁnition, however.

Finally, Rule 5 is for differentiating parallel composition. It is worth looking at special cases of it.

Let X = 2, that is Πx∈2 fx = f1 × f2 : V1 ×V2 → W1 ×W2. We can calculate

( f1 × f2)(cid:48)(v1, v2) = ∆(( f1
= ∆( f1
= f1

(cid:48) × f2
(cid:48)(v1), f2
(cid:48)(v1) × f2

(cid:48))(v1, v2))
(cid:48)(v2))
(cid:48)(v2)

Let us consider f X = Πx∈X f where f ∈ V → W .

(cid:48)
( f X )

(v) = (Πx∈X f )(cid:48)(v)

= ∆((Πx∈X f (cid:48))(v))
= ∆( f (cid:48)X (v))

In words, to differentiate f X at value v ∈ V X , we need to compute the derivative of f at each element vx
of V X . This yields an element of (V (cid:40) W )X ; ﬁnally, ∆ gathers these component-wise derivatives into a
single derivative.

5 Afﬁne interpretation

A function h : V → W is afﬁne if it is the sum of a constant and a linear function, that is

h(v) = w + g(v)

for some w ∈ W and g ∈ V (cid:40) W . Note that w and g are uniquely determined by h. We call them the
constant and linear component of h, respectively, and write h ∈ V →≤1 W if h is afﬁne.

10

Combinatory Adjoints and Differentiation

(g ◦ f )[1](x) = let (fx, f (cid:48)x) = f [1](x) in

let (gfx, g(cid:48)fx) = g[1](fx) in
(gfx, g(cid:48)fx • f (cid:48)x)

Kw

[1](x) = (w, 0)
h[1](x) = (h(x), h)
(cid:5)[1](x) = let (u, v) = x in

if h : V (cid:40) W

(u (cid:5) v, (u(cid:5)) • π2 + ((cid:5)v) • π1)
(Πy∈Y fy)[1](x) = let (w, d) = unzip((Πy∈Y (λ x. fy

[1](x)))(x)) in

if (cid:5) : U ×V →2 W

(w, ∆(d))

if fy : Vy → Wy

Figure 1: Afﬁne interpretation of functions in combinatory form. See Section 5.2 for an explanation of
the underlined let .

We say that g : V →≤1 W is the afﬁne approximation of f : V → W at v ∈ V and write f (v) ≈ g if

lim
||dv||V →0

|| f (v + dv) − g(dv)||W
||dv||V

= 0

Proposition 5.1. A function has at most one afﬁne approximation at v, written f [1](v), where f [1](v)(dv) =
f (v) + f (cid:48)(v)(dv).

Thinking about differentiation in terms of computing afﬁne approximations is useful since computing
derivatives compositionally requires computing a function’s value paired with its derivative [13]. The
components of the afﬁne approximation of a function in combinatory form can be computed by structural
recursion. See Figure 1.

The parallel composition rule specializes to tuples and copowers as follows:

( f1 × f2)[1](x1, x2) = let (fx1, f (cid:48)x1) = f1

f X [1](v) = let (w, d) = unzip( f [1]X

[1](x1), (fx2, f (cid:48)x2) = f2
(v)) in (w, ∆(d))

[1](x2) in ((fx1, fx2), (f (cid:48)x1, f (cid:48)x2))

Note that unzipping the outputs of each component is the price we pay for separating the collective output
into a value and a derivative component.

Theorem 5.2. Assume p[1](v) = (p(v), p(cid:48)(v)) for all primitive functions. Then f [1](v) = ( f (v), f (cid:48)(v)) for
all functions in combinatory form.

5.1 Automatic differentation

The afﬁne approximation rules give rise to an interpreter

eval[1][[ ]] : Term(V → W ) → V → Term(W × (V (cid:40) W ))

where Term(V → W ) is a language for representing functions in combinatory form, including Term(V (cid:40)
W ) as a (sub)language for representing linear functions in combinatory form: Just replace t[1] in Figure 1
by eval[1][[t]]. When applied to a combinatory term t denoting f and a concrete value v, it returns a term
containing the value w = f (v) and a combinatory representation t(cid:48) denoting the derivative f (cid:48)(v). This
term t(cid:48) can be optimized using the rules of linear and tensor algebra prior to applying an interpreter

Elsman, Henglein, Kaarsgaard, Mathiesen, Schenck

11

eval(0)[[ ]] : Term(V (cid:40) W ) → V → W to t(cid:48) and an input differential value dv, which yields the output
differential f (cid:48)(v)(dv).

Behaviorally this corresponds to forward-mode automatic differentiation (AD), but is essentially
different. In elemental and tensor-based forward-mode AD [18, 1] we have an interpreter eval(f)[[ ]] for
2)) representing f such that
a term t : Term(⊕i∈n(Vi

2) → ⊕ j∈m(Vj

eval(f)[[t]]

: ⊕i∈n(Vi

2) → ⊕ j∈m(Vj

2).

requires both values and associated differentials as inputs at the same time. It computes

eval(f)[[t]]((v1, dv1), . . . , (vn, dvn)) = ((y1, dy1), . . . , (ym, dym)).
where (y1, . . . , ym) = f (v1, . . . , vn) and (dy1, . . . , dym) = f (cid:48)(v1, . . . , vn)(dv1, . . . , dvn).

The type of eval(f)[[t]] camouﬂages that the output values y1, . . . , yn do not depend on the second
components dv1, . . . , dvn, and that for ﬁxed v1, . . . , vn the dy1, . . . , dym are linear functions of dv1, . . . , dvn.
Note, in particular, that both values and differentials must be provided before execution can start.

In our formulation eval[1][[t]] requires no input differentials to run the code, only the input values
v1, . . . , vn, which manifests that the output values do not depend on any differentials. Furthermore, the
derivative is returned as a term in a language that guarantees that it denotes a linear function.

5.2 Symbolic differentiation

The afﬁne approximation rules are carefully written to facilitate symbolic differentiation by applying
eval[1][[t]] to a symbolic variable x. This amounts to specializing the code of eval[1][[ ]] to the concrete t
by partial evaluation.

The let -expressions without underlining can be eliminated by substitution, that is rewriting let (u, v) =
( f , g) in g to g[ f /u, g/v] during partial evaluation. Since the let-bound variables have single occurrences
the size of the expression does not grow. The let -expression for bilinear functions should not be elimi-
nated, however, since its let-bound variables are used twice. Substituting them would cause expression
swell. Conversely, not substituting them avoids expression swell: The size of the symbolically differen-
tiated expression is linear in the size of the input expression. Retaining let in the output is the reason for
having

eval[1][[ ]] : Term(V → W ) → V → Term(W × (V (cid:40) W ))

rather than

eval[1][[ ]] : Term(V → W ) → V → (W × Term(V (cid:40) W )).

This supports and generalizes to non-elemental symbolic differentiation that expression swell is a myth
[33]: Retaining sharing when applying the (generalized) product rule is both necessary and sufﬁcient to
avoid it.

6 Adjoints

The dual vector space V ∗ of vector space V is the vector space of linear functionals, also called covectors,
V (cid:40) R where R is the underlying ﬁeld of V . By the Riesz representation theorem, the inner product
induces an isomorphism dual deﬁned by

dual

: V (cid:40) V ∗

dual(v) = ((cid:12)v).

12

Combinatory Adjoints and Differentiation

In particular, dual−1((cid:12)v) = v, and v and ((cid:12)v) are called duals of each other.1

Some applications require computing the dual of a covector. For example, given a scalar function

f : V → R, the gradient ∇ f : V → V is deﬁned by

∇ f (v) = dual−1( f (cid:48)(v)).

If we implement covectors as functions that can only be applied, the only way of implementing dual−1
is by applying it to each of the base vectors of V , which is problematic if V is of high dimension, say a
million or a billion.

Similarly, sometimes we may want to implement the transpose

: W ∗ (cid:40) V ∗

f †
f † = (• f )

of f : V (cid:40) W . The transpose is the continuation-passing style version of f where a linear continuation
is passed as the ﬁrst argument. To wit, we have f †(κ)(v) = κ( f (v)) where κ is the continuation.

A general idea permeating mathematical and computer science applications of linear algebra is rep-
resenting a covector by its dual vector v with an indication that it represents ((cid:12)v) (“I am contravariant”),
not v itself.

We would thus like to ﬁnd a linear function f ∗ : W (cid:40) V that implements the transpose f † by using
ordinary vectors rather than covectors to represent linear continuations; that is, it should be the case that
f ∗(w) = v whenever f †((cid:12)w) = ((cid:12)v).

Deﬁnition 6.1.

f ∗ : W (cid:40) V is the adjoint of f : V (cid:40) W if

f †((cid:12)w) = ((cid:12)v) ⇔ f ∗(w) = v

By the Riesz representation theorem we immediately have that

Proposition 6.2.

f ∗ exists and is unique for Hilbert spaces.

The deﬁning property of an adjoint can be restated as the familiar property where f is pushed from

one argument to the other argument of the inner product.

Proposition 6.3.
v ∈ V, w ∈ W .

f ∗ : W (cid:40) V is the adjoint of f : V (cid:40) W if and only if f (v) (cid:12) w = v (cid:12) f ∗(w) for all

We can implement dual−1 using the adjoint:

Proposition 6.4. Let f : V (cid:40) R, that is f ∈ V ∗. Then dual−1( f ) = f ∗(1) and thus ∇ f (v) = ( f (cid:48)(v))∗(1).

Linear functions are built from other linear functions. We provide general rules for calculating ad-

joints symbolically of linear functions in combinatory form.

6.1 Adjoint calculation

Adjoints can be calculated symbolically for linear functions in combinatory form.

1For ﬁnite index sets the constructible Hilbert spaces are ﬁnite-dimensional. Note that the Riesz representation theorem also

holds for inﬁnite-dimensional Hilbert spaces.

Elsman, Henglein, Kaarsgaard, Mathiesen, Schenck

13

Theorem 6.5. Let X,Y be ﬁnite sets, R ⊆ X ×Y , and RT = {(y, x) | (x, y) ∈ R}. Then:

id∗ = id
(g • f )∗ = f ∗ • g∗
0∗ = 0
(v∗)∗ = (vT ∗)
(∗w)∗ = (∗wT )
∗
(ι X
x )

= π X
x

(Πx∈X fx)∗ = Πx∈X fx

∗

redR

∗ = redRT

Furthermore, the inverses of unitary operators are also their adjoints.

These rules are not accidentally symmetric; indeed the above language of linear functions has been
designed to yield these symmetries, where each construct has an adjoint construct. This is the reason for
using tensor contraction ∗ instead of ⊗ as the primitive universal bilinear function.

The adjoint of a partially applied tensor contraction (v∗) is the transpose2 vT of v. Recall that

T : V ⊗W (cid:40) W ⊗V swaps components of formal tensor product sums.

Note also that the adjoint of redR is redRT ; in particular, if R is a function, RT is generally not a
function. Allowing for R to be a relation rather than restricting it to be a function makes expressing its
adjoint in terms of red possible.

The adjoints for other operations can be derived from their deﬁnitions in terms of these primitive

functions and constructs. For example, we can derive

(k·)∗ = (k·)
(·v)∗ = ((cid:12)v)

6.2 Adjoint differentiation
The adjoint derivative of f : V → W at v ∈ V is ( f (cid:48)(v))∗. We can compute it by employing our afﬁne
interpreter eval[1][[ f ]]; applying it to v; extracting the term representing f (cid:48)(v); applying the adjoint calcu-
lation rules of Theorem 6.5 to calculate a term representating ( f (cid:48)(v))∗; and ﬁnally applying the derived
adjoint to output differentials to compute input differentials.3 Alternatively, we can construct the adjoint
derivative during afﬁne interpretation; see Figure 2.

This provides us with a method behaviorally equivalent to reverse-mode automatic differentiation. In
the ﬁrst phase, the value of f at v and the term of ( f (cid:48)(v))∗, which includes all—and only— the relevant
intermediate results of f (v) are computed by f [1r] from v alone. Only in the second phase, the output
term representing ( f (v))∗ is interpreted as a function by applying it to output differentials. After the ﬁrst
phase, the adjoint derivative can be optimized using algebraic simplications, and it can be compiled for
efﬁcient data parallel execution on a GPU.

Going one step further, the ﬁrst phase can be done symbolically, which amounts to a specialization of
the adjoint afﬁne interpreter to the particular source code for f . The result of doing so can be compiled
for efﬁcient data parallel execution before the values of v and output differential dy are available.

2Not to be confused with the transpose of a linear function, which it is related, but different.
3When used in the adjoint direction, from output to input, the variables containing differentials are often called adjoint

variables.

14

Combinatory Adjoints and Differentiation

(g ◦ f )[1r](x) = let (fx, f (cid:48)xa) = f [1r](x) in

let (gfx, g(cid:48)fxa) = g[1r](fx) in
(gfx, f (cid:48)xa • g(cid:48)fxa)

Kw

[1r](x) = (w, 0)
h[1r](x) = (h(x), h∗)
(cid:5)[1r](x) = let (u, v) = x in

(Πy∈Y fy)[1r](x) = let (w, d) = unzip((Πy∈Y (λ x. fy

[1r](x)))(x)) in

(u (cid:5) v, ι 2

2 • (u(cid:5))∗ + ι 2

1 • ((cid:5)v)∗)

(w, ∆(d))

if h : V (cid:40) W

if (cid:5) : U ×V →2 W

if fy : Vy → Wy

Figure 2: Adjoint afﬁne interpretation of functions in combinatory form

7 Applications

We illustrate combinatory differentiation by applying it to neural networks. Additional examples show-
ing the application of equational reasoning to derive derivatives (sic!) can be found in Appendix B.

Example 7.1. A k-layer neural network Nk consists of a composition of k layers with the i-th layer given
by

gi(xi,Wi, bi) = hi

mi (Wi (cid:63) xi + bi),

where Wi ∈ Rmi×mi−1, bi ∈ Rmi, and xi ∈ Rmi−1 along with a loss function l(v, y) : Rmk → R:

Nk : Rm0 × Rm1×m0 × Rm1 × Rm2×m1 × Rm2 × · · · × Rmk×mk−1 × Rmk × Rmk → R
Nk(x,W1, b1,W2, b2, . . . ,Wk, bk, y) = l(gk(Wk, bk(· · · (g2(W2, b2(g1(W1, b1, x)))))), y).

For simplicity, we use l(v, y) = (v − y) (cid:12) (v − y) for the loss function. In point-free form, the i-th layer of
the network must propagate the inputs for all subsequent layers; gi and l in point-free form are

mi ◦ (((cid:63)) ◦ (cid:104)π ni

gi = (cid:104)hi
l = ((cid:12)) ◦ dup ◦ (π 2

1 (cid:105) + π ni
2 , π ni
2 ),

1 − π 2

3 ), π ni

4 , . . . , π ni

ni (cid:105),

where ni = 2(k + 2 − i). Hence, the entire network is constructed as

Nk = l ◦ gk ◦ · · · g2 ◦ g1.

Elsman, Henglein, Kaarsgaard, Mathiesen, Schenck

15

Applying the differentiation rules of Theorem 4.1, we differentiate gi and l

g(cid:48)
i(xi,Wi, bi, . . . ,Wk, bk, y)
= {deﬁnition of gi}
mi ◦ (((cid:63)) ◦ (cid:104)π ni
2 , π ni
= {by Rules 1, 3, and 5}

(cid:104)hi

1 (cid:105) + π ni

3 ), π ni

4 , . . . , π ni

ni (cid:105)(cid:48)(xi,Wi, bi, . . . ,Wk, bk, y)

(cid:104)(hi

mi)(cid:48)(Wi (cid:63) xi + bi) • (((cid:63))(cid:48)((cid:104)Wi, xi(cid:105)) • (cid:104)π ni

2 , π ni

1 (cid:105) + π ni

3 ), π ni

4 , . . . , π ni
ni (cid:105)

= {by Rules 5 and 4 }

(cid:104)∆(h(cid:48)
i

mi(Wi (cid:63) xi + bi)) • (((Wi(cid:63)) • π 2

2 + ((cid:63)xi) • π 2

1 ) • (cid:104)π ni

2 , π ni

1 (cid:105) + π ni

3 ), π ni

4 , . . . , π ni

ni (cid:105),

l(cid:48)(v, y)
= {deﬁnition of l}
(((cid:12)) ◦ dup ◦ (π 2

1 − π 2

2 ))(cid:48)(v, y)

= {by Rule 1}

((cid:12))(cid:48)(v − y, v − y) • (dup ◦ (π 2

1 − π 2

2 ))(cid:48)(v, y)

= {by Rules 1, 4, and 3}

2 + ((cid:12)(v − y)) • π 1
Repeated application of Rule 1 now yields the entire differentiated network

1 ) • (dup • (π 2

(((v − y)(cid:12)) • π 2

1 − π 2

2 )).

k(x,W1, b1,W2, b2, . . . ,Wk, bk, y) = l(cid:48)((gk ◦ · · · ◦ g1)(x,W1, b1,W2, b2, . . . ,Wk, bk, y))
N(cid:48)

• g(cid:48)
• · · · • g(cid:48)

k((gk−1 ◦ · · · ◦ g1)(x,W1, b1,W2, b2, . . . ,Wk, bk, y))

1(x,W1, b1,W2, b2, . . . ,Wk, bk, y).

Straight-forward application of Theorem 6.5 may subsequently be used to obtain the adjoint of N(cid:48)
k,
(N(cid:48)

k(x,W1, b1,W2, b2, . . . ,Wk, bk, y))∗ : R (cid:40) Rm0 × Rm1×m0 × · · · × Rmk .

8 Discussion

We have provided a functional-analysis based compositional framework for differentiation and adjoint
differentiation that encompasses both symbolic and automatic differentiation. It highlights that, very
generally, adjoint differentiation is the combination of symbolic Fr´echet differentiation and symbolic
calculation of adjoints over Hilbert spaces, where both derivatives and adjoints retain the data parallelism
in their input functions.

Why Hilbert spaces? A Hilbert space is a vector space that is equipped with an inner product (cid:12) and
is metrically complete. The inner product is crucial: it establishes an isomorphism with the dual space
such that ordinary ﬁrst-order vectors can be used to represent linear functionals rather than having to
code these as procedures in a programming language. This representation trick is the essence of adjoints,
which run linear functions in reverse, from output differential to input differential and, in particular,
compute gradients as the input differentials resulting from a single evaluation of the adjoint derivative to
the output differential 1.

The metric completness is, in some sense, irrelevant: it only pops up in the deﬁnition of Fr´echet

derivative and checking that it constitutes a valid model of the differentiation rules.

16

Combinatory Adjoints and Differentiation

Why symbolic tensor products? We could deﬁne the tensor product of Rm and Rn to be the matrix
space Rm×n, but matrices as data structures for derivatives are too inefﬁcient for large values of m, n.
Symbolic tensor decompositions can provide more efﬁcient representations [16].

For example, Griewank [19] gives f : Rn → Rm deﬁned by

f (x) = b sin(aT x)

with a ∈ Rn and b ∈ Rn as an example where computing the Jacobian derivative of f at x0 requires m · n
multiplications, whereas the original function requires only n + m multiplications. But this is only due
to insisting on representing the derivative as a Jacobian matrix. Translated into combinatory form and
employing our differentiation rules we arrive at a corresponding representation of the matrix as

c · (b ⊗ a)

where c = cos(aT x0). Note that this is the output: itt uses symbolic scalar product and tensor product
operators. This representation can be computed using only n scalar multiplications. Furthermore, ap-
plying it to a vector dx ∈ Rn produces the term d · b where d = c · (a (cid:12) dx), which requires only n + 1
multiplications. Using the matrix equivalent of c · (b ⊗ a) takes m · n multiplications.

8.1 Related work

The origin of symbolic differentiation using electronic computers for functions on scalar variables dates
back to the 1950s [30]. Forward-mode AD for scalar variables was discovered independently by a num-
ber of researchers in the 1950s and 1960s [21]. The history of reverse-mode AD dates back to the early
1970s and is surveyed by Griewank [19]. Linnainmaa [34, 35] observed early on that reverse-mode AD
on scalar variables consists of building a computation graph [2] and then reversing its dependency arrows,
which is tantamount to transposing sparse matrices in a sequential composition of matrix multiplications.
This observation has since been made repeatedly in both elemental and tensor settings.

Derivatives of functions on scalar variables are conventionally represented by Jacobian matrices.
Computing the matrix with a minimum number of steps is NP-hard, however [40]. As we have shown,
matrices are often not even a good data structure for derivatives, however. As we have shown, they can be
represented more compactly and efﬁciently using a combinatory language for linear functions, including
symbolic operators for scalar multiplication, addition and tensor product [37, 25].

Functional languages have served well for exploring AD techniques both as a host language for
capturing AD techniques [31, 32], and as the language under investigation, featuring, for instance, multi-
variate functions, higher-dimensional data, higher-order functions, higher-degree differentiation (e.g.,
through a lazy inﬁnite tower of derivatives) [15, 22], and even differentiation of formal languages [14].
Whereas many of the above-mentioned features are well-suited for forward-mode AD (no memoization
of primal values is needed), capturing the essence of reverse-mode AD has proven difﬁcult. We believe
this is due to using λ -calculus formulation [45] rather than a combinatory formulation, representing the
“tape” as (the code of) a function or procedure [13, 51] and/or employing matrices to represent linear
functions instead of asymptotically more compact and efﬁcient data structures made possible by symbolic
tensor products and useful constants (identity, projections and injections).

Reverse-mode AD for higher-order languages has been studied by Mazza [38] and on capturing
reverse-mode AD by building a library for functional representation general differentiation based on the
speciﬁcations of the functionality . Our work follows Elliott’s [13] lead:4 It is also based on adjoint afﬁne

4While inspired by Elliott’s elegant presentation of Fr´echet derivatives in a Haskell framework [15], our work on functional-
analysis based AD started in 2015 and developed independently of Elliott’s work, but has so far remained unpublished except
for a presentation at a Workshop in honor of Tom Reps’s 60th birthday in 2016 [24].

Elsman, Henglein, Kaarsgaard, Mathiesen, Schenck

17

interpretation, but additionally employs specialized and efﬁcient representations for linear functions,
supports sums, tensor products and copowers, avoids expression swell, identiﬁes and exploits general
differentiation rules for bilinear operators, and supports relational reduction and parallel composition.

Other work has focused on exploring how AD techniques can be applied in a functional parallel set-
ting while preserving the parallel properties of functions, also in the differentiated code [43, 44]. Recent
work [46] on reverse- and forward-mode AD operators embedded in Futhark [26] has shown that even
nested parallelism can be handled in reverse mode effectively with excellent GPU-utilization and perfor-
mance. Our combinatory language features powerful parallel operations and general differentiation and
adjoint rules that retain semantic data parallelism, but does not devise a general implementation method
for compact representation of relations and efﬁcient parallel implementation of relational reduction. This
is future work.

An approach to combinatory differentiation based on category theory rather than linear algebra is
differential categories [6, 10] and its many variations (e.g., [7, 9, 11]). In brief, a differential category
is an additive monoidal category with a differential combinator and a modality allowing differentiable
morphisms to be identiﬁed by their signature. The variation most closely resembling the one presented
here is that of reverse derivative categories [11], as they can be thought of as categories of smooth maps
equipped with the ability to take adjoints (a “dagger”). This approach is ultimately closer to symbolic
differentiation rather than AD, though it could be interesting to integrate our approach into a notion of
differential category with a distinction between semantic and syntactic data (see also [12]).

The categorical semantics of both forward and reverse mode AD with higher types was recently given
a uniﬁed treatment in [50], with models based on so-called biadditive categories: indexed categories
with biproducts at each index, preserved by reindexing. Interestingly, when applying the Grothendieck
construction (cid:82) (−) to a biadditive category C, the resulting ﬁbred category (cid:82) C describes forward mode
AD, while its dual (cid:82) Cop describes reverse mode AD. This highlights the formal connection between
duality (via adjoints) and reverse mode AD, as also argued in [11] and in Section 6.2.

On the practical side, a variety of systems provide tooling for automatically differentiating source
code. These tools include (but are far from limited to) Python tools such as Autograd [36], JAX [8,
47], C/C++ tools such as Adept [27], ADOL-C [20] and Tapenade [23], DiffSharp for F# [3], tools for
MATLAB [5, 41], Julia [28], FutharkAD [46] and even tools for the LLVM IR [39]. Most of these
tools feature both forward-mode and backward-mode AD and are therefore applicable for a variety of
domains and applications, including physics simulation [47], ﬁnance [22, 4, 17] and economics [49].
AD has also received renewed attention due to its application to deep learning, where backpropagation
is reverse-mode AD for scalar functions, as shown in Example 7.1. AD techniques have therefore been
incorporated, either directly or indirectly (through library APIs), into most of the major general machine
learning frameworks, including Caffe [29], TensorFlow [1], and PyTorch [42]. For a general overview,
consult [3]. Work has also been done at benchmarking many of the commonly used AD tools [48].

Acknowledgements. This work was made possible by Independent Research Fund Denmark grants FUTHARK:
Functional Technology for High-performance Architectures, Deep Probabilistic Programming for Protein Struc-
ture Prediction (DPP), and DFF–International Postdoc 0131-00025B. We would like to thank Gabriele Keller, Ken
Friis Larsen and Dimitrios Vytionitis for collaborative discussions over the last six years that have greatly helped
in developing the foundations of combinatory differentiation and our colleagues on FUTHARK and DPP, in par-
ticular Cosmin Oancea, Troels Henriksen, Thomas Hamelryck and Ola Rønning. Furthermore, the second author
would like to thank Conal Elliott for stimulating exchanges on AD in the period he was working on his ICFP 2018
paper [13]. We greatly appreciate and thank the three anonymous referees for their recommendations.

18

References

Combinatory Adjoints and Differentiation

[1] Martın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado,
Andy Davis, Jeffrey Dean, Matthieu Devin et al. (2016): Tensorﬂow: Large-scale machine learning on
heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, doi:10.48550/ARXIV.1603.04467.

[2] F. L. Bauer (1974): Computational Graphs and Rounding Error. SIAM Journal on Numerical Analysis

11(1), pp. 87–96, doi:10.1007/BF01386233.

[3] Atilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul & Jeffrey Mark Siskind
arXiv:1502.05767 [cs, stat],

(2018): Automatic Differentiation in Machine Learning: A Survey.
doi:10.48550/ARXIV.1502.05767. arXiv:1502.05767.

[4] C. H. Bischof, H. M. B¨ucker & B. Lang (2002): Automatic Differentiation for Computational Finance. In E. J.
Kontoghiorghes, B. Rustem & S. Siokos, editors: Computational Methods in Decision-Making, Economics
and Finance, chapter 15, Applied Optimization 74, Kluwer Academic Publishers, Dordrecht, pp. 297–310,
doi:10.1007/978-1-4757-3613-7 15.

[5] Christian H. Bischof, H. Martin B¨ucker, Bruno Lang, Arno Rasch & Andre Vehreschild (2002): Com-
bining Source Transformation and Operator Overloading Techniques to Compute Derivatives for MAT-
LAB Programs.
In: Proceedings of the Second IEEE International Workshop on Source Code Anal-
ysis and Manipulation (SCAM 2002), IEEE Computer Society, Los Alamitos, CA, USA, pp. 65–72,
doi:10.1109/SCAM.2002.1134106.

[6] R. F. Blute, J. R. B. Cockett & R. A. G. Seely (2006): Differential categories. Mathematical Structures in

Computer Science 16(6), pp. 1049–1083, doi:10.1017/S0960129506005676.

[7] R. F. Blute, J. R. B. Cockett & R. A. G. Seely (2009): Cartesian differential categories. Theory and Appli-

cations of Categories 22(23), pp. 622–672.

[8] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin,
George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne & Qiao Zhang (2018): JAX: Com-
posable Transformations of Python+NumPy Programs. Available at http://github.com/google/jax.

[9] J. R. B. Cockett, G. S. H. Cruttwell & J. D. Gallagher (2011): Differential restriction categories. Theory and

Applications of Categories 25(21), pp. 537–613, doi:10.48550/ARXIV.1208.4068.

[10] J. R. B. Cockett & J.-S. Lemay (2017): There Is Only One Notion of Differentiation. In Dale Miller, editor:
2nd International Conference on Formal Structures for Computation and Deduction (FSCD 2017), Leibniz
International Proceedings in Informatics (LIPIcs) 84, Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik,
pp. 13:1–13:21, doi:10.4230/LIPIcs.FSCD.2017.13.

[11] R. Cockett, G. Cruttwell, J. Gallagher, J.-S. Pacaud Lemay, B. MacAdam, G. Plotkin & D. Pronk (2020):
Reverse Derivative Categories. In M. Fern´andez & A. Muscholl, editors: 28th EACSL Annual Conference
on Computer Science Logic (CSL 2020), Leibniz International Proceedings in Informatics (LIPIcs) 152,
Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik, pp. 18:1–18:16, doi:10.4230/LIPIcs.CSL.2020.18.

[12] O. Danvy (1996): Type-Directed Partial Evaluation.

In: Proceedings of the 23rd ACM SIGPLAN-
SIGACT Symposium on Principles of Programming Languages (POPL ’96), ACM, pp. 242–257,
doi:10.1145/237721.237784.

[13] Conal Elliott (2018): The simple essence of automatic differentiation. Proceedings of the ACM on Program-

ming Languages 2(ICFP), p. 70, doi:10.1145/355586.364791.

[14] Conal Elliott (2021): Symbolic and automatic differentiation of languages. Proceedings of the ACM on

Programming Languages 5(ICFP), pp. 1–18, doi:10.1016/S0019-9958(61)80020-X.

[15] Conal M. Elliott (2009): Beautiful Differentiation. In: Proceedings of the 14th ACM SIGPLAN International
Conference on Functional Programming, ICFP ’09, Association for Computing Machinery, New York, NY,
USA, pp. 191–202, doi:10.1145/1596550.1596579.

Elsman, Henglein, Kaarsgaard, Mathiesen, Schenck

19

[16] Patrick Gelß (2017): The Tensor-Train Format and Its Applications: Modeling and Analysis of Chemical Re-
action Networks, Catalytic Processes, Fluid Flows, and Brownian Dynamics. Ph.D. thesis, Freie Universit¨at
Berlin.

[17] Michael B. Giles & Paul Glasserman (2006): Smoking Adjoints: fast evaluation of Greeks in Monte Carlo
calculations. In Chris Kenyon & Andrew Green, editors: Landmarks in XVA: From Counterparty Risk to
Funding Costs and Capital, chapter 25, Risk books, Infopro digital, Houndsditch, London.

[18] Andreas Griewank (1989): On Automatic Differentiation. In: Mathematical Programming: Recent Develop-

ments and Applications, pp. 83–108. ISBN 978-0792304906.

[19] Andreas Griewank (2012): Who invented the reverse mode of differentiation. Documenta Mathematica, Extra

Volume ISMP, pp. 389–400.

[20] Andreas Griewank, David Juedes & Jean Utke (1996): Algorithm 755: ADOL-C: A Package for the Automatic
Differentiation of Algorithms Written in C/C++. ACM Transactions on Mathematical Software 22(2), pp.
131–167. Available at http://doi.acm.org/10.1145/229473.229474.

[21] Andreas Griewank & Andrea Walther (2008): Evaluating derivatives: principles and techniques of algorith-

mic differentiation. Siam, doi:10.1137/1.9780898717761.

[22] Esben Bistrup Halvorsen (2012): Calculating Key Ratios for Financial Products using Automatic Differ-
entiation and Monte Carlo Simulation. Student Project, Department of Computer Science, University of
Copenhagen (DIKU).

[23] Laurent Hascoet & Val´erie Pascual (2013): The Tapenade Automatic Differentiation Tool: Principles, Model,

and Speciﬁcation. ACM Trans. Math. Softw. 39(3), doi:10.1145/2450153.2450158.

[24] Fritz Henglein (2016): Automatic Differentiation: From Functional Analysis to Functional Programming.

Presentation, Reps at Sixty Workshop at Static Analysis Symposium.

[25] Fritz Henglein, Robin Kaarsgaard & Mikkel Kragh Mathiesen (2022): The Programming of Algebra. In:
Proc. 9th Workshop on Mathematically Structured Functional Programming (MSFP), Electronic Proceedings
in Theoretical Computer Science (EPTCS), Munich, Germany.

[26] Troels Henriksen, Niels G. W. Serup, Martin Elsman, Fritz Henglein & Cosmin E. Oancea (2017): Futhark:
Purely Functional GPU-programming with Nested Parallelism and In-place Array Updates. In: Proceedings
of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI 2017,
ACM, New York, NY, USA, pp. 556–571, doi:10.1145/3062341.3062354.

[27] Robin J. Hogan (2014): Fast Reverse-Mode Automatic Differentiation Using Expression Templates in C++.

ACM Transactions on Mathematical Software 40(4), pp. 26:1–26:24, doi:10.1145/2560359.

[28] Michael Innes (2019): Don’t Unroll Adjoint: Differentiating SSA-Form Programs. arXiv:1810.07951 [cs],

doi:10.48550/ARXIV.1810.07951. arXiv:1810.07951.

[29] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadar-
rama & Trevor Darrell (2014): Caffe: Convolutional Architecture for Fast Feature Embedding.
In: Pro-
ceedings of the 22nd ACM International Conference on Multimedia, MM ’14, Association for Computing
Machinery, New York, NY, USA, pp. 675–678, doi:10.1145/2647868.2654889.

[30] Harry G Kahrimanian (1953): Analytical differentiation by a digital computer. MA Thesis, Temple Univer-

sity.

[31] Jerzy Karczmarczuk (1998): Functional Differentiation of Computer Programs.

In: Proceedings of the
Third ACM SIGPLAN International Conference on Functional Programming, ICFP ’98, Association for
Computing Machinery, New York, NY, USA, pp. 195–203, doi:10.1145/289423.289442.

[32] Jerzy Karczmarczuk (1999): Functional Coding of Differential Forms. In: Scottish Workshop on Functional

Programming. ISBN 978-1-84150-024-9.

[33] S¨oren Laue (2019): On the equivalence of forward mode automatic differentiation and symbolic differentia-

tion. arXiv preprint arXiv:1904.02990, doi:10.48550/arXiv.1904.02990.

20

Combinatory Adjoints and Differentiation

[34] Seppo Linnainmaa (1970): The representation of the cumulative rounding error of an algorithm as a Taylor

expansion of the local rounding errors. Master’s Thesis (in Finnish), Univ. Helsinki, pp. 6–7.

[35] Seppo Linnainmaa (1976): Taylor Expansion of the Accumulated Rounding Error. BIT 16(2), pp. 146–160,

doi:10.1007/BF01931367.

[36] Dougal Maclaurin (2016): Modeling, Inference and Optimization with Composable Differentiable Proce-

dures. Ph.D. thesis, Harvard University.

[37] Mikkel Kragh Mathiesen (2016): Inﬁnite-Dimensional Linear Algebra for Efﬁcient Query Processing. Mas-

ter’s thesis, Department of Computer Science, Unversity of Copenhagen (DIKU).

[38] Damiano Mazza & Michele Pagani (2021): Automatic Differentiation in PCF. Proceedings of the ACM on

Programming Languages 5(POPL), pp. 1–27, doi:10.1145/3434309. arXiv:2011.03335.

[39] William S. Moses, Valentin Churavy, Ludger Paehler, Jan H¨uckelheim, Sri Hari Krishna Narayanan, Michel
Schanen & Johannes Doerfert (2021): Reverse-Mode Automatic Differentiation and Optimization of GPU
Kernels via Enzyme.
In: Proceedings of the International Conference for High Performance Computing,
Networking, Storage and Analysis, SC ’21, Association for Computing Machinery, New York, NY, USA,
doi:10.1145/3458817.3476165.

[40] Uwe Naumann (2007): Optimal Jacobian Accumulation Is NP-Complete. Mathematical Programming

112(2), pp. 427–441, doi:10.1007/s10107-006-0042-z.

[41] Richard D. Neidinger (2010): Introduction to Automatic Differentiation and MATLAB Object-Oriented Pro-

gramming. SIAM Review 52(3), pp. 545–563, doi:10.1137/080743627.

[42] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin,
Alban Desmaison, Luca Antiga & Adam Lerer (2017): Automatic Differentiation in PyTorch. In: Proc. 31st
Conference on Neural Information Processing Systems (NIPS).

[43] Adam Paszke, Daniel Johnson, David Duvenaud, Dimitrios Vytiniotis, Alexey Radul, Matthew
Index Sets
arXiv:2104.05372 [cs],

Jonathan Ragan-Kelley & Dougal Maclaurin (2021): Getting to the Point.

Johnson,
and Parallelism-Preserving Autodiff
doi:10.48550/ARXIV.2104.05372. arXiv:2104.05372.

for Pointful Array Programming.

[44] Adam Paszke, Matthew J. Johnson, Roy Frostig & Dougal Maclaurin (2021): Parallelism-Preserving Au-
tomatic Differentiation for Second-Order Array Languages.
In: Proceedings of the 9th ACM SIGPLAN
International Workshop on Functional High-Performance and Numerical Computing, FHPNC 2021, Associ-
ation for Computing Machinery, New York, NY, USA, pp. 13–23, doi:10.1145/3471873.3472975.

[45] Barak A. Pearlmutter & Jeffrey Mark Siskind (2008): Reverse-Mode AD in a Functional Framework:
Lambda the Ultimate Backpropagator. ACM Transactions on Programming Languages and Systems 30(2),
pp. 1–36, doi:10.1145/1330017.1330018.

[46] Robert Schenck, Ola Rønning, Troels Henriksen & Cosmin E. Oancea (2022): AD for an Array Language

with Nested Parallelism, doi:10.48550/arXiv.2202.10297. arXiv:2202.10297.

[47] Samuel Schoenholz & Ekin Dogus Cubuk (2020): JAX MD: A Framework for Differentiable Physics. In
H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan & H. Lin, editors: Advances in Neural Information
Processing Systems, 33, Curran Associates, Inc., pp. 11428–11441. Available at https://proceedings.
neurips.cc/paper/2020/file/83d3d4b6c9579515e1679aca8cbc8033-Paper.pdf.

[48] Filip ˇSrajer, Zuzana Kukelova & Andrew Fitzgibbon (2018): A Benchmark of Selected Algorithmic Dif-
ferentiation Tools on Some Problems in Computer Vision and Machine Learning. arXiv:1807.10129 [cs],
doi:10.48550/arXiv.1807.10129. arXiv:1807.10129.

[49] E. M. Tadjouddine (2009): Algorithmic Differentiation Applied to Economics.

In S. I. Ao, O. Castillo,
C. Douglas, D. D. Feng & J.-A. Lee, editors: Proceedings of the of the International MultiConference of
Engineers and Computer Scientists 2009 (IMECS 2009), Hong Kong, March 18–20, 2009, 2, International
Association of Engineers, Newswood Limited, pp. 2199–2204.

Elsman, Henglein, Kaarsgaard, Mathiesen, Schenck

21

[50] Matthijs V´ak´ar (2021): Reverse AD at Higher Types: Pure, Principled and Denotationally Correct.

In
Nobuko Yoshida, editor: ESOP 2021: Programming Languages and Systems, Springer, pp. 607–634,
doi:10.1007/978-3-030-72019-3 22.

[51] Fei Wang, Daniel Zheng, James Decker, Xilun Wu, Gr´egory M. Essertel & Tiark Rompf (2019): Demystify-
ing Differentiable Programming: Shift/Reset the Penultimate Backpropagator. Proceedings of the ACM on
Programming Languages 3(ICFP), pp. 1–31, doi:10.1145/3341700.

[52] R. E. Wengert (1964): A Simple Automatic Derivative Evaluation Program. Communications of the ACM

7(8), pp. 463–464, doi:10.1145/355586.364791.

A Derivatives

Informally, the derivative of a function f at a particular input value x is a mathematical object that
describes how inﬁnitesimal changes dx to x incur changes dy to the result y = f (x) of f at x. There
are multiple notions of increasing generality and abstraction in mathematics that make “describing”,
“inﬁnitesimal” and “changes” precise.

A.1 Leibniz derivative

For a scalar function of one (scalar) variable f : R → R, the Leibniz derivative of f at x is the number
a ∈ R that satisﬁes

f (x + dx) ≈ f (x) + a · dx
where · is multiplication on R and ≈ expresses that the error on the right-hand side vanishes as dx
becomes inﬁnitesimally small. Speciﬁcally, a is the derivative of f at x if

lim
|dx|→0

| f (x + dx) − ( f (x) + a · dx)|
|dx|

= 0.

For example, for f (x) = x2 we have that 8 is the derivative of f at 4, and 14 is the derivative of f at 7.

The Leibniz derivative of f is the function f (cid:48) that maps x to the derivative of f at x. For example, for

f (x) = x2 we have f (cid:48)(x) = 2 · x.

A.2

Jacobi derivative

For a vector-valued function f : Rn → Rm, the Jacobi derivative of f at v is the m × n-matrix M that
satisﬁes

f (v + dv) ≈ f (v) + M (cid:63) dv.

Here (cid:63) is matrix/vector multiplication and ≈ generalizes the case of scalar functions:

lim
||dv||→0

|| f (v + dv) − ( f (v) + M (cid:63) dv)||
||dv||

= 0

where || . . . || is the Euclidean norm. We call Mi j, the (i, j)-th entry of M, the partial derivative of the j-th
output of f with respect to its i-th input at v.

For example, for

f



 =

(cid:21)

(cid:20)x1 + x2
x1 · x3

: R3 → R2





x1
x2
x3

22

Combinatory Adjoints and Differentiation

the matrix

(cid:21)

(cid:20) 1
1 0
−2 0 4

is the derivative of f at



.





4
0
−2

The Jacobi derivative of f is the function f (cid:48) that maps a vector v to the Jacobi derivative of f at v.

For example, for f as above we have

f (cid:48)(x1, x2, x3) =

(cid:20) 1
0
1
x3 0 x1

(cid:21)

.

The partial derivative of the j-th output of f with respect to its i-th input is the function ∂ fi j(v) =

f (cid:48)(v)i j. This is usually written ∂ f j
∂ xi

or even ∂ y j
∂ xi

.5

The Leibniz derivative is the special case of a Jacobi derivative for m = n = 1.

A.3 Fr´echet derivative

Jacobi derivatives are restricted to functions of the form f : Rn → Rm, that is, ﬁnite-dimensional Eu-
clidean spaces over the real numbers. Sometimes it is convenient or even necessary to write functions
where inputs are not tuples of scalars, but elements of possibly high-dimensional (or even inﬁnite-
dimensional) vector spaces.
Example A.1. A layer of a neural network is parameterized by a weight matrix W ∈ Rm×n and bias
vector b ∈ Rm and takes a data vector v ∈ Rn as input, where |m|, |n| (cid:29) 0 may be in the millions or
billions. It can be deﬁned in a single line by

g(W, b, v) = map h (W (cid:63) v + b)

where map h applies h to each element of a vector, h : R → R is an activation function such as tanh, (cid:63) is
matrix/vector multiplication and + is vector addition. It can be straightforwardly evaluated using data-
parallel implementations of these operations. Trying to write such a function using only scalar variables
would be a bad idea for multiple reasons.

Deﬁnition A.2 (Fr´echet derivative). For a function f : V → W on Banach spaces V,W , the linear function
A ∈ V (cid:40) W is the Fr´echet derivative of f at v if it satisﬁes

that is

f (v + dv) ≈ f (v) + A(dv),

lim
||dv||V →0

|| f (v + dv) − ( f (v) + A(dv))||W
||dv||V

= 0

where || . . . ||U is the norm that comes with the Banach space U.

The Fr´echet derivative of f : V → W is the partial function f (cid:48) : V → (V (cid:40) W ) that maps a vector

v ∈ V to the Fr´echet derivative of f at v.

Example A.3. Consider the function

5We avoid this notation since the choi.ce of variable names for inputs and outputs of a function has nothing to do with the

notion of derivative.

gW,b(v) = map h (W (cid:63) v + b).

Elsman, Henglein, Kaarsgaard, Mathiesen, Schenck

23

Its Fr´echet derivative is

gW,b

(cid:48)(v) = ∆(map h(cid:48) (W (cid:63) v + b)) • (W (cid:63))

where ∆( f1, . . . fm)(x1, . . . , xm) = ( f1(x1), . . . , fm(xm)) is zip-apply and • is linear function composition.
For h = tanh we have

tanh(cid:48)(x) = ((1 − tanh2(x))·).

Note that tanh(cid:48)(x) : R (cid:40) R, which explains the use of the section notation ((1 − tanh2(x))·). Since
R (cid:40) R is isomorphic to R, the Leibniz derivative is tanh(cid:48)(x) = 1 − tanh2(x) via implicit application of
the isomorphism to its Fr´echet derivative.6 In particular we can rewrite gW,b

(cid:48) as

gW,b

(cid:48)(v) = ∆(map h(cid:48) (W (cid:63) v + b)) • (W (cid:63))

= ∆(map (λ x.(1 − tanh2(x))·)(W (cid:63) v + b)) • (W (cid:63))
= zipWith(·)(map(λ x.(1 − tanh2(x)))(W (cid:63) v + b)) • (W (cid:63))

Note that the right-hand side is built by composing linear functions into a linear function: zipWith(·) is
bilinear and thus zipWith(·)(map(λ x.(1 − tanh2(x)))) is linear; likewise, (cid:63) is bilinear and thus the section
(W (cid:63)) deﬁned by (W (cid:63))(v) = W (cid:63) v is linear; and functional composition of linear functions by • preserves
linearity.

We can η-expand the combinatory expression on the right-hand side into a more familiar looking

term representation:

gW,b

(cid:48)(v)(dv) = zipWith(·) (map(λ x.(1 − tanh2(x)))(W (cid:63) v + b)) (W (cid:63) dv)

This holds for any dimensions of W and can be computed entirely symbolically, based on general differ-
entiation rules for composition (chain rule), constant, linear and bilinear function, general second-order
operators such as map and a dictionary of derivatives for primitive functions such as tanh.

The Fr´echet derivative generalizes the Jacobi derivative. If M is the Jacobi derivative of f at x then

A = (M(cid:63)) is its Fr´echet derivative.

A.4 Gateaux derivative

Deﬁnition A.4 (Gateaux differential, Gateaux derivative). For a function f : V → W on Banach spaces
V,W over ﬁeld K (either R or C), v an interior point of V , dv ∈ V an input differential, the output
differential dy ∈ W is the Gateaux differential of f at v in the direction dv if

dy = lim
t→0

|| f (v + t · dv) − f (v)||W
||t||K

where || . . . ||W is the norm that comes with the Banach space W and the underlying ﬁeld K, respectively.
The Gateaux derivative of f : V → W is the partial function f (cid:48) : V ×V → W that maps v, dv ∈ V to

the Gateaux differential f (cid:48)(v, dv).

Function f is Gateaux differentiable at v if its Gateaux differential exists at v for all directions dv.

6The pleasant compositional nature and applicability of Fr´echet derivatives arrives from not performing this isomorphism
such that the chain rule is always functional composition of linear functions, no matter which vector space the arguments and
results of functions belong to.

24

Combinatory Adjoints and Differentiation

Gateaux derivatives generalize directional derivatives in multivariate analysis analogous to Fr´echet
derivatives generalizing total derivatives. They are more general than Fr´echet derivatives in the sense
that a function may be Gateaux differentiable at v without also being Fr´echet differentiable, but if the
Fr´echet derivative exists then it determines the Gateaux derivative:

Gateaux(v, dv) = f (cid:48)
f (cid:48)

Fr´echet(v)(dv).

There are more general notions of derivatives. Some distinguish the spaces of vectors and differen-
tials, some apply to continuous functions that that are not conventionally differentiable everywhere such
as the absolute-value function f (x) = |x|. For the purposes of this paper, Fr´echet and Gateaux derivatives
are sufﬁcient.

A.5 Gateaux versus Fr´echet derivatives for automatic differentiation

Gateaux derivatives are conceptually the basis of forward-mode automatic differentiation, since they give
rise to interpreting a term (or program) representation of a function as operating on dual numbers/dual
tensors (v, dv):

f [fad]
Gateaux(v, dv) = ( f (v), f (cid:48)(v, dv)),

which preserves functional composition

(g ◦ f )[fad]

Gateaux = g[fad]

Gateaux ◦ f [fad]

Gateaux

and is thus easy to implement by replacing the ordinary implementation of numbers and tensors by dual
numbers and tensors, respectively. This camouﬂages, however, that the ﬁrst component, called the primal
value, of the output only depends on the primal value of the input and that the second component, called
the tangent value, always depends linearly on the input tangent value. These universal properties can
be partially recovered by partially evaluating f [fad] with static v and dynamic dv, which, by deﬁnition
of f [fad], always succeeds with statically computing the primal value and leaving a partially evaluated
program representing the derivative behind, but rendered in an expressive programming language that
does not inherently capture that this is always a linear function. For example, in elemental and tensor-
based automatic differentiation, the partially evaluated output will result in a data structure corresponding
to a computation graph [2, 35, 34], also called tape7 or trace. Reversing its edges amounts to forming
the adjoint of the linear function represented by the computation graph.

First employing Gateaux derivatives underlying the dual number/tensor interpretation of a program
just to recover Fr´echet derivatives by partial evaluation seems like an unnecessary detour. Following
Henglein [24] and Elliott [13], we argue that Fr´echet derivatives are better suited for both symbolic and
automatic differentiation since they capture and reify that the output value of a function only depends on
its input value and the output differential depends on both input value and input differential, but always
linearly on the input differential:

f [fad]
Fr´echet(v) = ( f (v), f (cid:48)

Fr´echet(v)) ∈ W × (V (cid:40) W ).

Since the derivative is always a linear function, it can be represented in a combinatory domain-speciﬁc
language (DSL) that is closed under linear functions and thus syntactically guarantees linearity of all

7It is sometimes referred to as a Wengert tape, which we ﬁnd surprising since Wengert describes only forward-mode AD in

his 2-page article [52].

Elsman, Henglein, Kaarsgaard, Mathiesen, Schenck

25

constructed functions. This facilitates not only universal applicability of properties of linear functions,
such as f (x + y) = f (x) + f (y) for any f , but also symbolically computing adjoints, which are only
deﬁned for linear functions. Linear functions generated during differentiation or adjoint differentiation
(generating the adjoint of the derivative during differentiation) can be represented as ordinary functions
(λ -abstractions) [13], of course, but this eliminates the possibility of subsequent optimization using linear
and tensor algebra.

Executing adjoint derivatives, possibly after algebraic optimization, as ordinary functions (programs)
provides computation of “cheap” gradients [19] for scalar functions. We believe that the functional-
analysis based approach including tensor products in this paper provides an implementation and data
structure framework for not only cheap parallel computation of gradients for scalar functions, but also for
cheap adjoint derivatives for non-scalar functions, where tensor decomposition (formal tensor products),
tensor contraction and relational reduction have important roles to play.

B Applications

We consider a series of examples of increasing complexity to illustrate how the functions are represented
in point-free notation and how their derivative is computed symbolically.

Example B.1. Let h(x) = ln(sin x), that is h = ln ◦ sin in point-free notation. Thus

h(cid:48)(x) = ln(cid:48)y • sin(cid:48)(x)

where y = sin x by Rule 1, the chain rule. Since ln(cid:48)y = (· 1

y ) and sin(cid:48)x = (· cos x) we get

h(cid:48)(x) = (ln ◦ sin)(cid:48)(x)

= ln(cid:48)(sin x) • sin(cid:48)(x)

= (

= (

= (

1
sin x
1
sin x
cos x
sin x

·) • (cos x ·)

· cos x ·)

·)

Example B.2. Consider y = f (x1, x2) = ln(x1) + x1 · x2 − sin(x2) [3, p.9]. It can be written in point-free
form as

f = ln ◦π1 + π1ˆ·π2 − sin ◦π2

26

Combinatory Adjoints and Differentiation

where ˆ· is · lifted to functions: ( f ˆ·g)(x) = f (x) · g(x). Employing our rules of differentation we obtain

f (cid:48)(x1, x2) = (ln ◦π1 + π1ˆ·π2 − sin ◦π2)(cid:48)(x1, x2)

= (ln ◦π1)(cid:48)(x1, x2) + (π1ˆ·π2)(cid:48)(x1, x2) − (sin ◦π2)(cid:48)(x1, x2)
(cid:48)(x1, x2) +
= ln(cid:48)(π1(x1, x2)) • π1
(cid:48)(x1, x2)) + (· π2(x1, x2)) • π1
(π1(x1, x2)) ·) • π2
(cid:48)(x1, x2)
sin(cid:48)(π2(x1, x2)) • π2

(cid:48)(x1, x2) −

= ln(cid:48)(x1) • π1 +

(x1 ·) • π2 + (· x2) • π1 −
sin(cid:48)(x2) • π2

= (

·) • π1 +

1
x1
(x1 ·) • π2 + (· x2) • π1 −
(cos x2 ·) • π2

The last line is easily transformed into a familiar looking expressing by recognizing that ∂ x1 = (•π1) and
∂ x2 = (•π2):

f (cid:48)(x1, x2)(∂ x1, ∂ x2) = (

·)(∂ x1) + (x1 ·)(∂ x2) + (· x2)(∂ x1) − (cos x2 ·)(∂ x2)

1
x1
1
x1

=

· ∂ x1 + x1 · ∂ x2 + ∂ x1 · x2 − cos x2 · ∂ x2

In Leibniz notation, we can get the partial derivative f (cid:48)(x1,x2)
the righthand side by ∂ x1. Aanalogously we can compute f (cid:48)(x1,x2)
get the familiar looking:

∂ x1

∂ x2

by setting ∂ x2 = 0 and formally dividing
. Finally, writing ∂ f instead of f (cid:48) we

∂ f (x1, x2)
∂ x1
∂ f (x1, x2)
∂ x2

=

1
x1

+ x2

= x1 − cos x2

The derivative for the unreadable version of f is as follows:

f (cid:48)(x1, x2) = ((+) ◦ ((+) × ((−) ◦ sin)) ◦ (cid:104)(cid:104)ln ◦ π1, (·)(cid:105), π2(cid:105))(cid:48)(x1, x2)
= ( +(cid:48) ) ◦ (( +(cid:48) ) × (−(cid:48) ◦ sin(cid:48)x2)) ◦ (cid:104)(cid:104)ln(cid:48)x1 ◦ π1

(cid:48) , x1·(cid:48)x2(cid:105), π2

(cid:48) (cid:105)

= (+) ◦ ((+) × ((−) ◦ (cos x2·))) ◦ (cid:104)(cid:104)(

1
x1

·) ◦ π1, (+) ◦ ((x2·) × (x1·))(cid:105), π2(cid:105)

Since the right-hand side in this example is constructed from linear functions and combinators that
preserve linearity we can see that it is linear, as it should be. (The derivative at any point is linear by
deﬁnition.) More speciﬁcally, the code for the derivative is parametric in x1, x2: it is the same for each
point (x1, x2) ∈ R × R. It is not a deﬁnitional requirement that the derivative at one point have the same
expression as the derivative at another point, but when it does it is useful in practice: If we need to
compute the derivative at many different points and the derivative is described by the same program at
each point, it makes sense to optimize that program prior to executing it. What makes this extra intriguing
is that derivatives, always denoting linear functions, are essentially ﬁrst-order data with strong algebraic
properties admitting powerful optimizations.

