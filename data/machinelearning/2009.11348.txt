A Sample-Efﬁcient Algorithm for Episodic Finite-Horizon MDP with
Constraints

Krishna C. Kalagarla, Rahul Jain, Pierluigi Nuzzo
Ming Hsieh Department of Electrical and Computer Engineering, University of Southern California, Los Angeles
kalagarl,rahul.jain,nuzzo

Email:

{

@usc.edu
}

0
2
0
2

p
e
S
3
2

]

G
L
.
s
c
[

1
v
8
4
3
1
1
.
9
0
0
2
:
v
i
X
r
a

Abstract

Constrained Markov Decision Processes (CMDPs) formalize sequential decision-making problems whose objective is to
minimize a cost function while satisfying constraints on various cost functions. In this paper, we consider the setting of
episodic ﬁxed-horizon CMDPs. We propose an online algorithm which leverages the linear programming formulation of ﬁnite-
horizon CMDP for repeated optimistic planning to provide a probably approximately correct (PAC) guarantee on the number of
episodes needed to ensure an ǫ-optimal policy, i.e., with resulting objective value within ǫ of the optimal value and satisfying
the constraints within ǫ-tolerance, with probability at least 1 − δ. The number of episodes needed is shown to be of the
order ˜O(cid:0) |S||A|C2H 2
δ (cid:1), where C is the upper bound on the number of possible successor states for a state-action pair.
Therefore, if C ≪ |S|, the number of episodes needed have a linear dependence on the state and action space sizes |S| and
|A|, respectively, and quadratic dependence on the time horizon H.

log 1

ǫ2

I. INTRODUCTION

Markov decision processes (MDPs) [1] offer a natural framework to express sequential decision-making problems and
reason about autonomous system behaviors. However, the single cost objective of a traditional MDP formulation may
fall short of fully capturing problems with multiple conﬂicting objectives and additional constraints that must be satisﬁed.
Consider, for example, an autonomous car that is required to reach a destination at the earliest, but also satisfy a set of safety
requirements and fuel consumption constraints, while keeping a desired comfort level [2]. The framework of constrained
MDPs (CMDPs) [3] extended MDPs by considering additional constraints on the expected long-term performance of a
policy. The objective in a CMDP is to minimize the expected cumulative cost while satisfying the additional constraints. In
this paper, we consider episodic ﬁnite-horizon CMDPs, where an agent interacts with a CMDP repeatedly in episodes of
ﬁxed length, a setting that can model a large number of repetitive tasks such as goods delivery or customer service.

We address the problem of online learning of CMDPs with unknown transition probabilities, by requiring only observed
trajectories rather than sampling the transition function for any state-action pair from a generative model, which may not
always be available. An important question which arises in online learning is the exploration-exploitation dilemma, i.e., the
trade-off between exploration, to gain more information about the model, and exploitation, to minimize the cost. In this
respect, the performance of learning algorithms is commonly evaluated in terms of (i) regret, i.e., the difference between
the cumulative cost of the agent and that of the optimal policy in hindsight, and (ii) sample complexity, i.e., the number
of steps for which the learning agent may not play a near-optimal policy. We consider a policy to be near optimal if the
expected cumulative cost is close to the optimal and the constraints are satisﬁed within a small tolerance. In this paper, we
address sample-efﬁciency by proposing an algorithm that provide Probably Approximately Correct (PAC) guarantees.

Our algorithm leverages the concept of optimism-in-the-face-of-uncertainty [4], [5] to balance exploration and exploitation.
The learning agent repeatedly deﬁnes a set of statistically plausible transition models given the observations made so far.
It then chooses an optimistic transition probability model and optimistic policy with respect to the given constrained MDP
problem. This planning step is formulated as a linear programming (LP) problem in occupancy measures, whose solution
gives the desired optimistic policy. This policy is then executed for multiple episodes until a state-action pair has been visited
sufﬁciently often. The total visitation counts are then updated and these steps are repeated.

We show that the number of episodes in which the learning agent plays an ǫ-suboptimal policy is upper bounded by
|S||A|C 2H2
δ, where C is the upper bound on the number of possible successor states
ǫ2

with probability at least 1

−

˜
O
for a state-action pair.

log 1
δ

(cid:1)

(cid:0)
Contribution. In this paper, we present one of the ﬁrst online algorithms with PAC guarantees for episodic constrained
MDPs with unknown transition probabilities. We build on the work of [6] which provides a PAC algorithm for unconstrained
episodic MDPs. However, differently from planning based on the Bellman optimality equations [6], we address the presence
of constraints by formulating an optimistic planning problem as an LP in occupancy measures. Consequently, our formulation
leverages a novel construction for the set of plausible transition models and results in a sample complexity that is quadratic
in the time-horizon H, thus improving on the cubic bounds previously obtained with regret-based formulations (e.g., see [7]).
Related Work. There has been signiﬁcant work on efﬁcient learning for unconstrained MDPs. Algorithms like UCBVI [8],
UBEV [9], EULER [10] and EULER-GP [11] focus on the setting of regret analysis for unconstrained episodic ﬁnite-horizon
MDPs. The setting of PAC algorithms for unconstrained MDPs is addressed by [6], [12], [13]. While these previously
mentioned algorithms are model-based RL algorithms, model-free algorithms UCB-H and UCB-B [14] have also been
shown to be sample efﬁcient.

 
 
 
 
 
 
Sample efﬁcient exploration in CMDPs has recently started to receive attention. The regret analysis for multiple model-
based and model-free algorithms [7] has been performed in the setting of episodic CMDPs with stochastic cost functions
and unknown transition probabilities. Our work addresses PAC complexity, and is therefore complementary to [7]. There
has also been other parallel works on regret analysis for constrained MDPs in the setting of average cost [15], adversarial
cost with tabular MDPs [16] and adversarial cost with linear MDPs [17].

There has also been work on constrained MDPs with stronger requirements. Algorithm C-UCRL [18] has been shown to
have sublinear regret and satisfy the constraints even while learning, albeit in the setting of known transition probabilities
and unknown cost functions. Regret optimal algorithm for constrained MDPs with concave objectives and convex and
hard constraints (knapsacks) (i.e., problems with a ﬁxed budget such that the learning is stopped as soon as the budget is
consumed) is studied by [19]. Several of these regret algorithms can be modiﬁed following an idea from [14] to provide
PAC guarantees for constrained MDP with time-horizon dependence of at least H 3. But, this procedure is impractical as it
entails saving an extremely large number of policies and uniformly sampling them to get the PAC optimal policy.

There are also policy optimization and Lagrangian based works on constrained MDPs [20], [21], [22], [23] but these lack

regret or PAC analysis.

In this section, we introduce preliminary concepts from ﬁnite-horizon MDPs and CMDPs.

II. PRELIMINARIES

A. Notation

We denote the set of natural numbers by N and use h

N to denote time-step inside an episode and
phase index respectively. The indicator function I(s = s1) is 1 when s = s1 and 0 otherwise. The probability simplex over
set S is denoted by ∆S. We use the notation ˜
O

notation but ignores logarithmic factors.

which is similar to the usual

[1 : H] and k

O

∈

∈

B. Finite-Horizon MDPs

|

We consider episodic ﬁnite-horizon MDPs [1], which can be formally deﬁned by a tuple
and

, H, s1, p, c), where
denote the ﬁnite state and action spaces, respectively. The agent interacts with the environment in episodes of
S
A
length H, with each episode starting with the same initial state s1. The non-stationary transition probability is denoted by
s, a) is the the probability of transitioning to state s′ upon taking action a at state s at time step h. Further,
p where ph(s′
we denote by Succ(s, a) the set of possible successor states of state s and action a. The maximum number of possible
. The non-stationary cost of taking action a in state s at time step
successor states is denoted by C = maxs,a |
[1 : H] is a random variable Ch(s, a)
h
∈
∈
A non-stationary randomized policy π = (π1, . . . , πH )
over the action space. We denote by ah ∼
state s
and time step h
∈
π, c, p is omitted) is deﬁned as:

∆A, maps each state to a probability simplex
πh(sh), the action taken at time step h at state sh according to policy π. For a
h (s; c, p) (when clear,

Succ(s, a)
|
[0, 1], with mean ch(s, a). Finally, we set c = c1, . . . , cH .

[1 : H], the value function of a non-stationary randomized policy, V π

Π where πi :

S →

∈ S

= (

M

A

∈

S

,

H

h (s; c, p) = E
V π

ci(si, ai)
|

sh = s, π, p

,

#

"
Xi=h

where the expectation is over the environment and policy randomness. Similarly, for a state s
time step h

[1 : H], the Q-value function is deﬁned as Qπ

h(s, a; c, p) =:

, an action a

and

∈ A

∈ S

∈

ch(s, a) + E

H

"

Xi=h+1

ci(si, ai)
|

sh = s, ah = a, π, p

.

#

∗

∗

There always exists an optimal non-stationary deterministic policy π∗ [1] such that V π
Qπ
h (s, a) = Q∗
backward induction:

h (s) and
h(s, a). The Bellman optimality equations [1] enable us to compute the optimal policy by

h(s, a) = infπQπ

h (s) = infπV π

h (s) = V ∗

V ∗
h (s) = mina∈A
Q∗
h(s, a) = ch(s, a) + ph(
·|

(cid:2)

ch(s, a) + ph(

s, a)V ∗
·|
s, a)V ∗
h+1,

h+1

,

(cid:3)

where V ∗

H+1(s) = 0 and V ∗

h (s) = mina∈AQ∗

h(s, a). The optimal policy π∗ is thus greedy with respect to Q∗
h.

C. Finite-Horizon Constrained MDPs

A ﬁnite-horizon constrained MDP is a ﬁnite-horizon MDP along with additional I constraints [3] expressed by pairs of
[1 : H] with
[0, 1], with mean di,h(s, a). The total expected
[1 : I] is the respective value function from the

constraint cost functions and thresholds,
respect to the ith constraint cost function is a random variable Di,h(s, a)
cost of an episode under policy π with respect to cost functions c, di, i

I
i=1. The cost of taking action a in state s at time step h

di, li}

∈

{

∈
∈

1 (s1; c), V π

initial state s1, i.e., V π
[1 : I] respectively (by deﬁnition). The objective of a CMDP is to ﬁnd a policy
which minimizes the total expected objective cost under the constraint that the total expected constraint costs are below the
respective desired thresholds. Formally,
π∗

1 (s1; di), i

∈

V π
1 (s1; c, p)

argmin
π∈Π

∈

s.t. V π

1 (s1; di, p)

i
li ∀

[1 : I] .

∈
The optimal value is V ∗ = V π
1 (s1; c, p). The optimal policy may be randomized [3], i.e., an optimal deterministic policy
may not exist as in the case of the ﬁnite-horizon MDP. Further, the Bellman optimality equations do not hold due to the
constraints. Thus, we cannot leverage backward induction as before to ﬁnd an optimal policy. A linear programming approach
has been shown [3] to ﬁnd an optimal policy.

≤

∗

Linear Programming for CMDPs: Occupancy measures [3] allow formulating the optimization problem (1) as a linear
program (LP). Occupancy measure qπ of a policy π in a ﬁnite-horizon MDP is deﬁned as the expected number of visits to
a state-action pair (s, a) in an episode at time step h. Formally,

h (s, a; p) = E [I
qπ
{

sh = s, ah = a

}|

s1 = s1, π, p] = P r [sh = s, ah = a

s1 = s1, π, p] .

|

It is easy to see that the occupancy measure qπ of a policy π satisfy the following properties expressing non-negativity and
ﬂow conservation respectively:

(1)

0,

qπ
h (s, a)
≥
qπ
1 (s, a) = π1(a
qπ
h (s, a) =

(s, a, h)
∀
s)I(s = s1),
|
s′a′)qπ
ph−1(s

∈ S × A ×

[1 : H] ,

∀

(s, a)
h−1(s′, a′),

,
∈ S × A

|

s
∀

∈ S

, h

∈

[2 : H] ,

a
X

Xs′,a′

where I(s = s1) is the initial state distribution. The space of the occupancy measures satisfying the above constraints is
denoted by ∆(

). A policy π generates an occupancy measure q

) if

∆(

M

∈

M

πh(a

|

s) =

qh(s, a)
b qh(s, b)

,

(s, a, h)

∀

∈ S × A ×

[1 : H] .

(2)

Thus, there exists a unique generating policy for all occupancy measures in ∆(
) and vice versa. Further, the total expected
cost of an episode under policy π with respect to cost function c can be expressed in terms of the occupancy measure as
follows:

M

P

The optimization problem (1) can then be reformulated as a linear program [3], [24] as follows:

V π
1 (s1; c, p) =

qπ
h (s, a; p)ch(s, a).

Xh,s,a

argmin
q∈∆(M)

q∗

∈

s.t.

Xh,s,a
qh(s, a)di,h(s, a)

qh(s, a)ch(s, a),

i
li ∀

∈

≤

[1 : I] .

The optimal policy π∗ can be obtained from q∗ following (2).

Xh,s,a

III. THE LEARNING PROBLEM

the
di, li}

We
,

consider
, H, s1, p, c,

setting where
I
i=1) with stationary transition probability (i.e., ph = p,

interacts with

repeatedly

agent

an

A

(
S
length H, starting from the same initial state s1. For simplicity of analysis,1 we assume that the cost functions c,
I
i=1
are known to the learning agent, but the transition probability p is unknown. The agent estimates the transition probability
in an online manner by observing the trajectories over multiple episodes.

di}

∈

h

∀

{

{

a ﬁnite-horizon CMDP

=
[1 : H]) in episodes of ﬁxed

M

The main objective is to design an online learning algorithm such that for given ǫ, δ

, the number of
episodes for which the agent follows an ǫ-suboptimal policy is bounded above by a polynomial (up to logarithmic factors) in
δ (PAC guarantee). A policy
the relevant quantities (
π is said to be ǫ-optimal if the total expected objective cost of an episode under policy π is within ǫ of the optimal value,
i.e., V π
[1 : I].
We make the following assumption of feasibility.

V ∗ + ǫ and the constraints are satisﬁed within an ǫ tolerance, i.e., V π

δ ) with high probability, i.e., with probability at least 1

(0, 1) and CMDP

1 (s1; di, p)

1 (s1; c, p)

li + ǫ,

, H, 1

ǫ , 1

|A|

|S|

M

≤

−

≤

∈

∈

∀

i

,

1The complexity of learning the transition probability dominates the complexity of learning the cost functions [25]. The algorithm can be readily extended
to the setting of unknown cost functions by using an optimistic lower bound of the cost function obtained from its empirical estimate in place of the known
cost function.

Assumption 1: The given CMDP

M

is feasible, i.e., there exists a policy π such that the constraints are satisﬁed.

IV. THE UC-CFH ALGORITHM

Algorithm Description.: We consider an adaptation of the model-based algorithm UCFH [6] to the setting of CMDP,
which we call Upper-Conﬁdence Constrained Fixed-Horizon episodic reinforcement learning (UC-CFH) algorithm. The
algorithm leverages the approach of optimism-in-the-face-of-uncertainty [5] to balance exploration and exploitation.

The algorithm operates in phases indexed by k and whose length is not ﬁxed, but instead depends on the observations
made until the current episode. Each phase consists of three stages: planning, policy execution, and update of the visitation
counts.

For each phase k, UC-CFH deﬁnes a set of plausible transition models based on the number of visits to state-action pairs
(s, a) and transition tuples (s, a, s′) so far. A policy πk is then chosen by solving an optimistic planning problem, which is
expressed as an LP problem (lines 13-16 in Algorithm 1). The planning problem, referred to as ConstrainedExtendedLP in
the algorithm, is detailed below.

The algorithm maintains two types of visitation counts. Counts v(s, a) and v(s, a, s′) are the number of visits to state-
action pairs (s, a) and transition tuples (s, a, s′), respectively, since the last update of state-action pair (s, a). Counts n(s, a)
and n(s, a, s′) are the total number of visits to state-action pairs (s, a) and transition tuples (s, a, s′), respectively, before
the update of state-action pair (s, a). These visitation counts are all initialized to zero.

During the policy execution stage of phase k (lines 18-27 in Algorithm 1), the agent executes the current policy πk,
observes the tuples (st, at, st+1), and updates the respective visitation counts v(st, at) and v(st, at, st+1). This policy πk
is executed until a state action pair (s, a) has been visited often enough since the last update of (s, a), i.e., v(s, a) is large
enough (lines 26-27 in Algorithm 1).

In the next stage of phase k (lines 29-33 in Algorithm 1), the visitation counts n(s, a), n(s, a, s′) corresponding to the
sufﬁciently visited state action pair (s, a) are updated as n(s, a) = n(s, a) + v(s, a), n(s, a, s′) = n(s, a, s′) + v(s, a, s′)
and visitation counts v(s, a), v(s, a, s′) are reset to 0. This iteration of planning-execution-update describes a phase of the
algorithm.

Optimistic Planning.: At the start of each phase k, UC-CFH estimates the true transition model by its empirical average

as:

¯pk(s′

s, a) =

nk(s, a, s′)

,

(s, a, s′)
∀

.

1, nk(s, a)
}
The algorithm further deﬁnes conﬁdence intervals for the transition probabilities of the CMDP, such that the true transition
probabilities lie in them with high probability. Formally, for any (s, a)

∈ S × A × S

, we deﬁne:

max
{

|

Bk

p (s, a) =

˜p(.

s, a)

∆S :

s′
∀

,

˜p(s′

s, a)

∈ S × A
¯pk(s′

s, a)

p (s, a, s′)
βk
}

,

∈
|
p (s, a, s′) is built using the empirical Bernstein inequality [26]. For any (s, a, s′)
where the size of the conﬁdence intervals βk

∈ S

| ≤

−

{

|

|

|

∈

S × A × S

, it is deﬁned as:

p (s, a, s′) =
βk

2¯pk(s′

|

s

¯pk(s′

s, a)(1
max(1, nk(s, a))

−

|

s, a)) ln 4
δ′

+

7 ln 4
δ′

3 max(1, nk(s, a)

,

1)

−

where δ′ is as deﬁned in the algorithm and ¯pk(s′
¯pk(s′

s, a).

|

s, a)(1

|

¯pk(s′

|

−

s, a)) is the variance associated with the empirical estimate

Given the conﬁdence intervals Bk

p , the algorithm then computes a policy πk by performing optimistic planning. Given a
conﬁdence set of possible transition models, it selects an optimistic transition probability model and optimistic policy with
respect to the given constrained MDP problem. This can be expressed as the following optimization problem:

(˜pk, πk) = argmin
π∈Π, ˜p∈Bk
p

V π
1 (s1; c, ˜p)

s.t. V π

1 (s1; di, ˜p)

i
li ∀

∈

≤

[1 : I] .

(3)

We allow time-dependent transitions, i.e., choosing different transition models at different time steps of an episode, even if
the true CMDP has stationary transition probability. This does not affect the theoretical guarantees, since the true transition
probability still lies in the conﬁdence sets with high probability.

These conﬁdence intervals differ from the ones considered in UCFH [6] which have an additional condition that the standard
¯p). We
deviation associated with a transition model, i.e.,
remove this condition to be able to express the optimistic planning problem (3) as a linear program. However, this causes
the PAC bound to have a quadratic dependence on C instead of a linear dependence.

˜p) must be close to that of the empirical estimate

˜p(1

¯p(1

p

p

−

−

∈

−

(0; 1], failure tolerance δ

(0; 1], ﬁxed-horizon MDP

∈
δ, ǫ-optimal policy

Algorithm 1 UC-CFH: Upper-Conﬁdence Constrained Fixed-Horizon episodic reinforcement learning algorithm
1: Input: Desired tolerance ǫ
2: Result: With probability at least 1
3:
4: k := 1, wmin =
5: Nmax =
|S||A|
6:
7: m = 2304C 2H2
(log2 log2 H)2 log2
2
8: n(s, a) = v(s, a) = n(s, a, s′) = 0,
, s′
9:
10:
11: while True do
12:

ǫ
4H|S||A| ,
|S|H
;
wmin

8H2|S|2|A|
ǫ

δ
2NmaxC ;

Succ(s, a);

ln 4
δ′ ;

δ′ =

log2

s
∀

∈ A

∈ S

M

, a

∈

ǫ2

13:

14:
15:

16:

17:
18:

19:
20:

21:

22:
23:

24:

29:

30:
31:

32:

)

|
∈ S

′
n(s,a,s
max{1,n(s,a,s′)} ,

s, a) =
, a

¯p(s′
s
∀
πk = CONSTRAINEDEXTENDEDLP(¯p, n);

Succ(s, a);

∈ A

, s′

∈

repeat

for t = 0 to H-1 do
πk
h(st);
at ∼
p(.
st+1 ∼
v(st, at) = v(st, at) + 1;
v(st, at, st+1) = v(st, at, st+1) + 1;

st, at);

|

end for

until there is a (s, a)

25:
26: with v(s, a)
27: n(s, a) <
28:

|S|

max

≥
mH

{

,
∈ S × A
mwmin, n(s, a)
}

and

n(s, a) = n(s, a) + v(s, a);
n(s, a, s′) = n(s, a, s′) + v(s, a, s′);
v(s, a) = v(s, a, s′) = 0,
s′
Succ(s, a);
∀
∈
k = k + 1;

33:
34:
35: end while

CONSTRAINEDEXTENDEDLP Algorithm.: Problem (3) can be expressed as an extended LP by leveraging the state-
h (s, a; p) [7] to express the conﬁdence

h (s, a, s′; p) = ph(s′
action-state occupancy measure zπ(s, a, s′; p) deﬁned as zπ
intervals of the transition probabilities. The extended LP over z is as follows:

s, a)qπ

|

zh(s, a, s′)ch(s, a),

min
z

s.t.

zh(s, a, s′)

Xh,s,a,s′
0,
≥

(s, a, s′, h)
∀

zh(s, a, s′)di,h(s, a)

∈ S × A × S ×
[1 : I]
li,

i
∀

∈

≤

[1 : H]

zh(s′, a′, s),

s
∀

,

∈ S

h

∀

∈

[2 : H]

Xh,s,a,s′

zh(s, a, s′) =

Xa,s′

Xs′,a′

zh(s, a, s′) = I(s = s1),

Xa,s′
zh(s, a, s′)

(¯pk(s′

|

−

s
∀

∈ S

y
X
p (s, a, s′))
βk

y
X

s, a) + βk

p (s, a, s′))

zh(s, a, y)

0,

≤

(s, a, s′, h)
∀

∈ S × A × S ×

[1 : H]

zh(s, a, s′) + (¯pk(s′

s, a)

|

−

−

zh(s, a, y)

0,

≤

(s, a, s′, h)
∀

∈ S × A × S ×

[1 : H] .

The last two constraints of the above LP encode the condition that the transition probability must lie in the desired conﬁdence
interval. The desired policy πk and the chosen transition probabilities are recovered from the computed occupancy measures
as:

πk
h(a

|

s) =

s′ zh(s, a, s′)
a,s′ zh(s, a, s′)

,

h(s′
˜pk

|

s, a) =

zh(s, a, s′)
s′ zh(s, a, s′)

.

P
P

Theorem 1: For ǫ, δ

The above planning is referred to as CONSTRAINEDEXTENDEDLP in the algorithm. Such an approach was also used in [27],
[28] in the context of adversarial MDPs. The following theorem establishes the PAC guarantee for the algorithm UC-CFH.
log 1
δ

∈
episodes with ǫ-suboptimal policies πk, i.e., V πk
(cid:1)
, the number of episodes needed by algorithm
Thus, in the natural setting of a limited size of successor states i.e., C
UC-CFH to obtain an ǫ-optimal policy with high probability has a linear dependence on the state and action space sizes
S

δ, algorithm UC-CFH yields at most ˜
O
[1 : I].

, respectively, and quadratic dependence on the time horizon H.

(0, 1), with probability at least 1

V ∗ > ǫ or V πk

li > ǫ, for any i

|S||A|C 2H2
ǫ2

(s1, di)
S

(s1, c)

≪ |

and

P

−

−

−

∈

(cid:0)

1

1

|

|

|

A
|

|

V. PAC ANALYSIS
For state-action pairs, we now introduce a notion of knownness indicating how often the pair has been visited relative to
its expected number of visits under a policy and a notion of importance indicating the inﬂuence that the pair has on the
total expected cost of a policy [6]. We consider a ﬁne grained categorization of knownness of state-action pairs similar to
[29], [6] instead of the binary categorization [12], [13]. These are essential for the analysis of the algorithm.

We deﬁne the weight of a state-action pair (s, a) under policy πk as its expected number of visits in an episode, i.e.,

The importance ιk of a state-action pair (s, a) with respect to policy πk is an integer deﬁned as its relative weight with

H

wk(s, a) =

P r

st = s, at = a

t=1
X

(cid:2)

πk, s1

.

|

(cid:3)

respect to wmin on a log scale:

where z1 = 0, zi = 2i−2,

i

∀

≥

Similarly, knownness κk of a state-action pair (s, a) is an integer deﬁned as:

ιk(s, a) = min

zi : zi ≥
(cid:26)

wk(s, a)
wmin (cid:27)

,

2.

where z1 = 0, zi = 2i−2,

i

∀

≥

κk(s, a) = max

zi : zi ≤
2. We then divide the (s, a)-pairs into categories as follows:

(cid:27)

(cid:26)

,

nk(s, a)
mwk(s, a)

Xk,κ,ι =
¯
Xk =
: ιk(s, a) > 0

(s, a)
{
S × A\Xk,

∈ Xk : κk(s, a) = κ, ιk(s, a) = ι
}

,

{

(s, a)

Xk =

where
Xk is the inactive set, i.e., the set of state-action pairs that
are unlikely to be visited under policy πk. The idea is that the model estimated by the algorithm is accurate if only a small
number of state-action pairs are in categories with low knownness, that is, they are important under the current policy but
have not yet been sufﬁciently observed.

∈ S × A

}

is the active set and ¯

We therefore distinguish between phases k where the condition

κ for all κ and ι holds and phases where this
does not hold. This condition ensures that the number of state-action pairs in categories with low knownness are small and
there are more state-action pairs in categories with higher knownness. We will further prove that the policy is ǫ-optimal in
episodes which satisfy this condition.

|Xk,κ,ι| ≤

A. Proof of Theorem 1

The proof of Theorem 1 consists of the following parts: We ﬁrst show in Lemma 2 that the true transition model is
contained within the conﬁdence sets for all phases with high probability, i.e., the true transition probability p belongs to Bk
p
δ
for all k with probability at least 1
2 . Presentation of the technical lemmas used in the proof is postponed to the next
subsection to improve readability.

−

κ, ι :

We then use a result from [6] restated as Lemma 3 (with minor modiﬁcation to accommodate randomized policies instead
of deterministic policies) which provides a high probability upper bound on the number of episodes for which the condition
> κ for some κ, ι is bounded above by 6N Emax,
δ
2 (Note that the choice of m in
δ
2 , we have
κ

∀
where N =
Theorem 1 satisﬁes the condition on m in Lemma 3). Thus, with high probability, i.e., at least 1
for all κ, ι for the remaining episodes.

κ is violated. Thus, the number of episodes with

with probability at least 1

m and Emax = log2

log2 |S||A|

|Xk,κ,ι| ≤

|Xk,κ,ι| ≤

|Xk,κ,ι|

H
wmin

|S||A|

−

−

Thus, by union bound, for episodes beyond the ﬁrst 6N Emax, we have that

|Xk,κ,ι| ≤

κ for all κ, ι and p

Bk

p with

∈

Further, in Lemma 9, we show that in episodes with

κ for all κ, ι, the optimistic expected total cost is ǫ-close

probability at least 1

δ.

−

to the true expected total cost. Thus,

V πk
1
We note that ˜pk, πk were obtained by solving the following optimization problem:

i
∀

| ≤

−

ǫ,

∈

|

[1 : I] .

|Xk,κ,ι| ≤
˜V πk
1

−
(s1, di)

(s1, c)
˜V πk
1

V πk
1

|
(s1, di)

(s1, c)

ǫ,

| ≤

(˜pk, πk) = argmin
π∈Π, ˜p∈Bk
p

V π
1 (s1; c, ˜p)

s.t. V π

1 (s1; di, ˜p)

i
li ∀

∈

≤

[1 : I] .

(4)

Thus, for p

Bk

p , we have,

∈

V πk
1

(s1, c)

−

V ∗ = V πk
1
V πk
1
ǫ

(s1, c)

˜V πk
1
˜V πk
(s1, c)
1
−
(By Lemma 9).

≤

−

≤

(s1, c) + ˜V πk
(s1, c)

1

(s1, c)

V ∗

−

(By (4), since p

Bk
p )

∈

Similarly for all i

[1 : I],
∈
V πk
1

(s1, di)

−

li = V πk
1
V πk
1
ǫ

(s1, di)

˜V πk
1
˜V πk
(s1, di)
1
(By Lemma 9).

≤

−

−

≤

(s1, di) + ˜V πk
(s1, di)

1

(s1, di)

li

−

(Since πk satisﬁes constraints of (4))

Thus, putting all
m log2

H
wmin

|S||A|

6

log2 |S||A|

ǫ-suboptimal episodes.

the above together we have that with probability at

least 1

δ, UC-CFH has at most

−

B. Technical Lemmas

We state the main lemmas used in the proof of Theorem 1.

1) Capturing the true transition model with high probability: We ﬁrst restate the following lemma which provides an

upper bound on the total number of phases in the algorithm UC-CFH from [6].

Lemma 1: The total number of phases in the algorithm is bounded above by Nmax =

log2

|S|H
wmin

.

|S||A|

The above result is used along with concentration results based on empirical Bernstein inequality [26] and union bounds to
show that the true transition model is contained within the conﬁdence sets for all phases with high probability.

Lemma 2: The true transition probability is contained within the conﬁdence intervals for all phases with high probability,

i.e., p

Bk
p ,
∈
Proof:

k with probability at least 1

∀

δ
2 .

−

Following from [26], we have,
Let Z = (Z1 . . . Zn) be independent random variables with values in [0, 1] and let 0 < δ < 1. Then, with probability at

least 1

−

δ, we have:

"
where Vn(Z) is the sample variance,

E

1
n

n

i=1
X

Zi

# −

1
n

Zi ≤ s

Vn(Z) ln 2
δ
n

+

7 ln 2
δ
1)

3(n

−

,

n

i=1
X

1

Vn(Z) =

n(n

1)

−

X1≤i<j≤n

(Zi −
2

Zj)2

.

By symmetry and union bound, this implies that with probability at least 1

2δ,

E

|

"

That is, with probability at least 1

−

E

|

"

1
n

δ,

1
n

n

i=1
X

n

i=1
X

Zi

# −

Zi

# −

1
n

1
n

n

i=1
X

n

i=1
X

−
Vn(Z) ln 2
δ
n

+

Zi| ≤ s

7 ln 2
δ
1)

3(n

−

Zi| ≤ s

Vn(Z) ln 4
δ
n

+

7 ln 4
δ
1)

3(n

−

.

.

(5)

For a single (s, a) pair, s′

∈
choosing action a in state s as a Bernoulli random variable with probability p(s′
trivially true for nk(s, a) = 0, 1), with probability at least 1

δ′,

Succ(s, a) and phase k, we can consider the event that s′ is the next state of the MDP when
s, a). Thus, by (5) for nk(s, a) > 1 (and

|

−

¯pk(s′

s, a)

|

p(s′

|

−

|

s, a)

| ≤ s

≤ s

2¯pk(s′

2¯pk(s′

|

|

s, a)(1

¯pk(s′
nk(s, a)

−

s, a)) ln 4
δ′

|

¯pk(s′

s, a)(1
max(1, nk(s, a))

−

|

s, a)) ln 4
δ′

7 ln 4
δ′
3(nk(s, a)

1)

−
7 ln 4
δ′

+

+

(6)

(7)

,

3max(1, nk(s, a)

1)

−
s, a) at phase k. ( Vn(Z) of (5) simpliﬁes to

|

s, a) is the empirical estimate of the transition probability p(s′

where ¯pk(s′
2¯pk(s′

|
s, a)(1

|

−

|

¯pk(s′

s, a)) in this case).

There are at most Nmax updates or phases by Lemma 1 and in each phase, a single (s, a) pair with at most C successor
and by

states is updated. Therefore, there are at most NmaxC such inequalities to consider. Thus, by setting δ′ =
using union bound, the lemma is proved.

δ
2CNmax

The above lemma implies that the extended LP of the planning stage is feasible in all phases with high probability, since
the true CMDP is feasible by Assumption 1.

2) Number of episodes which violate

κ, ι : We restate the following result from [6] (with minor modiﬁcation
to accommodate randomized policies instead of deterministic policies) which provides a high probability upper bound on
the number of episodes for which

|Xk,κ,ι| ≤

κ, ι is violated.

κ,

κ,

∀

|Xk,κ,ι| ≤

∀

Lemma 3: Let E be the number of episodes for which there are κ, ι with

Then,

> κ and and let m

|Xk,κ,ι|

6H2
ǫ

ln 2Emax
δ

.

≥

where N =

|S||A|

m and Emax = log2

H
wmin

(E

6N Emax)

≤

P
log2 |S||A|

.

1

−

≥

δ/2,

3) Difference between true and optimistic total cost: We use the following value difference lemma [7] to express the
difference in value functions of policy π at time step h with respect to MDPs of different transition probabilities p, ˜p, i.e.,
V π
˜pt), t > h as
h −
follows. We use shorthand V π
h (s; c, ˜p) respectively in the next lemma and further. Cost
function c is omitted when clear.

t , t > h and difference in transition probabilities (pt −

h in terms of the value functions beyond h, ˜V π
˜V π

h (s; c) for V π

h (s; c, p), ˜V π

h (s; c), ˜V π

Lemma 4: Consider MDPs M = (

,

, p =

{
{
Then, the difference in the values with respect to the same policy π for any s, h can be written as:

A

A

S

{

H
h=1, c =

ph}

ch}

H

h=1) and ˜M = (
S

,

, ˜p =

˜ph}

H
h=1, c =

H
h=1)).

ch}

{

H

Xi=h

V π
h (s)

−

˜V π
h (s) = E

"

(pi(

·|

si, ai)

˜pi(

·|

−

si, ai)) ˜V π

h+1|

π, p, sh = s

.

#

(8)

Proof: The statement is trivially true for h = H +1 (V π

H+1(s), ˜V π

H+1(s) = 0). Let us assume it holds true for h+1.Then,

V π
h (s)

−

˜V π
h (s)

= E

ch(sh, ah) + ph(

·|

sh, ah)V π

h+1|

π, sh = s

(By Bellman equation)

(cid:2)

E

−

h

(cid:3)

ch(sh, ah) + ˜ph(

·|

sh, ah) ˜V π

h+1|

π, sh = s

i

= E

+ E

= E

sh, ah)V π

·|
sh, ah) ˜V π

h+1|

h+1|
sh, ah)(V π

ph(

(cid:2)
ph(
h

·|
ph(

·|

π, sh = s

π, sh = s

E

(cid:3)

E

−

˜ph(
h
ph(
−
i
h
˜V π
π, sh = s
h+1)
|
+ E

h+1 −
π, p, sh = s

h

= E

V π
h+1 −
By assumption,

h

˜V π
h+1|

i

h

sh, ah) ˜V π

h+1|

·|

sh, ah) ˜V π

h+1|

·|
+ E

π, sh = s

π, sh = s

i

(ph(

sh, ah)

·|
˜ph(

·|
−
sh, ah)) ˜V π

·|

i
(ph(
·|

h

sh, ah)

−

i
˜ph(

sh, ah)) ˜V π

h+1|

π, sh = s

i

h+1|

π, sh = s

i

(pi(

·|

si, ai)

˜pi(

·|

−

si, ai)) ˜V π

h+1|

π, p, sh+1

π, p, sh = s

#

# |

H

""

Xi=h+1

= E

+ E

= E

(ph(
h

H

·|

sh, ah)

˜ph(

·|

−

(pi(

·|

si, ai)

˜pi(

·|

−

Xi=h

"

sh, ah)) ˜V π

h+1|

π, sh = s

i

si, ai)) ˜V π

h+1|

π, p, sh = s

.

#

Hence, proved by induction.

in (8) in terms
We prove the following lemma which is used to upper bound the difference in transition probability
of ˜p and visitation counts n. The lemma is proved by viewing (9) as a quadratic inequality in terms of √¯p and solving for
¯p. The resulting inequality is then substituted back in the original inequality to get the desired result.

˜p
|

−

p

|

Lemma 5: Let ¯p, ˜p, p

[0, 1], δ

∈

∈

(0, 1) such that p, ˜p

CI where,

∈

(9)

(10)

(11)

CI :=

p′

{

∈

[0, 1] :

p′

|

¯p
| ≤ s

−

2¯p(1

¯p) ln 4
δ

−
max(1, n)

+

7 ln 4
δ
3 max(1, n

.

1) }

−

Then,

2√2

p

˜p

max(1,n−1) .
Proof: The lemma is trivially true for n = 0, 1. For n > 1 and p

max(1,n−1) + 5(

| ≤

q

−

|

ln 4
max(1,n−1) )
δ

3

4 + 21 ln 4

δ

˜p ln 4
δ

CI, we have,

∈

p

|

¯p
| ≤ s

−

2¯p(1

¯p) ln 4
δ
−
n

+

7 ln 4
δ
1)

3(n

−

2

s

≤

¯p ln 4
δ
1)

2(n

−

+

7 ln 4
δ
1)

3(n

.

−

For ˜p

∈

CI, we similarly have,

By simplifying (10), we get,

˜p

|

−

¯p

| ≤

2

s

¯p ln 4
δ
1)

2(n

−

+

7 ln 4
δ
1)

3(n

−

.

p + 2

¯p

≤

s

=

⇒

(√¯p)2

≤

(√p)2 + 2√¯p

s

¯p ln 4
δ
1)

2(n

−
ln 4
δ

2(n

1)

−

+

+

(√¯p)2

=

⇒

2√¯p

s

−

ln 4
δ

2(n

1)

−

+

ln 4
δ

ln 4
δ

2(n

−

1) ≤

(√p)2 +

(√¯p

=

⇒

− s

)2

(√p)2 + (

1)

2(n

≤
Since for a, b

−

s
0, a2 + b2

≥

ln 4
δ

=

⇒ |

√¯p

− s

2(n

1) | ≤

−

√p +

s

3(n

7 ln 4
δ
1)

−
7 ln 4
δ
1)

3(n

−
17 ln 4
δ
1)
6(n

−
17 ln 4
δ
1)
6(n
(a + b)2,

)2

−

≤

17 ln 4
δ
1)
6(n

−

Substituting (12) in (10), we get,

=

⇒

√¯p

≤

√p + (

1
√2

+

17
6

)
s

r

ln 4
δ
1)

−

(n

.

(12)

p

|

−

¯p
| ≤ s

≤ s

=

s

2 ln 4
δ
1)
(n

−
2p ln 4
δ
1)
(n

−
2p ln 4
δ
1)
(n

−

ln 4
δ
1)

(n

) +

−
7 ln 4
δ
1)

3(n

−

(√p + (

1
√2

+

17
6

)
s

r

+ (1 +

17
3

)

r

+ (

10
3

+

17
3

)

r

ln 4
δ
1)

(n

+

−
ln 4
δ
1)

(n

.

−

7 ln 4
δ
1)

3(n

−

Similarly, substituting (12) in (11), we get,

˜p

|

−

¯p
| ≤ s

2p ln 4
δ
1)
(n

−

+ (

10
3

+

17
3

)

r

ln 4
δ
1)

(n

−

Thus,

p

˜p

|

−

˜p

−

| ≤ |

10
3

+

17
3

)

r

ln 4
δ
1)

(n

s

2

≤

(cid:16)
= 2√2

¯p

+

¯p
|
|
2p ln 4
δ
1)
(n

p

|

−

+ (

−
ln 4
δ
1)

−
ln 4
δ
1)

−

p

(

|
p

s

(n

(

p

−

˜p + ˜p) + (

2√2

s

≤

(n

p

˜p
|

−

+ ˜p) + (

20
3

+ 2

17
3

)

r

(cid:17)

−
20
3

+ 2

17
3

)

r

ln 4
δ
1)

(n

−
ln 4
δ
1)

−

(n

Since for a, b

≥

≤

0, √a + b

√a + √b, and using (13),

ln 4
δ
1) v
u
u
t

−

+ 4

s

(n

ln 4
δ
1)

(n

)

−

+ 4

s

(n

ln 4
δ
1)

−

ln 4
δ
1)

(n

)

−

+ 4

s

(n

ln 4
δ
1)

−

≤
˜p ln 4
δ
1)
(n

r
˜p ln 4
δ
1)
(n

r
˜p ln 4
δ
1)
(n

−
17
3

−
17
3

−
17
3

2√2

s

+ (

20
3

+ 2

2√2

≤

s

+ (

20
3

+ 2

2√2

≤

s

+ (

20
3

+ 2

2√2

≤

s

ln 4
δ
1)

(n

)

−

+ 5(

r
˜p ln 4
δ
1)
(n

−

ln 4
δ
1)

−

(n

3

4 +

)

21 ln 4
δ
1)
(n

−

.

2p ln 4
δ
1)
(n

−

s

+ (

10
3

+

17
3

)

r

ln 4
δ
1)

(n

−

(

2p ln 4
δ
1)
(n

−

(cid:16)

1
4 +

)

10
3

s

+

17
3 s

r

(

2 ln 4
δ
1)
(n

−

(cid:16)

1

4 +

)

10
3

s

+

17
3 s

r

ln 4
δ
1)

−

(n

ln 4
δ
1)

−

(n

(13)

(cid:17)

(cid:17)

For each phase k, the true transition probability p belongs to the conﬁdence set Bk

p with high probability and the optimistic
transition model ˜pk is chosen from the conﬁdence set and thus p and ˜pk belong to CI for suitable δ, by deﬁnition of Bk
p .
Therefore, by Lemma 5,

can be upper bounded in terms of ˜pk and n as described above.

˜pk

p

|
The following lemma upper bounds the summand in (8), (p

−

|

˜ph)(

s, a) ˜Vh+1 which is the difference of the expected

value of successor states in MDPs with true transition probability p and optimistic transition probability model ˜p.

−

·|

| ≤

c1(s, a) + c2(s, a)

˜ph(s′

s, a),

|

p

Lemma 6: Let,

for all s, s′

and a

∈ S

p(s′

s, a)

˜ph(s′

s, a)

|

|

−

|
. Then, for any policy π,
s, a) ˜Vh+1| ≤

c1(s, a)
|

∈ A
˜ph)(

(p

|

−

·|
, where ˜σ2

∈ S × A
h(s, a) = E
˜σ2

Proof: Let ˆV (s′) = E( ˜Vh+1(sh+1)
|

(p

|

−

˜ph)(

·|

s, a) ˜Vh+1|

for any (s, a)

h is the local variance function deﬁned as:

Succ(s, a)

˜Vh+1k∞ + c2(s, a)

|k

˜σh(s, a),
Succ(s, a)
|

|
p

( ˜Vh+1(sh+1)
h

E( ˜Vh+1(sh+1)
|

−

sh = s, ˜p, π))2

|

sh = s, ˜p, π) be a constant function. Then,

sh = s, ah = a, ˜p
i

.

(By triangular inequality)

ˆV )
|

s, a)( ˜Vh+1 + ˆV
−
ˆV )
s, a)( ˜Vh+1 −
|
s, a) ˆV = 0 as ˆV is a constant function.)
·|
ˆV (s′)
p(s′
|

˜Vh+1(s′)

˜p(s′

s, a)

s, a)

−

−

||

|

|

|

(c1(s, a) + c2(s, a)

˜ph(s′

˜Vh+1(s′)

s, a))
|

p

|
−
˜Vh+1k∞ (as value is non negative)
˜ph(s′

s, a)( ˜Vh+1(s′)

ˆV (s′))2

|

−

ˆV (s′)
|

=

=

|

(p

(p

|
( as (p

−

−

·|

˜ph)(
˜ph)(
·|
˜ph)(

−

Xs′∈Succ(s,a)

≤

≤

Xs′∈Succ(s,a)
c1(s, a)
|

≤
+ c2(s, a)

Succ(s, a)

|k

Xs′∈Succ(s,a) q
Succ(s, a)

c1(s, a)
|

≤

(By Cauchy-Schwartz inequality)

˜Vh+1k∞ + c2(s, a)

|k

Succ(s, a)
|

Xs′∈Succ(s,a)

˜ph(s′

|

s, a)( ˜Vh+1(s′)

ˆV (s′))2

−

|
s

Hence, proved.

= c1(s, a)
|

Succ(s, a)

|k

˜Vh+1k∞ + c2(s, a)

|
p

Succ(s, a)
|

˜σh(s, a).

We then consider a sequence of MDPs
but different cost functions c(d). A similar sequence of MDPs ˜
M
considered.

M

(d) which have the same transition probability as that of the true MDP, i.e., p
(d) which have the same transition probability ˜p is also

In both sequences, for d = 0, the cost function is the same as that of the original cost function, i.e., c(0)

H. The following cost functions are then deﬁned recursively as c(2d+2)

h
h
the local variance of the value function under policy π with respect to the costs c(d) and deﬁned as:

(s, a), ˜c(2d+2)
h

(s, a) = ˜σ(d),2

≤

h

h , ˜c(0)
h = ch, 1
(s) where ˜σ(d),2

h

≤
is

˜σ(d),2
h

(s) = E

h+1(sh+1)

( ˜V (d)
h

E( ˜V (d)

h+1(sh+1)
|

−

sh = s, ˜p, π))2

Note that c(d)

h (s, a)

0, H d

We also use the following lemma [6] to bound

∈

(cid:2)

. We use the notation V (d) and ˜V (d) for value functions of
˜σ2
i (si)
|

sh = s, ˜p, π

H
i=1

E

(cid:3)

O(H 3).

.

|

sh = s, π, ˜p
i
(d) and ˜
M
in Lemma 8 by O(H 2) instead of the trivial

(d) respectively.

M

(cid:2)
Lemma 7: The variance of the value function deﬁned as

P

Bellman equation
H 2c2

max, we have 0

Vh(s) = E [

V1 ≤

sh = s, π] + σ2
Vh(sh+1)
|
E
σ2
sh = s, π
i (si)
|

H
i=1

≤

π

h (s) = E

(
h(s) which gives
h

V

H 2c2

Vh(s) =
P
.
max for all s
∈ S

(cid:3)
H
i=h ci(si, ai)
H
i=h

P

(cid:2)

−
E

i (si))2
V π
σ2
i (si)
|

sh = s, π

|
sh = s, π

satisﬁes a

. Since 0
i

≤

(cid:3)

≤

P

(cid:2)

(cid:3)

Bk

∈

If p, ˜p

p , the condition of Lemma 6 holds true by Lemma 5 for suitable constants. Then, by utilizing Lemmas 4, 6
˜V (d)
when the
1
κ for all (κ, ι) holds true. The analysis follows by splitting the state action pairs by importance, i.e.,

V (d+1)
1

˜V (d+1)
1

(s1)
|

(s1)
|

V (d)
1

(s1)

(s1)

with

−

−

|

|

and 7, we have the following recursive relation relating
condition
(s, a)

|Xκ,ι| ≤
and (s, a)

and using the deﬁnitions of weight w, knownness κ and importance ι.
p . If

κ for all (κ, ι). Then,

∈ X

Lemma 8: Let p, ˜p

6∈ X

Bk

∈
V (d)
1

|

|Xκ,ι| ≤
˜V (d)
1

−

(s1)
|

(s1)

:= ∆d ≤

ˆAd + ˆB1

d + ˆB2

d + min
{

ˆCd, ˆC′

d + ˆC′′

∆2d+2}

p

where,

Proof:

∆d =

ˆAd =

ǫH d
4

,

ˆB1

d = 42CH d+1

ln 4
δ′

|K × I|
m

,

ˆB2

d = 10CH d+5/4

ln 4
δ′

3/4

,

|K × I|
m

16C

ˆC′

d =

r

|K × I|
m

ln

(cid:0)
4
δ′ H 2d+2,

(cid:1)
ˆCd = ˆC′

d√H,

(cid:0)
16C

|K × I|
m

ln

(cid:1)
4
δ′ .

ˆC′′ =

r

V (d)
1

|

(s1)
H

−

˜V (d)
1

(s1)
|

(p

˜ph)(

·|

sh, ah) ˜V (d)
h+1|

−

π, p, s1

#

(p

˜ph)(

·|

−

sh, ah) ˜V (d)
h+1

π, p, s1

|

i

(By Lemma 4)

(cid:12)
(cid:12)

(By triangular inequality and Jensen’s inequality)

(cid:12)
(cid:12)
(p

(cid:12)
(cid:12)
(p

(cid:12)
(cid:12)
E [I
{

I
{

sh = s, ah = a

}

˜ph)(

·|

−

sh, ah) ˜V (d)
h+1

π, p, s1

|

#

(cid:12)
(cid:12)
π, p, s1

i

|

(cid:12)
(cid:12)

E

I
{

h

s,a
X

Xh=1
H

sh = s, ah = a

}

˜ph)(

·|

−

s, a) ˜V (d)
h+1

s,a
X

Xh=1
(cid:12)
(cid:12)
H

Xs,a6∈X

Xh=1
H

(p

˜ph)(

·|

−

s, a) ˜V (d)
h+1

sh = s, ah = a

π, p, s1]

}|

(cid:12)
(cid:12)
s, a) ˜V (d)
h+1

(p

˜ph)(

·|

−

E [I
{

sh = s, ah = a

π, p, s1]

}|

(cid:12)
(cid:12)
(p

˜ph)(

·|

−

s, a) ˜V (d)
h+1

(cid:12)
(cid:12)
E [I
{

sh = s, ah = a

π, p, s1]

}|

˜V (d)
h+1k∞E [I
{

(cid:12)
(cid:12)
sh = s, ah = a

π, p, s1]

}|

c1(s, a)
|

Succ(s, a)

|k

˜V (d)
h+1k∞ + c2(s, a)

˜σ(d)
h (s, a)
Succ(s, a)
|

|

E [I
{

sh = s, ah = a

π, p, s1]

}|

(By Lemma 6, with c1(s, a), c2(s, a) obtained from Lemma 5)

p

(cid:1)

H

Xs,a6∈X

Xh=1
H

Xs,a∈X

Xh=1

H d+1E [I
{

sh = s, ah = a

π, p, s1]

}|

c1(s, a)
|

Succ(s, a)
|

(cid:0)

H d+1 + c2(s, a)

|
p

Succ(s, a)
|

˜σ(d)
h (s, a)

E [I
{

sh = s, ah = a

π, p, s1]

}|

(cid:1)

H d+1w(s, a) +

Xs,a6∈X

Xs,a∈X

c2(s, a)

Succ(s, a)
|

|

Xs,a∈X

p

(By deﬁnition of w(s, a))

c1(s, a)
|

Succ(s, a)
|

H d+1w(s, a)

H

Xh=1

˜σ(d)
h (s, a)E [I
{

sh = s, ah = a

π, p, s1]

}|

≤

Xs,a6∈X

H d+1w(s, a)

+

c1(s, a)CH d+1w(s, a)

Xs,a∈X

A(s1)

{z

|

}

|

B(s1)

{z

}

E

=

(cid:12)
(cid:12)

H

"

Xh=1
E

Xh=1
H

h(cid:12)
(cid:12)

E

Xh=1

"
H

s,a
X

Xs,a∈X

Xh=1
H

(cid:12)
(cid:12)

k

Xs,a6∈X

Xh=1
H

Xs,a∈X

Xh=1

(cid:0)

≤

=

=

=

=

+

≤

+

≤

+

=

+

+

c2(s, a)√C

Xs,a∈X

H

Xh=1

˜σ(d)
h (s, a)E [I
{

sh = s, ah = a

π, p, s1] .

}|

}

ln 4
δ′

max(1, n(s, a)

1)

−

3

4 +

)

21 ln 4
δ′

max(1, n(s, a)

.

1)

−

(Since C is an upper bound of

|
By assumption that p, ˜p

∈

C(s1)
Succ(s, a)
{z
∀
|
|
p , we have from Lemma 5,

,

Bk

s, a.)

c2(s, a) = 2√2

ln 4
δ′

s

max(1, n(s, a)

,

1)

−

c1(s, a) = 5(

Using these, we now simply A(s1), B(s1), C(s1).

A(s1) =

H d+1w(s, a)

s, a

)

6∈ X

Xs,a6∈X
( as w(s, a)

≤
=

≤
wminH d+1
ǫH d+1
4H
= ˆAd.

|S||A|

|S||A|

wmin ∀
|S||A|
=

ǫH d
4

B(s1) =

c1(s, a)CH d+1w(s, a)

Xs,a∈X

= CH d+1

ln 4
δ′

max(1, n(s, a)

1)

−

3
4 w(s, a) +

)

Xs,a∈X

5(





Xs,a∈X

Now,

21 ln 4
max(1, n(s, a)

δ′ w(s, a)

−

.

1) 



w(s, a)

w(s, a)

max(1, n(s, a)

1) ≤

−

Xs,a∈X

κ,ι
X

Xs,a∈Xκ,ι

=

max(1, n(s, a)

1)

−
n(s, a)

w(s, a)
n(s, a)

max(1, n(s, a)

1)

−

For s, a

Further, since

1
κm
κ, we have for all relevant (s, a) pairs, n(s, a) > 0

w(s, a)
n(s, a) ≤

∈ Xκ,ι, we have n(s, a)
|Xκ,ι| ≤

≥

.

κ,ι
Xs,a∈Xκ,ι
X
mw(s, a)κ. Thus,

1
κm ·

2

Xs,a∈Xκ,ι
2
(as
m

|Xκ,ι| ≤

κ)

≤

≤

=

κ,ι
X

κ,ι
X
2

|K × I|
m

.

Continuing,

w(s, a)
(max(1, n(s, a)

Xs,a∈X

1))3/4 ≤

−

κ,ι
X

Xs,a∈Xκ,ι

w(s, a)
(max(1, n(s, a)

w(s, a)
(n(s, a))3/4

1))3/4

−

(n(s, a))3/4

(max(1, n(s, a)

1))3/4

−

w(s, a)
(n(s, a))3/4

(as before)

=

≤

Xs,a∈Xκ,ι

κ,ι
X
2

κ,ι
X

Xs,a∈Xκ,ι

= 2

(w(s, a))1/4(

κ,ι
X

Xs,a∈Xκ,ι

w(s, a)
n(s, a)

)3/4

(w(s, a))1/4(

1
κm

)3/4 (as before)

1
κm

)1/2(

w(s, a)
κm

)1/4

(

w(s, a)
κm

Xs,a∈Xκ,ι

)1/2 (By Cauchy-Schwartz inequality)

(

w(s, a)
κm

)1/2 (as

|Xκ,ι| ≤

κ)

(

w(s, a)
κm

)1/2 (By Cauchy-Schwartz inequality)

(

w(s, a)
κ

)1/2

κ,ι
X

Xs,a∈Xκ,ι

κ,ι
X

Xs,a∈Xκ,ι

|Xκ,ι|
κ

κ,ι v
u
X
u
t

Xs,a∈Xκ,ι

w(s, a) (By Cauchy-Schwartz inequality)

w(s, a) (as

|Xκ,ι| ≤

κ)

κ,ι s Xs,a∈Xκ,ι
X

|K × I|

s

κ,ι
X

Xs,a∈Xκ,ι

w(s, a) (By Cauchy-Schwartz inequality)

2

≤

= 2

κ,ι
X

Xs,a∈Xκ,ι

(

κ,ι
X

Xs,a∈Xκ,ι
|Xκ,ι|
κm

2

κ,ι v
u
X
u
t
2
m1/2

≤

≤

≤

=

≤

≤

≤

≤

κ,ι v
u
X
u
t
1/2

Xs,a∈Xκ,ι

2

2

2

2

2

2

|K × I|

m1/2 v
u
u
t

1/2

|K × I|

m3/4 v
u
u
t

1/2
m3/4 v
u
u
u
t

1/2

|K × I|

|K × I|

m3/4 v
u
u
t

1/2

|K × I|

m3/4 v
u
u
t

3/4

m3/4 H 1/4.
|K × I|

Thus, putting together the above two, we get that,

Now,

B(s1)

≤

42CH d+1

|K × I|
m

(cid:0)

ˆB1
d

{z

|

H

ln 4
δ′

(cid:1)

}

C(s1) =

c2(s, a)√C

ln 4
δ′

3/4

.

(cid:1)

}

+ 10CH d+5/4

|K × I|
m

(cid:0)

ˆB2
d

{z

|

}|

π, p, s1]

˜σ(d)
h (s, a)E [I
{

Xh=1
H

sh = s, ah = a

E [I
{

sh = s, ah = a

π, p, s1]

}|

Xs,a∈X

√C

Xs,a∈X
H

≤

Xh=1

× v
u
u
t
= √C

c2(s, a)

Xh=1

v
u
u
t
(s, a)E [I
{

˜σ(d),2
h

sh = s, ah = a

}|

π, p, s1] (By Cauchy-Schwartz inequality)

8w(s, a)
max(1, n(s, a)

ln

4
δ′

1)

−

H

Xh=1

Xs,a∈X

v
u
u
t

( By deﬁnition of w(s, a) and c2(s, a) from Lemma 5)

˜σ(d),2
h

(s, a)E [I
{

sh = s, ah = a

π, p, s1]

}|

√C

≤

κ,ι
X

Xs,a∈Xκ,ι

v
u
u
t

8w(s, a)
max(1, n(s, a)

ln

4
δ′

H

Xh=1

1)

−

˜σ(d),2
h

(s, a)E [I
{

sh = s, ah = a

π, p, s1]

}|

√C

≤

|Xκ,ι|

κ,ι
X

v
u
u
t

Xs,a∈Xκ,ι

(By Cauchy-Schwartz inequality)

8w(s, a)
max(1, n(s, a)

ln

4
δ′

H

Xh=1

1)

−

˜σ(d),2
h

(s, a)E [I
{

sh = s, ah = a

π, p, s1]

}|

16
m

ln

4
δ′

˜σ(d),2
h

(s, a)E [I
{

sh = s, ah = a

}|

π, p, s1] (as before )

H

Xh=1
H

˜σ(d),2
h

(s, a)E [I
{

sh = s, ah = a

}|

π, p, s1] (By Cauchy-Schwartz inequality)

Xs,a∈X

Xh=1

H

Xh=1
H

Xs,a∈S×A

Xs,a∈S×A
H

Xh=1

Xh=1 Xs,a∈S×A
H

˜σ(d),2
h

(s, a)E [I
{

sh = s, ah = a

π, p, s1]

}|

˜σ(d),2
h

(s, a)

P

(sh = s, ah = a

˜σ(d),2
h

(s, a)

P

(sh = s, ah = a

π, p, s1)

π, p, s1)

|

|

√C

≤

κ,ι
X

v
u
u
t

Xs,a∈Xκ,ι

16C

16C

16C

16C

16C

|K × I|
m

ln

|K × I|
m

ln

|K × I|
m

ln

|K × I|
m

ln

|K × I|
m

ln

4
δ′

4
δ′

4
δ′

4
δ′

4
δ′

≤ v
u
u
t

≤ v
u
u
t

=

=

v
u
u
t

v
u
u
t

=

v
u
u
t
Since

E

˜σ(d),2
h
h

(sh)
|

π, p, s1

i

Xh=1

˜σ(d),2
h
k
16C

k∞ ≤

|K × I|
m

H 2d+2, we have,

H 2d+3

ln

4
δ′ = ˆCd.

C(s1)

Else,

≤ r

C(s1)

|K × I|
m

ln

|K × I|
m

ln

4
δ′

4
δ′

16C

16C

≤ v
u
u
t

=

v
u
u
t

H

Xh=1
H

E

˜σ(d),2
h
h

(sh)
|

π, p, s1

i

(E

˜σ(d),2
h

Xh=1

h
H

(sh)
|

π, p, s1

E

˜σ(d),2
h
h

−

i

(sh)
|

π, ˜p, s1

+ E

˜σ(d),2
h

i

h

(sh)
|

π, ˜p, s1

)

i

π, ˜p, s1

(d)
1 (s1)

= ˜
V

H 2d+2. Using this,

≤

By Lemma 7, we have that

16C

16C

16C

|K × I|
m

ln

|K × I|
m

ln

|K × I|
m

ln

≤ r

≤ r

≤ r

4
δ′
4
δ′
(cid:0)
4
δ′ H 2d+2

E

˜σ(d),2
h

h

(sh)
|

Xh=1

H 2d+2 + V (2d+2)

1

(s1)

−

(cid:0)
H 2d+2 + ∆2d+2

i
˜V (2d+2)
1

(s1)

(cid:1)

16C

(cid:1)
|K × I|
m

ln

4
δ′

∆2d+2.

+

r

( Since for a, b

|

ˆC ′
d

{z
≥

ˆC ′′

0, √a + b

}
≤

√a + √b)

|

{z

Putting all the above together, the lemma is proved.

p

}

κ for all κ, ι, the optimistic total
This recurrence relation is simpliﬁed to show in Lemma 9 that in phases with
expected cost ˜V πk
(s1) is close to that of the true one, V πk
(s1). This lemma plays an important role in the ﬁnal theorem
to show that the policy obtained after sufﬁciently large number of episode is ǫ-optimal with respect to the objective and
constraints.

|Xk,κ,ι| ≤

1

1

Lemma 9: Let p, ˜p

Bk

p . If

∈

|Xk,κ,ι| ≤

κ for all κ, ι and 0 < ǫ

1 and

≤

2304C2H 2
ǫ2

m

≥

(log2 log2H)2 log2
2

8H 2

2

|S|
ǫ

|A|

ln

4
δ′ ,

V πk
1

˜V πk
1

|

(s1)

(s1)

then
Proof: From Lemma 8, we have the following recursive relation,
d + ˆC′

ˆAd + ˆB1

d + ˆB2

| ≤

−

ǫ.

which is the of the form ∆d ≤
use of √a + b
∀
≤

√a + √b,

Yd + Z
a, b

≥

0, we get,
p

d + ˆC′′
∆d ≤
∆2d+2. Expanding this recursive expression up to level γ =

∆2d+2,

p

ln H
2 ln 2 ⌉

⌈

with repeated

∆0 ≤

Y0 + Z

∆2

≤

≤

≤

≤

Y0 + Z

Y0 + Z
...

p

q

∆6

Y2 + Z
Y2 + Z 3/2∆1/4
p

6

p

Z

2d
2+d Y

2
2+d

d + Z

2
2γ
2+γ
2+γ ∆
γ

,

Xd∈D\{γ}

where D =
m

{
40C2H
|K × I|

≥

0, 2, 6, 14, . . . , γ

. Further, for m
ˆB2

}
δ′ , we have ˆC′

110.25C
d. Thus, for m

≥

ln 4

d ≥

|K × I|

ln 4
110.25C2H

δ′ , we have ˆC′
|K × I|

d ≥
ln 4

ˆB1

d. Similarly, for

δ′ , we can set,

≥
Yd = ˆAd + 3 ˆC′
d,
Z = ˆC′′.

Further, since ˆCd = ˆC′

d√H

≥

ˆC′

Let m1 = 16C|K×I|H2

mǫ2

ln 4

d, for the large enough m, we have,
ˆCγ, ˆC′

γ + min

γ + ˆC′′

{

ˆAγ + 2 ˆC′
ˆAγ + 3 ˆCγ.

∆2γ+2}

p

∆γ ≤
≤
δ′ and thus get,

Z = ˆC′′ =

√m1ǫ
H
Yd = ˆAd + 3 ˆC′

d = (

,

Thus,

ˆAγ + 3 ˆCγ = (

∆γ ≤

1
4
1
4

+ 3√m1)H dǫ,

+ 3

m1H)H dǫ.

p

(Z 2dY 2

d )(2+d)

−1

(Z 2γ∆2

γ)(2+γ)

−1

Putting the above together, we have,

= (md

= ǫ(md

= (mγ

1ǫd(

1ǫ2d+2(
1
4
1 ǫ2γ+2(
1
4

1 ǫγ(

= ǫ(mγ

+ 3

−1

+ 3√m1)2)(2+d)

1
4
+ 3√m1)2)(2+d)
1
4

+ 3

−1

,

m1H)2)(2+γ)

−1

p
m1H)2)(2+γ)

−1

.

p

∆0
ǫ ≤

=

Xd∈D\{γ} (cid:20)
1
4

+ 3√m1 +

(ǫm1)

d
d+2 (

+ 3√m1)

2

2+d )

+ (ǫm1)

γ

γ+2 (

1
4

+ 3

m1H)

2

2+γ )

1
4

p

(ǫm1)

d
d+2 (

(cid:21)
1
4

+ 3√m1)

2
2+d )

(cid:21)

Xd∈D\{0,γ} (cid:20)

+ (ǫm1)

γ

γ+2 (

1
4

+ 3

m1H)

2

2+γ )

+ 3√m1 +

p

(ǫm1)

d
d+2

1
4

≤

1
4

)

(
(cid:20)

2

2+d + (3√m1)

2

2+d )

(cid:21)

γ
γ+2

+ (ǫm1)

(
(cid:20)
( Since for a, b

Xd∈D\{0,γ}
)
2+γ + (3

2

1
4

m1H)

2

2+γ )

p
0 < φ < 1,

0,

≥

(cid:21)
(a + b)φ

aφ + bφ)

≤

And now using,

1

2 + γ ≤

1
2 + ln H
2 ln 2

=

2 ln 2

4 ln 2 + ln H ≤

2 logH 2,

we bound H

1
2+γ

≤

4. Thus,

∆0
ǫ ≤

1
4

+ 3√m1 +

Xd∈D\{0,γ}
)
2+γ + 4(3√m1)

2

+ (ǫm1)

γ
γ+2

1
4

(
(cid:20)

+ 3√m1 +

(ǫm1)

d
d+2

(By change of parameters,)

Xd∈D\{0}

(ǫm1)

d
d+2

1
4

)

(
(cid:20)

2

2+d + (3√m1)

2

2+d )

(cid:21)

2

2+γ )

(cid:21)
1
)
4

(

(cid:20)

2

2+d + 4(3√m1)

2
2+d )

(cid:21)

log2 γ

i=1
X
log2 γ

i=1
X
log2 γ

+ 3√m1 +

+ 3√m1 +

+ 3√m1 +

log2 γ

i=1
X
1
4

i=1
X
log2 γ

+ 3√m1 +

+ 3√m1 +

1
4

(ǫm1)1−2

−i

−i

(m1)1−2

(cid:20)

(

(cid:20)

−i

)2

(

1
4

+ 4(3√m1)2

−i

)
(cid:21)

−i

)2

1
4

+ 4(3√m1)2

−i

(as ǫ

1)

≤

)
(cid:21)

−i

(m1)1−2

−i

(4)−2

+ 4(3√m1)2

−i

h
−i

(4m1)1−2

log2 γ

+ 4

−i

(m1)1−2

−i−1

(9m1)2

)

i

(4m1)1−2

−i

i=1
X
log2 γ

+ 36

i=1
X

(

m1
9

−i−1

)1−2

.

i=1
X

1
4

≤

1
4

1
4

1
4

1
4

1
4

≤

≤

=

=

=

By requiring m1 ≤
m1 ≤

1
9 ,

1

4 , we have 4m1 < 1 and m1

9 < 1. Further for i

1, 1

2−i

−

≥

≥

1
2 and 1

−

2−i−1

≥

3
4 . Thus, for

∆0
ǫ ≤

=

1
4
1
4

+ 3√m1 +

+ 3√m1 +

1
4

√4m1 log2 γ + 36(

m1
4

r

log2 γ + 36(

)3/4 log2 γ

m1
9
m1
)3/4 log2 γ.
9

41 (log2 γ)−4/3, we get ∆0 ≤
(2 log2 γ)−2 and m1 ≤
1/144, m1 ≤
By requiring that m1 ≤
1
1
144 (log2 log2H)−2 (as log2 γ = log2(
144 (log2 γ)−2
m1 ≤
⌈
≤
on m1. Now using the deﬁnition of m1, we get that this implies,

ǫ. Thus taking
log2 log2H)) meets all our previous requirements

log2 H
2

⌉ ≤

1

2304C

H 2

|K × I|
ǫ2

m

≥

(log2 log2H)2 ln

4
δ′ .

But, we had an earlier requirement that m

110.25C2H

ln 4

δ′ . Thus, we choose the stronger condition on m i.e.,

≥

2304C2

|K × I|
H 2

m

≥

|K × I|
ǫ2

(log2 log2H)2 ln

4
δ′ .

By construction, ι(s, a)

Thus,

|K × I| ≤

log2

2H
wmin
8H2|S||A|
ǫ

≤

= 8H2|S||A|
log2

4|S|2|A|H2
ǫ

ǫ

. Also, κ(s, a)
log2
2

≤
8H2|S|2|A|
ǫ

≤

|S|mH
mwmin

= 4|S|2|A|H2

ǫ

.

. Substituting this in m gives us the desired result.

VI. CONCLUSIONS

Motivated by safe reinforcement learning for autonomous systems in unknown environments, we addressed the problem
of ﬁnding approximately optimal policies for ﬁnite-horizon MDPs with constraints and unknown transition probability. We
introduced the UC-CFH algorithm that is based on the optimism-in-the-face-of-uncertainty principle and offered, to the best
of our knowledge, the ﬁrst result in terms of provable PAC guarantees for both performance and constraint violations. Our
PAC bound exhibits quadratic dependence on the horizon length. In the future, we plan to consider other types of constraints,
e.g., chance or risk constraints, and extensions to the inﬁnite-horizon setting.

REFERENCES

[1] M. L. Puterman, Markov Decision Processes: Discrete Stochastic Dynamic Programming, 1st ed. New York, NY, USA: John Wiley & Sons, Inc.,

1994.

[2] H. Le, C. Voloshin, and Y. Yue, “Batch policy learning under constraints,” ser. Proceedings of Machine Learning Research, vol. 97, 2019, pp.

3703–3712.

[3] E. Altman, Constrained Markov Decision Processes. CRC Press, 1999, vol. 7.
[4] T. L. Lai and H. Robbins, “Asymptotically efﬁcient adaptive allocation rules,” Advances in applied mathematics, vol. 6, no. 1, pp. 4–22, 1985.
[5] P. Auer, T. Jaksch, and R. Ortner, “Near-optimal regret bounds for reinforcement learning,” in Advances in neural information processing systems,

2009, pp. 89–96.

[6] C. Dann and E. Brunskill, “Sample complexity of episodic ﬁxed-horizon reinforcement learning,” in Advances in Neural Information Processing

Systems, 2015, pp. 2818–2826.

[7] Y. Efroni, S. Mannor, and M. Pirotta, “Exploration-exploitation in constrained mdps,” arXiv preprint arXiv:2003.02189, 2020.
[8] M. G. Azar, I. Osband, and R. Munos, “Minimax regret bounds for reinforcement learning,” in Proceedings of the 34th International Conference on

Machine Learning - Volume 70, ser. ICML’17, 2017, p. 263272.

[9] C. Dann, T. Lattimore, and E. Brunskill, “Unifying pac and regret: Uniform pac bounds for episodic reinforcement learning,” in Advances in Neural

Information Processing Systems, 2017, pp. 5713–5723.

[10] A. Zanette and E. Brunskill, “Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function
bounds,” ser. Proceedings of Machine Learning Research, vol. 97. Long Beach, California, USA: PMLR, 09–15 Jun 2019, pp. 7304–7312.
[11] Y. Efroni, N. Merlis, M. Ghavamzadeh, and S. Mannor, “Tight regret bounds for model-based reinforcement learning with greedy policies,” in

Advances in Neural Information Processing Systems, 2019, pp. 12 224–12 234.

[12] R. I. Brafman and M. Tennenholtz, “R-max-a general polynomial time algorithm for near-optimal reinforcement learning,” Journal of Machine

Learning Research, vol. 3, no. Oct, pp. 213–231, 2002.

[13] A. L. Strehl, L. Li, and M. L. Littman, “Reinforcement learning in ﬁnite mdps: Pac analysis.” Journal of Machine Learning Research, vol. 10, no. 11,

2009.

[14] C. Jin, Z. Allen-Zhu, S. Bubeck, and M. I. Jordan, “Is q-learning provably efﬁcient?” in Advances in Neural Information Processing Systems, 2018,

pp. 4863–4873.

[15] R. Singh, A. Gupta, and N. B. Shroff, “Learning in markov decision processes under constraints,” arXiv preprint arXiv:2002.12435, 2020.
[16] S. Qiu, X. Wei, Z. Yang, J. Ye, and Z. Wang, “Upper conﬁdence primal-dual reinforcement learning for cmdp with adversarial loss,” 2020.
[17] D. Ding, X. Wei, Z. Yang, Z. Wang, and M. R. Jovanovi´c, “Provably efﬁcient safe exploration via primal-dual policy optimization,” arXiv preprint

arXiv:2003.00534, 2020.

[18] L. Zheng and L. Ratliff, “Constrained upper conﬁdence reinforcement learning,” ArXiv, vol. abs/2001.09377, 2020.
[19] K. Brantley, M. Dudik, T. Lykouris, S. Miryooseﬁ, M. Simchowitz, A. Slivkins, and W. Sun, “Constrained episodic reinforcement learning in

concave-convex and knapsack settings,” arXiv preprint arXiv:2006.05051, 2020.

[20] V. S. Borkar, “An actor-critic algorithm for constrained markov decision processes,” Systems & control letters, vol. 54, no. 3, pp. 207–213, 2005.
[21] J. Achiam, D. Held, A. Tamar, and P. Abbeel, “Constrained policy optimization,” arXiv preprint arXiv:1705.10528, 2017.
[22] C. Tessler, D. J. Mankowitz, and S. Mannor, “Reward constrained policy optimization,” arXiv preprint arXiv:1805.11074, 2018.
[23] S. Miryooseﬁ, K. Brantley, H. Daume III, M. Dudik, and R. E. Schapire, “Reinforcement learning with convex constraints,” in Advances in Neural

Information Processing Systems, 2019, pp. 14 093–14 102.

[24] A. Zimin and G. Neu, “Online learning in episodic markovian decision processes by relative entropy policy search,” in Advances in neural information

processing systems, 2013, pp. 1583–1591.

[25] P. Auer and R. Ortner, “Online regret bounds for a new reinforcement learning algorithm,” in Proceedings 1st Austrian Cognitive Vision Workshop,

2005.

[26] A. Maurer and M. Pontil, “Empirical bernstein bounds and sample variance penalization,” arXiv preprint arXiv:0907.3740, 2009.
[27] T. Jin and H. Luo, “Learning adversarial mdps with bandit feedback and unknown transition,” arXiv preprint arXiv:1912.01192, 2019.
[28] A. Rosenberg and Y. Mansour, “Online convex optimization in adversarial markov decision processes,” in International Conference on Machine

Learning, 2019, pp. 5478–5486.

[29] T. Lattimore and M. Hutter, “Pac bounds for discounted mdps,” in International Conference on Algorithmic Learning Theory. Springer, 2012, pp.

320–334.

