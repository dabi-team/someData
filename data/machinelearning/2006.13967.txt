0
2
0
2

n
u
J

4
2

]
L
M

.
t
a
t
s
[

1
v
7
6
9
3
1
.
6
0
0
2
:
v
i
X
r
a

Labeled Optimal Partitioning

Toby Dylan Hocking, toby.hocking@nau.edu
Anuraag Srivastava, as4378@nau.edu

June 26, 2020

Abstract

In data sequences measured over space or time, an important problem is accurate
detection of abrupt changes. In partially labeled data, it is important to correctly pre-
dict presence/absence of changes in positive/negative labeled regions, in both the train
and test sets. One existing dynamic programming algorithm is designed for prediction
in unlabeled test regions (and ignores the labels in the train set); another is for accurate
ﬁtting of train labels (but does not predict changepoints in unlabeled test regions). We
resolve these issues by proposing a new optimal changepoint detection model that is
guaranteed to ﬁt the labels in the train data, and can also provide predictions of unla-
beled changepoints in test data. We propose a new dynamic programming algorithm,
Labeled Optimal Partitioning (LOPART), and we provide a formal proof that it solves
the resulting non-convex optimization problem. We provide theoretical and empirical
analysis of the time complexity of our algorithm, in terms of the number of labels and
the size of the data sequence to segment. Finally, we provide empirical evidence that
our algorithm is more accurate than the existing baselines, in terms of train and test
label error.

1

Introduction

In the context of ﬁelds such as medical monitoring [Fotoohinasab et al., 2020] and ge-
nomics [Rigaill et al., 2013], where data are measured over space or time, detecting abrupt
changes is an important problem. There are many diﬀerent algorithms available for detect-
ing changepoints, and in this paper we focus on algorithms that compute a solution to a
well-deﬁned mathematical optimization problem. For example, the classical optimal parti-
tioning (OPART) algorithm was introduced by Jackson et al. [2005] in order to solve the
penalized changepoint problem for a sequence of N data x = [x1 · · · xN ]. Inferring the most
likely parameter vector m corresponds to minimizing a sum of losses (cid:96) plus a penalty λ for
each changepoint:

ˆCN = min
m∈RN

N
(cid:88)

i=1

(cid:96)(mi, xi) + λ

N −1
(cid:88)

i=1

1

I[mi (cid:54)= mi+1].

(1)

 
 
 
 
 
 
No label constraints

Label constraints

Best model with K segments Best model for penalty λ

Segment Neighborhood
[Auger and Lawrence, 1989]
SegAnnot
[Hocking et al., 2014]

Optimal Partitioning
[Jackson et al., 2005]
LOPART
This paper

Table 1: Relationship of the proposed LOPART algorithm to previous dynamic programming
algorithms for changepoint detection (rows for constraints, columns for problem formulation).
The previous SegAnnot algorithm has the same label constraints but a diﬀerent problem
formulation which makes it impossible to predict new changepoints in unlabeled regions
(may have test errors). The previous Optimal Paritioning algorithm has the same problem
formulation which can predict new changepoints in unlabeled regions, but ignores the given
labels (may have train errors).

The loss function (cid:96)(m, x) is typically the negative log likelihood of the parameter m given the
data x; smaller loss values indicate a better ﬁt. The indicator function I returns 1 if there is
a change between positions i and i + 1, and it returns 0 otherwise. The penalty parameter
λ ≥ 0 controls the number of detected changepoints (small λ results in an overﬁt model with
too many changepoints, large λ results in an underﬁt model with too few changepoints).
The penalty λ can be selected using theoretically-motivated unsupervised criteria such as
AIC/BIC [Akaike, 1973, Schwarz, 1978, Yao, 1988, Zhang and Siegmund, 2007], or using
supervised learning algorithms in the labeled data setting [Rigaill et al., 2013].

Although the loss (cid:96) is typically convex, the indicator functions I are non-convex, so
the optimization problem is non-convex, and gradient-based algorithms can not be used.
Instead, there are eﬃcient dynamic programming algorithms which can compute a global
optimum in quadratic O(N 2) time [Auger and Lawrence, 1989, Jackson et al., 2005]. More
recently, functional pruning algorithms such as FPOP [Maidstone et al., 2016] have been
used to compute a global optimum, with time complexity that is also quadratic O(N 2) in
the worst case, but log-linear O(N log N ) empirically.

Novelty with respect to previous work. Our paper proposes a new changepoint de-
tection algorithm for the case of partially labeled data sequences. The labeled data setting
arises in the context of interactive systems such as SegAnnDB [Hocking et al., 2014] and
CpLabel [Ford, 2020] which allow users to view the data and drag with the mouse to deﬁne
regions with/without signiﬁcant changepoints in subsets of the data. The previous OPART
and FPOP algorithms can be characterized as “unsupervised” because the labels are not
used, so they may be inconsistent with the labels (train errors). In contrast the previous
SegAnnot algorithm has constraints which ensure the changepoints are consistent with the
labels [Hocking et al., 2014], but there are no other changepoints outside the labels (test
errors). In this paper we resolve both issues, resulting in our new Labeled Optimal Parti-
tioning (LOPART) algorithm which is more accurate in terms of both train and test errors
(Section 4.2). The novelty of LOPART is that it combines the label constraints of SegAnnot

2

with the penalized formulation of OPART (Table 1).

2 Changepoint model for labeled data

2.1 Labeled data setting

In the context of changepoint detection we have a sequence of N data points to segment,
x = [x1 · · · xN ], and the goal is to predict a set of positions f (x) ⊆ {1, . . . , N − 1} with
signiﬁcant changepoints immediately after. For example, we will treat the simple case of
real-valued univariate data x1, . . . , xN ∈ R which occur in settings such as detection of copy
number changes in cancer genomics [Rigaill et al., 2013, Hocking et al., 2014]. In this setting
we typically use the square loss (cid:96)(m, x) = (m − x)2 for a predicted mean parameter m and
a data value x. However we note that it is straightforward to generalize our algorithm to
other kinds of data, by changing the loss function.

j

j

j

, pj, yj)}M

We assume the same kind of supervision/labels as were used with the previous SegAnnot
j=1. Each label
algorithm [Hocking et al., 2014]. We have a set of M labels Y = {(p
∈ {1, . . . , N − 1} is the start of a labeled region,
j ∈ {1, . . . , M } has three attributes: p
pj ∈ {2, . . . , N } is the end of a labeled region, and yj ∈ {0, 1} is the number of changes
, pj]. Note that we could generalize the algorithm to support other
expected in the region [p
kinds of yj values, but in this paper we only study 0/1 labels. In this labeled data setting
we want a changepoint prediction f (x) ⊆ {1, . . . , N − 1} which minimizes the number of
incorrectly predicted labels.
For example Y = {(p

= 4, p2 = 7, c2 = 1)} means that there is
no change after the ﬁrst data point, there can be 0–2 changes between data points 2 and 4
(four possibilities: no changes, change after 2, change after 3, or change after both), and there
must be exactly one change somewhere between data points 4 and 7 (three possibilities). A
more complex example with M = 3 labels is shown in Figure 1. This example shows how the
labels are typically used to encode prior knowledge about the expected/desired changepoints.
A positive label is typically used in a region with a change of low signal/noise ratio (e.g. a
change in mean from 7 to 8 after position 50). A negative label is typically used in a region
with outliers (e.g. at position 86).

= 1, p1 = 2, c1 = 0), (p

1

2

For the remainder of the paper we assume the labeled regions are ordered:

1 ≤ p

< p1 ≤ p

2

1

< p2 ≤ · · · ≤ p

M

< pM ≤ N.

(2)

If this is not the case, we can sort them in log-linear O(M log M ) time using standard
algorithms. The number of possible labels is M ∈ {0, 1, . . . , N − 1}.

2.2 Optimization problem with label constraints

The main new idea of our model is to add constraints to original optimal partitioning prob-
lem (1) in order to ensure that the changepoints predicted by the model are consistent

3

x1

p

1

τ = 0

12.5

10.0

7.5

5.0

2.5

-5700

-5800

-5900

-6000

y1 = 1

= 20

p1 = 30

p

2

= 45

y2 = 1

p2 = 55

y3 = 0

p

3

= 80

p3 = 90

d
a
t
a

Algorithm

a
n
d
m
o
d
e
l
s

x100

a

a

OPART

LOPART

τ ∗
100 = 75

τ ∗
100 = 86

τ = 99

c
o
s
t

o
f

l
a
s
t

c
h
a
n
g
e

label

0

1

0

10

20

30

40

50
position

60

70

80

90

100

Figure 1: Example with N = 100 data (grey circles) and M = 3 labels (colored rectagles),
showing the novelty of the proposed LOPART algorithm (black) with respect to the classical
OPART algorithm (blue). Both algorithms were run with the square loss and a penalty of λ =
10. Top: LOPART changepoints (vertical lines) are consistent with all three labels, whereas
the OPART model is inconsistent with the second label (should have y2 = 1 change but
OPART predicts 0) and the third label (should have y3 = 0 changes but OPART predicts 2).
Bottom: when computing the optimal cost up to position t = 100 the dynamic programming
algorithms minimize over each possible last changepoint τ using (6) for OPART and (17) for
LOPART (infeasible τ are shown with inﬁnite cost at the top of the panel).

with the labels. This is similar to the idea of SegAnnot [Hocking et al., 2014], which adds
constraints based on the labels to the segment neighborhood problem (Table 1).

To determine whether or not the predicted changepoints are consistent with the given
, pj]. To

labels, we need to count the number of predicted changes in each labeled region [p
do that we deﬁne

j

H(p, p, m) =

I[mi (cid:54)= mi+1],

p−1
(cid:88)

i=p

j

which counts the number of changepoints in the mean vector m. If the predicted number
, pj, m) is equal to the expected number of changes yj, then the model m
of changes H(p
is considered to be consistent with the label j. To deﬁne the optimization problem that we
would like to solve, we ﬁrst deﬁne an abbreviation for the cost function, which is the same
as in the previous problem (1). The cost of a mean vector m with penalty λ from data point
p to data point p is

C(p, p, m, x, λ) =

p
(cid:88)

i=p

(cid:96)(mi, xi) + λH(p, p, m).

Now we can deﬁne the labeled optimal partitioning problem using this cost function and a

4

constraint for each label,

min
m∈RN
subject to

C(1, N, m, x, λ)

for all j ∈ {1, . . . , M }, H(p

, pj, m) = yj.

j

(3)

(4)

There is one constraint per label j ∈ {1, . . . , M } (4), and each constraint ensures that the
labeled number of changes yj is predicted between p

and pj.

j

3 New Dynamic Programming Algorithm

Our main contribution is the ﬁrst algorithm which computes an optimal solution to prob-
lem (3). In this section we ﬁrst present some related sub-problems which also need to be
solved, then prove the dynamic programming update rules, and ﬁnally give pseudocode for
the algorithm.

3.1 Related optimization problems

Because our algorithm is based on ideas used to solve the optimal partitioning problem (1),
we ﬁrst review that algorithm [Jackson et al., 2005]. We need to compute the optimal loss
given a single segment with mean parameter µ starting at p and ending at p, which is

L(p, p, x) = min
µ∈R

p
(cid:88)

i=p

(cid:96)(µ, xi).

(5)

Note that for many data types and loss functions, one optimal loss value L(p, p, x) can
be computed in constant O(1) time (e.g. with real-valued data and the square loss, given
cumulative sums of the data). The dynamic programming algorithm recursively computes
the optimal cost in terms of the last changepoint τ ,

ˆCN =

min
τ ∈{0,1,...,N −1}

ˆCτ + λ + L(τ + 1, N, x).

(6)

Note that for τ = 0, there is only one segment (no changepoints), and we let ˆC0 = −λ
so that in (6) we can write the optimal cost in the same way for each value of τ . In this
paper we propose an algorithm for solving (3) based on similar ideas. The novelty of our
algorithm is that it also accounts for the label constraints (4), which reduce the space of
possible changepoint τ values that we need to search. To be clear about which constraints
are involved in each sub-problem that we need to solve, we ﬁrst deﬁne for any data point
t ∈ {1, . . . , N } the index of the last label that we need to consider when computing the cost
up to that data point,

Jt = max{0} ∪ {j : p

< t}.

j

(7)

5

We can then deﬁne the cost of the model up to t data points that is consistent with all of
the labels up to Jt,

Ct = min
m∈Rt
subject to

C(1, t, m, x, λ)

for all j ≤ Jt, H(p

, pj, m) = yj.

j

(8)

(9)

It is clear that CN as deﬁned by (8) is equivalent to the original problem we want to solve
(3). However it does not admit a simple recursion as with ˆCN in (6). For example, consider
N = 3 data with M = 1 label that forces exactly one change (p
= 1, p1 = 3, y1 = 1). The
optimal cost can be written as

1

C3 = min

(cid:40)

L(1, 1, x) + L(2, 3, x) + λ
L(1, 2, x) + L(3, 3, x) + λ

if τ = 1
if τ = 2.

(10)

First note that τ = 0 (no changepoint) is not a possibility because of the label constraint.
Also note that if τ = 1 the cost can be written recursively, i.e. C3 = C1 + L(2, 3, x) + λ.
However for τ = 2 there is no recursion involving C2, and in fact C2 is not even well-deﬁned,
because the constraint is H(1, 3, m) = I[m1 (cid:54)= m2] + I[m2 (cid:54)= m3] = 1 but in (8) there are
only two optimization variables m1, m2 (m3 is undeﬁned).

To resolve this issue we need another optimization problem which ensures that there are
no changes in the most recent label. Therefore we deﬁne another optimal cost value Vt which
we will compute for all t that occur inside the labeled regions,

Vt = min
m∈Rt
subject to

C(1, t, m, x, λ)

for all j < Jt, H(p

, pj, m) = yj,

j

and H(p

Jt

, t, m) = 0.

(11)

(12)

(13)

Note that this optimization problem (11) has the same objective function as (8) but two
kinds of constraints. The ﬁrst Jt − 1 constraints (12) ensure that the model is consistent
with all labels before label Jt. The last constraint (13) ensures that there are no changes
in the current label Jt. Continuing the example with N = 3 data points above (10), we see
that C3 can be written in terms of C1 and V2:

C3 = min

(cid:40)

C1 + L(2, 3, x) + λ
V2 + L(3, 3, x) + λ

if τ = 1
if τ = 2.

(14)

This example shows that in this case, to compute the ﬁnal optimal cost CN , we need to
compute either Ct or Vt for each t < N . In the next section we prove that this logic can be
used for any set of labeled data.

6

3.2 Dynamic programming update rules

To state the rules of the dynamic programming algorithm, we ﬁrst need to deﬁne the optimal
cost that we will result in a recursion. For any data point t ∈ {1, . . . , N } we deﬁne the optimal
cost to be

Wt =

(cid:40)

Vt
Ct

if t ∈ {p

j

+ 1, . . . , pj − 1} for some label j

otherwise.

(15)

This deﬁnition uses the Vt cost (current label constrained to have no changes) for data
points t inside labels, and the standard cost Ct otherwise (model must be consistent with
all previous labels).
In particular it is clear that WN = CN is equivalent to the optimal
cost of all N data that we would like to compute (3). We also need to deﬁne the set of
previous changepoints that we will search over to compute the optimal cost. We deﬁne this
set recursively, starting with T0 = {} (empty set) and then for any t ∈ {1, . . . , N }:

Tt =

Tt−1
Tt−1
{p

j






, . . . , t − 1}

Tt−1 ∪ {t − 1}

if ∃j : yj = 0 and t ∈ {p
if ∃j : yj = 1 and t ∈ {p
if ∃j : yj = 1 and t = pj
otherwise.

j

j

+ 1, . . . , pj}
+ 1, . . . , pj − 1}

(16)

The ﬁrst two cases of this deﬁnition require no changepoints to be added to the set, for all
data points t that are inside a label (or at the end of a negative yj = 0 label). The third case
only applies to data points t that occur at the end of a positive yj = 1 label, and reinitializes
the set to positions within that label. The ﬁnal case is used for data points t in unlabeled
regions (or at the start of a label), and adds one possible changepoint at the previous data
point t − 1. We can now give the following deﬁnition for the dynamic progamming update
rules.

Deﬁnition 1 (Dynamic programming algorithm for labeled optimal partitioning). The cost
is initialized ˜W0 = −λ and dynamic programming updates can be computed for any t ∈
{1, . . . , N } via

˜Wt = min
τ ∈Tt

˜Wτ + λ + L(τ + 1, t, x).

(17)

Note the similarity with the update rules for the unconstrained problem (6). In fact the
only diﬀerence is optimization of the last changepoint τ over Tt rather than {0, 1, . . . , t − 1}.
If there are no labels, then ˜Wt = ˆCt and Tt = {0, 1, . . . , t − 1} for all t, so the unconstrained
(6) and constrained (17) dynamic programming update rules are identical in this case. In
general, we have the following theorem which proves the optimality of the recursive dynamic
progamming update rules.

Theorem 1. The recursively computed cost ˜Wt is equal to the optimal cost Wt for all data
points t ∈ {1, . . . , N }.

7

Proof. The proof is by induction. The base case is t = 1 for which the set of changepoints
is T1 = {0} and the recursive cost is ˜W1 = ˜W0 + λ + L(1, 1, x) = C1 = W1.

Now for any t ∈ {2, . . . , N }, we assume that for all τ < t we have ˜Wτ = Wτ (induction
hypothesis), and we aim to prove that ˜Wt = Wt. We proceed by considering the diﬀerent
cases that are possible.

j

inside a labeled region. We assume t ∈ {p

+ 1, . . . , pj − 1} for some label
Case 1:
, t, m) = 0
j = Jt, so Wt = Vt is the optimal cost subject to no changes from p
If there are no
from (13) which implies a upper bound on the last changepoint τ < p
j
− 1} \ A0 where
previous positive labels then the set of possible last changes is {0, . . . , p
A0 = ∪k:yk=0{p
, . . . , pk − 1} is the set of all negative labeled regions. If there is at least one
previous positive label k then the set of possible last changes is {p
− 1}. In both
cases the set of possible changes is equal to the recursively deﬁned set Tt. For any τ ∈ Tt we
+ 1, . . . , pj − 1} for some label j, and Wτ = Cτ otherwise. Therefore
have Wτ = Vτ if τ ∈ {p
the optimal cost can be written as Wt = Vt = minτ ∈Tt Wτ + λ + L(τ + 1, t, x), which by the
induction hypothesis equals minτ ∈Tt

˜Wτ + λ + L(τ + 1, t, x) = ˜Wt.

to t, i.e. H(p
.

, . . . , p

k

k

j

j

j

j

j

+ 1, . . . , pj − 1} for any label j, so
Case 2: outside a labeled region. We assume t (cid:54)∈ {p
Wt = Ct is the optimal cost subject to all previous labels. If there are no previous positive
labels then the set of possible last changes is {0, . . . , t − 1} \ A0. If there is at least one
, . . . , t − 1} \ A0. In both
previous positive label k then the set of possible last changes is {p
cases this set of possible last changes is equal to the recursively deﬁned set Tt. Therefore,
using an argument analogous to case 1, the optimal cost is Wt = Ct = ˜Wt, which completes
the proof of optimality of the recursive update rules.

k

j

3.3 Pseudocode, implementation, complexity

Algorithm 1 (LOPART) inputs a data vector x, a non-negative penalty parameter λ, and a
set of M labels which are assumed to be sorted in increasing order (line 1). On line 2 the
algorithm initializes the cost W0 and possible changepoints T0. The for loop on line 3 imple-
ments the dynamic programming for all data points t from 1 to N . Since any changepoint
τ ∈ A0 (in a negative label) never appears in any set Tt (16), we can further optimize the
algorithm by running the dynamic programming computations of Tt, Wt, τ ∗
t for t (cid:54)∈ A0 (out-
side of negative labels). Line 4 updates the set of possible changepoints using (16). Line 5
implements update rule (17), storing the optimal cost in Wt and the optimal last changepoint
in τ ∗
t . Overall the algorithm is similar to the original optimal partitioning algorithm, but

8

with a more complex update rule on line 4 (which exploits the structure of the labels).

Algorithm 1: Labeled Optimal Partitioning (LOPART)
1 Input: Data x ∈ RN , penalty λ ∈ [0, ∞), labels y ∈ {0, 1}M , positions p, p such that

1

1 ≤ p

< p1 ≤ · · · ≤ p
2 Initialization: W0 ← −λ, T0 = {} ;
3 Dynamic progamming: for t = 1 to N do
4

< pM ≤ N ;

M

Tt ← update(Tt−1, y, p, p) // using (16) ;
Wt, τ ∗

t ← min, arg minτ ∈Tt Wτ + λ + L(τ + 1, t, x);

5
6 end

Example and comparison with classical OPART. Consider the example with N =
100 data and M = 3 labels shown in Figure 1. The classical OPART algorithm ignores
the labels, so at t = 100 it computes the optimal cost by minimizing over all possible last
changepoints τ ∈ {0, . . . , 99}, and ﬁnds that τ ∗
100 = 86 is optimal. This changepoint from
the outlier x86 = 10.0 is in a negative y3 = 0 label so the resulting model is inconsistent
with this label.
It is also inconsistent with the second positive y2 = 1 label because the
= 45 and p2 = 55. In constrast the LOPART
model predicts no changepoints between p
algorithm computes the optimal cost by minimizing over the constrained set of changepoints
T100 = {45, . . . , 79, 90, . . . , 99} and ﬁnds that τ ∗
100 = 75 is optimal. The resulting model has
changepoints that are consistent with all of the labels.

2

Computational complexity. Computing each Tt update on line 4 is amortized constant
O(1) time on average, but linear O(N ) time in the worst case (for t = pj = N when there is
a single positive label j spanning the entire data sequence). Computing each minimization
on line 5 is takes O(|Tt|) time, which is O(N ) in the worst case (for t = N when there are no
labels). The total number of operations over all iterations of the for loop is (cid:80)N
t=1 |Tt| which
is O(N ) in the best case (labels covering the entire data sequence) and O(N 2) in the worst
case (no labels). The space complexity of the algorithm is O(N ).

Implementation details. Overall the algorithm can be eﬃciently implemented in stan-
dard C using arrays. The set Tt can be implemented using an array of size N . Only the
most recent set Tt must be stored (previous sets Tτ for τ < t can be discarded), and only the
ﬁrst |Tt| elements of the array are used. Optimal cost W and last changepoint τ ∗ vectors can
also be implemented using arrays. Optimal segment mean parameters, e.g. µ in (5) from
solving L(τ + 1, t, x), can be computed and stored during the dynamic programming for loop
at no extra computational complexity. As in the original optimal partitioning algorithm,
the overall optimal changepoints can be computed by examining the values of the τ ∗ vector
starting with τ ∗
N .

Implementation for inﬁnite penalty. LOPART deﬁnes a path of optimal models. At
one extreme with penalty λ = 0 we have changes in all unlabeled regions. The model at the

9

N = 10, 000 data

1 second

OPART

1e+01

s
d
n
o
c
e
s

1e-01

1e-03

LOPART
FPOP

1.00

0.10

0.01

s
d
n
o
c
e
s

LOPART is O(N log N ) time
with O(N ) positive labels

label.density: 0.001

label.density: 0.1

1 second

1

10

100
1000
Number of labels M

1e+01

1e+03

1e+05 1e+01
Number of data N

1e+03

1e+05

Figure 2: Empirical time complexity in simulated data sets (median line and quartile band
computed over several data sets of a given size). Left: with N = 105 data, LOPART takes
the same amount of time as OPART for a small number of labels, and the same amount of
time as FPOP for a large number of labels. Right: When there are O(N ) positive labels
LOPART takes O(N log N ) time (same as FPOP). Larger label density (M/N ) reduces
constant factors.

other extreme has a change in each positive label, and no changes elsewhere. This model
can be computed when the user inputs inﬁnite penalty λ = ∞, which can be treated as a
special case. First we create a new set of labels, by keeping only the positive labels, and
putting negative labels elsewhere. Then we run Algorithm 1 with no penalty λ = 0 to get
the optimal changepoints (the optimal cost is inﬁnite).

Previous algorithms which can be used to solve special cases.
In the trivial case
of M = 0 labels, the LOPART optimization problem (3) is the same as the classic optimal
partitioning problem (1) which can be solved by the OPART algorithm [Jackson et al., 2005].
Also, when we take an inﬁnite penalty, λ = ∞, then there are no predicted changes outside
of positive labels, and the resulting model can be computed by the SegAnnot algorithm
[Hocking et al., 2014].

4 Empirical results

4.1 Empirical time complexity in simulated data sets

As discussed in the previous section, the theoretical/expected time complexity of LOPART
is O(N ) in the best case (all data labeled) and O(N 2) in the worst case (no data labeled).
To verify this empirically, we conducted timings experiments with simulated data sequences
using the standard normal distribution (no changes in mean, but these simulations are only
to evaluate time complexity, so they should be representative of real data as well because our
algorithm depends only on the number/type of labels, not the data distribution). The CPU
we used was a 2.40GHz Intel(R) Core(TM)2 Duo CPU P8600. For baselines we considered
the original OPART algorithm which is quadratic O(N 2) time [Jackson et al., 2005], and

10

the log-linear O(N log N ) time FPOP algorithm [Maidstone et al., 2016]. Both baselines
compute an optimal solution to the changepoint problem (1) with penalty λ and no label
constraints.

In the ﬁrst experiment, we ﬁxed the data set size at N = 105 (using random normal
data as explained in the previous paragraph) and used a variable number of positive labels
M ∈ {1, . . . , 1000}, each of size 9, every 10 data points. As expected, we observed LOPART
timings similar to OPART when the number of labels is small, and timings similar to FPOP
In the second experiment, we ﬁxed
when the number of labels is large (Figure 2, left).
the label density M/N = 0.001 (one positive label per 1000 data points) and varied the
number of random normal data N .
In this case LOPART is log-linear O(N log N ) time
(same as FPOP) and for N ≥ 1000 data it showed substantial speedups over the quadratic
O(N 2) time OPART (Figure 2, middle). In the third experiment, we ﬁxed the label density
M/N = 0.1 and varied the number of random normal data N . As expected with many labels,
LOPART is much faster, and in fact faster than FPOP (by constant factors) for N ≥ 1000
data (Figure 2, right). Overall these experiments show that LOPART is at least as fast as
OPART, and can be substantially faster when there are many labels.

4.2 Empirical accuracy with respect to labels in real genomic data

Data sets. To examine the changepoint prediction accuracy of LOPART, we performed the
following experiments in real genomic data. For baseline algorithms we considered OPART
[Jackson et al., 2005] and SegAnnot [Hocking et al., 2014]. Genomic scientists created labels
for 413 data sequences from cancer DNA copy number proﬁles using the SegAnnDB system
[Hocking et al., 2014].
In these data there are separate sequences for each patient and
chromosome; abrupt changes in a sequence are important diagnostic markers for aggressive
cancer subtypes [Schleiermacher et al., 2010]. The number of data points per sequence ranges
from N = 39 to 43628, and the number of labels ranges from M = 2 to 12 (with at least one
positive and one negative label per sequence).

Evaluation metrics. The main evaluation metric that we use is the total number of label
errors, which is the sum of false positives and false negatives over all labels j in the train/test
, pj, m) > yj (more predicted changes than
sets. A false positive is a label j such that H(p
, pj, m) = 0 < yj = 1
expected for either a positive or negative label), a false negative is H(p
(no predicted changes for a positive label), and a true positive is H(p
, pj, m) ≥ yj = 1
(one or more predicted changes for a positive label). We also perform Receiver Operating
Characteristic (ROC) analysis, which examines the True Positive Rate as a function of the
False Positive Rate (diﬀerent points on the ROC curve are computed using diﬀerent penalty
λ values).

j

j

j

Cross-validation setup. For each data sequence we ﬁrst randomly assigned each label to
a fold ID, and used K = 2 fold cross-validation to obtain two train/test splits per sequence
(each train/test set has at least one label per sequence). We also tried sequential rather than

11

Figure 3: Comparing LOPART with baselines in terms of best case label errors in 2-fold
cross-validation on real genomic data (penalty λ selected for each algorithm/sequence/split
by minimizing the total label errors, train+test). Left: LOPART never has more label errors
than OPART (grey horizontal line indicates equal test label errors, vertical line indicates
equal train label errors). Right: LOPART never has more test errors than SegAnnot (grey
diagonal line indicates equal test errors for LOPART and SegAnnot).

random assignment (ﬁrst half of labels on each data sequence are fold 1, second half are fold
2), and we observed qualitatively similar results (same ranking of algorithms), so we report
only the results for random assignment below.

Grid of penalty values. For each data sequence and train/test split we ran LOPART
(using only the labels in the train set) and OPART, both with a grid of 21 penalty values
evenly spaced on the log scale, λ ∈ {10−5, 10−4.5, . . . , 105}.

Best penalty analysis. The goal of this analysis is to determine label error diﬀerences
between algorithms in the best case for each algorithm (i.e. when the penalty is properly
chosen). For each split/sequence/algorithm we chose a penalty which minimized the total
number of label errors (train+test), and then we analyzed the train/test error diﬀerences
between algorithms (Figure 3).

Best penalty comparison with OPART/FPOP. Since LOPART has zero train label
errors by deﬁnition, we expected OPART to have more errors in some cases, even after
optimizing over penalty values. We observed that the best OPART model had 0 train label
errors in 719/826 = 87% of sequences/splits (counts on vertical grey line in Figure 3, left),
but 1–2 train label errors in 107/826 = 13% of sequences/splits (counts right of vertical
grey line in Figure 3, left). We also compared the number of test label errors per algorithm,
after optimizing over penalty values. We observed that LOPART had the same number of
test label errors in 780/826 = 94% of sequences/splits (counts on horizontal grey line in
Figure 3, left), and 1–2 fewer test label errors in 46/826 = 6% of sequences/splits (counts
above horizontal grey line in Figure 3, left). We did not observe any data sets or splits
for which LOPART had more train or test label errors than OPART (after optimizing over

12

6858129145912012012OPART train label errors(LOPART is always=0)Test label error difference(OPART−LOPART)012log10(seqs)Best case comparisonwith OPART2824194038155155241012024Number of LOPART true positive labels(SegAnnot always has true positives=0)Number of LOPARTfalse positive labels(SegAnnot always hasfalse positives=0)0.00.51.01.52.02.5log10(seqs)Best case comparison with SegAnnotpenalty values). These data indicate that after optimizing over penalty values LOPART is
always at least as accurate as OPART in these real data, and LOPART is sometimes more
accurate. These conclusions also hold for FPOP, because it computes the same optimal
solution as OPART.

Best penalty comparison with SegAnnot. LOPART and SegAnnot both have con-
straints that ensure zero label errors with respect to the train set, so we compared them
by computing the number of label errors with respect to the test set (after optimizing over
penalty λ values for LOPART; SegAnnot is equivalent to always taking penalty λ = ∞ in
LOPART). SegAnnot never predicts any changes in unlabeled regions, so it always has zero
false positives and maximal false negatives with respect to the test labels. We expected
LOPART to have decreased label error rates due to decreased false negative rates (it can
predict changepoints in unlabeled regions). In 324/826 = 39% of sequences/splits LOPART
and SegAnnot had the same number of test errors (counts on diagonal grey line in Figure 3,
right). In 502/826 = 61% of sequences/splits LOPART had fewer test errors than SegAnnot
(more true positives than false positives, counts below diagonal grey line in Figure 3, left).
We did not observe any sequences/splits for which LOPART had more test label errors than
SegAnnot. Overall these data indicate that LOPART with best penalty is always as accurate
as SegAnnot, and frequently more accurate in these real genomic data sets.

Predicted penalty analysis. The main goal of this analysis is to determine the extent
to which a penalty learned using OPART can be used for prediction with LOPART. The
LOPART algorithm has no train label errors for any penalty λ, because it uses the train
labels in the deﬁnition of its optimization problem (3). To choose the penalty λ to use with
LOPART, we propose to learn a penalty using OPART (which does not use the train labels in
its optimization problem, so it may have train label errors). To do this we ﬁrst run OPART
for several penalty values, then we compute label error rates for each penalty/sequence/split.
We then use three diﬀerent methods for learning/predicting the penalty λ to use for each
test data sequence:

BIC.0 uses the classical Bayesian Information Criterion of Schwarz [1978], which means
predicting λi = log Ni for each data sequence i, where Ni is the number of data points
to segment (this is unsupervised since it ignores the labels; 0 learned parameters).

constant.1 uses grid search to choose a penalty value with minimal train label errors, then
predicts this constant λ for each test data sequence (this is supervised since it uses the
labels; 1 learned parameter).

linear.2 uses the linear penalty function learning algorithm of Rigaill et al. [2013], with a
single feature xi = log log Ni for each data sequence i. To make a prediction log λi =
f (xi) = wT xi + b we ﬁrst learn the weight w and bias b using convex optimization of
a squared hinge loss which approximates the train label error (supervised; 2 learned
parameters). The feature xi = log log Ni was chosen to facilitate comparison with the

13

Figure 4: Comparing LOPART with baselines in terms of test accuracy using predicted
penalties in real genomic data. Using both unsupervised (BIC.0) and supervised (constant.1,
linear.2) penalty prediction methods, LOPART is slightly more accurate than OPART, and
much more accurate than SegAnnot (in both test sets).

unsupervised BIC penalty, which corresponds to always using w = 1, b = 0 in this
model.

We used each predicted penalty value with both OPART and LOPART, then analyzed the
test accuracy (Figure 4) and area under the ROC curve (Figure 5).

Predicted penalty comparison with OPART/FPOP. We expected that penalties
learned using OPART should result in reasonable predictions using LOPART, because the
two algorithms use the penalty in the same way (penalty λ added to cost for each change-
point). Surprisingly, we observed that LOPART is slightly but consistently more accurate
than OPART (Figure 4), with diﬀerences of 0.6–2.1% across the three penalty prediction
methods and two test folds. The ROC analysis also indicates that LOPART is slightly more
accurate than OPART (Figure 5). Over the two test folds and three penalty prediction
methods we observed that LOPART had 0.009–0.02 larger AUC than OPART. These data
indicate that OPART/FPOP can be used to learn a penalty for predition with LOPART, and
that LOPART has slightly more accurate predictions than OPART/FPOP with the learned
penalty.

Predicted penalty comparison with SegAnnot. We expected LOPART with learned
penalties to be more accurate than SegAnnot, for the same reasons as in the best penalty
comparison (SegAnnot never predicts any changes in unlabeled regions so always has 100%
false negative rate).
In agreement with this expectation, we observed that LOPART has
consistently much larger test accuracy rates than SegAnnot (Figure 4). Over the two test
folds and three penalty prediction methods, we observed improvements of 13–47% accuracy.
In the ROC analysis, SegAnnot is a single point at TPR=FPR=0% (Figure 5). Overall this
analysis indicates that LOPART yields consistently more accurate predictions than SegAnnot
in real genomic data.

14

test.fold: 1test.fold: 2406080406080BIC.0constant.1linear.2Test accuracy (percent correctly predicted labels)Penalty.ParamsAlgorithmSegAnnotOPARTLOPARTFigure 5: Receiver Operating Characteristic (ROC) analysis of penalty predictions us-
ing three methods (panels from left to right) in cross-validation using two folds (panels
from top to bottom) in real genomic data. LOPART has consistently larger Area Under
the Curve (AUC) than OPART. No ROC curve drawn for SegAnnot because it has no
penalty/regularization parameter (never predicts any changepoints in unlabeled test data).

5 Discussion and Conclusions

We proposed a new algorithm, LOPART, for changepoint detection in a partially labeled
sequence of N data. It combines ideas from Optimal Partitioning [Jackson et al., 2005] with
SegAnnot [Hocking et al., 2014], which is the only previous changepoint detection algorithm
that guarantees consistency with the given labels, but does not predict any changepoints
in unlabeled/test regions. The novelty of LOPART with respect to SegAnnot is the penal-
ized formulation, in which SegAnnot can be viewed as the special case with inﬁnite penalty;
decreasing the penalty results in increasing the number of predicted changepoints in unla-
beled/test regions.

Our theoretical result proves that that LOPART dynamic programming update rule com-
putes an optimal solution subject to the label constraints in O(N ) time in the best case, and
O(N 2) in the worst case. Our empirical timings in simulated data showed that LOPART
runs faster with more labels, and actually runs in log-linear O(N log N ) time when the num-
ber of positive labels is O(N ). Our empirical accuracy analysis using best penalties in real
genomic data showed that LOPART is always at least as accurate as the OPART/FPOP
and SegAnnot baselines, and LOPART is often more accurate. Finally our predicted penalty
analysis demonstrated the feasibility of learning a penalty using OPART/FPOP and then
using it for prediction using LOPART. Surprisingly, we observed that LOPART is slightly
more accurate than OPART/FPOP, and much more accurate than SegAnnot (using either
unsupervised penalties or supervised penalties learned with OPART/FPOP). These advan-

15

SegAnnotOPART AUC=0.907LOPART AUC=0.921SegAnnotOPART AUC=0.924LOPART AUC=0.935SegAnnotOPART AUC=0.893LOPART AUC=0.904SegAnnotOPART AUC=0.908LOPART AUC=0.917SegAnnotOPART AUC=0.908LOPART AUC=0.928SegAnnotOPART AUC=0.929LOPART AUC=0.945Penalty: BICParameters: 0Penalty: constantParameters: 1Penalty: linearParameters: 2test.fold: 1test.fold: 200.5100.5100.5100.5100.51False Positive Rate (test set labels)True Positive Rate (test set labels)tages suggest that when a user requires a model that is consistent with the given labels,
LOPART should be used rather than SegAnnot. FPOP may be preferred for its empirical
log-linear O(N log N ) complexity when there are many data N and few labels M (although
it may have some train label errors).

For future work, we would like to solve the same problem with label constraints (3)
using inequality pruning [Killick et al., 2012] or functional pruning [Maidstone et al., 2016],
which we expect would be faster (log-linear rather than quadratic, even with few labels).
Furthermore, we could use functional pruning to solve more complex problems with diﬀerent
kinds of labels [Hocking et al., 2016] and additional constraints on the directions of changes
[Hocking et al., 2017].

Reproducible research statement. Our C code that implements LOPART is in a
free/open-source R package on GitHub (https://github.com/tdhock/LOPART). We also
have created a dedicated GitHub repository with the code and data necessary to reproduce
our ﬁgures and empirical results (https://github.com/tdhock/LOPART-paper).

References

H. Akaike.

Information theory as an extension of the maximum likelihood principle.

In
B. Petrov and F. Csaki, editors, Second International Symposium on Information Theory,
pages 267–281. Akademiai Kiado, Budapest, 1973.

I. Auger and C. Lawrence. Algorithms for the optimal identiﬁcation of segment neighbor-

hoods. Bull Math Biol, 51:39–54, 1989.

O. Ford. CpVis: Interactive Visualization for Change Point Exploration and Labeling, 2020.
URL https://github.com/OllieFord/ChangepointVis. R package version 0.0.0.9000.

A. Fotoohinasab, T. Hocking, and F. Afghah. A Graph-constrained Changepoint Detection

Approach for ECG Segmentation. arXiv:2004.13558, 2020.

T. Hocking, V. Boeva, G. Rigaill, G. Schleiermacher, I. Janoueix-Lerosey, O. Delattre,
W. Richer, F. Bourdeaut, M. Suguro, M. Seto, F. Bach, and J. Vert. SegAnnDB: in-
teractive Web-based genomic segmentation. Bioinformatics, 30(11):1539–46, 2014.

T. D. Hocking, P. Goerner-Potvin, A. Morin, X. Shao, T. Pastinen, and G. Bourque. Op-
timizing ChIP-seq peak detectors using visual labels and supervised machine learning.
Bioinformatics, 33(4):491–499, 11 2016. ISSN 1367-4803.

T. D. Hocking, G. Rigaill, P. Fearnhead, and G. Bourque. A log-linear time algorithm for

constrained changepoint detection. arXiv:1703.03352, 2017.

B. Jackson, J. Scargle, D. Barnes, S. Arabhi, A. Alt, P. Gioumousis, E. Gwin, P. Sang-
trakulcharoen, L. Tan, and T. Tsai. An algorithm for optimal partitioning of data on an
interval. IEEE Signal Process Lett, 12:105–108, 2005.

16

R. Killick, P. Fearnhead, and I. A. Eckley. Optimal detection of changepoints with a linear
computational cost. Journal of the American Statistical Association, 107(500):1590–1598,
2012.

R. Maidstone, T. Hocking, G. Rigaill, and P. Fearnhead. On optimal multiple changepoint
algorithms for large data. Statistics and Computing, pages 1–15, 2016. ISSN 1573-1375.

G. Rigaill, T. Hocking, J.-P. Vert, and F. Bach. Learning sparse penalties for change-point
detection using max margin interval regression. In Proc. 30th ICML, pages 172–180, 2013.

G. Schleiermacher, I. Janoueix-Lerosey, A. Ribeiro, J. Klijanienko, J. Couturier, G. Pierron,
V. Mosseri, A. Valent, N. Auger, D. Plantaz, H. Rubie, D. Valteau-Couanet, F. Bour-
deaut, V. Combaret, C. Bergeron, J. Michon, and O. Delattre. Accumulation of segmental
alterations determines progression in neuroblastoma. Journal of Clinical Oncology, 28(19):
3122–3130, 2010. doi: 10.1200/JCO.2009.26.7955.

G. Schwarz. Estimating the dimension of a model. Ann. Statist., 6(2):461–464, 1978.

Y.-C. Yao. Estimating the number of change-points via Schwarz’ criterion. Statistics &

Probability Letters, 6(3):181–189, February 1988.

N. R. Zhang and D. O. Siegmund. A Modiﬁed Bayes Information Criterion with Applications
to the Analysis of Comparative Genomic Hybridization Data. Biometrics, 63:22–32, 2007.

17

