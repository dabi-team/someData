2
2
0
2

g
u
A
8
1

]
L
M

.
t
a
t
s
[

2
v
3
4
2
7
0
.
8
0
2
2
:
v
i
X
r
a

Convergence Rates for Stochastic Approximation on a Boundary

Kody Law1, Neil Walton2, and Shangda Yang1

1University of Manchester
2Durham University

August 19, 2022

Abstract

We analyze the behavior of projected stochastic gradient descent focusing on the case where the
optimum is on the boundary of the constraint set and the gradient does not vanish at the optimum.
Here iterates may in expectation make progress against the objective at each step. When this
and an appropriate moment condition on noise holds, we prove that the convergence rate to the
optimum of the constrained stochastic gradient descent will be diﬀerent and typically be faster than
the unconstrained stochastic gradient descent algorithm. Our results argue that the concentration
around the optimum is exponentially distributed rather than normally distributed, which typically
determines the limiting convergence in the unconstrained case. The methods that we develop rely
on a geometric ergodicity proof. This extends a result on Markov chains by Hajek [14] to the area
of stochastic approximation algorithms. As examples, we show how the results apply to linear
programming and tabular reinforcement learning.

1 Introduction.

We analyze the behavior of stochastic gradient descent focusing on the case where the optimum
is on the boundary of the constraint set and the gradient does not vanish at the optimum. For a
projected stochastic gradient descent algorithm, when the expected value of the objective decreases
on each iteration, we will show that a constrained stochastic gradient descent algorithm has a
diﬀerent rate of convergence that would be anticipated by standard results for stochastic gradient
descent.

General convergence rates are known for (projected) stochastic gradient descent with a convex
objective function. Here to obtain an (cid:15)-approximation of the optimum, we require O((cid:15)−2) iterations
[26; 27; 6]. Such results can be improved to require O((cid:15)−1) iterations when the objective function
is strongly convex [26; 27; 6]; however, even in this case, the distance to the optimum will still be
of order O((cid:15)−2). Moreover, there are commonly applied functions such as linear or piecewise linear
objectives which are not strongly convex. Thus standard results would suggest O((cid:15)−2) iterations
are required to obtain an (cid:15)-approximation of the optimizer. In this paper, we argue that a stochastic
approximation procedure can achieve the faster rate of O((cid:15)−1).

In our main result, we consider a constrained stochastic gradient descent algorithm minimizing
t=0 belong to a constraint set X and with a learning rate of the form 1/tγ
f (x) with iterates {xt}∞
with γ ∈ [0, 1]. If the optimum is on the boundary then we can expect the algorithm to make
progress against its objective in proportion to the learning rate. This is Condition C1. Further we
assume sub-exponential noise. This is Condition C2. When Condition C1 and C2 hold then we
prove that

E[f (xt)] − min
x∈X

f (x) = O

(cid:19)

.

(cid:18) 1
tγ

This result is more formally stated in Theorem 1 and, speciﬁcally, Corollary 1. An exponential
concentration bounds is also given. Taking γ = 1, we see that an (cid:15)-approximation of the optimum
requires O((cid:15)−1) iterations.

The conditions required for Theorem 1 are generic. Thus the results are not intended to be
applicable to any one particular stochastic gradient descent algorithm, nor do we place speciﬁc
design restrictions on the algorithm. What the result emphasizes is that a diﬀerent convergence
rate may hold for a broad class of stochastic gradient descent algorithms when there are constraints.

1

 
 
 
 
 
 
(a) Unconstrainted Stochastic Gradient Descent.

(b) Constrainted Stochastic Gradient Descent

Figure 1: The above plots a simulation of stochastic gradient descent algorithm with a constant step
sizeon the function f (x) = (x + 1)2. Figure 1a): When the objective is unconstrained the density
of the location of iterates is well approximated by a normal distribution with variance σ2 = O(α),
where α is the step size of the algorithm. The distance to the optimum is O(α1/2). Figure 1b): When
the value of x is constrained to the positive orthant, the distribution of iterates away for zero are
now well approximated by an exponential distribution with rate λ = O(α−1). So for step size α, the
distant to the optimum is O(α). This paper proves that the exponential decay holds more generally
for constrained stochastic approximation procedures with time-dependent learning rates.

This diﬀers and may well be faster than the well established convergence rates of stochastic gradient
descent in the unconstrained case.

The high-level intuition for this behavior in a stochastic gradient algorithm is as follows. Con-
sider a projected stochastic gradient descent algorithm with a small but ﬁxed learning rate (cf.
Section 3 for a formal deﬁnition). When the optimum is in the interior of the constraint set, the
progress of the algorithm will slow as the iterates approaches the minimizer in a manner that is
roughly proportional to the distance to the optimum.
In this regime, the process behaves in a
manner that is similar to an Ornstein-Uhlenbeck (OU) process [23]. An OU process is known to
have a normal distribution as its limiting stationary distributions. This stationary distribution
determines convergence to the optimum [8]. If we consider the same iterates but instead these are
now projected to belong to a polytope then, assuming that the optimum is on the boundary, the
resulting process behaves in a manner that is approximated by a reﬂected Brownian motion [19; 20].
When the gradient is non-zero on the boundary, it is well known that a reﬂected Brownian motion
with negative drift has an exponential distribution as its stationary distribution [15]. The normal
and exponential distributions found have a very diﬀerent concentration in probability. Thus we
can anticipate a diﬀerent convergence behavior. In particular, for the one-dimensional case, simple
calculations using Kingman’s bound [16] indicate that the distance from the optimum should be
within (cid:15) of the optimum in O((cid:15)−1) steps for a projected stochastic gradient descent algorithm. The
argument of Kingman also indicates that an exponential Lyapunov function might be more appro-
priate than the standard expansion and interpolation methods used for the convergence analysis
of stochastic gradient descent. The aim of this paper is to rigorously conﬁrm this for stochastic
approximation algorithms.

3.02.52.01.51.00.50.00.51.0x012345yy=(x+1)23.02.52.01.51.00.50.00.51.0x0.00.20.40.60.81.0DensityN(1,2)1.00.50.00.51.01.52.02.5x0246810yy=(x+1)2y=(x+1)21.00.50.00.51.01.52.02.5x01234Densityexp(x)The proof method that we develop uses a martingale argument ﬁrst described by Hajek [14] for
Markov chains. The Lyapunov function bounds of [14] and also [3] are commonly employed to give
exponential tail bounds for a stationary Markov processes. These are often applied in the analysis
of queueing networks. Here we adapt this analysis and apply it to stochastic gradient descent.
To the best of our knowledge this is the ﬁrst paper to apply this approach to gain concentration
bounds in the context of stochastic approximation.

Above, we discuss our results for projected stochastic gradient descent; however, the results
hold more broadly. In particular, the main result (Theorem 1) requires a condition on the progress
of algorithm and constraints on the noise induced by the gradient descent steps. Thus the results
in Theorem 1 are not speciﬁc to any one algorithm.
Instead they give general conditions for
exponential concentration of measure to hold for a stochastic approximation procedure. For this
reason, any constrained stochastic approximation procedure may enjoy faster convergence rates
than those achieved by the corresponding unconstrained algorithm.

However, it is also important to identify scenarios when these assumptions are known to hold.
In Theorem 2, we prove a convergence result for projected stochastic gradient descent when applied
to a linear program. Here we show that when the set of constraints are known but the costs in the
objective are unknown then the assumptions of Theorem 1 are satisﬁed and we can expect a faster
convergence rate.

Finally as an application, we apply our results to tabular reinforcement learning. We consider
a discounted Markov Decision Process (MDP) where the state transition function is known but
the reward function is unknown. Algorithms such as Q-learning are well known to require O((cid:15)−2)
iterations to reach an (cid:15)-approximation of the optimal value function. It is well known that a MDP
can be characterized as a linear program: either in a primal formulation that calculates the value
function; or in a dual formulation that gives the occupancy measure of the MDP. We apply projected
stochastic gradient descent to the dual linear programming formulation of the MDP. This can be
thought of as a relatively simple policy gradient algorithm. Here we prove a gap dependent bound
which requires O((cid:15)−1) to ﬁnd an (cid:15)-approximation of the optimal occupancy measure. This policy
gradient algorithm is faster than the rate of convergence that would be achieved for instance by
Q-learning. Certainly, bespoke algorithms exist in recent reinforcement learning literature [33; 35];
however, it is striking that with a relatively simple change –to optimize over the policy’s occupancy
measure– a standard projected gradient descent yields a faster algorithm.

Other other applications and extensions of our results would be possible. We have not considered
diﬀerent forms of averaging gradients which would likely yield a faster convergence rate. We
assume a somewhat canonical choice of variance in gradient steps.
It is
possible to investigate the dependence on the moment generating function of the gradient. This
may be applicable, for instance, in online reinforcement learning. Further, there may be diﬀerent
formulations of the exponential martingale argument given in Section 4. For instance, it is possible
to prove similar results using the Martingale approach of Kingman [17].

(See Condition C2.)

The remainder of the paper is organized as follows. In Section 2, we review relevant literature.
In Section 3, we introduce the basic model, notation and algorithms that we consider. In Section
4, we present our main result: Theorem 1. Theorem 1 gives an exponential concentration bound
for any stochastic approximation procedure satisfying two conditions, C1 and C2. We also provide
a simpler formulation of the main result, namely, Corollary 1.
In Section 5 and speciﬁcally in
Theorem 2, we apply our results to linear programs. In Section 6, we then apply Theorem 2 to
solve a tabular reinforcement learning problem. The proof of Theorem 1 is contained in Section 7.
Further results from Section 5 and Section 6 are proved in Section 8.

2 Literature Review.

We now review relevant literature. We review results on the rate of convergence of stochastic gra-
dient descent. We review the martingale approach that we use to establish exponential tail bound.
We discuss its typical usage for establishing exponential ergodicity in Markov chains. We discuss
the changes required to adapt these arguments to time-dependent stochastic approximation. We
review results on constrained stochastic approximation. We apply our results to linear programs
with unknown costs. So we brieﬂy review probabilistic concentration results in this setting. We
apply our results to reinforcement learning. We review the linear programming approach to dy-
namic programming and discuss related results in reinforcement learning.

Lyapunov Bounds. A key ingredient of our analysis is an exponential tail bound acheived using a
geometric Lyanpunov function argument. We note that such arguments are commonly employed

in order to establish the exponential ergodicity of a Markov chain. For instance, see [25, Chapter
15]. Speciﬁcally, Hajek [14] provides a proof that converts a linear drift condition into an expo-
nential Martingale that can then be used to gain fast convergence rates for ergodic Markov chains.
Extending this argument to stochastic approximation is one of the key contributions of this paper.
The argument of Hajek can be thought of as an extension of the Martingale bound of Kingman for
the G/G/1 queue [17]. Indeed it is possible to derive a diﬀerent version of the proof of the given in
this paper using the argument of Kingman; however, the approach of Hajek [14] is cleaner and so
we do not peruse this Kingman proof here [17]. There are alternative arguments for achieving an
exponential tail bound using a Lyapuov function with linear drift [3]. However, these arguments
can apply to time-inhomogeneous Markov chains, and so are not applicable in our context.

As discussed in the introduction, many of these bounds have typically been applied in the con-
text of queueing networks. This because many queueing processes can be viewed as random walks
chains constrained to belong to a polytope region. Here it is broadly known that a linear negative
drift with constraints leads to an exponential distribution bounds [15]. There are results connecting
the limit behavior of stochastic approximation procedures with constrained random walks and dif-
fusions [19]. Nonetheless, the concentration results proven here are, to the best of our knowledge,
new in the context of stochastic approximation.

Stochastic Approximation and Stochastic Gradient Descent. Due to its applicability in machine
learning, there is now a vast literature on stochastic gradient descent [5]. The argument used origi-
nally by Robbins-Monro [30] as well as more recent approaches used in online convex optimization
[37] tend to focus on interpolating the (Euclidean) distance to the optimum. This contrasts the
exponential Lyapunov approach discussed above.

As discussed in the introduction, the rate of convergence of a stochastic gradient descent proce-
t)
dure on a convex objective (either with or without projection) is well known to be within O(1/
of the optimum after t-iterations the algorithm. If it is further assumed that the objective function
is strongly convex then this rate of convergence can be improved to be O(1/t) [27; 26; 6]. These
results are standard and hold for projected stochastic gradient descent.

√

The convergence of stochastic approximation algorithms on constrained regions is given in detail
in the text of Kushner and Clark [18]. See also [19]. The geometric approach taken in this paper is
inﬂuenced by the early work on projected stochastic gradient descent [12]. See also [1, Chapter 3].
Convergence rates for constrained stochastic approximation algorithms are considered by Buche
and Kushner [7]. Here the authors note that analysis typically applied to analyze unconstrained
stochastic approximation does not readily apply to the constrained case, and a diﬀusion analysis
is conducted with results depending on which constraints of the optimization are active. Large
deviations analysis is a further method used to better understand boundary behavior in stochas-
tic approximation with constraints [10]. These results [10; 7] concern deviations around a rate of
convergence of the order O(1/
t), whereas we are interested in establishing conditions for a faster
O(1/t) rate of convergence.

√

Linear Programs and Markov Decision Processes. In this article, we apply our results to linear
programs and then to solve Markov decision processes which in both cases have unknown random
costs. This gives two examples where it is possible to apply the results of this paper.

The study of stochastic linear programs is common within operations research. We refer to
Prekopa [29], Birge and Louveaux [4] and Shapiro et al.
[32] as standard textbooks that cover
both theory as well as variety of linear and non-linear programming examples where stochastic
perturbations in their coeﬃcients can occur. We provide concentration results for our stochastic
gradient descent algorithm at the optimum. The work of [11] focuses on concentration of probability
for linear programs with random objectives.

A Markov decision process can be characterized as a linear program [24]. Schweitzer and
Seidmann [31] provides a modern formulation of a Markov decision process as a linear program.
This is an extended approach and popularized as a method for approximate dynamic program [9].
Such linear programs can be characterized in either primal or dual form. The dual form can be
more tractable for speciﬁc problem classes for example in scheduling [28]. In our case, this dual
form is most convenient.

As an example of our results, we consider tabular reinforcement learning. Here we consider a
ﬁnite state ﬁnite action Markov decision process where costs are unknown and must be estimated
by sampling.
In our case, we assume that the dynamics of the system are known. There are
multiple tabular reinforcement learning algorithms, the most well known being Q-learning [36].
Q-learning requires O((cid:15)−2) samples for an (cid:15) approximation [22]. General minimax lower-bounds
require O((cid:15)−2) samples for a (cid:15) approximation of the optimal policy [13]. Minimax results consider

the best performance on worse case problem. However, problem speciﬁc convergence rates can
improve these bounds by allowing dependence on gap between the optimal policy and a sub-optimal
policy. (This is similar to bandit algorithms where regret bounds are typically O(log T ) but the
minimax regret is typical O(
T ) [21].) For a recent analysis and bound for gap-dependent bound
for tabular RL see [33].

√

3 Model and Notation

This section acts as a brief recap of a number of well-known optimization formulations. We deﬁne
the projected stochastic gradient descent as an algorithm to solve these problems. We also deter-
mine conditions that we place on learning rates for such algorithms. We deﬁne linear programs
and Markov decision processes as special cases of our optimization formulation which we will later
analyze as example applications.

Basic Notation. We apply the convention that Z+ = {n : n = 0, 1, 2, ...} and R+ = {x : x ≥ 0}.
We apply the convention that (implied) multiplication has precedence over division, i.e. 2a/3b =
(2 × a)/(3 × b). We deﬁne a ∨ b = max{a, b} and a ∧ b = min{a, b}.

Optimization Notation. We let X denote a closed bounded convex subset of Rd. For a function
f : X → R, we consider the minimization

min
x∈X

f (x) .

(1)

We let X (cid:63) be the set of minimizers of the above optimization. We let ΠX (x) denote the projection of
x onto the set X . That is ΠX (x) := argminy∈X ||x−y||2, where ||·|| denotes the Euclidean norm. We
let d(x, X ) denote the distance from the point to its projection. That is d(x, X ) := miny∈X ||x−y||.
We let F be the gap between the maximum and minimum of f (x) on X , that is

F := max
x∈X

f (x) − min
x∈X

f (x) .

(2)

Stochastic Gradient Descent with Constraints. We consider stochastic iterative procedures for
solving the optimization (1). A standard algorithm for this problem is projected stochastic gradient
descent.

Projected stochastic gradient descent is a randomized procedure for optimizing the objective
(1) when random estimates of the gradient of f (x) are available. Speciﬁcally, the sequence {xt}∞
t=0
obeys the iteration

xt+1 = ΠX (xt − αtct) ,
(PSGD)
where, for each t ∈ Z+, αt is a positive real number and ct is a random variable such that
E[ct|c1, x1, ..., ct−1, xt] = ∇f (xt). We let Ft = (c1, x1, ..., ct−1, xt).
(More formally we let Ft
be the σ-ﬁeld generated by (c1, x1, ..., ct−1, xt) and {Ft}t≥0 is its associated ﬁltration.)

When f (x) is convex and the sequence αt obeys the condition

∞
(cid:88)

t=0

αt = ∞,

and

αt −−−→
t→∞

0 ,

(3)

then d(xt, X (cid:63)) → 0 as t → ∞ [19]. Sequences typically considered take the form αt = a/(b + t)γ
for a, b > 0 and γ ∈ [0, 1]. Instead of condition (3) in this paper, we will assume that {αt}t≥0 is a
deterministic non-increasing sequence such that

∞
(cid:88)

t=0

αt = ∞,

lim inf
t→∞

α2t
αt

> 0

and

lim
t→∞

αt − αt+1
αt

= 0 .

(4)

We do not necessarily assume that αt → 0 as we later wish to consider the case of small but
constant step sizes. The above condition is satisﬁed by any sequence of the form αt = a/(b + t)γ
for a, b > 0 and γ ∈ [0, 1]. (Note that for any sequence satisfying (3) it holds that lim inf t→∞(αt −
αt+1)/αt = 0.)

The results in the paper are not speciﬁc to Projected stochastic gradient descent. In general, our
results consider a random sequence {xt}∞
t=0 with xt ∈ X for each
t ∈ Z+. The distance between successive terms is determined by the sequence {αt}∞
t=0. Speciﬁcally,
we assume E[||xt+1 − xt||] = O(αt). We refer to any such algorithm as a constrained stochastic

t=0 adapted to a ﬁltration {Ft}∞

gradient descent algorithm. Projected stochastic gradient descent is one speciﬁc example of a con-
strained stochastic gradient descent algorithm.

Linear Programming Notation. We will be interested in the special case where X is a polytope.
That is X = {x ∈ Rd : Hx ≤ b} for a matrix H ∈ Rp×d and a vector b ∈ Rp. We assume that
the objective f is linear. That is f (x) = ¯c(cid:62)x for a constant vector ¯c ∈ Rd . Thus the above
optimization is a linear program:

minimize ¯c(cid:62)x subject to Hx ≤ b over x ∈ Rd .

We let E be the set of extreme points of the set X . The set E is ﬁnite for any polytope and the
optimum of a linear program is alway achieved at an extreme point [2]. We let E (cid:63) be the set of
extreme points in X (cid:63).

Markov Decision Process Notation. A Markov Decision Process (MDP) is a stochastic process
(St : t ∈ Z+) deﬁned on a set of states S. The states visited are determined by actions (At : t ∈ Z+)
deﬁned on the set of actions A. Speciﬁcally, the transitions to the next state St+1 is determined
by the current state St, the action At and an independent identically distributed random variable
Ut. That is there is a function F : S × A × [0, 1] → S such that

We let P (s(cid:48)|s, a) = P(St+1 = s(cid:48)|St = s, At = a) . We assume that the set of states, S, and actions,
A, are both ﬁnite. For each state and action pair (St, At), there is a real-valued cost

St+1 = F (St, At; Ut) .

Ct = c(St, At).

Costs can be positive or negative. We assume that the costs are bounded by cmax < ∞ that is
|c(s, a)| ≤ cmax for all s ∈ S and a ∈ A. A policy is a function, π : S → A, that selects an action
for each state. Speciﬁcally under policy π,

At = π(St).

Note that under a policy π the process (St : t ∈ Z+) is a Markov chain. We let Eπ
[·] be the
s0
expectation of the Markov chain S under policy π conditional on the initial state S0 = s0. We let
P denote the set of policies, which is the set of functions from X to A.

A discounted MDP has the objective to minimize the discounted costs of a MDP:

V (s0) := minimize Eπ
s0

(cid:20) ∞
(cid:88)

(cid:21)

βtCt

t=0

over π ∈ P,

where the discount factor β is such that β ∈ (0, 1).

4 An Exponential Lyapunov Bound

When the slope of the objective function does not go to zero as we approach the boundary, we can
anticipate that a stochastic gradient descent algorithm will expect to always make progress towards
minimizing its objective proportional to the learning rate αt, except perhaps at the optimum or at
points within a small region of the optimum.

More precisely, we can expect a constrained stochastic gradient descent sequence {xt}∞

t=0 to

satisfy the condition

E[f (xt+1) − f (xt)|Ft] ≤ −2αtδ
whenever f (xt) − f (x(cid:63)) ≥ αtB for some δ > 0 and some B > 0. We also place some bounds on
the size of the change in f (xt). Speciﬁcally, we assume that there exists a constant η > 0 and a
random variable Y such that

(C1)

(cid:2)|f (xt+1) − f (xt)|(cid:12)

(cid:12)Ft

(cid:3) ≤ αtY

and E[eα0ηY ] < ∞ .

(C2)

Condition (C1) need not hold for all projected gradient descent algorithms and problem instances.
Nonetheless, the condition is reasonable as one might expect a stochastic gradient descent algorithm
to make progress against its objective when away from a point with zero gradient. The Condition

(C2) is a reasonably mild assumption. For example, in projected stochastic gradient descent, this
amounts to assuming that the magnitude of gradient estimates ||ct|| have a moment generating
function, and thus a sub-exponential tail. (See Lemma 7 for a veriﬁcation of this claim.)

Shortly we will estaiblish the conditions (C1) and (C2) when applying projected stochastic
gradient descent to linear program. However, for now we leave (C1) and (C2) as general conditions
that can be satisﬁed by a constrained stochastic gradient descent algorithm.

Below we present our main convergence result. In essence, the result establishes that once the
s=1 αs is suﬃciently large the error of a constrained stochastic gradient descent is of the

sum (cid:80)t
order of αt.

Theorem 1. When Condition (C1) and (C2) are satisﬁed by a constrained stochastic gradient
descent algorithm, there exist positive constants E, G, H such that for any n with t/2n > 1

P(f (xt+1) − f (x(cid:63)) ≥ z) ≤ e− δGn

2Eαt

(z−αtB−F −α0B+(cid:80)t

s=(cid:98)t/2n(cid:99) αs

δ

2 ) + He− δGn

2Eαt

(z−αtB) .

(5)

Further, for any t such that (cid:80)t

s=(cid:98)t/2n(cid:99) αsδ ≥ 2(F + α0B) there exists a constant C such that

E[f (xt+1) − f (x(cid:63))] ≤ Cαt .

(6)

We can interpret the above result as follows. (When reading the above result, it is best to
consider the parameter n = 1. Though for faster decaying learning rates, e.g. αt = 1/t, a diﬀerent
choice of n is required to obtain the best bound.) In (5), we see that the probability that f (xt) −
f (x(cid:63)) is above z can be split into two terms. The ﬁrst term accounts for the time it takes for the
initial condition to be forgotten and for the process f (xt) to approach the value f (x(cid:63)). Notice if
step sizes αt are kept constant then this term decays exponentially fast. Following this, the second
term, accounts for excursions in the process away from f (x(cid:63)). These excursions exhibit a behavior
that is analogous to a G/G/1 queue [17] and has a tail behavior that is exponentially distributed.
Notice this is diﬀerent from typical behavior of an Ornstein-Uhlenbeck process (which we would
typically associate with constant step-size stochastic gradient descent) which is know to have a
normally distributed stationary distribution.

From (6), we see that once the dependence on initial state of the system has been accounted for
then the process f (xt) will be within a factor of αt of the optimum. In particular, for step sizes of
the form αt = 1/tγ, we have that

E[f (xt) − f (x(cid:63))] = O

(cid:19)

.

(cid:18) 1
tγ

This ﬁnal observation is conﬁrmed in the following corollary.

Corollary 1. For learning rates of the form

αt =

a
(b + t)γ

with a, b > 0 and γ ∈ [0, 1], if Conditions (C1) and (C2) are satisﬁed by a stochastic gradient
descent algorithm, then

and

P(f (xt+1) − f (x(cid:63)) ≥ z) ≤ Ie−tγ Jz

E[f (xt+1) − f (x(cid:63))] ≤

K
tγ

(7)

(8)

for time independent constants I, J and K.

It is worth remarking that Theorem 1 and Corollary 1 hold for any algorithm for which the
generic conditions (C1) and (C2) hold. Thus the results are not intended to applicable to any
one particular stochastic gradient descent algorithm, nor do we place speciﬁc design restrictions on
the algorithm. Moreover, it is not necessary to verify these conditions in order to implement any
algorithm. The result emphasizes that a convergence rate may hold for a broad class of stochastic
gradient descent algorithms when there are constraints. This convergence may well be faster than
that anticipated by the well established convergence in the unconstrained case.

5 Linear Programming Proofs

In the proof of Theorem 1 and Corollary 1, we did not specify the stochastic approximation proce-
dure used nor did we explore settings where the key Conditions C1 and C2 hold. The aim of this
section is to provide a concrete example where we prove that Conditions C1 and C2 hold and thus
Theorem 1 and Corollary 1 apply. Here we consider a linear program where the cost function that
we wish to minimize must be sampled and where the constraints of the optimization are determin-
istic. We consider the projected stochastic gradient descent algorithm where the costs sampled are
batched. When the batch size is suﬃciently large and assuming the sample would cost a reasonably
light tailed then it is possible to verify the Conditions C1 and C2. Thus in this case, we prove that
projected stochastic gradient descent has a faster convergence rate than (unconstrained) stochastic
gradient descent.

As discussed in Section 3, we are interested in solving a linear program of the form

minimize ¯c(cid:62)x subject to Hx ≤ b over x ∈ Rd ,

(9)

where X = {x ∈ Rd : Hx ≤ b} is a bounded polytope. We suppose that the constraint set X is
deterministic and known, however, the cost vector ¯c is unknown but can be sampled. Speciﬁcally,
t ∈ Z+, we let ci
t, i = 1, ..., B be independent sub-Gaussian random vectors in Rd, with mean ¯c
that is

E[ci

t|Ft] = ¯¯c

for i = 1, ..., B and t ∈ Z+, and there exists a constant λ > 0 such that

E(cid:2)eη(cid:62)ci

t|Ft−1

(cid:3) ≤ eη(cid:62)¯¯c+λ||η||2

,

∀η ∈ Rd .

(10)

We think of the set of costs (ci
stochastic gradient descent step. That is, we let

t : i = 1, ..., B) as a batch which we then average in order to make a

ct =

1
B

B
(cid:88)

i=1

ci
t ,

and then apply projected stochastic gradient descent

xt+1 = ΠX

(cid:0)xt − αtct

(cid:1) .

(11)

We can prove that the conditions of Theorem 1 are satisﬁed for linear programs. From this we

obtain the following results.

Theorem 2. For the projected stochastic gradient descent (11) applied to a linear program (9) with
learning rates of the form

αt =

a
(b + t)γ

for a, b > 0 and γ ∈ [0, 1], if the batch size B is suﬃciently large then

P(¯c(cid:62)xt+1 − ¯c(cid:62)x(cid:63) ≥ z) ≤ Ie−tγ Jz

and

E[¯c(cid:62)xt+1 − ¯c(cid:62)x(cid:63)] ≤

K
tγ

(12)

(13)

where I, J and K are time independent constants.

To prove Theorem 2, we must show that the premise of Theorem 1 is satisﬁed. Speciﬁcally, we
must prove that projected stochastic gradient descent makes constant progress against its objective.
The key ingredient that we require in order to prove is a lemma, which shows that the angle
between a sub-optimal point and the closest optimal point is always bounded away from zero.
Indeed, although we prove this results for polytope regions it is reasonable to expect that the
angle between a suboptimal point and the optimum might be bounded below for a wide variety of
optimization problems, beyond linear programs.

6 Markov Decision Process Proofs

There are a large number of linear programs for speciﬁc problems where the above analysis ap-
plied. However, we now focus in this section on the example of solving discounted Markov decision
processes using the results from the last section. Here we use a linear programming approach to
prove the convergence of a simple policy gradient algorithm. Here we considered a Markov decision
process where the dynamics of the system are known but the costs accrude are unknown.

Here we look to optimize a Markov decision process through sampling these costs. We deﬁne
a Markov decision process in Section 3. It is well known that a Markov decision process can be
formulated as a linear program, where the primal form of this linear program solves for the optimal
value function and the dual form ﬁnds the optimal occupancy measure. (See the appendix, Section
A.1, for a brief discussion on this.)

In this linear programming formula, this dual problem takes the form:

Minimize

(cid:88)

(cid:88)

¯c(s, a)x(s, a)

(Dual)

subject to

over

s∈S
(cid:88)

a∈A
x(s(cid:48), a) = ξ(s(cid:48)) + β

(cid:88)

(cid:88)

x(s, a)P (s(cid:48)|s, a),

∀s(cid:48) ∈ S

a∈A
a∈A
(x(s, a) : s ∈ S, a ∈ A) ∈ RS×A

s∈S

.

+

Here (ξ(s) : s ∈ S) is a positive vector. We assume that the dynamics as given by (P (s(cid:48)|s, a) :
a ∈ A, s, s(cid:48) ∈ S) are known but costs are unknown and must be sampled, then above we have a
linear program with an unknown objective and known constraints. For this reason we can apply
the analysis developed in the last section.

Here we assume that we are able to sample costs ¯c = (¯c(s, a) : s ∈ S, a ∈ A) where the states
and actions as distributed according to some predetermined probability distribution π = (π(s, a) :
a ∈ A, s ∈ S). There are a number of ways of sampling the cost vector ct for each t. The most
straight-forward is as follows. For each t, the cost ct = (ct(s, a) : s ∈ S, a ∈ A) is assumed as
follows. We take IID samples (si
t), i = 1, ..., B with distribution π = (π(s, a) : s ∈ S, a ∈ A)
where π(s, a) > 0 for all s ∈ S and a ∈ A. We then deﬁne

t, ai

ct(s, a) =

1
B

B
(cid:88)

i=1

¯c(si
π(si

t, ai
t)
t, ai
t)

I[(si

t, ai

t) = (s, a)] .

We can think of this method of observing costs as x experience reply. In experience reply a policy
is followed and states and actions are observed according to some distribution π. As samples
come from a Markov process, these samples are stored in memory and resampled independently at
random. The costs observed would then be approximated by the above dynamics.

We consider the projected gradient descent

xt+1 = ΠX

(cid:0)xt − αtct

(cid:1) .

(14)

Here the projection above is on to the constraint set of the dual problem:

X =

(cid:110)

x ∈ RS×A

+

(cid:88)

:

a∈A

x(s(cid:48), a) = ξ(s(cid:48)) + β

(cid:88)

(cid:88)

s∈S

a∈A

x(s, a)P (s(cid:48)|s, a), ∀s(cid:48) ∈ S

(cid:111)

.

Given the description above the following result is a direct consequence of Corollary 1.

Theorem 3. For the projected stochastic gradient descent (14) applied to a Markov Decision
Process (Dual) with learning rates of the form

αt =

a
(b + t)γ

for a, b > 0 and γ ∈ [0, 1], there exists a B0 such that for all batch sizes B ≥ B0 the following
bounds hold:

and

P(¯c(cid:62)xt+1 − ¯c(cid:62)x(cid:63) ≥ z) ≤ Ie−tγ Jz

E[¯c(cid:62)xt+1 − ¯c(cid:62)x(cid:63)] ≤

K
tγ

(15)

(16)

for time independent constants I, J and K.

The fastest convergence rate that can be obtained above is when γ = 1. We note that this rate
of convergence is faster than would typically be observed when applying a tabular reinforcement
learning algorithm such as Q-learning. The rate of convergence of Q-learning is known to be of the
order of t−1/2. We note however that the constant K given in the above bound depends on the
structure of the polytope X . Thus our bound (13) is an instance dependent bound rather than a
minimax bound which would typically be applied to gain a bound across all problem instances.

Although we will focus on the above sampling, we now breiﬂy discuss other methods where
costs can be sampled. The above sampling mimics the behaviour where there is a policy that
visits each state and action with distribution π(s, a) and then by storing in memory the states and
actions visited by the policy we can independently draw from the list of state and actions visited.
This method of achieving a batch of (approximately) independent samples is know as experience
replay. Another approach would apply to episodic environments. Here we assumed that the MDP
terminates in a ﬁnite expected time and the MDP can then be restarted from some ﬁxed state
s0 ∈ S. Under this assumption, the costs reached under a ﬁxed policy over successive episodes are
independent. We would consider B episodes and apply the average cost achieved per episode. It
is also possible that a sequence of state action cost triples is sampled directly from a ﬁxed policy.
Further, a particular choice is where the costs are sampled directly from the current policy choice
as indicated by x.

7 Proof of Theorem 1 and Corollary 1

In this section, we prove our main result, Theorem 1, as well as Corollary 1, which is a simpliﬁed
version of Theorem 1.

The following is a brief outline of the proofs of Theorem 1 and Corollary 1. The proof of Theorem
1 uses Lemma 1, Lemma 2 and Proposition 1, which are stated below. Lemma 1, although not
critical to our analysis, simpliﬁes the drift condition C1 by eliminating some additional terms and
It converts
boundary eﬀects. Lemma 2, on-the-other-hand, is a key component of our proof.
the linear drift condition C1 into an exponential bound which we then iteratively expand. The
lemma extends Theorem 2.3 from Hajek [14] by allowing for adaptive time dependent step sizes.
Proposition 1 applies standard moment generating function inequalities to the results found in
Lemma 2. After Proposition 1 is proven the proof of Theorem 1 is essentially complete. We simply
need to translate results back to the notation of our original formulation. Corollary 1 follows after
carefully bounding some of the series stated in Theorem 1. For the learning rates considered,
cleaner bounds are obtained in Corollary 1. (Three other generic, technical lemmas with standard
proofs are required. These are proven in Section A.2 of the appendix.)

We now proceed with the steps outlined above. We let

Lt := f (xt) − f (x(cid:63)) − αtB

(17)

where αt satisﬁes (4). First we simplify the above conditions C1 and C2 to give the Lyapunov
conditions (18) and (19) stated below.

Lemma 1. Given Conditions C1 and C2 hold, there exists a constant t0 such that the sequence of
random variables (Lt : t ≥ t0) satisﬁes

E(cid:2)Lt+1 − Lt

(cid:12)
(cid:12)Ft

(cid:3)I[Lt ≥ 0] < −αtδ,

and

[|Lt+1 − Lt||Ft] ≤ αtZ where D := E[eα0ηZ] < ∞ .

Proof. Applying the deﬁnition of Lt and the drift condition C1 gives

(18)

(19)

E[Lt+1 − Lt|Ft] = E[f (xt+1) − f (xt)|Ft] + (αt − αt+1)B
≤ − 2αtδ + (αt − αt+1)B
≤ − 2αtδ [1 − (αt − αt+1)B/αtδ]

Since (αt − αt+1)/αt → 0, there exists a constant t0 such that (αt − αt+1)/αt < δ/2B for all t ≥ t0.
This gives the ﬁrst drift condition (18).

For the second condition, for t ≥ t0 with t0 as just deﬁned:

(cid:2)|Lt+1 − Lt|(cid:12)

(cid:12)Ft

(cid:3) ≤ (cid:2)|f (xt+1) − f (xt)|(cid:12)
(cid:12)Ft
(αt − αt+1)
αt

≤ αtY + αt

B

(cid:3) + |αt+1 − αt|B

≤ αt(Y + δ/2) .

Taking Z = Y + δ/2, it is clear that condition (19) holds for Z as an immediate consequence of the
boundedness condition on Y in C2.

Now given (19) holds, we will convert the drift condition (18) into a exponential bound and

then iterate to give the bound below.

Lemma 2.

E[eηLt+1] ≤ E[eηLt1 ]

t
(cid:89)

ρt + D

t+1
(cid:88)

t
(cid:89)

ρk ,

for t ≥ t1 ≥ t0 where ρt = e−αtηδ+α2

k=t1

τ =t1+1
t η2E, and E = E (cid:2)(eηα0Z − 1 − α0ηZ)/η2(cid:3) < ∞.

k=τ

Proof. Let Zt = (Lt+1 − Lt)/αt. From (19), we have [|Zt||Ft] ≤ Z where E[eηα0Z] < ∞. From
(18), we have E[Zt|Ft] ≤ −δ on the event {Lt ≥ 0}. Thus, on the event {Lt ≥ 0} the following
holds:

E[eη(Lt+1−Lt)|Ft] = E[eαtηZt |Ft] = 1 + αtηE[Zt|Ft] + α2

t η2E

(cid:20) eαtηZt − 1 − αtηZt
α2
t η2

(cid:21)

(cid:12)
(cid:12)
(cid:12)Ft

= 1 + αtηE[Zt|Ft] + α2

t η2

∞
(cid:88)

k=2

1
k!

E[Z k

t |Ft]ηk−2αk−2

t

≤ 1 − αtηδ + α2

t η2

∞
(cid:88)

k=2

1
k!

E[Z k]ηk−2αk−2

0

= 1 − αtηδ + α2

t η2E

(cid:20) eηα0Z − 1 − α0ηZ
η2

(cid:21)

= 1 − αtηδ + α2
≤ e−αtηδ+α2

t η2E
t η2E =: ρt .

(20)

(21)

In the ﬁrst equality above, we apply a Taylor expansion. In the ﬁrst inequality we apply (18) and
(19) above, and also recall that αt is decreasing. In the ﬁnal inequality, we applied the standard
bound 1 + x ≤ ex. We note that ρt as deﬁne above satisﬁes ρt < 1 whenever αt < δ/ηE. We note
that E is ﬁnite since by assumption E[eηα0Z] < ∞. Also from the expansion given in (20), it is
clear that E is positive.

The bound (21) holds on the event {Lt ≥ 0}. Now notice

E[eηLt+1 |Ft] = E[eη(Lt+1−Lt)|Ft]eηLtI[Lt ≥ 0] + E[eη(Lt+1−Lt)|Ft]eηLtI[Lt < 0]

≤ ρteηLtI[Lt ≥ 0] + E[eα0Z]eηLtI[Lt < 0]
≤ ρteηLtI[Lt ≥ 0] + DI[Lt < 0]
≤ ρteηLt + D .

The ﬁrst inequality applies the above bound (21) and the second inequality applies the boundedness
condition (19). Taking expectations above gives

E[eηLt+1] ≤ ρtE[eηLt] + D.

E[eηLt+1] ≤ E[eηLt1 ]

t
(cid:89)

k=t1

ρt + D

t+1
(cid:88)

t
(cid:89)

ρk ,

τ =t1+1

k=τ

By induction, we have

as required.

The following is a technical lemma proven in the appendix that we will require for Proposition

1 below.
Lemma 3. If αt, t ∈ Z+, is a decreasing positive sequence, then

min
s=t1,...,t

(cid:41)

(cid:40) (cid:80)t
(cid:80)t

k=s αk
k=s α2
k

=

(cid:80)t

(cid:80)t

k=t1

k=t1

αk
α2
k

(22)

Moreover, if αt, t ∈ Z+ satisﬁes the learning rate condition (4) then

min
s=(cid:98)t/2n(cid:99),...,t

(cid:41)

(cid:40) (cid:80)t
(cid:80)t

k=s αk
k=s α2
k

≥

Gn
αt

(23)

for some positive constant G and for n ∈ N such that t/2n > 1.

With the moment generating function bound in Lemma 2 and the above lemma, we can bound

the tail probabilities and expectation of Lt.

Proposition 1. For any sequence satisfying (4), there exists a constant H such that

P(Lt+1 ≥ z) ≤ e− δGn
for z ≥ 0. Further, for t is such that (cid:80)t

2Eαt

(z−F −α0B+(cid:80)t

s=(cid:98)t/2n(cid:99) αs

δ

2 ) + He− δGn

2Eαt

z

s=1 αs

δ
2 ≥ F + α0B, then

E[Lt+1] ≤

2 (1 + H) E
Gnδ

αt .

Proof. We apply Lemma 2 with t1 = (cid:98)t/2n(cid:99) which gives

P(Lt+1 ≥ z) ≤ e−ηzE[eηLt]

≤ e−ηzE[eηLt1 ]

t
(cid:89)

k=t1

ρt + e−ηzD

t+1
(cid:88)

t
(cid:89)

ρk

τ =t1+1

k=τ

= e−ηzE[eηLt1 ]e

(cid:80)t

k=t1

−αkηδ+α2

kη2E + e−ηzD

t+1
(cid:88)

e

τ =t1+1

(cid:80)t

k=τ −αtηδ+α2

t η2E .

We let η be such that 0 ≤ η ≤ mint1≤s≤t

(cid:110) (cid:80)t
2 (cid:80)t

k=s αkδ
k=s α2
kE

(cid:111)

. Notice, for such η, it holds that

t
(cid:88)

k=τ

−αkηδ + α2

kη2E ≤ −

1
2

t
(cid:88)

k=τ

αkηδ,

∀τ = t1, ..., t .

Applying this to (24) gives

P(Lt+1 ≥ z) ≤ e−ηzE[eηLt1 ]e

(cid:80)t

k=t1

−αkη δ

2 + e−ηzD

≤ e−ηzE[eηLt1 ]e

(cid:80)t

k=t1

−αkη δ

2 + e−ηzD

≤ e−ηzE[eηLt1 ]e

(cid:80)t

k=t1

−αkη δ

2 + e−ηzD

t+1
(cid:88)

τ =t1+1

t+1
(cid:88)

(cid:80)t

k=τ −αkη δ
2

e

e−(t−τ )αkη δ

2

τ =t1+1
eαtη δ
1 − e−αtη δ

2

2

(24)

(25)

In the 2nd inequality above we note that αk ≥ αt for all k ≤ t. In the 3rd inequality, we note that
the summation over τ are terms from a geometric series, so we upper bound this by the appropriate
inﬁnite sum.

By Lemma 3, we see that

min
t1≤s≤t

(cid:41)

(cid:40) (cid:80)t
2 (cid:80)t

k=s αkδ
k=s α2
kE

=

δ
2E

(cid:80)t

(cid:80)t

k=t1

k=t1

αk
α2
k

≥

δGn
2αtE

for a constant G > 0. In particular, we take η = δGn/2Eαt. Thus, the bound (25) becomes

(cid:104)
P(Lt+1 ≥ z) ≤ E
e

δGnLt1
2Eαt

(cid:105)

e− δGn

2Eαt

(z+(cid:80)t

s=t1

αs

δ

2 ) + e− δGn

2Eαt

zD

4E

e δ2Gn
1 − e− δ2Gn

4E

.

Noting that Lt1 ≤ maxx∈X f (x) − minx∈X f (x) + α0B = F + α0B, by the deﬁnition of F in (2).
We simplify the above expression as follows

P(Lt+1 ≥ z) ≤ e

δGn
2Eαt

[F +α0B−z−(cid:80)t

s=(cid:98)t/2n(cid:99) αs

δ

2 ] + e− δGn

2Eαt

zD

≤ e− δGn

2Eαt

(z+(cid:80)t

s=(cid:98)t/2n(cid:99) αs

δ

2 −F −α0B) + He− δGn

2Eαt

(26)

4E

e δ2Gn
1 − e− δ2Gn
z .

4E

Above we deﬁne H := De δ2Gn

4E /(1 − e− δ2Gn
Notice if t is such that F + α0B − (cid:80)t

4E ).

s=(cid:98)t/2n(cid:99) αs

δ
2 ≤ 0, then the above inequality (26) can be

bounded by

Thus

as required.

P(Lt+1 ≥ z) ≤ (1 + H) e− δGn

2Eαt

z.

E[Lt+1] ≤ E[Lt+1 ∨ 0] =

(cid:90) ∞

P(Lt+1 ≥ z)dz ≤ (1 + H)

= 2 (1 + H)

0

αtE
2δGn ,

(cid:90) ∞

0

e− δGn

2Eαt

z dz

With Proposition 1 in place we can translate our result back into our original notation and

prove Theorem 1.

Proof of Theorem 1. From Proposition 1

P(Lt+1 ≥ z(cid:48)) ≤ e− δGn

2Eαt

(z(cid:48)−F −α0B+(cid:80)t

s=(cid:98)t/2n (cid:99) αs

δ

2 ) + He− δGn

2Eαt

z(cid:48)

for z(cid:48) ≥ 0 where f (xt) = Lt + αtB + f (x(cid:63)). Taking z(cid:48) = z − αtB, gives

P(f (xt) − f (x(cid:63)) ≥ z) = P(Lt ≥ z − αtB)

≤ e− δGn

2Eαt

(z−αtB−F −α0B+(cid:80)t

s=(cid:98)t/2n (cid:99) αs

which gives (5) as required.

Also by Proposition 1, we have

E[f (xt+1) − f (x(cid:63))] = E[Lt+1] + αt+1B

δ

2 ) + He− δGn

2Eαt

(z−αtB) ,

≤ 2 (1 + H)

(cid:20)

=

2 (1 + H)

αtE
2δGn + αt+1B

E
2δGn + B

(cid:21)

αt.

Thus taking C = [2 (1 + H) E/2δGn + B], the required bound (6) holds.

We now prove Corollary 1 as a consequence of Theorem 1. This involves carefully bounding the

terms in Theorem 1.

Proof of Corollary 1. First we consider the case γ ∈ [0, 1), after we will consider the case where
γ = 1.

When γ ∈ [0, 1) we take n = 1 in Theorem 1. Notice that

u−b
(cid:88)

s=(cid:98)(u−b)/2n(cid:99)

αs =

≥

u−b
(cid:88)

a
(b + s)γ

s=(cid:98)(u−b)/2n(cid:99)
(cid:90) u−b

a

(b + s)γ ds =

u/2−b

a
1 − γ

(cid:20)

1 −

(cid:21)

1
21−γ

u1−γ .

(27)

When t = u − b is such that

t ≥ b +

α0B + F
(cid:2)1 − 1
21−γ

a
1−γ

(cid:3) =: t0 ,

then applied to the above bound, we have that

t
(cid:88)

s=(cid:98)t/2n(cid:99)

αs

δ
2

≥ α0B + F ,

Thus applying bound (5) from Theorem 1 for t ≥ t0, with t0 as deﬁne above and n = 1, we see that

P(f (xt+1) − f (x(cid:63)) ≥ z) ≤ (1 + H)e− δG

2Eαt

(z−αtB) .

Integrating the above bound then gives

E[f (xt+1) − f (x(cid:63))] ≤

(cid:20) 2c(1 + H)
δG

(cid:21)

+ B

αt .

Now suppose that γ = 1 then the analogous bound to (27) is

u−b
(cid:88)

s=(cid:98)(u−b)/2n(cid:99)

αs =

≥

u−b
(cid:88)

a
(b + t)

s=(cid:98)(u−b)/2n(cid:99)
(cid:90) u−b

a
(b + t)

We see that if we chose n such that

u/2n−b

ds = a [log(b + t)]u−b

u/2n−b = an log 2 .

then

n =

(cid:24) (α0B + F )
a log 2

(cid:25)

t
(cid:88)

s=(cid:98)t/2n(cid:99)

αs

δ
2

≥ α0B + F ,

holds. Thus is we choose n as above and t0 = 1 then, again applying (5) from Theorem 1, we have
the bound

P(f (xt+1) − f (x(cid:63)) ≥ z) ≤ (1 + H)e− δGn

2Eαt

(z−αtB) .

Thus we see that the bound (12) holds with I := H + 1 and J := δGn/2E.

Integrating the above bound then gives

E[f (xt+1) − f (x(cid:63))] ≤

(cid:20) 2E(1 + H)
δGn

(cid:21)

+ B

αt .

Thus we see that (13) holds with K := B + 2E(1 + H)/δGn.

8 Proof of Theorem 2 and Theorem 3

Theorem 2 and Theorem 3 show how Theorem 1 and Corollary 1 can be applied in the case of
linear programming and dynamic programming.

The proof of Theorem 2 is an application of Theorem 1. As such, we are required to show that
the conditions of Theorem 1 as satisﬁed. Speciﬁcally we must prove that Conditions C1 and C2 hold.
The moment generating function bound in Condition C2 follows in a reasonably straightforward
manner when random costs have a sub-Gaussian tail behavior. However, Condition C1 requires a
more careful analysis. Here we give a geometric argument which proves that there is a non-zero
angle between any sub-optimal point in X and the nearest optimal point in X (cid:63). This is Lemma 5.
(Note that Lemma 5 holds for linear programs, though may hold for many other convex optimization
problems.) We then provide an perturbation analysis of this result in Corollary 2. We then apply
a series of probabilistic bounds to this in Proposition 2. The results from Proposition 2 can then
be collected together to verify that Conditions C1 and C2 hold and thus we prove Theorem 2.

The proof of Theorem 3 relies of the linear programming formulation of a Markov decision

process and thus Theorem 3 follows immediately from Theorem 2.

The following result bounds the angle between optimal and sub-optimal points for a polytope.
(Indeed other constrain sets X would satisfy this result and thus would potentially be amenable to
our analysis.)

Lemma 4. If X is a bounded polytope and

Then there exists a constant K > 0 such that

X (cid:63) = argminx∈X ¯c(cid:62)x.

¯c(cid:62)(x − x(cid:63))
||¯c||||x − x(cid:63)||

≥ K,

for x(cid:63) the projection of x onto X (cid:63).

Proof. We assume without loss of generality that ¯c(cid:62)x(cid:63) = 0 and ||¯c|| = 1. Let E be the extreme
points of X . Let E (cid:63) be the extreme points in X (cid:63). Then let E (cid:48) := E \ E (cid:63) and X (cid:48) is the convex
closure of E (cid:48). Let a := minx∈X (cid:48) ¯c(cid:62)x and D := maxx(cid:63)∈X (cid:63),x(cid:48)∈X (cid:48) ||x(cid:63) − x(cid:48)||. We will show we can
take K := a/D.

For all x ∈ X \ X (cid:63), x must be a convex combination of a point in X (cid:63) and a point in X (cid:48).

Specially,

x = (1 − p)x0 + px1,
for x0 ∈ X (cid:63) and x1 ∈ X (cid:48) and p ∈ (0, 1]. Then, as required,

(28)

¯c(cid:62)x
||x − x(cid:63)||

≥

¯c(cid:62)(x − x0)
||x − x0||

=

¯c(cid:62)(x1 − x0)
||x1 − x0||

≥

a
D

= K > 0 .

The ﬁrst inequality above uses the fact that x(cid:63) is closest to x. The equality applies (28). Then
ﬁnally, we apply the deﬁnitions of a, D and K.

The following lemma is a deterministic version of the result that we want. Here we provide a
geometric argument which shows that if we apply a projected gradient descent algorithm with the
deterministic cost vector ¯c then we always make progress that is proportional to step size against
the objective.

Lemma 5. If x ∈ X is such that

then ˆy = ΠX (x − α¯c) is such that

¯c(cid:62)x − ¯c(cid:62)x(cid:63) ≥ 2α||¯c||2,

¯c(cid:62) ˆy − ¯c(cid:62)x ≤ −α(cid:0)1 −

(cid:112)

1 − K 2(cid:1)||¯c||2.

(29)

(30)

Proof. The proof follows by a geometric argument. To help with this we refer the reader to Figure
2. Now, let

y(cid:48) = x + α¯c.

(31)

Let x(cid:63) be the projection of x onto X (cid:63) = argminx∈X ¯c(cid:62)x. By Lemma 4,

¯c(cid:62)(x − x(cid:63))
||x − x(cid:63)||

≥ K.

Let y(cid:48)(cid:48) be the projection of y(cid:48) onto the line segment between x and x(cid:63). By convexity, y(cid:48)(cid:48) ∈ X .
Also, by (29), y(cid:48)(cid:48) (cid:54)= x(cid:63). To see this notice that ||y(cid:48) − x|| = α||¯c|| by deﬁnition, where as

||y(cid:48) − x(cid:63)|| ≥

¯c
(cid:107)¯c(cid:107)

(y(cid:48) − x(cid:63)) ≥ 2α(cid:107)¯c(cid:107)

Thus since y(cid:48) is closer to x ∈ X than x(cid:63). So the projection, y(cid:48)(cid:48), cannot equal x(cid:63).

Further, by Lemma 4

¯c(cid:62)(x − x(cid:63))
||¯c||||x − x(cid:63)||

> K.

Note that because y(cid:48)(cid:48) is the projection of y(cid:48) onto the line between x and x(cid:63) we have that

||x − y(cid:48)(cid:48)|| = (x − y(cid:48))(cid:62) (x − x(cid:63))
||x − x(cid:63)||

.

(32)

(33)

(Note that the left and right are both ||x − y(cid:48)|| cos θ where θ is the angle between x(cid:63), x and y(cid:48).
See Figure 5.)

Combining (31), (32), and (33) gives

||x − y(cid:48)(cid:48)||
||x − y(cid:48)||

=

(x − y(cid:48))(cid:62)
||x − y(cid:48)||

(x − x(cid:63))
||x − x(cid:63)||

=

¯c(cid:62)
||¯c||

(x − x(cid:63))
||x − x(cid:63)||

> K.

Also by Pythagoras’ Theorem,

||x − y(cid:48)||2 = ||y(cid:48) − y(cid:48)(cid:48)||2 + ||y(cid:48)(cid:48) − x||2.

Figure 2: A representation of the terms in Lemma 5. Note here that, by Lemma 4, cos θ > K.

Thus,

||y(cid:48) − y(cid:48)(cid:48)||2 = ||x − y(cid:48)||2 − ||y(cid:48)(cid:48) − x||2

≤ ||x − y(cid:48)||2 − K 2||x − y(cid:48)||2
= ||x − y(cid:48)||2(1 − K 2)
= α2||¯c||2(1 − K 2).

(34)

Since ||y(cid:48) − ˆy|| ≤ ||y(cid:48) − y(cid:48)(cid:48)||, we have

||y(cid:48) − ˆy|| ≤ α||¯c||

1 − K 2.

(cid:112)

Since ¯c(cid:62)(y(cid:48) − x) ≤ −α||¯c||2, it must be that

¯c(cid:62)( ˆy − x) = ¯c(cid:62)( ˆy − y(cid:48)) + ¯c(cid:62)(y(cid:48) − x)

≤ ||¯c|| || ˆy − y(cid:48)|| − α||¯c||2
≤ α||¯c||2(cid:112)

1 − K 2 − α||¯c||2 = −α(1 −

(cid:112)

1 − K 2)||¯c||2.

Thus we see that (30) holds.

Lemma 5 shows that projected gradient descent will make constant progress on a linear program
when the objective function is known. Of course there is some room for error in this estimate. The
corollary below gives such a bound.

Corollary 2. If x ∈ X is such that

and x(cid:48) ∈ Rd is such that

¯c(cid:62)x − ¯c(cid:62)x(cid:63) ≥ 2α||¯c||2,

||x(cid:48) − x + α¯c|| ≤ αδ||¯c||

for

δ ≤ 1 −

(cid:112)

1 − K 2 ,

then ˆx = ΠX (x(cid:48)) is such that

for some constant p > 0.

¯c(cid:62) ˆx − ¯c(cid:62)x ≤ −αp||¯c||2,

(35)

(36)

Proof. The argument here follows from the proof of Lemma 5 and we reuse the notation given
there. Notice that

¯c(cid:62)( ˆx − x) = ¯c(cid:62)( ˆx − ˆy) + ¯c(cid:62)( ˆy − y)
(cid:112)

≤ ||¯c|| · || ˆx − ˆy|| − α(1 −

≤ ||¯c|| · ||x(cid:48) − y(cid:48)|| − α(1 −

≤ δ||¯c||2 − α(1 −

(cid:112)

1 − K 2)||¯c||2 .

1 − K 2)||¯c||2
(cid:112)

1 − K 2)||¯c||2

The ﬁrst inequality above applies the Cauchy-Schwartz inequality and Lemma 5. The second
follows since projections reduce distances and we apply (35) recalling that y(cid:48) := x − α¯c. The ﬁnal
inequality follows from assumptions on δ in (35).

The following is a fairly standard bound on sub-Gaussian random variables. Similar bounds

can be found in texts such as Vershynin [34]. A proof of Lemma 6 can be found in Appendix B.

Lemma 6. Given that the costs ¯ci

t are sub-Gaussian, it holds that

(cid:32)

(cid:13)
(cid:13)
(cid:13)

P

1
B

B
(cid:88)

i=1

(cid:33)

(cid:13)
ci
(cid:13)
(cid:13) ≥ δ||¯c||
t − ¯c

≤ 3d−1e− δ2

16

||¯c||2

λ B

(37)

for all δ > 0

The following proposition applies Lemma 6 to Corollary 2. So that we have a bound on the

expected drift of our stochastic gradient descent algorithm.

Proposition 2. There exist a batch size B such that

E[¯c(cid:62)xt+1 − ¯c(cid:62)xt|Ft] ≤ −αp(cid:48)||¯c||

on the event

{¯c(cid:62)xt − ¯c(cid:62)x(cid:63) ≥ 2α||¯c||2}

for some p(cid:48) > 0.

Proof. Let x(cid:48)

t = xt − αtct. By Corollary 2, if

αt||ct − ¯c|| = ||x(cid:48)

t − xt + αt ¯c|| ≤ αtδ||¯c||

(38)

then

¯c(cid:62)xt+1 − ¯c(cid:62)xt ≤ −αtp||¯c||2 .

Thus when (38) holds progress against the linear programming objective is guarenteed. We now
seek to bound the error when (38) does not hold.
Applying the Cauchy-Schwarz Inequality,

¯c(cid:62)xt+1 − ¯c(cid:62)xt ≤ ||¯c||||xt+1 − xt||

≤ αt||¯c|| · ||ct||.

From this upper-bound, observe that the errors are determined by the ﬂuctuations in ct. Further,
notice that the left-hand side of (38) equals αt||ct − ¯c||. Given these two observations, we analyze
the term

(cid:104)
||ct|| · I(cid:2)||ct − ¯c|| ≥ δ||¯c||(cid:3)(cid:105)
E

.

This term bounds the error in ¯c(cid:62)xt+1 − ¯c(cid:62)xt when (38) does not hold.

Notice that by the Triangle Inequality

(cid:104)
E

||ct|| · I(cid:2)||ct − ¯c|| ≥ δ||¯c||(cid:3)(cid:105)

≤ ||¯c|| · P(cid:0)||ct − ¯c|| ≥ δ||¯c||(cid:1)

(cid:104)
+ E

||ct − ¯c|| · I(cid:2)||ct − ¯c|| ≥ δ||¯c||(cid:3) (cid:105)

.

We now bound the two terms (39) and (40).
We can bound (39) using Lemma 6

P(cid:0)||ct − ¯c|| ≥ δ||¯c||(cid:1) ≤ 3d−1e− δ2

16

||¯c||2

λ B .

We can bound (40) with Lemma 6 also. Speciﬁcally,

(cid:104)
E

||ct − ¯c|| · I(cid:2)||ct − ¯c|| ≥ δ||¯c||(cid:3)(cid:105)

= δ−1

≤ δ−1

(cid:90) ∞

δ||¯c||

(cid:90) ∞

xP(cid:0)||ct − ¯c|| ≥ x(cid:1) dx

x3d−1e− B

16

x2
λ dx

δ||¯c||
16 · 3d−1λ
Bδ

=

e− δ2

16

||¯c||2

λ B .

(39)

(40)

(41)

(42)

Applying (41) & (42) to (39) & (40) above gives that

(cid:104)
E

(cid:105)
||ct|| · I[||ct − ¯c|| ≥ δ||¯c||]

≤ 3d−1

(cid:20)

1 +

(cid:21)

16 · 3d−1λ
Bδ

e− δ2

16

||¯c||2

λ B .

We can the combine the above inequalities

E[¯c(cid:62)xt+1 − ¯c(cid:62)xt|Ft]
(cid:104)
= E

(cid:12)
(cid:104)
+ E
(¯c(cid:62)xt+1 − ¯c(cid:62)xt)I[||ct − ¯c|| ≤ δ||¯c||]
(cid:12)
(cid:12)Ft
(cid:104)
≤ −αtp||¯c|| · P(cid:0)||ct − ¯c|| ≤ δ||¯c||(cid:1) + αt||¯c|| · E

(cid:105)

(¯c(cid:62)xt+1 − ¯c(cid:62)xt)I[||ct − ¯c|| ≥ δ||¯c||]
(cid:12)
||ct||I[||ct − ¯c|| ≥ δ||¯c||]
(cid:12)
(cid:12)Ft

(cid:105)

(cid:105)

(cid:12)
(cid:12)
(cid:12)Ft

≤ −αtp||¯c||

(cid:18)

1 − 3d−1e− δ2

16

||¯c||2

λ B

(cid:19)

+ αt||¯c||3d−1

(cid:20)

1 +

(cid:21)

16 · 3d−1λ
Bδ

e− δ2

16

||¯c||2

λ B

= −αtp||¯c|| + αt||¯c||e− δ2

16

(cid:20)

||¯c||2

λ B

2 +

(cid:21)

16 · 3d−1λ
Bδ

= −αtp(cid:48)||¯c||

where

p(cid:48) = p − e− δ2

16

||¯c||2

λ B

(cid:20)

2 +

16 · 3d−1λ
Bδ

(cid:21)

.

Since p is positive and independent of B it should be clear that for B suﬃciently large we can
choose p(cid:48) > 0.

The proof of Theorem 2 is now immediate.

Proof of Theorem 2. By Proposition 2, it holds that

E[¯c(cid:62)xt+1 − ¯c(cid:62)xt|Ft] ≤ −2αtp(cid:48)||c||

(43)

whenever ¯c(cid:62)xt − ¯c(cid:62)x(cid:63) ≥ 2α||¯c||2. In otherwords, Condition C1 of Theorem 1 is satisﬁed with
δ = 2||¯c|| and B = p(cid:48)||¯c||.

Further we show that the moment generating function condition on increments, C2, is satisﬁed.

In particular notice

(cid:2)|¯c(cid:62)xt+1 − ¯c(cid:62)xt|(cid:12)

(cid:12)Ft

(cid:3) = αt ¯c(cid:62)ct .

Thus in the context of condition C1 we can take Y = ¯c(cid:62)ct. Notice by the sub-Gaussian assumption
on costs (10), for ct = (cid:80)B

t/B, we have that

i=1 ci

E[eY ] = E[e

(cid:80)B

i=1 ¯c(cid:62)ci

t/B] ≤ [e¯c(cid:62) ¯c/B+λ||¯c||2/B2

]B = e||¯c||2+λ||¯c||2/B < ∞ .

Thus we see that the moment generating function condition C2 is also satisﬁed. Since Condition
C1 and C2 hold Theorem applies (as well as corollary). Thus theorem holds.

A Proofs of Additional Results

A.1 Linear Programming Formulation of MDPs.

In this section, we brieﬂy recall the linear programming formulation of a Markov decision process.
The optimum of a discounted Markov decision process (cf. Section 3) is the unique solution to the
Bellman equation:

(cid:110)

V (s) = min
a∈A

c(s, a) + β

V (s(cid:48))P (s(cid:48)|s, a)

(cid:111)
.

(cid:88)

s(cid:48)∈S

The value function V = (V (s) : s ∈ S) is (component-wise) the largest vector such that the
following inequality holds:

V (s) ≤ c(s, a) + β

(cid:88)

s(cid:48)∈S

V (s(cid:48))P (s(cid:48)|s, a)

s ∈ S, a ∈ A.

In other words, the optimal value function V can be found by solving the following linear program:

Maximize

ξ(s)V (s)

(cid:88)

s∈S

subject to

V (s) ≤ c(s, a) + β

V (s(cid:48))P (s(cid:48)|s, a), ∀s ∈ S, a ∈ A

(cid:88)

s(cid:48)∈S

over

(V (s) : s ∈ S) ∈ RS

where ξ = (ξ(s) : s ∈ S) is a positive vector. (Here setting ξ(s) = 1 for all s ∈ S would be a
standard choice.) The dual of the above linear program is

Minimize

subject to

over

(cid:88)

s∈S
(cid:88)

c(s, a)x(s, a)

(Dual)

x(s(cid:48), a) = ξ(s(cid:48)) + β

(cid:88)

(cid:88)

x(s, a)P (s(cid:48)|s, a),

∀s(cid:48) ∈ S

a∈A
a∈A
(x(s, a) : s ∈ S, a ∈ A) ∈ RS×A

s∈S

.

+

Here the variables x(s, a) are sometimes referred to as occupancy measures. For a given variable x
satisfying the constraints of the dual, we can determine a policy

π(a|s) =

x(s, a)
a(cid:48)∈A x(s, a(cid:48))

(cid:80)

.

A.2 Proof of Additional Lemma’s from Section 4.
Lemma 3. If αt, t ∈ Z+, is a decreasing positive sequence, then

min
s=t1,...,t

(cid:41)

(cid:40) (cid:80)t
(cid:80)t

k=s αk
k=s α2
k

=

(cid:80)t

(cid:80)t

k=t1

k=t1

αk
α2
k

(44)

Moreover, if αt, t ∈ Z+ satisﬁes the learning rate condition (4) then for n ∈ N such that t/2n > 1
(cid:40) (cid:80)t
(cid:80)t

min
s=(cid:98)t/2n(cid:99),...,t

(45)

(cid:41)

≥

Gn
αt

k=s αk
k=s α2
k

for some positive constant G.

Proof. It is straight-forward to show that for a, a(cid:48), A, A(cid:48) > 0

a
a(cid:48) ≤

A
A(cid:48)

if and only if

a + A
a(cid:48) + A(cid:48) ≤

A
A(cid:48) .

Take positive numbers as, a(cid:48)

[Note that both expression above are equivalent to AA(cid:48) + aA(cid:48) ≤ AA(cid:48) + a(cid:48)A.]
s, s = 1, ..., t. If
as
a(cid:48)
s

≤

ak
a(cid:48)
k

for k = s − 1, ..., t, then

Thus,

t
(cid:88)

a(cid:48)
kas ≤

t
(cid:88)

aka(cid:48)
s.

k=s−1

k=s−1

as
a(cid:48)
s

≤

(cid:80)t

(cid:80)t

.

k=s−1 ak
k=s−1 a(cid:48)
k
k=s−1 a(cid:48)

Thus applying (46) with A = (cid:80)t

k=s−1 ak and A(cid:48) = (cid:80)t

k gives

(cid:80)t

(cid:80)t

k=s ak
k=s a(cid:48)
k

≤

(cid:80)t

(cid:80)t

k=s−1 ak
k=s−1 a(cid:48)
k

.

(49)

Finally, taking ak = αk and a(cid:48)
k = α2
(49), we see that the result (44) holds.

k, we see that (48) holds since αt is decreasing. Thus, from

(46)

(47)

(48)

If the condition (4) holds then lim inf t→∞ α2t/αt > 0 implies

√

G

>

αt
α(cid:98)t/2(cid:99)

(50)

for some G > 0. Since the sequence is decreasing and (50) holds, we have that

(cid:80)t

(cid:80)t

k=(cid:98)t/2n(cid:99) αk
k=(cid:98)t/2n(cid:99) α2
k

≥

(t − (cid:98)t/2n(cid:99))αt

(t − (cid:98)t/2n(cid:99))α2

(cid:98)t/2n(cid:99)

=

α2
t

α2

(cid:98)t/2n(cid:99)

1
αt

=

α2
t

α2

(cid:98)t/2(cid:99)

× ... ×

α2
(cid:98)t/2n−1(cid:99)
α2

(cid:98)t/2n(cid:99)

1
αt

≥

Gn
αt

.

Applying this to (44) with t1 = (cid:98)t/2n(cid:99) gives

min
s=(cid:98)t/2n(cid:99),...,t

(cid:41)

(cid:40) (cid:80)t
(cid:80)t

k=s αk
k=s α2
k

≥

Gn
αt

.

Thus (45) holds as required.

Lemma 7. For projected stochastic gradient descent [cf. (PSGD)], if for all t ∈ Z+ there exists
a, b > 0 such that

and ∇f (x) is continuous, then (C2) holds, that is

P(||ct|| ≥ z|Ft) ≤ ae−bz

(cid:104)

|f (xt+1) − f (xt)|(cid:12)

(cid:12)Ft

(cid:105)

≤ αtY , with E[eηY ] < ∞

(51)

(52)

for some η > 0.

Proof. Notice by Taylor’s Theorem, for some yt belonging to the line segment between xt and
xt+1, it holds that

|f (xt+1) − f (xt)| =|(xt+1 − xt) · ∇f (yt)|

≤ ||xt+1 − xt|| · ||∇f (yt)||
≤ αt||ct|| max
x∈X

||∇f (x)|| .

Thus we can see that the moment generating function condition (52) holds if we can prove that

E[eη||ct|||Ft] < ∞

(53)

for some η > 0.

The condition (53) is a consequence of (51). Speciﬁcally,

E[eη||ct|||Ft] =

(cid:90) ∞

P(eη||ct|| > y |Ft)dy

0

≤ 1 +

≤ 1 +

(cid:90) ∞

1
(cid:90) ∞

1

(cid:16)

P

||ct|| ≥

log y

(cid:17)

(cid:12)
(cid:12)
(cid:12)Ft

dy

1
η

ay−b/ηdy < ∞

for 0 < η < b .

(54)

Thus from (54) we see that (53) holds and thus the required condition (52) holds.

B Additional Lemmas from Section 5

Lemma 6. Given that the costs ci

t are sub-Gaussian, it holds that

(cid:32)

(cid:13)
(cid:13)
(cid:13)

P

1
B

B
(cid:88)

i=1

ci
t − c

(cid:33)

(cid:13)
(cid:13)
(cid:13) ≥ δ||c||

≤ 3d−1e− δ2

16

||c||2

λ B

(55)

for all δ > 0

Proof. The proof follows a reasonably standard argument for random vectors. (For instance, see
[34, Chapter 4].)

Letting ct = (cid:80)B

i=1 ci

t/B, ﬁrst notice that

(cid:107)ct − c(cid:107) = sup

η(cid:62)(ct − c) .

η:||η||=1

(56)

2 -net of the sphere Sd−1 := {η ∈ Rd : (cid:107)η(cid:107) = 1}. (Recall that N is an (cid:15)-net is
We let N be the 1
a subset of S such that all points in S are within (cid:15) of a point in N .) The following bound on the
cardinality of N is known

from [34, Corollary 4.2.13].

Further it is straight-forward to show that

|N | ≤ 3d ,

(cid:107)ct − c(cid:107) ≤ 2 sup
η∈N

η(cid:62)(ct − c) .

(57)

(58)

To see why (58) holds: let η(cid:63) achieve the maximum in (56) and let η0 be the point closest to η(cid:63) in
N . Then

(cid:107)ct − c(cid:107) = η(cid:62)

(cid:63) (ct − c) =(η(cid:63) − η0)(cid:62)(ct − c) + η(cid:62)
≤ (cid:107)η(cid:63) − η0(cid:107) (cid:107)ct − c(cid:107) + sup
η∈N

0 (ct − c)

η(cid:62)(ct − c)

≤

1
2

(cid:107)ct − c(cid:107) + sup
η∈N

η(cid:62)(ct − c)

(59)

In the ﬁrst inequality above, we apply the Cauchy-Schwartz Inequality; then we apply the deﬁnition
of η0; rearranging (59) gives (58), as required.

We now achieve the bound (55) as follows: for θ = δ||c||/4λ ,

(cid:32)

(cid:13)
(cid:13)
(cid:13)

P

1
B

B
(cid:88)

i=1

ci
t − c

(cid:33)

(cid:13)
(cid:13)
(cid:13) ≥ δ||c||

(cid:16)

= P

sup
η∈Sd−1

(cid:17)
η(cid:62)(ct − c) ≥ δ||c||

≤ P

(cid:18)

2 sup
η∈N

η(cid:62)(ct − c) ≥ δ||c||

(cid:19)

≤ |N | sup
η∈N

P

(cid:18)

η(cid:62)(ct − c) ≥

(cid:19)

δ||c||
2

≤ 3d−1 sup
η∈N

≤ 3d−1 sup
η∈N

(cid:16)

P

eBθη(cid:62)(ct−c) ≥ eBθ δ||c||

2

(cid:17)

e−Bθ δ||c||

2 E

eBθη(cid:62)(ct−c)(cid:105)
(cid:104)

(60)

(61)

(62)

(63)

e−Bθ δ||c||

2 E

(cid:104)
eθη(cid:62)(ci

t−c)(cid:105)B

= 3d−1 sup
η∈N
≤ 3d−1e−Bθ δ||c||
= 3d−1e−B δ2 ||c||2

16λ

2 eθ2λB

.

(64)

The equality (60) follows from (56) and the deﬁnition of ct. The inequality (61) applies (58). The
inequality (62) is a union bound. Inequality (63) applies the bound on the cardinality of |N | from
(57). The inequalities from (63) to (64) applies the sub-Gaussian assumption on costs along with
standard moment generating function bounds for sums of independent random variables.

References

[1] D. Bertsekas. Convex optimization algorithms. Athena Scientiﬁc, 2015.

[2] D. Bertsimas and J. N. Tsitsiklis.
Scientiﬁc Belmont, MA, 1997.

Introduction to linear optimization, volume 6. Athena

[3] D. Bertsimas, D. Gamarnik, and J. N. Tsitsiklis. Performance of multiclass markovian queueing
networks via piecewise linear lyapunov functions. Annals of Applied Probability, pages 1384–
1428, 2001.

[4] J. R. Birge and F. Louveaux. Introduction to stochastic programming. Springer Science &

Business Media, 2011.

[5] L. Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of

COMPSTAT’2010, pages 177–186. Springer, 2010.

[6] L. Bottou, F. E. Curtis, and J. Nocedal. Optimization methods for large-scale machine learn-

ing. Siam Review, 60(2):223–311, 2018.

[7] R. Buche and H. J. Kushner. Rate of convergence for constrained stochastic approximation

algorithms. SIAM journal on control and optimization, 40(4):1011–1041, 2002.

[8] Z. Chen, S. Mou, and S. T. Maguluri. Stationary behavior of constant stepsize SGD type
algorithms: An asymptotic characterization. Proceedings of the ACM on Measurement and
Analysis of Computing Systems, 6(1):1–24, 2022.

[9] D. P. De Farias and B. Van Roy. The linear programming approach to approximate dynamic

programming. Operations research, 51(6):850–865, 2003.

[10] P. Dupuis and H. J. Kushner. Asymptotic behavior of constrained stochastic approximations

via the theory of large deviations. Probability theory and related ﬁelds, 75(2):223–244, 1987.

[11] M. E. Dyer, A. M. Frieze, and C. J. McDiarmid. On linear programs with random costs.

Mathematical Programming, 35(1):3–16, 1986.

[12] E. M. Gafni and D. P. Bertsekas. Convergence of a gradient projection method. 1982.

[13] M. Gheshlaghi Azar, R. Munos, and H. J. Kappen. Minimax PAC bounds on the sample
complexity of reinforcement learning with a generative model. Machine learning, 91(3):325–
349, 2013.

[14] B. Hajek. Hitting-time and occupation-time bounds implied by drift analysis with applications.

Advances in Applied probability, 14(3):502–525, 1982.

[15] J. M. Harrison and R. J. Williams. Multidimensional reﬂected brownian motions having

exponential stationary distributions. The Annals of Probability, pages 115–137, 1987.

[16] J. Kingman. The single server queue in heavy traﬃc.

In Mathematical Proceedings of the
Cambridge Philosophical Society, volume 57, pages 902–904. Cambridge University Press, 1961.

[17] J. F. Kingman. A martingale inequality in the theory of queues. In Mathematical Proceedings of
the Cambridge Philosophical Society, volume 60, pages 359–361. Cambridge University Press,
1964.

[18] H. Kushner and D. Clark. Stochastic Approximation Methods for Constrained and Un-
ISBN

constrained Systems. Applied Mathematical Sciences. Springer New York, 1978.
9781468493528.

[19] H. Kushner and G. Yin. Stochastic Approximation and Recursive Algorithms and Applications.
Stochastic Modelling and Applied Probability. Springer New York, 2003. ISBN 9780387008943.

[20] H. J. Kushner and H. Kushner. Heavy traﬃc analysis of controlled queueing and communica-

tion networks, volume 28. Springer, 2001.

[21] T. Lattimore and C. Szepesv´ari. Bandit algorithms. Cambridge University Press, 2020.

[22] G. Li, Y. Wei, Y. Chi, Y. Gu, and Y. Chen. Sample complexity of asynchronous q-learning:
Sharper analysis and variance reduction. Advances in neural information processing systems,
33:7031–7043, 2020.

[23] S. Mandt, M. Hoﬀman, and D. Blei. A variational analysis of stochastic gradient algorithms.

In International conference on machine learning, pages 354–363. PMLR, 2016.

[24] A. S. Manne. Linear programming and sequential decisions. Management Science, 6(3):259–

267, 1960.

[25] S. P. Meyn and R. L. Tweedie. Markov chains and stochastic stability. Springer Science &

Business Media, 2012.

[26] E. Moulines and F. Bach. Non-asymptotic analysis of stochastic approximation algorithms for

machine learning. Advances in neural information processing systems, 24, 2011.

[27] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation ap-
proach to stochastic programming. SIAM Journal on optimization, 19(4):1574–1609, 2009.

[28] J. Patrick, M. L. Puterman, and M. Queyranne. Dynamic multipriority patient scheduling for

a diagnostic resource. Operations research, 56(6):1507–1525, 2008.

[29] A. Pr´ekopa. Stochastic programming, volume 324. Springer Science & Business Media, 2013.

[30] H. Robbins and S. Monro. A stochastic approximation method. The annals of mathematical

statistics, pages 400–407, 1951.

[31] P. J. Schweitzer and A. Seidmann. Generalized polynomial approximations in markovian
decision processes. Journal of mathematical analysis and applications, 110(2):568–582, 1985.

[32] A. Shapiro, D. Dentcheva, and A. Ruszczynski. Lectures on stochastic programming: modeling

and theory. SIAM, 2021.

[33] M. Simchowitz and K. G. Jamieson. Non-asymptotic gap-dependent regret bounds for tabular

mdps. Advances in Neural Information Processing Systems, 32, 2019.

[34] R. Vershynin. High-dimensional probability: An introduction with applications in data science,

volume 47. Cambridge university press, 2018.

[35] X. Wang, Q. Cui, and S. S. Du. On gap-dependent bounds for oﬄine reinforcement learning.

arXiv preprint arXiv:2206.00177, 2022.

[36] C. J. Watkins and P. Dayan. Q-learning. Machine learning, 8(3):279–292, 1992.

[37] M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In
Proceedings of the 20th international conference on machine learning (icml-03), pages 928–936,
2003.

