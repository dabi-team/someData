2
2
0
2

p
e
S
9

]

G
L
.
s
c
[

2
v
1
7
2
7
0
.
7
0
2
2
:
v
i
X
r
a

Set-based Value Operators for Non-stationary and Uncertain
Markov Decision Processes ‹

Sarah H.Q. Li a, Assal´e Adj´e b, Pierre-Lo¨ıc Garoche c, Beh¸cet A¸cikme¸se a

aDepartment of Aeronautics and Astronautics, University of Washington, Seattle, USA. (e-mail:{sarahli, behcet}@uw.edu).

bLAMPS, Universit´e de Perpignan Via Domitia, Perpignan, France. (e-mail: assale.adje@univ-perp.fr).

c ´Ecole Nationale de l’Aviation Civile, Universit´e de Toulouse, Toulouse, France. (e-mail: Pierre-Loic.Garoche@enac.fr).

Abstract

This paper analyzes ﬁnite state Markov Decision Processes (MDPs) with uncertain parameters in compact sets and re-examines
results from robust MDP via set-based ﬁxed point theory. To this end, we generalize the Bellman and policy evaluation
operators to contracting operators on the value function space and denote them as value operators. We lift these value operators
to act on sets of value functions and denote them as set-based value operators. We prove that the set-based value operators are
contractions in the space of compact value function sets. Leveraging insights from set theory, we generalize the rectangularity
condition in classic robust MDP literature to a containment condition for all value operators, which is weaker and can be
applied to a larger set of parameter-uncertain MDPs and contracting operators in dynamic programming. We prove that
both the rectangularity condition and the containment condition suﬃciently ensure that the set-based value operator’s ﬁxed
point set contains its own extrema elements. For convex and compact sets of uncertain MDP parameters, we show equivalence
between the classic robust value function and the supremum of the ﬁxed point set of the set-based Bellman operator. Under
dynamically changing MDP parameters in compact sets, we prove a set convergence result for value iteration, which otherwise
may not converge to a single value function. Finally, we derive novel guarantees for probabilistic path planning problems in
planet exploration and stratospheric station-keeping.

Key words: Markov decision process, contraction operator, stochastic control, decision making and autonomy

1 Introduction

Markov decision process (MDP) is a versatile model for
decision making in stochastic environments and is widely
used in trajectory planning [1], robotics [22], and oper-
ations research [4]. Given state-action costs and transi-
tion probabilities, ﬁnding an optimal policy of the MDP
is equivalent to solving for the ﬁxed point value function
of the corresponding Bellman operator. Known as dy-
namic programming approaches, the resulting solutions
extend to the dynamic-free setting via value-based rein-
forcement learning [20].

Many application settings of MDPs, including traﬃc
light control, motion planning, and dexterous manip-
ulation, deal with environmental non-stationarity—
dynamically changing MDP cost and transition prob-
factors or the presence of
abilities due to external

interfering decision makers. This environmental non-
stationarity corresponds to uncertainty in the MDP
transition and cost parameters and diﬀers from an
MDP’s internal stochasticity, which corresponds to sta-
tionary MDP transition and cost parameters and mod-
els the stochastic dynamics whose probability distribu-
tions do not change over time. Under environmental
non-stationarity, robust MDP, risk-sensitive reinforce-
ment learning, and zero-sum stochastic games derive
policies for the optimal worst-case performance—value
functions that result from adversarial selections of the
MDP parameters. A standard approach is to take an
appropriate Bellman operator-variant and solve for its
ﬁxed point using a minmax formulation. By assuming
that the MDP parameters are chosen adversarially,
the worst-case approach is able to derive asymptotic
bounds of the value function trajectory aﬀected by the
parameter non-stationarity.

‹ This paper was not presented at any IFAC meeting. Cor-
responding author Sarah H.Q. Li. Email. sarahli@uw.edu.

Much of the robust MDP literature focuses on the
worst-case analysis of the MDP policy improvement and

Preprint submitted to Automatica

13 September 2022

 
 
 
 
 
 
MDP policy evaluation problem under a rectangularity
assumption on a set of MDP parameters. Mathemat-
ically, these problems correspond to the Bellman and
policy evaluation operators. However, recent progress
in learning-based methods utilizes other contraction
operators such as Q-learning [23] and temporal diﬀer-
ence [21]. In order to broaden the application of our
results, we ground our analysis in a more general class
of value operators, and ask the following question from
a less adversarial perspective: is it possible to charac-
terize the transient behavior of an MDP contraction
operator with dynamically changing parameters?

Contributions. For compact sets of ﬁnite state MDP
parameter uncertainties, we propose the set-extensions
of value operators: a general class of contraction opera-
tors that are order-preserving on the space of value func-
tions and Lipschitz in the space of MDP parameters. We
prove the existence of compact ﬁxed point sets of the set-
based value operators and show that the set-based value
iteration converges. In a non-stationary Markovian en-
vironment, standard value iteration may not converge.
However, we can show that the point-to-set distance of
the resulting value function trajectory to the ﬁxed point
set always goes to zero in the limit. We derive a contain-
ment condition that is suﬃcient for the ﬁxed point sets
to contain their own extremal elements. Within robust
MDPs, we show that the containment condition general-
izes the rectangularity condition, such that the optimal
worst-case policy, or the robust policy, exists when the
containment condition is satisﬁed. We then derive the
relationship between the ﬁxed point sets of 1) the set-
based optimistic policy evaluation operator, 2) the set-
based robust policy evaluation operator, and 3) the set-
based Bellman operator. Given a value operator and a
compact MDP parameter uncertainty set, we present an
algorithm that computes the bounds of the correspond-
ing ﬁxed point set and derive its convergence guaran-
tees. Finally, we apply our results to the wind-assisted
navigation of high altitude platform systems relevant to
space exploration [25] and show that our algorithms can
be used to derive policies with better guarantees.

Related research. MDP with parameter uncertainty is
well studied in robust control and reinforcement learn-
ing. In control theory, the worst-case cost-to-go with re-
spect to state-decoupled parameter uncertainties is de-
rived via a minmax variation of the Bellman operator
in [5, 8, 15, 24]. The cost-to-go under parameter uncer-
tainty with coupling between states and time steps is
similarly bounded in [6, 12]. The eﬀect of statistical un-
certainty on the optimal cost-to-go is studied in [12, 15,
24, 26]. Recently, MDP with parameter uncertainty has
gained traction in the reinforcement learning community
due to the presence of uncertainty in real world problems
such as traﬃc signal control and multi-agent coordina-
tion [9, 10, 16]. Most RL research extends the minmax
worst-case analysis to methods such as Q-learning and
SARSA. Recently, methods for value-based RL using

non-contracting operators have been investigated in [3].

As opposed to the worst-case approach to analyzing
MDPs under parameter uncertainty, we do not assume
adversarial MDP parameter selection. Instead, we de-
rive a set of cost-to-gos that is invariant with respect
to the compact parameter uncertainty sets for order-
preserving, α-contracting operators, a class that the
Bellman operator belongs to. We continue from our
previous work [11], in which we analyzed the set-based
Bellman operator for cost uncertainty only.

Notation: A set of N elements is given by rN s “
t0, . . . , N ´ 1u. We denote the set of matrices of i rows
and j columns with real (non-negative) entries as Riˆj
(Riˆj
` ), respectively. Matrices and some integers are
denoted by capital letters, X, while sets are denoted
by cursive typeset X . The set of all compact subsets of
Rd is denoted by KpRdq. The column vector of ones of
size N P N is denoted by 1N “ r1, . . . , 1sT P RN ˆ1. The
identity matrix of size S is denoted by IS. The simplex
of dimension S is denoted by

∆S “ tp P RS | 1J

S p “ 1, p ě 0u.

(1)

A vector h P RS has equivalent notation ph1, . . . , hsq,
where hs is the value of h in the sth coordinate, s P rSs.
Throughout the paper, (cid:107)¨(cid:107) denotes the inﬁnity norm in
RS.

2 Discounted inﬁnite-horizon MDP

A discounted inﬁnite-horizon ﬁnite state MDP is given
by prSs, rAs, P, C, γq, where γ P p0, 1q is the discount
factor, rSs “ t1, . . . , Su is the ﬁnite set of states and
rAs “ t1, . . . , Au is the ﬁnite set of actions. Without
loss of generality, assume that each action is admissible
from each state s P rSs.

MDP Costs. C P RSˆA is the matrix encoding the
MDP cost. Each Csa P R is the cost of taking action
a P rAs from state s P rSs. We also denote the cost of
all actions at state s by cs “ rCs1, . . . , CsAs P RA, such
that C “ rc1, . . . , cSsJ.

MDP Transition Dynamics. The transition proba-
bilities when action a is taken from state s are given
by psa P ∆S. Collectively, all possible transition prob-
abilities from state s P rSs are given by the matrix
S Ă RSˆA, and all possible tran-
Ps “ rps1, . . . , psAs P ∆A
sition probabilities in the MDP are given by the matrix
S Ă RSˆSA.
P “ rP1, . . . , PSs P ∆SA

MDP Objective. We want to minimize the expected
cost-to-go, or the value vector V P RS, deﬁned per
state as

)
t“0 γtCstat | s0 “ s

8

, @ s P rSs,

(2)

! ř

Vs :“ Es

2

where Est¨u is the expected value of the input with re-
spect to initial state s, and (st, at) are the state and ac-
tion at time t. The value vector can be minimized by the
choice of actions at at every time step t:

V ‹
s :“ minatPrAs

Es

! ř

)
t“0 γtCstat | s0 “ s

8

, @ s P rSs,
(3)

Remark 1 Although value function is the standard term
for the expected cost-to-go, we use value vector in this
paper to emphasize that the cost-to-go values of ﬁnite
MDPs belong in a ﬁnite dimensional space.

MDP Policy. We optimize the objective (3) via a pol-
icy, denoted as π “ rπ1, . . . , πSs P ∆S
A, where the ath
element of πs P ∆A is the conditional probability of ac-
tion a being chosen from state s. Under policy πs, the
s πs P R and
expected immediate cost at s is given by cJ
the expected transition probabilities from s is given by
Psπs P ∆S.

2.1 Value operators

Solving an MDP is equivalent to ﬁnding the value vec-
tor and the associated policy that minimizes the objec-
tive (3). Typical solution methods utilize order preserv-
ing [19, Def.3.1], α-contractive operators whose ﬁxed
points are the optimal value vectors (e.g. Bellman oper-
ator [17, Thm.6.2.3], Q-value operator [13]).

Deﬁnition 1 (α-Contraction) Let pX , dq be a metric
space with metric d. The operator H : X ÞÑ X is an α-
contraction if and only if there exists α P r0, 1q such that

dpHpV q, HpV 1qq ď αdpV, V 1q,

@ V, V 1 P X .

(4)

Deﬁnition 2 (Order Preservation) Let pX , ďq be an
ordered space with partial order ď. The operator H :
X ÞÑ X is order preserving if for all V, V 1 P X such that
V ď V 1, HpV q ď HpV 1q.

These operators are typically locally Lipschitz in MDP
parameter space.

Deﬁnition 3 (KpV q-Lipschitz) Let pX , dX q be a met-
ric space with metric dX and pY, dY q be a metric space
with metric dY . The operator H : X ˆ Y ÞÑ X is KpV q-
Lipschitz with respect to M Ă Y if for all V P X , there
exists KpV q P R` such that

dX pHpV, mq, HpV, m1qq ď KpV qdY pm, m1q, @m, m1 P M.
(5)

Remark 2 The α-contraction property is a special in-
stance of Lipschitz continuity in which the input and out-
put spaces are identical and the Lipschitz constant is less
than 1.

3

Fig. 1. Illustration of the three value operator properties. (a)
α-contraction on RS, (b) Order preservation on RS, and (c)
KpV q-Lipschitz in input space M.

To capture operators with these properties, we deﬁne a
value operator that takes inputs: value vector, MDP
cost, and MDP transition probability. The MDP cost
and transition probability are selected from an MDP
parameter set M.

Deﬁnition 4 (Value operator) Consider the opera-
tor h,

h : RS ˆ M ÞÑ RS, M Ď RSˆA ˆ ∆SA
S .

(6)

We say h (6) is a value operator on RS ˆ M if

(1) For all m P M, hp¨, mq is an α-contraction in RS.
(2) For all m P M, hp¨, mq is order preserving in RS.
(3) For all V P RS, hpV, mq is KpV q-Lipschitz on M.

Remark 3 While we only consider value operators
whose input’s ﬁrst component is RS, Deﬁnition 4 and the
subsequent results can be extended to the space of Q-value
functions by swapping RS for RSA in Deﬁnition 4 [13].

An immediate consequence of the value operator h being
an α-contractive and order-preserving operator on RS is
that h is continuous on RS ˆ M.

Lemma 1 (Continuity) If h (6) is a value operator on
RS ˆ M, h is continuous on RS ˆ M.

pV, mq

P RS ˆ M and consider
PROOF. Let
a sequence tpVk, mkqukPN Ă RS ˆ M that con-
verges to pV, mq. It holds that (cid:107)hpVk, mkq ´ hpV, mq(cid:107)
ď (cid:107)hpVk, mkq ´ hpV, mkq(cid:107) ` (cid:107)hpV, mkq ´ hpV, mq(cid:107),
where from the α-contractive property of hp¨, mkq,
(cid:107)hpVk, mkq ´ hpV, mkq(cid:107) ď α (cid:107)Vk ´ V (cid:107). From the KpV q-
Lipschitz property of hpV, ¨q,

(cid:107)hpV, mkq ´ hpV, mq(cid:107) ď KpV q (cid:107)mk ´ m(cid:107) .

As both limkÑ8 (cid:107)Vk ´ V (cid:107) Ñ 0 and limkÑ8 (cid:107)mk ´ m(cid:107) Ñ
0, (cid:107)hpVk, mkq ´ hpV, mq(cid:107) Ñ 0 and h is continuous. l

We make the following assumption on the MDP param-
eter set M with respect to h.

Assumption 1 (Containment condition) The
MDP parameter set M satisﬁes the containment condi-
tion with respect to h if M is compact and for all V P RS,
č

č

argmin
mPM

sPrSs

hspV, mq ‰ H,

argmax
mPM

sPrSs

hspV, mq ‰ H.

(7)

Fig. 2. We illustrate argmaxmPM hspV, mq for a value opera-
tor h when S “ 2. Here, argmaxmPM h1pV, mq “ tm2, m3u,
argmaxmPM h2pV, mq “ tm1, m2u. Therefore, m2 is the
common parameter that achieves maxmPM hspV, mq for all
s P rSs.

Remark 4 Assumption 1 is an h-dependent condition
imposed on the structure of M, and is independent of
M’s convexity and connectivity.

2.2 Bellman and policy evaluation operators

Examples of value operators include the Bellman op-
erator and the policy evaluation operators when the
MDP cost and transition probability are input parame-
ters rather than ﬁxed parameters.

Deﬁnition 5 (Policy evaluation operator) Given
a policy π P Π, the vector-valued operator gπ “
ÞÑ RS is deﬁned per
pgπ
state as

Sq : RS ˆ RSˆA ˆ ∆SA

1 , . . . , gπ

S

gπ
s pV, C, P q :“ cJ

s πs ` γ

Psπs

´

¯

J

V, @s P rSs.

(8)

Given pC, P q, gπp¨, C, P q : RS ÞÑ RS is a vector-valued
operator whose ﬁxed point is the expected cost-to-go
of the MDP prSs, rAs, C, P, γq under π, denoted as
V πpC, P q [17, Thm.6.2.5].

V πpC, P q “ gπpV π, C, P q, V πpC, P q P RS.

(9)

When the context is clear, we denote V πpC, P q as V π.

Deﬁnition 6 (Bellman operator) The vector-valued
operator f “ pf1, . . . , fSq : RS ˆ RSˆA ˆ ∆SA
S ÞÑ RS is
deﬁned per each state as

fspV, C, P q :“ inf

πsP∆A

gπ
s pV, C, P q, @ s P rSs.

(10)

The corresponding optimal policy π‹ “ pπ‹
deﬁned per state as π‹

s P argminπs gπ

s q is
s pV, C, P q (10) and

1, . . . , π‹

4

s “ 1 @s P rSs. One such policy is deﬁned

S π‹
satisﬁes 1J
for all ps, aq P rSs ˆ rAs by
$
&

π‹
s,a :“

ą 0 a P argmin
aPrAs
otherwise.

%
0

Csa ` γpJ

saV,

(11)

where argminaPrAsphq is the set of minimizing actions for
the function h. An optimal policy in the form (11) always
exists for a discounted inﬁnite horizon MDP [17, Thm
6.2.10]. Given parameters pC, P q, f p¨, C, P q : RS ÞÑ
RS is a vector operator whose ﬁxed point is the opti-
mal cost-to-go for the MDP prSs, rAs, P, C, γq, denoted
as V BpC, P q.

V BpC, P q “ f pV B, C, P q, V BpC, P q P RS.

(12)

When the context is clear, we denote V BpC, P q as V B.

We show that both (8) and (10) are value operators.

Lemma 2 The Bellman operator (10) and the policy
evaluation operators (8) for all π P Π are value operators
on RS ˆ M where M Ď RSˆA ˆ ∆SA

(6).

S

PROOF. We show that both the Bellman operator f
and the policy evaluation operator gπ satisfy the contrac-
tive, order preserving and Lipschitz properties given in
Deﬁnition 4. Contraction: given pC, P q P M, gπp¨, C, P q
and f p¨, C, P q are both γ-contractions [17, Prop.6.2.4]
on the complete metric space pRS, (cid:107)¨(cid:107)8q, where γ ă 1 is
the discount factor.

Order preservation: given pC, P q P M, the operator
gπp¨, C, P q is order preserving [17, Lem.6.1.2]. Con-
sider U, V P RS where U ď V . If gπp¨, C, P q is order-
preserving, gπpU, C, P q ď gπpV, C, P q for all π P Π.
Taking the inﬁmum over Π, we have f pU, C, P q “
inf πPΠ gπpU, C, P q ď inf πPΠ gπpV, C, P q “ f pV, C, P q.

KpV q-Lipschitz: given pC, P q, pC 1, P 1q P M and V P RS,
let ˆπ (11) be the optimal policy for f pV, C 1, P 1q and π‹
be the optimal policy for f pV, C, P q. For s P rSs, suppose
fspV, C 1, P 1q ě fspV, C, P q, then 0 ď fspV, C 1, P 1q ´
fspV, C, P q ď pc1
s qJV .
s π‹
Since π‹ is sub-optimal for f pV, C 1, P 1q, we can upper
bound |fspV, C 1, P 1q ´ fspV, C, P q| ď pc1
s `
γrpP 1

‹sJV . We conclude that

s ˆπsqJV ´γpPsπ‹

s ´ csqJπ‹

sqJ ˆπs´cJ

s `γpP 1

s ´ Psqπs

|fspV, C 1, P 1q ´ fspV, C, P q|
(cid:13)
(cid:13)8`γ (cid:13)
ď (cid:13)
(cid:13)
(cid:13)c1
(cid:13)8

s ´ Ps

s ´ cs

(cid:13)P 1

maxt(cid:107)π‹

s (cid:107)8 , (cid:107)ˆπs(cid:107)8u (cid:107)V (cid:107)8 .

(13)

Since π‹
arguments,

s , ˆπs P ∆A, (cid:107)π‹

s (cid:107)8 ď 1 and (cid:107)ˆπs(cid:107)8 ď 1. By similar
(13) is true if fspV, C 1, P 1q ď fspV, C, P q.

We can upper bound f pV, mq ´ f pV, m1q “ f ´ f 1 as

(cid:13)f ´ f 1(cid:13)
(cid:13)

(cid:13)8 ď max
sPrSs

t(cid:13)
(cid:13)c1

s ´ cs

(cid:13)8 ` γ (cid:13)
(cid:13)

(cid:13)pPs ´ P 1

sqJV (cid:13)

ď maxp1, γ (cid:107)V (cid:107)8q (cid:13)

(cid:13)m ´ m1(cid:13)
(cid:13)8

.

(cid:13)8u
(14)
(15)

The policy evaluation operator gπ satisﬁes (13) if
maxt(cid:107)π‹
replaced by (cid:107)πs(cid:107)8. Since
(cid:107)πs(cid:107)8 ď 1, gπ is KpV q-Lipschitz. l

s (cid:107)8 , (cid:107)ˆπs(cid:107)8u

is

Remark 5 Beyond the policy evaluation operator and
the Bellman operator, many algorithms in reinforcement
learning can be reformulated using value operators. For
example, it’s not diﬃcult to show that the Q-learning
operator [13] is a value operator on the vector space RSA.

2.3 Containment-satisfying MDP parameter sets

Assumption 1 restricts the structure of M with respect
to the value operator h. Thus whether or not M satisﬁes
Assumption 1 must always be determined with respect
to the operator h. With respect to the Bellman opera-
tor f (10) and the policy evaluation operators gπ (8),
the following conditions in robust MDP are suﬃcient to
satisfy Assumption 1.

Deﬁnition 7 (ps, aq-rectangular sets [8, 15]) The
uncertainty set M Ă RSˆA ˆ∆SA

S is ps, aq-rectangular if

ą

M “

ps,aqPrSsˆrAs

Msa, Msa Ă Rˆ∆S, @ps, aq P rSsˆrAs.

(16)

Intuitively, ps, aq-rectangularity implies that the MDP
parameter uncertainty is decoupled between each state-
action. A more general condition is if the parameter un-
certainty is decoupled between diﬀerent states but not
between diﬀerent actions within the same state.

Deﬁnition 8 (s-rectangular sets) The
set M Ă RSˆA ˆ ∆SA
is s-rectangular if
ą

S

uncertainty

Ms, Ms Ă RA ˆ ∆A

S , @s P rSs.

(17)

M “

sPrSs

Fig. 3. MDP with parameter coupling in transition proba-
bility across diﬀerent states.

the transition probabilities of trend i P rN s are given by
P i
s. If the wind pattern strictly switches between the dis-
crete wind trends, then the transition uncertainty at state
s P rSs is Ps “ tP 1
s u. If the wind pattern is a
mixture of the discrete wind trends, the transition uncer-
s | α P ∆N u. Both
tainty at state s P rSs is Ps “ t
wind patterns lead to s-rectangular uncertainty, given by
P “

s , . . . , P N

i αiP i

Ś

ř

sPrSs Ps.

We show that the rectangularity conditions indeed are
suﬃcient for satisfying Assumption 1 with respect to
f (10) and gπ (8).

Proposition 1 If M is compact and s-rectangular (Def-
inition 8), M satisﬁes Assumption 1 with respect to
f (10) and gπ (8) for all π P Π.

PROOF. We ﬁrst show that M satisﬁes Assumption 1
with respect to the Bellman operator. Given s P rSs,
fspV, C, P q only depends on the s component of C and P .
s, P ‹
From Lemma 1, fs is continuous in pcs, Psq. Let pc‹
s q
be the solution to argminpcs,PsqPMs fspV, C, P q for all
@ s P rSs. If Ms is compact, pc‹
s q P Ms. We can
construct C ‹ “ rc‹
1 , . . . , P ‹
Ss. If
M is s-rectangular, then pC ‹, P ‹q P M and pC ‹, P ‹q P
argminmPM fspV, C, P q for all s P rSs. We conclude that
M satisﬁes Assumption 1.

s, P ‹
Ss and P ‹ “ rP ‹

1, . . . , c‹

Given π P Π and s P rSs, gπ
s only depends on cs and Ps as
well. We can similarly show that there exists an optimal
parameter pC ‹, P ‹q P argminpC,P qPM gπ
s pV, C, P q for all
s P rSs such that pC ‹, P ‹q P M. l

s-rectangularity generalizes ps, aq-rectangularity—i.e.
ps, aq-rectangularity implies s-rectangularity.

Example 1 (Wind uncertainty) Consider an MDP
in which the states correspond to geographical coordi-
nates, the actions correspond to navigation choices (up,
down, left, right), and the transition probabilities cor-
respond to the local wind patterns that vary between N
major wind trends over time per state. At state s P rSs,

Beyond s-rectangularity, there are sets that satisfy As-
sumption 1 with respect to speciﬁc value operators.

Example 2 (Beyond rectangularity) In Figure 3,
we visualize a four state MDP with transition uncer-
tainty M parameterized by α. MDP states are the nodes
and MDP actions are the arrows. Actions that transi-
tion to multiple states are visualized by multi-headed
arrows. Each head has an associated tuple pcsa, psa,s1q
denoting its state-action cost and transition probability.

5

All states have a single action except for state s4, where
two actions exist and are distinguished by diﬀerent col-
ors. Both s2 and s3 are absorbing states with a unique
action, such that V2 “ 1
1´γ and V3 “ 0 for both f and
gπ for all π P Π, where γ is the discount factor.

The states s1 and s4 have transition uncertainty
parametrized by α P r0, 1s. Therefore, M violates s-
rectangularity (Deﬁnition 8). The optimal cost-to-go
values V1 and V4 occur at diﬀerent α’s. Therefore, M
violates Assumption 1 with respect to f . However, sup-
pose that at s4, we only consider policies that exclusively
choose the action colored green in Fig. 3. Then the ex-
pected cost-to-go at s4, V4, is independent of α. The
minimum and maximum values of V1 under π occur
at α “ 1 and α “ 0, respectively. Therefore, M sat-
isﬁes Assumption 1 with respect to operator gπ for all
π “ rπs1, . . . , πs4 s where πs4 “ r1, 0s.

3 Set-based value operators

Motivated by the uncertain MDP parameters encoun-
tered in robust MDP, stochastic games, and reinforce-
ment learning in uncertain environments, we now con-
sider value operators with respect to a compact set of
uncertain MDP parameters. To understand the eﬀect of
both stationary and dynamic parameter uncertainty on
the value vector, we extend value operators to set-based
value operators, and prove the existence of ﬁxed point
sets on the space of compact subsets of RS.

To facilitate our set-based analysis, we ﬁrst introduce
Hausdorﬀ-type set distances.

Deﬁnition 9 (Point-to-set Distance) The distance
between a value vector and a set V Ď RS is given by

W ÞÑ dpW, Vq :“ inf
V PV

(cid:107)W ´ V (cid:107) .

(18)

On the space of compact subsets of RS, given by KpRSq,
the distance between value vector sets extends (18) and
is given by the Hausdorﬀ distance [7].

Deﬁnition 10 (Set-to-set Distance) The Hausdorﬀ
distance between two value vector sets V, W Ď RS is
given by

"

*

dKpV, Wq :“ max

sup
V PV

dpV, Wq, sup
W PW

dpW, Vq

. (19)

We use pKpRSq, dKq to denote the metric space formed
by the set of all compact subsets of RS under the Haus-
dorﬀ distance dK. The induced Hausdorﬀ space is com-
plete if and only if the original metric space is com-
plete [7, Thm 3.3]. Therefore, pKpRSq, dKq is a complete
metric space.

6

Fig. 4. Illustration of the set-based operator HpVq applied
to the singleton set V “ tV u Ă RS, we compute hpV, mq
for every parameter m P M and collect the output hpV, mq,
such that HpVq “ YmPMhpV, mq.

For a value operator h (6), we ask the following question:
what is the set of possible value vectors when the MDP
has parameter uncertainty given by M? To resolve this,
we deﬁne the set-based value operator H.

Deﬁnition 11 (Set-based Value Operator) The
set-valued operator H is induced by h on RS ˆ M (6)
and is deﬁned as

HpVq :“ thpV, mq | pV, mq P V ˆ Mu Ď RS,

(20)

where V Ď RS is a subset of the value vector space.

We denote the set-based value operator induced by
the Bellman operator (10) and policy evaluation oper-
ators (8) as F and Gπ, respectively, such that for any
value vector set V Ď RS,

F pVq :“ tf pV, C, P q | pV, C, P q P V ˆ Mu ,

(21)

GπpVq :“ tgπpV, C, P q | pV, C, P q P V ˆ Mu , @ π P Π.
(22)
The set-based Bellman operator F is the union over all
the optimal value vectors, where the optimal policy that
corresponds to each f pV, C, P q P F pVq varies based on
pC, P q. On the other hand, Gπ is the union over all value
vectors that results from a constant π and all possible
pC, P q P M parameters.

We can ask the following question: is there a set of value
vectors that is invariant with respect to H? Similar to the
value operators h from Deﬁnition 4, we can aﬃrmatively
answer this question by showing that H is α-contractive
on KpRSq.

Theorem 1 If h is a value operator on RS ˆ M (6) and
M is compact, then the induced set value operator H (20)
satisﬁes

(1) For all V P KpRSq, H pVq P KpRSq;
(2) H is an α-contractive on pKpRSq, dKq (19) with a

unique ﬁxed point set V ‹ given by

HpV ‹q “ V ‹, V ‹ P KpRSq;

(23)

(3) The sequence tV kukPN where V k`1 “ HpV kq con-

verges to V ‹ for any V 0 P KpRSq.

4 Properties of the ﬁxed point set

In particular, these hold for F (21) and Gπ (22), whose
ﬁxed point sets are denoted as V B and V π, respectively.

F pV Bq “ V B P KpRSq, GπpV πq “ V π P KpRSq, @π P Π.
(24)

PROOF. The ﬁrst statement follows from Lemma 1,
since the image of a compact set by a continuous function
is compact [18]. Let us prove the second statement: for
some β P p0, 1q, for all, V, V 1 P KpRSq:

dKpHpVq, HpV 1qq

$
’&

’%

“ max

sup
V PV
mPM
ďβdKpV, V 1q

`

˘
hpV, mq, HpV 1q

d

dphpV 1, m1q, HpVqq

, sup
V 1PV 1
m1PM

Take pV, mq P V ˆ M, then dphpV, mq, HpV 1qq ď
inf V 1PV 1 (cid:107)hpV, mq ´ hpV 1, mq(cid:107) ď α inf V 1PV 1 (cid:107)V ´ V 1(cid:107)
holds from the α-contractive property of h. Finally,

dphpV, mq, HpV 1qq ďα sup
V PV

inf
V 1PV 1

(cid:13)V ´ V 1(cid:13)
(cid:13)
(cid:13)8

sup
V PV
mPM

ďαdKpV, V 1q

We use the same technique to prove that

dphpV 1, m1q, HpVqq ď αdKpV, V 1q.

sup
V 1PV 1
m1PM

For the MDP parameters pC, P q, the ﬁxed point of
hp¨, C, P q is typically meaningful for the corresponding
MDP. For example, the ﬁxed point of a policy evalua-
tion operator gπp¨, C, P q (8) is the expected cost-to-go
under policy π, and the ﬁxed point of the Bellman op-
erator f p¨, C, P q (10) is the minimum cost-to-go when π
can be freely chosen. In this section, we derive proper-
ties of the ﬁxed point set V of H (20) in the context of
non-stationary value iteration.

4.1 Non-stationary value iteration

Given a value operator h on RS ˆ M, we consider value
iteration under a dynamic parameter uncertainty model
discussed in [15], where at every iteration, a new set of
MDP parameters mk is chosen from M as

V k`1 “ hpV k, mkq, V 0 P RS, mk P M, @k P N.

(26)

,
/.

/-

In robust MDP literature [8, 15], mk is modiﬁed by an
adversarial opponent of the MDP decision maker such
that (26) converges to a worst-case value vector. We con-
sider a more general scenario in which mk is chosen from
the closed and bounded set M without any probabilis-
tic prior. In this scenario, convergence of V k in RS will
not occur for all possible sequences of tmkukPN. How-
ever, we can show convergence results on the set domain
by leveraging our ﬁxed point analysis of the set-based
operator H (20).

Proposition 2 Let V ‹ be the ﬁxed point set of the set-
based value operator H (20) induced by h on RS ˆM (6).
If
the non-stationary value iteration (26) satisﬁes
tmkukPN Ă M, then the sequence tV kukPN deﬁned
by (26) satisﬁes

Finally, dKpHpVq, HpV 1qq ď αdKpV, V 1q. From the
Banach ﬁxed point theorem and the completeness of
pKpRSq, dKq [7, Thm 3.3], H has a unique ﬁxed point
H ‹ in KpRSq.

(1) limkÑ`8 dpV k, V ‹q “ 0,
(2) there exists a sub-sequence tV ϕpkqukPN that con-
verges to a point in V ‹ as limkÑ8 V ϕpkq P V ‹.

The third point is a consequence of the Banach ﬁxed
point theorem. Finally, f and gπ are value operators (6)
on RS ˆ M, therefore this theorem’s statements ap-
ply. l

Remark 6 (Set-based value iteration) An impor-
tant consequence of Theorem 1 is the existence of the
set-based value iteration, given by

V k`1 “ HpV kq, V 0 P KpRSq.

(25)

Analogous to standard value iteration, (25) is a sequence
of value vector sets in KpRSq that converges to the ﬁxed
point set V ‹ P KpRSq.

PROOF. Let tV kukPN be a sequence deﬁned by V 0 “
tV 0u and V k`1 “ HpV kq, where H (20) is the set op-
erator induced by h on RS ˆ M. We ﬁrst show state-
ment 1). From Theorem 1, limkÑ8 V k converges to V ‹
in dK. Therefore, 0 ď dpV k, V ‹q “ inf yPV ‹
(cid:13)8 ď
supxPV k inf yPV ‹ (cid:107)x ´ y(cid:107)8 ď dH pV k, V ‹q Ñ 0 as k tends
to `8.

(cid:13)
(cid:13)V k ´ y(cid:13)

Next, for all k P N, there exists N P N such that for all
n ě N , dpV n, V ‹q ď pk ` 1q´1. We deﬁne the strictly
increasing function ψ1 : N Ñ N, such that ψ1p0q “
0 and for all k ‰ 0, ψ1pkq :“ mintN ą ψ1pk ´ 1q :
@n ě N, dpV n, V ‹q ă pk ` 1q´1u. Then, for all k P N‹,
there exists yψ1pkq P V ‹ such that (cid:13)
(cid:13) ă

(cid:13)V ψ1pkq ´ yψ1pkq(cid:13)

7

pk ` 1q´1. As V ‹ is compact, there exists ψ2 : N Ñ N
strictly increasing such that pyψ1pψ2pkqqqk converges to
some y‹ P V ‹ [18, Thm 3.6]. Finally, let ε ą 0, there
exist K1, K2 P N such that for all l ě K1, pψ2plqq´1 ă
(cid:13)
(cid:13)
(cid:13)yψ1pψ2pl1qq ´ y‹
(cid:13)
(cid:13)
ε{2 and for all l1 ě K2,
(cid:13) ă ε{2. So,
taking k ě maxtK1, K2u, we have (cid:13)
(cid:13)V ψ1pψ2pkqq ´ y‹(cid:13)
(cid:13) ď
(cid:13)V ψ1pψ2pkqq ´ yψ1pψ2pkqq(cid:13)
(cid:13)
(cid:13) ď ε and
pV ψ1pψ2pkqqqk converges to y‹ P V ‹.

(cid:13)yψ1pψ2pkqq ´ y‹(cid:13)
(cid:13)

(cid:13) `

l

In addition to containing all asymptotic behavior of
value vector trajectories under time-varying value iter-
ation, the ﬁxed point set V also contains all ﬁxed points
of the value operator hp¨, C, P q when pC, P q P M (6)
are ﬁxed.

Corollary 1 Let h (6) be a value operator on RS ˆ M
where M is compact. For all m P M, if V “ hpV, mq P
RS and V ‹ is the ﬁxed point set of the induced set-based
value operator H (20), V P V ‹.

PROOF. We construct sequence tV ku where V k`1 “
hpV k, mq and V 0 “ V . Then V k “ V for all k P N. From
the second point of Proposition 2, V P V ‹ follows. l

Going further, we can bound the transient behavior
of (26) when V 0 is an element of the ﬁxed point set V ‹.

Corollary 2 (Transient behavior) Let V ‹
be the
ﬁxed point of the set-based value operator H (20) induced
by h on RS ˆ M. If M is compact and V 0 P V ‹, then
the sequence generated by (26) satisﬁes tV kukPN Ď V ‹.

PROOF. As a ﬁxed point set of H (20), V ‹ (23) satisﬁes
V ‹ “ HpV ‹q, then the following is true by deﬁnition of
H: if V k P V ‹, then V k`1 “ hpV k, mkq P V ‹. If V 0 P V ‹,
then tV kukPN Ď V ‹ follows by induction. l

Remark 7 Proposition 2 and Corollary 2 bound the
asymptotic and transient behavior of
the sequence
thpV k, mkqukPN generated from (26), regardless of the
convergence of the value vector sequence. This is a more
general result then the classic convergence results for
MDPs and robust MDPs.

4.2 Bounds of the ﬁxed point set

In Theorem 1, the compactness of M implied the com-
pactness of V ‹. This relationship carries over to the
supremum and inﬁmum elements of M and V ‹—i.e., if
M satisﬁes Assumption 1 with respect to h, then V ‹
contains its own supremum and inﬁmum elements.

Greatest and least elements. We deﬁne the supre-
mum and inﬁmum elements of a value vector set V P
KpRSq element-wise as follows,

V s :“ sup
V PV

Vs, V s :“ inf
V PV

Vs, @ s P rSs.

(27)

Fig. 5. The greatest least bounds of three diﬀerent value
function sets V i P R2, where p0, 0q the origin is located on
the lower left. Note that V 2 and V 3 contains their own great-
est and least elements, but V 1 does not. In V 1, the coordi-
nate-wise greatest and least elements are achieved by some
element in V 1 but not at the same time.

If a set V Ď RS is compact, the projection of V on each
state s is compact. Then, the coordinate-wise supremum
and inﬁmum values for each state s are achieved by V.
However in general, no single element of the set V may
simultaneously achieve the minimum over all states—
i.e., V pV q may not be an element of V. This is illustrated
in Figure 5.

Given h and parameter uncertainty set M, we wish to 1)
bound the supremum and inﬁmum elements of the ﬁxed
point set V ‹ (23) and 2) derive suﬃcient conditions for
when they are elements of V ‹. To facilitate bounding V ‹,
we introduce the following bound operators.

Deﬁnition 12 (Bound Operators) The bound oper-
ators induced by the value operator h on RS ˆ M are
coordinate-wise deﬁned at each s P rSs as

hspV q “ inf
mPM

hspV, mq, hspV q “ sup
mPM

hspV, mq. (28)

Remark 8 Corollary 2 also implies that V ‹ is invariant
with respect to the non-stationary value iteration (26),
and may prove useful in the analysis and design of MDPs
with known parameter uncertainties.

We want to bound the ﬁxed point set V of the set-based
value operator H (20) by the bound operators h{h (28).
First we show that h{h are themselves α-contractive and
order preserving on RS.

8

Fig. 6. We visualize the bound operator for HpVq for a given
value operator h on RS ˆ M. The input set V is a singleton
tV u in R2. Here, because h1 and h2are reached for two dif-
ferent parameters m P M, the resulting hpV q lies outside of
the ﬁxed point set.

Lemma 3 (α-Contraction) If h (6) is a value operator
on RS ˆ M and M is compact, then h and h (28) are
α-contractions with ﬁxed points X, X, respectively.

hpXq “ X,

hpXq “ X, X, X P RS.

(29)

PROOF. From Lemma 1, h is continuous and M
is compact, then for all X, Y P RS, there exists
ˆmpsq P M such that hspY q “ hspY, ˆmpsqq and
hspXq ď hspX, ˆmpsqq. We upper-bound hspXq ´ hspY q
by hspX, ˆmpsqq ´ hspY, ˆmpsqq, and use the α-contraction
property of h to derive

hspXq ´ hspY q ď |hspX, ˆmpsqq ´ hspY, ˆmpsqq|
ď α (cid:107)X ´ Y (cid:107)8 .

Since X and Y are arbitrarily ordered, we con-
clude that (cid:107)hpXq ´ hpY q(cid:107)8 ď α (cid:107)X ´ Y (cid:107)8. The
for h follows a similar reasoning and takes
proof
ˆmpsq “ supmPM hspX, mq. The existence of XpXq fol-
lows from applying Banach’s ﬁxed point theorem. l

Lemma 4 (Order Preservation) The bound opera-
tors h and h (28) are order-preserving on RS (Deﬁni-
tion 2).

@ U, V P RS, U ď V ñ hpU q ď hpV q,

hpU q ď hpV q.

PROOF. The lemma statement follows directly from
the fact that order preservation is conserved through
composition with inf and sup. If hpU, mq ď hpV, mq, then
inf mPM hpU, mq ď inf mPM hpV, mq. A similar argument
follows for hp¨q “ supmPM hp¨, mq. l

Theorem 2 (Bounding ﬁxed point sets) If h (6) is
a value operator on RS ˆ M and M is compact,

X ď V ď X, @ V P V ‹,

(30)

where X and X (29) are the ﬁxed points of the bound
operators h and h (28), and V ‹ is the ﬁxed point set of
the set-based value operator H (20) induced by h (6) on
RS ˆ M.

PROOF. For V 0 “ tX, Xu and V k`1 “ HpV kq (25),
we ﬁrst show

X ď V ď X, @ V P V k,

(31)

via induction. Suppose that (31) is satisﬁed for V k.
implies
The order preserving property of hp¨, mq
that hpX, mq ď hpV, mq ď hpX, mq holds for all
pV, mq P V k ˆ M. We take the inﬁmum and supremum
over hpX, mq and hpX, mq, respectively, to show that
for all pV, mq P V k ˆ M and s P rSs,

inf
m1PM

hspX, m1q ď hspV, mq ď sup
m1PM

hspX, m1q.

Since X and X are the ﬁxed points of inf m1PM hsp¨, m1q
and supm1PM hsp¨, m1q for all s P rSs, respectively, we
conclude that (31) holds for V k`1.

Next, we show that X and X bounds the ﬁxed point set
V ‹ for the h-induced operator H (20). From Lemma 5, we
know that for all V P V ‹, there exists a strictly increasing
sequence φ : N ÞÑ N and corresponding value vectors
tW φpnqu such that limnÑ8 W φpnq “ V and W φpnq P
V φpnq for the sequence of value vector sets generated
from V 0 “ tX, Xu. Since X ď W φpnq ď X holds for all
n, we conclude (30) holds. l

When Assumption 1 is satisﬁed, the ﬁxed point of H (20)
contains its own supremum and inﬁmum.

Theorem 3 If h (6) on RS ˆM satisﬁes Assumption 1,
then there exists m, m P M such that h and h (28) and
their ﬁxed points X and X (29) satisﬁes

hpXq “ hpX, mq “ X, hpXq “ hpX, mq “ X.

(32)

Additionally, X and X are the least and the greatest ele-
ments of H’s ﬁxed point set V ‹, V ‹, V
(27) respectively,
and both belong to V ‹ (23).

‹

We show that the ﬁxed points X and X bounds the ﬁxed
point set V ‹ of the set-based value operator H (20).

X “ V ‹, X “ V

‹

, X, X P V ‹.

9

PROOF. From Theorem 2, X and X are the lower
and upper bounds on the ﬁxed point set V ‹. We
show that these are the inﬁmum and supremum
elements of V ‹ by showing that they are also el-
ements of V ‹. From Assumption 1,
there exists
m, m P M such that hspX, mq “ minmPM hspX, mq
and hspX, mq “ minmPM hspX, mq for all s P rSs. Since
X and X are ﬁxed points of hp¨, mq and hp¨, mq, we
apply Corollary 1 to conclude that X, X P V ‹. l

5 Revisiting robust MDP

We re-examine robust MDP with the set-theoretical
analysis in this section, and show that Assumption 1
generalizes the rectangularity assumption made in ro-
bust MDPs, thus enabling robust dynamic program-
ming techniques to be available to a wider class of MDP
problems and contraction operators.

PROOF. Recall the Bellman operator f (6). When
M ˆ ∆A is compact, the formulation of the ﬁxed point
of f (28) is equivalently given by

f pXq “ min

pC,P qPM

min
πsP∆A

gπ
s pX, C, P q, @s P rSs.

(37)

We note that (37) is identical to the formulation of
W o (33). Therefore, W o “ X is the ﬁxed point of
f . When M is compact, W o exists due to Lemma 3.
From (35), πo
s pW o, C, P q,
s is the optimal argument of gπ
a continuous function in πs, C, P minimized over com-
pact sets ∆A ˆ M for all s P rSs. Therefore πo
s exists.
Since πo “ pπo

Sq, the optimal πo P Π exists.

1, . . . , πo

For the robust scenario: when M is compact, the ﬁxed
point of f (28), X, exists from Lemma 3 and is given by

X s “ max

pC,P qPM

min
πsP∆A

gπ
s pX, C, P q, @s P rSs.

(38)

Recall the optimistic value vector W o P RS and ro-
bust value vectors W r P RS of a discounted MDP
prSs, rAs, C, P, γq from [8, 15] as the ﬁxed points of the
following operators.

The function gπ
s pX, C, P q is concave in pC, P q and con-
vex in π. If M is convex, then we apply the minimax
theorem [14] to switch the order of min and max in (38)
to derive

W o

s “ min
πsP∆A

min
pC,P qPM

s pW o, C, P q, @s P rSs
gπ

(33)

X s “ min
πsP∆A

max
pC,P qPM

gπ
s pX, C, P q, @s P rSs.

(39)

W r

s “ min
πsP∆A

max
pC,P qPM

gπpW r, C, P q, @s P rSs

(34)

The optimistic policy πo and robust policy πr are the
optimal policies corresponding to (33) and (34), respec-
tively.

πo
s P argmin
πsP∆A

min
pC,P qPM

s pW o, C, P q, @s P rSs
gπ

(35)

πr
s P argmin
πsP∆A

max
pC,P qPM

s pW r, C, P q, @s P rSs
gπ

(36)

For readability, we denote the policy evaluation operator
under πo as go and the policy evaluation operator under
πr as gr.

When M is ps, aq-rectangular (16), the set of policies sat-
isfying (35) and (36) are non-empty and includes deter-
ministic policies [8, Thm 3.1]. When M is s-rectangular
and convex, the set of policies satisfying (36) is non-
empty but may be mixed [24, Thm 4]. When M is con-
vex, we show that policies (35) and (36) exist.

Proposition 3 If the MDP parameter set M is compact
and convex, then

(1) W o (33) and W r (34) exist and satisfy f pW rq “
W r, f pW oq “ W o, where f and f (28) are the
bound operators of the Bellman operator (10).

(2) πo (35) and πr (36) exist.

Equation (39) is identical to (34), therefore W r “ X and
exists by Lemma 3. In (39), maxpC,P qPM gπ
s pX, C, P q is
piece-wise linear in πs and ∆A is compact for all s P
rSs, thus argminπsP∆A maxpC,P qPM gπ
s pX, C, P q is non-
Sq, πr exists. l
1, . . . , πr
empty. Finally since πr “ pπr

Remark 9 Since maxpC,P qPM gπ
linear in πs, the optimal πr
This is consistent with the results in [24].

s pX, C, P q is piecewise
s is mixed policy in general.

Proposition 3 generalizes the results from [24] to show
that (34) exists when M is compact and convex instead
of s-rectangular and convex. From Theorem 2, W o and
W r bound of the ﬁxed point sets of the πo and πr. They
become inﬁmum and supremum elements when M sat-
isﬁes Assumption 1 with respect to go and gr. We ex-
plicitly derive this result next. First, we introduce some
notations: let Go “ Gπo, the ﬁxed point of Go be V o,
Gr “ Gπr
, and the ﬁxed point of Gr be V r.

V o “ tgopV, C, P q | pC, P q P M, V P V ou,

(40)

V r “ tgrpV, C, P q | pC, P q P M, V P V ru.

(41)
Additionally, the supremum elements of V o and V r are
V
respectively and the inﬁmum elements are
V o and V r, respectively.

and V

o

r

V r

s “ min
V PV r

Vs, V

r
s “ max
V PVr

Vs, @s P rSs.

(42)

10

V o

s “ min
V PV o

Vs, V

o
s “ max
V PV o

Vs, @s P rSs.

(43)

We compare these with the ﬁxed point set of the Bellman
operator, V B “ tminπ gπpV, C, P q | pC, P q P M, V P
V Bu (23), denoted by V

and V B as

B

V B

s “ min
V PV B

rV ss, V

B
s “ max
V PV B

Vs, @s P rSs.

(44)

result proves
o

Our next
V B, V o, V r, V
, V
satisfy Assumption 1.

, V

B

r

the relationship between
when f, go, and gr on RS ˆ M

minπPΠ maxpC,P qPM gπpV r, C, P q. From Assumption 1,
there exists pC, P q P M that maximizes gπpV , C, P q, so
V

equivalently satisﬁes

r

r

V

“ min
πPΠ

r

gπpV

, C, P q.

r

r

B

ď V
B

. Next we show V

From Corollary 1, this implies that V
fore V
orem 3, V
maxpC,P qPM minπ gπpV
equality,

P V B and there-
. From The-
“
, C, P q, From the min-max in-

is the ﬁxed point of f , such that V

ď V

B

B

B

r

Theorem 4 If f, go, gr satisfy Assumption 1 on RS ˆ
M, then the bounding elements (43) (42) (44) of the
corresponding ﬁxed point sets V B,V o (40) and V r (41)
are ordered as

B

V

ď min
πPΠ

max
pC,P qPM

B

gπpV

, C, P q.

V B “ V o ď V r, V

B

“ V

r

ď V

o

.

(45)

Since πr P Π,

B

V

ď max

pC,P qPM

B

grpV

, C, P q.

(50)

PROOF. Since V o is the inﬁmum element for the ﬁxed
point set V o (43), we can apply Theorem 3 to derive

V o “ minpC,P qPM gopV o, C, P q.

(46)

By deﬁnition of πo (35), minpC,P qPM gopV o, C, P q “
minpC,P qPM minπPΠ gπpV o, C, P q. As the two minima
commute,

min
pC,P qPM

gopV o, C, P q “ min

pC,P qPM

gπpV o, C, P q.

min
πPΠ

(47)
Combining (46) and (47), V o is exactly the unique ﬁxed
point of minpC,P qPM minπPΠ gπp¨, C, P q. However, by ap-
plying Theorem 3 to f on RS ˆM, V B is also the unique
ﬁxed point of minpC,P qPM minπPΠ gπp¨, C, P q. Therefore
V o “ V B.

From (42), V r “ minpC,P qPM grpV r, C, P q, we can min-
imize over the policy space to lower bound V r as

V r ě min
πPΠ

min
pC,P qPM

grpV r, C, P q.

(48)

Since the right hand side of (48) is equivalent to
f pV rq, (48) is equivalent to V r ě f pV rq. From
Lemma 4, f is order-preserving in V , we conclude that
V o “ V ‹ ď V r.

From Theorem 3, V

r

is the ﬁxed point of gr, such that

r

V

“ max

pC,P qPM

r

grpV

, C, P q.

(49)

We apply minπ to both sides of (49) and use the def-
is the ﬁxed point of
inition of πr to derive that V

r

B

B

ď grpV

The right-hand side of (50) is grpV
q (28), such that (50)
B
q. We consider the sequence
is equivalent to V
V k`1 “ grpV kq where V 1 “ V
. Since gr is a con-
traction, limkÑ8 V k “ V r, the ﬁxed point of gr. From
Lemma 4, gr is order preserving. Therefore V
“ V 1 ď
V r.

B

B

o

Finally, Theorem 3 implies that V
of go: V
V
max inequality,

ě minπPΠ maxpC,P qPM gπpV

“ maxpC,P qPM gopV

o

o

is the ﬁxed point
, C, P q. By construction,
o
, C, P q. From the min-

o

min
πPΠ

max
pC,P qPM

o

gπpV

, C, P q ě max

pC,P qPM

min
πPΠ

o

gπpV

, C, P q,

o

such that the right hand side of the inequality is equiv-
q. Following the monotonicity properties
alent to f pV
of the Bellman operator f [17, Thm.6.2.2], we conclude
that V

. l

ě V

B

o

Remark 10 Through our ﬁxed-point analysis, we see
that in addition to having the best worst-case performance
among tV o, V B, V ru, V r also has the smallest variation
in performance for the same uncertainty set M.

Finally, we generalize the s-rectangularity condition by
showing that the optimistic and robust policies exist
when the MDP parameter set M satisﬁes Assumption 1.

Corollary 3 (Robust MDP under Assumption 1)
If M is compact and convex, and f, go, gr satisfy As-
sumption 1 on RS ˆ M, then W o (33) and W r (34)
are the inﬁmum and supremum value vectors for the

11

Fig. 7. Illustration of Theorem 4. The purple, green, blue
regions indicate the ranges of V r, V o, and V B, respectively.

policy evaluation operator under πo (35) and πr (36),
respectively.

W o

s “ inf
V PV o

rV ss, W r

s “ sup
V PV r

rV ss, @s P rSs,

(51)

where V o (40) and V r (41) are the ﬁxed point sets of poli-
cies πo and πr under parameter uncertainty M, respec-
tively.

PROOF. When f satisﬁes Assumption 1 on RS ˆ M,
Theorem 3 shows that V B “ W o, V
“ W r. If f, go,
and gr also satisﬁes Assumption 1 on RS ˆ M, then we
r
apply Theorem 4 to derive W o “ V o and W r “ V
.
This proves the corollary statement. l

B

Remark 11 When Assumption 1 is not satisﬁed, W o
and W r still bound V o and V
. This result is also stated
in [24].

r

6 Value iteration for ﬁxed point set computa-

tion

In the previous sections, we proved the existence of a
ﬁxed point set for value operators with compact param-
eter uncertainty sets and re-interpreted robust control
through our techniques. Next, we derive an iterative al-
gorithm for computing the bounds of the ﬁxed point set
V given a value operator h and parameter uncertainty
set M.

Algorithm Sketch. Based on the set-based value it-
eration (25), we iteratively ﬁnd the one-step bounds of
HpV kq to converge the bounds of the ﬁxed point set.

For any compact set V P KpRSq, the one step bounds of
HpVq are equivalent to the one-step output of the bound
operators h and h (28) applied to the extremal points of
V.

Theorem 5 (One step H bounds) Consider a set
operator H (20) and its bound operators h and h (28)
induced by h on RS ˆ M (6). For a compact set V Ă RS,
HpVq is bounded by hpV q and hpV q (28) as

hpV q ď V ď hpV q,

@ V P HpVq.

(52)

where V and V (27) are the extremal elements of V. If h
satisﬁes Assumption 1 on RS ˆ M and V , V P V, then
hpV q and hpV q are the supremum and inﬁmum elements
of HpVq, respectively— for all s P rSs, hspV q and hspV q
satisfy

hspV q “

inf
pV,mqPVˆM

hspV, mq, hspV q “

sup
pV,mqPVˆM

hspV, mq.

(53)

PROOF. For all s P rSs, hspV, mq ď hspV q for all m P
M. If h is KpV q-Lipschitz and α-contractions in M,
then h is order-preserving (Lemma 4) such that hspV q ď
hspV q for all V P V. We conclude that

hpV, mq ď hpV q, @pV, mq P V ˆ M.

(54)

Since h is an upper bound, and sup is the least upper
bound, it holds that supV,mrhpV, mqss ď hpV q. We use
the deﬁnition of HpVq (20) to conclude that V ď hpV q
for all V P HpVq. The inequality hpV q ď V @V P HpVq
can be similarly proved.

If h satisﬁes Assumption 1 on RS ˆ M and V , V P V,
Assumption 1 states that there exists m P M such that
hpV , mq “ hpV q. Therefore, hpV q P HpVq. Since hpV q
also lower bounds all the elements of HpVq, it is the
inﬁmum element of HpVq. The fact that the greatest
element of HpVq is hpV q can be similarly proved. l

Based on Theorem 5, we propose the following bound
approximation algorithm of the ﬁxed point set V ‹ (23)
for a set-valued operator H (6).

:“ V 0

Algorithm 1 Bound approximation of the ﬁxed point
set V
Input: C, P, V 0, (cid:15).
Output: V , V
0
1: V 0 :“ V
2: e0 “ 1´γ
γ (cid:15)
3: while γ
1´γ ek ě (cid:15) do
s “ minmPM hspV k, mq,
V k`1
4:
k`1
s “ maxmPM hspV
, mq,
V
!
(cid:13)V k`1 ´ V k(cid:13)
(cid:13)
ek`1 “ max
(cid:13) ,
k “ k ` 1

@s P rSs
@s P rSs
k`1
´ V

(cid:13)
(cid:13)
(cid:13)V

k(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)

5:

6:

)

k

7:
8: end while

6.1 Computing one-step optimal parameters

Algorithm 1 is stated for a general MDP parameter set
M and does not specify how to compute lines 4 and 5.
Here we discuss solution methods for diﬀerent shapes of
M.

12

(1) Finite M. If M “ tm1, . . . , mN u is a set with ﬁnite
number of elements, we can directly compute line 4
as

V k`1 “ min

)
!
hspV k, miq | i “ t1, . . . , N u

. (55)

For line 5, we replace min with max in (55).

(2) Convex M. When M is a convex set, the computa-
tion depends on h. If h “ gπ is the policy operator,
lines 4 and 5 can be solved as convex optimization
problems. If h is the Bellman operator f , lines 4
and 5 take on min-max formulation and is NP-hard
to solve in the general form [24]. When M can be
characterized by an ellipsoidal set of parameters,
the solutions to lines 4 and 5 is given in [24].

We recall the stochastic path planning problem from
Example 1 with the two diﬀerent parameter uncertainty
scenarios. When the wind ﬁeld uncertainty is discrete,
M is ﬁnite, when wind ﬁeld is a combination of the major
wind trends, M is convex.

k

k

k`1

´ V

´ V

bound (58) as

‹(cid:13)
(cid:13)
(cid:13) ď α

´ V
‹(cid:13)
(cid:13)
(cid:13) ď

(cid:13)
(cid:13)
We can then use
(cid:13)V
(cid:13)
(cid:13)
(cid:13)V
A similar argument can show that

(cid:13)
(cid:13)
(cid:13)V
(cid:13)
1
(cid:13)
(cid:13)V
1´α
(cid:13)
(cid:13)V k ´ V ‹(cid:13)
(cid:13)

‹(cid:13)
(cid:13)
(cid:13) to
k`1(cid:13)
(cid:13)
(cid:13).
(cid:13)
(cid:13) ď
(cid:13)
(cid:13). When Algorithm 1’s while condition
!
(cid:13)V k ´ V ‹(cid:13)
ď (cid:15). This

(cid:13)V k ´ V k`1(cid:13)

‹(cid:13)
(cid:13)
(cid:13) ,

(cid:13)
(cid:13)
(cid:13)V

1
1´α

´ V

´ V

is satisﬁed, max
concludes our proof. l

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

)

k

k

In particular, the Bellman operator f and policy opera-
tor gπ are γ-contractive on RS, where γ is the discount
factor, therefore Theorem 5 applies with α “ γ.

Remark 12 Theorem 6 implies that at the termina-
tion of Algorithm 1, the ﬁxed point set V ‹ can be over-
approximated by

V ‹ Ď Vapprox :“

ź

rV k`1

s ´ (cid:15), V

k`1
s ` (cid:15)s,

sPrSs

6.2 Algorithm Convergence Rate

where k is the last iterate before Algorithm 1 terminates.

When lines 4 and 5 are solvable, Algorithm 1 asymptot-
ically converges to approximations of the bounding ele-
ments of V ‹. If M satisﬁes Assumption 1 with respect
to h, Algorithm 1 derives the exact bounds of V. Al-
gorithm 1 has similar rates of convergence in Hausdorﬀ
distance as standard value iteration using h on RS.

Theorem 6 Consider the value operator h, compact un-
certainty set M, and the ﬁxed point set V ‹ of the set-based
operator H (20) induced by h on RS ˆ M. If M satisﬁes
Assumption 1 with respect to h, then at each iteration k,

(cid:13)
(cid:13)

(cid:13)V k`1 ´ V ‹(cid:13)

(cid:13)
(cid:13) ď α

(cid:13)
(cid:13)

(cid:13)V k ´ V ‹(cid:13)
(cid:13)
(cid:13) ,

(cid:13)
(cid:13)
(cid:13)V

k`1

´ V

(cid:13)
(cid:13)
(cid:13) ď α

(cid:13)
(cid:13)
(cid:13)V

k

´ V

(56)
where all norms are inﬁnity norms, and V ‹, V
are the
inﬁmum and supremum bounds of V, respectively. At Al-
gorithm 1’s termination, V k, V

satisﬁes

k

‹

maxt

(cid:13)
(cid:13)

(cid:13)V k ´ V ‹(cid:13)
(cid:13)
(cid:13) ,

k

(cid:13)
(cid:13)
(cid:13)V

´ V

‹(cid:13)
(cid:13)
(cid:13)u ă (cid:15).

(57)

PROOF. From Algorithm 1, V
Lemma 3, h is an α-contraction. We obtain

k`1

k

k`1

“ hpV
(cid:13)
(cid:13)
(cid:13)V

q. From
‹(cid:13)
(cid:13)
´ V
(cid:13) ď

k

(cid:13)
(cid:13)
(cid:13)V

α

´ V

‹(cid:13)
(cid:13)
(cid:13) and note that (56) holds by induction.
‹(cid:13)
(cid:13)
(cid:13) to

(cid:13)
(cid:13)
(cid:13)V

´ V

k

Next, we apply triangle inequality to
derive

k

(cid:13)
(cid:13)
(cid:13)V

´ V

‹(cid:13)
(cid:13)
(cid:13) ď

(cid:13)
(cid:13)
(cid:13)V

k

´ V

k`1(cid:13)
(cid:13)
(cid:13) `

(cid:13)
(cid:13)
(cid:13)V

k`1

´ V

‹(cid:13)
(cid:13)
(cid:13) .

(58)

7 Path Planning in Time-varying Wind Fields

We apply set-based value iteration to wind-assisted
probabilistic path planning of a balloon in strong, uncer-
tain wind ﬁelds [25]. MDP as a model for wind-assisted
path planning of balloons in the stratosphere and ex-
oplanets has recently gained traction [2, 25]. Discrete
state-action MDPs have been shown to be a viable high-
level path planning model [25] for such applications.

‹(cid:13)
(cid:13)
(cid:13) ,

Mission Objective. In the two dimensional wind-ﬁeld,
we assume that the wind-assisted balloon is tasked with
reaching target state p8, 8q in Figure 8 using minimum
fuel.

Uncertain Wind Fields. By collecting a set of wind
data on the environment’s wind ﬁeld, an MDP can be
created and a policy that handles stochastic planning
can be deployed. However, wind can be a time-varying
factor that causes the expected optimal policy to have
worse-than-expected worst-case performance. We built
an ideal uncertain wind ﬁeld to demonstrate how the set
Bellman operator can be used to predict the best and
worst-case behavior of a robust policy.

MDP Modeling Assumptions. Following the frame-
work described in [25], we model the path planning prob-
lem in an uncertain wind ﬁeld as an inﬁnite horizon,
discounted MDP with discrete state-actions in a two-
dimensional space. While balloons typically traverse in
three dimensions, we assume that the wind is consistent
in the vertical direction and that the ﬁnal target is any

13

vertical position along the given two-dimensional coor-
dinates. As a result, we can disregard the vertical posi-
tion during planning.

(a) Wind ﬁeld traversed
by the balloon,
discretized into 81
states.

(b) At each state, 9
actions corresponding
to diﬀerent thrust
vectors are available.

Fig. 8.

States. A total of 81 states represent
the two-
dimensional space, composed of three diﬀerent regions
characterized by their wind variability as shown in
Figure 8.

(1) Calm wind. In calm states Scalm, the wind magni-
tude varies uniformly between r0, 0.5s, and the wind
direction is uniformly sampled between r0, 2πs.
Scalm “ tpi, jq | p0, 0q ď pi, jq ď p2, 8q, p6, 0q ď
pi, jq ď p8, 8qu.

(2) Gusty wind. In states with gusts Sgusty, wind
magnitude is consistently 1, while the wind direc-
tion is uniformly sampled between r0, 2πs. Sgusty “
tpi, jq | p3, 3q ď pi, jq ă p6, 6qu.

(3) Unreliable wind. In unreliable states Sunreliable, a
predictable wind front occasionally moves across an
otherwise windless region. In other words, the wind
magnitude is either 0 or 1 and the wind direction
varies uniformly between rπ{4, π{2s.

Actions. The balloon is equipped with an actuator that
provides a constant thrust of 1 in 8 discretized direc-
tions shown in Figure 8b. The the only stationary ac-
tion vector with zero magnitude is highlighted in blue in
the center of Figure 8b. We assume that the actuation
force is enough to move the balloon across one state in
wind with magnitude ď 0.5, and is otherwise not strong
enough to overcome wind eﬀects.

Transition Probabilities. The transition probabilities
in Scalm and Sgusty are certain. At each state s, we con-
sider the following neighboring states.

(1) N psq: all 8 neighboring states of state s.
(2) N ps, a, 0q: the neighboring state of s in the direction

of a.

(3) N ps, a, 1q: the neighboring state of s in the direc-
tion of a plus the two adjacent states as shown in
Figure 9a.

(a) State
transition in
calm wind.

(b) State
transition in
unreliable wind.

(c) State
transition in
gusty wind.

Fig. 9. Transition probabilities for the three diﬀerent wind
regions.

(4) N ps, a, 2q: the up and upper-right neighbors of s,

as shown in Figure 9b.

In the calm wind region, the transition probabilities are
given by

#

Psa,s1 “

1
N ps,a,1q ,
0

s1 P N ps, a, 1q
otherwise

, @ s P rScalms.

(59)
In the gusty wind region, the transition probabilities are
given by

#

Psa,s1 “

1
N psq ,
0

s1 P N psq
otherwise

, @ s P rSgustys, @ a P rAs.

(60)
In the unreliable wind region, the transition probabilities
vary between transition dynamics P 1

s and P 2
s .

P 1

sa,s1 “

P 2

sa,s1 “

"

1,
0

"

0.5,
0

s1 P N ps, a, 0q
otherwise

, @ s P rSgustys, @ a P rAs.

(61)

s1 P N ps, a, 2q
otherwise

, @ s P rSgustys, @ a P rAs.

s and P 2

(62)
s collectively form the uncertainty

Collectively, P 1
set Ps Ă ∆A

S deﬁned at each state.

Ps “ tP i

sa | i P t1, 2u, a P rAsu, @s P rSunreliables. (63)

Cost. We deﬁne the following state-action cost to
achieve the mission objective: at each state-action, the
cost is the sum of the current distance from target posi-
tion starg “ p8, 8q, as well as the fuel expended by given
action.

b

Cppi, jq, aq “

pi ´ stargr0sq2 ` pj ´ stargr1sq2` 1

2 (cid:107)a(cid:107)2 .

We take a “ 1 for all actions except for the staying still
action, where a “ 0.

14

7.1 Bellman, optimistic policy, and robust policy

We ﬁrst compute the optimistic and robust bounds of
the MDP with parameter uncertainty in P when s P
rSunreliables by running Algorithm 1. The results are
shown in Figure 10.

policy deployment (66) achieve better upper-bound at
each MDP iteration. The dynamically changing policy
deployment (66) achieves less than 70 in cost-to-go on
average, which is the best among all three deployments.
As we discussed in Remark 10, the robust policy deploy-
ment has the smallest variance in value in the presence
of wind uncertainty, achieving a value diﬀerence of less
than 0.1.

Sampled solutions. We can compute a sampled MDP
model based on 50 samples of wind vectors for each state.
Based on these samples, we add the action vector and
compute the statistical distribution of state transitions.
We then compute the value of these stationary sampled
MDPs, and compare 9 randomly selected states’ values.
The resulting scatter plot is shown in Figure 12.

(a) Optimistic case with
expected objective of 54.2

(b) Robust case with
expected objective of 96.7.

8 Conclusion

Fig. 10.

We denote the optimistic policy as πo and the robust
policy as πr, and derive the bounds of their respective
value vector sets V o (40) and V r (41) using Algorithm 1.
The output is compared against the bounds of the set-
based Bellman operator’s ﬁxed point set V ‹ in Table 1.

Set Maximum value Minimum value

V ‹
V o
V r

70.61

101.58

70.63

62.25

62.25

70.52

Table 1
Bellman, optimistic policy, robust policy value bounds of the
uncertain wind ﬁeld.

Time-varying wind ﬁeld Next, we consider a time-
varying wind ﬁeld: at each time step k, the transition
probability P k is chosen at random from P (63). In this
time-varying wind ﬁeld, we compare three diﬀerent pol-
icy deployments: 1) stationary optimistic policy πo as
policy operator go (40), 2) stationary robust policy πr
as policy operator gr (41), and 3) dynamically changing
policy that is optimal for the MDP prSs, rAs, P k, C, γq
as f (10). These three diﬀerent policy deployments are
given by

V k`1 “ gopV k, C, P kq,
V k`1 “ grpV k, C, P kq,
V k`1 “ f pV k, C, P kq.

(64)

(65)

(66)

The resulting cost-to-go at state sorig “ r0, 0s is plot-
ted in Figure 11. Here, we see that the optimistic pol-
icy deployment (64) has the greatest variation in value
over the course of 50 MDP time steps. Both the robust
policy deployment (65) and the dynamically changing

In this paper, we categorized a class of operators uti-
lized to solve Markov decision processes as value oper-
ators and lifted their input space from vectors to com-
pact sets of vectors. We showed using ﬁxed point analy-
sis that the set extensions of value operators have ﬁxed
point sets which remain invariant given a compact set of
MDP parameter uncertainties. These sets were applied
to robust dynamic programming to further enrich exist-
ing results and generalize the k-rectangularity assump-
tion for robust MDPs. Finally, we applied our results to
a path planning problem for time-varying wind ﬁelds.
For future work, we plan on applying set-based value
operators to stochastic games in the presence of unco-
ordinated players such as humans, as well as applying
value operators to reinforcement learning to synthesize
robust learning algorithms.

References

[1] Wesam H Al-Sabban, Luis F Gonzalez, and Ryan N Smith.
Wind-energy based path planning for unmanned aerial
In 2013 IEEE
vehicles using markov decision processes.
International Conference on Robotics and Automation, pages
784–789. IEEE, 2013.

[2] Marc G Bellemare, Salvatore Candido, Pablo Samuel
Castro, Jun Gong, Marlos C Machado, Subhodeep Moitra,
Sameera S Ponda, and Ziyu Wang. Autonomous navigation of
stratospheric balloons using reinforcement learning. Nature,
588(7836):77–82, 2020.

[3] Marc G Bellemare, Georg Ostrovski, Arthur Guez, Philip
Thomas, and R´emi Munos. Increasing the action gap: New
operators for reinforcement learning. In Proceedings of the
AAAI Conference on Artiﬁcial Intelligence, volume 30, 2016.

[4] Prashant Doshi, Richard Goodwin, Rama Akkiraju, and
Kunal Verma. Dynamic workﬂow composition: Using markov
decision processes.
International Journal of Web Services
Research (IJWSR), 2(1):1–17, 2005.

[5] Robert Givan, Sonia Leach, and Thomas Dean. Bounded-
parameter markov decision processes. Artiﬁcial Intelligence,
122(1-2):71–109, 2000.

15

(a) Optimistic Policy with V o’s bounding values.

(b) Robust Policy with V r’s bounding values.

Fig. 12. Comparison of diﬀerent optimal value vectors under
the Bellman operator for 50 randomly sampled MDPs. On
the x-axis, the state number is computed as i ˆ 9 ` j.

[8] Garud N Iyengar.

Robust dynamic programming.

Mathematics of Operations Research, 30(2):257–280, 2005.

[9] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey
Levine. Conservative q-learning for oﬄine reinforcement
learning.
Information Processing
Systems, 33:1179–1191, 2020.

Advances in Neural

[10] Erwan Lecarpentier and Emmanuel Rachelson.

Non-
stationary markov decision processes, a worst-case approach
using model-based reinforcement learning. Advances in
neural information processing systems, 32, 2019.

[11] Sarah HQ Li, Assal´e Adj´e, Pierre-Lo¨ıc Garoche, and Beh¸cet
A¸cıkme¸se. Bounding ﬁxed points of set-based bellman
operator and nash equilibria of stochastic games. Automatica,
130:109685, 2021.

[12] Shie Mannor, Oﬁr Mebel, and Huan Xu. Robust mdps
with k-rectangular uncertainty. Mathematics of Operations
Research, 41(4):1484–1509, 2016.

[13] Francisco S Melo. Convergence of q-learning: A simple proof.
Institute Of Systems and Robotics, Tech. Rep, pages 1–4,
2001.

[14] J v Neumann.

Zur
Mathematische annalen, 100(1):295–320, 1928.

theorie der gesellschaftsspiele.

[15] Arnab Nilim and Laurent El Ghaoui. Robust control of
markov decision processes with uncertain transition matrices.
Operations Research, 53(5):780–798, 2005.

[16] Sindhu Padakandla, Prabuchandran KJ, and Shalabh
Bhatnagar. Reinforcement learning algorithm for non-
stationary environments. Applied Intelligence, 50(11):3590–
3606, 2020.

[17] Martin L Puterman. Markov Decision Processes.: Discrete
Stochastic Dynamic Programming. John Wiley & Sons, 2014.

[18] Walter Rudin et al. Principles of mathematical analysis,

volume 3. McGraw-hill New York, 1964.

[19] Bernd SW Schr¨oder. Ordered sets. Springer, 29:30, 2003.

[20] Csaba Szepesv´ari and Michael L Littman. A uniﬁed analysis
of value-function-based reinforcement-learning algorithms.
Neural computation, 11(8):2017–2060, 1999.

[21] Gerald Tesauro et al. Temporal diﬀerence learning and td-
gammon. Communications of the ACM, 38(3):58–68, 1995.

[22] Herke Van Hoof, Tucker Hermans, Gerhard Neumann, and
Jan Peters. Learning robot in-hand manipulation with tactile
features. In 2015 IEEE-RAS 15th International Conference
on Humanoid Robots (Humanoids), pages 121–127. IEEE,
2015.

[23] Christopher JCH Watkins and Peter Dayan. Q-learning.

Machine learning, 8(3):279–292, 1992.

(c) Dynamically changing policy with V B’s
bounding values.

Fig. 11. Comparison of robust policy, optimistic policy, and
Bellman policy’s value trajectories in time-varying wind
ﬁelds. Center blue line is the average over 50 trials. The
shaded blue region denotes the standard variation. The top
and bottom lines are the supremum and inﬁmum values of
the ﬁxed points.

[6] Vineet Goyal and Julien Grand-Clement. Robust markov
decision processes: Beyond rectangularity. Mathematics of
Operations Research, 2022.

[7] Jeﬀ Henrikson. Completeness and total boundedness of
In MIT Undergraduate Journal of

the hausdorﬀ metric.
Mathematics. Citeseer, 1999.

16

[24] Wolfram Wiesemann, Daniel Kuhn, and Ber¸c Rustem.
Mathematics of

Robust markov decision processes.
Operations Research, 38(1):153–183, 2013.

[25] Michael T Wolf, Lars Blackmore, Yoshiaki Kuwata, Nanaz
Fathpour, Alberto Elfes, and Claire Newman. Probabilistic
motion planning of balloons in strong, uncertain wind ﬁelds.
In 2010 IEEE International Conference on Robotics and
Automation, pages 1123–1129. IEEE, 2010.

[26] Insoon Yang.

A convex optimization approach to
distributionally robust markov decision processes with
wasserstein distance. IEEE control systems letters, 1(1):164–
169, 2017.

Lemma 5 Let tVnu Ď KpRSq be a converging sequence
for dK with Vn Ñ V as n Ñ 8. For all V P V, there
exists a converging subsequence tV ϕpnqunPN whose limit
is V for (cid:107)¨(cid:107).

PROOF. Let V P V. We can deﬁne the strictly in-
creasing function ϕ on N as follows: ϕp0q :“ 0 and
for all n P N, ϕpn ` 1q :“ mintj ą ϕpnq | D V j P
(cid:13)V ´ V j(cid:13)
V j, (cid:13)
(cid:13) “ dpV, V jq ď pn ` 1q´1u. Finally, as for
(cid:13)V ´ V ϕpnq(cid:13)
all n P N˚, (cid:13)
(cid:13) ď pϕpnq ` 1q´1, the result
holds. l

17

