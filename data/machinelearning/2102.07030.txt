1
2
0
2

n
u
J

1
2

]
L
M

.
t
a
t
s
[

3
v
0
3
0
7
0
.
2
0
1
2
:
v
i
X
r
a

Diﬀusion Approximations for a Class of Sequential Testing Problems

Victor Araman∗

Ren´e Caldentey†

Abstract

We consider a decision maker who must choose an action in order to maximize a reward function
that depends on the action that she selects as well as on an unknown parameter Θ. The decision
maker can delay taking the action in order to experiment and gather additional information on Θ. We
model the decision maker’s problem using a Bayesian sequential experimentation framework and use
dynamic programming and diﬀusion-asymptotic analysis to solve it. For that, we scale our problem
in a way that both the average number of experiments that is conducted per unit of time is large and
the informativeness of each individual experiment is low. Under such regime, we derive a diﬀusion
approximation for the sequential experimentation problem, which provides a number of important
insights about the nature of the problem and its solution. First, it reveals that the problems of
(i) selecting the optimal sequence of experiments to use and (ii) deciding the optimal time when to
stop experimenting decouple and can be solved independently. Second, it shows that an optimal
experimentation policy is one that chooses the experiment that maximizes the instantaneous volatility
of the belief process. Third, the diﬀusion approximation provides a more mathematically malleable
formulation that we can solve in closed form and suggests eﬃcient heuristics for the non-asympototic
regime. Our solution method also shows that the complexity of the problem grows only quadratically
with the cardinality of the set of actions from which the decision maker can choose.

We illustrate our methodology and results using a concrete application in the context of assortment
selection and new product introduction. Speciﬁcally, we study the problem of a seller who wants
to select an optimal assortment of products to launch into the marketplace and is uncertain about
consumers’ preferences. Motivated by emerging practices in e-commerce, we assume that the seller
is able to use a crowdvoting system to learn these preferences before a ﬁnal assortment decision is
made. In this context, we undertake an extensive numerical analysis to assess the value of learning and
demonstrate the eﬀectiveness and robustness of the heuristics derived from the diﬀusion approximation.

Keywords: Sequential experimentation, sequential testing, Bayesian demand learning, experiment de-
sign, optimal stopping, dynamic programming, crowdvoting

1 Introduction

This paper is concerned with the problem faced by a decision maker (or DM for short) who must choose
an action a from a ﬁnite set of available actions A in order to maximize a reward function
(a, Θ) that
depends on the action a taken, as well as on a parameter Θ. The DM does not know the true value of Θ
but has only incomplete information about it and hence about the reward function. Instead of selecting
an action immediately, the DM has the option of postponing this decision in order to experiment and
gather additional information about the true value of Θ. In this context, the decision maker needs to
select the most eﬀective sequence of experiments to implement through time as well as the time when to
stop these experiments and select a ﬁnal action a

A .

R

∗Olayan School of Business, American University of Beirut, Beirut, Lebanon.
†Booth School of Business, The University of Chicago.

∈

 
 
 
 
 
 
A wide range of applications can be modeled using the above general framework. For example, the DM can
be a factory manager who needs to decide if a batch of production meets speciﬁc quality standards. For
that she can sample items sequentially to measure their individual condition and accordingly, extrapolate
the quality assessment on the entire batch (Qiu, 2014). Alternatively, the DM can be a pharmaceutical
company conducting a sequence of clinical trials to evaluate the eﬃcacy of some new drug or vaccine
(Armitage et al., 2002). In yet another example, the decision maker can be an educational institution
designing computerized adaptive testing systems to assess the level of proﬁciency of a cohort of examinees
in a particular subject area (Bartroﬀ et al., 2008, Finkelman, 2008).

One particular application, which has served as our initial motivation for this paper, relates to the prob-
lem of assortment selection in the context of new product introduction. Launching new products into the
marketplace oﬀers great opportunities for companies to generate new revenue streams and increase sales.
However, such endeavors represent risky bets as consumers’ preferences are typically unknown and un-
successful products are a major liability generating possibly great capital expenditure, early markdowns,
serious goodwill cost, and loss of market share. It is not infrequent to witness major brands preferring to
discontinue a product, shortly after its introduction, rather than taking more risks and incurring higher
draining costs†. To mitigate these risks, companies seek to test the market’s reaction (e.g., value for the
price) to new products before launching decisions are made.

In general experimentation can be expensive and diﬃcult to conduct eﬀectively and probably worth doing
only seldomly. However, in many situations, this reality is now changing as companies are beginning to
recognize the potential to crowdsource such market testing activities. Online experimentation has been
indeed growing exponentially in the last decade or so. Companies such as Uber, Netﬂix, Amazon, Mi-
crosoft and many more‡ have been aggressively implementing market experimentation, through dedicated
platforms, with the objective of continuously improving the online experiences of their customers and
infer customers preferences. In the context of new product introduction, some companies have created
crowdvoting platforms (e.g., Threadless.com§) where customers can vote for their favorite products among
a menu of available options. By doing so, companies generate continuous feedbacks from the “crowd”
at almost no cost, except often for the lack of accuracy and veracity of the data gathered. In view of
these challenges, an eﬀective execution of a crowdvoting system is required which involves deciding what
is the best assortment of products to display to each individual voter in order to maximize the speed
of learning as well as when to stop the experimentation process and decide which new products should
be commercialized (Kohavi and Thomke (2017)). Section 6 is devoted to this particular crowdvoting
example, which we use to illustrate the methodology and results that we develop ﬁrst in Section 4 for the
general case.

Motivated by the operating conditions of many online experimentation platforms, our general formulation
of the decision maker’s problem and its analysis are based on two important and distinctive features:
(i) We assume that the time epochs at which experimentation is possible are driven by an exogenous

†Making the wrong selection has even driven many major brands to discontinue some of their products, shortly after

introduction (see, Sell Big or Die Fast, New York Times, J. Wortham and V.G. Kopytoﬀ, August 23, 2011).

‡We refer the reader to the spot light articles of the March-April 2020 issue of the Harvard Business Review.
§A site where anyone can design a T-shirt and submit it to a weekly contest. Viewers vote for their favorite T-shirts and

the winning designs are selected for production and their designers get rewarded.

2

point process that the DM does not control. (ii) We consider environments in which the average number
of experiments that can be conducted per unit of time is large but the amount of information generated
by each individual experiment is low. In the context of the crowdvoting example, the ﬁrst assumption
accounts for a stochastic arrival of viewers/voters to the platform website. As for the second feature, it
depicts, as mentioned above, the high velocity at which data can be collected online but also captures the
fact that such data is inherently more noisy and less reliable than when experiments are more targeted
and carefully designed (e.g., focus groups or surveying experts). Under these conditions, we are able
to use asymptotic analysis to derive a diﬀusion approximation for both the sequential experimentation
problem and the underlying optimal stopping problem that the decision maker must solve. As we will
see, the diﬀusion model provides a number of important insights about the nature of the problem and
its solution. First, it reveals that the problems of (i) selecting the optimal sequence of experiments and
(ii) deciding the optimal time to stop experimenting decouple and can be solved independently. Second,
it shows that an optimal experimentation policy is one that chooses the experiment that maximizes the
instantaneous volatility of the belief process, a proxy of the learning process. This maximum volatility
principle reduces dramatically the complexity of the dynamic experimentation selection problem and its
solution. Third, the diﬀusion approximation also provides a more mathematically malleable formulation
of the optimal stopping problem that we can solve in closed form.
Interestingly, the computational
complexity of the latter grows only quadratically with the cardinality of the set of actions A ; in fact
we show that solving a problem with
1)
problems each with only two actions. Fourth, by reinterpreting the maximum volatility principle, we
can reformulate the problem of selecting an optimal experimentation policy as a Tchebycheﬀ moment
problem that sheds some light on how one could tackle the problem of experiment design, i.e., which
experiments to make available in the ﬁrst place to the decision maker. In addition, we obtain from our
diﬀusion approximations, heuristics-policies for the moderate, non-asymptotic regime. These heuristics
turn out to be extremely eﬀective and robust as shown in our numerical analysis. Finally, as a by-product
of our analysis of the crowdvoting example in Section 6, we derive diﬀusion approximations for a setting
in which experimentation and learning are driven by the choices that voters make under a multinomial
choice model (MNL). Given the popularity of the MNL model to represent consumer preferences, we
believe that our approach to obtaining diﬀusion approximations can possibly be applied to a number of
other applications beyond those discussed in this paper.

actions is equivalent to solving a collection of

A
|

A
|

A
|

| −

(

|

|

2 Related Literature

Our paper is related to two streams of literature. Methodologically, we contribute to the literature on
hypothesis testing and sequential design of experiments initiated by Wald (1947) in the early 40’s. In
terms of applications, we contribute to the operations literature on assortment planning and demand
learning (e.g., Caro and Gallien, 2007 and K¨ok et al., 2009).

Sequential analysis is concerned with the problem of eﬀectively detecting the validity of a hypothesis
through sequential sampling or tests. After each (possibly costly) test and on the basis of the observed
history of outcomes, the decision maker needs to either accept one of the hypotheses being tested or

3

continue the experimentation. The sequential probability ratio test (SPRT) developed by Wald (1945) (see
also Wald and Wolfowitz, 1948) establishes that under certain conditions an optimal policy is determined
by the ﬁrst exit time of an appropriately deﬁned likelihood ratio process from a bounded interval; the end
points of this interval are determined by pre-speciﬁed type I and II error targets. The initial formulation
and ideas of Wald’s SPRT test have been applied to a wide range of applications and extended in
many diﬀerent directions (e.g., Siegmund, 1985 and Lai, 2001). One important extension relevant to our
work relates to the problem of sequential design of experiments, where the DM chooses dynamically the
experiments to undertake from a set of available options (e.g., Robbins, 1952, Chernoﬀ, 1959, 1972), and
do that until she decides to stop and selects what she believes is the true hypothesis. For brevity we
denote thereafter this type of problem, sequential hypothesis testing.

In terms of solution techniques large sample analysis has been commonly used to study sequential hy-
pothesis testing problems and evaluate the asymptotic optimality of concrete (often simple) policies. The
asymptotic regime in many of these studies is obtained by assuming that the cost of experimentation
goes to zero (e.g., Chernoﬀ, 1959, and Keener, 1984). Chernoﬀ mentions that “it may pay to continue
sampling even though we are almost convinced about which is the true state of nature.” The alluded “in-
eﬃciency” in Chernoﬀ’s regime is required to guarantee a probability of error that is proportional to the
cost of experimentation that is becoming increasingly small. Our work also relies on a type of asymptotic
analysis in which the number of experiments grow large, however, our approach diﬀers signiﬁcantly from
large sample methods as we not only scale the number of experiments but simultaneously decrease the
informativeness of each experiment. As a result, in such asymptotic regime the ‘rate’ of information that
the DM collects remains comparable to those in small sample problems, and therefore when the DM is
experimenting it does so only because she is still unsure of the true hypothesis. This interplay between
larger sample sizes and less informative experiments was also recently explored by Naghshvar and Javidi
(2013). They also rely on large sample analysis, but introduce a multiple hypothesis setting, and repre-
sent the limited informativeness by scaling the number of hypotheses. Their results are a generalization
of Chernoﬀ (1959) where they suggest adjusted policies and ﬁnd tight bounds to prove their asymptotic
optimality. In the context of multi-armed bandit problems, Wager and Xu (2021) and Fan and Glynn
(2021) are two recent arXiv preprints that study a similar type of asymptotic regime and diﬀusion limits
as the ones considered in this paper. In particular, they consider a regime in which the mean rewards of
the arms scale as 1/√n, where n is the number of arm pulls. Wager and Xu (2021) suggest a framework
governed by a well behaved sampling function to implement such approach in the context of sequential
experimentation. Fan and Glynn (2021) develop the theory from ﬁrst principles in the speciﬁc context
of Thompson sampling. In our two hypothesis setting, we introduce a general framework to model lack
of informativeness. This framework includes for instance the case of asymptotically indistinguishable
hypothesis as well as settings where the experiments generate increasingly noisy outcomes. We show
that under our asymptotic regime, the sequential experimentation problem reduces to a diﬀusion free
boundary problem which we are able to solve and develop approximations for the non-asymptotic regime.
Other papers have studied diﬀusion models in the context of sequential testing (e.g., Chernoﬀ (1961),
Breakwell and Chernoﬀ (1964), Peskir and Shiryaev (2006) or Harrison and Sunar (2015)), although in
our case we make no Gaussian assumption regarding the initial process that is being observed. Other
examples of sequential analysis papers that have relied on diﬀusion approximations include the work on

4

Bayesian multi-armed bandits by Chang and Lai (1987) and Brezzi and Lai (2002), on ranking and selec-
tion problems by Chick and Gans (2009) and Chick and Frazier (2012), and also in the context of strategic
experimentation, with Bolton and Harris (1999) who consider a many-agent two-armed Bernoulli bandit
problem in which agents can learn from the experimentation of other agents (i.e., information as a public
good).

Our work also contributes to a growing stream of sequential hypothesis testing problems in the context
of best arm identiﬁcation (BAI) (see, Russo, 2020, Garivier and Kaufmann, 2016, and Kaufmann et al.,
2016). In our sequential hypothesis testing setup, we interpret each available experiment as an ‘arm’ that
when pulled generates information on the true hypothesis. A key diﬀerence between our model and this
literature is that we allow for the possibility that the set of arms available for learning to be diﬀerent
from the sets of arms from which the DM chooses a ﬁnal action. One feature of our model is that the
DM learns about the true hypothesis from any pulled arm. This behavior is similar to some BAI settings
where the unknown parameters can aﬀect the reward of multiple correlated arms, (see, Soare et al., 2014).
Moreover, in the illustrative example of Section 6, we assume that an experiment is an assortment of
products oﬀered to a customer and the outcome is the product selected by that customer. We assume
in this example that this selection happens following an MNL model making this setup similar to an
MNL-bandit like exploration (see the recent work of, Agrawal et al., 2019 and Oh and Iyengar, 2019).
Despite some structural diﬀerence with BAI and more generally, MAB literature, we compare in the
numerical section the performance of some MNL-bandit algorithms - introduced in the literature - with
the ones we suggest here.

Finally, we recall that this work naturally belongs to the broad area of reinforcement learning. Our sug-
gested heuristics can be viewed as approximate DP techniques for solving a dynamic learning problem.
Such techniques have been shown to be eﬀective in managing the curse of dimensionality (see, Powell,
2016). In this recent review, Powell divides ADP policies in four categories: myopic cost function ap-
proximations, lookahead policies, policy function approximations and policies based on value function
approximations. The latter two are often based on the speciﬁc structure of the problem. Indeed, most of
the heuristics we suggest (see, Section 5) belong to these two categories and are obtained either by reduc-
ing carefully the set of policies we are optimizing on, or by approximating the value function itself. These
approximations are primarily inspired and obtained based on our asymptotic analysis. In our numerical
analysis (see Section 7) we also include a lookahead type policy. Some recent works have highlighted the
eﬀectiveness of simple policies in the context of dynamic learning such as greedy algorithms (e.g., Bastani
et al., 2020) and Certainty-Equivalence (e.g., Keskin and Zeevi, 2018). The greedy algorithm behaves
well when exploration is expensive while in our case it is free. As for the certainty equivalence (CE),
it is not appropriate in our setting. Indeed, in a Bayesian setting, CE would assume that the current
belief is constant moving forward and hence would always recommend to stop and never to explore.
Having said that, we do show that in our case a simple (static) experimentation policy behaves well and
is asymptotically optimal.

Our paper also contributes to the operations literature on sequential testing and demand learning. There
is a growing stream of papers in revenue management that have focused on the problem of characterizing
optimal dynamic pricing strategies when there is incomplete information about consumers’ price sensi-

5

tivity (see, Araman and Caldentey, 2011 and den Boer, 2015). In this context, pricing strategies play a
dual role. On one hand, they have a direct impact on sales and revenues. On the other, they act as tools
for experimentation used by sellers to learn demand characteristics. Optimal pricing strategies are those
that balance the so-called exploration-exploitation tradeoﬀ between these two roles, e.g., Araman and
Caldentey (2009), Besbes and Zeevi (2009), Harrison et al. (2012), den Boer and Zwart (2014), Broder
and Rusmevichientong (2012), Gallego and Talebian (2012) and Keskin and Zeevi (2014). Another stream
of papers, which is closer to the crowdvoting example that we consider in Section 6, focuses on optimal
assortment planning under unknown demand characteristics. In this literature, the decision maker wants
to identify a revenue maximizing assortment of products from a (possibly very large) set of available
options. Consumers’ preferences over assortments are typically described in the form of a Luce-type
choice model –with the MNL being by far the most popular choice–with unknown parameters. In this
setting, the DM experiments by displaying diﬀerent assortments to diﬀerent consumers over time. Some
representative papers in this area include Caro and Gallien (2007), Ulu et al. (2012), Saur´e and Zeevi
(2013), Agrawal et al. (2019) and Feng et al. (2018). A variant of this line of research is the recent paper
of Keskin and Birge (2019), where the seller faces unknown cost functions that increase with the quality of
the products. At each period, the ﬁrm selects vertically diﬀerentiated products and self-selection pricing
mechanisms to learn and maximize its proﬁt over a ﬁnite horizon.

Finally, our research also contributes to the recent and growing literature on crowdsourcing and speciﬁcally
crowdvoting. We mention the work of Krager et al. (2014) that looks at adaptively allocating small tasks
to workers through crowdsourcing while meeting some reliability target. The recent work of Papanastasiou
et al. (2018) tackles the provision of information dissemination in an online setting where customers’
selection of products/services is aﬀected by historical outcomes. On the crowdfunding end, Alaei et al.
(2016) suggest a dynamic model of crowdfunding and assess the probability of success of a campaign by
introducing the notion of anticipating random walks. On crowdvoting, the paper by Marinesi and Girotra
(2013) focuses on measuring the information that is acquired from a customer voting system. Using a
two-period game-theoretical model, they prove among other results that by oﬀering a suﬃciently high
discount during the voting phase, crowdvoting systems - used to decide whether to develop the product or
not - represent an eﬀective way to elicit information on customers willingness-to-pay. Finally, motivated
by recent applications in blockchain-based platforms, Tsoukalas and Falk (2019) consider the problem of
information aggregation from a collection of partially informed agents having private information about
some unknown state of the world (e.g., the quality of product). Agents submit a vote –in the form of an
estimate of the true value of state of the world– and the platform aggregates these votes to produce a
ﬁnal estimate. The value of the platform and the payoﬀs collected by the agents depend on the accuracy
of this ﬁnal estimate. The paper studies the impact of using diﬀerent weighting mechanisms to aggregate
votes on agents voting strategies and the informativeness of the resulting equilibrium outcomes. Our
focus however through our crowdvoting example, is on how to operationally manage a voting platform
that faces a stream of myopic (non-strategic) consumers. In that regard our work is close to Feng et al.
(2018).

The rest of the paper is organized as follows. In the next section, we introduce the diﬀerent components
of the general model together with the main assumptions and formulate the problem as a two-stage

6

dynamic programming problem where the ﬁrst stage is concerned with the experiment design while the
second stage tackles the duration of the experimentation. We also prove the convexity of the value
function and discuss how one can leverage this property to simplify the optimization and generate a
simple heuristic for the experimentation. Section 4 is fully devoted to the asymptotic analysis. We start
by describing the scaling and the corresponding regime and obtain a diﬀusion formulation of the original
problem. We move next to solving for the corresponding optimal experimentation policy as well as the
optimal stopping of the experimentation phase. These two decisions are shown to decouple. We ﬁrst
show that a static experimentation policy is optimal at the limit and the optimal experiment is the one
that maximizes the volatility of the belief process. As for the optimal duration of experimentation, it is
formulated as an optimal stopping problem very much in the spirit of Wald’s SPRT test. The solution
is fully characterized by a partition of the belief space into a collection of intervals that determine those
regions where experimentation is needed or not. Both the value function and the expected value of the
stopping time are obtained in closed form. Motivated by the simple optimality principle that characterizes
the diﬀusion problem, we suggest in Section 5 heuristics for the experimentation policy under a non-
asymptotic regime, and discuss ways to identify the various components of the heuristics while relying
only on the initial primitives of the problem. In Section 6, we discuss in detail an illustrative example of
the assortment selection in the context of new product introduction. The results of the previous sections
are adapted to this setting followed, in Section 7, by an extensive numerical analysis. For that, various
heuristics of the display set policy are introduced and compared numerically to the diﬀusion-derived
heuristics, conﬁrming the high performance and robustness of the latter. Finally, given the connection of
our setting in Section 6 with MNL-bandit and best arm identiﬁcation literature, we also show numerically
that our suggested heuristics outperforms oﬀ-the shelf algorithms from this literature. We conclude in
Section 8 and oﬀer some possible directions for future work. Most proofs have been relegated to the
appendix.

3 Model Description

We consider a decision maker (DM) who must choose an action a in order to maximize a reward function
(a, Θ) that depends on both the action a as well as a parameter Θ. The DM selects the action a from
R
a ﬁnite set of available actions A and does not know the value of Θ that can take one of two possible
values
. Speciﬁcally, the DM has incomplete information on Θ, and hence on the reward function,
having a prior that Θ = θ0 with probability δ

θ0, θ1}
{

(0, 1).

∈

We assume that the decision maker is risk neutral. If she were to make a decision at time t = 0, she
would then select an action that maximizes her expected reward conditional on her prior belief δ. That
is, she would select an action a∗ ∈
] is the expectation operator
conditional on the prior belief that Θ = θ0 with probability δ†. We deﬁne the optimal expected reward
function

(a, Θ)], where Eδ[
·

A that maximizes Eδ[

R

G(δ) := max
A

a

∈

Eδ[

R

(a, Θ)] = max
A

a

∈

R

δ

(a, θ0) + (1

δ)

(a, θ1)

−

R

(cid:110)
†To be precise, the probabilistic framework that we consider is deﬁned by a probability space (Ω, F; P0, P1) equipped
with two probability measures P0 and P1. For each δ ∈ [0, 1], we associate a probability measure Pδ = δ P0 + (1 − δ) P1 and
let Eδ[·] denote its expectation operator. Finally, Θ is a Bernoulli random variable that satisﬁes Pδ(Θ = 0) = δ.

(cid:111)

(1)

7

⊆

A∗(δ)

A be the set of actions at which the maximum reward is achieved. Without loss of
and let
A , there exists a δ
∈ A∗(δ) (otherwise,
optimality, we assume that for every a
some actions are uniformly dominated and can be removed from the set A of available actions). It is
worth noticing that, since A is a ﬁnite set, the function G(δ) is piece-wise linear in δ.

(0, 1) such that a

∈

∈

3.1 The Experimentation Process

Instead of selecting immediately an action from the set
A∗(δ), the DM has the option of postponing this
decision in order to experiment and gather additional information about the true value of Θ. The type
of experimentation process that we consider is characterized by two key features:

1. The decision maker has at her disposal a ﬁnite set E of experiments. Each experiment

associated a ﬁnite set

XE

of possible outcomes and a likelihood function

E has

E ∈

(x,

L

E

) :=

Q(x,
Q(x,

, θ1)
, θ0)

,

E
E

x

,

∈ XE

where Q(x,
E
experiment
E
sense that there exists x

, θ) := Pθ(x
is used and Θ = θ. We assume that every experiment
(x,

such that

= 1.

|E

) is the conditional probability of observing outcome x

E ∈

)

when the
∈ XE
E is informative in the

∈ E

L

E

2. There exists an exogenous Poisson process Nt, with rate Λ, that determines the time epochs

1
≥
. As a result, while the decision
at which experiments are conducted, where ti = inf
maker selects the experiment at each experimentation epoch, she does not have control over the
exact times when these experiments are conducted‡.

0 : Nt ≥

ti}i

i
}

≥

{

{

t

}

{

Et2, . . . ,

xt1, xt2, . . . , xtNτ }

to conduct at the experimentation epochs

In this setting, a policy is a triplet (π, τ, aτ ), where π is an experimentation policy that adaptively
determines the sequence of experiments
{Et1,
1,
ti}i
{
A is the action
τ is a stopping time that deﬁnes the duration of the experimentation process, and aτ ∈
taken at time τ . Since at optimality we have a∗τ ∈ A∗(δτ ), we will simply denote by (π, τ ) a generic
policy. We also denote by
Ft
the history (ﬁltration) generated by the experimentation process up to time t. We denote by T the set
of stopping times with respect to F = (
0. Also, and using a slight abuse of notation, we denote by
Et the experiment that is used at time t and by xt the corresponding outcome. Naturally, we must have
t.
xt ∈ XE
By judiciously selecting an experimentation policy π and observing the outcomes of each experiment,
the decision maker can gradually learn the true value of Θ over time. In particular, we deﬁne the belief
process δt := Pδ(Θ = θ0|Ft) whose evolution is governed by Bayes rule.

the sequence of outcomes of the experiments and by

Ft)t

≥

≥

‡This description assumes that the outcomes of the experiments are instantly observed. Alternatively, we can think that
each experiment takes an exponential random time (with rate Λ) to generate an outcome and only at this point in time the
next experiment can be set. As a result, the outcomes of the experiments will follow again a Poisson process with rate Λ.
For instance, in the crowdvoting example mentioned in the introduction, experimentation occurs when a customer arrives
to the online platform and votes, which we model as a Poisson process. See Section 6 for more details.

8

(cid:54)
be the
Lemma 1 (Belief Process). Let
corresponding sequence of observed outcomes. If the decision maker has a prior belief δ = Pδ(Θ = θ0),
Ft-martingale given by:
then the belief process δt evolves as an

be a sequence of experiments and

xt1, xt2, . . .
{

Et2, . . .

{Et1,

}

}

δt =

δ + (1

δ

−

δ) Lt

, where Lt is the likelihood-ratio function Lt :=

Nt

(cid:89)i=1

(xti,

L

Eti).

(2)

Proof: This and other proofs are relegated to the Appendix. (cid:50)

3.2 The Optimization Problem

E

L

(x,

) the belief process converges to 0 or 1
Under some mild assumptions on the likelihood ratios
depending on whether Θ = θ0 or Θ = θ1, respectively. Hence, an inﬁnitely patient decision maker will
eventually learn the true value of Θ. However, by running a long experimentation process the decision
If we assume that, ceteris paribus,
maker is also delaying the time when the ﬁnal decision is made.
the decision maker prefers to collect these rewards as early as possible then she faces a trade-oﬀ between
learning the true value of Θ (exploration) and collecting the reward
(a, Θ) (exploitation). To model this
trade-oﬀ we assume that the decision maker’s objective is to maximize the expected discounted reward
that she will collect at the time a ﬁnal decision is made. That is, she is interested in solving the following
optimal stopping time problem:

R

Π(δ) := sup
(π,τ )

Eδ

e−

r τ G(δτ )

,

(3)

where r is the decision maker’s discount factor. We will tackle the solution of (3) using dynamic pro-
gramming. To this end, we ﬁnd it convenient to express the dynamic evolution of the belief process δt in
equation (2) using the following SDE representation.

(cid:2)

(cid:3)

Lemma 2. The belief process in (2) admits the SDE representation:

dδt = η(δt

−

, xt,

Et
−

) dNt, where

η(δ, x,

) := (1

δ) δ

−

E

(cid:18)

1

(x,
δ)

)
E
(x,
L

− L
−

δ + (1

.

)

E

(cid:19)

In the statement of the previous lemma, the left-limit notation
that is chosen (observed) right before a jump of Nt at time t. The factor η(δ, x,
the belief process (i.e., the “amount” of learning) if an experiment
E
x when the belief process (just before the experiment) is equal to δ.

Et
−

(δt

−

) stands for the experiment (belief)
) is the size of the jump of
is chosen that produces an outcome

E

Equipped with Lemma 2, we formulate the decision maker’s problem as a Markov Decision Problem
(MDP) and without loss of optimality, restrict our attention to the class of deterministic Markovian
policies (e.g., Blackwell, 1965 and Section 4.4 in Puterman, 2005). In particular, the experimentation
E and the stopping time τ is a hitting
policy π maps each value of the belief δ to an experiment π(δ)
time of the belief process on some intervention set
depending on
the context. In the following deﬁnition,
is the set of Borel sets in [0, 1].

(E ) is the set of measurable functions from [0, 1] to E and

. We will interchangeably use τ and

M

∈

B

I

I

9

)

I

×B

(E )

∈ M

. For all δ

Deﬁnition 1. (Deterministic Markovian Policy) A deterministic Markovian policy corresponds to a pair
E . On the other hand, for δ
the
(π,
∈ I
∈ A∗(δ).
decision maker chooses to stop the experimentation process and implements an optimal action a
Putting all the pieces together, the decision maker’s optimization in (3) can be rewritten as the following
optimal control problem:

, the DM displays experiment π(δ)

(cid:54)∈ I

∈

Π(δ) := sup
)

(π,

Eδ

e−

r τ G(δτ )

I
subject to: dδt = η(δt

(cid:2)

, xt, π(δt

−

(cid:3)
)) dNt,

δ0 = δ,

and τ = inf

−

(4)

.

t > 0 : δt ∈ I
(cid:8)

(cid:9)

Finally, we can express the optimality conditions of the control problem in (4) in the form of the following
Hamilton Jacobi Bellman (HJB) equation:

0 = max

G(δ)

Π(δ) , Λ max
E

−

E∈

Eδ
(cid:110)

(cid:26)

Π

δ + η(δ, x,

)

Π(δ)

r Π(δ)

,

(5)

E

−

−

(cid:27)

(cid:104)
with border conditions Π(0) = G(0) and Π(1) = G(1) since both δ = 0 and δ = 1 are absorbing belief
states (see Lemma 1). By solving the inner maximization, we can compute an optimal experimentation
policy

(cid:105)(cid:111)

(cid:0)

(cid:1)

E ∗(δ), that is,

Π

δ + η(δ, x,

.

(6)

(cid:104)
The HJB equation in (5) leads to a tractable computational approach to solve the decision maker’s
problem. For instance, we can implement the value iteration algorithm

(cid:0)

E∈

∗(δ)

argmax
E

∈

E

Eδ
(cid:110)

E

)
(cid:1)(cid:105)(cid:111)

Π0(δ) = G(δ)

and

Πl+1(δ) = max

G(δ),

(cid:26)

Λ
Λ + r

max
E
E∈

Eδ
(cid:110)

Πl
(cid:104)

(cid:0)

δ + η(δ, x,

E

,

)
(cid:1)(cid:105)(cid:111)(cid:27)

(7)

which deﬁnes a sequence of continuous functions
and converge uniformly to a limit Π(δ) = liml
proof of Proposition 1 for details).

that are monotonically increasing in l
Πl(δ) : l
Πl(δ) that satisﬁes the HJB equation in (5), (see the
(cid:8)
→∞

≥

(cid:9)

0

Despite its computational simplicity, the HJB equation (5) is not particularly malleable for the purpose
of analysis and to derive structural results about an optimal solution and its properties. For this reason,
in the next sections, we tackle the decision maker’s optimization problem using a diﬀusion approxima-
tion that preserves the same trade-oﬀs as in the original formulation but provides a more transparent
representation of the problem and its optimal solution.

We end this section with a numerical example that illustrates the value iteration method used above and
highlights some feature of an optimal solution.

Example 1. Suppose the decision maker has four alternative actions to choose from (i.e.,
corresponding payoﬀs
R2(δ) = 4
R1(δ) = 6
are nine possible experiments that the DM can use (i.e.,
outcome, that is,
of the nine experiments for Θ = θ0 and Θ = θ1. Finally, we let Λ = 8 and r = 0.5.

= 4) with
20 + 25 δ. There
R3(δ) = 3 δ and
R4(δ) =
= 9) and each experiment produces a binary
|
, Θ) for each

E . The table below speciﬁes the probability Q(0,

5 δ,
E
|

0, 1
}

for all

30 δ,

E ∈

XE

=

−

−

−

E

{

|

A
|

10

Experiment
Θ = θ0
Θ = θ1

1
0.1
0.03

2
0.2
0.04

3
0.3
0.09

4
0.4
0.16

Q(0,

, Θ)

E
5
0.5
0.25

6
0.6
0.36

7
0.7
0.49

8
0.8
0.68

9
0.9
0.86

Table 1: Probabilities of observing outcome ‘0’ for each of the nine experiments as a function of the value of Θ.

Figure 1 depicts the numerically computed solution using the value iteration in (7) after 200 iterations.
E ∗(δ).
The left panel shows the value function Π(δ) while the right panel shows the optimal experiment
We use the convention
E ∗(δ) = 0 for those values of δ at which Π(δ) = G(δ) and no experimentation is
used.

Figure 1: Numerically computed solution. The left panel depicts the value functions Π(δ). The right panel depicts the optimal
experiment E ∗(δ). Data: R1(δ) = 6 − 30 δ, R2(δ) = 4 − 5 δ, R3(δ) = 3 δ, R4(δ) = −20 + 25 δ, r = 0.5 and Λ = 8.

As we can see, in an optimal solution, the belief space is partitioned into a collection of intervals that
deﬁne the regions where experimentation is used or not used. For example, in the interval δ
[0.1, 0.31]
the decision maker does not use any experimentation and selects immediately (at time t = 0) an action
in
). On the other hand, in the
(0.31, 0.69) the DM wants to experiment. In this case the interval (0.31, 0.69) is further
interval δ
partitioned into a collection of subintervals in which a speciﬁc experiment is selected. For instance, for

A∗(δ) that maximizes her expected reward (in this case

A∗(δ) =

2
}

∈

∈

{

(0.45, 0.51) the decision maker uses experiment 3 while for δ

δ
∈
We note that experimentation occurs around those values of δ where the payoﬀ function G(δ) = maxi{Ri(δ)
}
has a kink, i.e., where two payoﬀ functions intersect. Intuitively, in these regions a small change in the
value of δ can lead to a discrete change in the optimal action to select and so the DM has locally more
incentive to experiment and learn in these regions.

(0.51, 0.61) she uses experiment 4.

∈

11

00.20.40.60.81Belief: δ0123456Value Function00.20.40.60.81Belief: δ0123456789Optimal ExperimentR1( )R3( )R2( )R4( )E⇤( )⇧( )3.3 On the Convexity of the Value Function

The following proposition will prove useful in various places in the analysis that follows.

Proposition 1. The functions G(δ) and Π(δ) are both convex in δ

[0, 1].

∈

One way in which we can take advantage of this property is to simplify the optimization problem. In
some applications, the cardinality of the set of possible experiments E can be rather large adding an extra
layer of complexity to the problem of solving the HJB equation in (5)§. One possible step to mitigate
this issue is to reduce the number of potential experiments to consider. Speciﬁcally, we can use the fact
that the value function Π is convex to eliminate those experiments that are dominated in a convex order
dominance sense¶.

E , let Z(δ,

E

∈

(0, 1) and

) := δ + η(δ, x,

E . Suppose that for two experiments
E2) dominates Z(δ,

Indeed, for every δ
E ∈
value of the posterior belief when the prior belief is δ and experiment
δ for all
E ∈
random variable Z(δ,
1994). Then, by convexity of Π in Proposition 1, we get that Eδ[Π(Z(δ,
result, experiment
belief process equals δ.
speciﬁc knowledge of the value function beyond the fact that it is convex.

) be the random variable that deﬁnes the
is selected. Note that Eδ[Z(δ,
)] =
E2), that is, the
≤cx Z(δ,
E1) in the convex order sense (see Shaked and Shanthikumar,
E2)]. As a
E1 can be excluded from the set of possible experiments to be considered when the
It is worth highlighting that this elimination procedure does not rely on any

E we have that Z(δ,

Eδ[Π(Z(δ,

E1)]

E2 ∈

E1)

E1,

≤

E

E

E

= 2 for all

E1) is contained in the range of Z(δ,

One class of problems for which this elimination scheme is particularly simple is the class of problems in
E . This is
which each experiment can only generate two possible outcomes, that is,
an important special case given the popularity of pairwise comparison methods (see Sz¨or´enyi et al., 2015,
Heckel et al., 2019 and references therein). In this case, Z(δ,
E2) if and only if the range
E2). But this is the same as requiring that the range of the
of Z(δ,
likelihood ratio
E2), which is a condition that is independent of δ
(
L
and one can check eﬃciently. For example, if we apply this elimination scheme to the special instance in
Example 1, we get that Experiments 1, 8 and 9 can be eliminated. To see this, note that the likelihood
ratio of Experiment 1 takes values
while the likelihood ratio for Experiment 2 takes
values
. It follows that Experiment 2 dominates Experiment 1. (Similar calculations
}
reveal that Experiments 7 dominates Experiments 8 and 9.)

E1) is contained in the range of
(
L

≤cx Z(δ,

0.3, 1.078

0.2, 1.2

E1)
(

E2)

E1)

(
L

|XE |

E ∈

∈ {

∈ {

L

}

E2)

We can use stochastic dominance one step further to derive a simple experimentation policy. Since
Z(δ,
E1)], we can implement a heuristic policy
E2)]
)] =
which for each value of δ selects the experiment
E
E[η2(δ, x,

≤
H(δ) that maximizes Var[Z(δ,
E
)] and so this heuristic policy reduces to

E1) implies that Var[Z(δ,

)]. But Var[Z(δ,

≤cx Z(δ,

Var[Z(δ,

E

E

(cid:110)
In the following sections we will show that this simple experimentation policy is indeed optimal in an

(cid:105)(cid:111)

(cid:104)

E∈

H(δ) = argmax

E

η2(δ, x,

)

.

E

E

E

§For example, in the context of an optimal assortment selection problem with n products, there are 2n −1 possible display

sets that could be oﬀered. We will discuss this example in detail Section 6.

¶The notion that an experiment dominates another one is similar to the notion that an experiment is more informative

than another one as discussed in Blackwell (1951) (see also Lindley, 1956 and Cam, 1996).

12

appropriate asymptotic regime in which the magnitude of the jumps η2(δ, x,
zero, i.e., in a regime in which each experiment becomes less and less informative.

E

) converges uniformly to

We can also use the convexity of the value function to gain some intuition about this asymptotic result.
Indeed, if we assume that the value function is twice-continuously diﬀerentiable, then a second order
expansion of Π(δ) leads to the following equality:

Π

δ + η(δ, x,

)

E

−

Π(δ) = ˙Π(δ) η(δ, x,

) +

E

1
2

¨Π(δ) (η(x, δ,

E

))2 + O((η(δ, x,

))3).

E

The optimal experiment is characterized in equation (6), which we write here as follows,

∗(δ) = argmax

Π

δ + η(δ, x,

Eδ
(cid:110)

(cid:104)

E

E∈

)

E

−

Π(δ)

.

(cid:105)(cid:111)

By Lemma 2, δt is a martingale and so we have that Eδ[η(δ, x,
function, ¨Π(δ)
of η(δ, x,

(cid:1)
)] = 0. Also, by convexity of the value
0. Combining these two observations together with the assumption that the magnitude

) is uniformly small, we conclude that

≥

E

(cid:0)

(cid:1)

E

(cid:0)

E

E

∗(δ) = argmax

E

E∈

Eδ

η2(x, δ,

(cid:104)

E

)

.

(cid:105)

4 Asymptotic Approximation

In this section we specialize the problem described in the previous section to a particular class of instances
in which (i) experiments are conducted at “high frequency” while (ii) the “informativeness” of each
experiment is low. There are many natural and practical situations in which the decision maker has
access to a large number of experiments, but where the informativeness of each individual one is low. For
instance, online experiments are becoming quite common in the business world where each experiment is
often linked to one visitor who is oﬀered a set of choices to select from. Such common setup generates a
large volume of experiments in a relatively short time period. However, one of the major issues faced by
the experimenter is the relevance and veracity of the data generated (we refer the reader to the section
“Beware of Low-Quality data” in Kohavi and Thomke, 2017). In some cases, the heterogeneity of the
experementees in online experimentation can generate very noisy data. Moreover, the hypotheses being
tested can be marginally diﬀerent making the task of distinguishing them harder. Both settings are typical
and are examples of how little informative online experiments can be. Our illustrative example in Section 6
build on these ideas. The notion of limited informativeness of experiments is also present in other settings.
In their recent work, Lewis and Rao, 2015 show how diﬃcult it is to prove the return on investment of
advertising campaigns. The paper notes speciﬁcally that informative advertising experiments can require
more than 10-million person-weeks, which reﬂects exactly the tension of our regime between large sample
size and little informativeness. Clinical trials is another major area that suﬀers from serious data error
and lack of accuracy, see, Nahm, 2012 and as a result would require large sample sizes.

13

4.1 Diﬀusion Formulation

low informativeness” regime of experimentation, we
To formalize the notion of a “high frequency vs.
consider a sequence of instances of the problem indexed by a non-negative integer k in such a way that as
k grows large both the number of experiments conducted per unit of time becomes high and the ‘amount’
of information generated from each individual experiment goes to zero. Under our proposed scaling the
magnitude of the jumps of δt as well as the time between two experiments converge to zero resulting in
a belief process that converges weakly to a diﬀusion process.

, θ) be the conditional probability of observing outcome x

We let Qk(x,
is conducted conditional on Θ = θ for the kth instance of the problem. To capture the notion of low
informativeness of an experiment, we impose the following requirement on the sequence

when experiment

∈ XE

E ∈

E

.

E

Qk(x,
{

, θ)
}

E

Assumption 1. (Low Informativeness Regime) For each
θ0, θ1}

) such that for θ

,
(
·

∈ {

Q

E

E , there exists a probability distribution

E ∈

√k

(cid:18)

Qk(x,
E
(x,
E

Q

, θ)
) −

α(x,

, θ),

E

−→

1

(cid:19)

(8)

where α(x,

E

, θ) satisﬁes

α(x,

, θ)

(x,

) = 0.

E

Q

E

(cid:88)x
∈X
Intuitively, the asymptotic scaling in Assumption 1 has the following property: as k
function
ηk(δ, x,
of an experiment become less and less informative as k grows large.

, the likelihood
and as a result the jumps
, θ0) converges to one for every x
L
) of δt (see, Lemma 2) converge to zero. In other words, in this asymptotic regime, the outcomes

, θ1)/Qk(x,

) = Qk(x,

→ ∞

∈ XE

k(x,

E

E

E

E

E

On its own, the scaling in equation (8) would lead to a trivial limit in which δt remains constant over
time. To counterbalance the fact that individual experiments become less informative under (8), we also
scale up the arrival rate of Nt in a way that the ‘amount’ of information collected by the experimentation
process per unit of time remains comparable to the one in the original unscaled system. Speciﬁcally, let
N k

t denote the Poisson process that determines the experimentation epochs for the kth instance.

Assumption 2. (High Frequency Regime) Let Λk be the intensity of N k

t . Then, Λk satisﬁes:

Λk = k Λ,

(9)

for some ﬁxed constant Λ > 0.

Our objective at this point is to suggest a diﬀusion approximation of the general formulation problem
(4). For that, we combine the parameter scalings in (8) and (9) to obtain a well-deﬁned diﬀusion limit for
†. We derive this limit over the class of continuous randomized Markovian policies
the belief process, δt
deﬁned below and show that this class contains an ε-optimal policy for any ε > 0 (see, Proposition 3).

In the following deﬁnition, ∆
experiments in E and

Mc(∆E ) is the set of continuous measurable functions from [0, 1] to ∆
†In Sections 5 and 6.2 we show how to interpret and operationalize our asymptotic scaling in practical settings.
(cid:0)

is the set of probability distributions on the collection of possible
. For

E

(cid:0)

(cid:1)

(cid:1)

E

14

a randomized experimentation policy π
experiment

, which is continuous in δ for all

∈ Mc(∆E ), we let π(δ,
E .

E

E ∈

) denote the probability of selecting

E

Deﬁnition 2. (Continuous Randomized Markovian Policy) A continuous randomized Markovian policy
E with probability
is a pair (π,
× B
the decision maker chooses to stop the experimentation process and
). On the other hand, for δ
π(δ,
implements an optimal action a

, the DM displays experiment

∈ Mc(∆E )

. For all δ

E ∈

(cid:54)∈ I

I

E

)

∈ I
∈ A∗(δ).

Next, we move to state our limiting result for the belief process under the scalings given in (8) and (9).

Proposition 2. Consider a ﬁxed experimentation policy π
induced by π for instance k under the scaling in equations (8) and (9). Then, we have that δk
k

, where ˜δt is a diﬀusion process solution of the SDE

∈ Mc(∆E ) and let δk

t be the belief process
˜δt as

t ⇒

→ ∞

where Wt is a Wiener process and,

d ˜δt = ˜σ(˜δt, π) ˜δt (1

˜δt) dWt,

−

˜σ2(δ, π) := Λ

π(δ,

E

)

α(x,

, θ1)

E

−

α(x,

E

2

, θ0)

(x,

Q

).

E

(cid:0)

(cid:1)

E
(cid:88)E∈

(cid:88)x
∈X

E

(10)

Remark 1. Throughout the paper, we use tildes (‘
approximation. (cid:51)

∼

’) to denote quantities that are related to the asymptotic

The next result shows that restricting our attention to continuous randomized policies is without a
signiﬁcance loss of optimality in the sense of the L1 norm.

Proposition 3. Let (π,
× B
function Π(δ). For any ε > 0, there exists a continuous randomized policy (πc,
expected payoﬀ function

be an optimal Markovian policy with a corresponding value
with

Mc(∆E )

Ic) in

Π(δ) such that,

∈ M

× B

I

)

(E )

where

(cid:107) · (cid:107)1 is the L1 norm in [0, 1].

(cid:98)

Π
(cid:107)

Π

(cid:107)1 < ε,

−

(cid:98)

In sum, and in light of Proposition 2 and 3, we suggest the following diﬀusion-asymptotic approximation
of the decision maker’s problem given in (4):

(π,

)
∈M

I

c(E )

×B

(cid:104)

(cid:105)

Π(δ) =

sup

Eδ

e−

r τ G(˜δτ )

s.t. d ˜δt = ˜σ(δ, π) ˜δt (1

˜δt) dWt

−

and τ = inf

t > 0 : ˜δt ∈ I

.

(cid:9)
(11)
(cid:101)
We can view problem (11) as having two decision variables, namely, the experimentation policy π and
the intervention region
. Interestingly, it turns out that we can decouple the optimization of these two
decisions, in particular we can solve for the optimal experimentation π∗ without computing explicitly
I ∗. Surprisingly, this implies that the choice of an optimal experiment is independent of the intervention
region, and thus of how long the decision maker decides to run the experimentation process. We formalize
this observation in the following section.

(cid:8)

I

15

4.2 Asymptotically Optimal Experimentation Policy

From the diﬀusion approximation in equation (11), one can easily see that the impact of an experimenta-
tion policy π has on the decision maker’s optimization problem is channelled only through the volatility
of the belief process ˜σ(δ, π). We use this fact to derive a rather simple solution to the problem of selecting
an asymptotically optimal policy πA. (The superscript ‘A’ is mnemonic of Asymptotic). To this end, let
us deﬁne the mapping

T π
t

:=

0
(cid:90)

t

1
˜σ2(δs, π)

ds,

which acts as a random time change in the following proposition.

Proposition 4. The optimization problem in (11) is equivalent to

(cid:101)

Π(δ) =

sup

(π,

I

)
∈M

c(E )

×B

Eδ

e−

(cid:104)

r T π

τ G(˜δτ )
(cid:105)

s.t. d ˜δt = ˜δt (1

−

˜δt) dWt

and

τ = inf

t > 0 : ˜δt ∈ I
(cid:8)

.

(cid:9)

The previous result provides an alternative interpretation of the eﬀect an experimentation policy π has
on the decision maker’s performance. According to Proposition 4, a policy π impacts only the discount
factor, r T π
τ that the decision maker uses to penalize the time value of money. The following corollary
follows directly from this observation.

, an optimal asymptotic experimentation
Corollary 1. (Maximum Volatility) For any stopping set
policy πA minimizes the modiﬁed discount factor r T π
τ pathwise, or equivalently, maximizes pointwise the
belief process’s volatility ˜σ2(δ, π). Thus, from (10), we conclude that we can select πA to be a static
experimentation policy, namely, πA(δ,

A is given by

for all δ

c where

I ∈ B

) = 11

=

A

E

E

E

∈ I

E

A = argmax

E 


E∈

(cid:88)x
∈X

E (cid:0)

E

(cid:101)

(cid:0)

α(x,

(cid:1)

(cid:101)
, θ1)

E

α(x,

E

−

2

, θ0)

(x,

Q

(cid:1)

(cid:101)
)

E

.




(If there are multiple experiments that maximize the expression inside the brackets then we can select any
static experimentation policy that uses an experiment

A from the argmax set.)





E

I

(cid:101)

A few remarks about this result are in order. First, Corollary 1 conﬁrms our previous claim that an optimal
experimentation policy is independent of the choice of the stopping set
and so we can eﬀectively decouple
the problem of determining an optimal experimentation strategy and that of when to stop experimenting.
We also note that an optimal static experimentation strategy is continuous in δ and so we can invoke the
weak convergence in Proposition 2 directly to πA(δ,

E
Example 2. (Example 1 Revisited) To illustrate the result in Corollary 1, let us revisit the instance in
(cid:101)
Example 1 in the context of the asymptotic regime. To this end, suppose the probabilities Qk(0,
, θ) for
the kth instance of the problem are equal to
Note that we are not including Experiments 1, 8 and 9 since they are dominated (see Section 3.3). Also,
the original probabilities in Table 1 correspond to the case k = 1. Figure 2 mimics Figure 1 but for
k = 10, 000.
Consistent with the result in Corollary 1, for k suﬃciently large, the optimal experimentation strategy
A(δ) consists of a single experiment independent of δ, which in this case corresponds to Experiment 5.

E
One can check that Experiment 5 maximizes the instantaneous volatility of the belief process. (cid:51)

A).

E

16

Qk(0,

, θ) =

(0,

Q

)

E

E

1 + α(0,E,θ)

√

k

(cid:16)

(cid:17)

, where

Experiment
)
E
, θ0)
E
, θ1)
E

(0,
Q
α(0,
α(0,

2
0.2
0.0
-0.8

3
0.3
0.0
-0.7

4
0.4
0.0
-0.6

5
0.5
0.0
-0.5

6
0.6
0.0
-0.4

7
0.7
0.0
-0.3

.

Figure 2: Data: R1(δ) = 6 − 30 δ, R2(δ) = 4 − 5 δ, R3(δ) = 3 δ, R4(δ) = −20 + 25 δ, r = 0.5, Λ = 8 k and k = 10, 000.

4.3 Optimal Stopping of Experimentation

Let us now turn to the problem of determining the optimal intervention region in the asymptotic regime
under consideration. In what follows, we assume that an optimal experimentation policy has been selected
based on Corollary 1. That is, we focus on solving problem (11) given the optimal experimentation policy
πA(δ,
. We ﬁnd convenient to rewrite this problem using the following optimal stopping
=
) = 11
time formulation:

E

E

E

A

(cid:0)

(cid:1)

(cid:101)

G

(δ) := sup
T

τ

∈

Eδ

e−

(cid:104)

r τ G(˜δτ )
(cid:105)

subject to

d ˜δt = ˜σ ˜δt (1

−

˜δt) dWt,

˜δ0 = δ.

(12)

(cid:101)

E

For notational convenience, throughout this section we suppress the dependence of
set

A since it remains ﬁxed.

and ˜σ on the display

G

(cid:101)

(cid:101)

We approach the problem in two steps. First, we derive optimality conditions in the form of a set of partial
diﬀerential inequalities that characterize the optimal stopping time. Then, we use these inequalities to
characterize an optimal solution and the corresponding payoﬀ.

4.3.1 Quasi-Variational Inequalities

k[0, 1] denote the set of real-valued continuous functions on [0, 1] having derivatives of order k

Let
We deﬁne also the set

C

0.

≥

2 :=

f

(cid:110)

C

(cid:98)

1[0, 1]

∈ C

: there exists a ﬁnite set Nf ⊆

[0, 1] such that f (cid:48)(cid:48)(δ) exists

δ
∀

∈

[0, 1]

\

Nf

.

(13)

(cid:111)

17

00.20.40.60.81Belief: 0123456Value Function00.20.40.60.81Belief: 01234567Optimal ExperimentationR1( )R3( )R2( )R4( )E⇤( )⇧( )(Note that the set Nf depends on the speciﬁc function f .) We also deﬁne the operator

f (δ) :=

H

1
2

˜σ δ (1

−

δ) f (cid:48)(cid:48)(δ)

−

r f (δ),

for all δ

[0, 1]

Nf .

\

∈

on

C

H

2 as follows

(cid:98)

(14)

Deﬁnition 3. (QVI) The function f
problem (12), if for all δ
Nf

[0, 1]

∈

\

2 satisﬁes the quasi-variational inequalities for the optimization

∈

C

(cid:98)

G(δ)

0

≥

0

≤

f (δ)

−
f (δ)

H
f (δ)

G(δ)

−

H

f (δ) = 0.

(cid:50)

(15)

(cid:1)
As one might expect, a solution to these QVI conditions partition the interval [0, 1] into two regions: a
continuation region in which the ﬁrm’s optimal strategy is to keep experimenting and an intervention
region in which stopping the experimentation process is optimal.

(cid:0)

Continuation:

Intervention:

:=

δ

:=

δ

(cid:8)

∈

∈

C

I

[0, 1] : f (δ) > G(δ)

and

[0, 1] : f (δ) = G(δ)

and

f (δ) = 0

H

f (δ)

H

≤

0
(cid:9)

(cid:9)

For every solution of the QVI conditions we can associate a control τ

(cid:8)

T.

∈

Deﬁnition 4. Let f

∈

C

2 be a solution of the QVI conditions in (15). We deﬁne the control τ as follows

(cid:98)

τ = inf

t > 0 : f (˜δt) = G(˜δt)

and refer to it as the QVI-control associated to f .

(cid:8)

(cid:9)

We are now ready to formalize the veriﬁcation theorem that provides the connection between the QVI
conditions and the original optimization problem in (12).

Theorem 1. (Verification) Let f

∈

C

2 be a solution of the QVI in (15). Then,

f (δ)

(cid:98)
≥

G

(δ)

for every δ

[0, 1].

∈

In addition, if there exists a QVI-control τ associated with f such that E[τ ] <
f (δ) =

(δ).

(cid:101)

G

, then τ is optimal and

∞

(cid:101)

This veriﬁcation theorem reduces the problem of determining the value function
(δ) to that of solving
the QVI equations deﬁned above. In order to ﬁnd a solution, we take full advantage of the fact that the
[0, 1] (see equation (1)). Moreover,
payoﬀ function G(δ) is a piecewise linear continuous function of δ
an important building block in our methodology is the solution to a special case in which G(δ) has only
two linear pieces, that is, the set A includes only two actions. We will focus on this simpler case ﬁrst
and then show how to leverage this solution and extend it to the general case in which A includes an
arbitrary number of actions.

∈

G

(cid:101)

18

4.3.2 Special Case:

A
|

|

= 2

(cid:101)

(cid:9)

R

, where

Gij(δ) = max

ai, aj}
{
Rn(δ) = Eδ[
Gij(δ)
≥
(cid:101)

for two distinctive actions ai and aj. Let us denote
(an, Θ)] for n = i, j (see equation (1)). Without
Rj(δ)
[0, 1]. Let us denote by ˆδij the value of the
(cid:101)
Rj(ˆδij). (Recall that we have assumed that there is no action in the set A that
(cid:101)

Suppose the set of actions is given by A =
by
Ri(δ),
loss of generality, we will assume that
(cid:8)
(cid:101)
Ri(ˆδij) =
belief at which
(cid:101)
is uniformly dominated and this assumption guarantees the existence of ˆδij ∈
(cid:101)
To solve the QVI conditions in this special case we take “an educated guess” approach and assume that the
continuation region
1. Furthermore,
[δij, ¯δij]. To illustrate, consider the example in Figure 3 that
we assume the intuitive fact that ˆδij ∈
depicts the value function

Cij is given by an interval [δij, ¯δij], for two thresholds 0

¯δij ≤

0 for all δ

δij ≤

[0, 1].)

≤

∈

Gij(δ) as well as the payoﬀ functions
(cid:101)

Ri(δ) and
(cid:101)

Rj(δ) for products i and j.
(cid:101)

Figure 3: Example of the value function (cid:101)Gij(δ) and continuation region Cij for the case in which A includes two actions.
Data: (cid:101)Ri(δ) = 3 δ, (cid:101)Rj(δ) = 4 − 5 δ, r = 1 and ˜σ = 2.

By deﬁnition, in the interior of the continuation region we have that
the third QVI condition, in this region the value function must satisfy
diﬀerential equation

whose general solution is given by

(˜σ δ (1
−
2

δ))2

(cid:48)(cid:48)ij(δ)

G

r

−

(cid:101)

Gij(δ) = 0,
(cid:101)

Gij(δ) <

Gij(δ). Hence, according to
Gij(δ) = 0. This is a second-order
(cid:101)
(cid:101)

H
(cid:101)

ij

Gij(δ) = C0
(cid:101)

(1

−
δγ
−

δ)γ
1 + C1

ij

δγ
δ)γ

−

1 ,

−

(1

where γ :=

1 +

1 + 8r/˜σ2
2

(cid:112)

,

(16)

ij and C1

ij are two constants of integration.

and C0
To complete our proposed characterization of the value function, we need to determine the constants
of integration as well as the two thresholds δij and ¯δij that deﬁne the continuation region. To do that
we impose the so-called value-matching and smooth-pasting conditions that regulate the behavior of the
value function at the boundaries between the intervention and continuation regions. Speciﬁcally, we

19

00.20.40.60.81−101234eGij( )ˆ ij ij¯ ijCij:ContinuationRegionBelief: eRi( )eRj( )Gij(δ)

and

(cid:48)ij(δ) =

G

G(cid:48)ij(δ)

for δ = δij, ¯δij.

(17)

impose the conditions

Gij(δ) =
(cid:101)

(cid:101)
We formalize our previous discussion in the next proposition.

(cid:101)

(cid:101)

Proposition 5. Let γ = (1 +
that we denote by ˜

Gij(
·

) given as follows
(cid:112)

1 + 8r/˜σ2)/2. If A =

ai, aj}
{

then the QVI conditions admit a solution

δ)1

−

γ δγ

−

if

if

if

0

δ

≤

≤

δij

δij ≤

δ

≤

¯δij

¯δij ≤

δ

≤

1.

(18)

C0

ij (1

−

Gij(δ) =
(cid:101)






Gij(δ)

δ)γ δ1

(cid:101)
γ + C1
−

ij (1

Gij(δ)

(cid:101)

where δij, ¯δij ∈
and smooth pasting conditions in (17). The function

(0, 1) and C0

ij and C1

ij are positive constants all determined by imposing the value matching

The veriﬁcation theorem guarantees that the solution expressed in Proposition 5 is such that ˜
when A =

. In terms of implementation, this solution corresponds to the following policy.
}

i, j

(cid:98)

{

Gij = ˜
G

Gij(δ) is convex and in
(cid:101)

2.

C

Asymptotically Optimal Intervention Policy: Suppose the initial belief lies in the
(δij, ¯δij). In this case, the decision maker runs
interior of the continuation region
∈
(δij, ¯δj). As soon as the belief
an experimentation process and keeps it running as long as δt ∈
process δt hits one of the two thresholds δij or ¯δij then the experimentation process stops and
Gij at that time. On the other hand, if the
the decision maker selects the action that maximizes
initial belief δ is not in the interior of the continuation region then no experimentation is needed
and the decision maker selects the action that maximizes

Cij that is, δ

Gij(δ) at time 0. (cid:51)

(cid:101)

(cid:101)

The simple representation of the value function in Proposition 5 is due to the diﬀusion approximation
obtained in this asymptotic regime. Moreover, this same diﬀusion approximation allows one to use some
standard results for one-dimensional diﬀusion processes (e.g., Section 5.5 in Karatzas and Shreve, 1991)
(δij, ¯δij) experimentation should
to analyze its optimal solution. For instance, in those cases where δ
be conducted, and its duration would correspond to the ﬁrst exit time of δt from the interval (δij, ¯δij).
The following corollary characterizes the expected duration of this experimentation phase as well as the
likelihood that action ai or aj will be eventually selected.

∈

Corollary 2. Suppose δ
Then,

∈

(δij, ¯δij) and let τ ∗ = inf

t > 0 : ˜δt (cid:54)∈

{

(δij, ¯δij)
}

and ¯p(δ) = P(˜δτ ∗ = ¯δij |

δ0 = δ).

¯p(δ) =

δij
δ
−
¯δij −
δij
(δ) is the function

where

T

and

E[˜τ ∗] = ¯p(δ)

(¯δij) + (1

T

¯p(δ))

(δij)

T

− T

(δ),

−

(δ) :=

T

2
˜σ2 (2δ

−

1) ln

δ

1

(cid:18)

δ

(cid:19)

−

.

20

We conclude our discussion of this special case with
to derive upper and lower bounds for the value function.

(cid:107)

A

= 2 by exploiting the result in Proposition 4

(cid:107)

Proposition 6. The value function

Furthermore,

Gij(δ)

≤

(cid:101)

Gij(δ)
(cid:101)
max
(0,1)
δ
∈

4.3.3 General Case:

A

|

| ≥

2

Gij satisﬁes
(cid:101)
≤

Gij(0) (1

−

δ) +

Gij(1) δ

for all δ

(0, 1).

∈

(cid:101)
Gij(δ)
(cid:110)
(cid:101)

−

Gij(δ)

(cid:111)

(cid:101)

(cid:101)
Gij(ˆδij)
=
(cid:101)

Gij(ˆδij).

−

(cid:101)

Let us now turn to the general case in which the set A includes an arbitrary but ﬁnite number of actions.
(δ) in (12) will be obtained based on the solution derived in the
Our derivation of the value function
Gij(δ) in Proposition 5 is the value
previous section. For each pair of actions
ai, aj} ∈
{
Gij(δ) for all
(δ)
function of a problem in which only actions ai and aj are available. It follows that
G
(cid:101)
δ
(cid:101)

A , the function

[0, 1] and so

≥

∈

G

(cid:101)

(cid:101)

(δ)

≥

V (δ) := max
}∈

ai,aj

{

A

G

(cid:101)

(cid:101)

Gij(δ)
(cid:110)
(cid:101)

,

(cid:111)

V (δ) is the point-wise maximum of the functions

Gij(δ). Our main result in this section establishes
V (δ). To prove this, we will show that the
(δ) =
(cid:101)
V (δ) satisﬁes the QVI conditions so that we can invoke the veriﬁcation Theorem 1. To this end,

where
that the inequality is in fact an equality, that is,
(cid:101)
function
we ﬁrst show that
(cid:101)

V (δ) satisﬁes all three QVI conditions in (15).

G

(cid:101)

(cid:101)

Proposition 7. For all δ
(cid:101)
0 and
such that
V (δ)

∈
V (δ)

H

≤

G(δ)

−

H

[0, 1], we have that

V (δ)
V (δ) = 0 for all δ

≥

G(δ). Also, there exists a ﬁnite set N (cid:101)V ⊆
(cid:101)

N (cid:101)V .

[0, 1]

∈

\

(cid:101)

[0, 1]

(cid:1)

(cid:0)

(cid:101)

(cid:101)

(cid:101)

(cid:101)

The attentive reader might have noticed that the result in Proposition 7 is not enough to invoke the
veriﬁcation Theorem 1. The reason is that, besides verifying the QVI conditions, we also need to show
2 (see equation (13)). We formalize
that the function
this condition in the following result.
(cid:101)

V (δ) is suﬃciently smooth and belongs to the set

Theorem 2. The function

V (δ) = max
{

Gij(δ)
(cid:9)
(cid:8)
It is worth noticing that the previous theorem shows that the complexity of the diﬀusion optimal stopping
(cid:101)
problem grows only quadratically with the cardinality of the action set A . In fact, Theorem 2 reveals
that solving a problem with
1) problems
each with only two actions.

actions is equivalent to solving a collection of

2. As a result,

V (δ).

(δ) =

A
|

A
|

is in

| −

ai,aj

A

}∈

G

(cid:101)

(cid:101)

(cid:98)

(cid:101)

(cid:98)

C

C

A

(

|

|

|

To illustrate the result in Theorem 2 and resulting optimal policy, let us consider the example in Figure 4
in which the set A has four actions. The left panel depicts all six functions
while
the right panel depicts the function
ai,aj
After a quick inspection, we can check that in this example there exist two (non-unique) thresholds δ and
¯δ such that

Gij(δ) :
(cid:8)
(cid:101)

V (δ) := max
{

Gij(δ)
(cid:101)

ai, aj} ∈

A

}∈

(cid:8)

(cid:9)

(cid:9)

(cid:101)

{

A

.

V (δ) = 


(cid:101)



if 0
if δ
if ¯δ

δ
δ
δ

≤
≤
≤

≤
≤
≤

δ
¯δ
1.

G12(δ)
G23(δ)
(cid:101)
G34(δ)
(cid:101)
(cid:101)

21

Figure 4: Example in which the oﬀer set O includes four products. The left panel depicts the value functions (cid:101)Gij(δ) derived in
(cid:101)Gij(δ)(cid:9). Data: R1(δ) = 6 − 30 δ, R2(δ) = 4 − 5 δ,
Proposition 5. The right panel depicts the function V (δ) := max{i,j}∈O
R3(δ) = 3 δ, R4(δ) = −20 + 25 δ, r = 1 and ˜σ = 2.

(cid:8)

G23(δ) =
R2(δ).
G23(δ) meet smoothly since
Furthermore, at δ = δ the functions
G12(δ) and
R3(¯δ). As a result, since each of the
G34(¯δ) =
G23(¯δ) =
A similar smooth pasting occurs at δ = ¯δ since
(cid:101)
(cid:101)
(cid:101)
(cid:101)
2. Note also that an optimal
2 by Proposition 5, it follows that
functions
V (δ) is also in
Gij(δ) is in
(cid:101)
(cid:101)
(cid:101)
policy is given by a sequence of thresholds that deﬁne the continuation and intervention regions. In this
(cid:101)
(cid:98)
(cid:101)
example, we have that

G12(δ) =
(cid:101)

(cid:98)

C

C

Continuation:

Intervention:

A = (δ12, ¯δ12)
A = [0, δ12]

∪

C

I

(δ23, ¯δ23)

(δ34, ¯δ34)

∪
[¯δ12, δ23]

∪
[¯δ23, δ34]

∪

[¯δ34, 1],

∪

where the thresholds δij and ¯δij are deﬁned in Proposition 5.

5 Non-Asymptotic Experimentation Policies

In this section we discuss how to interpret the asymptotic analysis developed in the previous section to
construct experimentation policies that can be used in an arbitrary instance. Recall from Deﬁnition 1
that a policy consists of two components: (a) an intervention region
that deﬁnes the set of beliefs δ at
which the decision maker stops the experimentation process and selects an optimal action a∗ ∈ A∗(δ),
E that identiﬁes the experiment that the DM should conduct
and (b) an experimentation policy π(δ)
c.
=
at each δ in the continuation region

I

∈
I

C

The asymptotic analysis of the previous section produces an experimentation strategy π(δ) =
an intervention region
implement these policies directly since they are computed in terms of the non-primitive quantities
α(x,

A and
A deﬁned in Corollary 1 and Proposition 5, respectively. However, we cannot
),
, θ1) appearing in Assumption 1. Therefore, in order to recover a solution to an

, θ0) and α(x,

(cid:101)
(x,
Q

I

E

E

E

E

22

00.20.40.60.81−101234500.20.40.60.81−1012345eG23( )eG34( )Belief: Belief: eG24( )eG13( )eG14( )eG12( )eR1( )eR2( )eR3( )eR4( )eV( )arbitrary instance of the problem from the asymptotic analysis of the previous section, we need to derive
, θ1) from the primitives of the model, namely, from the values
the values of
of Λ, Q(x,

), α(x,
E
Q
, θ0) and Q(x,

(x,

E

, θ0) and α(x,
E
, θ1).
E

E

In some settings this derivation can be done directly by imposing a speciﬁc parametric structure in the
, θ1). The idea in these settings is that the parametric structure is used
deﬁnitions of Q(x,
to capture a distinctive feature of the problem at hand which in turn would determine the asymptotic
regime of interest. Let us illustrate this point with a concrete example.

, θ0) and Q(x,

E

E

E

Example 3. Consider a setting where the primitives Q(x,
, θ) are known and assumed to be continuously
diﬀerentiable in θ for θ in some open neighborhood that contains the two hypothesis Θ = θ0 and Θ = θ1.
Suppose that we are interested in a setup where a distinctive characteristic of the problem is that the two
hypotheses are hard to distinguish. We can model this feature by setting θ1 = θ0 + ξ/√k for some ﬁxed
, θ0) ξ/√k+
scalar ξ. Using a ﬁrst order Taylor expansion, it follows that Q(x,
o(k1/2), where Qθ(x,
, θ) with respect to θ. Under this speciﬁc
parametrization of the problem, we can now apply the asymptotic analysis of the previous section (by
letting k go to inﬁnity) to derive the corresponding values of
, θ1). In this
case, it is not hard to see that

, θ) is the partial derivative of Q(x,

, θ0) and α(x,

, θ0)+Qθ(x,

, θ1) = Q(x,

), α(x,

(x,

Q

E

E

E

E

E

E

E

E

(x,

Q

E

) = Q(x,

, θ0),

E

α(x,

E

, θ0) = 0

and α(x,

, θ1) = Qθ(x,

E

, θ0) ξ. (cid:51)

E

, θ) are
In Section 6 we consider at length a concrete application related to crowdvoting where Q(x,
viewed as choice probabilities governed by an MNL model. In this context, similarly to Example 3 we
also derive the quantities
, θ) not only for the case of indistinguishable hypotheses but
E
also for the case where the experiments outcomes are very noisy.

) and α(x,

(x,

Q

E

E

The previous example provides some insights on how one can leverage some concrete knowledge about
the structure of the problem to identify the proper asymptotic regime to use. However, this approach
does not generalize in an obvious way to an arbitrary setting for which such knowledge is not available.
In what follows we propose a methodology that does not rely on any additional information beyond the
values of Λ, Q(x,

, θ0) and Q(x,

, θ1).

E

E

Combining the asymptotic scalings in equations (8) and (9), we have that the input parameters Λk,
Qk(x,

, θ1) satisfy the following relationship for k large

, θ0) and Qk(x,

E

E

√Λk

(cid:18)

Qk(x,
E
(x,
E

Q

, θ)
) −

1

≈

(cid:19)

α(x,

, θ).

E

(19)

Furthermore, going back to the conditions that deﬁne the asymptotic regime in Corollary 1, we see that
the value of

) is such that the quantity

(x,

Q

E

Qk(x,
E
(x,
E

Q

, θ)
) −

1

converges to zero at a rate of 1/√k uniformly in
regime, we can reinterpret this condition as one that requires

E ∈

E , x

and θ = θ0, θ1. Thus, in a non-asymptotic
, θ) as possible
(x,

) to be as close as Q(x,

E

∈ E
Q

E

23

E , x

E ∈

for all
problem that minimizes the ‘distance’ between
formulation to compute

∈ E

(x,

)

Q

E

and θ = θ0, θ1. In other words, we can represent this problem as an optimization
, θ). We propose the following min-max

) and Q(x,

(x,

Q

E

For every

E solve:

E ∈

min
0
Q≥

max
θ0,θ1
∈{

θ

}

1

subject to

(x,

Q

E

) = 1.

(20)

(cid:88)x
∈E

After computing the value of
that is,

(x,

Q

E

), we can obtain the values of α(x,

, θ0) and α(x,

E

E

, θ1) using (19),

E

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Q(x,
E
(x,
E

Q

, θ)
) −

max
x

∈E (cid:12)
(cid:12)
(cid:12)
(cid:12)

α(x,

, θ)

E

≈

√Λ

Q(x,
E
(x,
E

, θ)
) −

(cid:18)

1

,

for θ = θ0, θ1.

Q
The following proposition establishes the consistency between the value of the probability kernel
computed in (20) and the corresponding asymptotic limit.

(cid:19)

(21)

(x,

Q

)

E

Proposition 8. Consider a sequence of probability distributions
satisfying the condition in Assumption 1. In particular, Qk(x,
), for θ = θ0, θ1. Moreover, for each k and Qk(x,

E
, θ), let

(x,

E

Q
to (20). Then,

k(x,

Q

E

) converges to

(x,

Q

E

) as k

↑ ∞

E
for all

E , x

, θ);

E ∈

, θ = θ0, θ1}
∈ E
) for some probability kernel
) be the corresponding solution

E

Qk(x,

E
(x,

{
, θ)
→ Q
k(x,
Q
E and x

E

E ∈

.

∈ E

Let us now turn to the issue of how to adapt the asymptotic solutions to derive implementable policies.
Equations (20) and (21) allow us to compute the values of
, θ1) that are
needed to derive the asymptotic strategy π(δ) =

E
A. From Corollary 1, we have that

, θ0) and α(x,

), α(x,

A and

(x,

Q

E

E

E

I

A = argmax

E 


E∈

(cid:88)x
∈X

E (cid:0)

E

(cid:101)

α(x,

(cid:101)

E

, θ1)

α(x,

E

−

2

, θ0)

(cid:1)

(x,

Q

E

.

)



A is obtained from our diﬀusion analysis of the optimal stopping problem
On the other hand, the value of
combining the results in Proposition 5 and Theorem 2. The volatility ˜σ of the underlying diﬀusion process
is the one identiﬁed in Proposition 2, that is,

I





˜σ2 =

(cid:88)x
∈X (cid:101)EA (cid:0)

α(x,

E

A, θ1)

α(x,

E

−

A, θ0)

2

(x,

Q

).

E

(cid:101)

(cid:1)

(cid:101)

We use this asymptotic solution (

A,

E

I

A) to propose two concrete approximation policies.

• Asymptotic Policy (A): This policy implements directly the strategy (
E

(cid:101)

A,

I

A).

• Maximum Volatility Policy (MV): This policy uses the same intervention region

A as the Asymptotic
policy. On the other hand, in terms of experimentation, the MV policy reinterprets the solution
in Corollary 1 and for each δ in the continuation region selects the experiment that maximizes the
MV(δ), where
instantaneous volatility, that is, πMV(δ) =

(cid:101)

I

E

MV(δ) = argmax

E0

E (cid:40)

E∈

E

1
δ + (1

(cid:34) (cid:0)

− L
−

(
E
δ)

2

)

(
(cid:1)
L

E

) (cid:35)(cid:41)

.

(22)

24

(The subscript ‘MV’ is mnemonic for ‘Maximum Volatility’.)

Note that the Asymptotic policy suggests a static experimentation while the Maximum volatility oﬀers
a dynamic experimentation, function of the current belief. However, it should be clear from our previous
discussion that both of these policies are asymptotically equal and optimal in the limiting regime deﬁned
It is also worth noticing that in contrast to the derivation of an optimal
by equations (8) and (9).
experimentation policy in equation (6) that requires full knowledge of the value function, the MV policy
can be computed directly using only the knowledge of the likelihood function
). This, of course,
simpliﬁes signiﬁcantly its computational complexity.

(x,

L

E

In Section 7, we conduct a set of numerical experiments to test the performance of our proposed policies
using a concrete application in the context of new product introduction that we present in the next section.
We conclude this section with a remark on how to extend some of the insights that have developed to
the problem of designing the type of experiments that the DM can use.

5.1 A Remark on the Optimal Design of Experiments

In some applications (such as the assortment selection problem that will be discussed in the next section),
the decision maker has some degree of control over the design of the set E of available experiments. In
MV(δ) can be reformulated over a more
such cases, observe that the optimization in (22) that deﬁnes
abstract set L of likelihood ratios, where each
. As a result, the
optimization problem that deﬁnes the Maximum Volatility policy is given by,

L corresponds to an experiment

L ∈

E

E

Eδ

max
L
L∈

(cid:34)(cid:18)

1

δ + (1

− L
−

δ)

2

(cid:35)

L (cid:19)

,

or equivalently,

E0

max
L
L∈

(1
δ + (1

− L
−

)2
δ)

(cid:20)

L (cid:21)

where, E0[

] denotes the expectation under the probability measure P0(
·

).
·

Depending on the nature of the set L, the optimization problem above can be casted as a Tchebycheﬀ
moment problem. Consider the following setting where the DM can design experiments that correspond
to any possible likelihood ratio
is bounded by two given quantities L and L. In this case
the following result holds:

, as long as

L

L

Proposition 9. Suppose that L =
L, and let

L

(cid:8)

: E0[

L

] = 1 and L

≤ L ≤

L

for two non-negative scalars L and

∗ = argmax

L

L

L∈

E0

(1
δ + (1

− L
−

)2
δ)

(cid:20)

(cid:9)
.

L (cid:21)

Then,

L∗ is a random variable with a two-point distribution with mass at L and L.

Proof: The result follows from noticing that the function (1
solution is a two-point distribution with mass at L and L. (cid:50)

−

(cid:96))2/(δ + (1

−

δ)(cid:96)) is convex and so an optimal

The solution in Proposition 9 suggests that the decision maker should select an experimentation policy
that maximizes the range of the likelihood function. In Section 7, we explore this idea and propose a
variation of the Maximum Volatility policy that incorporates this ‘maximum range’ condition and show
very good numerical performance.

25

6 Illustrative Example: New Product Introduction

We discuss in this section a concrete application of the methodology and results presented in the previous
sections in the context of a new product introduction problem. The literature on the topic is quite
broad (see, the recent work of Sunar et al. (2019) and references their). In particular, we consider an
environment in which the experimentation outcomes are the result of a consumers’ voting process driven
by a Multinomial choice model (MNL). Our objective in developing this example is twofold. First, we use
it to provide some speciﬁc details on how to formulate and derive our proposed asymptotic approximation
policies discussed in the previous section. As a by-product of this discussion, we also show how to obtain
diﬀusion approximations for a belief process that is governed by an MNL model using two diﬀerent types
of asymptotic regimes. Given the popularity of the MNL model to represent consumer preferences, we
believe that our diﬀusion approximation has applications beyond the one discussed in this section. Our
second objective is to use this concrete example in Section 7 to conduct a set of numerical experiments
to test the quality of our proposed methodology. For instance, we are interested in testing the accuracy
of the maximum volatility principle derived in Corollary 1 and the two heuristic policies introduced in
Section 5, which provide remarkably simple rules for conducting dynamic experimentation.

6.1 Model Setup

The speciﬁc setting that we consider is as follows. Consider a seller (or ﬁrm) who is contemplating the
possibility of introducing a new product (or products) into the marketplace. In the process of developing
these new products, the seller has prototyped n diﬀerent versions and would like to decide which is the
right subset to commercialize, if any. These prototypes diﬀer in terms of some speciﬁc set of attributes
which might include their price and quality as well as launching and manufacturing costs, to name a few.
[n] is equal to ui(Θ), where
We assume that the intrinsic utility that a consumer assigns to version i
Θ > 0 is some unknown real parameter.

∈

Example 4. (Linear Utilities) A popular modeling approach is to assume that the utilities ui(Θ) are
linear in the unknown parameter Θ. For instance, we can have ui(Θ) = qi −
pi Θ, where qi and pi are
product i’s quality and price, respectively. In this case Θ measures consumers’ price sensitivity. (cid:51)

The seller is uncertain about market conditions and does not know the value of the parameter Θ. In an
attempt to reduce the risk of launching the wrong version(s), the seller sets up an online voting system
in which potential customers (those visiting the seller’s website) can vote for the diﬀerent prototypes.
For simplicity, we assume that each voter votes for at most one version and the seller only tracks the
cumulative number of votes for each one. (In practice, we could imagine a more sophisticated interface
using a more detailed scoring system, e.g., a 0 to 10 scale, or even allowing for consumer reviews.) This
voting phase occurs before the seller decides to launch a product and has the potential of oﬀering a
win-win situation whereby a consumer who votes hopes to inﬂuence the seller to commercialize the right
version; and on the other hand, these votes and their pace provide valuable information that the seller
can use to better forecast the value of Θ. As we show later, it is not necessarily optimal for the seller to
display the entire set [n] during the voting phase. Hence, we assume that the seller selects a subset
of
be its cardinality. To
prototypes to show during the voting phase. We call

the display set and let

E

|E|

E

26

keep some consistency between the notation in this and the previous sections, we note that, in the most
general case, both the set of experiments E and available actions A coincide with the power set of [n],
that is, E = A = 2[n]. In some cases, however, one might need to restrict the set of experiments and
actions. For instance, if the number of prototypes is large then it might be impractical to display the
entire menu and experimentation should be restricted to display sets of a given cardinality. Similarly, it
is also possible that the seller is constrained in the number of versions that she can launch.

Voters arrive according to a Poisson process with rate Λ and vote for one alternative from the display
assigns to
set according to a multinomial choice model. Speciﬁcally, a voter who observes a display set
each version i
are idiosyncratic utility shocks that are
independent and identically distributed according to a Gumbel distribution with mean zero and variance
Var[ε] = π2/(6 µ2), for some ﬁxed constant µ > 0. It follows that a utility-maximizing voter votes for
version i

Ui(Θ) = ui(Θ) + εi, where

with probability

a utility

∈ E}

εi : i

∈ E

E

{

∈ E

, Θ) := P

Q(i,

E

Ui(Θ)

≥ Uj(Θ),

j
∀

∈ E

=

(cid:0)

(cid:1)

(cid:80)

exp(µ ui(Θ))

exp(µ uj(Θ))

j

∈E

.

(23)

Note that our formulation allows for the possibility that a voter might end-up not selecting any of the
available options. To model this no-vote option we simply include version ‘0’ with quality, price and
intrinsic utility equal to zero, u0(Θ) = 0. In what follows we assume that every display set
includes
the non-purchase option.

E

We assume that the seller has a prior belief about the value of Θ that can take one of two possible values
(0, 1). We let ui(θ0) and ui(θ1) denote voters’
θ0, θ1}
{
intrinsic utilities under these two hypotheses for i

, and her prior is that Θ = θ0 with probability δ

[n] and deﬁne the likelihood ratio function by

∈

∈

(i,

L

E

) :=

Q(i,
Q(i,

, θ1)
, θ0)

E
E

i
∀

.

∈ E

(24)

We complete the description of the model by specifying the seller’s objective function. As in the general
case, we assume that there exists a piecewise linear function G(δ) (see equation (1)) that represents the
seller’s expected payoﬀ as function of her belief δ. The seller’s optimization problem is given by

Π(δ) = sup
)

(π,

I

Eδ

e−

r δτ G(δτ )
(cid:105)

(cid:104)

,

subject to τ = inf

t > 0 : δt ∈ I

.

(cid:8)

(cid:9)

(25)

Recall that a policy is deﬁned by an experimentation policy π that determines the collection of display
sets
that deﬁnes
t
the duration of the voting campaign.

to use throughout the voting process and an intervention region

{Et ∈

E : 0

≤

≤

I

}

τ

Remark 2. (Payoﬀs from Sales) To illustrate a concrete example of a piecewise linear payoﬀ function G(δ)
in the context of new product introduction, consider the case in which the seller is interested in maximizing
the expected discounted value of the cash-ﬂows generated by the sales that occur after time τ . Speciﬁcally,
A of products to launch based on
at time τ , the seller stops the voting process and selects a subset
the available information at this time. Suppose consumers arrive according to a Poisson process of rate Λs
and make buying decision according to the same MNL model that governs the voting process. Under this

A ∈

27

assumption, the seller expected discounted payoﬀ is given by

(δτ ,

R

) := E

A

(cid:34)

∞

e−

r (t

−

τ )(pi −

τ
∈A (cid:90)
(cid:88)i
) + β(

= φ(

A

) δτ ,

A

ci) dSit −

Ki

=

(cid:35)

Fτ
(cid:12)
(cid:12)
(cid:12)

∈A (cid:20)
(cid:88)i

ci)

(pi −
r

Λs E

Q(i,

(cid:104)

Fτ

−

(cid:105)

Ki

(cid:21)

A

, Θ)

(cid:12)
(cid:12)
(cid:12)

where pi, ci and Ki are the per-unit price, manufacturing cost and ﬁxed launching cost of product i
respectively, and

∈

S ,

ci)

(pi −
r

A

A

φ(

Ki

) :=

, θ1)

∈A (cid:20)
(cid:88)i

Λs Q(i,

, θ1))
(cid:21)
In the case that all products are discarded, one can assume the seller receives a ﬁxed payoﬀ
R0 (possibly zero)
which captures the opportunity cost of her business. Finally, the seller’s payoﬀ function in this case is given
by G(δ) = max

Λs (Q(i,

∈A (cid:20)
(cid:88)i

, θ0)

. (cid:51)

Q(i,

) :=

and β(

(δ,

A

A

A

A

−

−

) :

(cid:21)

.

ci)

(pi −
r

R

A

A ∈

(cid:110)

(cid:111)

6.2 Asymptotic Approximation

We move now to apply the results in Section 4 to approximate the optimization in (25) by a diﬀusion
control problem. In order to invoke the weak convergence result in Proposition 2, we need to specify an
asymptotic regime under which the MNL choice probabilities satisfy the condition in equation (8). In
what follows we propose two concrete alternatives, each capturing a diﬀerent type of uninformativeness
associated with the voting process.

6.2.1 Noisy Preferences

Motivated by the issue of low-quality data that has been reported in the context of online learning
applications and advertising (Kohavi and Thomke, 2017, Lewis and Rao, 2015), we consider a regime in
which the variance of the MNL idiosyncratic shocks in the kth instance of the problem grows proportionally
with k, namely, Var[εk] = k π2/(6 µ2). In other words, this asymptotic regime is one in which votes –and
the information they contain– become more and more noisy as k grows large.
Under this scaling, one can show that the choice probability Qk(i,

, θ) in (23) can be written as

E

Qk(i,

, θ) =

E

1

|E|

1 +





µ
√k

|E|

(ui(θ)

−

(cid:88)j
∈E

which satisﬁes the requirements in Assumption 1 with

uj(θ)) + o(k−

1/2)





,

(26)

(i,

Q

E

) =

1

|E|

and

α(i,

E

, θ) = µ

ui(θ)

¯u(

E

−

, θ)

where

, θ) :=

¯u(

E

(cid:0)

(cid:1)

1

|E| (cid:88)j
∈E

uj(θ).

Recall that under our asymptotic scaling, voters arrive according to a Poisson process N k
t with intensity
Λk = k Λ in the kth instance of the problem. Given this scaling of Var[εk] and Λk, we can use the result
in Proposition 2 to obtain the following corollary.

28

Corollary 3. Let ∆ui := ui(θ1)
static display policy
solution of the SDE

Et =

E

−

ui(θ0) and ∆¯u(

) := ¯u(

, θ1)

E

E

during the voting process. Then, the belief process δk

¯u(

E

−

, θ0). Suppose the seller uses a
t converges weakly to the

d ˜δt = ˜σ(
E

) ˜δt (1

−

˜δt) dWt,

where

˜σ2(
E

) =

Λ µ2

|E| (cid:88)i
∈E (cid:0)

∆ ui −

∆¯u(

E

2,

)

(cid:1)

and Wt is a Wiener process.

Combining this result together with the maximum volatility principle in Corollary 1 we can now identify
an optimal display set in this asymptotic regime under consideration, namely

(The subscript ‘NP’ stands for Noisy Preferences regime.)

A

NP = argmax
E

E (cid:40)

E∈

(cid:101)

1

|E| (cid:88)i
∈E (cid:0)

∆ ui −

∆¯u(

E

.

(cid:41)

2

)

(cid:1)

Without loss of generality, let us index the prototypes in ascending order of ∆u so that ∆u1 ≤
n, let us deﬁne the display set
· · · ≤

∆un. Also, for 0

≤

≤

≤

j

i

[i, j] :=

0

{

} ∪ {

1, . . . , i

j, . . . , n
}

,

} ∪ {

E

(27)

∆u2 ≤

(28)

which includes the non-purchase option together with the ﬁrst i prototypes with the lowest values of ∆u
and the n

j + 1 prototypes with the highest values of ∆u.

−

A

A

Proposition 10. Let
such that
∆u1 ≥
the non-purchase option ‘0’, that is,

NP =
E
E
0 or ∆un ≤

(cid:101)
0) then

(cid:101)

E

A

NP be a solution to (27), then there exist integers n1 and n2 with 0

[n1, n2]. Furthermore, in the special case that all the

≤

n1 < n2 ≤

n
have the same sign (i.e.,
together with

S

: i

∈

}

{

∆ui}
∆ui|
{|

NP consists of a single prototype i∗ = argmax
E

A

NP =

E

.
0, i∗}

{

(cid:101)

An important corollary of Proposition 10 is that instead of solving (27) over the power set of E we can
restrict ourselves to the much simpler problem of maximizing the volatility of the belief process over the
signiﬁcantly smaller class of display sets

which has a cardinality of O(n2).

[i, j] : 0

n

(cid:101)

j

i

E

(cid:8)

≤

≤

≤

(cid:9)

Example 5. (Example 4 Revisited) Suppose the intrinsic utility of product i is equal to ui(Θ) = qi −
θ1). If all the
then ∆ui = pi (θ0 −
of the products, then the
∆ui}
{
prototype, namely, the one with the highest price. (cid:51)

pi Θ,
are of the same sign, for example, if they correspond to the prices
NP includes a single

are also of the same sign and the optimal display set

pi}

E

{

A

6.2.2 Asymptotically Indistinguishable Hypotheses

(cid:101)

An alternative regime in which we can apply the asymptotic analysis of Section 4 corresponds to the case
in which the values of θ0 and θ1 become indistinguishable as k grows large. To be precise, let us consider
the case in which ui(θ1) = ui(θ0)+ξi/√k for i
are ﬁxed constants independent
[n], where

∈

ξ1, ξ2, . . . , ξn}

{

29

of k. Under this scaling, the choice probability Qk(i,

, θ) in (23) admit the following representation:

E

Qk(i,

E

, θ0) =

νi

j

∈E

νj

and Qk(i,

, θ1) =

E

νi

j

∈E

1 +

νj (cid:34)

j

1
√k (cid:80)

∈E

νj (ξi −
νj

j

∈E

ξj)

+ o(k−

1/2)

(29)

,

(cid:35)

(cid:80)

(cid:80)

(cid:80)

where νi := exp(µ ui(θ0)). It follows that these choice probabilities satisfy the conditions in Assumption 1
with

(i,

Q

E

) =

νi

j

∈E

(cid:80)

,

νj

α(i,

E

, θ0) = 0

and

α(i,

E

, θ1) =

(ξi −

(cid:88)j
∈E

ξj)

(j,

Q

).

E

From Corollary 1 the optimal display set in this asymptotic regime is given by

(The subscript ‘IH’ stands for Indistinguishable Hypotheses regime.)

A

IH = argmax
E

E (cid:40)

E∈

(cid:88)i
∈E (cid:16)

(cid:101)

α(i,

E

2

, θ1)

(cid:17)

(i,

Q

.

)
(cid:41)

E

(30)

To get some intuition about
variables taking values in
assume that

E
ξ1, ξ2, . . . , ξn}
{
(cid:101)
) = 0 if i
(cid:54)∈ E

(i,

Q

E

A

IH, consider an arbitrary display set

with probability distribution

.) Then, (30) can be rewritten as

E ∈

E and let ξ(
E
).
(i,

) be a random
(In this deﬁnition we

Q

E

A

IH = argmax
E

E

E∈

Var[ξ(
E

(cid:110)

)]

.

(cid:111)

(cid:101)

Remark 3. It is worth noticing that the asymptotic regime in which the two alternative hypotheses Θ = θ0
and Θ = θ1 are asymptotically indistinguishable does not imply that the DM optimization problem becomes
trivial in the limit. To see this, let us consider the payoﬀ structure discussed in Remark 2, where
) is
when her belief is δ. Under
the discounted payoﬀ that the DM expects to collect if she launches assortment
the scaling in (29) it is not hard to show that for the kth instance

(δ,

R

A

A

k(1,

R

)

A

− R

k(0,

) =

A

ci)

(pi −
r

Λk
s
√k

∈A (cid:20)
(cid:88)i

α(i,

, θ1)

A

(i,

)
A

Q

(cid:21)

+ o(k−

1/2),

s is the selling rate after launching. Thus, depending on the rate of grow of Λk

where Λk
s with k there is a
non-negligible diﬀerence in payoﬀs between the two hypotheses and so it is in the DM best interest to try to
learn which one holds true.† (cid:51)

7 Numerical Experiments

In this section, we conduct a set of numerical experiments to assess the quality of our methodology using
the application discussed in the previous section. In particular, we are interested in investigating the
performance of our proposed Asymptotic and Maximum Volatility policies introduced in Section 5.

†During the voting phase we have assumed that the arrival rate of voters Λk is O(k) but during the selling phase the
k). In this case, the diﬀerent payoﬀs between

s does not need to be of the same order and could drop to O(

√

arrival rate Λk
the two hypotheses would still be signiﬁcant.

30

Optimality Gap: In our ﬁrst set of computational experiments, we numerically evaluate the optimality
gap of the Asymptotic and Maximum Volatility policies with respect to an optimal policy using the Noisy
Preferences model in Section 6.2.1. We let ΠA(δ), ΠMV(δ) and Π(δ) denote the value functions generated
by the A, MV and optimal policy, respectively, and deﬁne the optimality gap of these policies by

∆Πj := max
(0,1)

δ

∈

(cid:26)

Π(δ)

Πj(δ)

−
Π(δ)

j = A, MV.

,

(cid:27)

∈

We measure ∆ΠA and ∆ΠMV using a set of 500 random instances of the problem. Speciﬁcally, we
consider a problem with n = 5 products, whose intrinsic utilities ui(θ0) and ui(θ1) are randomly generated
uniformly in [0, 1] for all i
[n]. For each random instance we run ﬁve diﬀerent scenarios in which Λ = k
and Var[ε] = k π2/(6 µ2), with k = 10κ for κ = 0, 1, 2, 3, 4. The rest of the parameters are kept ﬁxed with
. This is the
µ = 1, r = 0.05 and the terminal payoﬀ function G(δ) = max
{
same terminal payoﬀ function that we used in the examples in Figures 2 and 4. Finally, in these and the
rest of our numerical computations we evaluate the value function of a given policy using Gauss-Seidel
3 over a mesh of size
value iteration (see section 6.3 in Puterman, 2005) with an error tolerance of 10−
10−

3 for the [0, 1] interval that deﬁnes the domain of δ.

20 + 25 δ

5 δ, 3 δ,

30 δ, 4

−

−

−

}

6

Table 2 presents the mean optimality gap –as well as the maximum value and standard deviation–
computed over a run of 500 randomly generated instances. As we can see from the table, the two policies

Optimality Gap: ∆ΠA

Mean
Max
St. Dev.

k = 1
2.39%
26.19%
4.88%

k = 10
1.77%
22.95%
4.18%

k = 100
0.87%
13.67%
1.55%

k = 1, 000
0.27%
1.99%
0.39%

k = 10, 000
0.13%
0.87%
0.13%

Optimality Gap: ∆ΠMV
k = 100
0.26%
Mean
3.56%
Max
St. Dev.
0.41%
Data: µ = 1, r = 0.05, G(δ) = max{6 − 30 δ, 4 − 5 δ, 3 δ, −20 + 25 δ} and Λ = 2k, Var[ε] = k π2/(6 µ2).

k = 10, 000
0.09%
0.43%
0.06%

k = 1, 000
0.15%
1.87%
0.22%

k = 10
0.12%
1.47%
0.22%

k = 1
0.56%
4.09%
0.86%

Table 2: Optimality gap of the Asymptotic and Maximum Volatility policies.

performs very well on average, although, the MV policy is substantially better than the A policy, especially
for small value of k. As k grow large both policies approach the optimal policy, which is consistent with
our asymptotic analysis in Section 4. By comparing the ‘Max’ rows that report the maximum optimality
gap, we can also see that the MV policy is signiﬁcantly more robust than the A policy for small values
of k.

Running Times: Another dimension of performance is the computational time required to compute
a policy and its corresponding value function. Table 3 shows the average running time (in seconds) of
the optimal, Asymptotic and Maximum Volatility policies, as a function of the number of products n

31

available in the menu of prototypes. As we can see, the time required to compute an optimal solution
grows exponentially fast with the number of products while the time needed to compute the Asymptotic
or Maximum Volatility solution remains low across the range of values of n considered in Table 3.

Average Running Time (in seconds)

Optimal
A
MV

n = 3
0.90
0.10
0.11

n = 6
19.00
0.19
0.22

n = 9
208.60
0.22
0.24

n = 12

n = 15

102

22.20

×
0.22
0.34

103

24.90

×
0.27
0.97

Data: µ = 1, r = 0.05, G(δ) = max{6 − 30 δ, 4 − 5 δ, 3 δ, −20 + 25 δ} and Λ = 2, Var[ε] = π2/(6 µ2).

Table 3: Running times of the optimal, Asymptotic and Maximum Volatility policies.

The results in Tables 2 and 3 lead us to conclude that the MV heuristic dominates the A heuristic as
it has consistently better optimality gap and comparable running times. For this reason, in the rest of
our numerical experiments we will focus exclusively on further exploring the performance the Maximum
Volatility policy.

Benchmark Analysis: We next conduct a benchmark analysis in which we compare the performance
of the MV policy against the following three alternative policies:

• Full Display (F): This policy always displays the entire set of prototypes, that is,

Full Display Policy:

F(δ) :=

E

0, 1, 2, . . . , n
{

}

(31)

This is a simple and popular benchmark that does not require any type of optimization.

• One-Step-Look-Ahead Policy Approximation (LA): This is a commonly used value function approxi-
mation, which in our setting corresponds to selecting an optimal experiment to display under the
assumption that a ﬁnal decision must be made after the outcome of this experiment is revealed.
That is,

One-Step-Look-Ahead Policy:

LA(δ)

E

∈

argmax
E

E∈

Eδ
(cid:110)

(cid:104)

G

δ + η(δ, x,

(cid:0)

E

)
.
(cid:1)(cid:105)(cid:111)

(32)

• Maximum Range Policy (MR): Motivated by the result in Proposition 10, we consider the policy

Maximum Range Policy:

E

MR(δ) := argmax
n

0

i

j
≤

≤

≤

E0

1
δ + (1

(cid:34) (cid:0)

− L
−

(
E
δ)

[i, j])
(
E
L

[i, j]) (cid:35)
(cid:1)

2

.

(33)

A key advantage of the MR policy over the Maximum Violate (MV) policy is that MR maximizes
the instantaneous volatility of the smaller set of experiments
[i, j] deﬁned in (28), which simpliﬁes
its computation.

E

We assess the performance of these three policies relative to the Maximum Volatility policy using the
following relative error measure:

32

Relative Error:

¯∆Πj =

1

0
(cid:90)

ΠMV(δ)

Πj(δ)

−
ΠMV(δ)

dδ,

j = F, LA, MR.

Figure 5 shows the distribution of this relative error measure for 1000 randomly generated instances of
the problem with n = 10 products each.

Figure
randomly
Range
Data: µ = 1, r = 0.05, G(δ) = max{6 − 30 δ, 4 − 5 δ, 3 δ, −20 + 25 δ} and Λ = 2, Var[ε] = π2/(6 µ2).

(F), One-Step-Look-Ahead

the Maximum Volatility

5:
(MR)

Distribution

policies

relative

relative

policy

1,000

error

over

Full

the

to

of

of

(LA)

and Maximum
instances.

generated

As we can see form the ﬁgure, the MV policy substantially outperforms the LA and F policies, which
have an average relative error of 10.34% and 15.01%, respectively. On the other hand, the MR policy is
essentially equivalent to the MV policy with an average relative error of 0.001%. A similar conclusion
holds when we compare the average running times of these policies. Indeed, the average running time
per instance are equal to 0.292, 0.224, 0.520 and 16.184 for the MV, MR, F and LA policies, respectively
(all times in seconds).

Value of Optimal Stopping: We continue our numerical experiments investigating the option value
that the DM has by being able to stop the experimentation process at an arbitrary time. Our interest
in measuring the value of optimal stopping is driven by the fact that most practical implementations of
crowdvoting are executed with a ﬁx, predetermined, time horizon and so we are interested in measuring
the opportunity costs of these implementations.

To this end, let us compare the expected payoﬀs that the DM collects if she uses the Maximum Volatility
MV(δ) in equation (22) with and without optimal stopping. For the case with
experimentation policy
optimal stopping, this expected payoﬀ is ΠMV as deﬁned above. For the case without optimal stopping,
we assume that the DM has a ﬁxed predetermined “budget of experimentation” of T votes that she can
collect. In practice, this budget might reﬂect external constraints on the amount of time or monetary
resources available to experiment. Within this budget of experimentation we assume the DM implements

E

33

-0.0500.050.10.150.20.25Relative Error01002003004005006007008009001000LA: Avg. Relative Error = 10.34% F : Avg. Relative Error = 15.01%MR: Avg. Relative Error = 0.001%the maximum volatility policy
of optimal stopping by

E

MV(δ). We denote by ΠMV

T

the corresponding payoﬀ. We deﬁne the value

Value of Optimal Stopping: max
[0,1]

δ

∈

(cid:26)

ΠMV

ΠMV
T

−
ΠMV

.

(cid:27)

Figure 6 illustrates the average value of optimal stopping in a concrete instance of the problem with
n = 5 products for 100 randomly generated instances in which consumers’ utilities u0(i) and u1(i) are
[n]. Panel (a) depicts the average value of optimal stopping when
uniformly distributed in [0,1] for i
experimentation is constrained to last exactly T rounds. On the other hand, panel (b) depicts the average
value of optimal stopping when the experimentation is constrained to be at most T rounds, that is, in
this case the DM is able to stop experimenting before collecting T votes. Also, for comparison purposes,
Figure 6 includes the average value of optimal stopping when the full display rule

F in (31) is used.

∈

E

Figure 6: Value of Optimal Stopping as a function of the number of votes T for the maximum volatility and full display strategies.
Data: µ = 1, r = 0.05, G(δ) = max{6 − 30 δ, 4 − 5 δ, 3 δ, −20 + 25 δ}, Λ = 2, Var[ε] = π2/(6 µ2).

As we can see from the ﬁgure, the value of optimal stopping can be quite signiﬁcant depending on the
value of T . This is specially clear on panel (a) in which the value of optimal stopping can be as large
as 20% or more if the number of votes T is too small or too large. Intuitively, when T is too small the
DM is not able to collect enough information and ends up making wrong decisions. On the other hand,
when T is too large and the DM exhausts the experimentation budget, then she is guaranteed to collect
a large amount of information but pays the price of delaying a ﬁnal decision too much, which again has
a negative eﬀect on payoﬀs because of discounting, i.e., the DM collects more information that needed.
For panel (b), as expected, the value of optimal stopping decreases monotonically with T as the DM in
this case is not forced to exhaust all her experimentation budget.

In this example, the average value of optimal stopping under a maximum volatility experimentation rule
is minimized around T = 40 when the DM operates under the constraint of collecting exactly T votes

34

050100150T00.050.10.150.20.250.3Value of Optimal Stopping (%)(a) Number of votes = T050100150200250T00.050.10.150.20.250.3Value of Optimal Stopping (%)(b) Number of votes  TMaximum VolatilityMaximum VolatilityFull DisplayFull Display(panel a) and is about 15%. In contrast, if a Full display policy is used under the same constraint the
value of optimal stopping is signiﬁcantly higher, achieving a minimum around 18% when T = 45 votes.
By comparing panels (a) and (b) we can appreciate the option value of optimal stopping; not only the
value of optimal stopping is monotonically decreasing in T on panel (b) but is also signiﬁcantly smaller
compared to panel (a). These results underscore the signiﬁcance of giving the DM the option to stop at
MV(δ) instead
any time as well as the beneﬁts –from a learning perspective– of using maximum volatility
of the popular full display strategy.

E

Comparison to MNL Bandit Algorithms: We conclude our numerical experiment by comparing our
proposed Maximum Volatility policy to a couple of policies from the growing literature on multi-armed
bandit problems. We speciﬁcally selected one MNL-bandit algorithm and one best arm identiﬁcation
algorithm. The reason for selecting algorithms from this part of the broad literature on sequential testing
is that we can cast our assortment selection model in Section 6 as a multi-armed bandit, where each
assortment can be viewed as an arm and where at each arrival the DM has to pull one of them to
experiment with. Moreover, the growing literature on bandit problems and speciﬁcally MNL-bandit
setups have considered assortment planning as one of their primary and most natural application (see,
e.g. Caro and Gallien (2007) and Agrawal et al. (2019)). Many of the algorithms in this literature have
been developed with the objective of minimizing the DM regret over a ﬁnite time horizon. This setting
is diﬀerent than ours in the sense that our objective is to identify, as quick as possible, the best possible
“arm” (action) to choose. However, we can still adapt our proposed Maximum Volatility methodology to
this minimum regret setting. This shouldn’t be of great concern given that our approach is obtained for
a general reward function and as discussed our experimentation policy is independent of the duration.

To this end, we assume that the DM has a non-informative uniform prior (i.e., δ = 0.5) and uses the
MV policy for a ﬁxed number of votes T . Using a slight abuse of notation, let us denote by δMV
the
DM’s posterior belief after this voting period has ended and let aMV
T ) be the optimal action she
chooses. For simplicity, we will consider the case in which the optimal action sets
A∗(δ) are restricted to
include a single product, in other words, the DM wants to launch a single product into the marketplace.
T , Θ) be the reward associated with this policy as a function of Θ. On the other hand, we
We let
deﬁne
to be the optimal reward of a clairvoyant who knows the true value
of Θ. The terminal regret under this modiﬁed MV policy is given by ∆

R
R∗(Θ) = maxa

T ∈ A∗(δMV

MV :=

(a, Θ)

(aMV

(aMV

{R

}

A

∈

T

T , Θ).

R

R∗(Θ)

− R

The following are the two alternative algorithms that we use for comparison:

• MNL-Bandit. The ﬁrst algorithm that we consider is the one proposed by Agrawal et al. (2019)
(Algorithm 1). This is a ‘general purpose’ algorithm that makes no prior assumption on the MNL
model, except for requiring that the no purchase option is the most frequent choice. The algorithm
is also designed with the objective of minimizing the rate at which cumulative regret grows as
a function of the number of votes T rather than the terminal regret at T , so the comparison is
not ideal. The MNL-Bandit is a UCB-type algorithm that periodically during the voting process
[n] and uses
estimates upper bounds on the attraction scores vi = exp(µ ui) of each product i
these upper bounds to display the assortment that maximizes rewards. In the implementation of the
MNL-Bandit algorithm, we initialize the value of the attraction scores to one. We let viT denote the

∈

35

terminal estimate of the attraction score for product i after T votes. Using these terminal scores,
we deﬁne aMNL-B
to be the action (product) that maximizes the DM expected reward. The terminal
MNL-B :=
regret of thr MNL-Bandit algorithm is given by ∆

, Θ).

T

(aMNL-B
T

R

R∗(Θ)

− R

• Top-Two Probability Sampling (TTPS). This algorithm is a variation of a recently proposed
algorithm by Russo (2020) for best arm identiﬁcation. Like MV, TTPS is a Bayesian algorithm
that updates the belief δ after each vote. The key diﬀerence is in the experiment that is used at
every voting epoch. For each value of δ, TTPS identiﬁes the best and second best experiments,
in terms of the reward they generate, and selects one of them at random with probabilities β and
β, respectively, where β is a tuning parameter. In our simulations we use β = 0.5, which is
1
the default value used by Russo (2020). We let δTTPS
denote the posterior belief produced by the
∈ A∗(δTTPS
TTPS algorithm after T votes and let aTTPS
) be the corresponding optimal action. The
R∗(Θ)
terminal regret of TTPS is equal to ∆

T
TTPS :=

(aTTPS
T

, Θ).

− R

R

−

T

T

TTPS.
In our numerical experiments we use simulation to evaluate the values of ∆
Figure 7 depicts the average terminal regret of these three policies for values of T ranging from 100 to
one million votes for a speciﬁc instance with n = 5 products. The attraction scores vi(θ) = exp(µ ui(θ))
ci for each of the ﬁve products is reported in Table 4.
and per unit margin pi −

MNL-B and ∆

MV, ∆

R

R

R

Product
vi(θ0)
vi(θ1)
ci
pi −

1
0.05
0.032
210

2
0.08
0.07
121.5

3
0.012
0.018
506

4
0.05
0.12
42

5
0.04
0.043
208

Table 4: Vectors of attraction scores and margins for the instance used in the computational experiments reported in Figure 7.

For each algorithm and value of T , we run 1,000 simulations to compute the average terminal regret.
Figure 7 also depicts the 95% conﬁdence interval of the mean terminal regret (error bars).
As we can see form the ﬁgure, the MV Policy outperforms the other two in terms of achieving a lower
1, 000, the MV policy has essentially zero
terminal regret with signiﬁcantly fewer votes. Indeed, for T
terminal regret. On the other hand, the TTPS policy needs T
100, 000 to achieve a zero terminal
regret. Finally, the MNL-Bandit algorithm does not produce a zero terminal regret for any value of T .

≥

≥

We note that we need to read the results in Figure 7 with cautious. The fact that the MNL-Bandit
algorithm does not perform well in terms of minimizing terminal regret should not be surprising as this
policy is not designed for this purpose but rather to minimize cumulative regret. To provide a complete
picture of the performance of these policies, we have also run a set of experiments to measure their
cumulative regret as a function of T .
Figure 8 depicts the average (per vote) cumulative regret of the three policies. As we can see, only the
MNL Bandit algorithm achieves a sublinear regret in T while both MV and TTPS have linear cumulative
regret. Again, this should be expected since MV and TTPS are pure learning policies designed to identify
as quickly as possible the best assortment.

36

Figure 7: Average terminal regret for the MV, TTPS and MNL-Bandit algorithms as a function of the number of votes T . For
each value of T , the average is calculated over 1,000 simulations. The error bars indicate the 95% conﬁdence interval for the
mean.

Figure 8: Average cumulative regret for the MV, TTPS and MNL-Bandit algorithms as a function of the number of votes T .
For each value of T , the average is calculated over 1,000 simulations. The error bars indicate the 95% conﬁdence interval for the
mean.

8 Conclusion

We considered in this paper a DM that must select an action to maximize a reward function. This function
is parameterized by an unknown quantity that the DM learns by experimenting. The DM has to decide
dynamically which experiment to conduct and when to stop the experimentation in order to generate the
discounted reward. We formulate this problem as a sequential Bayesian testing problem with dynamic

37

MV Policy1001,00010,000100,0001,000,000Number of Votes: T00.050.10.150.20.250.30.35Terminal Regret: RTTPS Policy1001,00010,000100,0001,000,000Number of Votes: T00.20.40.60.811.2MNL-Bandit Policy1001,00010,000100,0001,000,000Number of Votes: T01234567MV Policy1001,00010,000100,0001,000,000Number of Votes: T0510152025Cumulative Regret: RTTPS Policy1001,00010,000100,0001,000,000Number of Votes: T0510152025MNL-Bandit Policy1001,00010,000100,0001,000,000Number of Votes: T0510152025experimentation. We adopt a novel diﬀusion-asymptotic analysis technique that relies on scaling two
parameters of the problem. We do that by speeding up the frequency at which experiments are conducted
while simultaneously reducing the informativeness of the outcome of each individual experiment. As a
result, we derive a diﬀusion approximation for the underlying sequential testing problem. The beneﬁt of
such scaling is that it allows the limiting regime to remain comparable to the initial setting in terms of the
informativeness of the experimentation process per unit of time. Therefore, one expects the corresponding
asymptotic approximation to be more accurate than ones from other regimes. Interestingly, this high
frequency vs. low informativeness regime has also its own merits and depicts many practical situations.
It is speciﬁcally in line with online experimentation where the velocity of data (high frequency) is always
contrasted with its veracity and lack of accuracy (low informativeness). The diﬀusion model we obtain
provides a number of important insights with respect to the nature of the problem and its solution.
In particular, it shows that an optimal experimentation policy is one that chooses the experiment that
maximizes the instantaneous volatility of the belief process. This maximum volatility principle reduces
dramatically the complexity of the problem and its solution. On the implementation side, we suggest a
universal approach to interpret the diﬀusion approximation solution and “unscale” it in order to derive
a heuristic for the original problem i.e., in the non-asymptotic regime.

The model that we have studied in this paper is very general and can accommodate a number of appli-
cations. However, to test our solution, we consider in Section 6 a speciﬁc setting on assortment selection
in the context of new product introduction where the experimentation follows a crowdvoting setup. As a
by-product of this analysis, we derive a diﬀusion limit for the belief process updated following consumers
votes, themselves governed by an MNL model. Given the popularity of the MNL model to represent
consumer preferences, we believe that our diﬀusion analysis and approximation have applications beyond
the one discussed in this paper.

Our work opens also some interesting and natural research avenues. The diﬀusion approximation obtained
by counterbalancing large sample sizes with little informativeness has revealed to be extremely eﬀective
and suggest that such approach should be considered in other related settings where it might oﬀer new
approximations that complement those obtained by scaling only one parameter, (e.g., the sample size).
The recent work of Wager and Xu (2021) on MAB and Zenios and Wang (2021) in the context of clinical
trials, represent another conﬁrmation of this claim.

One simplifying assumption that resonated well with the online experimentation motivation of this work,
is the cost-free experimentation. However, there are other applications (such as clinical trials) where
experimentation can be expensive and cannot be discarded. Adding an experimentation cost will aﬀect the
principle that governs the selection of the experiment and certainly induce an earlier stopping. Another
ingredient of our model that made some of our analysis more tractable is the discrete set of experiments
available to the DM from the start. In Section 5.1 we brieﬂy discussed the design of the experimentation
set in a way that leverages our model setup and analysis. We believe this is an interesting avenue to
explore further by adopting probably a continuous and inﬁnite set of possible experiments from which
the DM “designs” dynamically the ones that are more eﬀective for learning. Finally, our assumption
that the unknown parameter takes only two values is restrictive yet, this assumption is an important
ﬁrst step in unravelling the multiple layers the problem and the approach followed have to oﬀer. The

38

multi-hypothesis case requires a much more complex analysis and is left for a future work. We believe
for instance that the principle of maximum volatility will be preserved, yet would require an adaptation
of the deﬁnition of volatility to accommodate the multi-dimensional processes involved. As a result, the
optimal experimentation would become state dependent and the diﬀusion limit of the belief processes
would require even more advanced machinery than the one used in the two hypothesis case. We also
conjecture that the optimal stopping problem would again decouple from the dynamic experimentation.
However, ﬁnding the stopping regions in this multi-dimensional setting will not be easy to characterize
(see Dayanik et al. (2008), in the case of a compound Poisson process).

Acknowledgement: The authors are very grateful to the Department Editor, Omar Besbes, for en-
couraging them to further expand the scope of the proposed asymptotic regime in which the information
content of experiments is very low. The authors are also very grateful to the Associate Editor and three
referees for their careful reading of the paper and for the many helpful and constructive comments. The
second author thank the University of Chicago Booth School of Business for ﬁnancial support.

References

Agrawal, S., V. Avadhandula, V. Goyal, A. Zeevi. 2019. MNL-Bandit: A dynamic learning approach to

assortment selection. Oper. Res. 67(5) 1453–1485. 5, 6, 35

Alaei, S., A. Malekian, M. Mostagir. 2016. A dynamic model of crowdfunding. Working Paper, Ross

School of Business, University of Michigan. 6

Araman, V., R. Caldentey. 2009. Dynamic pricing for nonperishable products with demand learning.

Oper. Res. 57(5) 1169–1188. 6

Araman, V., R. Caldentey. 2011. Revenue management with incomplete demand information. Wiley
Encyclopedia of Operations Research and Management Science (John Wiley and Sons, Hoboken, NJ) .
6

Armitage, P., G. Berry, J.N.S. Matthews. 2002. Statistical Methods in Medical Research. Fourth edition

ed. Blackwell Science, Massachusetts, USA. 2

Bartroﬀ, J., M. Finkelman, T.L. Lai. 2008. Modern sequential anlaysis and its applications to computer-

ized adaptive testing. Psychometrika 73(3) 473–486. 2

Bastani, H., M. Bayati, K. Khashayar. 2020. Mostly exploration-free algorithms for contextual bandits.

Management Sci. Forthcoming . 5

Besbes, O., A. Zeevi. 2009. Dynamic pricing without knowing the demand function: risk bounds and

near-optimal algorithms. Oper. Res. 57 1407–1420. 6

Blackwell, D. 1951. Comparison of experiments. Proceedings of the Second Berkeley Symposium on
Mathematical Statistics and Probability. University of California Press, Berkeley, Calif., 93–102. URL
https://projecteuclid.org/euclid.bsmsp/1200500222. 12

Blackwell, D. 1965. Discounted dynamic programming. Ann. Math. Stat. 36(1) 226–235. 9
Bolton, P., C. Harris. 1999. Strategic experimentation. Econometrica 67(2) 349–374. 5
Breakwell, J., H. Chernoﬀ. 1964. Sequential tests for the mean of a normal distribution ii (large t). Ann.

39

Math. Statist. 35(1) 162–173. doi:10.1214/aoms/1177703738. URL https://doi.org/10.1214/aoms/
1177703738. 4

Brezzi, M., T.L. Lai. 2002. Optimal learning and experimentation in bandit problems. Journal of
Economic Dynamics and Control 27(1) 87–108. URL https://EconPapers.repec.org/RePEc:eee:
dyncon:v:27:y:2002:i:1:p:87-108. 5

Broder, J., ¨P. Rusmevichientong. 2012. Dynamic pricing under a general parametric choice model. Oper.

Res. 60(4) 965–980. 6

Cam, L. Le. 1996. Comparison of experiments: A short review. Lecture Notes-Monograph Series 30

127–138. 12

Caro, F., J. Gallien. 2007. Dynamic assortment with demand learning for seasonal consumer goods.

Management Sci. 53(2) 276–292. 3, 6, 35

Chang, Fu, Tze Leung Lai. 1987. Optimal stopping and dynamic allocation. Advances in Applied

Probability 19(4) 829–853. URL http://www.jstor.org/stable/1427104. 5

Chernoﬀ, H. 1959. Sequential design of experiments. Ann. Math. Statist. 30(3) 755–770. doi:10.1214/

aoms/1177706205. URL https://doi.org/10.1214/aoms/1177706205. 4

Chernoﬀ, H. 1961. Sequential tests for the mean of a normal distribution. Proc. Fourth Berkeley Symp.

Math. Statist. Probab. 1 612–624. 4

Chernoﬀ, H. 1972. Sequential Analysis and Optimal Design. SIAM, Philadelphia, PA. 4
Chick, S., P. Frazier. 2012. Sequential sampling with economics of selection procedures. Management

Sci. 58(3) 550–569. 5

Chick, S., N. Gans. 2009. Economic analysis of simulation selection problems. Management Sci. 55(3)

421–437. 5

Dayanik, Savas, H. Vincent Poor, Semih O. Sezer. 2008. Sequential multi-hypothesis testing for compound

poisson processes. Stochastics 80(1) 19–50. 39

den Boer, A.V. 2015. Dynamic pricing and learning: Historical origins, current research, and new

directions. Surveys in Operations Research and Management Science 20 1–18. 6

den Boer, A.V., B. Zwart. 2014. Simultaneously learning and optimizing using controlled variance pricing.

Management Sci. 60(3) 770–783. 6

Fan, L., P. W. Glynn. 2021. Diﬀusion approximations for thompson sampling. Tech. rep., Working Paper

Management Science and Engineering, Stanford University. 4

Feng, Y., R. Caldentey, C.T. Ryan. 2018. Learning customer preferences from personalized assortments.

Tech. rep., University of Chicago. 6

Finkelman, M. 2008. On using stochastic curtailment to shorten the sprt in sequential mastery testing.

Journal of Educational and Behavioral Statistics 33(4) 442–463. 2

Gallego, G., M. Talebian. 2012. Demand learning and dynamic pricing for multi-versions products. J.

Revenue and Pricing Management 11(3) 303–318. 6

Garivier, A., E. Kaufmann. 2016. Optimal best arm identiﬁcation with ﬁxed conﬁdence. COLT . 998–1027.

5

Harrison, J. M., N. Sunar. 2015. Investment timing with incomplete information and multiple means of

learning. Oper. Res. 63(2) 442–457. 4

Harrison, J.M., N.B. Keskin, A. Zeevi. 2012. Bayesian dynamic pricing policies: Learning and earning

40

under a binary prior distribution. Management Science 58(3) 570–586. 6

Heckel, R., S.B. Nihar, K. Ramchandran, Martin J M.J. Wainwright, et al. 2019. Active ranking from
pairwise comparisons and when parametric assumptions do not help. Ann. of Stat. 47(6) 3099–3126.
12

Jacod, J., A.N. Shiryaev. 2003. Limit Thorems for Stochastic Processes. Springer, New York. 45, 46
Karatzas, I., S.E. Shreve. 1991. Brownian Motion and Stochastic Calculus. Springer-Verlag, New York,

NY. 20

Kaufmann, E., O. Capp´e, A. Garivier. 2016. On the complexity of best arm identiﬁcation in multi-armed

bandit models. J. Mach. Learn. Res. 17(1) 1–42. 5

Keener, R. 1984. Second order eﬃciency in the sequential design of experiments. Ann. Statist. 12 510–532.

4

Keskin, G., J.R. Birge. 2019. Dynamic selling mechanisms for product diﬀerentiation and learning.

Operations Research 67(4) 1069–1089. 6

Keskin, G., A. Zeevi. 2014. Dynamic pricing with an unknown demand model: Asymptotically optimal

semi-myopic policies. Operations Research 6(5) 1142–1167. 6

Keskin, G., A. Zeevi. 2018. On incomplete learning and certain-equivalence control. Operations Research

66(4) 1136–1167. 5

Kohavi, R., S. Thomke. 2017. The surprising power of online experiments. Harvard Business Review

(September-October). 2, 13, 28

K¨ok, A.G., M.L. Fisher, R. Vaidyanatha. 2009. Assortment planning: Review of literature and industry
practice. N. Agrawal, S.A. Smith, eds., Retail Supply Chain Management: Quantitative Models and
Empirical Studies, chap. 6. International Series in Operations Research and Management Science,
Springer, New York, USA. 3

Krager, D.R., S. Oh, D. Shah. 2014. Budget-optimal task allocation for reliable crowdsourcing systems.

Operations Research 62(1) 1–24. 6

Lai, Tze Leung. 2001. Sequential analysis: Some classical problems and new challenges. Statistica Sinica

11(2) 303–351. 4

Lewis, R. A., J. R. Rao. 2015. The unfavorable economics of measuring the returns to advertising 130(4)

1941–1973. 13, 28

Lindley, D. V. 1956. On a measure of the information provided by an experiment. Ann. Math. Statist.
27(4) 986–1005. doi:10.1214/aoms/1177728069. URL https://doi.org/10.1214/aoms/1177728069.
12

Marinesi, S., K. Girotra. 2013. Information acquisition through customer voting systems. Tech. rep.,

Working Paper, INSEAD. 6

Naghshvar, M., T. Javidi. 2013. Active sequential hypthesis testing. Ann. Statist. 41(6) 2703–2738. 4
Nahm, M. 2012. Data quality in cleanical research. R. L. Richession, J. E. Andrews, eds., Clinical

Research Informatics, chap. 10. Springer, New York, USA. 13

Oh, M., G. Iyengar. 2019. Thompson sampling for multinomial logit contextual bandits. In Advances in

Neural Information Processing Systems 3145–3155. 5

Papanastasiou, Y., K. Bimpikis, N. Savva. 2018. Crowdsourcing exploration. Management Sci. 64(4)

1727–1746. 6

41

Peskir, G., A.N. Shiryaev. 2006. Optimal Stopping and Free-Boundary Problems. Birkh¨auser Verlag,

Basel, Switzerland. 4

Powell, W.B. 2016. Perspectives of approximate dynamic programming. Ann. Oper. Res. 241 319–356.

5

Protter, P.E. 2004. Stochastic Integration and Diﬀerential Equations. Springer, Germany. 50
Puterman, M.L. 2005. Markov Decision Processes: Discrete Stochastic Dynamic Programming. 2nd ed.

Wiley. 9, 31, 47

Qiu, P. 2014. Introduction to Statistical Process Control . Chapman & Hall/CRC, Boca Raton, FL. 2
Robbins, H. 1952. Some aspects of the sequential design of experiments. Bull. Amer. Math. Soc. 58(5)

527–535. 4

Russo, D. 2020. Simple Bayesian algorithms for best arm identiﬁcation. Operations Research 68(6)

1625–1647. 5, 36

Saur´e, D., A. Zeevi. 2013. Optimal dynamic assortment planning with demand learning. Manufacturing

Service Oper. Management 15(3) 387–404. 6

Shaked, Moshe, J.G. Shanthikumar. 1994.

Stochastic Orders and Their Applications. Probabil-
ity and mathematical statistics, Academic Press. URL https://books.google.com.lb/books?id=
ynyEQgAACAAJ. 12

Siegmund, D. 1985. Sequential Analysis: Tests and Conﬁdence Intervals.. Springer-Verlag, New York,

NY. 4

Soare, M., A. Lazariac, R. Munos. 2014. Best-arm identiﬁcation in linear bandits. Advances in Neural

Information Processing Systems 828–836. 5

Sunar, Nur, John R. Birge, Sinit Vitavasiri. 2019. Optimal dynamic product development and launch for

a network of customers. Operations Research 67(3) 770–790. 26

Sz¨or´enyi, B., R. Busa-Fekete, P. Adil, E. H¨ullermeier. 2015. Online rank elicitation for Plackett-Luce: A

dueling bandits approach. NeurIPS . 604–612. 12

Tsoukalas, G., B. H. Falk. 2019. Token-weighted crowdsourcing. Tech. rep., University of Pennsylvania.

6

Ulu, C., D. Honhon, A. Alptekino˘glu. 2012. Learning consumer tastes through dynamic assortments.

Operations Research 60(4) 833–849. 6

Wager, S., K. Xu. 2021. Diﬀusion asymptotics for sequential experiments. Tech. rep., Working Pa-

per,Stanford Graduate School of Business. 4, 38

Wald, A. 1945. Sequential tests of statistical hypotheses. Ann. Math. Stat. 16(2) 117–186. 4
Wald, A. 1947. Sequential Analysis. John Wiley and Sons, New York, NY. 3
Wald, A., J. Wolfowitz. 1948. Optimum character of the sequential probability ratio test. Ann. Math.

Stat. 19(3) 326–339. 4

Zenios, S., Z. Wang. 2021. Adaptive design of clinical trials: A sequential learning approach. Tech. rep.,

Working Paper,Stanford Graduate School of Business. 38

42

A Appendix: Proofs

Proof of Lemma 1: Let ti be the ith jump of Nt and let δti
observing the outcome xti of experiment

−

δti = P(Θ = θ0|Fti

−

, xti) =

P(xti

−|Fti

−

=

Eti)
By iterating this recursion, with δt0 = δ, we get that

+ (1

δti

−

L

−

−

be the decision maker’s belief just before

Eti. Then, by Bayes’s rule we have that
)

, Θ = θ0) P(Θ = θ0|Fti
P(xti|Fti
)
δti
−
(xti,
)
δti

−

.

−

=

Q(xti,

Eti, θ0) δti
+ Q(xti,

−

Q(xti,

Eti, θ0) δti

−

Eti, θ1) (1

δti

)

−

−

δti =

δ + (1

δ

−

,

δ) Lti

where Lti =

i

(cid:89)j=1

(xtj ,

L

Etj ).

Finally, the result follows from noticing that δt is a pure jump process and so δt = δtNt

. (cid:50)

Proof of Proposition 1: First, the convexity of G(δ) follows directly from its representation in (1)
and the fact that the ‘max’ of convex functions is also a convex function. To prove the convexity of
(0, 1), let us ﬁrst recall that the value function Π(δ) satisﬁes the HJB
the value function Π(δ) for δ
equation:

∈

(cid:26)
Now consider a sequence of functions

Π(δ) = max

G(δ) ,

Π

δ + η(δ, x,

Λ
Λ + r

max
E
E∈

Eδ
(cid:110)

(cid:104)
0 deﬁned recursively by

(cid:0)

E

.

)
(cid:1)(cid:105)(cid:111)(cid:27)

(A-1)

Πk(δ)
{

}k

≥

Λ
Λ + r

max
E
E∈

Eδ
(cid:110)

Πk+1(δ) = max

G(δ) ,

(cid:26)

(cid:0)
with Π0(δ) = G(δ). It is easy to see that the functions
monotonically increasing in k, that is, Πk+1(δ)
converges uniformly to a limit Π(δ) := limk
this, note that

→∞

,

(cid:104)

E

Πk

δ + η(δ, x,

k = 0, 1, . . .

)
(cid:1)(cid:105)(cid:111)(cid:27)
0 are continuous in δ and pointwise
(0, 1). Furthermore, the sequence
≥
Πk(δ) that satisﬁes the HJB equation in (A-1). To see

}k
{
Πk(δ) for all δ

Πk(δ)

∈

≥

0

≤

Πk+1(δ)

Πk(δ)

−

Λ
Λ + r
Λ
Λ + r
Λ
Λ + r

≤

≤

≤

where

E

max
E
(cid:20)
E∈
Eδ

Eδ
(cid:110)
Πk
(cid:104)
Πk

(cid:104)
Eδ

(cid:104)

(cid:0)

δ + η(δ, x,

)

E

Πk
(cid:104)

(cid:0)
δ + η(δ, x,

(cid:1)(cid:105)(cid:111)
Eδ
−

Πk

1

−

(cid:1)(cid:105)
−

−

max
E
E∈

Πk

1

−

1

Πk

Eδ
(cid:110)
(cid:0)
δ + η(δ, x,

(cid:104)

−

(cid:104)

(cid:0)

(cid:0)
δ + η(δ, x,

E

∗k (δ))

,
(cid:1)(cid:105)

δ + η(δ, x,

∗k (δ))

E

(cid:1)(cid:105)(cid:105)

∗k (δ))

E

∗k (δ))

E

(cid:1)

(cid:0)
δ + η(δ, x,

E

)
(cid:1)(cid:105)(cid:111)(cid:21)

∗k (δ) := argmax

E

E∈

Eδ
(cid:110)

(cid:104)

Πk

δ + η(δ, x,

(cid:0)

E

)
.
(cid:1)(cid:105)(cid:111)

43

Taking the ‘sup’ over δ, it follows that

ρk ≤

Λ
Λ + r

ρk

1 ≤
−

Λ
Λ + r

k

(cid:19)

(cid:18)

sup
(0,1)

∈

δ

G(δ)

,

(cid:110)

(cid:111)

where ρk := sup
(0,1)

δ

∈

Πk+1(δ)

(cid:110)

−

Πk(δ)

(cid:111)

0 as k

.
→ ∞

and so ρk →
We now complete the proof by showing that the HJB operator preserves convexity. That is, if Πk(δ) is
convex in (0, 1) then Πk+1(δ) is also convex. First, since G(δ) is convex, it follows trivially that Π1(δ)
is convex. Now, let us suppose that Πk(δ) is convex in (0, 1). Then, since the ‘max’ operator preserves
convexity, we just need to show that Eδ
is convex. We can rewrite this expectation
as follows:

δ + η(δ, x,

Πk

E

)

(cid:2)

(cid:0)

(cid:1)(cid:3)

Eδ

Πk

δ + η(δ, x,

(cid:104)

(cid:0)

Πk

=

E

)
(cid:1)(cid:105)

(cid:88)x
∈X

E

δ Q(x,

(cid:18)

E

δ Q(x,
, θ0) + (1

E
−

, θ0)

δ) Q(x,

, θ1)

E

(cid:19)

(cid:0)

δ Q(x,

E

, θ0) + (1

δ) Q(x,

, θ1)

.

E

−

(cid:1)

Since the sum of convex functions is convex, we will show that each summand on the right-hand side
above is convex. To ease notation, let us deﬁne Q0 = Q(x,
δ) Q1
and deﬁne the function

, θ1), y = δ Q0 + (1

, θ0), Q1 = Q(x,

−

E

E

H(y) := y Πk

ay + b
y

,

(cid:19)

(cid:18)

where a :=

Q0
Q0 −

Q1

and b :=

Q0 Q1
Q0
Q1 −

.

(Q0, Q1) (as-
Since y is a linear transformation of δ we can focus on proving the convexity of H(y) for y
suming, without loss of generality, that Q0 < Q1). To this end, we will use the following characterization
of a convex function:

∈

Let a continuous function h(δ) be such that for any δ in the interior of the domain of h the
subdiﬀerential ∂h(δ) is not empty. Then h is convex. (see Theorem 3.2.6 in Bazaraa et al.
1993)

H(y) + ∂Hy (z

Thus, we would like to show that for every y
H(z)
exists a subdiﬀerential ∂Πδ such that Πk(z)
deﬁne

y), for all z

−

≥

∈

≥

∈

(Q0, Q1). Since Πk(δ) is convex then for every δ

(Q0, Q1), there exists a subdiﬀerential ∂Hy such that
(0, 1) there
(Q0, Q1)

∈
(0, 1). For y

Πk(δ) + ∂Πδ (z

δ), for all z

−

∈

∈

ˆy :=

a y + b
y

then by the convexity of Πk(δ) for any z

(Q0, Q1) we have

∈

a z + b
z

Πk

(cid:18)

Πk

(cid:18)

≥

(cid:19)

a y + b
y

+ ∂Πˆy

(cid:19)

(cid:18)

a z + b

z −

a y + b
y

.

(cid:19)

Multiplying by z y (which is nonnegative since Q0 > 0) and rearranging terms we get that

y H(z)

z H(y)

b ∂Πˆy (z

y)

−

−

≥

⇐⇒

H(z)

≥

H(y) +

H(y)

b ∂Πˆy

−
y

(cid:18)

(z

y)

−

(cid:19)

44

and so

∂Hy :=

H(y)

b ∂Πˆy

−
y

(cid:18)

(cid:19)

is a subdiﬀerential for H at y. This completes the proof. (cid:50)

Proof of Lemma 2: Recall that the belief process can be written in terms of the likelihood function L
as follows:

δ

δt =

δ + (1

δ) Lt

,

where Lt :=

Nt

(xti,

Eti).

L

−
Now if we consider the log-likelihood function we can rewrite δt as follows:

(cid:89)i=0

δt = f (Yt) where

f (Y ) :=

δ
δ) exp(Y )

,

δ + (1

−

Nt

Yt :=

βi

and βi := ln (

(cid:88)i=0

(xti,

Eti)) .

L

Using Itˆo’s lemma, we can express δt as the solution of the SDE

) dYt + f (Yt)

f (Yt

−

)

−

−

f (cid:48)(Yt

−

) ∆Yt

dδt = f (cid:48)(Yt

−
= f (Yt)

−

)

f (Yt

−
+ βNt)

=

f (Yt

)
−
Q(xt,
(cid:1)
Et, θ0) δt
where the second equality follows from the fact that Yt is a pure jump process, i.e., dYt = ∆Yt. (cid:50)

Et, θ1)
Et, θ1) (1

−
+ Q(xt,

Et, θ0)

(cid:0)
= (1

Q(xt,

Q(xt,

dNt.

f (Yt

dNt

) δt

δt

δt

−

−

−

(cid:18)

(cid:19)

−

−

−

−

−

)

Proof of Proposition 2: To prove the result we invoke Theorem 4.21 in Chapter IX in Jacod and
Shiryaev (2003) related to the convergence of Markov processes to diﬀusions. To this end, note that from
Lemma 2 it follows that the belief process δk
is a pure jump Markov process that admits a generator of
t
the form

where the kernel Kk(δ, y) satisﬁes

kf (δ) =

A

(0,1)

(cid:90)y

∈

[f (δ + y)

−

f (δ)] Kk(δ, dy),

f (y) Kk(δ, dy) = Λk

(cid:90)y

(0,1)

∈

E
(cid:88)E∈

(cid:88)x
∈E

f (ηk(δ, x,

)) Qk

δ (x,

E

) π(δ,

)

E

E

where Qk

δ (x,

E

) := δ Qk(x,

, θ0) + (1

E

−

δ) Qk(x,

, θ1) and

E

ηk(δ, x,

) := (1

δ) δ

−

E

(cid:18)

1

k(x,
δ)

)
E
k(x,

L

− L
−

δ + (1

.

)

E

(cid:19)

It follows that the instantaneous drift and volatility of δk

t are given by

bk(δ) :=

y Kk(δ, dy) = Λk

(cid:90)y

(0,1)

∈

E
(cid:88)E∈

(cid:88)x
∈E

45

ηk(δ, x,

) Qk

δ (x,

E

) π(δ,

) = 0

E

E

and

ck(δ) :=

y2 Kk(δ, dy) = Λk

(ηk(δ, x,

(0,1)

(cid:90)y

∈

E
(cid:88)E∈
= Λk δ (1

(cid:88)x
∈E

δ)

−

E
(cid:88)E∈

(cid:88)x
∈E

= Λ δ (1

δ)

−

E
(cid:88)E∈

(cid:88)x
∈E

It follows by Assumption 1 that

))2 Qk

δ (x,

E

) π(δ,

)

E

E

Qk(x,

E

(cid:0)

, θ0)
Qk

−
δ (x,

Qk(x,
)

2

, θ1)

E

(cid:1)

π(δ,

)

E

αk(x,
E
1 + (δ αk(x,

(cid:0)

E
αk(x,

, θ0)

−
, θ0) + (1

E

2

, θ0)
E
Q
δ) αk(x,
(cid:1)

E

−

k(x,
)
E
, θ1))/√k

π(δ,

).

E

k

b(δ) := lim
→∞
c(δ) := lim
→∞

k

bk(δ) = 0

and

ck(δ) = Λ δ (1

δ)

−

α(x,

, θ0)

E

−

α(x,

E

2

, θ1)

(cid:1)

(x,

Q

E

) π(δ,

).

E

E
(cid:88)E∈

(cid:88)x
∈E (cid:0)

(Note that the convergence of bk(δ) and ck(δ) is trivially locally uniformly in (0, 1)).
Since the jump size ηk(δ, x,
) converges to zero as k
that for all (cid:15) > 0

uniformly in δ for all

→ ∞

E

E

and all x

, we get

∈ E

sup
(0,1) (cid:90)y

∈

δ

(0,1)

∈

y2 11(y > (cid:15)) Kk(δ, dy) = Λk

(ηk(δ, x,

E
(cid:88)E∈

(cid:88)x
∈E
as k

.
→ ∞

0

→

ηk(δ, x,
))2 11(
|

)
|

E

E

> (cid:15)) Qk

δ (x,

) π(δ,

)

E

E

is informative and so we must have

∈ Mc. We also have c(δ) > 0 for all δ

To conclude, note that b(δ) is trivially bounded and c(δ) is continuous and bounded in (0, 1) since are
(0, 1) since we have assumed that every
assuming that π
2 > 0. So, by Theorem
experiment
, θ0)
x
2.34 in Chapter III in Jacod and Shiryaev (2003) the semimartingale problem with characteristics (b, c)
(cid:80)
(0, 1). In sum, all the required conditions in Theorem
has a unique solution for every initial condition δ
4.21 in Chapter IX in Jacod and Shiryaev (2003) are satisﬁed and so δk
t converges weakly to a diﬀusion
process ˜δt with characteristics (b, c). (cid:50)

∈
α(x,

α(x,

, θ1)

−

∈E

∈

E

E

E

(cid:1)

(cid:0)

Proof of Proposition 3: Consider an arbitrary instance of the problem, and let (π,
× B
be an optimal Markovian policy with corresponding value function Π(δ). For future references, we recall
that for any bounded function f (δ), the value-iteration recursion

∈ M

(E )

I

)

Πj+1(δ) = 11(δ

) G(δ) + 11(δ

∈ I

c) ρ Eπ

∈ I

Πj(δ1)
(cid:104)

δ0 = δ
|

, Π0(δ) = f (δ), where ρ :=
(cid:105)

Λ
Λ + r

(A-2)

produces a sequence of functions
of the belief process after one jump (vote) and Eπ[
] is the expectation operator induced by policy (π,
·
which satisﬁes

0 that converges pointwise to Π. In (A-2), δ1 denotes the value
),

Πj}j

I

{

≥

Eπ[f (δj+1)
δj = δ] = 11(δ
|

∈ I

) f (δ) + 11(δ

c)

∈ I

f (δ + η(δ, x,

)) Qδ(x,

) π(δ,

).

E

E

E

E
(cid:88)E∈

(cid:88)x
∈E

46

Note that in (A-2) and in the deﬁnition of Eπ[
·
). Also, the fact that
policy (π,
mapping argument (e.g., Chapter 6 in Puterman, 2005).

Πj}j

I

{

≥

] have used the fact that the set

is absorbing under
0 converges pointwise to Π follows by a standard contraction

I

Now, since the class of continuous experimentation strategies
ˆπn}n
norm, there exists a sequence of continuous strategies
{
Let us denote by
the function

Πn satisﬁes the ﬁxed-point condition

Πn be the expected payoﬀ function under the policy (ˆπn,

I

Mc(∆E ) is dense in
1 in

(∆E ) under the L1
M
Mc(∆E ) that converges in L1 to π.
). It follows that for each n,

≥

(cid:98)

(cid:98)

Πn(δ) = 11(δ

∈ I

) G(δ) + 11(δ

∈ I

c) ρ Eˆπn

Πn(δ1)

δ0 = δ
|

(cid:105)

(cid:104)

,

(A-3)

where the expectation operator Eˆπn[
] under policy (ˆπn,
·
fact to keep in mind is that by the optimality of (π,

(cid:98)

) is deﬁned in a similar way to Eπ[
·

(cid:98)

I

) we have that Π(δ)

Πn(δ).

] above. One

I

≥

We want to show that
(A-2) with initial condition Π0(δ) =

Πn converges to Π(δ) in L1 as n

Πn(δ). Combining (A-2) and (A-3), one can show that

. To this end, let us use the recursion in

(cid:98)

→ ∞

(cid:98)

Π1(δ) =

Πn(δ) + 11(δ

(cid:98)

where

c) ρ

∈ I

(cid:98)
(cid:88)x
∈E

E
(cid:88)E∈

Πn(δ + η(δ, x,

)) Qδ(x,

) [π(δ,

)

E

−

ˆπn(δ,

)]

E

≤

E

E

Πn(δ) + Fn(δ),

(cid:98)

(cid:98)

Fn(δ) := 11(δ

c) ρ max

δ

∈ I

Πn(δ)

π(δ,

)

E

−

ˆπn(δ,

)

.

E

(cid:110)

(cid:98)

E
(cid:111) (cid:88)E∈

(cid:12)
(cid:12)

(cid:12)
(cid:12)

We note that
Fn(cid:107)1 ≤
of n follows from the fact that the

K

−

π

(cid:107)

(cid:107)

ˆπn(cid:107)1 for some ﬁxed constant K. (The fact that we can choose K independent

Πn are uniformly bounded above by Π.)

Let us iterate the recursion (A-2) one more time for Π2. Using the inequality Π1(δ)
get that

(cid:98)

≤

If we keep iterating this inequality we get that

(cid:98)

Π2(δ)

≤

Πn(δ) + Fn(δ) + ρ Eπ

δ0 = δ
Fn(δ1)
|

.

(cid:105)

(cid:104)

Πn(δ) + Fn(δ), we

(cid:98)

where δ(cid:96) denotes the state of the belief process after (cid:96) jumps (votes). Let us denote by J the random
0,
0 : δ(cid:96) ∈ I}
time at which δ(cid:96) enters
we have that

. Since Fn(δ(cid:96)) = 0 for all (cid:96)

, that is, J := inf

J and Fn(δ)

(cid:96)
{

≥

≥

≥

I

Πj(δ)

≤

Πn(δ) + Eπ

ρ(cid:96) Fn(δ(cid:96))

δ0 = δ

,

(cid:35)

j

1

−

(cid:34)

(cid:88)(cid:96)=0

J

1

−

(cid:34)

(cid:88)(cid:96)=0

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:98)

(cid:98)

Πj(δ)

≤

Πn(δ) + Eπ

ρ(cid:96) Fn(δ(cid:96))

δ0 = δ

.

(cid:35)

Π(δ)

≤

Πn(δ) + Eπ

(cid:98)

ρ(cid:96) Fn(δ(cid:96))

δ0 = δ

(cid:12)
(cid:12)

.

(cid:35)

J

1

−

(cid:34)

(cid:88)(cid:96)=0

47

Hence, taking limit as j

↑ ∞

and using the pointwise convergence of

Πj}

{

to Π, we get

Since Π is the optimal value function, it follows that Π(δ)

Πn(δ) and so

≥

Π

Πn(cid:107)1 ≤
(cid:98)
Next, we use a localization argument. Let us deﬁne ¯F := supn maxδ{
nonnegative integer. Then, the previous inequality implies

δ0 = δ

(cid:88)(cid:96)=0

0
(cid:90)

−

(cid:35)

(cid:34)

(cid:12)
(cid:12)

(cid:107)

dδ.

1

Eπ

J

1

−

(cid:98)
ρ(cid:96) Fn(δ(cid:96))

Fn(δ)
}

and let T be a ﬁxed

Π

(cid:107)

−

Πn(cid:107)1 ≤
(cid:98)

(cid:88)(cid:96)=0

0
(cid:90)

T

1

−

ρ(cid:96)

1

Eπ[Fn(δ(cid:96))

δ0 = δ] dδ + ¯F

Λ
r

ρT

1 Eπ

−

1

T )+

ρ(J

−

.

−

(cid:104)

(cid:105)

(cid:12)
(cid:12)
To complete the proof, we will show in Lemma 3 below that there exists a constant ¯µ > 0 such that

π

(cid:107)

−

¯F

Λ
r

(cid:12)
(cid:12)

1

0
(cid:90)

Eπ[Fn(δ(cid:96))

δ0 = δ] dδ

¯µ(cid:96)

Fn(cid:107)1 ≤

(cid:107)

≤

K ¯µ(cid:96)

π
(cid:107)

ˆπn(cid:107)1.

−

As a result,

Π

(cid:107)

−

Taking limit as n

↑ ∞

(cid:12)
(cid:12)
(ρ ¯µ)T
ρ ¯µ

1

K

Πn(cid:107)1 ≤
(cid:18)
(cid:19)
(cid:98)
and using the fact that

−
1

−

π

(cid:107)

Λ
r

ˆπn(cid:107)1 + ¯F
−
ˆπn(cid:107)1 ↓

ρT

1 Eπ

−

1

T )+

ρ(J

−

.

−

(cid:105)

0 as n

we get that

↑ ∞

Πn(cid:107)1 ≤
(cid:98)
Finally, since T is arbitrary, we can let T

lim
→∞ (cid:107)
n

−

Π

ρT

1 Eπ

−

1

T )+

ρ(J

−

.

−

(cid:104)

to complete the proof. (cid:50)

↑ ∞

(cid:104)

(cid:105)

Lemma 3. There exists a constant ¯µ > 0 such that for any non-negative function f (δ) with f (δ) = 0 in

I

1

0
(cid:90)

Eπ[f (δ(cid:96))

δ0 = δ] dδ

¯µ(cid:96)

f
(cid:107)

(cid:107)1.

≤

Proof of Lemma 3: We use a proof by induction. Let us consider ﬁrst the case (cid:96) = 1. From the deﬁnition of
Eπ[
·

] we have that

1

0
(cid:90)

Eπ[f (δ(cid:96))

δ0 = δ] dδ

(cid:12)
(cid:12)

1

1

1

E
(cid:88)E∈

0
∈E (cid:90)
(cid:88)x

E
(cid:88)E∈

0
∈E (cid:90)
(cid:88)x

≤

=

=

f (δ + η(δ, x,

)) Qδ(x,

) π(δ,

) dδ

E

E

E

δ
δ)

f

δ + (1

(cid:18)

−
f (u) Qδu(x,

(x,

)

E
L
) π(δu,
) u

(cid:19)
)
E
L
u)2
−

E

E
(x,

Qδ(x,

) π(δ,

) dδ

E

E

E
(cid:88)E∈

≤

max
E
E∈

0
∈E (cid:90)
(cid:88)x
max
x

∈E (cid:26)

(1 +

L
1
(x,

L

(x,

L

),

E

= max
E
E∈

max
x

∈E (cid:26)

(x,

L

),

E

1
(x,

L

(x,

)

E

du with δu =

(x,
(x,

L
L

E
E

) u
) u

u

−

1 +

1

f (u) Qδu(x,

0
∈E (cid:90)
(cid:88)x
f (u) du.

) π(δu,

)du

E

E

(A-4)

)

E

E
(cid:27) (cid:88)E∈
1

)

E

0
(cid:27) (cid:90)

48

In the second inequality we have used the fact that

L
(1 + L u

max
[0,1]
u
∈

u)2 ≤

−

max

L,

(cid:26)

1
L

.

(cid:27)

So, the result in Lemma 3 holds for (cid:96) = 1 with

¯µ := max
E

E∈

max
x

∈E (cid:26)

(x,

L

),

E

1
(x,

L

.

)

E

(cid:27)

1. From the law of iterated expectations we have
Suppose that the result in Lemma 3 is true for j = 1, . . . , (cid:96)
δ0 = δ] with g(δ) := Eπ[f (δ(cid:96))
that Eπ[f (δ(cid:96))
δ0 = δ] = Eπ[Eπ[f (δ(cid:96))
1 = δ].
1)
|
|
|
But, by the Markov property this is the same as g(δ) := Eπ[f (δ1)
δ0 = δ]. It follows from the hypothesis of
|
induction and the inequality (A-4) that

δ0 = δ] = Eπ[g(δ(cid:96)
−

δ(cid:96)
|

1]
|

δ(cid:96)

−

−

−

1

0
(cid:90)

Eπ[f (δ(cid:96))

δ0 = δ] dδ =

1

Eπ[g(δ(cid:96)

(cid:12)
(cid:12)

¯µ(cid:96)

f
(cid:107)

≤

0
(cid:90)
(cid:107)1. (cid:50)

1)

−

δ0 = δ] dδ
|

≤

1

¯µ(cid:96)
−

1

0
(cid:90)

g(δ) dδ = ¯µ(cid:96)
−

1

1

0
(cid:90)

Eπ[f (δ1)

δ0 = δ] dδ
|

Proof of Proposition 4: For a given experimentation policy π consider the mapping

T π
t

:=

t

1
˜σ2(˜δs, π)

ds,

0
(cid:90)

where ˜σ2(δ, π) is deﬁned in equation (10). Since, ˜σ2(δ, π) > 0 for all δ, the mapping T π
t
increasing t with T π
process

is strictly
t as a random time change and let us deﬁne the

0 = 0. As a result, let us view T π

Let

G˜δ denote the inﬁnitesimal generator of ˜δt. Then, by Proposition 2 it follows that

ˆδt := ˜δT π
t .

G˜δ = ˜σ2(δ, π) δ2 (1

−

δ)2 ∂2
∂δ2 .

It follows that the inﬁnitesimal generator

Gˆδ of ˆδt is given by

Gˆδ = ˙T π

t G˜δ =

1
˜σ2(δ, π)

˜σ2(δ, π) δ2 (1

δ)2 ∂2

∂δ2 = δ2 (1

δ)2 ∂2
∂δ2 .

−

−

In other words, ˆδt is a diﬀusion process that satisﬁes the SDE

for some Wiener process Wt. Also, for a given stopping time τ for ˜δt, let us deﬁne the stopping time ˆτ

dˆδt = ˆδt (1

ˆδ) dWt,

−

(A-5)

49

for ˆδt such that ˆδˆτ = ˜δτ . It follows that

ˆτ

1
˜σ2(˜δs, π)

ds.

τ =

0
(cid:90)

(A-6)

Finally, the result in Proposition 4 follows from equations (A-5) and (A-6). (cid:50)

Proof of Theorem 1: Let f be a solution to the QVI in equation (15). Given the assumptions on f ,
we can apply integration by parts followed by Itˆo’s lemma (see Protter, 2004) to get that

e−

r τ f (δτ ) = f (δ) +

τ

r t

e−

0
(cid:90)

f (δt) dt +

H

τ

0
(cid:90)

e−

r t˜σ δt (1

−

δt) f (cid:48)(δt) dWt.

Note that the process

f (δ) +

t

0
(cid:90)

e−

r s˜σ δs (1

−

δs) f (cid:48)(δs) dWs,

is a local martingale, thus, by the non-negativity of f , also a supermartingale. With this, one can take
expectation, canceling the stochastic integral, and use the fact that
0 (second QVI condition)
to get that

f (δ)

H

≤

This inequality together with the ﬁrst QVI condition imply

E[e−

r τ f (δτ )]

f (δ).

≤

E[e−

r τ

G(δτ )]

E[e−

r τ f (δτ )]

≤

f (δ).

≤

Because these inequalities hold for any stopping time τ , we conclude that f (δ)
(δ). Finally, we note
that all the inequalities above become equalities for the QVI-control associated to f . This follows from
Dynkin’s formula and the fact that the QVI-control is the ﬁrst exit time from a bounded set (continuation
region

). (cid:50)

≥

G

(cid:101)

(cid:101)

C

Proof of Proposition 5: We need to prove both the existence and optimality of the function
in equation (18). Let us start by proving the optimality using the QVI conditions.

Gij(δ)
(cid:101)

The ﬁrst step is to show that
(δij, ¯δij) the function
δ
fact that by construction it satisﬁes the ODE

Gij(δ) is convex in [0, 1]. To see this note that in the continuation region
γ δγ is convex. This follows from the
Gij(δ) = C0
(cid:101)
(cid:101)

γ + C1

δ)γ δ1

ij (1

−

∈

−

−

−

δ)1
ij (1
Gij(δ) = 0, and so
(cid:101)
G

(cid:48)(cid:48)ij(δ) = r

H
δ))2

0

≥

Gij(δ)
(cid:101)

(˜σ δ (1
−
2

(cid:101)

This together with the value matching and smooth pasting conditions at ¯δij and ¯δij ensure that
convex in [0, 1].
Now, by convexity and the smooth-pasting and value matching conditions, both
Gij(δ) in the domain δ
supporting hyperplanes of
∈
holds, that is,
.
Rj(δ)
Ri(δ),
Gij(δ) = max
}
{
(cid:101)
To prove the second and third QVI conditions note that in the continuation region δ
(cid:101)
(cid:101)

Rj(δ) are
[0, 1]. We conclude that the ﬁrst QVI condition
(cid:101)

Ri(δ) and
(cid:101)

Gij(δ) is
(cid:101)

(δij, ¯δij), we have

Gij(δ)
(cid:101)

≥

∈

(cid:101)

50

Gij(δ) and (b)

Gij(δ) = 0 (by construction). On the other hand, in the intervention region δ
H
Gij(δ) =
that (a)
Gij(δ)
r
H
−
(cid:101)
δij, ¯δij, ˆδij}
Finally, if we deﬁne the set Nij =
, it is easy to see that the function
{
(cid:101)
(cid:101)
has second derivative for all δ
[0, 1]
\
optimal.

Gij(δ) =
(cid:101)

Nij. We conclude that

≤

0.

∈

∈

∈

(cid:101)

[0, δij]

∪

[¯δij, 1], we have

1[0, 1] and
ˆ
2 and so by Theorem 1 it is
C

Gij(δ) is in
(cid:101)

C

Gij(δ)
(cid:101)

Let us now turn to the issue of existence. For this, we need to show that there exist thresholds δij and
¯δij so that the smooth pasting and value matching conditions are satisﬁed. To ﬁx ideas, let us suppose
that

≥

Ri(0)
(cid:101)

Rj(0) and let us consider the auxiliary function
(cid:101)
V (δ; δ) := 


Ri(δ)
(cid:101)
γ + C1

δ)γ δ1

C0

ij(δ) (1

ij(δ) (1

−

if

0

δ

≤

≤

δ

δ)1

−

γ δγ

if

δ

δ

1

≤
where the parameter δ
ij(δ) are chosen to ensure value matching
and smooth pasting at δ = δ. Recall that the payoﬀ associated to each action is linear in δ, that is, of
the form

−

[0, ˆδij] and the constants C0

ij(δ) and C1

−

≤

∈

ij(δ) and C1

ij(δ) are equal to

Ri(δ) = ˜αi + ˜βi δ. It follows that the constants C0
(cid:101)

γ

δ) ˜αi

δ

C0

ij(δ) =

(γ

−

1) δ ˜βi + (γ
1) δ

(2 γ

−

(cid:34)

−

1

(cid:35) (cid:18)

δ

(cid:19)

−

and C1

ij(δ) =

γ δ ˜βi + (γ
(2 γ

(cid:34)

−
−

1 + δ) ˜αi
1) δ

1

−
δ

δ

γ

1

−

.

(cid:19)

(cid:35) (cid:18)

∈ {

. The ﬁgure also shows the payoﬀ functions

Of course, by construction the function V (δ; δ) satisﬁes the value matching and smooth pasting conditions
at δ. Next, we show that by varying the value of δ we can also enforce these conditions at the upper
threshold ¯δ. To get some intuition, consider the example in Figure 9 which depicts the function V (δ; δ)
Ri(δ) and
0.1, 0.295, 0.43
for three diﬀerent values of δ
}
Rj(δ). We note that when δ is small (in the example δ = 0.1) the function V (δ; δ) is greater than
Rj(δ)
(cid:101)
δ. On the opposite case, when δ is large (in the example δ = 0.43) the function V (δ; δ)
for all δ
≥
(cid:101)
(cid:101)
δ. By continuity, there is a value of δ (in the example δ = 0.295) so that
Rj(δ) for some δ
intersects
≥
Rj(δ) meet smoothly at some ¯δ
δ.
V (δ; δ) and
(cid:101)
0 C0
To formalize the previous discussion based on the example in Figure 9, let us ﬁrst note that limδ
ij(δ) =
↓
(cid:101)
0 C1
0 and limδ
(0, 1]. This shows
↓
that if δ is suﬃciently small the function V (δ, δ) will be strictly greater than
δ.
R(cid:48)j(δ). In other
On the ﬂip side, we have that limδ
words, if δ is suﬃciently close to ˆδij then the function V (δ, δ) will intersect and go below the function
(cid:101)
Rj(δ).
Finally, since the function V (δ, δ) is continues in δ, we conclude that there exists a value δ = δij ∈
(cid:101)
such that

for all δ
∈
Rj(δ) for all δ
V (cid:48)(δ, δ) = ˜βi < ˜βj =
(cid:101)

Rj(δ) and limδ
(cid:101)

(recall that γ > 1). Hence, the limδ

0 V (δ, δ) =
↓

V (δ, δ) =

ij(δ) =

[0, ˆδij]

∞

∞

ˆδij

ˆδij

≥

≥

→

→

∈
The value of δ that solves the minimization is the upper threshold ¯δij. (cid:50)

(cid:8)

min
[ˆδij ,1]

δ

V (δ, δij)

= 0.

−

}

Rj(δ)
(cid:101)

Proof of Corollary 2: For notational convenience, let us write δ = δij and ¯δ = ¯δij.

51

Figure 9: Value of V (δ; δ) for three values of δ ∈ {0.1, 0.295, 0.43}. For δ = 0.295, the function V (δ; δ) satisﬁes the smooth-
pasting condition at ¯δ. Data: (cid:101)Rj(δ) = 3 δ, (cid:101)Ri(δ) = 4 − 5 δ, r = 1 and ˜σ = 2.

To compute the probability ¯p(δ) = P(˜δτ ∗ = ¯δ
|

˜δ0 = δ), we use Dynkin’s formula to get

E[f (˜δτ ∗)] = f (δ) + E

τ ∗

0 G

(cid:34)(cid:90)

f (˜δt) dt

,

(cid:35)

where

f (δ) :=

G

δ))2

(˜σ δ (1
−
2

d2f (δ)
dδ2

,

G

G

is the inﬁnitesimal generator of the diﬀusion process ˜δt in equation (12). Consider the identity
f (δ) = 0 and by Dynkin’s formula E[˜δτ ∗] = δ. But since τ ∗ is the ﬁrst
¯p(δ)) δ

and
function f (δ) = δ. It follows that
exit time of the process ˜δt from the continuation region (δ, ¯δ) we have that E[˜δτ ∗] = ¯p(δ) ¯δ + (1
and the result part of the Corollary follows.
To compute the expectation E[τ ∗], we consider a function
that the function
˜σ2 (2δ
(˜δτ ∗)] =
that E[
T

)(δ) = 1. One can verify
G
satisﬁes this condition. It follows from Dynkin’s formula

(δ) = 2
1) ln
T
(δ) + E[τ ∗] and the result follows. (cid:50)

(δ) such that

−

−

(cid:16)

(cid:17)

T

T

T

−

(

1

δ

δ

Gij(δ)
(cid:101)

Proof of Proposition 7: By Proposition 5, if follows that
G(δ).
result,

max

=

V (δ) = max
{

i,j

Gij(δ)
}

Gij(δ)
(cid:101)

i,j

{

(cid:101)

}∈

(cid:101)
O{

} ≥

(cid:101)
O{
(cid:101)
}∈
By Proposition 5, we know that each function
Gij(δ) is continuously diﬀerentiable everywhere in [0, 1]
(cid:101)
and admits a second derivative almost everywhere in [0, 1] except in the set Nij :=
. Also, two
(cid:101)
functions
Gk(cid:96)(δ) can cross at most a ﬁnite number of times. This follows from noticing that in
the continuation regions the functions C0
−
−
(cid:101)
δ)1
γ δγ can only cross at most once. Let us denote by Eij,k(cid:96) the ﬁnite set of values of δ at which these
two functions cross (if any) and let us deﬁne

Gij(δ) and
(cid:101)

δij, ¯δij}
{

γ δγ and C0

γ + C1

γ + C1

δ)γ δ1

δ)γ δ1

k(cid:96) (1

k(cid:96) (1

ij (1

ij (1

δ)1

−

−

(cid:101)

−

−

−

−

Gij(δ) for all δ

[0, 1]. As a

∈

≥

N (cid:101)V =

Nij ∪

(cid:91)i,j
(cid:101)
O
∈

(cid:91)i,j,k,(cid:96)
∈

(cid:101)
O

Eij,k(cid:96)}

.

{

52

00.20.40.60.810246Belief: eRi( )eRj( )V( ;0.295)V( ;0.1)V( ;0.43)0.430.2950.1subject to

d ˜δt = ˜σ ˜δt (1

˜δt) dWt,

˜δ0 = δ. (A-7)

−

∈

[0, 1]

V (x) =

Now, by our previous construction, it follows that for each δ
B(δ) containing δ such that
function
conditions, we conclude that

Gij(x) for all x
i, j
Gij(x) is twice-continuously diﬀerentiable in B(δ). Since each function
(cid:101)
V (δ) = 0 for all δ
≤
(cid:101)

N (cid:101)V there exists an open neighborhood
. Furthermore, the
} ∈
Gij(x) satisﬁes the QVI
(cid:101)
N (cid:101)V . (cid:50)
[0, 1]
∈
(cid:101)
Gij(δ)
2 (Proposition 5). Furthermore, each function
(cid:101)
(cid:101)

(cid:1)
Proof of Theorem 2: We wish to prove that the function
each function
optimization problem

(cid:101)
V (δ) = max
{

B(δ) for some pair

0 and

V (δ)

V (δ)

G(δ)

(cid:101)
H

H

O

−

(cid:101)
O

∈

}∈

(cid:8)

i,j

(cid:101)

(cid:101)

(cid:101)

C

\

{

\

(cid:0)

2, where
is in
C
Gij(δ) solves the
(cid:9)
(cid:98)
(cid:101)

Gij(δ) is convex and in
(cid:101)
E

r τ max

e−

(cid:98)
Rj(δ)
(cid:101)

Ri(δ),
(cid:101)

(cid:104)

(cid:8)

Gij(δ) = sup
(cid:101)

∈

T

τ

(cid:9)(cid:105)
V (δ) is not in

Let us suppose, by contradiction, that
diﬀerentiable. But since each
Gk(cid:96)(δ) that intersect and that simultaneously solve the maximization in the deﬁnition of
value δ(cid:63). That is,
(cid:101)

2, then it must exist a δ(cid:63) at which
V (δ) is not
Gij(δ) and
Gij(δ) is smooth it follows that there are at least two functions
(cid:101)
V (δ) at this
(cid:101)
(cid:101)

(cid:101)

(cid:98)

(cid:101)

C

V (δ(cid:63)) =

(cid:101)

Gij(δ(cid:63)) =
(cid:101)

Gk(cid:96)(δ(cid:63))
(cid:101)
V (δ) =

and

d
dδ

d
dδ

Gij(δ(cid:63))
(cid:101)
(δ(cid:63) −

=

Gk(cid:96)(δ(cid:63)).
(cid:101)
(cid:15), δ(cid:63)] and

To ﬁx ideas, let us suppose that
Gij(δ) for all δ
∈
[δ(cid:63), δ(cid:63) + (cid:15)) for some small (cid:15) > 0 (as in Figure 10). In this case, the two conditions above imply that
(cid:101)
Gk(cid:96)(δ(cid:63) + (cid:15)) >
(cid:101)

(cid:101)
Gij(δ(cid:63) + (cid:15)) (i.e., point B is above point C).
(cid:101)

Gk(cid:96)(δ) for all δ
(cid:101)

V (δ) =

∈

(cid:101)

Figure 10: Schematic of what needs to happen for (cid:101)V (δ) = max (cid:8)

(cid:101)Gij(δ), (cid:101)Gk(cid:96)(δ)(cid:9) to be non-smooth at some point δ(cid:63).

V (δ). For this, we will exploit the optimality of the

We will now show that point D cannot belong to
functions

Gij(δ) and
(cid:101)

1. Suppose that δ(cid:63) belongs to at least one of the continuation regions

Gk(cid:96)(δ) in the sense of equation (A-7). We distinguish two cases:
(cid:101)
Ck(cid:96) = (δk(cid:96), ¯δk(cid:96))
(cid:101)
associated to
Gij(δ) and
Gk(cid:96)(δ), respectively (see equation (18) in Proposition 5). For concreteness
let us assume that δ(cid:63) ∈ Cij.
(cid:101)
(cid:101)
By choosing (cid:15) small enough we can guarantee that both δ(cid:63) −

Cij = (δij, ¯δij) or

(cid:15) and δ(cid:63) + (cid:15) also belong to

Cij. It

53

eGk`( )eGij( )A B C D  ? ✏ ?+✏ ?(cid:54)
follows then, by the principle of optimality, that

Gij(δ(cid:63)) = E
(cid:101)

r τ(cid:63)

e−

(cid:104)

Gij(˜δτ(cid:63))
(cid:105)
(cid:101)

, where

τ(cid:63) := inf

t > 0 : ˜δt (cid:54)∈

(δ(cid:63) −

(cid:15), δ(cid:63) + (cid:15))

.

(cid:8)

(cid:9)

In words, this identity states that the value
evolve in the region (δ(cid:63) −
value of
is hit ﬁrst).

Gij(δ(cid:63)) can be obtained by letting the belief process ˜δt
(cid:15), δ(cid:63) +(cid:15)) and as soon as one of the boundaries is hit, then the corresponding
(cid:101)
Gij(δ) is collected (point A if the left boundary is hit ﬁrst or point C if the right boundary
(cid:101)

Now, using the fact that

Gk(cid:96)(δ(cid:63) + (cid:15)) >
Gij(δ(cid:63)) < E
(cid:101)
(cid:101)

Gij(δ(cid:63) + (cid:15)) we get that
Gij(˜δτ(cid:63)),
(cid:101)
e−
(cid:110)
(cid:101)

r τ(cid:63) max

(cid:104)

Gk(cid:96)(˜δτ(cid:63))
(cid:101)

.

(cid:111)(cid:105)

But

V (δ(cid:63)) =

2. Let us now suppose that δ(cid:63) belong to both intervention regions

Gij(δ(cid:63)) and so the previous inequality contradicts the optimality of
(cid:101)

V (δ(cid:63)).

[¯δk(cid:96), 1]. Without loss of generality, let us assume that

(cid:101)
[0, δk(cid:96)]
That is, δ(cid:63) = ˆδik the intersection point of
from Proposition 5 we have that

∪

Iij := [0, δij]

Gij(δ(cid:63)) =
(cid:101)

[¯δij, 1] and
(cid:101)
Gk(cid:96)(δ(cid:63)) =
(cid:101)

Ik(cid:96) :=
∪
Rk(δ(cid:63)).
Ri(δ(cid:63)) and
Rk(δ) (see Figure 3 in Section 4.3.2). But,
(cid:101)
(cid:101)
(cid:101)
Gik(δ(cid:63)),
(cid:101)

Ri(δ) and
(cid:101)
Ri(δ(cid:63)) <
V (δ(cid:63)).
(cid:101)

which again contradicts the optimality of

From the previous two cases we conclude that the situation in Figure 10 cannot happen at optimality,
that is, that

V (δ) must be smooth in (0, 1). (cid:50)

(cid:101)

(cid:101)

Proof of Proposition 8: The result follows from the continuity of the operator deﬁned by the optimiza-
tion problem (20). For completeness, assume that k is large enough so that
< ε.
Observe that

Qk(x,

, θ)/

1
|

(x,

Q

−

E

E

)

|

Qk(x,
E
Q(cid:48)(x,
E

, θ)
) −

1

(cid:12)
(cid:12)
(cid:12)
(x,
(cid:12)
E

x

∈E Q

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

(1

max

ε)
Q
−
Q(cid:48)(x,
E
) = 1, we have that

(cid:26)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(x,
)

(1 + ε)
Q
Q(cid:48)(x,
E

(x,
)

)

E

−

)

E

−

,

1
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

(cid:27)

1
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Therefore, subject to

θ

max
x

min
(cid:48)
0
≥
Q

max
θ0,θ1
∈{

(cid:80)
Qk(x,
E
Q(cid:48)(x,
E
The second inequality is obtained by taking
continuous at

(cid:12)
(cid:12)
(cid:12)
(cid:12)
as k

, θ)
) −

∈E (cid:12)
(cid:12)
(cid:12)
(cid:12)

min
(cid:48)
0
≥
Q

and that

≤

1

}

max
x
∈E

Q

→ Q

→ ∞

k
Q

Q(cid:48) =
. (cid:50)

Q

(1

max

ε)
−
Q
Q(cid:48)(x,
E

(x,
)

)

E

1

,

−

(1 + ε)
Q
Q(cid:48)(x,
E

(x,
)

)

E

−

(cid:26)(cid:12)
(cid:12)
(cid:12)
(cid:12)
. This shows that the optimization operator is

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:27)

1
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ε.

≤

Proof of Proposition 10: Let
by showing that

E ∗ be a solution to (27). We will prove the ﬁrst part of the proposition
E ∗ satisﬁes the following properties:
(cid:101)
E ∗ for all j
E ∗) then j
E ∗ and ∆ui ≥
(cid:101)
E ∗ for all j
E ∗) then j
E ∗ and ∆ui ≤
(cid:101)
(cid:101)
(cid:101)
(cid:101)
(cid:101)
(cid:101)

i such that ∆ui < ∆uj.

i such that ∆uj < ∆ui.

∆¯u(

∆¯u(

≥

≤

∈

∈

54

1. If i

2. If i

∈

∈

These two conditions imply that there exist two integers n1 and n2 such that
We will only show the ﬁrst point since the second follows the same line of arguments. Suppose by
∆ui < ∆uj. Let us consider another
E ∗ such that ∆¯u(
contradiction that there exist i
. We will show that ˜σ2(
) > ˜σ2(
display set
E ∗.
=
E
E
(cid:101)
E ∗ and
denote the cardinality of the sets
∗ = m
Let m = m
(cid:101)
(cid:98)
(cid:98)
(cid:101)
(cid:101)
E
(cid:101)

E ∗)
E ∗) which contradicts the optimality of
(cid:101)
, we have that
E
(cid:101)
(cid:98)

E ∗ and j
(cid:101)

j
E ∗ ∪ {
(cid:98)
E

E ∗ =
(cid:101)

i
}
} \ {

[n1, n2].

≤

∈

(cid:54)∈

E

˜σ2(
E

∗) =

(cid:101)

=

1
m

1
m

(∆uk)2

1
m2

−

∗
(cid:88)k
(cid:101)
E
∈

∗
(cid:16) (cid:88)k
(cid:101)
∈
E
(∆uk)2 + (∆ui)2
(cid:16) (cid:88)k
(cid:98)
E
∈

−

2

∆uk

(cid:17)
(∆uj)2

1
m2

−

(cid:17)

(cid:16) (cid:88)k
(cid:98)
E
∈

∆uk + ∆ui −

∆uj

2

(cid:17)

= ˜σ2(
E

) +

∆uj

∆ui −
m

∆ui + ∆uj −



2
m

∆uk −

∆uj

∆ui −
m



(cid:98)

= ˜σ2(
E

) +

∆uj

∆ui −
m





(m

−

1) ∆ui + (m + 1) ∆uj

m

−

2
m

(cid:88)k
(cid:98)
E
∈

(cid:98)

= ˜σ2(
E

) + 2

(cid:98)
) + 2

= ˜σ2(
E

∆ui −
m

∆ui −
m



∆uj



(m + 1) ∆ui + (m

2 m

∆uj



(m + 1) ∆ui + (m

(cid:18)

2 m

1) ∆uj

1) ∆uj

−

−

−

−


∆uk + ∆uj −

∆ui

(cid:17)





∗
(cid:16) (cid:88)k
(cid:101)
E
∈
1
m

∆¯u(

∆uk

< ˜σ2(
E

(cid:19)

),

(cid:98)

∗
(cid:88)k
(cid:101)
E
∈
∗)

E

(cid:101)

(cid:98)

where the last inequality follows from noticing that the argument inside the large parentheses in the last
line is positive since ∆¯u(
E ∗)
Let us now turn to the proof of second part of the proposition. To this end, let us suppose that all
(cid:101)
are non-negative. (The proof of the case where all
prove the result by invoking the following lemma.

∆ui}
{
non-positive uses the same argument.) We will

∆ui < ∆uj.

∆ui}

≤

{

Lemma 4. Let X a bounded random variable on [0, A]. Then Var[X]

A2/4.

≤

Proof of Lemma 4: We prove ﬁrst the lemma. For this notice that

Var[X] = E[X 2]

(E[X])2

A E[X]

−

≤

−

(E[X])2 = E[X](A

E[X]).

−

The inequality is due to the fact that X
[0, A] at x = A/2 with g(1/2) = A2/4. Hence, Var[X]

∈

[0, A]. Finally, we observe that g(x) = x(A

x) is maximized on

−

To use this result, note maximizing the value of ˜σ2(
non-negative random variable X
E
from Lemma 4 that

E

taking values in the set (∆ui : i

E

is equivalent to maximize the variance of a
) with equal probability. It follows

A2/4. (cid:50)

≤
) over

Var[X
E

]

≤

1
4

max
i
∈E

∆u2
i }

{

=

∈ E
(∆un)2
4

.

For the second inequality, recall that we have indexed the products so that ∆u1 ≤
we are assuming that they are all non-negative (i.e., ∆u1 ≥

0). At the same time, if we set

∆u2 ≤ · · · ≤

∆un and
,
0, n
}
{

E ∗ =
(cid:101)

55

then it is easy to see that

We conclude that

E ∗ maximizes ˜σ(
(cid:101)

E

(∆un)2
4

.

Var[X
∗] =
(cid:101)
E
E . (cid:50)

E ∈

) over

56

