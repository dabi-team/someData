9
1
0
2

y
a
M
1

]

G
L
.
s
c
[

1
v
7
4
1
0
0
.
5
0
9
1
:
v
i
X
r
a

Fair Classiﬁcation and Social Welfare

Lily Hu and Yiling Chen

May 2, 2019

Abstract

Now that machine learning algorithms lie at the center of many important resource al-
location pipelines, computer scientists have been unwittingly cast as partial social planners.
Given this state of aﬀairs, important questions follow. What is the relationship between fair-
ness as deﬁned by computer scientists and longer standing notions of social welfare? In this
paper, we present a welfare-based analysis of classiﬁcation and fairness regimes. We trans-
late a loss minimization program into a social welfare maximization problem with a set of
implied welfare weights on individuals and groups—weights that can then be analyzed from
a distribution justice lens. Working in the converse direction, we ask what the space of pos-
sible labelings is for a given dataset X and hypothesis class H. We provide an algorithm
that answers this question with respect to linear hyperplanes in Rd that runs in O(ndd). Our
main ﬁndings on the relationship between fairness criteria and welfare center on sensitivity
analyses of fairness-constrained empirical risk minimization programs. We characterize the
ranges of ∆(cid:15) perturbations to a fairness parameter (cid:15) that yield better, worse, and neutral out-
comes in utility for individuals and by extension, groups. We show that applying more strict
fairness criteria that are codiﬁed as parity constraints, can worsen welfare outcomes for both
groups. More generally, always preferring “more fair” classiﬁers does not abide by the Pareto
Principle—a fundamental axiom of social choice theory and welfare economics. Recent work
in machine learning has rallied around these notions of fairness as critical to ensuring that
algorithmic systems do not have disparate negative impact on disadvantaged social groups.
By showing that these constraints often fail to translate into improved outcomes for these
groups, we cast doubt on their eﬀectiveness as a means to ensure fairness and justice.

1 Introduction

In his 1979 Tanner Lectures, Amartya Sen noted that since nearly all theories of fairness are
founded on an equality of some sort, the heart of the issue rests on clarifying the “equality of
what?” problem [1]. The ﬁeld of fair machine learning has not escaped this essential question.
Does machine learning have an obligation to assure probabilistic equality of outcomes across various
social groups [2, 3]? Or does it simply owe an equality of treatment [4]? Does fairness demand
that individuals (or groups) be subject to equal mistreatment rates [5, 6]? Or does being fair refer
only to avoiding some intolerable level of algorithmic error?

Currently, the task of accounting for fair machine learning cashes out in the comparison of
myriad metrics—probability distributions, error likelihoods, classiﬁcation rates—sliced up every
way possible to reveal the range of inequalities that may arise before, during, and after the learning
[8], fundamental statistical
process. But as shown in Chouldechova [7] and Kleinberg et al.
incompatibilities rule out any solution that can satisfy all parity metrics. Fairness-constrained loss
minimization oﬀers little guidance on its own for choosing among the fairness desiderata, which
appear incommensurable and result in diﬀerent impacts on diﬀerent individuals and groups. We are
thus left with the harsh but unavoidable task of adjudicating between these measures and methods.
How ought we decide? For a given application, who actually beneﬁts from the operationalization of
a certain fairness constraint? This is a basic but critical question that must be answered if we are to
understand the impact that fairness constraints have on classiﬁcation outcomes. Much research in
fairness has been motivated by the well-documented negative impacts that these systems can have
on already structural disadvantaged groups. But do fairness constraints as currently formulated
in fact earn their reputation as serving to improve the welfares of marginalized social groups?

As algorithms continue to be adopted in social environments—consider, for example, the place of
predictive systems in the ﬁnancial services industry—classiﬁer performance and outcomes directly
bear on individuals’ welfares. In light of this new terrain of machine learning, our paper views

1

 
 
 
 
 
 
predictions as resource allocations awarded to individuals and by extension, to various social groups.
With this orientation in mind, we build out a conceptual framework and methodology that analyzes
classiﬁcations and fairness regimes from a utility and welfare-centric perspective.

In Section 3, we cast the loss minimization task at the center of supervised learning as a social
welfare maximization problem prevalent in social choice theory and welfare economics.
In the
Planner’s Problem, a social planner seeks to maximize social welfare represented as the sum of
weighted utility functions, where each individual’s weight represents the value placed by society on
her welfare. Inverting the Planner’s Problem of eﬃcient social welfare maximization generates a
question that concerns social equity: “Given a particular allocation, what is the presumptive social
weight function that would yield the allocation as optimal?” We show that the set of predictions
issued by the optimal classiﬁer of any loss minimization task can also be given as the set of optimal
allocations in the Planner’s Problem, over the same individuals, endowed with a given set of welfare
weights. These weights lie at the heart of debates over fairness of distribution in economics.

A demonstration of the converse result—given a social welfare maximizing allocation, what
is the hypothesis that can achieve an equivalent classiﬁcation?—depends on the particulars of a
given dataset and the hypothesis class under a learner’s consideration. In Section 4, we provide an
algorithm that computes all achievable labelings in O(ndd) time for the class of linear d-dimensional
hyperplanes. Because the fair machine learning literature focuses on classiﬁcation outcomes across
diﬀerent protected attribute social groups e. g., race, this approach exhaustively records the set
of utilities—deﬁned by the number of positively labeled individuals belonging to that group—
achievable for various social groups.

Our main result is presented in Section 5 and shows that how “fair” a classiﬁer is—how well
it accords with a group parity constraint such as equality of opportunity or balance for false
positives—does not neatly translate into statements about how it impacts diﬀerent groups’ wel-
fares. Using techniques from parametric programming and ﬁnding a SVM’s regularization path, we
show that so long as a fairness constraint binds, i.e., applying the constraint changes the optimal
SVM solution, tightening the (cid:15)-level fairness constraint always leads to learner loss but does not
necessarily improve classiﬁcation outcomes for either group. In particular, we prove two surprising
results: ﬁrst, starting at any nonzero (cid:15)-fair optimal SVM solution, there exists a ∆(cid:15) < 0 perturba-
tion that tightens the fairness constraint and leads to classiﬁer-output allocations that are weakly
Pareto dominated by those issued by the “less fair” original classiﬁer. Second, there are nonzero
(cid:15)-fair optimal SVM solutions, such that there exist ∆(cid:15) < 0 perturbations that yield classiﬁca-
tions that are strongly Pareto dominated by those issued by the “less fair” original classiﬁer. We
demonstrate these ﬁndings on the Adult dataset. In general, our results show that when notions of
fairness rest entirely on leading parity-based notions, always preferring more fair machine learning
classiﬁers does not accord with the Pareto Principle, an axiom typically seen as fundamental in
social choice theory and welfare economics generally.

The purposes of our paper are twofold. The ﬁrst is simply to encourage a more welfare-centric
understanding of algorithmic fairness. Whenever machine learning is deployed within important
social and economic processes, concerns for fairness arise when shared societal norms are in ten-
sion with the decision-maker’s goals and desires. Most leading methodologies have focused on
optimization of utility or welfare to the vendor, limiting our ability to answer questions about how
individuals, groups, and society-at-large fare under various distributive allocations. The social
welfare perspective directly engages both questions of eﬃciency, in the task of maximization, and
equity, in the design of welfare weights. This perspective is especially enlightening when applied
to sectors in which the government, acting as the Planner, maintains a strong interest in issues of
distributive fairness and can justiﬁably make interpersonal comparisons of utility.

We also seek to highlight the limits of conceptualizing fairness only in terms of group-based
parity measures. Our results show that at current, making a system “more fair” as deﬁned by
popular metrics can harm the vulnerable social populations that were ostensibly meant to be served
by the imposition of such constraints in the ﬁrst place. Though the Pareto Principle is not without
faults, the frequency with which “more fair” classiﬁcation outcomes are welfare-wise dominated by
“less fair” ones occurs is troublesome and should lead scholars to reevaluate the methodologies by
which we understand the impact of machine learning on diﬀerent social populations.

2

1.1 Related Work

Research in fair machine learning has largely centered on ﬁrst computationally deﬁning “fairness”
as a property of a classiﬁer and then showing that techniques can be invented to satisfy such a
notion [9, 4, 10, 2, 11, 12, 3, 13, 14, 15, 16, 5, 6, 17, 18, 19]. Since most methods are meant
to apply to learning problems generally, many such notions of fairness center on parity-based
statistical metrics about a classiﬁer’s behavior on various protected social groups rather than on
matters of utility or welfare.

Most of the works that do look toward a social welfare-based framework for interpreting appeals
to fairness sit at the intersection of computing and economics. Mullainathan [20] also makes
a comparison between policies as set by machine learning systems and policies as set by a social
planner. He argues that algorithmic systems that make explicit their description of a global welfare
function are less likely to perpetrate biased outcomes and are more successful at ameliorating social
inequities. Heidari et al. [21] propose using social welfare functions as fairness constraints on loss
minimization programs. They suggest that a learner ought to optimize her classiﬁer while in Rawls’
original position. As a result, their approach to social welfare is closely tied with considerations
of risk. Rather than integrate social welfare functions into the supervised learning pipeline, we
keep the two perspectives distinct to emphasize the non-welfarist aspects of fair machine learning.
By translating a machine learning classiﬁcation into a social welfare allocation, we encourage a
conception of fairness that refers to the social weights that various individuals and groups have
in a society. There is a rich body of literature in social choice theory and welfare economics that
investigates the normative and empirical bases of various distributions of social weights [22, 23,
24, 25]. This research directly bears on public policy issues ranging from tax schemes to social
programs.

The techniques that we use to perform sensitivity analysis of fairness constraints are related to
a number of existing works. The proxy fairness constraint that we use in our instantiation of the
(cid:15)-fair SVM problem original appeared in Zafar et al.’s [11] work on restricting the disparate impact
of machine classiﬁers. Their research introduces this particular proxy fairness constrained program
and shows that it can be eﬃciently solved and well approximates target fairness constraints. We
use the constraint to demonstrate our overall ﬁndings about the eﬀect of fairness criteria on indi-
vidual and group welfares. We share some of the preliminary formulations of our fair SVM problem
with Donini et al.
[18] though they focus on the statistical and fairness guarantees of their gen-
eralized empirical risk minimization program. Lastly, though work on tuning hyperparameters of
SVMs is far aﬁeld from questions of fairness and welfare, our analysis on the eﬀect of ∆(cid:15) fairness
perturbations on welfare take advantage of methods in that line of research [26, 27, 28].

2 Problem Formalization

Our framework and results are motivated by those algorithmic uses-cases in which considerations
of fairness and welfare stand alongside those of eﬃciency. Before we formalize the correspondence
between loss minimization and welfare maximization, we ﬁrst provide an overview of these two
distinct perspectives on using optimization to pursue social notions of fairness.

In the empirical loss minimization task, a learner seeks a classiﬁer h that issues the most
accurate predictions when trained on set of n data points {xi, zi, yi}n
i=1. Each triple gives an
individual’s feature vector xi ∈ X , protected class attribute zi ∈ {0, 1},1 and true label yi ∈
{−1, +1}. A model that assigns an incorrect label h(xi) (cid:54)= yi incurs a penalty.
(cid:80)n
The empirical risk minimizing predictor is given by h∗ := arg minh∈H

i=1 (cid:96)(h(xi), yi) where
hypothesis h : X → R gives a learner’s model, the loss function (cid:96) : R × {−1, +1} → R gives the
penalty incurred by a prediction, and H is the hypothesis class under the learner’s consideration. In
this paper, we will mainly consider H to be the class of separators based on hyperplane boundaries
hθ(x) = θ(cid:124)x + b with x, θ ∈ Rd and b ∈ R. For binary classiﬁcation, the learner issues a prediction
hM L(x) = sgn(hθ(x)).

Notions of fairness have been formalized in a variety of ways in the machine learning literature.
Though Dwork et al.’s initial conceptualization remains prominent and inﬂuential [4], much recent
work has deﬁned fairness as a parity notion applied across diﬀerent protected class groups [3, 7, 8,
5, 18, 19]. The following deﬁnition gives the general form of these types of fairness criteria.

1Though individuals in a dataset will typically be coded with many protected class attributes, in this paper we

will consider only a single sensitive attribute of focus.

3

Deﬁnition 1. A classiﬁer h satisﬁes a general group-based notion of (cid:15)-fairness if

|E[g((cid:96), h, xi, yi)|Ezi=1] − E[g((cid:96), h, xi, yi)|Ezi=0]| ≤ (cid:15)

(1)

where g is some function of classiﬁer h performance, and Ezi=0 and Ezi=1 are events that occur
with respect to groups z = 0 and z = 1 respectively.

Further speciﬁcations of the function g and the events E instantiate particular group-based
fairness notions. For example, when g((cid:96), h, xi, yi) = h(xi) and E refers to the events in which
yi = 1 for each group, Deﬁnition 1 gives an (cid:15)-approximation of equality of opportunity [3]. When
g((cid:96), h, xi, yi) = (cid:96)(h(xi), yi) and E refers to all events for each group, Deﬁnition 1 gives the notion of
(cid:15)-approximation of overall error rate balance [7]. Notice that as (cid:15) increases, the constraint loosens,
and as (cid:15) decreases, the fairness constraint becomes more strict.

In the Planner’s Problem, a Planner maximizes a social welfare functional (SWF) given as a
weighted sum of individual utilities, W = (cid:80)n
i=1 wiui. An individual i’s contribution to society’s
total welfare is a product of her utility ui and her social weight wi ∈ [0, 1] normalized so that
(cid:80)n
i wi = 1. Utility functions ui : X → R+ assign positive utilities to a set of attributes or goods
xi. We suppose a utility function is everywhere continuous and diﬀerentiable with respect to its
inputs.

Since a Planner who allocates a resource h impacts her recipients’ utilities, she solves hSW F (x; w) :=

(cid:80)n

i=1 wiu(xi, hi) under a budget constraint: (cid:80)n

i=1 hi ≤ B. Since we consider cases of
arg maxh
social planning in which a desirable good is being allocated, it is natural to suppose that u is
strictly monotone with respect to h. As is common in welfare economics, we take u to be concave
in h, so that receiving the good exhibits diminishing marginal returns. Further, we require that
the social welfare functional W be symmetric: W (h; x, w) = W (σ(h); σ(x), σ(w)) for all possible
permutations of σ(·). This property implies that the utility functions in the Planner’s problem are
not individualized. In the case of binary classiﬁcation, the Planner decides whether to allocate the
discrete good to individual i or not (hi ∈ {0, 1}).

3 Correspondence between Loss Minimization and Social

Welfare Maximization

To highlight the correspondence between the machine learning and welfare economic approaches to
social allocation, we ﬁrst show that we can understand loss minimizing solutions to also be welfare
maximizing ones, albeit under a particular instantiation of the social welfare function. Since social
welfare is given as the weighted sum of individuals’ utilities, it is clear that manipulating weights w
signiﬁcantly alters the Planner’s solution. Thus just as we can compute optimal allocations under
a ﬁxed set of welfare weights, we can also begin with an optimal allocation and ﬁnd welfare weights
that would support them. In welfare economics, the form of w corresponds to societal preferences
about what constitutes a fair distribution. For example, the commonly-called “Rawlsian” social
welfare function named after political philosopher John Rawls, can be written as WRawls = mini ui
where ui gives the utility of individual i. This function is equivalent to the general form (cid:80)n
i=1 wiui
where the individual i with the lowest utility ui has welfare weight wi = 1 and all individuals
k (cid:54)= i have weight wk = 0. On the other hand, the commonly-called “Benthamite” social welfare
function named after the founder of utilitarianism Jeremy Bentham, aggregates social welfare such
that an extra unit of utility contributes equally to the social welfare regardless of who receives it.
Benthamite weights are equal across all individuals: wi = 1

n for all i ∈ [n].

Thus associating an optimal (possibly fairness constrained) loss minimizing allocation with a
set of welfare weights that would make it socially optimal lends insight into how socially “fair”
a classiﬁcation is from a welfare economic perspective. The following Proposition formally states
this correspondence between loss minimization and social welfare maximization.

Proposition 1. For any vector of classiﬁcations hM L(xi) that solves a loss minimization task,
there exists a set of welfare weights w with (cid:80)n
i=1 wi = 1 such that the Planner who maximizes
social welfare W with a budget B selects an optimal allocation hSW F (xi) = hM L(xi) for all i ∈ [n].

Proof. First, we know that since W (x, w) is a weighted sum of functions u, which are concave in
h, the Planner can indeed ﬁnd a social welfare maximizing allocation hSW F . Let hM L(x) be the

4

empirical loss-minimizing classiﬁer for {xi, zi, yi}n
the social welfare maximization problem to ﬁnd the weights that w support them.

i=1. With these allocations given, we can invert

For a given utility function u, we evaluate ∂u(x,h)

∂h

= mi ∀i ∈ [n], which gives the

(cid:12)
(cid:12)
(cid:12){xi,hM L(xi)}

marginal gain in utility for individual i from having received an inﬁnitesimal additional allocation
of h. Notice that at a welfare maximizing allocation h, we must have that

wi

∂u(x, h)
∂h

(cid:12)
(cid:12)
(cid:12){xi,hi}

= wj

∂u(x, h)
∂h

(cid:12)
(cid:12)
(cid:12){xj ,hj }

for all i, j ∈ [n]

(2)

When the allocation hM L(x) has been ﬁxed, we must have that wimi = wjmj = k, where the
constant k is set by the Planner’s budget B, for all i, j along with (cid:80)n
i=1 wi = 1. Since u is
strictly monotone with respect to h, mi > 0 for all i. We thus have a non-degenerate system of n
equations with n variables, and there exists a unique solution of welfare weights w that support
the allocation.

Note that in the case of binary classiﬁcation hM L(x) ∈ {−1, +1, }, so allocations are not
awarded at a fractional level. Thus rather than the partial ∂u(x,h)
, the Planner must consider the
margin gain of receiving a positive classiﬁcation. Nevertheless, Proposition 1 still holds, and the
proof carries through with ∆u(x, h(x)) = u(x, 1) − u(x, 0) in place of partial derivatives ∂u(x,h)

∂h

.

The equations given in (2) set an optimality condition for the Planner. Its structure, though
simple, reveals that welfare weights must be inversely proportional to an individuals’ marginal
utility gain from receiving an allocation. This result is formalized in the Proposition below.

∂h

(cid:80)n

Proposition 2. For any set of optimal allocations h = arg maxh
i=1 ¯wiu(xi, hi) with strictly
monotonic utility function u concave in h, the supporting welfare weights have the form ¯wi = k
mi
where mi = ∂u(xi)

∂h |{xi,hi} and k > 0 is a constant set by the Planner’s budget B = (cid:80)n
By associating a set of classiﬁcation outcomes with a set of implied welfare weights, one can
inquire about the social fairness of the allocation scheme by investigating the distribution of welfare
weights across individuals or across groups. While there may not be a single distribution of welfare
weights that can be said to be “most fair,” theoretical and empirical work in economics has been
conducted on the range of fair distributions of societal weights [23, 25]. This research has considered
weights as implied by current social policies [22, 29, 30], philosophical notions of justice [31, 24],
and individuals’ preferences in surveys and experiments [22, 32, 25]. They thus oﬀer substantive
notions of fairness currently uncaptured by many current algorithmic fairness approaches.

i=1 hi.

4 An Algorithm that records all Possible Labelings

In the previous section, we showed that for any vector of classiﬁcations, one can compute the
implied societal welfare weights of the generic SWF that would yield the same allocations in the
Planner’s Problem. In this section, we work in the converse direction: Beginning with a Planner’s
social welfare maximization problem, does there exist a classiﬁer hM L ∈ H that generates the same
classiﬁcation as the Planner’s optimal allocation such that for all i ∈ [n], hM L(xi) = hSW F (xi)?

We answer this question for the hypothesis class of linear decision boundary-based classiﬁers by
providing an algorithm that accomplishes a much more general task: Given a set X , containing n
d-dimensional nondegenerate data points x ∈ Rd, our algorithm enumerates all linearly separable
labelings and can output a hyperplane parameterized by θ ∈ Rd and b ∈ R that achieves that set
of labels. In order to build intuition for its construction, we ﬁrst consider a hyperplane separation
technique that applies to a very speciﬁc case: a case in which a hyperplane separates sets A and
B, intersecting A at a single point and intersecting B at d − 1 points.

Lemma 1. Consider linearly separable sets A and B of points x ∈ Rd. For any d − 1-dimensional
hyperplane hV with hV ∩ A = v and hV ∩ B = P where |P | = d − 1 that separates A and B into
closed halfspaces ¯h+
V , one can construct a d − 1-dimensional hyperplane h that separates A
and B into open halfspaces h+ and h−.

V and ¯h−

Because its techniques are not of primary relevance for this Section, we defer the full proof of
this Lemma to the Appendix but provide a brief exposition. The construction on which the Lemma
relies is a “pivot-and-translate” maneuver. A hyperplane as described can separate points in open

5

ALGORITHM 1: Record all possible labelings on a dataset X by linear separators
Input: Set X of n data points x ∈ Rd
Output: All possible partitions A, B attainable via linear separators; supporting hyperplane h
for all V ⊂ X with |V | = d do

Construct d − 1-dimensional hyperplane hV deﬁned by v ∈ V ;
for each point v ∈ V do

P = V \ v;
h = pivot(hV , P, v) ;
h = translate(h, v) ;
Record A = {x|x ∈ h+}, B = {x|x ∈ h−}, h;

// hV pivots around the d − 2-dimensional plane P away from v
// h translates toward v

end

end

halfspaces by ﬁrst pivoting (inﬁnitesimally) on a d − 2-dimensional facet P of a convex hull C(B)
away from v ∈ C(A) and then translating (inﬁnitesimally) back toward v and away from C(B).
We show that all separable convex sets can be separated by such a hyperplane and procedure.

Note that since we seek enumerations of all labelings achievable by a linear separator on a given
dataset, we are not a priori given convex hulls to separate. That is, we want to know which points
can be made into distinct convex hulls and which cannot. Thus we take the preceding procedure
and invert it—the central idea is to begin with the separators and from there, search for all possible
convex hulls: Beginning with an arbitrary d−1-dimensional hyperplane h deﬁned by d data points,
we construct convex hulls out of the points in each halfspace created by h. Then we can use the
pivot-and-translate procedure to construct a separation of the two sets into two open halfspaces.
We must show that such a procedure is indeed exhaustive.

Theorem 1. Given a dataset X consisting of n nondegenerate points x ∈ Rd, Algorithm 1 enu-
merates all possible labelings achievable by a d − 1-dimensional hyperplane in O(ndd) time and
outputs hyperplane parameters (θ, b) that achieve each one.

Proof. We have already shown that the pivot-and-translate construction is suﬃcient to linearly
separate two sets A and B in the very speciﬁc case given in Lemma 1. But we must prove
that all linearly separable sets can be constructed via Algorithm 1. We prove it is exhaustive by
contradiction.

Suppose there exists a separation of X that is not captured by Algorithm 1. Then there exists
disjoint sets A and B such that their convex hulls C(A) and C(B) do not intersect. By the
hyperplane separating theorem, there exists a d − 1-dimensional hyperplane hV1 that separates
A and B, deﬁned by a set V1 of d vertices v, at least one of which is on the boundary of each
convex hull. Without loss of generality, we assume that for all x ∈ A, x ∈ h+
and for all x ∈ B,
V1
x ∈ h−
. Notice that this hyperplane is indeed “checked” by the Algorithm, and this hyperplane
V1
hV1 correctly separates x ∈ X \ V1 into the two sets A and B. Thus if the separation is not
disclosed via the procedure, the omission must occur due to the pivot-and-translate procedure’s
being incomplete.

In Algorithm 1, the set V1 is partitioned so that V1 = vf,1 ∪ P1 where vf,1 is the “free vertex”
and P1 is the pivot set consisting of d − 1 vertices. This partition occurs d times so that each vertex
v ∈ V1 has its turn as the “free vertex.” Thus we can view the pivot-and-translate procedure as
constituting a second partition—a partition of the d vertices that deﬁne the initial separating
hyperplane. By contradiction, we claim that there exists a partition D1, E1 ⊂ V1 such that
(cid:96) E1 = V1 where D1 ⊂ A and E1 ⊂ B that is unaccounted for in the d pivot-and-translate
D1
operations applied to hV1. Thus |D1|, |E1| ≥ 2. We use a “gift-wrapping” argument, a technique
common in algorithms that construct convex hulls, to show that the partition A and B is indeed
covered by Algorithm 1.

Select v ∈ D1 to be the free vertex vf,1, and let the pivot set P1 = V1 \ vf,1. We pivot
around P1 and away from vf,1 so that vf,1 ∈ h+
. Rotations in d-dimensions are precisely deﬁned
V1
as being around d − 2-dimensional planes. Thus pivoting around the ridge P1 away from vf,1 is
a well-deﬁned rotation in Rd. Since hV1 is a supporting hyperplane to C(B), E1 constitutes a
|E1| − 1-dimensional facet of C(B). There exists a vertex vE ∈ C(B) such that E1 ∪ vE gives a
|E1|-dimensional facet of C(B). Let hV2 be deﬁned by the set V2 = P1 ∪ vE. hV2 continues to
correctly separate all x ∈ X \ V2.

6

We once again partition V2 into sets D2 and E2 whose members must be ultimately classiﬁed
in sets A and B respectively. Notice that |D2| = |D1| − 1, since hV2 correctly classiﬁes vf,1 as
belonging to set A. Thus with each iteration of the pivot procedure, the separating classiﬁer
unhinges from a vertex in C(A) and “wraps” around C(B) just as in the gift wrapping algorithm
to attach onto another vertex in C(B). At each step, the hyperplane deﬁned by d vertices continues
to support and separate C(A) and C(B). Thus process iterates until in the |D1| − 1-th round,
(cid:12)
the hyperplane hV|D1|−1 has partition D|D1|−1 and E|D1|−1 with (cid:12)
(cid:12) = 1. Applying the full
pivot-and-translate procedure ensures the desired separation of sets A and B into open halfspaces.
Thus starting from a separable hyperplane deﬁned by d vertices on the convex hulls C(A) and
C(B), which must exist in virtue of the separability of sets A and B, we were able to use the
pivot procedure in order to “gift-wrap” around one convex hull until we arrived at a d-dimensional
separating hyperplane with only one vertex vf ∈ C(A). This hyperplane is obviously checked by
the ﬁrst for-loop of Algorithm 1. The subsequent for-loop that performs the second partition of
the d vertices into the free vector vf and the pivot set P then directly applies and performs the
pivot-and-translate procedure given in Algorithm 1 to achieve the desired separation.

(cid:12)D|D1|−1

Degeneracies in the dataset can be handled by combining Algorithm 1 with standard solutions
to degeneracy problems in geometric algorithms, which perform slight perturbations to degenerate
data points to transform them into nondegenerate ones [33].
In concert with these solutions,
Algorithm 1 automatically reveals which social welfare maximization solutions are attainable on a
given dataset X via hyperplane-based classiﬁcation and the 0 − 1 accuracy loss each entails.

5 Sensitivity Analysis of Fairness Constraints

In this Section, we perform welfare-minded sensitivity analyses on the standard empirical risk
minimization (ERM) program with fairness constraints. Assuming, as before, that an individual
beneﬁts from receiving a positive classiﬁcation, we deﬁne group utilities as

W0 =

1
n0

(cid:88)

i|zi=0

h(xi) + 1
2

,

W1 =

1
n1

(cid:88)

i|zi=1

h(xi) + 1
2

(3)

where n0 and n1 give the number of individuals in groups z = 0 and z = 1 respectively.

First, we present an instantiation of the (cid:15)-fair ERM problem with a fairness constraint proposed
in prior work in algorithmic fairness. We work from a Soft-Margin SVM program and derive the
various dual formulations that will be of use in the following analyses. In Section 5.2, we move on
to show how ∆(cid:15) perturbations to the fairness constraint yield changes in classiﬁcation outcomes
for individuals and by extension, how they impact a group’s overall welfare. Standard sensitivity
analyses show how the objective value changes as constraints are tightened and loosened, but
they are unable to show how classiﬁcations themselves are aﬀected by a changing constraint. Our
approach, which draws a connection between fairness perturbations and searches for an optimal
SVM regularization parameter, tracks changes in an individual’s classiﬁcation by taking advantage
of codependence of variables in the Dual of the SVM. By perturbing the fairness constraint, we
observe changes in not its own corresponding Dual variable but in the corresponding Dual of the
margin constraints, which relay the classiﬁcation fates of data points. Via this technique, we plot
the full “solution paths” of the Dual variable as a function of (cid:15) and as a result, we compute group
welfares as a function of (cid:15). We close this Section by working from the shadow price of the fairness
constraint to derive local and global sensitivities of the optimal solution to ∆(cid:15) perturbations.

Our results show that tightening a fairness constraint leads to idiosyncratic changes to individ-
uals’ classiﬁcation fates. We show that requiring a classiﬁer to abide by a stricter fairness standard
does not necessarily lead to improved outcomes for the disadvantaged group. Our results indicate
that preferring a classiﬁer that emits narrower parity disparities can lead to choosing outcomes that
are actually Pareto dominated by seemingly “less fair” alternatives. In these cases, the machine
learning goal of ensuring group-based fairness is incompatible with the Pareto Principle.

Deﬁnition 2 (Pareto Principle). Let x, y be two social alternatives. Let (cid:23)1, ..., (cid:23)n be the preference
ordering of individuals i ∈ [n] and let (cid:23)P be the preference ordering of a Planner who maximizes
social welfare. A Planner abides by the Pareto Principle if x (cid:23)P y whenever x (cid:23)i y for all i.

7

In welfare economics, the Pareto Principle is a standard requirement of social welfare functionals—

it would appear that the intentional implementation of an allocation that is Pareto dominated by
an available alternative would be undesirable and even irresponsible! Nevertheless we show that
in many cases, applying fairness criteria to loss minimization tasks do just that. For the sake of
clarity in exposition and greater continuity with previous literature, we conduct our analysis with
respect to the Soft-Margin SVM optimization problem, however the analyses and results in this
Section can be applied to fairness-constrained convex loss minimization programs more generally.

5.1 Setting up the (cid:15)-fair ERM program

The general fairness-constrained empirical loss minimization program can be written as

minimize
h ∈ H

(cid:96)(h(x), y)

subject to fh(x, y) ≤ (cid:15)

(4)

where (cid:96)(h(x), y) gives the empirical loss of a classiﬁer h ∈ H on the dataset X . To maximize
accuracy, the learner ought to minimize 0-1 loss; however because the function ((cid:96)0−1) is non-
convex, it is diﬃcult to minimize. A convex surrogate loss such as hinge loss ((cid:96)h) or log loss ((cid:96)log)
is frequently substituted in its place to ensure that globally optimal solutions may be eﬃciently
found. fh(x, y) ≤ (cid:15) gives a group-based fairness constraint of the type given in Deﬁnition 1. (cid:15) > 0
is the unfairness “tolerance parameter”—a greater (cid:15) permits greater group disparity on a metric
of interest; a smaller (cid:15) more tightly restricts the level of permissible disparity.
We examine the behavior of fairness-constrained linear SVM classiﬁers.

In particular, our
learner minimizes hinge loss with L1 regularization; equivalently she, seeks a Soft-Margin SVM
that is “(cid:15)-fair.” The fair empirical risk minimization program that will be of central interest is
given as

minimize
θ, b

(cid:107)θ(cid:107)2 + C

1
2

n
(cid:88)

i=1

ξi

subject to yi(θ(cid:124)xi + b) − 1 + ξi ≥ 0,
ξi ≥ 0,
fθ,b(x, y) ≤ (cid:15)

((cid:15)-fair Soft-SVM)

where the learner seeks linear hyperplane parameters θ, b; ξi are non-negative slack variables that
violate the margin constraint in the Hard-Margin SVM problem yi(θ(cid:124)xi + b) − 1 ≥ 0, and C > 0
is a hyperparameter tunable by the learner to optimize the trade-oﬀ between preferring a larger
margin and penalizing violations of the margin.

The abundant literature on algorithmic fairness presents a long menu of options for the various
forms that fθ could take, but generally speaking, the constraints are non-convex and and thus
require other methods of training classiﬁers that deviate from directly pursuing eﬃcient fairness
constraint-based convex programming methods [9, 6, 17, 19, 5]. In response, researchers have de-
vised convex proxy alternatives, which have been shown to approximate the results of the original
fairness constraints well [11, 18, 34]. Since we will use the well-tread machinery of convex opti-
mization, we primarily work with these convex fairness constraints. In particular, we will work
[11] which disallows disparities in covariance
with the proxy constraint proposed by Zafar et al.
between group membership and the (signed) distance between individuals’ feature vectors and the
hyperplane decision boundary that exceed (cid:15). The fairness constraint is written as

fθ,b =

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

i=1

(cid:12)
(cid:12)
(zi − ¯z)(θ(cid:124)xi + b)
(cid:12)
(cid:12)
(cid:12)

≤ (cid:15)

(5)

where ¯z reﬂects the bias in the demographic makeup of X : ¯z = 1
n

(cid:80)n

i=1 zi. Let ((cid:15)-fair-SVM1-P) be

8

the Soft-Margin SVM program with this covariance constraint. The corresponding Lagrangian is

LP (θ, b, ξ, λ, µ, γ1, γ2) =

(cid:107)θ(cid:107)2 + C

1
2

n
(cid:88)

i=1

ξi −

n
(cid:88)

i=1

λi −

n
(cid:88)

i=1

µi(yi(θ(cid:124)xi + b) − 1 + ξi)

− γ1

(cid:0)(cid:15) −

1
n

n
(cid:88)

(zi − ¯z)(θ(cid:124)xi + b)(cid:1) − γ2

i=1

(cid:0)(cid:15) −

1
n

n
(cid:88)

(¯z − zi)(θ(cid:124)xi + b)(cid:1)

i=1

((cid:15)-fair-SVM1-L)

where θ ∈ Rd, b ∈ R, ξ ∈ Rn are Primal variables. The (non-negative) Lagrange multipliers
λ, µ ∈ Rn correspond to the n non-negativity constraints ξi ≥ 0 and the margin-slack constraints
yi(θ(cid:124)xi + b) − 1 + ξi ≥ 0 respectively. The multipliers γ1, γ2 ∈ R correspond to the two linearized
forms of the absolute value fairness constraint. By complementary slackness, dual variables reveal
information about the satisfaction or violation of their corresponding constraints. The sensitivity
analyses in the subsequent two subsections will focus on these interpretations.

By the Karush-Kuhn-Tucker conditions, at the solution of the convex program, the gradients

of L with respect to θ, b, and ξi are zero. Plugging in these conditions, the Dual Lagrangian is

LD(θ, ξ, λ, µ, γ1, γ2) = −

1
2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

µiyixi −

γ
n

n
(cid:88)

i=1

(zi − ¯z)xi

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

+

n
(cid:88)

i=1

µi − |γ|(cid:15)

(6)

where γ = γ1 − γ2. Thus the Dual maximizes this objective subject to the constraints µi ∈ [0, C]
for all i and (cid:80)

i=1 µiyi = 0. We thus derive the full Dual problem

−

1
2

maximize
µ, γ

subject to

n
(cid:88)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

µiyixi −

i=1
µi ∈ [0, C],

γ
n

n
(cid:88)

i=1

(zi − ¯z)xi

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
i = 1, . . . , n,

+

n
(cid:88)

i=1

µi − V (cid:15)

((cid:15)-fair-SVM1-D)

n
(cid:88)

i=1

µiyi = 0,

γ ∈ [−V, V ]

where we have introduced the variable V to eliminate the absolute value function |γ| in the objec-
tive. Notice that when γ = 0 and neither of the fairness constraints bind, we recover the standard
dual SVM program. Since we are concerned with fairness constraints that alter an optimal solution,
we are interested in cases in which V is strictly positive. As such, we can rewrite the preceding as

µiyi(I − Pu)xi

maximize
µ,β−,β+

subject to

−

1
2

n
(cid:88)

n
(cid:88)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

i=1
µi ∈ [0, C],

µiyi = 0,

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

+

n
(cid:88)

i=1

µi +

2n (cid:80)

i µiyi(cid:104)xi, u(cid:105) + n2(β− − β+)

2(cid:107)u(cid:107)2

(β− − β+)

i = 1, . . . , n,

((cid:15)-fair SVM2-D)

i=1
β−, β+ ≥ 0,
β− + β+ = (cid:15)

where I, Pu ∈ Rd×d. The former is the identity matrix, and the latter is the projection matrix onto
the vector deﬁned by u = (cid:80)n
i=1(zi − ¯z)xi. As was also observed by Donini et al., the (cid:15) = 0 version
of ((cid:15)-fair SVM2-D) is thus equivalent to the standard formulation of the dual SVM program with
Kernel K(xi, xj) = (cid:104)(I − Pu)xi, (I − Pu)xj(cid:105) [18].

5.2 Sensitivity on Candidates

In this Section, we investigate the eﬀects of perturbing a ﬁxed (cid:15)-fair SVM by some ∆(cid:15) on the
classiﬁcation outcomes that are issued. We ask, “How are groups’ classiﬁcations, and thus their
utilities, impacted when a learner tightens or loosens her fairness constraint?” The insight is that

9

rather than perform sensitivity analysis directly on the Dual variable corresponding to the fairness
constraint—which, as we will see in Section 4.3, only gives information about the change in the
learner’s objective value—we track changes in the classiﬁer’s behavior by analyzing the eﬀect of
∆(cid:15) on another set of Dual variables: µi that correspond to the Primal margin constraints. We
harness techniques that have been used in ﬁnding SVM regularization solution paths to demarcate
the range of perturbations that yield outcomes that either improve or worsen a group’s utility
[27, 28, 26].

We show that perturbations to (cid:15) do not necessarily correspond to meaningful changes in group
utilities. We ﬁnd that decreasing (cid:15), which corresponds to making the Soft-Margin SVM “more
fair,” need not translate into improved utilities. In fact, in terms of welfare, we show that policies
that na¨ıvely prefer “more fair” classiﬁer solutions do not abide by the Pareto Principle deﬁned
in (2). Perturbations of (cid:15) to (cid:15) + ∆(cid:15) < (cid:15), which tighten the fairness constraint, do not generally
translate into improved outcomes for either of the two groups. And since a learner’s loss never
decreases when a fairness condition is made more strict and classiﬁer outcomes can make both
groups worse-oﬀ, then optimal SVM classiﬁers that are subject to more “unfair” constraint can
yield classiﬁcations that Pareto dominate those that arise under more “fair” conditions. That is,
every stakeholder group prefers the outcomes issued by the “unfair” classiﬁer.

Deﬁne a function p((cid:15)) : R → R that assigns the optimal value of the (cid:15)-fair loss minimizing
program ((cid:15)-fair-SVM1-P). We begin at a solution p((cid:15)) and consider classiﬁcations at the solution
p((cid:15) + ∆(cid:15)), where ∆(cid:15) can be either positive or negative. For clarity of exposition, we assume that
the positive covariance fairness constraint binds, and thus that γ = V > 0. This is without loss of
generalization—the same analyses apply when γ = V < 0. The Dual (cid:15)-fair SVM program is thus

minimize
µ

1
2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

µiyi(I − Pu)xi

subject to

µi ∈ [0, C],

n
(cid:88)

i=1

µiyi = 0

−

n
(cid:88)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
i=1
i = 1, . . . , n,

µi +

n(cid:15)(2 (cid:80)

i µiyi(cid:104)xi, u(cid:105) − n(cid:15))

2(cid:107)u(cid:107)2

((cid:15)-fair SVM-D)

At the optimal solution, the classiﬁcation fate of each data point xi is encoded in the dual variable
µ∗

i . Let D be the value of the objective function in ((cid:15)-fair SVM-D), then we have that

∂D
∂µ∗
j
∂D
∂µ∗
j
∂D
∂µ∗
j

> 0 −→ µ∗

j = 0, and j ∈ F

= 0 −→ µ∗

j ∈ [0, C], and j ∈ S

< 0 −→ µ∗

j = C, and j ∈ E

(7)

(8)

(9)

Partitioning the dataset X based on ∂D
at any optimal solution, xj are either free vectors (7),
∂µ∗
j
support vectors in the margin (8), or error vectors (9). To analyze the impact that applying a
fairness constraint has on a group’s welfare, we can track the behavior of ∂D
and observe how
∂µi
vectors’ membership in sets F, S, and E change under a perturbation to (cid:15). This information
will in turn reveal how classiﬁcations change or are stable upon tightening or loosening a fairness
constraint.

Fairness perturbations are not guaranteed to shuﬄe data points across the diﬀerent membership
sets F, S, and E. It is clear that for j ∈ {F, E}, so long as a perturbation does not cause ∂D
to
∂µ(cid:15)
j
ﬂip signs or to vanish, then j will belong to the same set and h(cid:15)(xj) = h(cid:15)+∆(cid:15)(xj) where h(cid:15)(xj)
gives the (cid:15)-fair classiﬁcation outcome for xj. In these cases, a candidate’s welfare is unaﬀected by
the change in the fairness tolerance level. In contrast, support vectors xj with j ∈ S are subject
to a diﬀerent condition to ensure that they stay in the margin: ∂D
= 0. So we have that
∂µ(cid:15)
i

= ∂D

∂µ(cid:15)+∆(cid:15)
i

∂D
∂µ(cid:15)
j

=

n
(cid:88)

i=1

µiyi(I − Pu)xiyj(I − Pu)xj +

n(cid:15)yj(cid:104)xj, u(cid:105)
(cid:107)u(cid:107)2

+ byj − 1 = 0

(10)

10

Let rj∆(cid:15) be the change in µj upon perturbing (cid:15) by ∆(cid:15), then we have

µ(cid:15)+∆(cid:15)
j

= µ(cid:15)

j + rj∆(cid:15)

(11)

where µ(cid:15)
j is the optimal µj value at the optimal solution p((cid:15)). Let r0 be the change in the oﬀset b;
then we can solve for rj ∈ Rn+1 for all unchanging xj ∈ S by taking the ﬁnite diﬀerence of (10)
with respect to a ∆(cid:15) perturbation,

n
(cid:88)

i=1

ri∆(cid:15)yiyj(cid:104)(I − Pu)xi, (I − Pu)xj(cid:105) + r0yj =

−nyj∆(cid:15)
(cid:107)u(cid:107)2

(cid:104)u, xj(cid:105)

It is clear that for all i ∈ {F, S}, the corresponding µ(cid:15)
i sensitivity to perturbations must have
ri∆(cid:15) = 0, so ri = 0 for all i. We can then simplify the previous expression by summing only over
those ri where i ∈ S.

(cid:88)

i∈S

ri∆(cid:15)yiyj(cid:104)(I − Pu)xi, (I − Pu)xj(cid:105) + r0yj =

−nyj∆(cid:15)
(cid:107)u(cid:107)2

(cid:104)u, xj(cid:105)

Thus rj can be found by inverting the matrix

K =



















0

y1
...

y2

y|S|

y1

y2

. . .

y|S|

yiyj(cid:104)(I − Pu)xi, (I − Pu)xj(cid:105)



















∈ R(|S|+1)×(|S|+1)

(12)

where indices are renumbered to reﬂect only those i, j ∈ S. This matrix is invertible so long
as the Kernel K(xi, xj) = (cid:104)(I − Pu)xi, (I − Pu)xj(cid:105) forms a positive deﬁnite matrix. Since the
objective function in ((cid:15)-fair SVM-D) is quadratic, then a suﬃcient condition for the Kernel matrix
to be invertible is that it is strictly convex—we assume this as a technical condition. Then the
sensitivities of µj for j ∈ S to ∆(cid:15) perturbations are given by

r = K −1(cid:16) −n

(cid:107)u(cid:107)2 v

(cid:17)

, where v =









0
...
yj(cid:104)u, xj(cid:105)
...









∈ R|S|+1

(13)

The sensitivities rj (cid:54)= 0 for j ∈ S do aﬀect the quantities ∂D
for all j ∈ [n], and thus we need
∂µj
additional conditions to hold to ensure that the vectors not on the margin are also unshuﬄed by
the fairness perturbation. Deﬁne

dj =

∂D
∂µj∂(cid:15)

=

(cid:88)

i∈S

riyiyj(cid:104)(I − Pu)xi, (I − Pu)xj(cid:105) + r0yj

(14)

and the quantity of interest for stability of vectors xj for j /∈ S is then given by

∂D
∂µ(cid:15)
j
dj

≷ 0

(15)

where > entails that j ∈ F and < entails that j ∈ E. Now we bound ∆(cid:15) such that no vectors are
shuﬄed across diﬀerent sets. It follows that perturbations in this range do not alter classiﬁcations.

11

Proposition 3. Let p((cid:15)) be the optimal (cid:15)-fair SVM loss and denote the optimal µ∗ at p((cid:15)) as
µ(cid:15). Let dj = ∂D
. All
perturbations of (cid:15) in the range ∆(cid:15) ∈ (cid:0) maxj mj, minj Mj

i yi(I − Pu)xiyj(I − Pu)xj + n(cid:15)yj (cid:104)xj ,u(cid:105)

∂µj ∂(cid:15) and gj = 1 −

(cid:107)u(cid:107)2 + byj

(cid:1) where

i=1 µ(cid:15)

(cid:16) (cid:80)n

(cid:17)

mj =






,

(cid:40) gj
dj
−∞,

min{
(cid:40)

−∞,
gj
,
dj

C−µ(cid:15)
j
rj

j ∈ F, dj > 0
j ∈ F, dj < 0
},

−µ(cid:15)
j
rj

,

j ∈ E, dj > 0
j ∈ E, dj < 0

j ∈ S

,

Mj =






(cid:40)

∞,
gj
,
dj
min{
(cid:40) gj
dj
∞,

,

j ∈ F, dj > 0
j ∈ F, dj < 0
−µ(cid:15)
C−µ(cid:15)
j
j
rj
rj
j ∈ E, dj > 0
j ∈ E, dj < 0

},

,

j ∈ S

(16)

yield no changes to memberships in the partition {F, S, E}.

We defer the interested reader to the Appendix for the full proof of this Proposition. The result
do not threaten i’s

ensure that i ∈ E stay in the same partition, but perturbations that increase ∂D
∂µ(cid:15)
i

follows from observing that for i ∈ F, any perturbations ∆(cid:15) that increase ∂D
∂µ(cid:15)
i
exiting F; if ∆(cid:15) leads to a decrease in ∂D
∂µ(cid:15)
i
decrease ∂D
can
∂µ(cid:15)
i
cause i to shuﬄe into S. Support vectors in the margin must maintain µ(cid:15)+∆(cid:15)
∈ [0, C]. Once µ(cid:15)
i
hits either endpoint of the interval, the vector xi risks shuﬄing across to F or E. Computing these
transition inequalities results in a set of conditions that ensure that a partition is stable. Since ∆(cid:15)
can be either positive or negative, we take the maximum of the lower bounds and the minimum of
the upper bounds to arrive at the range of stable perturbations given in (16).

, then i can enter S. Inversely, perturbations ∆(cid:15) that

i

This Proposition reveals a surprising ineﬀectiveness of fairness constraints. So long as the fair-
ness constraint is binding and its associated dual variable γ > 0, then tightening or loosening a
fairness constraint does alter the loss of the optimal learner classiﬁer—the actual SVM solution
changes—yet analyzed from the perspective of the individual agents xi, so long as the ∆(cid:15) per-
turbation occurs within the range given by (16), classiﬁcations issued under this (cid:15) + ∆(cid:15)-fair SVM
solution are identical to those under the (cid:15)-fair solution. Thus despite the apparent more ‘fair”
signal that a classiﬁer abiding by (cid:15) + ∆(cid:15) sends, agents are no better oﬀ in terms of welfare. This
result is summarized in the following Corollary.

Corollary 1. Let {p((cid:15)), W0((cid:15)), W1((cid:15))} be a triple expressing the utilities of the learner, group
z = 0, and group z = 1 under the (cid:15)-fair SVM solution. Then for any ∆(cid:15) ∈ (maxj mj, 0) where mj
is deﬁned in (16), {p((cid:15)), W0((cid:15)), W1((cid:15))} (cid:37) {p((cid:15) + ∆(cid:15)), W0((cid:15) + ∆(cid:15)), W1((cid:15) + ∆(cid:15))}.

By demarcating the limits of ∆(cid:15) perturbations that yield no changes to the sets F, S, E, we can
move on to consider the eﬀects of perturbations ∆(cid:15) that exceed the stable region given by (16).
There are four ways that vectors can be shuﬄed across the partition:

1. j ∈ E (cid:15) moves into S (cid:15)+∆(cid:15)

2. j ∈ F (cid:15) moves into S (cid:15)+∆(cid:15)

3. j ∈ S (cid:15) moves into F (cid:15)+∆(cid:15)

4. j ∈ S (cid:15) moves into E (cid:15)+∆(cid:15)

At each “breakpoint” event when ∆(cid:15) reaches maxj mj or minj Mj, the set S changes, and rj
for all j ∈ S must be recomputed via (13). The new rj sensitivities hold until the next breakpoint.

Lemma 2. µ(cid:15)

i for all i ∈ [n] are piecewise linear in (cid:15).

We defer the full proof to the Appendix but provide a brief exposition of the result. If perturba-
tions ∆(cid:15) are in the stable region given in (16), then for all µj((cid:15)) with j ∈ {F, E}, µj((cid:15)) = µj((cid:15)+∆(cid:15)).
For points xj that are in the margin and thus j ∈ S, µj((cid:15) + ∆(cid:15)) = µj((cid:15)) + rj∆(cid:15). Since index
transitions at the breakpoint only occur by way of the margin, showing that the µi((cid:15)) paths are

12

ALGORITHM 2: Sensitivity Analysis of Group Welfares to Changing (cid:15)
Input: set X of n data points {xi, zi, yi}
Output: solutions paths µ((cid:15)) and group welfare curves {W0((cid:15)), W1((cid:15))}
µ0 = arg minµ D(µ) of (0-fair SVM-D);
F = ∅, S = ∅, E = ∅;
(cid:15) = 0, ∆(cid:15) = 0;
while (cid:15) < 1 do
for each µ(cid:15)

i do

update F, S, E according to (7), (8), (9)

end
compute r, d according to (13), (14);
∆(cid:15) = mini Mi as given in (16);
µ(cid:15)+∆(cid:15)
i + ri∆(cid:15) for i ∈ S, µ(cid:15)
= µ(cid:15)
i
(cid:15) = (cid:15) + ∆(cid:15);
{W0((cid:15)), W1((cid:15))} = gp welf are(µ, y, z) ;
return ((cid:15), µ)

i = µ(cid:15)+∆(cid:15)
i

end

for i ∈ F, E;

// calls Algorithm 3 in Appendix to compute group welfare

continuous is suﬃcient in order to conclude that they are piecewise linear.

By parameterizing dual variables µj((cid:15)), we can associate a group utility with the optimal
classiﬁcation scheme of each of the ∆(cid:15) perturbation breakpoints. As already illustrated, partitions
are static in the stable regions around each breakpoint, so group utilities will also be unchanged
in these regions. As such, we can directly compare group utilities at neighboring breakpoints. Of
the four possible events that occur a breakpoint, index transitions between the partitions S and
E correspond to changed classiﬁcations that thus aﬀect group utilities. The following Proposition
characterizes those breakpoint transitions that eﬀect utility triples for group A, group B, and the
learner {p((cid:15)), W0((cid:15)), W1((cid:15))} that are strictly Pareto dominated by the utility triple supported at a
neighboring (cid:15) breakpoint. The full proof is left to the Appendix.

Proposition 4. Consider the utility triple at the optimal (cid:15)-fair SVM solution given by {p((cid:15)), W0((cid:15)), W1((cid:15))}.
Let bL = maxj mj < 0 be the neighboring lower breakpoint, and let bU = minj Mj > 0 be the neigh-
boring upper breakpoint. If bL = gj
where j ∈ S (cid:15) and
dj
yj = +1, then

where j ∈ E (cid:15) and yj = −1 or if bL =

C−µ(cid:15)
j
rj

{p((cid:15) + bL), W0((cid:15) + bL), W1((cid:15) + bL)}} ≺ {p((cid:15)), W0((cid:15)), W1((cid:15))}

Let bU = gj
dj

where j ∈ E (cid:15) and yj = +1 or if bU =

C−µ(cid:15)
j
rj

where j ∈ S (cid:15) and yj = −1, then

{p((cid:15) + bU ), W0((cid:15) + bU ), W1((cid:15) + bU )} (cid:31) {p((cid:15)), W0((cid:15)), W1((cid:15))}

Since (cid:15) breakpoints allow the comparison of group utilities that result from various (cid:15)-fair SVM
solutions, we can track the solution paths of the µi((cid:15)) for all individuals i in a group z in order to
construct a single curve of group z’s welfare that is also parameterized by the fairness tolerance
level (cid:15). Algorithm 2 and Algorithm 3 (in the Appendix) gives an implementation that constructs
solution paths µi((cid:15)) and outputs the curves tracking group welfare.

We thus ﬁnd that minimizing loss in the presence of stricter fairness constraints does not cor-
respond to monotonic gains or losses in the welfare level of candidate groups. As a result, fairness
perturbations do not have a straightforwardly predictable eﬀect on classiﬁcation decisions at all!
Further, these results do not only arise as an unfortunate outcome of using the particular proxy
fairness constraint suggested by Zafar et al [11]. In fact, so long as the (cid:15) parameter appears in the
linear part of the dual soft-margin SVM objective function, the µ paths will exhibit a piecewise
linear form that can be similarly characterized by stable regions and breakpoints. Thus these
results apply to many of the risk minimization programs subject to proxy fairness criteria that
have been proposed in the literature [18, 34, 11]. Further, even when the dual variable paths µi((cid:15))
are not piecewise linear, so long as they are non-monotonic, fairer classiﬁcation outcomes do not
necessarily confer welfare beneﬁts to the disadvantaged group.

The preceding analyses show that although fairness constraints are often intended to improve
classiﬁcation outcomes for some disadvantaged group, they in general do not abide by the Pareto

13

Principle, a common welfare economic axiom for deciding among social alternatives. That is, ask-
ing that an algorithmic procedure abide by a more stringent fairness criteria can lead to enacting
classiﬁcation schemes that actually make every stakeholder group worse-oﬀ, both social groups as
well as the learner. Here, the supposed “improved fairness” achieved by decreasing the unfair-
ness tolerance parameter (cid:15) fails to translate into any meaningful improvements in the number of
desirable outcomes issued to members of either group.

Theorem 2. Consider two fairness-constrained ERM programs parameterized by (cid:15)1 and (cid:15)2 where
(cid:15)1 < (cid:15)2. Then a decision-maker who always prefers the classiﬁcation outcomes issued under the
“more fair” (cid:15)1-fair solution to those under the “less fair” (cid:15)2-fair solution does not abide by the
Pareto Principle.

Figure 1: Sensitivity analysis of (cid:15)-fair SVM-solution on Adult dataset. Increasing (cid:15) from left to
right loosens fairness constraint, and classiﬁcation outcomes become “less fair.” Paths level oﬀ at
(cid:15) ≈ 0.175 when constraint ceases to bind at the optimal solution. Top Panel: Example “paths”
of dual variables µi as a function of (cid:15). For µi = 0, i ∈ F (correctly labeled); for µi ∈ (0, 1), i ∈ S
(correctly labeled, in margin); for µi = 1, i ∈ E (incorrectly labeled). Paths are piecewise linear,
show changes in classiﬁcation of xi as (cid:15) changes. Solid paths are coded female; dotted paths are
coded male. Bottom Panel: Relative group-speciﬁc welfare change at (cid:15)-fair SVM solution given
as percentage of group welfare at the unconstrained SVM solution. Corresponding plot of absolute
welfare changes is given in the Appendix. Non-monotonicity of welfare curves shows that always
preferring more fair solutions does not abide by the Pareto Principle. Green dashed vertical lines
to the left of purple ones give examples of classiﬁcations that are Pareto-dominated but “more
fair.”

5.3 Sensitivity Analysis on Learner Optimal Value

Having proven the main welfare-relevant sensitivity result for groups, we return to conduct more
standard analysis of the eﬀect of ∆(cid:15) perturbations on the learner’s loss. Recall that in this case,
we directly inquire the Dual variable of the fairness constraint. Solving for γ∗ in ((cid:15)-fair SVM2-D)
yields

γ∗ =

n(n(β− − β+) + (cid:80)n
(cid:107)u(cid:107)2

i=1 µiyi(cid:104)xi, u(cid:105))

(17)

By complementary slackness, one of β− and β+ is zero, while the other is equal to (cid:15). In particular,
if β− = 0, then β+ = (cid:15), and we know that γ = V > 0. Thus the original fairness constraint that
binds is the upper bound on covariance, suggesting that the optimal classiﬁer must be constrained
to limit its positive covariance with group z = 1. Similarly, if β+ = 0, then γ = −V < 0, and the
optimal classiﬁer must be constrained to limit its positive covariance with group z = 0.

We can interpret the value of the Dual variable Lagrange multiplier γ∗ given in (17) as the
shadow price of the fairness constraint. It gives the additional loss in accuracy that the learner

14

would achieve if the fairness constraint were inﬁnitesimally loosened. Whenever a fairness con-
straint binds, its shadow price is readily computable and is given by

|γ| =

n|n(cid:15) + (cid:80)n

i=1 µiyi(cid:104)xi, u(cid:105)|
(cid:107)u(cid:107)2

(18)

It bears noting that because ((cid:15)-fair Soft-SVM) is not a linear program, (18) can onl be inter-
preted as a measure of local sensitivity, valid only in a small neighborhood around an optimal
solution. But through an alternative lens of sensitivity analysis, we can derive a lower bound on
global sensitivity due to changes in the fairness tolerance parameter (cid:15). By writing (cid:15) as a perturba-
tion variable, we can perform sensitivity analysis on the same (cid:15)-constrained problem. Returning
to the perturbation function p((cid:15)), we have

where L(µ∗, γ∗) gives the optimal solution to the SVM problem with (cid:15) = 0:

p((cid:15)) ≥ sup
µ,γ

{L(µ∗, γ∗) − (cid:15)|γ∗|}

L(µ∗, γ∗) = max

µ∈[0,C]n,γ

−

1
2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

µiyi(I − Pu)xi

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

+

(cid:88)

i=1

µi

(19)

(20)

The perturbation formulation given in (19) is identical in form to the original program ((cid:15)-fair-
SVM1-P) but gives a global bound on p((cid:15)) for all (cid:15) ∈ [0, 1]. Since (19) gives a lower bound, the
global sensitivity bound yields an asymmetric interpretation.

Proposition 5. If ∆(cid:15) < 0 and |γ∗| (cid:29) 0, then p((cid:15) + ∆(cid:15)) − p((cid:15)) (cid:29) 0. If ∆(cid:15) > 0 and |γ∗| is small,
then p((cid:15) + ∆(cid:15)) − p((cid:15)) < 0 but small in magnitude.

Proposition 5 reveals that tightening the fairness constraint when the shadow price of the
fairness constraint is high leads to a great increase in vendor loss, but loosening the fairness
constraint when the shadow price is small leads only to a small decrease in loss.

6 Discussion

As algorithmic systems increasingly make life-shaping social and economic decisions, researchers
in machine learning must reevaluate both their lodestars of optimality and eﬃciency as well as
their latest metrics of fairness. Since notions of fairness are invariably context-dependent and
always informed by background normative views, it is unsurprising that there has been such wide
disagreement within the community about which of the many fairness deﬁnitions is the “right”
one. In reality, the search space is much larger, and there is no objective winner.

This paper does not look to oﬀer another fairness deﬁnition. Instead, by viewing classiﬁcation
outcomes as allocations of a good, we incorporate considerations of individual and group utility in
our analysis of classiﬁcation regimes. The role of the concept of “utility” in evaluations of social
policy has been controversial since Bentham popularized the notion over 200 years ago. But in many
cases of social distribution, utility considerations provide a partial but still important perspective
on what is at stake within a task of distribution. An individual needs various social and economic
resources throughout the course of her life; utility-based notions of welfare can capture the relative
beneﬁt that a particular good can have on a particular individual. If machine learning systems
are in eﬀect serving as resource distribution mechanisms, then questions about fairness should
align with questions of “Who beneﬁts?” Our results show that many parity-based formulations of
fairness in machine learning do not ensure that disadvantaged groups beneﬁt. Working to ensure
that a classiﬁer better accords with a fairness measure can lead to selecting allocations that lower
the welfare for every group (as well as the learner). There are several reasons that favor limiting
levels of inequality that are not reﬂected in utilitarian calculus, but without acknowledging and
accounting for these reasons, well-intentioned optimization tasks that seek to be “fairer” can further
disadvantage social groups for no reason but to satisfy a given fairness metric.

We propose that the ﬁeld of algorithmic fairness look to work in welfare economics for both
speciﬁc insights into formalizing substantive notions of fairness in distribution and also general
insights into how to build a “technical” ﬁeld and methodology that more eﬀectively grapples with
normative questions. Welfare economics exists as a branch of economics that is explicitly concerned

15

with what public policies ought to be, how to maximize individuals’ well-beings, and what types of
distributive outcomes are preferable. Answers to these questions appeal to values and judgments
that do not refer only to descriptive or predictive facts about a state of aﬀairs. It would appear
that the success of fair machine will largely hang on how well it can adapt to a similar ambitious
task.

References

[1] Amartya Sen. Equality of What? Cambridge University Press, Cambridge, 1980. Reprinted
in John Rawls et al., Liberty, Equality and Law (Cambridge: Cambridge University Press,
1987).

[2] Michael Feldman, Sorelle A Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkata-
subramanian. Certifying and removing disparate impact. In Proceedings of the 21th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 259–
268. ACM, 2015.

[3] Moritz Hardt, Eric Price, Nati Srebro, et al. Equality of opportunity in supervised learning.

In Advances in Neural Information Processing Systems, pages 3315–3323, 2016.

[4] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness
through awareness. In Proceedings of the 3rd Innovations in Theoretical Computer Science
Conference, pages 214–226. ACM, 2012.

[5] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi.
Fairness beyond disparate treatment & disparate impact: Learning classiﬁcation without dis-
parate mistreatment. In Proceedings of the 26th International Conference on World Wide Web,
pages 1171–1180. International World Wide Web Conferences Steering Committee, 2017.

[6] Yahav Bechavod and Katrina Ligett. Learning fair classiﬁers: A regularization-inspired ap-

proach. arXiv preprint arXiv:1707.00044, 2017.

[7] Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism

prediction instruments. Big data, 5(2):153–163, 2017.

[8] Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-oﬀs in the fair
determination of risk scores. In Proceedings of the 8th Innovations in Theoretical Computer
Science Conference, pages 43:1–43:23. ACM, 2017.

[9] Toshihiro Kamishima, Shotaro Akaho, and Jun Sakuma. Fairness-aware learning through
regularization approach. In Data Mining Workshops (ICDMW), 2011 IEEE 11th International
Conference on, pages 643–650. IEEE, 2011.

[10] Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair repre-

sentations. In International Conference on Machine Learning, pages 325–333, 2013.

[11] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi.
Fairness constraints: Mechanisms for fair classiﬁcation. arXiv preprint arXiv:1507.05259,
2015.

[12] Matthew Joseph, Michael Kearns, Jamie H Morgenstern, and Aaron Roth. Fairness in learn-
ing: Classic and contextual bandits. In Advances in Neural Information Processing Systems,
pages 325–333, 2016.

[13] Geoﬀ Pleiss, Manish Raghavan, Felix Wu, Jon Kleinberg, and Kilian Q Weinberger. On
fairness and calibration. In Advances in Neural Information Processing Systems, pages 5680–
5689, 2017.

[14] Flavio P Calmon, Dennis Wei, Karthikeyan Natesan Ramamurthy, and Kush R Varshney. Op-
timized data pre-processing for discrimination prevention. arXiv preprint arXiv:1704.03354,
2017.

16

[15] Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. In

Advances in Neural Information Processing Systems, pages 4066–4076, 2017.

[16] Niki Kilbertus, Mateo Rojas Carulla, Giambattista Parascandolo, Moritz Hardt, Dominik
In
Janzing, and Bernhard Sch¨olkopf. Avoiding discrimination through causal reasoning.
Advances in Neural Information Processing Systems, pages 656–666, 2017.

[17] Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. Preventing fairness gerry-
mandering: Auditing and learning for subgroup fairness. arXiv preprint arXiv:1711.05144,
2017.

[18] Michele Donini, Luca Oneto, Shai Ben-David, John Shawe-Taylor, and Massimiliano Pontil.
Empirical risk minimization under fairness constraints. arXiv preprint arXiv:1802.08626,
2018.

[19] Alekh Agarwal, Alina Beygelzimer, Miroslav Dud´ık, John Langford, and Hanna Wallach. A

reductions approach to fair classiﬁcation. arXiv preprint arXiv:1803.02453, 2018.

[20] Sendhil Mullainathan. Algorithmic fairness and the social welfare function. In Proceedings of

the 2018 ACM Conference on Economics and Computation, pages 1–1. ACM, 2018.

[21] Hoda Heidari, Claudio Ferrari, Krishna P Gummadi, and Andreas Krause. Fairness be-
hind a veil of ignorance: A welfare analysis for automated decision making. arXiv preprint
arXiv:1806.04959, 2018.

[22] Lucy F Ackert, Jorge Martinez-Vazquez, and Mark Rider. Social preferences and tax policy

design: some experimental evidence. Economic Inquiry, 45(3):487–501, 2007.

[23] Marc Fleurbaey and Fran¸cois Maniquet. A theory of fairness and social welfare, volume 48.

Cambridge University Press, 2011.

[24] Marc Fleurbaey, Fran¸cois Maniquet, et al. Optimal taxation theory and principles of fairness.
Technical report, Universit´e catholique de Louvain, Center for Operations Research and ?,
2015.

[25] Emmanuel Saez and Stefanie Stantcheva. Generalized social marginal welfare weights for

optimal tax theory. American Economic Review, 106(1):24–45, 2016.

[26] Christopher P Diehl and Gert Cauwenberghs. Svm incremental learning, adaptation and
optimization. In Neural Networks, 2003. Proceedings of the International Joint Conference
on, volume 4, pages 2685–2690. IEEE, 2003.

[27] Trevor Hastie, Saharon Rosset, Robert Tibshirani, and Ji Zhu. The entire regularization path
for the support vector machine. Journal of Machine Learning Research, 5(Oct):1391–1415,
2004.

[28] Gang Wang, Dit-Yan Yeung, and Frederick H Lochovsky. A kernel path algorithm for support
vector machines. In Proceedings of the 24th international conference on Machine learning,
pages 951–958. ACM, 2007.

[29] Floris T Zoutman, Bas Jacobs, and Egbert LW Jongen. Optimal redistributive taxes and

redistributive preferences in the netherlands. Erasmus University Rotterdam, 2013.

[30] Vidar Christiansen and Eilev S Jansen. Implicit social preferences in the norwegian system of

indirect taxation. Journal of Public Economics, 10(2):217–245, 1978.

[31] Matthew Adler. Well-being and fair distribution: beyond cost-beneﬁt analysis. Oxford Uni-

versity Press, 2012.

[32] Ilyana Kuziemko, Michael I Norton, Emmanuel Saez, and Stefanie Stantcheva. How elastic
are preferences for redistribution? evidence from randomized survey experiments. American
Economic Review, 105(4):1478–1508, 2015.

[33] Herbert Edelsbrunner and Ernst Peter M¨ucke. Simulation of simplicity: a technique to cope
with degenerate cases in geometric algorithms. ACM Transactions on Graphics (tog), 9(1):66–
104, 1990.

17

[34] Blake Woodworth, Suriya Gunasekar, Mesrob I Ohannessian, and Nathan Srebro. Learning

non-discriminatory predictors. arXiv preprint arXiv:1702.06081, 2017.

18

7 Appendix

7.1 Dual derivations of the (cid:15)-fair SVM program

In this Appendix section, we walk through the preliminary setup of the (cid:15)-fair SVM program given
in Section 5.1 and present intermediate derivations omitted from the main text.
Recall that the fair empirical risk minimization program of central focus is

minimize
θ, b

(cid:107)θ(cid:107)2 + C

1
2

n
(cid:88)

i=1

ξi

subject to yi(θ(cid:124)xi + b) − 1 + ξi ≥ 0,
ξi ≥ 0,
fθ,b(x, y) ≤ (cid:15)

((cid:15)-fair Soft-SVM)

The linear hyperplane parameters are θ ∈ Rd and b ∈ R. The non-negative ξi allow the margin
constraints to have some slack—this is why these variables are commonly called “slack variables.”
In the Soft-Margin (as opposed to the Hard-Margin) SVM, the margin is permitted to be less than
1. A slack variable ξi > 0 corresponds to a point xi having a functional margin of less than 1.
There is a cost associated with this margin violation, even though it need not correspond to a
classiﬁcation error. C > 0 is a hyperparameter tunable by the learner to optimize this trade-oﬀ
between preferring a larger margin and penalizing violations of the margin.

When we combine the general Soft-Margin SVM with the the covariance constraint in (5)

proposed by Zafar et al. [11], we have the program

minimize
θ, b

subject to

(cid:107)θ(cid:107)2 + C

1
2

n
(cid:88)

i=1

ξi

yi(θ(cid:124)xi + b) − 1 + ξ ≥ 0,
n
(cid:88)

(cid:12)
(cid:12)
(zi − ¯z)(θ(cid:124)xi + b)
(cid:12)
(cid:12)
(cid:12)

≤ (cid:15)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

((cid:15)-fair-SVM1-P)

where ¯z reﬂects the bias in the demographic makeup of X : ¯z = 1
n
Lagrangian is

(cid:80)n

i=1 zi. The corresponding

LP (θ, b, ξ, λ, µ, γ1, γ2) =

(cid:107)θ(cid:107)2 + C

1
2

n
(cid:88)

i=1

ξi −

n
(cid:88)

i=1

λi −

n
(cid:88)

i=1

µi(yi(θ(cid:124)xi + b) − 1 + ξi)

− γ1

(cid:0)(cid:15) −

1
n

n
(cid:88)

(zi − ¯z)(θ(cid:124)xi + b)(cid:1) − γ2

i=1

(cid:0)(cid:15) −

1
n

n
(cid:88)

(¯z − zi)(θ(cid:124)xi + b)(cid:1)

i=1

((cid:15)-fair-SVM1-L)

where θ ∈ Rd, b ∈ R, ξ ∈ Rn are Primal variables. The (non-negative) Lagrange multipliers
λ, µ ∈ Rn correspond to the n non-negativity constraints ξi ≥ 0 and the margin-slack constraints
yi(θ(cid:124)xi + b) − 1 + ξi ≥ 0 respectively. The multiplier µi relays information about the functional
margin of its corresponding point xi. If the margin is greater than 1 in the Primal, i.e., there is
slack in the constraint), then by complementary slackness, µi = 0. Otherwise, if the constraint
holds with equality, µi ∈ (0, C]. When the classiﬁer commits an error on xi, yi(θ(cid:124)xi + b) ≤, and
then by the KKT conditions, µi = C.

The multipliers γ1, γ2 ∈ R correspond to the two linearized forms of the absolute value fairness
constraint. Notice that these two constraints cannot simultaneously hold with equality for (cid:15) > 0.
Thus, by complementary slackness again, we know that at least one of γ1, γ2 is zero, and the other
is strictly positive.

By the Karush-Kuhn-Tucker conditions, at the solution of the convex program, the gradients

19

of L with respect to θ, b, and ξi are zero:

:= 0 ⇒ θ =

n
(cid:88)

i=1

µiyixi −

γ
n

(

n
(cid:88)

(zi − ¯z)xi)

i=1

:= 0 ⇒

n
(cid:88)

i=1

µiyi =

γ
n

n
(cid:88)

i=1

(zi − ¯z) = 0

:= 0 ⇒ λi + µi = C,

i = 1, . . . , n

∂L
∂θ

∂L
∂b

∂L
∂ξi

Plugging in these conditions, the Dual Lagrangian is

LD(θ, ξ, λ, µ, γ1, γ2) = −

1
2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n
(cid:88)

i=1

µiyixi −

γ
n

n
(cid:88)

i=1

(zi − ¯z)xi

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

+

n
(cid:88)

i=1

µi − |γ|(cid:15)

(21)

where γ = γ1 − γ2. Thus the Dual maximizes this objective subject to the constraints µi ∈ [0, C]
for all i and (cid:80)

i=1 µiyi = 0. We thus derive the full Dual problem
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
i = 1, . . . , n,

i=1
µi ∈ [0, C],

maximize
µ, γ

(zi − ¯z)xi

subject to

µiyixi −

n
(cid:88)

n
(cid:88)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

γ
n

1
2

i=1

−

+

n
(cid:88)

i=1

µi − V (cid:15)

((cid:15)-fair-SVM1-D)

n
(cid:88)

i=1

µiyi = 0,

γ ∈ [−V, V ]

where we have introduced the variable V to eliminate the absolute value function |γ| in the objec-
tive. Notice that when γ = 0 and neither of the fairness constraints bind, we recover the standard
dual SVM program. Since we are concerned with fairness constraints that alter an optimal solution,
we are interested in cases in which V is strictly positive. As such, we can rewrite the preceding by
plugging in the optimal γ∗ as given in (17):

γ∗ =

n(n(β− − β+) + (cid:80)n
(cid:107)u(cid:107)2

i=1 µiyi(cid:104)xi, u(cid:105))

Thus we can write ((cid:15)-fair-SVM1-D) as

µiyi(I − Pu)xi

maximize
µ,β−,β+

subject to

−

1
2

n
(cid:88)

n
(cid:88)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

i=1
µi ∈ [0, C],

µiyi = 0,

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

+

n
(cid:88)

i=1

µi +

2n (cid:80)

i µiyi(cid:104)xi, u(cid:105) + n2(β− − β+)

2(cid:107)u(cid:107)2

(β− − β+)

i = 1, . . . , n,

((cid:15)-fair SVM2-D)

i=1
β−, β+ ≥ 0,
β− + β+ = (cid:15)

where I, Pu ∈ Rd×d. The former is the identity matrix, and the latter is the projection matrix onto
the vector deﬁned by u = (cid:80)n
i=1(zi − ¯z)xi. As was also observed by Donini et al., the (cid:15) = 0 version
of ((cid:15)-fair SVM2-D) is thus equivalent to the standard formulation of the dual SVM program with
Kernel K(xi, xj) = (cid:104)(I − Pu)xi, (I − Pu)xj(cid:105) [18].

7.2 Algorithms

7.3 Additional Figures

7.4 Proofs

7.4.1 Proof of Lemma 1
Proof. Let A and B be a pair of disjoint non-empty convex sets that partition X ⊂ Rd: A (cid:96) B = X .
Then by the hyperplane separation theorem, there exists a pair (θ, b) such that for all x ∈ A,

20

ALGORITHM 3: Compute Group Welfares
Input: dual variables µ, true labels y, group memberships z
Output: group welfares {W0, W1}
W0 = 0;
W1 = 0;
|n0| = (cid:80)n
|n1| = (cid:80)n
for each µi do

1[zi = 0];
1[zi = 1];

i=1

i=1

if (µi < C & yi = 1) || (µi = C & yi = 0) then

Wzi = Wzi + 1;

end

end
return ( W0
n0

, W1
n1

)

Figure 2: Sensitivity analysis of (cid:15)-fair SVM-solution on Adult dataset. Increasing (cid:15) from left to
right loosens fairness constraint, and classiﬁcation outcomes become “less fair.” Paths level oﬀ at
(cid:15) ≈ 0.175 when constraint ceases to bind at the optimal solution.Top Panel: Learner objective
value monotonically decreases as fairness constraint loosens. Bottom Panel: Absolute group-
speciﬁc welfare change at (cid:15)-fair SVM solution given as absolute change in the number of positively
labeled examples compared to the group unconstrained baseline.

θ(cid:124)x ≥ b—call this closed halfspace ¯h+—and for all x ∈ B, θ(cid:124)x ≤ b—call this closed halfspace ¯h−.
One such hyperplane can be constructed to separate the convex hulls of A and B

C(A) = (cid:8)

|A|
(cid:88)

i=1

αixi|xi ∈ A, αi ≥ 0,

|A|
(cid:88)

i=1

αi = 1(cid:9)

C(B) = (cid:8)

|B|
(cid:88)

i=1

αixi|xi ∈ B, αi ≥ 0,

|B|
(cid:88)

i=1

αi = 1(cid:9)

Let hV be the d − 1-dimensional hyperplane deﬁned by the set V with |V | = d such that
V ∩ C(A) (cid:54)= ∅ and V ∩ C(B) (cid:54)= ∅. In order for the hyperplane to separate C(A) and C(B), hV
must also support each hull—we know that such a hyperplane always exists. In order to separate
C(A) and C(B) so they are contained within open halfspaces h+
V , we wiggle the hyperplane
so that it no longer passes through vertices v ∈ V but still maintains convex hull separation. This
“wiggle” step is the ﬁnal step of separating A and B.

V and h−

Suppose V can be partitioned into a single vertex vA in C(A) and a set P = {v|v ∈ C(B)}
with |P | = d − 1. The set P deﬁnes a ridge on C(B), since it is a d − 2-dimensional facet of
C(B). Rotations in d-dimensions are precisely deﬁned as being around d − 2-dimensional planes.
Thus pivoting hV around the ridge P away from vA is a well-deﬁned rotation in Rd. Selecting
any inﬁnitesimally small rotation angle ρ will be enough to have C(A) ∈ h+
V . After the pivot,
we translate hV away from the ridge P back toward vA. An inﬁnitesimal translation is suﬃcient,
since we simply wish to dislodge hV from the ridge P , so that C(B) ∈ h−
V .

21

7.4.2 Proof of Proposition 5

Proof. Following much of the exposition in the main text, recall we have that the perturbation
function in (19) is given as

p((cid:15)) ≥ sup
µ,γ

{L(µ∗, γ∗) − (cid:15)|γ∗|}

which gives a global lower bound. Thus when a perturbation ∆(cid:15) < 0 causes L(µ∗, γ∗) − (cid:15)|γ∗|
to increase, then p((cid:15) + ∆(cid:15)) is guaranteed to increase by at least ∆(cid:15)|γ∗|. Thus when |γ∗| (cid:29) 0,
p((cid:15) + ∆(cid:15)) − p((cid:15)) (cid:29) 0. The learner experience a signiﬁcant increase in her optimal value p((cid:15)) (which
she wishes to minimize).

On the other hand, when ∆(cid:15) > 0, then L(µ∗, γ∗) − (cid:15)|γ∗| decreases. But the decrease gives only
the lower bound, and thus when |γ∗| is small, her optimal value p((cid:15)) decreases but it is guaranteed
not to decrease by much.

7.4.3 Proof of Proposition 3

Proof. For all j ∈ F (cid:15), remaining in F (cid:15)+∆(cid:15) after the perturbation requires that ∂D
∂µj
the perturbation. Let µ(cid:15)
quantity ∂D
∂µj

> 0 after
i be the optimal µi solution at p((cid:15)). Then following (10), we rewrite the

as

gj = 1 −

(cid:16) n
(cid:88)

i=1

µ(cid:15)

i yi(I − Pu)xiyj(I − Pu)xj +

n(cid:15)yj(cid:104)xj, u(cid:105)
(cid:107)u(cid:107)2

(cid:17)

< 0

+ byj

If dj∆(cid:15) > 0, then j ∈ F (cid:15)+∆(cid:15). Otherwise, for dj∆(cid:15) < 0, if ∆(cid:15) < gj
dj
after the perturbation. (cid:88)
The same reasoning follows for j ∈ E (cid:15), except we have that gj > 0. Thus if dj∆(cid:15) < 0, then
j ∈ E (cid:15)+∆(cid:15). Otherwise, for dj∆(cid:15) > 0, if ∆(cid:15) < gj
> 0, and j ∈ E (cid:15)+∆(cid:15) after the
dj
perturbation. (cid:88)

> 0, and j ∈ F (cid:15)+∆(cid:15)

∂D
∂µ(cid:15)+∆(cid:15)
j

∂D
∂µ(cid:15)+∆(cid:15)
j

, then

, then

To ensure that support vectors do not escape the margin, we can directly look to rj = ∂µj
∂(cid:15) .
j ∈ [0, C], then staying in the margin and set S (cid:15)+∆(cid:15) depends on the sign of

Since for all j ∈ S (cid:15), µ(cid:15)
rj and requires that

rj < 0 −→

rj > 0 −→

C − µ(cid:15)
j
rj
−µ(cid:15)
j
rj

< ∆(cid:15) <

−µ(cid:15)
j
rj
C − µ(cid:15)
j
rj

< ∆(cid:15) <

(22)

(23)

Thus taking the minimum of the positive quantities gives an upper bound, while taking the maxi-
mum of the negative quantities gives a lower bound on ∆(cid:15) perturbations, such that {F, S, E}(cid:15) =
{F, S, E}(cid:15)+∆(cid:15). Let

mj =






,

(cid:40) gj
dj
−∞,

min{
(cid:40)

−∞,
gj
,
dj

C−µ(cid:15)
j
rj

j ∈ F, dj > 0
j ∈ F, dj < 0
},

−µ(cid:15)
j
rj

,

j ∈ E, dj > 0
j ∈ E, dj < 0

j ∈ S

,

Mj =






(cid:40)

∞,
gj
,
dj
min{
(cid:40) gj
dj
∞,

,

j ∈ F, dj > 0
j ∈ F, dj < 0
C−µ(cid:15)
−µ(cid:15)
j
j
rj
rj
j ∈ E, dj > 0
j ∈ E, dj < 0

},

,

j ∈ S

Thus all perturbations of (cid:15) within the range

∆(cid:15) ∈ (cid:0) max

j

mj, min

j

(cid:1)

Mj

satisfy the necessary conditions to ensure stable sets {F, S, E}. Stable classiﬁcations ˆyi follow.

7.4.4 Proof of Corollary 1

Proof. For all ∆(cid:15) in the stable region given in (16), Wi((cid:15)) = Wi((cid:15) + ∆(cid:15)) where i gives group
membership z = i. Thus the groups are welfare-wise indiﬀerent between classiﬁcations at (cid:15) and

22

∆(cid:15). For all ∆(cid:15) < 0, where the fairness constraint is tightened,p((cid:15)) ≤ p((cid:15) + ∆(cid:15)). Since the learner
prefers lower loss, we have that p((cid:15)) (cid:23) p((cid:15) + ∆(cid:15)). Comparing the triples at each (cid:15) value, we thus
have

{p((cid:15)), W0((cid:15)), W1((cid:15))} (cid:23) {p((cid:15) + ∆(cid:15)), W0((cid:15) + ∆(cid:15)), W1((cid:15) + ∆(cid:15))}

as desired.

7.4.5 Proof of Lemma 2

.

∂(cid:15) = 0, and µ(cid:15)0

Proof. Consider the dual variables µ(cid:15)0 at the optimal SVM solution p((cid:15)0). By Proposition (3), for
all perturbations ∆(cid:15)0 ∈ (maxi mi, mini Mi), µi for i ∈ S change according to (11), which is clearly
linear in ∆(cid:15); for all i /∈ S, ∂µi
i = µ(cid:15)0+∆(cid:15)0
i
For ∆(cid:15)0 perturbations beyond this range, at least one vector xi leaves its original set at (cid:15)0 and
enters another at (cid:15)0 +∆(cid:15)0. Without loss of generality consider the upper bound to the ∆(cid:15)0 stability
region, denoted b0 = mini Mi. We want to show that all possible µi paths at this breakpoint are
continuous and piecewise linear. Consider j ∈ E (cid:15)0 moving into j ∈ S (cid:15)0+b0: µ(cid:15)0
= C for
all ∆(cid:15)0 ∈ [(cid:15)0, (cid:15)0 + b0). At ∆(cid:15)0 = b0, we update the partition such that S (cid:15)0+b0 = S(cid:15)0 ∪ {j} and
recompute rj for all j ∈ S. Following (11), we consider new perturbations ∆(cid:15)1 from (cid:15)1 = (cid:15)0 + b0.
thus have that µ(cid:15)1+∆(cid:15)1
= C + rj∆(cid:15)1, which is linear in ∆(cid:15). The same argument follows for j ∈ F (cid:15)
0
j
moving into j ∈ S (cid:15)0+b0 where µ(cid:15)0
= 0 for all ∆(cid:15)0 ∈ ((cid:15)0, (cid:15)0 + b0). For j ∈ S (cid:15)0 moving to
= µ(cid:15)0
j ∈ F (cid:15)0+b0 , µ(cid:15)0+b0
j + b0rj = C. Thus the
µj paths are continuous in ∆(cid:15) and between breakpoints (equivalently, in stable regions), they are
either constant or linear in ∆(cid:15) and as such are piecewise linear over perturbations ∆(cid:15).

j + b0rj = 0; if moving into E (cid:15)0+b0 , µ(cid:15)0+b0

j = µ(cid:15)0+∆(cid:15)0

j = µ(cid:15)0+∆(cid:15)0

= µ(cid:15)0

j

j

j

j

7.4.6 Proof of Proposition 4

C−µ(cid:15)
j
rj

Proof. Fix (cid:15) ∈ (0, 1) and consider the stable region of ∆(cid:15) perturbations given by (bL, bU ). Suppose
bL = gj
with j ∈ E, then if yj = −1, ˆyj = +1. Thus at the breakpoint ∆(cid:15) = bL, j moves into
dj
(cid:15)+bL and ˆyj = +1 and uzj ((cid:15) + bL) < uzj ((cid:15)) where zj gives the group membership of xj. Since no
S
other points transition, u¯z((cid:15) + bL) = u¯z((cid:15)) for all ¯z (cid:54)= zj. Since bL < 0, the fairness constraint is
tightened and associated with a shadow price given by γ > 0 such that p((cid:15) + bL) < p((cid:15)). (cid:88)

Suppose bL =

and j ∈ S (cid:15) with yj = +1, then j moves into j ∈ E (cid:15)+bL such that ˆyj = −1.
Thus uzj ((cid:15) + bL) < uzj ((cid:15)) and u¯z((cid:15) + bL) = u¯z((cid:15)) where zj is the group membership of xj and
¯z (cid:54)= zj, and p((cid:15) + bL) ≤ p((cid:15)). (cid:88)

Suppose bU = gj
dj

> 0 where j ∈ E (cid:15), yj = +1, and ˆyj = −1. At the breakpoint, j moves
into S (cid:15)+bU such that yj = −1. Then uzj ((cid:15) + bU ) > uzj ((cid:15)) where zj is the group membership of
xj. For ¯z (cid:54)= zj, u¯z((cid:15) + bU ) = u¯z((cid:15)), and since bU > 0, the fairness constraint is loosened and
p((cid:15) + bU ) > p((cid:15)).
Suppose bU =

> 0 where j ∈ S (cid:15) and yj = −1. At the breakpoint, j moves into E (cid:15)+bU such
that ˆyj = +1. Then uzj ((cid:15) + bU ) > uzj ((cid:15)) where zj gives the group membership of xj. For ¯z (cid:54)= zj,
u¯z((cid:15) + bU ) = u¯z((cid:15)), and since bU > 0, the fairness constraint is loosened and p((cid:15) + bU ) ≥ p((cid:15)).
(cid:88)

C−µ(cid:15)
j
rj

7.4.7 Proof of Theorem 2

Proof. Theorem 2 follows from Proposition 3, Corollary 1, Lemma 2, and Proposition 4.

23

