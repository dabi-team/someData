An Integer Programming Approach to Deep Neural Networks
with Binary Activation Functions

Jannis Kurtz 1 Bubacarr Bah 2 3

0
2
0
2

g
u
A
7

]

C
O
.
h
t
a
m

[

3
v
6
2
3
3
0
.
7
0
0
2
:
v
i
X
r
a

Abstract

We study deep neural networks with binary ac-
the activation
tivation functions (BDNN), i.e.
function only has two states. We show that the
BDNN can be reformulated as a mixed-integer
linear program which can be solved to global op-
timality by classical integer programming solvers.
Additionally, a heuristic solution algorithm is pre-
sented and we study the model under data uncer-
tainty, applying a two-stage robust optimization
approach. We implemented our methods on ran-
dom and real datasets and show that the heuristic
version of the BDNN outperforms classical deep
neural networks on the Breast Cancer Wisconsin
dataset while performing worse on random data.

1. Introduction

Deep learning (DL) methods have of-late reinvigorated inter-
est in artiﬁcial intelligence and data science, and they have
had many successful applications in computer vision, nat-
ural language processing, and data analytics (LeCun et al.,
2015). The training of deep neural networks relies mostly on
(stochastic) gradient descent, hence the use of differentiable
activation functions like ReLU, sigmoid or the hyperbolic
tangent is the state of the art (Rumelhart et al., 1986; Good-
fellow et al., 2016). On the contrary discrete activations,
which may be more analogous to biological activations,
present a training challenge due to non-differentiability and
even discontinuity. If additionally the weights are consid-
ered to be binary, the use of discrete activation networks can
reduce the computation and storage complexities, provides
for better interpretation of solutions, and has the potential

*Equal contribution 1Chair for Mathematics of Information Pro-
cessing, RWTH Aachen University, Aachen, Germany 2African
Institute for Mathematical Sciences, Cape Town, South Africa
3Division of Applied Mathematics, Stellenbosch University, Stel-
lenbosch, South Africa. Correspondence to:
Jannis Kurtz
<kurtz@mathc.rwth-aachen.de>.

Workshop on Beyond ﬁrst-order methods in ML systems at the 37 th
International Conference on Machine Learning, Vienna, Austria,
2020. Copyright 2020 by the author(s).

to be more robust to adversarial perturbations than the con-
tinuous activation networks (Qin et al., 2020). Furthermore
low-powered computations may beneﬁt from discrete ac-
tivations as a form of coarse quantizations (Plagianakos
et al., 2001; Bengio et al., 2013; Courbariaux et al., 2015;
Rastegari et al., 2016). Nevertheless, gradient descent-based
training behaves like a black box, raising a lot of questions
regarding the explainability and interpretability of inter-
nal representations (Hampson & Volper, 1990; Plagianakos
et al., 2001; Bengio et al., 2013).

On the other hand, integer programming (IP) is known as a
powerful tool to model a huge class of real-world optimiza-
tion problems (Wolsey, 1998). Recently it was successfully
applied to machine learning problems involving sparsity
constraints and to evaluate trained neural networks (Bertsi-
mas et al., 2017; 2019b; Fischetti & Jo, 2018).

Contributions:
In Section 3, we reformulate the binary
neural network as an IP formulation, referred to in the sequel
as a binary deep neural network (BDNN), which can be
solved to global optimality by classical IP solvers. All our
results can also be applied to BDNNs with binary weights.
Solving the latter class of networks to optimality was still an
open problem. We note that it is straight forward to extend
the IP formulation to more general settings and several
variations of the model. We present a heuristic algorithm to
calculate local optimal solutions of the BDNN in Section
3.1 and apply a two-stage robust optimization approach to
protect the model against adversarial attacks in Section 4.
In Section 5 we present computational results for random
and real datasets and compare the BDNN to a deep neural
network using ReLU activations (DNN). Despite scalability
issues and a slightly worse accuracy on random datasets, the
results indicate that the heuristic version outperforms the
DNN on the Breast Cancer Wisconsin dataset.

2. Related Literature

The interest in BDNN goes back to (McCulloch & Pitts,
1943) where BDNNs were used to simulate Boolean func-
tions. However, until the beginning of this century con-
certed efforts were made to train these networks either by
speciﬁc schemes (Gray & Michel, 1992; Kohut & Stein-

 
 
 
 
 
 
Deep Neural Networks with Binary Activation Functions

bach, 2004) or via back propagation by modiﬁcations of the
gradient descent method (Widrow & Winter, 1988; Toms,
1990; Barlett & Downs, 1992; Goodman & Zeng, 1994;
Corwin et al., 1994; Plagianakos et al., 2001). More recent
work regarding the back propagation, mostly motivated by
the low complexity of computation and storage, build-on
the pioneering works of (Bengio et al., 2013; Courbariaux
et al., 2015; Hubara et al., 2016; Rastegari et al., 2016; Kim
& Smaragdis, 2016); see (Qin et al., 2020) for a detailed
survey. Regarding the generalization error of BDNN, it
was already proved that the VC-Dimension of deep neural
networks with binary activation functions is ρ log(ρ), where
ρ is the number of weights of the BDNN; see (Baum &
Haussler, 1989; Maass, 1994; Sakurai, 1993).

One the other hand IP methods were successfully applied
to sparse classiﬁcation problems (Bertsimas et al., 2017),
sparse regression (Bertsimas et al., 2019b) and to evaluate
trained neural networks (Fischetti & Jo, 2018). In (Icarte
et al., 2019) BDNNs with weights restricted to {−1, 1} are
trained by a hybrid method based on constraint program-
ming and mixed-integer programming. In (Khalil et al.,
2018) the authors calculate optimal adversarial examples for
BDNNs using a MIP formulation and integer propagation.
Furthermore, robust optimization approaches were used to
protect against adversarial attacks for other machine learn-
ing methods (Xu et al., 2009b;a; Bertsimas et al., 2019a).

3. Discrete Neural Networks

As

i.e.

(BDNN),

framework,

In this work we study binary deep neural net-
works
classical deep neural net-
works with binary activation functions.
in
the classical
for a given input vector
x ∈ Rn we study classiﬁcation functions f of the form
f (x) = σK (cid:0)W KσK−1 (cid:0)W K−1 . . . , σ1 (cid:0)W 1x(cid:1) . . .(cid:1)(cid:1) for
weight matrices W k ∈ Rdk×dk−1 and activation functions
σk, which are applied component-wise. The dimension dk
is called the width of the k-th layer . In contrast to the
recent developments of the ﬁeld we consider the activation
functions to be binary, more precisely each function is of
the form

σk(α) =

(cid:40)

0
1

if α < λk
otherwise

(1)

for α ∈ R where the parameters λk ∈ R can be learned by
our model simultaneously with the weight matrices which
is normally not the case in the classical neural network
approaches. Note that it is also possible to ﬁx the values λk
in advance.

Given a set of labeled training samples

X × Y = (cid:8)(xi, yi) | i ∈ [m](cid:9) ⊂ Rn × {0, 1}

we consider loss functions

(cid:96) : {0, 1} × RdK → R

(2)

and the task is to ﬁnd the optimal weight matrices which
minimize the empirical loss over the training samples, i.e.
we want to solve the problem

min

s.t.

m
(cid:88)

i=1

(cid:96) (cid:0)yi, zi(cid:1)

zi = σK (cid:0)W KσK−1 (cid:0). . . σ1 (cid:0)W 1xi(cid:1) . . .(cid:1)(cid:1) ∀i ∈ [m]
W k ∈ Rdk×dk−1 ∀k ∈ [K]
λk ∈ R ∀k ∈ [K]

(3)
for given dimensions d0, . . . dK where d0 = n. We set
dK =: L. All results in this work even hold for regression
problems, more precisely for loss functions

(cid:96)r : RL × RL → R

(cid:0)xi, zi(cid:1) in-
where we minimize the empirical loss (cid:80)m
stead. We use labels {0, 1} instead of the classical labels
{−1, 1} due to ease of notation in our model. Nevertheless
the same approach can be adapted to labels ˜yi ∈ {−1, 1}
by adding the additional constraints

i=1 (cid:96)r

˜yi = −1 + 2yi.

Several variants of the model can be considered:

• The output of the neural network is 0 or 1, deﬁning
the class the input is assigned to. As a loss function,
we count the number of misclassiﬁed training samples.
This variant can be modeled by choosing the last layer
to have width dK = 1 together with the loss function
(cid:96)(y, z) = |y − z|.

• The output of the neural network is a continuous vector
z ∈ RL. This case can be modeled by choosing the ac-
tivation function of the last layer σK as the identity and
dK = L. Any classical loss functions maybe consider,
e.g. the squared loss (cid:96)(x, z) = (cid:107)y − z(cid:107)2
2 . Note that
a special case of this model is the autoencoder where
in contrast to the continuous framework the input is
encoded into a vector with 0-1 entries.

• All results can easily be generalized to multiclass clas-

siﬁcation; see Section 3.

In the following we use the notation [p] := {1, . . . , p} for
p ∈ N.

In the following lemma we show how to reformulate Prob-
lem (3) as an integer program.

Deep Neural Networks with Binary Activation Functions

Lemma 1. Assume the euclidean norm of each data point
in X is bounded by r, then Problem (3) is equivalent to the
mixed-integer non-linear program

min

m
(cid:88)

i=1

(cid:96) (cid:0)yi, ui,K(cid:1)

s.t.

W 1xi < M1ui,1 + λ1
W 1xi ≥ M1(ui,1 − 1) + λ1
W kui,k−1 < Mkui,k + λk ∀k ∈ [K] \ {1}
W kui,k−1 ≥ Mk(ui,k − 1) + λk ∀k ∈ [K] \ {1}
W k ∈ [−1, 1]dk×dk−1, λk ∈ [−1, 1] ∀k ∈ [K]
ui,k ∈ {0, 1}dk ∀k ∈ [K], i ∈ [m],

(4)

(5)

(6)

(7)

(8)

(9)

(10)

where M1 := (nr + 1) and Mk := (dk−1 + 1).

Proof. First we show that, due to the binary activation
functions, we may assume W k ∈ [−1, 1]dk×dk−1 and
λk ∈ [−1, 1] for all k ∈ [K] in our model. To prove this
assume we have any given solution W 1, . . . , W K and cor-
responding λ1, . . . , λK of problem (3) with arbitrary values
in R. Consider any ﬁxed layer k ∈ [K]. The k-th layer re-
ceives a vector hk−1 ∈ {0, 1}dk−1 from the previous layer,
which is applied to W k and afterwards the activation func-
tion σk is applied component-wise, i.e. the output of the
k-th layer is a vector

hk
j =

(cid:40)

0
1

j )(cid:62)hk−1 < λk

if (wk
otherwise

where wk

j is the j-th row of the matrix W k. Set

β := max{|λk|, max

j=1,...,dk
l=1,...,dk−1

|wk

jl|}

and deﬁne ˜W k := βW k and ˜λk := βλk. Then replacing
W k by ˜W k and λk by ˜λk in the k-th layer yields the same
j )(cid:62)hk−1 < λk
output vector hk, since the inequality (wk
j )(cid:62)hk−1 < ˜λk holds.
holds if and only if the inequality ( ˜wk
Furthermore all entries of ˜W k and ˜λk have values in [−1, 1].

since otherwise Constraint (6) would be violated. Note that
if ui,1
j = 1, then Constraint (5) is always satisﬁed since
all entries of W 1 are in [−1, 1] and all entries of xi are in
[−r, r] and therefore |W 1xi| ≤ nr < M1. Similarly we
can show that ui,1
j )(cid:62)xi < λ1. Hence
j = 0 if and only if (w1
ui,1 is the output of the ﬁrst layer for data point xi which
is applied to W 2 in Constraints (7) and (8). By the same
reasoning applied to Constraints (7) and (8) we can show
that ui,k is equal to the output of the k-th layer for data point
xi for each k ∈ [K] \ {1}. Note that instead of the value nr
we can use dk−1 here since the entries of ui,k−1 can only
have values 0 or 1 and the dimension of the rows of W k is
dk−1.

The formulation in Lemma 1 is a non-linear mixed-integer
programming (MINLP) formulation, since it contains prod-
ucts of variables, where each is a product of a continuous
variable and an integer variable. By linearizing these prod-
ucts we can prove the following theorem.

Theorem 2. Under the assumption of Lemma 1 Problem
(3) is equivalent to the mixed-integer program

(MIP) : min

m
(cid:88)

i=1

(cid:96) (cid:0)yi, ui,K(cid:1)

s.t.

W 1xi < M1ui,1 + λ1 ∀i ∈ [m]
W 1xi ≥ M1(ui,1 − 1) + λ1 ∀i ∈ [m]
dk−1
(cid:88)

si,k
l < Mkui,k + λk

l=1

dk−1
(cid:88)

l=1

∀i ∈ [m], k ∈ [K] \ {1}

si,k
l ≥ Mk(ui,k − 1) + λk

∀i ∈ [m], k ∈ [K] \ {1}
lj ≥ −ui,k
si,k

,

j

j

lj ≤ ui,k
si,k

∀i ∈ [m], k ∈ [K] \ {1}, l ∈ [dk−1], j ∈ [dk]

si,k
lj ≤ wk

lj + (1 − ui,k
j )

∀i ∈ [m], k ∈ [K] \ {1}, l ∈ [dk−1], j ∈ [dk]

Next we show that the constraints (5)–(8) correctly model
the equation

si,k
lj ≥ wk

lj − (1 − ui,k
j )

zi := ui,K

= σK (cid:0)W KσK−1 (cid:0)W K−1 . . . , σ1 (cid:0)W 1xi(cid:1) . . .(cid:1)(cid:1)

of Problem (3). The main idea is that the ui,k-variables
model the output of the activation functions of data point
i in layer k, i.e. they have value 0 if the activation value
is 0 or value 1 otherwise. More precisely for any solution
W 1, . . . , W k and λ1, . . . , λk of the Problem in Lemma 1
the variable ui,1
j )(cid:62)xi ≥ λ1
j

is equal to 1 if and only if (w1

∀i ∈ [m], k ∈ [K] \ {1}, l ∈ [dk−1], j ∈ [dk]

W k ∈ [−1, 1]dk×dk−1 ∀k ∈ [K]
λk ∈ [−1, 1] ∀k ∈ [K]
ui,k ∈ {0, 1}dk ∀k ∈ [K], i ∈ [m]
si,k
l ∈ [−1, 1]dk ∀i ∈ [m], k ∈ [K] \ {1}, l ∈ [dk−1]

ljui,k−1
Proof. We replace each product of variables wk
in the formulation of Lemma (1) by a new variable

j

Deep Neural Networks with Binary Activation Functions

si,k
lj ∈ [−1, 1]. To ensure that
ljui,k−1

wk

j

= si,k
lj

holds, we have to add the set of inequalities
lj ≤ ui,k
si,k
si,k
lj ≥ −ui,k
si,k
lj ≤ wk
si,k
lj ≥ wk

lj + (1 − ui,k
j )
lj − (1 − ui,k
j ).

j

j

lj = 0. Since wk

Note that if ui,k
that si,k
feasible for the last two constraints. If ui,k
last two constraints ensure, that si,k
lj = wk
ui,k
j

j = 0, then the ﬁrst two constraints ensure
lj ∈ [−1, 1] this combination is also
j = 1, then the
lj, while si,k
lj and

are still feasible for the ﬁrst two constraints.

Remark 3. The regression variant can be modeled by
replacing the ui,K variables by continuous variables
zi ∈ RdK for each i ∈ [m], replacing the objective function
(4) by

m
(cid:88)

(cid:0)xi, zi(cid:1)

(cid:96)r

i=1

and replacing Constraints (7)-(8) with k = K by the con-
straints

zi = W Kui,k−1.
Remark 4. Besides others the following classical loss func-
tions can be considered:

• To model the empirical error we simply deﬁne the loss

function by

m
(cid:88)

i=1

1{zi(cid:54)=yi} =

m
(cid:88)

zi +

m
(cid:88)

(1 − zi)

i=1:yi=0

i=1:yi=1

In this case we obtain a mixed-integer linear problem
(MILP) in Theorem 2.

• Theorem 2 shows that the regression variant with
2 can be modelled

quadratic loss (cid:96)r(x, z) = (cid:107)x − z(cid:107)2
as a mixed-integer quadratic problem (MIQP).

Problem (MIP) can be solved by any off-the-shelf solvers
like Gurobi for many classical loss functions (Gurobi Op-
timization, 2020). Unfortunately, the number of integer
variables of the MIP formulation is O(DKm), where D is
the maximum dimension of the layers, and therefore grows
linear with the number of training samples and with the
number of layers. For practical applications requiring large
training sets solving the MIP formulation can be a hard or
even impossible task. To tackle these difﬁculties we propose
a fast heuristic in Section 3.1. Despite the latter drawback
the MIP formulation has a lot of advantages and can give
further insights into the analysis of deep neural networks:

• The BDNN with binary weights can be modeled by
simply setting the weight variables to be in {0, 1}. The
linearization results of Theorem (2) apply also in this
case.

• Multiclass classiﬁcation tasks can be modeled by using

integer variables ui,K

j ∈ Z ∩ [a, b] in the last layer.

• More general discrete activation functions of the form

σk(α) = v if λv

k ≤ α ≤ λv

k, v ∈ V ⊂ Z

k, λv

for a ﬁnite set V and pairwise non-intersecting inter-
vals [λv
k] can be modeled by adding copies ui,k,v
of the ui,k variables for each v ∈ V and adding the
constraints

W k

W k

(cid:32)

(cid:88)

v∈V

(cid:32)

(cid:88)

v∈V

(cid:33)

(cid:33)

vui,k−1,v

vui,k−1,v

≤ Mk

(cid:0)1 − ui,k,v(cid:1) + λv

k

≥ −Mk

(cid:0)1 − ui,k,v(cid:1) + λv

k

for each v ∈ V , k ∈ [K] \ {1} and i ∈ [m]. The
two constraints for the ﬁrst layer are deﬁned similarly,
replacing (cid:0)(cid:80)
v∈V vui,k−1,v(cid:1) by xi. Note that the val-
ues λv
k, λv
k either have to be ﬁxed in advance for each
v ∈ V or additional constraints have to be added which
ensure the interval structure.

• The MIP formulation can easily be adjusted for ap-
plications where further constraints are desired. E.g.
sparsity constraints of the form

(cid:107)W k(cid:107)0 ≤ s

can be easily added to the formulation. Here (cid:107)W k(cid:107)0
is the number of non-zero entries of W k.

• Any classical approaches handling uncertainty in the
data can be applied to the MIP formulation. In Section
4 we will apply a robust optimization approach to the
MIP formulation.

• The model is very ﬂexible regarding changes in the
training set. To add new data points that were not
yet considered we just have to add the corresponding
variables and constraints for the new data points to our
already existing model and restart a solver, which is
based on the idea of online machine learning.

• Classical solvers like Gurobi use branch & bound meth-
ods to solve MIP formulations. During these methods
at each time the optimality gap, i.e. the percental differ-
ence between the best known upper and lower bound,
is known. These methods can be stopped after a certain
optimality gap is reached.

Deep Neural Networks with Binary Activation Functions

3.1. Heuristic Algorithm

In Problem (H1) all variables in

In this section, we present a heuristic algorithm that is based
on local search applied to the non-linear formulation in
Lemma 1. This method is also known under the name
Mountain-Climbing method; see (Nahapetyan, 2009). The
idea is to avoid the quadratic terms by alternately optimiz-
ing the MINLP formulation over a subset of the variables
and afterward over the complement of variables. Since for
given weight variables exactly one feasible solution for the
u-variables exists, this procedure would terminate after one
iteration if all u-variables are contained in one of the prob-
lems. To avoid this problem we go through all layers and
alternately ﬁx the u or the W -variables. The second problem
uses exactly the opposite variables for the ﬁxation. More
precisely we iteratively solve the following two problems:

min

m
(cid:88)

i=1

(cid:96) (cid:0)yi, ui,K(cid:1)

s.t. W 1xi < M1ui,1 + λ1 ∀i ∈ [m]

W 1xi ≥ M1(ui,1 − 1) + λ1 ∀i ∈ [m]
W kui,k−1 < Mkui,k + λk

∀i ∈ [m], k ∈ [K] \ {1}
W kui,k−1 ≥ Mk(ui,k − 1) + λk
∀i ∈ [m], k ∈ [K] \ {1}

W k ∈ [−1, 1]dk×dk−1 , λk ∈ [−1, 1]
∀k ∈ [K] : k odd

ui,k ∈ {0, 1}dk

∀i ∈ [m], k ∈ [K − 1] : k odd

ui,K ∈ {0, 1}dK ∀i ∈ [m].

(H1)

and

min

m
(cid:88)

i=1

(cid:96) (cid:0)yi, ui,K(cid:1)

s.t. W 1xi < M1ui,1 + λ1 ∀i ∈ [m]

W 1xi ≥ M1(ui,1 − 1) + λ1 ∀i ∈ [m]
W kui,k−1 < Mkui,k + λk

∀i ∈ [m], k ∈ [K] \ {1}
W kui,k−1 ≥ Mk(ui,k − 1) + λk
∀i ∈ [m], k ∈ [K] \ {1}

W k ∈ [−1, 1]dk×dk−1, λk ∈ [−1, 1]
∀k ∈ [K] : k even

ui,k ∈ {0, 1}dk

∀i ∈ [m], k ∈ [K − 1] : k even

ui,K ∈ {0, 1}dK ∀i ∈ [m].

(H2)

ﬁx := {W k, λk, ui,k where k (cid:54)= K and k is even}
V 1

are ﬁxed to given values; while in Problem (H2) all vari-
ables in

ﬁx := {W k, λk, ui,k where k (cid:54)= K and k is odd}
V 2

are ﬁxed to given values. In the heuristic procedure the ﬁxed
variables are always set to the optimal value of the preceding
problem. Note that both problems are linear mixed-integer
problems with roughly half of the variables of Problem
(MIP). The heuristic is shown in Algorithm 1. Note that
Algorithm 1 terminates after a ﬁnite number of steps, since
there only exist ﬁnitely many possible variable assignments
for the variables ui,K, and therefore only ﬁnitely many
objective values exist.

Algorithm 1 (Local Search Heuristic)
Input: X × Y , K, d0, . . . , dK
Output: Weights W 1, . . . , W K.
Draw random values for the variables in V 1
ﬁx
repeat

ﬁx to the correspond-

Calculate optimal solution of (H1), if feasible, for the
current ﬁxations in V 1
ﬁx.
Set the values of all variables in V 2
ing optimal values of (H1).
Calculate optimal solution of (H2) if feasible for the
current ﬁxations in V 2
ﬁx.
Set the values of all variables in V 1
ing optimal values of (H2).
until no better solution is found
Return: W = {W k}k∈[K]

ﬁx to the correspond-

4. Data Uncertainty

In this section we consider labeled data

X × Y = (cid:8)(xi, yi) | i ∈ [m](cid:9)

where the euclidean norm of each data point is bounded by
r > 0 and the data points are subject to uncertainty, i.e. a
true data point xi ∈ Rn can be disturbed by an unknown
deviation vector δ ∈ Rn. One approach to tackle data uncer-
tainty is based on the idea of robust optimization and was
already studied for regression problems and support vector
machines in (Bertsimas et al., 2019a; Xu et al., 2009b;a). In
the robust optimization setting, we assume that for each data
point xi we have a convex set of possible deviation vectors
U i ⊂ Rn called uncertainty set which is deﬁned by

U i = {δ ∈ Rn | (cid:107)δ(cid:107) ≤ ri}

for a given norm (cid:107) · (cid:107) and radii r1, . . . , rm. Note that clas-
sical convex sets like boxes, polyhedrons, or ellipsoids can

Deep Neural Networks with Binary Activation Functions

be modeled as above by using the (cid:96)1 or (cid:96)2 norm. The task
is to ﬁnd the weights of a neural network which are ro-
bust against all possible data disturbances in the uncertainty
set U := U 1 × · · · × U m, i.e. we want to ﬁnd weights
W 1, . . . , W k which minimize the worst-case loss over the
training set and which are feasible for all possible distur-
bances. The difference to the situations studied in (Bertsi-
mas et al., 2019a; Xu et al., 2009b;a) is that the variables
ui,k in our model, which are representing the value of the ac-
tivation function, should be determined after the disturbance
is known since the disturbance inﬂuences the data point
and therefore the value of the activation function. Hence
we can interpret the decision as a function of the uncertain
parameters, i.e. ui,k : U i → {0, 1}. A useful approach
to tackle this situation is called two-stage robustness (or
adaptive robustness) and was already studied intensively in
the literature (Buchheim & Kurtz, 2018). Applied to our
model the two-stage robust counterpart is

min
W k,λk

max
δ∈U

min
ui,k

(cid:26) m
(cid:88)

i=1

(cid:96) (cid:0)yi, ui,K(cid:1) : W 1, . . . , W k,

λ1, . . . , λk, u1,1, . . . , um,K ∈ P (X, δ)

(cid:27)

.

(11)

where P (X, δ) is the set of feasible solutions of the inequal-
ity system

1ui,1 + λ1 ∀i ∈ [m]
1(ui,1 − 1) + λ1 ∀i ∈ [m]

W 1(xi + δi) < M i
W 1(xi + δi) ≥ M i
W kui,k−1 < Mkui,k + λk ∀i ∈ [m], k ∈ [K] \ {1}
W kui,k−1 ≥ Mk(ui,k − 1) + λk ∀i ∈ [m], k ∈ [K] \ {1}
W k ∈ [−1, 1]dk×dk−1, λk ∈ [−1, 1] ∀k ∈ [K]
ui,k ∈ {0, 1}dk ∀i ∈ [m], k ∈ [K]

and M i
1 := n(r + ri). The only difference to Constraints
(5) – (10) is the appearance of the disturbance vectors δi in
the ﬁrst layer.

The idea is that in the ﬁrst-stage the variables W k and λk
have to be calculated before the disturbance δ is known.
For each possible ﬁrst-stage solution the worst-case objec-
tive value overall scenarios (δ1, . . . , δm) ∈ U is considered
in the objective function.
In the second-stage, after the
scenario is known, the best second-stage decision for the
variables ui,k is made. Since for each combination of ﬁrst-
stage variables and scenarios in U exactly one second-stage
variable is feasible, the second-stage minimization problem
is equivalent to just ﬁnding this unique feasible solution, i.e.
checking for each neuron if it is activated under the given
disturbance or not. Problems of this form can be solved by
so-called column-and-constraint algorithms (Zeng & Zhao,

2013). The idea of this method is to start with a ﬁnite subset
of scenarios in U and iteratively add new scenarios to the
problem. The new scenario is determined by an adversarial
problem. This scenario is then added to the master-problem
together with a copy of all second-stage variables, which
will deﬁne the second-stage reaction to the new scenario. In
this method, an increasing lower bound of Problem (11) is
given by the master-problem, while the adversarial problem
provides an upper bound to the problem. Therefore in each
iteration, the method provides an upper bound on the opti-
mality gap. Unfortunately, this method performs very bad
for discrete optimization problems even if the uncertainty
only affects the objective function (K¨ammerling & Kurtz,
2020). Since to date no appropriate alternative methods ex-
ist, algorithmic progress in the ﬁeld of two-stage robustness
is desired to solve Problem (11) on realistic data sets.

5. Computations

In this section, we computationally compare the BDNN to
the classical DNN with the ReLU activation function. We
study networks with one hidden layer of dimension d1. All
solution methods were implemented in Python 3.8 on an
Intel(R) Core(TM) i5-4460 CPU with 3.20GHz and 8 GB
RAM. The classical DNN was implemented by using the
Keras API where we used the ReLU activation function
on the hidden layer and the Softmax on the output layer.
We used the binary cross entropy loss function. The num-
ber of epochs was set to 100. The exact IP formulation
is given in Theorem 2 and all IP formulations used in the
local search heuristic were implemented in Gurobi 9.0 with
standard parameter settings. The strict inequalities in the IP
formulations were replaced by non-strict inequalities adding
−0.0001 to the right-hand-side. For the IP formulations, we
set a time limit (wall time) of 24 hours.

We generated 10 random datasets in dimension N = 100
each with m = 200 data points. The entries of the data
points were drawn from a uniform distribution with values
in [0, 10] for one-third of the data points, having label 1, and
with values in [−10, 0] for the second third of the data points,
having label 0. The remaining data points were randomly
drawn with entries in [−1, 1] and have randomly assigned
labels. We split each dataset into a training set of 160 sam-
ples and a testing set of 40 samples. All computations were
implemented for neural networks with one hidden layer of
dimension d1 ∈ {20, 50, 100}. Figure 1 shows the average
classiﬁcation accuracy on the testing set over all 10 datasets
achieved by the methods trained on m of the training points.
The results indicate that the exact and the heuristic version
of the BDNN have lower accuracy than the DNN. Further-
more, the performance of both BDNN methods seem to be
much more unstable and depend more on the choice of the
training set. Interestingly for a hidden layer of dimension

Deep Neural Networks with Binary Activation Functions

Figure 1. Average accuracy over 10 random instances for networks
with one hidden layer of dimension d1 trained on m data points.

Figure 2. Average runtime (logarithmic scale) over 10 random in-
stances for networks with one hidden layer of dimension d1 trained
on m data points.

100, the heuristic method performs much better and can
even compete with the classical DNN. In Figure 2 we show
the runtime of all methods over m. Clearly, the runtime of
the BDNN methods is much higher and seems to increase
linearly in the number of data points. For real-world data
sets with millions of data points, this method will fail us-
ing state-of-the-art solvers. Surprisingly, the runtime of the
heuristic algorithm seems to be nearly the same as for the
exact version, while the accuracy can be signiﬁcantly better.

Additionally, we study all methods on the Breast Cancer
Wisconsin dataset (BCW) (Dua & Graff, 2017). Here we
also test the BDNN version where the values λk are all set
to 0 instead of being part of the variables; we indicate this

version by BDNN0. The dataset has m = 699 data points
with N = 9 attributes which we split into 80% training
data and 20% testing data. Again all computations were
implemented for neural networks with one hidden layer of
dimension d1 ∈ {25, 50}. In Tables 1 and 2 we show the
accuracy, precision, recall and F1 score of all methods on
the BCW dataset for a ﬁxed shufﬂe of the data returned by
the scikit-learn method train test split with the seed set to
42. It turns out that the exact BDNN performs better if the
values λk are set to 0, while the heuristic version performs
better if the λk are trained. The heuristic version of the
BDNN has the best performance for d1 = 25, signiﬁcantly

20406080100120140160m0.00.20.40.60.81.0Accuracyd1=20DNNBDNNBDNN Heuristic20406080100120140160m0.00.20.40.60.81.0Accuracyd1=50DNNBDNNBDNN Heuristic20406080100120140160m0.00.20.40.60.81.0Accuracyd1=100DNNBDNNBDNN Heuristic20406080100120140160m101100Runtime in sd1=20DNNBDNNBDNN Heuristic20406080100120140160m101100Runtime in sd1=50DNNBDNNBDNN Heuristic20406080100120140160m100Runtime in sd1=100DNNBDNNBDNN HeuristicDeep Neural Networks with Binary Activation Functions

better than the DNN. For d1 = 50 the DNN is slightly
better. Nevertheless, the best accuracy of 95% for the BCW
dataset was achieved by the heuristic BDNN for d1 = 25.
In Table 3 we compare the heuristic BDNN to the DNN
on 10 random shufﬂes of the BCW dataset and record the
average, maximum, and minimum accuracies over all 10
shufﬂes. It turns out that the heuristic BDNN outperforms
the DNN with the best accuracy of 97.1%.

No results are reported for the robust BDNN version, pre-
sented in Problem (11) since the column-and-constraint
algorithm was not able to terminate during 24 hours on the
random instances even for a network with d1 = 20 and
20 data points. Further future research regarding more ef-
ﬁcient optimization methods for discrete robust two-stage
problems is necessary to solve Problem (11).

6. Conclusion

We show that binary deep neural networks can be modeled
by mixed-integer programming formulations, which can be
solved to global optimality by classical integer program-
ming solvers. Additionally, we present a heuristic algorithm
and a robust version of the model. Our tests on random and
real datasets indicate that binary deep neural networks are
much more unstable and computationally harder to solve
than classical DNNs. Nevertheless, the heuristic algorithm
outperforms the classical DNN on the Breast Cancer Wis-
consin dataset. Moreover, the mixed-integer programming
formulation is very adjustable to variations of the model
and could give new insights into the understanding of deep
neural networks. This motivates further research into the
scalability of the IP method and the study of other tractable
reformulations of DL algorithms.

Table 1. Performance on the Breast Cancer Wisconsin dataset.

References

METHOD

BDNN
BDNN0
BDNN HEUR.
BDNN0 HEUR.
DNN
BDNN
BDNN0
BDNN HEUR.
BDNN0 HEUR.
DNN

d1

25
25
25
25
25
50
50
50
50
50

ACC. (%) OPT. GAP (%)

69.3
83.6
95.0
30.0
91.4
69.3
84.3
89.3
71.4
91.4

0.0
0.0
0.0
0.0
-
0.0
0.51
0.0
0.0
-

Table 2. Performance on the Breast Cancer Wisconsin dataset.

METHOD

BDNN
BDNN0
BDNN HEUR.
BDNN0 HEUR.
DNN
BDNN
BDNN0
BDNN HEUR.
BDNN0 HEUR.
DNN

d1

25
25
25
25
25
50
50
50
50
50

PREC. (%)

REC. (%)

F1 (%)

48.0
83.2
95.0
38.7
91.8
63.6
86.4
91.5
74.6
91.4

69.3
83.6
95.0
30.0
91.4
69.3
84.3
89.3
71.4
91.4

56.7
83.1
95.0
17.6
91.5
58.0
84.7
89.6
72.3
91.3

Table 3. Accuracy on the Breast Cancer Wisconsin dataset over 10
random shufﬂes of the data.

METHOD

BDNN HEUR.
DNN

d1

25
25

AVG. (%) MAX (%) MIN (%)

93.2
89.1

97.1
91.4

85.0
85.7

Barlett, P. L. and Downs, T. Using random weights to train mul-
tilayer networks of hard-limiting units. IEEE Transactions on
Neural Networks, 3(2):202–210, 1992.

Baum, E. B. and Haussler, D. What size net gives valid general-
ization? In Advances in neural information processing systems,
pp. 81–90, 1989.

Bengio, Y., L´eonard, N., and Courville, A. Estimating or prop-
agating gradients through stochastic neurons for conditional
computation. arXiv preprint arXiv:1308.3432, 2013.

Bertsimas, D., Pauphilet, J., and Van Parys, B. Sparse classiﬁcation:
a scalable discrete optimization perspective. arXiv preprint
arXiv:1710.01352, 2017.

Bertsimas, D., Dunn, J., Pawlowski, C., and Zhuo, Y. D. Robust
classiﬁcation. INFORMS Journal on Optimization, 1(1):2–34,
2019a.

Bertsimas, D., Pauphilet, J., and Van Parys, B. Sparse regression:
Scalable algorithms and empirical performance. arXiv preprint
arXiv:1902.06547, 2019b.

Buchheim, C. and Kurtz, J. Robust combinatorial optimization
under convex and discrete cost uncertainty. EURO Journal on
Computational Optimization, 6(3):211–238, 2018.

Corwin, E. M., Logar, A. M., and Oldham, W. J. An iterative
method for training multilayer networks with threshold func-
tions. IEEE Transactions on Neural Networks, 5(3):507–508,
1994.

Courbariaux, M., Bengio, Y., and David, J.-P. Binaryconnect:
Training deep neural networks with binary weights during prop-
agations. In Advances in neural information processing systems,
pp. 3123–3131, 2015.

Dua, D. and Graff, C. UCI machine learning repository, 2017.

URL http://archive.ics.uci.edu/ml.

Fischetti, M. and Jo, J. Deep neural networks and mixed integer

linear optimization. Constraints, 23(3):296–309, 2018.

Deep Neural Networks with Binary Activation Functions

Rastegari, M., Ordonez, V., Redmon, J., and Farhadi, A. XNOR-
Net: Imagenet classiﬁcation using binary convolutional neural
In European conference on computer vision, pp.
networks.
525–542. Springer, 2016.

Rumelhart, D. E., Hinton, G. E., and Williams, R. J. Learning
representations by back-propagating errors. nature, 323(6088):
533–536, 1986.

Sakurai, A. Tighter bounds of the VC-dimension of three layer
networks. In Proceedings of the World Congress on Neural
Networks, volume 3, pp. 540–543. Erlbaum, 1993.

Toms, D. Training binary node feedforward neural networks by
back propagation of error. Electronics letters, 26(21):1745–
1746, 1990.

Widrow, B. and Winter, R. Neural nets for adaptive ﬁltering and
adaptive pattern recognition. Computer, 21(3):25–39, 1988.

Wolsey, L. A. Integer programming, volume 52. John Wiley &

Sons, 1998.

Xu, H., Caramanis, C., and Mannor, S. Robust regression and
LASSO. In Advances in Neural Information Processing Systems,
pp. 1801–1808, 2009a.

Xu, H., Caramanis, C., and Mannor, S. Robustness and regulariza-
tion of support vector machines. Journal of machine learning
research, 10(Jul):1485–1510, 2009b.

Zeng, B. and Zhao, L. Solving two-stage robust optimization
problems using a column-and-constraint generation method.
Operations Research Letters, 41(5):457–461, 2013.

Goodfellow, I., Bengio, Y., and Courville, A. Deep learning. MIT

press, 2016.

Goodman, R. M. and Zeng, Z. A learning algorithm for multi-layer
perceptrons with hard-limiting threshold units. In Proceedings
of IEEE Workshop on Neural Networks for Signal Processing,
pp. 219–228. IEEE, 1994.

Gray, D. L. and Michel, A. N. A training algorithm for binary
feedforward neural networks. IEEE Transactions on Neural
Networks, 3(2):176–194, 1992.

Gurobi Optimization, L. Gurobi optimizer reference manual, 2020.

URL http://www.gurobi.com.

Hampson, S. E. and Volper, D. J. Representing and learning
boolean functions of multivalued features. IEEE transactions
on systems, man, and cybernetics, 20(1):67–80, 1990.

Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., and Bengio,
Y. Binarized neural networks: Training neural networks with
weights and activations constrained to + 1 or-1. arXiv preprint
arXiv:1602.02830, 2016.

Icarte, R. T., Illanes, L., Castro, M. P., Cire, A. A., McIlraith, S. A.,
and Beck, J. C. Training binarized neural networks using MIP
and CP. In International Conference on Principles and Practice
of Constraint Programming, pp. 401–417. Springer, 2019.

K¨ammerling, N. and Kurtz, J. Oracle-based algorithms for binary
two-stage robust optimization. Computational Optimization
and Applications, pp. 1–31, 2020.

Khalil, E. B., Gupta, A., and Dilkina, B. Combinatorial attacks on
binarized neural networks. arXiv preprint arXiv:1810.03538,
2018.

Kim, M. and Smaragdis, P. Bitwise neural networks. arXiv preprint

arXiv:1601.06071, 2016.

Kohut, R. and Steinbach, B. Boolean neural networks. Transac-

tions on Systems, 2:420–425, 2004.

LeCun, Y., Bengio, Y., and Hinton, G. Deep learning. nature, 521

(7553):436–444, 2015.

Maass, W. Perspectives of current research about the complexity
of learning on neural nets. In Theoretical advances in neural
computation and learning, pp. 295–336. Springer, 1994.

McCulloch, W. S. and Pitts, W. A logical calculus of the ideas
immanent in nervous activity. The bulletin of mathematical
biophysics, 5(4):115–133, 1943.

Nahapetyan, A. G. Bilinear Programming, pp. 279–282. Springer
US, Boston, MA, 2009.
doi:
10.1007/978-0-387-74759-0 48. URL https://doi.org/
10.1007/978-0-387-74759-0_48.

ISBN 978-0-387-74759-0.

Plagianakos, V., Magoulas, G., Nousis, N., and Vrahatis, M. Train-
ing multilayer networks with discrete activation functions. In
IJCNN’01. International Joint Conference on Neural Networks.
Proceedings (Cat. No. 01CH37222), volume 4, pp. 2805–2810.
IEEE, 2001.

Qin, H., Gong, R., Liu, X., Bai, X., Song, J., and Sebe, N. Binary
neural networks: A survey. Pattern Recognition, pp. 107281,
2020.

