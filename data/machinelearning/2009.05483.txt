Towards Interpretable Multi-Task Learning
Using Bilevel Programming

Francesco Alesiani1 ((cid:0)), Shujian Yu1, Ammar Shaker1, and Wenzhe Yin1

NEC Laboratories Europe, 69115 Heidelberg, Germany
http://www.neclab.eu (cid:63)
{Francesco.Alesiani,Shujian.Yu,Ammar.Shaker}@neclab.eu
Wenzhe.Yin@stud.uni-heidelberg.de(cid:63)(cid:63)

Abstract. Interpretable Multi-Task Learning can be expressed as learn-
ing a sparse graph of the task relationship based on the prediction
performance of the learned models. Since many natural phenomenon
exhibit sparse structures, enforcing sparsity on learned models reveals the
underlying task relationship. Moreover, diﬀerent sparsiﬁcation degrees
from a fully connected graph uncover various types of structures, like
cliques, trees, lines, clusters or fully disconnected graphs. In this paper,
we propose a bilevel formulation of multi-task learning that induces sparse
graphs, thus, revealing the underlying task relationships, and an eﬃcient
method for its computation. We show empirically how the induced sparse
graph improves the interpretability of the learned models and their re-
lationship on synthetic and real data, without sacriﬁcing generalization
performance. Code at https://bit.ly/GraphGuidedMTL

Keywords: Interpretable machine learning · Multi-Task learning · Struc-
ture learning · Sparse graph · Transfer Learning

1

Introduction

Multi-task learning (MTL) is an area of machine learning that aims at exploiting
relationships among tasks to improve the collective generalization performance
of all tasks. In MTL, learning of diﬀerent tasks is performed jointly, thus, it
transfers knowledge from information-rich tasks via task relationship [45] so
that the overall generalization error can be reduced. MTL has been successfully
applied in various domains ranging from transportation [8] to biomedicine [29].
The improvement with respect to learning each task independently is signiﬁcant
when each task has only a limited amount of training data [2].

Various multi-task learning algorithms have been proposed in the literature
(see Zhang and Yang [43] for a comprehensive survey on state-of-the-art methods).
Feature learning approaches [1] and low-rank approaches [6] assume all the tasks
are related, which may not be true in real-world applications. Task clustering

(cid:63) Manuscript accepted at ECML PKDD 2020.
(cid:63)(cid:63) Work done while at the NEC Laboratories Europe.

0
2
0
2

p
e
S
1
1

]

G
L
.
s
c
[

1
v
3
8
4
5
0
.
9
0
0
2
:
v
i
X
r
a

 
 
 
 
 
 
2

F. Alesiani, S. Yu, A. Shaker, W. Yin

approaches [27] can deal with the situation where diﬀerent tasks form clusters.
He et al. [19] propose a MTL method that is both accurate and eﬃcient; thus
applicable in presence of large number of tasks, as in the retail sector. However,
despite being accurate and scalable, these methods lack interpretability, when it
comes to task relationship.

Trustworthy Artiﬁcial Intelligence, is an EU initiative to capture the main
requirements of ethical AI. Transparency and Human oversight are among the
seven key requirements developed by the AI Expert Group [23]. Even if MTL
improves performance w.r.t to individual models, predictions made by black-box
methods can not be used as basis for decisions, unless justiﬁed by interpretable
models [17,30].

Interpretability can be deﬁned locally. LIME [36] and its generalizations [32]
are local method that extract features for each test sample that most contribute
to the prediction and aim at ﬁnding a sparse model that describes the decision
boundary. These methods are applied downstream of an independent black-box
machine learning method that produces the prediction. Inpretability can also
achieved globaly. For example linear regression, logistic regression and decision
tree [17,30] are considered interpretable1, since their parameters are directly
interpretable as weights on the input features. Global interpretability, on the
other hand, could reduce accuracy. In general, MTL methods [37,31] are not
directly interpretable, unless single tasks are learned as linear models which
are considered interpretable due to their simplicity [38,14,33]. This property,
however, is no longer guaranteed when tasks and their relations are learned
simultaneously, mainly because the relative importance of task relationship is not
revealed. Since natural phenomena are often characterized by sparse structures,
we explore the interpretability resulting from imposing the relationship among
tasks to be sparse.

To ﬁll the gap of interpretabilty in MTL, this paper introduces a novel al-
gorithm, named Graph Guided Multi-Task regression Learning (GGMTL). It
integrates the objective of joint interpretable (i.e. sparse) structure learning with
the multi-task model learning. GGMTL enjoys a closed-form hyper-gradient com-
putation on the edge cost; it also provides a way to learn the graph’s structure by
exploiting the linear nature of the regression tasks, without excessively scarifying
the accuracy of the learned models. The detailed contribution of this paper is
multi-fold:

Bilevel MTL Model: A new model for the joint learning of sparse graph
structures and multi-task regression that employs graph smoothing on the
prediction models (Sec.3.1);

Closed-form hyper-gradient: Presents a closed-form solution for the hyper-

gradient of the graph smoothing multi-task problem (Sec.3.4)

Interpretable Graph: The learning of interpretable graph structures;
Accurate Prediction: Accurate predictions on both synthetic and real-world

datasets despite the improved interpretability (Sec.4);

1 inteterpretability depends also on the application, where for example it may be

associated with weights being integer, and can be deﬁned thus diﬀerently

Towards Interpretable Multi-Task Learning Using Bilevel Programming

3

Eﬃcient computation: of the hyper-gradient of the proposed bilevel problem;
Eﬃcient method: that solves the proposed bilevel problem (Sec.3.2,3.3);
Veracity measures: to evaluate the ﬁdelity of learned MTL graph structure

(Sec.4.1)

2 Related work

2.1 Multi-task structure learning

Substantial eﬀorts have been made on estimating model parameters of each
task and the mutual relationship (or dependency) between tasks. Usually, such
relationship is characterized by a dense task covariance matrix or a task precision
matrix (a.k.a., the inverse of covariance matrix). Early methods (e.g., [34]) assume
that all tasks are related to each other. However, this assumption is over-optimistic
and may be inappropriate for certain applications, where diﬀerent tasks may
exhibit diﬀerent degrees of relatedness. To tackle this problem, more elaborated
approaches, such as clustering of tasks (e.g., [24]) or hierarchical structured tasks
(e.g., [18]) have been proposed in recent years.

The joint convex learning of multiple tasks and a task covariance matrix
was initialized in Multi-Task Relationship Learning (MTRL) [44]. Later, the
Bayesian Multi-task with Structure Learning (BMSL) [13] improves MTRL by
introducing sparsity constraints on the inverse of task covariance matrix under
a Bayesian optimization framework. On the other hand, the recently proposed
multi-task sparse structure learning (MSSL) [15] directly optimizes the precision
matrix using a regularized Gaussian graphical model. One should note that,
although the learned matrix carries partial dependency between pairwise tasks,
there is no guarantee that the learned task covariance or prediction matrix can be
transformed into a valid graph Laplacian [9]. From this perspective, the learned
task structures from these works suﬀer from poor interpretability.

2.2 Bilevel optimization in machine learning

Bilevel problems [7] raise when a problem (outer problem) contains another
optimization problem (inner problem) as constraint. Intuitively, the outer problem
(master) deﬁnes its solution by predicting the behaviour of the inner problem
(follower). In machine learning, hyper-parameter optimization tries to ﬁnd the
predictive model’s parameters w, with respect to the hyper-parameters vector λ
that minimizes the validation error. This can be mathematically formulated as
the bilevel problem

F (λ) = Es∼Dval{f (wλ, λ, s)}

min
λ
s.t. wλ = arg min

Es(cid:48)∼Dtr{g(w, λ, s(cid:48))},

w

(1a)

(1b)

The outer objective is the minimization of the generalization error Es∼Dval {f (wλ,
λ, s)} on the hyper-parameters and validation data Dval, whereas Es∼Dtr{g(w,

4

F. Alesiani, S. Yu, A. Shaker, W. Yin

λ, s)} is the regularized empirical error on the training data Dtr, see [11], where
Dval (cid:83) Dtr = D. The bilevel optimization formulation has the advantage of
allowing to optimize two diﬀerent cost functions (in the inner and outer problems)
on diﬀerent data (training/validation), thus, alleviating the problem of over-ﬁtting
and implementing an implicit cross validation procedure.

In the context of machine learning, bilevel optimization has been adopted
mainly as a surrogate to the time-consuming cross-validation which always
requires grid search in high-dimensional space. For example, [25] formulates cross-
validation as a bilevel optimization problem to train deep neural networks for
improved generalization capability and reduced test errors. [12] follows the same
idea and applies bilevel optimization to group Lasso [42] in order to determine
the optimal group partition among a huge number of options.

Given the ﬂexibility of bilevel optimization, it becomes a natural idea to cast
multi-task learning into this framework. Indeed, [28, Chapter 5] ﬁrst presents
such a formulation by making each of the individual hyperplanes (of each task)
less susceptible to variations within their respective training sets. However,
no solid examples or discussions are provided further. This initial idea was
signiﬁcantly improved in [10], in which the outer problem optimizes a proxy of
the generalization error over all tasks with respect to a task similarity matrix
and the inner problem estimates the parameters of each task assuming the task
similarity matrix is known.

3 Graph guided MTL

3.1 Bilevel multi-tasking linear regression with graph smoothing

We consider the problem of ﬁnding regression models {wi} for n tasks, with
input/output data {(Xi, yi)}n
i=1, where Xi ∈ RNi×d, yi ∈ RNi×1 and d is the
feature size, while Ni is the number of samples for the ith task2. We split the data
i=1 and training {(X tr
into validation {(X val
i=1 sets and formulate
the problem as a bilevel program:

i , ytr

i )}n

, yval
i

)}n

i

min
e

(cid:88)

i∈[n]

||X val

i we,i − yval

i

||2 + ξ||e||2

2 + η||e||1 + γH(e)

Ve = arg min

V

(cid:88)

i∈[n]

||X tr

i wi − ytr

i ||2 +

1
2

λ tr(V T LeV ) ,

(2a)

(2b)

1 , . . . , wT

n ]T is the models’ vectors, Le = (cid:80)
where V = [wT
ij∈G eij(di − dj)(di −
dj)T = E diag(e)ET is the Laplacian matrix deﬁned using the incident matrix
E, e = vec([eij]) is the edge weight vector with [eij] being the adjacent matrix,
and di is the discrete indicator vector which is zero everywhere except at the

2 In the following we assume for simplicity Ni = N for all tasks, but results extend

straightforward.

Towards Interpretable Multi-Task Learning Using Bilevel Programming

5

i-th entry. We use [n] for the set {1, . . . , n}. The regularization term in the inner
problem is the Dirichlet energy [5]

tr(V T LeV ) =

(cid:88)

ij∈G

eij||wi − wj||2
2 ,

(3)

where G is the graph whose Laplacian matrix is Le. H(e) = − (cid:80)
|eij|) is the un-normalized entropy of the edge values.

ij∈G(|eij| ln |eij|−

The inner problem (model learning) aims at ﬁnding the optimal model for
a given structure (i.e. graph), while the outer problem (structure learning)
aims at minimizing a cost function that includes two terms: (1) the learned
model’s accuracy on the validation data, and (2) the sparseness of the graph.
We capture the sparseness of the graph with three terms: (a) the (cid:96)2
2 norm of the
edge values, measuring the energy of the graph, (b) the (cid:96)1 norm measuring the
sparseness of the edges, and (c) H(e) measuring the entropy of the edges. In
the experiment, we limit the edges to have values in the interval [0, 1], which
can be interpreted as a relaxation of the mixed integer non-linear programming
problem when eij ∈ {0, 1} as deﬁned in Eq.2. The advantage of formulating the
MTL learning as a bilevel program (Eq.2) is the ability to derive a closed-form
solution for the hyper-gradient (see Thm.1). Moreover, for a proper choice of the
regularization parameter ((cid:96)1), all edge weights have a closed-form solution (see
Thm.2). For the general case, we propose a gradient descent algorithm (Alg.1).
Entropy regularization term has superior sparsiﬁcation performance to the (cid:96)1
norm regularization [21], thus, the latter can be ignored during hyper-parameter
search to reduce the search space at the expense of a improved ﬂexibility.

For simplicity, we deﬁne the functions:

||X val

i we,i − yval

i

||2 + ξ||e||2

2 + η||e||1 + γH(e)

(4a)

f (Ve, e) =

g(V, e) =

(cid:88)

i∈[n]

(cid:88)

i∈[n]

||X tr

i wi − ytr

i ||2 +

1
2

λ tr(V T LeV ) ,

which allow us to write the bilevel problem in the compact form:

min
e

f (Ve, e) s.t. Ve = arg min

V

g(V, e) .

The proposed formulation optimally selects the sparser graph among tasks

that provides the best generalization performance on the validation dataset.

3.2 The (cid:96)2 norm-square regularization (cid:96)2

2-GGMTL algorithm

We propose an iterative approach that computes the hyper-gradient of f (Ve, e)
(eq.4a) with respect to the graph edges (the hyper-parameters); this hyper-
gradient is then used for updating the hyper-parameters based on the gradient
descend method, i.e.,

e(t+1) = e(t) + νdef (Ve(t), e(t)) ,

(6)

(4b)

(5)

6

F. Alesiani, S. Yu, A. Shaker, W. Yin

Algorithm 1: GGMTL: (cid:96)2-GGMTL, (cid:96)2

2-GGMTL

Input
Output
for i ← 1 to n do

: {Xt, yt} for t = {1, 2, ..., n}, ξ, η, λ, ν
n ]T , Le
: V = [wT

1 , ..., wT

Solve wi by Linear Regression on {Xi, yi}
Construct k-nearest neighbor graph G on V ;
Construct E the incident matrix of G;
// validation-training split
{X tr
t }, {X val
while not converge do

t } ← split({Xt, yt}) ;

t , ytr

, yval

t

// compute hyper-gradient
compute def (Ve(t) , e(t)) using Eq. (10), (where, with (cid:96)2 norm, Eq. (10) is
computed using e = e ◦ l(V ) of Eq.8 and alternating with solution of Eq.9
(given by Eq.12 or in Thrm.1).;
// edges’ values update
Update e: e(t+1) = [e(t) + νdef (Ve(t) , e(t))]+;

// Train on the full datasets with alternate optimization
Solve Eq.7 on {Xt, yt};
return V, Le;

where de is the hyper-gradient and ν is the learning rate. Algorithm Alg.1 depicts
the structure of the GGMTL learning method, where [x]+ = max(0, x). The
stopping criterion is evaluated on the convergence of the validation and training
errors. As a ﬁnal step, the tasks’s models are re-learned on all training and
validation data based on the last discovered edge values.

3.3 The (cid:96)2 norm regularization (cid:96)2-GGMTL algorithm

The energy smoothing term Eq.3 in the inner problem of Eq.2 is a quadratic
term. However, if two models are unrelated, but connected by an erroneous edge,
this term grows quadratically dominating the loss. To reduce this undesirable
eﬀect, a term proportional to the distance can be achieved using not-squared (cid:96)2
norm. Therefore, we extend the inner problem of Eq.(2) of the previous model to
become:

arg min

V

g(V, e) = arg min

V

(cid:88)

i∈[n]

||X tr

i wi − ytr

i ||2 +

1
2

λ

(cid:88)

ij∈G

eij||wi − wj||2 , (7)

where the regularization term in the inner problem is the non-squared (cid:96)2. This
can be eﬃciently solved using alternating optimization [19], by deﬁning the vector
of edges’ multiplicative weights l = vec([lij]) = l(V ) such that:

lij = 0.5/||wi − wj||2 .

(8)

Towards Interpretable Multi-Task Learning Using Bilevel Programming

7

We can now formulate a new optimization problem equivalent to Eq.7

Ve(l) = arg min

V

g(V |e, l)

= arg min

V

(cid:88)

i∈[n]

||X tr

i wi − ytr

i ||2 +

1
2

λ tr(V T Le◦lV ) +

1
4

l−◦

(9a)

(9b)

where ◦ is the element-wise product, Le◦l is the Laplacian matrix whose edge
values are the element-wise product of e and l (e ◦ l), while l−◦ is a short notation
for the element-wise inverse of l. Having ﬁxed l, the last term of Eq.9 can be
ignored, while optimizing the inner problem w.r.t. V . The modiﬁed algorithm
Alg.1 ((cid:96)2-GGMTL), which can also be found in the supplementary material
(Alg.3), uses alternate optimization between the closed-form solution in Eq.8 and
the solution of Eq.9 over V .

3.4 Hyper-gradient

The proposed method (Alg.1) is based on the computation of the hyper-gradient
of Eq.2. This hyper-gradient has a closed-form as deﬁned by Thm.1 and can be
computed eﬃciently.

Theorem 1. The hyper-gradient of problem of Eq.2a is

def (Ve, e) = ξe + η sign(e) − γ sign(e) ◦ ln e

−λ(V T ⊗ Im)(BT ⊗ Id)A−T X val,T (X valV − Y val)

(10)

11, . . . , bnnbT

where B = [b11bT
nn] ∈ Rn×nm and B is build with only the m non-zero
edges (i.e. |{ij|ij ∈ G}| = m).. The other variables are bij = (di − dj) ∈ Rn×1,
A = λLe ⊗ Id + X T X ∈ Rdn×dn and V = A−1X T Y , Le = (cid:80)
ij, V =
n ]T ∈ Rdn×1, X = diag(X1, . . . , Xn) ∈ RN n×dn, Y = [y1, . . . , yn] ∈
[wT
RN n×1. ln e is the element wise logarithm of the vector e and ◦ is the Hadamard
product. sign(x) is the element-wise sign function of x.

ij eijbijbT

1 , . . . , wT

We notice that B and A in Thm.1 are sparse matrices. This leads to eﬃcient
computation of the hyper-gradient, as shown in Thm.3. All proofs are reported
in the Supplementary Material (Sec.A.1).

3.5 Closed-form hyper-edges

Alternative to applying gradient descent methods using the hyper-gradient up-
dates, the optimal edges’ values can also be directly computed. We compute e
(the edge vector) as the solution of def (Ve, e) = 0, since the optimal solution has
zero hyper-gradient. In the case when (cid:96)1 is the only term that has a non-zero
weight in Eq.2a, the edge vector e has a closed-form solution as proven in Thm.2.

8

F. Alesiani, S. Yu, A. Shaker, W. Yin

Theorem 2. Let suppose ξ = 0, γ = 0, η (cid:54)= 0, then the hyper-edges of problem
of Eq.2 is the solution of

U e = v

(11)

where U = (zT ⊗ (E ⊗ Id))K, K = [vec(diag(d0) ⊗ Id), . . . , vec(diag(dm−1) ⊗
Id)] ∈ Rm2d2×m, where di ∈ Rm×1 is the indicator vector and u = M −11m,
v = 1/ηC − 1/λX T Xu, z = (ET ⊗ Id)u, M = (V T ⊗ Im)(BT ⊗ Id) ∈ Rm×dn ,
C = X val,T (X valV − Y val) ∈ Rnd×1 and E, V, B as in Thm.1.

3.6 Complexity analysis

GGMTL algorithm computes the tasks’ models kNN graph, whose computational
complexity can be reduced from O(n2) to O(nd ln n)[3] 3, while one iteration
of GGMTL algorithm computes the hyper-gradient. A naive implementation of
this step requires inverting a system of dimension nd × nd, whose complexity is
O((dn)3). It would thus come to surprise that the actual computational complexity
of the GGMTL method is O(nd ln n + (nd)1.31 + dn2), where the second two
terms follow from Thm.3, while O(dn2) is the matrix-vector product which can
be performed in parallel.

Theorem 3. The computational complexity of solving hyper-gradient of Thrm. 1
is O((nd)1.31 + dn2) (or O((nd) lnc(nd) + dn2), with c constant).

4 Experimental results

We evaluate the performance of GGMTL against four state-of-the-art multi-
task learning methodologies (namely MTRL [44], MSSL [15], BSML [13], and
CCMTL [19]) on both synthetic data and real-world applications. Among the four
competitors, MTRL learns a graph covariance matrix, MSSL and BSML directly
learn a graph precision matrix which can be interpreted as a graph Laplacian.
By contrast, CCMTL does not learn task relationship, but uses a ﬁxed k-NN
graph before learning model parameters4.

4.1 Measures

Synthetic dataset measure for veracity To evaluate the performance of
the proposed method on the synthetic dataset, we propose a reformulation of
the measures: accuracy, recall and precision by applying the (cid:32)Lukasiewicz fuzzy
T-norm (cid:62)(a, b) = max(a + b − 1, 0) and T-conorm ⊥(a, b) = min(a, b) [26], where
a, b represent truth values from the interval [0, 1]. Given the ground truth graph

3 or, using Approximate Nearest Neighbour (ANN) methods, to O(nd) [22]
4 We performed grid-search hyper-parameter search for all methods

Towards Interpretable Multi-Task Learning Using Bilevel Programming

9

G1 and the predicted graph G2 (on n tasks) with proper adjacency matrices A1
and A2 (i.e., a(1)
i,j ∈ [0, 1] for all 0 ≤ i, j ≤ n), we deﬁne:

i,j , a(2)

recall =

precision =

accuracy = 1 −

(cid:80)

(cid:80)

i,j ,a(2)
i,j )

i,j

(cid:80)

0≤i,j≤n (cid:62)(a(1)
0≤i,j≤n a(1)
0≤i,j≤n (cid:62)(a(1)
0≤i,j≤n a(2)
0≤i,j≤n ⊕(a(1)
n2

(cid:80)

(cid:80)

i,j

i,j ,a(2)
i,j )

,

,

i,j ,a(2)
i,j )

s.t ⊕(a, b) = (cid:62)(⊥(a, b), 1 − (cid:62)(a, b)) is the fuzzy XOR, see [4]. The deﬁnition of
the F1 score remains unchanged as the harmonic mean of precision and recall.
These measures inform about the overlap between a predicted (weighted) graph
and a ground truth sparse structure, in a similar way to imbalanced classiﬁcation.
An alternative and less informative approach would be to compute Hamming
distance between the two adjacency matrices (ground truth and induced graph),
provided they are both binary.

Regression performance The generalization performance is measured in terms
of the Root Mean Square Error (RMSE) averaged over tasks.

Table 1: Results on the synthetic data of each of GGMTL, BMSL, MMSL and
CCMTL (KNN) in terms of the measures described in Subsec. 4.1.

BMSL

MSSL

CCMTL

GGMTL

recall

accuracy line 0.384 ± 4.1e−2 0.180 ± 1.1e−2 0.631 ± 2.1e−2 0.648 ± 3.3e−2
tree 0.388 ± 5.2e−2 0.141 ± 1.1e−2 0.722 ± 1.6e−2 0.770 ± 1.2e−2
star 0.581 ± 8.4e−2 0.726 ± 2.9e−2 0.405 ± 3.1e−2 0.460 ± 7.8e−2
line 0.688 ± 5.6e−2 0.958 ± 1.5e−2 0.288 ± 4.4e−2 0.726 ± 1.0e−1
tree 0.653 ± 6.3e−2 0.946 ± 1.3e−2 0.390 ± 2.5e−2 0.790 ± 7.8e−2
star 0.318 ± 1.3e−1 0.311 ± 1.1e−1 0.706 ± 7.2e−2 0.664 ± 1.6e−1
precision line 0.100 ± 8.1e−4 0.100 ± 1.5e−4 0.083 ± 7.1e−3 0.175 ± 2.7e−2
tree 0.065 ± 8.4e−4 0.065 ± 9.9e−5 0.092 ± 2.6e−3 0.185 ± 1.6e−2
star 0.149 ± 4.7e−2 0.238 ± 5.4e−2 0.176 ± 1.2e−2 0.185 ± 3.1e−2
line 0.175 ± 2.6e−3 0.182 ± 3.4e−4 0.129 ± 1.3e−2 0.282 ± 4.3e−2
tree 0.117 ± 1.9e−3 0.121 ± 1.8e−4 0.149 ± 4.0e−3 0.300 ± 2.6e−2
star 0.199 ± 6.7e−2 0.266 ± 7.3e−2 0.281 ± 2.1e−2 0.288 ± 5.e−2

F1

RMSE line 6.968 ± 1.5
tree 7.444 ± 1.4
star 4.784 ± 2.4e−1 1.616 ± 2.7e−1 0.507 ± 2.25e−1 0.300 ± 2.11e−1

4.838 ± 7.6e−1 4.342 ± 7.21e−1 4.342 ± 7.21e−1
4.879 ± 1.3

4.207 ± 1.141

4.207 ± 1.141

10

F. Alesiani, S. Yu, A. Shaker, W. Yin

4.2 Synthetic data

In order to evaluate the veracity of the proposed method, we generate three
synthetic datasets where the underlying structure of the relationship among tasks
is known. Each task t in these datasets is a linear regression task whose output is
controlled by the weight vector wt. Each input variable x, for task t, is generated
i.i.d. from an isotropic multivariate Gaussian distribution, and the output is
taken by y = wT

t x + (cid:15), where (cid:15) ∼ N (0, 1).

(a) GGMTL (line)

(b) GGMTL (tree)

(c) GGMTL (star)

(d) kNN (line)

(e) kNN (tree)

(f) kNN (star)

Fig. 1: The discovered graphs by GGMTL and k-NN on the synthetic datasets:
Line, Tree and Star.

The ﬁrst dataset Line mimics the structure of a line, where each task is
generated with an overlap to its predecessor task. This dataset contains 20 tasks of
30 input dimensions. The coeﬃcient vector for tasks t is wt = wt−1 +0.1u30 (cid:12)b30,
where (cid:12) denotes the pointwise product, u30 is a 30-dimensional random vector
with each element uniformly distributed between [0, 1], b30 is also a 30-dimensional
binay vector whose elements are Bernoulli distributed with p = 0.7, and w0 ∼
N (1, I30).

The tasks of the second dataset Tree are created in a hierarchical manner
simulating a tree structure such that wt = wt(cid:48) + 0.1u30 (cid:12) b30, where wt(cid:48) is
coeﬃcient vector of the parent task (t(cid:48) = (cid:98)(t − 1)/2(cid:99)), and w0 ∼ N (1, I30) is

Towards Interpretable Multi-Task Learning Using Bilevel Programming

11

for the root task. In order to create a proper binary tree, we generate 31 tasks
(30-dimensional) which creates a tree of ﬁve levels.

The distribution of the third dataset’s tasks takes a star-shaped structure,
hence called Star. The Coeﬃcient vector of each task t is randomly created
(wt ∼ N (1, I20) for t ∈ {1, . . . , 10}), and the center one is a mixture of them
w0 = (cid:80)
t∈{1,...,10} wt · (e2t−1 + e2t), where ei ∈ RT is an indicator vector with
the ith element set to 1 and the others to 0. We evaluate the performance of our
method in comparison to the other methods on two aspects: (i) the ability to
learn graphs that recover hidden sparse structures, and (ii) the generalization
performance. For generalization error we use Root Mean Square Error (RMSE).

Tab.1 depicts the results of comparing the graphs learned by GGMTL with
those of CCMTL, and the covariance matrices of MSSL and BMSL when consid-
ered as adjacency matrices, after few adjustments 5. It is apparent that GGMTL
always achieves the best accuracy except on the Star dataset when MSSL per-
forms best in terms of accuracy; this occurs only because MSSL predicts an
extremely sparse matrix leading to poor recall, precision and F1 score. More-
over, GGMTL has always the best F1 score achieved by correctly predicting
the right balance between edges (with 2nd best recall) and sparseness (always
best precsion), thus, leading to correctly interpreting and revealing the latent
structure of task relations. Besides the quantitative measures, interpretability
is also conﬁrmed qualitatively in Fig.1 where the discovered edges reveal to a
large extent the ground truth structures. The ﬁgure also plots the k-NN graph
next to that of GGMTL, this shows how graphs of GGMTL pose a reﬁnement of
those of k-NN by removing misplaced edges while still maintaining the relevant
ones among tasks. Finally, Tab.1 also shows that GGMTL commits the smallest
generalization error in terms of RMSE with a large margin to BMSL and MSSL.

Table 2: RMSE (mean±std) on Parkinson’s disease data set over 10 independent
runs on various train/test ratios r. Best two performances underlined.

split MTRL

MSSL

BMSL

CCMTL

GGMTL

r = 0.3 4.147 ± 3.038 1.144 ± 0.007 1.221 ± 0.11 1.037 ± 0.013 1.037 ± 0.012
r = 0.4 3.202 ± 2.587 1.129 ± 0.011 1.150 ± 0.1
r = 0.5 1.761 ± 0.85 1.130 ± 0.009 1.110 ± 0.085 1.010 ± 0.008 1.010 ± 0.008
r = 0.6 1.045 ± 0.05 1.123 ± 0.013 1.068 ± 0.036 0.998 ± 0.017 1.000 ± 0.017

1.017 ± 0.008 1.019 ± 0.01

5 A negative correlation is considered as a missing edge between tasks, hence, negative
entries are set to zero. Besides, we normalize each matrix by division over the largest
entry after setting the diagonal to zero

12

F. Alesiani, S. Yu, A. Shaker, W. Yin

(a) GGMTL (Parkinson)

(b) kNN (Parkinson)

(c) GGMTL (School)

(d) kNN (School)

(e) GGMTL (temperature)

(f) kNN (temperature)

Fig. 2: The discovered graphs by GGMTL and k-NN on the Parkinson, School
and South datasets.

Towards Interpretable Multi-Task Learning Using Bilevel Programming

13

4.3 Real-world applications

Parkinson’s disease assessment. Parkinson is a benchmark multi-task re-
gression dataset 6, comprising a range of biomedical voice measurements taken
from 42 patients with early-stage Parkinson’s disease. For each patient, the goal
is to predict the motor Uniﬁed Parkinson’s Disease Rating Scale (UPDRS) score
based 18-dimensional record: age, gender, and 16 jitter and shimmer voice mea-
surements. We treat UPDRS prediction for each patient as a task, resulting in 42
tasks and 5, 875 observations in total. We compare the generalization performance
of GGMTL with that of the other baselines when diﬀerent ratios of the data is
used for training, ratio r ∈ {0.3, 0.4, 0.5, 0.6}. The results depicted in Tab.2 show
that GGMTL performance is close to that of CCMTL, outperforming MSSL and
BMSL. However, when plotting the learned graphs, see Fig.2(a) and Fig.2(b),
GGMTL clearly manage to separate patients into a few distinct groups unlike
the heavily connected k-NN graph used by CCMTL. Interestingly, these groups
are easily distinguished by Markov Clustering [41] when applied on the learned
graph; this very same procedure fails to distinguish reasonable clusters when
applied on the K-NN graph (35 clusters were discovered with only one task in
each, and one cluster with ﬁve tasks).

Table 3: RMSE (mean±std) on School data set over 10 independent runs on
various train/test ratios r. Best two performances underlined.

MTRL

MSSL

BMSL

CCMTL

GGMTL

r = 0.2 11.276 ± 0.103 11.727 ± 0.137 10.430 ± 0.056 10.129 ± 0.038 10.137 ± 0.034
r = 0.3 10.761 ± 0.045 11.060 ± 0.045 10.223 ± 0.044 10.078 ± 0.038 10.072 ± 0.037
r = 0.4 10.440 ± 0.069 10.632 ± 0.076 10.084 ± 0.067 10.059 ± 0.067 10.056 ± 0.068
r = 0.5 10.267 ± 0.065 10.437 ± 0.040 10.048 ± 0.043 9.994 ± 0.052 9.996 ± 0.051

Exam score prediction School is a classical benchmark dataset in Multi-task
regression [1,27,45]; it consists of examination scores of 15, 362 students from 139
schools in London. Each school is considered as a task and the aim is to predict
the exam scores for all the students. The school dataset is available in the Malsar
package [46].

Tab.3 reports the RMSE on the School dataset. It is noticeable that both
GGMTL and CCMTL have similar but dominating performance over the other
methods. As with the Parkinson data, Fig.2(c) and Fig.2(d) compare the graphs
induced by GGMTL and CCMTL(k-NN), and the results of applying Markov
clustering on their nodes. These ﬁgures show again that graphs induced by
GGMTL are easier to interpret and lead to well separated clusters with only few
intercluster edges.

6 https://archive.ics.uci.edu/ml/datasets/parkinsons+telemonitoring

14

F. Alesiani, S. Yu, A. Shaker, W. Yin

Table 4: RMSE (mean±std) on US America Temperature data. The best two
performances are underlined.

Hours MTRL MSSL BMSL CCMTL GGMTL

8
16

NA 1.794 0.489
NA 1.643 0.606

0.101
0.0975

0.101
0.0973

Temperature forecasting in U.S. The Temperature dataset7 contains hourly
temperature data for major cities in the United States, collected from n = 109
stations for 8759 hours in 2010. Data is cleaned and manipulated as described in
[20]. The temperature forecasting with a horizon of 8 or 16 hours in advance at
each station is model as a task. We select the ﬁrst 20% observations (roughly
5 weeks) to train and left the remaining 80% observations for test. We use
previous 30 hours of temperature as input to the model. Tab.4 reports the RMSE
of the methods. Learning the graph structure using GGMTL does not impact
performance in term of regression error. Fig.2[e-f] shows node clustering on the
graph learned on Temerature dataset, where the number of edges is reduced by
60% using GGMTL.

5 Conclusions

In this work, we present a novel formulation of joint multi-task and graph structure
learning as a bilevel problem, and propose an eﬃcient method for solving it based
on a closed-form of hyper-gradient. We also show the interpretability property
of the proposed method on synthetic and real world datasets. We additionally
analyze the computational complexity of the proposed method.

References

1. Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil. Multi-task

feature learning. In NIPS, pages 41–48, 2007.

2. Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil. Convex multi-

task feature learning. Machine Learning, 73(3):243–272, 2008.

3. Sunil Arya, David M Mount, Nathan S Netanyahu, Ruth Silverman, and Angela Y
Wu. An optimal algorithm for approximate nearest neighbor searching ﬁxed
dimensions. Journal of the ACM (JACM), 45(6):891–923, 1998.

4. Benjam´ın C Bedregal, Renata HS Reiser, and Gra¸caliz P Dimuro. Xor-implications
and e-implications: classes of fuzzy implications based on fuzzy xor. Electronic
notes in theoretical computer science, 247:5–18, 2009.

5. Mikhail Belkin and Partha Niyogi. Laplacian Eigenmaps and Spectral Techniques
for Embedding and Clustering. In Advances in Neural Information Processing
Systems 14, pages 585–591. MIT Press, 2002.

7 https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-

based-datasets/climate-normals/1981-2010-normals-data

Towards Interpretable Multi-Task Learning Using Bilevel Programming

15

6. Jianhui Chen, Jiayu Zhou, and Jieping Ye. Integrating low-rank and group-sparse
structures for robust multi-task learning. In KDD, pages 42–50. ACM, 2011.

7. Benoˆıt Colson, Patrice Marcotte, and Gilles Savard. An overview of bilevel opti-

mization. Annals of operations research, 153(1):235–256, 2007.

8. Dingxiong Deng, Cyrus Shahabi, Ugur Demiryurek, and Linhong Zhu. Situation
aware multi-task learning for traﬃc prediction. In ICDM, pages 81–90. IEEE, 2017.
9. Xiaowen Dong, Dorina Thanou, Pascal Frossard, and Pierre Vandergheynst. Learn-
ing laplacian matrix in smooth graph signal representations. IEEE Transactions
on Signal Processing, 64(23):6160–6173, 2016.

10. R´emi Flamary, Alain Rakotomamonjy, and Gilles Gasso. Learning constrained task
similarities in graphregularized multi-task learning. Regularization, Optimization,
Kernels, and Support Vector Machines, page 103, 2014.

11. Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimilano
Pontil. Bilevel Programming for Hyperparameter Optimization and Meta-Learning.
arXiv:1806.04910 [cs, stat], June 2018.

12. Jordan Frecon, Saverio Salzo, and Massimiliano Pontil. Bilevel learning of the
group lasso structure. In Advances in Neural Information Processing Systems, pages
8301–8311, 2018.

13. Andre Goncalves, Priyadip Ray, Braden Soper, David Widemann, Mari Nyg˚ard,
Jan F Nyg˚ard, and Ana Paula Sales. Bayesian multitask learning regression for
heterogeneous patient cohorts. Journal of Biomedical Informatics: X, 4:100059,
2019.

14. Andre R. Goncalves, Puja Das, Soumyadeep Chatterjee, Vidyashankar Sivakumar,
Fernando J. Von Zuben, and Arindam Banerjee. Multi-task Sparse Structure
Learning. Proceedings of the 23rd ACM CIKM ’14, 2014.

15. Andr´e R Gon¸calves, Fernando J Von Zuben, and Arindam Banerjee. Multi-task
sparse structure learning with gaussian copula models. The Journal of Machine
Learning Research, 17(1):1205–1234, 2016.

16. Andre R Goncalves, Fernando J Von Zuben, and Arindam Banerjee. Multi-task
Sparse Structure Learning with Gaussian Copula Models. Journal of Machine
Learning Research 17 (2016), page 30, 2016.

17. Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Gian-
notti, and Dino Pedreschi. A Survey of Methods for Explaining Black Box Models.
ACM Computing Surveys, 51(5):1–42, August 2018.

18. Lei Han and Yu Zhang. Learning multi-level task groups in multi-task learning. In

AAAI, volume 15, pages 2638–2644, 2015.

19. Xiao He, Francesco Alesiani, and Ammar Shaker. Eﬃcient and Scalable Multi-task
Regression on Massive Number of Tasks. In The Thirty-Third AAAI Conference
on Artiﬁcial Intelligence (AAAI-19), 2019.

20. Fei Hua, Roula Nassif, C´edric Richard, Haiyan Wang, and Ali H Sayed. Online dis-
tributed learning over graphs with multitask graph-ﬁlter models. IEEE Transactions
on Signal and Information Processing over Networks, 6:63–77, 2020.

21. Shuai Huang and Trac D. Tran. Sparse Signal Recovery via Generalized Entropy
Functions Minimization. IEEE Transactions on Signal Processing, 67(5):1322–1337,
March 2019.

22. Ville Hyv¨onen, Teemu Pitk¨anen, Sotiris Tasoulis, Elias J¨a¨asaari, Risto Tuomainen,
Liang Wang, Jukka Corander, and Teemu Roos. Fast k-nn search. arXiv:1509.06957,
2015.

23. High-Level Expert Group on Artiﬁcial Intelligence. Policy and investment recom-
mendations for trustworthy AI. June 2019. Publisher: European Commission Type:
Article; Article/Report.

16

F. Alesiani, S. Yu, A. Shaker, W. Yin

24. Laurent Jacob, Jean-philippe Vert, and Francis R Bach. Clustered multi-task

learning: A convex formulation. In NIPS, pages 745–752, 2009.

25. Simon Jenni and Paolo Favaro. Deep bilevel learning. In Proceedings of the European

Conference on Computer Vision (ECCV), pages 618–633, 2018.

26. Erich Peter Klement, Radko Mesiar, and Endre Pap. Triangular Norms. Kluwer

Academic Publishers, Dordrecht, The Netherlands, 2000.

27. Abhishek Kumar and Hal Daume III. Learning task grouping and overlap in

multi-task learning. ICML, 2012.

28. Gautam Kunapuli. A bilevel optimization approach to machine learning. PhD

thesis, PhD thesis, Rensselaer Polytechnic Institute, 2008.

29. Limin Li, Xiao He, and Karsten Borgwardt. Multi-target drug repositioning by
bipartite block-wise sparse multi-task learning. BMC systems biology, 12, 2018.
30. Zachary C. Lipton. The mythos of model interpretability. Communications of the

ACM, 61(10):36–43, September 2018.

31. Pengfei Liu, Jie Fu, Yue Dong, Xipeng Qiu, and Jackie Chi Kit Cheung. Multi-task

Learning over Graph Structures. arXiv:1811.10211 [cs], November 2018.

32. Scott Lundberg and Su-In Lee. A Uniﬁed Approach to Interpreting Model Predic-

tions. arXiv:1705.07874 [cs, stat], November 2017. arXiv: 1705.07874.

33. Keerthiram Murugesan, Jaime Carbonell, Hanxiao Liu, and Yiming Yang. Adaptive

Smoothed Online Multi-Task Learning. page 11, 2016.

34. Guillaume Obozinski, Ben Taskar, and Michael I Jordan. Joint covariate selection
and joint subspace selection for multiple classiﬁcation problems. Statistics and
Computing, 20(2):231–252, 2010.

35. Kaare Brandt Petersen and Michael Syskind Pedersen. The Matrix Cookbook.

February 2008.

36. Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ”Why Should I Trust
You?”: Explaining the Predictions of Any Classiﬁer. arXiv:1602.04938 [cs, stat],
August 2016.

37. Sebastian Ruder. An overview of multi-task learning in deep neural networks. arXiv

preprint arXiv:1706.05098, 2017.

38. Avishek Saha, Piyush Rai, Hal Daume Iii, and Suresh Venkatasubramanian. Online

Learning of Multiple Tasks and Their Relationships. page 9, 2011.

39. Daniel A. Spielman and Shang-Hua Teng. Solving Sparse, Symmetric, Diagonally-
Dominant Linear Systems in Time $O (mˆ{1.31})$. arXiv:cs/0310036, March
2004.

40. Daniel A. Spielman and Shang-Hua Teng. Nearly-Linear Time Algorithms for
Preconditioning and Solving Symmetric, Diagonally Dominant Linear Systems.
arXiv:cs/0607105, September 2012.

41. Stijn Marinus Van Dongen. Graph clustering by ﬂow simulation. PhD thesis, 2000.
42. Ming Yuan and Yi Lin. Model selection and estimation in regression with grouped
variables. Journal of the Royal Statistical Society: Series B (Statistical Methodology),
68(1):49–67, 2006.

43. Yu Zhang and Qiang Yang. A survey on multi-task learning. arXiv preprint

arXiv:1707.08114v2, 2017.

44. Yu Zhang and Dit-Yan Yeung. A convex formulation for learning task relationships
in multi-task learning. In Proceedings of the Twenty-Sixth Conference on UAI,
pages 733–742, 2010.

45. Yu Zhang and Dit-Yan Yeung. A regularization approach to learning task relation-

ships in multitask learning. ACM Transactions on TKDD, 8, 2014.

46. Jiayu Zhou, Jianhui Chen, and Jieping Ye. Malsar: Multi-task learning via structural

regularization. Arizona State University, 21, 2011.

Towards Interpretable Multi-Task Learning Using Bilevel Programming

17

A Supplementary material

This Supplementary material contains:

– The proofs of the Theorems stated in the main part (Annex A.1);
– the two versions of the algorithm: (cid:96)2

2 − GGM T L and (cid:96)2 − GGM T L (Annex

A.2)

– additional datasets on the America climate. This dataset shows also the
computational eﬃciency of the method that we were able to apply to moderate
large size dataset (i.e. 490 nodes of the North America dataset) (Annex A.3)

A.1 Proof of theorems

Proof (Theorem 1). Since V = [wT
n ]T ∈ Rdn×1 the vector of all models,
1 , . . . , wT
we deﬁne X = diag(X1, . . . , Xn) ∈ RN n×dn the block matrix of input and
Y = [y1, . . . , yn] ∈ RN n×1 the vector of the output, the solution of the lower
problem is

(λLe ⊗ Id + X T X)Ve = X T Y

or

Ve = A−1X T Y

(12)

where we deﬁne the auxiliary matrix A = λLe ⊗ Id + X T X

Using the Sherman Morrison formula

(A + uvT )−1 = A−1 −

A−1uvT A−1
1 + vT A−1u

and deﬁning b = (cid:112)λδeij(di − dj) ⊗ Id we can write the increment dijV on the
edge ij of δeij as the diﬀerence of the models,

dijV = (bbT + A)−1X T Y − V

= A−1X T Y −

A−1bbT A−1
1 + bT A−1b

= −

A−1bbT A−1
1 + bT A−1b

X T Y

X T Y − A−1X T Y

if δeij → 0 then bT A−1b → 0

∇eij V (e) = −λA−1((di − dj) ⊗ Id)((di − dj)T ⊗ Id)A−1X T Y

= −λA−1((di − dj)(di − dj)T ⊗ Id)V

(13)

(14)

(15)

(16)

dij V
since ∇eij V (e) = limδeij→0
δeij
of parameter gradient with respect to the hyper-parameters

(see also the Thm.5). Thus we have the shape

18

F. Alesiani, S. Yu, A. Shaker, W. Yin

∇eT V (e) = −λA−1([b11bT

11, . . . , bnnbT
(cid:123)(cid:122)
B: m items

nn]
(cid:125)

(cid:124)

= −λA−1(B ⊗ Id)(V ⊗ Im)

⊗Id)(V ⊗ Im)

(17)

(18)

where Bij = bijbT
and ∇eT V (e) ∈ Rdn×m.

ij = (di −dj)(di −dj)T , A = λLe ⊗Id +X T X, V = A−1X T Y

We have that

or for all models

∇wif (V, e) = X val,T

i

(X val

i wi − yval

i

)

∇V f (V, e) = X val,T (X valV − Y val)

The general expression of hyper-gradient or total derivative is given by

dλf (wλ, λ) = ∇λf (wλ, λ) + ∇λwλ∇wf (wλ, λ)

(19)

where ∇λ is the partial derivative, while dλ is the total derivative, and we
assume w, λ the parameters and hyper-parameters. We notice that ∇eH(e) =
−∇e
ij∈G(|eij| ln |eij| − |eij|) = − sign(e) ◦ ln e, where ◦ is the Hadamard
product. We can now write the hyper-gradient for our problem

(cid:80)

def (Ve, e) = ∇ef (Ve, e) + ∇eVe∇V f (Ve, e)
= ξe + η sign(e) − γ sign(e) ◦ ln e

(20)

−λ(V T ⊗ Im)(BT ⊗ Id)A−T X val,T (X valV − Y val)

(21)

11, . . . , bnnbT

nn] ∈ Rn×nm. The other variables previously deﬁned
where B = [b11bT
are bij = (ei − ej) ∈ Rn×1, A = λLe ⊗ Id + X T X ∈ Rdn×dn and V = A−1X T Y ,
Le = (cid:80)
n ]T ∈ Rdn×1, X = diag(X1, . . . , Xn) ∈ RN n×dn,
1 , . . . , wT
Y = [y1, . . . , yn] ∈ RN n×1 If e ≥ 0 then sign(e) = 1m.

l eijbl, V = [wT

Theorem 4. The matrix A of Thm. 1 deﬁnes a Sparse, Symmetric, Diagonally-
Dominant (SDD) linear system.

Proof (Theorem 4). The property comes from inspecting the component of
A = λLe ⊗ Id + X T X. The ﬁrst term is a block diagonal matrix, whose block
are the Laplacian matrix of the graph. This matrix is symmetric and sparse. The
second element is a block diagonal matrix whose blocks are X T
i Xi ∈ Rd×d, that
are symmetric matrices. Thus the system AX = Y is a SDD linear system.

Theorem 5. The directional derivative of F (A) = A−1 along B is −A−1BA−1.

Proof (Theorem 5). According to [35] page 19, Ch.3.4 , Eq. 167, we have that
(A + hB)−1 ≈ A−1 − hA−1BA−1 when h (cid:28) A, B. When now consider the
following directional derivative of the function F : Rn×n → Rn×n : A → A−1 or
F (A) = A−1, when then compute F (A + hB) − F (A) = (A + hB)−1 − A−1 ≈
A−1−hA−1BA−1−A−1 = −hA−1BA−1. It follows that the directional derivative
is −A−1BA−1.

Towards Interpretable Multi-Task Learning Using Bilevel Programming

19

Proof (Theorem 2). We notice that by setting def (Ve(t) , e(t)) = 0 we can compute
the optimal e in closed-form. If we assume e ≥ 0 and ξ = 0, we have

1m − λ(cid:48)(V T ⊗ Im)(BT ⊗ Id)A−T X val,T (X valV − Y val) = 0

where λ(cid:48) = λ/η. Suppose that M = (V T ⊗ Im)(BT ⊗ Id) ∈ Rm×dn and C =
X val,T (X valV − Y val) ∈ Rnd×1, we have

AM −11m = λ(cid:48)C

(λLe ⊗ Id + X T X)M −11m = λ(cid:48)C

(λLe ⊗ Id)M −11m = λ(cid:48)C − X T XM −11m

λ((E diag(e)ET ) ⊗ Id)u = λ(cid:48)C − X T Xu
((E diag(e)ET ) ⊗ Id)u = v
(E ⊗ Id)(diag(e) ⊗ Id)(ET ⊗ Id)u = v
(zT ⊗ (E ⊗ Id)) vec(diag(e) ⊗ Id) = v
(zT ⊗ (E ⊗ Id))Ke = v

(22)

(23)

(24)

(25)

(26)

(27)

(28)

where u = M −11m, v = 1/ηC − 1/λX T Xu, z = (ET ⊗ Id)u. The matrix
K = [vec(diag(d0)⊗Id), . . . , vec(diag(dm−1)⊗Id)] ∈ Rm2d2×m, where di ∈ Rm×1
is the indicator vector.

Proof (Theorem 3). In A = λLe ⊗ Id + X T X, the ﬁrst term is a block diagonal
matrix, each block is a Laplacian matrix that has 4m non zero elements, in
total O(dm). The second element is a block diagonal matrix whose blocks are
X T
i Xi ∈ Rd×d of d2 entries, in total nd2. In total, computing A has complexity
O(nd2 + dm) = O(nd2). Since A for Thm.2 is a SSD, solving in A [39,40]
has complexity is O((nd)1.31) or O((nd) lnc(nd)), with c constant, since A has
dimension nd × nd and the number of edges is m = kn. Since the matrix B ⊗ Id
has 4md non zero elements and V ⊗ Im has ndm non zero elements, the product
of the (V T ⊗ Im)(BT ⊗ Id)c requires O(dnm) = O(dn2) operation, for c ∈ Rdn×1.

A.2 The (cid:96)2

2-GGMTL and (cid:96)2-GGMTL algorithms

This section attempts to clarify the diﬀerence of the two variations of the proposed
method. The ﬁrst modiﬁcation of Alg.1 with (cid:96)2
2 norm (Sec.3.2) is presented in
Alg.2, while the modiﬁcation with (cid:96)2

2 norm (Sec.3.3) is described in Alg.3.

A.3 Visualization of GGMTL graphs on other datasets

Two additional datasets are presented and graph visualized to show the property
of the proposed method.

20

F. Alesiani, S. Yu, A. Shaker, W. Yin

Algorithm 2: (cid:96)2

2-GGMTL

Input
Output
for i ← 1 to n do

: {Xt, yt} for t = {1, 2, ..., n}, ξ, η, λ, ν
n ]T , Le
: V = [wT

1 , ..., wT

Solve wi by Linear Regression on {Xi, yi}
Construct k-nearest neighbor graph G on V ;
Construct E the incident matrix of G;
{X tr
, yval
t }, {X val
while not converge do

t } ← split({Xt, yt})

t , ytr

t

(cid:46) validation-training split;

compute def (Ve(t) , e(t)) using Eq. (10)
Update e: e(t+1) = [e(t) + νdef (Ve(t) , e(t))]+

(cid:46) compute hyper-gradient;
(cid:46) edges’ values update;

Solve Eq.2b on {Xt, yt}
return V, Le;

(cid:46) Train on the full dataset;

South America climate data South data contains the monthly mean tem-
perature of 250 spatial locations in South America for 100 years (over the time
period 1901-2000)8. These locations are distributed over a 2.5o x 2.5o grid on the
geographic coordinate system (latitudes, longitudes), see [16]. Following [14], ﬁve
partitions are created by considering a moving window over the ﬁrst 50 years for
training and a window over the following ten years for testing. By shifting these
two window ten years for each partitions, we obtain the ﬁve training/testing
pairs.

North America climate data Similarly, the North data contains the monthly
mean temperature of 490 spatial locations in North America.

8 MSSL code: bitbucket.org/andreric/mssl-code

Weather dataset: https://www-users.cs.umn.edu/~agoncalv/softwares.html

Towards Interpretable Multi-Task Learning Using Bilevel Programming

21

Algorithm 3: (cid:96)2-GGMTL

Input
Output
for i ← 1 to n do

: {Xt, yt} for t = {1, 2, ..., n}, ξ, η, λ, ν
n ]T , Le
: V = [wT

1 , ..., wT

Solve wi by Linear Regression on {Xi, yi}
Construct k-nearest neighbor graph G on V ;
Construct E the incident matrix of G;
// validation-training split
{X tr
t }, {X val
while not converge do

t } ← split({Xt, yt}) ;

t , ytr

, yval

t

// compute hyper-gradient
compute def (Ve(t) , e(t)) using Eq. (10), with e = e ◦ l(V ) of Eq.8 and
alternating with solution of Eq.9 (given by Eq.12).;

// edges’ values update
Update e: e(t+1) = [e(t) + νdef (Ve(t) , e(t))]+;

// Train on the full datasets with alternate optimization
Solve Eq.7 on {Xt, yt};
return V, Le;

(a) GGMTL

(b) kNN

Fig. 3: The discovered graphs by GGMTL and k-NN on the the North datasets.

22

F. Alesiani, S. Yu, A. Shaker, W. Yin

(a) GGMTL (south)

(b) kNN (south)

Fig. 4: The discovered graphs by GGMTL and k-NN on the the South datasets.

