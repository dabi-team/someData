1
2
0
2

v
o
N
6

]

G
L
.
s
c
[

3
v
4
8
7
5
0
.
6
0
1
2
:
v
i
X
r
a

Programming Puzzles

Tal Schuster
MIT

Ashwin Kalyan
Allen Inst. for AI

Oleksandr Polozov
Microsoft Research

Adam Tauman Kalai
Microsoft Research

Abstract

We introduce a new type of programming challenge called programming puzzles,
as an objective and comprehensive evaluation of program synthesis, and release an
open-source dataset of Python Programming Puzzles (P3).1 Each puzzle is deﬁned
by a short Python program f , and the goal is to ﬁnd an input which makes f
return True. The puzzles are objective in that each one is speciﬁed entirely by the
source code of its veriﬁer f , so evaluating f is all that is needed to test a candidate
solution. They do not require an answer key or input/output examples, nor do they
depend on natural language understanding. The dataset is comprehensive in that it
spans problems of a range of difﬁculties and domains, ranging from trivial string
manipulation problems, to classic programming puzzles (e.g., Tower of Hanoi), to
interview/competitive-programming problems (e.g., dynamic programming), to
longstanding open problems in algorithms and mathematics (e.g., factoring). We
develop baseline enumerative program synthesis, GPT-3 and Codex solvers that are
capable of solving puzzles—even without access to any reference solutions—by
learning from their own past solutions. Codex performs best, solving up to 18%
of 397 test problems with a single try and 80% of the problems with 1,000 tries
per problem. In a small user study, we ﬁnd a positive correlation between puzzle-
solving performance and coding experience, and between the puzzle difﬁculty
for humans and AI solvers. Therefore, further improvements on P3 could have a
signiﬁcant impact on many program synthesis areas.

1

Introduction

Puzzles are often used to teach and evaluate human programmers. Classic puzzles such as the Tower
of Hanoi teach fundamental concepts such as recursion. Programming competition problems, also
referred to as puzzles [34], evaluate a participant’s ability to apply these concepts. Puzzles are also
used to evaluate programmers in job interviews, and puzzles such as the RSA-factoring challenge test
the limits of state-of-the-art algorithms. Each of these types of puzzles is described in its own format,
often in a natural language such as English. Evaluations often include a hidden test set.

We introduce a novel puzzle representation called a programming puzzle or simply a puzzle, which
captures the essence of these challenges in a form convenient for machines and programmers. At a
minimum, a puzzle is speciﬁed by a function f (y), and the goal is to ﬁnd y such that f (y) = True.
More generally, a puzzle can include input variables x. Then, the puzzle can be seen as an output
veriﬁer f (y, x) that validates y. The answer y is typically the output of a synthesized program g. In
order to ﬁnd g(x) → y, a synthesizer is given the source code of f (and possibly x), with the goal of
generating a program g such that f (g(x), x) = True. Importantly, puzzles make for an objective and
explicit programming evaluation based solely on code with no formal requirement for input/output
examples, natural language descriptions, or reference solutions.

Puzzles may have multiple valid outputs y and some puzzles, even if very short, are extremely
challenging. Figure 1 illustrates three puzzles that are diverse in domain, difﬁculty, and algorithmic

1https://github.com/microsoft/PythonProgrammingPuzzles

35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks.

 
 
 
 
 
 
# Find a string that when reversed and concatenated with "world" gives "Hello world"
def f1(y: str):

return y[::-1] + "world" == "Hello world"

# Tower of Hanoi, often teaches recursion. Move [i, j] means move top disk on tower i to j, with 1 ≤ i,j ≤ 3
def f2(moves: List[List[int]], num_disks=8):

state = [1] * num_disks # All disks start at tower 1.
for [i, j] in moves:

assert state.index(i) <= (state + [1, 2, 3]).index(j), "bigger disk on top"
state[state.index(i)] = j # Move smallest disk from tower i to tower j.

return state == [3] * num_disks # All disks must end on tower 3.

# Find a non-trivial integer factor d of a large number n
def f3(d: int, n=100433627766186892221372630609062766858404681029709092356097):

return 1 < d < n and n % d == 0

Figure 1: Programming puzzles ranging from trivial to longstanding open algorithmic challenges
in multiple domains. f1 is solved by y="Hello "[::-1], a recursive program (see Figure H.1 on
page 37) outputting 255 moves solves f2, and f3 requires computational number theory algorithms.

tools. The ﬁrst puzzle is an easy (for humans) puzzle that tests one’s understanding of basic syntax
and properties of strings. The second is the quintessential example of recursion, and the last is a hard
problem requiring advanced algorithms such as the quadratic sieve.

We also release a growing open-source Python Programming Puzzles dataset, called P3, which is
already comprehensive in terms of difﬁculty, domain, and algorithmic tools. This dataset uniﬁes many
of the types of puzzles mentioned above. While P3’s puzzles are all speciﬁed in Python, solution
programs g can be written in any language, or simulated by neural networks.

As describe in §3, P3 also contains numerous classic puzzles; optimization puzzles such as solving a
linear programming; graph puzzles such as shortest path; and competitive-programming problems.
The most difﬁcult puzzles involve longstanding open problems such as learning parity with noise [11];
factoring [24, 30]; or ﬁnding a cycle in the 3n + 1 process which would disprove the Collatz
conjecture [35]. Thus, if AI were to surpass human-level performance on this dataset, it would lead to
breakthroughs on major open problems. The P3 puzzles were inspired by sources such as Wikipedia,
algorithms books, programming competitions, and other program synthesis datasets. This dataset is
growing as an open-source project, and anyone can add a puzzle by simply writing a function f.

One motivation for programming puzzles is that improvements in solving puzzles may lead to
performance gains at other tasks. Recently, neural Language Models (LMs) have advanced the
state-of-the-art of AI systems performance in offering code completions and synthesizing source code
in general-purpose programming languages, such as Python, based on English descriptions [5, 13].
While such systems represent a major advance over prior approaches, Chen et al. [13] point out
that they also reproduce elementary programming mistakes. Figure 2 illustrates how state-of-the-art
GitHub Copilot [63] solves a complex problem, handles the ambiguity of English, and yet makes
elementary errors. Performance gains in solving programming puzzles may result in fewer errors or
solving more sophisticated algorithms problems in downstream tasks such as code completion.

Puzzles are objective, meaning that it is easy to unambiguously evaluate whether one’s own answer
is valid without consulting an answer key. This evaluation also allows bootstrapping, even on test
puzzles without gold solutions. Given a set of puzzles (fi, xi), one can attempt to solve them with
solutions gi, determine with certainty which solutions are correct, and use those to improve one’s
ability to solve the remaining puzzles [19]. Inspired by success in playing games [53, 56], self-training
has also proven useful in program synthesis [see, e.g., 6, 15]. Other commonly used representations,
including natural language descriptions or Programming by Example (PbE), have inherent ambiguity.
See Appendix F for a comparison of a competition problem represented in English and as a puzzle.

From a theoretical point of view, as we shall discuss, objectivity can be formalized as the com-
plexity class NP of non-deterministic polynomial-time decision problems. Moreover, the puzzle
decision problem is NP-complete, meaning puzzles can readily express any NP problem, including
polynomial-time problems and other NP-complete problems such as Boolean satisﬁability.

We compare several enumerative random forest and Transformers-based top-down solvers, as well
as GPT-3 and Codex LM solvers with different prompt types (e.g., zero/few-shot and with/without
English descriptions). In our experiments, without access to any reference solutions, only utilizing

2

Figure 2: GitHub Copilot code completion examples (in gray). Left: Copilot correctly implements a
seven-line function. Top right: the completion adds a space character that may or may not have been
intended by the user. Middle and bottom right: errors indicating a lack of basic understanding.

self-training bootstrapping, our enumerative models solved up to 43% more P3 problems than a naive
brute force baseline. Our LM solvers were able to solve many of the puzzles, given enough tries.

To address the questions of whether puzzles measure programming proﬁciency and how puzzle
difﬁculty compares between humans and computers, we performed a small user study. Puzzles were
accessible and enjoyable for programmers with varying levels of experience. While both GPT-3 and
enumerative techniques can solve a fraction of the puzzles, human programmers outperform them.
For example, bootstrapping GPT-3 with up to 10K tries solved 60% of the puzzles, lower than both
beginner and experienced participants that solved 76% and 87% puzzles on average, respectively.
Overall, we ﬁnd perceived puzzle difﬁculty to scale similarly for both humans and AI.

The main contributions of this paper are introducing:

1. programming puzzles: a new type of problem suitable for algorithmic problem-solving (for

both machines and humans);

2. P3, an open-source dataset of puzzles covering diverse domains and difﬁculties; and
3. an evaluation of humans and baselines demonstrating that puzzles can be used to measure

algorithmic problem-solving progress.

Progress in code completion is rapid—even between the time of submission of this paper and its
publication, an API to Codex (a GPT-3 model ﬁne-tuned for code completion) was released [13]. Our
evaluation does in fact show signiﬁcant improvements of Codex over other baselines.

2 Problem formulation

Programs, inputs and outputs can all be formally represented as strings, where Σ∗ is the set of ﬁnite
strings over alphabet Σ. The set of veriﬁcation functions is denoted by F ⊆ Σ∗, with inputs and
outputs X , Y ⊆ Σ∗, respectively. A puzzle is deﬁned by pair (f, x) ∈ F × X and the result of
running veriﬁer f on output y ∈ Y is denoted f (y, x) ∈ {0, 1}. Output y ∈ Y is valid if it satisﬁes
f (y, x) = 1, i.e., f outputs 1 when run on (y, x), within a speciﬁed amount of time. To ensure that
puzzles can be quickly veriﬁed, it is necessary to upper-bound the time required for puzzle veriﬁcation.
This ensures that the puzzle decision problem, namely the problem of determining whether, given
f, x, there is y such that f (y, x) = 1, is in the complexity class NP. Formally, the puzzle decision
problem is, given strings f and x denoting the puzzle (represented as, say, a Turing machine) and
input, and a timeout t, does the puzzle output 1 in time ≤ t. See Appendix D for further details.

A solver takes n puzzles and timeouts (f1, x1, t1), . . . , (fn, xn, tn), and produces outputs yi to as
many puzzles as it can within a time bound T . Of course T (cid:29) (cid:80) ti is signiﬁcantly larger than the
veriﬁcation timeouts. Formally, the score of solver S : F n → X n is the number of puzzles fi for
which fi(yi, xi) outputs 1 in time ≤ ti. Although we do not study it in this paper, it would be natural
to assign different values to different puzzles. For example, solving open problems such as ﬁnding a
Collatz cycle or factoring the largest RSA challenge integer (currently unsolved, with a $200,000
prize offered), should be of greater value than solving a simple hello-world puzzle.

It is convenient, though not required, to solve puzzles by outputting a program g which, when run,
computes output y = g(x). Such a program is called a solution g. Short solutions may have long
outputs, e.g., the puzzle (f=lambda y: len(y) == x, x=1000000) requires a string of length
one million as solution g=lambda x: ’a’ * x. In this example, y = g(1000000) is a valid output
of length one million. Of course, another solution would be to explicitly write a string of length one
million in the code, though this implementation may not pass a human code review. In the dataset
and this paper, we provide solutions since they may be signiﬁcantly shorter. Many puzzles ﬁt a single

3

problem template, meaning they share the same veriﬁer f but have different inputs x. Thus a dataset
may have many more puzzles than problems.

3 The P3 dataset

P3 uses Python, the de facto language of ML research, as the programming language for specifying
puzzles. At the time of publication, P3 currently has 397 problems, summarized in Table 1. The latest
dataset can be generated by simply running make_dataset.py in the repository. More puzzles may
be created by increasing the number of puzzles per problem argument, though most experiments in
this paper use only one puzzle per problem. Every puzzle is described by a function with a required
typed argument (i.e., the candidate output) that returns True upon success. Since Python is not
type-safe, we add type-checking to ensure that outputs match the declared type. Figure C.1 on page
19 illustrates a puzzle where type checking is important.

We also provide code for serializing Python objects to and from strings in a json-like format, so
that programs implemented in any language can produce outputs. Moreover, strings are universal in
that they can encode arbitrary Python objects including functions, as in the Quine puzzle (lambda
quine: eval(quine) == quine)2 motivated by the classic challenge of writing a program that
outputs its own source code. As evaluation of the string quine can lead to an inﬁnite loop, this puzzle
illustrates the necessity of the evaluation timeout t for attempted solutions.

While not necessary for evaluation (since puzzles are self-contained) we follow the common practice
of programming competitions and provide a reference solution to most (over 90%) of the puzzles.
Some puzzles have more than one solution. A handful of puzzles represent major open problems in
computer science and mathematics including Factoring (and Discrete Log), Planted Clique, Learning
Parity with Noise, Graph Isomorphism, and ﬁnding a Collatz cycle,3 as described in Appendix E. We
also provide English descriptions for each puzzle in the dataset to support research involving natural
language. Appendix F compares programming competition problems to puzzles.

Creation process. The following sources were used for identifying possible puzzles:
• Wikipedia, speciﬁcally the Logic puzzles category, the List of unsolved problems in mathematics,

and the List of algorithms.

• Competitions, primarily the competitive programming website codeforces.com but also a hand-
ful of problems from the International Collegiate Programming Contest and the International
Mathematical Olympiad (IMO)–a high school mathematics competition.

• Puzzles inspired by the HumanEval dataset used for evaluating Codex [13], added in v0.2.
• The Python programming language itself, with trivial puzzles created to test understanding of basic

functions, such as the the hello-world puzzle which tests string concatenation.

P3 is organized topically into modules listed in Table 1. These topics include domains such as number
theory, graph theory, chess puzzles, game theory, etc., as well as puzzles inspired by a speciﬁc source
such as a speciﬁc programming competition. One ﬁnding in this paper is that many types of puzzles
can be captured in spirit, if not exactly, as succinct puzzles. Common patterns include:
• Problems that are naturally puzzles. For instance, search problems such as the TowerOfHanoi (f2,
Figure 1) and SlidingPuzzle simply test the sequence of moves to see if they lead to the goal state.
• Problems that have an equivalent natural puzzle. For instance, the standard deﬁnition of the
factoring problem, namely factorizing an integer into its prime factors would require a puzzle that
tests primality. However the simpler problem of ﬁnding any non-trivial integer factor, f3 in Figure
1, can be recursively called to solve the prime factorization problem.

• Optimization problems. Some such problems have equivalent natural puzzles, e.g., linear pro-
gramming is well-known [18] to be equivalent to solving a zero-sum game which is the ZeroSum
puzzle. For others, such as LongestMonotonicSubstring or ShortestPath, we specify a bound θ on
the objective, and the goal is to ﬁnd a feasible y with objective better than θ. In order to generate θ
(included in x), we ﬁrst solve the optimization problem ourselves, but the puzzle generation code is
not provided to the solvers.

2GPT-3 generated a 5-character solution to the quine puzzle while the authors’ solution was 88 characters.
3The solution to this problem would disprove the Collatz conjecture that is believed to be true, but no proof

has been found yet. Therefore, if the conjecture is true, the maximum attainable score in P3 is < 100%.

4

Table 1: Number of problems (and how many of them have at least one reference solution) per domain
in P3 v0.2. The right two columns show the average size of puzzles and solutions, measured by the
number of nodes in the Python AST.

Domain

Problems

Solutions

Algebra
Basic
Chess
Classic puzzles
Codeforces
Compression
Conways Game of Life
Games
Graphs
HumanEval
ICPC
IMO
Lattices
Number Theory
Probability
Study
Trivial inverse
Tutorial

Total # / Average size

4
23
5
23
47
2
3
7
12
164
4
6
2
16
5
30
39
5

397

4
23
3
23
45
2
2
7
11
164
4
6
2
12
5
30
38
5

386

|f |

70
54
221
101
73
126
189
225
105
81
304
173
70
47
85
40
27
27

79

|g|

172
44
186
211
70
113
345
299
152
62
569
256
228
68
72
21
30
13

84

• Problems that ask how many items in a certain set satisfy a given property, may be converted to
problems that require an explicit enumeration of all such items. See for example the AllPandigital-
Squares puzzle that requires all 174 pandigital perfect squares as input.

• Problems that involve game-playing can often be converted to puzzles. In chess, this includes the
classic Eight Queens and Knights Tour search problems. A puzzles Mastermind involves exhibiting
a winning strategy tree, and a nim puzzle involves beating a given computer opponent.

In order to ensure that each puzzle is achieving its goals, the puzzle design process has a step which
automatically tests for trivial solutions such as small integers or common strings.

Exclusions. Many programming challenges do not make as good puzzles. First, simple translation
tasks, where goal is translate a sequence of steps described in natural language into a program, do not
make good puzzles. Second, some challenges require problem-solving that is not easily expressed as
a program. For example, computing the probability of rolling a total of 350 when rolling 100 dice
relies on external knowledge about probability theory. Third, “soft” challenges involving natural
language or images are not in NP and not easily veriﬁable. This includes challenges involving human
commonsense or world knowledge about names, dates, or image classiﬁcation. Finally, interactive
challenges do not make for good programming puzzles. Fortunately, several other benchmarks cover
these latter two types of exclusions [see, e.g., 3, 32, 37, 42, 46, 48, 49, 51–53, 55, 58, 61, 62].

Growth process. The focus of this paper is in creating a framework with an initial dataset; and
demonstrating its utility for developing and evaluating AI solvers. As a GitHub repository, the dataset
can grow over time in a standard manner with the ability to reference previous versions. We plan
to continue adding puzzles and hope that others will as well. Popular competitive-programming
websites such as codeforces may be a source of thousands of puzzles of varying difﬁculties.

4 Solvers

In this section, we describe the models we develop as baselines for the dataset. We consider both
solving problems independently and joint solvers that bootstrap from previously obtained solutions to
ﬁnd new ones. We also consider both enumerative solvers that use standard techniques from program
synthesis and LM solvers that use GPT-3 and Codex to solve puzzles. While a direct comparison
between these two different approaches is difﬁcult because they run on different hardware (the LMs
call an API), we can still compare the relative difﬁculty with which they solve different puzzles, and
also to human difﬁculty rankings among puzzles.

5

4.1 Enumerative solvers

Following prior work [1, 6, 15, 40], we develop models to guide the search for g over the space of
all possible functions. In particular, we implement a grammar that generates Abstract Syntax Trees
(ASTs) for a large subset of Python. The grammar covers basic Python functionality and is described
in Appendix B.1. Speciﬁcally, each Python function is translated to an AST using a given set R of
rules. Based on the puzzle, a context-speciﬁc distribution over rule probabilities is computed. To
facilitate efﬁcient top-down search, the context of a rule is deﬁned to be the rule used by the parent
node and the index of the current node among the parent’s children. Thus if the parent node was a
division binary operator, then the two children would each have different contexts, but if two such
divisions were present in the same program, both numerators would share the same context.

Each puzzle f is represented by a feature vector φ(f ) and each context is represented by a vector
c(p, i) where p is the parent rule and i is the child index. Each rule r ∈ R is also associated with a
feature vector ρ(r). The probability distribution over R is determined based on ρ(r), φ(f ), c(p, i),
and the likelihood of a solution g is the product of all rules constituting its AST. Naturally, this
scoring mechanism introduces a bias towards shorter programs (i.e., smaller trees), which is desirable
as a short solution is easy to inspect.

COPY rules. Solutions often reuse constants or puzzle parameters, for example the constant 25 or
the variable s in example f2 in Figure 1. As in prior work [40], for each puzzle, the global rules
bank is expanded to include COPY rules for constants and parameters of the examined puzzle.4 When
composing solutions, this rule can reduce the complexity of the solution by simply learning to copy
part of the puzzle rather than having to generate it from scratch. For simplicity, we create copy rules
for each of the supported types and assign the probabilities uniformly across all the puzzle’s constants
of that type. In other words, our models learn when a certain type should be copied from the puzzle,
and rank all available constants and parameters of that type the same.

To solve a new puzzle, we perform a top-down search. Speciﬁcally, at each node, we apply a selected
model over all rules in R whose type matches the context, and re-normalize the scores to create a
valid probability distribution. The solver enumerates solutions in order of decreasing likelihood until
it ﬁnds a solution g such that f(g()) evaluates to True in time ≤ t, for a maximum number of tries
M . See Appendix B for details on the search and rules. Next, we brieﬂy describe our models.

Uniform. The ﬁrst model is a simple uniform rule that assigns the same probability to all rules.
The only exception is COPY rules, which have a larger, ﬁxed probability in order to bias the solver
towards utilizing this option. As we score programs by their joint probability, this bias effectively
favors shorter programs. We use this model to ﬁnd solutions to the easier problems, satisﬁed by a
simple and short answer, and use these to bootstrap the learning of the parametric models. This model
also provides a naive brute force baseline to compare the parametric models with.

The remaining two models have parameters that are ﬁt based on bootstrapping. Namely, given
previously obtained solutions, we collect all parent-child rule pairs as self-supervision and ﬁt the
model’s parameters on them. The training size is then the total number of nodes in all the trees among
solutions discovered up until that point. We implement two bigram parametric models to predict
P(cid:0)r | ρ(r), φ(f ), c(p, i)(cid:1), where r is a candidate rule to appear in g’s tree under p as its i’s argument.

In this model, we represent f as a bag-of-rules ∪{rk ∈ f }. Speciﬁcally, φ(f ) is a
Random forest.
vector of length |R| representing the number of occurrences of each rule in f . p and i are encoded as
a one-hot vector and concatenated to f ’s representation to construct the input to the model. Given
past solution trees, we train the model to predict the index of r out of |R| given f, p, i examples.

Transformer. Following the recent success of transformer models [20, 57] in encoding source
code [21, 31, 54, inter alia], we turn to these encoders for richer representations. We use a RoBERTa-
based [36] Transformer to encode puzzles and rules directly from their code. The probability of a
rule r being the i’s child of p in g is proportional to the dot product of the deep joint representation
of f, p, i and the Transformer encoding ρ(r). We pretrain the Transformer with a masked language
model task on Python GitHub repositories [27].5 Then, our solver concatenates the Transformer

4When executing a solution, COPY rules are simply the identity function (COPY = lambda x: x in Python).
5Our pretrained model and tokenizer are available at https://huggingface.co/tals/roberta_python.

6

def f(li: List[int]):

return len(li) == 10 and li.count(li[3]) == 2

assert True == f(. . .

Figure 3: A Short prompt for a puzzle requesting a list of ten integers where the fourth item occurs
exactly twice, with valid completion . . .[1,2,3,4,5]*2). Appendix C has Medium/Long prompts.

encodings φ(f ) and ρ(p) with a learned embedding for i, following by non-linear layers to compute
the joint representation. We ﬁne-tune the solver on parent-child rule pairs from previously acquired
solutions. See Appendix B.2 for extended details, and Figure B.1 on page 19 for a model diagram.

4.2 Autoregressive Language Model solvers

We experiment with the transfomer-based GPT-3 [12] and Codex [13] LMs with billions of parameters.
Codex was trained over large amounts of publicly available code from GitHub, speciﬁcally aimed
for coding applications. We follow the recent strategy of designing a prompt that directs the text
generation to our desired task. This approach has shown to be useful in converting natural language
descriptions to programming code and guide theorem proving [45]. Unlike our enumerative models
that build an AST, LMs generate the solution as a string that is directly evaluated as Python code.

We consider four different prompts: (a) A short zero-shot prompt based solely on the puzzle at hand
(illustrated in Figure 3); (b) a medium 5-shot prompt that includes the ﬁve example puzzles that
had been shown to (human) programmers during our user study (Appendix Figures C.2-C.3); (c) a
long prompt with the same ﬁve puzzles augmented by English descriptions of the tasks in comments
(Figures C.4-C.5); and (d) a bootstrapping prompt which uses only solutions to problems that it has
already solved (Figures C.7). The bootstrapping prompt begins with no solutions but quickly exceeds
the API maximum length as more puzzles are solved. At that point, previously solved puzzles are
randomly sampled to form the prompt. The prompts used for Codex are slightly more sophisticated
but enable multi-line programs.

The completions which parse as valid Python expressions are then evaluated. Appendix C gives
further details of the execution environment, the API parameters and other prompts we investigated.

5 Experiments

We use our P3 dataset to evaluate the performance of the solvers from §4. We assume no access
to reference solutions6 and measure how many puzzles are solved by each solver with up to k tries
per puzzle, where each try is a potential solution that is evaluated. For the enumerative solvers, this
is equivalent to having a valid solution ranked in the top k. For LM solvers, we use pass@k [13]
which is an unbiased estimator of the probability of obtaining a solution within k tries. First, we
test the solvers bootstrapping efﬁcacy in leveraging previously solved problems to solve new ones.
Then, once solutions to a single instance of certain problems are found, we test whether solvers also
succeed on other problem instances (i.e., puzzles originated from the same problem). In §5.1, we
present our user study results that compares human’s performance with AI solvers. Finally, in §5.2,
we test whether P3 can distinguish between subtly different variants of Codex, using the larger v0.2
release of the dataset (the current version at the time of publication).

Learning from past solutions. This ﬁrst experiment was run on the v0.1 release of P3.7 We use a
single puzzle instance per problem. We ﬁrst identiﬁed the 138 of the 200 v0.1 problems supported by
our grammar (see Appendix B.1). For the enumerative solvers, we then ran the uniform solver with
k = 104 on these 138 problems supported by our, solving 38 of them. The solutions contain a total
of 2,475 rules that we use to train the parametric models. In the bootstrapping variant, we repeat the
training for 6 cycles, each time adding the new solutions found with k = 104. In the ﬁnal round, we
allow up to k = 106 solving tries (including previous cycles). For comparison to GPT-3/Codex,8 we

6With the exception of the Medium and Long prompts that including ﬁve Tutorial problems and solutions.
7https://github.com/microsoft/PythonProgrammingPuzzles/tree/v0.1
8The Codex API was released after this experiment had been run on v0.1 using the enumerative and GPT-3
solvers. Thus, we simply replaced the GPT-3 solver with the Codex solver and re-ran on the same 138 puzzles.

7

(a) Enumerative solvers

(b) GPT-3 solvers

(c) Codex solvers (extended y scale)

Figure 4: Increasing the number of tries allows solving new problems. Better solvers, though, solve
new problems signiﬁcantly faster by learning from past experience. Parametric enumerative solvers
(a) initialized with the solutions of the uniform solver at k = 104 accelerate the solution search.
Additional self-training bootstrapping cycles (marked with B.) solve even more problems. GPT-3
(b) and Codex Davinci (c) solvers were evaluated with up to 104 attempts. Having natural language
descriptions (Long) provides small improvements over Medium. Adding previously found solutions
to the prompt (Bootstrap) allows signiﬁcant improvements for enumerative and GPT-3, and matches
Long for Codex. Overall, the Codex models performed best, solving up to 127 of the examined 138
puzzles. (a), (b) are averaged across three runs and the shaded areas show the standard deviation.

use the same 138 problems and start with a zero-shot prompt. As valid solutions are found, they are
appended to the prompt as discussed in §C.

Figure 4a shows the total number of puzzles solved by each enumerative solver, with and without
the self-training bootstrapping cycles. We report the average results across three runs and present
the standard deviation in the graph. We see that the parametric models quickly improve over the
naive uniform search and that the bootstrapping process facilitates solving many new problems. At
k = 106, the random forest and Transformer-based enumerative models solved a total of 68 and 76
problems, respectively, which is 28% and 43% more than the uniform solver.

The GPT-3 solver also improves by learning from previously found solutions. As Figure 4b shows,
few-shot settings with tutorial examples perform better than zero-shot (Short) and solve new problems.
Including natural language descriptions (Long) helps for solving ﬁve more puzzles, with up to 104
tries. The best strategy, however, is the bootstrapping one that starts without any reference and adds
solutions to the prompt as they are found. Codex, trained on large amounts of code, performs the best
(see Figure 4c) but does not beneﬁt signiﬁcantly from bootstrapping.

Generalizing to other problem instances.
In the previous experiment, we attempted to solve the
default single puzzle instance of each problem. Next, we examine whether our solvers can also solve
other puzzle instances, originating from the same problems. We collect a set of 700 puzzles that
are random instances of 35 problems for which both our bootstrapping enumerative models solved
the default puzzle. At k = 104, the random forest and Transformer models solved 75% and 79%,
respectively. As a reference, the uniform model solves only 62% of these puzzles.

5.1 User study

In a small user study, 21 participants with varying experience in Python programming attempted
to solve 30 puzzles, as found in v0.1 dataset as the study module. Each puzzle was allocated a
maximum of 6 minutes to solve, and the study was conducted virtually using Jupyter notebooks.
Participants were employees at a major software company and were recruited by email and at a
hackathon. No compensation was offered. Participants were ﬁrst given a short tutorial about puzzles
and how to submit solutions. The user study ﬁles are available in the open-source dataset, and
Appendix G has further details including the 30 puzzles.

The ﬁrst ﬁnding is that success in puzzles correlates with programming experience. For our retrospec-
tive study analysis, we split the participants by the median years of Python programming experience.
We had 10 beginners with less than three years of experience, and 11 experienced participants with
at least three years. We ﬁnd that 9 of the 30 puzzles were solved by all beginners, while 17 of the
puzzles were solved by all experienced participants. Also, beginners spent on average 194 seconds
per puzzle, while experienced spent only 149 seconds on average. The average solving time provides

8

Figure 6: The difﬁculty score per study puzzle for both humans and AI solvers, sorted by the human’s
scores. The difﬁculty score for humans is measured by the average fraction of solving time out of the
maximum allowed. For AI, we use the fraction of allotted attempts required. Most of the puzzles
solved by AI (low difﬁculty score) are also easier for humans (left hand side of the plot).

a useful proxy to the perceived difﬁculty of each puzzle. Overall, we see that puzzles are easier for
experienced programmers, indicating their value for evaluating programming proﬁciency.

Next, we compare human’s performance to Codex-davinci. We
use the Medium prompt as it is similar to the study format (i.e.,
same 5 tutorial examples, no docstrings). Participants solved
an average of 24.6 out of the 30 puzzles (22.9 for beginners
and 26.2 for experienced) within the 6 minutes per puzzle time
limit. Only one out of the 21 participants solved all puzzles. As
Figure 5 shows, Codex required 1K tries per puzzle to match
the performance of beginner programmers in our study.

Figure 5: Number of solved puzzles
by Codex-davinci (blue bars), com-
pared to human coders with 6 min-
utes per puzzle (horizontal lines).

Finally, we ﬁnd that difﬁcult puzzles for humans are also harder
for AI. Figure 6 shows that most of the puzzles solved by AI
solvers with limited number of tries are the ones that are easier
for humans (i.e., solved faster). To compare the two, we deﬁne
a puzzle’s perceived difﬁculty score as the average solving
time for humans and the expected number of required tries for
machines (normalized to [0, 1], where the score of unsolved puzzles is set to 1). The Spearman’s
rank coefﬁcient of humans with B. GPT-3 is 0.512, and with Codex (Med.) is 0.563. The AI solvers
correlation is stronger with beginner programmers (0.541 and 0.562), than with the experienced
ones (0.470 and 0.544, respectively). On the one hand, this suggests that additional computational
power might allow AI solvers to match humans. However, as Figure 4 shows, this improvement
is logarithmic, leading to diminishing returns. Encouragingly, we see that even within the same
budget, modeling choices can improve performance. We hope that P3 will support the research and
development of new AI solvers that will solve more puzzles with less computational effort.

5.2 Comparing small and large Codex models

engine (prompt)

Table 2: Codex pass@k results over P3 v0.2.

In addition to the standard davinci-codex engine,
the API offers an alternate cushman-codex engine
that they report is signiﬁcantly faster and only
slightly less accurate. To test the ability of P3 as
an evaluation of such ﬁne distinctions, we ran the
Medium and Long prompts on both engines across
the most recent v0.2 release9 of 397 puzzles. As
can be seen in the results of Table 2, the larger
engine indeed slightly outperformed the smaller
engine across all k. Thus, in this experiment, puzzle solving success aligns with code completion
success. Also, we observe that English descriptions (Long prompt) are helpful for both engines.
Inasmuch as puzzles are useful for code completion, the < 20% success rates at k = 1 leaves
substantial room for improvement.

26.7% 51.7% 68.3%
36.7% 60.6% 75.3%

42.4% 63.9% 76.5%
48.7% 69.1% 79.8%

cushman (Long)
davinci (Long)

cushman (Med.)
davinci (Med.)

7.1%
11.2%

14.9%
18.3%

k = 1

1,000

100

10

9https://github.com/microsoft/PythonProgrammingPuzzles/tree/v0.2

9

Table 3: Codex-davinci and Codex-cushman number of solved problems per domain with up to
1,000 tries for Medium and Long prompts. The ﬁrst row also shows the number of available P3 v0.2
problems in that domain. The score is the average percent solved across domains.

Model

Algebra

Basic

Chess

Classic

CodeForces Compression Conway’s Games Graphs

cushman (Med.)
davinci (Med.)
cushman (Long)
davinci (Long)

3/4
2
2
2

15/23
20
21
22

0/5
0
0
1

6/23
8
5
4

32/47
35
38
39

0/2
0
0
0

0/3
0
0
0

1/7
1
1
1

6/12
9
8
8

Model

HumanEval

ICPC

IMO Lattices N. Theory

Probability

Study

Trivial−1

Tutorial

Score

cushman (Med.)
davinci (Med.)
cushman (Long)
davinci (Long)

139/164
145
149
155

2/4
2
1
1

1/6
1
1
1

0/2
1
1
2

8/16
9
9
10

2/5
3
3
3

21/30
22
24
25

33/39
36
36
38

5/5
5
5
5

44.2
51.2
49.8
54.8

Table 3 shows the number of achieved solutions per domain, as well as an overall score computed as
the macro-average of solving rates across domains.

6 Related Work

Program synthesis has taken drastically different forms for different applications, often resulting in
one-off evaluations rather than common datasets. A major paradigm is Programming by Example
(PbE) where problems are speciﬁed by input-output examples. For instance, several studies focus on
text processing [22] or robot navigation [43]. While convenient for end user applications (e.g., many
in [44]), PbE alone is inadequate to objectively describe many sophisticated algorithmic programming
challenges. A recent ARC dataset [14] adopts PbE for evaluating abstraction and reasoning in AI, but
as in all PbE applications, there can be ambiguity.

Program synthesis from formal speciﬁcations has a long history of study [surveyed in 23], bench-
marked by e.g., the SyGuS competition [2]. In this setting, however, the AI system has to synthesize
an algorithm that correctly and efﬁciently solves a problem on all inputs (and often prove correctness
as well). Writing and testing such formal speciﬁcations is often non-trivial.

English descriptions, often mixed with examples, are becoming an increasingly popular problem
representation as LMs improve [28, 33, 60]. In independent work, Hendrycks et al. [26] created a
large dataset of English programming problems with examples on which they ﬁne-tuned GPT models.
In another concurrent work, the Codex model that powers the new GitHub Copilot auto-completion
tool [13] was evaluated with short problem descriptions paired with a set of unit tests that should
validate the described speciﬁcation. Our work, together with this very recent and concurrent work
[5, 13, 26], represent the ﬁrst controlled evaluation of large Transformer-based LMs on general-
purpose program synthesis.

The recent CodeXGLUE benchmark [38] collected several code-related datasets. To evaluate genera-
tion, they use CodeBLEU [47] which relies on ASTs and other code-speciﬁc aspects. This evaluation
still requires reference solutions and, therefore, does not resolve the answer-key bias with ambiguous
speciﬁcations. Several neighboring ﬁelds that have made substantial progress in reasoning include
theorem proving [8], two-player game playing [53], and SAT-solving [9]. In all these ﬁelds, important
progress has been made by encoding the problems, be they theorems, game rules, or optimization
problems, in machine-readable formats that do not involve the ambiguities of natural language.

7 Conclusions

We introduce Python Programming Puzzles (P3), an open-source dataset with puzzles described only
in source code. As discussed in §3, the puzzle framework captures NP problems, which include a
wide range of interesting challenges. Puzzles allow fast and objective evaluation, thereby supporting
unsupervised solving without training solutions. We implemented and evaluated several enumerative
program-synthesis and LM baselines, and found a positive correlation between their per-puzzle
performance and the difﬁculty for human programmers. Similarly, LMs that performed better at code
completion also solved more puzzles with less tries.

We welcome contributions to P3 and hope it will grow in size, coverage, and utility.

10

Acknowledgments. We would like to thank Mariia Mykhailova for suggesting doing a Python
Programming Puzzles Hackathon. We are especially grateful to the participants in our user study and
hackathon. We are grateful to the creators of Codex and GPT-3 and to Nicoló Fusi for suggesting its
use in this project. We would like to thank David Alvarez Melis and Alec Helbing for suggesting quine
puzzles. We are grateful to Ana-Roxana Pop for helpful discussions and feedback. We also thank
Tianxiao Shen, Adam Fisch and the rest of the MIT NLP members for valuable writing feedback.

References

[1] Uri Alon, Roy Sadaka, Omer Levy, and Eran Yahav. Structural language models of code. In

International Conference on Machine Learning, pages 245–256. PMLR, 2020.

[2] Rajeev Alur, Dana Fisman, Saswat Padhi, Rishabh Singh, and Abhishek Udupa. SyGuS-Comp

2018: Results and analysis. 2019.

[3] Jacob Andreas, John Bufe, David Burkett, Charles Chen, Josh Clausman, Jean Crawford, Kate
Crim, Jordan DeLoach, Leah Dorner, Jason Eisner, Hao Fang, Alan Guo, David Hall, Kristin
Hayes, Kellie Hill, Diana Ho, Wendy Iwaszuk, Smriti Jha, Dan Klein, Jayant Krishnamurthy,
Theo Lanman, Percy Liang, Christopher H. Lin, Ilya Lintsbakh, Andy McGovern, Aleksandr
Nisnevich, Adam Pauls, Dmitrij Petters, Brent Read, Dan Roth, Subhro Roy, Jesse Rusak, Beth
Short, Div Slomin, Ben Snyder, Stephon Striplin, Yu Su, Zachary Tellman, Sam Thomson,
Andrei Vorobev, Izabela Witoszko, Jason Wolfe, Abby Wray, Yuchen Zhang, and Alexander
Zotov. Task-oriented dialogue as dataﬂow synthesis. Transactions of the Association for
Computational Linguistics, 8:556–571, 2020. doi: 10.1162/tacl_a_00333. URL https://
aclanthology.org/2020.tacl-1.36.

[4] Sanjeev Arora and B. Barak. Computational complexity: A modern approach. 2009.

[5] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David
Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program synthesis
with large language models, 2021.

[6] Matej Balog, Alexander L. Gaunt, Marc Brockschmidt, Sebastian Nowozin, and Daniel Tarlow.
In International Conference on Representation

Deepcoder: Learning to write programs.
Learning (ICLR), 2017.

[7] E. Berlekamp, R. McEliece, and H. van Tilborg. On the inherent intractability of certain coding
problems (corresp.). IEEE Transactions on Information Theory, 24(3):384–386, 1978. doi:
10.1109/TIT.1978.1055873.

[8] Yves Bertot and Pierre Castéran. Interactive Theorem Proving and Program Development:
Coq’Art: The Calculus of Inductive Constructions. Springer Science & Business Media, 2004.

[9] Armin Biere, Marijn Heule, and Hans van Maaren. Handbook of satisﬁability, volume 185.

IOS press, 2009.

[10] N. Biggs, P.M.L.S.E.N.L. Biggs, London Mathematical Society, London Mathematical Society
lecture note series, and S.P.G.N.J. Hitchin. Finite Groups of Automorphisms: Course Given
at the University of Southampton, October-December 1969. Lecture note series. Cambridge
University Press, 1971. ISBN 9780521082150. URL https://books.google.com/books?
id=flA4AAAAIAAJ.

[11] Avrim Blum, Adam Kalai, and Hal Wasserman. Noise-tolerant learning, the parity problem,
and the statistical query model. J. ACM, 50(4):506–519, July 2003. ISSN 0004-5411. doi:
10.1145/792538.792543. URL https://doi.org/10.1145/792538.792543.

[12] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-
wal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,
Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCan-
dlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot

11

In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors,
learners.
Advances in Neural Information Processing Systems, volume 33, pages 1877–1901. Cur-
ran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.

[13] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harri
Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger,
Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick
Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter,
Philippe Tillet, Felipe Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth
Barnes, Ariel Herbert-Voss, Will Guss, Alex Nichol, Igor Babuschkin, Suchir Balaji, Shantanu
Jain, Andrew Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford,
Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew,
Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large
language models trained on code, 2021.

[14] François Chollet. On the measure of intelligence. arXiv preprint arXiv:1911.01547, 2019.

[15] Konstantina Christakopoulou and Adam Tauman Kalai. Glass-box program synthesis: A
machine learning approach. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018.

[16] John Horton Conway. Five $1,000 problems (update 2017). 2017. URL https://oeis.org/

A248380/a248380.pdf. [Online; accessed 12/15/2020].

[17] Valentina Dagien˙e and Gerald Futschek. Bebras international contest on informatics and
In International conference on informatics in

computer literacy: Criteria for good tasks.
secondary schools-evolution and perspectives, pages 19–30. Springer, 2008.

[18] George B Dantzig. A proof of the equivalence of the programming problem and the game

problem. Activity analysis of production and allocation, 13:330–338, 1951.

[19] Eyal Dechter, Jonathan Malmaud, Ryan P Adams, and Joshua B Tenenbaum. Bootstrap learning

via modular concept discovery. In IJCAI, pages 1302–1309, 2013.

[20] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of
deep bidirectional transformers for language understanding. In Proceedings of the 2019 Confer-
ence of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis,
Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.
URL https://www.aclweb.org/anthology/N19-1423.

[21] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Lin-
jun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. CodeBERT: A pre-trained
In Findings of the Association for Com-
model for programming and natural languages.
putational Linguistics: EMNLP 2020, pages 1536–1547, Online, November 2020. Asso-
ciation for Computational Linguistics. doi: 10.18653/v1/2020.ﬁndings-emnlp.139. URL
https://www.aclweb.org/anthology/2020.findings-emnlp.139.

[22] Sumit Gulwani. Automating string processing in spreadsheets using input-output examples.
In Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of
Programming Languages, POPL ’11, page 317–330, New York, NY, USA, 2011. Association
for Computing Machinery. ISBN 9781450304900. doi: 10.1145/1926385.1926423. URL
https://doi.org/10.1145/1926385.1926423.

[23] Sumit Gulwani, Oleksandr Polozov, Rishabh Singh, et al. Program synthesis. Foundations and

Trends® in Programming Languages, 4(1-2):1–119, 2017.

[24] G. H. Hardy, E. M. Wright, D. R. Heath-Brown, and Joseph H. Silverman. An introduction to the
theory of numbers. Oxford University Press, Oxford; New York, 2008. ISBN 9780199219858
0199219850 9780199219865 0199219869.

[25] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). 2016.

12

[26] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo,
Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. Measuring coding
challenge competence with APPS. 2021.

[27] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt.
CodeSearchNet challenge: Evaluating the state of semantic code search. arXiv preprint
arXiv:1909.09436, 2019.

[28] Alexander Skidanov Illia Polosukhin. Neural program search: Solving data processing tasks
from description and examples. In ICLR Workshop Acceptance Decision, 2018. URL https:
//openreview.net/forum?id=B1KJJf-R-.

[29] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training

by reducing internal covariate shift. 2015.

[30] Burt Kaliski. RSA factoring challenge, pages 531–532. Springer US, Boston, MA, 2005. ISBN
978-0-387-23483-0. doi: 10.1007/0-387-23483-7_362. URL https://doi.org/10.1007/
0-387-23483-7_362.

[31] Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. Learning and evaluating
contextual embedding of source code. In Proceedings of the 37th International Conference on
Machine Learning, ICML 2020, 12-18 July 2020, Proceedings of Machine Learning Research.
PMLR, 2020.

[32] Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu,
Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, Zhiyi Ma, Tristan Thrush,
Sebastian Riedel, Zeerak Waseem, Pontus Stenetorp, Robin Jia, Mohit Bansal, Christopher
Potts, and Adina Williams. Dynabench: Rethinking benchmarking in NLP. In Proceedings
of the 2021 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages 4110–4124, Online, June 2021. Association
for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.324. URL https://
aclanthology.org/2021.naacl-main.324.

[33] Sumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex Aiken, and
Percy S Liang. SPoC: Search-based pseudocode to code.
In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information
Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.
neurips.cc/paper/2019/file/7298332f04ac004a0ca44cc69ecf6f6b-Paper.pdf.

[34] A. Laaksonen. Guide to Competitive Programming: Learning and Improving Algorithms
Through Contests. Undergraduate Topics in Computer Science. Springer International Pub-
lishing, 2020.
ISBN 9783030393571. URL https://books.google.com/books?id=
3JbiDwAAQBAJ.

[35] Jeffrey C. Lagarias. The 3x + 1 problem and its generalizations. The American Mathematical
Monthly, 92(1):3–23, 1985. ISSN 00029890, 19300972. URL http://www.jstor.org/
stable/2322189.

[36] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. 2019.

[37] Nicholas Lourie, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Unicorn on rainbow:
A universal commonsense reasoning model on a new multitask benchmark. AAAI, 2021.

[38] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin
Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long
Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng,
Shengyu Fu, and Shujie Liu. Codexglue: A machine learning benchmark dataset for code
understanding and generation, 2021.

[39] C. Mayer. Python One-Liners: Write Concise, Eloquent Python Like a Professional. No
Starch Press, Incorporated, 2020. ISBN 9781718500501. URL https://books.google.
com/books?id=jVv6DwAAQBAJ.

13

[40] Aditya Krishna Menon, Omer Tamuz, Sumit Gulwani, Butler W Lampson, and Adam Kalai. A

machine learning framework for programming by example. In ICML, pages 187–195, 2013.

[41] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,
P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher,
M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine
Learning Research, 12:2825–2830, 2011.

[42] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu,
and Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019
Conference on Empirical Methods in Natural Language Processing and the 9th International
Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463–2473, Hong
Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/
D19-1250. URL https://aclanthology.org/D19-1250.

[43] Chris Piech and Eric Roberts. Karel the robot learns python. https://compedu.stanford.
[Online; accessed 07-Jun-

edu/karel-reader/docs/python/en/intro.html, 2020.
2021].

[44] Oleksandr Polozov and Sumit Gulwani. FlashMeta: A framework for inductive program
synthesis. In Proceedings of the 2015 ACM SIGPLAN International Conference on Object-
Oriented Programming, Systems, Languages, and Applications, pages 107–126, 2015.

[45] Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving.

CoRR, abs/2009.03393, 2020. URL https://arxiv.org/abs/2009.03393.

[46] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+
In Proceedings of the 2016 Conference on
questions for machine comprehension of text.
Empirical Methods in Natural Language Processing, pages 2383–2392, Austin, Texas, Novem-
ber 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. URL
https://aclanthology.org/D16-1264.

[47] Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming
Zhou, Ambrosio Blanco, and Shuai Ma. Codebleu: a method for automatic evaluation of code
synthesis, 2020.

[48] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision
(IJCV), 115(3):211–252, 2015. doi: 10.1007/s11263-015-0816-y.

[49] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social IQa:
Commonsense reasoning about social interactions. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing and the 9th International Joint Conference
on Natural Language Processing (EMNLP-IJCNLP), pages 4463–4473, Hong Kong, China,
November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1454. URL
https://aclanthology.org/D19-1454.

[50] Roei Schuster, Congzheng Song, Eran Tromer, and Vitaly Shmatikov. You autocomplete me:
Poisoning vulnerabilities in neural code completion. In 30th USENIX Security Symposium
(USENIX Security 21). USENIX Association, August 2021. URL https://www.usenix.org/
conference/usenixsecurity21/presentation/schuster.

[51] Tal Schuster, Adam Fisch, and Regina Barzilay. Get your vitamin C! robust fact veriﬁcation
with contrastive evidence. In Proceedings of the 2021 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies,
pages 624–643, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/
v1/2021.naacl-main.52. URL https://aclanthology.org/2021.naacl-main.52.

[52] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur
Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy
Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis
Hassabis. Mastering the game of go without human knowledge. Nature, 550(7676):354–359,
2017. doi: 10.1038/nature24270. URL https://doi.org/10.1038/nature24270.

14

[53] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur
Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general
reinforcement learning algorithm that masters chess, shogi, and go through self-play. Science,
362(6419):1140–1144, 2018.

[54] Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundaresan. IntelliCode Compose:
code generation using Transformers. Proceedings of the 28th ACM Joint Meeting on European
Software Engineering Conference and Symposium on the Foundations of Software Engineering,
Nov 2020. doi: 10.1145/3368089.3417058. URL http://dx.doi.org/10.1145/3368089.
3417058.

[55] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA:
A question answering challenge targeting commonsense knowledge. In Proceedings of the
2019 Conference of the North American Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4149–
4158, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi:
10.18653/v1/N19-1421. URL https://aclanthology.org/N19-1421.

[56] Gerald Tesauro. Temporal difference learning and TD-Gammon. Communications of the ACM,

38(3):58–68, 1995.

[57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need.
In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett,
editors, Advances in Neural Information Processing Systems, volume 30. Curran As-
sociates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.

[58] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman.
GLUE: A multi-task benchmark and analysis platform for natural language understanding.
In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting
Neural Networks for NLP, pages 353–355, Brussels, Belgium, November 2018. Association
for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://aclanthology.
org/W18-5446.

[59] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony
Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,
Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain
Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-
art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods
in Natural Language Processing: System Demonstrations, pages 38–45, Online, October 2020.
Association for Computational Linguistics. URL https://www.aclweb.org/anthology/
2020.emnlp-demos.6.

[60] Maksym Zavershynskyi, Alexander Skidanov, and Illia Polosukhin. NAPS: natural program
synthesis dataset. CoRR, abs/1807.03168, 2018. URL http://arxiv.org/abs/1807.03168.

[61] Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. SWAG: A large-scale adversarial
In Proceedings of the 2018 Conference on
dataset for grounded commonsense inference.
Empirical Methods in Natural Language Processing, pages 93–104, Brussels, Belgium, October-
November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1009. URL
https://aclanthology.org/D18-1009.

[62] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition:
Visual commonsense reasoning. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.

[63] Albert Ziegler. Research recitation: A ﬁrst look at rote learning in github copilot suggestions.,
June 2021. https://docs.github.com/en/github/copilot/research-recitation,
Last accessed on 2021-11-01.

15

We provide complementary details, analysis and results in the following sections:

A Example solutions by enumerative models
B Enumerative solvers details
C Language Model solvers details
D NP-completeness
E Open problems
F Comparing puzzles to competitive-programming problems
G User Study Details
H Solution to Tower of Hanoi

A Example solutions by enumerative models

Page 16
Page 16
Page 19
Page 22
Page 23
Page 23
Page 24
Page 25

We provide examples of our examined enumerative solvers to three P3 puzzle in Figure A.1 on page
17 (examples of LM solutions are found in the P3 repository). The solution to the ﬁrst puzzle is
general and will work for any other instance of this problem. For the two other puzzles, the obtained
solutions are instance-speciﬁc and don’t even use the input variables. Yet, it is possible that the
logical steps to achieve the answer are implicitly executed by the model. To test this, we evaluate the
solvers on other problem instances (i.e., puzzles originated from the same problem).

The solvers’ solutions to the ﬁrst puzzle in Figure A.1 are simpler than the one created by humans
(though less efﬁcient in terms of input length). This illustrates another potential use case of AI solvers:
debugging puzzles by ﬁnding easy solutions.

B Enumerative solvers details

We train our random forest solver with the Python Skickit-learn library [41]. The Transformer
model is implemented on top of the Hugging Face repository [59]. We use GPUs for training the
Transformer and for querying it for rule probabilities. All other computations are performed with
CPUs. Making up to 104 solution tries takes only a few seconds to a few tens of seconds, depending
on the puzzle and the attempted solutions. Running up to 106 solution tries usually takes less than an
hour but for some puzzles can take longer. We run the solver in parallel on multiple puzzles to reduce
the global computation time.

Solution validation. Given the AST, a solution is generated in the form of a Python program
(possibly multiple lines) that is evaluated by the Python interpreter to get an answer that is tested
by the puzzle. To address long-running programs and inﬁnite loops, timeout checks are added to
the puzzles and to the solution during conversion from AST to Python. Alternatively, the programs
could be evaluated in a sandbox as is done in programming competitions and as we did for the LM
generators, though a sandbox imposes an additional overhead.

B.1 Vocabulary

We use a grammar for a subset of Python covering the following basic objects: Booleans, unlimited-
precision integers, ﬂoats, strings, lists, sets, dictionaries, generators, and tuples. Table B.1 summarizes
the grammar. These rules occur multiply, for instance the addition rule has instantiations for adding
two strings, two integers, an integer and a ﬂoat, etc., where each Python type corresponds to a
non-terminal in our grammar. However, because Python is a duck-typed language, in several cases a
variable can be used with multiple different types. To handle such programs, we also have a generic
non-terminal which can correspond to any Python object, and this makes our grammar ambiguous.
For instance, the program 1+1 can be parsed either as the sum of two integers or as the sum of two
Python objects, also using a rule mapping an object to an integer. This latter program is a larger AST
and hence will typically have lower probability, hence we have the advantages of types when possible
but the ﬂexibility to generate fully duck-typed code. In this manner we are able to parse puzzles
from 138 of our 200 problems. We also use this grammar to generate timed and safe Python code. In
particular, we inject timing checks into comprehensions and loops, and we also add timing checks to
potentially time-consuming operations such as exponentiation or string multiplication. This grammar
is available upon request for researchers who wish to use it in further projects.

16

# Sum of digits.
def sat1(x: str, s: int=679):

return s == sum([int(d) for d in x])

# B. Random forest solution.
def sol(s):

return ((chr(49))*(COPY(s)))

# B. Transformer solution.
def sol(s):

return ((COPY(s))*(str(1)))

# Human-written solution.
def sol(s):

return int(s/9) * ’9’ + str(s%9)

----
# Line intersection.
def sat2(e: List[int], a: int=2, b: int=-1, c: int=1, d: int=2021):

x = e[0] / e[1]
return abs(a * x + b - c * x - d) < 10 ** -5

# B. Random forest and B. Transformer solution (identical).
def sol(a, b, c, d):

return ([2022, 1, ])

# Human-written solution.
def sol(a, b, c, d):

return [d - b, a - c]

---
# Find the three slice indices that give the speciﬁc target in string s.
def sat3(inds: List[int], s: str="hello world", target: str="do"):

i, j, k = inds
return s[i:j:k] == target

# B. Random forest solution.
def sol(s, target):

return ([12, 5, -(3), ])

# B. Transformer solution.
def sol(s, target):

return ([11, 1, -(6), ])

# Human-written solution.
def sol(s, target):

from itertools import product
for i, j, k in product(range(-len(s) - 1, len(s) + 1), repeat=3):

try:

if s[i:j:k] == target:

return [i, j, k]
except (IndexError, ValueError):

pass

Figure A.1: Example of three P3 puzzles and the solutions found by our examined solvers. The
natural language description of each problem is provided for ease of read, but is hidden to these
models. Human-written solutions are provided here for reference, but are also hidden from AI solvers.

17

Rule name

rule

!=
&
(tuple)
(tuple)
(tuple)
*
**
*=
*=
*args
*args
+
+=
+=
+unary
-
-=
-unary
/
//
//=
:slice
<
<<
<=
=
==
>
>=
COPY
[-1]
[-2]
[-3]
[-4]
[0]
[1]
[2]
[3]
[i]

(_)!=(_)
(_)&(_)
(_, _)
(_, _, _)
(_, _, _, _)
(_)*(_)
(_)**(_)
(_)*=(_)
_ *= (_)
*_
*_, **_
(_)+(_)
(_)+=(_)
_ += (_)
+(_)
(_)-(_)
(_)-=(_)
-(_)
(_)/(_)
(_)//(_)
(_)//=(_)
_:_:_
(_)<(_)
(_)<<(_)
(_)<=(_)
(_)=(_)
(_)==(_)
(_)>(_)
(_)>=(_)
COPY(_)
(_)[-1]
(_)[-2]
(_)[-3]
(_)[-4]
(_)[0]
(_)[1]
(_)[2]
(_)[3]
(_)[_]

Table B.1: The grammar for a subset of Python.
Rule name

Rule name

rule

[list]
%
{set}
^
abs
all
and
any
append
arg
arg
assert
assert
bool
chr
cos
count
def
def_ANY_tuple
default_arg
default_arg
endswith
exp
f_string
float
float-const
float-const-large
float-const-tiny
for
for
for_in_if
formatted_value {_:_}

[_]
(_)%(_)
{_}
(_)^(_)
abs(_)
all(_)
(_)and(_)
any(_)
(_).append(_)
_, _
_: _, _
assert _
assert _, _
bool(_)
chr(_)
cos(_)
(_).count(_)
def _(_): _
(_)
_: _=_, _
_=_, _
(_).endswith(_)
exp(_)
f’_’
float(_)
_._
_._e_
_._e-_
for (_) in (_): _
for (_, _) in (_): _
for _ in (_) if _

if
if
ifExp
in
index
int
is

if _: _
if _: _ else: _
(_) if (_) else (_)
(_) in (_)
(_).index(_)
int(_)
(_)is(_)

is not
issubset
issuperset
join
len
list
log
max
min
not
not in
or
ord
range
range
range
replace
return
reversed
revsorted
round
round
set
sin
sorted
split
split
startswith
str
str-const
sum
tuple
type
union
zip
zip
|

rule

(_)is not(_)
(_).issubset(_)
(_).issuperset(_)
(_).join(_)
len(_)
list(_)
log(_)
max(_)
min(_)
not (_)
(_) not in (_)
(_)or(_)
ord(_)
range(_)
range(_, _)
range(_, _, _)
(_).replace(_, _)
return (_)
reversed(_)
sorted(_, reverse=True)
round(_)
round(_, _)
set(_)
sin(_)
sorted(_)
(_).split(_)
(_).split()
(_).startswith(_)
str(_)
"_"
sum(_)
tuple(_)
type(_)
(_).union(_)
zip(_, _)
zip(_, _, _)
(_)|(_)

B.2 Transformer implementation

We use the RoBERTa-base 12-layers Transformer [36] pretrained on English text and ﬁne-tune it
on Python code using the Hugging Face library [59]. For ﬁne-tuning data, we use Python functions
with their documentation text from GitHub repositories [27]. In order to better adjust the tokenizer
to Python code, we retrain a Byte-level BPE tokenizer on our Python ﬁne-tuning data. We use the
same vocabulary size as the original tokenizer and keep the token embeddings of the overlapping
ones (39%). For the other tokens, we initialize new token embeddings. Thereafter, we ﬁne-tune
RoBERTa with a masked language modeling task for 30 epochs. This model, which we denote by
TP , achieved an impressive 3.3 perplexity score on held-out evaluation data, indicating its success in
learning Python’s syntax.

Next, we use TP to encode dense embeddings er = TP (r) for all the rules r in our vocabulary R. As
input to the Transformer, we use a string representation of the Python operation and types of each
rule. For example, (x)//(y)’ ’// -> FLOAT :: (x: INT, y: FLOAT) is used to describe the
rule for the // operation with an integer and ﬂoat inputs, resulting in a ﬂoat. Then, we take er as the
average across the top-layer embeddings of all tokens.
Finally, we design a neural model on top of TP to predict P(rj|φ(f ), p, i) for each puzzle f where
p is the parent rule and i is the child index. The model computes a hidden representation of the
puzzle with the parent rule as a concatenation h = [TP (f ), W1ep, ei] ∈ Rd+dr+di, where ei ∈ Rdi
is a learned embedding for the child rule index, W1 ∈ Rdr is a learned linear projection, and d
is the hidden dimension of TP . To obtain φ(f ), we duplicate TP and further ﬁne-tune it with the
rest of the solver parameters, while keeping the rule Transformer ﬁxed as TP . Speciﬁcally, we use
the [CLS] embedding of the top layer as φ(f ). Fixing the rule encoder prevents overﬁtting to the
rules seen in the puzzle-solution ﬁne-tuning pairs. h is then passed through two non-linear layers,
where the ﬁrst also projects it to Rdr , with a gelu activation [25] and batch normalization [29] to

18

Figure B.1: An illustration of our Transformer-based enumerative solver. The rule strings are encoded
with a Transformer pretrained on Python code. The puzzle Transformer is initialized the same, but is
further ﬁne-tuned on puzzles, together with the rest of the solver’s parameters shown in blue color.
The left hand side of a diagram represents the encoding of the puzzle f , parent rule p, and child index
i, each separately and then combined to a joint representation ef,p,i. All rules r ∈ R are also encoded
with the Transformer and projected to the same dimension as ef,p,i. The output probability of r being
the i’s child of p in the solution tree g to puzzle f is computed by a softmax over the product of ef,p,i
with all rule representations. Encoding the puzzle and the parent rule ﬁrst separately, allows passing
the puzzle only once during inference, and computing all rule embeddings in advance.

def f(s: str):

return s.count("o") == 1000 and s.count("oo") == 0

Figure C.1: A puzzle where type-checking is important. A type-safe solution is computed by the
program returning "ox" * 1000. However, ["o"] * 1000 would be considered invalid as it is a
list of strings, though it does satisfy the puzzle as stated.

get a joint puzzle and parent rule embedding ef,p,i. The score of rule rj then being the i’s argument
of r in the solution to f is determined by the dot product of its projected embedding erj with the
parent’s embedding: prj |φ(f ),ep,ei ∝ ef,p,i · (W2erj )T . Similar to the Random Forest ﬁtting process,
we use all parent-child rule pairs from the previously obtained solutions for ﬁne-tuning. We use
cross-entropy loss with an Adam optimizer. See Figure B.1 for a model diagram.

C Language Model solvers details

The GPT-3 and Codex APIs were used to generate completions based on prompts. For all models, the
completions were generated in batches of n=32 with temp=0.9, for a maximum of 150 tokens, with
default values of top_p=1, presence_penalty=0, frequency_penalty=0, and best_of=1. The
resulting programs were evaluated in a sandbox limited to 1 second on Intel Xeon Platinum 8272CL
CPUs at 2.60GHz. The timeout was necessary since a number of solution generators would take
prohibitive resources such as "a"*(10**(100)) which would generate a string of length googol.
The solutions where also checked to be of the type requested in the problem, as was the case for the
top-down solver. Figure C.1 illustrates a puzzle where type checking matters.

Prompt programming. The space of possible prompts is practically boundless. Our current prompt
designs leverage the API without ﬁne-tuning. For GPT-3, among the prompts we experimented
with, we found that the assert structure worked best but it was limited to one-line Python solutions.
One-line Python programs are considered, by some, to be a useful form of programming with books
dedicated to the topic [see, e.g., 39]. For Codex, we found a prompt that resembled a legal python ﬁle
with a multi-line solution structure worked better.

19

Puzzle TransformerRule  TransformerCLSRule  TransformerJoint Encoder1.....def f1(s: str):

return "Hello " + s == "Hello world"

assert True == f1("world")

---

def f2(s: str):

return "Hello " + s[::-1] == "Hello world"

assert True == f2("world"[::-1])

---
def f3(x: List[int]):

return len(x) == 2 and sum(x) == 3

assert True == f3([1, 2])

---
def f4(s: List[str]):

return len(set(s)) == 1000 and all(

(x.count("a") > x.count("b")) and (’b’ in x) for x in s)

assert True == f4(["a" * (i + 2) + "b" for i in range(1000)])

---

def f5(n: int):

return str(n * n).startswith("123456789")

assert True == f5(int(int("123456789" + "0"*9) ** 0.5) + 1)

---

def f6(li: List[int]):

return len(li) == 10 and li.count(li[3]) == 2

assert True == f6(...

Figure C.2: The medium-length prompt, used for GPT-3. The ﬁrst ﬁve example puzzles f1-f5 were
shown to people in the user study and f6 is the one that is being solved. GPT-3’s completion was
...[1,2,3,3,4,5,6,7,8,9])

Numerous other prompts were considered. For instance, we tried adding a preface stating, “A
Programming Puzzle is a short python function, and the goal is to ﬁnd an input such that the function
True. In other words, if program computes a function f, then the goal is to ﬁnd x such that f(x)=True.”

Interestingly, a handful of generations included potentially dangerous commands such as eval
and __import__("os").system(..., but a cursory inspection did not detect any that used them
in malicious ways. We do advise caution in executing generated code, as malicious actors can
take advantage of such routine [50]. There are several libraries available for scoring programming
competitions to serve this purpose. Also, some of the generated code seemed especially human-like,
e.g.: raise RuntimeError("this is a joke.") which of course did not solve the puzzle at
hand.

Figures 3, C.2, C.5-C.4, and C.7 show our prompts for the Short, Medium, Long, and Bootstrap
prompts, respectively.

20

from typing import List

def f1(s: str):

return "Hello " + s == "Hello world"

def g1():

return "world"

assert f1(g1())

def f2(s: str):

return "Hello " + s[::-1] == "Hello world"

def g2():

return "world"[::-1]

assert f2(g2())

def f3(x: List[int]):

return len(x) == 2 and sum(x) == 3

def g3():

return [1, 2]

assert f3(g3())

def f4(s: List[str]):

return len(set(s)) == 1000 and all((x.count("a") > x.count("b")) and (’b’ in x)
for x in s)

def g4():

return ["a"*(i+2)+"b" for i in range(1000)]

assert f4(g4())

def f5(n: int):

return str(n * n).startswith("123456789")

def g5():

return int(int("123456789" + "0"*9) ** 0.5) + 1

assert f5(g5())

def f6(inds: List[int], string="Sssuubbstrissiingg"):

return inds == sorted(inds) and "".join(string[i] for i in inds) == "substring"

def g6(string="Sssuubbstrissiingg"):

Codex completed it successfully as:

inds = []
ind = 0
for c in "substring":

while string[ind] != c:

ind += 1
inds.append(ind)
ind += 1

return inds

Figure C.3: The medium-length prompt, used for Codex. The ﬁrst ﬁve example puzzles f1-f5 were
given in the tutorial to participants in the user study and f6 is the puzzle that is being solved.

21

Smoothing evaluation. Rather than simply generating solutions until the ﬁrst correct one is found,
to evaluate the Short, Medium and Long prompts, we generate 10,000 solutions for each puzzle. This
gives us more than one solution for some puzzles, which we use for improved accuracy in estimating
how many solutions are necessary (on average) to solve each puzzle shown in Figure 4b. We use the
unbiased estimator of pass@k deﬁned by Chen et al. [13].

D NP-completeness

Before formally proving that the puzzle decision problem is NP-complete, note that the Boolean
Satisﬁability problem (SAT) is NP-complete and any Boolean SAT formula such as (x0 ∨ ¬x7 ∨
x17) ∧ . . . can trivially be rewritten as a puzzle, e.g.,

def f(x: List[bool]):

return (x[0] or not x[7] or x[17]) and ...

The size of f is linear in the formula size. Thus converting a SAT formula to a puzzle is natural and
does not make the problem much bigger or harder.

However, a common misconception is that NP-complete problems are all equally intractable, but the
theory of NP-completeness only speaks to the worst-case complexity of solving all puzzles. While
any of our puzzles could theoretically be converted to a SAT formula, the resulting formula would be
mammoth without any abstraction or intuition. For example, consider the following puzzle,

# ﬁnd a non-trivial integer factor

def f(d: int):
"""Hint ,
n = 100433627766186892221372630609062766858404681029709092356097
return 0 < d < n and n % d == 0

try d = 618970019642690137449562111 ;−)"""

This puzzle is identical to the factoring puzzle f3 from Figure 1 except that the answer is given
away in a comment. Any natural compiler from Python to SAT would ignore comments so the
SAT form of this trivial puzzle would be quite hard. While we are not aware of such a compiler,
there are programs that convert a factoring problem to a SAT instance. We ran such a converter
http://cgi.cs.indiana.edu/ sabry/cnf.html on this n and it generated a formula with 113,878 variables
and 454,633 terms! This illustrates that not all polynomials are small, and that some easy puzzles
may become hard puzzles in such a conversion. The theory of NP-completeness only guarantees that
if one can efﬁciently solve every SAT instance one could efﬁciently solve every puzzle, but speciﬁc
easy puzzles may become quite hard SAT formulas.

D.1 Proof of NP-completeness

Formally, a puzzle f represents a Turing machine as a string, a timeout t is a positive integer
represented in unary, and the decision problem is, given (f, x, t), does there exist y such that when
the Turing machine f is run on (y, x), it halts in fewer than t steps and outputs 1. The time constraint
is necessary to ensure that the puzzle decision problem is in NP. It is well-known that this problem is
in NP and, moreover is NP-complete:

Observation 1. The puzzle decision problem is NP-complete.

Proof. One can test whether a given puzzle string f encoding a Turing machine halts on a witness
y in time ≤ t by simulating running f on (y, x) for t steps. Since simulating a Turing machine of
size |f | running for t steps can be done in poly(|f |, t) time, this can be done in time poly(|f |, t) as
required for NP.

To see that the problem is complete, note that given any other NP problem deﬁned by a Turing
machine T (x, y) that runs on input x ∈ Σ∗ and witness y ∈ Σ∗ in polynomial time t = p(|x|) is a
type of puzzle itself for f = T (with inputs swapped).

22

E Open problems

The following ﬁve puzzles would each represent a major breakthrough in computer science or
mathematics if solved.

1. Factoring. In the traditional version of this ancient problem, the goal is to efﬁciently ﬁnd
the prime factorization of a given integer. In the puzzle version, we state the equivalent
problem of ﬁnding any non-trivial factor of a given integer. The puzzle is equivalent in the
sense that one can recursively call the puzzle on each of the factors found until one achieves
the complete prime factorization. A number of factoring algorithms have been developed
over decades that factor larger and larger numbers. The RSA Factoring Challenge [see, e.g.,
30] has awarded tens of thousands of dollars in prize money and RSA offered $200,000 for
factoring the largest RSA challenge number with 617 digits. The closely related Discrete
Log problem is also unsolved.

2. Graph Isomorphism. Given two isomorphic graphs, ﬁnd the bijection that relates the two of
them. In a breakthrough, Babai has claimed a quasi-polynomial time for this problem, but
no polynomial time algorithm is known.

3. Planted Clique. In this classic graph-theory problem, an n-node Erd˝os–Rényi random graph
random graph is chosen and then k nodes are selected at random and the edges are added so
that they form a clique. The problem is to ﬁnd the clique. It is not known whether there is a
polynomial-time algorithm for this problem [see, e.g., 4].

4. Learning Parity with Noise. This is a binary classiﬁcation problem in computational learning
theory. Roughly speaking, the problem is to efﬁciently learn a parity function with random
classiﬁcation noise. The fastest known algorithm for this problem runs in time ˜O(2n/ log n)
[11]. The problem is also closely related to efﬁciently decoding random linear codes [7] and
various assumptions in cryptography. Note that some of the instances of this problem are
small (and thus easy) while others are quite large.

5. Collatz cycle. The problem is to ﬁnd a cycle in the famous 3n + 1 process, where you start
with integer n > 0 and repeatedly set n to n/2 if n is even, otherwise 3n + 1, until you
reach 1. The Collatz cycle conjecture is that there are no cycles in this process. According
to the Wikipedia article on the topic, Jeffrey Lagarias stated that it “is an extraordinarily
difﬁcult problem, completely out of reach of present day mathematics” and Paul Erd˝os said
“Mathematics may not be ready for such problems.” He also offered $500 for its solution.

Each of these problems is described by a short (1-5 line) python function. Now, for the algorithms
problems 1-3, the puzzle involves solving given instances and not exactly with the open problem:
coming up with a provably polynomial-time algorithm, and it is entirely possible that no poly-
time algorithm exists. However, these are all problems that have been intensely studied and an
improvement, even a practical one, would be a breakthrough. For the Collatz cycle, if the Collatz
conjecture holds then there is no cycle. However, we give problems involving ﬁnding integers with
large Collatz delays which could be used to, at least, break records. Also noteworthy but perhaps
not as well-known is Conway’s 99 puzzle, an unsolved problem in graph theory due to Conway and
[10] (as cited by Wikipedia). The two-line puzzle describes ﬁnding an undirected graph with 99
vertices, in which each two adjacent vertices have exactly one common neighbor, and in which each
two non-adjacent vertices have exactly two common neighbors. Conway [16] offered $1,000 for its
solution.

There are also several unsolved puzzles in terms of beating records, e.g., ﬁnding oscillators or
spaceships of certain periods in Conway’s game of life and ﬁnding uncrossed knights tours on chess
boards of various sizes.

F Comparing puzzles to competitive-programming problems

Figure F.1 illustrates an elementary codeforces.com problem. As is typical in programming
competitions, the authors have concocted an entertaining story to motivate the problem. Dagien˙e
and Futschek [17] include “should be funny” and “should have pictures” among desirable criteria
for competitive programming problems. Also, as is typical the ﬁrst step is explaining how the input

23

is formatted and how the output should be formatted. One difﬁculty in authoring such competitive-
programming challenges is ensuring that the English description unambiguously matches with the
hidden test cases. The ICPC rules state: “A contestant may submit a claim of ambiguity or error in
a problem statement by submitting a clariﬁcation request. If the judges agree that an ambiguity or
error exists, a clariﬁcation will be issued to all contestants.” With puzzles, this is not necessary—a
mistake in a puzzle either means that the puzzle is unsolvable or that the puzzle has an unexpected
(often trivial) solution, neither of which cause major problems as it would still be a fair comparison
of different solvers.
The puzzle form InvertPermutation10 has no story, no description of input/output format, and no
examples. The input/output formatting is taken care of simply by the type hints.

The intention is for puzzles to isolate the essence of the part of the problem that involves reasoning.
Other datasets already address natural language understanding and input/output string formatting.

G User Study Details

The user study began with a short tutorial about puzzles, which included the puzzles shown in Figure
C.2. The 30 puzzles (see Figures G.6-G.7) were divided into three parts of 10 puzzles each: numbers
1-10, 11-20, and 20-30. Since each puzzle took at maximum of 6 minutes, no part took more than
one hour. In the internal IRB approval (July 22, 2020), the key discussion points were that we would
not collect age, gender or any other PII since it was not relevant to our study.

G.1 Provided instructions

Figures G.1-G.3 present the initial instructions that participants were given before starting the study.
Figures G.4-G.5 show the interface that they used for retrieving puzzles and submitting solutions.
We run implement a Python backend to store progress logs and to serve each puzzle in its turn, so
participants won’t accidentally be exposed to any of the puzzles in advance. We asked participants to
follow the simple interface and not to attempt any sophisticated hacking techniques that will give
them any personal advantage. We did not observe any such malicious behaviour and received positive
feedback for the stability and clarity of the interface.

G.2 Qualitative feedback.

Our Jupyter notebook interface also allowed users to submit qualitative feedback. As an example
of this last point, participants mentioned that they were not familiar with functions such as zip or
all but learned them in the course of the study. Overall, Three themes emerged in the feedback:
participants enjoyed solving the puzzles, they felt that 6 minutes was not enough time to solve the
puzzles, and they felt they learned Python from doing the puzzles.

G.3 Results summary

A total of 21 participants completed the user study. Participants solved between 12-30 puzzles,
with 6 participants solving more than 28 puzzles, and only a single participant solving all 30. As
Figure G.8 shows, the participants Python experience ranged between a few months to 8 years, with a
median of 3 years. For post study analysis purposes, we denote participants with less than 3 years of
experience as beginners and the rest as experienced. Figure G.9 shows the number of participants
that solved each puzzle, grouped by experience. 9 of the puzzles were solved by all beginners,
whereas 17 puzzles were solved by all experienced. This positive correlation between the number of
programming experience and number of puzzles solved, indicates the effectiveness of our puzzles as
a proxy to evaluating programming proﬁciency.

We also notice that experienced programmers solve puzzles faster (149 seconds per puzzle on
average, compared to 194 seconds for beginners). Figure G.10 shows the distribution of time spent
by participants on each puzzle. We use the per puzzle average solving time as an indicator to its

10In P3, we have slightly modiﬁed the problem so that it is only inspired by the codeforces problem and not a
direct translation. The P3 problem is harder in that characters not in the permutation may also appear in the
string unmodiﬁed.

24

perceived difﬁculty. As discussed in the main paper (§5.1), we see a strong correlation between the
perceived difﬁculty of different puzzles for humans and for our examined AI solvers.

H Solution to Tower of Hanoi

Codex’s solution to the Tower of Hanoi puzzle is shown in Figure H.1. Even though the puzzle did
not mention the word Hanoi, Codex’s solution clearly knew the reference, in fact offering a link to the
Wikipedia page. The ﬁrst part of the URL is correct, but there is no “Advanced computer algorithm”
section on the page, so the link simply resolves to the Wikipedia page. The Python code on the
Wikipedia page is only similar in spirit, in part because the way the puzzle asks for the moves is
somewhat different from the Wikipedia page. This is a difﬁcult puzzle for which solutions are found
at a rate of approximately 0.03%. Surprisingly, Codex was not able to solve the puzzle when we
renamed the variable num_disks to n and removed the string "bigger disk on top", possibly
because the association with Tower of Hanoi was weaker.

25

def f1(s: str):

"""Find a string that when concatenated onto ’ Hello ’ gives ’ Hello world ’. """
return "Hello " + s == "Hello world"

assert True == f1("world")

---

def f2(s: str):

"""Find a string that when reversed and concatenated onto ’ Hello ’ gives ’ Hello
world ’. """
return "Hello " + s[::-1] == "Hello world"

assert True == f2("world"[::-1])

---

def f3(x: List[int]):
"""Find a list of
return len(x) == 2 and sum(x) == 3

two integers whose sum is 3. """

assert True == f3([1, 2])

---

def f4(s: List[str]):

"""Find a list of 1000 distinct
at
return len(set(s)) == 1000 and all(

least one ’b ’. """

strings which each have more ’a’s than ’b’s and

(x.count("a") > x.count("b")) and (’b’ in x) for x in s)

assert True == f4(["a" * (i + 2) + "b" for i in range(1000)])

---

def f5(n: int):

"""Find an integer whose perfect
representation . """
return str(n * n).startswith("123456789")

square begins with 123456789 in its decimal

assert True == f5(int(int("123456789" + "0"*9) ** 0.5) + 1)

---

def f6(li: List[int]):
"""Find a list of
return len(li) == 10 and li.count(li[3]) == 2

length 10 where the fourth element occurs exactly twice . """

assert True == f6(...

Figure C.4: An example GPT-3 Long prompt which includes English descriptions in the Python
docstrings. As in the medium-length prompts, the ﬁrst ﬁve example puzzles f1-f5 were shown to
people in the user study and f6 is the one that is being solved.

26

from typing import List

def f1(s: str):

return "Hello " + s == "Hello world"

def g1():

"""Find a string that when concatenated onto ’ Hello ’ gives ’ Hello world ’. """
return "world"

assert f1(g1())

def f2(s: str):

return "Hello " + s[::-1] == "Hello world"

def g2():

"""Find a string that when reversed and concatenated onto ’ Hello ’ gives ’ Hello world ’.
"""
return "world"[::-1]

assert f2(g2())

def f3(x: List[int]):

return len(x) == 2 and sum(x) == 3

def g3():

"""Find a list of
return [1, 2]

assert f3(g3())

two integers whose sum is 3. """

def f4(s: List[str]):

return len(set(s)) == 1000 and all(

(x.count("a") > x.count("b")) and (’b’ in x) for x in s)

def g4():

"""Find a list of 1000 distinct
one ’b ’. """
return ["a"*(i+2)+"b" for i in range(1000)]

strings which each have more ’a’s than ’b’s and at

least

assert f4(g4())

def f5(n: int):

return str(n * n).startswith("123456789")

def g5():

"""Find an integer whose perfect

square begins with 123456789 in its decimal

representation . """

return int(int("123456789" + "0"*9) ** 0.5) + 1

assert f5(g5())

def sat(d: int, n=6002685529):

return n % d == 0 and all(i in "47" for i in str(d))

def sol(n=6002685529):
"""Find a integer
"""

factor of n whose decimal

representation

consists only of 7’s and 4’s .

Figure C.5: An example Codex long prompt which includes English descriptions in the Python
docstrings. As in the medium-length prompts, the ﬁrst ﬁve example puzzles f1-f5 were shown to
people in the user study and f6 is the one that is being solved. The representation here supports
multi-line solutions. Codex’s successful completion is shown in Figure C.6.

27

# return the closest factor to n which is inferior either to n or to the sqrt(n)

if n == 1:

# def.: n>1!

return 1
i = int(n ** 0.5)
while True:

if f6(i):

return i
elif i == 2:

raise Exception("Could not find any 7 or 4 number factors

for n.")

i -= 1

Figure C.6: An example Codex completion to the long prompt illustrated in C.5. The solution cleverly
calls f6, a trick that some participants used to solve a couple of the user study puzzles. Codex also
generated English comments.

28

from typing import List

def f1(item: int, li=[17, 2, 3, 9, 11, 11], index=4):

return li.index(item) == index

def g1(li=[17, 2, 3, 9, 11, 11], index=4):

return li[index]

assert f1(g1())

def f2(s: str, word="antidisestablishmentarianism", max_len=10):

if len(word) <= max_len:
return word == s

return int(s[1:-1]) == len(word[1:-1]) and word[0] == s[0] and word[-1] == s[-1]

def g2(word="antidisestablishmentarianism", max_len=10):

if len(word) == max_len:

return word

return word[0] + str(len(word[1:-1])) + word[-1]

assert f2(g2())

#
# omitting 28 random puzzles that Codex solved...
#

assert f31(g31())

def f32(x: List[int], a=7, s=5, e=200):

return x[0] == a and x[-1] <= e and (x[-1] + s > e) and all([x[i] + s == x[i+1]
for i in range(len(x)-1)])

def g32(a=7, s=5, e=200):

def f1(s: str, a: List[str]=[’cat’, ’dot’, ’bird’], b: List[str]=[’tree’, ’fly’, ’

dot’]):
return s in a and s in b

assert True == f1(’dot’)

---

def f2(li: List[int]):

return all([sum(li[:i]) == i for i in range(20)])

assert True == f2(list(map(lambda x: 1, range(100))))

#
# omitting 22 random puzzles that GPT-3 solved...
#

---

def f25(probs: List[float]):

assert len(probs) == 3 and abs(sum(probs) - 1) < 1e-6
return max(probs[(i + 2) % 3] - probs[(i + 1) % 3] for i in range(3)) < 1e-6

assert True == f25(

Figure C.7: Example bootstrapping prompts for the Codex and GPT-3 LMs. The prompts includes
random solved puzzles among those that the LM solved, truncated to the token limit of the API (2048
for GPT3 and 4096 for Codex).

29

Codeforces problem 474 A. Keyboard
Our good friend Mole is trying to code a big message. He is typing on an unusual keyboard
with characters arranged in following way:

qwertyuiop
asdfghjkl;
zxcvbnm,./

Unfortunately Mole is blind, so sometimes it is problem for him to put his hands accurately. He
accidentally moved both his hands with one position to the left or to the right. That means that
now he presses not a button he wants, but one neighboring button (left or right, as speciﬁed in
input).
We have a sequence of characters he has typed and we want to ﬁnd the original message.

Input
First line of the input contains one letter describing direction of shifting (’L’ or ’R’ respectively
for left or right).

Second line contains a sequence of characters written by Mole. The size of this sequence will be
no more than 100. Sequence contains only symbols that appear on Mole’s keyboard. It doesn’t
contain spaces as there is no space on Mole’s keyboard.

It is guaranteed that even though Mole hands are moved, he is still pressing buttons on keyboard
and not hitting outside it.

Output
Print a line that contains the original message.

Examples

input
R
s;;upimrrfod;pbr

output
allyouneedislove

def f(s: str, perm="qwertyuiopasdfghjkl;zxcvbnm,./", target="s;;upimrrfod;pbr"):

return "".join(perm[perm.index(c) + 1] for c in s) == target

Figure F.1: Example of an introductory competition problem https://codeforces.com/
problemset/problem/474/A (top) and the respective puzzle version (bottom) that is only us-
ing code and is short to read. In this problem, there is a given permutation of characters π, and a
given target string t, and one wants to ﬁnd a source string s such that when each character of s has
been permuted with π, the target is achieved. The puzzle has been simpliﬁed to always shift right.

30

Figure G.1: Instructions page provided to the study participants as a Jupyter notebook (part 1).

Figure G.2: Instructions page provided to the study participants as a Jupyter notebook (part 2).

31

Figure G.3: Instructions page provided to the study participants as a Jupyter notebook (part 3).

Figure G.4: The introduction of the study notebook given to participants.

32

(a) Initial view.

(b) View while solving a puzzle. The progress bar advances towards the 6 minutes limit.

(c) View after submitting a successful solution.

(d) View after 6 minutes have passed since viewing the puzzle without submitting a valid solution.

(e) View when submitting a wrong solution to a puzzle (before timeout is reached).

Figure G.5: The interface used by participants to solve puzzles during the study. Each sub-ﬁgure
shows a different state of the notebook according to the user’s interaction.

33

def f1(s: str):

return s.count("o") == 1000 and s.count("oo") == 100 and s.count("ho") == 801

def f2(s: str):

return s.count("o") == 1000 and s.count("oo") == 0

def f3(x: List[int]):

return sorted(x) == list(range(999)) and all(x[i] != i for i in range(len(x)))

def f4(x: List[int]):

return len(x) == 10 and x.count(x[3]) == 2

def f5(x: List[int]):

return all([x.count(i) == i for i in range(10)])

def f6(n: int):

return n % 123 == 4 and n > 10**10

def f7(s: str):

return str(8**2888).count(s) > 8 and len(s) == 3

def f8(s: List[str]):

return s[1234] in s[1235] and s[1234] != s[1235]

def f9(x: List[int]):

return ["The quick brown fox jumps over the lazy dog"[i] for i in x] \

== list("The five boxing wizards jump quickly")

def f10(s: str):

return s in str(8**1818) and s==s[::-1] and len(s)>11

def f11(x: List[str]):

return min(x) == max(x) == str(len(x))

def f12(x: List[int]):

return all(a + b == 9 for a, b in zip([4] + x, x)) and len(x) == 1000

def f13(x: float):

return str(x - 3.1415).startswith("123.456")

def f14(x: List[int]):

return all([sum(x[:i]) == i for i in range(20)])

def f15(x: List[int]):

return all(sum(x[:i]) == 2 ** i - 1 for i in range(20))

Figure G.6: The ﬁrst 15 puzzles in the user study.

34

def f16(x: str):

return float(x) + len(x) == 4.5

def f17(n: int):

return len(str(n + 1000)) > len(str(n + 1001))

def f18(x: List[str]):

return [s + t for s in x for t in x if s!=t] == ’berlin berger linber linger
gerber gerlin’.split()

def f19(x: Set[int]):

return {i+j for i in x for j in x} == {0, 1, 2, 3, 4, 5, 6, 17, 18, 19, 20, 34}

def f20(x: List[int]):

return all(b in {a-1, a+1, 3*a} for a, b in zip([0] + x, x + [128]))

def f21(x: List[int]):

return all([x[i] != x[i + 1] for i in range(10)]) and len(set(x)) == 3

def f22(x: str):

return x[::2] in x and len(set(x)) == 5

def f23(x: List[str]):

return tuple(x) in zip(’dee’, ’doo’, ’dah!’)

def f24(x: List[int]):

return x.count(17) == 3 and x.count(3) >= 2

def f25(s: str):

return sorted(s)==sorted(’Permute me true’) and s==s[::-1]

def f26(x: List[str]):

return "".join(x) == str(8**88) and all(len(s)==8 for s in x)

def f27(x: List[int]):

return x[x[0]] != x[x[1]] and x[x[x[0]]] == x[x[x[1]]]

def f28(x: Set[int]):

return all(i in range(1000) and abs(i-j) >= 10 for i in x for j in x if i != j) \

and len(x)==100

def f29(x: Set[int]):

return all(i in range(1000) and abs(i*i - j*j) >= 10 for i in x for j in x if i

!= j) and len(x) > 995

def f30(x: List[int]):

return all([123*x[i] % 1000 < 123*x[i+1] % 1000 and x[i] in range(1000)

for i in range(20)])

Figure G.7: The last 15 puzzles in the user study.

35

Figure G.8: Years of Python programming experience distribution of our study participants. For post
study analysis purposes, we split the group by the median (3 years) to beginners and experienced
programmers. The right violin plot shows the fraction of puzzles solved by participants with different
years of experience. The lines in the violin show the four quartiles.

Figure G.9: Fraction of participants, divided to experienced and beginners, that solved each of the 30
puzzles in less than 6 minutes.

Figure G.10: The time that participants spent on each study puzzle, up to 360 seconds per puzzle.
For unsolved puzzles, we count the time as using the full 6 minutes. The orange colored areas on
the right show the time distribution for beginner Python coders with less than three years experience
(10 out of 21). The blue colored areas on the left present the solving time distribution per puzzle for
users with at least three years of experience in Python.

36

The prompt was the medium Codex prompt (without English descriptions):

from typing import List

def f1(s: str):

return "Hello " + s == "Hello world"

def g1():

return "world"

assert f1(g1())

def f2(s: str):

return "Hello " + s[::-1] == "Hello world"

def g2():

return "world"[::-1]

assert f2(g2())

# skipping two puzzles, see Figure C.3

assert f5(g5())

def f6(moves: List[List[int]], num_disks=8):

state = [1] * num_disks
for [i, j] in moves:

assert state.index(i) <= (state + [1, 2, 3]).index(j), "bigger disk

on top"

state[state.index(i)] = j

return state == [3] * num_disks

def g6(num_disks=8):

And a valid completion from Codex, the smaller cushman-codex engine, was:

# Algorithm is equivalent to moving all disks.
# From https://en.wikipedia.org/wiki/Tower_of_Hanoi#Advanced_computer_algorithm
def hanoi(n, p, q, r):

if n > 0:

hanoi(n - 1, p, r, q)
moves.append([p, r])
hanoi(n - 1, q, p, r)

moves = []
hanoi(num_disks, 1, 2, 3)
assert f6(moves, num_disks)
return moves

Figure H.1: A solution to the Tower of Hanoi puzzle found by the small Codex. The Wikipedia
page link generated by the model indeed contains (though not in the Advanced computer algorithm
section) a solution to a slightly different formulation of this puzzle, see Figure H.2. Note that the
medium prompt doesn’t mention the name of the puzzle. Codex made the correct association, and
adjusted the solution code to the state-based representation of this puzzle as given in f6. Interestingly,
replacing the use of disks in the puzzle’s variable names with other non-descriptive options seems to
prevent Codex from solving this puzzle.

37

A = [3, 2, 1]
B = []
C = []

def move(n, source, target, auxiliary):

if n > 0:

# Move n - 1 disks from source to auxiliary, so they are out of the way
move(n - 1, source, auxiliary, target)

# Move the nth disk from source to target
target.append(source.pop())

# Display our progress
print(A, B, C, ’##############’, sep=’\n’)

# Move the n - 1 disks that we left on auxiliary onto target
move(n - 1, auxiliary, target, source)

# Initiate call from source A to target C with auxiliary B
move(3, A, C, B)

Figure H.2:
The algorithm from https://en.wikipedia.org/wiki/Tower_of_Hanoi#
Recursive_implementation (November, 2021) that solves Tower of Hanoi for a representation in
which the three towers are lists with disk indices.

38

