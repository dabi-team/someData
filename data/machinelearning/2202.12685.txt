Machine learning ﬂow control with few sensor feedback and measurement noise

Machine learning ﬂow control with few sensor feedback and measurement
noise

R. Castellanos∗,1, 2 G. Y. Cornejo Maceda,3 I. de la Fuente,1 B. R. Noack,3, 4 A. Ianiro,1 and S.Discetti1
1)Aerospace Engineering Research Group, Universidad Carlos III de Madrid, Leganés 28912,
Spain
2)Theoretical and Computational Aerodynamics Branch, Flight Physics Department, Spanish National Institute for Aerospace
Technology (INTA), Torrejón de Ardoz 28850, Spain
3)School of Mechanical Engineering and Automation, Harbin Institute of Technology (Shenzhen), University Town, Xili,
Shenzhen 518055, People’s Republic of China
4)Institut für Strömungsmechanik und Technische Akustik (ISTA), Technische Universität Berlin, Müller-Breslau-Straße 8,
D-10623 Berlin, Germany

(*Electronic mail: rcastell@ing.uc3m.es)

(Dated: 22 April 2022)

A comparative assessment of machine learning (ML) methods for active ﬂow control is performed. The chosen bench-
mark problem is the drag reduction of a two-dimensional Kármán vortex street past a circular cylinder at a low Reynolds
number (Re = 100). The ﬂow is manipulated with two blowing/suction actuators on the upper and lower side of a cylin-
der. The feedback employs several velocity sensors. Two probe conﬁgurations are evaluated: 5 and 11 velocity probes
located at different points around the cylinder and in the wake. The control laws are optimized with Deep Reinforce-
ment Learning (DRL) and Linear Genetic Programming Control (LGPC). By interacting with the unsteady wake, both
methods successfully stabilize the vortex alley and effectively reduce drag while using small mass ﬂow rates for the
actuation. DRL has shown higher robustness with respect to variable initial conditions and to noise contamination of
the sensor data; on the other hand, LGPC is able to identify compact and interpretable control laws, which only use
a subset of sensors, thus allowing reducing the system complexity with reasonably good results. Our study points at
directions of future machine learning control combining desirable features of different approaches.

Keywords: Machine Learning; Deep Reinforcement Learning; Linear Genetic Programming; Flow Control; Drag

Reduction.

I.

INTRODUCTION

Flow control of turbulent ﬂows, in particular with the pur-
pose of drag reduction, is a recurrent research objective that
has regained interest during the last decade1. Machine learn-
ing, in particular, plays a key role in the development of so-
phisticated and efﬁcient ﬂow-control algorithms2 and presents
a possible solution to the difﬁculties imposed by the non-
linearity, time-dependence, and high dimensionality inherent
to the Navier-Stokes equations.

Control of wake ﬂows, in particular, has attracted signiﬁ-
cant interest due to its relevance in a wide variety of research
and industrial applications. Control strategies commonly tar-
get two main types of drag: the skin friction drag, caused due
to the viscosity of the ﬂuid interacting with the wall surface,
and the wake drag, which originates after the affected body.

Passive drag reduction methods, such as the classical dim-
ples on the surface of a sphere3 to delay transition or the use
of splitter plates in the wake4 to suppress the vortex shedding,
have shown to be quite successful. Nonetheless, during these
last decades, hardware/software advances are pushing towards
active methods instead, which exploit the potential advantage
of tuning the action according to the ﬂow state.

Active drag reduction techniques initially focused on in-
creasing base pressure to reduce pressure drag, using transpi-
ration and vibration techniques for such an objective. Contin-
uous or pulsating-based-bleeds could be used to modify the
ﬂow in the separated region. For the latter, drag reduction
could be achieved with zero net mass addition, with maxi-

mum effectiveness at a frequency twice the Kármán shedding
frequency5. Among the wide variety of active drag reduc-
tion techniques that resulted effectively, small jets have shown
to be very efﬁcient, enabling separation control with weak
actuation6.

The design of effective ﬂow control techniques is a chal-
lenging objective, especially when the solution is based only
on limited velocity or pressure data extracted from the ﬂuid
ﬂow7.
In its essence, the ﬂow control problem can be de-
scribed as a functional optimization problem, in which the
state of the dynamical system has to be inferred from a lim-
ited number of observable quantities. The objective is to ﬁnd
a control function that minimizes (or maximizes) a cost (or
reward) function, based on the desired features for the con-
trolled ﬂow conﬁguration.

Considering the categorization of control strategies, one of
the main classiﬁcations is based on the existence of a model to
describe the system to be controlled, distinguishing between
model-based and model-free control. The latter encloses the
kind of control strategies where an optimized control law is
extracted without imposing any model of the dynamical sys-
tem. The popularity of these approaches has been growing
considerably in the last decade, thanks to the popularization
and advances in machine learning techniques. Two of the
most prominent model-free control techniques from the ma-
chine learning literature are Reinforcement Learning8 (RL)
and Genetic Programming9 (GP). Reinforcement Learning is
an unsupervised learning technique focused on optimizing a
decision-making process interactively, which makes it a pre-

2
2
0
2

r
p
A
1
2

]
n
y
d
-
u
l
f
.
s
c
i
s
y
h
p
[

2
v
5
8
6
2
1
.
2
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
Machine learning ﬂow control with few sensor feedback and measurement noise

2

ferred choice in ﬂow control over its alternatives. On the other
hand, Genetic Programming algorithms are focused on recom-
bining good control policies by systematic testing, exploiting
the ones with the best results and exploring possible alterna-
tives in the solution space.

Genetic Programming, originally pioneered by Koza 9 be-
longs to the family of Evolutionary Algorithms (EA), which
have a common workaround:
a population of individu-
als, called a generation, compete at a given task with a
well-deﬁned cost function, and evolve based on a set of
rules, promoting the most successful strategies to the next
generation7,10. It constitutes a powerful regression technique
able to re-discover and combine ﬂow control strategies, which
have been proven useful in the cases of multi-frequency forc-
ing, direct feedback control and controls based on ARMAX
(Autoregressive Moving Average eXogenous), without any
physics information11,12. Machine Learning Control (MLC)7
based on tree-based Genetic Programming has been able to
develop laws from small to moderate complexity, e.g. the pha-
sor control, threshold-level based control, periodic or multi-
frequency forcing,
including jet mixing optimization with
multiple independently unsteady minijets placed upstream of
nozzle exit13, the analysis of the effect of a single unsteady
minijet for control14, drag reduction past bluff bodies15 , shear
ﬂow separation control16, reduction of vortex-induced vibra-
tions of a cylinder17, mixing layer control18, and wake sta-
bilization of complex shedding ﬂows19 among others. Some
relevant improvements have been made to the MLC frame-
work such as the integration of a Linear-based Genetic Pro-
gramming (LGP) algorithm20, which is the chosen option in
this study. Recently, a faster version of MLC based on the
addition of intermediate gradient-descent steps between the
generations (gMLC) has been developed12

Reinforcement Learning is based on an agent learning an
optimized policy based on the different inputs and outputs.
The key feature of RL is that the only information available
for the algorithm is given in the form of a penalty/reward
concerning a certain action performed by the model, but no
prior information is available on the best action to take. In
Deep RL (DRL), the agent is modelled by an Artiﬁcial Neu-
ral Network (ANN) which needs to be trained21. ANN have
a long history of use to parametrize control laws22, or to
ﬁnd the optimized ﬂow control strategy for several problems
such as swimming of ﬁsh schoolings23, control of unmanned
aerial vehicles24, and optimization of glider trajectory taking
ascendance25. On the other hand, the exploitation of DRL
speciﬁcally on ﬂow control is relatively recent. The ﬁrst ap-
plications targeted the control of the shedding wake of a cylin-
der in simulations21,26 and in experiments27. DRL has been
enforced in tuning the heat transport in a two-dimensional
Rayleigh–Benard convection28, in the control of the the in-
terface of unsteady liquid ﬁlms29 and in stabilizing the wake
past a cylinder by imposing a rotation on two control cylinders
located at both sides30. Recently, Li and Zhang 31 investigated
how to use and embed physical information of the ﬂow in the
DRL control of the wake of a conﬁned cylinder, and Paris,
Beneddine, and Dandois 32 explored the utilization of DRL
for optimal sensor layout to control the ﬂow past a cylinder.

It can be argued that LGP and DRL share many similarities,
up to the point in which many ﬁtness functions in LGP can be
considered as DRL systems33. The recent ongoing develop-
ments of DRL and LGP in ﬂow control applications open up
relevant questions on their applicability in experimental en-
vironments, where the number of sensors is limited and data
are likely to be corrupted by noise. Furthermore, the general-
ization of the identiﬁed policies is often hindered by the chal-
lenges of interpreting the control laws. This work sheds light
on the main features of Deep Reinforcement Learning as Lin-
ear Genetic Programming Control (LGPC) in this direction.
To the author’s best knowledge, only the recent contribution
by Pino et al. 34 performs a comparative assessment of ma-
chine learning methods for ﬂow control. The work focuses on
the comparison of the relation of DRL and LGPC with optimal
control for a reduced set of sensors. The robustness to noise
and the effect of initial condition of such algorithms is not dis-
cussed. The present study aims to address the performance of
DRL and LGPC in the simple scenario of the control of the 2D
shedding wake of a cylinder at a low Reynolds number in the
conditions of a limited number of sensors. The robustness of
both processes to noise contamination on the sensor data and
variable initial conditions for training individuals is assessed.
Finally, an interpretation of the control actions using a cluster-
based technique is provided. For this purpose, the same DRL
framework and simulation environment used by Rabault and
Kuhnle 21 is considered, and compared against the LGPC en-
vironment developed by Li et al. 20 . It is to be noted that this
contribution is a proof of concept. The simulation environ-
ment proposed by Rabault and Kuhnle 21 was chosen given
its simplicity and affordability for extensive analysis as herein
presented. Nonetheless, the application of akin algorithms to
similar environments, though with more challenging condi-
tions, was investigated by Ren, Rabault, and Tang 35 , achiev-
ing a robust control in the ﬂow past the conﬁned cylinder at
multiple Reynolds numbers, concluding that the drag reduc-
tion increases with the Reynolds number. Additionally, Ren,
Rabault, and Tang 36 exploits the framework by Rabault and
Kuhnle 21 at weak turbulent conditions (Re = 1000) with a
drag reduction of 30%. Nonetheless, in these studies, 236 and
151 probes are considered respectively, and the robustness to
noise is not explored.

The present article is structured as follows: Section II de-
ﬁnes the main methodologies applied to implement the differ-
ent machine learning models, the simulation environment and
the problem description. Results are collected and described
in Section III while the interpretation of the achieved controls
and their performance is outlined in Section IV. Ultimately,
the conclusions of the study are drawn in Section V.

II. METHODOLOGY

This Section is focused on the main methodologies to de-
ﬁne the environments in which the Machine Learning models
will be tested, as well as the conditions imposed. Although the
methodologies to be compared present important differences,
they share several fundamental characteristics that must be

Machine learning ﬂow control with few sensor feedback and measurement noise

3

FIG. 1: Description of the numerical setup adapted from Rabault et al. 26 . (a) Sketch of the numerical domain and the
non-structured mesh, deﬁning the diameter D = 1, length L = 20, and height H = 4.1. The sensor arrangement is shown for (b)
5 and (c) 11 probes, being probes identiﬁed as ( ) and actuators as ( ). (d) Detail of the cylinder and the jet actuators deﬁnition.

highlighted before delving into each of the independent meth-
ods.

A. Simulation Environment

The active drag reduction is performed in a 2D Direct Nu-
merical Simulation (DNS) environment that builds upon that
of Rabault et al. 26 , differing only for the sensor strategy. The
environment is described here for completeness.

The geometry of the simulation, adapted from state of the
art benchmarks37, consists of a cylinder of diameter D im-
mersed in a box of total length L = 22D (along the x-axis) and
height H = 4.1D (along the y-axis) as shown in ﬁgure 1(a).
The inﬂow velocity (on the left wall of the domain) is mod-
elled as a parabolic proﬁle so that the mean velocity magni-
tude results in U. A no-slip boundary condition is imposed
on the top and bottom walls of the channel, and also on the
solid cylinder walls. An outﬂow Dirichlet boundary condition
is imposed on the right wall of the domain. The Reynolds
number based on the mean velocity magnitude and cylinder
diameter (Re = UD
ν , with ν the kinematic viscosity) is set
to Re = 100. This choice is based on the previous work by
Rabault et al. 26 which has been a pioneer application of DRL
to ﬂow control and the baseline to compare within this study.
Working with higher Reynolds numbers would have implied a
tremendous increase in computational cost, making unafford-
able all the set of simulations required for changing the initial
conditions and assessing the noise robustness that will be dis-
cussed in the following.

The control action is performed by two jets (1 and 2) con-
trolled through a non-dimensional mass ﬂow rate Qjet by
imposing a parabolic velocity proﬁle with the jet width of
ω = 10◦. The jets are perpendicular to the cylinder wall and
located at angles θ1 = 90◦ and θ2 = 270◦ relative to the ﬂow

direction as shown in ﬁgure 1(d), what guarantees that all the
promoted drag reduction is the result of indirect ﬂow con-
trol, rather than direct injection of momentum26 To prevent
numerical instability while presenting a more realistic sce-
nario, the total mass ﬂow rate injected by the jets is zero,
i.e. Qjet1 = −Qjet2. Note, however, that the cylinder does not
present physical cavities on its surface, meaning that there is
no physical interference of the jet slot with the ﬂow ﬁeld.

The simulation environment is based on the open-source
ﬁnite-element framework FEniCS38 version 2017.2.0, solv-
ing the unsteady Navier–Stokes equations equations by DNS.
Computations are performed on an unstructured mesh gen-
erated with Gmsh39. The mesh is reﬁned around the cylin-
der and is composed of 9262 triangular elements (see ﬁg-
ure 1(a)). A non-dimensional, constant numerical time step
dt = 5 × 10−3 is used. The CFL condition is enforced in the
problematic zones, that is, close to the actuation jets, by im-
posing a maximum jet mass ﬂow rate (|Qjet| < Qjetmax).

The ﬂow control framework developed by Rabault et al. 26
was conceived to work either with velocity or pressure probes
as sensing. Since pressure probes are often more difﬁcult to
be installed in a customary location for an experimental ap-
plication, it was preferred to chose the velocity probes which
resembles what could be extracted from hot wire anemome-
try (HWA). Two sensor conﬁgurations are considered in the
present study with 5 and 11 probes which report the local
value of the horizontal and vertical components of the veloc-
ity ﬁeld (see ﬁgure 1(b) and 1(c), respectively). The probes
are located in the wake of the cylinder to enable the controller
to learn from the vortex shedding pattern.

657981032(b)(a)(c)(d)xyxyθωDH01432014LMachine learning ﬂow control with few sensor feedback and measurement noise

4

FIG. 2: Implementation of the control algorithms. (a) Sketch of the closed-loop control based on Deep Reinforcement
Learning. (b) Sketch of the closed-loop control based on Linear genetic Programming.

B. Formulation of the optimization problem

The drag reduction of a 2D cylinder wake ﬂow is a classi-
cal optimization problem with a simple target, i.e. reducing
drag. The control problem is formulated as a regression prob-
lem, i.e. to ﬁnd the control law which optimizes a given cost
function J7. The proposed cost function has been shaped as
a combination of drag and lift coefﬁcients, modifying the one
proposed by Rabault et al. 26 as follows:

J = 1 + (cid:104)CD(cid:105)T − (cid:10)CD0

(cid:11)
T + 0.2 |(cid:104)CL(cid:105)T |

(1)

where (cid:104)CD0(cid:105)T = 3.206 is the drag coefﬁcient of the un-
forced ﬂow, and (cid:104)·(cid:105)T indicates the sliding average back in time
over a duration corresponding to one vortex shedding cycle T
of the unforced vortex shedding ﬂow.

The cost function in equation 1 has shown to be better than
using just the instantaneous drag coefﬁcient, i.e. J(t) = CD(t).
First, the average values of the lift and drag coefﬁcients over
one vortex shedding cycle reduce the oscillations of the ob-
jective function, which has been reported to improve learn-
ing speed and stability26. Secondly, the drag reduction is
deﬁned as the increment with respect to the unforced ﬂow
(cid:11)
(∆CD = (cid:104)CD(cid:105)T − (cid:10)CD0
T ), which is intended to be as nega-
tive as possible. Thirdly, a penalization term based on the lift
coefﬁcient is considered to prevent the controller from ﬁnding
undesired asymmetric solutions26. Finally, adding the unity to

the cost function provides a bias to prevent J negative values
that could affect the convergence of the learning process. Ul-
timately, the algorithm would try to minimise J, by reducing
drag while maintaining low lift components.

From an optimization perspective, the resultant control is
the strategy that minimizes the cost function with a control law
b(t) = K(s(t)), where b(t) = (b1(t), ..., bNb(t))T comprises Nb
actuation commands, and s(t) = (s1(t), ..., sNs(t))T consist of
Ns sensor signals. In this study, the actuation b is performed
with two jets that are related due to the net zero ﬂux condition
(Nb = 1) at the bottom and top sides of the cylinder, using
velocity probes (Ns = 5 or 11) as sensing. The control problem
is equivalent to ﬁnding K∗ such that

K∗(s) = arg min

J(K(s))

(2)

K
The optimized feedback K∗ is computed following LGPC
and DRL, as described in the following sections. The resul-
tant control laws map Ns sensor signals into Nb actuation com-
mands. The resultant control action is subjected to a smooth-
ing operation to ensure continuous control signals without
abrupt alterations in the pressure and velocity due to the use of
an incompressible solver. The control action is then adjusted
from one-time step in the simulation to the next by

Qjet(t) = Qjet(t − dt) + α (cid:2)b(t) − Qjet(t − dt)(cid:3) ,
being α = 0.1 a numerical parameter, Qjet(t) the jet actuation
used by the plant at time instant t, and b(t) the actuation pro-
posed by the machine learning model at time instant t.

(3)

134Nind-1Nind2Replicationb(s) = K(s)MutationCrossoverNind-1NindElitismSort Individualsnth generation(n+1)th generationNumerical PlantNumerical PlantControl LawJIndividualsJJSensor(s)KiIndividualsTournamentAction Q_jReward (r)Action (b)AgentSensors/State (s)(a)(b)1342Machine learning ﬂow control with few sensor feedback and measurement noise

5

C. Deep Reinforcement Learning

Deep Reinforcement Learning (DRL) is an ML control al-
gorithm in which an agent (managed by an ANN) learns the
best control by interacting with the environment to exchange
information in a closed-loop process (see ﬁgure 3(b)). In the
present work, the plant or environment is the above-mentioned
simulation environment, which interacts with the agent by
the observation or sensor state, s (Ns point
three channels:
measurements of velocity); the action, b (Nb values of mass
ﬂow rate to impose on the jets); and the reward, r (the cost
function in equation 1 based on CD and CL, i.e. r = J). Based
on the sensing data and the reward of the current state, DRL
trains an ANN to ﬁnd the optimized closed-loop control strate-
gies that maximize the expected reward.

The DRL framework is the same as in Rabault et al. 26
in which the agent uses the proximal policy optimization
(PPO) method40 for performing learning. The PPO method
is episode-based, so that it iteratively learns by applying a
certain control for a limited amount of time (episode dura-
tion) before analysing the rewards and sensors, and resuming
learning with a new episode. The considered architecture for
the ANN is relatively simple, being composed of two dense
layers of 512 fully-connected neurons, the input layer to ac-
quire data from the probes, and the output layer to generate
data for the two jets. For more details, readers are referred
to Rabault et al. 26 . The training loop of DRL is sketched in
Figure 2(a).

At the beginning of the learning process, the PPO explores
purely random controls to assess the values of the reward
function. This initial approach implies difﬁculties to learn the
necessity to set time-correlated, continuous control signals.
To solve this issue, Rabault et al. 26 implemented the agent
such that the control value provided by the network is kept
constant for a duration of 50 numerical time steps. Therefore,
the PPO agent interacts and updates the ANN coefﬁcients only
every 50 time steps, which is the duration of a ﬁxed actuation.
This numerical trick together with the smoothing described in
equation 3 provides a continuous control signal.

D. Linear Genetic Programming

Linear Genetic Programming41 is an evolutionary algo-
rithm, that applies biological-inspired operations to select the
ﬁttest individuals for a given purpose. The control laws are ef-
fectively mappings between the outputs (sensor information)
and the inputs (actuation) of a dynamical system. In the fol-
lowing, the control laws are also referred to as individuals to
comply with the evolutionary terminology. LGP is able to
learn control laws in a model-free meaner, optimizing both
the structure of the function and its parameters. In practice,
the control laws are internally represented by a matrix encod-
ing a list of instructions. Each row of the matrix codes for a
mathematical operation from a set of input registers, constants
and operations and stores the result in a memory register. The
matrix is then read linearly modifying sequentially the mem-
ory registers, hence the name of the method. The control law

is then read in the ﬁrst register. LGPC is here selected as the
preferred option for its simpler implementation of the genetic
operators for Multiple-Input-Multiple-Output control42.

The learning process, sketched in Figure 2(b), is divided
into an outer loop devoted to evolving the generations and an
inner loop to evaluate all the individuals in a real-time control
process. First, an initial population of individuals is randomly
generated and evaluated with a Monte-Carlo optimization to
explore the control law space. We recall that the individuals
are analytical functions of the input data, i.e. the velocity sen-
sor signals. A measure of the performance of each individual
is given by its cost J (equation 1). Once the entire popula-
tion is evaluated, the next generation of individuals is created
with genetic operators (crossover, mutation, replication) ap-
plied to the most performing individuals. The best individuals
are selected with the tournament selection method. Crossover
combines two individuals and generates a new pair of indi-
viduals by exchanging randomly their instructions. This op-
eration contributes to the exploitation of the learned data by
recombining well-performing individuals. The mutation oper-
ation modiﬁes randomly elements of one given control law to
explore potentially new and better minima. Replication is the
memory operator. It assures that good structures are not lost in
the evolution process. Finally, an elitism operation saves the
best individuals of one generation to the next, ensuring that the
performance does not degrade after each generation. The ge-
netic operators (crossover, mutation and replication) are cho-
sen following respective probabilities (Pc, Pm, Pr). The process
is repeated for every new generation until the stopping crite-
rion is met or if the termination is triggered. In this study, all
the training processes have been performed for a ﬁxed number
of generations Ng = 15 as explained later.

Among the wide variety of custom settings when dealing
with LGPC, the most relevant parameters for proper perfor-
mance and convergence of the algorithms are the popula-
tion size, the number of generations, the tournament selec-
tion size and the genetic operators’ probability (Pc, Pm, Pr).
LGPC parameters are chosen following the recommendations
of Duriez, Brunton, and Noack 7 and Li et al. 20. They are
summarized in table I

Number of controllers
Number of sensors
Population size
Number of generations
Tournament selection size
Crossover probability
Mutation probability
Replication probability
Elitism
Operations

1
5,11
100
15
7
0.6
0.3
0.1
1
+, −, ×, ÷, sin, cos, tanh

TABLE I: Selection of LGPC parameters

An important difference between the DRL and the LGPC
algorithm is related to their learning process during an
episode. In DRL, the agent builds its internal representation
of how the ﬂow in a given state will be affected by actuation,
and how this will affect the reward value. This is done glob-

Machine learning ﬂow control with few sensor feedback and measurement noise

6

ally at the end of the episode and also each time the actuation
changes. The agent is modiﬁed according to the expected re-
ward (total or partial, respectively), which is not an immediate
value just after the actuation, but also after the medium/long-
term reward. This means that the neural network on which the
DRL is capable of learning both from the committed errors
but also from the future effect associated with each actuation.
In LGPC, there is no chance of the actuation law during the
simulation run, as it corresponds to a single individual map-
ping from the input to the outputs. It is therefore interesting
considering the incorporation of time-delayed sensor signals.
Following Cornejo Maceda et al. 12, the values of the probes
at 1/4, 1/2 and 3/4 of the shedding period are included, as
well as the instantaneous value. Assuming a periodic ﬂow,
the addition of such time delays enables the reconstruction of
the ﬂow phase.

E. Training standards

The training process of both DRL and LGPC is performed
with the same parameters, which were chosen based on the
recommendations by Rabault et al. 26 and extensive empirical
analysis. The duration of the simulation (or episode duration,
according to DRL nomenclature) is set to Tsim = 20.0, which
translates into approximately 6 vortex shedding periods, and
corresponds to 4000 numerical time steps. Note, however,
that the cost function in equation 1 is evaluated for the last
shedding period (650 numerical time steps) in which a new
steady state is expected to be reached upon actuation.

Regarding the action, the DRL agent adjusts the policy ev-
ery 50 numerical time steps, which means that the control is
updated 80 times during the episode. The transition from the
current action to the updated following action is smooth and
continuous based on the smoothing described in equation 3.
On the other hand, LGPC provides analytical control laws that
are continuous and fully dependent on the sensing data, which
means that the control action is adjusted every numerical time
step mildly thanks to the smoothing.

Given the intrinsic differences between LGPC and DRL,
it is required to set the training standard to guarantee a fair
comparison. The followed criterion is to keep the same com-
putational time during the training process. The PPO agent is
able to learn a fully-stabilized control after approximately 400
epochs (corresponding to 32000 sample actions); however, the
convergence rate is lower when applying noise to the probes.
On the other hand, for LGPC with a pool size of 100 individu-
als per generation, a converged control law is achieved before
the 10th generation both with and without noise consideration.
Both for LGPC and DRL, it is straightforward to assess that
the main computational cost comes from the plant, i.e. from
the ﬂuid mechanic simulation of the 2D cylinder wake. The
evaluation of the sensing, update of the agent or generation of
new individuals are operations with negligible time consump-
tion. Based on these ﬁgures of merit, it was decided to set
the training duration to 1500 episodes in the case of DRL and
15 generations of 100 individuals in the case of LGPC. This
common criterion guarantees that the computational effort is

FIG. 3: Control interpretation methodology. The sensor
vector is built from the vertical and horizontal components of
the measured velocity. The elements of the sensor vector are
grouped in clusters (three clusters (1,2,3) are presented here
for clarity). High probability transitions are depicted with
darker colours. γ1 and γ2 deﬁne the projection plane for the
proximity map. The actuation magnitude is represented by
rectangles in the network model; red (blue) for a positive
(negative) actuation with respect to the mean value. See the
text for more details.

the same for both algorithms since a total of 1500 simulations
are launched for each method.

For the investigation of robustness to noise, a perturbation
with Gaussian distribution is added to the probes such that the
input used by the DRL agent or the LGPC control laws are
altered. The noise implementation is the following,

ui(t) = ui(t) + ε · (cid:122)i(t)

∀ i = [1, 2, ..., Nb],

(4)

being ui(t) the velocity value, (cid:122)i(t) a random normally
distributed value for the probe i at time instant t, and ε the
noise level or intensity. Three noise levels are considered, i.e.
1%, 5% and 10% of the freestream velocity. On the contrary,
the mean state quantities used to compute the cost function
(i.e., CD and CL) are not altered by noise since the averaging
operator in the cost function would make the noise of minor
relevance.

F. Control law visualization

A cluster-based interpretation method is considered to have
an insight into the actuation mechanisms involved in the con-

sssssMachine learning ﬂow control with few sensor feedback and measurement noise

7

trol learned by DRL and LGPC. Cluster-based methods have
been recently applied to build network models able to repro-
duce the main characteristics of ﬂuid ﬂows and dynamical sys-
tems, such as temporal evolution and ﬂuctuation levels43,44.

Understanding the relationship between the control inputs
and the corresponding actuation command is not an easy task,
especially with a large number of inputs. Thus, clustering
is employed to extract representative states from the sensors.
Figure 3 summarizes the main steps of the control interpreta-
tion methodology described below. For this study, the metric
employed for the cluster analysis is the one induced by the
L2 norm. The sensor time series are reduced to 10 clusters.
For each cluster, its centroid is computed as the average of all
states in the cluster. Then, the 10 centroids represent the main
states of the ﬂow. Moreover, the transition information from
one ﬂow state to another is gathered in a probability transition
matrix (P = [pi, j]i, j) that translates the probability to jump
from one cluster to another. The probability transition from
cluster i to cluster j is deﬁned by pi, j = ni, j/ni, being ni, j the
number of states from cluster i that transition to cluster j and
ni the total number of states in cluster i.

The centroids combined with the transition matrix allow for
building a network model of the controlled ﬂow based only on
the controller inputs. On the other hand, the sensor time se-
ries are projected on a 2D proximity map with classical multi-
dimensional scaling45–47 (MDS). MDS is a powerful tool for
dimensionality reduction, that projects the data in the two di-
rections (γ1,γ2) of maximum dispersion of the feature vector
distance matrix. The distance matrix is computed with the
same metric as the clustering. Combining the network model
and the proximity map allows to have a reconstruction of the
ﬂow phase space. Finally, the average actuation performed in
the cluster is associated with each centroid. The resulting 2D
visualization represents the dynamics of the ﬂow alongside
the actuation performed allowing an easy interpretation of the
control actuation mechanism.

III. PERFORMANCE ANALYSIS

In this section, the performances of reinforcement learning
and linear genetic programming are analyzed. In each training
episode, the initial condition is randomly selected, thus repli-
cating an experimental scenario, in which full control of the
initial condition to start the actuation is difﬁcult to achieve.
The particular case where the starting trigger can be set corre-
sponding to a speciﬁc case is included in Appendix A.

A. Controller in the absence of noise

DRL and LGPC are ﬁrst trained on clean data, i.e. in ab-
sence of noise. It is important to remark that the performance
of each selected actuation depends on the corresponding ﬂow
condition when the actuation is started, i.e. the same control
law (or weight distribution for the ANN) determine different
performances if run under different initial condition.

The probability distribution function (PDF) of the drag co-
efﬁcient for the ﬁnal selected actuation after training is illus-
trated in Figure 4(a) for the case of the DRL, and in Figure
4(c)c for LGPC, both considering 5 and 11 probes. These dis-
tributions are obtained analyzing the CD value obtained run-
ning the simulation with 55 equispaced initial phases over the
whole unperturbed shedding cycle and averaging the last 650
simulations steps, corresponding to 3.25t∗ (being t∗ = tU∞/D
and U∞ the freestream velocity), i.e. a shedding period of the
unforced conﬁguration. This is the same time interval adopted
for the computation of the cost function J.

The PDF demonstrates that the DRL is less sensitive to the
effect of the initial condition if compared to LGPC. This re-
sult is not surprising, considering that the agent of the DRL is
more complex than the control laws obtained through LGPC
(see Table III), thus it is potentially more ﬂexible to variable
initial conditions. While this is a desirable aspect of DRL, on
the downside it comes at the expense of a less interpretable
control policy. A bit more surprisingly, DRL shows a degra-
dation in performance when passing from 11 to 151 probes,
which is the case evaluated by Rabault et al. 26. As an hy-
pothesis, this might be due to the non-sufﬁciently complex ar-
chitecture of the ANN since it is common despite the sensing
conﬁguration. The input probe number is an order of mag-
nitude larger for 151 probes, thus possibly requiring a more
powerful network for the agent and more intense training.
Nonetheless, it can be concluded that increasing the number
of probes to a reasonable extent seems to reduce the variabil-
ity of the drag coefﬁcient, thus delivering reliable and robust
action against variable initial conditions. This is also observed
in Figure 4b, where the envelope of the drag coefﬁcient his-
tory in the set of analyzed initial conditions is presented. The
shaded region is centred on the mean drag coefﬁcient at each
instant, and the half-width is set to one standard deviation of
the drag coefﬁcient. The mean values corresponding to such
initial conditions are reported in Table II.

Observing the results for LGPC, it can be observed that it
beneﬁts mainly in terms of average drag coefﬁcient when in-
creasing the probe number (see Figure 4d), although there is
no signiﬁcant impact on the dispersion of the drag coefﬁcient.
The time history of the cost function J, the drag and lift
coefﬁcients CD,CL and the ﬂow rate of the actuator Q j, are re-
ported in Figure 5 for the ﬁnal selected actuation after training
of DRL and LGPC. The initial condition is selected among the
50 tested cases like the one resulting in a ﬁnal drag coefﬁcient
closest to the mean value. The results are presented for 5 and
11 probes, including the case without actuation as a reference.
In the limit of low probe number (5 probes), LGPC is ob-
served to have slightly superior performance than DRL in
terms of CD. The average lift coefﬁcient in the ﬁnal phases of
the observation horizon is in both cases weakly negative (i.e.
LGPC and DRL converge to an asymmetric ﬂow conﬁgura-
tion determined by the control action). This effect is slightly
more signiﬁcant for LGPC, thus showing that the actuation
is also aiming to alter the ﬂow symmetry to reduce drag. In
terms of the actuation ﬂow rate, DRL converges to a substan-
tially lower standard deviation of the ﬂow rate of a single jet
(used here as a parameter, since the net mass ﬂow rate is zero),

Machine learning ﬂow control with few sensor feedback and measurement noise

8

Algoritm

Probes

Noise

– No Control –

DRL
DRL
DRL
DRL
DRL

LGPC
LGPC
LGPC
LGPC
LGPC

5
11
11
11
11

5
11
11
11
11

0%
0%
1%
5%
10%

0%
0%
1%
5%
10%

(cid:104)J(cid:105)

1.160

0.992
0.807
0.808
0.811
0.843

0.966
0.846
0.797
0.984
0.901

(cid:104)CD(cid:105)

3.206

3.085
2.958
2.961
2.961
2.982

3.065
2.954
2.946
2.997
2.984

(cid:104)CL(cid:105)

0.022

-0.207
0.138
-0.095
0.001
0.043

-0.271
0.058
-0.129
0.233
0.155

std(Q jet )(×104)
0

5.744
6.526
6.3848
8.828
10.382

10.225
20.285
12.912
24.446
12.246

TABLE II: Summary of results.

Probes

Noise

5

11

11

11

11

0%

0%

1%

5%

10%

sin (cid:0)tanh (cid:0)(cid:0)tanh (cid:0)(cid:0)(cid:0)(cid:0)u4

(cid:1) + v0
(cid:0)t − 3T
4
(cid:0)t − 3T
4
(cid:1)(cid:1)(cid:1)(cid:1) − v2

(cid:1) + v0
(cid:0)t − 3T
4

(cid:0)(cid:0)cos (cid:0)(cid:0)(cid:0)u4
(cid:0)t − 3T
v3
4

Control Law (Q jet × 102)
(cid:1)(cid:1) + (cid:0)(cid:0)v0 + (cid:0)u2 − u4
(cid:0)t − T
2
(cid:0)t − T
2

(cid:0)t − T
2
(cid:1)(cid:1) + (cid:0)(cid:0)v0 − (cid:0)0 − (cid:0)u2 − u4
(cid:1) − u4

(cid:1)(cid:1)(cid:1) · v3
(cid:0)t − T
2
(cid:0)t − 3T
4

(cid:1)(cid:1)(cid:1) + v[1](cid:1)(cid:1) ·
(cid:0)t − 3T
4
(cid:1)(cid:1)(cid:1)(cid:1)
·
(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)

(cid:0)t − T
2
(cid:1)(cid:1)

(cid:1)(cid:1) + (cid:0)v[2] + (cid:0)u4
(cid:0)t − 3T
v6 + (cid:0)v2 − v2
4
tanh (sin (sin (v6))) − cos (cid:0)u7
tanh (tanh (v6)) + v2

(cid:0)t − T
4

(cid:1)(cid:1)

0.72965 · (cid:0)sin (cid:0)u4

(cid:0)t − T
4

(cid:1)(cid:1) − v4

(cid:0)t − T
4

(cid:1)(cid:1) · (tanh (v7) + sin (v6))

TABLE III: Control laws from LGPC.

thus meaning that it requires less power consumption for the
actuation.

B. Robustness to measurement noise

The differences are more signiﬁcant for the case with 11
probes. The actuation obtained with DRL features a faster
convergence to the asymptotic drag and lift coefﬁcient, with
minimal ﬂuctuations of the latter. This is achieved with strong
action in the initial phase, which rapidly damps to a signiﬁ-
cantly lower intensity to counteract the triggering of the shed-
ding. The actuation identiﬁed by LGCP, on the other hand,
needs a signiﬁcantly longer time to converge. The perfor-
mances in terms of ﬁnal drag coefﬁcient are similar to DRL,
although the oscillations of the lift coefﬁcient and the ﬂow
rate of the actuators are more signiﬁcant. It is nonetheless re-
markable the simplicity of the obtained control law (see Table
III). For the case of 11 probes with a random initial condition,
LGPC converges to a law that involves only two probes, both
using the crosswise velocity component, located in the mid-
dle and on one side of the cylinder (see Figure 1 for probe
numbering). Remarkably, LGPC is able to identify the ﬂow
symmetry and address time-delayed feedback (for probe 2 the
control law also includes a time-delayed signal with a delay of
3/4 of the period). While the identiﬁed control law seems less
robust to the effect of the initial conditions, it leads to identi-
fying a subset of probes that is sufﬁcient to obtain effective
control.

In this section, the robustness of DRL and LGPC in pres-
ence of noise is addressed. Additive noise with Gaussian dis-
tribution is included in the sensor data. Three noise levels are
investigated, i.e. 1%, 5% and 10% of the freestream velocity.
Similarly to the §III A, the ﬁnal actuation policy obtained
after training is tested under a range of initial conditions for
the case of 11 probes. The PDF of the drag coefﬁcient, as well
as its time evolution and the corresponding dispersion for dif-
ferent initial conditions, are illustrated in Figure 6a,b. For the
DRL, the scatter around the mean drag coefﬁcient is not sig-
niﬁcantly affected in the initial steps of the actuation (t∗ < 5),
in which the actuation is aiming to displace the ﬂow conﬁg-
uration from one limit cycle to another. For larger times, the
dispersion around the mean drag coefﬁcient seems to increase
with the noise level, as expected. The ﬁnal achieved perfor-
mance shows only a minor degradation for noise levels up to
5%, while the penalty becomes more signiﬁcant at 10% noise
level. Figure 7 reports the evolution with time of the cost func-
tion, drag and lift coefﬁcients and actuator ﬂow rate for differ-
ent noise levels. The initial condition is chosen according to
the PDF of the drag coefﬁcient. It is set to be the one yield-
ing the drag coefﬁcient most similar to the mean. It can be
observed that the agent is capable to obtain in all tested cases
a drag coefﬁcient with relatively small ﬂuctuations, while the
lift coefﬁcient experiences larger variations with the increas-
ing noise level. This is expected since the lift coefﬁcient is

Machine learning ﬂow control with few sensor feedback and measurement noise

9

FIG. 4: Inﬂuence of the initial condition on the control performance. Results are shown for DRL (a-b) and LGPC (c-d). The
probability density function (a,c) and the CD envelope (b,d) are computed from the CD proﬁles extracted for 55 equispaced

initial phases over the whole unperturbed shedding cycle. Distributions are shown for 5 (

) and 11 (

) probes.

directly affected by the asymmetry introduced by the actua-
tion, whose ﬂow rate is directly related by the agent to the
signal recorded by the probes. The drag coefﬁcient, on the
other hand, is related to the wake conﬁguration and thus the
effect is partially damped. Interestingly enough, the results re-
ported in Table II also show that the average lift coefﬁcient is
close to zero, i.e. the noise has the effect of avoiding the agent
tricking the policy to achieve drag reduction by introducing
asymmetries.

For the case of LGPC, the effect of noise appears more sig-
niﬁcant, as illustrated in Figure 6c,d. According to the re-
sults in Table II, the drag coefﬁcient is quite signiﬁcantly af-
fected by increasing noise. Interestingly, for the noise level
of 1% and 5%, the obtained control law is relatively simple
and still identiﬁes that 2 probes are sufﬁcient to perform an
effective control action. In particular, probe 6 and an off-axis
probe (either 2 or 7) are selected in both cases. The history
of the cost function, force coefﬁcients and actuator ﬂow rate
are also presented in Figure 7. It is indeed conﬁrmed that, for
the case of low noise, the optimization successfully reduce the
drag coefﬁcient and the oscillations of the lift coefﬁcient, with
even more satisfying results than for the case without noise.
For larger noise levels, the control law identiﬁed by LGPC is
not capable of reducing the oscillations of the lift coefﬁcients,
thus inevitably affecting also the share of drag coefﬁcient as-
cribed to vortex shedding.

A direct undesirable consequence of the simplicity of the
control law is that, with the increasing noise level, the action

is less effective. The different effects of noise between DRL
and LGPC can be addressed on one side by the different com-
plexity of the policy, and on the other side by the procedure
to determine the action. As described in §II, the action se-
lected by the DRL agent is obtained by weighting the current
ANN output with the previous step control action, thus intro-
ducing a certain soothing effect. It can be speculated that low-
pass ﬁltering of the probe signal could improve LGPC per-
formance. Nonetheless, for fairness of comparison, we main-
tained the same implementation of LGPC presented originally
by Li et al. 20, and leave this as an object for future study.

The robustness study against noise for the 5 probes con-
ﬁguration leads to similar conclusions as for the 11 probes
case. Hence, for the sake of brevity, it is not included. With a
smaller number of probes, a higher noise is observed, as ﬁlter-
ing out noise becomes more difﬁcult. The probes in the wake
of the cylinder have shown to be the most relevant for feed-
back because the vortex shedding is more pronounced and the
signal-to-noise ratio is better. Intriguingly, LGPC has shown
to have lower performance degradation than DRL when using
only 5 probes. This is not surprising, since all the control laws
extracted with LGPC in the 11 probe case lead to parsimo-
nious use of probes, rarely exceeding 3 or 4 sensors.

DRLLGPCMachine learning ﬂow control with few sensor feedback and measurement noise

10

FIG. 5: Evolution of J, CD, CL, and Q j upon the actuation of DRL (

), and LGPC (

) for 5 and 11 probes. Unforced case

(

) shown for reference.

IV. CONTROL RESULTS AND DISCUSSION

In this section, the controls achieved by DRL and LGPC

are analysed with the clustering method described in § II F.

For all cases the transient has been excluded by the analysis

by removing the ﬁrst 3000 time steps, corresponding to 15t∗.
It must be remarked, however, that for the LGPC cases with
0% and 1%-level noise, the transient lasts for more than the
observed 60 time units.

Figure 8 shows the cluster-based interpretation methodol-
ogy applied to the laws/policies learned by DRL and LGPC

0.740.881.021.161.3J5probes11probes2.933.013.093.173.25CD-2.4-1.201.22.4CL0102030405060t$-90-4504590Qj(#104)0102030405060t$Machine learning ﬂow control with few sensor feedback and measurement noise

11

FIG. 6: Inﬂuence of the initial condition on the control performance in the presence of noise. Results are shown for DRL (a-b)
and LGPC (c-d). The probability density function (a,c) and the CD envelope (b,d) are computed from the CD proﬁles extracted
for 55 equispaced initial phases over the whole unperturbed shedding cycle. Distributions are shown for 1% (
), and
10% (

) noise level.

), 5% (

for the 5-sensor cases without noise. The interpretation
methodology is applied independently to each case. First, note
that the proximity maps of the sensor time series are reminis-
cent of phase space. Such visualization is not surprising as
the controlled ﬂow remains mainly periodic and therefore can
be represented in a 2D space. The transition matrices dis-
play high probabilities for the self-transitions (diagonal val-
ues). This is due to the high sampling rate that excessively
populates each centroid. The transition states are then under-
represented, hence the low values of the transition probabil-
ities outside the diagonal. Thus, the sensor time series have
been subsampled to 1/5th of the data to artiﬁcially increase
the transition probabilities located outside the diagonal. This
operation is done to render the transition matrix easier to read
while it does not change either the results or the interpreta-
tions. For the DRL control case, note that for each cluster
there is only one arrival cluster. This is translated in the con-
trol network by a cycle that can be interpreted as a limit cycle
in the phase space. For the LGPC case, a limit cycle can also
be inferred from the data. However, two centroids close to the
centre of the limit cycle (centroids 7 and 8) play a role in short-
circuiting the limit cycle. Concerning the control achieved, in
both cases, the plane is divided into two regions: one of posi-
tive actuation including half the centroids and one of negative
actuation including the other half. Furthermore, note that the
actuation level increases with the distance to the dividing line.
Such organization of the actuation around a limit cycle indi-

cates that the actuation mechanism employed for drag reduc-
tion is phasor control, meaning that the control depends on the
ﬂow phase.

In addition to this visualization, an analysis combining the
two dynamics (not included here for brevity) is performed.
For this, the time series have been appended and 20 centroids
have been chosen for the description of the ﬂow. The resulting
proximity map shows the two limit cycles on the same plane,
almost concentric, revealing that the controlled ﬂows are close
dynamically. However, the probability transition matrix loses
the dynamic relationship between the centroids as some clus-
ters contain states from both learning strategies.

Figure 9, displays the control network for both the number
of sensors (5 and 11). Note that the overall shape and relations
between the centroids remain the same when going from 5
sensors to 11 sensors. Again, the main limit cycle is divided
into two regions of positive and negative actuation revealing,
again, a phasor control mechanism.

Finally, the impact of noise on the actuation mechanism
is investigated. Figure 10 displays time series and the cen-
troids projected in the proximity map for the controlled ﬂows
with increasing noise. First, note that going from 0% to 1%,
the noise disturbs the smooth distribution of data. For DRL,
as the noise level increases the clusters starts to overlap so
much that for the 10%-level noise case, separating visually
the clusters becomes impossible on the proximity map. The
resulting control network, not plotted because of its complex-

DRLLGPCMachine learning ﬂow control with few sensor feedback and measurement noise

12

FIG. 7: Evolution of J, CD, CL, and Q j upon the actuation of the DRL (
5% and 10% noise levels. Unforced case (

), and LGPC (
) shown for reference.

) controller in the presence of 1%,

ity, shows that for the post-transient regime the dynamics are
mainly driven by the noise. Nonetheless, note that the cen-
troids are distributed around an ellipsoid, indicating a period-
icity in the ﬂow. As for the LGPC cases, the proximity maps
describe successfully the controlled dynamics. For 0% and
1%-noise level, the centroids describe a spiral which is consis-
tent with the long transients (see corresponding lift coefﬁcient
CL in ﬁgures 7 and 6). For the 5%-noise level, the persistent
high-amplitude oscillations plotted in ﬁgure 7 are represented
by a clear limit cycle in the proximity map and for the 10%-
noise level, the centroids describe a cycle blurred by the noise
which is consistent with the low-amplitude oscillations of the
lift coefﬁcient in the post-transient regime (ﬁgure 7). In sum-
mary, in all the cases, the ﬂow includes a periodic behaviour.
Regarding the actuation command, all the proximity maps can
be separated into two regions, one of positive actuation and
negative actuation, indicating that both DRL and LGPC man-
aged to learn a phasor control regardless of the noise level.

V. CONCLUSIONS

Two machine-learning-based strategies which minimize the
drag of a cylinder exhibiting vortex shedding wake are evalu-
ated. The performance of Deep Reinforcement Learning and
Linear Genetic Programming Control in identifying effective
control policies has been assessed in the realistic conditions
of a limited number of sensors and noise contamination of the
sensed data. The training is performed using random initial
conditions, in the attempt to reproduce an experimental sce-
nario in which it is not feasible to control the full ﬂow state
when the actuation is started.

It is observed that, in absence of noise, the average per-
formance achieved by DRL and LGPC is similar in terms of
average lift and drag coefﬁcient, although with a signiﬁcant
advantage of the DRL in terms of the standard deviation of the
mass ﬂow rate of the actuation. It must be remarked, however,
that the amount of power needed to control has not been in-

0.740.881.021.161.3J1%5%10%2.933.013.093.173.25CD-2.4-1.201.22.4CL0102030405060t$-90-4504590Qj(#104)0102030405060t$0102030405060t$Machine learning ﬂow control with few sensor feedback and measurement noise

13

FIG. 8: Visualization of the laws/policies learned by DRL (top) and LGPC (bottom) for the 5-sensor conﬁguration. (left) The
proximity maps of the sensor signals of the controlled regime show each cluster in a different colour with their centroids
denoted by numbers (1-10). (centre) The transition matrix is associated with the clustering process. (right) The control network.
The yellow boxes set the maximum range of b during the control and the red and blue boxes indicates positive and negative
levels (the sign of b). The dashed line and red/blue background indicate the assumed separation between the actuation regions.

cluded in the loss function. Furthermore, the policy obtained
by DRL appears to be more robust in terms of dependency on
the initial condition. On the other hand, LGPC achieves sig-
niﬁcantly more compact and interpretable control laws, which
also identify only subsets of the probes as being relevant to de-
ﬁne control actions. In particular, for the cases with 11 probes
and low to moderate noise levels, LGPC identiﬁes that only 2
probes are sufﬁcient to deﬁne a sufﬁciently robust control law.
The potential reduction in sensorization complexity is a very
desirable feature for experimental application.

Regarding the effect of noise, DRL shows superior perfor-
mances in terms of robustness to noise of the sensors up to rel-
atively high noise levels (10%). LGPC, on the other hand, is
able to identify control laws that are effective in reducing the
average drag coefﬁcient, although maintaining a larger level
of ﬂuctuations around the mean lift and drag coefﬁcient if
compared to DRL, and in general larger standard deviation
of the actuator mass ﬂow rate. This superior robustness of
the DRL can certainly be ascribed to the higher complexity of
the adopted agent if compared to the control policies identi-
ﬁed by LGPC. Also in presence of noise, LGPC converges to
compact control laws and automatically identiﬁes only a few
signiﬁcant probes for the control, almost independently of the
noise level within the tested range.

Finally, an analysis using clustering of the sensor data us-

FIG. 9: Control network for laws/policies learned by DRL
(top) and LGPC (bottom) for the 5 (left) and 11 (right)
probes. For more details see the caption of Figure 8

Machine learning ﬂow control with few sensor feedback and measurement noise

14

FIG. 10: Proximity map of the clustered sensors and centroids for the velocity time series extracted from the DRL/LGPC
controls with increasing noise. The noise intensity increases from left to right: 0%, 1%, 5% and 10%. The dashed black line
divides the reconstructed phase space into two regions: one of positive actuation and one with negative actuation (the sign of b)
denoted respectively by a red + and a blue −. The γ1 and γ2 axis have been reﬂected such the red + is located in the top right
corner for all cases.

ing MDS has been carried out to interpret the control laws
obtained by DRL and LGPC. In absence of noise, it is rather
evident the convergence to a limit cycle in both cases and a
clear relation between the phase of the shedding and the con-
trol actuation. This is an indicator that both solutions converge
to phasor control, which was an expected result for this simple
ﬂow conﬁguration.

Although the number of sensors might seem high for a real
application, it is remarkable that this study has already consid-
erably reduced the number of probes compared to other recent
contributions21,26,36. Having a smaller set of probes is viable
but not recommended for a proof of concept as the one pre-
sented herein, since the information provided to the controller
would be very limited and hence the solution would probably
be suboptimal. The chosen sets of sensors allow testing the
performance of both algorithms and also evaluate their rele-
vance for the ﬁnal controller. Consequently, one of the main
conclusions of the study is the capability of LGPC of identify-
ing a subset of probes as the most relevant, which suggests the
possibility of further reduction. This is a milestone for future
implementations in a real-world application or experiment.

Any comparison contains subjective biases associated with
the computational load, the number of parameters, the com-
plexity of the control problem, and even the experience of
authors with various approaches. Also, each approach could
have been improved. e.g., DRL has many architectures with
different performances and LGPC consistently proﬁts from
subplex iterations12. Even, the very formulation of the con-
trol ansatz will affect the performance. Yet, our study points
already to desirable features of two different machine learning
approaches. Future machine learning control can be expected

to integrate the fast adaptive learning of DRL, the analytical
laws and interpretability of LGPC, the fast optimization of
cluster-based control for smooth control laws48 and the math-
ematically rigorous framework of Bayesian optimization49, to
name only a few aspects.

ACKNOWLEDGMENTS

Work produced with the support of a 2020 Leonardo Grant
for Researchers and Cultural Creators, BBVA Foundation,
grant n. IN[20]_ING_ING_0163. The Foundation takes no
responsibility for the opinions, statements and contents of this
project, which are entirely the responsibility of its authors.

Funding of the National Natural Science Foundation China
(NSFC) under grants 12172109 and 12172111 and a Natu-
ral Science & Engineering grant of the Guangdong province,
China, is gratefully acknowledged.

DATA AVAILABILITY STATEMENT

The data that support the ﬁndings of this study are available

from the corresponding author upon reasonable request.

Appendix A: Flow control performance training at ﬁxed Initial
Condition

The case with training starting from a ﬁxed initial condition
is analyzed here. Although this condition is difﬁcult (if not

Machine learning ﬂow control with few sensor feedback and measurement noise

15

Since in this case the initial condition for DRL and LGPC
is the same, it is interesting to compare the different strate-
gies adopted by the two control laws to bring the system to a
different limit cycle than the original unperturbed system. In
the ﬁrst shedding cycle, the control law identiﬁed by LGPC
acts with a strong blowing on the top actuator, thus determin-
ing a signiﬁcant asymmetric and a net negative lift coefﬁcient.
On the other hand, DRL acts with a quasi-periodic change in
phase in opposition to the original shedding cycle.

While the performance in terms of drag coefﬁcient is sim-
ilar for DRL and LGPC, more signiﬁcant differences arise in
the lift coefﬁcient. The control law identiﬁed by DRL reduces
more signiﬁcantly the oscillation of the CL, thus also requir-
ing a smaller actuator ﬂow rate for the control action. The
bar plot of the cost function and of the drag coefﬁcient re-
duction in ﬁgure 12 further support this assertion. For both
LPGC and DRL, a reduction in performances when passing
from random to ﬁxed initial condition is observed. This is a
ﬁrst glance surprising; it is hypothesized that having a ran-
dom initial condition aids the exploration in both algorithms,
allowing the discovery of more robust control strategies.

Surprisingly, even if using the same set of probes, LGPC
exhibits a much more complex control law if compared to the
random initial condition case, and fails to identify a compact
law,

Q jet = 10−2 ×

(cid:20)(cid:18)(cid:18)

(cid:18)

u7

t −

T
2

(cid:19)

(cid:18)

(cid:18)

·

v6 + u6

t −

3T
4

(cid:19)(cid:19)(cid:19)

(cid:18)

0.1551 −

(cid:18)

(cid:18)

v9

t −

(cid:19)

T
2

(cid:18)

− u0

t −

T
4

(cid:18)

(cid:18)

u8

t −

3T
4
(cid:19)(cid:19)(cid:19)(cid:19)(cid:19)(cid:19)

(cid:19)

(cid:18)

(cid:18)

·

v0

t −

(cid:18)

(cid:18)

∗ cos

u7

t −

T
4
T
2

(cid:19)

+

+

(cid:19)(cid:19)(cid:21)

This might be ascribed to a tendency of the algorithm to

overﬁt the control law to the prescribed initial condition.

REFERENCES

1B. R. Noack, “Closed-loop turbulence control-from human to machine
learning (and retour),” in Fluid-Structure-Sound Interactions and Control.
Proceedings of the 4th Symposium on Fluid-Structure-Sound Interactions
and Control, edited by Y. Zhou, M. Kimura, G. Peng, A. D. Lucey, and
L. Hung (Springer Singapore, 2019).
2S. L. Brunton, B. R. Noack, and P. Koumoutsakos, “Machine Learning for
Fluid Mechanics,” Annual Review of Fluid Mechanics 52, 477–508 (2020).
3P. W. Bearman and J. K. Harvey, “Control of circular cylinder ﬂow by the
use of dimples,” AIAA Journal 31, 1753–1756 (1993).
4S. Ozono, “Flow control of vortex shedding by a short splitter plate asym-
metrically arranged downstream of a cylinder,” Physics of Fluids 11, 2928–
2934 (1999).
5D. R. Williams and C. W. Amato, “Unsteady pulsing of cylinder wakes,”
in Frontiers in Experimental Fluid Mechanics, edited by M. Gad-el Hak
(Springer Berlin Heidelberg, Berlin, Heidelberg, 1989) pp. 337–364.
6A. Glezer, “Some aspects of aerodynamic ﬂow control using synthetic-jet
actuation,” Philosophical Transactions of the Royal Society A: Mathemati-
cal, Physical and Engineering Sciences 369, 1476–1494 (2011).

FIG. 11: Evolution of J, CD, CL, and Q j for the unforced case

), RL (

(
) at ﬁxed initial condition
during the training process. Results are shown 11 probes.

), and LGPC (

impossible) to achieve in experiments, it can be approximated
reasonably well in the case of shedding-dominated ﬂows at
a low-to-moderate Reynolds number. The evolution of the
cost function, the force coefﬁcients and the actuator ﬂow rate
are reported in Figure 11 for the case of the ﬁnal resulting
actuation after the training. For brevity, only the case with 11
probes is analyzed.

0.740.881.021.161.3J2.933.013.093.173.25CD-2.4-1.201.22.4CL0102030405060t$-90-4504590Qj(#104)Machine learning ﬂow control with few sensor feedback and measurement noise

16

FIG. 12: Key performance indicators, cost function (cid:104)J(cid:105) (left) and drag reduction −∆(cid:104)CD(cid:105) =

and ﬁxed (

) initial condition.

(cid:104)CD0

(cid:105)−(cid:104)CD0
(cid:104)CD(cid:105)

(cid:105)

(right), for random (

)

7T. Duriez, S. L. Brunton, and B. R. Noack, Machine Learning Control
– Taming Nonlinear Dynamics and Turbulence, Fluid Mechanics and Its
Applications, Vol. 116 (Springer International Publishing, 2017).
8R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction
(MIT press, 2018).
9J. Koza, “Genetic programming as a means for programming com-
puters by natural selection,” Statistics and Computing 4
(1994),
10.1007/bf00175355.

10W. Banzhaf, P. Nordin, R. E. Keller, and K. D. Francone, Genetic program-
ming: an introduction: on the automatic evolution of computer programs
and its applications (Morgan Kaufmann Publishers Inc., 1998).

11G. Y. Cornejo Maceda, B. R. Noack, F. Lusseyran, N. Deng, L. Pastur, and
M. Morzy´nski, “Artiﬁcial intelligence control applied to drag reduction of
the ﬂuidic pinball,” Proc. Appl. Math. Mech. 19, 1–2 (2019).

12G. Y. Cornejo Maceda, Y. Li, F. Lusseyran, M. Morzy´nski, and B. R.
Noack, “Stabilization of the ﬂuidic pinball with gradient-enriched machine
learning control,” Journal of Fluid Mechanics 917, A42 (2021).

13Y. Zhou, D. Fan, B. Zhang, R. Li, and B. R. Noack, “Artiﬁcial intelligence

control of a turbulent jet,” Journal of Fluid Mechanics 897 (2020).

14Z. Wu, F. Dewei, Y. Zhou, R. Li, and B. R. Noack, “Jet mixing optimiza-
tion using machine learning control,” Experiments in Fluids 59, 131 (2018),
arXiv: 1802.01252.

15R. Li, J. Boree, B. R. Noack, L. Cordier, and F. Harambat, “Drag reduc-
tion mechanisms of a car model at moderate yaw by bi-frequency forcing,”
Physics Reviews Fluids 4, 034604 (2019).

16N. Gautier, J. L. Aider, T. Duriez, B. R. Noack, M. Segond, and M. Abel,
“Closed-loop separation control using machine learning,” Journal of Fluid
Mechanics 770, 442–457 (2015).

17F. Ren, C. Wang, and H. Tang, “Active control of vortex-induced vibration
of a circular cylinder using machine learning,” Physics of Fluids 31, 093601
(2019).

18V. Parezanovi´c, L. Cordier, A. Spohn, T. Duriez, B. R. Noack, J.-P. Bonnet,
M. Segond, M. Abel, and S. L. Brunton, “Frequency selection by feedback
control in a turbulent shear ﬂow,” J. Fluid Mech. 797, 247–283 (2016).
19C. Raibaudo, P. Zhong, B. R. Noack, and R. J. Martinuzzi, “Machine learn-
ing strategies applied to the control of a ﬂuidic pinball,” Physics of Fluids
32, 015108 (2020).

20R. Li, B. R. Noack, L. Cordier, J. Borée, and F. Harambat, “Drag reduc-
tion of a car model by linear genetic programming control,” Experiments in
Fluids 58, 103 (2017).

21J. Rabault and A. Kuhnle, “Accelerating deep reinforcement learning strate-
gies of ﬂow control through a multi-environment approach,” Physics of Flu-
ids 31, 094–105 (2019).

22C. Lee, J. Kim, D. Babcock, and R. Goodman, “Application of neural
networks to turbulence control for drag reduction,” Physics of Fluids 9,
1740–1747 (1997).

23M. Gazzola, B. Hejazialhosseini, and P. Koumoutsakos, “Reinforcement
learning and wavelet adapted vortex methods for simulations of self-
propelled swimmers,” SIAM Journal on Scientiﬁc Computing 36, B622–

B639 (2014).

24E. Bøhn, E. M. Coates, S. Moe, and T. A. Johansen, “Deep reinforce-
ment learning attitude control of ﬁxed-wing uavs using proximal policy op-
timization,” in 2019 International Conference on Unmanned Aircraft Sys-
tems (ICUAS) (IEEE, 2019) pp. 523–533.

25G. Reddy, A. Celani, T. J. Sejnowski, and M. Vergassola, “Learning to
soar in turbulent environments,” Proceedings of the National Academy of
Sciences 113, E4877–E4884 (2016).

26J. Rabault, M. Kuchta, A. Jensen, U. Réglade, and N. Cerardi, “Artiﬁ-
cial neural networks trained through deep reinforcement learning discover
control strategies for active ﬂow control,” Journal of ﬂuid mechanics 865,
281–302 (2019).

27D. Fan, L. Yang, Z. Wang, M. S. Triantafyllou, and G. E. Karniadakis,
“Reinforcement learning for bluff body active ﬂow control in experiments
and simulations,” Proceedings of the National Academy of Sciences 117
(2020).

28G. Beintema, A. Corbetta, L. Biferale,

and F. Toschi, “Controlling
rayleigh–bénard convection via reinforcement learning,” Journal of Turbu-
lence 21, 585–605 (2020).

29V. Belus, J. Rabault, J. Viquerat, Z. Che, E. Hachem, and U. Reglade, “Ex-
ploiting locality and physical invariants to design effective deep reinforce-
ment learning control of the unstable falling liquid ﬁlm,” arXiv preprint
arXiv:1910.07788 (2019).

30H. Xu, W. Zhang, J. Deng, and J. Rabault, “Active ﬂow control with rotat-
ing cylinders by an artiﬁcial neural network trained by deep reinforcement
learning,” Journal of Hydrodynamics 32, 254–258 (2020).

31J. Li and M. Zhang, “Reinforcement-learning-based control of conﬁned
cylinder wakes with stability analyses,” Journal of Fluid Mechanics 932
(2021), 10.1017/jfm.2021.1045.

32R. Paris, S. Beneddine, and J. Dandois, “Robust ﬂow control and optimal
sensor placement using deep reinforcement learning,” Journal of Fluid Me-
chanics 913 (2021).

33W. Banzhaf, P. Nordin, and R. E. Keller, Genetic Programming: An Intro-

duction (Springer Singapore, 1997).

34F. Pino, L. Schena, J. Rabault, A. Kuhnle, and M. Mendez, “Compara-
tive analysis of machine learning methods for active ﬂow control,” arXiv
preprint arXiv:2202.11664 (2022).

35F. Ren, J. Rabault, and H. Tang, “Robust active ﬂow control over a range of
Reynolds numbers using an artiﬁcial neural network trained through deep
reinforcement learning,” Physics of Fluids 32, 053605 (2020).

36F. Ren, J. Rabault, and H. Tang, “Applying deep reinforcement learning
to active ﬂow control in weakly turbulent conditions,” Physics of Fluids 33
(2021).

37M. Schäfer, S. Turek, F. Durst, E. Krause, and R. Rannacher, “Benchmark
computations of laminar ﬂow around a cylinder,” in Flow simulation with
high-performance computers II (Springer, 1996) pp. 547–566.

38A. Logg, K. A. Mardal, and G. Wells, Automated solution of differential
equations by the ﬁnite element method: The FEniCS book, Vol. 84 (Springer
Science & Business Media, 2012).

hJi0.8070.8460.8180.856DRLLGPC0.70.750.80.850.9!"hCDi7.73%7.85%7.27%6.72%DRLLGPC5%6%7%8%9%Machine learning ﬂow control with few sensor feedback and measurement noise

17

39C. Geuzaine and J. Remacle, “Gmsh: A 3-D ﬁnite element mesh generator
with built-in pre-and post-processing facilities,” International journal for
numerical methods in engineering 79, 1309–1331 (2009).

40J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal

Policy Optimization Algorithms,” (2017), arXiv: 1707.06347.

41M. Wahde, Biologically Inspired Optimization Methods: An Introduction

(WIT Press, 2008).

42G. Y. Cornejo Maceda, Gradient-enriched machine learning control exem-
pliﬁed for shear ﬂows in simulations and experiments, Ph.D. thesis, Univer-
sité Paris-Saclay (2021).
43D. Fernex, B. N. Noack,

and R. Semaan, “Cluster-based network
modeling—from snapshots to complex dynamical systems,” Science Ad-
vances 7 (2021), 10.1126/sciadv.abf5006.

44H. Li, D. Fernex, R. Semaan, J. Tan, M. Morzy´nski, and B. R. Noack,
“Cluster-based network model of an incompressible mixing layer,” J. Fluid
Mech. 906 (2021), 10.1017/jfm.2020.785.

45E. Kaiser, R. Li, and B. R. Noack, “On the control landscape topology,”
in The 20th World Congress of the International Federation of Automatic
Control (IFAC) (Toulouse, France, 2017) pp. 1–4.

46Y. Li, W. Cui, Q. Jia, Q. Li, Z. Yang, M. Morzy´nski, and B. R. Noack,
“Explorative gradient method for active drag reduction of the ﬂuidic pin-
ball and slanted ahmed body,” Journal of Fluid Mechanics 932 (2022),
10.1017/jfm.2021.974.

47F. Foroozan, V. Guerrero, A. Ianiro, and S. Discetti, “Unsupervised mod-
elling of a transitional boundary layer,” Journal of Fluid Mechanics 929
(2021).

48A. Nair, C.-A. Yeh, E. Kaiser, B. R. Noack, S. L. Brunton, and K. Tiara,
“Cluster-based feedback control of turbulent post-stall separated ﬂows,”
J. Fluid Mech. 875, 345–375 (2019).

49A. Blanchard, G. Y. Cornejo Maceda, D. Fan, Y. Q. Li, Y. Zhou, B. R.
Noack, and T. Sapsis, “Bayesian optimization of active ﬂow control,” Acta
Mechanica Sinica (online) (2022).

