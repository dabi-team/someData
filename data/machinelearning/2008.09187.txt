Designing optical glasses by
machine learning coupled with a
genetic algorithm

Daniel R. Cassar1, Gisele G. dos Santos, Edgar D. Zanotto

Department of Materials Engineering, Federal University of São Carlos, São Carlos, SP, Brazil

Abstract

Engineering new glass compositions have experienced a sturdy tendency to move forward from
(educated)   trial-and-error   to   data-   and   simulation-driven   strategies.   In   this   work,  we  developed  a
computer program that combines data-driven predictive models (in this case, neural networks) with a
genetic   algorithm   to   design   glass   compositions   with   desired   combinations   of   properties.   First,   we
induced   predictive   models   for  the   glass   transition   temperature   (Tg)   using   a   dataset   of   45,302
compositions with 39 different chemical elements, and for the refractive index (nd) using a dataset of
41,225   compositions   with   38   different   chemical   elements.   Then,   we   searched   for   relevant   glass
compositions using a genetic algorithm  informed by a design trend of glasses having high nd (1.7 or
more) and low Tg (500 °C or less). Two candidate compositions suggested by the combined algorithms
were selected and produced in the laboratory. These compositions are significantly different from those
in the datasets used to induce the predictive models, showing that the used method is indeed capable of
exploration. Both glasses met the constraints of the work, which supports the proposed framework.
Therefore, this new tool can be immediately used for accelerating the design of new glasses. These
results are a stepping stone in the pathway of machine learning-guided design of novel glasses.

Keywords:   oxide   glass,   optical   glass,   machine   learning,   genetic   algorithm,   refractive   index,  glass
transition temperature 

Publisher version available at https://doi.org/10.1016/j.ceramint.2020.12.167.

Please cite this paper as: Cassar, D.R., Santos, G.G., and Zanotto, E.D. (2021). Designing optical
glasses by machine learning coupled with a genetic algorithm. Ceramics International 47, 10555–
10564. doi: 10.1016/j.ceramint.2020.12.167

1 Corresponding author: contact@danielcassar.com.br.

1

1. Introduction

Glass   science  and  technology  are  currently  experiencing  an “artificial  intelligence  renaissance.”
Even though many of the tools being used are not new, the interface between data and glass sciences
has never seen so much interest [1–17]. Naturally, the number of reports on machine learning-based
property prediction of glasses has surged in the past three years due to the availability of powerful
computational tools and hardware, and the recent licensing of the SciGlass database under a permissive
license (https://github.com/epam/SciGlass)—which has approximately 400,000 entries on composition-
properties of glasses. Moreover, the high correlation between composition and properties for inorganic
non-metallic glasses  makes  using data-driven tools  significantly easier for these materials  than for
polycrystalline materials.

Most of the tools and machine learning models reported so far have been focused on predicting a
chosen property given a glass composition [2,4,8,12,17–19]. For new glass development, however, it is
paramount to solve the  inverse  design problem, finding possible compositions that are predicted to
have a desired set of properties. This inverse design problem cannot be solved by traditional machine
learning methods alone, to the best of our knowledge. Still, it can be tackled by combining machine
learning and optimization algorithms. In this context, Nakamura and co-authors [16] recently used
Bayesian optimization coupled with Gaussian process regression to search for oxide glasses with high
refractive   indices.   Still   in   the   subject   of   optical   glasses,   Tokuda   and   co-authors   [20]   applied   the
knowledge obtained from trained ML models to dope a glass composition to obtain a material with a
high refractive index and low Abbe number.

This   work   aims   to   propose   and   test   a   framework   to   solve   inverse   design   problems   for   glass
development using genetic algorithms. While the proposed framework is general, here it will be tested
for designing new optical glasses. Neural networks will induce the predictive models used here.

2. Design trends in optical glasses

The growth of the smartphone market and the demand for increasingly smaller and better-defined
security and car cameras have attracted significant attention and fostered optical glass research. There
is enormous interest in obtaining increasingly smaller, thinner, and more efficient light transmission
lenses. The recent technological advances in 4K and 8K applications and Virtual/Mixed Reality lenses
are also fueling the field of optical glasses. A relevant review article by Peter Hartmann and co-authors
[21] listed some of the hottest trends in optical glass research, which were:  i) high refractive indices
(1.7 or more), ii) a high Abbe number (60 or more), iii) high refractive indices and a high Abbe number,
and iv) high refractive indices and a low Abbe number.

Glasses   with   high   refractive   indices   are   desired   because   they   reduce   the   degree   of   spherical
aberration and enable lens design with reduced dimensions, e.g., targeting smartphones and small car
cameras. Glasses with a high Abbe number (low dispersion) are used in optical systems that require a
low degree of chromatic aberration. Glasses with high refractive indices and a high Abbe number could
significantly impact optical glass technology, as it would make it possible to obtain smaller and thinner

2

lenses with less color dispersion, again targeting the smartphone market. Glasses with high refractive
indices and a low Abbe number are used for color correction in specific optical systems [21]. Figure 1
shows a 2D histogram of the Abbe diagram for oxide glasses in the SciGlass database. This figure
clearly shows the current property envelope of the optical properties of oxide glasses.

Figure 1.  2D histogram visualization of the Abbe diagram showing the refractive indices versus the
Abbe number. Approximately 24,300 oxide glass data points were used to build this plot.
Each rectangle has horizontal sides of 1, vertical sides of 0.01, and comprehends a density of
experimental points depicted by its color (color bar on the right).

In addition to the four design trends previously discussed, there is additional interest in glasses
having low glass transition temperature for optical lens production via precision molding techniques.
This   technique   consists   of   applying   pressure   to   a   mold   containing   a   glass-forming   liquid,   with   a
controlled atmosphere and temperatures between the glass transition temperature (Tg) and the softening
point. The material obtained in this way is already in its final form, without the need for expensive,
time-consuming additional steps, such as polishing and finishing. The most commonly used molds are
made of tungsten carbide or silicon carbide with different coatings, and they are the most expensive
parts   of   this   process.   Tungsten   carbide   molds,   for   example,   are   sensitive   to   oxidation   at   high
temperatures, requiring the lenses to be conformed below 500 ºC, ideally below 450 ºC [21]. However,
the   same   weak   intermolecular   bonds   that   allow   a   glass   to   have   a   low  Tg  often   bring   some
disadvantages,   such   as   low   chemical   stability,   which   makes   meeting   this   constraint   a   significant
challenge.

3

3. Materials and methods

3.1. Data collection and partition

All the data used in this work were collected from the SciGlass database, which is now licensed
under   the   ODC   Open   Database   License   (OdbL).  This   database   collects   glass   properties   and   their
respective chemical compositions reported in scientific articles, books, and patents.

We collected data on glass transition temperature and refractive index (nd) of oxide glasses to induce
predictive models for these properties via neural networks. The definition of oxide glasses considered
here is the same as what we have used in previous work [10], that is: materials having an atomic
fraction of oxygen of at least 0.3 and not having the chemical elements S, H, C, Pt, Au, F, Cl, N, Br,
and I. Briefly, these are either elements that are too volatile or that occupy oxygen sites.

Both properties of interest depend on the thermal history and measurement conditions, which were
not considered as features in this work as this information was not available in the database. While this
lack of information increases the uncertainty of the prediction, it will be shown that induced models in
this condition are still sufficiently accurate for the task at hand.

Before inducing the models, the dataset for each property was pre-processed following three steps:
removal   of   glasses   made   with   chemical   elements   having   low   representability,   removal   of   glasses
having properties with extremely low or high values, and replacement of duplicate entries by their
median values. The  first  step is an iterative process where the fraction of examples containing each
chemical element is computed, and then removing those glasses having chemical elements that are
present in less than 1% of the examples. This process is repeated until all chemical elements are present
in at least 1% of the examples. The rationale behind this choice is that each chemical element adds a
new compositional dimension for the training of the model, and generalization may be compromised by
having a small number of examples.

The second step is related to examples having extreme values of the glass properties, which were
also removed. Here, extreme property values are defined as those below the 0.05% percentile and
above  the  99.95% percentile. The  rationale  behind  this   choice  is  the  knowledge  that  the  SciGlass
database does not curate its entries, and a significant portion of typos or mistakes are located in these
extreme regions.

The  third  and final step is related to examples with duplicate features, i.e., entries with the same
nominal composition. These duplicate entries were grouped into a single entry with the median value of
the property. The rationale behind this choice is to avoid a problem called data leakage [22], where the
prediction of the model is artificially improved because it had “access” to data in the reserved dataset
for testing. In other words, with duplicated data, it is possible that glasses having the same composition
end up in different datasets; thus, information in the test dataset can “leak” into training.

We computed descriptive statistics on the datasets after collection and pre-processing. After this
step, each dataset was partitioned into the holdout dataset (20%) and the training and validation (80%)
dataset. The holdout set was not used for training the models nor for hyperparameter tuning; its main

4

purpose was to measure the predictive power of the models. Finally, the final model used in the genetic
algorithm optimization was trained using  all  the pre-processed data, as this is the usual practice for
inducing the final predictive model.

3.2. Property prediction using neural networks

Neural networks (NN) are a group of machine learning (ML) algorithms that are excellent at finding
patterns in data. They are the most used type of ML algorithm in the field of oxide glasses [1–4,6,8–
13,15,17–19,23,24],   and   their   success   is   due   to   their   possibility   of   approximating   any   continuous
function. The mathematical and statistical support for NNs is discussed in depth in the textbook by
Charu Aggarwal [25].

This work investigates deep feedforward NNs with two hidden layers, which are reasonably good at
predicting glass properties with acceptable precision [4,17]. One critical choice is the NN architecture
because   it   is   known   that   different   problems   often   require   different   architectures.   We   used   a
hyperparameter tuning routine [26,27] to investigate some NN architectures, similar to previous work
[4]. This process is described in detail in the Supplementary Material.

Each property of interest was investigated independently. In the end, we obtained a predictive model
for each property. These models are functions for which the arguments lie in the chemical composition
domain, and the output is a real number representing the predicted value of a given property. While the
models   can   predict   the   properties   of   any   glass   with   chemical   elements   within   the   domain   of   the
functions, the expectation is that the prediction of compositions outside the training domain will lead to
a much higher error.

3.3. Inverse design of glass compositions

Solving   the   inverse   design   problem,   discussed   in   the   introduction,   requires   an   optimization
algorithm. A genetic algorithm (GA) was chosen because it usually finds better solutions than random
search and simulated annealing for problems with more than one optimal solution. Moreover, GA is
trivially parallelizable, which may even yield better performance than simulated annealing in some
cases.

GAs are meta-heuristics inspired by the theory of evolution and natural selection, bringing concepts
of individuals, population, selection, reproduction, and mutation, and using them to navigate the multi-
dimensional space of a single- or multi-objective optimization problem. More information on GAs is
available in Koza’s textbook [28] for a general view or Chakraborti’s article [29] for a report focusing
on materials design. To the best of our knowledge, the first published work to apply GA in the context
of oxide liquids is that of Ojovan et al. [30], whereas the first to apply GA in the context of oxide
glasses is that of Tandia et al. [8].

To use GA, one must first define how the “genome” of “individuals” are represented. In this work,
an individual is defined as a glass with a particular chemical composition, having a genome that is
represented as a row vector I = [x1, x2, …, xn], where each “gene” xi is an integer in the range [0, 100]
that stores the amount (in moles) of a particular chemical compound ci. Here, n=28 compounds were
considered for the search space: Al2O3, B2O3, BaO, Bi2O3, CaO, CdO, Gd2O3, GeO2, K2O, La2O3, Li2O,
5

MgO, Na2O, Nb2O5, P2O5, PbO, Sb2O3, SiO2, SnO2, SrO, Ta2O5, TeO2, TiO2, WO3, Y2O3, Yb2O3, ZnO,
and ZrO2. These compounds were selected because they are possible inputs for all predictive models
trained here (see Table 3 in Section 4.1), i.e., they are common to both properties, and they were readily
available in our laboratory for melting candidate compositions found by the GA.

A population P with m individuals is defined as a matrix m×n, where each row holds the information
of a single individual. The population size of this work was m=400, and the initial population P1 was
generated by randomly sampling integers in the range of [0, 100] and building a 400×28 matrix. These
randomly generated individuals are probably inadequate solutions to the optimization problem that is
being investigated; many of them may not even form a glass or be within any constraint for which the
problem is being optimized.

However, by pure chance, some randomly generated individuals of P1 will be closer to a possible
solution than others, even if they do not meet all the requirements of the problem. The word “possible”
is emphasized because, for any given inverse design problem, a solution may or may not exist, which is
an issue that is not directly related to GA.

The   next   step   is   to   select   the   individuals   of  P1  that   will   “survive”   to   the   next   generation   and
compose population  P2. This step starts by computing the fitness score for each individual. In this
work, the fitness function  f  was a weighted Euclidean distance in the property space, Eq. (1). The
smaller the value of f, the better chances the individual has to survive.

(1)

In the previous equation, x and y are the values of two different properties of a particular individual;
xd and yd are the desired values for these two properties, which depend on the inverse design problem
that is being solved; wx and wy are the weights that each property has to compute the fitness score; and
ε1, ε2, and ε3 are penalty factors that will be discussed later on in this section.

We studied only optimization problems with two properties, but Eq. (1) can be easily expanded for
inverse design with more properties. To be clear, the values of x and y in Eq. (1) are predicted by the
trained neural network models that were discussed in Section 3.2. Before the prediction, the chemical
composition of each individual needs to be converted to atomic fraction and normalized to have a total
sum of 1, as this was the format of the input data used for training the models. The weights w used here
were 1 for Tg and 20 for nd; these values were chosen to balance out the different magnitudes of these
properties. This configuration was sufficient for this work, but poor convergence problems may benefit
from tuning these weights.

After calculating the fitness score for all individuals in  P1, the selection phase begins. There are
some selection strategies available. In this paper, we used the tournament selection where 3 individuals
are selected at random from the population, and the one with the lowest fitness score from this group is
selected to be part of the next generation, which is P2 in this example. This process continues until P2
has the same number of individuals as P1.

The   next   phase   is   mating,   where   pairs   of  P2  individuals   can   exchange   genetic   material,   which
replaces the original pair (the parents) with two new individuals (the offspring). The new individuals
6

have a uniform chance of receiving each bit of genetic material from both parents, a process called
uniform crossover. The chance of mating was set to 50%.

Finally, the last step of this iteration is the mutation phase, a critical step as it is the only opportunity
for introducing different genetic material that was not present in the randomly generated P1. Here, each
individual of P2 has a 20% chance to undergo mutation. If selected, then each gene has a 5% chance of
changing its value to a random integer in the range of [0, 100]. On the one hand, if the mutation
probabilities are too high, then the problem may not converge, as the “memory” of the best individuals
is easily lost to mutations. On the other hand, if the mutation probabilities are too low, then the number
of iterations required to reach a solution may become prohibitively large.

After these steps, the whole process is repeated by computing the fitness score and performing
selection, mating, and mutation on P2 to generate P3. This iterative process was done until a solution
was found or generation 5000 was reached.

We introduced two constraints for the GA search, one related to the minimum amount of glass-
formers, and the other related to the chemical domain for which the predictive models were trained.
Both constraints were computed independently for each individual. The first constraint checks for the
ratio φ between the sum of the glass network-forming oxides (Al2O3, SiO2, B2O3, GeO2, P2O5, Sb2O3,
and   TeO2)   and   the   total   sum   of   compounds.   If   this   ratio   was   below   45%,   then   a   penalty
ε1=(100(0.45−φ))2 was computed and considered in Eq. (1), otherwise ε1=0. The rationale behind this
constraint is to increase the chances that a composition found by the algorithm can be made into a
glass. We know that this procedure does not guarantee that all compositions that meet this constraint
can   be   vitrified   by   laboratory   melt   and   quench   techniques;   however,   it   significantly   increases   the
chances.

The second constraint checks if the composition is inside the chemical domain of the predictive
models considered in the calculation of f in Eq. (1). For each chemical element i that is present in the
individual, a distance di is computed, which is zero if the atomic fraction of the said element is within
the chemical domain of all the predictive models, or it is the absolute difference between the atomic
fraction of the element and the closest atomic fraction within the domain of all predictive models. The
penalty ε2=(100∑i di)2 is then computed for each individual. The rationale behind adding this constraint
is that NNs trained using only the chemical composition as features are prone to higher prediction
errors   for   compositions   outside   the   domain.   Sometimes,   however,   it   may   be   desirable   to   explore
chemical compositions close to the training domain, but not necessarily within it. Here, we relaxed the
composition domain of each chemical element by 20%. It is important to stress that the chemical
domain   is   only   being   checked   by   comparing   the   amount   of   each   individual   chemical   element
independently. This is different from another approach that is checking if the composition is inside the
convex envelope considering all the chemical elements. A common misconception is that this penalty
ε2 would only allow the GA to find compositions that are far too similar to the training dataset of the
predictive models, resulting in a procedure only capable of exploitation but not exploration. This is not
true, as shown by the two glasses suggested by the algorithm (and produced here) being quite different
from any composition in the available datasets (see the next section).

To have a clear difference in the fitness score between individuals that meet all constraints and
individuals that do not, a final static penalty ε3  is computed: if ε1≠0 or ε2≠0, then ε3=100, otherwise
7

ε3=0. The rationale behind adding this penalty is that we do not want to allow individuals outside the
constraints   of  the  problem  (also  known  as   infeasible  individuals)   to  have  even   a  small  chance  of
winning the selection tournament against individuals that meet all the constraints. In other words, a
death penalty when infeasible individuals compete against feasible individuals [31].

Finally, being a heuristic algorithm, GA is not guaranteed to reach a solution even if it exists. Any
solution obtained is dependent on the randomly generated first population and the various steps that are
due to chance. Because of this, the GA code was run many times to obtain a diverse set of solutions.
The code used in this work was written in Python using the DEAP module [32], and it is available
under   the   GPL3   license   as   the  GLAS  module   [33],   which   stands   for  Genetic   Lookup   for  Apt
Substances.

3.4. Experimental tests

The design trend that guided our research was optical glasses with high refractive indices (1.7 or
more) and low glass transition temperature (500 ºC or less). During an exploratory phase, we observed
that   some   candidate   glasses   had   poor   chemical   durability.   Because   of   this   problem,   we   manually
reduced the search domain of the elements boron and phosphorous to [0, 0.02] and [0, 0.03] in atomic
fraction, respectively. The rationale was that these two elements often decrease the chemical durability
of glasses.

We   obtained   many   composition   candidates   by   running   the   GLAS   software   several   times.   We
selected two glasses from the candidate list. The first was called Glass 1 and was the simplest glass
from the list, that is, the one with the least amount of chemical compounds. The second was called
Glass 2, and it was selected because it did not have an excessive amount of ZrO2 and Al2O3 and a fair
percentage of alkali and alkaline earth oxides to aid in the melting procedure. Their compositions are
shown in Table 1, together with the target and predicted values of the properties of interest. We also
checked   how   different   these   two   selected   glasses   are   from   those   present   in   the   collected   datasets
(discussed in Section 3.1). To do so, we computed the Manhattan distance between the composition
vector of Glass 1 and all the composition vectors of the glasses used for training the Tg and nd models;
the smallest this distance, the closer the chemical composition is. The glass shown in the “Closest to
Glass 1” column in Table 1 is the one with the smallest distance. The same procedure was done for
Glass 2; these analyses will be discussed later in the manuscript. The Manhattan distance was used
instead of the Euclidean distance as the first performs better in high dimensional space [34].

8

Table 1.  Composition (mol%), target, and predicted properties of the two glasses produced in this
work. † To make this glass, we did not use MnO to avoid a strong color, instead we replaced
it with ZnO. ‡ The uncertainty in the prediction is estimated by the RMSE value reported in
Table 4.

Oxide

SiO2

Sb2O3

CaO

B2O3

Li2O

Nb2O5

K2O

GeO2

Na2O

SnO2

ZrO

MnO

ZnO

La2O3

Al2O3

Glass 1 Glass 2 Closest to Glass 1 Closest to Glass 2

66.67

21.21

3.03

3.03

3.03

3.03

0

0

0

0

0

0

0

0

0

41.75

27.18

1.94

0

0

0

8.74

7.77

3.88

2.91

1.94

1.94†

0.97†

0.97

0

81.08

18.92

0

0

0

0

0

0

0

0

0

0

0

0

0

50

40

0

0

0

0

0

0

0

0

0

0

0

0

10

Target property

Refractive index

Glass transition temperature (°C)

Predicted property‡

Refractive index

Glass 1 Glass 2

1.70

450

1.75

400

Glass 1 Glass 2

1.71(3) 1.76(3)

Glass transition temperature (°C) 460(30) 400(30)

The  reactants   used  and  their  respective  purity  are  reported  in  the  Supplementary  Material. The
chemicals were mixed, weighed, and homogenized in a rotation jar mill for 12 hours to make each
glass. At the end of this process, the mixture was melted in a platinum crucible at a temperature range
of 1000–1200 °C  in a Deltech electric furnace, then  poured over a  metallic  surface, crushed,  and
remelted for homogenization. This process was repeated three times. The melt was finally poured into a
1.5 × 1.5 × 3 cm graphite mold.

The glass transition temperature was  determined for small pieces  of the glasses  by Differential
Scanning Calorimetry (DSC, NETZSCH STA 449 F3 Jupiter), with a heating rate of 20 °C/min for
Glass 1 and 10 °C/min for Glass 2. The refractive index was measured in 1.5 × 1.5 × 1.5 cm samples

9

using the Na d-line (589.6 nm) of a Carl Zeiss Jena Pulfrich-refractometer PR2. Two adjacent faces of
the samples (those that interacted with the light beam in the refractometer) were ground using 150–
1200 mesh sandpaper and polished in velvet fabric with an aqueous cerium oxide suspension. No sign
of chemical attack was observed. The refraction angle was measured and converted to refractive index
using a conversion table provided by the equipment manufacturer.

Finally, both Glass 1 and Glass 2 were powdered and analyzed via X-ray diffraction in an Ultima IV,
made by  Rigaku. Glass pieces were milled until their particle size was reduced to 20 μm;  they were
analyzed in the step-scan mode in the interval between 10 and 70 degrees, with steps of 0.02° and 1
second per step. The Cu Kα radiation was used; no sign of crystallization was observed. 

4. Results and discussion

4.1. Data analysis

Figure 2 shows the histogram for the two datasets used to induce the predictive neural network
models. The distribution of the refractive index values has a single mode, with a clear skew to the right.
The distribution of the glass transition temperature also has a single mode but is not visually skewed.
Table 2 shows the descriptive statistics of both distributions.

Figure 2:  Distribution of the values of the (a) refractive index dataset and the (b) glass  transition

temperature dataset.

10

Table 2:  Descriptive statistics of the datasets used for inducing the predictive neural network model.

Glass transition temperature in Kelvin.

Statistic
Count

Number of chemical elements

Mean

Standard deviation

Minimum

Median

Maximum

Skewness

Kurtosis

Refractive index  Glass transition temperature

41,225

38

1.69

0.18

1.41

1.64

2.67

1.31

2.11

45,302

39

778.28

150.56

380.15

773.15

1271.15

0.14

−0.33

Fig. 3 complements the analysis of the datasets by showing the distribution of examples with respect
to the number of chemical elements. Glasses made with 4 chemical elements are the most common in
both datasets, and multi-component glasses made with more than 10 elements are significantly less
frequent than other multi-component glasses. Table 3 shows the chemical domain of both datasets,
indicating the minimum and maximum atomic fraction for each element. As already mentioned, this
information   is   relevant   during   the   genetic   algorithm   search,   as   candidates   that   fall   outside   the
intersection of chemical domains are penalized.

Figure 3:  Distribution of the number of different chemical elements that make the glasses in the (a)

refractive index dataset and (b) glass transition temperature dataset.

11

Table 3:  Chemical domain for the refractive index and the glass transition temperature datasets in

atomic fraction. Elements with a dash (–) are not present in the dataset.

Refractive index Glass transition temperature

Element

Min Max

Min

Ag

Al

As

B

Ba

Be

Bi

Ca

Cd

Ce

Cs

Cu

Er

Fe

Ga

Gd

Ge

K

La

Li

Mg

Mn

Mo

Na

Nb

Nd

O

P

Pb

Sb

–

0

0

0

0

0

0

0

0

–

0

–

0

0

0

0

0

0

0

0

0

0

–

0

0

0

0.379

0

0

0

–

0.38

0.4

0.4

0.326

0.183

0.374

0.273

0.312

–

0.4

–

0.171

0.222

0.4

0.4

0.376

0.418

0.4

0.471

0.304

0.219

–

0.553

0.26

0.175

0.739

0.286

0.437

0.4

0

0

0

0

0

–

0

0

–

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0.316

0

0

0

12

Max

0.421

0.367

0.4

0.4

0.244

–

0.376

0.308

–

0.158

0.456

0.333

0.143

0.316

0.334

0.179

0.385

0.497

0.255

0.584

0.228

0.231

0.22

0.553

0.263

0.2

0.745

0.286

0.442

0.4

Si

Sn

Sr

Ta

Te

Th

Ti

V

W

Y

Yb

Zn

Zr

0

0

0

0

0

0

0

–

0

0

0

0
0

0.353

0.217

0.24

0.229

0.333

0.14

0.332

–

0.222

0.4

0.4

0.286
0.232

0

0

0

0

0

–

0

0

0

0

–

0
0

0.331

0.283

0.247

0.241

0.338

–

0.273

0.286

0.231

0.188

–

0.321
0.202

4.2. Predictive models

Table 4 shows the hyperparameters used to induce the predictive neural networks. As expected,
different problems often require different NN architectures, which is observed here by the architecture
for   predicting   the   refractive   index   that   is   reasonably   different   from   the   one   to   predict   the   glass
transition temperature.

Table 4 also shows some metrics for the two models computed for the holdout dataset, which was
not used for training the NNs nor during the hyperparameter tuning routine. Therefore, the metrics
computed with this dataset simulate how the models behave with new unseen data. However, it is
important to stress that the final predictive model used in the GLAS software was trained with all the
available data, as our interest is to build the best predictive model within the considered framework. We
expect that this final model will have a smaller prediction error than the one trained with only 80% of
the dataset; however, by using the whole dataset to train the NN, we lost the ability to estimate the
prediction errors. In other words, the expected errors of the final model are probably lower than the
ones shown in Table 4, but we are unable to estimate them.

13

Table 4:  Hyperparameters used to induce the predictive neural networks and metrics of the models.
The hyperparameter tuning procedure is described in the Supplementary Material. Except for
the R2, all other metrics were rounded to one significant digit. † The numbers in parentheses
refer to the hyperparameters of the first and second hidden layers, respectively. ‡ There are
many ways to compute R2; here, it was computed considering a linear model without an
intercept as the alternative hypothesis.

Hyperparameter

Activation function

Number of neurons†

Dropout†

Adam optimizer learning rate

Adam optimizer epsilon

Patience of the early stopping routine

Batch size

Metrics (holdout dataset)

Coefficient of determination‡ (R2)

Relative deviation (RD)

Root mean squared error (RMSE)

Mean absolute error (MAE)

Median absolute error (MedAE)

Metrics (train & validation dataset)

Coefficient of determination‡ (R2)

Relative deviation (RD)

Root mean squared error (RMSE)

Mean absolute error (MAE)

Median absolute error (MedAE)

Refractive index Glass transition temperature

ReLU

(295, 115)

(11%, 27%)

3.6 × 10−4

7.05 × 10−7

12

256

0.9997

0.9%

0.03

0.02

0.01

0.9997

0.8%

0.03

0.01

0.009

Sigmoid

(190, 290)

(8.2%, 25%)

1.3 × 10−3

2.57 × 10−5

14

128

0.998

3%

30 K

20 K

10 K

0.999

2%

30 K

20 K

10 K

Neural networks can easily overfit the data due to the significant number of parameters in the model.
This issue can happen even when hyperparameter tuning and cross-validation routines are employed,
although the expectation is that these procedures help in reducing the problem. The best way to check
for overfitting is by looking at the performance of the models on the holdout datasets; the performance
reported here is comparable with state of the art in the field [10] and reasonable from an experimental
point of view. Table 4 also shows the metrics for the training and validation datasets; they are only
slightly better than the holdout dataset metrics, supporting that the final model is not overfitting the
data. Figure 4 shows the loss for the train and validation datasets during the training of the NNs,
showing no signs of overfitting (which would be a steep increase in the validation loss, meaning that
the model is losing generalization power by fitting the noise in the data).

14

Figure 4: Loss curves for the training and validation datasets for (a) refractive index and (b) glass
transition temperature. The validation loss is lower than the training loss because the dropout
is not enabled during the validation phase. An epoch is when all the training data passes
through the network during the training process.

Figure 5 complements the analysis of the predictive models by showing a correlation plot between
predicted and reported values of the properties. These calculations were performed for the holdout
dataset to understand how the predictive models behave in interpolating unseen data. Overall, most of
the data points are close to the identity line, meaning that the predictions are reasonably close to the
reported values. A more significant spread is observed in Fig. 5b when compared with Fig. 5a. One
feature worth noting is the distribution of the prediction residuals for the refractive index, shown in the
inset of Fig. 5a. This distribution is not symmetric, which merits further investigations.

15

Figure 5:  2D histogram of the predicted versus reported values for (a) refractive index and (b) glass
transition temperature, computed for the holdout dataset. The identity line is shown in black.
The inset is the histogram of the prediction residuals, the difference between the reported and
the predicted values. The vertical color bar shows the frequency of data points.

From a residual plot analysis (see Fig. S.1a in the Supplementary Material), we observe that the
prediction   residuals   have   a   weak   positive   dependence   with   the   predicted   refractive   index   values,
suggesting that additional features of glasses may improve the predictive power of the model. Chemical
features can be easily extracted from the chemical composition, as described in detail by Ward et al.
[35]. This procedure, however, will be left for future works as the current precision of the model (see
Table 4) is sufficient for tackling the inverse design problem of this work.

The refractive index a reasonably linear property with respect to the chemical composition. In fact, a
simple ordinary least squares linear regression (OLS) has a performance almost as good as the NN
model, with  an  R2  of 0.9993, RD of 1.3%, RMSE of 0.05, MAE of 0.02, and MedAE of 0.01. For
investigations of other models to predict the refractive index see Refs. [17,36]. As expected, an OLS
was   not   enough   to   capture   the   behavior   of   the   glass   transition   temperature   with   respect   of   the
composition, yielding a model significantly worse than the NN with R2 of 0.994, RD of 6%, RMSE of
62 K, MAE of 45 K, and MedAE of 34 K.

The final analysis of the predictive models is the mean and standard deviation of the prediction
residuals for each chemical element, shown in Fig. 6. Again, these calculations were performed for the
holdout dataset. We observe from this figure that the prediction error depends on the chemical elements
present in the glasses. Some chemical elements are susceptible to have data with more noise than
others.  Transition   metals   such   as   vanadium   and   volatile   substances   such   as   lead   are   examples   of
elements that can have additional noise in the data. Unaccounted changes in the charge of elements or
the stoichiometry of the glass impact the non-crystalline structure, which governs many properties of
glasses. The chemistry of some chemical elements such as erbium in Fig. 6a and arsenic in Fig. 6b was
not captured by the respective models. In these cases, the data used for training the models was not
representative enough for the model to generalize the element chemistry. All the chemical elements

16

used to prepare Glass 1 and Glass 2 have an “average” standard deviation of the prediction residuals,
except germanium when predicting the refractive index (only present in Glass 2), in which the standard
deviation is high.

Figure 6:  Mean and standard deviation of the prediction residual for each chemical element in the
holdout dataset. (a) Refractive index and (b) glass transition temperature. The number in
parentheses  is the number of glass  compositions  containing the chemical element in the
holdout dataset. The prediction residuals are the difference between the reported and the
predicted values. The order of the elements is from the least to the most frequent, from left to
right.

4.3. Experimental tests

The   glass   compositions   we   selected   for   the   experimental   tests   (Table   1)   contained   a   balanced
amount of glass formers and other elements typically found in optical glasses. Both glasses made here
are novel concerning the knowledge used to induce the predictive models—as can be seen by the

17

closest composition in the training datasets being significantly different than the melted glasses (also
shown in Table 1)—, showing that the proposed method is capable of exploration.

To make Glass 2, we replaced manganese oxide for zinc oxide due to the variation of oxidation
numbers in the former that could give our glass strong colors. This procedure could’ve been easily
implemented  as  a  compound   restriction  in   the  GLAS   search  space   as   well.  Even   with  this  minor
manual modification, our glasses end up showing a slightly yellowish color. This is likely because
some   elements,   such   as   antimony   and   tin   have   variable   valences.   The   viscosity   of   Glass   1   was
considerably high, making it a challenge to obtain a homogeneous glass free of striae. This is not
surprising when dealing with optical glasses; hence this glass required using a bar-shaped mold and a
special casting technique to avoid cords.

The DSC traces used to measure the glass transition temperature are shown in Fig. 7, for which we
obtained a value of 438(4) °C for Glass 1 and 450(13) °C for Glass 2. These analyses and computations
of the uncertainty were done using software developed by Matthew Mancini [37]. The Glass 1  Tg  is
within the predicted range of 460(30) °C; however, the Glass 2 Tg is not within the predicted range of
400(30) °C, having a higher value than expected.  Tg  is a tricky property to predict as besides the
composition, it also depends on measurement conditions such as the heating rate of the experiment.
Nevertheless, both glasses met the  Tg  design trend that informed this work, with Glass 1 having the
lowest value of Tg, an advantage over Glass 2.

Figure 7.  DSC traces focused on the glass transition region for (a) Glass 1 and (b) Glass 2. A linear
baseline was subtracted for building the plots. The dashed red line shows Tg, and the dotted
gray lines show the range of Tg considering the uncertainty. 

The measured refractive index for Glass 1 and Glass 2 were 1.713(1) and 1.749(1). Both are within
the predicted value range of 1.71(3) for the first and 1.76(3) for the latter. It is important to mention that
the predicted value for Glass 2 was for the original composition before the (minor) manual change that
we discussed in the second paragraph of this section.

The obtained glasses met the material design that informed this work (refractive index above 1.7 and
a   glass   transition   temperature   below   500   °C).  All   in   all,   these   procedures,   tools,   and   results   are
compelling, and we believe they are a stepping stone in the pathway of machine learning-guided design
of new glasses for technological applications.

18

5. Summary and Conclusion

We developed a new computer program that couples data-driven predictive models with a genetic
algorithm to aid in the design of new glass compositions with certain combinations of properties. As an
example, after training predictive models for the glass transition temperature and refractive index, we
searched for relevant glass compositions guided by a design trend regarding optical glasses—high
refractive index and low glass transition temperature. Two candidate compositions suggested by the
combined algorithms were selected and produced in the laboratory. The experimental properties of
these glasses met the material design that informed this work, supporting the proposed framework. 

This new tool can be immediately used for accelerating the design of new glasses, significantly
minimizing trial-and-error. As it reduces the quantity of resources needed; it contributes to a greener
glass development approach. These results pave the way for machine learning-guided design of novel
glasses.

Conflicts of interest

There are no conflicts of interest to declare.

Acknowledgments

This   study   was   financed   by   the   São   Paulo   State   Research   Foundation   support   (FAPESP  grant
numbers   2017/12491-0   and   2013/07793-6),   in   part   by   the   National   Council   for   Scientific   and
Technological Development (CNPq, grant number: PQ 303886/2015-3 and 167434/2017-9), and in part
by the Coordenação de Aperfeiçoamento de Pessoal de Nível Superior - Brasil (CAPES) - Finance
Code 001. The Nippon Sheet Glass Foundation (NSG Foundation) Overseas grant is much appreciated.
We would also like to thank our colleagues Ricardo Lancelotti and Dr. Laís Dantas, for helping with the
refractive index and DSC measurements, respectively. We also thank Matthew Mancini for computing
the glass transition values using his newly developed algorithm.

6. References

[1] O. Bošák, S. Minárik, V. Labaš, Z. Ančíková, P. Koštial, O. Zimnỳ, M. Kubliha, M. Poulain, M.T. 

[2]

Soltani, Artificial neural network analysis of optical measurements of glasses based on Sb 2O3, Journal 
of Optoelectronics and Advanced Materials. 18 (2016) 240–247.
J.C. Mauro, A. Tandia, K.D. Vargheese, Y.Z. Mauro, M.M. Smedskjaer, Accelerating the Design of 
Functional Glasses through Modeling, Chem. Mater. 28 (2016) 4267–4277. 
https://doi.org/10.1021/acs.chemmater.6b01054.

[3] N.M. Anoop Krishnan, S. Mangalathu, M.M. Smedskjaer, A. Tandia, H. Burton, M. Bauchy, 
Predicting the dissolution kinetics of silicate glasses using machine learning, Journal of Non-
Crystalline Solids. 487 (2018) 37–45. https://doi.org/10.1016/j.jnoncrysol.2018.02.023.

19

[4] D.R. Cassar, A.C.P.L.F. de Carvalho, E.D. Zanotto, Predicting glass transition temperatures using 

[5]

neural networks, Acta Materialia. 159 (2018) 249–256. https://doi.org/10.1016/j.actamat.2018.08.022.
J.C. Mauro, Decoding the glass genome, Current Opinion in Solid State and Materials Science. 22 
(2018) 58–64. https://doi.org/10.1016/j.cossms.2017.09.001.

[6] S. Bishnoi, S. Singh, R. Ravinder, M. Bauchy, N.N. Gosvami, H. Kodamana, N.M.A. Krishnan, 

Predicting Young’s modulus of oxide glasses with sparse datasets using machine learning, Journal of 
Non-Crystalline Solids. 524 (2019) 119643. https://doi.org/10.1016/j.jnoncrysol.2019.119643.
[7] H. Liu, Z. Fu, K. Yang, X. Xu, M. Bauchy, Machine learning for glass science and engineering: A 

review, Journal of Non-Crystalline Solids. (2019) 119419. 
https://doi.org/10.1016/j.jnoncrysol.2019.04.039.

[8] A. Tandia, M.C. Onbasli, J.C. Mauro, Machine Learning for Glass Modeling, in: J.D. Musgraves, J. 
Hu, L. Calvez (Eds.), Springer Handbook of Glass, Springer International Publishing, Cham, 2019: 
pp. 1157–1192.

[9] K. Yang, X. Xu, B. Yang, B. Cook, H. Ramos, N.M.A. Krishnan, M.M. Smedskjaer, C. Hoover, M. 
Bauchy, Predicting the Young’s Modulus of Silicate Glasses using High-Throughput Molecular 
Dynamics Simulations and Machine Learning, Scientific Reports. 9 (2019) 8739. 
https://doi.org/10.1038/s41598-019-45344-3.

[10] E. Alcobaça, S.M. Mastelini, T. Botari, B.A. Pimentel, D.R. Cassar, A.C.P. de L.F. de Carvalho, E.D. 

Zanotto, Explainable Machine Learning Algorithms For Predicting Glass Transition Temperatures, 
Acta Materialia. 188 (2020) 92–100. https://doi.org/10.1016/j.actamat.2020.01.047.

[11] D.R. Cassar, ViscNet: Neural network for predicting the fragility index and the temperature-

dependency of viscosity, Acta Materialia, (2020), https://doi.org/10.1016/j.actamat.2020.116602
[12] B. Deng, Machine learning on density and elastic property of oxide glasses driven by large dataset, 

Journal of Non-Crystalline Solids. 529 (2020) 119768. 
https://doi.org/10.1016/j.jnoncrysol.2019.119768.

[13] T. Han, N. Stone-Weiss, J. Huang, A. Goel, A. Kumar, Machine learning as a tool to design glasses 

with controlled dissolution for healthcare applications, Acta Biomaterialia. 107 (2020) 286–298. 
https://doi.org/10.1016/j.actbio.2020.02.037.

[14] Y.-J. Hu, G. Zhao, M. Zhang, B. Bin, T. Del Rose, Q. Zhao, Q. Zu, Y. Chen, X. Sun, M. de Jong, L. 
Qi, Predicting densities and elastic moduli of SiO2-based glasses by machine learning, Npj Comput 
Mater. 6 (2020) 25. https://doi.org/10.1038/s41524-020-0291-z.

[15] J.N.P. Lillington, T.L. Goût, M.T. Harrison, I. Farnan, Predicting radioactive waste glass dissolution 

with machine learning, Journal of Non-Crystalline Solids. 533 (2020) 119852. 
https://doi.org/10.1016/j.jnoncrysol.2019.119852.

[16] K. Nakamura, N. Otani, T. Koike, Search for oxide glass compositions using Bayesian optimization 

with elemental-property-based descriptors, J. Ceram. Soc. Japan. 128 (2020) 569–572. 
https://doi.org/10.2109/jcersj2.20118.

[17] R. Ravinder, K.H. Sridhara, S. Bishnoi, H.S. Grover, M. Bauchy, Jayadeva, H. Kodamana, N.M.A. 
Krishnan, Deep learning aided rational design of oxide glasses, Mater. Horiz. 7 (2020) 1819–1827. 
https://doi.org/10.1039/D0MH00162G.

[18] C. Dreyfus, G. Dreyfus, A machine learning approach to the estimation of the liquidus temperature of 

glass-forming oxide blends, Journal of Non-Crystalline Solids. 318 (2003) 63–78. 
https://doi.org/10.1016/S0022-3093(02)01859-8.

[19] D.S. Brauer, C. Rüssel, J. Kraft, Solubility of glasses in the system P2O5–CaO–MgO–Na2O–TiO2: 

Experimental and modeling using artificial neural networks, Journal of Non-Crystalline Solids. 353 
(2007) 263–270. https://doi.org/10.1016/j.jnoncrysol.2006.12.005.

[20] Y. Tokuda, M. Fujisawa, D.M. Packwood, M. Kambayashi, Y. Ueda, Data-driven design of glasses 
with desirable optical properties using statistical regression, AIP Advances. 10 (2020) 105110. 
https://doi.org/10.1063/5.0022451.

20

[21] P. Hartmann, R. Jedamzik, S. Reichel, B. Schreder, Optical glass and glass ceramic historical aspects 

and recent developments: a Schott view, Appl. Opt. 49 (2010) D157–D176. 
https://doi.org/10.1364/AO.49.00D157.

[22] S. Kaufman, S. Rosset, C. Perlich, O. Stitelman, Leakage in data mining: Formulation, detection, and 

avoidance, ACM Trans. Knowl. Discov. Data. 6 (2012) 15:1–15:21. 
https://doi.org/10.1145/2382577.2382579.

[23] J. Ruusunen, Deep Neural Networks for Evaluating the Quality of Tempered Glass, M.Sc Dissertation,

Tampere University of Technology, 2018.

[24] M.C. Onbaşlı, A. Tandia, J.C. Mauro, Mechanical and Compositional Design of High-Strength 
Corning Gorilla® Glass, in: W. Andreoni, S. Yip (Eds.), Handbook of Materials Modeling: 
Applications: Current and Emerging Materials, Springer International Publishing, Cham, 2020: pp. 
1997–2019. https://doi.org/10.1007/978-3-319-44680-6_100 (accessed June 11, 2020).

[25] C.C. Aggarwal, Neural Networks and Deep Learning: A Textbook, Springer International Publishing, 

2018. https://doi.org/10.1007/978-3-319-94463-0.

[26] J. Bergstra, D. Yamins, D.D. Cox, Hyperopt: A python library for optimizing the hyperparameters of 

machine learning algorithms, in: Proceedings of the 12th Python in Science Conference, 2013: pp. 13–
20.

[27] J.S. Bergstra, R. Bardenet, Y. Bengio, B. Kégl, Algorithms for hyper-parameter optimization, in: 

Advances in Neural Information Processing Systems, 2011: pp. 2546–2554.

[28] J.R. Koza, Genetic programming: on the programming of computers by means of natural selection, 

MIT Press, Cambridge, Mass, 1992.

[29] N. Chakraborti, Genetic algorithms in materials design and processing, International Materials 

Reviews. 49 (2004) 246–260. https://doi.org/10.1179/095066004225021909.

[30] M.I. Ojovan, K.P. Travis, R.J. Hand, Thermodynamic parameters of bonds in glassy materials from 
viscosity-temperature relationships, Journal of Physics: Condensed Matter. 19 (2007) 415107-
415107–12. https://doi.org/10.1088/0953-8984/19/41/415107.

[31] C.A. Coello Coello, Theoretical and numerical constraint-handling techniques used with evolutionary 
algorithms: a survey of the state of the art, Computer Methods in Applied Mechanics and Engineering.
191 (2002) 1245–1287. https://doi.org/10.1016/S0045-7825(01)00323-1.

[32] F.-A. Fortin, F.-M.D. Rainville, M.-A. Gardner, M. Parizeau, C. Gagné, DEAP: Evolutionary 
Algorithms Made Easy, Journal of Machine Learning Research. 13 (2012) 2171–2175.

[33] D.R. Cassar, drcassar/glas: GLAS v0.1.0.dev2, Zenodo, 2020. 

https://doi.org/10.5281/zenodo.3991781.

[34] C.C. Aggarwal, A. Hinneburg, D.A. Keim, On the surprising behavior of distance metrics in high 

dimensional space, in: J. Van den Bussche, V. Vianu (Eds.), Database Theory — ICDT 2001, Springer 
Berlin Heidelberg, Berlin, Heidelberg, 2001: pp. 420–434. https://doi.org/10.1007/3-540-44503-X_27.

[35] L. Ward, A. Agrawal, A. Choudhary, C. Wolverton, A general-purpose machine learning framework 
for predicting properties of inorganic materials, Npj Computational Materials. 2 (2016) 16028. 
https://doi.org/10.1038/npjcompumats.2016.28.

[36] D.R. Cassar, S.M. Mastelini, T. Botari, E. Alcobaça, A.C.P.L.F. de Carvalho, E.D. Zanotto, Predicting 
thermal, mechanical, and optical properties of oxide glasses by machine learning using large datasets, 
ArXiv:2009.03194 [Cond-Mat]. (2020). http://arxiv.org/abs/2009.03194 (accessed September 8, 
2020).

[37] M. Mancini, M. Sendova, J.C. Mauro, Geometric representation of the calorimetric glass transition 
with constant cooling rate cycles, Under Review for the Journal of the American Ceramic Society. 
(2020).

21

Supplementary material 
1. Chemicals used to produce the glasses

The  chemical  reactants   used   in  this  work  are  shown  in  the  Table  S.1.  We  used   nitrates   (when
available)  to create an  oxidative atmosphere during  the melting  operation  to control the  oxidation
numbers, as well as avoid chemical attacks on our platinum crucible.

Table S.1. Composition, manufacturer, and purity of the chemical reagents used in this work.

Substance

Manufacturer

Purity

SiO2

H3BO3

LiNO3

Aldrich

Vetec

Aldrich

Ca(NO3)2.4H2O

Vetec

>99.99%

99.5%

95%

99%

99.99%

>99%

Alfa Aesar

Aldrich

Riedel-de-Haen >99%

Aldrich

Aldrich

CBMM

>99%

>99%

98%

Alfa Aesar

99.90%

Riedel-de-Haen >99%

Alfa Aesar

99.70%

La2O3

Sb2O3

GeO2

KNO3

NaNO3

Nb2O5

SnO2

ZnO

ZrO

2. Hyperparameter tuning

Hyperparameter tuning was done using the Python module hyperopt [26]. The search space of the
hyperparameters are shown in Table S.2, and it was navigated with suggestions from a Tree-structured
Parzen   Estimator   (TPE)   algorithm   [27].  A  total   of   150   hyperparameter   sets   were   tested   for   each
property of interest. The mathematical expressions of the activation functions that we explored are
shown in Eqs. (S.1) to (S.3).

22

Table S.2. Search space of the hyperparameters of the neural networks. ReLU is the rectifier linear unit

function and ELU is the exponential linear unit.

Hyperparameter

Activation function

Search space

ReLU, ELU, or Sigmoid

Number of neurons in the first layer

Number of neurons in the second layer

Dropout probability of the first layer (%)

Dropout probability of the second layer (%)

Adam optimizer learning rate

Adam optimizer epsilon

Patience of the early stopping routine

[20, 300]

[20, 300]

[0, 30]

[0, 30]

[10−4, 10−2]

[10−7, 10−3]

[10, 14]

Batch size

64, 128, or 256

(S.1)

(S.2)

(S.3)

Before the hyperparameter tuning, the training and validation dataset was partitioned into 80% for
local training, 10% for local validation, and 10% for local testing. Please note that the  training and
validation dataset was defined in Section 3.1 of the manuscript, and it does not contain the data in the
holdout dataset. For each of the 150 sets of hyperparameters tested, a neural network was trained with
this local training dataset and validated on the local validation dataset after each epoch. The validation
step is important as the training stops if there is no improvement in the prediction of the validation
dataset for a certain number of epochs defined by the patience hyperparameter. If this early stopping
routine is never met, the neural network is then trained for 500 epochs.

Each of the 150 hyperparameter sets received a score value that is the mean squared error (MSE) of
the prediction of the local test dataset. Those 10 sets with the lowest MSE score were tested again, this
time in a 5-fold cross-validation analysis, where the hyperparameter set with the lowest average MSE
score (considering all the folds) was the one selected to induce the final models. The selected sets of
hyperparameters are shown in the main manuscript in the Table 4.

23

3. Residual plot analysis

Figure S.1 shows the residual plot analysis for the predictive models induced in this work. 

Figure S.1:

2D histogram of the prediction residual versus predicted (a) refractive index and (b)
glass transition temperature, computed for the holdout dataset. 

24

