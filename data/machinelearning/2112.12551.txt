1
2
0
2

c
e
D
1
2

]

G
L
.
s
c
[

1
v
1
5
5
2
1
.
2
1
1
2
:
v
i
X
r
a

Preprocessing in

Inductive Logic Programming

Brad Hunter

Linacre College

University of Oxford

A dissertation submitted for the degree of

Master of Mathematics and Foundations of Computer Science

September 15, 2021

 
 
 
 
 
 
Inductive logic programming is a type of machine learning in which logic pro-

grams are learned from examples[22]. This learning typically occurs relative to some

background knowledge provided as a logic program. This dissertation introduces bot-
tom preprocessing, a method for generating initial constraints on the programs an

ILP system must consider. Bottom preprocessing applies ideas from inverse entail-

ment to modern ILP systems. Inverse entailment is an inﬂuential early ILP approach

introduced with Progol[20]. This dissertation also presents ⊥-Popper, an implemen-
tation of bottom preprocessing for the modern ILP system Popper[7]. It is shown

experimentally that bottom preprocessing can reduce learning times of ILP systems

on hard problems. This reduction can be especially signiﬁcant when the amount of

background knowledge in the problem is large.

Contents

1 Introduction

2 Problem Setting

2.1 Logic Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.1.1

Syntax . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.1.2
2.1.3 Logic Programming Languages

Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .

2.2 Problem Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.2.1 Language Bias . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.2.1.1 Mode Declarations . . . . . . . . . . . . . . . . . . .
2.2.1.2 Declaration Deﬁnitions . . . . . . . . . . . . . . . . .

2.2.2 Hypothesis constraints . . . . . . . . . . . . . . . . . . . . . .

2.2.3 Problem Input

. . . . . . . . . . . . . . . . . . . . . . . . . .

2.2.4

Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3 Generalization and Specialization . . . . . . . . . . . . . . . . . . . .

3 Preprocessing Theory

3.1 Preprocessing Problem . . . . . . . . . . . . . . . . . . . . . . . . . .

3.2 Bottom Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2.1 Bottom Clauses . . . . . . . . . . . . . . . . . . . . . . . . . .

3.2.2 Negative Bottom Preprocessing . . . . . . . . . . . . . . . . .

3.2.3 Positive Bottom Preprocessing . . . . . . . . . . . . . . . . . .

3.2.4 Bottom Preprocessing Deﬁnition . . . . . . . . . . . . . . . .
3.3 Generalization Relations . . . . . . . . . . . . . . . . . . . . . . . . .

3.3.1 Negative Bottom Preprocessing . . . . . . . . . . . . . . . . .

3.3.2 Positive Bottom Preprocessing . . . . . . . . . . . . . . . . . .

i

1

6

6

6

8
8

9

9

9
10

13

15

16
16

18

18

19
20

21

22

25
26

27

27

4 ⊥-Popper Implementation

4.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.1.1 High Level Algorithm . . . . . . . . . . . . . . . . . . . . . . .
4.1.2 L: Hypothesis Language . . . . . . . . . . . . . . . . . . . . .

4.1.3 Bottom Clause Encoding . . . . . . . . . . . . . . . . . . . . .

4.2 hyp_constraints: Hypothesis Constraints
. . . . . . . . . . . . .
4.3 gen_rels: Generalization Relations . . . . . . . . . . . . . . . . . .
4.4 bc_enc: Bottom Clause Encoding . . . . . . . . . . . . . . . . . . .
4.4.1 Optimization of Substitutions . . . . . . . . . . . . . . . . . .
4.5 bc_prog: Bottom Clause Generation . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
4.6 Recursion Constraints

5 Experimental Results

5.1 Trains Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.1.1 Random Trains Problems

. . . . . . . . . . . . . . . . . . . .

5.1.2

Irrelevant Background Knowledge . . . . . . . . . . . . . . . .
5.2 List Manipulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6 Related Work

6.1

Inverse Entailment

. . . . . . . . . . . . . . . . . . . . . . . . . . . .

6.2 Meta-level ILP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7 Conclusions

7.1 Limitations

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Bibliography

A Mode declaration code

B bc_prog code

C bc_enc code

D hyp_constraints and gen_rels code

ii

29

29

29
30

31

32

34
35

38

39

41

43

44

45

47
50

55

55

57

59

59

61

63

66

67

76

84

E List manipulation problems ﬁles

E.1 Bias

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

E.1.1 dropk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
E.1.2 droplast . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

E.1.3 evens . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

E.1.4 ﬁnddup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

E.1.5 last . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
E.1.6 len . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

E.1.7 member

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

E.1.8 sorted . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

E.1.9 sumlist . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
E.2 BK . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

F Trains problem ﬁles

F.1 Bias

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

F.2 BK . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

86

86

87
87

87

87

88
88

88

88

88
88

90

90

91

iii

List of Figures

5.1 Examples from the Michalski trains problem. . . . . . . . . . . . . . .
5.2 A logic program constituting background knowledge for train t3.
. .
5.3 Mean execution time spent in each step of ﬁnding a solution for random

44
45

trains problems solvable by Popper in less than 20 seconds. . . . . . .

46

5.4 Execution time of Popper and ⊥-Popper as irrelevant dyadic predicate
. . . . . . . . . .

symbols are added to the BK of a trains problem.

5.5 Execution time of Popper and ⊥-Popper as irrelevant monadic predi-

cate symbols are added to the BK of a trains problem.

. . . . . . . .
5.6 Total number of programs with n literals generated by ⊥-Popper− and
Popper as n increases. These results are for a randomly chosen dropk

49

50

task.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

53

iv

Chapter 1

Introduction

A common software engineering interview question asks the interviewee to write a pro-

gram which, given a sequence of characters, determines if the sequence is a palindrome
[17]. A palindrome is a sequence of characters that is written the same way forwards

and backwards [9]. An interviewer might provide some examples of palindromes. The

sequences ‘racecar’, ‘deed’, ‘mom’, ‘a’, and ‘’ are positive examples because they are

palindromes. The words ‘carriage’, ‘dead’, ‘mop’, ‘at’, and ‘palindrome’ are negative
examples. If an interviewee struggles, the task can be simpliﬁed by providing them a

function which reverses a sequence of characters.

Suppose this task is given to a computer instead. The problem of automatically

generating a program which satisﬁes a set of speciﬁcations is the domain of program
synthesis [10]. This dissertation focuses on Inductive Logic Programming (ILP), a

type of machine learning which can be applied to program synthesis. ILP systems

learn logic programs from examples. They take as input a set of positive examples,

a set of negative examples, and background knowledge (BK) in the form of a logic
program. In the learning from entailment ILP setting used in this dissertation, a logic

program covers an example if the program logically entails the example relative to

the background knowledge. The ideal solution to an ILP problem is a logic program

which covers all positive examples and none of the negative examples [19, 22].

Among the advantages of ILP systems are that they can often learn from very

few examples [16], they produce human readable and modiﬁable output, and they

support transfer learning. Support for transfer learning means that the output of one

ILP problem can be used as background knowledge for other problems [29]. This is
a desirable property for learning complex systems [6].

Example 1.1 (Palindrome identiﬁcation). Palindrome identiﬁcation can be encoded
as an ILP problem using the logic programming language Prolog. The problem can

1

be written as the sets (E+, E−, B) where E+ contains the positive examples, E−
contains the negative examples, and B is the background knowledge. We might write
the speciﬁcation given in the ﬁrst paragraph as follows1:






palindrome([r, a, c, e, c, a, r]).
palindrome([m, o, m]).
palindrome([h, e, e, d]).
palindrome([a]).
palindrome([]).






palindrome([c, a, r, r, i, a, g, e]).
palindrome([m, o, p]).
palindrome([p, a, l, i, n, d, r, o, m, e]).
palindrome([d, e, a, d]).
palindrome([a, t]).

last([H], H).
last([_|T],X):- last(T, X).
first([H|_],H).
middle([_|T], T2):- droplast(T, T2).
droplast([_], []).
droplast([H|T], [H|T2]):- droplast(T, T2).
empty([]).
one(1).










E+ =

E− =

B =










In less than a minute, a modern ILP system can ﬁnd the solution:

palindrome(A) :- empty(A).
palindrome(A) :- one(B), length(A,B).
palindrome(A) :- middle(A,C),palindrome(C),last(A,B),first(A,B).

Stated a bit informally, this says that a sequence of characters, A, is a palindrome

if A is of length zero or one. Otherwise, A is a palindrome if its ﬁrst and last elements
are equal and the rest of the sequence is a palindrome.

Suppose the following deﬁnition of reverse is added to B to produce B(cid:48).

B(cid:48) = B ∪

(cid:26) reverse([], []).

(cid:27)

reverse([H|T], L):- reverse(T, T2), append(T2,[H],L).

Then a modern ILP system given B(cid:48) as background knowledge can ﬁnd the fol-

lowing shorter program almost immediately:

palindrome(A) :- reverse(A, A).

As with the human interviewee, providing some extra help can simplify the prob-

lem tremendously.

1Lists in Prolog are comma-separated sequences of terms enclosed in square brackets. The syntax

[H|T] denotes a relation that holds when H is the head of a list and T is the tail of the list.

2

As Example 1.1 shows, the choice of background knowledge is critical for ILP prob-

lems [4]. Ideally supplying additional background knowledge would reduce learning

times. Unfortunately many ILP systems scale poorly as the size of the background
knowledge in a problem increases. This sensitivity to the size of the background

knowledge limits the practicality of transfer learning in ILP [6].

This dissertation introduces bottom preprocessing, a new technique which com-

putes information from the examples of an ILP problem and uses it to reduce the
I show that bottom preprocessing
set of programs an ILP system must consider.

can reduce learning times for an ILP system. Experiments using the Michalski trains

problem [13], described in Chapter 5, demonstrate a mean reduction to learning times

by over 10x when bottom preprocessing is employed. In a variation of these experi-
ments, I show that bottom preprocessing can also signiﬁcantly improve the scaling of

an ILP system as the background knowledge of a problem increases in size.

This dissertation also presents ⊥-Popper, an implementation of bottom prepro-

cessing as an extension to the ILP system Popper [7]. Popper encodes an ILP problem
as a constraint satisfaction problem (CSP) on the syntax of the logic programming

language Prolog. In ILP, possible programs are often called hypotheses. The space of

possible programs a system explores is called the hypothesis space. Popper uses the

learning from failures technique to generate new constraints whenever it produces
a failed hypothesis. These constraints prune hypotheses from its hypothesis space.

One of the limitations of Popper is that it only consults examples when testing new

hypotheses. This means that its initial hypothesis space is unconstrained by the

examples of the problem.

Earlier ILP systems, such as Progol [20] and HAIL [26], use a technique called

inverse entailment. Rather than search for whole programs as Popper does, they

search for clauses one at a time to add to a growing hypothesis. For each clause, they

begin by ﬁrst computing a bottom clause from a single positive example. They then
use that bottom clause to guide their search for the best clause to add to a hypothesis.

The goal of ⊥-Popper is to use the bottom clause employed in earlier ILP systems

to overcome the initial hypothesis space limitation of Popper. The bottom clause
of an example e relative to the background knowledge, ⊥B(e), is the logically most
speciﬁc clause that entails e. Informally, the body of ⊥B(e) consists of all facts that
cannot be false when e is true under some background knowledge. Any additional
facts that could be added to the body of ⊥B(e) are irrelevant or would negate e. Thus
the bottom clause can be viewed as creating a lower bound on any clause that should
appear in a hypothesis.

3

The key idea of bottom preprocessing is that a hypothesis H can be pruned from

the hypothesis space of an ILP problem in the following two cases:

• e− is a negative example and H logically entails ⊥B(e−).

• e+ is a positive example and H does not logically entail ⊥B(e+).

Example 1.2. The bottom clause of the negative example palindrome([a, t])
from Example 1.1, call it e−, relative to the original background knowledge B is:



palindrome(A):-




⊥B(e−) =



first(A,B), last(A,C), middle(A,D), empty(D),
length(A,F), length(D,G), one(E).



Suppose C is the clause palindrome(A):- middle(A,D), empty(D). Then C
entails ⊥B(e−). Any hypothesis containing C can be pruned from the hypothesis
space of the problem. This makes sense because non-palindromes can have empty
middles. Thus C entails too much. Note that, in this case, C is a subset of ⊥B(e−).
Clausal subsets will become important later.

An immediate diﬃculty arises with this proposed approach because entailment

between clauses is undecidable [2]. Thus bottom preprocessing follows inverse en-
tailment approaches in comparing clauses using θ-subsumption [25]. θ-subsumption

implies entailment but, while decidable, it is weaker than entailment [24]. As chapter

3 shows, the consequence of this choice is that ⊥-Popper can, in some cases, prune

solutions from Popper’s hypothesis space. This means that ⊥-Popper is incomplete
for ﬁnding solutions to ILP problems. Experiments in Chapter 5 show that this

incompleteness is not a signiﬁcant limitation on many ILP problems.

⊥-Popper is implemented as a wrapper around Popper.

It relies on providing

hypothesis constraints as input to Popper. A hypothesis constraint is a constraint
written in the language of Popper’s solver which can prune a hypothesis from its

hypothesis space. To use hypothesis constraints generated from bottom clauses, they

must be written in a form Popper’s solver can use. ⊥-Popper encodes constraints

in the Answer Set Programming (ASP) language. It encodes a subsumption check
between clauses as well as a set of constraints on hypotheses.

Popper is part of a category of modern ILP systems employing a meta-level ap-

proach to ILP. Meta-level ILP systems reason about the structure of logic programs.

They often use a solver to search their hypothesis space [5]. While the implemen-
tation of ⊥-Popper is speciﬁc to Popper, bottom preprocessing can be applied more

broadly to any meta-level ILP system that supports hypothesis constraints.

4

Contributions. This dissertation makes the following contributions:

1. It formalizes the use of bottom preprocessing to generate hypothesis constraints

for meta-level ILP systems.

2. It proves that bottom preprocessing of an ILP problem:

(a) does not prune solutions when applied to negative examples.

(b) can prune solutions when applied to positive examples. However, there

is a well-deﬁned subset of solutions which are never pruned by these con-
straints.

3. It describes ⊥-Popper, a working implementation of bottom preprocessing for

Popper.

4. It demonstrates empirically that bottom preprocessing can

(a) reduce learning times on hard ILP problems.

(b) improve the scaling of meta-level ILP systems as the amount of background

knowledge in an ILP problem increases.

Outline. This dissertation has the following structure. Chapter 2 establishes the

problem setting for bottom preprocessing. Chapter 3 contains theory related to the
soundness of bottom preprocessing. Chapter 4 describes the implementation of ⊥-

Popper. Chapter 5 introduces experimental results using the Michalski trains problem

[13] and list manipulation programming problems. Chapter 6 discusses related work.

Chapter 7 concludes the dissertation with a discussion of the limitations of bottom
preprocessing and possible future work.

5

Chapter 2

Problem Setting

This chapter provides1 an overview of logic programming and then deﬁnes the problem
setting for bottom preprocessing precisely. The problem setting is an extension of
the learning from failures (LFF) problem setting described in Learning programs by

learning from failures [7]. Many of the deﬁnitions are only slightly modiﬁed from the

ones given in this paper.

2.1 Logic Programming

Some familiarity with logic programming is assumed. The following brief summary

is intended to clearly deﬁne the terminology that will be used throughout the disser-

tation.

2.1.1 Syntax

In logic programming, variables begin with uppercase letters and constant symbols be-

gin with lowercase letters. A function symbol is a sequence of lowercase characters. A
term is a variable, a constant symbol, or a compound term of the form f (t1, t2, · · · , tn)
where f is a function symbol and each ti is a term [8].

A predicate symbol is also a sequence of lowercase characters. An atom is of the
form p(t1, t2, · · · , tn) where p is a predicate symbol and each ti is a term. While the
syntax of atoms and compound terms are similar, their semantics are diﬀerent.

The arity of a predicate symbol or function symbol deﬁnes how many arguments it

takes. A predicate symbol (respectively, function symbol) of arity n is usually written

p/n (f /n) to distinguish predicate symbols (function symbols) of diﬀerent arities from

each other. A comma separated sequence of terms enclosed in parentheses, such as

1Error: ‘proves’

6

(t1, t2, · · · , tn), is sometimes referred to as a tuple. A comma separated sequence of
terms surrounded by square brackets, such as [t1, t2, · · · , tn], is a list. Tuples and lists
are compound terms.

The symbol ¬ indicates negation. A literal is either an atom a, in which case it is
called positive, or the negation of an atom ¬a, in which case it is called negative [24].

A clause is a set of literals. A set of clauses is a clausal theory. A Horn clause is a

clause in which no more than one literal is positive. A deﬁnite clause is a Horn clause
with exactly one positive literal. A logic program is a clausal theory containing only
Horn clauses.2 A deﬁnite program or deﬁnite theory is a clausal theory containing
only deﬁnite clauses. A term, atom, or clause is ground if it contains no variables.

A clause represents the disjunction of its literals.

In logic programming, the
implication symbol, ←, is written :-. Conjunction is represented by commas. Clauses

end with full stops. Variables are implicitly universally quantiﬁed [5].

The following is a universally quantiﬁed Horn clause in ﬁrst order logic, call it C:

∀x, y. p(x) ∨ ¬q(x, y) ∨ ¬r(y)

C can be rewritten as an implication:

∀x, y. p(x) ← q(x, y) ∧ r(y)

A convention in logic programming is to use uppercase letters at the beginning of

the alphabet for variables. Thus, C can be written as the logic program:

p(A) :- q(A, B), r(B).

The negated literals of a Horn clause are called the body. The single positive

literal, if it exists, is called the head. A clause with no head is a constraint. A ground

Horn clause with no body is called a fact.

A substitution of the form θ = {v1/t1, v2/t2, · · · , vn/tn} is an assignment of terms
to variables [8]. The application of a substitution θ to a clause C, written Cθ, replaces

variables in C with their corresponding terms in θ. If C is the clause palindrome(

A):- middle(A, B), empty(B) and θ is the substitution {A/[a, t], B/[]} then

Cθ is the ground clause palindrome([a, t]):- middle([a, t], []), empty
([]). Substitutions can also be applied to variables and clausal theories in the same

way.

2Some logic programming languages such as Prolog allow more than just Horn clauses in their

syntax. To keep things short, this dissertation uses this simpler deﬁnition of a logic program.

7

2.1.2 Semantics

The vocabulary, V, of a logic program is the set of all its constant symbols, function

symbols and predicate symbols. The Herbrand universe is the set of all ground
terms which can be constructed from the constant symbols and function symbols

in V. The Herbrand base is the set of all ground atoms which can be constructed

from the predicate symbols in V and the ground terms in the Herbrand universe. A

Herbrand interpretation is a subset of the Herbrand base. The atoms in a Herbrand
interpretation are treated as true. Any atoms not in the Herbrand interpretation, but

in the Herbrand base, are false [5, 24].

Deﬁnition 1 (Herbrand Model). Suppose C = h ← b1, b2, ..., bn is a clause. A
Herbrand interpretation I is a Herbrand model for C if and only if, for all substitutions
θ, whenever hθ ∈ i, all biθ ∈ I [8].

A Herbrand interpretation I is a Herbrand model of a clausal theory T if I is a
Herbrand model of every clause in T . A clausal theory T entails a clause C, written

T |= C, if every Herbrand model of T is also a Herbrand model of C. A clausal theory

T is satisﬁable if T has a Herbrand model and unsatisﬁable otherwise. A Herbrand

interpretation I is a least Herbrand model for T if I is a Herbrand model for T and,
for all other Herbrand interpretations I (cid:48) such that I (cid:48) is a Herbrand model of T , I ⊆ I (cid:48)
[21].

If C is a constraint we say that C is violated for a clausal theory T , if the body

of the constraint is in any Herbrand model of T . In other words, all of the literals in
C cannot be true. If F is a fact in T then F is in every Herbrand model of T .

A logic L is monotonic if the set of clauses entailed by a clausal theory T ∈ L

cannot be reduced by adding a clause C ∈ L to T . Deﬁnite programs are monotonic

[8].

2.1.3 Logic Programming Languages

Datalog is a logic programming language with a subset of the syntax described

above. Datalog programs consist of deﬁnite clauses. Datalog does not allow nested

compound terms. This means that Datalog programs have a ﬁnite Herbrand base.
Datalog requires that every variable in the head of a clause also appear in the body.

These restrictions cause Datalog programs to be decidable and thus guaranteed to

terminate[5].

8

Prolog is a popular, Turing complete, logic programming language.

It allows

the full syntax described in the syntax section. However, Prolog is not a purely

declarative language [5]. Despite being very expressive and widely used, Prolog can
be quite diﬃcult to reason about precisely. Popper both produces and takes as input

Prolog programs.

Answer Set Programming (ASP) is a type of logic programming which is

Its syntax is
convenient for deﬁning and solving constraint satisfaction problems.
similar to the logic program syntax described above with extensions not discussed in

this dissertation. Popper encodes its CSP problem in ASP.

2.2 Problem Deﬁnitions

2.2.1 Language Bias

The hypothesis space for an ILP system has only been loosely deﬁned thus far. The

total set of possible programs an algorithm could potentially explore is inﬁnite in

most cases. In Example 1.1 it was implicitly assumed that only predicate symbols

which appeared in the background knowledge could be used in a hypothesis. Instead,
most ILP systems use a language bias as a form of inductive bias to precisely deﬁne

their hypothesis space [5].

2.2.1.1 Mode Declarations

The inverse entailment setting [20] originated mode declarations as a means to es-
tablish its language bias. Mode declarations are now used by many ILP systems

[20, 27, 26, 15]. The LFF problem setting uses a diﬀerent but isomorphic set of dec-

larations to deﬁne its language bias. Mode declarations are explained brieﬂy here

because they are important for the computation of bottom clauses. They are also
commonly encountered in ILP literature.

A mode declaration has either the form modeh(r, atom) or modeb(r, atom). The

modeh declarations restrict the form of atoms occurring in the head of a hypothesis

clause and the modeb declarations restrict atoms in the body.

Example 2.1 (Mode declarations). The mode declarations for Example 1.1 might

include:

modeh(*, palindrome(+list)).
modeb(*, last(+list, -element)).
modeb(*, empty(-list)).
modeb(*, reverse(+list, -list)).

9

Including a modeb declaration for reverse gives permission to the ILP system

to use the reverse predicate symbol in the body of a hypothesis.

The parameter r in a mode declaration is the recall. For this dissertation, recall

can be ignored. It defaults to ∗ which enforces no bound on hypotheses.

The parameter atom is a compound term of the form p(t1, t2, · · · , tn). The function
symbol p in a modeh (respectively modeb) declaration indicates that the predicate
symbol p/n, with n being the number of arguments in atom, can occur in the head
(body) of a clause in a hypothesis. The terms ti are constant symbols made up of a
direction in {+, −, #} followed by a type.

The direction symbols +, − and # indicate that an argument must be input,
output, or ground respectively. Input arguments must be variables instantiated in the

head of a clause or another literal of the clause before being used. Output arguments

are variables instantiated by the literal they are used in. Ground arguments must be

terms [5]. Directions create a strong bias by enforcing a relation between literals in a
clause. For instance, the + symbol on the ﬁrst argument of modeb(*, last(+list
, -element)) means that palindrome(A):- last(A, B) is mode consistent but

palindrome(A):- last(C, B) is not. While directions are required for Prolog,

they can be omitted for Datalog.

The type enforces a very simple type system. If a variable or term occurs in an
atom with a mode declaration that assigns it type t1, then it cannot be assigned a
diﬀerent type t2 by the mode declaration of any other literal in the clause.

2.2.1.2 Declaration Deﬁnitions

The language bias of the original LFF problem setting does not require recall, direc-

tions or types. Directions and types are very useful for bounding the computation of

the bottom clause though. The declarations of the LFF problem setting are extended

here to include type and direction declarations.

Deﬁnition 2 (Head declaration). A head declaration is a ground atom of the form

head_pred(p,a) where p is a predicate symbol of arity a.

Deﬁnition 3 (Body declaration). A body declaration is a ground atom of the form

body_pred(p,a) where p is a predicate symbol of arity a.

Deﬁnition 4 (Type declaration). A type declaration is a ground atom of the form

type(p,s) where p is a predicate symbol of arity a and s is a tuple of length a

containing constant symbols.

10

Deﬁnition 5 (Direction declaration). A direction declaration is a ground atom of

the form direction(p,s) where p is a predicate symbol of arity a and s is a tuple

of length a containing only constant symbols in {in, out}.

These expanded declarations are compatible with the LFF problem setting.3 Note
that these declarations can encode the same information as mode declarations, as-

suming recall defaults to ∗ and ground arguments are not allowed. Appendix A has
a program to convert between the declarations of the expanded LFF problem setting

and mode declarations.

Deﬁnition 6 (Language bias). A language bias is a tuple (Lh, Lb, Lt, Ld) with Lh a
set of head declarations, Lb a set of body declarations, Lt a set of type declarations,
and Ld a set of direction declarations.

Example 2.2. Suppose L = {Lh, Lb, Lt, Ld} is an example language bias for Example
1.1. If the bias is somewhat reduced to keep the example simple, it might look like

the following:

Lh = {head_pred(palindrome,1).}

Lb =

Lt =

Ld =















body_pred(first,2).
body_pred(last,2).
body_pred(middle,2).
body_pred(empty,1).






type(palindrome,(list,)).
type(first,(list,element)).
type(last,(list,element)).
type(middle,(list,list)).
type(empty,(list,)).






direction(palindrome,(in,)).
direction(first,(in,out)).
direction(last,(in,out)).
direction(middle,(in,out)).
direction(empty,(out,)).






These deﬁnitions establish the syntax of the language bias. No indication has been

given of the restrictions the declarations actually impose on the hypothesis space. The

ﬁrst step is to deﬁne what it means for a clause to be consistent with a given language
bias.

3The Popper implementation supports type and direction declarations though this is not men-

tioned in the original paper.

11

Deﬁnition 7 (Language bias consistent clause). Let L = (Lh, Lb, Lt, Ld) be a lan-
guage bias and C = h ← b1, b2, · · · , bn be a deﬁnite clause. Then C is language bias
consistent with L if and only if:

• h is an atom of the form p(X1, · · · , Xn) and head_pred(p, n) is in Lh.

• Every bi is an atom of the form p(X1, · · · , Xn) and body_pred(p, n) is in Lb.

• Every Xi is a variable.

• For all variables Z which occur in C, if Z occurs in two literals l1 = p(X1, · · · , Xn)
n ))4

and l2 = q(Y1, · · · , Ym) as Xi and Yj respectively, then type(p, (T p
m)) are in Lt and T p
and type(q, (T q

1 , · · · , T p

1 , · · · , T q

i = T q
j .

• a is an atom of the form p(X1, · · · , Xn) and direction(p, (Dp

1, · · · , Dp

n)) ∈ Ld

then:

– If a = h and Dp

i is out, then one of the following must be true:

∗ the variable Xi must equal some variable Xj in h and Dp
j must be in.
∗ the variable Xi must occur in some bk = q(Y1, · · · , Ym) as some Yj and

Dq

j must be out in direction(q, (Dq

1, · · · , Dq

m)) ∈ Ld.

– If a = bj and Dp
i

is in, then the variable Xi must occur in some c =
m)) ∈

1, · · · , Dq

q(Y1, · · · , Ym) as Yj. There must also be direction(q, (Dq
Ld and either c = h and Dq

j is in or c = bx and Dq

j is out.

We also extend Popper’s deﬁnition of a declaration bias consistent hypothesis to

cover type and direction declarations.

Deﬁnition 8 (Language bias consistent hypothesis). A language bias consistent hy-

pothesis H relative to a language bias L is a deﬁnite clausal theory where each clause

C ∈ H is language bias consistent with L.

Example 2.3 (Language bias consistent hypotheses). Given the language bias L

from Example 2.2, the following hypotheses are language bias consistent with L:

• palindrome(A):- last(A, B), middle(A, C), empty(C).

• palindrome(A):- first(A, B), last(A, B).

4Error: Sp
1

12

• palindrome(A):- empty(A).

palindrome(A):- first(A, C), middle(A, B), last(A, C).

Example 2.4 (Language bias inconsistent hypotheses). Given the language bias L

from Example 2.2, the following hypotheses are not language bias consistent with L:

• palindrome(A):- length(A, B), one(B).

length/2 and one/1 are not allowed as predicate symbols in the body of the

clause because body_pred(length, 2) and body_pred(one, 1) are not in
Lb.

• empty(A):- middle(A, C), empty(C).

empty/1 is not an allowed predicate symbol in the head of the clause because
head_pred(empty, 1) is not in Lh.

• palindrome(A):- first(A), empty(A).

first cannot appear with arity 1 because body_pred(first, 1) is not in
Lb.

• palindrome(A):- first(A, B), empty(B).

The types do not match. B must have type element from type(first, (
list, element)) in Lt. However, type(empty, (list)) in Lt requires B
to have type list5.

• palindrome(A):- last(B, A), first(B, A).

palindrome(A):- middle(A, B), empty(B).

The direction of B in the ﬁrst clause is incorrect. Due to direction(first
,(in,out)) and direction(last,(in,out)) in Ld, B is an input variable.
B does not occur as an output variable in another literal in the body or as an
input variable in the head.

2.2.2 Hypothesis constraints

Bottom preprocessing operates by generating a set of hypothesis constraints for a

meta-level ILP system. The speciﬁc language of these constraints can diﬀer depending
on the target ILP system. Thus, to keep the problem setting general, hypothesis

constraints are deﬁned in terms of how they aﬀect the hypothesis space. The speciﬁc

hypothesis language used by the ⊥-Popper implementation is given in Chapter 4.

5Error: element

13

Deﬁnition 9 (Hypothesis constraint). Let L6 be a language that deﬁnes hypotheses,
i.e. a meta-language. Then a hypothesis constraint is a constraint expressed in L.

Example 2.5. Suppose H is the hypothesis from Example 1.2:

palindrome(A) :- middle(A, B), empty(B).

Suppose the hypothesis language L is ASP with an encoding of hypotheses similar

to the one used by Popper. An explanation of the speciﬁc encoding Popper uses is
described in Chapter 4. For now it suﬃces to know that H could be encoded in

Popper as:

clause(0).
head_literal(0, palindrome, 1, (0,)).
body_literal(0, middle, 2, (0, 1)).
body_literal(0, empty, 1, (1,)).

Suppose clause_size(C, N) holds when the clause C contains exactly N lit-

erals. Then, to banish any hypothesis containing exactly the single clause in H, a
hypothesis constraint, call it C1 could be written as follows:
:- clause(C),

head_literal(C, palindrome, 1, (0,)),
body_literal(C, middle, 2, (0, 1)),
body_literal(C, empty, 1, (1,)),
clause_size(C, 3).

Deﬁnition 10 (Hypothesis Space). Suppose B is a language bias and C is a set of
hypothesis constraints in L. Then the hypothesis space, HB,C, is the set of all bias
consistent, relative to B, hypotheses which, when written in L, do not violate any

constraints in C.

Example 2.6. Suppose B is a language bias and HB is the set of all hypotheses
written in L that are language bias consistent with B. Suppose H1, H2 and H3 are
hypotheses consistent with B deﬁned as follows:

H1 = {palindrome(A):- middle(A, B), empty(B).}
(cid:26) palindrome(A):- middle(A, B), empty(B).
palindrome(A):- length(A, B), one(B).

H2 =

(cid:27)

H3 = {palindrome(A):- length(A, B), one(B).}

Suppose C = {C1} is a set of hypothesis constraints written in L containing the
constraint C1 from Example 2.5. Then {H1, H2, H3} ⊆ HB but H1 (cid:54)∈ HB,C and
H2 (cid:54)∈ HB,C.

6Several errors used LH here

14

Suppose B is a language bias. Suppose C and C (cid:48) are sets of hypothesis constraints.
Then the hypothesis space generated by the combined set C ∪ C (cid:48) is written HB,C∪C(cid:48).
Suppose H is a hypothesis. If H ∈ HB,C∪C(cid:48) then H does not violate any constraint
in C ∪ C (cid:48). Thus H ∈ HB,C and H ∈ HB,C(cid:48). Similarly, it follows that if H ∈ HB,C
and H ∈ HB,C(cid:48) then H ∈ HB,C∪C(cid:48). Thus HB,C∪C(cid:48) = HB,C ∩ HB,C(cid:48).

2.2.3 Problem Input

Having deﬁned the language bias and hypothesis constraints of an LFF problem, it
is now possible to give a full description of an LFF problem input.

Deﬁnition 11 (LFF Problem Input). A problem input is a tuple (B, L, C, E+, E−)
where

• B is a Horn program denoting background knowledge. B has the following

restrictions:

– For all e ∈ E+ ∪ E−, it must be the case that B (cid:54)|= e.

– If p/a is a predicate symbol such that head_pred(p, a) ∈ Lh then p/a

cannot appear in the body of any clause in B.

• L is a language bias.

• H is a set of hypothesis constraints.

• E+ is a set of facts denoting positive examples.

• E− is a set of facts denoting negative examples.

The restrictions on the BK exist to ensure three things:

• A problem is not trivially unsatisﬁable because the BK entails a negative ex-

ample.

• No positive example is irrelevant, as it would be if the BK entailed the example

without a hypothesis.

• A clause in the BK cannot resolve with a clause in the hypothesis. This ensure

a hypothesis is not made recursive due to a clause in the BK.

15

2.2.4 Solutions

With LFF problem input deﬁned, it is now possible to give a formal deﬁnition of

what it means for a hypothesis to be a solution to a problem in the LFF setting.

Deﬁnition 12 (Solution). Given an input tuple (B, L, C, E+, E−), a hypothesis H ∈
HL,C is a solution when

• ∀e ∈ E+. B ∪ H |= e. The solution is complete.

• ∀e ∈ E−. B ∪ H (cid:54)|= e. The solution is consistent.

The ability to handle misclassiﬁed examples, a type of noise, is important in many

types of machine learning [8]. The LFF problem setting, and by extension bottom

preprocessing, assume that all examples are correctly classiﬁed. If noise exists in the
examples, an LFF system might be unable to ﬁnd a solution.

Many meta-level ILP systems will learn an optimal solution if one exists [5]. There

are potentially many ways to deﬁne optimality. In the LFF problem setting, optimal-

ity is deﬁned in terms of program size.

Deﬁnition 13 (Optimal Solution). Let P = (B, L, C, E+, E−) be a problem input.
Let H ∈ HL,C be a hypothesis. Then H is an optimal solution if both:

1. H is a solution to P .

2. for all H (cid:48) ∈ HL,C where H (cid:48) is a solution, the number of literals in H (cid:48) is greater

than or equal to the number of literals in H.

2.3 Generalization and Specialization

Deciding entailment between clauses is undecidable [2]. Due to this, inverse entail-
ment based systems test entailment between clauses using a weaker relation called

subsumption.

Deﬁnition 14 (Clausal Subsumption). A clause C1 subsumes a clause C2 if and only
if there exists a substitution θ such that C1θ ⊆ C2.

Example 2.7 (Subsumption). Suppose the clauses C1, C2, and C3 are deﬁned as
follows:

16

C1 = palindrome(A):- first(A, B), middle(A, C), last(A, B).

C2 = palindrome(A):- first(A, C), middle(A, B).

C3 = palindrome(A):- first(A, B), last(A, B).

Then C3 ⊆ C1 so C3 subsumes C1. C2θ ⊆ C1 with substitution θ = {C/B, B/C}
so C2 subsumes C1. C1 does not subsume either C2 or C3. Finally, C2 does not
subsume C3 and C3 does not subsume C2.

If a clause C1 subsumes C2 then C1 |= C2. However, the converse is not necessarily

true. We cannot conclude that if C1 |= C2 then C1 subsumes C2 [24].

Since most meta-level ILP systems search for entire clausal theories [6], it’s useful

to extend subsumption to clausal theories. The theory that follows this chapter will

also operate mostly at the level of clausal theories rather than clauses.

Deﬁnition 15 (Theory subsumption). A Horn theory T1 subsumes a Horn theory
T2, denoted T 1 (cid:22) T 2, if and only if ∀C2 ∈ T2, ∃C1 ∈ T1 such that C1 subsumes C2.

If T1 and T2 are clausal theories and T1 (cid:22) T2 then T1 |= T2 [7]. Again, the converse

is not necessarily true.

It is worth noting that a single clause can also be treated like a one element clausal

theory. We sometimes say that a clausal theory T is a generalization/specialization
of a clause C. When we do this, we are implicitly comparing T to the clausal theory
TC which is the one element set consisting only of C.

Rather than using subsumption directly, it can be useful to consider generaliza-

tions and specializations of a clausal theory.

Deﬁnition 16 (Generalization). A clausal theory T1 is a generalization of a clausal
theory T2 if and only if T1 (cid:22) T2.

Deﬁnition 17 (Specialization). A clausal theory T1 is a specialization of a clausal
theory T2 if and only if T2 (cid:22) T1.

Finally, it will be useful to deﬁne a most speciﬁc generalization. Note that a theory

which is a most speciﬁc generalization of another theory is not necessarily unique.

Deﬁnition 18 (Most speciﬁc generalization). A clausal theory T1 is a most speciﬁc
generalization of another clausal theory T2 if T1 (cid:22) T2 and for all other clausal theories
T3 such that T3 (cid:22) T2, T3 (cid:22) T1.

17

Chapter 3

Preprocessing Theory

The fundamental idea of this dissertation is to use information about the examples and

background knowledge in an ILP problem to prune its initial hypothesis space. This
can be done by preprocessing the examples to generate a set of additional hypothesis

constraints.

Remark. Throughout this chapter, the BK of an LFF problem input is assumed to

be a deﬁnite program with a ﬁnite Herbrand base.

A consequence of this remark is that the BK is monotonic and does not allow
function symbols, also called function-free. In Chapter 4, methods are described for

handling inﬁnite Herbrand bases in practice.

3.1 Preprocessing Problem

Deﬁnition 19 (Preprocessing problem). Suppose P = (B, L, C, E+, E−) is an LFF
problem input. Suppose |HL,C| = n is the number of hypotheses in the hypothesis
space of P . The preprocessing problem is to generate a set of hypothesis constraints
C (cid:48) such that |HL,C∪C(cid:48)| < n.

Note that the hypothesis space of an LFF problem input is bounded in practice

and thus ﬁnite for a given language bias L [7]. This means it is possible to compute
both |HL,C| and |HL,C∪C(cid:48)|. A description of this bound is given in Subsection 3.2.4.
Ideally the preprocessing problem can be solved without pruning any optimal

solutions from the hypothesis space. In fact, the best possible solution to the pre-

processing problem would also prune any hypothesis that is not an optimal solution

from the hypothesis space.

18

Deﬁnition 20 (Optimal preprocessing). Suppose P = (B, L, C, E+, E−) is an LFF
problem input. Suppose H is a hypothesis in HL,C. The optimal preprocessing prob-
lem is to generate a set of hypothesis constraints C (cid:48) such that H ∈ HL,C∪C(cid:48) if and
only if H is an optimal solution to P .

Suppose C (cid:48) is a set of hypothesis constraints generated by optimal preprocessing
of some LFF problem input P = (B, L, C, E+, E−). It is clear from Deﬁnition 20 that
any system which can compute HL,C∪C(cid:48) can trivially solve P optimally by selecting
any H ∈ HL,C∪C(cid:48).

In other words, optimal preprocessing is a solution to the general ILP problem.

There are many classes of ILP problems which can be encoded as LFF problem input
and are not polynomial time learnable [24].

Perhaps a better approach than optimal preprocessing is to consider cases when

preprocessing is guaranteed not to prune any optimal solutions from a hypothesis

space. First it can be useful to deﬁne soundness for hypothesis constraints.

Deﬁnition 21 (Sound hypothesis constraints). Suppose P = (B, L, C, E+, E−) is an
LFF problem input. Then a set of hypothesis constraints C (cid:48) is sound for P if, for all
H ∈ HL,C such that H is an optimal solution to P , H ∈ HL,C∪C(cid:48).

Sound preprocessing generates sound hypothesis constraints.

Deﬁnition 22 (Sound preprocessing). Suppose P = (B, L, C, E+, E−) is an LFF
problem input. The sound preprocessing problem is to generate a set of hypothesis
constraints C (cid:48) such that C (cid:48) is sound for P .

3.2 Bottom Preprocessing

Preprocessing is an option for any meta-level ILP system that supports hypothesis

constraints. There are potentially many diﬀerent approaches to hypothesis constraints

that could be generated with preprocessing. The remainder of this dissertation focuses
on hypothesis constraints generated from the bottom clauses of the examples in an

ILP problem. This approach, called bottom preprocessing, is deﬁned formally at the

end of this section.

19

3.2.1 Bottom Clauses

Deﬁnition 23 (Bottom clause). Given background knowledge B and a fact e, the
bottom clause of e relative to B, ⊥B(e), is the set of all literals ¬L such that B ∪¬e |=
L.

Bottom clauses can be computed for clauses. However, the LFF problem setting

restricts examples to be facts. Since this dissertation is only concerned with the

bottom clauses of examples, this deﬁnition is restricted to the bottom clauses of facts.

Suppose B is background knowledge, C is a clause and e is a fact. The motivation
for this deﬁnition of the bottom clause is that if B ∪ C |= e then B ∪ ¬e |= ¬C [11].

The bottom clause of a fact e given background knowledge B always exists and is

ﬁnite as long as the following conditions are met [21]:

• The Herbrand base of B is ﬁnite.

• B ∪ ¬e is satisﬁable. i.e. B is consistent and B (cid:54)|= e.

Bottom clauses were introduced as part of the inverse entailment approach to
ILP [20]. Inverse entailment underlies the Progol and Aleph systems as well as many

successors [27, 26, 31].

Deﬁnition 24. A clause C can be derived by inverse entailment from an example e
given background knowledge B if and only if C subsumes ⊥B(e).

Yamamoto noted that inverse entailment might more accurately be called inverse
subsumption to better distinguish it from other approaches that do not use subsump-

tion to invert entailment [31]. This dissertation uses inverse entailment in its original

form as given by Deﬁnition 24.

According to Deﬁnition 23, the bottom clause for the negative example palindrome

([a, t]) from Example 1.1 1 is:

palindrome([a, t]) :- first([a, t],a), last([a, t],t), middle([a,
t],[]), empty([]), length([a, t],2), length([a, t],0), one(1).

This is a more speciﬁc clause than the bottom clause given in Example 1.2. That

is because this clause is ground and thus not bias consistent. A bottom clause can

be lifted to be bias consistent by replacing constants with variables and removing
literals which violate bias declarations. This process results in a generalization of the

bottom clause. The deﬁnition below uses the equivalent formulation of the bottom
clause ⊥(e) as the most speciﬁc clause such that B ∪ ⊥B(e) |= e [28].

1Note that this running example is not function-free.

It is used here purely for illustrative

purposes. Chapter 4 discusses how BK that is not function-free is handled in practice.

20

Deﬁnition 25 (Bias consistent bottom clause). Given an LFF problem input P =
(B, L, C, E+, E−), the bias consistent bottom clause of an example e ∈ E+ ∪ E−
relative to B and L, denoted ⊥B,L(e), is the most speciﬁc bias consistent clause such
that B ∪ ⊥B,L(e) |= e.

Note that the bias consistent bottom clause always exists if the bottom clause

does. In the extreme case, the bias consistent bottom clause is the example fact lifted
to variables. Chapter 4 describes a bottom clause algorithm which ﬁnds the bias

consistent bottom clause of an example directly.

The next proposition is the backbone of the bottom preprocessing approach. It

shows that a hypothesis that generalizes the bottom clause of an example entails,
relative to the background knowledge, the example itself. This is a well known result

in inverse entailment and could be presented as a corollary of the original presentation

[20]. It is re-derived here to align it with the other deﬁnitions and expand the original

result to deﬁnite clausal theories.

Proposition 1. Given an LFF problem input P = (B, L, C, E+, E−) and a deﬁnite
clausal theory T , if e ∈ E+ ∪ E− and T is a generalization of ⊥B,L(e) then B ∪ T |= e.

Proof. Suppose P and T are as stated, e ∈ E+ ∪ E− and T is a generalization of
⊥B,L(e). Then T |= ⊥B,L(e) by the deﬁnition of generalization and the fact that
subsumption implies entailment. ⊥B,L(e) is a deﬁnite clause since it is bias consis-
tent. B and T are also deﬁnite programs by deﬁnition. Since deﬁnite programs are
monotonic, B ∪ T |= B ∪ ⊥B,L(e). B ∪ ⊥B,L(e) |= e by the deﬁnition of the bias
consistent bottom clause so, by transitivity of entailment, B ∪ T |= e.

3.2.2 Negative Bottom Preprocessing

Bottom preprocessing generates two diﬀerent sets of hypothesis constraints. One set

is generated by preprocessing the bottom clauses of negative examples in an LFF

problem. The other set is generated by preprocessing the bottom clauses of positive

examples. This subsection deﬁnes negative bottom preprocessing as the approach
which generates hypothesis constraints from negative examples.

Deﬁnition 26 (Negative bottom preprocessing). Suppose P = (B, L, C, E+, E−) is
an LFF problem input. Suppose H ∈ HL,C. Negative bottom preprocessing generates
a set of hypothesis constraints C (cid:48) such that H ∈ HL,C∪C(cid:48) if and only if, for all examples
e− ∈ E−, H is not a generalization of ⊥B,L(e−).

21

Using Proposition 1, it is straightforward to show that the hypothesis constraints

generated by negative bottom preprocessing are sound.

Proposition 2. Negative bottom preprocessing is sound.

Proof. Suppose P = (B, L, C, E+, E−) is an LFF problem input. Suppose H ∈
HL,C. Suppose C (cid:48) is a set of hypothesis constraints generated by negative bottom
preprocessing of P . To show negative bottom preprocessing is sound, it suﬃces to
show C (cid:48) is a sound set of hypothesis constraints. To show C (cid:48) is a sound set of
hypothesis constraints, it is necessary to show that if H (cid:54)∈ HL,C∪C(cid:48) then H is not an
optimal solution to P .

Suppose H (cid:54)∈ HL,C∪C(cid:48). Since C (cid:48) was generated by negative bottom preprocessing,
it must be the case that H is a generalization of ⊥B,L(e−) for some e− ∈ E−. Any
hypothesis in HL,C is a deﬁnite clausal theory by deﬁnition. By Proposition 1 B∪H |=
e−. Then, by Deﬁnition 12, H is not consistent and cannot be a solution to P .

3.2.3 Positive Bottom Preprocessing

This subsection deﬁnes positive bottom preprocessing, the portion of the bottom pre-

processing approach which generates hypothesis constraints from positive examples
of an ILP problem.

Deﬁnition 27 (Positive bottom preprocessing). Suppose P = (B, L, C, E+, E−) is
an LFF problem input. Suppose H ∈ HL,C. Positive bottom preprocessing generates a
set of hypothesis constraints C (cid:48) such that H ∈ HL,C∪C(cid:48) if and only if, for all examples
e+ ∈ E+, H is a generalization of ⊥B,L(e+).

It would be ideal if positive bottom preprocessing were provably sound. However,

a hypothesis which is not a generalization of the bottom clause of a positive example

may still be a solution to an ILP problem. As noted when subsumption was deﬁned,
for clausal theories T1 and T2, if T1 |= T2 it may not be the case that T1 (cid:22) T2.
The contrapositive then also does not hold. If T1 (cid:54)(cid:22) T2 it may still be the case that
T1 |= T2.

The classic example of the incompleteness of subsumption for inverse entailment

given by Yamamoto [30] was updated to be function-free by Muggleton [21]. It is as
follows:

22

Example 3.1. Suppose B is background knowledge, e+ is a positive example, ⊥B(e+)
is the bottom clause of e+, and H is a hypothesis. Suppose they have the form:

B =






even(0).
zero(0).
even(A):- succ(B, A), odd(B).






e+ = {odd(3):- succ(0, 1), succ(1, 2), succ(2, 3).}

H = {odd(A):- succ(B, A), even(B)}

⊥B(e+) = {odd(A):-zero(B),even(B),succ(B, C),succ(C, D),succ(D, A).}

Then H is not a generalization of ⊥B(e+) but B ∪ H |= e+.

Other approaches to characterizing bottom were devised which could ﬁnd hy-

potheses outside the subsumption lattice between the bottom clause and the empty

clause [26]. This dissertation uses inverse entailment and the bottom clause as given
In future work,
in the original paper [20] and acknowledges their incompleteness.

other techniques to generate hypothesis constraints for positive examples could be

attempted using improvements on inverse entailment.

The incompleteness of inverse entailment means that positive bottom preprocess-
ing is unsound. It is possible, however, to show that positive bottom preprocessing

does not prune a speciﬁc subset of hypotheses deﬁned by relative subsumption [25].

Deﬁnition 28 (Relative subsumption). A clause C1 subsumes a clause C2 relative
to a clausal theory B, written C1 (cid:23)B C2 if there is a substitution θ such that B |=
∀(C1θ → C2) [24].

The ∀ symbol in this deﬁnition quantiﬁes over all possible assignments of ground
terms in the Herbrand universe to the variables in C1θ and C2. An alternative deﬁ-
nition of relative subsumption characterizes it as follows. Suppose T = B ∪ C1 is a
deﬁnite theory. Suppose T (cid:96)c C2 if and only if C2 can be derived by resolution from
T using each clause in T a maximum of one time. Then C1 (cid:23)B C2 if and only if
T (cid:96)c D for some clause D such that D subsumes C2 [22].

Example 3.2 (Relative Subsumption). This example of relative subsumption is given

in Foundations of Inductive Logic Programming [24]. Suppose the following:

C1 = {small(X):- cat(X).}

C2 = {cuddly_pet(X):- fluffy(X), cat(X).}

B =

(cid:26) pet(X):- cat(X).

(cid:27)

cuddly_pet(X):- small(X), fluffy(X), pet(X).

Then C1 (cid:23)B C2.

23

For Horn theories T1, T2 and B, T1 (cid:23)B T2 if, for all clauses C2 ∈ T2, there exists
a clause C1 ∈ T1 such that C1 (cid:23)B C2. Note that if T1 (cid:22) T2 then it is also the case
that T1 (cid:23)B T2 for any Horn theory B. Further, if T1 (cid:23)B T2 then B ∪ T1 |= T2. The
inverse is not true for either case. This means subsumption is weaker than relative

subsumption which is weaker than entailment [24].

Deﬁnition 29 (Subsumption-complete solution). Suppose P = (B, L, C, E+, E−) is
an LFF problem input. Suppose H is a solution to P . H is a subsumption complete
solution if for all e+ ∈ E+ it is the case that H (cid:23)B e+.

Suppose P = (B, L, C, E+, E−) is an LFF problem input. Suppose H ∈ HL,C is
a subsumption-complete solution to P . Then for all e+ ∈ E+, H ∪ B |= e+. Thus
it is correct to deﬁne the set of subsumption-complete solutions to P as a subset of
the set of solutions to P . However, since for some e+ ∈ E+, if H ∪ B |= e+ it may
not be the case that H (cid:23)B e+, the set of solutions and set of subsumption-complete
solutions are not equal.

Deﬁnition 30 (Subsumption sound preprocessing). Suppose P = (B, L, C, E+, E−)
is an LFF problem input. Suppose H ∈ HL,C. Then subsumption-sound preprocessing
generates a set of hypothesis constraints C (cid:48) such that if H is a subsumption-complete
solution to P , H ∈ HL,C∪C(cid:48).

With these deﬁnitions it is possible to show that positive bottom preprocessing

is subsumption-sound. This proof relies on Theorem 5 from Yamamoto [30] which

shows that inverse entailment is complete with respect to relative subsumption if
the example is not a tautology and the background knowledge does not entail the

example.

Proposition 3. Positive bottom preprocessing is subsumption-sound.

Proof. Suppose P = (B, L, C, E+, E−) is an LFF problem input, H ∈ HL,C, and
e+ ∈ E+. Suppose C (cid:48) is generated by positive bottom preprocessing of P . Suppose
H (cid:54)∈ HL,C∪C(cid:48). To show positive bottom preprocessing is sound, it suﬃces to show
that H is not a subsumption-complete solution to P .

Suppose, for contradiction, that H is a subsumption complete solution to P .
There must be some clause D ∈ H such that D (cid:23)B e+. Since e+ is not a tautology
and B (cid:54)|= e+ by the deﬁnition of an LFF problem, Yamamoto’s theorem applies.
This means that D can be found by inverse entailment. It must be the case that D
subsumes ⊥B(e+). This means B ∪ D |= e+. D must be bias consistent if it can be

24

found by inverse entailment. From the deﬁnition of the bias consistent bottom clause,
it must be the case that D subsumes ⊥B,L(e+). But then since e+ is arbitrary, H is
a generalization of ⊥B,L(e+) for all e+ ∈ E+. This contradicts the assumption that
H (cid:54)∈ HL,C∪C(cid:48).

Thus positive bottom preprocessing does not prune a well-deﬁned subset of solu-
tions to an ILP problem. However, there is no guarantee that a subsumption-complete

solution to a problem exists. Thus positive bottom preprocessing may prune all so-

lutions and is not guaranteed to preserve optimality. The degree to which this is a

problem for positive bottom preprocessing is explored experimentally in Chapter 5.

3.2.4 Bottom Preprocessing Deﬁnition

Having deﬁned positive and negative preprocessing, it is possible to give a full deﬁ-

nition of bottom preprocessing.

Deﬁnition 31 (Bottom preprocessing). Suppose P = (B, L, C, E+, E−) is an LFF
problem input. Suppose H ∈ HL,C. Bottom preprocessing generates a set of hypoth-
esis constraints C (cid:48) such that H ∈ HL,C∪C(cid:48) if and only if C (cid:48) = C + ∪ C − where all of
the following hold:

• C − is generated by negative bottom preprocessing of P .

• C + is generated by positive bottom preprocessing of P .

• H ∈ HL,C∪C+ and H ∈ HL,C∪C−.

To determine the number of hypotheses pruned by bottom preprocessing, suppose
that P = (B, L, C, E+, E−) is and LFF problem input and its initial hypothesis
space is unconstrained. Thus C = {}. The set of hypotheses pruned by bottom
preprocessing is then described by:

HL,{} \ (HL,C+ ∩ HL,C−)

In the original LFF paper [7], the unconstrained hypothesis size for an LFF prob-

lem was given an upper bound. Suppose L is a language bias with maximum arity a
for any predicate symbol in Lb or Lh. Suppose v is the maximum number of variables
allowed in a clause, m is the maximum number of body literals allowed in a clause and

25

n is the maximum number of clauses allowed in a hypothesis. Then the maximum
number of hypotheses in HL,{} was proven to be:

n
(cid:88)

j=1

(cid:18)|Lh|va (cid:80)m
i=1
j

(cid:0)|Lb|va
i

(cid:1)
(cid:19)

i=1

(cid:0)|Lb|va
i

The expression |Lh|va (cid:80)m

(cid:1) deﬁnes the size of the set of all clauses that
can exist in a hypothesis in HL,{}. Call that set of clauses c. Deﬁne c+ to be the
subset of clauses in c that generalize ⊥B,L(e+) for at least one e+ ∈ E+. Then deﬁne
c− to be the subset of clauses in c that generalize ⊥B,L(e−) for at least one e− ∈ E−.
(cid:1) − |c+| + |c+ ∩ c−| is the total number of clauses that either
Then |Lh|va (cid:80)m
do not generalize a positive example bottom clause or generalize a negative example
bottom clause.

(cid:0)|Lb|va
i

i=1

Finally, suppose h is the set of all hypotheses H in which all clauses in H are
in c+ but H does not generalize ⊥B,L(e+) for all e+ ∈ E+. Then the total number
of hypotheses pruned by bottom preprocessing can be characterized, very loosely, as
follows:

n
(cid:88)

(cid:18)|Lh|va (cid:80)m

i=1

(cid:0)|Lb|va
i

(cid:1) − |c+| + |c+ ∩ c−|
j

(cid:19)

+ |h|

j=1

In the worst case, all hypotheses generalize the bottom clauses of all positive
(cid:0)|Lb|va
(cid:1)) and no possible clauses generalize
examples (|h| = 0 and |c+| = |Lh|va (cid:80)m
i
the bottom clause of a negative example (|c−| = 0). As long as that is not the case, it
is safe to assume that bottom preprocessing will prune some hypotheses from HL,{}.

i=1

3.3 Generalization Relations

Underlying bottom preprocessing is a generalization relation based on subsumption.
Subsumption testing between clauses is NP-complete [8]. Chapter 5 shows experi-

mentally that the full subsumption check can sometimes signiﬁcantly increase learning

times for hard ILP problems. The remainder of this chapter proves ways in which

the generalization relation used by bottom preprocessing can be weakened without
pruning any extra solutions.

Chapter 2 used the symbol (cid:22) to represent subsumption. The generalization rela-
tions discussed here use the symbols (cid:22)s and (cid:22)c to indicate that they are a weakened
form of subsumption. Deﬁnitions of soundness and completeness are once again given,
this time for generalization relations.

26

Deﬁnition 32 (Subsumption-sound generalization relation). A generalization rela-
tion (cid:22)s is subsumption-sound if for all clausal theories A, B such that A (cid:22)s B, A (cid:22) B.

Deﬁnition 33 (Subsumption-complete generalization relation). A generalization re-
lation (cid:22)c is subsumption-complete if for all clausal theories A, B such that A (cid:22) B,
A (cid:22)c B.

3.3.1 Negative Bottom Preprocessing

It is now possible to extend Proposition 2 to show that negative bottom preprocessing
is still sound if a subsumption-sound generalization relation (cid:22)s is used.

Proposition 4. Suppose P = (B, L, C, E+, E−) is an LFF problem input. Suppose
(cid:22)s is a subsumption-sound generalization relation. Suppose C (cid:48) is a set of hypothesis
constraints generated by negative bottom preprocessing with (cid:22)s deﬁning generalization.
Then C (cid:48) is sound.

Proof. Suppose P , (cid:22)s, and C (cid:48) are as stated. Suppose e− ∈ E− and H ∈ HL,C.

Suppose H (cid:54)(cid:22)s ⊥B,L(e−). Then H ∈ HL,C∪C(cid:48). C (cid:48) is sound regardless of whether

H is a solution to P .

Suppose H (cid:22)s ⊥B,L(e−). Then H (cid:54)∈ HL,C∪C(cid:48). To show C (cid:48) is sound, it is suﬃcient
to show that H is not a solution to P . Since (cid:22)s is sound, H must be a generalization
of ⊥B,L(e−). In Proposition 2, it was shown that H cannot a solution to P and thus
C (cid:48) is sound.

Proposition 4 means that negative bottom preprocessing is sound as long as a
sound generalization relation is used. This means the generalization relation is free

to be incomplete and can misclassify some generalizations as non-generalizations. The

most extreme version of this might be a generalization relation that returns false, i.e.

it assumes nothing is a generalization.

3.3.2 Positive Bottom Preprocessing

Proposition 3 can be extended to show that positive bottom preprocessing is still
subsumption-sound if it uses a complete generalization relation (cid:22)c.

Proposition 5. Suppose P = (B, L, C, E+, E−) is an LFF problem input. Sup-
pose (cid:22)c is a subsumption-complete generalization relation. Suppose C (cid:48) is a a set
of hypothesis constraints generated by positive bottom preprocessing with (cid:22)s deﬁning
generalization. Then C (cid:48) is subsumption-sound.

27

Proof. Suppose P , (cid:22)c, and C (cid:48) are as stated. Suppose e+ ∈ E+ and H ∈ HL,C.

Suppose H (cid:22)c ⊥B,L(e+). Then H ∈ HL,C∪C(cid:48). C (cid:48) is subsumption sound regardless

of whether H is a solution to P .

Suppose H (cid:54)(cid:22)c ⊥B,L(e+). Then H (cid:54)∈ HL,C∪C(cid:48). Since (cid:22)c is subsumption-complete,
H is not a generalization of ⊥B,L(e+). In Proposition 3 it was shown that if H is a
subsumption complete solution to P then H must a generalization of ⊥B,L(e+). Thus
H is not a subsumption complete solution to P and C (cid:48) is sound.

From Proposition 5 it suﬃces to use a complete deﬁnition of generalization to en-

sure positive bottom preprocessing does not prune subsumption-complete solutions.
it
In the extreme case a complete generalization relation might return true, i.e.

assumes everything is a generalization. Chapter 4 shows that useful complete gener-

alization relations are harder to compute than sound but incomplete generalization

relations. Experiments in chapter 5 demonstrate that even hypothesis constraints
generated by bottom preprocessing of positive examples using an incomplete deﬁni-

tion of generalization still prune very few solutions on a variety of problems.

28

Chapter 4

⊥-Popper Implementation

⊥-Popper implements bottom preprocessing for the meta-level ILP system Popper [7].

This chapter describes the implementation of ⊥-Popper. While ⊥-Popper is a speciﬁc
implementation, the techniques described here could be adapted to other meta-level

ILP systems that support hypothesis constraints.

Remark. The background knowledge in this chapter is assumed to be written in Pro-

log. It is not guaranteed to have a ﬁnite Herbrand base.

Prolog also supports negation as failure. This can cause Prolog programs to be
non-monotonic. It is assumed that the BK is written such that it is a deﬁnite program.

4.1 Overview

⊥-Popper takes an LFF problem input of the form (B, L, C, E+, E−) as given in
Deﬁnition 11. It generates a set of additional hypothesis constraints C (cid:48) using bottom
preprocessing. Then it passes a new LFF problem input (B, L, C ∪ C (cid:48), E+, E−) to
Popper for solving and returns the result. Thus ⊥-Popper functions as a wrapper

around Popper.

The interface between ⊥-Popper and Popper is deﬁned by the hypothesis language

L introduced in Deﬁnition 9. All together this means that ⊥-Popper is agnostic to

the implementation details of Popper. A brief explanation of how Popper works is

given in Chapter 6.

4.1.1 High Level Algorithm

A simpliﬁed version of the ⊥-Popper algorithm is shown in Algorithm 1. ⊥-Popper

consists of four distinct components which are described in separate sections:

29

Algorithm 1 The ⊥-Popper algorithm.
1: g ← gen rels
2: h ← hyp constraints
3: function ⊥-Popper(e+, e−, B, L, C, P )
b ← bc prog(e+, e−, B, L, P )
4:
b(cid:48) ← bc enc(b, P )
5:
C (cid:48) ← C ∪ b(cid:48) ∪ g ∪ h
6:
return Popper(e+, e−, B, L, C (cid:48), P )
7:
8: end function

1. hyp_constraints: Positive and negative bottom preprocessing hypothesis

constraints in L which depend on a generalization relation.

2. gen_rels: A generalization relation in L written in terms of bottom clause

encodings.

3. bc_enc: A program which generates encodings in L of a set of bottom clauses.

4. bc_prog: A Prolog program which computes the bias consistent bottom clauses

of all examples in an ILP problem.

In addition to LFF problem input, ⊥-Popper expects parameters P of the form
(max_vars, max_literals, max_clauses). These parameters limit respectively

the number of variables in a hypothesis, the number of literals in a clause, and the

number of clauses in a hypothesis. Only max_vars is used by ⊥-Popper. The re-

maining arguments are passed on to Popper without inspection.

4.1.2 L: Hypothesis Language

Hypothesis constraints in Popper are written in ASP. This gives ⊥-Popper the full

expressivity of ASP in which to write hypothesis constraints, generalization relations,

and bottom clause encodings.

Popper encodes guessed hypotheses in ASP using the following predicate symbols:

head_literal/2, body_literal/2. ⊥-Popper prunes the hypothesis space by

encoding constraints on facts involving these predicate symbols and clause/1. This

section brieﬂy describes the encoding. The details of the full encoding can be found
in chapter 4 of the Popper paper [7].

Suppose H is the current hypothesis guessed by Popper and H consists of clauses
{C1, C2, · · · , Cn}. Then H corresponds to a set of facts F in L encoding each Ci. It
must be the case that n is less than max_clauses and clause(i) ∈ F for each Ci

30

automatically. Suppose X = [V0, V1, · · · , Vn] is a list of the variables in Ci sorted by
their variable name. Then θ = {V0/0, V1/1, · · · , Vn/n} is a substitution of each Vi for
its index i in X. Suppose Ciθ is of the form h ← b1, b2, · · · , bn. Then Ciθ is encoded
in F as follows:

• If h is of the form p(x0, x1, · · · , xn) then head_literal(idCi, p, n, (x0, x1, · · · , xn)) ∈

F .

• For each bi of the form p(x0, x1, · · · , xn), body_literal(idCi, p, n, (x0, x1, · · · , xn)) ∈

F .

Example 4.1. Example 2.5 already demonstrated this encoding. It is brieﬂy repeated

here. Suppose the hypothesis H is:

palindrome(A) :- middle(A, B), empty(B).

Then the encoding in L of H is:

clause(0).
head_literal(0, palindrome, 1, (0,)).
body_literal(0, middle, 2, (0, 1)).
body_literal(0, empty, 1, (1,)).

4.1.3 Bottom Clause Encoding

⊥-Popper encodes multiple bottom clause variants for each bottom clause. The details
of how bottom clause variants are chosen is given in Section 4.4. Here bottom clause

variants are deﬁned and an explanation of how they are encoded is given.

Deﬁnition 34 (Bottom clause variant). Suppose P = (B, L, C, E+, E−) is an LFF
problem input. Suppose e ∈ E+ ∪ E−. A variant of the bottom clause ⊥B,L(e) is a
bias consistent clause D such that D subsumes ⊥B,L(e).

A bottom clause variant can be encoded in L as a set of facts in ASP. Suppose
P = (B, L, C, E+, E−) is an LFF problem input and D is a variant of ⊥B,L(e) for some
example e ∈ E+ ∪ E−. Suppose id⊥(e) is a unique identiﬁer for ⊥B,L(e). Suppose idD
is a unique identiﬁer for D. Suppose X = [V0, V1, · · · , Vn] is a list of the variables in
D sorted by their variable name. Then θ = {V0/0, V1/1, · · · , Vn/n} is a substitution
of each Vi for its index i in X. Suppose Dθ is of the form h ← b1, b2, · · · bn. Then Dθ
can be encoded as a set of ground facts F in the hypothesis language L as follows:

• If e ∈ E+, pos_bottom_clause(id⊥(e)) ∈ F . Otherwise when e ∈ E−,

neg_bottom_clause(id⊥(e)) ∈ F .

31

• bottom_clause_variant(id⊥(e), idD) ∈ F .

• If the atom h,

the head of Dθ,

is of the form p(x0, x1, · · · , xn) then
bottom_clause_head(idD, p, (x0, x1, · · · , xn)) ∈ F . Since D is deﬁnite, h al-
ways exists. Note that p is a predicate symbol and (x1, x2, · · · , xn) is a tuple of
integers.

• For each atom bi of

the form p(x0, x1, · · · , xn)

in the body of Dθ,

bottom_clause_body(idD, p, (x0, x1, · · · , xn)) ∈ F .

Example 4.2. Suppose ⊥B,L(e) is the bottom clause of the negative example
palindrome([a, t]) from Example 1.2:

palindrome(A) :- first(A,B), last(A,C), middle(A,D), empty(D),

length(A,F), length(D,G), one(E).

Suppose the variant D is equal to the entire bottom clause ⊥B,L(e). The unique

identiﬁer id⊥(e) might be neg_1 and idD might be neg_1_1. Then Dθ is:

palindrome(0) :- first(0,1), last(0,2), middle(0,3), empty(3),

length(0,5), length(3,6), one(4).

The encoding of D in L is then:

neg_bottom_clause(neg_1).
bottom_clause_variant(neg_1, neg_1_1).
bottom_clause_head(neg_1_1, palindrome, (0,)).
bottom_clause_body(neg_1_1, first, (0,1,)).
bottom_clause_body(neg_1_1, last, (0,2,)).
bottom_clause_body(neg_1_1, middle, (0,3,)).
bottom_clause_body(neg_1_1, empty, (3,)).
bottom_clause_body(neg_1_1, length, (0,5,)).
bottom_clause_body(neg_1_1, length, (3,6,)).
bottom_clause_body(neg_1_1, one, (4,)).

4.2 hyp_constraints: Hypothesis Constraints

The hypothesis constraints for ⊥-Popper are straightforward encodings into ASP
of bottom preprocessing as given in Deﬁnition 31. This ASP code is given in Algo-

rithm 2. The symbol _ is an anonymous variable. If generalizes_bottom_clause

(_, D) holds then there is some clause Cl in the current guessed hypothesis such that

generalizes_bottom_clause(Cl, D). In other words if generalizes_bottom_clause
(_, D) holds, the current hypothesis H is a generalization of D.

32

Algorithm 2 ASP code for the component hyp_constraints

1 % The following is constraint C−
2 :-

neg_bottom_clause(D),
generalizes_bottom_clause(_, D).

3

4

5

6 % The following is constraint C+
7 :-

8

9

pos_bottom_clause(D),
not generalizes_bottom_clause(_, D).

Proposition 6. Suppose P = (B, L, C, E+, E−) is an LFF problem input. Suppose
H ∈ HL,C is the current hypothesis encoded by L. Suppose neg_bottom_clause(D)
holds if and only if D is ⊥B,L(e−) for some e− ∈ E−. Suppose pos_bottom_clause
(D) holds if and only if D is ⊥B,L(e+) for some e+ ∈ E+. Then the following are
true:

• If generalizes_bottom_clause/2 is subsumption-sound, hypothesis constraint

C − is sound.

• If generalizes_bottom_clause/2 is subsumption-complete, hypothesis con-

straint C + is subsumption-sound.

• If generalizes_bottom_clause/2 is both subsumption-sound and subsumption-
complete, the hypothesis constraints in hyp_constraints are subsumption-

sound.

Proof. Suppose P , neg_bottom_clause/1, pos_bottom_clause/1, and H are as

stated.

Suppose generalizes_bottom_clause/2 is subsumption-sound. Suppose D
is ⊥B,L(e−) for some e− ∈ E−. Then neg_bottom_clause(D) holds. Suppose
generalizes_bottom_clause(_, D) holds. This means C − is violated. So H (cid:54)∈
Inversely, since H ∈ HL,C, if H (cid:54)∈ HL,C∪C− then C − must be violated.
HL,C∪C−.
So generalizes_bottom_clause(_, D) must hold. Taking the contrapositive, we
have that for all e− ∈ E−, H ∈ HL,C∪C− if and only if ¬ generalizes_bottom_clause
(_, D). So by Proposition 4, C − is sound.

Suppose generalizes_bottom_clause/2 is subsumption-complete. Very simi-
lar reasoning shows that C + is a constraint such that for all e+ ∈ E+, H ∈ HL,C∪C+

33

if and only if generalizes_bottom_clause(_, D). So, by Proposition 5, C + is
subsumption-sound.

Suppose generalizes_bottom_clause/2 is both subsumption-sound and subsumption-

complete. Then C − is sound and C + is subsumption-sound. Suppose H is a sub-
sumption complete solution to P . Since C − is sound and H is a solution to P ,
H ∈ HL,C∪C−. Since C + is subsumption-sound and H is a subsumption-complete
solution to P , H ∈ HL,C∪C+. So H ∈ HL,C∪C−∪C+ and the hypothesis constraints in
hyp_constraints are subsumption-sound.

Proposition 6 provides a speciﬁcation for the rest of the implementation of ⊥-

Popper. Ideally the following things should be true:

1. neg_bottom_clause(D) holds whenever D is the bottom clause of a negative

example.

2. pos_bottom_clause(D) holds whenever D is the bottom clause of a positive

example.

3. generalizes_bottom_clause(_, D) is a sound and complete generalization

relation. It suﬃces that generalizes_bottom_clause(C, D) is a subsumption-

sound and subsumption-complete generalization relation between clauses.

Item 3 is computationally challenging since subsumption checking is NP-complete.

The next section on gen_rels describes a subsumption-sound implementation of

generalizes_bottom_clause/2. The section on bc_enc shows how to increase
the coverage of generalizes_bottom_clause/2 toward subsumption-completeness

by encoding many variants of the bottom clause.

4.3 gen_rels: Generalization Relations

The component gen_rels encodes a generalization relation between clauses in
ASP. If a clause C1 generalizes a clause C2 then there is a substitution θ such that
C1θ ⊆ C2. Suppose that substitution is ignored and it is assumed that θ = {}. Then
C1θ = C1. If C1 ⊆ C2, it must be the case that for all literals l ∈ C1, l ∈ C2. This
test turns out to be ineﬃcient to encode in ASP. Instead, it is easier to test whether
there exists some literal l in C1 such that l (cid:54)∈ C2. For this reason gen_rels tests
whether C1 ⊆ C2 by checking if ¬(C1 (cid:54)⊆ C2).

A generalization relation which ignores substitution is subsumption-sound. Sup-
pose C1 and C2 are clauses and sound_gen/2 is a generalization relation such that

34

Algorithm 3 ASP code for the component gen_rels

1 generalizes_bottom_clause(Cl, D) :-

2

3

4

clause(Cl),
bottom_clause_variant(D, V),
not does_not_generalize_bottom_clause_variant(Cl, V)

5
6 does_not_generalize_bottom_clause_variant(Cl, V):-

7

8

bottom_clause_variant(_, V),
head_literal(Cl, P, _, Vars),
not bottom_clause_head(V, P, Vars).

9
10 does_not_generalize_bottom_clause_variant(Cl, V):-

11

12

13

bottom_clause_variant(_, V),
body_literal(Cl, P, _, Vars),
not bottom_clause_body(V, P, Vars).

sound_gen(C1, C2) if and only if ¬(C1 (cid:54)⊆ C2). Then whenever sound_gen(C1, C2)
holds, C1θ ⊆ C2 for the substitution θ = {}. Thus C1 generalizes C2.

The generalization relation sound_gen/2 is far from complete though. The next

section describes a component bc_enc which generates a set of bottom clause variants

from a bottom clause. Suppose bc_enc is a function such that if ⊥ is a bottom clause,
C is a clause and θ is some substitution, Cθ ⊆ ⊥ if and only if C ⊆ ⊥1 for some bot-
tom clause variant ⊥1 ∈ bc_enc(⊥). Say that bc_enc is complete in this case. Sup-
pose complete_gen/2 is a generalization relation such that complete_gen(C, ⊥)
if and only if ¬(C (cid:54)⊆ ⊥1) for some ⊥1 ∈ bc_enc(⊥). Then complete_gen/2 is
a subsumption-complete generalization relation if and only if bc_enc is a complete
encoding of bottom clause variants.

The ASP code for the component gen_rels is given in Algorithm 3. When a bot-

tom clause is encoded as a single variant equal to itself, generalizes_bottom_clause

/2 is equivalent to sound_gen/2. Thus generalizes_bottom_clause/2 is subsumption-
sound in that case. When bc_enc is complete then generalizes_bottom_clause

/2 is equivalent to complete_gen/2. Thus the subsumption-completeness and subsumption-

soundness of gen_rels depend on the bottom clause variants encoded by bc_enc.

4.4 bc_enc: Bottom Clause Encoding

bc_enc is a component which generates multiple bottom clause variants from each

bottom clause and encodes them in L. Ideally this approach can ensure that gen_rels

is both subsumption-sound and subsumption-complete.

35

When ⊥B(e) for a fact e is lifted to variables, it is assumed that any co-occurences
of the same ground term in ⊥B(e) should be lifted to the same variable. For instance,
a ground atom middle([], []) might be lifted to the atom middle(A, A).
If
⊥B,L(e) is then palindrome(A):- middle(A, A), the clause palindrome(A):-
middle(A, B) generalizes ⊥B,L(e) with the substitution θ = {B/A}.

The original Progol implementation handled this case by performing a technique

called variable splitting during its search [20]. Later systems such as Aleph and ATOM
perform variable splitting on the bottom clause instead [1].

To describe variable splitting it is ﬁrst necessary to deﬁne the direction of a variable
in a literal. Suppose L is a language bias. Suppose C is a clause. Suppose Vi is a
variable occurring in a literal a ∈ C with form p(V1, V2, · · · , Vn). Suppose there is a
direction declaration of the form direction(p, (D1, D2, · · · , Dn)) ∈ Ld. Then Vi has
direction Di in a. As per the deﬁnition of direction declarations, Di is either in or
out.

Variable splitting a clause C under L generates a clause CV S according to the

following algorithm:

1. Initialize CV S to be C.

2. Suppose CV S has the form h ← b1, b2, · · · , bn. A variable X is splittable if X
occurs in h or X occurs with direction out in some bi. Find two co-occurrences
of a splittable variable V in two literals a1 and a2 in CV S. If a1 = a2 then V
must occur as arguments i and j where i (cid:54)= j. If no co-occurences of splittable
variables exist, return CV S.

3. Replace the occurrence of V in a2 with a new variable V (cid:48) to create a literal a(cid:48)
2.
Replace b in CV S with b(cid:48). Also add V (cid:48) to [[V ]], the equivalence class of V .

4. For all literals bi with V having direction in in bi, replace V with V (cid:48) in a copy

bi to create bn+1. Add bn+1 to CV S. Return to step 2.

Variable splitting ensures that all variables with direction out in CV S are unique.
It can also add a large number of extra literals to the bottom clause when a copied

literal is split further.

Aleph and ATOM add equality literals V = V (cid:48) to the bottom clause if variable V (cid:48)
was added to the equivalence class of V during variable splitting. Thus the example
C =palindrome(A):- middle(A, A) might become CV S =palindrome(A):-
middle(A, B), A = B after variable splitting. Then any subset of the literals in
CV S is logically equivalent to a generalization of C.

36

bc_enc takes a slightly diﬀerent approach. Suppose C is a clause and CV S is C
after variable splitting. Suppose V = {V0, V1, · · · , Vn} is the set of all unique variables
in CV S. bc_enc encodes a bottom clause variant for each possible substitution θ of
the variables in V to a subset of the variables in V where Vi/Vk and Vj/Vk in θ only
if Vj ∈ [[Vi]]. In other words Vi and Vj can only map to the same variable in θ if they
were added to the same equivalence class during variable splitting.

When working on this, I was under the impression that variable splitting would
make bc_enc complete for subsumption. Speciﬁcally, that the following would be

true. Suppose C and D are bias consistent clauses for some language bias L. Suppose
DV S is D after undergoing variable splitting. If Cθ ⊆ D for some substitution θ then
there exists a substitution θ(cid:48) such that C ⊆ DV Sθ(cid:48).

However, there is a counterexample to this in a paper by Tamaddoni-Nezhad

and Muggleton discussing the completeness of Progol’s reﬁnement operator [28].
Suppose D is palindrome(A):- middle(A, A) and DV S is then palindrome(
A):- middle(A, B) according to the variable splitting algorithm. Suppose D(cid:48) =
palindrome(A):- middle(A, B), middle(B, A). Then D(cid:48) subsumes D but D(cid:48) (cid:54)⊆
DV Sθ for any substitution θ. It can be argued that D(cid:48) is redundant because it is log-
ically equivalent to D and thus it would never appear in an optimal solution.

This example still brings into question the completeness of this bottom clause
I was unable to complete a proof I felt was strong enough to

encoding approach.

include here. Experiments in Chapter 5 show that if generalizes_bottom_clause

/2 is not a complete generalization relation with this implementation of bc_enc,

it at least works well in practice. In the limitations section of Chapter 7, I brieﬂy
describe several ways the implementation of a generalization relation for positive

bottom preprocessing could be changed to overcome this possible limitation.

bc_enc encodes bottom clause variants by the following steps:

1. For each bottom clause ⊥B,L(e) where e ∈ E+ ∪ E−, generate the variable split

version of the bottom clause, ⊥V S

B,L(e).

2. Find the set T of all possible substitutions of ⊥V S

B,L(e).

3. Encode a bottom clause variant in L by computing ⊥V S

B,L(e)θ for each substitu-

tion θ ∈ T .

In many cases, with enough examples, a target hypothesis can be found without

variable splitting. However, there are certain problems for which a target hypothesis

cannot be found without variable splitting [28]. ⊥-Popper can be run with or without

37

variable splitting enabled. If variable splitting is disabled, ⊥V S
B,L(e) = ⊥B,L(e) and the
substitutions in T are one-to-one. Experiments in Chapter 5 show that variable

splitting can dramatically increase learning times and is frequently unnecessary to
ﬁnd an optimal solution.

4.4.1 Optimization of Substitutions

Suppose a variable split bottom clause ⊥V S contains n variables (V0, V1, · · · , Vn).
Then there are (cid:0)2n
There are several optimizations that can reduce the number of substitutions that need

(cid:1) possible substitutions of the n variables to some subset of (V0, V1, · · · , Vn).

n

to be considered.

The restriction described in the previous section must be obeyed for soundness.
If Vi/Vk and Vj/Vk are in some substitution θ then Vi and Vj must be assigned to
the same equivalence class during variable splitting. In the worst case, all variables

are in the same equivalence class and this does not change the number of variants

generated.

Suppose max_vars is j for some problem. Suppose body_literal(Cl, P, N,
Vars) is a body literal guessed as part of a hypothesis H by Popper. Then Vars is a

tuple of integers. Call these variable indices. Popper’s guessing is bounded such that

it is not possible that there exists a variable index i ∈ Vars such that i ≥ j. Then

it is not necessary to consider any substitution θ where the encoding of ⊥θ would
contain a variable index greater than j − 1. This reduces the number of substitutions
that need to be considered to (cid:0)n+j
(cid:1). In fact, as a further optimization, all literals
l ∈ ⊥V S containing a variable Vi such that Vi (cid:54)∈ θ can be dropped from ⊥V Sθ.

j

Suppose head_literal(C, f, N, Vars) is the head of the current hypothesis
H in Popper. The variables in Vars are always assigned ﬁxed indices 0, 1, · · · , N .

Thus, the variables in the head of a bottom clause variant ⊥ can also be ﬁxed.

Substitutions which permute these variables do not need to be considered. This

reduces the number of substitutions that need to be considered for a bottom clause
(cid:1).
with n variables, max_vars j, and a head predicate symbol with arity h to (cid:0)n+j−h
Since j is frequently defaulted to 5 in Popper and h is at least 1 and very frequently 2

j−h

or greater, the number of variants can become manageable with these optimizations.

Finally, taking a subset of the bottom clause might cause a bottom clause variant
to no longer be bias consistent. Speciﬁcally, it can violate direction declarations.

Thus, any substitution which would generate a variant that is not bias consistent can

be ignored. The reduction this causes in the number of substitutions considered is

diﬃcult to compute since it depends heavily on the language bias provided.

38

All of these optimizations are implemented in ⊥-Popper by an ASP program.

Given a clause and a set of constraints encoding these restrictions, the ASP program

generates all necessary substitutions of size max_vars. The code that initializes and
runs this ASP program is written in Python. It can be found in Appendix C

Suppose some bottom clause variant D contains k literals. Then the encoding of
D in L contains k + 1 facts. There is also one fact encoded per e ∈ E+ ∪ E−. Suppose
V is the set of bottom clause variants generated by bc_enc and m is the maximum
number of literals in any bottom clause variant in V . Then the maximum number of
facts encoded in L by bottom clause encoding is (m + 1)|V | + |E+ ∪ E−|.

On a small technical note, a practical optimization in the implementation of Pop-

per ensured that Popper never generated hypotheses in which the same variable oc-
curred twice in a literal. This actually compromises the optimality of Popper. For

instance, with this optimization, Popper was unable to ﬁnd the simplest program for

a palindrome: palindrome(A):- reverse(A, A). This optimization would have

made variable splitting unnecessary in many cases and possibly aﬀected the experi-
ments in the next chapter. So this optimization was disabled in ⊥-Popper and the

version of Popper used for experiments in Chapter 5.

4.5 bc_prog: Bottom Clause Generation

So far this chapter has shown how subsumption-sound hypothesis constraints can

be generated from the bottom clauses of examples. It remains to describe the last

component, bc_prog, which constructs a bottom clause for each example in an LFF

problem input.

When the BK of the problem is a Datalog program, the bottom clause of an

example can be computed by ﬁnding the least Herbrand model of B ∪ ¬e [8]. Since
B ∪⊥B(e) |= e it must be the case that B ∪¬e |= ¬⊥B(e). If the least Herbrand model
of B ∪¬e is a set of ground facts b1 ∧b2 ∧· · ·∧bn then ⊥B(e) is ¬(¬e∧b1 ∧b2 ∧· · ·∧bn).
This means ⊥B(e) is the clause e ← b1, b2, ...bn. Note that the use of negation in this
manner is only valid because e and all bi are facts. This algorithm could be called
bottom clause construction with forward chaining, or BCF C in brief, since the least
Herbrand model of a Datalog program can be computed using a forward chaining
algorithm [8].

Two problems exist with BCF C when the BK is written in Prolog. Since Prolog is
not guaranteed to have a ﬁnite Herbrand base, the computation of the least Herbrand

39

model can be inﬁnite. Forward chaining can also fail if clauses in the BK are not range-

restricted, i.e they contain variables in the head that do not exist in the body. Since

Popper allows Prolog BK, ⊥-Popper instead uses the bottom clause construction
algorithm given in the original inverse entailment paper [20], call it BCIE. BCIE has
the beneﬁts of being more eﬃcient than BCF C and computing the bias consistent
bottom clause of an example directly. However, BCIE is signiﬁcantly more complex
than BCF C.

bc_prog implements BCIE in approximately 200 lines of Prolog. Since BCIE is
well studied and the proof of its correctness is given in the original paper, a complete

description is omitted here. The code used to perform bottom clause construction in

⊥-Popper can be found in Appendix B. Appendix A also provides a program which
converts the LFF language bias given in Deﬁnition 6 into mode declarations. The fact

that bc_prog constructs bias consistent bottom clauses then follows directly from the
proof that BCIE constructs mode consistent bottom clauses.

BCIE ensures its construction of a bottom clause will terminate by bounding it

using a concept called variable depth.

Deﬁnition 35 (Variable Depth). If C is a clause and V = {v1, v2, · · · , vn} is the
set of unique variables occurring in C, then the variable depth of a variable vi ∈ V ,
depth(vi), is deﬁned as follows:

• depth(vi) is 0 if vi occurs in the head of C.

• Otherwise, let Avi be the set of atoms in C in which vi occurs. Let U =
{u1, u2, · · · , un} be the set of variables occurring in the atoms in Avi. Suppose
d is the minimum, for all ui ∈ U such that ui (cid:54)= vi, of depth(ui). Then depth(vi)
is d + 1.

The computational complexity of bottom clause construction is exponential in the

value of max_variable_depth [20]. However, setting the value too low can exclude
literals from the bottom clause. Aleph [27], a popular implementation of Progol in

Prolog, defaults max_variable_depth to 3.

⊥-Popper defaults max_variable_depth for BCIE to max_vars−1 in bc_prog.

The following proposition shows why this is a sound choice.

Proposition 7. If C is a clause and V = {v1, v2, · · · , vn} is the set of unique variables
occurring in C, the maximum variable depth of any variable vi ∈ V is |V | − 1.

40

Proof. Suppose C and V are as stated and |V | = k. Suppose, for contradiction,
vi ∈ V has depth(vi) = k. Then there must be some chain of unique variables
L = l0, l1, · · · , lk−1 such that for each j between 0 and k − 1, depth(lj) = j, lj ∈ V
and lj (cid:54)= vi. The length of L is k which means C must contain k + 1 unique variables.
This is a contradiction.

Suppose M is the set of mode declarations in a language bias L, j+ and j−
are the maximum number of input and output variables respectively in any mode

declaration in M , r is the maximum recall in any mode declaration in M , and i

is max_variable_depth. As stated in the original paper [20], the computational
complexity of BCIE is bounded by (j+j−r|M |)ij+. Thus bottom clause construction
in ⊥-Popper can theoretically scale poorly in the size of the language bias or the

value of max_vars. In practice, this is rarely a problem as will be shown in chapter
5. Usually max_vars defaults to 5 in Popper problems so i = 4. Typically, j+ ≤ 2.
Since recall defaults to ∗, r is potentially inﬁnite but the term j+j−r is also bounded
by the number of true atoms in the least Herbrand model at depth i which is typically

fairly small.

4.6 Recursion Constraints

Proposition 7 proves that no hypothesis Popper produces can contain a clause

with a variable having variable depth greater than max_vars - 1. However, if a

hypothesis can self-resolve, then the result of multiple self-resolutions might produce a
clause with variables having higher variable depths than the total number of variables

in the theory. For this reason, ⊥-Popper prunes recursive theories more cautiously

than other theories.

contains

additional

hypothesis

Algorithm 4

loosen
generalizes_bottom_clause/2 in the case of problems which allow recursion.
Suppose C is a clause of the form h ← b1, · · · , bn in which some br, with 1 ≤ r ≤ n,
has the same predicate symbol as h. Then C is a recursive clause. Suppose k is the
minimum variable depth of the input variables in br. Say that the literal bi precedes
recursion if depth(Vj) < k for all output variables Vj in bi.

constraints which

The constraints in Algorithm 4 add an additional case to generalizes_bottom_clause

/2 when the bottom clause is constructed from a positive example and the prob-

lem supports recursion.
In this case, for a clause C and a positive bottom clause
variant D, generalizes_bottom_clause(C, D) holds if the subset of C consist-
ing of literals that precede recursion, call it Cr,

is a subset of D. This makes

41

Algorithm 4 ASP code which loosens hypothesis constraints in the presence of
recursion.

1 generalizes_bottom_clause(Cl, E) :-

2

3

4

5

6

recursive,
clause(Cl),
pos_bottom_clause(E),
bottom_clause_variant(E, V),
not does_not_recursively_generalize_bottom_clause_variant(Cl, V).

7
8 does_not_recursively_generalize_bottom_clause_variant(Cl, V):-

9

10

11

bottom_clause_variant(_, V),
precedes_recursion(Cl, P, Vars),
not bottom_clause_body(V, P, Vars).

12
13 precedes_recursion(Cl, P, Vars):-

14

15

head_literal(Cl, PI, AI, _),
body_literal(Cl, PI, AI, VarsI),
in_var_is_out_var(Cl, PI, VarsI, P, Vars).

16
17 precedes_recursion(Cl, P, Vars):-

18

19

precedes_recursion(Cl, PI, VarsI),
in_var_is_out_var(Cl, PI, VarsI, P, Vars).

22

20
21 in_var_is_out_var(Cl, PI, VarsI, P, Vars) :-
var_direction(PI, VarsI, in, Var),
body_literal(Cl, P, _, Vars),
var_direction(P, Vars, out, Var).

24

23

generalizes_bottom_clause/2 an unsound generalization relation during positive
bottom preprocessing of recursive problem. However, generalizes_bottom_clause

/2 can still be a complete generalization relation in this case. Proposition 5 showed

this this condition is suﬃcient for positive bottom preprocessing to remain subsumption-

sound.

Note that Algorithm 4 requires some deﬁnition of the undeﬁned predicate symbol

var_direction/4. Suppose var_direction(P, Vars, Dir, Var) holds if Vars
has the form (V1, · · · , Vn), Var is some Vi and the direction declaration of the form
direction(P, (D1, · · · , Dn)) in the language bias of the problem has Di = Dir.
Then Algorithm 4 encodes this restriction.

42

Chapter 5

Experimental Results

Three major claims have been made about how bottom preprocessing performs in

practice. The experiments in this chapter test these claims by answering the following
questions:

• Q1: Can bottom preprocessing signiﬁcantly reduce the number of programs a

meta-level ILP system explores?

• Q2: Can bottom preprocessing reduce learning times of a meta-level ILP system

on hard problems?

• Q3: Can bottom preprocessing improve the scaling of a meta-level ILP system

as the amount of irrelevant BK increases?

Beyond these primary questions, there are several more speciﬁc claims about the

implementation of ⊥-Popper which are worth evaluating experimentally:

• Q4: Are positive and negative bottom preprocessing similarly eﬀective at re-

ducing the hypothesis space of LFF problems?

• Q5: Does unsound bottom preprocessing frequently preserve optimal solutions

in practice?

• Q6: Does variable splitting signiﬁcantly increase learning times?

To evaluate questions Q4 through Q6, we use several diﬀerent versions of ⊥-

Popper:

• ⊥-Popper is the default version.

It uses both positive and negative example
bottom preprocessing. However, it performs no variable splitting. This version

may prune subsumption-complete solutions.

43

Figure 5.1: Examples from the Michalski trains problem.

• ⊥-PopperV S is ⊥-Popper with variable splitting enabled. This version may

prune solutions but does not prune subsumption-complete solutions.

• ⊥-Popper− performs only negative example bottom preprocessing. This version

does not prune solutions.

• ⊥-Popper+ performs only positive example bottom preprocessing. This version

may prune subsumption-complete solutions.

All of the experiments in this chapter were run on an Amazon EC2 c5.4xlarge

instance with 16 vCPUs and 32 GB of memory.

5.1 Trains Problems

Michalski’s train problem [13] is a classic problem in machine learning. Figure 5.1

shows four example trains in the problem. Trains t1 and t2 are westbound, facing
left, while t3 and t4 are eastbound, facing right. The goal for a trains problem is

to ﬁnd a relation f(Train) which holds for eastbound trains but not for westbound

trains. Finding such a relation was the basis of a machine learning competition held

in 1994. These images were taken from that competition [18].

Since the instance of the trains problem in Figure 5.1 is simple, the reader might

be able to ﬁnd a solution with a quick visual inspection. All eastbound trains have a

car carrying a rectangular load while none of the westbound cars carry a rectangular

load. Popper, given an encoding of this problem as an LFF problem quickly ﬁnds the
following solution:

f(A) :- has_car(A, B), has_load(B, C), rectangle(C).

In the following experiments, trains problems are formed from ﬁve positive and

ﬁve negative examples. As the experiments show, randomly selected problems like

this can be quite hard for an ILP system to solve

The background knowledge for train t3 in Figure 5.1 is encoded in Prolog as
shown in Figure 5.2. The full background knowledge for the trains problem contains

similar encodings of 1000 trains. The language bias for the problem allows f as a head

44

train(t3).
has_car(t3, t3_c1).
has_car(t3, t3_c2).
short(t3_c1).
short(t3_c2).
two_wheels(t3_c1).
two_wheels(t3_c2).

roof_open(t3_c1).
roof_open(t3_c2).
has_load(t3_c1, t3_c1_l1).
has_load(t3_c2, t3_c2_l1).
triangle(t3_c1_l1).
rectangle(t3_c2_l1).

Figure 5.2: A logic program constituting background knowledge for train t3.

predicate symbol and 22 possible predicate symbols in the body. It also contains type

and direction declarations for each predicate symbol. The settings used for the

trains problem include max_vars(5), max_clauses(4), and max_body(5). The
full language bias for the trains problem can be found in Appendix F. The full BK

is over 28,000 lines and too long to include. A link to the full BK is provided in in

Appendix F

5.1.1 Random Trains Problems

The ﬁrst experiment tests Q1, Q2, Q4, Q5, and Q6 by comparing Popper and ⊥-

Popper on random trains problems. The background knowledge, language bias, and

settings for each trains problem are as described above.

200 instances of the trains problem were generated. Each problem in-
Methods
stance was generated by selecting 5 random trains in the BK as positive examples

and 5 diﬀerent random trains as negative examples. Each instance of the trains prob-

lem was passed as input to Popper and each ⊥-Popper version. Each system was given

ﬁve minutes to solve each trains problem. Systems which timed out were recorded
as ﬁnding no solution for that problem. The following statistics were recorded after

executing each system on each problem: execution time, number of programs gener-

ated, whether a solution was found, the number of literals in the solution, and the

number of bottom clause variants generated. A solution for any version of ⊥-Popper
was considered non-optimal if Popper was able to ﬁnd a solution with less literals.

Results The mean results for this experiment are shown in Table 5.1.

From this experiment, it seems the answer to Q1 is yes. On random trains prob-

lems bottom preprocessing can reduce the number of programs generated by Popper
on average by a factor of over 1000. This shows that bottom preprocessing can sub-

stantially reduce the space of programs a meta-level ILP system explores. Bottom

45

⊥-Popper

Popper

⊥-Popper+ ⊥-Popper− ⊥-PopperV S

Execution time
Programs generated
Percent solved
Percent optimal
⊥ variants

18 ± 3
7 ± 5
96
99.5
2927 ± 75

197 ± 9
8611 ± 393
45
100
N/A

140 ± 9
3168 ± 205
66
99.5
1474 ± 46

163 ± 10
4382 ± 265
55
100
1520 ± 50

20 ± 3
7 ± 5
96
99.5
2918 ± 77

Table 5.1: Aggregate results on random trains problems for Popper and versions of
⊥-Popper. The rows are, from top to bottom, mean execution time in seconds per
problem, mean number of programs generated per problem, percentage of total prob-
lems solved, percentage of problems solved optimally, and mean number of bottom
clause variants generated per problem. The error given is standard error.

Figure 5.3: Mean execution time spent in each step of ﬁnding a solution for random
trains problems solvable by Popper in less than 20 seconds.

preprocessing has a cost though. Q2 asks if this reduction in the hypothesis space

improves the runtime.

The random trains experiment shows that the answer to Q2 is yes. Bottom

preprocessing can reduce learning times of a meta-level ILP system on hard problems.

⊥-Popper was 10.8 times faster on average than Popper when solving random trains

problems. Perhaps more importantly, ⊥-Popper was able to ﬁnd a solution for 96
percent of the problems while base Popper could only ﬁnd a solution for 45 percent.

Bottom preprocessing has an initial cost. It must compute bottom clauses and

then ground the additional hypothesis constraints it sends to ASP. Figure 5.3 shows

the mean amount of time Popper and ⊥-Popper spent in each step of ﬁnding a

46

solution. It only includes the subset of trains problems which Popper could solve in

less than 20 seconds. For these problems, the mean execution time for Popper was 4.4

seconds while the mean execution time for ⊥-Popper was 7.6 seconds. The stage ‘init
solver’ involves grounding the initial hypothesis constraints for a problem. This is

where most of the overhead from bottom preprocessing occurs. Figure 5.3 shows that

for problems with answers that were easy for Popper to ﬁnd quickly, the overhead of

preprocessing seemed to dominate ⊥-Popper’s runtime.

This experiment shows that the answer to Q4 is yes. Both ⊥-Popper− and ⊥-
Popper+ perform better than Popper. They also generate fewer programs. However,
it is the combination of both sets of constraints in ⊥-Popper that produces the most

signiﬁcant improvements for the random trains problems. It would seem that both
positive and negative bottom preprocessing are useful.

The answer to Q5 is also yes. The unsoundness of positive example bottom

preprocessing is not a signiﬁcant problem on random trains problems. Only a single

non-optimal solution was produced by any version of ⊥-Popper. For one problem, ⊥-
Popper, ⊥-Popper+, and ⊥-PopperV S found a solution with six literals while Popper
was able to ﬁnd a solution with ﬁve literals. As expected, ⊥-Popper− was able to
ﬁnd the optimal solution in this case.

For this experiment, the answer to Q6 is no. Variable splitting did not reduce
performance signiﬁcantly. The structure of the trains problem is such that there
are rarely any variables to split. Thus ⊥-PopperV S produced similar bottom clause
variants to ⊥-Popper and performed only slightly worse.

5.1.2

Irrelevant Background Knowledge

The random trains problems experiment did not answer Q3. It did not show anything

about how bottom preprocessing aﬀects performance in the presence of irrelevant

background knowledge. To determine this, another experiment was run on a single

trains problem which was modiﬁed to have increasing amounts of irrelevant BK. The
starting background knowledge, starting language bias, and settings for each trains

problem were the same as those used in the random trains problems experiment.

Methods A single set of ﬁve positive and ﬁve negative examples was used for every

problem. The timeout given to each problem was ten minutes. Two diﬀerent methods
were tested for expanding the starting BK and language bias.

47

Dyadic predicate symbol

In the ﬁrst conﬁguration 100 problems were generated

by adding i, ranging from 1 to 100, additional irrelevant predicate symbols to the
language bias and BK. Thus Li was Li−1 with the following addition:

body_pred(has_useless_{i},2).
direction(has_useless_{i},(in, out)).
type(has_useless_{i},(train, car)).

Bi was Bi−1 with the following addition:

has_useless_{i}(t1000, t1000_c1).

Since the train t1000 was not one of the trains used in the examples, this addi-
tional BK was not relevant to the problem. This is called the dyadic conﬁguration

because the added predicate symbols have arity 2.

Monadic predicate symbol

In the second conﬁguration 200 trains problems were

generated in a similar manner to the dyadic conﬁguration. However, the added pred-
icate symbols had arity 1, making this the monadic conﬁguration. Li was Li−1 with
the following addition:

body_pred(c{i},1).
direction(c{i},(out,)).
type(c{i},(load,)).

Bi was Bi−1 with the following addition:

c{i}(t1000_c1_l1).

Again since the train t1000 was not one of the trains used in the examples, this
additional BK was not relevant to the problem. t1000_c1_l1 is a load which can

only appear on t1000.

Results The eﬀect of the additional predicate symbols on the execution time of

Popper and three versions of ⊥-Popper is shown in Figure 5.4 and Figure 5.5.

The answer to Q2 is yes. Figure 5.4 shows that bottom preprocessing can in

some cases signiﬁcantly improve the scaling of Popper when irrelevant background
knowledge is present. However, the scaling is not improved when the additional

predicate symbols are monadic, as shown in Figure 5.5

Note that the direction declarations in the monadic conﬁguration, direction(

c{i},(out,)), have direction out for all arguments. Monadic predicate symbols
with this form of direction declaration are used in Popper as constant surrogates.

Constant surrogates are monadic predicate symbols that represent a constant symbol

in an LFF hypothesis. They are necessary because ground terms are not allowed

48

Figure 5.4: Execution time of Popper and ⊥-Popper as irrelevant dyadic predicate
symbols are added to the BK of a trains problem.

in LFF hypotheses. To include a constant such as 0, it is necessary to introduce a

constant surrogate, such as zero/1.

The diﬃculty for ⊥-Popper arises when a constant surrogate such as zero(0) is

a fact that is true in every model of B. Since zero(0) is always bias consistent,

it occurs in the bottom clause of every example relative to B. Since the number of

bottom clause encodings generated by ⊥-Popper scales with the number of literals in
the bottom clause, every constant surrogate introduced reduces performance.

Suppose l is a literal with the dyadic predicate symbol has_useless_i/2. In the

dyadic conﬁguration, the direction declaration direction(has_useless_i,(in,

out)) suﬃces to exclude l from all bottom clauses of examples. This is because l only
occurs in a bottom clause ⊥ if the ﬁrst argument of l is instantiated by another literal

in ⊥. Given the BK used in the experiment, the ﬁrst argument of l is always t1000.

Since t1000 is not relevant to any of the examples in the problem, t1000 will never be

instantiated by any other literal. Thus the predicate symbol has_useless_i/2 does
not occur in any positive example bottom clause. In that case, bottom preprocessing

hypothesis constraints ensure it cannot occur in any solution.

This shows that direction declarations are quite important when using bottom

preprocessing. In cases where the language bias can exclude irrelevant literals from a
bottom clause, bottom preprocessing can improve scaling. It is also worth noting that

49

Figure 5.5: Execution time of Popper and ⊥-Popper as irrelevant monadic predicate
symbols are added to the BK of a trains problem.

the positive example bottom preprocessing seems to do most of the work. ⊥-Popper−
has the same scaling issues as Popper does in the presence of irrelevant BK.

5.2 List Manipulation

Chapter 4 described how positive bottom preprocessing uses weaker constraints for

recursive hypotheses. This experiment tests whether bottom preprocessing is still
eﬀective on problems that require recursion. The original Popper paper used list

manipulation problems to evaluate Popper’s performance on diﬃcult recursive tasks.

The same task set and methodology is used here.

An example of a list manipulation problem is dropk. The dropk task is to ﬁnd
a relation f(L1, k, L2) such that L2 is L1 without its ﬁrst k elements. Thus f([3,
2, 1], 1, [2, 1]) is a positive example but f([3, 2, 1], 1, [3, 1]) is a

negative example. A solution to this problem as a logic program is the following:

f(A,B,C) :- one(B),tail(A,C).
f(A,B,C) :- tail(A,D),decrement(B,E),f(D,E,C).

The nine list manipulation problems used in this experiment are dropk, droplast

, evens, finddup, last, len, member, sorted, sumlist. The language bias

provided to the tasks includes the following relations: head/2, tail/2, geq/2,
empty/1, even/1, odd/1, one/1, zero/1, decrement/2. The following ad-

50

Task

⊥-Popper

Popper

⊥-Popper+ ⊥-Popper− ⊥-PopperV S

dropk
droplast
evens
ﬁnddup
last
len
member
sorted
sumlist

17 ± 1.7
75 ± 6.0
17 ± 0.5
8 ± 0.5
4 ± 0.3
4 ± 0.1
2 ± 0.0
86 ± 6.9
11 ± 0.7

21 ± 1.5
24 ± 2.1
14 ± 0.2
44 ± 1.5
5 ± 0.3
19 ± 0.6
2 ± 0.0
50 ± 4.4
430 ± 12.2

19 ± 1.8
23 ± 1.8
15 ± 0.5
8 ± 0.7
3 ± 0.2
3 ± 0.1
1 ± 0.0
68 ± 6.2
8 ± 0.4

40 ± 2.5
121 ± 8.7
42 ± 1.0
75 ± 3.6
14 ± 0.8
34 ± 2.9
3 ± 0.2
114 ± 6.7
450 ± 10.0

17 ± 1.6
405 ± 27.7
17 ± 0.6
21 ± 1.0
5 ± 0.3
9 ± 0.4
2 ± 0.0
116 ± 9.2
113 ± 7.5

Table 5.2: Mean execution time per list manipulation task. Results are for Popper
and versions of ⊥-Popper. The error given is standard error.

ditional bias was given to some tasks. droplast was given cons/3. finddup was

given element/2. len was given increment/2. sum was given sum/2. Type and

direction declarations for all relations were provided. Background knowledge imple-
menting all the listed relations was also provided. The full background knowledge

and language bias for each task can be found in Appendix E. All problems were given

settings of max_vars(5), max_clauses(2), and max_body(5).

Methods

10 random positive and negative examples were used to generate each

problem. 20 such problems per task were then passed as input to each system. The

timeout was set to 10 minutes for each problem. The parameter eval_timeout was

set to .001. This enforced a 1ms timeout on testing hypotheses. Similarly to the
trains problems the following statistics were recorded after executing each system on

each problem: execution time, number of programs generated, whether a solution was

found, the number of literals in the solution, and the number of bottom clause variants

generated. Each solution was evaluated by running it on 1000 random positive and
1000 random negative examples and recording its accuracy.

Results The mean execution time for each task on each version is summarized in

Table 5.2. The mean number of programs generated for each task on each system is

summarized in Table 5.3. Table 5.4 shows the mean number of bottom clause variants
generated for each task on each system.

This experiment shows that the answer to Q1 is again yes. ⊥-Popper explored

less programs than Popper on every task. Note that this question diﬀers from the

question of whether bottom preprocessing can reduce the initial hypothesis space of
a meta-level ILP system. This second question is answered theoretically in Chapter

51

Task

⊥-Popper

Popper

⊥-Popper+ ⊥-Popper− ⊥-PopperV S

dropk
droplast
evens
ﬁnddup
last
len
member
sorted
sumlist

251 ± 31
95 ± 13
46 ± 3
163 ± 18
73 ± 10
49 ± 4
1 ± 0
601 ± 54
31 ± 4

355 ± 31
154 ± 15
438 ± 3
1513 ± 41
313 ± 23
712 ± 21
75 ± 2
947 ± 56
4125 ± 40

339 ± 37
107 ± 14
144 ± 4
284 ± 32
84 ± 9
50 ± 3
1 ± 0
836 ± 71
32 ± 3

616 ± 38
177 ± 18
455 ± 7
1818 ± 58
484 ± 24
914 ± 63
69 ± 5
992 ± 48
4071 ± 7

239 ± 29
128 ± 18
43 ± 3
151 ± 16
88 ± 10
49 ± 3
1 ± 0
630 ± 56
29 ± 4

Table 5.3: Mean number of programs generated per list manipulation task. Results
are for Popper and versions of ⊥-Popper. The error given is standard error.

Task

⊥-Popper ⊥-Popper+ ⊥-Popper− ⊥-PopperV S

dropk
droplast
evens
ﬁnddup
last
len
member
sorted
sumlist

340 ± 11
3957 ± 42
3498 ± 31
811 ± 9
763 ± 8
604 ± 19
726 ± 9
2924 ± 41
797 ± 36

157 ± 2
385 ± 5
1747 ± 20
412 ± 8
364 ± 6
310 ± 15
388 ± 8
1157 ± 31
415 ± 28

183 ± 10
3572 ± 42
1751 ± 19
399 ± 6
398 ± 8
294 ± 12
338 ± 6
1767 ± 19
382 ± 18

413 ± 16
20149 ± 64
3724 ± 11
3050 ± 28
790 ± 9
1696 ± 58
766 ± 10
3830 ± 31
6357 ± 706

Table 5.4: Mean number of bottom clause variants generated per list manipulation
task. Results are for Popper and versions of ⊥-Popper. The error given is standard
error. The bolded values in this table are maximums.

3. The experimental results shown here demonstrate that this reduction in the ini-
tial hypothesis space translates into ⊥-Popper exploring less programs overall before
ﬁnding a solution. The results for ⊥-Popper− show that this is not always the case.
⊥-Popper− frequently explores more programs than Popper. This phenomenon is
explored in more depth in relation to Q4 below.

The answer to Q2 is also yes. ⊥-Popper or ⊥-Popper+ demonstrated improved
performance over Popper on all but two tasks: evens and sorted.
In this case
⊥-Popper+ was the most consistent performer. It was only signiﬁcantly slower than
Popper on the sorted task, taking 1.36x as long on average. However, it was 51.9x
faster on sumlist, 6.5x faster on len, and 5.4x faster on finddup.

For this experiment, the answer to Q4 is no. Positive and negative example

bottom preprocessing are not equally eﬀective. In fact, in all cases negative bottom

preprocessing is strictly worse on these recursive tasks than no preprocessing at all.
There are several reasons for the poor performance of ⊥-Popper−. First, no re-

52

Figure 5.6: Total number of programs with n literals generated by ⊥-Popper− and
Popper as n increases. These results are for a randomly chosen dropk task.

cursive clauses generalize a negative example bottom clause since the head predi-

cate symbol of any clause in a hypothesis cannot occur in the bottom clause. This
means negative bottom preprocessing can only prune recursive hypotheses due to

non-recursive clauses.

Second, it is possible for hypothesis constraints to prune hypotheses that help Pop-
per learn better constraints. This happens in the cases where ⊥-Popper− generates
more programs than Popper. Figure 5.6 shows, for the dropk task, the total number
of programs with n literals generated by ⊥-Popper− and Popper as n increases. In
this case, negative bottom preprocessing prunes slightly more hypotheses of size 3.

Yet it then generates many more programs of larger sizes, presumably because Pop-
per learns better constraints from the programs negative bottom preprocessing never

allows it to evaluate. Popper can learn multiple constraints from a failed hypothesis,

including both generalization and specialization constraints.

Third, negative example bottom preprocessing can generate hypothesis constraints
that increase the time it takes for ASP to generate any program. On one droplast
problem, ⊥-Popper− spends 12 seconds preprocessing and grounding constraints
while Popper only spends 0.7 seconds. This time investment doesn’t pay oﬀ though
as ⊥-Popper− spends 130 seconds generating a larger number of hypotheses then
Popper. Popper only takes 16 seconds to generate hypotheses in this case. The extra

constraints make the CSP search harder without actually causing Popper to generate

53

Task

⊥-Popper Popper ⊥-Popper+ ⊥-Popper− ⊥-PopperV S

dropk
droplast
evens
ﬁnddup
last
len
member
sorted
sumlist

99 ± 3
100 ± 0
100 ± 0
99 ± 2
100 ± 0
100 ± 0
100 ± 0
97 ± 8
100 ± 0

100 ± 0
100 ± 0
100 ± 0
99 ± 2
100 ± 0
100 ± 0
100 ± 0
97 ± 8
100 ± 0

100 ± 2
100 ± 0
100 ± 0
99 ± 2
100 ± 0
100 ± 0
100 ± 0
98 ± 7
100 ± 0

99 ± 3
100 ± 0
100 ± 0
99 ± 2
100 ± 0
100 ± 0
100 ± 0
98 ± 7
100 ± 0

99 ± 2
90 ± 21
100 ± 0
99 ± 1
100 ± 0
100 ± 0
100 ± 0
99 ± 5
100 ± 0

Table 5.5: Mean accuracy for each list manipulation task. The error given is standard
deviation. Bolded text highlights instances where ⊥-Popper and Popper diﬀered.

fewer hypotheses.

Thus it could be better for ⊥-Popper to simply not perform negative example

bottom preprocessing when the problem allows recursive solutions.

The answer to Q5 is yes. All versions of ⊥-Popper were able to ﬁnd a solution
for all problems except two instances in which ⊥-PopperV S timed out on droplast.
Only two non-optimal solutions were found for 180 problems. In one case, ⊥-Popper−
found a non-optimal solution. This could indicate a bug in the code. There were no
cases in which variable splitting was necessary to ﬁnd an optimal solution. Figure

5.5 compares the accuracy of the diﬀerent versions on the list manipulation tasks.

None of the diﬀerences in the table are stastically signiﬁcant. The fact that any

diﬀerences exist on dropk and droplast can be explained by the two instances of
non-optimality and the two timeouts described above. The diﬀerences on sorted are

assumed to be random chance.

The answer to Q6 is a weak yes. ⊥-PopperV S was always slower than ⊥-Popper.
Sometimes it was signiﬁcantly slower. It achieved no beneﬁt since it never found an
optimal solution when ⊥-Popper+ did not. Table 5.4 shows that ⊥-PopperV S gener-
ated many more bottom clause variants than ⊥-Popper+ in all cases. For instance,
on the droplast task ⊥-PopperV S produced 52.3 times more variants and performed
18 times slower than ⊥-Popper+. ⊥-PopperV S also had lower accuracy. Future work
could explore other datasets designed speciﬁcally to require variable splitting.

54

Chapter 6

Related Work

This chapter provides a summary of research on inductive logic programming related

to bottom preprocessing. It serves as a short literature review of inverse entailment
and meta-level ILP systems. Rather than attempt comprehensiveness, it highlights

the most relevant research for bottom preprocessing. This chapter also discusses the

similarities and diﬀerences between existing research and the generation of hypothesis

constraints through preprocessing. This is intended to assist the reader in assessing
the novelty of this dissertation’s contributions.

6.1

Inverse Entailment

The inverse entailment approach to ILP was introduced in a 1995 paper by Stephen
Muggleton [20]. This paper also introduced the bottom clause and the ﬁrst inverse

entailment implementation, Progol. Inverse entailment and Progol have been highly

inﬂuential in ILP. Further systems which extend inverse entailment include Aleph

[27], HAIL [26], CF-Induction [11], Imparo [12], and ATOM [1].

In Deﬁnition 24, inverse entailment was deﬁned as deriving a clause C from an
example e, given background knowledge B, if and only if C subsumes ⊥B(e). Diﬀerent
implementations of inverse entailment take diﬀerent approaches to ﬁnding the clause
C given ⊥B(e). Progol makes use of the fact that ⊥B(e) imposes a greatest lower
bound on a lattice formed by the subsumption relation. This lattice has the empty
clause (cid:3) at its top. Given a problem input (B, M, E+, E−) containing background
knowledge, mode declarations, positive examples and negative examples respectively,

Progol proceeds as follows:

1. Initialize the current hypothesis H to an empty set of clauses.

55

2. Choose a positive example e ∈ E+ which is not covered by H. If all positive

examples are covered, return H.

3. Compute ⊥B,M (e), the mode declaration consistent bottom clause of e, using

the algorithm described in chapter 4.

4. Use A* search to ﬁnd the optimal clause Ce in the lattice between (cid:3) and
⊥B,M (e). This search starts with a clause Ce = (cid:3).
It uses a specialization
operator, ρ, to produce and evaluate candidate specializations of Ce. If it ﬁnds
a better candidate, the candidate becomes Ce. The A* heuristic used can take
many forms. The original choice was Occam compression which Aleph deﬁnes

as ”P - N - L + 1, where P, N are the number of positive and negative examples

covered by the clause, and L the number of literals in the clause [27].”

5. After ﬁnding the optimal clause Ce for some e, add Ce to H and returns to step

2.

As discussed in chapter 3, Yamamoto showed that inverse entailment is not com-

plete for entailment [30]. In the same paper, he showed that inverse entailment is

complete for relative subsumption. Muggleton attempted to resolve the incomplete-

ness of inverse entailment by expanding the computation of the bottom clause for
inverse entailment but, as he noted, this made inverse entailment unsound [21].

Yamamoto later deﬁned a bridge theory F , given background knowledge B and

example e, as any ground clausal theory such that B ∪ ¬e |= F [31]. The bottom

clause of e with respect to B is one of many possible bridge theories. Several inverse
entailment based systems introduce bridge theories that extend inverse entailment

to cover new cases. HAIL [26], for instance, uses a bridge theory called kernel sets.

Kernel sets enable clausal theories to be learned from a single example using inverse

entailment. HAIL extends the possible hypotheses that can be learned by inverse
entailment outside the bounds of Plotkin’s relative subsumption.CF-Induction [11]

presents a bridge theory called characteristic clauses which extends inverse entailment

to full clausal theories.

The bottom preprocessing approach described in this dissertation makes use of
the bottom clause as originally described with Progol. Alternative bridge theories

could possibly be used in future work to expand the hypotheses positive bottom

preprocessing allows.

56

The ILP system ATOM [1] introduces the use of a solver to inverse entailment.

ATOM encodes the clausal search in Progol as a constraint satisfaction problem. Sim-

ilarly to bottom preprocessing, the encoding ATOM uses ensures that every clause
in a hypothesis generalizes the bottom clause of a positive example. Since bottom

preprocessing generates hypothesis constraints in the hypothesis language of a target

ILP system, ATOM’s encoding diﬀers signiﬁcantly from ⊥-Popper’s. ATOM also

searches for clauses that generalize a single example at a time while bottom pre-
processing constrains entire hypotheses. ATOM also does not make use of negative

examples in its initial constraints. Instead, when it ﬁnds a clause C which entails a

negative example, it generates pruning constraints that eliminate generalizations of

C from its search. This approach to handling negative examples is more similar to
Popper’s learning from failures than bottom preprocessing.

6.2 Meta-level ILP

Meta-level ILP systems generally diﬀer from inverse entailment based systems by
evaluating entire hypotheses rather than iteratively ﬁnding individual clauses. Since

Popper is a meta-level ILP system and ⊥-Popper is an extension of Popper, this

section begins with an explanation of how Popper works. Popper implements the

learning from failures approach which this dissertation’s problem setting is based on.
Popper encodes an LFF problem as a constraint satisfaction problem on the syntax

of logic programming, speciﬁcally as an ASP program. Popper uses a generate-test-

constrain loop. In each loop:

1. The ASP solver generates a hypothesis program.

2. Popper tests the hypothesis against the examples.

3. If the program fails, Popper constrains future hypotheses in the ASP solver.

Popper can generates three types of constraints from a failing hypothesis H:

• Popper generates a generalization constraint when H entails a negative example.

In this case, no generalization of H can be a consistent solution.

• Popper generates a specialization constraint when H does not entail a positive

example. In this case, no specialization of H can be a complete solution.

57

• Popper generates an elimination constraint when H does not entail any positive
examples. Elimination constraints disallow H from appearing as a subset of the

clauses in any hypothesis that cannot self-resolve.

Bottom preprocessing can be viewed as taking Popper’s approach of generating

constraints from a hypothesis and extending it to generating constraints from the

bottom clauses of examples. Note that specialization constraints generated from
bottom clauses would be sound. Any specialization of the bottom clause of an example

e does not even entail e itself. These specialization constraints were not considered

for bottom preprocessing as they are too speciﬁc to be useful in most cases.

ASPAL [3] and ILASP [15] are also meta-level systems which encode ILP prob-
lems using ASP constraints. These systems diﬀer from Popper and ⊥-Popper in

that they learn ASP programs and expect BK written in ASP. They can learn non-

monotonic programs from non-monotonic BK. This means hypothesis constraints can

be provided directly in the BK. However, they struggle with inﬁnite domains [7]. Fu-
ture work could explore whether bottom preprocessing is also eﬀective when used to

generate initial constraints for these systems.

FastLAS [14] is the system which employs the most similar approach to bottom

preprocessing. FastLAS uses the Learning from Answer Sets setting introduced with
ILASP. It takes a scoring function which ranks hypotheses. It produces an optimal

solution according to this scoring function. The ﬁrst phase of FastLAS computes

the characterization of each positive and negative example for a LAS problem T . It

then sucessively reﬁnes three hypothesis spaces from these characterizations called
respectively the reduced, generalized and optimized characteristic hypothesis spaces

of T . The solving step of the FastLAS algorithm can be performed by passing the

optimised characteristic hypothesis space and the problem speciﬁcation T to ILASP.

Thus FastLAS can perhaps be viewed as an implementation of hypothesis con-
It generates hypothesis constraints from bridge theories of

straint preprocessing.

examples and then uses those constraints to prune the hypothesis space of an ILP

problem. However, the theoretical framework and algorithm of FastLAS diﬀer sub-

stantially from bottom preprocessing. FastLAS generates ASP programs not deﬁnite
programs. Characterizations are computed diﬀerently than bottom clauses. The con-

straints enforced on the hypothesis space are entirely diﬀerent from the subsumption

based generalization constraints of bottom preprocessing. The FastLAS algorithm is

also deeply tied to the optimization of solutions based on the scoring function it takes
as input. Finally FastLAS suﬀers from the same limitations mentioned for ILASP.

58

Chapter 7

Conclusions

This dissertation introduced bottom preprocessing, a method for pruning the initial

hypothesis space of an ILP problem. Bottom preprocessing generates a set of hypoth-
esis constraints from the bottom clauses of a problem’s examples. The constraints it
generates are based on two ideas. Any hypothesis H − that generalizes the bottom
clause of a negative example also entails that negative example. Thus H − is incon-
sistent and can be pruned from the hypothesis space. Any hypothesis H + that does
not generalize the bottom clauses of all positive examples is unlikely to entail the
examples. Pruning H + is not sound but it was shown that doing so works well in
practice.

Bottom preprocessing is implemented in ⊥-Popper, an extension to the meta-level
ILP system Popper [7]. ⊥-Popper encodes the ideas above as constraints in ASP. It

incorporates Progol’s bottom clause construction algorithm to ﬁnd bottom clauses

for each example in an ILP problem. It then encodes many bottom clause variants to

encode the subsumption check between a hypothesis and a bottom clause.

Experiments were run on the Michalski trains problem and list manipulation pro-

gramming problems. It was shown that bottom preprocessing can signiﬁcantly reduce

the initial hypothesis space of an ILP problem. Bottom preprocessing can also reduce

learning times of a meta-level ILP system on diﬃcult problems. In some cases, bot-
tom preprocessing can enable an ILP system to eﬃciently handle large background

knowledge with many irrelevant predicate symbols.

7.1 Limitations

This section summarizes several limitations of bottom preprocessing that could be

addressed in future work.

59

Soundness Chapter 3 showed that sound hypothesis constraints cannot be gener-

ated from positive examples using inverse entailment. The experiments performed

with the trains problem and list manipulation showed that this unsoundness may not
be a problem in practice. However, unsoundness compromises the optimality guar-

antee of an ILP system like Popper. Future work could try other bridge theories for

inverse entailment such as kernel sets [26] or characteristic clauses [11].

Chapter 3 also showed that a complete generalization relation is necessary for
positive bottom preprocessing to generate subsumption-sound constraints. Chapter

4 used variable splitting to expand its generalization relation toward completeness.

Chapter 5 showed that variable splitting could be very expensive when it causes

bottom preprocessing to generate many extra bottom clause variants. One approach
to resolving this issue could be to ﬁnd a generalization relation that is complete but

not sound. As mentioned in chapter 3, a generalization relation which holds for

all possible pairs of clauses would satisfy this requirement. It would not prune any

hypotheses though. A complete, unsound generalization relation with a tighter bound
might prove quite useful for positive bottom preprocessing.

Initial cost The experiments with random trains problems showed that bottom

preprocessing has an initial cost on easier problems that it doesn’t always overcome.

One approach to addressing this is to perform bottom preprocessing lazily rather
than up front. Popper, for instance, searches hypotheses by incrementally increasing

the number of literals allowed in the hypothesis. Intuitively, it is not necessary to

compute the bottom clause to its maximum variable depth in cases where the number

of possible literals in the hypothesis is small. Further, there might be some subset
of bottom clause variants that are irrelevant for a given number of literals in the

hypothesis. Lazy generation of the bottom clause and bottom clause variant encodings

as the search space expands could amortize some of the up front cost of bottom

preprocessing.

Implementation Chapter 4 noted that encoding all variants of the variable split

bottom clause may not ensure completeness of the generalization relation ⊥-Popper

uses. It was also not clear that this approach was more eﬃcient than alternatives.

Since straightforward subsumption checking algorithms exist [8], it should be possible
to implement one of these algorithms into ASP. It would then be possible to compare

the performance of ⊥-Popper with a diﬀerent subsumption check to the current ver-

sion of ⊥-Popper. It is possible that this subsumption check could be implemented

60

directly as hypothesis constraints instead of a separate program which encodes bot-

tom clause variants. This approach is appealing because it might enable interactions

between the constraints Popper learns and the subsumption check of bottom prepro-
cessing.

Constants The experiment which added monadic predicate symbols to the lan-

guage bias of a trains problem showed that ⊥-Popper does not scale well when con-
stant surrogates are present. Since a non-bias consistent bottom clause is ground

before being lifted, it can be interpreted as containing the full set of ground terms

which are relevant to an example. Future research could explore whether this could

be used to remove the restriction that the hypothesis contain no ground terms in the
LFF problem setting. Recursion and the incompleteness of inverse entailment might

make this set of ground terms incomplete. It is still a potentially interesting avenue of

investigation. Enabling Popper to use ground terms in hypotheses would have many

beneﬁts, one of which would be no longer requiring constant surrogates in the BK of
an LFF problem.

7.2 Future Work

There are other directions future work with bottom preprocessing could take besides

overcoming the current limitations of ⊥-Popper.

Predicate Invention A recent paper extends Popper with predicate invention.
Predicate invention is the ability of an ILP system to invent new predicate symbols

that can be used in a hypothesis. Predicate invention can enable an ILP system to

expand its own BK during solving [6]. However, predicate invention is diﬃcult for

any inverse entailment system because any predicate symbols the system invents will
not exist in the bottom clauses of the examples. Bottom preprocessing inherits this

limitation. The current implementation of ⊥-Popper simply disables bottom prepro-

cessing constraints for problems which use predicate invention. Further work could

explore a more nuanced approach to integrating predicate invention with bottom
preprocessing.

Other preprocessing approaches Bottom clauses are not the only information
that can be extracted from the examples and background knowledge of an ILP prob-

lem. GOLEM [23], one of the earliest ILP systems, used relative least general general-

ization (rlgg) to ﬁnd solutions to ILP problems. The rlgg is a single clause that covers

61

a set of examples. ATOM showed that constraints can be generated on hypotheses

if predicate symbols are marked as functional, i.e. the truth value is not important

[1]. Future work could explore these and potentially other creative ways to eﬃciently
generate initial constraints for meta-level ILP systems.

62

Bibliography

[1] John Ahlgren and Shiu Yin Yuen. Eﬃcient program synthesis using constraint
satisfaction in inductive logic programming. The Journal of Machine Learning

Research, 14(1):3649–3682, 2013.

[2] Alonzo Church. A note on the entscheidungsproblem. The journal of symbolic

logic, 1(1):40–41, 1936.

[3] Domenico Corapi. Nonmonotonic inductive logic programming as abductive

search. 2012.

[4] Andrew Cropper. Forgetting to learn logic programs. In Proceedings of the AAAI

Conference on Artiﬁcial Intelligence, volume 34, pages 3676–3683, 2020.

[5] Andrew Cropper and Sebastijan Dumanˇci´c. Inductive logic programming at 30:

a new introduction. arXiv preprint arXiv:2008.07912, 2020.

[6] Andrew Cropper, Sebastijan Dumanˇci´c, and Stephen H Muggleton. Turning 30:

New ideas in inductive logic programming. arXiv preprint arXiv:2002.11002,

2020.

[7] Andrew Cropper and Rolf Morel. Learning programs by learning from failures.

Machine Learning, 110(4):801–856, 2021.

[8] Luc De Raedt. Logical and relational learning. Springer Science & Business

Media, 2008.

[9] Oxford English Dictionary. Oxford english dictionary. Simpson, Ja & Weiner,

Esc, 1989.

[10] Sumit Gulwani, Oleksandr Polozov, Rishabh Singh, et al. Program synthesis.
Foundations and Trends® in Programming Languages, 4(1-2):1–119, 2017.

63

[11] Katsumi Inoue. Induction as consequence ﬁnding. Machine Learning, 55(2):109–

135, 2004.

[12] Tim Kimber, Krysia Broda, and Alessandra Russo. Induction on failure: Learn-

ing connected horn theories. In International Conference on Logic Programming

and Nonmonotonic Reasoning, pages 169–181. Springer, 2009.

[13] James Larson and Ryszard S Michalski. Inductive inference of vl decision rules.

ACM SIGART Bulletin, (63):38–44, 1977.

[14] Mark Law, Alessandra Russo, Elisa Bertino, Krysia Broda, and Jorge Lobo. Fast-

las: scalable inductive logic programming incorporating domain-speciﬁc optimi-

sation criteria. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,

volume 34, pages 2877–2885, 2020.

[15] Mark Law, Alessandra Russo, and Krysia Broda. Inductive learning of answer

set programs. In European Workshop on Logics in Artiﬁcial Intelligence, pages
311–325. Springer, 2014.

[16] Dianhuan Lin, Eyal Dechter, Kevin Ellis, Joshua B Tenenbaum, and Stephen H

Muggleton. Bias reformulation for one-shot function induction. 2014.

[17] Gayle Laakmann McDowell. Cracking the coding interview: 189 programming

questions and solutions. CareerCup, LLC, 2015.

[18] Donald Michie, Stephen Muggleton, David Page, and Ashwin Srinivasan. To

the international computing community: A new east-west challenge. Distributed

email document available from http://www. doc. ic. ac. uk/˜ shm/Papers/ml-
chall. pdf, 1994.

[19] Stephen Muggleton. Inductive logic programming. New generation computing,

8(4):295–318, 1991.

[20] Stephen Muggleton. Inverse entailment and progol. New generation computing,

13(3):245–286, 1995.

[21] Stephen Muggleton. Completing inverse entailment. In International Conference

on Inductive Logic Programming, pages 245–249. Springer, 1998.

[22] Stephen Muggleton and Luc De Raedt. Inductive logic programming: Theory

and methods. The Journal of Logic Programming, 19:629–679, 1994.

64

[23] Stephen Muggleton, Cao Feng, et al. Eﬃcient induction of logic programs. Cite-

seer, 1990.

[24] Shan-Hwei Nienhuys-Cheng and Ronald De Wolf. Foundations of inductive logic

programming, volume 1228. Springer Science & Business Media, 1997.

[25] Gordon Plotkin. Automatic methods of inductive inference. 1972.

[26] Oliver Ray, Krysia Broda, and Alessandra Russo. Hybrid abductive inductive

learning: A generalisation of progol. In International Conference on Inductive
Logic Programming, pages 311–328. Springer, 2003.

[27] Ashwin Srinivasan. The aleph manual, 2001.

[28] Alireza Tamaddoni-Nezhad and Stephen Muggleton. The lattice structure and

reﬁnement operators for the hypothesis space bounded by a bottom clause. Ma-

chine learning, 76(1):37–72, 2009.

[29] Lisa Torrey, Jude Shavlik, Trevor Walker, and Richard Maclin. Relational macros

for transfer in reinforcement learning. In International Conference on Inductive

Logic Programming, pages 254–268. Springer, 2007.

[30] Akihiro Yamamoto. Which hypotheses can be found with inverse entailment?

In International Conference on Inductive Logic Programming, pages 296–308.
Springer, 1997.

[31] Yoshitaka Yamamoto, Katsumi Inoue, and Koji Iwanuma. Inverse subsumption

for complete explanatory induction. Machine learning, 86(1):115–139, 2012.

65

Appendix A

Mode declaration code

The following Prolog code can generate a list of mode declarations from an LFF

language bias. The deﬁnitions of head_preds and body_preds can be found in
Appendix ??.

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

modes(L) :-

head_preds(Heads),
body_preds(Bodies),
maplist(make_mode(head), Heads, HeadModes),
maplist(make_mode(body), Bodies, BodyModes),
L1 = [HeadModes, BodyModes],
flatten(L1, L).

make_mode(Slot, (PredName/_, _, Meta), FinalMode) :-

make_type_dirs(Meta, [], TypeDirAtoms),
FinalPred=..[PredName|TypeDirAtoms],
(Slot=head -> FinalMode=(:- modeh(1, FinalPred));
Slot=body -> FinalMode=(:- modeb(*, FinalPred))).

make_type_dirs([], LIn, LOut) :- reverse(LIn, LOut).
make_type_dirs([(Type, Direction)|Meta], LIn, Atoms) :-

(Direction=in, DirChar=(+);
Direction=out, DirChar=(-)),
atom_concat(DirChar, Type, Atom),
make_type_dirs(Meta, [Atom | LIn], Atoms).

66

Appendix B

bc_prog code

1 :- dynamic

2

3

4

5

6

7

type/2,
neg/1,
pos/1,
head_pred/2,
body_pred/2,
direction/2.

8
9 %% Bias handling

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

head_preds(Preds) :-

preds(head, Preds).

body_preds(Preds) :-

preds(body, Preds).

preds(Slot, Preds) :-

findall((Name/Arity, Term, Variables),
pred_ext(Name, Arity, Slot, Term, Variables), Preds).

pred_ext(Name, Arity, Slot, Term, Meta) :-

find_pred(Name, Arity, Slot, Types,

Directions),

(cid:44)→
pick_vars(Arity, Vars),
Term =.. [Name | Vars],
zip(Types, Directions, Meta).

pred(Name, body, Arity) :-

body_pred(Name, Arity).

pred(Name, head, Arity) :-

head_pred(Name, Arity).

find_pred(Name, Arity, Slot, Types, Directions) :-

resolve_pred(Name, Arity, Slot, Directions),
resolve_type(Name, Arity, Types).

resolve_type(Name, _, Types) :-

type(Name, TypesT),
list_from_tuple(TypesT, Types), !.

resolve_type(_, Arity, Types) :-
none_types(Arity, Types).

67

39

40

41

42

43

44

45

46

47

48

49

50

51

52

53

54

55

56

57

58

59

60

61

62

63

64

65

66

67

68

69

70

71

72

73

74

75

76

77

78

79

80

81

82

83

84

85

86

87

88

89

90

91

92

resolve_pred(Name, Arity, Slot, Directions) :-

pred(Name, Slot, Arity),
direction(Name, DirectionsT),
list_from_tuple(DirectionsT, Directions).

none_types(0, []).
none_types(I, [none | L]) :-

Idec is I - 1,
none_types(Idec, L), !.

pick_vars(Arity, L) :-

length(L, Arity). %%Maybe these should be the same variables?

tuple_from_list([A], A) :- !.
tuple_from_list([A|T], (A, T1)) :-
!, tuple_from_list(T, T1).

list_from_tuple(B, [B]) :-

var(B), !.

list_from_tuple((A, B), [A|L]) :-
!, list_from_tuple(B, L).

list_from_tuple(B, [B]).

zip([], [], []).
zip([A|As], [B|Bs], [(A,B)|T]) :- zip(As, Bs, T).

%% Bottom clauses
example(positive, X) :-

pos(X).

example(negative, X) :-

neg(X).

print_neg_bottom(X, B) :-

neg_bottom(X, B), print(B).

neg_bottom(X, B) :-

example_bottom(negative, X, B).

print_pos_bottom(X, B) :-

pos_bottom(X, B), print(B).

pos_bottom(X, B) :-

example_bottom(positive, X, B).

example_bottom(Type, X, B) :-

example(Type, X),
bottom(X, B).

bottom(Clause, Bottom) :-

split_clause(Clause, Head, BodyList),
assert_all(BodyList),
find_bottom(Head, Bottom),
retract_all(BodyList).

assert_all([]).

68

93

94

95

96

97

98

99

100

101

102

103

104

105

106

107

108

109

110

111

112

113

114

115

116

117

118

119

120

121

122

123

124

125

126

127

128

129

130

131

132

133

134

135

assert_all([H | T]) :-

assertz(H), !, assert_all(T).

retract_all([]).
retract_all([H | T]) :-

retract(H), !, retract_all(T).

find_bottom(Head, Bottom) :-

b_setval(variable_index_counter, 0),
find_head(Head, TermMapHead, BottomHead, InTerms1),
find_body(TermMapHead, InTerms1, BottomBody),
Bottom = (BottomHead :- BottomBody), !.

find_head(Term, TermMap, Mode, InTerms) :-

head_preds(L),
empty_assoc(TermMapIn),
find_head_r(Term, L, TermMapIn, TermMap, Mode, InTerms).

find_head_r(Term, [(_, ModeTerm, Meta)|_], TermMapIn, TermMap,

(cid:44)→

FinalTerm, InTerms) :-
unifiable(ModeTerm, Term, Substitution),
!,
update_mode_term(ModeTerm, head, Meta, Substitution, TermMapIn,

(cid:44)→

TermMap, FinalTerm, InTerms).

find_head_r(Term, [_|T], TermMapIn, TermMap, FinalTerm, Substitution)

(cid:44)→

:-
find_head_r(Term, T, TermMapIn, TermMap, FinalTerm,

(cid:44)→

Substitution).

find_body(TermMap, InTerms, BottomBody) :-

body_preds(L),
find_body_loop_depth(L, 0, [], TermMap, InTerms, BottomBodyL),
tuple_from_list(BottomBodyL, BottomBody).

find_body_loop_depth(_, Depth, BottomBodyIn, _, _, BottomBodyIn) :-

max_variable_depth(Depth), !.

find_body_loop_depth(L, Depth, BottomBodyIn, TermMapIn, InTermsIn,

(cid:44)→

BottomBody) :-
find_body_loop_modes(L, BottomBodyIn, TermMapIn, InTermsIn,
InTermsIn, TermMapOut, InTermsOut, BottomBodyOut),

(cid:44)→
NewDepth is Depth+1,
find_body_loop_depth(L, NewDepth, BottomBodyOut, TermMapOut,

(cid:44)→

InTermsOut, BottomBody).

find_body_loop_modes([], BottomBodyIn, TermMapIn, _, TempInTerms,

TermMapIn, TempInTerms, BottomBodyIn).

(cid:44)→
find_body_loop_modes([(_, BodyTerm, Meta)|T], BottomBodyIn,

(cid:44)→

(cid:44)→

TermMapIn, InTermsIn, TempInTerms, TermMap, InTerms, BottomBody)
:-
find_body_term(BodyTerm, Meta, TermMapIn, InTermsIn, TermMapOut,

InTermsOut, BottomBodyOut),

(cid:44)→
ord_union(BottomBodyOut, BottomBodyIn, BottomBodyNext),

69

136

137

138

139

140

141

142

143

144

145

146

147

148

149

150

151

152

153

154

155

156

157

158

159

160

161

162

163

164

165

166

167

168

169

170

171

172

173

ord_union(TempInTerms, InTermsOut, NextTempInTerms),
find_body_loop_modes(T, BottomBodyNext, TermMapOut, InTermsIn,

(cid:44)→

NextTempInTerms, TermMap, InTerms, BottomBody).

find_body_term(BodyTerm, Meta, TermMapIn, InTermsIn, TermMap,

(cid:44)→

InTerms, BottomBody) :-
in_variables(BodyTerm, Meta, InVars),
find_body_substitutions(BodyTerm, InVars, InTermsIn, InTermsIn,

[], BodySubs),

(cid:44)→
make_body_substitutions(BodyTerm, BodySubs, Meta, TermMapIn,

(cid:44)→

InTermsIn, TermMap, InTerms, BottomBody).

in_variables(Term, Meta, InVars) :-

term_variables(Term, Variables),
zip(Variables, Meta, VarsWithMeta),
include(direction_match(in), VarsWithMeta, L1),
maplist(extract_var_and_type, L1, InVars).

extract_var_and_type((Var, (Type, _)), Var-Type).

find_body_substitutions(_, _, _, [], _, []).
find_body_substitutions(BodyTerm, [], _, _, Substitution, [BodySub])

(cid:44)→

:-
substitute(Substitution, BodyTerm, BodySub).

find_body_substitutions(BodyTerm, [H-Type|T], InTerms,
[Term-Type|Terms], Substitution, BodySubs) :-
find_body_substitutions(BodyTerm, T, InTerms, InTerms,

(cid:44)→

[H=Term|Substitution], BodySubs1),

(cid:44)→
find_body_substitutions(BodyTerm, [H-Type|T], InTerms, Terms,

Substitution, BodySubs2),

(cid:44)→
append(BodySubs1, BodySubs2, BodySubs).

find_body_substitutions(BodyTerm, L, InTerms, [_|Terms],

(cid:44)→

Substitution, BodySubs) :-

find_body_substitutions(BodyTerm, L, InTerms, Terms,

(cid:44)→

Substitution, BodySubs).

make_body_substitutions(BodyTerm, BodySubs, Meta, TermMapIn,

(cid:44)→

InTermsIn, TermMap, InTerms, BottomBody) :-
maplist(findall_reorder, BodySubs, L1),
flatten(L1, L2),
maplist(unifiable(BodyTerm), L2, Substitutions),
update_body_terms(BodyTerm, Substitutions, Meta, TermMapIn,
InTermsIn, [], TermMap, InTerms, BottomBodyTerms),

(cid:44)→
list_to_ord_set(BottomBodyTerms, BottomBody).

findall_reorder(X, L) :- findall(X, X, L).

update_body_terms(_, [], _, TermMapIn, InTermsIn, BottomBodyIn,

TermMapIn, InTermsIn, BottomBodyIn).

(cid:44)→
update_body_terms(BodyTerm, [Substitution|Substitutions], Meta,

(cid:44)→

(cid:44)→

TermMapIn, InTermsIn, BottomBodyIn, TermMap, InTerms, BottomBody)
:-
update_mode_term(BodyTerm, body, Meta, Substitution, TermMapIn,

(cid:44)→

TermMapOut, BottomBodyTerm, InTermsOut),

70

174

175

176

177

178

179

180

181

182

183

184

185

186

187

188

189

190

191

192

193

194

195

196

197

198

199

200

201

202

203

204

205

206

207

208

209

210

211

212

213

214

215

216

217

ord_union(InTermsIn, InTermsOut, NextInTerms),
update_body_terms(BodyTerm, Substitutions, Meta, TermMapOut,

(cid:44)→

(cid:44)→

NextInTerms, [BottomBodyTerm|BottomBodyIn], TermMap, InTerms,
BottomBody).

update_mode_term(ModeTerm, Slot, Meta, Substitution, TermMapIn,

(cid:44)→

TermMap, FinalTerm, InTerms) :-
match_sub_with_meta(ModeTerm, Substitution, Meta, SubWithMeta),
filter_subs(SubWithMeta, Slot, InTerms),
replace_mode_term(ModeTerm, SubWithMeta, TermMapIn, TermMap,

(cid:44)→

FinalTerm).

filter_subs(SubWithMeta, Slot, InTerms) :-

(Slot=head -> Direction=in;
Slot=body -> Direction=out),
include(direction_match(Direction), SubWithMeta, L1),
maplist(extract_term_and_type, L1, InTermsL),
list_to_ord_set(InTermsL, InTerms).

match_sub_with_meta(Term, Substitution, Meta, SubWithMeta) :-

term_variables(Term, Variables),
zip(Variables, Meta, VarsWithMeta),
maplist(find_term(Substitution), VarsWithMeta, SubWithMeta).

find_term([], _, _) :-

throw(bad_substitution).

find_term([(Var1=Term)|_], (Var2, Meta), ((Var1=Term), Meta)) :- Var1

== Var2, !.

(cid:44)→
find_term([_|T], Pair, Result) :-

find_term(T, Pair, Result).

extract_term_and_type(((_=Term), (Type, _)), Term-Type).
direction_match(Direction, (_, (_, Direction))).

replace_mode_term(ModeTerm, SubWithMeta, TermMapIn, TermMap,

(cid:44)→

FinalTerm) :-
swap_subs(SubWithMeta, TermMapIn, TermMap, NewSubstitution), !,
substitute(NewSubstitution, ModeTerm, FinalTerm), !.

swap_subs([], TermMap, TermMap, []).
%% TODO: Do we need to handle the case when the direction is # or

insert none somehow?

(cid:44)→
swap_subs([((Var1=Term), (_, none))| T], TermMapIn, TermMap,

(cid:44)→

[(Var1=Term)|T2]) :-
!, swap_subs(T, TermMapIn, TermMap, T2).

swap_subs([(Var1=Term, _)| T], TermMapIn, TermMap,

(cid:44)→

[(Var1=NewTerm)|T2]) :-
get_assoc(Term, TermMapIn, NewTerm), !,
swap_subs(T, TermMapIn, TermMap, T2).

swap_subs([(Var1=Term, _)| T], TermMapIn, TermMap,

(cid:44)→

[(Var1=NewTerm)|T2]) :-
b_getval(variable_index_counter, N),
NewTerm = '$VAR'(N),
N1 is N + 1,

71

218

219

220

221

222

223

224

225

226

227

228

229

230

231

232

233

234

235

236

237

238

239

240

241

242

243

244

245

246

247

248

249

250

251

252

253

254

255

256

257

258

259

260

261

262

263

264

265

266

267

268

269

b_setval(variable_index_counter, N1),
put_assoc(Term, TermMapIn, NewTerm, TermMapOut),
swap_subs(T, TermMapOut, TermMap, T2).

substitute(_, A, A) :- ground(A), !.
substitute([V=S|_], A, S) :- var(A), A == V, !.
substitute([_|T], A, B) :- var(A), substitute(T, A, B), !.
substitute([], A, A) :- var(A), !.
substitute(L, A, B) :- compound(A), A=..[F|T], maplist(substitute(L),

(cid:44)→

T, T2), B=..[F|T2].

split_clause((Head :- Body), Head, L) :- list_from_tuple(Body, L), !.
split_clause(Head, Head, []).

%% Discovering chains within the bottom clause
find_chains((_ :- Body), Result) :-

list_from_tuple(Body, BodyLiterals),
empty_assoc(VariableAssocs),
discover_chains_r(BodyLiterals, VariableAssocs, Result).

discover_chains_r([], A, A).
discover_chains_r([Head | Tail], InAssocs, OutAssocs) :-
add_variable_chains(Head, InAssocs, TempAssocs),
discover_chains_r(Tail, TempAssocs, OutAssocs).

add_variable_chains(Term, InAssocs, OutAssocs) :-
zip_vars_with_direction(Term, _, ZippedVars),
direction_vars(ZippedVars, in, InVars),
direction_vars(ZippedVars, out, OutVars),
update_chain_assocs(InVars, OutVars, InAssocs, OutAssocs).

zip_vars_with_direction(Term, Pred, ZippedVars) :-

Term =.. [Pred | Args],
length(Args, N),
direction(Pred, Directions),
list_from_tuple(Directions, DirectionL),
length(DirectionL, N), !,
zip(Args, DirectionL, ZippedVars).

update_chain_assocs([], _, In, In).
update_chain_assocs(InVars, OutVars, In, Out) :-
maplist(extract_var_index, InVars, InVarsL),
tuple_from_list(InVarsL, InVarsTuple),
update_chain_assocs_r(InVarsTuple, OutVars, In, Out).

update_chain_assocs_r(_, [], In, In).
update_chain_assocs_r(InVarsTuple, ['$VAR'(OutVar) | Tail], InAssocs,

(cid:44)→

OutAssocs) :-
find_chain_assocs(OutVar, InAssocs, ChainList),
ord_add_element(ChainList, InVarsTuple, NewChainList),
put_assoc(OutVar, InAssocs, NewChainList, TempAssocs),
update_chain_assocs_r(InVarsTuple, Tail, TempAssocs, OutAssocs).

extract_var_index('$VAR'(N), N).

72

270

271

272

273

274

275

276

277

278

279

280

281

282

283

284

285

286

287

288

289

290

291

292

293

294

295

296

297

298

299

300

301

302

303

304

305

306

307

308

309

310

311

312

313

314

315

316

317

318

319

320

321

find_chain_assocs(OutVar, InAssocs, Out) :-
get_assoc(OutVar, InAssocs, Out), !.

find_chain_assocs(_, _, []).

direction_vars(L, Dir, Out) :-

include(has_direction(Dir), L, Pairs),
maplist(extract_var, Pairs, Out).

has_direction(Dir, (_, Dir)).
extract_var((V, _), V).

%% Making variables unique and adding equality literals for

(cid:44)→

completeness in positive example bottom clauses

complete_bottom_clause((H :- B), H :- B1, EqualityPairs) :-

max_var_number((H :- B), 0, N),
N1 is N + 1,
b_setval(variable_index_counter, N1),
empty_assoc(InMap),
list_from_tuple(B, BodyLiterals),
make_vars_unique(BodyLiterals, InMap, NewBodyLiterals, OutMap),
duplicate_literals(NewBodyLiterals, OutMap, L1),
tuple_from_list(L1, B1),
assoc_to_list(OutMap, EqualityList),
include(not_empty_list, EqualityList, EqualityListFiltered),
maplist(pair_to_tuple, EqualityListFiltered, EqualityPairs), !.

not_empty_list(_-T) :-
length(T, N),
N > 0.

make_vars_unique([], InMap, _, InMap).
make_vars_unique([T1 | Tail], InMap, [T2 | Tail2], OutMap) :-

!,
make_vars_unique(T1, InMap, T2, OutMap1),
make_vars_unique(Tail, OutMap1, Tail2, OutMap).

make_vars_unique(Term, InMap, OutTerm, OutMap) :-

compound(Term),
zip_vars_with_direction(Term, Pred, ZippedVars),
map_vars_to_new_variables(ZippedVars, InMap, OutVs, OutMap),
OutTerm =.. [Pred | OutVs].

% TODO Need to add new literals when the in-vars are in InMap
map_vars_to_new_variables([], InMap, [], InMap).
map_vars_to_new_variables([(V, out)|T], InMap, [VOut|TOut], OutMap)

(cid:44)→

:-
associate_var(V, InMap, VOut, OutMap1), !,
map_vars_to_new_variables(T, OutMap1, TOut, OutMap).

map_vars_to_new_variables([(V, in)|T], InMap, [V|TOut], OutMap) :-

map_vars_to_new_variables(T, InMap, TOut, OutMap).

associate_var('$VAR'(VarNum), InMap, '$VAR'(N), OutMap) :-

get_assoc(VarNum, InMap, NMappings), !,

73

322

323

324

325

326

327

328

329

330

331

332

333

334

335

336

337

338

339

340

341

342

343

344

345

346

347

348

349

350

351

352

353

354

355

356

357

358

359

360

361

362

363

364

365

366

367

368

369

370

371

372

b_getval(variable_index_counter, N),
NewNMappings = [N|NMappings],
N1 is N + 1,
b_setval(variable_index_counter, N1),
put_assoc(VarNum, InMap, NewNMappings, OutMap).

associate_var('$VAR'(VarNum), InMap, '$VAR'(VarNum), OutMap) :-

NMappings = [],
put_assoc(VarNum, InMap, NMappings, OutMap).

duplicate_literals([], _, _).
duplicate_literals([T1 | Tail], InMap, L) :-

!,
duplicate_literals(T1, InMap, T2),
duplicate_literals(Tail, InMap, Tail2),
append(T2, Tail2, L).

duplicate_literals(Term, InMap, OutLits) :-

zip_vars_with_direction(Term, Pred, ZippedVars),
duplicate_literal(Pred, [], ZippedVars, InMap, OutLits).

duplicate_literal(Pred, StartVars, [], _, OutLits) :-

reverse(StartVars, RevStartVars),
OutLit =.. [Pred | RevStartVars],
OutLits = [OutLit].

duplicate_literal(Pred, StartVars, [('$VAR'(V), in) | Tail], InMap,

(cid:44)→

OutLits) :-
get_assoc(V, InMap, NMappings), !,
maplist(duplicate_literal_map(Pred, StartVars, Tail, InMap), [V |

NMappings], NewLitsL),

(cid:44)→
flatten(NewLitsL, OutLits).

duplicate_literal(Pred, StartVars, [(V, _) | Tail], InMap, OutLits)

(cid:44)→

:-
NewStartVars = [V | StartVars],
duplicate_literal(Pred, NewStartVars, Tail, InMap, OutLits).

duplicate_literal_map(Pred, StartVars, Tail, InMap, V, OutLits) :-

NewStartVars = ['$VAR'(V) | StartVars],
duplicate_literal(Pred, NewStartVars, Tail, InMap, OutLits).

%% Formatting for python
neg_bottom_plus_chains_for_python(X, S, C) :-

neg_bottom(X, B),
format_bottom_plus_chains_for_python(B, S, C).

pos_bottom_plus_chains_for_python(X, S, C) :-

pos_bottom(X, B),
format_bottom_plus_chains_for_python(B, S, C).

pos_bottom_complete_plus_chains_for_python(X, S, C, E) :-

pos_bottom(X, B),
complete_bottom_clause(B, BottomUnique, E1),
term_string(E1, E),
format_bottom_plus_chains_for_python(BottomUnique, S, C).

neg_bottom_complete_plus_chains_for_python(X, S, C, E) :-

74

373

374

375

376

377

378

379

380

381

382

383

384

385

386

387

388

389

390

391

392

393

394

395

396

397

398

399

400

401

402

403

404

405

406

407

408

409

410

411

412

413

414

415

416

417

418

419

420

neg_bottom(X, B),
complete_bottom_clause(B, BottomUnique, E1),
term_string(E1, E),
format_bottom_plus_chains_for_python(BottomUnique, S, C).

format_bottom_plus_chains_for_python(B, S, C) :-

find_chains(B, Chains),
format_chains_for_python(Chains, ChainPairs),
format_clause_for_python(B, Term),
term_string(Term, S),
term_string(ChainPairs, C).

format_chains_for_python(Chains, ChainPairs) :-

assoc_to_list(Chains, ChainList),
maplist(pair_to_tuple, ChainList, ChainPairs).

pair_to_tuple(X-Y, (X, Y)).

format_clause_for_python(Head :- Body, Output) :-

list_from_tuple(Body, BodyL),
format_literal_for_python(Head, FormattedHead),
maplist(format_literal_for_python, BodyL, FormattedBody),
Output = (FormattedHead, FormattedBody), !.

format_literal_for_python(Literal, FormattedLiteral) :-

Literal =.. [Predicate | Arguments],
atom_string(Predicate, PredicateName),
maplist(format_argument_for_python, Arguments, ArgumentNames),
FormattedLiteral = (PredicateName, ArgumentNames).

format_argument_for_python('$VAR'(VarNumber), VarNumber).

% Just leaving Vars as numbers for now.
% format_argument_for_python('$VAR'(VarNumber), VarName) :-
%
%
%
%
%
%

char_code('A', AIndex),
divmod(VarNumber, 26, Quotient, Remainder),
Index is Remainder + AIndex,
char_code(C, Index),
(Quotient is 0 -> VarName = C;
atomic_concat(C, Quotient, VarName)).

format_argument_for_python(Argument, ArgumentFormatted) :-

atom(Argument), !,
atom_string(Argument, ArgumentFormatted).

% Not including this for now so that we get an error if we have

compound arguments.

(cid:44)→
%format_argument_for_python(Argument, ArgumentFormatted) :-
%

compound(Argument), format_literal_for_python(Argument,

(cid:44)→

ArgumentFormatted).

75

Appendix C

bc_enc code

The following ﬁle is entirely the author’s own work. Unfortunately, it is dependent

on a larger set of code from the full Popper implementation. The current version of
Popper can be found here: https://github.com/logic-and-learning-lab/Popper

Other ﬁles were not included as they are not entirely the author’s contribution.

1 import clingo
2 from pyswip import Prolog
3 from ast import literal_eval

4
5 from .generate import var_name
6 from .tester import init_prolog as init_popper_prolog
7 from .core import Literal, Clause, Constraint

8
9 import itertools

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

NEGATIVE = "neg"
POSITIVE = "pos"
BASE_GROUNDING_PROGRAM = """
#show v_var/2.
v_val(0..num_v_vals-1).
:- v_var(X, _), not sat(X).
"""

## Format functions for bias predicates
def basic_pred_format(symbol):

return f'{symbol.name}({symbol.arguments[0].name},

(cid:44)→

{str(symbol.arguments[1])})'

def other_pred_format(symbol):

args = ', '.join([argument.name for argument in

symbol.arguments[1].arguments])

(cid:44)→
return f'{symbol.name}({symbol.arguments[0].name}, ({args}))'

BIAS_PREDS = [

("body_pred", 2, basic_pred_format),
("head_pred", 2, basic_pred_format),
("direction", 2, other_pred_format),

76

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

52

53

54

55

56

57

58

59

60

61

62

63

64

65

66

67

68

69

70

71

72

73

74

("type", 2, other_pred_format),

]

class Preprocessor:

def __init__(self, experiment):

self.process_experiment_settings(experiment)

self.init_solver()

self.init_prolog()

def process_experiment_settings(self, experiment):

self.experiment = experiment

self.complete = experiment.preprocessing.complete
self.example_types = experiment.preprocessing.example_types
self.debug = experiment.preprocessing.debug
self.single_variant_positive =

experiment.preprocessing.single_variant_positive

(cid:44)→
self.max_variable_depth =

(cid:44)→

experiment.preprocessing.max_variable_depth

def init_solver(self):

self.solver = clingo.Control(self.experiment.clingo_args)

self.solver.add('bias', [], self.experiment.bias_string)

self.solver.ground([('bias', [])])

max_vars_atoms =

(cid:44)→

self.solver.symbolic_atoms.by_signature('max_vars',
arity=1)
(cid:44)→
self.max_vars =

(cid:44)→

next(max_vars_atoms).symbol.arguments[0].number

if not self.max_variable_depth:

self.max_variable_depth = self.max_vars - 1

def init_prolog(self):

self.prolog = Prolog()

init_popper_prolog(self.prolog, [self.experiment.bk_filename,

(cid:44)→

self.experiment.exs_filename], 'preprocess.pl')

for (pred_name, pred_arity, pred_format_function) in

(cid:44)→

BIAS_PREDS:
pred_atoms =

(cid:44)→

self.solver.symbolic_atoms.by_signature(pred_name,
arity=pred_arity)

(cid:44)→
for pred_symbol in pred_atoms:

(cid:44)→

self.prolog.assertz(pred_format_function(pred_symbol.symbol))

77

75

76

77

78

79

80

81

82

83

84

85

86

87

88

89

90

91

92

93

94

95

96

97

98

99

100

101

102

103

104

105

106

107

108

109

(cid:44)→

self.prolog.assertz(f'max_variable_depth({self.max_variable_depth})')

def preprocess_constraints(self):

for example_type in self.example_types:

yield from

(cid:44)→

self.preprocess_constraints_by_type(example_type)

if self.experiment.show_stats:

self.experiment.preprocessing.print_stats()

def preprocess_constraints_by_type(self, example_type):

if self.complete:

bottom_results =

(cid:44)→

(cid:44)→

self.prolog.query(f"{example_type}_bottom_complete_plus_chains_for_python(X,
B, C, E)")

else:

bottom_results =

(cid:44)→

(cid:44)→

self.prolog.query(f"{example_type}_bottom_plus_chains_for_python(X,
B, C)")

for (i, bottom_result) in enumerate(bottom_results):

clause = self.parse_clause(bottom_result["B"])
chains = self.parse_dict(bottom_result["C"])
equalities = self.parse_dict(bottom_result["E"]) if

(cid:44)→

self.complete else {}

clause_handle = f"{example_type}_bc_{i}"

bc_constraint =

(cid:44)→

(cid:44)→

(cid:44)→

(cid:44)→

(cid:44)→

Constraint(f"{example_type}_bottom_clause",
Literal(f"{example_type}_bottom_clause",
(clause_handle,)), [])

self.experiment.preprocessing.register_bottom_clause(example_type,
clause, bc_constraint, chains, equalities)

yield bc_constraint

variants = self.find_variants(clause, chains, equalities,

(cid:44)→

example_type)

for (j, variant) in enumerate(variants):

variant_handle = f"{clause_handle}_v_{j}"
constraints = [Constraint("bottom_clause_variant",

(cid:44)→

Literal("bottom_clause_variant", (clause_handle,
variant_handle)), [])]

(cid:44)→
constraints +=

(cid:44)→

(cid:44)→

(cid:44)→

(cid:44)→

self.constraints_for_variant(variant_handle,
clause, variant)

self.experiment.preprocessing.register_variant(clause,
variant, constraints)

78

110

111

112

113

114

115

116

117

118

119

120

121

122

123

124

125

126

127

128

129

130

131

132

133

134

135

136

137

138

139

140

141

142

143

144

145

146

147

148

149

150

151

152

153

154

155

156

yield from constraints

def find_variants(self, clause, chains, equalities,

(cid:44)→

example_type):
program = self.generate_grounding_program(clause, chains,

(cid:44)→

equalities, example_type)

grounder = clingo.Control(['--rand-freq=0'])

# ask for all models
grounder.configuration.solve.models = 0

grounder.add('base', [], program)

grounder.ground([("base", [])])

out = []

# TODO Maybe clean this up. This is modified copied code from

aspsolver.py

(cid:44)→
# Can we reuse the grounder?
def on_model(m):

xs = m.symbols(shown = True)
# map a variable to a program variable
assignment = {}
for x in xs:

var = x.arguments[0].number
val = x.arguments[1].number
assignment[var] = val

out.append(assignment)

grounder.solve(on_model=on_model)
return out

def generate_grounding_program(self, clause, chains, equalities,

(cid:44)→

example_type):
program = self.initialize_grounding_program(clause, chains,

(cid:44)→

equalities)

if example_type == POSITIVE and self.single_variant_positive:
program.extend(self.positive_grounding_constraints())

else:

program.extend(self.variant_grounding_constraints(clause,

(cid:44)→

chains))

return "\n".join(program)

def initialize_grounding_program(self, clause, chains,

(cid:44)→

equalities):
program = [BASE_GROUNDING_PROGRAM]

program.append(f"#const num_v_vars={len(clause.all_vars)}.")
program.append(f"#const num_v_vals={self.max_vars}.")

79

157

158

159

160

161

162

163

164

165

166

167

168

169

170

171

172

173

174

175

176

177

178

179

180

181

182

183

184

185

186

187

188

189

190

191

192

193

194

195

196

197

198

199

200

is_complete = self.complete and equalities
if is_complete:

program.append(":- v_val(X), v_var(I, X), v_var(J, X), I

(cid:44)→

!= J, not eq_var(I, J).")

if len(clause.all_vars) > self.max_vars:

program.append("0 {v_var(V,X): v_val(X)} 1:-

V=0..num_v_vars-1.")

(cid:44)→
if is_complete:

program.append(":- v_val(X), #count{I : v_var(I,X)} <

(cid:44)→

1.")

else:

program.append(":- v_val(X), #count{I : v_var(I,X)}

(cid:44)→

!= 1.")

else:

if is_complete:

program.append("1 {v_var(V,X): v_val(X)}:-

(cid:44)→

V=0..num_v_vars-1.")

else:

program.append("1 {v_var(V,X): v_val(X)} 1:-

V=0..num_v_vars-1.")

(cid:44)→
program.append(":- v_val(X), #count{I : v_var(I,X)} >

(cid:44)→

1.")

head_args = set(clause.head.arguments)
for v in head_args:

program.append(f"v_var({v}, {v}).")
program.append(f"sat({v}).")

for v in clause.all_vars:

if v not in chains:

if v not in head_args:

program.append(f"sat({v}).")

continue

dependencies = chains[v]
for dependency in dependencies:

d_vars = list(dependency) if isinstance(dependency,

tuple) else [dependency]

(cid:44)→
sats = [f"sat({d_var}), v_var({d_var}, _)" for d_var

in d_vars]

(cid:44)→
program.append(f"sat({v}) :- {', '.join(sats)}.")

for (k, v) in equalities.items():

var_set = set([k] + v)
for (x, y) in itertools.combinations(var_set, 2):

program.append(f"eq_var({x}, {y}).")
program.append(f"eq_var({y}, {x}).")

return program

def positive_grounding_constraints(self):

80

201

202

203

204

205

206

207

208

209

210

211

212

213

214

215

216

217

218

219

220

221

222

223

224

225

226

227

228

229

230

231

232

233

234

235

236

237

238

239

240

241

242

243

# Variables must be assigned indexes in ascending order. This

enforces a single syntactic variant.

(cid:44)→
return [":- v_var(I, X), v_var(J, Y), X < Y, I > J."]

def variant_grounding_constraints(self, clause, chains):

head_args = set(clause.head.arguments)
depths = {}
for arg in head_args:

depths[arg] = 0

depths = self.compute_depths(clause, chains, depths)

depth_offset_for_head = len(head_args) - 1

constraints = []
for (v, depth) in depths.items():

if v in head_args:

continue

constraints.append(f"depth({v}, {depth +

(cid:44)→

depth_offset_for_head}).")

# Variables must have indices equal to or higher than their

depth.

(cid:44)→
constraints.append(":- v_var(X, I), depth(X, J), I < J.")

return constraints

def constraints_for_variant(self, variant_handle, clause,

(cid:44)→

variant):
yield self.bottom_clause_head_constraint(variant_handle,

(cid:44)→

clause, variant)

for body_lit in clause.body:

if not all(arg in variant for arg in body_lit.arguments):

continue

swapped_args = tuple([variant[arg] for arg in

body_lit.arguments])

(cid:44)→
yield self.bottom_clause_constraint("bottom_clause_body",

(cid:44)→

variant_handle, body_lit, swapped_args)

def bottom_clause_head_constraint(self, variant_handle, clause,

(cid:44)→

variant):
# TODO(Brad): This is just a sanity check. I should probably

remove it or make it better after testing.

(cid:44)→
for arg in clause.head.arguments:

if variant[arg] != arg:

raise Exception("Something went wrong! Head variables

(cid:44)→

were not assigned their index.")

return self.bottom_clause_constraint("bottom_clause_head",
variant_handle, clause.head, clause.head.arguments)

(cid:44)→

81

244

245

246

247

248

249

250

251

252

253

254

255

256

257

258

259

260

261

262

263

264

265

266

267

268

269

270

271

272

273

274

275

276

277

278

279

280

281

282

283

284

285

286

287

288

def bottom_clause_constraint(self, predicate, variant, literal,

(cid:44)→

arguments):
args = tuple([variant, literal.predicate, arguments])
return Constraint(predicate, Literal(predicate, args), [])

def parse_clause(self, clause_string):

# This assumes we always get a tuple
((head_pred, head_args), body_literals) =

literal_eval(clause_string.decode('UTF-8').strip())

(cid:44)→
head = Literal(head_pred, tuple(head_args))
body = [Literal(body_pred, tuple(body_args)) for (body_pred,

body_args) in body_literals]

(cid:44)→
return Clause(head, body)

def parse_dict(self, dict_string):

return

(cid:44)→

dict(literal_eval(dict_string.decode('UTF-8').strip()))

def compute_depths(self, clause, chains, depths):
for variable_name in sorted(clause.all_vars):

self.compute_depth(variable_name, chains, depths, set())

return depths

def compute_depth(self, variable_name, chains_dict, depths_dict,

(cid:44)→

path):
if variable_name in depths_dict:

return depths_dict[variable_name]

if variable_name not in chains_dict:

depths_dict[variable_name] = 1
return 1

if variable_name in path:

return None

min_depth = None
for dependency in chains_dict[variable_name]:

if isinstance(dependency, tuple):

results = [self.compute_depth(dependency_name,

(cid:44)→

(cid:44)→

chains_dict, depths_dict,
path.union({variable_name})) for dependency_name
in dependency]

(cid:44)→
filtered = [v for v in results if v != None]
depth = min(filtered) if filtered else None

else:

depth = self.compute_depth(dependency, chains_dict,

(cid:44)→

depths_dict, path.union({variable_name}))

if min_depth == None:
min_depth = depth

elif depth != None and depth < min_depth:

min_depth = depth

if min_depth != None:

depths_dict[variable_name] = min_depth + 1

82

289

290

291

return min_depth + 1

else:

return None

83

Appendix D

hyp_constraints and gen_rels
code

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

%% PREPROCESSING CONSTRAINTS
does_not_generalize_bottom_clause_variant(Cl, V):-

bottom_clause_variant(_, V),
head_literal(Cl, P, _, Vars),
not bottom_clause_head(V, P, Vars).

does_not_generalize_bottom_clause_variant(Cl, V):-

bottom_clause_variant(_, V),
body_literal(Cl, P, _, Vars),
not bottom_clause_body(V, P, Vars).

does_not_recursively_generalize_bottom_clause_variant(Cl, V):-

bottom_clause_variant(_, V),
precedes_recursion(Cl, P, Vars),
not bottom_clause_body(V, P, Vars).

generalizes_bottom_clause(Cl, E) :-

clause(Cl),
bottom_clause_variant(E, V),
not does_not_generalize_bottom_clause_variant(Cl, V).

generalizes_bottom_clause(Cl, E) :-

recursive,
clause(Cl),
pos_bottom_clause(E),
bottom_clause_variant(E, V),
not does_not_recursively_generalize_bottom_clause_variant(Cl, V).

not enable_pi,
neg_bottom_clause(E),
generalizes_bottom_clause(_, E).

:-

:-

not enable_pi,
not relaxed_positive_bottom_clause_constraints,
pos_bottom_clause(E),

84

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

52

53

54

55

56

57

58

59

60

61

62

63

64

65

66

not generalizes_bottom_clause(_, E).

generalizes_positive_bottom_clause(Cl) :-

pos_bottom_clause(E),
generalizes_bottom_clause(Cl, E).

:-

not enable_pi,
relaxed_positive_bottom_clause_constraints,
clause(Cl),
pos_bottom_clause(_),
not generalizes_positive_bottom_clause(Cl).

% Recursion handling for predicate invention
var_direction(P, Vars, Dir, Var) :-

direction_(P, Pos, Dir),
var_pos(Var, Vars, Pos).

in_var_is_out_var(Cl, PI, VarsI, P, Vars) :-

var_direction(PI, VarsI, in, Var),
body_literal(Cl, P, _, Vars),
var_direction(P, Vars, out, Var).

precedes_recursion(Cl, P, Vars):-

head_literal(Cl, PI, AI, _),
body_literal(Cl, PI, AI, VarsI),
in_var_is_out_var(Cl, PI, VarsI, P, Vars).

precedes_recursion(Cl, P, Vars):-

precedes_recursion(Cl, PI, VarsI),
in_var_is_out_var(Cl, PI, VarsI, P, Vars).

85

Appendix E

List manipulation problems ﬁles

E.1 Bias

1 max_vars(5).
2 max_body(5).
3 max_clauses(3).

4
5 enable_recursion.

6
7 body_pred(head,2).
8 body_pred(tail,2).
9 body_pred(geq,2).

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

body_pred(empty,1).
body_pred(even,1).
body_pred(odd,1).
body_pred(one,1).
body_pred(zero,1).
body_pred(decrement,2).

type(cons,(element,list,list)).
direction(cons,(in,in,out)).

type(head,(list,element)).
direction(head,(in,out)).

type(tail,(list,list)).
direction(tail,(in,out)).

type(empty,(list,)).
direction(empty,(in,)).

type(element,(list,element)).
direction(element,(in,out)).

type(increment,(element,element)).
direction(increment,(in,out)).

type(decrement,(element,element)).
direction(decrement,(in,out)).

86

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

52

53

54

type(geq,(element,element)).
direction(geq,(in,in)).

type(even,(element,)).
direction(even,(in,)).

type(odd,(element,)).
direction(odd,(in,)).

type(one,(element,)).
direction(one,(in,)).

type(zero,(element,)).
direction(zero,(out,)).

type(sum,(element,element,element)).
direction(sum,(in,in,out)).

E.1.1 dropk

1 head_pred(f,3).
2 type(f,(list,element,list)).
3 direction(f,(in,in,out)).

E.1.2 droplast

1 head_pred(f,2).
2 type(f,(list,list)).
3 direction(f,(in,out)).

4
5 body_pred(cons,3).

E.1.3

evens

1 head_pred(f,1).
2 type(f,(list,)).
3 direction(f,(in,)).

E.1.4 ﬁnddup

1 head_pred(f,2).
2 type(f,(list, element)).
3 direction(f,(in,out)).

4
5 %% ESSENTIAL
6 body_pred(element,2).

87

E.1.5

last

1 head_pred(f,2).
2 type(f,(list, element)).
3 direction(f,(in,out)).

E.1.6

len

1 head_pred(f,2).
2 type(f,(list,element)).
3 direction(f,(in,out)).

4
5 body_pred(increment,2).

E.1.7 member

1 head_pred(f,2).
2 type(f,(list,element)).
3 direction(f,(in,out)).

E.1.8 sorted

1 head_pred(f,1).
2 type(f,(list,)).
3 direction(f,(in,)).

E.1.9

sumlist

1 head_pred(f,2).
2 type(f,(list, element)).
3 direction(f,(in,out)).

4
5 body_pred(sum,3).

E.2 BK

1 is_list([]).
2 is_list([_|_]).

3
4 increment(A,B):-

5

6

7

(nonvar(A) -> \+ is_list(A); true),
(nonvar(B) -> \+ is_list(B); true),
succ(A,B).

8
9 decrement(A,B):-

10

11

12

13

14

15

(nonvar(A) -> \+ is_list(A); true),
(nonvar(B) -> \+ is_list(B); true),
succ(B,A).

my_length(A,B):-

(nonvar(A) -> is_list(A); true),

88

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

52

53

54

55

56

57

58

59

60

61

62

63

64

(nonvar(B) -> \+is_list(B); true),
length(A,B).

cons(A,B,C):-

append([A],B,C).

comps([H|T],H,T).

tail([_|T],T).
head([H|_],H).
sum(A,B,C):-

(nonvar(A) -> \+ is_list(A); true),
(nonvar(B) -> \+ is_list(B); true),
(nonvar(C) -> \+ is_list(B); true),
C is A+B.

empty([]).

element([X|_],X):-!.
element([_|T],X):-

element(T,X).

empty_in([]).
empty_out([]).

zero(0).
one(1).

gt(A,B):-

nonvar(A),
nonvar(B),
\+is_list(A),
\+is_list(B),
A > B.

geq(A,B):-

nonvar(A),
nonvar(B),
\+is_list(A),
\+is_list(B),
A >= B.

even(A):-

nonvar(A),
\+ is_list(A),
0 is A mod 2.

odd(A):-

nonvar(A),
\+ is_list(A),
1 is A mod 2.

89

Appendix F

Trains problem ﬁles

F.1 Bias

1 max_clauses(4).
2 max_vars(5).
3 max_body(5).

4
5 head_pred(f,1).
6 body_pred(has_car,2).
7 body_pred(has_load,2).
8 body_pred(long,1).
9 body_pred(short,1).

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

body_pred(two_wheels,1).
body_pred(three_wheels,1).
body_pred(roof_arc,1).
body_pred(roof_closed,1).
body_pred(roof_flat,1).
body_pred(roof_jagged,1).
body_pred(roof_open,1).
body_pred(roof_peaked,1).
body_pred(zero_load,1).
body_pred(one_load,1).
body_pred(two_load,1).
body_pred(three_load,1).
body_pred(circle,1).
body_pred(diamond,1).
body_pred(hexagon,1).
body_pred(inverted_triangle,1).
body_pred(rectangle,1).
body_pred(triangle,1).

type(f,(train,)).
type(has_car,(train,car)).
type(has_load,(car,load)).
type(long,(car,)).
type(short,(car,)).
type(two_wheels,(car,)).
type(three_wheels,(car,)).
type(roof_arc,(car,)).

90

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

52

53

54

55

56

57

58

59

60

61

62

63

64

65

66

67

68

69

70

71

72

73

74

75

76

type(roof_closed,(car,)).
type(roof_flat,(car,)).
type(roof_jagged,(car,)).
type(roof_open,(car,)).
type(roof_peaked,(car,)).
type(zero_load,(load,)).
type(one_load,(load,)).
type(two_load,(load,)).
type(three_load,(car,)).
type(circle,(load,)).
type(diamond,(load,)).
type(hexagon,(load,)).
type(inverted_triangle,(load,)).
type(rectangle,(load,)).
type(triangle,(load,)).

direction(f,(in,)).
direction(has_car,(in,out)).
direction(has_load,(in,out)).
direction(long,(in,)).
direction(short,(in,)).
direction(two_wheels,(in,)).
direction(three_wheels,(in,)).
direction(roof_arc,(in,)).
direction(roof_closed,(in,)).
direction(roof_flat,(in,)).
direction(roof_jagged,(in,)).
direction(roof_open,(in,)).
direction(roof_peaked,(in,)).
direction(zero_load,(in,)).
direction(one_load,(in,)).
direction(two_load,(in,)).
direction(three_load,(in,)).
direction(circle,(in,)).
direction(diamond,(in,)).
direction(hexagon,(in,)).
direction(inverted_triangle,(in,)).
direction(rectangle,(in,)).
direction(triangle,(in,)).

F.2 BK

The BK for the trains problem is very large. It can be found here: https://github.com/logic-

and-learning-lab/Popper/blob/main/examples/trains/bk.pl

91

