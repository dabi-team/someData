An Empirical Evaluation of Flow Based Programming in the
Machine Learning Deployment Context

Andrei Paleyes
University of Cambridge
Department of Computer Science and
Technology
United Kingdom
ap2169@cam.ac.uk

Christian Cabrera
University of Cambridge
Department of Computer Science and
Technology
United Kingdom
chc79@cam.ac.uk

Neil D. Lawrence
University of Cambridge
Department of Computer Science and
Technology
United Kingdom
ndl21@cam.ac.uk

2
2
0
2

r
p
A
7
2

]
E
S
.
s
c
[

1
v
1
8
7
2
1
.
4
0
2
2
:
v
i
X
r
a

ABSTRACT
As use of data driven technologies spreads, software engineers are
more often faced with the task of solving a business problem using
data-driven methods such as machine learning (ML) algorithms.
Deployment of ML within large software systems brings new chal-
lenges that are not addressed by standard engineering practices
and as a result businesses observe high rate of ML deployment
project failures. Data Oriented Architecture (DOA) is an emerging
approach that can support data scientists and software developers
when addressing such challenges. However, there is a lack of clarity
about how DOA systems should be implemented in practice. This
paper proposes to consider Flow-Based Programming (FBP) as a par-
adigm for creating DOA applications. We empirically evaluate FBP
in the context of ML deployment on four applications that represent
typical data science projects. We use Service Oriented Architecture
(SOA) as a baseline for comparison. Evaluation is done with respect
to different application domains, ML deployment stages, and code
quality metrics. Results reveal that FBP is a suitable paradigm for
data collection and data science tasks, and is able to simplify data
collection and discovery when compared with SOA. We discuss the
advantages of FBP as well as the gaps that need to be addressed to
increase FBP adoption as a standard design paradigm for DOA.

CCS CONCEPTS
‚Ä¢ Software and its engineering ‚Üí Software design tradeoffs;
‚Ä¢ Computing methodologies ‚Üí Machine learning.

KEYWORDS
Machine learning, software engineering, flow-based programming,
service-oriented architecture

ACM Reference Format:
Andrei Paleyes, Christian Cabrera, and Neil D. Lawrence. 2021. An Empirical
Evaluation of Flow Based Programming in the Machine Learning Deploy-
ment Context. In 1st Conference on AI Engineering - Software Engineering for
AI (CAIN‚Äô22), May 16‚Äì24, 2021, Pittsburgh, PA, USA. ACM, New York, NY,
USA, 11 pages. https://doi.org/10.1145/3522664.3528601

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
CAIN‚Äô22, May 16‚Äì24, 2021, Pittsburgh, PA, USA
¬© 2021 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-9275-4/21/05.
https://doi.org/10.1145/3522664.3528601

1 INTRODUCTION
When deploying machine learning (ML) algorithms in real-world
systems, software developers face a new set of challenges [12, 41].
In particular, real-world systems produce large quantities of hetero-
geneous, time varying, high dimensional data that feeds decision
making in these systems. This challenges the effectiveness and
efficiency of current software development and deployment prac-
tices. The challenges are present across the entire ML application
workflow, including the stages of data engineering, model learn-
ing, model verification, and model deployment. For example, data
analysts spend most of their time in looking for, acquiring, un-
derstanding, cleaning and preparing the data before using a ML
algorithm [36]. These challenges arise because the ML solutions
are deployed on top of existing software solutions which were built
to fulfill goals that are important but not directly related to ML,
for example high availability, robustness, and low latency. Machine
learning poses a new set of challenges that the majority of existing
software architectures were not designed for [25, 39, 41].

This paper considers software architectures that might be more
appropriate for converting data into business value. One paradigm
proposed in the research community is known as Data Oriented
Architectures (DOA). DOA is an emerging software architectural
pattern that aims to facilitate the integration of machine learning
algorithms within modern software systems [18, 24, 51]. DOA treats
data in the system as a first class citizen in a shared information
model, where stateless system components perform distributed
processing. These components communicate between each other
using an asynchronous message exchange protocol. Such features
enable DOA to achieve high data discoverability, availability, and
reuse. The stateless and loosely coupled system components also
allow DOA to deal with large-scale dynamic environments [51].

Despite a high level consensus about the potential benefits that
DOA brings to the implementation and deployment of ML algo-
rithms, there is no clarity on which tools, frameworks and pro-
gramming paradigms should be used as the building blocks of a
DOA system in practice. This paper presents a quantitative evalua-
tion of flow-based programming (FBP) [34] as a paradigm to use
for DOA-based applications. FBP defines applications as networks
of "black box" processes, which communicate via data streaming
connections, where the connections are specified externally to the
processes. Through these principles, such as external connections
and named ports, FBP promotes data coupling between system
components. We evaluate to what extent FBP is suitable for the
development of ML-based applications at different stages of the ML

 
 
 
 
 
 
CAIN‚Äô22, May 16‚Äì24, 2021, Pittsburgh, PA, USA

Andrei Paleyes, Christian Cabrera, and Neil D. Lawrence

deployment workflow. As a baseline for comparison in our evalua-
tion we use the currently prevalent Service Oriented Architecture
(SOA) [38, 43].

Our evaluation follows the Goal-Question-Metric (GQM) method-
ology for experimentation in software engineering proposed by
Wohlin et. al. [52]. We first use the GQM framework to define goals
of the evaluation, their respective questions, and the evaluation
metrics. We next describe and develop four data processing applica-
tions set in different domains and formulate a business problem for
each application that can be solved with ML. We then carry out the
complete workflow of integrating an ML solution on top of each
application, collecting previously defined metrics to the code base
at each stage of the deployment. Finally, we compare the fitness
of each of FBP and SOA paradigm for ML deployment based on
the measurements taken. Results show that FBP is able to address
some of the key challenges around ML deployment by exposing
the data in the system. At the same time there are still some gaps
that remain before it can be considered a go-to paradigm for DOA.
Specifically, our paper makes following contributions:

‚Ä¢ We quantitatively evaluate FBP in the context of ML deploy-
ment, analyzing its fitness with regard to data collection and
model integration stages.

‚Ä¢ We compare FBP with SOA, highlighting strengths of the
former paradigm as well as identifying gaps that shall be
covered to improve development experience with FBP.
‚Ä¢ We show how FBP can be used to implement software sys-
tems according to DOA principles, by providing implementa-
tions of four applications set in different business domains.

2 MOTIVATION
Deployment of ML in production faces a variety of challenges that
affect every step of the workflow [41]. Data discovery and collection
is one of the primary areas of concern [29, 44]. In this section we
explain how the problems in the currently most prevalent software
design approach motivated us to consider FBP.

Modern systems are built around "service-client" relationship1.
If a software client, perhaps a utility or a service or an object, needs
an output of a certain operation, and there is a service capable of
performing this operation, the client makes a call to that service
passing in some input data through the service‚Äôs API, and receives
back a reply with the output data. An architectural paradigm that
that is built entirely on this type of relationships is known service-
oriented architecture (SOA) and is considered a de-facto standard
of modern software engineering [38].

Such approaches that rely on API calls can often make data
ephemeral, as we show later. That hinders efforts to re-use this
data, for example in model training or validation. This presents
a critical challenge for systems that are focused on data-driven
decision making or data analysis.

To illustrate why this is the case, let us consider a system of
two services, A and B, where B is making a call to A, A does some
computations internally, and then returns output to B, as shown on
Figure 1. Imagine we would like to collect a dataset that allows us

1By service, or software service, we understand a software functionality or a set of
software functionalities provided via an interface also known as an API, that different
clients can reuse for different purposes by calling the named API.

to study the behavior of service A. That could be for the purposes
of measuring its performance, running a business analysis task, or
training a machine learning model. For that we would need a set
of (ùëã, ùë¶) pairs, where each ùëã is a sample of an input that B sends
to A, and each ùë¶ is the corresponding output that A sent back to B.
In the current architecture there is nothing that guarantees such
pairs were ever recorded. Even more so, there is no guarantee that
ùëã or ùë¶ are available anywhere as separate data collection.

Figure 1: System of two software services. Service B sends
a request with some input to service A, and receives a re-
sponse with some output. Input and output data are passed
over the network, and are not necessarily stored anywhere.

Consider also a more complex system with one more service C
(Figure 2). In this system, before replying to service B, service A also
makes an interim call to service C to collect some additional input
it needs to complete the computation. Here service A introduces
hidden state, which complicates the task of collecting all the data
required to describe A‚Äôs behavior. Not only do we need to store and
match inputs from B to the outputs, we also need to match each
such pair to corresponding call to C. The problem quickly grows
with the number of software services involved in the system.

Figure 2: System of three software services. Service A makes
an interim call to service C before generating a response to
B‚Äôs request. All inputs and outputs are passed over the net-
work, and without separate effort it is difficult to match re-
sponse from C to the corresponding request from B.

In practice some of the necessary data can be found in logs and
databases. However the decision on what data to store is left to
developers of services A, B and C. Moreover, developers of different
services within one software system may choose different tech-
nologies and formats for their logs and databases. In Confluent, the
software company behind Apache Kafka, this situation is known
as "The Data Dichotomy": while high quality data management is
about exposing data, services are about hiding it [48].

When data scientists are presented with a business problem, their
first step is to understand what data is available within the sys-
tem, and to collect it into a dataset suitable for ML model training.
With the majority of data hidden behind services, data scientists
have to spend significant time discovering this data. This involves
talking to service owners, examining databases, parsing logs, merg-
ing multiple existing data sources, and partnering with software
engineering teams to collect additional data. These struggles are
described in detail by Lin and Ryaboy, who describe how the scal-
able service-oriented architecture in Twitter becomes a source of
multiple problems for data science projects inside the company
[27]. Authors point out that while each service outputs rich logging
information which could, in theory, serve as a dataset source, in
practice these logs are usually inconsistent, incomplete and difficult
to parse. Furthermore, the single responsibility principle that drives
SOA architectures means it is difficult for data scientists to collect
a complete dataset, and many data science projects get stuck at the
data discovery stage. Similar observations are made by Nazabal et
al. [36], who collectively call these issues ‚Äòdata organization‚Äô and
recognize them as one of the major challenges data scientists have
to solve.

The main point is that there is nothing embedded in the SOA
design paradigm that simplifies data discovery and collection. Any
data-driven tasks within the boundaries of this system requires
separate efforts from service developers and data scientists. Orga-
nizational measures, such as team structures and conventions, can
address this to a certain extent. However organizational measures
on their own scale poorly and tend to break in long term, and shall
be supported by strong and agile architectural framework [22, 37].
This motivated our search for an alternative architecture paradigm
that considers data a first-class citizen of a software system, thus
fulfilling the growing demand for creating data-driven solutions.

3 BACKGROUND
In this section we briefly introduce concepts and paradigms used
throughout the paper.

3.1 Data-oriented architecture
Data-oriented architecture (DOA) is an emerging software pattern,
where the goal is to create real-time information systems without
centralised servers [51]. DOA proposes to treat the system‚Äôs data
as a first class citizen in a shared information model with its respec-
tive access methods. System components in DOA are distributed,
stateless, and communicate between each other using an asyn-
chronous message exchange protocols [18]. Such characteristics
have the potential to support software developers when addressing
ML deployment challenges [41]. Early ideas of applying DOA in
ML deployment context were first introduced by Diethe et al. [10]
and further developed by Lawrence [24] and Borchert [3]. In these
works DOA is described as a design approach that uses data stream-
ing to create data-first software systems. Borchert also presented
Milan, a domain-specific language that can be used for building sys-
tems according to the DOA principles. However, the most practical
way to implement DOA in practice is an unresolved question, as
references above all employ different methods and do not provide
a clear comparison with modern software engineering practices.

CAIN‚Äô22, May 16‚Äì24, 2021, Pittsburgh, PA, USA

In that light our work can be seen as a step towards moving DOA
beyond a high level concept by applying it to practical tasks and
comparing against other popular software design methodologies.

3.2 Flow-based programming
Flow-based programming was created by J.P. Morrison [34], and
can be considered a special case of the more general dataflow pro-
gramming paradigm. It realises the DOA principles as it defines
software applications as a set of processes which exchange data
via connections that are external to those processes. FBP exhibits
‚Äòdata coupling‚Äô, which is considered in computing to be the loosest
form of coupling between components, thus promoting a flexible
software architecture. FBP has a reputation as a paradigm that op-
timizes speed, bandwidth and processing power in multi-tasking,
parallel processing applications. For example Szydlo et. al. consider
FBP‚Äôs application to IoT [49], Lampa et.al. explored FBP‚Äôs potential
in the context of drug discovery [21], Zaman et. al. presented an
FBP-based framework for mobile development [53]. Recent years
saw birth of several general-purpose projects built on flow-based
programming principles, such as NoFlo [2] and Node-RED2. Node-
RED in particular became popular in the IoT community [7, 8], as
the FBP model was found to be a good fit for building data process-
ing pipelines in IoT. Developing this idea further, in this paper we
argue that there is potential in a wider use of FBP beyond IoT.

A notable feature of FBP is the ability to present the whole pro-
gram visually as a graph of data flow. This feature has two important
implications. First, the graph-like structure allows reasoning about
the entire system in a way that is often impossible in case of OOP or
SOA. We leverage this aspect later in this work. Second, it allows for
visual no-code programming which in some cases may aid adoption
of FBP. In particular it is believed to be useful for beginners who
have little or no prior coding experience [30].

We make use of data streams as connectors in flow-based pro-
grams. A data stream is a sequence of data records that are made
available over time. Machine learning on data streams is not a new
concept. Data processing platforms such as Apache Spark [32],
Apache Flink [6] or Google Cloud Dataflow [20] are widely used
for manipulating large datasets and executing machine learning
tasks. Apache Kafka [19] and AWS Kinesis3 are some of the most
commonly used data streaming services.

3.3 Service-oriented architecture
Service-oriented architecture (SOA) is a well known paradigm for
development of software applications [43]. Under this paradigm the
application is broken down into several components called services,
which interact between each other via a pre-defined protocol to pro-
vide the application‚Äôs users with necessary functionality. Services
only interact on API level, thus hiding details of each service‚Äôs im-
plementation, which results in a set of loosely coupled components.
Service orientation gives developers a range of important benefits:
encapsulation, loose coupling, modularity, scalability and ease of
integration with other systems, so it is a reasonable choice for those
who need to build scalable software [4, 42]. In recent years an SOA
derivative known as microservices gained substantial popularity

2https://nodered.org/
3https://aws.amazon.com/kinesis/

CAIN‚Äô22, May 16‚Äì24, 2021, Pittsburgh, PA, USA

Andrei Paleyes, Christian Cabrera, and Neil D. Lawrence

and can be considered de-facto standard distributed software design
approach [38]. Therefore for the reminder of this paper we will
assume microservices approach when talking about SOA.

4 EXPERIMENT DESIGN AND

IMPLEMENTATION

Our evaluation of the DOA-based applications follows the method-
ology proposed by Wohlin et. al. [52]. We first define a set of metrics
following the GQM framework, and then develop four applications
that cover a variety of business domains and areas of ML4 . These
applications allow us to study the properties of FBP and compare it
against classical SOA approach in the experimental setting. In this
section we describe the metrics we used, the applications we imple-
mented, and give details of the experiment we defined to examine
fitness of both paradigms in the ML deployment context.

4.1 Metrics Definition
We follow the GQM framework to define the metrics evaluation
[52]. This framework proposes to define the evaluation goals as a
first step. These goals are then mapped to metrics through questions
required to achieve the goals. Table 1 introduces the goals of this
evaluation, the questions we want to answer to achieve these goals,
and the metrics that will allow us to provide such answers. The
resulting metrics are defined as follows:

‚Ä¢ Logical Lines of Code, which counts executable statements
and ignores comments and blank lines. We use it as a measure
of an application‚Äôs codebase size. Specifically, we assess how
much additional code is needed to implement additional
functionalities in the applications.

‚Ä¢ Maintainability Index, as defined in the Radon package5,
is a composite metric that is calculated using a number of
other metrics as operands. This is a unitless metric that
assigns a codebase a score between 0 and 100. We use it to
assess how maintainable is the application‚Äôs codebase.
‚Ä¢ Cognitive complexity [5], which measures the complexity
of the control flow of code. We use it to assess how easy it is
to understand the applications code. This metric is similar
to McCabe‚Äôs cyclomatic complexity [31], and was proposed
as a replacement that focuses on human‚Äôs understanding of
the source code, which is critical for software development
and maintenance. Since cognitive complexity is measured
separately for each code block, we consider average cognitive
complexity across the whole codebase.

‚Ä¢ Number of Affected Components, which counts the num-
ber of components that were added or changed during a
certain stage of development. It allows us to evaluate intru-
siveness of a particular feature, that is how many parts of the
codebase had to be changed or added for implementation of
that feature. For the purposes of this experiment we identify
processing nodes and data streams as components of a FBP
program, and APIs and data access routines as components
of SOA program.

4.2 Applications
We have implemented four applications separately with FBP and
SOA paradigms. For each application we formulated a business task
that is illustrious of a typical task a practicing data scientist might
face. We then carried out deployment of a data-powered solution to
the task while observing the evolution of the codebase throughout
the deployment cycle. In this section we give a brief description of
each application as well as deployment stages.

Ride Allocation application maps incoming ride requests to
available drivers and tracks history of existing rides. We have formu-
lated a task of estimating pickup wait time for each ride allocation
based on historical data. This task is approached as a supervised
learning regression problem with offline model training of the col-
lected dataset. More elaborate description of this application and
its implementation details can be found in our earlier work [40].

MBlogger is a micro-blogging platform that keeps track of users,
their following/follower relationships between each other, and
builds a timeline of posts for each user based on activity of those
this user follows. As a task we decided to build a post-generating
bot that, given a particular user, can generate posts this user is
likely to be interested in. The solution can be considered a simple
generative NLP approach. In MBlogger we do not collect offline
dataset file, instead storing all data that is needed to generate posts
on the fly using runtime infrastructure.

Insurance Claims models a workflow that processes insurance
claims. Claims undergo a series of classification routines which
affect the choice of the final payout process. In this application we
use ML to replace all internal logic with one classification model.
As is the case with Ride Allocation, this model is trained offline and
then is used for online inference. Additionally, this application is
different from the ones described above because albeit ML model
is being deployed, it does not affect the user interface and only
changes the internal data processing mechanism of the application.
Playlist Builder6 creates a movie playlist for a specified genre.
Initial functionality builds playlists at random, and later stages
only add highest-grossing movies to a playlist. Unlike all other
applications, here we do not collect raw data or train any models.
Instead quantiles of movies‚Äô gross earnings are calculated, and then
used for filtering online. Playlist Builder illustrates a simple yet
realistic use case where a data scientist needs to build a solution that
collects certain statistics about the data and then makes automated
decisions based on it.

We defined three stages of the implementation of each applica-

tion to evaluate codebase changes:

‚Ä¢ Stage 1: minimal code to provide basic functionality without
any ML-powered capabilities. The stage is denoted in the
code and this paper by suffix min.

‚Ä¢ Stage 2: code for Stage 1 plus implementation of data collec-

tion. Denoted by suffix data.

‚Ä¢ Stage 3: code for Stage 2 plus code necessary for integrating
the data-driven solution. If the ML model is created, it is
trained on the dataset collected at the previous stage. De-
noted by suffix ml.

4Full source code of the project can be found at https://github.com/mlatcl/fbp-vs-soa
5https://radon.readthedocs.io/en/latest/intro.html#maintainability-index

6This application closely follows Metaflow tutorial: https://docs.metaflow.org/getting-
started/tutorials/season-1-the-local-experience/episode01

CAIN‚Äô22, May 16‚Äì24, 2021, Pittsburgh, PA, USA

Table 1: Evaluation Metrics

Goals

Questions

Goal 1: Analysing software paradigms
for the purpose of evaluating their
impact in the software development
process with respect to data collection tasks
from the point of view of
software developers in the context of
the development of data processing
applications.

Goal 2: Analysing software paradigms
for the purpose of evaluating their
impact in the software development
process with respect to
ML model integration
from the point of view of software developers
in the context of the development of
data processing applications.

Question 1: How much additional code is required
to implement a data collection task in the evaluated
applications for each paradigm?
Question 2: How does the system‚Äôs maintainability
change when data collection tasks are implemented
in the evaluated application for each paradigm?
Question 3: How complex does the evaluated application
become after implementation of data collection task
for each paradigm?
Question 4: How intrusive is the dataset collection
task in the evaluated applications for each paradigm?
Question 1: How much additional code is required
to implement an integration task in the evaluated
applications for each paradigm?
Question 2: How does the system‚Äôs maintainability
change when model integration task is implemented
in the evaluated applications for each paradigm?
Question 3: How complex does the evaluated application
become after integration of ML model for each paradigm?
Question 4: How intrusive is model integration task
in the evaluated applications for each paradigm?

Metrics

Logical lines of code

Maintainability Index

Cognitive Complexity

Number of affected components

Logical lines of code

Maintainability Index

Cognitive Complexity

Number of affected components

Our experiment contains six implementations of each of the
applications described above. Table 2 summarizes these versions,
and Figure 3 illustrates all six implementations for one of the appli-
cations (Insurance Claims) with sequence (for SOA) and data flow
(for FBP) diagrams.

4.3 Implementation notes
Each application in the experiment is structured the same way:
the application itself and the code that simulates events happening
in the outside world. The application part processes the incoming
data according to some business logic, and outputs data that is then
used by the simulation. The simulation part is implemented as a
discrete-event simulation and is responsible for generating events
that would be happening if the system was deployed in real life.

The SOA version of each application is done in a form of a REST-
ful service. We use the lightweight Flask7 framework to develop the
SOA applications because of its flexibility and popularity among
the Python community. Microservices persist and manipulate data
using SQLite8. This popular database engine offers a small, fast,
self-contained, high-reliability, full-featured platform for data man-
agement. We implement a set of services that offer the required
capabilities for each application and implementation stage, and data
access layer that manages database queries. All services are hosted
locally, and the communication is happening via HTTP requests.
The FBP version of each application is built with two major
building blocks: data streams and stateless processing nodes. We
use the lightweight Python "FBP-inspired" framework flowpipe9. A
data stream is a collection of data records of the same type that is
updated whenever a new record arrives. Each data stream within
the application belongs to one of three categories:

7Flask Framework - https://palletsprojects.com/p/flask/
8SQLite - https://www.sqlite.org/index.html
9flowpipe - https://github.com/PaulSchweizer/flowpipe

‚Ä¢ Input streams, that receive data from the outside world;
‚Ä¢ Output streams, that hold data produced by the application;
‚Ä¢ Internal streams, that hold intermediate data within the ap-
plication. These streams are necessary because processing
nodes by definition are not allowed to have state.

A processing node takes one or more streams as an input, per-
forms some operations on them, and then puts the result into output
or internal data streams. Such nodes do not carry internal state, and
do not make external calls to outside services or databases. All data
influencing a processing node should be registered in the system,
so if such additional input is necessary, it should be represented as
a data stream.

Figure 3 shows diagrams that describe all 6 versions of one app10.
As can be seen on these diagrams, entry point for all versions is
an App object, that either orchestrates calls to necessary services
(SOA) or triggers an evaluation of the data flow graph (FBP). Within
each paradigm subsequent stages introduced new APIs or graph
nodes, all of which are highlighted for clarity.

Since applications are implemented using Python frameworks,
we used Python tools Radon11 and Flake812 for metrics collection.

5 EVALUATION
In this section we present the results of the experiment. We follow
the goals and questions formulated in Table 1 and answer each
question by analyzing the corresponding metric.

5.1 Data collection task
First we address the questions around the data collection task, by
measuring the changes between min and data stages.

10We have chosen to show only one application, Insurance Claims, because of space
constraints.
11Radon - https://radon.readthedocs.io/en/latest/
12Flake8 - https://flake8.pycqa.org/en/latest/

CAIN‚Äô22, May 16‚Äì24, 2021, Pittsburgh, PA, USA

Andrei Paleyes, Christian Cabrera, and Neil D. Lawrence

(b) FBP min stage.

(d) FBP data stage.

(a) SOA min stage.

(c) SOA data stage.

(e) SOA ml stage.

(f) FBP ml stage.

Figure 3: Diagrams of Insurance Claims app evolution through three stages of ML deployment for both paradigms. We use
sequence diagrams for SOA (left) and data flow diagrams for FBP (right). Yellow boxes in SOA diagrams represent individual
services. Circles in FBP diagrams represent processing nodes, and boxes represent data streams: red for input, yellow for
internal, green for output streams. Deployment stages top to bottom are: min, data, ml. New or updated components and APIs
at each subsequent stage are highlighted with bold red dashed lines.

Table 2: List of all created versions created for each application. First column gives the key by which a particular version is
referred to in the codebase and in the paper.

CAIN‚Äô22, May 16‚Äì24, 2021, Pittsburgh, PA, USA

Key
ùëì ùëè_ùëéùëùùëù_ùëöùëñùëõ
ùëì ùëè_ùëéùëùùëù_ùëëùëéùë°ùëé
ùëì ùëè_ùëéùëùùëù_ùëöùëô
ùë†ùëúùëé_ùëéùëùùëù_ùëöùëñùëõ
ùë†ùëúùëé_ùëéùëùùëù_ùëëùëéùë°ùëé
ùë†ùëúùëé_ùëéùëùùëù_ùëöùëô

(a)

(c)

Paradigm Stage Description
FBP
FBP
FBP
SOA
SOA
SOA

Basic functionality
Same as ùëì ùëè_ùëéùëùùëù_ùëöùëñùëõ plus data collection
Same as ùëì ùëè_ùëéùëùùëù_ùëëùëéùë°ùëé plus model integration
Basic functionality
Same as ùë†ùëúùëé_ùëéùëùùëù_ùëöùëñùëõ plus dataset collection
Same as ùë†ùëúùëé_ùëéùëùùëù_ùëëùëéùë°ùëé plus model integration

1
2
3
1
2
3

(b)

(d)

Figure 4: Measurements of the impact data collection task had on FBP and SOA implementations of four applications.

5.1.1 How much additional code is required to implement a data
collection task? We used Logical Lines of Code (LLOC) metric to an-
swer this question, thus measuring how much code, in percentage
of the initial size of the codebase, was added for the data collection
stage. We use percentage change to make this comparison inde-
pendent of the amount of boilerplate code that might be different
for different paradigms. As can be seen from Figure 4a, two out
of four FBP and SOA applications required comparable amount of
additional code to implement data collection. In both cases the dif-
ference is small: growth difference was less than 0.2% for Insurance
Claims codebase and 4% for Ride Allocation codebase. MBlogger
and Playlist Builder show different behavior, suggesting that set-
ting up additional runtime infrastructure for data may bear higher

initial cost for FBP13. Overall SOA exhibits same or slower growth
of the codebase for implementation of data collection.

5.1.2 How does the system‚Äôs maintainability change when data col-
lection tasks are implemented? We used Maintainability index (MI)
metric to answer this question. MI is a score from 0 to 100, and does
not scale with the size of the codebase, which allow us to compare
its absolute values. For three out of four applications maintainabil-
ity was not dramatically impacted in either paradigm, dropping
similar amount of score points (between 2 and 5). But importantly,
MI score has consistently decreased more for the FBP codebase

13As a reminder, MBlogger and Playlist Builder do not create an offline dataset file to
accomplish the data collection task.

InsuranceClaimsMBloggerRideAllocationPlaylistBuilder0.000.050.100.150.200.25Logical Lines of Code,percentage changeFBPSOAInsuranceClaimsMBloggerRideAllocationPlaylistBuilder6543210Maintainability Index,absolute differenceFBPSOAInsuranceClaimsMBloggerRideAllocationPlaylistBuilder0.000.250.500.751.001.251.50Avg Cognitive ComplexityFBPSOAInsuranceClaimsMBloggerRideAllocationPlaylistBuilder0246Number ofAffected ComponentsFBPSOACAIN‚Äô22, May 16‚Äì24, 2021, Pittsburgh, PA, USA

Andrei Paleyes, Christian Cabrera, and Neil D. Lawrence

than it did for the SOA codebase across all four applications, as can
be seen on Figure 4b. The reason for that is the fact that to collect
data in FBP we needed to implement a relatively complex dataflow
graph traversal logic. Since all our codebases are small (average
size is 314 LLOC), this traversal code had a great impact on the MI
metric. A tool designed to collect data from a dataflow graph that
encapsulates such traversal operations would improve developer
experience.

5.1.3 How complex does the application become after implemen-
tation of the data collection task? This question is answered with
the help of cognitive complexity metric. To make this comparison
independent of the size of the codebase, we measure complexity
of each block of code independently, and average these measure-
ments across the whole codebase. Results can be found on Figure
4c. Because of its focus on modeling how the data flows through
the system, FBP code shows lower average complexity across all
four applications, and on three occasions the cognitive complexity
is at least twice as low. This suggests that overall system is simpler
to comprehend when it models dataflow, and thus might be easier
work with in long term.

5.1.4 How intrusive is the data collection task? We counted the
number of components that were affected to implement the data
collection task. FBP required less changes for all applications (Fig-
ure 4d). Only one component had to change for two out of four FBP
codebases that collected the dataset for offline. Lower impact of FBP
in this metric can be explained with the ability of FBP programs
to traverse the dataflow graph, which allows for more localized
changes in the system when it comes to data collection.

5.2 ML model integration task
We now address the same questions in the context of ML model
integration, by comparing measurements of data and ml stages.

5.2.1 How much additional code is required to implement the model
integration task? FBP codebase grew relatively slower at model
integration stage for three applications: growths difference is 15%
for Insurance Claims, 5.3% for MBlogger and 10% for Playlist Builder,
as can be seen on Figure 5a. The difference between the paradigms
is pronounced than at the previous stage, which is a benefit of loose
data coupling. We conclude that FBP requires same or less effort to
host a trained model than SOA.

5.2.2 How does the system‚Äôs maintainability change when the model
integration task is implemented? In terms of maintainability, FBP
exhibited similar behavior at this stage compared to data collection,
dropping between 1 and 4 points, see Figure 5b. Interestingly, SOA
exhibited a different behavior, with much more severe impact for
Insurance Claims and MBlogger: MI of these applications decreased
by 4 and 5 points, compared to 2 points for data collection. The
maintainability of these applications is more impacted because
they need more code (Figure 5a) at this stage. FBP shows more
maintainable way of adding a component that hosts a ML model.

5.2.3 How complex does the application become after implemen-
tation of the ML model integration task? Both paradigms allow to
host a trained models or other data-driven components in a way
that is loosely coupled with the rest of the system. Therefore the

picture around cognitive complexity does not change significantly
when compared to data stage - FBP solutions are still easier to
comprehend, and deployment of data-driven components does not
affect the complexity of the overall solution in either paradigm.

5.2.4 How intrusive is the model integration task? As was the case
with the data collection task, FBP again required changes in fewer
components in all four cases. But the underlying reason for this is
different. Recall that in data collection stage FBP required a dataflow
graph traversal, which on two occasions could be implemented
from a single component of the application. For model hosting no
traversal is required. However the model needs to be given the
data to make predictions. In case of FBP that only means additional
wiring within the graph, to make sure model node is connected to
necessary data streams. In SOA that means changes that have to
made both on service and data access layer, to ensure right services
are invoked and right data is fetched for the model. All in all, FBP
allows for less intrusive changes for both stages of the deployment.

6 DISCUSSION
We evaluated FBP as a potential paradigm for building DOA soft-
ware because of the way it exposes data passing through the system
via the dataflow graph. We expected FBP to be a clearly better choice
for data management and ML deployment than SOA. In this section
we discuss the results of our evaluation and compare the experiment
outcome against these expectations.

SOA-based applications showed smaller size growth in data col-
lection phase. In contrast, FBP codebase often has to deal with
traversing the dataflow graph for the purposes of collecting an
offline dataset from multiple sources, which can be a complex op-
eration to implement. Additionally, FBP required more boilerplate
code to add new components for those applications where data was
collected online. All of these resulted in bigger impact on FBP code-
bases in LLOC and Maintainability Index metrics. However, size of
the codebase is not the only factor when it comes to maintainability
of a system, and FBP excelled in other aspects of maintainabil-
ity. Changes in FBP code are always more localized and have to
be made in fewer parts of the system (according to the Number
of Affected Components metric). Smaller number of components‚Äô
changes brings down the costs of maintenance and increases ro-
bustness of the system over time [47]. FBP code is also easier to
comprehend, according to the Cognitive Complexity metric, which
is again beneficial for maintenance. Quantification of these bene-
fits of DOA systems implemented with FBP can be a subject to a
long-term observational follow-up study.

Interestingly, FBP code showed same or less impact by the model
integration stage across all four metrics. This result might be sur-
prising considering that SOA is usually chosen by professionals
because of its loose coupling of components, and extensibility and
scalability benefits it brings. This kind of robustness to deploy-
ment of a data-driven component is the direct consequence of data
coupling that DOA promotes and FBP exhibits via external data
connections principle.

The evaluation highlighted the gaps that need to be covered to
make FBP a default choice for building DOA software. We have
kept the number of programming tools and frameworks to a mini-
mum, aiming at comparison of the paradigms in their vanilla state.

CAIN‚Äô22, May 16‚Äì24, 2021, Pittsburgh, PA, USA

(a)

(c)

(b)

(d)

Figure 5: Measurements of the impact ML model integration task had on FBP and SOA implementations of four applications.

This also means our evaluation can highlight the areas where ap-
propriate tools can make the biggest improvement for the overall
development experience. For FBP this is clearly the processing of
the dataflow graph. Operations on that graph add a lot of com-
plexity (e.g., node wiring and traversals). We believe that targeted
tools that provide such operations would simplify the development
process with FBP and thus increase the adoption of this paradigm
for data-driven applications.

DOA promotes explicit handling of all data in the system, which
makes data discovery available by design. FBP realises this principle
with the dataflow graph. As discussed in Section 2, in the control
flow approaches (e,g. SOA) data and data relationships are diffi-
cult to discover. This problem grows with scale, as data becomes
scattered between multiple data storage facilities. Therefore the
dataflow graph feature is a significant improvement in FBP applica-
tions compared to SOA. Notably, ability to build, access and traverse
this graph can be achieved with hybrid approaches as well, for in-
stance utilizing FBP approach for definition of the whole graph
and microservices or serverless computing for implementation of
individual components.

7 THREATS TO VALIDITY
7.1 Internal validity
We used a particular toolset for development of all the applications
described in this study. Specifically, we used Python for all our
development, Flask for SOA implementations, and flowpipe for
FBP implementations. The particular choice of tools might have
affected the code metrics collected. We have compensated for this
in a number of ways. First, we have kept the number of tools to a
minimum, to ensure we evaluate the paradigms on their own, and
implementing our own boilerplate whenever necessary (e.g. data
stream abstraction for FBP, entity mapping for SOA). Second, we
developed different types of applications from different domains
following the same architectures and using the same tools. This
variety allows us to counter the possible tools impact.

Code metrics we used might be affected by the particularities of
development process within a single paradigm. For instance, size-
dependent metrics can produce larger values for paradigms that
require more boilerplate code. We accounted for that by considering
relative growth (LLOC) or average values (Cognitive Complexity),
thus converting the metrics to size-independent ones.

7.2 External validity
The main threat to external validity of our work is its environment.
Ideally the experiment described in this paper shall be run in a

InsuranceClaimsMBloggerRideAllocationPlaylistBuilder0.000.050.100.150.200.25Logical Lines of Code,percentage changeFBPSOAInsuranceClaimsMBloggerRideAllocationPlaylistBuilder543210Maintainability Index,absolute differenceFBPSOAInsuranceClaimsMBloggerRideAllocationPlaylistBuilder0.00.51.01.5Avg Cognitive ComplexityFBPSOAInsuranceClaimsMBloggerRideAllocationPlaylistBuilder0123456Number ofAffected ComponentsFBPSOACAIN‚Äô22, May 16‚Äì24, 2021, Pittsburgh, PA, USA

Andrei Paleyes, Christian Cabrera, and Neil D. Lawrence

real production setting and observed over multiple projects. How-
ever that would require running two production ready systems
with equal functionality simultaneously. That would double the
requirements on workforce and computational resources, which is
prohibitively expensive to expect from a business. Additionally, FBP
at the moment is a niche paradigm, rarely used outside of IoT and
embedded systems, which would introduce a skew into such study,
either positive or negative depending on the business domain. In
fact the main purpose of our work is a wider adoption of FBP and
other dataflow-type approaches outside of these areas. For that rea-
son we designed the experiment to model the real ML deployment
process as closely as possible, separating out jobs done by software
engineers and data scientists, and considering a range of various
data science tasks. In that sense the scale of the experiment we
chose is a trade-off between realistic setup and clarity of our mes-
sage. Nevertheless, long-term benefits of adopting FBP as a primary
design paradigm for data driven projects are not fully explored in
this paper, and can be a subject of an extension industrial study.

Design of software systems has to account for other aspects, be-
sides availability and collection of data. Modern distributed systems
have to be fast, resilient, scalable, and secure. We left evaluation
of FBP in all these areas out of scope of this study to keep our
work focused on data science related tasks. However, all of these
aspects are important and shall influence design choices made by
software engineers while designing new systems. An extent of
their influence depends on the experience of the engineering team,
business requirements, available hosting infrastructure, and other
project-specific factors.

8 RELATED WORK
Given the popularity of SOA, many attempts have been made to
re-think this software design approach and enhance it with better
data handling capabilities. G√∂tz et al. [14] proposes a data-driven ap-
proach towards designing microservices, which requires a complete
data picture of a business as a prerequisite - something that can be
difficult to acquire for any medium-to-large-sized business because
of a sheer number of data flows and no built-in solutions to track
them. Uber Engineering‚Äôs Domain-Oriented Microservice Architec-
ture [13] can be seen as a step forward in this line of thinking, as this
design approach only requires high-level understanding of main
entities and the domains a business operates with. On a different
note, Safina et al. extends Jolie, a programming language based on
the microservices paradigm, in a way that allows to build services
with Jolie in data-driven fashion [46]. Z. Dehghani from technology
consultancy company Thoughtworks proposed data meshes - a
distributed data storage architecture that replaces monolithic data
lakes [9], which improves data ownership decoupes data process-
ing pipelines, but still hides data behind services. Contrary to this
strand of software engineering research, rather than incrementally
improving SOA, we wanted to consider using radically different
approaches for better data management in software development.
The machine learning community is well aware of the concept of
data flow. It is often used for implementation of ML tools. Notable
examples include Tensorflow [1], a general purpose library with
focus on neural networks, Neuflow [11], a hardware architecture op-
timized for the computation of general-purpose vision algorithms,

RLFlow [26], a reinforcement learning library, and more. The key
difference between these tools and the analysis presented in this
paper is that while the aforementioned tools are used to implement
new ML algorithms, we explore general receptivity of FBP-driven
software application to deployment of ML systems. Metaflow devel-
oped by Netflix [15] shares our motivation described in Section 2
and implements many concepts discussed in the paper, and yet is
also positioned solely as a tool for data science projects. We argue
that there is certain benefit for considering Metaflow and similar
tools in a broader scope of any data-processing applications.

A duality of control flow and data flow for building software was
explored by the computer science community before [16, 23, 50].
As a form of dataflow, FBP was contrasted with other paradigms
and design principles, although never in the context of ML. In his
book ‚ÄúFlow-Based Programming: A new approach to application
development‚Äù the creator of FBP J.P. Morrison compares FBP and
Object-Oriented Programming (OOP) [35]. Similarly, Roosta con-
trasted dataflow and functional programming paradigms in his book
‚ÄúParallel Processing and Parallel Algorithms: theory and computa-
tion‚Äù [45]. On a more practical side, Mironov et al. [33] compared
OOP and data-oriented approaches in the context of back-testing
for trading strategies. They found the data-oriented approach is
more efficient because of parallelism mechanisms. Lobunets and
Krylovskiy implemented the same IoT processing system with SOA
and FBP, and reported their experiences in regard to code coupling,
debugging and testing [28]. While our methodology is similar to
this work, our evaluation focuses on data and ML.

9 CONCLUSIONS AND FUTURE WORK
This paper presents the results of an evaluation of FBP for ML
deployment by comparing it against widely used SOA paradigm.
We implemented four data-driven applications in both paradigms,
completed a data science task for each of them, and measured
evolution of the codebase throughout the process.

FBP is considered primarily a paradigm for distributed comput-
ing. Our experiment shows that FBP is a viable paradigm for devel-
oping general-purpose software according to DOA principles. We
showed how FBP features (e.g., dataflow graph and data coupling)
make data discovery and collection easier than in a traditional SOA
setting. In addition, we highlighted how better tooling, that allows
developers to define and traverse dataflow graphs at a higher level
of abstraction, could help improve FBP adoption.

There are multiple ways to extend the experiment described in
this paper. The same ML deployment perspective can be considered
in the distributed context, where data streaming platforms (e.g.,
Apache Kafka) can be used. Observing evolution of a production
DOA system implemented with FBP over a long period of time can
quantify maintainability effects. Other paradigms (e.g., Actor model
[17]) can be considered for comparison. Given that a lot of systems
are already implemented with SOA, an analysis of full or partial
migration from SOA to FBP for an existing system can be relevant.
We hope that our results highlight potential for FBP to become
a standard paradigm for implementation of DOA systems. Explo-
ration of approaches that promote better treatment of data is crucial
to address challenges in ML deployment, considering the growing
demand to leverage data for scientific and business purposes.

REFERENCES
[1] Mart√≠n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al.
2016. Tensorflow: Large-scale machine learning on heterogeneous distributed
systems. arXiv preprint arXiv:1603.04467 (2016).

[2] H Bergius. 2015. NoFlo‚Äìflow-based programming for Javascript. URL: http:

// noflojs.org (2015).

[3] Tom Borchert. 2020. Milan: An Evolution of Data-Oriented Programming. Available

at https://tborchertblog.wordpress.com/2020/02/13/28/.

[4] Christian Cabrera, Fan Li, Vivek Nallur, Andrei Palade, MA Razzaque, Gary
White, and Siobh√°n Clarke. 2017. Implementing heterogeneous, autonomous,
and resilient services in IoT: An experience report. In A World of Wireless, Mobile
and Multimedia Networks (WoWMoM), 2017 IEEE 18th International Symposium
on. IEEE, 1‚Äì6.

[5] G Ann Campbell. 2018. Cognitive complexity: An overview and evaluation. In

Proceedings of the 2018 international conference on technical debt. 57‚Äì58.

[6] Paris Carbone, Asterios Katsifodimos, Stephan Ewen, Volker Markl, Seif Haridi,
and Kostas Tzoumas. 2015. Apache Flink: Stream and batch processing in a
single engine. Bulletin of the IEEE Computer Society Technical Committee on Data
Engineering 36, 4 (2015).

[7] Zenon Chaczko and Robin Braun. 2017. Learning data engineering: Creating
IoT apps using the node-RED and the RPI technologies. In 2017 16th Interna-
tional Conference on Information Technology Based Higher Education and Training
(ITHET). IEEE, 1‚Äì8.

[8] Diego Clerissi, Maurizio Leotta, Gianna Reggio, and Filippo Ricca. 2018. Towards
an approach for developing and testing Node-RED IoT systems. In Proceedings
of the 1st ACM SIGSOFT International Workshop on Ensemble-Based Software
Engineering. 1‚Äì8.

[9] Zhamak Dehghani. 2019. How to move beyond a monolithic data lake to a

distributed data mesh. Martin Fowler‚Äôs Blog (2019).

[10] Tom Diethe, Tom Borchert, Eno Thereska, Borja Balle, and Neil Lawrence. 2019.

Continual learning in practice. arXiv preprint arXiv:1903.05202 (2019).

[11] Cl√©ment Farabet, Berin Martini, Benoit Corda, Polina Akselrod, Eugenio Cu-
lurciello, and Yann LeCun. 2011. Neuflow: A runtime reconfigurable dataflow
processor for vision. In CVPR 2011 workshops. IEEE, 109‚Äì116.

[12] Iris Figalist, Christoph Elsner, J. Bosch, and Helena Holmstr√∂m Olsson. 2022.
Breaking the vicious circle: A case study on why AI for software analytics and
business intelligence does not take off in practice. J. Syst. Softw. 184 (2022),
111135.

[13] Adam Gluck. 2020. Introducing Domain-Oriented Microservice Architecture.

Uber Engineering Blog (2020).

[14] Benjamin G√∂tz, Daniel Schel, Dennis Bauer, Christian Henkel, Peter Einberger,
and Thomas Bauernhansl. 2018. Challenges of production microservices. Procedia
CIRP 67 (2018), 167‚Äì172.

[15] Savin Goyal. 2020. More Data Science, Less Engineering: A Netflix Original.

USENIX Association.

[16] Wilhelm Hasselbring, Maik Wojcieszak, and Schahram Dustdar. 2021. Control
Flow Versus Data Flow in Distributed Systems Integration: Revival of Flow-Based
Programming for the Industrial Internet of Things. IEEE Internet Computing 25,
4 (2021), 5‚Äì12.

[17] Carl Hewitt. 2010. Actor model of computation: scalable robust information

systems. arXiv preprint arXiv:1008.1459 (2010).

[18] Rajive Joshi. 2007. Data-oriented architecture: A loosely-coupled real-time SOA.

whitepaper, Aug (2007).

[19] Jay Kreps, Neha Narkhede, Jun Rao, et al. 2011. Kafka: A distributed messaging

system for log processing. In Proceedings of the NetDB, Vol. 11. 1‚Äì7.

[20] SPT Krishnan and Jose L Ugia Gonzalez. 2015. Google Cloud Dataflow. In Building

Your Next Big Thing with Google Cloud Platform. Springer, 255‚Äì275.

[21] Samuel Lampa, Jonathan Alvarsson, and Ola Spjuth. 2016. Towards agile large-
scale predictive modelling in drug discovery with flow-based programming
design principles. Journal of cheminformatics 8, 1 (2016), 1‚Äì12.

[22] Stig B. M. Larsson, Anders Wall, and Peter Wallin. 2007. Assessing the influence

on processes when evolving the software architecture. In IWPSE ‚Äô07.

[23] Hugh C Lauer and Roger M Needham. 1979. On the duality of operating system

structures. ACM SIGOPS Operating Systems Review 13, 2 (1979), 3‚Äì19.

[24] Neil Lawrence. 2019. Modern Data Oriented Programming. Available at http://
inverseprobability.com/talks/notes/modern-data-oriented-programming.html.
[25] Grace A. Lewis, Ipek Ozkaya, and Xiwei Xu. 2021. Software Architecture
Challenges for ML Systems. In 2021 IEEE International Conference on Soft-
ware Maintenance and Evolution (ICSME). 634‚Äì638. https://doi.org/10.1109/
ICSME52107.2021.00071

[26] Eric Liang, Zhanghao Wu, Michael Luo, Sven Mika, and Ion Stoica. 2020.
Distributed Reinforcement Learning is a Dataflow Problem. arXiv preprint
arXiv:2011.12719 (2020).

[27] Jimmy Lin and Dmitriy Ryaboy. 2013. Scaling big data mining infrastructure: the
Twitter experience. Acm SIGKDD Explorations Newsletter 14, 2 (2013), 6‚Äì19.

CAIN‚Äô22, May 16‚Äì24, 2021, Pittsburgh, PA, USA

[28] O Lobunets and A Krylovskiy. 2014. Applying flow-based programming method-

ology to data-driven applications development for smart environments.

[29] Lucy Ellen Lwakatare, Aiswarya Raj, Ivica Crnkovic, J. Bosch, and Helena Holm-
str√∂m Olsson. 2020. Large-scale machine learning systems in real-world industrial
settings: A review of challenges and solutions. Inf. Softw. Technol. 127 (2020),
106368.

[30] Dave Mason and Kruti Dave. 2017. Block-based versus flow-based programming
for naive programmers. In 2017 IEEE Blocks and Beyond Workshop (B&B). IEEE,
25‚Äì28.

[31] Thomas J McCabe. 1976. A complexity measure. IEEE Transactions on software

Engineering 4 (1976), 308‚Äì320.

[32] Xiangrui Meng, Joseph Bradley, Burak Yavuz, Evan Sparks, Shivaram Venkatara-
man, Davies Liu, Jeremy Freeman, DB Tsai, Manish Amde, Sean Owen, et al.
2016. Mllib: machine learning in Apache Spark. The Journal of Machine Learning
Research 17, 1 (2016), 1235‚Äì1241.

[33] Timur Mironov, Lilia Motaylenko, Dmitry Andreev, Igor Antonov, and Mikhail
Aristov. 2021. Comparison of Object-Oriented Programming and Data-Oriented
Design for Implementing Trading Strategies Backtester. In ENVIRONMENT.
TECHNOLOGIES. RESOURCES. Proceedings of the International Scientific and
Practical Conference, Vol. 2. 124‚Äì130.

[34] J Paul Morrison. 1994. Flow-based programming. In Proc. 1st International Work-
shop on Software Engineering for Parallel and Distributed Systems. 25‚Äì29.
[35] J Paul Morrison. 2010. Flow-Based Programming: A new approach to application

development. CreateSpace.

[36] Alfredo Nazabal, Christopher KI Williams, Giovanni Colavizza, Camila Rangel
Smith, and Angus Williams. 2020. Data engineering for data analytics: a classifi-
cation of the issues, and case studies. arXiv preprint arXiv:2004.12929 (2020).
[37] Robert L. Nord, Ipek Ozkaya, and Philippe B Kruchten. 2014. Agile in Distress:

Architecture to the Rescue. In XP Workshops.

[38] O‚ÄôReilly. 2020. Microservices Adoption in 2020: A survey. Available at https:

//www.oreilly.com/radar/microservices-adoption-in-2020/.

[39] Ipek Ozkaya. 2020. What Is Really Different in Engineering AI-Enabled Systems?
IEEE Software 37, 4 (2020), 3‚Äì6. https://doi.org/10.1109/MS.2020.2993662
[40] Andrei Paleyes, Christian Cabrera, and Neil D Lawrence. 2021. Towards better
data discovery and collection with flow-based programming. Data-centric AI
workshop, NeurIPS (2021).

[41] Andrei Paleyes, Raoul-Gabriel Urma, and Neil D Lawrence. 2020. Challenges in de-
ploying machine learning: a survey of case studies. arXiv preprint arXiv:2011.09926
(2020).

[42] Michael P Papazoglou and Dimitrios Georgakopoulos. 2003. Service-oriented

computing. Commun. ACM 46, 10 (2003), 25‚Äì28.

[43] Randall Perrey and Mark Lycett. 2003. Service-oriented architecture. In 2003
Symposium on Applications and the Internet Workshops, 2003. Proceedings. IEEE.
[44] Neoklis Polyzotis, Sudip Roy, Steven Euijong Whang, and Martin Zinkevich.
2018. Data lifecycle challenges in production machine learning: a survey. ACM
SIGMOD Record 47, 2 (2018), 17‚Äì28.

[45] Seyed H Roosta. 2012. Parallel processing and parallel algorithms: theory and

computation. Springer Science & Business Media.

[46] Larisa Safina, Manuel Mazzara, Fabrizio Montesi, and Victor Rivera. 2016. Data-
driven workflows for microservices: Genericity in Jolie. In 2016 IEEE 30th Interna-
tional Conference on Advanced Information Networking and Applications (AINA).
IEEE, 430‚Äì437.

[47] Ian Sommerville. 2011. Software engineering 9th Edition. ISBN-10 137035152

(2011), 18.

[48] Ben Stopford. 2016. The Data Dichotomy: Rethinking the Way We Treat Data and
Services. Available at https://www.confluent.io/blog/data-dichotomy-rethinking-
the-way-we-treat-data-and-services/.

[49] Tomasz Szydlo, Robert Brzoza-Woch, Joanna Sendorek, Mateusz Windak, and
Chris Gniady. 2017. Flow-based programming for IoT leveraging fog computing.
In 2017 IEEE 26th International Conference on Enabling Technologies: Infrastructure
for Collaborative Enterprises (WETICE). IEEE, 74‚Äì79.

[50] Philip C Treleaven. 1982. Towards a Decentralised General-Purpose Computer.

In Programmiersprachen und Programmentwicklung. Springer, 21‚Äì31.

[51] Christian Vorhemus and Erich Schikuta. 2017. A data-oriented architecture
for loosely coupled real-time information systems. In Proceedings of the 19th
International Conference on Information Integration and Web-based Applications &
Services. 472‚Äì481.

[52] Claes Wohlin, Per Runeson, Martin H√∂st, Magnus C Ohlsson, Bj√∂rn Regnell, and
Anders Wessl√©n. 2012. Experimentation in software engineering. Springer Science
& Business Media.

[53] Jesse Zaman, Lode Hoste, and Wolfgang De Meuter. 2015. A flow-based pro-
gramming framework for mobile app development. In Proceedings of the 3rd
International Workshop on Programming for Mobile and Touch. 9‚Äì12.

