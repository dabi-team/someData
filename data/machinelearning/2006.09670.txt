LazyIter: A Fast Algorithm for Counting Markov Equivalent DAGs and
Designing Experiments

Ali AhmadiTeshnizi 1 Saber Salehkaleybar 1 Negar Kiyavash 2

0
2
0
2

n
u
J

7
1

]
L
M

.
t
a
t
s
[

1
v
0
7
6
9
0
.
6
0
0
2
:
v
i
X
r
a

Abstract

The causal relationships among a set of random
variables are commonly represented by a Di-
rected Acyclic Graph (DAG), where there is a di-
rected edge from variable X to variable Y if X
is a direct cause of Y . From the purely observa-
tional data, the true causal graph can be identiﬁed
up to a Markov Equivalence Class (MEC), which
is a set of DAGs with the same conditional inde-
pendencies between the variables. The size of an
MEC is a measure of complexity for recovering
the true causal graph by performing interventions.
We propose a method for efﬁcient iteration over
possible MECs given intervention results. We
utilize the proposed method for computing MEC
sizes and experiment design in active and passive
learning settings. Compared to previous work for
computing the size of MEC, our proposed algo-
rithm reduces the time complexity by a factor of
O(n) for sparse graphs where n is the number of
variables in the system. Additionally, integrating
our approach with dynamic programming, we de-
sign an optimal algorithm for passive experiment
design. Experimental results show that our pro-
posed algorithms for both computing the size of
MEC and experiment design outperform the state
of the art.

1. Introduction

Directed Acyclic Graphs (DAGs) are the most commonly
used structures to represent causal relations between
random variables, where a directed edge X → Y means
that variable X is a direct cause of variable Y . Condi-

1Department of Electrical Engineering,

Sharif Uni-
Iran 2School of Manage-
versity of Technology, Tehran,
ment of Technology, Ecole Polytechnique Federale de
Correspondence to: Ali Ahma-
Lausanne, Switzerland.
diTeshnizi
Saber
Salehkaleybar <saleh@sharif.edu>, Negar Kiyavash <ne-
gar.kiyavash@epﬂ.ch>.

<ali.ahmadi215@student.sharif.edu>,

Copyright 2020 by the author(s).

In some scenarios,

tional independencies between different variables can be
inferred from observational data and consequently,
the
ground truth graph is identiﬁed up to Markov Equivalence
Class (MEC) (Pearl, 2009; Spirtes et al., 2000). Unique
identiﬁcation of the ground truth DAG among the graphs
in an MEC generally requires interventions on variables
(Eberhardt & Scheines, 2007).
in-
in biological
terventions could be costly (for instance,
experiments), and therefore, selecting the optimal interven-
tion target to learn the causal structure is of great interest
(Eberhardt et al., 2005; He & Geng, 2008; Eberhardt,
Shanmugam et al.,
2012; Hauser & Bhlmann,
2018;
2015; Kocaoglu et al.,
Lindgren et al., 2018; Agrawal et al., 2019).
Several
metrics have been suggested in the literature for target
selection (He & Geng, 2008; Hauser & Bhlmann, 2014;
Ghassami et al., 2018; Agrawal et al., 2019). A good
metric for measuring the effectiveness of an intervention
is the number of remaining DAGs in an MEC after the
intervention (He & Geng, 2008). To use this metric for
target selection, we must be able to efﬁciently count the
number of DAGs in an MEC.

2017; Ghassami et al.,

2014;

Some previous work used the clique tree representation
of chordal graphs to divide the causal graph into smaller
subgraphs, and perform counting on each subgraph sep-
arately (Ghassami et al., 2019; Talvitie & Koivisto, 2019).
The main issue with this approach is the dependence on
the maximum clique size which can result in O(n!) oper-
ations in some cases where n is the number of variables.
Ghassami et al. (2019) and Talvitie & Koivisto (2019) used
dynamic programming to count DAGs in an MEC. The
best time complexity of these approaches is in the order
of O(2nn4). However, both approaches do not take advan-
tage of sparsity if the graph is sparse. In some other work,
the number of edges oriented after an intervention is pro-
posed as the target selection metric (Hauser & Bhlmann,
2014). Hauser & Bhlmann (2014) used the idea of condi-
tioning on different edge orientations for edges connected
to a single node to choose the optimal single-node inter-
vention target. The time complexity of proposed algorithm
depends on the size of largest clique in MEC which in the
worst case is exponential. Recently, several work have been
proposed for experiment design in passive and active learn-

 
 
 
 
 
 
LazyIter: A Fast Algorithm for Counting Markov Equivalent DAGs and Designing Experiments

ing settings which use the aforementioned metrics for tar-
get selection (Ghassami et al., 2018; Kocaoglu et al., 2017;
Agrawal et al., 2019). As such, it is desirable to efﬁciently
compute the MEC size.

In this paper, we propose “LazyIter”, a method for efﬁ-
ciently iterating over possible DAGs that we might get after
an intervention on a single node. In this method, we start by
setting a node as the root of the DAG and ﬁnding the corre-
sponding essential graph resulting from intervening on this
node. Subsequently, we take advantage of similarities be-
tween different candidate graphs to eliminate the recalcula-
tion of edge orientations and ﬁnd other graphs just by reori-
enting a small subset of the edges. We utilize this method
to design algorithms for computing the size of MECs and
solving the budgeted experiment design problem in active
and passive settings. The main contributions of this paper
are the following:

• We propose an algorithm for computing the MEC size
of a graph, which improves the time complexity by a
factor of O(n) in sparse graphs with respect to previ-
ous work (Talvitie & Koivisto, 2019; Ghassami et al.,
2019). Our experiments show that the algorithm out-
performs previous work in dense graphs too.

• In the active learning setting, we propose two al-
gorithms for designing experiments for both met-
rics discussed earlier (number of edges and size of
MEC). These algorithms are up to O(n) times faster
than the previous approaches (He & Geng, 2008;
Hauser & Bhlmann, 2014).

• In the passive learning setting, we propose a dynamic
programming algorithm for experiment design. To the
best of our knowledge, this is the ﬁrst efﬁcient ex-
act algorithm capable of ﬁnding the optimal solution
in the passive learning setting. The most closely re-
lated work is an approximation algorithm presented
in (Ghassami et al., 2019), which has a considerably
higher computational complexity.

The paper is organized as follows: First, we discuss the ter-
minology and preliminaries in Section 2. Then, in Section
3, we explain our iteration approach and prove its correct-
ness. In Sections 4 and 5, we apply this approach to design
algorithms for computing the MEC size and experiment de-
sign and also analyze their complexities. Finally, in Sec-
tion 6, we demonstrate the efﬁciency of these algorithms
by evaluating them on a diverse set of MECs.

2. Preliminaries

2.1. Graph Terminology

A graph G(V, E) is represented with a set of nodes V and
a set of edges E, where each edge is a pair (a, b) such
that a, b ∈ V . We say there is an undirected edge be-
tween nodes a and b if both (a, b) ∈ E, (b, a) ∈ E, and
say there is a directed edge from a to b if (a, b) ∈ E, and
(b, a) /∈ E. A directed (undirected) edge is denoted with
a → b ∈ G (or a − b ∈ G). We also use (a, b) ∈ E
and (a, b) ∈ G subsequently. The set of all directed edges
of G is denoted by Dir(G), and the number of directed
edges in G is denoted by |Dir(G)|. A graph is called undi-
rected (directed) if all of its edges are undirected (directed),
and is called partially directed if it has both undirected and
directed edges. The induced subgraph G[S] is the graph
with node set S and with edge set containing all of the
edges in E that have both endpoints in S. Union of graphs
G1(V, E1), G2(V, E2), ..., Gk(V, Ek) with the same set of
k
k
nodes is deﬁned as
i=1 Ei). For conve-
i=1 Gi = G(V,
nience, we may use G and V interchangeably. Two graphs
are equal if they have the same set of nodes and the same
set of edges.

S

S

A path is a sequence of nodes x1, x2, x3, . . . , xk such that
∀1 ≤ i < k : (xi, xi+1) ∈ E. A cycle is a sequence of
nodes x1, x2, ..., xk such that ∀1 ≤ i ≤ k : (xi, xi+1) ∈ E
where xk = x1. A path (cycle) is called directed if all of
its edges are directed. Node x is called a descendant of
node v if there is directed path from v to x, and there are
no directed paths from x to v in the graph. A chain graph
is a graph with no directed cycles, and a chain component
is a connected component of a chain graph after removing
all its directed edges. An undirected graph is chordal if for
every cycle of length four or more in it, there exists an edge
which is not a part of the cycle but connects two nodes of
the cycle to each other.

Let G(V, E) be a partially directed graph. The skeleton
of G is an undirected graph that we get by replacing all
of the directed edges in E by undirected edges. We say
node v ∈ V is separated from node u by set T ⊂ V if
there is no path from v to u in the skeleton of G[V \T ], and
we call T , a (v, u)-separator in G 1. We denote parents,
children, and neighbors of node v ∈ V by paG(v), chG(v),
and neG(v), respectively. A perfect elimination ordering
(PEO) in a graph G is an ordering of its vertices such that
for every vertex v, v and its neighbors prior to it in the
ordering form a clique. A graph is chordal if and only if
it has a perfect elimination ordering (Fulkerson & Gross,
1965).

1Please note that the deﬁnition of separator here is different
from the deﬁnition of d-separation in causal Bayesian networks.

LazyIter: A Fast Algorithm for Counting Markov Equivalent DAGs and Designing Experiments

2.2. Causal Model

3. LazyIter

A causal DAG D is a DAG with variables V1, · · · , Vn
where there is a directed edge from Vi to Vj if Vi is a direct
cause of Vj . A joint probability distribution P over these
variable satisﬁes Markov property with respect to D if any
variable is independent of its non-descendants given its par-
ents. A Markov Equivalence Class (MEC) is a set of DAGs
with the same Markov property. Verma & Pearl (1992)
showed that the graphs in an MEC have the same skeleton
and the same set of v-structures (induced subgraphs of the
form a → b ← c). The essential graph of D is deﬁned as a
partially directed graph G(V, E) where E is the union of all
edge sets of the DAGs in the same MEC as D. An essential
graph is necessarily a chain graph with chordal chain com-
ponents (Hauser & Bhlmann, 2011). Verma & Pearl (1992)
showed that having observational data, essential graph is
obtainable by applying four rules (called ”Meek” rules)
consecutively on the graph, until no more rules are appli-
cable. A valid orientation of edges of a chain component
is an orientation in which no cycles and no v-structures are
formed. An intervention target I ⊆ V is a set of nodes
which we intervene on simultaneously. An intervention
family I is a set of intervention targets. Intervention graph
D(I) is the DAG we get from D after removing all edges
directed towards nodes in I.

Deﬁnition 1. For a set of intervention targets I, two DAGs
D1 and D2 are called I-Markov Equivalent (denoted with
D1 ∼I D2) if they are statistically indistinguishable under
intervention targets in I.

Hauser & Bhlmann (2011) proved that two DAGs D1 and
D2 are I-Markov equivalent if and only if D1 and D2 have
the same set of v-structures, and D(I)
have the
1
same skeleton for every I ∈ I ∪ {∅}.

and D(I)
2

The I-essential graph EI (D) of a DAG D(V, E) is a par-
tially directed graph with the node set V and the edge set
equal to the union of all edge sets of the DAGs which are
I-Markov equivalent with D. I-MEC is deﬁned as the set
of all DAGs that are I-Markov equivalent.

Deﬁnition 2. For undirected chordal chain graph (UCCG)
G(V, E) and intervention family I, the intervention result
space is deﬁned as:

IRI(G) = {EI(D) : D ∈ D(G)},

where D(G) denotes the set of all DAGs inside MEC corre-
sponding to G.

We use M EC(EI (D)) to show the set of all DAGs in an I-
MEC. Throughout the paper, we assume UCCGs are chain
components of observational essential graphs.

We ﬁrst propose a method to select the best single-node in-
tervention target in an essential graph. As I-essential graph
EI (D) on DAG D is a chain graph with undirected chordal
chain components, it could be shown that knowing orien-
tations of edges inside a component does not provide any
information about the orientation of edges in other com-
ponents (Hauser & Bhlmann, 2011). Hauser & Bhlmann
(2014) showed that each chain component can be treated
as an observational essential graph when it comes to inter-
vening on the nodes (i.e., D(G) is the same set of DAGs,
whether G is an observational essential graph or it is a
chain component of an I-essential graph). Consequently,
we can restrict our attention to UCCGs. He & Geng (2008)
presented a method to ﬁnd I-essential graph from interven-
tion results when the intervention target is the root, which
takes O(n∆2) operations. We will use this method in the
next sections as a subroutine for computing the size of I-
essential graph whenever conditioning on edge orientations
results in the intervention target becoming root.

Let G(V, E) be a UCCG and {v} be a single-node interven-
tion target on it. After the intervention, we will obtain an
I-essential graph E{{v}}(D) ∈ IR{{v}}(G) based on the
ground truth DAG D. The following theorem allows us to
use parent set of v for uniquely representing the resulting
I-essential graph:
Proposition 1. Let G(V, E) be a UCCG, D ∈ M EC(G)
be a DAG, and v ∈ V be an arbitrary node. Then each
I-essential graph E{{v}}(D) could be uniquely determined
given the parent set of v in D, and there is a one-to-one cor-
respondence between sets {P ⊆ neG(v) : P is a clique}
and IR{{v}}(G) .

The proof of this proposition as well as all other proofs are
available in the supplementary material. The theorem sug-
gests a way for iterating over IR{{v}}(G): Iterate over all
cliques in the neighborhood of v and set each clique as the
parent set of v and then apply Meek rules to orient as many
edges as possible (Hauser & Bhlmann, 2014). According
to Proposition 1, the essential graph E{{v}}(D) can be de-
termined by paD(v). Thus, we use the notation of P P
v (G)
to point to E{{v}}(D) where D ∈ D(G) is a DAG such that
paD(v) = P .

Let R(V, E′) = P P
v (G) be a possible single-node-
intervention result on a UCCG G(V, E). Setting aside the
nodes in P , we divide the other nodes of R into three dis-
tinct groups CR, AR, and DR. CR is the set of children of
v, and CR ∪ P = neG(v). AR is the set of all nodes which
are separated from v by P , and DR is the set of all other
nodes. We have:

AR = {a ∈ V \neG(v) : P is an (a, v)-separator in G}

DR = V \(AR ∪ CR ∪ P ).

LazyIter: A Fast Algorithm for Counting Markov Equivalent DAGs and Designing Experiments

v

AR

P

CR

DR

a

AR

P

M

v

u

b

CR\{u}

DR\M

Figure1. a) A separation of R = P P
from R by moving u from CR to P and M
from DR to AR. An arrow between two sets/nodes means that any edge between them is directed in the corresponding direction. A
straight line between two sets/nodes, means that any edge between them is undirected. Dashed lines show how nodes are moved from
DR to AR upon the construction.

v into different sets. b) Constructing R′ = P P ∪{u}

v

See Figure 1a for an illustration. The following theorem
states several key properties of the three proposed node
groups.
Theorem 1. Let R = P P
a UCCG G(V, E). The following statements hold:

v (G) be an I-essential graph on

• There are no edges in G connecting a node in AR to a

node in CR ∪ DR ∪ {v}.

• Every edge (a, b) in R where a ∈ P and b ∈ CR is

directed as a → b.

• Every edge (a, b) in R where a ∈ CR ∪P and b ∈ DR

is directed as a → b.

• All of the edges in R[AR ∪ P ] are undirected.

Now we show that direction of many edges in P P
v (G)
stay intact when we change the parent set P slightly, and
therefore if we already know direction of edges in an I-
essential graph, we can ﬁnd the direction of edges in other
I-essential graphs by reorienting just a small fraction of the
edges.

v

Assume we are given R and we want to ﬁnd R′ =
P P ∪{u}
(G) where u ∈ CR, and G[P ∪ {u}] is a clique.
Note that the skeleton of both R′ and R is G, and they only
differ in the direction of some edges.

It is easy to see AR ⊆ AR′ as every node which is separated
from v by P is also separated from v by P ∪{u}. Moreover
we know that AR ∪DR = AR′ ∪DR′ as both of them repre-
sent the set of nodes in V \(neG(v) ∪ {v}). Consequently,
we have DR′ ⊆ DR and DR\DR′ = AR′ \AR = M . We
construct R′ from R by moving u from children to the par-
ents and M from DR to AR, and then reorienting some

speciﬁc edges as we explain. We have:

M = {a ∈ DR : P ∪ {u} is an (a, v)-separator in G}

CR′ = CR\{u}, AR′ = AR ∪ M, DR′ = DR\M.

Applying the ﬁrst statement of Theorem 1 to R′, we con-
clude that there are no edges between M and DR\M in
G (as M ⊆ AR′ and DR\M = DR′ ). The third state-
ment of Theorem 1 implies that in R′, any edge between
(P ∪ {u}) ∪ CR′ = P ∪ CR and DR′ = DR\M is di-
rected towards the node in DR\M . The same thing is
true in R, as we have DR\M ⊆ DR. This means any
edge in R[DR\M ] which is directed by applying Meek
rules, can be similarly directed in R′[DR\M ], and there-
fore R′[DR\M ] = R[DR\M ]. Moreover, we can say that
R′[AR ∪ P ] = R[AR ∪ P ] because both are undirected
graphs on the same skeleton. Using the fourth statement of
Theorem 1, we can infer that all of edges in R′[M ∪{u}] are
undirected, as M ∪ {u} ⊆ AR′ . The same is true for edges
with one end in M ∪ {u} and the other end in P . Finally,
by the second statement of Theorem 1, all of the edges in
R′[CR] which are connected to u are directed away from u.
This means we can ﬁnd the orientation of edges in R′ by
executing the following three steps on R:

1. Obtain the set M by ﬁnding nodes in G[V \AR] which
are separated from v by (P ∪ {u}). If we execute a
breadth ﬁrst search (BFS) in G[V \(P ∪ {u})] with v
as root, the nodes which are not observed in the BFS
constitute AR′ . By removing nodes of AR from AR′
we will get the set M . This will take O(n + m) =
O(n + n∆) = O(n∆) operations, where n, m, and ∆
are the number of variables, the number of the edges,
and the maximum degree of the graph respectively.

LazyIter: A Fast Algorithm for Counting Markov Equivalent DAGs and Designing Experiments

Algorithm 1 LazyIter

1: Input: UCCG G(V, E), Node v ∈ V
2: Output: IR{{v}}(G)
3: L ← ∅
4: Find P ∅
5: Iter(P ∅
6: return L

v (G) by setting v as the root of G and orienting as much edges as possible.
v (G), v)

———————————————————————————————————————————————

v (G), v)

7: function Iter(P P
8: Add P P
v (G) to L.
9: for u ∈ CR do
10:
11:
12:
13:
14:
15:
16:
17:
18:
end if
19:
20: end for

if u is connected to all nodes in P then

v (G)

N ewGraph ←− P P
M ←− Set of all nodes separated from v by P ∪ {u} in G[V \AP P
Change direction of v → u to u → v in N ewGraph
In N ewGraph, make all edges with both ends in M ∪ {u} undirected
In N ewGraph, make all edges connecting a node in P to a node in M ∪ {u} undirected
In N ewGraph, direct all edges between u and a node c ∈ CR as u → c
In N ewGraph[CP P
Iter(N ewGraph, v)

v (G)]

v (G)], orient edges using Meek rules until no more undirected edges are orientable

2. Remove the directions of all edges inside R[M ∪ {u}]
and all edges between M ∪ {u} and P . This could be
done in O(n∆) operations.

3. Direct all edges u − x in R[CR] as u → x, and apply
Meek rules on R[CR] to ﬁnd R′[CR\{u}]. This could
be done in O(∆3) operations (He et al., 2015), as we
have |CR| ≤ ∆.

The procedure for ﬁnding IR{{v}}(G) is given in Algo-
rithm 1.
In order to ﬁnd IR{{v}}(G), ﬁrst we obtain
P ∅
v (G) by setting v as the root of the graph and direct-
ing edges based on Meek rules in O(n∆2) operations
(He et al., 2015). Then we initiate L as an empty set and
call LazyIter(P ∅
v (G), v) which will add all desired I-
essential graphs to set L (for ﬁnding P ∅
v (G) we can use
the algorithm presented in (He et al., 2015) which needs
O(n∆2) operations). The algorithm will call itself recur-
sively O(2∆) times, and the three mentioned operations are
executed in each call in order to ﬁnd the new I−essential
graph corresponding to the new parent set. When the ex-
ecution is completed, L will contain the list of all obtain-
able I-essential graphs. The complexity of the algorithm is
O(n∆2 + 2∆(n∆ + ∆3)) = O(2∆(n∆ + ∆3)). The ﬁrst
step is executed in line 12 of Algorithm 1, the second step
is executed in lines 14 and 15, and the last step is executed
in lines 16 and 17.

4. Computing size of MEC

We count the number of DAGs inside an MEC by partition-
ing them into I-Markov equivalence classes.
Lemma 1. Let G(V, E) be a UCCG and I be an arbitrary
intervention family. Then we have:

|M EC(G)| =

X
R∈IRI (G)

h Y

C∈C(R)

|M EC(C)|i,

where C(R) denotes the set of all chain components of R.

Assume we are given a UCCG G(V, E) and want to cal-
culate |M EC(G)|. We ﬁrst choose an arbitrary node
v ∈ V , set I = {{v}}, and use LazyIter to ﬁnd all
of the I-essential graphs. Then for each of them, we cal-
culate the number of DAGs inside its corresponding I-
MEC by multiplying size of its chain components. As
each chain component of an I-essential graph is a UCCG
(Hauser & Bhlmann, 2014), Lemma 1 is applicable on it
and we could do the calculation recursively. Finally, we
sum up all these values to get |M EC(G)|.

We take advantage of dynamic programming to elim-
(2019);
inate repetitive calculations.
Talvitie & Koivisto (2019) used a similar idea for obser-
vational essential graphs, which we extended to interven-
tional cases.

Ghassami et al.

The algorithm is presented in Algorithm 2. Every time
Count(S) is called, it will take O(1) operations if DP [S]
is already calculated. Otherwise, it calls LazyIter once

LazyIter: A Fast Algorithm for Counting Markov Equivalent DAGs and Designing Experiments

Algorithm 2 LazyCount

Algorithm 3 Active Learning by Minimizing I-MEC size

1: Input: UCCG G(V, E)
2: Output: |M EC(G)|
3: CountDP [] ← A storage indexed on S ⊆ V and initi-

1: Input: UCCG G(V, E)
2: Output: A single-node intervention target {vopt}
3: CountDP [] ← A storage indexed on S ⊆ V and initi-

ated by 1 if |S| = 1 and NULL otherwise.

ated by 1 if |S| = 1 and NULL otherwise.

4: return Count(V )

————————————————————

return DP [S]

5: function Count(S)
6: if CountDP [S] is not N U LL then
7:
8: end if
9: CountDP [S] ← 0
10: v ← an arbitrary node in S
11: L ← LazyIter(G[S], v)
12: for R ∈ L do
num ← 1
13:
for C(S′, E′) ∈ C(R) do
14:
15:
16:
17:
18: end for
19: return CountDP [S]

num ← num × Count(S′)

end for
CountDP [S] ← CountDP [S] + num

which takes O(2∆(n∆ + ∆3)) operations, and executes
the two for-loops. The outer for-loop is executed at most
2∆ times, and the inner for-loop is executed at most n times.
Calculation of C(E) could also be done in O(n∆) steps. Af-
ter these calculations, DP [S] will be saved and there is no
need to calculate it in later calls. On the other hand, there
are at most 2n values for index of DP , and therefore the
time complexity of Algorithm 2 is:

O(cid:16)2n

2∆(n∆+∆3)+2∆(n+n∆)
(cid:0)

(cid:1)(cid:17) = O(2n2∆(n∆+∆3)).

5. Experiment Design

Assume we want to ﬁnd the best intervention target I ⊆ V
in UCCG G(V, E). For experiment design, given an objec-
tive function, we need to compare the efﬁciency of differ-
ent intervention targets based on it. A common objective
function is the size of I-essential graph obtained after in-
tervention (Ghassami et al., 2019). The smaller the class is,
the more information we have gained from the intervention.
If we consider the worst-case setting, we have:

Iopt = arg min

I⊆V (cid:16) max

R∈IR{I}(G)

|M EC(R)|(cid:17).

(1)

Another objective function used in previous work is
the number of directed edges after an intervention
(Ghassami et al., 2018; Hauser & Bhlmann, 2014):

Iopt = arg max

I⊆V (cid:16)

min
R∈IR{I}(G)

|Dir(R)|(cid:17),

(2)

L ← LazyIter(G, v)
sv ← 0
for R ∈ L do

4: sopt ← 0
5: vopt ← N U LL
6: for v ∈ V do
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
end if
19:
20: end for
21: return vopt

end for
if sv < sopt then
sopt ← sv
vopt ← v

mecsize ← 1
for C(V ′, E′) ∈ C(R) do

mecsize ← mecsize × Count(V ′)

end for
sv ← max(sv, mecsize)

We solve the experiment design problem for both of these
objective functions, in both active and passive learning set-
tings.

5.1. Active Learning

In the active learning, the information obtained from the
former interventions can be used to choose the next targets.
Similar to the approach taken in Hauser & Bhlmann (2014),
we aim to ﬁnd the best single-node intervention target in
each learning step. We take advantage of LazyIter and
LazyCount for this purpose.

Let G(V, E) be a UCCG. Considering objective function
(1), we want to ﬁnd a node v such that intervening on
it, minimizes the size of the resulting I-MEC. We ﬁrst
use LazyIter to ﬁnd the set of all I-essential graphs for
different single-node intervention targets. Then, for each
I-essential graph P P
v (G), we obtain the size of its corre-
sponding I-MEC by multiplying sizes of its chain com-
ponents. Finally, we use these values to ﬁnd the optimal
intervention target. The description of this algorithm is pre-
sented in Algorithm 3. The procedure is almost the same
for objective function (2). We just need to calculate num-
ber of directed edges for each I-essential graph, instead of
calculating its I-MEC size.

All of the operations in Algorithm 3 could be divided to
two parts:

LazyIter: A Fast Algorithm for Counting Markov Equivalent DAGs and Designing Experiments

• Calculating the values of CountDP [] using function
Count(), which takes at most O(2n2∆(n∆ + ∆3))
operations.

Algorithm 4 Passive Learning by Maximizing Number of
Oriented Edges
1: Input: UCCG G(V, E), budget b, intervention cost for

• Iterating over the three for-loops (taking n, 2∆,
and n steps respectively), calling LazyIter (taking
O(2∆(n∆ + ∆3)) operations), and calculating C(R)
(taking O(n∆) operations). All of these steps together
need O(n2∆(n∆ + ∆3)) operations.

Therefore Algorithm 3 calculates the MEC size in at
most O(2n2∆(n∆ + ∆3)) + O(n2∆(n∆ + ∆3)) =
O(2n2∆(n∆ + ∆3)) operations.
If we want to ﬁnd the
best target with respect to objective function (2), there is
no need to calculate CountDP [], but all other operations
should be executed similarly. Consequently, the time com-
plexity in this case would be O(n2∆(n∆ + ∆3)).

5.2. Passive Learning

Let G(V, E) be a UCCG, where each node v ∈ V is as-
signed a cost cv. We aim to ﬁnd a set of k single-node
interventions, and therefore our intervention family is of
the form I = {{v1}, {v2}, ..., {vk}}, similar to the model
considered in Ghassami et al. (2018). Using the following
lemma, we break the problem down to smaller subproblems
and take advantage of dynamic programming:

be

2. Let G(V, E)

Lemma
=
{{v1}, {v2}, ..., {vk}} an intervention family, D the
ground truth DAG of G, and for each chain component
C ∈ C(E{{v1}}(G)), {vC
mC } ⊆ V be the subset
of intervention targets which are inside C. Then we have:

a UCCG, I

2 , ..., vC

1 , vC

Dir(E{{v1},{v2},...,{vk}}(D)) = Dir(E{{v1}}(D)) ∪ Z,

where

Z =

[
C∈C(E{{v1}}(D))

Dir(E{{vC

1 },{vC

2 },...,{vC

mC }}(D[C])).

Assume we want to ﬁnd the optimum intervention target
with respect to objective function (2). For any T, S ⊆
V where T = {v1, v2, ..., vt} and T ⊆ S, we deﬁne
DP [S][T ] as follows:

DP [S][T ] = min

D∈D(G)

|Dir(E{{v1},{v2},...,{vt}}(D[S])|.

(3)

Proposition 2. The following equation holds for DP func-
tion (3):

DP [S][T ] =

R∈IR{{v1}}(G[S])n|Dir(cid:16)R(cid:17)| +

min

X
C∈C(R)

DP hCihT ∩ Cio.

(4)

each node v ∈ V as costv

2: Output: A single-node intervention target {vopt}
3: DP [S][T ] ← A storage indexed on S ⊆ V and T ⊆ S,
and initiated by 0 if |S| = 1 and NULL otherwise.

Px∈T costx ≤ budget then
if Calculate(V, best) ≤ Calculate(V, T ) then

if

4: best ← ∅
5: for T ⊆ V do
6:
7:
8:
9:
end if
10:
11: end for
12: return best

end if

best ← T

—————————————————————-

return DP [S][T ]

13: function Calculate(S, T )
14: if DP [S][T ] is not NULL then
15:
16: end if
17: DP [S][T ] ← ∞
18: v ← an arbitrary member of T
19: L ← LazyIter(G, v)
20: for R ∈ L do
21:
22:
23:
24:
25: DP [S][T ] ← min(DP [S][T ], num)
26: end for
27: return DP [S][T ]

num ← |Dir(R)|
for C(S′, E′) ∈ C(R) do

end for

num ← num + Calculate(S′, T ∩ S′)

This proposition suggests that we could select an arbitrary
intervention target, iterate over all I-essential graphs in its
intervention result space, and ﬁnd number of directed edge
in each case using already-calculated DP values. After
ﬁnding all DP [V ][T ] values, we can choose the one which
has a cost less than our budget and maximizes number of
directed edges. For optimization with respect to objective
function (1), we can deﬁne DP [S][T ] as the maximum size
of I-MEC obtained from G[S] after intervening on nodes
in T . With the similar arguments, we can show that if we
substitute |Dir(R)| with |M EC(R)| in equation (4), the
resulting equation holds for this new DP array.

The number of DP elements is 3n, as each node is either
in T , or in S\T , or in V \S. For calculation of each DP
value, LazyIter is called once and then two for-loops are
executed, iterating for 2∆ and n steps respectively. Hence,
Algorithm 5.2 ﬁnds the best passive intervention target with
respect to objective function (2) in O(3n2∆(n∆ + ∆3))
operations.

LazyIter: A Fast Algorithm for Counting Markov Equivalent DAGs and Designing Experiments

LazyIter
Hauser2014

2,000

1,500

)
s
(
e
m

i
t

1,000

500

0

LazyCount
MemoMAO

2,000

1,500

1,000

500

0

150

175

200

225

250

275

250

300

350

number of edges

a

number of edges

b

o
i
t
a
r
y
r
e
v
o
c
s
i
d
e
g
d
e

0.7

0.6

0.5

0.4

Our Method
Random
MaxDegree

10

15

20

25

30

35

graph order (n)
c

0.8

0.7

0.6

0.5

0.4

0.3

Our Method
Random
MaxDegree

0.8

0.6

0.4

0.2

Our Method
Random
MaxDegree

0.1

0.15

0.2

0.25

0.3

0.35

0.4

1

1.5

2

2.5

3

edge density (r)

d

budget (b)
e

Figure2. (a) Comparison between execution times of LazyIter and algorithm in Hauser & Bhlmann (2014) versus number of edges
for graphs with 30 nodes. (b) Comparison between execution times of LazyCount and MemoMAO (Talvitie & Koivisto, 2019) versus
number of edges for graphs with 30 nodes. Comparison between edge discovery ratio versus (c) graph order for b = 2 and r = 0.4, (d)
edge density for n = 35 and b = 3, (e) budget for n = 40 and r = 0.3

6. Experimental Results

We compared LazyIter and LazyCount against previous
work. The performance of our active learning algorithms
depend on these two routines. Our DP-based passive learn-
ing algorithm is the ﬁrst exact algorithm for worst-case
experiment design, so we compared it with Random and
MaxDegree heuristics. The only related previous work
Ghassami et al. (2019) is an approximation designed for
the average-case passive learning. Their algorithm has a
time complexity of O(kN n(∆+1)) (where N is the number
of sampled DAGs and k is the budget), and is considerably
more computationally expensive than our algorithm. How-
ever, the results are not comparable as their algorithm does
not solve the problem in the worst-case setting. For each
test, we generated 100 graphs using the method presented
in He et al. (2015) and calculated the average test results on
them. As we can see in Figure 2 (a), LazyIter outperforms

(Hauser & Bhlmann, 2014) in all cases, especially when
the graph is dense. We also tested LazyCount against
M emoM AO, which is the state-of-the-art MEC size calcu-
lation algorithm (Talvitie & Koivisto, 2019). Even though
the difference in execution times is not considerable for
sparse graphs, our algorithm performs much better for
dense graphs, as seen in Figure 2 (b). The main reason
for this is that LazyCount requires fewer DP values in
its execution. Figures 2 (c), (d), and (e) present the discov-
ered edge ratio (the number of edges whose orientations
are inferred from experiments to the number of edges in
the graph) of the passive learning algorithm versus differ-
ent graph orders, edge densities (ratio of the number of
edges to the maximum possible number of edges), and bud-
gets (number of interevetions), respectively. As the graph
order increases, ﬁnding the optimal target becomes harder,
and therefore the difference between our algorithm and the
heuristics becomes more considerable.

LazyIter: A Fast Algorithm for Counting Markov Equivalent DAGs and Designing Experiments

He, Y.-B. and Geng, Z. Active learning of causal networks
with intervention experiments and optimal designs. Jour-
nal of Machine Learning Research, 9(Nov):2523–2547,
2008.

Kocaoglu, M., Shanmugam, K., and Bareinboim, E. Ex-
perimental design for learning causal graphs with latent
variables. In Advances in Neural Information Processing
Systems, pp. 7021–7031, 2017.

Lindgren, E., Kocaoglu, M., Dimakis, A. G., and Vish-
wanath, S. Experimental design for cost-aware learning
of causal graphs. In Advances in Neural Information Pro-
cessing Systems, pp. 5279–5289, 2018.

Pearl, J. Causality: Models, Reasoning and Inference.
Cambridge University Press, USA, 2nd edition, 2009.
ISBN 052189560X.

Shanmugam, K., Kocaoglu, M., Dimakis, A. G., and Vish-
wanath, S. Learning causal graphs with small interven-
tions. In Advances in Neural Information Processing Sys-
tems, pp. 3195–3203, 2015.

Spirtes, P., Glymour, C., and Scheines, R. Causation, Pre-

diction, and Search. Springer, 2000.

Talvitie, T. and Koivisto, M. Counting and sampling
markov equivalent directed acyclic graphs. In The Thirty-
Third AAAI Conference on Artiﬁcial Intelligence, pp.
7984–7991. AAAI Press, 2019.

Verma, T. and Pearl, J. An algorithm for deciding if a set of
observed independencies has a causal explanation. Un-
certainty in Artiﬁcial Intelligence, pp. 323330, 1992.

7. Conclusion

We proposed a new method to iterate efﬁciently over possi-
ble I-essential graphs and utilized it to design algorithms
for computing MEC size and experiment design for ac-
tive and passive learning settings. Experimental results
showed that the proposed algorithms outperform other re-
lated works in terms of time complexity. As a direction
of future research, it would be interesting to extend to the
proposed algorithms for other objective functions in design-
ing experiments, such as average number of oriented edges.
Moreover, one can work on designing algorithms in the pas-
sive learning setting where we can intervene on multiple
variables in each experiment.

References

Agrawal, R., Squires, C., Yang, K., Shanmugam, K., and
Uhler, C. Abcd-strategy: Budgeted experimental design
for targeted causal structure discovery. arXiv preprint
arXiv:1902.10347, 2019.

Eberhardt, F. Almost optimal intervention sets for causal

discovery. arXiv preprint arXiv:1206.3250, 2012.

Eberhardt, F. and Scheines, R.

Interventions and causal
inference. Philosophy of Science, 74(5):981–995, 2007.

Eberhardt, F., Glymour, C., and Scheines, R. On the num-
ber of experiments sufﬁcient and in the worst case nec-
essary to identify all causal relations among n variables.
pp. 178–184, 2005.

Fulkerson, D. R. and Gross, O. A. Incidence matrices and
interval graphs. Paciﬁc J. Math., 15(3):835–855, 1965.

Ghassami, A., Salehkaleybar, S., Kiyavash, N., and Barein-
boim, E. Budgeted experiment design for causal struc-
ture learning. In International Conference on Machine
Learning, pp. 1724–1733, 2018.

Ghassami, A., Salehkaleybar, S., Kiyavash, N., and Zhang,
K. Counting and sampling from markov equivalent dags
using clique trees. Proceedings of the AAAI Conference
on Artiﬁcial Intelligence, 33:36643671, Jul 2019.

Hauser, A. and Bhlmann, P. Characterization and greedy
learning of interventional markov equivalence classes of
directed acyclic graphs, 2011.

Hauser, A. and Bhlmann, P. Two optimal strategies for
active learning of causal models from interventional data.
International Journal of Approximate Reasoning, 55(4):
926939, Jun 2014. ISSN 0888-613X.

He, Y., Jia, J., and Yu, B. Counting and exploring sizes
of markov equivalence classes of directed acyclic graphs.
The Journal of Machine Learning Research, 16(1):2589–
2609, 2015.

LazyIter: A Fast Algorithm for Counting Markov Equivalent DAGs and Designing Experiments

A. Appendices

A.1. Proof of Proposition 1

Proof. It could be shown that a DAG D is a member
of MEC corresponding to G if and only if it has no v-
structures (He et al., 2015). Let v ∈ V be an arbitrary node.
For every DAG in the MEC, the parent set of node v is
deﬁnitely a clique, because a v-structure is formed other-
wise. If D1 and D2 be two members of MEC such that
paD1(v) = paD2(v) then D1 ∼{{v}} D2, and therefore
D1 and D2 are indistinguishable under the single-node in-
tervention target {v} (Hauser & Bhlmann, 2014). So ev-
ery E{{v}}(D) is determined uniquely with paD(v). Every
LexBFS-ordering σ on G, is also a perfect elimination or-
dering and if we orient edges of G according to σ, we get a
DAG without v-structures (Hauser & Bhlmann, 2014). For
an arbitrary clique P ⊆ neG(v) in neighbors of v, if we
orient edge set E according to LexBF S((P, v, ...), E), the
resulting DAG D is a member of MEC and paD(v) = P .
This shows that there is a one-to-one correspondence be-
tween E{{v}}(D)s and cliques P ⊆ neG(v).

A.2. Proof of Theorem 1

Proof. The proofs of four statements is respectively as fol-
lows:

• Every node which is separated from v by P is inside
AR, so for every d ∈ DR there is a path from d to v in
G[V \AR]. Now assume that there is an edge between
two arbitrary nodes a ∈ AR and d ∈ DR. As there
is a path from v to d in G[V \AR], and edge a − d is
also present in G[V \AR], there is a path from v to a
in G[V \AR] and therefore P is not an (a, v)-separator
in G, which could not be true.

• The cycle a → v → b → a is formed otherwise.

• If a ∈ CR and the edge be directed as b → a, the
v-structure v → a ← b will be formed. If a ∈ P , let
v, x1, x2, ..., xk, b be the shortest path between v and b
in G[{v} ∪ CR ∪ DR]. No two non-consecutive nodes
of this path are connected to each other, because we
will ﬁnd a shorter path otherwise. It is also obvious
that x1 ∈ CR and therefore v → x1 ∈ R. If x1 − x2
be directed as x1 ← x2 in R, the v-structure v →
x1 ← x2 will be formed, so x1 → x2 ∈ R. With
a similar arguement, we can say xi → xi+1 ∈ R,
for 1 ≤ i ≤ k, where xk+1 = b. Therefore v →
x1 → x2 → ... → xk → b is a directed path in R.
If b → a ∈ R, we will have a cycle in R which is
impossible, and therefore a → b ∈ R.

• None of the edges inside R[AR ∪ P ] are oriented as
a direct result of intervention, so every edge in this

subgraph should be oriented using Meek rules. Let
a → b be the ﬁrst edge oriented inside R[AR ∪ P ], so
we have a, b ∈ AR ∪ P . In all of the four Meek rules,
there is at least one already oriented edge directed to-
wards one of the two endpoints of the edge which is
being oriented. This means that there should exist ei-
ther an edge x → a ∈ R or and edge x → b ∈ R. But
this is impossible, because we know that there are no
edges directed towards any of the nodes in AR ∪ P in
the graph we get after intervention. This means that
no Meek rules are applicable for orienting edges in
R[AR ∪ P ], and this subgraph is undirected.

A.3. Proof of Lemma 1

We break the lemma into two smaller lemmas and prove
them separately:

Lemma 3. Let G(V, E) be a UCCG and I be an arbitrary
intervention family. Then we have:

|M EC(G)| =

|M EC(R)|.

X
R∈IRI (G)

Proof. Every DAG D in MEC corresponding to G is ex-
actly in one of the I-essential graphs in IR(G), based on
direction of the edges connected to intervention targets in-
side that DAG. Therefore, each DAG is exactly counted
once in the summation.

Lemma 4. Consider I-essential graph EI(D) of a DAG
D and intervention target I. Let C(EI(D)) be the set of all
chain components of EI(D). Then we have:

|M EC(EI(D))| =

|M EC(C)|.

Y
C∈C(EI (D))

Proof. (Hauser & Bhlmann, 2014) showed that the direc-
tion of edges inside each chain component of an I-essential
graph is unrelated to the direction of edges in other compo-
nents. Therefore edges inside each chain component could
be oriented independently, and number of valid orientations
of edges in EI(D) (orientations without v-structures) is
equal to multiplication of number of valid orientations in
each chain component. He et al. (2015) proved a similar
lemma for observational cases.

Lemma 3 shows that we can calculate the size of MEC
represented by G via calculating sizes of I-MECs repre-
sented by members of IR{{v}}(G). For counting number
of DAGs in each of these I-MECs, we use Lemma 4, and
therefore the equation in Lemma 1 holds.

LazyIter: A Fast Algorithm for Counting Markov Equivalent DAGs and Designing Experiments

A.4. Proof of Lemma 2

A.5. Proof of Proposition 2

We need these two lemmas for the proof:

Lemma 5. (Ghassami et al., 2018) For any DAG D(V, E)
and sets I1, I2 ⊆ V , we have:

Dir(E{I1∪I2}(D)) = Dir(E{I1}(D)) ∪ Dir(E{I2}(D)).

Lemma 6. (Hauser & Bhlmann, 2014) Consider an I-
essential graph of some DAG D, and let C ∈ C(EI(D))
be one of its chain components. Let I ⊆ V, I /∈ I be an-
other intervention target. Then we have:

Proof. We know that every valid orientation of all undi-
rected edges in all of the chain components gives us a DAG
in the I-MEC E{{v1}}(D). Moreover we know that the
minimum value of |Dir(E{{vC
mC }}(D)[C])| is
2 },...,{vC
DP [C][{vC
mC }] = DP [C][T ∩ C]. As chain
components have distinct edges sets, we have:

2 , ..., vC

1 , vC

1 },{vC

EI∪{I}(D)[C] = E{∅,I∩V ′}(D[C])

X
C∈C(E{{v1}}(D))

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

[
C∈C(E{{v1}}(D))

Dir(E{{vC

1 },{vC

2 },...,{vC

Dir(E{{vC

1 },{vC

2 },...,{vC

=

mC }}(D[C]))(cid:12)
(cid:12)
(cid:12)
mC }}(D[C]))(cid:12)
(cid:12)
(cid:12)

.

Now we prove Lemma 2.

Proof. Using Lemma 6 we can say:

E{{v1},{v2},...,{vk}}(D)[C] =

E{{v1}}∪{{v2},...,{vk}}(D)[C] =

E{∅,{{v2},...,{vk}}∩V ′}(D[C]) =

E{{{v2},...,{vk}}∩V ′}(D[C]) =

Lemma 2 implies that for counting number of directed
edges in each I-essential graph, we could consider each
component independently and therefore the minimum num-
ber of directed edges for each chain component can be
found via DP values. We can iterate over all possible
E{{v1}}(D)s and use DP values to ﬁnd the minimum num-
ber of directed edges for each case. This means DP [V ][T ]
could be calculated by the recursive formula (4).

E{{vC

1 },...,{vC

m}}(D[C])

Where the equality between third and fourth lines comes
from the fact that we already know the observational es-
sential graph of the chain component, as we are given the
UCCG. Using Lemma 5, we have:

Dir(E{{v1},{v2},...,{vk}}(D))

= Dir(E{{v1}}(D)) ∪ Dir(E{{v2},{v3},...,{vk}}(D))

But as we mentioned earlier, direction of edges inside one
chain component gives us no information about direction
of edges in other chain components.

We can say:

Dir(R1)∪Dir(E{{v2},{v3},...,{vk}}(D))

= Dir(R1)

= Dir(R1)

[
C∈C(R1)

[
C∈C(R1)

Dir(E{{v2},{v3},...,{vk}}(D)[C])

Dir(E{{vC

1 },{vC

2 },...,{vi

mC }}(D)[C]).

