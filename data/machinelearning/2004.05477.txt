0
2
0
2

v
o
N
5

]
h
p
-
p
m
o
c
.
s
c
i
s
y
h
p
[

2
v
7
7
4
5
0
.
4
0
0
2
:
v
i
X
r
a

Learned discretizations for passive scalar advection in a 2-D turbulent ﬂow

Jiawei Zhuang,1, 2 Dmitrii Kochkov,2 Yohai Bar-Sinai,1, 2 Michael P. Brenner,1, 2 and Stephan Hoyer2
1School of Engineering and Applied Sciences, Harvard University, Cambridge, MA
2Google Research, 1600 Amphitheatre Pkwy, Mountain View, CA

The computational cost of ﬂuid simulations increases rapidly with grid resolution. This has given
a hard limit on the ability of simulations to accurately resolve small scale features of complex
ﬂows. Here we use a machine learning approach to learn a numerical discretization that retains
high accuracy even when the solution is under-resolved with classical methods. We apply this
approach to passive scalar advection in a two-dimensional turbulent ﬂow. The method maintains
the same accuracy as traditional high-order ﬂux-limited advection solvers, while using 4× lower grid
resolution in each dimension. The machine learning component is tightly integrated with traditional
ﬁnite-volume schemes and can be trained via an end-to-end diﬀerentiable programming framework.
The solver can achieve near-peak hardware utilization on CPUs and accelerators via convolutional
ﬁlters. Code is available at https://github.com/google-research/data-driven-pdes.

I.

INTRODUCTION

A key problem in the numerical simulation of complex
phenomena is the need to accurately resolve spatiotem-
poral features over a wide range of length scales. For
example, the computational requirement for simulating
a high Reynolds number ﬂuid ﬂow scales like Re3, imply-
ing that a tenfold increase in Reynolds number requires
a thousand fold increase in computing power. Over the
past decades, the extra computing power made available
through Moore’s law has been used to increase grid res-
olution dramatically, leading to breakthroughs in turbu-
lence modeling [1], weather prediction [2], and climate
projection [3]. Nonetheless, there is still a formidable
gap towards resolving the ﬁnest spatial scales of inter-
est [4], especially with the recent slow-down of Moore’s
Law [5, 6]. Machine learning has given a potential way
out of this conundrum, by training low-resolution models
to learn the rules from their high-resolution counterparts
[7–10]. The learned models aim to produce high-ﬁdelity
simulations using much less computational resources. In-
corporating machine learning into numerical models also
facilitates the adoption of emerging hardware, consider-
ing that the fastest growth in computing power now re-
lies on domain-speciﬁc architectures like Graphical Pro-
cessing Units (GPUs) [11] and Tensor Processing Units
(TPUs) [12, 13] that are optimized for machine learning
tasks.

Recently we introduced data driven discretizations [14]
to learn numerical methods that achieve the same ac-
curacy as traditional ﬁnite diﬀerence methods but with
much coarser grid resolution. These methods are equa-
tion speciﬁc, and require training a coarse resolution
solver with high resolution ground truth simulations.
Since the dynamics of a partial diﬀerential equation is
entirely local, the high resolution simulations can be
carried out on a small domain. We demonstrated the
method with a set of canonical one-dimensional equa-
tions, demonstrating a 4 ∼ 8× upscaling of eﬀective res-
olution [14]. Here we extend this methodology to two-
dimensional advection of passive scalars in a turbulent

ﬂow, a canonical problem in physics [15] and a classic
challenge in atmospheric modeling [16]. We show that
machine-learned advection solver can use a grid with 4×
coarser resolution than classic high-order solvers while
still maintaining the same accuracy.

II. DATA-DRIVEN SOLUTION TO
ADVECTION EQUATION

A. Advection equation

We consider the advection of a scalar concentration

ﬁeld C((cid:126)x, t) under a speciﬁed velocity ﬁeld (cid:126)u((cid:126)x, t):

∂C
∂t

+ ∇ · ((cid:126)uC) = 0

If the prescribed velocity ﬁeld is divergence-free

∇ · (cid:126)u = 0,

then, Eq. (1) reduces [17]

∂C
∂t

+ (cid:126)u · ∇C = 0.

(1)

(2)

(3)

A classical Eulerian scheme uses discretizations of the
∂x , often in a form of:

spatial derivative ∂C

∂C
∂x

(cid:12)
(cid:12)
(cid:12)
(cid:12)x=xi

k
(cid:88)

=

j=−k

αjCi+j

(4)

where {x1, .., xN } is the spatial grid points, Cj is the con-
centration at point xj, and {α−k, ..., αk} are predeﬁned
ﬁnite-diﬀerence coeﬃcients. For example, a ﬁrst-order
forward diﬀerence Ci+1−Ci
(where Ci+1 is in the upwind-
ing direction) leads to the upwind scheme. Sophisticated
high-order methods with ﬂux limiters will choose diﬀer-
ent coeﬃcients depending on local ﬁelds [18]. Extension
to two-dimensions can be done by either operator split-
ting (solve for each dimension separately) [19] or a true
two-dimensional discretization [20].

∆x

 
 
 
 
 
 
Although high-order Eulerian schemes are highly ac-
curate under idealized ﬂows [21], their accuracy breaks
down to ﬁrst-order under turbulent or strongly sheared
ﬂows, resulting in signiﬁcant numerical diﬀusion [16].
Adaptive mesh reﬁnement can reduce such numerical
diﬀusion [22], but increases software complexity. La-
grangian methods avoid numerical diﬀusion [23], but
have inhomogeneous spatial coverage and also diﬃculties
in dealing with nonlinear chemical reaction [24]. Semi-
Lagrangian approaches involve remapping from a dis-
torted Lagrangian mesh to a regular Eulerian mesh [25],
and such remapping step exhibits similar numerical dif-
fusion as Eulerian methods. Flow-map approaches [26]
can achieve Lagrangian-like accuracy on a Eulerian mesh,
but need to solve for the advection trajectory over multi-
ple steps and requires a special treatment to incorporate
additional terms (e.g. chemical reaction) between advec-
tion steps. Diﬀerent from existing methods, here we aim
to develop an ultra-accurate advection solver under the
requirements of: (1) a strictly Eulerian framework on a
ﬁxed grid, (2) explicit time-stepping, and (3) only relying
on the current state to predict the next time step.

B. Learning optimal coeﬃcients

Instead of using predeﬁned rules to compute ﬁnite-
diﬀerence coeﬃcients (Eq. 4), our data driven discretiza-
tions [14] predict the local-ﬁeld-dependent coeﬃcients
(cid:126)α = {α−k, ..., αk} via a convolutional neural network:

(cid:126)α = f (C, (cid:126)u; W )

(5)

The coeﬃcients (cid:126)α|x=xj depend on the local environ-
ment around xj, with the inputs to the neural net-
work being the neighboring ﬁelds {Cj, Cj±1, ...} and
{(cid:126)uj, (cid:126)uj±1, ...}. For simplicity of presentation, here we
use 1-D indices {j, j ± 1, ...} to denote spatially adjacent
points. For 2-D advection problems, this computation in-
volves 2-D convolution across both x and y dimensions.
We learn the neural network weights W by minimizing
the diﬀerence between the machine learning prediction
and the true solution.

Fig. 1 shows the forward solver workﬂow and train-
ing framework. During the forward solve, we replace the
computation of ﬁnite-diﬀerence coeﬃcients with a convo-
lution neural network, while still using classic approaches
for the rest of the steps (computing the advection ﬂux
and doing the time-stepping). During training, we accu-
mulate the forward solver prediction results over 10 time
steps and then compare to the reference solution over
this time period, by computing the mean absolute error
(MAE) over the entire spatial domain between the two
time series:

M AE =

1
N · M

N
(cid:88)

M
(cid:88)

i=1

j=1

(cid:12)
(cid:12)C predict
(cid:12)

j

(ti) − C true

j

(cid:12)
(cid:12)
(ti)
(cid:12)

(6)

2

The MAE is used as the loss function for neural net-
work training [27]. We ﬁnd that using this multi-step
loss function (as opposed to a single time step) stabilizes
the forward integration, similar to the ﬁndings by [28]. In
our experiments, we found using MAE resulted in slightly
more accurate predictions than using mean square error
(MSE), but the diﬀerence was not large.

The training of a neural network inside a classic numer-
ical solver is made possible by writing the entire program
in a diﬀerentiable programming framework [29], which
allows eﬃcient gradient-based optimization of arbitrary
parameters in the code using automatic diﬀerentiation
(AD) [30]. AD tools have a long history, dating back
to Fortran 77 [31]. Recent developments of AD frame-
works, such as TensorFlow [32], PyTorch [33], JAX [34],
Flux.jl [35], and Swift [36], are even easier to program
and support hardware accelerators like GPUs and TPUs.
Those developments make it easier to incorporate ma-
chine learning into scientiﬁc computing code (e.g. [37]).
We implemented our advection solver in TensorFlow Ea-
ger [38].

C. Baseline solver and reference solution

As a baseline method, we use the second-order VanLeer
advection scheme with a monotonic ﬂux limiter [39]. To
obtain the reference “true” solution, we run the baseline
advection solver at suﬃciently high resolution to ensure
the solution has converged. We then down-sample the
high-resolution results using conservative averaging, to
produce the training and test datasets for our machine-
learning-based model on a coarse grid.

We remark that although higher-order schemes with
more advanced limiters would be more accurate, any ﬂux-
limited high-order schemes break to ﬁrst-order under tur-
bulent ﬂows in order to ensure monotonicity [16]. Start-
ing from second-order, increasing the spatial resolution is
generally more eﬀective than further improving the solver
order or the limiter [40, 41].

D. Physical constraints

There is growing emphasis on embedding physical con-
straints into the design of machine learning methods.
This is typically done either either by adding “soft” con-
straints as terms the loss function [42, 43], or “hard”
constraints in the model architecture [14, 44–48]. Since
here we only replace a small component in the numerical
solver with machine learning, we can impose arbitrary
physical constraints before and after the neural network
components. Using hard constraints allows the machine
learning algorithm to focus on approximation problems,
by imposing physical consistency requirements by con-
struction. In particular, we require:

(1) Finite-volume representation for mass conserva-
tion. We compute the ﬂux across grid cell boundaries,

3

FIG. 1. End-to-end learning framework with diﬀerential programming. During training, the model is optimized to
predict future concentrations across multiple time steps, based on a precomputed dataset of snapshots from high resolution
simulations. During inference, the optimized model is repeatedly applied to predict time evolution. The neural network
component contains a stack of 2-D convolutional layers with ReLU activation functions (degraded to 1-D convolution for 1-D
problems.). Physical constraints are imposed before and after the convolutional layers (Section II D). In the “Time-stepping”
block, H is the advection operator that computes the concentration update based on the machine-learning estimate of spatial
derivatives.

and then apply the ﬂux to update the concentration ﬁelds
Ci. This ensures that mass is exactly conserved. The
machine-learning estimate of spatial derivatives ∂C
∂x is
used for obtaining the optimal interpolation values Ci+ 1
2
at cell boundaries, which is then used for calculating the
ﬂux via ui+ 1

Ci+ 1

.

2

2

(2) Polynomial accuracy constraints. Following [14],
we can force the machine-learning-predicted coeﬃcients
to satisfy an m-th order polynomial constraint, so that
the approximation error decays as O(∆xm). This ensures
that if the learned discretization is ﬁt to solutions that
are smooth on the scale of the mesh, we will recover clas-
sical ﬁnite-diﬀerence methods.
In our experiments, we
ﬁnd that a ﬁrst-order constraint gives the best result on
coarse grids. This preserves a balance between accuracy
constraints and model ﬂexibility that may be particu-
lar valuable in non-monotonic regions, where higher or-
der advection schemes often revert to ﬁrst-order upwind-
ing [18]. First-order accuracy requires (cid:80)k
j=−k αj = 0,
and can be enforced by applying an aﬃne transforma-
tion to the original neural network output (our imple-
mentation), or by having the neural network only output
{α−k, ..., αk−1} and solving for the last αk. We choose
the constant vector in the aﬃne transformation to match
a centered, ﬁrst order scheme (equal weight on the two
nearest grid cells). Accordingly, our randomly initialized
neural net at the start of training produces interpolation
coeﬃcients that are very close to a centered, ﬁrst order
scheme.

(3) Input feature normalization. Before feeding the
current concentration ﬁeld C to the neural network, we
normalize it globally to [0, 1]. This ensures that the over-

all magnitude of the concentration does not aﬀect the
prediction of ﬁnite-diﬀerence coeﬃcients, and thus our
solver satisﬁes the “semi-linear” requirement for advec-
tion schemes that H(aC + b) = aH(C) + b where H is
the advection operator and {a, b} are constants (Eq 2.12-
2.13 of [19]). Without such normalization, we ﬁnd that
the trained model diverges quickly during the forward
integration.

E. Other choices of learned terms

Our training framework can be easily adapted to learn
other parameters besides the ﬁnite-diﬀerence coeﬃcients.
In this section, we describe other approaches that we
experimented with but did not choose.

Numerical methods introduce artiﬁcial numerical dissi-
pation, so it is natural to consider adding explicit correc-
tions to diﬀusion. One of the earliest ﬂux-correct trans-
port (FCT) algorithms [49] includes an anti-diﬀusion co-
eﬃcient of 1/8 as a correction term, though the choice
of 1/8 was subjective and it was later acknowledged
that such correction should better be velocity- and
wavenumber- dependent [50]. We considered learning dif-
fusive correction directly, in the form:

∂C
∂t

(cid:18)

+∇·((cid:126)uC)+

Dxx

∂2C
∂x2 + Dxy

∂2C
∂x∂y

+ Dyy

(cid:19)

∂2C
∂y2

= 0,

(anti-)diﬀusion

(7)
=
coeﬃcients
the
where
{Dxx, Dxy, Dyy} are
computed by a convolutional
neural network (cid:126)D = f (C, (cid:126)u; W ), while the advection-
diﬀusion equation itself is still solved by a traditional

(cid:126)D

Physical stateSpatial derivativesNeural networkTime-steppingLoss function to minimizeCoeﬃcientsconv2dreluconv2dreluconv2dpolynomial constraintfeature normalizationReference “true solution”Prediction across multiple stepsUpdatestateRecordstateCompareUpdate weightsForward pathTraining pathhigh-order ﬁnite volume method. The idea resembles
learning the Reynolds stress tensor [10] in a Reynolds
averaged Navier Stokes (RANS) simulation. As in
Section II B, here the neural network is trained by
minimizing the diﬀerence between the model prediction
and the reference solution.
In practice, we found that
this learned diﬀusion model achieves about 3× upscaling
compared to the second-order baseline solver, but
performs slightly worse than our original approach of
learning ﬁnite-diﬀerence coeﬃcients (Section II B) that
can achieve 4× upscaling.

We also experimented with other learned terms, in-
cluding (1) a pure machine learning approach, by hav-
ing the neural network directly predict the concentration
at the next step C(t + ∆t) based on the current state
C(t) and (cid:126)u(t); and (2) having the neural network directly
predict the spatial derivative ∂C
∂x instead of the ﬁnite-
diﬀerence coeﬃcients (cid:126)α that need to be further multi-
plied with the concentration ﬁeld C to obtain the spatial
derivative. We found those methods to be unstable due
to the lack of physical constraints (Section II D).

III. NUMERICAL RESULTS

We apply the data driven discretization to one- or
two- dimensional advection. Two-dimensional advection
is highly relevant for atmospheric modeling, as the ver-
tical dimension can be decoupled from the horizontal di-
mensions and solved independently [19].

The performance of our learned advection solver (the
“neural network model” hereafter) depends on the hyper-
parameters of the convolutional neural network compo-
nent. For simplicity, this section only presents the results
with the default hyperparameter conﬁguration. For 1-D
problems, we use 4 convolutional layers and 32 ﬁlters in
each layer; For 2-D problems, we use 10 convolutional
layers and 128 ﬁlters in each layer. All cases use a 3-
point ﬁnite diﬀerence stencil (k = 1 in Eq. 4). The
impact of hyperparameters on model accuracy and com-
putational speed is further examined in Section IV. We
use the Adam optimizer [51] with default parameters for
neural network training. Our simple convolutional neu-
ral network achitecture already achieves a high accuracy,
without additional operations like residual connections
and batch normalization.

A. 1-D advection under constant velocity

We ﬁrst show that our neural network model can
achieve near-perfect result for a canonical test problem:
1-D advection constant velocity [39]. We consider a pe-
riodic 1-D grid of 32 grid points. The concentration ﬁeld
is shifted by a constant distance per time step, deter-
mined by the Courant–Friedrichs–Lewy (CFL) number
u∆t
∆x . We set CFL = 0.5 (∆x = 1, ∆t = 0.5, u = 1),
so that the concentration ﬁeld is shifted by half grid box

4

FIG. 2. One test sample for 1-D advection under con-
stant velocity. The concentration ﬁeld is advected by half
grid box every time step, and returns to the original position
after every 64 time steps because the domain is periodic. Our
neural network model is able to maintain the initial shape
indeﬁnitely, while traditional solvers accumulates numerical
diﬀusion over time.

every time step, and returns to the original position after
every 64 time steps.

To generate training data, we initialize 30 square waves
with heights randomly-sampled from [0.1, 0.9] and widths
from 2 ∼ 8 grid points. Test data are randomly sampled
from the same range of width and height. The refer-
ence “true” solution is generated by the baseline solver
at 8× resolution (256 grid points) and down-sampled to
the original coarse grid.

Fig. 2 shows one test sample during the forward in-
tegration. The ﬁrst-order upwind scheme exhibits large
numerical diﬀusion, due to its second-order spatial dis-
cretizaion error [52]. The second-order VanLeer scheme
(our baseline) is more accurate but stills accumulates dif-
fusion over time. In contrast, our neural network model
closely tracks the reference “true solution” obtained by
the 8× resolution baseline. When a slight numerical dif-
fusion occurs at one step, the next step applies a slight
anti-diﬀusion to correct it. Intuitively, the solver learns
that the optimal solution in one-dimensional advection is
to maintain the initial shape.

Fig. 3 shows the mean absolute error over time, aver-
aged over all test samples. The error indicates the devi-
ation from the reference solution obtained by the base-
line solver at 256 grid points. The neural network model
achieves a factor of 8 less error than the baseline second-
order VanLeer scheme.

We further investigate this intriguing behavior of our
neural network model using out-of-sample test data. As
shown in Fig. 4, when the model (trained on square
waves) is applied to Gaussian initial conditions, it grad-

0.00.20.40.60.8ConcentrationTime step = 0Time step = 16020x0.00.20.40.60.8ConcentrationTime step = 64020xTime step = 256Neural netBaselineFirst order5

FIG. 5. Result on 2-D deformational ﬂow. The ﬂow re-
verses at t = 2.5 and returns to the initial condition at t = 5.
The neural network model is able to maintain a sharp gradi-
ent, while the baseline model incurs large numerical diﬀusion.
The spatial domain is [0, 1] × [0, 1] (not plotted on axis).

B. 2-D deformational ﬂow

We next demonstrate that our neural network model
can also achieve near-perfect result for a 2-D deforma-
tional ﬂow test, originally proposed by [17] and later ex-
tended to spherical coordinates as a standard test for
atmospheric advection schemes [53, 54]. The spatial do-
main is a square [0, 1] × [0, 1], and the velocity ﬁeld is a
periodic swirling ﬂow:

u(x, y, t) = sin2(πx) sin(2πy) cos(πt/T )
v(x, y, t) = sin2(πy) sin(2πx) cos(πt/T )

(8)

where the period T = 5 in our setup. The direction of
this ﬂow reverses at t = (n − 1
2 )T for any positive integer
n. The exact solution at t = nT is identical to the initial
condition.

The initial concentration ﬁeld is a blob centered at

[1/4, 1/4]:

1
2

[1 + cos(πr)]

C(x, y) =
r(x, y) = min(1, 4(cid:112)(x − 1/4)2 + (y − 1/4)2)

(9)

FIG. 3. Error for 1-D advection on test data. Here
we only plot even time steps (0, 2, 4, ...) for a smooth curve,
because the error oscillates between odd and even time steps
(a result of CFL=0.5).

FIG. 4. Neural network prediction on out-of-sample
data. The neural network model is only trained on square
waves, but applied to Gaussian initial conditions. The model
gradually turns Gaussian waves into squares, and then main-
tains the squares indeﬁnitely.

ually turns Gaussian waves into squares, which are the
only shape in the training data. Then, the model can
maintain the squares indeﬁnitely. Such phenomenon of
“turning other shapes to squares” also exists in manually-
designed schemes that are overly-optimized for square
waves [50]. The over-ﬁtting problem here can be eas-
ily ﬁxed by adding Gaussian shapes into training data;
after that the neural network model can track both Gaus-
sian and square shapes perfectly. Given that the input
features for convolutional neural networks are localized
in space, covering representative input patterns only re-
quires limited amounts of training data.

The model is not directly trained on this deformational
ﬂow, but instead on an ensemble of periodic, divergence-
free, random velocity ﬁelds,
implemented as superpo-
sitions of Sinusoidal waves as described by [55]. The
trained model is able to generalize across diﬀerent ﬂows
as long as the training data contain representative local
patterns.

Fig. 5 shows the advection under deformational ﬂow
for the baseline and the neural network models, both on
64 × 64 grid points. The time step is chosen so that the
maximum CFL number is 0.5. The neural network model
is able to maintain a sharp concentration gradient, while

0100200Time step0.000.020.040.060.080.10Mean absolute errorNeural netBaselineFirst order0.00.10.2ConcentrationTime step = 0Time step = 16020x0.00.10.2ConcentrationTime step = 64020xTime step = 256Sample 0Sample 1Sample 2Initial condition; t=0Neural net; t=2.5Neural net; t=5.0Velocity field; t=0Baseline; t=2.5Baseline; t=5.00.00.20.40.60.81.0concentration6

is [0, 2π] × [0, 2π] with periodic boundary condition. We
use a 256 × 256 grid for generating the reference solu-
tion using the baseline solver, and a 32 × 32 coarse grid
for model evaluation. As in previous cases, here the ad-
vection time step is chosen so that the maximum CFL
number is 0.5.

The training and test velocities are generated from dif-
ferent random seeds. We start with the McWilliams-84
random initial condition [57] and let the turbulence decay
with time. We discard the initial 4 seconds of the sim-
ulation so that the velocity ﬁeld can be resolved on the
coarse grid. For the initial concentration ﬁeld, we use an
ensemble of 10 blobs with width 0.5 at random locations.
Note that the spatial scale of the concentration ﬁeld un-
der turbulent advection can become much smaller than
the scale of the velocity ﬁeld [15, 58], making it challeng-
ing for traditional advection solvers to resolve the con-
centration gradient. We use 20 random initial conditions
for training data and 20 for test data. The actual sample
size for the training dataset is much larger, as each initial
condition is integrated into a time series of 1024 steps on
the ﬁne grid or 128 steps on the coarse grid, which is fur-
ther broken into many 10-step time series for calculating
the multi-step loss function.

Fig. 7 shows one test sample under the 2-D turbulent
ﬂow, for both the initial condition (the left column of
Fig. 7) and the integration results after 256 time steps
(the middle and right columns of Fig. 7). Note that
this is twice the maximum number of time-steps used
for model training. The initial blobs are stretched into
thin ﬁlaments under the turbulent ﬂow. The baseline
solver (second-order VanLeer scheme) can resolve such
ﬁlaments on the ﬁne-resolution grid, but incurs large nu-
merical diﬀusion on the coarse grid and loses the sharp
concentration gradient. However, when the ﬁne-grid so-
lution is directly resampled to the coarse grid, most sharp
features can actually be preserved. Thus, the inability to
resolve sharp gradients is not due to the coarse grid it-
self, but instead due to the numerical error in the baseline
advection solver. Our neural network model, trained to
track the optimal reference solution on the coarse grid,
is able to preserve sharp features during the forward in-
tegration. The model performs well on all test samples,
with more shown in Appendix.

Fig. 8 shows a variety of error metrics for advection un-
der turbulent ﬂow, averaged over all test samples. The
error is computed as the deviation from the reference so-
lution obtained by the baseline solver at 256 × 256 grid.
We also compare the baseline solver at intermediate grid
resolutions (64 × 64 and 128 × 128). All solutions are re-
sampled to the 32 × 32 coarse grid for error calculation.
We use two measurements of accuracy, mean absolute
error (our training loss) and the coeﬃcient of determi-
nation R2, which means the goodness of ﬁt for linear
regression models for to reference solution. Based these
metrics, our neural network model achieves roughly the
same accuracy as the baseline method at 4× resolution
(128 × 128). We also evaluate the entropy for all so-

FIG. 6. Entropy for advection under 2-D deforma-
tional ﬂow. Entropy is conserved under pure advection and
increases under diﬀusion. Traditional monotonic solvers are
only allowed to increase entropy, while our neural network
model is allowed to decrease entropy and and thus minimizes
diﬀusion error over a long time.

the baseline VanLeer scheme incurs large numerical dif-
fusion when the initial blob is stretched to a thin ﬁlament
[16].

To quantify the numerical diﬀusion, we use the entropy

S as a metric [56]:

S = −β

(cid:88)

i

Ci · log(Ci)

(10)

where the concentration C is scaled such that the initial
conditions falls in the range [0, 1], and β is a normaliza-
tion factor so that the initial entropy is 1. Entropy is
conserved under pure advection and increases under dif-
fusion. To avoid an undeﬁned answer if any Ci < 0, we
use ﬁrst set negative values in the concentration to zero,
and evaluate C = 0 via the limit x log x = 0 as x → 0.

Fig. 6 shows the entropy over time. Any monotonic
advection solver can only increase entropy; any entropy
decrease indicates nonphysical anti-diﬀusion, which often
occurs due to numerical instability. Strikingly, the neural
network model can decrease entropy, while still remain-
ing numerically stable. Although such behavior seems to
be nonphysical, it is indeed the best possible solution on
such a coarse grid. On a grid that perfectly resolves the
concentration ﬁeld, the entropy remains constant under
the deformational ﬂow. Yet on a coarse grid view, the
computed entropy increases when the initial blob turns
into ﬁlament due to conservative averaging, and then
decreases when the ﬁlament reverts back into a blob.
Our neural network model can disobey the commonly-
used constraint of non-decreasing entropy, and thus more
closely matches the exact solution, when compared to
traditional monotonic solvers.

C. 2-D turbulent ﬂow

As the ﬁnal test, we use the velocity ﬁelds from freely-
evolving, decaying 2-D turbulence simulations in pyqg
(https://github.com/pyqg/pyqg). The spatial domain

05101520Time1.001.251.501.752.002.252.50EntropyBaselineNeural netFlow reversed7

FIG. 7. One test sample under 2-D turbulent ﬂow. The initial blobs (ﬁrst column) are stretched into thin ﬁlaments
under the turbulent ﬂow (last column), illustrated by the vorticity ω = ∂xuy − ∂yuv. The baseline solver (second-order VanLeer
scheme) can resolve such ﬁlaments on the ﬁne-resolution grid, but incurs large numerical diﬀusion on the coarse grid. The
neural network model can preserve the sharp features on the coarse grid. The spatial domain is [0, 2π] × [0, 2π] (not plotted on
axis).

lutions based on Eq. (10), which suggests that from a
statistical perspective our neural net model almost per-
fectly matches the reference simulation on which it was
trained.

Figure 9 illustrates the limitations of stability and
generalization for our neural net model by integrating
for far longer than the 128 time steps used for train-
ing data. After 1000 time integration steps, our neural
net model shows obvious numerical artifacts (checker-
board patterns) and very poor accuracy for about 10%
of random seeds. Unlike the baseline models, our neural
net model does not guarantee properties such as mono-
tonicity, and when presented with examples outside of its
training data it occasionally extrapolates poorly. Figur-
ing out how to guarantee stability for neural net models,
either by training on more comprehensive datasets or by
imposing architecture constraints, is an important topic
for future work.

Finally, to get a glimpse into the inner workings of the
trained model, Fig. 10 examines predicted interpolation
coeﬃcients for x component of the velocity ﬁeld. We
see that similar to hand-crafted methods, the learned in-
terpolation depends on both velocity and concentration.

Error for 2-D turbulent advection on test
FIG. 8.
data. The neural network model achieves the same accuracy
as the second-order VanLeer baseline scheme at 4× resolution,
and entropy similar to the baseline at 8× resolution.

Fine ReferenceTime step = 0Time step = 64Time step = 128Time step = 192Time step = 256xyVelocity fieldCoarse ReferencexyNeural net 32 x 32xyBaseline 32 x 32xy0.00.10.20.30.40.50.60.70.8Concentation15105051015Vorticity0.000.020.040.06Mean absolute errorNeural net 32 x 32ReferenceBaseline 128 x 128Baseline 64 x 64Baseline 32 x 32064128192256Time step0.60.81.0R2064128192256Time step1.01.21.41.6Entropy8

FIG. 9.
Limitations of long time stability under 2-
D turbulent ﬂow. (a) Ten randomly chosen examples of
concentrations ﬁelds from the neural net model after 1024
time steps. One ﬁeld (ﬁrst row, fourth column) is entirely
covered by “checkerboard” artifacts, and two others (top left
and bottom right) show checkerboard artifacts in limited re-
gions. (b) Empirical distribution function for absolute error
across all models after 256 and 1024 time steps. The neu-
ral net performance suﬀers signiﬁcantly, with about 10% of
solutions having an absolute error greater than 1.

While some of the symmetries have been clearly learned
from the data, we believe that incorporating such priors
could improve the results further.

IV. COMPUTATIONAL PERFORMANCE AND
ACCURACY WITH DIFFERENT
HYPERPARAMETERS

There is a tradeoﬀ between accuracy and speed for our
neural network model, as using a larger convolutional
neural network increases both the accuracy and the run
time. We performed a grid search on model hyperpa-
rameters, for the number of layers ranging from [4, 6, 8,
10], the number of convolutional ﬁlers ranging from [16,
32, 64, 128], and the ﬁnite-diﬀerence stencil size ranging
from [3, 5], with each case replicated 3 times with dif-
ferent random seeds. The model accuracy is evaluated
on the 2-D turbulence case in Section III C, and the run
time is measured on a single Nvidia V100 GPU.

Interpolation coeﬃcients for 2D advection.
FIG. 10.
Illustration of how prediction of the interpolation coeﬃcients
changes for diﬀerent combinations of concentration (top row)
and velocity ﬁeld (left column) inputs. Concentration values
represented by color; velocity ﬁeld has unit magnitude and
changes direction as shown in the plot. The target location
for the interpolation is marked by a red bar on the concen-
tration plots. The model predominantly interpolates along
the velocity ﬁeld and concentration edges, rediscovering the
upwinding-like methods at the corner cases of the facet. While
the model learned some general symmetries, we expect even
better results for models that incorporate symmetries by con-
struction.

Fig. 11 shows the model accuracy and speed using dif-
ferent hyperparameters. The performance of the base-
line solver at intermediate grid resolutions (64 × 64 and
128 × 128) is overlaid on for comparison. A large neural
network (8 ∼ 10 layers and 128 ﬁlters) achieves compa-
rable accuracy and speed as the baseline solver at 4×
resolution, while a small neural network (4 layers and 32
ﬁlters) performs similarly to the baseline solver at 2×
resolution. Fig. 12 shows that using 64 ﬁlters and 10 lay-
ers achieves a good balance between accuracy and speed,
in which case the model achieves similar accuracy as the
4× resolution baseline while being 80% faster.

The model speed largely depends on the code imple-
mentation and software conﬁguration. Our current im-
plementation of the neural network model has a lot of
room for performance optimization. For example, our
code still requires unnecessary, large memory allocation,
and does not use the reduced-precision Tensor Cores in
the V100 GPU. With more performance tuning as well as
techniques like model compression and quantization [59],

103102101100Absolute error0.00.20.40.60.81.0ProportionTime step = 256103102101100Absolute errorTime step = 1024Neural net 32 x 32Baseline 128 x 128Baseline 64 x 64Baseline 32 x 320.00.10.20.30.40.5concentation(a)(b)1.501.000.400.300.150.100.050.050.100.150.300.401.001.50Interpolation weights01ConcentrationVelocityConcentration9

tem) models running on next generation architectures
use less than 2% of peak performance” [60]. This is
because classic numerical methods (e.g. ﬁnite diﬀer-
ence, ﬁnite volume) are limited by memory bandwidth
rather than processor speed [61] [62]. In contrast, neural
networks mostly consist of dense matrix multiplications
with a high compute-to-memory ratio, and therefore can
achieve near-peak performance on both CPUs and hard-
ware accelerators (see the Rooﬂine charts [63] in [64]).
We measure the machine utilization using Perf on CPU
and NVProf on GPU, and ﬁnd that the neural network
model achieves 80% of peak FLOPs (ﬂoating point op-
erations per second), while the baseline solver only uses
1 ∼ 2% of peak FLOPs. Clever use of neural network
emulations inside existing models may provide a way to
squeeze out “free compute cycles” that are currently not
utilized.

V. CONCLUSION

We developed a data-driven discretization for solving
passive scalar advection in one- or two- dimensions. In-
stead of using pre-deﬁned coeﬃcients to compute the spa-
tial derivatives in the partial diﬀerential equation (PDE),
we used a convolutional neural network to learn the opti-
mal ﬁnite diﬀerence coeﬃcients, so that the solution best
matches the “true” result obtained by high-resolution ref-
erence simulations.

Our neural-network-based model is able to obtain near-
perfect results for idealized 1-D and 2-D test problems,
while a traditional high-order solver incurs signiﬁcant dif-
fusion error. Under a 2-D turbulent ﬂow, the neural net-
work model running on a coarse grid can achieve the same
accuracy as a traditional high-order solver running on a
4× higher resolution grid.

The neural network model exhibits several interesting
behaviors that may help explain its unusual accuracy.
Our learned models have been speciﬁcally optimized for
modeling speciﬁc class of ﬂows used as training data,
which limits their range of validity. For example, in 1D
the model converts unseen shapes into known shapes,
and on 2D turbulent ﬂows the model occasionally fails
entirely when asked to make predictions for much longer
times than were used in training. An important chal-
lenge for future work is identify techniques that can en-
sure learned discretizations are robust even to such out-
of-distribution inputs. Alternatively, it may be able to
identify training datasets that cover the full range of phe-
nomena of interest, e.g., in the context of weather or pol-
lution forecasts where the same equations are solved day
after day.

At the same accuracy, the speed of our neural network
model is comparable to the baseline high-order solver
(that runs at 4× higher resolution). There is a lot of
room for further optimizing the neural network perfor-
mance in our code implementation. Notably, the neural
network model can achieve a much higher machine uti-

FIG. 11. Accuracy-speed tradeoﬀ for neural network
model. Each data point is a neural network model with dif-
ferent hyperparamters (detailed in Fig. 12). The performance
of the baseline solver at intermediate grid resolutions (64 × 64
and 128 × 128) is overlaid for comparison. The model accu-
racy is evaluated on the 2-D turbulence case after 256 time
steps (Section III C), and the run time is measured on a single
Nvidia V100 GPU. The x-axis shows the wall-clock time per
advection time step on the coarse grid, which requires 2 or 4
time steps for the 64 × 64 or 128 × 128 baseline due to the
CFL condition.

FIG. 12. Hyper-parameter eﬀect on neural network
model performance. Same as Fig. 11, but here the data
points are grouped by diﬀerent numbers of convolutional lay-
ers and the number of ﬁlters, with shape denoted the size of
the stencil. Duplicate points corresponds to identical models
trained with diﬀerent random seeds.

the neural network model may signiﬁcantly outperform
the baseline in terms of computational performance.

Incorporating neural networks into numerical meth-
ods also allows better utilization of current and emerg-
It is reported that “current (Earth sys-
ing hardware.

0.00.20.40.60.81.01.2Run time per step (seconds)0.0200.0250.0300.0350.0400.0450.050Mean absolute errorBaseline 128 x 128Baseline 64 x 64Neural net 32 x 32 (different model sizes)0.020.030.040.05Mean absolute errornum_layers = 4num_layers = 60.00.51.0Runtime per step0.020.030.040.05Mean absolute errornum_layers = 80.00.51.0Runtime per stepnum_layers = 10filters163264128stencil_size3x35x5lization than traditional ﬁnite-diﬀerence methods, and
will better utilize emerging hardware accelerators.

10

An open question is how to apply our method in
existing computational ﬂuid dynamics (CFD) or cli-
mate/weather models, which tend to be implemented in
large codebases of C++ or Fortran. Although past work
has successfully replaced one component in a model with
neural networks [65], our approach works best in an end-
to-end diﬀerentiable program. Recent eﬀorts in imple-
menting models in Julia [66] and JAX [37] should ease the
integration of scientiﬁc computing and machine learning.
avail-
for
https://github.com/google-research/

and tutorials

this work

Code

are

able
at
data-driven-pdes.

ACKNOWLEDGEMENTS

We thank Anton Gerashenko, Jamie Smith, Peynman
Milanfar, Pascal Getreur, Ignacio Garcia Dorado and Rif
Saurous for important conversations. Y.B.S acknowl-
edges support from the James S. McDonnel post-doctoral
fellowship for the study of complex systems. M.P.B ac-
knowledges support from NSF DMS-1715477 as well as
the Simons Foundation.

APPENDIX: SAMPLE RESULTS FOR 2-D
TURBULENT ADVECTION

Figure S1 shows more test samples for the 2-D turbu-
lent advection problem in Section III C. In all test sam-
ples, the neural network model is able to maintain a sharp
gradient that closely matches the reference true resolu-
tion, while the baseline model incurs signiﬁcant numeri-
cal diﬀusion error.

11

FIG. S1.
Sample results for advection under 2-D turbulent ﬂow. Concentration ﬁelds after 256 time steps are
illustrated for six diﬀerent randomly initialized concentration and velocity ﬁelds. The models are the same as those illustrated
in Fig. 7.

Fine ReferenceSample = 0Sample = 1Sample = 2Sample = 3Sample = 4Sample = 5Coarse ReferenceNeural net 32 x 32Baseline 32 x 320.00.10.20.30.40.50.60.70.8concentation[1] J. Jim´enez, Computers and turbulence, European Jour-

nal of Mechanics-B/Fluids (2019).

[2] P. Bauer, A. Thorpe, G. Brunet, The quiet revolution of
numerical weather prediction, Nature 525 (7567) (2015)
47–55 (2015).

[3] T. Schneider, J. Teixeira, C. S. Bretherton, F. Brient,
K. G. Pressel, C. Sch¨ar, A. P. Siebesma, Climate goals
and computing the future of clouds, Nature Climate
Change 7 (1) (2017) 3–5 (2017).

[4] P. Neumann, P. D¨uben, P. Adamidis, P. Bauer,
M. Br¨uck, L. Kornblueh, D. Klocke, B. Stevens, N. Wedi,
J. Biercamp, Assessing the scales in numerical weather
and climate predictions: will exascale be the res-
cue?, Philosophical Transactions of the Royal Society A
377 (2142) (2019) 20180148 (2019).

[5] T. N. Theis, H.-S. P. Wong, The end of moore’s law: A
new beginning for information technology, Computing in
Science & Engineering 19 (2) (2017) 41–50 (2017).

[6] J. Shalf, The future of computing beyond moore’s
law, Philosophical Transactions of the Royal Society A
378 (2166) (2020) 20190061 (2020).

[7] T. Schneider, S. Lan, A. Stuart, J. Teixeira, Earth system
modeling 2.0: A blueprint for models that learn from ob-
servations and targeted high-resolution simulations, Geo-
physical Research Letters 44 (24) (2017) 12–396 (2017).
[8] M. Reichstein, G. Camps-Valls, B. Stevens, M. Jung,
J. Denzler, N. Carvalhais, et al., Deep learning and pro-
cess understanding for data-driven earth system science,
Nature 566 (7743) (2019) 195–204 (2019).

[9] J. N. Kutz, Deep learning in ﬂuid dynamics, Journal of

Fluid Mechanics 814 (2017) 1–4 (2017).

[10] K. Duraisamy, G. Iaccarino, H. Xiao, Turbulence model-
ing in the age of data, Annual Review of Fluid Mechanics
51 (2019) 357–377 (2019).

[11] S. Hatﬁeld, M. Chantry, P. D¨uben, T. Palmer, Acceler-
ating high-resolution weather models with deep-learning
hardware, in: Proceedings of the Platform for Advanced
Scientiﬁc Computing Conference, 2019, pp. 1–11 (2019).
[12] J. L. Hennessy, D. A. Patterson, A new golden age
for computer architecture, Communications of the ACM
62 (2) (2019) 48–60 (2019).

[13] J. Dean, The deep learning revolution and its implica-
tions for computer architecture and chip design, arXiv
preprint arXiv:1911.05289 (2019).

[14] Y. Bar-Sinai, S. Hoyer, J. Hickey, M. P. Brenner, Learn-
ing data-driven discretizations for partial diﬀerential
equations, Proceedings of the National Academy of Sci-
ences 116 (31) (2019) 15344–15349 (2019).

[15] B. I. Shraiman, E. D. Siggia, Scalar turbulence, Nature

405 (6787) (2000) 639–646 (2000).

[16] Y. Rastigejev, R. Park, M. P. Brenner, D. J. Jacob, Re-
solving intercontinental pollution plumes in global mod-
els of atmospheric transport, Journal of Geophysical Re-
search: Atmospheres 115 (D2) (2010).

[17] R. J. LeVeque, High-resolution conservative algorithms
for advection in incompressible ﬂow, SIAM Journal on
Numerical Analysis 33 (2) (1996) 627–665 (1996).
[18] P. K. Sweby, High resolution schemes using ﬂux limiters
for hyperbolic conservation laws, SIAM journal on nu-
merical analysis 21 (5) (1984) 995–1011 (1984).

12

[19] S.-J. Lin, R. B. Rood, Multidimensional ﬂux-form semi-
lagrangian transport schemes, Monthly Weather Review
124 (9) (1996) 2046–2070 (1996).

[20] P. A. Ullrich, C. Jablonowski, B. Van Leer, High-order
ﬁnite-volume methods for the shallow-water equations on
the sphere, Journal of Computational Physics 229 (17)
(2010) 6104–6134 (2010).

[21] P. Colella, M. D. Sekora, A limiter for ppm that preserves
accuracy at smooth extrema, Journal of Computational
Physics 227 (15) (2008) 7069–7076 (2008).

[22] A. Semakin, Y. Rastigejev, Numerical simulation of
global-scale atmospheric chemical transport with high-
order wavelet-based adaptive mesh reﬁnement algorithm,
Monthly Weather Review 144 (4) (2016) 1469–1486
(2016).

[23] A. Stohl, C. Forster, A. Frank, P. Seibert, G. Wotawa,
The lagrangian particle dispersion model ﬂexpart version
6.2 (2005).

[24] S. D. Eastham, D. J. Jacob, Limits on the ability of
global eulerian models to resolve intercontinental trans-
port of chemical plumes., Atmospheric Chemistry &
Physics 17 (4) (2017).

[25] P. H. Lauritzen, R. D. Nair, P. A. Ullrich, A con-
servative semi-lagrangian multi-tracer transport scheme
(cslam) on the cubed-sphere grid, Journal of Computa-
tional Physics 229 (5) (2010) 1401–1424 (2010).

[26] C. S. Kulkarni, P. F. Lermusiaux, Advection with-
out compounding errors through ﬂow map composition,
Journal of Computational Physics 398 (2019) 108859
(2019).

[27] Using this MAE loss indicates that we require point-wise
agreement between the machine learning prediction and
the reference true solution, on every time step. This crite-
ria is relevant for atmospheric transport modeling where
the goal is to simulate the instantaneous, deterministic
concentration ﬁeld [16]. For turbulence modeling, match-
ing the high-order statistics might be more desirable than
requiring point-wise agreement of instantaneous ﬁelds.
This would require changes in the loss function.

[28] N. D. Brenowitz, C. S. Bretherton, Prognostic valida-
tion of a neural network uniﬁed physics parameterization,
Geophysical Research Letters 45 (12) (2018) 6289–6298
(2018).

[29] M. Innes, A. Edelman, K. Fischer, C. Rackauckus,
E. Saba, V. B. Shah, W. Tebbutt, Zygote: A dif-
ferentiable programming system to bridge machine
learning and scientiﬁc
arXiv preprint
arXiv:1907.07587 (2019).

computing,

[30] A. G. Baydin, B. A. Pearlmutter, A. A. Radul, J. M.
Siskind, Automatic diﬀerentiation in machine learning: a
survey, The Journal of Machine Learning Research 18 (1)
(2017) 5595–5637 (2017).

[31] C. Bischof, P. Khademi, A. Mauer, A. Carle, Adifor 2.0:
Automatic diﬀerentiation of fortran 77 programs, IEEE
Computational Science and Engineering 3 (3) (1996) 18–
32 (1996).

[32] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis,
J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Is-
ard, et al., Tensorﬂow: A system for large-scale machine
learning, in: 12th {USENIX} Symposium on Operating
Systems Design and Implementation ({OSDI} 16), 2016,

pp. 265–283 (2016).

[33] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang,
Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, A. Lerer,
Automatic diﬀerentiation in pytorch (2017).

[34] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson,
C. Leary, D. Maclaurin, S. Wanderman-Milne, Jax: com-
posable transformations of python+ numpy programs,
URL http://github.com/google/jax.

[35] M. Innes, Flux: Elegant machine learning with julia,
Journal of Open Source Software 3 (25) (2018) 602
(2018).

[36] Swift diﬀerentiable programming manifesto, 2018, URL

https://github.com/apple/swift.

[37] S. S. Schoenholz, E. D. Cubuk, Jax, md: End-to-end
diﬀerentiable, hardware accelerated, molecular dynamics
in pure python, arXiv preprint arXiv:1912.04232 (2019).
[38] A. Agrawal, A. N. Modi, A. Passos, A. Lavoie,
A. Agarwal, A. Shankar, I. Ganichev, J. Levenberg,
M. Hong, R. Monga, et al., Tensorﬂow eager: A multi-
stage, python-embedded dsl for machine learning, arXiv
preprint arXiv:1903.01855 (2019).

[39] S.-J. Lin, W. C. Chao, Y. Sud, G. Walker, A class
of the van leer-type transport schemes and its applica-
tion to the moisture transport in a general circulation
model, Monthly Weather Review 122 (7) (1994) 1575–
1593 (1994).

[40] H. Smaoui, B. Radi, Comparative study of diﬀerent ad-
vective schemes: Application to the mecca model, Envi-
ronmental Fluid Mechanics 1 (4) (2001) 361–381 (2001).
[41] H. Hassanzadeh, J. Abedi, M. Pooladi-Darvish, A com-
parative study of ﬂux-limiting methods for numerical
simulation of gas–solid reactions with arrhenius type
reaction kinetics, Computers & Chemical Engineering
33 (1) (2009) 133–143 (2009).

[42] M. Raissi, G. E. Karniadakis, Hidden physics models:
Machine learning of nonlinear partial diﬀerential equa-
tions, Journal of Computational Physics 357 (2018) 125–
141 (2018).

[43] M. Raissi, P. Perdikaris, G. E. Karniadakis, Physics-
informed neural networks: A deep learning framework for
solving forward and inverse problems involving nonlinear
partial diﬀerential equations, Journal of Computational
Physics 378 (2019) 686–707 (2019).

[44] E. d. B´ezenac, E. de B´ezenac, A. Pajot, P. Gallinari,
Deep learning for physical processes: incorporating prior
scientiﬁc knowledge (2019).
URL http://dx.doi.org/10.1088/1742-5468/ab3195

[45] K. Um, Raymond, Fei, P. Holl, R. Brand, N. Thuerey,
Solver-in-the-Loop: Learning from diﬀerentiable physics
to interact with iterative PDE-Solvers (Jun. 2020).
arXiv:2007.00016.
URL http://arxiv.org/abs/2007.00016

[46] J. Pathak, M. Mustafa, K. Kashinath, E. Motheau,
T. Kurth, M. Day, Using machine learning to augment
Coarse-Grid computational ﬂuid dynamics simulations
(Sep. 2020). arXiv:2010.00072.
URL http://arxiv.org/abs/2010.00072

[47] H. Frezat, G. Balarac, J. Le Sommer, R. Fablet,
R. Lguensat, Physical invariance in neural networks for
subgrid-scale scalar ﬂux modeling (Oct. 2020). arXiv:
2010.04663.
URL http://arxiv.org/abs/2010.04663

[48] J. Ling, A. Kurzawski, J. Templeton, Reynolds averaged
turbulence modelling using deep neural networks with

13

embedded invariance, J. Fluid Mech. 807 (2016) 155–166
(Nov. 2016).
URL
journals/journal-of-fluid-mechanics/article/
reynolds-averaged-turbulence-modelling-using-deep-neural-networks-with-embedded-invariance/
0B280EEE89C74A7BF651C422F8FBD1EB

https://www.cambridge.org/core/

[49] J. P. Boris, D. L. Book, Flux-corrected transport.

i.
shasta, a ﬂuid transport algorithm that works, Journal
of computational physics 11 (1) (1973) 38–69 (1973).
[50] D. L. Book, The conception, gestation, birth, and infancy
of fct, in: Flux-Corrected Transport, Springer, 2012, pp.
1–21 (2012).

[51] D. P. Kingma, J. Ba, Adam: A method for stochastic
optimization, arXiv preprint arXiv:1412.6980 (2014).
[52] M. T. Odman, A quantitative analysis of numerical dif-
fusion introduced by advection algorithms in air quality
models, Atmospheric Environment 31 (13) (1997) 1933–
1940 (1997).

[53] R. D. Nair, P. H. Lauritzen, A class of deformational ﬂow
test cases for linear transport problems on the sphere,
Journal of Computational Physics 229 (23) (2010) 8868–
8887 (2010).

[54] P. H. Lauritzen, A standard test case suite for two-
dimensional linear transport on the sphere: results from
a collection of state-of-the-art schemes, Geoscientiﬁc
Model Development (2014).

[55] T. Saad, J. C. Sutherland, Comment on “diﬀusion by a
random velocity ﬁeld”[phys. ﬂuids 13, 22 (1970)], Physics
of Fluids 28 (11) (2016) 22 (2016).

[56] J. Zhuang, D. J. Jacob, S. D. Eastham, The importance
of vertical resolution in the free troposphere for model-
ing intercontinental plumes, Atmospheric Chemistry and
Physics 18 (8) (2018) 6039 (2018).

[57] J. C. McWilliams, The emergence of isolated coherent
vortices in turbulent ﬂow, Journal of Fluid Mechanics
146 (1984) 21–43 (1984).

[58] J. Methven, B. Hoskins, The advection of high-resolution
tracers by low-resolution winds, Journal of the atmo-
spheric sciences 56 (18) (1999) 3262–3285 (1999).

[59] Y. Cheng, D. Wang, P. Zhou, T. Zhang, A survey of
model compression and acceleration for deep neural net-
works, arXiv preprint arXiv:1710.09282 (2017).

[60] J. C. Carman, T. Clune, F. Giraldo, M. Govett,
A. Kamrath, T. Lee, D. McCarren, J. Michalakes,
S. Sandgathe, T. Whitcomb, Position paper on high per-
formance computing needs in earth system prediction,
URL https://doi.org/10.7289/V5862DH3 (2017).

[61] T. C. Schulthess, P. Bauer, N. Wedi, O. Fuhrer, T. Hoe-
ﬂer, C. Sch¨ar, Reﬂecting on the goal and baseline for
exascale computing: a roadmap based on weather and
climate simulations, Computing in Science & Engineer-
ing 21 (1) (2018) 30–41 (2018).

[62] Indeed, not all traditional solvers have ultra-low machine
utilization. For example, the discontinuous Galerkin
(DG) method can have a much higher machine utiliza-
tion of 20% [67]. Although computational ﬂuid dynamics
(CFD) models can exploit high-order methods, climate
models tend to use low-order schemes [61].

[63] S. Williams, A. Waterman, D. Patterson, Rooﬂine: an in-
sightful visual performance model for multicore architec-
tures, Communications of the ACM 52 (4) (2009) 65–76
(2009).

[64] N. P. Jouppi, C. Young, N. Patil, D. Patterson,
G. Agrawal, R. Bajwa, S. Bates, S. Bhatia, N. Boden,

A. Borchers, et al., In-datacenter performance analysis
of a tensor processing unit, in: Proceedings of the 44th
Annual International Symposium on Computer Architec-
ture, 2017, pp. 1–12 (2017).

[65] S. Rasp, M. S. Pritchard, P. Gentine, Deep learning to
represent subgrid processes in climate models, Proceed-
ings of the National Academy of Sciences 115 (39) (2018)

9684–9689 (2018).

[66] Clima:

Climate
https://github.com/climate-machine/CLIMA.

machine,

14

URL

[67] A. Breuer, Y. Cui, A. Heinecke, Petaﬂop seismic simula-
tions in the public cloud, in: International Conference on
High Performance Computing, Springer, 2019, pp. 167–
185 (2019).

