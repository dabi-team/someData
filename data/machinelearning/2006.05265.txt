1
2
0
2

n
u
J

2

]

G
L
.
s
c
[

6
v
5
6
2
5
0
.
6
0
0
2
:
v
i
X
r
a

MISIM: A Neural Code Semantics Similarity System
Using the Context-Aware Semantics Structure

Fangke Ye ∗
Intel Labs and Georgia Institute of Technology
yefangke@gatech.edu

Shengtian Zhou ∗
Intel Labs
shengtian.zhou@intel.com

Anand Venkat
Intel Labs
anand.venkat@intel.com

Ryan Marcus
Intel Labs and MIT
ryanmarcus@csail.mit.edu

Nesime Tatbul
Intel Labs and MIT
tatbul@csail.mit.edu

Jesmin Jahan Tithi
Intel Labs
jesmin.jahan.tithi@intel.com

Niranjan Hasabnis
Intel Labs
niranjan.hasabnis@intel.com

Paul Petersen
Intel
paul.petersen@intel.com

Timothy Mattson
Intel Labs
timothy.g.mattson@intel.com

Tim Kraska
MIT
kraska@mit.edu

Pradeep Dubey
Intel Labs
pradeep.dubey@intel.com

Vivek Sarkar
Georgia Institute of Technology
vsarkar@gatech.edu

Justin Gottschlich
Intel Labs and University of Pennsylvania
justin.gottschlich@intel.com

Abstract

Code semantics similarity can be used for many tasks such as code recommendation,
automated software defect correction, and clone detection. Yet, the accuracy of
such systems has not yet reached a level of general purpose reliability. To help
address this, we present Machine Inferred Code Similarity (MISIM), a neural code
semantics similarity system consisting of two core components: (i) MISIM uses a
novel context-aware semantics structure, which was purpose-built to lift semantics
from code syntax; (ii) MISIM uses an extensible neural code similarity scoring
algorithm, which can be used for various neural network architectures with learned
parameters. We compare MISIM to four state-of-the-art systems, including two
additional hand-customized models, over 328K programs consisting of over 18
million lines of code. Our experiments show that MISIM has 8.08% better accuracy
(using MAP@R) compared to the next best performing system.

∗ Lead authors.

Preprint. Under review.

 
 
 
 
 
 
1

Introduction

The ﬁeld of machine programming (MP) is concerned with the automation of software develop-
ment [Gottschlich et al., 2018]. In recent years, there has been an emergence of many MP systems
due, in part, to advances in machine learning, formal methods, data availability, and computing
efﬁciency [Allamanis et al., 2018a, Alon et al., 2018, 2019b,a, Ben-Nun et al., 2018, Cosentino et al.,
2017, Li et al., 2017, Luan et al., 2019, Odena and Sutton, 2020, Finkel and Laguna, 2020, Tufano
et al., 2018, Wei and Li, 2017, Zhang et al., 2019, Zhao and Huang, 2018]. An open challenge in MP
is in the construction of accurate code similarity systems. Code similarity, which determines if two
or more code fragments are similar, can be reasoned about in many ways. Two principle ways are
through syntactic similarity and semantic analysis. While attention has historically centered around
code syntax similarity (i.e., whether two or more code fragments are syntactically similar) recent
work has revealed many advantages of code semantics similarity (i.e., whether two or more code
fragments are similar in meaning, even in the presence of syntactic differences) [Iyer et al., 2020, Lee
et al., 2021]. Precisely, we deﬁne two code fragments Ci and Cj to be semantically equivalent if for
a given set of inputs, I, both Ci and Cj, produce an identical respective set of outputs, O.

Figure 1: Three structural representations of Program A: CASS, AST, and SPT.

Code semantics similarity systems aim to determine if code fragments are solving a similar (or
equivalent) problem, even if implemented differently (e.g., various algorithms of sort() [Cormen
et al., 2009]). They can be used in many ways. One way is to improve programmer productivity on
tasks such as code recommendation, clone detection, bug detection and mitigation, and language-
to-language translation (i.e., transpilation), to name a few [Allamanis et al., 2018b, Ahmad et al.,
2019, Bader et al., 2019, Barman et al., 2016, Bhatia et al., 2018, Dinella et al., 2020, Kamil et al.,
2016, Luan et al., 2019, Pradel and Sen, 2018]. Moreover, with improved accuracy, code semantics
similarity systems can likely be leveraged to automate many other parts of software development,
which may become necessary to keep pace with the heterogeneous growth of programming languages
and hardware systems [Ahmad et al., 2019, Batra et al., 2018, Bogdan et al., 2019, Chen et al., 2020,
Deng et al., 2020, Hannigan et al., 2019]. Yet, despite advances in the space of code semantics
similarity, fundamental questions remain open. One open question, which is the principle focus of
this paper, is regarding how the structural representations impact learning semantics similarity in
syntactically diverse code sets [Alam et al., 2019, Allamanis et al., 2018b, Becker and Gottschlich,
2017, Ben-Nun et al., 2018, Dinella et al., 2020, Iyer et al., 2020, Luan et al., 2019].

While prior work has explored some structural representations of code in the space of code similarity
and understanding, these explorations are still in their early stages. The classical abstract syntax tree
(AST) is used in the code2vec and code2seq systems [Alon et al., 2019b,a], while two novel structures
called the conteXtual ﬂow graph (XFG) and the simpliﬁed parse tree (SPT) are used in Neural Code
Comprehension (NCC) [Ben-Nun et al., 2018] and Aroma [Luan et al., 2019], respectively. While
each of these representations has beneﬁts in certain contexts, they possess one or more limitations
when considered more broadly. For example, the AST – while having a notable historical importance

2

(b) Abstract Syntax Tree (AST)(a) Context-Aware Semantics Structure (CASS1-2-0-0-1)(c) Simplified Parse Tree (SPT)compound statementboolean conditioncall expression argument listglobal attributes tableparenthetical expressionnodes that are invariant across treesexpression statementcondition/argument listidentifiercompound statement that encodes the cardinality of child statementstwo non-keyword tokens or SPTlink to direct descendentlink to indirect descendenthidden subtree…#compound_stmt#{#}#while_stmt##condition##compound_stmt#{#}#if_stmt##condition##condition#………#call_expr#isdigit#arg_list#cinpeek…compound_stmtwhile_stmt……paren_exprparen_exprparen_exprexpr_stmtexpr_stmtcall_exprcompound_stmt…while_stmtif_stmtidentifierarg_listisdigitidentifiercinpeek{$$$}while$$……($){$$}while$$if$$…identifier($)($)$$isdigitcinpeek($)…………Global attributes table (GAT)0input_cardinality1output_cardinalitySyntactically denseAmbiguity introductionLegend#while_stmt#for optimizing compilers – can be syntactically dense (see Figure 1(b)). Such syntax density can
mislead code similarity systems into memorizing syntax, rather than learning semantics (i.e., the
meaning behind the syntax). Alternatively, the XFG can capture important data dependencies, but is
obtained from an intermediate representation (IR) that requires code compilation. This restriction
can limit its application in interactive developer environments such as live code auto-completion or
recommendation. The SPT is structurally driven, which enables it to lift certain semantic meaning
from code, yet, it does not always resolve syntactic ambiguities. Instead, it can sometimes introduce
them, due to its intentional coarse-grain approach (see Figure 1(c)).

Learning from these observations, we present our code semantics similarity system called Machine
Inferred Code Similarity (MISIM). We principally focus on MISIM’s two main novelties: (i) its novel
code structural representation, the context-aware semantics structure (CASS), and (ii) its neural-based
learned code similarity scoring algorithm. This paper makes the following technical contributions:

• We present MISIM’s context-aware semantics structure (CASS), a conﬁgurable representation of
code designed to (i) lift semantic meaning from code syntax, which (ii) supports language-speciﬁc
and language-agnostic extensibility.

• We present MISIM’s learned deep neural network (DNN) semantics similarity scoring framework
and show its efﬁcacy across three DNN topologies: (i) bag-of-features, (ii) a recurrent neural
network (RNN), and (iii) a graph neural network (GNN).

• We compare MISIM to four existing code similarity systems: (i) code2vec, (ii) code2seq, (iii)
Neural Code Comprehension, and (iv) Aroma. To deepen the experimental analysis, we also
include two customized source code token sequence models and include reported results from IBM
and MIT’s CodeNet repository, who conducted their own comparison of MISIM to Aroma. Across
approximately 18 million lines of code (not including CodeNet), our results show that MISIM has,
at its worst, 8.08% better accuracy than the next best performing system.

2 Code Representations for Code Semantics Similarity Analysis

Code representation is a core component of code semantics similarity analysis. Existing approaches
fall under two categories: syntax-based representations and semantics-based representations. We
provide a brief anecdotal analysis of these representations, discuss their strengths and weaknesses for
code semantics similarity, and motivate a new approach that offers more conﬁgurability.

A Real-World Example. We analyze two simple code examples (Program A and B) taken from the
POJ-104 dataset [Mou et al., 2016]. These programs correctly solve the same problem (#88), where
the goal is to emit all digits in a given input string. Hence, while the implementations of Program A
and B are syntactically dissimilar, they are semantically equivalent. Code for Program A and B is
shown below; Figure 1 illustrates three different ways to represent Program A.

Program A

Program B

int a;
// algorithm
while (!cin.eof()) {

while (!cin.eof() && !isdigit(cin.peek()))

cin.get(); // ignore

// print out result
if (cin >> a)

cout << a << endl;

}

char *p, *head, c;
p = (char *) malloc(sizeof(char) * 30);
head = p; scanf("%c", p);
while (*p != ’\n’) { p++; *p = getchar();}
*p = ’\0’; p = head;
for (; *p != ’\0’; p++) {

if(*p <= ’9’ && *p >= ’0’){printf("%c",*p);}
else if(*(p+1) < 58 && *(p+1) > 47){putchar(’\n’);}

}

Syntax-based Representations. Compilers have successfully used syntax-based representations
of programs, such as parse trees (a.k.a., concrete syntax trees) and abstract syntax trees (ASTs),
for several decades [Baxter et al., 1998]. More recently, the code understanding systems code2vec
and code2seq have utilized ASTs as their basic representation of code for program semantics
analysis [Alon et al., 2019b,a]. Parse trees are typically built by a language-speciﬁc parser during
the compilation process and faithfully capture every syntactic detail of the source program. ASTs,
on the other hand, abstract away certain syntactic details (e.g., parentheses), which can simplify
program analysis. However, ASTs can still be syntactically dense, which can mislead code similarity
systems into memorizing syntax rather than learning semantics. For instance, the AST in Figure 1(b)
represents the parentheses of while statements in Program A as paren_expr nodes, while missing

3

the semantic binding they have to the condition expression of the while statements. Syntax-based
representations are also conceptually ill-ﬁt for cross-language code semantics similarity in general.

Semantics-based Representations. Semantics-based representations capture the semantics of vari-
ous program constructs instead of syntax. ConteXtual ﬂow graph (XFG) used by NCC and simpliﬁed
parse tree (SPT) used by Aroma are two such representations. NCC hypothesizes that program
statements that operate in similar contexts are semantically similar, where the context of a statement
is deﬁned as the other surrounding statements with direct data- and control-ﬂow dependencies. These
dependencies are captured at the IR level by deﬁning programs’ XFG representations. inst2vec
embeddings are then trained for IR instructions a la word2vec [Mikolov et al., 2013a] and skip-gram
models [Mikolov et al., 2013b]. Overall, NCC views instructions as semantically similar, if their
embedding vectors are closer. Unfortunately, NCC’s dependency on a compiler IR restricts XFG
only to compilable code. Aroma uses SPT as a program representation to enable ML-based code
search and recommendation. Aroma builds SPT of a given query program, featurizes it using a set of
manually-selected features, and performs dot product of the query’s feature vector and that of the
candidates to ﬁnd similar code snippets. SPT is different than AST in that, it only consists of program
tokens and does not use any special language-speciﬁc rule names. Thus, SPT is language-agnostic,
which enables uniform handling of code written in different programming languages.

Limitations. While semantics-based code representations, and SPT in particular, are more suitable
for general-purpose code semantics similarity analysis, our example reveals some critical limitations
with SPT. First of all, although SPT is structurally-driven (as opposed to the syntax-driven AST [Luan
et al., 2019]), it may unintentionally carry semantic ambiguity. For instance, the SPT in Figure 1(c)
does not distinguish between the argument list of a function call (e.g., "(cin.peek())") and the
condition node of an if statement (e.g., "(cin >> a)"), and represents both of them by "($)".
Moreover, it also captures program details that may be irrelevant for code semantics similarity. For
instance, it captures the number of program statements in main in the root node (e.g., as 3 $’s for
Program A, and 8 $’s for Program B), yet, while both of these programs have different number of
statements, they are semantically equivalent. Therefore, featurizing on this metric could mislead and
mistrain an ML system to infer such information as semantically meaningful.

A New Approach. We propose a new way to represent code, called the context-aware semantics
structure (CASS), to improve upon the limitations of the existing representations. CASS is a
semantics-based representation that builds on and extends SPT into a conﬁgurable structure, which
enables it to ﬂexibly capture a wide variety of structural representations (details in Section 3).
Figure 1(a) shows the CASS for Program A, obtained from one of its 216 different conﬁgurations.
In this example, CASS resolves the aforementioned ambiguity introduced by SPT by representing
argument list and condition node of an if and while statement differently, while simultaneously
eliding away unnecessary syntactic density produced by the AST (e.g., identifier nodes). These
modiﬁcations help in building a more accurate neural backend system (e.g., for this particular case,
CASS-based MISIM outperforms both its AST- and SPT-based counterparts by more than a 4.3%
margin).

3 The MISIM System

In Figure 2, we provide an overview of the MISIM system. A core component of MISIM is
the novel context-aware semantics structure (CASS), which aims to capture semantically salient
properties of the input code. Moreover, CASS is designed to be context-aware. That is, it can
capture information that describes the context of the code (e.g., parenthetical operator disambiguation
between a function call, mathematical operator precedence, Boolean logic ordering, etc.) that may
otherwise be ambiguous without such context-sensitivity. Once these CASSes are constructed,
they are vectorized and used as input to a neural network, which produces a feature vector for a
corresponding CASS. Once a feature vector is generated, a code similarity measurement (e.g., cosine
similarity [Baeza-Yates and Ribeiro-Neto, 1999]) calculates the similarity score.

3.1 Context-Aware Semantics Structure (CASS)

We have designed CASS with the following guiding principles: (i) it should not require compilation,
(ii) it should be a ﬂexible representation that captures code semantics, and (iii) it should be capable of
resolving code ambiguities in both its context sensitivity to the code and its environment. The ﬁrst

4

Figure 2: Overview of the MISIM system.

principle (i) originates from the observation that unlike programs written in higher-level scripting
languages (e.g., Python [Van Rossum and Drake, 2009], JavaScript [Flanagan, 2006]), C/C++
programs found “in the wild” may not be well-formed (e.g., due to specialized compiler dependencies)
or exhaustively include all of their dependencies (e.g., due to assumptions about library availability)
and therefore may not compile. Moreover, for code recommendation systems that are expected to
function in a live setting, requiring compilation may severely constrain their practical application.
We address this by introducing a structure such that it does not require compilation (Section 3.1.1).
The second (ii) and third (iii) principles originate from the observation that different scenarios may
require attention to different semantics (e.g., embedded memory-bound systems may prefer to use
algorithms that do not use recursion due to a potential call stack overﬂow) and that programming
languages (PLs) evolve and new PLs continue to emerge. We attempt to address these issues with
CASS’s conﬁguration categories (Section 3.1.2).

3.1.1 CASS tree and global attributes table

Here we provide an informal deﬁnition of CASS
(a formal deﬁnition is in Appendix A). The CASS
consists of one or more CASS trees and an optional
global attributes table (GAT). A CASS tree is a tree,
in which the root node represents the entire span
of the code snippet. During the construction of a
CASS tree, the program tokens are mapped to their
corresponding node labels using the grammar of the
high-level programming language. A CASS’s GAT
contains exactly one entry per unique function def-
inition in the code snippet. A GAT entry currently
includes only the input and output cardinality values
for each corresponding function, but can be extended
as new global attributes are needed.

3.1.2 CASS conﬁguration categories

In general, CASS conﬁgurations can be broadly clas-
language-speciﬁc and
siﬁed into two categories:
language-agnostic. Exact values of the options for

5

Table 1: CASS Conﬁguration Options.
Language-speciﬁc

Type

Node Preﬁx
Label

Option
0. No change (original SPT)
1. Add a preﬁx to each internal node label
2. Add a preﬁx to parenthesis node label

Type

Compound
Statements

Global
Variables

Global
Functions

(C/C++ Speciﬁc)

Language-agnostic

Option
0. No change (original SPT)
1. Drop all features relevant to compound

statements

2. Replace with ‘{#}’
0. No change (original SPT)
1. Drop all features relevant to global vars
2. Replace with ‘#GVAR’
3. Replace with ‘#VAR’ (label for local vars)
0. No change (original SPT)
1. Drop all features relevant to global functions
2. Drop function identiﬁer and replace with

‘#EXFUNC’

Function I/O
Cardinality

0. No change
1. Include the input and output cardinality

per function in GAT

Phase 0 : source codePhase 1 : CASS featurizationPhase 3 : code similarity scoring……CASS i1CASS i2Learned embeddingFeatured CASS i1Featured CASS i2Sub-network (e.g., RNN, GNN)ScoreOutputLearned embeddingSub-network (e.g., RNN, GNN)OutputInferenceTrainingCASS representation•Manual features•Serialized CASS•CASS tree Snippet 1 CASS representationDNN 1DNN 2DNN•Neural bag-of-features•Recurrent neural net•Graph neural netSimilarity metric (e.g., cosine, dot product, etc.)Metric learning loss (e.g., circle loss, triplet loss, etc.)Error minimizationfunction 1…function Nfunction 1…function MGlobal Attributes TableGlobal Attributes TableSimilarity metricSnippet 2 CASS representationPhase 2 : twin code semantic DNN trainingTraining Code Snippet i1Training Code Snippet i2……CASS j1CASS j2Featured CASS j1Featured CASS j2Snippet 1 CASS representationfunction 1…function Xfunction 1…function YGlobal Attributes TableSnippet 2 CASS representationTesting Code Snippet j1Twin code semantic DNNTwin code semantic DNNTesting Code Snippet j2j = 0..Ttest , where Ttestis the maximum number of testing tuples.i = 0..Ttrain , where Ttrainis the maximum number of training tuples. N, M, X, Y are the maximum number of functions for code snippets i1, i2, j1, j2  respectively.each of the conﬁguration categories are described in
Table 1. Below we provide an intuitive description of the categories and their values.

Language-speciﬁc conﬁgurations (LSCs). Language-speciﬁc conﬁgurations are meant to capture
semantic meaning by resolving syntactic ambiguities that could be present in the concrete syntax
trees. It also introduces speciﬁcity related to the high-level programming language. For example,
the parentheses operator is overloaded in many programming languages to enforce an order of
evaluation of operands as well as to enclose a list of function arguments, amongst other things. CASS
disambiguates these by explicitly embedding the semantic contextual information in the CASS tree
nodes using the node preﬁx label (deﬁned in Appendix A).
(A) Node Preﬁx Label. The conﬁguration options for node preﬁx labels1 correspond to various levels
of semantic to syntactic information. In Table 1, option 0 corresponds to the extreme case of a
concrete syntax embedding, option 1 corresponds to eliminating irrelevant syntax, and option 2 is
principally equivalent to option 1, except it applies only to parentheticals, which we have identiﬁed –
through empirical evaluation – to often have notably divergent semantic meaning based on context.

Language-agnostic conﬁgurations (LACs). LACs can improve code similarity analysis by unbind-
ing overly-speciﬁc semantics that may be present in the original concrete syntax tree structure.

(B) Compound Statements. The compound statements conﬁguration option enables the user to control
how much non-terminal node information is incorporated into the CASS. Option 0 is equivalent to
Aroma’s SPT, option 1 omits separate features for compound statements altogether, and option 2 does
not discriminate between compound statements of different lengths and speciﬁes a special label to
denote the presence of a compound statement.

(C) Global Variables. The global variables conﬁguration speciﬁes the degree of global variable-
speciﬁc information contained in a CASS. In other words, it provides the user with the ability to
control the level of abstraction – essentially binding or unbinding global variable names as needed. If
all code similarity analysis will be performed against the same software program, retaining global
variable names may help elicit deeper semantic meaning. If not, unbinding global variable names
may improve semantic meaning.

(D) Global Functions. The global functions conﬁguration serves the dual purpose of (i) controlling
the amount of function-speciﬁc information to featurize and (ii) to explicitly disambiguate between
the usage of global functions and global variables (a feature that is absent in Aroma’s SPT design).

(E) Function I/O Cardinality. The function I/O cardinality conﬁguration aims to abstract the
semantics of certain groups of functions through input and output cardinality (i.e., embedded semantic
information that can be implicitly derived by analyzing the number of input and output parameters of
a function). I/O cardinality values of every function are recorded in GAT, if this option is enabled.

We have found that the speciﬁc context in which code similarity is performed provides an indication
of the optimal CASS conﬁguration. We discuss this in greater detail in Appendix A.1.

3.2 Neural Scoring Algorithm

MISIM’s neural scoring algorithm aims to compute the similarity score of two input programs. The
algorithm consists of two phases. The ﬁrst phase involves a neural network model that maps a
featurized CASS to a real-valued code vector. The second phase generates a similarity score between
a pair of code vectors using a similarity metric.2 We describe the details of the scoring model, its
training strategy, and other neural network model choices in this section.

3.2.1 Model

We investigated three neural network approaches for MISIM’s scoring algorithm: (i) a graph neural
network (GNN), (ii) a recurrent neural network (RNN), and (iii) a bag of manual features (BoF)
neural network. We name these models MISIM-GNN, MISIM-RNN, and MISIM-BoF respectively.
The graphical nature of CASS, as well as the recent success in applying GNNs in the program
domain Allamanis et al. [2018b], Brockschmidt et al. [2019], Dinella et al. [2020], Wei et al. [2020],

1Analytically deriving the optimal selection of node preﬁx labels across all C/C++ code may be untenable.

To accommodate this, we currently provide two levels of granularity for C/C++ node preﬁx labels in CASS.

2For this work, we have chosen cosine similarity as the similarity metric used within MISIM.

6

leads us to design the MISIM-GNN model that directly encodes the graphical structure of CASS.
MISIM-RNN is based on Hu et al. [2018], which serializes a CASS into a sequence and uses an RNN
to encode the structure.3 Unlike the two aforementioned models, MISIM-BoF takes in not the CASS
but a bag of manual features extracted from it, and uses a feed-forward network to encode them into a
vector. We compared the three models in our experiments and observed that MISIM-GNN performed
the best overall. Therefore, we describe it in detail in this section. Appendix B has details of the
MISIM-RNN and MISIM-BoF models.

MISIM-GNN. In the MISIM-GNN model, an input program’s CASS representation is transformed
into a graph. Then, each node in the graph is embedded into a trainable vector, serving as the node’s
initial state. Next, a GNN is used to update each node’s state iteratively. Finally, a global readout
function is applied to extract a vector representation of the entire graph from the ﬁnal states of the
nodes. We describe each of these steps in more detail below.

Input Graph Construction. We represent each program as a single CASS instance. Each instance
can contain one or more CASS trees, where each tree corresponds to a unique function of the program.
The CASS instance is converted into a single graph representation to serve as the input to the model.
The graph is constructed by ﬁrst transforming each CASS tree and its GAT entry into an individual
graph. These graphs are then merged into a single (disjoint) graph. For a CASS consisting of a CASS
tree T = (V, E) and a GAT entry a, we transform it into a directed graph G = (V (cid:48), E(cid:48), R), where
V (cid:48) is the set of graph nodes, R is the set of edge types, and E(cid:48) = {(v, u, r) | v, u ∈ V (cid:48), r ∈ R} is
the set of graph edges. The graph is constructed as follows:

V (cid:48) = V ∪ {a}, R = {p, c}, E(cid:48) = {(v, u, p) | (v, u) ∈ E} ∪ {(v, u, c) | (u, v) ∈ E}.

The two edge types, p and c, represent edges from CASS tree nodes to their parent and children
nodes, respectively.
Graph Neural Network. MISIM embeds each node v ∈ V (cid:48) in the input graph G into a vector by
assigning a trainable vector to each unique node label (with the optional preﬁx) and GAT attribute.
The node embeddings are then used as node initial states (h(0)
v ) by a relational graph convolutional
network (R-GCN [Schlichtkrull et al., 2018]) speciﬁed as the following:

(cid:32)

h(l)

v = ReLU

1
r∈R |N r
v |

(cid:80)

(cid:88)

(cid:88)

r∈R

u∈N r
v

W(l)

r h(l−1)
u

+ W(l)

0 h(l−1)

v

(cid:33)

v ∈ V (cid:48), l ∈ [1, L],

where L is the number of GNN layers, N r
connect to v through an edge of type r ∈ R, and W(l)
Code Vector Generation. To obtain a code vector c that represents the entire input graph, we apply
a graph-level readout function as speciﬁed below:

v = {u | (u, v, r) ∈ E(cid:48)} is the set of neighbors of v that

0 are weight matrices to be learned.

r , W(l)

(cid:16)(cid:104)

c = FC

AvgPool

(cid:16)(cid:110)

h(L)
v

(cid:12)
(cid:12)

(cid:12) v ∈ V (cid:48)(cid:111)(cid:17)

; MaxPool

(cid:16)(cid:110)

h(L)
v

(cid:12)
(cid:12)

(cid:12) v ∈ V (cid:48)(cid:111)(cid:17)(cid:105)(cid:17)

The output vectors of average pooling and max pooling on the nodes’ ﬁnal states are concatenated
and fed into a fully-connected layer, yielding the code vector for the entire input program.

3.2.2 Training

We train the neural network model following the setting of metric learning [Schroff et al., 2015,
Hermans et al., 2017, Musgrave et al., 2020, Sun et al., 2020], which tries to map input data to a
vector space where, under a distance (or similarity) metric, similar data points are close together (or
have large similarity scores) and dissimilar data points are far apart. The metric we use is the cosine
similarity in the code vector space. As shown in the lower half of Figure 2, we use pair-wise labels to
train the model. Each pair of input programs are mapped to two code vectors by the model, from
which a similarity score is computed and optimized using a metric learning loss function.

3We did not include a Transformer-based model using the serialized CASS because in our experiments, we
had observed consistently worse performance from Transformers than that from RNNs when applying them to
source code token sequences.

7

(a) MAP@R on GCJ.

(b) AP on GCJ.

(c) MAP@R on POJ-104.

(d) AP on POJ-104.

Figure 3: Summarized accuracy results on the test sets for code2vec, code2seq, NCC, Aroma, token
sequence models, and MISIM (avg over 3 runs; min/max values for error bars.)

(a) AST vs SPT vs. CASS (2-1-3-1-1)

(b) SPT vs. SPT + neural backends

Figure 4: Figures showing effects of CASS conﬁgurability and neural backend separately on POJ-104.

4 Experimental Evaluation

In this section, we analyze the performance of MISIM compared to code2vec, code2seq, NCC, and
Aroma on two datasets containing a total of more than 328,000 programs.4 We also compare it against
our own hand-tuned and best performing recurrent neural network (Seq-RNN) and Transformer
(Seq-Transformer), which take tokenized source code as input (see Appendix C.3 for details). Overall,
we ﬁnd that MISIM has greater accuracy than these systems across two metrics. We also perform an
abbreviated analysis of two MISIM variants, each trained with a different CASS conﬁguration, to
provide insight into when different conﬁgurations may be better ﬁt for different code corpora.

Datasets. Our experiments are conducted on two datasets: the Google Code Jam (GCJ) dataset [Ul-
lah et al., 2019] and the POJ-104 dataset [Mou et al., 2016]. The GCJ dataset consists of so-
lutions to programming problems in Google’s Code Jam coding competitions. We use a subset
of it that consisting of C/C++ programs that solve 297 problems. The POJ-104 dataset consists
of student-written C/C++ programs solving 104 problems. For both datasets, we label two pro-
grams as similar if they are solutions to the same problem. After a ﬁltering step, which removes
unparsable/non-compilable programs, we split each dataset by problem into three subsets for train-
ing, validation, and testing. Detailed statistics of the dataset partitioning are shown in Table 2.

Table 2: Dataset statistics.

Training. Unless otherwise speciﬁed, we use the same
training procedure in all experiments. The models are
built and trained using PyTorch [Paszke et al., 2019].
To train the models, we use the Circle loss [Sun et al.,
2020], a state-of-the-art metric learning loss function
that has been tested effective in various similarity learn-
ing tasks. Following the P-K sampling strategy [Her-
mans et al., 2017], we construct a batch of programs
by ﬁrst randomly sampling 16 different problems, and then randomly sampling at most 5 different
solutions for each problem. The loss function takes the similarity scores of all intra-batch pairs and
their pair-wise labels as input. Further details about the training procedure are in Appendix C.1.

#Problems #Programs #Problems #Programs

Training
Validation
Test

223,171
36,409
22,795

28,137
7,193
10,450

237
29
31

POJ-104

64
16
24

282,375

45,780

Total

Split

GCJ

104

297

Evaluation Metrics. The accuracy metrics we use for evaluation are Mean Average Precision at
R (MAP@R) [Musgrave et al., 2020] and Average Precision (AP) [Baeza-Yates and Ribeiro-Neto,
1999]. Since these metrics are already deﬁned, we do not detail them here (Details in Appendix C.4.)

Conﬁguration Identiﬁer. In the following sections, we refer to a conﬁguration of CASS by its unique
identiﬁer (ID). A conﬁguration ID is formatted as A-B-C-D-E. Each of the ﬁve letters corresponds

4Although other code similarity systems exist, we were not able to compare to them due to the differences in

target languages, problem settings, and lack of open-source availability.

8

code2veccode2seqNCCNCC-w/o-inst2vecAroma-DotAroma-CosSeq-RNNSeq-TransformerMISIM-GNNMISIM-RNNMISIM-BoF020406080100MAP@R (%)code2veccode2seqNCCNCC-w/o-inst2vecAroma-DotAroma-CosSeq-RNNSeq-TransformerMISIM-GNNMISIM-RNNMISIM-BoF020406080100AP (%)code2veccode2seqNCCNCC-w/o-inst2vecAroma-DotAroma-CosSeq-RNNSeq-TransformerMISIM-GNNMISIM-RNNMISIM-BoF020406080100MAP@R (%)code2veccode2seqNCCNCC-w/o-inst2vecAroma-DotAroma-CosSeq-RNNSeq-TransformerMISIM-GNNMISIM-RNNMISIM-BoF020406080100AP (%)AST-DotSPT-DotCASS-DotAST-CosSPT-CosCASS-Cos050100MAP@R (%)45.1252.0755.5947.3955.0860.78AST-DotSPT-DotCASS-DotAST-CosSPT-CosCASS-Cos050100AP (%)35.9845.9448.3145.3155.3360.42SPT-DotSPT-CosSPT-GNNSPT-RNNSPT-BoF050100MAP@R (%)52.0755.0882.4574.0174.38SPT-DotSPT-CosSPT-GNNSPT-RNNSPT-BoF050100AP (%)45.9455.3382.0081.6482.95to a conﬁguration type in the second column of Table 1, and will be replaced by an option number
speciﬁed in the third column of the table. Conﬁguration 0-0-0-0-0 corresponds to Aroma’s SPT.

4.1 End-to-End Results

Figure 3 shows the accuracy of code similarity systems compared in our experiments.5 The blue
bars show the results of the MISIM system variants trained using CASS conﬁguration 2-1-3-1-1.
The orange bars show the results of code2vec, code2seq, NCC, Aroma, and the two token sequence
models. We observe that MISIM-GNN results in the best performance for MAP@R, yielding 1.08×
to 43.89× improvements over the other systems. In summary, MISIM system performs better than
other systems on both the metrics.

Additional Results. Table 3 shows additional experimental
results of Aroma and MISIM gathered by the authors of Co-
deNet [Puri et al., 2021]. In their experiment, they evaluated
Aroma and MISIM on code semantics similarity problem
using their proposed C++1000 and C++1400 datasets. These
two datasets are similar to the POJ-104 dataset. The C++1000 dataset consists of 1000 classes and
500,000 programs, and the C++1400 dataset consists of 1400 classes and 420,000 programs.6

Table 3: Similarity MAP@R score from
CodeNet (credit: [Puri et al., 2021]).
C++1400
C++1000
0.15
0.17
0.75
0.75

Aroma
MISIM

4.2 Specialized Experiment: Ablation Study

To understand the effectiveness of CASS’s conﬁgurability and neural backends separately, we
performed two additional experiments. In the ﬁrst experiment, we compared AST and Aroma’s
SPT representation (CASS conﬁg 0-0-0-0-0) with CASS’s 2-1-3-1-1 conﬁguration and excluded
neural backends. We replaced neural backends with dot product and cosine similarity, and obtained
MAP@R and AP scores on POJ-104 dataset. Figure 4a shows the results. To summarize, CASS’s
conﬁguration delivers 3-5% better accuracy over SPT and 10-13% better accuracy over AST. In
the second experiment, we added MISIM’s three neural backends to Aroma (Figure 4b) and found
that Aroma’s accuracy improves by 22-27%. In essence, both CASS’s conﬁgurability and the neural
backends independently help in improving MISIM’s accuracy over existing systems.

4.3 Specialized Experiment: CASS Conﬁgurations

We provide early an abbreviated investigation indicating that no CASS conﬁguration is invariably best
for all code. We ran experiments that train MISIM-GNN models with two CASS conﬁgurations (C 1:
0-0-0-0-0 and C 2: 2-2-3-2-1) on randomly sampled sub-training sets and compared their accuracy.
Table 4 shows the results from four selected sub-training sets, named T A, T B, T C, and T D from
POJ-104. When trained on T A or T B, the system using conﬁguration C 2 performs better than that
using C 1 in both the accuracy metrics. However, for T C or T D, the results are inverted.

Subset

Conﬁg

MAP@R (%)

Table 4: Test accuracy (avg, min, max) of MISIM-
GNN trained on different training subsets

We compared the semantics features of T A∩T B
to T C ∩T D. We observed CASS-deﬁned seman-
tically salient features (e.g., global variables)
that C 2 had been customized to extract, oc-
curred less frequently in T A ∩ T B. For the
POJ-104 dataset, when global variables are used
more frequently, they are more likely to have
consistent meaning across different programs.
Abstracting them away as C2 does for T C, T D,
leads to a loss in semantic information salient
to code similarity. Conversely, when global
variables are not frequently used, there is an
increased likelihood that the semantics they ex-
tract are program-speciﬁc. Retaining their names in a CASS, may increase syntactic noise, reducing
model performance. C 2 eliminates them for T A, T B, and has improved accuracy.

72.47 (-0.95/+1.24)
69.83 (-1.03/+1.60)

66.86 (-2.31/+2.81)
63.86 (-3.06/+3.43)

63.53 (-1.08/+1.53)
61.23 (-2.04/+1.57)

61.78 (-0.46/+0.47)
60.86 (-1.59/+0.90)

68.58 (-2.51/+2.85)
69.86 (-3.34/+1.79)

76.39 (-1.68/+1.51)
79.89 (-1.20/+0.71)

69.78 (-0.42/+0.21)
71.99 (-0.26/+0.45)

63.45 (-1.58/+1.92)
67.40 (-1.85/+1.23)

C1
C2

C1
C2

C1
C2

C1
C2

AP (%)

T D

T B

T A

T C

5A table for these results can be found in Appendix C.5.
6IBM/MIT’s CodeNet was publicly available a week prior to the NeurIPS submissions. We only include their
results on MISIM and Aroma. If accepted, we will perform a comprehensive study for camera-ready submission.

9

5 Related Work

There is a body of work on code comprehension not directly intended for code semantics similarity,
but may provide value to it. Some researchers have studied applying machine learning to learn from
the AST for completing various tasks on code [Alon et al., 2019a, Chen et al., 2018, Hu et al., 2018, Li
et al., 2018, Mou et al., 2016]. Odena and Sutton [2020] and Odena et al. [2021] represent a program
as property signatures inferred from input-output pairs, which may be used to improve program
synthesizers, amongst other things. There has also been work exploring graph representations. For
bug detection and code generation, Allamanis et al. [2018b] and Brockschmidt et al. [2019] represent
a program as a graph with a backbone AST and additional edges representing lexical ordering
and semantic relations between the nodes. Dinella et al. [2020] also use an AST-backboned graph
representation of programs to learn bug ﬁxing through graph transformation. Hellendoorn et al.
[2020] introduce a simpliﬁed graph containing only AST leaf nodes for program repair. Wei et al.
[2020] extract type dependency graphs from JavaScript programs for probabilistic type inference.

6 Conclusion

This paper presented MISIM, a code semantics similarity system. MISIM has two core novelties.
First, it uses the context-aware semantics structure (CASS) designed to lift semantic meaning from
code syntax. Second, it provides a neural-based code semantics similarity scoring algorithm for
learning semantics similarity scoring using CASS. Puri et al. [2021] and our experimental evaluation
showed that MISIM outperforms four state-of-the-art code semantics similarity systems and two
hand-optimized models. We also provided an anecdotal analysis illustrating that there may not be
one universally optimal CASS conﬁguration. An open research question for MISIM is in how to
automatically derive the proper conﬁguration of its various components for a given code corpus,
speciﬁcally the CASS and neural scoring algorithms, which we plan to explore in future work.

References

Maaz Bin Safeer Ahmad, Jonathan Ragan-Kelley, Alvin Cheung, and Shoaib Kamil. Automatically
Translating Image Processing Libraries to Halide. ACM Trans. Graph., 38(6), November 2019.
ISSN 0730-0301. doi: 10.1145/3355089.3356549.

Mejbah Alam, Justin Gottschlich, Nesime Tatbul, Javier S Turek, Tim Mattson, and Abdullah
Muzahid. A Zero-Positive Learning Approach for Diagnosing Software Performance Regressions.
In H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlchBuc, E. Fox, and R. Garnett, editors,
Advances in Neural Information Processing Systems 32, NeurIPS 2019, pages 11623–11635.
Curran Associates, Inc., 2019.

Miltiadis Allamanis, Earl T. Barr, Premkumar Devanbu, and Charles Sutton. A Survey of Machine
Learning for Big Code and Naturalness. ACM Computing Surveys, 51(4), September 2018a.

Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. Learning to Represent Programs

with Graphs. In International Conference on Learning Representations, 2018b.

Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. A General Path-Based Representation
for Predicting Program Properties. In Proceedings of the 39th ACM SIGPLAN Conference on
Programming Language Design and Implementation, PLDI 2018, page 404–419, New York, NY,
USA, 2018. Association for Computing Machinery. ISBN 9781450356985. doi: 10.1145/3192366.
3192412.

Uri Alon, Omer Levy, and Eran Yahav. code2seq: Generating Sequences from Structured Represen-

tations of Code. In International Conference on Learning Representations, 2019a.

Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. code2vec: Learning Distributed Rep-
resentations of Code. Proc. ACM Program. Lang., 3(POPL):40:1–40:29, January 2019b. ISSN
2475-1421. doi: 10.1145/3290353.

Johannes Bader, Andrew Scott, Michael Pradel, and Satish Chandra. Getaﬁx: Learning to Fix Bugs
Automatically. Proc. ACM Program. Lang., 3(OOPSLA), October 2019. doi: 10.1145/3360585.

10

Ricardo A. Baeza-Yates and Berthier Ribeiro-Neto. Modern Information Retrieval. Addison-Wesley

Longman Publishing Co., Inc., USA, 1999. ISBN 020139829X.

Shaon Barman, Sarah Chasins, Rastislav Bodik, and Sumit Gulwani. Ringer: Web Automation
ISSN 0362-1340. doi:

by Demonstration. SIGPLAN Not., 51(10):748–764, October 2016.
10.1145/3022671.2984020.

Gaurav Batra, Zach Jacobson, Siddarth Madhav, Andrea Queirolo, and Nick Santhanam. Artiﬁcial-

Intelligence Hardware: New Opportunities for Semiconductor Companies, 2018.

I. D. Baxter, A. Yahin, L. Moura, M. Sant’Anna, and L. Bier. Clone detection using abstract syntax
trees. In Proceedings. International Conference on Software Maintenance (Cat. No. 98CB36272),
pages 368–377, 1998.

Kory Becker and Justin Gottschlich. AI Programmer: Autonomously Creating Software Programs

Using Genetic Algorithms. CoRR, abs/1709.05703, 2017.

Tal Ben-Nun, Alice Shoshana Jakobovits, and Torsten Hoeﬂer. Neural Code Comprehension: A
Learnable Representation of Code Semantics. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31,
pages 3585–3597. Curran Associates, Inc., 2018.

Sahil Bhatia, Pushmeet Kohli, and Rishabh Singh. Neuro-Symbolic Program Corrector for Introduc-
tory Programming Assignments. In Proceedings of the 40th International Conference on Software
Engineering, ICSE ’18, page 60–70, New York, NY, USA, 2018. Association for Computing
Machinery. ISBN 9781450356381. doi: 10.1145/3180155.3180219.

Paul Bogdan, Fan Chen, Aryan Deshwal, Janardhan Rao Doppa, Biresh Kumar Joardar, Hai (Helen)
Li, Shahin Nazarian, Linghao Song, and Yao Xiao. Taming Extreme Heterogeneity via Machine
Learning Based Design of Autonomous Manycore Systems. In Proceedings of the International
Conference on Hardware/Software Codesign and System Synthesis Companion, CODES/ISSS ’19,
New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450369237. doi:
10.1145/3349567.3357376.

Marc Brockschmidt, Miltiadis Allamanis, Alexander L. Gaunt, and Oleksandr Polozov. Generative
Code Modeling with Graphs. In International Conference on Learning Representations, 2019.

Xinyun Chen, Chang Liu, and Dawn Song. Tree-to-tree Neural Networks for Program Translation.
In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors,
Advances in Neural Information Processing Systems 31, pages 2547–2557. Curran Associates, Inc.,
2018.

Yiran Chen, Yuan Xie, Linghao Song, Fan Chen, and Tianqi Tang. A Survey of Accelerator
Architectures for Deep Neural Networks. Engineering, 6(3):264 – 274, 2020. ISSN 2095-8099.
doi: https://doi.org/10.1016/j.eng.2020.01.007.

Kyunghyun Cho, Bart van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. Learning Phrase Representations using RNN Encoder–
Decoder for Statistical Machine Translation. In Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing (EMNLP), pages 1724–1734, Doha, Qatar, October
2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1179.

Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to

Algorithms, Third Edition. The MIT Press, 3rd edition, 2009. ISBN 0262033844.

V. Cosentino, J. L. Cánovas Izquierdo, and J. Cabot. A Systematic Mapping Study of Software
Development With GitHub. IEEE Access, 5:7173–7192, 2017. ISSN 2169-3536. doi: 10.1109/
ACCESS.2017.2682323.

S. Deng, H. Zhao, W. Fang, J. Yin, S. Dustdar, and A. Y. Zomaya. Edge Intelligence: The Conﬂuence
of Edge Computing and Artiﬁcial Intelligence. IEEE Internet of Things Journal, 7(8):7457–7469,
2020.

11

Elizabeth Dinella, Hanjun Dai, Ziyang Li, Mayur Naik, Le Song, and Ke Wang. Hoppity: Learning
Graph Transformations to Detect and Fix Bugs in Programs. In International Conference on
Learning Representations, 2020.

D. Feitelson, A. Mizrahi, N. Noy, A. Ben Shabat, O. Eliyahu, and R. Sheffer. How Developers
Choose Names. IEEE Transactions on Software Engineering, pages 1–1, 2020. ISSN 2326-3881.
doi: 10.1109/TSE.2020.2976920.

Hal Finkel and Ignacio Laguna. Program Synthesis for Scientiﬁc Computing, August 2020.

David Flanagan. JavaScript: The Deﬁnitive Guide. "O’Reilly Media, Inc.", 2006.

Luciano Floridi. Artiﬁcial Intelligence, Deepfakes and a Future of Ectypes. Philosophy & Technology,

31, 08 2018. doi: 10.1007/s13347-018-0325-3.

Edward M Gellenbeck and Curtis R Cook. An Investigation of Procedure and Variable Names as
Beacons During Program Comprehension. In Empirical studies of programmers: Fourth workshop,
pages 65–81. Ablex Publishing, Norwood, NJ, 1991.

Justin Gottschlich, Armando Solar-Lezama, Nesime Tatbul, Michael Carbin, Martin Rinard, Regina
Barzilay, Saman Amarasinghe, Joshua B. Tenenbaum, and Tim Mattson. The Three Pillars of
Machine Programming. In Proceedings of the 2nd ACM SIGPLAN International Workshop on
Machine Learning and Programming Languages, MAPL 2018, pages 69–80, New York, NY, USA,
2018. ACM. ISBN 978-1-4503-5834-7. doi: 10.1145/3211346.3211355.

Eileen Hannigan, Ondrej Burkacky, Peter Kenevan, Abhijit Mahindroo, Richard Johnson, Jonathon
Rivait, Heather Byer, Venetia Simcock, Elizabeth Brown, Roger Draper, Gwyn Herbein, Pamela
Norton, Katya Petriwsky, Charmaine Rice, John C. Sanchez, Dana Sand, Sneha Vats, Pooja
Yadav, Belinda Yu, Lucia Rahilly, Michael T. Borruso, Bill Javetski, Mark Staples, and Leff
Communications. McKinsey on Semiconductors, 2019.

Vincent J. Hellendoorn, Charles Sutton, Rishabh Singh, Petros Maniatis, and David Bieber. Global
Relational Models of Source Code. In International Conference on Learning Representations,
2020.

Alexander Hermans, Lucas Beyer, and Bastian Leibe. In Defense of the Triplet Loss for Person

Re-Identiﬁcation. CoRR, abs/1703.07737, 2017.

Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. Deep Code Comment Generation. In Proceedings
of the 26th Conference on Program Comprehension, ICPC ’18, page 200–210, New York, NY,
USA, 2018. Association for Computing Machinery. ISBN 9781450357142. doi: 10.1145/3196321.
3196334.

Roshni Iyer, Yizhou Sun, Wei Wang, and Justin Gottschlich. Software language comprehension
using a program-derived semantics graph. In NeurIPS 2020 Workshop on Computer-Assisted
Programming, 2020.

Shoaib Kamil, Alvin Cheung, Shachar Itzhaky, and Armando Solar-Lezama. Veriﬁed Lifting of
Stencil Computations. In Proceedings of the 37th ACM SIGPLAN Conference on Programming
Language Design and Implementation, PLDI ’16, pages 711–726, New York, NY, USA, 2016.
ACM. ISBN 978-1-4503-4261-2. doi: 10.1145/2908080.2908117.

Celine Lee, Justin Gottschlich, and Dan Roth. Toward Code Generation: A Survey and Lessons from

Semantic Parsing, 2021.

Jian Li, Yue Wang, Michael R. Lyu, and Irwin King. Code Completion with Neural Attention
and Pointer Networks. In Proceedings of the 27th International Joint Conference on Artiﬁcial
Intelligence, IJCAI’18, page 4159–25. AAAI Press, 2018. ISBN 9780999241127.

Liuqing Li, He Feng, Wenjie Zhuang, N. Meng, and B. Ryder. Cclearner: A deep learning-based
clone detection approach. 2017 IEEE International Conference on Software Maintenance and
Evolution (ICSME), pages 249–260, 2017.

12

Tie-Yan Liu. Learning to Rank for Information Retrieval. Found. Trends Inf. Retr., 3(3):225–331,

March 2009. ISSN 1554-0669. doi: 10.1561/1500000016.

Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization.

In International

Conference on Learning Representations, 2019.

Sifei Luan, Di Yang, Celeste Barnaby, Koushik Sen, and Satish Chandra. Aroma: Code Recom-
mendation via Structural Code Search. Proc. ACM Program. Lang., 3(OOPSLA):152:1–152:28,
October 2019. ISSN 2475-1421. doi: 10.1145/3360578.

Tomas Mikolov, Kai Chen, Greg S. Corrado, and Jeffrey Dean. Efﬁcient estimation of word

representations in vector space, 2013a.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed representa-
tions of words and phrases and their compositionality. In Proceedings of the 26th International
Conference on Neural Information Processing Systems - Volume 2, NIPS’13, page 3111–3119,
Red Hook, NY, USA, 2013b. Curran Associates Inc.

Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin. Convolutional Neural Networks over Tree
Structures for Programming Language Processing. In Proceedings of the Thirtieth AAAI Conference
on Artiﬁcial Intelligence, AAAI ’16, page 1287–1293. AAAI Press, 2016.

Kevin Musgrave, Serge Belongie, and Ser-Nam Lim. A Metric Learning Reality Check, 2020.

Augustus Odena and Charles Sutton. Learning to Represent Programs with Property Signatures. In

International Conference on Learning Representations, 2020.

Augustus Odena, Kensen Shi, David Bieber, Rishabh Singh, Charles Sutton, and Hanjun Dai.
{BUSTLE}: Bottom-up program synthesis through learning-guided exploration. In International
Conference on Learning Representations, 2021.

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imperative Style, High-Performance
Deep Learning Library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox,
and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8024–8035.
Curran Associates, Inc., 2019.

Michael Pradel and Koushik Sen. DeepBugs: A Learning Approach to Name-Based Bug Detection.

Proc. ACM Program. Lang., 2(OOPSLA), October 2018. doi: 10.1145/3276517.

Ruchir Puri, David S. Kung, Geert Janssen, Wei Zhang, Giacomo Domeniconi, Vladmir Zolotov,
Julian Dolby, Jie Chen, Mihir Choudhury, Lindsey Decker, Veronika Thost, Luca Buratti, Saurabh
Pujar, and Ulrich Finkler. Project codenet: A large-scale ai for code dataset for learning a diversity
of coding tasks. 2021.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language

Models are Unsupervised Multitask Learners, 2019.

Michael Sejr Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max
Welling. Modeling relational data with graph convolutional networks. In Aldo Gangemi, Roberto
Navigli, Maria-Esther Vidal, Pascal Hitzler, Raphaël Troncy, Laura Hollink, Anna Tordai, and
Mehwish Alam, editors, The Semantic Web - 15th International Conference, ESWC 2018, Herak-
lion, Crete, Greece, June 3-7, 2018, Proceedings, volume 10843 of Lecture Notes in Computer
Science, pages 593–607. Springer, 2018. doi: 10.1007/978-3-319-93417-4\_38.

Florian Schroff, Dmitry Kalenichenko, and James Philbin. FaceNet: A Uniﬁed Embedding for Face
Recognition and Clustering. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), Jun 2015.

Yifan Sun, Changmao Cheng, Yuhan Zhang, Chi Zhang, Liang Zheng, Zhongdao Wang, and Yichen
Wei. Circle Loss: A Uniﬁed Perspective of Pair Similarity Optimization. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2020.

13

Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin White, and Denys
Poshyvanyk. Deep Learning Similarities from Different Representations of Source Code. In
Proceedings of the 15th International Conference on Mining Software Repositories, MSR ’18,
page 542–553, New York, NY, USA, 2018. Association for Computing Machinery.
ISBN
9781450357166. doi: 10.1145/3196398.3196431.

F. Ullah, H. Naeem, S. Jabbar, S. Khalid, M. A. Latif, F. Al-turjman, and L. Mostarda. Cyber
Security Threats Detection in Internet of Things Using Deep Learning Approach. IEEE Access, 7:
124379–124389, 2019.

Guido Van Rossum and Fred L. Drake. Python 3 Reference Manual. CreateSpace, Scotts Valley, CA,

2009. ISBN 1441412697.

Huihui Wei and Ming Li. Supervised deep features for software functional clone detection by
exploiting lexical and syntactical information in source code. In Proceedings of the Twenty-Sixth
International Joint Conference on Artiﬁcial Intelligence, IJCAI-17, pages 3034–3040, 2017. doi:
10.24963/ijcai.2017/423.

Jiayi Wei, Maruth Goyal, Greg Durrett, and Isil Dillig. LambdaNet: Probabilistic Type Inference
using Graph Neural Networks. In International Conference on Learning Representations, 2020.

W. Wulf and Mary Shaw. Global Variable Considered Harmful. SIGPLAN Not., 8(2):28–34, February

1973. ISSN 0362-1340. doi: 10.1145/953353.953355.

Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, Kaixuan Wang, and Xudong Liu. A Novel
Neural Source Code Representation Based on Abstract Syntax Tree. In Proceedings of the 41st
International Conference on Software Engineering, ICSE ’19, page 783–794. IEEE Press, 2019.
doi: 10.1109/ICSE.2019.00086.

Gang Zhao and Jeff Huang. DeepSim: Deep Learning Code Functional Similarity. In Proceedings of
the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering, ESEC/FSE 2018, page 141–151, New York, NY,
USA, 2018. Association for Computing Machinery. ISBN 9781450355735. doi: 10.1145/3236024.
3236068.

14

A Context-Aware Semantics Structure Details

The following is the formal deﬁnition of CASS.
Deﬁnition 1 (Context-aware semantics structure (CASS)). A CASS consists of one or more CASS
trees and an optional global attributes table (GAT). A CASS tree, T , is a collection of nodes,
V = {v1, v2, . . . , v|V |}, and edges, E = {e1, e2, . . . , e|E|}, denoted as T = (V, E). Each edge is
directed from a parent node, vp to a child node, vc, or ek = (vp, vc) where ek ∈ E and vp, vc ∈ V .
The root node, vr, of the tree signiﬁes the beginning of the code snippet and has no parent node, i.e.,
(cid:64)vp, (vp, vr) ∈ E. A child node is either an internal node or a leaf node. An internal node has at
least one child node while a leaf node has no child nodes. A CASS tree can be empty, in which it
has no nodes. The CASS GAT contains exactly one entry per unique function deﬁnition in the code
snippet. A GAT entry includes the input and output cardinality values for the corresponding function.
Deﬁnition 2 (Node labels). Every CASS node has an associated label, lv. During the construction of
a CASS tree, the program tokens at each node, tv are mapped to its corresponding label or lv = f (tv).
This is depicted with an expression grammar for node labels and the function mapping tokens to
labels below.7

(cid:104)bin-op(cid:105)
(cid:104)unary-op(cid:105)
(cid:104)leaf-node-label(cid:105)
(cid:104)exp(cid:105)
(cid:104)internal-node-label(cid:105)

::= ‘+’ | ‘-’ | ‘*’ | ‘/’ | ...
::= ‘++’ | ‘--’ | ...
::= LITERAL | IDENT | ‘#VAR’ | ‘#GVAR’ | ‘#EXFUNC’ | ‘#LIT’ | ...
::= ‘$’ | ‘$’ (cid:104)bin-op(cid:105) ‘$’ | (cid:104)unary-op(cid:105) ‘$’ | ...
::= ‘for’ ‘(’ (cid:104)exp(cid:105) ‘;’ (cid:104)exp(cid:105) ‘;’ (cid:104)exp(cid:105) ‘)’ (cid:104)exp(cid:105) ‘;’
|
|
|
|

‘int’ (cid:104)exp(cid:105) ‘;’
‘return’ (cid:104)exp(cid:105) ‘;’
(cid:104)exp(cid:105)
...

lv = f (tv) =

(cid:26)(cid:104)leaf-node-label(cid:105)

if v is a leaf node

(cid:104)internal-node-label(cid:105) otherwise

Deﬁnition 3 (Node preﬁx label). A node preﬁx label is a string preﬁxed to a node label. A node
preﬁx label may or may not be present.

A.1 Discussion

We believe there is no silver bullet solution for code similarity for all programs and programming
languages. Based on this belief, a key intuition of CASS’s design is to provide a structure that
is semantically rich based on structure, with inspiration from Aroma’s SPT, while simultaneously
providing a range of customizable parameters to accommodate a wide variety of scenarios. CASS’s
language-agnostic and language-speciﬁc conﬁgurations and their associated options serve for explo-
ration of a series of tree variants, each differing in their granularity of detail of abstractions.

For instance, the compound statements conﬁguration provides three levels of abstraction. Option 0
is Aroma’s baseline conﬁguration and is the ﬁnest level of abstraction, as it featurizes the number
of constituents in a compound statement node. Option 2 reduces compound statements to a single
token and represents a slightly higher level of abstraction. Option 1 eliminates all features related to
compound statements and is the coarsest level of abstraction. The same trend applies to the global
variables and global functions conﬁgurations. It is our belief, based on early evidence, that the
appropriate level of abstraction in CASS is likely based on many factors such as (i) code similarity
purpose, (ii) programming language expressiveness, and (iii) application domain.

Aroma’s original SPT seems to work well for a common code base where global variables have
consistent semantics and global functions are standard API calls also with consistent semantics (e.g.,
a single code-base). However, for cases outside of such spaces, some question about applicability
arise. For example, assumptions about consistent semantics for global variables and functions may
not hold in cases of non-common code-bases or non-standardized global function names [Wulf and

7Note: the expression grammar we provide is non-exhaustive due to space limitations. The complete set of

standard C/C++ tokens or binary and unary operators is collectively denoted in shorthand as ‘...’.

15

Shaw, 1973, Gellenbeck and Cook, 1991, Feitelson et al., 2020]. Having the capability to differentiate
between these cases, and others, is a key motivation for CASS.

We do not believe that CASS’s current structure is exhaustive. With this in mind, we have designed
CASS to be extensible, enabling a seamless mechanism to add new conﬁgurations and options.
Our intention with this paper is to present initial ﬁndings in exploring CASS’s structure. Based on
our early experimental analysis, presented in Section C.7, CASS seems to be a promising research
direction for code similarity.

An Important Weakness. While CASS provides added ﬂexibility over SPT, such ﬂexibility may
be misused. With CASS, system developers are free to add or remove as much syntactic differ-
entiation detail they choose for a given language or given code body. Such overspeciﬁcation (or
underspeciﬁcation), may result in syntactic overload (or underload) which may cause reduced code
similarity accuracy over the original SPT design, as we illustrate in Section C.7.

B MISIM Models

In this section, we describe the models evaluated in our experiments other than MISIM-GNN, and
discuss the details of the experimental procedure.

B.1 MISIM-BoF

The MISIM-BoF model takes a set of manual features extracted from a CASS as its input. The features
include the ones extracted from CASS trees, using the same procedure described in Aroma [Luan
et al., 2019], as well as the entries in CASS GATs. The MISIM-BoF model is speciﬁed as below:

c = FC (AvgPool ({ex | x ∈ S})) ,

where S is the feature set of the input program and ex is the embedding vector of feature x. The
output code vector is computed by performing average pooling on the feature embeddings and
projecting its result into the code vector space with a fully connected layer.

B.2 MISIM-RNN

The input to the MISIM-RNN model is a serialized representation of a CASS. Each CASS tree,
representing a function in the program, is converted to a sequence using the technique proposed
in [Hu et al., 2018]. The GAT entry associated with a CASS tree is both prepended and appended to
the tree’s sequence, forming the sequence of the corresponding function. The model architecture can
be expressed as:

hf = biGRU (¯ef ) ,
c = FC ([AvgPool ({hf | f ∈ F }) ; MaxPool ({hf | f ∈ F })]) ,

where F is the set of functions in the input program and ¯ef is the sequence of embedding vectors
for the serialized CASS of function f . Each function’s sequence ﬁrst has its tokens embedded, and
then gets summarized to a function-level vector by a bidirectional GRU layer [Cho et al., 2014].
The code vector for the entire program is subsequently computed by performing average and max
pooling on the function-level vectors, concatenating the resulting vectors, and passing it through a
fully connected layer.

C Experimental Details

C.1 Training Procedure and Hyperparameters

We use the AdamW optimizer [Loshchilov and Hutter, 2019] with a learning rate of 10−3 for all
the models except Seq-Transformer, for which a learning rate of 10−4 is used to stabilize training.
The training runs for 100 epochs, each containing 1,000 iterations, and the model that gives the best
validation accuracy is used for testing.8 The hyperparameters used for the Circle loss are γ = 80

8We have observed that the validation accuracy stops to increase before the 100th epoch in all experiments.

16

and m = 0.4. For all of our MISIM models, we use 128-dimensional embedding vectors, hidden
states, and code vectors. We also apply dropout with a probability of 0.5 to the embedding vectors.
To handle rare or unknown tokens, a token that appears less than 5 times in the training set is replaced
with a special UNKNOWN token.

C.2 Modiﬁcations to code2vec, code2seq, NCC, and Aroma

To compare with code2vec, code2seq, NCC, and Aroma, we adapt them to our experimental setting in
the following ways. The original code2vec takes a function as an input, extracts its AST paths to form
the input to its neural network, and trains the network using the function name prediction task. In our
experiments, we feed the AST paths from all function(s) in a program into the neural network and train
it using the metric learning task described in Section 3.2.2. We make similar adaptions to code2seq
by combining AST paths from the whole program as one input sample. Additionally, we replace
the sequence decoder of code2vec with an attention-based path aggregator used in code2vec. NCC
contains a pre-training phase, named inst2vec, on a large code corpus for generating instruction
embeddings, and a subsequent phase that trains an RNN for a downstream task using the pre-trained
embeddings. We train the downstream RNN model on our metric learning task in two ways. The ﬁrst
uses the pre-trained embeddings (labeled as NCC in our results). The second trains the embeddings
from scratch on our task in an end-to-end fashion (labeled as NCC-w/o-inst2vec). For both code2vec
and NCC, we use the same model architectures and embedding/hidden sizes suggested in their papers
and open-sourced implementations. The dimension of their output vectors (i.e., code vectors) is set
to the same as our MISIM models. Aroma extracts manual features from the code and computes
the similarity score of two programs by taking the dot product of their binary feature vectors. We
experiment with both its original scoring mechanism (labeled as Aroma-Dot) and a variant that uses
the cosine similarity (labeled as Aroma-Cos).

C.3 Token Sequence Models

Here we provide details of the two token sequence model (Seq-RNN and Seq-Transformer) used in
our experiments.

The input to both models is tokenized source code. Identiﬁers and non-identiﬁer tokens are embedded
differently. For identiﬁers, we ﬁrst split their text values into subtokens according to camel case
and snake case patterns. Then we embed each subtoken into a trainable vector and generate an
identiﬁer’s embedding by taking the sum of its subtokens’ embedding vectors. For non-identiﬁer
tokens, we assign a trainable embedding vector to each unique token. All embedding vectors are
128-dimensional.

Seq-RNN employs a stacked two-layer bidirectional GRU that operates on token embedding se-
quences. Each direction in a layer has a 128-dimensional hidden state, and a dropout of rate 0.1
is applied between the two layers. The last hidden states in the second layer in both directions are
concatenated and projected into a 128-dimensional code vector using a linear layer. This code vector
is then used to compute cosine similarities between programs.

Seq-Transformer employs a six-layer Transformer encoder with a learned positional embedding.
The dimensionality of the Transformer is 128, and the feed-forward layers’ hidden states are 512
dimensional. A dropout of rate 0.1 is applied inside each Transformer encoder layer. The length of
the input token sequence is capped to 512. A special token is prepended to each sequence, and the
output at the location of this token after the last encoder layer is passed into a linear layer to generate
a 128-dimensional code vector for computing cosine similarities.

C.4 Evaluation Metrics

MAP@R measures how accurately a model can retrieve similar (or relevant) items from a database
given a query. MAP@R rewards a ranking system (e.g., a search engine, a code recommendation
engine, etc.) for correctly ranking relevant items with an order where more relevant items are ranked
higher than less relevant items. It is deﬁned as the mean of average precision scores, each of which is
evaluated for retrieving R most similar samples given a query. In our case, the set of queries is the
set of all test programs. For a program, R is the number of other programs in the same class (i.e., a
POJ-104 problem). MAP@R is applied to both validation and testing. We use AP to measure the

17

performance in a binary classiﬁcation setting, in which the models are viewed as binary classiﬁers
that determine whether a pair of programs are similar by comparing their similarity score with a
threshold. AP is only used for testing. They are computed from the similarity scores of all program
pairs in the test set, as well as their pair-wise labels. For the systems that require training (i.e., systems
with ML learned similarity scoring), we train and evaluate them three times with different random
seeds.

C.5 MISIM Accuracy Results (in tabular form)

Table 5 shows the results of MISIM in comparison to other systems. Same results are presented in
the graphical form in Figure 3.

Table 5: Code similarity system accuracy. Results are shown as the average and min/max values,
relative to the average, over 3 runs. We had to make a few modiﬁcations to adapt code2vec, code2seq,
NCC and Aroma to our experimental settings. Please refer to Appendix C.2 for details.

Method

GCJ

POJ-104

MAP@R (%)

AP (%)

MAP@R (%)

AP (%)

code2vec
code2seq
NCC
NCC-w/o-inst2vec
Aroma-Dot
Aroma-Cos
Seq-RNN
Seq-Transformer

7.76 (-0.79/+0.88)
11.67 (-1.98/+1.73)
17.26 (-1.11/+0.57)
34.88 (-5.72/+7.63)
29.08
29.67
69.27 (-0.52/+0.54)
47.81 (-2.28/+2.90)

17.95 (-1.24/+1.76)
23.09 (-3.24/+2.49)
31.56 (-1.11/+1.46)
56.12 (-7.63/+9.96)
42.47
36.21
82.66 (-0.23/+0.27)
71.66 (-2.30/+4.22)

1.90 (-0.43/+0.38)
3.12 (-0.45/+0.67)
39.95 (-2.29/+1.64)
54.19 (-3.18/+3.52)
52.07
55.08
72.28 (-0.76/+1.10)
48.81 (-1.10/+1.66)

5.30 (-0.80/+0.60)
6.43 (-0.37/+0.48)
50.42 (-2.98/+1.61)
62.75 (-5.49/+4.42)
45.94
55.33
79.19 (-1.11/+1.27)
54.65 (-3.41/+3.55)

MISIM-GNN
MISIM-RNN
MISIM-BoF

74.87 (-0.10/+0.15)
72.50 (-3.62/+2.09)
71.25 (-0.64/+0.42)

91.32 (-0.18/+0.20)
86.65 (-1.76/+2.32)
89.28 (-0.41/+0.45)

83.39 (-0.30/+0.59)
75.61 (-2.97/+2.54)
74.85 (-0.27/+0.31)

84.69 (-1.82/+1.24)
82.37 (-2.20/+1.36)
82.97 (-0.36/+0.41

C.6 CASS vs AST

Some recent research on code representation uses the AST-based representation [Dinella et al., 2020]
or AST paths [Alon et al., 2019b,a]. In this subsection, we explore how AST and CASS perform on
the task of code semantic representation described here.

We compared the code similarity performance of ASTs and CASSes on the test set of POJ-104,
as shown in table 2, by transforming both kinds of representations into feature vectors. using the
same method described in [Luan et al., 2019] and compute the similarity scores using dot or cosine
similarity. For each program in the dataset, we extracted its CASS under three different conﬁgurations:
0-0-0-0-0 9, the base conﬁguration, and 2-1-3-1-1/1-2-1-0-0, the best/worst performing conﬁguration
according to our preliminary evaluation of CASS (see Appendix C.7 for details). We also extracted
the ASTs of function bodies in a program. Each syntax node in the AST is labeled by its node type,
and an identiﬁer (or literal) node also gets a single child labeled by the corresponding identiﬁer name
(or literal text).

Table 6: Test Accuracy for AST and CASS conﬁgurations on POJ-104.

Method

AST-Dot
AST-Cos

SPT-Dot
SPT-Cos

CASS (2-1-3-1-1)-Dot
CASS (2-1-3-1-1)-Cos
CASS (1-2-1-0-0)-Dot
CASS (1-2-1-0-0)-Cos

MAP@R (%)

AP (%)

45.12
47.39

52.07
55.08

55.59
60.78
52.74
57.99

35.98
45.31

45.94
55.33

48.31
60.42
40.73
54.75

9Conﬁguration 0-0-0-0-0 is the duplicate of SPT. As shown in Table 6, conﬁguration 2-1-3-1-1 shows better

accuracy than conﬁguration 0-0-0-0-0.

18

As shown in Table 6, CASS conﬁgurations show an improvement in accuracy over the AST up to
1.33× in both evaluation metrics described in Appendix C.4. To better understand the performance
difference, we investigated a few solutions for the same problems from the POJ-104 dataset. One of
the interesting observations we found is that for the same problem, a solution may have a different
naming convention for local variables than that of another solution (e.g., English vs Mandarin
description of variables), but the resulting different variable names may carry the same semantic
meaning. AST uses variable names in its structure, but CASS has the option to not use variable
names. Thus the erasure of local variable names in CASS might help in discovering the semantic
similarity between code with different variable names. This might explain some of the performance
differences between AST and CASS in this experiment.

C.7 Experimental Results of Various CASS conﬁgurations

In this section, we discuss our experimental setup and analyze the performance of CASS compared
to Aroma’s simpliﬁed parse tree (SPT). In Section C.7.1, we explain the dataset grouping and
enumeration for our experiments. We also discuss the metrics used to quantitatively rank the
different CASS conﬁgurations and those chosen for the evaluation of code similarity. Section C.7.2
demonstrates that, a code similarity system built using CASS (i) has a greater frequency of improved
accuracy for the total number of problems and (ii) is, on average, more accurate than SPT. For
completeness, we also include cases where CASS conﬁgurations perform poorly.

C.7.1 Experimental Setup

In this section, we describe our experimental setup. At the highest level, we compare the performance
of various conﬁgurations of CASS to Aroma’s SPT. The list of possible CASS conﬁgurations is
shown in Table 1.

Dataset. The experiments use the same POJ-104 dataset introduced in Section 4.

Problem Group Selection. Given that POJ-104 consists of 104 unique problems and nearly 50,000
programs, depending on how we analyze the data, we might face intractability problems in both
computational and combinatorial complexity. With this in mind, our initial approach is to construct
1000 sets of ﬁve unique, pseudo-randomly selected problems for code similarity analysis. Using
this approach, we evaluate every conﬁguration of CASS and Aroma’s original SPT on each pair of
solutions for each problem set. We then aggregate the results across all the groups to estimate their
overall performance. While this approach is not exhaustive of possible combinations (in set size or set
combinations), we aim for it to be a reasonable starting point. As our research with CASS matures,
we plan to explore a broader variety of set sizes and a more exhaustive number of combinations.

Code Similarity Performance Evaluation. For each problem group, we exhaustively calculate
code similarity scores for all unique solution pairs, including pairs constructed from the same program
solution (i.e., program A compared to program A). We use G to refer to the set of groups and g to
indicate a particular group in G. We denote |G| as the number of groups in G (i.e. cardinality) and
|g| as the number of solutions in group g. For g = Gi, where i = {1, 2, . . . , 1000}, the total unique
program pairs (denoted by gP ) in Gi is |gP | = 1
To compute the similarity score of a solution pair, we use Aroma’s approach. This includes calculating
the dot product of two feature vectors (i.e., a program pair), each of which is generated from a CASS
or SPT structure. The larger the magnitude of the dot product, the greater the similarity.

2 |g|(|g| + 1).

We evaluate the quality of the recommendation based on average precision. Precision is the ratio of
true positives to the sum of true positives and false positives. Here, true positives denote solution
pairs correctly classiﬁed as similar and false positives refer to solution pairs incorrectly classiﬁed as
similar. Recall is the ratio of true positives to the sum of true positives and false negatives, where
false negatives are solution pairs incorrectly classiﬁed as different. As we monotonically increase the
threshold from the minimum value to the maximum value, precision generally increases while recall
generally decreases. The average precision (AP) summarizes the performance of a binary classiﬁer
under different thresholds for categorizing whether the solutions are from the same equivalence class
(i.e., the same POJ-104 problem) [Liu, 2009]. AP is calculated using the following formula over all
thresholds.

19

1. All unique values from the M similarity scores, corresponding to the solution pairs, are
gathered and sorted in descending order. Let N be the number of unique scores and
s1, s2, . . . , sN be the sorted list of such scores.

2. For i in {1, 2, . . . , N }, the precision pi and recall ri for the classiﬁer with the threshold

being si is computed.

3. Let r0 = 0. The average precision is computed as:

AP =

N
(cid:88)

i=1

(ri − ri−1)pi

C.7.2 Results

(a) Breakdown of the Number of
Groups with AP Greater or Less
than SPT.

(b) Average Precision for the
Group Containing the Best Case.

(c) Mean of Average Precision
Over All Program Groups.

Figure 5: Comparison of CASS and SPT. The blue bars in (a) and (b), and all the bars in (c), from
left to right, correspond to the best two, the median, and the worst two CASS conﬁgurations, ranked
by the metric displayed in each subﬁgure.

Figure 5a depicts the number of problem groups where a particular CASS variant performed better
(blue) or worse (orange) than SPT. For example, the CASS conﬁguration 2-0-0-0-1 outperformed SPT
in 859 of 1000 problem groups, and underperformed in 141 problem groups. This equates to a 71.8%
accuracy improvement of CASS over SPT. Figure 5a shows the two best (2-0-0-0-1 and 0-0-0-0-1),
the median (2-2-3-0-0), and the two worst (1-0-1-0-0 and 1-2-1-0-0) conﬁgurations with respect
to SPT. Although we have seen certain conﬁgurations that perform better than SPT, there are also
conﬁgurations that perform worse. We observed that the conﬁgurations with better performance have
function I/O cardinality option as 1. We also observed that the conﬁgurations with worse performance
have function I/O cardinality option as 0. These observations indicate that function I/O cardinality
seems to improve code similarity accuracy, at least, for the data we are considering. We speculate
that these conﬁguration results may vary based on programming language, problem domain, and
other constraints.

Figure 5b shows the group containing the problems for which CASS achieved the best performance
relative to SPT, among all 1000 problem groups. In other words, Figure 5b shows the performance
of SPT and CASS for the single problem group with the greatest difference between a CASS
conﬁguration and SPT. In this single group, CASS achieves the maximum improvement of more
than 30% over SPT for this problem group on two of its conﬁgurations. We note that, since we
tested 216 CASS conﬁgurations across 1000 different problem groups, there is a reasonable chance
of observing such a large difference even if CASS performed identically to SPT in expectation. We do
not intend for this result to demonstrate statistical signiﬁcance, but simply to illustrate the outcome of
our experiments.

Figure 5c compares the mean of AP over all 1000 problem groups. In it, the blue bars, moving left to
right, depict the CASS conﬁgurations that are (i) the two best, (ii) the median, and (iii) the two worst
in terms of average precision. Aroma’s baseline SPT conﬁguration is highlighted in orange. The best
two CASS conﬁgurations show an average improvement of more than 1% over SPT, while the others
degraded performance relative to the baseline SPT conﬁguration.

These results illustrate that certain CASS conﬁgurations can outperform the SPT on average by a
small margin, and can outperform the SPT on speciﬁc problem groups by a large margin. However,
we also note that choosing a good CASS conﬁguration for a domain is essential. We leave automating
this conﬁguration selection to future work.

20

2-0-0-0-10-0-0-0-12-2-3-0-01-0-1-0-01-2-1-0-0Configuration100050005001000Number of Problem Groups85982814651141172854995999Greater AP than SPT Less AP than SPT SPT0-1-3-0-12-1-3-0-11-0-2-1-01-1-1-0-01-2-1-0-0Configuration0.00.20.40.60.81.0Average Precision0.6810.8930.8920.7020.6500.644SPT2-1-3-1-12-1-2-1-11-2-2-2-01-2-1-0-11-2-1-0-0Configuration0.50.60.70.8Mean of Average Precision0.7110.7200.7200.6950.6740.672C.7.3 Analysis of Conﬁgurations

(a) Node Preﬁx Labels.

(b) Compound Statements.

(c) Global Variables.

(d) Global Functions.

(e) function I/O cardinality.

Figure 6: The Distributions of Performance for Conﬁgurations with a Fixed Option Type.

Figures 6a-6e serve to illustrate the performance variation for individual conﬁgurations. Figure 6a
shows the effect of varying the options for the node preﬁx label conﬁguration. Applying the node
preﬁx label for the parentheses operator (option 2) results in the best overall performance while
annotating every internal node (option 1) results in a concrete syntax tree and the worst overall
performance. This underscores the trade-offs in incorporating syntax-binding transformations in
CASS. In Figure 6b we observe that removing all features relevant to compound statements (option
1) leads to the best overall performance when compared with other options. This indicates that
adding separate features for compound statements obscures the code’s intended semantics when the
constituent statements are also individually featurized.

Figure 6c shows that removing all features relevant to global variables (option 1) degrades perfor-
mance. We also observe that eliminating the global variable identiﬁers and assigning a label to signal
their presence (option 2) performs best overall, possibly because global variables appearing in similar
contexts may not use the same variable identiﬁers. Further, option 2 performs better than the case
where global variables are indistinguishable from local variables (option 3). Figure 6d indicates
that removing features relevant to identiﬁers of global functions, but ﬂagging their presence with a
special label as done in option 2, generally gives the best performance. This result is consistent with
the intuitions for eliminating features of function identiﬁers in CASS as discussed in Section A.1.
Figure 6e shows that capturing the input and output cardinality improves the average performance.
This aligns with our assumption that function I/O cardinality may abstract the semantics of certain
groups of functions.

A Subtle Observation. A more nuanced and subtle observation is that our results seem to indicate
that for each CASS conﬁguration the optimal granularity of abstraction detail is different. For
compound statements, the best option seems to correspond to the coarsest level of abstraction detail,
while for node preﬁx label, global variables, and global functions the best option seems to corresponds
to one of the intermediate levels of abstraction detail. Additionally, for function I/O cardinality, the
best option has a ﬁner level of detail. For our future work, we aim to perform a deeper analysis on
this and hopefully learn such conﬁgurations, to reduce (or eliminate) the overhead necessary of trying
to manually discover such conﬁgurations.

D Broader Impact

To discuss the broader impact of our project, we will categorize impacts by their degree of inﬂuence.
For example, by the phrase “ﬁrst-degree negative impact” we will refer to a scenario where a given

21

0-*-*-*-*1-*-*-*-*2-*-*-*-*Configuration0.680.700.72Mean of Average Precision*-0-*-*-**-1-*-*-**-2-*-*-*Configuration0.680.700.72Mean of Average Precision*-*-0-*-**-*-1-*-**-*-2-*-**-*-3-*-*Configuration0.680.700.72Mean of Average Precision*-*-*-0-**-*-*-1-**-*-*-2-*Configuration0.680.700.72Mean of Average Precision*-*-*-*-0*-*-*-*-1Configuration0.680.700.72Mean of Average Precisionresearch idea can be directly used for harm (e.g., DeepFake [Floridi, 2018], DeepNude10, so on).
Similarly, by "second-degree negative impact" we will refer to a scenario where a research idea may
have a direct negative or positive impact based on how it is used (e.g., facial recognition for security
vs. oppressing minorities, GPT [Radford et al., 2019] to create an empathetic chatbot vs. malicious
fake news, etc). We call a research idea to have a "third-degree negative impact" if the idea by itself
represents an abstract concept (e.g., a similarly metric) and cannot harm by its own, but can be used
to build a second application which can then have a negative impact based on its use.

We envision the following positive broader impacts of the research idea presented in this paper. As
brieﬂy mentioned in the introduction, an end-to-end code similarity system can be incorporated in
programming tools (e.g., Visual Studio, Eclipse, etc.) to improve the productivity of a programmer
by offering him/her a similar but "known to be more efﬁcient" code snippet. It can be used in coding
education by displaying better (e.g., concise, faster, space-efﬁcient, etc.) code for a given code snippet,
in assisting program debugging by identifying potential missing parts, for plagiarism detection, for
automated bug-detection and ﬁxing, in automatic code transformations (e.g., replacing a Python
function with an equivalent C function) and so on. If used wisely with proper control and governance,
we believe it can create many positive impacts.

We can envision the following third-degree negative impacts. If a tool that uses code similarity
becomes mature enough to automatically generate correct compilable codes, it can be potentially
used to automatically replace code from one language to another or to replace a slow code with a fast
one. A malicious person can leverage the code similarity tool to crawl the web and steal codes on the
web, ﬁnd common patterns and security ﬂaws in the code available on the web, and then ﬁnd ways
to hack at a massive scale. Codes generated from the same code generators are likely to be more
vulnerable to such attacks. If systems allow automatic code patching/ﬁxing based on code-similarity
without proper testing, it might create security ﬂaws if hacked. If programmers get used to getting
help from a programming tool, that might negatively reduce the learning ability of programmers
unless the tool also offers explainability. Explainability would be required to understand what the
tool is learning about the code similarity and to educate the programmers about it.

To summarize, code similarity is an abstract concept that is likely to have numerous positive applica-
tions. However, if used in other tools, it might also play a role in creating a third-order negative impact.
It may be used to develop tools and applications which, if mature enough, may cause unacceptable
or dangerous situations. To mitigate the negative impacts, we would need to ensure proper policy
and security measures are in place to prevent negative usage. In particular, such secure systems may
require a human-in-the-loop so that any such tool is used to enhance the capability and productivity
of programmers.

10We have intentionally not included a citation to this work. We do not want to be seen, in any way, as

endorsing or promoting it. We believe such an act would be ethically irresponsible.

22

