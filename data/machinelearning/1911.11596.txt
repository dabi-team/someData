9
1
0
2

v
o
N
5
2

]

G
L
.
s
c
[

1
v
6
9
5
1
1
.
1
1
9
1
:
v
i
X
r
a

Distortion and Faults in Machine Learning
Software⋆

Shin NAKAJIMA

National Institute of Informatics
Tokyo, Japan

Abstract. Machine learning software, deep neural networks (DNN) soft-
ware in particular, discerns valuable information from a large dataset,
a set of data. Outcomes of such DNN programs are dependent on the
quality of both learning programs and datasets. Unfortunately, the qual-
ity of datasets is diﬃcult to be deﬁned, because they are just samples.
The quality assurance of DNN software is diﬃcult, because resultant
trained machine learning models are unknown prior to its development,
and the validation is conducted indirectly in terms of prediction perfor-
mance. This paper introduces a hypothesis that faults in the learning
programs manifest themselves as distortions in trained machine learning
models. Relative distortion degrees measured with appropriate observer
functions may indicate that there are some hidden faults. The proposal
is demonstrated with example cases of the MNIST dataset.

1

Introduction

Machine learning software, deep neural networks (DNN) software, is based on
inductive methods to discern valuable information from a given vast amount
of data [3]. The quality of such software is usually viewed from predication
performance that obtained approximation functions exhibit for incoming data.
Functional behavior of the resultant inference functions is dependent on trained
learning models, which learning programs calculate with training datasets as
their input.

The quality of DNN software is dependent on both the learning programs
and training datasets; either or both is a source of degraded quality. The learn-
ing programs result in inappropriate trained learning models if they are not
implemented faithfully in regard to well-designed machine learning algorithms
[4], Moreover, problematic datasets, suﬀering from sample selection bias [12] for
example, have negative impacts on trained learning models.

Although the learning programs and datasets are sources to aﬀect the quality
of inference functions, they are more or less indirect. Trained learning models de-
termine the quality directly, but have not been considered as ﬁrst-class citizens
to study quality issues. The models are important numeral data, but are inter-
mediate in that they are synthesized by learning programs and then transferred
to inference programs.
⋆ Presented at the 9th SOFL+MSVL Workshop in Shenzhen, November 2019.

 
 
 
 
 
 
2

This paper adapts a hypothesis that distortions in the trained learning models
manifest themselves as faults resulting in quality degradation. Although such
distortion is diﬃcult to be measured directly as they are, relative distortion
degrees can be deﬁned. Moreover, this paper proposes a new way of generating
datasets that show characteristics of the dataset diversity [9], which is supposed
to be eﬀective in testing machine learning software from various ways.

2 Machine Learning Software

2.1 Learning Programs

We consider supervised machine learning classiﬁers using networks of percep-
trons [4] or deep neural networks [3]. Our goal is to synthesize, inductively
from a large dataset, an approximation input-output relation classifying a multi-
dimensional vector data a into one of C categories. The given dataset LS is a set
of N number of pairs, hxn, tni (n = 1, · · ·, N ), where a supervisor tag tn takes a
value of c (c∈[1, · · ·, C]). A pair hxn, tni in LS means that xn is classiﬁed as tn.
Given a learning model y(W ; x) as a multi-dimensional non-linear function,
diﬀerentiable with respect to both learning parameters W and input data x.
Learning aims at obtaining a set of learning parameter values (W ∗) by solving
a numerical optimization problem.

W ∗ = argmin
W

E(W ; X),

E(W ; X) =

1
N

N

X
n=1

ℓ(y(W ; xn), tn)

The function ℓ( , ) denotes distances between its two parameters, representing
how much a calculated output y(W ; xn) diﬀers from its accompanying supervisor
tag value tn.

We denote a function to calculate W ∗ as Lf (LS), which is a program to solve
the above optimization problem with its input dataset LS. Moreover, we denote
the empirical distribution of LS as ρem. W ∗ is a collection of learning parameter
values to minimize the mean of ℓ under ρem.

From viewpoints of the software quality, Lf (LS), a learning program, is con-
cerned with the product quality, because it must be a faithful implementation
of a machine learning method, the supervised learning method for this case.

2.2 Inference Programs

We introduce another function If (x), using a trained learning model W ∗ or
y(W ∗; x), calculates inference results of an incoming data x.

For classiﬁcation problems, the inference results are often expressed as prob-
ability that the data x belongs to a category c. P rob(W, a; c), a function of c,
is probability such that the data a is classiﬁed as c. This P rob is readily imple-
mented in If (x) if we choose Softmax as an activation function of the output
layer of the learning model.

3

The prediction performance of If (x) is, indeed, deﬁned compactly as the
accuracy of classiﬁcation results for a dataset T S diﬀerent from LS used to
calculate W ∗; T S = {hxm, tmi}. For a speciﬁed W , Correct is a set-valued
function to obtain a subset of data vectors in T S.

Correct(W ; T S)≡ { xm | c∗ = argmax
c∈[1,...,C]

P rob(W, xm; c) ∧ tm = c∗ }

If we express | S | as a size of a set S, then an accuracy is deﬁned as a ratio as
below.

Accuracy(W ; T S) =

| Correct(W ; T S) |
| T S |

Given a W ∗ obtained by Lf (LS), the predication performance of If (LS) is
deﬁned in terms of Accuracy(W ∗; T S) for a dataset T S diﬀerent from LS. For an
individual incoming data a, a function of c P rob(W ∗, a; c) is a good performance
measure.

2.3 Quality Issues

Loss and Accuracy An NN learning problem is non-convex optimization, and
thus reaching a globally optimal solution is not guaranteed (e.g. [4]). The learning
program Lf (LS) is iterated over epochs to search for solutions and is taken as
converged when the value of the loss (E(W ; LS)) is not changed much between
two consecutive epochs. The learning parameter values at this converged epoch
are taken as W ∗. The derived W ∗ may not be optimal, and thus an accuracy is
calculated to ensure that the obtained W ∗ is appropriate. Moreover, W ∗ may
be over-ﬁtting to the training dataset LS.

In the course of the iteration, at an epoch e, the learning parameter values
W (e) are extracted. The iteration is continued until the accuracy becomes sat-
isfactory. Both Accuracy(W (e), LS) and Accuracy(W (e), T S) are monitored to
ensure the training process goes as desired. If the accuracy of T S is not much
diﬀerent from the accuracy with LS, we may think that the learning result does
not have the over-ﬁtting problem.

(a) Probably Correct (ProgPC)

(b) Bug-Injected (ProgBI)

Fig. 1. Loss and Accuracy: MNIST Dataset

4

Figure 1 shows loss and accuracy graphs as epochs proceed measured during
experiments1. The graphs, for example in Figure 1 (a), actually demonstrate that
the search converges at a certain desirable point in the solution space because the
loss decreases to be almost stable below a certain threshold, and the accuracies
of both T S and LS reach a satisfactory level of higher than 0.95. Figure 1
shows that the loss together with the accuracy may be good indicators to decide
whether NN training processes behave well or not.

Sources of Faults Intuitively, NN machine learning software shows good qual-
ity if prediction performance of If is acceptable. The graphs in Figure 1, how-
ever, depict that there is a counterexample case as discussed in [9], in which the
learning task uses MNIST dataset, for classifying hand-written numbers.

Figure 1(a) are graphs of loss and accuracy of a probably correct implemen-
tation of NN learning program, while Figure 1(b) are those of a bug-injected
program. The two graphs for loss are mostly the same to be converged. The two
accuracy graphs are similar as well, although the program of Figure 1(b) has
faults in it.

MNIST dataset is a standard benchmark and is supposed to be well-prepared
free from any sample selection bias. A bug-injected program Lf accepts a training
dataset LS of MNIST and calculates a set of trained learning parameters W ∗.
Intuitively, this W ∗ is inappropriate, because a bug-injected program produces
it. However, the accuracy graphs show that there is no clear sign of faults in the
prediction results of If , although its behavior is completely determined by the
probably inappropriate W ∗.

A question arises how faults in Lf aﬀect W ∗, which follows another question

whether such faults in W ∗ are observable.

3 Distortion Degrees

3.1 Observations and Hypotheses

Firstly, we introduce a few new concepts and notations. For two datasets DS1
and DS2, a relation DS1 (cid:22) DS2 denotes that DS2 is more distorted than DS1.
For two sets of trained learning parameters W ∗
2 of the same machine
learning model y(W ; ), a relation W ∗
2 is more distorted
than W ∗
1 . A question here is how to measure such degrees of the distortion. We
assume a certain observer function obs and a relation Robj with a certain small
threshold ǫobs such that Robj = (|obs(W ∗
2 )| ≤ ǫobs). The distortion
relation is deﬁned in terms of Robj, W ∗
2 ). We introduce
below three hypotheses referring to these notions.

1 and W ∗
2 denotes that W ∗

2 ⇔ ¬Robj (W ∗

1 ) − obs(W ∗

1 (cid:22) W ∗

1 (cid:22) W ∗

1 , W ∗

[Hyp-1] Given a training dataset LS, a machine learning program
Lf (LS), either correct or faulty, derives its optimal solution W ∗. For

1 We return to the experiment later in Section 4.

5

the training dataset LS and a testing dataset T S, if both are appro-
priately sampled or both follows the same empirical distribution, then
Accuracy(W ∗, T S) is almost the same as Accuracy(W ∗, LS).

[Hyp-2] For a training program Lf and two training datasets LSj
j = Lf (LSj), then LS1(cid:22)LS2 ⇒ W ∗
(j = 1 or 2), if W ∗

1 (cid:22)W ∗
2 .

[Hyp-3] For two training datasets LSj (j = 1 or 2) such that LS1(cid:22)LS2
and a certain appropriate obs, if Lf is correct with respect to its func-
tional speciﬁcations, then two results, W ∗
j = Lf (LSj), are almost the
same, written as W ∗
2 )). However, if L′
f is a
faulty implementation, then W ′∗

2 (or obs(W ∗
1 )≈obs(W ∗
1 ≺W ′∗
2 .

1 ≈W ∗

The accuracy graph in Figure 1 is an instance of [Hyp-1]. In Figure 1 (a), the
accuracy graphs for T S and LS are mostly overlapped, and the same is true for
the case of Figure 1 (b), which illustrates that the accuracy is satisfactory even
if the learning programs is buggy.

Moreover, the example in [9] is an instance of the [Hyp-2] because of the fol-
lowings. A training dataset LS(K+1) is obtained by adding a kind of disturbance
signal to LS(K) so that LS(K)(cid:22)LS(K+1). With an appropriate observer function
obsd, Robsd (W (K)∗, W (K+1)∗) is falsiﬁed where W (K)∗ = Lf (L(K)).

3.2 Generating Distorted Dataset

We propose a new test data generation method. We ﬁrst explain the L-BFGS
[14], which illustrates a simple way to calculate adversarial examples.

Given a dataset LS of {hxn, tni}, W ∗ = Lf (LS). An adversarial example is

a solution of an optimization problem;

xA = argmin

x

Aλ(W ∗; xS, tT , x),

Aλ(W ∗; xS, tT , x) = ℓ(y(W ∗; x), tT ) + λ·ℓ(x, xS).

Such a data xA is visually close to a seed xS for human eyes, but is actually
added a faint noise so as to induce miss-inference such that y(W ∗; xA) = tT .

Consider an optimization problem, in which a seed xS is xn and its target

label tT is tn.

xn∗ = argmin

x

Aλ(W ∗; xn, tn, x).

The method is equivalent to constructing a new data xn∗ to be added small
noises. Because the inferred label is not changed, xn∗ is not adversarial, but is
distorted from the seed xn. When the value of the hyper-parameter λ is chosen
to be very small, the distortion of xn∗ is large from xn. On the other hand, if λ
is appropriate, the eﬀects of the noises on xn∗ can be small so that the data is
close to the original xn.

6

By applying the method to all the elements in LS, a new dataset is obtained
to be {hxn∗, tni}. We introduce a function TA to generate such a dataset from
LS and W ∗.

TA({hxn, tni}, W ∗) = { hargmin
x

Aλ(W ∗; xn, tn, x), tni }.

Now, LS(K+1) = TA(LS(K), Lf (LS(K))) (for K≥0 and LS(0) = LS).

3.3 Some Properties

This section presents some properties that generated datasets LS(K) satisfy;
LS(K) = TA(LS(K−1), Lf (LS(K−1))) where LS(0) is equal to be a given training
dataset LS.

[Prop-1] LS(K) serves the same machine learning task as LS does.

We have that W (0)∗ = Lf (LS). As the optimization problem with W (0)∗ indi-
cates, x(n)∗, an element of LS(1) does not deviate much from x(n) in LS, and
is almost the same as x(n) in special cases. Therefore, LS(1) serves as the same
machine learning task as LS does. Similarly, LS(K) serves as the same machine
learning task as LS(K−1) does. By induction, LS(K) serves as the same machine
learning task as LS does, although the deviation may be large. ✷

[Prop-2] LS(K) (cid:22) LS(K+1)

The distortion relation is satisﬁed by construction if we take LS(K) as a starting
criterion.✷

[Prop-3] LS(K) is more over-ﬁtted to W (K−1)∗ than LS(K−1).

In the optimization problem, if the loss ℓ(y(W (K−1)∗; x∗), tT ) is small, x∗ in
LS(K) can be considered to be well-ﬁtted to W (K−1)∗ because the data recon-
struct the supervisor tag tT well. We make the loss is so small as in the above
by choosing carefully an appropriate λ value.✷

[Prop-4] There exists a certain Kc such that, for all K to satisfy a
relation K≥Kc, Accuracy(W (K)∗, T S) ≈ Accuracy(W (Kc)∗, T S) and
Accuracy(W (0)∗, T S) ≥ Accuracy(W (Kc)∗, T S). T S is a dataset diﬀer-
ent from LS, but follows the same empirical distribution ρem.

From Prop-3, we can see LS(K) is over-ﬁtted to W (Kc)∗ if K = Kc + 1. Be-
cause W (K)∗ = Lf (LS(K)) and both LS and T S follow the empirical distri-
bution ρem, we have Accuracy(W (K)∗, T S) ≈ Accuracy(W (Kc)∗, T S). Further-
more, LS (cid:22) LS(Kc) implies Accuracy(W (0)∗, LS) ≥ Accuracy(W (Kc)∗, LS) and
thus Accuracy(W (0)∗, T S) ≥ Accuracy(W (Kc)∗, T S).✷

7

[Prop-5] The dataset and trained learning model reach respectively
LS∞ and W (∞)∗ if we repeatedly conduct the training Lf and the
dataset generation TA interleavingly.

If we choose a K to be suﬃciently larger than Kc, we have, from Prop-4,
Accuracy(W (K)∗, LS(K)) ≈ Accuracy(W (Kc)∗, LS(K)), which may imply that
Accuracy(W (K)∗, LS(K)) ≈ Accuracy(W (K+1)∗, LS(K+1)). From Prop-3, LS(K+1)
is over-ﬁtted to W (K)∗, and thus we have LS(K) ≈ LS(K+1), which implies that
we can choose a representative LS(∞) from them. Using this dataset, we have
that W (∞)∗ = Lf (LS(∞)), and that W (∞)∗ is a representative. ✷

4 A Case Study

4.1 MNIST Classiﬁcation Problem

MNIST dataset is a standard problem of classifying handwritten numbers [6]. It
consists of a training dataset LS of 60,000 vectors, and a testing dataset T S of
10,000. Both LS and T S are randomly selected from a pool of vectors, and thus
are considered to follow the same empirical distribution. The machine learning
task is to classify an input sheet, or a vector data, into one of ten categories
from 0 to 9. A sheet is presented as 28×28 pixels, each taking a value between
0 and 255 to represent gray scales. Pixel values represent handwritten strokes,
and a number appears as a speciﬁc pattern of these pixel values.

In the experiments, the learning model is a classical neural network with a
hidden layer and an output layer. Activation function for neurons in the hid-
den layer is ReLU ; its output is linear for positive input values and a constant
zero for negatives. A softmax activation function is introduced so that the infer-
ence program If returns probability that an incoming data belongs to the ten
categories.

4.2 Experiments

We prepared two learning programs LP C
f . The former is a probably
f
correct implementation of a learning algorithm, and the latter is a bug-injected
version of the former. We conducted two experiments in parallel, one using LP C
and the other with LBI
f , and made comparisons. Below, we use notations such
as LMD
f

where M D is either P C or BI.

and LBI

f

MD = LMD

Training with MNIST dataset We conducted trainings the MNIST training
dataset LS(0); W (0)∗
(LS(0)). Figure 1 illustrates several graphs to
show their behavior, that are obtained in the training processes. Both accuracy
graphs in Figure 1 show that Accuracy(W (0)∗
MD , T S)
are mostly the same. In addition, Accuracy(W (0)∗
BI , )
are indistinguishable. The above observation is consistent with [Hyp-1].

MD , LS) and Accuracy(W (0)∗

P C , ) and Accuracy(W (0)∗

f

8

(a) Probably Correct (ProgPC)

(b) Bug-Injected (ProgBI)

Fig. 2. Distorted Data

(a) Probably Correct (ProgPC)

(b) Bug-Injected (ProgBI)

Fig. 3. Loss and Accuracy: Distorted Training Dataset

Generating Distorted Datasets We generated distorted datasets with the
method described in Section 3.2. We introduce here short-hand notations such
as LS(K)
(LS(0))).

MD = TA(LS(0), LMD

MD; LS(1)

f

Figure 2 shows a fragment of LS(1)

MD. We recognize that all the data are
not so clear as those of the original MNIST dataset and thus are considered
distorted. We may consider them as LS (cid:22) LS(1)
MD, which is an instance of [Prop-
2]. Furthermore, for human eyes, Figure 2 (b) for the case with LBI
is more
f
P C (cid:22) LS(1)
, which may be described as LS(1)
distorted than Figure 2 (a) of LP C
BI .

f

Training with Distorted Datasets We then conducted trainings the dis-
torted dataset LS(1)
MD). Figure 3 shows loss and accu-
racy graphs in their learning processes. Comparing Figure 3 with Figure 1 leads
to the following observations.

MD = LMD

MD; W (1)∗

(LS(1)

f

Firstly, the overall loss values of Figure 3 are smaller than those of Figure 1
counterparts, and the metrics concerning with the diﬀerences (ℓ( , )) are small
for the distorted dataset cases.

Secondly, for the MNIST testing dataset T S, Accuracy(W (1)∗
MD , T S), while Accuracy(W (1)∗

MD , LS(1)

than Accuracy(W (0)∗
Together with the fact of LS (cid:22) LS(1)
is consistent with [Hyp-2].

MD, the above implies W (0)∗

MD , T S) is lower
MD) reaches close to 100%.
MD (cid:22) W (1)∗
MD , which

9

(a) Probably Correct (ProgPC)

(b) Bug-Injected (ProgBI)

Fig. 4. Loss and Accuracy: Distorted Testing Dataset

Thirdly, we consider how much the accuracies diﬀer. We deﬁne the relation
Robs where obs(W ∗) = Accuracy(W ∗, T S). Let RMD
obs be deﬁned in terms of
Accuracy(W (0)∗
MD , T S) with a certain ǫMD. Comparing the graphs in Figure 1(a)
and Figure 3(a), we observe, for LP C
from
Figure 1(b) and Figure 3(b), ǫBI is about 0.4. If we choose a threshold to be
about 0.2, the two cases are distinguishable.

, ǫP C is about 0.1. Contrarily, for LBI
f

f

Moreover, we deﬁne the ≈ relation for (ǫ<0.2). As we know that LP C

probably correct and LBI
BI ≺ W (1)∗
and (b) W (0)∗

is bug-injected, we have followings. (a) W (0)∗
BI . These are, indeed, consistent with [Hyp-3].

f

is
P C ≈ W (1)∗
P C ,

f

Accuracy for Distorted Testing Datasets We generated distorted datasets
from the MNIST testing dataset T S; T S(1)
(LS)). We, then,
MD, T S(1)
checked the accuracy Accuracy(W (1)
MD) and Accuracy(W (1)
MD, T S(1)
in Figure 4. Accuracy(W (1)
tinguishable, because both LS(1)
with TA( , LMD
graphs are consistent again with [Hyp-1].

MD = TA(T S, LMD
MD), whose monitored results are shown
MD, LS(1)
MD) are not dis-
MD are constructed in the same way
(LS)) and thus their empirical distributions are the same. The

MD and T S(1)

f

f

5 Discussions

5.1 Neuron Coverage

As explained in Section 3.1, the distortion relation ( (cid:22) ) between trained learning
parameters is calculated in terms of observer functions. However, depending on
the observer, the resultant distortion degree may be diﬀerent. In an extreme
case, a certain observer is not adequate to diﬀerentiate distortions. A question
arises whether such distortion degrees are able to be measured directly. We will
study neuron coverage [11] whether we can use it as such a measure.

A neuron is said to be activated if its output signal out is larger than a given
threshold when a set of input signals inj is presented; out = σ(Pwj×inj). The
weight values wjs are constituents of trained W ∗. Activated Neurons below refer

10

2000 

1600 

1200 

800 

400 

s
e
c
n
e
r
r
u
c
c
o
f
o
r
e
b
m
u
n
e
h
T

0 

!"
0

ProgPC

ProgBI

!!"
0.2

#!"
0.4

$!"
0.6

%!"
0.8

&!"
1.0

The percentage of inactive neurons

Fig. 5. Frequencies of Inactive Neurons

to a set of neurons that are activated when a vector data a is input to a trained
learning model as y(W ∗; a).

N euron coverage (N C) =

| Activated N eurons |
| T otal N eurons |

In the above, | X | denotes the size of a set X. Using this neuron coverage as
a criterion is motivated by an empirical observation that diﬀerent input-output
pairs result in diﬀerent degrees of neuron coverage [11].

Results of Experiment We focus on the neurons constituting the hidden layer
in our NN learning model. As its activation function is ReLU, we choose 0 as
the threshold. Figure 5 is a graph to show the numbers of input vectors leading
to the chosen percentages of inactive neurons, (1 − N C). These input vectors
constitute the MNIST testing dataset T S of the size 10, 000.

According to Figure 5, the graph for the case of ProgPC, the ratio of inactive
neurons is almost 20%; namely, 80% of neurons in the hidden layer are activated
to have eﬀects on the classiﬁcation results. However, the ProgBI graph shows
that about 60% of them are inactive and do not contribute to the results. To
put it diﬀerently, this diﬀerence in the ratios of inactive neurons implies that
the trained learning model W ∗
P C of ProgPC,
W ∗

P C (cid:22)W ∗
From the generation method of the distorted dataset, we have LS (cid:22) LS(1)
P C
and LS (cid:22) LS(1)
BI may also be satisﬁed, which is in accordance with
the visual inspection of Figure 2. Furthermore, because of [Hyp-2] (Section 3.1),
W (1)
BI is true. It is consistent with the situation shown in Figure 5 in
BI are fewer than those in W (1)∗
that activated neurons in W (1)∗
P C .

BI of ProgBI is distorted from W ∗

P C (cid:22) W (1)

P C (cid:22)LS(1)

BI . LS(1)

BI .

Figure 3 can be understood from a viewpoint of the neuron coverage. The em-
pirical distribution of MNIST testing dataset T S is the same as that of MNIST
training dataset LS. Because of the distortion relationships on training datasets,
the distribution of T S is diﬀerent from those of LS(1)
BI ). More-
over, Figure 3 shows that |Accuracy(W (1)∗
P C , T S)| is

P C or LS(1)
MD (LS(1)
P C , T S) − Accuracy(W (0)∗

 
 
 
11

smaller than |Accuracy(W (1)∗
that the relationship LS(1)
W (1)∗
situation shown in Figure 5.

P C , T S)|. Therefore, we see
BI is satisﬁed. Because of [Hyp-2], it implies
BI . Therefore, the diﬀerence seen in Figure 3 is consistent with the

P C , T S) − Accuracy(W (0)∗
P C (cid:22)LS(1)

P C (cid:22) W (1)∗

In summary, the neuron coverage would be a good candidate as the metrics to
quantitatively deﬁne the distortion degrees of trained learning models. However,
because this view is based on the MNIST dataset experiments only, further
studies are desirable.

5.2 Test Input Generation

We will see how the dataset or data generation method in Section 3.2 is used in
software testing. Because the program is categorized as untestable [13], Metamor-
phic Testing (MT) [1] is now a standard practice for testing of machine learning
programs. We here indicate that generating an appropriate data is desirable to
conduct eﬀective testing. In the MT framework, given an initial test data x, a
translation function T generates a new follow-up test data T (x) automatically.
For testing machine learning software, either Lf (whether a training program
is faithful implementation of machine learning algorithms) [7][17] or If (whether
an inference program show acceptable prediction performance against incoming
data), generating a variety of data to show Dataset Diversity [9] is a key issue.
The function TA introduced in Section 3.2 can be used to generate such a follow-
up dataset used in the MT framework [10]. In particular, corner-case testing
would be possible by carefully chosen such a group of biased datasets.

DeepTest [15] employs Data Augmentation methods [5] to generate test data.
Zhou and Sun [19] adopts generating fuzz to satisfy application-speciﬁc proper-
ties. Both works are centered around generating test data for negative testing,
but do not refer to statistical distribution of datasets.

DeepRoad [18] adopts an approach with Generative Adversarial Networks
(GAN) [2] to synthesize various weather conditions as driving scenes. GAN is
formulated as a two-player zero-sum game. Given a dataset whose empirical
distribution is ρDS, its Nash equilibrium, solved with Mixed Integer Linear Pro-
graming (MILP), results in a DNN-based generative model to emit new data to
satisfy the relation x ∼i.i.d. ρDS. Thus, such new data preserve characteristics
of the original machine learning problem. Consequently, we regard the GAN-
based approach as a method to enlarge coverage of test scenes within what is
anticipated at the training time.

Machine Teaching [20] is an inverse problem of machine learning, and is a
methodology to obtain a dataset to optimally derive a given trained learning
model. The method is formalized as a two-level optimization problem, which is
generally diﬃcult to solve. We regard the machine teaching as a method to gen-
erate unanticipated dataset. Obtained datasets can be used in negative testing.
Our method uses an optimization problem with one objective function for
generating datasets that are not far from what is anticipated, but probably are
biased to build up the dataset diversity.

12

6 Concluding Remarks

We introduced a notion of distortion degrees which would manifest themselves as
faults and failures in machine learning programs, and studied the characteristics
in terms of neuron coverages. However, further study would be needed how we
rigorously measure the distortion degrees, which will make it possible to debug
programs with the measurement results. If the measurement is light-weight and
can be conducted for in-operation machine learning systems, we will be able to
diagnose systems at operation time.

Acknowledgment

The work is supported partially by JSPS KAKENHI Grant Number JP18H03224,
and is partially based on results obtained from a project commissioned by the
NEDO.

References

1. T.Y. Chen, F.-C. Kuo, H. Liu, P.-L. Poon, D. Towey, T.H. Tse, and Z.Q. Zhou :
Metamorphic Testing: A Review of Challenges and Opportunities, ACM Comput-
ing Surveys 51(1), Article No.4, pp.1-27, 2018.

2. I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
A. Courville, anY. d Bengio : Generative Adversarial Nets, In Adv. NIPS 2014,
pp.2672-2680, 2014.

3. I. Goodfellow, Y. Bengio, and A. Courville : Deep Learning, The MIT Press 2016.
4. S. Haykin : Neural Networks and Learning Machines (3ed.), Pearson India 2016.
5. A. Krizhevsky, I. Sutskever, and G. E. Hinton: Imagenet classiﬁcation with deep

convolutional neural networks. In Adv. NIPS 2012, pp. 10971105, 2012.

6. Y. LeCun, L. Bottou, Y. Bengio, and P. Haﬀner: Gradient-based learning applied
to document recognition, In Proceedings of the IEEE, 86(11), pp.2278-2324, 1998.
7. S. Nakajima and H.N. Bui : Dataset Coverage for Testing Machine Learning Com-

puter Programs, In Proc. 23rd APSEC, pp.297-304, 2016.

8. S. Nakajima : Quality Assurance of Machine Learning Software, In Proc. IEEE

GCCE 2018, pp.601-604, 2018.

9. S. Nakajima : Dataset Diversity for Metamorphic Testing of Machine Learning

Software, In Post-Proc. 8th SOFL+MSVL, pp.21-38, 2018.

10. S. Nakajima and T.Y. Chen: Generating Biased Dataset for Metamorphic Testing
of Machine Learning Programs, In Proc. IFIP-ICTSS 2019, pp.56-64, 2019.
11. K. Pei, Y. Cao, J. Yang, and S. Jana : DeepXplore: Automated Whitebox Testing

of Deep Learning Systems, In Proc. 26th SOSP, pp.1-18, 2017.

12. J. Quinonero-Candela, M. Sugiyama, A. Schwaighofer, and N.D. Lawrence (eds.)

: Dataset Shift in Machine Learning, The MIT Press 2009.

13. S. Segura, D. Towey, Z.Q. Zhou and T.Y. Chen: Metamorphic Testing: Testing the

Untestable, IEEE Software (in press).

14. C. Szegedy, W. Zaremba, I. Sutskever, J. Bruma, D. Erhan, I. Goodfellow, and R.

Fergus : Intriguing properties of neural networks, In Proc. ICLR, 2014.

15. Y. Tian, K. Pei, S. Jana, and B. Ray : DeepTest: Automated Testing of Deep-
Neural-Network-driven Autonomous Cars, In Proc. 40th ICSE, pp.303-314, 2018.

13

16. D. Warde-Farley and I. Goodfellow: Adversarial Perturbations of Deep Neural
Networks, in Perturbation, Optimization and Statistics, The MIT Press 2016.
17. X. Xie, J.W.K. Ho, C. Murphy, G. Kaiser, B. Xu, and T.Y. Chen : Testing and
Validating Machine Learning Classiﬁers by Metamorphic Testing, J. Syst. Softw.,
84(4), pp.544-558, 2011.

18. M. Zhang, Y. Zhang, L. Zhang, C. Liu, and S. Khurshid: DeepRoad: GAN-Based
Metamorphic Testing and Input Validation Framework for Autonomous Driving
Systems, In Proc. 33rd ASE, pp.132-142, 2018.

19. Z.Q. Zhou and L. Sun: Metamorphic Testing of Driverless Cars, Comm. ACM,

vol.62, no.3, pp.61-67, 2019.

20. X. Zhu : Machine Teaching: An Inverse Problem to Machine Learning and an

Approach Toward Optimal Education, In Proc. 29th AAAI, pp.4083-4087, 2015.

