Veriﬁcation of Neural Network Behaviour:
Formal Guarantees for Power System Applications

Andreas Venzke, Student Member, IEEE, and Spyros Chatzivasileiadis, Senior Member, IEEE

1

0
2
0
2

l
u
J

0
3

]

Y
S
.
s
s
e
e
[

4
v
4
2
6
1
0
.
0
1
9
1
:
v
i
X
r
a

time,

Abstract—This paper presents for the ﬁrst

to our
knowledge, a framework for verifying neural network behavior in
power system applications. Up to this moment, neural networks
have been applied in power systems as a black box;
this
has presented a major barrier for their adoption in practice.
Developing a rigorous framework based on mixed-integer linear
programming, our methods can determine the range of inputs
that neural networks classify as safe or unsafe, and are able to
systematically identify adversarial examples. Such methods have
the potential to build the missing trust of power system operators
on neural networks, and unlock a series of new applications in
power systems. This paper presents the framework, methods to
assess and improve neural network robustness in power systems,
and addresses concerns related to scalability and accuracy. We
demonstrate our methods on the IEEE 9-bus, 14-bus, and 162-
bus systems, treating both N-1 security and small-signal stability.

Index Terms—Neural networks, mixed-integer linear program-

ming, security assessment, small-signal stability.

I. INTRODUCTION

A. Motivation

M ACHINE learning, such as decision trees and neural

networks, has demonstrated signiﬁcant potential for
highly complex classiﬁcation tasks including the security
assessment of power systems [1]. However, the inability to
anticipate the behavior of neural networks, which have been
usually treated as a black box, has been posing a major
barrier in their application in safety-critical systems, such as
power systems. Recent works (e.g. [2]) have shown that neural
networks that have high prediction accuracy on unseen test
data can be highly vulnerable to so-called adversarial examples
(small input perturbations leading to false behaviour), and that
their performance is not robust. To our knowledge, the robust-
ness of neural networks has not been systematically evaluated
in power system applications before. This is the ﬁrst work that
develops provable formal guarantees of the behavior of neural
networks in power system applications. These methods allow
to evaluate the robustness and improve the interpretability of
neural networks and have the potential to build the missing
trust of power system operators in neural networks, enabling
their application in safety-critical operations.

B. Literature Review

For a comprehensive review, the interested reader is referred
to [1], [3] and references therein. A recent survey in [4]
reviews applications of (deep) reinforcement learning in power
systems. In the following, we focus on applications related to
power system operation and security. Recent examples include
[5] which compares different machine learning techniques
for probabilistic reliability assessment and shows signiﬁcant
reductions in computation time compared to conventional
approaches. Neural networks obtain the highest predictive
accuracy. The works in [6], [7] rely on machine learning
techniques to learn local control policies for distributed energy
resources in distribution grids. Ref. [8] uses machine learning
to predict the result of outage scheduling under uncertainty,
while in [9] neural networks rank contingencies for security
assessment. Neural networks are used in [10] to approximate
the system security boundary and are then included in the
security-constrained optimal power ﬂow (OPF) problem.

Recent developments in deep learning have sparked renewed
interest for power system applications [11]–[16]. There are a
range of applications for which deep learning holds signiﬁcant
promise including online security assessment, system fault
diagnosis, and rolling optimization of renewable dispatch as
outlined in [11]. Deep neural networks are used in [12] to
predict the line currents for different N-1 outages. By encoding
the system state inside the hidden layers of the neural network,
the method can be applied to unseen N-2 outages with high
accuracy [13]. The work in [14] proposes a deep autoencoder
to reduce the high-dimensional input space of security as-
sessment and increase classiﬁer accuracy and robustness. Our
work in [15] represents power system snapshots as images to
take advantage of the advanced deep learning toolboxes for
image processing. Using convolutional neural networks, the
method assesses both N-1 security and small signal stability
at a fraction of the time required by conventional methods.
The work in [16] uses convolutional neural networks for
N-1 contingency screening of a large number of uncertainty
scenarios, and reports computational speed-ups of at least two
orders of magnitude. These results highlight the potential of
data-driven applications for online security assessment. The
black box nature of these tools, however, presents a major
obstacle towards their application in practice.

Machine learning algorithms including neural networks
have been applied in power system problems for decades.

A. Venzke and S. Chatzivasileiadis are with the Department of Electrical
Engineering, Technical University of Denmark, 2800 Kgs. Lyngby, Denmark
e-mail: {andven, spchatz}@elektro.dtu.dk.

This work is supported by the multiDC project, funded by Innovation Fund

Denmark, Grant Agreement No. 6154-00020.

C. Main Contributions

In power system applications, to the best of our knowl-
edge, the robustness of learning approaches based on neural
networks has so far not been systematically investigated.
Up to now, neural network performance has been evaluated
by splitting the available data in a training and a test set,

 
 
 
 
 
 
and assessing accuracy and other statistical metrics on the
previously unseen test set data (e.g. [12]–[15]). Recent works
(e.g. [2]) have shown that neural networks that have high
prediction accuracy on unseen test data can be highly vul-
nerable to adversarial examples and the resulting prediction
accuracy on adversarially crafted inputs is very low [17].
Adversarial examples also exist in power system applications
as demonstrated in [18]. This highlights the importance to
develop methodologies which allow to systematically evaluate
and improve the robustness of neural networks.

In this work, we present for the ﬁrst time a framework
to obtain formal guarantees of neural network behaviour for
power system applications, building on recent advances in
the machine learning literature [19]. Our contributions are
threefold. First, we evaluate the robustness of neural net-
through a systematic
works in power system applications,
identiﬁcation of adversarial examples (small input perturba-
tions which lead to false behaviour) or by proving that no
adversarial examples can exist for a continuous range of
neural network inputs. Second, we improve the interpretability
of neural networks by obtaining provable guarantees about
how continuous input regions map to speciﬁc classiﬁcations.
Third, using systematically identiﬁed adversarial examples, we
retrain neural networks to improve robustness.

In the rest of this paper, we refer to veriﬁcation as the
process of obtaining formal guarantees for neural network
behaviour. These formal guarantees are in the form of con-
tinuous input regions in which the classiﬁcation of the neural
network provably does not change, i.e., no adversarial ex-
amples exist. Being able to determine the continuous range
of inputs (instead of discretized samples) that
the neural
network classiﬁes as safe or unsafe makes the neural network
interpretable. Accuracy is no longer a pure statistical metric
but can be supported by provable guarantees of neural network
behavior. This allows operators to either build trust in the
neural network and use it in real-time power system operation,
or decide to retrain it. Increasing the robustness, and provably
verifying target properties of these algorithms is therefore a
prerequisite for their application in practice. To this end, the
main contributions of our work are:

1) Using mixed-integer linear programming (MILP), we
present a neural network veriﬁcation framework for
power system applications which, for continuous ranges
of inputs, can guarantee if the neural network will
classify them as safe or unsafe.

2) We present a systematic procedure to identify adversarial
examples and determine neural network input regions in
which provably no adversarial examples exist.

3) We improve the robustness of neural networks by re-
training them on enriched training datasets that include
adversarial examples identiﬁed in a systematic way.
4) Formulating the veriﬁcation problem as a mixed-integer
linear program, we investigate, test and apply techniques
to maintain scalability; these involve bound tightening
and weight sparsiﬁcation.

5) We demonstrate our methods on the IEEE 9-bus, 14-
bus, and 162-bus system, treating both N-1 security and
small-signal stability. For the IEEE 9-bus system, we

2

re-train the neural network using identiﬁed adversarial
examples, and show improvements both in accuracy and
robustness.

This work is structured as follows: In Section II, we describe
the neural network architecture and training, in Section III we
formulate the veriﬁcation problems as mixed-integer programs
and in Section IV we address tractability. In Section V we
deﬁne our simulation setup and present results on formal
guarantees for a range of power system security classiﬁers.

II. NEURAL NETWORK ARCHITECTURE AND TRAINING

A. Neural Network Structure

Before moving on with the formulation of the veriﬁcation
problem, in this section we explain the general structure of the
neural networks we consider in this work [20]. An illustrative
example is shown in Fig. 1. A neural network is deﬁned by
the number K of its fully-connected hidden layers, with each
layer having Nk number of neurons (also called nodes or
hidden units), with k = 1, ..., K. The input vector is denoted
with x ∈ RN0, and the output vector with y ∈ RNK+1. The
input to each layer ˆzk+1 is a linear combination of the output
of the previous layer, i.e. ˆzk+1 = Wkzk + bk, where Wk
is a Nk+1 × Nk weight matrix and bk is a Nk+1 × 1 bias
vector between layers k and k + 1. Each neuron in the hidden
layers incorporates an activation function, z = f (ˆz), which
usually applies a non-linear transformation to the scalar input.
There is a range of possible activation functions, such as the
sigmoid function, the hyberbolic tangent, the Rectiﬁer Linear
Unit (ReLU), and others. Recent advances in computational
power and machine learning have made possible the successful
training of deep neural networks [21]. The vast majority of
such networks use ReLU as the activation function as this has
been shown to accelerate their training [22]. For the rest of
this paper we will focus on ReLU as the chosen activation
function. A framework for neural network veriﬁcation consid-
ering general activation functions such as the sigmoid function
and the hyberbolic tangent is proposed in [23]. ReLU is a
piecewise linear function deﬁned as z = max(ˆz, 0). For each
of the hidden layers we have:

zk = max(ˆzk, 0)

∀ k = 1, ..., K

ˆzk+1 = Wk+1zk + bk+1 ∀ k = 0, 1, ..., K − 1

(1)

(2)

where z0 = x, i.e. the input vector. Throughout this work, the
max operator on a vector ˆzk ∈ RNk is deﬁned element-wise
as zn
k , 0) with n = 1, ..., Nk. The output vector is
then obtained as follows:

k = max(ˆzn

y = WK+1zK + bK+1

(3)

In this work, we will focus on classiﬁcation networks, that is
each of the output states yi corresponds to a different class.
For example, within the power systems context, each operating
point x can be classiﬁed as y1= safe or y2= unsafe (binary
classiﬁcation). The input vector x encodes the necessary in-
formation to determine the operating point, e.g. the generation
dispatch and loading in the DC optimal power ﬂow (OPF).
Each input is classiﬁed to the category that corresponds to the
largest element of the output vector y. For example, if y1 > y2
then input x is safe, otherwise unsafe.

Input
layer

Hidden
layer 1

Hidden
layer 2

Hidden
layer 3

Output
layer

W3, b3

W2, b2

W1, b1

x1

x2

x3

z1
1

z2
1

z3
1

z4
1

z1
2

z2
2

z3
2

z4
2

W4, b4

y1

y2

z1
3

z2
3

z3
3

z4
3

Fig. 1.
Illustrative neural network for binary classiﬁcation: Fully connected
neural network with three inputs x and three hidden layers. Between each
layer, a weight matrix W and bias b is applied. Each hidden layer has four
neurons with non-linear ReLU activation functions. Based on the comparison
of the two outputs y, the input is classiﬁed in one of the two categories, i.e.
y1 > y2 or y1 < y2.

B. Neural Network Training

The training of neural networks requires a dataset of labeled
samples S, where each sample consists of the input x and
the true output classiﬁcation ¯y. The dataset S is split into
a training and a testing dataset denoted with Strain and Stest,
respectively. During training, the weights W and biases b are
optimized using the training dataset Strain with respect to a
deﬁned objective function. Different objective functions (also
called loss functions) exist [20]. In this work we use one of
the most commonly employed for classiﬁcation: the softmax
cross entropy, which is deﬁned as follows. First, the softmax
function takes the vector of the neural network output y and
transforms it to an equal-size vector of real numbers in the
range between 0 and 1, the sum of which have to be equal to 1.
This corresponds to a probability distribution of the output of
the neural network belonging to a certain class. The probability
pi of the input belonging to the different classes i ∈ NK+1 is
deﬁned as:

pi =

eyi
(cid:80)NK+1
j=1

eyj

,

∀i ∈ NK+1

(4)

Note that e{.} refers to the exponential function here. We
deﬁne the softmax cross entropy function as:

f (¯y, p) = −

NK+1
(cid:88)

i=1

¯yi log(pi)

(5)

This objective function can be understood as the squared error
of the distance from the true classiﬁcation ¯y to the probability
distribution over the classes p predicted by the neural network.
This formulation has theoretical and practical advantages over
penalizing the squared error directly [24]. During training, we
solve the following optimization problem:

min
W,b,x,y,¯y,z,ˆz,p

f (¯y, p)

s.t.

(1), (2), (3), (4) and (5)

(6)

(7)

3

function (5). Third, using back-propagation, we compute the
gradients of the loss function with respect to weights W and
biases b and update these using stochastic gradient descent (in
our simulation studies we use the Adam optimizer [25]). Then,
we return to the second step, and repeat this procedure until
a deﬁned number of training iterations, also called epochs, is
reached. After the training has terminated, we evaluate the
neural network performance by calculating its accuracy on
the unseen test dataset Stest. This gives us a measure of the
generalization capability of the neural network. For a detailed
explanation of neural network training please refer to e.g. [26].

III. VERIFICATION AS A MIXED-INTEGER PROGRAM

As the accuracy on the test dataset is not sufﬁcient to
provide provable guarantees of neural network behavior, in
the following, we formulate veriﬁcation problems that allow
us to verify target properties of neural networks and identify
adversarial examples in a rigorous manner. To this end, we
ﬁrst reformulate trained neural networks as mixed-integer
programs. Then, we introduce veriﬁcation problems to a) prove
the absence of adversarial examples, b) evaluate the neural
input regions
network robustness, and c) compute largest
with same classiﬁcation. Finally, we discuss how to include
additional constraints on the input to the neural network and
how to extend the framework to regression problems.

A. Reformulation of ReLU as Mixed-Integer Program (MIP)
First, we reformulate the ReLU function zk = max(ˆzk, 0),
shown in (1), using binary variables rk ∈ {0, 1}Nk , following
the work in [19]. For all k = 1, ..., K, it holds:

zk = max(ˆzk, 0) ⇒






k (1 − rk)

zk ≤ ˆzk − ˆzmin
zk ≥ ˆzk
zk ≤ ˆzmax
zk ≥ 0
rk ∈ {0, 1}Nk

k rk

(8a)

(8b)

(8c)

(8d)

(8e)

is true. Conversely, if rn

k = 0, the ReLU is inactive and zn

The entries of the binary vector rk signal whether the cor-
responding ReLU unit in layer k is active or not. Note that
the operations in (8a)–(8d) are performed element-wise. For
a ReLU unit n, if rn
k is
constrained to be zero through (8c) and (8d) given that the
expression 0 ≤ ˆzk − ˆzmin
k is 1, then
zn
k is constrained to be equal to ˆzn
k through (8a) and (8b)
given that the expression ˆzk ≤ ˆzmax
is true. To ensure that
both expressions are always true, the bounds on the ReLU
output ˆzmin and ˆzmax have to be chosen sufﬁciently large to
not be binding, but as low as possible to provide tight bounds
in the branch-and-bound algorithm during the solution of the
MIP. In Section IV-1, we present a possible approach to tighten
these bounds using interval arithmetic (IA).

k

k

The training follows these steps: First, we initialize the weights
W and biases b randomly. Second, we substitute the variables
x and ¯y with the samples from the training dataset Strain. Based
on the current value of weights W and biases b, we compute
the corresponding neural network prediction y, and the loss

B. Formulating Veriﬁcation Problems

Using (8a)-(8e), we can now verify neural network prop-
erties by formulating mixed-integer linear programs (MILPs).
Without loss of generality, in this paper we assume that the

neural network classiﬁes the output in only two categories:
y1 and y2 (e.g. safe and unsafe). Note that in case the neural
network outputs are of the same magnitude (y1 = y2), we
classify the input as ‘unsafe’; this avoids the risk of classifying
a true ‘unsafe’ operating point as ‘safe’ in those cases.

that result

input xref

a) Verifying against adversarial examples: Assume a
given input xref is classiﬁed as y1, i.e. for the neural network
output holds y1 > y2. Adversarial examples are instances
close to the original
to a different
(wrong) classiﬁcation [2]. Imagine for example an image of
2000×1000 pixels showing a green trafﬁc light. An adversarial
example exists if by changing just 1 pixel at one corner of
the image, the neural network would classify it as a red light
instead of the initial classiﬁcation as green (e.g. see Fig.
16 of [27]). Machine learning literature reports on a wide
range of such examples and methods for identifying them,
as they can be detrimental for safety-critical application (such
as autonomous vehicles). Due to the nature of this problem,
however, most of these methods rely on heuristics. Veriﬁcation
can be a very helpful tool towards this objective as it can help
us discard areas around given inputs by providing guarantees
that no adversarial example exists [19].

Turning back to our problem, assume that

the system
operator knows that within distance (cid:15) from the operating point
xref the system remains safe (y1 > y2). If we can guarantee
that the neural network will output indeed a safe classiﬁcation
for any input (cid:107)x − xref(cid:107) ≤ (cid:15), then we can provide the operator
with the guarantees required in order to deploy methods based
on neural networks for real-time power system applications. To
this end, we can solve the following mixed-integer program:

min
x,ˆz,z,y
s.t.

y1 − y2

(2), (3), (8a) − (8e)
(cid:107)x − xref(cid:107)∗ ≤ (cid:15)

(9a)

(9b)

(9c)

If the resulting objective function value is strictly positive (and
assuming zero optimality gap), then y1 > y2 for all (cid:107)x −
xref(cid:107)∗ ≤ (cid:15), and we can guarantee that no adversarial input
exists within distance (cid:15) from the given point. The norm in
(9c) can be chosen to be e.g. ∞-norm, 1-norm or 2-norm. In
the following, we focus on the ∞-norm, which allows us to
formulate the optimization problem (9) as MILP. In addition,
the ∞-norm has a natural interpretation of allowing each input
dimension in magnitude to vary at most by (cid:15). By considering
both the 1- and ∞-norm in (9c), adversarial robustness with
respect to all lp-norms with p ≥ 2 can be achieved [28].
Conversely, in case input xref was originally classiﬁed as y2,
we minimize y2 − y1 in objective function (9a) instead. Note
that we model z, ˆz and y as optimization variables in the
mixed-integer program in (9), as we are searching for an input
x with output y that changes the classiﬁcation. For ﬁxed input
x these optimization variables are then uniquely determined.
b) Adversarial robustness: Instead of solving (9) for a
single (cid:15) and a single operating point xref , we can solve a
series of optimization problems (9) for different values of (cid:15)
and different operating points xref and assess the adversarial
accuracy as a measure of neural network robustness. The
adversarial accuracy is deﬁned as the share of samples that

4

do not change classiﬁcation from the correct ground-truth
classiﬁcation within distance (cid:15) from the given input xref . In
our simulation studies, we use all samples in the training data
or unseen test data set to evaluate the adversarial accuracy. As
such, the adversarial accuracy for a distance measure of zero
((cid:15) = 0) is equal to the share of correctly predicted samples,
i.e. the prediction accuracy. The adversarial accuracy can be
used as an indicator for the robustness/brittleness of the neural
network: low adversarial accuracy for very small (cid:15) is in most
cases an indicator of poor neural network performance [29].
Furthermore, utilizing this methodology, we can systemat-
ically identify adversarial examples to evaluate the adversar-
ial robustness and re-train neural networks to improve their
adversarial robustness. First, for a range of different values
of (cid:15) and using either training or test dataset, we solve the
optimization problems in (9). For all samples xref for which an
input perturbation exists within a distance of (cid:15) that does change
the classiﬁcation, we need to assess whether this change occurs
as the sample is located at the power system security boundary
(and the input perturbation places the adversarial input across
the boundary) or if it is in fact an adversarial example and
the change in classiﬁcation indicates an incorrect boundary
prediction by the neural network. This can be achieved by
computing the ground-truth classiﬁcation ¯y for the perturbed
input, e.g., by evaluating the system security and stability using
conventional methods for the identiﬁed potential adversarial
inputs, and comparing it to the neural network prediction. The
share of identiﬁed adversarial examples (i.e. false classiﬁcation
changes) serves as measure of adversarial robustness.

In our simulation studies in Section V, we compute the
adversarial accuracy and identify adversarial examples for
several illustrative test cases using the proposed methodology.
In case the neural network robustness is not satisfactory,
we can use the aforementioned procedure to systematically
identify adversarial examples from the training dataset, add
these to the training dataset and re-train the neural network
by optimizing (6)–(7) using the enriched training dataset
to improve robustness. Other directions for improving the
performance of the neural network include to (i) use a dataset
creation method that provides a more detailed description of
the security boundary e.g. using the algorithm in [30], and
(ii) re-train the neural networks to be adversarially robust by
modifying the objective function (5) as outlined e.g. in [29].
c) Computing largest regions with same classiﬁcation:

To establish trust of neural networks among power system
operators it is crucial to be able to determine the range of
inputs a neural network will classify as safe or unsafe. The
formulation (10) does that by computing the maximum input
range around a given input xref for which the classiﬁcation
will not change. Note that we achieve this by computing
the minimum distance to a sample which does change the
classiﬁcation:

min
x,ˆz,z,y,(cid:15)
s.t.

(cid:15)

(2), (3), (8a) − (8e)
(cid:107)x − xref(cid:107)∗ ≤ (cid:15)
y2 ≥ y1

(10a)

(10b)

(10c)

(10d)

Create training and test dataset
Strain and Stest

Standard neural network training procedure:
optimize (6) – (7) using Strain

Evaluate confusion matrix and
accuracy using Strain and Stest

Use Stest; evaluate adversarial accuracy
by solving (9) for different (cid:15) and xref;
to identify adversarial examples compute
ground-truth classiﬁcation (Step A)

Is robustness
satisfactory?

yes

end

no

Use Strain and repeat previous step A;
add identiﬁed adversarial examples to Strain

Solve (10) to determine largest input
regions with guaranteed classiﬁcation

Fig. 2. Flowchart illustrating the methodology: First, we create datasets
which are split into training and test set. Using the training set only, we
train the neural network. Following standard procedure, we evaluate the neural
network performance with the confusion matrix. Then, using the mixed-integer
linear reformulation of the trained neural network and the test dataset, we
evaluate the adversarial accuracy and identify adversarial examples. If the
neural network robustness is not satisfactory, we repeat this step with the
training test set, add the identiﬁed adversarial examples to the training set and
re-train the neural network. Note that we cannot use the test set for this step,
as the test set information should not be used in the training process; instead
the test set should only be used to evaluate the generalization capability on
unseen data. Finally, as shown in the last block as a separate stream, with our
methods we are also able to determine input regions around selected input
samples with guaranteed classiﬁcation to provide formal guarantees for neural
network behaviour.

Here again we select the ∞-norm in (10c), turning (10) to
a MILP. If input xref
is classiﬁed as y2, then we replace
(10d) with y1 ≥ y2 instead. Solving veriﬁcation problem (10)
enables us to provide the operator with guarantees that e.g. all
system states where Generator 1 produces between 50 MW
and 100 MW and Generator 2 produces between 30 MW
and 80 MW, will be classiﬁed by the neural network as safe.
Thus, the neural network is no longer a black box, and it
is up to the operator to determine if its results are accurate
enough. If they are not, the operator can retrain the neural
network with adjusted parameters to increase the accuracy
for regions the operator is mostly interested. Alternatively,
the operator can follow a risk-based approach attaching a
certain conﬁdence/risk level to the neural network results,
or the operator can choose to use the neural network for a

5

limited range of inputs where its results are provably 100%
accurate. Our overall methodology is illustrated in a ﬂowchart
in Figure 2.

d) Input constraints: In all veriﬁcation problems, we can
add additional constraints characterizing the admissible region
for the input vector x and, thus, can obtain larger regions with
the same classiﬁcation. Given that in neural network training
it is standard practice to normalize the input between 0 and 1,
we add constraint (11) to both formulations in (9) and (10):

0 ≤ x ≤ 1

(11)

These limits correspond for example to the minimum and
maximum generator limits if the generator limits are included
as part of the input vector. In Section V-B, we will show how
including the DC power balance (13) as additional constraint
on the input allows to further improve the obtained bounds in
(10a).

e) Regression problems: Note that the proposed frame-
work can be extended to regression problems. As an example,
neural networks can be used to predict the security margin
of the power systems (e.g. the damping ratio of the least
damped mode in small signal stability analysis). Then, we
can solve veriﬁcation problems of similar structure as (10)
and (9) to determine regions in which the neural network
prediction is guaranteed to deviate less than a predeﬁned
constant from the actual value. At the same time, for a deﬁned
region, we can compute the minimum and maximum security
margin predicted. This allows us to analyse the robustness of
regression neural networks.

IV. IMPROVING TRACTABILITY OF VERIFICATION
PROBLEMS

By examining the complexity of the MILPs presented in
Section 4, we make two observations. First, from (8a) – (8e)
it becomes apparent that the number of required binaries in
the MILP is equal to the number of ReLU units in the neural
network. Second, the weight matrices W are dense as the
neural network layers are fully connected. As neural networks
can potentially have several hidden layers with a large number
of hidden neurons, improving the computational tractability of
the MILP problems is necessary. To this end, we apply two
different methods from [19] and [31]: tightening the bounds
on the ReLU output, and sparsifying the weight matrices.

1) ReLU bound tightening:

In the reformulation of the
maximum operator in (8a) and (8c), we introduced upper
and lower bounds ˆzmax and ˆzmin, respectively. Without any
speciﬁc knowledge about the bounds, these have to be set to a
large value in order not to become binding. A computationally
cheap approach to tighten the ReLU bounds is through interval
arithmetic (IA) [19]. We propagate the initial bounds on the
input zmin
0 = xmax through each layer to obtain
individual bounds on each ReLU in layer k = 1, ..., K:
k−1 + W−
k−1 + W−

k−1 + bk−1
k−1 + bk−1

k = W+
ˆzmax
k = W+
ˆzmin

k−1ˆzmax,+
k−1ˆzmin,+

k−1ˆzmin,+
k−1ˆzmax,+

0 = xmin, zmax

(12b)

(12a)

The max and min-operator are denoted in compact form as:
x+ = max(x, 0) and x− = min(x, 0). For example,
in

1 = W+

our simulation studies, we restrict the input x to be between
xmax = 0 and xmin = 1. Then, the bounds on the ﬁrst layer
evaluate to ˆzmax
0 1 + b0 and ˆzmin
0 1 + b0. The
bounds for the remaining layers can be obtained by applying
(12) sequentially. Methods to compute tighter bounds also
exist, e.g. by solving an LP relaxation while minimizing or
maximizing the ReLU bounds [32], but this is out of scope of
this paper.

1 = W−

2) Training Neural Networks for Easier Veriﬁability:
A second possible approach to increase the computational
tractability of the veriﬁcation problems is to (re-)train the
neural network with the additional goal to sparsify the weight
matrices W. Here, we rely on an automated gradual pruning
algorithm proposed in [31]. Starting from 0% sparsity, where
all weight matrices W are non-zero, a deﬁned share of the
weight entries are set to zero. The weight entries selected
are those with the smallest absolute magnitude. Subsequently,
the neural network is re-trained for the updated sparsiﬁed
structure. This procedure is repeated until a certain sparsity
target is achieved. There are two important observations: First,
through sparsiﬁcation the classiﬁcation accuracy can decrease,
as less degrees of freedom are available during training. Sec-
ond, larger sparsiﬁed networks can achieve better performance
than smaller dense networks [31]. As a result, by forming
slightly larger neural networks we can maintain the required
accuracy while achieving sparsity. As we will see in Section V,
sparsiﬁcation maintains a high classiﬁcation accuracy and,
thus, a signiﬁcant computational speed-up is achieved when
solving the MILPs. As a further beneﬁt of sparsiﬁcation, the
interpretability of the neural network increases, as the only
neuron connections kept are the ones that have the strongest
contribution to the classiﬁcation. Computational tractability
can further increase by pruning ReLUs during training, i.e.
ﬁxing them to be either active or inactive, in order to eliminate
the corresponding binaries in the MILP [33]. This approach
along with the LP relaxation for bound tightening will be
object of our future work.

V. SIMULATION AND RESULTS

A. Simulation Setup

The goal of the following simulation studies is to illustrate
the proposed methodology for neural networks which classify
operating points as ‘safe’ or ‘unsafe’ with respect to different
power system security criteria. The neural network input
x encodes variables such as active generator dispatch and
uniquely determines an operating point. The neural network
output y corresponds to the two possible classiﬁcations: ‘safe’
or ‘unsafe’ with respect to the speciﬁed security criteria.

In the following, we will present four case studies. The ﬁrst
two case studies use a 9-bus and 162-bus system, respectively.
The security criteria is feasibility to the N-1 security con-
strained DC optimality power ﬂow (OPF). The third case study
uses a 14-bus system and as security criteria the combined
feasibility to the N-1 security constrained AC-OPF and small-
signal stability. The fourth case study uses a 162-bus system
and as security criteria the feasiblity to the N-1 security
constrained AC-OPF under uncertainty. The details of the

6

OPF formulations are provided in the Appendix. Please note
that in both the formulation of the N-1 security constrained
DC- and AC-OPF we do not consider load shedding, as this
should be avoided at all times. Here, the goal is to provide the
system operator with a tool to rapidly screen a large number
of possible operating points and identify possible critical
scenarios. For these critical scenarios only, a more dedicated
security constrained OPF could be solved to minimize the cost
of load shedding or to redispatch generation units.

To train the neural networks to predict these security criteria,
it is necessary to have a dataset of labeled samples that map
operating points (neural network inputs x) to an output secu-
rity classiﬁcation ¯y. The neural network predicts the output y
which should be as close as possible to the ground truth ¯y. We
train the neural network as outlined in Section II to achieve
satisfactory predictive performance and extract the weights
W and biases b obtained. Then, based on these, we can
formulate the veriﬁcation problems in (9) and (10) to derive
formal guarantees and assess and improve the robustness of
these neural networks including the existence of adversarial
examples. For a detailed overview of our methodology please
refer to the ﬂowchart in Figure 2.

The created dataset can include both historical and simu-
lated operating points. Absent historical data in this paper,
we created simulated data for our studies. We will detail the
dataset creation for each case study in the corresponding sub-
section. To facilitate a more efﬁcient neural network training
procedure, we normalize each entry of the input vector x to
be between 0 and 1 using the upper and lower bounds of the
variables (we do that for all samples x ∈ S). Empirically, this
has been shown to improve classiﬁer performance.

After creating the training dataset, we export it to Tensor-
Flow [25] for the neural network training. We split the dataset
into a training set and a test set. We choose to use 85% of
samples for training and 15% for testing. During training,
we minimize the cross-entropy loss function (5) using the
Adam optimizer with stochastic gradient descent [25], and use
the default options in TensorFlow for training. In the cases
where we need to enforce a certain sparsity of the weight
matrices we re-train with the same objective function (5), but
during training we reduce the number of non-zero weights
until a certain level of sparsity is achieved, as explained in
Section IV-2. To allow for the neural network veriﬁcation
and the identiﬁcation of adversarial examples, after the neural
network training we export the weight matrices W and biases
b in YALMIP, formulate the MILPs, and solve them with
Gurobi. If not noted otherwise, we solve all MILPs to zero
optimality gap, and as a result obtain the globally optimal
solution. All simulations are carried out on a laptop with
processor Intel(R) Core(TM) i7-7820HQ CPU @ 2.90 GHz
and 32GB RAM.

B. Neural Network Veriﬁcation for the IEEE 9-bus system

1) Test Case Setup: For the ﬁrst case study, we consider
a modiﬁed IEEE 9-bus system shown in Fig. 3. This system
has three generators and one wind farm. We assume that the
load at bus 7 is uncertain and can vary between 0 MW and
200 MW. The wind farm output is uncertain as well and

8

7

6

2

G2

L3

9

5

3

G3

L2

L1

W1

4

1

G1

Fig. 3. Modiﬁed IEEE 9-bus system with wind farm W 1 at bus 5, and
uncertain load L3 at bus 7. Generation and loading marked in blue correspond
to the input vector x of the neural network.

can vary between 0 MW and 300 MW. The line limits are
increased by 25% to accommodate the added wind generation
and increased loading. Remaining parameters are deﬁned
according to [34]. As mentioned in the previous subsection,
in this case we employ the N-1 security for the DC power
ﬂow as a security criterion. If the SC-DC-OPF results to
infeasibility, then the sample is ‘unsafe’. We consider the
single outage of each of the six transmission lines and use
MATPOWER to run the DC power ﬂows for each contingency
ﬁxing the generation and load to the deﬁned operating point.
We evaluate the violation of active generator and active line
limits to determine wheter an operating point is ‘safe’ or
‘unsafe’. We create a dataset of 10’000 possible operating
points with input vector x = [PG2 PG3 PL3 PW 1], which are
classiﬁed y = {y1 = safe , y2 = unsafe}. Note that the ﬁrst
generator PG1 is the slack bus and its power output is uniquely
determined through the dispatch of the other units; therefore, it
is not considered as input to the neural network. The possible
operating points are sampled using Latin hypercube sampling
[35] which ensures that the distance between each sample is
maximized.

To compute the ground-truth (and thus be able to assess the
neural network performance) we use a custom implementation
of the N-1 preventive security constrained DC-OPF (SC-DC-
OPF) in YALMIP using pre-built functions of MATPOWER
to compute the bus and line admittance matrix for the intact
and outaged system state. We model the the uncertain loads
and generators as optimization variables. With ground truth,
we refer to the region around an infeasible sample for which
we can guarantee that no input exists which is feasible to the
SC-DC-OPF problem. We can compute this by minimizing
the distance from an infeasible sample to a feasible operation
point. For the detailed mathematical formulation of the N-1
security constrained DC-OPF and the ground truth evaluation
please refer to Appendix A-B.

2) Neural Network Training: Using the created dataset,
we train two neural networks which only differ with respect
to the enforced sparsity of the weight matrices. Employing
the ﬁrst neural network is trained with
loss function (5),
dense weight matrices. Using this as a starting point for the
retraining, on the second neural network we enforce 80%
sparsity, i.e. only 20% of the entries are allowed to be non-
zero. In both cases, the neural network architecture comprises
three hidden layers with 50 neurons each, as this allows us to
achieve high accuracy without over-ﬁtting. For comparison, if
we would only train a single-layer neural network with 150
neurons, the maximum test set classiﬁcation accuracy is only
90.7%, highlighting the need for a multi-layer architecture.

7

TABLE I
CONFUSION MATRICES FOR IEEE 9-BUS TEST CASE

Neural network without sparsiﬁed weight matrices

Test samples: 1500

Predicted: Safe

Unsafe

Accuracy

True: Safe (326)
True: Unsafe (1174)

311
11

15
1163

98.3%

Neural network with 80% sparsiﬁed weight matrices

Test samples: 1500

Predicted: Safe

Unsafe

Accuracy

True: Safe (326)
True: Unsafe (1174)

308
15

18
1159

97.8%

)
.

m
r
o
n
(

3
G
P

1

0.8

0.6

0.4

0.2

0

0

0.2 0.4 0.6 0.8
PG2 (norm.)

1

1

0.8

0.6

0.4

0.2

0

0

0.2 0.4 0.6 0.8
PG2 (norm.)

1

correct safe

correct unsafe misclassiﬁed

veriﬁcation samples

Fig. 4. Regions around four veriﬁcation samples in which the classiﬁcation
is guaranteed not to change. The left ﬁgure uses the neural network without
sparse weight matrices, and the right ﬁgure uses the neural network with
imposed 80% sparsity on the weight matrices. For visualization purposes, the
load PL3 and wind level PW 1 are ﬁxed to 40% and 1’000 new samples are
created and classiﬁed according to the respective neural network.

Our three-layer neural network without sparsity has a clas-
siﬁcation accuracy of 98.3% on the test set and 98.4% on the
training set. This illustrates that neural networks can predict
with high accuracy whether an operating point is safe or
unsafe. If we allow only 20% of the weight matrix entries
to be non-zero and re-train the three-layer neural network,
the classiﬁcation accuracy evaluates to 97.8% on the test set
and 97.3% on the training set. The corresponding confusion
matrices for both neural networks for the test set are shown
in Table I. As we see, the sparsiﬁcation of the neural network
leads to a small reduction of 0.5% in classiﬁcation accuracy,
since less degrees of freedom are available during training. The
beneﬁts from sparsiﬁcation are, however, substantially more
signiﬁcant. As we will see later in our analysis, sparsiﬁcation
substantially increases the computational tractability of veriﬁ-
cation problems, and leads to an increase in interpretability of
the neural network.

3) Visualization of Veriﬁcation Samples: To be able to
visualize the input regions for which we can guarantee that
the trained neural networks will not change their classiﬁcation,
we shall reduce the input dimensionality. For this reason, we
will study here only a single uncertainty realization, where
we assume that both the load PL3 and wind power output
PW 1 amount to 40% of their normalized output (note that
our results apply to all possible inputs). The resulting input
space is two-dimensional and includes generators PG2 and

PG3. For visualization purposes only, we take 1’000 new
samples from the reduced two-dimensional space and classify
them with the neural networks as safe or unsafe. In addition,
we compute their true classiﬁcation using MATPOWER. The
resulting classiﬁed input spaces are shown in Fig. 4 with the
left ﬁgure corresponding to the neural network with full weight
matrices, and the right ﬁgure to the sparsiﬁed weight matrices.
We can observe that misclassiﬁcations occur at the security
boundary. This is to be expected as the sampling for this
visualization is signiﬁcantly more dense than the one used for
training. What is important here to observe though, is that even
if the neural network on the right contains only 20% of the
original number of non-zero weights, the neural networks have
visually comparable performance in terms of classiﬁcation
accuracy. As a next step, we solve the veriﬁcation problem
described in (10). In that, for several samples xref we compute
the minimum perturbation (cid:15) that changes the classiﬁcation.
We visualize the obtained regions using black boxes around
the veriﬁcation samples in Fig. 4. The average solving time
in Gurobi is 5.5 s for the non-sparsiﬁed and 0.3 s for the
sparsiﬁed neural network. We see that by sparsifying the
weights we achieve a 20× computational speed-up. Solving
the MILP to zero optimality gap, we are guaranteed that within
the region deﬁned by the black boxes around the veriﬁcation
samples no input exists which can change the classiﬁcation.

4) Provable Guarantees:

In the following, we want

to
provide several provable guarantees of behaviour for both
neural networks while considering the entire input space.
For this purpose, we solve veriﬁcation problems of the form
(10). Since the neural network input x is normalized between
0 ≤ x ≤ 1 based on the bounds of the input variables, we
include the corresponding constraint (11) in the veriﬁcation
problem. Similarly, no input to the neural network would
violate the limits of the slack bus generator PG1. These inputs
could be directly classiﬁed as unsafe. Even if PG1 is not part
of the input, its limits can be deﬁned based on the input x
through the DC power balance, as shown in (13):

G1 ≤ PL1 + PL2 + PL3 − PG2 − PG3 − PW 1 ≤ P max
P min
G1 ,

(13)

where x = [PG2 , PG3 , PL3 , PW 1]T . The DC power balance
ensures that the system generation equals the system loading.
As a result, the slack bus has to compensate the difference
between system loading and generation. In this section, we
show how additional a-priori qualiﬁcations of the input such
as (13), affect the size of the regions in which we can guarantee
a certain classiﬁcation.

In Table II, we compare the obtained bounds to the ground
truth provided by the SC-DCOPF and report the computational
time for three properties, when we do not include and when we
do include the power balance constraint. The second reported
computational time uses the tightened ReLU bounds (12) with
interval arithmetic (IA) (see Section IV-1 for details on the
method). The ﬁrst property is the size of the largest region
around the operating point xref = 1 (1-vector) with the same
guaranteed classiﬁcation. This operating point corresponds to
the maximum generation and loading of the system. For this
input, the classiﬁcation of the neural networks is known to be

8

TABLE II
VERIFICATION OF NEURAL NETWORK PROPERTIES FOR 9-BUS SYSTEM

w/o power balance

with power balance

(cid:15)

Sol. Time (s)

(cid:15)

Sol. Time (s)

Property 1: ∀x ∈ [0, 1] : |x − 1|∞ ≤ (cid:15) −→ Classiﬁcation: insecure

NN (w/o sparsity)
50.7%
NN (80% sparsity) 48.7%
53.7%
SC-DC-OPF

2.7 | IA: 1.1 54.4%
0.3 | IA: 0.2 54.4%
- 53.7%

1.5 | IA: 1.3
0.3 | IA: 0.2
-

Property 2: ∀x ∈ [0, 1] : |x − 0|∞ ≤ (cid:15) −→ Classiﬁcation: secure

NN (w/o sparsity)
NN (80% sparsity) 32.7%
SC-DC-OPF1
31.7%

29.2% 501.9 | IA: 400.7 29.3% 473.0 | IA: 817.9
1.6
-

1.1 32.7%
- 31.7%

3.3 | IA:

2.1 | IA:

Property 3: ∃x ∈ [0, 1] : |PW 2 − 0|∞ ≤ (cid:15) −→ Classiﬁcation: secure

NN (w/o sparsity)
97.4%
NN (80% sparsity) 99.1%
SC-DC-OPF
92.5%
2 The SC-DC-OPF can only provide infeasbility certiﬁcates.

12.2 | IA: 11.4 90.3%
0.4 | IA: 0.3 89.0%
- 92.5%

6.5 | IA: 3.7
0.4 | IA: 0.3
-

We compute this by re-sampling a very large number of samples.

unsafe. We observe for both neural networks that when the
power balance constraint is not included in the veriﬁcation
problem, the input region guaranteed to be classiﬁed as unsafe
is smaller than the ground truth of 53.7% (provided by the SC-
DC-OPF). By including the power balance in the veriﬁcation
problem, the input region classiﬁed as unsafe is signiﬁcantly
enlarged and more closely matches the target of 53.7%.

For the second property, we consider the region around the
operating point where all generating units and the load are
at their lower bound, i.e., xref = 0. This point corresponds
to a low loading of the system and is therefore secure (i.e.
no line overloadings, etc.). We solve the veriﬁcation problem
minimizing the distance from xref = 0 leading to an insecure
classiﬁcation. In this way, we obtain the maximum region
described as hyper-cube around the zero vector in which
the classiﬁcation is guaranteed to be always ‘safe’. We can
observe in this case that the neural network without sparsity
slightly underestimates while the neural network with 80%
sparsity slightly overestimates the safe region compared to
the ground truth (31.7%). Sparsiﬁcation allows a 150×–500×
computational speed-up (two orders of magnitude). For this
property,
including the power balance constraint does not
change the obtained bounds.

The third property we analyze is the maximum wind infeed
for which the neural network identiﬁes a secure dispatch.
To this end, we solve the veriﬁcation problem by maximiz-
ing the wind infeed PW 1 in the objective function while
maintaining a secure classiﬁcation. The true maximum wind
infeed obtained by solving this problem directly with the SC-
DC-OPF is 92.5%, i.e. 277.5 MW can be accommodated. If
we do not enforce constraint (13), then we observe that the
obtained bounds 97.4% and 99.12% are not tight. This happens
because the obtained solutions x from (10) keep generation
and loading to zero and only maximize wind power; this
violates the generator bounds on the slack bus, as it has to
absorb all the generated wind power. Enforcing the DC power
balance (13) in the veriﬁcation problem allows a more accurate
representation of the true admissible inputs, and we obtain
signiﬁcantly improved bounds of 90.3% and 89.0%.

100

10

1

)

%

(

0.1

0.01

0.1

1

10

(cid:15) (%)

100% – adversarial accuracy
adversarial and misclassiﬁed samples

misclassiﬁed samples

Fig. 5. Adversarial accuracy, and share of adversarial and misclassiﬁed
samples are shown for the test data set of the 9 bus test case and different
levels of input perturbation (cid:15). The adversarial accuracy refers to the share
of correctly classiﬁed samples, for which no input perturbation exists which
changes the classiﬁcation within distance (cid:15) of that sample (by solving (9)
for each sample). Please note that both axes are logarithmic, and 100%
minus the adversarial accuracy is shown, i.e. the share of samples which are
not adversarially robust. Out of these, to determine whether an adversarial
example has been identiﬁed, the ground-truth classiﬁcation is computed. The
dashed lines show the performance of the re-trained neural network when
adding 1’152 identiﬁed adversarial examples from the training dataset to the
updated training dataset. It can be observed that both prediction accuracy and
adversarial robustness, i.e. the share of adversarial examples, are signiﬁcantly
improved.

For all three properties, we observe that both interval arith-
metic (IA) and weight sparsiﬁcation improve computational
tractability, in most instances achieving the lowest solver times
when used combined. From the two, weight sparsiﬁcation has
the most substantial effect as it allows to reduce the number
of binaries in the resulting MILP problem.

5) Adversarial robustness and re-training: In Fig. 5, the
adversarial robustness and the share of adversarial and mis-
classiﬁed samples are shown for the test dataset and different
magnitudes of input perturbation (cid:15) in the range from 0.01% to
10%, following the methodology described in Fig. 2. The full
lines are computed using the sparsiﬁed neural network with
performance described in the confusion matrix in Table I. The
sparsiﬁed neural network has a predictive accuracy of 97.8%
on the training dataset, i.e. 2.2% of test samples are misclas-
siﬁed. It can be observed that for small input perturbations (cid:15)
the share of adversarial and misclassiﬁed samples increases to
3.5%, i.e. for an additional 1.3% of test samples the identiﬁed
adversarial input leads to a wrong misclassiﬁcation. Increasing
the input perturbation (cid:15) to 10%, these shares increase to 5.1%
and 2.9%, respectively. This indicates that parts of the security
boundary are not correctly represented by the neural network.
To improve performance, we run the same methodology for
the training dataset of 8’500 samples, and include all identiﬁed
adversarial examples of the training dataset (a total of 1’152
samples) in the updated training dataset and re-train the neural
network. Note that we only use the training dataset for this
step as the unseen test dataset is used to evaluate the neural
network performance. The resulting performance is depicted
with dashed lines in Fig. 5. Re-training the neural network has
two beneﬁts in this test case: First, it improves the predictive
accuracy on the unseen test data from 97.8% to 99.5%, i.e.
only 0.5% of test samples are misclassiﬁed. Second, the share
of identiﬁed adversarial samples is reduced, for (cid:15) = 1% from
1.3% to 0.7% and for (cid:15) = 1% from 2.9% to 0.9%, showing
improved neural network robustness. At the same time, it can
be observed that for perturbations larger than (cid:15) = 1%, the

9

TABLE III
CONFUSION MATRIX FOR IEEE 162-BUS TEST CASE (WITH SPARSITY)

Test samples: 3000

Predicted: Safe Unsafe Accuracy

True: Safe (1507)
True: Unsafe (1493)

1501
11

6
1482

Accuracy

99.4%

adversarial accuracy is similar between both networks. This
shows that in this case for a large amount of samples an
adversarial input can be identiﬁed which correctly leads to
a different classiﬁcation, i.e. the adversarial input moves the
operating point across the true security boundary and is not
an adversarial example leading to a false misclassiﬁcation.
The neural network robustness could be further improved by
repeating this procedure for additional iterations.

C. Scalability for IEEE 162-bus system

1) Test Case Setup: For the second case study, we con-
sider the IEEE 162-bus system with parameters taken from
[34]. We add ﬁve uncertain wind generators located at buses
{3, 20, 25, 80, 95} with a rated power of 500 MW each.
As security criterion, we consider again N-1 security based
on DC power ﬂow, considering the outage of 24 critical
lines: {22, ..., 27, 144, ..., 151, 272, ..., 279}. The input vector
is deﬁned as x = [PG1−G12 PW1−W5 ]T . To construct the
dataset for the neural network training, we ﬁrst apply a
bound tightening of the generator active power bounds, i.e.
we maximize and minimize the corresponding bound consid-
ering the N-1 SC-DC-OPF constraints. The tightened bounds
exclude regions in which the SC-DC-OPF is guaranteed to be
infeasible and therefore allows to decrease the input space.
In the remaining input space, we draw 10’000 samples using
Latin hypercube sampling and classify them as safe or unsafe.
As usual in power system problems, the ‘safe’ class is sub-
stantially smaller than the ‘unsafe’ class, since the true safe
region is only a small subset of the eligible input space. To
mitigate the dataset imbalance, we compute the closest feasible
dispatch for each of the infeasible samples by solving an SC-
DC-OPF problem and using the ∞-norm as distance metric.
As a result, we obtain a balanced dataset of approximately
20’000 samples.

2) Neural Network Training: We choose a neural network
architecture with 4 layers of 100 ReLU units at each layer,
as the input dimension increases from 4 to 17 compared to
the 9-bus system. We train again two neural networks: one
without enforcing sparsity and a second with 80% sparsity. The
trained four-layer network without sparsity has a classiﬁcation
accuracy 99.3% on the test set and 99.7% on the training set.
For the sparsiﬁed network, these metrics evaluate to 99.7%
for the test set and 99.4% for the training set. The confusion
matrix of the sparsiﬁed network is shown in Table III and it
can be observed that the neural network has high accuracy in
classifying both safe and unsafe operating points.

3) Provable Guarantees: Assuming the given load proﬁle,
the property of interest is to determine the minimum distance
from zero generation xref = 0 to a feasible (‘safe’) solution.

TABLE IV
VERIFICATION OF NEURAL NETWORK PROPERTIES FOR 162-BUS SYSTEM

TABLE V
CONFUSION MATRIX FOR IEEE 14-BUS TEST CASE (WITH SPARSITY)

10

w/o power balance

with power balance

(cid:15)

Sol. Time (s)

(cid:15)

Sol. Time (s)

Property 1: ∀x ∈ [0, 1] : |x − 0|∞ ≤ (cid:15) −→ Classiﬁcation: insecure

NN (w/o sparsity)
-
-
NN (80% sparsity) 59.4% 560 | IA: 1217 65.5%
- 66.2%
SC-DC-OPF

> 50 min

66.2%

> 50 min
22 | IA: 30
-

100

10

1

)

%

(

0.1

0.01

0.1

1

10

(cid:15) (%)

100% – adversarial accuracy
adversarial and misclassiﬁed samples

misclassiﬁed samples

Fig. 6. Adversarial accuracy, and share of adversarial and misclassiﬁed
samples are shown for the test data set of the 162 bus test case and different
levels of input perturbation (cid:15). The adversarial accuracy refers to the share
of correctly classiﬁed samples, for which no input perturbation exists which
changes the classiﬁcation within distance (cid:15) of that sample (by solving (9)
for each sample). Please note that both axes are logarithmic, and 100%
minus the adversarial accuracy is shown, i.e. the share of samples which are
not adversarially robust. Out of these, to determine whether an adversarial
example has been identiﬁed, the ground-truth classiﬁcation is computed.

In Table IV, we compare the result of the veriﬁcation problem
with the ground truth obtained from solving the SC-DC-
OPF problem. We can see that
the bound of 59.4% we
obtain without including the DC power balance is not tight
compared to the ground truth of 66.2%. Including the DC
power balance increases the bound to 65.2% which is reason-
ably close, indicating satisfactory performance with respect to
this property. This conﬁrms the ﬁndings for the 9-bus (cmp.
Tables II and IV) showing that including the additional power
balance constraint in the veriﬁcation problems leads to bounds
on (cid:15) closer to the ground-truth. Regarding computational
time, we can observe that for this larger neural network, the
sparsiﬁcation of the weight matrices becomes a requirement to
achieve tractability. For both cases with and without including
the DC power balance, the MILP solver did not identify a
solution after 50 minutes for the non-sparsiﬁed network.

4) Adversarial Robustness: In Fig. 6, the adversarial accu-
racy, and share of adversarial and misclassiﬁed samples are
computed for the test data set and different levels of input
perturbation (cid:15). It can be noted that for small input perturbations
(cid:15) ≤ 1% the adversarial accuracy is above 99% (i.e. for
> 99% of test samples no input exists which can change
the classiﬁcation), indicating neural network robustness. For
larger input perturbations, the adversarial accuracy decreases
as system input perturbations can move the operating point
across system security boundaries. The number of identiﬁed
adversarial examples increases as well. For a large input per-
turbation (cid:15) = 10% for 129 (4.3%) test samples an adversarial
input exists which falsely changes the classiﬁcation. This
indicates that the security boundary estimation of the neural
network is inaccurate in some parts of the high-dimensional

Test samples: 1500

Predicted: Safe Unsafe Accuracy

True (15): Safe
True (1485): Unsafe

Accuracy

12
0

3
1485

99.8%

input space. As for the 9 bus test case, a re-training of the
neural network by including identiﬁed adversarial examples
in the training data set could improve performance.

D. N-1 Security and Small Signal Stability

Security classiﬁers using neural networks can screen operat-
ing points for security assessment several orders of magnitudes
faster than conventional methods [15]. There is, however, a
need to obtain guarantees for the behaviour of these classiﬁers.
We show in the following how our framework allows us to
analyse the performance of a classiﬁer for both N-1 security
and small signal stability.

1) Dataset Creation: For the third case study, we consider
the IEEE 14-bus system [34] and create a dataset of operating
points which are classiﬁed according to their feasibility to both
the N-1 SC-AC-OPF problem and small signal stability. We
consider the outages of all lines except lines 7-8 and 6-13,
as the 14 bus test case is not N-1 secure for these two line
outages. Furthermore, we assume that if an outage occurs, the
apparent branch ﬂow limits are increased by 12.5% resembling
an emergency overloading capability. We use a simple brute-
force sampling strategy to create a dataset of 10’000 equally
spaced points in the four input dimensions x = [PG2−G5].
The generator automatic voltage regulators (AVRs) are ﬁxed
to the set-points deﬁned in [34]. For each of these operating
points, we run an AC power ﬂow and the small-signal stability
analysis for each contingency. Operating points which satisfy
operational constraints and the eigenvalues of the linearized
system matrix have negative real parts for all contingencies
are classiﬁed as ’safe’ and ’unsafe’ otherwise. Note that,
is unbalanced as
similar to the 162-bus case,
only 1.36% of the overall created samples are feasible. A
method to create balanced datasets for both static and dynamic
security is object of future work. For the full mathematical
details on the N-1 SC-AC-OPF formulation and the small-
signal stability constraints, please refer to Appendices A-C
and A-D, respectively.

the dataset

2) Neural Network Training and Performance: We choose
a three-layer neural network with 50 neurons for each layer,
as in Section V-B. Based on the analysis of the previous case
studies, here we only train an 80% sparsiﬁed network. The
network achieves the same accuracy of 99.8% both on the
training and on the test set. The confusion matrix on the
test set is shown in Table V. Note that here the accuracy
does not carry sufﬁcient information as a metric of the neural
network performance, as simply classifying all samples as
unsafe would lead to a test set accuracy of 99.0% due to the
unbalanced classes. Besides the use of supplementary metrics,
such as speciﬁcity and recall (see e.g. [15]), obtaining provable

100

10

1

)

%

(

0.1

0.01

0.1

1

10

(cid:15) (%)

0.8

0.7

0.6

0.5

)
.

m
r
o
n
(

4
G
P

0.4

0.4

0.5

0.6
PG3 (norm.)

0.7

0.8

11

predicted safe
predicted unsafe
true safe
true unsafe
adversarial example

100% – adversarial accuracy
adversarial and misclassiﬁed samples

misclassiﬁed samples

Fig. 7. Adversarial accuracy, and share of adversarial and misclassiﬁed
samples are shown for the test data set of the 14 bus test case and different
levels of input perturbation (cid:15). The adversarial accuracy refers to the share
of correctly classiﬁed samples, for which no input perturbation exists which
changes the classiﬁcation within distance (cid:15) of that sample (by solving (9)
for each sample). Please note that both axes are logarithmic, and 100%
minus the adversarial accuracy is shown, i.e. the share of samples which are
not adversarially robust. Out of these, to determine whether an adversarial
example has been identiﬁed, the ground-truth classiﬁcation is computed.

guarantees of the neural network behavior becomes, therefore,
of high importance.

One of the main motivations for data-driven approaches
are their computational speed-up compared to conventional
methods. To assess the computational complexity, we compare
the computational time required for evaluating 1’000 randomly
sampled operating points using AC power ﬂows and the small-
signal model to using the trained neural network to predict
power system security. We ﬁnd that, on average, computing the
AC power ﬂows and small-signal model for all contingencies
takes 0.22 s, and evaluating the neural network 7 × 10−5 s,
translating to a computational speed up of factor 1000 (three
orders of magnitude). Similar computational speed-ups are
reported in [15], [16] for deep learning applications to system
security assessment. Note that the works in [15], [16] do not
provide performance guarantees and do not examine robust-
ness. Contrary, in the following, we provide formal guarantees
for the performance of the security classiﬁer by analysing the
adversarial accuracy and identifying adversarial examples.

3) Evaluating Adversarial Accuracy: Adversarial accuracy
identiﬁes the number of samples whose classiﬁcation changes
from the ground-truth classiﬁcation if we perform a perturba-
tion to their input. Assuming that points in the neighborhood
of a given sample should share the same classiﬁcation, e.g.
points around a ‘safe’ sample would likely be safe, a possible
classiﬁcation change indicates that we are either very close
to the security boundary or we might have discovered an
adversarial example. Carrying out
this procedure for our
whole test dataset, we would expect that in most cases the
classiﬁcation in the vicinity of the sample will not change
(except for the comparably small number of samples that
is close to the security boundary). In Fig. 7 the adversarial
accuracy is depicted for the sparsiﬁed neural network. It can
be observed that for small perturbations, i.e. (cid:15) ≤ 1%, the
adversarial accuracy stays well above 99%; that is, for 99% of
test samples no input exists within distance of (cid:15) which changes
the classiﬁcation. Only if large perturbations to the input are
allowed (i.e. (cid:15) ≥ 1%) the adversarial accuracy decreases.
This shows that the classiﬁcation of our neural network is
adversarially robust in most instances. Note that the adversarial

Fig. 8. Classiﬁcation of 400 new equally spaced samples for the IEEE 14-bus
system. For 2-D visualization purposes, the active power of PG2 and PG5
are ﬁxed to their maximum output. Blue stars mark identiﬁed adversarial
examples. For these samples a small input perturbation exist which falsely
changes the classiﬁcation. The reason is the inaccurate prediction of the system
security boundary.

accuracy only makes a statement regarding the classiﬁcation
by the neural network and is not related to the ground truth
classiﬁcation. In a subsequent step, as we show in the next
paragraph, we need to assess whether identiﬁed samples are
in fact misclassiﬁed.

4) Identifying Adversarial Examples: Having identiﬁed re-
gions where adversarial examples may exist, we can now
proceed with determining if they truly exist. The resulting
share of identiﬁed adversarial examples is shown in Fig. 7.
Focusing on the test samples which are not adversarially robust
for small (cid:15), i.e. (cid:15) ≤ 1%, we identify an adversarial example
for the sample x = [1.0 1.0 0.4444 0.7778] with classiﬁcation
‘safe’. Modifying this input only by (cid:15) = 0.5%, we identify the
adversarial input xadv = [0.9950 0.9950 0.4394 0.7728] which
falsely changes the classiﬁcation to ‘unsafe’, i.e. yadv,1 >
yadv,2. Allowing an input modiﬁcation of magnitude (cid:15) = 1%,
we identify two additional adversarial examples for inputs x =
[1.0 1.0 0.6667 0.5556] and x = [1.0 1.0 0.7778 0.4444] with
classiﬁcation ‘safe’, respectively. These have the correspond-
ing adversarial inputs xadv = [0.9750 0.9750 0.6417 0.5306]
and xadv = [0.9750 0.9750 0.7528 0.4194] which falsely
change the classiﬁcation to ‘unsafe’, respectively.

For illustrative purposes in Fig. 8, we re-sample 400 equally
spaced samples and compute both the neural network predic-
tion and ground truth. In Fig. 8 the location of the adversarial
examples is marked by a star. We can observe that the neural
network boundary prediction is not precise and as a result,
the adversarial inputs get falsely classiﬁed as unsafe. This
highlights the additional beneﬁt of the presented framework
to identify adversarial examples, and subsequently regions in
which additional detailed sampling for re-training the classiﬁer
is necessary.

E. N-1 Security and Uncertainty

For the fourth case study, to further demonstrate scalability
of our methodology, we use the IEEE 162 bus test case
with parameters deﬁned in [36], and train a neural network
to predict power system security with respect
to the N-1
security constrained AC-OPF under uncertainty. Compared to
the previous 14 bus test case, we assume that the voltage set-
points of generators can vary within their deﬁned limits (i.e.
they are part of the input vector x), and we consider both
uncertain injections in power generation and demand. For the
N-1 security assessment, we consider the possible outages
of lines {6, 8, 24, 50, 128}, assuming the same parameters

12

TABLE VI
CONFUSION MATRIX FOR IEEE 162-BUS TEST CASE (N-1 SECURITY
AC-OPF AND UNCERTAINTY)

Test samples: 18204

Predicted: Safe Unsafe Accuracy

True: Safe (4260)
True: Unsafe (13944)

3148
609

1112
13335

Accuracy

90.5%

100

)

%

(

10

0.01

0.1

1

10

(cid:15) (%)

for the outaged system state as for the intact system state.
Furthermore, we place 3 wind farms with rated power of
500 MW and consider 3 uncertain loads with ±50% vari-
ability, i.e., a total of 6 uncertain power injections, at buses
{60, 90, 145, 3, 8, 52}. For all uncertain injections, we assume
a power factor cos φ = 1.

1) Dataset creation: As the resulting input dimension of
the considered test case is high (29 inputs) and large parts
of the input space correspond to infeasible operating points,
a sampling strategy based on Latin hypercube sampling from
the entire input space is not successful at recovering feasible
samples. To create the dataset, we rely on an efﬁcient dataset
creation method proposed in [37]. The dataset creation method
consists of several steps. First, all the upper and lower bounds
in the AC-OPF problem and on the input variables in x are
tightened using bound tightening algorithms in [38] and [39].
Second, relying on infeasibility certiﬁcates based on convex
relaxations of the AC-OPF, large parts of the input space
can be characterized as infeasible, i.e. ‘unsafe’ with respect
to the power system security criteria. These infeasibility
certiﬁcates take the form of hyperplanes and the remaining
unclassiﬁed space can be described as a convex polyhedron
Ax ≤ b. Third, a large number of samples (here 10’000)
are sampled uniformly from inside the convex polyhedron
and their classiﬁcation is computed. For infeasible samples,
the closest feasible sample is determined to characterize the
security boundary. Finally, a Gaussian distribution is ﬁtted to
the feasible boundary samples, and an additional large number
of samples (here 100’000) are sampled from this distribution
and their feasibility to the N-1 security constrained AC-OPF is
assessed. This methodology results to a database of 121’358
samples out of which 23.2% correspond to ‘safe’ operating
points. Note that computing the infeasibility certiﬁcates allows
to reduce the considered input volume from a normalized
volume of 1 (i.e. all bounds on x are between 0 and 1) to
a volume of 6 × 10−10. This highlights the need for advanced
methods for dataset creation, as direct sampling strategies
are not able to produce balanced datasets balanced of ‘safe’
and ‘unsafe’ operating points. For more details on the dataset
creation method, for brevity, the reader is referred to [37].

2) Neural Network Training and Adversarial Robustness:
We train a neural network with 3 hidden layers of 100 neurons
each and enforce a weight sparsity of 80%. The neural network
has an accuracy of 91.3% on the training dataset and 90.5%
on the test dataset. The confusion matrix for the test dataset is
shown in Table VI. Similar to previous test cases, we evaluate
the adversarial accuracy in Fig. 9. We ﬁnd that the neural
network is not adversarially robust, already for an input mod-
iﬁcation of (cid:15) = 0.1% for 4.2% of test samples an adversarial

100% – adversarial accuracy
adversarial and misclassiﬁed samples

misclassiﬁed samples

Fig. 9. Adversarial accuracy, and share of adversarial and misclassiﬁed
samples are shown for the test data set of the 162 bus test case (N-1 security
and uncertainty) and different levels of input perturbation (cid:15). The adversarial
accuracy refers to the share of correctly classiﬁed samples, for which no
input perturbation exists which changes the classiﬁcation within distance (cid:15)
of that sample (by solving (9) for each sample). Please note that both axes
are logarithmic, and 100% minus the adversarial accuracy is shown, i.e.
the share of samples which are not adversarially robust. Out of these, to
determine whether an adversarial example has been identiﬁed, the ground-
truth classiﬁcation is computed.

example is identiﬁed (in addition to the initially misclassiﬁed
9.5% of test samples). For an input modiﬁcation of (cid:15) = 1% this
value increases to 31.6%. This systematic process to identify
adversarial examples proposed in this paper allows us to obtain
additional insights about the quality of the training database.
Assessing the adversarial accuracy (i.e. the number of samples
that change classiﬁcation within a distance (cid:15)) versus the actual
adversarial examples, we ﬁnd that the change in classiﬁcation
in this case often occurs because the operating points have
been moved across the true system security boundary. This
indicates that many samples are placed close to the correctly
predicted true system boundary in the high-dimensional space.
Using high-performance computing, an additional detailed re-
sampling of the system security boundary or re-training by
including adversarial examples as shown in Section V-B5
could improve neural network robustness.

VI. CONCLUSION

Neural networks in power system applications have so far
been treated as a black box; this has become a major barrier
towards their application in practice. This is the ﬁrst work
to present a rigorous framework for neural network veriﬁca-
tion in power systems and to obtain provable performance
guarantees. To this end, we formulate veriﬁcation problems
as mixed-integer linear programs and train neural networks to
be computationally easier veriﬁable. We provably determine
the range of inputs that are classiﬁed as safe or unsafe,
and systematically identify adversarial examples, i.e. slight
modiﬁcations in the input that lead to a mis-classiﬁcation
by neural networks. This enables power system operators
to understand and anticipate the neural network behavior,
building trust in them, and remove a major barrier toward their
adoption in power systems. We verify properties of a security
classiﬁer for an IEEE 9-bus system, improve its robustness and
demonstrate its scalability for a 162-bus system, highlighting
the need for sparsiﬁcation of neural network weights. Finally,
we further identify adversarial examples and evaluate the
adversarial accuracy of neural networks trained to assess N-1
security under uncertainty and small-signal stability.

APPENDIX A
OPTIMAL POWER FLOW (OPF) FORMULATIONS

In the following, for completeness, we provide a brief
overview of the DC and AC optimal power ﬂow formulations
including N-1 security and small-signal stability. For more
details please refer to [40]–[42].

A. Preliminaries

We consider a power grid which consists of buses (denoted
with the set N ) and transmission lines (denoted with the
set L). The transmission lines connect one bus i ∈ N to
another bus j ∈ N ,
i.e., (i, j) ∈ L. For the AC-OPF
formulation, we consider the following variables for each bus:
The voltage magnitudes V , the voltage angles θ, the active
power generation PG, the reactive power generation QG, the
active wind power generation PW , and the active and reactive
power demand PL and QL. Each of these vectors have the
size nb×1, where nb is the number of buses in the set N . Note
that during operation, for one speciﬁc instance, the active wind
power generation and active and reactive power demand are
assumed to be ﬁxed (and the curtailment of wind generators
and load shedding are to be avoided at all times). To comply
with the N-1 security criterion, we consider the possible single
outage of a set of lines, which we denote with the set C. The
ﬁrst element of this set corresponds to the intact system state,
denoted with ’0’. The superscript ’c’ denotes the variables
corresponding to the intact and outaged system states.

B. N-1 Security-Constrained DC-OPF

In Section V-B and Section V-C, we create datasets of
operating points classiﬁed according to their feasibility to
the N-1 security constrained DC-OPF. The DC-OPF approx-
imation neglects reactive power and active power losses, and
assumes that the voltage magnitudes V are ﬁxed, for a detailed
treatment please refer to [41]. Considering a set C of possible
single line outages, the preventive N-1 security constrained
DC-OPF problem can then be formulated as follows:

min
G,PL,PW ,θc

P c

f (P 0
G)

s.t. P c

busθc ∀c ∈ C
∀c ∈ C

lineθc ≤ P max,c

line

G + PW − PL = Bc
line ≤ Bc
G ≤ P c
G ≤ P max
W ≤ PW ≤ P max
W
L ≤ PL ≤ P max

P min,c
P min
P min
P min
P wosb,0
G

L
= P wosb,c
G

G

∀c ∈ C

∀c ∈ C

(14)

(15)

(16)

(17)

(18)

(19)

(20)

The objective function f minimizes e.g. the generation cost
of the intact system state. The nodal power balance in (15)
has to be satisﬁed for the intact and outaged system states.
The bus and line admittance matrices are denoted with Bbus
and Bline, respectively. Upper and lower limits are enforced
for the active power line ﬂows, generation, wind power, and
load demands in (16), (17), (18) and (19), respectively. The
constraint in (20) enforces preventive control action of gener-
ators. The superscript ’wosb’ denotes all generators except the

13

G

G

slack bus generator. The independent variables characterizing
an operating point are x := [P wosb,0
, PW , PL]. To create
datasets, we compute the classiﬁcation for each operation
point by ﬁrst running DC power ﬂows to determine θc and
the slack bus generator dispatch P slack,c
for the intact and
each outaged system states. The superscript ‘slack’ denotes
the slack bus. Then, we check satisfaction of the constraints
on active generator power (17) and active line ﬂows (16). In
operations, it is usually assumed that both the wind power and
W = P min
loading are ﬁxed, i.e. P max
L . Here,
we model them as variables to be able to compute the ground-
truth for the region around an infeasible sample xinfeas in which
no feasible sample exist. To this end, we solve the following
optimization problem computing the minimum distance from
the infeasible sample to an operating point that is feasible to
the N-1 security-constrained DC-OPF:

W and P max

L = P min

min
G,PL,PW ,θc

P c

|x − xinfeas|∞

s.t. (15) − (20)

x = [P wosb,0

G

, PW , PL]

(21)

(22)

(23)

As this optimization problem is convex, we can provably
identify the closest feasible sample x to xinfeas. Note that we
solve this optimization problem to compute the results denoted
with ’SC-DC-OPF’ in Table II and Table IV.

C. N-1 Security-Constrained AC-OPF

In Section V-D and Section V-E, we create datasets of
operating points classiﬁed according to their feasibility to
the N-1 security constrained AC-OPF. In Section V-D, we
consider small-signal stability constraints as well. These will
be discussed in the Appendix A-D. We can formulate the N-1
security constrained AC-OPF problem as follows:

(24)

G, Qc

G, PW , PL, QL} ∀c ∈ C (25)
∀c ∈ C (26)
∀c ∈ C (27)

min
zc

f (P0
g)
s.t. zc := {V c, θc, P c
sbalance(zc) = 0
|sline(zc)| ≤ smax,c
line
G ≤ P c
G ≤ P max
P min
G ≤ Qc
G ≤ Qmax
Qmin
G
W ≤ PW ≤ P max
P min
W
L ≤ PL ≤ P max
P min
Qmin
L ≤ QL ≤ Qmax
L
Vmin ≤ Vc ≤ Vmax
P wosb,0
G

= P wosb,c
G

G

L

∀c ∈ C (28)

∀c ∈ C (29)

(30)

(31)

(32)

∀c ∈ C (33)

, V c

∀c ∈ C (34)

G = V c
G
The vector zc collects all variables for the intact and outaged
system states in (25). The non-linear AC power ﬂow nodal bal-
ance sbalance in (26) has to hold for intact and outaged system
states. The absolute apparent line ﬂow |sline| is constrained by
an upper limit in (27). For the full mathematical formulation,
for brevity, please refer to [40]. The upper and lower limits
on the system variables are deﬁned in the constraints (28) –
(33). The constraint (34) enforces preventive control actions
for N-1 security: Both the generator active power set-points

G

, PW , PL, QL, V 0

and voltage set-points remain ﬁxed during an outage. Note that
the vector VG refers to the voltage set-points of the generators.
We do not ﬁx the entry in PG corresponding to the slack
bus, as the slack bus generator compensates the mismatch
in the losses. The independent variables that characterize
an operating point are x := [P wosb,0
G].
To create the datasets, based on the operating point x, we
solve the AC power ﬂow equations in (26) to determine
the dependent variables for each contingency and the intact
system state. Then, we check the satisfaction of the operational
constraints of the N-1 SC-AC-OPF problem including active
and reactive generator limits (28) and (29), apparent branch
ﬂow limits (27), and voltage limits (33). For an operating
point to be classiﬁed as feasible to the the N-1 SC-AC-OPF
problem, it must satisfy all these constraints for all considered
contingencies.

D. Small-Signal Stability

For the IEEE 14 bus test case in Section V-D we evaluate the
feasibility of operating points with respect to combined small-
signal stability and N-1 security. For the small signal stability
model, we rely on standard system models and parameters
commonly used for small signal stability analysis deﬁned
in Refs. [43] and [44]. We use a sixth-order synchronous
machine model with an Automatic Voltage Regulator (AVR)
Type I with three states. A more detailed description of the
system model and parameters can be found in the Appendix
of Ref. [45]. To determine small-signal stability, we linearize
the dynamic model of the system around the current operating
point, and compute the eigenvalues λ of the resulting system
matrix A. If all eigenvalues have negative real parts (i.e. lie
in the left-hand plane), the system is considered small-signal
stable, otherwise unstable. This can be formalized as follows:

A(zc)νc = λcνc ∀c ∈ C
λc ≤ 0
∀c ∈ C

(35)

(36)

The set of variables zc is deﬁned in (25). The term νc denotes
the right-hand eigenvectors of the system matrix A. As we
consider both N-1 security and small-signal stability, we have
to modify the small-signal stability model for each operating
point and contingency. We use Mathematica to derive the
small signal model symbolically, MATPOWER AC power
ﬂows to initialize the system matrix, and Matlab to compute
its eigenvalues and damping ratio, and assess the small-signal
stability for each operating point and contingency.

REFERENCES

[1] L. Duchesne, E. Karangelos, and L. Wehenkel, “Recent developments
in machine learning for energy systems reliability management,” Pro-
ceedings of the IEEE, 2020.

[2] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing
adversarial examples,” International Conference on Learning Represen-
tations (ICLR 2015), 2015.

[3] L. A. Wehenkel, Automatic learning techniques in power systems.

Springer Science & Business Media, 2012.

[4] M. Glavic, “(deep) reinforcement learning for electric power system
control and related problems: A short review and perspectives,” Annual
Reviews in Control, 2019.

14

[5] L. Duchesne, E. Karangelos, and L. Wehenkel, “Using machine learning
to enable probabilistic reliability assessment in operation planning,” in
2018 Power Systems Computation Conference.

IEEE, 2018.

[6] R. Dobbe, O. Sondermeijer, D. Fridovich-Keil, D. Arnold, D. Callaway,
and C. Tomlin, “Towards distributed energy services: Decentralizing
optimal power ﬂow with machine learning,” IEEE Transactions on Smart
Grid, pp. 1–1, 2019.

[7] S. Karagiannopoulos, P. Aristidou, and G. Hug, “Data-driven local
control design for active distribution grids using off-line optimal power
ﬂow and machine learning techniques,” IEEE Transactions on Smart
Grid, vol. 10, no. 6, pp. 6461–6471, Nov 2019.

[8] G. Dalal, E. Gilboa, S. Mannor, and L. Wehenkel, “Chance-constrained
outage scheduling using a machine learning proxy,” IEEE Transactions
on Power Systems, 2019.

[9] S. R, R. S. Kumar, and A. T. Mathew, “Online static security assessment
module using artiﬁcial neural networks,” IEEE Transactions on Power
Systems, vol. 28, no. 4, pp. 4328–4335, Nov 2013.

[10] V. J. Gutierrez-Martinez, C. A. Canizares, C. R. Fuerte-Esquivel,
A. Pizano-Martinez, and X. Gu, “Neural-network security-boundary
constrained optimal power ﬂow,” IEEE Transactions on Power Systems,
vol. 26, no. 1, pp. 63–72, Feb 2011.

[11] F. Li and Y. Du, “From alphago to power system ai: What engineers
can learn from solving the most complex board game,” IEEE Power and
Energy Magazine, vol. 16, no. 2, pp. 76–84, March 2018.

[12] B. Donnot, I. Guyon, M. Schoenauer, P. Panciatici, and A. Marot,
“Introducing machine learning for power system operation support,” X
Bulk Power Systems Dynamics and Control Symposium, 2017.

[13] B. Donnot, I. Guyon, M. Schoenauer, A. Marot, and P. Panciatici, “Fast
power system security analysis with guided dropout,” 26th European
Symposium on Artiﬁcial Neural Networks, 2018.

[14] M. Sun, I. Konstantelos, and G. Strbac, “A deep learning-based feature
extraction framework for system security assessment,” IEEE Transac-
tions on Smart Grid, 2018.

[15] J.-M. H. Arteaga, F. Hancharou, F. Thams, and S. Chatzivasileiadis,
“Deep learning for power system security assessment,” in 13th IEEE
PowerTech 2019.

IEEE, 2019.

[16] Y. Du, F. F. Li, J. Li, and T. Zheng, “Achieving 100x acceleration for n-1
contingency screening with uncertain scenarios using deep convolutional
neural network,” IEEE Transactions on Power Systems, 2019.

[17] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and
A. Swami, “The limitations of deep learning in adversarial settings,” in
2016 IEEE European Symposium on Security and Privacy (EuroS&P).
IEEE, 2016, pp. 372–387.

[18] Y. Chen, Y. Tan, and D. Deka, “Is machine learning in power systems

vulnerable?” in 2018 IEEE SmartGridComm.

IEEE, 2018, pp. 1–6.

[19] V. Tjeng, K. Y. Xiao, and R. Tedrake, “Evaluating robustness of neural
networks with mixed integer programming,” in International Conference
on Learning Representations (ICLR 2019), 2019.

[20] C. Bishop, Pattern Recognition and Machine Learning. Springer, 2006.
[21] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521,

pp. 436–444, 2015.

[22] X. Glorot, A. Bordes, and Y. Bengio, “Deep sparse rectiﬁer neural
networks,” in Proceedings of the fourteenth international conference on
artiﬁcial intelligence and statistics, 2011, pp. 315–323.

[23] H. Zhang, T.-W. Weng, P.-Y. Chen, C.-J. Hsieh, and L. Daniel, “Ef-
ﬁcient neural network robustness certiﬁcation with general activation
functions,” in Advances in Neural Information Processing Systems 31,
2018, pp. 4939–4948.

[24] D. M. Kline and V. L. Berardi, “Revisiting squared-error and cross-
entropy functions for training neural network classiﬁers,” Neural Com-
puting & Applications, vol. 14, no. 4, pp. 310–318, 2005.

[25] M. Abadi et al., “TensorFlow: Large-scale machine learning on
heterogeneous systems,” 2015, software available from tensorﬂow.org.
[Online]. Available: https://www.tensorﬂow.org/

[26] X. Glorot and Y. Bengio, “Understanding the difﬁculty of training
deep feedforward neural networks,” in Proceedings of the thirteenth
international conference on artiﬁcial intelligence and statistics, 2010,
pp. 249–256.

[27] M. Wu et al., “A game-based approximate veriﬁcation of deep neural
networks with provable guarantees,” Theoretical Computer Science,
2019.

[28] F. Croce and M. Hein, “Provable robustness against all adversarial l p-
perturbations for p ≥ 1,” arXiv preprint arXiv:1905.11213, 2019.
[29] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards
deep learning models resistant to adversarial attacks,” in International
Conference on Learning Representations (ICLR 2018), 2018.

15

Andreas Venzke (S’16) received the M.Sc. degree in
Energy Science and Technology from ETH Zurich,
Zurich, Switzerland in 2017. He is currently work-
ing towards the Ph.D. degree at
the Department
of Electrical Engineering, Technical University of
Denmark (DTU), Kongens Lyngby, Denmark. His
research interests include power system operation
under uncertainty, convex relaxations of optimal
power ﬂow and machine learning applications for
power systems.

Spyros Chatzivasileiadis (S’04, M’14, SM’18) is
an Associate Professor at the Technical University
of Denmark (DTU). Before that he was a post-
doctoral researcher at the Massachusetts Institute of
Technology (MIT), USA and at Lawrence Berkeley
National Laboratory, USA. Spyros holds a PhD from
ETH Zurich, Switzerland (2013) and a Diploma
in Electrical and Computer Engineering from the
National Technical University of Athens (NTUA),
Greece (2007). In March 2016, he joined the Center
for Electric Power and Energy at DTU. He is cur-
rently working on power system optimization and control of AC and HVDC
grids, and machine learning applications for power systems.

[30] F. Thams, A. Venzke, R. Eriksson, and S. Chatzivasileiadis, “Efﬁcient
database generation for data-driven security assessment of power sys-
tems,” IEEE Transactions on Power Systems, 2019.

[31] M. Zhu and S. Gupta, “To prune, or not

to prune: exploring the
efﬁcacy of pruning for model compression,” International Conference
on Learning Representations (ICLR 2018), 2018.

[32] K. Dvijotham, R. Stanforth, S. Gowal, T. Mann, and P. Kohli, “A dual
approach to scalable veriﬁcation of deep networks,” in Conference on
Uncertainty in Artiﬁcial Intelligenc (UAI-18). Corvallis: AUAI Press,
2018, pp. 162–171.

[33] K. Y. Xiao, V. Tjeng, N. M. M. Shaﬁullah, and A. Madry, “Training
for faster adversarial robustness veriﬁcation via inducing reLU stability,”
in International Conference on Learning Representations (ICLR 2019),
2019.

[34] R. D. Zimmerman, C. E. Murillo-S´anchez, and R. J. Thomas, “Mat-
power: Steady-state operations, planning, and analysis tools for power
systems research and education,” IEEE Transactions on Power Systems,
vol. 26, no. 1, pp. 12–19, 2010.

[35] M. D. McKay, R. J. Beckman, and W. J. Conover, “Comparison of three
methods for selecting values of input variables in the analysis of output
from a computer code,” Technometrics, vol. 21, no. 2, pp. 239–245,
1979.

[36] IEEE PES Task Force on Benchmarks for Validation of Emerging
Power System Algorithms, “The power grid library for benchmarking
AC optimal power ﬂow algorithms,” arXiv:1908.02788, Aug. 2019.
[Online]. Available: https://github.com/power-grid-lib/pglib-opf
[37] A. Venzke, D. K. Molzahn, and S. Chatzivasileiadis, “Efﬁcient creation
of datasets for data-driven power system applications,” XXI Power
Systems Computation Conference (PSCC 2020), 2020.

[38] D. Shchetinin, T. T. De Rubira, and G. Hug, “Efﬁcient bound tightening
techniques for convex relaxations of AC optimal power ﬂow,” IEEE
Transactions on Power Systems, vol. 34, no. 5, pp. 3848–3857, 2019.

[39] K. Sundar et al., “Optimization-based bound tightening using a
the optimal power ﬂow problem,”

strengthened QC-relaxation of
arXiv:1809.04565, 2018.

[40] M. B. Cain, R. P. Oneill, and A. Castillo, “History of optimal power
ﬂow and formulations,” Federal Energy Regulatory Commission, vol. 1,
pp. 1–36, 2012.

[41] B. Stott, J. Jardim, and O. Alsac¸, “Dc power ﬂow revisited,” IEEE

Transactions on Power Systems, vol. 24, no. 3, pp. 1290–1300, 2009.

[42] F. Capitanescu, J. M. Ramos, P. Panciatici, D. Kirschen, A. M. Mar-
colini, L. Platbrood, and L. Wehenkel, “State-of-the-art, challenges, and
future trends in security constrained optimal power ﬂow,” Electric Power
Systems Research, vol. 81, no. 8, pp. 1731–1741, 2011.

[43] P. W. Sauer and M. A. Pai, Power system dynamics and stability.

Prentice hall Upper Saddle River, NJ, 1998, vol. 101.

[44] F. Milano, Power system modelling and scripting. Springer Science &

Business Media, 2010.

[45] F. Thams, L. Halilbasic, P. Pinson, S. Chatzivasileiadis, and R. Eriks-
son, “Data-driven security-constrained opf,” in X Bulk Power Systems
Dynamics and Control Symposium, 2017.

