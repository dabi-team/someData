1
2
0
2

g
u
A
6
2

]

G
L
.
s
c
[

2
v
1
2
9
0
1
.
8
0
1
2
:
v
i
X
r
a

16-811 Math Fundamentals Project 1–12, 2020

16-811 Math Fundamentals Project

The Word is Mightier than the Label
Learning without Pointillistic Labels using Data
Programming

Chufan Gao
Mononito Goswami
Robotics Institute, Carnegie Mellon University

chufang@andrew.cmu.edu
mgoswami@andrew.cmu.edu

Abstract
Most advanced supervised Machine Learning (ML) models rely on vast amounts of point-
by-point labelled training examples. Hand-labelling vast amounts of data may be tedious,
expensive, and error-prone. Recently, some studies have explored the use of diverse sources
of weak supervision to produce competitive end model classiﬁers. In this paper, we survey
recent work on weak supervision, and in particular, we investigate the Data Programming
(DP) framework (Ratner et al., 2016). Taking a set of potentially noisy heuristics as
input, DP assigns denoised probabilistic labels to each data point in a dataset using a
probabilistic graphical model of heuristics. We analyze the math fundamentals behind
DP and demonstrate the power of it by applying it on two real-world text classiﬁcation
tasks. Furthermore, we compare DP with pointillistic active and semi-supervised learning
techniques traditionally applied in data-sparse settings.
Keywords: Data programming, weak supervision, active learning, semi-supervised learn-
ing, text classiﬁcation

1. Introduction

Advances in supervised machine learning (ML) have enabled ML models to perform at par
with humans in several classiﬁcation and decision making settings. However, most advanced
ML models such as deep neural networks (NNs) rely on vast quantities of pointillistically
labelled training data. While unlabelled raw data is abundant, hand-labelling large quanti-
ties of data is tedious, expensive, time intensive and prone to error. For instance, (Hannun
et al., 2019) recently developed a model to diagnose irregular heartbeats (arrhythmia) from
ECG signals, better than individual cardiologists1. However, their Convolutional Neural
Network (CNN) was trained on as many as 64, 121 ECG records from 29, 163 patients, each
hand-labelled by an expert cardiologist. Another example is the prostate, lung, colorec-
tal and ovarian (PLCO) cancer screening trial of the National Cancer Institute, a dataset
of several thousand chest radiographs collected over 13 years (Team et al., 2000). The
healthcare domain is replete with ML models achieving amazing results, limited only by
availability of large, cleaned and annotated datasets. Additionally, in many other cases,
obtaining millions of unlabelled images from cameras, videos, and vast quantities of textual
data from the web is straightforward – the burden lies in annotation.

Many ML techniques have been devised to train competitive models by actively and
intelligently sampling training data (Settles, 2009), combining both unlabelled and labelled

1. https://stanfordmlgroup.github.io/projects/ecg/

© 2020 C. Gao & M. Goswami.

 
 
 
 
 
 
Learning without Pointillistic Labels

data (Van Engelen and Hoos, 2020) or harnessing crowd-sourced labels (Gao et al., 2011).
In this work, we explore the use of diverse sources of weak supervision to train competitive
end model classiﬁers. We use the recently introduced data programming framework to
intelligently combine these weak supervision sources to probabilistically label data and
train competitive downstream classiﬁers without access to ground truth labels (Ratner
et al., 2016).

Our major contribution in this study is to come up with simple ways to automatically
acquire weak supervision sources for text classiﬁcation problems with minimal human ef-
fort. Furthermore, we compare end model classiﬁers obtained using weak supervision, full
supervision, active learning and semi-supervised learning. Our experiments on the widely
used IMDb review sentiment classiﬁcation dataset (Maas et al., 2011) and the 20 News-
groups dataset (Lang, 1995), reveal that weak supervision performs at par with its fully
supervised counterpart, and outperforms alternatives such as active and semi-supervision.
Our results are also consistent with prior work which show that keywords (unigrams) are
excellent sources of weak supervision (Boecking and Dubrawski, 2019; Ratner et al., 2016).
The rest of the paper is organised as follows. In section 2 we do a brief literature survey
of current approaches to ML data-sparse settings. Section 3 brieﬂy overviews the data
programming label framework and label model. In section 4, we describe our experimental
setup, the acquisition of weak supervision sources and datasets. We conclude the paper
with discussion of our results in Section 5.

2. Prior Work and Alternative Approaches

In this section, we will overview 2 baseline techniques commonly used in machine learning
with less labels.

2.1. Active Learning

The active learning process is straightforward–the dataset is split into two subsets, labeled
and unlabeled. The algorithm is trained on the labeled dataset, where it predicts labels
on the unlabeled dataset. Depending on the conﬁdence on the predicted labels, queries are
made for the expert to label more data. The new labeled data is then added to the existing
labeled dataset. Algorithm 1 shows the complete psuedocode.

Additionally, there are multiple methods in considering the “conﬁdence” on predicted
labels. One of which is entropy based, where we measure the predicted labels based on
Shannon Entropy H(x) = − (cid:80) P (x)log(P (x)), where we ask the oracle to label high entropy
label predictions. The other is simple taking the prediction probability of the prediction
labels and asking the oracle to label low probability label predictions.

2.2. Semi-supervised learning

Semi-supervised learning is very similar to Active Learning, but with one key diﬀerence.
The problem setup is the same, where we split the dataset into unlabeled and labeled
subsets. For each iteration of semi-supervised learning, the model is trained on the labeled
dataset, then is used to predict labels in the unlabeled dataset. The label predictions with

2

Learning without Pointillistic Labels

Let S = {(xi, yi)}n
i=1, (x, y) ∼ X × Y ∪ {−1} represent the training dataset, where −1
represents the unlabelled data points. We can then partition S into disjoint labelled
Sl = {(x, y)} such that y ∈ Y and unlabelled subsets Su = {(x, y)} such that y = −1.
Let T represent the predeﬁned number of active learning iterations. Our goal is to ﬁnd a
hypothesis hT ∈ H which minimizes empirical risk under an arbitrary loss function l.

for t ← 0 in T do
ht = arg min

ht∈H

// predict proba returns probabilistic

E(ht(x),y)∈S[l(x, y)]
P = ht.predict proba(Su)
(p(y = 1), p(y = 0)) for each (x, y) ∈ Su
Xquery = acquisition function(P) // Xquery is a set of unlabelled data points.
Yquery = get labels(Xquery) // get labels is a function to acquire labels from the
oracle
Sl := Sl ∪ {Xquery, Yquery}
Su := Su − {Xquery, Yquery}

labels

end

Algorithm 1: Active learning pseudocode

Let S = {(xi, yi)}n
i=1, (x, y) ∼ X × Y ∪ {−1} represent the training dataset, where −1
represents the unlabelled data points. We can then partition S into disjoint labelled
Sl = {(x, y)} such that y ∈ Y and unlabelled subsets Su = {(x, y)} such that y = −1.
Let T represent the predeﬁned number of active learning iterations. Our goal is to ﬁnd a
hypothesis hT ∈ H which minimizes empirical risk under an arbitrary loss function l. Let
C be the cutoﬀ threshold.

for t ← 0 in T do
ht = arg min

ht∈H

E(ht(x),y)∈S[l(x, y)]
P = ht.predict proba(Su)
(p(y = 1), p(y = 0)) for each (x, y) ∈ Su
Xconf ident = Su[argmax(P) > C] is the data points corresponding conﬁdent predictions.
Yconf ident = P[argmax(P) > C] // the predicted labels
Sl := Sl ∪ {Xconf ident, Yconf ident}
Su := Su − {Xconf ident, Yconf ident}

// predict proba returns probabilistic

labels

end

Algorithm 2: Semi-supervised learning pseudocode

the highest conﬁdence is added to the labeled dataset with the predicted labels (without
any input from an oracle). Algorithm 2 shows the complete psuedocode.

3. Data Programming

In this section, we shall provide a brief overview of the data programming model. Figure 1)
shows the overall advantages of this weak supervision model compared to full supervision.

3

Learning without Pointillistic Labels

Figure 1: Flowchart of the Data Programming process compared to traditional supervised
machine learning. The orange boxes indicate the eﬀort required by the users
(annotators). Instead of having to label everything by hand, the work lies only
in deﬁning some labeling functions, which is much more eﬃcient.

For the remainder of the section, we will explain the mathematical background behind data
programming.

Consider a binary classiﬁcation task in which we have a distribution π over object-class
pairs (x, y) ∈ X × {0, 1}. To specify a data programming model, users deﬁne a number of

labelling functions (LFs), λ
label subsets of the data.

def
= X (cid:55)→ {−1, 0, 1}, which encode domain knowledge to noisily

Given n i.i.d training object Sx = {x1, x2, . . . , xn} ∈ X and a set of m labelling functions
Λ = {λ1, . . . , λm}, data programming infers probabilistic labels Y ∈ [0, 1]n for all training
objects in Sx. We can then use these probabilistic labels to train a downstream classiﬁer
via Empirical Risk Minimization (ERM) under a noise-aware loss. Given training data of
the form S = ((x1, y1), . . . , (xn, yn)), xi ∈ X , yi ∈ {0, 1}, the ERM algorithm outputs the
hypothesis h ∈ H which minimizes empirical risk:

ˆh = arg min

h∈H

1
|S|

(cid:88)

x,y ∈ S

l(h(x), y)

where l can be any loss function. Due to the probabilistic nature of the labels Y inferred by
data programming label model, we can instead minimize a noise-aware empirical risk, which
weighs the loss incurred by training objects in proportion to the label model’s conﬁdence
on its own label assignment to the object. In other words, a noise-aware loss may give more
weight to those training objects which the label model is conﬁdent about (having higher
values of y ∈ Y).

4

Learning without Pointillistic Labels

3.1. Label Model

Now we will describe the data programming label model which assumes that the LFs Λ
label independently given the true latent class labels Y . Under this model, let us assume
that each LF λi labels an object with probability βi, and with probability αi labels the
object correctly. We also assume that the positive and negative class are equally prevalent,
i.e. each class has a probability of 1

2 . This model has the following distribution:

µα,β(Λ, Y ) =

1
2

m
(cid:89)

i=1

(βiαi1Λi=Y + βi(1 − αi)1Λi=−Y + (1 − βi)1Λi=0)

(1)

where Λ = {−1, 0, 1}m are the labels output by the LFs and Y ∈ {−1, 1} are the latent
predicted class labels. When we allow the parameters α ∈ Rm and β ∈ Rm to vary, Eqn. 1
deﬁnes a family of generative models.

Our goal is to learn the parameters α and β which are the most consistent with our
observations i.e. the unlabelled training data Sx, using maximum likelihood estimation.
For a particular training set Sx ∈ X , we solve the following problem:
log P(Λ,Y )∼µα,β (Λ = λ(x))

(ˆα, ˆβ) = arg max

(cid:88)

(2)

α,β

x∈Sx

Hence, our goal is to maximize the likelihood that the observed labels (λ(x)) occur under
the generative model deﬁned in Eqn. 1.

3.2. From Noisy LF votes to Probabilistic Labels

The noisy labels Λ from the LFs can be aggregated using an arbitrary rule into “denoised”
probabilistic labels. The quality of the ﬁnal labels, in addition to the LFs, also depends on
the aggregation rule (Li, 2015). Majority voting is a natural choice for an aggregation rule.
While majority voting often works well in practice, it weighs each LF the same. However,
in practice we may want to weigh more accurate LFs more than the less accurate ones.
Hence, data programming uses the weighted majority vote, weighing each vote of a LF λi
by its accuracy αi.

Before formally describing the weighted aggregation rule, we shall assume the following
notation. Let [m] = {1, 2, ...m} and ˆyi denote the predicted label of xi ∈ Sx. Also let
Vn×m ∈ {−1, 0, 1}n×m denote the vote matrix, such that vij ∈ {−1, 0, 1} corresponds to
the vote of λj ∈ Λ on xi ∈ Sx. Then weighted majority aggregation rule can be formally
deﬁned as:

m
(cid:88)

ˆyi = sign(

wjVij)

(3)

We can derive the majority vote aggregation rule by setting wj = 1.
In addition to the predicted label ˆyi, data programming also evaluates probabilistic
labels given by the pair (P( ˆyi = 1), P( ˆyi = −1)). These probabilitis are computed using the
softmax function as follows:

j=1

P( ˆyi = 1) =

exp ((cid:80)m

j=1 wj1Vij =1)
j=1 wj1Vij =1) + exp ((cid:80)m

exp ((cid:80)m

j=1 wj1Vij =−1)

P( ˆyi = −1) = 1 − P( ˆyi = 1)

5

(4)

(5)

Learning without Pointillistic Labels

Algorithm

Hyper-parameter Value

Random forests

Active learning

Semi-sup. learning

Weak supervision

# estimators
Min. samples in a leaf
Seed samples
Query size
# iterations
Seed samples
Batch size
# iterations
Optimizer
Learning rate

500
5
20
1
1000
1000
10
25
SGD
0.01

Table 1: Hyper-parameters of various models.

and ˆαj is derived from the label model in Eqn. 2.

where wj = ˆαj
(cid:80)m

j=1 ˆαj

4. Experiments

In order to evaluate the eﬃcacy of data programming, we compared it with four baselines,
namely, fully supervised classiﬁcation (FS), active learning (AL), semi-supervised learning
(SSL) and majority vote (Maj.) label model. For all our experiments, we used the ran-
dom forest implementation of scikit-learn2 comprising of 500 decision tree estimators.
We chose random forest over other classiﬁers because of its ability to learn complex deci-
sion boundaries without overﬁtting to training data (Goswami et al., 2020a,b). We used
document embeddings from a pre-trained BERT model (Reimers and Gurevych, 2019)3 as
featurization for our classiﬁers.

We implemented two variants of active learning with uncertainty sampling, namely en-
tropy-based and least conﬁdence (Settles, 2009). Both the implementations were identical4
except the uncertainty measure used to query the most informative samples. While the
former queried the instance having the highest entropy (Shannon, 2001), the latter queried
the instance whose best labelling is the least conﬁdent i.e. choosing the instance with poste-
rior closest to 0.5 for binary classiﬁcation. All our active learning experiments started with
a randomly chosen class-balanced seed set of 20 labelled objects. In each active learning
iteration, the object with the highest entropy or least conﬁdence was chosen to be labelled
by the expert, for a total of 1000 active learning iterations.

On the other hand, our semi-supervised model used the self-labelling strategy (Van En-
gelen and Hoos, 2020). For our semi-supervised model we started with a randomly chosen
In each semi-supervision iteration, we
class-balanced seed set of 1000 labelled objects.
added a batch of 10 most conﬁdently labelled objects to the training set. We repeated this
procedure for a total of 25 semi-supervision iterations. The hyper-parameters of all our
models are listed in Tab. 1.

Furthermore, recall that our label model returns probabilistic labels of the form (P( ˆyi =
1), P( ˆyi = −1)). We can then formulate the conﬁdence Ci ∈ [0, 1] of the label model on its

2. https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.

RandomForestClassifier.html

3. Speciﬁcally, we used the sentence-transformers (https://www.sbert.net/) Python package.
4. For binary classiﬁcation both variants are equivalent.

6

Learning without Pointillistic Labels

Figure 2: Examples of keyword labelling functions for both the IMDb and Newsgroups

datasets.

label prediction on xi as:

Ci = 2 ×

(cid:18)

max(P( ˆyi = 1), P( ˆyi = −1)) −

(cid:19)

1
2

(6)

where ˆyi is the label predicted by the label model. Using these conﬁdence scores, we
can weigh conﬁdently labelled objects more than the uncertain ones. Since we could not
introduce sample weights into scikit-learn’s random forest implementation, we instead
only trained our weakly supervised model on samples with a conﬁdence score greater than
C = 0.6.

4.1. Datasets

In this study, we use two datasets, the IMDb Large Movie Reviews (IMDb)5 (Maas et al.,
2011) and the 20 Newsgroups dataset6 (Newsgroups) (Lang, 1995). The IMDB dataset is
for binary sentiment classiﬁcation dataset comprising of 25, 000 highly polar positive and
negative movie reviews for training and an equal number for testing. The 20 Newsgroups
dataset is a collection of approximately 20, 000 newsgroup documents from 20 categories.
In this study, we use a subset of 4 categories7 for a four class text classiﬁcation task. In
total, for the 20 Newsgroups dataset, we had 2,508 training samples and 1,669 test samples.

4.2. Weak supervision sources

For both the IMDb and Newsgroups datasets, we used simple keyword LFs as shown in Fig. 2.
Keywords and key phrases have been shown to be excellent sources of weak supervision.
To derive LFs we ﬁrst found an exhaustive set of top 95-percentile frequent unigrams. In
the next step, we assigned these candidate keywords to categories (classes) either based on
existing lexicons or similarity functions.

For the IMDb dataset, we used the VADER (Valence Aware Dictionary and sEntiment
Reasoner) lexicon (Gilbert and Hutto, 2014) to classify the candidate keywords as being

5. https://ai.stanford.edu/~amaas/data/sentiment/
6. http://qwone.com/~jason/20Newsgroups/
7. comp.windows.x, talk.politics.misc, sci.space, talk.religion.misc

7

Learning without Pointillistic Labels

Models
Active (LC) + BERT
Active (IG) + BERT
Semi sup. + BERT
Fully sup. + BERT
Majority vote + BERT
Weakly sup. + BERT
Fully sup. + TF-IDF
Weakly sup. + TF-IDF

# labelled AUC Accuracy FPR@50%TPR FNR@50%TNR

1020
1020
1000
25000
0
0
25000
0

0.842
0.842
0.820
0.857
0.166
0.850
0.868
0.827

0.758
0.758
0.724
0.772
0.500
0.768
0.782
0.749

0.073
0.073
0.078
0.060
0.075
0.066
0.062
0.108

0.078
0.078
0.105
0.967
0.095
0.076
0.050
0.076

Table 2: Summary of results for the IMDb dataset. Our weakly supervised model, trained
using negligible labelling eﬀort performs at par with its fully supervised counter-
part. Active (LC) and (IG) correspond to least conﬁdence and information gain
based active learning models, respectively.

Models
Active (IG) + BERT
Semi sup. + BERT
Fully sup. + BERT
Majority vote + BERT
Weakly sup. + BERT
Fully sup. + TF-IDF
Weakly sup. + TF-IDF

# labelled Accuracy W. precision W. recall W. F1-score
0.812
0.373
0.799
0.142
0.802
0.777
0.768

0.805
0.413
0.797
0.128
0.800
0.773
0.765

0.805
0.525
0.797
0.271
0.801
0.772
0.766

0.805
0.525
0.797
0.271
0.801
0.772
0.766

1020
1000
2028
0
0
2028
0

Table 3: Summary of results for the Newsgroups dataset. Our weakly supervised model,
trained using negligible labelling eﬀort performs at par with its fully supervised
counterpart. W. indicates weighted scores.

indicative of a positive, neutral or negative sentiment. We ﬁnally used the positive and
negative unigrams as LFs. For instance, if a review contains the keyword ‘masterpiece’, it
is more likely to be positive. On the other hand, for the every candidate keyword in the
Newsgroups training data, we assigned it to the ‘closest’ category in terms of the cosine
similarity of their corresponding BERT (Devlin et al., 2019) embeddings. For example,
the keyword ‘keyboard ’ is closest to ‘computer ’ and hence suggestive of a comp.winds.x
news item. Additionally, we choose all the words above a threshold that is similar to the
category. Tab. 4 and 5 list the complete set of keywords used for both the IMDb and
Newsgroups dataset.

5. Results and Discussion

Tab. 2 and 3 summarize the results of our experiments. Some of our key ﬁndings are as
follows.

Weak supervision yields competitive end models with negligible pointillistic
supervision. Our results reveal that our weakly supervised models, trained without any
access to ground truth labels, performed at par with the their fully supervised counterparts,

8

Learning without Pointillistic Labels

(i)

(ii)

(iii)

(iv)

(v)

(vi)

(vii)

(viii)

Figure 3: Performance trajectories of information gain-based active learning (AL) (i, ii,
v, vi ) and semi-supervised learning (SSL) (iii, iv, vii, viii ) with the learning
iterations for the IMDb (i - iv) and Newsgroups datasets, respectively. While AL
performance trajectories demonstrate a logarithmic increase, the SSL trajectories
are fairly ﬂat. The red bands are 95% Wilson’s score conﬁdence intervals.

for both the IMDb and Newsgroups datasets.

Alternative approaches require more eﬀort and show inferior performance. For
both the IMDb and Newsgroups datasets, our active learning (AL) models required at least
600 labelled samples (see Fig. 3) to perform at par with the supervised learning models. In
fact, in the Newgroups dataset, our AL model outperforms both the full and weakly super-
vised models. Our ﬁndings are consistent with several studies (Settles, 2009) which have
theoretically shown that AL can achieve the same error rate as full supervision with fewer
but intelligently chosen query objects. In comparison, we found that self-labelling semi-
supervised learning (SSL) did not demonstrate a signiﬁcant improvement in performance
over the performance in the initial seed set. In fact, in the IMDb dataset, we observed a
degradation in performance as the end model classiﬁers were re-trained on more self-labelled
data points (see Fig. 3).

Weak supervision signiﬁcantly outperformed majority voting. Recall that while
majority voting weighs each LF equally, data programming weighed each LF in proportion
to its empirical accuracy estimated by the label model (see Eqns. 3 and 5). Our results
illustrate the distinct advantage of the accuracy-weighted majority voting over the simple

9

Learning without Pointillistic Labels

majority voting.

BERT does not oﬀer a signiﬁcant advantage over TF-IDF embeddings. In ad-
dition to using BERT embeddings, we also compared the performance of our weakly and
fully supervised models using TF-IDF (Term Frequency, Inverse Document Frequency) em-
beddings as featurization. Since TF-IDF embeddings are sparse8, instead of using the raw
embeddings, we used the ﬁrst 500 components obtained from the Principal Components
Analysis (PCA) algorithm. We found that both the weakly and full supervised models
demonstrated comparable performance in both the settings. However, we observed a nat-
ural deterioration of performance when using TF-IDF instead of BERT embeddings. This
is an important result since BERT is pre-trained on a very large corpus and oﬀers another
source of weak supervision. The goal of this experiment was to quantify if this provided
any advantages to the weakly supervised model.

Keywords are excellent sources of weak supervision. The major ﬁnding of our
study is that keywords are excellent sources of weak supervision for the data programming
label model. While we were able to easily and automatically “gather” supervision at the
unigram level using existing lexicons and similarity functions, for more involved problems,
such alternate supervision sources may be hard to ﬁnd. Moreover, the limitations of our
automated keyword based methodology may simply not be visible given the nature of the
datasets we considered. However, even in this case, we may still be able to eﬃciently train a
simple keyword classiﬁer using active learning. We must note that obtaining supervision at
the keyword or keyphrase level is still much more eﬃcient than labelling the entire corpus
since the manual eﬀort is upper-bounded by the size of the frequent vocabulary9 which is
usually signiﬁcantly smaller than size of the corpus.

6. Conclusion

In this work we show that weak supervision is a competitive alternative to fully super-
vised classiﬁcation models. Our results over multiple text classiﬁcation datasets reveal that
weak supervision performs at par with its fully supervised counterpart without access to
pointillistic ground truth labels. We found that the considered alternatives–majority voting,
active, and semi-supervised learning demonstrated inferior performance despite relying on
more manual eﬀort. Our results also underscored the importance of the accuracy-weighted
majority voting aggregation followed by data programming in comparison to simple major-
ity voting. Future work involves empirically evaluating the performance of keywords and
keyphrases as sources of weak supervision over multiple datasets as well as extensions to
applications beyond text data.

References

Benedikt Boecking and Artur Dubrawski. Pairwise feedback for data programming. arXiv

preprint arXiv:1912.07685, 2019.

8. They are of the same length as the vocabulary
9. Top n% frequent words

10

Learning without Pointillistic Labels

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training
of deep bidirectional transformers for language understanding. In Proceedings of the 2019
Conference of the North American Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–
4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi:
10.18653/v1/N19-1423. URL https://www.aclweb.org/anthology/N19-1423.

Huiji Gao, Geoﬀrey Barbier, and Rebecca Goolsby. Harnessing the crowdsourcing power of

social media for disaster relief. IEEE Intelligent Systems, 26(3):10–14, 2011.

CHE Gilbert and Erric Hutto. Vader: A parsimonious rule-based model for sentiment anal-
ysis of social media text. In Eighth International Conference on Weblogs and Social Media
(ICWSM-14). Available at (20/04/16) http://comp. social. gatech. edu/papers/icwsm14.
vader. hutto. pdf, volume 81, page 82, 2014.

Mononito Goswami, Lujie Chen, and Artur Dubrawski. Discriminating cognitive disequilib-
rium and ﬂow in problem solving: A semi-supervised approach using involuntary dynamic
behavioral signals. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol-
ume 34, pages 420–427, 2020a.

Mononito Goswami, Minkush Manuja, and Maitree Leekha. Towards social & engaging
peer learning: Predicting backchanneling and disengagement in children. arXiv preprint
arXiv:2007.11346, 2020b.

Awni Y Hannun, Pranav Rajpurkar, Masoumeh Haghpanahi, Geoﬀrey H Tison, Codie
Bourn, Mintu P Turakhia, and Andrew Y Ng. Cardiologist-level arrhythmia detection
and classiﬁcation in ambulatory electrocardiograms using a deep neural network. Nature
medicine, 25(1):65, 2019.

Ken Lang. Newsweeder: Learning to ﬁlter netnews. In Proceedings of the Twelfth Interna-

tional Conference on Machine Learning, pages 331–339, 1995.

Hongwei Li. Theoretical analysis and eﬃcient algorithms for crowdsourcing. PhD thesis,

UC Berkeley, 2015.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and
Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the
49th Annual Meeting of the Association for Computational Linguistics: Human Language
Technologies, pages 142–150, Portland, Oregon, USA, June 2011. Association for Com-
putational Linguistics. URL http://www.aclweb.org/anthology/P11-1015.

Alexander J Ratner, Christopher M De Sa, Sen Wu, Daniel Selsam, and Christopher R´e.
Data programming: Creating large training sets, quickly. In Advances in neural infor-
mation processing systems, pages 3567–3575, 2016.

Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-

networks, 2019.

Burr Settles. Active learning literature survey. Technical report, University of Wisconsin-

Madison Department of Computer Sciences, 2009.

11

Learning without Pointillistic Labels

C. E. Shannon. A mathematical theory of communication. SIGMOBILE Mob. Comput.
Commun. Rev., 5(1):3–55, January 2001. ISSN 1559-1662. doi: 10.1145/584091.584093.
URL https://doi.org/10.1145/584091.584093.

PLCO Project Team, John K Gohagan, Philip C Prorok, Richard B Hayes, and Barnett-S
Kramer. The prostate, lung, colorectal and ovarian (plco) cancer screening trial of the
national cancer institute: history, organization, and status. Controlled clinical trials, 21
(6):251S–272S, 2000.

Jesper E Van Engelen and Holger H Hoos. A survey on semi-supervised learning. Machine

Learning, 109(2):373–440, 2020.

Appendix A. Keyword LFs

Sentiment

Words

Positive

Negative

’amazing’, ’amusing’, ’appreciate’, ’attractive’, ’awesome’, ’beautiful’, ’beautifully’, ’best’,
’better’, ’brilliant’, ’certainly’, ’charm’, ’charming’, ’clearly’, ’clever’, ’compelling’,
’convincing’, ’cool’, ’creative’, ’deﬁnitely’, ’delightful’, ’eﬀective’, ’engaging’,
’enjoy’, ’enjoyable’, ’enjoyed’, ’entertaining’, ’entertainment’, ’excellent’, ’exciting’, ’fantastic’,
’fascinating’, ’favorite’, ’ﬁne’, ’fun’, ’funniest’, ’funny’, ’glad’, ’good’,
’gorgeous’, ’great’, ’greatest’, ’happy’, ’hilarious’, ’impressed’, ’impression’, ’impressive’,
’interesting’, ’laugh’, ’laughed’, ’laughing’, ’laughs’, ’like’, ’liked’, ’likes’, ’love’,
’loved’, ’lovely’, ’loves’, ’loving’, ’lucky’, ’masterpiece’, ’nice’, ’nicely’, ’novel’, ’original’,
’outstanding’, ’perfect’, ’perfectly’, ’pleasure’, ’powerful’, ’recommend’, ’recommended’, ’remarkable’,
’smart’, ’solid’, ’special’, ’strong’, ’stunning’, ’super’, ’superb’, ’superior’, ’surprise’, ’surprised’,
’surprising’, ’surprisingly’, ’terriﬁc’, ’top’, ’treat’, ’truly’, ’unbelievable’, ’well’, ’wonderful’,
’wonderfully’, ’worth’, ’worthy’, ’wow’
’avoid’, ’awful’, ’bad’, ’badly’, ’bizarre’, ’bored’, ’boring’, ’bother’, ’confused’, ’confusing’, ’crap’,
’desperate’, ’disappointed’, ’disappointing’, ’disappointment’, ’dull’, ’dumb’, ’fail’, ’failed’, ’fails’,
’forced’, ’forget’, ’forgotten’, ’hard’, ’hate’, ’hated’, ’hell’, ’horrible’, ’ill’, ’lack’, ’lame’,
’mess’, ’miss’, ’missed’, ’missing’, ’mistake’, ’nasty’, ’negative’, ’nonsense’, ’odd’, ’pain’, ’painful’,
’pathetic’, ’poor’, ’ridiculous’, ’sad’, ’sadly’, ’shame’, ’sick’, ’stupid’,
’sucks’, ’terrible’, ’terribly’, ’tired’, ’torture’, ’ugly’, ’unfortunately’, ’warning’, ’waste’,
’wasted’, ’weak’, ’weird’, ’worse’, ’worst’, ’wrong’, ’not’, ’don\’t’

Table 4: Positive and negative keywords for IMDb dataset

12

Learning without Pointillistic Labels

Category

religion

politics

computer

space

Words

’religion’, ’religions’, ’religious’, ’christianity’, ’christians’, ’church’, ’faith’,
’christian’, ’beliefs’, ’holy’, ’christ’, ’bible’, ’belief’, ’god’, ’jesus’, ’islam’, ’mass’,
’mary’, ’biblical’, ’cross’, ’matthew’, ’moral’, ’convert’, ’islamic’, ’paul’, ’followers’,
’country’, ’john’, ’values’, ’believe’, ’community’, ’believing’, ’morality’, ’message’, ’lord’,
’nature’, ’sites’, ’peter’
’political’, ’debate’, ’opinions’, ’government’, ’opinion’, ’discussion’, ’policy’,
’discuss’, ’congress’, ’talk’, ’topic’, ’arguments’, ’history’, ’argument’, ’dc’, ’report’,
’media’, ’leaders’, ’reports’, ’business’, ’issue’, ’issues’, ’law’, ’sources’, ’president’,
’police’, ’comments’, ’comment’, ’document’, ’phone’, ’companies’, ’statements’, ’meeting’,
’news’, ’articles’, ’events’, ’washington’, ’war’, ’oﬃcial’, ’military’, ’court’
’pc’, ’hardware’, ’software’, ’keyboard’, ’machine’, ’machines’, ’mouse’, ’unix’,
’graphics’, ’ibm’, ’mode’, ’interface’, ’internet’, ’mac’, ’user’, ’screen’, ’systems’, ’compile’,
’ﬁle’, ’com’, ’technology’, ’ﬁles’, ’usenet’, ’memory’, ’input’, ’widget’, ’programming’, ’system’,
’board’, ’comp’, ’function’, ’technical’, ’info’, ’output’, ’levels’, ’functions’,
’button’, ’display’, ’engineering’, ’control’, ’widgets’, ’library’, ’code’, ’os’,
’program’
’mars’, ’spacecraft’, ’orbit’, ’nasa’, ’orbital’, ’planet’, ’universe’, ’planetary’, ’rocket’,
’moon’, ’satellites’, ’shuttle’, ’rockets’, ’satellite’, ’apollo’, ’ﬂight’, ’plane’, ’physics’, ’launch’,
’ﬂy’, ’air’, ’mission’, ’astronomy’, ’nuclear’, ’solar’, ’earth’, ’world’, ’conﬁguration’, ’lunar’, ’energy’

Table 5: Category-related keywords for the Newsgroups dataset

13

