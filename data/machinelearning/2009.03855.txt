1
2
0
2

r
a

M
6
1

]
I

A
.
s
c
[

2
v
5
5
8
3
0
.
9
0
0
2
:
v
i
X
r
a

Journal of Artiﬁcial Intelligence Research 70 (2021) 1031-1116

Submitted 08/2020; published 03/2021

Induction and Exploitation of Subgoal Automata for
Reinforcement Learning

Daniel Furelos-Blanco
Mark Law
Department of Computing
Imperial College London
London, SW7 2AZ, United Kingdom

d.furelos-blanco18@imperial.ac.uk
mark.law09@imperial.ac.uk

Anders Jonsson
Department of Information and Communication Technologies
Universitat Pompeu Fabra
Roc Boronat 138, 08018 Barcelona, Spain

Krysia Broda
Alessandra Russo
Department of Computing
Imperial College London
London, SW7 2AZ, United Kingdom

anders.jonsson@upf.edu

k.broda@imperial.ac.uk
a.russo@imperial.ac.uk

Abstract

In this paper we present ISA, an approach for learning and exploiting subgoals in
episodic reinforcement learning (RL) tasks.
ISA interleaves reinforcement learning with
the induction of a subgoal automaton, an automaton whose edges are labeled by the task’s
subgoals expressed as propositional logic formulas over a set of high-level events. A subgoal
automaton also consists of two special states: a state indicating the successful completion
of the task, and a state indicating that the task has ﬁnished without succeeding. A state-
of-the-art inductive logic programming system is used to learn a subgoal automaton that
covers the traces of high-level events observed by the RL agent. When the currently
exploited automaton does not correctly recognize a trace, the automaton learner induces a
new automaton that covers that trace. The interleaving process guarantees the induction of
automata with the minimum number of states, and applies a symmetry breaking mechanism
to shrink the search space whilst remaining complete. We evaluate ISA in several grid-
world and continuous state space problems using diﬀerent RL algorithms that leverage the
automaton structures. We provide an in-depth empirical analysis of the automaton learning
performance in terms of the traces, the symmetry breaking and speciﬁc restrictions imposed
on the ﬁnal learnable automaton. For each class of RL problem, we show that the learned
automata can be successfully exploited to learn policies that reach the goal, achieving an
average reward comparable to the case where automata are not learned but handcrafted
and given beforehand.

1. Introduction

Reinforcement learning (RL) is a family of algorithms for controlling an agent that acts
in an environment with the purpose of maximizing some measure of cumulative reward it
receives. These algorithms have played a key role in recent breakthroughs like human-level
video game playing from raw sensory input (Mnih et al., 2015) and mastering complex

©2021 AI Access Foundation. All rights reserved.

 
 
 
 
 
 
Furelos-Blanco, Law, Jonsson, Broda, & Russo

board games (Silver et al., 2018). However, despite of these impressive advancements, RL
algorithms still struggle to discover and exploit the structure of a task. A possible way to
represent these structures is through (temporal) abstractions.

Finite-state automata have been extensively used as a means for abstraction across dif-
ferent areas of Artiﬁcial Intelligence (AI), including the control of agents in robotics (Brooks,
1989) and games (Buckland, 2004), as well as automated planning (Bonet et al., 2009; Hu
& De Giacomo, 2011; Segovia Aguas et al., 2018). In the context of RL, automata have
been used for multiple purposes, such as to represent abstract decision hierarchies (Parr
& Russell, 1997; Leonetti et al., 2012), be used as memory in partially observable envi-
ronments (Meuleau et al., 1999; Toro Icarte et al., 2019), or ease the interpretation of the
policies encoded by a neural network (Koul et al., 2019). In particular, Toro Icarte et al.
(2018) recently proposed reward machines (RMs), which are automata that play the role of
reward functions while revealing the task’s structure to the RL agent. This approach has
drawn attention from the community and several works have recently attempted to learn
RMs (Toro Icarte et al., 2019; Xu et al., 2020) or similar kinds of automata (Furelos-Blanco
et al., 2020; Gaon & Brafman, 2020).

In this paper we propose ISA (Induction of Subgoal Automata for Reinforcement Learn-
ing), a method for learning and exploiting a minimal automaton that encodes the subgoals
of an episodic goal-oriented task. Indeed, these automata are called subgoal automata since
each transition is labeled by a subgoal, which is a boolean formula over a set of high-level
events that characterizes the task. A (possibly empty) set of high-level events is sensed by
the agent at each step. Besides, subgoal automata have accepting and rejecting states. The
former indicate the successful completion of the task, while the latter indicate that the task
has been ﬁnished but without succeeding.

We represent subgoal automata using Answer Set Programming (ASP, Gelfond & Kahl,
2014), a logic programming language. A state-of-the-art inductive logic programming (ILP)
system for learning ASP programs, ILASP (Law et al., 2015b), is used to learn the automata.
Speciﬁcally, given a set of traces of high-level events, and suﬃciently large numbers of
automaton states and allowed edges from one state to another, ILASP learns the transitions
between states such that the traces are correctly recognized. For instance, all those traces
achieving the task’s goal must ﬁnish in the accepting state. To speed up automaton learning,
we devise a symmetry breaking mechanism that discards multiple equivalent automata in
order to shrink the search space.

ISA interleaves automaton learning and reinforcement learning. The automaton learner
is executed only when the RL agent ﬁnds a trace not correctly recognized by the current
automaton. The interleaving scheme guarantees that the induced subgoal automaton is
minimal (i.e., has the minimum number of states).

Importantly, subgoal automata address two types of abstraction: state abstraction and
action abstraction (Konidaris, 2019; Ho, Abel, Griﬃths, & Littman, 2019). The set of
automaton states is an abstraction of the original state space: they determine the level of
completion of a given task. That is, they indicate which subgoals have been achieved and
which remain to be achieved. Conversely, the subgoals labeling the edges can be seen as local
objectives of abstract actions. The latter has been successfully addressed by hierarchical
reinforcement learning (HRL, Barto & Mahadevan, 2003), which divides a single task into
several subtasks that can be solved separately. We use HRL methods to exploit subgoal

1032

Induction and Exploitation of Subgoal Automata for RL

automata by deﬁning one subtask per subgoal, as well as methods that exploit similar
automaton structures like the aforementioned reward machines. All in all, abstractions in
the form of subgoal automata can potentially:

1. Make learning simpler since mastering a subtask should be easier than mastering the

whole task.

2. Allow for better exploration since the agent moves more quickly between abstract

states (i.e., diﬀerent levels of completion of the task).

3. Allow for generalization between diﬀerent tasks if they share common subtasks.

4. Handle partial observability by acting as an external memory.

We evaluate ISA in several grid-world and continuous state space tasks. We show that a
subgoal automaton can be simultaneously induced from interaction and exploited by diﬀer-
ent RL algorithms to learn a policy that achieves the task’s goal. Importantly, the perfor-
mance achieved by ISA in the limit is comparable to that where a handcrafted automaton is
given beforehand. Furthermore, we make a thorough analysis of how reinforcement learning
aﬀects automaton learning and vice versa.

The description of our approach previously appeared in a conference paper (Furelos-
Blanco et al., 2020). Compared to the conference version, the present paper includes the
following novel material:

• A method for breaking symmetries in our automata that speeds up the automaton

learning phase.

• An extensive experimental analysis of our interleaving method. The experiments
include two new domains, one of which is characterized by a continuous state space
and, thus, requires the use of function approximation techniques. Besides, we also
evaluate a hierarchical RL algorithm that was not included in the conference version
of the paper.

• A detailed description of recent related work, some of which was not yet available at

the time of submission of the conference paper.

• A discussion on the limitations of our work and ideas to be developed in future work.

The paper is organized as follows. Section 2 introduces the background of our work.
The formalization of the main components of our approach (the tasks, the automata and
the traces) is given in Section 3. Section 4 describes how subgoal automata and traces are
represented using ASP, while Section 5 explains how this class of automata can be learned
from traces. In Section 6 we introduce a set of constraints for checking and guaranteeing that
a given automaton complies with speciﬁc structural properties (determinism and symmetry
breaking constraints). Section 7 details how automaton learning is interleaved with diﬀerent
RL algorithms that exploit the resulting automata. The eﬀectiveness of our method across
diﬀerent tasks is evaluated in Section 8, followed by a discussion on related work in Section 9.
Section 10 concludes the paper and suggests directions for future work.

1033

Furelos-Blanco, Law, Jonsson, Broda, & Russo

2. Background

In this section we brieﬂy summarize the key background concepts of reinforcement learning
and inductive learning of answer set programs, which are the main building blocks of our
approach.

2.1 Reinforcement Learning

S

=

×

→

M

S, A, p, r, γ
(cid:104)

A
×
R is a reward function, and γ
S, executes action at

Reinforcement learning (RL, Sutton & Barto, 1998) is a family of algorithms for learning
to act in an unknown environment. Typically, this learning process is formulated as a
Markov Decision Process (MDP), i.e., a tuple
, where S is a ﬁnite set of
∆(S) is a transition probability function,1
states, A is a ﬁnite set of actions, p : S
→
[0, 1) is a discount factor. At time t, the agent
r : S
A
∈
×
observes state st
st, at)
A, transitions to the next state st+1
and receives reward r(st, at, st+1). We consider episodic MDPs that terminate in a given
set of terminal states, which can be either goal states or undesirable states (i.e., dead-ends).
Let ST
ST the set of goal states. The aim is to
⊆
⊆
A, a mapping from states to actions,2 that maximizes the expected
ﬁnd a policy π : S
sum of discounted reward (or return), Rt = E[(cid:80)n
k=t γk−trk], where n is the last step of the
episode.

S be the set of terminal states and SG

→

p(

∼

∈

∈

·|

(cid:105)

In model-free RL the transition probability function p and reward function r are un-
known to the agent, and a policy is learned via interaction with the environment. Q-learning
(Watkins, 1989) computes an action-value function Q(s, a) = E[Rt
st = s, at = a] that es-
|
timates the return from each state-action pair when following an approximately optimal
policy. In each iteration t the estimates are updated as

Q(st, at) = Q(st, at) + α

r(st, at, st+1) + γ max

a(cid:48)

(cid:18)

Q(st+1, a(cid:48))

−

(cid:19)

Q(st, at)

,

where α is a learning rate and st+1 is the state after applying at in st. The term r(st, at, st+1)+
γ maxa(cid:48) Q(st+1, a(cid:48)) is the target, while the whole expression within parentheses is the Bell-
man error. Usually, an (cid:15)-greedy policy selects a random action with probability (cid:15) and the
action maximizing Q(s, a) otherwise. The policy is deﬁned as the action that maximizes
Q(s, a) in each s.

M

Σ =

In this work we focus on Partially Observable MDPs (POMDPs, Kaelbling et al., 1998),
which are MDPs where the agent cannot observe the real state of the environment. Since
we later use the terms “observable” and “observation” to denote other concepts, we refer
to unobserved states as latent, and to observed states as visible. Formally, a POMDP is
, where S, A, p, r and γ are deﬁned as for MDPs, Σ is
a tuple
S, Σ, A, p, r, γ, ν
(cid:105)
(cid:104)
the set of visible states, and ν : S
∆(Σ) is a mapping from latent states to probability
distributions over visible states. In the episodic setting, just like episodic MDPs, a POMDP
ST . We model the
includes a set of terminal states ST
interaction between the agent and an episodic POMDP environment as follows. At time
t , σG
t, the latent state is st
, where
t (cid:105)
σΣ
indicates whether st is
t ∼
t ∈
(cid:62)}
1. For any ﬁnite set X, ∆(X) = {µ ∈ RX | (cid:80)
x µ(x) = 1, µ(x) ≥ 0 (∀x)} is the probability simplex over X.
2. In this work we learn deterministic policies, but it could be extended to support stochastic policies too.

S, and the agent observes a tuple σt =

Σ is a visible state such that σΣ

S and a set of goal states SG

st), σT
·|

t , σT
σΣ
(cid:104)

t ∈ {⊥

→

ν(

⊆

⊆

∈

,

1034

Induction and Exploitation of Subgoal Automata for RL

∈

∈

t = I[st

ST ]),3 and σG

t = I[st
,
SG]).4 If the state is non-terminal, the agent executes action at

terminal (i.e., σT
indicates whether st is a goal state
(i.e., σG
A, the
st, at), and the agent observes
environment transitions to the next latent state st+1
a new tuple σt+1 and receives reward r(st, at, st+1). The policy π : Σ∗
A becomes a
mapping from histories of visible states to actions. Note that the history is required since
the Markov property might not hold for the states in Σ and, similarly, the reward function
becomes non-Markovian over Σ.

t ∈ {⊥

(cid:62)}

→

p(

∼

∈

·|

(cid:105)

=

S, A, p, r, γ
(cid:104)

, an option is a tuple ω =

Options (Sutton et al., 1999)

address temporal abstraction in RL. Given an MDP
S is the option’s
where Iω
⊆
M
[0, 1] is the option’s
initiation set, πω : S
termination condition.5 An option is available in state s
Iω. If the option is
started, the actions are chosen according to πω. The option terminates at a given state
s
A can be viewed as an option that
terminates in any state with probability 1.

S with probability βω(s). Note that an action a

A is the option’s policy, and βω : S

Iω, πω, βω
(cid:104)

→
∈

S if s

→

∈

∈

∈

(cid:105)

An MDP whose action set is extended with options is a Semi-Markov Decision Process
(SMDP). The learning methods for SMDPs require minimal changes with respect to MDPs.
The counterpart of Q-learning for SMDPs is called SMDP Q-learning (Bradtke & Duﬀ,
1994). The update it performs when an option ω has terminated is:

Q(st, ωt) = Q(st, ωt) + α

(cid:18)

r + γk max

ω(cid:48)

Q(st+k, ω(cid:48))

−

(cid:19)

Q(st, ωt)

,

where k is the number of steps between st and st+k, and r = (cid:80)k
j=1 γj−1rt+j is the cumulative
discounted reward over this time. Similarly to Q-learning, α is a learning rate and st+k is
the state in which option ωt terminates.

Intra-option learning (Kaelbling, 1993; Sutton et al., 1998) is an often used method for
learning option policies. During intra-option learning, an experience
gen-
erated by the current option can be used not only to update its policy, but also other options’
policies. Since experience accumulates faster, the convergence speed is often increased.

st, at, rt+1, st+1
(cid:104)

(cid:105)

2.2 Inductive Learning of Answer Set Programs

In this section we describe answer set programming (ASP) and the ILASP system for
learning ASP programs.

2.2.1 Answer Set Programming

Answer Set Programming (ASP, Gelfond & Kahl, 2014) is a declarative programming lan-
guage for knowledge representation and reasoning. An ASP problem is expressed in a logical
format and the models (called answer sets) of its representation provide the solutions to
that problem. In the paragraphs below we describe the main concepts of ASP used in the
paper.

3. The indicator function I(x) returns true ((cid:62)) if the condition x holds, and false (⊥) otherwise.
4. Note that σT is true whenever σG is true since SG ⊆ ST (i.e., all goal states are terminal).
5. In the case of POMDPs, the option components are deﬁned using the set of visible states Σ instead of

the set of latent states S.

1035

Furelos-Blanco, Law, Jonsson, Broda, & Russo

An atom is an expression of the form p(t1, . . . , tn) where p is a predicate symbol of arity
n and t1, . . . , tn are terms. If n = 0, we omit the parentheses. In this paper, a term can be
either a variable or a constant. By convention, variables are denoted using upper case (e.g.,
X or Y), while constants are written in lower case (e.g., coffee or mail). An atom is said
to be ground if none of its terms is a variable. A literal is an atom a or its negation not a.
The not symbol is called negation as failure.

An ASP program P is a set of rules. In this paper, we assume that this set is formed by
normal rules, choice rules and constraints. Given an atom h and a set of literals b1, . . . , bn,
a normal rule is of the form h : - b1, . . . , bn, where h is the head and b1, . . . , bn is the
body of the rule. A normal rule with an empty body is a fact. A choice rule is of the
ub : - b1, . . . , bn, where lb and ub are integers, h1, . . . , hm are atoms and
form lb
}
{
b1, . . . , bn are literals. Rules of the form : - b1, . . . , bn are called constraints.

h1, . . . , hm

Given a set of ground atoms (or interpretation) I, a ground normal rule is satisﬁed if
the head is satisﬁed by I when the body literals are satisﬁed by I. The head of a choice
rule is satisﬁed by I if and only if the number of satisﬁed atoms in the head is between lb
and ub (both included), i.e., lb
ub. A ground constraint is satisﬁed
h1, . . . , hm
if the body is not satisﬁed by I. The reduct P I of a program P with respect to I is built
in 4 steps (Law et al., 2015a):6

I
≤ |

}| ≤

∩ {

1. Replace the heads of all constraints with

.
⊥

2. For each choice rule R:

• if its head is not satisﬁed, replace its head with
• if its head is satisﬁed then remove R and for each atom h in the head of R such
I, add the rule h : - body(R) (where body(R) is the set of literals forming

, or

⊥

that h
∈
the body of R).

3. Remove any rule R such that the body of R contains the negation of an atom in I.

4. Remove all negation from any remaining rules.

An interpretation I is an answer set of P if and only if (1) I satisﬁes the rules in P I , and
(2) no subset of I satisﬁes the rules in P I .

Example 2.1. Given the following ASP program formed by two facts and two normal rules

P = (cid:8)p(X) : - not q(X), r(X). q(X) : - not p(X), r(X). r(1). r(2).(cid:9) ,

the following four interpretations are all the answer sets of P :

r(1). r(2). p(1). p(2).
r(1). r(2). p(2). q(1).

} {
} {

r(1). r(2). p(1). q(2).
}
r(1). r(2). q(1). q(2).
}

{
{

Note that the interpretation

r(1). r(2). p(1). p(2). q(1). q(2).
{

}

is not an answer set.

6. This is a non-standard form of building the reduct, but it is proven to be equivalent (Law et al., 2015a)

to the standard deﬁnitions (e.g., Calimeri et al., 2020).

1036

Induction and Exploitation of Subgoal Automata for RL

2.2.2 ILASP

ILASP (Inductive Learning of Answer Set Programs, Law et al., 2015b) is an inductive logic
programming system for learning ASP programs from partial answer sets.

A context-dependent partial interpretation (CDPI, Law et al., 2016) is a pair
, where:

ectx

einc, eexc

(cid:104)(cid:104)

,

(cid:105)

einc, eexc
(cid:104)
and eexc as the inclusions and exclusions respectively.

is a pair of sets of atoms, called a partial interpretation. We refer to einc

(cid:105)

(cid:105)
•

• ectx is an ASP program, called a context.

A program P accepts a CDPI
P

ectx such that einc
An ILASP task (Law et al., 2016) is a tuple T =

A and eexc

(cid:105)
A =

, ectx
.

einc, eexc

⊆

(cid:104)(cid:104)

∪

∩

∅

(cid:105)

if and only if there is an answer set A of

B, SM ,
(cid:104)

(cid:104)

E+, E−

where

(cid:105)(cid:105)

• B is the ASP background knowledge, which describes a set of known concepts before

learning;

• SM is the set of ASP rules allowed in the hypotheses; and

• E+ and E− are sets of CDPIs called, respectively, the positive and negative examples.

A hypothesis H

SM is an inductive solution of T if and only if:

1.

2.

e
∀
e
∀

∈

∈

⊆
E+, B

E−, B

∪

∪

H accepts e, and

H does not accept e.

Example 2.2. Let T =

B =

B, SM ,
(cid:104)
p : - not q.
{



}

,

,
,

(cid:104)(cid:104){
(cid:104)(cid:104){

E+ =

r.
{
r.
{
,
}(cid:105)
A candidate hypothesis is H =
q : - not p, r.
}
{
examples are accepted, and the negative is not:

,
p
}
,
q
}
p
(cid:104)(cid:104){

q
{
p
{
,
}

}(cid:105)
}(cid:105)
∅(cid:105)

}(cid:105)
}(cid:105)
q
{



,


,

,


E+, E−

(cid:104)

(cid:105)(cid:105)

be an ILASP task where:

SM =

(cid:26) q.

q : - not p.
q : - p, r. q : - not p, r.

(cid:27)

,

E− = (cid:8)

q
}

(cid:104)(cid:104){

,

{

p

}(cid:105)

,

(cid:9) .

∅(cid:105)

. Now we have to check whether the positive

• For the positive examples

q
{
has two answer sets A1 =

p
}

(cid:104)(cid:104){

r.

H
the ﬁrst example because

∪ {

}

,

r.
,
and
{
}(cid:105)
}(cid:105)
and A2 =
p, r
}
{
A1 =
A1 and
q
{

r.
,
p
{
}(cid:105)
{
. Then, B
}

,
q
∪
}
(cid:104)(cid:104){
accepts
q, r
{
∪
, and also the second one because
∅

, the program B
r.
}

} ∩

∪ {

}(cid:105)

H

q

} ⊆

{

A2 and

p
{

} ∩

{
A2 =

p

} ⊆
.
∅

• For the positive example

H

program B
∪ ∅
ﬁrst example since
since

∪
(cid:40) A(cid:48)

,

,

p
}

q
{
has a single answer set A(cid:48)
1 =
p
{
1 (also,

(cid:104)(cid:104){
A(cid:48)

} ∩

A(cid:48)

∅(cid:105)

}(cid:105)

1 and
A(cid:48)
=
1 (cid:54)

q
{
).
∅

} ⊆
p
{

∅

q
}

and the negative example
. Then, B
p
}
{

, the
p
q
}
∅(cid:105)
}(cid:105)
{
(cid:104)(cid:104){
accepts the
H
∪ ∅
∪
, and does not accept the second one

1 =

,

,

{

} ∩
Therefore, H is an inductive solution of T . In contrast, H (cid:48) =
solution. For instance, given the positive example
has a single answer set, A(cid:48)(cid:48)
1 =
=
p
}

,
∪ ∅
. Then, this program does not accept the example because

p
}
{
), which causes H (cid:48) not to be an inductive solution.

q.
{
}
, the program B

is not an inductive

1 (also,

A(cid:48)(cid:48)
1 (cid:54)

(cid:40) A(cid:48)(cid:48)

q
}

q
{

q
{

} ∩

H (cid:48)

(cid:104)(cid:104){

}(cid:105)

∅(cid:105)

∪

∅

{

,

1037

Furelos-Blanco, Law, Jonsson, Broda, & Russo

3. Problem Formulation

The objectives of this work are twofold:

1. Propose a method for learning a subgoal automaton from traces of a given reinforce-

ment learning task (Sections 4-6).

2. Propose a method that interleaves automaton learning and reinforcement learning

(Section 7).

In this section we formalize the main components of our automaton-driven RL approach,
including the class of RL tasks, the notion of traces generated by the RL agent and the
In later sections, we show how subgoal automata and
deﬁnition of subgoal automata.
traces are represented in ASP (see Section 4), and how learning a subgoal automaton can
be formalized as an ILASP learning task that uses traces as examples (see Section 5).

3.1 Tasks

The class of RL tasks we consider are episodic POMDPs
enhanced by a set of observables
can sense while interacting with the environment. A labeling function L : Σ
visible state into a subset of observables (or observation) O
that state. We emphasize that an observation solely depends on a visible state.

Σ =
S, ST , SG, Σ, A, p, r, γ, ν
(cid:105)
(cid:104)
. An observable is a propositional event that the agent
2O maps a
→
perceived by the agent in

⊆ O

M

O

We assume that the combination of a visible state and a history of observations seen
during an episode is suﬃcient to obtain the Markov property. Formally, the set of latent
states S is a subset of the cross product of the set of visible states Σ and a history of
observations (2O)∗. Given this assumption, a policy over Σ
(2O)∗ could be learned;
however, this space can become very large for long histories. Throughout the paper, we
explain how the automata we propose can be seen as a compact representation of observation
histories. In addition, we also assume that the history of observations seen during an episode
is suﬃcient to determine whether a terminal state is reached and, if so, whether it is a goal
state.

×

×

We use the OfficeWorld environment (Toro Icarte et al., 2018) as a running example
12 grid (see Figure 1) where an agent ( ) can move
to explain our method. It consists of a 9
. The
in the four cardinal directions; that is, the action set is A =
}
agent always moves in the intended direction (i.e., actions are deterministic), and remains
in the same location if it moves towards a wall. The set of visible states Σ is the set of all
locations in the grid (i.e., the agent knows the coordinate of the location it is stepping on).
, all of which correspond to visible
The set of observables is
locations. The agent picks up the coﬀee and the mail when it steps on locations (cid:75) and (cid:66)
respectively, and delivers them to the oﬃce when it steps on location o. The decorations
break if the agent steps on them. There are also four locations labeled A, B, C and D.

up, down, left, right
{

(cid:75), (cid:66), o, A, B, C, D,

∗}

O

=

{

∗
Three tasks with diﬀerent goals are deﬁned in this environment:

• Coffee: deliver coﬀee to the oﬃce.

• CoffeeMail: deliver coﬀee and mail to the oﬃce.

• VisitABCD: visit A, B, C and D in order.

1038

Induction and Exploitation of Subgoal Automata for RL

8
7
6
5
4
3
2
1
0

D

∗

A

(cid:75)

∗

o

∗

C

∗

B

∗

(cid:66)

∗

(cid:75)

0 1 2 3 4 5 6 7 8 9 10 11

Figure 1: Example grid used in the OfficeWorld environment (Toro Icarte et al., 2018).

The tasks terminate when the goal is achieved or a decoration is broken (this is a dead-end
state). A reward of 1 is given when the goal is achieved, else the reward is 0.

3.2 Traces

We deﬁne the diﬀerent kinds of traces that can be generated in our class of RL tasks.

is a ﬁnite sequence of tuples σi =

σ0, a0, r1, σ1, a1, . . . , an−1, rn,
Deﬁnition 3.1 (Execution trace). An execution trace λ =
i , σT
σΣ
σn
, actions and rewards induced by a
(cid:104)
(potentially changing) policy during an episode. An execution trace λ can be one of the
following:

i , σG
i (cid:105)

(cid:105)

(cid:104)

• A goal execution trace λG if σG

(cid:62)
• A dead-end execution trace λD if σT
n =

n =

been reached).

(i.e., a latent goal state has been reached).

σG
n =

⊥

(cid:62) ∧

(i.e., a latent dead-end state has

• An incomplete execution trace λI if σT

n =

(i.e., the ﬁnal latent state is not terminal).

⊥
Execution traces are the traces perceived by an RL agent. However, these are not the
traces that are used to learn the subgoal automata. Since subgoal automata aim to provide
the RL agent with a subgoal structure that is independent from the state space, they are
deﬁned at a higher level of abstraction. Therefore, the subgoal automaton learner needs
traces of higher level events as an input. These traces are called observation traces.

Deﬁnition 3.2 (Observation trace). An observation trace λL,O is a sequence of observations
Oi
Σ
i
in an execution trace λ =

n, obtained by applying a labeling function L to each visible state σΣ

σ0, a0, r1, σ1, a1, . . . , an−1, rn, σn

i ∈

⊆ O

, 0

≤

≤

(cid:104)

λL,O = (cid:10)O0, . . . , On

|

Oi = L(σΣ

. Formally,
(cid:105)
i ), σΣ

λ(cid:11) .

i ∈

For the rest of the paper, we may simply use the term trace when we refer to an

observation trace.

1039

Furelos-Blanco, Law, Jonsson, Broda, & Russo

A set of execution traces is denoted by Λ = ΛG

ΛI , where ΛG, ΛD and ΛI are
sets of goal, dead-end and incomplete execution traces, respectively. The associated set of
observation traces is analogously denoted by ΛL,O = ΛG

ΛD

ΛD

ΛI

∪

∪

L,O.

L,O ∪

L,O ∪

Example 3.1. The ﬁrst trace below is a goal execution trace for the OfficeWorld’s
Coffee task using the grid in Figure 1. The second trace is the resulting observation trace.

λG =

λG
L,O =

(cid:104)(cid:104)

(cid:104)

(4, 6),

(4, 5),
(cid:75)
}

{

,

,
⊥
,
⊥
,

{

,

⊥(cid:105)
,
⊥(cid:105)
(cid:75)
}

,

(cid:104){}

,

, 0,

←

(3, 6),
(cid:104)

,
⊥

,

⊥(cid:105)

, 0,

→

(4, 6),
(cid:104)

,
⊥

,

↓

⊥(cid:105)

, 0,

, 0,

←
, 1,
↓

(3, 6),
(cid:104)
(4, 4),
(cid:104)

,
⊥
,
(cid:62)
.

⊥(cid:105)
,
(cid:62)(cid:105)(cid:105)

,
{}

,
{}

o

{

}(cid:105)

The visible states in λG correspond to positions in the grid, whereas the arrows correspond
), down (
to the diﬀerent actions: up (
↓
↑

) and right (

), left (

→

←

).

3.3 Subgoal Automata

Now we formally deﬁne the kind of automaton that is learned and used together with the
RL component in our approach. The edges that characterize this class of automata are
. These formulas can be
labeled by propositional logic formulas over a set of observables
interpreted as the subgoals of the task represented by the automaton. Therefore, we refer
to these automata as subgoal automata.7

O

Deﬁnition 3.3 (Subgoal automaton). A subgoal automaton is a tuple
uR

where

(cid:105)
• U is a ﬁnite set of automaton states,

=

U,
(cid:104)

O

A

, δϕ, u0, uA,

is a ﬁnite set of observables (or alphabet),

•

O
• δϕ : U

2O

×

→

U is a deterministic transition function that takes an automaton state

and a subset of observables (or observation) and returns an automaton state,

• u0

• uA

• uR

∈

∈

∈

U is the unique initial state,

U is the unique absorbing accepting state, and

U is the unique absorbing rejecting state.

The accepting state (uA) denotes the task’s goal achievement. In contrast, the rejecting
state (uR) indicates that the goal can no longer be achieved. Therefore, these states are both
absorbing meaning that they do not have transitions to other states. That is, δϕ(u, O) = u
.
for u

and any observation O

uA, uR

To determine how the behavior of an agent during an episode is evaluated by a subgoal

∈ {

}

⊆ O

automaton, we introduce the concept of automaton traversal.

Deﬁnition 3.4 (Automaton traversal). Given a subgoal automaton
trace λL,O =
O0, . . . , On
(cid:104)
sequence of automaton states such that

, an automaton traversal
(cid:105)

(λL,O) =

A

A
v0, v1, . . . , vn+1
(cid:104)

and an observation
is a unique

(cid:105)

7. For the rest of the paper, we may simply use the term automata when we refer to subgoal automata.

1040

Induction and Exploitation of Subgoal Automata for RL

1. v0 = u0, and

2. δϕ(vi, Oi) = vi+1 for i = 0, . . . , n.

A subgoal automaton
v0, v1, . . . , vn+1
(cid:104)

(cid:105)

A

accepts an observation trace λL,O if the automaton traversal

(λL,O) =

is such that vn+1 = uA. Analogously,

A
rejects λL,O if vn+1 = uR.

A

Now that we know how a subgoal automaton evaluates an observation trace, we need
to determine whether this evaluation (acceptance, rejection, or neither) complies with the
type of the trace (goal, dead-end, or incomplete). The following deﬁnition introduces the
concept of validity with respect to an observation trace (i.e., whether the type of the trace
matches the automaton’s evaluation). This concept is crucial to prove the correctness of
the ASP encoding (see Section 4), as well as to learn subgoal automata (see Section 5).

Deﬁnition 3.5. Given an observation trace λ∗

is valid with respect to λ∗

L,O if one of the following holds:

L,O, where

G, D, I

, a subgoal automaton
}

∗ ∈ {

A

•

•

•

A

A

A

accepts λ∗

L,O and

rejects λ∗

L,O and

= G (i.e., λ∗

∗
= D (i.e., λ∗

∗

L,O is a goal trace).

L,O is a dead-end trace).

does not accept nor reject λ∗

L,O and

∗

= I (i.e., λ∗

L,O is an incomplete trace).

The transition function δϕ is constructed from a logical transition function ϕ that maps
, each representing a subgoal of the task. Our

state pairs into propositional formulas over
approach represents and learns logical transition functions.

O

Deﬁnition 3.6 (Logical transition function). A logical transition function ϕ : U
→
DNFO is a transition function that maps a state pair into a disjunctive normal form (DNF)
formula over
U (i.e., ϕ only represents transitions to
O
diﬀerent states).

, where ϕ(u, u) =

for each u

⊥

×

U

∈

Expressing ϕ(u, u(cid:48)) as a DNF formula allows representing multiple edges between the
same pair of states u and u(cid:48). That is, each of the conjunctions inside a DNF formula ϕ(u, u(cid:48))
labels a diﬀerent edge between u and u(cid:48). The following notation is used throughout the
paper:

•

ϕ(u, u(cid:48))
|
|

denotes the number of conjunctive formulas that form the formula ϕ(u, u(cid:48)),

• conji denotes the i-th conjunction (left-to-right) in the DNF formula ϕ(u, u(cid:48)), and

• O

= ϕ(u, u(cid:48)) denotes that observation O
|

⊆ O

Note that O is used as a truth assignment where observables in the set (i.e., o
O) are false. Formally,
are true and observables that are not in the set (i.e., o /
∈

∈

satisﬁes the DNF formula ϕ(u, u(cid:48)).
O)

O

= ϕ(u, u(cid:48))
|

conji ∈

≡ ∃

ϕ(u, u(cid:48)) such that O

= conji.
|

1041

Furelos-Blanco, Law, Jonsson, Broda, & Russo

otherwise

start

u0

(cid:75)

o

∧ ¬

o

(cid:75)

∗ ∧ ¬

(cid:75)

o

∧

u1

uA

otherwise

uR

otherwise

o

∗ ∧ ¬

otherwise

Figure 2: Subgoal automaton for the OfficeWorld’s Coffee task.

Given a logical transition function ϕ, the transition function δϕ can be formally deﬁned

in terms of ϕ as follows:

δϕ(u, O) =

(cid:40)

u(cid:48)
u

= ϕ(u, u(cid:48))
if O
|
if (cid:64)u(cid:48)

U such that O

∈

.

= ϕ(u, u(cid:48))
|

(1)

Note that loop transitions are implicitly deﬁned by the absence of a satisﬁed formula on
outgoing transitions to other states. Besides, this mapping only works if ϕ is deterministic;
that is, given a state u
, at most one formula is satisﬁed.
= ϕ(u, u(cid:48)(cid:48)), and
Formally, there are not two states u(cid:48), u(cid:48)(cid:48)
|
u(cid:48)
= u(cid:48)(cid:48). Determinism is guaranteed when all pairs of outgoing transitions from a given state
to two diﬀerent states are mutually exclusive; that is, an observable appears positively in
one edge and negatively in another.

= ϕ(u, u(cid:48)), O
|

U and an observation O

U such that O

⊆ O

∈

∈

Example 3.2. Figure 2 shows a minimal subgoal automaton for the OfficeWorld’s Cof-
fee task. The edges are labeled according to the logical transition function given below and
loop transitions are only taken if no outgoing transition holds.

ϕ(u0, u1) = (cid:75)
ϕ(u1, uA) = o

o

∧ ¬

ϕ(u0, uA) = (cid:75)
ϕ(u1, uR) =

∧

o

∗ ∧ ¬

o

ϕ(u0, uR) =

(cid:75)

∗ ∧ ¬

For all absent pairs of states (u, u(cid:48)), ϕ(u, u(cid:48)) =
. The automaton covers two accepting
cases: (1) (cid:75) and o are observed in the same tile (i.e., direct path from u0 to uA) and (2)
(cid:75) and o are observed in diﬀerent tiles (i.e., path from u0 to uA through u1). The transition
function is deterministic because all pairs of outgoing transitions from a given state to two
diﬀerent states are mutually exclusive (e.g., the formulas o and
o are mutually exclusive
because o appears positively in the former and negatively in the latter). Note that, in this
case, no pairs of states have multiple edges; that is,

= 1 for all pairs of states.

∗∧¬

⊥

ϕ(u, u(cid:48))
|

|

Example 3.3. The automaton traversal for the observation trace λL,O =
in the automaton of Figure 2 is

(λL,O) =

u0, u0, u1, u1, u1, uA
(cid:104)

.
(cid:105)

A

1042

,

,

(cid:75)
}

{

,
{}

,
{}

{

o

}(cid:105)

(cid:104){}

(cid:54)
Induction and Exploitation of Subgoal Automata for RL

4. Representation of Subgoal Automata in Answer Set Programming

In this section we explain how subgoal automata are represented using Answer Set Program-
ming (ASP). First, we describe how traces and subgoal automata are represented. Then,
we present the general rules that describe the behavior of a subgoal automaton. Finally, we
prove the correctness of the representation.

Deﬁnition 4.1 (ASP representation of an observation trace). Given an observation trace
λL,O =

, M (λL,O) denotes the set of ASP facts that describe it:

O0, . . . , On

(cid:104)

(cid:105)

M (λL,O) = {
{
{

obs(o, t).
step(t).
last(n).

0
|
0
.

|
}

Ot

∈

} ∪

t

n, o
≤
n
≤

} ∪

≤
t
≤

The obs(o, t) predicate indicates that observable o

is observed at step t, step(t)

states that t is a step of the trace, and last(n) indicates that the trace ends at step n.

∈ O

Example 4.1. The set of ASP facts for the observation trace λL,O =
M (λL,O) =

obs(a, 0)., obs(b, 2)., obs(c, 2)., step(0), step(1)., step(2)., last(2).
}

b, c
}(cid:105)
{
.

,
{}

a
}

(cid:104){

{

,

is

Deﬁnition 4.2 (ASP representation of a subgoal automaton). Given a subgoal automaton
) denotes the set of ASP rules that
)
A

, δϕ, u0, uA, uR

, M (
(cid:105)

) = MU (

U,
(cid:104)

Mϕ(

A

A

=

∪

A
O
describe it, where:

and

Mϕ (

) =

A

MU (

) =

A

state (u) .
{

|

u

∈

U

}






ed(u, u(cid:48), i).
¯ϕ(u, u(cid:48), i, T) : - not obs(o1, T), step(T).
...
¯ϕ(u, u(cid:48), i, T) : - not obs(on, T), step(T).
¯ϕ(u, u(cid:48), i, T) : - obs(on+1, T), step(T).
...

¯ϕ(u, u(cid:48), i, T) : - obs(om, T), step(T).

u

U

uA, uR

∈
u(cid:48)

\ {
U

∈
1
i
≤
≤ |
conji ∈
conji = o1

,
u
\ {
}
ϕ(u, u(cid:48))
|
ϕ(u, u(cid:48)),
on

}

,

,

∧ · · · ∧

on+1

∧ ¬

∧ · · · ∧ ¬

om






.

The rules in M (

) are described as follows:

A

• Facts state(u) indicate that u is an automaton state.

• Facts ed(u, u(cid:48), i) indicate that there is a transition from state u to u(cid:48) using edge i.

Note that i is the i-th conjunction in the DNF formula ϕ(u, u(cid:48)).8

• Normal rules whose head is of the form ¯ϕ(u, u(cid:48), i, T) state that the transition from
state u to state u(cid:48) with edge i does not hold at step T. The body of these rules consists
of a single obs(o, T) literal and an atom step(T) indicating that T is a step. Remember
that we represent variables using upper case letters, which is the case of steps T here.

8. Remember that each conjunction in the DNF formula ϕ(u, u(cid:48)) represents a diﬀerent edge between states

u and u(cid:48).

1043

Furelos-Blanco, Law, Jonsson, Broda, & Russo

Note that ¯ϕ represents the negation of the logical transition function ϕ. This is because,
as discussed in more detail later in Section 5, learning the negation of the logical transition
function ϕ makes the search space smaller and, thus, makes the learning process faster.

Example 4.2. The following rules represent the automaton in Figure 2 (p. 1042).






state(uA). state(uR).

state(u0). state(u1).
ed(u0, u1, 1). ed(u0, uA, 1). ed(u0, uR, 1). ed(u1, uA, 1). ed(u1, uR, 1).
¯ϕ(u0, u1, 1, T) : - not obs((cid:75), T), step(T).
¯ϕ(u0, uA, 1, T) : - not obs((cid:75), T), step(T).
, T), step(T).
¯ϕ(u0, uR, 1, T) : - not obs(
∗
¯ϕ(u1, uA, 1, T) : - not obs(o, T), step(T).
, T), step(T).
¯ϕ(u1, uR, 1, T) : - not obs(
∗

¯ϕ(u0, u1, 1, T) : - obs(o, T), step(T).
¯ϕ(u0, uA, 1, T) : - not obs(o, T), step(T).
¯ϕ(u0, uR, 1, T) : - obs((cid:75), T), step(T).

¯ϕ(u1, uR, 1, T) : - obs(o, T), step(T).






In order to check whether an automaton accepts or rejects an observation
General Rules.
trace, it is necessary to reason about the automaton’s behavior. This is done by means of
a set of rules R that deﬁne how a subgoal automaton processes an observation trace. This
set of rules is given by the union of diﬀerent components, R = Rϕ
Rst. The subsets
∪
Rϕ and Rδ deﬁne the rules related to the automaton transition function:

Rδ

∪

• The ﬁrst rule in Rϕ deﬁnes the logical transition function ϕ in terms of its negation
¯ϕ and ed atoms. The second rule indicates that an outgoing transition from state X
is taken at step T.

Rϕ =

(cid:26)ϕ(X, Y, E, T) : - not ¯ϕ(X, Y, E, T), ed(X, Y, E), step(T).

(cid:27)

out ϕ(X, T) : - ϕ(X, , , T).

• The rules in Rδ deﬁne the transition function δ in terms of ϕ, as deﬁned in Equa-
tion 1 (p. 1042). The ﬁrst rule states that X transitions to Y at step T if an outgoing
transition to Y holds at that step. In contrast, the second rule indicates that state X
transitions to itself at step T if no outgoing transition is satisﬁed at that step.

Rδ =

(cid:26)δ(X, Y, T) : - ϕ(X, Y, , T).

(cid:27)

δ(X, X, T) : - not out ϕ(X, T), state(X), step(T).

The subset Rst is used to deﬁne the automaton traversal of the trace (that is, the
sequence of visited automaton states), and the criteria for accepting or rejecting a trace.
The st(T, X) atoms indicate that a trace is in state X at step T. The ﬁrst rule deﬁnes that
the agent is in u0 at step 0. The second rule determines that at step T+1 the agent will be
in state Y if it is in state X at step T and a transition between them holds at that step. The
third (resp. fourth) rule indicates that the observation trace is accepted (resp. rejected) if
the state at the trace’s last step is uA (resp. uR).

Rst =





st(0, u0).
st(T+1, Y) : - st(T, X), δ(X, Y, T).
accept : - last(T), st(T+1, uA).
reject : - last(T), st(T+1, uR).





1044

Induction and Exploitation of Subgoal Automata for RL

Proposition 4.1 (Correctness of the ASP encoding). Given a ﬁnite observation trace λ∗
where
P = M (
if

G, D, I
R
= G, and (2) reject

L,O) has a unique answer set AS and (1) accept

, and an automaton
}
M (λ∗

L,O,
L,O, the program
AS if and only

that is valid with respect to λ∗

AS if and only if

∗ ∈ {
)
∪
A

= D.

A

∪

∈

∗

∈

∗

Proof. See Appendix A.

5. Learning Subgoal Automata from Traces

This section describes our approach for learning a subgoal automaton. Firstly, we formalize
the task of learning an automaton from traces.

Deﬁnition 5.1. An automaton learning task is a tuple TA =
where

U,
(cid:104)

, u0, uA, uR, ΛL,O, κ
(cid:105)

,

O

• U

⊇ {

accepting state and uR is the rejecting state;

}

u0, uA, uR

is a set of automaton states, where u0 is the initial state, uA is the

•

O

is a set of observables;

• ΛL,O = ΛG

L,O ∪

ΛD

L,O ∪

ΛI

L,O is a set of observation traces; and

• κ is the maximum number of directed edges (u, u(cid:48)) from a state u

u(cid:48)

U

.
u
}

\ {

∈

U to another state

∈

An automaton
in ΛL,O; that is, if and only if it accepts all goal traces in ΛG
in ΛD

is a solution of TA if and only if it is valid with respect to all the traces
L,O, rejects all dead-end traces

L,O, and does not accept nor reject any incomplete trace in ΛI

L,O.

A

Note that (i) κ can be seen as the maximum number of disjuncts that a DNF formula
ϕ(u, u(cid:48)) between two states u and u(cid:48) can have, and (ii) U will be the set of states of the
learned automaton.

Given an automaton learning task TA, we map it into an ILASP learning task M (TA) =
and use the ILASP system (Law et al., 2015b) to ﬁnd a minimal inductive
SM that covers the examples.9 We deﬁne the diﬀerent components of

E+,
B, SM ,
∅(cid:105)(cid:105)
(cid:104)
(cid:104)
solution Mϕ(
)
A
M (TA) below.

⊆

Background Knowledge. The background knowledge B = BU
R is a set of rules that
describe the general behavior of any subgoal automaton. The set of rules BU consists of
state(u) facts for each automaton state u
U , while R is the set of general rules that
∈
deﬁnes how subgoal automata process observation traces (see the deﬁnition in Section 4).
Describing known concepts through the background knowledge is useful to avoid learning
everything from scratch. In this case, we only need to learn the edges of the automata, and
not the general deﬁnitions of how subgoal automata work (i.e., the rules in R).

∪

9. Note that we do not use negative examples (E− = ∅).

1045

Furelos-Blanco, Law, Jonsson, Broda, & Russo

Hypothesis Space. The hypothesis space SM contains all ed and ¯ϕ rules that character-
ize a transition from a non-terminal state u
u
}
using edge i

∈
[1, κ]. Formally, it is deﬁned as

to a diﬀerent state u(cid:48)

uA, uR

\ {

\ {

U

U

∈

}

∈

SM =






ed(u, u(cid:48), i).
¯ϕ(u, u(cid:48), i, T) : - obs(o, T), step(T).
¯ϕ(u, u(cid:48), i, T) : - not obs(o, T), step(T).

u

∈
u(cid:48)

i

∈

U

}

\ {
U
∈
\ {
[1, κ] , o

uA, uR
,
u
}
∈ O


,




.

Loop transitions are not included since they are unsatisﬁable formulas (see Deﬁnition 3.6).
Note that it is possible to learn unlabeled transitions, which are taken unconditionally (that
is, regardless of the current observation). For example, for a transition from u to u(cid:48) using
edge i, an inductive solution may only include ed(u, u(cid:48), i) and not ¯ϕ(u, u(cid:48), i, T).

As mentioned before, the learner induces the negation ¯ϕ of a logical transition function
ϕ.10 A diﬀerent hypothesis space where the learned rules characterize ϕ directly could have
been deﬁned. However, this requires guessing the maximum number of literals that label
a transition between two states.11 Therefore, we represent subgoal automata using ¯ϕ and
instead of imposing a maximum size for the conjunctive formulas, we impose a limit (κ) on
the number of edges from one state to another.

A

It is important to realize that the learned hypothesis is denoted by Mϕ(

) and not
M (
) (see Deﬁnition 4.2). The set of automaton states is given in the background knowl-
edge B, and the hypothesis space SM only contains transition rules. Hence, the hypothesis
is a smallest subset of transition rules that covers all the examples. Since the set of automa-
ton states is provided through the background knowledge, a minimal automaton (i.e., an
automaton with the minimum number of states) is only guaranteed to be learned when the
set of automaton states is the minimal one. The mechanism that interleaves reinforcement
learning and automaton learning described in Section 7 ensures that the learned automaton
is minimal for a speciﬁc κ.

A

Example Sets. Given a set of traces ΛL,O = ΛG
examples is deﬁned as

L,O ∪

ΛD

L,O ∪

ΛI

L,O, the set of positive

E+ =

e∗, M (λL,O)

{(cid:104)

G, D, I

(cid:105) | ∗ ∈ {

, λL,O
}

∈

Λ∗

L,O}

,

where

• eG =

• eD =

• eI =

reject
{

,
}(cid:105)

accept

,
}
reject

(cid:104){

(cid:104){

,

(cid:104){}

{

,
}

accept
{
accept, reject

}(cid:105)

, and

}(cid:105)

are the partial interpretations for goal, dead-end and incomplete traces. The accept and
reject atoms express whether a trace is accepted or rejected by the automaton; hence,
goal traces must only be accepted, dead-end traces must only be rejected, and incomplete
traces cannot be accepted or rejected. Note that the context of each example is the set of
ASP facts M (λL,O) that represents the corresponding trace (see Deﬁnition 4.1).

10. Remember that the set of rules R introduced in Section 4 deﬁnes ϕ in terms of ¯ϕ.
11. This is because ILASP has the maximum length of learnable rules as a parameter. This is a problem to
enforce determinism (see Section 6.1) since we do not know how many literals are going to be needed
to make two formulas mutually exclusive. Besides, allowing for an arbitrarily large number of literals to
overcome the problem increases the hypothesis massively.

1046

Induction and Exploitation of Subgoal Automata for RL

Correctness of the Learning Task. The following theorem captures the correctness of
the automaton learning task.

is a solution of TA if and only if Mϕ(

Theorem 5.1. Given an automaton learning task TA =
tomaton
B, SM ,
(cid:104)
Proof. Assume

is a solution of TA.

A
E+,
(cid:104)

.
∅(cid:105)(cid:105)

A

, an au-
, u0, uA, uR, ΛL,O, κ
(cid:105)
) is an inductive solution of M (TA) =

U,
(cid:104)

O

A

is valid with respect to all traces in ΛL,O (i.e.,

accepts all traces in ΛG

L,O,

⇐⇒ A

rejects all traces in ΛD

L,O and does not accept nor reject any trace in ΛI

By Proposition 4.1, for each trace λ∗
Λ∗
⇐⇒
L,O) has a unique answer set AS and (1) accept

L,O ∈

= D.

AS if and only if
∗
For each example e
For each example e
Mϕ(

E+, R
E+, B
) is an inductive solution of M (TA).

M (
Mϕ(

A
A

∪
∪

∈
∈

A

M (λ∗
reject

∈
⇐⇒
⇐⇒
⇐⇒

A

L,O where

L,O).
G, D, I
∗ ∈ {
AS if and only if

∈

)
, M (
∪
= G, and (2)

A

R

∪

}
∗

) accepts e.
) accepts e (the two programs are identical).

Optimization. To make the formalization simpler, we have always included uA and uR
in the set of automaton states U of the automaton learning task TA. However, in practice,
uA is not included in U when the set of goal traces ΛG
L,O is empty. Likewise, uR is not
included in U when the set of dead-end traces ΛD
L,O is empty. Removing these states when
they are not needed is helpful to make the hypothesis space smaller.

6. Veriﬁcation of Structural Properties

The formalization of the automaton learning task described in Section 5 induces non-
deterministic automata. That is, there can exist an observation that simultaneously satisﬁes
two formulas labeling outgoing edges to two diﬀerent states. In order to comply with the
deﬁnition of the logical transition function (see Deﬁnition 3.6, p. 1041), we need to constrain
it to be deterministic. The automaton states act as a proxy for determining what is the
current level of completion of the task; in other words, which subgoals have been achieved.
If the automaton is non-deterministic, several diﬀerent levels of completion can be active
simultaneously, which is impractical from the perspective of an RL agent. These agents
need to know exactly in which stage they are in order to make an appropriate decision.

Determinism is not the only property we can impose. Note that an automaton can
be easily transformed into an equivalent one by rearranging its state and edge identiﬁers.
Therefore, ILASP can visit multiple symmetric automata during the search for a solution
that covers the examples. By avoiding revisiting parts of the search space, the automaton
learning time can be greatly reduced.

In this section we introduce rules that allow us to verify that a given automaton is deter-
ministic and complies with speciﬁc properties that characterize a canonical representation
(e.g., a criteria for naming the automaton states). Crucially, these veriﬁcation rules can
also be used to enforce these properties during search.

Both the determinism and symmetry properties are related to the automaton structure;
) of a
given in Deﬁnition 4.2 represents the formulas on the edges as rules.

that is, the edges and formulas that label them. The ASP representation M (
subgoal automaton

A

A

1047

Furelos-Blanco, Law, Jonsson, Broda, & Russo

Consequently, we cannot impose constraints over them easily. To solve this problem, we
) to facts of the form pos(u, u(cid:48), i, o) and neg(u, u(cid:48), i, o). These facts
map the ¯ϕ rules in M (
A
appears positively (resp. negatively) in the edge i from state
express that observable o
u to state u(cid:48). Formally, given the ASP encoding M (
, its corresponding
mapping into facts F (M (

)) is deﬁned as follows:

) of automaton

∈ O

A

A

A
state(u).
ed(u, u(cid:48), i).
pos(u, u(cid:48), i, o1).
...
pos(u, u(cid:48), i, on).
neg(u, u(cid:48), i, on+1).
...
neg(u, u(cid:48), i, om).






state(u).
ed(u, u(cid:48), i).
¯ϕ(u, u(cid:48), i, T) : - not obs(o1, T), step(T).
...
¯ϕ(u, u(cid:48), i, T) : - not obs(on, T), step(T).
¯ϕ(u, u(cid:48), i, T) : - obs(on+1, T), step(T).
...
¯ϕ(u, u(cid:48), i, T) : - obs(om, T), step(T).






.

F (M (

A

)) =

Note that the right hand side of the set corresponds to a given M (
). The facts state and
ed are replicated, while the rules ¯ϕ are transformed into pos and neg facts.12 Given this
factual representation of a subgoal automaton, we can now deﬁne constraints for enforcing
determinism and a canonical structure over these facts.

A

Example 6.1. The set of facts below represents the formulas on the edges of the automaton
in Figure 2 (p. 1042) generated from the logical transition function in Example 4.2 (p. 1044).





pos(u0, u1, 1, (cid:75)).
neg(u0, u1, 1, o).


pos(u0, uA, 1, (cid:75)). pos(u0, uA, 1, o).
neg(u0, uR, 1, (cid:75)).
pos(u0, uR, 1,
).
∗


pos(u1, uA, 1, o).
).
pos(u1, uR, 1,
∗

neg(u1, uR, 1, o).

Verifying that a learned automaton complies with a set of structural properties can be
done over its factual encoding. That is, those automata that violate the properties are
discarded as solutions to the automaton learning task. However, this is clearly computa-
tionally expensive. ILASP allows the above mapping to be included in the learning task
through meta-program injection (Law et al., 2018). This enables the learner to verify the
properties during the search for an automaton, which eﬀectively shrinks the search space
and speeds up automaton learning.

In what follows, we use the factual representation above to encode the determinism
constraints (Section 6.1) and a canonical structure for breaking symmetries (Section 6.2).

6.1 Determinism

As described in Section 3.3, a logical transition function is deterministic if no observation can
satisfy two outgoing formulas to two diﬀerent states simultaneously. As these formulas are

12. Note that we could have learned the factual representation F (M (A)) instead of the one based on normal
rules M (A). We opted for the latter since it requires less grounding and can be potentially used to
represent more complex automata in future work (see Section 10).

1048

Induction and Exploitation of Subgoal Automata for RL

conjunctions of literals, two formulas are mutually exclusive (i.e., cannot be both satisﬁed
at the same time) if and only if an observable appears positively in the ﬁrst formula and
negatively in the second (or vice versa). The set of rules below encodes this deﬁnition by
means of the mutex(X, Y, EY, Z, EZ) predicate which indicates that the formula on the edge
from X to Y with index EY is mutually exclusive with the edge X to Z with index EZ. The ﬁrst
and second rules specify that two outgoing edges from state X to two diﬀerent states (Y and
Z) are mutually exclusive if an observable O appears positively in one edge and negatively
in the other.13 The third rule enforces edges from a state X to two diﬀerent states Y and Z
to be mutually exclusive.






mutex(X, Y, EY, Z, EZ) : - pos(X, Y, EY, O), neg(X, Z, EZ, O), Y<Z.
mutex(X, Y, EY, Z, EZ) : - neg(X, Y, EY, O), pos(X, Z, EZ, O), Y<Z.
: - not mutex(X, Y, EY, Z, EZ), ed(X, Y, EY), ed(X, Z, EZ), Y<Z.






6.2 Symmetry Breaking

In this section we describe rules which aim to enforce that the induced automata follow a
canonical structure in order to make the search for solution faster by breaking symmetries.
There are several types of symmetries we are interested in breaking. Firstly, any two
states except for u0, uA and uR are interchangeable. For example, Figure 3 shows two
automata whose states u1, u2 and u3 can be used interchangeably. Secondly, there can
also be symmetries in the automaton edges. The inductive solution to the automaton
learning task M (TA) contains transition rules whose edge indices range between 1 and
κ (the maximum number of edges from one state to another). For instance, if κ = 2
a potentially learned representation of the Coffee automaton (see Figure 2, p. 1042) is
shown below. Note that edges can arbitrarily be labeled 1 or 2 even though there is a single
edge between every pair of states.






ed(u0, u1, 2). ed(u0, uA, 1). ed(u0, uR, 2). ed(u1, uA, 1). ed(u1, uR, 2).
¯ϕ(u0, u1, 2, T) : - not obs((cid:75), T), step(T).
¯ϕ(u0, uA, 1, T) : - not obs((cid:75), T), step(T).
, T), step(T).
¯ϕ(u0, uR, 2, T) : - not obs(
∗
¯ϕ(u1, uA, 1, T) : - not obs(o, T), step(T).
, T), step(T).
¯ϕ(u1, uR, 2, T) : - not obs(
∗

¯ϕ(u0, u1, 2, T) : - obs(o, T), step(T).
¯ϕ(u0, uA, 1, T) : - not obs(o, T), step(T).
¯ϕ(u0, uR, 2, T) : - obs((cid:75), T), step(T).

¯ϕ(u1, uR, 2, T) : - obs(o, T), step(T).






.

Finally, the indices of two edges between the same pair of states can be also interchanged.
The idea of our symmetry breaking mechanism is to impose a unique assignment of
state and edge indices given a labeling of the automaton edges. In Section 6.2.1 we propose
a symmetry breaking mechanism for a particular class of labeled directed graphs, whereas
in Section 6.2.2 we explain how this method applies to subgoal automata, which can be
represented as graphs in this class.

13. The comparison Y<Z is done instead of Y!=Z for eﬃciency purposes. Note that both comparisons are
equivalent in this context. The former imposes a lexicographical order to evaluate the rules and thus
avoids reevaluating the expression when Y and Z are interchanged.

1049

Furelos-Blanco, Law, Jonsson, Broda, & Russo

start

u0

A

∧ ¬∗

u1

∗

uR

∗

∗

∗

B

∧ ¬∗

u2

C

∧ ¬∗

uA

D

∧ ¬∗

(a) VisitABCD

u3

start

u0

(cid:75)

(cid:66)

∧ ¬

(cid:66)

(cid:75)

¬

∧

(cid:75)

(cid:66)

o

∧

∧

u1

(cid:66)

o

∧

uA

(cid:75)

o

∧

u2

(cid:75)

(cid:66)

o

∧ ¬

∧

o

u3

(cid:66)

o

∧ ¬

(b) CoffeeMail

(cid:75)

o

∧ ¬

Figure 3: Minimal subgoal automata for two OfficeWorld tasks. Self-loops and transi-
tions to the rejecting state uR in (b) are omitted for simplicity. The shaded states
can be interchanged in the absence of symmetry breaking.

6.2.1 Graph Indexing

In this section we propose a symmetry breaking mechanism for a particular class of labeled
directed graphs.

L

=

Let

l1, . . . , lk
}
{
with a set of nodes V =
(u, v, L), where u, v
each node u
from u, and let Ei(u) =
We deﬁne a class

∈

∈

G

V , let Eo(u) =

{

v1, . . . , vn
{

be a set of labels, and let G = (V, E) be a labeled directed graph
and a set of edges E. Each edge in E is of the form
is a subset of labels. For
be the set of outgoing labeled edges

V are the two connected nodes, and L

(v, w, L)

u = v

⊆ L

}

(v, w, L)
{
of labeled directed graphs by imposing three assumptions:

be the set of incoming labeled edges.

E

∈

}

|

E
∈
|
u = w

}

Assumption 1. The node v1 is a designated start node.

Assumption 2. Each node u

V

v1

}

\ {

∈

is reachable on a directed path from v1.

Assumption 3. Outgoing label sets from each node are unique, i.e. for each u
label set L

there is at most one edge (u, v, L)

Eo(u).

V and

∈

⊆ L

∈

As a consequence of Assumption 2, it holds that

Ei(u)
|

| ≥

1 for each u

V

v1

.

}

\ {

∈

Example 6.2. The following ﬁgure is a labeled directed graph G =
class

(cid:104)
a, b, c, d, e, f

, where V =

=

V, E
.

v1, . . . , v5
{

. The set of labels is
}

L

G

{

}

that belongs to

(cid:105)

1050

Induction and Exploitation of Subgoal Automata for RL

a, f
{

}

start

v1

v5

a, b
}

{

b, e
}

{

v4

b
}

{

v3

a
}

{

c
}

{

d
}

{

v2

Label Set Ordering. Given a set of labels
label sets as follows.

=

l1, . . . , lk
{

, we impose a total order on
}

L

Deﬁnition 6.1. A label set L
exists a label l1≤m≤k

⊆ L
such that

1. lm /
∈

L and lm

∈

∈ L

L(cid:48), and

is lower than a label set L(cid:48)

, denoted L < L(cid:48), if there

⊆ L

2. there is not a label lm(cid:48)|m(cid:48)<m such that lm(cid:48)

L and lm(cid:48) /
∈

∈

L(cid:48).

A label set L

can be mapped into a binary string B(L) =

k where Bm(L) = 1
}
if lm
L and 0 otherwise. Note that Bm(L) denotes the m-th digit in B(L). Then, a label
set L is lower than another label set L(cid:48) if its binary representation B(L) is lexicographically
lower than B(L(cid:48)).

⊆ L

0, 1

∈

{

Example 6.3. Given the label set
, the following inequalities between
some of its subsets hold. Note that the second column contains the corresponding binary
representations of the sets.

a, b, c, d, e, f
{

=

L

}

{}
d
}
{
b, e
}
{
a, f
}
{

<
<
<
<

}

a, d, f
{
c
{
}
a, f
}
{
a, b
}
{

(000000 < 100101),
(000100 < 001000),
(010010 < 100001),
(100001 < 110000).

Graph Indexing. Given a graph G
unique integers to each node in V and, for each node u
Γu
Formally, a graph indexing is a tuple
{
s.t. f (v1) = 1,
Eo(u)
,
|}
|

f : V
→ {
Γu : Eo(u)

V
|
|}
1, . . . ,

I
1, . . . ,

∈
u∈V
}

(G) =

f,
(cid:104)

→ {

∈ G

∈

u

∀

(cid:105)

V.

, we next introduce a graph indexing that assigns
V , to each outgoing edge in Eo(u).

of bijections deﬁned as

Hence a graph indexing always assigns 1 to the designated start node v1. Since outgoing
label sets are unique due to Assumption 3, we use Γu(L) as shorthand for Γu(u, v, L).
v1

(G), we introduce an associated parent function ΠI : V

Given a graph indexing

N from nodes (excluding the start node v1) to pairs of integers, deﬁned as

\{

} →

I

1, . . . ,
{

|

V

|} ×

ΠI(v) =

min
(u,v,L)∈Ei(v)

(f (u), Γu(L)).

1051

Furelos-Blanco, Law, Jonsson, Broda, & Russo

Here, the minimum is with respect to a lexicographical ordering of integer pairs. Hence
ΠI(v) = (i, e) is the smallest integer i assigned to any node on an incoming edge to v and,
in the case of ties, the smallest integer e on such an edge. Note that the parent function is
well-deﬁned since

due to the reachability assumption.

\ {
We are particularly interested in the graph indexing that corresponds to a breadth-ﬁrst

1 for each v

Ei(v)

| ≥

v1

∈

V

}

|

search (BFS) traversal of the graph G, which we proceed to deﬁne.

Deﬁnition 6.2. A graph indexing

I

(G) is a BFS traversal if the following conditions hold:

1. For each pair of nodes u and v in V

v1

}

\ {

, ΠI(u) < ΠI(v)

⇔

f (u) < f (v).

2. For each node u

V and each pair of outgoing edges (u, v, L) and (u, v(cid:48), L(cid:48)) in Eo(u),

L < L(cid:48)

⇔

∈
Γu(L) < Γu(L(cid:48)).

V
Due to the second condition in Deﬁnition 6.2, the bijection Γu of each node u
clearly orders outgoing edges by their label sets. Due to the ﬁrst condition, the bijection f
orders node u before node v if the parent function of u is smaller than that of v. In a BFS
traversal from v1, nodes are processed in the order they are ﬁrst visited. In this context,
the parent function identiﬁes the edge used to visit a node for the ﬁrst time. Together,
these facts imply that f assigns integers to nodes in the order they are visited by a BFS
traversal from v1, given that the label set ordering is used to break ties among edges. This
BFS traversal can be characterized by a BFS subtree whose edges are deﬁned by the parent
function.

∈

Example 6.4. The ﬁgure below shows a graph indexing for the graph in Example 6.2, with
nodes and edges labeled by their assigned integer. This graph indexing is a BFS traversal
since nodes are ordered according to their distance from the start node v1, and since the edge
integers used to break ties are consistent with the label set ordering shown in Example 6.3.
The parent function is given by ΠI(v2) = (4, 1), ΠI(v3) = (1, 3), ΠI(v4) = (1, 1), and
ΠI(v5) = (1, 2), and the corresponding BFS subtree appears in bold.

3 : v5

2 :

a, f
{

}

start

1 : v1

3 :

a, b
}
{

1 :

b
}

{

2 :

c
}
{

4 : v3

5 : v2

1 :

b, e
}

{

2 : v4

1 :

d
}

{

1 :

a
}

{

The important property that we exploit about BFS traversals is that they are unique,
which we prove in Lemma 6.1. To prove it, we use the result in Proposition 6.1 when BFS
is applied to any kind of directed graph where all nodes are reachable.

Proposition 6.1. If BFS visits the neighbors of each node in a ﬁxed order, the resulting
tree is unique.

1052

Induction and Exploitation of Subgoal Automata for RL

Proof. By contradiction. Assume that two diﬀerent BFS trees, T and T (cid:48), are produced
using the same visitation criteria. Then T contains an edge (u, v) that is not in T (cid:48), and T (cid:48)
contains an edge (u(cid:48), v) that is not in T . In the case of T , this means that u was visited
before u(cid:48). Analogously, u(cid:48) was visited before u to produce T (cid:48). Therefore, the visitation
criteria is diﬀerent for each of the BFS trees. This is a contradiction.

Lemma 6.1. Each graph G

∈ G

has a unique associated BFS traversal

(G).

I

Proof. Intuitively, the lemma holds because there is only one way to perform a BFS traversal
from v1, given that we use the label set ordering to break ties among edges.

∈

Formally, for each u

V , since outgoing label sets are unique, there is a unique bijection
Γu that satisﬁes the second condition in Deﬁnition 6.2. If Γu orders outgoing edges in any
other way, there will always be two outgoing edges (u, v, L) and (u, v(cid:48), L(cid:48)) in Eo(u) such
that L < L(cid:48) and Γu(L) > Γu(L(cid:48)), thus violating the condition.
Then, if we ﬁx the correct deﬁnition of Γu for each u

V , there is a unique bijection f
that satisﬁes the ﬁrst condition in Deﬁnition 6.2. To recover this bijection we can simply
perform a BFS traversal from v1, using the bijections Γu, u
V , to break ties among
edges. By Proposition 6.1, this results in a unique BFS tree. If f orders nodes in any other
way, there will always be two nodes u and v in V
such that ΠI(u) < ΠI(v) and
f (u) > f (v), thus violating the condition.

\ {

v1

∈

∈

}

Appendix B.1 shows an encoding of our symmetry breaking mechanism in the form of

a satisﬁability (SAT) formula and formally prove several of its properties.

6.2.2 Application to Subgoal Automata

In this section we devise a symmetry breaking mechanism based on the graph indexing from
the previous subsection. The key idea is to encode rules that force ILASP to generate a
graph indexing which is also a BFS traversal. Since this graph indexing is unique due to
Lemma 6.1, ILASP can only represent each graph in one way, precluding multiple symmetric
variations. In fact, we could have used any unique graph indexing for this purpose.

We ﬁrst show that a subgoal automaton
in the class

=
is a special case of a
(cid:105)
A
. The set of automaton states U corresponds
label directed graph G =
G
to the set of nodes V , while the logical transition function ϕ corresponds to the set of edges
E. Crucially, a subgoal automaton complies with all three assumptions we made about
graphs in the class

, δϕ, u0, uA, uR

V, E
(cid:104)

U,
(cid:104)

O

(cid:105)

:

G
• Assumption 1 holds because

has an initial state u0.

A

• Assumption 2 is enforced by the set of rules below. The ﬁrst rule deﬁnes u0 to be
reachable, while the second rule indicates that a state is reachable if it has an incoming
edge from a reachable state. Finally, the third rule enforces all states to be reachable.






reachable(u0).
reachable(Y) : - reachable(X), ed(X, Y, ).
: - not reachable(X), state(X).






1053

Furelos-Blanco, Law, Jonsson, Broda, & Russo

• Assumption 3 holds because the automaton is deterministic (see Section 6.1). If the
automaton is deterministic, the formulas labeling two outgoing edges from a given
state to two diﬀerent states are mutually exclusive and, thus, are diﬀerent (which
fulﬁlls the assumption). Besides, two outgoing edges from a given state to another
state cannot be equal because the automaton learner, ILASP, induces a minimal
hypothesis (in our case, a minimal set of transition rules). Since a hypothesis with
two equally labeled edges is not minimal, the assumption still holds.

Even though a subgoal automaton complies with the three assumptions of labeled di-

rected graphs, there are two diﬀerences between them:

1. The edges of a subgoal automaton are labeled by propositional formulas over a set of
, whereas the edges of a labeled directed graph are deﬁned over a set of

observables
labels

O

.
L

2. The edge indices in an indexed labeled directed graph are diﬀerent from those in our
representation of a subgoal automaton. In the former, the edge indices from a given
node range between 1 to the number of outgoing edges from than node. In contrast,
the indices range from 1 to the number of edges between each pair of states. In other
words, the edge indices from a given node in the labeled directed graph are unique,
whereas in a subgoal automaton they can be repeated.

To address these diﬀerences we just need to set a mapping from the representation used by
the subgoal automata to that characterizing a labeled directed graph. The set of symmetry
breaking constraints is deﬁned on top of this mapping. Appendix B.2 shows two possible
ways of encoding the symmetry breaking mechanism in ASP using the factual representation
introduced at the beginning of this section.

7. Interleaved Automaton Learning

In this section we describe ISA (Induction of Subgoal Automata for Reinforcement Learn-
ing), a method that combines reinforcement learning and automaton learning. Firstly, we
explain two reinforcement learning algorithms that we use to exploit a given subgoal au-
tomaton (Section 7.1). Secondly, we explain how reinforcement learning and automaton
learning are interleaved to learn a minimal automaton (Section 7.2). The automaton learn-
ing process includes the constraints introduced in Section 6 for enforcing determinism while
the symmetry breaking constraints are optional (i.e., we can still learn a usable deterministic
subgoal automaton without the latter).

7.1 Reinforcement Learning Algorithms

In this section we describe two methods to learn a policy for an episodic POMDP
S, ST , SG, Σ, A, p, r, γ, ν
(cid:104)
method is characterized by a diﬀerent way of using options (Sutton et al., 1999):

Σ =
by exploiting the automaton structure given by its subgoals. Each

M

(cid:105)

1. Learning an option for each outgoing edge in the automaton and a metacontroller to

choose between options in each automaton state (Section 7.1.1).

1054

Induction and Exploitation of Subgoal Automata for RL

2. Learning an option for each automaton state (Section 7.1.2).

In general, a subgoal automaton is used as follows. Firstly, the agent selects an option
when it reaches an automaton state. Note that in (1) there can be multiple options to
choose from, whereas in (2) there is a single option. Once an option is chosen, the agent
selects actions according to that option’s policy until its termination. An option terminates
when either (a) the episode ends, or (b) when a formula on an outgoing edge from the
in
current automaton state is satisﬁed. After the agent experiences a tuple
automaton state u at step t, it transitions to the automaton state u(cid:48) = δϕ(u, L(σΣ
t+1)) given
by the transition function of the automaton. Remember that the labeling function L maps
a visible state into an observation.

σt, at, σt+1
(cid:104)

(cid:105)

The automaton is used as an external memory since each automaton state indicates
which subgoals have been achieved so far. Crucially, the use of the automaton as an external
memory causes tasks to have the Markov property given two assumptions made earlier (see
Section 3.1). First, that observations only depend on visible states. Second, that the
combination of a visible state and a history of observables (i.e., a sequence of achieved
subgoals) is suﬃcient to be Markovian. Because each option is only executed for a speciﬁc
history of observations (represented by the current automaton state), we can deﬁne an
option’s policy over visible states only.

In the following sections we describe the features of the RL methods in detail including:

• how options are modeled,

• how policies are learned, and

• which optimality guarantees they have.

7.1.1 Learning an Option for each Outgoing Edge and a Metacontroller

for each Automaton State (HRL)

O

The edges of a subgoal automaton are labeled by propositional formulas over a set of
. The idea is that each of these formulas represents a subgoal of the task
observables
encoded by the automaton. Therefore, an intuitive approach to exploit the automaton
structure consists in learning an option that aims to reach a state whose observation satisﬁes
a given formula, and a metacontroller that learns which option to take at each automaton
state. Since decisions are taken at two diﬀerent hierarchical levels, we will refer to this
approach as HRL (Hierarchical Reinforcement Learning).

Option Modeling. Given a subgoal automaton
options in a non-terminal automaton state u
to other states) is

∈

=

, the set of
(cid:105)
U (that is, a state with outgoing transitions

, δϕ, u0, uA, uR

U,
(cid:104)

A

O

Ωu = (cid:8)ωu,φ

φ

|

∈

ϕ(u, u(cid:48)), u

= u(cid:48)(cid:9) ,

where ωu,φ is the option that attempts to satisfy the formula φ in state u
U . Note that
φ is a disjunct of the DNF formula ϕ(u, u(cid:48)).14 Each option at the automaton state u is a
tuple ωu,φ =

where:

∈

Iu, πφ, βu
(cid:104)

(cid:105)

14. Remember that the logical transition function ϕ : U × U → DNFO maps an automaton state pair into

a DNF formula over O.

1055

(cid:54)
Furelos-Blanco, Law, Jonsson, Broda, & Russo

• The initiation set is simply the set of visible states (formally, Iu = Σ) since when the
automaton state u is reached, it does not matter which is the current visible state
σΣ

Σ: an option in that automaton state will be started.

∈

• The policy πφ : Σ

satisfying the propositional formula φ.

→

A maps a visible state to a primitive action with the goal of

automaton state u
action a

∈

• The termination condition βu : Σ

[0, 1] indicates that the option
,
(cid:62)} × {⊥
terminates if any formula on an outgoing edge from u holds (i.e., the formula does
not necessarily have to be φ) or a terminal state is reached:

,
× {⊥

(cid:62)} →

βu(σ) =

(cid:40)

u(cid:48)
1 if
∃
0 otherwise

∈

U such that L(σΣ)

= ϕ(u, u(cid:48)) or σT =
|

(cid:62)

.

Note that the policy πφ can be shared by diﬀerent options in the automaton. This oppor-
tunity arises when two diﬀerent automaton states have outgoing edges labeled by the same
formula φ. Indeed, we store a dictionary that maps a formula φ into its Q-function and use
it for the diﬀerent options that depend on φ.

In the case of terminal states (e.g., uA and uR), the set of available options Ωu at
U is formed by one-step options, each corresponding to a primitive

A. Their initiation sets are the same as for non-terminal automaton states.

∈
In this approach, decisions are taken at two levels and, thus, two types
Policy Learning.
of policies are learned: (1) policies over options (i.e., a metacontroller) and (2) option
policies.

A policy over options Πu : Σ

U maps a visible state into an option
available at u. These policies are learned using SMDP Q-learning (Bradtke & Duﬀ, 1994)
with (cid:15)-greedy exploration. Given an experience tuple
Ωu is an
option taken in automaton state u at step t, the update rule is the following:
(cid:18)

Ωu in state u

σt, ωt, σt+k

, where ωt

→

(cid:19)

∈

∈

(cid:105)

(cid:104)

Qu(σΣ

t , ωt) = Qu(σΣ

t , ωt) + α

r + γk max
ω(cid:48)∈Ωu(cid:48)

Qu(cid:48)(σΣ

t+k, ω(cid:48))

Qu(σΣ

t , ωt)

,

−

where k is the number of steps between σt and σt+k, r is the cumulative discounted reward
over this time, and u(cid:48) is the automaton state when the option ends. Note that the discounted
term depends on u(cid:48) (i.e., the policy in u depends on that in u(cid:48)) and becomes 0 when σT
is true (i.e., the latent state is terminal) since there is no applicable action thereafter.

t+k

An option policy πφ : Σ

A aiming to satisfy a formula φ is not learned using the

rewards from the POMDP. Instead, we use a pseudoreward function rφ : Σ

→

,
× {⊥

(cid:62)} ×

,
{⊥

(cid:62)} →

R, which is deﬁned as follows:

rφ(σ) =






rsuccess
rdeadend
rstep

if L(σΣ)
if σT =
otherwise

= φ
|
(cid:62) ∧

σG =

,

⊥

where rsuccess > 0 is given when the next state satisﬁes φ, rdeadend
state is a dead-end state, and rstep

0 is given if the next
0 is given after every step otherwise.15 Note that the

≤

≤

15. In Section 8 we instantiate rφ in two diﬀerent ways and show the impact they have on learning.

1056

Induction and Exploitation of Subgoal Automata for RL

last case includes the scenario in which a formula diﬀerent from φ labeling an edge from the
current automaton state is satisﬁed. These policies are learned using Q-learning (Watkins,
1989) with (cid:15)-greedy exploration. The update rule for a given formula φ and an experience
tuple

is:

σt, at, σt+1
(cid:104)

(cid:105)

Qφ(σΣ

t , at) = Qφ(σΣ

t , at) + α

(cid:18)

rφ(σt+1) + γ max

a(cid:48)

Qφ(σΣ

t+1, a(cid:48))

−

(cid:19)

Qφ(σΣ

t , at)

,

(2)

where the second term of the target becomes 0 when either a terminal state is reached (i.e.,
t+1 is true) or the next observation satisﬁes φ (i.e., L(σΣ
σT

t+1)

Intra-option learning is easily applicable to update an option policy πφ(cid:48) while another
policy πφ is being followed. That is, given an option policy πφ, an experience tuple
generated by this policy is used to update the Q-value of (σΣ
t , at) of another
σt, at, σt+1
(cid:104)
formula φ(cid:48) through Equation 2.

(cid:105)

= φ).
|

Optimality. Since the sets of available options Ωu in non-terminal states u
U do not
include any primitive actions as one-step options, then optimal policies over the set of
available options are in general suboptimal policies of the core MDP (Dietterich, 2000;
Barto & Mahadevan, 2003).

∈

7.1.2 Learning an Option for each Automaton State (QRM)

Instead of learning one option for each outgoing edge and a metacontroller for each au-
tomaton state, we can learn a single policy over the space Σ
U . This policy is distributed
among automaton states; speciﬁcally, we learn a single option for each automaton state.
However, despite of this distribution, the global policy is still coupled everywhere since we
bootstrap the action-value from one automaton state to the next (details below). Therefore,
the policy of each single option in the automaton chooses the action that appears globally
best. In contrast, the previous method (HRL) decouples the option policies by making them
independent of one another (i.e., each attempts to satisfy a speciﬁc formula).

×

The method we describe here is better known as Q-learning for Reward Machines (QRM,
Toro Icarte et al., 2018). QRM was created to exploit the structure of Reward Machines
(RMs), a family of automata similar to our subgoal automata. The main diﬀerence is
that each transition in a RM is not only labeled by a propositional formula over a set of
observables, but also by a reward function. An in-depth comparison is made in Section 9.1.
We explain QRM in terms of options for a better comparison with the method we presented
in the previous section.

Option Modeling. Given a subgoal automaton
u

U encapsulates an option ωu =

A
where:

Iu, πu, βu
(cid:104)

∈

(cid:105)
• The initiation set Iu and the termination βu are deﬁned as in Section 7.1.1 for non-

=

U,

(cid:104)

O

, δϕ, u0, uA, uR

, each state
(cid:105)

terminal automaton states.

• The policy πu : Σ

A selects the action that appears globally best at a given state
(i.e., the action that leads to the fastest achievement of the task’s goal).
In other
words, the policy does not attempt to satisfy a particular formula: it will eventually
satisfy the formula that appears to be the best to reach the task’s goal. Therefore,

→

1057

Furelos-Blanco, Law, Jonsson, Broda, & Russo

the agent may act towards reaching diﬀerent formulas for diﬀerent regions of the state
space.

Policy Learning. Given an experience tuple
(cid:104)
learned through Q-learning updates of the form:

σt, at, σt+1

, the policy of an option ωu is
(cid:105)

Qu(σΣ

t , at) = Qu(σΣ

t , at) + α

r(u, u(cid:48)) + γ max

(cid:18)

Qu(cid:48)(σΣ

t+1, a(cid:48))

−

(cid:19)

Qu(σΣ

t , at)

,

(3)

a(cid:48)

where the discounted term depends on the next automaton state u(cid:48) (i.e., the policy in
the automaton state u is coupled with that of u(cid:48)) and becomes 0 when the next state is
terminal (i.e., σT
t+1 is true) since there is no applicable action thereafter. Crucially, the
reward r used in the update does not come from the POMDP. As said before, QRM is
originally applied on automata whose transitions are also labeled by reward functions. The
reward r is obtained by evaluating those functions. Since subgoal automata are not labeled
R of the
with reward functions, we assume that the reward function r : S
underlying POMDP is as follows:16

→

×

×

A

S

r(s, a, s(cid:48)) =

(cid:40)
1
0

if s(cid:48)
SG
∈
otherwise

.

Then, given the current automaton state u
∈
reward r in Equation 3 is always 0 except when we transition to the accepting state:

U and the next automaton state u(cid:48)

∈

U the

r(u, u(cid:48)) =

(cid:40)
1
0

= uA, u(cid:48) = uA

if u
otherwise

.

QRM performs the update in Equation 3 for all the options given a single

σt, at, σt+1
(cid:105)
(cid:104)
U , the next automaton
experience. That is, given the option ωu of automaton state u
state u(cid:48)
t+1)
of the next visible state in u. Note this is a form of intra-option learning: we update the
policies of all options from the experience generated by a single option’s policy.

U used for bootstrapping is determined by evaluating the observation L(σΣ

∈

∈

Optimality. Toro Icarte et al. (2018) proved that in the tabular case QRM is guaranteed
to converge to an optimal policy in the limit. Essentially, optimality is possible since action-
values are bootstrapped from one automaton state to the next.

Reward Shaping. A subgoal automaton does not only provide the subgoals of a given
task, but also gives an intuition of how far the agent is from achieving the task goal.
Intuitively, the closer the agent is to the accepting state, the closer it is to the task goal.
Therefore, we can provide the agent with an extra positive reward signal when it gets closer
to the accepting state. The idea of giving additional rewards to the agent to guide its
behavior is known as reward shaping.

Ng et al. (1999) proposed a function that provides the agent with additional reward

while guaranteeing that optimal policies remain unchanged:

16. This reward function is chosen because all evaluation tasks in Section 8 are characterized by it.

F (st, at, st+1) = γΦ(st+1)

Φ(st),

−

1058

(cid:54)
Induction and Exploitation of Subgoal Automata for RL

where γ is the MDP’s discount factor and Φ : S
automaton structure can be exploited by deﬁning F : (U
the automaton states instead (Camacho et al., 2019; Furelos-Blanco et al., 2020):

R is a real-valued function. The
R in terms of
U

uA, uR

\ {

→

→

×

}

)

F (u, u(cid:48)) = γΦ(u(cid:48))

Φ(u),

−

R. Consequently, Equation 3 is rewritten as:

where Φ : U

→

Qu(σΣ

t , at) = Qu(σΣ

t , at) + α

(cid:18)

r(u, u(cid:48)) + F (u, u(cid:48)) + γ max

a(cid:48)

Qu(cid:48)(σΣ

t+1, a(cid:48))

−

(cid:19)

Qu(σΣ

t , at)

.

Since we want the value of F (u, u(cid:48)) to be positive when the agent gets closer to the accepting
state uA, we deﬁne Φ as

Φ(u) =

U
|

| −

d(u, uA),

|

U
|

is the number of automaton states, and d(u, uA) is a measure of distance between
where
u and uA. If uA is unreachable from u, then d(u, uA) =
acts as an upper
bound of the maximum length (i.e., number of directed edges) of an acyclic path between
u and uA. The distance between u and uA can be given either by the length of the shortest
path between them (dmin) or by the length of the longest acyclic path between them (dmax).

.17 Note that

U
|

∞

|

Example 7.1. The following ﬁgures show the additional rewards generated by the reward
shaping function using dmin (left) and dmax (right) with γ = 0.99 in the Coffee task’s
automaton (see Figure 2, p. 1042). The numbers inside the states correspond to the values
returned by Φ, whereas the numbers on the edges are the values returned by F . Note that
U
|

= 4 and that the only diﬀerence in the Φ values occurs in the initial state.

|

0.03

−

0.02

−

start

3.0

start

2.0

+0.96

−∞

0.03

−

+0.96

+1.96

−∞

+0.97

+0.96

3.0

0.03

−

4.0

−∞

−∞

3.0

0.03

−

4.0

−∞

−∞

7.2 Interleaved Automaton Learning Algorithm

In this section we describe how the ISA algorithm interleaves reinforcement learning and
automaton learning. Given an episodic POMDP
, a set
2O, and a maximum number of edges κ
of observables

S, ST , SG, Σ, A, p, r, γ, ν
(cid:104)

, a labeling function L : Σ

Σ =

M

(cid:105)

O

→

17. In practice, we use a suﬃciently big number to represent ∞ (e.g., 106).

1059

Furelos-Blanco, Law, Jonsson, Broda, & Russo

, δϕ, u0, uA, uR

=
between two states, ISA aims to learn and iteratively reﬁne a subgoal automaton
from the experience of a reinforcement learning agent. The subgoal
U,
(cid:104)
O
is the automaton with the smallest number of states that has at most κ edges
automaton
from one state to another and is valid with respect to all the traces observed by the agent.

A

A

(cid:105)

Algorithm 1 ISA Algorithm
Input: An initial state (u0), an accepting state (uA), a rejecting state (uR), a set of ob-
, a labeling function L, and max. number of edges between two states (κ).

servables

(cid:46) Set of counterexamples

(cid:46) Initialize trace

do

(cid:46) Run episode

O
u0, uA, uR
U,

← {
A ← (cid:104)

}
, δϕ, u0, uA, uR

1: U
2:
O
3: ΛL,O
← {}
4: InitQFunctions(
5: for l = 0 to num episodes do
6:

)
A

(cid:105)

Env.InitialState()

←
δϕ(u0, L(σΣ))
L(σΣ)
(cid:105)

σΣ, σT , σG
u
←
λL,O
if IsCounterexample(σT , σG, u) then
OnCounterexampleFound(λL,O)
u

δϕ(u0, L(σΣ))

← (cid:104)

←
0

←

t
while t < max episode length
∧
SelectAction(σΣ, u)

σT =

⊥

←

←

Env.Step(a)

δϕ(u, L(σ(cid:48)Σ))

a
σ(cid:48)Σ, σ(cid:48)T , σ(cid:48)G, r
←
u(cid:48)
UpdateTrace(λL,O, L(σ(cid:48)Σ))
if IsCounterexample(σ(cid:48)T , σ(cid:48)G, u(cid:48)) then
OnCounterexampleFound(λL,O)
break

σΣ
t

←
t + 1

σ(cid:48)Σ; σT

σ(cid:48)T ; u

u(cid:48)

←

←

←

24:
25: function IsCounterexample(σT , σG, u)
σG =
u
26:
27: function OnCounterexampleFound(λL,O)
λL,O
28:

return (σG =

(σT =

= uA)

(cid:62) ∧

(cid:62) ∧

∨

}

ΛL,O

∪ {
true

ΛL,O
←
is unsat
←
while is unsat do
, is unsat

A
if is unsat then

←

U

U

←
ResetQFunctions(

∪ {

u|U |−2}
)
A

1060

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

17:

18:

19:

20:

21:

22:

23:

29:

30:

31:

32:

33:

34:

else

UpdateQFunctions(σΣ, a, σ(cid:48)Σ, σ(cid:48)T , σ(cid:48)G, L(σ(cid:48)Σ), r)

u

= uR)

(σT =

∨

u

⊥ ∧

uA, uR

∈ {

)
}

⊥ ∧

LearnAutomaton(U, u0, uA, uR, ΛL,O, κ)

(cid:54)
(cid:54)
Induction and Exploitation of Subgoal Automata for RL

Algorithm 1 contains the pseudocode describing how RL and automaton learning are
interleaved. The pseudocode applies to both of the methods described in Section 7.1 and
consists of two functions related to automaton learning:

• The IsCounterexample function (lines 25-26) checks whether the current automa-
. It returns true in
(cid:105)

ton state u correctly recognizes the current tuple σ =
the following cases:

σΣ, σT , σG
(cid:104)

– a goal state is reached and u is not the accepting state (σG =
u
– a dead-end state is reached and u is not the rejecting state (σT =

(cid:62) ∧

= uA), or

σG =

(cid:62) ∧

u

= uR), or

⊥ ∧

– the state is not a terminal state and u is either the accepting or rejecting state

(σT =

u

⊥ ∧

uA, uR

∈ {

).
}

• The OnCounterexampleFound function (lines 27-34) determines what to do when

a trace λL,O is not correctly recognized by the current automaton:

(a) Add λL,O to the corresponding set of traces (line 28):

– to the set of goal traces ΛG
– to the set of dead-end traces ΛD

σG =

), or
– to the set of incomplete traces ΛI

(cid:62) ∧

⊥

).

⊥

L,O if a goal state is reached (σG =

), or

(cid:62)

L,O if a dead-end state is reached (σT =

L,O if a terminal state is not reached (σT =

(b) Run the automaton learner (lines 29-33).

If the automaton learning task is
unsatisﬁable, it means that the hypothesis space does not include the automaton
we are looking for. Therefore, we add a new state to U .18 We adopt this iterative
deepening strategy to ﬁnd the subgoal automaton with the lowest number of
states with at most κ edges from one state to another.

(c) When a new automaton is learned, the Q-functions are reset (line 34). We later
explain what resetting the Q-functions means for the two RL algorithms we use.

We now describe the main function of the algorithm:

1. Initially, the set of states is formed by the initial state u0, the accepting state uA
and the rejecting state uR (line 1). The automaton is initialized such that it does not
accept nor reject anything; that is, there are no edges between the states in U (line 2).
The set of counterexample traces and the Q-functions are also initialized (lines 3-4).

2. When an episode starts, the current automaton state u is u0. One transition is then
applied depending on the agent’s initial observation L(σΣ) (lines 6-7). For instance,
in OfficeWorld’s Coffee task, then u must be
if the agent initially observes
the state where the agent has already observed (cid:75) (see Figure 2, p. 1042). The episode
trace λL,O is initialized with the initial observation L(σΣ) (line 8). If a counterexample
is detected at the beginning of the episode (line 9), a new automaton is learned
(line 10), the automaton state is reset (line 11) and the episode continues.

(cid:75)
}

{

18. Non-special states (i.e., not u0, uA or uR) are labeled from 1 upwards (u1, u2, . . .).

1061

(cid:54)
(cid:54)
Furelos-Blanco, Law, Jonsson, Broda, & Russo

3. At each episode’s step, we select an action a in state σΣ (line 14) and apply it in the
environment (line 15). Based on the new observation L(σ(cid:48)Σ), we get the next state u(cid:48)
(line 16) and update the observation trace λL,O (line 17). If a counterexample trace
λL,O is found (line 18), a new automaton is learned (line 19) and the episode ends
(line 20). Else, the Q-functions are updated (line 22) and the episode continues.

Theorem 7.1 shows that if the target automaton is in the hypothesis space, there will only
be a ﬁnite number of learning steps in the algorithm before it converges to such automaton
(or an equivalent one).

Theorem 7.1. Given a target ﬁnite automaton
automaton-counterexample pairs
such that
i does not cover ei, and (3)
(2)

A
i, ei
i: (1)
i is in the ﬁnite hypothesis space SM .

∗, there is no inﬁnite sequence ρ of
i covers all examples e1, . . . , ei−1,

A

∀

(cid:105)

(cid:104)A
A

A

Proof. By contradiction. Assume that ρ is inﬁnite. Given that SM is ﬁnite, the number
must appear in ρ at least twice,
of possible automata is ﬁnite. Hence, some automaton
say as
j covers ei. This is a
j, i < j. By deﬁnition,
contradiction.

i does not cover ei and

i =

A

A

A

A

A

In the following paragraphs we describe important aspects regarding the implementation
of the algorithm. Firstly, we describe how the Q-functions are managed for the two diﬀerent
algorithms we consider (HRL and QRM). Secondly, we introduce two optimizations to make
automaton learning more eﬃcient.

Management of Q-functions. A critical aspect of the algorithm is how Q-functions are
initialized, updated and reset when a new automaton is learned. In the tabular case, all the
Q-values for the diﬀerent state-action and/or state-option pairs are initialized to 0 for both
HRL and QRM. The Q-functions are updated using the rules described in Sections 7.1.1
and 7.1.2 respectively. Note that in the case of HRL, Algorithm 1 omits the call to the
function responsible for updating the metacontroller Q-functions, which occurs when the
selected option terminates.19

Ideally, when a new automaton is learned, we would like to reuse the knowledge from

the previous automata into the new one:

• In the case of HRL, as described in Section 7.1.1, the policies of all options that have
been used throughout learning are stored in a dictionary. Therefore, these policies can
be reused whenever their corresponding formulas appear in the automaton. There are
two choices to make in this case:

1. Which Q-functions to update. There are two alternatives: (i) update all the
stored Q-functions or (ii) update only the Q-functions of the formulas appearing
in the current automaton. While (i) is more costly, the fact that all Q-functions
are updated makes them more reliable if they are to be used in the future. Exper-
imentally, we use (i) for the tabular case and (ii) in the function approximation
case because of the running time.

19. We have omitted it for the generality of the pseudocode. Note that according to the termination condition
deﬁned in Section 7.1, an option ends when the episode terminates or the next automaton state is
diﬀerent. Therefore, such a check can be done at the end of each step (between lines 22 and 23).

1062

Induction and Exploitation of Subgoal Automata for RL

2. Reuse Q-functions by copying those deﬁned for similar formulas. Given that
options correspond to propositional formulas, we can compare how similar two
formulas are and, thus, initialize a new Q-function from an existing one. We
use the number of matching positive literals between two formulas; in case of
a draw, the formula whose Q-function has been updated the most is chosen.
Experimentally, the number of matching positive literals works better than the
number of all matching literals (positive and negative). Taking into account
negative literals can lead to negative transfer because they usually emerge from
the determinism constraints to make two formulas mutually exclusive; therefore,
they do not usually represent the “important” part of the subgoal formula in the
tasks we use later for evaluation. For this reason, we use the number of matching
positive literals in the experiments.

Unlike option policies, it is more diﬃcult to determine when to transfer the metacon-
troller (i.e., a policy over options) from one automaton to another. In this case, we
create a new Q-function and do not reuse any previous knowledge.

• In the case of QRM, the policy at each automaton state selects the action that appears
best towards satisfying the task’s ﬁnal goal. Therefore, it can attempt to satisfy
diﬀerent formulas for diﬀerent regions of the state space. This makes the transfer
of policies between automata non-trivial. In this case we just reinitialize all the Q-
functions, which causes the agent to forget everything it learned. Nevertheless, the
reward shaping mechanism introduced before can be helpful to alleviate this problem.

Optimizations.
learning phase more eﬃcient:

In practice we use two additional optimizations to make the automaton

1. The agent does not learn an automaton for the ﬁrst time until a goal trace is found
(i.e., the goal is achieved). Experimentally, we have observed that in tasks where
dead-ends are frequent, the learner constantly ﬁnds counterexamples with the aim to
reﬁne the paths to the rejecting state. Starting to learn automata only when there is
a goal trace in the counterexample set has proved to be a better strategy.

2. As explained in Section 5, the rejecting state uR is not included in the set of states
U if the set of dead-end traces is empty. This avoids an unnecessary increase in the
number of rules in the hypothesis space, specially in tasks without dead-end states.

8. Experiments

In this section, we evaluate the eﬀectiveness of ISA in diﬀerent domains. Our analysis
focuses on evaluating how the behavior of the RL agent and the task being learned aﬀect
automaton learning and vice versa. Firstly, we describe the main characteristics of our
evaluation methodology. Secondly, we make a thorough analysis of the performance of
our approach using the OfficeWorld (Toro Icarte et al., 2018), CraftWorld (Andreas
et al., 2017) and WaterWorld (Toro Icarte et al., 2018) domains.

We use ILASP2 to learn the automata with a 2 hour timeout for each automaton
learning task. All experiments ran on 3.40GHz Intel® Core™ i7-6700 processors. The
code is available at https://github.com/ertsiger/induction-subgoal-automata-rl.

1063

Furelos-Blanco, Law, Jonsson, Broda, & Russo

D

∗

A

∗
o
(cid:75)

∗

(cid:66)

(cid:75)

∗

∗

C

∗

B

start

u0

o

uA

Figure 4: Example of an OfficeWorld grid whose traces cause automata overgeneraliza-

tion for the Coffee task.

8.1 Experimental Setting

In this section, we describe how we evaluate our approach and introduce some restrictions
that we can apply on the automaton learning task. Then, we explain the nomenclature we
use for the diﬀerent RL algorithms we evaluate, and how we report the results.

D

=

M

{M

Σ
|D|}

Σ
1 , . . . ,

. All POMDPs in

Sets of POMDPs. We consider the problem of learning an automaton given a set of
correspond to the same task (e.g., Of-
POMDPs
ficeWorld’s Coffee task) and are enhanced by the same set of observables
. However,
they do not need to share the same state and action spaces. Algorithm 1 is applied on a set
by iteratively running one full episode for each POMDP, and repeating until
of POMDPs
a maximum total number of episodes is reached. We impose a maximum episode length
N to guarantee all episodes terminate in a reasonable amount of time, especially when the
terminal states are hard to reach. There are two reasons for which these sets are useful:

O

D

D

1. The automaton will generalize to several POMDPs. For example, in OfficeWorld
the coﬀee ((cid:75)) and the oﬃce (o) can sometimes be in the same location. Therefore, the
learned automaton should reﬂect these two situations: when (cid:75) and o are together and
when they are not (see Figure 2, p. 1042). Furthermore, using diﬀerent POMDPs can
help to avoid overgeneralization, which is related to the fact that a minimal automaton
is learned from positive examples only (see discussion in Section 10). Figure 4 shows
an OfficeWorld grid that if used alone to learn an automaton for Coffee would
produce the automaton on the right. Since the agent ( ) cannot see any trace that
reaches o without having seen (cid:75), it does not learn that observing (cid:75) is important to
reach the goal.

2. The set of POMDPs may contain some POMDPs in which it is easier to reach the
goal than in others. Therefore, automaton learning can be initialized earlier. The
RL agent can then immediately exploit the automaton in the harder POMDPs to
eﬀectively reduce the amount of exploration needed to reach the goal in them.

1064

Induction and Exploitation of Subgoal Automata for RL

The sets of POMDPs used in these experiments are not handcrafted, but randomly gen-
erated (e.g., placing observables randomly in a grid).20 Therefore, (1) there is no guarantee
that certain observations will be seen (e.g., two observables in the same tile of a grid), and
(2) the diﬃculty of the POMDPs is not fully controlled. Given the two previous conse-
quences of using randomly generated sets of POMDPs, we will evaluate how diﬀerent sets
aﬀect automaton learning and RL. Note that it is easy to handcraft tasks such that the
target automaton is learned. In the case of a grid-world, like OfficeWorld, we need at
most one grid for each path to the accepting state (in the absence of dead-ends). How-
ever, we believe that using a set of randomly generated POMDPs reduces our bias on the
automaton learning.

Restrictions. The following are restrictions that can be imposed on the automaton struc-
ture, the traces and the observables, and that are later used in the evaluation:21

• Avoid learning purely negative formulas. We assume that the tasks’ subgoals cannot
be characterized only by the non-occurrence of certain observables. That is, the
formula labeling an edge cannot be formed only by negated observables. The minimal
automata for the tasks we consider in this paper comply with this assumption, which
helps to slightly simplify the automaton learning phase.

• Acyclicity. There are tasks whose corresponding minimal automata do not contain
cycles (i.e., a previously visited automaton state cannot be revisited). For instance, the
three OfficeWorld tasks we have considered so far belong to this class of automata.
Thus, the search space can be made smaller by ruling out solutions containing cycles.

• Trace compression. The traces used as counterexamples in ISA depend on the agent
behavior. While the agent has not managed to reach the goal, its behavior is random.
Consequently, the counterexample that the agent provides to the automaton learner
can be long and include many observables that are irrelevant to the task at hand.
These two factors, as we will see later, have a negative impact on the time required
to learn an automaton.

• Restricted observable set. To simplify the traces, we can use only those observables
relevant to the task at hand. For example, if the task is CoffeeMail, then the set
. This
of observables becomes
greatly simpliﬁes the automaton learning tasks since the hypothesis space becomes
smaller and ILASP does not have to discern which are the relevant observables.

(cid:75), (cid:66), o, A, B, C, D,

instead of

(cid:75), o,

∗}

∗}

O

O

=

=

{

{

Reinforcement Learning Algorithms. We use the following nomenclature for the dif-
ferent RL algorithms applied in the experiments:

• HRL: HRL where rsuccess = 1.0, rdeadend = 0.0 and rstep = 0.0.

• HRLG: HRL where rsuccess = 1.0, rdeadend =

N and rstep =

−

0.01.

−

• QRM: QRM without reward shaping.

20. The constraints we impose in the POMDP generation are explained later for each of the used domains.
21. The details of these restrictions (e.g., implementation and examples) are given in Appendix C.1.

1065

Furelos-Blanco, Law, Jonsson, Broda, & Russo

• QRMmin: QRM with reward shaping based on the length of the shortest path to the

accepting state (dmin).

• QRMmax: QRM with reward shaping based on the length of the longest acyclic path

to the accepting state (dmax).

In the case of HRL algorithms, the option policies are updated as follows:

• In the tabular case, we update all the Q-functions of the formulas in the dictionary

after every step (i.e., all discovered formulas during learning).

• In the function approximation case, we update only the Q-functions of the formulas

appearing in the current automaton.

Reporting Results. We report results using tables and ﬁgures. In the tables we report
the following average automaton learning statistics across runs where the automaton learner
has not timed out and at least one automaton has been learned:

• Total time (in seconds) used to run the automaton learner.

• Number of examples needed to learn the ﬁnal automaton.

• Length of the examples used to learn the ﬁnal automaton.

For the ﬁrst two cases, the numbers in brackets correspond to the standard error, while in
the last case we use the standard deviation since it is an average of the example lengths
across all runs. We mark with an asterisk (*) cases where either no automaton has been
learned22 or the automaton learner has timed out between 1 and 10 runs. A dash (-) is
used if the number of such cases is higher than 10.

The ﬁgures show the average reward across the POMDPs in set

and the number of
runs (20). Each point of the learning curve represents the sum of rewards obtained by the
greedy policy at a given episode. By default, the greedy policy is evaluated after every
training episode for one episode. The dotted vertical lines correspond to episodes where an
automaton was learned. When the automaton learner times out, the reward is set to 0 for
the entire interaction.

D

8.2 Experiments in OfficeWorld

The OfficeWorld domain (Toro Icarte et al., 2018), introduced in Section 3, is char-
12 grid shown in Figure 1 (p. 1039). The set of observables is
acterized by the 9
. The grid contains one observable of each type except for
(6). The agent and the observables are

O
the coﬀee location (cid:75) (2) and the decoration
randomly placed in the grid using the following criteria:

(cid:75), (cid:66), o, A, B, C, D,
{

∗}

=

×

∗

• The agent cannot be initially placed with decorations

or observables A

D.

−

∗

• The decorations

∗

do not share a location with any other observable.

22. Remember that automata are started to be learned once a goal trace has been observed.

1066

Induction and Exploitation of Subgoal Automata for RL

0.1
0.1
0.99
10,000

Learning rate (α)
Exploration rate ((cid:15))
Discount factor (γ)
Number of episodes
Avoid learning purely negative formulas (cid:51)
50
Number of tasks (
)
|D|
250
Maximum episode length (N )
(cid:51)
Trace compression
(cid:51)
Enforce acyclicity
1
Number of disjuncts (κ)
(cid:55)
Use restricted observable set

Table 1: Parameters used in the OfficeWorld experiments.

• The decorations

and observables A

D cannot be placed next to each other (in-

cluding diagonals) nor in locations that connect two rooms (e.g., (1, 2) and (1, 3)).

∗

−

• Observables A

−

D and the oﬃce o cannot be in the same location.

Note that (cid:75), (cid:66) and o are allowed to be in the same location, and that (cid:75) and (cid:66) can share
a location with any observable A

D.

The tasks in this domain are the ones we introduced in Section 3: Coffee, CoffeeMail
and VisitABCD. These tasks constitute a good test-bed since their automata are diﬀerent
and they are incrementally more challenging:

−

• The Coffee task has 2 subgoals and is represented by a 4 state minimal automaton.

• The CoffeeMail task has 3 subgoals and is represented by a 6 state minimal au-

tomaton.

• The VisitABCD task has 4 subgoals and is represented by a 6 state minimal au-

tomaton.

In the tasks we consider in this paper, the number of subgoals is deﬁned as the number of
directed edges of the longest acyclic path from the initial state to the accepting state in
the minimal automaton. In general, the longer the sequence of subgoals is, the harder it
becomes to achieve the goal.

Table 1 shows the parameters used throughout these experiments. The parameters α,
(cid:15) and γ are the same for both the metacontrollers and the options in the case of HRL.
In the following paragraphs we compare the learning curves produced by the two types of
RL algorithms we have previously presented (HRL and QRM). We show how learning the
automata in an interleaved manner aﬀects RL and how introducing guidance (e.g., reward
shaping in QRM) inﬂuences when counterexamples are found. In Appendix C.2 we analyze
how parameter tuning aﬀects automaton learning and reinforcement learning.

Figure 5 shows how the learning curves with interleaved automaton learning (ISA-HRL,
ISA-QRM) compare to those obtained with handcrafted automata (HRL, QRM). We ob-
serve the following:

1067

Furelos-Blanco, Law, Jonsson, Broda, & Russo

• The algorithms using auxiliary guidance (HRLG, QRMmin and QRMmax) converge
faster than their respective basic versions (HRL and QRM). The use of auxiliary re-
ward signals helps to explore the state space more eﬀectively, which results in observ-
ing counterexample traces early. Consequently, automaton learning is less frequent
in the last episodes, which is convenient to avoid resetting the Q-functions late and
leave enough episodes to converge.

• QRMmax converges faster than QRMmin except in VisitABCD where they perform
exactly the same because there is a single path to the accepting state. As previously
shown in Example 7.1, QRMmax provides a positive reward signal for any path that
allows the agent to approach the accepting state. In contrast, QRMmin only provides
a positive signal for the shortest path(s). If the shortest path is not available in a
certain grid (e.g., (cid:75) and o occurring together), QRMmin gives a negative reward for
choosing the only available path to the accepting state. Consequently, convergence is
not as fast as in QRMmax.

• HRL converges faster than QRM across the diﬀerent tasks. In the absence of reward
shaping, QRM needs to satisfy the formula on an edge to the accepting state to start
propagating positive reward through the diﬀerent states. On the other hand, HRL
can independently update the Q-functions of each of the formulas in the automaton
without having to achieve the task’s goal.

• The curves for the settings involving automaton learning perform closely to the ones
with handcrafted automata. Naturally, sometimes the convergence is slower due to the
fact that a proper automaton cannot be exploited from episode 0. This is noticeable in
VisitABCD, where a stable automaton requires several relearning steps to be found.

Figure 6 shows the impact of automaton learning on the HRL and QRM learning curves
of the Coffee task in a single run.23 Remember that an HRL agent only forgets what it
learned at the metacontroller level and keeps the Q-functions of the formulas unchanged.
In contrast, QRM forgets
Besides, it reuses the Q-functions between similar formulas.
everything it has learned. The plot illustrates this behavior: while HRL quickly recovers,
QRM requires a few more episodes to match HRL again.

Table 2 shows the automaton learning statistics for the presented OfficeWorld tasks

using HRLG. We observe that:

• The running time increases with the number of subgoals. An automaton for the
VisitABCD task takes (on average) more time than one for the CoffeeMail task
even though they are both characterized by automata with the same number of states.

• The number of examples increases with the number of subgoals of the task. The
number of goal examples is approximately the same as the number of paths to the
accepting state. For instance, in VisitABCD there is only one such path, so the
number of goal examples is approximately 1.24 On the other hand, the observables that

23. Note that abrupt changes in the learning curves do not occur in other plots because they are averaged

across 20 runs. Hence, changes are smoother and not fully visible.

24. The number of goal trace examples can sometimes be higher than 1 for VisitABCD if the ﬁrst used goal
trace example is complex (e.g., longer than needed or with many unnecessary symbols), thus making the
subgoals unclear. In such cases a simpler goal trace might be found as a counterexample.

1068

Induction and Exploitation of Subgoal Automata for RL

x
a
m
M
R
Q
A
S
I

-

i

n
m
M
R
Q
A
S
I

-

M
R
Q
A
S
I

-

x
a
m
M
R
Q

i

n
m
M
R
Q

M
R
Q

G
L
R
H
A
S
I

-

L
R
H
A
S
I

-

G
L
R
H

L
R
H

1069

ﬀ
o

s
i

g
n
i
n
r
a
e
l

n
o
t
a
m
o
t
u
a

d
e
v
a
e
l
r
e
t
n
i

n
e
h
w

s
k
s
a
t

d
l
r
o
W
e
c
i
f
f
O
e
h
t

n
i

s

m
h
t
i
r
o
g
l
a

L
R

t
n
e
r
e
ﬀ
i
d

r
o
f

s
e
v
r
u
c

g
n
i
n
r
a
e
L

:
5

e
r
u
g
i
F

.
)

M
R
Q
A
S
I

-

,

L
R
H
A
S
I
(

-

n
o

d
n
a

)

M
R
Q

,

L
R
H
(

02004006008001000Numberofepisodes0.00.20.40.60.81.0AveragerewardCoffee05001000150020002500Numberofepisodes0.00.20.40.60.81.0AveragerewardCoffeeMail02000400060008000Numberofepisodes0.00.20.40.60.81.0AveragerewardVisitABCD02004006008001000Numberofepisodes0.00.20.40.60.81.0AveragerewardCoffee05001000150020002500Numberofepisodes0.00.20.40.60.81.0AveragerewardCoffeeMail02000400060008000Numberofepisodes0.00.20.40.60.81.0AveragerewardVisitABCDFurelos-Blanco, Law, Jonsson, Broda, & Russo

Figure 6: Example of the impact that interleaved automaton learning has on the learning
curves of the Coffee task. An automaton is learned around episode 300. While
HRL quickly recovers (it only has to relearn the policies over options), QRM
needs some more episodes because it forgets everything.

Time (s.)

# Examples

Example Length

All

G

D

I

0.4 (0.0)
Coffee
18.9 (3.3)
CoffeeMail
VisitABCD 163.2 (44.3)

8.7 (0.4)
29.0 (1.5)
54.9 (3.8)

2.4 (0.1)
3.9 (0.3)
1.6 (0.1)

3.0 (0.1)
9.3 (0.6)
15.2 (0.9)

3.2 (0.3)
15.8 (1.0)
38.1 (3.1)

2.8 (2.1)
4.0 (2.6)
5.5 (3.1)

Table 2: Automaton learning statistics for the OfficeWorld tasks using HRLG.

characterize the CoffeeMail automaton can appear jointly or not; consequently,
there are more paths to the accepting state and, thus, the required number of goal
examples increases. Furthermore, while there is a relationship between the number
of goal examples and the number of paths to the accepting state, we do not observe
such relationship between the number of dead-end examples and the number of paths
to the rejecting state. The number of dead-end and incomplete examples is higher
than that of goal examples; thus, we hypothesize that these two kinds of examples are
mainly used to reﬁne the automaton given the set of goal examples.

• The example length increases with the number of subgoals.

Intuitively, the more
subgoals, the longer the agent will have to interact with the environment to achieve
the goal. Therefore, the observed counterexamples tend to be longer for the tasks
with more subgoals.

1070

02004006008001000Numberofepisodes0.00.20.40.60.81.0AveragerewardHRLQRMInduction and Exploitation of Subgoal Automata for RL

8.3 Experiments in CraftWorld

The CraftWorld domain (Andreas et al., 2017) consists of a 39
39 grid without walls.
The grid contains raw materials (wood, grass, iron) and tools/workstations (toolshed, work-
bench, factory, bridge, axe), which constitute the set of observables
. There are 5 labeled
locations for each material, and 2 labeled locations for each tool/workstation. Like in Of-
ficeWorld: (i) the agent moves in the four cardinal directions and remains in the same
location if it tries to cross the grid’s limits, and (ii) at each timestep, the agent knows at
which cell of the grid it is (i.e., the history of achieved subgoals is not stored in the state),
and sees the observables at that cell. The grids are randomly generated such that all items
must be in diﬀerent locations (i.e., the observations consist of one observable at most).

O

×

The tasks in this domain consist in observing a speciﬁc sequence of materials and

tools/workstations. We use the set of tasks in (Toro Icarte et al., 2018) for evaluation:

1. MakePlank: wood, toolshed.

2. MakeStick: wood, workbench.

3. MakeCloth: grass, factory.

4. MakeRope: grass, toolshed.

5. MakeShears: iron, wood, workbench (the iron and the wood can be observed in any

order).

6. MakeBridge:

iron, wood, factory (the iron and the wood can be observed in any

order).

7. GetGold: iron, wood, factory, bridge (the iron and the wood can be observed in any

order).

8. MakeBed: wood, toolshed, grass, workbench (the grass can be observed anytime

before the workbench.)

9. MakeAxe: wood, workbench, iron, toolshed (the iron can be observed anytime before

the toolshed).

10. GetGem: wood, workbench, iron, toolshed, axe (the iron can be observed anytime

before the toolshed).

Tasks 1-4 have 2 subgoals and are represented by 3 state minimal automata. Tasks 5-6
have 3 subgoals and are represented by 5 state minimal automata. Task 7 has 4 subgoals
and is represented by a 6 state minimal automaton. Tasks 8-9 have 4 subgoals and are
represented by 7 state minimal automata. Task 10 has 5 subgoals and is represented by
an 8 state minimal automaton. The agent gets a reward of 1 upon the goal’s achievement
and 0 otherwise. Unlike OfficeWorld, this domain has no dead-end states, so the set of
dead-end examples is always empty.

Table 3 lists the parameters used in these experiments. The only diﬀerence with respect
, which is 100

to the default parameters used in OfficeWorld is the number of tasks

|D|

1071

Furelos-Blanco, Law, Jonsson, Broda, & Russo

0.1
0.1
0.99
10,000

Learning rate (α)
Exploration rate ((cid:15))
Discount factor (γ)
Number of episodes
Avoid learning purely negative formulas (cid:51)
Number of tasks (
)
|D|
Maximum episode length (N )
Trace compression
Enforce acyclicity
Number of disjuncts (κ)
Use restricted observable set

100
250
(cid:51)
(cid:51)
1
(cid:55)

Table 3: Parameters used in the CraftWorld experiments.

in this case. Experimentally, we observed that using 100 instead of 50 was a better choice
for tasks 8-10 which, as we will explain later, occasionally time out.

Table 4 shows the automaton learning statistics for the presented CraftWorld tasks
using HRLG. Note that we have divided the tasks into several groups according to the
number of subgoals they have and the number of states that their corresponding minimal
automata have. Figure 7 shows the learning curves for one representative of each group of
tasks25 with and without interleaved learning of automata. We observe the following:

• Like in the OfficeWorld tasks, the more subgoals and automaton states, the higher
the values for the collected metrics (running time, number of examples and example
length). Besides, the number of goal examples still corresponds to the number of
paths from the initial state to the accepting state. The ﬁgure also shows that, as
before, learning becomes more frequent as the tasks become harder.

• The automaton learning statistics are very close between groups of tasks, especially
for the ones having simpler automata. As the tasks become harder, the diﬀerences
between tasks in the same group become bigger (e.g., MakeBed and MakeAxe).
Naturally, it is extremely unlikely that an agent observes two equivalent sets of exam-
ples for two diﬀerent tasks, especially when examples become longer (as we have seen
before, the more subgoals a task has, the longer the examples become). Therefore, it
is normal that these diﬀerences arise for harder tasks.

• The running time increases dramatically from GetGold to MakeBed and MakeAxe.
Actually, the automaton learner has timed out a few times for the latter tasks: 5 for
MakeBed and 4 for MakeAxe. Furthermore, in the case of GetGem, the harder
task, it has timed out 9 times. The number of timeouts varies between algorithms,
which is probably caused by exploration. For example, standard HRL has timed out
8 times for MakeAxe and only once for MakeBed.

• The diﬀerence between the curves where interleaved automaton learning is on (ISA-
HRL, ISA-QRM) and oﬀ (HRL, QRM) is small for most of the tasks, like in Of-

25. The learning curves are similar between members of each group, so we report just one of them.

1072

Induction and Exploitation of Subgoal Automata for RL

Time (s.)

# Examples

Example Length

MakePlank
MakeStick
MakeCloth
MakeRope

MakeShears
MakeBridge

GetGold

MakeBed
MakeAxe

GetGem

0.2 (0.0)
0.2 (0.0)
0.3 (0.0)
0.2 (0.0)

2.0 (0.3)
1.7 (0.3)

All

G

I

4.4 (0.3)
3.6 (0.2)
4.9 (0.4)
4.2 (0.3)

16.2 (0.8)
15.5 (1.3)

1.4 (0.1)
1.2 (0.1)
1.2 (0.1)
1.2 (0.1)

3.3 (0.2)
3.0 (0.2)

3.0 (0.3)
2.4 (0.2)
3.6 (0.4)
2.9 (0.3)

12.8 (0.8)
12.5 (1.2)

60.7 (25.7)

30.6 (3.2)

2.2 (0.2)

28.5 (3.2)

2140.4 (1071.7)*
2990.3 (717.7)*

37.1 (3.1)*
46.6 (3.5)*

3.8 (0.3)*
3.3 (0.2)*

33.3 (3.0)*
43.3 (3.5)*

6179.4 (2784.8)*

116.8 (14.7)*

1.2 (0.1)*

115.6 (14.7)*

2.2 (1.2)
2.2 (1.4)
2.4 (1.5)
2.4 (1.4)

3.4 (1.7)
3.0 (1.5)

4.0 (1.9)

4.0 (1.7)*
4.3 (1.9)*

5.2 (2.0)*

Table 4: Automaton learning statistics for the CraftWorld tasks using HRLG.

ficeWorld. This shows that the induced automata are useful to learn a policy that
reaches the goal. Note that for the hardest tasks (MakeAxe, MakeBed and Get-
Gem) the gap between HRL and QRM with respect to ISA-HRL and ISA-QRM is
usually bigger than for other tasks. This is due to the presence of timeouts in the
approaches that induce automata, as explained before.

• When an automaton is handcrafted, QRMmin performs like QRMmax because the
minimum and maximum distances to the accepting state are the same in these au-
tomata. This similarity also occurs when an automaton is learned; however, they are
not identical since the intermediately learned automata may cause some variances.

• The approaches not using guidance (HRL, QRM) start converging faster than the ones
that use it (HRLG, QRMmin, QRMmax). However, the latter eventually learn to reach
the goal earlier in the interaction. The initially slower convergence for the approaches
using guidance can be due to the fact that guidance encourages more exploration. A
CraftWorld grid is bigger than an OfficeWorld grid, which causes the agent
to explore the environment for longer and delays the start of convergence. However,
all the knowledge acquired while exploring is later quickly exploited and the learning
curves surpass those of the approaches without guidance.

In Section 10 we present several proposals for future work to reduce the running time
that ILASP needs to ﬁnd an automaton, thus making the approach more scalable to tasks
whose minimal automata have many states, such as GetGem.

8.4 Experiments in WaterWorld

The WaterWorld domain (Toro Icarte et al., 2018), which is illustrated in Figure 8,
consists of a 2D box containing 12 balls of 6 diﬀerent colors (2 balls per color). Each ball
moves at a constant speed in a given direction. The balls bounce only when they collide
with a wall. The agent is a white ball that can change its velocity in any of the four cardinal
is formed by the balls’ colors. The
directions. The set of observables

=

r, g, b, y, c, m
{

}

O

1073

Furelos-Blanco, Law, Jonsson, Broda, & Russo

F
i
g
u
r
e

7
:

L
e
a
r
n
i
n
g

c
u
r
v
e
s

f
o
r

d
i
ﬀ
e
r
e
n
t

R
L

a
l
g
o
r
i
t
h
m

s

i
n

t
h
e

C
r
a
f
t
W
o
r
l
d

t
a
s
k
s

w
h
e
n

i
n
t
e
r
l
e
a
v
e
d

a
u
t
o
m
a
t
o
n

l
e
a
r
n
i

n
g

i
s

o
ﬀ

a
u
t
o
m
a
t
a

c
u
r
v
e
s

b
e
c
a
u
s
e

o
f

t
h
e

t
i

m
e
o
u
t

r
u
n
s
.

W
h
e
n

a

r
u
n

ﬁ
n
i
s
h
e
s

s
u
c
c
e
s
s
f
u
l
l
y
,

b
o
t
h

t
y
p
e
s

o
f

c
u
r
v
e
s

a
r
e

s
i

m

i
l
a
r
.

(
H
R
L

,

Q
R
M

)

a
n
d

o
n

-

(
I
S
A
H
R
L

,

-

I
S
A
Q
R
M

)
.

T
h
e

I
S
A
c
u
r
v
e
s

f
o
r

M
a
k
e
B
e
d

a
n
d
G
e
t
G
e
m
a
r
e

b
e
l
o
w
t
h
e

h
a
n
d
c
r
a
f
t
e
d

-

I
S
A
Q
R
M
m
a
x

H
R
L

H
R
L
G

-

I
S
A
H
R
L

-

I
S
A
H
R
L
G

-

I
S
A
Q
R
M
m
n

i

Q
R
M

Q
R
M
m
a
x

Q
R
M
m
n

i

-

I
S
A
Q
R
M

1074

0200040006000800010000Numberofepisodes0.00.20.40.60.81.0AveragerewardMakeStick0200040006000800010000Numberofepisodes0.00.20.40.60.81.0AveragerewardMakeShears0200040006000800010000Numberofepisodes0.00.20.40.60.81.0AveragerewardMakeStick0200040006000800010000Numberofepisodes0.00.20.40.60.81.0AveragerewardMakeShears0200040006000800010000Numberofepisodes0.00.20.40.60.81.0AveragerewardGetGold0200040006000800010000Numberofepisodes0.00.20.40.60.81.0AveragerewardMakeBed0200040006000800010000Numberofepisodes0.00.20.40.60.81.0AveragerewardGetGold0200040006000800010000Numberofepisodes0.00.20.40.60.81.0AveragerewardMakeBed0200040006000800010000Numberofepisodes0.00.20.40.60.81.0AveragerewardGetGem0200040006000800010000Numberofepisodes0.00.20.40.60.81.0AveragerewardGetGemInduction and Exploitation of Subgoal Automata for RL

Figure 8: The WaterWorld domain (Toro Icarte et al., 2018).

agent observes a color when it overlaps with the ball painted with it. For example, in
Figure 8 the agent would observe
(green). Note that several balls can overlap at the
same time; thus, the agent can simultaneously observe several colors. The tasks we consider
consist in observing a sequence of colors in a speciﬁc order:

g
{

}

• RGB: red (r) then green (g) then blue (b). It consists of 3 subgoals and is represented

by a 4 state minimal automaton.

• RG-B: red (r) then green (g) and (independently) blue (b). Note that there are two

sequences and they can be interleaved. For instance,
and
task. It consists of 3 subgoals and is represented by a 6 state minimal automaton.

}(cid:105)
are three possible goal traces. Note that RGB is a subcase of this

,
b
}
(cid:104){

,
b
}(cid:105)

r
(cid:104){

r
(cid:104){

,
b
}

g
{

g
{

g
{

r
{

,
}

,
}

}(cid:105)

{

}

}

{

,

,

• RGBC: touch red (r) then green (g) then blue (b) then cyan (c).

It consists of 4

subgoals and is represented by a 5 state minimal automaton.

The agent gets a reward of 1 upon the goal’s achievement and 0 otherwise. The tasks we
consider have no dead-end states, like in CraftWorld. The balls start with a random
position and direction at the beginning of each episode.

Unlike OfficeWorld and Craftworld, the state space is continuous, so we cannot
Instead, like Toro Icarte et al. (2018), we use a Double DQN
use tabular Q-functions.
(DDQN, van Hasselt et al., 2016) to approximate the Q-functions in both HRL and QRM.
The neural networks consist of 4 hidden layers of 64 neurons, each followed by a ReLU. A
network’s input is a vector containing the absolute position and velocity of the agent, and
the relative positions and velocities of the other balls. Just like in a standard DQN (Mnih
et al., 2015), the output contains the estimated Q-value for each action (or option in the
case of a metacontroller in HRL). We train the neural networks using the Adam optimizer
10−5. The target networks are updated every 100 steps.
(Kingma & Ba, 2015) with α = 1
A total of 50,000 episodes are run for each setting with parameters (cid:15) = 0.1 and γ = 0.9.
The Q-functions are updated using batches of 32 experiences uniformly sampled from an
experience replay buﬀer of size 50,000. The learning starts after collecting 1,000 samples.

×

Table 5 lists the mentioned neural network hyperparameters along with the automaton
learning and RL parameters. The main diﬀerence with respect to the previous experiments
is the use of a POMDP set consisting of a single POMDP. Since the balls in WaterWorld

1075

Furelos-Blanco, Law, Jonsson, Broda, & Russo

10−5

1
Learning rate (α)
×
0.1
Exploration rate ((cid:15))
0.9
Discount factor (γ)
50,000
Number of episodes
150
Maximum episode length (N )
50,000
Replay memory size
1,000
Replay start size
32
Batch size
1
Number of tasks (
100
Target network update frequency
(cid:51)
Trace compression
(cid:51)
Enforce acyclicity
Number of disjuncts (κ)
1
Avoid learning purely negative formulas (cid:51)
(cid:55)
Use restricted observable set

)
|D|

Table 5: Parameters used in the WaterWorld experiments.

Time (s.)

# Examples

Example Length

All

G

I

3.7 (0.3)
RGB
RG-B 129.5 (23.9)
RGBC 111.8 (23.1)

26.9 (1.1)
48.4 (1.2)
61.4 (2.7)

7.0 (0.4)
15.4 (0.4)
11.7 (0.5)

20.0 (0.9)
33.0 (1.0)
49.6 (2.4)

4.3 (2.3)
4.4 (2.2)
5.5 (2.7)

Table 6: Automaton learning statistics for the WaterWorld tasks using HRLG.

are constantly moving, it is easier for the agent to observe all possible combinations of
observables. Therefore, we do not need to use a larger set to learn a general automaton.

Figure 9 shows the learning curves for the WaterWorld tasks introduced before.
We show how those that use automaton learning (ISA-HRL, ISA-QRM) compare to those
obtained with handcrafted automata (HRL, QRM). The greedy policy was evaluated every
500 episodes: the evaluation consisted in running the greedy policy for 10 diﬀerent episodes
and averaging the reward obtained across them. The learning curves are smoothed using
a sliding window of size 1,000. Table 6 shows the automaton learning statistics for these
tasks using HRLG.26 We observe the following:

• The tasks with more subgoals run the automaton learner more often. For instance,
automaton learning is concentrated at the beginning of the interaction in RGB; in
contrast, it is called at many diﬀerent times in RGBC.

• Even though the minimal automaton of RG-B has more states than that of RGB,
the reward is sparser in the latter since there are not as many ways to achieve the
goal as in the former. This is why RGB’s learning curve does not converge faster
than RG-B’s. Note that the time and the number of examples needed to learn the

26. The automaton learning statistics for the other RL algorithms are similar, so we do not report them.

1076

Induction and Exploitation of Subgoal Automata for RL

RG-B automaton are higher than those for RGB because its set of automaton states
is bigger. Finally, the fact that RG-B is the task requiring more goal traces shows
that it is the task where achieving the goal is easier although it makes automaton
learning harder.

• Even though RGBC has more subgoals than RG-B, its running time is lower. The
most likely reason is that the minimal automaton for RG-B has more states and,
therefore, the hypothesis space is bigger. Naturally, the bigger the hypothesis space
is, the harder it becomes to ﬁnd a solution. However, as we saw in the grid-world
experiments, the tasks with more subgoals require more examples.
In particular,
RGBC needs more incomplete examples than RG-B possibly because there are more
sequences of candidate subgoals to be discarded.

• The approaches based on HRL perform better than the ones based on QRM, specially

in RGBC. We hypothesize there are two possible causes for this behavior:

(i) The Q-functions resetting. Remember that while in HRL only the metacon-
trollers are reset, all the Q-functions in QRM are reset. Given that new automata
are learned throughout the entire interaction, the agents using QRM rarely have
the chance to converge to a stable policy.

(ii) While the HRL agent commits to satisfying a given formula (determined by
the metacontroller) at a given step, the QRM agent selects the globally best
action at each step. Unlike the grid-worlds, in this domain the observables are
constantly changing their position, so it can be more diﬃcult to learn a function
that generalizes to many scenarios (i.e., diﬀerent initializations of the task).27

To determine which of these two causes is more plausible, we examine the performance
of QRM with a handcrafted automaton. In general, the performance of approaches
using automaton learning are very similar to the ones using a handcrafted automaton.
This shows that the forgetting eﬀect is not as present in WaterWorld as in the grid-
world domains. The use of experience replay is likely to be responsible for this because
the agents can update the Q-functions without having to relive successful experiences
that happened in the past. Therefore, we conclude that cause (ii) is better supported
by our experiments.

• There is barely any diﬀerence between the learning curves of the algorithms that do
not use auxiliary guidance (HRL, QRM), and the ones that do use it (HRLG, QRMmin,
QRMmax). Camacho et al. (2019) also showed similar behavior in the case of QRM
using handcrafted reward machines with a diﬀerent reward shaping mechanism. We
hypothesize that since the Q-functions must generalize to diﬀerent settings (remember
that all episodes start with a random conﬁguration), an agent that does not use guid-
ance might explore similarly to an agent that does use it. Therefore, using guidance
does not help much in these tasks.

27. We highlight that our evaluation QRM in the WaterWorld domain diﬀers a bit from the one by Toro
Icarte et al. (2018). While we randomly initialize the environment at the start of every episode, they use
a ﬁxed map. In the latter case, QRM quickly converges in RGBC (we have been able to reproduce the
results), but we consider that learning Q-functions that generalize to diverse scenarios is more interesting.

1077

(
H
R
L

,

Q
R
M

)

a
n
d

o
n

-

(
I
S
A
H
R
L

,

-

I
S
A
Q
R
M

)
.

F
i
g
u
r
e

9
:

L
e
a
r
n
i
n
g

c
u
r
v
e
s

f
o
r

d
i
ﬀ
e
r
e
n
t

R
L

a
l
g
o
r
i
t
h
m

s

i
n

t
h
e

W
a
t
e
r
W
o
r
l
d

t
a
s
k
s

w
h
e
n

i
n
t
e
r
l
e
a
v
e
d

a
u
t
o
m
a
t
o
n

l
e
a
r
n
i

n
g

i
s

o
ﬀ

Q
R
M

Q
R
M
m
n

i

Q
R
M
m
a
x

-

I
S
A
Q
R
M

-

I
S
A
Q
R
M
m
n

i

-

I
S
A
Q
R
M
m
a
x

Furelos-Blanco, Law, Jonsson, Broda, & Russo

H
R
L

H
R
L
G

-

I
S
A
H
R
L

-

I
S
A
H
R
L
G

1078

01000020000300004000050000Numberofepisodes0.00.20.40.60.81.0AveragerewardRGB01000020000300004000050000Numberofepisodes0.00.20.40.60.81.0AveragerewardRG-B01000020000300004000050000Numberofepisodes0.00.20.40.60.81.0AveragerewardRGBC01000020000300004000050000Numberofepisodes0.00.20.40.60.81.0AveragerewardRGB01000020000300004000050000Numberofepisodes0.00.20.40.60.81.0AveragerewardRG-B01000020000300004000050000Numberofepisodes0.00.20.40.60.81.0AveragerewardRGBCInduction and Exploitation of Subgoal Automata for RL

8.5 Summary of the Results

Throughout this section we have observed several commonalities between the results across
domains. We provide a short summary of the main common ﬁndings below:

• The higher the number of task subgoals and the number of required states are, the
higher the values for the collected metrics (running time, number of examples and
example length).

• The number of goal examples used to learn an automaton is approximately the same
as the number of paths from the initial state to the accepting state.
Incomplete
and dead-end examples are used to reﬁne those paths and increase as the number of
subgoals and automaton states increases.

• Using auxiliary reward signals (or guidance) has been shown useful to speed up conver-
gence in grid-world tasks. Importantly, as mainly shown in OfficeWorld, it helps
to learn an automaton early in the interaction and thus reduces relearning later. This
is extremely helpful in QRM since it resets all the Q-functions when a new automaton
is learned. On the other hand, in WaterWorld there is not a big diﬀerence between
approaches using guidance and those which do not use it.

• HRL usually converges faster than QRM, specially in the absence of guidance.

In
such case, QRM needs to satisfy the goal at least once to start propagating reward
through the automaton states. In contrast, HRL can update the Q-functions of the
formulas independently and, importantly, these can be kept throughout the entire
learning process.

• The diﬀerence between the approaches that learn an automaton and those that use
a handcrafted one is not big except for the cases where an automaton cannot be
normally learned under the imposed timeout. This shows that the learned automata
properly represent the subgoal structure of the tasks.

9. Related Work

The following sections describe the works that share more commonalities with ours. First,
we qualitatively compare subgoal automata and ISA with other forms of automata and
automaton learning methods that have been recently used in RL. Second, we brieﬂy discuss
some work on discovering hierarchies in HRL. Finally, we describe some of the work on
breaking symmetries in graphs and automata in particular.

9.1 Automata in Reinforcement Learning

In this section we describe others forms of automata that have been used in reinforcement
learning and how their structure has been exploited. We compare these automata and the
approaches for learning them to subgoal automata and ISA respectively.

Reward Machines. Subgoal automata are similar to another formalism that uses au-
tomata in RL called reward machines (RMs, Toro Icarte et al., 2018). Both automata

1079

Furelos-Blanco, Law, Jonsson, Broda, & Russo

consist of edges labeled by propositional formulas over a set of observables
diﬀerences with respect to our formalism are:

O

. The main

1. RMs do not have explicit accepting and rejecting states. Therefore, they can be used

to represent continuing tasks.

2. RMs specify a reward-transition function δr : U
an automaton state pair into a reward function.

U

[S

A

S

×

×

→

×

→

R] that maps

U

×

→

Toro Icarte et al. (2018) also deﬁne simple RMs where the reward-transition function δr :
R maps an automaton state pair into a reward instead of a reward function.
U
The authors propose the QRM (Q-learning for Reward Machines) algorithm to exploit the
structure of RMs, which was one of the RL algorithms we have applied on our automata.
Diﬀerent methods for simultaneously learning and exploiting simple reward machines
from observation traces have been recently proposed. Toro Icarte et al. (2019) introduce
LRM, which formulates the RM learning problem as a discrete optimization problem and use
a local search algorithm called tabu search to solve it. They use QRM to learn the policies.
Unlike our approach, LRM starts by performing random actions for a ﬁxed number of steps
to collect some traces. These traces are used for two things:

1. Learn an initial automaton.

2. Know which is the set of observables

of the task.28 This is done by looping through
the traces. Interestingly, if two events happen at the same time in these traces, then
they will constitute a single observable. For example, if the traces show that (cid:75) and o
happen together in OfficeWorld’s Coffee task, then the set observables
would
include a single observable (cid:75)o instead of two distinct observables (cid:75) and o.

O

O

Note that these two aspects are diﬀerent from our approach (ISA) because:

1. ISA does not learn automata from a set of randomly collected traces.

Instead, it

learns a new automaton when a counterexample is found.

2. ISA is given the set of observables

in advance. However, we could also determine

the set of observables just like LRM does.

O

Even though LRM learns an initial automaton from random traces, it uses counterexamples
to reﬁne the automata as well. LRM is also diﬀerent from ISA in the following aspects:

• The traces used by LRM consist of observations formed by a single observable. This
described above. Therefore,
is due to the process for getting the set of observables
O
. This can be problematic
conjunctions are not learned but assumed to be given in
if the automaton is to be used in a task where the set of observables is diﬀerent.
Keeping observables separated and explicitly learning the conjunctions as we do is
less constrained, but allows for better generalization. Note that the observations we
use can contain an arbitrary number of observables.

O

28. The set can be later extended if more observables are found in the following steps.

1080

Induction and Exploitation of Subgoal Automata for RL

• LRM does not need to enforce mutual exclusivity between the edges to two diﬀerent
automaton states. The reason is that each observable in
always happens alone, as
explained before. In contrast, ISA needs to enforce mutual exclusivity (see Section 6).

O

• LRM’s optimization scheme is used to ﬁnd an automaton that is good at predicting
what will be the next diﬀerent observable given a maximum number of states. Thus,
they do not aim to ﬁnd a minimal automaton. In contrast, ISA aims to ﬁnd a minimal
automaton that covers the example traces through the use of an iterative deepening
strategy on the number of states. This has two main consequences:

– The fact that ISA looks for a minimal automaton using only positive examples
(i.e., traces obtained from the agent-environment interaction) makes it prone to
overgeneralize in certain tasks (see Section 10 for a detailed discussion). LRM
does not suﬀer from this problem given that they do not look for a minimal
automaton, but an automaton that is good at predicting what will be the next
diﬀerent observation.

– Since LRM aims to be good at predicting which will be the next diﬀerent ob-
servation, it uses traces where no two consecutive observations are equal. This
can be seen as a form of trace compression similar to the one we use in the
experiments (see details in Appendix C.1). Therefore, their method cannot be
applied to tasks like counting how many times an observable has been seen in a
row, while ours can handle these tasks as long as the traces are not compressed.

• LRM does not classify examples into diﬀerent categories. The reward machine it aims
to learn can also represent continuing tasks; therefore, they do not use an explicit
notion of goal or dead-end as we do.

• LRM does not apply a symmetry breaking mechanism and may consider diﬀerent
equivalent solutions during the search for an automaton. ISA uses symmetry breaking
constraints to shrink the search space and speed up automaton learning.

Xu et al. (2020) propose another algorithm to learn simple RMs called JIRP (Joint
Inference of Reward Machines and Policies). The authors express the automaton learning
problem as a SAT problem and use QRM to learn a policy for a given task. JIRP is similar
to ISA in the following aspects:

1. It aims to learn a minimal automaton based on an iterative deepening strategy on the

number of states.

2. The automaton learner is triggered when a counterexample is found.

3. It learns only from positive examples (i.e., attainable traces by the RL agent).

4. It learns RMs for episodic tasks where the reward is 1 only when the goal is achieved
and 0 otherwise.29 Therefore, like in our case, these RMs consist of absorbing accept-
ing and rejecting states. The rejecting state, however, seems to be induced implicitly
and not indicated explicitly as in our case.

29. Note that we assumed 0/1 reward tasks only when we used QRM and not HRL (see Section 7.1.2).

1081

Furelos-Blanco, Law, Jonsson, Broda, & Russo

Unlike ISA, the traces used by JIRP do not only consist of observations but they also include
rewards. Crucially, these sequences of rewards are used to determine counterexamples: if a
trace in the environment yields a diﬀerent sequence of rewards in the automaton, then that
trace is a counterexample. Furthermore, there are four other diﬀerences:

• JIRP does not call the automaton learner after every single counterexample. Instead,

it accumulates them into a batch and calls the automaton learner periodically.

• JIRP learns reward machines whose edges are labeled by sets of observables, which
is similar to what LRM does.30 In contrast, the edges of a subgoal automaton are
labeled by propositional formulas over a set of observables.

• JIRP learns conditions for the loop transitions. In contrast, our approach takes loop

transitions only when outgoing transitions to other states cannot be taken.

• JIRP reuses the Q-functions learned by QRM when a new automaton is learned. The
Q-function at a given state u is reused in a new state u(cid:48) if they are equivalent. Two
states are equivalent if they yield the exact same sequence of rewards for all traces in
the set of counterexamples. We decided not to include this mechanism in the QRM
experiments for several reasons. As explained in Section 7.1.2, the policy at a given
automaton state selects the action that appears to be best to achieve the task’s ﬁnal
In other words, that policy might aim to satisfy any of the formulas on the
goal.
outgoing edges, so it is unclear what should be transferred to the new automaton.
Importantly, the transfer proposed by Xu et al. is based on the sequence of rewards a
trace yields from a given automaton state, and not on which subgoals the policies are
trying to achieve. In our view, the latter aspect is what should be taken into account.

Gaon and Brafman (2020) learn a deterministic ﬁnite automaton (DFA), so the tran-
sitions are labeled by symbols instead of propositional formulas. However, it still shares
some commonalities with reward machines and subgoal automata. The tasks they consider
are also episodic and terminate when the goal is reached. The main diﬀerence with re-
spect to subgoal automata is that the set of observables
contains an observable for each
action. Therefore, their automata are learned from action traces. The authors use two
well-known algorithms from the grammatical inference literature (de la Higuera, 2010) to
learn a minimal DFA: L* and EDSM (Evidence Driven State Merging):

O

• L* (Angluin, 1987) is an active learning algorithm that can learn a DFA from just a

polynomial number of queries. Typically, two types of queries are considered:

– Membership queries: the learner requests to label a trace (i.e., state whether the

trace belongs to the language or not).

– Equivalence queries: the learner asks whether its automaton captures the target

language. If it does not, a counterexample is returned.

30. Xu et al.’s paper shows RMs whose edges are labeled by propositional formulas. However, we have
veriﬁed through personal communication with the authors that the transitions are indeed labeled by sets
of observables. The propositional formulas were used to make the representation simpler in the paper.

1082

Induction and Exploitation of Subgoal Automata for RL

Gaon and Brafman use the RL agent as the oracle. A membership query is answered
by trying to reproduce the sequence of actions in it, whereas an equivalence query
is answered by checking if the trace has been observed in the past. Note that the
equivalence queries can be unfeasible traces and, thus, overgeneralization is controlled.
However, their method might be prone to making wrong guesses, specially in large
state and action spaces where it is unlikely that a given trace has been seen before.

• EDSM (Lang et al., 1998) is a state-merging approach to automaton learning, which

consists of two phases:

1. Build an initial DFA called Preﬁx Tree Acceptor (PTA), which is a tree-structured
DFA built from the preﬁxes of a ﬁnite set of traces such that it accepts the pos-
itive traces in the set and rejects the negative ones.

2. Iteratively choose pairs of equivalent states to merge and produce a new automa-
ton. If the automaton does not cover all the examples, it backtracks and chooses
another pair of states. When no additional merging is possible, it stops.

The automaton learned by EDSM depends on the quality of the example set and,
under speciﬁc conditions, the algorithm is proved to converge to the minimal DFA.
The complexity of the algorithm is polynomial in the number of examples.

To apply EDSM, Gaon and Brafman keep a record of the traces that reach the goal
and those that do not reach the goal. Note that this is similar to what we do, although
our dead-end and incomplete traces would be both inside their set of traces that do
not reach the goal because they do not have an explicit rejecting state.

State-merging approaches follow a diﬀerent path to minimality than our approach and
Xu et al.’s. The former start from a big set of states and aim to reduce it, whereas
the latter begin with a small set of states and increase it when becomes insuﬃcient to
cover the examples. Nevertheless, even though these approaches are slightly diﬀerent,
they can both generate overgeneralized automata if only positive examples are used,
as we show in Section 10.

The authors combine the automaton learning approaches with Q-learning and a model-based
RL algorithm called R-max (Brafman & Tennenholtz, 2002) and test them in tabular tasks.
Note that while the minimality of our automaton is dependent on the maximum number of
edges between two states (κ), the approaches by Xu et al. (2020) and Gaon and Brafman
(2020) do not mention this dependence (they both show examples of automata involving
disjunctions).

Automaton structures have also been exploited in reward machines to give bonus re-
ward signals. Camacho et al. (2019) convert reward functions expressed in various formal
languages (e.g., linear temporal logic) into RMs, and propose a reward shaping method that
runs value iteration on the RM states. Similarly, Camacho et al. (2017) use automata as
representations of non-Markovian rewards and exploit their structure to guide the search of
an MDP planner using reward shaping. In our case, we have proposed two reward shaping
mechanisms based on the maximum and minimum distances to the accepting state.

1083

Furelos-Blanco, Law, Jonsson, Broda, & Russo

Hierarchical Abstract Machines (HAMs). Throughout the paper we have shown
the connection between subgoal automata and the options framework (Sutton et al., 1999),
which is one of the classical approaches for HRL along with HAMs (Parr & Russell, 1997)
and MAXQ (Dietterich, 2000). Even though our method is closer to options, it is also similar
to HAMs in that both use an automaton. However, HAMs are non-deterministic automata
whose transitions can invoke lower level machines and are not labeled by observables (the
high-level policy consists in deciding which transition to ﬁre).

Relational Macros. Torrey et al. (2007) learn a kind of automata, similar to subgoal
automata, called relational macros, which are ﬁnite state machines where both states and
transitions are characterized by ﬁrst-order logic formulas. These formulas are built on the
ﬁrst-order logic predicates that describe the environment states. The formulas on the au-
tomaton states indicate which action to take, while the formulas on the transitions say when
the transition is taken. The learning of the relational macros is done in two phases. The
structure learning phase ﬁnds a sequence of actions that distinguishes traces reaching the
goal from those which do not and composes them into an automaton. In the ruleset learning
phase, their system learns the conditions for choosing actions and for taking transitions.
The authors use Aleph (Srinivasan, 2001), an inductive-logic programming system, to learn
the rules in both phases from positive and negative examples. The learned automaton is
used for transfer learning: the target task follows the strategy encoded by the automaton
for some steps to estimate the Q-values of the actions in the strategy, and then stops using
the automaton and acts according to the Q-values. This approach diﬀers from ours in that:

1. The traces it uses to learn the automata are formed by actions and not high-level
events (observables). However, similarly to us, the traces are divided into groups
depending on whether they reach the goal or not.

2. The transitions are labeled by ﬁrst-order logic formulas instead of propositional for-

mulas.

3. It learns logic rules that describe what action to take in each automaton state, while

we learn policies to choose the actions.

4. A relational macro requires that the target task has the same action space as the source
task since the rules are deﬁned on these actions. In contrast, a subgoal automaton
can be reused in another task if the set of observables and the goal are the same, even
when the state and action spaces are diﬀerent.

Note that the ﬁrst-order logic predicates are similar to our observables. Even though ob-
servables are propositional, both provide a high-level abstraction of the state space.

Policy Graphs. Meuleau et al. (1999) propose to represent policies with ﬁnite memory
using a class of ﬁnite-state automata called policy graphs. Like subgoal automata, policy
graphs are applied to POMDPs. The states of a policy graph are labeled with actions,
while each edge is labeled by a single visible state.31 This diﬀers from our case where the
edges are labeled by propositional formulas over a set of high-level events (i.e., the observ-
ables). Another diﬀerence is that the transition function between states in the automaton is

31. Note that we use the terminology for POMDPs introduced in Section 2.

1084

Induction and Exploitation of Subgoal Automata for RL

probabilistic. This function is represented as a parametric function and is learned through
stochastic gradient descent using a set of traces obtained by the RL agent. Similarly to us,
policy graphs are applied to tasks with a subset of states characterizing the tasks’ goals.

Moore Machines. Koul et al. (2019) transform the policy encoded by a Recurrent Neural
Network (RNN) into a Moore machine, which is a quantized version of the RNN. That is,
the Moore machine is deﬁned in terms of quantized state and observation representations
of the RNN. Unlike our method and the previously presented works, the authors use the
resulting machine for interpretability and do not exploit its structure.

9.2 Hierarchical Reinforcement Learning (HRL)

In Section 7 we described two RL algorithms for exploiting the structure of a subgoal
automaton using the options framework (Sutton et al., 1999), which is one of the classical
approaches for HRL along with HAMs (Parr & Russell, 1997) and MAXQ (Dietterich,
2000).

One of the core problems in the options framework is ﬁnding a set of options that
helps to maximize the return instead of handcrafting such set. This problem is known
as option discovery. The method we propose in this paper, ISA, can certainly be seen as
an option discovery method. The family of option discovery methods where ISA ﬁts best
are bottleneck methods, which ﬁnd “bridges” between regions of the latent state space. In
particular, each state of our automata can represent a diﬀerent region of the state space,
and the bottleneck is represented by the formula connecting two automaton states. The
option discovery method most similar to ours, except for the reward machine related ones
(see Section 9.1), is due to McGovern and Barto (2001). Their approach uses diverse density
to ﬁnd landmark states in state traces, and it is similar to ours because:

1. It learns from traces.

2. It classiﬁes traces into two diﬀerent categories depending on whether they achieve the

goal or not.

3. It interleaves option discovery and learning of policies for the discovered options.

The main diﬀerence is that while our bottlenecks are propositional logic formulas, theirs
are crucial states to achieve the task’s goal. Therefore, they do not use/require a set of
propositional events (i.e., observables) to be provided in advance.

Just like some option discovery methods (e.g., McGovern & Barto, 2001; Stolle & Precup,
2002), our approach requires the task to be solved at least once. Other methods (e.g.,
Menache et al., 2002; Simsek & Barto, 2004; Simsek et al., 2005; Machado et al., 2017)
discover options without solving the task and, thus, are also suited to continuing tasks.

Alternative formalisms to automata for expressing formal languages, like grammars,
have been used to discover options. Lange and Faisal (2019) induce a straight-line gram-
mar, a non-branching and loop-free context-free grammar, which can only generate a single
string. The authors use greedy algorithms to ﬁnd a straight-line grammar from the shortest
sequence of actions that leads to the goal. The production rules are then ﬂattened, lead-
ing to one macro-action (a sequence of actions) per production rule. These macro-actions
constitute the set of options.

1085

Furelos-Blanco, Law, Jonsson, Broda, & Russo

Similarly to options, there has been work on learning the structures used in other HRL
frameworks. Leonetti et al. (2012) synthesize a HAM from the set of shortest solutions to
a non-deterministic planning problem, and use it to reﬁne the choices at non-deterministic
points through RL. Mehta et al. (2008) propose a method for discovering MAXQ hierarchies
from a trace that reaches the task’s goal.

9.3 Symmetry Breaking

The symmetry breaking mechanism that we have proposed in this paper has been shown to
help decrease the time needed to ﬁnd a subgoal automaton that covers a set of examples.
In this section we brieﬂy mention some of the most related works to ours; that is, those
addressing the problem of breaking symmetries in graphs (speciﬁcally, automata) or that
use ASP to encode problems.

SAT-based approaches to learning deterministic ﬁnite automata (DFA) have used sym-
metry breaking constraints to shrink the search space. Heule and Verwer (2010) reduce
the DFA learning problem to a graph coloring problem, which is translated into SAT. The
graph to be colored is derived from the Preﬁx Tree Acceptor (PTA, see p. 1083) of the
examples by connecting two states if they cannot be merged. The vertices in a k-clique
must be colored diﬀerently and there are k! diﬀerent ways of coloring them. The authors
propose to break these symmetries by imposing a way of assigning colors after ﬁnding a
large clique using an approximation algorithm (given that the problem is NP-complete).
On the other hand, similarly to us, orderings based on well-known search algorithms like
breadth-ﬁrst search (BFS, Ulyantsev et al., 2015; Zakirzyanov et al., 2019) and depth-ﬁrst
search (DFS, Ulyantsev et al., 2016) have also been proposed. State-merging approaches
to learning DFA have also used BFS to break symmetries (Lambeau et al., 2008). These
BFS-based methods are diﬀerent from ours in that they do not need to deﬁne a comparison
criteria for sets of symbols since they are applied to DFA. Remember that the edges in a
DFA are labeled by a single symbol, whereas the edges in a subgoal automaton might be
labeled by formulas with more than one symbol. Besides, unlike previous works, we prove
that the assignment of state and edge indices given by our mechanism is unique.

Given the successes of the use of symmetry breaking in SAT solving, the ASP community
has also produced some work on symmetry breaking. Drescher et al. (2011) propose sbass,
a system that detects symmetries in a ground ASP program through a reduction to a
graph automorphism problem. Then, it adds constraints to the initial program to break
the detected symmetries.

Codish et al. (2019) propose a method that breaks symmetries in undirected graphs by

imposing a lexicographical order in the rows of the adjacency matrix.

In our previous work (Furelos-Blanco et al., 2020), we introduced a method for breaking

symmetries in acyclic subgoal automata, which consists in:

1. assigning an integer index to each automaton state such that u0 has the lowest index

and uA and uR have the highest indices; and

2. imposing that a trace must visit automaton states in increasing order of indices.

1086

Induction and Exploitation of Subgoal Automata for RL

However, this method cannot break symmetries when there is not a trace that traverses all
states in the automaton (e.g., if there are two diﬀerent paths to the accepting state). We
illustrate this drawback with the example below.

Example 9.1. Figure 3 (p. 1050) showed two automata whose states u1, u2 and u3 can
be used interchangeably if no symmetry breaking is used. If we assign indices 0, . . . , 3 to
states u0, . . . , u3 and apply the symmetry breaking rule in (Furelos-Blanco et al., 2020), the
learned automaton for OfficeWorld’s VisitABCD task will always be the one showed
in Figure 3a. On the other hand, the automaton states u1 and u2 in Figure 3b can still be
switched since there is no trace that traverses both of them.

The method presented in this paper does not depend on the sequence of states visited

by the traces. Therefore, it can break both symmetries given in the example above.

10. Conclusions and Future Work

In this paper we have proposed ISA, a method that interleaves the learning and exploitation
of an automaton whose edges encode the subgoals of an episodic goal-oriented task. The
subgoals are expressed as propositional logic formulas over a set of high-level events that
the agent observes when interacting with the environment. These automata are represented
using a logic programming language and learned with a state-of-the-art inductive logic
programming system from traces of high-level events observed by the agent. Importantly,
we have devised a symmetry breaking mechanism that speeds up the automaton learning
phase by avoiding to revisit equivalent solutions in the hypothesis space. Besides, the
interleaving mechanism we propose ensures that the learned automata are minimal (i.e.,
have the fewest number of states). We have experimentally tested ISA with diﬀerent types
of tasks, showing that it is capable of learning automata that can be exploited by existing
reinforcement learning techniques. We have also shown how automaton learning aﬀects
reinforcement learning and vice versa, and that ISA achieves a performance comparable to
the setting where the automaton is handcrafted and given beforehand.

We now discuss possible improvements to our algorithm and directions for future work.

Learning from Positive and Negative Examples.
ISA aims to learn a minimal sub-
goal automaton from positive examples only (i.e., traces that the agent can observe), which
can lead to overgeneralization (Angluin, 1980). More speciﬁcally, in our case, the learned
automaton will be too general in tasks where there is a temporal dependency between the
observables (e.g., an observable o can only be observed if another observable o(cid:48) has been
observed before). Previously, we also showed that overgeneral automata might be learned
when key traces to learn all subgoals cannot be observed (see Figure 4, p. 1064).32

for the OfficeWorld environment
Example 10.1. Imagine that the set of observables
includes an observable g that states whether the task’s goal has been achieved. In the case of
the Coffee task, g is observed after the agent has been in the coﬀee location (cid:75) and then in
the oﬃce location o; thus, a possible goal trace is
. The automaton learner
could then output the automaton below.

o, g
{

(cid:75)
}

,
{}

}(cid:105)

(cid:104){

O

,

32. Remember that we addressed this issue by learning an automaton that generalizes to a set of POMDPs.

1087

Furelos-Blanco, Law, Jonsson, Broda, & Russo

start

u0

g

uA

This automaton is minimal and there is no other positive example that contradicts it because
g is only observed when the goal is reached.

To avoid overgeneralization, we need traces that are impossible to observe from the
agent-environment interaction. These traces would constitute the set of negative examples,
which are supported by ILASP, the system we have used to learn the automata. For instance,
the trace
would be a negative example for the task described in Example 10.1. Since
it is impossible to observe such traces, the agent must be able to learn or hypothesize what
is unfeasible in the environment.

g
(cid:104){

}(cid:105)

To discover negative examples, our method would need to determine whether a given
trace is feasible or not. We hypothesize that approaches which learn the POMDP’s model
(that is, the transition probability function) would be useful to solve the problem along
with more sophisticated exploration strategies than (cid:15)-greedy.

Observable Discovery.
ISA assumes that an appropriate set of observables is given in
advance. The same occurs with other methods that learn automaton structures, like reward
machines (Toro Icarte et al., 2019; Xu et al., 2020). Discovering the set of observables
from interaction is an important task towards automating the entire automaton learning
process. Possible future work could extract observables from object keypoints in images
(Kulkarni et al., 2019) or represent the abstract states from an abstracted state space using
observables.

Improve Scalability. Learning a minimal automaton from examples is a well-known
hard problem (Gold, 1978). The main factors that aﬀect the scalability of our approach
are the number of states, whether cycles are allowed, the observable set size, the length of
the counterexample traces, and the maximum number of edges between two states. Even
though we have introduced some methods for improving scalability (symmetry breaking
and trace compression), there are several interesting directions for future work:

• Other trace compression methods. It has been shown that shorter traces make learning
faster. More sophisticated compression techniques than the one we have presented (see
Appendix C.1) could be considered. However, compression can make the learning of
certain automata unfeasible. Thus, the assumptions on the environments and target
automata must be clearly stated.

• Hierarchies of automata. While learning complex automata can take some time with
the current approach, learning small automata does not take long. Therefore, we can
think of hierarchies of small automata that invoke each other. Furthermore, once a
small automaton has been learned in one task, it might be reusable in another task.

• Revision of traces. Some of the collected counterexamples during learning can be
long and/or contain many observables which are irrelevant to the task at hand (e.g.,
observing (cid:66) in OfficeWorld’s Coffee task). Thus, if at some point in the learning
we realize that a counterexample is no longer needed because it is subsumed by a
simpler one (i.e., shorter and without irrelevant observables), then we can replace it.

1088

Induction and Exploitation of Subgoal Automata for RL

• Other automaton learning methods. The L* algorithm (Angluin, 1987) is a query
learning algorithm that can learn a DFA from just a polynomial number of membership
and equivalence queries made to an oracle. One of the main criticisms of this paradigm
is that it does not adapt to practical situations where there is not an oracle. However,
there is recent work where Recurrent Neural Networks (RNNs) are used to play the
role of the oracle (Weiss et al., 2018). Michalenko et al. (2019) have shown that there
is a close relationship between the internal representations used by RNNs and ﬁnite
state automata. Therefore, we can consider using this family of methods in the future.

More Expressive Automata. A natural extension of this work is to learn automata
whose edges are labeled by ﬁrst-order logic formulas instead of propositional logic formulas.
This would allow to enable features not currently supported by subgoal automata, such
as counting (without using an arbitrary number of edges). The ILASP system can learn
ﬁrst-order logic rules.

Acknowledgments

The authors would like to thank the anonymous reviewers for their helpful comments and
suggestions. Anders Jonsson is partially supported by the Spanish grants PCIN-2017-082
and PID2019-108141GB-I00.

Appendix A. Proof of Proposition 4.1

To prove Proposition 4.1, we use the following result due to Gelfond and Lifschitz (1988):

Theorem A.1. If an ASP program P is stratiﬁed, then it has a unique answer set.

Now, we give the deﬁnition of a stratiﬁed ASP program and proceed to prove Proposi-

tion 4.1.

Deﬁnition A.1. An ASP program P is stratiﬁed when there is a partition

P = P0

P1

∪

∪ · · · ∪

Pn

(Pi and Pj disjoint for all i

= j)

such that, for every predicate p

• the deﬁnition of p (all clauses with p in the head) is contained in one of the partitions

Pi

and, for each 1

i

≤

≤

n:

• if a predicate occurs positively in a clause of Pi then its deﬁnition is contained within

(cid:83)

j≤i Pj.

• if a predicate occurs negatively in a clause of Pi then its deﬁnition is contained within

(cid:83)

j<i Pj.

1089

(cid:54)
Furelos-Blanco, Law, Jonsson, Broda, & Russo

Proposition 4.1 (Correctness of the ASP encoding). Given a ﬁnite observation trace λ∗
where
P = M (
if

G, D, I
R
= G, and (2) reject

L,O) has a unique answer set AS and (1) accept

, and an automaton
}
M (λ∗

L,O,
L,O, the program
AS if and only

that is valid with respect to λ∗

AS if and only if

∗ ∈ {
)
∪
A

= D.

A

∪

∈

∗

∈

∗

Proof. First, we prove that the program P = M (
L,O) has a unique answer set.
By Theorem A.1, if P is stratiﬁed then it has a unique answer set. Therefore, we show there
is a possible way of partitioning P following the constraints in Deﬁnition A.1. A possible
partition is P = P0

P3, where:

P2

P1

A

R

∪

∪

)

M (λ∗

∪

∪

∪

P0 = M (λ∗

L,O), P1 = M (

), P2 = Rϕ, P3 = Rδ

Rst.

∪

A

Remember that R = Rϕ
where ASi corresponds to partition Pi:

Rδ

∪

∪

Rst. The unique answer set is AS = AS0

AS1

AS2

∪

∪

∪

AS3,

AS0 = (cid:8)obs(o, t).

λ∗
L,O[t], 0

o

|

∈

n(cid:9)

t

≤

≤

∪ {

step(t).

0

|

≤

t

≤

n

} ∪ {

last(n).
}

,

AS1 =

state(u).
{
|
ed(u, u(cid:48), i).
|
{
(cid:110)
¯ϕ(u, u(cid:48), i, t).

u
U
∈
u, u(cid:48)

} ∪
∈
u, u(cid:48)

U, 1
i
≤
≤ |
U, conji ∈

∈

ϕ(u, u(cid:48))
|} ∪
ϕ(u, u(cid:48)), 0

n, λ∗

L,O[t]

t

≤

= conji
(cid:54)|

≤

AS2 =

(cid:110)

ϕ(u, u(cid:48), i, t).

(cid:110)

out-ϕ(u, t).

u, u(cid:48)

u

∈

∈
U, 0

U, conji ∈
n,
t
≤
≤

u(cid:48)
∃

∈

ϕ(u, u(cid:48)), 0

t

n, λ∗

≤
U s.t. λ∗

≤
L,O[t]

= conji
|
(cid:111)

L,O[t]
= ϕ(u, u(cid:48))
|

|

|

|

,
(cid:111)

(cid:111)

∪
,

AS3 =

(cid:110)

δ(u, u(cid:48), t).

(cid:110)

δ(u, u, t).
st(0, u0).
{
(cid:110)
st(t, u).

(cid:110)

accept.

(cid:110)

reject.

} ∪
1

|

| A

| A

u, u(cid:48)

|

|

u

∈

∈
U, 0

U, 0

t

≤

≤

t

≤

n, λ∗

= ϕ(u, u(cid:48))
L,O[t]
|
U s.t. λ∗

L,O[t]

≤
n, (cid:64)u(cid:48)

∈

= ϕ(u, u(cid:48))
|

(cid:111)

∪

(cid:111)

∪

.

L,O)[t]

(cid:111)

∪

t

n + 1, u =

≤
(λ∗

(λ∗

≤

A
L,O)[n + 1] = uA
L,O)[n + 1] = uR

(λ∗
(cid:111)

∪

(cid:111)

We now prove that accept
= G then, since the automaton is valid with respect to λ∗

AS if and only if

∈

∗

∗

If
automaton traversal
This holds if and only if accept

(λ∗

A

= G (i.e., the trace achieves the goal).
L,O (see Deﬁnition 3.5), the
L,O)[n+1] = uA.

(λ∗

A

L,O) ﬁnishes in the accepting state uA; that is,

The proof showing that reject

AS if and only if

dead-end) is similar to the previous one. If
respect to λ∗

L,O, the automaton traversal

∗
(λ∗

(λ∗

A

L,O)[n + 1] = uR. This holds if and only if reject

AS.

∈

= D (i.e., the trace reaches a
= D then, since the automaton is valid with
L,O) ﬁnishes in the rejecting state uR; that is,

∗

AS.

∈

∈

A

1090

Induction and Exploitation of Subgoal Automata for RL

Appendix B. Symmetry Breaking Encodings

In this section we describe the details of our symmetry breaking mechanism. For ease of
presentation, we ﬁrst encode it in the form of a satisﬁability (SAT) formula and formally
prove several of its properties. We later explain how to convert this SAT formula into
ASP rules. Finally, we propose a more eﬃcient ASP encoding of the symmetry breaking
constraints than the direct translation from SAT.

B.1 SAT Encoding

Γu
f,
of
Our idea is to deﬁne a SAT formula that encodes a BFS traversal
u∈V
}
{
(cid:104)
(cid:105)
. Since
l1, . . . , lk
, deﬁned on a set of labels
a given graph G = (V, E) in the class
}
{
a graph indexing
(G) assigns unique integers to nodes and edges, we use i to refer to a
node u such that f (u) = i, (i, e) or (i, e, L) to refer to an edge (u, v, L) such that f (u) = i
. We sometimes extend this notation in the
and Γu(L) = e, and m to refer to a label lm
natural way, e.g. by writing Γi(L), Eo(i) and ΠI(i).

(G) =
=

I
L

∈ L

G

I

Variables. We ﬁrst deﬁne a set
symbols (sometimes with restrictions as indicated):

X

of propositional SAT variables for all combinations of

1. ed(i, j, e),
2. label(i, e, m),
3. pa(i, j),
4. sm(i, j, e),
5. lt(i, e

[edge (i, e) ends in node j]
[the label set on edge (i, e) includes lm
[node i is the parent of j in the BFS subtree]
[e is the smallest integer on a BFS edge from node i to j]

]
∈ L

i < j,
i < j,

−

1, e, m), e > 1, [there is a label lm(cid:48)|m(cid:48)≤m on (i, e) and not on (i, e
Intuitively, variables ed(i, j, e) and label(i, e, m) are used to encode a graph G together with
(G), variables pa(i, j) and sm(i, j, e) are used to encode the
an associated graph indexing
1, e, m) are used to encode the label set ordering.
parent function ΠI, and variables lt(i, e

1)]

−

I

Clauses. We next deﬁne a set
for all combinations of symbols (some-
times with restrictions as indicated). The ﬁrst set of clauses (1-8) enforces the ﬁrst condition
in Deﬁnition 6.2: for any two nodes i > 1 and j > 1, ΠI(i) < ΠI(j)

i < j.

X

C

−
of clauses on

i|i<j pa(i, j),

1. (cid:87)
2. pa(i, j)
3. pa(i, j)
4. pa(i, j)
5. sm(i, j, e)
6. sm(i, j, e)
7. sm(i, j, e)
8. sm(i, j, e)

⇒ ¬
(cid:87)
⇒
⇒ ¬
⇒
⇒ ¬
⇒
⇒ ¬

pa(i(cid:48), j),
e sm(i, j, e),
ed(i(cid:48), j(cid:48), e),
pa(i, j),

j > 1,
i < i(cid:48) < j,
i < j,
i(cid:48) < i < j
i < j,

j(cid:48),

≤
sm(i, j, e(cid:48)), i < j, e < e(cid:48),

ed(i, j, e),

i < j,
ed(i, j(cid:48), e(cid:48)), i < j

≤

⇔

[node j > 1 has incoming BFS edge]
[incoming BFS edge is unique]
[BFS edge implies smallest integer]
[respect BFS order]
[smallest integer implies BFS edge]
[smallest integer is unique]
[smallest integer implies edge]

j(cid:48), e(cid:48) < e, [correctly break ties]

Intuitively, Clauses 1 and 2 state that each node j > 1 has a unique parent node i in the
BFS subtree. Clauses 3, 5 and 6 state that each node j > 1 has a unique incoming edge
(i, e) with smallest integer e from its parent i in the BFS subtree. Clause 7 ensures that the
incoming edge (i, e) to j in the BFS subtree corresponds to an actual edge in the graph G.
Clauses 4 and 8 constitute the core of symmetry breaking by enforcing the condition
that ΠI(i) < ΠI(j) should imply i < j. By deﬁnition of ΠI, the incoming edge (i, e) to j

1091

Furelos-Blanco, Law, Jonsson, Broda, & Russo

in the BFS subtree should be the lexicographically smallest such integer pair. Hence the
graph G cannot contain any incoming edge (i(cid:48), e(cid:48)) to j from a node i(cid:48) < i. In addition,
no node j(cid:48) > j can have such an incoming edge either, since otherwise its parent function
would be smaller than that of j, thus violating the desired condition. These two facts are
jointly encoded in Clause 4 by enforcing the restriction j(cid:48)

j.

Likewise, if (i, e) is the incoming edge to j in the BFS subtree, graph G cannot contain
an incoming edge (i, e(cid:48)) from the same node i with e(cid:48) < e. Again, no node j(cid:48) > j can have
such an incoming edge either, since otherwise its parent function would be smaller than that
of j. These two facts are jointly encoded in Clause 8 by enforcing the restriction j(cid:48)

j.

The second set of clauses (9-14) assigns edge integers to the outgoing edges from each
node, enforcing the second condition in Deﬁnition 6.2: for each node i and pair of outgo-
ing edges (i, e, L) and (i, e(cid:48), L(cid:48)), L < L(cid:48)
e < e(cid:48). Due to the transitivity of the relation
<, it is suﬃcient to check that the condition holds for all pairs of consecutive edge inte-
Eo(i)
.
1, e). Clauses 9 and 10 enforce that edge integers are unique between 1 and
gers (e
|
|

⇔

−

≥

≥

9.
ed(i, j, e)
10. ed(i, j, e)

(cid:87)

j(cid:48) ed(i, j(cid:48), e
ed(i, j(cid:48), e),

⇒
⇒ ¬

−

1), e > 1,

[edge integers start at 1 and are contiguous]

j < j(cid:48), [edge integers cannot be duplicated]

Clauses 11-14 are used to enforce that two consecutive edges (i, e
L < L(cid:48). Formally, variable lt(i, e
lm(cid:48) /
∈

1, e, m) is only true if there exists m(cid:48)
L(cid:48). This is implemented using the following two clauses:

L and lm(cid:48)

−

−

≤

1, L) and (i, e, L(cid:48)) satisfy
m such that

∈
1, e, m)
1, e, m)

11. lt(i, e
12. lt(i, e

−
−

label(i, e
−
label(i, e, m)

⇒ ¬
⇒

∨

1, m)

lt(i, e

∨
lt(i, e

−
1, e, m

1, e, m
1),

−

1), e > 1,
e > 1.

−

−

Hence if lt(i, e
m
sures that for each edge (i, e) with e > 1, lt(i, e

1, e, m) holds, either lm /
∈

1. The disjuncts mentioning m

−

−

−

L(cid:48), or lt(i, e

L and lm

1) holds for
1 are only evaluated when m > 1. The next clause en-
:
∈ L

1, e, m) is true for at least one label lm

1, e, m

−

−

−

∈

13. ed(i, j, e)

(cid:87)

m lt(i, e

−

⇒

1, e, m), e > 1.

Finally, the following clause encodes the second part of Deﬁnition 6.1, ensuring that the
label set on edge (i, e) is not lower than that on (i, e

1):

−

14. lt(i, e

1, e, m)

label(i, e

1, m)

∨

−

∨ ¬

−

label(i, e, m), e > 1.

Properties. We proceed to prove several properties about the SAT encoding. Concretely,
we show that there is a one-to-one correspondence between the BFS traversal of a graph
and a solution to the SAT encoding.

Deﬁnition B.1. Given a graph G = (V, E)
and an associated graph indexing
f,
(cid:104)
SAT variables in

, let X(G,
(G) =
(cid:105)
, assigning false to all variables in

∈ G
Γu
}
{

u∈V

I

deﬁned on a set of labels

I
except as follows:

=
}
) be an assignment to the

l1, . . . , lk

L

{

X

X

• For each edge (u, v, L)

• For each edge (u, v, L)

∈

∈

E, ed(f (u), f (v), Γu(L)) is true.

E and each label lm

∈

L, label(f (u), Γu(L), m) is true.

1092

Induction and Exploitation of Subgoal Automata for RL

• For each node v

∈
• For each node u

V

such that Γu(L) = Γu(L(cid:48))
if there exists m(cid:48)

∈

}

v1

with ΠI(v) = (i, e), pa(i, f (v)) and sm(i, f (v), e) are true.

\ {
V , each pair of outgoing edges (u, v, L) and (u, v(cid:48), L(cid:48)) in Eo(u)
, lt(f (u), Γu(L), Γu(L(cid:48)), m) is true
L(cid:48).

1, and each label lm
L and lm(cid:48)

−

m such that lm(cid:48) /
∈

≤

∈ L
∈

Example B.1. Given the graph G, graph indexing
I
from Example 6.4 (p. 1052), the assignment X(G,
variables in

:

I

(G) and set of labels

}
L
) assigns true to the following SAT

=

a, b, c, d, e, f
{

X
ed(1, 2, 1)
ed(1, 4, 3)
ed(3, 4, 1)
pa(1, 2)
sm(1, 2, 1)
lt(1, 1, 2, 1)
lt(1, 2, 3, 2)
lt(4, 1, 2, 3)






label(1, 1, 2)
label(1, 3, 1)
label(3, 1, 2) ed(4, 5, 1)
pa(1, 3)
sm(1, 3, 2)
lt(1, 1, 2, 2)
lt(1, 2, 3, 3)
lt(4, 1, 2, 4)

pa(1, 4)
sm(1, 4, 3)
lt(1, 1, 2, 3)
lt(1, 2, 3, 4)
lt(4, 1, 2, 5)

label(1, 1, 5) ed(1, 3, 2)
label(1, 3, 2) ed(2, 4, 1)

label(1, 2, 1)
label(2, 1, 1)

label(4, 1, 4) ed(4, 5, 2)
pa(4, 5)
sm(4, 5, 1)
lt(1, 1, 2, 4)
lt(1, 2, 3, 5)
lt(4, 1, 2, 6)

lt(1, 1, 2, 5)
lt(1, 2, 3, 6)

label(1, 2, 6)

label(4, 2, 3)

lt(1, 1, 2, 6)






Theorem B.1. Given a graph G and a graph indexing
the SAT variables in

satisﬁes all SAT clauses in

(G), the assignment X(G,

) to
(G) is a BFS traversal.

I

if and only if

I

X

C

I

Proof.
tion 6.2, p. 1052). We show that each clause in

: Assume that

⇐

I

is satisﬁed:

(G) is a BFS traversal (i.e., it satisﬁes the conditions of Deﬁni-

C
i|i<j pa(i, j) holds for v such that f (v) = j > 1 since pa(i, j) is true for ΠI(v) = (i, e).

1. (cid:87)

pa(i(cid:48), j) holds since pa(i, j) is true for a single i.

2. pa(i, j)

3. pa(i, j)

⇒ ¬
(cid:87)

⇒

are true for ΠI(v) = (i, e).

e sm(i, j, e) holds for v such that f (v) = j since pa(i, j) and sm(i, j, e)

4. pa(i, j)

⇒ ¬

ed(i(cid:48), j(cid:48), e) holds for v such that f (v) = j and i(cid:48) < i < j = j(cid:48) since
ΠI(v) = (i, e) is the lexicographically smallest integer pair on incoming edges to v,
implying that G cannot contain an edge to v from a node u with f (u) = i(cid:48) < i.
Moreover, since
(G) is a BFS traversal, we cannot have ΠI(w) < ΠI(v) for a node
w with f (w) = j(cid:48) > j, else the second condition in Deﬁnition 6.2 is violated. Hence
G cannot contain an edge to w from a node u with f (u) = i(cid:48) < i, so the clause also
holds for i(cid:48) < i < j < j(cid:48).

I

5. sm(i, j, e)

pa(i, j) holds for v such that f (v) = j since pa(i, j) and sm(i, j, e) are

⇒
true for ΠI(v) = (i, e).

6. sm(i, j, e)

7. sm(i, j, e)

sm(i, j, e(cid:48)) holds since sm(i, j, e) is true for a single i and e.

⇒ ¬

ed(i, j, e) holds for v such that f (v) = j since ΠI(v) = (i, e) implies that

there exists an edge (u, v, L)

⇒

E with f (u) = i and Γu(L) = e.

∈

1093

Furelos-Blanco, Law, Jonsson, Broda, & Russo

8. sm(i, j, e)

⇒ ¬

ed(i, j(cid:48), e(cid:48)) holds for v such that f (v) = j and j = j(cid:48) since ΠI(v) = (i, e)
is the lexicographically smallest integer pair on incoming edges to v, implying that
G cannot contain an edge (u, v, L) with f (u) = i and Γu(L) < e. For j < j(cid:48), the
parent function of the node w with f (w) = j(cid:48) cannot be smaller than that of v, else
the second condition in Deﬁnition 6.2 is violated. Hence G cannot contain an edge
(u, w, L) with f (u) = i and Γu(L) < e. Thus the clause also holds for the case j < j(cid:48).

9. ed(i, j, e)
1, . . . ,
{

(cid:87)
⇒
Eo(u)
|
10. ed(i, j, e)

−

1) holds for u with f (u) = i since Γu is a bijection onto

j(cid:48) ed(i, j(cid:48), e
.
|}
ed(i, j(cid:48), e) holds for u with f (u) = i since Γu is a bijection.

11. lt(i, e

−

label(i, e
outgoing edges (u, v, L), (u, v(cid:48), L(cid:48)) with Γu(L) = e
implies that either lm /
∈

−
L or there exists m(cid:48) < m such that lm(cid:48) /
∈

1) holds for u, f (u) = i, and
1, e, m)
1 since lt(i, e
L(cid:48).
L and lm(cid:48)

−
1 = Γu(L(cid:48))

1, e, m

lt(i, e

1, m)

⇒ ¬

−
∈

−

−

−

∨

1, e, m)
1, e, m) implies that either lm

label(i, e, m)

lt(i, e

⇒

∨

1, e, m
−
L(cid:48) or there exists m(cid:48) < m such that lm(cid:48) /
∈
∈

1) holds for the same setting since
L

−

⇒ ¬
1, e, m)

12. lt(i, e
−
lt(i, e
−
and lm(cid:48)

∈
13. ed(i, j, e)

L(cid:48).

(cid:87)

j(cid:48) ed(i, j(cid:48), e

−

⇒

(G) is a BFS traversal, implying that the
I
bijection Γu for u with f (u) = i satisﬁes L < L(cid:48) whenever Γu(L) < Γu(L(cid:48)) (and in
1). Since L < L(cid:48), there has to exist at least
particular when Γu(L) = e
m
one m, 1

L(cid:48) due to Deﬁnition 6.1.

1) holds since

1 = Γu(L(cid:48))

−
L and lm

≤
≤
1, e, m)
1 =
1 implies L < L(cid:48). Hence for the given m, Deﬁnition 6.1 is satisﬁed either
1, e, m) is true; or 2) by a label lm(cid:48)>m,

14. lt(i, e
−
Γu(L(cid:48))
1) by a label lm(cid:48)≤m, implying that lt(i, e
implying that lm

label(i, e, m) also holds since Γu(L) = e

−
L(cid:48) cannot both be true.

k such that lm /
∈
1, m)

label(i, e

∨ ¬

−

−

−

−

∈

∨

L and lm /
∈

∈

: Assume that X(G,

) satisﬁes all SAT clauses. We show that

⇒
First note from above that X(G,
is not a BFS traversal. Hence we can focus exclusively on these four clauses.

) satisﬁes all clauses except 4, 8, 13 and 14 even if

I

I

I

(G) is a BFS traversal.
(G)

I

We ﬁrst analyze Clauses 13 and 14. For Clause 13 to be true, any edge (i, e) induced
. Let u
from graph G with e > 1 has to satisfy lt(i, e
−
be the node with f (u) = i and let (u, v, L) and (u, v(cid:48), L(cid:48)) be the two outgoing edges in Eo(u)
such that Γu(L) = e
k
L(cid:48). For the smallest such m there cannot exist m(cid:48) < m such
such that lm /
∈
L(cid:48), else Clause 14 would be violated for m(cid:48). Hence Deﬁnition 6.1
that lm(cid:48)
L and lm(cid:48) /
∈
holds for label lm, implying L < L(cid:48).

1, e, m) for at least one label lm

1. By deﬁnition of X(G,

) there exists m, 1

−
L and lm

1 = Γu(L(cid:48))

∈ L

m

≤

−

≤

∈

∈

I

We next analyze Clauses 4 and 8. Let u be the node such that f (u) = j and ΠI(u) =
(i, e), and let v be any node such that f (v) = j(cid:48) > j. Since Clause 4 holds, graph G cannot
contain an edge (w, v, L) such that f (w) = i(cid:48) < i. Since Clause 8 holds, graph G cannot
contain an edge (w, v, L) such that f (w) = i and Γw(L) < e. Since ΠI(v) cannot equal
(i, e), it has to be larger than (i, e), implying ΠI(u) < ΠI(v).

and v in V

We have shown that the two conditions in Deﬁnition 6.2 hold: for each pair of nodes u
V and pair
, ΠI(u) < ΠI(v) implies f (u) < f (v), and for each node u
}

\ {

v1

∈

1094

Induction and Exploitation of Subgoal Automata for RL

of outgoing edges (u, v, L) and (u, v(cid:48), L(cid:48)) in Eo(u), L < L(cid:48) implies Γu(L) < Γu(L(cid:48)). Hence
by deﬁnition

(G) is a BFS traversal.

I

. Given an edge (i, e), let L(i, e) =

that satisﬁes the SAT
Deﬁnition B.2. Let X be an assignment to the SAT variables
X
clauses in
be the label set induced by
label(i, e, m)
}
X. We deﬁne a mapping
(G)) from X to a graph G = (V, E) and associated
f,
graph indexing
(cid:104)

(X) = (G,
G
Γu
{

as follows:

u∈V
}

(G) =

lm

I

C

{

|

, where n is the largest node index in the assign-
}
V .

I
• The set of nodes is V =

(cid:105)
v1, . . . , vn

ment, and f (vi) = i for each vi

{

∈
(vi, vj, L(i, e))

• The set of edges is E =

{

ed(i, j, e)
}

|

and Γvi(L(i, e)) = e.

• The parent function of each vj

∈

V equals ΠI(vj) = (i, e) if sm(i, j, e) is true.

Theorem B.2. Given an assignment X to the SAT variables
in
graph indexing

(G)) induces a graph G in the class

, the mapping

(X) = (G,

G
(G).

X

I

C

G

that satisﬁes all clauses
and a well-deﬁned

I

|

|

|

{

I

≤

|}

|}

|}

∈ {

∈ L

1, . . . ,

1, . . . ,

, where

Eo(vi)

Eo(vi)

label(i, e

m such that

Proof. We ﬁrst show that the induced graph indexing
V
is a bijection onto
|

(G) is well-deﬁned. Clearly f
by deﬁnition. We next show that Γvi is a bijection onto
V . Clause 10 ensures that ed(i, j, e) and ed(i, j(cid:48), e)
∈
= j(cid:48). Due to Clause 9, if edge (i, e) is deﬁned for e > 1,
1). Applying this argument recursively implies that (i, e) is uniquely deﬁned
−
Eo(vi)
|

for each node vi
{
cannot be true simultaneously for j
then so is (i, e
1, . . . ,
for e

is the largest integer of an outgoing edge from i.

We next show that the induced label set L(i, e) on each outgoing edge (i, e) from i is
1, e, m) is true for at
unique. Clause 13 implies that for each edge (i, e) with e > 1, lt(i, e
1, e, m) is true only if there
least one label lm
1, m(cid:48)) and label(i, e, m(cid:48)) are true. For the smallest such
exists m(cid:48)
¬
m(cid:48) there cannot exist m(cid:48)(cid:48) < m(cid:48) such that label(i, e
label(i, e, m(cid:48)(cid:48)) are true, else
Clause 14 would be violated for m(cid:48)(cid:48). Hence label lm satisﬁes the condition in Deﬁnition 6.1
1) < L(i, e).
with respect to the induced label sets L(i, e
−
Eo(vi)
1, . . . ,
,
|}
Eo(vi)
.
1, . . . ,
|}
, this implies that Γvi is

In conclusion, we have shown that (i, e) is uniquely deﬁned for e

and that L(i, e
Since Γvi is deﬁned as Γvi(L(i, e)) = e for each e
a well-deﬁned bijection from Eo(vi) to

1) < L(i, e) holds for each pair of consecutive integers in

. Clauses 11 and 12 ensure that lt(i, e

−
1, m(cid:48)(cid:48)) and

|
We also need to show that the induced parent function ΠI is well-deﬁned, i.e. that for
each j > 1, sm(i, j, e) is true for a single i and e, and that ΠI(vj) = (i, e) is consistent with
the deﬁnition of ΠI. Clauses 1 and 2 imply that pa(i, j) is true for a single i. Clauses 3,
5 and 6 imply that sm(i, j, e) can only be true for the same i and j as pa(i, j), and that
sm(i, j, e) is true for a single e. For ΠI(vj) = (i, e) to hold, G has to contain the edge
(vi, vj, L(i, e)), which is guaranteed by Clause 7. Moreover, G cannot contain any edge
(vi(cid:48), vj, L(i(cid:48), e(cid:48))) such that (i(cid:48), e(cid:48)) < (i, e), which is guaranteed by Clauses 4 and 8.

1) and L(i, e), implying L(i, e

1, . . . ,
.
|}

Eo(vi)
|

∈ {
Eo(vi)

∈ {
{

1, . . . ,

−

−

−

−

−

|}

¬

|
|

{

We ﬁnally show that the induced graph G belongs to the class

, i.e. that the three
assumptions on page 1050 are satisﬁed. We satisfy Assumption 1 by designating v1 as the
start node. We have already shown above that the induced label set L(i, e) on each outgoing
edge (i, e) from i is unique, satisfying Assumption 3. It remains to show that Assumption 2

G

1095

(cid:54)
Furelos-Blanco, Law, Jonsson, Broda, & Russo

holds, i.e. that each node vj, j > 1, is reachable from v1. Since sm(i, j, e) is true for a
single i and e such that i < j, G has to contain the edge (vi, vj, L(i, e)) due to Clause 7.
Aggregating these incoming edges for all nodes diﬀerent from v1 results in a BFS subtree
rooted in v1, and each node vj, j > 1, is reachable from v1 in this subtree.

By combining Theorems B.1 and B.2, it follows that the mapping

(G)) of
(G) is a BFS traversal.
a satisfying assignment X to the SAT clauses in
Since by Lemma 6.1 (p. 1053) each graph G
has a unique BFS traversal, it follows that
∈ G
the SAT encoding cannot generate two permutations of node integers that represent the
same graph G. Hence the SAT encoding breaks the symmetries in graphs such as those in
Figure 3 (p. 1050).

is such that

(X) = (G,

G

I

I

C

We remark that the SAT encoding is not forced to include edges in the induced graph
G other than those needed to correctly represent the parent function ΠI. However, by
combining the SAT encoding with another encoding for generating automata, we ensure
that the encoding cannot produce two symmetric automata.

B.2 ASP Encodings

In this section we present two diﬀerent encodings of our symmetry breaking method in
ASP for its application to subgoal automata. The ﬁrst method (Appendix B.2.1) is a direct
translation from the SAT clauses introduced in the previous section. However, there are
certain aspects that can be made more eﬃcient in ASP, so we propose a second method for
encoding the symmetry breaking constraints (Appendix B.2.2).

In order to apply the proposed symmetry breaking mechanism to a subgoal automaton,

we have to take the following aspects into account:

1. The edges of a subgoal automaton are labeled by propositional formulas over a set of
, whereas the edges of a labeled directed graph are deﬁned over a set of

observables
labels

.

O

L

2. The graph indexing presented for labeled directed graphs assigns a diﬀerent edge index
for each of the outgoing edges from a node. In contrast, in the ASP representation
M (
the edge indices are unique only from one state to
another (i.e., can be repeated between other pairs of states).

) of a subgoal automaton

A

A

We now partially describe how we address (1), which is common to both methods. We
map the set of observables
, which
consists of integer values for easy comparison. Each of these integer values must encode
either an observable or its negation. Given a set of observables
, the set
of labels

used by the subgoal automaton into a set of labels

L
o1, . . . , o|O|}

is deﬁned as:

O

O

=

{

L

=

i, i +

oi

L

|O| |

{
corresponds to its negation,

∈ O}

,

where i corresponds to oi and i +
of integers from 1 to 2
labels
ASP using the following predicates:

+ 1 . . . 2

|O|

|O|

consists
correspond to each of the observables, and
|O|
correspond to the observable negations. This mapping is encoded in

|O|
where labels 1 . . .

oi. Therefore,

|O|

L

¬

• obs id(o, i) indicates that observable o

is assigned id i.

∈ O

1096

Induction and Exploitation of Subgoal Automata for RL

• num obs(i) indicates that the set of observables has size i.

• valid label(l) indicates that l is a label.

We simply ground the above predicates according to their descriptions:

obs id(oi, i).

{

oi

|

∈ O} ∪ {

num obs(

).

|O|

} ∪ {

valid label(l).

1

|

l

2

)
|O|

}

.

≤

≤

The mapping is not complete: we need to map the formulas on the edges into sets of labels.
Speciﬁcally, we map the factual representation of the automaton introduced in Section 6 into
label facts similar to the variables of the same name used in the SAT encoding. However,
these facts are slightly diﬀerent between encodings, so we describe them in detail in their
respective sections. The second aspect commented above is also addressed diﬀerently for
each encoding.

Another common feature between the encodings is that states in the automaton are
assigned an integer index for easy comparison. This is needed only for the rules enforcing
the BFS traversal on the automaton, which corresponds to Clauses 1-8 in the SAT encoding.
The predicate state id(u, i) denotes that the automaton state u has index i and its ground
instances are:

state id(ui, i).
{

|

ui

∈

U

\ {

uA, uR

.

}}

Note that, without loss of generality, the ﬁrst state index is 0 (because of the initial state
u0)33 and that not all states are assigned an index. Given that the accepting state uA
and the rejecting state uR are ﬁxed, they cannot be interchanged with any other state.
Furthermore, they cannot be the parent of any other state since they are absorbing states.
Thus, they are excluded from the BFS ordering.
In order to easily compare the indices
of two states, we introduce the set of rules below. The ﬁrst rule deﬁnes the predicate
state lt(u, u(cid:48)) to express that the index of state u is lower than that of u(cid:48). Similarly, the
second rule deﬁnes the predicate state leq(u, u(cid:48)) to express that the index of state u is
lower or equal than that of u(cid:48).

(cid:26)state lt(X, Y) : - state id(X, XID), state id(Y, YID), XID<YID.

(cid:27)

state leq(X, Y) : - state id(X, XID), state id(Y, YID), XID<=YID.

B.2.1 SAT-Based Encoding

The encoding we present in this section is a direct translation from the SAT encoding
introduced in Appendix B.1. The presentation of this encoding is divided into three parts.
First, we introduce a mapping from the edge indices used by the subgoal automata into
the edge indices used in the symmetry breaking method. Second, we describe how the
previously introduced mapping from observables into labels is applied to map formulas over
observables into label sets. Third, we transform the SAT clauses into ASP rules.

Edge Index Mapping. The set of rules below encodes the edge index mapping. Firstly,
we deﬁne a predicate edge id(i), where i is an edge index, and ground it for values between
1 and (
|

1)κ:

| −

U

edge id(i)
{

|

1

i

≤

≤

U
(
|

| −

.

1)κ
}

33. Remember that in Section 6.2 the bijection f assigns indices starting from 1.

1097

Furelos-Blanco, Law, Jonsson, Broda, & Russo

| −

Note that (
U
| −
|
can have edges to

1)κ is the maximum number of outgoing edges from a state: each state
1 diﬀerent states and κ edges are allowed from one state to another.
U
|
We use facts of the form mapping(u, v, e, e(cid:48)) to indicate that edge e between u and v
is mapped into e(cid:48). The mapping is enforced using the set of rules below. The ﬁrst rule
describes that an edge index E from X to Y is mapped into exactly one edge index EE in the
range given by the edge id facts. The second rule enforces that two outgoing edges from
a state X to two diﬀerent states Y and Z must be mapped into diﬀerent edge indices. The
third rule enforces that two edge indices E and EP between the same pair of states X and Y
must be mapped into diﬀerent edge indices. The fourth rule indicates that if there are two
edge indices E and EP between states X and Y such that E<EP, then the indices to which
they are mapped (EE and EEP) must preserve the same ordering (EE<EEP).

mapping(X, Y, E, EE) : edge id(EE)
}


1

{
: - mapping(X, Y, , EE), mapping(X, Z, , EE), Y<Z.
: - mapping(X, Y, E, EE), mapping(X, Y, EP, EE), E<EP.
: - ed(X, Y, E), ed(X, Y, EP), E<EP, mapping(X, Y, E, EE), mapping(X, Y, EP, EEP), EE>EEP.

1 : - ed(X, Y, E).








Given the mapping, it is straightforward to redeﬁne the ed atoms, as well as the pos

and neg facts used in the factual representation of the automata introduced in Section 6:






map ed(X, Y, EP) : - ed(X, Y, E), mapping(X, Y, E, EP).
map pos(X, Y, EP, O) : - pos(X, Y, E, O), mapping(X, Y, E, EP).
map neg(X, Y, EP, O) : - neg(X, Y, E, O), mapping(X, Y, E, EP).






The predicates map ed, map pos and map neg are used in the ASP encoding of the symmetry
breaking constraints explained later.

Mapping of Formulas into Label Sets. The set of rules below uses the previously
described observable-to-label and edge index mappings to transform formulas over a set
of observables into label sets. The ﬁrst rule sets OID as a label of edge E from X if the
corresponding observable O appears positively in that edge. The second rule sets OID+N as
a label of edge E from X if the corresponding observable O appears negatively in that edge
and N is the number of observables.

(cid:26)label(X, E, OID) : - map pos(X, Y, E, O), obs id(O, OID).

(cid:27)

label(X, E, OID+N) : - map neg(X, Y, E, O), obs id(O, OID), num obs(N).

Symmetry Breaking Rules. Similarly to the SAT encoding, we divide the resulting
ASP rules into three sets. First, we introduce the set of rules enforcing the indexing given
by the BFS traversal on the automaton. These rules are deﬁned in terms of an auxiliary
predicate ed sb(u, u(cid:48), i) equivalent to map ed(u, u(cid:48), i) but only deﬁned for those states u(cid:48)
that have a state index. Remember that the accepting (uA) and rejecting (uR) states are
excluded from the traversal and, thus, they are the only states without an index. The
following rule deﬁnes ed sb in terms of map ed:

ed sb(X, Y, E) : - map ed(X, Y, E), state id(Y, ).

1098

Induction and Exploitation of Subgoal Automata for RL

The set of rules below that corresponds to Clauses 1-8 in the SAT encoding.

: - state(Y), state id(Y, YID), YID>0.






sm(X, Y, E) : edge id(E)
}

pa(X, Y) : state(X), state lt(X, Y)
}

1
{
: - pa(X, Y), pa(XP, Y), state lt(X, XP), state lt(XP, Y).
1
{
: - pa(X, Y), ed sb(XP, YP, ), state lt(XP, X), state leq(Y, YP).
: - sm(X, Y, ), not pa(X, Y).
: - sm(X, Y, E), sm(X, Y, EP), E<EP.
: - sm(X, Y, E), not ed sb(X, Y, E).
: - sm(X, Y, E), ed sb(X, YP, EP), state lt(X, Y), state leq(Y, YP), EP<E.

: - pa(X, Y).






The next set of rules encodes Clauses 9 and 10, which enforce that edge indices are

unique between 1 and the number of outgoing edges from X:

(cid:26): - map ed(X, Y, E), not map ed(X, , E

−
: - map ed(X, Y, E), map ed(X, Z, E), Y<Z.

1), E>1.

(cid:27)

The following rules encode SAT Clauses 11-14 in ASP. Note that Clauses 11 and 12 have

been divided into two rules respectively to cover the diﬀerent values of L.






: - lt(X, E
: - lt(X, E
: - lt(X, E
: - lt(X, E
lt(X, E
1
{
: - not lt(X, E

1, L), not lt(X, E
1, L), E>1, L=1.

1, E, L), label(X, E
−
1, E, L), label(X, E
−
1, E, L), not label(X, E, L), not lt(X, E
1, E, L), not label(X, E, L), E>1, L=1.
1, E, L) : valid label(L)
1, E, L), label(X, E

−
−
−
−
−

−

−

−

1, E, L

1), E>1, L>1.

−

1, E, L

1), E>1, L>1.

−

−

: - map ed(X, Y, E), E>1.

}
1, L), not label(X, E, L), map ed(X, , E), E>1.






B.2.2 Alternative Encoding

The encoding we describe in this section is an alternative to the one presented before.
Despite of the diﬀerences, it also enforces the graph indexing given by the BFS traversal
described in Section 6.

Similarly to the SAT-based encoding, we need to address the fact that the mechanism
uses an indexing for the edges diﬀerent from the one we use to represent a subgoal au-
tomaton. In the SAT-based approach, we deﬁned a mapping from the edge indices used
by a subgoal automaton to those required by the symmetry breaking. In this approach, we
do not use such mapping and directly operate on the edge indices used by the automata.
The edge indexing used in the symmetry breaking is such that each outgoing edge from a
given state has a diﬀerent index. In other words, each edge is uniquely identiﬁed by an
integer number. Here we preserve the same uniqueness principle by expressing the edges as
(u, (v, e)) meaning that there is an edge from u to v with edge index e. Note that the tuple
(v, e) uniquely identiﬁes each outgoing edge from u.

The rest of this section is divided into two parts. First, like in the SAT-based approach,
we map the propositional formulas on the edges into sets of labels. Then, we describe the
set of ASP rules we use for breaking symmetries.

Mapping of Formulas into Label Sets. We use the predicate label(u, (v, e), l) to
express that label l appears in the edge from state u to state v with index e. Similarly

1099

Furelos-Blanco, Law, Jonsson, Broda, & Russo

to the SAT-based encoding, the set of rules below transforms the formulas over a set of
observables into label sets. The ﬁrst rule sets OID as a label of edge (Y, E) from X if the
corresponding observable O appears positively in that edge. Likewise, the second rule sets
OID+N as a label of edge (Y, E) from X if the corresponding observable O appears negatively
in that edge and N is the number of observables.

(cid:26)label(X, (Y, E), OID) : - pos(X, Y, E, O), obs id(O, OID).

(cid:27)

label(X, (Y, E), OID+N) : - neg(X, Y, E, O), obs id(O, OID), num obs(N).

Note that while the label predicate in the SAT encoding only used the edge index for
referring to an outgoing edge, here we use a state-edge pair as explained at the beginning
of the section.

Symmetry Breaking Rules. We start by describing the rules which enforce outgo-
ing edges from a given state to be ordered by their respective label sets. The predicate
ed lt(X, (Y, E), (YP, EP)) indicates that the edge from X to Y with edge index E is lower than
the edge from X to YP with edge index EP. The set of rules below describes how this ordering
is determined and what constraints we impose on it. The ﬁrst rule determines that given
two outgoing edges from X, (Y, E) and (YP, EP), either (Y, E) is lower than (YP, EP) or vice
versa. Now, the order between outgoing edges from a state X must respect two constraints:

• The second rule enforces transitivity. That is, if Edge1 is lower than Edge2, and Edge2

is lower than Edge3, then Edge1 must be lower than Edge3.

• The third rule enforces that two edges to the same state Y must be ordered according
to their edge index. That is, given edges (Y, E) and (Y, EP) from X such that E<EP,
edge (Y, E) must be lower than (Y, EP).






1
{

1 : - ed(X, Y, E), ed(X, YP, EP),
ed lt(X, (Y, E), (YP, EP)); ed lt(X, (YP, EP), (Y, E))
}

(Y, E)<(YP, EP).

: - ed lt(X, Edge1, Edge2), ed lt(X, Edge2, Edge3), not ed lt(X, Edge1, Edge3),

Edge1!=Edge3.

: - ed lt(X, (Y, E), (Y, EP)), ed(X, Y, E), ed(X, Y, EP), E>EP.






Note that the previous set of rules guesses an ordering of outgoing edges from a given state.
However, this ordering must comply with that of the label sets given in Deﬁnition 6.1.
We use the predicate label lt(X, Edge1, Edge2, L) to indicate that there is label L(cid:48)
L
that appears in Edge2 and does not appear in a lower edge Edge1, where both Edge1 and
Edge2 are outgoing edges from X. Note that this predicate encodes the ﬁrst condition in
Deﬁnition 6.1 up to a speciﬁc label. The set of rules below prunes solutions where outgoing
edges do not follow the established label ordering criteria. The ﬁrst rule indicates that
label lt(X, Edge1, Edge2, L) is true if Edge1 is lower than Edge2, and the label L does not
appear in Edge1 and appears in Edge2. The second rule states that label lt is true for a
valid label L+1 if it is true for L. The third rule states that if Edge1 is lower than Edge2,
then the label set on Edge1 must be lower than that on Edge2. Note that the three last

≤

1100

Induction and Exploitation of Subgoal Automata for RL

literals on the constraint enforce both conditions in Deﬁnition 6.1.






label lt(X, Edge1, Edge2, L) : - ed lt(X, Edge1, Edge2), not label(X, Edge1, L),

label(X, Edge2, L).
label lt(X, Edge1, Edge2, L+1) : - label lt(X, Edge1, Edge2, L), valid label(L+1).
: - ed lt(X, Edge1, Edge2), label(X, Edge1, L), not label(X, Edge2, L),

not label lt(X, Edge1, Edge2, L).






The set of rules below imposes that lower edge indices cannot be left unused. First,
we deﬁne a fact for each possible edge index between 1 and κ. Remember that κ is the
maximum number of edges from one state to another. Second, the constraint indicates that
if there is an edge from X to Y with edge index E where E>1, then there must be an edge
between the same states but with edge index E

1 as well.

−

(cid:26)edge id(1..κ).

(cid:27)

: - ed(X, Y, E), not ed(X, Y, E

1), edge id(E), E>1.

−

Finally, we introduce a set of rules for enforcing a BFS traversal of the subgoal automa-
ton. Firstly, like in the SAT-based approach, we use a predicate ed sb(X, Y, E) which is
ground for all the edges except for those directed to a state without an index (the accepting
and rejecting states):

ed sb(X, Y, E) : - ed(X, Y, E), state id(Y, ).

We now introduce a predicate pa(X, Y) denoting that state X is the parent of Y in the
BFS subtree. Note that it is equivalent to the variable pa(i, j) from the SAT encoding. The
set of rules below uses this predicate to enforce the BFS ordering. The ﬁrst rule deﬁnes
that state X is the parent of Y if there is an edge from X to Y, X has a lower index than Y,
and there is no state Z whose index is lower than X’s and has an edge to Y.34 The second
rule indicates that all states with an index except for the initial state must have a parent.
The third rule imposes the BFS ordering similarly to Clause 4 in the SAT encoding. Note
that the ﬁrst two rules encode Clauses 1 and 2 of the SAT encoding.




pa(X, Y) : - ed sb(X, Y, ), state lt(X, Y),

#false : ed sb(Z, Y, ), state lt(Z, X).






: - state id(Y, YID), YID>0, not pa( , Y).
: - pa(X, Y), ed sb(XP, YP, ), state lt(XP, X), state leq(Y, YP).



Now we need to enforce that the BFS children from a given state are correctly ordered;
that is, those children pointed by lower edges should be identiﬁed by lower state indices.
We use the state ord(X) predicate to indicate that state X is properly ordered with respect
to its siblings (i.e., other states with the same parent state). The set of rules below enforces
this ordering. The ﬁrst rule deﬁnes that a state Y is correctly ordered with respect to its
siblings if the edge from their parent X to Y, (Y, E), is lower than the edge to another state
(YP, EP) if Y<YP. That is, the edges must be ordered according to the order of the state

34. Note that what follows #false : must not hold in order to make the body of the rule true.

1101

Furelos-Blanco, Law, Jonsson, Broda, & Russo

indices. The second rule enforces that all states with a parent must be correctly ordered
with respect to their siblings.







state ord(Y) : - ed sb(X, Y, E), pa(X, Y),

#false : ed sb(X, YP, EP), state lt(Y, YP), ed lt(X, (YP, EP), (Y, E)).



: - pa( , Y), not state ord(Y).



Appendix C. Additional Experimental Details

In this section we describe the experimental details omitted in Section 8. Firstly, we ex-
plain how some of the restrictions used in the experiments are implemented. Secondly, we
make a thorough analysis of how enabling and disabling these restrictions (as well as other
parameters) aﬀects automaton learning and reinforcement learning.

C.1 Automaton Learning Restrictions

In this section we provide details about the restrictions on automaton learning outlined
in Section 8.1. All the rules introduced below are added to the automaton learning task
deﬁned in Sections 5 and 6.

Avoid Learning Purely Negative Formulas. This restriction consists in avoiding
that any edge is labeled by a formula formed only by negated observables. The following
constraint discards such solutions by enforcing an observable to occur positively whenever
an observable appears negatively in a given edge:35

: - neg(X, Y, E, ), not pos(X, Y, E, ).

Acyclicity. The following set of rules enforces the automaton to be acyclic; that is, two
automaton states cannot be reached from each other. The path(X, Y) predicate indicates
there is a directed path (i.e., a sequence of directed edges) from X to Y. The ﬁrst rule states
that there is a path from X to Y if there is an edge from X to Y. The second rule indicates
that there is a path from X to Y if there is an edge from X to an intermediate state Z from
which there is a path to Y. Finally, the third rule rules out those answer sets where X and
Y can be reached from each other through directed edges.






path(X, Y) : - ed(X, Y, ).
path(X, Y) : - ed(X, Z, ), path(Z, Y).
: - path(X, Y), path(Y, X).






Trace Compression. The method we propose for shortening an observation trace is
based on the following assumptions:

1. Empty observations are irrelevant.

2. Seeing the same observation twice or more in a row is equivalent to seeing it once.

Given these two assumptions, we deﬁne a subtype of observation trace called compressed
observation trace.

35. Remember that the pos and neg facts were introduced in Section 6.

1102

Induction and Exploitation of Subgoal Automata for RL

Deﬁnition C.1 (Compressed observation trace). A compressed observation trace ˆλL,O =
ˆO0, . . . , ˆOm
is the result of removing empty observations and, thereafter, removing con-
(cid:104)
tiguous equal observations from an observation trace λL,O =

(cid:105)

.

O0, . . . , On
(cid:104)

(cid:105)

Example C.1. The ﬁrst trace below is a goal observation trace for the OfficeWorld’s
Coffee task. The second trace is the resulting compressed observation trace.

λG
L,O =
ˆλG
L,O =

,

(cid:104){}
(cid:75)

(cid:104){

(cid:75)
}
o
{

{
,
}

,

{

}(cid:105)

,

,
{}

,
{}

{

o

}(cid:105)

,

(cid:75)
}

.

As we will see experimentally (see Appendix C.2), compressed observation traces are
helpful to speed up automaton learning. However, their applicability is limited to tasks
where the assumptions above hold. Thus, these traces should not be used to learn the
automata for tasks where every single observation is important, such as “observe (cid:75) twice
in a row”.

The automaton learning component of our method does not distinguish between ob-
servation traces and compressed observation traces. Therefore, we do not have to make
any change in the encoding of the learning tasks. However, since the information about the
number of performed steps is lost and there are no empty observations, unlabeled transitions
are meaningless when these traces are used. Thus, we rule out automata with unlabeled
edges using the following constraint when trace compression is enabled:

: - ed(X, Y, E), not pos(X, Y, E, ), not neg(X, Y, E, ).

This constraint rules out any inductive solution where an edge from X to Y with index E is
not labeled by a positive or a negative literal.

In the case of the RL component, the use of compressed observation traces aﬀects the way
in which an automaton is traversed by the agent. Since empty observations and contiguous
duplicated observations are ignored, the transition function δϕ is only queried when the
current observation (1) is not empty and (2) it is diﬀerent from the last observation. If
these two conditions do not hold, the agent remains in the same automaton state.

C.2 Parameter Tuning Experiments

In this section we make a thorough analysis of the impact that the reinforcement learning
and automaton learning parameters have on the overall performance of ISA using the Of-
ficeWorld domain. Table 7 shows the parameters used throughout these experiments.
The parameter set at the top of the table remains unchanged for all experiments. In con-
trast, the parameters in the bottom part of the table can change between experiments.

C.2.1 POMDP Sets, Number of Tasks & Number of Steps per Episode

In these experiments we check the variability in the performance for two diﬀerent sets
of randomly generated POMDPs, diﬀerent numbers of POMDPs in a POMDP set, and
Σ
2 =
for diﬀerent numbers of steps per episode. Let
1,100}
D
Σ
i de-
i,j} ⊆ D
M
{M
note the subset of the ﬁrst j POMDPs from the i-th POMDP set, and N be the maximum

be two sets of 100 POMDPs. Let

Σ
1,1, . . . ,
M
Σ
i,1, . . . ,

Σ
2,100}

Σ
2,1, . . . ,

{M
j
i =

1 =

{M

and

M

D

D

1103

Furelos-Blanco, Law, Jonsson, Broda, & Russo

Learning rate (α)
Exploration rate ((cid:15))
Discount factor (γ)
Number of episodes
Avoid learning purely negative formulas (cid:51)
RL algorithm

0.1
0.1
0.99
10,000

HRLG

)
|D|

Number of tasks (
Maximum episode length (N )
Trace compression
Enforce acyclicity
Number of disjuncts (κ)
Use restricted observable set

50
250
(cid:51)
(cid:51)
1
(cid:55)

Table 7: Parameters used in the OfficeWorld experiments. The top part of the table
contains those parameters that remain unchanged, while the bottom part contains
those that change across experiments.

10, 50, 100

Given the set of POMDPs

episode length (i.e., the maximum number of steps within an episode). These experiments
are performed with j

.
1, Figure 10 shows the learning curves for the diﬀerent
combinations of POMDP subsets and steps. Although all experiments were run for 10,000
episodes, the curves are just shown for some of them to clearly display the variances between
diﬀerent values of N . We make the following observations:

100, 250, 500
}

and N

∈ {

∈ {

D

}

• The lowest maximum episode length (N = 100) works ﬁne when a goal state is easy
to achieve (i.e., the number of subgoals is low, like in Coffee). As the number of
subgoals in the task increases, the maximum episode length needs to be increased to
reach a task’s ﬁnal goal. If N is not high enough, then there is only a low chance
that the agent will observe the required counterexamples to reﬁne an automaton.
Remember that if the goal is not achieved at least once, the agent will not be able to
exploit the knowledge given by an automaton since it will not learn any. For instance,
in the case of VisitABCD, we observe that the learning curves barely converge when
N = 100 because no automaton is learned. Even when the number of POMDPs is
high (100), a goal trace to start automaton learning is only found in 9 out of 20 runs.

• Small sets of POMDPs are suﬃcient to learn an automaton and policies that achieve
the goal in the Coffee task. However, in the tasks involving more subgoals, a small
set of POMDPs (10) does not speed up convergence as much as bigger sets of POMDPs
(50 or 100). For example, in VisitABCD, automata are rarely learned when the
number of POMDPs is 10 for any maximum episode length. Increasing the number
of POMDPs increases the chance that easier grids are included in the set and, thus,
also increases the chance of observing counterexamples to start learning automata.

• Small values of N and small number of POMDPs often cause automaton learning
to occur through the entire interaction. In contrast, higher values of N and bigger

1104

Induction and Exploitation of Subgoal Automata for RL

N = 100

N = 250

N = 500

Figure 10: Learning curves for diﬀerent combinations of POMDP sets (

maximum episode lengths (100, 250, 500).

10
1 ,

50
1 ,

D

100
1
D

) and

D

POMDP sets usually concentrate learning early in the interaction. Again, when N
is small, the agent has a lower chance of observing a counterexample trace to reﬁne
the automaton since the interaction with the environment is shorter. This is specially
detrimental in the case of smaller POMDP sets where the goal is particularly diﬃcult
to achieve (e.g., if the observables are sparsely distributed in the grid).

A high maximum episode length seems to be the best choice to make sure that a policy
that achieves the goal is learned.
Intuitively, however, such choice can produce longer
traces and can make automaton learning more complex since (1) the chance of observing
irrelevant observables to the task at hand increases (i.e., the system has to learn they are
not important) and (2) it is more diﬃcult to ﬁgure out the order in which subgoals must be
observed. Table 8 shows the average total automaton learning time, while Table 9 shows the

1105

02004006008001000Numberofepisodes0.00.20.40.60.81.0AveragerewardCoffee(D101)02004006008001000Numberofepisodes0.00.20.40.60.81.0AveragerewardCoffee(D501)02004006008001000Numberofepisodes0.00.20.40.60.81.0AveragerewardCoffee(D1001)05001000150020002500Numberofepisodes0.00.20.40.60.81.0AveragerewardCoffeeMail(D101)05001000150020002500Numberofepisodes0.00.20.40.60.81.0AveragerewardCoffeeMail(D501)05001000150020002500Numberofepisodes0.00.20.40.60.81.0AveragerewardCoffeeMail(D1001)02000400060008000Numberofepisodes0.00.20.40.60.81.0AveragerewardVisitABCD(D101)02000400060008000Numberofepisodes0.00.20.40.60.81.0AveragerewardVisitABCD(D501)02000400060008000Numberofepisodes0.00.20.40.60.81.0AveragerewardVisitABCD(D1001)Furelos-Blanco, Law, Jonsson, Broda, & Russo

10
1
D
N = 100 N = 250

N = 500

Coffee
CoffeeMail
VisitABCD

0.3 (0.0)
5.9 (1.5)
-

0.3 (0.0)
4.2 (1.1)
-

0.4 (0.0)
9.4 (3.3)
2966.4 (1323.9)*

50
1
D
N = 250

N = 500

N = 100

100
1

D
N = 250

N = 500

0.4 (0.0)
18.9 (3.3)
163.2 (44.3)

0.6 (0.0)
49.1 (12.0)
311.9 (63.4)

0.4 (0.0)
9.2 (1.2)
-

0.5 (0.0)
29.1 (9.4)
230.7 (99.9)

0.5 (0.0)
64.3 (15.3)
230.8 (48.2)

N = 100

0.4 (0.0)
7.8 (1.5)
-

Table 8: Total automaton learning time in seconds for diﬀerent combinations of POMDP

sets (

10
1 ,

D

50
1 ,

D

100
1
D

) and maximum episode lengths (100, 250, 500).

10
1
D

N = 100 N = 250

N = 500

50
1
D
N = 100 N = 250 N = 500

100
1
D

N = 100 N = 250 N = 500

Coffee
CoffeeMail
VisitABCD

5.8 (0.3)
24.2 (1.5)
-

7.0 (0.3)
20.6 (1.5)
-

7.4 (0.3)
24.4 (1.7)
86.0 (12.0)*

8.4 (0.3)
24.8 (1.6)
-

8.7 (0.4)
29.0 (1.5)
54.9 (3.8)

10.7 (0.5)
33.0 (1.5)
50.6 (3.5)

8.6 (0.4)
29.6 (1.2)
-

9.6 (0.5)
35.7 (1.2)
55.2 (6.0)

9.4 (0.4)
35.9 (1.6)
49.9 (2.5)

Table 9: Number of examples needed to learn the last automaton for diﬀerent combinations
) and maximum episode lengths (100, 250, 500).

of POMDP sets (

10
1 ,

50
1 ,

100
1
D

D

D

average number of examples needed to learn the last automaton in each run. The following
is observed from these tables:

• The running times generally increase with the maximum episode length. Moreover,
50
1 POMDP set, which shows

Table 10 contains the average example length for the
that the longer the episode is, the longer the counterexamples become.

D

• The running times increase when the set of POMDPs becomes bigger, speciﬁcally
50
when changing from
1 . This is due to the presence of observations that
occur within the latter and not in the former. For instance, observables (cid:75) and o only
10
occur together in
1 , so additional time is spent to learn
the transition (cid:75)

50
1 and
D
o for the Coffee and CoffeeMail tasks.

but not in

100
1
D

10
1
D

to

D

D

∧

• The number of examples remains similar across diﬀerent sets of POMDPs. There is
50
only a noticeable diﬀerence between
1 because the latter includes observa-
to
tions that do not happen in the former, as explained before. Note that such changes
do not occur in VisitABCD because there is a single path to the accepting state.

10
1
D

D

• As shown in Section 8.2, the running time and the number of examples increases with
the number of subgoals that characterizes the task. Besides, Table 10 shows that the
example length is longer for the tasks with more subgoals, which can deﬁnitely have
an eﬀect on the time needed to learn the automaton.

We now brieﬂy examine the results for the set

2. Figure 11 displays the average
learning curves for diﬀerent combinations of POMDP subsets and values of N . Table 11
shows the average automaton learning time, whereas Table 12 contains the average number
of examples needed to learn an automaton for diﬀerent subsets of tasks and maximum
episode lengths (N ). Table 13 shows the average example length of each kind of trace for
diﬀerent values of N . Table 14 contains the average number of examples needed to learn the

D

1106

Induction and Exploitation of Subgoal Automata for RL

last automaton in a speciﬁc setting. In qualitative terms, the main changes we observe with
1 occur in the VisitABCD task. While N = 100 was usually insuﬃcient to
respect to
D
50
2 . By comparing the VisitABCD
learn an automaton in
50
1 .
curves, it is clear that
D
100
100
Furthermore, in the case of CoffeeMail,
,
2
1
D
D
thus more examples are needed in that case and the ﬁnal automaton is slightly diﬀerent.
This shows that the set of POMDPs has an impact on automaton learning.

consists of POMDPs requiring less steps than those in
has more types of joint events than

50
1 across runs, it is enough in
50
2
D

D

D

N = 100

N = 250

N = 500

Figure 11: Learning curves for diﬀerent combinations of POMDP sets (

10
2 ,

50
2 ,

D

100
2
D

) and

D

maximum episode lengths (100, 250, 500).

1107

02004006008001000Numberofepisodes0.00.20.40.60.81.0AveragerewardCoffee(D102)02004006008001000Numberofepisodes0.00.20.40.60.81.0AveragerewardCoffee(D502)02004006008001000Numberofepisodes0.00.20.40.60.81.0AveragerewardCoffee(D1002)05001000150020002500Numberofepisodes0.00.20.40.60.81.0AveragerewardCoffeeMail(D102)05001000150020002500Numberofepisodes0.00.20.40.60.81.0AveragerewardCoffeeMail(D502)05001000150020002500Numberofepisodes0.00.20.40.60.81.0AveragerewardCoffeeMail(D1002)02000400060008000Numberofepisodes0.00.20.40.60.81.0AveragerewardVisitABCD(D102)02000400060008000Numberofepisodes0.00.20.40.60.81.0AveragerewardVisitABCD(D502)02000400060008000Numberofepisodes0.00.20.40.60.81.0AveragerewardVisitABCD(D1002)Furelos-Blanco, Law, Jonsson, Broda, & Russo

N = 100

N = 250

N = 500

G

D

I

G

D

I

G

D

I

Coffee
CoffeeMail
VisitABCD

2.8 (1.0)
4.4 (1.4)
-

2.1 (1.1)
3.3 (2.0)
-

1.5 (0.9)
3.1 (1.8)
-

3.6 (1.8)
5.5 (2.7)
9.2 (3.0)

3.0 (2.5)
4.1 (2.8)
5.1 (3.0)

1.9 (1.4)
3.6 (2.2)
5.4 (3.0)

5.7 (4.6)
7.2 (4.3)
12.4 (4.2)

3.6 (2.8)
4.9 (3.8)
6.2 (4.0)

2.2 (1.4)
3.5 (2.3)
6.3 (4.2)

Table 10: Example length of the goal, dead-end and incomplete examples used to learn the

last automaton in the

50
1

D

setting.

10
2
D

N = 100 N = 250

N = 500

N = 100

50
2
D
N = 250

N = 500

N = 100

100
2
D
N = 250

N = 500

Coffee
CoffeeMail
VisitABCD

0.4 (0.0)
2.8 (0.4)
-

0.4 (0.0)
3.9 (1.1)
-

0.4 (0.0)
8.1 (2.2)
802.1 (297.1)

0.4 (0.0)
7.8 (3.1)
130.1 (46.4)

0.4 (0.0)
10.3 (1.6)
290.2 (123.1)

0.4 (0.0)
24.5 (7.5)
481.6 (115.1)

0.4 (0.0)
6.8 (1.4)
53.2 (15.6)

0.4 (0.0)
30.7 (14.6)
269.2 (108.0)

0.4 (0.0)
22.2 (5.3)
355.8 (68.2)

Table 11: Total automaton learning time in seconds for diﬀerent combinations of POMDP

sets (

10
2 ,

D

50
2 ,

D

D

100
2

) and maximum episode lengths (100, 250, 500).

10
2
D
N = 100 N = 250 N = 500

50
2
D
N = 100 N = 250 N = 500

100
2
D

N = 100 N = 250 N = 500

Coffee
CoffeeMail
VisitABCD

7.3 (0.3)
17.8 (1.1)
-

7.6 (0.4)
17.8 (0.8)
-

7.5 (0.5)
20.6 (1.4)
74.8 (8.2)

8.2 (0.3)
24.4 (1.5)
57.7 (5.6)

8.3 (0.3)
26.2 (1.2)
53.8 (6.2)

8.6 (0.5)
27.2 (1.4)
58.6 (3.9)

8.3 (0.3)
24.9 (1.2)
42.5 (3.3)

8.0 (0.3)
29.1 (1.4)
51.9 (3.3)

8.2 (0.3)
27.4 (1.3)
53.3 (3.3)

Table 12: Number of examples needed to learn the last automaton for diﬀerent combinations
) and maximum episode lengths (100, 250, 500).

of POMDP sets (

10
2 ,

50
2 ,

100
2
D

D

D

N = 100

N = 250

N = 500

G

D

I

G

D

I

G

D

I

2.8 (1.0)
Coffee
3.5 (1.4)
CoffeeMail
VisitABCD 7.1 (1.9)

2.1 (1.1)
3.2 (1.7)
4.6 (2.3)

1.5 (0.9)
2.7 (1.5)
4.6 (1.9)

3.6 (1.8)
4.3 (2.8)
8.9 (3.7)

3.0 (2.5)
4.0 (2.6)
5.5 (3.6)

1.9 (1.4)
3.4 (2.6)
5.1 (2.6)

5.7 (4.6)
5.1 (4.2)
11.2 (4.6)

3.6 (2.8)
4.1 (3.1)
7.1 (4.8)

2.2 (1.4)
3.5 (2.7)
5.8 (3.3)

Table 13: Example length of the goal, dead-end and incomplete examples used to learn the

last automaton in the

50
2

D

setting.

All

G

D

I

8.7 (0.4)
Coffee
26.2 (1.2)
CoffeeMail
VisitABCD 53.8 (6.2)

2.4 (0.1)
5.3 (0.3)
1.6 (0.1)

3.0 (0.1)
8.2 (0.5)
16.2 (1.4)

3.2 (0.3)
12.6 (0.9)
36.0 (4.9)

Table 14: Number of examples (total, goal, dead-end and incomplete) needed to learn the
50
2 and N = 250 setting.

last automaton in the

D

1108

Induction and Exploitation of Subgoal Automata for RL

Time (s.)

C

U

# Examples

C

U

Example Length

C

U

0.4 (0.0)
Coffee
18.9 (3.3)
CoffeeMail
VisitABCD 163.2 (44.3)

1.5 (0.2)
9314.6 (1859.7)
-

8.7 (0.4)
29.0 (1.5)
54.9 (3.8)

11.4 (0.4)
34.1 (1.4)
-

2.8 (2.1)
4.0 (2.6)
5.5 (3.1)

58.1 (64.6)
78.1 (65.7)
-

Table 15: Automaton learning statistics when trace compression is on (C) and oﬀ (U).

Time (s.)

# Examples

Example Length

O

0.4 (0.0)
Coffee
18.9 (3.3)
CoffeeMail
VisitABCD 163.2 (44.3)

ˆ
O
0.3 (0.0)
1.5 (0.1)
9.2 (1.6)

O
8.7 (0.4)
29.0 (1.5)
54.9 (3.8)

ˆ
O
6.4 (0.2)
16.1 (0.6)
30.9 (2.3)

O
2.8 (2.1)
4.0 (2.6)
5.5 (3.1)

ˆ
O
1.5 (0.6)
2.7 (1.4)
3.7 (1.8)

Table 16: Automaton learning statistics when the set of observables is unrestricted (

restricted to a particular task ( ˆ
O

).

) or

O

C.2.2 Trace Compression

Compressed traces are applicable to the OfficeWorld tasks since (1) empty observations
are meaningless (i.e., the number of steps is unimportant), and (2) seeing an observation for
more than one step in a row is equivalent to seeing it once. Table 15 shows the impact of
compressed observation traces in automaton learning. We omit the learning curves since the
learned automata (if any) are similar for both types of traces and exhibit close performance.
All runs using uncompressed traces ﬁnished on time for Coffee and CoffeeMail; in
contrast, all such runs timed out for VisitABCD. Crucially, compressed traces allow to
learn an automaton for VisitABCD on time. Besides, an automaton is also learned orders
of magnitude faster in CoffeeMail using trace compression. Note that a compressed trace
is considerably shorter than an uncompressed one even in the simplest task (Coffee).

C.2.3 Restricted Observable Set
Table 16 shows how using the restricted observable set ˆ
for each OfficeWorld task
O
compares to using the full observable set
. The learning curves are very similar for both
cases, so we do not report them. The restricted observable set ˆ
causes a sensible decrease
O
in the automaton learning time, specially for the harder tasks. Similarly, fewer examples
are needed to learn a helpful automaton and the average example length is also reduced.
Intuitively, using only the observables that describe the subgoals eases the learning: the
hypothesis space is smaller and no examples to discard irrelevant observables are needed.

O

C.2.4 Cyclicity

Table 17 shows the eﬀect that enforcing the automaton to be acyclic has on automaton
learning. The tasks that we have considered so far do not require cycles. Therefore, we

1109

Furelos-Blanco, Law, Jonsson, Broda, & Russo

Time (s.)

# Examples

Example Length

Acyclic

Cyclic

Acyclic

Cyclic

Acyclic

Cyclic

Coffee
CoffeeMail
VisitABCD

0.4 (0.0)
18.9 (3.3)
163.2 (44.3)

0.5 (0.0)
774.7 (434.4)
1961.7 (1123.8)

8.7 (0.4)
29.0 (1.5)
54.9 (3.8)

9.6 (0.4)
33.8 (1.7)
81.0 (6.6)

2.8 (2.1)
4.0 (2.6)
5.5 (3.1)

2.7 (2.2)
4.0 (2.6)
5.5 (3.3)

CoffeeDrop
CoffeeMailDrop

13.9 (8.6)*
-

0.6 (0.0)
312.2 (145.9)

15.0 (3.1)*
-

9.9 (0.5)
37.8 (1.7)

5.4 (3.5)*
-

5.3 (3.6)
7.0 (5.3)

Table 17: Comparison of diﬀerent automaton learning statistics for the cases where au-

tomata must be acyclic and where automata can have cycles.

introduce two other tasks, CoffeeDrop and CoffeeMailDrop, whose minimal automata
involve cycles to show that our approach can learn such structures. These tasks modify the
OfficeWorld dynamics such that when the agent steps on a decoration (
), the coﬀee is
∗
dropped if the agent holds it. In such case, the agent must go back to the coﬀee location.
Note that in these tasks there are not dead-end states and, thus, the learned automata do
not have rejecting states.

The results show a considerable increase in the running time for CoffeeMail and
VisitABCD when the automaton is allowed to have cycles. The fact that the hypothesis
space is bigger does not only cause an increase in the running times, but also in the number
of examples, which must rule out solutions with cycles. In contrast, the average example
length remains approximately the same.

Regarding the performance in the additional tasks, the results in the cyclic setting
for CoffeeDrop and CoffeeMailDrop are similar to the ones for Coffee and Cof-
feeMail respectively. Note that the average running time is lower in CoffeeMailDrop
than in CoffeeMail since the former has less states than the latter (it does not have a
rejecting state). Besides, the length of the examples is longer since there are no dead-end
states to actively avoid. In contrast, in the acyclic setting an automaton is only found in
10/20 and 2/20 runs for CoffeeDrop and CoffeeMailDrop respectively. Note that
the number of automaton states depends on the trace with the maximum number of times
where the coﬀee has been picked and dropped. Clearly, these tasks are not well-suited to
be expressed by an acyclic automaton.

C.2.5 Maximum Number of Edges between States

Table 18 shows the eﬀect that increasing the maximum number of edges between two states
from κ = 1 to κ = 2 has on automaton learning. Since the OfficeWorld tasks we consider
have at most one edge from a state to another, we add a new task that can only be learned
with κ > 1 to show ISA can learn such automata. The CoffeeOrMail task consists in
going to the coﬀee or mail location (it does not matter which) and then go to the oﬃce
while avoiding the decorations.

The results show that the automaton learner does not scale well to higher values of
κ. The case of VisitABCD is the most notable one: only one run has not timed out
with κ = 2. The number of successful runs has also decreased in CoffeeMail from 20

1110

Induction and Exploitation of Subgoal Automata for RL

Time (s.)

# Examples

Example Length

κ = 1

κ = 2

κ = 1

κ = 2

κ = 1

κ = 2

Coffee
CoffeeMail
VisitABCD

0.4 (0.0)
18.9 (3.3)
163.2 (44.3)

1.0 (0.1)
2252.7 (623.2)*
-

8.7 (0.4)
29.0 (1.5)
54.9 (3.8)

11.7 (0.6)
32.5 (1.6)*
-

2.8 (2.1)
4.0 (2.6)
5.5 (3.1)

3.1 (2.4)
4.0 (2.5)*
-

CoffeeOrMail

0.9 (0.1)

1.0 (0.1)

11.7 (0.6)

11.2 (0.4)

2.7 (1.8)

2.4 (2.1)

Table 18: Automaton learning statistics for diﬀerent maximum number of edges from one

state to another (κ).

Acyclic

Cyclic

No SB

SB

No SB

SB

0.5 (0.0)
Coffee
277.4 (70.2)
CoffeeMail
VisitABCD 1070.0 (725.6)

0.4 (0.0)
18.9 (3.3)
163.2 (44.3)

0.5 (0.0)
4204.3 (1334.4)*
3293.5 (1199.2)*

0.5 (0.0)
774.7 (434.4)
1961.7 (1123.8)

Table 19: Total automaton learning time when symmetry breaking is disabled (No SB) and

enabled (SB).

to 15; besides, the running time is orders of magnitude higher. While the increase on the
hypothesis space size has caused the running time to vastly increase, the average number
of examples and the average example lengths remain similar with respect to κ = 1.

Regarding the CoffeeOrMail task, note that it is not much harder than the Coffee
task: a minimal automaton consists of 4 and 5 states for κ = 2 and κ = 1, respectively.
Remember that a minimal automaton for the Coffee task consists of 4 states (regardless
of κ). Despite of the increase on the number of states from κ = 1 to κ = 2, the running
time and the other statistics remain almost the same. However, while κ = 1 is eﬀective in
this case (although it cannot return a minimal automaton), it would not be enough to learn
an automaton for any task requiring an edge with a disjunction to the accepting state (e.g.,
“observe (cid:75) or (cid:66)”).

C.2.6 Symmetry Breaking

Table 19 shows the eﬀect that symmetry breaking constraints have on the time required to
learn an automaton. The average number of examples and example length barely change
when symmetry breaking is used, so we do not report the results. The symmetry breaking
constraints speed up automaton learning by an order of magnitude in CoffeeMail (acyclic,
cyclic) and VisitABCD (acyclic). Furthermore, while two runs have timed out for the case
where automata can contain cycles and no symmetry breaking is used (one for CoffeeMail
and one for VisitABCD), no run has timed out when the symmetry breaking is used.

1111

Furelos-Blanco, Law, Jonsson, Broda, & Russo

References

Andreas, J., Klein, D., & Levine, S. (2017). Modular Multitask Reinforcement Learning
In Proceedings of the International Conference on Machine

with Policy Sketches.
Learning (ICML), pp. 166–175.

Angluin, D. (1980).

Inductive Inference of Formal Languages from Positive Data.

Inf.

Control., 45 (2), 117–135.

Angluin, D. (1987). Learning Regular Sets from Queries and Counterexamples. Inf. Com-

put., 75 (2), 87–106.

Barto, A. G., & Mahadevan, S. (2003). Recent Advances in Hierarchical Reinforcement

Learning. Discrete Event Dynamic Systems, 13 (4), 341–379.

Bonet, B., Palacios, H., & Geﬀner, H. (2009). Automatic Derivation of Memoryless Policies
and Finite-State Controllers Using Classical Planners. In Proceedings of the Interna-
tional Conference on Automated Planning and Scheduling (ICAPS).

Bradtke, S. J., & Duﬀ, M. O. (1994). Reinforcement Learning Methods for Continuous-Time
In Proceedings of the Advances in Neural Information

Markov Decision Problems.
Processing Systems (NeurIPS) Conference, pp. 393–400.

Brafman, R. I., & Tennenholtz, M. (2002). R-MAX - A General Polynomial Time Algorithm
for Near-Optimal Reinforcement Learning. J. Mach. Learn. Res., 3, 213–231.

Brooks, R. A. (1989). A Robot that Walks; Emergent Behaviors from a Carefully Evolved

Network. Neural Computation, 1 (2), 253–262.

Buckland, M. (2004). AI Game Programming by Example. Wordware Publishing Inc.

Calimeri, F., Faber, W., Gebser, M., Ianni, G., Kaminski, R., Krennwallner, T., Leone, N.,
Maratea, M., Ricca, F., & Schaub, T. (2020). ASP-Core-2 Input Language Format.
Theory Pract. Log. Program., 20 (2), 294–309.

Camacho, A., Chen, O., Sanner, S., & McIlraith, S. A. (2017). Non-Markovian Rewards
Expressed in LTL: Guiding Search Via Reward Shaping. In Proceedings of the Inter-
national Symposium on Combinatorial Search (SOCS), pp. 159–160.

Camacho, A., Icarte, R. T., Klassen, T. Q., Valenzano, R. A., & McIlraith, S. A. (2019).
LTL and Beyond: Formal Languages for Reward Function Speciﬁcation in Reinforce-
ment Learning.
In Proceedings of the International Joint Conference on Artiﬁcial
Intelligence (IJCAI), pp. 6065–6073.

Codish, M., Miller, A., Prosser, P., & Stuckey, P. J. (2019). Constraints for symmetry

breaking in graph representation. Constraints, 24 (1), 1–24.

de la Higuera, C. (2010). Grammatical Inference: Learning Automata and Grammars. Cam-

bridge University Press.

Dietterich, T. G. (2000). Hierarchical Reinforcement Learning with the MAXQ Value Func-

tion Decomposition. J. Artif. Intell. Res., 13, 227–303.

Drescher, C., Tifrea, O., & Walsh, T. (2011). Symmetry-breaking Answer Set Solving. AI

Commun., 24 (2), 177–194.

1112

Induction and Exploitation of Subgoal Automata for RL

Furelos-Blanco, D., Law, M., Russo, A., Broda, K., & Jonsson, A. (2020). Induction of Sub-
goal Automata for Reinforcement Learning. In Proceedings of the AAAI Conference
on Artiﬁcial Intelligence (AAAI), pp. 3890–3897.

Gaon, M., & Brafman, R. I. (2020). Reinforcement Learning with Non-Markovian Rewards.
In Proceedings of the AAAI Conference on Artiﬁcial Intelligence (AAAI), pp. 3980–
3987.

Gelfond, M., & Kahl, Y. (2014). Knowledge Representation, Reasoning, and the Design of
Intelligent Agents: The Answer-Set Programming Approach. Cambridge University
Press.

Gelfond, M., & Lifschitz, V. (1988). The Stable Model Semantics for Logic Programming. In
Proceedings of the International Conference and Symposium on Logic Programming,
pp. 1070–1080.

Gold, E. M. (1978). Complexity of Automaton Identiﬁcation from Given Data. Information

and Control, 37 (3), 302–320.

Heule, M., & Verwer, S. (2010). Exact DFA Identiﬁcation Using SAT Solvers.

In Pro-
ceedings of the International Colloquium on Grammatical Inference: Algorithms and
Applications (ICGI), pp. 66–79.

Ho, M. K., Abel, D., Griﬃths, T. L., & Littman, M. L. (2019). The value of abstraction.

Current Opinion in Behavioral Sciences, 29, 111 – 116.

Hu, Y., & De Giacomo, G. (2011). Generalized Planning: Synthesizing Plans that Work
for Multiple Environments. In Proceedings of the International Joint Conference on
Artiﬁcial Intelligence (IJCAI), pp. 918–923.

Kaelbling, L. P. (1993). Hierarchical Learning in Stochastic Domains: Preliminary Results.
In Proceedings of the International Conference on Machine Learning (ICML), pp.
167–173.

Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning and Acting in

Partially Observable Stochastic Domains. Artif. Intell., 101 (1-2), 99–134.

Kingma, D. P., & Ba, J. (2015). Adam: A Method for Stochastic Optimization. In Proceed-

ings of the International Conference on Learning Representations (ICLR).

Konidaris, G. (2019). On the necessity of abstraction. Current Opinion in Behavioral

Sciences, 29, 1 – 7.

Koul, A., Fern, A., & Greydanus, S. (2019). Learning Finite State Representations of Re-
current Policy Networks. In Proceedings of the International Conference on Learning
Representations (ICLR).

Kulkarni, T. D., Gupta, A., Ionescu, C., Borgeaud, S., Reynolds, M., Zisserman, A., & Mnih,
V. (2019). Unsupervised Learning of Object Keypoints for Perception and Control.
In Proceedings of the Advances in Neural Information Processing Systems (NeurIPS)
Conference, pp. 10723–10733.

Lambeau, B., Damas, C., & Dupont, P. (2008). State-Merging DFA Induction Algorithms
with Mandatory Merge Constraints. In Proceedings of the International Colloquium
on Grammatical Inference: Algorithms and Applications (ICGI), pp. 139–153.

1113

Furelos-Blanco, Law, Jonsson, Broda, & Russo

Lang, K. J., Pearlmutter, B. A., & Price, R. A. (1998). Results of the Abbadingo One
DFA Learning Competition and a New Evidence-Driven State Merging Algorithm.
In Proceedings of the International Colloquium on Grammatical Inference: Algorithms
and Applications (ICGI), pp. 1–12.

Lange, R. T., & Faisal, A. (2019). Semantic RL with Action Grammars: Data-Eﬃcient

Learning of Hierarchical Task Abstractions. CoRR, abs/1907.12477.

Law, M., Russo, A., & Broda, K. (2015a). Simpliﬁed Reduct for Choice Rules in ASP. Tech.
rep., DTR2015-2, Imperial College of Science, Technology and Medicine, Department
of Computing.

Law, M., Russo, A., & Broda, K. (2015b). The ILASP System for Learning Answer Set

Programs..

Law, M., Russo, A., & Broda, K. (2016). Iterative Learning of Answer Set Programs from
Context Dependent Examples. Theory Pract. Log. Program., 16 (5-6), 834–848.

Law, M., Russo, A., & Broda, K. (2018). The Meta-program Injection Feature in ILASP.

Tech. rep..

Leonetti, M., Iocchi, L., & Patrizi, F. (2012). Automatic Generation and Learning of Finite-
State Controllers. In Proceedings of the International Conference on Artiﬁcial Intel-
ligence: Methodology, Systems, Applications (AIMSA), pp. 135–144.

Machado, M. C., Bellemare, M. G., & Bowling, M. H. (2017). A Laplacian Framework
for Option Discovery in Reinforcement Learning. In Proceedings of the International
Conference on Machine Learning (ICML), pp. 2295–2304.

McGovern, A., & Barto, A. G. (2001). Automatic Discovery of Subgoals in Reinforcement
Learning using Diverse Density. In Proceedings of the International Conference on
Machine Learning (ICML), pp. 361–368.

Mehta, N., Ray, S., Tadepalli, P., & Dietterich, T. G. (2008). Automatic Discovery and
In Proceedings of the International Conference on

Transfer of MAXQ Hierarchies.
Machine Learning (ICML), pp. 648–655.

Menache, I., Mannor, S., & Shimkin, N. (2002). Q-Cut - Dynamic Discovery of Sub-goals
in Reinforcement Learning. In Proceedings of the European Conference on Machine
Learning (ECML), pp. 295–306.

Meuleau, N., Peshkin, L., Kim, K., & Kaelbling, L. P. (1999). Learning Finite-State Con-
trollers for Partially Observable Environments. In Proceedings of the Conference on
Uncertainty in Artiﬁcial Intelligence (UAI), pp. 427–436.

Michalenko, J. J., Shah, A., Verma, A., Baraniuk, R. G., Chaudhuri, S., & Patel, A. B.
(2019). Representing Formal Languages: A Comparison Between Finite Automata
and Recurrent Neural Networks. In Proceedings of the International Conference on
Learning Representations (ICLR).

Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves,
A., Riedmiller, M. A., Fidjeland, A., Ostrovski, G., Petersen, S., Beattie, C., Sadik,
A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., & Hassabis, D.

1114

Induction and Exploitation of Subgoal Automata for RL

(2015). Human-level control through deep reinforcement learning. Nature, 518 (7540),
529–533.

Ng, A. Y., Harada, D., & Russell, S. J. (1999). Policy Invariance Under Reward Transforma-
tions: Theory and Application to Reward Shaping. In Proceedings of the International
Conference on Machine Learning (ICML), pp. 278–287.

Parr, R., & Russell, S. J. (1997). Reinforcement Learning with Hierarchies of Machines.
In Proceedings of the Advances in Neural Information Processing Systems (NeurIPS)
Conference, pp. 1043–1049.

Segovia Aguas, J., Jim´enez, S., & Jonsson, A. (2018). Computing Hierarchical Finite State

Controllers With Classical Planning. J. Artif. Intell. Res., 62, 755–797.

Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre,
L., Kumaran, D., Graepel, T., Lillicrap, T., Simonyan, K., & Hassabis, D. (2018). A
general reinforcement learning algorithm that masters chess, shogi, and Go through
self-play. Science, 362 (6419), 1140–1144.

Simsek, ¨O., & Barto, A. G. (2004). Using Relative Novelty to Identify Useful Temporal Ab-
stractions in Reinforcement Learning. In Proceedings of the International Conference
on Machine Learning (ICML).

Simsek, ¨O., Wolfe, A. P., & Barto, A. G. (2005).

Identifying Useful Subgoals in Rein-
forcement Learning by Local Graph Partitioning. In Proceedings of the International
Conference on Machine Learning (ICML), pp. 816–823.

Srinivasan, A. (2001). The Aleph Manual..

Stolle, M., & Precup, D. (2002). Learning Options in Reinforcement Learning. In Proceed-
ings of the International Symposium on Abstraction, Reformulation and Approxima-
tion (SARA), pp. 212–223.

Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: An Introduction. MIT Press.

Sutton, R. S., Precup, D., & Singh, S. P. (1998). Intra-Option Learning about Temporally
Abstract Actions. In Proceedings of the International Conference on Machine Learning
(ICML), pp. 556–564.

Sutton, R. S., Precup, D., & Singh, S. P. (1999). Between MDPs and Semi-MDPs: A
Framework for Temporal Abstraction in Reinforcement Learning. Artif. Intell., 112 (1-
2), 181–211.

Toro Icarte, R., Klassen, T. Q., Valenzano, R. A., & McIlraith, S. A. (2018). Using Re-
ward Machines for High-Level Task Speciﬁcation and Decomposition in Reinforce-
ment Learning. In Proceedings of the International Conference on Machine Learning
(ICML), pp. 2112–2121.

Toro Icarte, R., Waldie, E., Klassen, T. Q., Valenzano, R. A., Castro, M. P., & McIlraith,
S. A. (2019). Learning Reward Machines for Partially Observable Reinforcement
Learning. In Proceedings of the Advances in Neural Information Processing Systems
(NeurIPS) Conference, pp. 15497–15508.

1115

Furelos-Blanco, Law, Jonsson, Broda, & Russo

Torrey, L., Shavlik, J. W., Walker, T., & Maclin, R. (2007). Relational Macros for Transfer in
Reinforcement Learning. In Proceedings of the International Conference on Inductive
Logic Programming (ILP), pp. 254–268.

Ulyantsev, V., Zakirzyanov, I., & Shalyto, A. (2015). BFS-Based Symmetry Breaking Pred-
icates for DFA Identiﬁcation. In Proceedings of the International Conference on Lan-
guage and Automata Theory and Applications (LATA), pp. 611–622.

Ulyantsev, V., Zakirzyanov, I., & Shalyto, A. (2016). Symmetry Breaking Predicates for

SAT-based DFA Identiﬁcation. CoRR, abs/1602.05028.

van Hasselt, H., Guez, A., & Silver, D. (2016). Deep Reinforcement Learning with Double
Q-Learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence (AAAI),
pp. 2094–2100.

Watkins, C. (1989). Learning from Delayed Rewards. Ph.D. thesis, King’s College, Cam-

bridge, UK.

Weiss, G., Goldberg, Y., & Yahav, E. (2018). Extracting Automata from Recurrent Neural
Networks Using Queries and Counterexamples. In Proceedings of the International
Conference on Machine Learning (ICML), pp. 5244–5253.

Xu, Z., Gavran, I., Ahmad, Y., Majumdar, R., Neider, D., Topcu, U., & Wu, B. (2020). Joint
Inference of Reward Machines and Policies for Reinforcement Learning. In Proceedings
of the International Conference on Automated Planning and Scheduling (ICAPS), pp.
590–598.

Zakirzyanov, I., Morgado, A., Ignatiev, A., Ulyantsev, V., & Marques-Silva, J. (2019). Ef-
ﬁcient Symmetry Breaking for SAT-Based Minimum DFA Inference. In Proceedings
of the International Conference on Language and Automata Theory and Applications
(LATA), pp. 159–173.

1116

