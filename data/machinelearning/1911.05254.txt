0
2
0
2

n
a
J

9

]
h
p
-
p
m
o
c
.
s
c
i
s
y
h
p
[

2
v
4
5
2
5
0
.
1
1
9
1
:
v
i
X
r
a

Feature engineering and symbolic regression methods for
detecting hidden physics from sparse sensor observation
data

Harsha Vaddireddy,1 Adil Rasheed,2 Anne E Staples,3 and Omer San1, a)
1)School of Mechanical & Aerospace Engineering, Oklahoma State University, Stillwater,
OK 74078, USA.
2)Department of Engineering Cybernetics, Norwegian University of Science and
Technology, N-7465, Trondheim, Norway.
3)Department of Biomedical Engineering and Mechanics, Virginia Tech, Blacksburg,
VA 24061, USA.

(Dated: 10 January 2020)

We put forth a modular approach for distilling hidden ﬂow physics from discrete
and sparse observations. To address functional expressiblity, a key limitation of
the black-box machine learning methods, we have exploited the use of symbolic
regression as a principle for identifying relations and operators that are related
to the underlying processes. This approach combines evolutionary computation
with feature engineering to provide a tool for discovering hidden parameterizations
embedded in the trajectory of ﬂuid ﬂows in the Eulerian frame of reference. Our
approach in this study mainly involves gene expression programming (GEP) and
sequential threshold ridge regression (STRidge) algorithms. We demonstrate our
results in three diﬀerent applications: (i) equation discovery, (ii) truncation error
analysis, and (iii) hidden physics discovery, for which we include both predicting
unknown source terms from a set of sparse observations and discovering subgrid
scale closure models. We illustrate that both GEP and STRidge algorithms are
able to distill the Smagorinsky model from an array of tailored features in solving
the Kraichnan turbulence problem. Our results demonstrate the huge potential of
these techniques in complex physics problems, and reveal the importance of feature
selection and feature engineering in model discovery approaches.

Keywords: Symbolic regression, gene expression programming, compressive sens-
ing, model discovery, modiﬁed equation analysis, hidden physics discovery.

I.

INTRODUCTION

Since the dawn of mathematical modelling of complex physical processes, scientists have
been attempting to formulate predictive models to infer current and future states. These
ﬁrst principle models are generally conceptualized from conservation laws, sound physical
arguments, and empirical heuristics drawn from either conducting experiments or hypothesis
made by an insightful researcher. However, there are many complex systems (some being
climate science, weather forecasting, and disease control modelling) with their governing
equations known partially and their hidden physics await to be modelled.
In the last
decade, there have been rapid advances in machine learning1,2 and easy access to rich data,
thanks to the plummeting costs of sensors and high performance computers.

This paradigm shift in data driven techniques can be readily exploited to distill new or
improved physical models for nonlinear dynamical systems. Extracting predictive models
based on observing complex patterns from vast multimodal data can be loosely termed
as reverse engineering nature. This approach is not particularly new, for example, Kepler
used planets’ positional data to approximate their elliptic orbits. The reverse engineering

a)Electronic mail: osan@okstate.edu

 
 
 
 
 
 
2

approach is most appropriate in the modern age as we can leverage computers to directly
infer physical laws from data collected from omnipresent sensors that otherwise might not
be comprehensible to humans. Symbolic regression methods are a class of data driven algo-
rithms that aim to ﬁnd a mathematical model that can describe and predict hidden physics
from observed input-response data. Some of the popular machine learning techniques that
are adapted for the task of symbolic regression are neural networks3,4, compressive sens-
ing/sparse optimization5,6, and evolutionary algorithms7,8.

Symbolic regression (SR) approaches based on evolutionary computation7,9 are a class of
frameworks that are capable of ﬁnding analytically tractable functions. Traditional deter-
ministic regression algorithms assume a mathematical form and only ﬁnd parameters that
best ﬁt the data. On the other hand, evolutionary SR approaches aim to simultaneously
ﬁnd parameters and also learn the best-ﬁt functional form of the model from input-response
data. Evolutionary algorithms search for functional abstractions with a preselected set of
mathematical operators and operands while minimizing the error metrics. Furthermore, the
optimal model is selected from Pareto front analysis with respect to minimizing accuracy
versus model complexity. Genetic programming (GP)7 is a popular choice leveraged by
most of the SR frameworks. GP is an extended and improved version of a genetic algo-
rithm (GA)10,11 which is inspired by Darwin’s theory of natural evolution. Seminal work
was done in identifying hidden physical laws12,13 from the input-output response using the
GP approach. GP has been applied in the context of the SR approach in digital signal
processing14, nonlinear system identiﬁcation15 and aerodynamic parametric estimation16.
Furthermore, GP as an SR tool was applied to identify complex closed-loop feedback con-
trol laws for turbulent separated ﬂows17–20. Hidden physical laws of the evolution of a
harmonic oscillator based on sensor measurements and the real world prediction of solar
power production at a site were identiﬁed using GP as an SR approach21.

Improved versions of GP focus on better representation of the chromosome, which helps
in the free evolution of the chromosome with constraints on the complexity of its growth,
and faster searches for the best chromosome. Some of these improved versions of GP are
gene expression programming (GEP)8, parse matrix evolution (PME)22, and linear genetic
programming (LGP)23. GEP takes advantage of the linear coded chromosome approach
from GA and the parse tree evolution of GP to alleviate the disadvantages of both GA and
GP. GEP was applied to diverse applications as an SR tool to recover nonlinear dynamical
systems24–27. Recently, GEP was modiﬁed for tensor regression, termed as multi-GEP, and
has been applied to recover functional models approximating the nonlinear behavior of the
stress tensor in the Reynolds-averaged Navier-Stokes (RANS) equations28. Furthermore,
this novel algorithm was extended to identify closure models in a combustion setting for large
eddy simulations (LES)29. Similarly, a new damping function has been discovered using
the GEP algorithm for the hybrid RANS/LES methodology30. Generally, evolutionary
based SR approaches can identify models with complex nonlinear compositions given enough
computational time.

Compressive sensing (CS)5,6 is predominately applied to signal processing in seeking
the sparsest solution (i.e., a solution with the fewest number of features). Basis pursuit
algorithms31, also identiﬁed as sparsity promoting optimization techniques32,33, play a fun-
damental role in CS. Ordinary least squares (OLS) optimization generally results in identify-
ing models with large complexity which are prone to overﬁtting. In sparse optimization, the
OLS objective function is regularized by an additional constraint on the coeﬃcient vector.
This regularization helps in taming and shrinking large coeﬃcients and thereby promoting
sparsity in feature selection and avoiding overﬁtted solutions. The least absolute shrinkage
and selection operator (LASSO)32,34 is one of the most popular regularized least squares
(LS) regression methods. In LASSO, an L1 penalty is added to the LS objective function to
recover sparse solutions35. In Bayesian terms, LASSO is a maximum a posteriori estimate
(MAP) of LS with Laplacian priors. LASSO performs feature selection and simultaneously
shrinks large coeﬃcients which may manifest to overﬁt the training data. Ridge regression36
is another regularized variant where an L2 penalty is added to the LS objective function.
Ridge regression is also deﬁned as a MAP estimate of LS with a Gaussian prior. The L2

3

penalty helps in grouping multiple correlated basis functions and increases robustness and
convergence stability for ill-conditioned systems. The elastic net approach37,38 is a hybrid
of the LASSO and ridge approaches combining the strengths of both algorithms.

Derived from these advances, a seminal work was done in employing sparse regression to
identify the physical laws of nonlinear dynamical systems39. This work leverages the struc-
ture of sparse physical laws, i.e., only a few terms represent the dynamics. The authors have
constructed a large feature library of potential basis functions that has the expressive power
to deﬁne the dynamics and then seek to ﬁnd a sparse feature set from this overdetermined
system. To achieve this, a sequential threshold least squares (STLS) algorithm39 has been
introduced in such a way that a hard threshold on OLS coeﬃcients is performed recur-
sively to obtain sparse solutions. This algorithm was leveraged to form a framework called
sparse identiﬁcation of nonlinear dynamics (SINDy)39 to extract the physical laws of non-
linear dynamical systems represented by ordinary diﬀerential equations (ODEs). This work
re-envisioned model discovery from the perspective of sparse optimization and compressive
sensing. The SINDy framework recovered various benchmark dynamical systems such as the
chaotic Lorenz system and vortex shedding behind a cylinder. However, STLS regression
ﬁnds it challenging to discover physical laws that are represented by spatio-temporal data
or high-dimensional measurements and have highly correlated features in the basis library.
This limitation was addressed using a regularized variant of STLS called the sequential
threshold ridge regression (STRidge) algorithm40. This algorithm was intended to discover
unknown governing equations that are represented by partial diﬀerential equations (PDEs),
hence forming a framework termed as PDE-functional identiﬁcation of nonlinear dynamics
(PDE-FIND)40. PDE-FIND was applied to recover canonical PDEs representing various
nonlinear dynamics. This framework also performs reasonably well under the addition of
noise to data and measurements. These sparse optimization frameworks generally have a
free parameter associated with the regularization term that is tuned by the user to recover
models ranging from highly complex to parsimonious.

In a similar direction of discovering governing equations using sparse regression tech-
niques, L1 regularized LS minimization was used to recover various nonlinear PDEs41,42
using both high ﬁdelity and distorted (noisy) data. Additionally, limited and distorted
data samples were used to recover chaotic and high-dimensional nonlinear dynamical
systems43,44. To automatically ﬁlter models with respect to model complexity (number of
terms in the model) versus test accuracy, Bayes information criteria were used to rank the
most informative models45. Furthermore, SINDy coupled with model information criteria
is used to infer canonical biological models46 and introduce a reduced order modelling
(ROM) framework47. STRidge40 was applied as a deterministic SR method to derive
algebraic Reynolds-stress models for the RANS equations48. Recently, various sparse re-
gression algorithms like LASSO32, STRidge40, sparse relaxed regularized regression49, and
the forward-backward greedy algorithm50 were investigated to recover truncation error
terms of various modiﬁed diﬀerential equations (MDEs) coming from canonical PDEs51.
The frameworks discussed above assume that the structure of the model to be recovered is
sparse in nature; that is, only a small number of terms govern the dynamics of the system.
This assumption holds for many physical systems in science and engineering.

Fast function extraction (FFX)52 is another deterministic SR approach based on pathwise
regularized learning that is also called the elastic net algorithm37. The resulting models of
FFX are selected through non-dominated ﬁltering concerning accuracy and model complex-
ity, similar to evolutionary computations. FFX is inﬂuenced by both GP and CS to better
distill physical models from data. FFX has been applied to recover hidden physical laws21,
canonical governing equations53 and Reynolds stress models for the RANS equations54.
Some other potential algorithms for deterministic SR are elite bases regression (EBR)55
and prioritized grammar enumeration (PGE)56. EBR uses only elite features in the search
space selected by measuring correlation coeﬃcients of features for the target model. PGE is
another deterministic approach that aims for the substantial reduction of the search space
where the genetic operators and random numbers from GP are replaced with grammar
production rules and systematic choices.

4

An artiﬁcial neural network (ANN), also referred to as deep learning if multiple hidden
layers are used, is a machine learning technique that transforms input features through non-
linear interactions and maps to output target features3,4. ANNs attracted attention in re-
cent times due to their exemplary performance in modelling complex nonlinear interactions
across a wide range of applications including image processing57, video classiﬁcation58 and
autonomous driving59. ANNs produce black-box models that are not quite open to physical
inference or interpretability. Recently, physics-informed neural networks (PINNs)60 were
proposed in the ﬂavor of SR that is capable of identifying scalar parameters for known
physical models. PINNs use a loss function in symbolic form to help ANNs adhere to the
physical structure of the system. Along similar directions, a Gaussian process regression
(GPR) has been also investigated for the discovery of coeﬃcients by recasting unknown
coeﬃcients as GPR kernel hyper-parameters for various time dependent PDEs61,62. As a
nonlinear system identiﬁcation tool, the GPR approach provides a powerful framework to
model dynamical systems63,64. State calibration with the four dimensional variational data
assimilation (4D VAR)65 and deep learning techniques such as long short-term memory
(LSTM)66 have been used for model identiﬁcation in ROM settings. Convolutional neural
networks (CNNs) are constructed to produce hidden physical laws from using the insight of
establishing direct connections between ﬁlters and ﬁnite diﬀerence approximations of diﬀer-
ential operators67,68. This approach has been demonstrated to discover underlying PDEs
from learning the ﬁlters by minimizing the loss functions69,70.

In this paper, we have exploited the use of SR in three diﬀerent applications, equation
discovery, truncation error analysis, and hidden physics discovery. We demonstrate the
use of the evolutionary computation algorithm, GEP, and the sparse regression algorithm,
STRidge, in the context of the SR approach to discover various physical laws represented by
linear and nonlinear PDEs from observing input-response data. We begin by demonstrating
the identiﬁcation of canonical linear and nonlinear PDEs that are up to ﬁfth order in space.
For identifying one particular PDE, we demonstrate the natural feature extraction ability
of GEP and the limits in the expressive and predictive power of using a feature library when
dealing with STRidge in discovering physical laws. We then demonstrate the discovery of
highly nonlinear truncation error terms of the Burgers MDE using both GEP and STRidge.
We highlight that the analysis of truncation errors is very important in the implicit large
eddy simulation as a way to determine inherent turbulence models. This analysis is usually
very tedious and elaborate, and our study provides a clear example of how SR tools are
suitable in such research. Following truncation error terms identiﬁcation, we apply GEP
using sparse data to recover hidden source terms represented by complex function compo-
sitions for a one-dimensional (1D) advection-diﬀusion process and a two-dimensional (2D)
vortex-merger problem. Furthermore, both GEP and STRidge are used to demonstrate the
identiﬁcation of the eddy viscosity kernel along with its ad-hoc modelling coeﬃcient closing
LES equations simulating the 2D decaying turbulence problem. An important result is
the ability of the proposed methodology to distill the Smagorinsky model from an array of
tailored features in solving the Kraichnan turbulence problem.

The rest of the paper is organized as follows. Section II gives a brief description of the
GEP and STRidge algorithms. In Section III, GEP, and STRidge are tested on identifying
diﬀerent canonical PDEs. Section IV deals with the identiﬁcation of nonlinear truncation
terms of the Burgers MDE using both STRidge and GEP. In Section V we exploit GEP for
identiﬁcation of hidden source terms in a 1D advection-diﬀusion process and a 2D vortex-
merger problem. We additionally demonstrate recovery of the eddy viscosity kernel and its
modelling coeﬃcient by both GEP and STRidge for closing the LES equations simulating
the 2D decaying turbulence problem in the same section. Finally, Section VI draws our
conclusions and highlights some ideas for future extensions of this work.

5

II. METHODOLOGY

We recover various physical models from data using two symbolic regression tools namely,
GEP, an evolutionary computing algorithm, and STRidge, which is a deterministic algo-
rithm that draws its inﬂuences from compressive sensing and sparse optimization. We take
the example of the equation discovery problem that is discussed in Section III to elaborate
on the methodology of applying GEP and STRidge for recovering various physical models.
We restrict the PDEs to be recovered to quadratic nonlinear and up to the ﬁfth order in
space. The general nonlinear PDE to be recovered is in the form of,

ut = F (σ, u, u2, ux, u2

x, uux, u2x, . . . , u2

5x),

(1)

where subscripts denote order of partial diﬀerentiation and σ is an arbitrary parameter. For
example, consider the problem of identifying the viscous Burgers equation as shown below,

ut + uux = νu2x,

(2)

Rm

×

n is the velocity ﬁeld and ν is the kinematic viscosity. In our study, m
where u(x, t)
is the number of time snapshots and n is the number of spatial locations. The solution ﬁeld
u(x, t) is generally obtained by solving Eq. 2 analytically or numerically. The solution ﬁeld
might also be obtained from sensor measurements that can be arranged as shown below,

∈

spatial locations

u1(t1) u2(t1) . . . un(t1)
u1(t2) u2(t2) . . . un(t2)
(cid:125)(cid:124)

...

...

. . .

...

u1(tm) u2(tm) . . . un(tm)

(cid:122)
u = 





(cid:123)









time snapshots

(3)

For recovering PDEs, we need to construct a library of basis functions called as feature
library that contains higher order derivatives of the solution ﬁeld u(x, t). Higher order
spatial and temporal partial derivative terms can be approximated using any numerical
scheme once the recording of the discrete data set given by Eq. 3 is available.
In our
current setup, we use the leapfrog scheme for approximating the temporal derivatives and
central diﬀerence schemes for spatial derivatives as follows,

ut =

1

−

up
j

up+1
j −
2dt

up+1
j −

u2t =

j + up

j

1

−

1

2up
dt2
up
j
−

2up
dx2
2up

up
j+1 −
2dx

up
j+1 −

j + up

j

1

−

up
j+2 −

up
j+2 −

up
j+3 −

j

j+1 + 2up
2dx3
j+1 + 6up
dx4
j+2 + 5up

4up

4up

ux =

u2x =

u3x =

u4x =

u5x =

up
j
−

2

1 −

−

j +

4up
j
−

−

5up
j
−

j+1 −
2dx5

2

up
j
−

1 −
1 + 4up

j

up
j
−

3

2 −

−






,

(4)

where temporal and spatial steps are given by dt and dx, respectively. Within the expres-
sions presented in Eq. 4, the spatial location is denoted using subscript index j, and the
temporal instant using superscript index p.

We note that other approaches such as automatic diﬀerentiation or spectral diﬀerentiation
for periodic domains can easily be adopted within our study. Both GEP and STRidge take

the input library consisting of features (basis functions) that are built using Eq. 2 and Eq. 3.
This core library, used for the equation discovery problem in Section III, is shown below,

V(t) =

Ut

Θ(U) =

U Ux U2x U3x U4x U5x

(cid:3)

(cid:2)

(cid:41)

.

(5)

6

The solution u(x, t) and its spatial and temporal derivatives are arranged with size m

1
×
in each column of Eq. 5,. For example, the features (basis functions) U and U2x are arranged
as follows,

n

(cid:101)

·

(cid:2)

(cid:3)

U =

, U2x =

,

(6)

u(x0, t0)
u(x0, t1)

u(xj, tp)

u(xn, tm)



















u2x(x0, t0)
u2x(x0, t1)

u2x(xj, tp)

u2x(xn, tm)



















where subscript j denotes the spatial location and subscript p denotes the time snapshot.
Θ(U) is expanded to include interacting
The features (basis functions) in the core library
features limited to quadratic nonlinearity and also a constant term. The ﬁnal expanded
library is given as,

(cid:101)

Θ(U) =

1 U U2 Ux UUx U2

x . . . U2
5x

,

(7)

(cid:3)

(cid:2)

×

∈

Note that the core feature library

Rm
Nβ and Nβ is number of features (basis
n
where the size of the library is Θ(U)
·
functions) i.e., Nβ = 28 for our setup. For example, if we have 501 spatial points and 101
time snapshots with 28 bases, then Θ(U) (Eq. 7) contains 501
101 rows and 28 columns.
×
Θ(U) in Eq. 5 is given as an input to GEP to recover
PDEs and the algorithm extracts higher degree nonlinear interactions of core features in
Θ(U) automatically. However, for sparse optimization techniques such as STRidge, explicit
input of all possible combinations of core features in Eq. 5 are required. Therefore, Θ(U)
in Eq. 7 forms the input to STRidge algorithm for equation identiﬁcation. This forms
(cid:101)
the fundamental diﬀerence in terms of feature building for both algorithms. The following
Subsection II A gives a brief introduction to GEP and its speciﬁc hyper-parameters that
control the eﬃcacy of the algorithm in identifying physical models from observing data.
Furthermore, the Subsection II B describes how to form linear system representations in
terms of V(t) and Θ(U) and brieﬂy describe STRidge optimization approach to identifying
sparse features and thereby building parsimonious models using spatio-temporal data.

(cid:101)

A. Gene Expression Programming

Gene expression programming (GEP)8,71 is a genotype-phenotype evolutionary optimiza-
tion algorithm which takes advantage of simple chromosome representation of genetic al-
gorithm (GA)10 and the free expansion of complex chromosomes of genetic programming
(GP)7. As in most evolutionary algorithms, this technique also starts with generating
initial random populations, iteratively selecting candidate solutions according to a ﬁtness
function, and improving candidate solutions by modifying through genetic variations using
one or more genetic operators. The main diﬀerence between GP and GEP is how both
techniques deﬁne the nature of their individuals. In GP, the individuals are nonlinear en-
tities of diﬀerent sizes and shapes represented as parse trees and in GEP the individuals
are encoded as linear strings of ﬁxed length called genome and chromosome, similar to GA
representation of individual and later expressed as nonlinear entities of diﬀerent size and
shape called phenotype or expression trees (ET). GEP is used for a very broad range of
applications, but here it is introduced as a symbolic regression tool to extract constraint
free solutions from input-response data.

7

FIG. 1. ET of a gene/chromosome with its structure in GEP. Q represents the square root operator.

The arrangement of a typical gene/chromosome in GEP is shown in Fig. 1. The GEP gene
is composed of head and tail regions as illustrated in Fig. 1. The head of a gene consists
of both symbolic terms from functions (elements from a function set F ) and terminals
(elements from a terminal set T ) whereas the tail consists of only terminals. The function
set F may contain arithmetic mathematical operators (e.g., +,
, /), nonlinear functions
(e.g., sin, cos, tan, arctan, sqrt, exp), or Boolean operators (e.g., Not , Nor , Or , And) and
the terminal set contains the symbolic variables. The gene always starts with a randomly
generated mathematical operator from the function set F . The head length is one of the
important hyper-parameters of GEP, and it is determined using trial and error as there is
no deﬁnite method to assign it. Once the head length is determined, the size of the tail
is computed as a function of the head length and the maximum arity of a mathematical
operator in the function set F 9. It can be calculated by the following equation,

−

×

,

tail length = head length

(amax −

×

1) + 1,

(8)

where amax is the maximum argument of a function in F . The single gene can be extended
to multigenic chromosomes where individual genes are linked using a linking function (eg.,
). The general rule of thumb is to have a larger head and higher number of genes
+,
when dealing with complex problems9.

, /,

×

−

The structural organization of the GEP gene is arranged in terms of open reading frames
(ORFs) inspired from biology where the coding sequence of a gene equivalent to an ORF
begins with a start codon, continue with an amino acid codon and ends with a termination
codon. In contrast to a gene in biology, the start site is always the ﬁrst position of a gene in
GEP, but the termination point does not always coincide with the last position of a gene.
These regions of the gene are termed non coding regions downstream of the termination
point. Only the ORF region is expressed in the ET and can be clearly seen in Fig. 1.

Even though the none-coding regions in GEP genes do not participate in ﬁnal solution,
the power of GEP evolvability lies in this region. The syntactically correct genes in GEP
evolve after modiﬁcation through diverse genetic operators due to this region chromosome.
This is the paramount diﬀerence between GEP and GP implementations where in latter,
many syntactically invalid individuals are produced and need to be discarded while evolving
the solutions and additional special constraint are imposed on the depth/complexity of
candidate solution to be evolved to avoid bloating problem19.

Fig. 2 displays the typical ﬂowchart of the GEP algorithm. The process is described

brieﬂy below,

1. The optimization procedure starts with a random generation of chromosomes built
upon combinations of functions and terminals. The size of the random population is a
hyper-parameter and the larger the population size, better the probability of ﬁnding
the best candidate solution.

  b    a   b      a      b  a   a   b   a   b   b   a   a   b  b  a×+− +  0    1    2    3   4   5    6   7      8   9   0   1   2   3   4   5   6   7   8  9  0HeadTailORF regionNone-coding region×+− babaExpression tree (ET):Function setTerminal setEquation: b(a+b-) ‾√2. After the population is generated, the chromosomes are expressed as ETs, which is
converted to a numerical expression. This expression is then evaluated using a ﬁtness
function. In our setup, we employ the mean squared error between the best predicted
model f ∗ and the true model f as the ﬁtness function given by,

8

M SE =

N

1
N

f ∗(lk) −

2

,

f(l)

(9)

(cid:88)l=1 (cid:16)
where f ∗lk is the value predicted by the chromosome k for the ﬁtness case l (out of N
samples cases) and fl is the true or measurement value for the lth ﬁtness case.

(cid:17)

FIG. 2. Flowchart of the gene expression programming.

3. The termination criteria is checked after all ﬁtness evaluations, to continue evolving
or to save the best ﬁtness chromosome as our ﬁnal predicted model. In our current
setup, we terminate after a speciﬁed number of generations.

4. The evolvability/reproduction of chromosome through genetic operators which is the
core part of the GEP evolutionary algorithm executes if termination criteria is not
met. Before the genetic operations on chromosome begins, the best chromosome
according to ﬁtness function is cloned to the next generations using a selection method.
Popular selection methods include tournament selection with elitism and roulette-
wheel selection with elitism. In our current setup, we use tournament selection with
elitism.

5. The four genetic operators that introduce variation in populations are mutation, inver-
sion, transposition, and recombination. The GEP transposition operator is applied to
the elements of the chromosome in three ways: insertion sequence (IS), root insertion
sequence (RIS) and gene insertion sequence and similarly three kinds of recombination
are applied namely one point, two point, and gene recombination.

6. The process is continued up to termination criteria is met, which is the number of

generations in our current setup.

Numerical constants occur in most mathematical models and, therefore, it is important
to any symbolic regression tools to eﬀectively integrate ﬂoating point constants in their
optimization search. GP7 handles numerical constants by introducing random numerical
constants in a speciﬁed range to its parse trees. The random constants are moved around
the parse trees using the crossover operator. GEP handles the creation of random numerical
constants (RNCs) by using an extra terminal ‘?’ and a separate domain Dc composed of
symbols chosen to represent random numerical constants9. This Dc speciﬁc domain starts
from the end of the tail of the gene.

Initial population ofchromosomesExpress chromosomes &execute each programEvaluate ﬁtnessIterate orTerminate ?Save best chromosomeSelectionKeep best chromosomeReplication,Mutation,IS,RIS & geneTransposition,1 point, 2 point & geneRecombination.YesNoReproduction×−acaRandom chromosomescreated×− bba...........TABLE I. GEP hyper-parameters for various genetic operators selected for all the test cases in
this study.

Hyper-parameters

Value

9

Selection

Mutation rate

Inversion

IS transposition rate

RIS transposition rate

Gene transposition rate

One point recombination

Two point recombination

Gene recombination

Dc speciﬁc mutation rate

Dc speciﬁc inversion rate

Dc speciﬁc transposition rate

Random constant mutation rate

Tournament selection

0.05

0.1

0.1

0.1

0.1

0.3

0.2

0.1

0.05

0.1

0.1

0.02

For each gene, RNCs are generated during the creation of a random initial population and
kept in an array. To maintain the genetic variations in the pool of RNCs, additional genetic
operators are introduced to take eﬀect on Dc speciﬁc regions. Hence in addition to the usual
genetic operators such as mutation, inversion, transposition and recombination, the GEP-
RNC algorithm has Dc speciﬁc inversion, transposition, and random constant mutation
operators. Hence, with these modiﬁcations to the algorithm, an appropriate diversity of
random constants can be generated and evolved through operations of genetic operators.
The values for each genetic operator selected for this study are listed in Table I. These
values are selected from various examples given by Ferreira9 combined with the trial and
error approach. Additionally, to simplify our study, we use the same parameters for all the
test cases even though they may not be the best values for the test case under investigation.

Once decent values of genetic operators that can explore the search space are selected,
the size of the head length, population, and the number of genes form the most important
hyper-parameters for GEP. Generally, larger head length and a greater number of genes are
selected for identifying complex expressions. Larger population size helps in a diverse set
of initial candidates which may help GEP in ﬁnding the best chromosome in less number
of generations. However, computational overhead increases with an increase in the size of
the population. Furthermore, the best chromosome can be identiﬁed in fewer generations
with the right selection of the linking function between genes. GEP algorithm inherently
performs poor in predicting the numerical constants that are ubiquitous in physical laws.
Hence, the GEP-RNC algorithm is used where a range of random constants are predeﬁned
to help GEP to ﬁnd numerical constants. This also becomes important in GEP identifying
the underlying expression in fewer generations. Finally, we note that due to the heuris-
tic nature of evolutionary algorithms, any other combinations of hyper-parameters might
work perfectly in identifying the symbolic expressions. In this study, we use geppy72, an
open source library for symbolic regression using GEP, which is built as an extension to
distributed evolutionary algorithms in Python (DEAP) package73. All codes used in this
study are made available on Github (https://github.com/sayin/SR).

10

B. Sequential Threshold Ridge Regression

Compressive sensing/sparse optimization6,74 has been exploited for sparse feature selec-
tion from a large library of potential candidate features and recovering dynamical systems
represented by ODEs and PDEs39,40,45 in a highly eﬃcient computational manner. In our
setup, we use this STRidge40 algorithm to recover various hidden physical models from
observed data. In continuation with the Section II where we deﬁne feature library Θ(U)
and target/output data V(t), this subsection brieﬂy explains the formation of an overde-
termined linear system for STRidge optimization to identify various physical models from
data.

The Burgers PDE given in Eq. 2 or any other PDE under consideration can be written

in the form of linear system representation in terms of Θ(U) and V(t),

·

β,

V(t) = Θ(U)

(10)
where β = [β1, β2, . . . , βNβ ] is coeﬃcient vector of size RNβ where Nβ is number of features
(basis functions) in library Θ(U). Note that Θ(U) is an over-complete library (the number
of measurements is greater than the number of features) and having rich feature (column)
space to represent the dynamics under consideration. Thus, we form an overdetermined
linear system in Eq. 10. The goal of STRidge is to ﬁnd a sparse coeﬃcient vector β that
only consists of active features, which best represent the dynamics. The rest of the features
are hard thresholded to zero. For example, in the Burgers equation given by Eq. 2, STRidge
ideally has to ﬁnd the coeﬃcient vector β that corresponds to the features uux and u2x and
simultaneously it should set all other feature coeﬃcients to zero.

FIG. 3. Structure of compressive matrices with sparse non zero entries in coeﬃcient vector β. Red
boxes in β vector correspond to active feature coeﬃcients and all other coeﬃcients being set to
zero.

The linear system deﬁned in Eq. 10 can be solved for β using the ordinary least squares
(OLS) problem. But OLS minimization tries to form a functional relationship with all the
features in Θ(U) resulting in all non zero values in the coeﬃcient vector β. Thus solving
Eq. 10 using OLS infers radically complex functional form to represent the underlying
PDE and generally results in overﬁtted models. Regularized least square minimization
can be applied to constraint the coeﬃcients and avoid overﬁtting. Hence regularized LS
optimization is preferred to identify the sparse features (basis functions) along with their
coeﬃcient estimation. Typical estimation of sparse coeﬃcient vector with P non zero entries
in β is shown in Fig. 3. General sparse regression objective function to approximate the
solution of the coeﬃcient vector β is given by,

||0,
β
(11)
where λ is regularizing weight and
||0 corresponds to L0 penalty which makes the problem
np-hard. Hence to arrive at convex optimization problem of Eq. 12, L1 and L2 penalty is
generally used to approximate the solution of the coeﬃcient vector β.

β∗ = arg minβ ||
||

−

β

β

·

V(t)

2
2 + λ
||
||

Θ

=× ( )Θ( )  nonzero entries .    ×  measurements . ×1×1  The addition of L1 penalty to LS objective function which corresponds to maximum a
posteriori estimate (MAP) of Laplacian prior and termed as least absolute shrinkage and
selection operator (LASSO) in compressive sensing. It is deﬁned by,

11

β∗ = arg minβ ||

Θ

β

·

−

V(t)

2
2 + λ
||
||

β

||1.

(12)

However, the performance of LASSO deteriorates when the feature space is correlated40.
The sequential threshold least squares (STLS) algorithm was proposed to identify dynami-
cal systems represented by ODEs39. In STLS, a hard threshold is performed on least square
estimates of regression coeﬃcients and hard threshold is recursively performed on remain-
ing non zero coeﬃcients. However, the eﬃcacy of STLS reduces when dealing with the
identiﬁcation of systems containing multiple correlated columns in Θ. Hence L2 regularized
least squares termed as ridge regression36, which corresponds to the maximum a posteriori
estimate using a Gaussian prior, is proposed to handle the identiﬁcation of PDEs. Ridge
regression is deﬁned by,

β∗ = arg minβ ||

V(t)
Θ
= (ΘT Θ + λT I)ΘT V(t).

−

β

·

2
2 + λ
||
||

β

||2,

(13)

Ridge regression is substituted for ordinary least squares in STLS and the resulting al-
gorithm as sequential threshold ridge regression (STRidge)40. The STRidge framework40
is illustrated in Algorithm 1 for the sake of completeness. Note that, if λ = 0, STRidge
becomes STLS procedure. For more elaborate details on updating tolerance (tol) to perform
hard thresholding in Algorithm 1, readers are encouraged to refer supplementary document
of Rudy et al40.

Algorithm 1: STRidge(Θ, V(t), λ, tol, iters)40

Input: Θ, V(t), λ, tol, iters
Output: β∗
β∗ = arg minβ ||Θ · β − V(t)||2
large = {p : |β∗
β∗[ large] = 0
β∗[large] = STRidge(Θ[:, large], V(t), λ, tol, iters − 1)
return β∗

2 + λ||β||2
2

p | ≥ tol}

We use the framework provided by Rudy et al.40 in our current study. The hyper-
parameters in STRidge include the regularization weight λ and tolerance level tol which are
to be tuned to identify appropriate physical models. In the present study, the sensitivity
of feature coeﬃcients for various values of λ and the ﬁnal value of λ where the best model
is identiﬁed is showed. The following sections deal with various numerical experiments to
test the GEP and STRidge frameworks.

III. EQUATION DISCOVERY

Partial diﬀerential equations (PDEs) play a prominent role in all branches of science and
engineering. They are generally derived from conservation laws, sound physical arguments,
and empirical heuristic from an insightful researcher. The recent explosion of machine
learning algorithms provides new ways to identify hidden physical laws represented by
PDEs using only data. In this section, we demonstrate the identiﬁcation of various linear
and nonlinear canonical PDEs using the GEP and STRidge algorithms from using data
alone. Analytical solutions of PDEs are used to form the data. Table II summarizes various
PDEs along with their analytical solutions u(t, x) and domain discretization. Building a

TABLE II. Summary of canonical PDEs selected for recovery.

PDE

Wave eq.
ut = −aux

Heat eq.
ut = −αu2x

Exact solution

u(t, x) = sin(2π(x − at))

u(t, x) = −sin(x)exp(−αt)

12

Constant
parameters

Discretization
n (spatial)
m (temporal)

a = 1.0

x ∈ [0, 1] (n = 101),
t ∈ [0, 1] (m = 101)

α = 1.0

x ∈ [−π, π] (n = 201),
t ∈ [0, 1]] (m = 101)

Burgers eq. (i)
ut = −uux + νu2x

u(t, x) =

(t + 1)

(cid:16)

x

√

1 + (

t + 1)exp( 1
16ν

(cid:17)

4x2−t−1
t+1

)

ν = 0.01

x ∈ [0, 1] (n = 101),
t ∈ [0, 1] (m = 101)

Burgers eq. (ii)
ut = −uux + νu2x

u(t, x) =

2νπexp(−π2νt)sin(πx)
a + exp(−π2νt)cos(πx)

ν = 0.01,
a = 5/4

x ∈ [0, 1] (n = 101),
t ∈ [0, 100] (m = 101)

Korteweg-de Vries eq.
ut = −αuux − βu3x

u(t, x) = 12

(cid:18) 4cosh(2x − 8t) + cosh(4x − 64t) + 3
(3cosh(x − 28t) + cosh(3x − 36t))2

(cid:19)

α = 6.0,
β = 1.0

x ∈ [−10, 10] (n = 501),
t ∈ [0, 1] (m = 201)

Kawahara eq.
ut = −uux − αu3x − βu5x

u(t, x) =

105
169

sech

(cid:18) 1
√
2

13

(cid:19)4

(x − at)

Newell-Whitehead-Segel eq.
ut = κu2x + αu − βuq

u(t, x) =

(cid:18)

1 + exp(

1

x
√
6

(cid:19)2

−

5t
6

)

α = 1.0,
β = 1.0,
a = 36/169

κ = 1.0,
α = 1.0,
β = 1.0,
q = 2

x ∈ [−20, 20] (n = 401),
t ∈ [0, 1] (m = 101)

x ∈ [−40, 40] (n = 401),
t ∈ [0, 2] (m = 201)

Sine-Gordon eq.
u2t = κu2x − αsin(u)

u(t, x) = 4tan−1(sech(x)t)

κ = 1.0,
α = 1.0

x ∈ [−2, 2] (n = 401),
t ∈ [0, 1] (m = 101)

TABLE III. GEP hyper-parameters selected for identiﬁcation of various PDEs.

Hyper-parameters

Wave eq. Heat eq. Burgers eq. (i) Burgers eq. (ii)

Head length

Number of genes

Population size

Generations

Length of RNC array

Random constant minimum

Random constant maximum

2

1

25

100

10

−10

10

2

2

25

100

10

−1

1

4

1

20

500

30

−1

1

2

2

50

500

5

−1

1

feature library and corresponding response data to identify PDEs is discussed in detail in
Section II.

We reiterate the methodology for PDE identiﬁcation in Section II. The analytical solution
u(t, x) is solved at discrete spatial and temporal locations resulting from the discretization
of space and time domains as given in Table II. The discrete analytical solution is used as
input data for calculating higher order spatial and temporal data using the ﬁnite diﬀerence

13

TABLE IV. GEP hyper-parameters selected for identiﬁcation of various PDEs.

Hyper-parameters

KdV eq. Kawahara eq. NWS eq.

Sine-Gordon eq.

Head length

Number of genes

Population size

Generations

Length of RNC array

Random constant minimum

Random constant maximum

6

5

20

500

30

1

10

2

1

20

100

5

−1

1

5

3

30

100

25

−10

10

3

2

100

500

20

−10

10

TABLE V. GEP functional and terminal set used for equation discovery. ‘?’ is a random constant.

Parameter

Value

Function set

Terminal set

+, −, ×, /, sin, cos

(cid:101)Θ(U), ?

Linking function

+

approximations listed in Eq. 4. Furthermore, the feature library is built using discrete
solution u(t, x) and higher order derivative which is discussed in Section II. As GEP is
Θ(U) given in Eq. 5 is enough to form
a natural feature extractor, core feature library
input data, i.e., GEP terminal set. Table V shows the function set and terminal set used
for equation identiﬁcation and Table I lists the hyper-parameter values for various genetic
operators. However, extended core feature library Θ(U) which contains a higher degree
interactions of features is used as input for STRidge as the expressive power of STRidge
depends on exhaustive combinations of features in the input library. The temporal derivative
of u(t, x) is target or response data V(t) given in Eq. 5 for both GEP and STRidge.

(cid:101)

A. Wave Equation

Our ﬁrst test case is the wave equation which is a ﬁrst order linear PDE. The PDE and
its analytical solution are listed in Table II. We choose the constant wave speed a = 1.0 for
propagation of the solution u(t, x). Fig. 4 shows the analytical solution u(t, x) of the wave
equation. The GEP hyper-parameters used for identiﬁcation of the wave equation are listed
in Table III. We use a smaller head length and a single gene for simple cases like a linear
wave PDE. We note that any other combinations of hyper-parameters may identify the
underlying PDE. Fig. 5 illustrates the identiﬁed PDE in the ET form. When the ET form
is simpliﬁed, we can show that the resulting equation is the correct wave PDE, identiﬁed
with its wave propagation speed parameter a.

The regularization weight (λ) in STRidge is swept across various values as shown in
Fig. 6. The yellow line in Fig. 6 represents the value of λ at which the best identiﬁed PDE
is selected. Note that in this simple case STRidge was able to ﬁnd the wave equation for
almost all the values of λ’s that are selected. Table VI shows the wave PDE recovered by
both GEP and STRidge.

14

FIG. 4. Analytical solution of the wave equation.

TABLE VI. Wave equation identiﬁed by GEP and STRidge.

Recovered

Test error

True

GEP

STRidge

ut = −1.00 ux

ut = −1.00 ux

ut = −1.00 ux

1.72 × 10−28

9.01 × 10−29

FIG. 5. Wave equation in terms of ET identiﬁed by GEP.

B. Heat Equation

We use the heat equation which is a second order linear PDE to test both SR approaches.
The PDE and its analytical solution is listed in Table II. The physical parameter α = 1.0
may represent thermal conductivity. Fig. 7 displays the analytical solution u(t, x) of the
heat equation. Table III lists the GEP hyper-parameters used for identiﬁcation of the heat
equation. Fig. 8 shows the identiﬁed PDE in the form of an ET. When the ET form is
simpliﬁed, we can show that the resulting model is the heat equation identiﬁed with its
coeﬃcient α.

The regularization weight (λ) in STRidge is swept across various values as shown Fig. 9.
The yellow line in Fig. 9 represents the value of λ selected at which STRidge ﬁnds the

x0.00.20.40.60.81.0t0.00.20.40.60.81.0u(t,x)1.000.750.500.250.000.250.500.751.000.50.00.5+×−1−ux−6615

FIG. 6. STRidge coeﬃcients as a function of regularization parameter λ for the wave equation.

FIG. 7. Analytical solution of the heat equation.

heat equation accurately. Note that STRidge was able to ﬁnd the heat equation for low
values of the regularization weight λ as shown in Fig. 9. Table VII shows the heat equation
recovered by both GEP and STRidge. STRidge was able to ﬁnd a more accurate coeﬃcient
(α) value than GEP. Furthermore, a small constant value is also identiﬁed along with the
heat equation by GEP.

TABLE VII. Heat equation identiﬁed by GEP and STRidge.

Recovered

Test error

True

GEP

ut = −1.00 u2x
ut = −0.99 u2x − 5.33 × 10−15

STRidge

ut = −1.00 u2x

5.55 × 10−24

4.09 × 10−30

C. Burgers Equation (i)

Burgers equation is a fundamental nonlinear PDE occurring in various areas such as ﬂuid
mechanics, nonlinear acoustics, gas dynamics and traﬃc ﬂow75,76. The interest in the Burg-
ers equation arises due to the non linear term uux and presents a challenge to both GEP

10201017101410111081051021.00.80.60.40.20.0Coefficientsuxx3210123t0.00.20.40.60.81.0u(t,x)1.000.750.500.250.000.250.500.751.000.50.00.516

FIG. 8. Heat equation in terms of ET identiﬁed by GEP.

FIG. 9. STRidge coeﬃcients as a function of regularization parameter λ for the heat equation.

and STRidge in the identiﬁcation of its PDE using data. The form of the Burgers PDE
and its analytical solution77 is listed in Table II. The physical parameter ν = 0.01 can be
considered as the kinematic viscosity in ﬂuid ﬂows. Fig. 10 shows the analytical solution
u(t, x) of the Burgers equation. Table III shows the GEP hyper-parameters used for iden-
tiﬁcation of the Burgers equation. Fig. 11 shows the identiﬁed PDE in the form of the ET.
When ET form is simpliﬁed, we can show that the resulting model is the Burgers equation
identiﬁed along with the coeﬃcient of the nonlinear term and the kinematic viscosity. GEP
uses more generations for identifying the Burgers PDE due to its nonlinear behavior along
with the identiﬁcation of feature interaction term uux.

The regularization weight (λ) in STRidge is swept across various values as shown in
Fig. 12. The yellow line in Fig. 12 represents the value of λ at which the best identiﬁed
PDE is selected. Note that the STRidge algorithm was able to ﬁnd the Burgers equation at
multiple values of regularization weights λ. Table VIII shows the Burgers PDE recovered
by both GEP and STRidge. There is an additional constant coeﬃcient term recovered by
GEP. Furthermore, the recovery of the nonlinear term using a limited set of input features
shows the usefulness of GEP.

+×−0.99−u2x−98.9910201017101410111081051021.000.750.500.250.000.250.500.751.00Coefficientsu2x17

FIG. 10. Analytical solution of the Burgers equation (i).

TABLE VIII. Burgers equation (i) identiﬁed by GEP and STRidge.

Recovered

Test error

True

GEP

ut = −uux + 0.01 u2x
ut = −uux + 0.01 u2x − 1.23 × 10−5 6.10 × 10−08
5.19 × 10−08

STRidge ut = −uux + 0.01 u2x

FIG. 11. Burgers equation (i) in terms of ET identiﬁed by GEP.

D. Burgers Equation (ii)

Burgers PDE with a diﬀerent analytical solution is used to test the eﬀectiveness of GEP
and STRidge as the input data is changed but represented by the same physical law. The
analytical solution of the Burgers equation (ii) is listed in Table II. The physical parameter
ν = 0.01 is used to generate the data. Fig. 13 shows the alternate analytical solution u(t, x)
of the Burgers equation. Table III shows the GEP hyper-parameters used for identiﬁcation

x0.00.20.40.60.81.0t0.00.51.01.52.0u(t,x)0.000.050.100.150.200.250.300.350.10.20.3+×−1−×uxu×u2x0.011.23×10−518

FIG. 12. STRidge coeﬃcients as a function of regularization parameter λ for the Burgers equation
(i).

of the Burgers equation (ii). Fig. 14 shows the identiﬁed PDE in the form of ET. When ET
form is simpliﬁed, we can show that the resulting model is the Burgers equation identiﬁed
along with the coeﬃcient of nonlinear term and kinematic viscosity. With an alternate
solution, GEP uses a larger head length, more genes, and a larger population for identifying
the same Burgers PDE.

FIG. 13. Analytical solution of the Burgers equation (ii).

The regularization weight (λ) in STRidge is swept across various values as shown Fig. 15.
The yellow line in Fig. 15 represents the value of λ at which the best identiﬁed PDE is
selected. Note that STRidge was able to ﬁnd the Burgers equation at various values of
regularization weight λ. Table IX shows the Burgers PDE recovered by both GEP and
STRidge.

TABLE IX. Burgers equation (ii) identiﬁed by GEP and STRidge.

Recovered

Test error

True

GEP

ut = −1.00 uux + 0.01 u2x
ut = −1.01 uux + 0.01 u2x − 3.33 × 10−6 1.94 × 10−09
1.85 × 10−08

STRidge ut = −0.99 uux + 0.01 u2x

1051041031021011.000.750.500.250.000.250.500.751.00Coefficientsu2xuuxx0.00.20.40.60.81.0t020406080100u(t,x)0.000.010.020.030.040.050.060.070.080.020.040.0619

FIG. 14. Burgers equation (ii) in terms of ET identiﬁed by GEP.

FIG. 15. STRidge coeﬃcients as a function of regularization parameter λ for the Burgers equation
(ii).

E. Korteweg-de Vries (KdV) Equation

Korteweg and de Vries derived the KdV equation to model Russells phenomenon of
solitons78,79. The KdV equation also appears when modelling the behavior of magneto-
hydrodynamic waves in warm plasma’s, acoustic waves in an inharmonic crystal and ion-
acoustic waves80. Many diﬀerent forms of the KdV equation available in the literature but
we use the form given in Table II. Fig. 16 shows the analytical solution u(t, x) of the KdV
equation81.
It can be seen that this analytical solution refers to two solutions colliding
together which forms good test case for SR techniques like GEP and STRidge. Table IV
shows the GEP hyper-parameters used for identiﬁcation of the KdV equation. Due to
the higher nonlinear dynamics represented by higher order PDE, GEP requires large head
length and genes compared to other test cases in equation discovery. Fig. 17 shows the
identiﬁed PDE in the form of the ET. When ET form is simpliﬁed, we can observe that the
resulting model is the KdV equation identiﬁed along with its coeﬃcients.

The regularization weight (λ) in STRidge is swept across various values as shown Fig. 18.
The yellow line in Fig. 18 represents the value of λ at which the best identiﬁed PDE is
selected. Note that STRidge was able to ﬁnd the KdV equation at various values of the
regularization weights (λ). Table X shows the KdV equation recovered by both GEP and
STRidge. The physical model identiﬁed by STRidge is more accurate to the true PDE than

+×−0.01007+×−−10.01u2x×/u0.01ux3.33×10−610101081061041021.00.50.00.51.0Coefficientsu2xuux20

FIG. 16. Analytical solution of the KdV equation.

the model identiﬁed by GEP.

TABLE X. KdV equation identiﬁed by GEP and STRidge.

Recovered

Test error

True

GEP

ut = −6.00 uux + 1.00 u3x
ut = −5.96 uux + 0.99 u3x − 5.84 × 10−4

STRidge ut = −6.04 uux + 1.02 u3x

0.29

0.02

FIG. 17. KdV equation in terms of ET identiﬁed by GEP.

F. Kawahara Equation

We consider the Kawahara equation, which is a ﬁfth-order nonlinear PDE82 shown in
Table II. This equation is sometimes also referred to as a ﬁfth-order KdV equation or
singularly perturbed KdV equation. The ﬁfth-order KdV equation is one of the most well
known nonlinear evolution equation which is used in the theory of magneto-acoustic waves
in a plasma82, capillary-gravity waves83 and the theory of shallow water waves84. This
test case is intended to test GEP and STRidge for identifying higher order derivatives
from observing data. We use an analytical solution85 which is a traveling wave solution
given in Table II. This analytical solution also satisﬁes the linear wave equation and hence
both GEP and STRidge may recover a wave PDE (not shown here) as this is the sparsest

x1050510t0.00.10.20.30.40.5u(t,x)1234567246×−5.96+6−×uxuux−−1×66×1ux/u3x621

FIG. 18. STRidge coeﬃcients as a function of regularization parameter λ for the KdV equation.

model represented by observed data (Fig. 19). For simplifying the analysis, we remove the
potential basis ux from the feature library42 (Θ(U)) for STRidge and additionally include
uux basis in core feature library (

Θ(U)) for GEP.

(cid:101)

FIG. 19. Analytical solution of the Kawahara equation.

Table IV shows the GEP hyper-parameters used for the identiﬁcation of the Kawahara
equation. Due to simplifying the feature library, GEP requires smaller head length and
single gene. Fig. 20 shows the identiﬁed PDE in the form of ET. When ET form is simpliﬁed,
we can show that the resulting model is the Kawahara equation identiﬁed correctly along
with its coeﬃcients. For STRidge, the regularization weight (λ) is swept across various
values as shown in Fig. 21. The yellow line in Fig. 21 represents the value of λ at which the
best identiﬁed PDE is selected. Note that STRidge was able to ﬁnd the Kawahara equation
at various values of regularization weights (λ). Table XI shows the Kawahara equation
identiﬁed by both GEP and STRidge.

G. Newell-Whitehead-Segel Equation

Newell-Whitehead-Segel (NWS) equation is a special case of the Nagumo equation86.
Nagumo equation is a nonlinear reaction-diﬀusion equation that models pulse transmission
line simulating a nerve axon87, population genetics88, and circuit theory89. The NWS
equation and its analytical solution are shown in Table II. We use a traveling wave solution90

10101081061041026543210Coefficientsu3xuuxx201510505101520t0.00.20.40.60.81.0u(t,x)0.10.20.30.40.50.60.20.40.622

TABLE XI. Kawahara equation identiﬁed by GEP and STRidge.

Recovered

True

GEP

ut = −1.0 uux − 1.00 u3x − 1.0 u5x
ut = −1.0 uux − 1.00 u3x − 1.0 u5x − 8.27 × 10−8

STRidge

ut = −1.0 uux − 0.99 u3x − 1.0 u5x

Test error

5.29 × 10−11

1.35 × 10−12

FIG. 20. Kawahara equation in terms of ET identiﬁed by GEP.

FIG. 21. STRidge coeﬃcients as a function of regularization parameter λ for the Kawahara equa-
tion.

that satisﬁes both wave and NWS equations (Fig. 22). We carry similar changes to the
feature library that was applied to discovering the Kawahara equation.

Table IV shows the GEP hyper-parameters used for identiﬁcation of the NWS equation.
However contrast to identifying the Kawahara equation with smaller head length and single
gene from simplifying the feature library, for NWS case GEP requires larger head length
and more genes for identifying PDE as shown in Table IV. This is due to the identiﬁcation
of nonlinear interaction feature u2 that appears in the NWS equation. Fig. 23 shows the
identiﬁed PDE in the form of ET. When ET form is simpliﬁed, we can show that the
resulting model is the NWS equation identiﬁed along with its coeﬃcients. For STRidge,
the regularization weight (λ) is swept across various values as shown Fig. 24. The yellow
line in Fig. 24 represents the value of λ at which the best identiﬁed PDE is selected. Note
that STRidge was able to ﬁnd the NWS equation at various values of regularization weights

+×1.0−−u5xu3xuux−8.27×10−810101091081071061051041031021.000.750.500.250.000.250.500.751.00Coefficientsu3xu5xuux23

FIG. 22. Analytical solution of the NWS equation.

(λ). Table XII shows the NWS equation identiﬁed by both GEP and STRidge.

TABLE XII. NWS equation identiﬁed by GEP and STRidge.

Recovered

True

GEP

STRidge

ut = 1.00 u2x + 1.00 u − 1.00 u2
ut = 0.99 u2x + 0.99 u − 0.99 u2 − 8.27 × 10−8
ut = 1.00 u2x + 0.99 u − 0.99 u2

Test error

3.02 × 10−11

1.36 × 10−11

FIG. 23. NWS equation in terms of ET identiﬁed by GEP.

H. Sine-Gordon Equation

Sine-Gordon equation is a nonlinear PDE that appears in propagating of ﬂuxions in
Josephson junctions91, dislocation in crystals92 and nonlinear optics76. Sine-Gordon equa-
tion has a sine term that needs to be identiﬁed by GEP and STRidge by observing data
(Fig. 25). This test case is straight forward for GEP as the function set includes trigono-
metric operators that help to identify the equation. However, the application of STRidge is
suitable if features library is limited to basic interactions and does not contain a basis with

x40302010010203040t0.00.51.01.52.0u(t,x)0.20.40.60.80.20.40.60.8+×−0.99+−+u×uu+u2xu−+u×−33+uu4x−u4xu−8.9924

FIG. 24. STRidge coeﬃcients as a function of regularization parameter λ for the NWS equation.

trigonometric dependencies. STRidge may recover inﬁnite series approximations if higher
degree basic feature interactions are included in the feature library39. Note that the output
or target data for the Sine-Gordon equation consists of second order temporal derivative of
velocity ﬁeld u(t, x). Hence, V(t) consists of u2t measurements instead of ut.

FIG. 25. Analytical solution of the Sine-Gordon equation.

Table IV shows the GEP hyper-parameters used for identifying the Sine-Gordon equation.
For our analysis, GEP found the best model when the larger population size used. Fig. 26
shows the identiﬁed PDE in the form of ET. When ET form is simpliﬁed, we can show
that the resulting model is the Sine-Gordon equation identiﬁed along with its coeﬃcients.
Table XIII shows the identiﬁed equation by GEP. This test case demonstrates the usefulness
of GEP in identifying models with complex function composition and limitation of the
expressive and predictive power of the feature library in STRidge.

TABLE XIII. Sine-Gordon equation identiﬁed by GEP.

Recovered

Test error

True u2t = 1.00 u2x − 1.00 sin(u)
GEP u2t = 0.99 u2x − 0.99 sin(u) − 1.82 × 10−5 1.57 × 10−4

10101091081071061051041031021.000.750.500.250.000.250.500.751.00Coefficientsuu2xu2x2.01.51.00.50.00.51.01.52.0t0.00.20.40.60.81.0u(t,x)0.00.51.01.52.02.53.00.51.01.52.02.53.025

FIG. 26. Sine-Gordon equation in terms of ET identiﬁed by GEP.

IV. TRUNCATION ERROR ANALYSIS

This section deals with constructing a modiﬁed diﬀerential equation (MDE) for the Burg-
ers equation. We aim at demonstrating both GEP and STRidge techniques as SR tools in
the identiﬁcation of truncation errors resulting from an MDE of the Burgers nonlinear PDE.
MDEs provide valuable insights into discretization schemes along with their temporal and
spatial truncation errors. Initially, MDE analysis was developed to connect the stability of
nonlinear diﬀerence equations with the form of the truncation errors93. In continuation, the
symbolic form of MDEs were developed and a key insight was proposed that only the ﬁrst few
terms of the MDE dominate the properties of the numerical discretization94. These develop-
ments of MDE analysis lead to increasing accuracy by eliminating leading order truncation
error terms95, improving stability of schemes by adding artiﬁcial viscosity terms96, preserv-
ing symmetries97,98, and ultimately sparse identiﬁcation of truncation errors51. Therefore,
MDE analysis plays a prominent role in implicit large eddy simulations (ILES)99 as trunca-
tion errors are shown to have inherent turbulence modelling capabilities100. Discretization
schemes are tuned in the ILES approach as to model the subgrid scale tensor using trunca-
tion errors. As the construction of MDEs becomes cumbersome and intractable for complex
ﬂow conﬁgurations, data driven SR tools such as GEP and STRidge can be exploited for
the identiﬁcation of MDEs by observing the data.

TABLE XIV. GEP hyper-parameters selected for identiﬁcation of truncation error terms of MDEs.

Hyper-parameters

Burgere eq. (i) Burgers eq. (ii)

Head length

Number of genes

Population size

Generations

Length of RNC array

8

5

70

1000

20

8

4

70

1000

20

Random constant minimum

1.0 × 10−6

1.0 × 10−5

Random constant maximum

0.01

0.01

For demonstration purposes, we begin by constructing an MDE of the Burgers equation,

and discretizing Eq. (14) using ﬁrst order schemes (i.e., forward in time and backward in
space approximations for the spatial and temporal derivatives, respectively) and a second

ut + uux = νu2x,

(14)

+×−0.99+−sinu−u2x4××1078−563.99order accurate central diﬀerence approximation for the second order spatial derivatives.
The resulting discretized Burgers PDE is shown below,

26

up
j

up+1
j −
dt

+ up
j

up
j −

up
j
−
dx

1

up
j+1 −

= ν

2up
dx2

j + up

j

1

−

,

(15)

where temporal and spatial steps are given by dt and dx, respectively. In Eq. 15, the spatial
location is denoted using subscript index j and the temporal snapshot using superscript
index p.

To derive the modiﬁed diﬀerential equation (MDE) of the Burgers PDE, we substitute

the Taylor approximations for each term,

j = up
up+1

j + dt(ut)p

j +

dt2
2

(u2t)p

j +

dt3
6

(u3t)p

j + . . .

j+1 = up
up

j + dx((ux))p

j +

(u2x)p

j +

(16)

dx2
2
dx2
(u2x)p
2

j −

(u3x)p

dx3
6
dx3
(u3x)p
6

j + . . .

j + . . .






up
j
−

1 = up

j −

dx(ux)p

j +

When we substitute these approximations into Eq. 15, we obtain the Burgers MDE as
follows,

(ut + uux −
where R represents truncation error terms of the Burgers MDE given as,

j =

R,

−

νu2x)p

R =

dt
2

(u2t)p

j +

dx
2

(uux)p

j −

νdx2
12

(u4x)p

j + O(dt2, dx4).

(17)

(18)

Furthermore, temporal derivative in Eq. 18 is substituted with spatial derivatives resulting
in,

R = dtuu2

x −

dtνuxu2x −

dtνuu3x

dx
2

−

uu2x +

dt
2

u2u2x −

νdx2
12

u4x + O(dt2, dx4).

(19)

The truncation error or residual of discretized equation considering u(t, x) as exact solu-
tion to the Burgers PDE is equal to the diﬀerence between the numerical scheme (Eq. 15)
and diﬀerential equation (Eq. 14)101. This results in discretized equation with residual as
shown below,

up+1
j −

j + up
up

j dt

up
j −

up
j
−
dx

1

up
j+1 −

νdt

−

j + up

j

2up
dx2

1

= Rdt.

(20)

−

We follow the same methodology for constructing the output data and feature library as
discussed in Section II for the equation discovery. However, the output or target data V(t)
is stored with the left hand side of Eq. 20 denoted from now as Uer. The resulting output
and core feature library are shown below,

V(t) =

Uer

Θ(U) =

U Ux U2x U3x U4x

(cid:3)

(cid:2)

(cid:41)

.

(21)

The computation of the output data V(t) in Eq. 21 can be obtained using the analytical
Θ(U) are

solution of the Burgers PDE. Furthermore, the derivatives in core feature library

(cid:101)

(cid:2)

(cid:3)

(cid:101)

calculated using the ﬁnite diﬀerence approximations given by Eq. 4. We use both analytical
solutions listed in Table II for the Burgers equation (i) and the Burgers equation (ii) to test
GEP and STRidge for recovering truncation error terms.

27

We use the same extended feature library

Θ(U) as input to STRidge given in Eq. 7, but
without the ﬁfth order derivative. However, we add additional third degree interaction of
Θ(U) to recover the truncation error terms containing third degree nonlineari-
features to
(cid:101)
ties. The extra nonlinear features that are added to

Θ(U) are given below,

(cid:101)

[U2Ux U2U2x U2U3x U2U4x
UU2

(cid:101)
x UUxU2x UUxU3x UUxU4x].

Θ(U) as input as it identiﬁes the higher order non-
In contrast, GEP uses the core feature
linear feature interactions automatically. This test case shows the natural feature extraction
capability of GEP and need to modify the feature library to increase the expressive power
of STRidge.

(cid:101)

TABLE XV. GEP functional and terminal sets used for truncation error term recovery.
random constant.

‘?’

is a

Parameter

Function set

Terminal set

Value

+, −, ×

(cid:101)Θ(U), ?

Linking function

+

FIG. 27. Truncation error of the Burgers MDE using analytical solution of the Burgers equation
(i) in terms of ET identiﬁed by GEP.

The functional and terminal sets used for truncation error identiﬁcation are listed in
Table XV. First, we test the recovery of truncation errors using the analytical solution of the
Burgers equation (i) with the same spatial and temporal domain listed in Table II. However,
we set spatial discretization to be dx = 0.005 and temporal discretization to dt = 0.005 for
storing the analytical solution u(t, x). This test case needs a large population size, bigger
head length, more genes and more iterations as given in Table XIV, as the truncation error
terms consist of nonlinear combinations of features and the coeﬃcients of error terms that
are generally diﬃcult for GEP to identify. Fig. 27 shows the ET form of the identiﬁed
truncation error terms. The regularization weight λ for STRidge is swept across a range of
values as shown in Fig. 28. The vertical yellow line in Fig. 28 is the value of λ where STRidge
identiﬁes the best truncation error model. Table XVI shows the recovered error terms by
GEP and STRidge along with their coeﬃcients. Both GEP and STRidge perform well in
identifying the nonlinear spatial error terms with STRidge predicting the error coeﬃcient
better than GEP.

+×0.1+×−1.323×10−4×u2xu×−×2uxux×uu3x×u1.133×10−4×−5.092×10−6×u2xux×−3.423×10−6×u3xu×−1.383×10−6u4x−8.57×10−1728

FIG. 28. STRidge coeﬃcients as a function of regularization parameter λ for truncation error of
the Burgers MDE (i).

In the second case, we test the recovery of truncation errors using an analytical solution
of the Burgers eq. (ii) with the same spatial and temporal domain listed in Table II. We
select the spatial discretization dx = 0.005 and the temporal discretization dt = 0.1 for
propagating the analytical solution u(t, x). This test case also follows the previous case
where a large population size, bigger head length, more genes, and more iterations are
needed as shown in Table XIV. Fig. 29 shows the ET form of identiﬁed truncation error
terms. The regularization weight λ for STRidge is swept across a range of values as shown in
Fig. 30. In this test case, the coeﬃcients change rapidly in respect to λ, and the best model
is recovered only at the value of λ shown by the vertical yellow line in Fig. 30. Table XVII
shows the recovered error terms by GEP and STRidge along with their coeﬃcients. Similar
to the previous test case, STRidge predicts the truncation error coeﬃcients better than
GEP.

TABLE XVI. Identiﬁed truncation error terms along with coeﬃcients for the Burgers MDE (i) by
GEP and STRidge.

True

GEP

Relative error (%)

STRidge

Relative error (%)

uu2
x

uxu2x

uu3x
u2u2x

u4x

uu2x

2.5 × 10−5

2.26 × 10−5

−5.0 × 10−7

−5.09 × 10−7

−2.5 × 10−7

−3.42 × 10−7

1.25 × 10−5

1.25 × 10−9

1.13 × 10−5

1.38 × 10−9

−1.25 × 10−5

−1.33 × 10−5

9.6

1.8

36.8

9.6

10.4

6.4

2.48 × 10−5

−5.02 × 10−7

−2.29 × 10−7

1.22 × 10−5

1.16 × 10−9

−1.24 × 10−5

0.8

0.4

8.4

2.4

7.2

0.8

V. HIDDEN PHYSICS DISCOVERY

In this section, we demonstrate the identiﬁcation of hidden physical laws from sparse data
mimicking sensor measurements using GEP and STRidge. Furthermore, we demonstrate
the usefulness of GEP as a natural feature extractor that is capable of identifying complex
functional compositions. However, STRidge in its current form is limited by its expressive
power which depends on its input feature library. Many governing equations of complex
systems in the modern world are only partially known or in some cases still awaiting ﬁrst
principle equations. For example, atmospheric radiation models or chemical reaction models

10101091081071061051041031020.250.000.250.500.751.001.25Coefficientsx104u4xuu2xuu3xuxu2xu2u2xuu2x29

FIG. 29. Truncation error term of the Burgers MDE using analytical solution of the Burgers
equation (ii) in terms of ET identiﬁed by GEP.

FIG. 30. STRidge coeﬃcients as a function of regularization parameter λ for truncation error of
the Burgers MDE (ii).

TABLE XVII. Identiﬁed truncation error terms along with coeﬃcients for the Burgers MDE (ii)
by GEP and STRidge.

True

GEP

Relative error (%)

STRidge

Relative error (%)

uu2
x

uxu2x

uu3x
u2u2x

u4x

uu2x

1.0 × 10−2

8.19 × 10−3

−2.0 × 10−4

−2.64 × 10−4

−1.0 × 10−4

−1.55 × 10−4

5.0 × 10−3

5.0 × 10−7

4.21 × 10−3

5.65 × 10−7

−2.5 × 10−4

−2.75 × 10−4

18.1

32.0

55.0

15.8

13.0

10

9.92 × 10−3

−1.99 × 10−4

−9.91 × 10−5

5.08 × 10−3

4.94 × 10−7

−2.54 × 10−4

0.8

0.5

0.9

1.6

1.2

1.6

might be not fully known in governing equations of environmental systems102,103. These
unknown models are generally manifested in the right hand side of the known governing
equations (i.e., dynamical core) behaving as a source or forcing term. The recent explosion
of rapid data gathering using smart sensors104 has enabled researchers to collect data that
the true physics of complex systems but their governing equations are only known partially.
To this end, SR approaches might be able to recover these unknown physical models when

+×5.25×10−2+×−0.00504×u2xux×+×ux×1.950ux×uu2x×u0.0801×+u2x×u3x2.819×10−3×u−0.00558×1.1344×10−5u4x−2.71×10−091073×1084×1086×1083210123Coefficientsx102u4xuu2xuu3xuxu2xu2u2xuu2x30

exposed to data representing full physics.

To demonstrate the proof of concept for identiﬁcation of unknown physics, we formulate
a 1D advection-diﬀusion PDE and a 2D vortex-merger problem. These problems include
a source term that represents the hidden physical law. We generate synthetic data that
contains true physics and substitute this data set in to the known governing equations.
This results in an unknown physical model left as a residual that must be recovered by GEP
when exposed to a target or output containing the known part of the underlying processes.
Furthermore, both GEP and STRidge are tested to recover eddy viscosity kernels for the
2D Kraichnan turbulence problem. These eddy viscosity kernels are manifested as source
terms in the LES equations that model unresolved small scales. Additionally, the value of
the ad-hoc free modelling parameter that controls the dissipation in eddy viscosity models
is also recovered using GEP and STRidge.

TABLE XVIII. GEP hyper-parameters selected for identifying source terms for the 1D advection-
diﬀusion and the 2D vortex-merger problem.

Hyper-parameters

1D advection-diﬀusion eq.

2D vortex-merger problem

Head length

Number of genes

Population size

Generations

Length of RNC array

Random constant minimum

Random constant maximum

6

2

50

1000

5
π
4
π

A. 1D Advection-Diﬀusion PDE

5

3

50

500

8

−π

π

In the ﬁrst test case, we consider a 1D non-homogeneous advection-diﬀusion PDE which
appears in many areas such as ﬂuid dynamics105, heat transfer106, and mass transfer107.
The non-homogeneous PDE takes the form,

ut + cux = αu2x + S(t, x),

(22)

where c =

, α =

and S(t, x) is the source term.

1
3π

1
4

We use an analytical solution u(t, x) for solving Eq. 22. The exact solution for this

non-homogeneous PDE is as follows,

u(t, x) = exp

π2t
4

(cid:19)

(cid:18)

sin(πx),

(23)

where the spatial domain x
[0, 1]. We discretize the
space and time domains with n = 501 and m = 1001, respectively. Fig. 31 shows the
corresponding analytical solution u(t, x).

[0, 1] and the temporal domain t

∈

∈

The source term S(t, x), which satisﬁes Eq. 22 for the analytical solution provided by

Eq. 23, is given as,

S(t, x) =

π2
2

exp

π2t
4

(cid:19)

(cid:18)

sin(πx) +

1
3

exp

π2t
4

(cid:19)

(cid:18)

cos(πx).

(24)

Our goal is to recover this hidden source term once the solution u(t, x) is available ei-
ther by solving the analytical equation given by Eq. 23 or by sensor measurements in real

31

world applications. Furthermore, we select 64 random sparse spatial locations to mimic
experimental data collection. After the solution u(t, x) is stored at selected sparse spatial
locations, we follow the same procedure for constructing output data and feature building as
discussed in Section II. The corresponding output data V and feature library for recovering
source term using GEP are given as,

V =

Ut + cUx

Θ =

(cid:2)

x t

αU2x

−

.

(cid:41)
(cid:3)

(25)

(cid:2)

(cid:3)

(cid:101)

The derivatives in the output data V are calculated using Eq. 4. Hence, to calculate spatial
derivatives, we also store additional stencil data u(t, x) around the randomly selected sparse
locations (u)p
1. Table XIX gives the functional and terminal sets used by
GEP to recover the source term S(t, x) given in Eq. 24.

j i.e, (u)p

j+1 ,(u)p

−

j

FIG. 31. Solution to the 1D advection-diﬀusion PDE with source term.

TABLE XIX. GEP functional and terminal sets used for source term identiﬁcation. ‘?’ is a random
constant.

Parameter

Value

Function set

Terminal set

Linking function

+, −, ×, /, exp, sin, cos

(cid:101)Θ, ?

+

Table XVIII lists the hyper-parameters used by GEP for recovering source term of the
1D advection-diﬀusion equation. As the hidden physical law given in Eq. 24 consists of
complex functional compositions, GEP requires a larger head length, and more generations
are required by GEP for identiﬁcation. The ET form of the source term S(t, x) found by
GEP is shown in Fig. 32. The identiﬁed source term after simplifying the ET form found
by GEP is listed in Table XX. GEP was able to identify the source term S(t, x) given in
Eq. 24 from sparse data.

x0.00.20.40.60.81.0t0.00.20.40.60.81.0u(t,x)024681024681032

FIG. 32. Hidden source term of the 1D advection-diﬀusion PDE in terms of ET identiﬁed by GEP.

TABLE XX. Hidden source term (S) of the 1D advection-diﬀusion PDE identiﬁed by GEP.

Recovered

Test error

True

GEP

S = 4.93 exp(2.47 t) sin(3.14 x) + 0.33 exp(2.47 t) cos(3.14 x)

S = 4.93 exp(2.46 t) sin(3.14 x) + 0.33 exp(2.46 t) cos(3.14 x) − 3.12 × 10−5

3.34 × 10−7

B. 2D Vortex-Merger Problem

In this section, we demonstrate the recovery of hidden physical law from the data gen-
erated by solving the vortex-merger problem with source terms. The initial two vortices
merge to form a single vortex when they are located within a certain critical distance from
each other. This two-dimensional process is one of the fundamental processes of ﬂuid mo-
tion and it plays a key role in a variety of simulations, such as decaying two-dimensional
turbulence108,109 and mixing layers110. This phenomenon also occurs in other ﬁelds such as
astrophysics, meteorology, and geophysics111. The Vortex-merger problem is simulated by
using the 2D incompressible Navier-Stokes equations in the domain with periodic boundary
conditions.

We speciﬁcally solve the system of PDEs called vorticity-streamfunction formulation.
This system of PDEs contains the vorticity transport equation derived from taking the curl
of the 2D incompressible Navier-Stokes equations and the Poisson equation representing
the kinematic relationship between the streamfunction (ψ) and vorticity (ω). The resulting
vorticity-streamfunction formulation with source term is given as,

ωt + J(ω, ψ) =

2ψ =

∇

1
Re ∇
ω

−

2ω + S(t, x, y)




(26)

where the Reynolds number is set to Re = 2000. In Eq. 26, S(t, x, y) is the source term
ψxωy. We use the Cartesian domain
and J(ω, ψ) is the Jacobian term given as ψyωx −
(x, y)
128. The initial vorticity ﬁeld
consisting of a co-rotating vortex pair is generated using the superposition of two Gaussian-
distributed vortices given by,

[0, 2π] with a spatial resolution of 128

[0, 2π]

×

×

∈



ω(0, x, y) = Γ1exp

ρ

(x

−

−

(cid:0)

(cid:2)

x1)2 + (y

−

y1)2

(cid:3)(cid:1)+ Γ2exp

ρ

(x

−

−

x2)2 + (y

−

y2)2

,

(27)

(cid:0)

(cid:2)

(cid:3)(cid:1)

++××4.932sin×x3.141exp×t2.466××0.333cos×x3.141exp×t2.466−3.122×10−533

where the circulation Γ1 = Γ2 = 1, the interacting constant ρ = π and the intial vortex
centers are located near each other with coordinates (x1, y1) = ( 3π
4 , π).
We choose the source term S(t, x) as,

4 , π) and (x2, y2) = ( 5π

S(t, x, y) = Γ0sin(x)cos(y)exp

4π2
Re

−

t

,

(cid:19)

(cid:18)

(28)

where the magnitude of the source term is set to Γ0 = 0.01.

The vorticity ﬁeld ω and streamfunction ﬁeldψ are obtained by solving the Eq. 26 nu-
merically. We use a third-order Runge-Kutta scheme for the time integration, and a second
order Arakawa scheme112 for the discretization of the Jacobian term J(ω, ψ). As we have
a periodic domain, we use a fast Fourier transform (FFT) for solving the Poisson equation
in Eq. 26 to obtain the streamfunction at every time step. Numerical details for solving
the vortex-merger problem can be found in San et al110,113. We integrate the solution from
time t = 0 to t = 20 with a temporal step dt = 0.01.

FIG. 33. The 2D vortex-merger problem with source term at time t = 0.0 and t = 20.0. The
red markers shows 64 random sensor locations used to collect vorticity (ω) and streamfunction (ψ)
data for recovering source term S(t, x, y).

Fig. 33 shows the merging process of two vortices at the initial and ﬁnal times. The red
markers in Fig. 33 are 64 randomly selected sparse locations to collect both streamfunction
ψ and vorticity ω data. Once the streamfunction and vorticity data at sparse locations
Θ as discussed in
are available, we can construct the target data V and feature library
Section II. The resulting input-response data is given as,

V =

ωt + J (ω, ψ)

(cid:20)

Θ =

x y t

1
Re

−

∇2ω

.


(cid:21)


(cid:101)

(29)

(cid:2)

(cid:101)

The derivatives in the output data V(t) are calculated using ﬁnite diﬀerence approxi-
mations similar to Eq. 4. As streamfunction (ψ)p
i,j data are selected
i+1,j, (ψ)p
only at sparse spatial locations, we also store the surrounding stencil, i.e., (ψ)p
1,j,
(ψ)p
1 in order to calculate the deriva-
tives. The index i represents spatial location in x direction, and j represents spatial location
in y direction.


i,j and vorticity (ω)p

1, and (ω)p

i,j+1, (ψ)p

i,j+1, (ω)p

i+1,j, (ω)p

1,j, (ω)p

i,j

i,j

−

−

−

−

(cid:3)

i

i

01234560123456t=0.00123456t=20.00.080.000.080.160.240.320.400.480.560.6434

FIG. 34. Hidden source term of the 2D vortex-merger problem in terms of ET identiﬁed by GEP.

In this test case, we demonstrate the identiﬁcation of hidden physics which is the source
term S(t, x, y) given by Eq. 28 from the data obtained at sparse spatial locations using
GEP. Table XVIII lists the hyper-parameters used by GEP to recover the hidden physical
law. We use the same function and terminal sets as shown in Table XIX but
is used as a
linking function. Fig. 34 shows the ET form of hidden physical law (source term) obtained
by GEP. Simpliﬁcation of the ET form shows the identiﬁed source term which is close to
true source term as shown in Table XXI.

×

TABLE XXI. Hidden source term (S) of the 2D vortex-merger problem identiﬁed by GEP.

Recovered

Test error

True

GEP

S = 0.0100 sin(x) cos(y) exp(−0.078 t)

S = 0.0099 sin(x) cos(y) exp(−0.078 t) − 1.47 × 10−6

1.35 × 10−8

The 1D advection-diﬀusion and 2D vortex-merger problem demonstrate the usefulness of
GEP in recovering hidden physics, i.e., a source term that composed of complex functions
using randomly selected sparse data. The expressive power of the feature library limits the
applications of STRidge for identifying complex composition models. However, STRidge
might be able to identify the inﬁnite series approximations of these nonlinear functions39.
In the next test case, we use both STRdige and GEP to identify eddy viscosity kernels along
with their free modelling coeﬃcient that controls the dissipation of these kernels.

C. 2D Kraichnan Turbulence

The concept of two-dimensional turbulence helps in understanding many complex phys-
ical phenomenon such as geophysical and astrophysical ﬂows114,115. The equations of two-
dimensional turbulence can model idealized ﬂow conﬁgurations restricted to two-dimensions
such as ﬂows in rapidly rotating systems and in thin ﬁlms over rigid bodies. The physical
mechanism associated with the two-dimensional turbulence is explained by the Kraichnan-
Batchelor-Leith (KBL) theory116–118. Generally, large eddy simulation (LES) is performed
for both two and three dimensional ﬂows to avoid the ﬁne resolution and thereby com-
putational requirements of direct numerical simulation (DNS)119,120.
In LES, the ﬂow
variables are decomposed into resolved low wavenumber (or large scale) and unresolved
high wavenumber (or small scale). This is achieved by the application of a low pass spatial
ﬁlter to the ﬂow variables. By arresting high wavenumber content (small scales), we can

+×0.0099cosysinxexp××t0.0788−1−1.47×10−635

reduce the high resolution requirement of DNS, and hence faster simulations and reduced
storage requirements. However, the procedure of introducing a low pass ﬁltering results in
an unclosed term for the LES governing equations representing the ﬁner scale eﬀects in the
form of a source term.

Thus the quality of LES depends on the modeling approach used to close the spatial ﬁl-
tered governing equations to capture the eﬀects of the unresolved ﬁner scales121. This model
also called the subgrid scale model is a critical part of LES computations. A functional or
eddy viscosity approach is one of the popular approaches to model this closure term. These
approaches propose an artiﬁcial viscosity to mimic the dissipative eﬀect of the ﬁne scales.
Some of the popular functional models are the Smagorinsky122, Leith123, Balwin-Lomax124
and Cebeci-smith models125. All these models require the speciﬁcation of a model constant
that controls the quantity of dissipation in the simulation, and its value is often set based
on the nature of the particular ﬂow being simulated. In this section, we demonstrate the
identiﬁcation of an eddy viscosity kernel (model) along with its ad-hoc model constant from
observing the source term of the LES equation using both GEP and STRidge as robust SR
tools. To this end, we use the vorticity-streamfunction formulation for two-dimensional
ﬂuid ﬂows given in Eq. 26. We derive the LES equations for the two dimensional Kraichnan
turbulence by applying a low pass spatial ﬁlter to the vorticity-streamfunction PDE given
in Eq. 26. The resulting ﬁltered equation is given as,

ωt + J(ψ, ω) =

1
Re ∇

2ω,

(30)

where Re is the Reynolds number of the ﬂow and J(ω, ψ) is the Jacobian term given as
ψyωx −

ψxωy. Furthermore the Eq. 30 is rearranged as,

ωt + J(ψ, ω), =

1
Re ∇

2ω + Π,

(31)

where the LES source term Π is given as,

Π = J(ψ, ω)

J(ψ, ω).

(32)

−
The source term Π in Eq. 32 represents the inﬂuence of the subgrid scales on larger
resolved scales. The term J(ψ, ω) is not available, which necessitates the use of a clo-
sure modelling approach. In functional or eddy viscosity models, the source term of LES
equations is represented as,

Π = νe∇
where eddy viscosity νe is given by, but not limited to, the Smagorinsky, Leith, Baldwin-
Lomax, and Cebeci-Smith kernels. The choice of these eddy viscosity kernels essentially
implies the choice of a certain function of local ﬁeld variables such as the strain rate or
gradient of vorticity as a control parameter for the magnitude of νe.

(33)

2ω.

TABLE XXII. GEP functional and terminal sets used for identifying eddy viscosity kernel. ‘?’ is
a random constant.

Parameter

Function set

Terminal set

Linking function

Value

+, −, ×, /

(cid:101)Θ, ?

+

In Smagorisnky model, the eddy viscosity kernel is given by,

νe = (csδ)2

S
|

,
|

(34)

where cs is a free modelling constant that controls the magnitude of the dissipation and δ
is a characteristic grid length scale given by the square root of the product of the cell sizes
in each direction. The
is based on the second invariant of the ﬁltered ﬁeld deformation,
and given by,

S
|

|

36

xy + (ψ2x −
The Leith model proposes that eddy viscosity kernel is a function of vorticity and given

(cid:113)

=

S

ψ2y)2,

4ψ2

|

|

(35)

as,

νe = (csδ)3

ω

,

|

|∇

(36)

ω

where
controls the dissipative character of the eddy viscosity as against the resolved
strain rate used in the Smagorinsky model. The magnitude of the gradient of vorticity is
deﬁned as,

|∇

|

ω2

x + ω2
y.

ω

|

|∇

=

(cid:113)

(37)

TABLE XXIII. GEP hyper-parameters selected for identiﬁcation of the eddy viscosity kernel for
the Kraichnan turbulence.

Hyper-parameters

Kraichnan turbulence

Head length

Number of genes

Population size

Generations

Length of RNC array

Random constant minimum

Random constant maximum

2

2

20

500

3

−1

1

The Baldwin-Lomax is an alternative approach that models the eddy viscosity kernel as,

νe = (csδ)2

ω

,

(38)

|
where
is the absolute value of the vorticity considered as a measure of the local energy
content of the ﬂow at a grid point and also a measure of the dissipation required at that
location.

ω
|

|

|

The Cebeci-Smith model was devised for the Reynolds Averaged Navier-Stokes (RANS)

applications. The model is modiﬁed for LES setting, and is given as,

νe = (csδ)2

Ω
|

,
|

where

Ω
|

|

is given as,

=

Ω

|

|

ψ2

2x + ψ2

2y.

(cid:113)

(39)

(40)

High ﬁdelity DNS simulations are performed for Eq. 30. We use a square domain of
length 2π with periodic boundary conditions in both directions. We simulate homogeneous
isotropic decaying turbulence which may be speciﬁed by an initial energy spectrum that
decays through time. High ﬁdelity DNS simulations are carried out for Re = 4000 with
1024 resolution from time t = 0 to t = 4.0 with time step 0.001. The ﬁltered
1024

×

37

FIG. 35. Samgorisnsky kernel in terms of ET identiﬁed for the two-dimensional Kraichnan turbu-
lence problem by GEP.

TABLE XXIV. LES source term (Π) for two-dimensional Kraichnan turbulence problem identiﬁed
by GEP and STRidge.

Recovered

GEP

Π = 0.000128 |S| w2x + 0.000128 |S| w2y − 0.362

STRidge

Π = 0.000132 |S| w2x + 0.000129 |S| w2y

ﬂow quantities and LES source term Π in Eq. 32 are obtained from coarsening the DNS
quantities to obtain quantities with a 64
64 resolution. The further details of solver and
coarsening can be found in San and Staples109. Once the LES source term Π in Eq. 32 and
ﬁltered ﬂow quantities are obtained, we build the feature library and output data similar
to the discussion in Section II. The resulting input-response data is given as,

×

V =

Π

Θ =

(cid:2)

ω2x ω2y |S| |∇ω| |ω| |Ω|

(cid:3)

(cid:41)

.

(41)

GEP uses the output and feature library given in Eq. 41 to automatically extract the
best eddy viscosity kernel for decaying turbulence problems along with the model’s ad-hoc
coeﬃcient.

(cid:101)

(cid:2)

(cid:3)

FIG. 36. STRidge coeﬃcients as a function of regularization parameter λ for the two-dimensional
Kraichnan turbulence problem.

+×−0.0001276+×(cid:12)(cid:12)S(cid:12)(cid:12)wyy×(cid:12)(cid:12)S(cid:12)(cid:12)wxx−0.36271051031011011031052.01.51.00.50.00.51.0Coefficientsx10338

FIG. 37. Controur plots for the two-dimensional Kraichnan turbulence problem at t = 4. SR refers
to the identiﬁed model of the Smagorinsky kernel with cs = 0.12. UDNS and FDNS refer to the
no-model and ﬁltered DNS simulations, respectively.

The extended feature library is constructed to include nonlinear interactions up to the
quadratic degree to expand the expressive power for the STRidge algorithm. The resulting
extended feature library is given as,

Θ =

1 ω2x ω2

2x ω2y ω2xω2y ω2

2y . . . |Ω|2

.

(42)

(cid:2)

The function and terminal sets used for identiﬁcation of eddy viscosity kernel by GEP are
listed in Table XXII. Furthermore, the hyper-parameters of GEP are listed in Table XXIII.
Both GEP and STRidge identify the Smagorinsky kernel with approximately the same
coeﬃcients as shown in Table XXIV. The ET form of the Smagorinsky kernel found by
GEP is shown in Fig. 35. The regularization weight λ is varied to recover multiple models
of diﬀerent complexity as shown in Fig. 36. The yellow line in Fig. 36 corresponds to
the value of λ where STRidge identiﬁes the Smagorinsky kernel. We can take the average
coeﬃcient from both SR tools and derive the value of the free modelling constant identiﬁed
by SR approaches. The average model of both approaches is given by,

(cid:3)

Π = 0.000129 (

S
|

|

w2x +

S

|

|

w2y).

(43)

By comparing with Eq. 33 and Eq. 34 and using the spatial cell size δ = 2π
the free modelling constant is retrieved as cs = 0.12.

64 , the value of

The SR identiﬁed Smagorinsky kernel with cs = 0.12 is plugged into the LES source term
Π in Eq. 31 and a forward LES simulation is run for the 2D decaying turbulence problem.
Fig. 37 shows the vorticity ﬁelds at time t = 4.0 for the DNS, under-resolved no-model
simulation (UDNS), ﬁltered DNS (FDNS), and LES with SR retrieved Smagorinsky kernel.
Energy spectra at time t = 4.0 are showed in Fig. 38. We can observe that SR approaches
satisfactorily identify the value of the modelling constant cs, which controls reasonably well
the right amount of dissipation needed to account for the unresolved small scales. We
also highlight that several deep learning frameworks such as ANNs have been exploited
for subgrid scale modelling for 2D Kraichnan turbulence126–128. The importance of feature
selection can be seen in these works where diﬀerent invariant kernels, like those listed in the
feature library given in Eq. 41, are used as inputs to improve the ANN’s predictive perfor-
mance. The authors compared a posteriori results with diﬀerent free modelling coeﬃcients
of the Smagorinsky and Leith models. Furthermore, it is evident from the energy spectrum
comparisons in their studies that the appropriate addition of dissipation with the right tun-
ing of the free modelling coeﬃcient can lead to better predictions of the energy spectrum.

0123456x0123456yDNS0123456x0123456yUDNS0123456x0123456yFDNS0123456x0123456ySR161284048121616128404812161612840481216161284048121639

FIG. 38. Energy spectra for the two-dimensional Kraichnan turbulence problem at t = 4. SR refers
to the identiﬁed model of the Smagorinsky kernel with cs = 0.12. UDNS and FDNS refer to the
no-model and ﬁltered DNS simulations, respectively.

To this end, SR approaches automatically distill traditional models along with the right
values for the ad-hoc free modelling coeﬃcients. Although the present study establishes a
modular regression approach for discovering the relevant free parameters in LES models,
we highlight that it can be extended easily to a dynamic closure modelling framework re-
constructed automatically by sparse data on the ﬂy based on the ﬂow evolution, a topic we
would like to address in future.

VI. CONCLUSION

Data driven symbolic regression tools can be extremely useful for researchers for inferring
complex models from sensor data when the underlying physics is partially or completely
unknown. Sparse optimization techniques are envisioned as an SR tool that is capable of
recovering hidden physical laws in a highly eﬃcient computational manner. Popular sparse
optimization techniques such as LASSO, ridge, and elastic-net are also known as feature
selection methods in machine learning. These techniques are regularized variants of least
squares regression adapted to reduce overﬁtting and promote sparsity. The model predic-
tion ability of sparse regression methods is primarily dependent on the expressive power
of its feature library which contains exhaustive combinations of nonlinear basis functions
that might represent the unknown physical law. This limits the identiﬁcation of physical
models that are represented by complex functional compositions. GEP is an evolution-
ary optimization algorithm widely adapted for the SR approach. This genotype-phenotype
algorithm takes advantage of the simple chromosome representations of GA and the free
expansion of complex chromosomes of GP. GEP is a natural feature extractor that may not
need a priori information of nonlinear bases other than the basic features as a terminal set.
Generally, with enough computational time, GEP may recover unknown physical models
that are represented by complex functional compositions by observing the input-response
data.

In this paper, we demonstrate that the sparse regression technique STRidge and the evo-
lutionary optimization algorithm GEP are eﬀective SR tools for identifying hidden physical
laws from observed data. We ﬁrst identify various canonical PDEs using both STRidge
and GEP. We demonstrate that STRidge is limited by its feature library for identifying
the Sine-Gordon PDE. Following equation discovery, we demonstrate the power of both
algorithms in identifying the leading truncation error terms for the Burgers MDE. While
both algorithms ﬁnd the truncation terms, coeﬃcients found by STRidge were more ac-
curate than coeﬃcients found by GEP. We note that, when the feature library is capable
of expressing the underlying physical model, the application of STRidge is suitable due

100101102103k10121010108106104102100E(k)k3DNSUDNSFDNSSR40

to its fewer hyper-parameters and lower computational overhead. Next, we illustrate the
recovery of hidden physics that is supplied as the source or forcing term of a PDE. We use
randomly selected sparse measurements that mimic real world data collection. STRdige
is not applied in this setting as the feature library was limited to represent the unknown
physical model that consists of complex functional compositions. GEP was able to identify
the source term for both 1D advection-diﬀusion PDE and 2D vortex-merger problem using
sparse measurements. Finally, both STRdige and GEP were applied to discover the eddy
viscosity kernel along with its ad-hoc modelling coeﬃcient as a subgrid scale model for the
LES equations simulating the 2D Kraichnan turbulence problem. This particular example
demonstrates the capability of inverse modelling or parametric estimation for turbulence
closure models using SR approaches. Future studies will focus on identifying LES closure
models that augment the known closure models by accounting for the various nonlinear
physical process. Furthermore, various SR tools are being investigated for the identiﬁca-
tion of nonlinear truncation error terms of MDEs for implicit LES approaches that can be
exploited for modelling turbulent ﬂows without the need for explicit subgrid scale models.

ACKNOWLEDGMENTS

This material is based upon work supported by the U.S. Department of Energy, Of-
ﬁce of Science, Oﬃce of Advanced Scientiﬁc Computing Research under Award Number
de-sc0019290. O.S. gratefully acknowledges their support. Disclaimer: This report was
prepared as an account of work sponsored by an agency of the United States Government.
Neither the United States Government nor any agency thereof, nor any of their employees,
makes any warranty, express or implied, or assumes any legal liability or responsibility for
the accuracy, completeness, or usefulness of any information, apparatus, product, or process
disclosed, or represents that its use would not infringe privately owned rights. Reference
herein to any speciﬁc commercial product, process, or service by trade name, trademark,
manufacturer, or otherwise does not necessarily constitute or imply its endorsement, rec-
ommendation, or favoring by the United States Government or any agency thereof. The
views and opinions of authors expressed herein do not necessarily state or reﬂect those of
the United States Government or any agency thereof.

1M. I. Jordan and T. M. Mitchell, “Machine learning: Trends, perspectives, and prospects,” Science 349,
255–260 (2015).
2V. Marx, “Biology: The big challenges of big data,” Nature 498, 255–260 (2013).
3F. Rosenblatt, “The perceptron: a probabilistic model for information storage and organization in the
brain.” Psychological Review 65, 386 (1958).
4Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature 521, 436 (2015).
5E. J. Candes, M. B. Wakin, and S. P. Boyd, “Enhancing sparsity by reweighted (cid:96) 1 minimization,”
Journal of Fourier Analysis and Applications 14, 877–905 (2008).
6E. J. Candes and M. B. Wakin, “An introduction to compressive sampling,” IEEE Signal Processing
Magazine 25, 21–30 (2008).
7J. R. Koza, Genetic programming: on the programming of computers by means of natural selection,
Vol. 1 (MIT Press, Cambridge, MA, USA, 1992).
8C. Ferreira, “Gene expression programming: a new adaptive algorithm for solving problems,” arXiv
preprint cs/0102027 (2001).
9C. Ferreira, Gene expression programming: mathematical modeling by an artiﬁcial intelligence, Vol. 21
(Springer, 2006).

10M. Mitchell, An introduction to genetic algorithms (MIT press, 1998).
11J. H. Holland, “Adaptation in natural and artiﬁcial systems. 1975,” Ann Arbor, MI: University of

Michigan Press and (1992).

12M. Schmidt and H. Lipson, “Distilling free-form natural laws from experimental data,” Science 324,

81–85 (2009).

13J. Bongard and H. Lipson, “Automated reverse engineering of nonlinear dynamical systems,” Proceedings

of the National Academy of Sciences 104, 9943–9948 (2007).

14Y. Yang, C. Wang, and C. Soh, “Force identiﬁcation of dynamic systems using genetic programming,”

International Journal for Numerical Methods in Engineering 63, 1288–1312 (2005).

15L. Ferariu and A. Patelli, “Elite based multiobjective genetic programming for nonlinear system identi-
ﬁcation,” in International Conference on Adaptive and Natural Computing Algorithms (Springer, 2009)
pp. 233–242.

41

16C. Luo, Z. Hu, S.-L. Zhang, and Z. Jiang, “Adaptive space transformation: An invariant based method
for predicting aerodynamic coeﬃcients of hypersonic vehicles,” Engineering Applications of Artiﬁcial
Intelligence 46, 93–103 (2015).

17S. L. Brunton and B. R. Noack, “Closed-loop turbulence control: progress and challenges,” Applied

Mechanics Reviews 67, 050801 (2015).

18N. Gautier, J.-L. Aider, T. Duriez, B. Noack, M. Segond, and M. Abel, “Closed-loop separation control

using machine learning,” Journal of Fluid Mechanics 770, 442–457 (2015).

19T. Duriez, V. Parezanovi´c, K. von Krbek, J.-P. Bonnet, L. Cordier, B. R. Noack, M. Segond, M. Abel,
N. Gautier, J.-L. Aider, et al., “Feedback control of turbulent shear ﬂows by genetic programming,”
arXiv preprint arXiv:1505.01022 (2015).

20A. Debien, K. A. Von Krbek, N. Mazellier, T. Duriez, L. Cordier, B. R. Noack, M. W. Abel, and
A. Kourta, “Closed-loop separation control over a sharp edge ramp using genetic programming,” Exper-
iments in Fluids 57, 40 (2016).

21M. Quade, M. Abel, K. Shaﬁ, R. K. Niven, and B. R. Noack, “Prediction of dynamical systems by

symbolic regression,” Physical Review E 94, 012214 (2016).

22C. Luo and S.-L. Zhang, “Parse-matrix evolution for symbolic regression,” Engineering Applications of

Artiﬁcial Intelligence 25, 1182–1193 (2012).

23M. F. Brameier and W. Banzhaf, Linear genetic programming (Springe-Verlag, New York, 2007).
24R. S. Faradonbeh and M. Monjezi, “Prediction and minimization of blast-induced ground vibration using

two robust meta-heuristic algorithms,” Engineering with Computers 33, 835–851 (2017).

25R. S. Faradonbeh, A. Salimi, M. Monjezi, A. Ebrahimabadi, and C. Moormann, “Roadheader perfor-
mance prediction using genetic programming (GP) and gene expression programming (GEP) techniques,”
Environmental Earth Sciences 76, 584 (2017).

26F. S. Hoseinian, R. S. Faradonbeh, A. Abdollahzadeh, B. Rezai, and S. Soltani-Mohammadi, “Semi-
autogenous mill power model development using gene expression programming,” Powder Technology
308, 61–69 (2017).

27H. C¸ anakcı, A. Baykaso˘glu, and H. G¨ull¨u, “Prediction of compressive and tensile strength of Gaziantep
basalts via neural networks and gene expression programming,” Neural Computing and Applications 18,
1031 (2009).

28J. Weatheritt and R. Sandberg, “A novel evolutionary algorithm applied to algebraic modiﬁcations of

the rans stress–strain relationship,” Journal of Computational Physics 325, 22–37 (2016).

29M. Schoepplein, J. Weatheritt, R. Sandberg, M. Talei, and M. Klein, “Application of an evolutionary
algorithm to les modelling of turbulent transport in premixed ﬂames,” Journal of Computational Physics
374, 1166–1179 (2018).

30J. Weatheritt and R. D. Sandberg, “Hybrid reynolds-averaged/large-eddy simulation methodology from

symbolic regression: formulation and application,” AIAA Journal , 5577 – 5584 (2017).

31H. Rauhut, Compressive sensing and structured random matrices, Vol. 9 (Walter de Gruyter GmbH &

Co. KG, Berlin, 2010) pp. 1–92.

32R. Tibshirani, “Regression shrinkage and selection via the LASSO,” Journal of the Royal Statistical

Society: Series B 58, 267–288 (1996).

33G. James, D. Witten, T. Hastie, and R. Tibshirani, An introduction to statistical learning, Vol. 112

(Springer Science+Business Media, New York, 2013).

34R. Tibshirani, M. Wainwright, and T. Hastie, Statistical learning with sparsity: the LASSO and gener-

alizations (Chapman and Hall/CRC, Florida, USA, 2015).

35E. J. Candes, J. K. Romberg, and T. Tao, “Stable signal recovery from incomplete and inaccurate
measurements,” Communications on Pure and Applied Mathematics: A Journal Issued by the Courant
Institute of Mathematical Sciences 59, 1207–1223 (2006).

36K. P. Murphy, Machine learning: a probabilistic perspective (MIT Press, Cambridge, MA, USA, 2012).
37H. Zou and T. Hastie, “Regularization and variable selection via the elastic net,” Journal of the Royal

Statistical Society: Series B (Statistical Methodology) 67, 301–320 (2005).

38J. Friedman, T. Hastie, and R. Tibshirani, “Regularization paths for generalized linear models via

coordinate descent,” Journal of Statistical Software 33, 1–22 (2010).

39S. L. Brunton, J. L. Proctor, and J. N. Kutz, “Discovering governing equations from data by sparse
identiﬁcation of nonlinear dynamical systems,” Proceedings of the National Academy of Sciences 113,
3932–3937 (2016).

40S. H. Rudy, S. L. Brunton, J. L. Proctor, and J. N. Kutz, “Data-driven discovery of partial diﬀerential

equations,” Science Advances 3, e1602614 (2017).

41H. Schaeﬀer, R. Caﬂisch, C. D. Hauck, and S. Osher, “Sparse dynamics for partial diﬀerential equations,”

Proceedings of the National Academy of Sciences 110, 6634–6639 (2013).

42H. Schaeﬀer, “Learning partial diﬀerential equations via data discovery and sparse optimization,” Pro-
ceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences 473, 20160446 (2017).
43G. Tran and R. Ward, “Exact recovery of chaotic systems from highly corrupted data,” Multiscale

Modeling & Simulation 15, 1108–1129 (2017).

44H. Schaeﬀer, G. Tran, and R. Ward, “Extracting sparse high-dimensional dynamics from limited data,”

SIAM Journal on Applied Mathematics 78, 3279–3295 (2018).

45N. M. Mangan, J. N. Kutz, S. L. Brunton, and J. L. Proctor, “Model selection for dynamical systems via
sparse regression and information criteria,” Proceedings of the Royal Society A: Mathematical, Physical

42

and Engineering Sciences 473, 20170009 (2017).

46N. M. Mangan, S. L. Brunton, J. L. Proctor, and J. N. Kutz, “Inferring biological networks by sparse
identiﬁcation of nonlinear dynamics,” IEEE Transactions on Molecular, Biological and Multi-Scale Com-
munications 2, 52–63 (2016).

47J.-C. Loiseau, B. R. Noack, and S. L. Brunton, “Sparse reduced-order modelling: sensor-based dynamics

to full-state estimation,” Journal of Fluid Mechanics 844, 459–490 (2018).

48M. Schmelzer, R. Dwight, and P. Cinnella, “Data-driven deterministic symbolic regression of nonlin-
ear stress-strain relation for rans turbulence modelling,” in 2018 Fluid Dynamics Conference (AIAA
Aviation Forum, 2018) p. 2900.

49P. Zheng, T. Askham, S. L. Brunton, J. N. Kutz, and A. Y. Aravkin, “A uniﬁed framework for sparse

relaxed regularized regression: SR3,” IEEE Access 7, 1404–1423 (2018).

50T. Zhang, “Adaptive forward-backward greedy algorithm for sparse learning with linear models,” in Ad-
vances in Neural Information Processing Systems (Neural Information Processing Systems Foundation,
Inc., 2009) pp. 1921–1928.

51S. Thaler, L. Paehler, and N. A. Adams, “Sparse identiﬁcation of truncation errors,” Journal of Com-

putational Physics 397, 108851 (2019).

52T. McConaghy, “FFX: Fast, scalable, deterministic symbolic regression technology,” in Genetic Pro-

gramming Theory and Practice IX (Springer, 2011) pp. 235–260.

53H. Vaddireddy and O. San, “Equation discovery using fast function extraction: a deterministic symbolic

regression approach,” Fluids 4, 111 (2019).

54M. Schmelzer, R. P. Dwight, and P. Cinnella, “Machine learning of algebraic stress models using deter-

ministic symbolic regression,” arXiv preprint arXiv:1905.07510 (2019).

55C. Chen, C. Luo, and Z. Jiang, “Elite bases regression: a real-time algorithm for symbolic regression,” in
2017 13th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery
(ICNC-FSKD) (IEEE, 2017) pp. 529–535.

56T. Worm and K. Chiu, “Prioritized grammar enumeration: symbolic regression by dynamic program-
ming,” in Proceedings of the 15th Annual Conference on Genetic and Evolutionary Computation (ACM,
2013) pp. 1021–1028.

57D. Ciregan, U. Meier, and J. Schmidhuber, “Multi-column deep neural networks for image classiﬁcation,”

in 2012 IEEE Conference on Computer Vision and Pattern Recognition (2012) pp. 3642–3649.

58A. Karpathy and L. Fei-Fei, “Deep visual-semantic alignments for generating image descriptions,” in
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2015) pp. 3128–
3137.

59A. E. Sallab, M. Abdou, E. Perot, and S. Yogamani, “Deep reinforcement learning framework for

autonomous driving,” Electronic Imaging 2017, 70–76 (2017).

60M. Raissi, P. Perdikaris, and G. E. Karniadakis, “Physics-informed neural networks: A deep learning
framework for solving forward and inverse problems involving nonlinear partial diﬀerential equations,”
Journal of Computational Physics 378, 686–707 (2019).

61M. Raissi, P. Perdikaris, and G. E. Karniadakis, “Numerical gaussian processes for time-dependent and
nonlinear partial diﬀerential equations,” SIAM Journal on Scientiﬁc Computing 40, A172–A198 (2018).
62M. Raissi and G. E. Karniadakis, “Hidden physics models: Machine learning of nonlinear partial diﬀer-

ential equations,” Journal of Computational Physics 357, 125–141 (2018).

63J. Kocijan, A. Girard, B. Banko, and R. Murray-Smith, “Dynamic systems identiﬁcation with Gaussian

processes,” Mathematical and Computer Modelling of Dynamical Systems 11, 411–424 (2005).

64G. Gregorˇciˇc and G. Lightbody, “Nonlinear system identiﬁcation: From multiple-model networks to

Gaussian processes,” Engineering Applications of Artiﬁcial Intelligence 21, 1035–1055 (2008).

65L. Cordier, B. R. Noack, G. Tissot, G. Lehnasch, J. Delville, M. Balajewicz, G. Daviller, and R. K.

Niven, “Identiﬁcation strategies for model-based control,” Experiments in Fluids 54, 1580 (2013).

66Z. Wang, D. Xiao, F. Fang, R. Govindan, C. C. Pain, and Y. Guo, “Model identiﬁcation of reduced order
ﬂuid dynamics systems using deep learning,” International Journal for Numerical Methods in Fluids 86,
255–268 (2018).

67J.-F. Cai, B. Dong, S. Osher, and Z. Shen, “Image restoration: total variation, wavelet frames, and

beyond,” Journal of the American Mathematical Society 25, 1033–1089 (2012).

68B. Dong, Q. Jiang, and Z. Shen, “Image restoration: Wavelet frame shrinkage, nonlinear evolution

PDEs, and beyond,” Multiscale Modeling & Simulation 15, 606–660 (2017).

69Z. Long, Y. Lu, X. Ma, and B. Dong, “PDE-net: Learning PDEs from data,” in Proceedings of the 35th
International Conference on Machine Learning, Proceedings of Machine Learning Research, Vol. 80,
edited by J. Dy and A. Krause (PMLR, Stockholmsmssan, Stockholm Sweden, 2018) pp. 3208–3216.
70Z. Long, Y. Lu, and B. Dong, “PDE-Net 2.0: Learning PDEs from data with a numeric-symbolic hybrid

deep network,” Journal of Computational Physics 399, 108925 (2019).

71C. Ferreira, “Gene expression programming in problem solving,” in Soft Computing and Industry

(Springer, 2002) pp. 635–653.

72G. Shuhua, “geppy: a gene expression programming framework in python,” https://github.com/

ShuhuaGao/geppy (2019).

73F.-A. Fortin, F.-M. De Rainville, M.-A. Gardner, M. Parizeau, and C. Gagn´e, “DEAP: Evolutionary

algorithms made easy,” Journal of Machine Learning Research 13, 2171–2175 (2012).

74R. G. Baraniuk, “Compressive sensing,” IEEE Signal Processing Magazine 24, 118–124 (2007).

43

75H. Bateman, “Some recent researches on the motion of ﬂuids,” Monthly Weather Review 43, 163–170

(1915).

76G. B. Whitham, Linear and nonlinear waves, Vol. 42 (John Wiley & Sons, 2011).
77M. Maleewong and S. Sirisup, “On-line and Oﬀ-line POD assisted projective integral for non-linear
problems: A case study with Burgers equation,” International Journal of Mathematical, Computational,
Physical, Electrical and Computer Engineering 5, 984–992 (2011).

78D. J. Korteweg and G. de Vries, “On the change of form of long waves advancing in a rectangular canal,

and on a new type of long stationary waves,” Philosophical Magazine 39, 422–443 (1895).

79L. Wazzan, “A modiﬁed tanh–coth method for solving the KdV and the KdV–Burgers equations,”

Communications in Nonlinear Science and Numerical Simulation 14, 443–450 (2009).

80T. Ozis and S. Ozer, “A simple similarity-transformation-iterative scheme applied to Korteweg–de Vries

equation,” Applied Mathematics and Computation 173, 19–32 (2006).

81G. L. Lamb Jr, Elements of soliton theory (Wiley-Interscience, New York, 1980).
82T. Kawahara, “Oscillatory solitary waves in dispersive media,” Journal of the Physical Society of Japan

33, 260–264 (1972).

83T. Kawahara, N. Sugimoto, and T. Kakutani, “Nonlinear interaction between short and long capillary-

gravity waves,” Journal of the Physical Society of Japan 39, 1379–1386 (1975).

84J. K. Hunter and J. Scheurle, “Existence of perturbed solitary wave solutions to a model equation for

water waves,” Physica D: Nonlinear Phenomena 32, 253–268 (1988).

85Sirendaoreji, “New exact travelling wave solutions for the Kawahara and modiﬁed Kawahara equations,”

Chaos Solitons & Fractals 19, 147–150 (2004).

86C. Zhi-Xiong and G. Ben-Yu, “Analytic solutions of the nagumo equation,” IMA Journal of Applied

Mathematics 48, 107–115 (1992).

87J. Nagumo, S. Arimoto, and S. Yoshizawa, “An active pulse transmission line simulating nerve axon,”

Proceedings of the IRE 50, 2061–2070 (1962).

88D. G. Aronson and H. F. Weinberger, “Multidimensional nonlinear diﬀusion arising in population ge-

netics,” Advances in Mathematics 30, 33–76 (1978).

89A. Scott, “Neuristor propagation on a tunnel diode loaded transmission line,” Proceedings of the IEEE

51, 240–240 (1963).

90M. Dehghan and F. Fakhar-Izadi, “Pseudospectral methods for nagumo equation,” International Journal

for Numerical Methods in Biomedical Engineering 27, 553–561 (2011).

91A. Barone, F. Esposito, C. Magee, and A. Scott, “Theory and applications of the sine-gordon equation,”

La Rivista del Nuovo Cimento (1971-1977) 1, 227–267 (1971).

92J. Perring and T. Skyrme, “A model uniﬁed ﬁeld equation,” Nuclear Physics 31, 550–555 (1962).
93C. W. Hirt, “Heuristic stability theory for ﬁnite-diﬀerence equations,” Journal of Computational Physics

2, 339–355 (1968).

94R. D. Ritchmyer and K. Norton, Diﬀerence methods for initial value problems (Jonh Wiley & Sons, New

York, 1967).

95G. Klopfer and D. S. McRae, “Nonlinear truncation error analysis of ﬁnite diﬀerence schemes forthe

euler equations,” AIAA Journal 21, 487–494 (1983).

96A. Majda and S. Osher, “A systematic approach for correcting nonlinear instabilities,” Numerische

Mathematik 30, 429–452 (1978).

97E. Ozbenli and P. Vedula, “Numerical solution of modiﬁed diﬀerential equations based on symmetry

preservation,” Physical Review E 96, 063304 (2017).

98E. Ozbenli and P. Vedula, “High order accurate ﬁnite diﬀerence schemes based on symmetry preserva-

tion,” Journal of Computational Physics 349, 376–398 (2017).

99N. Adams, S. Hickel, and S. Franz, “Implicit subgrid-scale modeling by adaptive deconvolution,” Journal

of Computational Physics 200, 412–431 (2004).

100L. G. Margolin and W. J. Rider, “A rationale for implicit turbulence modelling,” International Journal

for Numerical Methods in Fluids 39, 821–841 (2002).

101C. Hirsch, Numerical computation of internal and external ﬂows: The fundamentals of computational

ﬂuid dynamics (Elsevier, Burlington, MA, 2007).

102V. M. Krasnopolsky and M. S. Fox-Rabinovitz, “Complex hybrid models combining deterministic and
machine learning components for numerical climate modeling and weather prediction,” Neural Networks
19, 122–134 (2006).

103V. M. Krasnopolsky and M. S. Fox-Rabinovitz, “A new synergetic paradigm in environmental numer-
ical modeling: Hybrid models combining deterministic and machine learning components,” Ecological
Modelling 191, 5–18 (2006).

104S. Dhingra, R. B. Madda, A. H. Gandomi, R. Patan, and M. Daneshmand, “Internet of things mobile-air
pollution monitoring system (IoT-Mobair),” IEEE Internet of Things Journal 6, 5577 – 5584 (2019).
105N. Kumar, “Unsteady ﬂow against dispersion in ﬁnite porous media,” Journal of Hydrology 63, 345–358

(1983).

106J. Isenberg and C. Gutﬁnger, “Heat transfer to a draining ﬁlm,” International Journal of Heat and Mass

Transfer 16, 505–512 (1973).

107V. Guvanasen and R. Volker, “Numerical solutions for solute transport in unconﬁned aquifers,” Inter-

national Journal for Numerical Methods in Fluids 3, 103–123 (1983).

108P. Meunier, S. Le Diz`es, and T. Leweke, “Physics of vortex merging,” Comptes Rendus Physique 6,

44

431–450 (2005).

109O. San and A. E. Staples, “High-order methods for decaying two-dimensional homogeneous isotropic

turbulence,” Computers & Fluids 63, 105–127 (2012).

110O. San and A. E. Staples, “A coarse-grid projection method for accelerating incompressible ﬂow com-

putations,” Journal of Computational Physics 233, 480–508 (2013).

111J. N. Reinaud and D. G. Dritschel, “The critical merger distance between two co-rotating quasi-

geostrophic vortices,” Journal of Fluid Mechanics 522, 357–381 (2005).

112A. Arakawa, “Computational design for long-term numerical integration of the equations of ﬂuid motion:
Two-dimensional incompressible ﬂow. part i,” Journal of Computational Physics 1, 119–143 (1966).
113S. Pawar and O. San, “CFD Julia: A learning module structuring an introductory course on computa-

tional ﬂuid dynamics,” Fluids 4, 159 (2019).

114G. Boﬀetta and S. Musacchio, “Evidence for the double cascade scenario in two-dimensional turbulence,”

Physical Review E 82, 016307 (2010).

115G. Boﬀetta and R. E. Ecke, “Two-dimensional turbulence,” Annual Review of Fluid Mechanics 44,

427–451 (2012).

116R. H. Kraichnan, “Inertial ranges in two-dimensional turbulence,” The Physics of Fluids 10, 1417–1423

(1967).

117G. K. Batchelor, “Computation of the energy spectrum in homogeneous two-dimensional turbulence,”

The Physics of Fluids 12, II–233 (1969).

118C. Leith, “Atmospheric predictability and two-dimensional turbulence,” Journal of the Atmospheric

Sciences 28, 145–161 (1971).

119U. Piomelli, “Large-eddy simulation: achievements and challenges,” Progress in Aerospace Sciences 35,

335–362 (1999).

120C. Meneveau and J. Katz, “Scale-invariance and turbulence models for large-eddy simulation,” Annual

Review of Fluid Mechanics 32, 1–32 (2000).

121P. Sagaut, Large eddy simulation for incompressible ﬂows: an introduction (Springer Science & Business

Media, 2006).

122J. Smagorinsky, “General circulation experiments with the primitive equations: I. the basic experiment,”

Monthly Weather Review 91, 99–164 (1963).

123C. E. Leith, “Diﬀusion approximation for two-dimensional turbulence,” The Physics of Fluids 11, 671–

672 (1968).

124B. Baldwin and H. Lomax, “Thin-layer approximation and algebraic model for separated turbulentﬂows,”

in 16th aerospace sciences meeting (AIAA Meeting Paper, 1978) p. 257.

125A. Smith and T. Cebeci, “Numerical solution of the turbulent-boundary-layer equations,” Tech. Rep.

DAC 33735 (DTIC, 1967).

126R. Maulik, O. San, A. Rasheed, and P. Vedula, “Subgrid modelling for two-dimensional turbulence

using neural networks,” Journal of Fluid Mechanics 858, 122–144 (2019).

127R. Maulik, O. San, A. Rasheed, and P. Vedula, “Data-driven deconvolution for large eddy simulations

of kraichnan turbulence,” Physics of Fluids 30, 125109 (2018).

128R. Maulik, O. San, J. D. Jacob, and C. Crick, “Sub-grid scale model classiﬁcation and blending through

deep learning,” Journal of Fluid Mechanics 870, 784–812 (2019).

