2
2
0
2

b
e
F
8
2

]
P
S
.
s
s
e
e
[

1
v
2
5
6
3
1
.
2
0
2
2
:
v
i
X
r
a

Hierarchical Multi-Agent DRL-Based Framework
for Joint Multi-RAT Assignment and Dynamic
Resource Allocation in Next-Generation HetNets

Abdulmalik Alwarafy1, Bekir Sait Ciftler1, Member, IEEE, Mohamed Abdallah1, Senior Member, IEEE,
Mounir Hamdi1, Fellow Member, IEEE, and Naofal Al-Dhahir2, Fellow Member, IEEE
1Division of Information and Computing Technology, College of Science and Engineering,
Hamad Bin Khalifa University, Doha, Qatar
2Electrical and Computer Engineering Department, Erik Jonsson School of Engineering and Computer Science,
The University of Texas at Dallas, USA

Abstract—This paper considers the problem of cost-aware
downlink sum-rate maximization via joint optimal radio access
technologies (RATs) assignment and power allocation in next-
generation heterogeneous wireless networks (HetNets). We con-
sider a future HetNet comprised of multi-RATs and serving multi-
connectivity edge devices (EDs), and we formulate the problem as
a mixed-integer non-linear programming (MINP) problem. Due
to the high complexity and combinatorial nature of this problem
and the difﬁculty to solve it using conventional methods, we
propose a hierarchical multi-agent deep reinforcement learning
(DRL)-based framework, called DeepRAT, to solve it efﬁciently
and learn system dynamics. In particular, the DeepRAT frame-
work decomposes the problem into two main stages; the RATs-
EDs assignment stage, which implements a single-agent Deep Q
Network (DQN) algorithm, and the power allocation stage, which
utilizes a multi-agent Deep Deterministic Policy Gradient (DDPG)
algorithm. Using simulations, we demonstrate how the various
DRL agents efﬁciently interact to learn system dynamics and
derive the global optimal policy. Furthermore, our simulation
results show that the proposed DeepRAT algorithm outperforms
existing state-of-the-art heuristic approaches in terms of network
utility. Finally, we quantitatively show the ability of the DeepRAT
model to quickly and dynamically adapt to abrupt changes in
network dynamics, such as EDs’ mobility.

Index Terms—Deep Reinforcement Learning, Deep Q Network,
Deep Deterministic Policy Gradient, Resource Allocation, Multi-
RAT Assignment, Power Allocation, Heterogeneous Networks.
I. INTRODUCTION
Heterogeneous wireless networks (HetNets) are expected
to be one of the key enablers for next-generation wireless
communication networks [2]. In such networks, a massive
number of multi-radio access technologies (multi-RATs) in the
licensed and unlicensed frequency bands across the ground,
space, and underwater coexist to enhance the network’s quality
of service (QoS). The main goal of next-generation HetNets
is to support the stringent QoS requirements of the emerging
disruptive wireless applications in terms of rate, coverage, and
reliability with ubiquitous connectivity. On the other hand,
emerging user edge devices (EDs) are equipped with advanced
multi-access physical capabilities that enable them to simul-
taneously aggregate radio resources from various RATs (i.e.,

A conference version of this work was published in the IEEE International
Conference on Communications Workshops (ICC Workshops) 2021 Proceed-
ings [1].

multi-homing mode of operation) to guarantee an enhanced
reliable communication for their running applications [3]. The
number of such EDs is expected to be around 30 Billion by
2023, including smartphones, IoT devices, and sensors [4].

Radio resource allocation is crucial in the planning, orches-
tration, and resource optimization of next-generation HetNets.
It is mainly used to guarantee enhanced system efﬁciency,
increased network connectivity, and reduced energy con-
sumption. However, allocating and managing radio resources,
such as power, spectrum, and rate,
in the next-generation
HetNets is a persistent challenge. In particular, the multi-
RAT assignment for EDs (i.e., RATs-EDs associations) and
RATs’ power allocation are among the key issues in the
emerging HeNets. Various methods have been proposed to
address these two radio resource allocation issues, such as
optimization theory, ranking-based, and game theory [5]–
[7]. However, most of these conventional resource allocation
methods generally suffer from the following shortcomings.
They require full and real-time knowledge of the network
dynamics. Unfortunately, obtaining such information is not
possible as most real-world networks are dynamic and change
over time causing a rapid variation of the wireless channel.
In addition, most of these conventional resource allocation
methods suffer from high computational complexity, lack of
scalability, and do not always guarantee convergence. These
issues are even exacerbated in next-generation HetNets due
to the large-scale and massive heterogeneous nature of these
networks in terms of the underlying RATs and QoS demands
of supported applications as well as the explosive number and
type of emerging EDs. All of the above mentioned issues
and challenges render the use of existing resource allocation
techniques for future wireless HetNets quite difﬁcult if not
even impossible [8]. Hence, it is of a paramount importance
to develop alternative resource allocation solutions that can
overcome these challenges while quickly adapting to the
varying systems’ dynamics.

Deep reinforcement learning (DRL) has emerged recently as
one of the most promising branches of the artiﬁcial intelligence
(AI) ﬁeld. In DRL, intelligent agents are trained to make
autonomous decisions and observe their results in order to

 
 
 
 
 
 
techniques. First,

learn optimal control policies. In the context of radio resource
allocation, DRL methods possess several advantages over
they provide autonomous
state-of-the-art
and real-time decision-making even for highly complex and
large-scale HetNets. Second, DRL provides efﬁcient solutions
for complex and high-dimensional wireless radio resource
allocation optimization problems with limited channel state
information (CSI) knowledge. These unique features make
DRL techniques one of the key enabling technologies that can
be utilized to address the radio resource allocation in next-
generation HetNets.

In light of the imperative need for radio resource allocation
in next-generation HetNets, the shortcomings of traditional
radio resource allocation approaches, and the efﬁciency of
DRL techniques in solving complex radio resource alloca-
tion optimization problems, this paper proposes a DRL-based
framework for radio resource allocation in next-generation
HetNets. In particular, we propose a hierarchical DQN and
DDPG-based scheme called DeepRAT to study the problem
of adaptive multi-RAT assignment and continuous downlink
power allocation for multi-homing EDs in next-generation
HetNets. Our aim is to jointly optimize the sum rate of the
entire network, monetary cost, and power allocation while
satisfying the QoS demands of EDs and the constraints of
the RATs’ power resources. Our simulation results demon-
strate that the DeepRAT algorithm can efﬁciently learn the
optimal policy, and it outperforms the greedy, random, and
ﬁxed algorithms in terms of satisfying the objective of the
optimization problem. Moreover, after training, our proposed
algorithm converges 2.5 times faster than the case with the
initial training. In general, the main contributions of this paper
are summarized as follows:

• We formulate an optimization problem whose objective is
to cost-effectively maximize the downlink sum-rate of the
multi-RAT HetNet via jointly optimizing the RATs-EDs
assignment and RATs’ power allocations while consid-
ering the limited RATs’ power resources, multi-homing
capabilities of EDs, and QoS data requirements of EDs.
• Due to the extensive computational complexity and com-
binatorial nature of the formulated problem, as well as the
difﬁculty of applying conventional approaches to solve
it, we propose a DRL-based algorithm called DeepRAT
to solve the problem hierarchically and learn the system
dynamics using a mix of value-based and policy-based
DRL algorithms.

• Using simulations, we show how the various agents of the
DeepRAT model interact in order to learn the global opti-
mal policy and solve our proposed optimization problem,
relying only on limited information about the network
dynamics and CSI.

• We demonstrate quantitatively that our proposed algo-
rithm outperforms existing heuristic-based methods in
terms of satisfying the objective of the optimization
problem. Also, we show how the DeepRAT algorithm can
quickly adapt to the abrupt changes in network dynamics,

such as EDs’ mobility.

The rest of this paper is organized as follows. Table I deﬁnes
the main acronyms used in this paper. Section II describes
some related work that implements DRL methods for radio
resource allocation. Section III presents the proposed system
model and formulates the optimization problem. Section IV
shows the architecture of the proposed DeepRAT framework
and explains its underlying DRL-based models along with
a brief mathematical background. Section V explains the
simulation setup and discusses the corresponding numerical
results. Finally, Section VI concludes the paper.

II. RELATED WORK

DRL-based techniques have attracted considerable research
lately in the context of radio resource allocation for wireless
networks [6], [8]. The authors in [9] proposed DRL models
based on the single and multi-agent actor-critic algorithms to
address the problem of total sum-rate maximization via power
allocation for cellular networks. In [10], the authors used DRL
methods to study the joint optimization of user association
and power allocation in orthogonal frequency division multiple
access (OFDMA)-based HetNets. Ye et al. [11] presented
a DRL-based mechanism to study the problem of resource
allocation for unicast and broadcast scenarios in vehicle-to-
vehicle (V2V) networks. The work in [12] investigated the
power control problem of device-to-device (D2D)-enabled
networks in time-varying environments using a centralized
DRL algorithm. Zhang et al. [13] presented a DRL algorithm
to study the problem of energy-efﬁcient resource allocation in
ultra-dense cellular networks. The authors in [14] presented a
multi-agent DQN-based model to study the problem of joint
power, bandwidth, and throughput allocation in unmanned
aerial vehicle (UAV)-assisted IoT networks. In [3], the authors
proposed a non-cooperative multi-agent DQN-based method to
study the problem of power allocation in hybrid RF/VLC net-
works. The authors show via simulation that the convergence
rate of the DQN-based model is 96.1% compared to that of
the Q-learning-based algorithm, which is 72.3%.

On the other hand, using deep deterministic policy gradient
(DDPG) models has also gained increasing interest recently.
They have shown superior performance in addressing the
radio resource allocation problems in continuous and high
dimensionality environments compared to the vanilla DQN
algorithms [15]. The authors in [16] presented a comparative
study for the applications of three DRL algorithms, namely the
DDPG, Neural Episodic Control (NEC), and Variance Based
Control, in the optimization of wireless networks. The authors
concluded that the DDPG and VBC methods achieve better
performance than the NEC-based algorithm. In [7], the authors
presented a single-agent DDPG algorithm to address the
problem of network selection in heterogeneous health systems.
Their goal was to optimize the medical data delivery from
Patient Edge Nodes (PENs) via multi-radio access networks
(RANs) to the core network. Nasir et al. [17] presented a
multi-agent DDPG-based algorithm to study the problem of
joint power and spectrum allocation in wireless networks.

Acronym
RAT
MINLP
DDPG
OFDMA
NEC
DSRC
SDN
AWGN
C-RAN
CDF
OFDM

Table I: DEFINITIONS OF MAIN ACRONYMS USED IN THIS PAPER.
Deﬁnition
Radio Access Technology
Mixed-Integer Non-Linear Programming
Deep Deterministic Policy Gradient
Orthogonal Frequency Division Multiple Access
Neural Episodic Control
Dedicated Short-Range Communication
Software-Deﬁned Networking
Additive White Gaussian Noise
Cloud/Centralized Radio Access Network
Cumulative Distribution Function
Orthogonal Frequency Division Multiplexing

Deﬁnition
Heterogeneous Network
Deep Reinforcement Learning
Quality of Service
Channel State Information
Visible Light Communication
Patient Edge Node
Deep Neural Network
Signal to Noise Ratio
Long-Term Evolution
Unmanned Aerial Vehicle
Internet of Things

Acronym
HetNet
DRL
QoS
CSI
VLC
PEN
DNN
SNR
LTE
UAV
IoT

Acronym
ED
DQN
AI
V2V
D2D
RF
ES
AP
BBU
OU
NR

Deﬁnition
Edge Device
Deep Q Network
Artiﬁcial Intelligence
Vehicle-to-Vehicle
Device-to-Device
Radio Frequency
Edge Server
Access Point
Baseband Unit
Ornstein-Uhlenbeck
New Radio

Based on simulation results, the authors demonstrated how
their proposed technique outperforms the conventional frac-
tional programming algorithm. In [18], the authors investigated
the problem of rate resource allocation for 5G network slices.
The authors decomposed the problem into a master-slave, and
proposed a multi-agent DDPG-based algorithm to solve it.
Experimental results showed that their proposed algorithm
performs better than some baseline approaches and provides
a near-optimal solution.

In this paper, we present a multi-agent algorithm based
on DRL called the DeepRAT to study the problem of cost-
effective sum-rate maximization of HetNets via dynamic
multi-RAT assignment and continuous power allocation for
multi-connectivity multi-homing EDs. Towards this end, we
formulate this problem as a mixed-integer non-linear program-
ming (MINLP) problem and, due to the high complexity and
combinatorial nature of the problem, we propose the DeepRAT
algorithm to solve it efﬁciently and learn system dynamics,
relying only on limited information about the network and
CSI.

III. SYSTEM MODEL AND PROBLEM FORMULATION
This section describes our proposed system model and

formulates the optimization problem.

A. System Model

We consider a next-generation HetNet as depicted in Fig.
1. It consists of various RAT access points (APs), such as
sub 6GHz, dedicated short-range communication (DSRC) for
vehicular networks, 5G NR, 4G long-term evolution (4G LTE),
and WiFi. It is assumed that the RATs have different operating
characteristics, such as carrier frequency, spectrum, data rate,
energy consumption, the monetary cost for using RAT ser-
vices, and transmission delay. In order to guarantee judicious
and efﬁcient management of networks radio resources, we
assume that the RATs are controlled by a cloud-based edge
server (ES). The RATs are assumed to serve multi-connectivity
(i.e., multi-access), multi-homing EDs. Note that, unlike our
previous work in [1] in which each ED can be assigned one
RAT at any time in a greedy fashion, i.e., multi-mode, the
EDs in this paper are assumed to have the ability to connect to
multiple RATs at any time to aggregate RATs’ radio resources.

Deﬁnition

Table II: DEFINITIONS OF KEY SYMBOLS USED IN THIS
PAPER.
Symbol
L
U
Lu
Ul
xlu
Ulu
Rlu
Ru
Rmin
u
Clu
glu
Γlu
plu
P max
l
hlu
εl

The total number of RATs.
The total number of EDs.
The subset of RATs assigned to the uth ED.
The subset of EDs assigned to the lth RAT.
The assignment indicator for uth ED over lth RAT.
The utility function of uth ED over lth RAT
The data rate for uth ED over lth RAT.
The achieved rate by uth ED from its assigned RATs.
The minimum required rate for uth ED.
The monetary cost per second for uth ED over lth RAT.
The channel gain for uth ED over lth RAT.
The SNR for uth ED over lth RAT.
The allocated power for uth ED over lth RAT.
The total power/bandwidth of lth RAT.
The small scale fading for uth ED over lth RAT.
The monetary cost per bit for lth RAT.
The weighting coefﬁcients representing relative
importance of the two metrics of utility function.
The weighting factors used in rewards to show if satisfying
the constraints has priority over maximizing objective.

αu & γu

ζES , ηES ,
η1, η2, ζl

/Wl

1) reducing the monetary cost of connection,
2) assigning each ED to the optimal set of RAT(s),
3) allocating the optimal downlink power for each RAT-ED

communication link,

4) meeting the QoS requirements of EDs in terms of the

minimum required data rate.

We denote by L (cid:44) {1, 2, · · · , l, · · · , L} the set of all RATs
indices, where L represents the total number of RAT APs. We
also denote by U (cid:44) {1, 2, · · · , u, · · · , U }, the set of all EDs
indices, where U represents the total number of multi-homing
EDs. The subset of EDs assigned to the lth RAT is denoted by
Ul such that Ul ⊆ U ∀l ∈ L. In addition, the subset of RATs
assigned to the uth ED is denoted by Lu such that Lu ⊆ L
∀u ∈ U.

Assuming an OFDM-based system with ﬂat fading for each
ED and the total bandwidth of each RAT is equally divided
between the RATs’ assigned EDs, i.e., Wl/Ul, the upper bound
of downlink data rate for the uth ED over the lth RAT at time
slot t is expressed as [1], [3], [19]:

Rlu(t) =

(cid:32)

log2

1 +

Wl
Ul

(cid:33)
,

glu(t)plu(t)

Wl
Ul

σ2
l

(1)

B. Optimization Problem Formulation

Our objective is to study the problem of downlink sum-rate

maximization of the multi-RAT HetNet while;

where Wl is the total bandwidth of the lth RAT and σ2
l is the
lth RAT power spectral density of the additive white Gaussian
noise (AWGN). The parameter glu(t) in (1) represents the

Fig. 1: System model of our proposed DeepRAT framework for next-generation HetNets. It comprises L radio access technology
(RAT) access points (APs) controlled by a cloud-based edge server (ES) and serves U multi-homing edge devices (EDs).

channel gain for the uth ED over the lth RAT at time slot t,
which is deﬁned based on the channel model of the RAT used.
In Section V, we will simulate three types of channel models,
namely, the mmWave with beamforming for 5G NR [20], the
COST 231 for 4G-LTE [20], and the exponential for 3G [10],
[19]. Also, we will simulate a dynamic wireless system, as we
will discuss later in Section V.

Now we deﬁne εl as the monetary cost of the lth RAT,
which is expressed in Euro per bit [7] and can be obtained
from e.g., the IEEE 802.21 standard [21]. In our system model,
such information can be easily gathered by the ES and stored
in advance and only updated if there are changes in RATs’
pricing. The monetary cost, expressed in Euro per second,
resulting from using the lth RAT by the uth ED to receive
the data rate Rlu(t) at time slot t is expressed as Clu(t) =
εlRlu(t). Table II summarizes the deﬁnitions of the symbols
used in this paper.

In this paper, we are optimizing the downlink sum-rate
of the HetNet in a cost-effective way, such that the QoS
requirements of EDs are guaranteed. This is achieved via
jointly : 1) assigning the optimal set of RAT(s) to each ED,
i.e., xlu∀u ∈ U, l ∈ L, and 2) allocating the optimal downlink
i.e., plu(t), ∀l ∈ L,
power to each active RAT-ED link,
u ∈ Ul. Consequently, our optimization problem is formulated

as follows:

P1:

max
xlu,plu

L
(cid:88)

U
(cid:88)

xluUlu(t)

subject to C1 :

l=1

u=1
L
(cid:88)

xlu ≥ 1, xlu ∈ {0, 1} ∀u ∈ U,

l=1
U
(cid:88)

u=1
L
(cid:88)

C2 :

C3 :

xluplu(t) ≤ P max

l

∀l ∈ L,

(2)

xluRlu(t) ≥ Rmin

u

∀u ∈ U,

l=1
C4 : plu(t) ≥ 0,

∀l ∈ L, u ∈ U.

where xlu is the RTAs-EDs assignment indicator, such that
xlu = 1 if the ES assigns the lth RAT to the uth ED, and xlu =
0 otherwise. To achieve our goal, we combine the data rate
and monetary cost as a weighted sum objective utility function.
In the weighted sum method, the Pareto optimal values can
be achieved by adjusting the weighting parameters [22], and
thus there is no optimality loss in the problem formulation.
Therefore, we deﬁne Ulu(t) in P1 as the utility function of
the uth ED over the lth RAT at time slot t, which is given by:

Ulu(t) = αuRlu(t) − γuClu(t)

(3)

where αu and γu are weighting parameters representing the
relative importance of the objectives of jointly maximiz-
ing data rate and reducing the cost at each ED, such that
αu + γu = 1. In other words, αu and γu are EDs’-deﬁned

parameters used to show if the ED cares more about getting
a higher data rate over the monetary cost or getting a lower
monetary cost over the data rate. Our objective is to ﬁnd the
optimum data rates Rlu(t) that maximize our utility function
via jointly controlling the RATs-EDs assignment and links’
power allocations while considering the EDs preferences in
terms of αu, γu, and Rmin
and the RATs’ rate prices (cid:15)l. Note
that all quantities in (3) are normalized to their maximum
values in order to make them comparable.

u

The optimization problem (2) is over the two unknowns:
xlu and plu subject to four constraints. C1 ensures that each
ED can be connected to multiple RATs simultaneously, and
it reﬂects the multi-homing capabilities of EDs. C2 and C4
ensure that the power allocations from RATs to their assigned
EDs do not violate the RATs’ available power resources. C3
ensures that the achievable data rates for EDs from their as-
signed RATs are greater than the minimum QoS requirements.

algorithm can quickly adapt, in terms of convergence speed,
to the abrupt changes of the network, such as EDs’ mobility.
Hence, and due to the high complexity of our formulated
optimization problem, we propose to solve it using emerg-
ing DRL techniques instead. In particular, we hierarchically
decompose P1 into two optimization sub-problems, such that
each sub-problem is a function of only one decision variable
and, hence, can be solved separately and independently of the
other sub-problem. The ﬁrst sub-problem is to ﬁnd the optimal
RAT-EDs assignment xlu, which depends on the parameters
of the EDs. It is considered a global variable relevant to the
overall HetNet system, and it can be solved at the ES level.
The second sub-problem is to ﬁnd the optimal power allocation
for each RAT-ED plu(t). It is considered a local variable that
depends only on the parameters of RATs and can be solved
at the RATs level. Consequently, P1 is decomposed into the
following two optimization sub-problems:

C. Why DRL?

Problem (2) is a combinatorial Mixed-Integer Non-Linear
Programming (MINLP) [23], which is highly complex to
solve using traditional approaches. In particular, applying the
exhaustive search algorithm to ﬁnd xlu followed by opti-
mization approaches to ﬁnd the corresponding plu is not
practical as the search space will grow exponentially. For
example, as we will show later in Section V, we simulate
a scenario with L = 3 and U = 10. This means that
applying the exhaustive method requires a full search over
(2L − 1)U = 282, 475, 249 possible combinations, each of
which is followed by a constrained optimization process to
ﬁnd plu. This is quite difﬁcult and impractical. In addition,
transforming the problem into a geometric program is not
possible due to constraint C1 with the nonlinearity of the
objective [23], [24]. Note that compared to the problem for-
mulation in [25], in P1 we add the monetary cost aspect to the
utility function, the multi-homing capabilities of EDs, and the
power allocation issue. These aspects added new dimensions
to the formulated problem and increased its complexity. In
addition, compared to the problem formulation in [26], we
added the following four additional dimensions in P1: the
monetary cost issue to the objective function, the multi-homing
constraint, the RATs’ power allocation constraint, and the
QoS requirements constraint of EDs. These new dimensions
enriched the problem while making it more computationally
expensive and difﬁcult to solve using conventional approaches
(e.g., optimization, ranking-based, and game theory methods
discussed in Section I). The same observations are made when
comparing P1 with the problems formulated in [27] and [28].
Furthermore, unlike the previous works in [29], [30], we added
the monetary cost issue and the multi-homing constraint in
P1. Note that compared to our problem formulation in [1],
we add the monetary cost and multi-homing dimensions. In
addition, this paper carefully addresses the scalability issue of
the proposed DeepRAT algorithm for the increasing number of
EDs. Finally, we demonstrate how our proposed DRL-based

SP1: max
xlu

L
(cid:88)

U
(cid:88)

l=1

u=1

xluUlu(t)

subject to C1

and C3.

and

SP2: max
plu

L
(cid:88)

U
(cid:88)

l=1

u=1

xluUlu(t)

subject to C2 − C4.

(4)

(5)

Next, we proceed with our proposed methodology to solve

sub-problems SP1 and SP2 using DRL.

IV. DRL FOR DYNAMIC MULTI-RAT ASSIGNMENT AND

POWER ALLOCATION

In this section, we ﬁrst explain our proposed DeepRAT
algorithm to solve sub-problems SP1 and SP2. Then, we
provide a detailed description of the elements of the DQN
and DDPG models used in the proposed DeepRAT model.

A. The DeepRAT Framework

We propose a multi-agent DRL-based framework called the
DeepRAT, which hierarchically solves SP1 and SP2 iteratively
and interactively in two stages; RATs-EDs assignment and
continuous power allocation. DeepRAT employs two types of
DRL algorithms, a single-agent DQN at the ES and multi-
agent DDPG at the RATs level, as depicted in Fig. 1. The
methodology of the DeepRAT algorithm to solve the two sub-
problems is explained as follows. For sub-problem SP1, Deep-
RAT utilizes a single-agent DQN algorithm to optimize the
RATs-EDs assignment xlu at the ES level, while considering
plu as constants, which are passed by the RATs. Note that this
RATs-EDs assignment is initially performed randomly by the
ES without prior knowledge of whether it would be optimal
or not. Then, the ES broadcasts xlu to the multi-agent DDPG
algorithms of each RAT in order to optimize their power
allocation plu according to sub-problem SP2 while considering
xlu as constants. The ES then receives feedback ACK signals
from all RATs indicating whether the objective of P1 has

been successfully solved for the current RATs-EDs assignment
xlu(t) (i.e., the RATs are in good status) or not (i.e., the RATs
are in bad status). Based on these ACK signals, the ES starts
learning to make better assignments in the future time slots.
These two stages are iteratively executed until all DeepRAT’s
agents learn the global policy that solves our main problem
in P1, i.e., the single-agent DQN learns the optimal RATs-
EDs assignment policy, and the multi-agents DDPG learn the
optimal power allocation policy. The main elements of these
two types of DRL models are deﬁned next.

B. DeepRAT Stage 1: DQN Algorithm for RATs-EDs Assign-

ment

Due to the discrete nature of the RATs-EDs assignment
problem, we adopt the DQN algorithm to act as an ES agent to
learn the optimal policy for this problem. Below, we deﬁne the
state space, action space, and reward function for the single-
agent ES DQN model.

t

1) DQN action space:
At each time step t, the main role of the DQN ES agent is to
take an action aES
that optimally assigns each of the EDs to
the optimal set of RAT(s), i.e., obtaining xlu in SP1. As shown
in the optimization problem SP1, the DQN assignment action
should: 1) maximize the objective function, and 2) satisﬁes C1
and C3. These conditions can be achieved iteratively by the
design of the reward function. This action is a combinatorial
problem that scales exponentially with the number of EDs
and RATs, causing degradation in both system scalability and
convergence speed. Unlike our previous work [1], in which the
size of action space was proportional to both the number of
EDs and RATs, i.e., LU , in this paper we carefully address this
scalability issue. Speciﬁcally, we make the size of the action
space proportional to only the number of RATs, i.e., 2L. The
scalability enhancement achieved is evident. As an example,
for L = 3 and U = 5, the size of the action space proposed
in this paper is around 30 times less than the one proposed in
[1]. Similarly, when L = 4 and U = 10, the size reduction of
the action space is around 65536. This will greatly enhance
system scalability and reduce convergence time. Therefore, the
action space of the DQN ES agent is discrete, corresponding
to assigning the optimal set of RAT(s) Lu to the uth ED ∀u ∈
U, where Lu ∈ L, which is expressed as:

AES(t) = [a1(t), a2(t), · · · , au(t), · · · , aU (t)],
au(t) = {L1, L2, · · · , L2L},

(6)

2) DQN state space:
Due to the holistic view of the DQN ES agent, its state
space must include effective and rich information about RATs
and EDs to help the DQN agent in taking optimal assignment
actions. To address the scalability issue mentioned previously,
the RATs-EDs assignment is done by the ES iteratively for
each ED, i.e., the ES assigns the uth ED to the best set of
RATs Lu, while considering the assignment of the remaining
EDs constant. Therefore, the ES DQN is conﬁgured to run on
episodes of U EDs time steps, and the DQN state must indicate
the ED investigated [7]. In addition, unlike our previous work

in [1] in which we assumed that
the CSI is available at
the agents, in this work we consider a more practical and
challenging scenario by assuming that the agents have limited
information about network dynamics and CSI. In particular,
we assume that only historical information about the achieved
data rates is available to the agents. With this in mind, the
state space of ES is comprised of two main components,
global information related to all RATs and EDs in the network
and local information related to the uth ED investigated. The
global information contains three elements; the matrix of all
xlu at the previous time step (i.e., xlu(t − 1) ∈ CU ×L ∀l, u),
the matrix of all Rlu at the previous time step (i.e., Rlu(t−1) ∈
CU ×L ∀l, u), and the vector of all Rmin
u ∈ CU ×1 ∀u. The local
information is only related to the uth ED under investigation,
which has three scalar elements;
the index of ED under
investigation at the current time step uth(t) ∈ C1×1, the
uth ∈ C1×1, and the
minimum required date of this ED Rmin
achieved data rate of this ED from its assigned RATs at the
previous time step Ruth(t − 1) ∈ C1×1. Consequently, the
state space of the DQN ES agent is expressed as:

SES(t) = [Global information, Local information],
SES(t) = [xlu(t − 1), Rlu(t − 1), Rmin
,
(cid:125)

(cid:124)

u

(cid:123)(cid:122)
Global information

(7)

uth(t), Rmin
(cid:124)

uth, Ruth(t − 1)
(cid:125)

(cid:123)(cid:122)
Local information

].

3) DQN reward function:
The reward function is designed to incorporate the objective
of our optimization problem in P1 on one hand and the
constraints C1 and C3 on the other hand. Hence, the agent will
receive a negative punishment if the constraints are violated.
The instantaneous reward rES that the ES agent receives when
taking action aES given state sES for the uth ED is expressed
as:

rES(t) (cid:44) ηESconstraints + ζESobjective,
(cid:16) (cid:88)

U
(cid:88)

(cid:17)

Rlu(t) − Rmin

u

+

rES(t) = ηES

(8)

u

l∈Lu

ζES

L
(cid:88)

U
(cid:88)

l=1

u=1

xluUlu(t).

where ηES and ζES are weighting factors used to indicate
whether satisfying the constraints has priority over maximizing
the objective or not. These factors are manually tuned during
simulation.

The RATs-EDs assignment problem can be formulated
based on the immediate rewards achieved. Towards this
goal, the expected accumulated discounted instantaneous re-
the time horizon T is deﬁned as RES =
ward over
(cid:104)(cid:80)T
E
, where 0 ≤ γ ≤ 1 is a discounted
factor [15]. The objective of the DQN ES agent is to obtain the
optimal decision policy π∗
ES (i.e., selecting the optimal RATs-
EDs assignment xlu) that maximizes RES. This is expressed
as π∗

t=1 γt−1rES(t)

ES = argmax

RES.

(cid:105)

π

Qπ∗

ES = arg max
, aES
t

However, as we discussed earlier in Subsection III-C, this
RATs-EDs assignment problem is space-hard and it is quite
difﬁcult for traditional resource allocation techniques to solve
it [6], [8]. Therefore, the DQN algorithm can be leveraged
instead to learn π∗
ES. In DQN, the optimal policy is expressed
as π∗
), where the function
Qπ∗
) is called the state-action value function.
This value function deﬁnes the expected accumulated dis-
counted instantaneous reward achieved when executing action
aES
and then following the policy πES there-
t
after. The value-function is deﬁned as Qπ∗
) =
Eτ πES [RES|sES
], and the DQN algorithm utilizes the
following iterative Bellman equation to compute it:

in state sES

ES (sES

ES (sES

ES (sES

, aES
t

, aES
t

, aES
t

ES

ES

ES

a

t

t

t

t

t

t

Q∗

, aES
t

, aES
t

ES(sES
t

) = rES(sES

t+1),
(9)
At each decision time step t, the deep neural network (DNN)
of the ES DQN model iteratively updates its weights θES to
minimize the following loss function:

)+γ max
aES
t

t+1, aES

ES(sES

Q∗

LES(θt) = Es,a,r,s(cid:48)∈DES [(yES(t)−QES(sES

t

, aES
t

|θES))2],
(10)
+
the target value, which

rES(sES

, aES
t

)

t

where
γ max
aES
t+1

yES(t)
(cid:16)
t+1, aES
sES

t+1|θ

(cid:48)
ES

=
(cid:17)

is

QES

is obtained from the target network with old weights θ
and DES represents the DQN replay buffer.

(cid:48)

ES,

C. DeepRAT Stage 2: DDPG Algorithm for Power Allocation

The DDPG is an efﬁcient DRL algorithm developed to
learn policies for continuous-based problems with high di-
mensionality in state and action spaces [16], [18], [31]. The
DDPG algorithm will be leveraged in our second stage, i.e.,
solving the power optimization problem. In particular, a multi-
agent deployment is considered in which each RAT employs
a DDPG agent, whose main goal is solving its own objective
function in SP2 for its assigned EDs, Ul. The main elements
of the multi-agent DDPG algorithm used are deﬁned below.

the main goal of each DDPG agent

1) DDPG action space:
Each DDPG RAT agent takes action independently and
uncooperatively from the other agents. At time slot t, once
the DQN ES agent executes the assignment action for the
uth ED (Lu),
is to
optimize the power allocation plu in SP2 ∀u ∈ U and
∀l ∈ L. As shown in the optimization problem SP1, the
power allocation action of the lth DDPG RAT agent should:
1) maximize the objective function, and 2) satisfy C2, C3,
and C4. These conditions can be achieved by incorporating
them into the reward function. Consequently, the action space
of the lth DDPG RAT agent is continuous with a size of U ,
corresponding to deciding the optimal power allocation for
each of the RATs-EDs communication links (i.e., plu(t)). The
action space of the lth DDPG RAT agent is deﬁned as:

Al(t) = [pl1(t), pl2(t), · · · , plu(t), · · · , plU (t)]
] W att,

plu(t) ∈ [0, P max

where

l

(11)

The action exploration-exploitation problem in the DDPG
algorithm is addressed via adding some Gaussian or Ornstein-
Uhlenbeck (OU) noise nt to the selected action al

t [31].

2) DDPG state space:
We assume that the DDPG RAT agents do not cooperate,
and there is no direct communication between them. The
agents, however, have direct communication with the DQN
ES, which has a holistic view of all DDPG agents and can
coordinate them. This means that each DDPG agent can
acquire information about the other agents via the ES. The
state space of the lth DDPG RAT agent is designed to contain
useful information on the underlying HetNet. Four main types
of representative information are incorporated in each lth agent
state space. The ﬁrst type of information is discrete and is
directly related to the current RATs-EDs assignment action of
the ES agent, which is a vector of the set of EDs assigned
to the lth RAT at the current time step (Ul(t) ∈ CU ×1). The
second type of information is continuous, which is the vector
∈ CU ×1, ∀u ∈ Ul. The third information is the
of Rmin
u
vector of Rlu for the lth RAT at the previous time slot (i.e.,
Rlu(t − 1) ∈ CU ×1, ∀u ∈ Ul). The fourth information is
the vector of Ru at the previous time slot (i.e., Ru(t − 1) ∈
CU ×1, ∀u ∈ Ul). Note that while Rlu(t − 1) denotes the
downlink rate for a single link between the uth ED and lth
RAT at the previous time slot, the notation Ru(t − 1) denotes
the rate achieved by the uth ED from its assigned RATs Lu
at the previous time slot, i.e., Ru(t − 1) = (cid:80)
Rlu(t − 1).
Consequently, the state space of the lth DDPG RAT agent is
represented as:

l∈Lu

Sl(t) = [Ul(t), Rmin

u

, Rlu(t − 1), Ru(t − 1)].

(12)

3) DDPG reward function:
The reward of the lth DDPG RAT agent is expressed as a
continuous function that is governed by the RAT’s achieved
constrained objective function. It is quantiﬁed by including the
optimization constraints C2, C3, and C4 of (2) into the reward
function so that the instantaneous reward reﬂects whether the
constraints are satisﬁed or not [7], [16]. The reward function
is given by (13). In (13), η1, η2, and ζl are also weighting
factors used to indicate whether satisfying the constraints has
priority over maximizing the objective or not. These factors
are manually tuned during simulation.

π

π

(cid:80)T

t, al

Rl = argmax

t=0 γt−1rl(sl

In this second stage of our problem SP2, the main objective
is to derive the optimal power allocation policy π∗
that
l
maximizes the long-term reward of the lth agent Rl, i.e.,
π∗
t). Towards
l = argmax
this goal, we implement the DDPG algorithm to derive this
policy π∗
l . The DDPG algorithm integrates the DQN and actor-
critic algorithms [16], [31], and will be utilized to perform
the training of the RATs’ DNNs. The DDPG has one param-
eterized actor function and one parameterized critic function
represented by µ(sl
t, al
l ), respectively, where
θµ
l and θQ
l denote the weights of the actor and critic networks,
respectively. The parameterized actor function is used to
derive the policy, and it is implemented with a DNN trained

l ) and Ql(sl

t|θQ

t|θµ

rl(t) (cid:44) constraints + ζlobjective,

rl(t) = η1

(cid:16)

P max
l

−

(cid:88)

(cid:17)

plu(t)

+ η2

(cid:88)

(cid:16) (cid:88)

Rlu(t) − Rmin

u

(cid:17)

+ ζlUlu(t).

(13)

u∈Ul

u∈Ul

l∈Lu

t, al

t, al

l (sl

l (sl

t) = Eτ π[Rl|sl

t), deﬁned as Qµ

based on the iterative Bellman equation. On the other hand,
the parameterized critic function is used to derive the value
function Qµ
t, al
t],
and it is implemented using a DQN. The goal of each DDPG
is to ﬁnd the optimal policy π∗
that maximizes the long-
l
term reward Rl using π∗
t). The value
function Qµ∗
l (sl
t) is derived iteratively using the Bellman
equation similar to (9), and the policy π∗
l is found via training
the DNN of the lth DDPG RAT agent to minimize the Bellman
loss function given by the following formula:

l = arg max

l (sl

Qµ∗

t, al

t, al

a

Ll(θQ

l ) = Es,a,r,s(cid:48)∈Dl [(yl(t) − Ql(sl

t, al

t|θQ

l ))2]

(14)

where Dl denotes the lth DDPG agent’s replay memory and
yl(t) represents the target value, which is derived from the
target network and obtained from the following equation:

yl(t) = rl(sl

t, al

t)+γ max
at+1

(cid:18)

Ql

t+1, µl(sl
sl

t+1|θπ

l )|θQ

l

(cid:48)
l

(cid:48)

(cid:19)

(15)

(cid:48)

where θQ
denotes the weights of target critic network, which
l
has the same architecture as the main Q-network. These
weights are mainly used to make the training more stable,
and they are periodically updated based on the weights of the
main Q-network θQ
l .

The actor network of the lth DDPG agent is trained via
applying the chain rule to the expected return from the
cumulative reward distribution J with respect to θµ

l [31]:

∇θµ

l

J = E

(cid:104)

∇θµ

l

Ql(sl

t, al

t|θQ

l )|s=st,a=µ(st,θµ
l )

(cid:105)

(16)

The detailed pseudo-code of our proposed multi-homing
DeepRAT algorithm for solving (2) is given in Algorithm
1 and explained next. Lines 1 to 3 initialize the network
parameters, ES DQN model, RATs DDPG models, and initial
states. The episode begins with initial states for all the agents
and iterates over all EDs. In Lines 4 to 12, the DQN ES agent
observes the state for each ED and takes the corresponding
assignment action to the RATs, Lu. In Lines 13 to 23, each
DDPG RAT agent observes the state space (including the
current assignment action of the DQN agent) and takes the
corresponding power allocation action to the EDs plu ∀u ∈ Ul.
In Lines 24 to 27, the DQN ES agent receives the reward for
each ED assignment and learns to take a better assignment in
future episodes. This process is repeated until the DeepRAT
converges to the optimal policy that solves our main problem
P1 in (2), i.e., the DQN ES agent learns the optimal RATs-
EDs assignment policy and all the DDPG RAT agents learn
the optimal power allocation policy.

Algorithm 1 The Multi-Homing DeepRAT Algorithm
Input: L, U, Rmin
Output: Optimal RATs-EDs assignment xlu & power allocation plu.

, αu, γu.

u

1: Initialization: Set t = 0 and initialize DES of ES DQN agent

and Dl of RATs’ DDPG agents, ∀l ∈ L.

(cid:48)

ES), and

2: Randomly initialize weights of ES’s DQN (θES & θ
(cid:48)

(cid:48)

RATs’ DDPG (θµ

l , θQ

l , θµ

l

, & θQ
l

).

for ED = 1 to U do

3: Initialize states s0 of ES and RATs with initial observations.
4: for episode = 1 to M do
5:
6:
7:
8:

Generate a random number x from [0, 1]
if x ≥ (cid:15)(t) then

from ES’s action space AES(t)
|θES

(cid:0)sES

, aES
t

(cid:1).

QES

t

Choose action aES
t
according to max
a∈AES

else

Choose a random action aES

t

from AES(t).

end if
Observe state for ES sES
t
for tRAT = 0 to K do

and perform action aES

t

.

i, sl

i, rl

i, al

i|θQ

t = µl(sl

t+1) in Dl.

of N transitions

i=1 (yl(i) − Ql(sl

l with OU noise, i.e., al
t, al

t, al
random mini-batch
i+1) from Dl.

t|θµl
t) using (13), observe sl
t, rl, sl

t using (12) and take action
l ) + nl
t.
t+1 using

l by minimizing loss (from (14)): Ll(θQ
l ))2.
l using sampled policy gradient (from (16)):
i|θQ

Observe state for each RAT sl
t from Ap
al
Receive reward rl(sl
(12), and store transitions (sl
Sample
i, al
(sl
Set yl(i) based on (15).
Update θQ
(cid:80)N
1
N
Update θµ
Ql(sl
J = 1
l )|s=si,a=µ(si,θµ
∇θµ
l )
N
Update weights of all RATs’ DDPG target networks:
(cid:48)
θQ
l ← τ θQ
l ← τ θµ
θµ
end for
Receive reward rES(sES
t
using (8), & store (sES
Sample
(sES
i+1) from DES.
i
Update weights of ES θES to minimize loss in (10).

t
random mini-batch
, rES,i, sES

t+1) in DES,
of M transitions

l + (1 − τ )θQ
l + (1 − τ )θµ

, aES
t
, rES, sES
, aES
t

) using (9), observe sES
t+1

i=1 ∇θµ

l ) =

, aES
i

i, al

(cid:80)N

l
(cid:48)

,

l

l

l

(cid:48)

(cid:48)

9:
10:
11:
12:
13:
14:

15:

16:

17:
18:

19:

20:

21:

22:
23:
24:

25:

26:
27:
28: end for

end for

D. Deployment Scenario of the DeepRAT Framework

Our proposed multi-homing DeepRAT algorithm is simple
yet practical for implementation using simple software-deﬁned
radios (SDRs). During the training phase of the DeepRAT
model, the expensive computations are conducted ofﬂine on
quasi-centralized hardware, such as GPUs and/or tensor pro-
cessing units. Once the DeepRAT algorithm learns the optimal
global policy, it can be deployed online to perform optimal
decisions autonomously by relying only on its learned policies
without inducing any extra delay. This aspect will be quantiﬁed
in the next section. It is noteworthy that updating DeepRAT’s

DNNs is only required if the characteristics of the wireless
environment have changed signiﬁcantly, and they are no longer
reﬂecting the training experiences. Such a case occurs once per
several weeks or even months.

In addition, the design principle of the DeepRAT framework
is quite practical for the modern AI-driven wireless networks.
Here, we provide three practical deployment scenarios of the
proposed DeepRAT ecosystem. First, the DeepRAT can be
deployed in the Cloud/Centralized Radio Access Networks
(C-RANs) architecture,
in which the cloud-based ES can
the baseband unit (BBU) pool of the C-
be allocated at
RANs. Second, the DeepRAT framework can be deployed
in the Software-Deﬁned Networking (SDN) architecture, in
which the ES can be placed at the control-plane side of the
architecture. Third, the DeepRAT framework can be deployed
in the future self-organizing/sustaining networks [2], in which
the ES can be placed at the self-organizing/sustaining server.

V. PERFORMANCE EVALUATION

This section presents a detailed description of the simulation
setup we used to evaluate the performance of our proposed
multi-homing DeepRAT algorithm. We ﬁrst discuss the spec-
iﬁcations of the HetNet under investigation, the DRL models
used, and the EDs requirements. Then, we present and discuss
the simulation results. Also, we present a practical scenario
where there are abrupt changes in EDs mobility to demonstrate
the ability of our proposed DeepRAT model to dynamically
adapt to these varying system dynamics.

A. Simulation Setup

We consider a practical scenario of a next-generation cel-
lular network comprised of three multi-RAT OFDM-based
systems, i.e., L = 3, speciﬁcally, 5G NR, 4G LTE, and 3G,
as shown in Fig. 1. The speciﬁcations of these three systems
are shown in Table III. Note that during our simulation, we
considered three practical channel models for each RAT with
ﬂat fading, namely, the mmWave with beamforming for 5G
NR [20], the COST 231 for 4G-LTE [20], and the exponential
for 3G [10], [19]. The speciﬁcations of these channel models
are listed in Table III. The RATs are 100 meters apart. Ten
single-antenna EDs are assumed, i.e., U = 10, requesting
services from the ES with random QoS requirements i.e.,
Rmin
, αu, and γu as shown in Table IV. In order to model a
u
dynamic wireless system, we assume a time-varying network
where the mobility of EDs varies over time with random
speeds ranging from 2 to 6 km/h. This means that the CSI,
in terms of channel gain, of all links will dynamically change
over time.

The number of agents is four; one DQN-based located at
the ES side and three DDPG-based located at each RAT, as
depicted in Fig. 1. These DRL models are simulated in Python
using the Pytorch library, with architectures as shown in Table
V. Relu activation functions are used at the output layers of all
NNs, and the weights are updated using the Adam optimizer
[32]. Also, in order to satisfy C4 in P1, we employ the sigmoid
function at the output layers of the DDPG actor networks.

Table III: SIMULATION PARAMETERS [10], [20].

Parameter
Frequency (GHz)
Bandwidth (MHz)
Max power (dBm)
Noise spectral density (dBm/MHz)
Channel model
Path loss exponent
Number of uniform linear array antennas
Number of multipaths
Antenna gain (dBi)
Shadowing (dB)
(cid:15)l (Euro/bit)

RAT1 (5G)
28
200
43
-57
Directional
2(LOS), 4(NLOS)
4
4
3
3.1
9e-6

RAT2 (4G LTE)
6
40
40
-57
COST 231 (Urban)
-
4
4
11
3
6e-6

RAT3 (3G)
2.4
27
42
-57
Exponential
2 (LOS)
1
-
-
1.8
1e-6

Table IV: QOS REQUIREMENTS OF EDS.

(bps)

u

ED ID Rmin
ED1
ED2
ED3
ED4
ED5
ED6
ED7
ED8
ED9
ED10

8.3 ×104
8.49 ×104
1.17 ×104
4.78 ×104
1.37 ×104
1.43 ×104
6.1 ×104
1.58 ×104
8.93 ×104
7.24 ×104

αu
0.4
0.3
0.2
0.2
0
0.5
0.4
0.6
0.6
0.1

γu
0.6
0.7
0.8
0.8
1
0.5
0.6
0.4
0.4
0.9

Table V: HYPERPARAMETERS OF THE DEEPRAT MODEL.

Hyperparameter

Buffer size
Batch size
γ

Single-Agent DQN
1000
64
0.99

Number of layers

2

Number of neurons

Learning rate

(256, 128)

8 × 10−4

Exploration/
Exploitation noise

Rewards weighting
factors

(cid:15)start = 1
(cid:15)end = 0.005
(cid:15)decay = 5 × 10−4
ηES = 1 × 103 &
ζES = 8 × 10−4

Value

Multi-Agent DDPG
500
16
0.99
actor=2
critic=2
(16, 16)
actor=5 × 10−4
critic=5 × 10−4

OU (θ = 0.15,
σ = 0.03)

η1 = 1, η2 = 1 × 103,
& ζl = 5 × 10−3

B. Numerical Results

In this subsection, we present simulation results to evaluate
the performance of our proposed DeepRAT algorithm when
deployed in an online fashion. Also, in order to evaluate the
performance of our proposed multi-homing DeepRAT algo-
rithm, we compare it against four state-of-the-art benchmarks.
1) The multi-mode method (i.e., maximum or greedy method),
in which the ES will greedily assign each ED to only one
RAT that gives the maximum utility after the convergence of
the DeepRAT algorithm, and the DDPG agents are utilized for
power allocation, i.e., similar to our conference version in [1]
and the work in [29]. It is noteworthy that the core difference
between our implementation in this paper and the previous im-
plementation in [1] is the limited information about the multi-
RAT network in this paper. In particular, our implementation of
the multi-mode algorithm in [1] assumes that the agents have
full knowledge of network dynamics and CSI, i.e., channel
gains glu, and we compared our approach against the CVXPY
solver’s solution [33]. However, our proposed multi-homing
DeepRAT approach presented in this paper assumes that the
agents have limited information about system dynamics and
CSI). 2) The random approach, in which the ES will randomly
assign each ED to only one RAT after the convergence of the

Fig. 2: Reward training convergence of the DeepRAT’s DQN and multi-agents DDPG models.

the multi-RATs and instantaneous CSI. 3) Additionally, our
DeepRAT algorithm adapts to abrupt network changes such
as EDs’ mobility. However, these conventional approaches are
not adaptable, which severely degrades performance, accuracy,
and reliability of the learned policies [6], [8].

Fig. 2 shows the training rewards of all DeepRAT’s DQN
and DDPG agents. The DQN converges to the steady-state of
optimal RATs-EDs assignment policy after 226 DQN episodes,
while all three DDPG agents converge to the optimal power
allocation policy after 2252 DDPG episodes. These results
clearly show how our various value-based (i.e., DQN) and
policy-based (i.e., DDPG) DRL agents efﬁciently interact with
each other in order to learn a uniﬁed global optimal policy that
solves our optimization problem in P1.

Fig. 3 shows the utility function, i.e., the objective of the
optimization problem P1, which clearly demonstrates that
the proposed multi-homing DeepRAT algorithm converges
to the optimum solution after 226 episodes. Also, Fig. 3
shows that the utility of the proposed multi-homing DeepRAT
algorithm outperforms the ones achieved by the state-of-the-
art approaches mentioned above, i.e., the multi-mode, random,
ﬁxed, and CVXPY approaches. In particular, the steady-state
utility function of the proposed DeepRAT algorithm is 1.73
compared to 1.58, 1.52, 1.33, and 1.4, respectively, for the
multi-mode, CVXPY, random, and ﬁxed methods. Recall that
the multi-mode, random, and ﬁxed methods do not guarantee
that they satisfy the EDs’ QoS requirements. In addition, note
that the multi-mode outperforms the CVXPY as the latter does
not assign EDs to the optimal set of RATs leading to a lower
utility value. Fig. 4 also shows the corresponding cumulative
distribution function (CDF) of the utility function for the
proposed multi-homing DeepRAT algorithm and these four
state-of-the-art approaches. It clearly shows that the median
of the utility for multi-homing DeepRAT is 1.73 compared
to 1.58, 1.52, 1.33, and 1.4, respectively, for the multi-mode,
CVXPY, random, and ﬁxed methods.

In Fig. 5, we present the total sum-rate achieved by our
proposed multi-homing algorithm compared to the four con-
ventional approaches. The steady-state of total sum-rate of our
proposed DeepRAT approach is 4.2 Gbps compared to 4.33
Gbps, 3.86 Gbps, 3.23 Gbps, and 3.42 Gbps for the CVXPY,

Fig. 3: Utility of the proposed multi-homing DeepRAT al-
gorithm compared to the multi-mode, random, ﬁxed, and
CVXPY schemes. Note that, unlike the proposed multi-
homing DeepRAT technique, these four heuristic-based meth-
ods are not guaranteed to meet the EDs’ QoS requirements in
terms of rates, αu, and γu preferences.

DeepRAT algorithm, and the DDPG agents are used for power
allocation. 3) The ﬁxed approach, in which the ES will assign
EDs to all existing RATs, and the RATs will allocate their
power equally to all EDs. 4) The CVXPY solver’s solution
with MOSEK sub-solver [33], in which the ES will assign
EDs to all existing RATs, and the CVXPY solver is utilized to
solve the power allocation optimization problem assuming full
knowledge of system dynamics and instantaneous CSI, similar
to the works in [1], [18]. However, we should emphasize the
following. 1) Although these conventional approaches show
good results compared to our proposed multi-homing ap-
proach, they do not always guarantee an optimal solution, i.e.,
the QoS requirements of EDs are not guaranteed. 2) Unlike
our proposed multi-homing DeepRAT approach, which works
based on limited information about system dynamics and CSI,
the ﬁxed and CVXPY methods require perfect knowledge of

Fig. 4: CDF of
the proposed
multi-homing DeepRAT algorithm compared with the other
heuristic-based schemes.

the utility function of

multi-mode, random, and ﬁxed methods, respectively. Hence,
the percentage increase in total sum-rate achieved by the multi-
homing DeepRAT over the multi-mode, random, and ﬁxed is
8.81%, 30.03%, and 22.81%, respectively. Note that although
the rate achieved by the CVXPY solver is slightly higher
than the proposed DeepRAT algorithm, it has the following
shortcomings. 1) Unlike the multi-homing DeepRAT method,
the CVXPY-based approach does not guarantee that the EDs
are assigned to the optimal set of RATs as it assigns all EDs
to all existing RATs. 2) The CVXPY solver requires full and
instantaneous knowledge of CSI to solve the power allocation
problem, whereas the proposed DeepRAT method does not.
This aspect is important when we discuss the fast adaptivity
of our proposed DeepRAT algorithm in the next subsection.
3) The utility values obtained by the DeepRAT method are
higher than those obtained by the CVXPY solver, as observed
from Figs. 3 and 4.

The optimal RATs-EDs assignment process after the con-
vergence of all DeepRAT’s models is shown in Fig. 6. It shows
which EDs have been assigned to RATs and the corresponding
percentages of downlink data rates achieved. The top plot
shows the percentage of downlink data rates delivered by
each RAT to its assigned EDs, while the bottom plot shows
the percentage of data rates delivered to each ED from its
assigned set of RATs. As an example, the top plot shows
that the ES assigned RAT1 to six EDs, namely ED2, ED3,
ED4, ED6, ED8, and ED9 (i.e., U1 = {2, 3, 4, 6, 8, 9}). The
percentages of data rates delivered to these EDs from RAT1 are
52.9%, 4.42%, 10%, 0.08%, 14.7%, and 17.9%, respectively.
Also, the bottom plot shows that the ES assigned ED3 to
all three RATs (i.e., L3 = {1, 2, 3}), and the percentages
of data rates delivered from RATs 1, 2, 3 are 53%, 30.7%,
and 16.3%, respectively. For convenience, Table VI shows the
results presented at the bottom of Fig. 6 in a tabular form.

Fig. 7 shows the achieved data rate for each multi-homing

Fig. 5: Total sum-rate achieved by the proposed multi-homing
DeepRAT algorithm compared to the multi-mode, random,
ﬁxed, and CVXPY schemes. Although the CVXPY scheme
provides a slightly better rate than DeepRAT, it does not assign
EDS to the optimal set of RATs, requires full knowledge of
CSI, and has inferior utility values as shown in Figs. 3 and 4.

Table VI: PERCENTAGES OF DATA RATES DELIVERED BY
RATS TO THEIR ASSIGNED EDS, AS SHOWN AT THE BOT-
TOM OF FIG. 6.
ED ID
ED1
ED2
ED3
ED4
ED5
ED6
ED7
ED8
ED9
ED10

RAT3 (% )
23.1
0.54
16.3
22.5
100
0
100
8.2
0
99.3

RAT2 (% )
76.2
0.36
30.7
1.3
0
39.9
0
0
0
0.7

RAT1 (%)
0
99.1
53
76.2
0
60.1
0
91.8
100
0

u

ED from its assigned RAT(s). All EDs converge to the
optimal rate and reach steady-state after 226 episodes. Also,
when comparing these results with the minimum data rate
requirements of EDs Rmin
in Table IV, we can see that the ES
assignment guarantees that all EDs satisfy their QoS data rate
requirements in a manner that cost-effectively maximizes the
network sum-rate. For example, the data rate requirement of
ED4 is 47.8 kbps, whereas the achievable rate after connecting
ED6 to RATs 1 and 2 is 503 Mbps bps, which is much
greater than the required rate. Indeed, our proposed scheme
can signiﬁcantly enhance the performance of the HetNet by
enabling data transfer from RATs to EDs in a cost-effective
manner while guaranteeing satisfactory QoS.

C. DeepRAT’s Adaptivity to the Mobility of EDs

In this subsection, we demonstrate the efﬁciency of the
DeepRAT algorithm to quickly and dynamically adapt,
in
terms of the convergence speed, to abrupt network changes.

Fig. 8: Adaptivity of the DeepRAT algorithm to the dynamic
mobility of EDs. The EDs randomly move at every 1000
episodes, and the DeepRAT quickly adapts to these changes.
The convergence speed after training is around 2.5 faster than
the one at the initial training.

is deﬁned as having the utility values constant for the last 200
episodes [3].

We investigate a practical scenario where the EDs move
randomly during each 1000 episodes. Fig. 8 shows the corre-
sponding simulation results for the utility function. We notice
that the DeepRAT algorithm adapts very quickly, in terms of
the convergence speed, to the abrupt system dynamics, i.e.,
EDs mobility, and it dynamically ﬁnds the optimal solution
of the problem in P1. These trends are clear at episodes
1000, 2000, 3000, and 4000, where the DeepRAT algorithm
converges and reaches steady-states after 246, 1081, 2097,
3078, and 4034 episodes, respectively. The initial training
phase takes around 246 episodes to converge, whereas the
worst-case after the training takes only less than 97 episodes
to converge, i.e., to re-solve the optimization problem P1. This
means that the convergence speed after training is around 2.5
faster than the convergence speed at the initial training, which
quantiﬁes the dynamic adaption performance of the DeepRAT
algorithm for the random changes in network dynamics.

VI. CONCLUSION

This paper investigated the problem of cost-effective down-
link sum-rate maximization in multi-RAT multi-homing Het-
Nets. The problem was formulated as a MINLP whose ob-
jective is to cost-effectively maximize network sum-rate via
jointly assigning EDs to the optimal set of RATs and allocating
the optimal RATs’ power levels. Due to the high complexity
and combinatorial nature of the problem on the one hand
and the limited knowledge of network statistics on the other
hand, we proposed to solve the problem using DRL methods.
Towards this goal, we proposed a multi-agent DQN and
DDPG-based DRL algorithm, called DeepRAT, which solved
the problem hierarchically in two stages; dynamic RATs-
EDs assignment and power allocation. Our simulation results
showed that the proposed multi-homing DeepRAT algorithm
outperforms four benchmark heuristic algorithms in terms

Fig. 6: Percentage of data rate delivered from each RAT to its
assigned EDs (top) and achieved by each ED from its assigned
RATs (bottom).

Fig. 7: Total achieved rate by each multi-homing ED from
its assigned RATs. All rates are greater than the minimum
required data rate requested by EDs as shown in Table IV.

During the simulation, we deﬁne the convergence speed as the
number of episodes required to reach the steady-state, which

of utility value. In addition, our simulation results showed
the ability of the DeepRAT algorithm to quickly adapt to
abrupt network changes, such as EDs’ mobility, and that its
convergence speed after training is around 2.5 faster than the
initial training. As future work, we will extend the multi-
homing DeepRAT algorithm to address the problem of joint
optimization of both power and spectrum.

ACKNOWLEDGMENT

This publication was made possible by NPRP-Standard
(NPRP-S) Thirteen (13th) Cycle grant # NPRP13S-0201-
200219 from the Qatar National Research Fund (a member
of Qatar Foundation). The ﬁndings herein reﬂect the work,
and are solely the responsibility, of the authors.

REFERENCES

[1] A. Alwarafy, B. S. Ciftler, M. Abdallah, and M. Hamdi, “DeepRAT: A
DRL-based framework for multi-RAT assignment and power allocation
in hetnets,” in 2021 IEEE International Conference on Communications
Workshops (ICC Workshops).

IEEE, 2021, pp. 1–6.

[2] W. Saad, M. Bennis, and M. Chen, “A vision of 6g wireless systems:
Applications, trends, technologies, and open research problems,” IEEE
network, vol. 34, no. 3, pp. 134–142, 2019.

[3] B. S. Ciftler, M. Abdallah, A. Alwarafy, and M. Hamdi, “DQN-based
multi-user power allocation for hybrid RF/VLC networks,” in ICC 2021-
IEEE International Conference on Communications.
IEEE, 2021, pp.
1–6.
[4] CISCO
2023).
//www.cisco.com/c/en/us/solutions/collateral/executive-perspectives/
annual-internet-report/white-paper-c11-741490.pdf

Report
Available:

(2020)
White

(2018-
https:

[Online].

Internet

Annual

Paper.

Cisco

[5] F. Hussain, S. A. Hassan, R. Hussain, and E. Hossain, “Machine learning
for resource management
in cellular and IoT networks: Potentials,
current solutions, and open challenges,” IEEE Communications Surveys
& Tutorials, vol. 22, no. 2, pp. 1251–1275, 2020.

[6] A. Alwarafy, M. Abdallah, B. S. Ciftler, A. Al-Fuqaha, and M. Hamdi,
“Deep reinforcement learning for radio resource allocation and manage-
ment in next generation heterogeneous wireless networks: A survey,”
arXiv preprint arXiv:2106.00574, 2021.

[7] Z. Chkirbene, A. Awad, A. Mohamed, A. Erbad, and M. Guizani, “Deep
reinforcement learning for network selection over heterogeneous health
systems,” IEEE Transactions on Network Science and Engineering,
2021.

[8] N. C. Luong, D. T. Hoang, S. Gong, D. Niyato, P. Wang, Y.-C.
Liang, and D. I. Kim, “Applications of deep reinforcement learning
in communications and networking: A survey,” IEEE Communications
Surveys & Tutorials, vol. 21, no. 4, pp. 3133–3174, 2019.

[9] A. A. Khan and R. S. Adve, “Centralized and distributed deep rein-
forcement learning methods for downlink sum-rate optimization,” IEEE
Transactions on Wireless Communications, vol. 19, no. 12, pp. 8410–
8426, 2020.

[10] H. Ding, F. Zhao, J. Tian, D. Li, and H. Zhang, “A deep reinforce-
ment learning for user association and power control in heterogeneous
networks,” Ad Hoc Networks, vol. 102, p. 102069, 2020.

[11] H. Ye, G. Y. Li, and B.-H. F. Juang, “Deep reinforcement learning based
resource allocation for V2V communications,” IEEE Transactions on
Vehicular Technology, vol. 68, no. 4, pp. 3163–3173, 2019.

[12] Z. Bi and W. Zhou, “Deep reinforcement learning based power allocation
for D2D network,” in 2020 IEEE 91st Vehicular Technology Conference
(VTC2020-Spring).

IEEE, 2020, pp. 1–5.
[13] Z. Zhang, H. Qu, J. Zhao, and W. Wang, “Deep reinforcement learning
method for energy efﬁcient resource allocation in next generation
wireless networks,” in Proceedings of the 2020 International Conference
on Computing, Networks and Internet of Things, 2020, pp. 18–24.
[14] Y. Y. Munaye, R.-T. Juang, H.-P. Lin, G. B. Tarekegn, and D.-B. Lin,
“Deep reinforcement learning based resource management in UAV-
assisted IoT networks,” Applied Sciences, vol. 11, no. 5, p. 2163, 2021.
[15] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.

MIT press, 2018.

[16] K. Yang, C. Shen, and T. Liu, “Deep reinforcement learning based
wireless network optimization: A comparative study,” in IEEE INFO-
COM 2020-IEEE Conference on Computer Communications Workshops
(INFOCOM WKSHPS).
IEEE, Conference Proceedings, pp. 1248–
1253.

[17] Y. S. Nasir and D. Guo, “Deep reinforcement

learning for joint
spectrum and power allocation in cellular networks,” arXiv preprint
arXiv:2012.10682, 2020.

[18] Q. Liu, T. Han, N. Zhang, and Y. Wang, “Deepslicing: Deep rein-
forcement learning assisted resource allocation for network slicing,”
in GLOBECOM 2020-2020 IEEE Global Communications Conference.
IEEE, 2020, pp. 1–6.

[19] J. Kong, Z.-Y. Wu, M. Ismail, E. Serpedin, and K. A. Qaraqe, “Q-
learning based two-timescale power allocation for multi-homing hybrid
rf/vlc networks,” IEEE Wireless Communications Letters, vol. 9, no. 4,
pp. 443–447, 2020.

[20] F. B. Mismar, B. L. Evans, and A. Alkhateeb, “Deep reinforcement
learning for 5G networks: Joint beamforming, power control, and inter-
ference coordination,” IEEE Transactions on Communications, vol. 68,
no. 3, pp. 1581–1592, 2020.

[21] I. . W. Group et al., “IEEE standard for local and metropolitan area
networks-part 21: Media independent handover,” IEEE Std 802.21-2008,
pp. c1–c301, 2009.

[22] A. Zhou, B.-Y. Qu, H. Li, S.-Z. Zhao, P. N. Suganthan, and Q. Zhang,
“Multiobjective evolutionary algorithms: A survey of the state of the
art,” Swarm and evolutionary computation, vol. 1, no. 1, pp. 32–49,
2011.

[23] S. Boyd, S. P. Boyd, and L. Vandenberghe, Convex optimization.

Cambridge university press, 2004.

[24] A. A. Abdellatif, M. S. Allahham, A. Mohamed, A. Erbad, and
M. Guizani, “ONSRA: an optimal network selection and resource alloca-
tion framework in multi-RAT systems,” in ICC 2021-IEEE International
Conference on Communications.

IEEE, 2021, pp. 1–6.

[25] N. Zhao, Y.-C. Liang, D. Niyato, Y. Pei, M. Wu, and Y. Jiang, “Deep
reinforcement
learning for user association and resource allocation
in heterogeneous cellular networks,” IEEE Transactions on Wireless
Communications, vol. 18, no. 11, pp. 5141–5152, 2019.

[26] Q. Zhang, Y.-C. Liang, and H. V. Poor, “Intelligent user association
for symbiotic radio networks using deep reinforcement learning,” IEEE
Transactions on Wireless Communications, vol. 19, no. 7, pp. 4535–
4548, 2020.

[27] Z. Li, C. Wang, and C.-J. Jiang, “User association for load balancing in
vehicular networks: An online reinforcement learning approach,” IEEE
Transactions on Intelligent Transportation Systems, vol. 18, no. 8, pp.
2217–2228, 2017.

[28] Y. Chen, J. Li, W. Chen, Z. Lin, and B. Vucetic, “Joint user association
and resource allocation in the downlink of heterogeneous networks,”
IEEE Transactions on Vehicular Technology, vol. 65, no. 7, pp. 5701–
5706, 2016.

[29] Y. Choi, H. Kim, S.-w. Han, and Y. Han, “Joint resource allocation for
parallel multi-radio access in heterogeneous wireless networks,” IEEE
Transactions on Wireless Communications, vol. 9, no. 11, pp. 3324–
3329, 2010.

[30] J. Miao, Z. Hu, C. Wang, R. Lian, and H. Tian, “Optimal resource
allocation for multi-access in heterogeneous wireless networks,” in 2012
IEEE 75th Vehicular Technology Conference (VTC Spring).
IEEE,
Conference Proceedings, pp. 1–5.

[31] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
D. Silver, and D. Wierstra, “Continuous control with deep reinforcement
learning,” arXiv preprint arXiv:1509.02971, 2015.

[32] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”

arXiv preprint arXiv:1412.6980, 2014.

[33] S. Diamond and S. Boyd, “CVXPY: A Python-embedded modeling lan-
guage for convex optimization,” Journal of Machine Learning Research,
vol. 17, no. 83, pp. 1–5, 2016.

