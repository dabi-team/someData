0
2
0
2

y
a
M
4

]

C
O
.
h
t
a
m

[

1
v
7
2
6
1
0
.
5
0
0
2
:
v
i
X
r
a

April 2020

Multiagent Value Iteration Algorithms in

Dynamic Programming and Reinforcement Learning

Dimitri Bertsekas†

Abstract

We consider inﬁnite horizon dynamic programming problems, where the control at each stage consists of

several distinct decisions, each one made by one of several agents.

In an earlier work we introduced a

policy iteration algorithm, where the policy improvement is done one-agent-at-a-time in a given order, with
knowledge of the choices of the preceding agents in the order. As a result, the amount of computation for each

policy improvement grows linearly with the number of agents, as opposed to exponentially for the standard
all-agents-at-once method. For the case of a ﬁnite-state discounted problem, we showed convergence to an

agent-by-agent optimal policy. In this paper, this result is extended to value iteration and optimistic versions
of policy iteration, as well as to more general DP problems where the Bellman operator is a contraction

mapping, such as stochastic shortest path problems with all policies being proper.

1. MULTIAGENT PROBLEM FORMULATION

We consider an abstract form of inﬁnite horizon dynamic programming (DP) problem, which contains as

special case ﬁnite-state discounted Markovian decision problems (MDP), as well as more general problems

where the Bellman operator is a monotone weighted sup-norm contraction. The distinguishing feature of

the problem is that the control u consists of m components uℓ, ℓ = 1, . . . , m, where m > 1:

u = (u1, . . . , um).

(1.1)

Conceptually, each component may be viewed as chosen by a distinct agent, with knowledge of the selections

of the other agents. We consider value iteration (VI) algorithms that involve minimization component-by-

component as opposed to minimization over all components at once. This is similar to what is done in

coordinate descent methods for multivariable optimization, and can lead to dramatic gains in computational

eﬃciency for large and even moderate values of m. We propose several methods and we show their conver-

gence to an agent-by-agent optimal policy, a type of policy that is related to the notion of person-by-person

optimality from the theory of teams. Our analysis extends and complements our earlier proposals of rollout

and policy iteration (PI) algorithms [Ber20].

† McAfee Professor of Engineering, MIT, Cambridge, MA, and Fulton Professor of Computational Decision

Making, ASU, Tempe, AZ.

1

 
 
 
 
 
 
We assume that u is chosen from a ﬁnite constraint set U (x) when the system is at state x. In our

earlier paper [Ber20], we have made a stronger assumption: we assumed that each control component uℓ,

ℓ = 1, . . . , m, is separately constrained to lie in a given ﬁnite set Uℓ(x). In this case U (x) is the Cartesian

product set

U (x) = U1(x) × · · · × Um(x).

(1.2)

In this paper, we do not impose this assumption, except occasionally to discuss its implications. As a result

our algorithms must ensure that the selection of a control component at a given state and stage does not

preclude the feasibility of selection of the other control components at the same state and stage. This

complicates our algorithms relative to the Cartesian product case (1.2). We will discuss the mechanism for

dealing with this issue in Section 2. For the remainder of this section, we will assume no special structure

for the constraint set U (x) other than ﬁniteness.

The α-Discounted MDP Case

A major context for application of our algorithmic ideas is the standard inﬁnite horizon discounted MDP

with states x = 1, . . . , n. Here, at state x, a control u is applied, and the system transitions to a next

state y with transition probabilities pxy(y) and cost g(x, u, y). The control is chosen at state x from a ﬁnite

constraint set U (x). The cost function of a stationary policy µ that applies control µ(x) ∈ U (x) at state x

is denoted by Jµ(x), and the optimal cost [the minimum over µ of Jµ(x)] is denoted by J *(x).

The standard VI algorithm starts from some initial guess J 0 and iterates as follows:†

J k+1 = T J k,

k = 0, 1, . . . ,

where T is the Bellman operator, which maps a vector J =

J(1), . . . , J(n)
(cid:1)

(cid:0)

to the vector

T J =

(T J)(1), . . . , (T J)(n)
(cid:1)
(cid:0)

according to

(T J)(x) = min

u∈U(x)

n

X
y=1

g(x, u, y) + αJ(y)
pxy(u)
(cid:1)
(cid:0)

,

x = 1, . . . , n.

(1.3)

Thus each VI involves a comparison of all the Q-factors

Q(x, u) =

n

X
y=1

g(x, u, y) + αJ(y)
pxy(u)
(cid:1)
(cid:0)

,

x = 1, . . . , n, u ∈ U (x).

(1.4)

† Throughout the paper, we will be using componentwise equality and inequality notation, whereby for any pair

of real-valued functions J, J ′

, we write J = J ′

(or J ≤ J ′

) if J(x) = J ′

(x) [or J(x) ≤ J ′

(x), respectively] for all

x ∈ X.

2

A related algorithm is optimistic PI , which involves simultaneous value and policy iterations, using the

Bellman operator Tµ deﬁned for each policy µ by

(TµJ)(x) =

n

X
y=1

pxy

µ(x)
(cid:0)

(cid:1)(cid:16)g

x, µ(x), y
(cid:0)

(cid:1)

+ αJ(y)(cid:17),

x = 1, . . . , n.

(1.5)

Given a pair (µk, J k), this algorithm generates (µk+1, J k+1) according to

Tµk+1J k = T J k,

J k+1 = T q

µk+1J k,

k = 0, 1, . . . ,

(1.6)

where q is a positive integer (which in some cases may depend on k), and T q

µ denotes the mapping obtained

by q-fold application of the mapping Tµ. When q = 1 we obtain the VI algorithm J k+1 = T J k and when

q → ∞, we have J k+1 = Jµk (in the limit), so the algorithm approaches the standard PI algorithm where
µk+1 is obtained from µk according to

Tµk+1 Jµk = T Jµk .

(1.7)

Unfortunately, iterating with the mapping T is inconvenient for problems involving even a moderate

number of agents, because the size of the control constraint set U (x) typically grows exponentially with

m.

In particular, in the Cartesian product case (1.2), if each constraint set Uℓ(x) consists of at most s

elements, minimization over U (x) involves a comparison of as many as sm Q-factors of the form (1.4). This

motivates us to consider versions of the preceding algorithms that involve a simpler form of minimization. For

example, minimization over the component constraint sets Uℓ(x), one component at a time, which involves

a comparison of s Q-factors for each agent, for a total of s · m Q-factors.

The General Contractive DP Case

It is convenient and useful to develop our algorithm in a more general setting, which involves an operator-

based framework from the author’s abstract DP book [Ber18]. In particular, we consider a ﬁnite set X of

states and a ﬁnite set U of controls, and for each x ∈ X, a nonempty control constraint set U (x) ⊂ U .† We

denote by M the set of all functions µ : X 7→ U with µ(x) ∈ U (x) for all x ∈ X, which we refer to as policies.

We introduce a mapping H : X × U × R(X) 7→ ℜ, where ℜ denotes the real line and R(X) denotes the set

of real-valued functions J : X 7→ ℜ. For each policy µ ∈ M, we consider the mapping Tµ : R(X) 7→ R(X)

deﬁned by

(TµJ)(x) = H

x, µ(x), J
(cid:0)

(cid:1)

,

x ∈ X.

† The abstract DP framework of [Ber18] does not require ﬁniteness of the state and control spaces. We impose

the ﬁniteness assumption in order to obtain the most powerful algorithmic results possible. However, at several

points in the paper, and particularly in Section 5, we speculate around the possibility of extending our algorithms

and analysis to inﬁnite state and control spaces.

3

We also consider the mapping T deﬁned by

(T J)(x) = min

u∈U(x)

H(x, u, J) = min
µ∈M

(TµJ)(x),

x ∈ X.

Note that the α-discounted MDP is obtained when H is given by

H(x, u, J) =

n

X
y=1

g(x, u, y) + αJ(y)
pxy(u)
(cid:1)
(cid:0)

,

x = 1, . . . , n.

(1.8)

The problem is to ﬁnd a function J * ∈ R(X) such that

J *(x) = min
u∈U(x)

H(x, u, J *) = (T J *)(x),

x ∈ X,

i.e., to ﬁnd a ﬁxed point of T within R(X) (we can view J ∗ = T J * as a generalized form of Bellman’s

equation). We also want to obtain a policy µ∗ ∈ M such that Tµ∗J * = T J *. We assume that the control u

consists of the m components uℓ, ℓ = 1, . . . , m, [cf. Eq. (1.1)]. Note that since the state and control spaces

are assumed ﬁnite, the control constraint set U (x) and the set of policies M are also ﬁnite, so the minimum

of various expressions over U (x) or M is attained.

We will adopt throughout the following monotonicity and contraction assumptions.

Assumption 1.1: (Monotonicity)

If J, J ′ ∈ R(X) and J ≤ J ′, then

H(x, u, J) ≤ H(x, u, J ′),

∀ x ∈ X, u ∈ U (x).

For the contraction assumption, we introduce a function v : X 7→ ℜ with

We consider the weighted sup-norm

v(x) > 0,

∀ x ∈ X.

kJk = max
x∈X

J(x)
(cid:12)
(cid:12)
v(x)

(cid:12)
(cid:12)

on R(X), the space of real-valued functions J on X.

Assumption 1.2: (Contraction)

For some α ∈ (0, 1), we have

kTµJ − TµJ ′k ≤ αkJ − J ′k,

∀ J, J ′ ∈ R(X), µ ∈ M.

4

The monotonicity and contraction assumptions are satisﬁed in the α-discounted ﬁnite-state MDP case

(1.8), as well as other ﬁnite-state DP problems, such as stochastic shortest path problems in the special

case where all policies are proper; see the books [BeT96], [Ber12], [Ber18] for an extensive discussion. In

particular, for the α-discounted MDP, Tµ is a contraction with respect to the unweighted sup-norm with

contraction modulus α, whereas in the stochastic shortest path case, Tµ is a contraction with respect to a

weighted sup-norm with weights and contraction modulus that depend on the maximum expected time to

reach the destination using proper policies (see [BeT96], Prop. 2.2).

General abstract DP models under Assumptions 1.1 and 1.2 have been investigated in detail in the

author’s monograph [Ber18] [without assuming ﬁniteness of X and U , but with R(X) replaced by the set

B(X) of all uniformly bounded functions over X, equipped with a weighted sup-norm]. The main results are

that T is a contraction mapping and has as unique ﬁxed point the optimal cost function J * (the equation

J * = T J * is Bellman’s equation). Also Jµ is the unique ﬁxed point of Tµ. Moreover µ is optimal if and

only if TµJ * = T J * (or equivalently TµJµ = T Jµ). Algorithmic results include the convergence of VI [i.e.,

T kJ → J * for all J ∈ R(X)], and also convergence results for the PI algorithm (1.7) and some of its

variations. We will be using these results in what follows in this paper, with the monograph [Ber18] as a

general reference. For parts of our analysis, only the monotonicity and contraction Assumptions 1.1 and 1.2

are essential: the ﬁniteness of the state and control spaces can be eliminated with minor mathematical proof

modiﬁcations.

2. AGENT-BY-AGENT VALUE ITERATION

The salient feature of the multiagent DP problem of this paper is that the control u consists of m components,

u = (u1, . . . , um);

cf. Eq. (1.1). We will aim to develop a computationally eﬃcient variant of the standard VI algorithm

J k+1 = T J k, i.e.,

J k+1(x) =

min
(u1,...,um)∈U(x)

H(x, u1, . . . , um, J k),

x ∈ X.

Rather than simultaneous minimization over all the components u1, . . . , um, our multiagent VI algorithm

involves sequential minimization of H(x, u1, . . . , um, J k) over a single component uℓ, with the remaining
components uℓ′, ℓ′

6= ℓ, ﬁxed at the values obtained through the preceding minimizations. We maintain

these control component values in a policy that is continually updated to incorporate the results of new

minimizations.

Let µ be a given policy that applies at x the control

µ(x) =

µ1(x), . . . , µm(x)
(cid:1)
(cid:0)

.

5

We deﬁne a constraint set for the ℓth control component uℓ that is given by

Uℓ,µ(x) = nuℓ |

µ1(x), . . . , µℓ−1(x), uℓ, µℓ+1(x), . . . , µm(x)
(cid:1)
(cid:0)

∈ U (x)o,

ℓ = 1, . . . , m.

(2.1)

Note that since a policy µ by deﬁnition satisﬁes the feasibility constraint

µ1(x), . . . , µm(x)
(cid:1)
(cid:0)

∈ U (x),

x ∈ X,

the set Uℓ,µ(x) contains µℓ(x), so it is nonempty. Note also that when U (x) has the Cartesian product form

U1(x) × · · · × Um(x), the set Uℓ,µ(x) is simply equal to Uℓ(x) for all µ.

Our algorithm generates a double sequence {J k, µk}, starting from some pair (J 0, µ0): at the kth

iteration, given (J k, µk), the algorithm obtains (J k+1, µk+1) after m successive minimizations, one for each

of the components uℓ, ℓ = 1, . . . , m. In particular, given the typical pair (J, µ), our algorithm generates the
next pair ( ˜J, ˜µ) as the last of a sequence of cost function-policy component pairs

( ˆJ1, ˆµ1), ( ˆJ2, ˆµ2), . . . , ( ˆJm, ˆµm),

to be deﬁned shortly, i.e., it sets

˜J(x) = ˆJm(x),

˜µ(x) =

ˆµ1(x), . . . , ˆµm(x)
(cid:1)

,

(cid:0)

x ∈ X.

The cost function-policy pairs (2.2) are obtained as follows:

(2.2)

(2.3)

For every ℓ = 1, . . . , m, given ( ˆJℓ−1, ˆµ1, . . . , ˆµℓ−1), the algorithm generates ( ˆJℓ, ˆµℓ) according to

ˆJℓ(x) =

min
uℓ∈Uℓ,(ˆµ1,...,ˆµℓ−1,µℓ ,...,µm)(x)

H

ˆµℓ(x) ∈

arg min
uℓ∈Uℓ,(ˆµ1,...,ˆµℓ−1,µℓ,...,µm)(x)

H

x, ˆµ1(x), . . . , ˆµℓ−1(x), uℓ, µℓ+1(x), . . . , µm(x), ˆJℓ−1(cid:1)
,
(cid:0)
x, ˆµ1(x), . . . , ˆµℓ−1(x), uℓ, µℓ+1(x), . . . , µm(x), ˆJℓ−1(cid:1)
,
(cid:0)

x ∈ X, (2.4)

x ∈ X, (2.5)

where the constraint set in the two preceding minimizations,

Uℓ,(ˆµ1,...,ˆµℓ−1,µℓ,...,µm)(x),

is deﬁned by Eq. (2.1); it is the set of uℓ, which are consistent (in terms of feasibilility) with the previously

chosen components ˆµ1(x), . . . , ˆµℓ−1(x) and the component choices µℓ+1(x), . . . , µm(x) speciﬁed by the policy
µ. To start this process, only the initial function ˆJ0 is needed (in addition to µ), and it is given by

ˆJ0(x) = J(x),

x ∈ X.

(2.6)

Note that each of the minimizations (2.4) is performed for every state x ∈ X, and that there may be

multiple possible policies ˜µ that can be generated by this process [cf. Eq. (2.3)], since the minimum in Eq.

(2.5) may not be uniquely attained. This set of policies is denoted by
M(J, µ). Similarly, there may be
f
multiple possible functions ˜J that can be generated by this process [since the minimization (2.4) is aﬀected

by the multiplicity of possible policies ˆµ1, . . . , ˆµℓ−1], and this set of functions is denoted by

summary, our multiagent VI algorithm, starting from (J k, µk), generates (J k+1, µk+1) according to

J (J, µ).
e

In

J k+1 ∈

J (J k, µk),
e

µk+1 ∈

M(J k, µk).
f

(2.7)

6

Optimistic and Asynchronous PI Algorithms

In the preceding algorithm (2.7), each iteration involves a policy improvement operation, i.e., an m-step

minimization that cycles through all control components one-by-one. In Section 3, we will also consider an

optimistic PI variant where the m-step minimization is performed for only an inﬁnite subset K ⊂ {0, 1, . . .} of

the iterations, while for the complementary subset of iterations, k /∈ K, we use the (less expensive) standard

policy evaluation update J k+1 = Tµk J k, and no policy update:

J k+1 ∈

J (J k, µk),
e

J k+1 = Tµk J k,

µk+1 ∈

M(J k, µk),
f

µk+1 = µk,

∀ k /∈ K.

∀ k ∈ K,

(2.8)

(2.9)

We call the algorithm (2.8)-(2.9) multiagent optimistic PI . It is a natural multiagent extension of the standard

(single agent) optimistic PI algorithm (1.6), which is described in many sources for MDP and other problems,

e.g., [Ber12], [Ber18], [Ber19]. Note that when K = {0, 1, . . .}, the optimistic PI algorithm is the same as the

multiagent VI algorithm (2.7). In cases where K is a “small” subset of {0, 1, . . .}, the multiagent optimistic PI

algorithm involves nearly exact policy evaluations and “approaches” the multiagent PI algorithm proposed

in our earlier paper [Ber20].

In the preceding multiagent algorithms (2.7) and (2.8)-(2.9), the iterations are performed simultane-

ously for all states x ∈ X. In Section 4, we will also consider an asynchronous distributed version of the

multiagent optimistic PI algorithm (2.8)-(2.9), whereby iteration k is performed for only a subset Xk of the

states. There is a requirement here is that each state x belongs inﬁnitely often to some subset Xk, so that

there are inﬁnitely many policy improvements at every state. This algorithm is well suited for distributed

asynchronous computation, involving a partition of the state space into subsets, and with a processor assigned

to each set of the partition.

3. CONVERGENCE TO AN AGENT-BY-AGENT OPTIMAL POLICY

We will prove that the mutiagent VI algorithm (2.7) converges to an agent-by-agent optimal policy, which

we deﬁne as follows.

Deﬁnition 3.1: (Agent-by-Agent Optimality) We say that a policy µ = {µ1, . . . , µm} is agent-

by-agent optimal if for all x ∈ X and ℓ = 1, . . . , m, we have

H

x, µ1(x), . . . , µm(x), Jµ
(cid:0)

(cid:1)

= min

uℓ∈Uℓ,µ(x)

H

x, µ1(x), . . . , µℓ−1(x), uℓ, µℓ+1(x), . . . , µm(x), Jµ
(cid:0)

,
(cid:1)

(3.1)

where the constraint set Uℓ,µ(x) is deﬁned by Eq. (2.1).

7

To interpret this deﬁnition, let a policy µ = {µ1, . . . , µm} be given, and consider for every ℓ = 1, . . . , m
6= ℓ the ℓ′th policy components are ﬁxed at µℓ′, while the

the single agent DP problem where for all ℓ′

ℓth policy component is subject to optimization. The deﬁnition (3.1) is the optimality condition for all the

single agent problems; see [Ber18], Chapter 2 [Eq. (3.1) can be written as Tµ,ℓJµ = TℓJµ, where Tℓ and Tµ,ℓ

are the Bellman operators (1.3) and (1.5) that correspond to the single agent problem involving agent ℓ].

We can then conclude that µ = {µ1, . . . , µm} is agent-by-agent optimal if each component µℓ is optimal for

the ℓth single agent problem, where it is assumed that the remaining policy components remain ﬁxed; in

other words by using µℓ, each agent ℓ acts optimally, assuming all other agents ℓ′ 6= ℓ continue to use the

corresponding policy components µℓ′ .

Our deﬁnition of an agent-by-agent optimal policy is related to the notion of “person-by-person”

optimality from team theory, which has been studied primarily in the context of multiagent decision problems

with nonclassical information patterns, whereby the agents may not share the information on which they

base their decision. Thus team problems do not assume the shared information pattern that is characteristic

of DP problems. For the origins of team theory and control with a nonclassical information pattern, we

refer to Marschak [Mar55], Radner [Rad62], and Witsenhausen [Wit71a], [Wit71b], [Wit88]. For a sampling

of subsequent works, we refer to the survey by Ho [Ho80], and the papers by Krainak, Speyer, and Marcus

[KLM82a], [KLM82b], de Waal and van Schuppen [WaS00]. For more recent works, see Nayyar, Mahajan,

and Teneketzis [NMT13], Nayyar and Teneketzis [NaT19], Li et al. [LTZ19], Gupta [Gu20], the book by

Zoppoli, Parisini, Baglietto, and Sanguineti [ZPB19], and the references quoted there.

Note that an (overall) optimal policy is agent-by-agent optimal, but the reverse may not be true. This

is similar to properties of person-by-person optimal solutions in team theory. It is also similar to what may

happen in coordinate descent methods for multivariable optimization, where it is possible (in the absence

of favorable assumptions) to stop at a nonoptimal point where no progress can be made along any one

coordinate; some examples involving a Cartesian product constraint set of the form (1.2) are given in the

paper [Ber20].

While an agent-by-agent optimal policy may be either optimal or adequate for practical purposes, it

may oﬀer no guarantees of quality. For a simple example, let U (x) be the intersection of a Cartesian product

of ﬁnite subsets Uℓ(x) of the real line and the unit simplex:

U (x) =

U1(x) × · · · × Um(x)

∩ {u | u1 + · · · + um = 1}.

(cid:8)

(cid:9)

Then it can be seen that the constraint set Uℓ,µ(x) consists of just the single point µℓ(x), so that all

feasible policies are agent-by-agent optimal. This is due to the extreme coupling of the control components

through the simplex constraint.

It would not happen if the constraint set was just a Cartesian product

U1(x) × · · · × Um(x), in which case Uℓ,µ(x) = Uℓ(x) for all ℓ. Nonetheless, one should be aware that

the method of partitioning of the control into components may seriously impact the eﬀectiveness of our

8

multiagent VI algorithm through the creation of spurious agent-by-agent optimal policies.

We will now prove our main convergence result, under the following assumption, which is reminiscent

of strict convexity assumptions in the analysis of coordinate descent methods (see e.g., [Ber16], Section 3.7).

While we do not have a concrete counterexample, we speculate based on experience with coordinate descent

methods, that the assumption cannot be easily dispensed with.

Assumption 3.1: (Uniqueness Property) The cost functions of district policies are distinct, i.e.,

for any two policies µ and µ′

µ 6= µ′

⇒

Jµ 6= Jµ′ .

Our convergence result also assumes that the initial condition (J 0, µ0) satisﬁes

Tµ0 J 0 ≤ J 0.

(3.2)

This assumption is unnecessary for the α-discounted MDP where T and Tµ are given by Eqs. (1.3) and (1.5).
0

The reason is that if we replace J 0 by a function J

obtained by shifting J 0 by a constant c [i.e., replace

J 0(x) by J 0(x) + c for all x], we will have

0

(Tµ0J

)(x) = (Tµ0 J 0)(x) + αc ≤ J 0(x) + c = J

0

(x),

provided c is large enough, thereby satisfying the assumption (3.2). At the same time, it can be seen that by

replacing J 0 with J

0

the generated policies will not be aﬀected, while the generated iterates J k will just be

shifted by an appropriate constant. Thus for discounted MDP the assumption (3.2) is unnecessary for the

following convergence result, since the same sequence of policies will be obtained whether we use J 0 or J

0

.

For other types of problems the assumption (3.2) is needed. However, thanks to the contraction

property of Assumption 1.2, it can be typically satisﬁed by adding to J 0(x) a suﬃciently large constant c

for all x. In particular, any function J that satisﬁes

αkJ − Jµk ≤

J(x)
v(x)

−

Jµ(x)
v(x)

,

∀ x ∈ X, µ ∈ M,

(3.3)

(for example a suﬃciently large constant function) also satisﬁes the condition (3.2). To see this, note that

for J ≤ J and x ∈ X,

(TµJ)(x)
v(x)

≤

(TµJ)(x)
v(x)

≤

(TµJµ)(x)
v(x)

+ αkJ − Jµk =

Jµ(x)
v(x)

+ αkJ − Jµk ≤

J(x)
v(x)

,

9

where the ﬁrst inequality follows from the monotonicity of H, the second inequality follows by applying the

contraction property with J = J, J ′ = Jµ, and the third inequality is Eq. (3.3). Thus, for J satisfying Eq.

(3.3), we have TµJ ≤ J for all µ ∈ M.

Proposition 3.1: (VI Convergence to an Agent-by-Agent Optimal Policy) Let Assumptions

1.1, 1.2, and 3.1 hold, and assume further that the state and control spaces X and U are ﬁnite, and

that the initial pair (J 0, µ0) satisﬁes Eq. (3.2). Let {J k, µk} be a sequence generated by the agent-

by-agent VI algorithm (2.7). Then there is an agent-by-agent optimal policy ¯µ and an index k such

that for all k ≥ k, we have µk = ¯µ, and

kJ k+1 − J¯µk ≤ αkJ k − J¯µk,

(3.4)

while the sequence {J k} converges to J¯µ.

Proof: The critical step of the proof is to show that for all (J, µ) with

TµJ ≤ J,

and all ( ˜J, ˜µ) with ˜J ∈

holds

J (J, µ) and ˜µ ∈
e

M(J, µ) [cf. Eq. (2.7)], the following monotone decrease inequality
f

T˜µ ˜J ≤ ˜J = ˆJm ≤ ˆJm−1 ≤ ˆJm−2 ≤ · · · ≤ ˆJ1 ≤ TµJ ≤ J,

where for all ℓ = 1, . . . , m, and x ∈ X,

ˆJℓ(x) =

T(ˆµ1,...,ˆµℓ,µℓ+1,...,µm)
(cid:0)

(x)

ˆJℓ−1(cid:1)
H

=

min
uℓ∈Uℓ,(ˆµ1,...,ˆµℓ−1,µℓ,...,µm)(x)

x, ˆµ1(x), . . . , ˆµℓ−1(x), uℓ, µℓ+1(x), . . . , µm(x), ˆJℓ−1(cid:1)
,
(cid:0)

(3.5)

(3.6)

with ˆJ0 = J [cf. Eq. (2.4)], and

ˆµℓ(x) ∈

arg min
uℓ∈Uℓ,(ˆµ1,...,ˆµℓ−1,µℓ ,...,µm)(x)

H

x, ˆµ1(x), . . . , ˆµℓ−1(x), uℓ, µℓ+1(x), . . . , µm(x), ˆJℓ−1(cid:1)
,
(cid:0)

[cf. Eq. (2.5)].

Indeed the relation (3.5) is proved starting from the right side, which is the assumption

(3.2), and by using the deﬁnition of the algorithm, and the monotonicity Assumption 1.1 to prove ﬁrst that
ˆJ1 ≤ TµJ ≤ J, and then by proceeding sequentially to the inequality ˆJm ≤ ˆJm−1. In particular, at the
typical step, assuming that ˆJℓ−1 ≤ ˆJℓ−2, we show that ˆJℓ ≤ ˆJℓ−1 by writing
x, ˆµ1(x), . . . , ˆµℓ−1(x), µℓ(x), µℓ+1(x), . . . , µm(x), ˆJℓ−2(cid:1)
,
(cid:0)
x, ˆµ1(x), . . . , ˆµℓ−1(x), µℓ(x), µℓ+1(x), . . . , µm(x), ˆJℓ−1(cid:1)
,
(cid:0)

ˆJℓ−1(x) = H

≥ H

≥

min
uℓ∈Uℓ,(ˆµ1,...,ˆµℓ−1,µℓ ,...,µm)(x)

H

x, ˆµ1(x), . . . , ˆµℓ−1(x), uℓ, µℓ+1(x), . . . , µm(x), ˆJℓ−1(cid:1)
,
(cid:0)

= ˆJℓ(x),

10

where the ﬁrst inequality follows by using the monotonicity Assumption 1.1 and the hypothesis ˆJℓ−1 ≤ ˆJℓ−2.
Finally, by applying T˜µ to the relation ˆJm ≤ ˆJm−1 to obtain T˜µ ˆJm ≤ T˜µ ˆJm−1, and by using the facts
˜J = ˆJm = T˜µ ˆJm−1, we obtain the leftmost relation T˜µ ˜J ≤ ˜J = ˆJm in Eq.(3.5). (Note that the contraction

assumption is not needed for the preceding argument, and this is useful for applying this line of proof in

other DP problem contexts.)

From Eq. (3.5), we see that the sequence of functions J k converges monotonically to some function J,

and the same is true for all the sequences of intermediate functions J k

1 , . . . , J k

m−1. For each ℓ, let the policies

(µk+1
1

, . . . , µk+1

ℓ

, µk

ℓ+1, . . . , µk

m)

be equal to some policy ¯µ[ℓ] =

¯µ1[ℓ], . . . , ¯µm[ℓ]
(cid:0)

(cid:1)

inﬁnitely often (such a policy exists since the set of policies

is ﬁnite). Then we will have for all x ∈ X and ℓ = 1, . . . , m,

ℓ (x) = H(x, ¯µ[ℓ](x), J k
J k

ℓ−1) =

min
uℓ∈Uℓ,¯µ[ℓ](x)

H

x, ¯µ1[ℓ](x), . . . , ¯µℓ−1[ℓ](x), uℓ, ¯µℓ+1[ℓ](x), . . . , ¯µm[ℓ](x), J k
(cid:0)

,
ℓ−1(cid:1)

inﬁnitely often. By taking limit as k → ∞ and using the continuity of H(x, u, ·) (which is implied by the

contraction property of T¯µ[ℓ]), we have

J(x) = H

x, ¯µ[ℓ](x), J
(cid:0)

(cid:1)

= (T¯µ[ℓ]J)(x),

ℓ = 1, . . . , m, x ∈ X,

(3.7)

as well as

J(x) =

min
uℓ∈Uℓ,¯µ[ℓ](x)

H

x, ¯µ1[ℓ](x), . . . , ¯µℓ−1[ℓ](x), uℓ, ¯µℓ+1[ℓ](x), . . . , ¯µm[ℓ](x), J
(cid:0)

,
(cid:1)

ℓ = 1, . . . , m, x ∈ X.

(3.8)

Equation (3.7) and the contraction property of T¯µ[ℓ] imply that J is equal to the cost functions J¯µ[ℓ] of all of

the m policies ¯µ[ℓ], ℓ = 1, . . . , m. In view of the uniqueness Assumption 3.1, this implies that all the policies

¯µ[ℓ], ℓ = 1, . . . , m, are equal to some policy ¯µ, which has cost function J, and in view of Eq. (3.8), satisﬁes

J(x) = min

uℓ∈Uℓ,¯µ(x)

H

x, ¯µ1(x), . . . , ¯µℓ−1(x), uℓ, ¯µℓ+1(x), . . . , ¯µm(x), J
(cid:0)

,
(cid:1)

ℓ = 1, . . . , m.

(3.9)

It follows that ¯µ is agent-by-agent optimal.

Finally, the preceding argument shows that J is the cost function of every policy that is repeated

inﬁnitely often. Thus the uniqueness Assumption 3.1 implies that ¯µ is the only policy that is repeated

inﬁnitely often. Since there are ﬁnitely many policies, it follows that µk = ¯µ for all k after some index.

Hence from the deﬁnition of the algorithm, the sequence {J k} satisﬁes J k+1 = T¯µJ k for all k after some

index, which in view of the contraction Assumption 1.2, implies Eq. (3.4). Q.E.D.

Note that the preceding proposition does not guarantee convergence to the optimal policy (which is

unique by Assumption 3.1). In particular, if our algorithm is started at a pair (J¯µ, ¯µ), where ¯µ is an agent-

by-agent optimal policy, it will not move from ¯µ [in fact it can be shown that this will happen even if the

11

algorithm is started at a pair (J 0, ¯µ), where J 0 is suﬃciently close to J¯µ]. Thus every agent-by-agent optimal

policy behaves like a “local minimum,” with its own “region of attraction,” and is a potential convergence

limit of our algorithm. The limit will depend on the starting pair, as well as the order in which the agents

select their components. The algorithm guarantees convergence to the optimal policy only under additional

assumptions that guarantee that there are no additional agent-by-agent optimal policies. We postpone a

discussion of this issue for Section 5.

Ensuring Convergence to an Optimal Policy with Randomization Schemes

Another possibility to enhance the convergence properties of the algorithm, and ensure convergence to an

optimal policy, is to enlarge the constraint sets

Uℓ,(ˆµ1,...,ˆµℓ−1,µℓ,...,µm)(x)

in Eq. (2.4), to allow minimization over subsets of multiple control components. These subsets may be

selected with some form of randomization: at some iterations minimize over a single control component as

in iteration (2.4)-(2.5), while at some other randomly chosen iterations minimize over multiple or even all

control components. Schemes of this type have been considered for the purpose of enhancing the conver-

gence properties of asynchronous PI; see [Ber18], Section 2.5.3. Randomization over sets of multiple control

components can also be used in the context of the optimistic agent-by-agent PI methods of the next section,

and they can similarly enhance their convergence properties.

We will not consider randomized control component selection schemes in this paper. Their analysis

is similar to the one of [Ber18], Section 2.5.3, their implementation is likely problem-dependent, and their

practical performance is an interesting subject for further research. Their principal drawback is that simul-

taneous minimization over multiple control components can be very costly (depending on the number of

components involved), even if it used in only a small proportion of the total number of iterations.

4. AGENT-BY-AGENT OPTIMISTIC POLICY ITERATION

Let us now consider an optimistic PI variant where we introduce an inﬁnite subset K ⊂ {0, 1, . . .} of the

iterations, and the complementary subset of iterations k /∈ K. For the latter subset, we use the (less

expensive) standard policy evaluation update J k+1 = Tµk J k, and no policy update:

J k+1 ∈

J (J k, µk),
e

µk+1 ∈

M(J k, µk),
f

∀ k ∈ K,

J k+1 = Tµk J k,

µk+1 = µk,

∀ k /∈ K.

(4.1)

(4.2)

We have the following convergence result:

12

Proposition 4.1: (Optimistic PI Convergence to an Agent-by-Agent Optimal Policy) Let

the assumptions of Prop. 3.1 hold, and let {J k, µk} be a sequence generated by the optimistic agent-

by-agent PI algorithm (4.1)-(4.2). Then there is an index k such that for all k ≥ k, we will have

µk = ¯µ, where ¯µ is an agent-by-agent optimal policy, while the sequence {J k} will converge to J¯µ.

Proof: The proof is essentially identical to the one of Prop. 3.1. Q.E.D.

The algorithm admits also a distributed implementation, whereby the iteration (4.1)-(4.2) is executed

at the subset of times k ∈ K only for a subset Xk of the states, while for the remaining states x /∈ K the

values of J k+1(x) and µk+1(x) remain unchanged:

J k+1(x) = J k(x),

µk+1(x) = µk(x),

∀ x /∈ Xk, k ∈ K.

(4.3)

In addition to the set K being inﬁnite, there is a requirement here is that each state x belongs inﬁnitely

often to some subset Xk, so that there are inﬁnitely many policy improvements at every state. Algorithms

of this type have been proposed in the book [BeT96], Section 2.2.3, and in [Ber12]. The convergence proof of

Prop. 3.1 still goes through; see also the proof of Prop. 2.5 of [BeT96]. Note, however, that for this type of

algorithm to be provably convergent, (J 0, µ0) must satisfy the condition Tµ0J 0 ≤ J 0 [cf. Eq. (3.2)] even for

discounted MDP, as demonstrated with counterexamples by Williams and Baird [WiB93] (see also [Ber10]).

In a more complex version of the algorithm, the information on the cost function iterates at each

iteration is allowed to be out-of-date, while modiﬁcations are introduced to eliminate the need for the initial

condition assumption of Eq. (3.2). Distributed asynchronous PI algorithms of this type have been proposed

and analyzed in the paper by Bertsekas and Yu [BeY10] [see also [BeY12], [YuB13], and the books [Ber12]

(Section 2.6), and [Ber18] (Section 2.6)]. See also the randomized optimistic PI algorithms of [Ber18] (Section

2.5.3). Multiagent versions of such algorithms are a subject for further research.

5. CONDITIONS FOR OBTAINING AN OPTIMAL POLICY

We proved earlier that our multiagent VI algorithm will ﬁnd an agent-by-agent optimal policy under our

assumptions of Prop. 3.1, but this policy need not be optimal. We will now discuss approaches that can be

used to show that the policy obtained is optimal, under the same or alternative assumptions. One possibility

is to impose conditions under which every agent-by-agent optimal policy is optimal. To this end we introduce

the following deﬁnition.

13

Deﬁnition 5.1: (Component-by-Component Minimum)

For a state x and a function J ∈

R(X) we say that a control u = (u1, . . . , um) ∈ U (x) is a component-by-component minimum of H at

(x, J) if

uℓ ∈ arg min

uℓ∈U ℓ,¯u(x)

H(x, u1, . . . , uℓ−1, uℓ, uℓ+1, . . . , um, J),

ℓ = 1, . . . , m.

(5.1)

where the sets U ℓ,¯u(x) are deﬁned by

U ℓ,¯u(x) =

(cid:8)

uℓ | (u1, . . . , uℓ−1, uℓ, uℓ+1, . . . , um) ∈ U (x)

(cid:9)

,

ℓ = 1, . . . , m.

Note that from the deﬁnition of agent-by-agent optimality, we have that ¯µ is agent-by-agent optimal

if for every x ∈ X, the control ¯µ(x) is a component-by-component minimum of H at (x, J¯µ). We have the

following proposition.

Proposition 5.1: (Agent-by-Agent Optimality Criterion)

(a) Assume that for every state x ∈ X and policy ¯µ such that ¯µ(x) is a component-by-component

minimum of H at (x, J¯µ), the control ¯µ(x) minimizes H(x, u, J¯µ) over u ∈ U (x). Then every

agent-by-agent optimal policy is optimal.

(b) If an agent-by-agent optimal policy ¯µ is optimal, then for all x ∈ X the control ¯µ(x) is a

component-by-component minimum of H at all (x, J¯µ).

Proof:

(a) Let ¯µ be agent-by-agent optimal. Then from the deﬁnition of agent-by-agent optimality, we

have that for all x ∈ X, ¯µ(x) is a component-by-component minimum of H at (x, J¯µ). By our assumption,

this implies that for all x ∈ X, ¯µ(x) minimizes H(x, u, J¯µ) over u ∈ U (x), or T¯µJ¯µ = T J¯µ. From general

properties of contractive abstract DP models (cf. [Ber18], Chapter 2), we also have T¯µJ¯µ = J¯µ. Hence

T¯µJ¯µ = T J¯µ, which implies that J¯µ = J * (cf. [Ber18], Chapter 2), so ¯µ is optimal.

(b) Optimality of ¯µ implies that T¯µJ¯µ = T J¯µ (cf. [Ber18], Chapter 2). This implies that for all x ∈ X the

control ¯µ(x) is a component-by-component minimum of H at all (x, J¯µ). Q.E.D.

In view of Prop. 5.1(a), an important issue is to delineate suﬃcient conditions that guarantee that

14

component-by-component minima of H at (x, Jµ) minimize H(x, u, Jµ) over u ∈ U (x). Somewhat similar

questions have been addressed in two related contexts:

(a) Team theory in connection with the notion of person-by-person optimality mentioned earlier.

(b) The theory of convergence of coordinate descent methods in nonlinear optimization.

In the theory of teams and other related decentralized control problem formulations, the most prominent

analytical issues arise when the team members select control components based on diﬀerent information. By

contrast in our framework the agents choose actions based on shared information, namely the current state

xk of the system. Because of this fundamental structural assumption, DP algorithms such as VI and PI

apply to our problem, but do not apply to team problems with nonclassical information pattens, exempliﬁed

by the famous counterexample of Witsenhausen [Wit68].

In the theory of coordinate descent methods, the result most related to our context is that if a function

F (y1, . . . , ym) of m vectors y1, . . . , ym is strictly convex and diﬀerentiable over the Cartesian product Y1 ×

· · · × Ym of closed convex sets Y1, . . . , Ym, then a vector ¯y = (¯y1, . . . , ¯ym) is a global minimum of F over

Y1 × · · · × Ym if and only if it has the component-by-component minimization property

¯yℓ ∈ arg min
yℓ∈Yℓ

F (¯y1, . . . , ¯yℓ−1, yℓ, ¯yℓ+1, . . . , ¯ym),

for all ℓ = 1, . . . , m.

(5.2)

Thus when F is strictly convex and diﬀerentiable, the block coordinate descent method cannot get trapped

into a solution that is component-by-component minimum but is not a global minimum [this is not true,

however, if F is strictly convex but nondiﬀerentiable, since the condition (5.2) may hold at vectors ¯y that

are not global minima, and at which F is nondiﬀerentiable]. Some related results are known for the case

where the sets Yℓ are discrete, under assumptions that can be viewed as discrete space substitutes for strict

convexity; see e.g., de Waal and van Schuppen [WaS00], and Bauso and Pesenti [BaP08], [BaP12].

While the coordinate descent and the team theory results provide some analytical guidance, they do

not apply directly to the DP context of this paper. The reason is that the mapping H involves the functions

Jµ, whose properties have to be veriﬁed through analysis. We leave this line of investigation as a subject for

further research, and we outline another analytical approach, which assumes continuous state and control

spaces X and U , and is based on strict convexity and diﬀerentiability assumptions.

Continuous Spaces, Strict Convexity, and Diﬀerentiability

Let us remove the assumption that the state and control spaces X and U are ﬁnite, while continuing to assume

that the control has m components, u = (u1, . . . , um) that are constrained by u ∈ U (x) for all x ∈ X. We

continue to adopt the monotonicity and contraction Assumptions 1.1 and 1.2, with the modiﬁcation that

R(X) is replaced by the space B(X) of bounded functions over X, with respect to a weighted sup-norm.

15

Moreover, we assume that the various minima over control components in the deﬁnition of the algorithms

are attained. Models of this type have been analyzed extensively in the monograph [Ber18] (Chapter 2), to

which we refer for a detailed discussion. The deﬁnitions of agent-by-agent optimality and component-by-

component minimum carry over without change to the continuous spaces setting, and so does the associated

agent-by-agent optimality criterion [cf. Prop. 5.1(a)]. Furthermore, the key inequality (3.4) for the proof of

the convergence result of Prop. 3.1 goes through, under the condition Tµ0J 0 ≤ J 0 [cf. Eq. (3.2)]. As a result,

the proof of monotonic decrease of the sequence {J k} to some function J goes through as well.

In conclusion, without assuming ﬁniteness of the state and control spaces X and U , our algorithm,

under the monotonicity and contraction Assumptions 1.1 and 1.2, and the condition Tµ0J 0 ≤ J 0 [cf. Eq.

(3.2)], converges monotonically to some J, which can be seen to be pointwise bounded below by the optimal

cost function J *, which belongs to B(X), so that J ∈ B(X). Further conditions, involving strict convexity

and diﬀerentiability, need to be imposed to guarantee that J = J *, that J * is convex and diﬀerentiable, and

that an optimal policy can be obtained. A stochastic optimal control model, involving a linear system, a

convex cost per stage, and convex state and control constraints, was formulated and analyzed in 1973 by the

author [Ber73], and is well suited for this purpose. We leave further analysis along this line as a subject for

further research.

6. CONCLUDING REMARKS

We have shown that in the context of multiagent problems, agent-by-agent versions of the VI algorithm and

related optimistic PI algorithms have greatly reduced computational requirements, while still maintaining

a meaningful convergence property. While these algorithms may terminate with a suboptimal policy that

is agent-by-agent optimal, they can be dramatically more eﬃcient than the standard VI and optimistic PI

algorithms, which may be computationally intractable even for a moderate number of agents.

Several unresolved questions remain regarding algorithmic variations and conditions that guarantee

that our algorithms obtain an optimal policy rather than one that is agent-by-agent optimal. Approximate

versions of our algorithms of the type used in neuro-dynamic programming/reinforcement learning are also

of interest, and are a subject for further investigation. Moreover, the basic idea of our approach, simplifying

the minimization deﬁning the VI operator while maintaining some form of convergence guarantee, can be

extended in other directions to exploit special problem structures.

7. REFERENCES

[BaP08] Bauso, D., and Pesenti, R., 2008. “Generalized Person-by-Person Optimization in Team Problems

with Binary Decisions,” Proc. 2008 American Control Conference, pp. 717-722.

16

[BaP12] Bauso, D., and Pesenti, R., 2012. “Team Theory and Person-by-Person Optimization with Binary

Decisions,” SIAM Journal on Control and Optimization, Vol. 50, pp. 3011-3028.

[Ber73] Bertsekas, D. P., 1973. “Linear Convex Stochastic Control Problems Over an Inﬁnite Horizon,” IEEE

Transactions on Aut. Control, Vol. AC-18, pp. 314-315.

[BeT89] Bertsekas, D. P., and Tsitsiklis, J. N., 1989. Parallel and Distributed Computation: Numerical

Methods, Prentice-Hall, Englewood Cliﬀs, NJ; republished in 1996 by Athena Scientiﬁc, Belmont, MA.

[BeT96] Bertsekas, D. P., and Tsitsiklis, J. N., 1996. Neuro-Dynamic Programming, Athena Scientiﬁc, Bel-

mont, MA.

[BeY10] Bertsekas, D. P., and Yu, H., 2010. “Asynchronous Distributed Policy Iteration in Dynamic Pro-

gramming,” Proc. of Allerton Conf. on Communication, Control and Computing, Allerton Park, Ill, pp.

1368-1374.

[BeY12] Bertsekas, D. P., and Yu, H., 2012. “Q-Learning and Enhanced Policy Iteration in Discounted

Dynamic Programming,” Math. of OR, Vol. 37, pp. 66-94.

[Ber10] Bertsekas, D. P., 2010. “Williams-Baird Counterexample for Q-Factor Asynchronous Policy Itera-

tion,” http://web.mit.edu/dimitrib/www/Williams-Baird Counterexample.pdf.

[Ber12] Bertsekas, D. P., 2012. Dynamic Programming and Optimal Control, Vol. II, 4th edition, Athena

Scientiﬁc, Belmont, MA.

[Ber16] Bertsekas, D. P., 2016. Nonlinear Programming, 3rd Edition, Athena Scientiﬁc, Belmont, MA.

[Ber18] Bertsekas, D. P., 2018. Abstract Dynamic Programming, Athena Scientiﬁc, Belmont, MA; on-line at

http://web.mit.edu/dimitrib/www/RLbook.html.

[Ber19] Bertsekas, D. P., 2019. Reinforcement Learning and Optimal Control, Athena Scientiﬁc, Belmont,

MA.

[Ber20] Bertsekas, D. P., 2020. “Multiagent Rollout Algorithms and Reinforcement Learning,” arXiv preprint,

arXiv:2002.07407.

[Gup20] Gupta, A., 2020. “Existence of Team-Optimal Solutions in Static Teams with Common Information:

A Topology of Information Approach,” SIAM J. on Control and Optimization, Vol. 58, pp. 998-1021.

[Ho80] Ho, Y. C., 1980. “Team Decision Theory and Information Structures,” Proceedings of the IEEE, Vol.

68, pp. 644-654.

[KLM82a] Krainak, J. L. S. J. C., Speyer, J., and Marcus, S., 1982. “Static Team Problems - Part I:

Suﬃcient Conditions and the Exponential Cost Criterion,” IEEE Transactions on Automatic Control, Vol.

27, pp. 839-848.

[KLM82b] Krainak, J. L. S. J. C., Speyer, J., and Marcus, S., 1982. “Static Team Problems - Part II: Aﬃne

17

Control Laws, Projections, Algorithms, and the LEGT Problem,” IEEE Transactions on Automatic Control,

Vol. 27, pp. 848-859.

[LTZ19] Li, Y., Tang, Y., Zhang, R., and Li, N., 2019. “Distributed Reinforcement Learning for De-

centralized Linear Quadratic Control: A Derivative-Free Policy Optimization Approach,” arXiv preprint

arXiv:1912.09135.

[Mar55] Marschak, J., 1975. “Elements for a Theory of Teams,” Management Science, Vol. 1, pp. 127-137.

[NMT13] Nayyar, A., Mahajan, A. and Teneketzis, D., 2013. “Decentralized Stochastic Control with Partial

History Sharing: A Common Information Approach,” IEEE Transactions on Automatic Control, Vol. 58,

pp. 1644-1658.

[NaT19] Nayyar, A. and Teneketzis, D., 2019. “Common Knowledge and Sequential Team Problems,” IEEE

Transactions on Automatic Control, Vol. 64, pp. 5108-5115.

[Rad62] Radner, R., 1962. “Team Decision Problems,” Ann. Math. Statist., Vol. 33, pp. 857-881.

[WiB93] Williams, R. J., and Baird, L. C., 1993. “Analysis of Some Incremental Variants of Policy Itera-

tion: First Steps Toward Understanding Actor-Critic Learning Systems,” Report NU-CCS-93-11, College of

Computer Science, Northeastern Univ., Boston, MA.

[WaS00] de Waal, P. R., and van Schuppen, J. H., 2000. “A Class of Team Problems with Discrete Action

Spaces: Optimality Conditions Based on Multimodularity,” SIAM J. on Control and Optimization, Vol. 38,

pp. 875-892.

[Wit68] Witsenhausen, H., 1968. “A Counterexample in Stochastic Optimum Control,” SIAM Journal on

Control, Vol. 6, pp. 131?147.

[Wit71a] Witsenhausen, H., 1971. “On Information Structures, Feedback and Causality,” SIAM J. Control,

Vol. 9, pp. 149-160.

[Wit71b] Witsenhausen, H., 1971. “Separation of Estimation and Control for Discrete Time Systems,” Pro-

ceedings of the IEEE, Vol. 59, pp. 1557-1566.

[Wit88] Witsenhausen, H., 1988. “Equivalent Stochastic Control Problems,” Math. Control Signals Systems,

Vol. 1, pp. 3-11.

[YuB13] Yu, H., and Bertsekas, D. P., 2013. “Q-Learning and Policy Iteration Algorithms for Stochastic

Shortest Path Problems,” Annals of Operations Research, Vol. 208, pp. 95-132.

[ZPB19] Zoppoli, R., Parisini, T., Baglietto, M., and Sanguineti, M., 2019. Neural Approximations for

Optimal Control and Decision, Springer.

18

