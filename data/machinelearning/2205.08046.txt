2
2
0
2

p
e
S
5

]

G
L
.
s
c
[

3
v
6
4
0
8
0
.
5
0
2
2
:
v
i
X
r
a

Shape Complexity in Cluster Analysis

Eduardo J. Aguilar
Instituto de Ciˆencia e Tecnologia
Universidade Federal de Alfenas
Rod. Jos´e Aur´elio Vilela, 11999
37715-400 Po¸cos de Caldas - MG, Brazil

Valmir C. Barbosa∗
Programa de Engenharia de Sistemas e Computa¸c˜ao, COPPE
Universidade Federal do Rio de Janeiro
Centro de Tecnologia, Sala H-319
21941-914 Rio de Janeiro - RJ, Brazil

Abstract

In cluster analysis, a common ﬁrst step is to scale the data aiming
to better partition them into clusters. Even though many diﬀerent tech-
niques have throughout many years been introduced to this end, it is
probably fair to say that the workhorse in this preprocessing phase has
been to divide the data by the standard deviation along each dimension.
Like division by the standard deviation, the great majority of scaling tech-
niques can be said to have roots in some sort of statistical take on the
data. Here we explore the use of multidimensional shapes of data, aiming
to obtain scaling factors for use prior to clustering by some method, like
k-means, that makes explicit use of distances between samples. We bor-
row from the ﬁeld of cosmology and related areas the recently introduced
notion of shape complexity, which in the variant we use is a relatively
simple, data-dependent nonlinear function that we show can be used to
help with the determination of appropriate scaling factors. Focusing on
what might be called “midrange” distances, we formulate a constrained
nonlinear programming problem and use it to produce candidate scaling-
factor sets that can be sifted on the basis of further considerations of the
data, say via expert knowledge. We give results on some iconic data sets,
highlighting the strengths and potential weaknesses of the new approach.
These results are generally positive across all the data sets used.

Keywords: Cluster analysis, Data scaling, Distance-based clustering,
Shape complexity.

∗Corresponding author (valmir@cos.ufrj.br).

1

 
 
 
 
 
 
1

Introduction

The common wisdom regarding the processing of data prior to cluster analy-
sis, particularly when a distance-based clustering method like k-means or some
forms of hierarchical clustering are used, is that data should be scaled to improve
results. Even though researchers have been proliﬁc in creating domain-speciﬁc
forms of scaling (cf., e.g., [1]), already the earliest studies systematically ap-
proaching the subject viewed division by the standard deviation or by the range
in each dimension as the natural candidates they still are to this day [2,3]. This
is not to say that alternative divisors were not considered: they were [4], but the
situation seems to have remained largely unchanged until very recently, with the
introduction of the so-called pooled standard deviation [5], which continues to
support division by the standard deviation unless this would make the dimen-
sion in question lose information crucial to partitioning the data into clusters.
Should this be the case, a weighted averaged of the standard deviations local-
ized around statistically signiﬁcant modes in that dimension is used instead.
This average is the pooled standard deviation of the data in that dimension,
henceforth denoted by σpool
for dimension k. Notably, the essential motivation
k
for the creation of σpool
seems well aligned with concerns that have been voiced
since the late 1960’s (cf. [3] for comments on this).

k

In a similar vein, it has for several decades been clear that some form of opti-
mization problem must exist whose solution yields scale factors that make some
sort of sense for the various dimensions. And indeed this has been pursued,
though to the best of our knowledge not for the last three decades, at least.
Noteworthy representatives of these attempts include optimizing for a linear
transformation of the data [6]; maximizing the square of a correlation between
two sets of distances between samples [7]; a least-squares method for determin-
ing scale factors that make such distances approach those in the dendrogram
resulting from hierarchical clustering [8]; and determining scaling factors by
considering the modal structure of the data in each dimension in a way that,
to a certain degree, preﬁgures the above deﬁnition of the pooled standard devi-
ation [9]. Each of these approaches seems to have either disappointed its own
creator [6], or remained tailored to the generally uninteresting cases of nonover-
lapping clusters [7], or been tested only superﬁcially [8], or simply remained
untested [9].

Here we introduce the use of a shape-complexity function to guide the de-
termination of scale factors. By this denomination we are not referring to one
of the many forms of complexity used to characterize the computer representa-
tions of three-dimensional shapes [10]. Instead, we refer to a generalization to
multiple dimensions of the homonymous three-dimensional concept introduced
recently in cosmology and related disciplines [11,12]. If we imagine (up to three
dimensions) that the disposition of data samples in space gives the data set an
inherent shape, then clearly being able to shrink or stretch each dimension in-
dependently of all others is an important source of shape variation, one that we
explore for the purpose of cluster analysis. One particular facet of shape com-
plexity that we ﬁnd especially relevant to this end is that it allows what should

2

be intercluster distances to be considered side-by-side with distances that should
be intracluster. While normally cluster analysts know which distance is which
type only in a very limited manner, shape complexity provides a handle that
can help in posing a nonlinear programming problem for the automatic deter-
mination of scale factors (or rather, candidate scale factors to undergo further
scrutiny based on what analysts do know of the domain in question).

We proceed in the following manner. In Section 2 we introduce the form
of shape complexity we use, giving its deﬁnition and properties of interest. We
also discuss why it relates closely to the role of scale factors in cluster analysis
and how determining such factors from it can be formulated. We then proceed
with a description of our experimental setup, in Section 3. This includes the
data sets on which we experiment, the tools and algorithms we use, and how we
evaluate a data set’s partition into clusters. Importantly, we perform clustering
solely via the k-means method, owing mainly to its great potential to perform
well when clusters overlap [13], and also to its long history, during which many
implementations and variants have appeared [14]. Results and discussion are
given in Sections 4 and 5, respectively. We close in Section 6.

2 Shape complexity

We consider an norig × d data matrix X with norig, d > 1, where norig is the
number of d-dimensional real samples. For 1 ≤ k ≤ d, we use σ2
k to denote the
samples’ variance on dimension k, and αk > 0 to denote the scale factor to be
used on the samples along this dimension in order to facilitate clustering. Factor
αk is assumed to be applied to the various Xik’s along with their division by
σk. That is, each original Xik is to undergo scaling by the factor αk/σk. This
makes it easier to assess the eﬀect of factor αk relative to the more common
1/σk and also enables some key developments later in this section.

For reasons to be discussed shortly, here we propose that the appropriate
αk’s be determined with the guidance of the so-called shape complexity of the
norig points in d-dimensional real space that deﬁne the samples. This notion
is borrowed from the physics of multiple bodies interacting gravitationally with
one another. For d = 3 and the points having masses associated with them,
shape complexity has been shown to help account for structure as it arises in
the form of clusters during the system’s evolution [11, 12].

The version of shape complexity we use, denoted by SC, is given by

SC =

(cid:19)1/2

(cid:18)(cid:88)

i<j

r2
ij

r−1
ij ,

(cid:88)

i<j

(1)

where each i and j are distinct samples and rij is the Euclidean distance between
them. That is, the number of samples SC takes into account is n such that
1 < n ≤ norig (duplicates may thus exist only if norig > 2) and

(cid:88)

r2
ij =

kρ2
α2

ijk,

(2)

k

3

with

ρijk =

Xik − Xjk
σk

.

(3)

SC is therefore a function of the αk’s, but we refrain from denoting this
explicitly for the sake of notational clarity. Importantly, the use of i < j in the
summations of Eq. (1) indicates that they occur on the set of all (cid:0)n
(cid:1) unordered
pairs of distinct samples. Likewise, the summation on k in Eq. (2) indicates
that it occurs over all d dimensions.

2

Radial invariance. One of the key properties for which SC is appreciated in
its ﬁelds of origin is scale invariance, which in our terms is to be understood as
follows. If αk has the same value for every k, then clearly SC remains unchanged
however this common value is varied. But if SC is to be used to improve the
results of clustering algorithms on the data, setting every αk to the same value
is in general not an option. Scale invariance, nevertheless, is a special case of
the much more useful radial invariance we discuss next. The radial invariance
of SC can be seen in more than one way, but here we choose the perspective of
certain directional derivatives of SC. This requires us to already consider the
gradient of SC, which will be instrumental later on.

For f a diﬀerentiable function of the αk’s, we let f (cid:48)

kth component of the gradient of f , and moreover write g = ((cid:80)
h = (cid:80)

so that SC = gh. We get

k denote ∂f /∂αk, the
ij)1/2 and
i<j r2

i<j r−1

ij

SC(cid:48)

k = g(cid:48)

kh + gh(cid:48)
k,

where

and

k = αkg−1 (cid:88)
g(cid:48)

ρ2
ijk

i<j

h(cid:48)
k = −αk

r−3
ij ρ2

ijk.

(cid:88)

i<j

(4)

(5)

(6)

Radial invariance comes from realizing that the directional derivative of SC
is zero along any straight line extending out from the origin into the positive
d-dimensional real orthant, that is,

αkSC(cid:48)

k = 0

(cid:88)

k

(7)

for any valuation of the αk’s. To put it diﬀerently, SC has the same value at
1 , . . . , v(2)
any two assignments of values to α1, . . . , αd, say v(1)
d ,
such that v(2)

for every k and some t > 0.

d and v(2)

1 , . . . , v(1)

k = tv(1)

k

4

To see how Eq. (7) comes about, simply write

(cid:88)

g

αkSC(cid:48)

k = h

(cid:88)

α2
k

(cid:88)

ijk − g2 (cid:88)
ρ2

α2
k

(cid:88)

r−3
ij ρ2
ijk

k

k
(cid:88)

i<j
(cid:88)

= h

= h

(cid:88)

i<j
kρ2
α2

ijk − g2 (cid:88)

k
ij − g2 (cid:88)
r2

i<j
r−3
ij r2
ij

k

r−3
ij

i<j
(cid:88)

k

kρ2
α2

ijk

i<j
= hg2 − g2h.

i<j

(8)

(9)

(10)

(11)

The role of Eq. (2) in this development highlights a condition equivalent to
radial invariance: that the value of any rij becomes scaled by t when moving
from v(1)

to v(2)

1 , . . . , v(2)
d .

1 , . . . , v(1)

d

Shape complexity and clustering.
Increasing any rij always increases g
while decreasing h. Notably, the most signiﬁcant increases in g come from
increasing the largest rij’s (since ∂g/∂rij = g−1rij), while the most signiﬁcant
decreases in h come from increasing the smallest rij’s (since ∂h/∂rij = −r−2
ij ).
Because increases in the rij’s are mediated by increases in the αk’s, the eﬀect
of increasing any speciﬁc αk on the rij’s of speciﬁc relative magnitudes is best
understood by considering how the ratios g(cid:48)
k/h relate to each other.
Two cases must be considered, as follows.

k/g and −h(cid:48)

C1. If g(cid:48)

k/g > −h(cid:48)

k/h (i.e., increasing αk causes more of a relative increase in
g than a relative decrease in h), then larger rij’s are being increased more
than smaller rij’s.

C2. If g(cid:48)

k/g < −h(cid:48)

k/h (i.e., increasing αk causes more of a relative decrease
in h than a relative increase in g), then smaller rij’s are being increased
more than larger rij’s.

In the context of data clustering, assume for a moment that larger rij’s
are generally intercluster distances while smaller rij’s are generally intracluster
distances. Cases C1 and C2 above are then in strong opposition to each other,
as clearly case C1 could be good for clustering and case C2 bad for clustering.
We might then expect to be well-oﬀ if we targeted case C1 for every k, but surely
an assignment of values to α1, . . . , αd might satisfy case C1 for a speciﬁc k while
satisfying case C2 for another. In this case it would seem better to pursue the
intermediate goal of getting as close as possible to achieving g(cid:48)
k/h for
every k.

k/g = −h(cid:48)

Real-world data, however, rarely comply with the dichotomy we momen-
tarily assumed above.
Instead, quite often larger distances are intracluster,
and likewise smaller distances are intercluster. In any case, the centerpiece of
the strategy we adopt henceforth is the same that would be appropriate had
the dichotomy always held true, that is, seeking the equilibrium represented
by g(cid:48)
k/h for every k. On top of this, we essentially look for several

k/g = −h(cid:48)

5

scaling-factor schemes approaching such conditions as closely as possible and
select the one (or more than one) that upon closer inspection of the data leads
to a reasonable partition into clusters.

The optimization problem. A consequence of our discussion of the radial-
invariance property of SC is that all assignments of values to α1, . . . , αd on
any straight line emanating from the origin into the positive d-dimensional real
orthant are equivalent at providing scaling factors for distance-based clustering.
That is, choosing any such assignment will lead any distance-based clustering
algorithm to yield the same result. This follows from the fact that the rij’s for
a given assignment on that line are scaled versions, by the same factor on all
dimensions, of rij’s for any of the other assignments.

In what follows, all but one of such equivalent assignments are ignored. The

one that is taken into account is that for which

α2

k = d.

(cid:88)

k

(12)

√

That is, valid assignments of values to the αk’s must be on the d-dimensional
d centered at the origin. This choice of radius allows for
sphere of radius
αk = 1 for every k to be a valid assignment. This, we recall from earlier in this
section, is the assignment that scales the data along dimension k by the factor
1/σk.

k/g = −h(cid:48)

Seeking to approximate g(cid:48)

k/h for every k given this equality con-
straint boils down to the problem of ﬁnding a local minimum or maximum of
SC given the constraint. Because SC is inextricably based on the data to be
clustered, it seems to have no characteristic that can be directly exploited to
this end. We follow an indirect route and begin by considering the ﬁrst-order
necessary condition for local optimality in this case [15], which requires not
only the equality constraint in Eq. (12) to be satisﬁed but also the gradient
of the corresponding Lagrangian with respect to the αk’s to equal zero. The
Lagrangian in this case is

L = SC + λ(

(cid:88)

α2

k − d),

k

(13)

where λ is the Lagrange multiplier corresponding to the single equality con-
straint. Its gradient’s kth component is L(cid:48)
k + 2λαk. Writing this
in more detail yields

kh + gh(cid:48)

k = g(cid:48)

L(cid:48)

k = αk

(cid:18)

g−1h

ρ2
ijk − g

(cid:88)

i<j

(cid:88)

i<j

(cid:19)

r−3
ij ρ2

ijk + 2λ

,

(14)

from which it follows that, in order to achieve L(cid:48)

k = 0 for every k, we must have

r−3
ij

(cid:88)

i<j

ρ2
ijk
i<j ρ2

ijk

(cid:80)

g−1h + 2λ((cid:80)
g

i<j ρ2

ijk)−1

=

(15)

6

for each of them.

Even though it would seem that the right-hand side of Eq. (15) may have a
k can be

diﬀerent value for each k, letting N = norig(norig − 1) we note that σ2
written as

k = N −1 (cid:88)
σ2

(Xik − Xjk)2,

which leads to

i<j

ρ2
ijk = N.

(cid:88)

i<j

(16)

(17)

The right-hand side of Eq. (15) is therefore the same for every k, so its left-hand
side, which is in fact the inner product of two vectors in (cid:0)n
(cid:1)-dimensional real
space, must also not depend on k. The ijth component of one of the two vectors
involved in this inner product is N −1ρ2
ijk, so all d such vectors are coplanar,
since by Eq. (17) they all lie on the (cid:0)n
(cid:1)-dimensional plane (cid:80)
i<j xij = 1. In
2
order for the inner product to have the same value regardless of k, the vector
of ijth component r−3
ij must therefore be orthogonal to this plane. That is, we
must have

2

(cid:88)

r−3
ij N −1(ρ2

ijk − ρ2

ij(cid:96)) = 0,

(18)

i<j

where k, (cid:96) are any two of the d dimensions. In the formulation that follows we
use k = 1, (cid:96) = 2.

Determining the αk’s directly from Eq. (18) and the equality constraint is not
possible, so we resort to the following nonlinear programming problem instead.

ij N −1(ρ2

ij1 − ρ2

ij2)(cid:1)2

minimize (cid:0)(cid:80)
subject to (cid:80)

i<j r−3
k α2
k = d,
αk > 0.

(19)

(20)

(21)

∀k ∈ {1, . . . , d}

Solving this problem will return αk’s that approximate Eq. (18) as well as pos-
sible. Even if a good approximation is returned, it must be kept in mind that
only ﬁrst-order necessary conditions are being taken into account. The second-
order necessary and suﬃcient conditions, which involve the second derivatives
of SC, are not. Further methodological steps must then be taken, as detailed
in Section 3 along with the necessary tool set. Henceforth, we refer to the
optimization problem given in Eqs. (19)–(21) simply as Problem P.

Further remarks. Another consequence of Eq. (17) is that

(cid:88)

i<j

r2
ij = N

(cid:88)

α2
k,

k

which allows g to be rewritten as

(cid:18)

g =

N

(cid:88)

α2
k

(cid:19)1/2

.

k

7

(22)

(23)

Table 1: Data sets used and their properties.

Data set,
number of
clusters

Number of Number of Number of
samples
(norig)

unique
samples
(n)
149
465
62
1348
569

actual/original
dimensions
(d/dorig)
4/4
9/9
3/496
3/4
10/30

Number of
dimensions with
missing values
(dmiss)
0
1
392
0
0

Iris, 3
BCW, 2
BC-DR3, 4
BNA-DR3, 2
BCW-Diag-10, 2

150
699
62
1372
569

3 Experimental setup

Solving Problem P to discover the αk’s is the centerpiece of our approach.
Several candidate sets of these scaling factors can be obtained by solving the
problem repeatedly in a sequence of random trials, each one ﬁrst selecting an
initial point for the minimization and then attempting to converge to a set
of αk’s for which the problem’s objective function is locally minimum. The
resulting scaling-factor sets can then be pitched against one another, engaging
the user’s knowledge of the data set for the selection of a small set of candidates
(perhaps even a single one) to carry on with.

Because clustering is an approach to data analysis that depends strongly on
a domain expert’s knowledge of and familiarity with the data set, uncertainties
during the process of selecting appropriate scaling-factor sets from those turned
up by solving Problem P are inevitable. To illustrate some strategies to deal
with this, in Sections 4 and 5 we discuss our experience with analyzing ﬁve well-
known benchmarks in light of SC. Dealing with these data sets has of course
been greatly facilitated by the availability of the reference partition into clusters
for each one. This will not be available in a real-world scenario, except perhaps
in some fragmentary form, but in our discussion of the benchmarks we attempt
to provide viewpoints that may be useful even then.

We continue this section with the presentation of the benchmarks we use,

and of the tools, algorithms, and evaluation method we enlist.

Data sets. The ﬁve data sets we use are listed in Table 1, along with crucial
information on them. We divide them into two groups, based on our experience
in handling them, particularly on the diﬃculty in obtaining good partitions. The
ﬁrst group contains those for which it has proven possible to obtain partitions
that approximate the corresponding reference partition well. The second group
contains those for which approximating the reference partition, even if only
reasonably, has proven harder.

The ﬁrst group has two members, the Iris data set (downloaded from [16]
and then corrected to exactly match the data in the original publication [17])
and BCW, the original version of the Wisconsin breast cancer data sets [18].

8

The second group comprises three data sets, viz.: BC-DR3, which comes
from the version of Perou et al.’s breast cancer data set [19] compiled and made
available by the proponents of scaling by the σpool
’s mentioned in Section 1;
BNA-DR3, from a data set containing wavelet-transform versions and the en-
tropy of banknote images for authentication [20]; and BCW-Diag-10, from the
so-called diagnostic version of the Wisconsin breast cancer data sets [21].

k

The three data sets in the second group have fewer dimensions than origi-
nally available, which is indicated in Table 1 by the d < dorig values on the fourth
column. We reduced these data sets’ numbers of dimensions as an attempt to
make clustering succeed better than it would otherwise.
In two cases this is
indicated by the “DR3” in the data sets’ names, which refers to dimensionality
reduction by adopting the ﬁrst three principal components output by principal
component analysis (PCA) [22] on the data after centering (but not scaling)
the samples. This was done in the R language, using function prcomp with op-
tions center = T and scale = F. The resulting BC-DR3 and BNA-DR3 retain
35.85% and 97.02% of the original variance, respectively.

The third case is that of BCW-Diag-10, which contains only the ﬁrst 10 of
the original 30 dimensions. Each sample in this data set is an image and each
dimension is a statistic computed on that image. The 10 dimensions we use are
mean values.

As per the second and third columns in the table, three of the data sets
(Iris, BCW, and BNA-DR3) contain more samples (norig) than unique sam-
ples (n). The diﬀerence corresponds to duplicates, which were discarded so
that SC, and consequently Problem P, could be deﬁned properly. The table’s
ﬁfth column is also worthy of attention, since it gives for each data set the
number of dimensions (dmiss) for which missing values are to be found. Such
values were synthesized in the Wolfram Mathematica 13.0 system, using func-
tion SynthesizeMissingValues with default settings. This caused no further
duplicates to appear and, in the case of BC-DR3, was of course done before
PCA.

Computational tools and algorithms. For each of the data sets in Table 1,
ﬁrst we ran 1 000 trials, each one aiming to obtain a scaling-factor candidate set
by solving Problem P. We did the required optimization in the Wolfram Mathe-
matica 13.0 system, using function FindMinimum, mostly with default settings,
to ﬁnd a local minimum of the problem’s objective function for each trial. Our
only choices of a non-default setting for FindMinimum were the following: for
each trial we speciﬁed an initial point in [0.5, 1.5]d, selected uniformly at random
by an application of function RandomReal to each dimension; we allowed for a
larger number of iterations with the MaxIterations -> 5000 option; and we
precluded any symbolic manipulation with Gradient -> "FiniteDifference"
(because Problem P is strongly data-dependent, allowing Mathematica to per-
form the symbolic manipulations that come so naturally to it can quickly lead
to memory overﬂow).

9

On occasion we have noticed that coding the constraint in Eq. (21) as is can
lead to division-by-zero errors. We thus avoided this by expressing the constraint
as αk ≥ 10−5 instead of αk > 0. Still regarding errors during minimization,
FindMinimum can also fail by not attaining convergence within the speciﬁed
maximum number of iterations. This error can take more than one form, but
in general we have observed it in no more than 0.2% of the trials for each data
set. When a failure of this type does occur a solution is still output, but in our
experiments such outputs were discarded when compiling results.

One crucial step in this study is of course partitioning the data into clusters
after they have been appropriately scaled. We performed this step in the R
language, using in all cases the k-means method as implemented in function
kmeans. Because k-means has certain randomized components, we ﬁrst set
a ﬁxed seed, via set.seed(1234), to facilitate consistency checks. Function
kmeans receives as input the scaled version of the norig × d data matrix X and
also the desired number of clusters (the same as in the data set’s reference
partition). For the results we report in Section 4, scaling happened according
to one of four possibilities: either each Xik remained unchanged (no scaling), or
it became one of (1/σk)Xik, (1/σpool
)Xik, or (αk/σk)Xik. The latter is scaling
as indicated by some random trial with Problem P.

k

Partition evaluation. To evaluate the partition resulting from clustering we
use the variant of the well-known Adjusted Rand Index (ARI) [23] that seems
most appropriate in our present context, which is that every possible resulting
partition of a data set by the clustering algorithm in use must have a ﬁxed
number of clusters (“fnc,” used in notations henceforth) [24]. That this is clearly
the case follows from our use of k-means described above. The ARI variant is

ARIfnc =

RI − Efnc[RI]
1 − Efnc[RI]

,

(24)

where RI is the original Rand Index and Efnc[RI] is its expected value given the
ﬁxed number of clusters condition.

Letting C denote the number of clusters that any obtained partition will

have, the formulas for RI and Efnc[RI] are

and

with

and

RI =

(cid:19)−1

(cid:18)norig
2

(TS + TD)

Efnc[RI] = U V + (1 − U )(1 − V ),

U =

(cid:26)norig
C

(cid:27)−1(cid:26)norig − 1

(cid:27)

C

V =

(cid:19)−1

(cid:18)norig
2

(TS + FD).

10

(25)

(26)

(27)

(28)

Table 2: Performance of k-means, according to ARIfnc, on various scaled versions
of the data sets in Table 1.

Data set

ARIfnc

No scaling

Iris
BCW
BC-DR3
BNA-DR3
BCW-Diag-10

0.728
0.840
0.492
0.050
0.456

Scaling by
1/σk
0.621
0.825
0.518
0.023
0.673

Scaling by
1/σpool
k
0.886
0.825
0.518
0.023
0.673

Scaling by
αk/σk
0.571–0.904
0.189–0.877
(−0.021)–0.535
(−0.000)–0.659
0.633–0.655

In Eqs. (25) and (28), TS (for “true similar”) counts the number of sample
pairs that are in the same cluster according to the obtained partition and in the
same cluster according to the reference partition; TD (“true dissimilar”) counts
pairs that are split between diﬀerent clusters according to both the obtained
partition and the reference partition; and FD (“false dissimilar”) counts those
that are split between diﬀerent clusters according to the obtained partition but
are in the same cluster according to the reference partition. Curly brackets are
used in Eq. (27) to denote Stirling numbers of the second kind.

ARIfnc equals at most 1, which happens for TS + TD = (cid:0)norig

2
the obtained partition and the reference partition are identical).

(cid:1) (i.e., when

4 Results

All our results are in reference to the data sets listed in Table 1 and are sum-
marized in Table 2. In this table, the value of ARIfnc resulting from the use of
k-means is given for each of several scaled versions of the norig × d data matrix
X that corresponds to each data set. There are four schemes in each case: the
no-scaling scheme, in which each Xik is used directly as it appears in the data
matrix; the scheme that makes use of the standard deviation in each dimension,
in which Xik is scaled by 1/σk; the scheme that uses pooled standard deviations
instead, in which Xik is scaled by 1/σpool
; and the scheme that uses scaling fac-
tors obtained by solving Problem P, in which Xik is scaled by αk/σk for the
resulting αk.

k

ARIfnc values for the latter type of scaling are presented in Table 2 as in-
tervals, indicating in each case the lowest and the highest value observed in the
1 000 random trials with Problem P (slightly fewer trials if optimization errors
happened). For BC-DR3 and BNA-DR3, the intervals begin at slightly negative
values of ARIfnc (indicating that RI < Efnc[RI]).

The intervals on the rightmost column of Table 2 are supplemented by the
panels in Figure 1, where each row of panels (rows A through E) corresponds
to one of the data sets. Such panels allow viewing the various ARIfnc values
inside those intervals from diﬀerent perspectives. The left panel on each row is
a plot of SC against α1 for all trials on the corresponding data set. The choice

11

of α1 is completely arbitrary and meant only to oﬀer a glimpse into how SC
depends on the αk’s turned up by solving Problem P. Points are color-coded to
indicate how their ARIfnc values relate to one another. The right panel on each
row allows viewing such values as a histogram.

5 Discussion

Table 2 conﬁrms, for the selection of data sets we are considering, what by
and large has been known for a long while. That is, that scaling by 1/σk
can sometimes be worse than simply attempting to partition the data in X into
clusters without any scaling. In the table, this is the case mainly of the Iris data
set. Table 2 also conﬁrms what has been known since the recent introduction
of scaling by 1/σpool
, which is that proceeding in this way, once again in the
case of Iris, leads to superior performance. The table goes farther than this,
however, since it also makes clear that the fallback role of σk as a surrogate for
σpool
in the approach of [5] may be taken more frequently than initially realized.
k
This is shown in the table for all but the Iris data set.

k

But the most relevant contribution of the results in Table 2 is the realization
that in almost all cases the best performing set of αk’s for each data set performs
strictly better than the other three alternatives. The only exception is the BCW-
Diag-10 data set, although in this case every one of the sets of αk’s can be said
to lie, so to speak, in the same ballpark as 1/σk (or 1/σpool
). In fact, the plots
in Figure 1(E) strongly suggest that scaling the data for BCW-Diag-10 by the
outcome of virtually any of the random trials with Problem P would be equally
acceptable. This would be so even if a reference partition (and hence ARIfnc
values) had not been available, because comparing the obtained partitions with
one another would already suﬃce.

k

Of course, the latter is based almost entirely on the highly concentrated
character of the ARIfnc histogram in Figure 1(E), which to a degree is also
true of Figures 1(B) and 1(C), which refer to the BCW and BC-DR3 data sets,
respectively. For each of these two data sets, comparing the partitions resulting
from the random trials with Problem P with one another, and adopting any of
those that by the ARIfnc histogram seem not only to be one and the same but
also to recur very frequently during the trials, would lead to equally acceptable
scaling decisions.

This leaves us with the Iris and BNA-DR3 data sets. In these two cases,
choosing the set of αk’s to use out of those produced by the random trials
with Problem P by simply comparing the obtained partitions and looking for
a consensus with strong support would lead to disastrous results. This is clear
from the ARIfnc histograms in Figures 1(A) and 1(D), which peak signiﬁcantly
to the left of the best values attained in the trials. Beyond comparing obtained
partitions, one must therefore also use one’s knowledge of the domain in question
and look at what they are doing to the data. The guiding principle to be used is
essentially in the spirit of our discussion in Section 2: in the end, the candidate
set of αk’s to be chosen must lead to a partition that makes sense, either visually

12

Figure 1: Results of the random trials with Problem P on Iris (A), BCW (B),
BC-DR3 (C), BNA-DR3 (D), and BCW-Diag-10 (E), expanding on the sum-
mary given on the rightmost column of Table 2. Each point on each left panel
corresponds to a trial and is color-coded according to the accompanying palette
to reﬂect the value of ARIfnc it leads to by way of clustering with k-means.
The point leading to the highest ARIfnc value is marked by the crosshair in the
panel. Each right panel provides a view of how ARIfnc is distributed over all
pertaining trials.

13

Figure 1: Continued.

14

Table 3: Scaling factors used in Figures 2 (Iris) and 3 (BNA-DR3). The αk’s are
the ones leading to the highest values of ARIfnc in the intervals on the rightmost
column of Table 2.

k

Iris

BNA-DR3

1/σk
1.207
2.294
0.566
1.311

αk/σk
0.453
1.291
0.859
1.459

1
2
3
4

1/σk
0.141
0.327
0.477

αk/σk
0.073
0.373
0.571

or by inspection of the “midrange” distances between samples, those that can
be more easily mistaken for intracluster when they are intercluster or conversely.
We proceed with the aid of Table 3, which lists each 1/σk and each αk/σk
(this one for the highest ARIfnc listed in Table 2) for Iris and BNA-DR3. Note,
for the Iris data set, that k = 1 and k = 3 are the dimensions for which switching
from scaling by 1/σk to scaling by αk/σk provides the greatest scaling-factor
reduction and ampliﬁcation (given, in fact, by the value of αk), respectively. For
the BNA-DR3 data set, dimension k = 1 has its weight on distances strongly
reduced in moving from the former scaling scheme to the latter, while for both
k = 2 and k = 3 the scaling factor is ampliﬁed by about the same proportion.

For the Iris data set, in Figure 2 we give six panels. These are arranged in
three columns, the leftmost one dedicated to the data set’s reference partition,
each of the other two to a diﬀerent scaling scheme (scaling by 1/σk and scaling
by αk/σk, with factors as in Table 3). The top panel in each column contains a
scatterplot of the samples, each color-coded for the data set’s three classes, as
represented by dimensions k = 1 and k = 3 (cf. the discussion above in reference
to Table 3). The bottom panel is a histogram of the rij’s, the distances between
samples in d-dimensional real space.
It is important to emphasize that, in
regard to the leftmost column, and unlike what happens with the other two, the
scatterplot in it is color-coded to reﬂect the reference partition, not the obtained
partition that results from the no-scaling scheme.

As we examine the scatterplots in the ﬁgure we see that scaling by 1/σk
stretches dimension k = 1 excessively just as dimension k = 3 is excessively
shrunk, resulting in more confusion between the two clusters that are not linearly
separable. We also see why scaling by αk/σk as in Table 3 is a better choice: the
previous stretching of dimension k = 1 and the shrinking of dimension k = 3 are
both undone, though to diﬀerent degrees, which allows some of the previously
added confusion to be reverted. Examining the distance histograms reveals
that scaling by 1/σk causes distances to become more concentrated, lengthening
some of the smallest ones and shortening some of the largest. So another way
to view the further confusion added by this scaling scheme is to recognize that
it aﬀects the already potentially problematic midrange distances. Scaling by
αk/σk restores the overall appearance of the no-scaling histogram (the one in
the leftmost column), but seemingly with sharper focus around those distances.

15

Figure 2: Reference partition for the Iris data set (leftmost column of panels)
and the eﬀects of two scaling schemes: scaling by 1/σk (middle column) and
scaling by αk/σk (rightmost column), with factors as in Table 3. Eﬀects can be
seen both with respect to the shape of the data set (top row of panels, all plots
drawn to the same scale) and to the distribution of distances between samples
(the rij’s; bottom row, all plots drawn to the same scale).

16

This is important because, as we know from Table 2, scaling the Iris data set by
the αk/σk factors of Table 3 improves not only on the use of the 1/σk factors
but also on the no-scaling scheme, on which k-means already performs more
than reasonably well.

Figure 3 has the same six panels as Figure 2, and also identically arranged,
but now referring to the BNA-DR3 data set. This data set provides a much
more striking contrast between the two scaling schemes given in Table 3 than
Iris, as per Table 2 the ratio of the ARIfnc value that scaling by αk/σk yields to
that of scaling by 1/σk is about 28.65.

The most direct pictorial evidence we have of this comes from comparing
the middle and rightmost scatterplots of Figure 3 with each other, having the
leftmost one as the reference partition. Because the ratio of αk/σk to 1/σk
(i.e., the value of αk) is slightly higher than only 1.1 for both k = 2 and k = 3
(once again, cf. our earlier comment on this), what really accounts for the very
signiﬁcant diﬀerence between the two scaling schemes has to do with dimension
k = 1, for which a ratio of about 0.517 ensues. Visually inspecting the two
obtained partitions vis-`a-vis the reference partition provides immediate conﬁr-
mation of how crucial this shrinking of dimension k = 1 is. As with the Iris
data set, inspecting the histograms in the ﬁgure provides insight similar to the
one we gleaned in that case. Even though the middle and rightmost histograms,
corresponding respectively to scaling by 1/σk and αk/σk, may seem similar to
each other particularly when viewed in comparison to the no-scaling histogram
(the leftmost one), closer inspection tells a diﬀerent story. That is, moving from
scaling by 1/σk to scaling by αk/σk seems to restore some of the no-scaling
histogram’s slow descent from its peak through the midrange distances. This
comes about by virtue of both a lower peak and the appearance of some residual
pair count beyond the rij = 5 bar when the additional αk factor is put to use.
This is curious, especially as we note from Table 2 that no scaling and scaling by
1/σk both lead k-means to essentially the same poor performance. This seems
to be suggesting that, in the no-scaling scheme, such poor performance is to be
attributed essentially to the excessive spread of distances.

6 Concluding remarks

In this paper we have revisited the problem of scaling a data set’s dimensions
to facilitate clustering by those methods that, like k-means, make explicit use of
distances between samples. For each dimension k, we have framed our study as
the determination of a scale factor αk > 0 to be applied on top of the customary
division by σk, the standard deviation of the data in that dimension. That is,
we have targeted a scaling factor of the form αk/σk. Our guiding principle has
been to focus on the eﬀects of scaling the data on the multidimensional shapes
that ensue: essentially, we have equated any facilitation of the clustering task
with mistaking intercluster distances for intracluster distances (or conversely) as
seldom as possible. Because we normally think of the former type of distances as

17

Figure 3: As in Figure 2, now for the BNA-DR3 data set.

18

being large, and the latter as being small, we have aimed our eﬀorts at midrange
distances.

To make such notions precise, we enlisted the shape complexity of the scaled
data, given by SC, which depends heavily on the data matrix X (as a constant)
and on the various αk’s (as variables). The function SC embodies a lot of
the tension between large and small distances between samples and, as such,
allows midrange distances to be characterized as being the equilibrium between
extremes that occurs at those αk’s for which the gradient of SC is zero. We
have viewed such scaling-factor sets as candidates, each one obtained by solving
Problem P, given in Eqs. (19)–(21), from a randomly chosen initial point.

All our results refer to data sets that are essentially manageable when con-
sidering both their numbers of samples and numbers of dimensions. The perfor-
mance of k-means clustering on them, as measured by ARIfnc, ranges from very
poor to well above average, so calling them “manageable” refers not at all to
how amenable to clustering by k-means they are, but rather to the possibility
of solving Problem P for them multiple times without too much computational
eﬀort.

Our results can be summarized very simply:

for all data sets we tackled,
generating scaling-factor candidate sets via Problem P has yielded at least one
set for which scaling by αk/σk (as opposed to 1/σk) leads to strictly better
performance (with one single exception, where “strictly better” becomes “com-
parable”). The overall method cannot be used as a blind procedure, though,
since in at least two cases we came across the need for carefully considered visual
inspections of the scaled data, perhaps even of their distance histograms.

In spite of the radial invariance of SC, which we used to constrain the αk’s
when formulating Problem P, the number of possibilities outside the reach of
Problem P is limitless. Suppose, for example, that we use the following alter-
native formulation of the nonlinear programming problem.

maximize SC
subject to αk ≥ 10−5.

∀k ∈ {1, . . . , d}

(29)

(30)

In signiﬁcant ways this is still in the spirit of Problem P, even though it limits the
notion of a gradient-zero point to those that correspond to local maxima of SC.
We mention this particular formulation because solving it for the Iris data set,
as explained in Section 3 for Problem P (now with FindMaximum substituting for
FindMinimum and selecting the initial points from [10−5, 1]d), yielded ARIfnc =
0.922 in the best case, with α1 = α2 = 10−5, α3 = 5.09372 × 1017, and α4 =
2.48504×1017. This is an interesting outcome, and not only because it surpasses
the best result reported in Table 2 (ARIfnc = 0.904). What these αk’s are saying
is: reduce the importance of dimensions k = 1 and k = 2 as far as allowed by
the constraint in Eq. (30) while dimensions k = 3 and k = 4 are very strongly
ampliﬁed. This is to a degree already what Table 3 is suggesting, though only
in relation to scaling by 1/σk and moreover much more timidly.

Alternative formulations like this, and the surprising results they may lead
to, serve to illustrate the rich store of possibilities for shape complexity-based

19

cluster analysis. Additional investigations to further explore SC and its role
in helping determine appropriate scaling factors for any given data set could
well be worth the eﬀort. A crucial ingredient will be the use of techniques to
not only reduce the number of dimensions in the data, but also the number
of samples (e.g., as discussed in [25]), aiming to make possible the solution of
problems like Problem P. In this regard, we note that, already for the precursor
of the BC-DR3 data set, with norig = 62 and dorig = 496, solving one trial with
Problem P is expected to take a few hundred hours.

Acknowledgments

The authors acknowledge partial support from Conselho Nacional de Desen-
volvimento Cient´ıﬁco e Tecnol´ogico (CNPq), Coordena¸c˜ao de Aperfei¸coamento
de Pessoal de N´ıvel Superior (CAPES), and a BBP grant from Funda¸c˜ao Carlos
Chagas Filho de Amparo `a Pesquisa do Estado do Rio de Janeiro (FAPERJ).

References

[1] R. A. van den Berg, H. C. J. Hoefsloot, J. A. Westerhuis, A. K. Smilde, and
M. J. van der Werf. Centering, scaling, and transformations:
improving
the biological information content of metabolomics data. BMC Genomics,
7:142, 2006.

[2] C. Edelbrock. Mixture model tests of hierarchical clustering algorithms:
the problem of classifying everybody. Multivariate Behavioral Research,
14:867–884, 1979.

[3] G. W. Milligan and M. C. Cooper. A study of standardization of variables

in cluster analysis. Journal of Classiﬁcation, 5:181–204, 1988.

[4] D. Steinley. Standardizing variables in k-means clustering. In D. Banks,
F. R. McMorris, P. Arabie, and W. Gaul, editors, Classiﬁcation, Cluster-
ing, and Data Mining Applications, pages 53–60. Springer-Verlag, Berlin,
Germany, 2004.

[5] J. Raymaekers and R. H. Zamar. Pooled variable scaling for cluster analysis.

Bioinformatics, 36:3849–3855, 2020.

[6] J. B. Kruskal. Linear transformations of multivariate data to reveal clus-
tering. In R. N. Shepard, A. K. Romney, and S. B. Nerlove, editors, Multi-
dimensional Scaling: Theory and Applications in the Behavioral Sciences,
volume 1. Theory, pages 181–191. Seminar Press, New York, NY, 1972.

[7] W. S. DeSarbo, J. D. Carroll, and L. A. Clark. Synthesized clustering:
a method for amalgamating alternative clustering bases with diﬀerential
weighting of variables. Psychometrika, 49:57–78, 1984.

20

[8] G. De Soete, W. S. DeSarbo, and J. D. Carroll. Optimal variable weighting
for hierarchical clustering: an alternating least-squares algorithm. Journal
of Classiﬁcation, 2:173–192, 1985.

[9] J. Hohenegger. Weighted standardization: a general data transformation
method preceding classiﬁcation procedures. Biometrical Journal, 28:295–
303, 1986.

[10] J. Rossignac. Shape complexity. The Visual Computer, 21:985–996, 2005.

[11] F. Mercati. Shape Dynamics: Relativity and Relationalism. Oxford Uni-

versity Press, New York, NY, 2018.

[12] J. Barbour. The Janus Point: A New Theory of Time. Basic Books, New

York, NY, 2020.

[13] P. Fr¨anti and S. Sieranoja. K-means properties on six clustering benchmark

datasets. Applied Intelligence, 48:4743–4759, 2018.

[14] A. K. Jain. Data clustering: 50 years beyond k-means. Pattern Recognition

Letters, 31:651–666, 2010.

[15] D. G. Luenberger.

Introduction to Linear and Nonlinear Programming.

Addison-Wesley, Reading, MA, 1973.

[16] Iris data set. https://archive.ics.uci.edu/ml/datasets/Iris. Ac-

cessed: 2022-03-19.

[17] R. A. Fisher. The use of multiple measurements in taxonomic problems.

Annals of Eugenics, 7:179–188, 1936.

[18] Breast cancer Wisconsin (original) data set. https://archive.ics.uci.
Accessed:

edu/ml/datasets/breast+cancer+wisconsin+(original).
2022-03-19.

[19] C. M. Perou, T. Sørlie, M. B. Eisen, M. van de Rijn, S. S. Jeﬀrey, C. A. Rees,
J. R. Pollack, D. T. Ross, H. Johnsen, L. A. Akslen, Ø. Fluge, A. Pergamen-
schikov, C. Williams, S. X. Zhu, P. E. Lønning, A.-L. Børresen-Dale, P. O.
Brown, and D. Botstein. Molecular portraits of human breast tumours.
Nature, 406:747–752, 2000.

[20] Banknote authentication data set. https://archive.ics.uci.edu/ml/

datasets/banknote+authentication. Accessed: 2022-03-19.

[21] Breast cancer Wisconsin (diagnostic) data set. https://archive.ics.
uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic). Ac-
cessed: 2022-03-19.

[22] I. T. Jolliﬀe and J. Cadima. Principal component analysis: a review and
recent developments. Philosophical Transactions of the Royal Society A,
374:20150202, 2016.

21

[23] L. Hubert and P. Arabie. Comparing partitions. Journal of Classiﬁcation,

2:193–218, 1985.

[24] A. J. Gates and Y.-Y. Ahn. The impact of random models on clustering

similarity. Journal of Machine Learning Research, 18:1–28, 2017.

[25] S. Eschrich, J. Ke, L. O. Hall, and D. B. Goldgof. Fast accurate fuzzy
clustering through data reduction. IEEE Transactions on Fuzzy Systems,
11:262–270, 2003.

22

