Contrastive Code Representation Learning

Paras Jain∗ and Ajay Jain∗ and Tianjun Zhang and
Pieter Abbeel and Joseph E. Gonzalez and Ion Stoica
Department of EECS, UC Berkeley
{parasj, ajayj, tianjunz,
pabbeel, jegonzal, istoica}@berkeley.edu

2
2
0
2

n
a
J

6

]

G
L
.
s
c
[

4
v
3
7
9
4
0
.
7
0
0
2
:
v
i
X
r
a

Abstract

Recent work learns contextual representations
of source code by reconstructing tokens from
their context. For downstream semantic under-
standing tasks like code clone detection, these
representations should ideally capture pro-
gram functionality. However, we show that the
popular reconstruction-based RoBERTa model
is sensitive to source code edits, even when the
edits preserve semantics. We propose Con-
traCode: a contrastive pre-training task that
learns code functionality, not form. Con-
traCode pre-trains a neural network to iden-
tify functionally similar variants of a pro-
gram among many non-equivalent distractors.
We scalably generate these variants using
an automated source-to-source compiler as a
form of data augmentation. Contrastive pre-
training outperforms RoBERTa on an adver-
sarial code clone detection benchmark by 39%
AUROC. Surprisingly, improved adversarial
robustness translates to better accuracy over
natural code; ContraCode improves summa-
rization and TypeScript type inference accu-
racy by 2 to 13 percentage points over com-
petitive baselines. All source is available at
https://github.com/parasj/contracode.

1

Introduction

Programmers increasingly rely on machine-aided
programming tools that analyze or transform code
automatically to aid software development (Kim
et al., 2012). Traditionally, code analysis uses
hand-written rules, though the wide diversity of
programs encountered in practice can limit their
generality. Recent work leverages machine learn-
ing for richer language understanding, such as
learning to detect bugs (Pradel and Sen, 2018) and
predict performance (Mendis et al., 2019).

Still, neural models of source code are suscep-
tible to adversarial attacks. Yefet et al. (2020)
and Schuster et al. (2021) ﬁnd accuracy degrades

∗ equal contribution

Figure 1: Robust code clone detection: On source
code, RoBERTa is not robust to simple label-preserving
code edits like renaming variables. Adversarially se-
lecting between possible edits lowers performance be-
low random guessing (dashed line). Contrastive pre-
training with ContraCode learns a more robust repre-
sentation of functionality, consistent across code edits.

Figure 2: For many analyses, programs with the same
functionality should have similar representations. Con-
traCode learns such representations by pre-training an
encoder to retrieve equivalent, transformed programs
among many distractors.

signiﬁcantly under adversarial perturbations for
both discriminative and generative code models.
In our work, we investigate adversarial attacks on
code clone detection. Successful adversarial at-
tacks could circumvent malware detectors.

While self-supervision can improve adversar-
ial robustness (Hendrycks et al., 2019), we ﬁnd
that RoBERTa is sensitive to stylistic implemen-
tation choices of code inputs. Fig. 1 plots the per-
formance of RoBERTa and ContraCode, our pro-
posed method, on a code clone detection task as
small label-preserving perturbations are applied to
the input code syntax. With just three minor ad-
versarial edits to code syntax, RoBERTa underper-
forms the random classiﬁer (in gray). In Fig. 3,

function (len) {  for (i = 0; i < len, i++) {    ...  }}function (n) { while (i < n) { ... } }function (str, len) { return str.slice(0, len); }function f(n) { return n<2 ? 1 : f(n-1) + f(n-2); }function (arr) { for (i of arr) { ... } }Maximize similarity with equivalent programsMinimize similarity withfunctionally diﬀerent programsGiven a program, 
 
 
 
 
 
we show that RoBERTa’s representations of code
are sensitive to code edits in agreement with prior
work (Wang and Christodorescu, 2019; Wang and
Su, 2019; Rabin and Alipour, 2020).

To address this issue, we develop ContraCode: a
self-supervised representation learning algorithm
that captures program semantics. We hypothesize
that programs with the same functionality should
have similar underlying representations for down-
stream code understanding tasks.

ContraCode generates syntactically diverse but
functionally equivalent programs using source-to-
source compiler transformation techniques (e.g.,
dead code elimination, obfuscation and constant
folding).
It uses these programs in a challeng-
ing discriminative pretext task that requires the
model to identify similar programs out of a large
dataset of distractors (Fig. 2). To solve this task,
the model must embed code semantics rather than
syntax. ContraCode improves adversarial robust-
ness in Fig. 1. Surprisingly, adversarial robustness
transfers to better natural code understanding.

Our novel contributions include:

1. the novel use of compiler-based transforma-

tions as data augmentations for code,

2. the concept of program representation learn-
ing based on functional equivalence, and

3. a detailed analysis of architectures, code
transforms and pre-training strategies, show-
ing ContraCode improves type inference top-
1 accuracy by 9%, learned inference by 2%–
13%, summarization F1 score by up to 8%
and clone detection AUROC by 2%–46%.

2 Related work

Self-supervised learning (SSL) is a learning
strategy where some attributes of a datapoint
are predicted from remaining parts. BERT (De-
vlin et al., 2019) is a SSL method for NLP
that reconstructs masked tokens as a pretext
task. RoBERTa (Liu et al., 2019) further tunes
BERT. Contrastive approaches minimize distance
between learned representations of similar ex-
amples (positives) and maximize distance be-
tween dissimilar negatives (Hadsell et al., 2006).
CPC (Oord et al., 2018; Hénaff, 2020) encodes
segments of sequential data to predict future
segments.
SimCLR (Chen et al., 2020a) and
MoCo (He et al., 2020; Chen et al., 2020b) use
many negatives for dense loss signal.

Figure 3: A UMAP visualization of JavaScript method
representations learned by RoBERTa and ContraCode,
in R2. Programs with the same functionality share
color and number. RoBERTa’s embeddings often do
not cluster by functionality, suggesting that it is sensi-
tive to implementation details. For example, many dif-
ferent programs overlap, and renaming the variables
of Program 19 signiﬁcantly changes the embedding.
In contrast, variants of Program 19 cluster in Contra-
Code’s embedding space.

Code representation learning
We address
clone detection (White et al., 2016), type infer-
ence (Hellendoorn et al., 2018), and summariza-
tion (Alon et al., 2019a). Others explored summa-
rization (Movshovitz-Attias and Cohen, 2013; Al-
lamanis et al., 2016; Iyer et al., 2016; Ahmad et al.,
2020) and types (Pradel et al., 2019; Pandi et al.,
2020; Wei et al., 2020; Allamanis et al., 2020;
Bielik and Vechev, 2020; Allamanis et al., 2018)
for various languages. Inst2vec (Ben-Nun et al.,
2018) embeds statements in LLVM IR by process-
ing a ﬂow graph with a context prediction objec-
tive (Mikolov et al., 2013). Code2seq (Alon et al.,
2019a) embeds AST paths with an attentional en-
coder for seq2seq tasks. Kanade et al. (2020) and
Feng et al. (2020) pre-train a Transformer on code
using the masked language modeling (MLM) ob-
jective (Devlin et al., 2019; Taylor, 1953).

Adversarial attacks on code models
Yefet
et al. (2019) ﬁnd code models are highly sensitive
to adversarial code edits in a discrimative setting.
Schuster and Paliwal (1997) discovers in-the-wild
attacks on code autocompletion tools. Compared
to language models, code models may be more
vulnerable to adversarial attacks due to synthetic
labels (Ferenc et al., 2018; Pradel and Sen, 2018;
Benton et al., 2019) and duplication (Allamanis,
2019) that degrade generalization.

RoBERTaembeddingsUMAP1UMAP2Program19VariantBProgram19VariantA……ContraCodeembeddingsUMAP1Figure 4: A JavaScript method from our unlabeled training set with two auto-
matically generated semantically-equivalent programs. The method is from the
StackEdit Markdown editor.

Figure 5: Histogram of the num-
ber of unique transformed vari-
ants per JavaScript method dur-
ing pre-training.

3 Approach

Our core insight is to use compiler transforms as
data augmentations, generating a dataset of equiv-
alent functions (§3.1, 3.2). We then use a con-
trastive objective to learn a representation invari-
ant to these transforms (§3.3).

3.1 Compilation as data augmentation

Modern programming languages afford great ﬂex-
ibility to software developers, allowing them to
implement the same function in different ways.
Yet, crowdsourcing equivalent programs from
GitHub is difﬁcult as verifying equivalence is un-
decidable (Joshi et al., 2002; Bansal and Aiken,
2006) and approximate veriﬁcation is costly and
runs untrusted code (Massalin, 1987).

Instead of searching for equivalences, we pro-
pose correct-by-construction data augmentation.
We apply compiler transforms to unlabeled code
to generates many variants with equivalent func-
tionality, i.e. operational semantics. For example,
dead-code elimination (DCE) is an optimization
that removes operations that do not change func-
tion output. While DCE preserves functionality,
Wang and Christodorescu (2019) ﬁnd that up to
12.7% of the predictions of current supervised al-
gorithm classiﬁcation models change after DCE.

We parse a particular source code sequence,
e.g.
W*x + b into a tree-structured representa-
tion (+ (* W x) b) called an Abstract Syntax
Tree (AST). We then transform the AST with au-
tomated traversal passes. A rich body of prior
programming language work explores parsing and
transforming ASTs to optimize a program.
If
source code is emitted by the compiler rather
than machine code, this is called source-to-source
transformation or transpilation. Transpilation is
common for optimizing and obfuscating dynamic
languages like JavaScript. Further, if each trans-

Code compression

Identiﬁer modiﬁcation

(cid:51) Variable renaming (VR)
(cid:51) Identiﬁer mangling (IM)
Regularization

(cid:51) Reformatting (R)
(cid:51) Beautiﬁcation (B)
(cid:51) Compression (C)
(cid:51) Dead-code elimination (DCE) (cid:51) Dead-code insertion (DCI)
(cid:51) Type upconversion (T)
(cid:51) Constant folding (CF)
(cid:51) = semantics-preserving transformation (cid:55) = lossy transformation

(cid:51) Subword regularization (SW)
(cid:55)
Line subsampling (LS)

Table 1: We augment programs with 11 automated
source-to-source compiler transforms. 10 are correct-
by-construction and preserve operational semantics.

form preserves code semantics, then any composi-
tion also preserves semantics.

We implement our transpiler with the Ba-
bel and Terser compiler infrastructures (McKen-
zie et al., 2020; Santos et al., 2020) for the
JavaScript programming language. In future work,
a language-agnostic compiler (Koppel et al., 2018)
could be used to extend ContraCode to other lan-
guages. Each compiler transformation is a func-
tion τ :
is
composed of the set of valid ASTs and the set of
programs in tokenized source form. Fig. 4 shows
variants of an example program. Table 1 and Ap-
pendix A list program transformations in detail,
but we broadly group them into three categories:

, where the space of programs

P → P

P

• Code compression changes the syntactic
structure of code and performs correct-
such as pre-
by-construction transforms
computing constant expressions.

• Identiﬁer modiﬁcations substitute method
and variable names with random tokens,
masking some human-readable information
in a program but preserving functionality.

• Finally, Regularizing transforms improve
model generalization by reducing the number
of trivial positive pairs with high text over-
lap. The line subsampling pass in this group

function x(maxLine) {  const section = {    text: '',    data  };  for (; i < maxLine; i += 1) {    section.text += `${lines[i]}\n`;  }  if (section) {    parsingCtx.sections.push(section);  }}Original JavaScript methodfunction x(t) {  const n = {    'text': '',    'data': data  };  for (;i < t; i += 1) {    n.text += lines[i] + '\n';  }  n && parsingCtx.sections.push(n);}Renamed variables, explicit object style, explicit concatenation, inline conditionalfunction x(t){const n={'text':'','data':data};for(;i<t;i+=1)n.text+=lines[i]+'\n';n&&parsingCtx.sections.push(n)}Mangled source withcompressed whitespace5101520Unique transformed program variants050100150Frequency (x1000)Algorithm 1 Transform dropout for stochastic
program augmentation.
1: Input: Program source x,

transformation functions

τ1, . . . τk, transform probabilities p1, . . . pk, count N

2: Returns: N variants of x
3: V ← {x}, a set of augmented program variants
4: for SAMPLE i ← 1 . . . N − 1 do
5:
6:
7:
8:
9:

x(cid:48) ← x
for transform t ← 1 . . . k do

Sample yt ∼ Bernoulli(pt)
if yt = 1 then

if REQUIRESAST(τt(·)) and ¬ISAST(x(cid:48))
then x(cid:48) ← PARSETOAST(x(cid:48))
else if ¬REQUIRESAST(τt(·)) and ISAST(x(cid:48))
then x(cid:48) ← LOWERTOSOURCE(x(cid:48))
x(cid:48) ← τt(x(cid:48))

end for
if ISAST(x(cid:48)) then x(cid:48) ← LOWERTOSOURCE(x(cid:48))
V ← V ∪ {x(cid:48)}

10:

end if

11:
12:
13:
14:
15:
16: end for
17: return V

potentially modiﬁes program semantics.

3.2 Diversity through transform dropout

Stochastic augmentations in other modalities like
random crops generate diverse outputs, but most
of our compiler-based transformations are deter-
ministic. To produce a diverse set of transformed
programs, we randomly apply a subset of available
compiler passes in a pre-speciﬁed order, apply-
ing transform τi with probability pi. Intermediate
programs are converted between AST and source
form as needed for the compiler. Algorithm 1 de-
tails our transform dropout procedure.

Figure 5 measures the resulting diversity in pro-
grams. We precompute up to 20 augmentations
of 1.8M JavaScript methods from GitHub. Algo-
rithm 1 deduplicates method variants before pre-
training since some transforms will leave the pro-
gram unchanged. 89% of the methods have more
than one alternative after applying 20 random se-
quences of transformations. The remaining meth-
ods without syntactically distinct alternatives in-
clude one-line functions that are obfuscated. We
apply subword regularization (Kudo, 2018) as a
ﬁnal transformation to derive different tokeniza-
tions every batch, so pairs derived from the same
original method will still differ. All transforma-
tions are fast; our compiler transforms 300 func-
tions per second on a single CPU core.

3.3 Contrastive pre-training

We extend the Momentum Contrast
(MoCo)
methodology (He et al., 2020) that was designed

for contrastive image representation learning. In
our case, we learn a program encoder fq that maps
a sequence of program tokens to a single, ﬁxed
dimensional embedding. We organize programs
into functionally similar positive pairs and dissim-
ilar negative pairs. Generating two augmentations
of the same GitHub program yields a positive pair
(xq, xk+), and an augmentation of a different pro-
gram yields a negative xk−
. The program xq is
called a “query” used to retrieve the correspond-
ing “key” xk+
during contrastive pre-training. We
use these to shape representation space, drawing
positives together and pushing away from nega-
tives. Negatives are important to prevent the en-
coder fq from mapping all programs to the same,
trivial representation (Saunshi et al., 2019).

Pre-training objective Like He et al. (2020),
we use the InfoNCE loss (Oord et al., 2018), a
tractable objective that frames contrastive learn-
ing as a classiﬁcation task: can the positives be
identiﬁed among negatives? InfoNCE computes
the probability of selecting the positive by taking
the softmax of projected embedding similarities
across a batch and a queue of negatives. Eq. (1)
shows the InfoNCE loss, a function whose value
is low when q is similar to the positive key em-
bedding k+ and dissimilar to negative key embed-
dings k−. t is a temperature hyperparameter pro-
posed by Wu et al. (2018).

log

−

exp(q

·

exp(q
·
k+/t) + (cid:80)

k+/t)
k− exp(q

(1)

k−/t)

·

The query representation q = fq(xq) is computed
by the encoder network fq, and xq is a query pro-
gram. Likewise, k = fk(xk) using a separate key
encoder fk. The summation (cid:80)
k− in the normal-
izing denominator is taken over the queue of pre-
computed negatives in the batch.

Following He et al. (2020), to reduce memory
consumption during pre-training, we cache em-
bedded programs from past batches in a queue
containing negative samples, as shown in Fig. 6.
The query encoder fq is trained via gradient de-
scent while the key encoder fk is trained slowly
via an exponential moving average (EMA) of the
query encoder parameters. The EMA update sta-
bilizes the pre-computed key embeddings across
training iterations. Since keys are only embedded
once per epoch, we use a very large set of nega-
tives, over 100K, with minimal additional compu-
tational cost and no explicit hard negative mining.

Figure 6: ContraCode pre-trains a neural program encoder fq and transfers it to downstream tasks. A-B. Unlabeled
programs are transformed C. into augmented variants. D. We pre-train fq by maximizing similarity of projected
embeddings of positive program pairs–variants of the same program–and minimizing similarity with a queue of
cached negatives. E. ContraCode supports any architecture for fq that produces a global program embedding such
as Transformers and LSTMs. fq is then ﬁne-tuned on smaller labeled datasets.

ContraCode is agnostic to the architecture of the
program encoder fq. We evaluate contrastive pre-
training of 6-layer Transformer (Vaswani et al.,
2017) and 2-layer BiLSTM (Schuster and Paliwal,
1997; Huang et al., 2015) architectures (§4).

Transfer learning After pre-training converges,
the encoder fq is transferred to downstream tasks.
For code clone detection, we use fq(x) without
ﬁne-tuning. For tasks where the output space dif-
fers from the encoder, we add a task-speciﬁc MLP
or Transformer decoder after fq, then ﬁne-tune the
resulting network end-to-end on labeled task data.

4 Evaluation

In order to evaluate whether ContraCode defend
against adversarial code inputs, we benchmark ad-
versarial code clone detection accuracy (Baker,
1992). We evaluate results over natural and adver-
sarial edits. We then evaluate how improvements
to adversarial robustness translate to improve-
ments on established in-the-wild code bench-
marks. While improvements on adversarial bench-
marks would not be expected to translate to real
code, we ﬁnd signiﬁcant improvements in extreme
code summarization (Allamanis et al., 2016) and
type inference (Hellendoorn et al., 2018) tasks.

Clone detection experiments show that con-
trastive and hybrid representations with our
compiler-based augmentations are predictive of
program functionality in-the-wild, and that con-
trastive representations are the most robust to ad-
versarial edits (§4.1). Contrastive pre-training out-
performs baseline supervised and self-supervised

methods on all three tasks (§4.1-4.3). Finally, ab-
lations suggest it is better to augment unlabeled
programs during pre-training rather than augment-
ing smaller supervised datasets (§4.4).

Experimental setup Models are pre-trained on
CodeSearchNet, a large corpus of methods ex-
tracted from popular GitHub repositories (Husain
et al., 2019). CodeSearchNet contains 1,843,099
JavaScript programs. Only 81,487 methods have
both a documentation string and a method name.
The asymmetry between labeled and unlabeled
programs stems from JavaScript coding practices
where anonymous functions are widespread. The
pre-training dataset described in Section 3.1 is the
result of augmenting all 1.8M programs.

As our approach supports any encoder, we eval-
uate two architectures: a 2-layer Bidirectional
LSTM with 18M parameters, similar to the super-
vised model used by Hellendoorn et al. (2018),
and a 6-layer Transformer with 23M parameters.
For a baseline self-supervised approach, we pre-
train both architectures with the RoBERTa MLM
objective, then transfer it to downstream tasks.

4.1 Robust Zero-shot Code Clone Detection

ContraCode learns to match variants of programs
with similar functionality. While transformations
produce highly diverse token sequences (quanti-
ﬁed in the supplement), they are artiﬁcial and do
In con-
not change the underlying algorithm.
trast, human programmers can solve a problem
with many data structures, algorithms and pro-
gramming models. To determine whether pre-

function (...) {    for ...}function log() {    var num = ...}function () {... }UnlabeledprogramsContraCodecompilerfunction (...) {    while ...}function x() {    var a = ... }function () {... }AugmentedvariantsSample & tokenizepositivesfq<latexit sha1_base64="kwvFlZZfW/SthOGV+o99mjhVx5Y=">AAAB+HicbVBNS8NAEN3Urxo/GvXoZbEUPJWkCnosevFYwbZCE8pmu2mXbjZxdyLU0F/ixYMiXv0p3vw3btsctPXBwOO9GWbmhangGlz32yqtrW9sbpW37Z3dvf2Kc3DY0UmmKGvTRCTqPiSaCS5ZGzgIdp8qRuJQsG44vp753UemNE/kHUxSFsRkKHnEKQEj9Z1Kzc+xDyTD/tSO+g99p+rW3TnwKvEKUkUFWn3nyx8kNIuZBCqI1j3PTSHIiQJOBZvafqZZSuiYDFnPUElipoN8fvgU14wywFGiTEnAc/X3RE5irSdxaDpjAiO97M3E/7xeBtFlkHOZZsAkXSyKMoEhwbMU8IArRkFMDCFUcXMrpiOiCAWTlW1C8JZfXiWdRt07qzduz6vNqyKOMjpGJ+gUeegCNdENaqE2oihDz+gVvVlP1ov1bn0sWktWMXOE/sD6/AFdEpJA</latexit>fk<latexit sha1_base64="/tyrMSrE/ZiFZKSpzmob4gk8Ehs=">AAAB+HicbVDLSgNBEJyNr7g+EvXoZTAEPIXdKOgx6MVjBPOA7LLMTmaTIbMPZnqEuORLvHhQxKuf4s2/cZLsQRMLGoqqbrq7wkxwBY7zbZU2Nre2d8q79t7+wWGlenTcVamWlHVoKlLZD4ligiesAxwE62eSkTgUrBdObud+75FJxdPkAaYZ82MySnjEKQEjBdVK3cuxB0Rjb2ZHwSSo1pyGswBeJ25BaqhAO6h+ecOU6pglQAVRauA6Gfg5kcCpYDPb04plhE7IiA0MTUjMlJ8vDp/hulGGOEqlqQTwQv09kZNYqWkcms6YwFitenPxP2+gIbr2c55kGlhCl4siLTCkeJ4CHnLJKIipIYRKbm7FdEwkoWCysk0I7urL66TbbLgXjeb9Za11U8RRRqfoDJ0jF12hFrpDbdRBFGn0jF7Rm/VkvVjv1seytWQVMyfoD6zPH1P6kjo=</latexit>Embedq<latexit sha1_base64="H/jqpSm08dtfkSIPszp3ThFAjbM=">AAAB+3icbVBNS8NAEJ3Urxq/Yj16WSwFTyWpgh6LXjxWsK3QhLDZbtqlmw93N2IJ+StePCji1T/izX/jts1BWx8MPN6bYWZekHImlW1/G5W19Y3Nreq2ubO7t39gHdZ6MskEoV2S8ETcB1hSzmLaVUxxep8KiqOA034wuZ75/UcqJEviOzVNqRfhUcxCRrDSkm/VGm6OXIUz5BZmI/Qn5oNv1e2mPQdaJU5J6lCi41tf7jAhWURjRTiWcuDYqfJyLBQjnBamm0maYjLBIzrQNMYRlV4+v71ADa0MUZgIXbFCc/X3RI4jKadRoDsjrMZy2ZuJ/3mDTIWXXs7iNFM0JotFYcaRStAsCDRkghLFp5pgIpi+FZExFpgoHZepQ3CWX14lvVbTOWu2bs/r7asyjiocwwmcggMX0IYb6EAXCDzBM7zCm1EYL8a78bForRjlzBH8gfH5A8Rekvg=</latexit>k+<latexit sha1_base64="t0gZiTCKHWVxVpLPSKOU8Yznuqw=">AAAB/XicbVDLSsNAFJ34rPEVHzs3g6UgCCWpgi6LblxWsA9oYphMJ+3QyYOZG6GG4q+4caGIW//DnX/jtM1CWw9cOJxzL/feE6SCK7Dtb2NpeWV1bb20YW5ube/sWnv7LZVkkrImTUQiOwFRTPCYNYGDYJ1UMhIFgrWD4fXEbz8wqXgS38EoZV5E+jEPOSWgJd86rLg5doFk2B2bldAfmsP7U98q21V7CrxInIKUUYGGb325vYRmEYuBCqJU17FT8HIigVPBxqabKZYSOiR91tU0JhFTXj69fowrWunhMJG6YsBT9fdETiKlRlGgOyMCAzXvTcT/vG4G4aWX8zjNgMV0tijMBIYET6LAPS4ZBTHShFDJ9a2YDogkFHRgpg7BmX95kbRqVeesWrs9L9evijhK6AgdoxPkoAtURzeogZqIokf0jF7Rm/FkvBjvxsesdckoZg7QHxifP+DBk48=</latexit>{⌧}<latexit sha1_base64="j3QNhmhlNDyVQ5We0b7LPMuLalM=">AAAB/XicbVDLSsNAFJ34rPEVHzs3g6XgxpJUQZdFNy4r2Ac0MUymk3bo5MHMjVBD8VfcuFDErf/hzr9x2mahrQcuHM65l3vvCVLBFdj2t7G0vLK6tl7aMDe3tnd2rb39lkoySVmTJiKRnYAoJnjMmsBBsE4qGYkCwdrB8Hritx+YVDyJ72CUMi8i/ZiHnBLQkm8dujl2gWTYHZuV0B+aleH9qW+V7ao9BV4kTkHKqEDDt77cXkKziMVABVGq69gpeDmRwKlgY9PNFEsJHZI+62oak4gpL59eP8YVrfRwmEhdMeCp+nsiJ5FSoyjQnRGBgZr3JuJ/XjeD8NLLeZxmwGI6WxRmAkOCJ1HgHpeMghhpQqjk+lZMB0QSCjowU4fgzL+8SFq1qnNWrd2el+tXRRwldISO0Qly0AWqoxvUQE1E0SN6Rq/ozXgyXox342PWumQUMwfoD4zPH+Z4k5E=</latexit>Enqueue as future negativeMaximizeMinimizek <latexit sha1_base64="3xGimkeO8CBBu+b44T+IBr6WERk=">AAAB/XicbVDLSsNAFJ34rPEVHzs3g6XgxpJUQZdFNy4r2Ac0MUymk3bo5MHMjVBD8VfcuFDErf/hzr9x2mahrQcuHM65l3vvCVLBFdj2t7G0vLK6tl7aMDe3tnd2rb39lkoySVmTJiKRnYAoJnjMmsBBsE4qGYkCwdrB8Hritx+YVDyJ72CUMi8i/ZiHnBLQkm8dVtwcu0Ay7I7NSugPzeH9qW+V7ao9BV4kTkHKqEDDt77cXkKziMVABVGq69gpeDmRwKlgY9PNFEsJHZI+62oak4gpL59eP8YVrfRwmEhdMeCp+nsiJ5FSoyjQnRGBgZr3JuJ/XjeD8NLLeZxmwGI6WxRmAkOCJ1HgHpeMghhpQqjk+lZMB0QSCjowU4fgzL+8SFq1qnNWrd2el+tXRRwldISO0Qly0AWqoxvUQE1E0SN6Rq/ozXgyXox342PWumQUMwfoD4zPH+PJk5E=</latexit>k <latexit sha1_base64="3xGimkeO8CBBu+b44T+IBr6WERk=">AAAB/XicbVDLSsNAFJ34rPEVHzs3g6XgxpJUQZdFNy4r2Ac0MUymk3bo5MHMjVBD8VfcuFDErf/hzr9x2mahrQcuHM65l3vvCVLBFdj2t7G0vLK6tl7aMDe3tnd2rb39lkoySVmTJiKRnYAoJnjMmsBBsE4qGYkCwdrB8Hritx+YVDyJ72CUMi8i/ZiHnBLQkm8dVtwcu0Ay7I7NSugPzeH9qW+V7ao9BV4kTkHKqEDDt77cXkKziMVABVGq69gpeDmRwKlgY9PNFEsJHZI+62oak4gpL59eP8YVrfRwmEhdMeCp+nsiJ5FSoyjQnRGBgZr3JuJ/XjeD8NLLeZxmwGI6WxRmAkOCJ1HgHpeMghhpQqjk+lZMB0QSCjowU4fgzL+8SFq1qnNWrd2el+tXRRwldISO0Qly0AWqoxvUQE1E0SN6Rq/ozXgyXox342PWumQUMwfoD4zPH+PJk5E=</latexit>k <latexit sha1_base64="3xGimkeO8CBBu+b44T+IBr6WERk=">AAAB/XicbVDLSsNAFJ34rPEVHzs3g6XgxpJUQZdFNy4r2Ac0MUymk3bo5MHMjVBD8VfcuFDErf/hzr9x2mahrQcuHM65l3vvCVLBFdj2t7G0vLK6tl7aMDe3tnd2rb39lkoySVmTJiKRnYAoJnjMmsBBsE4qGYkCwdrB8Hritx+YVDyJ72CUMi8i/ZiHnBLQkm8dVtwcu0Ay7I7NSugPzeH9qW+V7ao9BV4kTkHKqEDDt77cXkKziMVABVGq69gpeDmRwKlgY9PNFEsJHZI+62oak4gpL59eP8YVrfRwmEhdMeCp+nsiJ5FSoyjQnRGBgZr3JuJ/XjeD8NLLeZxmwGI6WxRmAkOCJ1HgHpeMghhpQqjk+lZMB0QSCjowU4fgzL+8SFq1qnNWrd2el+tXRRwldISO0Qly0AWqoxvUQE1E0SN6Rq/ozXgyXox342PWumQUMwfoD4zPH+PJk5E=</latexit>k <latexit sha1_base64="3xGimkeO8CBBu+b44T+IBr6WERk=">AAAB/XicbVDLSsNAFJ34rPEVHzs3g6XgxpJUQZdFNy4r2Ac0MUymk3bo5MHMjVBD8VfcuFDErf/hzr9x2mahrQcuHM65l3vvCVLBFdj2t7G0vLK6tl7aMDe3tnd2rb39lkoySVmTJiKRnYAoJnjMmsBBsE4qGYkCwdrB8Hritx+YVDyJ72CUMi8i/ZiHnBLQkm8dVtwcu0Ay7I7NSugPzeH9qW+V7ao9BV4kTkHKqEDDt77cXkKziMVABVGq69gpeDmRwKlgY9PNFEsJHZI+62oak4gpL59eP8YVrfRwmEhdMeCp+nsiJ5FSoyjQnRGBgZr3JuJ/XjeD8NLLeZxmwGI6WxRmAkOCJ1HgHpeMghhpQqjk+lZMB0QSCjowU4fgzL+8SFq1qnNWrd2el+tXRRwldISO0Qly0AWqoxvUQE1E0SN6Rq/ozXgyXox342PWumQUMwfoD4zPH+PJk5E=</latexit>k <latexit sha1_base64="3xGimkeO8CBBu+b44T+IBr6WERk=">AAAB/XicbVDLSsNAFJ34rPEVHzs3g6XgxpJUQZdFNy4r2Ac0MUymk3bo5MHMjVBD8VfcuFDErf/hzr9x2mahrQcuHM65l3vvCVLBFdj2t7G0vLK6tl7aMDe3tnd2rb39lkoySVmTJiKRnYAoJnjMmsBBsE4qGYkCwdrB8Hritx+YVDyJ72CUMi8i/ZiHnBLQkm8dVtwcu0Ay7I7NSugPzeH9qW+V7ao9BV4kTkHKqEDDt77cXkKziMVABVGq69gpeDmRwKlgY9PNFEsJHZI+62oak4gpL59eP8YVrfRwmEhdMeCp+nsiJ5FSoyjQnRGBgZr3JuJ/XjeD8NLLeZxmwGI6WxRmAkOCJ1HgHpeMghhpQqjk+lZMB0QSCjowU4fgzL+8SFq1qnNWrd2el+tXRRwldISO0Qly0AWqoxvUQE1E0SN6Rq/ozXgyXox342PWumQUMwfoD4zPH+PJk5E=</latexit>k <latexit sha1_base64="3xGimkeO8CBBu+b44T+IBr6WERk=">AAAB/XicbVDLSsNAFJ34rPEVHzs3g6XgxpJUQZdFNy4r2Ac0MUymk3bo5MHMjVBD8VfcuFDErf/hzr9x2mahrQcuHM65l3vvCVLBFdj2t7G0vLK6tl7aMDe3tnd2rb39lkoySVmTJiKRnYAoJnjMmsBBsE4qGYkCwdrB8Hritx+YVDyJ72CUMi8i/ZiHnBLQkm8dVtwcu0Ay7I7NSugPzeH9qW+V7ao9BV4kTkHKqEDDt77cXkKziMVABVGq69gpeDmRwKlgY9PNFEsJHZI+62oak4gpL59eP8YVrfRwmEhdMeCp+nsiJ5FSoyjQnRGBgZr3JuJ/XjeD8NLLeZxmwGI6WxRmAkOCJ1HgHpeMghhpQqjk+lZMB0QSCjowU4fgzL+8SFq1qnNWrd2el+tXRRwldISO0Qly0AWqoxvUQE1E0SN6Rq/ozXgyXox342PWumQUMwfoD4zPH+PJk5E=</latexit>Contrastivepre-trainingArchitectureagnosticBADCEPoolMHALSTMLSTMLSTMTransformerBiLSTMfq<latexit sha1_base64="kwvFlZZfW/SthOGV+o99mjhVx5Y=">AAAB+HicbVBNS8NAEN3Urxo/GvXoZbEUPJWkCnosevFYwbZCE8pmu2mXbjZxdyLU0F/ixYMiXv0p3vw3btsctPXBwOO9GWbmhangGlz32yqtrW9sbpW37Z3dvf2Kc3DY0UmmKGvTRCTqPiSaCS5ZGzgIdp8qRuJQsG44vp753UemNE/kHUxSFsRkKHnEKQEj9Z1Kzc+xDyTD/tSO+g99p+rW3TnwKvEKUkUFWn3nyx8kNIuZBCqI1j3PTSHIiQJOBZvafqZZSuiYDFnPUElipoN8fvgU14wywFGiTEnAc/X3RE5irSdxaDpjAiO97M3E/7xeBtFlkHOZZsAkXSyKMoEhwbMU8IArRkFMDCFUcXMrpiOiCAWTlW1C8JZfXiWdRt07qzduz6vNqyKOMjpGJ+gUeegCNdENaqE2oihDz+gVvVlP1ov1bn0sWktWMXOE/sD6/AFdEpJA</latexit>MLPMLPNatural code

Adversarial (N =4) Adversarial (N =16)

AUROC AP AUROC
69.55±0.81 73.75 31.63±0.82
Edit distance heuristic
72.31±0.79 75.82 22.72±0.20
Randomly initialized Transformer
74.04±0.77 77.65 25.83±0.21
+ RoBERTa MLM pre-train
75.73±0.75 78.02 64.97±0.24
+ ContraCode pre-train
+ ContraCode + RoBERTa MLM 79.39±0.70 81.47 37.81±0.24

AP
42.85
37.73
39.46
66.23
51.42

AUROC
12.11±0.54
3.09±0.28
4.51±0.33
58.32±0.88
10.09±0.50

AP
32.46
30.95
31.17
59.66
32.52

Table 2: Zero-shot code clone detection with cosine similarity probe. Contrastive and hybrid representations
improve clone detection AUROC on unmodiﬁed (natural) HackerRank programs by +8% and +10% AUROC
over a heuristic textual similarity probe, respectively, suggesting they are predictive of functionality. Contrastive
representations are also the most robust to adversarial code transformations.

Figure 7: Code clone detection example. These programs solve the same HackerRank coding challenge (reading
and summing two integers), but use different coding conventions. The neural code clone detector should classify
this pair as a positive, i.e. a clone.

trained representations are consistent across pro-
grams written by different people, we benchmark
code clone detection, a binary classiﬁcation task to
detect whether two programs solve the same prob-
lem or different ones (Fig. 7). This is useful for
deduplicating, refactoring and retrieving code, as
well as checking approximate code correctness.

Benchmarks exist like BigCloneBench (Sva-
jlenko et al., 2014), but to the best of our knowl-
edge, there is no benchmark for the JavaScript.
We collected 274 in-the-wild JavaScript programs
that correctly solve 33 problems from the Hack-
erRank interview preparation website. There are
2065 pairs solving the same problem and 70K
pairs solving different problems, which we ran-
domly subsample to 2065 to balance the classes.

Since we probe zero-shot performance based
on pre-trained representations, there is no train-
Instead, we threshold cosine similarity
ing set.
of pooled representations of the programs u and
v: uT v/
. Many code analysis methods for
(cid:107)
clone detection measure textual similarity (Baker,

v
(cid:107)(cid:107)

u

(cid:107)

1992). As a baseline, we threshold the dissimi-
larity score, a scaled Levenshtein edit distance be-
tween normalized and tokenized programs.

Table 2 reports the area under the ROC curve
(AUROC) and average precision (AP, area under
Precision-Recall). All learned representations im-
prove over the heuristic on natural code. Self-
supervision through RoBERTa MLM pre-training
improves over a randomly initialized network by
+1.7% AUROC. Contrastive pre-training achieves
+3.4% AUROC over the same baseline. A hybrid
objective combining both the contrastive loss and
MLM has the best performance with +7.0% AU-
ROC (+5.4% over MLM alone). Although MLM
is still useful over natural code, ContraCode learns
overall stronger representations of functionality.

However, are these representations robust to
code edits? We adversarially edit one program in
each pair by applying the loss-maximizing code
compression and identiﬁer modiﬁcation transfor-
mation among N samples from Algorithm 1.
These transformations preserve program function-

ality, so ground-truth labels are unchanged. With
only 4 edits, RoBERTa performs worse than the
heuristic (-5.8% AUROC) and worse than random
guessing (50% AUROC), indicating it is highly
sensitive to these kinds of implementation de-
tails. ContraCode retains much of its performance
(+39% AUROC over RoBERTa) as it explicitly
optimizes for invariance to code edits. Surpris-
ingly, the hybrid model is less robust than Contra-
Code alone, perhaps indicating that MLM learns
non-robust features (Ilyas et al., 2019).

4.2 Fine-tuning for Type Inference

JavaScript is a dynamically typed language, where
variable types are determined at runtime based on
the values they represent. Manually annotating
code with types helps tools ﬂag bugs by detecting
incompatible types. Annotations also document
code, but are tedious to maintain. Type inference
tools automatically predict types from context.

To learn to infer

types, we use the an-
notated dataset of TypeScript programs from
DeepTyper (Hellendoorn et al., 2018), excluding
GitHub repositories that were made private or
deleted since publication. The training set con-
tains 15,570 TypeScript ﬁles from 187 repositories
with 6,902,642 total tokens. Validation and test
sets are from held-out repositories. For additional
supervision, missing types are inferred by static
analysis to augment user-deﬁned types as targets.
A 2-layer MLP head predicts types from token
embeddings output by the DeepTyper LSTM. We
early stop based on validation set top-1 accuracy.
the rest of our experiments, baseline
RoBERTa models are pre-trained on the same aug-
mented data as ContraCode for fair comparison.
Learning representations that transfer from unla-
beled JavaScript programs is challenging because
TypeScript supports a superset of JavaScript’s
grammar, with types annotations and other syntac-
tic sugar that need to be learned during ﬁne-tuning.
Further, the pre-training data only has methods
while DeepTyper’s dataset uses entire ﬁles (mod-
ules). The model is only given source code for a
single ﬁle, not dependencies.

For

In Table 3, contrastive pre-training outperforms
all baseline learned methods. ContraCode is ap-
plied in a drop-in fashion to each of the base-
lines. Pre-training with our contrastive objective
and data augmentations yields absolute accuracy
improvements of +1.2%, +6.3%, +2.3% top-1 and

Acc@1 Acc@5
45.11% —

Method
TypeScript CheckJS
DeepTyper, variable name only 28.94% 70.07%
GPT-3 Codex (zero-shot, 175B) 26.62% —
GPT-3 Codex (few-shot, 175B) 30.55% —
Transformer

45.66% 80.08%
40.85% 75.76%
+ RoBERTa MLM pre-train
46.86% 81.85%
+ ContraCode pre-train
+ ContraCode + MLM (hybrid) 47.16% 81.44%
51.73% 82.71%
DeepTyper BiLSTM
50.24% 82.85%
54.01% 85.55%

+ RoBERTa MLM pre-train
+ ContraCode pre-train

Table 3: Type inference accuracy on TypeScript
programs. As ContraCode does not modify model
architecture, contrastive pre-training improves both
BiLSTM and Transformer accuracy (1.5% to 2.28%).
Compared with TypeScript’s built-in type inference,
we improve accuracy by 8.9%.

+1.8%, +5.7%, +2.8% top-5 over the Transformer,
RoBERTa, and DeepTyper, respectively.

The RoBERTa baseline may perform poorly
since the MLM objective focuses on token re-
construction that is overly sensitive to local syn-
tactic structure, or because sufﬁcient ﬁne-tuning
data is available, described as weight “ossiﬁca-
tion” by Hernandez et al. (2021). To combine
the approaches, we minimized our loss in addi-
tion to MLM as a hybrid local-global objective to
pre-training a Transformer, improving accuracy by
+6.31% over the RoBERTa Transformer.

We also evaluate the recent GPT-3 Codex model
by OpenAI (an, 2021) using their API. We bench-
mark the 175B parameter DaVinci model in both
a zero-shot as well as a few-shot prompting setup.
Although the Codex model was trained over Type-
Script programs, it performs poorly as it achieves
an accuracy of 26.6% in the zero-shot setup and
30.6% in the few-shot setup. We only evaluate
Top-1 accuracy for GPT-3 models as GPT-3 does
not reliably output conﬁdence scores.

Learning outperforms static analysis by a large
margin. Overall, our best model has +8.9%
higher top-1 accuracy than the built-in Type-
Script CheckJS type inference system, showing
the promise of learned code analysis. Surfac-
ing multiple candidate types can also be useful to
users, while CheckJS only has a single prediction.
Fig. 8 shows two ﬁles from held-out reposito-
ries. For the ﬁrst, our model consistently pre-
dicts the correct return and parameter types. The

Figure 8: A variant of DeepTyper pre-trained with ContraCode generates type annotations for two held-out pro-
grams. The model consistently predicts correct function return types, and often correctly predicts project-speciﬁc
variable types imported at the top of the ﬁle. Metrics are in the top row of Table 8 (not our best performing model).

F1

Precision Recall
Method
10.78% 8.24% 9.34%
code2vec
12.17% 7.65% 9.39%
code2seq
RoBERTa MLM 15.13% 11.47% 12.45%
18.11% 15.78% 16.86%
Transformer
20.34% 14.96% 17.24%
+ ContraCode

Table 4: Results for different settings of code sum-
marization: supervised training with 81K functions,
masked language model pre-training,
training from
scratch and contrastive pre-training with ﬁne-tuning.

model correctly predicts that the variable message
is a string, even though its type is ambiguous with-
out access to the imported write method signa-
ture. For the second, ContraCode predicts 4 of
8 types correctly including ViewContainerRef and
ChangeDetectorRef from the AngularJS library.

4.3 Extreme Code Summarization

The extreme code summarization task asks a
model to predict the name of a method given
its body (Allamanis et al., 2016).
These
names often summarize the method, such as
reverseString(...). Summarization models could
help programmers interpret poorly documented
code. We create a JavaScript summarization
dataset using the 81,487 labeled methods in the
CodeSearchNet dataset. The name is masked in
the method declaration. A sequence-to-sequence
model with an autoregressive decoder is trained to

function x(url, callback, error) {

var img = new Image();
img.src = url;
if(img.complete){

return callback(img);

}
img.onload = function(){

img.onload = null;
callback(img);

};
img.onerror = function(e){

img.onerror = null;
error(e);

};

}

Ground truth: loadImage
Prediction: loadImage

Top predictions:

1. getImageItem

2. createImage

3. loadImageForBreakpoint

4. getImageSrcCSS

Figure 9: A held-out JavaScript program from Code-
SearchNet and method names generated by a Trans-
former pre-trained with ContraCode.
The correct
method name is predicted as the most likely decoding.

maximize log likelihood of the ground-truth name,
a form of abstractive summarization. All models
overﬁt, so we stop early according to validation
loss. As proposed by Allamanis et al. (2016), we
evaluate model predictions by precision, recall and
F1 scores over the set of method name tokens.

Table 4 shows results in four settings: (1) su-
pervised training using baseline tree-structured
architectures that analyze the AST (code2vec,
code2seq), (2) pre-training on all 1.8M programs
using MLM followed by ﬁne-tuning on the labeled
programs (RoBERTa), (3) training a Transformer
from scratch and (4) contrastive pre-training fol-
lowed by ﬁne-tuning with augmentations.

Contrastive pre-training outperforms code2seq
by +8.2% test precision, +7.3% recall, and
+7.9% F1 score. ContraCode outperforms self-

Code summarization model
Transformer (Table 4)
+ augmentations

F1
16.86
15.65

Type inference model
Transformer (Table 3)
+ augmentations
DeepTyper (Table 3)
+ augmentations

Acc@1
45.66
44.14
51.73
50.33

Table 5: Compiler data augmentations degrade perfor-
mance when training supervised models from scratch.

supervised pre-training with RoBERTa by +4.8%
F1. ContraCode also achieves slightly higher
performance than the Transformer learned from
scratch. While this improvement is smaller, code
summarization challenging as identiﬁer names are
not consistent between programmers.

Figure 9 shows a qualitative example of pre-
dictions for the code summarization task. The
JavaScript method is not seen during training. A
Transformer pre-trained with ContraCode predicts
the correct method name through beam search.
The next four predictions are reasonable, captur-
ing that the method processes an image. The 2nd
and 3rd most likely decodings, getImageItem and
createImage, use get and create as synonyms for
load, though the ﬁnal two unlikely decodings in-
clude terms not in the method body.

4.4 Understanding augmentation importance

We analyze the effect of augmentations on super-
vised learning and on pre-training.

Supervised learning with augmentations As a
baseline, we re-train models from scratch with
compiler transforms during supervised learning
rather than pre-training. Data augmentation artiﬁ-
cially expands labeled training sets. For sequence-
to-sequence summarization, we apply a variety of
augmentations (LS, SW, VR, DCI). These all pre-
serve the method name. For type inference, labels
are aligned to input tokens, so they must be re-
aligned after transformation. We only apply token-
level transforms (LS, SW) as we can track labels.
Table 5 shows results. Compiler-based data
augmentations degrade supervised models, per-
haps by creating a training distribution not reﬂec-
tive of evaluation programs. However, as shown
in §4.1–4.3, augmenting during ContraCode pre-
training yields a more accurate model. Our con-

Acc@1 Acc@5
Pre-training augmentations
52.65% 84.60%
All augmentations (Table 3)
w/o identiﬁer modiﬁcation (-VR, -IM) 51.94% 84.43%
w/o line subsampling (-LS)
51.05% 81.63%
w/o code compression (-T,C,DCE,CF) 50.69% 81.95%

Table 6: Ablating compiler transformations used dur-
ing contrastive pre-training. The DeepTyper BiLSTM
is pre-trained with constrastive learning for 20K steps,
then ﬁne-tuned for type inference. Augmentations are
only used during pre-training. Each transformation
contributes to accuracy.

trastive learning framework also allows learning
over large numbers of unlabeled programs that su-
pervised learning alone cannot leverage. The ab-
lation indicates that augmentations do not sufﬁce,
and contrastive learning is important.

Ablating pre-training augmentations Some
data augmentations could be more valuable than
others. Empirically, pre-training converges faster
with a smaller set of augmentations at the same
batch size since the positives are syntactically
more similar, but this hurts downstream perfor-
mance. Table 6 shows that type inference accu-
racy degrades when different groups of augmen-
tations are removed. Semantics-preserving code
compression passes that require code analysis are
the most important, improving top-1 accuracy by
1.95% when included. Line subsampling serves
as a regularizer, but changes program semantics.
LS is relatively less important, but does help accu-
racy. Identiﬁer modiﬁcations preserve semantics,
but change useful naming information.

5 Conclusion

Large-scale code repositories like GitHub are a
powerful resource for learning machine-aided pro-
gramming tools. However, most current code rep-
resentation learning approaches need labels, and
popular label-free self-supervised methods like
RoBERTa are not robust to adversarial inputs. In-
stead of reconstructing tokens like BERT, learn-
ing what code says, we learn what code does. We
propose ContraCode, a contrastive self-supervised
algorithm that learns representations invariant to
transformations via compiler-based data augmen-
tations. In experiments, ContraCode learns effec-
tive representations of functionality, and is robust
to adversarial code edits. We ﬁnd that Contra-
Code signiﬁcantly improves performance on three
downstream JavaScript code understanding tasks.

Acknowledgments

We thank Lisa Dunlap, Jonathan Ho, Koushik
Sen, Rishabh Singh, Aravind Srinivas, Daniel
Rothchild, and Justin Wong for helpful feedback.
In addition to NSF CISE Expeditions Award CCF-
1730628, the NSF GRFP under Grant No. DGE-
1752814, and ONR PECASE N000141612723,
this research is supported by gifts from Amazon
Web Services, Ant Financial, Ericsson, Facebook,
Futurewei, Google, Intel, Microsoft, NVIDIA,
Scotiabank, Splunk and VMware.

References

Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and
Kai-Wei Chang. 2020. A transformer-based ap-
proach for source code summarization. In Proceed-
ings of the 58th Annual Meeting of the Association
for Computational Linguistics, pages 4998–5007,
Online. Association for Computational Linguistics.

Miltiadis Allamanis. 2019. The adverse effects of code
duplication in machine learning models of code. In
Proceedings of the 2019 ACM SIGPLAN Interna-
tional Symposium on New Ideas, New Paradigms,
and Reﬂections on Programming and Software, On-
ward! 2019, page 143–153, New York, NY, USA.
Association for Computing Machinery.

Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu,
and Charles Sutton. 2018. A survey of machine
learning for big code and naturalness. ACM Com-
puting Surveys (CSUR), 51(4):81.

Miltiadis Allamanis, Earl T. Barr, Soline Ducousso,
and Zheng Gao. 2020. Typilus: Neural type hints.
In Programming Language Design and Implemen-
tation (PLDI).

Miltiadis Allamanis, Hao Peng, and Charles Sutton.
2016. A convolutional attention network for ex-
In Proceed-
treme summarization of source code.
ings of the 33nd International Conference on Ma-
chine Learning, ICML 2016, New York City, NY,
USA, June 19-24, 2016, volume 48 of JMLR Work-
shop and Conference Proceedings, pages 2091–
2100. JMLR.org.

Uri Alon, Shaked Brody, Omer Levy, and Eran Ya-
hav. 2019a. code2seq: Generating sequences from
In 7th Inter-
structured representations of code.
national Conference on Learning Representations,
ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.
OpenReview.net.

Uri Alon, Meital Zilberstein, Omer Levy, and Eran Ya-
hav. 2019b. code2vec: Learning distributed repre-
sentations of code. Proceedings of the ACM on Pro-
gramming Languages.

Brenda S. Baker. 1992. A program for identifying du-
plicated code. Computing Science and Statistics.

Sorav Bansal and Alex Aiken. 2006. Automatic gener-
ation of peephole superoptimizers. In Proceedings
of the 12th International Conference on Architec-
tural Support for Programming Languages and Op-
erating Systems, ASPLOS XII, page 394–403, New
York, NY, USA. Association for Computing Ma-
chinery.

Tal Ben-Nun, Alice Shoshana Jakobovits, and Torsten
Hoeﬂer. 2018. Neural code comprehension: A
learnable representation of code semantics. In Ad-
vances in Neural Information Processing Systems
31: Annual Conference on Neural Information Pro-
cessing Systems 2018, NeurIPS 2018, December 3-
8, 2018, Montréal, Canada, pages 3589–3601.

Samuel Benton, Ali Ghanbari, and Lingming Zhang.
2019. Defexts: A curated dataset of reproducible
real-world bugs for modern jvm languages. In 2019
IEEE/ACM 41st International Conference on Soft-
ware Engineering: Companion Proceedings (ICSE-
Companion), pages 47–50. IEEE.

Pavol Bielik and Martin T. Vechev. 2020. Adversarial
robustness for code. In Proceedings of the 37th In-
ternational Conference on Machine Learning, ICML
2020, 13-18 July 2020, Virtual Event, volume 119 of
Proceedings of Machine Learning Research, pages
896–907. PMLR.

Ting Chen, Simon Kornblith, Mohammad Norouzi,
and Geoffrey E. Hinton. 2020a. A simple frame-
work for contrastive learning of visual representa-
tions. In Proceedings of the 37th International Con-
ference on Machine Learning, ICML 2020, 13-18
July 2020, Virtual Event, volume 119 of Proceedings
of Machine Learning Research, pages 1597–1607.
PMLR.

Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaim-
Improved baselines with mo-
ArXiv preprint,

ing He. 2020b.
mentum contrastive learning.
abs/2003.04297.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
In Proceedings of the 2019 Conference
standing.
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers),
pages 4171–4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.

Dumitru Erhan, Yoshua Bengio, Aaron Courville,
Pierre-Antoine Manzagol, Pascal Vincent, and Samy
Bengio. 2010. Why does unsupervised pre-training
help deep learning? Journal of Machine Learning
Research, 11(Feb):625–660.

Mark Chen an. 2021. Evaluating large language mod-
els trained on code. ArXiv preprint, abs/2107.03374.

Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-
aocheng Feng, Ming Gong, Linjun Shou, Bing Qin,

Ting Liu, Daxin Jiang, and Ming Zhou. 2020. Code-
BERT: A pre-trained model for programming and
In Findings of the Association
natural languages.
for Computational Linguistics: EMNLP 2020, pages
1536–1547, Online. Association for Computational
Linguistics.

Rudolf Ferenc, Zoltán Tóth, Gergely Ladányi, István
Siket, and Tibor Gyimóthy. 2018. A public uniﬁed
In Proceedings of the 14th
bug dataset for Java.
International Conference on Predictive Models and
Data Analytics in Software Engineering, pages 12–
21.

James Fogarty, Ryan S. Baker, and Scott E. Hudson.
2005. Case studies in the use of roc curve analysis
for sensor-based estimates in human computer inter-
action. In Proceedings of Graphics Interface 2005,
GI ’05, page 129–136, Waterloo, CAN. Canadian
Human-Computer Communications Society.

Raia Hadsell, Sumit Chopra, and Yann LeCun. 2006.
Dimensionality reduction by learning an invariant
mapping. In 2006 IEEE Computer Society Confer-
ence on Computer Vision and Pattern Recognition
(CVPR’06), volume 2, pages 1735–1742. IEEE.

Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and
Ross B. Girshick. 2020. Momentum contrast for un-
In 2020
supervised visual representation learning.
IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition, CVPR 2020, Seattle, WA, USA,
June 13-19, 2020, pages 9726–9735. IEEE.

Vincent J Hellendoorn, Christian Bird, Earl T Barr, and
Miltiadis Allamanis. 2018. Deep learning type in-
ference. In Proceedings of the 2018 26th ACM Joint
Meeting on European Software Engineering Confer-
ence and Symposium on the Foundations of Software
Engineering, pages 152–162.

Olivier J. Hénaff. 2020. Data-efﬁcient image recogni-
tion with contrastive predictive coding. In Proceed-
ings of the 37th International Conference on Ma-
chine Learning, ICML 2020, 13-18 July 2020, Vir-
tual Event, volume 119 of Proceedings of Machine
Learning Research, pages 4182–4192. PMLR.

Dan Hendrycks, Mantas Mazeika, Saurav Kadavath,
and Dawn Song. 2019. Using self-supervised learn-
ing can improve model robustness and uncertainty.
In Advances in Neural Information Processing Sys-
tems 32: Annual Conference on Neural Information
Processing Systems 2019, NeurIPS 2019, December
8-14, 2019, Vancouver, BC, Canada, pages 15637–
15648.

Danny Hernandez, Jared Kaplan, Tom Henighan, and
Sam McCandlish. 2021. Scaling laws for transfer.

Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis
Allamanis, and Marc Brockschmidt. 2019. Code-
SearchNet challenge: Evaluating the state of seman-
tic code search. ArXiv preprint, abs/1909.09436.

Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras,
Logan Engstrom, Brandon Tran, and Aleksander
Madry. 2019. Adversarial examples are not bugs,
In Advances in Neural Infor-
they are features.
mation Processing Systems 32: Annual Conference
on Neural Information Processing Systems 2019,
NeurIPS 2019, December 8-14, 2019, Vancouver,
BC, Canada, pages 125–136.

Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and
Luke Zettlemoyer. 2016. Summarizing source code
In Proceedings of
using a neural attention model.
the 54th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
pages 2073–2083, Berlin, Germany. Association for
Computational Linguistics.

Rajeev Joshi, Greg Nelson, and Keith Randall. 2002.
In Pro-
Denali: A goal-directed superoptimizer.
ceedings of the ACM SIGPLAN 2002 Conference
on Programming Language Design and Implemen-
tation, PLDI ’02, page 304–314, New York, NY,
USA. Association for Computing Machinery.

Aditya Kanade, Petros Maniatis, Gogul Balakrish-
nan, and Kensen Shi. 2020. Pre-trained contex-
tual embedding of source code. ArXiv preprint,
abs/2001.00059.

Miryung Kim, Thomas Zimmermann, and Nachiap-
pan Nagappan. 2012. A ﬁeld study of refactor-
ing challenges and beneﬁts. In Proceedings of the
ACM SIGSOFT 20th International Symposium on
the Foundations of Software Engineering, pages 1–
11.

James Koppel, Varot Premtoon, and Armando Solar-
Lezama. 2018.
One tool, many languages:
Language-parametric transformation with incre-
mental parametric syntax. Proc. ACM Program.
Lang., 2(OOPSLA).

Taku Kudo. 2018. Subword regularization: Improv-
ing neural network translation models with multiple
subword candidates. In Proceedings of the 56th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 66–
75, Melbourne, Australia. Association for Compu-
tational Linguistics.

Yinhan Liu, Myle Ott, Naman Goyal, and Jingfei Du
an. 2019. Roberta: A robustly optimized bert pre-
training approach. ArXiv preprint, abs/1907.11692.

Laurens van der Maaten and Geoffrey Hinton. 2008.
Journal of machine

Visualizing data using t-sne.
learning research, 9(Nov):2579–2605.

Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec-
tional lstm-crf models for sequence tagging. ArXiv
preprint, abs/1508.01991.

Henry Massalin. 1987. Superoptimizer: A look at the
smallest program. In Proceedings of the Second In-
ternational Conference on Architectual Support for

Programming Languages and Operating Systems,
ASPLOS II, page 122–126, Washington, DC, USA.
IEEE Computer Society Press.

Fábio Santos et al. 2020. Terser: Javascript parser,
mangler and compressor toolkit for es6+. https:
//github.com/terser/terser.

Sebastian McKenzie et al. 2020. Babel: compiler
for writing next generation javascript. https://
github.com/babel/babel.

Charith Mendis, Alex Renda, Saman P. Amarasinghe,
and Michael Carbin. 2019.
Ithemal: Accurate,
portable and fast basic block throughput estimation
using deep neural networks. In Proceedings of the
36th International Conference on Machine Learn-
ing, ICML 2019, 9-15 June 2019, Long Beach, Cal-
ifornia, USA, volume 97 of Proceedings of Machine
Learning Research, pages 4505–4515. PMLR.

Tomás Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013. Distributed rep-
resentations of words and phrases and their com-
In Advances in Neural Information
positionality.
Processing Systems 26: 27th Annual Conference on
Neural Information Processing Systems 2013. Pro-
ceedings of a meeting held December 5-8, 2013,
Lake Tahoe, Nevada, United States, pages 3111–
3119.

Dana Movshovitz-Attias and William W. Cohen. 2013.
Natural language models for predicting program-
In Proceedings of the 51st An-
ming comments.
nual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), pages 35–40,
Soﬁa, Bulgaria. Association for Computational Lin-
guistics.

Aaron van den Oord, Yazhe Li, and Oriol Vinyals.
2018. Representation learning with contrastive pre-
dictive coding. ArXiv preprint, abs/1807.03748.

Irene Vlassi Pandi, Earl T. Barr, Andrew D. Gordon,
and Charles Sutton. 2020. Opttyper: Probabilistic
type inference by optimising logical and natural con-
straints.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learning
in Python. Journal of Machine Learning Research,
12:2825–2830.

Michael Pradel, Georgios Gousios, Jason Liu, and
Satish Chandra. 2019. Typewriter: Neural type
ArXiv
prediction with search-based validation.
preprint, abs/1912.03768.

Michael Pradel and Koushik Sen. 2018. Deepbugs:
A learning approach to name-based bug detection.
Proceedings of
the ACM on Programming Lan-
guages, 2(OOPSLA):1–25.

Md. Raﬁqul

Islam Rabin and Mohammad Amin
Alipour. 2020. Evaluation of generalizability of
neural program analyzers under semantic-preserving
transformations.

Nikunj Saunshi, Orestis Plevrakis, Sanjeev Arora,
Mikhail Khodak, and Hrishikesh Khandeparkar.
2019. A theoretical analysis of contrastive unsuper-
vised representation learning. In Proceedings of the
36th International Conference on Machine Learn-
ing, ICML 2019, 9-15 June 2019, Long Beach, Cal-
ifornia, USA, volume 97 of Proceedings of Machine
Learning Research, pages 5628–5637. PMLR.

Mike Schuster and Kuldip Paliwal. 1997. Bidirectional
recurrent neural networks. Signal Processing, IEEE
Transactions on, 45:2673 – 2681.

Roei Schuster, Congzheng Song, Eran Tromer, and Vi-
taly Shmatikov. 2021. You autocomplete me: Poi-
soning vulnerabilities in neural code completion. In
30th
USENIX
}
{
Security 21).

Security Symposium (

USENIX
}
{

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words
with subword units. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1715–
1725, Berlin, Germany. Association for Computa-
tional Linguistics.

Jeffrey Svajlenko, Judith F. Islam, Iman Keivanloo,
Chanchal K. Roy, and Mohammad Mamun Mia.
Towards a big data curated benchmark
2014.
In Proceedings of
of inter-project code clones.
the 2014 IEEE International Conference on Soft-
ware Maintenance and Evolution, ICSME ’14, page
476–480, USA. IEEE Computer Society.

Wilson L Taylor. 1953. “Cloze procedure”: A new
Journalism Quar-

tool for measuring readability.
terly, 30(4):415–433.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, December 4-
9, 2017, Long Beach, CA, USA, pages 5998–6008.

Ke Wang and Mihai Christodorescu. 2019. COSET:
A benchmark for evaluating neural program embed-
dings. ArXiv preprint, abs/1905.11445.

Ke Wang and Zhendong Su. 2019. Learning blended,

precise semantic program embeddings. ArXiv.

Jiayi Wei, Maruth Goyal, Greg Durrett, and Isil Dil-
lig. 2020. Lambdanet: Probabilistic type infer-
In 8th Inter-
ence using graph neural networks.
national Conference on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30,
2020. OpenReview.net.

Martin White, Michele Tufano, Christopher Vendome,
and Denys Poshyvanyk. 2016. Deep learning code
In 2016 31st
fragments for code clone detection.
IEEE/ACM International Conference on Automated
Software Engineering (ASE), pages 87–98. IEEE.

Zhirong Wu, Yuanjun Xiong, Stella X. Yu, and Dahua
Lin. 2018. Unsupervised feature learning via non-
In 2018 IEEE
parametric instance discrimination.
Conference on Computer Vision and Pattern Recog-
nition, CVPR 2018, Salt Lake City, UT, USA, June
18-22, 2018, pages 3733–3742. IEEE Computer So-
ciety.

Noam Yefet, Uri Alon, and Eran Yahav. 2019. Adver-
sarial examples for models of code. ArXiv preprint,
abs/1910.07517.

Noam Yefet, Uri Alon, and Eran Yahav. 2020. Ad-
Pro-
versarial examples for models of code.
ceedings of the ACM on Programming Languages,
4(OOPSLA):1–30.

Appendices

A Program transformation details

We use the Babel compiler infrastructure (McKen-
zie et al., 2020) and the terser JavaScript li-
brary for AST-based program transformations. We
perform variable renaming and dead code inser-
tion (variable declaration insertion) using cus-
tom Babel
transforms, subword regularization
with sentencepiece Python tokenization library,
line subsampling using JavaScript string manipu-
lation primatives and other transformations with
terser. Terser has two high-level transformation
modes, mangling and compression, each with ﬁner
grained controls such as formatting, comment and
log removal, and dead code elimination. We show
an example merge sort with variants in Figure 10.
Reformatting, beautiﬁcation, compression
(R, B, C): Personal coding conventions do not af-
fect the semantics of code; auto-formatting nor-
malizes according to a style convention.

Dead-code elimination (DCE): In this pass,
all unused code with no side effects are removed.
Various statements can be inlined or removed as
stale or unneeded functionality.

Type upconversion (T): In JavaScript, some
types are polymorphic & can be converted be-
tween each other. As an example, booleans can
be represented as true or as 1.

Constant folding (CF): During constant fold-
ing, all expressions that can be pre-computed at
compilation time can be inlined. For example, the
expression (2 + 3) * 4 is replaced with 20.

Variable renaming, identiﬁer mangling (VR,
IM): Arguments can be renamed with random
word sequences and identiﬁers can be replaced
with short tokens to make the model robust to
naming choices. Program behavior is preserved
despite obfuscation.

Dead-code insertion (DCI): Commonly used
no-ops such as comments and logging are inserted.
Subword regularization (SW): From Kudo
(2018), text is tokenized in several different ways,
with a single word (_function) or subtokens (_func
tion).

Line subsampling (LS): We randomly sample
(p = 0.9) lines from a method body. While not
semantics-preserving, line subsampling serves as
a regularizer.

// Split the array into halves and merge

them recursively
function mergeSort (arr) {
if (arr.length === 1) {

// return once we hit an array with a

single item

return arr

}
const middle = Math.floor(arr.length / 2)
// get the middle item of the array

rounded down

const left = arr.slice(0, middle)
// items on the left side
const right = arr.slice(middle)
// items on the right side
return merge(

mergeSort(left),
mergeSort(right)

)

}

Original merge sort program

function mergeSort(e) {
if (e.length === 1) {

return e;

}
const t = Math.floor(e.length / 2);
const l = e.slice(0, t);
const n = e.slice(t);
return merge(mergeSort(l), mergeSort(n));

}

Variable renaming, comment removal, refor-
matting

function mergeSort(e) {

if (1 === e.length) return e;
const t = Math.floor(e.length / 2), r =
e.slice(0, t), n = e.slice(t);

return merge(mergeSort(r), mergeSort(n));

}

Combining variable declarations, inlining con-
ditional

Figure 10: Given a JavaScript code snippet imple-
menting the merge sort algorithm, we apply semantics-
preserving transformations to produce functionally-
equivalent yet textually distinct code sequences. Vari-
able renaming and identiﬁer mangling passes change
variable names. Compression passes eliminate unnec-
essary characters such as redundant variable declara-
tions and brackets.

B How similar are transformed

programs?

To understand the diversity created by program
transformations, we compute the Levenshtein

Figure 11: Histogram of pairwise token dissimilarity for contrastive positives (transformed variants of the same
method) and negatives (transformed variants of different methods). Code transformations produce positives with
dissimilar token sequences.

minimum edit distance between positive pairs in
the precomputed pre-training dataset, i.e.
trans-
formed variants of the same source method. For
comparison, we also compute the edit distance be-
tween negative pairs: transformed variants of dif-
ferent programs.

The edit distance D(xq, xk) computes the mini-
mum number of token insertions, deletions or sub-
stitutions needed to transform the tokenized query
progrm xq into the key program xk. To normalize
, let
by sequence length

| · |

dissimilarityD(xq, xk) =

D(xq, xk)
max(
xk
xq
|
|

,
|

)
|

(2)

Dissimilarity ranges from 0% for programs
with the same sequence of tokens,
to 100%
for programs without any shared tokens. Note
that whitespace transformations do not affect the
metric because the tokenizer collapses repeated
whitespace. For the positives, we estimate dissim-
ilarity by sampling one pair per source program
in the CodeSearchNet dataset (1.6M source pro-
grams with at least one pair). We sample the same
number of negative pairs.

Fig. 11 shows a histogram of token dissimilar-
ity. Positive pairs have 65% mean dissimilarity,
while negatives have 86%. Negatives are more
dissimilar on average as source sequences could
have different lengths, idioms and functionality.
Still, the transformations generated quite different
positive sequences, with less than half of their to-
kens shared. The 25th, median and 75th percentile
dissimilarity is 59%, 66% and 73% for positives,
and 82%, 87% and 90% for negatives.

C Experimental setup

Architectures
The Transformer encoder has 6
layers (23M parameters) in all experiments. For
code summarization experiments, we add 4 de-
coder layers with causal masking to generate the
natural language summary. We leverage the de-
fault positional embedding function (sin, cos) as
used in the original Transformer architecture. The
network originally proposed in DeepTyper (Hel-
lendoorn et al., 2018) had 11M parameters with
a 300 dimensional hidden state. We increase the
hidden state size to 512 to increase model capac-
ity, so our BiLSTM for type prediction has 17.5M
parameters. During ﬁne-tuning, across all experi-
ments, we optimize parameters using Adam with
linear learning rate warmup and decay. For the
Transformer, the learning rate is linearly increased
for 5,000 steps from 0 to a maximum of 10−4.
For the bidirectional LSTM, the learning rate is
increased for between 2,500 and 10,000 steps to a
maximum of 10−3. Type inference hyperparame-
ters are selected by validation top-1 accuracy.

−

←

mθk + (1

ContraCode pre-training The InfoNCE ob-
jective is minimized with temperature t = 0.07
following He et al. (2020). Also following He
et al. (2020), the key encoder’s parameters are
computed with the momentum update equation
m)θq, equivalent to an EMA
θk
of the query encoder parameters θq. To pretrain a
Transformer using the ContraCode objective, we
ﬁrst embed each token in the program using the
Transformer. However, the InfoNCE objective is
deﬁned in terms of a single embedding for the
full program. The ContraCode Transformer is
pre-trained with a batch size of 96. Our model
averages the 512-dimensional token embeddings
across the sequence, then applies a two-layer MLP

020406080100Token difference (%)0.000.010.020.030.040.050.06Fraction of pairsPositivesNegativeswith 512 hidden units and a ReLU activation to ex-
tract a 128-dimensional embedding for the loss.

The DeepTyper bidirectional LSTM architec-
ture has two choices for extracting a global pro-
gram representation. We aggregate a 1024-
dimensional representation of the program by
concatenating its four
terminal hidden states
(from two sequence processing directions and two
stacked LSTM layers), then apply the same MLP
architecture as before to extract a 128-dimensional
representation. Alternatively, we can average
the hidden state concatenated from each direction
across the tokens in the sequence before applying
the MLP head. We refer to the hidden-state conﬁg-
uration as a global representation and the sequence
averaging conﬁguration as a local representation
in Tab. 8. We pre-train the BiLSTM with large
batch size of 512 and apply weight decay.

Code clone detection on HackerRank pro-
grams
Figure 7 shows two programs sam-
pled from the HackerRank clone detection dataset.
These programs successfully solve the same prob-
lem, so they are clones. We report metrics that
treat code clone detection as a binary classiﬁcation
task given a pair of programs. 2065 pairs of pro-
grams solving the same HackerRank problem and
2065 pairs of programs solving different problems
are sampled to construct an evaluation dataset. We
use the area under the Receiver Operating Char-
acteristic (AUROC) metric and Average Precision
(AP) metrics. The standard error of the AUROC is
reported according to the Wilcoxon statistic (Fog-
arty et al., 2005). Average Precision is the area
under the Precision-Recall curve. AUROC and
AP are both computed using the scikit-learn li-
brary (Pedregosa et al., 2011).

A Transformer predicts contextual embeddings
of each token in a program, but our thresholded
cosine similiarity classiﬁer requires ﬁxed length
embeddings of whole programs. To determine if
two programs that may differ in length are clones,
we pool the token representations across the se-
quence. We evaluated both mean pooling and max
pooling the representation. For the hybrid model
pre-trained with both RoBERTa (MLM) and con-
trastive objectives, mean pooling achieved the best
AUROC and AP. For other models, max pooling
performed the best.

(a) Character length per code sample

(b) Character length per method name

Figure 12: CodeSearchNet code summarization dataset
statistics: (a) The majority of code sequences are un-
der 2000 characters, but there is long tail of programs
that span up to 15000 characters long, (b) JavaScript
method names are relatively short compared to lan-
guages like C(cid:93) and Java.

15,570 TypeScript ﬁles, totaling 6,902,642 tokens.
We tune hyperparameters on a validation set of
23 distinct projects with 1,803 ﬁles and 490,335
tokens, and evaluate on a held-out test set of 24
projects with 2,206 ﬁles and 958,821. The training
set is smaller than originally used in DeepTyper as
several projects were made private or deleted from
GitHub before May 2020 when we downloaded
the data, but we used the same commit hashes
for available projects so our splits are a subset of
the original. We have released the data with our
open-source code to facilitate further work on a
stable benchmark as more repositories are deleted
over time. We perform early stopping to select the
number of training epochs. We train each model
for 100 epochs and select the checkpoint with the
minimum accuracy@1 metric (all types, including
any) on the validation set. Except for the model
learned from scratch, the Transformer architec-
tures are pre-trained for 240K steps. Models with
the DeepTyper architecture converge faster on the
pre-training tasks and are pre-trained for 20K iter-
ations (unless otherwise noted).

Type prediction Following DeepTyper (Hel-
lendoorn et al., 2018), our regenerated dataset
for type prediction has 187 training projects with

Extreme code summarization by method
name prediction We train method prediction
models using the labeled subset of CodeSearch-

050001000015000Method body length00.250.500.751CDF0204060Identifier length00.250.500.751CDFNet. Neither method names nor docstrings are
provided as input to the model:
the docstring is
deleted, and the method name is replaced with the
token ‘x’. Thus, the task is to predict the method
name using the method body and comments alone.
To decode method names from all models ex-
cept the code2vec and code2seq baselines which
implement their own decoding procedures, we use
a beam search with a beam of size 5 and a maxi-
mum target sequence length of 20 subword tokens.
We detail the cumulative distribution of program
lengths in Figure 12. The ContraCode summa-
rization Transformer only needed to be pre-trained
for 20K iterations, with substantially faster con-
vergence than RoBERTa (240K iterations). Dur-
ing ﬁne-tuning, we apply the LS,SW,VR,DCI aug-
mentations to ContraCode.

D Baselines

Baselines for code summarization and type predic-
tion trained their models on an inconsistent set of
programming languages and datasets. In order to
normalize the effect of datasets, we selected sev-
eral diverse state-of-the-art baselines and reimple-
mented them on the JavaScript dataset.

AST-based models

The authors of
code2vec (Alon et al., 2019b) and code2seq (Alon
et al., 2019a), AST-based code understanding
models, made both data and code available, but
train their model on the Java programming lan-
guage. In order to extend the results in their paper
to JavaScript for comparison with our approach,
we generated an AST path dataset for the Code-
SearchNet dataset. The sensitivity of path-mining
embeddings to different datasets is documented in
prior work, so published F1 scores are not directly
comparable; F1 scores for code2vec (Alon et al.,
2019b) vary between 19 (Alon et al., 2019a) and
43 (Alon et al., 2019b) depending on the dataset
used. Therefore, we use the same dataset genera-
tion code as the authors for fair comparison. We
ﬁrst parse the source functions using the Babel
compiler infrastructure. Using the original code
on these ASTs, up to 300 token-to-token (leaf-to-
leaf) paths are extracted from each function’s AST
as a precomputed dataset. Then, we generate a
token and AST node vocabulary using the same
author-provided code, and train the models for 20
epochs, using early stopping for code2seq. We ob-
served that code2vec overﬁts after 20 epochs, and
longer training was not beneﬁcial.

DeepTyper (Hellendoorn et al., 2018) Deep-
Typer uses a two layer GRU with a projection
over possible classes, with an embedding size
of 300 and hidden dimension of 650. However,
we found improved performance by replacing the
GRU with a bidirectional LSTM (BiLSTM). We
normalize the LSTM parameter count to match our
model, and therefore use a hidden dimension size
of 512. We also use subword tokenization rather
than space delimited tokens according to Kudo
(2018), as subwords are a key part of state-of-the-
art models for NLP (Sennrich et al., 2016).

RoBERTa

We pre-trained an encoder
using RoBERTa’s masked language modeling
loss on our augmented version of CodeSearch-
the same data used to pre-train Contra-
Net,
Code. This model is then ﬁne-tuned on down-
stream datasets. Unlike the original BERT pa-
per which cuBERT (Kanade et al., 2020) is based
on, hyperparameters from RoBERTa have been
found to produce better results during pre-training.
RoBERTa pre-trains using a masked language
modeling (MLM) objective, where 15% of tokens
in a sentence are masked or replaced and are re-
constructed by the model. We did not use the
BERT Next Sentence Prediction (NSP) loss which
RoBERTa ﬁnds to be unnecessary. We normalize
baseline parameter count by reducing the number
of Transformer layers from 24 to 6 for a total of
23M parameters.

E Additional results and ablations

Code clone detection ROC, PR curves Fig. 13
plots true postive rate vs false positive rate and pre-
cision vs recall for different zero-shot classiﬁers
on the code clone detection downstream tasks.
These classiﬁers threshold a similarity score given
by token-level edit distance for the heuristic ap-
proach or cosine similarity for the neural network
representations. The hybrid self-supervised model
combining ContraCode’s contrastive objective and
MLM achieves better tradeoffs than the other ap-
proaches. Fig. 14 shows the AUROC and Average
Precision of four Transformer models on the same
task under adversarial transformations of one in-
put program. Untrained models as well as models
pre-trained with RoBERTa’s MLM objective are
not robust to these code transformations. How-
ever, the model pre-trained with ContraCode pre-
serves much of its performance as the adversarial
attack is strengthened.

Table 7: If local representations are learned, transfer-
ring part of the Contrastive MLP head improves type
inference. The encoder is a 2-layer BiLSTM (d=512),
with a 2-layer MLP head for both pre-training purposes
and type inference. The mean hidden state representa-
tion is optimized for 10K iterations for the purposes of
this ablation.

Warm-started layers
Acc@1 Acc@5
49.32% 80.03%
BiLSTM
BiLSTM, 1 layer of MLP 49.15% 82.58%

trastive representation q = fq(x) (global features).
Token-level features might capture more syntactic
details, but averaging pooling ignores order. Ta-
ble 8 shows the accuracy of a BiLSTM pre-trained
with each strategy. Using the global features for
pre-training yields signiﬁcantly improved perfor-
mance, +2.38% acc@1 after 10K iterations of pre-
training (not converged for the purposes of abla-
tion). The global pre-training strategy achieves
our best results.

Do pre-trained encoders help more with shal-
low decoders?
For the sequence-to-sequence
code summarization task, ContraCode only pre-
trains the encoder of the Transformer.
In Ta-
ble 9, we ablate the depth of the decoder to un-
derstand how much shallow decoders beneﬁt from
contrastive pre-training of the encoder. Similar
experiments were performed in a vision context
by (Erhan et al., 2010), where different numbers
of layers of a classiﬁer are pre-trained. After 45k
pre-training steps, the 4-layer decoder achieves
0.50% higher precision, 0.64% higher recall and
0.77% higher F1 score than the 1-layer model, so
additional decoder depth is helpful for the down-
stream task. The 1-layer decoder model also ben-
eﬁts signiﬁcantly from longer pre-training, with
a 6.3% increase in F1 from 10k to 45k itera-
tions. This large of an improvement indicates
that ContraCode could be more helpful for pre-
training when the number of randomly initialized
parameters at the start of ﬁne-tuning is small. For
larger decoders, more parameters must be opti-
mized during-ﬁnetuning, and the value of pre-
training is diminished.

Contrastive representation learning strate-
gies
In Figure 15, we compare two strategies
of refreshing the MoCo queue of key embeddings
(the dictionary of negative program representa-
tions assumed to be non-equivalent to the batch of
positives). In the ﬁrst strategy, we add 8 items out

Figure 13: Receiver Operating Characteristic (ROC)
and Precision-Recall (PR) curves for non-adversarial
classiﬁers on the code clone detection task. Equal F1
score curves are shown on right.

Figure 14: Adversarial AUROC and Average Precision
for four models on the code clone detection task: a
randomly initialized transformer, and transformers pre-
trained on code with the RoBERTa MLM objective, our
contrastive objective, or both. Representations learned
by the contrastive model transfer robustly.

Which part of the model should be trans-
ferred?
SimCLR (Chen et al., 2020a) proposed
using a small MLP head to reduce the dimension-
ality of the representation used in the InfoNCE
loss during pre-training, and did not transfer the
MLP to the downstream image-classiﬁcation task.
In contrast, we ﬁnd it beneﬁcial to transfer part
of the contrastive MLP head to type inference,
showing a 2% improvement in top-5 accuracy over
transferring the encoder only (Table 7). We be-
lieve the improvement stems from ﬁne-tuning both
the encoder and MLP which allows feature adap-
tation, while SimCLR trained a linear model on
top of frozen features. We only transferred the
MLP when contrasting the mean of token embed-
dings during pre-training, not the terminal hidden
states, as the dimensionality of the MLP head dif-
fers. These representations are compared next.

Should we pre-train global or local represen-
tations? We compare pre-training DeepTyper
with two variants of ContraCode. We either use
the mean of token hidden states across the program
(averaging local features), or the terminal hidden
states as input to the MLP used to extract the con-

0.00.20.40.60.81.0False positive rate0.00.20.40.60.81.0True positive rateHybridContrastiveRoBERTaEdit distance0.00.20.40.60.81.0Recall0.00.20.40.60.81.0Precision0.20.40.80246810121416Numberofcodeedits0%20%40%60%80%AreaunderROCContrastiveRoBERTaHybridRandomtransformer0246810121416Numberofcodeedits0%20%40%60%80%AveragePrecisionTable 8: Contrasting global, sequence-level representations outperforms contrasting local representations. We
compare using the terminal (global) hidden states of the DeepTyper BiLSTM and the mean pooled token-level
(local) hidden states.

Representation Optimization

Global

Local

InfoNCE with terminal hidden state, 20K steps
InfoNCE with terminal hidden state, 10K steps
InfoNCE with mean token rep., 10K steps

Acc@1
Acc@5
52.65% 84.60%
51.70% 83.03%
49.32% 80.03%

Table 9: Training time and decoder depth ablation on the method name prediction task. Longer pre-training
signiﬁcantly improves downstream performance when a shallow, 1 layer decoder is used.

Decoder

Pre-training
(1.8M programs)
Transformer, 1 layer MoCo, 10k steps
Transformer, 1 layer MoCo, 45k steps
Transformer, 4 layers MoCo, 45k steps

Supervision
(81k programs)
Original set
Original set
Original set

Precision

Recall

F1

7.49%
5.96%
11.91%
17.71% 12.57% 13.79%
18.21% 13.21% 14.56%

Figure 15: Pre-training quickly converges if negative
programs in the queue are frequently changed.

Figure 16:
t-SNE (Maaten and Hinton, 2008) plot
of mean pooled program representations learned with
masked language modeling (RoBERTa), contrastive
learning (ContraCode), and a hybrid loss (RoBERTa +
ContraCode). Transformed variants of the same pro-
gram share the same color. Note that colors may be
similar across different programs.

by

space

visualizing

representation
self-
supervised representations of variants of 28
programs using t-SNE (Maaten and Hinton, 2008)
in Figure 16. Representations of transformed
variants of the same program are plotted with the
same color. ContraCode (BiLSTM) clusters vari-
ants closely together. Indeed, contrastive learning
learns representations that are invariant to a wide
class of automated compiler-based transforma-
tions. In comparison, the representations learned
by masked language modeling (RoBERTa) show
more overlap between different programs, and
variants do not cleanly cluster. With a hybrid
loss combining masked language modeling and
contrastive learning, representations of variants of
the same program once again cluster.

×

), while in the sec-
of the batch to the queue (1
ond we add 96 items (12
). In addition, we use a
×
larger queue (65k versus 125k keys) and a slightly
larger batch size (64 versus 96). We observe that
for the baseline queue ﬁll rate, the accuracy de-
creases for the ﬁrst 8125 iterations as the queue
ﬁlls. This decrease in accuracy is expected as the
task becomes more difﬁcult due to the increasing
number of negatives during queue warmup. How-
ever, it is surprising that accuracy grows so slowly
once the queue is ﬁlled. We suspect this is be-
cause the key encoder changes signiﬁcantly over
thousands of iterations: with a momentum term
m = 0.999, the original key encoder parame-
10−4 by the
ters are decayed by a factor of 2.9
moving average. If the queue is rapidly refreshed,
queue embeddings are predicted by recent key en-
coders, not old parameters. This also indicates that
a large diversity of negative, non-equivalent pro-
grams are helpful for rapid convergence of Con-
traCode pre-training.

×

t-SNE visualization of representations We
qualitatively inspect the structure of the learned

Top5accuracy1xqueuefillrate12xqueuefillrateTSNE1TSNE2RoBERTA embeddingsTSNE1TSNE2ContraCode embeddingsTSNE1TSNE2RoBERTA + ContraCode embeddings