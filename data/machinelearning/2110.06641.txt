DICTIONARY LEARNING WITH CONVEX UPDATE (ROMD)

Cheng Cheng and Wei Dai

Department of Electrical and Electronic Engineering, Imperial College London, UK

1
2
0
2

t
c
O
5
2

]
P
S
.
s
s
e
e
[

2
v
1
4
6
6
0
.
0
1
1
2
:
v
i
X
r
a

ABSTRACT

Dictionary learning aims to ﬁnd a dictionary under which the
training data can be sparsely represented, and it is usually
achieved by iteratively applying two stages: sparse coding
and dictionary update. Typical methods for dictionary up-
date focuses on reﬁning both dictionary atoms and their cor-
responding sparse coefﬁcients by using the sparsity patterns
obtained from sparse coding stage, and hence it is a non-
convex bilinear inverse problem. In this paper, we propose
a Rank-One Matrix Decomposition (ROMD) algorithm to re-
cast this challenge into a convex problem by resolving these
two variables into a set of rank-one matrices. Different from
methods in the literature, ROMD updates the whole dictio-
nary at a time using convex programming. The advantages
hence include both convergence guarantees for dictionary up-
date and faster convergence of the whole dictionary learning.
The performance of ROMD is compared with other bench-
mark dictionary learning algorithms. The results show the
improvement of ROMD in recovery accuracy, especially in
the cases of high sparsity level and fewer observation data.

Index Terms— ADMM, conjugate gradient method, con-

vex optimization, dictionary learning

1. INTRODUCTION

Sparse signal representation has drawn extensive research in-
terests in recent decades, and it has found a wide range of
applications including signal denoising [1, 2], restoration [3,
4], source separation [5, 6], classiﬁcation [7, 8], recognition
[9, 10, 11], image super-resolution [12, 13] to name a few.
The basic idea of sparse signal representation is that an ob-
served signal can be approximated as a linear combination of
a few number of codewords selecting from a certain dictio-
nary. Thus, a resulting topic from this observation namely
dictionary learning has drawn numerous researchers’ atten-
tion.

As a bilinear inverse problem, typical dictionary learning
algorithms alternate between two stages: sparse coding and
dictionary update. The principle is to ﬁx one variable and
optimize the other. Accordingly, in the sparse coding stage,
the purpose is to ﬁnd the sparse coefﬁcients based on the ﬁxed
dictionary. Its solutions can be generally divided into two cat-
egories, greedy algorithms and ℓ1-norm relaxation. Greedy

algorithms include matching pursuit (MP) [14], orthogonal
matching pursuit (OMP) [15, 16], subspace pursuit (SP) [17],
CoSaMP [18], that sequentially select the support set from
the sparse coefﬁcients. ℓ1-norm relaxation, also known as
basis pursuit (BP) [19], convexiﬁes the problem using a sur-
rogate ℓ1-norm, the variants of which consist of its uncon-
strained version named Lasso [20] and iterative shrinkage-
thresholding algorithms (ISTA) [21, 22, 23].

The other stage dictionary update aims to reﬁne the dictio-
nary using the sparse coefﬁcients obtained from the previous
stage.
In this stage, columns of dictionary, or namely dic-
tionary atoms, are updated either simultaneously [24, 25]
or sequentially [26, 27, 28]. Method of optimal directions
(MOD) [25] is one of the earliest approaches that iteratively
alternate between two stages, and the whole dictionary is
updated in one step. In the dictionary update stage of MOD,
whole sparse coefﬁcient matrix is ﬁxed and then the problem
is formulated as a least squares problem. In many other meth-
ods including K-SVD [26], SimCO [27] and BLOTLESS
[28], only the sparsity pattern (the positions of non-zeros) of
sparse coefﬁcients is preserved, and both the dictionary and
the sparse coefﬁcients are updated. Speciﬁcally, K-SVD ﬁxes
all but one atom and the corresponding row of sparse coefﬁ-
cients, and obtains their difference to the input signal. Only
the elements of the residual at the sparsity pattern is consid-
ered, and the dictionary atom and the corresponding sparse
coefﬁcients is updated by using singular value decomposition
(SVD). SimCO updates multiple dictionary atoms and the
corresponding sparse coefﬁcients by viewing the coefﬁcients
as a function of the dictionary and performing a gradient
descent with respect to dictionary. BLOTLESS recasts the
dictionary update as a total least squares problem, and updates
the blocks of the dictionary and the corresponding elements
of sparse coefﬁcients sequentially. However, these methods
update only one atom or a block of the dictionary and the
corresponding elements in sparse coefﬁcients at a time, and
then sequentially update the whole dictionary.

In the paper, we focus on the dictionary update stage,
where only sparse patterns are given. We recast the whole
dictionary update problem as a convex optimization program-
ming. We decompose the product of two variable into a set
of rank-one matrices and use nuclear norm relaxation to pro-
mote the low-rankness. By considering the non-smoothness
of the nuclear norm, an ADMM framework is applied. Nu-

 
 
 
 
 
 
merical tests compare the performance of ROMD with other
benchmark dictionary learning algorithms in both synthetic
data and real image tests. The results show the improvement
of ROMD in higher recovery accuracy and fewer learning it-
erations.

where D:,k, Xk,: and Q0
k represent the k-th column of D, the
k-th row of X and the rank-one matrix equals to D:,kXk,:,
respectively. Note that a zero entry in X, say Xk,n, results
in a zero column in Qk, i.e., (Qk):,n = D:,kXk,n = 0. To
preserve the non-zero columns in Q0
k, we express a projection
operator Pk which is formulated as

2. BACKGROUND

A dictionary learning problem can be formulated as

kY − DXk2
F

min
D,X
s.t. kD:,kk2 = 1, kX:,nk0 ≤ S, ∀n ∈ [N ], ∀k ∈ [K].

(1)

where Y ∈ RM×N denotes the observed data, D ∈ RM×K
represents the unknown dictionary, X ∈ RK×N refers to
the sparse representation coefﬁcient matrix, X:,n is the n−th
column of X, k·kF represents the Frobenius norm, k·k0
indicates the number of non-zeros elements and [N ]
:=
1, 2, · · · , N . The constraint of unit norm of the dictionary
atom is added to avoid scaling ambiguity, and it is typical
that M < K, i.e., the dictionary is over-complete. The spar-
sity of each column of X is assumed at most S such that
S ≪ K. Most algorithms address (1) by alternating between
two stages: sparse coding and dictionary update.

In sparse coding stage, the dictionary D is ﬁxed, and the

columns of X are obtained by
k Y:,n − DX:,n k2
2

min
X:,n

s.t. k X:,n k0≤ S, ∀n ∈ [N ],

which can be solved by the pursuit algorithms [14, 15, 16, 17,
18].

In dictionary update stage, one can ﬁx either the whole
sparse coefﬁcients, e.g. MOD [25], or the sparsity patterns,
e.g. K-SVD [26], SimCO [27], and Blotless [28], to update
the dictionary. In this paper, we focus on the dictionary update
problem with given only the sparsity patterns.

3. DICTIONARY LEARNING VIA ROMD

3.1. Problem formulation

The dictionary update problem can be formulated as
s.t.RΩ(X) 6= 0,

k Y − DX k2
F

min
D,X

(3)

where Ω denotes the support set of X, and the operator
RΩ(X) retrieves the values of the entries of X in the support
set of Ω. Before we introduce ROMD, we ﬁrst reformulate
the matrix multiplication DX as

DX =

D:,kXk,: =

Q0
k

k
X

P ∗

k (Qk),

=

k
X

k
X

(2)

k
X

:= Q0
Q0
Qk = Pk
k
k,:,n 6= 0},
Ωk := {n : Q0
(cid:1)

(cid:0)

k,:,Ωk ,

(5)

and the operator P ∗

k in (4) denotes the adjoint operator of Pk
According to the rank-one property of the matrix Qk, we
can apply the formulation in (4) to recast the dictionary update
problem (3) into

rank(Qk)

min
Qk

k
X
s.t. Y =

P ∗

k (Qk) ,

(6)

k
X
which is an optimization problem with respect to a set of ma-
trices Qk, and is NP-hard regarding to the Rank function. In-
stead of solving (6) directly, we consider the following convex
relaxation problem

kQkk∗

min
Qk

k
X
s.t. Y =

P ∗

k (Qk) ,

(7)

by replacing the rank function with nuclear norm k·k∗.

It is worth noticing that the bilinear inverse problem (3) is
recast as an optimization problem w.r.t. one variable, which
is a set of matrices, in (7). Furthermore, ROMD formulation
(7) is convex and update the whole dictionary simultaneously.

3.2. An ADMM Solver for ROMD

As the nuclear norm is non-smooth, we solve the optimization
problem (7) via alternating direction method of multipliers
(ADMM) [29]. For (7) involves K variables Qk’s, we intro-
duce another K auxiliary variables Zk ∈ RM×N and rewrite
(7) into the standard ADMM form as

kZkk∗

s.t. Ax + Bz = c,

(8)

min
Qk,Zk

k
X

where x = vec([Q1, · · · , QK]) ∈ RM Pk nk and z =
vec([Z1, · · · , ZK]) ∈ RM Pk nk are the vectors formed by
stacking the columns of matrices Qk and Zk respectively,
and nk is the number of elements in k-th sparsity patterns Ωk.
The matrices A, B and the vector c are in the form of

(4)

A=
(cid:20)

IT
Ω1 ⊗ IM · · ·IT
IM PK nk

ΩK ⊗ IM

(cid:21)

∈ RM(N +Pk nk)×m Pk nk ,

B =

0
−IM Pk nk (cid:21)

(cid:20)

and

∈ RM(N +Pk nk)×m Pk nk ,

Algorithm 1 Standard conjugate gradient method [30]
Input:
q0 = 0,r0 = −ATb and p0 = r0.
Output:
ql.
for l = 0, 1, 2, · · · until rl = 0 do

vec(Y )
0
...
0

c = 







∈ RM(N +Pk nk),





where ⊗ denotes the Kronecker product, IM ∈ RM×M rep-
resents the identity matrix and IΩk ∈ Rnk×N expresses the
truncated identity matrix by removing all the rows in IN in-
dexed by j /∈ Ωk.

• αl = (rl)T rl

(Apl)T (Apl) .

• ql+1 = ql + αlpl.

• rl+1 = rl + αlAT(Apl).

• βl+1 = (rl+1)T rl+1

(rl)T rl

.

• pl+1 = −rl+1 + βl+1pl.

For readability, instead of using the standard ADMM

form (8), we write in the equivalent form as

end

min
Qk,Zk

kZkk∗

k
X
s.t. Y =

P ∗

k (Qk) and Zk = Qk, ∀k ∈ [K].

(9)

k
X

k nk many equality constraints
As there are M N + M
in (9), we denote the corresponding scaled Lagrange mul-
tipliers (see [29, §3.1.1] for details) by Λ0 ∈ RM×N and
Λk ∈ RM×nk , corresponding to the equality constraints
k (Qk) and Zk = Qk, respectively. Then the
Y =
augmented Lagrangian is given by

k P ∗

P

P

Lρ (Qk, Zk, Λ0, Λk)

ρ
2

=

kZkk∗ +

kQk − Zk + Λkk2
F

+

k (cid:16)
X
−Y + Λ0k2

F −

ρ
2

kΛ0k2

F −

ρ
2

(cid:17)
kΛkk2
F ,

k
X

P ∗

k (Qk)

ρ
2

k

k
X

(10)

where ρ > 0 is the penalty parameter. Then the ADMM iter-
ations are given by

k − Λl

0 and Bk = Z l

where B0 = Y − Λl
k. For this quadratic
problem involves a linear mapping of a huge dimension, it
takes a long runtime for using fundamental solvers. Accord-
ing to the speciﬁc structure in (11), we develop a conjugate
gradient (CG) method [30] solver to address this problem.
The standard algorithm of which can be found in Algorithm
1. Instead of directly calculating the matrix-vector products,
which cost the most computation in the original CG method,
we resolve them into efﬁcient calculations based on the struc-
ture of the matrix A. Speciﬁcally, in order to simplify the
matrix-vector products Ap and AT (Ap), we derive the com-
putations based on the structure of matrix A as follows. Let
b′ = Ap ∈ RM(Pk nk+N ). Deﬁne B′ ∈ RM×(Pk nk+N )
K, B′
such that b′ = vec(B′) and write B′ = [B′
0],
0 ∈ RM×N . The
where B′
vector p is in the form of p = vec(P ) = vec([P1, · · · , PK),
where Pk ∈ RM×nk . Then, by the deﬁnition of A, B′ can
k = Pk, ∀k ∈ [K], and B′
be obtained by B′
k (Pk)
which can be calculated efﬁciently using the following itera-
tive process:

k ∈ RM×nk , ∀k ∈ [K] and B′

1, · · · , B′

k P ∗

0 =

P

Ql+1

k = arg min

Qk − Z l

k + Λl
k

Z l+1

k = arg min

Qk
− Y + Λl

(cid:13)
(cid:13)

k
X
0k2
F ,
kZkk∗ +

ρ
2
Zk
k − Z l+1
k + Ql+1
k
Ql+1
P ∗
0 +
k
k

Λl+1

k = Λl
0 = Λl

Λl+1

kQl+1

k − Zk + Λl

kk2
F ,

,

− Y ,

k
X
where l denotes the iteration number.

(cid:1)

(cid:0)

2
F + k
(cid:13)
(cid:13)

k
X

P ∗

k (Qk)

(B′)0 = 0
for k = 1, · · · , K

(11)

(12)

(13)

(14)

(B′)0

:,Ωk = (B′)0

:,Ωk + B′
k

end

To compute w′ = AT (Ap) = AT b′, we deﬁne W ′ such
that w′ = vec(W′) and write W ′ = [W ′
K] ∈
RM×Pk nk . Again, according to the deﬁnition of A, it is
straightforward to attain that

1, · · · , W ′

W ′

k = Bk + Pk(B0) = Bk + (B0):,Ωk .

The optimization problem (11) is a quadratic program-
ming in the form of k Ax − b k2
2. The vector x and the
matrix A here are the same as in the standard ADMM for-
mulation of ROMD (8), and b = vec([B0, B1, · · · , BK]),

The optimization problem (12) can be addressed by soft-
thresholding algorithm. Deﬁne ˆZk = Ql+1
k and its
singular value decomposition (SVD) ˆZk = U diag(σ)V T .

k + Λl

Algorithm 2 Dictionary learning with convex dictionary up-
date (ROMD)
Input:
Measurement Y and initial dictionary D0.
Output:
Dictionary ˆD and Sparse coefﬁcient ˆX.
Repeat until convergence (stopping rule)

• Sparse coding stage: Update the sparsity patterns Ωl in

the sparse coefﬁcients X.

• Dictionary learning stage:
L
l=1 P ∗

Repeat until k

l (Ql) − Y kF /kY kF ≤ ǫ

P

– Update Ql using CG method.
– Update Zl using soft-thresholding.
– Update Λl via (13).
– Update Λ0 via (14).

Update dictionary ˆD and sparse coefﬁcients ˆX: Ql =
U ΣV T , ˆD:,l = u1, ˆXl,Ωl = σ1v1.

Then Z l+1
ˆZk

k

can be obtained by operating soft-thresholding to

M

Z l+1

k =

η

σm;

m=1
X

(cid:18)

1
ρ

(cid:19)

U:,mV T

:,m.

(15)

where η denotes the soft-thresholding operator formulated as
η(σm; 1

.

ρ ) = max

According to the above analysis, we propose our dictio-

nary learning algorithm described in Algorithm 2.

3.3. ROMD with noise

Consider the noisy case, that is the equality constraint in the
ROMD formulation (7) does not hold exactly. Speciﬁcally,
k (Zk) kF ≤ ǫ. For the simplicity of
we assume kY −
composing, we deﬁne the indicator function regarding to the
noisy power ǫ as

k P ∗

P

1k·kF ≤ǫ(X) :=

if k · kF ≤ ǫ,

0,
+∞ otherwise.

(

(16)

By introducing another auxiliary variable W ∈ RM×N , the
noisy form of the optimization problem (9) can be formulated
as

0, σm − 1
ρ
(cid:16)

(cid:17)

kZkk∗ + 1k·kF ≤ǫ(Y − W )

min
Qk,Zk

k
X

s.t. W =

P ∗

k (Qk) and Zk = Qk, ∀k ∈ [K].

(17)

k
X

Then the augmented Lagrangian can be derived as

Lρ (Qk, Zk, W , Λ0, Λk)

kZkk∗ +

kQk − Zk + Λkk2
F

+ 1k·kF ≤ǫ(Y − W )

ρ
2

P ∗

k (Qk) − W + Λ0k2

F −

(cid:17)
kΛ0k2

F −

ρ
2

ρ
2

kΛkk2
F ,

k
X

(18)

=

+

k (cid:16)
X
ρ
k
2

k
X

and the ADMM iterations are hence given by

(19)

(20)

(cid:1)
(21)

(22)

(23)

Ql+1

k = arg min

Qk

+ k

k
X

Z l+1

k = arg min

Zk

W l+1 = arg min

W

Qk − Z l

k + Λl
k

k
X

(cid:13)
(cid:13)

P ∗

k (Qk) − W + Λl

0k2
F ,

2
F

(cid:13)
(cid:13)

kZkk∗ +

ρ
2

kQl+1

k − Zk + Λl

kk2
F ,

1k·kF ≤ǫ(Y − W ) + k

P ∗
k

Ql+1
k

k
X

(cid:0)

,

− W + Λl

0k2
F ,
k − Z l+1
Ql+1

k

Λl+1

k = Λl

k +

Λl+1

0 = Λl

0 +

(cid:0)

P ∗
k

(cid:1)
Ql+1
k

− W

,

!

k
X

(cid:0)

(cid:1)

where l denotes the iteration number. The key difference be-
tween the noisy case and the noise-free case is the updating
step of W in (21). Deﬁne ˆW :=
+ Λl
0, and
the solution of (21) can be straightforwardly obtained by

Ql+1
k

k P ∗
k

W l+1 = ǫ

+ Y .

(24)

(cid:0)

(cid:1)

P
ˆW − Y
k ˆW − Y kF

4. NUMERICAL TESTS

In this section, we ﬁrst compare the performance of only dic-
tionary update stage using different dictionary learning algo-
rithms. The performance of whole dictionary learning in both
noise-free and noisy cases is also tested.

The observation signal Y is formed by Y = ˜D ˜X, where
˜D ∈ RM×K denotes the ground-truth dictionary and ˜X ∈
RK×N represents the ground-truth sparse coefﬁcients. The
values of entries in dictionary ˜D are generated by indepen-
dent standard Gaussian distribution, and each dictionary atom
˜D:,k is with unit ℓ2-norm, i.e. k ˜D:,kk2 = 1 ∀k ∈ [K]. Here
we assume the number of measurements is N and the num-
ber of nonzero coefﬁcients in the n-th column of X 0 is Sn.
The index set of the nonzero coefﬁcients are randomly gen-
and the values
erated from the uniform distribution on
of the nonzero coefﬁcients are independently generated from
the standard Gaussian distribution. In our simulations, we set
Sn = S ∈ N, ∀n ∈ [N ].

K
Sn

(cid:1)

(cid:0)

Performance of a dictionary learning algorithm is evalu-
ated by dictionary recovery error. Consider the permutation

 
20

15

10

5

M
N

/

0.5

0.4

0.3

0.2

0.1

0

0.2

0.4
S/M

0.6

(a) Phase transition ﬁgure of ROMD.

0.4

0.3

r
o
r
r

E

0.2

0.1

0

0

ROMD
BLOTLESS
K-SVD
MOD

50

100

150

Number of Iterations

20

15

10

5

M
N

/

0.5

0.4

0.3

0.2

0.1

0

20

15

10

5

M
N

/

0.5

0.4

0.3

0.2

0.1

0

0.2

0.4
S/M

0.6

0.2

0.4
S/M

0.6

(b) Phase transition ﬁgure of K-
SVD.

(c) Phase transition ﬁgure of MOD.

Fig. 1: Comparison of dictionary learning methods with vary-
ing sparsity levels and number of measurements. Results are
averages of 100 trials.

ambiguity of the trained dictionary and denote the estimated
dictionary as ˆD. As the columns of both ground-true and
estimated dictionaries are normalized, the inner product of a
ground truth dictionary atom and the corresponding column
of a successfully estimated dictionary tends to be 1. Thus, to
evaluate the dictionary recovery error, we formulate it as

Error :=

1
K

K

k=1
X

(1 − ˆDT
:,k

˜D:,ik ),

(25)

where

Fig. 2: Comparison of dictionary learning algorithms with
settings M = 16, K = 24, N = 200 and S = 3. Results are
averages of 100 trials.

the sparsity level is high and when the number of samples is
low.
It is noteworthy that, even for the sparsity level ratio
S/M larger than 0.5, ROMD can still reconstruct the whole
dictionary with sufﬁcient measurements. Also noteworthy
that, when sparsity level is extremely low, that is S = 1, it
needs more number of samples compared with the sparsity
level S = 2. The reason is that, for this case , some rows of
sparse coefﬁcient matrix could be all zeros, which eliminates
the impact of the corresponding columns of the dictionary.

4.2. Noise-free dictionary learning test

Consider the whole dictionary learning process. Here we
compare ROMD with BLOTLESS, K-SVD and MOD. OMP
is adopted for sparse coding stage for all the algorithms
henceforth. As the whole dictionary learning procedure
consists of two stages which alternatively updating two vari-
ables, over-tuning the dictionary update stage could cause the
whole learning process stuck in a local minimum. To avoid
this, we set the stopping criterion of dictionary update stage
for ROMD as

( ˆDT
:,k

ik := arg max
i∈Ik
Ik := [K]\{i1, · · · , iK−1},

˜D:,i),

ˆD:,k denotes the k-th column of estimated dictionary, and
˜D:,ik represents the ik-th column of ground truth dictionary.

4.1. Dictionary update test

In the ﬁrst test, we only focus on the dictionary update stage,
and compare ROMD with two typical dictionary learning al-
gorithms, K-SVD and MOD. We ﬁx the dictionary size to
M = 16 and K = 32. To have a more comprehensive view of
the performance of these three algorithms, we vary both spar-
sity level ratio S/M and number of samples ratio N/M from
1/16 to 3/4 and from 4 to 20, respectively. We repeat each
test for 100 trials to acquire an average value. The phase tran-
sition ﬁgures of three approaches are depicted in Figure 1, and
a darker colour represents a lower error. Compared with K-
SVD and MOD, ROMD has more dark areas, especially when

k

k P ∗

k ZkkF − kY kF
kY kF

P

≤ 10−5,

which is a value not too small. Here we set the penalty pa-
rameter ρ = 0.8 in ROMD.

Figure 2 illustrates the test where we ﬁx the dictionary
size, the sample number and the sparsity level as M = 16,
K = 32, N = 200 and S = 3 respectively, and run each dic-
tionary learning methods for 150 iterations. The results of all
the dictionary recovery errors are the averages of 100 trials.
Figure 2 shows the signiﬁcant improvement of ROMD com-
pared with other benchmark approaches both in the conver-
gence rate and the error rate. Note that ROMD only needs less
than 20 learning iterations to converge, while it takes more
than 100 iterations for the other benchmarks.

Another test compares different dictionary learning meth-
ods with varying number of measurements. There are totally
two simulations, including two settings of dictionary sizes
and sparsity levels of the sparse coefﬁcients. To make ev-
ery algorithm stop at a stable point even with few samples,

matrix decomposition technique. By using ROMD formu-
lation, the whole dictionary can be updated at a time. For
the non-smoothness of nuclear norm, we apply an ADMM
solver.
In numerical tests, we compare ROMD with other
benchmark algorithms in both noise-free and noisy cases.
The results demonstrate that ROMD outperforms the other
benchmarks in terms of the fewer learning iterations, fewer
required number of samples and lower dictionary recovery
error.

0.3

r
o
r
r

E

0.2

0.1

ROMD
BLOTLESS
K-SVD
MOD

0.4

0.3

r
o
r
r

E

0.2

0.1

ROMD
BLOTLESS
K-SVD
MOD

0
100

200

300

400

n
(a) M =16, K =32, S =3.

0

200

400
N
(b) M =24, K =48, S =6

600

Fig. 3: Comparison of dictionary learning methods for dif-
ferent number of measurements. Results are averages of 100
trials.

0.3

0.2

0.1

r
o
r
r

E

ROMD
BLOTLESS
K-SVD
MOD

0.3

0.2

0.1

r
o
r
r

E

ROMD
BLOTLESS
K-SVD
MOD

0
100

400

200

300
N
(a) M =16, K =32, S =3, SNR=
30dB.

500

0
100

200

300

400

500

600

N
(b) M = 16, K = 32, S = 3,
SNR= 20dB.

Fig. 4: Comparison of dictionary learning methods for dif-
ferent dictionary sizes, different sparsity levels and different
number of measurements. Results are averages of 100 trials.

we set the stopping iteration number at 500 for the bench-
mark algorithms and set 50 iterations for ROMD. Again, we
set the penalty parameter ρ = 0.8 in ROMD for both simula-
tions, and we run 100 trails to get an average value. Figure 3
demonstrates that for both tests, ROMD requires fewer sam-
ples to reconstruct the dictionary and can obtain a lower error
level than the other benchmark algorithms.

4.3. Noisy dictionary learning test

In this test, we compare different dictionary learning methods
in noisy cases. We ﬁx the dictionary size and sparsity level as
M = 16, K = 32, S = 3 respectively, and we test the noisy
cases with SNR= 30dB and SNR= 20dB. Here, as there is
a noise in observations, we set a relatively small value for
penalty parameter ρ = 0.02 to ensure that the denoised term
can be decomposed into rank-one matrices. Again, we vary
the number of samples and run each test for 100 trails. The
results are illustrated in Figure 4, which indicate that ROMD
still outperforms the benchmark algorithms.

5. CONCLUSION

In this paper, we propose a dictionary learning algorithm
rank-one matrix decomposition (ROMD), that recasts the dic-
tionary update stage into a convex optimization problem by

6. REFERENCES

[1] M. Elad and M. Aharon, “Image denoising via sparse
and redundant representations over learned dictionar-
ies,” IEEE Transactions on Image processing, vol. 15,
no. 12, pp. 3736–3745, 2006.

[2] K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian, “Im-
age denoising by sparse 3-d transform-domain collabo-
rative ﬁltering,” IEEE Transactions on image process-
ing, vol. 16, no. 8, pp. 2080–2095, 2007.

[3] J. Mairal, M. Elad, and G. Sapiro, “Sparse representa-
tion for color image restoration,” IEEE Transactions on
image processing, vol. 17, no. 1, pp. 53–69, 2008.

[4] W. Dong, L. Zhang, G. Shi, and X. Li, “Nonlocally
centralized sparse representation for image restoration,”
IEEE Transactions on Image Processing, vol. 22, no. 4,
pp. 1620–1630, 2013.

[5] Y. Li, S.-I. Amari, A. Cichocki, D. W. Ho, and S. Xie,
“Underdetermined blind source separation based on
sparse representation,” IEEE Transactions on signal
processing, vol. 54, no. 2, pp. 423–437, 2006.

[6] V. Abolghasemi, S. Ferdowsi, and S. Sanei, “Blind sep-
aration of image sources via adaptive dictionary learn-
ing,” IEEE Transactions on Image Processing, vol. 21,
no. 6, pp. 2921–2930, 2012.

[7] I. Tosic and P. Frossard, “Dictionary learning,” IEEE
Signal Processing Magazine, vol. 28, no. 2, pp. 27–38,
2011.

[8] K. Huang and S. Aviyente, “Sparse representation for
signal classiﬁcation,” in Advances in neural information
processing systems, 2007, pp. 609–616.

[9] J. Wright, A. Y. Yang, A. Ganesh, S. S. Sastry, and
Y. Ma, “Robust face recognition via sparse representa-
tion,” IEEE transactions on pattern analysis and ma-
chine intelligence, vol. 31, no. 2, pp. 210–227, 2009.

[13] W. Dong, L. Zhang, G. Shi, and X. Wu, “Image deblur-
ring and super-resolution by adaptive sparse domain se-
lection and adaptive regularization,” IEEE Transactions
on Image Processing, vol. 20, no. 7, pp. 1838–1857,
2011.

[14] S. Mallat and Z. Zhang, “Matching pursuit with time-
frequency dictionaries,” Courant Institute of Mathemat-
ical Sciences New York United States, Tech. Rep., 1993.

[15] Orthogonal matching pursuit: Recursive function ap-
proximation with applications to wavelet decomposi-
tion.

IEEE, 1993.

[16] J. A. Tropp and A. C. Gilbert, “Signal recovery from
random measurements via orthogonal matching pur-
suit,” IEEE Transactions on information theory, vol. 53,
no. 12, pp. 4655–4666, 2007.

[17] W. Dai and O. Milenkovic, “Subspace pursuit for com-
pressive sensing signal reconstruction,” IEEE transac-
tions on Information Theory, vol. 55, no. 5, pp. 2230–
2249, 2009.

[18] D. Needell and J. A. Tropp, “Cosamp: Iterative sig-
nal recovery from incomplete and inaccurate samples,”
Applied and computational harmonic analysis, vol. 26,
no. 3, pp. 301–321, 2009.

[19] S. S. Chen, D. L. Donoho, and M. A. Saunders, “Atomic
decomposition by basis pursuit,” SIAM review, vol. 43,
no. 1, pp. 129–159, 2001.

[20] R. Tibshirani, “Regression shrinkage and selection via
the lasso,” Journal of the Royal Statistical Society. Se-
ries B (Methodological), pp. 267–288, 1996.

[21] I. Daubechies, M. Defrise, and C. De Mol, “An iterative
thresholding algorithm for linear inverse problems with
a sparsity constraint,” Communications on Pure and Ap-
plied Mathematics: A Journal Issued by the Courant
Institute of Mathematical Sciences, vol. 57, no. 11, pp.
1413–1457, 2004.

[10] J. Wright, Y. Ma, J. Mairal, G. Sapiro, T. S. Huang, and
S. Yan, “Sparse representation for computer vision and
pattern recognition,” Proceedings of the IEEE, vol. 98,
no. 6, pp. 1031–1044, 2010.

[22] E. T. Hale, W. Yin, and Y. Zhang, “A ﬁxed-point con-
tinuation method for l1-regularized minimization with
applications to compressed sensing,” CAAM TR07-07,
Rice University, vol. 43, p. 44, 2007.

[11] L. Zhang, M. Yang, and X. Feng, “Sparse representation
or collaborative representation: Which helps face recog-
nition,” in Computer vision (ICCV), 2011 IEEE interna-
tional conference on.

IEEE, 2011, pp. 471–478.

[23] A. Beck and M. Teboulle, “A fast iterative shrinkage-
thresholding algorithm for linear inverse problems,”
SIAM journal on imaging sciences, vol. 2, no. 1, pp.
183–202, 2009.

[12] J. Yang, J. Wright, T. S. Huang, and Y. Ma, “Image
super-resolution via sparse representation,” IEEE trans-
actions on image processing, vol. 19, no. 11, pp. 2861–
2873, 2010.

[24] B. A. Olshausen and D. J. Field, “Emergence of simple-
cell receptive ﬁeld properties by learning a sparse code
for natural images,” Nature, vol. 381, no. 6583, p. 607,
1996.

[25] K. Engan, S. O. Aase, and J. H. Husoy, “Method of op-
timal directions for frame design,” in Acoustics, Speech,
and Signal Processing, 1999. Proceedings., 1999 IEEE
International Conference on, vol. 5.
IEEE, 1999, pp.
2443–2446.

[26] M. Aharon, M. Elad, A. Bruckstein et al., “K-svd:
An algorithm for designing overcomplete dictionaries
for sparse representation,” IEEE Transactions on signal
processing, vol. 54, no. 11, p. 4311, 2006.

[27] W. Dai, T. Xu, and W. Wang, “Simultaneous codeword
optimization (simco) for dictionary update and learn-
ing,” IEEE Transactions on Signal Processing, vol. 60,
no. 12, pp. 6340–6353, 2012.

[28] Q. Yu, W. Dai, Z. Cvetkovic, and J. Zhu, “Bilinear
dictionary update via linear least squares,” in ICASSP
2019-2019 IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP).
IEEE,
2019, pp. 7923–7927.

[29] S. Boyd, N. Parikh, E. Chu, B. Peleato, J. Eckstein et al.,
“Distributed optimization and statistical learning via the
alternating direction method of multipliers,” Founda-
tions and Trends® in Machine learning, vol. 3, no. 1,
pp. 1–122, 2011.

[30] J. Nocedal and S. J. Wright, “Conjugate gradient meth-
ods,” Numerical optimization, pp. 101–134, 2006.

