1
2
0
2

y
a
M
0
3

]

G
L
.
s
c
[

1
v
6
9
3
4
1
.
5
0
1
2
:
v
i
X
r
a

SyReNets: Symbolic Residual Neural Networks

Carlos Magno C. O. Valle
Technical University of Munich, Germany
carlos.valle@tum.de

Sami Haddadin
Technical University of Munich, Germany
haddadin@tum.de

Abstract

Many research efforts have been put into modeling physical systems dynamics
using data-driven methods. Despite successful seminal works on passive systems
in the literature, learning free-form physical laws for controlled dynamical systems
given experimental data is still an open problem. With precise models, complex
behaviors can be predicted, generalized, planned, and explored with signiﬁcant
accuracy. For decades, symbolic mathematical equations and system identiﬁcation
were the golden standards. Unfortunately, a set of assumptions about the properties
of the underlying system is required, which makes the model very rigid and unable
to adapt to unforeseen changes in the physical system, i.e., there is no individual
strategy that can be applied to hard-to-model or shape-shifting systems without
additional incorporation of speciﬁc prior knowledge about it. Neural networks,
on the other hand, are known universal function approximators but are prone to
over-ﬁt, limited accuracy, and bias problems, which makes them alone unreliable
candidates for such tasks. In this paper, we propose SyReNets, an approach that
leverages neural networks for learning symbolic relations to accurately describe
dynamic physical systems from data. It explores a sequence of symbolic layers
that build, in a residual manner, mathematical relations that describes a given
desired output from input variables. We apply it to learn the symbolic equation
that describes the Lagrangian of a given physical system. We do this by only
observing random samples of position, velocity, and acceleration as input and
torque as output. Therefore, using the Lagrangian as a latent representation from
which we derive torque using the Euler-Lagrange equations. The approach is
evaluated using a simulated controlled double pendulum and compared with a
vanilla implementation of neural networks, genetic programming, and traditional
system identiﬁcation. The results demonstrate that, compared to neural networks
and genetic programming, SyReNets converges to representations that are more
accurate and precise throughout the state space. Despite having slower convergence
than traditional system identiﬁcation, similar to neural networks, the approach
remains ﬂexible enough to adapt to an unforeseen change in the physical system
structure.

1

Introduction

Neural networks have successfully been applied in a complex range of hard to solve problems,
e.g. convolutional neural networks [1], generative adversarial networks [2] and transformers [3],
respectively changed their sub-ﬁelds of research. Yet, there exist areas that are still barely inﬂuenced
by neural architectures. In this paper, we are particularly interested in modeling dynamics of physical
systems. The importance of having a reliable dynamic model is clear for feed-forward motion
generation, for motion planning, and when a swift dynamic response is required. It also allows for
more accurate simulations of the underlying system, which in turn permits longer horizon of motion
predictions. Additionally, the controller would be able to work with smaller gains to achieve a given
state, which leads to less stiff actuators that are safer to interact with [4, 5].

For many decades, the standard approach for modeling the dynamics of a given physical system is
done by measuring or estimating the actuation positions, velocities, accelerations and ﬁnding relations

Preprint. Under review.

 
 
 
 
 
 
between the commanded inputs (torque for example). Those relations would then be formulated by a
set of mathematical equations that describes the motion of the system [6]. As an example, Equation 1
represents the inverse dynamics of a serial-chain rigid-body robotic system:

τ = M (q)¨q + C(q, ˙q) ˙q + G(q)

(1)

It describes the relation between torque (τ ) and position, velocity, and acceleration (q, ˙q, ¨q) at
joint-level. M , C and G are pre-formulated matrix functions that mathematically describe inertial
forces, centrifugal and Coriolis forces, and gravitational forces, respectively, in terms of mass, center
of mass, and inertial matrix of each rigid-body. Traditional system identiﬁcation methods aim to
approximate them and, therefore, is still used due to its mathematical stability in the whole observable
state space of the agent. The disadvantage is that the equations have ﬁxed terms and even if some
of the numerical parameters can be changed over time it limits the physical system to a given set of
assumptions (e.g. shape and actuation type), making it complex to optimize for systems made out of
hard to model materials (e.g. rubber) or that might be subject to unforeseen changes in structure for
different applications.

Neural networks are generally known to be universal function approximators [7] but they are prone to
over-ﬁt to the training data and have biased decisions when trained with an imbalanced or incomplete
dataset [8–10]. Therefore, they, alone, are not reliable enough to be a surrogate model. However,
for most dynamical systems belonging to our speciﬁc use case, there exists an analytical exact
mathematical solution that can accurately describe the dynamics. For those, a different class of
algorithms can be applied. For a lack of better term, we denote them as universal exact function
estimators since instead of approximating a function they seek to estimate the exact underlying one
that solves the problem. The most well known representative of this class is genetic programming
[11], which is an evolutionary strategy for evolving symbolic tree representations. Each tree encodes
a symbolic function. There, several generations of candidate symbolic solutions are tested and
combined until a convergence criterion is reached. Despite suffering the same problems as neural
networks, since those methods are mostly not dependent on gradient, if an underlying function exists
it can eventually be found due to the random nature of the evolutionary process, given enough time
and re-initializations. However, the search is highly inefﬁcient since the process is fundamentally
dependent on randomness, leading to repetitive and "unnecessary" evaluations of candidate solutions.
Therefore, those approaches tend to not scale well with the increase of dimensionality of the problem
[12]. The truth is that exact symbolic estimation is inherently an NP-hard problem, which explains
why dynamic model learning is still an open problem to solve.

We argue that a hybrid approach is a reasonable way to obtain the best of both worlds, namely the
expressiveness of neural network architectures and the stability potential of symbolic representations.
This would allow a general symbolic learning architecture to be applicable to any physical system that
can be expressed analytically, potentially mitigating the disadvantages of "approximation" algorithms
and the search inefﬁciency of the exact estimation methods.

In this paper, we present Symbolic Residual Neural Networks (SyReNets), a neural network archi-
tecture capable of learning symbolic mathematical relations from streams of data. SyReNets can
potentially be applied to estimate a vast number of functions, describing dynamics of many different
physical systems. Traditionally, most methods try to learn the inverse dynamics (as denoted by
Equation 1), since it conveys the relation between position, velocity and acceleration to applied
torques [13]. However, each actuator adds one function to the number of equations to learn. Aiming
at learning a ﬁx and minimal number of equations we chose to learn the Lagrangian representation of
the system:

L = T − V
(2)
Where, L represents the Lagrangian, T is the kinetic energy and V is the potential energy of the
system. By construction, it is always in R, making this a hyper-compressive representation that
maps from Rn → R for n inputs. From the Lagrangian equation, it is possible to derive the inverse
dynamics of the system using the Euler-Lagrange method:

τi =

d
dt

∂L
∂ ˙qi

−

∂L
∂qi

(3)

This operations transform the Lagrangian to m possible actuator torques, effectively constructing
a map from R → Rm. Therefore, the Lagrangian can be considered a highly compact latent
representation to a set of non-injective functions.

The paper is structured as follows: In Section 2 we will discuss related works that take advantage of
energy representations to approximate or estimate physical systems, highlighting what they might be
lacking to be of general use. In Section 3, we present our method in detail, followed by Section 4
where we evaluate the performance of the approach on a simulated double pendulum system against

2

vanilla neural networks, genetic programming and traditional system identiﬁcation. Finally, in
Section 5 we present our conclusions from the experiments, possible future works and potential
societal impacts of our method.

2 Related Works

Related works on learning dynamic representations for physical systems from data can be categorized
into two approaches (as brieﬂy described in Section 1):

2.1 Approximation Approaches

In [14], authors propose a neural network architecture for learning a parametric Lagrangian which
they use to compute the Euler-Lagrange equations by using an automatic differentiation procedure.
They show that it is possible to use the black-box representation of the Lagrangian to obtain estimates
for the acceleration of the system and use those to predict a given trained system behavior just by
having an initial position and velocity. They start from the assumption that the physical system is not
actuated since the energy is stationary (the left-hand side of (3)). This limits the applicability of the
method. This problem is partially solved in [15], where the authors propose to learn the Lagrangian
of the system with variable torque, they use it to build the dynamic equations. Unfortunately, they
assume the system is composed of rigid structures, limiting once more the applicability of the
approach.

The work of [16] proposes a similar approach to the previous one but using the Hamiltonian instead.
They take advantage of the Hamiltonian equations to describe the relation between position, momen-
tum, and Hamiltonian. The problem with their approach is that it depends on having momentum data,
which is not always available and, leads to additional problems as described in [14].

2.2 Exact Estimation Approaches

In the seminal work of Schmidt and Lipson [17], genetic programming [11] was used to evolve a set
of equations for exact symbolic estimation of unknown physical systems. In each generation, they
compute the partial derivative of pairs of variables and compare to numerical partial derivatives, if
the equality is within a given tolerance error the equation is saved, in that way a dictionary of valid
equalities is built. By observing the Pareto frontier on the error and function complexity, several
correct mathematical relations can be obtained. The problem with this approach is that it does not
scale well to more complex problems due to the higher complexity of the search space.

To mitigate this issue, Udrescu et al. [18] recently proposed a state of the art symbolic regression
method that search for pareto-optimal equations from data using a combination of brute-force search,
neural networks approximation and graph modularity simpliﬁcation in order to alleviate the curse of
dimensionality problem. They argue that their method is orders of magnitude more robust to noise
and bad data in comparison to the previous state of the art. But they experiment only with datasets
with directly measured outputs, which is not the goal with our work since we cant obtain measures
for the Lagrangian.

Figure 1: SyReNets high-level architecture

3 Methodology

We start from the minimum set of assumptions that the system of interest is accurately describable
by a mathematical equation of the form f (x) = z, where f (·) denotes the equation to be learned
which maps an input x into an output z. We also assume that z is not necessary directly measurable
(especially in our use case, where we intend to learn the Lagrangian), meaning that there is a known
function g(z) = y, that maps z to y, which is our observable variable. Therefore:

f (x) = z; g(z) = y

(4)

3

SymbolicLayer SymbolicLayer SymbolicLayer SymbolicLayer Figure 2: Detailed representation block of the symbolic layer

with g(·) being a known injective or non-injective transformation function. For our use case g(·) is
the Euler-Lagrange transformation, which is non-injective.

We assume f (·) can be deﬁned as an aggregation or composition of less complex components. Those,
can depend on the original function input x or other sub-components. Therefore, there exists a natural
hierarchical structure that can generate complex mathematical relations from simpler operations. This
relation is evident in:

f (x) = hl(x, hl−1(x, hl−2(...h1(x))))
(5)
Where hi(x, hi−1) is the ith function that applies a given set of operations to the input x and the
previous function hi−1.
Our proposed solution, SyReNets, takes advantage of this modular property assumption and, therefore,
is made of a sequence of simple symbolic layers, which are composed to obtain a more complex
equation. Figure 1 depicts the architecture on a high level. Each symbolic layer receives a set of
samples of the original input x and the output of the previous layer. This allows a residual inspired
behavior [19] where new symbolic relations with x can be generated at every layer and not exclusively
dependent on the outputs of the previous one. The input samples are randomly drawn from a uniform
distribution U(− π
2 ) in order to cover the majority of the state-space. Since we are learning the
Lagrangian of a physical system there is usually no inherit time-dependence, therefore, there is no
need to sample data sequentially.

2 , π

In Figure 2 its possible to see the high-level structure of each symbolic layer. They are composed of
two main parts, a representation transformation block, and k selection heads.

3.1 Representation transformation

The transformation block changes the inputs given to the layer to a representation that conveys the
relation between the input and possible mathematical transformation that can be applied to it. Figure 2
illustrates the process. After concatenating the input x with the previous layer output hi−1, the ﬁrst
sub-component is responsible for generating all possible unique combinations of the input samples
using a given set of mathematical operators. In the scope of this work, we explored the following

4

ConcatenateInputs()Previous LayerOutputs ()Outer-product Contractive Auto-EncoderDecoderEncoderLayer NormCosine similarity attentionRepresentation transformationFeed forward Selection head 1Selection head 1Selection head 1Selection head 1Selection head Symbolic Layer  Outputs ()Figure 3: Detailed selection head block of the symbolic layer

operators: (+, ×, sin, cos). Those were specially chosen because they are sufﬁcient to represent our
use case but many other mathematical operations could be used here. The arity and symmetry of
the operation are essential for an efﬁcient calculation, for unary operations we only need to apply
it to all input samples, but for binary we compute the outer product of the input samples using said
operator, if it is symmetric we care mostly for the upper (or lower) triangle, if not the full matrix
will be required. We did not explore higher arities in this work but an equivalent relation probably
exists. We then collect all unique terms in a vector v which represents all candidate changes that this
speciﬁc layer can do to the input.

In the next sub-block, since we do not necessarily know if the state-space of our output is the same
one as the observed output, We apply a contractive auto-encoder [20], composed by the encoder
fae(·) and the decoder gae(·). It embeds samples of the full vector of candidate changes into a latent
representation of size d that should be more compact, robust by design, and aid in the selection process,
this last relation will be clearer in Section 3.3. Normalization of inputs improves performance of the
auto-encoder, therefore, the inputs are normalized and denoted by v†. We apply layer normalization
[21] because the only information we can assume is that all auto-encoder inputs at a given sample are
from the same point in the state-space of x, therefore, we can ﬁnd relations in that dimension but no
assumption can be made regarding the order of samples. The same auto-encoder is shared between
all symbolic layers to ensure that the latent representation is not dependent on the speciﬁc layer it is
used.

The encoded outputs are then used to compute the cosine similarity between all samples, this serves
to observe similarities between candidate equations without assuming any particular order of samples,
in a similar way as observed in [3]. The symmetric matrix generated of size d × d at this point is
independent of the symbolic layer, since every block until now is shared between all layers. To allow
the architecture to specify for each symbolic layer we added a single fully connected neural network
layer with linear activation that transforms the symmetric matrix row by row, therefore, observing the
similarity between all candidates to a particular one in each operation. This operation denotes a local
representation with the same dimensions as before but conveying understanding of how a particular
candidate relates to the others, this information is then fed to all selection heads.

5

ConcatenateInputs()PreviousLayer Outputs()Symbolic Layer  Outputs ()RepresentationtransformationSelection head Feed forward Feed forward Layer NormFeed forward SoftPlusLayer NormSoftMaxFeed forwardLinear ConstantsFeed forwardSigmoid3.2 Selection heads
Each selection head is responsible for selecting one operation hi
j to ﬂow to the next layer, where j is
the jth selection head if the ith symbolic layer. Figure 3 illustrates the sub-modules of this block,
the ﬁrst operation computes a weighted sum of the input matrix, , converting d × d → d × 1 this is
achieved with a single output single layer neural network with no bias and linear activation. This
operation allows each head to focus on speciﬁc sub-groups of candidate solutions.

The next sub-module is responsible for the actual selection of the candidate, this is done with a fully
connected neural network with one hidden layer with r neurons with softplus activation and an output
layer with do neurons with softmax activation, all layers without bias but with layer normalization
before the activation function. The components of this sub-module were selected targeting minimizing
the number of parameters in each head without potentially damaging the performance of the selection
head. So this block may be subject to variation depending on the complexity of the problem. The
output of this block is a probability distribution P (Oi|hi−1, x) of selecting a given candidate solution.
If we chose to sample a candidate from this distribution we would lose all the information regarding
the gradients of the other candidate solutions since they would become zero. So we opted to use the
full probability distribution as part of the output of each selection head.

To complement, since probability distributions are naturally bounded between 0 and 1, we need to
add a scalar operation S that would allow the selected operation to virtually have no bound. We
used a single-layer fully connected neural network with constant inputs c and do outputs, for this
purpose. As a ﬁnal modiﬁcation we imbued the selection head with the ability to control if its output
should be used or not. We denominate this mechanic as on-off probability gate φ, similar to the forget
mechanism in long short-term memory units [22] and is described by a single-layer fully connected
neural network with sigmoid activation and no bias. It receives as inputs the current values for the
j|H i−1, x)) and the joint probability of the last layer
head selection probability distribution (P (Oi
selection probability distribution (P (Oi−1
|hi−2, x)). If we ensure that every head
has independent probabilities to each other we can easily apply to product rule, therefore:

, ..., Oi−1

, Oi−1
2

k

1

P (Oi−1
1

, Oi−1
2

, ..., Oi−1

k

|hi−2, x) =

P (Oi−1
j

|hi−2, x)

(cid:89)

j

P (φi

j|Oi−1, Oi

j) = σ(W1P (Oi

k|hi−1, x) + W2

P (Oi−1
j

|hi−2, x))

(cid:89)

j

(6)

(7)

where σ is the sigmoid activation function and W1 and W2 are the weights of the neural network.
Finally, all the previously mentioned components are aggregates in the following equation:

j = P (φi
hi

j|Oi−1, Oi
j)

(cid:88)

b

jbvi
Si

bP (Oi

jb|hi−1, x)

(8)

Equation 8 represents the output of the jth selection head of the ith layer. The sum over b represents
the sum over all the discrete probability bins representing candidate operations, this is possible
because we have a ﬁnite number of possible combinations to select from.

All subsequent layers have the same structure with exception to the lth layer, which is the last one.
This one sums all selection heads outputs into a single output, as described by:

ˆf (x) =

(cid:88)

hl
j

j

(9)

3.3 Loss function

The output of the SyReNet architecture is a scalar value generated by the learned estimated symbolic
equation ˆf (x). In our application it represents the Lagrangian of the learned system. Since we can
not easily measure energy, we need to convert it to some measurable magnitude. We, therefore, use
the Euler-Lagrange method as a transformation function g(·) to map the equivalent values in torque.
The basic loss function Lb is the mean squared error (MSE) between the desired torque and the value
obtained by the model, however, this loss alone is not enough for our particular architecture. We also
need to add the loss terms Lae obtained from the auto-encoder.

Lb =

1
N

N
(cid:88)

i

(yi − g( ˆf (xi)))2 + Lae

(10)

6

where,

Lae =

1
N

N
(cid:88)

i

[(vi† − gae(fae(vi†)))2 + λ1

(cid:13)Jfae (vi†)(cid:13)
(cid:13)
2
F ]
(cid:13)

(11)

where λ is a Lagrangian multiplayer that weights the importance of of this extra term in the loss and
Jfae (·) is the Jacobian of the non-linear mapping of the encoder, as stated in [20]. The auto-encoder
is an essential part of the architecture. It allows the attention sub-component to search for similarity
in a latent representation that is in a state space that has a higher probability to be related to y in (4)
since the gradients ﬂowing back from the Lb adjust all learnable weights in order to minimize. If the
auto-encoder was not used we would search for similarity directly in the input variable state space
which might have a non-linear relation to y.

However, some challenges require additional loss terms. One of them is that each symbolic layer is
composed of multiple independent selection heads, since there is no direct communication between
them it is possible to assume that, in some cases, they will converge to the same candidate solution.
For each layer, we want each head, Oi, distribution to be independent from one another, a clear way
to achieve this is to add a cross-entropy loss H(·, ·) between each different head to be maximized.
Additionally, entropy H(·) should be minimized, allowing simpler formulas to be discovered since
lower entropy increases the likelihood of a single equation selected by each head. A complementary
issue is that every time one of the heads changes too much its probability distribution it introduces
disturbances in the whole cascade of layer after it. To mitigate this, we multiplied the φi gate with the
Oi distribution before computing the previously mentioned operations. The idea behind it is to give
the network more ﬂexibility to shift the distribution when φi is low, generating something similar to a
null-space. Equation 12 formulates the above stated complementary loss Lc.

Lc =

l
(cid:88)

k
(cid:88)

i=0

j=0

λ2H(pi

j) − λ3

(cid:88)

j(cid:48)(cid:54)=j

H(pi

j(cid:48), pi
j)

pi
j = P (φi

j|Oi−1, Oi

j)P (Oi

j|hi−1, x))

L = Lb + Lc

(12)

(13)

(14)

where,

the ﬁnal loss is denoted by

4 Experiments

We evaluated the approach in a simulated double pendulum system, which can be veriﬁed analytically.
We aim to evaluate the convergence precision and accuracy of the method by comparing it to
representative approaches from each one of the groups presented in section 2 from the literature,
namely, neural network for approximation methods and genetic programming for exact estimation.
System identiﬁcation was also evaluated as a control measure to compare how far the performance is
to the current most reliable method.
The system under evaluation is dependent on 2 actuated joints, where the ith joint torque τi (outputs),
inverse dynamics dependents on q, ˙q and ¨q (inputs). The equation that describes the Lagrangian L of
this system is:

L = 1

2 ( m1

3 + m2)l2

1 ˙q2

1 + 1

2 ( m2

3 )l2

2 ˙q2

2 + 1

2 m2l1l2 ˙q1 ˙q2 cos (q1 − q2)

+ ( m1

2 + m2)gl1cos(q1) + ( m2

2 )gl2cos(q2)

(15)

where g is the gravity acceleration, mi, li are the mass and length of the ith link and qi, ˙qi and ¨qi
are position, velocity and acceleration for the ith joint. For our experiments we deﬁned the values
of m1 = 3.0, l1 = 2.67, m2 = 1.0, and li = 1.67, there is no particular reason for selecting those
values.
As stated in Section 3, we use a uniform distributed U(− π
2 ) input sampling from all joints q, ˙q
and ¨q. We calculate numerically their respective L values using (15) and we compute the torque
at each joint τi with (3), which represents our transformation function g(·). We used the automatic
differentiation package from the Pytorch Python library [23] to numerically calculate the required
derivatives of each model.

2 , π

We sampled 32000 sets of data points for training, subdivided into mini-batches of 32 samples. For
the test data-set we sampled 10000 sets of points. For the genetic programming tests we used the
GPTIPS 2.0 [24] toolbox (subject to the GNU public software license, GPL v3) in Matlab 2017b. The

7

neural networks, system identiﬁcation and the SyReNets implementation were developed in Python
3.6.13 using the Pytorch library version 1.8.1 (under BSD license), the SyReNets implementation will
be made publicly available in the near future.

For our evaluation hardware, the GPTIPS 2.0 toolbox used an Intel Core i9-9900KF CPU while
systems implemented using Pytorch were optimized to run on GPU and used an RTX 2080Ti.
Therefore, it is not fair to compare execution times with the genetic programming approach. For each
experiment a set of 10 random seeds were used, we will evaluate each method’s performance in three
groups: the 5 best performing ones, representing the best you can expect; all 10 seeds, presenting the
average performance; and the 5 worst performing ones.

The genetic programming representative is based the multi-gene genetic programming algorithm
(MGGP) as described in [25] with population size of 1000 candidates, ran for 300 generations, using
tournament of size 15 as selection and elite fraction of 10%. The operations provided to drive the
evolutionary process where the binary operations of +, −, × and the unary operations (cid:112)(·), (·)2,
(·)3, sin (·), and cos (·). The execution was terminated if the maximum number of iterations were
reached or if the MSE of the best performing solution was lower than 10−10.
The neural networks (NN) were implemented with ﬁve hidden layers with 300 neurons each with
softplus activation function and one output layer with one neuron with linear activation. The loss was
also the MSE. The network was trained via stochastic gradient descent using Adam optimizer [26]
with β1 = 0.9, β2 = 0.999, and initial learning rate of 10−3 which was divided by 10 for every 2000
iterations without any improvement in the loss up to 10−5. The step with the lowest training loss is
saved until a better performance is achieved or a time limit is reached, when the training is stopped.

The SyReNets were implemented with 3 symbolic layers, each with 12 selection heads. The auto-
encoder had 2 hidden layers with 128 neurons and softplus activation, both in the encoder and decoder.
The output of the encoder had 16 neurons with linear activation. The hidden state of the selection
head had 64 neurons, any other input to the selection head had the same number of inputs which
was the number of candidate solutions v. With exception to the auto-encoder, none of the neural
network layers used bias. The constant inputs for the scaling function were a vector of ones. the
loss terms λ1 = λ3 = 1 and λ2 = 0.001. The architecture was also trained with Adam with the
same hyper-parameters as stated previously but the learning rate decay was performed at every 1000
non-improving steps. We also store the best performing state of the system until a time limit is
reached and the training stops or a better performance is obtained.

The system identiﬁcation (SysId) setup had a symbolic form of the equation where it had to approxi-
mate the values for m1, l1, m2, and l2 using a neural network with a constant input vector of ones.
The network had one hidden layer with 64 neurons using softplus activation with no bias and one
output layer with 4 outputs using linear activation. The training used Adam with the same parameters
as before but with no decaying learning rate, which was kept constant at 10−3. During training,
similar to the previous ones, the parameters of the best performing state on the training data are stored
until a training time limit is reached

4.1 Learning from direct measurements

We ﬁrst experimented on the capability of the method to learn directly from L values, i.e., without
using any g(·). In Table 1, we can observe the performance of the method. All methods were trained
for 2000 seconds except for the MGGP, which trained until the last generation for all seeds. It is
clear that the best performing methods overall are SysId and MGGP that managed to achieve the
exact equation. However, SysId beneﬁts from that it has information regarding the structure of the
equation to learn, nevertheless, when we observe the worse performances we can see that some
times both SysId and MGGP are not converging to the correct values, denoting that there exist some
well-known sensitivity on those methods. Since SysId is fairly quick to converge (around 2 minutes
in our experiments) this problem can be easily circumvented by repeating the experiments. Also, it
might also be possible that the model used inside the SysId is inaccurately representing the system
to be learned, therefore, it might not be able to converge at all. Those problems were not observed
with NN and SyReNets, which presented consistent performance with all sets of seeds. SyReNets, in
special, presented up to 5000× better accuracy when compared to traditional neural networks which
denotes the potential of symbolic approaches as surrogate models for physical systems. Unfortunately,
the approach was not precise enough to estimate the exact equation as MGGP did, this is partially
due to the probabilistic nature of the architecture’s selection head which prevents that candidate
operations selection probabilities becomes zero and, therefore, stopping the gradient ﬂow through the
function.

8

Table 1: MSE of Learning the Lagrangian directly for 2000 seconds

Seeds
Best

5 Best

All 10

5 Worse

train
test
train
test
train
test
train
test

MGGP
0.000
0.000
11.293(±15.469)
11.827(±16.199)
16.152(±15.009)
16.964(±15.619)
21.010(±14.441)
22.101(±14.851)

NN
0.534
0.781
0.8264(±0.202)
0.6257(±0.118)
1.507(±1.037)
1.212(±1.032)
2.189(±1.104)
1.798(±1.235)

SyReNets
1e−4
1.4e−4
0.025(±0.031)
0.0316(±0.047)
0.207(±0.371)
0.137(±0.184)
0.434(±0.493)
0.270(±0.212)

SysId
9.775e−20
1.1e−18
1.730e−17(±1.887e−17)
5.976e−17(±6.33e−17)
362.152(±764.072)
363.408(±775.727)
724.304(±992.813)
726.817(±1011.82)

Table 2: MSE of Learning the Lagrangian indirectly (torque error) for 2000 seconds

Seeds
Best

5 Best

All 10

5 Worse

train
test
train
test
train
test
train
test

MGGP
5.264
5.211
92.484(±85.669)
92.347(±87.541)
428.741(±399.637)
444.862(±423.320)
764.998(±263.317)
797.376(±291.327)

NN
0.141
0.179
0.1556(±0.019)
0.162(±0.057)
0.207(±0.080)
0.192(±0.061)
0.259(±0.085)
0.221(±0.056)

SyReNets
1e−3
1e−3
0.004(±0.002)
0.004(±0.003)
0.011(±0.012)
0.012(±0.014)
0.020(±0.014)
0.131(±0.016)

SysId
0.000
3.831e−27
5.793e−32(±1.041e−31)
1.123e−27(±1.572e−27)
88.290(±186.189)
178.963(±383.053)
176.581(±241.890)
357.927(±500.073)

4.2 Learning from indirect measurements

Our second experiment evaluates the capability of the architectures to learn indirectly the Lagrangian
by observing the torque values as output of the transformation function g(·) which computes the
Euler-Lagrange equations. In Table 2 we can see that this time the MGGP algorithm could not
converge to the correct equation, denoting that exact function estimators might have a limitation. The
performance of neural networks and SyReNets once more proved to be stable for all tested seeds,
conﬁrming the results from the previous experiment and proving their indirect learning capability.

5 Conclusion

In this paper we explored ideas on how to incorporate symbolic reasoning into traditional machine
learning techniques, namely, we investigated the development of a hybrid architecture capable of
mixing the approximation capabilities of neural networks and the stability potential of symbolic
mathematical representations to learn dynamic models of physical systems. We also investigated the
potential of this approach in learning the Lagrangian of the system, which is an extremely compact
latent representations that typically can not be directly measured in real world applications. We
experimented with the approach in a simulated double pendulum system and evaluated the accuracy in
the task of learning the Lagrangian, ﬁrst from direct measurements of the Lagrangian as a baseline test
and later from indirect measurements, more speciﬁcally from torque measurements. The experiments
showed that, despite not converging to the exact equation, the performance was consistent in all
random seeds observed. We compared the performance with standard implementations of neural
networks, genetic programming and system identiﬁcations as a control measure. Experiments show
that system identiﬁcation and genetic programming are able to ﬁnd exact solutions but are also prone
to convergence problems that prevents the solution to be found in all the tested seeds. Neural networks
presented reasonable convergence but performed worse on average than SyReNets, showing that such
hybrid approaches has a lot of potential.

The downside of our approach is that we are, currently, dependent on all gradient information
of all candidate functions. Any attempt we tried to deal with zero gradients caused instability in
convergence and therefore were considered an architectural problem. Future efforts will be invested
in allowing the existence of zero gradients since it is a key point to get exact solutions. Another issue
experienced in training was with the number of steps to converge, while neural networks presented
consistent error reduction SyReNets had more variance with regard to the number of steps to converge.
We hypothesize it is related to the required symbolic depth required to express certain elements of the
equation, therefore, further investigation on compositionality has to be made in the near future.

Potentially, this research can help to automate safety-critical model changes in physical systems
such as robots in industrial and service environments, in particular in ﬂexible automation setups or
domestic environments. Also, the algorithms could be used as a fault detection and identiﬁcation
systems that compute symbolic explanation for better subsequent decision making. However, in
the long run, the speciﬁc use cases require the careful consideration of use, in particular in human
observation scenarios or in decision systems for military applications since explainable symbolic
representations of data streams give potentially more introspection into the observed data.

9

Acknowledgments and Disclosure of Funding

We thank Dr. Fan Wu for his helpful comments. We gratefully acknowledge the general support by
Microsoft Germany.

References
[1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional

neural networks. Advances in neural information processing systems, 25:1097–1105, 2012.

[2] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes,
N. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems, vol-
ume 27. Curran Associates, Inc., 2014. URL https://proceedings.neurips.cc/paper/2014/file/
5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf.

[3] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, pages 6000–6010, 2017. URL http:
//papers.nips.cc/paper/7181-attention-is-all-you-need.

[4] MC Good, LM Sweet, and KL Strobel. Dynamic models for control system design of integrated robot and

drive systems. 1985.

[5] Chae H An and John M Hollerbach. The role of dynamic models in cartesian force control of manipulators.

The International Journal of Robotics Research, 8(4):51–72, 1989.

[6] K.M. Lynch and F.C. Park. Modern Robotics: Mechanics, Planning, and Control. Cambridge University
Press, 2017. ISBN 9781316609842. URL https://books.google.de/books?id=8uS3AQAACAAJ.

[7] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal

approximators. Neural networks, 2(5):359–366, 1989.

[8] Shoujin Wang, Wei Liu, Jia Wu, Longbing Cao, Qinxue Meng, and Paul J Kennedy. Training deep neural
networks on imbalanced data sets. In 2016 international joint conference on neural networks (IJCNN),
pages 4368–4374. IEEE, 2016.

[9] Stuart Geman, Elie Bienenstock, and René Doursat. Neural networks and the bias/variance dilemma.

Neural computation, 4(1):1–58, 1992.

[10] Mateusz Buda, Atsuto Maki, and Maciej A Mazurowski. A systematic study of the class imbalance

problem in convolutional neural networks. Neural Networks, 106:249–259, 2018.

[11] John R Koza and John R Koza. Genetic programming: on the programming of computers by means of

natural selection, volume 1. MIT press, 1992.

[12] Richard Bellman. Dynamic programming. Science, 153(3731):34–37, 1966.

[13] Elmar Rueckert, Moritz Nakatenus, Samuele Tosatto, and Jan Peters. Learning inverse dynamics models in
o (n) time with lstm networks. In 2017 IEEE-RAS 17th International Conference on Humanoid Robotics
(Humanoids), pages 811–816. IEEE, 2017.

[14] Miles Cranmer, Sam Greydanus, Stephan Hoyer, Peter Battaglia, David Spergel, and Shirley Ho. La-
grangian neural networks. In ICLR 2020 Workshop on Integration of Deep Neural Models and Differential
Equations, 2020. URL https://openreview.net/forum?id=iE8tFa4Nq.

[15] Michael Lutter, Christian Ritter, and Jan Peters. Deep lagrangian networks: Using physics as model
prior for deep learning. In International Conference on Learning Representations, 2019. URL https:
//openreview.net/forum?id=BklHpjCqKm.

[16] Samuel Greydanus, Misko Dzamba, and Jason Yosinski. Hamiltonian neural networks. In H. Wallach,
H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural
Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.
neurips.cc/paper/2019/file/26cd8ecadce0d4efd6cc8a8725cbd1f8-Paper.pdf.

[17] Michael Schmidt and Hod Lipson. Distilling free-form natural laws from experimental data. Science,
324(5923):81–85, 2009. ISSN 0036-8075. doi: 10.1126/science.1165893. URL https://science.
sciencemag.org/content/324/5923/81.

[18] Silviu-Marian Udrescu, Andrew Tan, Jiahai Feng, Orisvaldo Neto, Tailin Wu, and Max Tegmark. Ai
feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity. In H. Larochelle, M. Ran-
zato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems,
volume 33, pages 4860–4871. Curran Associates, Inc., 2020. URL https://proceedings.neurips.
cc/paper/2020/file/33a854e247155d590883b93bca53848a-Paper.pdf.

10

[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.

[20] Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio. Contractive auto-encoders:

Explicit invariance during feature extraction. In Icml, 2011.

[21] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint

arXiv:1607.06450, 2016.

[22] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735–1780,

1997.

[23] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf,
Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit
Steiner, Lu Fang, Junjie Bai, and Soumith Chintala.
Pytorch: An imperative style, high-
performance deep learning library.
In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-
Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32,
pages 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/
9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf.

[24] Dominic P Searson. Gptips 2: an open-source software platform for symbolic data mining. In Handbook

of genetic programming applications, pages 551–573. Springer, 2015.

[25] Amir Hossein Gandomi and Amir Hossein Alavi. A new multi-gene genetic programming approach to
nonlinear system modeling. part i: materials and structural engineering problems. Neural Computing and
Applications, 21(1):171–187, 2012.

[26] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980, 2014.

11

