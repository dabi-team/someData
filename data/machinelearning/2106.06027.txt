Sparse and Imperceptible Adversarial Attack via a Homotopy Algorithm

Mingkang Zhu 1 Tianlong Chen 1 Zhangyang Wang 1

1
2
0
2

n
u
J

0
1

]

G
L
.
s
c
[

1
v
7
2
0
6
0
.
6
0
1
2
:
v
i
X
r
a

Abstract

Sparse adversarial attacks can fool deep neural
networks (DNNs) by only perturbing a few pixels
(regularized by (cid:96)0 norm). Recent efforts combine
it with another (cid:96)∞ imperceptible on the pertur-
bation magnitudes. The resultant sparse and im-
perceptible attacks are practically relevant, and
indicate an even higher vulnerability of DNNs
that we usually imagined. However, such attacks
are more challenging to generate due to the opti-
mization difﬁculty by coupling the (cid:96)0 regularizer
and box constraints with a non-convex objective.
In this paper, we address this challenge by propos-
ing a homotopy algorithm, to jointly tackle the
sparsity and the perturbation bound in one uni-
ﬁed framework. Each iteration, the main step
of our algorithm is to optimize an (cid:96)0-regularized
adversarial loss, by leveraging the nonmonotone
Accelerated Proximal Gradient Method (nmAPG)
for nonconvex programming; it is followed by an
(cid:96)0 change control step, and an optional post-attack
step designed to escape bad local minima. We also
extend the algorithm to handling the structural
sparsity regularizer. We extensively examine the
effectiveness of our proposed homotopy attack
for both targeted and non-targeted attack scenar-
ios, on CIFAR-10 and ImageNet datasets. Com-
pared to state-of-the-art methods, our homotopy
attack leads to signiﬁcantly fewer perturbations,
e.g., reducing 42.91% on CIFAR-10 and 75.03%
on ImageNet (average case, targeted attack), at
similar maximal perturbation magnitudes, when
still achieving 100% attack success rates. Our
codes are available at: https://github.com/
VITA-Group/SparseADV_Homotopy.

1The University of Texas at Austin, USA. Correspondence to:

Zhangyang Wang <atlaswang@utexas.edu>.

Proceedings of the 38 th International Conference on Machine
Learning, PMLR 139, 2021. Copyright 2021 by the author(s).

(a) otterhound

(b) stingray

(c) (cid:96)0 = 1803 (d) (cid:96)0 = 12264

(e) American coot

(f) stingray

(g) (cid:96)0 = 1400 (h) (cid:96)0 = 13467

(i) Boston bull

(j) stingray

(k) (cid:96)0 = 2986 (l) (cid:96)0 = 16974

Figure 1. Visualization of pixel-wise sparsity of targeted attack
when enforcing a (cid:96)∞ constraint of 0.05. The images in each row,
from the left to right are benign image, our adversarial example,
our perturbation position, GreedyFool (a recent state-of-the-art
approach in Dong et al. (2020))’s perturbation position. In the ﬁg-
ures of perturbation positions, black pixels denote no perturbation;
white pixels denote perturbing all three RGB channels; pure red,
green, blue pixels represent perturbing single channel; and pixels
with other color denote perturbing two channels.

1. Introduction

Deep neural networks (DNNs) have widely demonstrated
fragility to adversarial attacks, e.g., maliciously crafted
small perturbations to their inputs that can fool them to
make incorrect and implausible predictions (Carlini & Wag-
ner, 2017; Xiao et al., 2018; Athalye et al., 2018; Zhang
et al., 2020b; Zhou et al., 2020; Gao et al., 2020; Zhang
et al., 2020a; Du et al., 2021). Adversarial attacks raise
serious concern against DNNs’ applicability in high-stake,
risk-sensitive applications. Meanwhile, probing and lever-
aging adversarial attacks could help us troubleshoot the
weakness of a DNN, and further leading to strengthening it,
e.g., by adversarial training (Madry et al., 2018).

A non-trivial adversarial attack is meant to be small and “im-
perceptible” in certain sense (Nguyen et al., 2015): in this
way, it impacts little on the benign input’s semantic mean-
ing, hence uncovering the inherent instability of the DNN.

 
 
 
 
 
 
Sparse and Imperceptible Adversarial Attack via a Homotopy Algorithm

In the most common pixel-level additive attack setting, the
standard measure of attack magnitude is by the (cid:96)p-norm of
perturbations, with p = 0, 1, 2, or ∞. In particular, a number
of works (Modas et al., 2019; Fan et al., 2020; Xu et al.,
2019; Dong et al., 2020; Croce & Hein, 2019) demonstrated
that an attack can succeed by only perturbing a few pixels
under the (cid:96)0 constraint (sometimes (cid:96)1). Fig. 1 shows exam-
ples of successful pixel-wise sparse targeted attack by our
algorithm, compared with a recent state-of-the-art method
GreedyFool (Dong et al., 2020). Further, since the vanilla (cid:96)0-
attacks leave it unconstrained how they change each pixel,
the perturbed pixels may have very different intensity or
color than the surrounding ones and hence be easily visible
(Su et al., 2019). Therefore, more recent methods focus on
enforcing some element-wise magnitude constraints to also
ensure imperceptible perturbations. In total, such sparse and
imperceptible adversarial attacks become a practically rele-
vant topic of interest, indicating an even higher vulnerability
of DNNs that we usually imagined, and providing additional
insights about interpreting DNN failures (Xu et al., 2019).

1.1. Related Work

Many works on generating sparse and imperceptible adver-
sarial attacks have been proposed recently. The authors of
(Croce & Hein, 2019) proposed PGD0 to project the PGD
adversarial attack (Madry et al., 2018) to the (cid:96)0 ball, as well
as a heuristic CornerSearch strategy by traversing all the
pixels and to select a subset to perturb. SparseFool (Modas
et al., 2019) relaxes and approximates the (cid:96)0 objective us-
ing (cid:96)1 objective. SAPF (Fan et al., 2020) reformulates the
sparse adversarial attack problem into a mixed integer pro-
gramming (MIP) problem, to jointly optimize the binary
selection factors and continuous perturbation magnitudes of
all pixels, with a cardinality constraint to explicitly control
the degree of sparsity. It was solved by the (cid:96)p-Box ADMM
designed for integer programming. StrAttack (Xu et al.,
2019) focuses on group-wise sparsity and also solves the
problem using an ADMM based algorithm. The latest algo-
rithm, GreedyFool (Dong et al., 2020) presents a two-stage
greedy solution that ﬁrst picks perturbations with large gra-
dients and then reduces useless perturbations in the second
stage. Besides those white-box attacks, similar problems
were also studied in the black-box setting, e.g., One Pixel
(Su et al., 2019) and Pointwise Attack (Schott et al., 2019).

Despite progress, challenges remain for those existing solu-
tions. PGD0 and CornerSearch may fail to yield full success
attack rates (Croce & Hein, 2019); they moreover need
pre-speciﬁed numbers of perturbed pixels, which can also
cause inﬂexible and unnecessarily redundant perturbations.
SparseFool (Modas et al., 2019) did not directly control the
number of perturbed pixels, and was designed for the non-
targeted setting only. Among the latter optimization-based
solutions, StrAttack (Xu et al., 2019) and SAPF (Fan et al.,

2020) suffer from slow and sometimes unstable convergence
in practice, especially when the enforced sparsity is high; it
is also not uncommon to observe GreedyFool (Dong et al.,
2020) be stuck in local minima because of its greedy nature.

Looking for a more principled optimization solution, we
look back into the classical sparse optimization ﬁeld inten-
sively studied for decades. Among numerous methods pro-
posed, nonconvex regularization based methods, including
(cid:96)0 and (cid:96)p-norm (0 < p < 1)-based, have achieved consid-
erable performance improvement over the more traditional
convex relaxation (Wen et al., 2018). Specially, iterative
hard thresholding methods or proximal gradient methods
for (cid:96)0-regularized problems are shown to be scalable for
(group) sparse regression (Jain et al., 2016). A thorough
review of this ﬁeld can be found in (Bach et al., 2011).

1.2. Our Contributions

This work establishes an (cid:96)0-regularized and (cid:96)∞-constrained
optimization framework, for ﬁnding sparse and impercep-
tible adversarial attacks. Our technical foundations are
based on an innovative integration between the proximal
gradient method for nonconvex optimization described in
(Li & Lin, 2015), and the homotopy algorithm. In sparse
convex optimization (Soussen et al., 2015; Xiao & Zhang,
2013), the homotopy algorithm (a.k.a. continuation) keeps
a sequence of decreasing weights for regularizers, yielding
a multistage homotopy solution. In each stage with a ﬁxed
weight, the l0 or l1-regularized convex optimization problem
is optimized. That provides a sparse initial solution for the
next stage optimization with a smaller weight, progressively
leading to the ﬁnal sparse solution.

The core merit of our method is to enable a smooth relax-
ation of the sparsity constraint, which eventually leads to
a ﬂexible yet compactly sparse solution. We outline our
speciﬁc contributions below:

• We ﬁrst introduce the nonmonotone Accelerated Prox-
imal Gradient Method (nmAPG) to jointly tackle the
sparsity and the box constraint. The algorithm can also
be extended to handling group-wise sparsity. A main
hurdle in this regularization-based method is yet how to
achieve the desired sparsity: this is typically controlled
by some ad-hoc chosen regularization coefﬁcient.

• We then integrate the homotopy algorithm into our
framework, which allows us to optimize using a se-
quence of gradually decreasing weights for regularizers
(Soussen et al., 2015; Xiao & Zhang, 2013). It can con-
tinuously provide a sparse initial solution for the next
stage optimization, which leads to a compact sparse
solution by the end. To deal with the weight sensitivity
of the (cid:96)0-regularizer in the homotopy algorithm, we
design an (cid:96)0-change control step on the perturbation up-
date at each iteration, and another optional step called

Sparse and Imperceptible Adversarial Attack via a Homotopy Algorithm

post-attack perturbation to escape bad local minima.
Both lead to stable sparse solutions without extensive
weight searching for regularization-based methods.

• Extensive experiments on the CIFAR-10 (Krizhevsky,
2009) and ImageNet (Deng et al., 2009) endorse the
superiority of our new homotopy attack.
In both
targeted and non-targeted attack scenarios, it outper-
forms state-of-the-art methods by signiﬁcantly fewer
perturbations, e.g., reducing 42.91% on CIFAR-10 and
75.03% on ImageNet (average case, targeted attack), at
similar perturbation magnitudes, when still achieving
100% attack success rates. Visualizing the attacks also
gains us more interpretation of DNN fragility.

2. Method

2.1. Problem Formulation

Let X be the set of benign images. A trained DNN image
classiﬁcation model maps each image x ∈ Rn from X to its
class label y ∈ Y = {1, 2, · · · , K}. Given a benign image
x0 ∈ X , a targeted sparse and imperceptible adversarial
attack aims for ﬁnding a perturbation δ ∈ Rn, so that the
perturbed image x0 + δ is incorrectly classiﬁed to a tar-
get class t. In the meanwhile, the number of perturbations
should be as sparse as possible, and the perturbation mag-
nitude should remain small. This problem can be cast as

min
δ
s.t.

f (x0 + δ, t) + λ||δ||0
||δ||∞ ≤ (cid:15), 0 ≤ x0 + δ ≤ 1.

(1)

Here f () is the classiﬁcation loss such as the cross entropy
function, λ is the regularization coefﬁcient to control the (cid:96)0
sparsity of δ, and (cid:15) is the maximal allowable perturbation
magnitude. This above targeted attack formulation can be
easily adapted to the nontargeted attack by replacing the
adversarial loss f ():

The non-convexity and non-smoothness make solving (3) a
nontrivial task. The classical accelerated proximal gradient
(APG) methods (Nesterov, 2004) have been proven to be
efﬁcient for convex optimization problems, but it remains
unclear whether the usual APG can converge to a critical
point of nonconvex programming. In (Li & Lin, 2015),
the authors proposed two convergent accelerated proximal
gradient methods (APG) for nonconvex and nonsmooth
programs, of which (3) is a special case. In this paper, we
adopt their proposed nonmonotone APG with line search
(nmAPG) algorithm.

To be self-containing, we brieﬂy describe the high-level idea,
and refer the readers to (Li & Lin, 2015) for more details.
APG ﬁrst obtains an extrapolation solution using the current
and the previous solutions, and then solve proximal mapping
problems. When extending APG to nonconvex program-
ming, the main challenge is to prevent bad extrapolation
due to oscillatory landscapes (Beck & Teboulle, 2009). The
authors of (Li & Lin, 2015) introduced a monitor to prevent
and correct bad extrapolations so that they satisfy the sufﬁ-
cient descent property. In practice, nmAPG searches for a
step size using the backtracking line search initialized with
the BB step-length (Barzilai & Borwein, 1988) since the
Lipstchiz constant of gradients is not exactly known.

We next lay out how to solve (3) with nmAPG. Suppose
that f is a smooth nonconvex function, whose gradient has
Lipschitz constant L. Given δk, it holds that

f (x0 + δ, t) ≤ f (x0 + δk, t)+

∇δf (x0 + δk, t)T (δ − δk) + L

2 ||δ − δk||2,

where || · || is the || · ||2 norm for simplicity.

Then we consider the problem

min
δ

f (x0 + δk, t) + ∇δf (x0 + δk, t)T (δ − δk)+
L
2 ||δ − δk||2 + λ||δ||0 + I[l,u](δ),

min
δ
s.t.

f (x0 + δ, y0) + λ||δ||0
||δ||∞ ≤ (cid:15), 0 ≤ x0 + δ ≤ 1.

which is equivalent to

(2)

Since the algorithms for solving (1) and (2) will have most
in common, we will focus our discussion on (1) hereinafter,
while noting that all algorithmic steps can be readily utilized
towards solving (2) unless otherwise speciﬁed.

2.2. A Basic Solution using Accelerated Proximal

Gradient for Nonconvex Optimization

Since both constraints in (1) are box constraints, they can
be simpliﬁed to one box constraint as δ ∈ [l, u], where
l, u ∈ Rn, and 0 ∈ [l, u]. Let I[l,u] be the indicator function
such that I[l,u](δ) = 0 if δ ∈ [l, u], and I[l,u](δ) = +∞
otherwise. Problem (1) can be rewritten as

min
δ

L

2 ||δ − [δk − 1
λ||δ||0 + I[l,u](δ).

L ∇δf (x0 + δk, t)]||2+

(4)

Now letting g(δ) = λ||δ||0 + I[l,u](δ), we denote the solu-
tion to (4) as

L g(δk − 1

L ∇δf (x0 + δk, t)) = argmin
Prox 1
1
L ∇δf (x0 + δk, t)]||2 + λ||δ||0 + I[l,u](δ).

δ

L
2

||δ − [δk−

(5)

In fact, it can be obtained explicitly. Let

SL(δ) = δ −

1
L

∇δf (x0 + δ, t), ∀δ ∈ [l, u],

and

min
δ

f (x0 + δ, t) + λ||δ||0 + I[l,u](δ).

(3)

Π[l,u](δ) = argmin{||y − δ|| : y ∈ [l, u]}, ∀δ ∈ Rn.

Sparse and Imperceptible Adversarial Attack via a Homotopy Algorithm

It is easy to obtained that (Lu, 2013), the optimal solution
of problem (4) is (for i = 1, 2, · · · , n)

for i = 1, 2, · · · , m.

δk+1
i =






[Π[l,u](sL(δk))]i,

0,

i − [Π[l,u](

if [sL(δk)]2
sL(δk)) − sL(δk)]2
otherwise,

i > 2λ
L ;

Since g(δ) = λ||δ||0 + I[l,u](δ) is a proper and lower semi-
continuous function, the convergence of nmAPG to a critical
point of problem (3) can be assured, under some mild con-
ditions (Li & Lin, 2015).

2.3. Extension to Group-wise Sparsity

Group-wise sparsity has been recently incorporated into
adversarial attack by the recent work (Xu et al., 2019) to
provide more structured and explainable perturbations. To
extend our algorithm from the element-wise (cid:96)0 regularizer to
the group-wise sparsity, we need to reformulate the problem
and adapt the nmAPG solution.

Suppose the input image x0 can be partitioned into m groups
xT = (xT
). Then the problem concerning
G1
group-wise sparsity and imperceptibility can be cast as

, · · · , xT

Gm

min
δ
s.t.

f (x0 + δ, t) + λ||δ||2,0
||δ||∞ ≤ (cid:15), 0 ≤ x0 + δ ≤ 1,

(6)

where ||δ||2,0 is deﬁned as

||δ||2,0 = |{i : ||δGi|| (cid:54)= 0, i = 1, 2, · · · , m}|.

By similarly using the indicator function I[l,u](δ), problem
(6) can be further written equivalently as

min
δ

f (x0 + δ, t) + λ||δ||2,0 + I[l,u](δ).

(7)

Suppose that f is a smooth nonconvex function, whose
gradient is Lipschitzian with Lipschitz constant L. Similar
to (4), given δk, we consider the problem

1
L

∇δf (x0 + δk, t)]||2 + λ||δ||2,0

= min

||δ − sL(δk)||2 + λ||δ||2,0 + I[l,u](δ).

(8)

In (Beck & Hallak, 2019), the authors presented the solution
to a problem more general than (8). Although their deriva-
tion can be applied, we still exhibit the concise solution to
problem (8) and give a simple proof, for completeness.
Proposition 2.1. The minimum solution of problem (8) is

δk+1
Gi

=






[Π[l,u](sL(δk))]Gi,

0,

if ||[sL(δk)]Gi||2−
||[Π[l,u](sL(δk)) − sL(
δk)]Gi||2 > 2λ
L ;
otherwise,

(9)

min
δ

||δ − [δk −

L
2
+ I[l,u](δ)
L
2

δ

Proof. First, due to group-wise separability, Equation (8)
can be decomposed into m subproblems (i = 1, 2, · · · , m):

L
2

min
δGi

||[δ−sL(δk)]Gi||2+λ·sgn((cid:107)δGi(cid:107))+I[lGi ,uGi ](δGi),
(10)
If ||δGi|| = 0, which means δGi = 0, then the objective
value of problem (10) is L

2 ||[sL(δk)]Gi||2;

If ||δGi|| (cid:54)= 0, then sgn((cid:107)δGi(cid:107)) = 1, and problem (10)
becomes

λ +

(cid:88)

j∈Gi

min
δj

L
2

(δj − [sL(δk)]j)2 + I[lj ,uj ](δj).

(11)

it

is obvious that

the minimum solution is
In (11),
[Π[l,u](sL(δk))]j, ∀j ∈ Gi, which can be uniﬁed as
[Π[l,u](sL(δk))]Gi. And the minimum value of (11) is

λ +

(cid:88)

j∈Gi

L
2

([Π[l,u](sL(δk)) − sL(δk)]j)2,

which can be written simply as λ + L
2 ||[Π[l,u](sL(δk)) −
sL(δk)]Gi||2. Thus by comparing the two objectives, the
solution of problem (8) can be obtained as (9).

Now, let g(δ) = λ||δ||2,0 + I[l,u](δ). We denote the solution
of problem (8) as

Prox 1

L g(δk − 1
L
2

δ

L ∇δf (x0 + δk, t))
||δ − [δk −

1
L

= argmin

+ I[l,u](δ).

∇δf (x0 + δk, t)]||2 + λ||δ||2,0

Note that g(δ) = λ||δ||2,0 + I[l,u](δ) is also a proper and
lower semicontinuous function, the convergence of nmAPG
to a critical point can similarly be assured just like in the
element-wise sparsity case.

2.4. Putting Homotopy into Our Solution

Existing sparse adversarial attack methods that use regu-
larized optimization generally pre-select an ad-hoc ﬁxed
weight for the regularization term that controls sparsity (Fan
et al., 2020; Xu et al., 2019). This setting is over-idealistic,
and in practice requires multiple trial-and-errors on tuning
the weight in order to achieve certain desired sparsity level.
To alleviate this issue, we format our nmAPG based algo-
rithm in a homotopy manner (Xiao & Zhang, 2013; Lin &
Xiao, 2014), which allows us to optimize using a sequence
of decreasing weights for regularizers. It can keep a multi-
stage homotopy solution; in each stage, we optimize (1)
using nmAPG until it reaches the maximum inner iterations.

Sparse and Imperceptible Adversarial Attack via a Homotopy Algorithm

That can continuously provide a sparse initial solution from
the last stage for the next stage optimization, leading to the
ﬁnal sparse solution.

Despite homotopy being a highly effective and ﬂexible strat-
egy for convex optimization, it might not be directly applied
to highly nonconvex problems with high dimension such
as the adversarial attack of a DNN. For example, even if
an input perturbation into the nmAPG for problem (1) is
sparse, the converged solution by nmAPG might be a bad
local minimum and might not be sparse, since the optimized
function is highly nonconvex. Below, we describe several
strategies that we propose to stabilize and improve the ho-
motopy algorithm for our problem.

Initial Weight Search: The initial value of the weight λ
is important for subsequent performance. For homotopy,
an ideal initial weight in problem (1) should produce a
solution δ near 0 in the initial iteration of nmAPG. This can
be simply determined by increasing its value in a coarse-
grained manner until the nmAPG with only the ﬁrst iteration
gives δ = 0, since if λ is large enough, nmAPG will focus on
optimizing the sparsity function. Then the value is decreased
in a relatively ﬁne-grained manner until the ﬁrst iteration of
nmAPG starts to update on δ. The obtained value is ﬁnally
multiplied by a constant c and is assigned as the initial
weight λ0, to better enforce sparsity constraint. Algorithm 1
summarizes our strategy in details.

Algorithm 1 Our Subroutine for Initial Weight Search
(Lambda Search)
input x0: benign image; t: target;

eta, delta, rho: parameters of nmAPG;
c, v, β: parameters of this algorithm.

output λ.
1: δ0 = 0; λ = β;
2: repeat
δ1=nmAPG(δ0, x0, t, λ, eta, delta, rho, v, MaxIter=1);
3:
λ = λ + β;
4:
5: until δ1 = 0;
6: repeat
7:
8:
9: until δ1 (cid:54)= 0;
10: λ = λ ∗ c.

δ1=nmAPG(δ0, x0, t, λ, eta, delta, rho, v, MaxIter=1);
Decrease λ by a factor;

Additional Sparsity Control by (cid:96)0 Norm Changes: Even
with a good initial weight, the homotopy solution with (cid:96)0
regularizer remains to be sensitive to the regularization pa-
rameter λ along the homotopy path, which may not sufﬁce
for precise sparsity control for problem (1) when both the
loss function and the regularizer are nonconvex. Moreover,
due to the same reason, for the problem (1) with a ﬁxed
weight λ, even the input solution of nmAPG is sparse, the
converged solution might not be sparse. We hereby intro-
duce an additional control for sparsity.

Speciﬁcally, we constrain the maximum number of (cid:96)0
changes to be v in every outer iteration of homotopy. Sup-
pose r is the (cid:96)0-norm of the input perturbation δ to nmAPG
in the current outer iteration of homotopy. Then, at the
end of every inner iteration of nmAPG, we simply keep the
entries of δ with the top (r + v) absolute values and reduce
all other entries to 0. This will keep the number of nonzero
entries of the perturbation increased steadily, which avoids
the situation that a small decrease of the weight λ makes the
(cid:96)0-norm of the perturbation increase suddenly.

The value of v is an input constant of our homotopy algo-
rithm. However, it will be updated temporarily as a small
positive integer to be input to nmAPG, whenever the current
perturbation δk+1 satisﬁes that

||δk+1||1
||δk+1||0

≤ (cid:15) ∗ γ,

(12)

where γ < 1 is a user-speciﬁed constant. An intuition be-
hind this mechanism is that, if inequality (12) is satisﬁed,
then the average perturbation magnitude of δk+1 is much
smaller than the invisible threshold (cid:15), which means the num-
ber of ||δk+1||0 entries has not been fully used to attack the
classiﬁcation model at present. Hence v is set temporarily
to a small positive integer, and nmAPG will solve problem
(1) with a smaller v until inequality (12) is violated.

Optional Post-Attack Perturbation: The above scheme
can already generate sparse adversarial perturbations. How-
ever, we observe that our homotopy algorithm sometimes
gets stuck in bad local minima, which is inevitable due to the
nonconvex f . In this case, as λ decreases in the homotopy
algorithm, the (cid:96)1-norm of the perturbation increases dispro-
portionally to the corresponding (cid:96)0-norm, which means the
algorithm cannot effectively use the perturbed entries, and
inequality (12) is satisﬁed. So inequality (12) is regarded
as a trigger condition that our algorithm falls into a local
minimum after the nmAPG stage.

To alleviate this issue, we intend to provide some “push” to
help our algorithm escape from the local minimum faster.
In each outer iteration of the homotopy, we check whether
inequality (12) is satisﬁed. If it is the case, we propose to
execute the following post-attack perturbation step:

min
δ
s.t.

w1f (x0 + δ, t) + w2||δ||p
||δ||∞ ≤ (cid:15), 0 ≤ x0 + δ ≤ 1,

(13)

where w1 (cid:29) w2, p can be either 1, 2, or ∞, and δ is now
enforced only on nonzero entries of the output perturbation
of the nmAPG stage.

The assumption that w1 (cid:29) w2 makes an optimizer focus
on minimizing the function f , and the value of ||δ||p might
increase. Then the inequality (12) might be violated, which
means our algorithm might have escaped from the bad lo-
cal minimum. Since we only need to provide some push

Sparse and Imperceptible Adversarial Attack via a Homotopy Algorithm

directions rather than precise updates, we simply optimize
problem (13) using gradient descent for a number of iter-
ations proportional to the current (cid:96)0. After the post attack
stage, combined with the nmAPG stage with smaller v in the
next outer iteration of homotopy, the (cid:96)1-norm of the pertur-
bation would increase such that inequality (12) is violated,
and then we can set v back to normal.

adversarial examples are generated in total. As for the
ImageNet dataset, we select randomly 100 images from the
validation set and 9 classes as the targets for targeted attack,
so we generate 900 adversarial examples from the ImageNet
dataset. For nontargeted attack, we randomly select 5000
images from the test set of CIFAR-10, and 1000 images
from the validation set of ImageNet as the input images.

Our full homotopy attack algorithm are summarized as
Algorithm 2. Starting with the initial weight search, the
algorithm has an outer iteration by homotopy. Within each
outer iteration, the main step is to optimize an (cid:96)0-regularized
loss by leveraging nmAPG (which will have its inner itera-
tions), followed by an additional (cid:96)0 change control step, and
an optional post-attack to escape from bad local minima.

Algorithm 2 The Homotopy Attack Algorithm

input x0: benign image; t: target; (cid:15): invisible threshold;
eta, delta, rho, MaxIter: parameters of nmAPG;
c, v, β, γ: parameters of this algorithm.

δk+1 = nmAPG(δk, x0, t, λ, eta, delta, rho, v, MaxIter);
v = vini;
if not success then

output δ.
1: δ0 = 0; vini = v; k = 0;
2: λ =Lambda Search(x0, t, c, eta, delta, rho, v, β);
3: repeat
4:
5:
6:
7:
8:
9:
10:
11:
end if
12:
k = k + 1;
13:
14: until Attack Succeeds.

Set v as a small integer;
Conduct post-attack perturbation;

end if
Decrease λ by a factor;

if ||δk+1||1 ≤ ||δk+1||0 ∗ (cid:15) ∗ γ then

3. Experiments

In this section, we conduct comprehensive experiments with
diverse setups to validate the effectiveness of proposed ho-
motopy algorithm on the CIFAR-10 (Krizhevsky, 2009)
and the ImageNet (Deng et al., 2009) datasets. Section 3.1
collects our experimental setups. Then, we compare mul-
tiple previous state-of-the-art (SOTA) sparse adversarial
attack approaches in Section 3.2, 3.3, and 3.4, including
GreedyFool (Dong et al., 2020), SAPF (Fan et al., 2020),
and StrAttack (Xu et al., 2019). Moreover, ablation studies,
the application to group-wise sparsity, empirical time cost,
and visualization are provided in Section 3.5, 3.6, 3.7, and
3.8, respectively.

3.1. Experiment Setup

Dataset and Classiﬁcation Model Setting: Following the
practice of (Xu et al., 2019) and (Fan et al., 2020), we
randomly select 1000 images from the test set of CIFAR-10
for targeted attack. Each selected image would be attacked
with 9 target classes except its own class label, thus 9000

For the classiﬁcation model on CIFAR-10 dataset, we follow
the practice of (Fan et al., 2020), (Carlini & Wagner, 2017),
and (Xu et al., 2019) to train a network that consists of four
convolution layers, two max-pooling layers, and two fully-
connected layers. Our trained network can achieve 80%
classiﬁcation accuracy on the CIFAR-10 dataset. For the
classiﬁcation model on the ImageNet dataset, we choose to
use the pretrained Inception-v3 model (Szegedy et al., 2016),
which can achieve 77.45% top-1 classiﬁcation accuracy and
96% top-5 classiﬁcation accuracy on the ImageNet dataset.

Parameter Setting: For targeted attack, the adversarial
loss function f () is set as the cross entropy function. For
nontargeted attack, we adopt the loss function proposed in
(Carlini & Wagner, 2017) directly:

f (x0, y0) = max{D(x0)y0 − max
i(cid:54)=y0

{D(x0)i}, −κ},

where x0 is a benign image and y0 is its ground-truth class
label, D is the model to attack, and D(x0)y0 is the logit for
class y0. The conﬁdence parameter κ is set to 0.

Since we are highly interested in generating sparse and in-
visible adversarial perturbations while not extremely sparse
but visible ones, we maintain a relatively small (cid:96)∞-norm of
generated perturbations. That is, we set (cid:15) to 0.05, which is a
relatively small number in the [0, 1] range of a valid image.

While, except GreedyFool (Dong et al., 2020), the other two
compared methods (Fan et al., 2020; Xu et al., 2019) do not
have a control on the (cid:96)∞ of generated perturbations. They
either use a ﬁxed number as an upper bound on the (cid:96)0-norm,
or adopt a ﬁxed weight for sparsity regularizer. When en-
forcing a strict sparsity constraint, their methods may fail
or cause the (cid:96)∞ of generated perturbations to be extremely
large, which is perceptible by human eyes. Therefore, to
establish a fair comparison, we use our (cid:15) setting as a refer-
ence, and tune the parameters of their methods to ensure
that the average (cid:96)∞ of generated perturbations is close to (cid:15).
For GreedyFool (Dong et al., 2020), we use their original
implementation and set the upper bound on (cid:96)∞ to 0.05. The
similar level of (cid:96)∞ enables us to make a fair comparison on
the (cid:96)0 of generated perturbations.

Other parameters of our algorithm and the parameters for
group-wise sparsity are discussed in detail in the appendix.

Evaluation Metrics: We report the average (cid:96)p-norms of
generated perturbations, where p = 0, 1, 2, ∞, and the at-
tack success rates (ASR) of the experimented methods with

Sparse and Imperceptible Adversarial Attack via a Homotopy Algorithm

Table 1. Statistics of attack success rate and average (cid:96)p-norms (p = 0, 1, 2, ∞) of targeted attack on CIFAR-10 and ImageNet datasets.

Database

CIFAR-10

Method

ASR (cid:96)0
199
100
314
100
363
100
110
Homotopy-Attack 100

GreedyFool
SAPF
StrAttack

Best case
(cid:96)1
6.583
4.067
4.570
5.166

Average case
(cid:96)1

Worst case
(cid:96)1

(cid:96)2

(cid:96)2

0.468 0.048 100
0.258 0.051 100
0.248 0.050 100
0.467 0.047 100

(cid:96)∞ ASR (cid:96)0
289
518
546
165

(cid:96)2
(cid:96)∞
15.935 0.786 0.049
10.037 0.606 0.049 100
9.860
0.452 0.051
0.362 0.051 100
7.487
10.843 0.485 0.056
0.501 0.054 100
8.267
11.310 0.686 0.049
0.580 0.049 100
7.955
100 8937 283.705 3.287 0.049 100 10837 290.848 3.176 0.049 100 12197 330.049 3.552 0.049
100 37923 84.726 0.852 0.056 100 54743 117.488 0.980 0.054 100 60374 143.718 1.179 0.062
100 39593 95.122 1.039 0.061 100 55410 136.314 1.144 0.056 100 61328 165.434 1.203 0.067
99.178 2.030 0.049 100 2706 111.697 2.134 0.049 100 3065 126.933 2.288 0.050

(cid:96)∞ ASR (cid:96)0
445
594
657
234

ImageNet

GreedyFool
SAPF
StrAttack

Homotopy-Attack 100 2399

both the targeted and nontargeted settings. In the targeted
setting, we follow Fan et al. (2020) and Xu et al. (2019)
to separate the results into three cases: the best case, the
average case and the worst case. The best case presents
the results of target class that leads to the most sparsity, the
worst case shows the results of target class that leads to the
least sparsity, and the average case gives the average results
of all 9 target classes.

3.2. Targeted Attack

Table 1 presents the results of targeted attack on the CIFAR-
10 and ImageNet, where “Homotopy-Attack” denotes our
proposed algorithm. From this table, we can see that all
compared methods can achieve 100% attack success rate.
However, our algorithm outperforms the other compared
methods on sparsity level by a large margin. Speciﬁcally,
our algorithm can generally achieve 5.4% sparsity on 32 ×
32 × 3 CIFAR-10 images and 1% sparsity on 299 × 299 × 3
ImageNet images in average case, which is 42.91% fewer
on CIFAR-10 and 75.03% fewer on ImageNet, respectively,
when compared to SOTA.

From Table 1, a noticeable ﬁnding is that, the ratios of (cid:96)1
to (cid:96)0 of our algorithm are generally larger than the other
three methods on both the two datasets. We believe this is
not a bad phenomenon. To enforce higher level of sparsity,
there needs to be trade-offs between the (cid:96)0-norm and the (cid:96)1
and (cid:96)2-norms. Further, we can discover from Table 1 that,
the average ratios of (cid:96)1 to (cid:96)0 of our generated perturbations
are generally higher than 0.04, which is very close to the
(cid:96)∞-threshold (cid:15). This result indicates that our algorithm is
able to make the most trade-offs between the (cid:96)0-norm and
the (cid:96)1 and (cid:96)2-norms, which can lead to sparser results. This
may also indicate that relaxing the (cid:96)0 constraint to (cid:96)1 may
not be optimal in the case of sparse adversarial attack.

3.3. Nontargeted Attack

GreedyFool (Dong et al., 2020) outperforms the state-of-
the-art methods like SparseFool (Modas et al., 2019) and
PGD0 (Croce & Hein, 2019) by a signiﬁcant large margin on
nontargeted attack. Since SAPF (Fan et al., 2020) adopted

a similar optimization method and achieved better results
than StrAttack (Xu et al., 2019) in Table 1, we focus on
comparing compare our algorithm with GreedyFool and
SAPF on nontargeted attack.

Experimental results of nontargeted attack on the ImageNet
and CIFAR-10 datasets are listed in Table 2. From this
table, we can see that all methods achieve 100% attack
success rate. Looking at the (cid:96)p-norms, we can ﬁnd that our
algorithm on average only need to perturb 2.3% entries on
the 32 × 32 × 3 images from CIFAR-10 dataset, and 0.14%
entries on the 299 × 299 × 3 images from ImageNet dataset,
which is 37.93% fewer on CIFAR-10 and 65.69% fewer on
ImageNet, respectively, when compared to SOTA.

Further, we can ﬁnd from Table 2 that, the ratios of (cid:96)1 to (cid:96)0
of our algorithm are still larger than 0.04, and our algorithm
can generate much sparser result. Thus the ﬁndings men-
tioned in Subsection 3.2 also apply for nontargeted attack.

Database

Table 2. Statistics of attack success rate and average (cid:96)p-norms (p =
0, 1, 2, ∞) of nontargeted attack on CIFAR-10 and ImageNet.
(cid:96)∞
ASR
0.048
100
0.056
100
0.049
100
0.049
100
0.058
100
0.047
100

Method
GreedyFool
SAPF
Homotopy-Attack
GreedyFool
SAPF
Homotopy-Attack

(cid:96)1
4.395
6.070
3.427
35.764
24.907
16.387

(cid:96)2
0.392
0.365
0.367
0.964
0.637
0.685

(cid:96)0
116
352
72
1128
2063
387

CIFAR-10

ImageNet

3.4. Scale Up to Stronger Backbone Network

To further validate our method’s effectiveness on CIFAR-10
dataset, we train a ResNet-18 (He et al., 2016) which can
achieve 93% accuracy on CIFAR-10 dataset, and conduct
the previous experiments with GreedyFool (Dong et al.,
2020) and SAPF (Fan et al., 2020). The results are listed
in Tables 3. From the table, we can see that our algorithm
outperforms GreedyFool (SOTA) by a larger margin. Our
algorithm is able to reduce 61.46% sparsity, and 62.87%
sparsity respectively on nontargeted and targeted attacks
when compared to GreedyFool. It indicates that our algo-
rithm is more effective than greedy-based methods when
attacked networks become stronger.

Sparse and Imperceptible Adversarial Attack via a Homotopy Algorithm

Table 3. Statistics of targeted and nontargeted attacks on ResNet-
18 on CIFAR-10.

Attack Type

Nontargeted

Targeted

Method
GreedyFool
SAPF
Homotopy-Attack
GreedyFool
SAPF
Homotopy-Attack

ASR
100
100
100
100
100
100

(cid:96)0
192
723
74
668
1320
248

(cid:96)1
6.192
7.758
3.405
19.667
11.801
11.359

(cid:96)2
0.439
0.345
0.365
0.773
0.437
0.676

(cid:96)∞
0.048
0.055
0.047
0.048
0.071
0.049

3.5. Ablation Study

To further understand the contribution of each component in
our algorithm, ablation studies are conducted on the CIFAR-
10 and ImageNet datasets. Without loss of generality, we
only experiment on targeted attack, since it is more difﬁcult
than nontargeted attack. The results are listed in Table 4, in
which we term the homotopy algorithm without additional
sparsity control and optional post attack stage as “Pure-
Homotopy”, and the nmAPG algorithm with binary weight
search as “nmAPG”.

From Table 4, we can see that both nmAPG with binary
weight search and homotopy without additional sparsity
control or post attack stage perform much worse than our
homotopy attack algorithm. The sparsity levels are im-
proved signiﬁcantly after adding the additional control and
post attack stage to Pure-Homotopy on both datasets. We
believe there are 2 reasons behind this: (1) The (cid:96)0-norm reg-
ularizer is nonsmooth and nonconvex, and it is sensitive to
the change of weight λ in either the homotopy manner or the
binary search manner. So we cannot be certain about if the
decrease of λ in each iteration of the homotopy algorithm
will cause a signiﬁcant change of the (cid:96)0-norm, which we do
not want it to happen. So further control of the maximum
increase of (cid:96)0 is needed here. (2) Since the loss function f
is nonconvex, the optimization can easily be trapped in local
minima. If that happens, we need to either push it out or
give it some directions. Without the “push”, the algorithm
would require a huge increase of the (cid:96)0-norm to escape. So
the post attack stage is needed.

Table 4. Statistics of attack success rate and average (cid:96)p-norms
(p = 0, 1, 2, ∞) of component analysis experiment.

Database

CIFAR-10

ImageNet

Method
Pure-Homotopy
nmAPG
Homotopy-Attack
Pure-Homotopy
nmAPG
Homotopy-Attack

ASR
100
100
100
100
100
100

(cid:96)0
223
283
168
16343
13582
2889

(cid:96)1
10.087
12.687
7.409
340.691
303.195
117.97

(cid:96)2
0.644
0.714
0.555
3.094
2.998
2.209

(cid:96)∞
0.050
0.050
0.049
0.050
0.050
0.049

3.6. Group-wise Sparsity

In this subsection, we show the group-wise sparsity exper-
imental results of our algorithm on targeted attack. The
results are given in Table 5. Since we only constrain group-
wise sparsity, the average (cid:96)0-norm is larger than the corre-

sponding one in Table 1, where we conduct pixel-wise spar-
sity. However, we can see from the (cid:96)2,0 that, the group-wise
sparsity level is very high. Speciﬁcally, we can on average
achieve 6.88% and 2.05% group sparsity on CIFAR-10 and
ImageNet, respectively.

Table 5. Statistics of our algorithm on group-wise sparse targeted
attack.

Database
CIFAR-10
ImageNet

ASR
100
100

(cid:96)0
502
7711

(cid:96)2,0
5.501
8.667

(cid:96)1
19.407
292.862

(cid:96)2
0.881
3.461

(cid:96)∞
0.047
0.048

no. of groups
80
423

3.7. Empirical Time Cost

In this subsection, we compare the running time of our al-
gorithm with state-of-the-art methods. The results are given
in Table 6. From the table we can see that our algorithm is
much faster than SAPF (Fan et al., 2020) and StrAttack (Xu
et al., 2019), since our algorithm does not require multiple
reruns of the whole algorithm with binary searched weights
to ensure attack success and result quality.

GreedyFool (Dong et al., 2020), on the other hand, runs
faster than our algorithm because of its greedy nature. How-
ever, previous experiments in Tables 1, 2, and 3 show that
our algorithm always generate much sparser perturbations
with lower (cid:96)1 and (cid:96)2 norms than Greedyfool under the same
(cid:96)∞ constraints.

Table 6. Empirical time cost of targeted attack in seconds.

Database
CIFAR-10
ImageNet

GreedyFool Homotopy Attack

0.43
71.49

22.64
516.57

SAPF
100.15
1589.54

StrAttack
142.65
2242.78

3.8. Visualization

The visualizations for pixel-wise sparsity and group-wise
sparsity are shown in Figure 1, and 2 respectively. In Figure
2, we use class activation map (CAM) (Zhou et al., 2016) to
show whether the perturbations generated by our algorithm
have a good correspondence with localized class-speciﬁc
discriminative image regions. In the ﬁgures of perturbation
positions, black pixels denote no perturbation; white pixels
denote perturbing all three RGB channels; pure red, green,
blue pixels represent perturbing single channel; and pixels
with other color denote perturbing two channels.

From rows 2-6 of Figure 2, it is obvious that the generated
perturbations cover the most discriminative areas of the ob-
jects, thanks to our algorithm achieving high group-sparsity.
Row 1, on the other hand, might provide an interesting in-
sight of the mechanism of DNN. It might indicate a high
correlation between two different objects (trainer and killer
whale) in the training data of DNN. The corresponding
CAM also validates this ﬁnding.

Sparse and Imperceptible Adversarial Attack via a Homotopy Algorithm

The reason of perturbations generated with pixel-wise spar-
sity constraint not always being on discriminative areas like
row 3 of Figure 1 is that, our algorithm is able to ﬁnd some
pixels not on the object that offer more adversary when
enforcing pixel-wise constraint. However, when enforcing
group-wise constraint, our algorithm will perturb groups
with highest sum adversaries, which generally cover the
discriminative areas.

Figure 2. Visualization of targeted attack with group-wise sparsity.
Each row from the left to right represents benign image, adver-
sarial example, perturbation positions, and CAM of the original
label. The ground-truth label for each row are killer whale, Sussex
spaniel, bee eater, bee eater, titi monkey and oxcart, respectively.
The target class is face powder for row 1 and 6, cock for row 2 and
3, and wild boar for row 4 and 5.

4. Conclusion

In this paper, we have proposed a novel homotopy algo-
rithm for sparse adversarial attack based on Nonmonotone
Accelerated Proximal Gradient Methods for Nonconvex Pro-
gramming, an additional control of maximum (cid:96)0 updates
and an optional post attack stage per iteration. Extensive ex-
periments show that our algorithm can generate very sparse
adversarial perturbations while maintaining relatively low
perturbation magnitudes, compared to the state-of-the-art

methods. Also, our proposed control of maximum (cid:96)0 up-
dates and the optional post attack stage greatly improve the
sparsity level of the homotopy algorithm.

References

Athalye, A., Engstrom, L., Ilyas, A., and Kwok, K. Syn-
thesizing robust adversarial examples. In Proceedings of
the 35th International Conference on Machine Learning
(ICML), volume 80, pp. 284–293, 2018.

Bach, F., Jenatton, R., Mairal, J., and Obozinski, G. Op-
timization with sparsity-inducing penalties. Machine
Learning, 4(1):1–106, 2011.

Barzilai, J. and Borwein, J. M. Two-point step size gradient
methods. IMA Journal of Numerical Analysis, 8(1):141–
148, 1988.

Beck, A. and Hallak, N. Optimization problems involving
group sparsity terms. Mathematical Programming, 178:
39–67, 2019.

Beck, A. and Teboulle, M. Fast gradient-based algorithms
for constrained total variation image denoising and deblur-
ring problems. IEEE Transactions on Image Processing,
18(11):2419–2434, 2009.

Carlini, N. and Wagner, D. Towards evaluating the robust-
ness of neural networks. In 2017 IEEE Symposium on
Security and Privacy (SP), pp. 39–57, 2017.

Croce, F. and Hein, M. Sparse and imperceivable adversarial
attacks. In Proceedings of the IEEE/CVF International
Conference on Computer Vision (ICCV), pp. 4724–4732,
2019.

Deng, J., Dong, W., Socher, R., Li, L., Kai Li, and Li Fei-Fei.
Imagenet: A large-scale hierarchical image database. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 248–255, 2009.

Dong, X., Chen, D., Bao, J., Qin, C., Yuan, L., Zhang,
W., Yu, N., and Chen, D. Greedyfool: Distortion-aware
sparse adversarial attack. In Proceedings of the Advances
in Neural Information Processing Systems (NeurIPS), pp.
4724–4732, 2020.

Du, X., Zhang, J., Han, B., Liu, T., Rong, Y., Niu, G.,
Huang, J., and Sugiyama, M. Learning diverse-structured
arXiv preprint
networks for adversarial robustness.
arXiv:2102.01886, 2021.

Fan, Y., Wu, B., Li, T., Zhang, Y., Li, M., Li, Z., and Yang,
Y. Sparse adversarial attack via perturbation factorization.
In Proceedings of the European Conference on Computer
Vision (ECCV), pp. 35–50, 2020.

Sparse and Imperceptible Adversarial Attack via a Homotopy Algorithm

Gao, R., Liu, F., Zhang, J., Han, B., Liu, T., Niu, G., and
Sugiyama, M. Maximum mean discrepancy is aware of
adversarial attacks. arXiv preprint arXiv:2010.11415,
2020.

Soussen, C., Idier, J., Duan, J., and Brie, D. Homotopy
based algorithms for (cid:96)0-regularized least-squares. IEEE
Transactions on Signal Processing, 63(13):3301–3316,
2015.

He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 770–778, 2016.

Jain, P., Rao, N., and Dhillon, I. S. Structured sparse re-
gression via greedy hard thresholding. In Proceedings of
the Advances in Neural Information Processing Systems
(NeurIPS), volume 29, pp. 1516–1524, 2016.

Krizhevsky, A. Learning multiple layers of features from
tiny images. Technical report, Department of Computer
Science, University of Toronto, 2009.

Li, H. and Lin, Z. Accelerated proximal gradient meth-
In Proceedings of
ods for nonconvex programming.
the Advances in Neural Information Processing Systems
(NeurIPS), volume 28, pp. 379–387, 2015.

Lin, Q. and Xiao, L. An adaptive accelerated proximal gra-
dient method and its homotopy continuation for sparse
optimization. In Proceedings of the International Confer-
ence on Machine Learning (ICML), pp. 73–81, 2014.

Lu, Z. Iterative hard thresholding methods for l0 regularized
convex cone programming. Mathematical Programming,
147(1-2):125–154, 2013.

Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and
Vladu, A. Towards deep learning models resistant to
adversarial attacks. In Proceedings of the International
Conference on Learning Representations (ICLR), 2018.

Modas, A., Moosavi-Dezfooli, S.-M., and Frossard, P.
Sparsefool: A few pixels make a big difference. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition (CVPR), pp. 9087–9096,
2019.

Nesterov, Y. E. Introductory Lectures on Convex Optimiza-
tion - A Basic Course, volume 87 of Applied Optimization.
Springer, 2004.

Nguyen, A., Yosinski, J., and Clune, J. Deep neural net-
works are easily fooled: High conﬁdence predictions
for unrecognizable images. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 427–436, 2015.

Schott, L., Rauber, J., Bethge, M., and Brendel, W. Towards
the ﬁrst adversarially robust neural network model on
mnist. In Proceedings of the International Conference on
Learning Representations (ICLR), 2019.

Su, J., Vargas, D. V., and Sakurai, K. One pixel attack
for fooling deep neural networks. IEEE Transactions on
Evolutionary Computation, 23(5):828–841, 2019.

Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna,
Z. Rethinking the inception architecture for computer
vision. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), pp.
2818–2826, 2016.

Wen, F., Chu, L., Liu, P., and Qiu, R. C. A survey on noncon-
vex regularization-based sparse and low-rank recovery in
signal processing, statistics, and machine learning. IEEE
Access, 6:69883–69906, 2018.

Xiao, C., Li, B., Zhu, J., He, W., Liu, M., and Song, D. Gen-
erating adversarial examples with adversarial networks.
In Proceedings of the Twenty-Seventh International Joint
Conference on Artiﬁcial Intelligence (IJCAI), pp. 3905–
3911, 2018.

Xiao, L. and Zhang, T. A proximal-gradient homotopy
method for the sparse least-squares problem. SIAM Jour-
nal on Optimization, 23(2):1062–1091, 2013.

Xu, K., Liu, S., Zhao, P., Chen, P.-Y., Zhang, H., Fan,
Q., Erdogmus, D., Wang, Y., and Lin, X. Structured
adversarial attack: Towards general implementation and
better interpretability. In Proceedings of the International
Conference on Learning Representations (ICLR), 2019.

Zhang, J., Xu, X., Han, B., Niu, G., Cui, L., Sugiyama, M.,
and Kankanhalli, M. Attacks which do not kill training
make adversarial learning stronger. In International Con-
ference on Machine Learning, pp. 11278–11287. PMLR,
2020a.

Zhang, J., Zhu, J., Niu, G., Han, B., Sugiyama, M., and
Kankanhalli, M. Geometry-aware instance-reweighted
adversarial training. arXiv preprint arXiv:2010.01736,
2020b.

Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., and Torralba,
A. Learning deep features for discriminative localization.
In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pp. 2921–2929,
2016.

Zhou, H., Chen, D., Liao, J., Chen, K., Dong, X., Liu,
K., Zhang, W., Hua, G., and Yu, N. Lg-gan: Label
guided adversarial network for ﬂexible targeted attack of
point cloud based deep networks. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 10356–10365, 2020.

