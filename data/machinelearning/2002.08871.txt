Fast Differentiable Sorting and Ranking

Mathieu Blondel 1 Olivier Teboul 1 Quentin Berthet 1 Josip Djolonga 1

Abstract

architectures for which a gradient can be computed.

0
2
0
2

n
u
J

9
2

]
L
M

.
t
a
t
s
[

2
v
1
7
8
8
0
.
2
0
0
2
:
v
i
X
r
a

The sorting operation is one of the most com-
monly used building blocks in computer program-
ming. In machine learning, it is often used for
robust statistics. However, seen as a function,
it is piecewise linear and as a result includes
many kinks where it is non-differentiable. More
problematic is the related ranking operator, often
used for order statistics and ranking metrics. It
is a piecewise constant function, meaning that
its derivatives are null or undeﬁned. While nu-
merous works have proposed differentiable prox-
ies to sorting and ranking, they do not achieve
the O(n log n) time complexity one would expect
from sorting and ranking operations. In this pa-
per, we propose the ﬁrst differentiable sorting and
ranking operators with O(n log n) time and O(n)
space complexity. Our proposal in addition enjoys
exact computation and differentiation. We achieve
this feat by constructing differentiable operators
as projections onto the permutahedron, the con-
vex hull of permutations, and using a reduction
to isotonic optimization. Empirically, we conﬁrm
that our approach is an order of magnitude faster
than existing approaches and showcase two novel
applications: differentiable Spearman’s rank cor-
relation coefﬁcient and least trimmed squares.

1. Introduction

Modern deep learning architectures are built by composing
parameterized functional blocks (including loops and condi-
tionals) and are trained end-to-end using gradient backprop-
agation. This has motivated the term differentiable pro-
gramming, recently popularized, among others, by LeCun
(2018). Despite great empirical successes, many operations
commonly used in computer programming remain poorly
differentiable or downright pathological, limiting the set of

1Google Research, Brain team.

Correspondence to:
Mathieu
Olivier
Teboul <oliviert@google.com>, Quentin Berthet <qber-
thet@google.com>, Josip Djolonga <josipd@google.com>.

<mblondel@google.com>,

Blondel

Proceedings of the 37 th International Conference on Machine
Learning, PMLR 119, 2020. Copyright 2020 by the author(s).

sort-
We focus in this paper on two such operations:
ing and ranking. Sorting returns the given input vector
with its values re-arranged in monotonic order. It plays a
key role to handle outliers in robust statistics, as in least-
quantile (Rousseeuw, 1984) or trimmed (Rousseeuw &
Leroy, 2005) regression. As a piecewise linear function,
however, the sorted vector contains many kinks where it is
non-differentiable. In addition, when used in composition
with other functions, sorting often induces non-convexity,
thus rendering model parameter optimization difﬁcult.

The ranking operation, on the other hand, outputs the po-
sitions, or ranks, of the input values in the sorted vector.
A workhorse of order statistics (David & Nagaraja, 2004),
ranks are used in several metrics, including Spearman’s
rank correlation coefﬁcient (Spearman, 1904), top-k accu-
racy and normalized discounted cumulative gain (NDCG).
As piecewise constant functions, ranks are unfortunately
much more problematic than sorting: their derivatives are
null or undeﬁned, preventing gradient backpropagation. For
this reason, a large body of work has studied differentiable
proxies to ranking. While several works opt to approximate
ranking metrics directly (Chapelle & Wu, 2010; Adams &
Zemel, 2011; Lapin et al., 2016; Rol´ınek et al., 2020), oth-
ers introduce “soft” ranks, which can then be plugged into
any differentiable loss function. Taylor et al. (2008) use a
random perturbation technique to compute expected ranks
in O(n3) time, where n is the dimensionality of the vector
to rank. Qin et al. (2010) propose a simple method based
on comparing pairwise distances between values, thereby
taking O(n2) time. This method is reﬁned by Grover et al.
(2019) using unimodal row-stochastic matrices. Lastly, Cu-
turi et al. (2019) adopt an optimal transport viewpoint of sort-
ing and ranking. Their method is based on differentiating
through the iterates of the Sinkhorn algorithm (Sinkhorn &
Knopp, 1967) and costs O(T mn) time, where T is the num-
ber of Sinkhorn iterations and m ∈ N is a hyper-parameter
which trades computational cost and precision (convergence
to “hard” sort and ranks is only guaranteed if m = n).

In this paper, we propose the ﬁrst differentiable sorting and
ranking operators with O(n log n) time and O(n) memory
complexity. Our proposals enjoy exact computation and dif-
ferentiation (i.e., they do not involve differentiating through

 
 
 
 
 
 
Fast Differentiable Sorting and Ranking

the iterates of an approximate algorithm). We achieve this
feat by casting differentiable sorting and ranking as projec-
tions onto the permutahedron, the convex hull of all per-
mutations, and using a reduction to isotonic optimization.
While the permutahedron had been used for learning before
(Yasutake et al., 2011; Ailon et al., 2016; Blondel, 2019),
it had not been used to deﬁne fast differentiable operators.
The rest of the paper is organized as follows.

• We review the necessary background (§2) and show how
to cast sorting and ranking as linear programs over the
permutahedron, the convex hull of all permutations (§3).

• We introduce regularization in these linear programs,
which turns them into projections onto the permutahe-
dron and allows us to deﬁne differentiable sorting and
ranking operators. We analyze the properties of these
operators, such as their asymptotic behavior (§4).

• Using a reduction to isotonic optimization, we achieve
O(n log n) computation and O(n) differentiation of our
operators, a key technical contribution of this paper (§5).

• We show that our approach is an order of magnitude
faster than existing approaches and showcase two novel
applications: differentiable Spearman’s rank coefﬁcient
and soft least trimmed squares (§6).

2. Preliminaries

In this section, we deﬁne the notation that will be used
throughout this paper. Let θ := (θ1, . . . , θn) ∈ Rn. We
will think of θ as a vector of scores or “logits” produced by
a model, i.e., θ := g(x) for some g : X → Rn and some
x ∈ X . For instance, in a label ranking setting, θ may
contain the score of each of n labels for the features x.

We denote a permutation of [n] by σ = (σ1, . . . , σn) and
its inverse by σ−1. For convenience, we will sometimes use
π := σ−1. If a permutation σ is seen as a vector, we denote
it with bold, σ ∈ [n]n. We denote the set of n! permutations
of [n] by Σ. Given a permutation σ ∈ Σ, we denote the
version of θ = (θ1, . . . , θn) ∈ Rn permuted according to
σ by θσ := (θσ1, . . . , θσn) ∈ Rn. We deﬁne the reversing
permutation by ρ := (n, n − 1, . . . , 1) or ρ in vector form.
Given a set S ⊆ [n] and a vector v ∈ Rn, we denote the
restriction of v to S by vS := (vi : i ∈ S) ∈ R|S|.

We deﬁne the argsort of θ as the indices sorting θ, i.e.,

σ(θ) := (σ1(θ), . . . , σn(θ)),

where θσ1(θ) ≥ · · · ≥ θσn(θ). If some of the coordinates of
θ are equal, we break ties arbitrarily. We deﬁne the sort of
θ as the values of θ in descending order, i.e.,

s(θ) := θσ(θ).

We deﬁne the rank of θ as the function evaluating at coor-
dinate j to the position of θj in the descending sort (smaller

rank rj(θ) means that θj has higher value). It is formally
equal to the argsort’s inverse permutation, i.e.,

r(θ) := σ−1(θ).

For instance, if θ3 ≥ θ1 ≥ θ2, then σ(θ) = (3, 1, 2),
s(θ) = (θ3, θ1, θ2) and r(θ) = (2, 3, 1). All three op-
erations can be computed in O(n log n) time. Note that
throughout this paper, we use descending order for conve-
nience. The ascending order counterparts are easily obtained
by σ(−θ), −s(−θ) and r(−θ), respectively.

3. Sorting and ranking as linear programs

We show in this section how to cast sorting and ranking op-
erations as linear programs over the permutahedron. To that
end, we ﬁrst formulate the argsort and ranking operations
as optimization problems over the set of permutations Σ.

Lemma 1. Discrete optimization formulations
For all θ ∈ Rn and ρ := (n, n − 1, . . . , 1), we have

σ(θ) = argmax

(cid:104)θσ, ρ(cid:105), and

σ∈Σ

r(θ) = argmax

(cid:104)θ, ρπ(cid:105).

π∈Σ

(1)

(2)

A proof is provided in §B.1. To obtain continuous optimiza-
tion problems, we introduce the permutahedron induced by
a vector w ∈ Rn, the convex hull of permutations of w:
P(w) := conv({wσ : σ ∈ Σ}) ⊂ Rn.

A well-known object in combinatorics (Bowman, 1972;
Ziegler, 2012), the permutahedron of w is a convex poly-
tope, whose vertices correspond to permutations of w. It
In particular, when w = ρ,
is illustrated in Figure 1.
P(w) = conv(Σ). With this deﬁned, we can now derive
linear programming formulations of sort and ranks.

Proposition 1. Linear programming formulations
For all θ ∈ Rn and ρ := (n, n − 1, . . . , 1), we have

s(θ) = argmax
y∈P(θ)

(cid:104)y, ρ(cid:105), and

r(θ) = argmax
y∈P(ρ)

(cid:104)y, −θ(cid:105).

(3)

(4)

A proof is provided in §B.2. The key idea is to perform a
change of variable to “absorb” the permutation in (1) and
(2) into a permutahedron. From the fundamental theorem
of linear programming (Dantzig et al., 1955, Theorem 6),
an optimal solution of a linear program is almost surely
achieved at a vertex of the convex polytope, a permutation
in the case of the permutahedron. Interestingly, θ appears
in the constraints and ρ appears in the objective for sorting,
while this is the opposite for ranking.

Fast Differentiable Sorting and Ranking

Figure 1. Illustration of the permutahedron P(ρ), whose ver-
tices are permutations of ρ = (3, 2, 1). In this example, the ranks
of θ = (2.9, 0.1, 1.2) are r(θ) = (1, 3, 2). In this case, our pro-
posed soft rank rεQ(θ) with ε = 1 is exactly equal to r(θ). When
ε → ∞, rεQ(θ) converges towards the centroid of the permuta-
hedron. The gray line indicates the regularization path of rεQ(θ)
between these two regimes, when varying ε.

Differentiability a.e. of sorting. For s(θ), the fact that θ
appears in the linear program constraints makes s(θ) piece-
wise linear and thus differentiable almost everywhere. When
σ(θ) is unique at θ, s(θ) = θσ(θ) is differentiable at θ and
its Jacobian is the permutation matrix associated with σ(θ).
When σ(θ) is not unique, we can choose any matrix in
Clarkes generalized Jacobian, i.e., any convex combination
of the permutation matrices associated with σ(θ).

Lack of useful Jacobian of ranking. On the other hand,
for r(θ), since θ appears in the objective, a small pertur-
bation to θ may cause the solution of the linear program
to jump to another permutation of ρ. This makes r(θ) a
discontinuous, piecewise constant function. This means that
r(θ) has null or undeﬁned partial derivatives, preventing its
use within a neural network trained with backpropagation.

4. Differentiable sorting and ranking

As we have already motivated, our primary goal is the de-
sign of efﬁciently computable approximations to the sorting
and ranking operators, that would smoothen the numerous
kinks of the former, and provide useful derivatives for the
latter. We achieve this by introducing strongly convex reg-
ularization in our linear programming formulations. This
turns them into efﬁciently computable projection operators,
which are differentiable and amenable to formal analysis.

Projection onto the permutahedron. Let z, w ∈ Rn
and consider the linear program argmaxµ∈P(w)(cid:104)µ, z(cid:105).
Clearly, we can express s(θ) by setting (z, w) = (ρ, θ) and
r(θ) by setting (z, w) = (−θ, ρ). Introducing quadratic
regularization Q(µ) := 1
2 (cid:107)µ(cid:107)2 is considered by Martins &
Astudillo (2016) over the unit simplex and by Niculae et al.

Figure 2. Illustration of the soft sorting and ranking operators,
sεΨ(θ) and rεΨ(θ) for Ψ = Q; the results with Ψ = E are
similar. When ε → 0, they converge to their “hard” counterpart.
When ε → ∞, they collapse into a constant, as proven in Prop.2.

(2018) over marginal polytopes. Similarly, adding Q to our
linear program over the permutahedron gives

PQ(z, w) := argmax
µ∈P(w)

(cid:104)z, µ(cid:105)−Q(µ) = argmin
µ∈P(w)

1
2

(cid:107)µ−z(cid:107)2,

i.e., the Euclidean projection of z onto P(w). We also
consider entropic regularization E(µ) := (cid:104)µ, log µ − 1(cid:105),
popularized in the optimal transport literature (Cuturi, 2013;
Peyr´e & Cuturi, 2017). Subtly, we deﬁne

PE(z, w) := log argmax
µ∈P(ew)

(cid:104)z, µ(cid:105) − E(µ)

= log argmin
µ∈P(ew)

KL(µ, ez),

− (cid:80)

i ai + (cid:80)

where KL(a, b) := (cid:80)
i ai log ai
i bi is the
bi
Kullback-Leibler (KL) divergence between two positive
measures a ∈ Rn
+. PE(z, w) is therefore the
log KL projection of ez onto P(ew). The purpose of ew is
to ensure that µ always belongs to dom(E) = Rn
+ (since µ
is a convex combination of the permutations of ew) and that
of the logarithm is to map µ(cid:63) back to Rn.

+ and b ∈ Rn

More generally, we can use any strongly convex regulariza-
tion Ψ under mild conditions. For concreteness, we focus
our exposition in the main text on Ψ ∈ {Q, E}. We state all
our propositions for these two cases and postpone a more
general treatment to the appendix.

Soft operators. We now build upon these projections to
deﬁne soft sorting and ranking operators. To control the
regularization strength, we introduce a parameter ε > 0
which we multiply Ψ by (equivalently, divide z by).

For sorting, we choose (z, w) = (ρ, θ) and therefore deﬁne
the Ψ-regularized soft sort as

sεΨ(θ) := PεΨ(ρ, θ) = PΨ(ρ/ε, θ).

(5)

(3,2,1)(3,1,2)(2,1,3)(1,2,3)(1,3,2)(2,3,1)rQ(θ)r2Q(θ)r3Q(θ)r100Q(θ)θ(2.9,0.1,1.2)Fast Differentiable Sorting and Ranking

Figure 3. Effect of the regularization parameter ε. We take the vector θ := (0, 3, 1, 2), vary one of its coordinates θi and look at how
[sεΨ(θ)]i and [rεΨ(θ)]i change in response. For soft sorting with Ψ = Q, the function is still piecewise linear, like sorting. However, by
increasing ε we reduce the number of kinks, and the function eventually converges to a mean (Proposition 2). With Ψ = E, the function
tends to be even smoother. For soft ranking with Ψ = Q, the function is piecewise linear instead of piecewise constant for the “hard”
ranks. With Ψ = E, the function again tends to be smoother though it may contain kinks.

For ranking, we choose (z, w) = (−θ, ρ) and therefore
deﬁne the Ψ-regularized soft rank as

rεΨ(θ) := PεΨ(−θ, ρ) = PΨ(−θ/ε, ρ).

(6)

We illustrate the behavior of both of these soft operations
as we vary ε in Figures 2 and 3. As for the hard versions,
the ascending-order soft sorting and ranking are obtained by
negating the input as −sεΨ(−θ) and rεΨ(−θ), respectively.

Properties. We can further characterize these approxima-
tions. Namely, as we now formalize, they are differentiable
a.e., and not only converge to the their “hard” counterparts,
but also satisfy some of their properties for all ε.

Proposition 2. Properties of sεΨ(θ) and rεΨ(θ)

1. Differentiability. For all ε > 0, sεΨ(θ) and

rεΨ(θ) are differentiable (a.e.) w.r.t. θ.

2. Order preservation. Let s := sεΨ(θ), r :=
rεΨ(θ) and σ := σ(θ). For all θ ∈ Rn and
0 < ε < ∞, we have s1 ≥ s2 ≥ · · · ≥ sn
and rσ1 ≤ rσ2 ≤ · · · ≤ rσn .

3. Asymptotics. For all θ ∈ Rn without ties:

s(θ)

sεΨ(θ) −−−→
ε→0
−−−→
ε→∞

fΨ(θ)1

rεΨ(θ) −−−→
ε→0
−−−→
ε→∞

r(θ)

fΨ(ρ)1,

where fQ(u) := mean(u), fE(u) := log fQ(u).

The last property describes the behavior as ε → 0 and
ε → ∞. Together with the proof of Proposition 2, we
include in §B.3 a slightly stronger result. Namely, we derive
an explicit value of ε below which our operators are exactly
equal to their hard counterpart, and a value of ε above which
our operators can be computed in closed form.

Convexiﬁcation effect. Proposition 2 shows
that
[sεΨ(θ)]i and [rεΨ(θ)]i for all i ∈ [n] converge to convex
functions of θ as ε → ∞. This suggests that larger ε
make the objective function increasingly easy to optimize
(at the cost of departing from “hard” sorting or ranking).
This behavior is also visible in Figure 3, where [sεQ(θ)]2
converges towards the mean fQ, depicted by a straight line.

On tuning ε (or not). The parameter ε > 0 controls the
trade-off between approximation of the original operator
and “smoothness”. When the model g(x) producing the
scores or “logits” θ to be sorted/ranked is a homogeneous
function, from (5) and (6), ε can be absorbed into the model.
In our label ranking experiment, we ﬁnd that indeed tuning
ε is not necessary to achieve excellent accuracy. On the
other hand, for top-k classiﬁcation, we ﬁnd that applying a
logistic map to squash θ to [0, 1]n and tuning ε is important,
conﬁrming the empirical ﬁnding of Cuturi et al. (2019).

Relation to linear assignment formulation. We now dis-
cuss the relation between our proposal and a formulation
based on the Birkhoff polytope B ⊂ Rn×n, the convex hull
of permutation matrices. Our exposition corresponds to the
method of Cuturi et al. (2019) with m = n. Note that using
the change of variable y = P ρ and P(ρ) = Bρ, we can
rewrite (4) as r(θ) = P (θ)ρ, where

P (θ) := argmax

(cid:104)P ρ, −θ(cid:105).

P ∈B
Let D(a, b) ∈ Rn×n be a distance matrix. Simple calcula-
tions show that if [D(a, b)]i,j := 1

2 (ai − bj)2, then

P (θ) = argmin

(cid:104)P , D(−θ, ρ)(cid:105).

P ∈B

Similarly, we can rewrite (3) as s(θ) = P (θ)(cid:62)θ. To ob-
tain a differentiable operator, Cuturi et al. (2019) (see also
(Adams & Zemel, 2011)) propose to replace the permuta-
tion matrix P (θ) by a doubly stochastic matrix PεE(θ) :=

Fast Differentiable Sorting and Ranking

argminP ∈B (cid:104)P , D(−θ, ρ)(cid:105) + εE(P ), which is computed
approximately in O(T n2) using Sinkhorn (1967). In com-
parison, our approach is based on regularizing y = P ρ
with Ψ ∈ {Q, E} directly, the key to achieve O(n log n)
time and O(n) space complexity, as we now show.

5. Fast computation and differentiation

As shown in the previous section, computing our soft sort-
ing and ranking operators boils down to projecting onto
a permutahedron. Our key contribution in this section is
the derivation of an O(n log n) forward pass and an O(n)
backward pass (multiplication with the Jacobian) for these
projections. Beyond soft sorting and ranking, this is an
important sensitivity analysis question in its own right.

Reduction to isotonic optimization. We now show how
to reduce the projections to isotonic optimization, i.e., with
simple chain constraints, which is the key to fast computa-
tion and differentiation. We will w.l.o.g. assume that w is
sorted in descending order (if not the case, we sort it ﬁrst).

Proposition 3. Reduction to isotonic optimization
For all z ∈ Rn and sorted w ∈ Rn we have

PΨ(z, w) = z − vΨ(zσ(z), w)σ−1(z)

where

vQ(s, w) := argmin
v1≥···≥vn
vE(s, w) := argmin
v1≥···≥vn

(cid:107)v − (s − w)(cid:107)2, and

1
2
(cid:104)es−v, 1(cid:105) + (cid:104)ew, v(cid:105).

The function vQ is classically known as isotonic regression.
The fact that it can be used to solve the Euclidean projec-
tion onto P(w) has been noted several times (Negrinho &
Martins, 2014; Zeng & Figueiredo, 2015). The reduction of
Bregman projections, which we use here, to isotonic opti-
mization was shown by Lim & Wright (2016). Unlike that
study, we use the KL projection of ez onto P(ew), and not
of z onto P(w), which simpliﬁes many expressions. We
include in §B.4 a simple uniﬁed proof of Proposition 3 based
on Fenchel duality and tools from submodular optimization.
We also discuss an interpretation of adding regularization
to the primal linear program as relaxing the equality con-
straints of the dual linear program in §B.5.

Computation. As shown by Best et al. (2000), the clas-
sical pool adjacent violators (PAV) algorithm for isotonic
regression can be extended to minimize any per-coordinate
decomposable convex function f (v) = (cid:80)n
i=1 fi(vi) sub-
ject to monotonicity constraints, which is exactly the form
of the problems in Proposition 3. The algorithm re-
peatedly splits the coordinates into a set of contiguous

blocks B1, . . . , Bm that partition [n] (their union is [n]
and max Bj + 1 = min Bj+1).
It only requires access
to an oracle that solves for each block Bj the sub-problem
fi(γ), and runs in linear time.
γ(Bj) = argminγ∈R
Further, the solution has a clean block-wise constant struc-
ture, namely it is equal to γ(Bj) within block Bj. Fortu-
nately, in our case, as shown in §B.6, the function γ can be
analytically computed, as

i∈Bj

(cid:80)

γQ(Bj; s, w) :=

1
|Bj|

(cid:88)

si − wi, and

γE(Bj; s, w) := log

esi − log

i∈Bj
(cid:88)

i∈Bj

ewi .

(cid:88)

i∈Bj

(7)

(8)

Hence, PAV returns an exact solution of both vQ(s, w) and
vE(s, w) in O(n) time (Best et al., 2000). This means that
we do not need to choose a number of iterations or a level of
precision, unlike with Sinkhorn. Since computing PQ(z, w)
and PE(z, w) requires obtaining s = zσ(θ) beforehand, the
total computational complexity is O(n log n).

Differentiating isotonic optimization. The block-wise
structure of the solution also makes its derivatives easy to
analyze, despite the fact that we are differentiating the solu-
tion of an optimization problem. Since the coordinates of
the solution in block Bj are all equal to γ(Bj), which in turn
depends only on a subset of the parameters, the Jacobian
has a simple block-wise form, which we now formalize.

Lemma 2. Jacobian of isotonic optimization

Let B1, . . . , Bm be the ordered partition of [n] induced
by vΨ(s, w) from Proposition 3. Then,

∂vΨ(s, w)
∂s



BΨ
1

=




0
0

0

0
. . .
0
0 BΨ
m


 ∈ Rn×n,


where BΨ

j := ∂γΨ(Bj; s, w)/∂s ∈ R|Bj |×|Bj |.

A proof is given in §B.7. The Jacobians w.r.t. w are entirely
similar, thanks to the symmetry of (7) and (8).

∂s

In the quadratic regularization case, it was already derived
by Djolonga & Krause (2017) that BQ
j := 1/|Bj|. The mul-
tiplication with the Jacobian, ν := ∂vQ(s,w)
u for some vec-
tor u, can be computed as ν = (ν1, . . . , νm), where νj =
mean(uBj )1 ∈ R|Bj |. In the entropic regularization case,
novel to our knowledge, we have BE
j = 1 ⊗ softmax(sBj ).
Note that BE
j is column-wise constant, so that the multipli-
cation with the Jacobian ν := ∂vE (s,w)
u, can be computed
as νj = (cid:104)softmax(sBj ), uBj (cid:105)1 ∈ R|Bj |. In both cases, the
multiplication with the Jacobian therefore takes O(n) time.

∂s

Fast Differentiable Sorting and Ranking

Figure 4. Left, center: Accuracy comparison on CIFAR-10, CIFAR-100 (n = 10, n = 100). Right: Runtime comparison for one batch
computation with backpropagation disabled. OT and All-pairs go out-of-memory starting from n = 2000 and n = 3000, respectively.
With backpropagation enabled, the runtimes are similar but OT and All-pairs go out-of-memory at n = 1000 and n = 2500, respectively.

There are interesting differences between the two forms of
regularization. For quadratic regularization, the Jacobian
only depends on the partition B1, . . . , Bm (not on s) and
the blocks have constant value. For entropic regularization,
the Jacobian does depend on s and the blocks are constant
column by column. Both formulations are averaging the
incoming gradients, one uniformly and the other weighted.

Differentiating the projections. We now combine Propo-
sition 3 with Lemma 2 to characterize the Jacobians of the
projections onto the permutahedron and show how to multi-
ply arbitrary vectors with them in linear time.

Proposition 4. Jacobian of the projections

Let PΨ(z, w) be deﬁned in Proposition 3. Then,

∂PΨ(z, w)
∂z

= JΨ(zσ(z), w)σ−1(z),

where Jπ is the matrix obtained by permuting the rows
and columns of J according to π, and where

JΨ(s, w) := I −

∂vΨ(s, w)
∂s

.

Again, the Jacobian w.r.t. w is entirely symmetric. Unlike
the Jacobian of isotonic optimization, the Jacobian of the
projection is not block diagonal, as we need to permute its
rows and columns. We can nonetheless multiply with it in
linear time by using the simple identity (Jπ)z = (J zπ−1)π,
which allows us to reuse the O(n) multiplication with the
Jacobian of isotonic optimization.

6. Experiments

We present in this section our empirical ﬁndings. NumPy,
JAX, PyTorch and Tensorﬂow versions of our sorting and
ranking operators are available at https://github.
com/google-research/fast-soft-sort/.

6.1. Top-k classiﬁcation loss function

Experimental setup. To demonstrate the effectiveness of
our proposed soft rank operators as a drop-in replacement
for exisiting ones, we reproduce the top-k classiﬁcation
experiment of Cuturi et al. (2019). The authors propose a
loss for top-k classiﬁcation between a ground truth class
y ∈ [n] and a vector of soft ranks r ∈ Rn, which is higher
if the predicted soft ranks correctly place y in the top-k
elements. We compare the following soft operators

• OT (Cuturi et al., 2019): optimal transport formulation.

• All-pairs (Qin et al., 2010): noting that [r(θ)]i is equiva-
lent to (cid:80)
j(cid:54)=i 1[θi < θj] + 1, one can obtain soft ranks in
O(n2) by replacing the indicator function with a sigmoid.

• Proposed: our O(n log n) soft ranks rQ and rE. Al-
though not used in this experiment, for top-k ranking, the
complexity can be reduced to O(n log k) by computing
PΨ using the algorithm of Lim & Wright (2016).

We use the CIFAR-10 and CIFAR-100 datasets, with n =
10 and n = 100 classes, respectively. Following Cuturi
et al. (2019), we use a vanilla CNN (4 Conv2D with 2 max-
pooling layers, ReLU activation, 2 fully connected layers
with batch norm on each), the ADAM optimizer (Kingma &
Ba, 2014) with a constant step size of 10−4, and set k = 1.
Similarly to Cuturi et al. (2019), we found that squashing
the scores θ to [0, 1]n with a logistic map was beneﬁcial.

Differentiating sεΨ and rεΨ. With the Jacobian of
PΨ(z, w) w.r.t. z and w at hand, differentiating sεΨ and
rεΨ boils down to a mere application of the chain rule to (5)
and (6). To summarize, we can multiply with the Jacobians
of our soft operators in O(n) time and space.

Results. Our empirical results, averaged over 12 runs, are
shown in Figure 4 (left, center). On both CIFAR-10 and
CIFAR-100, our soft rank formulations achieve comparable
accuracy to the OT formulation, though signiﬁcantly faster,
as we elaborate below. Similarly to Cuturi et al. (2019),

Fast Differentiable Sorting and Ranking

we found that the soft top-k loss slightly outperforms the
classical cross-entropy (logistic) loss for these two datasets.
However, we did not ﬁnd that the All-pairs formulation
could outperform the cross-entropy loss.

The training times for 600 epochs on CIFAR-100 were 29
hours (OT), 21 hours (rQ), 23 hours (rE) and 16 hours (All-
pairs). Training times on CIFAR-10 were similar. While
our soft operators are several hours faster than OT, they are
slower than All-pairs, despite its O(n2) complexity. This is
due the fact that, with n = 100, All-pairs is very efﬁcient
on GPUs, while our PAV implementation runs on CPU.

6.2. Runtime comparison: effect of input dimension

To measure the impact of the dimensionality n on the run-
time of each method, we designed the following experiment.

Experimental setup. We generate score vectors θ ∈ Rn
randomly according to N (0, 1), for n ranging from 100 up
to 5000. For fair comparison with GPU implementations
(OT, All-pairs, Cross-entropy), we create a batch of 128 such
vectors and we compare the time to compute soft ranking
operators on this batch. We run this experiment on top of
TensorFlow (Abadi et al., 2016) on a six core Intel Xeon
W-2135 with 64 GBs of RAM and a GeForce GTX 1080 Ti.

Results. Run times for one batch computation with back-
propagation disabled are shown in Figure 4 (Right). While
their runtime is reasonable in small dimension, OT and All-
pairs scale quadratically with respect to the dimensionality
n (note the log scale on the y-axis). Although slower than a
softmax, our formulations scale well, with the dimension-
ality n having negligible impact on the runtime. OT and
All-pairs go out-of-memory starting from n = 2000 and
n = 3000, respectively. With backpropagation enabled,
they go out-of-memory at n = 1000 and n = 2500, due
to the need for recording the computational graph. This
shows that the lack of memory available on GPUs is prob-
lematic for these methods. In contrast, our approaches only
require O(n) memory and comes with the theoretical Jaco-
bian (they do not rely on differentiating through iterates).
They therefore suffer from no such issues.

6.3. Label ranking via soft Spearman’s rank

correlation coefﬁcient

We now consider the label ranking setting where supervision
is given as full rankings (e.g., 2 (cid:31) 1 (cid:31) 3 (cid:31) 4) rather than
as label relevance scores. The goal is therefore to learn to
predict permutations, i.e., a function fw : X → Σ. A clas-
sical metric between ranks is Spearman’s rank correlation
coefﬁcient, deﬁned as the Pearson correlation coefﬁcient
between the ranks. Maximizing this coefﬁcient is equiva-
lent to minimizing the squared loss between ranks. A naive
idea would be therefore to use as loss 1
2 (cid:107)r − r(θ)(cid:107)2, where

Figure 5. Label ranking accuracy with and without soft rank
layer. Each point above the line represents a dataset where our soft
rank layer improves Spearman’s rank correlation coefﬁcient.

θ = gw(x). This is unfortunately a discontinuous function
of θ. We therefore propose to rather use 1
2 (cid:107)r − rΨ(θ)(cid:107)2,
hence the name differentiable Spearman’s rank correlation
coefﬁcient. At test time, we replace rΨ with r, which is
justiﬁed by the order-preservation property (Proposition 2).

Experimental setup. We consider the 21 datasets from
(H¨ullermeier et al., 2008; Cheng et al., 2009), which has
both semi-synthetic data obtained from classiﬁcation prob-
lems, and real biological measurements. Following (Korba
et al., 2018), we average over two 10-fold validation runs,
in each of which we train on 90% and evaluate on 10% of
the data. Within each repetition, we run an internal 5-fold
cross-validation to grid-search for the best parameters. We
consider linear models of the form gW,b(x) = W x + b, and
for ablation study we drop the soft ranking layer rΨ.

Results. Due to the large number of datasets, we choose
to present a summary of the results in Figure 5. We postpone
detailed results to the appendix (Table 1). Out of 21 datasets,
introducing a soft rank layer with Ψ = Q works better on
15 datasets, similarly on 4 and worse on 2 datasets. We can
thus conclude that even for such simple model, introducing
our layer is beneﬁcial, and even achieving state of the art
results on some of the datasets (full details in the appendix).

6.4. Robust regression via soft least trimmed squares

We explore in this section the application of our soft sorting
operator sεΨ to robust regression. Let x1, . . . , xn ∈ X ⊆
Rd and y1, . . . , yn ∈ Y ⊆ R be a training set of input-
output pairs. Our goal is to learn a model gw : Rd → R that
predicts outputs from inputs, where w are model parameters.
We focus on gw(x) := (cid:104)w, x(cid:105) for simplicity. We further
assume that a certain proportion of examples are outliers
including some label noise, which makes the task of robustly
estimating gw particularly challenging.

The classical ridge regression can be cast as

min
w

1
n

n
(cid:88)

i=1

(cid:96)i(w) +

1
2ε

(cid:107)w(cid:107)2,

(9)

Fast Differentiable Sorting and Ranking

Figure 6. Empirical validation of inter-
polation between LTS and LS.

Figure 7. R2 score (higher is better) averaged over 10 train-time splits for increasing percentage
of outliers. Hyper-parameters are tuned by 5-fold cross-validation.

where (cid:96)i(w) := 1
2 (yi − gw(xi))2. In order to be robust
to label noise, we propose instead to sort the losses (from
larger to smaller) and to ignore the ﬁrst k ones. Introducing
our soft sorting operator, this can be formulated as

min
w

1
n − k

n
(cid:88)

i=k+1

(cid:96)ε
i (w)

(10)

where (cid:96)ε
i (w) := [sεΨ((cid:96)(w))]i is the ith loss in the soft sort,
and (cid:96)(w) ∈ Rn is the loss vector that gathers (cid:96)i(w) for
each i ∈ [n]. Solving (10) with existing O(n2) soft sorting
operators could be particularly computationally prohibitive,
since here n is the number of training samples.

When ε → 0, we have sεΨ((cid:96)(w)) → s((cid:96)(w)) and (10) is
known as least trimmed squares (LTS) (Rousseeuw, 1984;
Rousseeuw & Leroy, 2005). When ε → ∞, we have, from
Proposition 2, sεΨ((cid:96)(w)) → mean((cid:96)(w))1 and therefore
both (9) and (10) converge to the least squares (LS) objec-
tive, minw
i=1 (cid:96)i(w). To summarize, our proposed
objective (10), dubbed soft least trimmed squares, inter-
polates between least trimmed squares ((cid:15) → 0) and least
squares ((cid:15) → ∞), as also conﬁrmed empirically in Figure 6.

(cid:80)n

1
n

Experimental setup. To empirically validate our pro-
posal, we compare cross-validated results for increasing
percentage of outliers of the following methods:

• Least trimmed squares, with truncation parameter k,

• Soft least trimmed squares (10), with truncation parame-

ter k and regularization parameter ε,

• Ridge regression (9), with regularization parameter ε,

• Huber loss (Huber, 1964) with regularization parameter ε
and threshold parameter τ , as implemented in scikit-learn
(Pedregosa et al., 2011).

We consider datasets from the LIBSVM archive (Fan &
Lin, 2011). We hold out 20% of the data as test set and
use the rest as training set. We artiﬁcally create outliers,
by adding noise to a certain percentage of the training la-
bels, using yi ← yi + e, where e ∼ N (0, 5 × std(y)).
We do not add noise to the test set. For all methods,

we use L-BFGS (Liu & Nocedal, 1989), with a maxi-
mum of 300 iterations. For hyper-parameter optimiza-
tion, we use 5-fold cross-validation. We choose k from
{(cid:100)0.1n(cid:101), (cid:100)0.2n(cid:101), . . . , (cid:100)0.5n(cid:101)}, ε from 10 log-spaced values
between 10−3 and 104, and τ from 5 linearly spaced values
between 1.3 and 2. We repeat this procedure 10 times with
a different train-test split, and report the averaged R2 scores
(a.k.a. coefﬁcient of determination).

Results. The averaged R2 scores (higher is better) are
shown in Figure 7. On all datasets, the accuracy of ridge
regression deteriorated signiﬁcantly with increasing number
of outliers. Least trimmed squares (hard or soft) performed
slightly worse than the Huber loss on housing, compara-
bly on bodyfat and much better on cadata. We found that
hard least trimmed squares (i.e., ε = 0) worked well on
all datasets, showing that regularization is less important
for sorting operators (which are piecewise linear) than for
ranking operators (which are piecewise constant). Never-
theless, regularization appeared useful in some cases. For
instance, on cadata, the cross-validation procedure picked
ε > 1000 when the percentage of outliers is less than 20%,
and ε < 10−3 when the percentage of outliers is larger than
20%. This is conﬁrmed visually on Figure 7, where the soft
sort with Ψ = Q works slightly better than the hard sort
with few outliers, then performs comparably with more out-
liers. The interpolation effect enabled by ε therefore allows
some adaptivity to the (unknown) percentage of outliers.

7. Conclusion

Building upon projections onto permutahedra, we con-
structed differentiable sorting and ranking operators. We
derived exact O(n log n) computation and O(n) differen-
tiation of these operators, a key technical contribution of
this paper. We demonstrated that our operators can be used
as a drop-in replacement for existing O(n2) ones, with an
order-of-magnitude speed-up. We also showcased two ap-
plications enabled by our soft operators: label ranking with
differentiable Spearman’s rank correlation coefﬁcient and
robust regression via soft least trimmed squares.

Fast Differentiable Sorting and Ranking

Acknowledgements

We are grateful to Marco Cuturi and Jean-Philippe Vert for
useful discussions, and to Carlos Riquelme for comments
on a draft of this paper.

References

Dantzig, G. B., Orden, A., and Wolfe, P. The generalized
simplex method for minimizing a linear form under linear
inequality restraints. Paciﬁc Journal of Mathematics, 5
(2):183–195, 1955.

David, H. A. and Nagaraja, H. N. Order statistics. Encyclo-

pedia of Statistical Sciences, 2004.

Abadi, M. et al. Tensorﬂow: A system for large-scale ma-
chine learning. In 12th USENIX Symposium on Operating
Systems Design and Implementation, pp. 265–283, 2016.

Djolonga, J. and Krause, A. Differentiable learning of
submodular models. In Proc. of NeurIPS, pp. 1013–1023,
2017.

Adams, R. P. and Zemel, R. S. Ranking via sinkhorn propa-

gation. arXiv e-prints, 2011.

Ailon, N., Hatano, K., and Takimoto, E. Bandit online opti-
mization over the permutahedron. Theoretical Computer
Science, 650:92–108, 2016.

Bach, F. Learning with submodular functions: A convex
optimization perspective. Foundations and Trends R(cid:13) in
Machine Learning, 6(2-3):145–373, 2013.

Best, M. J., Chakravarti, N., and Ubhaya, V. A. Minimizing
separable convex functions subject to simple chain con-
straints. SIAM Journal on Optimization, 10(3):658–672,
2000.

Blondel, M. Structured prediction with projection oracles.

In Proc. of NeurIPS, 2019.

Blondel, M., Martins, A. F., and Niculae, V. Learning clas-
siﬁers with Fenchel-Young losses: Generalized entropies,
margins, and algorithms. In Proc. of AISTATS, 2019.

Blondel, M., Martins, A. F., and Niculae, V. Learning with
Fenchel-Young losses. arXiv preprint arXiv:1901.02324,
2019.

Edmonds, J. Submodular functions, matroids, and certain
polyhedra. In Combinatorial structures and their appli-
cations, pp. 69–87, 1970.

Fan, R.-E. and Lin, C.-J. LIBSVM datasets, 2011.

Grover, A., Wang, E., Zweig, A., and Ermon, S. Stochastic
optimization of sorting networks via continuous relax-
ations. arXiv preprint arXiv:1903.08850, 2019.

Huber, P. J. Robust estimation of a location parameter. The
Annals of Mathematical Statistics, 35(1):73–101, 1964.

H¨ullermeier, E., F¨urnkranz, J., Cheng, W., and Brinker, K.
Label ranking by learning pairwise preferences. Artiﬁcial
Intelligence, 172, 2008.

Kingma, D. P. and Ba, J. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980, 2014.

Korba, A., Garcia, A., and d’Alch´e Buc, F. A structured pre-
diction approach for label ranking. In Proc. of NeurIPS,
2018.

Lapin, M., Hein, M., and Schiele, B. Loss functions for
top-k error: Analysis and insights. In Proc. of CVPR,
2016.

Bowman, V. Permutation polyhedra. SIAM Journal on

Applied Mathematics, 22(4):580–589, 1972.

LeCun, Y. Deep Learning est mort. Vive Differentiable

Programming!, 2018.

Chapelle, O. and Wu, M. Gradient descent optimization
of smoothed information retrieval metrics. Information
retrieval, 13(3):216–235, 2010.

Lim, C. H. and Wright, S. J. Efﬁcient bregman projections
onto the permutahedron and related polytopes. In Proc.
of AISTATS, pp. 1205–1213, 2016.

Cheng, W., H¨uhn, J., and H¨ullermeier, E. Decision tree and
instance-based learning for label ranking. In International
Conference on Machine Learning (ICML-09), 2009.

Liu, D. C. and Nocedal, J. On the limited memory BFGS
method for large scale optimization. Mathematical Pro-
gramming, 45(1):503–528, 1989.

Cuturi, M. Sinkhorn distances: Lightspeed computation of

optimal transport. In Proc. of NeurIPS, 2013.

Cuturi, M., Teboul, O., and Vert, J.-P. Differentiable ranking
and sorting using optimal transport. In Proc. of NeurIPS,
2019.

Martins, A. F. and Astudillo, R. F. From softmax to sparse-
max: A sparse model of attention and multi-label classiﬁ-
cation. In Proc. of ICML, 2016.

Negrinho, R. and Martins, A. Orbit regularization. In Proc.

of NeurIPS, 2014.

Fast Differentiable Sorting and Ranking

Niculae, V., Martins, A. F., Blondel, M., and Cardie, C.
SparseMAP: Differentiable sparse structured inference.
In Proc. of ICML, 2018.

Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V.,
Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,
Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cour-
napeau, D., Brucher, M., Perrot, M., and Duchesnay, E.
Scikit-learn: Machine learning in Python. Journal of
Machine Learning Research, 12:2825–2830, 2011.

Peyr´e, G. and Cuturi, M. Computational Optimal Transport.
Foundations and Trends in Machine Learning, 2017.

Qin, T., Liu, T.-Y., and Li, H. A general approximation
framework for direct optimization of information retrieval
measures. Information retrieval, 13(4):375–397, 2010.

Rol´ınek, M., Musil, V., Paulus, A., Vlastelica, M., Michaelis,
C., and Martius, G. Optimizing rank-based metrics with
blackbox differentiation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pp. 7620–7630, 2020.

Rousseeuw, P. J. Least median of squares regression. Jour-
nal of the American statistical association, 79(388):871–
880, 1984.

Rousseeuw, P. J. and Leroy, A. M. Robust regression and
outlier detection, volume 589. John wiley & sons, 2005.

Sinkhorn, R. and Knopp, P. Concerning nonnegative ma-
trices and doubly stochastic matrices. Paciﬁc Journal of
Mathematics, 21(2):343–348, 1967.

Spearman, C. The proof and measurement of association
between two things. American Journal of Psychology,
1904.

Suehiro, D., Hatano, K., Kijima, S., Takimoto, E., and
Nagano, K. Online prediction under submodular con-
In International Conference on Algorithmic
straints.
Learning Theory, pp. 260–274, 2012.

Taylor, M., Guiver, J., Robertson, S., and Minka, T. Soft-
rank: optimizing non-smooth rank metrics. In Proceed-
ings of the 2008 International Conference on Web Search
and Data Mining, pp. 77–86, 2008.

Yasutake, S., Hatano, K., Kijima, S., Takimoto, E., and
Takeda, M. Online linear optimization over permuta-
tions. In International Symposium on Algorithms and
Computation, pp. 534–543. Springer, 2011.

Zeng, X. and Figueiredo, M. A. The ordered weighted
(cid:96)1 norm: Atomic formulation and conditional gradient
algorithm. In Proc. of SPARS, 2015.

Ziegler, G. M. Lectures on polytopes, volume 152. Springer

Science & Business Media, 2012.

Fast Differentiable Sorting and Ranking

Appendix

A. Additional empirical results

We include in this section the detailed label ranking results on the same 21 datasets as considered by H¨ullermeier et al.
(2008) as well as Cheng et al. (2009).

For entropic regularization E, in addition to rE, we also consider an alternative formulation. Since ρ is already strictly
positive, instead of using the log-projection onto P(eρ), we can directly use the projection onto P(ρ). In our notation, this
can be written as ˜rεE(θ) = ˜rE(θ/ε), where

˜rE(θ) := argmin
µ∈P(ρ)

KL(µ, e−θ) = ePE (−θ,log ρ).

Spearman’s rank correlation coefﬁcient for each method, averaged over 5 runs, is shown in the table below.

Dataset

fried
wine
authorship
pendigits
segment
glass
vehicle
iris
stock
wisconsin
elevators
vowel
housing
cpu-small
bodyfat
calhousing
diau
spo
dtt
cold
heat

rQ (L2)

rE (log-KL)

˜rE (KL)

No projection

1.00 ± 0.00
0.96 ± 0.03 (-0.01)
0.96 ± 0.01
0.96 ± 0.00 (+0.02)
0.95 ± 0.01 (+0.02)
0.89 ± 0.04 (+0.03)
0.88 ± 0.02 (+0.04)
0.89 ± 0.07 (+0.06)
0.82 ± 0.02 (+0.04)
0.79 ± 0.03 (+0.01)
0.81 ± 0.00 (+0.04)
0.76 ± 0.03 (+0.03)
0.77 ± 0.03 (+0.07)
0.55 ± 0.01 (+0.05)
0.35 ± 0.07 (-0.01)
0.27 ± 0.01 (+0.01)
0.26 ± 0.02
0.18 ± 0.02
0.15 ± 0.04
0.09 ± 0.03
0.06 ± 0.02

1.00 ± 0.00
0.95 ± 0.04 (-0.02)
0.95 ± 0.01
0.96 ± 0.00 (+0.02)
0.95 ± 0.00 (+0.02)
0.88 ± 0.05 (+0.02)
0.88 ± 0.02 (+0.03)
0.87 ± 0.07 (+0.04)
0.81 ± 0.02 (+0.03)
0.77 ± 0.03 (-0.01)
0.81 ± 0.00 (+0.04)
0.77 ± 0.01 (+0.05)
0.78 ± 0.02 (+0.08)
0.56 ± 0.01 (+0.05)
0.34 ± 0.07 (-0.02)
0.27 ± 0.01
0.26 ± 0.02
0.19 ± 0.02 (+0.01)
0.16 ± 0.04
0.09 ± 0.03
0.06 ± 0.02

1.00 ± 0.00
0.96 ± 0.03 (-0.01)
0.95 ± 0.01
0.96 ± 0.00 (+0.02)
0.95 ± 0.01 (+0.02)
0.89 ± 0.04 (+0.03)
0.89 ± 0.02 (+0.04)
0.87 ± 0.07 (+0.05)
0.83 ± 0.02 (+0.05)
0.79 ± 0.03 (+0.01)
0.81 ± 0.00 (+0.04)
0.78 ± 0.02 (+0.05)
0.77 ± 0.03 (+0.07)
0.54 ± 0.01 (+0.04)
0.34 ± 0.08 (-0.02)
0.27 ± 0.01 (+0.01)
0.26 ± 0.02
0.18 ± 0.02
0.14 ± 0.04 (-0.01)
0.10 ± 0.03 (+0.01)
0.06 ± 0.02

1.00 ± 0.00
0.97 ± 0.02
0.95 ± 0.01
0.94 ± 0.00
0.93 ± 0.01
0.87 ± 0.05
0.85 ± 0.03
0.83 ± 0.09
0.78 ± 0.02
0.78 ± 0.03
0.77 ± 0.00
0.73 ± 0.02
0.70 ± 0.03
0.50 ± 0.02
0.36 ± 0.07
0.26 ± 0.01
0.26 ± 0.02
0.18 ± 0.02
0.15 ± 0.04
0.09 ± 0.04
0.06 ± 0.02

Table 1. Detailed results of our label ranking experiment. Blue color indicates better Spearman rank correlation coefﬁcient compared to
using no projection. Red color indicates worse coeffcient.

Fast Differentiable Sorting and Ranking

B. Proofs

B.1. Proof of Lemma 1 (Discrete optimization formulation)

For the ﬁrst claim, we have for all w ∈ Rn such that w1 > w2 > · · · > wn

and in particular for w = ρ. The second claim follows from

σ∈Σ

σ(θ) = argmax

(cid:104)θσ, w(cid:105)

σ(θ) = argmax

(cid:104)θ, wσ−1(cid:105) = argmax
π−1∈Σ

(cid:104)θ, wπ(cid:105) =

σ∈Σ

(11)

(cid:18)

argmax
π∈Σ

(cid:19)−1

(cid:104)θ, wπ(cid:105)

.

B.2. Proof of Proposition 1 (Linear programming formulations)

Let us prove the ﬁrst claim. The key idea is to absorb θσ in the permutahedron. Using (11), we obtain for all θ ∈ Rn and
for all w ∈ Rn such that w1 > · · · > wn

θσ(θ) = argmax
θσ : σ∈Σ

(cid:104)θσ, w(cid:105) = argmax
y∈Σ(θ)

(cid:104)y, w(cid:105) = argmax
y∈P(θ)

(cid:104)y, w(cid:105),

where in the second equality we used P(θ) = conv(Σ(θ)) and the fundamental theorem of linear programming (Dantzig
et al., 1955, Theorem 6). For the second claim, we have similarly

wr(θ) = argmax
wπ : π∈Σ

(cid:104)θ, wπ(cid:105) = argmax
y∈P(w)

(cid:104)θ, y(cid:105).

Setting w = ρ and using ρr(θ) = ρσ−1(θ) = σ−1(−θ) = r(−θ) proves the claim.

B.3. Proof of Proposition 2 (Properties of soft sorting and ranking operators)

Differentiability. Let C be a closed convex set and let µ(cid:63)(z) := argmaxµ∈C(cid:104)µ, z(cid:105) − Ψ(z). If Ψ is strongly convex over
C, then µ(cid:63)(z) is Lipschitz continuous. By Rademachers theorem, µ(cid:63)(z) is differentiable almost everywhere. Furthermore,
since PΨ(z, w) = ∇Ψ(µ(cid:63)(z)) with C = P(∇Ψ−1(w)), PΨ(z, w) is differentiable a.e. as long as Ψ is twice differentiable,
which is the case when Ψ ∈ {Q, E}.

Order preservation. Proposition 1 of Blondel et al. (2019) shows that µ(cid:63)(z) and z are sorted the same way. Furthermore,
since PΨ(z, w) = ∇Ψ(µ(cid:63)(z)) with C = P(∇Ψ−1(w)) and since ∇Ψ is monotone, PΨ(z, w) is sorted the same way as
z, as well. Let s = sεΨ(θ) and r = rεΨ(θ). From the respective deﬁnitions, this means that s is sorted the same way as ρ
(i.e., it is sorted in descending order) and r is sorted the same way as −θ, which concludes the proof.

Asymptotic behavior. We will now characterize the behavior for sufﬁciently small and large regularization strength ε.
Note that rather than multiplying the regularizer Ψ by ε > 0, we instead divide s by ε, which is equivalent.

Lemma 3. Analytical solutions of isotonic optimization in the limit regimes

If ε ≤ εmin(s, w) := mini∈[n−1]

si−si+1
wi−wi+1

, then

vQ(s/ε, w) = vE(s/ε, w) = s/ε − w.

If ε > εmax(s, w) := maxi<j

si−sj
wi−wj

, then

vQ(s/ε, w) =

1
n

n
(cid:88)

(si/ε − wi)1 and vE(s/ε, w) = (LSE(s/ε) − LSE(w))1,

i=1

where LSE(x) := log (cid:80)

i exi.

Fast Differentiable Sorting and Ranking

Proof. We start with the ε ≤ εmin(s, w) case. Recall that s is sorted in descending order. Therefore, since we chose ε
sufﬁciently small, the vector v = s/ε − w is sorted in descending order as well. This means that v is feasible, i.e., it belongs
to the constraint sets in Proposition 3. Further, note that vi = γQ({i}; s/ε, w) = γE({i}; s/ε, w) = si/ε − wi so that v is
the optimal solution if we drop the constraints, which completes the argument.

Next, we tackle the ε > εmax(s, w) case. Note that the claimed solutions are exactly γQ([n]; s, w) and γE([n]; s, w),
so the claim will immediately follow if we show that [n] is an optimal partition. The PAV algorithm (cf. §B.6) merges
at each iteration any two neighboring blocks B1, B2 that violate γΨ(B1; s/ε, w) ≥ γΨ(B2; s/ε, w), starting from the
partitions consisting of singleton sets. Let k ∈ {1, . . . , n − 1} be the iteration number. We claim that the two blocks,
B1 = {1, 2, . . . , k} and B2 = {k + 1}, will always be violating the constraint, so that they can be merged. Note that in the
quadratic case, they can be merged only if

which is equivalent to

k
(cid:88)

(si/ε − wi)/k < sk+1/ε − wk+1,

i=1

k
(cid:88)

i=1

si − sk+1
kε

<

k
(cid:88)

i=1

(wi − wk+1),

which is indeed satisﬁed when ε > εmax(s, w). In the KL case, they can be merged only if

log

k
(cid:88)

i=1

esi/ε − log

k
(cid:88)

i=1

ewi < sk+1/ε − wk+1 ⇐⇒ log

⇐⇒ log

⇐⇒ log

⇐⇒

k
(cid:88)

i=1

k
(cid:88)

i=1

k
(cid:88)

i=1

k
(cid:88)

i=1

esi/ε − sk+1/ε < log

k
(cid:88)

i=1

ewi − wk+1

esi/ε − log esk+1/ε < log

k
(cid:88)

i=1

ewi − log ewk+1

e(si−sk+1)/ε < log

k
(cid:88)

i=1

ewi−wk+1

e(si−sk+1)/ε <

k
(cid:88)

i=1

ewi−wk+1.

This will be true if the ith term on the left-hand side is smaller than the ith term on the right-hand side, i.e., when
(si − sk+1)/ε < wi − wk+1, which again is implied by the assumption.

We can now directly characterize the behavior of the projection operator PΨ in the two regimes ε ≤ εmin(s(z), w) and
ε > εmax(s(z), w). This in turn implies the results for both the soft ranking and sorting operations using (5) and (6).

Proposition 5. Analytical solutions of the projections in the limit regimes

If ε ≤ εmin(s(z), w), then

If ε > εmax(s(z), w), then

PΨ(z/ε, w) = wσ−1(z).

PQ(z/ε, w) = z/ε − mean(z/ε − w)1, and
PE(z/ε, w) = z/ε − LSE(z/ε)1 + LSE(w)1.

Therefore, in these two regimes, we do not even need PAV to compute the optimal projection.

Fast Differentiable Sorting and Ranking

B.4. Proof of Proposition 3 (Reduction to isotonic optimization)

Before proving Proposition 3, we need the following three lemmas.

Lemma 4. Technical lemma
Let f : R → R be convex, v1 ≥ v2 and s2 ≥ s1. Then, f (s1 − v1) + f (s2 − v2) ≥ f (s2 − v1) + f (s1 − v2).

Proof. Note that s2 − v2 ≥ s2 − v1 ≥ s1 − v1 and s2 − v2 ≥ s1 − v2 ≥ s1 − v1. This means that we can express s2 − v1
and s1 − v2 as a convex combination of the endpoints of the line segment [s1 − v1, s2 − v2], namely

s2 − v1 = α(s2 − v2) + (1 − α)(s1 − v1)

and s1 − v2 = β(s2 − v2) + (1 − β)(s1 − v1).

Solving for α and β gives α = 1 − β. From the convexity of f , we therefore have

f (s2 − v1) ≤ αf (s2 − v2) + (1 − α)f (s1 − v1)

and f (s1 − v2) ≤ (1 − α)f (s2 − v2) + αf (s1 − v1).

Summing the two proves the claim.

Lemma 5. Dual formulation of a regularized linear program
Let µ(cid:63) = argmaxµ∈C(cid:104)µ, z(cid:105) − Ψ(µ), where C ⊆ Rn is a closed convex set and Ψ is strongly convex. Then, the
corresponding dual solution is u(cid:63) = argminu∈Rn Ψ∗(z − u) + sC(u), where sC(u) := supy∈C(cid:104)y, u(cid:105) is the support
function of C. Moreover, µ(cid:63) = ∇Ψ∗(z − u(cid:63)).

Proof. The result is well-known and we include the proof for completeness. Let us deﬁne the Fenchel conjugate of a
function Ω : Rn → R ∪ {∞} by

Let Ω := Ψ + Φ, where Ψ is strongly convex and Φ is convex. We have

Ω∗(z) := sup
µ∈Rn

(cid:104)µ, z(cid:105) − Ω(µ).

Ω∗(z) = (Ψ + Φ)∗(z) = inf
u∈Rn

Φ∗(u) + Ψ∗(z − u),

which is the inﬁmal convolution of Φ∗ with Ψ∗. Moreover, ∇Ω∗(z) = ∇Ψ∗(z − u(cid:63)). The results follows from choosing
Φ(µ) = IC(µ) and noting that I ∗

C = sC.

For instance, with Ψ = Q, we have Ψ∗ = Q, and with Ψ = E, we have Ψ∗ = exp.

The next lemma shows how to go further by choosing C as the base polytope B(F ) associated with a cardinality-based
submodular function F , of which the permutahedron is a special case. The polytope is deﬁned as (see, e.g., Bach (2013))

(cid:40)

B(F ) :=

µ ∈ Rn :

(cid:88)

i∈S

µi ≤ F (S) ∀S ⊆ [n],

(cid:41)

µi = F ([n])

.

n
(cid:88)

i=1

Lemma 6. Reducing dual formulation to isotonic regression

Let F (S) = g(|S|) for some concave g. Let B(F ) be its corresponding base polytope. Let σ be a permutation of [n]
such that z ∈ Rn is sorted in descending order, i.e., zσ1 ≥ zσ2 ≥ · · · ≥ zσn . Assume Ψ(µ) = (cid:80)n
i=1 ψ(µi), where ψ
is convex. Then, the dual solution u(cid:63) from Lemma 5 is equal to v(cid:63)

σ−1, where

v(cid:63) = argmin
v1≥···≥vn
= − argmin
v(cid:48)
1≤···≤v(cid:48)
n

Ψ∗(zσ − v) + (cid:104)fσ, v(cid:105)

Ψ∗(v(cid:48)

σ + z) − (cid:104)fσ, v(cid:48)(cid:105).

Fast Differentiable Sorting and Ranking

Proof. The support function sB(F )(u) is known as the Lov´asz extension of F . For conciseness, we use the standard notation
f (u) := sB(F )(u). Applying Lemma 5, we obtain

u(cid:63) = argmin
u∈Rn

Ψ∗(z − u) + f (u).

Using the “greedy algorithm” of Edmonds (1970), we can compute f (u) as follows. First, choose a permutation σ that
sorts u in descending order, i.e., uσ1 ≥ uσ2 ≥ · · · ≥ uσn . Then a maximizer f ∈ B(F ) ⊆ Rn is obtained by forming
fσ = (fσ1, . . . , fσn ), where

fσi := F ({σ1, . . . , σi}) − F ({σ1, . . . , σi−1}).

Moreover, (cid:104)f , u(cid:105) = f (u).

Let us ﬁx σ to the permutation that sorts u(cid:63). Following the same idea as from (Djolonga & Krause, 2017), since the Lov´asz
extension is linear on the set of all vectors that are sorted by σ, we can write

argmin
u∈Rn

Ψ∗(z − u) + f (u) = argmin

Ψ∗(z − u) + (cid:104)f , u(cid:105).

uσ1 ≥···≥uσn

This is an instance of isotonic optimization, as we can rewrite the problem as

argmin
v1≥···≥vn

Ψ∗(z − vσ−1 ) + (cid:104)f , vσ−1(cid:105) = argmin
v1≥···≥vn

Ψ∗(zσ − v) + (cid:104)fσ, v(cid:105),

(12)

with u(cid:63)

σ = v(cid:63) ⇔ u(cid:63) = v(cid:63)

σ−1.

Let s := zσ. It remains to show that s1 ≥ · · · ≥ sn, i.e., that s and the optimal dual variables v(cid:63) are both in descending
order. Suppose sj > si for some i < j. Let s(cid:48) be a copy of s with si and sj swapped. Since ψ∗ is convex, by Lemma 4,

Ψ∗(s − v(cid:63)) − Ψ∗(s(cid:48) − v(cid:63)) = ψ∗(si − v(cid:63)

i ) + ψ∗(sj − v(cid:63)

j ) − ψ∗(sj − v(cid:63)

i ) − ψ∗(si − v(cid:63)

j ) ≥ 0,

which contradicts the assumption that v(cid:63) and the corresponding σ are optimal. A similar result is proven by Suehiro et al.
(2012, Lemma 1) but for the optimal primal variable µ(cid:63).

We now prove Proposition 3. The permutahedron P(w) is a special case of B(F ) with F (S) = (cid:80)|S|
· · · ≥ wn. In that case, fσ = (fσ1, . . . , fσn) = (w1, . . . , wn) = w.
For P(∇Ψ∗(w)), we thus have fσ = ∇Ψ∗(w). Finally, note that if Ψ is Legendre-type, which is the case of both Q and E,
then ∇Ψ∗ = (∇Ψ)−1. Therefore, ∇Ψ(µ(cid:63)) = z − u(cid:63), which concludes the proof.

i=1 wi and w1 ≥ w2 ≥

B.5. Relaxed dual linear program interpretation

We show in this section that the dual problem in Lemma 6 can be interpreted as the original dual linear program (LP) with
relaxed equality constraints. Consider the primal LP

As shown by Bach (2013, Proposition 3.2), the dual LP is

max
y∈B(F )

(cid:104)y, z(cid:105).

min
λ∈C

(cid:88)

S⊆V

λS F (S)

where

(cid:40)

C :=

λ ∈ R2V

: λS ≥ 0 ∀S ⊂ V, λV ∈ R, zi =

(cid:41)

λS ∀i ∈ [n]

.

(cid:88)

S : i∈S

(13)

(14)

Moreover, let σ be a permutation sorting z in descending order. Then, an optimal λ is given by (Bach, 2013, Proposition
3.2)

λS =






zσi − zσi+1
zσn
0

if S = {σ1, . . . , σi}
if S = {σ1, . . . , σn}
otherwise.

Now let us restrict to the support of λ and do the change of variable

Fast Differentiable Sorting and Ranking

λS =

(cid:26) vi − vi+1
vn

if S = {σ1, . . . , σi}
if S = {σ1, . . . , σn}.

The non-negativity constraints in C become v1 ≥ v2 ≥ · · · ≥ vn and the equality constraints in C become zσ = v. Adding
quadratic regularization 1
2 (cid:107)y(cid:107)2 in the primal problem (13) is equivalent to relaxing the dual equality constraints in (14) by
smooth constraints 1
2 (cid:107)zσ − v(cid:107)2 (this can be seen by adding quadratic regularization to the primal variables of Bach (2013,
Eq. (3.6))). For the dual objective (14), we have

(cid:88)

S⊆V

λS F (S) =

n−1
(cid:88)

(vi − vi+1)F ({σ1, . . . , σi}) + vnF ({σ1, . . . , σn})

i=1
n
(cid:88)

=

(F ({σ1, . . . , σi}) − F ({σ1, . . . , σi−1}))vi

i=1
= (cid:104)fσ, v(cid:105),

where in the second line we used (Bach, 2013, Eq. (3.2)). Altogether, we obtain minv1≥···≥vn
is exactly the expression we derived in Lemma 6. The entropic case is similar.

1

2 (cid:107)zσ − v(cid:107)2 + (cid:104)fσ, v(cid:105), which

B.6. Pool adjacent violators (PAV) algorithm

Let g1, . . . , gn be convex functions. As shown in (Best et al., 2000; Lim & Wright, 2016),

argmin
v1≥···≥vn

n
(cid:88)

i=1

gi(vi)

can be solved using a generalization of the PAV algorithm (note that unlike these works, we use decreasing constraints for
convenience). All we need is a routine for solving, given some set B of indices, the “pooling” sub-problem

argmin
γ∈R

(cid:88)

i∈B

gi(γ).

Thus, we can use PAV to solve (12), as long as Ψ∗ is separable. We now give the closed-form solution for two special cases.
To simplify, we denote s := zσ and w := fσ.

Quadratic regularization. We have gi(vi) = 1

2 (si − vi)2 + viwi. We therefore minimize

The closed-form solution is

(cid:88)

i∈B

gi(γ) =

1
2

(cid:88)

i∈B

(si − γ)2 + γ

(cid:88)

i∈B

wi.

γ(cid:63)
Q(s, w; B) =

1
|B|

(cid:88)

(si − wi).

i∈B

Entropic regularization. We have gi(vi) = esi−vi + viewi. We therefore minimize

(cid:88)

i∈B

gi(γ) =

(cid:88)

i∈B

esi−γ + γ

ewi.

(cid:88)

i∈B

The closed-form solution is

where LSE(x) := log (cid:80)

i exi.

γ(cid:63)
E(s, w; B) = − log

(cid:80)
(cid:80)

i∈B wi
i∈B esi

= LSE(sB) − LSE(wB),

Although not explored in this work, other regularizations are potentially possible, see, e.g., (Blondel et al., 2019).

Fast Differentiable Sorting and Ranking

B.7. Proof of Proposition 4 (Jacobian of isotonic optimization)

Let B1, . . . , Bm be the partition of [n] induced by v := vΨ(s, w). From the PAV algorithm, for all i ∈ [n], there is a unique
block Bl ∈ {B1, . . . , Bm} such that i ∈ Bl and vi = γΨ(Bl; s, w). Therefore, for all i ∈ [n], we obtain

∂vi
∂sj

=

(cid:40) ∂γΨ(Bl;s,w)

∂sj
0

if i, j ∈ Bl
otherwise.

Therefore, the Jacobian matrix is block diagonal, i.e.,

∂v
∂s



BΨ
1

=




0
0




 .

0

0
. . .
0
0 BΨ
m

For the block Bl, the non-zero partial derivatives form a matrix BΨ
one sj and contains the value ∂γΨ(Bl;s,w)

∂sj

(all values in a column are the same). For quadratic regularization, we have

l ∈ R|Bl|×|Bl| such that each column is associated with

∂vi
∂sj

=

(cid:26) 1
|Bl|
0

if i, j ∈ Bl
otherwise.

For entropic regularization, we have

(cid:40)

esj
j(cid:48) ∈B es

(cid:80)

∂vi
∂sj

=

j(cid:48) = softmax(sBl )j

0

if i, j ∈ Bl
otherwise.

The multiplication with the Jacobian uses the fact that each block is constant column-wise.

Remark. The expression above is for points s where v is differentiable. For points where v is not differentiable, we can
take an arbitrary matrix in the set of Clarke’s generalized Jacobians, the convex hull of Jacobians of the form limst→s ∂v/∂st.
The points of non-differentiability occur when a block of the optimal solution can be split up into two blocks with equal
values. In that case, the two directional derivatives do not agree, but are derived for quadratic regularization by Djolonga &
Krause (2017).

