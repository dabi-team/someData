Struct-MMSB: Mixed Membership Stochastic
Blockmodels with Interpretable Structured Priors

Yue Zhang and Arti Ramesh 1

0
2
0
2

b
e
F
1
2

]

G
L
.
s
c
[

1
v
3
2
5
9
0
.
2
0
0
2
:
v
i
X
r
a

Abstract. The mixed membership stochastic blockmodel (MMSB)
is a popular framework for community detection and network genera-
tion. It learns a low-rank mixed membership representation for each
node across communities by exploiting the underlying graph structure.
MMSB assumes that the membership distributions of the nodes are
independently drawn from a Dirichlet distribution, which limits its
capability to model highly correlated graph structures that exist in
real-world networks. In this paper, we present a ï¬‚exible richly struc-
tured MMSB model, Struct-MMSB, that uses a recently developed
statistical relational learning model, hinge-loss Markov random ï¬elds
(HL-MRFs), as a structured prior to model complex dependencies
among node attributes, multi-relational links, and their relationship
with mixed-membership distributions. Our model is speciï¬ed using a
probabilistic programming templating language that uses weighted
ï¬rst-order logic rules, which enhances the modelâ€™s interpretability.
Further, our model is capable of learning latent characteristics in real-
world networks via meaningful latent variables encoded as a complex
combination of observed features and membership distributions. We
present an expectation-maximization based inference algorithm that
learns latent variables and parameters iteratively, a scalable stochastic
variation of the inference algorithm, and a method to learn the weights
of HL-MRF structured priors. We evaluate our model on six datasets
across three different types of networks and corresponding model-
ing scenarios and demonstrate that our models are able to achieve
an improvement of 15% on average in test log-likelihood and faster
convergence when compared to state-of-the-art network models.

1

INTRODUCTION

Modeling the complex and intricate interactions existing within a
community is an important network science problem that has gained
attention in the last decade. Perhaps one of the most commonly and
widely used network generation and community detection model is
mixed membership stochastic blockmodel (MMSB) [1], owing to
its ï¬‚exibility in modeling different kinds of networks and communi-
ties. MMSB models the nodeâ€™s membership in latent groups using a
mixed-membership distribution, wherein the node can be part of mul-
tiple latent groups. This membership is used to generate the network
structure.

Related Work MMSB has been adapted in various different ways
to model complex network structures, such as hierarchical commu-
nity structures [19, 33, 18, 41], dynamic networks [10, 15, 35, 20],
multi-relational graph structures [37, 23, 30, 7], nodes participating
in multiple communities [43, 36, 36, 9], and feature-rich networks

1 SUNY Binghamton, USA, email: {yzhan202, artir}@binghamton.edu

[42, 12, 38]. MMSB has also recently been extended to heteroge-
nous networks [21]. Other probabilistic blockmodels have also been
developed, such as inï¬nite relational model (IRM) [22], Bi-LDA
and Multi-LDA with extension to hierarchical Dirichlet processes
(Multi-HDP) [31], binary space partitioning-tree process (BSP-tree)
[13], a generalized version of the Mondrian process [32], and random
function priors [29].

Recent advances in deep learning produced more powerful models
for graph data, such as graph neural networks (GNNs) [39, 6, 25],
GraphRNN [40], graph attention networks (GAT) [34]. Bojchevski
et al. developed NetGAN [4], which uses the generative adversarial
networks (GAN) framework to generate random walks on graphs and
De Cao et al. [8] developed MolGAN, which generates molecular
graphs using the combination of a GAN framework and a reinforce-
ment learning objective. These models differ from MMSB and its
variants in that they are not hand-engineered but can learn from data.
But, the downside is the opaqueness and lack of interpretability of
these models [39], which Struct-MMSB seeks to address.

The above-mentioned blockmodels with the exception of a notable
few ones such as the recently developed Copula-MMSB [11] assume
that the membership indicator pairs are drawn independently; this
limits the capability of the model to encode complex dependencies
in the network structure. Even when the existing approaches relax
this independence assumption, their applicability is restricted to only
speciï¬c network structures (such as a-MMSB models assortative struc-
ture in networks) and are not solely versatile enough to handle the
various kinds of dependencies present in real-world data, such as
presence of additional features, multiple relationships, provision for
learning meaningful latent variables. Further, as the models progress
toward capturing complex dependencies, they are speciï¬ed using
complex functions that have limited interpretability. For example, the
Copula-MMSB model uses the Copula function [11], latent mixed-
membership groups (LMMG) uses the logistic function as priors [24],
and the BSP-tree uses a complex variant of the Mondrian process [13].
In addition to only capturing a speciï¬c kind of relationship, these pri-
ors are also harder to specify and understand for domain experts who
may have a limited knowledge of machine learning. Incorporating cor-
relations among the latent groups and memberships that can be easily
interpreted provides the models with modeling power and possibility
of application in domains that require careful speciï¬cation of domain
knowledge such as computational social science, bioinformatics, and
other evolving application areas for societal good.

Contributions
In this paper, we introduce a scalable and general
purpose MMSB, Struct-MMSB, by enhancing MMSB with a struc-
tured prior using a recently developed graphical model, hinge-loss
Markov random ï¬elds (HL-MRFs). Our approach, Struct-MMSB, is

 
 
 
 
 
 
Table 1: A comparison of general-purpose stochastic blockmodel frameworks

Model

Dependencies

Meaningful Latent Variables

Features

Scalability

IRM [22]
MMSB [1]
LMMG [24]
a-MMSB [17]
Copula-MMSB [11]
Struct-MMSB (our approach)

Ã—
Ã—
Ã—
Ã—
(cid:88)
(cid:88)

Ã—
Ã—
Ã—
Ã—
Ã—
(cid:88)

inspired from latent topic networks (LTN), a general-purpose latent
Dirichlet allocation (LDA) using structured priors [14]. Table 1 gives
a comparison of our model Struct-MMSB with other popular variants
of MMSB. Our model possesses the capability to encode: 1) the rela-
tional dependencies among membership distributions, 2) additional
node features, 3) multi-relational information, and their impact on the
membership distributions using a probabilistic programming templat-
ing language, thus remaining interpretable and easy to specify custom
network relationships. Further, our approach also incorporates the
ability to encode meaningful latent variables, which can be learned
as a complex combination of observed features, membership distribu-
tions, and group-group interaction probabilities, thus catering to many
latent-variable modeling scenarios in computational social science
problems.

Next, we present algorithms for inference and learning in Struct-
MMSB. We formulate an expectation maximization (EM) inference
for inferring the expected value of latent variables, mixed-membership
distributions of nodes in groups, and the group-group interaction
probabilities. Then, we develop a scalable inference method using
stochastic EM and show how to effectively incorporate relational
dependencies, while remaining scalable to large networks. We then
present a way to learn the weights of the ï¬rst-order logical rules by
maximizing the likelihood of the weights without additional ground
truth data, thus allowing the model to learn the predictive ability of
the logical rules in the data.

We demonstrate the versatility of our model in modeling different
network modeling scenarios on data from six real-world networks and
show that our model on average achieves a 15% better log-likelihood
and a better prediction performance than the state-of-the-art MMSB
variant, Copula-MMSB [11], IRM [22], and MMSB [1] and their
multi-relational and online variants.

2 BACKGROUND

In this section, we provide background on MMSB and HL-MRFs, and
then proceed to show how to incorporate HL-MRFs as a structured
prior in MMSB in Struct-MMSB. For ease of reference, we present a
list of notations and their meanings to be used in the equations and
derivations in this paper in Table 2.

2.1 Mixed Membership Stochastic Blockmodels

(MMSB)

The generative process for MMSB is deï¬ned as follows.

Ã—
Ã—
(cid:88)
Ã—
Ã—
(cid:88)

Ã—
(cid:88)
(cid:88)
(cid:88)
Ã—
(cid:88)

Table 2: Notations for STRUCT-MMSB

N
p, q
K
k, k1, k2
Î±, Î²(1), Î²(2)
Yp,q
zpâ†’q

Î 
Ï€p
Ï€(p)
k

B
Bk1,k2
Î›, Î»
Ïˆr
M (1)
M (2)
H (1), H (2)
X
Î·

number of nodes
speciï¬c nodes
number of communities
speciï¬c communities
hyperparameters of MMSB
link between nodes p and q
membership indicator for sender node p and receiver
node q
membership distributions of all the nodes
membership distribution of node p across communities

value in the membership distribution of node p for
group k
matrix capturing interactions between communities
interaction between communities k1 and k2
HL-MRF rule weights
HL-MRF potential function for rule r
number of HL-MRF potentials for Î 
number of HL-MRF potentials for B
latent variables in the HL-MRF prior
observed features
Lagrange coefï¬cient

â€“ Draw membership indicator for the receiver, (cid:126)zqâ†’p âˆ¼ Multino-

mial ( (cid:126)Ï€q)

â€“ Sample

the
their
pâ†’qB(cid:126)zqâ†’p), where B âˆ¼ Beta( (cid:126)Î²(1), (cid:126)Î²(2))
Bernoulli((cid:126)z(cid:62)

interaction,

value

of

Yp,q

âˆ¼

The independence assumptions implicit in the Dirichlet and Beta pri-
ors are what prevents MMSB from capturing complex dependencies.
The priors are therefore our point of attack in developing a rich, ï¬‚ex-
ible, and easy-to-encode stochastic blockmodel. In Struct-MMSB,
we replace the ï¬‚at Dirichlet and Beta priors with more expressive
HL-MRF priors, we discuss HL-MRFs and their suitability as priors
in MMSB below.

2.2 Hinge-loss Markov Random Fields

â€¢ For each node p âˆˆ N :

â€“ Draw a K dimensional mixed membership vector (cid:126)Ï€p âˆ¼ Dirichlet

((cid:126)Î±)

â€¢ For each pair of nodes (p, q) âˆˆ N Ã— N

â€“ Draw membership indicator for the initiator, (cid:126)zpâ†’q âˆ¼ Multino-

mial ((cid:126)Ï€p)

HL-MRFs are a recently developed scalable class of continuous,
conditional graphical models [3]. HL-MRFs can be speciï¬ed using
Probabilistic Soft Logic (PSL) [3], a ï¬rst-order logic templating lan-
guage. In PSL, random variables are represented as logical atoms
and weighted rules deï¬ne dependencies between them of the form:
Î» : P (a) âˆ§ Q(a, b) â†’ R(b), where P, Q, and R are predicates, a
and b are variables, and Î» is the weight associated with the rule. The
weight of the rule r indicates its importance in the HL-MRF model,

2

which is deï¬ned as

P (Y |X ) = exp

(cid:16)

âˆ’

M
(cid:88)

r=1

Î»rÏˆr(Y , X)

(cid:17)

/Z(Î»);

(1)

Z(Î») =

(cid:16)

âˆ’

exp

(cid:90)

Y

M
(cid:88)

r=1

Î»rÏˆr(Y , X )

(cid:17)

Ïˆr(Y , X ) = (max{lr(Y, X ), 0})Ïr

(2)

where P (Y |X ) is the probability density function of a subset
of logical atoms Y given observed logical atoms X, Ïˆr(Y , X ) is
a hinge-loss potential corresponding to an instantiation of a rule
r, and is speciï¬ed by a linear function lr and optional exponent
Ïr âˆˆ {1, 2}, and Z(Î») is the partition function. The logical conjunc-
tion of Boolean variables X âˆ§ Y can be generalized to continuous
variables using the hinge function max{X + Y âˆ’ 1, 0}, which is
known as the Lukasiewicz t-norm. HL-MRFs admit tractable MAP
inference regardless of the graph structure of the graphical model,
making it feasible to reason over complex dependencies. This is pos-
sible because HL-MRFs operate on continuous random variables and
encode dependencies using convex potential functions, so MAP in-
ference in these models is a convex optimization problem. Giannini
et al. [16] provide theoretical insight for understanding the convex
characterization of the Lukasiewicz t-norms and Horn clauses in PSL
[3].

Our resulting model, Struct-MMSB, upon replacing the priors with
HL-MRFs possess the following desirable qualities. They are struc-
tured and can capture complex relational dependencies among the
nodes, their features, their membership distributions and group-group
interactions using graphical model templates. These templates are
simple weighted rules that capture the general characteristics of the
network. The lucid nature of these rules make them inherently inter-
pretable and intuitive and hence pave the way for ease of speciï¬cation
by domain experts. Further, these priors can be enriched using mean-
ingful latent variables that further enhance their modeling power and
intuitiveness.

3 STRUCT-MMSB
Our goal in designing Struct-MMSB is to create a lucid, easy-to-
specify, and expressive generative model. We discuss the technical
details of Struct-MMSB below.

3.1 Struct-MMSB graphical model
To construct Struct-MMSB, we replace the Dirichlet priors and Beta
priors in the MMSB generative process with HL-MRF potential func-
tion Ïˆ. Figure 1 shows the plate diagram of Struct-MMSB. The
HL-MRF priors are indicated by ÏˆÏ€, which captures dependencies
between the membership distribution Ï€ of two nodes in the graph
and ÏˆB, which captures dependencies between groups in the group
interaction matrix B. Before delving into the mathematical details,
we provide some example dependencies that can be encoded using
Struct-MMSB:
Correlated nodes: correlated(p, q) âˆ§ Ï€(p, K) â†’ Ï€(q, K), if two
nodes p and q are correlated, then their corresponding membership
distributions (Ï€) are correlated.
Latent characteristics modeled as latent variables: link(p, r) âˆ§
link(q, r) â†’ similar(p, q), if two nodes p and q share common neigh-
bor r, then they are similar.
Inter-group and Intra-group links: correlated(p, q) âˆ§ Ï€(p, K1) âˆ§
B(K1, K2) â†’ Ï€(q, K2), if two nodes p and q are correlated and the
group interaction between K1 and K2 is high, then the value of the
membership distribution Ï€(p, K1) is similar to Ï€(q, K2).

Figure 1: Plate Diagram of Struct-MMSB

3.2 Combining MMSB with HL-MRF priors

In Struct-MMSB, the priors given by HL-MRFs deï¬ne probability
densities as follows:

P (Î , H (1)|X (1)) âˆ exp

M (1)
(cid:88)

(cid:18)

âˆ’

Î»(1)
j Ïˆ(1)

j

(Î , X (1), H (1))

P (B, H (2)|X (2)) âˆ exp

(cid:18)

âˆ’

j=1

M (2)
(cid:88)

j=1

Î»(2)
j Ïˆ(2)

j

(B, X (2), H (2))

(cid:19)

(cid:19)

j

where Ïˆ(a)
are hinge-loss potentials, M (a) are number of hinge-loss
potentials, and X (a) are observed features that are conditioned on,
and H (a) are additional latent variables introduced by Struct-MMSB,
and Î  and B are MMSB parameters.

The membership distributions Î  are constrained to sum to 1 for
each node as in MMSB. For membership distributions that do not
come from the HL-MRF priors, the default Dirichlet priors are used.
We also add smoothed Dirichlet and Beta potentials for Î , B, and 1 âˆ’
(1 âˆ’ Bk1,k2 )Î²(2)âˆ’1,
B, (cid:81)
respectively. Incorporating the HL-MRF structured priors together
with the smoothed Dirichlet and Beta potentials, the log-posterior of
the variables of interest (parameters Î  and B, latent variables H (1)
and H (1)) is,

p,k Ï€pÎ±âˆ’1

BÎ²(1)âˆ’1
k1,k2

, and (cid:81)

, (cid:81)

k1,k2

k1,k2

k

logP (Î , B, H (1), H (2)|Y, Î±, Î²(1), Î²(2), X (1), X (2), Î›) =

P (Yp,q, zp, zq|Ï€p, Ï€q, B)

(cid:19)

(cid:88)

log

(cid:18) (cid:88)

pâˆˆN,qâˆˆN
(cid:88)

+

zpâˆˆK,zq âˆˆK
(Î± âˆ’ 1)logÏ€p
k

pâˆˆN,kâˆˆK

âˆ’

+

âˆ’

M (1)
(cid:88)

j Ïˆ(1)
Î»(1)

j

(Î , X (1), H (1))

j=1

(cid:88)

(cid:18)

(Î²(1) âˆ’ 1)logBk1,k2 + (Î²(2) âˆ’ 1)log(1 âˆ’ Bk1,k2 )

(cid:19)

k1,k2

M (2)
(cid:88)

j=1

j Ïˆ(2)
Î»(2)

j

(B, X (2), H (2)) + const.

(3)

3.3 Training via EM

Our EM algorithm is motivated from latent variable learning in HL-
MRFs [2] and LTN [14]. We train the model by maximum a posteriori
(MAP) estimation, optimizing Eqn 3 with respect to Î , B, H (1),

3

 ğœ‹P ğœ‹qZp->qğ›¼ğœ“ğœ‹Yp,qBğœ“Bğ›½(1)ğ›½(2)Zq->pH (2). Since Eqn 3 cannot be optimized directly due to the sum inside
the logarithm, we develop an EM algorithm that iteratively optimizes
a lower bound arising from Jensenâ€™s inequality.

Applying

inequality, we

get

the

function
logP (Î , B, H|Y, Î±, Î²(1), Î²(2), X, Î›), where,

to

R(Î , B, X (1), X (2), H (1), H (2))

Jensenâ€™s
be,

objective
â‰¤

R(Î , B, X (1), X (2), H (1), H (2)) = âˆ’

M (1)
(cid:88)

j=1

Î»(1)
j Ïˆ(1)

j

(Î , X (1), H (1))

âˆ’

+

+

+

âˆ’

M (2)
(cid:88)

j=1

Î»(2)
j Ïˆ(2)

j

(B, X (2), H (2))

(cid:88)

(cid:18) (cid:88)

(Î³p,q,k1,k2 + Î³q,p,k2,k1 ) + Î± âˆ’ 1

(cid:19)

logÏ€p
k1

p,k1

(cid:88)

q,k2
(cid:18) (cid:88)

k1,k2

p,q

(cid:88)

(cid:18) (cid:88)

Î³p,q,k1,k2 Yp,q + Î²(1) âˆ’ 1

(cid:19)

logBk1,k2

Î³p,q,k1,k2 (1 âˆ’ Yp,q) + Î²(2) âˆ’ 1

(cid:19)

log(1 âˆ’ Bk1,k2 )

k1,k2
(cid:88)

p,q,k1,k2

p,q

Î³p,q,k1,k2 logÎ³p,q,k1,k2 + const

(4)

(cid:44) P ((cid:126)zpâ†’q = k1, (cid:126)zqâ†’p = k2|Î (t), B(t), Yp,q),
We deï¬ne Î³p,q,k1,k2
which encodes the distribution over the membership indicators (cid:126)zpâ†’q
and (cid:126)zqâ†’p based on the previous parameter values of Î  and B. The
algorithm consists of an E-step and an M-step, which are iterated until
convergence. We ï¬rst present the batch version of the algorithm, that
iterates on all the data points for each E and M-step update and then
present a stochastic variation that is scalable to large datasets.

3.3.1 E-step

To perform the E-step of the EM algorithm at iteration t, we ï¬rst
compute Î³p,q,k1,k2 in Equation 4.

Î³p,q,k1,k2 âˆ P (Yp,q|zp, zq, Î (t), B(t))P (zp, zq|Î (t), B(t))

= BYp,q
k1,k2

(1 âˆ’ Bk1,k2 )1âˆ’Yp,q Ï€(p)
k1

Ï€(q)
k2

Since the inference algorithm is iterative, the superscript (t) is used to
refer to the previous/initialized values of Î  and B.

3.3.2 M-step

We then perform the maximization step for Î  and B parameters
that are not involved in the HL-MRF prior, for which the update is
identical to the M-step of the EM algorithm for standard MMSB. For
Î , we add Lagrange terms âˆ’ (cid:80)
k ((cid:80)
k Ï€p
p Î·Î 
k âˆ’ 1) to constrain the
parameter vectors to sum to one, where Î·Î 
k is the Lagrange coefï¬cient.
To derive updates for B, we deï¬ne B and 1 âˆ’ B as B(i), i âˆˆ {0, 1},
k1k2 ((cid:80)
respectively, and add the Lagrange term âˆ’ (cid:80)
Î·B
âˆ’
1) to constrain the B(i)
k1k2 is the Lagrange
coefï¬cient. Taking derivatives and setting them to zero, we obtain the
updates,

k1k2
to sum to one, where Î·B

i B(i)
k1k2

k1k2

(cid:88)

Ï€p
k1

âˆ

(Î³p,q,k1,k2 + Î³q,p,k2,k1 ) + Î± âˆ’ 1;

q,k2
(cid:80)

B(0)

k1,k2

=

(cid:80)

p,q Î³p,q,k1,k2 Yp,q + Î²(1) âˆ’ 1
p,q Î³p,q,k1,k2 + Î²(1) + Î²(2) âˆ’ 2

Then, we optimize the lower bound jointly over the remaining Î 
and B parameters by ï¬xing the parameters above that are not up-
dated using HL-MRF priors. The hinge-loss potentials for Î  and B
can be optimized separately as evident from Eqn 4. We minimize
the negative of each of these two subproblems âˆ’R(Î , H (1), X (1)),
âˆ’R(B, H (2), X (2)) using a consensus-optimization algorithm based
on the alternating direction method of multipliers (ADMM).

To accomplish this, we extend the consensus optimization algo-
rithm by Bach et al. [3] for MAP inference in HL-MRFs. The algo-
rithm creates a local copy for each of the variables, thus dividing the
original problem into independent subproblems that may be solved in
parallel. The algorithm iteratively solves the independent subproblems
and then updates the global consensus variables to be the average of
the local copies. This is guaranteed to ï¬nd the global optimum solu-
tion of the objective function. For more details, we refer the reader to
[3] and [5].

p,q,k1,k2

We extend this algorithm to include the objective function
terms in the lower bound R in Eqn 4 for Î  and B. For Î ,
the terms 1 and 3 in Eqn 4 are included and for B, the terms
2, 4, and 5 in Eqn 4 are included. The entropy term (term 6,
âˆ’ (cid:80)
Î³p,q,k1,k2 logÎ³p,q,k1,k2 ) and the constant term (term
7) are not included in the M-step objective as they do not have Î 
or B in them. To cast this as a consensus optimization problem, we
create local copies of our variables of interest (Î  and B, denoted by
Ci in the ADMM update equation below), which we refer to as ci,
where i refers to each Ï€p
k and Bk1,k2 , in the respective consensus
optimization equations. The consensus optimization problem entails
solving the subproblems with local copies ci independently for both
the variables and then iterating till a consensus is reached, which is
enforced via an equality constraint in the Lagrange term with coefï¬-
cient Î·i. For convenience, we refer to our objective terms as Ailogci,
where Ai denotes the coefï¬cient in term 3 in Equation 4, which equals
to ((cid:80)
(Î³p,q,k1,k2 + Î³q,p,k2,k1 ) + Î± âˆ’ 1), and ci denotes the local
copy of the variable.

q,k2

The consensus ADMM update [5] for ci is given by,

ci = arg min

(âˆ’Ailogc(cid:48)

i + Î·i(c(cid:48)

i âˆ’ Ci) +

c(cid:48)
i

Ï
2

(c(cid:48)

i âˆ’ Ci)2)

where Ï is an ADMM step-size parameter, ci is the local copy of the
variable used by the ADMM consensus optimization algorithm, and
Ci refers to the original variable. In order to solve the ADMM update
equation, we take its derivative with respect to ci and set it equal to
zero, âˆ‚J
i(Î·i âˆ’ ÏCi) âˆ’ Ai = 0.
âˆ‚c(cid:48)
i

i + c(cid:48)

= Ïc(cid:48)2

Solving this quadratic equation, we get two solutions. One of them
is negative and can be discarded; ci is set to the positive solution.
The Lagrange coefï¬cients are updated using ADMM updates for
Lagrangian dual, Î·i â† Î·i + Ï(c(cid:48)

i âˆ’ Ci).

3.4 Stochastic EM

To scale the batch model to large networks, we present a stochastic
EM inference method. Our algorithm is motivated from the online
EM algorithm [28] and the stochastic optimization for a-MMSB [17].
We compute the expected sufï¬cient statistics of Î  and B and update
it using randomly sampled node-pair (p, q) from distribution g(p, q)
instead of the entire sample, by computing the stochastic natural
gradient in the space of sufï¬cient statistics.

To obtain the stochastic EM algorithm, we ï¬rst subsample the graph
and compute the expected sufï¬cient statistics for the sample, given the
current settings of the Î  and B. We then update Î  and B using these
sufï¬cient statistics. We introduce three global variables: Î¸p
k, Ï†k1k2 ,

4

k is equal to the normalized Î¸p

and Ï†(cid:48)
distribution Ï€p
B(0)
k1k2
B(1)
k1k2
To derive the expected sufï¬cient statistics of Ï€p

k1k2 to derive the stochastic EM updates. The membership
k. The afï¬nity blockmodel
k1k2 ), and
k1k2 ).

(which refers to B) is given by Ï†k1k2 /(Ï†k1k2 + Ï†(cid:48)
k1k2 /(Ï†k1k2 + Ï†(cid:48)
(which refers to 1 âˆ’ B) equals Ï†(cid:48)

k for a speciï¬c node

p and community k1, we have,
(cid:18) (cid:88)

(Î³p,q,k1,k2 + Î³q,p,k2,k1 ) + Î± âˆ’ 1

(cid:19)

logÏ€p
k1

q,k2
(cid:18) (cid:88)

=

q,k2

(cid:18)

=

Eg[

g(p, q)

1
g(p, q)

(Î³p,q,k1,k2 + Î³q,p,k2,k1 ) + Î± âˆ’ 1

(cid:19)

logÏ€p
k1

1
g(p, q)

(Î³p,q,k1,k2 + Î³q,p,k2,k1 )] + Î± âˆ’ 1

(cid:19)

logÏ€p
k1

The intuition behind the above expected sufï¬cient statistics comes
from online EM for unsupervised models [28] and stochastic varia-
tional EM for a-MMSB [17]. Here, we sample a single node pair (p,
q) and compute the Î³ if our entire graph consisted of p and q, repeated
1/g(p, q) times. In practice, instead of a single node a mini-batch
is sampled. Liang et al. [28] specify that updating on multiple data
instances using a mini-batch adds more stability than updating after
each sample. This translates to all q in the mini-batch for a speciï¬c
node of interest p.

(cid:80)

, sp,k1 = 1
g

Hence, under each mini-batch, without HL-MRF priors, the ex-
pected sufï¬cient statistics of Î¸p
q,k2(Î³p,q,k1,k2 +
k1
Î³q,p,k2,k1 ) + Î± âˆ’ 1. With HL-MRF priors, we ï¬rst use ADMM
to ï¬nd local optimal value Ï€âˆ—p
under the current mini-batch, then the
k1
p,k1 = Ï€âˆ—p
expected sufï¬cient statistics is calculated as sP SL
sp,k1 .
k1
Similarly, we calculate the expected sufï¬cient statistics for B. As
Bk1,k2 and 1 âˆ’ Bk1,k2 factorize into separate terms in Eqn 4, we
treat them as two different variables and calculate the expected sufï¬-
cient statistics separately. For Bk1,k2 , we have,

(cid:80)

k1

(cid:88)

p,q

Î³p,q,k1,k2 Yp,qlogBk1,k2

(cid:88)

=

g(p, q)

p,q

(cid:18) 1

g(p, q)

(cid:19)

Î³p,q,k1,k2 Yp,q

logBk1,k2

= Eg[

1
g(p, q)

Î³p,q,k1,k2 Yp,q]logBk1,k2

Under mini-batch, without HL-MRF priors, expected sufï¬cient statis-
p,q Î³pqk1k2 Yp,q + Î²(1) âˆ’ 1.
tics of Ï†k1,k2 equals to, sk1k2 = 1
g
Similarly, for B(cid:48)

k1,k2 = 1 âˆ’ Bk1,k2 , we have,

(cid:80)

(cid:88)

p,q

Î³p,q,k1,k2 (1 âˆ’ Yp,q)log(B(cid:48)

k1,k2 )

(cid:88)

=

g(p, q)

p,q

(cid:18) 1

g(p, q)

Î³p,q,k1,k2 (1 âˆ’ Yp,q)

(cid:19)

log(B(cid:48)

k1,k2 )

= Eg[

1
g(p, q)

Î³p,q,k1,k2 (1 âˆ’ Yp,q)]log(B(cid:48)

k1,k2 )

g

k1k2 = 1

k1,k2 equals to s(cid:48)

Under mini-batch, without HL-MRF priors, expected sufï¬cient statis-
(cid:80)
tics of Ï†(cid:48)
p,q Î³p,q,k1,k2 (1âˆ’Yp,q)+Î²(2)âˆ’
1. With HL-MRF priors, to compute the expected sufï¬cient statistics
for Ï†k1k2 and Ï†(cid:48)
k1k2 , ï¬rst we need to use ADMM to ï¬nd local optimal
value Bâˆ—
k1k2 under current mini-batch. Then, the expected sufï¬cient
k1k2 (sk1k2 + s(cid:48)
k1k2 = Bâˆ—
statistics for Ï†k1k2 becomes sP SL
k1k2 ) and for
Ï†(cid:48)
k1k2 , s(cid:48)P SL
k1k2 = (1 âˆ’ Bâˆ—
k1k2 )(sk1k2 + s(cid:48)
k1k2 ). Intuitively speaking,
the expected sufï¬cient statistics with HL-MRF priors re-assign the

total mass without HL-MRF priors, (cid:80) s, and make it proportional
to the optimal value using the estimated Ï€âˆ—p
k1k2 values from
k1
ADMM.

and Bâˆ—

In our experiments, we use mini-batch instead of just one node pair.
Evaluating the rewritten above equations for a node pair sampled from
the graph gives a noisy but unbiased estimate of the batch model. We
scale the expected sufï¬cient statistics estimated from the subsample
by
g(p,q) so that they are unbiased estimates of the true sufï¬cient
statistics as illustrated by Gopalan et al. [17]. For instance, if we
sample nodes uniformly at random, then g(p, q) = miniBatch/N 2.
We calculate this as the difference between expected sufï¬cient

1

statistics and previous value as follows

âˆ‚Î¸t
pk = sP SL
k1k2 = sP SL
k1k2 = s(cid:48)P SL

pk âˆ’ Î¸tâˆ’1
pk
k1k2 âˆ’ Ï†tâˆ’1
k1k2
k1k2 âˆ’ Ï†(cid:48)tâˆ’1
k1k2

âˆ‚Ï†t
âˆ‚Ï†(cid:48)t

(5)

In the next step, we calculate the stochastic natural gradients computed
from a sampled node pair (p, q) or mini-batch and update the global
variables,

k â† Î¸p
Î¸p

k + Ït âˆ— âˆ‚Î¸t

pk

Ï†k1k2 â† Ï†k1k2 + Ït âˆ— âˆ‚Ï†t
k1k2 + Ït âˆ— âˆ‚Ï†(cid:48)t
k1k2 â† Ï†(cid:48)
Ï†(cid:48)

k1k2

k1k2

(6)

where Ï is global step size. Results from the stochastic approximation
literature requires that (cid:80)
t Ït = âˆ to guarantee
convergence to a local optimum. We set Ït (cid:44) (Ï„0 + t)âˆ’Îº, where
Îº âˆˆ (0.5, 1] is the learning rate and Ï„0 â‰¥ 0 downweights early
iterations.

t < âˆ and (cid:80)

t Ï2

Distributed Implementation We implement a distributed comput-
ing environment using Apache Thrift framework for the stochastic EM
inference. This further helps in scaling our models to larger datasets.
We perform stochastic EM inference at each client side using sub-
sampled mini-batch and then update the global parameters at server
side.

3.5 Weight learning

The weights of the HL-MRF ï¬rst-order-logic rules, Î›, can be selected
based on domain knowledge. The challenge to learning weights arises
because often there is no ground truth data for our variables of interest,
Î  and B. Following Bach et al. [3], we present a maximum-likelihood
estimation by maximizing the log-likelihood of the training data given
parameter values to learn the weights.

âˆ‚R
âˆ‚Î›q

=

âˆ‚logP (Î , H|X)
âˆ‚Î›q

= EÎ›[Î¨q(X, H, Î )] âˆ’ Î¨q(X, H, Î )

EÎ›[Î¨q(X, H, Î )] =

(cid:90)

Î 

Î¨q(X, H, Î )P (Î |X, H)

We approximate the expectation using the value of the potential
functions at the most probable setting of Î  with the current parameters.
During weight learning, we treat latent variables H as observations
using the inferred value from the EM step.

4 EXPERIMENTS

We conduct experiments to answer the questions:

5

1) How our models perform on different network modeling scenarios?
To demonstrate the versatility of Struct-MMSB and its ability to
model different real-world scenarios, we evaluate the performance
of our model on a total of 6 datasets and three network modeling
scenarios: a) presence of additional features (case 1), b) presence
of multiple links (case 2), and c) neighborhood similarity (case 3),
[27]. We induce three different structured priors that correspond to
the characteristics of each of these modeling scenarios. We compare
the training and test log-likelihood and area under the ROC curve
(AUC-ROC) of our models at convergence with MMSB [1] or its
online/stochastic variant, IRM [22], and the state-of-the-art variant of
MMSB (Copula-MMSB [11]), which has been compared to some
other variants of MMSB and network models (MMSB, IRM, LFRM
[30], and iMMM [26]) and shown to be better. Since Copula-MMSB
in the present form is not capable of handling multi-relational data,
we extend Copula-MMSB to handle multi-relational data wherever
appropriate; we refer to this model as Multi-Copula. We show that
our model achieves better performance and faster convergence
both under batch and stochastic inference when compared to the
above-mentioned models.

2) How informative are the latent variables in our intuitive HL-MRF
priors? While performance is one important aspect, the more inter-
esting contribution of our approach is the interpretable and intuitive
nature of Struct-MMSB models, succinctly capturing domain knowl-
edge in the different modeling scenarios. This interpretable nature is
further enhanced by the presence of meaningful latent variables that
can abstract the relationship between the features, nodes, and network
connections. We show how to encode meaningful latent variables
in all the three modeling scenarios: presence of additional features,
presence of multiple links, and neighborhood similarity. We present
qualitative analysis of the values learned by our latent variables and
show that our models are able to encode and learn meaningful latent
variable values that enhance the modeling power and interpretability
of our model.

4.1 Experimental settings

In all our experiments, our models use same initializations as stan-
dard MMSB. In the stochastic inference setting, our models also
use the same mini-batches as online MMSB, so that they are ex-
actly comparable. We initialize the hyper-parameters Î²(1) and Î²(2)
of parameter B by setting Î²(1)/(Î²(1) + Î²(2)) =link-rate, where
link-rate= link/(link + non-link). We set number of communities
to 5 for small datasets (feature-based similarity (case 1) and multi-
relational experiments (case 2)), and to 8 for the large datasets in
link-based similarity (case 3) experiment. And, we randomly initialize
the Î  and B values. Statistically signiï¬cant values with a rejection
threshold of p = 0.05 are typed in bold.

4.2 Case 1: feature-based similarity

Here, we design models that take additional node features into account
to guide community discovery and network generation.

together. The ï¬rst rule in Table 3 means if node p and node q have
the same features (multiple instantiations of Rule 1), then we infer
that node p and node q are similar. similarity(p,q) is a latent variable
that helps us model the degree of similarity between two nodes. The
second rule captures that if two nodes are similar, then we can infer
that they have similar membership distributions.

Table 3: HL-MRF priors to guide community discovery based on
presence of multiple common features measured using latent variable
similarity(p, q)

Feature-Based Similarity Model
feature(p, T) âˆ§ feature(q, T) âˆ§ link(p, q) â†’ similarity(p, q)
similarity(p, q) âˆ§ Ï€(p, K) â†’ Ï€(q, K)
similarity(p, q) âˆ§ Â¬Ï€(p, K) â†’ Â¬Ï€(q, K)

4.2.2 Results

We evaluate this model on two Facebook Ego datasets: 1) Ego-414
dataset containing 159 nodes and 3386 links, and 2) Facebook Ego-
686 dataset containing 170 nodes and 3312 links. Table 4 shows that
our model achieves a better training and test log-likelihood and AUC
on two Facebook ego datasets when compared with standard MMSB,
Copula-MMSB, and IRM.

Our latent variables, apart from providing the model with model-
ing power, also bring interpretability to the model. Figures 2(a) and
2(b) illustrate the correlation between latent variable similarity and
the number of common features and membership distributions. This
conforms with our structured prior that a commonality in features
between a pair of nodes can indicate that they belong to the same
communities. We also observe that our latent variable values act as
a proxy to the relationship between similarity in features and their
membership distributions and helps us interpret them better. For ex-
ample, in Ego-414 dataset, we get the value for the latent variable
similarity for nodes 74 and 88 to be 0.714 and we observe that they
both have similar membership distributions after training, having a
high value for the same community.

Table 4: Results on Facebook Ego-414 and Ego-686 datasets com-
paring Struct-MMSB with Copula-MMSB, Standard MMSB, and
IRM.

Dataset Model

Log-
Likelihood

Test Log-
Likelihood

Ego414

Ego686

Struct-MMSB
Copula-MMSB
MMSB
IRM

Struct-MMSB
Copula-MMSB
MMSB
IRM

-1042.986
-1909.378
-1309.271
-2205.484

-1628.858
-2002.216
-1690.593
-2840.591

-787.406
-1404.075
-986.321
-1521.684

-1169.224
-1407.204
-1221.328
-1866.839

AUC

0.954
0.844
0.9220
0.746

0.892
0.854
0.875
0.659

4.2.1 Model

In this modeling scenario, we specify HL-MRF priors to utilize feature
commonality between nodes to model their community membership.
The priors in Table 3 capture that if two nodes have many common
features, then there is a greater chance of these nodes being grouped

4.3 Case 2: multi-relational graphs

In our next experiment, we consider a multi-relational graph set-
ting, where there are multiple different relationships between a sin-
gle pair of nodes. Here, we experiment on two different kinds of
multi-relational data: i) the presence of multiple different types of
relationships between a pair of nodes, and ii) the presence of the same

6

(a) Correlation between similarity
value and number of common fea-
tures

(b) Correlation between similarity
value and community membership
distributions

Figure 2: Correlation between latent variable values and membership
distributions

type of relationship between a pair of nodes at different timestamps,
each timestamp capturing a single relationship.

4.3.1 Model

To guide the generation of the model in the multi-relational setting,
our structured HL-MRF prior captures that if a pair of two nodes p and
q have multiple links between them (Rule 1 in Table 5), then, they are
â€œcloserâ€ than other pairs of nodes. This closeness in their relationship
is modeled using the latent variable close(p, q). Rule 2 in Table 5
captures that if two nodes are closer (i.e., have a higher value of
latent variable close(p, q)), then there is a higher probability of a link
existing between them in the network. Note that in Rule 2, we connect
the blockmodel B that captures the interaction between communities,
membership distributions of nodes p and q with the latent variable
close induced by the structured prior to guide the generation process.

Table 6: Comparing Struct-MMSB with Standard and Multi-Copula
MMSB on UMLS and Email-Temporal datasets.

Dataset Model

UMLS

Email

Struct-MMSB
Multi-Copula
MMSB

Struct-MMSB
Multi-Copula
MMSB

Log-
Likelihood

-11868.051
-12242.841
-12061.241

-1523.488
-1777.112
-1562.353

Test Log-
Likelihood

-2945.953
-3153.354
-2998.037

-175.871
-210.090
-180.620

AUC

0.831
0.785
0.814

0.997
0.995
0.995

(a) Progression toward convergence
of batch Struct-MMSB in UMLS
dataset

(b) Progression toward convergence
of stochastic Struct-MMSB in Face-
book Ego107

Figure 3: Struct-MMSB progresses faster toward convergence in both
batch and stochastic scenarios

4.4 Case 3: community discovery using

neighborhood similarity

Here, we consider the problem of discovering communities in graphs
using neighborhood similarity. Note that this is the setting for which
MMSB was originally conceived. In this experiment, we evaluate the
performance of the stochastic Struct-MMSB with the online variant
of MMSB on performance and the ability to converge faster.

Table 5: Capturing proximity between nodes based on the presence of
multiple links between them using latent variable close(p, q)

4.4.1 Model

Node Proximity in Multi-Relational Graphs
link(p, q, T) â†’ close(p, q)
close(p, q) âˆ§ B(K1,K2) âˆ§ Ï€(p, K1) â†’ Ï€(q, K2)

4.3.2 Results

We evaluate the performance of our model on two datasets: 1) Uniï¬ed
Medical Language System dataset (UMLS) consisting of 135 nodes,
49 relations, and a total of 6752 links, and 2) Email-Temporal dataset
consisting of 142 nodes, 525 timestamps, and a total of 48, 141 links.
Table 6 shows the comparison of the performance scores of our model
with standard MMSB and Multi-Copula (Copula extended to the multi-
relational case to enable a fair comparison) at training and testing time
on UMLS and Email-Temporal datasets, where our model achieves
better log-likelihood scores on training and test.

Also, as the priors are expected to guide the model in the right
direction in the beginning, a better measure of the effectiveness of the
structured priors is evaluating the progression toward convergence.
Figure 3(a) shows the progression of the models toward convergence.
We observe that Struct-MMSB progresses faster toward convergence
and gets a higher AUC value right from the early iterations.

The mixed membership Ï€ is a low rank representation of nodeâ€™s
neighborhood. We guide Ï€ by capturing that if two nodes have mul-
tiple common neighbors (Rule 1 in Table 7), then they have similar
membership distributions (Rules 2 and 3 in Table 7). We capture the
neighborhood similarity using the latent variable similarity(p, q).

Table 7: HL-MRF priors capturing neighborhood similarity between
nodes based on the presence of multiple neighbors using latent vari-
able similarity(p, q)

Community Discovery using Neighborhood Similarity
link(p, r) âˆ§ link(q, r) â†’ similarity(p, q)
similarity(p, q) âˆ§ Ï€(p, K) â†’ Ï€(q, K)
similarity(p, q) âˆ§ Â¬Ï€(p, K) â†’ Â¬Ï€(q, K)

4.4.2 Results
We evaluate performance on the Facebook Ego-107 dataset, which
contains 1045 nodes and 53498 links and Email-Eu-core dataset,
which contains 1005 nodes and 25571 links. We implement the
stochastic optimization for both Struct-MMSB and standard MMSB
and compare our stochastic Struct-MMSB with stochastic/online
MMSB. For each mini-batch iteration, we randomly sample 10%

7

0.20.30.40.50.60.7similarity value24681012# common features0.20.40.60.8similarity value00.20.40.60.81% node-pairs in same community0501001502000.60.650.70.750.80.85Struct-MMSBMMSBMulti-Copula010k20k30k40k0.60.70.80.9Struct-MMSBOnline-MMSBTable 8: Results on Ego-107 and Email-Eu-Core datasets comparing
stochastic Struct-MMSB with online MMSB

Dataset Model

Ego107

Email

Struct-MMSB
Online-MMSB

Struct-MMSB
Online-MMSB

Log-
Likelihood

-45202.096
-48534.366

-31425.953
-31562.378

Test Log-
Likelihood

-11441.823
-12262.921

-8059.303
-8133.698

AUC

0.922
0.900

0.862
0.861

of the nodes from all the data. We add all links that exist between
the sampled nodes to the mini-batch. In total, we run 48,000 mini-
batch iterations. Our stochastic model achieves better log-likelihood
at training and test time when compared to online-MMSB as evident
from Table 8. We also observe a similar convergence trend in the
stochastic model as well (Figure 3(b)), where our model progresses
faster toward convergence even while only observing a small subset
of all the nodes and relationships during each iteration.

5 Conclusion

We presented a versatile general-purpose MMSB, Struct-MMSB, that
is capable of encoding multi-relational data, additional features, and
dependencies among the membership distributions, community inter-
action matrix, and learn meaningful latent variables in the process. We
present a batch inference algorithm using EM and a scalable variant
using stochastic EM and a method to learn the weights of the struc-
tured HL-MRF priors. Our experimental evaluation demonstrates the
ability of our model to achieve a superior log-likelihood on training
and held-out test data and faster convergence on three different mod-
eling scenarios across 6 datasets and the interpretable nature of our
learned latent variables.

REFERENCES

[1] Edoardo M Airoldi, David M Blei, Stephen E Fienberg, and Eric P Xing,
â€˜Mixed membership stochastic blockmodelsâ€™, JMLR, 9(Sep), 1981â€“2014,
(2008).

[2] Stephen Bach, Bert Huang, Jordan Boyd-Graber, and Lise Getoor,
â€˜Paired-dual learning for fast training of latent variable hinge-loss mrfsâ€™,
in ICML, pp. 381â€“390, (2015).

[3] Stephen H Bach, Matthias Broecheler, Bert Huang, and Lise Getoor,
â€˜Hinge-loss markov random ï¬elds and probabilistic soft logicâ€™, JMLR,
18(109), 1â€“67, (2017).

[4] Aleksandar Bojchevski, Oleksandr Shchur, Daniel Zugner, and Stephan
Gunnemann, â€˜Netgan: Generating graphs via random walksâ€™, in ICML,
(2018).

[5] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein,
et al., â€˜Distributed optimization and statistical learning via the alternating
direction method of multipliersâ€™, Foundations and Trends R(cid:13) in Machine
learning, 3, 1â€“122, (2011).
Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann Lecun, â€˜Spec-
tral networks and locally connected networks on graphsâ€™, in ICLR,
(2014).

[6]

[7] Bei Chen, Ning Chen, Jun Zhu, Jiaming Song, and Bo Zhang, â€˜Dis-
criminative nonparametric latent feature relational models with data
augmentation.â€™, in AAAI, (2016).

[8] Nicola De Cao and Thomas Kipf, â€˜Molgan: An implicit generative model
for small molecular graphsâ€™, arXiv preprint arXiv:1805.11973, (2018).
Ismail El-Helw, Rutger Hofman, Wenzhe Li, Sungjin Ahn, Max Welling,
and Henri Bal, â€˜Scalable overlapping community detectionâ€™, in Parallel
and Distributed Processing Symposium Workshops, (2016).

[9]

[10] Xuhui Fan, Longbing Cao, and Richard Yi Da Xu, â€˜Dynamic inï¬nite
mixed-membership stochastic blockmodelâ€™, IEEE transactions on neural
networks and learning systems, 26, 2072â€“2085, (2015).

[11] Xuhui Fan, Richard Yi Da Xu, and Longbing Cao, â€˜Copula mixed-

membership stochastic blockmodel.â€™, in IJCAI, (2016).

[12] Xuhui Fan, Richard Yi Da Xu, Longbing Cao, and Yin Song, â€˜Learning
nonparametric relational models by conjugately incorporating node
information in a networkâ€™, IEEE transactions on cybernetics, 47, 589â€“
599, (2017).

[13] Xuhui Fan, Bin Li, and Scott Sisson, â€˜The binary space partitioning-tree

[14]

processâ€™, in AISTATS, (2018).
James Foulds, Shachi Kumar, and Lise Getoor, â€˜Latent topic networks:
A versatile probabilistic programming framework for topic modelsâ€™, in
ICML, (2015).

[15] Wenjie Fu, Le Song, and Eric P Xing, â€˜Dynamic mixed membership

blockmodel for evolving networksâ€™, in ICML, (2009).

[16] Francesco Giannini, Michelangelo Diligenti, Marco Gori, and Marco
Maggini, â€˜Characterization of the convex Å‚ukasiewicz fragment for learn-
ing from constraintsâ€™, in AAAI, (2018).

[17] Prem K Gopalan, Sean Gerrish, Michael Freedman, David M Blei, and
David M Mimno, â€˜Scalable inference of overlapping communitiesâ€™, in
NeurIPS, (2012).

[18] Qirong Ho, Ankur Parikh, Le Song, and Eric Xing, â€˜Multiscale commu-
nity blockmodel for network explorationâ€™, in AISTATS, (2011).
[19] Qirong Ho, Ankur P Parikh, Le Song, and Eric P Xing, â€˜Inï¬nite hierar-
chical mmsb model for nested communities/groups in social networksâ€™,
Statistics, 1050, 9, (2010).

[20] Qirong Ho, Le Song, and Eric Xing, â€˜Evolving cluster mixed-
membership blockmodel for time-evolving networksâ€™, in AISTATS,
(2011).

[21] Weihong Huang, Yan Liu, and Yuguo Chen. Mixed membership stochas-

tic blockmodels for heterogeneous networks, 2018.

[22] Charles Kemp, Joshua B Tenenbaum, Thomas L Grifï¬ths, Takeshi Ya-
mada, and Naonori Ueda, â€˜Learning systems of concepts with an inï¬nite
relational modelâ€™, in AAAI, (2006).

[23] Dae Il Kim, Michael Hughes, and Erik Sudderth, â€˜The nonparametric

metadata dependent relational modelâ€™, in ICML, (2012).

[24] Myunghwan Kim and Jure Leskovec, â€˜Latent multi-group membership

graph modelâ€™, in ICML, (2012).

[25] Thomas N Kipf and Max Welling, â€˜Semi-supervised classiï¬cation with

graph convolutional networksâ€™, in ICLR, (2017).

[26] Phaedon-Stelios Koutsourelakis and Tina Eliassi-Rad, â€˜Finding mixed-
memberships in social networks.â€™, in AAAI Spring Symposium: Social
Information Processing, (2008).
Jure Leskovec and Rok SosiË‡c, â€˜Snap: A general-purpose network analy-
sis and graph-mining libraryâ€™, ACM Transactions on Intelligent Systems
and Technology (TIST), 8(1), 1, (2016).

[27]

[28] Percy Liang and Dan Klein, â€˜Online em for unsupervised modelsâ€™, in

[29]

NAACL-HLT, (2009).
James Lloyd, Peter Orbanz, Zoubin Ghahramani, and Daniel M Roy,
â€˜Random function priors for exchangeable arrays with applications to
graphs and relational dataâ€™, in NIPS, (2012).

[30] Kurt Miller, Michael I Jordan, and Thomas L Grifï¬ths, â€˜Nonparametric

[31]

latent feature models for link predictionâ€™, in NeurIPS, (2009).
Ian Porteous, Evgeniy Bart, and Max Welling, â€˜Multi-hdp: A non para-
metric bayesian model for tensor factorization.â€™, in AAAI, (2008).
[32] Daniel M Roy, Yee Whye Teh, et al., â€˜The mondrian process.â€™, in NIPS,

pp. 1377â€“1384, (2008).

[33] Tracy M Sweet, Andrew C Thomas, and Brian W Junker, â€˜Hierarchical
mixed membership stochastic blockmodels for multiple networks and
experimental interventionsâ€™, Handbook on mixed membership models
and their applications, 463â€“488, (2014).

[34] Petar VeliË‡ckoviÂ´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero,
Pietro Lio, and Yoshua Bengio, â€˜Graph attention networksâ€™, ICLR,
(2018).

[35] Eric P Xing, Wenjie Fu, Le Song, et al., â€˜A state-space mixed mem-
bership blockmodel for dynamic network tomographyâ€™, The Annals of
Applied Statistics, 4, 535â€“566, (2010).

[36] Yunfeng Xu, Hua Xu, Dongwen Zhang, and Yan Zhang, â€˜Finding over-
lapping community from social networks based on community forest
modelâ€™, Knowledge-Based Systems, 109, 238â€“255, (2016).

[37] Hongxia Yang and Aurelie Lozano, â€˜Multi-relational learning via hierar-
chical nonparametric bayesian collective matrix factorizationâ€™, Journal
of Applied Statistics, 42, 1133â€“1147, (2015).
Jaewon Yang, Julian McAuley, and Jure Leskovec, â€˜Community detec-
tion in networks with node attributesâ€™, in ICDM, (2013).

[38]

[39] Rex Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure
Leskovec, â€˜Gnn explainer: A tool for post-hoc explanation of graph
neural networksâ€™, in NeurIPS, (2019).

8

[40]

Jiaxuan You, Rex Ying, Xiang Ren, William L. Hamilton, and Jure
Leskovec, â€˜Graphrnn: Generating realistic graphs with deep auto-
regressive models.â€™, in ICML, (2018).

[41] Yuan Zhang, Tianshu Lyu, and Yan Zhang, â€˜Hierarchical community-
level information diffusion modeling in social networksâ€™, in SIGIR,
(2017).

[42] He Zhao, Lan Du, and Wray L. Buntine, â€˜Leveraging node attributes for

incomplete relational dataâ€™, in ICML, (2017).

[43] Mingyuan Zhou, â€˜Inï¬nite edge partition models for overlapping commu-

nity detection and link predictionâ€™, in AISTATS, (2015).

9

