Struct-MMSB: Mixed Membership Stochastic
Blockmodels with Interpretable Structured Priors

Yue Zhang and Arti Ramesh 1

0
2
0
2

b
e
F
1
2

]

G
L
.
s
c
[

1
v
3
2
5
9
0
.
2
0
0
2
:
v
i
X
r
a

Abstract. The mixed membership stochastic blockmodel (MMSB)
is a popular framework for community detection and network genera-
tion. It learns a low-rank mixed membership representation for each
node across communities by exploiting the underlying graph structure.
MMSB assumes that the membership distributions of the nodes are
independently drawn from a Dirichlet distribution, which limits its
capability to model highly correlated graph structures that exist in
real-world networks. In this paper, we present a ﬂexible richly struc-
tured MMSB model, Struct-MMSB, that uses a recently developed
statistical relational learning model, hinge-loss Markov random ﬁelds
(HL-MRFs), as a structured prior to model complex dependencies
among node attributes, multi-relational links, and their relationship
with mixed-membership distributions. Our model is speciﬁed using a
probabilistic programming templating language that uses weighted
ﬁrst-order logic rules, which enhances the model’s interpretability.
Further, our model is capable of learning latent characteristics in real-
world networks via meaningful latent variables encoded as a complex
combination of observed features and membership distributions. We
present an expectation-maximization based inference algorithm that
learns latent variables and parameters iteratively, a scalable stochastic
variation of the inference algorithm, and a method to learn the weights
of HL-MRF structured priors. We evaluate our model on six datasets
across three different types of networks and corresponding model-
ing scenarios and demonstrate that our models are able to achieve
an improvement of 15% on average in test log-likelihood and faster
convergence when compared to state-of-the-art network models.

1

INTRODUCTION

Modeling the complex and intricate interactions existing within a
community is an important network science problem that has gained
attention in the last decade. Perhaps one of the most commonly and
widely used network generation and community detection model is
mixed membership stochastic blockmodel (MMSB) [1], owing to
its ﬂexibility in modeling different kinds of networks and communi-
ties. MMSB models the node’s membership in latent groups using a
mixed-membership distribution, wherein the node can be part of mul-
tiple latent groups. This membership is used to generate the network
structure.

Related Work MMSB has been adapted in various different ways
to model complex network structures, such as hierarchical commu-
nity structures [19, 33, 18, 41], dynamic networks [10, 15, 35, 20],
multi-relational graph structures [37, 23, 30, 7], nodes participating
in multiple communities [43, 36, 36, 9], and feature-rich networks

1 SUNY Binghamton, USA, email: {yzhan202, artir}@binghamton.edu

[42, 12, 38]. MMSB has also recently been extended to heteroge-
nous networks [21]. Other probabilistic blockmodels have also been
developed, such as inﬁnite relational model (IRM) [22], Bi-LDA
and Multi-LDA with extension to hierarchical Dirichlet processes
(Multi-HDP) [31], binary space partitioning-tree process (BSP-tree)
[13], a generalized version of the Mondrian process [32], and random
function priors [29].

Recent advances in deep learning produced more powerful models
for graph data, such as graph neural networks (GNNs) [39, 6, 25],
GraphRNN [40], graph attention networks (GAT) [34]. Bojchevski
et al. developed NetGAN [4], which uses the generative adversarial
networks (GAN) framework to generate random walks on graphs and
De Cao et al. [8] developed MolGAN, which generates molecular
graphs using the combination of a GAN framework and a reinforce-
ment learning objective. These models differ from MMSB and its
variants in that they are not hand-engineered but can learn from data.
But, the downside is the opaqueness and lack of interpretability of
these models [39], which Struct-MMSB seeks to address.

The above-mentioned blockmodels with the exception of a notable
few ones such as the recently developed Copula-MMSB [11] assume
that the membership indicator pairs are drawn independently; this
limits the capability of the model to encode complex dependencies
in the network structure. Even when the existing approaches relax
this independence assumption, their applicability is restricted to only
speciﬁc network structures (such as a-MMSB models assortative struc-
ture in networks) and are not solely versatile enough to handle the
various kinds of dependencies present in real-world data, such as
presence of additional features, multiple relationships, provision for
learning meaningful latent variables. Further, as the models progress
toward capturing complex dependencies, they are speciﬁed using
complex functions that have limited interpretability. For example, the
Copula-MMSB model uses the Copula function [11], latent mixed-
membership groups (LMMG) uses the logistic function as priors [24],
and the BSP-tree uses a complex variant of the Mondrian process [13].
In addition to only capturing a speciﬁc kind of relationship, these pri-
ors are also harder to specify and understand for domain experts who
may have a limited knowledge of machine learning. Incorporating cor-
relations among the latent groups and memberships that can be easily
interpreted provides the models with modeling power and possibility
of application in domains that require careful speciﬁcation of domain
knowledge such as computational social science, bioinformatics, and
other evolving application areas for societal good.

Contributions
In this paper, we introduce a scalable and general
purpose MMSB, Struct-MMSB, by enhancing MMSB with a struc-
tured prior using a recently developed graphical model, hinge-loss
Markov random ﬁelds (HL-MRFs). Our approach, Struct-MMSB, is

 
 
 
 
 
 
Table 1: A comparison of general-purpose stochastic blockmodel frameworks

Model

Dependencies

Meaningful Latent Variables

Features

Scalability

IRM [22]
MMSB [1]
LMMG [24]
a-MMSB [17]
Copula-MMSB [11]
Struct-MMSB (our approach)

×
×
×
×
(cid:88)
(cid:88)

×
×
×
×
×
(cid:88)

inspired from latent topic networks (LTN), a general-purpose latent
Dirichlet allocation (LDA) using structured priors [14]. Table 1 gives
a comparison of our model Struct-MMSB with other popular variants
of MMSB. Our model possesses the capability to encode: 1) the rela-
tional dependencies among membership distributions, 2) additional
node features, 3) multi-relational information, and their impact on the
membership distributions using a probabilistic programming templat-
ing language, thus remaining interpretable and easy to specify custom
network relationships. Further, our approach also incorporates the
ability to encode meaningful latent variables, which can be learned
as a complex combination of observed features, membership distribu-
tions, and group-group interaction probabilities, thus catering to many
latent-variable modeling scenarios in computational social science
problems.

Next, we present algorithms for inference and learning in Struct-
MMSB. We formulate an expectation maximization (EM) inference
for inferring the expected value of latent variables, mixed-membership
distributions of nodes in groups, and the group-group interaction
probabilities. Then, we develop a scalable inference method using
stochastic EM and show how to effectively incorporate relational
dependencies, while remaining scalable to large networks. We then
present a way to learn the weights of the ﬁrst-order logical rules by
maximizing the likelihood of the weights without additional ground
truth data, thus allowing the model to learn the predictive ability of
the logical rules in the data.

We demonstrate the versatility of our model in modeling different
network modeling scenarios on data from six real-world networks and
show that our model on average achieves a 15% better log-likelihood
and a better prediction performance than the state-of-the-art MMSB
variant, Copula-MMSB [11], IRM [22], and MMSB [1] and their
multi-relational and online variants.

2 BACKGROUND

In this section, we provide background on MMSB and HL-MRFs, and
then proceed to show how to incorporate HL-MRFs as a structured
prior in MMSB in Struct-MMSB. For ease of reference, we present a
list of notations and their meanings to be used in the equations and
derivations in this paper in Table 2.

2.1 Mixed Membership Stochastic Blockmodels

(MMSB)

The generative process for MMSB is deﬁned as follows.

×
×
(cid:88)
×
×
(cid:88)

×
(cid:88)
(cid:88)
(cid:88)
×
(cid:88)

Table 2: Notations for STRUCT-MMSB

N
p, q
K
k, k1, k2
α, β(1), β(2)
Yp,q
zp→q

Π
πp
π(p)
k

B
Bk1,k2
Λ, λ
ψr
M (1)
M (2)
H (1), H (2)
X
η

number of nodes
speciﬁc nodes
number of communities
speciﬁc communities
hyperparameters of MMSB
link between nodes p and q
membership indicator for sender node p and receiver
node q
membership distributions of all the nodes
membership distribution of node p across communities

value in the membership distribution of node p for
group k
matrix capturing interactions between communities
interaction between communities k1 and k2
HL-MRF rule weights
HL-MRF potential function for rule r
number of HL-MRF potentials for Π
number of HL-MRF potentials for B
latent variables in the HL-MRF prior
observed features
Lagrange coefﬁcient

– Draw membership indicator for the receiver, (cid:126)zq→p ∼ Multino-

mial ( (cid:126)πq)

– Sample

the
their
p→qB(cid:126)zq→p), where B ∼ Beta( (cid:126)β(1), (cid:126)β(2))
Bernoulli((cid:126)z(cid:62)

interaction,

value

of

Yp,q

∼

The independence assumptions implicit in the Dirichlet and Beta pri-
ors are what prevents MMSB from capturing complex dependencies.
The priors are therefore our point of attack in developing a rich, ﬂex-
ible, and easy-to-encode stochastic blockmodel. In Struct-MMSB,
we replace the ﬂat Dirichlet and Beta priors with more expressive
HL-MRF priors, we discuss HL-MRFs and their suitability as priors
in MMSB below.

2.2 Hinge-loss Markov Random Fields

• For each node p ∈ N :

– Draw a K dimensional mixed membership vector (cid:126)πp ∼ Dirichlet

((cid:126)α)

• For each pair of nodes (p, q) ∈ N × N

– Draw membership indicator for the initiator, (cid:126)zp→q ∼ Multino-

mial ((cid:126)πp)

HL-MRFs are a recently developed scalable class of continuous,
conditional graphical models [3]. HL-MRFs can be speciﬁed using
Probabilistic Soft Logic (PSL) [3], a ﬁrst-order logic templating lan-
guage. In PSL, random variables are represented as logical atoms
and weighted rules deﬁne dependencies between them of the form:
λ : P (a) ∧ Q(a, b) → R(b), where P, Q, and R are predicates, a
and b are variables, and λ is the weight associated with the rule. The
weight of the rule r indicates its importance in the HL-MRF model,

2

which is deﬁned as

P (Y |X ) = exp

(cid:16)

−

M
(cid:88)

r=1

λrψr(Y , X)

(cid:17)

/Z(λ);

(1)

Z(λ) =

(cid:16)

−

exp

(cid:90)

Y

M
(cid:88)

r=1

λrψr(Y , X )

(cid:17)

ψr(Y , X ) = (max{lr(Y, X ), 0})ρr

(2)

where P (Y |X ) is the probability density function of a subset
of logical atoms Y given observed logical atoms X, ψr(Y , X ) is
a hinge-loss potential corresponding to an instantiation of a rule
r, and is speciﬁed by a linear function lr and optional exponent
ρr ∈ {1, 2}, and Z(λ) is the partition function. The logical conjunc-
tion of Boolean variables X ∧ Y can be generalized to continuous
variables using the hinge function max{X + Y − 1, 0}, which is
known as the Lukasiewicz t-norm. HL-MRFs admit tractable MAP
inference regardless of the graph structure of the graphical model,
making it feasible to reason over complex dependencies. This is pos-
sible because HL-MRFs operate on continuous random variables and
encode dependencies using convex potential functions, so MAP in-
ference in these models is a convex optimization problem. Giannini
et al. [16] provide theoretical insight for understanding the convex
characterization of the Lukasiewicz t-norms and Horn clauses in PSL
[3].

Our resulting model, Struct-MMSB, upon replacing the priors with
HL-MRFs possess the following desirable qualities. They are struc-
tured and can capture complex relational dependencies among the
nodes, their features, their membership distributions and group-group
interactions using graphical model templates. These templates are
simple weighted rules that capture the general characteristics of the
network. The lucid nature of these rules make them inherently inter-
pretable and intuitive and hence pave the way for ease of speciﬁcation
by domain experts. Further, these priors can be enriched using mean-
ingful latent variables that further enhance their modeling power and
intuitiveness.

3 STRUCT-MMSB
Our goal in designing Struct-MMSB is to create a lucid, easy-to-
specify, and expressive generative model. We discuss the technical
details of Struct-MMSB below.

3.1 Struct-MMSB graphical model
To construct Struct-MMSB, we replace the Dirichlet priors and Beta
priors in the MMSB generative process with HL-MRF potential func-
tion ψ. Figure 1 shows the plate diagram of Struct-MMSB. The
HL-MRF priors are indicated by ψπ, which captures dependencies
between the membership distribution π of two nodes in the graph
and ψB, which captures dependencies between groups in the group
interaction matrix B. Before delving into the mathematical details,
we provide some example dependencies that can be encoded using
Struct-MMSB:
Correlated nodes: correlated(p, q) ∧ π(p, K) → π(q, K), if two
nodes p and q are correlated, then their corresponding membership
distributions (π) are correlated.
Latent characteristics modeled as latent variables: link(p, r) ∧
link(q, r) → similar(p, q), if two nodes p and q share common neigh-
bor r, then they are similar.
Inter-group and Intra-group links: correlated(p, q) ∧ π(p, K1) ∧
B(K1, K2) → π(q, K2), if two nodes p and q are correlated and the
group interaction between K1 and K2 is high, then the value of the
membership distribution π(p, K1) is similar to π(q, K2).

Figure 1: Plate Diagram of Struct-MMSB

3.2 Combining MMSB with HL-MRF priors

In Struct-MMSB, the priors given by HL-MRFs deﬁne probability
densities as follows:

P (Π, H (1)|X (1)) ∝ exp

M (1)
(cid:88)

(cid:18)

−

λ(1)
j ψ(1)

j

(Π, X (1), H (1))

P (B, H (2)|X (2)) ∝ exp

(cid:18)

−

j=1

M (2)
(cid:88)

j=1

λ(2)
j ψ(2)

j

(B, X (2), H (2))

(cid:19)

(cid:19)

j

where ψ(a)
are hinge-loss potentials, M (a) are number of hinge-loss
potentials, and X (a) are observed features that are conditioned on,
and H (a) are additional latent variables introduced by Struct-MMSB,
and Π and B are MMSB parameters.

The membership distributions Π are constrained to sum to 1 for
each node as in MMSB. For membership distributions that do not
come from the HL-MRF priors, the default Dirichlet priors are used.
We also add smoothed Dirichlet and Beta potentials for Π, B, and 1 −
(1 − Bk1,k2 )β(2)−1,
B, (cid:81)
respectively. Incorporating the HL-MRF structured priors together
with the smoothed Dirichlet and Beta potentials, the log-posterior of
the variables of interest (parameters Π and B, latent variables H (1)
and H (1)) is,

p,k πpα−1

Bβ(1)−1
k1,k2

, and (cid:81)

, (cid:81)

k1,k2

k1,k2

k

logP (Π, B, H (1), H (2)|Y, α, β(1), β(2), X (1), X (2), Λ) =

P (Yp,q, zp, zq|πp, πq, B)

(cid:19)

(cid:88)

log

(cid:18) (cid:88)

p∈N,q∈N
(cid:88)

+

zp∈K,zq ∈K
(α − 1)logπp
k

p∈N,k∈K

−

+

−

M (1)
(cid:88)

j ψ(1)
λ(1)

j

(Π, X (1), H (1))

j=1

(cid:88)

(cid:18)

(β(1) − 1)logBk1,k2 + (β(2) − 1)log(1 − Bk1,k2 )

(cid:19)

k1,k2

M (2)
(cid:88)

j=1

j ψ(2)
λ(2)

j

(B, X (2), H (2)) + const.

(3)

3.3 Training via EM

Our EM algorithm is motivated from latent variable learning in HL-
MRFs [2] and LTN [14]. We train the model by maximum a posteriori
(MAP) estimation, optimizing Eqn 3 with respect to Π, B, H (1),

3

 𝜋P 𝜋qZp->q𝛼𝜓𝜋Yp,qB𝜓B𝛽(1)𝛽(2)Zq->pH (2). Since Eqn 3 cannot be optimized directly due to the sum inside
the logarithm, we develop an EM algorithm that iteratively optimizes
a lower bound arising from Jensen’s inequality.

Applying

inequality, we

get

the

function
logP (Π, B, H|Y, α, β(1), β(2), X, Λ), where,

to

R(Π, B, X (1), X (2), H (1), H (2))

Jensen’s
be,

objective
≤

R(Π, B, X (1), X (2), H (1), H (2)) = −

M (1)
(cid:88)

j=1

λ(1)
j ψ(1)

j

(Π, X (1), H (1))

−

+

+

+

−

M (2)
(cid:88)

j=1

λ(2)
j ψ(2)

j

(B, X (2), H (2))

(cid:88)

(cid:18) (cid:88)

(γp,q,k1,k2 + γq,p,k2,k1 ) + α − 1

(cid:19)

logπp
k1

p,k1

(cid:88)

q,k2
(cid:18) (cid:88)

k1,k2

p,q

(cid:88)

(cid:18) (cid:88)

γp,q,k1,k2 Yp,q + β(1) − 1

(cid:19)

logBk1,k2

γp,q,k1,k2 (1 − Yp,q) + β(2) − 1

(cid:19)

log(1 − Bk1,k2 )

k1,k2
(cid:88)

p,q,k1,k2

p,q

γp,q,k1,k2 logγp,q,k1,k2 + const

(4)

(cid:44) P ((cid:126)zp→q = k1, (cid:126)zq→p = k2|Π(t), B(t), Yp,q),
We deﬁne γp,q,k1,k2
which encodes the distribution over the membership indicators (cid:126)zp→q
and (cid:126)zq→p based on the previous parameter values of Π and B. The
algorithm consists of an E-step and an M-step, which are iterated until
convergence. We ﬁrst present the batch version of the algorithm, that
iterates on all the data points for each E and M-step update and then
present a stochastic variation that is scalable to large datasets.

3.3.1 E-step

To perform the E-step of the EM algorithm at iteration t, we ﬁrst
compute γp,q,k1,k2 in Equation 4.

γp,q,k1,k2 ∝ P (Yp,q|zp, zq, Π(t), B(t))P (zp, zq|Π(t), B(t))

= BYp,q
k1,k2

(1 − Bk1,k2 )1−Yp,q π(p)
k1

π(q)
k2

Since the inference algorithm is iterative, the superscript (t) is used to
refer to the previous/initialized values of Π and B.

3.3.2 M-step

We then perform the maximization step for Π and B parameters
that are not involved in the HL-MRF prior, for which the update is
identical to the M-step of the EM algorithm for standard MMSB. For
Π, we add Lagrange terms − (cid:80)
k ((cid:80)
k πp
p ηΠ
k − 1) to constrain the
parameter vectors to sum to one, where ηΠ
k is the Lagrange coefﬁcient.
To derive updates for B, we deﬁne B and 1 − B as B(i), i ∈ {0, 1},
k1k2 ((cid:80)
respectively, and add the Lagrange term − (cid:80)
ηB
−
1) to constrain the B(i)
k1k2 is the Lagrange
coefﬁcient. Taking derivatives and setting them to zero, we obtain the
updates,

k1k2
to sum to one, where ηB

i B(i)
k1k2

k1k2

(cid:88)

πp
k1

∝

(γp,q,k1,k2 + γq,p,k2,k1 ) + α − 1;

q,k2
(cid:80)

B(0)

k1,k2

=

(cid:80)

p,q γp,q,k1,k2 Yp,q + β(1) − 1
p,q γp,q,k1,k2 + β(1) + β(2) − 2

Then, we optimize the lower bound jointly over the remaining Π
and B parameters by ﬁxing the parameters above that are not up-
dated using HL-MRF priors. The hinge-loss potentials for Π and B
can be optimized separately as evident from Eqn 4. We minimize
the negative of each of these two subproblems −R(Π, H (1), X (1)),
−R(B, H (2), X (2)) using a consensus-optimization algorithm based
on the alternating direction method of multipliers (ADMM).

To accomplish this, we extend the consensus optimization algo-
rithm by Bach et al. [3] for MAP inference in HL-MRFs. The algo-
rithm creates a local copy for each of the variables, thus dividing the
original problem into independent subproblems that may be solved in
parallel. The algorithm iteratively solves the independent subproblems
and then updates the global consensus variables to be the average of
the local copies. This is guaranteed to ﬁnd the global optimum solu-
tion of the objective function. For more details, we refer the reader to
[3] and [5].

p,q,k1,k2

We extend this algorithm to include the objective function
terms in the lower bound R in Eqn 4 for Π and B. For Π,
the terms 1 and 3 in Eqn 4 are included and for B, the terms
2, 4, and 5 in Eqn 4 are included. The entropy term (term 6,
− (cid:80)
γp,q,k1,k2 logγp,q,k1,k2 ) and the constant term (term
7) are not included in the M-step objective as they do not have Π
or B in them. To cast this as a consensus optimization problem, we
create local copies of our variables of interest (Π and B, denoted by
Ci in the ADMM update equation below), which we refer to as ci,
where i refers to each πp
k and Bk1,k2 , in the respective consensus
optimization equations. The consensus optimization problem entails
solving the subproblems with local copies ci independently for both
the variables and then iterating till a consensus is reached, which is
enforced via an equality constraint in the Lagrange term with coefﬁ-
cient ηi. For convenience, we refer to our objective terms as Ailogci,
where Ai denotes the coefﬁcient in term 3 in Equation 4, which equals
to ((cid:80)
(γp,q,k1,k2 + γq,p,k2,k1 ) + α − 1), and ci denotes the local
copy of the variable.

q,k2

The consensus ADMM update [5] for ci is given by,

ci = arg min

(−Ailogc(cid:48)

i + ηi(c(cid:48)

i − Ci) +

c(cid:48)
i

ρ
2

(c(cid:48)

i − Ci)2)

where ρ is an ADMM step-size parameter, ci is the local copy of the
variable used by the ADMM consensus optimization algorithm, and
Ci refers to the original variable. In order to solve the ADMM update
equation, we take its derivative with respect to ci and set it equal to
zero, ∂J
i(ηi − ρCi) − Ai = 0.
∂c(cid:48)
i

i + c(cid:48)

= ρc(cid:48)2

Solving this quadratic equation, we get two solutions. One of them
is negative and can be discarded; ci is set to the positive solution.
The Lagrange coefﬁcients are updated using ADMM updates for
Lagrangian dual, ηi ← ηi + ρ(c(cid:48)

i − Ci).

3.4 Stochastic EM

To scale the batch model to large networks, we present a stochastic
EM inference method. Our algorithm is motivated from the online
EM algorithm [28] and the stochastic optimization for a-MMSB [17].
We compute the expected sufﬁcient statistics of Π and B and update
it using randomly sampled node-pair (p, q) from distribution g(p, q)
instead of the entire sample, by computing the stochastic natural
gradient in the space of sufﬁcient statistics.

To obtain the stochastic EM algorithm, we ﬁrst subsample the graph
and compute the expected sufﬁcient statistics for the sample, given the
current settings of the Π and B. We then update Π and B using these
sufﬁcient statistics. We introduce three global variables: θp
k, φk1k2 ,

4

k is equal to the normalized θp

and φ(cid:48)
distribution πp
B(0)
k1k2
B(1)
k1k2
To derive the expected sufﬁcient statistics of πp

k1k2 to derive the stochastic EM updates. The membership
k. The afﬁnity blockmodel
k1k2 ), and
k1k2 ).

(which refers to B) is given by φk1k2 /(φk1k2 + φ(cid:48)
k1k2 /(φk1k2 + φ(cid:48)
(which refers to 1 − B) equals φ(cid:48)

k for a speciﬁc node

p and community k1, we have,
(cid:18) (cid:88)

(γp,q,k1,k2 + γq,p,k2,k1 ) + α − 1

(cid:19)

logπp
k1

q,k2
(cid:18) (cid:88)

=

q,k2

(cid:18)

=

Eg[

g(p, q)

1
g(p, q)

(γp,q,k1,k2 + γq,p,k2,k1 ) + α − 1

(cid:19)

logπp
k1

1
g(p, q)

(γp,q,k1,k2 + γq,p,k2,k1 )] + α − 1

(cid:19)

logπp
k1

The intuition behind the above expected sufﬁcient statistics comes
from online EM for unsupervised models [28] and stochastic varia-
tional EM for a-MMSB [17]. Here, we sample a single node pair (p,
q) and compute the γ if our entire graph consisted of p and q, repeated
1/g(p, q) times. In practice, instead of a single node a mini-batch
is sampled. Liang et al. [28] specify that updating on multiple data
instances using a mini-batch adds more stability than updating after
each sample. This translates to all q in the mini-batch for a speciﬁc
node of interest p.

(cid:80)

, sp,k1 = 1
g

Hence, under each mini-batch, without HL-MRF priors, the ex-
pected sufﬁcient statistics of θp
q,k2(γp,q,k1,k2 +
k1
γq,p,k2,k1 ) + α − 1. With HL-MRF priors, we ﬁrst use ADMM
to ﬁnd local optimal value π∗p
under the current mini-batch, then the
k1
p,k1 = π∗p
expected sufﬁcient statistics is calculated as sP SL
sp,k1 .
k1
Similarly, we calculate the expected sufﬁcient statistics for B. As
Bk1,k2 and 1 − Bk1,k2 factorize into separate terms in Eqn 4, we
treat them as two different variables and calculate the expected sufﬁ-
cient statistics separately. For Bk1,k2 , we have,

(cid:80)

k1

(cid:88)

p,q

γp,q,k1,k2 Yp,qlogBk1,k2

(cid:88)

=

g(p, q)

p,q

(cid:18) 1

g(p, q)

(cid:19)

γp,q,k1,k2 Yp,q

logBk1,k2

= Eg[

1
g(p, q)

γp,q,k1,k2 Yp,q]logBk1,k2

Under mini-batch, without HL-MRF priors, expected sufﬁcient statis-
p,q γpqk1k2 Yp,q + β(1) − 1.
tics of φk1,k2 equals to, sk1k2 = 1
g
Similarly, for B(cid:48)

k1,k2 = 1 − Bk1,k2 , we have,

(cid:80)

(cid:88)

p,q

γp,q,k1,k2 (1 − Yp,q)log(B(cid:48)

k1,k2 )

(cid:88)

=

g(p, q)

p,q

(cid:18) 1

g(p, q)

γp,q,k1,k2 (1 − Yp,q)

(cid:19)

log(B(cid:48)

k1,k2 )

= Eg[

1
g(p, q)

γp,q,k1,k2 (1 − Yp,q)]log(B(cid:48)

k1,k2 )

g

k1k2 = 1

k1,k2 equals to s(cid:48)

Under mini-batch, without HL-MRF priors, expected sufﬁcient statis-
(cid:80)
tics of φ(cid:48)
p,q γp,q,k1,k2 (1−Yp,q)+β(2)−
1. With HL-MRF priors, to compute the expected sufﬁcient statistics
for φk1k2 and φ(cid:48)
k1k2 , ﬁrst we need to use ADMM to ﬁnd local optimal
value B∗
k1k2 under current mini-batch. Then, the expected sufﬁcient
k1k2 (sk1k2 + s(cid:48)
k1k2 = B∗
statistics for φk1k2 becomes sP SL
k1k2 ) and for
φ(cid:48)
k1k2 , s(cid:48)P SL
k1k2 = (1 − B∗
k1k2 )(sk1k2 + s(cid:48)
k1k2 ). Intuitively speaking,
the expected sufﬁcient statistics with HL-MRF priors re-assign the

total mass without HL-MRF priors, (cid:80) s, and make it proportional
to the optimal value using the estimated π∗p
k1k2 values from
k1
ADMM.

and B∗

In our experiments, we use mini-batch instead of just one node pair.
Evaluating the rewritten above equations for a node pair sampled from
the graph gives a noisy but unbiased estimate of the batch model. We
scale the expected sufﬁcient statistics estimated from the subsample
by
g(p,q) so that they are unbiased estimates of the true sufﬁcient
statistics as illustrated by Gopalan et al. [17]. For instance, if we
sample nodes uniformly at random, then g(p, q) = miniBatch/N 2.
We calculate this as the difference between expected sufﬁcient

1

statistics and previous value as follows

∂θt
pk = sP SL
k1k2 = sP SL
k1k2 = s(cid:48)P SL

pk − θt−1
pk
k1k2 − φt−1
k1k2
k1k2 − φ(cid:48)t−1
k1k2

∂φt
∂φ(cid:48)t

(5)

In the next step, we calculate the stochastic natural gradients computed
from a sampled node pair (p, q) or mini-batch and update the global
variables,

k ← θp
θp

k + ρt ∗ ∂θt

pk

φk1k2 ← φk1k2 + ρt ∗ ∂φt
k1k2 + ρt ∗ ∂φ(cid:48)t
k1k2 ← φ(cid:48)
φ(cid:48)

k1k2

k1k2

(6)

where ρ is global step size. Results from the stochastic approximation
literature requires that (cid:80)
t ρt = ∞ to guarantee
convergence to a local optimum. We set ρt (cid:44) (τ0 + t)−κ, where
κ ∈ (0.5, 1] is the learning rate and τ0 ≥ 0 downweights early
iterations.

t < ∞ and (cid:80)

t ρ2

Distributed Implementation We implement a distributed comput-
ing environment using Apache Thrift framework for the stochastic EM
inference. This further helps in scaling our models to larger datasets.
We perform stochastic EM inference at each client side using sub-
sampled mini-batch and then update the global parameters at server
side.

3.5 Weight learning

The weights of the HL-MRF ﬁrst-order-logic rules, Λ, can be selected
based on domain knowledge. The challenge to learning weights arises
because often there is no ground truth data for our variables of interest,
Π and B. Following Bach et al. [3], we present a maximum-likelihood
estimation by maximizing the log-likelihood of the training data given
parameter values to learn the weights.

∂R
∂Λq

=

∂logP (Π, H|X)
∂Λq

= EΛ[Ψq(X, H, Π)] − Ψq(X, H, Π)

EΛ[Ψq(X, H, Π)] =

(cid:90)

Π

Ψq(X, H, Π)P (Π|X, H)

We approximate the expectation using the value of the potential
functions at the most probable setting of Π with the current parameters.
During weight learning, we treat latent variables H as observations
using the inferred value from the EM step.

4 EXPERIMENTS

We conduct experiments to answer the questions:

5

1) How our models perform on different network modeling scenarios?
To demonstrate the versatility of Struct-MMSB and its ability to
model different real-world scenarios, we evaluate the performance
of our model on a total of 6 datasets and three network modeling
scenarios: a) presence of additional features (case 1), b) presence
of multiple links (case 2), and c) neighborhood similarity (case 3),
[27]. We induce three different structured priors that correspond to
the characteristics of each of these modeling scenarios. We compare
the training and test log-likelihood and area under the ROC curve
(AUC-ROC) of our models at convergence with MMSB [1] or its
online/stochastic variant, IRM [22], and the state-of-the-art variant of
MMSB (Copula-MMSB [11]), which has been compared to some
other variants of MMSB and network models (MMSB, IRM, LFRM
[30], and iMMM [26]) and shown to be better. Since Copula-MMSB
in the present form is not capable of handling multi-relational data,
we extend Copula-MMSB to handle multi-relational data wherever
appropriate; we refer to this model as Multi-Copula. We show that
our model achieves better performance and faster convergence
both under batch and stochastic inference when compared to the
above-mentioned models.

2) How informative are the latent variables in our intuitive HL-MRF
priors? While performance is one important aspect, the more inter-
esting contribution of our approach is the interpretable and intuitive
nature of Struct-MMSB models, succinctly capturing domain knowl-
edge in the different modeling scenarios. This interpretable nature is
further enhanced by the presence of meaningful latent variables that
can abstract the relationship between the features, nodes, and network
connections. We show how to encode meaningful latent variables
in all the three modeling scenarios: presence of additional features,
presence of multiple links, and neighborhood similarity. We present
qualitative analysis of the values learned by our latent variables and
show that our models are able to encode and learn meaningful latent
variable values that enhance the modeling power and interpretability
of our model.

4.1 Experimental settings

In all our experiments, our models use same initializations as stan-
dard MMSB. In the stochastic inference setting, our models also
use the same mini-batches as online MMSB, so that they are ex-
actly comparable. We initialize the hyper-parameters β(1) and β(2)
of parameter B by setting β(1)/(β(1) + β(2)) =link-rate, where
link-rate= link/(link + non-link). We set number of communities
to 5 for small datasets (feature-based similarity (case 1) and multi-
relational experiments (case 2)), and to 8 for the large datasets in
link-based similarity (case 3) experiment. And, we randomly initialize
the Π and B values. Statistically signiﬁcant values with a rejection
threshold of p = 0.05 are typed in bold.

4.2 Case 1: feature-based similarity

Here, we design models that take additional node features into account
to guide community discovery and network generation.

together. The ﬁrst rule in Table 3 means if node p and node q have
the same features (multiple instantiations of Rule 1), then we infer
that node p and node q are similar. similarity(p,q) is a latent variable
that helps us model the degree of similarity between two nodes. The
second rule captures that if two nodes are similar, then we can infer
that they have similar membership distributions.

Table 3: HL-MRF priors to guide community discovery based on
presence of multiple common features measured using latent variable
similarity(p, q)

Feature-Based Similarity Model
feature(p, T) ∧ feature(q, T) ∧ link(p, q) → similarity(p, q)
similarity(p, q) ∧ π(p, K) → π(q, K)
similarity(p, q) ∧ ¬π(p, K) → ¬π(q, K)

4.2.2 Results

We evaluate this model on two Facebook Ego datasets: 1) Ego-414
dataset containing 159 nodes and 3386 links, and 2) Facebook Ego-
686 dataset containing 170 nodes and 3312 links. Table 4 shows that
our model achieves a better training and test log-likelihood and AUC
on two Facebook ego datasets when compared with standard MMSB,
Copula-MMSB, and IRM.

Our latent variables, apart from providing the model with model-
ing power, also bring interpretability to the model. Figures 2(a) and
2(b) illustrate the correlation between latent variable similarity and
the number of common features and membership distributions. This
conforms with our structured prior that a commonality in features
between a pair of nodes can indicate that they belong to the same
communities. We also observe that our latent variable values act as
a proxy to the relationship between similarity in features and their
membership distributions and helps us interpret them better. For ex-
ample, in Ego-414 dataset, we get the value for the latent variable
similarity for nodes 74 and 88 to be 0.714 and we observe that they
both have similar membership distributions after training, having a
high value for the same community.

Table 4: Results on Facebook Ego-414 and Ego-686 datasets com-
paring Struct-MMSB with Copula-MMSB, Standard MMSB, and
IRM.

Dataset Model

Log-
Likelihood

Test Log-
Likelihood

Ego414

Ego686

Struct-MMSB
Copula-MMSB
MMSB
IRM

Struct-MMSB
Copula-MMSB
MMSB
IRM

-1042.986
-1909.378
-1309.271
-2205.484

-1628.858
-2002.216
-1690.593
-2840.591

-787.406
-1404.075
-986.321
-1521.684

-1169.224
-1407.204
-1221.328
-1866.839

AUC

0.954
0.844
0.9220
0.746

0.892
0.854
0.875
0.659

4.2.1 Model

In this modeling scenario, we specify HL-MRF priors to utilize feature
commonality between nodes to model their community membership.
The priors in Table 3 capture that if two nodes have many common
features, then there is a greater chance of these nodes being grouped

4.3 Case 2: multi-relational graphs

In our next experiment, we consider a multi-relational graph set-
ting, where there are multiple different relationships between a sin-
gle pair of nodes. Here, we experiment on two different kinds of
multi-relational data: i) the presence of multiple different types of
relationships between a pair of nodes, and ii) the presence of the same

6

(a) Correlation between similarity
value and number of common fea-
tures

(b) Correlation between similarity
value and community membership
distributions

Figure 2: Correlation between latent variable values and membership
distributions

type of relationship between a pair of nodes at different timestamps,
each timestamp capturing a single relationship.

4.3.1 Model

To guide the generation of the model in the multi-relational setting,
our structured HL-MRF prior captures that if a pair of two nodes p and
q have multiple links between them (Rule 1 in Table 5), then, they are
“closer” than other pairs of nodes. This closeness in their relationship
is modeled using the latent variable close(p, q). Rule 2 in Table 5
captures that if two nodes are closer (i.e., have a higher value of
latent variable close(p, q)), then there is a higher probability of a link
existing between them in the network. Note that in Rule 2, we connect
the blockmodel B that captures the interaction between communities,
membership distributions of nodes p and q with the latent variable
close induced by the structured prior to guide the generation process.

Table 6: Comparing Struct-MMSB with Standard and Multi-Copula
MMSB on UMLS and Email-Temporal datasets.

Dataset Model

UMLS

Email

Struct-MMSB
Multi-Copula
MMSB

Struct-MMSB
Multi-Copula
MMSB

Log-
Likelihood

-11868.051
-12242.841
-12061.241

-1523.488
-1777.112
-1562.353

Test Log-
Likelihood

-2945.953
-3153.354
-2998.037

-175.871
-210.090
-180.620

AUC

0.831
0.785
0.814

0.997
0.995
0.995

(a) Progression toward convergence
of batch Struct-MMSB in UMLS
dataset

(b) Progression toward convergence
of stochastic Struct-MMSB in Face-
book Ego107

Figure 3: Struct-MMSB progresses faster toward convergence in both
batch and stochastic scenarios

4.4 Case 3: community discovery using

neighborhood similarity

Here, we consider the problem of discovering communities in graphs
using neighborhood similarity. Note that this is the setting for which
MMSB was originally conceived. In this experiment, we evaluate the
performance of the stochastic Struct-MMSB with the online variant
of MMSB on performance and the ability to converge faster.

Table 5: Capturing proximity between nodes based on the presence of
multiple links between them using latent variable close(p, q)

4.4.1 Model

Node Proximity in Multi-Relational Graphs
link(p, q, T) → close(p, q)
close(p, q) ∧ B(K1,K2) ∧ π(p, K1) → π(q, K2)

4.3.2 Results

We evaluate the performance of our model on two datasets: 1) Uniﬁed
Medical Language System dataset (UMLS) consisting of 135 nodes,
49 relations, and a total of 6752 links, and 2) Email-Temporal dataset
consisting of 142 nodes, 525 timestamps, and a total of 48, 141 links.
Table 6 shows the comparison of the performance scores of our model
with standard MMSB and Multi-Copula (Copula extended to the multi-
relational case to enable a fair comparison) at training and testing time
on UMLS and Email-Temporal datasets, where our model achieves
better log-likelihood scores on training and test.

Also, as the priors are expected to guide the model in the right
direction in the beginning, a better measure of the effectiveness of the
structured priors is evaluating the progression toward convergence.
Figure 3(a) shows the progression of the models toward convergence.
We observe that Struct-MMSB progresses faster toward convergence
and gets a higher AUC value right from the early iterations.

The mixed membership π is a low rank representation of node’s
neighborhood. We guide π by capturing that if two nodes have mul-
tiple common neighbors (Rule 1 in Table 7), then they have similar
membership distributions (Rules 2 and 3 in Table 7). We capture the
neighborhood similarity using the latent variable similarity(p, q).

Table 7: HL-MRF priors capturing neighborhood similarity between
nodes based on the presence of multiple neighbors using latent vari-
able similarity(p, q)

Community Discovery using Neighborhood Similarity
link(p, r) ∧ link(q, r) → similarity(p, q)
similarity(p, q) ∧ π(p, K) → π(q, K)
similarity(p, q) ∧ ¬π(p, K) → ¬π(q, K)

4.4.2 Results
We evaluate performance on the Facebook Ego-107 dataset, which
contains 1045 nodes and 53498 links and Email-Eu-core dataset,
which contains 1005 nodes and 25571 links. We implement the
stochastic optimization for both Struct-MMSB and standard MMSB
and compare our stochastic Struct-MMSB with stochastic/online
MMSB. For each mini-batch iteration, we randomly sample 10%

7

0.20.30.40.50.60.7similarity value24681012# common features0.20.40.60.8similarity value00.20.40.60.81% node-pairs in same community0501001502000.60.650.70.750.80.85Struct-MMSBMMSBMulti-Copula010k20k30k40k0.60.70.80.9Struct-MMSBOnline-MMSBTable 8: Results on Ego-107 and Email-Eu-Core datasets comparing
stochastic Struct-MMSB with online MMSB

Dataset Model

Ego107

Email

Struct-MMSB
Online-MMSB

Struct-MMSB
Online-MMSB

Log-
Likelihood

-45202.096
-48534.366

-31425.953
-31562.378

Test Log-
Likelihood

-11441.823
-12262.921

-8059.303
-8133.698

AUC

0.922
0.900

0.862
0.861

of the nodes from all the data. We add all links that exist between
the sampled nodes to the mini-batch. In total, we run 48,000 mini-
batch iterations. Our stochastic model achieves better log-likelihood
at training and test time when compared to online-MMSB as evident
from Table 8. We also observe a similar convergence trend in the
stochastic model as well (Figure 3(b)), where our model progresses
faster toward convergence even while only observing a small subset
of all the nodes and relationships during each iteration.

5 Conclusion

We presented a versatile general-purpose MMSB, Struct-MMSB, that
is capable of encoding multi-relational data, additional features, and
dependencies among the membership distributions, community inter-
action matrix, and learn meaningful latent variables in the process. We
present a batch inference algorithm using EM and a scalable variant
using stochastic EM and a method to learn the weights of the struc-
tured HL-MRF priors. Our experimental evaluation demonstrates the
ability of our model to achieve a superior log-likelihood on training
and held-out test data and faster convergence on three different mod-
eling scenarios across 6 datasets and the interpretable nature of our
learned latent variables.

REFERENCES

[1] Edoardo M Airoldi, David M Blei, Stephen E Fienberg, and Eric P Xing,
‘Mixed membership stochastic blockmodels’, JMLR, 9(Sep), 1981–2014,
(2008).

[2] Stephen Bach, Bert Huang, Jordan Boyd-Graber, and Lise Getoor,
‘Paired-dual learning for fast training of latent variable hinge-loss mrfs’,
in ICML, pp. 381–390, (2015).

[3] Stephen H Bach, Matthias Broecheler, Bert Huang, and Lise Getoor,
‘Hinge-loss markov random ﬁelds and probabilistic soft logic’, JMLR,
18(109), 1–67, (2017).

[4] Aleksandar Bojchevski, Oleksandr Shchur, Daniel Zugner, and Stephan
Gunnemann, ‘Netgan: Generating graphs via random walks’, in ICML,
(2018).

[5] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein,
et al., ‘Distributed optimization and statistical learning via the alternating
direction method of multipliers’, Foundations and Trends R(cid:13) in Machine
learning, 3, 1–122, (2011).
Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann Lecun, ‘Spec-
tral networks and locally connected networks on graphs’, in ICLR,
(2014).

[6]

[7] Bei Chen, Ning Chen, Jun Zhu, Jiaming Song, and Bo Zhang, ‘Dis-
criminative nonparametric latent feature relational models with data
augmentation.’, in AAAI, (2016).

[8] Nicola De Cao and Thomas Kipf, ‘Molgan: An implicit generative model
for small molecular graphs’, arXiv preprint arXiv:1805.11973, (2018).
Ismail El-Helw, Rutger Hofman, Wenzhe Li, Sungjin Ahn, Max Welling,
and Henri Bal, ‘Scalable overlapping community detection’, in Parallel
and Distributed Processing Symposium Workshops, (2016).

[9]

[10] Xuhui Fan, Longbing Cao, and Richard Yi Da Xu, ‘Dynamic inﬁnite
mixed-membership stochastic blockmodel’, IEEE transactions on neural
networks and learning systems, 26, 2072–2085, (2015).

[11] Xuhui Fan, Richard Yi Da Xu, and Longbing Cao, ‘Copula mixed-

membership stochastic blockmodel.’, in IJCAI, (2016).

[12] Xuhui Fan, Richard Yi Da Xu, Longbing Cao, and Yin Song, ‘Learning
nonparametric relational models by conjugately incorporating node
information in a network’, IEEE transactions on cybernetics, 47, 589–
599, (2017).

[13] Xuhui Fan, Bin Li, and Scott Sisson, ‘The binary space partitioning-tree

[14]

process’, in AISTATS, (2018).
James Foulds, Shachi Kumar, and Lise Getoor, ‘Latent topic networks:
A versatile probabilistic programming framework for topic models’, in
ICML, (2015).

[15] Wenjie Fu, Le Song, and Eric P Xing, ‘Dynamic mixed membership

blockmodel for evolving networks’, in ICML, (2009).

[16] Francesco Giannini, Michelangelo Diligenti, Marco Gori, and Marco
Maggini, ‘Characterization of the convex łukasiewicz fragment for learn-
ing from constraints’, in AAAI, (2018).

[17] Prem K Gopalan, Sean Gerrish, Michael Freedman, David M Blei, and
David M Mimno, ‘Scalable inference of overlapping communities’, in
NeurIPS, (2012).

[18] Qirong Ho, Ankur Parikh, Le Song, and Eric Xing, ‘Multiscale commu-
nity blockmodel for network exploration’, in AISTATS, (2011).
[19] Qirong Ho, Ankur P Parikh, Le Song, and Eric P Xing, ‘Inﬁnite hierar-
chical mmsb model for nested communities/groups in social networks’,
Statistics, 1050, 9, (2010).

[20] Qirong Ho, Le Song, and Eric Xing, ‘Evolving cluster mixed-
membership blockmodel for time-evolving networks’, in AISTATS,
(2011).

[21] Weihong Huang, Yan Liu, and Yuguo Chen. Mixed membership stochas-

tic blockmodels for heterogeneous networks, 2018.

[22] Charles Kemp, Joshua B Tenenbaum, Thomas L Grifﬁths, Takeshi Ya-
mada, and Naonori Ueda, ‘Learning systems of concepts with an inﬁnite
relational model’, in AAAI, (2006).

[23] Dae Il Kim, Michael Hughes, and Erik Sudderth, ‘The nonparametric

metadata dependent relational model’, in ICML, (2012).

[24] Myunghwan Kim and Jure Leskovec, ‘Latent multi-group membership

graph model’, in ICML, (2012).

[25] Thomas N Kipf and Max Welling, ‘Semi-supervised classiﬁcation with

graph convolutional networks’, in ICLR, (2017).

[26] Phaedon-Stelios Koutsourelakis and Tina Eliassi-Rad, ‘Finding mixed-
memberships in social networks.’, in AAAI Spring Symposium: Social
Information Processing, (2008).
Jure Leskovec and Rok Sosiˇc, ‘Snap: A general-purpose network analy-
sis and graph-mining library’, ACM Transactions on Intelligent Systems
and Technology (TIST), 8(1), 1, (2016).

[27]

[28] Percy Liang and Dan Klein, ‘Online em for unsupervised models’, in

[29]

NAACL-HLT, (2009).
James Lloyd, Peter Orbanz, Zoubin Ghahramani, and Daniel M Roy,
‘Random function priors for exchangeable arrays with applications to
graphs and relational data’, in NIPS, (2012).

[30] Kurt Miller, Michael I Jordan, and Thomas L Grifﬁths, ‘Nonparametric

[31]

latent feature models for link prediction’, in NeurIPS, (2009).
Ian Porteous, Evgeniy Bart, and Max Welling, ‘Multi-hdp: A non para-
metric bayesian model for tensor factorization.’, in AAAI, (2008).
[32] Daniel M Roy, Yee Whye Teh, et al., ‘The mondrian process.’, in NIPS,

pp. 1377–1384, (2008).

[33] Tracy M Sweet, Andrew C Thomas, and Brian W Junker, ‘Hierarchical
mixed membership stochastic blockmodels for multiple networks and
experimental interventions’, Handbook on mixed membership models
and their applications, 463–488, (2014).

[34] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero,
Pietro Lio, and Yoshua Bengio, ‘Graph attention networks’, ICLR,
(2018).

[35] Eric P Xing, Wenjie Fu, Le Song, et al., ‘A state-space mixed mem-
bership blockmodel for dynamic network tomography’, The Annals of
Applied Statistics, 4, 535–566, (2010).

[36] Yunfeng Xu, Hua Xu, Dongwen Zhang, and Yan Zhang, ‘Finding over-
lapping community from social networks based on community forest
model’, Knowledge-Based Systems, 109, 238–255, (2016).

[37] Hongxia Yang and Aurelie Lozano, ‘Multi-relational learning via hierar-
chical nonparametric bayesian collective matrix factorization’, Journal
of Applied Statistics, 42, 1133–1147, (2015).
Jaewon Yang, Julian McAuley, and Jure Leskovec, ‘Community detec-
tion in networks with node attributes’, in ICDM, (2013).

[38]

[39] Rex Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure
Leskovec, ‘Gnn explainer: A tool for post-hoc explanation of graph
neural networks’, in NeurIPS, (2019).

8

[40]

Jiaxuan You, Rex Ying, Xiang Ren, William L. Hamilton, and Jure
Leskovec, ‘Graphrnn: Generating realistic graphs with deep auto-
regressive models.’, in ICML, (2018).

[41] Yuan Zhang, Tianshu Lyu, and Yan Zhang, ‘Hierarchical community-
level information diffusion modeling in social networks’, in SIGIR,
(2017).

[42] He Zhao, Lan Du, and Wray L. Buntine, ‘Leveraging node attributes for

incomplete relational data’, in ICML, (2017).

[43] Mingyuan Zhou, ‘Inﬁnite edge partition models for overlapping commu-

nity detection and link prediction’, in AISTATS, (2015).

9

