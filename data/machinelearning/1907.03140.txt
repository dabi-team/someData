9
1
0
2

p
e
S
6
2

]

C
O
.
h
t
a
m

[

3
v
0
4
1
3
0
.
7
0
9
1
:
v
i
X
r
a

ReLU networks as surrogate models in mixed-integer linear programs

Bjarne Grimstada,∗, Henrik Anderssonb

aSolution Seeker AS, Gaustadall´een 21, 0349 Oslo, Norway
bDepartment of Industrial Economics and Technology Management, Norwegian University of Science and Technology,
NO-7491 Trondheim, Norway

Abstract

We consider the embedding of piecewise-linear deep neural networks (ReLU networks) as surrogate models
in mixed-integer linear programming (MILP) problems. A MILP formulation of ReLU networks has
recently been applied by many authors to probe for various model properties subject to input bounds.
The formulation is obtained by programming each ReLU operator with a binary variable and applying
the big-M method. The eﬃciency of the formulation hinges on the tightness of the bounds deﬁned by
the big-M values. When ReLU networks are embedded in a larger optimization problem, the presence
of output bounds can be exploited in bound tightening. To this end, we devise and study several bound
tightening procedures that consider both input and output bounds. Our numerical results show that
bound tightening may reduce solution times considerably, and that small-sized ReLU networks are suitable
as surrogate models in mixed-integer linear programs.

Keywords: Deep Neural Networks, ReLU Networks, Mixed-Integer Linear Programming, Surrogate
Modeling, Regression

1. Introduction

Access to larger datasets and more computational power have created vast possibilities for the ap-
plication of machine learning algorithms to data-driven decision support systems studied in operations
research and mathematical programming. We are now able to exploit the data and build highly complex
models to describe nonlinear phenomena. A challenge in mathematical optimization is how to use these
models in an optimization framework and how to eﬃciently solve the problems that arise.

Models that act as substitutes for nonlinear relationships in mathematical programming problems
are often called surrogate models. The study of surrogate models is important for several reasons: 1) it
allows optimization of functions with an unknown/hidden mathematical structure, for which derivative
information is unavailable; 2) it may reduce solution times when complex functions can be substituted by
simpler surrogate models with better properties for optimization (e.g. smoothness and linearity); and 3)
for problems with unknown functions that are in some sense expensive to sample, surrogate models can be
used to handle the trade-oﬀ between exploration and exploitation [1, 2]. The view of surrogate modeling
is thus useful when considering the challenge of embedding machine learning models in mathematical
programs.

Functions with the above properties, which are accessible only via sampling, are called “black-box”
functions. Optimization problems containing a mix of black-box functions and explicitly known functions
(for example algebraic expressions) are often referred to as “grey-box” problems [3]. Problems with this
setup are commonly encountered in process optimization. A promising approach to the optimization of
grey-box problems is to approximate and replace the black-box functions with surrogate models. The
approximate problem can then be eﬃciently solved by derivative-based optimization methods. This is
the approach taken by model-based derivative-free optimization (DFO) methods [4].

Model-based DFO methods can be categorized as local and global DFO methods [5]. Local DFO
methods are specialized to search eﬃciently for a local optimum and often rely on building rather simple
surrogate models. In contrast, global DFO methods build surrogate models over the entire feasible region,
and attempt to locate a global optimum. As global surrogate models are necessarily required to be non-
convex, global optimization algorithms are usually employed to solve the problem [6]. Most DFO methods
incorporate a sampling algorithm that decides at which points to sample the black-box functions. An

∗Corresponding author
Email address: bjarne.grimstad@gmail.com (Bjarne Grimstad)

Preprint submitted to Elsevier

September 30, 2019

 
 
 
 
 
 
initial sample is used to ﬁt the surrogate models, and subsequent samples can be taken to locally reﬁne
the surrogate models before reoptimizing the problem. A review of surrogate modeling and optimization
can be found in [7], and more recently in [5]. [5] also provide an overview of software for building various
surrogate models, including some of the surrogate models we mention below.

Many of the classic function approximation methods have been applied to build surrogate models
for process analysis and optimization [5]. Examples include radial basis functions [8], support vector
machines [9], tensor-product splines [10], and artiﬁcial neural networks [11, 12]. Generally, the resulting
surrogate models are constructed to be continuous and smooth to facilitate optimization by nonlinear
(global) solvers. An important class of surrogate models is the piecewise-linear (PWL) models, which
naturally lend themselves to mixed-integer linear programming (MILP) formulations. Various MILP
formulations exist for PWL models of one or multiple variables, including the ones listed in [13]. The
starting point of these formulations is a set of sample points to be interpolated. As will be discussed
subsequently, some of these formulations put restrictions on the sample structure in addition to the
interpolation constraints. This limits their applicability to low-dimensional functions and relatively small
datasets with a few thousand data points.

When modeling on large and noisy datasets, for example to ﬁt global surrogate models, regression
methods are more suitable than interpolation methods. The modeler can then leverage tools for statistical
analysis and machine learning to build accurate PWL approximations by controlling model complexity to
avoid overﬁtting the data. With appropriate formulations, the resulting regression models can be brought
into a MILP framework for optimization. In this spirit, many works have been published recently on
how to embed trained machine learning models in optimization [14, 15, 16, 17]. For example, [14]
recently employed a MILP formulation for multivariate adaptive regression splines (MARS), while [15]
developed a framework for parameter prediction that incorporates the objective and constraints of the
optimization problem explicitly. The desire to bring surrogate models into a MILP framework stems
from the demonstrated eﬀectiveness of MILP solution methods in solving large-scale instances of complex
problems; see for example [18, 19, 20, 21].

We follow a similar path in this work by employing a MILP formulation for the general class of
ReLU networks. These are networks composed of max-aﬃne spline operators (piecewise-linear and convex
operators), which include: aﬃne operators, ReLU-type activations, absolute value activations, convolution
operators, and max/mean pooling. By construction, ReLU networks are aﬃne spline operators, which
are piecewise linear, but not necessarily convex [22]. In this class of deep neural networks (DNNs) we
ﬁnd widely applied architectures such as convolutional neural networks [23], residual neural networks
[24], inception networks [25], maxout networks [26], network-in-networks [27], and their variants using
max-aﬃne spline operators. These networks are used to model complex relationships and often achieve
state-of-the-art performance in terms of modeling accuracy [28, 29, 30]. In this work we consider ReLU
networks composed solely of aﬃne operators and ReLU activations, since these operators are suﬃcient
for many function approximation tasks; a shallow ReLU network with one hidden layer is a universal
approximator [31].

An exact MILP formulation of a ReLU network can be obtained by programming each ReLU operator
with a binary variable and applying the big-M method. This formulation has recently been applied to
formal veriﬁcation [32, 33, 34, 35, 36], to count linear regions [37], and to compress DNNs [38]. Common
for these works is consideration of a single ReLU network subject to input bounds. These studies show
that the application of bound tightening techniques to compute big-M values can reduce solution times
considerably.

We use the same formulation to solve optimization problems with multiple ReLU networks embedded.
The problems we consider can be viewed as grey-box problems, where ReLU networks are used as global
In this setting, multiple ReLU networks are interlinked by algebraic constraints,
surrogate models.
eliciting bounds on the network outputs. The studies listed above are limited to bound tightening
techniques that propagate input bounds forward through a single network. To take advantage of the
output constraints present in grey-box problems, we study stronger bound tightening techniques that are
capable of propagating output bounds backwards. The consideration of bound tightening in the presence
of output bounds, and the optimization of problems with multiple ReLU networks, distinguishes our
study from previous works.

The focus of this paper is to show how ReLU networks can be used to model complex phenomena
in a mixed-integer linear framework and how bound tightening aﬀect the performance of the big-M
formulation. Our main contributions are summarized as follows:

– A discussion about the advantages of using ReLU networks to model complex phenomena in a

MILP framework.

2

– A framework for tightening bounds that unify diﬀerent bound tightening procedures for output-

bounded ReLU networks under one general procedure.

– A computational study showing the strength of the bound tightening procedures, but also the im-
portant trade-oﬀ between time spent on bound tightening and the solution time of the optimization
problem.

– A real application where bound tightening makes the diﬀerence between not ﬁnding a feasible

solution and solving the problem to optimality within the practical limit on solution time.

The literature contains numerous applications of neural networks to model and locally optimize com-
plex processes; a list all previous works would be quite comprehensive so we only point to a few examples
here [39, 40, 41]. We ﬁrmly believe that ReLU networks, which are the de-facto standard in deep learning
[23], is an important class of surrogate models that deserves being studied. Our work is also motivated by
the desire to pair state-of-the-art software for machine learning with mixed-integer linear optimization.
In particular, we wish to leverage powerful modeling frameworks like TensorFlow [42] to build PWL
surrogate models, and then incorporate these in a MILP model that we can solve with a state-of-the-art
MILP solver.

Our assessment is on the applicability of ReLU networks as global surrogate models in a grey-box
approach to process optimization. We do not consider sampling to reﬁne and reoptimize the problem, but
focus on building highly accurate approximations to be used in a single global optimization. However,
our ﬁndings can be used to develop more sophisticated model-based global DFO methods based on ReLU
networks.

1.1. Structure of the paper

We begin in Section 2 by discussing the various aspects of building piecewise-linear approximations
of nonlinear functions, and embedding these as surrogate models in optimization. We highlight the
advantages of modeling multi-variable functions via ReLU networks, as opposed to using interpolation
on simplices, which has been the traditional approach in MILP literature. After presenting the MILP
formulation in Section 3, we devise several optimization-based bound tightening procedures for output-
bounded ReLU networks in Section 4. To study the procedures’ eﬃciency, and the feasibility of using
ReLU networks as surrogate models in process optimization, we present a computational study in Section
5. Our test suite includes a challenging oil production optimization problem, for which the complicated
physics of multiphase ﬂow in pipes is modeled by ten ReLU networks. Concluding remarks and promising
research directions are given in Section 6.

2. Piecewise-linear approximations of nonlinear functions

We consider the modeling of a nonlinear function f : D

Rn
(we assume that D is a polytope). To incorporate f in a MILP model, it must be approximated by a
piecewise-linear function. For cases where f is unknown, nonseparable (for n
2), or highly complex, a
general approach to the approximation of f is to sample it on the domain D, and then build a piecewise-
linear approximation from the sample. In cases where f is not a mathematical construct, for example
if f is a real process, the sample may represent a set of experiments. In general, a sample consists of
non-structured sample points scattered in D.

R, on a compact domain D

Rn

→

⊂

⊂

≥

A common restriction of many approximation methods is that f must be sampled on a rectilinear grid
G, covering a hyper-rectangular domain D. A rectilinear grid on which variable xi is partitioned into mi
intervals, results in (cid:81)n
i=1 mi boxes (hyperrectangles). Clearly, the number of boxes grows exponentially
with n. To obtain a piecewise linear model that interpolates the sample points, each of these boxes
must be divided into a set of simplices. The minimum number of simplices needed to triangulate an
n-dimension hypercube is 1, 2, 5, 16, 67, 308, and 1493 for 1
7, respectively, see [43]. In general,
an upper bound is given by n!. Thus, a partitioning of D may consist of up to n! (cid:81)n

≤

≤

n

i=1 mi simplices.

Table 1 lists the lower bound on the number of simplices resulting from a rectilinear grid partitioning.
The bounds are given for 1
7, with each variable partitioned into ten intervals. Reading the table,
we see that the exponential increase in the number of boxes (or simplices) prohibits any practical use of
such a partitioning for n > 3. For example, [44] and [13] consider grids for n

≤

≤

3.

n

Another disadvantage with rectilinear sampling, stemming from its structure, is its inability to locally
control the sampling resolution. That is, increasing the sampling resolution in a region of D where f
large gradients), will also increase it in other regions where it may not
has important features (e.g.

≤

3

Table 1: Number of simplices resulting from a rectilinear grid partitioning of the domain, where each variable is partitioned
into ten intervals. We have used the lower bound on the number of simplices per box.

Dimension n

1
2
3
4
5
6
7

# boxes

10
100
1 000
10 000
100 000
1 000 000
10 000 000

# simplices/box

1
2
5
16
67
308
1 493

# simplices

10
200
5 000
160 000
6 700 000
308 000 000
14 930 000 000

be necessary to sample densely. Rectilinear sampling is also subject to high sparsity of sample points
in higher dimensions. Although this aspect of the “curse of dimensionality” applies to all sampling
methods, it is becomes especially prominent for rectilinear sampling due to its inability to locally control
the sampling resolution.

For high-dimensional problems, there exist sampling algorithms with better space-ﬁlling properties
than rectilinear sampling, such as Latin hypercube sampling. The sampling strategy can either be static,
as in rectilinear sampling, or sequential. With sequential sampling, sample points are successively selected
to optimize a space-ﬁlling criterion or to minimize the (expected) approximation error of a surrogate
model [45, 46, 47]. In the latter case, the properties of the surrogate model can be exploited to reduce
the sample size [48]. These sampling stategies cannot escape the “curse of dimensionality”, but may scale
more graciously to higher dimensions by being economic in the placement of the sample points. This
ability is especially important when f is expensive to evaluate and there are practical limits to the sample
size. We refer the reader to [49] for a thorough review of modern sampling algorithms.

Sampling algorithms like those described above typically result in sample points that are scattered in
D without any particular structure. The same holds true for most samples obtained from running real
experiments. A piecewise-linear model can be obtained from such samples via triangulation of the sample
points, resulting in a simplex partitioning of D. There exist several MILP models for PWL functions that
have no requirements on the family of polytopes/simplices produced by the triangulation, and which can
be used with a triangulation scheme for scattered sample points. One example is the DLog model in [13],
which uses a logarithmic number of binary variables to model a PWL function. This model has shown
good computational performance for higher-dimensional functions (n
2), compared to MILP models
with a linear number of binary variables. However, as is shown subsequently, this modeling approach is
not feasible for the oil production optimization case considered in this paper.

≥

There exist MILP formulations for PWLs that achieve higher eﬃciency by putting requirements on
the partitioning scheme. E.g., the Log model of [13] requires a triangulation scheme compatible with the
J1 (“Union Jack”) triangulation, while SOS2 models require a rectilinear partitioning [50]. In general,
these highly structured partitioning schemes do not conform with a sample of scattered points and are
also subject to the dimensionality issues discussed above.

So far we have only discussed approximation methods that interpolate the sample points.1 If this
requirement is relaxed, we may consider regression methods using piecewise-linear splines. These methods
solve a regression problem by minimizing the approximation error on the sample. We may divide these
methods into two classes: non-adaptive methods for which the domain partitioning is ﬁxed (given by
break-points or knots); and adaptive methods where the model parameters deﬁne the polytopic partition
of the domain and the linear relationship in each polytope. MARS and ReLU networks are examples of
adaptive methods.

Regression models oﬀer a nice alternative to the interpolation methods in that their capacity (number
of linear pieces) is easily controlled. This is a key property when building predictive models, as it
allows the modeler to strike a balance between underﬁtting and overﬁtting the data. Additionally, it
can be advantageously used to reduce unnecessary complexity of surrogate models in a mathematical
optimization problem. As the number of binary variables in a MILP formulation scales directly with the
number of partition regions (number of linear pieces), it is reasonable to expect that surrogate models with
fewer regions can be optimized faster. In accordance with the principle of Occam’s razor, unnecessary
complexity, which here translates to regions that do not signiﬁcantly lower the model accuracy, is favorably
avoided in both the modeling and optimization setting.

1We note here that for n ≥ 2, SOS2 models do not interpolate all sample points.

4

2.1. ReLU networks as piecewise-linear surrogate models

Below, we highlight some important properties of ReLU networks that make them attractive as

surrogate models in optimization.

Piecewise-linear and continuous. ReLU networks are composed of max-aﬃne spline operators and are
piecewise-linear and continuous functions [22]. This means that a ReLU network can be exactly for-
mulated and optimized in a MILP framework. Networks composed of other nonlinear operators, such
as sigmoid or hyperbolic functions, can only be formulated approximately in a MILP framework via
piecewise-linear approximation of the operators.

Suited for modeling of high-dimensional and complex functions. It is easy to control the capacity of a
ReLU network by adjusting its depth and width. As depth and width increases, so does the number of
nodes (also called neurons) and eﬀectively the capacity of the model. An upper bound on the number
of regions (linear pieces) generated by a neural network with L hidden layers of width W and inputs
Rn0, is O(W Ln0) [51]. For ﬁxed W and n0, this upper bound indicates that the number of regions
x
grows exponentially in the network depth L. The bound shows that deep networks can be more expressive
than shallow networks (can be seen by keeping W L constant and varying L). This observation has been
used to partly explain the success of DNNs [37].

∈

Adaptive partitioning of the domain. Changes in the parameters of a ReLU network automatically induce
changes in the partition of the domain [22]. When the network parameters are updated during training,
regions of the domain where the data has important features are automatically described with smaller
polytopes, while other parts of the domain are described by larger polytopes. Figure 1 shows an example
of a domain partition, which is reﬁned from one layer to the next.

Figure 1: Example of domain partition generated by a multilayered neural network; adopted from [51].

Can be trained on scattered and noisy data. The process of training a ReLU network does not put
requirements on the distribution of the data points and is thus compatible with any sampling algorithm.
Furthermore, an arsenal of regularization techniques is available for neural networks in general. These
techniques allow neural networks to generalize from noisy data [23]. Scattered and noisy data are issues
that occur in many practical applications.

Scale to large datasets. Neural networks are usually trained with (mini-batch) stochastic gradient descent
algorithms [52]. These algorithms optimize over the parameters Θ of a neural network fΘ in search of an
approximation of some true function f . For regression tasks, it is common to minimize the empirical loss

=

L

1
N

N
(cid:88)

i=1

y(i)
(cid:107)

2
fΘ(x(i))
2 + λ
(cid:107)

Θ
(cid:107)

2
2,
(cid:107)

−

x(i), y(i)
{

N
i=1 are N samples of f . The ﬁrst term speciﬁes the mean squared error, while the second
where
}
is an L2 regularization term that penalizes large values of Θ. In addition to the network width and depth,
R≥0 is used to control model complexity and encourage a low generalization error.
the hyperparameter λ

∈

5

During optimization, the parameters are updated using gradients ∂

/∂Θ computed using backward-
propagation. The batch size N can be selected to control the computation and memory resources required
to evaluate
and its gradients. This enables training of deep neural networks on large datasets with
millions of data points.

L

L

Many software tools available for modeling. A plethora of software tools exists for designing, training and
performing inference tasks with ReLU networks. In this work we use the machine learning framework
TensorFlow [42].

3. A 0–1 MILP model for ReLU networks

We consider a ReLU network with K + 1 layers, numbered from 0 to K. Layer 0 is the input layer

(usually not counted as a layer), while the last layer K is the output layer. Each layer k
}
has nk nodes, numbered from 1 to nk. For simplicity we consider networks of fully connected layers with
Rnk . We state the network
parameters θk = (W k, bk) for k
as fΘ : Rn0
Let xk

∈
RnK , where K, n0, and nK are implied by the set of parameters Θ := (cid:8)θ1, . . . , θK(cid:9).

, where W k
}

Rnk×nk−1 and bk

1, . . . , K

0, . . . , K

∈ {

∈ {

∈

j the output of the j-th node for j = 1, . . . , nk. Following
is the j-th output value (out of nK) of the network. For

→
Rnk be the output of layer k, and xk
j is the j-th input value, and xK
j

∈

this notation, x0
each (hidden) layer k

1, . . . , K

∈ {

1
}

−

, the output vector xk is computed as

xk = σ(W kxk−1 + bk),

(1)

where we denote by σ(y) := max
0, y
{
y. The output of the network, xK

(componentwise) the ReLU activation function for a real vector

}
RnK , is given by the aﬃne equation xK = W KxK−1 + bK.

Figure 2 shows the structure of a ReLU network. The output of layer k is calculated using the input

∈

xk−1 from the previous layer and the parameters θk = (W k, bk) according to (1).

1

K

−
xK−1

Output layer
K

xK

θK

Input layer
0

x0

θ1

1

x1

θk−1

xk−1
1

xk−1
2

xk−1
nk−1

Hidden layers
k

1

k

−
xk−1

xk

θk

θK−1

bk
1

xk
1

bk
2

xk
2

bk
nk

xk
nk

W k
11

Wk
21

W

k

n

k

1

W k

nknk−1

Figure 2: The structure of a ReLU network. The top part shows the aggregated structure from the input vector x0 to
the output vector xK . The lower part shows a disaggregated view of two layers where the variables and parameters of the
network are shown.

The ReLU operator can be programmed in multiple ways. Following [36], we consider the linear

equation

wTy + b = x

s,

−

(2)

where the output of the ReLU is decoupled into a positive part x
0. The
output of the ReLU can then be obtained by imposing that at least one of the two terms x and s must be

0 and negative part s

≥

≥

6

zero. Assuming that we may compute ﬁnite values L and U so that L
program the ReLU logic via big-M constraints:

≤

wTy + b

≤

U , we may explicitly

x

s

z

U z

≤

L(1
0, 1
}

≤ −

∈ {

z)

−

(3)

where we have introduced a binary activation variable z.

Using a binary activation variable for each node (j, k), the network can be expressed with the following

0–1 MILP formulation:

0

≤

≤

−

U 0,

Input layer
x0
L0
Hidden (ReLU) layers
W kxk−1 + bk = xk
xk, sk
zk
j ∈ {
xk
j ≤
sk
j ≤ −

≥
0, 1
}
j zk
U k
j
Lk
j (1
Output layer
W KxK−1 + bK = xK,
LK

U K.

zk
j )

xK

(cid:41)

−

≤

≤

(cid:41)

sk

k = 1, . . . , K

1,

−

k = 1, . . . , K

k = 1, . . . , K

−

−

1, j = 1, . . . , nk

1, j = 1, . . . , nk

(4a)

(4b)

(4c)

(4d)

(4e)

(4f)

This is an exact formulation of the ReLU network, meaning that the output of this formulation always
is the same as the output from the ReLU network for the same input. That is, for a ﬁxed input vector
x0, all the other variables in (4) are ﬁxed, including the output variables xK. The only “degenerate”
solutions occur when the input to a node (j, k) is zero, in which case the value of zk
j can arbitrarily be
set to 0 or 1 without aﬀecting the output.

There is one binary variable for each hidden node in the network, this means that the formulation
scales linear with the number of hidden nodes. The constraint sets (4b) and (4e) can be written as
Axx + Ass =
b, where Ax and As have a special block angular structure (depends on how we stack the
variables x and s), see Figure 3. Unless sparsity is encouraged, for example by utilizing L1 regularization,
the parameters W k and bk are typically dense for a trained network.

−

x0
W 1

s1
I

x1
I

−
W 2

s2

I

x2

I

−

xK−2

sK−1

xK−1

xK

W K−1

I

I

−
W K

I

−

=

=

=

=

b1

b2

−

−

bK−1

bK

−

−

Figure 3: The block-angular structure of the constraint set of a ReLU network.

The tightness of an LP relaxation of (4) depends heavily on the size of the big-M values Lk and U k,
for layers k = 1, . . . , K
1. Weak LP relaxations result from large big-M values, and may severely hamper
the eﬃciency of the solver. We explore several bound tightening procedures in the upcoming section.
The computed bounds, Lk
j , constrain the variables as follows:

j and U k

−

xk
j ≤
sk
j ≤
Before continuing, we remark that alternative formulations exist for the ReLU logic. The logic may

0, U k
j }
{
Lk
0,
j }
{

0, Lk
{
U k
0,
{

j } ≤
j } ≤

max

max

max

max

(5)

−

−

for example be programmed via complementary constraints (xs = 0) or indicator constraints (z = 0 =

⇒

7

≤

0 and z = 1 =

x
0). An advantage with these formulations is that they do not require
explicit big-M values. However, these formulations tend to produce very hard mixed-integer instances
that challenge state-of-the-art solvers [36].

⇒

≤

s

4. Bound tightening procedures

We study procedures for computing valid bounds B := (cid:8)B0, . . . , BK(cid:9), where Bi
i = 0, . . . , K. We begin by deﬁning the feasible set of a ReLU network fΘ bounded by B as

:= [Li, U i] for

P (Θ, B) :=

(x, s, z) : (4) is satisﬁed
{

}

.

(6)

The projection of this set onto (x0, xK), given by F (Θ, B) :=
, yields
all possible input-output pairs of fΘ. The inputs x0 and outputs xK are directly constrained by the
bounding boxes B0 and BK, respectively.

(x0, xK) : (x, y, z)
{

P (Θ, B)
}

∈

Now consider the additional constraints x0

polytopes. We then have that

D and xK

∈

∈

E, where D

∈

Rn0 and E

RnK are

∈

(x0, xK) : x0
{

∈

D, xK

∈

E, (x, y, z)

F (Θ, B)

∈

} ⊆

F (Θ, B)

(7)

The reason for making this, perhaps obvious point, is that a DNN programmed by (4) may be part of
a larger optimization problem with additional constraints on x0 and xK. These constraints may tighten
variable bounds and should be considered when performing bound tightening.

The previous works on bound tightening (BT) of (4), listed in the introduction, did not include output
bounds E. The primary goal has been to search in F (Θ, B) subject only to input bounds D, which explains
the focus on forward-propagating BT procedures. To ﬁnd optimal bounds with the addition of output
bounds E, it is required that the output bounds are propagated backwards through the DNN. In the
following, we study several bound tightening procedures for (4) with D and E included. We will highlight
the procedures that are capable of propagating bounds backwards and subsequently investigate the eﬀect
of backwards propagation of bounds. We denote the directions of bound propagation by forward bound
propagation (FBP) and backward bound propagation (BBP), respectively.

To simplify the notation, we assume in the following that D and E are hyperrectangles included in

B0 = [L0, U 0] and BK = [LK, U K].

4.1. Relaxations of ReLU networks

We deﬁne the feasible set of the LP relaxation of (4) as

RR(Θ, B) :=

(x, s, z) : (4a), (4b), (4d)
{

≤
where the binary restrictions on z are replaced by 0
1. This relaxation can be interpreted as
a relaxation of the ReLU logic, and we refer to it as ReLU relaxation. The ReLU relaxation makes it
possible to increase the output from the ReLU and we get the following bounds on the output from node
j of layer k:

−
z

≤

≤

≤

}

(4f) are satisﬁed, 0

z

1

,

(8)

(W kxk−1 + bk)j

xk
j ≤

U k
j

≤

(W kxk−1 + bk)j
Lk
U k
j

j −

Lk
j

−

(9)

We note two things from (9). First, even if (W kxk−1 + bk)j < 0 the unit can still give a positive output
in the ReLU relaxation. Second, the upper bound depends on both U k
j or
increase in Lk

j directly lowers the bound. This clearly motivates a study of BT procedures.

j , and a decrease in U k

j and Lk

We use the notation RR(Θ, B, I) to denote a partial ReLU relaxation for which the ReLUs of nodes
(j, k) in the index set I are relaxed. A MILP problem results from a partial ReLU relaxation. We let the
LP relaxation RR(Θ, B) correspond to RR(Θ, B, I) with all nodes indexed by I.

When considering the bound tightening of a node (j, k), it is possible to relax P (Θ, B) by removing
constraints related to other layers than k from (4). We may for example remove all layers following k
(k + 1, . . . , K) to simplify the problem while retaining FBP from layer 0 through k. A weaker relaxation
can be obtained by removing all layers but k and k
1. This relaxation can be used to propagate bounds
−
forward one layer at the time. In both these cases, where all layers following k are excluded, node (j, k)
can be considered an output node. We may then also remove the constraints of all the other nodes in
layer k to reduce the problem size. We name this type of relaxation layer relaxation and denote it by
RL(Θ, B, I), where we specify by the index set I which nodes to remove/relax.

For the relaxations discussed above, we have that P (Θ, B)

RL(Θ, B, I)
for any index set I. Thus, a BT procedure utilizing these relaxation produces valid bounds; i.e., the
tightened bounds do not diminish the set of input-output pairs F (Θ, B).

RR(Θ, B, I) and P (Θ, B)

⊆

⊆

8

4.2. Prototype for bound tightening procedures

The bound tightening procedures we discuss subsequently are special cases of the prototype in Algo-

rithm 1.

Algorithm 1 Prototype for bound tightening procedures
Require: Parameters Θ, initial bounds B− and scheme S

B−
B
←
k
0
←
K do
for k
≤
j
1
←
for j

nk do

≤

Build constraint set C k
Solve for upper bound uk
Solve for lower bound lk
Update bounds B: U k
j
←
end for
k
←
end for
return Tightened bounds B

k + 1

j + 1

j ←

j from Θ and B following S

j s.t. C k
j

j = max tk
j = min tk
j and Lk
uk

j s.t. C k
j
lk
j
j ←

(cid:46) Iterate over layers

(cid:46) Iterate over nodes

Given a network parameterized by Θ, some initial bounds B− and a scheme S, the prototypical
procedure computes tightened bounds B. The scheme S state for each node (j, k) which constraint set
C k
j represents a subset of the constraints in (4), or a valid relaxation of these.
tk
uk
j , by minimizing and maximizing
j ≤

j of each node (j, k) are computed so that lk

j that should be used. C k
j and uk
j deﬁned as:

Bounds lk
the variable tk

j ≤

tk
j :=






xk
j
xk
j −
xk
j

sk
j

k = 0
k = 1, . . . , K
k = K

1

−

(10)

If a strictly positive lower bound is found for a hidden node (j, k), i.e. Lk
j > 0, the corresponding
j = 1 and sk
j = 0 (or removing them from the problem). Similarly, if
j = 0; for this case we say that the neuron (j, k) is dead. This eﬀectively

ReLU may be relaxed by ﬁxing zk
j = 0 and xk
U k
reduces the number of binary variables in the formulation.

j < 0, we may ﬁx zk

When tightening the bounds of (4), we should consider in which order to processes the nodes. For
FBP, it is natural to process nodes in the order of layers, beginning with the nodes in the ﬁrst hidden
layer (k = 1). However, when bound information can ﬂow backwards from the output bounds BK, it
is not obvious in which order the nodes should be processed. In Appendix A, we provide an argument
for using the same strategy also when bound information may propagate backwards.2 The argument is
that, since BBP is much weaker than FBP, the bound tightening procedure should focus on propagating
bounds forward. In line with this argument, Algorithm 1 makes one forward pass through the layers,
processing the nodes of each layer in the order they are indexed. We may apply the argument again when
considering the ordering within each layer. Since the only interaction between the nodes in a layer is via
BBP on subsequent nodes, diﬀerent processing orders will likely perform similarly.

We note that k is initialized to 0, since the input bounds [L0, U 0] may be tightened by the output
j allow for BBP. For instances of the procedure for which BBP cannot

bounds when the constraint sets C k
occur due to the choice of constraint set C k

j , k may be initialized to 1.

4.3. Feasibility-based bound tightening (FBBT)

FBBT subsumes BT procedures that use purely primal feasibility arguments and remove parts of the
domains in which no feasible solutions are contained [53]. These procedures rely on interval arithmetic
to compute the bounds on constraint activations over the variable domains, and conversely, propagating
the bounds on the constraint activities back to the variable domains.

2These procedures must not be confused with the forward and backward propagation used in inference and training of

neural networks.

9

Using simple interval arithmetic we may infer bounds lk

j ≤

tk
j ≤

uk
j for node (j, k) in layer k

2 as

≥

follows:

uk
j =

lk
j =

nk−1
(cid:88)

i=1
nk−1
(cid:88)

i=1

max (cid:8)wk

ji max
{

0, U k−1
i

, wk
}

ji max
{

0, Lk−1
i

(cid:9) + bk
j ,
}

min (cid:8)wk

ji max
{

0, U k−1
i

, wk
}

ji max

0, Lk−1
i
{

}

(cid:9) + bk
j ,

(11)

where we have utilized the bounds on xk−1
problems:

j

in (5). The same bounds can be found by solving the LP

j = max (cid:8)tk
uk
j = min (cid:8)tk
lk

j : tk
j : tk

j ∈
j ∈

C k
j
C k
j

(cid:9) ,
(cid:9) ,

(12)

for the constraint set
j = (cid:8)tk
C k

j : tk

j = wk

j xk−1 + bk, xk−1

0, Lk−1
[max
{

, max
}

0, U k−1
{

]
}

∈

⊂

Rnk−1 (cid:9) .

(13)

To compute the bounds in the ﬁrst hidden layer k = 1, we remove the inner max-operators in (11) or
(13), since there is no ReLU operator on the input layer.

For a DNN, we may propagate the input bounds forward by solving (11) or (12), and then update the
bounds according to (5) for units in successive layers, beginning with the units in layer k = 1. That is,
we follow the prototypical procedure in Algorithm 1, but initialize with k = 1. This procedure employs
both layer and ReLU relaxations, as seen from C k
j , and we refer to it as LRR in the rest of this paper.
The procedure is computationally cheap since it considers only the constraints in (4b) for the node being
tightened.

In general, feasibility-based procedures compute bounds that are sub-optimal, due to the weak re-
laxations. Stronger BT procedures can be devised by including more constraints and exploiting integer
information. For example, the LRR procedure propagates bounds in the forward direction only, and may
not take advantage of the output bounds BK. To do so, a BT procedure must be able to perform BBP,
either by starting at the last layer, or by including enough constraints to link the variable domain being
tightened to the output bounds. Next, we consider some alternative BT procedures that use stronger
relaxations, but are more computationally demanding.

4.4. Optimization-based bound tightening (OBBT)

We devise several OBBT procedures for ReLU networks using the relaxations in Section 4.1. These
k=0 nk optimization problems to ﬁnd the tightest variable bounds

procedures rely on solving a series of 2 (cid:80)K
on relaxations of (4).

RR procedure. First, we consider an OBBT procedure using the LP relaxation RR(Θ, B) of (4). The
procedure computes bounds on tk
j by solving the optimization problems:

j = max (cid:8)tk
uk
j = min (cid:8)tk
lk

j : (x, s, z)
j : (x, s, z)

RR(Θ, B)(cid:9) ,
RR(Θ, B)(cid:9) ,

∈

∈

j = RR(Θ, B) in Algorithm 1, we obtain an LP-based

for k = 0, . . . , K and j = 1, . . . , nk. Setting C k
OBBT procedure which we call RR.

The subsequent procedures require solving 2 (cid:80)K

k=0 nk MILP problems of increasing complexity (the

number of constraints and binary variables increase with layer depth).

LR procedure. Next, we consider a procedure that utilizes layer relaxations RL(Θ, B, I k
for node (j, k), we construct the index set I k
0, . . . , k
(j, k) and nodes in preceding layers
k = 1 and set C k

j ). When solving
j so that the constraints are removed for all nodes except
. In the framework of Algorithm 1, we initialize from
1
}

j ). We name this procedure LR since it only utilizes layer relaxations.

j = RL(Θ, B, I k

−

{

SEMI-RR procedure. The LR procedure removes all layers following a node (j, k) and may not propagate
bounds backwards. To address this potential weakness, we devise a procedure that keeps all following
layers, but relaxes their ReLU operators. The procedure is given by Algorithm 1 with C k
j ),
where I k
(excluding layer K since it is aﬃne). The procedure
is named SEMI-RR since it relaxes the ReLUs in layer k onwards.

j indexes the nodes in layers

j = RR(Θ, B, I k

k, . . . , K
{

1
}

−

10

NO-R procedure. Finally, we may compute the tightest possible bounds of a ReLU network by using the
full constraint set in (4); i.e. we set C k
j = P (Θ, B). The procedure does not relax any constraints and
we therefore name it NO-R.

4.5. Summary of bound tightening procedures

The various BT procedures discussed above employ two diﬀerent types of relaxations. They either
relax the integrality constraint on the binary variables z (ReLU relaxation), or remove nodes or layers
and their respective constraints (layer relaxation), or utilize both types of relaxations. The procedures,
which we have named based on the relaxations they employ, are listed in Table 2.

BT procedure BT type

Subproblem class

Layer relaxation

ReLU relaxation

Table 2: Bound tightening procedures.

LRR
RR
LR
SEMI-RR
NO-R

FBBT
OBBT
OBBT
OBBT
OBBT

LP/interval arithmetic
LP
MILP
MILP
MILP

(cid:88)
–
(cid:88)
–
–

(cid:88)
(cid:88)
-
*
–

*ReLUs of preceding nodes are not relaxed.
**BT procedure can propagate output bounds backwards through the network.

Backward
propagation**

–
(cid:88)
–
(cid:88)
(cid:88)

Let BLRR be the bounds found by running LRR, BRR by RR, and so on. By inspecting Table 2
MAD(BLRR). Likewise, we have that MAD(BNO-R)

we see that MAD(BRR)
≤
MAD(BLR), given that subproblems are not time limited. In the absence of output bounds, we have
that MAD(BNO-R) = MAD(BSEMI-RR) = MAD(BLR), and LR is clearly the cheaper procedure. It is
not trivial to compare the LP-based procedures with MILP-based procedures since they employ diﬀerent
relaxations. However, we do have that MAD(BSEMI-RR)
MAD(BRR), again assuming that subproblems
are not time limited.

MAD(BSEMI-RR)

≤

≤

≤

Several of the presented BT procedures occur in other works on veriﬁcation of neural networks; namely,
the LRR, RR, and LR procedures were employed in [35, 33, 36] for forward propagation of bounds. While
NO-R simply uses the full constraint set in (4), the SEMI-RR has not to our knowledge been presented
before.

We also note that multiple invocations of the RR or SEMI-RR procedure may strengthen the bounds.
The additional tightening is possible due to the BBP ability of these procedures. I.e., the computation
of a node’s bounds is aﬀected by the tightened bounds on deeper nodes from earlier runs. The eﬀect was
demonstrated in [35] for the RR procedure.

Remark 1 (Pre-computing bounds). Due to their computational cost, we only invoke the BT proce-
dures once before optimization (or at the root node of the branch-and-bound tree). The bounds can then
be stored and reused in subsequent optimizations. We rely on the solver’s bound tightening capabilities
further down the branch-and-bound tree.

Remark 2 (Subproblem time limit). For the MILP-based procedures LR, SEMI-RR, and NO-R we
may limit the solution time of each subproblem to reduce the overall computational cost. For subproblems
that do not terminate within this time limit we use the best bound found by the solver, ensuring that the
computed bounds are still valid. We introduce a naming convention where we postﬁx the BT procedure
name by the subproblem solution time limit (in seconds). For example, we write LR(60) for the LR
procedure with the solution time of the subproblems limited to 60 seconds.

Remark 3 (Bound initialization). We initialize all OBBT procedures with bounds computed by
LRR. This initialization is cheap and gives the procedures an identical starting point. Furthermore,
it allows us to compare the procedures with the mean relative distance (MRD) statistic in Appendix B.

5. Numerical study

We provide a computational study to evaluate the feasibility of using ReLU networks as surrogate
models in mixed-integer programs. We focus primarily on solution times for diﬀerent network archi-
tectures and bound tightening procedures in our investigation. The BT procedures in Section 4 are

11

compared based on computational eﬃciency and the statistics in Appendix B, as well as optimization
solution time.

We ﬁrst evaluate the BT procedures’ ability to propagate output bounds backwards on a set of
randomly generated neural networks. Next, we solve a series of increasingly challenging optimization
problems to test the practical performance of the big-M formulation in (4) with bounds computed by the
proposed BT procedures. We ﬁnally solve an oil production optimization problem including ten ReLU
network surrogate models.

The neural networks are initialized and trained using TensorFlow [42]. We solve all optimization
problems using Gurobi 8.1 with default settings [54], on a machine equipped with an Intel Core i7-8700K
processor and 32 GB of RAM memory.

5.1. Bound tightening for randomly initialized ReLU networks with output bounds

We generate ten networks with layers (3, 20, 20, 10, 1) to investigate the BT procedures’ ability to
propagate output bounds backwards. The networks parameters are initialized using the method by [29],
so that for inputs x0
1, 1]3 and
output bounds to BK = E100 := [
1, 1]. We then proceed by running the procedures for diminishing
output bounds: E75 = [
. The
}
results are shown in Figure 4, where time and MAD values are averaged over the ten generated networks.

(0, 1). We set the input bounds to B0 = [

0.25, 0.25], and E0 = [0, 0] =

−
0.75, 0.75], E50 = [

(0, 1), the output xK

0.5, 0.5], E25 = [

∼ N

∼ N

0
{

−

−

−

−

(a)

(b)

Figure 4: Average MAD (a) and solution time (b) resulting from running the various BT procedures on ten neural networks.
NO-R is the only procedure that signiﬁcantly reduces the MAD as the output bounds are diminished.

Comparing the MAD values in Figure 4a for E100, we see that LRR produces the loosest bounds,
followed by RR. The MILP-based BT procedures, LR, SEMI-LR, and NO-R, obtain the same average
MAD value. This is due to the fact that the output bounds E100 have no tightening eﬀect on preceding
B0. However, as the output bounds are diminished, we see that
units; that is, f (x)
the average MAD decreases for NO-R. For comparison, we report the average MAD values for E0 and
E100 (denoted MAD0 and MAD100) in Table 3. As reported in the table, NO-R is the only BT procedure
capable of signiﬁcantly reducing the MAD by propagating the E0 bounds backwards.

E100 for all x

∈

∈

Table 3: Average MAD for output bounds E0 and E100.

BT procedure

LRR
RR
LR
SEMI-LR
NO-R

MAD100

3.65616
1.94224
1.25684
1.25684
1.25684

MAD0

3.65616
1.91441
1.25684
1.24049
0.95079

100 × MAD0/MAD100

100.00
98.57
100.00
98.70
75.65

12

E100E75E50E25E0Output bounds1.01.52.02.53.03.5MADLRRRRLRSEMI-LRNO-RE100E75E50E25E0Output bounds01020304050Time (sec)LRRRRLRSEMI-LRNO-RTurning to Figure 4b, we see that the average solution times of the BT procedures are ordered as
expected. The solution time increases with the number of constraints included in the BT procedure, and
jumps considerably as binary variables are included. The ordering is the same as in the legend of the
ﬁgure, LRR having the lowest and NO-R the highest solution times on average. The most interesting
observation in this ﬁgure is that the solution times for NO-R increases as the output bound is tightened,
while the other BT procedures seem to be unaﬀected. Possible explanations are that NO-R, to an
increasing degree, must propagate the output bounds backwards, thus linking more variables (nodes) in
the MILP problems and that ﬁnding feasible integer solutions are harder with a tight output bound.
Furthermore, the constraints through which the output bounds must be propagated backwards are likely
quite weak (see discussion in Appendix A), which may aﬀect the numerical performance of the MILP
solver.

5.2. Optimization of approximated n-dimensional quadratic functions

We consider n-dimensional quadratic functions

q(x) = x(cid:62)Ax + b(cid:62)x + c,

∈

∈
∼ N

Rn×n, b

Rn, and c

Rn, A
(0, 1),
where x
and c
(0, 1). The quadratic functions resulting from this construction are likely to be indeﬁnite and
non-separable, and thus diﬃcult to optimize. Furthermore, the higher variance on the coeﬃcients of the
quadratic terms increases the curvature of the generated functions. Due to the curvature, many pieces
are required to obtain an accurate piecewise-linear approximation of these functions.

R. The coeﬃcients are drawn as Aij

(0, 5), bi

∼ N

∼ N

∈

∈

For n = 1, . . . , 6, we generate ten quadratic functions and approximate them by ReLU networks with
the conﬁgurations given in Table 4. The table also lists the number of training samples, and the resulting
mean absolute percentage error (MAPE) averaged over the ten models. One of the two-dimensional
approximations is shown in Figure 5. For the displayed case, the drawn A matrix is indeﬁnite, leading
to a saddle surface.

n

1
2
3
4
5
6

Table 4: ReLU network approximations of quadratic functions.

Layers

# parameters

# training samples

MAPE (%)

(1, 10, 5, 1)
(2, 20, 10, 1)
(3, 40, 20, 1)
(4, 50, 20, 1)
(5, 50, 30, 30, 1)
(6, 80, 40, 40, 1)

81
281
1 001
1 291
2 791
5 481

100
500
2 000
5 000
10 000
20 000

Using the ReLU approximations, we consider optimization problems on the following form:

min

fΘ1(x) : fΘ2(x) = α, x
{

∈

[

1, 1]n

−

,

}

2.3
3.5
3.7
3.7
2.6
2.0

(Qn)

where fΘ1 and fΘ2 are ReLU networks, and α is a real constant. For n = 1, . . . , 6, we construct ﬁve
optimization problems by pairing the ReLU networks described above.

We set the objective function to fΘ1 , and optimize under the constraint that the second network, fΘ2,
must intersect a plane at level α at the solution. Notice that the feasible set, speciﬁed by the constant
R, is disconnected when fΘ2 has multiple crossings of the α-plane. This can be seen for the saddle
α
surface in Figure 5, for which the intersection to an α-plane is a hyperbola (unless the α-plane intersects
the saddle point).

∈

The optimization results are given in Table 5. When no feasible solution is found within the total
in the Gap column. From the table, we
time for one or more of the networks, we mark this with
see that the LRR procedure has the best performance for the low dimensional problems (n = 1, 2, 3). A
likely explanation is that the solver’s internal BT procedures work well for small network sizes, and that
the stronger bound tightening procedures lead to computational overhead. For n = 4, the RR procedure
takes the lead by striking a good balance between time spent on BT and resulting bound tightness. For
the largest problems (n = 5, 6), the LR(1) procedure is the best performer in terms of total solution
time. The trend seems to be that larger network sizes beneﬁt more from stronger bound tightening. We
note that for n = 6, we are able to solve all ﬁve instances when using the LR(1) procedure. Even with
subproblem solution time is limited to one second, the SEMI-LR(1) and NO-R(1) procedures use several

∞

13

Figure 5: ReLU network approximation of a quadratic function with an indeﬁnite A matrix.

Table 5: Average solution times and optimality gap for Qn (ﬁve instances for each value of n). Solution times for bound
tightening (TBT) and optimization (TOPT) are given in seconds. Total solution time, TBT + TOPT, is limited to 600 seconds.

n BT procedure

1

2

3

4

5

6

LRR
RR
LR(1)
SEMI-LR(1)
NO-R(1)

LRR
RR
LR(1)
SEMI-LR(1)
NO-R(1)

LRR
RR
LR(1)
SEMI-LR(1)
NO-R(1)

LRR
RR
LR(1)
SEMI-LR(1)
NO-R(1)

LRR
RR
LR(1)
SEMI-LR(1)
NO-R(1)

LRR
RR
LR(1)
SEMI-LR(1)
NO-R(1)

TBT

0.013
0.026
0.054
0.138
0.070

0.024
0.078
0.374
1.575
1.959

0.065
0.293
3.772
17.42
36.65

0.082
0.403
7.717
32.02
84.29

0.164
2.252
95.59
181.2
303.9

0.286
5.97
195.8
335.0
472.9

TOPT

TBT + TOPT

Gap (%)

# solved

0.003
0.002
0.003
0.002
0.001

0.070
0.054
0.042
0.047
0.037

0.418
0.302
0.237
0.235
0.206

3.497
1.644
1.217
1.273
0.528

204.1
137.9
13.2
17.7
16.0

433.0
295.0
78.1
128.2
69.0

0.016
0.028
0.057
0.140
0.071

0.094
0.132
0.416
1.622
1.996

0.48
0.60
4.01
17.66
36.86

3.58
2.05
8.93
33.29
84.82

204.3
140.1
108.8
198.9
319.9

433.3
301.0
273.9
463.2
541.9

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

10.6
0.4
0
0
0

∞
∞
0
∞
2.7

5/5
5/5
5/5
5/5
5/5

5/5
5/5
5/5
5/5
5/5

5/5
5/5
5/5
5/5
5/5

5/5
5/5
5/5
5/5
5/5

4/5
4/5
5/5
5/5
5/5

2/5
3/5
5/5
3/5
3/5

hundred seconds on bound tightening, leaving little time for solving the optimization problems within
the total time limit of 600 seconds.

The results given in Table C.10, Appendix C, show that the BT procedures that utilize stronger

14

x1−2−1012x2−2−1012˜q(x)−40−30−20−1001020relaxations also compute tighter bounds, as expected. Another expected observation is that the number
of dead neurons identiﬁed is strongly correlated with bound tightness.

In terms of BT solution times, we see that the interval arithmetic-based LRR procedure scales well
with network size. The LP-based procedures scale moderately, while the MILP-based procedures scale
poorly and quickly become computationally demanding. For the larger networks, the subproblem time
limit of one second helps to limit BT solution times, at the cost of looser bounds. This explains why LR,
which has the smallest subproblems among the MILP-based procedures, computes the tightest bounds
for n = 6.

5.3. Oil production optimization case

To test the practical performance of the proposed methods we solve a production optimization case
from an oﬀshore petroleum production ﬁeld involving eight subsea wells producing oil, gas, and water.
Each of the wells produce to a topside processing facility, a platform, via one of two risers (pipelines
transporting the ﬂuid from the seabed to the topside processing facility).

An oﬀshore production system consists of several interconnected modules with corresponding interde-
pendencies. The reservoir is the subsurface structure where oil and gas are located prior to extraction. A
number of wells are drilled into the reservoir. The ﬂow coming from a well is a mixture of water, oil and
gas, called production phases, and is routed to a platform through a network of pipelines. Manifolds are
used to connect diﬀerent pipelines and to mix the incoming ﬂows. At the platform, separators split the
production phases and route the resulting streams of oil and gas to an export line. The export lines lead
production phases oﬀ-site. The production ﬂow from a well may be routed to a subset of the separators
on the platform. A well can only be routed to a single separator at any given time. The routing decision
must take into consideration the interdependencies between wells which produce to the same separator.

11

12

g(9,11)

10

9

4

f1

1

2

3

Separators, N s

Risers, Er

Manifolds, N m

Pipelines, Ed

5

6

7

8

Wells, N w

Figure 6: Production system ﬂow graph, with nodes represented by grey circles and edges by arrows. Discrete edges are
dashed. The nonlinearities f1 and g(9,11), related to Node 1 and Edge (9,11), are shown.

The production system is modeled by a directed acyclic graph G = (N, E), with nodes N and edges
E. An illustration of the graph is given in Figure 6. To simplify modeling, we use the utility sets in Table
6. The set of nodes is N = N w
Er. As illustrated in Figure
N w) has two leaving discrete edges. By allowing zero or one of these edges to be
6, each well node (i
active, we model routing and on/oﬀ switching of well ﬂows.

N s, and the set of edges is E = Ed

N m

∪

∪

∪

∈

E,
N , the ﬂow rate qe,c of phase c
The variables are the pressure pi at each node i
∈
Ed. The variables ye are used to model routing
and the binary variable ye on the discrete edge e
∈
and on/oﬀ switching of wells. The problem also includes constants ce,gor (gas-oil ratio), ce,wor (water-oil
ratio), ﬂow rate bounds qL
i , which are
set to realistic values. For further details about the modeling approach we refer the reader to [10].

i , and separator pressures ps

e,c, pressure bounds pL

e,c and qU

C on edge e

i and pU

∈

∈

15

Set

Description

Table 6: Utility sets

N
N w
N m
N s
E
Ed
Er
Ein
i
Eout
i
C

Set of nodes in the network.
Set of well (source) nodes in the network. N w ⊂ N .
Set of manifold nodes in the network. N m ⊂ N .
Set of separator (sink) nodes in the network. N s ⊂ N .
Set of edges in the network. An edge e = (i, j) connects node i to node j, where i, j ∈ N .
Set of discrete edges that can be open or closed. Ed ⊂ E.
Set of riser edges. Er ⊂ E.
Set of edges entering node i, i.e. Ein
Set of edges leaving node i, i.e. Eout
C = {oil, gas, wat}, denoting the ﬂow rate of oil, gas, and water, respectively.

i = {e : e = (j, i) ∈ E}.
i = {e : e = (i, j) ∈ E}.

The complete formulation of the production optimization problem is given below.

maximize
y,q,p

z =

(cid:88)

e∈Er

qe,oil

subject to

(cid:88)

e∈Ein
i

qe,c =

(cid:88)

qe,c,

e∈Eout
i

pj = ge(qe,oil, qe,gas, qe,wat, pi),

ye)

pi

pj

(pU

i −

≤

−

≤

−

pL
j )(1

−

ye),

yeqU

e,c,

qe,c
≤
pU
i ,

≤

c
∀

∈

(

j + pL
pU
−
(cid:88)
ye

i )(1

1,

≤

e∈Eout
i
yeqL
pL
i ≤
(cid:88)

e,c ≤
pi

qe,oil = fi(pi),

e∈Eout
i
(cid:88)

e∈Eout
i
(cid:88)

qe,gas = ce,gor

(cid:88)

qe,oil,

e∈Eout
i
(cid:88)

qe,wat = ce,wor

qe,oil,

e∈Eout
i

e∈Eout
i
pi = ps
i ,
0, 1
ye
}

∈ {

,

C, i

c
∀

∈

∈

N m

e
∀
e
∀
i
∀

∈

∈

∈

Er
Ed

N w

Ed

C, e

∈
i
∀
i
∀

∈

N

∈
N w

N w

i
∀

∈

N w

i
∀

∈

N s
Ed

i
∀
e
∀

∈

∈

(14a)

(14b)

(14c)

(14d)

(14e)

(14f)

(14g)

(14h)

(14i)

(14j)

(14k)

(14l)

∈

The objective function (14a) maximizes the total oil production. Constraints (14b) are mass balance
constraints that make sure that the ﬂow into a manifolds equals the ﬂow out of it. The riser pressure drop
Er in equation (14c) calculate the pressure at the separator as
functions ge(qe,oil, qe,gas, qe,wat, pi) for e
a function of the ﬂow rates (qe,oil, qe,gas, qe,wat) in the riser and the pressure at the start of the riser pi.
Ed) that are open (ye = 1), as speciﬁed by the
We assume no pressure drop over the discrete edges (e
big-M constraints in (14d). Constraints (14e) state that the ﬂow from a well can be routed through at
most one pipeline, while constraints (14f) connect ﬂow rate through a pipeline with the routing decision.
The ﬂow must be within upper and lower bounds if the pipeline is open, and zero otherwise. The bounds
on the pressure at each node are given by constraints (14g). The well performance curves fi(pi) for
N w in equation (14h) relate the pressure at well i, pi, with the ﬂow rate of oil out of the well qe,oil for
i
Eout
e
. Constraints (14i) and (14j) connect the ﬂow rates of the diﬀerent phases through the gas-oil
i
and water-oil ratios. The pressure at the separators is ﬁxed through constraints (14k) and constraints
(14l) deﬁne the binary variables.

∈
∈

∈

The optimization problem in (14) contains a total of ten nonlinearities that we model with ReLU
N w, and two riser pressure drop functions
networks: eight well performance curves fi(pi) for i
Er. To test the eﬀect of network depth on solution time, we consider
ge(qe,oil, qe,gas, qe,wat, pi) for e
the shallow and deep network architectures given in Table 7. The networks are trained on scattered data
sampled from a multiphase ﬂow simulator. Each well and riser is modeled and simulated individually in

∈

∈

16

the simulator. We simulate 50 data points for each well, and 4000 for each riser. Since the simulated
data is free of noise, the DNNs are trained with a low L2 penalty on the parameters. The ReLU net-
work surrogates achieve a mean absolute percentage error (MAPE) of less than 1%, meaning that they
approximate the simulator with high accuracy.

Table 7: Deep neural networks for wells and risers.

DNN

Layers

# parameters

# 0–1 var.

MAPE (%)

Shallow well nets
Shallow riser nets

(1, 20, 20, 1)
(4, 50, 50, 1)

Deep well nets
Deep riser nets

(1, 10, 10, 10, 10, 1)
(4, 20, 20, 20, 20, 20, 1)

481
2851

361
1801

40
100

40
100

0.73
0.30

0.48
0.42

The MILP formulations of the shallow and deep networks in Table 7 have the same number of
continuous and binary variables. There are 40 binary variables for the wells and 100 binary variables
for the risers. The resulting optimization problem (14) has a total of 536 binary variables (including 16
binary variables for well routing) and 1146 continuous variables. The number of constraints is 1416 with
shallow networks, versus 1338 with deep networks. The problem size is thus similar with shallow and
deep networks.

The BT procedures achieve the values reported in Tables C.11-C.12 for the well networks and Tables
C.13-C.14 for the riser networks (see Appendix C). Due to the constraints on the separator pressures,
N s. Looking at the MRD values
the riser networks are output bounded to the singleton
for the riser networks, it is evident that NO-R computes tighter bounds than the other BT procedures
which are unable to fully utilize the output bounds via BBP. As can be seen from the solution times, the
NO-R procedure is computationally expensive. For the shallow riser networks the average run time is
306 seconds, while for the deep riser networks it is 5080 seconds. Limiting the subproblem solution time
to 60 seconds helps for the deeper networks, bringing the average solution time down to 1771 seconds
for NO-R(60). The limitation in solution time do not seem to aﬀect the bound tightness by much for
these networks sizes, and NO-R(60) achieves a MRD of 0.1 % compared to NO-R. For the well networks
and the shallow riser networks, the subproblem time limit is never eﬀective, which means that NO-R and
NO-R(60) compute identical bounds.

ps
i }

for i

∈

{

With the tightened bounds, we solve the production optimization problem in (14). The results are
given in Tables 8 and 9 for the shallow and deep ReLU networks, respectively. The eﬀect of bound
tightness on optimization solution time is striking. With the shallow networks, we are only able to close
the optimality gap within one hour using the bounds computed using the LR(60), NO-R(60) and NO-R
procedure. For the deep networks, the gap is closed when we use bounds computed by the NO-R(60)
and NO-R procedure. Notice that the comparison is somewhat unfair since the overall computation time
used by the NO-R procedure far exceeds that of the other BT procedures. It is, however, interesting to
compare the optimization solution times (TOPT) for the various BT procedures.

With NO-R(60), we solve both the deep and shallow problem within one hour, which is acceptable
5 and size exceeding 100 nodes,

for a practical application of (14). For larger networks with depth K
≥
we are not able to solve (14) within the practical time limit of one hour.

With the optimal bounds computed by NO-R for the deep networks, the optimization converges in
just 5 seconds. The expensive BT procedures thus show some merit on these problems, especially for
applications where it is interesting to solve the problem many times; perhaps with small adjustments to
the problem for each optimization run (required that the adjustments do not invalidate the computed
bounds).

Table 8: Solution times for production optimization problem with shallow ReLU networks. Bound tightening (TBT) and
optimization (TOPT) solution times are given in seconds. TOPT is limited to 3600 seconds. z(cid:63) is the objective value of the
best found solution and Gap is the diﬀerence between the upper and lower bounds relative to the lower bound.

BT procedure

LRR
RR
LR(60)
SEMI-RR(60)
NO-R(60)
NO-R

TBT

0.3
1.6
18.6
115.0
613.2
613.2

TOPT

TBT + TOPT

3600
3602
2402
3715
633
633

3600
3600
2383
3600
20
20

17

z(cid:63)

1.2864
1.2864
1.2864
1.2864
1.2864
1.2864

Gap (%)

7.5
4.7
0
15.5
0
0

Table 9: Solution times for production optimization problem with deep ReLU networks. Bound tightening (TBT) and
optimization (TOPT) solution times are given in seconds. TOPT is limited to 3600 seconds. z(cid:63) is the objective value of the
best found solution and Gap is the diﬀerence between the upper and lower bounds relative to the lower bound.

BT procedure

LRR
RR
LR(60)
SEMI-RR(60)
NO-R(60)
NO-R

TBT

0.3
2.5
220.1
273.8
3543.2
10161.2

TOPT

TBT + TOPT

3600
3600
3600
3600
57
5

3600
3603
3820
3874
3600
10166

z(cid:63)

–
–
–
–
1.3049
1.3049

Gap (%)

–
–
–
–
0
0

Remark 4. As an alternative approach to ReLU networks, we may model the well and riser nonlinearities
using a traditional MILP formulation that interpolates the sample points, as discussed in Section 2. For
each nonlinearity, we use Delaunay triangulation on the samples to partition the domain into simplices.
Speciﬁcally, for each riser model we create a Delaunay triangulation of 4000 scattered, four-dimensional
data points. With the partitions, we obtain PWL approximations using the DLog model in [13]. The
resulting formulation of 14 has 90 binary variables, 718 656 continuous variables, and 287 constraints.
When attempting to solve this problem, the solver quickly runs out of memory without ﬁnding any
feasible solution. We thus deem this approach unsuccessful for the production optimization case with
irregularly sampled data points.

6. Concluding remarks

As we discussed in Section 2, ReLU networks oﬀer practitioners a versatile framework for modeling
PWL functions. ReLU networks scale well to high input dimensions and can be trained on large datasets
of scattered and noisy data. The MILP formulation in (4) enables the embedding of ReLU networks in
optimization problems that can be solved by state-of-the-art MILP solvers.

In statistical learning it is common to follow the principle of Occam’s razor and avoid unnecessary
model complexity when searching for models that generalize well. According to this principle, deep ReLU
networks are favorable to shallow networks since they can achieve the same complexity (number of linear
partition regions) with fewer parameters. An example of this is given in Table 7, where shallow (2851
parameters) and deep (1801 parameters) riser networks achieve similar accuracy. Smaller networks also
tend to lower solution times in the optimization, which would lead us to conclude that deep networks are
favorable for surrogate modeling. Paradoxically, the deepening of networks which has yielded favorable
results on modeling tasks, seems to make the resulting MILP models harder to optimize (as shown in
the numerical results). One explanation to this may be that with deeper networks, more variables are
interlinked via the block-angular structure in Figure 3. Thus, what is a strength in the modeling setting,
may become a weakness in the optimization setting. We are thus compelled to search for a good trade-
oﬀ between model depth and optimization eﬃciency. The same observation was reported in [55, 36]
for global optimization of DNNs, supporting the claim that deeper networks are more challenging to
optimize. To conclude, we observe, as others have before us and as theory predicts, that the feasibility of
using the MILP formulation quickly fades with increasing network sizes. Our numerical study indicates
that the MILP formulation is unﬁt for applications involving large ReLU networks with thousands of
hidden nodes. For the oil production optimization case, we reached a practical limit on 100 hidden nodes
for the deep network architectures. The bulk of the solution time is then used on bound tightening the
large riser networks.

Solution times are sensitive to tightness of the variable bounds in the MILP formulation, which directly
translates to the big-M values in (4). Our study of bound tightening procedures, ranging from the cheap
LRR procedure to the optimal NO-R procedure, shows that it is problem speciﬁc which procedure strikes
the best computational eﬃciency. For optimization problems with small networks embedded, the cheaper
procedures seem to perform best, while for larger networks the more expensive procedures perform best.
Somewhat surprisingly, the MILP-based procedures, usually thought to be too computationally expensive,
perform quite well on the more challenging problems. This is further ampliﬁed in applications where the
optimization problem is solved many times using the same bounds. Here, spending the extra time in the
more expensive bound tightening procedures can really pay oﬀ.

When a ReLU network is embedded in an optimization problem it is likely to be subject to output
bounds. Our study is the ﬁrst to investigate bound tightening of (4) in the presence of output bounds.
In Table 2 we summarize the procedures and identify the ones capable of BBP. The numerical results

18

in Section 5 show that ReLU relaxation signiﬁcantly reduces bound tightness for both FBP and BBP.
From the results, and the argument in Appendix A, we conclude that BBP is less eﬀective than FBP. Of
the studied procedures, only NO-R is capable of exploiting output bounds to reduce bounds; see Section
5.1. This may explain why NO-R is the best-performing procedure for the challenging oil production
optimization case.

We ﬁnally remark that, while the application of (4) is practically limited to small-sized networks,
many interesting nonlinearities can be approximated by networks of this size (several examples can be
found in works cited in the introduction). Our case studies show that the MILP formulation of ReLU
networks in (4) is an attractive approach to surrogate modeling in process optimization.

6.1. Promising research directions

Optimization of ReLU networks is currently an active area of research. Below, we highlight some
promising research topics that may allow practitioners to embed larger ReLU networks in optimization
problems.

•

•

•

•

•

•

In this work we combined LRR with the other presented BT procedures. Other combinations may
be more eﬃcient in terms of tightening per processing time unit. For example, by combining RR
with NO-R, we may ﬁrst solve a round of LP subproblems, before we invoke the more expensive
NO-R for further tightening.

The feasible region deﬁned by the set of bounds B is a multidimensional box and all procedures
presented herein only work with one ReLU at a time.
It is possible to devise a procedure for
simultaneous tightening of multiple bounds. For example, a procedure could tighten the sum of
bounds in the same layer.

Strong MILP formulations and a related family of cutting planes for ReLU networks were recently
presented in [56]. These advancements could extend the applicability of ReLU networks as surrogate
models by lowering solution times.

Cuts and bound tightening may be used deeper in the MILP solver’s B&B tree. It may be advan-
tageous to employ cheap BT procedures specialized for the structure of ReLU network at selected
branches in the branch-and-bound tree.

L1 regularization can be utilized during training to encourage sparsity in the parameter matrices,
which likely leads to a reduction in MILP solution times [56]. Above some level of sparsity, the
generalization error will generally start to deteriorate due to the reduction in model capacity. How-
ever, as DNNs often are over-parameterized, many connections can typically be dropped without
any signiﬁcant impact on the generalization error.

Larger networks may be considered by utilizing specialized solution methods for ReLU networks.
In particular, in the literature on robustness and veriﬁcation of DNNs we ﬁnd methods such as the
modiﬁed Simplex algorithm in [57], the Lagrangian relaxation-based method in [58], and methods
based on linear and Lipschitz relaxations [59, 60]. While these works focus on forward bound-
propagation in a single network, their ideas may be exploited to eﬃciently tighten the big-M values
in (4) or to develop new solution methods for problems with multiple ReLU network surrogate
models.

Due to the wide application of ReLU networks for modeling, we believe that it is important to push the
practical limitation on network size to allow for more expressive networks as surrogate models in process
optimization.

Appendix A. Forward- vs backward-propagation of bounds

Consider n nodes with outputs xi feeding into a node y. The nodes are related as follows

y =

n
(cid:88)

i=1

wixi + b,

(A.1)

with parameters wi
on wi. Without loss of generality and to ease the presentation, we here instead require wi

R. If xi are outputs of ReLU units we have li

0 and no sign restrictions
0 and allow

R and b

≥

∈

∈

≥

19

negative bounds on xi, which yields an equivalent argument. A forward-propagation of bounds gives
y

[Lf , Uf ], with

∈

Lf =

n
(cid:88)

i=1

wili + b and Uf =

n
(cid:88)

i=1

wiui + b.

(A.2)

Now, consider the backward-propagation of bounds on y to node xj. Let y

Thus, it is clear that the bounds on y are linearly dependent on the bounds of the xi-s. A consequence
of this is that a tightening of any bound ui or li will have a tightening eﬀect on Uf or Lf , respectively.
0
is the amount of tightening in the upper bound of y. We are interested in how this aﬀects the bound
0 is the change in the upper bound of uj. Using the relationship between
xj
the nodes we have that

∆uj, where ∆uj

∆U , where ∆U

Uf

uj

−

≤

≥

≤

−

≥

The upper bound on xj is then found to be

wjxj = y

(cid:88)

−

i(cid:54)=j

wixi

b

−

wj(uj

−

∆uj) = Uf

∆U

−

(cid:88)

−

i(cid:54)=j

wili

b

−

=

=

n
(cid:88)

i=1

n
(cid:88)

i=1

wiui + b

∆U

−

(cid:88)

−

i(cid:54)=j

wili

b

−

wiui

∆U

−

(cid:88)

−

i(cid:54)=j

wili

Rearranging the last equation yields

∆uj =

∆U
wj −

(cid:88)

i(cid:54)=j

wi
wj

(ui

li)

−

Tightening of the upper bound on xj occurs only when ∆uj

0, and we obtain the requirement

≥

∆U
wj −

∆U

=

⇒

≥

(cid:88)

i(cid:54)=j
(cid:88)

i(cid:54)=j

wi
wj

(ui

li)

0

≥

−

wi(ui

li)

0

≥

−

(A.3)

An equivalent argument can be made for the lower bound xj
0 gives

0 and ∆L

Requiring ∆lj

≥

≥

lj+∆lj, given a tightening y

Lf +∆L.

≥

≥

∆L

(cid:88)

i(cid:54)=j

≥

wi(ui

li)

0.

≥

−

(A.4)

Inequalities (A.4) and (A.3) tell us that a considerable change in the bound of y is required for it to
have any tightening eﬀect on the bounds of preceding nodes. To better understand these requirements
Lf ), where ∆
we may cast them as a relative change in the bounds by considering the quantity ∆/(Uf
is the change in the bound of y (either ∆U , ∆L, or a combination of these). Since Uf
0, we may
write

−
Lf

−

≥

∆

−

Uf

Lf ≥

=

Uf
(cid:80)
i(cid:54)=j
n
(cid:80)
i=1

= 1

= 1

−

−

1

Lf

−
wi(ui

(cid:88)

i(cid:54)=j

wi(ui

li)

−

li)

−

wi(ui

li)

−
wj(uj
n
(cid:80)
i=1
δ,

−
wi(ui

lj)

li)

−

20

where we introduced δ
is required for backward-propagation of bounds to have any eﬀect. We see that δ
uj
between y and the other nodes.

[0, 1] in the last line. It now becomes evident that a relatively large change ∆
wi or
= j, meaning that the relationship between xj and y dominates the relationship

1 only if wj

li for all i

(cid:29)

(cid:29)

ui

≈

−

−

lj

∈

We end this discussion by considering the special case where wi = wj, ui = uj, and li = lj for all i, j.

In this case δ simpliﬁes to δ = 1/n and we get

∆

−

Uf

Lf ≥

1/n.

1

−

From this expression we see that for wide layers (large n) with “balanced” nodes, the tightening ∆ must
be very large for it to have any eﬀect on preceding nodes. E.g., for n = 100, a reduction of 99% is required
in the bounds on y.

Appendix B. Statistics for measuring bound tightness

We introduce two statistics to measure the quality of the bounds produced by a bound tightening

procedure. We measure the mean absolute distance (MAD) of a set of bounds B as

M AD(B) :=

K
(cid:88)

k=0

n−1
k

nk(cid:88)

j=1

uk
j −

|

,

lk
j |

(B.1)

sk
j for node (j, k).

where uk

j and lk

j are the upper and lower bound on xk

j −
To compare bounds, we use the mean relative distance (MRD), which measures the relative tightness
of bounds B compared to the best bounds B(cid:63) and the suboptimal LRR bounds denoted B−. The MRD
is deﬁned as follows:

MAD(B)
M RD(B, B(cid:63), B−) := 100 |
MAD(B−)
|
For a set of optimal bounds B = B∗, M RD(B, B(cid:63), B−) = 0. While for bounds B = B−, the MRD is
100. The bound tightening procedures used in this study are initialized with B0, and thus always obtain
an MRD in the range [0, 100]. Note that we deﬁne the MRD to be zero whenever the denominator is
zero.

MAD(B(cid:63))
|
MAD(B(cid:63))

−
−

(B.2)

.

|

Appendix C. Bound tightening results

21

(cid:54)
Table C.10: Bound tightening results for Qn. The reported values are the average results over ten models, for n = 1, . . . , 6.
Dead neurons (%)

n BT procedure

MRD (%)

Time (s)

MAD

1

2

3

4

5

6

LRR
RR
LR(1)
SEMI-LR(1)
NO-R(1)

LRR
RR
LR(1)
SEMI-LR(1)
NO-R(1)

LRR
RR
LR(1)
SEMI-LR(1)
NO-R(1)

LRR
RR
LR(1)
SEMI-LR(1)
NO-R(1)

LRR
RR
LR(1)
SEMI-LR(1)
NO-R(1)

LRR
RR
LR(1)
SEMI-LR(1)
NO-R(1)

0.013
0.026
0.054
0.138
0.070

0.024
0.078
0.374
1.575
1.959

0.065
0.293
3.772
17.42
36.65

0.082
0.403
7.717
32.02
84.29

0.164
2.252
95.59
181.2
303.9

0.286
5.970
195.8
335.0
472.9

29.3
41.3
32.7
42.0
43.3

9.3
12.0
12.3
12.7
16.7

9.0
9.2
10.2
10.5
13.8

4.9
4.9
6.1
6.1
8.9

4.0
4.1
7.5
6.9
7.0

3.4
3.4
4.2
3.4
3.5

2.075
1.254
1.695
1.238
1.179

5.832
2.916
2.749
2.598
2.228

9.879
4.865
3.866
3.747
3.245

14.76
6.53
4.75
4.68
4.13

67.47
18.15
8.79
9.18
8.55

48.18
13.41
7.14
9.59
8.49

100.0
7.4
39.2
4.5
0.0

100.0
21.3
15.2
10.7
0.0

100.0
27.6
12.4
9.6
0.0

100.0
25.1
7.6
6.8
0.0

100.0
17.4
1.6
2.2
1.1

100.0
15.4
0.0
6.1
3.4

Table C.11: Bound tightening results for shallow well networks with layers (1, 20, 20, 1). The reported values are the
average results of the eight models.

BT method

LRR
RR
LR(60)
SEMI-RR(60)
NO-R(60)
NO-R

Time (s)

Dead neurons (%)

0.020
0.062
0.097
0.548
0.104
0.104

0.0
0.7
1.8
1.8
1.8
1.8

MAD

0.2192
0.2027
0.2031
0.2027
0.2014
0.2014

MRD (%)

100.0
12.0
14.6
12.0
0.0
0.0

Table C.12: Bound tightening results for deep well networks with layers (1, 10, 10, 10, 10, 1). The reported values are the
average results of the eight models.

BT method

LRR
RR
LR(60)
SEMI-RR(60)
NO-R(60)
NO-R

Time (s)

Dead neurons (%)

0.022
0.065
0.183
0.503
0.156
0.156

0
0
0
0
0
0

22

MAD

0.5723
0.5457
0.5461
0.5457
0.5421
0.5421

MRD (%)

100.0
11.5
12.5
11.5
0.0
0.0

Table C.13: Bound tightening results for shallow riser networks with layers (4, 50, 50, 1). The reported values are the
average results of the two models.

BT method

LRR
RR
LR(60)
SEMI-RR(60)
NO-R(60)
NO-R

Time (s)

Dead neurons (%)

0.093
0.554
8.889
55.29
306.0
306.0

4.0
7.0
16.0
16.0
19.5
19.5

MAD

2.915
1.998
1.571
1.521
1.122
1.122

MRD (%)

100.0
46.0
27.2
23.6
0.0
0.0

Table C.14: Bound tightening results for deep riser networks with layers (4, 20, 20, 20, 20, 20, 1). The reported values are
the average results of the two models.

BT method

LRR
RR
LR(60)
SEMI-RR(60)
NO-R(60)
NO-R

Time (s)

Dead neurons (%)

0.061
0.991
109.3
134.9
1771
5080

0
1
4
4
11
11

MAD

13.418
4.120
2.449
2.377
0.678
0.662

MRD (%)

100.0
27.4
14.7
14.2
0.1
0.0

23

References

References

[1] D. R. Jones, M. Schonlau, W. J. Welch, Eﬃcient global optimization of expensive black-box functions,

Journal of Global Optimization 13 (4) (1998) 455 – 492.

[2] M. Ghavamzadeh, S. Mannor, J. Pineau, A. Tamar, et al., Bayesian reinforcement learning: A

survey, Foundations and Trends in Machine Learning 8 (5-6) (2015) 359 – 483.

[3] B. Beykal, F. Boukouvala, C. A. Floudas, N. Sorek, H. Zalavadia, E. Gildin, Global optimization
of grey-box computational systems using surrogate functions and application to highly constrained
oil-ﬁeld operations, Computers and Chemical Engineering 114 (2018) 99–110.

[4] A. R. Conn, K. Scheinberg, L. N. Vicente, Introduction to Derivative-Free Optimization, Society for

Industrial and Applied Mathematics, 2009.

[5] A. Bhosekar, M. Ierapetritou, Advances in surrogate based modeling, feasibility analysis, and opti-

mization: A review, Computers and Chemical Engineering 108 (2018) 250–267.

[6] F. Boukouvala, M. M. Hasan, C. A. Floudas, Global optimization of general constrained grey-box
models: new method and its application to constrained PDEs for pressure swing adsorption, Journal
of Global Optimization 67 (1-2) (2017) 3–42.

[7] L. M. Rios, N. V. Sahinidis, Derivative-free optimization: A review of algorithms and comparison of

software implementations, Journal of Global Optimization 56 (3) (2013) 1247–1293.

[8] R. G. Regis, C. A. Shoemaker, A Stochastic Radial Basis Function Method for the Global Optimiza-

tion of Expensive Functions, INFORMS Journal on Computing 19 (4) (2007) 497 – 509.

[9] A. Ciccazzo, G. D. Pillo, V. Latorre, Support vector machines for surrogate modeling of electronic

circuits, Neural Computing and Applications 24 (1) (2014) 69–76.

[10] B. Grimstad, B. Foss, R. Heddle, M. Woodman, Global optimization of multiphase ﬂow networks

using spline surrogate models, Computers & Chemical Engineering 84 (2016) 237 – 254.

[11] I. Fahmi, S. Cremaschi, Process synthesis of biodiesel production plant using artiﬁcial neural net-

works as the surrogate models, Computers & Chemical Engineering 46 (2012) 105 – 123.

[12] F. Ye, S. Ma, L. Tong, J. Xiao, P. B´enard, R. Chahine, Artiﬁcial neural network based optimiza-
tion for hydrogen puriﬁcation performance of pressure swing adsorption, International Journal of
Hydrogen Energy 44 (11) (2019) 5334 – 5344.

[13] J. P. Vielma, S. Ahmed, G. Nemhauser, Mixed-Integer Models for Nonseparable Piecewise-Linear

Optimization: Unifying Framework and Extensions, Operations Research 58 (2) (2010) 303 – 315.

[14] N. Martinez, H. Anahideh, J. M. Rosenberger, D. Martinez, V. C. Chen, B. P. Wang, Global op-
timization of non-convex piecewise linear regression splines, Journal of Global Optimization 68 (3)
(2017) 563 – 586.

[15] A. N. Elmachtoub, P. Grigas, Smart ”Predict, then Optimize” (2017). arXiv:1710.08005.

[16] V. V. Miˇsi´c, Optimization of Tree Ensembles (2017). arXiv:1705.10883.

[17] M. Biggs, R. Hariss, Optimizing Objective Functions Determined from Random Forests, SSRN

Electronic Journal (2017) 1 – 49.

[18] I. E. Grossmann, Advances in mathematical programming models for enterprise-wide optimization,

Computers & Chemical Engineering 47 (2012) 2 – 18.

[19] H. Andersson, M. Christiansen, G. Desaulniers, A new decomposition algorithm for a liqueﬁed
natural gas inventory routing problem, International Journal of Production Research 54 (2016) 564
– 578.

[20] L. Bach, T. Dollevoet, D. Huisman, Integrating timetabling and crew scheduling at a freight railway

operator, Transportation Science 50 (3) (2016) 878 – 891.

24

[21] M. Veenstra, M. Cherkesly, G. Desaulniers, G. Laporte, The pickup and delivery problem with time

windows and handling operations, Computers & Operations Research 77 (2017) 127 – 140.

[22] R. Balestriero, R. Baraniuk, Mad Max: Aﬃne Spline Insights into Deep Learning (2018). arXiv:

1805.06576.

[23] A. C. Ian Goodfellow, Yoshua Bengio, Deep Learning, MIT Press, 2016.

[24] K. He, X. Zhang, S. Ren, J. Sun, Deep Residual Learning for Image Recognition, in: Proceedings of

the IEEE conference on computer vision and pattern recognition, 2016, pp. 770–778.
URL http://arxiv.org/abs/1512.03385

[25] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, A. Rabi-
novich, Going deeper with convolutions, in: Proceedings of the IEEE conference on computer vision
and pattern recognition, Vol. June, 2016, pp. 1–9.

[26] I. J. Goodfellow, D. Warde-farley, M. Mirza, A. Courville, Y. Bengio, Maxout networks, in: Pro-

ceedings of the 30th International Conference on Machine Learning, 2013, pp. 1319–1327.

[27] M. Lin, Q. Chen, S. Yan, Network In Network (2013). arXiv:1312.4400.

[28] X. Glorot, A. Bordes, Y. Bengio, Deep Sparse Rectiﬁer Neural Networks, in: Proceedings of the
Fourteenth International Conference on Artiﬁcial Intelligence and Statistics, Vol. 15, 2011, pp. 315–
323.

[29] K. He, X. Zhang, S. Ren, J. Sun, Delving Deep into Rectiﬁers: Surpassing Human-Level Performance
on ImageNet Classiﬁcation, in: Proceedings of the IEEE international conference on computer vision,
2015, pp. 1026–1034.

[30] J. Schmidhuber, Deep Learning in neural networks: An overview, Neural Networks 61 (2015) 85–117.

[31] S. Sonoda, N. Murata, Neural network with unbounded activation functions is universal approxima-

tor, Applied and Computational Harmonic Analysis 43 (2) (2017) 233–268.

[32] R. Bunel, I. Turkaslan, P. H. S. Torr, P. Kohli, M. P. Kumar, A Uniﬁed View of Piecewise Linear

Neural Network Veriﬁcation (2017). arXiv:1711.00455.

[33] C.-H. Cheng, G. N¨uhrenberg, H. Ruess, Maximum resilience of artiﬁcial neural networks,

in:
D. D’Souza, K. Narayan Kumar (Eds.), Automated Technology for Veriﬁcation and Analysis,
Springer International Publishing, Cham, 2017, pp. 251 – 268.

[34] S. Dutta, S. Jha, S. Sanakaranarayanan, A. Tiwari, Output Range Analysis for Deep Feedforward

Neural Networks (2017). arXiv:1709.09130.

[35] V. Tjeng, K. Xiao, R. Tedrake, Evaluating Robustness of Neural Networks with Mixed Integer

Programming (2017). arXiv:1711.07356.

[36] M. Fischetti, J. Jo, Deep neural networks and mixed integer linear optimization, Constraints 23 (3)

(2018) 296 – 309.

[37] T. Serra, C. Tjandraatmadja, S. Ramalingam, Bounding and Counting Linear Regions of Deep

Neural Networks (2018). arXiv:1711.02114.

[38] A. Kumar, T. Serra, S. Ramalingam, Equivalent and Approximate Transformations of Deep Neural

Networks (2019). arXiv:1905.11428.

[39] F. A. Fernandes, Optimization of ﬁscher-tropsch synthesis using neural networks, Chemical Engi-

neering and Technology 29 (4) (2006) 449–453.

[40] H. R. Sant Anna, A. G. Barreto, F. W. Tavares, M. B. de Souza, Machine learning model and
optimization of a PSA unit for methane-nitrogen separation, Computers & Chemical Engineering
104 (2017) 377 – 391.

[41] T. A. AL-Qutami, R. Ibrahim, I. Ismail, M. A. Ishak, Virtual multiphase ﬂow metering using diverse
neural network ensemble and adaptive simulated annealing, Expert Systems with Applications 93
(2018) 72–85.

25

[42] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean,
M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser,
M. Kudlur, J. Levenberg, D. Man´e, R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens,
B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Vi´egas, O. Vinyals,
P. Warden, M. Wattenberg, M. Wicke, Y. Yu, X. Zheng, TensorFlow: Large-Scale Machine Learning
on Heterogeneous Systems, Software available from tensorﬂow.org (2015).
URL https://www.tensorflow.org/

[43] R. B. Hughes, M. R. Anderson, Simplexity of the cube, Discrete Mathematics 158 (1-3) (1996) 99 –

150.

[44] R. Misener, C. A. Floudas, Piecewise-linear approximations of multidimensional functions, Journal

of Optimization Theory and Applications 145 (1) (2010) 120 – 147.

[45] K. Crombecq, D. Gorissen, D. Deschrijver, T. Dhaene, A novel hybrid sequential design strategy
for global surrogate modeling of computer experiments, SIAM Journal of Scientiﬁc Computing 33
(2011) 1948 – 1974.

[46] D. Gorissen, I. Couckuyt, E. Laermans, Multiobjective global surrogate modeling, dealing with the

5-percent problem, Engineering with Computers 26 (2010) 81 – 98.

[47] J. van der Herten, I. Couckuyt, D. Deschrijver, T. Dhaene, A fuzzy hybrid sequential design strategy
for global surrogate modeling of high-dimensional computer experiments, SIAM Journal of Scientiﬁc
Computing 37 (2015) 1020 – 1039.

[48] C. A. Kieslich, F. Boukouvala, C. A. Floudas, Optimization of black-box problems using Smolyak
grids and polynomial approximations, Journal of Global Optimization 71 (4) (2018) 845–869.

[49] S. S. Garud, I. A. Karimi, M. Kraft, Design of computer experiments: A review, Computers and

Chemical Engineering 106 (2017) 71–95.

[50] E. M. L. Beale, J. A. Tomlin, Special facilities in a general mathematical programming system for
non-convex problems using ordered sets of variables, in: J. Lawrence (Ed.), Proceedings of the ﬁfth
international conference on operational research, 1970, pp. 447 – 454.

[51] M. Raghu, B. Poole, J. Kleinberg, S. Ganguli, J. Sohl-Dickstein, On the Expressive Power of Deep

Neural Networks (2016). arXiv:1606.05336.

[52] L. Bottou, F. E. Curtis, J. Nocedal, Optimization Methods for Large-Scale Machine Learning, SIAM

Review 60 (2) (2018) 223 – 311.

[53] A. M. Gleixner, T. Berthold, B. M¨uller, S. Weltge, Three enhancements for optimization-based

bound tightening, Journal of Global Optimization 67 (4) (2017) 731 – 757.

[54] Gurobi Optimization, LLC, Gurobi Optimizer Reference Manual (2018).

URL http://www.gurobi.com

[55] A. M. Schweidtmann, A. Mitsos, Global Deterministic Optimization with Artiﬁcial Neural Networks

Embedded (2018). arXiv:1801.07114.

[56] R. Anderson, J. Huchette, W. Ma, C. Tjandraatmadja, J. P. Vielma, Strong mixed-integer program-

ming formulations for trained neural networks (2018). arXiv:1811.01988.

[57] G. Katz, C. Barrett, D. Dill, K. Julian, M. Kochenderfer, Reluplex: An Eﬃcient SMT Solver for

Verifying Deep Neural Networks (2017). arXiv:1702.01135.

[58] Krishnamurthy, Dvijotham, R. Stanforth, S. Gowal, T. Mann, P. Kohli, A Dual Approach to Scalable

Veriﬁcation of Deep Networks (mar 2018). arXiv:1803.06567.

[59] T.-W. Weng, H. Zhang, H. Chen, Z. Song, C.-J. Hsieh, D. Boning, I. S. Dhillon, L. Daniel, Towards

Fast Computation of Certiﬁed Robustness for ReLU Networks (2018). arXiv:1804.09699.

[60] G. Singh, T. Gehr, M. Mirman, M. P¨uschel, M. Vechev, Fast and Eﬀective Robustness Certiﬁcation,

Advances in Neural Information Processing Systems 31 (2018) 10802 – 10813.

26

