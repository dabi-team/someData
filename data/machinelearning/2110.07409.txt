2
2
0
2

r
p
A
9
2

]

C
O
.
h
t
a
m

[

3
v
9
0
4
7
0
.
0
1
1
2
:
v
i
X
r
a

Published as a conference paper at ICLR 2022

THE GEOMETRY OF MEMORYLESS STOCHASTIC POL-
ICY OPTIMIZATION IN INFINITE-HORIZON POMDPS

Johannes M ¨uller
Max Planck Institute for Mathematics in the Sciences, Leipzig, Germany
jmueller@mis.mpg.de

Guido Mont ´ufar
Department of Mathematics and Department of Statistics, UCLA, CA, USA
Max Planck Institute for Mathematics in the Sciences, Leipzig, Germany
montufar@math.ucla.edu

ABSTRACT

We consider the problem of ﬁnding the best memoryless stochastic policy for an
inﬁnite-horizon partially observable Markov decision process (POMDP) with ﬁ-
nite state and action spaces with respect to either the discounted or mean reward
criterion. We show that the (discounted) state-action frequencies and the expected
cumulative reward are rational functions of the policy, whereby the degree is de-
termined by the degree of partial observability. We then describe the optimization
problem as a linear optimization problem in the space of feasible state-action fre-
quencies subject to polynomial constraints that we characterize explicitly. This
allows us to address the combinatorial and geometric complexity of the optimiza-
tion problem using recent tools from polynomial optimization. In particular, we
estimate the number of critical points and use the polynomial programming de-
scription of reward maximization to solve a navigation problem in a grid world.

1

INTRODUCTION

Markov decision processes (MDPs) were introduced by Bellman (1957) as a model for sequential
decision making and optimal planning (see, e.g., Howard, 1960; Derman, 1970; Puterman, 2014).
Many algorithms in reinforcement learning rely on the ideas and methods developed in the context of
MDPs (see, e.g., Sutton & Barto, 2018). Often in practice, the decisions need to be made based only
on incomplete information of the state of the system. This setting is modeled by partially observable
Markov decision processes (POMDPs) introduced by ˚Astr¨om (1965) (for a historical discussion see
Monahan, 1982), which have become an important model for planning under uncertainty. In this
work we pursue a geometric characterization of the policy optimization problem in POMDPs over
the class of stochastic memoryless policies and its dependence on the degree of partial observability.
It is well known that acting optimally in POMDPs may require memory ( ˚Astr¨om, 1965). A POMDP
with unlimited memory policies can be modeled as a belief state MDP, where the states are replaced
by probability distributions that serve as sufﬁcient statistics for the previous observations (Kaelbling
et al., 1998; Murphy, 2000). Finding an optimal policy in this class is PSPACE-complete for ﬁnite
horizons (Papadimitriou & Tsitsiklis, 1987) and undecidable for inﬁnite horizons (Madani et al.,
2003; Chatterjee et al., 2016). Therefore, it is of interest to consider POMDPs with constrained
policy classes. A natural class to consider are memoryless policies, also known as reactive or Markov
policies, which select actions based solely on the current observations. In this case, it is useful to
allow the actions to be selected stochastically, which not only allows for better solutions but also
provides a continuous optimization domain (Singh et al., 1994).

Although they are more restrictive than policies with memory, memoryless policies are attractive as
they are easier to optimize and are versatile enough for certain applications (Tesauro, 1995; Loch &
Singh, 1998; Williams & Singh, 1999; Kober et al., 2013). In fact, ﬁnite-memory policies can be
modeled in terms of memoryless policies by supplementing the state of the system with an external
memory (Littman, 1993; Peshkin et al., 1999; Icarte et al., 2021). Hence theoretical advances on
memoryless policy optimization are also of interest to ﬁnite-memory policy optimization. Theoret-

1

 
 
 
 
 
 
Published as a conference paper at ICLR 2022

ical aspects and optimization strategies over the class of memoryless policies have been studied in
numerous works (see, e.g., Littman, 1994; Singh et al., 1994; Jaakkola et al., 1995; Loch & Singh,
1998; Williams & Singh, 1999; Baxter et al., 2000; Baxter & Bartlett, 2001; Li et al., 2011; Az-
izzadenesheli et al., 2018). However, ﬁnding exact or approximate optimal memoryless stochastic
policies for POMDPs is still considered an open problem (Azizzadenesheli et al., 2016), which is
NP-hard in general (Vlassis et al., 2012). One reason for the difﬁculties in optimizing POMDPs
is that, even in a tabular setting, the problem is non-convex and can exhibit suboptimal strict local
optima (Bhandari & Russo, 2019). For memoryless policies the expected cumulative reward is a
linear function of the corresponding (discounted) state-action frequencies. In the case of MDPs the
feasible set of state-action frequencies is known to form a polytope, so that the optimization problem
can be reduced to a linear program (Manne, 1960; De Ghellinck, 1960; d’Epenoux, 1963; Hordijk
& Kallenberg, 1981). On the other hand, to the best of our knowledge, for POMDPs the speciﬁc
structure of the feasibility constraints and the optimization problem have not been studied, at least
not in the same level of detail (see related works below).

Related works
In MDPs, the (discounted) state-action frequencies form a polytope resp. a com-
pact convex set in the ﬁnite resp. countable state-action cases, whereby the extreme points are
given by the state-action frequencies of deterministic stationary policies (Derman, 1970; Altman
& Shwartz, 1991). Further, Dadashi et al. (2019) showed that the set of state value functions in ﬁnite
state-action MDPs is a ﬁnite union of polytopes. The set of stationary state-action distributions of
POMDPs has been studied by Mont´ufar et al. (2015) highlighting a decomposition into inﬁnitely
many convex subsets whose dimensions depend on the degree of observability. Although this de-
composition can be used to localize optimal policies to some extent, a description of the pieces in
combination is still missing, needed to capture the properties of the optimization problem. We will
obtain a detailed description in terms of ﬁnitely many polynomial constraints with closed form ex-
pressions and bound their degrees in terms of the observation mechanism. This yields a polynomial
programming formulation of POMDPs generalizing the linear programming formulation of MDPs.
This is different to the formulation as a quadratically constrained problem by Amato et al. (2006),
where the number and degree of constraints do not depend on the observability. Finally, Cohen &
Parmentier (2018) described ﬁnite horizon POMDPs as a mixed integer linear program.

Grinberg & Precup (2013) showed that the expected mean reward is a rational function and obtained
bounds on the degree of this function. We generalize this to the setting of discounted rewards and
reﬁne the result by relating the rational degree to the degree of observability. For both MDPs and
POMDPs the expected cumulative reward is known to be a non-convex function of the policy even
for tabular policy models (Bhandari & Russo, 2019). Nonetheless, for MDPs critical points can be
shown to be global maxima under mild conditions. In contrast, for POMDPs or MDPs with linearly
restricted policy models, it is known that non-global local optimizers can exist (Baxter et al., 2000;
Poupart et al., 2011; Bhandari & Russo, 2019). However, nothing is known about the number of local
optimizers. We will present bounds on the number of critical points building on our computation of
the rational degree and feasibility constraints.

The structure of the expected cumulative reward has been studied in terms of the location of the
global optimizers and the existence of local optimizers. Most notably, it is well known that in MDPs
there always exist optimal policies which are memoryless and deterministic (see Puterman, 2014). In
the case of POMDPs, optimal memoryless policies may need to be stochastic (see Singh et al., 1994).
Mont´ufar et al. (2015); Mont´ufar & Rauh (2017); Mont´ufar et al. (2019) obtained upper bounds on
the number of actions that need to be randomized by these policies, which in the worst case is equal
to the number of states that are compatible with the observation. Although we do not improve these
results (which are indeed tight in some cases), our description of the expected cumulative reward
function leads to a simpler proof of the bounds obtained by Mont´ufar et al. (2015).

Neyman (2003) considered stochastic games as semialgebraic problems showing that the minmax
and maxmin payoffs in an n-player game are semialgebraic functions of the discount factor. Al-
though this is not directly related to our work, we take a similar philosophy. We pursue a semi-
algebraic description of the feasible set of discounted state-action frequencies in POMDPs, which
is closely related to the general spirit of semialgebraic statistics, where this is usually referred to
as the implitization problem (Zwiernik, 2016). Based on this we characterize the properties of the
optimization problem by its algebraic degree, a concept that has been advanced in recent works on
polynomial optimization (Bajaj, 1988; Nie & Ranestad, 2009; ¨Ozl¨um C¸ elik et al., 2021).

2

Published as a conference paper at ICLR 2022

Contributions We obtain results for inﬁnite-horizon POMDPs with memoryless stochastic poli-
cies under the mean or discounted reward criteria which can be summarized as follows.

1. We show that the state-action frequencies and the expected cumulative reward can be written as
fractions of determinantal polynomials in the entries of the stochastic policy matrix. We show that
the degree of these polynomials is directly related to the degree of observability (see Theorem 4).
2. We describe the set of feasible state-action frequencies as a basic semialgebraic set, i.e., as the
solution set to a system of polynomial equations and inequalities, for which we also derive closed
form expressions (see Theorem 16 and Remark 18).

3. We reformulate the expected cumulative reward optimization problem as the optimization of
a linear function subject to polynomial constraints (see Remark 19), which we use to solve a
navigation problem in a grid world (see Appendix F). This is a POMDP generalization of the
dual linear programming formulation of MDPs (Kallenberg, 1994; Puterman, 2014).

4. We present two methods for computing the number of critical points, which rely, respectively, on
the rational degree of the expected cumulative reward function and the geometric description of
the feasible set of state-action frequencies (see Theorem 20, Proposition 21 and Appendix D).

2 PRELIMINARIES

Y . An element Q ∈ ∆X

We denote the simplex of probability distributions over a ﬁnite set X by ∆X . An element µ ∈ ∆X
is a vector with non-negative entries µx = µ(x), x ∈ X adding to one. We denote the set of Markov
kernels from a ﬁnite set X to another ﬁnite set Y by ∆X
Y is a |X | × |Y| row
Y and Q(2) ∈ ∆Y
stochastic matrix with entries Qxy = Q(y|x), x ∈ X , y ∈ Y. Given Q(1) ∈ ∆X
Z
we denote their composition into a kernel from X to Z by Q(2) ◦ Q(1) ∈ ∆X
Z . Given p ∈ ∆X
and Q ∈ ∆X
Y we denote their composition into a joint probability distribution by p ∗ Q ∈ ∆X ×Y ,
(p ∗ Q)(x, y) := p(x)Q(y|x). The support of v ∈ RX is the set supp(v) = {x ∈ X : vx (cid:54)= 0}.
A partially observable Markov decision process or shortly POMDP is a tuple (S, O, A, α, β, r). We
assume that S, O and A are ﬁnite sets which we call state, observation and action space respectively.
We ﬁx a Markov kernel α ∈ ∆S×A
O which
we call observation mechanism. Further, we consider an instantaneous reward vector r ∈ RS×A.
We call the system fully observable if β = id1, in which case the POMDP simpliﬁes to a Markov
decision process or shortly MDP.
As policies we consider elements π ∈ ∆O
corresponding effective policy. A policy induces transition kernels Pπ ∈ ∆S×A

A and call the Markov kernel τ = π ◦ β ∈ ∆S
S by

which we call transition mechanism and a kernel β ∈ ∆S

A its

S

S×A and pπ ∈ ∆S
(π ◦ β)(a|s)α(s(cid:48)|s, a).

Pπ(s(cid:48), a(cid:48)|s, a) := α(s(cid:48)|s, a)(π ◦ β)(a(cid:48)|s(cid:48))

and pπ(s(cid:48)|s) :=

(cid:88)

a∈A

For any initial state distribution µ ∈ ∆S , a policy π ∈ ∆O
A deﬁnes a Markov process on S × A with
transition kernel Pπ which we denote by Pπ,µ. For a discount rate γ ∈ (0, 1) and γ = 1 we deﬁne

Rµ

γ (π) := EPπ,µ

(cid:20)

(1 − γ)

∞
(cid:88)

(cid:21)
γtr(st, at)

t=0

and Rµ

1 (π) := lim
T →∞

EPπ,µ

(cid:20) 1
T

T −1
(cid:88)

t=0

r(st, at)

(cid:21)
,

called the expected discounted reward and the expected mean reward, respectively. The goal is to
maximize this function over the policy polytope ∆O
A. For a policy π we deﬁne the value function
γ ∈ RS via V π
V π
γ (s) := Rδs
γ (π), s ∈ S, where δs is the Dirac distribution concentrated at s. A
(s, a) = (cid:104)r, ηπ,µ
short calculation shows that Rµ
(cid:105)S×A (Zahavy et al., 2021),
where

(cid:40)

γ

γ (π) = (cid:80)
(1 − γ) (cid:80)∞
limT →∞

s,a r(s, a)ηπ,µ
t=0 γtPπ,µ(st = s, at = a),
(cid:80)T −1
t=0

Pπ,µ(st = s, at = a),

γ

1
T

if γ ∈ (0, 1)
if γ = 1.

ηπ,µ
γ

(s, a) :=

(1)

γ

Here, ηπ,µ
is an element of ∆S×A called expected (discounted) state-action frequency (Derman,
1970), (discounted) visitation/occupancy measure or on-policy distribution (Sutton & Barto, 2018).
Denoting the state marginal of ηπ,µ
(s)(π ◦ β)(a|s). We
recall the following well-known facts.

γ ∈ ∆S we have ηπ,µ

(s, a) = ρπ,µ

by ρπ,µ

γ

γ

γ

1More generally, the system is fully observable if the supports of {β(·|s)}s∈S are disjoint subsets of O.

3

Published as a conference paper at ICLR 2022

Proposition 1 (Existence of state-action frequencies and rewards). Let (S, O, A, α, β, r) be a
POMDP, γ ∈ (0, 1] and µ ∈ ∆S . Then ηπ,µ
, ρπ,µ
A and µ ∈ ∆S
γ
and are continuous in γ ∈ (0, 1] for ﬁxed π and µ.

γ (π) exist for every π ∈ ∆O

and Rµ

γ

For γ = 1 we work under the following standard assumption in the (PO)MDP literature2.
Assumption 2 (Uniqueness of stationary disitributions). If γ = 1, we assume that for any policy
π ∈ ∆O

A there exists a unique stationary distribution η ∈ ∆S×A of Pπ.

The following proposition shows in particular that for any initial distribution µ, the inﬁnite time
horizon state-action frequency ηπ,µ

is the unique discounted stationary distribution of Pπ.

γ

Proposition 3 (State-action frequencies are discounted stationary). Let (S, O, A, α, β, r) be a
POMDP, γ ∈ (0, 1] and µ ∈ ∆S . Then ηπ,µ
is the unique element in ∆S×A satisfying the dis-
γ
γ + (1 − γ)(µ ∗ (π ◦ β)). Further, ρπ,µ
π ηπ,µ
counted stationarity equation ηπ,µ
is the unique
element in ∆S satisfying ρπ,µ

γ = γP T
π ρπ,µ

γ + (1 − γ)µ.

γ = γpT

γ

We denote the set of all state-action frequencies in the fully and in the partially observable case by

N µ

γ := (cid:8)ηπ,µ

γ ∈ ∆S×A | π ∈ ∆S
A

(cid:9)

and N µ,β

γ

:= (cid:8)ηπ,µ

γ ∈ ∆S×A | π ∈ ∆O
A

(cid:9) .

We have seen that the expected cumulative reward function Rµ

γ : ∆O

A → R factorises according to

∆O
A

fβ−→ ∆S

A

Ψµ
γ−−→ N µ,β

γ → R,

π (cid:55)→ π ◦ β (cid:55)→ ηπ,µ

γ

(cid:55)→ (cid:104)r, ηπ,µ

γ

(cid:105)S×A.

This is illustrated in Figure 1. We make use of this decomposition in two different ways. First,
in Section 3 we study the algebraic properties of the parametrization Ψµ
γ of the set of state-action
frequencies N µ,β

. In Section 4 we derive a description of N µ,β

via polynomial inequalities.

γ

γ

Observation policies

State policies

State-action frequencies

1

0

1

π

∆O
A

fβ−−−→

linear

∆S
A
∆S,β
A

τπ

Ψµ
γ−−−−→
rational

∆S×A

ηπ,µ
γ

1

0

1

N µ,β
γ

N µ
γ

π (cid:55)→ R
rational

τ (cid:55)→ R
rational

η (cid:55)→ R
linear

Figure 1: Geometry of a POMDP with two states, two actions and two observations. The top shows
A; the associated state policy polytope ∆S
the observation policy polytope ∆O
A (yellow) along with
its subset of effective policies ∆S,β
A (blue); and the corresponding sets of discounted state-action
frequencies in the simplex ∆S×A (a tetrahedron in this case). The bottom shows the graph of the
expected cumulative discounted reward R as a function of the observation policy π; the state policy
τ ; and the discounted state-action frequencies η. We characterize the parametrization and geometry
of these domains and the structure of the expected cumulative reward function.

2Assumption 2 is weaker than ergodicity, for which well known criteria exist. For γ < 1 the assumption is

not required, since the discounted stationary distributions are always unique.

4

00.5100.5100.5100.5100.5100.51-101-10100.51Published as a conference paper at ICLR 2022

3 THE PARAMETRIZATION OF DISCOUNTED STATE-ACTION FREQUENCIES

In this section we show that the discounted state-action frequencies, the value function and the
expected cumulative reward of POMDPs are rational functions and relate their rational degree, which
can be interpreted as a measure of their complexity, to the degree of observability. Here, we say that
a function is a rational function of degree at most k if it is the fraction of two polynomials of degree
at most k. By Cramer’s rule (see Appendix B.2), it holds that

ηπ,µ
γ

(s, a) = (π ◦ β)(a|s)ρπ,µ

γ

(s) = (π ◦ β)(a|s) · (1 − γ) ·

det(I − γpT
π )µ
s
det(I − γpT
π )

,

π )µ

where (I − γpT
s denotes the matrix obtained by replacing the s-row of I − γpT
π with µ. Since pπ
depends linearly on π and the determinant is a polynomial, this is a rational function in the entries
of π. For the degree, we show the following result in Appendix B.1.
Theorem 4 (Degree of POMDPs). Let (S, O, A, α, β, r) be a POMDP, µ ∈ ∆S be an initial dis-
tribution and γ ∈ (0, 1) a discount factor. The state-action frequencies ηπ,µ
, the value
γ
function V π
γ (π) are rational functions with common de-
nominator in the entries of the policy π. Further, if they are restricted to the subset Π ⊆ ∆O
A of
policies which agree with a ﬁxed policy π0 on all states outside of O ⊆ O, they have degree at most
(cid:8)s ∈ S | β(o|s) > 0 for some o ∈ O(cid:9)(cid:12)
(cid:12) .

γ and the expected cumulative reward Rµ

and ρπ,µ

(cid:12)
(cid:12)

γ

Hence, the number of states that are compatible with o determines the algebraic complexity of the
discounted state-action frequencies, the value function and the reward function. Various reﬁnements
of the theorem are presented in Appendix B.1. For the mean reward case and under an ergodicity
assumption, Grinberg & Precup (2013) showed that the stationary distributions are a rational func-
tion of degree of most |S| of the policy. From Theorem 4 we can derive multiple implications (see
also Appendix D.5.2 for implications on the optimization landscape):
Corollary 5 (Feasible state-action frequencies and value functions form semialgebraic sets). Con-
sider a POMDP (S, O, A, α, β, r) and let µ ∈ ∆S be an initial distribution and γ ∈ (0, 1) a
discount factor. The set of discounted state-action frequencies and the set of value functions are
semialgebraic sets3.

Proof. By Theorem 4, both sets possess a rational and thus a semialgebraic parametrization and are
semialgebraic by the Tarski-Seidenberg theorem (Neyman, 2003).

We compute the deﬁning linear and polynomial (in)equalities of the set of feasible state-action fre-
quencies in Section 4 for MDPs and POMDPs respectively, which shows in particular that also
in the mean case the state-action frequencies form a semialgebraic set. The special properties of
degree-one rational functions, which we elaborate in the Appendix B.3, imply the following results.
The ﬁrst one is a reﬁnement of Dadashi et al. (2019, Lemma 4), stating that linear interpolation
between two policies that differ on a single state leads to a linear interpolation of the corresponding
value functions. We generalize this to state-action frequencies, explicitly compute the interpolation
speed and describe the curves obtained by interpolation between arbitrary policies. Further, our
formulation extends to the mean reward case (see Remark 43).
Proposition 6. Let (S, A, α, r) be an MDP and γ ∈ (0, 1). Further, let π0, π1 ∈ ∆S
A be two
policies that differ on at most k states. For any λ ∈ [0, 1] let Vλ ∈ RS and ηµ
λ ∈ ∆S×A denote the
value function and state-action frequency belonging to the policy π0 + λ(π1 − π0) with respect to
the discount factor γ, the initial distribution µ and the instantaneous reward r. Then the rational
degrees of λ (cid:55)→ Vλ and λ (cid:55)→ ηλ are at most k. If they differ on at most one state ˜s ∈ S then
λ = ηµ
ηµ

Vλ = V0 + c(λ) · (V1 − V0) and

0 + c(λ) · (ηµ

for all λ ∈ [0, 1],

1 − ηµ
0 )

where

c(λ) =

det(I − γp1)λ
det(I − γpλ)

=

det(I − γp1)λ
(det(I − γp1) − det(I − γp0))λ + det(I − γp0)

= λ ·

ρµ
λ(˜s)
ρµ
1 (˜s)

.

3A semialgebraic set is a set deﬁned by a number of polynomial inequalities or a ﬁnite union of such sets;

for details see Appendix A.2.

5

Published as a conference paper at ICLR 2022

In particular, for a blind controller with two actions the set of feasible value functions and the set of
feasible state-action frequencies are pieces of curves with rational parametrization of degree at most
k = |S|. By Theorem 4, the cumulative reward of (PO)MDPs is a degree-one rational function in
every row of the (effective) policy. Since degree-one rational functions attain their maximum in a
vertex (Corollary 39), we immediately obtain the existence of an optimal policy which is determin-
istic on every observation from which the state can be reconstructed, which has been shown using
other methods by Mont´ufar et al. (2015).
Proposition 7 (Determinism of optimal policies). Let (S, O, A, α, β, r) be a POMDP, µ ∈ ∆S be
an initial distribution and γ ∈ (0, 1) a discount factor and let π ∈ ∆O
A be an arbitrary policy and
denote the set of observations o such that |{s ∈ S | β(o|s) > 0}| ≤ 1 by O. Then there is a policy
˜π, which is deterministic on every o ∈ O such that Rµ

γ (˜π) ≥ Rµ

γ (π).

Proof. For o ∈ O, the reward function restricted to the o-component of the policy is a rational
function of degree at most one. By Corollary 39 (see Appendix B.3.2), there is a policy ˜π, which is
deterministic on o and satisﬁes Rµ

γ (π). Iterating over o ∈ O yields the result.

γ (˜π) ≥ Rµ

On observations which can be made from more than one state, bounds on the required stochasticity
were established by Mont´ufar & Rauh (2017); Mont´ufar et al. (2019).

4 THE SET OF FEASIBLE DISCOUNTED STATE-ACTION FREQUENCIES

In Corollary 5, we have seen that the state-action frequencies form a semialgebraic set. Now we
aim to describe its deﬁning polynomial inequalities. In the case of full observability, the feasible
state-action freqencies are known to form a polytope (Derman, 1970; Altman & Shwartz, 1991)
which is closely linked to the dual linear programming formulation of MDPs (Hordijk & Kallen-
berg, 1981), see also Figure 1. We ﬁrst describe the combinatorial properties of this polytope (see
Appendix C.1.2) and extend the result to the partially observable case, for which we obtain explicit
polynomial inequalities induced by the partial observability under a mild assumption. Most proofs
are postponed to Appendix C. In Section 5 we discuss how the degree of these deﬁning polynomials
allows us to upper bound the number of critical points of the optimization problem. We use the fol-
lowing explicit version of the classic characterization of the state-action frequencies as a polytope
(see Appendix C.1).
Proposition 8 (Characterization of N µ
bution and γ ∈ (0, 1]. It holds that

γ ). Let (S, A, α, r) be an MDP, µ ∈ ∆S be an initial distri-

N µ

γ = ∆S×A ∩ (cid:8)η ∈ RS×A | (cid:104)ws

γ, η(cid:105)S×A = (1 − γ)µs for s ∈ S(cid:9)

(2)

where ws

γ := δs ⊗ 1A − γα(s|·, ·). For γ ∈ (0, 1), ∆S×A can be replaced by [0, ∞)S×A in (2).

Now we turn towards the partially observable case and introduce the following notation.
Deﬁnition 9 (Effective policy polytope). We call the set of effective policies τ = π ◦ β ∈ ∆S
effective policy polytope and denote it by ∆S,β
A .

A the

Note that ∆S,β
A is indeed a polytope since it is the image of the polytope ∆O
A under the linear
mapping π (cid:55)→ π ◦ β = βπ. Hence, we can write it as an intersection ∆S,β
A ∩ U ∩ C,
where U, C ⊆ RS×A are an afﬁne subspace and a polyhedral cone and describe a ﬁnite set of linear
equalities and a ﬁnite set of linear inequalities respectively.

A = ∆S

Deﬁning linear inequalities of the effective policy polytope Obtaining inequality descriptions of
the images of polytopes under linear maps is a fundamental problem that is non-trivial in general. It
can be approached algorithmically, e.g., by Fourier-Motzkin elimination, block elimination, vertex
approaches, and equality set projection (Jones et al., 2004). In the special case where the linear map
is injective, one can give the deﬁning inequalities in closed form as we show in Appendix C.2.1.
Hence, for the purpose of obtaining closed-formulas for the effective policy polytope we make the
following assumption. However, our subsequent analysis in Section 4 can handle any inequalities.
Assumption 10. The matrix β ∈ ∆S

O ⊆ RS×O has linearly independent columns.

6

Published as a conference paper at ICLR 2022

Remark 11. The assumption above does not imply that the system is fully observable. Recall
that if β has linearly independent columns, the Moore-Penrose takes the form β+ = (βT β)−1βT .
An interesting special case is when β is deterministic but may map several states to the same
observation (this is the partially observed setting considered in numerous works).
In this case,
β+ = diag(n−1
|O|)βT , where no denotes the number of states with observation o. In this
case, β+
so agrees with the conditional distribution β(s|o) with respect to a uniform prior over the
states; however, this is not in general the case since β+ can have negative entries.

1 , . . . , n−1

Theorem 12 (H-description of the effective policy polytope). Let (S, O, A, α, β, r) be a POMDP
and let Assumption 10 hold. Then it holds that

∆S,β

A = ∆S

A ∩ U ∩ C = U ∩ C ∩ D,

(3)

where U = {π ◦ β | π ∈ RS×O} = ker(βT )⊥ is a subspace, C = {τ ∈ RS×A | β+τ ≥ 0}
is a pointed polyhedral cone and D = {τ ∈ RS×A | (cid:80)
a(β+τ )oa = 1 for all o ∈ O} an afﬁne
subspace. Further, the face lattices of ∆O

A are isomorphic.

A and ∆S,β

Deﬁning polynomial inequalities of the feasible state-action frequencies
inequalities in ∆S
A to inequalities in the set of state-action frequencies N µ
of π (cid:55)→ ηπ is given through conditioning (see Proposition 46) under the following assumption.
Assumption 13 (Positivity). Let ρπ,µ

γ > 0 hold entrywise for all policies π ∈ ∆S
A.

In order to transfer
γ , we use that the inverse

This assumption holds in particular, if either α > 0 and γ > 0 or γ < 1 and µ > 0 entrywise
(see Appendix C.1). Assumption 13 is standard in linear programming approaches and necessary
for the convergence of policy gradient methods in MDPs (Kallenberg, 1994; Mei et al., 2020). By
conditioning, we can translate linear inequalities in ∆S

A into polynomial inequalities in N µ
γ .
Proposition 14 (Correspondence of inequalities). Let (S, A, α, r) be an MDP, τ ∈ ∆S
A and let
η ∈ ∆S×A denote its corresponding discounted state-action frequency for some µ ∈ ∆S and
γ ∈ (0, 1]. Let c ∈ R, b ∈ RS×A and set S := {s ∈ S | bsa (cid:54)= 0 for some a ∈ A}. Then

(cid:88)

s,a

bsaτsa ≥ c

implies

(cid:88)

(cid:88)

bsaηsa

(cid:89)

(cid:88)

ηs(cid:48)a(cid:48) − c

(cid:89)

(cid:88)

ηs(cid:48)a(cid:48) ≥ 0,

s∈S

a

s(cid:48)∈S\{s}

a(cid:48)

s(cid:48)∈S

a(cid:48)

where the right is a multi-homogeneous polynomial4 in the blocks (ηsa)a∈A ∈ RA with multi-degree
1S ∈ NS . If further Assumption 13 holds, the inverse implication also holds.

The preceding proposition shows that the state-action frequencies of a linearly constrained policy
model, where the constraints only address the policy in individual states form a polytope. However,
the effective policy polytope is almost never of this box type (see Remark 55).

Example 15 (Blind controller). For a blind controller the linear equalities deﬁning the effective
policy polytope in ∆S
A are τs1a − τs2a = τ (a|s1) − τ (a|s2) = 0 for all a ∈ A, s1, s2 ∈ S. They
translate into the polynomial equalities ηs1aρs2 − ηs2aρs1 = 0 for all a ∈ A, s1, s2 ∈ S. In the case
that A = {a1, a2}, we obtain

0 = ηs1a1(ηs2a1 + ηs2a1) − ηs2a1(ηs1a1 + ηs1a1) = ηs1a1 ηs2a2 − ηs1a2ηs2a1

for all s1, s2 ∈ S,

which is precisely the condition that all 2 × 2 minors of η vanish. Hence, in this case the set of
state-action frequencies N µ,β
γ of state-action frequencies of the
associated MDP and the determinantal variety of rank one matrices.

is given as the intersection of N µ

γ

The following result describes the geometry of the set of feasible state-action frequencies.
Theorem 16. Let (S, O, A, α, β, r) be a POMDP, µ ∈ ∆S and γ ∈ (0, 1] and assume that As-
sumption 13 holds. Then we have N µ,β
γ ∩ V ∩ B, where V is a variety described by
multi-homogeneous polynomial equations and B is a basic semialgebraic set described by multi-
homogeneous polynomial inequalities. Further, the face lattices of ∆S,β
are isomorphic.

= N µ

γ

A and N µ,β

γ

4A polynomial p : Rn1 ×· · ·×Rnk → R is called multi-homogeneous with multi-degree (d1, . . . , dk) ∈ Nk,

if it is homogeneous of degree dj in the j-th block of variables for j = 1, . . . , k.

7

Published as a conference paper at ICLR 2022

Remark 17. The variety V corresponds to the subspace U and the basic semialgebraic set B to the
cone C from (3). Further, closed form expressions for the deﬁning polynomials can be computed
using Proposition 14 (see also Remark 18). The statement about isomorphic face lattices is in the
sense that ∆S,β
have the same number of surfaces of a given dimension with the same
neighboring properties. This can be seen in Figure 1, where the effective policy polytope and the set
of state-action frequencies both have four vertices, four edges, and one two-dimensional face.

A and N µ,β

γ

Remark 18. By Theorem 12 and Proposition 14, the deﬁning polynomials of the basic semialge-
braic set B from Theorem 16 are indexed by a ∈ A, o ∈ O and are given by

pao(η) :=

(cid:88)

(cid:18)

β+
osηsa

(cid:89)

(cid:88)

(cid:19)

ηs(cid:48)a(cid:48)

=

(cid:88)

(cid:18) (cid:88)

(cid:19) (cid:89)

β+
os(cid:48)

ηsf (s) ≥ 0, (4)

a(cid:48)

s∈So

f : So→A

s(cid:48)∈f −1({a})

s∈So
s(cid:48)∈So\{s}
where So := {s ∈ S | β+
os (cid:54)= 0}. The polynomials depend only on β and not on γ, µ nor α, and
have |So||A||So|−1 monomials of degree |So| of the form (cid:81)
ηsf (s) for some f : So → A. In
s∈So
particular, we can read of the multi-degree of pao with respect to the blocks (ηsa)a∈A which is given
by 1So (see also Proposition 14). A complete description of the set N µ,β
via (in)equalities follows
from the description of N µ
γ via linear (in)equalities given in (2). In Section 5 we discuss how the
degree of these polynomials controls the complexity of the optimization problem.
Remark 19 (Planning in POMDPs as a polynomial optimization problem). The semialgebraic de-
scription of the set N µ,β
of feasible state-action distributions allows us to reformulate the reward
maximization as a polynomially constrained optimization problem with linear objective (see also
Remark 59 and Algorithm 1). This reformulation allows the use of constrained optimization algo-
rithms, which we demonstrate in Appendix F on the toy example of Figure 1 and a grid world. Note
that this polynomial program is different to the quadratic program obtained by Amato et al. (2006).

γ

γ

5 NUMBER AND LOCATION OF CRITICAL POINTS

Although the reward function of MDPs is non convex, it still exhibits desirable properties from a
standpoint of optimization. For example, without any assumptions, every policy can be continuously
connected to an optimal policy by a path along which the reward is monotone (see Appendix D.5).
Under mild conditions, all policies which are critical points of the reward function are globally
optimal (Bhandari & Russo, 2019). In partially observable systems, the situation is fundamentally
different.
In this case, suboptimal local optima of the reward function can exist as can be seen
in Figure 1 (see also Poupart et al., 2011; Bhandari & Russo, 2019). In the following we use the
geometric description of the discounted state-action frequencies to study the number and location of
critical points. These are important properties of the optimization problem and have implications on
the required stochasticity of optimal policies. In Appendix D we discuss the mean reward case and
an example and describe the sublevelsets as semialgebraic sets.
We regard the reward as a linear function p0 over the set of feasible state-action frequencies N µ,β
.
Under Assumption 13 π (cid:55)→ ηπ is injective and has a full-rank Jacobian everywhere (see Ap-
pendix C.1.1). Hence, the critical points in the policy polytope ∆O
A correspond to the critical points
of p0 on N µ,β
(see Trager et al., 2019). In general, critical points of this linear function can occur on
every face of the semialgebraic set N µ,β
. The optimization problem thus has a combinatorial and a
geometric component, corresponding to the number of faces of each dimension and the number of
critical points in the relative interior of any given face. We have discussed the combinatorial part in
γ = {η ∈ RS×A | pi(η) ≤ 0, i ∈ I},
Theorem 16 and focus now on the geometric part. Writing N µ,β
we are interested in the number of critical points on the interior of a face,

γ

γ

γ

int(FJ ) = {η ∈ N µ,β

γ

| pj(η) = 0 for j ∈ J, pi(η) > 0 for i ∈ I \ J}.

Note that a point η is critical on int(FJ ), if and only if it is a critical point on the variety VJ :=
{η ∈ RS×A | pj(η) = 0 for j ∈ J}. For the sake of notation we write J = {1, . . . , m}. We can
bound the number of critical points in the interior of the face by the number of critical points of the
polynomial optimization problem of optimizing p0(η) subject to p1(η) = · · · = pm(η) = 0. This
number is upper bounded by the algebraic degree of the problem which controls also the (algebraic)
complexity of optimal policies (see Appendix D.1 for details). Using Theorem 12, Proposition 14
and an upper bound on the algebraic degree of polynomial optimization by Nie & Ranestad (2009)
yields the following result.

8

Published as a conference paper at ICLR 2022

Theorem 20. Consider a POMDP (S, O, A, α, β, r), γ ∈ (0, 1), assume that r is generic, that
β ∈ RS×O is invertible, and that Assumption 13 holds. For any given I ⊆ A × O consider the
following set of policies, which is the relative interior of a face of the policy polytope:

int(F ) = (cid:8)π ∈ ∆O

A | π(a|o) = 0 if and only if (a, o) ∈ I(cid:9) .

Let O := {o ∈ O | (a, o) ∈ I for some a} and set ko := |{a | (a, o) ∈ I}| as well as do := |{s |
β−1
os (cid:54)= 0}|. Then, the number of critical points of the reward function on int(F ) is at most

(cid:32)

(cid:89)

(cid:33)

dko
o

·

o∈O

(cid:88)

(cid:89)

(do − 1)io,

(cid:80)

o∈O io=l

o∈O

(5)

where l = |S|(|A| − 1) − |I|. If α and µ are generic, this bound can be reﬁned by computing the
polar degrees of multi-homogeneous varieties (see Proposition 21 for a special case). The same
bound holds in the mean reward case γ = 1 for l given in Remark 61.

By results from Mont´ufar & Rauh (2017) a POMDP has optimal memoryless stochastic policies with
| supp π(·|o)| ≤ lo, where lo = |supp β(o|·)| ≥ 1. Hence, we may restrict attention to optimization
over N µ,β
o∈O ko active inequalities (zeros in the policy), where ko = max{|A|−lo, 0}.
Over these faces of the feasible set, the algebraic degree of the reward maximization problem is
upper bounded by (cid:81)

o∈O(do − 1)io due to Theorem 20.

γ with k = (cid:80)

o∈O dko
o

i1+···+io=|S|(|A|−1)−k

(cid:80)

(cid:81)

In the special case of MDPs the bound shows that for MDPs only deterministic policies can be
critical points of the reward function (see Corollary 62). Setting I := ∅ shows that there are no
critical points in the interior of the policy polytope ∆O
A. This requires the assumption that β is
invertible (see Appendix D.4). The bound in Theorem 20 neglects the speciﬁc algebraic structure of
the problem, and can be reﬁned by considering polar degrees of determinantal varieties. This yields
the following tighter upper bound for a blind controller with two actions (see Appendix D.3).
Proposition 21 (Number of critical points in a blind controller). Let (S, O, A, α, β, r) be a POMDP
describing a blind controller with two actions, i.e., O = {o} and A = {a1, a2} and let r, α and µ
be generic and let γ ∈ (0, 1). Then the reward function Rµ
γ has at most |S| critical points in the
interior int(∆O

A) ∼= (0, 1) of the policy polytope and hence at most |S| + 2 critical points.

In Appendix D.4 we provide examples of blind controllers which have several critical points in
A) and strict maxima at the two endpoints of the interval [0, 1] ∼= ∆O
the interior (0, 1) ∼= int(∆O
respectively. Such points are called smooth and non-smooth critical points respectively.

A

6 CONCLUSION

We described geometric and algebraic properties of POMDPs and related the rational degree of the
discounted state-action frequencies and the expected cumulative reward function to the degree of
observability. We described the set of feasible state-action frequencies as a basic semialgebraic
set and computed explicit expressions for the deﬁning polynomials.
In particular, this yields a
polynomial programming formulation of POMDPs extending the linear programming formulation
of MDPs. Based on this we use polynomial optimization theory to bound the number of critical
points of the reward function over the polytope of memoryless stochastic policies. Our analysis
also yields insights into the optimization landscape, such as the number of connected components
of superlevel sets of the expected reward. Finally, we use a navigation problem in a grid world
to demonstrate that the polynomial programming formulation can offer a computationally feasible
approach to the reward maximization problem.

Our analysis focuses on inﬁnite-horizon problems and memoryless policies with ﬁnite state, obser-
vation, and action spaces. Continuous spaces are interesting avenues, since they occur in real world
application like robotics. The general bound on the number of critical points in Theorem 20 does
not exploit the special multi-homogeneous structure of the problem, which could allow for tighter
bounds as illustrated in Proposition 21 for blind controllers. Computing polar degrees is a challeng-
ing problem that remains to be studied using more sophisticated algebraic tools. Possible extensions
of our work include the generalization to policies with ﬁnite memories as sketched in Appendix E.1.
Further, we believe that it is interesting to explore to what extent our results can be used to identify
policy classes guaranteed to contain maximizers of the reward in POMDPs.

9

Published as a conference paper at ICLR 2022

ACKNOWLEDGMENTS

The authors thank Alex Tong Lin and Thomas Merkh for valuable discussions on POMDPs, Bernd
Sturmfels for sharing his expertise on algebraic degrees and Mareike Dressler, Marina Garrote-
L´opez and Kemal Rose for their discussions on polynomial optimization. The authors acknowledge
support by the ERC under the European Union’s Horizon 2020 research and innovation programme
(grant agreement no 757983). JM received support from the International Max Planck Research
School for Mathematics in the Sciences and the Evangelisches Studienwerk Villigst e.V..

REFERENCES

Eitan Altman and Adam Shwartz. Markov decision problems and state-action frequencies. SIAM

journal on control and optimization, 29(4):786–809, 1991.

Christopher Amato, Daniel S Bernstein, and Shlomo Zilberstein. Solving pomdps using quadrat-
ically constrained linear programs. In Proceedings of the ﬁfth international joint conference on
Autonomous agents and multiagent systems, pp. 341–343, 2006.

Miguel F Anjos and Jean B Lasserre. Handbook on semideﬁnite, conic and polynomial optimization,

volume 166. Springer Science & Business Media, 2011.

Kamyar Azizzadenesheli, Alessandro Lazaric, and Animashree Anandkumar. Open problem: Ap-
proximate planning of POMDPs in the class of memoryless policies. In Conference on Learning
Theory, pp. 1639–1642. PMLR, 2016.

Kamyar Azizzadenesheli, Yisong Yue, and Animashree Anandkumar. Policy Gradient in Partially

Observable Environments: Approximation and Convergence. arXiv:1810.07900, 2018.

Chanderjit Bajaj. The Algebraic Degree of Geometric Optimization Problems. Discrete & Compu-

tational Geometry, 3(2):177–191, 1988.

Serguei Barannikov, Alexander Korotin, Dmitry Oganesyan, Daniil Emtsev, and Evgeny Burnaev.

Barcodes as summary of loss function’s topology. arXiv:1912.00043, 2019.

Saugata Basu. Different Bounds on the Different Betti Numbers of Semi-Algebraic Sets. Discrete

and Computational Geometry, 30(1):65–85, 2003.

Saugata Basu. Algorithms in Real Algebraic Geometry: A Survey. arXiv:1409.1534, 2014.

Saugata Basu, Richard Pollack, and Marie-Franc¸oise Roy. Algorithms in Real Algebraic Geometry

(Algorithms and Computation in Mathematics). Springer-Verlag, Berlin, Heidelberg, 2006.

Jonathan Baxter and Peter L Bartlett.

Inﬁnite-Horizon Policy-Gradient Estimation. Journal of

Artiﬁcial Intelligence Research, 15:319–350, 2001.

Jonathan Baxter, Peter L Bartlett, et al. Reinforcement Learning in POMDP’s via Direct Gradient

Ascent. In ICML, pp. 41–48. Citeseer, 2000.

Richard Bellman. A Markovian decision process. Journal of mathematics and mechanics, 6(5):

679–684, 1957.

Jalaj Bhandari and Daniel Russo. Global Optimality Guarantees For Policy Gradient Methods.

arXiv:1906.01786, 2019.

Jacek Bochnak, Michel Coste, and Marie-Franc¸oise Roy. Real Algebraic Geometry, volume 36.

Springer Science & Business Media, 2013.

Paul Breiding, T¨urk¨u ¨Ozl¨um C¸ elik, Timothy Duff, Alexander Heaton, Aida Maraj, Anna-Laura
Sattelberger, Lorenzo Venturello, and O˘guzhan Y¨ur¨uk. Nonlinear Algebra and Applications.
arXiv:2103.16300, 2021.

Michael J Catanzaro, Justin M Curry, Brittany Terese Fasy, J¯anis Lazovskis, Greg Malen, Hans
Riess, Bei Wang, and Matthew Zabka. Moduli spaces of morse functions for persistence. Journal
of Applied and Computational Topology, 4(3):353–385, 2020.

10

Published as a conference paper at ICLR 2022

Krishnendu Chatterjee, Martin Chmel´ık, and Mathieu Tracol. What is decidable about partially
observable Markov decision processes with ω-regular objectives. Journal of Computer and System
Sciences, 82(5):878–911, 2016. URL https://www.sciencedirect.com/science/
article/pii/S0022000016000246.

Victor Cohen and Axel Parmentier. Linear Programming for Decision Processes with Partial Infor-

mation. arXiv:1811.08880, 2018.

Robert Dadashi, Adrien Ali Taiga, Nicolas Le Roux, Dale Schuurmans, and Marc G Bellemare.
The value function polytope in reinforcement learning. In International Conference on Machine
Learning, pp. 1486–1495. PMLR, 2019.

Guy De Ghellinck. Les problemes de decisions sequentielles. Cahiers du Centre d’Etudes de

Recherche Op´erationnelle, 2(2):161–179, 1960.

Francois d’Epenoux. A Probabilistic Production and Inventory Problem. Management Science, 10

(1):98–108, 1963.

Cyrus Derman. Finite state Markovian decision processes. Academic Press, 1970.

Joseph Leo Doob. Stochastic processes, volume 10. New York Wiley, 1953.

Jan Draisma, Emil Horobet¸, Giorgio Ottaviani, Bernd Sturmfels, and Rekha R. Thomas. The Eu-
clidean distance degree of an algebraic variety. Foundations of Computational Mathematics, 16
(1):99–149, 2016. URL https://doi.org/10.1007/s10208-014-9240-x.

Dean Gillette. 9. stochastic games with zero stop probabilities. In Contributions to the Theory of

Games (AM-39), Volume III, pp. 179–188. Princeton University Press, 1958.

D Yu Grigor’ev and NN Vorobjov. Counting connected components of a semialgebraic set in subex-

ponential time. Computational Complexity, 2(2):133–186, 1992.

Yuri Grinberg and Doina Precup. Average Reward Optimization Objective In Partially Observable

Domains. In International Conference on Machine Learning, pp. 320–328. PMLR, 2013.

J William Helton and Victor Vinnikov. Linear matrix inequality representation of sets. Communica-
tions on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of Mathemat-
ical Sciences, 60(5):654–674, 2007.

A Hordijk and LCM Kallenberg. Linear Programming Methods for Solving Finite Markovian De-

cision Problems . In DGOR, pp. 468–482. Springer, 1981.

Ronald A Howard. Dynamic programming and Markov processes. MIT Press, 1960.

Jeffrey J. Hunter. Chapter 2 - generating functions.

In Jeffrey J. Hunter (ed.), Mathematical
Techniques of Applied Probability, pp. 24–67. Academic Press, 1983. URL https://www.
sciencedirect.com/science/article/pii/B9780123618016500083.

Rodrigo Toro Icarte, Richard Valenzano, Toryn Q. Klassen, Phillip Christoffersen, Amir mas-
soud Farahmand, and Sheila A. McIlraith. The act of remembering: A study in partially ob-
servable reinforcement learning, 2021. URL https://openreview.net/forum?id=
uFkGzn9RId8.

Tommi Jaakkola, Satinder Singh, and Michael Jordan.

Reinforcement Learning Algo-
rithm for Partially Observable Markov Decision Problems.
In G. Tesauro, D. Touret-
zky, and T. Leen (eds.), Advances in Neural Information Processing Systems, volume 7.
MIT Press, 1995. URL https://proceedings.neurips.cc/paper/1994/file/
1c1d4df596d01da60385f0bb17a4a9e0-Paper.pdf.

Colin Jones, E. C. Kerrigan, and Jan Maciejowski. Equality Set Projection: A new algorithm for
the projection of polytopes in halfspace representation. Technical report, Cambridge, 2004. URL
http://publications.eng.cam.ac.uk/327023/.

Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning and acting in

partially observable stochastic domains. Artiﬁcial intelligence, 101(1-2):99–134, 1998.

11

Published as a conference paper at ICLR 2022

Lodewijk CM Kallenberg. Survey of linear programming for standard and nonstandard Markovian

control problems. Part I: Theory. Zeitschrift f¨ur Operations Research, 40(1):1–42, 1994.

Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The

International Journal of Robotics Research, 32(11):1238–1274, 2013.

HT Kung. The computational complexity of algebraic numbers. In Proceedings of the ﬁfth annual

ACM symposium on Theory of computing, pp. 152–159, 1973.

Amy N. Langville and William J. Stewart. The Kronecker product and stochastic automata networks.
Journal of Computational and Applied Mathematics, 167(2):429–447, 2004. URL https://
www.sciencedirect.com/science/article/pii/S0377042703009312.

Jean Bernard Lasserre. An introduction to polynomial and semi-algebraic optimization, volume 52.

Cambridge University Press, 2015.

Yanjie Li, Baoqun Yin, and Hongsheng Xi. Finding optimal memoryless policies of POMDPs
under the expected average reward criterion. European Journal of Operational Research, 211(3):
556–567, 2011. URL https://www.sciencedirect.com/science/article/pii/
S0377221710008805.

Michael L. Littman. An optimization-based categorization of reinforcement learning environ-
In Jean-Arcady Meyer, Herbert L. Roitblat, and Stewart W. Wilson (eds.), From Ani-
ments.
mals to Animats 2, pp. 262–270. MIT Press, 1993. URL http://www.cs.rutgers.edu/
˜mlittman/papers/sab92.giveout.ps.

Michael L. Littman. Memoryless policies: Theoretical limitations and practical results. In Proceed-
ings of the Third International Conference on Simulation of Adaptive Behavior: From Animals to
Animats 3: From Animals to Animats 3, SAB94, pp. 238–245. MIT Press, 1994.

John Loch and Satinder P. Singh. Using Eligibility Traces to Find the Best Memoryless Policy in
Partially Observable Markov Decision Processes. In Proceedings of the Fifteenth International
Conference on Machine Learning, ICML ’98, pp. 323–331, San Francisco, CA, USA, 1998.
Morgan Kaufmann Publishers Inc.

Omid Madani, Steve Hanks, and Anne Condon.

On the undecidability of probabilis-
Artiﬁcial Intelligence, 147(1):
tic planning and related stochastic optimization problems.
5–34, 2003. URL https://www.sciencedirect.com/science/article/pii/
S0004370202003788. Planning with Uncertainty and Incomplete Information.

Alan S Manne. Linear Programming and Sequential Decisions. Management Science, 6(3):259–267,

1960.

Peter McMullen and Egon Schulte. Abstract regular polytopes, volume 92. Cambridge University

Press, 2002.

Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. On the global convergence
rates of softmax policy gradient methods. In International Conference on Machine Learning, pp.
6820–6829. PMLR, 2020.

George E. Monahan. A survey of partially observable Markov decision processes: Theory, models,
and algorithms. Management Science, 28(1):1–16, 1982. URL http://www.jstor.org/
stable/2631070.

Guido Mont´ufar and Johannes Rauh. Geometry of Policy Improvement. In International Conference

on Geometric Science of Information, pp. 282–290. Springer, 2017.

Guido Mont´ufar, Keyan Ghazi-Zahedi, and Nihat Ay. Geometry and Determinism of Optimal Sta-
tionary Control in Partially Observable Markov Decision Processes. arXiv:1503.07206, 2015.

Guido Mont´ufar, Johannes Rauh, and Nihat Ay. Task-agnostic constraining in average reward
POMDPs. In Task-agnostic reinforcement learning Workshop at ICLR 2019. 2019. URL https:
//tarl2019.github.io/assets/papers/montufar2019taskagnostic.pdf.

12

Published as a conference paper at ICLR 2022

Kevin P. Murphy. A Survey of POMDP Solution Techniques. Environment, 2, 10 2000.

Tim Netzer and Andreas Thom. Polynomials with and without determinantal representations. Linear

algebra and its applications, 437(7):1579–1595, 2012.

Abraham Neyman. Real Algebraic Tools in Stochastic Games. In Stochastic games and applica-

tions, pp. 57–75. Springer, 2003.

Jiawang Nie and Kristian Ranestad. Algebraic Degree of Polynomial Optimization. SIAM Journal
on Optimization, 20(1):485–502, 2009. URL https://doi.org/10.1137/080716670.

Christos H Papadimitriou and John N Tsitsiklis. The complexity of Markov decision processes.

Mathematics of operations research, 12(3):441–450, 1987.

Leonid Peshkin, Nicolas Meuleau, and Leslie Pack Kaelbling. Learning Policies with External
Memory. In Proceedings of the 16th International Conference on Machine Learning, pp. 307–
314. Morgan Kaufmann, 1999.

Pascal Poupart, Tobias Lang, and Marc Toussaint. Analyzing and escaping local optima in plan-
ning as inference for partially observable domains. In Joint European Conference on Machine
Learning and Knowledge Discovery in Databases, pp. 613–628. Springer, 2011.

Martin L Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John

Wiley & Sons, 2014.

Johannes Rauh, Nihat Ay, and Guido Mont´ufar. A continuity result for optimal memoryless planning

in pomdps. 2021.

Jesus M Ruiz.

Semialgebraic and semianalytic sets. Cahiers du s´eminaire d’histoire des

math´ematiques, 1:59–70, 1991.

Satinder P Singh, Tommi Jaakkola, and Michael I Jordan. Learning without state-estimation in
partially observable Markovian decision processes. In Machine Learning Proceedings 1994, pp.
284–292. Elsevier, 1994.

Lucca Sodomaco. The Distance Function from the Variety of partially symmetric rank-one Tensors.
PhD thesis, University of Florence, Department of Mathematics and Computer Science, 2020.

Pierre-Jean Spaenlehauer. Solving multi-homogeneous and determinantal systems: algorithms, com-

plexity, applications. PhD thesis, Universit´e Pierre et Marie Curie (Univ. Paris 6), 2012.

Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. MIT press, 2018.

Richard S Sutton, David A McAllester, Satinder P Singh, Yishay Mansour, et al. Policy Gradient
In NIPs, volume 99, pp.

Methods for Reinforcement Learning with Function Approximation.
1057–1063. Citeseer, 1999.

Gerald Tesauro. Temporal Difference Learning and TD-Gammon. Commun. ACM, 38(3):58–68,

March 1995. URL https://doi.org/10.1145/203330.203343.

Sascha Timme. Numerical Nonlinear Algebra. PhD thesis, Technische Universit¨at Berlin (Ger-

many), 2021.

Matthew Trager, Kathl´en Kohn, and Joan Bruna. Pure and Spurious Critical Points: a Geometric
Study of Linear Networks. In International Conference on Learning Representations, 2019.

Nikos Vlassis, Michael L Littman, and David Barber. On the Computational Complexity of Stochas-
tic Controller Optimization in POMDPs. ACM Transactions on Computation Theory (TOCT), 4
(4):1–8, 2012.

Robert Vrabel. A note on the matrix determinant lemma. International Journal of Pure and Applied

Mathematics, 111(4):643–646, 2016.

Stephan Wilhelm Weis. Exponential Families with Incompatible Statistics and Their Entropy Dis-

tance. Friedrich-Alexander-Universit¨at Erlangen-N¨urnberg (Germany), 2010.

13

Published as a conference paper at ICLR 2022

John Williams and Satinder Singh.

Experimental Results on Learning Stochastic Memory-
less Policies for Partially Observable Markov Decision Processes.
In M. Kearns, S. Solla,
and D. Cohn (eds.), Advances in Neural Information Processing Systems, volume 11.
MIT Press, 1999. URL https://proceedings.neurips.cc/paper/1998/file/
1cd3882394520876dc88d1472aa2a93f-Paper.pdf.

Tom Zahavy, Brendan O’Donoghue, Guillaume Desjardins, and Satinder Singh. Reward is enough

for convex MDPs. arXiv:2106.00661, 2021.

G¨unter M Ziegler. Lectures on Polytopes, volume 152. Springer Science & Business Media, 2012.

Piotr Zwiernik. Semialgebraic statistics and latent tree models. Monographs on Statistics and

Applied Probability, 146:146, 2016.

Karl Johan ˚Astr¨om. Optimal Control of Markov Processes with Incomplete State Information. Jour-
nal of Mathematical Analysis and Applications, 10:174–205, 1965. URL https://lup.lub.
lu.se/search/ws/files/5323668/8867085.pdf.

T¨urk¨u ¨Ozl¨um C¸ elik, Asgar Jamneshan, Guido Mont´ufar, Bernd Sturmfels, and Lorenzo Ven-
turello. Wasserstein distance to independence models. Journal of Symbolic Computation, 104:
855–873, 2021. URL https://www.sciencedirect.com/science/article/pii/
S0747717120301152.

14

Published as a conference paper at ICLR 2022

APPENDIX

The Sections A–D of the Appendix correspond to the Sections 2–5 of the main body. We present the
postponed proofs and elaborate various remarks in more detail. In Appendix E we discuss possible
extensions of our results to memory policies and polynomial POMDPs. In Appendix F we provide
details on the example plotted in Figure 1 and provide a plot of a three dimensional state-action
frequency set.

A Details on the Preliminaries

A.1 Partially observable Markov decision processes . . . . . . . . . . . . . . . . . . .

A.2 Semialgebraic sets and their face lattices . . . . . . . . . . . . . . . . . . . . . . .

B Details on the Parametrizaton of Discounted State-Action Frequencies

B.1 The degree of determinantal polynomials

. . . . . . . . . . . . . . . . . . . . . .

B.2 The degree of POMDPs .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

B.3 Properties of degree-one rational functions . . . . . . . . . . . . . . . . . . . . . .

C Details on the Geometry of State-Action Frequencies

C.1 The fully observable case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

C.2 The partially observable case . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

D Details on the Optimization

D.1 Introduction to algebraic degrees . . . . . . . . . . . . . . . . . . . . . . . . . . .

D.2 General upper bound on the number of critical points . . . . . . . . . . . . . . . .

D.3 Number of critical points in a two-action blind controller

. . . . . . . . . . . . . .

D.4 Examples with multiple smooth and non-smooth critical points . . . . . . . . . . .

D.5 (Super)level sets of (PO)MDPs . . . . . . . . . . . . . . . . . . . . . . . . . . . .

E Possible Extensions

E.1 Application to ﬁnite memory policies

. . . . . . . . . . . . . . . . . . . . . . . .

E.2 Polynomial POMDPs .

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

F Examples

F.1 Toy example of Figure 1 .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

F.2 Navigation in a grid world . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

F.3 A three dimensional example . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

15

15

17

17

17

19

20

22

23

26

30

31

32

34

36

37

38

38

38

39

39

42

44

A DETAILS ON THE PRELIMINARIES

We elaborate the proofs that where ommited or only sketched in the main body.

A.1 PARTIALLY OBSERVABLE MARKOV DECISION PROCESSES

The statement of Proposition 1 can found in the work by Howard (1960) and we quickly sketch
In order to show that the expected state-action frequencies exist without any
the proof therein.

15

Published as a conference paper at ICLR 2022

assumptions, we recall that for a (row or column) stochastic matrix P , the Ces`aro mean is deﬁned
by

P ∗ := lim
T →∞

1
T

T −1
(cid:88)

t=0

P t

and exists without any assumptions. Further, P ∗ is the projection onto the subspace of stationary
distribution (Doob, 1953). For γ ∈ (0, 1), the matrix

P ∗

γ := (1 − γ)

∞
(cid:88)

t=0

γtP t = (1 − γ)(I − γP )−1

is known as the Abel mean of P , where we used the Neumann series. By the Tauberian theorem, it
holds that P ∗

γ → P ∗ for γ (cid:37) 1 (Gillette, 1958; Hunter, 1983).

Proposition 1 (Existence of state-action frequencies and rewards). Let (S, O, A, α, β, r) be a
POMDP, γ ∈ (0, 1] and µ ∈ ∆S . Then ηπ,µ
, ρπ,µ
A and µ ∈ ∆S
γ
and are continuous in γ ∈ (0, 1] for ﬁxed π and µ.

γ (π) exist for every π ∈ ∆O

and Rµ

γ

Proof. The existence of the state-action frequencies as well as the continuity with respect to the
discount parameter follows directly from the general theory since

γ = (P T
ηπ,µ

π )∗

γ(µ ∗ (π ◦ β))

for γ ∈ (0, 1) and ηπ,µ
for the state frequencies and for the reward.

1 = (P T

π )∗(µ ∗ (π ◦ β)). With an analogue argument, the statement follows

For γ = 1 we work under the following standard assumption in the (PO)MDP literature5.

Assumption 2 (Uniqueness of stationary disitributions). If γ = 1, we assume that for any policy
π ∈ ∆O

A there exists a unique stationary distribution η ∈ ∆S×A of Pπ.

The following proposition shows in particular that for any initial distribution µ, the inﬁnite time
horizon state-action frequency ηπ,µ
Proposition 3 (State-action frequencies are discounted stationary). Let (S, O, A, α, β, r) be a
POMDP, γ ∈ (0, 1] and µ ∈ ∆S . Then ηπ,µ
is the unique element in ∆S×A satisfying the dis-
γ
γ + (1 − γ)(µ ∗ (π ◦ β)). Further, ρπ,µ
π ηπ,µ
counted stationarity equation ηπ,µ
is the unique
element in ∆S satisfying ρπ,µ

is the unique stationary distribution of Pπ.

γ = γP T
π ρπ,µ

γ + (1 − γ)µ.

γ = γpT

γ

1

Proof. By the general theory of Ces`aro means, (P T
tions and hence the ηπ,µ
1 = (P T
unique stationary distribution. For γ ∈ (0, 1) we have

π )∗(µ ∗ (π ◦ β)) is stationary. Hence, by Assumption 2, ηπ,µ

π )∗ projects onto the space of stationary distribu-
is the

1

γ = (P T
ηπ,µ

π )∗

γ(µ ∗ (π ◦ β)) = (I − γP T

π )−1(µ ∗ (π ◦ β)),

which yields the claim. For the state distributions ρπ,µ
isation.

γ

the claim follows analogously or by marginal-

Since the state-action frequencies satisfy this generalized stationarity condition, we sometimes refer
to them as discounted stationary distributions.

5Assumption 2 is weaker than ergodicity and is satisﬁed whenever the Markov chain with transition kernel
P π is irreducible and aperiodic for every policy π, e.g., when the transition kernel satisﬁes α > 0. For
γ ∈ (0, 1) the assumption is not required, since the discounted stationary distributions are always unique since
I − γPπ is invertible because the spectral norm of Pπ is one.

16

Published as a conference paper at ICLR 2022

A.2 SEMIALGEBRAIC SETS AND THEIR FACE LATTICES

We recall the deﬁnition of semialgebraic sets, which are fundamental objects in real algebraic ge-
ometry (Bochnak et al., 2013). A basic (closed) semialgebraic set A is a subset of Rd deﬁned by
ﬁnitely many polynomial inequalities as

A = {x ∈ Rd | pi(x) ≥ 0 for i ∈ I},

where I is a ﬁnite index set and the pi are polynomials. A semialgebraic set is a ﬁnite union
of basic (not necessarily closed) semialgebraic sets, and a function is called semialgebraic if its
graph is semialgebraic. By the Tarski-Seidenberg theorem the range of a semialgebraic function
is semialgebraic. A simple algebraic set has a lattice associated to it, induced by the set of active
inequalities. More precisely, for a subset J ⊆ I we set FJ := {x ∈ A | pj(x) = 0 for j ∈ J} and
endow the set F := {FJ | J ⊆ I} with the partial order of inclusion. We call the elements of F
the faces of A. The faces described above are a generalization of the faces of a classical polytope
and a special case of the faces of an abstract polytope and we refer to McMullen & Schulte (2002)
for more details. Next, we want to endow this partially ordered set with more structure. A lattice F
carries two operations, the join ∧ and the meet ∨, which satisfy the absortion laws F ∨ (F ∧ G) = F
and F ∧ (F ∨ G) = F for all F, G ∈ F. In the face lattice of a basic semialgebraic set, the join and
meet are given by

F ∧ G := F ∩ G and F ∨ G :=

(cid:92)

H.

H∈F : F,G⊆H

A morphism between two lattices F and G is a mapping ϕ : F → G that respects the join and the
meet, i.e., such that ϕ(F ∧ G) = ϕ(F ) ∧ ϕ(G) and ϕ(F ∨ G) = ϕ(F ) ∨ ϕ(G) for all F, G ∈ F.
A lattice isomorphism is a bijective lattice morphism where the inverse is also a morphism. We say
that two basic semialgebraic sets with isomorphic face lattice are combinatorially equivalent.

B DETAILS ON THE PARAMETRIZATON OF DISCOUNTED STATE-ACTION

FREQUENCIES

B.1 THE DEGREE OF DETERMINANTAL POLYNOMIALS

Determinantal representation of polynomials play an important role in convex geometry (see for
example Helton & Vinnikov, 2007; Netzer & Thom, 2012), but often the emphasis is put on sym-
metric matrices. We adapt those arguments to the general case and present them here. We call p a
determinantal polynomial if it admits a representation

(cid:32)

p(x) = det

A0 +

m
(cid:88)

i=1

(cid:33)

xiAi

for all x ∈ Rm,

(6)

for some A0, . . . , Am ∈ Rn×n. Let us use the notations

A(x) := A0 +

m
(cid:88)

i=1

xiAi

and B(x) :=

m
(cid:88)

i=1

xiAi.

Proposition 22 (Degree of monic univariate determinantal polynomials). Let A, B ∈ Rn×n and A
be invertible and let λ1, . . . , λn ∈ C denote the eigenvalues of A−1B if repeated according to their
algebraic multiplicity. Then,

p : R → R,

t (cid:55)→ det(A + tB)

is a polynomial of degree

deg(p) = (cid:12)
(cid:8)j ∈ {1, . . . , n} | λj (cid:54)= 0(cid:9)(cid:12)
(cid:12)

(cid:12) ≤ rank(B).

The roots of p are given by {−λ−1
deg(p) = rank(B).

j

| j ∈ J} ⊆ C. If further A−1B is symmetric, then we have

17

Published as a conference paper at ICLR 2022

Proof. Let J ⊆ {1, . . . , n} denote the set of indices j such that λj (cid:54)= 0. For x (cid:54)= 0 we have6

p(t) = det(A) det(I + tA−1B) = xn det(A) det(A−1B + t−1I) = xn det(A)χA−1B(−t−1)

n
(cid:89)

= tn

(−t−1 − λi) = (−1)n−|J| ·

i=1

(−λj) ·

(cid:89)

j∈J

(cid:89)

j∈J

(cid:0)t + λ−1

j

(cid:1) ,

which is a polynomial of degree |J|. Note that |J| is upper bounded by the complex rank of A−1B.
Since the rank over C and R agree for a real matrix, we have deg(p) ≤ rank(A−1B) = rank(B).
Assume now that A−1B is symmetric, then the rank of A−1B coincides with the number |J| of non
zero eigenvalues. Further, the rank of B and A−1B is the same.

Remark 23. Note that the degree of p can be lower than rank(B), for example if

A = I

and B =

(cid:19)

(cid:18)1 −1
1 −1

(cid:19)

(cid:18)1
1

=

(1 −1) .

Then we have rank(B) = 1, but

p(λ) = det

(cid:18)1 + λ −λ
1 − λ

λ

(cid:19)

= (1 + λ)(1 − λ) + λ2 = 1

and therefore deg(p) = 0. Note that in this case A−1B = B has no non-zero eigenvalues.

Now we show that the degree of p is still bounded by rank(B) even if A is not invertible. However,
we loose an explicit description of the degree in this case.
Proposition 24 (Degree of univariate determinantal polynomials). Let A, B ∈ Rn×n and consider
the polynomial

p : R → R,

t (cid:55)→ det(A + tB).

Then either p = 0 or if p(t0) (cid:54)= 0 and λ1, . . . , λn ∈ C denote the eigenvalues of (A + t0B)−1B
repeated according to their algebraic multiplicity, then p has degree

(cid:8)j ∈ {1, . . . , n} | λj (cid:54)= 0(cid:9)(cid:12)
deg(p) = (cid:12)
(cid:12)

(cid:12) ≤ rank(B).

In particular, it always holds that deg(p) ≤ rank(B).

Proof. Let without loss of generality p(t0) (cid:54)= 0, then C = A + t0B is invertible. By Proposition 22,
the degree of q(t) = det(C + tB) is precisely |{j | λj (cid:54)= 0}|. Noting that p(t) = q(t − t0) yields
the claim.

The following result generalizes Proposition 24 to multivariate determinantal polynomials.
Proposition 25 (Degree of determinantal polynomials). Let p : Rm → R be a determinantal poly-
nomial with the representation (6). Then

deg(p) ≤ max (cid:8) rank(B(x)) | x ∈ Rm(cid:9).

Proof. Let us ﬁx x ∈ Rm and for t ∈ R set f (t) := p(tx) = det(A0 + tB(x)). By the next
proposition it sufﬁces to show that deg(f ) ≤ rank(B(x)). However, this is precisely the statement
of Proposition 24.

Proposition 26 (Degree of polynomials). Let p : Rn → R be a polynomial. Then there is a direction
x ∈ Rn such that t (cid:55)→ p(tx) is a polynomial of degree deg(p). Moreover, for any x ∈ Rn, the
univariate polynomial t (cid:55)→ p(tx) has degree at most deg(p).

Proof. Let without loss of generality p be non trivial. Decompose p into its leading and lower order
terms p = p1 + p2 and choose x ∈ Rn such that p1(x) (cid:54)= 0. Let k := deg(p), then we have
p1(tx) = tkp1(x) for all µ ∈ R. Since the degree of t (cid:55)→ p2(tx) is at most k − 1, the degree of
t (cid:55)→ p(tx) = p1(tx) + p2(tx) is k.

6Here, χC (λ) = det(C − λI) denotes the characteriztic polynomial of a matrix C.

18

Published as a conference paper at ICLR 2022

Remark 27. Analogue to the univariate case, it is possible to give a precise statement on the degree,
which is the following. If p is not vanishing and x0 ∈ Rm is such that p(x0) (cid:54)= 0, then A(x0) ∈
Rn×n is invertible. Writing λ1(x), . . . , λn(x) ∈ C for the eigenvalues of A(x0)−1B(x) and J(x)
for the indices j such that λj(x) (cid:54)= 0 we obtain

deg(p) = max (cid:8)|J(x)| | x ∈ Rm(cid:9).

B.2 THE DEGREE OF POMDPS

The general bounds on the degree of determinantal polynomials directly implies Theorem 4, which
we state again for the sake of convenience here.
Theorem 4 (Degree of POMDPs). Let (S, O, A, α, β, r) be a POMDP, µ ∈ ∆S be an initial dis-
tribution and γ ∈ (0, 1) a discount factor. The state-action frequencies ηπ,µ
, the value
γ
function V π
γ (π) are rational functions with common de-
nominator in the entries of the policy π. Further, if they are restricted to the subset Π ⊆ ∆O
A of
policies which agree with a ﬁxed policy π0 on all states outside of O ⊆ O, they have degree at most
(cid:8)s ∈ S | β(o|s) > 0 for some o ∈ O(cid:9)(cid:12)
(cid:12) .

γ and the expected cumulative reward Rµ

and ρπ,µ

(cid:12)
(cid:12)

γ

In fact, the results from the preceding pararaph imply the following sharper versions.
Theorem 28 (Degree of discounted state-action frequencies of POMDPs). Let (S, O, A, α, β, r) be
a POMDP, µ ∈ ∆S be an initial distribution and γ ∈ (0, 1) a discount factor. Then the discounted
state-action distributions can be expressed as

ηπ,µ
γ

(s, a) =

qs,a(π)
q(π)

for every π ∈ ∆O

A and s ∈ S,

(7)

where

qs,a(π) := (π ◦ β)(a|s) · (1 − γ) · det(I − γpT

π )µ
s

and

q(π) := det(I − γpT
π )

are polynomials in the entries of the policy. Further, if qs,a and q are restricted to the subset Π ⊆
∆O
A of policies which agree with a ﬁxed policy π0 on all states outside of O ⊆ O and if we set
S := {s ∈ S | β(o|s) > 0 for some o ∈ O}, then they have degree at most
s + 1S(s) ≤ |S|
)0

rank(pT

π − pT
π0

(8)

deg(qs,a) ≤ max
π∈Π

and

deg(q) ≤ max
π∈Π

rank(pT

π − pT
π0

) ≤ |S| .

(9)

Proof. Recall that we have

(s, a) = (π ◦ β)(a|s)ρπ,µ
Further, by Proposition 3, the state distribution is given by ρπ,µ
Cramer’s rule yields

ηπ,µ
γ

(s).

γ
γ = (1 − γ)(1 − γpT

π )−1µ. Applying

ρπ,µ
γ

(s) =

π )µ
det(I − γpT
s
det(I − γpT
π )

,

s denotes the matrix obtained by replacing the s-th row of I − γpT

π with µ, which

π )µ

where (I − γpT
shows (7). For the estimates on the degree, we note that
deg(qs,a) ≤ deg(I − γpT

π )µ

s + 1S(s).

Further, we can use Proposition 25 to estimate the degree over a subset Π ⊆ ∆S
agree with a ﬁxed policy π0 on all states outside of O ⊆ O. We obtain

A of policies which

deg(I − γpT

π )µ

s ≤ max
π∈Π

rank((I − γpT
π0

s − (I − γpT
)µ

π )µ

s ) = max
π∈Π

rank γ(pT

π − pT
π0

)0
s,

π − pT
π0

which shows the ﬁrst estimate in (8). To see the second inequality, we ﬁrst assume that s ∈ S. Then
)0
(pT
s has at most |S| − 1 non zero columns and hence rank at most |S| − 1. If s /∈ S, then
with the same argument, the rank of (pT
)0
s = pT
π − pT
π0 is at most |S| and the second estimate
in (8) holds in both cases. The estimates in (9) follow with completely analoguous arguments.

π − pT
π0

19

Published as a conference paper at ICLR 2022

Remark 29. The polynomial q is independent of the initial distribution µ, whereas the polynomi-
als qs,a and therefore also their degrees depend on µ. Further, Proposition 24 contains an exact
expressions for the degree of the polynomials qs,a and q depending on the eigenvalues of certain
matrices.
Corollary 30 (Degree of the reward and value function). Theorem 28 also yields the rational degree
of the reward and value function. Indeed, it holds that7

Rµ

γ (π) =

(cid:80)

s,a r(s, a)qs,a(π)
q(π)

= (1 − γ) ·

det(I − γpπ + rπ ⊗ µ)
det(I − γpπ)

− 1 + γ,

where we used the matrix determinant lemma (Vrabel, 2016), and

V π
γ (s) =

det(I − γpπ + rπ ⊗ δs)
det(I − γpπ)

− 1 + γ.

γ (s) is bounded
γ is bounded by the maximum degree of
γ (s) over the support of µ. An explicit formula for the rational degrees can be

The degree of their denominator is bounded by (9). The degree of the numerator of V π
by (8). Finally, the degree of the numerator of the reward Rµ
the numerators of V π
deduced from Remark 27.
Corollary 31 (Degree of curves). Let (S, A, α, r) be an MDP, µ ∈ ∆S be an initial distribution,
γ ∈ (0, 1) a discount factor and r ∈ RS×A. Further, let π0, π1 ∈ ∆S
A be two policies that disagree
on k states. Let ηλ and Vλ denote the discounted state-action frequencies and the value function
belonging to the policy πλ := π0 + λ(π1 − π0). Then both λ (cid:55)→ ηλ and λ (cid:55)→ V λ are rational
functions of degree at most k.

B.3 PROPERTIES OF DEGREE-ONE RATIONAL FUNCTIONS

B.3.1 A LINE THEOREM FOR DEGREE-ONE RATIONAL FUNCTIONS

First, we notice that certain degree-one rational functions map lines to lines which implies that they
map polytopes to polytopes. Further, the extreme points of the range lie in the image of the extreme
points which implies that degree-one rational functions are maximized in extreme points – just like
linear functions.
Deﬁnition 32. We say that a function f : Ω → Rm is a rational function of degree at most k with
common denominator if it admits a representation of the form fi = pi/q for polynomials pi and q
of degree at most k.
Remark 33. We have seen that the state-action frequencies, the reward function and the value
function of POMDPs are rational functions of degree at most |S| with common denominator. In
the case of MDPs and if a policy is ﬁxed on all but k states, it is a rational function with common
denominator of degree at most k.
Proposition 34. Let Ω ⊆ Rd be convex and f : Ω → Rm be a rational function of degree at most
one with common denominator and the representation fi(x) = pi(x)/q(x) for afﬁne linear functions
pi, q. Then, f maps lines to lines. More precisely, if x0, x1 ∈ Ω, then

c : [0, 1] → [0, 1],

λ (cid:55)→

q(x1)λ
q(xλ)

=

q(x1)λ
(q(x1) − q(x0))λ + q(x0)

is strictly increasing and satisﬁes

f ((1 − λ)x0 + λx1) = (1 − c(λ))f (x0) + c(λ)f (x1) = f (x0) + c(λ)(f (x1) − f (x0)).

(10)

Further, c is strictly convex if |q(x1)| < |q(x0)|, strictly concave if |q(x1)| > |q(x0)| and linear if
|q(x0)| = |q(x1)|.

Proof. We set xλ := (1 − λ)x0 + λx1 and by explicit computation we obtain

f (xλ) =

p(xλ)
q(xλ)

=

(1 − λ)p(x0) + λp(x1)
q(xλ)

=

(1 − λ)q(x0)
q(xλ)

· f (x0) +

λq(x1)
q(xλ)

· f (x1).

7Here, u ⊗ v := uvT denotes the Kronecker product.

20

Published as a conference paper at ICLR 2022

Noting that

and

λq(x1)
q(xλ)

=

λq(x1)
(1 − λ)q(x0) + λq(x1)

= c(λ)

(1 − λ)q(x0)
q(xλ)

+

λq(x1)
q(xλ)

=

(1 − λ)q(x0) + λq(x1)
q(xλ)

= 1

yields (10). Finally, we differentiate and obtain

c(cid:48)(λ) =

q(x0)q(x1)
q(xλ)2

.

(11)

Since q has no root in Ω it follows that q(x0) and q(x1) have the same sign and hence c(cid:48)(λ) > 0.
Differentiating a second time yields

c(cid:48)(cid:48)(λ) = −2q(x0)q(x1)(q(x1) − q(x0)) · q(xλ)−3.

Using that sgn(q(xλ)) = sgn(q(x0)) = sgn(q(x1)) yields the assertion.
Remark 35. The formula (10) holds for all λ ∈ R for which xλ = λx0 + (1 − λ)x1 ∈ Ω.
Proposition 36 (Level sets of degree one rational functions). Let Ω ⊆ Rd be convex and f : Ω → R
be a rational function of degree at most one. Then, Lα := {x ∈ Ω | f (x) = α} is the intersection
of an afﬁne space with Ω.

Proof. For x, y ∈ Lα the ray {x + t(y − x) | t ∈ R} ∩ Ω is contained in Lα by the line theorem.

B.3.2 EXTREME POINTS OF DEGREE-ONE RATIONAL FUNCTIONS

It is well known that linear functions obtain their maxima on extreme points. We show that this is
also the case for rational functions of degree at most one.
Deﬁnition 37. Let Ω ⊆ Rd. Then we call x ∈ Ω an extreme point of Ω if x is not the strict convex
combination of two other points in Ω, i.e., if x = (1 − λ)x0 + λx1 for x0, x1 ∈ Ω and λ ∈ (0, 1)
implies x0 = x1 = x. We denote the set of extreme points of Ω by extr(Ω).
Proposition 38. Let Ω ⊆ Rd be convex and f : Ω → Rm be a rational function of degree at most
one with common denominator. Then f (Ω) is convex and we have extr(f (Ω)) ⊆ f (extr(Ω)).

Proof. Let y0 = f (x0), y1 = f (x1) ∈ f (Ω). Then by the line theorem, the line connecting y0 and
y1 agrees with the image of the line connecting x0 and x1 under f , in particular, it is contained in
f (Ω) which shows the convexity of f (Ω). Pick now an extreme point y = f (x) ∈ extr(f (Ω)). If
x ∈ extr(Ω), there is nothing to show, so let x /∈ extr(Ω). Then by the Carath´eodory theorem we
can write x as a strict convex combination (cid:80)n
i=1 λixi, λi > 0, n ≥ 2 for some extreme points xi ∈
extr(Ω). In particular, it is possible to write x as the strict convex combination x = (1 − λ)x0 + λx1
by setting x0 := (cid:80)n

i=2 λixi. Now, by the line theorem we have

y = (1 − c(λ))f (x0) + c(λ)f (x1),

where c(λ) ∈ (0, 1). Since y is an extreme point, this implies f (x0) = f (x1) = y. In particular,
this shows that y = f (x1) ∈ f (extr(Ω)).
Corollary 39 (Maximizers of degree-one rational functions). Let Ω ⊆ Rd be a convex and compact
set and let f : Ω → R be a rational function of degree at most one with common denominator. Then
f is maximized in at least one extreme point of Ω. In particular, if Ω is a polytope, f is maximized
in at least one vertex.

Proof. Since Ω is compact and f is continuous, f (Ω) is a compact interval, lets say f (Ω) = [α, β].
By the preceding proposition we have {α, β} = extr(f (Ω)) ⊆ f (extr(Ω)), which shows that f is
maximized in at least one extreme point.

Corollary 40. Let P ⊆ Rd be a polytope and f : Ω → Rm be a rational function of degree at most
one with common denominator. Then f (P ) is a polytope and we have vert(f (P )) ⊆ f (vert(P )).

21

Published as a conference paper at ICLR 2022

Proof. By the preceding proposition, f (P ) is convex. Further, f (P ) has ﬁnitely many extreme
points since extr(f (P )) ⊆ f (extr(P )) = f (vert(P )), which implies the assertion.
Proposition 41. Let f : P → Rm be deﬁned on the Cartesian product P = P1 × · · · × Pk of
polytopes, which is a degree-one rational function with common denominator whenever all but one
components are ﬁxed. Then f (P ) has ﬁnitely many extreme points and it holds that

extr(f (P )) ⊆ f (vert(P )) = f (vert(P1) × · · · × vert(Pk)).

In particular, if m = 1 this shows that f is maximized in at least one vertex of P .

Proof. Let now x = (x(1), . . . , x(k)) ∈ P1 × · · · × Pk be such that f (x) ∈ extr(f (P )). If x(i) ∈
vert(Pi), there is nothing to show. Hence, we assume that x(i) /∈ vert(Pi). Let us denote the
restriction of f onto Pi by g, where we keep the other components ﬁxed to be x(j). Then we have
g(x(i)) ∈ extr(g(Pi)) and hence by Proposition 38 there is ˜x(i) ∈ vert(Pi) such that g(˜x(i)) =
g(x(i)) = f (x). Replacing x(i) by ˜x(i) and iterating over i yields the claim.

Remark 42. We have seen that both the value function as well as the discounted state-action fre-
quencies are degree-one rational functions in the rows of the policy in the case of full observability.
Hence, the extreme points of the set of all value functions and of the set of discounted state-action
frequencies are described by the proposition above. In fact we will see later that the discounted
state-action frequencies form a polytope; further, one can show that the set of value functions is a
ﬁnite union of polytopes (see Dadashi et al., 2019).

B.3.3

IMPLICATIONS FOR POMDPS

Proposition 6. Let (S, A, α, r) be an MDP and γ ∈ (0, 1). Further, let π0, π1 ∈ ∆S
A be two
policies that differ on at most k states. For any λ ∈ [0, 1] let Vλ ∈ RS and ηµ
λ ∈ ∆S×A denote the
value function and state-action frequency belonging to the policy π0 + λ(π1 − π0) with respect to
the discount factor γ, the initial distribution µ and the instantaneous reward r. Then the rational
degrees of λ (cid:55)→ Vλ and λ (cid:55)→ ηλ are at most k. If they differ on at most one state ˜s ∈ S then
ηµ
λ = ηµ

Vλ = V0 + c(λ) · (V1 − V0) and

0 + c(λ) · (ηµ

for all λ ∈ [0, 1],

1 − ηµ
0 )

where

c(λ) =

det(I − γp1)λ
det(I − γpλ)

=

det(I − γp1)λ
(det(I − γp1) − det(I − γp0))λ + det(I − γp0)

= λ ·

ρµ
λ(˜s)
ρµ
1 (˜s)

.

Proof. This is a direct consequence of Proposition 34 and Theorem 4.
Remark 43. The proposition above describes the interpolation speed λ · ρµ
1 (˜s) in terms of
the discounted state distribution in ˜s. This expressions extends to the case of mean rewards – note
that the determinants vanish – and the theorem can be shown to hold in this case as well, if we set
0/0 := 0. Note that the interpolation speed does not depend on the initial condition µ.
Remark 44. Reﬁnements on the upper bound of the rational degree of λ (cid:55)→ Vλ and λ (cid:55)→ ηλ can
be obtained using Proposition 24. Indeed, if we write ηλ(s, a) = qsa(λ)/q(λ) like in Theorem 28
those degrees can be upper bounded by
deg(qsa) ≤ rank(p1 − p0)0

s + 1S(s) ≤ rank(p1 − p0)

deg(q) ≤ rank(p1 − p0),

λ(˜s)/ρµ

and

where S ⊆ S is the set of states on which the two policies differ; see also the proof of Theorem 28
for more details on an analogue argument. Hence, the degree of the two curves λ (cid:55)→ Vλ and λ (cid:55)→ ηλ
is upper bounded by rank(p1 − p0).

C DETAILS ON THE GEOMETRY OF STATE-ACTION FREQUENCIES

The set of all state-action frequencies is known to be a polytope in the fully observable case (Derman,
1970) and we show that it is combinatorially equivalent to the conditional probability polytope ∆S
A.
We show that in the partially observable case the set of feasible state-action frequencies is cut out
from this polytope by a ﬁnite set of polynomial inequalities. We discuss the special structure of
those polynomials and give closed form expressions for them.

22

(12)

(13)

Published as a conference paper at ICLR 2022

C.1 THE FULLY OBSERVABLE CASE

Let νπ,µ

γ ∈ ∆S×S denote the expected number of transitions from s to s(cid:48) given by

(1 − γ)

∞
(cid:88)

t=0

γtPπ,µ(st = s, st+1 = s(cid:48))

and lim
T →∞

1
T

T −1
(cid:88)

t=0

Pπ,µ(st = s, st+1 = s(cid:48))

respectively. Note that we have

νπ,µ
γ

(s, s(cid:48)) =

(cid:88)

a∈A

ηπ,µ
γ

(s, a)α(s(cid:48)|s, a),

hence νπ,µ

γ

is the image of ηπ,µ

γ

under the linear transformation

fα : ∆S×A → ∆S×S ,

η (cid:55)→

(cid:32)

(cid:88)

a∈A

(cid:33)

η(s, a)α(s(cid:48)|s, a)

.

Therefore, we can hope to obtain a characterization of N µ
would like to understand the structural properties of νπ,µ
marginals since we can compute

γ

s,s(cid:48)∈S
γ using this mapping. In order to do so, we
. For γ = 1 those distributions have equal

(cid:88)

s(cid:48)∈S

νπ,µ
1

(s, s(cid:48)) −

(cid:88)

s(cid:48)∈S

νπ,µ
1

(s(cid:48), s) = lim
T →∞

1
T

In the discounted case, we compute similarly

(cid:0)Pπ,µ(s0 = s) − Pπ,µ(sT +1 = s)(cid:1) = 0.

(14)

(cid:88)

s(cid:48)∈S

νπ,µ
γ

(s, s(cid:48)) − γ

(cid:88)

s(cid:48)∈S

νπ,µ
γ

(s(cid:48), s) = (1 − γ)

(cid:32) ∞
(cid:88)

γtPπ,µ(st = s) −

t=0
= (1 − γ)µ(s).

γt+1Pπ,µ(st+1 = s)

(cid:33)

∞
(cid:88)

t=0

If we perceive νπ,µ

γ ∈ ∆S×S as a matrix, we have shown that

(νπ,µ
γ

)T 1S = γ(νπ,µ

γ

)1S + (1 − γ)µ,

which motivates the following deﬁnition.

We will see that the set of state-action frequencies is the pre-image of the following polytope under
a linear map.
Deﬁnition 45 (Discounted Kirchhoff polytopes). For a distribution µ ∈ ∆S and γ ∈ (0, 1] we
deﬁne the discounted Kirchhoff polytope (this is a generalization of a deﬁnition by Weis 2010)

γ := (cid:8)ν ∈ ∆S×S ⊆ RS×S | νT 1S = γν1S + (1 − γ)µ(cid:9),
Ξµ

where 1S ∈ RS is the all one vector.

So far, we have observed that fα(ηπ,µ

) ∈ Ξµ

γ
γ : ∆S
Ψµ

γ , i.e., that
A → ∆S×A,

π (cid:55)→ ηπ,µ

γ

α (Ξµ

γ ). In order to see that this mapping is surjective on f −1

maps to f −1
inverse is given through conditioning. The following proposition uses the ergodicity assumption.
Proposition 46. Let γ ∈ (0, 1] and η ∈ f −1
Set

γ ) and let ρ ∈ ∆S denote the state marginal of η.

γ ) we show that its right

α (Ξµ

α (Ξµ
(cid:26) η(·|s) = η(s, ·)/ρ(s)

π(·|s) :=

if ρ(s) > 0
arbitrary element in ∆A if ρ(s) = 0,

then we have ηπ,µ

γ = η.

Proof. We calculate

γ(P π)T η(s, a) = γ

(cid:88)

s(cid:48),a(cid:48)

α(s|s(cid:48), a(cid:48))π(a|s)η(s(cid:48), a(cid:48)) = γπ(a|s)

α(s|s(cid:48), a(cid:48))η(s(cid:48), a(cid:48))

(cid:88)

s(cid:48),a(cid:48)

= γπ(a|s)

(cid:88)

s(cid:48)

ν(s(cid:48), s) = π(a|s)

(cid:32)

(cid:88)

s(cid:48)

ν(s, s(cid:48)) − (1 − γ)µ(s)

(cid:33)

= π(a|s)ρ(s) − (1 − γ)π(a|s)µ(s) = η(s, a) − (1 − γ)(µ ∗ π)(s, a).

The unique characterization from Proposition 3 of ηπ,µ

γ

yields the assertion.

23

Published as a conference paper at ICLR 2022

The proposition states that we can reconstruct the policy from the state-action frequencies by con-
ditioning and is well known in the context of the dual linear programming formulation of MDPs
(Kallenberg, 1994). Hence, it will be convenient later to work under the following assumption in
which ensures that policies in ∆S
Assumption 13 (Positivity). Let ρπ,µ

A are one-to-one with state-action frequencies.

γ > 0 hold entrywise for all policies π ∈ ∆S
A.

Note that this positivity assumption holds in particular, if either α > 0 and γ > 0 or γ < 1 and
µ > 0 or entrywise. Indeed, if α > 0, then the transition kernel pπ is strictly positive for any policy
since

pπ(s(cid:48)|s) =

(cid:88)

(π ◦ β)(a|s)α(s(cid:48)|s, a) > 0,

since (π ◦ β)(a|s) > 0 for some a ∈ A. Since ρπ,µ
(Proposition 3), it holds that

γ

a

is discounted stationary with respect to pπ

ρπ,µ
γ

(s) = γ

(cid:88)

s(cid:48)

ρπ,µ
γ

(s(cid:48))pπ(s|s(cid:48)) + (1 − γ)µ(s) > 0

γ

since ρπ,µ
(s(cid:48)) > 0 for some s(cid:48) ∈ S. If µ > 0 and γ < 1, then ρπ,µ
consequence of Proposition 46, we obtain the following characterization of N µ
γ .
Proposition 8 (Characterization of N µ
bution and γ ∈ (0, 1]. It holds that
N µ

γ = ∆S×A ∩ (cid:8)η ∈ RS×A | (cid:104)ws

γ, η(cid:105)S×A = (1 − γ)µs for s ∈ S(cid:9)

γ

γ ). Let (S, A, α, r) be an MDP, µ ∈ ∆S be an initial distri-

(s) ≥ (1 − γ)µ(s) > 0. As a

(2)

where ws

γ := δs ⊗ 1A − γα(s|·, ·). For γ ∈ (0, 1), ∆S×A can be replaced by [0, ∞)S×A in (2).

Instead of proving this proposition directly, we ﬁrst present the following version of it.
γ = f −1
Proposition 47. Let (A, S, α, r) be an MDP and γ ∈ (0, 1]. It holds that N µ

α (Ξµ

γ ).

Proof. By (13) and (14), it holds that fα(N µ
Proposition46, for every η ∈ fα(Ξµ
holds that f −1

γ ) ⊆ Ξµ
γ ) there is a policy π ∈ ∆S

γ ) ⊆ f −1

α (Ξµ

α (Ξµ

γ ).

γ and thus N µ

γ ⊆ f −1
A such that ηπ,µ

α (Ξµ
γ ). However, by
γ = η and hence it

Proof of Proposition 8. By the preceding proposition η ∈ N µ
ν := fα(η) ∈ Ξµ

γ . Using the deﬁnition of Ξµ
(cid:88)
ν(s, s(cid:48)) = γ

γ this equivalent to
(cid:88)

ν(s(cid:48), s) + (1 − γ)µ(s)

γ is equivalent to η ∈ ∆S×A and

s(cid:48)

s(cid:48)

for all s ∈ S. Plugging in the deﬁnition of fα we see that the term on the left hand side is equivalent
to

(cid:88)

(cid:88)

η(s, a)α(s(cid:48)|s, a) =

η(s, a) = (cid:104)δs ⊗ 1A, η(cid:105)S×A.

(cid:88)

The ﬁrst term of the right hand side is precisely

s(cid:48)

a

a

(cid:88)

(cid:88)

γ

s(cid:48)

a

η(s(cid:48), a)α(s|s(cid:48), a) = (cid:104)γα(s|·, ·), η(cid:105)S×A.

Hence, we have seen that fα(η) ∈ Ξµ

γ is equivalent to the condition

Note that

(cid:104)ws

γ, η(cid:105)S×A = (1 − γ)µ(s)

for all s ∈ S.

(15)

{η | (cid:104)ws

γ, η(cid:105)S×A = (1 − γ)µ(s) for all s ∈ S} = η0 + {ws

γ | s ∈ S}⊥

for an arbitrary element η0 satisfying (15). This shows the ﬁrst equation in (2). The second equation
follows from the observation that (cid:80)

γ = (1 − γ)1S . Hence, for γ < 1 it holds that

s ws

(cid:0)η0 + {ws

γ | s ∈ S}⊥(cid:1) ∩ ∆S×A =

(cid:16)

η0 + (cid:0){ws
= (cid:0)η0 + {ws

γ | s ∈ S} ∪ {1S
γ | s ∈ S}⊥(cid:1) ∩ [0, ∞)S×A.

(cid:1)⊥(cid:17)

∩ [0, ∞)S×A

24

Published as a conference paper at ICLR 2022

C.1.1 DERIVATIVE OF THE DISCOUNTED STATE-ACTION FREQUENCIES

In this section we discuss the Jacobian of the parametrization π (cid:55)→ ηπ of the discounted state-action
frequencies. One motivation for this is that this Jacobian plays an important role in the relation
of critical points in the policy space and the space of discounted state-action frequencies. Note that
π )−1(µ∗π) is well deﬁned, whenever (cid:107)Pπ(cid:107)2 < γ−1. Hence, we can extend
Ψµ
γ (π) = (1−γ)(1−γP T
γ onto the neighborhood (cid:8)π ∈ RS×A | (cid:107)Pπ(cid:107)2 < γ−1(cid:9) of ∆S
Ψµ
A, which enables us to compute the
Jacobian of Ψµ
γ .
Proposition 48 (Jacobian of Ψµ

A and s ∈ S, a ∈ A it holds that

γ ). For any policy π ∈ ∆S

∂(s,a)Ψµ

γ (π) = (I − γP T

π )−1(ρπ,µ

γ

∗ ∂(s,a)π) = ρπ,µ

γ

(s)(I − γP T

π )−1(δs ⊗ δa),

(16)

where

(ρπ,µ
γ

∗ ∂(s,a)π)(s(cid:48), a(cid:48)) = ρπ,µ

(s(cid:48))∂(s,a)π(a(cid:48)|s(cid:48)) = ρπ,µ
γ (π) is identical to the (s, a)-th column of (I − γP T

(s)(δs ⊗ δa)(s(cid:48), a(cid:48)).
π )−1 up to the scaling factor of

γ

γ

(s). In particular, if ρπ,µ

(s) > 0 for all s ∈ S, the Jacobian DΨµ

γ

γ has full rank.

Hence, ∂(s,a)Ψµ
ρπ,µ
γ

Proof. Recall that for invertible matrices A(t), it holds that ∂tA(t)−1 = −A(t)−1(∂tA(t))A(t)−1.
We compute

(1 − γ)−1∂(s,a)Ψµ

γ (π) = ∂(s,a)(I − γP T
= (∂(s,a)(I − γP T
= −(1 − γ)−1(I − γP T

π )−1(µ ∗ π)
π )−1)(µ ∗ π) + (I − γP T
π )−1∂(s,a)(I − γP T

π )−1∂(s,a)(µ ∗ π)
π )ηπ,µ
γ

+ (I − γP T

π )−1(µ ∗ ∂(s,a)π)

= (I − γP T

π )−1 (cid:0)(1 − γ)−1γ(∂(s,a)P T

π )ηπ,µ

γ + µ ∗ ∂(s,a)π(cid:1) .

Further, direct computation shows

((∂(s,a)P T

π )ηπ,µ
γ

)(s, a) = ∂(s,a)π(a|s)

(cid:88)

s(cid:48),a(cid:48)

α(s|s(cid:48), a(cid:48))π(a(cid:48)|s(cid:48))ρπ,µ

γ

(s(cid:48))

Using the fact that ρπ,µ

γ

π ρπ,µ
γ
is the discounted stationary distribution, yields

∗ ∂(s,a)π)(s, a).

= (pT

(1 − γ)−1γ(∂(s,a)P T

π )ηπ,µ

γ + µ ∗ ∂(s,a)π = ((1 − γ)−1γpT
= (1 − γ)−1ρπ,µ

π ρπ,µ
∗ ∂(s,a)π,

γ

γ + µ) ∗ ∂(s,a)π

which shows (16). Note that (I − γP T
matrix (I − γP T
∂(s,a)Ψµ

π )−1(δs ⊗ δa) is precisely the (s0, a0)-th column of the
π )−1. Those columns are linearly independent, and so are the partial derivatives

γ (π), given that the discounted stationary distribution ρπ,µ

vanishes nowhere.

γ

Corollary 49 (Dimension of N µ
Then we have

γ ). Assume that ρπ,µ

γ > 0 entrywise for some policy π ∈ int(∆S

A).

dim(N µ

γ ) = dim(∆S

A) = |S|(|A| − 1).

Proof. By Proposition 48 the mapping Ψµ

γ is full rank in a neighborhood of π and hence, we have

dim(N µ

γ ) = dim(Ψµ

γ (∆S

A)) = dim(∆S

A).

Let us consider a parametrized policy model ΠΘ = {πθ | θ ∈ Θ} with differentiable parametrization
θ (cid:55)→ πθ.
Proposition 50 (Parameter derivatives of discounted state-action frequencies). It holds that

∂θiηπθ,µ

γ = (I − γP T
πθ

)−1(ρπθ,µ

γ

∗ ∂θiπθ), where (ρπθ,µ

γ

∗ ∂θiπθ)(s, a) = ρπθ,µ

γ

(s)∂θiπθ(a|s).

25

Published as a conference paper at ICLR 2022

Proof. This follows directly from the application of the chain rule and (16).

γ (πθ) and recover the well known policy gradient theorem, see Sutton et al. (1999).

Using this expression, we can compute the parameter gradient with respect to the discounted reward
F (θ) := Rµ
Deﬁnition 51 (state-action value function). We call Qπ := (I − γPπ)−1r ∈ RS×A the state-action
value function or the Q-value function of the policy π.
Corollary 52 (Policy gradient theorem). It holds that

∂θi F (θ) =

(cid:88)

s

ρπθ,µ
γ

(s)

(cid:88)

a

∂θiπθ(a|s)Qπθ (s, a) =

(cid:88)

s,a

ηπθ,µ
γ

(s, a)∂θi log(πθ(a|s))Qπθ (s, a).

Proof. Using the preceding proposition, we compute

∂θiF (θ) = (cid:104)ρπθ,µ

γ

∗ ∂θiπθ, Qπθ (cid:105)S×A =

(cid:88)

s

ρπθ,µ
γ

(s)

(cid:88)

a

∂θiπθ(a|s)Qπθ (s, a)

=

(cid:88)

s,a

ηπθ,µ
γ

(s, a)∂θi log(πθ(a|s))Qπθ (s, a).

Remark 53 (POMDPs as parametrized policy models). The case of partial observability can some-
times be regarded as a special case of parametrized policies. In fact the observation mechanism β
induces a linear map π (cid:55)→ π ◦ β. This interpretation together with the preceding proposition can be
used to calculate policy gradients in partially observable systems.

C.1.2 THE FACE LATTICE IN THE FULLY OBSERVABLE CASE

So far, we have seen that the set of state-action frequencies form a polytope in the fully observable
case. However, not all polytopes are equally complex and thus we aim to describe the face lattice of
N µ

γ , which describes the combinatorial properties of a polytope, see Ziegler (2012).

Theorem 54 (Combinatorial equivalence of N µ
(0, 1]. Then π (cid:55)→ ηπ,µ
∆S

γ , such that for every I ⊆ S × A it holds that

A and N µ

γ and ∆S

A). Let (A, S, α, r) be an MDP and γ ∈
induces an order preserving surjective morphism between the face lattices of

γ

(cid:8)π ∈ ∆S

A | π(a|s) = 0 for all (s, a) ∈ I(cid:9) (cid:55)→ (cid:8)η ∈ N µ

γ | η(s, a) = 0 for all (s, a) ∈ I(cid:9) .

If additionally Assumption 13 holds, this is an isomorphism and preserves the dimension of the faces.

A and N µ
Proof. First, we note that the faces of both ∆S
γ have the structure of the left and right hand
side of (54) respectively, which follows from (2). Denote now the left and right hand side in (54) by
F and G respectively, then we need to show that Ψµ

γ (F ) = G. For π ∈ F it holds that

ηπ,µ
γ

(s, a) = ρπ,µ

γ

(s)π(a|s) = 0 for all (s, a) ∈ I

and hence ηπ,µ
γ ∈ G. On the other hand for η ∈ G we can set π(·|s) := η(·|s) whenever deﬁned and
any other element such that π(a|s) = 0 for all (s, a) ∈ I otherwise. Then we surely have π ∈ F
and by Proposition 46 also ηπ,µ
γ > 0 holds entrywise for all policies
π ∈ int(∆S
γ , which shows that the mapping
deﬁned in (54) is injective. The assertion on the dimension follows from basic dimension counting,
from the fact that the rank is preserved by a lattice isomorphism or by virtue of Proposition 48.

A), the mapping η (cid:55)→ η(·|·) deﬁnes an inverse to Ψµ

γ = η. In the case that ρπ,µ

C.2 THE PARTIALLY OBSERVABLE CASE

In Corollary 5, we have seen that the discounted state-action frequencies form a semialgebraic set.
Now we aim to describe its deﬁning polynomial inequalities. In Section 5 we will discuss how the
degree of these polynomials allows us to upper bound the number of critical points of the optimiza-
tion problem.

26

Published as a conference paper at ICLR 2022

Deﬁnition 9 (Effective policy polytope). We call the set of effective policies τ = π ◦ β ∈ ∆S
effective policy polytope and denote it by ∆S,β
A .

A the

Note that ∆S,β
mapping π (cid:55)→ π ◦ β. Hence, we can write it as an intersection

A is indeed a polytope since it is the image of the polytope ∆O

A under the linear

∆S,β

A = ∆S

A ∩ U ∩ C,

(17)

where U, C ⊆ RS×A are an afﬁne subspace and a polyhedral cone and describe a ﬁnite set of linear
equalities and a ﬁnite set of linear inequalities respectively. In the following we will compute those
sets explicitely under mild conditions and see that they do not carry an afﬁne part.

C.2.1 DEFINING LINEAR INEQUALITIES OF THE EFFECTIVE POLICY POLYTOPE

Obtaining inequality descriptions of the images of polytopes under linear maps is a fundamental
problem that is non-trivial in general. It can be approached algorithmically, e.g., by Fourier-Motzkin
elimination, block elimination, vertex approaches, and equality set projection (Jones et al., 2004).
We discuss the special case where the linear map is injective, corresponding to the case where the
associated matrix B has linearly independent columns. As a polytope is a ﬁnite intersection of
closed half spaces H + = {x | nT x ≥ α}, it sufﬁces to characterize the image BH +. It holds that

BH + = {y ∈ range B | nT B+y ≥ α} = {y | ((B+)T n)T y ≥ α} ∩ ker(BT )⊥,

(18)

A corresponds to an inequality (cid:104)τ, (β+)T n(cid:105) ≥ 0 in the polytope ∆S
A.

where B+ is a pseudoinverse and where we have used that B+y consists of at most one element by
the injectivity of B. Let us now come back to the mapping π (cid:55)→ π ◦ β = βπ. By the “vec-trick”,
this map corresponds to vec(βπI) = (I T ⊗ β) vec(π). Hence the linear map is represented by the
matrix B = I ⊗ β. We observe that (I ⊗ β)+ = I ⊗ β+ (see Langville & Stewart, 2004, Section
2.6.3). Notice that B = I ⊗ β has linearly independent columns if and only if β does. By the
above discussion, if β has linearly independent columns, then an inequality (cid:104)π, n(cid:105) ≥ 0 in the policy
polytope ∆O
Assumption 10. The matrix β ∈ ∆S
Remark 11. The assumption above does not imply that the system is fully observable. Recall
that if β has linearly independent columns, the Moore-Penrose takes the form β+ = (βT β)−1βT .
An interesting special case is when β is deterministic but may map several states to the same
observation (this is the partially observed setting considered in numerous works).
In this case,
β+ = diag(n−1
|O|)βT , where no denotes the number of states with observation o. In this
case, β+
so agrees with the conditional distribution β(s|o) with respect to a uniform prior over the
states; however, this is not in general the case since β+ can have negative entries.
Theorem 12 (H-description of the effective policy polytope). Let (S, O, A, α, β, r) be a POMDP
and let Assumption 10 hold. Then it holds that

O ⊆ RS×O has linearly independent columns.

1 , . . . , n−1

∆S,β

A = ∆S

A ∩ U ∩ C = U ∩ C ∩ D,

(3)

where U = {π ◦ β | π ∈ RS×O} = ker(βT )⊥ is a subspace, C = {τ ∈ RS×A | β+τ ≥ 0}
is a pointed polyhedral cone and D = {τ ∈ RS×A | (cid:80)
a(β+τ )oa = 1 for all o ∈ O} an afﬁne
subspace. Further, the face lattices of ∆O

A are isomorphic.

A and ∆S,β

Proof. First, we recall the deﬁning linear (in)equalities of the policy polytope ∆O
by

A, which are given

π(a|o) = (cid:104)δo ⊗ δa, π(cid:105)O×A ≥ 0
π(a|o) = (cid:104)δo ⊗ 1A, π(cid:105)O×A = 1 for all o ∈ O.

for all a ∈ A, o ∈ O and

(cid:88)

a

Hence, by the general discussion from above, namely by (18), it holds that

∆S,β

A = ker(βT )⊥ ∩ {τ | β+τ ≥ 0} ∩

(cid:110)

τ |

(cid:88)

(β+τ )oa = 1 for all o ∈ O

(cid:111)
.

a

27

Published as a conference paper at ICLR 2022

Note that the linear inequalities (cid:80)
β+1S = 1O by the injectivity of β and β1O = 1S . Now we can check that
(cid:88)

a(β+τ )oa = 1 are redundant in ∆S

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(β+τ )oa =

β+
osτsa =

β+
os

τsa =

β+
os = 1.

A. To see this, we note that

a

a

s

s

a

s

This together with β(∆O

A) ⊆ ∆S

A shows that

∆S,β

A = ∆S

A ∩ ker(βT )⊥ ∩ {τ | β+τ ≥ 0}.

The reformulation of the sets C and D for deterministic observation mechanisms β follows from the
preceding remark.

C.2.2 DEFINING POLYNOMIAL INEQUALITIES OF THE FEASIBLE STATE-ACTION

FREQUENCIES

A into polynomial inequalities in N µ

Using that the inverse of π (cid:55)→ ηπ is given through conditioning (see Proposition 46), we can translate
linear inequalities in ∆S
γ . More precisely, we have the following
result, which can easily be extended to more general inequalities.
Proposition 14 (Correspondence of inequalities). Let (S, A, α, r) be an MDP, τ ∈ ∆S
A and let
η ∈ ∆S×A denote its corresponding discounted state-action frequency for some µ ∈ ∆S and
γ ∈ (0, 1]. Let c ∈ R, b ∈ RS×A and set S := {s ∈ S | bsa (cid:54)= 0 for some a ∈ A}. Then

(cid:88)

s,a

bsaτsa ≥ c

implies

(cid:88)

(cid:88)

bsaηsa

(cid:89)

(cid:88)

ηs(cid:48)a(cid:48) − c

(cid:89)

(cid:88)

ηs(cid:48)a(cid:48) ≥ 0,

s∈S

a

s(cid:48)∈S\{s}

a(cid:48)

s(cid:48)∈S

a(cid:48)

where the right is a multi-homogeneous polynomial8 in the blocks (ηsa)a∈A ∈ RA with multi-degree
1S ∈ NS . If further Assumption 13 holds, the inverse implication also holds.

Proof. Let τ ∈ ∆S
state marginal. Assuming that the left hand side holds, we compute

A and let η denote its corresponding discounted stationary distribution and ρ the

(cid:88)

(cid:88)

bsaηsa

(cid:89)

(cid:88)

ηs(cid:48)a(cid:48) =

(cid:88)

(cid:88)

bsaτsaρs

(cid:89)

ρs(cid:48)

s∈S

a

s(cid:48)∈S\{s}

a(cid:48)

=

s∈S
(cid:32)

a

s(cid:48)∈S\{s}

(cid:33)

bsaτsa

·

(cid:89)

s(cid:48)∈S

ρs(cid:48) ≥ c

(cid:89)

s(cid:48)∈S

ρs(cid:48),

(cid:88)

s,a

which shows the ﬁrst implication. If further Assumption 13 holds, the product over the marginals is
strictly positive, which shows the other implication.

Remark 55. According to the preceding proposition, a linear inequality in the state policy polytope
∆S
A involving actions of k different states yields a polynomial inequality of degree k in the set
of state-action frequencies N µ
γ . In particular, for a linearly constrained policy model Π ⊆ ∆S
A,
where every constraint only addresses a single state, the set of state-action frequencies induced
by these policies will still form a polytope. This shows that this type of box constraints are well
aligned with the algebraic geometric structure of the problem. The linear constraints arising from
partial observability never exhibit this box type structure – unless the system is equivalent to its
fully observable version. This is because the projection of the effective policy polytope ∆S,β
A onto
a single state always gives the entire probability simplex ∆A, which is never the case, if there is a
non trivial linear constraint concerning only this state.
Theorem 16. Let (S, O, A, α, β, r) be a POMDP, µ ∈ ∆S and γ ∈ (0, 1] and assume that As-
sumption 13 holds. Then we have N µ,β
γ ∩ V ∩ B, where V is a variety described by
multi-homogeneous polynomial equations and B is a basic semialgebraic set described by multi-
homogeneous polynomial inequalities. Further, the face lattices of ∆S,β
are isomorphic.

= N µ

γ

A and N µ,β

γ

8A polynomial p : Rn1 ×· · ·×Rnk → R is called multi-homogeneous with multi-degree (d1, . . . , dk) ∈ Nk,

if it is homogeneous of degree dj in the j-th block of variables for j = 1, . . . , k.

28

Published as a conference paper at ICLR 2022

(In)equalities of state policies

(In)equalities of state-action frequencies

MDPs

∆S

A is described by

τ (a|s) ≥ 0

Row normalization:
(cid:80)
a τ (a|s) − 1 = 0

–

–

∆S,β

A is described in ∆S

A by

Linear (in)equalities
See Section 4
Closed form under Assumption 10:
See Theorem 12

POMDPs

Closed form for deterministic observ.:
See Remark 57

N µ

γ is described by

η(s, a) ≥ 0

–

(cid:104)ws

Discounted stationarity:
γ, η(cid:105) − (1 − γ)µ(s) = 0
For γ = 1:
s,a ηsa − 1 = 0

(cid:80)

N µ,β
γ

is described in N µ

γ by

Polynomial (in)equalities
See Section 4, Proposition 14
Closed form under Assumption 10:
See Remark 18 for inequalities
See Remark 56 for equalities
Closed form for deterministic observ.:
See Remark 57

Table 1: Correspondence of the deﬁning linear and polynomial inequalities of the (effective) state
policies and the (feasible) state-action frequencies for MDPs and POMDPs respectively.

A and N µ,β

γ = N µ
Proof. The equation N µ,β
it is clear from Proposition 14 that the mapping Ψ : ∆S
face lattices of ∆S,β
F, G ∈ F(∆S,β
Ψ(F ∨ G) is a face of N µ,β
Ψ(F ∨ G). Further, for any face I of N µ,β
face of ∆S,β

γ ∩V ∩B is a direct consequence of (3) and Proposition 14. Further,
induces a bijection of the
. In order to see that the join and meet are respected, we note that for
A ) it holds that Ψ(F ∧ G) = Ψ(F ∩ G) = Ψ(F ) ∩ Ψ(G) = Ψ(F ) ∧ Ψ(G). Further,
containing Ψ(F ) and Ψ(G) and hence by deﬁnition Ψ(F ) ∨ Ψ(G) ⊆
containing Ψ(F ) and Ψ(G) it holds that Ψ−1(I) is a

A containing F and G and hence Ψ−1(I) ⊇ F ∨ G or equivalently Ψ(F ∨ G) ⊆ I.

γ , π (cid:55)→ ηπ,µ

A → N µ

γ

γ

γ

γ

A from the state policy polytope ∆S

Comparing (17) and Theorem 16 we see that the linear space U corresponds to the variety V, where
the cone C corresponds to the basic semialgebraic set B. In general, every linear (in)equality cutting
out the effective policy polytope ∆S,β
A of the associated MDP
corresponds to a polyomial (in)equality cutting out the feasible state-action frequencies N µ,β
from
all state-action frequencies N µ
γ of the corresponding MDP, see also Table 1. This correspondence
arises by relating state-action frequencies to state policies via conditioning. Hence, the problem
of computing the deﬁning polynomial inequalities of the feasible state-action frequencies reduces
to computing the deﬁning linear inequalities of the effective policy polytope. This can be done in
closed form if β has linearly independent columns or if it deterinistic, see Remark 18, 56 and 57.
Remark 18. By Theorem 12 and Proposition 14, the deﬁning polynomials of the basic semialge-
braic set B from Theorem 16 are indexed by a ∈ A, o ∈ O and are given by

γ

pao(η) :=

(cid:88)

(cid:18)

β+
osηsa

(cid:89)

(cid:88)

(cid:19)

ηs(cid:48)a(cid:48)

=

(cid:88)

(cid:18) (cid:88)

(cid:19) (cid:89)

β+
os(cid:48)

ηsf (s) ≥ 0, (4)

a(cid:48)

f : So→A

s∈So
s(cid:48)∈So\{s}
where So := {s ∈ S | β+
os (cid:54)= 0}. The polynomials depend only on β and not on γ, µ nor α, and
have |So||A||So|−1 monomials of degree |So| of the form (cid:81)
ηsf (s) for some f : So → A. In
s∈So
particular, we can read of the multi-degree of pao with respect to the blocks (ηsa)a∈A which is given
by 1So (see also Proposition 14). A complete description of the set N µ,β
via (in)equalities follows
from the description of N µ
γ via linear (in)equalities given in (2). In Section 5 we discuss how the
degree of these polynomials controls the complexity of the optimization problem.

s(cid:48)∈f −1({a})

s∈So

γ

29

Published as a conference paper at ICLR 2022

Remark 56 (Deﬁning polynomial equalities). Analogously to the deﬁning inequalities, we can
compute the deﬁning polynomial equalities in the following way. First, we need to compute a
basis {bj}j∈J of {βπ | π ∈ RO×A}⊥ = ker(βT ) ⊆ RS×A, which can easily be done using the
Gram-Schmidt process. Note that the deﬁning linear equalities of the effective policy polytope (in
the policy polytope) are given by (cid:104)bj, τ (cid:105)S×A = 0. Hence, by Proposition 14 the corresponding
polynomial equality is given by

qj(η) :=

(cid:88)

(cid:88)

bj
saηsa

(cid:89)

(cid:88)

ηs(cid:48)a(cid:48) = 0,

s∈Sj

a∈A

s(cid:48)∈Sj \{s}

a(cid:48)∈A

(19)

sa (cid:54)= 0 for some a ∈ A}.

where Sj := {s ∈ S | bj
Remark 57 (Polynomial constraints for deterministic observations). In the case, where β corre-
sponds to a determinstic mapping we can compute all polynomial constraints in closed form. Let us
assume that β(o|s) = δog(s) for some mapping g : S → O and write So := g−1({o}) ⊆ S, then
τ ∈ ∆S

A belongs to the effective policy polytope ∆S,β
τ (a|s1) = τ (a|s2)

A if and only if
for all s1, s2 ∈ So, a ∈ A, o ∈ O.

(20)

Note that this can be encoded in (cid:80)
ﬁx so ∈ So, then (20) is equivalent to

o|A|(|So| − 1) = |A|(|S| − |O|) linear equations; indeed if we

τ (a|s) − τ (a|so) = 0 for all s ∈ So \ {so}, a ∈ A, o ∈ O.

(21)

Another way to derive these linear equalities is by noticing that esa − esoa form a basis of ker(βT ),
compare also Remark 56. By Proposition 14 for η ∈ N µ
or to satisfy

γ it is equivalent to lie in N µ,β

γ

ηsa

(cid:88)

a(cid:48)

ηsoa(cid:48) − ηsoa

(cid:88)

a(cid:48)

ηsa(cid:48) = 0 for all s ∈ So \ {so}, a ∈ A, o ∈ O.

(22)

s∈So

Note that in this case, there are no polynomial inequalities; this can also be seen from Remark 11
and Remark 18. Indeed, it holds that β+ = βT diag(n1, . . . , n|O|) ≥ 0, where no := |So|. Hence,
the polynomial inequalities pao(η) ≥ 0 are redundant on the cone [0, ∞)S×A.
Remark 58. In the fully observable case we have |So| = 1 for each o. Hence, each of the polynomial
inequalities has a single term of degree 1. Indeed, in this case the inequalities are simply ηsa ≥ 0,
os = 1s∈So/|So|. For each o, a,
for each a, for each s. In the case of a deterministic β, we have β+
there is an inequality (cid:80)
f : So→A |f −1(a)| (cid:81)
ηsf (s) ≥ 0 of degree |So| equal to the number of
states that are compatible with o.
Remark 59 (Reformulation of reward maximization as a polynomial program). By the theorem
above and Proposition 14, reward maximization is equivalent to the maximization of a linear function
subject to polynomial constraints. This enables the use of any (approximate) solution technique of
polynomial optimization problems in order to solve POMDPs. Such methods have been developed
for a long time and have been applied to a variety of problems (Anjos & Lasserre, 2011; Lasserre,
2015). As meta algorithm, this is presented in Algorithm 1. Once, a solution η∗ is obtained, the cor-
A can be computed by conditioning, i.e. τ (a|s) := ηsa/((cid:80)
responding state policy τ ∗ ∈ ∆S
a(cid:48) ηsa(cid:48)).
Then, every π∗ ∈ ∆O
A with βπ∗ = τ ∗ is an optimal policy. Such a policy can be computed by solv-
ing a system of linear equations, which are βπ = τ and π ∈ ∆O
A, which is standard. In particular,
if β has linearly independent columns, it holds that π∗ := β+τ . We demonstrate that this offers a
computationally feasible approach to planning of POMPDs in Section F on the toy example used for
Figure 1 and a grid world.

D DETAILS ON THE OPTIMIZATION

Let us quickly recall how we can reformulate the reward maximization problem as a polynomial
optimization problem, which then leads us to the mighty tools of algebraic degrees. We perceive
the reward maximization problem again as the maximization of a linear function p0 over the set of
feasible state-action frequencies N µ,β
. Since under Assumption 13 the parametrization π (cid:55)→ ηπ
is injective and has a full-rank Jacobian everywhere (see Appendix C.1.1), the critical points in the
policy polytope ∆O
(Trager et al., 2019). In general,

A correspond to the critical points of p0 on N µ,β

γ

γ

30

Published as a conference paper at ICLR 2022

Algorithm 1 Polynomial programming for POMDPs
Require: α, β, γ, µ
for s ∈ S do

ws ← δs ⊗ 1A − γα(s|·, ·)

end for
for a ∈ A, o ∈ O do

Deﬁne pao according to Equation (4)

end for
Compute a basis {bj}j∈J of {βπ | π ∈ RO×A}⊥ ⊆ RS×A
for j ∈ J do

Deﬁne qj according to Equation (19)

end for
η∗ ← arg max(cid:104)r, η(cid:105) sbj. to η ≥ 0, (cid:104)ws, η(cid:105) = (1 − γ)µs, (cid:104)1S×A, η(cid:105) = 1, pao(η) ≥ 0, qj(η) = 0
R∗ ← (cid:104)r, η∗(cid:105)
τ ∗ ← η∗(·|·) ∈ ∆S
A
π∗ ← solution of βπ = τ ∗

return maximizer η∗, optimal value R∗, optimal policy π∗

critical points of this linear function can occur on every face of the semialgebraic set N µ,β
. The
optimization problem thus has a combinatorial and a geometric component, corresponding to the
number of faces of each dimension and the number of critical points in the interior of any given
face. We have discussed the combinatorial part in Theorem 16 and focus now on the geometric part.
γ = {η ∈ RS×A : pi(η) ≤ 0, i ∈ I}, we are interested in the number of critical points
Writing N µ,β
on the interior of a face

γ

int(FJ ) = {η ∈ N µ,β

γ

| pj(η) = 0 for j ∈ J, pi(η) > 0 for i ∈ I \ J}.

Note that a point η ∈ int(FJ ) is critical, if and only if it is a critical point on the variety

VJ := {η ∈ RS×A | pj(η) = 0 for j ∈ J}.

For the sake of notation, let us assume that J = {1, . . . , m} from now on. We can upper bound the
number of critical points in the interior of the face by the number of critical points of the polynomial
optimization problem

maximize p0(η)

subject to pj(η) = 0 for j = 1, . . . , m,

(23)

where the polynomials have n variables. The number of critical points of this problems is upper
bounded by the algebraic degree of the problem as we discuss now.

D.1

INTRODUCTION TO ALGEBRAIC DEGREES

We try to present the results from the mighty theory of algebraic degrees that we use here and refer
the interested reader to the excellent low level introduction by Breiding et al. (2021) and to the
references therein. Let us consider the polynomial optimization problem (23), where we do not
require p0 to be linear. Further, denote the number of variables by n (in the case of state-action
frequencies n = |S||A|) and denote the degrees of p0, . . . , pm by d0, . . . , dm. We call a point
critical, if it satisﬁes the KKT conditions (∇p0(x) + (cid:80)m
i=1 λi∇pi(x) = 0, p1(x) = · · · = pm(x) =
0), which can be phrased as a system of polynomial equations (see Nie & Ranestad, 2009). The
number of complex solutions to those criticality equations, when ﬁnite, is called the algebraic degree
of the problem. The algebraic degree is determined by the nature of the polynomials p0, . . . , pm and
captures the computational complexity of the optimization problem (Kung, 1973; Bajaj, 1988).9
A special case of (23) is when m = n and the polynomials p1, . . . , pm are generic. Then by
B´ezout’s theorem there are exactly d1 · · · dn isolated points satisfying the polynomial constraints
and all of them are critical and hence the algebraic degree is precisely d1 · · · dn (Timme, 2021). If
the polynomials p0, . . . , pm deﬁne a complete intersection, i.e., the co-dimension of their induced

9The coordinates of critical points can be shown to be roots of some univariate polynomials whose degree

equals the algebraic degree and whose coefﬁcients are rational functions of the coefﬁcients of p0, . . . , pm.

31

Published as a conference paper at ICLR 2022

variety is m + 1, the algebraic degree of (23) is upper bounded by

d1 · · · dm

(cid:88)

(d0 − 1)i0 · · · (dm − 1)im ,

(24)

i0+···+im=n−m

and this bound is attained for generic polynomials (Nie & Ranestad, 2009; Breiding et al., 2021).
For non-complete intersections, the expression (24) does not need to yield an upper bound if some
constraints are redundant. However, we can modify the expression to obtain a valid upper bound.
Indeed, if l and c = n − l denote the dimension and co-dimension of
V := {x | p1(x) = · · · = pm(x) = 0}
and if p0 is generic and if the degrees are ordered, i.e., d1 ≥ · · · ≥ dm, then the algebraic degree is
upper bounded by

d1 · · · dc

(cid:88)

(d0 − 1)i0 · · · (dc − 1)ic .

(25)

i0+···+ic=l

To see this, ﬁx a subset J ⊆ {1, . . . , m} of cardinality c, such that V = {x | pj(x) = 0 for j ∈ J}.
Then we can apply the bound from (24) and evaluate it to be
(d0 − 1)i0 ·

(dj − 1)ij ,

(cid:88)

(cid:89)

(cid:89)

dj

which is clearly upper bounded by (25). If p0 is linear, then d0 = 1 and the expression simpliﬁes to

j∈J

i0+(cid:80)

j∈J ij =n−c

j∈J

d1 · · · dc

(cid:88)

(d1 − 1)i0 · · · (dc − 1)ic .

i1+···+ic=l

If further di = 1 for i ≥ k for some k ≤ c, then we obtain

d1 · · · dk

(cid:88)

(d1 − 1)i1 · · · (dk − 1)ik .

(26)

i1+···+ik=l

If pk+1, . . . , pm are afﬁne linear (and in general position relative to p1, . . . , pk, the algebraic degree
of (23) is given by the (m − k)-th polar degree δm−k(V) of the variety
V := {η | pk+1(η) = · · · = pm(η) = 0},
see Draisma et al. (2016); ¨Ozl¨um C¸ elik et al. (2021). This relation is particularly useful, since
for state-action frequencies there are always active linear equations as described in (2). The po-
lar degrees of certain interesting cases (Segre-Veronese varieties) have been recently computed by
Sodomaco (2020, Section 5) and our proof of Proposition 21 builds on those formulas and their
presentation by ¨Ozl¨um C¸ elik et al. (2021).
Remark 60 (Genericity assumptions). In the case, where the polynomials p0, . . . , pm are not
generic, there might be inﬁnitely many critical points. Indeed, even for a linear program, i.e., when
all polynomials are linear, there might be inﬁnitely many and even a non-trivial face of global op-
tima. This is however not the case if p0 is generic. Hence, the genericity assumptions on the reward
vector r and also other elements of the POMDP are not surprising. For example, they prevent the
reward vector to be identical to zero or to be perpendicular on all vectors δs ⊗ 1A − γα(s|·, ·) in
which cases the reward function would be constant and every policy would be a global optimum.

D.2 GENERAL UPPER BOUND ON THE NUMBER OF CRITICAL POINTS

Theorem 20. Consider a POMDP (S, O, A, α, β, r), γ ∈ (0, 1), assume that r is generic, that
β ∈ RS×O is invertible, and that Assumption 13 holds. For any given I ⊆ A × O consider the
following set of policies, which is the relative interior of a face of the policy polytope:

int(F ) = (cid:8)π ∈ ∆O

A | π(a|o) = 0 if and only if (a, o) ∈ I(cid:9) .

Let O := {o ∈ O | (a, o) ∈ I for some a} and set ko := |{a | (a, o) ∈ I}| as well as do := |{s |
β−1
os (cid:54)= 0}|. Then, the number of critical points of the reward function on int(F ) is at most

(cid:32)

(cid:89)

(cid:33)

dko
o

·

(cid:88)

(cid:89)

(do − 1)io,

(5)

o∈O io=l
where l = |S|(|A| − 1) − |I|. If α and µ are generic, this bound can be reﬁned by computing the
polar degrees of multi-homogeneous varieties (see Proposition 21 for a special case). The same
bound holds in the mean reward case γ = 1 for l given in Remark 61.

o∈O

o∈O

(cid:80)

32

Published as a conference paper at ICLR 2022

Proof. The face G of the effective policy polytope corresponding to F is given by

int(G) =

(cid:110)

τ ∈ ∆S,β

A | (β+τ )oa = 0 ⇔ (a, o) ∈ I

(cid:111)

.

In order to describe the corresponding set of discounted state-action frequencies, we use the notation

pao(η) :=


β+

osηsa

(cid:88)

s∈So

(cid:89)

(cid:88)

s(cid:48)∈So\{s}

a(cid:48)



ηs(cid:48)a(cid:48)

 ,

then it holds that

N µ,β

γ = {η ∈ N µ

γ | pao(η) ≥ 0 for all a ∈ A, o ∈ O}.

Then, F and G correspond to the face

int(H) = (cid:8)η ∈ N µ,β
= (cid:8)η ∈ N µ

γ

| pao(η) = 0 ⇔ (a, o) ∈ I(cid:9)

γ | pao(η) ≥ 0 and equality if and only if (a, o) ∈ I(cid:9) .

In order to use the explicit description of N µ
1A − γα(s|·, ·). Then, it holds that

γ given in (2), we remind the reader that ws

γ := δs ⊗

int(H) = (cid:8)η ∈ [0, ∞)S×A | pao(η) ≥ 0 and equality if and only if (a, o) ∈ I

(cid:104)ws

γ, η(cid:105)S×A = (1 − γ)µs for s ∈ S(cid:9),

where we used Proposition 8. Since the discounted state distributions are all positive by assumption,
for η ∈ int(H) it holds ηsa = 0 if and only if τ (a|s) := η(a|s) = 0. Note that τ = π ◦ β for some
π ∈ ∆O

A by assumption and thus for η ∈ int(H) it holds that ηsa = 0 if and only if

0 = τ (a|s) =

β(o|s)π(a|o),

(cid:88)

o

which holds if and only if (a, o) ∈ I for every o ∈ O with β(o|s) > 0. Hence, if we write
J := {(s, a) | (a, o) ∈ I for all o ∈ O with β(o|s) > 0}, we obtain

int(H) =

(cid:110)

η ∈ RS×A | ηsa ≥ 0 and equality if and only if (s, a) ∈ J,

(cid:104)ws

γ, η(cid:105)S×A = (1 − γ)µs for s ∈ S,

pao(η) ≥ 0 and equality if and only if (a, o) ∈ I

(cid:111)
.

The number of critical points over this surface is upper bounded by the number of critical points
over

V = {η ∈ RS×A | ηsa = 0 for (s, a) ∈ J,
(cid:104)ws

γ, η(cid:105)S×A = (1 − γ)µs for s ∈ S, pao(η) = 0 for (a, o) ∈ I}.

Now we want to apply (26) and note that the objective p0 = r is generic. Further, we see that
there are |I| non-linear constraints and hence in the notation of (26) have k = |I|. Further, we can
calculate to dimension and co-dimension of V as follows. Note that F → V, π (cid:55)→ ηπ is a local
parametrization of V (meaning it parametrizes a full dimensional subset of V), which is injective
and has full rank Jacobian everywhere. Hence, we have

l = dim(V) = dim(F ) = |S|(|A| − 1) − |I| = |S||A| − |S| − |I|.

The co-dimension of V is given by |S||A| − dim(V) = |S| + |I| and with the notation from (26), we
have c = |S| + |I| ≥ k. Further, it holds that deg(pao) ≤ do and using (26) yields an upper bound
of

(cid:89)

do ·

(cid:88)

(cid:89)

(do − 1)jao =

(s,o)∈I

(cid:80)

(a,o)∈I jao=m

(a,o)∈I

(cid:89)

o∈O

dko
o ·

(cid:88)

(cid:89)

(do − 1)io.

(cid:80)

o∈O io=m

o∈O

33

Published as a conference paper at ICLR 2022

Remark 61 (The mean reward case). Theorem 20 can be generalized to the mean reward case, i.e.,
to the case of γ = 1 with some adjustments. Indeed, the proof can be carried out analogously,
however, the characterization of N µ
sa ηsa = 1, see also
Proposition 8. Indeed, in the mean reward case we have with the notation from the proof above

1 has the extra linear condition that (cid:80)

int(H) =

(cid:110)

η ∈ RS×A | ηsa ≥ 0 and equality if and only if (s, a) ∈ J,

(cid:104)ws

γ, η(cid:105)S×A = 0 for s ∈ S,

(cid:88)

sa

ηsa = 1,

pao(η) ≥ 0 and equality if and only if (a, o) ∈ I

(cid:111)
.

Hence, the upper bound in (5) remains valid if we set

(cid:110)

l := dim

η ∈ RS×A | ηsa = 0 for (s, a) ∈ J, (cid:104)ws

γ, η(cid:105)S×A = 0 for s ∈ S,

ηsa = 1, pao(η) = 0 for (a, o) ∈ I

(cid:111)
.

(cid:88)

sa

(27)

In the discounted case we obtained an explicit formulation for l. In the mean case the value obeys
a case distinction depending, in particular, on whether the all ones vector 1S lies in the span of
the vectors ws
γ. However, the value can be computed from the above expression (27) in any given
speciﬁc case.

Corollary 62 (Critical points of MDPs). Consider an MDP (S, A, α, r), γ ∈ (0, 1), assume that r
is generic, that β ∈ RS×O is invertible, and that Assumption 13 holds. Then, every critical point
π ∈ ∆O

A of the discounted expected reward function is deterministic.

Proof. We evaluate the bound of Equation (5). If the face is not a vertex, then the corresponding
index set I ⊆ A × O satisﬁes |I| < |O|(|A| − 1) and thus in the notation from Theorem 20 it holds
that l > 0. Note that do = 1 for every o ∈ O and hence there is at least one factor in the product
in (5) that vanishes and so does the whole expression in (5).

Remark 63 (Geometry around the critical points). The key argument in the proof of Theorem 20
is that a critical point π ∈ ∆O
A of the reward function corresponds to a critical point η of a linear
function over a multi-homogeneous variety V, where the deﬁning polynomials can be computed by
the means of Proposition 8 and Proposition 14. A closer study of this variety would shed light into
the geometry of the loss landscape around the critical points, which has important implications for
gradient based methods.

o|β(o|s) − β(cid:48)(o|s)|/2 ≤ ε for every s ∈ S. Then if π ∈ ∆O

Remark 64 (Efﬁcient design of observation mechanisms). The bound (5) could be used to design
observation mechanisms in such a way that the reward function has the least critical points, which
would potentially make the system more approachable for gradient based methods. Rauh et al.
(2021) showed that planning in POMDPs is stable under perturbations of the observation kernel β.
O satisfying (cid:107)β(·|s) − β(cid:48)(·|s)(cid:107)T V =
More precisely, consider two observation kernels β, β(cid:48) ∈ ∆S
(cid:80)
A is an optimal policy of
(S, A, O, α, β(cid:48), r), then it is a 2εγ(cid:107)r(cid:107)∞/(1 − γ)-optimal policy of (S, A, O, α, β, r). Hence, if
β does not fulﬁll the invertability assumption made in Theorem 20 an arbitrary small perturbation
of it does (given that β is a square matrix) and hence Theorem 20 provides an upper bound on the
number of critical points of an approximate problem. Further, note that the faces, which are guar-
anteed to contain an optimal policy by Mont´ufar & Rauh (2017) might be considerably fewer for
the POMDP (S, A, O, α, β(cid:48), r). The bound (5) could be used to identify the best perturbations of a
given magnitude to obtain a problem with a minimal number of critical points.

Remark 65 (Design of policy models). Knowledge about the location of critical points of the reward
function can be used to design policy models, which provably include those critical points and
therefore also the optimal policy.

D.3 NUMBER OF CRITICAL POINTS IN A TWO-ACTION BLIND CONTROLLER

This subsection is devoted to the proof of Proposition 21 that we restate here for convenience.

34

Published as a conference paper at ICLR 2022

Proposition 21 (Number of critical points in a blind controller). Let (S, O, A, α, β, r) be a POMDP
describing a blind controller with two actions, i.e., O = {o} and A = {a1, a2} and let r, α and µ
be generic and let γ ∈ (0, 1). Then the reward function Rµ
γ has at most |S| critical points in the
interior int(∆O

A) ∼= (0, 1) of the policy polytope and hence at most |S| + 2 critical points.

Before we present the proof of this result, we discuss how the bound on the rational degree of the
reward function leads to am upper bound on the number of critical points. We consider a blind
controller and restrict ourselves to the discounted case γ ∈ (0, 1). We associate the policy polytope
A with [0, 1] and for p ∈ [0, 1] we write πp and ηp for the associated policy and the state-action
∆O
frequency. From Theorem 4 we know that the reward function R = f /g : [0, 1] → R is a rational
function of degree at most k := |S|, which is well known to possess at most 2k − 2 critical points.
Hence, there are at most this many critical points in the interior (0, 1) if the reward function is not
constant. Now we use the geometric description of the set of state-action frequencies and yields a
reﬁned bound.

Proof of Proposition 21. First, we note that since µ is generic and γ < 1 Assumption 13 is sat-
isﬁed.
In this case, the combinatorial part is simple, since there are only two zero-dimensional
faces of the state-action frequencies (corresponding to the endpoints of the unit interval) and one
one-dimensional face (corresponding to the interior of the unit interval). Let us set

U = {η ∈ RS×A | (cid:104)ws

γ, η(cid:105)S×A = (1 − γ)µs for all s ∈ S},

where ws
action frequencies is given by

γ := δs ⊗ 1A − γα(s|·, ·). By Proposition 8 and Example 15 the set of discounted state-

N µ,β

γ = N µ

γ ∩ D1 = [0, ∞)S×A ∩ U ∩ D1.

A with [0, 1] and for p ∈ [0, 1] we write πp and ηp for
Like above, we associate the policy polytope ∆O
the associated policy and the state-action frequency. We aim to bound the number of critical points
of the reward function over (0, 1) or equivalently the number of critical over {ηp | p ∈ (0, 1)} where
we used that Assumption 13 holds. Further, recall that ηp(a|s) = ηp

s, we have that

sa/ρp

{ηp | p ∈ (0, 1)} = {η ∈ N µ,β
= {η ∈ N µ,β
= (0, ∞)S×A ∩ U ∩ D1.

γ

γ

| η(a|s) > 0 for all s ∈ S, a ∈ A}

| ηsa > 0 for all s ∈ S, a ∈ A}

Thus the number of critical points over {ηp | p ∈ (0, 1)} are upper bounded by the number of critical
points on U ∩ D1. Note that if α and µ are generic, the subspace U is in general position. Further, its
dimension is |S||A| − |S| = |S|, where we used |A| = 2. Hence, the number of complex solutions
to the KKT conditions over U ∩ D1 are given by the k-th polar degree δk(D1), where k := |S|,
where we also used the genericity of the reward vector. We can compute the polar degree using the
formula presented by ¨Ozl¨um C¸ elik et al. (2021) to obtain

δk(D1) =

k−2k+k+2
(cid:88)

s=0

(−1)s

(cid:18) k − s + 1

(cid:19)

2k − (k + 1)

(k − s)!





(cid:88)

i+j=s

(cid:1)

(cid:0)k
i
(k − 1 − i)!

·

(cid:1)
(cid:0)2
j
(2 − 1 − j)!





2
(cid:88)

(−1)s

=

s=0

(cid:19)

(cid:18)k − s + 1
k − 1

(k − s)!





(cid:88)

i+j=s

(cid:1)

(cid:0)k
i
(k − 1 − i)!

·

(cid:1)
(cid:0)2
j
(2 − 1 − j)!



 .

We calculate the three individual terms to be

(cid:19)

(cid:18)k + 1
k − 1



k!



(cid:88)

i+j=0

(cid:1)

(cid:0)k
i
(k − 1 − i)!

·

(cid:1)

(cid:0)2
j
(2 − 1 − j)!



 =

(k + 1)k
2

· k! ·

(cid:1)

(cid:0)k
0
(k − 1)!

·

(cid:1)
(cid:0)2
0
1!

=

(k + 1)k2
2

,

and

(cid:18) k

(cid:19)

k − 1

−

(k − 1)!

(cid:1)

(cid:32) (cid:0)k
1
(k − 2)!

(cid:1)

(cid:0)2
1
(k − 1)!

+

(cid:33)

(cid:18)

= −k!

k
(k − 2)!

+

2
(k − 1)!

(cid:19)

= −k2(k − 1) − 2k,

35

Published as a conference paper at ICLR 2022

and

(cid:19)

(cid:18)k − 1
k − 1

(k − 2)!

(cid:32) (cid:0)k
1

(cid:1)
(cid:1)(cid:0)2
1

(k − 2)!

(cid:1)

(cid:0)k
2
(k − 3)!

+

(cid:33)

= (k − 2)!

(cid:18) 2k

(k − 2)!

(cid:19)

+

k(k − 1)
2(k − 3)!

= 2k +

k(k − 1)(k − 2)
2

.

Adding those three summands we obtain

δk(D1) =

k3 + k2 + k3 − 3k2 + 2k
2

− k3 + k2 − 2k + 2k = k.

Note that there is also a more structural argument to obtain this polar degree. In fact, the polar
1 denotes the dual variety of D1 ( ¨Ozl¨um C¸ elik
degree δl(D1) = 0 for l > dim(D∗
et al., 2021). Note that in the case of k × 2 matrices D∗
1 = D1 (Draisma et al., 2016) and hence
it holds that δl(D1) = 0 for l > dim(D1) − 1 = k (Spaenlehauer, 2012). The largest non-zero
polar degree is equal to the degree of the dual variety (Draisma et al., 2016) and hence we obtain
δk(D1) = degree(D∗

1) = degree(D1) = k (Spaenlehauer, 2012).

1) − 1, where D∗

Note that this bound is not necessarily sharp, since it is exactly the number of complex solutions of
the criticality equations over U ∩ D1. Overall, we have seen that the study of the algebraic properties
of the reward function provided an upper bound on the number of critical points of the problem,
which can be improved using the description of the state-action frequencies as a basic semialgebraic
set and employing tools from algebraic geometry.

D.4 EXAMPLES WITH MULTIPLE SMOOTH AND NON-SMOOTH CRITICAL POINTS

It is the goal of this example to demonstrate that for a blind controller multiple critical points can
occur in the interior (0, 1) ∼= int(∆O
A of the policy
polytope. We refer to such points as smooth and non-smooth critical points. We consider a blind
controller with one observation, two actions a1, a2 and three states s1, s2, s3 and a deterministic
transition kernel α and reward described by the graph shown in Figure 2.

A) as well as at the two endpoints of [0, 1] ∼= ∆O

a1, 5

s1

a1, −30

a1, 0

a2, 30

s2

a2, −5

a2, 0

s3

Figure 2: Graph describing the deterministic transition kernel α and the associated instantaneous
rewards.

We make the usual identiﬁcation [0, 1] ∼= ∆O
A, where we associate p with π(a1|o). In Figure 3, the
reward function is plotted on the left for the three initial conditions µ = δs1 , δs2 , δs3. It is apparent
∼= [0, 1] for the
that the reward has two critical points in the interior of the policy polytope ∆O
A
two initial conditions µ = δs1 = δs3 . For µ = δs2 , there are two strict local maxima on the two
endpoints of the interval. In this example, the bound from Proposition 21 ensures that there are at
most |S| = 3 critical points in the interior and at most |S| + 2 = 5 critical points in the whole policy
polytope. We see that those bounds are not sharp in this speciﬁc setting. Note that this example is
stable under small perturbations of the transition kernel and reward vector and hence can occur for

36

Published as a conference paper at ICLR 2022

generic α and r. The right hand side of Figure 3 shows a three dimensional random projection of the
set of feasible discounted state-action frequencies. By Theorem 4 they are a curve in RS×A ∼= R6
with an injective rational parametrization of degree at most |S| = 3.

Figure 3: Plot of the reward function for initial distributions δs on the left and of a three-dimensional
random projection of the set of feasible discounted state-action frequencies on the right.

D.5

(SUPER)LEVEL SETS OF (PO)MDPS

D.5.1 CONNECTEDNESS OF SUPERLEVEL SETS IN MDPS

Theorem 66 (Existence of improvement paths in MDPs). For every policy π ∈ ∆S
A, there is a
continuous path connecting π to an optimal policy along which the reward is monotone. If further
π (cid:55)→ ηπ is injective, the reward is strictly monotone along this path, if π is suboptimal. In particular,
the superlevel sets of MDPs are connected.

Proof. Let us ﬁx π ∈ ∆S
A and set η0 := ηπ and η1 be a global optimum and ηt be the linear
interpolation and ρt be the corresponding state marginal. Note that for s ∈ S it holds that either
ρt(s) > 0 for all t ∈ (0, 1) or ρt(s) = 0 for all t ∈ [0, 1]. In the latter case, we can set πt(·|s) to
be an arbitrary element in ∆A. For the other states and t ∈ (0, 1) we can deﬁne the policy through
conditioning by πt(a|s) := ηt(s, a)/ρt(s) and will continuously extend the deﬁnition to t ∈ {0, 1}
in the following. If ρ0(s) > 0 or ρ1(s) > 0, then the deﬁnition extends naturally. Suppose that
ρ0(s) = 0, then we now that ρ1(s) > 0 since otherwise ρt(s) = 0 for all t ∈ [0, 1]. Now for t > 0
it holds that

,

=

=

=

πt(s, a) =

ηt(s, a)
ρt(s)

(1 − t)η0(s, a) + tη1(s, a)
(1 − t)ρ0(s) + tρ1(s)
which extends continuously to t = 0. If ρ1(s) = 0, then like before, πt(·|s) does not depend on t
and we can extend it to t = 1. Now we have constructed a continuous path πt, such that ηπt = ηt
and thus

tη1(s, a)
tρ1(s)

η1(s, a)
ρ1(s)

R(πt) = (cid:104)r, ηt(cid:105) = (1 − t)(cid:104)r, η0(cid:105) + t(cid:104)r, η1(cid:105) = R(π0) + t(R∗ − R(π0)),
which is strictly increasing if π0 is suboptimal. It remains to construct a continuous path between
π0 and π. Note that if ρ0(s) > 0, the policies π0 and π agree on the state s and so does the
linear interpolation between the two policies. Now, by Proposition 46 we see that every linear
interpolation between π0 and π has the state-action distribution η0. Gluing the two paths, we obtain
a path that ﬁrst leaves the state-action distribution unchanged and then increases the reward strictly
up to optimality.

D.5.2 THE SEMIALGEBRAIC STRUCTURE OF LEVEL AND SUPERLEVEL SETS FOR POMDPS

Consider a POMDP (S, A, O, α, β, r) and ﬁx a discount rate γ ∈ (0, 1) as well as an initial condition
µ ∈ ∆S . The levelset

La := {π ∈ ∆O

A | Rµ

γ (π) = a}

37

0.00.20.40.60.81.0Probability of selecting first actionDiscounted rewardstarting at s_3starting at s_2starting at s_1Published as a conference paper at ICLR 2022

of the reward function is the intersection of a variety generated by one determinantal polynomial
of degree at most |S| with the policy polytope ∆O
A. Indeed, by Theorem 4 the reward function Rµ
γ
is the fraction f /g of two determinantal polynomials f and g of degree at most |S|. The level set
consists of all policies, such that f (π) = ag(π). Thus, the levelset is given by

La = ∆O

A ∩ (cid:8)x ∈ RO×A | f (x) − ag(x) = 0(cid:9) .

Analogously, a superlevel set is the intersection

∆O

A ∩ (cid:8)x ∈ RO×A | f (x) − ag(x) ≥ 0(cid:9)
of a basic semialgebraic generated by one determinantal polynomial of degree at most |S| with the
policy polytope ∆O
A. In particular, both the levelset and superlevel sets of POMDPs are semialge-
braic sets deﬁned by linear inequalities and equations (corresponding to the conditional probability
polytope ∆O
A) and a determinantal (in)equality of degree at most |S|. This description can be used
to bounds the number of connected components, which captures important properties of the loss
landscape of an optimization problem (Barannikov et al., 2019; Catanzaro et al., 2020). By a the-
orem due to Łojasiewicz, level and superlevel sets possess ﬁnitely many connected (semialgebraic)
components (Ruiz, 1991; Basu et al., 2006) and there exist algorithmic approaches to computing the
number of connected components (Grigor’ev & Vorobjov, 1992) as well as explicits upper bounds,
which involve the dimension, the number of deﬁning polynomials as well as their degrees (Basu,
2003; 2014). Those results are generalizations of the classic result due to Milnor and Thom which
bounds the sum of all Betti numbers of a variety. If we apply the Milnor-Thom theorem to the vari-
ety V we obtain that there are at most |S|(2|S| − 1)|O||A|−1 many connected components of V. This
bound neglects the determinantal nature of the deﬁning polynomial and might therefore be coarse.
Using an analogue approach, we can also study the level and superlevel sets of the reward function
in the space of feasible state-action frequencies. Indeed, they are the intersections of the hyperplane
{η ∈ RS×A | (cid:104)r, η(cid:105)S×A = a} and halfspace {η ∈ RS×A | (cid:104)r, η(cid:105)S×A ≥ a} with the semialgebraic
set N µ,β

of state-action frequencies.

γ

E POSSIBLE EXTENSIONS

E.1 APPLICATION TO FINITE MEMORY POLICIES

In general, it is possible to reduce POMDPs with ﬁnite memory policies to a POMDP with mem-
oryless policies by augmenting the state and observation space with the memory. Say we consider
policies with a memory that stores the last k observations that were made. Then we could set
˜S := S × Ok−1 and ˜O := Ok. If the ﬁrst state is s0 and the ﬁrst observation that is being made is
o0, then we will associate it with ˜s0 := (s0, o0, . . . , o0) ∈ ˜S and ˜o0 := (o0, . . . , o0) ∈ ˜O respec-
tively. If after t steps, the current state is ˜st = (st, ot−k, . . . , ot−1) and the next observation is ot,
then we set ˜ot := (ot−k, . . . , ot). An analogue strategy can be taken when the memory does consist
of more than the history of observations and for example includes the history of decision. It remains
open to explore the implications of the translation of our results to policies with internal memory
with this identiﬁcation.

E.2 POLYNOMIAL POMDPS

γ

γ (π) = f (ηπ,µ

Zahavy et al. (2021) consider MDPs, where the objective is a convex function of the state-action fre-
) for some convex function f : RS×A → R and coin the name
quency, i.e., where Rµ
of convex MDPs. In analogy, we refer to the case where f is a polyomial function as polynomial
(PO)MDPs. In polynomial POMDPs, the problem of reward maximization is by deﬁnition an op-
timization problem of a polynomial function over the set N µ,β
of feasible state-action frequencies.
Since the feasible state-action frequencies form a basic semialgebraic set, the problem of reward
maximization in polynomials is a polynomial optimization problem. Hence, the method of bound-
ing the number of critical points as discussed in Section 5 generalizes to the case of polynomial
reward criteria. If f is a polynomial of degree d, the upper bound (5) from Theorem 20 takes the
form

γ

(cid:89)

dko
o ·

(cid:88)

(d − 1)i (cid:89)

(do − 1)io.

o∈O

i+(cid:80)

o∈O io=m

o∈O

38

Published as a conference paper at ICLR 2022

The use of polar degrees does not extend in general to the case of polynomial POMDPs, since they
require a linear objective function, but can still be related to the algebraic degree for a quadratic
objective as it is the case for the Euclidean distance function (Draisma et al., 2016).

F EXAMPLES

Here, we provide examples, which illustrate our ﬁndings. In particular, we compute the deﬁning
polynomial inequalities of the set of feasible state-action frequencies for the example from Figure 1
and a navigation problem in a grid world. We use an interior point method to solve the constrained
optimization problem corresponding to the polynomial programming formulation of the respective
POMPDs and see that in this offers a computationally feasible approach to the reward maximization
problem.

F.1 TOY EXAMPLE OF FIGURE 1

We discuss in detail a toy POMDP which we used to generate the plots in Figure 1. We consider
state, observation, and action spaces with two elements each, as well as following deterministic
transition mechanism α, observation mechanism β, and instantaneous reward r:

S = {s1, s2},
O = {o1, o2},
A = {a1, a2},

β =

(cid:18) 1
1/2

(cid:19)

,

0
1/2

α(si|sj, ak) = δik,

r(s, a) = δs1,sδa1,a + δs2,sδa2,a,

γ = 0.5.

The transitions, instantaneous rewards, and observations are shown in Figure 4. As an initial distri-
bution we take the uniform distribution µ = (δs1 + δs2 )/2 over the states.

a1, +1

s1

a2

a1

s2

a2, +1

s1

s2

1

1/2

1/2

o1

o2

Figure 4: The left shows the transition graph of the toy example. The right shows the observation
mechanism; the numbers on the edges indicate the observation probabilities.

Polynomial programming formulation To illustrate Theorem 16 (and Proposition 14), we de-
rive step-by-step the explicit polynomial program for the reward maximization in this toy example.
For this, we ﬁrst compute the deﬁning inequalities of the set of feasible state-action frequencies.
We begin with the linear constraints that deﬁne the set N µ
γ of state-action frequencies of the asso-
ciated MDP, given in general form in Proposition 8. In the remainder, we denote the state-action
frequencies as matrices

η =

(cid:18)η(s1, a1)
η(s2, a1)

(cid:19)

η(s1, a2)
η(s2, a2)

(cid:18)η11
η21

=

(cid:19)

η12
η22

∈ RS×A.

Following Proposition 8, the linear inequalities are ηij ≥ 0 for all i, j ∈ {1, 2}, and the linear
equations are (cid:104)wi

γ, η(cid:105)S×A = (1 − γ)µi = 1/4 for i ∈ {1, 2}, whereby here
(cid:18)1
0

γ = δ1 ⊗ 1A − γα(1|·, ·) =

(cid:18)1
1

(cid:19)
0
0

(cid:19)
1
0

−

=

(cid:18) 1
−1

w1

1
2

1
2

(cid:19)
2
0

and

w2

γ = δ2 ⊗ 1A − γα(2|·, ·) =

(cid:18)0
1

(cid:19)
0
1

−

(cid:18)0
0

1
2

(cid:19)
1
1

=

(cid:19)

(cid:18)0 −1
1

2

.

1
2

Thus the two linear equations are

2η11 + 4η12 − 2η21 = 1
−2η12 + 4η21 + 2η22 = 1.

(28)

39

Published as a conference paper at ICLR 2022

It remains to compute the polynomial inequalities, which can be done using Remark 18. We invert
the matrix β and obtain

β+ = β−1 =

(cid:18) 1
−1

(cid:19)
0
2

∈ RS×O.

Using the notation from Remark 18 we have So1 = {s1} and So2 = {s1, s2}, and thus the polyno-
mial inequalities are

η11 ≥ 0
η12 ≥ 0
−η11(η21 + η22) + 2η21(η11 + η12) ≥ 0
−η12(η21 + η22) + 2η22(η11 + η12) ≥ 0.

The ﬁrst two inequalities can be seen to be redundant and can be discarded. Finally, note that the
objective function is given by

(cid:104)r, η(cid:105)S×A = η11 + η22.
Hence, we have obtained the following explicit formulation of the reward maximization problem as
a polynomial optimization problem:

maximize η11 + η22

subject to





2η11 + 4η12 − 2η21 − 1 = 0
−2η12 + 4η21 + 2η22 − 1 = 0
η11, η12, η21, η22 ≥ 0
η11η21 + 2η21η12 − η11η22 ≥ 0
η12η22 + 2η11η22 − η12η21 ≥ 0.

(29)

Solution with constrained and polynomial optimization tools The formulation (29) allows us
to use polynomial optimization algorithms, semi-deﬁnite programming (SDP) solvers, or relaxation
hierarchies such as the popular Sum Of Squares (SOS). Using the modeling language JuMP and the
interior point solver Ipopt we directly obtained the globally optimal10 solution to problem (29)
(rounded to three digits)

η∗ =

(cid:18)0.667
0.167

(cid:19)

.

0
0.167

The corresponding optimal state policy τ ∗ is obtained simply by conditioning on states, and any
pre-image under the observation kernel is an optimal observation policy, in this case simply π∗ =
β−1τ ∗,

π∗ =

(cid:18)1
0

(cid:19)
0
1

∈ ∆O
A.

This policy achieves a reward of Rµ
γ (π∗) = (cid:104)r, η∗(cid:105)S×A = 0.833 (rounded to three digits). The
computations took 0.01s (on a 2 GHz Quad-Core Intel Core i5 processor). The command in JuMP
to call the optimizer Ipopt is simply:

model = Model(optimizer_with_attributes(Ipopt.Optimizer)
@variable(model, \eta[1:2, 1:2]>=0)
@constraint(model, 2\eta[1, 1] + 4\eta[1, 2] - 2\eta[2, 1] == 1)
@constraint(model, -2\eta[1, 2] + 4\eta[2, 1] + 2\eta[2, 2] == 1)
@constraint(model, \eta[1, 1]\eta[2, 1] + 2\eta[2, 1]\eta[1, 2]

- \eta[1, 1]\eta[2, 2] >= 0)

@constraint(model, \eta[1, 2]\eta[2, 2] + 2\eta[1, 1]\eta[2, 2]

- \eta[1, 2]\eta[2, 1] >= 0)

@NLobjective(model, Max, \eta[1, 1] + \eta[2, 2])
optimize!(model)

For completeness, we also provide the command to solve a relaxation in Python SumOfSquares,
which is the following, although we found this to run a bit slower depending on the selected de-
gree. Here we negate the objective in order to obtain a minimization problem and square the search
variables (which are required to be non-negative) in order to obtain polynomials of even degree:

10The SOS relaxation provides a certiﬁcate for global optimality in this case.

40

Published as a conference paper at ICLR 2022

e11, e12, e21, e22 = sp.symbols(’e11 e12 e21 e22’)
prob = poly_opt_prob([e11, e12, e21, e22], - e11**2 - e22**2,

eqs=[+ 2 * e11**2 + 4 * e12**2 - 2 * e21**2 - 1,
- 2 * e12**2 + 4 * e21**2 + 2 * e22**2 - 1,
+ e11**2 + e12**2 + e21**2 + e22**2 - 1],
ineqs=[e11**2 * e21**2 + 2 * e21**2 * e12**2 - e11**2 * e22**2,
e12**2 * e22**2 + 2 * e11**2 * e22**2 - e12**2 * e21**2], deg=2)

prob.solve()
print(prob.value)

Policy gradient methods may not ﬁnd a global optimum We want to demonstrate an important
problem of policy gradient methods, which is the well known possibility to get stuck in local optima,
in the case of the toy example. For this, we used a tabular softmax policy model to represent the
interior of the policy polytope ∆O

A, i.e. used the following parametric policy model

πθ(a|o) :=

exp(θoa)
a(cid:48) exp(θoa(cid:48))

(cid:80)

for θ ∈ RO×A.

We computed 15 policy gradient trajectories, where we used the policy gradient theorem (see Corol-
lary 52) to compute the update directions. The starting positions where generated randomly, such
that the initial conditions in the policy polytope ∆O
A are uniformly random. The trajectories in the
∼= [0, 1]2 are shown in Figure 5, which also shows a heat map of the reward
policy polytope ∆O
A
function. We observe that 5 of the trajectories converge to a suboptimal strict local minimum. Note
that this is not artefact of the parametrization, but of the fact that there is a strict local minimum
and hence every naive local optimization method will suffer from this problem. The reward of the
suboptimal local minimum

(cid:18)1
1

(cid:19)
0
0

∈ ∆O
A

R

is 0.747 if rounded to 3 digits.

π =

)
1
|
2
(
π

π(2|2)

Figure 5: Policy gradient optimization trajectories (shown as black curves) in the observation policy
polytope. As expected, since the problem is non-convex and has several distinct local optimizers,
the trajectories converge to different local optimizers depending on the initial policy.

Number of critical points We evaluate the bound of Theorem 20 for this toy problem. First note
(cid:1). Further, Assump-
that in this example the observation matrix β is invertible with β−1 = (cid:0) 1 0
tion 13 is satisﬁed for initial distributions µ with full support. Hence, we can apply Theorem 20.
Here, we have |S| = |A| = |O| = 2 and in the notation of Theorem 20 we have do1 = 1 and
do2 = 2. As discussed in the main body, the bound evaluates to zero if we consider the inte-
rior of the policy polytope, which corresponds to I = ∅. This means that there are no critical

−1 2

41

0.00.20.40.60.81.00.00.20.40.60.81.00.29880.35280.40680.46080.51480.56880.62280.67680.73080.7848Published as a conference paper at ICLR 2022

points in the interior of the policy polytope, in other words, all optimal policies lie at the boundary
and hence have one or more zero entries. The one-dimensional faces correspond to the index sets
{(a1, o1)}, {(a2, o1)}, {(a1, o2)}, {(a2, o2)}. The choices I = {(a1, o1)}, {(a2, o1)} correspond to
the two edges on the left and right of the policy polytope ∆O
A as shown in the top left corner of
Figure 1 or alternatively to the two straight faces of the set N µ,β
of state-action distributions shown
in the top right corner. The bound (5) evaluates to zero for those choices. This can also be seen in
the bottom row in Figure 1, where it is apparent that there are no critical points on the respective
faces. For the choices I = {(a1, o2)}, {(a2, o2)} the bound (5) evaluates to two. Indeed, these faces
contain critical points. The bound is not sharp in this case since the actual number of critical points
in any of the two faces of the policy polytope ∆O
A, which correspond to the two non-linear faces of
N µ,β
is one. Nonetheless, this illustrates how the theorem allows us to discard most faces of the
γ
polytope and focus the search for an optimal policy on just two faces.

γ

F.2 NAVIGATION IN A GRID WORLD

1

2

4

5

3

6

9

10

11

7

8

12

13

the reward of R is obtained in state 1, the actions are
Figure 6: Depiction of a grid world;
{R, L, U, D} corresponding to movements to the right, left, up and down; observed are the pos-
sible directions that the agent can move in. Once the agent transitions to state 1, she transfers to
state 7 and 13 uniformly.

We consider the grid world depicted in Figure 6 with 13 states and 7 observations, where it
is the goal to reach state 1. The four actions are {R, L, U, D} corresponding to the directions
right, left, up and down on the grid. The transitions are deterministic and lead to the cell right,
left, above or below the current cell, if this cell is admissible; from the goal state 1 one tran-
sitions uniformly to the states 7 and 13 independently of the chosen action. Further, we con-
sider deterministic observations, which correspond to the agent being able to observe its imme-
diate four neighboring positions. This observation mechanism partitions the state space into the
seven subsets {1, 7}, {2, 4, 9, 10}, {3, 8}, {5}, {6, 12}, {11}, {13}, which lead to the observations
o1, o2, o3, o4, o5, o6 and o7 respectively. Hence, by Remark 57 the polynomial constraints are given
by

η7aρ1 − η1aρ7 = 0 for all a ∈ A
η4aρ2 − η2aρ4 = 0 for all a ∈ A
η8aρ2 − η2aρ8 = 0 for all a ∈ A
η10aρ2 − η2aρ10 = 0 for all a ∈ A
η9aρ3 − η3aρ9 = 0 for all a ∈ A
η12aρ6 − η6aρ12 = 0 for all a ∈ A,

42

Published as a conference paper at ICLR 2022

where ρs = (cid:80)

a ηsa. The linear constraints apart from η ≥ 0 can be computed to be

ρ1 − γ(η12 + η13 + η14 + η22) = µ1
ρ2 − γ(η11 + η23 + η24 + η32) = µ2
ρ3 − γ(η21 + η33 + η42) = µ3
ρ4 − γ(η31 + η43 + η44 + η52) = µ4
ρ5 − γ(η41 + η53 + η54) = µ5
ρ6 − γ(η34 + η61 + η62 + η11,3) = µ6
ρ7 − γ(ρ1/2 + η72 + η73 + η74 + η82) = µ7
ρ8 − γ(η71 + η83 + η84 + η92) = µ8
ρ9 − γ(η81 + η93 + η10,2) = µ9
ρ10 − γ(η91 + η10,3 + η10,4 + η11,2) = µ10
ρ11 − γ(η10,1 + η11,1 + η11,4) = µ11
ρ12 − γ(η94 + η12,1 + η12,2 + η13,3) = µ12
ρ13 − γ(ρ1/2 + η12,4 + η13,1 + η13,2 + η13,4) = µ13.

Further, the objective function is given by

(cid:104)r, η(cid:105)S×A = η11 + η12 + η13 + η14.

Let us now consider the uniform distribution µs = 1/13 for s ∈ S as an initial distribution and
γ = 0.999 as a discount factor. Like for the toy problem we used the interior point method Ipopt
implemented in the Julia packages JuMP and Ipopt to solve this polynomial optimization problem.
The solver took around 0.03s consistently (on a 2 GHz Quad-Core Intel Core i5 processor). The
found solution is (rounded to three digits)

R

L

0.033 0.000
0.044 0.033
0.049 0.077
0.066 0.049
0.000 0.066
0.000 0.000
0.133 0.000
0.075 0.117
0.057 0.042
0.033 0.024
0.000 0.000
0.000 0.000
0.000 0.000






















1
2
3
4
5
6
7
8
9
10
11
12
13

U
0.000
0.000
0.000
0.000
0.000
0.033
0.000
0.000
0.000
0.000
0.033
0.017
0.016

D
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000






















η∗ =

∈ ∆S×A ⊆ RS×A

and has the objective value (cid:104)r, η∗(cid:105)S×A = 0.033. The corresponding optimal state policy τ ∗ is
obtained simply by conditioning on states, and any pre-image under the observation kernel is an
optimal observation policy, in this case simply π∗ = β+τ ∗ which is (rounded to two digits)

R
1.00
0.53
0.48
0.00
0.00
0.00
0.00











o1
o2
o3
o4
o5
o6
o7

L
0.00
0.47
0.52
1.00
0.00
0.00
0.00

U
0.00
0.00
0.00
0.00
0.99
1.00
0.99

D
0.00
0.00
0.00
0.00
0.00
0.00
0.00











π∗ =

∈ ∆O
A.

This policy

1. moves right on observation 1 corresponding to the states 1 and 7,
2. moves right and left with probability close to 1/2 on observation 2 corresponding to states

2, 4, 9 and 10,

43

Published as a conference paper at ICLR 2022

3. moves right and left with probability close to 1/2 on observation 3 corresponding to the

states 3 and 8,

4. moves left on observation 4 corresponding to the state 5,
5. moves up on observation 5 corresponding to the states 6 and 12,
6. moves up on observation 6 corresponding to the states 11,
7. moves up on observation 7 corresponding to the state 13.

The action choices of the policy π∗ are also shown in Figure 7. Note that the policy ˆπ selects
the best action in the states 1, 5, 6, 11, 12 and 13. Those are the states that are either identiﬁable
from its observation (this is the case for 5, 11 and 13) or where the optimal actions of all states
leading to the same observation agree (this is the case for the pairs {1, 7} and {6, 12}). In the other
states, where the corresponding observation is ambiguous, the policy randomizes among the two
actions, which are optimal for the compatible states. This is for example the case for the states
2, 4, 9 and 10, which all lead to observation 2. The optimal MDP policy would move left in state
2 and 4 and move right in the states 9 and 10. The POMDP policy has to randomize between
moving left and right, since otherwise the agent could never reach the goal state if starting in 7 or
13. The same consideration applies to the states 3 and 8, which both lead to observation 3. The Julia
code is available in the supplements and under https://github.com/muellerjohannes/
geometry-POMDPs-ICLR-2022.

→ ↔ ↔ ↔ ←

↑

→ ↔ ↔ ↔ ↑

↑

↑

Figure 7: Depiction of the policy ˆπ, which is found using the polynomial programming formulation
of the POMDP and applying the interior point method Ipopt implemented in the Julia libraries JuMP
and Ipopt; an arrow into two direction indicates that the agent moves into those two directions with
probability close to 1/2.

F.3 A THREE DIMENSIONAL EXAMPLE

Let us now discuss an example where the set of discounted state-action distributions is three-
dimensional and not two-dimensional as before. For this, we consider a generalization of the previ-
ous example where |S| = 3, |A| = 2 and |O| = 3 such that

dim(∆S

A) = dim(∆O

A) = 3 · (2 − 1) = 3.

The observation mechanism used is

β =

(cid:32) 1
1/2
1/3

0
1/2
1/3

(cid:33)

0
0
1/3

.

Further, the action mechanism α and the initial distribution µ are sampled randomly and the used
discount factor is 0.5. Since the initial distribution is generic and β is invertible, the set of state-
action frequencies N µ
γ and the set of feasible state-action frequencies N µ,β
are three-dimensional
∼= [0, 1]3 (see
∼= ∆O
and are in fact combinatorially equivalent to the three-dimensional cube ∆S
A
A
Theorem 16). In Figure 8 we plot a random three-dimensional projection of the sets. More precisely,
we plot their one-dimnesional faces dashed and solid for the MDP and POMDP respectively. The
combinatorial equivalence to the three-dimensional cube can be see in this plot.

γ

44

Published as a conference paper at ICLR 2022

Figure 8: A random three-dimensional projection of the set of discounted state-action frequencies of
the MDP is the polytope deﬁned by the dashed straight edges. The same random three-dimensional
projection of the set of feasible discounted state-action frequencies is the basic semialgebraic set
where the edges are shown by the solid lines. Both of those sets are combinatorially equivalent to a
three dimensional cube.

45

