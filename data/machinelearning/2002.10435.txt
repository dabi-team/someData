0
2
0
2

n
u
J

7

]

G
L
.
s
c
[

2
v
5
3
4
0
1
.
2
0
0
2
:
v
i
X
r
a

Learning Structured Distributions From Untrusted Batches:
Faster and Simpler

Sitan Chen∗

Jerry Li†

Ankur Moitra‡

June 9, 2020

Abstract

We revisit the problem of learning from untrusted batches introduced by Qiao and Valiant
[QV17]. Recently, Jain and Orlitsky [JO19] gave a simple semideﬁnite programming approach
based on the cut-norm that achieves essentially information-theoretically optimal error in poly-
nomial time. Concurrently, Chen et al. [CLM19] considered a variant of the problem where µ is
assumed to be structured, e.g. log-concave, monotone hazard rate, t-modal, etc. In this case, it
is possible to achieve the same error with sample complexity sublinear in n, and they exhibited
a quasi-polynomial time algorithm for doing so using Haar wavelets.

In this paper, we ﬁnd an appealing way to synthesize [JO19] and [CLM19] to give the best
of both worlds: an algorithm which runs in polynomial time and can exploit structure in the
underlying distribution to achieve sublinear sample complexity. Along the way, we simplify the
approach of [JO19] by avoiding the need for SDP rounding and giving a more direct interpreta-
tion of it via soft ﬁltering, a powerful recent technique in high-dimensional robust estimation.
We validate the usefulness of our algorithms in preliminary experimental evaluations.

1

Introduction

In this paper, we consider the problem of learning structured distributions from untrusted batches.
This is a variant on the problem of learning from untrusted batches, as introduced in [QV17]. Here,
there is an unknown distribution µ over {1, . . . , n}, and we are given N batches of samples, each
of size k. A (1 − (cid:15))-fraction of these batches are “good,” and consist of k i.i.d. samples from some
distribution µi with distance at most ω from µ in total variation distance,1 but an (cid:15)-fraction of
these batches are “bad,” and can be adversarially corrupted. The goal then is to estimate µ in
total variation distance.

This problem models a situation where we get batches of data from many diﬀerent users, for
instance, in a crowdsourcing application. Each honest user provides a relatively small batch of
data, which is by itself insuﬃcient to learn a good model, and moreover, can come from slightly
diﬀerent distributions depending on the user, due to heterogeneity. At the same time, a non-trivial

∗EECS, Massachusetts Institute of Technology. Email: sitanc@mit.edu. This work was supported in part by a

Paul and Daisy Soros Fellowship, NSF CAREER Award CCF-1453261, and NSF Large CCF-1565235.

†Microsoft Research AI. Email: jerrl@microsoft.com.
‡Department of Mathematics, Massachusetts Institute of Technology. Email: moitra@mit.edu. This work was
supported in part by a Microsoft Trustworthy AI Grant, NSF CAREER Award CCF-1453261, NSF Large CCF-
1565235, a David and Lucile Packard Fellowship, an Alfred P. Sloan Fellowship and an ONR Young Investigator
Award.

1The total variation distance between two distributions µ, ν over a shared probability space Ω is deﬁned to be

supU ⊆Ω µ(U ) − ν(U ).

1

 
 
 
 
 
 
fraction of data can come from malicious users who wish to game our algorithm to their own ends.
The high level question is whether or not we can exploit the batch structure of our data to improve
the robustness of our estimator.

For this problem, there are three separate, but equally important, metrics under which we can

evaluate any estimator:

Robustness How accurately can we estimate µ in total variation distance?

Runtime Are there algorithms that run in polynomial time in all the relevant parameters?

Sample complexity How few samples do we need in order to estimate µ?

In the original paper, Qiao and Valiant [QV17] focus primarily on robustness. They give an
algorithm for learning general µ from untrusted batches that uses a polynomial number of samples,
and estimates µ to within

(cid:16)

O

ω + (cid:15)/

k

√

(cid:17)

in total variation distance, and they proved that this is the best possible up to constant factors.
However, their estimator runs in time 2n. Qiao and Valiant [QV17] also gave an nk time algorithm
based on low-rank tensor approximation, however their algorithm also needs nk samples.

A natural question is whether or not this robustness can be achieved eﬃciently. [CLM19] gave
an nlog 1/(cid:15) time algorithm with nlog 1/(cid:15) sample complexity for the general problem based on the
sum-of-squares hierarchy. It estimates µ to within

(cid:18)

O

ω +

(cid:15)
√
k

(cid:19)

(cid:112)log 1/(cid:15)

in total variation distance.
In concurrent and independent work Jain and Orlitsky [JO19] gave
a polynomial time algorithm based on a much simpler semideﬁnite program that estimates µ to
within the same total variation distance. Their approach was based on an elegant way to combine
approximation algorithms for the cut-norm [AN04] with the ﬁltering approach for robust estimation
[DKK+19, SCV18, DKK+17, DKK+18, DHL19].

To some extent, the results of [CLM19, JO19] also address the third consideration, sample
complexity. In particular, the estimator of [JO19] requires N = (cid:101)Ω(n/(cid:15)2) batches to achieve the
error rate mentioned above. Without any assumptions on the structure of µ, even in the case where
there are no corruptions, any algorithm must take at least Ω(n/(cid:15)2) batches of size k are required
in order to learn µ to within total variation distance O(ω + (cid:15)/
k). Thus, this sample complexity
is nearly-optimal for this problem, unless we make additional assumptions.

√

Unfortunately, in many cases, the domain size n can be very large, and a sample complexity
which strongly grows with n can render the estimator impractical. However in most applications,
we have prior knowledge about the shape of µ that could in principle be used to drastically reduce
the sample complexity. For example, if µ is log-concave, monotone or multimodal with a bounded
number of modes, it is known that µ can be approximated by a piecewise polynomial function and
when there are no corruptions, this meta structural property can be used to reduce the sample
complexity to logarithmic in the domain size [CDSS14b]. An appealing aspect of the relaxation
in [CLM19] was that it was possible to incorporate shape-constraints into the relaxation, through
the Haar wavelet basis, which allowed us to improve the sample complexity to quasipolynomial in
d and s, respectively the degree and number of parts in the piecewise polynomial approximation,
and quasipolylogarithmic in n. Unfortunately, while [JO19] achieves better runtime and sample
complexity in the unstructured setting, their techniques do not obviously extend to obtain a similar
sample complexity under structural assumptions.

2

This raises a natural question: can we build on [JO19] and [CLM19], to incorporate shape
constraints into a simple semideﬁnite programming approach, that can achieve nearly-optimal ro-
bustness, in polynomial runtime, and with sample complexity which is sublinear in n? In this
paper, we answer this question in the aﬃrmative:

Theorem 1.1 (Informal, see Theorem 4.1). Let µ be a distribution over [n] that is approximated
by an s-part piecewise polynomial function with degree at most d. Then there is a polynomial-time
algorithm which estimates µ to within

(cid:18)

O

ω +

(cid:15)
√
k

(cid:19)

(cid:112)log 1/(cid:15)

in total variation distance after drawing N (cid:15)-corrupted batches, each of size k, where

is the number of batches needed.

N = (cid:101)O (cid:0)(s2d2/(cid:15)2) · log3(n)(cid:1)

√

Any algorithm for learning structured distributions from untrusted batches must take at least
Ω(sd/(cid:15)2) batches to achieve error O(ω + (cid:15)/
k), and an interesting open question is whether there
is a polynomial time algorithm that achieves these bounds. For robustly estimating the mean of
a Gaussian in high-dimensions, there is evidence for a Ω((cid:112)log 1/(cid:15)) gap between the best possible
estimation error and what can be achieved by polynomial time algorithms [DKS17].
It seems
plausible that the Ω((cid:112)log 1/(cid:15)) gap between the best possible estimation error and what we achieve
is unavoidable in this setting as well.

1.1 High-Level Argument

[JO19] demonstrated how to learn general distributions from untrusted batches in polynomial time
using a ﬁltering algorithm similar to those found in [DKK+19, SCV18, DKK+17, DKK+18, DHL19],
and in [CLM19] it was shown how to learn structured distributions from untrusted batches in
quasipolynomial time using an SoS relaxation based on Haar wavelets.

In this work we show how to combine the ﬁltering framework of [JO19] with the Haar wavelet
technology of [CLM19] to obtain a polynomial-time, sample-eﬃcient algorithm for learning struc-
tured distributions from untrusted batches. In the discussion in this section, we will specialize to
the case of ω = 0 for the sake of clarity.

Learning via Filtering A useful ﬁrst observation is that the problem of learning from un-
trusted batches can be thought of as robust mean estimation of multinomial distributions in L1
distance: given a batch of samples Yi = (Y 1
i ) from a distribution µ over [n], the frequency
vector { 1
i = a]}a∈[n] is distributed according to the normalized multinomial distribution
k
Mulk(µ) given by k draws from µ. Note that µ is precisely the mean of Mulk(µ), so the problem
of estimating µ from an (cid:15)-corrupted set of N frequency vectors is equivalent to that of robustly
estimating the mean of a multinomial distribution.

i , ..., Y k

1[Y j

(cid:80)k

j=1

As such, it is natural to try to adapt the existing algorithms for robust mean estimation of
other distributions; the fastest of these are based on a simple ﬁltering approach which works as
follows. We maintain weights for each point, initialized to uniform. At every step, we measure the
maximum “skew” of the weighted dataset in any direction, and if this skew is still too high, update
the weights by

1. Finding the direction v in which the corruptions “skew” the dataset the most.

3

2. Giving a “score” to each point based on how badly it skews the dataset in the direction v

3. Downweighting or removing points with high scores.

Otherwise, if the skew is low, output the empirical mean of the weighted dataset.

To prove correctness of this procedure, one must show three things for the particular skewness

measure and score function chosen:

• Regularity: For any suﬃciently large collection of (cid:15)-corrupted samples, a particular deter-

ministic regularity condition holds (Deﬁnition 4.3 and Lemma 4.6)

• Soundness: Under the regularity condition, if the skew of the weighted dataset is small, then
the empirical mean of the weighted dataset is suﬃciently close to the true mean (Lemma 4.7).

• Progress: Under the regularity condition, if the skew of the weighted dataset is large, then
one iteration of the above update scheme will remove more weight from the bad samples than
from the good samples (Lemma 4.10).

For isotropic Gaussians, skewness is just given by the maximum variance of the weighted dataset
in any direction, i.e. maxv∈Sn−1(cid:104)vv(cid:62), ˜Σ(cid:105) where ˜Σ is the empirical covariance of the weighted dataset.
Given maximizing v, the “score” of a point X is then simply its contribution to the skewness.

To learn in L1 distance, the right set of test vectors v to use is the Hamming cube {0, 1}n, so a
natural attempt at adapting the above skewness measure to robust mean estimation of multinomials
is to consider the quantity maxv∈{0,1}n(cid:104)vv(cid:62), ˜Σ(cid:105). But one of the key challenges in passing from
isotropic Gaussians to multinomial distributions is that this quantity above is not very informative
because we do not have a good handle on the covariance of Mulk(µ). In particular, it could be that
for a direction v, (cid:104)vv(cid:62), ˜Σ(cid:105) is high simply because the good points have high variance to begin with.

The Jain-Orlitsky Correction Term The clever workaround of [JO19] was to observe that we
know exactly what the projection of a multinomial distribution Mulk(µ) in any {0, 1}n direction
v is, namely Bin(k, (cid:104)v, µ(cid:105)). And so to discern whether the corrupted points skew our estimate
in a given direction v, one should measure not the variance in the direction v, but rather the
following corrected quantity: the variance in the direction v, minus what the variance would be if
the distribution of the projections in the v direction were actually given by Bin(k, (cid:104)v, ˜µ(cid:105)), where ˜µ
is the empirical mean of the weighted dataset. This new skewness measure can be written as

max
v∈{0,1}n

(cid:26)

(cid:104)vv(cid:62), ˜Σ(cid:105) −

1
k

((cid:104)v, ˜µ(cid:105) − (cid:104)v, ˜µ(cid:105)2)

(cid:27)

.

(1)

Finding the direction v ∈ {0, 1}n which maximizes this corrected quantity is some Boolean quadratic
programming problem which can be solved approximately by solving the natural SDP relaxation
and rounding to a Boolean vector v using the machinery of [AN04]. Using this approach, [JO19]
obtained a polynomial-time algorithm for learning general discrete distributions from untrusted
batches.

Learning Structured Distributions
[CLM19] introduced the question of learning from un-
trusted batches when the distribution is known to be structured. Learning structured distributions
in the classical sense is well-understood: if a distribution µ is η-close in total variation distance to
being s-piecewise degree-d, then to estimate µ in total variation distance it is enough to approx-
imate µ in a much weaker norm which we will denote by (cid:107) · (cid:107)AK , where K is a parameter that
depends on s and d. We review the details for this in Section 2.4.

4

[CLM19] gave a sum-of-squares algorithm for robust mean estimation in the AK norm that
(cid:112)log 1/(cid:15) error in quasipolynomial time, and a natural open question was to achieve

achieved (cid:15)√
k
this with a polynomial-time algorithm.

The key challenge that [CLM19] had to address was that unlike the Hamming cube or Sn−1, it is
unclear how to optimize over the set of test vectors dual to the AK norm. Combinatorially, this set
is easy to characterize: (cid:107)µ − ˆµ(cid:107)AK is small if and only if (cid:104)µ − ˆµ, v(cid:105) is small for all v ∈ V n
2K ⊂ {±1}n,
where V n
2K is the set of all v ∈ {±1}n with at most 2K sign changes when read as a vector from
left to right (for example, (1, 1, −1, −1, 1, 1, 1) ∈ V 7

2 ).
The main observation in [CLM19] is that vectors with few sign changes admit sparse representa-
tions in the Haar wavelet basis, so instead of working with V n
2K, one can simply work with a convex
relaxation of this Haar-sparsity constraint. As such, if we let K ⊆ Rn×n denote the relaxation of
the set of {vv(cid:62)|v ∈ V n
2K} to all matrices Σ whose Haar transforms are “analytically sparse” in some
appropriate, convex sense (see Section 3 for a formal deﬁnition), then as this set of test matrices
contains the set of test matrices vv(cid:62) for v ∈ V n
2K, it is enough to learn µ in the norm associated to
K, which is strictly stronger than the AK norm.2

Our goal then is to produce ˆµ for which (cid:107)ˆµ − µ(cid:107)K (cid:44) supΣ∈K(cid:104)Σ, (ˆµ − µ)⊗2(cid:105)1/2 is small. And
even though (cid:107) · (cid:107)K is a stronger norm, it turns out that the metric entropy of K is still small enough
that one can get good sample complexity guarantees. Indeed, showing that this is the case (see
Lemma A.1) was where the bulk of the technical machinery of [CLM19] went, and as we elaborate
on in Appendix B, the analysis there left some room for tightening. In this work, we give a reﬁned
analysis of K which allows us to get nearly tight sample complexity bounds.

Putting Everything Together Almost all of the pieces are in place to instantiate the ﬁltering
framework:
in lieu of the quantity in (1), which can be phrased as the maximization of some
quadratic (cid:104)vv(cid:62), M (w)(cid:105) over {±1}n, where M (w) ∈ Rn×n depends on the dataset and the weights
w on its points,3 we can deﬁne our skewness measure as maxΣ∈K(cid:104)Σ, M (w)(cid:105) = (cid:107)M (w)(cid:107)K, and we
can deﬁne the score for each point in the dataset to be its contribution to the skewness measure
(see Section 4.2).

At this point the reader may be wondering why we never round Σ to an actual vector v ∈ V n
2K
before computing skewness and scores. As our subsequent analysis will show, it turns out that
rounding is unnecessary, both in our setting and even in the unstructured distribution setting
considered in [JO19]. Indeed, if one examines the three proof ingredients of regularity, soundness,
and progress that we enumerated above, it becomes evident that the ﬁltering framework for robust
mean estimation does not actually require ﬁnding a concrete direction in Rn in which to ﬁlter,
merely a skewness measure and score functions which are amenable to showing the above three
statements. That said, as we will see, it becomes more technically challenging to prove these
ingredients when Σ is not rounded to an actual direction (see e.g. the discussion after Lemmas A.2
and A.3 in Appendix A), though nevertheless possible. We hope that this observation will prove
useful in future applications of ﬁltering.

2Note that in [CLM19], because moment bounds beyond degree 2 were used, they also needed to use higher-order

tensor analogues of K, but in this work it will suﬃce to work with degree 2.

3Note that we have switched to {±1}n in place of {0, 1}n. We do not belabor this point here, as the diﬀerence
2K , which is a

turns out to be immaterial, and the former is more convenient for understanding how we handle V n
subset of {±1}n.

5

1.2 Related Work

The problem of learning from untrusted batches was introduced by [QV17], and is motivated by
problems in reliable distributed learning and federated learning [MMR+17, KMY+16]. The general
question of learning from batches has been considered in a number of settings [LRR13, TKV17] in
the theoretical computer science community, but these algorithms do not work in the presence of
adversarial noise.

The study of univariate shape constrained density estimation has a long history in statistics and
computer science, and we cannot hope to do justice to it here. See [BBBB72] for a survey of classical
results in the area, and [O’B16, Dia16] for a survey of more recent results in this area. Of particular
relevance to us are the techniques based on the classical piecewise polynomial (or spline) methods,
see e.g. [WW83, Sto94, SHKT97, WN07]. Recent work, which we build oﬀ of, demonstrates that
this framework is capable of achieving nearly-optimal sample complexity and runtime, for a large
class of structured distributions [CDSS13, CDSS14b, CDSS14a, ADH+15, ADLS17].

Our techniques are also related to a recent line of work on robust statistics [DKK+19, LRV16,
CSV17, DKK+17, HL18, KSS18], a classical problem dating back to the 60s and 70s [Ans60, Tuk60,
Hub92, Tuk75]. See [Li18, Ste18, DK19] for a more comprehensive survey of this line of work.

Finally, the most relevant papers to our result are [CLM19, JO19], which improve upon the
result of [QV17] in terms of runtime and sample complexity. As mentioned above, our result can be
thought of as a way to combine the improved ﬁltering algorithm of [JO19] and the shape-constrained
technology introduced in [CLM19].

Concurrently and independently of this work, a newer work of Jain and Orlitsky [JO20] obtains
very similar results, though our quantitative guarantees are incomparable: the number of batches
N they need scales linearly in s · d and independently of n, but also scales with

k and 1/(cid:15)3.

√

Roadmap In Section 2, we overview notation, formally deﬁne our generative model, give miscel-
laneous technical tools, and review the basics on classical learning of structured distributions and on
Haar wavelets. In Section 3, we deﬁne the semideﬁnite program that we use to compute skewness.
In Section 4, we give our algorithm LearnWithFilter and prove our main result, Theorem 1.1.
In Section 5, we describe our empirical evaluations of LearnWithFilter on synthetic data. In
Appendices A, B, and C, we complete the proofs of some deferred technical statements relating to
deterministic regularity conditions and metric entropy bounds.

2 Technical Preliminaries

2.1 Notation

• Given p ∈ [0, 1], let Bin(k, p) denote the normalized binomial distribution, which takes values in

{0, 1/k, · · · , 1} rather than {0, 1, · · · , k}.

• Let ∆n ⊂ Rn be the simplex of nonnegative vectors whose coordinates sum to 1. Any p ∈ ∆n

naturally corresponds to a probability distribution over [n].

• Let 1n ∈ Rn denote the all-ones vector. We omit the subscript when the context is clear.

• Given matrix M ∈ Rn×n, let (cid:107)M (cid:107)max denote the maximum absolute value of any entry in M ,
let (cid:107)M (cid:107)1,1 denote the absolute sum of its entries, and let (cid:107)M (cid:107)F denote its Frobenius norm.

6

• Given µ ∈ ∆n, let Mulk(µ) denote the distribution over ∆n given by sampling a frequency vector
from the multinomial distribution arising from k draws from the distribution over [n] speciﬁed
by µ, and dividing by k.

• Given samples X1, · · · , XN ∼ Mulk(µ) and U ⊆ [N ], deﬁne w(U ) : [N ] → [0, 1/N ] to be the
set of weights which assigns 1/N to all points in U and 0 to all other points. Also deﬁne its
normalization ˆw(U ) (cid:44) w(U )/(cid:107)w(cid:107)1. Let W(cid:15) denote the set of weights w : [N ] → [0, 1/N ] which are
convex combinations of such weights for |U | ≥ (1 − (cid:15))N . Given w, deﬁne µ(w) (cid:44) (cid:80)N
Xi,
i=1
and deﬁne µ(U ) (cid:44) µ(w(U )), that is, the empirical mean of the samples indexed by U .

wi
(cid:107)w(cid:107)1

• Given samples X1, · · · , XN ∼ Mulk(µ), weights w, and ν1, ..., νN ∈ ∆n, deﬁne the matrices

A(w, {νi}) =

N
(cid:88)

i=1

wi(Xi − νi)⊗2

and

B({νi}) =

1
N

N
(cid:88)

i=1

E
X∼Mulk(νi)

[(X − νi)⊗2].

When ν1 = · · · = νN = ν, denote these matrices by A(w, ν) and B(ν) and note that

B(ν) =

1
k

(cid:0)diag(ν) − ν⊗2(cid:1) .

(2)

Also deﬁne M (w, {νi})) (cid:44) A(w, {νi}) − B({νi}) and M (w, ν) (cid:44) A(w, ν) − B(ν). We will also
denote M (w, µ(w)) by M (w) and M ( ˆw(U )) by MU .
To get intuition for these deﬁnitions, note that any bitstring v ∈ {0, 1}n corresponding to
S ⊆ [n] induces a normalized binomial distribution Y (cid:44) Bin(n, (cid:104)µ, v(cid:105)) ∈ [0, 1], and any sam-
ple Xi ∼ Mulk(µ) induces a corresponding sample (cid:104)Xi, v(cid:105) from Y . Then (cid:104)vv(cid:62), MU (cid:105) is the
diﬀerence between the empirical variance of Y and the variance of the binomial distribution
Bin(n, (cid:104)µ(U ), v(cid:105)).

2.2 The Generative Model

Throughout the rest of the paper, let (cid:15), ω > 0, n, k, N ∈ N , and let µ be some probability distri-
bution over [n].

Deﬁnition 2.1. We say Y1, ..., YN is an (cid:15)-corrupted ω-diverse set of N batches of size k from µ if
they are generated via the following process:

• For every i ∈ [(1 − (cid:15))N ], ˜Yi = ( ˜Y 1

i , ..., ˜Y k

i ) is a set of k iid draws from µi, where µi ∈ ∆n is

some probability distribution over [n] for which dTV(µ, µi) ≤ ω.

• A computationally unbounded adversary inspects ˜Y1, ..., ˜Y(1−(cid:15))N and adds (cid:15)N arbitrarily chosen
tuples ˜Y(1−(cid:15))N +1, ..., ˜YN ∈ [n]k, and returns the entire collection of tuples in any arbitrary
order as Y1, ..., YN .

Let SG, SB ⊂ [N ] denote the indices of the uncorrupted (good) and corrupted (bad) batches.

It turns out that we might as well treat each Yi as an unordered tuple. That is, for any Yi,
deﬁne Xi ∈ ∆n to be the vector of frequencies whose a-th entry is 1
i = a] for all a ∈ [n].
k
Then for each, i ∈ SG, Xi is an independent draw from Mulk(µi). Henceforth, we will work solely
in this frequency vector perspective.

1[Y j

(cid:80)k

j=1

7

2.3 Elementary Facts

In this section we collect miscellaneous elementary facts that will be useful in subsequent sections.

Fact 2.2. For X1, · · · , Xm ∈ Rn, weights w : [m] → R≥0, v ∈ Rn, µ ∈ Rn, and Σ ∈ Rn×n
symmetric,

(cid:88)

(cid:10)(Xi − µ)⊗2, Σ(cid:11) =

wi

(cid:88)

(cid:10)(Xi − µ(w))⊗2, Σ(cid:11) + (cid:107)w(cid:107)1 · (cid:10)(µ(w) − µ)⊗2, Σ(cid:11) .

wi

(3)

In particular, by taking Σ = vv(cid:62) for any v ∈ Rn,

(cid:88)

wi(cid:104)Xi − µ, v(cid:105)2 =

(cid:88)

wi(cid:104)Xi − µ(w), v(cid:105)2 + (cid:107)w(cid:107)1 · (cid:104)µ(w) − µ, v(cid:105)2.

That is, the function ν (cid:55)→ (cid:80)

i wi(cid:104)Xi − ν, v(cid:105)2 is minimized over ν ∈ Rn by ν = µ(w).

Proof. Without loss of generality we may assume (cid:107)w(cid:107)1 = 1. Using the fact that (cid:104)u⊗2, Σ(cid:105) −
(cid:104)v⊗2, Σ(cid:105) = (u − v)(cid:62)Σ(u + v) for symmetric Σ, we see that

(cid:10)(Xi − µ⊗2 − (Xi − µ(w))⊗2, Σ(cid:11) = (µ(w) − µ)(cid:62)Σ(2Xi − µ − µ(w)).

Because (cid:80) wiXi = µ(w), we see that

(cid:88)

wi(µ(w) − µ)(cid:62)Σ(2Xi − µ − µ(w)) = (cid:10)(µ(w) − µ)⊗2, Σ(cid:11) ,

from which (3) follows. The remaining parts of the claim follow trivially.

Fact 2.3. For any 0 < (cid:15) < 1, let weights w : [N ] → [0, 1/N ] satisfy (cid:80)
is the set of weights deﬁned by w(cid:48)
then we have that (cid:107)µ(w) − µ(w(cid:48))(cid:107)1 ≤ O((cid:15)).

i = wi for i ∈ SG and w(cid:48)

i∈[N ] wi ≥ 1 − O((cid:15)). If w(cid:48)
i = 0 otherwise, and if |SG| ≥ (1 − (cid:15))N ,

Proof. We may write

(cid:107)µ(w) − µ(w(cid:48))(cid:107)1 ≤ (cid:107)

1
(cid:107)w(cid:107)1

≤ O((cid:15)) +

(cid:88)

wiXi(cid:107)1 +

i∈SB
(cid:18) 1

(cid:107)w(cid:107)1

−

1
(cid:107)w(cid:48)(cid:107)1

(cid:107)w(cid:107)1
(cid:19)

(cid:88)

(cid:107)

i∈SG

(cid:18) 1

−

1
(cid:107)w(cid:48)(cid:107)1

(cid:19)

(cid:107)

(cid:88)

i∈SG

wiXi(cid:107)1

wiXi(cid:107)1 ≤ O((cid:15)),

where the ﬁrst step follows by deﬁnition of µ(·) and by triangle inequality, the second step follows by
(cid:12)
the fact that |SB| ≤ (cid:15)N , and the third step follows by the fact that |(cid:107)w(cid:107)1−(cid:107)w(cid:48)(cid:107)1| =
(cid:12)
(cid:12) ≤ (cid:15),
while (cid:107) (cid:80)

wiXi(cid:107)1 ≤ 1 as the samples Xi lie in ∆n.

(cid:12)
(cid:80)
(cid:12)
(cid:12)

i∈SB

wi

i∈SG

It will be useful to have a basic bound on the Frobenius norm of M (w, ν).

Lemma 2.4. For any ν ∈ ∆n and any weights w for which (cid:80) wi = 1, we have that (cid:107)M (w, ν)(cid:107)F ≤ 3.

Proof. For any sample X ∈ ∆n, we have that

(cid:107)(X − ν)(X − ν)(cid:62)(cid:107)F ≤ (cid:107)X − ν(cid:107)2

2 ≤ 2

and

1
k
from which the lemma follows by triangle inequality and the assumption that (cid:80) wi = 1.

(cid:107)B(ν)(cid:107)F ≤

2 ≤ 2/k,

(cid:107)ν(cid:107)2 +

(cid:107)ν(cid:107)2

1
k

8

2.4 AK Norms and VC Complexity

In this section we review basics about learning distributions which are close to piecewise polynomial.

Deﬁnition 2.5 (AK norms, see e.g. [DL01]). For positive integers K ≤ n, deﬁne AK to be the set
of all unions of at most K disjoint intervals over [n], where an interval is any subset of [n] of the
form {a, a + 1, · · · , b − 1, b}. The AK distance between two distributions µ, ν over [n] is

(cid:107)µ − ν(cid:107)AK = max
S∈AK

|µ(S) − µ(S)|.

Equivalently, say that v ∈ {±1}n has 2K sign changes if there are exactly 2K indices i ∈ [n − 1]
for which vi+1 (cid:54)= vi. Then if V n

2K denotes the set of all such v, we have

(cid:107)µ − ν(cid:107)AK =

1
2

max
v∈V n
2K

(cid:104)µ − ν, v(cid:105).

Note that

(cid:107) · (cid:107)A1 ≤ (cid:107) · (cid:107)A2 ≤ · · · ≤ (cid:107) · (cid:107)An/2 = (cid:107) · (cid:107)TV.

Deﬁnition 2.6. We say that a distribution µ over [n] is (η, s)-piecewise degree-d if there is a
partition of [n] into t disjoint intervals {[ai, bi]}1≤i≤t, together with univariate degree-d polynomials
r1, · · · , rt and a distribution µ(cid:48) on [n], such that dTV(µ, µ(cid:48)) ≤ η and such that for all i ∈ [t],
µ(cid:48)(x) = ri(x) for all x ∈ [n] in [ai, bi].

A proof of the following lemma, a consequence of [ADLS17], can be found in [CLM19].

Lemma 2.7 (Lemma 5.1 in [CLM19], follows by [ADLS17]). Let K = s(d + 1). If µ is (η, s)-
piecewise degree-d and (cid:107)µ − ˆµ(cid:107)AK ≤ ζ, then there is an algorithm which, given the vector ˆµ,
outputs a distribution µ∗ for which dTV(µ, µ∗) ≤ 2ζ + 4η in time poly(s, d, 1/η).

Henceforth, we will focus solely on the problem of learning in A(cid:96) norm, where

(cid:96) (cid:44) 2s(d + 1).

(4)

2.5 Haar Wavelets

We brieﬂy recall the deﬁnition of Haar wavelets, further details and examples of which can be found
in [CLM19].

Deﬁnition 2.8. Let m be a positive integer and let n = 2m. The Haar wavelet basis is an
orthonormal basis over Rn consisting of the father wavelet ψ0father,0 = n−1/2 · 1, the mother wavelet
ψ0mother,0 = n−1/2 · (1, · · · , 1, −1, · · · , −1) (where (1, · · · , 1, −1, · · · , −1) contains n/2 1’s and n/2
-1’s), and for every i, j for which 1 ≤ i < m and 0 ≤ j < 2i, the wavelet ψi,j whose 2m−i · j +
1, · · · , 2m−i · j + 2m−i−1-th coordinates are 2−(m−i)/2 and whose 2m−i · j + (2m−i−1 + 1), · · · , 2m−i ·
j + 2m−i-th coordinates are −2−(m−i)/2, and whose remaining coordinates are 0.

Additionally, we will use the following notation when referring to Haar wavelets:

• Let Hm denote the n × n matrix whose rows consist of the vectors of the Haar wavelet basis
for Rn. When the context is clear, we will omit the subscript and refer to this matrix as H.

• For ν ∈ [n], if the ν-th element of the Haar wavelet basis for Rn is some ψi,j, then deﬁne the

weight h(ν) (cid:44) 2−(m−i)/2.

9

• For any index i ∈ {0father, 0mother, 1, · · · , m − 1}, let Ti ⊂ [n] denote the set of indices ν for

which the ν-th Haar wavelet is of the form ψi,j for some j.

• Given any p ≥ 1, deﬁne the Haar-weighted Lp norm (cid:107)·(cid:107)p;h on Rn by (cid:107)w(cid:107)p;h (cid:44) (cid:107)w(cid:48)(cid:107)p, where for
(cid:44) h(a)wa. Likewise, given any norm (cid:107)·(cid:107)∗ on Rn×n, deﬁne the Haar-weighted
(cid:44) h(a)h(b)Ma,b.

every a ∈ [n], w(cid:48)
a
∗-norm (cid:107) · (cid:107)∗;h on Rn×n by (cid:107)M(cid:107)∗;h (cid:44) (cid:107)M(cid:48)(cid:107)∗, where for every a, b ∈ [n], M(cid:48)

a,b

The key observation is that any v ∈ {±1}n with at most (cid:96) sign changes, where (cid:96) is given by
(4), has an ((cid:96) log n + 1)-sparse representation in the Haar wavelet basis. We will use the following
fundamental fact about Haar wavelets, part of which appears as Lemma 6.3 in [CLM19].

Lemma 2.9. Let v ∈ {±1}n have at most (cid:96) sign changes. Then Hv has at most (cid:96) log n + 1 nonzero
entries, and furthermore (cid:107)Hv(cid:107)∞;h ≤ 1. In particular, (cid:107)Hv(cid:107)2

2;h, (cid:107)Hv(cid:107)1;h ≤ (cid:96) log n + 1.

Proof. We ﬁrst show that Hv has at most (cid:96) log n + 1 nonzero entries. For any ψi,j with nonzero
entries at indices [a, b] ⊂ [n] and such that i (cid:54)= 0father, if v has no sign change in the interval [a, b],
then (cid:104)ψi,j, v(cid:105) = 0. For every index ν ∈ [n] at which v has a sign change, there are at most m = log n
choices of i, j for which ψi,j has a nonzero entry at index ν, from which the claim follows by a union
bound over all (cid:96) choices of ν, together with the fact that (cid:104)ψ0father,0, v(cid:105) may be nonzero.

Now for each (i, j) for which (cid:104)ψi,j, v(cid:105) (cid:54)= 0, note that

2−(m−i)/2 · |(cid:104)ψi,j, v(cid:105)| ≤ 2−(m−i)/2 ·

(cid:16)

2−(m−i)/2 · 2m−i(cid:17)

= 1,

as claimed. The bounds on (cid:107)Hv(cid:107)1;h, (cid:107)Hv(cid:107)2

2;h follow immediately.

3 SDP for Finding the Direction of Largest Variance

Recall that in [JO19], the authors consider the binary optimization problem maxv∈{0,1}n |v(cid:62)MU v|.
|v(cid:62)MU v|. Motivated by [CLM19]
We would like to approximate the optimization problem maxv∈V n
and Lemma 2.9, we consider the following convex relaxation:

(cid:96)

Deﬁnition 3.1. Let (cid:96) be given by (4). Let K denote the (convex) set of all matrices Σ ∈ Rn×n for
which

1. (cid:107)Σ(cid:107)max ≤ 1.

2. (cid:107)HΣH (cid:62)(cid:107)1,1;h ≤ (cid:96) log n + 1.

3. (cid:107)HΣH (cid:62)(cid:107)2

F ;h ≤ (cid:96) log n + 1.

4. (cid:107)HΣH (cid:62)(cid:107)max;h ≤ 1.

5. Σ (cid:23) 0.

Let (cid:107) · (cid:107)K denote the associated norm given by (cid:107)M(cid:107)K (cid:44) supΣ∈K |(cid:104)M, Σ(cid:105)|. By abuse of notation, for
vectors v ∈ Rn we will also use (cid:107)v(cid:107)K to denote (cid:107)vv(cid:62)(cid:107)1/2
K .

Because K has an eﬃcient separation oracle, one can compute (cid:107) · (cid:107)K in polynomial time.

Remark 3.2. Note that, besides not being a sum-of-squares program like the one considered in
[CLM19], this relaxation is also slightly diﬀerent because of Constraints 3 and 4. As we will see in
Section B, these additional constraints will be crucial for getting reﬁned sample complexity bounds.

10

Note that Lemma 2.9 immediately implies that K is a relaxation of V n
(cid:96) :

Corollary 3.3 (Corollary of Lemma 2.9). vv(cid:62) ∈ K for any v ∈ V n
(cid:96) .

Note also that Constraint 1 in Deﬁnition 3.1 ensures that (cid:107) · (cid:107)K is weaker than (cid:107) · (cid:107)1 and more

generally that:

Fact 3.4. For any a, b ∈ Rn and Σ ∈ K, a(cid:62) · Σ · b ≤ (cid:107)a(cid:107)1 · (cid:107)b(cid:107)1. In particular, for any v ∈ Rn,
(cid:107)v(cid:107)K ≤ (cid:107)v(cid:107)1.

As a consequence, we conclude the following useful fact about stability of the B(·) matrix.

Corollary 3.5. For any µ, µ(cid:48) ∈ ∆n, (cid:107)B(µ) − B(µ(cid:48))(cid:107)K ≤ 3

k (cid:107)µ − µ(cid:48)(cid:107)1.

Proof. Take any Σ ∈ K. By symmetry, it is enough to show that (cid:104)B(µ) − B(µ(cid:48)), Σ(cid:105) ≤ 3
By Constraint 1, we have that (cid:104)µ − µ(cid:48), diag(Σ)(cid:105) ≤ (cid:107)µ − µ(cid:48)(cid:107)1. On the other hand, note that

k (cid:107)µ − µ(cid:48)(cid:107)1.

µ(cid:48)(cid:62)Σµ(cid:48) − µ(cid:62)Σµ = (µ(cid:48) − µ)(cid:62)Σ(µ(cid:48) + µ) ≤ (cid:107)µ(cid:48) − µ(cid:107)1 · (cid:107)µ(cid:48) + µ(cid:107)1 ≤ 2(cid:107)µ(cid:48) − µ(cid:107)1,

where the second step follows from Fact 3.4. The corollary now follows.

Note that if the solution to the convex program argmaxΣ∈K(cid:104)MU , Σ(cid:105) were actually integral, that
is, some rank-1 matrix vv(cid:62) for v ∈ V n
(cid:96) , it would correspond to the direction v in which the samples
in U have the largest discrepancy between the empirical variance and the variance predicted by the
empirical mean. Then v would correspond to a subset of the domain [s] on which one could ﬁlter
out bad points as in [JO19]. In the sequel, we will show that this kind of analysis applies even if
the solution to argmaxΣ∈K(cid:104)MU , Σ(cid:105) is not integral.

4 Filtering Algorithm and Analysis

In this section we prove our main theorem, stated formally below:

Theorem 4.1. Let µ be an (η, s)-piecewise degree-d distribution over [n]. Then for any 0 < (cid:15) <
1/2 smaller than some absolute constant, and any 0 < δ < 1, there is a poly(n, k, 1/(cid:15), 1/δ)-time
algorithm LearnWithFilter which, given

N = (cid:101)O (cid:0)log(1/δ)(s2d2/(cid:15)2) log3(n)(cid:1) ,

(cid:15)-corrupted, ω-diverse batches of size k from µ, outputs an estimate ˆµ such that (cid:107)ˆµ − µ(cid:107)1 ≤

(cid:18)

O

η + ω +

√
(cid:15)

(cid:19)

log 1/(cid:15)
√
k

with probability at least 1 − δ over the samples.

In Section 4.1, we ﬁrst describe and prove guarantees for a basic but important subroutine,
1DFilter, of our algorithm. In Section 4.2, we describe our learning algorithm, LearnWithFil-
ter, in full. In Section 4.3 we deﬁne the deterministic conditions that the dataset must satisfy
for LearnWithFilter to succeed, deferring the proof that these deterministic conditions hold
with high probability (Lemma 4.6) to Appendix A. In Section 4.4 we prove a key geometric lemma
(Lemma 4.7). Finally, in Section 4.5, we complete the proof of correctness of LearnWithFilter.

11

Algorithm 1: 1DFilter(τ, w)

Input: Scores τ : [N ] → R≥0, weights w : [N ] → R≥0
Output: New weights w(cid:48) with even less mass on bad points than good points (see

Lemma 4.2)

(cid:16)

1 τmax ← maxi:wi>0 τi
2 w(cid:48)
i ←
3 Output w(cid:48)

1 − τi
τmax

(cid:17)

wi for all i ∈ [N ]

4.1 Univariate Filter

In this section, we deﬁne and analyze a simple deterministic subroutine 1DFilter which takes as
input a set of weights w and a set of scores on the batches X1, · · · , XN , and outputs a new set of
weights w(cid:48) such that, if the weighted average of the scores among the bad batches exceeds that of
the scores among the good batches, then w(cid:48) places even less weight relatively on the bad batches
than does w. This subroutine is given in Algorithm 1 below.

Lemma 4.2. Let τ : [N ] → R≥0 be a set of scores, and let w : [N ] → R≥0 be a weight. Given a
partition [N ] = SG (cid:116) SB for which

(cid:88)

i∈SG

wiτi <

(cid:88)

i∈SB

wiτi,

then the output w(cid:48) of 1DFilter(τ, w) satisﬁes (a) w(cid:48)
a strict subset of the support of w, and (c) (cid:80)

i ≤ wi for all i ∈ [N ], (b) the support of w(cid:48) is
wi − w(cid:48)
i.

i < (cid:80)

wi − w(cid:48)

i∈SB

i∈SG

Proof. (a) and (b) are immediate. For (c), note that

wi − w(cid:48)

i =

(cid:88)

i∈SG

1
τmax

(cid:88)

i∈SG

τiwi <

1
τmax

(cid:88)

i∈SB

τiwi =

(cid:88)

i∈SB

wi − w(cid:48)
i,

from which the lemma follows.

We note that this kind of downweighting scheme and its analysis are not new, see e.g. Lemma

4.5 from [CSV17] or Lemma 17 from [SCV18].

4.2 Algorithm Speciﬁcation

We can now describe our algorithm LearnWithFilter. At a high level, we maintain weights
w : [N ] → R≥0 for each of the batches.
In every iteration, we compute Σ ∈ K maximizing
|(cid:104)M (w), Σ(cid:105)|. If |(cid:104)M (w), Σ(cid:105)| ≤ O (cid:0) (cid:15)
k log 1/(cid:15)(cid:1), then output µ(w). Otherwise, update the weights as
follows: for every batch Xi, compute the score τi given by

τi (cid:44) (cid:10)(Xi − µ(w))⊗2, Σ(cid:11) ,

(5)

and set the weights to be the output of 1DFilter(τ, w). The pseudocode for LearnWithFilter
is given in Algorithm 2 below.

12

Algorithm 2: LearnWithFilter({Xi}i∈[N ], (cid:15))

Input: Frequency vectors X1, · · · , XN coming from an (cid:15)-corrupted, ω-diverse set of batches

from µ, where µ is (η, s)-piecewise, degree d

(cid:18)

√
(cid:15)

log 1/(cid:15)
√
k

(cid:19)

, provided uncorrupted samples

Output: ˆµ such that (cid:107)ˆµ − µ(cid:107)1 ≤ O

η + ω +

(cid:15)-good

1 w ← w([N ])
2 while (cid:107)M (w)(cid:107)K ≥ Ω(ω + (cid:15)
3

k log 1/(cid:15)) do

Σ ← argmaxΣ(cid:48)∈K|(cid:104)M (w), Σ(cid:105)|
Compute scores τ : [N ] → R≥0 according to (5).
w ←1DFilter(τ, w)

4

5

6 Using the algorithm of [ADLS17] (see Lemma 2.7), output the s-piecewise, degree-d

distribution ˆw minimizing (cid:107)µ(w) − ˆµ(cid:107)s(d+1) (up to additive error η).

4.3 Deterministic Condition

Deﬁnition 4.3 ((cid:15)-goodness). Take a set of points U ⊂ [N ], and let {µi}i∈U be a collection of
distributions over [n]. For any W ⊆ U , deﬁne µW

i∈W µi. Denote µ (cid:44) µU .

(cid:80)

(cid:44) 1
|W |

We say U is (cid:15)-good if it satisﬁes that for all W ⊂ U for which |W | = (cid:15)|U |,

(I) (Concentration of mean)

(cid:107)µ(U ) − µ(cid:107)K ≤ O

(cid:33)

(cid:32)

(cid:15)(cid:112)log 1/(cid:15)
√
k

and

(cid:107)µ(W ) − µW (cid:107)K ≤ O

(cid:33)

(cid:32) (cid:112)log 1/(cid:15)
√

k

(II) (Concentration of covariance)

(cid:107)M ( ˆw(U ), {µi}i∈U )(cid:107)K ≤ O

(cid:19)

(cid:18) (cid:15) log 1/(cid:15)
k

and

(cid:107)A( ˆw(W ), {µi}i∈W (cid:107)K ≤ O

(cid:19)

(cid:18) log 1/(cid:15)
k

(III) (Concentration of variance proxy)

(cid:107)B(ˆµ(U )) − B({µi}i∈U )(cid:107)K ≤ O(ω2/k + (cid:15)/k)

(IV) (Heterogeneity has negligible eﬀect, see Lemma 4.4)

(cid:40)

(cid:40)

sup
Σ∈K

sup
Σ∈K

1
|U |

(cid:88)

i∈U

1
|W |

(cid:88)

i∈W

(cid:41)

(cid:32)

≤ O

ω ·

(cid:41)

(cid:32)

(cid:33)

(cid:15)(cid:112)log 1/(cid:15)
√
k

(cid:33)

(cid:112)log 1/(cid:15)
√
k

.

.

(µi − µ)(cid:62) · Σ · (Xi − µi)

(µi − µ)(cid:62) · Σ · (Xi − µi)

≤ O

ω ·

13

We ﬁrst remark that we only need extremely mild concentration in Condition (III), but it turns

out this suﬃces in the one place where we use it (see Lemma 4.9).

Additionally, note that we can completely ignore Condition (IV) when ω = 0. The following

makes clear why it is useful when ω > 0.

Lemma 4.4. For (cid:15)-good U , all W ⊂ U of size (cid:15)|U |, and all Σ ∈ K,

(cid:107)A(ˆµ(U ), µ) − A(ˆµ(U ), {µi})(cid:107)K ≤ O

ω +

(cid:32)

(cid:107)A(ˆµ(W ), µ) − A(ˆµ(W ), {µi})(cid:107)K ≤ O

ω +

(cid:32)

(cid:33)2

(cid:15)(cid:112)log 1/(cid:15)
√
k

(cid:112)log 1/(cid:15)
√
k

(cid:33)2

.

Proof. For S = U or S = W and any Σ ∈ K,

(cid:104)Σ, A(ˆµ(S), µ) − A(ˆµ(S), {µi})(cid:105)

=

=

=

1
|S|

1
|S|

2
|S|

(cid:88)

(cid:104)(Xi − µ)⊗2 − (Xi − µi)⊗2, Σ(cid:105)

i∈S
(cid:88)

(µi − µ)(cid:62) · Σ · (2Xi − µi − µ)

i∈S
(cid:88)

(µi − µ)(cid:62) · Σ · (Xi − µi) +

i∈S

1
|S|

(cid:88)

i∈S

(cid:104)(µi − µ)⊗2, Σ(cid:105).

(6)

The ﬁrst (resp. second) part of the lemma follows by taking S = U (resp. S = W ) and invoking
the ﬁrst (resp. second) part of Condition (IV) of (cid:15)-goodness to upper bound the ﬁrst term in (6),
and Fact 3.4 and the fact that (cid:107)µi − µ(cid:107)1 ≤ ω for all i to upper bound the second term in (6).

Corollary 4.5. If U is (cid:15)-good and µ (cid:44) 1
|U |

(cid:80)

i∈U µi, then

(cid:107)A( ˆw(U ), µ) − B({µi})(cid:107)K ≤ O

ω +

(cid:32)

(cid:15)(cid:112)log 1/(cid:15)
√
k

(cid:33)2

.

Proof. This follows immediately from Lemma 4.4 and the ﬁrst part of Condition (II) of (cid:15)-goodness.

In Appendix A, we will show that for N suﬃciently large, the set SG of uncorrupted batches

will satisfy the above deterministic condition.
Lemma 4.6 (Regularity of good samples). If U is a set of (cid:101)Ω (cid:0)log(1/δ)((cid:96)2/(cid:15)2) · log3(n)(cid:1) independent
samples from Mulk(µ1), ..., Mulk(µ|U |), then U is (cid:15)-good with probability at least 1 − δ.

14

4.4 Key Geometric Lemma

The key property of (cid:15)-good sets is the following geometric lemma bounding the accuracy of an
estimate µ(w) given by weights w in terms of (cid:107)M (w)(cid:107)K.

Lemma 4.7 (Spectral signatures). If SG is (cid:15)-good and |SG| ≥ (1 − (cid:15))N , then for any w ∈ W(cid:15),

(cid:107)µ(w) − µ(cid:107)K ≤ O

(cid:18) (cid:15)
√
k

(cid:112)log 1/(cid:15) + (cid:15) · ω +

(cid:114)
(cid:15)

(cid:16)

(cid:107)M (w)(cid:107)K + ω2 +

(cid:17)(cid:19)

.

log 1/(cid:15)

(cid:15)
k

It turns out the proof ingredients for Lemma 4.7 will also be useful in our analysis of Learn-

WithFilter later, so we will now prove this lemma in full.

Proof. Take any Σ ∈ K. Recalling that Σ is psd by Constraint 5 in Deﬁnition 3.1, we will sometimes
write it as Σ = Ev[vv(cid:62)], where the distribution over v is deﬁned according to the eigendecomposition
(cid:2)(cid:104)µ(w) − µ, v(cid:105)2(cid:3). By splitting wi (cid:44) 1/N − δi for i ∈ SG, we have that
of Σ. We wish to bound Ev

(cid:104)µ(w) − µ, v(cid:105) =

wi(cid:104)Xi − µ, v(cid:105)

N
(cid:88)

i=1

=

=

(cid:28) |SG|
N
(cid:28) |SG|
N

(cid:29)

(µ(SG) − µ), v

−

(cid:29)

(µ(SG) − µ), v

−

(cid:88)

i∈SG
(cid:88)

i∈SG

δi(cid:104)Xi − µ, v(cid:105) +

δi(cid:104)Xi − µ, v(cid:105) +

(cid:88)

i∈SB
(cid:88)

i∈SB

wi(cid:104)Xi − µ, v(cid:105),

wi(cid:104)Xi − µ(w), v(cid:105) + (cid:104)µ(w) − µ, v(cid:105)

(cid:88)

i∈SB

wi.

We may rewrite this as



1 −

(cid:88)

i∈SB



wi

 (cid:104)µ(w) − µ, v(cid:105) =

(cid:28) |SG|
N

(cid:29)

(µ(SG) − µ), v

−

(cid:88)

i∈SG

δi(cid:104)Xi − µ, v(cid:105) +

(cid:88)

i∈SB

wi(cid:104)Xi − µ(w), v(cid:105).

Note further that

so in particular,

(cid:88)

i∈SG

δi(cid:104)Xi − µ, v(cid:105) =

(cid:88)

i∈SG

δi(cid:104)Xi − µi, v(cid:105) +

(cid:88)

i∈SG

δi(cid:104)µi − µ, v(cid:105),



1 −

1
4

(cid:88)

i∈SB


2

wi



· E
v

(cid:2)(cid:104)µ(w) − µ, v(cid:105)2(cid:3) ≤ 1 + 2 + 3 + 4

(7)

where

1 (cid:44) |SG|2
N 2

E
v

(cid:2)(cid:104)µ(SG) − µ, v(cid:105)2(cid:3)

2 (cid:44) E
v









(cid:88)

i∈SG



2

δi(cid:104)Xi − µi, v(cid:105)





3 (cid:44) E
v









(cid:88)

i∈SG



2

δi(cid:104)µi − µ, v(cid:105)





4 (cid:44) E
v









(cid:88)

i∈SB



2

wi(cid:104)Xi − µ(w), v(cid:105)





15

For 1 , note that

|SG|2
N 2 (cid:107)µ(SG) − µ(cid:107)2
by the ﬁrst part of Condition (I) of (cid:15)-goodness of SG and the fact that |SG|/N ≥ 1 − (cid:15).

(cid:18) (cid:15)2 log 1/(cid:15)
k

K ≤ O

1 ≤

(cid:19)

For 2 , by Cauchy-Schwarz we have that





(cid:88)

2 ≤


 · E
v

δi





(cid:88)

i∈SG

δi(cid:104)Xi − µi, v(cid:105)2





i∈SG
(cid:42)

(cid:88)

≤ (cid:15) ·

δi(Xi − µi)⊗2, E
v

[vv(cid:62)]

(cid:43)

i∈SG
= (cid:15) (cid:104)A(δ, {µi}), Σ(cid:105)
(cid:19)

(cid:18) (cid:15)2
k

≤ O

log 1/(cid:15)

,

(8)

where the last step follows by Lemma 4.8 below.

For 3 , again by Cauchy-Schwarz,





(cid:88)

δi

3 ≤


 · E
v





(cid:88)

δi(cid:104)µi − µ, v(cid:105)2





i∈SG
(cid:88)

≤ (cid:15) ·

i∈SG
δi(cid:107)µi − µ(cid:107)2
K

i∈SG
≤ (cid:15)2 · max
i∈SG
≤ (cid:15)2 · ω2,

(cid:107)µi − µ(cid:107)2
1

where the penultimate step follows by Fact 3.4.

Finally, we will relate 4 to (cid:107)M (w)(cid:107)K. Let w(cid:48) be the set of weights given by w(cid:48)

i = wi for i ∈ SG

and w(cid:48)

i = 0 for i (cid:54)∈ SG. By another application of Cauchy-Schwarz,





(cid:88)

wi

4 ≤


 · E
v





(cid:88)

i∈SB

wi(cid:104)Xi − µ(w), v(cid:105)2





i∈SB

E
v

≤ (cid:15)

(cid:34) N
(cid:88)

i=1

wi(cid:104)Xi − µ(w), v(cid:105)2

(cid:35)

− E
v





(cid:88)

i∈SG

wi(cid:104)Xi − µ(w), v(cid:105)2









= (cid:15) (cid:10)A(w, µ(w)) − A(w(cid:48), µ(w)), Σ(cid:11)
≤ (cid:15) (cid:10)A(w, µ(w)) − A(w(cid:48), µ(w(cid:48))), Σ(cid:11)

(cid:28)

≤ (cid:15)

A(w, µ(w)) −

1
(cid:80) w(cid:48)
i

(cid:28)

= (cid:15)(cid:104)M (w), Σ(cid:105) + (cid:15)

B(µ(w)) −

≤ (cid:15)(cid:107)M (w)(cid:107)K + (cid:15)(cid:107)B(µ(w)) −

B(µ(w(cid:48)))(cid:107)K + O

B(µ(w(cid:48))), Σ

(cid:29)

(cid:18)

+ O

(cid:15) · ω2 +

(cid:19)

log 1/(cid:15)

(cid:15)2
k

B(µ(w(cid:48))), Σ

(cid:29)

+ O

(cid:18)

(cid:15) · ω2 +

(cid:19)

log 1/(cid:15)

(cid:15)2
k

(cid:18)

(cid:15) · ω2 +

(cid:19)

log 1/(cid:15)

(cid:15)2
k

1
(cid:80) w(cid:48)
i
1
(cid:80) w(cid:48)
i

16

(9)

(10)

(11)

(12)

where (9) follows by the deﬁnition of A(w, ν), (10) follows by Fact 2.2, (11) follows by Lemma 4.9
below. Lastly, by triangle inequality, we may upper bound (cid:107)B(µ(w)) − 1

B(µ(w(cid:48)))(cid:107)K by

(cid:80) w(cid:48)
i

(cid:107)B(µ(w)) − B(µ(w(cid:48)))(cid:107)K + O((cid:15)) · (cid:107)B(µ(w(cid:48)))(cid:107)K ≤

3
k

(cid:107)µ(w) − µ(w(cid:48))(cid:107)1 + O((cid:15)/k) ≤ O((cid:15)/k),

(13)

where the ﬁrst inequality follows by Corollary 3.5, and the bound on (cid:107)µ(w) − µ(w(cid:48))(cid:107)1 in the last
step follows from Fact 2.3. The lemma then follows from (7), (8), (12), and (13).

Next, we show in Lemma 4.8 that small subsets of the good samples cannot contribute too
much to the total energy. Lemma 4.9, which bounds the norm of M (w) for any set of weights w
which is close to the uniform set of weights over SG, will follow as a consequence.

Lemma 4.8. For any 0 < (cid:15) < 1/2, if U is (cid:15)-good, and δ : U → [0, 1/|U |] is a set of weights
satisfying (cid:80)

i∈U δi ≤ (cid:15), then we have the following bounds:

1. (cid:107)A(δ, {µi})(cid:107)K ≤ O( (cid:15)
2. (cid:107) (cid:80)

k log 1/(cid:15))
i∈U δi(Xi − µi)(cid:107)K ≤ O( (cid:15)√
k
(cid:16)

3. (cid:107)A(δ, µ)(cid:107)K ≤ O

(cid:15) · ω2 + (cid:15) log 1/(cid:15)

k

(cid:17)

(cid:112)log 1/(cid:15))

4. (cid:107) (cid:80)

i∈U δi(Xi − µ)(cid:107)K ≤ O( (cid:15)√
k

(cid:112)log 1/(cid:15) + (cid:15) · ω).

Proof. For the ﬁrst part, we may assume without loss of generality that (cid:80)
i∈U δi = (cid:15). But then we
may write δ as (cid:15) EW [ ˆw(W )] for some distribution over subsets W ⊂ U of size (cid:15)|U |. By Jensen’s
inequality and the second part of Condition (II) of (cid:15)-goodness of U , we conclude that

A(δ, {µi}) ≤ (cid:15) · E
W

[(cid:107)A( ˆw(W ), {µi})(cid:107)K] ≤ O

(cid:17)

log 1/(cid:15)

,

(cid:16) (cid:15)
k

giving the ﬁrst part of the lemma.

For the second part, for any Σ ∈ K of the form Σ = E[vv(cid:62)],

(cid:42)

Σ,

(cid:32)

(cid:88)

i∈U

(cid:33)⊗2(cid:43)

δi(Xi − µi)

= E





(cid:32)

(cid:88)

δi(cid:104)Xi − µi, v(cid:105)

(cid:33)2


i∈U

(cid:34)(cid:32)

(cid:88)

≤ E

(cid:33)

δi

·

(cid:32)

(cid:88)

i∈U

i∈U

(cid:33)(cid:35)

δi(cid:104)Xi − µi, v(cid:105)2

≤ (cid:15)(cid:107)A(δ, {µi})(cid:107) ≤ O

(cid:19)

log 1/(cid:15)

,

(cid:18) (cid:15)2
k

where the second step follows by Cauchy-Schwarz, the fourth step follows by the ﬁrst part of the
lemma. As this holds for all Σ ∈ K, we get the second part of the lemma.

This also implies the fourth part of the lemma because

(cid:88)

(cid:107)

i∈U

δi(Xi − µ)(cid:107)K ≤ (cid:107)

(cid:88)

δi(Xi − µi)(cid:107)K + (cid:107)

(cid:88)

i∈U

δi(µi − µ)(cid:107)K

i∈U
(cid:18) (cid:15)
√
k
(cid:18) (cid:15)
√
k

≤ O

≤ O

(cid:112)log 1/(cid:15)

(cid:19)

(cid:88)

+

δi(cid:107)µi − µ(cid:107)1

i∈U
(cid:19)

(cid:112)log 1/(cid:15) + (cid:15) · ω

,

17

where the second step follows by the above together with Fact 3.4 and triangle inequality.

Finally, for the third part of the lemma, upon regarding the weights δ as (cid:15) EW [ ˆw(W )] as before

and applying Jensen’s to the second part of Lemma 4.4, we get that

(cid:107)A(δ, µ) − A(δ, {µi})(cid:107)K ≤ (cid:15) · O

ω +

(cid:32)

(cid:33)2

(cid:112)log 1/(cid:15)
√
k

(cid:18)

(cid:15) · ω2 +

≤ O

(cid:15) log 1/(cid:15)
k

(cid:19)

.

The third part of the lemma then follows by the ﬁrst part, together with triangle inequality.
Lemma 4.9. If SG is (cid:15)-good, and w : SG → [0, 1] satisﬁes (cid:107)w − ˆw(SG)(cid:107)1 ≤ (cid:15) and (cid:80)
then (cid:107)M (w)(cid:107)K ≤ O(ω2 + (cid:15)
Proof. Deﬁne δi = 1/|SG| − wi for all i ∈ SG and take any Σ ∈ K.

k log 1/(cid:15)).

i∈SG

wi = 1,

By Fact 2.2 and the assumption that (cid:107)w(cid:107)1 = 1,

For the second term on the right-hand side of (14), note that we can write

(cid:104)A(w, µ(w)), Σ(cid:105) = (cid:104)A(w, µ), Σ(cid:105) − (cid:107)µ(w) − µ(cid:107)2
K.

(14)

µ(w) − µ =

=

(cid:88)

i∈SG
(cid:88)

i∈SG

wi(Xi − µ)

(1/|SG| − δi)(Xi − µ)

= (µ(SG) − µ) −

= (µ(SG) − µ) −

(cid:88)

i∈SG
(cid:88)

i∈SG

δi(Xi − µ)

δi(Xi − µi) −

(cid:88)

i∈SG

δi(µi − µ),

where the ﬁrst step follows by the fact that (cid:80)

i∈SG

(cid:107)µ(w) − µ(cid:107)K ≤ (cid:107)µ(SG) − µ(cid:107)K + (cid:107)

(cid:88)

i∈SG

wi = 1. So by triangle inequality,
(cid:18) (cid:15)
√
k

(cid:112)log 1/(cid:15) + (cid:15) · ω

δi(Xi − µ)(cid:107)K ≤ O

(cid:19)

(15)

where the second step follows by the ﬁrst part of Condition (I) in the deﬁnition of (cid:15)-goodness for
SG, together with the second part of Lemma 4.8.

Next, we bound the ﬁrst term on the right-hand side of (14). We have

|(cid:104)A(w, µ), Σ(cid:105)| ≤ |(cid:104)A( ˆw(SG), µ), Σ(cid:105)| + |(cid:104)A(δ, µ), Σ(cid:105)|

≤ |(cid:104)A( ˆw(SG), µ), Σ(cid:105)| + O

≤ |(cid:104)B({µi}), Σ(cid:105)| + O

(cid:16) (cid:15)
k

log 1/(cid:15) + (cid:15) · ω2(cid:17)
(cid:19)
(cid:15) log 1/(cid:15)
k

(cid:18)

ω2 +

≤ |(cid:104)B(ˆµ(SG)), Σ(cid:105)| + O

(cid:18)

ω2 +

(cid:15) log 1/(cid:15)
k

(cid:19)

,

(16)

where the second step follows by the third part of Lemma 4.8, the third step follows by Corollary 4.5,
and the fourth step follows by Condition (III) of (cid:15)-goodness.

Additionally, by Corollary 3.5, we can bound

|(cid:104)B(µ(w)), Σ(cid:105) − (cid:104)B(ˆµ(SG)), Σ(cid:105)| ≤

3
k

(cid:107)µ(w) − ˆµ(SG)(cid:107)1 ≤

3
k

(cid:107)w − ˆw(SG)(cid:107)1 ≤ O((cid:15)/k).

(17)

By (16) and (17) we conclude that (cid:104)A(w, µ), Σ(cid:105) ≤ (cid:104)B(µ(w)), Σ(cid:105) + O( (cid:15)

k log 1/(cid:15)), so this together

with (14) and (15) yields the desired bound.

18

4.5 Analyzing the Filter With Spectral Signatures

We now use Lemma 4.7 to show that under the deterministic condition that the uncorrupted points
are (cid:15)-good, LearnWithFilter satisﬁes the guarantees of Theorem 4.1.

The main step is to show that as long as we remain in the main loop of LearnWithFilter,
and we have so far thrown out more bad weight than good weight, we are guaranteed to throw out
more bad weight than good weight in the next iteration of the main loop:

Lemma 4.10. Let w and w(cid:48) be the weights at the start and end of a single iteration of the main loop
of LearnWithFilter. There is an absolute constant C > 0 such that if (cid:107)M (w)(cid:107)K > C · (cid:15)
k log 1/(cid:15)
and (cid:80)

wi − w(cid:48)

1

N − wi, then (cid:80)

1

N − wi < (cid:80)

wi − w(cid:48)
i.

i < (cid:80)

i∈SG

i∈SG

i∈SB

i∈SB

Proof. Suppose the scores τ1, · · · , τN in this iteration are sorted in decreasing order, and let T
denote the smallest index for which (cid:80)
i∈[T ] wi ≥ 2(cid:15). As Filter does not modify wi for i > T , we
just need to show that (cid:80)
i < (cid:80)
i, and by Lemma 4.2 it is enough to
show that

i∈SG∩[T ] wi − w(cid:48)
(cid:88)

i∈SB∩[T ] wi − w(cid:48)
(cid:88)

wiτi.

wiτi <

(18)

i∈SG∩[T ]
First note that because each weight is at most (cid:15), we may assume that (cid:80)

i∈SB∩[T ]

i∈[T ] wi ≤ 3(cid:15). We begin

by upper bounding the left-hand side of (18).
Lemma 4.11. (cid:80)

i∈SG∩[T ] wiτi ≤ O (cid:0) (cid:15)

Proof. Let w(cid:48)(cid:48) be the weights given by w(cid:48)(cid:48)
is equal to

k log 1/(cid:15) + (cid:15) · ω2 + (cid:15)2(cid:107)M (w)(cid:107)K
i for i ∈ SG ∩[T ] and w(cid:48)(cid:48)

(cid:1).

i = 0 otherwise. Then (cid:80)

SG∩[T ] wiτi

(cid:88)

i∈[N ]

w(cid:48)(cid:48)

i τi =

=

≤

≤

(cid:88)

i∈[N ]
(cid:88)

i∈[N ]
(cid:88)

i∈[N ]
(cid:88)

w(cid:48)(cid:48)
i

(cid:10)(Xi − µ(w))⊗2, Σ(cid:11)

w(cid:48)(cid:48)
i

w(cid:48)(cid:48)
i

(cid:10)(Xi − µ(w(cid:48)(cid:48)))⊗2, Σ(cid:11) + (cid:107)w(cid:48)(cid:48)(cid:107)1 · (cid:10)(µ(w(cid:48)(cid:48)) − µ(w))⊗2, Σ(cid:11)

(cid:10)(Xi − µ(w(cid:48)(cid:48)))⊗2, Σ(cid:11) + O((cid:15)) · (cid:107)µ(w(cid:48)(cid:48)) − µ(w)(cid:107)2

K

w(cid:48)(cid:48)
i

(cid:10)(Xi − µ)⊗2, Σ(cid:11) + O((cid:15)) · (cid:107)µ(w(cid:48)(cid:48)) − µ(w)(cid:107)2

K

(19)

(20)

(21)

i∈[N ]
(cid:16)

≤ O

(cid:15) · ω2 +

(cid:17)

log 1/(cid:15)

(cid:15)
k

+ O((cid:15)) · (cid:107)µ(w(cid:48)(cid:48)) − µ(w)(cid:107)2
K

where (19) and (21) both follow from Fact 2.2, (20) follows from the earlier assumption that
(cid:80)
i∈[T ] wi ≤ 3(cid:15) and the deﬁnition of (cid:107) · (cid:107)K, and the last step follows by the third part of Lemma 4.8.
Now note that

(cid:107)µ(w(cid:48)(cid:48)) − µ(w)(cid:107)K ≤ (cid:107)µ(w(cid:48)(cid:48)) − µ(cid:107)K + (cid:107)µ(w) − µ(cid:107)K

≤ O

≤ O

(cid:32) (cid:112)log 1/(cid:15)
√

k

(cid:32) (cid:112)log 1/(cid:15)
√

k

(cid:33)

+ ω

+ (cid:107)µ(w) − µ(cid:107)K

(cid:114)
(cid:15)

(cid:16)

+ ω +

(cid:107)M (w)(cid:107)K + ω2 +

(cid:33)

(cid:17)

,

log 1/(cid:15)

(cid:15)
k

where the second step follows by the fourth part of Lemma 4.8 and the third step holds by
Lemma 4.7. The desired bound follows.

19

One consequence of this is that outside of the tails, the scores among good samples are small.

Corollary 4.12. For all i > T , τi ≤ O( 1

k log 1/(cid:15) + (cid:15)(cid:107)M (w)(cid:107)K + ω2).

Proof. Note that

(cid:88)

wi =

(cid:88)

wi −

(cid:88)

wi ≥ 2(cid:15) −

i∈SB∩[T ]
i∈SG∩[T ]
so the claim follows from Lemma 4.11 and averaging.

i∈[T ]

(cid:88)

i∈SB

wi ≥ (cid:15),

Next, we show that the deviation of the total scores of the good points from their expectation

is negligible.
Lemma 4.13. (cid:80)

i∈SG
Proof. Let w(cid:48) be the weights given by w(cid:48)

wiτi − (cid:104)B(µ(w)), Σ(cid:105) ≤ O (cid:0) (cid:15)

k log 1/(cid:15) + (cid:15) · ω2 + (cid:15) · (cid:107)M (w)(cid:107)K

(cid:1).

i = wi for i ∈ SG and w(cid:48)

i = 0 otherwise. Then by Fact 2.2,

(cid:88)

i∈SG

wiτi =

(cid:88)

i∈SG

wi(cid:104)(Xi − µ(w(cid:48)))⊗2, Σ(cid:105) + (cid:107)w(cid:107)1 · (cid:104)(µ(w) − µ(w(cid:48)))⊗2, Σ(cid:105)

≤

1

(cid:16)

(cid:80)

i∈SG

wi

(cid:104)B(µ(w(cid:48))), Σ(cid:105) + O

(cid:17)(cid:17)

log 1/(cid:15)

(cid:16) (cid:15)
k

+ (cid:107)µ(w) − µ(w(cid:48))(cid:107)2
K

where in the second step we used Fact 2.2, and in the third step we used Lemma 4.9 and the
deﬁnition of (cid:107) · (cid:107)K. To bound the (cid:107)µ(w) − µ(w(cid:48))(cid:107)2

K term, note that

(cid:107)µ(w) − µ(w(cid:48))(cid:107)K ≤ (cid:107)µ(w) − µ(cid:107)K + (cid:107)µ(w(cid:48)) − µ(cid:107)K
(cid:15)(cid:112)log 1/(cid:15)
√
k
(cid:114)

≤ (cid:107)µ(w) − µ(cid:107)K + O

(cid:32)

(cid:32)

(cid:16)

≤ O

(cid:15)(cid:112)log 1/(cid:15)
√
k

(cid:33)

+ (cid:15) · ω

+ (cid:15) · ω +

(cid:15)

(cid:107)M (w)(cid:107)K + ω2 +

(cid:33)

(cid:17)

,

log 1/(cid:15)

(cid:15)
k

where the second step follows by the fourth part of Lemma 4.8, and the third step follows by
Lemma 4.7. Finally, by Corollary 3.5 we have that

(cid:104)B(µ(w(cid:48))), Σ(cid:105) ≤ (cid:104)B(µ(w)), Σ(cid:105) +

3
k

(cid:107)µ(w(cid:48)) − µ(w)(cid:107)1 ≤ (cid:104)B(µ(w)), Σ(cid:105) + O((cid:15)/k),

where the last step follows by Fact 2.3. This completes the proof of the claim.

We are now ready to complete the proof of Lemma 4.10. In light of Lemma 4.11, we wish to

lower bound the right-hand side of (18).
Claim 4.14. If C > 0 in the lower bound (cid:107)M (w)(cid:107)K > C( (cid:15)
(cid:104)M (w), Σ∗(cid:105) must be positive.

k log 1/(cid:15) + ω2) is suﬃciently large, then

Proof. Let w(cid:48) denote the weights given by w(cid:48)

i = wi for i ∈ SG and w(cid:48)
wi(Xi − µ(w))⊗2 − B(µ(w))

i = 0 otherwise. We have

M (w) =

(cid:23)

(cid:23)

(cid:88)

i∈[N ]
(cid:88)

i∈SG
(cid:88)

i∈SG

w(cid:48)

i(Xi − µ(w))⊗2 − B(µ(w))

w(cid:48)

i(Xi − µ(w(cid:48)))⊗2 − B(µ(w))

= M (w(cid:48)) + B(µ(w(cid:48))) − B(µ(w))

(22)

20

where the third step follows by Fact 2.2. Furthermore,

(cid:107)B(µ(w(cid:48))) − B(µ(w))(cid:107)K ≤

3
k

· (cid:107)µ(w(cid:48)) − µ(w)(cid:107)1 ≤ O((cid:15)/k)

(23)

by Corollary 3.5 and Fact 2.3. Lastly, we must bound (cid:107)M (w(cid:48))(cid:107)K. Letting ˆw(cid:48) denote the normalized
version of w(cid:48), we have that

(cid:107)M (w(cid:48))(cid:107)K ≤ (cid:107)M ( ˆw(cid:48))(cid:107)K + (cid:107)M (w(cid:48)) − M ( ˆw(cid:48))(cid:107)K

≤ (cid:107)M ( ˆw(cid:48))(cid:107)K + (cid:107)A( ˆw(cid:48) − w(cid:48), µ)(cid:107)K
log 1/(cid:15) + ω2(cid:17)

≤ O

,

(cid:16) (cid:15)
k

(24)

where the penultimate step follows by Fact 2.2 and the deﬁnition of the matrix M (·), and the last
step follows by Lemma 4.9 and the third part of Lemma 4.8.

We conclude by (22), (23), and (24) that

(cid:104)M (w), Σ(cid:105) ≥ −O

min
Σ∈K

log 1/(cid:15) + ω2(cid:17)

,

(cid:16) (cid:15)
k

(25)

so we simply need to take C larger than the constant implicit in the right-hand side of (25) to
ensure that (cid:104)M (w), Σ∗(cid:105) > 0.

By Claim 4.14 and the deﬁnition of the scores,

wiτi − (cid:104)B(µ(w)), Σ∗(cid:105) = (cid:104)M (w), Σ∗(cid:105) ≥ (cid:107)M (w)(cid:107)K.

(cid:88)

i∈[N ]

This, together with Lemma 4.13, yields (cid:80)
wiτi ≥ C(cid:48)(cid:107)M (w)(cid:107)K for some C(cid:48) < C which we
can take to be arbitrarily large. We want to show that this same sum, over only SB ∩ [T ], enjoys
essentially the same bound. Indeed,

i∈SB

(cid:88)

i∈SB∩[T ]

wiτi ≥ C(cid:48)(cid:107)M (w)(cid:107)K −

(cid:88)

wiτi

i∈SB\[T ]




≥ C(cid:48)(cid:107)M (w)(cid:107)K −



(cid:88)

i∈SB

wi

 · O

≥ C · (cid:107)M (w)(cid:107)K,

log 1/(cid:15) + ω2 + (cid:15)(cid:107)M (w)(cid:107)K

(cid:19)

(cid:18) 1
k

for some arbitrarily large absolute constant C, where the second step follows by Corollary 4.12,
and the last by the assumption that (cid:107)M (w)(cid:107)K > C · ( (cid:15)
k log 1/(cid:15) + ω2). On the other hand, by this
same assumption and by Lemma 4.11,

(cid:88)

wiτi ≤ O

i∈SG∩[T ]

(cid:16) (cid:15)
k

log 1/(cid:15) + (cid:15) · ω2 + (cid:15)2(cid:107)M (w)(cid:107)K

(cid:17)

≤ C · (cid:107)M (w)(cid:107)K,

where C can be taken to be smaller than C. This proves (18) and thus Lemma 4.10.

We can now combine Lemma 4.7 and Lemma 4.10 to get a proof of Theorem 4.1.

21

Proof of Theorem 4.1. Let ˆµ be the output of LearnWithFilter. By Lemma 2.7, it suﬃces to
(cid:112)log 1/(cid:15)), or equivalently that for all v ∈ V n
show that ˆµ satisﬁes (cid:107)ˆµ − µ(cid:107)As(d+1) ≤ O(ω + (cid:15)√
(cid:96) ,
k
(cid:112)log 1/(cid:15)). By Corollary 3.3,
where (cid:96) (cid:44) 2s(d + 1), we have that (cid:104)(ˆµ − µ)⊗2, vv(cid:62)(cid:105)1/2 ≤ O(ω + (cid:15)√
k
(cid:112)log 1/(cid:15)). By Lemma 4.7 together with the
it is enough to show that (cid:107)ˆµ − µ(cid:107)K ≤ O(ω + (cid:15)√
k
termination condition of the main loop of LearnWithFilter, we just need to show that the
algorithm terminates (in polynomial time) and that w ∈ WO((cid:15)).

But by induction and Lemma 4.10, every iteration of the loop removes more mass from the bad
points than from the good points. Furthermore, by Lemma 4.2, the support of w goes down by at
least one every time 1DFilter is run, so the loops terminates after at most N iterations, each of
which can be implemented in polynomial time. At the end, at most an (cid:15) fraction of the total mass
on SG has been removed, so the ﬁnal weights w satisfy w ∈ W2(cid:15) as desired.

5 Numerical Experiments

Figure 1: Arbitrary Distributions:

In this section we report on empirical evaluations of our algorithm on synthetic data. We
compared our algorithm LearnWithFilter, the naive estimator which simply takes the empirical
mean of all samples, the “oracle” algorithm which computes the empirical mean of the uncorrupted
k which our theorems show that LearnWithFilter achieves,
samples, and the threshold of (cid:15)/
√
k
up to constant factors (in Figures 1 and 2, these are labeled “ﬁlter”, “naive”, “oracle”, and (cid:15)/

√

22

050100(i) domain size n0.0000.0500.1000.1500.200A/2 distance/kfilteroraclenaive02505007501000(ii) batch size k0.0000.1000.2000.3000.400A/2 distance/kfilteroraclenaive0.00.10.20.30.4(iii) corruption 0.0200.0400.0600.0800.100A/2 distance/kfilteroraclenaive304050(iv) number of batches0.0200.0400.0600.080A/2 distance/kfilteroraclenaiveFigure 2: Structured Distributions:

respectively). Note that by deﬁnition, the oracle dominates the algorithms considered in [CLM19]
and [JO19] for the unstructured case, as those algorithms search for a subset of the data and
output the empirical mean of that subset. But as Theorem 4.1 predicts, LearnWithFilter
should actually outperform the oracle in settings where the underlying distribution µ is structured
and there are too few samples for the empirical mean of the uncorrupted points to concentrate
suﬃciently. In these experiments, we conﬁrm this empirically.

5.1 Experimental Design

Our experiments fall under two types: (A) those on learning an arbitrary distribution in A(cid:96)/2
norm and B) those on learning a structured distribution in total variation distance. The purpose
of experiments of type (A) will be to convey that LearnWithFilter can be used to learn from
untrusted batches in A(cid:96)/2 norm even for distributions which are not necessarily structured. The
purpose of experiments of type (B) will be to demonstrate that LearnWithFilter can outperform
the oracle for structured distributions.

Throughout, ω = 0 and (cid:96) = 10. While our algorithm can also be implemented for larger (cid:96) (as
the size of the SDP we solve does not depend on (cid:96)), we choose (cid:96) = 5 because it is small enough that
the sample complexity savings of our algorithm are very pronounced, yet large enough that for the
domain sizes n we work with, enumerating over V n
(cid:96) would be prohibitively expensive, justifying the
need to use an SDP.

23

050100(i) domain size n0.0000.0250.0500.0750.1000.125L1 distance/kfilteroraclenaive02505007501000(ii) batch size k0.0000.2000.4000.600L1 distance/kfilteroraclenaive0.00.10.20.30.4(iii) corruption 0.0250.0500.0750.1000.125L1 distance/kfilteroraclenaive304050(iv) number of batches0.0000.0250.0500.0750.1000.125L1 distance/kfilteroraclenaiveFor experiments of type (A), we chose the true underlying distribution µ by sampling uniformly
from [0, 1]n and normalizing, and for experiments of type B), we chose µ by sampling a uniformly
random piecewise constant function with (cid:96) = 5 pieces.

Given µ and a prescribed parameter δ, the distribution from which the corrupted batches were
drawn was taken to be Mulk(ν), where ν was constructed to satisfy dTV(µ, ν) = δ by adding 2δ
n
to the smallest entries of µ and subtracting 2δ
n from the largest. Sometimes this does not give a
probability distribution, in which case we resample µ. When k, (cid:15), N are clear from context and we
say that N (cid:15)-corrupted batches are drawn from the distribution speciﬁed by (µ, ν), we mean that
(cid:98)(1 − (cid:15))N (cid:99) samples are drawn from Mulk(µ) and N − (cid:98)(1 − (cid:15))N (cid:99) from Mulk(ν).

As noted in [JO19], choosing δ too high makes it too easy to detect the corruptions in the data,
while choosing δ too low means the naive estimator will already perform quite well. In light of
this and the fact that the above process for generating ν only ensures that dTV(µ, ν) = δ, whereas
(cid:107)µ − ν(cid:107)A(cid:96) might be much smaller, we chose δ for our experiments as follows. For experiments of
type (A), we took δ = 0.5 to ensure that the typical A(cid:96)/2 distance between the empirical mean
and the truth was still suﬃciently large that the the naive estimator was not competitive. For
experiments of type B) where we measure error in terms of total variation distance, we could aﬀord
to choose δ slightly smaller, namely δ = 0.3.

We ﬁrst describe the experiments of type (A). We examined the eﬀect of varying one of the
following four parameters at a time: domain size n, batch size k, corruption fraction (cid:15), and total
number of batches N . Each of the following four experiments was repeated for a total of ten trials.

(a) Varying domain size n: We ﬁxed (cid:15) = 0.4, k = 1000, and N = (cid:98) (cid:96)/(cid:15)2

1−(cid:15) (cid:99) to ensure (cid:98)(cid:96)/(cid:15)2(cid:99)
samples from Mulk(µ). We chose such large k to ensure the gap between empirical mean
and our algorithm was very noticable. In each trial and for each n ∈ [4, 8, 16, 32, 64, 128],
we randomly generated (µ, ν) via the above procedure, drew N (cid:15)-corrupted samples from
distribution speciﬁed by (µ, ν). Note that while N is independent of n, the performance of
our algorithm is comparable to that of the oracle.4

(b) Varying batch size k: We ﬁxed (cid:15) = 0.4, n = 64, and N = (cid:96)/(cid:15)2

1−(cid:15) (cid:99). In each trial, we randomly gen-

erated (µ, ν) via the above procedure, and then for each value of k ∈ [1, 50, 100, 250, 500, 750, 1000]
we drew N samples from the distribution speciﬁed by (µ, ν). Note that while our algorithm’s
error and the oracle’s error decay with k, the empirical mean’s error remains ﬁxed.

(c) Varying corruption fraction (cid:15): We ﬁxed (cid:15)∗ = 0.4, n = 64, k = 1000, and N = (cid:98)(cid:96)/(cid:15)∗2(cid:99). In
each trial, we randomly generated (µ, ν) via the above procedure and drew N samples from
Mul(k, µ). Then for each (cid:15) ∈ [0.0, 0.1, 0.2, 0.3, 0.4], we augmented this with an additional (cid:98) (cid:15)N
1−(cid:15)
samples from Mul(k, ν). Note that while our algorithm’s error remains close to (cid:15)∗/
k, the
empirical mean’s error increases linearly in (cid:15).

√

(d) Varying number of batches N : We ﬁxed (cid:15) = 0.4, n = 128, and k = 500. In each trial, we ran-
domly generated (µ, ν) via the above procedure, and then for each ρ ∈ [0.5, 0.75, 1, 1.25, 1.5],
we drew N = (cid:98)ρ · (cid:96)/(cid:15)2(cid:99) samples from the distribution speciﬁed by (µ, ν). Note that even with
such a small number of samples, our algorithm can compete with the oracle. Also note that
our error bottoms out at (cid:15)/

k while the oracle’s error goes beneath this threshold.

√

For type (B), we ran the exact same set of four experiments but over structured µ, with the
key diﬀerence that after generating an estimate with LearnWithFilter, we post-processed it by

4The naive estimator’s error is decreasing in n for an unrelated reason: as n increases, the above procedure for

sampling (µ, ν) appears to skew towards µ for which the resulting perturbation ν is close in A(cid:96)/2.

24

rounding to a piecewise constant function via a simple dynamic program. We then compare the
error of this piecewise constant estimator in total variation distance to that of the empirical mean
of the whole dataset, and the empirical mean of the uncorrupted points.

As is evident from Figure 2, our algorithm outperforms even the oracle, as predicted by Theo-

rem 4.1.

5.2 Implementation Details

The experiments were conducted on a MacBook Pro with 2.6 GHz Dual-Core Intel Core i5 processor
and 8 GB of RAM. The experiments of type (A) respectively took 110m36.499s, 73m19.477s,
50m54.655s, and 536m39.212s to run. The experiments of type (B) respectively took 64m28.346s,
52m7.859s, 39m36.754s, and 362m50.742s to run. The discrepancy in runtimes between (A) and
(B) can be explained by the fact that a number of unrelated processes were also running at the time
of the former. The experiment of varying the number of batches N was the most expensive because
we chose domain size n = 128 to accentuate the gap between our algorithm and the oracle. The
abovementioned runtimes imply that over a domain of size 128, LearnWithFilter takes roughly
7-10 minutes.

For the implementation, we used the SCS solver in CVXPY for our semideﬁnite programs. In
order to achieve reasonable runtimes, we needed to set the feasibility tolerance to 1e − 2, and as a
result the SDP solver would occasionally output matrices Σ which are moderately far from K; in
particular, one mode of failure that arose was that Σ might be non-PSD and give rise to negative
scores in LearnWithFilter. We chose to address this mode of failure heuristically by terminating
the algorithm whenever this happened and simply outputting the estimate for µ at that point in
time. Of the 480 total trials that were run across all experiments, this happened 53 times. Another
heuristic that we used was to terminate the algorithm as soon as (cid:107)Σ(cid:107)K stopped increasing during
a run of LearnWithFilter; this was primarily to have a stopping criterion that avoids the need
to tune constant factors. As demonstrated by Figures 1 and 2, these heuristic decisions ultimately
had negligible eﬀect on the performance of our algorithm.

All code, data, and documentation can be found at https://github.com/secanth/federated.

Acknowledgments We would like to thank the authors of the concurrent work [JO20] for coor-
dinating submissions with us.

References

[ADH+15] J. Acharya, I. Diakonikolas, C. Hegde, J. Li, and L. Schmidt. Fast and Near-Optimal

Algorithms for Approximating Distributions by Histograms. In PODS, 2015.

[ADLS17] Jayadev Acharya, Ilias Diakonikolas, Jerry Li, and Ludwig Schmidt. Sample-optimal
density estimation in nearly-linear time. In Proceedings of the Twenty-Eighth Annual
ACM-SIAM Symposium on Discrete Algorithms, pages 1278–1289. SIAM, 2017.

[AN04]

Noga Alon and Assaf Naor. Approximating the cut-norm via grothendieck’s inequality.
In Proceedings of the thirty-sixth annual ACM symposium on Theory of computing,
pages 72–80. ACM, 2004.

[Ans60]

Frank J Anscombe. Rejection of outliers. Technometrics, 2(2):123–146, 1960.

25

[BBBB72] Richard E Barlow, David J Bartholomew, James M Bremner, and H Daniel Brunk.
Statistical inference under order restrictions: The theory and application of isotonic
regression. Technical report, Wiley New York, 1972.

[CDSS13]

Siu-On Chan, Ilias Diakonikolas, Rocco A Servedio, and Xiaorui Sun. Learning mix-
tures of structured distributions over discrete domains. In Proceedings of the twenty-
fourth annual ACM-SIAM symposium on Discrete algorithms, pages 1380–1394. Society
for Industrial and Applied Mathematics, 2013.

[CDSS14a] S. Chan, I. Diakonikolas, R. Servedio, and X. Sun. Near-optimal density estimation in

near-linear time using variable-width histograms. In NIPS, pages 1844–1852, 2014.

[CDSS14b] Siu-On Chan, Ilias Diakonikolas, Rocco A Servedio, and Xiaorui Sun. Eﬃcient density
estimation via piecewise polynomial approximation. In Proceedings of the forty-sixth
annual ACM symposium on Theory of computing, pages 604–613. ACM, 2014.

[CLM19]

Sitan Chen, Jerry Li, and Ankur Moitra. Eﬃciently learning structured distributions
from untrusted batches. arXiv preprint arXiv:1911.02035, 2019.

[CSV17] Moses Charikar, Jacob Steinhardt, and Gregory Valiant. Learning from untrusted data.
In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing,
pages 47–60. ACM, 2017.

[DHL19]

Yihe Dong, Samuel Hopkins, and Jerry Li. Quantum entropy scoring for fast robust
mean estimation and improved outlier detection. In Advances in Neural Information
Processing Systems, pages 6065–6075, 2019.

[Dia16]

Ilias Diakonikolas. Learning structured distributions. Handbook of Big Data, 267, 2016.

[DK19]

Ilias Diakonikolas and Daniel M Kane. Recent advances in algorithmic high-dimensional
robust statistics. arXiv preprint arXiv:1911.05911, 2019.

[DKK+17]

[DKK+18]

[DKK+19]

[DKS17]

Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Ankur Moitra, and
Alistair Stewart. Being robust (in high dimensions) can be practical. In Proceedings of
the 34th International Conference on Machine Learning-Volume 70, pages 999–1008.
JMLR. org, 2017.

Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Jacob Steinhardt, and
Alistair Stewart. Sever: A robust meta-algorithm for stochastic optimization. arXiv
preprint arXiv:1803.02815, 2018.

Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Ankur Moitra, and Alistair
Stewart. Robust estimators in high-dimensions without the computational intractabil-
ity. SIAM Journal on Computing, 48(2):742–864, 2019.

Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. Statistical query lower bounds
for robust estimation of high-dimensional gaussians and gaussian mixtures. In 2017
IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS), pages
73–84. IEEE, 2017.

[DL01]

Luc Devroye and Gabor Lugosi. Combinatorial Methods in Density Estimation.
Springer Science & Business Media, 2001.

26

[HL18]

Samuel B Hopkins and Jerry Li. Mixture models, robustness, and sum of squares
proofs. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of
Computing, pages 1021–1034. ACM, 2018.

[Hub92]

Peter J Huber. Robust estimation of a location parameter. In Breakthroughs in statis-
tics, pages 492–518. Springer, 1992.

[JO19]

[JO20]

Ayush Jain and Alon Orlitsky. Robust learning of discrete distributions from batches.
arXiv preprint arXiv:1911.08532, 2019.

Ayush Jain and Alon Orlitsky. A general method for robust learning from batches.
arXiv preprint arXiv:2002.11099, 2020.

[KMY+16] Jakub Koneˇcn`y, H Brendan McMahan, Felix X Yu, Peter Richt´arik, Ananda Theertha
Suresh, and Dave Bacon. Federated learning: Strategies for improving communication
eﬃciency. arXiv preprint arXiv:1610.05492, 2016.

[KSS18]

Pravesh K Kothari, Jacob Steinhardt, and David Steurer. Robust moment estimation
and improved clustering via sum of squares. In Proceedings of the 50th Annual ACM
SIGACT Symposium on Theory of Computing, pages 1035–1046. ACM, 2018.

[Li18]

Jerry Zheng Li. Principled approaches to robust machine learning and beyond. PhD
thesis, Massachusetts Institute of Technology, 2018.

[LRR13]

Reut Levi, Dana Ron, and Ronitt Rubinfeld. Testing properties of collections of dis-
tributions. Theory of Computing, 9(1):295–347, 2013.

[LRV16]

Kevin A Lai, Anup B Rao, and Santosh Vempala. Agnostic estimation of mean and
covariance. In 2016 IEEE 57th Annual Symposium on Foundations of Computer Science
(FOCS), pages 665–674. IEEE, 2016.

[MMR+17] H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera
y Arcas. Communication-eﬃcient learning of deep networks from decentralized data. In
Proceedings of the 20th International Conference on Artiﬁcial Intelligence and Statistics
(AISTATS), 2017.

[O’B16]

[QV17]

[SCV18]

Carl M O’Brien. Nonparametric estimation under shape constraints: Estimators, al-
gorithms and asymptotics. International Statistical Review, 84(2):318–319, 2016.

Mingda Qiao and Gregory Valiant. Learning discrete distributions from untrusted
batches. arXiv preprint arXiv:1711.08113, 2017.

Jacob Steinhardt, Moses Charikar, and Gregory Valiant. Resilience: A criterion for
learning in the presence of arbitrary outliers. In 9th Innovations in Theoretical Com-
puter Science Conference (ITCS 2018). Schloss Dagstuhl-Leibniz-Zentrum fuer Infor-
matik, 2018.

[SHKT97] C. J. Stone, M. H. Hansen, C. Kooperberg, and Y. K. Truong. Polynomial splines and
their tensor products in extended linear modeling: 1994 wald memorial lecture. Ann.
Statist., 25(4):1371–1470, 1997.

[Ste18]

Jacob Steinhardt. Robust Learning: Information Theory and Algorithms. PhD thesis,
Stanford University, 2018.

27

[Sto94]

C. J. Stone. The use of polynomial splines and their tensor products in multivariate
function estimation. The Annals of Statistics, 22(1):pp. 118–171, 1994.

[TKV17] Kevin Tian, Weihao Kong, and Gregory Valiant. Learning populations of parameters.

In Advances in Neural Information Processing Systems, pages 5778–5787, 2017.

[Tuk60]

[Tuk75]

John W Tukey. A survey of sampling from contaminated distributions. Contributions
to probability and statistics, pages 448–485, 1960.

John W Tukey. Mathematics and the picturing of data.
In Proceedings of the In-
ternational Congress of Mathematicians, Vancouver, 1975, volume 2, pages 523–531,
1975.

[WN07]

R. Willett and R. D. Nowak. Multiscale poisson intensity and density estimation. IEEE
Transactions on Information Theory, 53(9):3171–3187, 2007.

[WW83]

E. J. Wegman and I. W. Wright. Splines in statistics. Journal of the American Statis-
tical Association, 78(382):pp. 351–365, 1983.

A Concentration

In this section we prove Lemma 4.6, restated here for convenience:
Lemma 4.6 (Regularity of good samples). If U is a set of (cid:101)Ω (cid:0)log(1/δ)((cid:96)2/(cid:15)2) · log3(n)(cid:1) independent
samples from Mulk(µ1), ..., Mulk(µ|U |), then U is (cid:15)-good with probability at least 1 − δ.

A.1 Technical Ingredients

The key technical fact we use to get sample complexity that depend quadratically on (cid:96) is:

Lemma A.1. For every 0 < η ≤ 1, there exists a net N ⊂ Rn×n of size O(n3(cid:96)2 log2 n/η)((cid:96) log n+1)2
of matrices such that for every Σ ∈ K, there exists some ˜Σ = (cid:80)
ν ∈ N such that the
following holds: 1) (cid:107)Σ − ˜Σ(cid:107)F ≤ η, 2) (cid:80)

ν Σ∗
ν(cid:107)max ≤ O(1).

ν αν ≤ 1, and 3) (cid:107)Σ∗

ν for Σ∗

Note that this is a strengthening of a special case of Lemma 6.9 from [CLM19]. We defer the

proof of Lemma A.1 to Appendix B.

For (cid:15)-goodness to hold, it will be crucial to establish the following sub-exponential tail bounds
for the empirical covariance of a set of samples X1, · · · , XN ∼ Mulk(µ), as well as for (cid:107)ˆµ − µ(cid:107)2
K,
where ˆµ is the empirical mean of those samples.

Lemma A.2. Let ξ > 0 and let N ⊂ Rn×n be any ﬁnite set for which (cid:107)Σ(cid:107)max ≤ O(1) for all
Σ ∈ N . Let µ1, ..., µN , µ ∈ ∆n satisfy µ (cid:44) 1
N

i=1 µi. Then for Xi ∼ Mulk(µi) for i ∈ [N ],

(cid:80)N

Pr

(cid:34)(cid:12)
(cid:42)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
N

N
(cid:88)

(Xi − µi)⊗2 −

i=1

E
X∼Mulk(µi)

(cid:2)(X − µi)⊗2(cid:3) , Σ

(cid:43)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:35)

(cid:18)

> t ∀ Σ ∈ N

< 2|N | exp

−Ω

(cid:18) N k2t2
1 + kt

(cid:19)(cid:19)

,

where the probability is over the samples X1, · · · , XN .

28

Lemma A.3. Let ξ > 0 and let N ⊂ Rn×n be any ﬁnite set for which (cid:107)Σ(cid:107)max ≤ O(1) for all
(cid:80)N
Σ ∈ N . For Xi ∼ Mulk(µi) for i ∈ [N ], ˆµ (cid:44) 1
N

i=1 Xi, and µ (cid:44) 1
N

(cid:80)N

Pr (cid:2)(cid:12)
(cid:12)

(cid:10)(ˆµ − µ)⊗2, Σ(cid:11) − E (cid:2)(cid:10)(ˆµ − µ)⊗2, Σ(cid:11)(cid:3)(cid:12)

(cid:12) > t ∀ Σ ∈ N (cid:3) < 2|N | exp

−Ω

i=1 µi,
(cid:18)

(cid:18) N 2k2t2
1 + N kt

(cid:19)(cid:19)

,

where the probability is over the samples X1, · · · , XN .

Lemma A.4. Let ξ > 0 and let N ⊂ Rn×n be any ﬁnite set for which (cid:107)Σ(cid:107)max ≤ O(1) for all
Σ ∈ N . Let µ1, ..., µN , µ ∈ ∆n satisfy (cid:107)µi − µ(cid:107)1 ≤ ω for all i ∈ [N ]. For Xi ∼ Mulk(µi) for i ∈ [N ],

Pr

(cid:34)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
N

N
(cid:88)

i=1

(cid:12)
(cid:12)
(µi − µ)(cid:62)Σ(Xi − µi)
(cid:12)
(cid:12)
(cid:12)

(cid:35)

> ω · t ∀ Σ ∈ N

< 2|N | exp (cid:0)−Ω (cid:0)kN t2(cid:1)(cid:1) ,

where the probability is over the samples X1, · · · , XN .

Note that if N consisted solely of matrices of the form vv(cid:62) for v ∈ {±1}n, these lemmas would
follow straightforwardly from standard binomial tail bounds.
Instead, we only have entrywise
bounds for the matrices in N and will therefore need to compute moment estimates from scratch
in order to prove Lemmas A.2 and A.3. We defer the details of this to Appendix C.

Lastly, we will need the following elementary consequence of Stirling’s formula:
(cid:1) ≤ 2m · (cid:15) log 1/(cid:15).

Fact A.5. For any m ≥ 1, log (cid:0) m
(cid:15)m

A.2 Proof of Lemma 4.6

We are now ready to prove that the four conditions for (cid:15)-goodness hold for a set U of independent
draws from Mulk(µ1), ..., Mulk(µ|U |) respectively, of size

|U | = (cid:101)Ω (cid:0)log(1/δ)((cid:96)2/(cid:15)2) · log3(n)(cid:1) .

(26)

Proof of Lemma 4.6. As (cid:107) · (cid:107)K is deﬁned as a supremum over K, we will reduce controlling the
inﬁnitely many directions in K to controlling a ﬁnite net of such directions by invoking Lemma A.1.
Speciﬁcally, recall that for any Σ ∈ K, by Lemma A.1, there is some ˜Σ = (cid:80)
ν such that
ν ∈ N and (cid:107)Σ − ˜Σ(cid:107)F ≤ η.
Σ∗

ν ανΣ∗

(Condition (I)) By Lemma A.3, with probability at least 1 − 2|N | exp

that for all Σ ∈ K,

(cid:16)

−Ω( N 2k2t2
1+N kt )

(cid:17)

, we have

(cid:10)(µ(U ) − µ)⊗2, Σ(cid:11) ≤

(cid:68)

(µ(U ) − µ)⊗2, ˜Σ

(cid:69)

+ (cid:107)µ(U ) − µ(cid:107)2

2 · (cid:107)Σ − ˜Σ(cid:107)F

≤

=

≤

(cid:68)

(cid:69)

(µ(U ) − µ)⊗2, ˜Σ
(cid:88)

+ 2η
(cid:10)(µ(U ) − µ)⊗2, Σ∗
ν

αν

(cid:11) + 2η

ν

1
N

N
(cid:88)

i=1

E (cid:2)(cid:10)(X − µi)⊗2, Σ∗

ν

(cid:11)(cid:3) +

(cid:88)

ν

αν · t + 2η

≤ O(1/k|U |) + t + 2η,

(27)

where the ﬁrst step follows by Cauchy-Schwarz and triangle inequality, the second step follows by
2 ≤ 2 and the bound on (cid:107)Σ − ˜Σ(cid:107)F guaranteed by Lemma A.1, the
the trivial bound (cid:107)µ(U ) − µi(cid:107)2

29

fourth step holds with the claimed probability by Lemma A.3 and the fact that (cid:107)Σ∗
ν(cid:107)max ≤ O(1)
for all ν by the guarantees of Lemma A.1, and the last step follows by the bound on (cid:80) αν by the
guarantees of Lemma A.1, as well as the moment bound in Lemma C.2 applied to r = 1.

If |U | satisﬁes (26) and η, t = O( (cid:15)2
For the second part, by the steps leading to (27), a union bound over the (cid:0) |U |
(cid:15)|U |

k log 1/(cid:15)), the ﬁrst part of Condition (I) holds.

(cid:1) subsets W and

Fact A.5, with probability at least

1 − 2 exp(2|U | · (cid:15) log 1/(cid:15)) · |N | exp

−Ω

(cid:18)

(cid:18) (cid:15)2|U |2k2t2
1 + (cid:15)|U |kt

(cid:19)(cid:19)

we have that (cid:107)µ(W ) − µW (cid:107)2

K ≤ O

(cid:16) 1

(cid:17)

(cid:15)k|U |

+ t + 2η for all W . Note that 2 log 1/(cid:15) ≤ O

(cid:16) (cid:15)|U |2k2t2
1+(cid:15)|U |kt

(cid:17)

, so if |U | satisﬁes (26) and η = O( log 1/(cid:15)

k

), the second part of Condition (I)

provided t = Ω
holds.

(cid:16) log 1/(cid:15)
k

(cid:17)

(Condition (II)) For the ﬁrst part, let ˆM (cid:44) M ( ˆw(U ), {µi}i∈U ). By Lemma A.2, with proba-

bility at least 1 − 2|N | exp

−Ω

, we have that for all Σ ∈ K,

(cid:16)

(cid:16) |U |k2t2
1+kt

(cid:17)(cid:17)

(cid:104) ˆM, Σ(cid:105) ≤ (cid:104) ˆM, ˜Σ(cid:105) + (cid:107) ˆM(cid:107)F · (cid:107)Σ − ˜Σ(cid:107)F

≤ (cid:104) ˆM, ˜Σ(cid:105) + 3η
αν(cid:104) ˆM, Σ∗

(cid:88)

≤

ν(cid:105) + 3η

ν
(cid:88)

≤

αν · t + 3η

ν
≤ t + 3η

(28)

where the ﬁrst step follows by Cauchy-Schwarz and triangle inequality, and the second step follows
by Lemma 2.4 and the bound on (cid:107)Σ − ˜Σ(cid:107)F guaranteed by Lemma A.1, the fourth step holds with
the claimed probability by Lemma A.2 and the fact that (cid:107)Σ∗
ν(cid:107)max ≤ O(1) for all ν by the guarantees
of Lemma A.1, and the last step follows by the bound on (cid:80) αν by the guarantees of Lemma A.1.
k log 1/(cid:15)(cid:1), the ﬁrst part of Condition (II) holds.
If |U | satisﬁes (26), η = O (cid:0) (cid:15)
For the second part, ﬁrst note that it is slightly diﬀerent from the ﬁrst part because we do not
), so this term is negligible.
(cid:1) subsets W , and Fact A.5, with probability

subtract out B(µ), the reason being that (cid:107)B(µ)(cid:107)K ≤ O(1/k) = o( log 1/(cid:15)
By the steps leading to (28), a union bound over the (cid:0) |U |
(cid:15)|U |
at least

k log 1/(cid:15)(cid:1), t = O (cid:0) (cid:15)

k

1 − 2|N | exp(2(cid:15)|U | log 1/(cid:15)) · exp

−Ω

(cid:18)

(cid:18) (cid:15)|U |k2t2
1 + kt

(cid:19)(cid:19)

,

we have that (cid:107)M ( ˆw(W ), {µi}i∈W )(cid:107)K ≤ t + 3η for all W . Note that 2 log 1/(cid:15) ≤ O

(cid:17)

(cid:16) k2t2
1+kt

provided

t = Ω

(cid:16) log 1/(cid:15)
k

(cid:17)

, so if |U | satisﬁes (26) and η = O

(cid:16) log 1/(cid:15)
k

(cid:17)

(Condition (III)) First note that

, the second part of Condition (II) holds.

B({µi}) − B(µ) =

1
|U |

1
k

(cid:88)

i∈U

(cid:0)diag(µi − µ) − (µ⊗2

i − µ⊗2)(cid:1) = −

1
|U |

1
k

(cid:88)

i∈U

(µ⊗2

i − µ⊗2).

Also note that
(cid:42)

Σ,

(cid:43)

(µ⊗2

i − µ⊗2)

=

1
|U |

(cid:88)

i∈U

1
|U |

(cid:88)

i∈U

(cid:10)(µi − µ)⊗2, Σ(cid:11) ≤ max

i

(cid:107)µi − µ(cid:107)2

1 ≤ ω2,

30

where in the last step we used Fact 3.4. So (cid:107)B({µi}) − B(µ)(cid:107)K ≤ ω2/k.

It remains to bound (cid:107)B(ˆµ(U ))−B(µ)(cid:107)K. As we only need to show extremely mild concentration

here, we will not make an eﬀort to obtain tight bounds. Note that by (2),

|(cid:104)Σ, B(ˆµ(U )) − B(µ)(cid:105)| ≤

1
k

|(cid:104)diag(ˆµ(U ) − µ), Σ(cid:105)| +

(cid:12)(cid:104)ˆµ(U )⊗2 − µ⊗2, Σ(cid:105)(cid:12)
(cid:12)
(cid:12) .

(29)

1
k

We have

(cid:104)diag(ˆµ(U ) − µ), Σ(cid:105) ≤

≤

(cid:88)

ν
(cid:88)

ν

αν(cid:104)diag(ˆµ(U ) − µ), Σ∗

ν(cid:105) + (cid:107)Σ − ˜Σ(cid:107)F · (cid:107)ˆµ(U ) − µ(cid:107)2

αν(cid:104)ˆµ(U ) − µ, diag(Σ∗

ν)(cid:105) + O(η).

(30)

Note that for any ν, (cid:104)ˆµ(U ) − µ, diag(Σ∗
ν). These are
independent, mean-zero, O(1)-bounded random variables, so by Hoeﬀding’s, for any ﬁxed ν we
ν)(cid:105)| ≤ t with probability at least 1 − 2 exp(−Ω(|U |t2)). If we union
have that |(cid:104)ˆµ(U ) − µ, diag(Σ∗
bound over N , then by taking η, t = O((cid:15)), and |U | satisfying (26), (30) will be at most O((cid:15)).

(cid:44) (cid:104)Xi − µi, diag(Σ∗

ν)(cid:105) = 1
|U |

i∈U Zν
i

for Zν
i

(cid:80)

We also have that

(cid:12)(cid:104)ˆµ(U )⊗2 − µ⊗2, Σ(cid:105)(cid:12)
(cid:12)

(cid:12) =

(cid:12)
(cid:12)
(cid:12)(cid:104)(ˆµ(U ) − µ)⊗2, Σ(cid:105) − 2µ(cid:62)Σ(ˆµ(U ) − µ)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)µ(cid:62)Σ(ˆµ(U ) − µ)
(cid:12)
(cid:12)
(cid:12) ,

(cid:18) (cid:15)2 log 1/(cid:15)
k

+ 2

(cid:19)

≤ O

where the second step follows by the ﬁrst part of this lemma. For the other term, we have

µ(cid:62)Σ(ˆµ(U ) − µ) ≤

≤

(cid:88)

ν
(cid:88)

ν

ανµ(cid:62)Σ∗

ν(ˆµ(U ) − µ) + (cid:107)Σ − ˜Σ(cid:107)F · (cid:107)µ(cid:107)2 · (cid:107)ˆµ(U ) − µ(cid:107)2

ανµ(cid:62)Σ∗

ν(ˆµ(U ) − µ) + O(η).

(31)

(32)

(cid:80)

i∈U W ν

ν(ˆµ(U )−µ) = 1
|U |

For any ν, µ(cid:62)Σ∗
i for W ν
ν(Xi −µi). These are independent, mean-
i
zero, O(1)-bounded random variables, so by Hoeﬀding’s, for any ﬁxed ν, we have that |µ(cid:62)Σ(ˆµ(U ) −
µ)| ≤ t with probability at least 1 − 2 exp(−Ω(|U |t2)). If we union bound over N , then by taking
η, t = O((cid:15)) and |U | satisfying (26) again, (32) and thus (31) will be at most O((cid:15)).
By (29), we thus conclude that (cid:107)B(ˆµ(U )) − B(µ)(cid:107)K ≤ O((cid:15)/k) as claimed.
(Condition (IV)) By Lemma A.4, with probability at least 1 − 2|N | exp (cid:0)−Ω (cid:0)k|U |t2(cid:1)(cid:1), we

(cid:44) µ(cid:62)Σ∗

have that for all Σ ∈ K,

1
|U |

(cid:88)

i∈U

(µi − µ)(cid:62)Σ(Xi − µi)

(µi − µ)(cid:62) ˜Σ(Xi − µi) +

(cid:88)

i∈U

1
|U |

(cid:88)

i∈U

(cid:107)Σ − ˜Σ(cid:107)F · (cid:107)µi − µ(cid:107)2 · (cid:107)Xi − µi(cid:107)2

αν ·

1
|U |

(cid:88)

i∈U

(µi − µ)(cid:62)Σ∗

ν(Xi − µi) + 2ω · η

αν · t + 2ω · η

≤

≤

≤

1
|U |

(cid:88)

ν
(cid:88)

ν

≤ ω · t + 2ω · η

(33)

31

where the ﬁrst step follows by triangle inequality and Cauchy-Schwarz, the second step follows by
the bound on (cid:107)Σ − ˜Σ(cid:107)F guaranteed by Lemma A.1 and the assumption that (cid:107)µi − µ(cid:107)2 ≤ ω, and
the third step holds with the claimed probability by Lemma A.4 and the fact that (cid:107)Σ∗
ν(cid:107)max ≤ O(1)
for all ν by Lemma A.1, and the last step follows by the bound on (cid:80) αν by the guarantees of
√
Lemma A.1. If |U | satisﬁes (26) and η, t = O

log 1/(cid:15)
√
k
For the second part, by the steps leading to (33), a union bound over W , and Fact A.5, with

, the ﬁrst part of Condition (IV) holds.

(cid:18) (cid:15)

(cid:19)

probability at least

1 − 2|N | exp(2(cid:15)|U | log 1/(cid:15)) · exp (cid:0)−Ω (cid:0)(cid:15)k|U |t2(cid:1)(cid:1) ,

we have that

1
|W |

(cid:80)

i∈W (µi − µ)(cid:62)Σ(Xi − µi) ≤ ω · t + 2ω · η for all W .

(cid:18) √

(cid:19)

log 1/(cid:15)
√
k

, so if |U | satisﬁes (26) and η =

Note that 2 log 1/(cid:15) ≤ O(kt2) provided t = Ω
(cid:18) √

(cid:19)

, the second part of Condition (IV) holds.

O

log 1/(cid:15)
√
k

B Netting Over K

In this section we prove Lemma A.1, restated here for convenience:
Lemma A.1. For every 0 < η ≤ 1, there exists a net N ⊂ Rn×n of size O(n3(cid:96)2 log2 n/η)((cid:96) log n+1)2
of matrices such that for every Σ ∈ K, there exists some ˜Σ = (cid:80)
ν ∈ N such that the
following holds: 1) (cid:107)Σ − ˜Σ(cid:107)F ≤ η, 2) (cid:80)

ν αν ≤ 1, and 3) (cid:107)Σ∗
As alluded to in Remark 3.2 and Appendix A, we will use the extra Constraints 3 and 4 in the

ν Σ∗
ν(cid:107)max ≤ O(1).

ν for Σ∗

deﬁnition of K to tighten the proof of Lemma 6.9 from [CLM19] to obtain Lemma A.1 above.

The following well-known trick will be useful.

Lemma B.1 (“Shelling”). If v ∈ Rm satisﬁes (cid:107)v(cid:107)2 ≤ C and (cid:107)v(cid:107)1 = C·
vectors v[1], ..., v[m/k] with disjoint supports for which 1) v = (cid:80)m/k
and 3) (cid:80)m/k

i=1 (cid:107)v[i](cid:107)∞ ≤ 1

k (cid:107)v(cid:107)1 + (cid:107)v(cid:107)∞.

√

k, then there exist k-sparse
i=1 (cid:107)v[i](cid:107)2 ≤ 2C,

i=1 v[i], 2) (cid:80)m/k

Proof. Assume without loss of generality that C = 1. Letting B1 ⊂ [m] be the indices of the
k largest entries of v in absolute value, B2 those of the next k largest, etc., we can write [m] =
B1 (cid:116) · · · (cid:116) Bm/k. For i ∈ [m/k], deﬁne v[i] ∈ Rm to be the restriction of v to the coordinates indexed
by Bi. For any i and j ∈ Bi, |vj| ≤ 1

k (cid:107)v[i − 1](cid:107)1. This immediately implies that

m/k
(cid:88)

i=1

(cid:107)v[i](cid:107)∞ ≤ (cid:107)v(cid:107)∞ +

1
k

m/k
(cid:88)

i=1

(cid:107)v[i](cid:107)1,

yielding 3) above. Likewise, it implies that

(cid:107)v[i](cid:107)2

2 =

(cid:88)

j∈Bi

v2
j ≤ k ·

1
k2 · (cid:107)v[i − 1](cid:107)2

1 =

1
k

(cid:107)v[i − 1](cid:107)2
1.

So (cid:107)v[i](cid:107)2 ≤ (cid:107)v[i − 1](cid:107)1/

√

k and thus

giving 2) above.

m/k
(cid:88)

i=1

(cid:107)v[i](cid:107)2 ≤ (cid:107)v[1](cid:107)2 +

1
√
k

(cid:107)v(cid:107)1 ≤ 2,

32

By rescaling the entries of v in Lemma B.1, we immediately get the following extension to

Haar-weighted norms:

√

Corollary B.2. If v ∈ Rm satisﬁes (cid:107)v(cid:107)2;h ≤ C and (cid:107)v(cid:107)1;h = C ·
vectors v1, ..., vm/k with disjoint supports for which 1) v = (cid:80)m/k
(cid:80)m/k

i=1 vi, 2) (cid:80)m/k

k, then there exist k-sparse
i=1 (cid:107)vi(cid:107)2;h ≤ 2C, and 3)

k (cid:107)v(cid:107)1;h + (cid:107)v(cid:107)∞;h.

i=1 (cid:107)v[i](cid:107)∞;h ≤ 1
We remark that whereas in [CLM19], shelling was applied to the unweighted L1, L2 norms,
and the only L2 information used about v ∈ V n
2 = n, in the sequel we will shell
under the Haar-weighted norms and use the reﬁned bounds on the Haar-weighted norms given by
Constraints 3 and 4 from Deﬁnition 3.1. This will be crucial to getting a net of size exponential in
(cid:96)2 rather than just poly((cid:96)).

(cid:96) was that (cid:107)v(cid:107)2

We now complete the proof of Lemma A.1.

Proof of Lemma A.1. Let s = (cid:96) log n+1, and let m = log n. Let N (cid:48) be an O (cid:0) η
n·s2
norm for all s2-sparse n × n matrices of unit Frobenius norm. Because Ss2−1 has an O (cid:0) η
n·s2
in L2 norm of size O(n · s2/η)s2, by a union bound we have that

(cid:1)-net in Frobenius
(cid:1)-net

|N (cid:48)| ≤

(cid:19)

(cid:18)n2
s2

· O(n · s2/η)s2

= O(n3(cid:96)2 log2 n/η)s2

Take any Σ ∈ K and consider L (cid:44) HΣH (cid:62). By Constraints 2, 3, 4 in Deﬁnition 3.1,

(cid:107)L(cid:107)1,1;h ≤ s2,

(cid:107)L(cid:107)2

F ;h ≤ s2,

and

(cid:107)L(cid:107)max;h ≤ 1.

(34)

We can use the ﬁrst two of these and apply Corollary B.2 to the n2-dimensional vector L to conclude
that L = (cid:80)
j Lj for some matrices {Lj}j of sparsity at most s2 and for which (cid:80)
j (cid:107)Lj(cid:107)F ;h ≤ 2s2
and (cid:80)

s2 (cid:107)Lj(cid:107)1,1;h + (cid:107)Lj(cid:107)max;h.

j (cid:107)Lj(cid:107)max;h ≤ 1

By deﬁnition of the Haar-weighted Frobenius norm, (cid:107)Lj(cid:107)F ≤ n · (cid:107)Lj(cid:107)F,µ, so

(cid:107)Lj(cid:107)F ≤ O(n · s2).

(cid:88)

j

For each Lj, there is some (L(cid:48))j ∈ N (cid:48) such that for ˜Lj (cid:44) (cid:107)Lj(cid:107)F · (L(cid:48))j,

(cid:107)Lj − ˜Lj(cid:107)F ≤ O

(cid:16) η

(cid:17)

n · s2

(cid:107)Lj(cid:107)F .

(35)

We conclude that if we deﬁne ˜L (cid:44) (cid:80)

˜Lj, then (cid:107)L − ˜L(cid:107)F ≤ η.
Now let N (cid:44) H −1N −1(H −1)(cid:62). As Σ = H −1L(H −1)(cid:62) and H −1 is an isometry, if we deﬁne
˜Σj, then we likewise get that (cid:107)Σ − ˜Σ(cid:107)F ≤ η, and clearly ˜Σj ∈ PN

˜Σj (cid:44) H −1 ˜Lj(H −1)(cid:62) and ˜Σ (cid:44) (cid:80)
j
for every j, concluding the proof of part 1) of the lemma.

j

For each ˜Σj, deﬁne

αj (cid:44) (cid:107)Lj(cid:107)max;h/2

(36)

and deﬁne Σj

∗ (cid:44) ˜Σj/αj so that ˜Σ = (cid:80)

j,σ,τ αj · Σj

∗. Note that by part 3) of Corollary B.2 and (34),

(cid:88)

j

αj =

≤

1
2

1
2

(cid:88)

(cid:107)Lj(cid:107)max;h

j
1
s2 (cid:107)L(cid:107)1,1;h + (cid:107)L(cid:107)max;h ≤ 1

33

where in the last step we used the fact that (cid:107)L(cid:107)1,1;h ≤ s2 and (cid:107)L[σ, τ ](cid:107)max;h ≤ 1. This concludes
the proof of part 2) of the lemma.
Finally, we need to bound (cid:107)Σj

∗(cid:107)max. Note ﬁrst that for any matrix J supported only on a
submatrix consisting of entries of L from the rows i (resp. columns j) for which i ∈ Tσ (resp.
j ∈ Tτ ), we have that

(cid:107)H −1J(H −1)(cid:62)(cid:107)max = 2−(m−σ)/2 · 2−(m−τ )/2 · (cid:107)J(cid:107)max =

2(σ+τ )/2
n

(cid:107)J(cid:107)max

because the Haar wavelets {ψσ,j}j (resp. {ψτ,j}j) have disjoint supports and L∞ norm 2−(m−σ)/2
(resp. 2−(m−τ )/2). For general J, by decomposing J into such submatrices, call them J[σ, τ ], we
get by triangle inequality that

(cid:107)H −1J(H −1)(cid:62)(cid:107)max ≤

2(σ+τ )/2
n

(cid:88)

σ,τ

(cid:107)J[σ, τ ](cid:107)max ≤ (cid:107)J(cid:107)max.

(37)

By applying this to J = ˜Σj, we get

(cid:107) ˜Σj(cid:107)max ≤

(cid:16)

(cid:107)H −1Lj(H −1)(cid:62)(cid:107)max + (cid:107)H −1 (cid:16)

Lj − ˜Lj(cid:17)

(H −1)(cid:62)(cid:107)max

(cid:17)

≤ (cid:107)Lj(cid:107)max + (cid:107)Lj − ˜Lj(cid:107)max
≤ (cid:107)Lj(cid:107)max + (cid:107)Lj − ˜Lj(cid:107)F
(cid:17)
≤ (cid:107)Lj(cid:107)max + O

(cid:16) η

(cid:107)Lj(cid:107)F

n · s2

≤ (cid:107)Lj(cid:107)max · (1 + O (η/n))
≤ 2 · (cid:107)Lj(cid:107)max,

where the ﬁrst inequality is triangle inequality, the second inequality follows by (37), the third
inequality follows from monotonicity of Lp norms, the fourth inequality follows from (35), and the
ﬁfth inequality follows from the fact that Lj is s2 sparse.

Recalling (36) and the deﬁnition of Σσ,τ ;j

, we conclude that (cid:107)Σσ,τ ;j

∗

∗

(cid:107)max ≤ O(1) as claimed.

C Sub-Exponential Tail Bounds From Section A

In this section, we provide proofs for Lemmas A.2, A.3, and A.4, restated here for convenience.

Lemma A.2. Let ξ > 0 and let N ⊂ Rn×n be any ﬁnite set for which (cid:107)Σ(cid:107)max ≤ O(1) for all
Σ ∈ N . Let µ1, ..., µN , µ ∈ ∆n satisfy µ (cid:44) 1
N

i=1 µi. Then for Xi ∼ Mulk(µi) for i ∈ [N ],

(cid:80)N

Pr

(cid:34)(cid:12)
(cid:42)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
N

N
(cid:88)

(Xi − µi)⊗2 −

i=1

E
X∼Mulk(µi)

(cid:2)(X − µi)⊗2(cid:3) , Σ

(cid:43)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:35)

(cid:18)

> t ∀ Σ ∈ N

< 2|N | exp

−Ω

(cid:18) N k2t2
1 + kt

(cid:19)(cid:19)

,

where the probability is over the samples X1, · · · , XN .
Lemma A.3. Let ξ > 0 and let N ⊂ Rn×n be any ﬁnite set for which (cid:107)Σ(cid:107)max ≤ O(1) for all
(cid:80)N
Σ ∈ N . For Xi ∼ Mulk(µi) for i ∈ [N ], ˆµ (cid:44) 1
N

i=1 Xi, and µ (cid:44) 1
N

(cid:80)N

Pr (cid:2)(cid:12)
(cid:12)

(cid:10)(ˆµ − µ)⊗2, Σ(cid:11) − E (cid:2)(cid:10)(ˆµ − µ)⊗2, Σ(cid:11)(cid:3)(cid:12)

(cid:12) > t ∀ Σ ∈ N (cid:3) < 2|N | exp

−Ω

i=1 µi,
(cid:18)

(cid:18) N 2k2t2
1 + N kt

(cid:19)(cid:19)

,

where the probability is over the samples X1, · · · , XN .

34

Lemma A.4. Let ξ > 0 and let N ⊂ Rn×n be any ﬁnite set for which (cid:107)Σ(cid:107)max ≤ O(1) for all
Σ ∈ N . Let µ1, ..., µN , µ ∈ ∆n satisfy (cid:107)µi − µ(cid:107)1 ≤ ω for all i ∈ [N ]. For Xi ∼ Mulk(µi) for i ∈ [N ],

Pr

(cid:34)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
N

N
(cid:88)

i=1

(cid:12)
(cid:12)
(µi − µ)(cid:62)Σ(Xi − µi)
(cid:12)
(cid:12)
(cid:12)

(cid:35)

> ω · t ∀ Σ ∈ N

< 2|N | exp (cid:0)−Ω (cid:0)kN t2(cid:1)(cid:1) ,

where the probability is over the samples X1, · · · , XN .

We remark that if we restricted our attention to test matrices of the form Σ = vv(cid:62) for v ∈ {±1}n,
these lemmas would follow straightforwardly from Bernstein’s and the sub-Gaussianity of binomial
distributions.

We will need the following well-known combinatorial fact, a proof of which we include for

completeness in Section C.1

Fact C.1. For any m, r ∈ Z, there are at most O(m)r · r! tuples (i1, ..., i2r) ∈ [m]t for which every
element of [m] occurs an even (possibly zero) number of times.

Central to the proofs of Lemmas A.2 and A.3 is the following sub-exponential moment bound.
We remark that this moment bound would be an immediate consequence of McDiarmid’s if Σ not
only satisﬁed (cid:107)Σ(cid:107)max but was also psd, but because the matrices arising from shelling need not be
psd, it turns out to be unavoidable that we must prove this moment bound from scratch.

In this section, given µ ∈ ∆n, let Dµ denote the distribution over standard basis vectors {ei}

of Rn where for any i ∈ [n], ei has probability mass equal to the i-th entry of µ.
Lemma C.2. Let Σ ∈ Rn×n have entries bounded in absolute value by O(1), and for µ1, ..., µm, µ ∈
If Y1, ..., Ym are independent draws from Dµi respectively, and ˆµ (cid:44)
∆n, let µ (cid:44) 1
m
1
m

i=1 Yi, then for every r ≥ 1, E

(cid:104)(cid:0)(ˆµ − µ)(cid:62)Σ(ˆµ − µ)(cid:1)r(cid:105)

≤ Ω(m)−r · r!.

i=1 µi.

(cid:80)m

(cid:80)m

Proof. Without loss of generality, suppose Σ has entries bounded in absolute value by 1. For
i, i(cid:48) ∈ [m], deﬁne Zi,i(cid:48) (cid:44) (Yi − µi)(cid:62)Σ(Yi(cid:48) − µi(cid:48)). Note that because (cid:107)Yi − µi(cid:107)1 ≤ 2 with probability 1
for all i ∈ [m], and the entries of Σ are bounded in absolute value by 1, |Zi,i(cid:48)| ≤ 4 with probability
1 for all i, i(cid:48) ∈ [m]. We can write E (cid:2)(cid:0)(ˆµ − µ)(cid:62)Σ(ˆµ − µ)(cid:1)r(cid:3) as





E





1
m2r

(cid:88)

i,i(cid:48)∈[m]



r

Zi,i(cid:48)



 =

1
m2r

(cid:88)

(i1,i(cid:48)

1),...,(ir,i(cid:48)
r)





E

r
(cid:89)

j=1



Zij ,i(cid:48)

j

 .

(38)

Now that if there exists some index i ∈ [m] which occurs an odd number of times among
r, then by the fact that the tensor E (cid:2)(Yi − µi)⊗a(cid:3) is identically zero for odd a, we have
1, ..., ir, i(cid:48)
(cid:104)(cid:81)r
. So the nonzero summands on the right-hand side of (38) correspond to indices
r appears an even

i1, i(cid:48)
that E
{(ij, i(cid:48)
number of times. By Fact C.1, there are O(m)r · r! such tuples.

j)}j∈[r] which must satisfy that every index appearing among i1, i(cid:48)

1, ..., ir, i(cid:48)

j=1 Zij ,i(cid:48)

(cid:105)

j

Finally, by the fact that |Zi,i(cid:48)| ≤ 4 with probability 1 for all i, i(cid:48) ∈ [M ], each monomial
is upper bounded by 4r. We conclude that E (cid:2)(cid:0)(ˆµ − µ)(cid:62)Σ(ˆµ − µ)(cid:1)r(cid:3) ≤ 1
m2r · O(m)r ·

j=1 Zij ,i(cid:48)

(cid:105)

(cid:104)(cid:81)r

E
r! · 4r, from which the claim follows.

j

Similarly, a crucial ingredient to the proof of Lemma A.4 is the following moment bound.

Lemma C.3. Let Σ ∈ Rn×n have entries bounded in absolute value by O(1), and suppose µ1, ..., µm, µ ∈
∆n satisfy (cid:107)µi −µm(cid:107)1 ≤ ω for all i ∈ [m]. Then for every r ∈ Z, E (cid:2)(cid:0) 1
is 0 if r is odd and at most O(rω2/m)r/2 otherwise.

i=1(µi − µ)(cid:62)Σ(Yi − µi)(cid:1)r(cid:3)

(cid:80)m

m

35

Proof. It is clear that the r-th moment is zero when r is odd. Henceforth, write r as 2r. Without
loss of generality, suppose Σ has entries bounded in absolute value by 1. For i ∈ [m], deﬁne
Zi (cid:44) (µi − µ)(cid:62)Σ(Yi − µi). Note that because (cid:107)Yi − µi(cid:107)1 ≤ 2 with probability 1 for all i ∈ [m], and
the entries of Σ are bounded in absolute value by 1, |Zi| ≤ 2ω with probability 1 for all i ∈ [m].
We can write E

i=1(µi − µ)(cid:62)Σ(Yi − µi)(cid:1)2r(cid:105)

(cid:80)m

(cid:104)(cid:0) 1
m

as





E





1
mr

(cid:88)

i∈[m]



r

Zi



 =

1
mr

(cid:88)





E

2r
(cid:89)



Zij

 .

i1,....,i2r

j=1

As in the proof of Lemma C.2, the only nonzero summands correspond to tuples (i1, ..., i2r) such
that every element of [m] appears an even (possibly zero) number of times. By Fact C.1, there are
at most O(m)r · r! such tuples, from which we can complete the proof.

Lemmas A.2 and A.3 will now follow as consequences of Lemma C.2 and the following standard

tail bound for random variables with sub-exponential moments:

Fact C.4. Let Z1, ..., Zm be random variables for which there exists a constant ν > 0 such that
E[Zr

i ] ≤ 1

2 νr · r! for all integers r ≥ 1 and i ∈ [m]. Then
(cid:34)(cid:12)
(cid:35)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Zi − E[Z]

m
(cid:88)

1
m

> t

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Pr

i=1

−Ω

(cid:16) mt2
ν2+νt

(cid:17)

.

≤ 2e

Similarly, Lemma A.4 will follow as a consequence of Lemma C.3 and the following standard

tail bound for random variables with sub-Gaussian moments:

Fact C.5. Let Z1, ..., Zm be random variables for which there exists a constant ν > 0 such that
E[Zr

i ] ≤ (r · ν2)r/2 for all integers r ≥ 1 and i ∈ [m]. Then
(cid:34)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
Zi − E[Z]
(cid:12)
(cid:12)

m
(cid:88)

1
m

> t

Pr

(cid:35)

i=1

≤ 2e−Ω(mt2/ν2).

Proof of Lemma A.2. This follows by taking m = k in Lemma C.2 and m = N in Fact C.4 and
noting that for any Σ ∈ N , (cid:107)Σ(cid:107)max ≤ O(1) by Lemma A.1.

Proof of Lemma A.3. This follows by taking m = kN in Lemma C.2 and m = 1 in Fact C.4 and
noting that for any Σ ∈ N , (cid:107)Σ(cid:107)max ≤ O(1) by Lemma A.1.

Proof of Lemma A.4. This follows by taking m = k in Lemma C.3 and m = N in Fact C.5 and
noting that for any Σ ∈ N , (cid:107)Σ(cid:107)max ≤ O(1) by Lemma A.1.

C.1 Proof of Fact C.1

Proof. To count the number N ∗ of such tuples (i1, ..., i2r), for every 1 ≤ s ≤ r let Ns denote the
(cid:1) ≤
number of tuples β ∈ {2, 4..., 2r}s for which (cid:80)s
( 3es
2r )r. Now note that to enumerate N ∗, we can 1) choose the number 1 ≤ s ≤ min(m, r) of unique
indices among {ij}, 2) choose a subset S of [m] of size s, 3) choose one of the Ns tuples β, and 4)

i=1 βi = 2r. By balls-and-bins, Ns = (cid:0)r+s−1

r

36

choose one of the (cid:0)
For convenience, let r(cid:48) (cid:44) min(m, r). We get an upper bound of

2r
β1,...,βs

(cid:1) ways of assigning index S1 to β1 indices in {ij}, S2 to β2 indices, etc.

(cid:19)

(cid:18)m
s

· Ns ·

(cid:18) 2r

(cid:19)

β1, ..., βs

N ∗ ≤

≤

≤

≤

min(m,r)
(cid:88)

s=1

min(m,r)
(cid:88)

s=1
mr(cid:48)
(r(cid:48))!
mr
(r)!

(cid:19)r

· (2s)!

(cid:18) 3es
ms
s!
2r
(cid:18) 3er(cid:48)
2r

(cid:19)r

· r(cid:48) ·

· (2r(cid:48))!

· r · (3e/2)r · (2r)!

= mr · r · (3e/2)r ·

≤ O(m)r · r!,

(cid:19)

(cid:18)2r
r

· r!

where in the second step we used basic bounds on binomial and multinomial coeﬃcients together
with the above bound on Ns, in the third step we used the fact that the summands are increasing
in s, and in the fourth step we used this fact along with the fact that r(cid:48) ≤ r by deﬁnition.

37

