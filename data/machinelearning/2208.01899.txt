2
2
0
2

g
u
A
3

]

G
L
.
s
c
[

1
v
9
9
8
1
0
.
8
0
2
2
:
v
i
X
r
a

Understanding Adversarial Imitation Learning in Small
Sample Regime: A Stage-coupled Analysis

Tian Xu *1, Ziniu Li∗

2,3, Yang Yu†1, and Zhi-Quan Luo†2,3

1National Key Laboratory for Novel Software Technology, Nanjing University
2The Chinese University of Hong Kong, Shenzhen
3Shenzhen Research Institute of Big Data

August 4, 2022

Abstract

Imitation learning learns a policy from expert trajectories. While the expert data is believed to be crucial
for imitation quality, it was found that a kind of imitation learning approach, adversarial imitation learning
(AIL), can have exceptional performance. With as little as only one expert trajectory, AIL can match the
expert performance even in a long horizon, on tasks such as locomotion control. There are two mysterious
points in this phenomenon. First, why can AIL perform well with only a few expert trajectories? Second,
why does AIL maintain good performance despite the length of the planning horizon? In this paper, we
theoretically explore these two questions. For a total-variation-distance-based AIL (called TV-AIL), our
) on a class of instances abstracted from
analysis shows a horizon-free imitation gap
is the state space size for a tabular Markov decision process, and N is
locomotion control tasks. Here
the number of expert trajectories. We emphasize two important features of our bound. First, this bound is
meaningful in both small and large sample regimes. Second, this bound suggests that the imitation gap of
TV-AIL is at most 1 regardless of the planning horizon. Therefore, this bound can explain the empirical
observation. Technically, we leverage the structure of multi-stage policy optimization in TV-AIL and present
a new stage-coupled analysis via dynamic programming1.

min

/N

|S|

|S|

(cid:112)

O

1,

{

{

}

(

1 Introduction

Imitation learning approaches train a policy from expert trajectories [Argall et al., 2009, Hussein et al., 2017,
Osa et al., 2018]. The classical algorithm behavioral cloning (BC) implements the direct policy matching via
supervised learning (i.e., maximum likelihood estimation) [Pomerleau, 1991]. This approach is simple and is
widely applied in various applications [Ross et al., 2011, Silver et al., 2016, Levine et al., 2016]. However, BC
is known to suffer compounding errors [Ross et al., 2011]. That is, decision errors may accumulate over time
steps, leading to poor performance. This issue becomes severe when expert trajectories are limited, and the
planning horizon (i.e., the maximum length of a trajectory) is long.

Later on, generative adversarial imitation learning (GAIL) [Ho and Ermon, 2016] is introduced. Unlike
the policy matching in BC, GAIL performs the state-action distribution matching2. According to the principle

*Equal contribution. Author ordering is determined by coin ﬂip. Email: xut@lamda.nju.edu.cn and ziniuli@link.cuhk.edu.cn
†Corresponding author. Email: yuy@nju.edu.cn and luozq@cuhk.edu.cn
1The ﬁrst version of this paper is available at https://arxiv.org/abs/2106.10424v3. This paper presents the horizon-free

guarantee part in the early version with clean-up.

2The difference between these two matching principles can be understood as follows: state-action distribution is a marginal
distribution, whereas policy is a conditional distribution. When inﬁnite expert trajectories are available, both two matching principles
can lead to the expert policy at optimality. However, they are different if expert trajectories are ﬁnite.

1

 
 
 
 
 
 
of state-action distribution matching, a large amount of adversarial imitation learning (AIL) methods are
designed3 and they are shown to have exceptional performance in practice [Fu et al., 2018, Kostrikov
et al., 2019, 2020, Brantley et al., 2020]. For example, for locomotion control tasks with 1 expert trajectory
and a planning horizon of 1000, it is empirically observed that AIL methods can nearly match the expert
performance whereas BC cannot [Ho and Ermon, 2016]. There are two mysterious points in this phenomenon.
First, why can AIL methods have superior performance with limited expert trajectories? Second, why do
AIL methods maintain good performance despite the length of the planning horizon? In fact, the ﬁrst
question has been raised in [Ghasemipour et al., 2019] and some hypotheses are proposed and evaluated by
experiments. In this paper, we theoretically explore these two questions.

In fact, we notice that lots of theoretical studies about AIL algorithms have been conducted [Sun et al.,
2019, Wang et al., 2020, Zhang et al., 2020, Rajaraman et al., 2020, 2021b, Xu et al., 2021, Swamy et al., 2021,
Liu et al., 2021b]. However, even in the simple tabular and episodic MDP (Markov decision process) setting,
the above questions are still not fully answered. We elaborate on the limitation of existing studies as follows.
The seminal works [Abbeel and Ng, 2004, Syed and Schapire, 2007] already have presented a reduction-and-
estimation framework to analyze AIL methods, which is also used in recent works [Wang et al., 2020, Xu et al.,
2020, Rajaraman et al., 2020, Liu et al., 2021b]. In particular, for a tabular and episodic MDP with ﬁnite states
and actions, it is proved that a representative AIL method called FEM [Abbeel and Ng, 2004] has a worst-case
imitation gap V(πE)
is the
|S||A|
action space size, H is the planning horizon, N is the number of expert trajectories, V(π) denotes the sum of
rewards obtained by a policy π, and the expectation is taken over the randomness in collecting N expert
)
trajectories. Another AIL method called GTAL has a similar imitation gap bound
}
[Syed and Schapire, 2007]. One may quickly ﬁnd that such bounds are meaningful in the large sample
regime (say N (cid:37)
); otherwise, the ﬁrst term dominates, and the bound becomes trivial4 (in this paper,
(cid:37) and (cid:45) denote greater or less than up to constants, respectively). As a consequence, these bounds cannot
explain the superior performance of FEM and GTAL in the small sample regime. Furthermore, these bounds
suggest the performance of FEM and GTAL may degenerate for long-horizon tasks, which is rarely observed
in practice.

is the state space size,

E[V(π)]

). Here

|S||A|

|S||A|

/√N

(min

(min

H, H

H, H

≤ O

/N)

|A|

|S|

(cid:112)

O

−

}

{

{

Let us brieﬂy discuss why the reduction-and-estimation framework has the above issues. We realize
that there are two main steps in this framework. In the ﬁrst step, the imitation gap is proved to be upper
bounded by the statistical estimation error of the expert state-action distribution. The second step controls
the estimation error via proper concentration inequalities. Through these two steps, the reduction-and-
estimation framework suggests that if the statistical estimation error is small, the imitation gap is small.
However, we believe this worst-case analysis could be loose for some practical tasks, as it upper bounds
the multi-step decision errors in a decoupled and independent way. To support this claim, we conduct
experiments and observe that even though the statistical estimation error is large, the imitation gap of AIL
methods could be very small; see the evidence in Table 5. In this paper, we establish a new theoretical
analysis that can explain why AIL methods can perform well even if the estimation error is large.

1.1 Our Contribution

First, we carefully construct a class of instances5 called RBAS MDPs (refer to Assumption 1). For such
instances, there exist reachable bad absorbing states (RBAS). We highlight that RBAS MDPs are abstracted
from practical tasks, including locomotion control and Atari games. For such tasks, if the agent makes a
wrong action, it goes to a terminal state with a zero reward (refer to the illustration in Figure 1). Importantly,
such a characteristic is considered in RBAS MDPs (refer to the illustration in Figure 2). Numerically, we
validate that the well-accepted empirical observations (i.e., AIL methods can perform well for long-horizon

3These methods implement the state-action distribution matching in an adversarial manner (i.e., min-max optimization), so usually
they are named after adversarial imitation learning. These methods differ in measuring the discrepancy between the agent’s state-action
distribution and the expert’s state-action distribution.

4Note that one-step reward is assumed to be between 0 and 1 and hence the maximum imitation gap is at most H.
5An imitation learning instance refers to the underlying MDP and associated expert policy. When the context is clear, we refer to an

instance as an MDP.

2

tasks in the small sample regime) also hold on RBAS MDPs. This suggests that RBAS MDPs can serve as a
mathematical model for studying algorithm properties of AIL methods.

Now, we provide an informal preview of our main result.

Theorem 1 (Informal Statement of Theorem 4). There is an AIL method with the total variation distance (called
) for any instance in the class of RBAS
(min
TV-AIL) that achieves the horizon-free imitation gap
MDPs.

/N

|S|

(cid:112)

O

1,

{

}

O

{

|S|

}

We remark that Theorem 1 is meaningful in both small sample and large sample regimes because the
imitation gap bound is at most 1, which is smaller than the maximal value H. Furthermore, Theorem 1
implies that the performance of TV-AIL does not degenerate for long-horizon tasks. These two theoret-
ical implications are well-aligned with empirical observations of TV-AIL. We also clarify that this good
result is not due to the total variation distance. Instead, it is based on a new theoretical analysis. In fact,
the mentioned reduction-and-estimation framework would suggest that the imitation gap of TV-AIL is
), which cannot explain the superior performance of TV-AIL.

(min

H, H

/N

(cid:112)

Technically, we obtain Theorem 1 by introducing a new stage-coupled analysis via dynamic programming,
which is different from the mentioned reduction-and-estimation-based analysis. To see the difference, we
recall that AIL methods solve a multi-stage optimization6. Importantly, we point out that time-dependent
policies could be coupled in TV-AIL, thanks to the state-action distribution in its optimization objective7.
As discussed, the existing analysis translates the imitation gap to the statistical estimation error, in which
multi-step decision errors are analyzed in a decoupled way. In contrast, we leverage the coupling information
to show that guidance from future time steps can help TV-AIL recover the expert action on non-visited states
(refer to Proposition 1). In this way, the horizon-free imitation gap bound in Theorem 1 is achieved.

Based on the above result, one may wonder whether the horizon-free imitation gap can hold for any
) and a matching

instance. To answer this question, we establish a lower bound Ω(min
upper bound for TV-AIL.

H, H

/N

|S|

(cid:112)

}

{

Theorem 2 (Informal Statement of Proposition 4 and Theorem 5). There exists a class of instances (refer to
Assumption 2) such that the worst-case imitation gap of TV-AIL

) is tight.

(min

H, H

/N

(cid:112)

O

{

|S|

}

We comment that Theorem 2 does not contradict Theorem 1, because the instance classes in two theorems
have no overlap. Notably, Theorem 2 says that in both small sample and large sample regimes, the imitation
gap of TV-AIL has a linear dependence on H in the worst case. The intuition is that for the hard instances
satisfying Assumption 2, each state is absorbing, and there is no connection between states, so the multi-stage
policy optimization essentially reduces to H independent imitation problems. This property is quite different
from that in RBAS MDPs. For our purpose, Theorem 2 suggests that without reasonable assumptions about
MDPs, the nice horizon-free imitation gap is impossible.

This paper is organized as follows. First, Section 2 reviews the previous work and Section 3 introduces the
background of imitation learning. Next, Section 4 presents the main result about the horizon-free imitation
gap of TV-AIL for RBAS MDPs. Subsequently, Section 5 discusses its worst-case performance. Finally,
conclusive remarks are given in Section 6.

2 Related Work

In the past years, substantial efforts have been put into understanding the classical algorithm behavioral
cloning (BC) [Ross and Bagnell, 2010, Syed and Schapire, 2010, Ross et al., 2011, Xu et al., 2020, Rajaraman

6We comment that the terminology of multi-stage optimization (rather than multi-step optimization) is usually used in the literature
on operational research and dynamic programming. In fact, MDPs are sometimes called multi-stage decision processes; refer to the
historical remark in [Sutton and Barto, 2018, Chapter 3]. We adopt such terminology. In our context, multi-stage optimization means we
aim to obtain a non-stationary policy (i.e., H time-dependent policies).

7In contrast, there is no coupling structure in BC’s optimization objective; refer to (2). As a result, time-dependent policies in BC are

learned in a decoupled way. This is one reason why BC may suffer compounding errors.

3

et al., 2020, 2021a,b, Swamy et al., 2021]. The seminal work [Ross and Bagnell, 2010] showed that BC might
have compounding errors at the population level (informally speaking, population means that there are inﬁ-
)
nite expert trajectories). Recently, [Rajaraman et al., 2020] established the imitation gap
}
when expert trajectories are ﬁnite. Furthermore, from an information-theoretic perspective, [Rajaraman et al.,
2020] made a remarkable step in proving a lower bound Ω(H2
/N) for ofﬂine imitation8. This result
(H2) term in the imitation gap
suggests that for all ofﬂine imitation learning algorithms, including BC, the
is inevitable (i.e., a fundamental limit in the ofﬂine setting). Similar conclusions also hold for discounted and
inﬁnite-horizon MDPs [Xu et al., 2021].

H2/N

(min

|S|

|S|

H,

O

O

{

As for adversarial imitation learning (AIL) methods, the principle of state-action distribution matching
dates back to apprenticeship learning algorithms [Abbeel and Ng, 2004, Syed and Schapire, 2007, Syed et al.,
2008, Ziebart et al., 2008]. In particular, classical algorithms FEM [Abbeel and Ng, 2004] and GTAL [Syed
and Schapire, 2007] use (cid:96)2-norm-based and (cid:96)∞-norm-based metrics to measure the state-action distribution
discrepancy, respectively. Moreover, a reduction-and-estimation-based analysis was developed in [Abbeel
and Ng, 2004, Syed and Schapire, 2007], and this analysis showed that the imitation gap is upper bounded by
) for FEM and GTAL. This kind of analysis is widely used in recent works

H, H poly(

)/√N

(min

O
[Wang et al., 2020, Rajaraman et al., 2020, Xu et al., 2021, Liu et al., 2021b].

|S||A|

{

}

A recent milestone in AIL is the introduction of generative adversarial imitation learning (GAIL) [Ho
and Ermon, 2016]. Unlike FEM and GTAL, GAIL implements the Jensen-Shannon divergence to measure the
state-action distribution discrepancy. Moreover, GAIL leverages powerful neural networks to adaptively
learn feature representations. For locomotion control tasks, AIL methods (e.g., GAIL, FEM, and GTAL)
are found to match the expert performance for long-horizon tasks (H = 1000) in the small sample regime
(N = 1) [Ho and Ermon, 2016]. For Atari games like Pong and Breakout, a similar phenomenon is also
observed [Yu et al., 2020, Cai et al., 2021]. These empirical results attract signiﬁcant attention. Since the work
of [Ho and Ermon, 2016], many empirical advances have been achieved for AIL methods [Fu et al., 2018,
Kostrikov et al., 2019, 2020, Brantley et al., 2020, Dadashi et al., 2021, Liu et al., 2021a].

O

On the theoretical side, the understanding of why AIL methods can perform well is limited. [Ghasemipour
et al., 2019] conjectured that “for common MDPs of interest, . . . , encouraging policies to explicitly match
expert state marginals is an important learning criterion”. However, a precise characterization of interesting
MDPs is missing, and the working mechanism is unclear (i.e., why does matching expert state marginals
yield good policies?). Some recent works [Xu et al., 2020, Swamy et al., 2021] provided an answer by
showing that the imitation gap of a class of AIL methods has a linear dependence on the planning horizon
(H2) in BC’s imitation gap, [Xu et al., 2020, Swamy et al., 2021]
H. Compared with the quadratic term
argued that the imitation gap of AIL methods could have a better dependence on H. Nevertheless, this good
result for AIL methods is true at the population level, which is different from the practical case where expert
trajectories are ﬁnite. For our purpose, one can use the reduction-and-estimation analysis to extend the
result in [Xu et al., 2020, Swamy et al., 2021]. Then, the obtained imitation gap bound is similar to that of
FEM and GTAL (see, e.g., [Xu et al., 2021]). In addition, [Rajaraman et al., 2020] proposed an AIL algorithm
), compared
called MIMIC-MD, which achieves a better imitation gap
with BC and classical AIL methods (e.g., FEM and GTAL). We mention that the improvement of MIMIC-MD
is based on a more accurate estimation of the expert state-action distribution; see [Rajaraman et al., 2020] for
more discussion. We highlight that this improvement holds in the large sample regime, but we care about
algorithmic behaviors in the small sample regime. On the other hand, an information-theoretic lower bound
Ω(H
/N) is established in [Rajaraman et al., 2020], which applies to AIL methods. We note that this lower
bound holds in the large sample regime, whereas the superior performance of AIL methods can be observed
in the small sample regime. Therefore, a gap separates theory from practice.

H, H3/2

/N, H

(min

/N

|S|

|S|

|S|

(cid:112)

O

}

{

In this paper, we focus on the setting of tabular MDPs, where there is no function approximation. We
notice that [Cai et al., 2019] and [Liu et al., 2021b] studied AIL methods with linear function approximation
while the neural network approximation case is studied in [Wang et al., 2020, Zhang et al., 2020, Xu et al.,
in the
2021]. The main message in this direction is that under structural assumptions, the dependence on

|S|

8“Ofﬂine” means that the agent only has access to a dataset collected by the expert policy. That is, the agent does not know the

transition function and cannot interact with the environment.

4

imitation gap can be improved to the inherent dimension d with function approximation. Nevertheless, we
mainly study algorithmic behaviors in terms of the dependence on H, which is usually unrelated to function
approximation. One may still worry that the function approximation is crucial to the superior performance
of AIL methods. To address this concern, we show that AIL methods can recover the expert policy with
one expert trajectory on constructed tabular MDPs, suggesting that the function approximation is not a key
factor for our research problem.

3 Preliminary

This section introduces the Markov decision process, imitation learning setup, and representative algorithms,
including behavioral cloning and adversarial imitation learning.

3.1 Episodic Markov Decision Process

,

,

S

P

=

A

· · ·

P1,

π1,

= (

and

, r, H, ρ). Here

, PH}
{
[H]9. Similarly, r =

In this paper, we consider episodic Markov decision process (MDP), which can be described by the tuple
are the state and action space, respectively. H is the planning horizon
speciﬁes the non-stationary transition function of
sh, ah) determines the probability of transiting to state sh+1 conditioned on
speciﬁes the reward function of
[H]. A non-stationary policy
s) gives the probability

M
A
P
S
and ρ is the initial state distribution.
this MDP; concretely, Ph(sh+1|
state sh and action ah in time step h, for h
∈
this MDP; without loss of generality, we assume that rh :
) and ∆(
π =
, where πh :
of selecting action a on state s in time step h, for h

[H].
The sequential decision process runs as follows: in the beginning of an episode, the environment is reset
sh);
sh, ah) and

A
∈
to an initial state according to ρ; then the agent observes a state sh and takes an action ah based on πh(ah|
consequently, the environment makes a transition to the next state sh+1 according to Ph(sh+1|
sends a reward rh(sh, ah) to the agent. This episode ends after H repeats.
The quality of a policy is measured by its policy value (i.e., the expected long-term return):
(cid:21)

) is the probability simplex, πh(a

, rH}
· · ·
[0, 1], for h

{
S × A →

, πh}

S →

∆(

· · ·

r1,

A

∈

{

|

rh(sh, ah)

s1 ∼

ρ; ah ∼

|

πh(

sh),

·|

sh+1 ∼

Ph(

·|

sh, ah),

h

∀

∈

[H]

.

V(π) :=E

(cid:20) H
∑
h=1

To facilitate later analysis, we introduce the state-action distribution induced by a policy π:
s1 ∼

h (s, a) :=P(sh = s, ah = a
dπ

s(cid:96)), s(cid:96)+1 ∼

ρ, a(cid:96) ∼

s(cid:96), a(cid:96)),

πh(

P(cid:96)(

∈

·|

·|

∀

(cid:96)

|

[h]).

In other words, dπ
according to the deﬁnition, we obtain the dual form of policy value [Puterman, 2014]:

h (s, a) quantiﬁes the visitation probability of state-action pair (s, a) in time step h. Then

V(π) =

H
∑
h=1

∑
(s,a)

dπ
h (s, a)rh(s, a).

(1)

This formula will be used in the analysis of AIL methods. Likewise, we can deﬁne the state distribution
dπ
h (s). According to the deﬁnition, we have dπ
h (s, a). Unless mentioned, when we use the symbol
dπ
h , we mean the state-action distribution.

h (s) = ∑a dπ

3.2

Imitation Learning

The goal of imitation learning is to learn a high quality policy directly from expert demonstrations. To this
end, we often assume there is a (nearly optimal) expert policy πE that could interact with the environment to
generate a dataset (i.e., N trajectories of length H):
, sH, aH) ; s1 ∼

tr = (s1, a1, s2, a2,

sh), sh+1 ∼

ρ, ah ∼

sh, ah),

πE
h (

Ph(

[H]

· · ·

=

D

∈

(cid:110)

(cid:111)

·|

·|

∀

h

.

9[x] denotes the set of integers from 1 to x.

5

Note that each trajectory is independently obtained by rolling out the expert policy πE. Then, the learner can
use the dataset
to mimic the expert and to obtain a good policy. The quality of imitation is measured by
the (expected) imitation gap:

D

where the expectation is taken over the randomness in collecting N expert trajectories (keep in mind that π
is a random variable that depends on dataset
). We hope a good learner can perfectly imitate the expert so
that the imitation gap is small. For analysis purpose, without loss of generality, we assume the expert policy
is deterministic, which is widely used in the literature [Xu et al., 2020, Rajaraman et al., 2020, 2021b].

D

V(πE)

E [V(π)] ,

−

3.3 Behavioral Cloning

Behavioral cloning [Pomerleau, 1991] is a classical ofﬂine algorithm to solve the imitation learning task. Its
main idea is to perform the maximum likelihood estimation for the expert trajectory. Specially, the likelihood
for a trajectory tr is

P (tr) = ρ(s1)π1(a1|

s1)

H
1
∏
−
h=1

Ph(sh+1|

sh, ah)πh+1(ah+1|

sh+1).

Then, we obtain that

log (πh(ah|
where the constant term is unrelated to policy π. By extending this idea to N expert trajectories in the given
dataset

, we obtain the following optimization problem for BC:

sh)) + constant,

log (P (tr)) =

H
∑
h=1

D

where Π is the set of stochastic policies. In the tabular setting, the optimal solution could be

H
∑
h=1

∑
(sh,ah)

max
Π
π
∈

∈D

log (πh(ah|

sh)) ,

πBC

h (a

|

s) =

(cid:40) # trh(
,
·
·
∑
# trh(
a(cid:48)
·
1

·

)=(s,a)
,

)=(s,a(cid:48))

) = s > 0

if # trh(
otherwise

·

(2)

(3)

|A|

,

·

·

) = (s, a) (# trh(

) = s) refers to the number of trajectories such that their state-action pairs
Here # trh(
(states) are equal to (s, a) (s) in time step h. That is, πBC
s) computes the empirical conditional distribution
h (a
for visited states and the uniform distribution for non-visited states. Since BC does not know the expert
action on non-visited states, it may suffer compounding errors issue [Ross and Bagnell, 2010]. On the
theoretical side, the imitation gap of BC is analyzed in [Rajaraman et al., 2020].

·

|

Theorem 3 ([Rajaraman et al., 2020]). For any tabular and episodic MDP, given N expert trajectories, the imitation
gap of BC is

H2/N

(min

H,

).

O

{

|S|

}

3.4 Adversarial Imitation Learning

Adversarial imitation learning (AIL) is a class of algorithms that perform the state-action distribution
matching. For instance, GAIL [Ho and Ermon, 2016] uses the Jensen-Shannon (JS) divergence to measure the
state-action distribution discrepancy:

(cid:18)

DJS

H
∑
h=1

min
Π
π
∈

dπ
h (

,

·

·

), (cid:100)dπE
h (

)

,

·

·

(cid:19)

,

6

where for two distributions p and q on the set
DKL(p, q) = ∑x p(x) log(p(x)/q(x)). Moreover, (cid:100)dπE
h
∑tr

X

, DJS(p, q) = DKL(p, (p + q)/2) + DKL(q, (p + q)/2), where
is the estimation of the marginal distribution dπE
h :
I

) = (s, a)

(4)

(cid:100)dπE
h (s, a) :=

∈D

{

·

tr(

,
·
N
) = (s, a)

}

,

is the indicator function, and ∑tr

where I
counts the number of expert trajectories
,
·
that have the state-action pair (s, a) in time step h. If we consider the dual form of the JS divergence, we
recover the common min-max formulation for GAIL:

{·}

tr(

∈D

}

{

I

·

min
Π
π
∈

max
c
∈CJS

E

H
∑
h=1

(s,a)

dπ
h

∼

[log ch(s, a)] + E

(s,a)

(cid:100)dπE
h

∼

[log(1

−

ch(s, a))] ,

where
formulation can be implemented with neural networks in practice.

CJS is the set of functions ch :

S × A →

(0, 1), which is often called discriminator. This min-max

In this paper, we mainly consider the total variation (TV) distance to measure the state-action distribution

discrepancy, which leads to the following formulation:

H
∑
h=1

min
Π
π
∈

∑

(s,a)

∈S×A

dπ
h (s, a)

|

(cid:100)dπE
h (s, a)

,

|

−

(5)

We call such an approach TV-AIL (total-variation-distance-based AIL) and use πAIL to denote any optimal
solution to (5). Similarly, there exists a min-max formulation for TV-AIL:

max
c
∈CTV
CTV is the set of functions ch :

where
with GAIL for practical tasks.

min
Π
π
∈

E

H
∑
h=1

(s,a)

dπ
h

∼

[ch(s, a)]

E

−

(s,a)

(cid:100)dπE
h

∼

[ch(s, a)] ,

(6)

S × A →

[

−

1, 1]. We note that TV-AIL has comparative performance

In this paper, we analyze TV-AIL in the known-transition setting. In this case, it is easy to compute dπ, so
the policy optimization problem is well-deﬁned10. This setting has been considered in the existing literature
[Rajaraman et al., 2020, 2021a, Xu et al., 2021]. In addition, our analysis of TV-AIL can be extended to other
AIL methods with additional efforts, but we omit this for the sake of clarity. Speciﬁcally, the main difference
between AIL methods is the divergence function (in terms of measuring the discrepancy of state-action
distributions). As mentioned, FEM and GTAL use the (cid:96)2-norm-based and (cid:96)∞-norm-based divergences,
respectively, while the (cid:96)1-norm-based divergence is considered in TV-AIL. Nevertheless, all norms are
“equivalent” in the ﬁnite dimension space in the sense that they can be upper bounded mutually [Yosida,
2012]. JS divergence and TV distance are connected via Pinsker’s inequality; see [Xu et al., 2020] for more
discussion. To this end, AIL methods should have similar algorithm properties, and we omit the general
analysis for the sake of clarity.

4 A Horizon-free Imitation Gap of TV-AIL

In this section, we present the main conclusion in this paper: TV-AIL can achieve the horizon-free imitation
gap for MDPs abstracted from tasks like locomotion control. To prepare, we ﬁrst review two empirical
observations from the literature [Ho and Ermon, 2016, Ghasemipour et al., 2019, Yu et al., 2020, Cai et al.,
2021]. The ﬁrst empirical observation is about the sample size.

Empirical Observation 1. For practical tasks (e.g., locomotion control), with a few expert trajectories (e.g., 1 expert
trajectory), AIL methods (e.g., TV-AIL) can achieve a small imitation gap, whereas BC cannot.

10For tasks like locomotion control from the MuJoCo benchmark and video games from the Atari benchmark, the transition function
is unknown, but interaction is allowed. In this scenario, the agent can roll out its policy to estimate dπ. As long as this estimation is
accurate, results in this paper should hold similarly.

7

Figure 1: An illustration of the Hopper task in the MuJoCo locomotion benchmark. Top row: by taking
the expert action, the robot can adequately jump forward with a positive reward. Bottom row: by taking a
wrong action (non-expert action), the robot falls and encounters a bad terminal (absorbing) state with a zero
reward.

Table 1: Imitation gap on Hopper with H = 1000. We report the mean of imitation gap with the standard
deviation over 5 independent experiments (same with Table 2).

N = 1

N = 4

N = 7

N = 10

BC
TV-AIL

2543.02
33.23

138.76
36.01

±
±

2885.41
5.78

118.13
9.21

±
±

2224.67
15.87

−

±
±

296.09
34.80

1396.51
7.44

307.72

±
33.90
±

For completeness, we provide the experiment result in Table 1 to support Empirical Observation 1. This
result suggests that AIL methods like TV-AIL can work well in the small sample regime. Another empirical
observation is about the planning horizon.

Empirical Observation 2. For practical tasks (e.g., locomotion control), even if the planning horizon is large (e.g.,
H = 1000), with limited expert trajectories (e.g., 1 expert trajectory), AIL methods (e.g., TV-AIL) can achieve a small
imitation gap.

The evidence is provided in Table 2. In fact, this empirical observation is less mentioned in the literature,
except for [Xu et al., 2020, 2021]. In [Xu et al., 2020, Figure 2], it is empirically shown that the performance of
AIL methods is less affected by the planning horizon, but there is no rigorous theoretical explanation in [Xu
et al., 2020]. We believe this empirical observation is vital to help us understand the algorithmic behaviors of
AIL methods.

Table 2: Imitation gap on Hopper with N = 1.

H = 100

H = 500

H = 1000

H = 2000

BC

2.56
TV-AIL 15.86

5.50
14.60

±
±

571.38
5.54

−

±
±

253.67
21.65

2543.02
33.23

138.76
36.01

±
±

6241.36
31.75

−

±
±

115.84
118.87

It is interesting to explore the mysteries in the above observations theoretically. Nevertheless, we realize
that this is difﬁcult because many factors (e.g., environment preprocessing, neural network architectures,
optimizers, etc.) are essential for superior performance [Orsini et al., 2021]. However, capturing all these

8

expert actionexpert actionbad actionany actionfactors in a simple and intuitive theory is hard. In this paper, we provide an answer in the tabular setting. In
this scenario, the expert policy can be well approximated, and efﬁcient computation procedures exist, so
factors like approximation and optimization errors do not matter. Instead, we can focus on the statistical
property. To start with, we investigate a class of MDPs with structural transitions.

4.1 RBAS MDPs

In this part, we introduce a class of tabular and episodic MDPs called RBAS MDPs, which will be used to
study algorithmic behaviors of TV-AIL.

Assumption 1. For a tabular and episodic MDP and an expert policy, we assume that

• State space is divided into the sets of “good” states and “bad” states, i.e.,

=

S

G

S

∪ S

B,

G

S

∩ S

B = ∅.

• For any good state, a1 is the expert action with a reward 1 and the others are non-expert actions with a reward

0.11 For any bad state, all actions have reward 0.

• For action a1, we have for any state s, s(cid:48) ∈ S
• For action a

= a1, we have for any state s, s(cid:48) ∈ S
• All bad states only have transitions to themselves, i.e., for any b

∈
G and h

G and h

∈

[H], Ph(s(cid:48)|

s, a1) > 0.

s, a) = 0.

[H], Ph(s(cid:48)|
B, a

∈ S

1.

and h

∈

[H], ∑

B Ph(b(cid:48)|

b(cid:48)∈S

b, a) =

∈ A

We mention two crucial features of instances satisfying Assumption 1. First, only by taking expert action,
the agent can maintain a good status of visiting good states. Second, once visiting a bad state, the agent
will get stuck in a bad state no matter which action is taken. We note that the second feature is crucial and
discuss how to relax the ﬁrst assumption in Section 4.3. Thanks to the second feature, instances satisfying
Assumption 1 are referred to as RBAS MDPs (RBAS means reachable bad absorbing states).

A simple example of RBAS MDPs with three states and two actions is shown in Figure 2. For this MDP, s1
and s2 are good states, while b is a bad state. Here we use the superscript to indicate the state index, which
can avoid the confusion with the time step in the subscript. Since there are only two actions, we use colors,
rather than the superscript, to distinguish them: a is the expert action with a reward 1 and a is the non-expert
action with a reward 0. Besides, H = 2 is considered in this MDP and transition probabilities are indicated
by digits on the arrows in Figure 2. The initial state distribution ρ is

ρ = (ρ(s1), ρ(s2), ρ(b)) =

(cid:18) 1
2

,

1
2

(cid:19)

, 0

.

As one can see, if the agent takes the non-expert action a on a good state, it goes to the bad absorbing state b.
In this case, the imitation gap can be at most 2.

We highlight that the construction of RBAS MDPs is inspired by locomotion control tasks from the
MuJoCo benchmark and some Atari games. For instance, a crucial feature of locomotion control tasks is:
once taking the non-expert/wrong action, the robot falls and goes into a bad terminal/absorbing state with
a reward 0; refer to Figure 1. Besides, in some Atari games (including Pong and Breakout), if executing
the non-expert action, the agent is dead and the game is over. As we have demonstrated, this feature is
considered in RBAS MDPs.

We note that the lower bound instance for ofﬂine imitation learning algorithms [Rajaraman et al., 2020]
1 good states and

also satisﬁes Assumption 1. Concretely, in the ofﬂine lower bound instance, there are
1 bad state. Furthermore, the initial state distribution ρ is formulated as:

|S| −

(cid:16)

ρ(s1),

ρ =

· · ·

, ρ(s|S|−

2), ρ(s|S|−

1), ρ(b)

(cid:17)

=

(cid:18) 1

N + 1

,

1
N + 1

, 1

−

2
|S| −
N + 1

,

· · ·

(cid:19)

, 0

.

11For the simplicity of notations, we assume that expert actions are the same on all good states and time steps. Nevertheless, our

results can be seamlessly extended to the case where expert actions are different on good states.

9

(cid:54)
Figure 2: A simple MDP corresponding to Assumption 1. Digits indicate the transition probabilities.

|

·

[

·|

∈

|S| −

) for i

si, a) = ρ(

The transition function of the ofﬂine lower bound instance is carefully designed in [Rajaraman et al.,
2020]. For each good state, executing the expert action a leads to a state transition according to ρ, i.e.,
b, a) = 1. Then, it is proved
Ph(
1]. For the bad state, it is absorbing, i.e., P(b
that any ofﬂine imitation learning approach (including BC) suffers an imitation gap at least Ω(
H2/N)
[Rajaraman et al., 2020]. Note that the imitation gap bound of BC (in Theorem 3) matches this lower bound,
so we know that BC has a tight imitation gap Θ(min

H,
Now, we verify that Empirical Observation 1 and Empirical Observation 2 hold for RBAS MDPs, too. We
= 2, whose transition structure is similar to that in Figure 2.
consider a RBAS MDP with
Speciﬁcally, there are 19 good states and 1 bad state. The initial state distribution is the uniform distribution
over good states. Please refer to Appendix E for details. First, Table 3 summarizes the imitation gaps of
BC and TV-AIL when the expert demonstrations are scarce, and the planning horizon is long. We see that
TV-AIL can nearly match the expert’s performance even only with 1 expert trajectory, whereas BC has poor
performance. This result is consistent with Empirical Observation 1. Second, we evaluate TV-AIL and
BC on the same MDP with different horizons; see the result in Table 4. In particular, we observe that the
imitation gap of TV-AIL almost does not increase when the horizon grows. This empirical observation
agrees with Empirical Observation 2. To summarize, we demonstrate that algorithmic behaviors in Empirical
Observation 1 and Empirical Observation 2 also hold for RBAS MDPs. Therefore, it is sufﬁcient to study
algorithmic behaviors on RBAS MDPs.

) on RBAS MDPs.

= 20 and

H2/N

|A|

|S|

|S|

|S|

{

}

Table 3: Imitation gap on the RBAS MDP with H = 1000. We report the mean of imitation gap with the
standard deviation over 20 independent experiments (same with the remaining tables).

N = 1

N = 4

N = 7

N = 10

BC
TV-AIL

998.87
0.71

0.15
±
0.00
±

998.57
0.64

0.14
±
0.01
±

998.12
0.61

0.16
±
0.02
±

997.60
0.55

0.30
±
0.02
±

Table 4: Imitation gap on the RBAS MDP with N = 1.

H = 100

H = 500

H = 1000

H = 2000

BC
TV-AIL

98.89
0.69

0.14
0.00

±
±

1998.88
0.71

0.10

±
0.00

±

498.91
0.70

0.10
±
0.00
±

998.87
0.71

0.15
±
0.00
±

10

39s1<latexit sha1_base64="rbb+MsaQt35yr3hXr4jTSkyIK8g=">AAACyHicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl0I64qmLZQqyTTaR2aJmEyUUrpxh9wq18m/oH+hXfGFNQiOiHJmXPvOTP33iAJRaoc57Vgzc0vLC4Vl0srq2vrG+XNrUYaZ5Jxj8VhLFuBn/JQRNxTQoW8lUjuD4OQN4PBqY4377hMRRxdqlHCO0O/H4meYL4iykuvx+7kplxxqo5Z9ixwc1BBvupx+QVX6CIGQ4YhOCIowiF8pPS04cJBQlwHY+IkIWHiHBOUSJtRFqcMn9gBffu0a+dsRHvtmRo1o1NCeiUpbeyRJqY8SVifZpt4Zpw1+5v32Hjqu43oH+ReQ2IVbon9SzfN/K9O16LQw7GpQVBNiWF0dSx3yUxX9M3tL1UpckiI07hLcUmYGeW0z7bRpKZ23VvfxN9Mpmb1nuW5Gd71LWnA7s9xzoLGQdV1qu7FYaV2ko+6iB3sYp/meYQazlCHR94Cj3jCs3VuJda9NfpMtQq5ZhvflvXwAZsxkSc=</latexit><latexit sha1_base64="rbb+MsaQt35yr3hXr4jTSkyIK8g=">AAACyHicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl0I64qmLZQqyTTaR2aJmEyUUrpxh9wq18m/oH+hXfGFNQiOiHJmXPvOTP33iAJRaoc57Vgzc0vLC4Vl0srq2vrG+XNrUYaZ5Jxj8VhLFuBn/JQRNxTQoW8lUjuD4OQN4PBqY4377hMRRxdqlHCO0O/H4meYL4iykuvx+7kplxxqo5Z9ixwc1BBvupx+QVX6CIGQ4YhOCIowiF8pPS04cJBQlwHY+IkIWHiHBOUSJtRFqcMn9gBffu0a+dsRHvtmRo1o1NCeiUpbeyRJqY8SVifZpt4Zpw1+5v32Hjqu43oH+ReQ2IVbon9SzfN/K9O16LQw7GpQVBNiWF0dSx3yUxX9M3tL1UpckiI07hLcUmYGeW0z7bRpKZ23VvfxN9Mpmb1nuW5Gd71LWnA7s9xzoLGQdV1qu7FYaV2ko+6iB3sYp/meYQazlCHR94Cj3jCs3VuJda9NfpMtQq5ZhvflvXwAZsxkSc=</latexit><latexit sha1_base64="rbb+MsaQt35yr3hXr4jTSkyIK8g=">AAACyHicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl0I64qmLZQqyTTaR2aJmEyUUrpxh9wq18m/oH+hXfGFNQiOiHJmXPvOTP33iAJRaoc57Vgzc0vLC4Vl0srq2vrG+XNrUYaZ5Jxj8VhLFuBn/JQRNxTQoW8lUjuD4OQN4PBqY4377hMRRxdqlHCO0O/H4meYL4iykuvx+7kplxxqo5Z9ixwc1BBvupx+QVX6CIGQ4YhOCIowiF8pPS04cJBQlwHY+IkIWHiHBOUSJtRFqcMn9gBffu0a+dsRHvtmRo1o1NCeiUpbeyRJqY8SVifZpt4Zpw1+5v32Hjqu43oH+ReQ2IVbon9SzfN/K9O16LQw7GpQVBNiWF0dSx3yUxX9M3tL1UpckiI07hLcUmYGeW0z7bRpKZ23VvfxN9Mpmb1nuW5Gd71LWnA7s9xzoLGQdV1qu7FYaV2ko+6iB3sYp/meYQazlCHR94Cj3jCs3VuJda9NfpMtQq5ZhvflvXwAZsxkSc=</latexit><latexit sha1_base64="rbb+MsaQt35yr3hXr4jTSkyIK8g=">AAACyHicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl0I64qmLZQqyTTaR2aJmEyUUrpxh9wq18m/oH+hXfGFNQiOiHJmXPvOTP33iAJRaoc57Vgzc0vLC4Vl0srq2vrG+XNrUYaZ5Jxj8VhLFuBn/JQRNxTQoW8lUjuD4OQN4PBqY4377hMRRxdqlHCO0O/H4meYL4iykuvx+7kplxxqo5Z9ixwc1BBvupx+QVX6CIGQ4YhOCIowiF8pPS04cJBQlwHY+IkIWHiHBOUSJtRFqcMn9gBffu0a+dsRHvtmRo1o1NCeiUpbeyRJqY8SVifZpt4Zpw1+5v32Hjqu43oH+ReQ2IVbon9SzfN/K9O16LQw7GpQVBNiWF0dSx3yUxX9M3tL1UpckiI07hLcUmYGeW0z7bRpKZ23VvfxN9Mpmb1nuW5Gd71LWnA7s9xzoLGQdV1qu7FYaV2ko+6iB3sYp/meYQazlCHR94Cj3jCs3VuJda9NfpMtQq5ZhvflvXwAZsxkSc=</latexit>s1<latexit sha1_base64="rbb+MsaQt35yr3hXr4jTSkyIK8g=">AAACyHicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl0I64qmLZQqyTTaR2aJmEyUUrpxh9wq18m/oH+hXfGFNQiOiHJmXPvOTP33iAJRaoc57Vgzc0vLC4Vl0srq2vrG+XNrUYaZ5Jxj8VhLFuBn/JQRNxTQoW8lUjuD4OQN4PBqY4377hMRRxdqlHCO0O/H4meYL4iykuvx+7kplxxqo5Z9ixwc1BBvupx+QVX6CIGQ4YhOCIowiF8pPS04cJBQlwHY+IkIWHiHBOUSJtRFqcMn9gBffu0a+dsRHvtmRo1o1NCeiUpbeyRJqY8SVifZpt4Zpw1+5v32Hjqu43oH+ReQ2IVbon9SzfN/K9O16LQw7GpQVBNiWF0dSx3yUxX9M3tL1UpckiI07hLcUmYGeW0z7bRpKZ23VvfxN9Mpmb1nuW5Gd71LWnA7s9xzoLGQdV1qu7FYaV2ko+6iB3sYp/meYQazlCHR94Cj3jCs3VuJda9NfpMtQq5ZhvflvXwAZsxkSc=</latexit><latexit sha1_base64="rbb+MsaQt35yr3hXr4jTSkyIK8g=">AAACyHicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl0I64qmLZQqyTTaR2aJmEyUUrpxh9wq18m/oH+hXfGFNQiOiHJmXPvOTP33iAJRaoc57Vgzc0vLC4Vl0srq2vrG+XNrUYaZ5Jxj8VhLFuBn/JQRNxTQoW8lUjuD4OQN4PBqY4377hMRRxdqlHCO0O/H4meYL4iykuvx+7kplxxqo5Z9ixwc1BBvupx+QVX6CIGQ4YhOCIowiF8pPS04cJBQlwHY+IkIWHiHBOUSJtRFqcMn9gBffu0a+dsRHvtmRo1o1NCeiUpbeyRJqY8SVifZpt4Zpw1+5v32Hjqu43oH+ReQ2IVbon9SzfN/K9O16LQw7GpQVBNiWF0dSx3yUxX9M3tL1UpckiI07hLcUmYGeW0z7bRpKZ23VvfxN9Mpmb1nuW5Gd71LWnA7s9xzoLGQdV1qu7FYaV2ko+6iB3sYp/meYQazlCHR94Cj3jCs3VuJda9NfpMtQq5ZhvflvXwAZsxkSc=</latexit><latexit sha1_base64="rbb+MsaQt35yr3hXr4jTSkyIK8g=">AAACyHicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl0I64qmLZQqyTTaR2aJmEyUUrpxh9wq18m/oH+hXfGFNQiOiHJmXPvOTP33iAJRaoc57Vgzc0vLC4Vl0srq2vrG+XNrUYaZ5Jxj8VhLFuBn/JQRNxTQoW8lUjuD4OQN4PBqY4377hMRRxdqlHCO0O/H4meYL4iykuvx+7kplxxqo5Z9ixwc1BBvupx+QVX6CIGQ4YhOCIowiF8pPS04cJBQlwHY+IkIWHiHBOUSJtRFqcMn9gBffu0a+dsRHvtmRo1o1NCeiUpbeyRJqY8SVifZpt4Zpw1+5v32Hjqu43oH+ReQ2IVbon9SzfN/K9O16LQw7GpQVBNiWF0dSx3yUxX9M3tL1UpckiI07hLcUmYGeW0z7bRpKZ23VvfxN9Mpmb1nuW5Gd71LWnA7s9xzoLGQdV1qu7FYaV2ko+6iB3sYp/meYQazlCHR94Cj3jCs3VuJda9NfpMtQq5ZhvflvXwAZsxkSc=</latexit><latexit sha1_base64="rbb+MsaQt35yr3hXr4jTSkyIK8g=">AAACyHicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdFl0I64qmLZQqyTTaR2aJmEyUUrpxh9wq18m/oH+hXfGFNQiOiHJmXPvOTP33iAJRaoc57Vgzc0vLC4Vl0srq2vrG+XNrUYaZ5Jxj8VhLFuBn/JQRNxTQoW8lUjuD4OQN4PBqY4377hMRRxdqlHCO0O/H4meYL4iykuvx+7kplxxqo5Z9ixwc1BBvupx+QVX6CIGQ4YhOCIowiF8pPS04cJBQlwHY+IkIWHiHBOUSJtRFqcMn9gBffu0a+dsRHvtmRo1o1NCeiUpbeyRJqY8SVifZpt4Zpw1+5v32Hjqu43oH+ReQ2IVbon9SzfN/K9O16LQw7GpQVBNiWF0dSx3yUxX9M3tL1UpckiI07hLcUmYGeW0z7bRpKZ23VvfxN9Mpmb1nuW5Gd71LWnA7s9xzoLGQdV1qu7FYaV2ko+6iB3sYp/meYQazlCHR94Cj3jCs3VuJda9NfpMtQq5ZhvflvXwAZsxkSc=</latexit>s2<latexit sha1_base64="frzY4hifoLpmGmHI/Dd3rl1kKgc=">AAACyHicjVHLSsNAFD2Nr/quunQTLIKrkhRBl0U34qqCaQu1SpJO69A0CTMTpZRu/AG3+mXiH+hfeGdMQS2iE5KcOfeeM3PvDdKIS+U4rwVrbn5hcam4vLK6tr6xWdrabsgkEyHzwiRKRCvwJYt4zDzFVcRaqWD+MIhYMxic6njzjgnJk/hSjVLWGfr9mPd46CuiPHk9rk5uSmWn4phlzwI3B2Xkq56UXnCFLhKEyDAEQwxFOIIPSU8bLhykxHUwJk4Q4ibOMMEKaTPKYpThEzugb5927ZyNaa89pVGHdEpEryCljX3SJJQnCOvTbBPPjLNmf/MeG099txH9g9xrSKzCLbF/6aaZ/9XpWhR6ODY1cKopNYyuLsxdMtMVfXP7S1WKHFLiNO5SXBAOjXLaZ9topKld99Y38TeTqVm9D/PcDO/6ljRg9+c4Z0GjWnGdintxWK6d5KMuYhd7OKB5HqGGM9ThkTfHI57wbJ1bqXVvjT5TrUKu2cG3ZT18AJ2SkSg=</latexit><latexit sha1_base64="frzY4hifoLpmGmHI/Dd3rl1kKgc=">AAACyHicjVHLSsNAFD2Nr/quunQTLIKrkhRBl0U34qqCaQu1SpJO69A0CTMTpZRu/AG3+mXiH+hfeGdMQS2iE5KcOfeeM3PvDdKIS+U4rwVrbn5hcam4vLK6tr6xWdrabsgkEyHzwiRKRCvwJYt4zDzFVcRaqWD+MIhYMxic6njzjgnJk/hSjVLWGfr9mPd46CuiPHk9rk5uSmWn4phlzwI3B2Xkq56UXnCFLhKEyDAEQwxFOIIPSU8bLhykxHUwJk4Q4ibOMMEKaTPKYpThEzugb5927ZyNaa89pVGHdEpEryCljX3SJJQnCOvTbBPPjLNmf/MeG099txH9g9xrSKzCLbF/6aaZ/9XpWhR6ODY1cKopNYyuLsxdMtMVfXP7S1WKHFLiNO5SXBAOjXLaZ9topKld99Y38TeTqVm9D/PcDO/6ljRg9+c4Z0GjWnGdintxWK6d5KMuYhd7OKB5HqGGM9ThkTfHI57wbJ1bqXVvjT5TrUKu2cG3ZT18AJ2SkSg=</latexit><latexit sha1_base64="frzY4hifoLpmGmHI/Dd3rl1kKgc=">AAACyHicjVHLSsNAFD2Nr/quunQTLIKrkhRBl0U34qqCaQu1SpJO69A0CTMTpZRu/AG3+mXiH+hfeGdMQS2iE5KcOfeeM3PvDdKIS+U4rwVrbn5hcam4vLK6tr6xWdrabsgkEyHzwiRKRCvwJYt4zDzFVcRaqWD+MIhYMxic6njzjgnJk/hSjVLWGfr9mPd46CuiPHk9rk5uSmWn4phlzwI3B2Xkq56UXnCFLhKEyDAEQwxFOIIPSU8bLhykxHUwJk4Q4ibOMMEKaTPKYpThEzugb5927ZyNaa89pVGHdEpEryCljX3SJJQnCOvTbBPPjLNmf/MeG099txH9g9xrSKzCLbF/6aaZ/9XpWhR6ODY1cKopNYyuLsxdMtMVfXP7S1WKHFLiNO5SXBAOjXLaZ9topKld99Y38TeTqVm9D/PcDO/6ljRg9+c4Z0GjWnGdintxWK6d5KMuYhd7OKB5HqGGM9ThkTfHI57wbJ1bqXVvjT5TrUKu2cG3ZT18AJ2SkSg=</latexit><latexit sha1_base64="frzY4hifoLpmGmHI/Dd3rl1kKgc=">AAACyHicjVHLSsNAFD2Nr/quunQTLIKrkhRBl0U34qqCaQu1SpJO69A0CTMTpZRu/AG3+mXiH+hfeGdMQS2iE5KcOfeeM3PvDdKIS+U4rwVrbn5hcam4vLK6tr6xWdrabsgkEyHzwiRKRCvwJYt4zDzFVcRaqWD+MIhYMxic6njzjgnJk/hSjVLWGfr9mPd46CuiPHk9rk5uSmWn4phlzwI3B2Xkq56UXnCFLhKEyDAEQwxFOIIPSU8bLhykxHUwJk4Q4ibOMMEKaTPKYpThEzugb5927ZyNaa89pVGHdEpEryCljX3SJJQnCOvTbBPPjLNmf/MeG099txH9g9xrSKzCLbF/6aaZ/9XpWhR6ODY1cKopNYyuLsxdMtMVfXP7S1WKHFLiNO5SXBAOjXLaZ9topKld99Y38TeTqVm9D/PcDO/6ljRg9+c4Z0GjWnGdintxWK6d5KMuYhd7OKB5HqGGM9ThkTfHI57wbJ1bqXVvjT5TrUKu2cG3ZT18AJ2SkSg=</latexit>s2<latexit sha1_base64="frzY4hifoLpmGmHI/Dd3rl1kKgc=">AAACyHicjVHLSsNAFD2Nr/quunQTLIKrkhRBl0U34qqCaQu1SpJO69A0CTMTpZRu/AG3+mXiH+hfeGdMQS2iE5KcOfeeM3PvDdKIS+U4rwVrbn5hcam4vLK6tr6xWdrabsgkEyHzwiRKRCvwJYt4zDzFVcRaqWD+MIhYMxic6njzjgnJk/hSjVLWGfr9mPd46CuiPHk9rk5uSmWn4phlzwI3B2Xkq56UXnCFLhKEyDAEQwxFOIIPSU8bLhykxHUwJk4Q4ibOMMEKaTPKYpThEzugb5927ZyNaa89pVGHdEpEryCljX3SJJQnCOvTbBPPjLNmf/MeG099txH9g9xrSKzCLbF/6aaZ/9XpWhR6ODY1cKopNYyuLsxdMtMVfXP7S1WKHFLiNO5SXBAOjXLaZ9topKld99Y38TeTqVm9D/PcDO/6ljRg9+c4Z0GjWnGdintxWK6d5KMuYhd7OKB5HqGGM9ThkTfHI57wbJ1bqXVvjT5TrUKu2cG3ZT18AJ2SkSg=</latexit><latexit sha1_base64="frzY4hifoLpmGmHI/Dd3rl1kKgc=">AAACyHicjVHLSsNAFD2Nr/quunQTLIKrkhRBl0U34qqCaQu1SpJO69A0CTMTpZRu/AG3+mXiH+hfeGdMQS2iE5KcOfeeM3PvDdKIS+U4rwVrbn5hcam4vLK6tr6xWdrabsgkEyHzwiRKRCvwJYt4zDzFVcRaqWD+MIhYMxic6njzjgnJk/hSjVLWGfr9mPd46CuiPHk9rk5uSmWn4phlzwI3B2Xkq56UXnCFLhKEyDAEQwxFOIIPSU8bLhykxHUwJk4Q4ibOMMEKaTPKYpThEzugb5927ZyNaa89pVGHdEpEryCljX3SJJQnCOvTbBPPjLNmf/MeG099txH9g9xrSKzCLbF/6aaZ/9XpWhR6ODY1cKopNYyuLsxdMtMVfXP7S1WKHFLiNO5SXBAOjXLaZ9topKld99Y38TeTqVm9D/PcDO/6ljRg9+c4Z0GjWnGdintxWK6d5KMuYhd7OKB5HqGGM9ThkTfHI57wbJ1bqXVvjT5TrUKu2cG3ZT18AJ2SkSg=</latexit><latexit sha1_base64="frzY4hifoLpmGmHI/Dd3rl1kKgc=">AAACyHicjVHLSsNAFD2Nr/quunQTLIKrkhRBl0U34qqCaQu1SpJO69A0CTMTpZRu/AG3+mXiH+hfeGdMQS2iE5KcOfeeM3PvDdKIS+U4rwVrbn5hcam4vLK6tr6xWdrabsgkEyHzwiRKRCvwJYt4zDzFVcRaqWD+MIhYMxic6njzjgnJk/hSjVLWGfr9mPd46CuiPHk9rk5uSmWn4phlzwI3B2Xkq56UXnCFLhKEyDAEQwxFOIIPSU8bLhykxHUwJk4Q4ibOMMEKaTPKYpThEzugb5927ZyNaa89pVGHdEpEryCljX3SJJQnCOvTbBPPjLNmf/MeG099txH9g9xrSKzCLbF/6aaZ/9XpWhR6ODY1cKopNYyuLsxdMtMVfXP7S1WKHFLiNO5SXBAOjXLaZ9topKld99Y38TeTqVm9D/PcDO/6ljRg9+c4Z0GjWnGdintxWK6d5KMuYhd7OKB5HqGGM9ThkTfHI57wbJ1bqXVvjT5TrUKu2cG3ZT18AJ2SkSg=</latexit><latexit sha1_base64="frzY4hifoLpmGmHI/Dd3rl1kKgc=">AAACyHicjVHLSsNAFD2Nr/quunQTLIKrkhRBl0U34qqCaQu1SpJO69A0CTMTpZRu/AG3+mXiH+hfeGdMQS2iE5KcOfeeM3PvDdKIS+U4rwVrbn5hcam4vLK6tr6xWdrabsgkEyHzwiRKRCvwJYt4zDzFVcRaqWD+MIhYMxic6njzjgnJk/hSjVLWGfr9mPd46CuiPHk9rk5uSmWn4phlzwI3B2Xkq56UXnCFLhKEyDAEQwxFOIIPSU8bLhykxHUwJk4Q4ibOMMEKaTPKYpThEzugb5927ZyNaa89pVGHdEpEryCljX3SJJQnCOvTbBPPjLNmf/MeG099txH9g9xrSKzCLbF/6aaZ/9XpWhR6ODY1cKopNYyuLsxdMtMVfXP7S1WKHFLiNO5SXBAOjXLaZ9topKld99Y38TeTqVm9D/PcDO/6ljRg9+c4Z0GjWnGdintxWK6d5KMuYhd7OKB5HqGGM9ThkTfHI57wbJ1bqXVvjT5TrUKu2cG3ZT18AJ2SkSg=</latexit>bbexpert action (reward = 1)non-expert action (reward = 0)0.50.50.50.51.01.01.04.2 Main Results

In this part, we present TV-AIL’s horizon-free imitation gap guarantee for RBAS MDPs. This result relies on
the following proposition, which gives the optimality condition of the problem in (5).

Proposition 1. For any tabular and episodic MDP satisfying Assumption 1, suppose that πAIL is a minimizer of (5).
When N

≥

1, we have the following optimality condition almost surely:
πAIL
h

s) = 1,

G, h

(a1

s

[H

1].

∈ S
Proposition 1 claims that TV-AIL can recover the expert actions on both visited and non-visited states
in the ﬁrst H
1 time steps. In contrast, we know that BC cannot achieve this; instead, BC may select the
non-expert action on non-visited states. We explain why two approaches yield different behaviors as follows.

−

−

∈

∀

|

Remark 1. We highlight that the state-action distribution matching in (5) involves a multi-stage policy optimization
problem, in which decision variables could be coupled. To see this, recall the deﬁnition of dπ

h (s, a):

h (s, a) = dπ
dπ

h (s)πh(a

s) =

|

=


 ∑
(s(cid:48),a(cid:48))


 ∑
(s(cid:48),a(cid:48))



dπ
h
−

1(s(cid:48), a(cid:48))Ph

1(s

|

−

s(cid:48), a(cid:48))

 πh(a

s)

|



dπ
h
−

1(s(cid:48))πh

1(a(cid:48)

|

−

s(cid:48))Ph

1(s

|

−

s(cid:48), a(cid:48))

 πh(a

s).

|

(7)

1 and πh are linked by the intermediate variable dπ
1 and πh
Therefore, decision variables πh
are optimized in a joint way. This is a huge difference between TV-AIL and BC. Though BC also solves a non-stationary
policy, BC’s objective has no coupling structure; refer to (2). On the other hand, it is easy to see that BC solves a convex
optimization problem in (2), whereas as a byproduct, we ﬁnd that TV-AIL may solve a non-convex policy optimization
problem due to the coupling structure in (7).

h (s, a), meaning that πh

−

−

Proposition 2. There exist tabular and episodic MDPs such that the objective of TV-AIL in (5) is non-convex.

Proof of Proposition 2 is deferred to Appendix A.3. The intuition is that function f (x, y) = xy is non-convex. In
1 and πh, respectively. Before the follow-up discussion about Proposition 1, we
our context, x and y may refer to πh
comment that even though the policy optimization in TV-AIL could be non-convex, there exists a linear-programming-
based procedure for (5), which runs in a polynomial time. Furthermore, gradient-based methods can also return an
approximately optimal solution for (6) in a polynomial time. Thus, we do not need to worry much about the computation
efﬁciency; please see Appendix D.1 for details.

−

Despite the non-convexity, we establish the global optimality condition in Proposition 1, based on a dynamic
programming analysis. We defer details to Section 4.3. Here we remark that the recursive structure in (7) is essential to
recover the expert action on non-visited states, as the state-action distribution matching loss in a large time step (say
(cid:100)dπE
dπ
h (cid:107)1) can affect the decision variables in a small time step (say π1). We will explain this point in Example 1.
h −
(cid:107)
Since there is no future guidance in the last time step, we cannot guarantee that the obtained policy in the last time step
follows the expert policy, which explains the claim in Proposition 1.

With Proposition 1, it is immediate to obtain the imitation gap of TV-AIL for RBAS MDPs.

Theorem 4 (Horizon-free Imitation Gap of TV-AIL on RBAS MDPs). For any tabular and episodic MDP
satisfying Assumption 1, suppose that πAIL is any minimizer of (5). Then we have that

V(πE)

−

(cid:104)

E

(cid:105)

V(πAIL)

(cid:32)

(cid:40)

(cid:114)

(cid:41)(cid:33)

≤ O

min

1,

|S|
N

,

(8)

where the expectation is taken over the randomness in collecting N expert trajectories.

Remark 2. Theorem 4 says that TV-AIL has two types of imitation gaps for RBAS MDPs, depending on the sample
size. In the small sample regime (i.e., N (cid:45)
), the ﬁrst term dominates in (8), indicating the imitation gap of TV-AIL

|S|

11

|S|

. On the other hand, in the large sample regime (i.e.,
is at most 1. In particular, this guarantee holds for any H and
|S|
N (cid:37)
), the second term in (8) dominates and the imitation gap diminishes to 0 as N goes to inﬁnity. This result can
explain the empirical results in Table 3 and Table 4. By the similarity between RBAS MDPs and locomotion tasks from
the MuJoCo benchmark, Theorem 4 can also help understand the superior performance of TV-AIL in practice. To our
best knowledge, this is the ﬁrst result that can show that AIL methods can perform well in the small sample regime.

We clarify that the good result in Theorem 4 is not due to the total variation distance. Instead, we observe that other
AIL methods (e.g., FEM, GTAL and GAIL) also have comparative performance with TV-AIL on RBAS MDPs; see the
numerical result in Appendix D.3.

|S|

Remark 3. Recall that the imitation gap of BC is Θ(min
regime (i.e., N (cid:45)
by the (coupled) multi-stage optimization, TV-AIL can overcome the compounding errors issue of ofﬂine imitation.

) for RBAS MDPs. In the small sample
), we see that the imitation gap bound of TV-AIL is much less than BC. This result suggests that

Careful readers may notice that in the large sample regime (i.e., N (cid:37)

H4), BC has a better imitation gap bound
(1/H2), which is extremely small.
than TV-AIL. Keep in mind that in this regime, the imitation gap is less than
Thus, this phenomenon may not often be observed in practice. Nevertheless, we comment that this sample barrier issue
is not essential for TV-AIL. This is because a slight modiﬁcation of TV-AIL can lead to the improved imitation gap
/N) for RBAS MDPs, which is better than BC’s in the whole sample regime; refer to Appendix D.2 for more

H2/N

|S|

|S|

H,

O

}

{

(

O
|S|
discussion.

Remark 4. Note that the horizon-free imitation gap in Theorem 4 does not contradict the lower bound Ω(H
/N)
in [Rajaraman et al., 2020]. There are two reasons. First, this lower bound holds in the large sample regime (i.e.,
N (cid:37)

). Second, the lower bound instance in Theorem 4 does not satisfy Assumption 1.

|S|

|S|

To get a better sense of the horizon-free imitation gap and the coupling structure in the state-action

distribution matching, we consider the following example.

1 (a

|

·

Example 1. Consider the mentioned MDP shown in Figure 2. Furthermore, assume that the agent is provided with 2
expert trajectories: tr1 = (s1, a)

(s2, a), where a is the expert action.

(s1, a) and tr2 = (s1, a)

Let us ﬁrst study the performance of BC. According to (3), we ﬁnd that BC exactly recovers the expert action except
that it poses a uniform policy on the non-visited s2 in time step h = 1. As a result, BC makes a mistake with probability
ρ(s2)

s2) = 0.25. Accordingly, its imitation gap is 0.25

2 = 0.5.

πBC

→

For TV-AIL, it makes sense to guess that the expert action is recovered on visited states (otherwise, it incurs
a state-action distribution matching loss). We argue that TV-AIL exactly recovers the expert action even on the
non-visited state s2 in time step h = 1. Consequently, the imitation gap of TV-AIL is 0, which is smaller than BC. The
formal proof of the above arguments is a little tricky (refer to Appendix A.4), and we explain the intuition here.

Assume that TV-AIL takes the expert action on visited states and let π1(a
that a positive β makes no difference for the loss function in time step h = 1, since

|

s2) = 1

β, where β

−

∈

[0, 1]. We note

→

·

dπ
1 (s2, a)

|

−

(cid:100)dπE
1 (s2, a)

+

dπ
1 (s2, a)

(cid:100)dπE
1 (s2, a)

=

dπ
1 (s2, a)

|

|

−

|

|

+

0

|

−

|

dπ
1 (s2, a)

0

|

−

= ρ(s2).

However, it matters for the loss function in time step h = 2. By (7), we can compute the state-action distribution in
time step h = 2:

2 (s1, a) = 0.25(1 + β), dπ
dπ
2 (s2, a) = 0.25(1 + β), dπ
dπ
dπ
2 (b, a) = 0.5(1
Then the state-action distribution matching loss becomes

β).

−

2 (s1, a) = 0,
2 (s2, a) = 0,

Loss(β) =

dπ
h (s, a)

(cid:100)dπE
h (s, a)

|

−

2
∑
∑
(s,a) |
h=1
= 1.0 + ∑
(s,a) |

dπ
2 (s, a)

(cid:100)dπE
2 (s, a)

|

−

= 1.0 + 2

|

0.25(1 + β)

0.5

|

−

+

|

0.5(1

β)

0

|

−

−

12

= 2

β,

−

which has a unique globally optimal solution at β = 1. In plain language, if the agent selects a wrong action in the ﬁrst
time step, it may go to the bad absorbing state in the second time step. This results in a large loss because the expert
policy never visits the bad absorbing state. Therefore, to minimize the cumulative state-action distribution matching
losses, TV-AIL has to select the action that can avoid the bad status.

4.3 Toward A Stage-coupled Analysis

In this part, we present the key analysis technique for establishing the horizon-free imitation gap of TV-AIL.
First, we brieﬂy review the classical reduction-and-estimation analysis used in previous works [Abbeel and
Ng, 2004, Syed and Schapire, 2007, Xu et al., 2020, Rajaraman et al., 2020]. Then, we explain why this analysis
fails to provide a tight bound on RBAS MDPs. Finally, we introduce a new dynamic-programming-based
analysis to disclose algorithmic behaviors of TV-AIL for RBAS MDPs.

Reduction-and-Estimation Analysis. To analyze AIL methods, the reduction-and-estimation analysis
reduces the imitation gap to the statistical estimation error of the expert’s state-action distribution. Concretely,
we have that
(cid:12)
(cid:12)V(πE)
(cid:12)

dπE
h (s, a)rh(s, a)

(s, a)rh(s, a)

V(πAIL)

dπAIL
h

(a)
=

∑

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

H
∑
h=1

(s,a)

∈S×A
(cid:12)
∑
(cid:12)dπE
(cid:12)
h (s, a)

dπAIL
h

(s, a)

(cid:12)
(cid:12)
(cid:12)

−

(b)

≤

H
∑
h=1

(s,a)

H
∑
h=1

(s,a)

2

H
∑
h=1

∈S×A
∑

∈S×A
∑

(s,a)

∈S×A

≤

(c)

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)

dπE
h (s, a)

(cid:100)dπE
h (s, a)

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

H
∑
h=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:100)dπE
h (s, a)

−

dπE
h (s, a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∑

(s,a)

∈S×A

(cid:100)dπE
h (s, a)

−

dπAIL
h

(s, a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(9)

h=1 (cid:107)

dπAIL
h −

(cid:100)dπE
h (cid:107)1 ≤

∈
∑H
h=1 (cid:107)

where equation (a) follows the dual representation of policy value in (1), inequality (b) is based on the
[0, 1], and inequality (c) holds because πAIL is the optimal solution to (5), i.e.,
assumption that rh(s, a)
(cid:100)dπE
∑H
h (cid:107)1 can be further
upper bounded via proper concentration inequalities. For instance, the (cid:96)1-risk estimation error typically
concentrates in a rate
is
is the estimation dimension) and N is the sample size. In the
the cardinality of the symbol set
|X |
dπE
context of imitation learning, we have that
h −
(cid:107)
assumption that the expert policy is deterministic so that the error bound does not depend on
the above two steps, one can obtain the imitation gap bound.

(cid:100)dπE
h (cid:107)1. Then, the estimation error ∑H

/N) [Weissman et al., 2003, Han et al., 2015, Kamath et al., 2015], where

[H], where we consider the
. Combing

dπE
h −

dπE
h −

(cid:100)dπE
h (cid:107)1

/N for h

h=1 (cid:107)

(cid:45) (cid:112)

((cid:112)

(i.e.,

|X |

|X |

|A|

|S|

O

X

∈

}

).

(cid:112)

/N

Theorem 5. For any tabular and episodic MDP (including RBAS MDPs), the imitation gap of TV-AIL is
H

|S|
This bound matches the counterpart of classical algorithms like FEM and GTAL. However, it cannot help
explain the empirical observations from RBAS MDPs and locomotion tasks from the MuJoCo benchmark. In
particular, this bound is only meaningful in the large sample regime N (cid:37)
(H)
in the imitation gap dominates, and it says nothing. In particular, one can verify that the imitation gap of
TV-AIL is small for RBAS MDPs, even though the estimation error could be very large; refer to the evidence
in Table 5.

; otherwise, the ﬁrst term

(min

H,

{

O

|S|

O

Stage-coupled Analysis. To bypass the hurdle in the reduction-and-estimation analysis, we turn to
analyze the optimal solution of TV-AIL via dynamic programming. In our context, this technique ﬁnds the
optimal solution πAIL =

in an inductive way.

πAIL
1

, πAIL
2

, . . . , πAIL
H }

{

13

Table 5: Imitation gap and estimation error of TV-AIL on the RBAS MDP with N = 1.

H = 100

H = 500

H = 1000

H = 2000

Imitation Gap
Estimation Error

0.69
±
189.47
±

0.00
0.00

0.70
±
947.37
±

0.00
0.00

0.71
±
1894.74

0.00

0.00

±

0.71
±
3789.47

0.00

±

0.00

(10)

Speciﬁcally, in time step h, we deﬁne π(cid:63)
h as
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:124)

argmin
πh

h ∈

π(cid:63)

(cid:100)dπE
h

dπ
h −
(cid:123)(cid:122)
Lossh

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)1
(cid:125)

(cid:13)
(cid:13)
(cid:13)
(cid:13)1
(cid:125)

H
∑
h(cid:48)=h+1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:124)

,

(cid:100)dπE
h(cid:48)

d(cid:63)
h(cid:48) −
(cid:123)(cid:122)
Loss(cid:63)
h(cid:48)
H. Here, π(cid:63)
h+1, . . . , π(cid:63)

−

−

−

}

h , d(cid:63)

h = πE

1, πh, π(cid:63)

h+1, . . . , d(cid:63)

h+1, . . . , π(cid:63)

1 to ﬁnd the desired π(cid:63)

H are computed by π1, . . . , πh

Note that our goal is to prove that π(cid:63)

h based on the condition that π1, . . . , πh

1; then, we move on to solve π(cid:63)
h
−

where dπ
H are deﬁned in
−
a recursive way, which are assumed to be known in time step h. Readers may realize that π1, . . . , πh
1
−
are unknown, so the optimal solution π(cid:63)
1. Ideally, we can solve π(cid:63)
h for
each instantiation of π1, . . . , πh
1 ; ﬁnally, we leverage the obtained
1 , . . . , π(cid:63)
π(cid:63)
1 are
h
not optimal, so it is sufﬁcient to analyze π(cid:63)
−

h naturally depends on π1, . . . , πh
1, . . . , π(cid:63)
h. As one can see, we do not care about the case where π1, . . . , πh
−
1 are also optimal.
π(cid:63) : π(cid:63) =
. Through the standard analysis of dynamic programming [Bertsekas, 2012], it can be Π(cid:63) =

We remark that the above recursive optimization can return an optimal solution set Π(cid:63) =
1 , . . . , π(cid:63)
H)

(π(cid:63)
ΠAIL [Bertsekas, 2012], where ΠAIL is the optimal solution set deﬁned by the original objective in (5).
h for all h
induction-based proof. The base step is to prove π(cid:63)
H
not discuss this part. As for the induction step, the assumption is: for each h + 1
π(cid:63)
= πE
h(cid:48)
h(cid:48)
policy, i.e., π(cid:63)
h = πE
solutions to the ﬁrst term, i.e., πE
solution to the second term, i.e., πE
unique optimal solution to (10) is πE
focus on the case where π1, . . . , πh
h

1] on RBAS MDPs. We achieve this by an
−
1, which is relatively easy to prove so we do
h(cid:48) ≤
1, we have that
. Then, the induction step is to prove that the unique optimal solution to (10) is also the expert
h is one of optimal
argmin Lossh. In the second step, we prove that πE
h is the unique optimal
h = argmin ∑H
. Combing these two steps, we see that the
h . We brieﬂy discuss these two steps as follows. Keep in mind that we
1 are optimal, which allows us to argue that dπ
G and

h . To this end, we follow two steps. In the ﬁrst step, we prove that πE

[H]12.
∈
Step I: we consider the term Lossh in (10) and prove that π(cid:63)

G, is an optimal solution to
minimizing Lossh. Note that πE always takes the expert action and does not visit bad states, so we have
that (cid:100)dπE
= a1, as the expert policy
always executes a1. Based on these facts, we obtain that

B. Furthermore, we have (cid:100)dπE

h (s, a) = 0 for s

h (s) > 0 for all s

[H
∈
1 = πE
H

h(cid:48)=h+1 Loss(cid:63)
h(cid:48)

h (s) = 0 for s

G and a

s) = 1,

h (a1

h ∈

∈ S

∈ S

∈ S

∈ S

≤

−

H

∀

{

−

−

−

s

|

Lossh = ∑
(s,a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

dπ
h (s, a)

−

(cid:100)dπE
h (s, a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:18) (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:100)dπE
h (s, a1)

−

dπ
h (s)πh(a1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

s)

|

+ dπ

h (s)

(cid:16)

1

πh(a1

s)

|

−

(cid:17) (cid:19)

.

= ∑
s

h (s) + ∑
dπ

G

∈S

s

∈S

B
Note that the ﬁrst term dπ
G, we can prove that πh(a1
s
regardless of the estimation (cid:100)dπE
Lossh. Then, we ﬁnish the ﬁrst step.

∈ S

|

h (s) is irrelevant to πh, so it is sufﬁcient to consider the second term. For a speciﬁc
s) = 1 is an optimal solution for such kind of piece-wise linear function,
G, is an optimal solution to minimizing

s) = 1,

h . Therefore πh(a1

s

∀
h is the unique optimal solution to the second term in (10), i.e., πE
h = argminπh
. In fact, this term manifests the nature of coupled multi-stage policy optimization: the

Step II: we prove that πE
h(cid:48)=h+1 Lossh(cid:48)
12This argument is based on the construction of RBAS MDPs and the optimality condition; please refer to Appendix A.1.

∈ S

|

∑H

14

(cid:54)
objective in stage h(cid:48) > h guides the policy optimization in stage h. To better see this, recall that

dπ
h+1(s, a) =


 ∑
(s(cid:48),a(cid:48))



dπ
h (s(cid:48))πh(a(cid:48)

s(cid:48))Ph(s

|

|

s(cid:48), a(cid:48))

 πh+1(a

s).

|

For h(cid:48) = h + 1, we therefore know that d(cid:63)
(s, a) in the term Lossh(cid:48)
h(cid:48)
h + 1.
is also determined by πh for h(cid:48) ≥
Lossh(cid:48)

To show that πE is the unique optimal solution to the sum of losses from stage h + 1 to H, we prove that
. That is, for h(cid:48) = h + 1, . . . , H, we will prove

πE is the unique optimal solution for each individual loss Lossh(cid:48)
πE

is decided by πh. Likewise, the other term

h = argmin

πh

= argmin
πh

Loss(cid:63)
h(cid:48)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∑
(s,a)

d(cid:63)
h(cid:48)

(s, a)

(cid:100)dπE
h(cid:48)

−

(s, a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∑

G

(cid:100)dπE
h(cid:48)

(s)

−

d(cid:63)
h(cid:48)

(s)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= argmin
πh

s

∈S

d(cid:63)
h(cid:48)

(s),

+ ∑
s

∈S

B

where the last step follows the mentioned facts in Step I and the induction assumption that π(cid:63)
h(cid:48)

Next, applying the Bellman ﬂow equation in (7) from step h to h(cid:48) yields that

(11)

= πE.

∑

B

d(cid:63)
h(cid:48)

s

(s) = ∑

dπAIL
h

s

B

(s) + ∑
s(cid:48)∈S

dπAIL
h

(s(cid:48))

(cid:16)

1

πh(a1

s(cid:48))

|

−

(cid:17)

.

G

s) results in a smaller probability of visiting bad states in time step

∈S
We see that a larger probability of πh(a1
h(cid:48). Namely,

∈S

|

πh(a1

s(cid:48))

|

s(cid:48)

,

↑

∀

∈ S

G =

⇒

s

∑

B

d(cid:63)
h(cid:48)

(s)

.

↓

∈S

To effectively minimize the second term in (11), πh is forced to take the expert action, even on a non-visited
state. The formal argument of the optimality of πE is deferred to Appendix.

Through the above two steps, we can prove that π(cid:63)

G is the unique optimal solution to
(10). Finally, applying the backward induction completes the proof of Proposition 1. Compared with the
classical reduction-and-estimation framework, our dynamic-programming-based analysis discloses that
there is no imitation gap for TV-AIL in the ﬁrst H
1 time steps. This is central to obtaining the desired
−
horizon-free imitation gap. As for the last time step H, the one-step imitation gap is proved to be upper
bounded by

). Then, we arrive at Theorem 4.

s) = 1,

h (a1

(min

∈ S

/N

(cid:112)

1,

∀

s

|

Before we ﬁnish the technical discussion, we point out that the assumption that bad absorbing states are
reachable is crucial to our analysis. Our analysis shows that selecting an action that leads to bad absorbing
states can result in a large matching loss. In addition, our analysis can easily extend to cases where good
actions are not unique, as long as the reward function is state-dependent. For instance, there could be
multiple actions that can maintain a good status, and the expert may take a stochastic policy over these
actions. For the sake of clarity, we omit the analysis in such cases.

O

{

|S|

}

4.4 When Exact Solutions Are Not Available

In the last part, we have put much effort into discussing the imitation gap of exactly optimal solutions. In
practice, people usually use gradient-based methods to solve (6), which yields an approximately optimal
solution. This section shows that previous conclusions do not change when the optimization error is
considered.

Deﬁnition 1 (ε-optimal solution). A policy π is an ε-optimal solution for TV-AIL, if

(cid:13)
(cid:13)
(cid:13)
(cid:13)

H
∑
h=1

dπ
h −

(cid:100)dπE
h

(cid:13)
(cid:13)
(cid:13)
(cid:13)1 ≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)

H
∑
h=1

min
Π
π
∈

dπ
h −

(cid:100)dπE
h

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

+ ε.

15

Table 6: Empirical Evaluation of Approximate TV-AIL on the RBAS MDP with N = 1.

H = 100

H = 500

H = 1000

H = 2000

ε/c(π)

0.42

0.00

0.43

0.00

0.44

0.00

0.44

0.00

±

±

±

±

First, we note that it is easy to consider optimization error in the reduction-and-estimation framework.

Namely, following steps in (9) with an additional triangle inequality, we have

(cid:12)
(cid:12)
(cid:12)V(π)

−

V(πE)

(cid:12)
(cid:12)
(cid:12) ≤

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)

H
∑
h=1

(cid:100)dπE
h −

dπE
h

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

+ ε.

Compared with the error bound in (9), there is an additive optimization error ε. However, it is non-trivial
to incorporate the optimization error in our stage-coupled analysis. Indeed, one may guess that due to
the optimization error, the approximately optimal policy π may select the non-expert action with a small
probability. Consequently, the agent may suffer compounding errors, and the horizon-free imitation gap
does not hold. This conjecture is reasonable if the optimization error is large. However, we will show that as
long as the optimization error is properly controlled, the horizon-free guarantee does not change. In the
following part, we formally present our claim.

Theorem 6 (Horizon-free Imitation Gap of Approximate TV-AIL on RBAS MDPs). For each tabular and episodic
MDP satisfying Assumption 1, deﬁne the candidate policy set Πopt =
.
Suppose that π

Πopt is an ε-optimal solution of (5). Let us deﬁne ε(cid:48) = 8ε/c(π), then we have

G, πh(a1

s) > 0

[H],

Π :

∈ S

∈

∈

π

∃

∀

}

{

h

s

|

∈

V(πE)

−

E [V(π)] (cid:45) min

1 + ε(cid:48),

(cid:40)

(cid:114)

(cid:41)

+ ε(cid:48)

,

|S|
N

where c(π) > 0 is deﬁned as

c(π) :=

min

(cid:110)

Pπ (cid:16)

sh = s

s(cid:96) = s(cid:48), a(cid:96) = a1(cid:17)(cid:111)

.

Here Pπ (cid:0)sh = s
which is jointly determined by the transition function and policy π.

|
s(cid:96) = s(cid:48), a(cid:96) = a1(cid:1) is the visitation probability of s in time step h by starting from s(cid:48), a(cid:48) in time step (cid:96),

H,s,s(cid:48)∈S

(cid:96)<h

≤

≤

|

1

G

Proof of Theorem 6 is rather technical and is deferred to Appendix A.6. The difﬁculty stems from the
fact that an ε-optimal solution does not necessarily mean that it is much close to the optimal solution (the
reverse argument is often true). We brieﬂy explain the intuition in Theorem 6. For an approximate solution,
the key is to control decision errors on non-visited states, which is reﬂected in c(π). Speciﬁcally, if π is the
expert policy (i.e., the exactly optimal policy), c(π) is large. Then, we expect that if the optimization error ε
is small, the horizon-free imitation gap still holds. On the other hand, if c(π) is small, we may not say much
about this ε-optimal solution. In fact, we clarify that experiments in Section 4.1 are run with gradient-based
methods to obtain approximately optimal solutions. Interestingly, we ﬁnd that gradient-based methods can
ﬁnd solutions with a relatively large c(π) and a small ε when the iteration number is large (see the empirical
evaluation in Table 6). Understanding why gradient-based methods ﬁnd these good solutions is left for
future work.

5 Beyond RBAS MDPs and Horizon-free Imitation Gap

Previously, we have proved a horizon-free imitation gap for TV-AIL for RBAS MDPs. It naturally begs the
question as to whether TV-AIL always exhibits a horizon-free imitation gap on any instance. In this section,
we provide an answer by showing a horizon-dependent lower bound and a matching upper bound for the
imitation gap of TV-AIL on some hard instances. In the sequel, we formally introduce these hard instances.

16

Figure 3: A simple MDP corresponding to Assumption 2. Digits indicate the transition probabilities.

Assumption 2 (MDPs with isolated absorbing states). For a tabular and episodic MDP and an expert policy, we
assume that

• Each state is absorbing and each action has the same transitions. i.e.,

s, a) = 1.

Ph(s

|

(s, a)

∀

∈ S × A

, h

∈

[H], we have

• For any state, a1 is the expert action with a reward 1 and the others are non-expert actions with a reward 0.

We emphasize two key features of the hard instances satisfying Assumption 2. First, each state is isolated
and absorbing, meaning states are not connected. Second, though all actions lead to the same transition,
only the expert action has a positive reward. These two features do not hold for the instances satisfying
Assumption 1. A simple example satisfying Assumption 2 with two states and two actions is shown in
Figure 3.

Why do the above two features result in hard imitation problems? We ﬂesh this out next. On the one hand,
we point out that the self-absorbing feature implies that decision variables over stages become disconnected.
Formally, for any instance satisfying Assumption 2, we have that

h (s, a) = dπ
dπ

h (s)πh(a

s) = ∑
a(cid:48)

|

dπ
h
−

s)

|

1(s)πh(a

= dπ
h
−
= . . .
= ρ(s)πh(a

s).

|

Then, we can obtain that

[H],
h
∈
1(s, a(cid:48))πh(a

∀

s)

|

h (s) = ∑
dπ

a

h (s, a) = ∑
dπ

a

ρ(s)πh(a

|

s) = ρ(s).

That is, the state visitation distribution equals the initial state distribution, which indicates that the policy does
not play a role in deciding the state visitation distribution. As a consequence, when we apply the dynamic
programming technique to analyze the optimal policy π(cid:63)
1).
By a little effort, it can be shown that π(cid:63)
h is also unrelated to (π(cid:63)
H). In other words, the obtained
time-dependent policies are decoupled in this case. Accordingly, the multi-stage policy optimization reduces
to H independent one-step state-action distribution matching problems. Mathematically speaking, the policy
optimization problem in (10) becomes: for all h

h, we ﬁnd that it is independent of (π1, . . . , πh

h+1, . . . , π(cid:63)

−

π(cid:63)

h ∈

argmin
πh

(cid:13)
(cid:13)
(cid:13)
(cid:13)

∈

[H],
(cid:13)
(cid:13)
(cid:13)
(cid:13)1

(cid:100)dπE
h

+

H
∑
h(cid:48)=h+1

(cid:13)
(cid:13)
(cid:13)
(cid:13)

argmin
πh

∈

ρ(s)πh(a

s)

|

−

(cid:100)dπE
h (s, a)

dπ
h −
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∑
(s,a)

(cid:100)dπE
h(cid:48)

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

,

,

(12)

d(cid:63)
h(cid:48) −
(cid:12)
(cid:12)
(cid:12)
(cid:12)

i.e., a piece-wise linear optimization problem.

On the other hand, we argue that the one-step state-action distribution matching cannot guarantee
optimality even on visited states. That is, TV-AIL may select a wrong action even on visited states. This is
mainly because the matching is performed in the marginal distribution space. We illustrate this point by the
following example.

Example 2. Consider a simple MDP where

=

{

S

s1, s2

and

=

a, a

}

{

A

}

; see Figure 3. Without loss of generality,

17

2Bandit1···|S| 1|S|010101s1s21···|S| 1|S|3expert action (reward = 1)non-expert action (reward = 0)1.01.01.01.0we let H = 1 and omit the subscript. Suppose the initial state distribution ρ = (0.5, 0.5). The agent is provided with
10 trajectories: 4 trajectories start from s1 and the others start from s2.
For TV-AIL, it is easy to calculate the empirical distribution:

(cid:100)dπE (s1, a) = 0.4, (cid:100)dπE (s1, a) = 0,

(cid:100)dπE (s2, a) = 0.6, (cid:100)dπE (s2, a) = 0.
Note that there are multiple optimal solutions for the piece-wise linear optimization in TV-AIL; refer to (12). For
instance, π(a1

s1) = 0.2, π(a1

s1) = 0.8, π(a2

s2) = 1.0, and

|

|

|

dπ(s1, a1) = 0.4, dπ(s1, a2) = 0.1,
dπ(s2, a1) = 0.5, dπ(s2, a2) = 0.0.

For such an optimal policy, the state-action distribution matching loss is 0.2 and the imitation gap is 0.1.

Based on the above discussion, we formally state the imitation gap of TV-AIL on instances satisfying

Assumption 2.

Proposition 3. For any tabular and episodic MDP satisfying Assumption 2, for each time step h, we deﬁne a set of

states

Wh :=

s

{

∈ S

: (cid:100)dπE

h (s) < ρ(s)

. Then,

}

• For each time step h, for an optimal solution πAIL, it satisﬁes

c
h is the complement set of

(a1
(a1

|

|

πAIL
h
πAIL
h
Wh.

where

W

s)

∈
s) = 1,

[(cid:100)dπE

h (s)/ρ(s), 1],

s

∀

∈ W

c
h,

• Among all possible optimal solutions, in the worst-case, we have
(cid:34) H
∑
h=1

E [V(π)] =

max
ΠAIL
π

V(πE)

1
2

−

E

∈

s

∀

∈ Wh,

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:100)dπE
h −

dπE
h

(cid:35)

.

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

• The largest imitation gap is achieved by the policy πh(a1

c
h.

W

s) = (cid:100)dπE

h (s)/ρ(s),

|

s

∀

∈ Wh and πh(a1

|

s) = 1,

s

∀

∈

Proof of Proposition 3 is provided in Appendix B.1. Proposition 3 conveys a message that for in-
stances satisfying Assumption 2, the imitation gap of TV-AIL equals the statistical estimation error of the
expert state-action distribution (up to constants). This matches the upper bound in (9) by the estimation-
and-reduction-based analysis. Then, we know that the imitation gap of TV-AIL is upper bounded by
) for instances satisfying Assumption 2. However, this is not sufﬁcient to answer our
O
question because we still do not know whether this bound is tight or not. We provide a lower bound to help
answer this question in the following part.

(min

H, H

/N

|S|

(cid:112)

{

}

Based on Proposition 3, we can establish the lower bound of imitation gap via establishing the lower

bound for the (cid:96)1-risk estimation.

Theorem 7. Consider a multinomial distribution Q over a ﬁnite set
consider the estimator (cid:98)Q:

X

. Given N i.i.d. samples (X1,

, XN) from Q,

· · ·

If N (cid:45)

|X |

, we have that

(cid:98)Q(i) =

∑j

I(Xj = i)
N

.

E

(cid:104)(cid:13)
(cid:13)
(cid:13)Q

(cid:13)
(cid:13)
(cid:13)1

(cid:98)Q

(cid:105) (cid:37) 1.

−

max
Q
∈Q

18

If N (cid:37)

|X |

, we have that

E

(cid:104)(cid:13)
(cid:13)
(cid:13)Q

(cid:98)Q

−

max
Q
∈Q

(cid:13)
(cid:13)
(cid:13)1
.

(cid:105) (cid:37)

(cid:114)

.

|X |
N

Here

is the set of all multinomial distributions on the set

Q

X
Proof of Theorem 7 is given in Appendix B.2. We note that the lower bound has been established in the
large sample regime [Kamath et al., 2015, Han et al., 2015] and our contribution is the lower bound in the
small sample regime. Based on Proposition 3 and Theorem 7, we immediately obtain the following lower
bound of imitation gap.

Proposition 4. To break the tie, suppose that TV-AIL outputs an optimal policy πAIL by uniformly sampling from all
possible optimal solutions. Then, there exists a tabular and episodic MDP satisfying Assumption 2 such that

V(πE)

−

(cid:104)

E

(cid:105)

V(πAIL)

(cid:32)

(cid:40)

(cid:114)

(cid:41)(cid:33)

min

H, H

|S|
N

.

Ω

≥

Remark 5. With Theorem 5, we validate that in the worst case, the imitation gap of TV-AIL must suffer a linear
dependence on H. This provides a negative answer to the question raised at the beginning of this section: it is impossible
to have a nice horizon-free imitation gap for any instance. On the other hand, Proposition 4 also suggests that the
mentioned features of Assumption 1 are important for the superior performance.

Finally, we emphasize that this failure case for TV-AIL may rarely hold in practice due to the specialty of
isolation and self-absorbing. Therefore, the established lower bound does not conﬂict with the observed
superior performance of AIL methods. Instead, this result may help us better understand when TV-AIL
might fail and what is crucial to success. We note that other AIL methods (e.g., FEM, GTAL and GAIL) may
also have a horizon-dependent imitation gap on hard instances satisfying Assumption 2; see the numerical
result in Appendix D.3.

6 Conclusion

This paper presents a theory that can help explain why adversarial imitation learning (AIL) approaches
match the expert performance in practice, especially in the small sample regime. At the core, we identify
a class of MDPs of interest, upon which the imitation gap of an AIL method (called TV-AIL) is at most
1, regardless of the planning horizon and state space size. In particular, these MDPs have reachable bad
absorbing states. Our theoretical result highlights that TV-AIL is inclined to reject actions that may lead to
visiting bad absorbing states, because the expert never visits these bad absorbing states. The underlying
mechanism is characterized by a stage-coupled analysis for the state-action distribution matching.

There are several promising directions for further study. The ﬁrst is to investigate the function approxi-
mation. This paper studies AIL with the tabular formulation, where there is no extrapolation. Nevertheless,
with Assumption 1, this paper proves that the TV-AIL method can generalize well on non-visited states. It
is challenging to consider parameterized functions in TV-AIL (or other AIL methods). To this end, more
assumptions are required. Otherwise, one can say nothing about the function approximation. It is not clear
whether existing assumptions (e.g., linear MDP [Jin et al., 2020] and Bellman completeness [Chen and Jiang,
2019]) from the reinforcement learning literature are suitable to establish a horizon-free imitation gap bound.
Nevertheless, we believe RBAS MDPs considered in this paper can shed some light. In addition, exploring
AIL methods for other problems related to imitating policies is valuable. For instance, imitation learning
approaches can also be used to recover the environment transitions [Venkatraman et al., 2015, Xu et al., 2020].
Environment learning is crucial for model-based reinforcement learning methods [Sutton and Barto, 2018]. It
is interesting to explore whether the nice horizon-free guarantee also holds in the context of environment
learning.

19

Acknowledgement

Tian Xu would like to thank Xianghan Kong for discussing a technical lemma. Ziniu Li would like to
thank Qingyan Meng, Jiancong Xiao and Yushun Zhang for reading the manuscript and providing helpful
comments.

References

P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the 21st

International Conference on Machine Learning, pages 1–8, 2004.

A. Agarwal, S. M. Kakade, J. D. Lee, and G. Mahajan. Optimality and approximation with policy gradient
methods in markov decision processes. In Proceedings of the 33rd Annual Conference on Learning Theory,
pages 64–66, 2020.

B. D. Argall, S. Chernova, M. Veloso, and B. Browning. A survey of robot learning from demonstration.

Robotics and autonomous systems, 57(5):469–483, 2009.

D. Bertsekas. Dynamic Programming and Optimal Control: Volume I. Athena scientiﬁc, 2012.

D. P. Bertsekas. Nonlinear Programming. Athena Scientiﬁc, 2016.

K. Brantley, W. Sun, and M. Henaff. Disagreement-regularized imitation learning. In Proceedings of the 8th

International Conference on Learning Representations, 2020.

Q. Cai, M. Hong, Y. Chen, and Z. Wang. On the global convergence of imitation learning: A case for linear

quadratic regulator. arXiv, 1901.03674, 2019.

X. Cai, Y. Ding, Y. Jiang, and Z. Zhou. Imitation learning from pixel-level demonstrations by hashreward. In
F. Dignum, A. Lomuscio, U. Endriss, and A. Nowé, editors, Proceedings of the 20th International Conference
on Autonomous Agents and Multiagent Systems, pages 279–287, 2021.

J. Chen and N. Jiang. Information-theoretic considerations in batch reinforcement learning. In Proceedings of

the 36th International Conference on Machine Learning, pages 1042–1051, 2019.

R. Dadashi, L. Hussenot, M. Geist, and O. Pietquin. Primal wasserstein imitation learning. In Proceeedings of

the 9th International Conference on Learning Representations, 2021.

J. Fu, K. Luo, and S. Levine. Learning robust rewards with adverserial inverse reinforcement learning. In

Proceedings of the 6th International Conference on Learning Representations, 2018.

S. K. S. Ghasemipour, R. S. Zemel, and S. Gu. A divergence minimization perspective on imitation learning

methods. In Proceedings of the 3rd Annual Conference on Robot Learning, pages 1259–1277, 2019.

I. J. Good. The population frequencies of species and the estimation of population parameters. Biometrika, 40

(3-4):237–264, 1953.

T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy deep
reinforcement learning with a stochastic actor. In Proceedings of the 35th International Conference on Machine
Learning, pages 1856–1865, 2018.

Y. Han, J. Jiao, and T. Weissman. Minimax estimation of discrete distributions under (cid:96)1 loss. IEEE Transactions

on Information Theory, 61(11):6343–6354, 2015.

J. Ho and S. Ermon. Generative adversarial imitation learning. In Advances in Neural Information Processing

Systems 29, pages 4565–4573, 2016.

20

A. Hussein, M. M. Gaber, E. Elyan, and C. Jayne. Imitation learning: A survey of learning methods. ACM

Computing Surveys, 50(2):1–35, 2017.

C. Jin, Z. Yang, Z. Wang, and M. I. Jordan. Provably efﬁcient reinforcement learning with linear function
approximation. In Proceedings of the 33rd Annual Conference on Learning Theory, pages 2137–2143, 2020.

S. Kamath, A. Orlitsky, D. Pichapati, and A. T. Suresh. On learning distributions from their samples. In

Proceedings of the 28th Annual Conference on Learning Theory, pages 1066–1100, 2015.

L. Ke, S. Choudhury, M. Barnes, W. Sun, G. Lee, and S. Srinivasa.

Imitation learning as f-divergence

minimization. In International Workshop on the Algorithmic Foundations of Robotics, pages 313–329, 2020.

I. Kostrikov, K. K. Agrawal, D. Dwibedi, S. Levine, and J. Tompson. Discriminator-actor-critic: Addressing
sample inefﬁciency and reward bias in adversarial imitation learning. In Proceedings of the 7th International
Conference on Learning Representations, 2019.

I. Kostrikov, O. Nachum, and J. Tompson.

Imitation learning via off-policy distribution matching.

In

Proceedings of the 8th International Conference on Learning Representations, 2020.

S. Levine, C. Finn, T. Darrell, and P. Abbeel. End-to-end training of deep visuomotor policies. Journal of

Machine Learning Research, 17(39):1–40, 2016.

Z. Li, T. Xu, Y. Yu, and Z.-Q. Luo. Rethinking valuedice: Does it really improve performance? arXiv,

2202.02468, 2022.

M. Liu, T. He, M. Xu, and W. Zhang. Energy-based imitation learning. In Proceedings of the 20th International

Conference on Autonomous Agents and Multiagent Systems, pages 809–817, 2021a.

Z. Liu, Y. Zhang, Z. Fu, Z. Yang, and Z. Wang. Provably efﬁcient generative adversarial imitation learning

for online and ofﬂine setting with linear function approximation. arXiv, 2108.08765, 2021b.

D. A. McAllester and L. E. Ortiz. Concentration inequalities for the missing mass and for histogram rule

error. Journal of Machine Learning Research, 4:895–911, 2003.

F. Orabona. A modern introduction to online learning. arXiv, 1912.13213, 2019.

M. Orsini, A. Raichuk, L. Hussenot, D. Vincent, R. Dadashi, S. Girgin, M. Geist, O. Bachem, O. Pietquin,
and M. Andrychowicz. What matters for adversarial imitation learning? Advances in Neural Information
Processing Systems 34, 2021.

T. Osa, J. Pajarinen, G. Neumann, J. A. Bagnell, P. Abbeel, and J. Peters. An algorithmic perspective on

imitation learning. Foundations and Trends in Robotic, 7(1-2):1–179, 2018.

D. Pomerleau. Efﬁcient training of artiﬁcial neural networks for autonomous navigation. Neural Computation,

3(1):88–97, 1991.

M. L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, 2014.

N. Rajaraman, L. F. Yang, J. Jiao, and K. Ramchandran. Toward the fundamental limits of imitation learning.

In Advances in Neural Information Processing Systems 33, pages 2914–2924, 2020.

N. Rajaraman, Y. Han, L. Yang, J. Liu, J. Jiao, and K. Ramchandran. On the value of interaction and function

approximation in imitation learning. Advances in Neural Information Processing Systems 34, 2021a.

N. Rajaraman, Y. Han, L. F. Yang, K. Ramchandran, and J. Jiao. Provably breaking the quadratic error

compounding barrier in imitation learning, optimally. arXiv, 2102.12948, 2021b.

S. Ross and D. Bagnell. Efﬁcient reductions for imitation learning. In Proceedings of the 13rd International

Conference on Artiﬁcial Intelligence and Statistics, pages 661–668, 2010.

21

S. Ross, G. J. Gordon, and D. Bagnell. A reduction of imitation learning and structured prediction to no-regret
online learning. In Proceedings of the 14th International Conference on Artiﬁcial Intelligence and Statistics, pages
627–635, 2011.

S. Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in Machine

Learning, 4(2):107–194, 2012.

D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou,
V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural networks and tree search.
Nature, 529(7587):484–489, 2016.

W. Sun, A. Vemula, B. Boots, and D. Bagnell. Provably efﬁcient imitation learning from observation alone. In

Proceeding of the 36th International Conference on Machine Learning, pages 6036–6045, 2019.

R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT press, 2018.

G. Swamy, S. Choudhury, J. A. Bagnell, and S. Wu. Of moments and matching: A game-theoretic framework
for closing the imitation gap. In Proceeding of the 38th International Conference on Machine Learning, pages
10022–10032, 2021.

U. Syed and R. E. Schapire. A game-theoretic approach to apprenticeship learning. In Advances in Neural

Information Processing Systems 20, pages 1449–1456, 2007.

U. Syed and R. E. Schapire. A reduction from apprenticeship learning to classiﬁcation. In Advances in Neural

Information Processing Systems 23, pages 2253–2261, 2010.

U. Syed, M. H. Bowling, and R. E. Schapire. Apprenticeship learning using linear programming. In W. W.
Cohen, A. McCallum, and S. T. Roweis, editors, Proceedings of the 25th International Conference on Machine
Learning, pages 1032–1039, 2008.

A. Venkatraman, M. Hebert, and J. A. Bagnell. Improving multi-step prediction of learned time series models.

In Proceedings of the 29th AAAI Conference on Artiﬁcial Intelligence, pages 3024–3030, 2015.

Y. Wang, T. Liu, Z. Yang, X. Li, Z. Wang, and T. Zhao. On computation and generalization of generative
adversarial imitation learning. In Proceedings of the 8th International Conference on Learning Representations,
2020.

T. Weissman, E. Ordentlich, G. Seroussi, S. Verdu, and M. J. Weinberger. Inequalities for the l1 deviation of

the empirical distribution. Hewlett-Packard Labs, Techical Report, 2003.

T. Xu, Z. Li, and Y. Yu. Error bounds of imitating policies and environments. In Advances in Neural Information

Processing Systems 33, pages 15737–15749, 2020.

T. Xu, Z. Li, and Y. Yu. Error bounds of imitating policies and environments for reinforcement learning. IEEE

Transactions on Pattern Analysis and Machine Intelligence, 2021.

K. Yosida. Functional analysis. Springer Science & Business Media, 2012.

X. Yu, Y. Lyu, and I. W. Tsang. Intrinsic reward driven imitation learning via generative model. In Proceedings

of the 37th International Conference on Machine Learning, volume 119, pages 10925–10935, 2020.

Y. Zhang, Q. Cai, Z. Yang, and Z. Wang. Generative adversarial imitation learning with neural network
parameterization: Global optimality and convergence rate. In Proceedings of the 37th International Conference
on Machine Learning, pages 11044–11054, 2020.

B. D. Ziebart, A. L. Maas, J. A. Bagnell, and A. K. Dey. Maximum entropy inverse reinforcement learning. In

Proceedings of the 23rd AAAI Conference on Artiﬁcial Intelligence, pages 1433–1438, 2008.

22

A Proof of Results in Section 4

A.1 RBAS MDPs and Useful Properties

In this part, we present some useful properties of TV-AIL on RBAS MDPs. For RBAS MDPs, we know the
expert policy never visits bad states. Thus, we have the following fact.

Fact 1. For any tabular and episodic MDP satisfying Assumption 1, and/ the estimation (cid:100)dπE

h (s, a), we have that

: (cid:100)dπE

h (s) = 0 and (cid:100)dπE

h (s, a) = 0,

h

h

∀

∀

∈

∈

∈ S

[H],
s
∀
[H] : ∑
s

G

B,

a

∀
∈ A
(cid:100)dπE
h (s, a1) = 1,

h

∀

∈

[H],

∀

G,

a

∀

= a1 : (cid:100)dπE

h (s, a) = 0.

∈ S

∈S
s

The following lemma states that on RBAS MDPs, in each time step, the optimal solution πAIL must take

the expert action on certain good state with a positive probability.

Lemma 1. For any tabular and episodic MDP satisfying Assumption 1, suppose that πAIL is an optimal solution of
(5). Then

G such that πAIL

s) > 0.

[H],

(a1

h

s

∀

∈

∃

∈ S

h

|

Proof of Lemma 1. The proof is based on contradiction. Assume that the original statement is false: there
exists a policy πAIL, which is an optimal solution of (5), such that
s) = 0.
G, πAIL
s
Let h denote the smallest time step index such that
h

∈ S
∈
s) = 0. It also implies that

G, πAIL
h

h
∃
(a1

[H],

(a1

∀

s

s

|

∀

∈ S

|

∀

∈

G, ∑

a

S

a1

∈A\{

}

πAIL
h

(a

s) = 1.

|

We construct another policy (cid:101)πAIL, which is only different from πAIL in time step h. In particular, in
G. Here we compare objective values of πAIL and (cid:101)πAIL.
time step h, we assume that (cid:101)πAIL(a1
s) = 1,
Since πAIL is the same as (cid:101)πAIL in the ﬁrst h
1
steps. We only need to compare state-action distribution matching losses from time step h. Notice that
d (cid:101)πAIL
h

s
∈ S
1 steps, their objective values are the same in the ﬁrst h

(s), we obtain

∀
−

−

|

(s) = dπAIL
h
Lossh(πAIL)
(cid:12)
(cid:12)
= ∑
(cid:12)
(cid:12)
(s,a)

(cid:100)dπE
h (s, a)

dπAIL
h

(s, a)

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:100)dπE
h (s, a1)

−

dπAIL
h

(s, a1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ ∑
=a1
a

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:100)dπE
h (s, a)

−

dπAIL
h

(s, a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)


 + ∑
∈S

s

B

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∑
a

(cid:100)dπE
h (s, a)

−

dπAIL
h

(s, a)

(cid:12)
(cid:12)
(cid:12)0

−

dπAIL
h

(s, a)

(cid:12)
(cid:12)
(cid:12)


 + ∑
∈S

s

B

(cid:12)
(cid:12)
(cid:12)0

∑
a

−

dπAIL
h

(s, a)

(cid:12)
(cid:12)
(cid:12)

(cid:100)dπE
h (s, a1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

0

−

(cid:18)

h (s) + dπAIL
(cid:100)dπE

h

(s)

+ ∑
=a1
a
(cid:19)

dπAIL
h

(s),

+ ∑
s

∈S

B









(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= ∑
s

∈S

G

= ∑
s

∈S
= ∑
s

∈S

G

G

and

Lossh( (cid:101)πAIL)
(cid:12)
(cid:12)
= ∑
(cid:100)dπE
h (s, a)
(cid:12)
(cid:12)
(s,a)

d (cid:101)πAIL
h

(s, a)

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= ∑
s

∈S

G





(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:100)dπE
h (s, a1)

−

d (cid:101)πAIL
h

(s, a1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ ∑
=a1
a

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:100)dπE
h (s, a)

−

d (cid:101)πAIL
h

(s, a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

23


 + ∑
∈S

s

B

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∑
a

(cid:100)dπE
h (s, a)

−

d (cid:101)πAIL
h

(s, a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:54)
(cid:54)
(cid:54)
(cid:54)




(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:100)dπE
h (s, a1)

−

d (cid:101)πAIL
h

(s, a1)

(cid:100)dπE
h (s)

dπAIL
h

(s)

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ ∑
s

∈S

B

= ∑
s

G

∈S
= ∑
s
Then we have

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∈S

G


 + ∑
∈S

s

B

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ ∑
a

=a1 |

0

0

|

−

dπAIL
h

(s).

(cid:12)
(cid:12)
(cid:12)0

∑
a

−

dπAIL
h

(s, a)

(cid:12)
(cid:12)
(cid:12)

Lossh( (cid:101)πAIL)

−

Lossh(πAIL) = ∑

G

s

∈S

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:100)dπE
h (s)

−

dπAIL
h

(s)

(cid:12)
(cid:12)
(cid:12)
(cid:12) −

(cid:100)dπE
h (s)

−

dπAIL
h

(s) < 0,

where the last strict inequality follows that there always exists s
h (s) + dπAIL
h(cid:48)
|

< (cid:100)dπE
(s)
h
G such that πAIL
(a1
s, a1) > 0, we therefore know dπAIL

(s). To argue dπAIL
1],
s) > 0. With the reachable assumption (refer to Assumption 1) that
G.

h (s) > 0 and dπAIL
h(cid:48) ∈

∈ S
(s) > 0 for s

G such that (cid:100)dπE

G, we note that

(s) > 0 for all s

(s) > 0,

dπAIL
h

∈ S

[h

−

∀

|

h

h

|

(cid:100)dπE
h (s)
so
−
there exists s
s, s(cid:48) ∈ S

∀

∈ S
G, Ph(s(cid:48)|

∈ S

h

For time step h(cid:48) where h + 1
≤
(cid:12)
(cid:12)
(πAIL) = ∑
(cid:12)
(cid:12)
(s,a)

Lossh(cid:48)

h(cid:48) ≤
(cid:100)dπE
h(cid:48)

H,

(s, a)

dπAIL
h(cid:48)

(s, a)

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= ∑
s
∈S
= ∑
s
∈S
= ∑
s

∈S

G

G

G

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∑
a

∑
a

(cid:100)dπE
h(cid:48)

(s, a)

(cid:100)dπE
h(cid:48)

(s, a)

−

−

dπAIL
h(cid:48)

(s, a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

0

+ ∑
s

∈S

B

(cid:100)dπE
h(cid:48)

(s) + ∑
s

B

∈S

dπAIL
h(cid:48)

(s) = 1 + 1 = 2,

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∑
a

(cid:100)dπE
h(cid:48)

(s, a)

−

dπAIL
h(cid:48)

(s, a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ ∑
s

∈S

B

(cid:12)
(cid:12)
(cid:12)0

∑
a

−

dπAIL
h(cid:48)

(s, a)

(cid:12)
(cid:12)
(cid:12)

(πAIL).

which is the maximal value of TV-AIL’s objective in each time step. Thus, we have that Lossh(cid:48)
Lossh(cid:48)

≤
h=1 Lossh(πAIL). This contra-
dicts the fact that πAIL is the optimal solution of TV-AIL’s objective. Hence the original statement is true and
we ﬁnish the proof.

Combing the above two arguments, we have that ∑H

h=1 Lossh( (cid:101)πAIL) < ∑H

( (cid:101)πAIL)

Lemma 1 claims that there exists certain good state such that the optimal policy must take the expert

action with a positive probability. The following lemma characterizes such states in the last time step.

Lemma 2. Consider any tabular and episodic MDP satisfying Assumption 1. For the estimation (cid:100)dπE

set of visited states as
of (5), then

Vh :=
∈ S
∈ VH, we have πAIL
H

∀

{

s

s

: (cid:100)dπE
(cid:0)a1

h (s) > 0
s(cid:1) > 0.

|

. Suppose that πAIL = (πAIL

1

,

· · ·

}

h (s), we deﬁne the
H ) is an optimal solution

, πAIL

Proof of Lemma 2. With Lemma 6, if πAIL = (πAIL
have

1

, . . . , πAIL
−

H

1, πAIL

H ) is an optimal solution to (5), then we

πAIL

H ∈

argmin
πH

∑
(s,a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

dπ
H(s)πH(a

s)

|

−

(cid:100)dπE
H (s, a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

where dπ

H

πAIL

H ∈

H(s) is computed by πAIL
, . . . , πAIL
1
−
(cid:12)
(cid:12)
∑
(cid:12)
(cid:12)
a
(cid:18) (cid:12)
(cid:12)
(cid:12)
(cid:12)

∑
s
∈VH
(cid:26)

argmin
πH

= argmin
πH

dπ
H(s)πH(a

dπ
H(s)πH(a1

∈A

∑
∈VH

s

1. Then, we obtain

s)

|

−

(cid:100)dπE
H (s, a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

s)

|

−

(cid:100)dπE
H (s, a1)

∑
a

+ ∑
s/
∈VH
+ ∑
=a1
a

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∈A
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

dπ
H(s)πH(a

s)

|

−

(cid:100)dπE
H (s, a)

dπ
H(s)πH(a

s)

|

−

(cid:100)dπE
H (s, a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:19)

24

(cid:54)
(cid:54)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

dπ
h (s)πH(a

s)

|

−

(cid:100)dπE
H (s, a)

(cid:27)

.

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ ∑
s/
∈VH
H (s, a) = 0 for a

∑
a

Since (cid:100)dπE

∈A
= a1, we obtain
(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:26)

πAIL

H ∈

argmin
πH

s

∑
∈VH
(cid:12)
(cid:12)
dπ
H(s)πH(a
(cid:12)
(cid:12)

(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)

∑
∈VH
(cid:12)
(cid:12)
dπ
H(s)πH(a
(cid:12)
(cid:12)

+ ∑
s/
∈VH

∑
a

∈A
(cid:26)

= argmin
πH

s

+ ∑
s/
∈VH

∑
a

s)

|

−

(cid:100)dπE
H (s, a)

(cid:27)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

s)

|

−

(cid:100)dπE
H (s, a)

(cid:27)

.

(cid:12)
(cid:12)
(cid:12)
(cid:12)

dπ
H(s)πH(a1

s)

|

−

(cid:100)dπE
H (s, a1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ dπ

H(s)

(cid:16)

1

πH(a1

s)

|

−

(cid:17)(cid:19)

dπ
H(s)πH(a1

s)

|

−

(cid:100)dπE
H (s, a1)

(cid:12)
(cid:12)
(cid:12)
(cid:12) −

dπ
H(s)πH(a1

(cid:19)

s)

|

∈A
The last equation follows that dπ
H(s) is independent of πH, so it does not contribute to the optimal solution.
Note that policies are independent by the tabular formulation, so we can consider the optimization problem
for each πH(

s) separately. Speciﬁcally, for each s

·|

πAIL

H (a1

|

s) = argmin
πH (a1

s)

[0,1]

|

∈

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∈ VH, we have
(cid:100)dπE
H (s, a1)
s)

|

−

dπ
H(s)πH(a1

(cid:12)
(cid:12)
(cid:12)
(cid:12) −

dπ
H(s)πH(a1

s).

|

H (a1

|

For the above one-dimension optimization problem, Lemma 8 claims that the optimal solution must be
positive, i.e., πAIL

s) > 0. Thus, we ﬁnish the proof if we can verify the conditions in Lemma 8.

In the following part, we verify the conditions required by Lemma 8 by setting a = dπ

(a1

Since πAIL is an optimal solution of TV-AIL’s objective, with Lemma 1, we have that
πAIL
h
h
G. Based on the deﬁnition, for each s

[H], s, s(cid:48) ∈ S
∈
H (s, a1) > 0. Now conditions required by Lemma 8 are

s) > 0. With the assumption that

s, a1) > 0, we have that dπ

G, Ph(s(cid:48)|

∈ S
s
∀

∀

|

H(s), c = (cid:100)dπE
H (s, a1).
G,
[H],
s
h
∃
h (s) > 0,

∈

S
veriﬁed and we obtain that πAIL

∀
∈
∈ VH, (cid:100)dπE
∈ VH.
∀

s

H (a1

|

s) > 0,

A.2 Proof of Proposition 1

Proof of Proposition 1. Recall that in (10), we have deﬁned π(cid:63)

π(cid:63)

h ∈

argmin
πh

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:124)

(cid:100)dπE
h

dπ
h −
(cid:123)(cid:122)
Lossh

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)1
(cid:125)

h as
H
∑
h(cid:48)=h+1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:124)

(cid:13)
(cid:13)
(cid:13)
(cid:13)1
(cid:125)

,

(cid:100)dπE
h(cid:48)

d(cid:63)
h(cid:48) −
(cid:123)(cid:122)
Loss(cid:63)
h(cid:48)
H. Here, π(cid:63)

−

h , d(cid:63)

1, πh, π(cid:63)

h+1, . . . , d(cid:63)

h+1, . . . , π(cid:63)

H are computed by π1, . . . , πh

where dπ
H are deﬁned in a
recursive way, which are assumed to be known in time step h. As discussed, we only consider the case where
π1, . . . , πh
1 are optimal. By the standard analysis of dynamic programming [Bertsekas, 2012], we know that
such deﬁned optimal policy π(cid:63) = (π(cid:63)
H) is also optimal with respect to the original objective in (5).
G on RBAS
h (a1
MDPs. The proof is based on backward induction. Speciﬁcally, our induction assumption is: for each
G. We ﬁrst consider the base case, i.e., we need to
h + 1
s
h(cid:48) ≤
≤
prove that π(cid:63)
H

Keep in mind that our goal is to prove that π(cid:63)

1, we assume that π(cid:63)
h(cid:48)
s

s) = 1 for all h

1 , . . . , π(cid:63)

h+1, . . . , π(cid:63)

1] and s

s) = 1,

s) = 1,

−
1(a1

∈ S

∈ S

(a1

[H

G.

−

H

∈

∀

−

|

|

G is the unique optimal solution, where recall

Base Case. We aim to prove that π(cid:63)
H

1(a1

−

∀

∈ S

|

−

s) = 1,

|

s

∀

∈ S

25

(cid:54)
that

π(cid:63)
H

1 ∈

−

argmin
πH

1

−

dπ
H

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:124)

(cid:91)
dπE
H
1
−

−

1 −
(cid:123)(cid:122)
LossH

1

−

(cid:13)
(cid:13)
(cid:13)
(cid:13)1
(cid:125)

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:124)

.

(cid:13)
(cid:13)
(cid:13)
(cid:13)1
(cid:125)

(cid:100)dπE
H

d(cid:63)
H −
(cid:123)(cid:122)
Loss(cid:63)
H

(13)

We will argue that π(cid:63)
s
H
∀
∈ S
1 Loss(cid:63)
H. Therefore, we can claim that π(cid:63)
optimal with respect to minπH
H
optimal solution to the problem in (13).

G is the optimal with respect to minπH

1(a1

|

−

s) = 1,

1(a1

−

−

|

1 LossH
s

−
s) = 1,

1 and the unique
G is the unique

−
∈ S

∀

For LossH

−

LossH

−

1 = ∑

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∈A
(cid:20) (cid:12)
(cid:12)
(cid:12)
(cid:12)

1, we have that
(cid:91)
∑
dπE
H
−
a
(cid:91)
dπE
H
−
(cid:12)
(cid:12)
(cid:12)
(cid:12)

s
∈S
= ∑
s
∈S
+ ∑
s
∈S
= ∑
s

∑
a
B
∈A
(cid:18) (cid:12)
(cid:91)
(cid:12)
dπE
(cid:12)
H
(cid:12)
−

G

G

∈S

1(s, a)

1(s, a1)

dπ
H

−

−

1(s)πH

1(a

s)

|

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

dπ
H

−

−

1(s)πH

s1)

1(a

|

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ ∑
=a1
a

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:91)
dπE
H
−

1(s, a)

dπ
H

−

−

1(s)πH

1(a

s)

|

−

(cid:21)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:91)
dπE
H
−

1(s, a)

dπ
H

−

−

1(s)πH

1(a

s)

|

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1(s)

dπ
H

−

−

1(s)πH

1(a1

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

s)

|

+ dπ
H

1(s)

−

(cid:16)

1

πH

−

−

1(a1

s)

|

(cid:17) (cid:19)

+ ∑
s

∈S

B

dπ
H

−

1(s).

The last equation follows Fact 1. Notice that dπ
H
following optimization problem:

1(s) is ﬁxed and independent of πH

−

−

1, so we can obtain the

argmin
πH

1

LossH

−

1 = argmin
πH

1

−

s

−
Since elements in
for each s

πH
G individually:

·|

1(

{

−

s) : s

G

∈ S

}

∈ S

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:91)
dπE
H
−

1(s)

∑

G

dπ
H

−

−

1(s)πH

1(a1

−

(cid:12)
(cid:12)
(cid:12)
(cid:12) −

s)

|

dπ
H

−

1(s)πH

1(a1

s).

|

−

∈S
are independent, we can consider the above optimization problem

argmin
1(a1

s)

|

∈

−

πH

[0,1]

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:91)
dπE
H
−

1(s)

dπ
H

−

−

1(s)πH

1(a1

−

(cid:12)
(cid:12)
(cid:12)
(cid:12) −

s)

|

dπ
H

−

1(s)πH

1(a1

s).

|

−

For this one-dimension optimization problem, we can use Lemma 7 to show that π(cid:63)
H
optimal solution.
For Loss(cid:63)

: (cid:100)dπE

s

1(a1

|

−

s) = 1 is an

, i.e., the set of visited states in time

step H. For any s /

H, let us introduce the notation
∈ VH, we have that (cid:100)dπE
d(cid:63)
(cid:100)dπE
H(s, a)
H (s, a)

H = ∑

Loss(cid:63)

−

VH :=

{
H (s) = 0. Then, we obtain

∈ S

H (s) > 0

}

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∑
a

∈A
∑
a
(cid:12)
(cid:12)
(cid:12)
(cid:12)

G

s
∈S
= ∑
s
∈S
= ∑
s
∈VH
(cid:124)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:100)dπE
H (s, a)

−

d(cid:63)
H(s, a)

∈A
(cid:100)dπE
H (s)

−
(cid:123)(cid:122)
Term I

d(cid:63)
H(s, a1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:125)

+ ∑
s
∈VH
(cid:124)

B

d(cid:63)
H(s)

+ ∑
s
∈S
∑
=a1
a
(cid:123)(cid:122)
Term II
H), where πH

d(cid:63)
H(s, a)

+

∑
G and s/
∈VH
(cid:123)(cid:122)
Term III

d(cid:63)
H(s)

(cid:125)

+ ∑
s
∈S
(cid:124)

B

(cid:123)(cid:122)
Term IV

(cid:125)

d(cid:63)
H(s)

.

(14)

s
∈S
(cid:124)

(cid:125)

H is computed by (π1, . . . , πH

Note that d(cid:63)
given. Speciﬁcally, π1, . . . , πH
care about the case where they are not optimal, and π(cid:63)
H is an optimal solution to the problem:
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1 is the decision variable and the others are
2 are considered to be optimal (to be deﬁned recursively) because we do not

1, π(cid:63)

(cid:100)dπE
H

dπ
H −

H ∈

π(cid:63)

−

−

−

.

argmin
πH

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

26

(cid:54)
(cid:54)
Now let us consider the ﬁrst three terms in (14). For d(cid:63)
(7), we have

H(s) with s

∈ S

G, with the Bellman-ﬂow equation in

H(s) = ∑
d(cid:63)
s(cid:48)∈S

∑
a

∈A

dπ
H

−

1(s(cid:48))πH

1(a

|

−

s(cid:48))PH

1(s

−

s(cid:48), a) = ∑
s(cid:48)∈S

|

G

dπ
H

−

1(s(cid:48))πH

1(a1

|

−

s(cid:48))PH

1(s

|

−

s(cid:48), a1).

Accordingly, we have

s

Term I = ∑
∈VH
= ∑
s
∈VH

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:100)dπE
H (s)

(cid:100)dπE
H (s)

−

−

(cid:18)

∑
s(cid:48)∈S
∑
s(cid:48)∈S

G

dπ
H

−

G

1(s(cid:48))πH

1(a1

|

−

s(cid:48))PH

1(s

|

−

s(cid:48), a1)

(cid:19)

π(cid:63)

H(a1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

s)

|

dπ
H

1(s(cid:48))PH

1(s

|

−

−

s(cid:48), a1)π(cid:63)

H(a1

s)πH

|

1(a1

s(cid:48))

|

−

s

Term II = ∑
∈VH
= ∑
s(cid:48)∈S

G

(cid:32)

(cid:17)

(cid:16)

1

π(cid:63)

H(a1

s)

|

−

πH

−

1(a1

s(cid:48))

|

(cid:32)

s

∑
∈VH

G

∑
s(cid:48)∈S
dπ
H

dπ
H

−

1(s(cid:48))πH

1(a1

|

−

s(cid:48))PH

1(s

|

−

s(cid:48), a1)

1(s(cid:48))PH

1(s

|

−

−

s(cid:48), a1)

(cid:16)

1

π(cid:63)

H(a1

s)

|

−

(cid:33)

(cid:17)

,

(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

(cid:33)

Term III =

∑
G and s/

∈VH

s

∈S

(cid:18)

∑
G
s(cid:48)∈S


dπ
H

−

1(s(cid:48))πH

1(a1

|

−

s(cid:48))PH

1(s

|

−

s(cid:48), a1)

(cid:19)



and

and

= ∑
s(cid:48)∈S

G

πH

1(a1

s(cid:48))



−

|

s

∑
G and s/

dπ
H

−

1(s(cid:48))PH

1(s

s(cid:48), a1)

 .

−

|

∈S

H(s) with s

∈VH
Next, we consider the last term in (14). For d(cid:63)
B, recall that when the agent takes a non-expert
action, it transits into bad states. Therefore, the probability of visiting bad states in time step H arises from
two parts. One is the probability of visiting bad states in time step H
1 and the other is the probability of
visiting good states and taking non-expert actions in time step H
H(s) = ∑
d(cid:63)
s(cid:48)∈S
= ∑
s(cid:48)∈S

1(s(cid:48)) + ∑
s(cid:48)∈S
1(s(cid:48)) + ∑
s(cid:48)∈S

1. Accordingly, we obtain

∑
=a1
a

1(s(cid:48))

1(s(cid:48))

1(a1

∈ S

dπ
H

dπ
H

dπ
H

dπ
H

1(a

∑

πH

πH

s(cid:48))

s(cid:48))

−
(cid:18)

−

−

∈S

(cid:19)

(cid:16)

(cid:17)

1

−

−

−

−

−

−

|

|

G

G

.

s

B

B

B

Then, it is ready to get

Term IV = ∑
s(cid:48)∈S
= ∑
s(cid:48)∈S

dπ
H

dπ
H

−

−

B

B

G

1(s) + ∑
s(cid:48)∈S
1(s(cid:48)) + ∑
s(cid:48)∈S

dπ
H

−

1(s(cid:48))

(cid:16)

1

πH

(cid:17)

1(a1

s(cid:48))

|

−

dπ
H

−

G

1(s(cid:48))

−

dπ
H

−

G

1(s(cid:48))πH

1(a1

s(cid:48)).

|

−

−
∑
s(cid:48)∈S

Subsequently, we merge the optimization variable πH

1 in the second, third, and forth terms to obtain

−

Term II + Term III + Term IV

πH

1(a1

s(cid:48))

|

−

(cid:32)

∑
s
∈VH
(cid:18)

dπ
H

−

1(s(cid:48))PH

1(s

|

−

s(cid:48), a1)

(cid:33)

(cid:17)

(cid:16)

1

π(cid:63)

H(a1

s)

|

−

πH

πH

−

−

G

G

1(a1

1(a1

|

|

s(cid:48))

s
s(cid:48))dπ
H

dπ
H

∑
G and s/
∈VH
∈S
1(s(cid:48)) + constant

−

−

1(s(cid:48))PH

G

= ∑
s(cid:48)∈S
+ ∑
s(cid:48)∈S
∑
s(cid:48)∈S

−

(cid:19)

s(cid:48), a1)

1(s

|

−

27

(cid:54)
= ∑
s(cid:48)∈S

G

+

s

∈S
= ∑
s(cid:48)∈S

G

∑
G and s/

∈VH

πH

−

1(a1

s(cid:48))

|

πH

−

1(a1

s(cid:48))

|

(cid:32)

∑
∈VH

s

dπ
H

−

1(s(cid:48))PH

1(s

|

−

s(cid:48), a1)

dπ
H

−

1(s(cid:48))PH

1(s

|

−

s(cid:48), a1)π(cid:63)

H(a1

s)

|

∑
∈VH

−

s
(cid:33)

dπ
H

−

1(s(cid:48))PH

1(s

|

−

s(cid:48), a1)

dπ
H

−

−

1(s(cid:48))

+ constant

(cid:32)

∑

G

s

∈S

dπ
H

−

1(s(cid:48))PH

1(s

|

−

s(cid:48), a1)

∑
∈VH

−

s

dπ
H

−

1(s(cid:48))PH

1(s

|

−

s(cid:48), a1)π(cid:63)

H(a1

s)

|

−

dπ
H

−

1(s(cid:48))

(cid:33)

(15)

=

−

∑
s(cid:48)∈S

G

πH

−

1(a1

s(cid:48))

|

(cid:32)

∑
∈VH

s

dπ
H

−

1(s(cid:48))PH

1(s

|

−

s(cid:48), a1)π(cid:63)

H(a1

s)

|

+ constant,

(cid:33)

G, we have ∑

G PH
where in the last equation we use the fact that for s(cid:48) ∈ S
and the third term in (15) are canceled. In the above equations, constant = ∑
which is independent of πH
1. Back to (14), we get that
−
(cid:12)
(cid:12)
H = ∑
(cid:100)dπE
H (s)
(cid:12)
(cid:12)
∈VH
∑
s(cid:48)∈S

∑
s(cid:48)∈S

s(cid:48), a1)π(cid:63)

s(cid:48), a1)π(cid:63)

∑
∈VH

1(s(cid:48))PH

1(s(cid:48))PH

Loss(cid:63)

H(a1

1(a1

G
(cid:32)

dπ
H

dπ
H

1(s

1(s

πH

s(cid:48))

−

−

∈S

−

−

−

−

−

|

|

|

|

G

s

s

s

s)πH

H(a1

Then we have that

s(cid:48), a1) = 1, so the ﬁrst term
1(s
|
B dπ
1(s(cid:48)),
s(cid:48)∈S
H

1(s(cid:48)) + ∑

G dπ
H

−

−

−
s(cid:48)∈S

1(a1

s(cid:48))

|

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:33)

s)

|

+ constant .

(16)

Loss(cid:63)
H

argmin
πH

1

−

= argmin
πH

1

−

(cid:26)

s

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∑
∈VH
∑
s(cid:48)∈S

(cid:100)dπE
H (s)

−

dπ
H

−

1(s(cid:48))PH

1(s

|

−

s(cid:48), a1)π(cid:63)

H(a1

s)πH

|

−

1(a1

πH

−

G

dπ
H

−

1(s(cid:48))PH

1(s

|

−

s(cid:48), a1)π(cid:63)

H(a1

s)

|

.

∑
s(cid:48)∈S
s(cid:48))

|

G
(cid:18)

∑
∈VH

s

(cid:12)
(cid:12)
(cid:12)
(cid:12)

s(cid:48))

1(a1

|
(cid:19)(cid:27)

s) = 1 is the unique
For this optimization problem, we will apply Lemma 10 to show that
optimal solution. In particular, we can verify the conditions required by Lemma 10 by deﬁning the following
terms:

∈ S

∀

−

s

|

G, π(cid:63)
H

1(a1

, n =

m =

|VH|
∈ VH, s(cid:48)
∈ S

s

s(cid:48)

∀

∀

∈ S
G, d(s(cid:48)) = ∑
∈VH

s

s

G(cid:12)
(cid:12)
(cid:12) ,

(cid:12)
(cid:12)
(cid:12)S
∀
G, A(s, s(cid:48)) = dπ
H
−
dπ
1(s(cid:48))PH
H

∈ VH, c(s) = (cid:100)dπE
1(s(cid:48))PH
1(s

−

|

−

H (s),

1(s
|
−
s(cid:48), a1)π(cid:63)

s(cid:48), a1)π(cid:63)
H(a1
H(a1
s).

s),

|

|

Now we verify the conditions in Lemma 10. To start with, we note that Lemma 1 implies that if πAIL
is an optimal solution to (8) on RBAS MDPs, then
Intuitively,
in each time step, πAIL should take the expert action on some good state with a positive probability.
Through the standard analysis of dynamic programming [Bertsekas, 2016], we have that Π(cid:63) = ΠAIL.
s) > 0. Second, keep in mind that we focus on the case
This implies that
∀
∈
where (π1, . . . , πH
2). With the reachable assumption (refer to Assumption 1) that
−
G, Ph(s(cid:48)|
1(s(cid:48)) > 0. With Lemma 2, we have
[H], s, s(cid:48) ∈ S
s(cid:48) ∈ S
h
∈
∀
∈ VH, π(cid:63)
s) > 0. Hence we have that A > 0, where > means element-wise comparison. Besides,
s
that
|
∀
on the one hand, we have that

[H],
h
s
∈ S
∃
1 , . . . , π(cid:63)
2) = (π(cid:63)
H

s, a1) > 0, we further infer that

G, πAIL
h

s) > 0.

G, dπ
H

G, π(cid:63)

H(a1

h (a1

∈ S

[H],

(a1

∈

∀

∃

∀

h

−

−

s

|

|

∑
∈VH

s

c(s) = ∑
∈VH

s

(cid:100)dπE
H (s) = 1.

28

On the other hand, we have that
∑
s(cid:48)∈S

∑
∈VH

s

Therefore, we obtain

A(s, s(cid:48))

G

≤

∑
∈VH

s

∑
s(cid:48)∈S

G

dπ
H

−

1(s(cid:48))PH

1(s

|

−

s(cid:48), a1)

1.

≤

∑
∈VH

s

∑
s(cid:48)∈S

G

A(s, s(cid:48))

c(s).

≤

∑
∈VH

s

For each s(cid:48) ∈ S

G, it holds that

∑
∈VH

A(s, s(cid:48)) = ∑
∈VH
Thus, we have veriﬁed the conditions in Lemma 10. With Lemma 10, we obtain that π(cid:63)
H
is the unique optimal solution of Loss(cid:63)
of minπH

∈ S
H, which completes the proof of the base case.

H. Therefore, π(cid:63)
H

1 + Loss(cid:63)

s) = d(s(cid:48)).

s(cid:48), a1)π(cid:63)

1(s(cid:48))PH

1 LossH

s) = 1,

H(a1

1(a1

dπ
H

1(s

∀

−

−

−

s

|

|

|

s

s

1(a1

s) = 1,

s

|
G is the unique optimal solution

∈ S

∀

−

G

−

−

Induction Step. The main proof strategy is similar to what we have used in the proof of the base case
1, π(cid:63)
G is the unique
(a1
h(cid:48)
∈ S
G is the unique optimal solution.

but is more tricky. We assume that for step h(cid:48) = h + 1, h + 2,
optimal solution. We aim to prove that for step h, π(cid:63)
h (a1
Recall that

, H
· · ·
s) = 1,

s) = 1,

−
s
∀

∈ S

∀

s

|

|

π(cid:63)

h ∈

argmin
πh

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:124)

(cid:100)dπE
h

dπ
h −
(cid:123)(cid:122)
Lossh

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)1
(cid:125)

,

(cid:13)
(cid:13)
(cid:13)
(cid:13)1
(cid:125)

(cid:100)dπE
h(cid:48)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:124)

H
∑
h(cid:48)=h+1

d(cid:63)
h(cid:48) −
(cid:123)(cid:122)
Loss(cid:63)
h(cid:48)
h+1, . . . , π(cid:63)

H are computed by π1, . . . , πh
H. Similarly, we will ﬁrst prove that
G is an optimal solution with respect to minπh Lossh. Then we will prove that for each
G is the unique optimal solution with respect to minπH
. In this

−

s

1, πh, π(cid:63)

1 Loss(cid:63)
h(cid:48)

−

G is the unique optimal solution.

∈ S
∀
s) = 1,

s

∀

∈ S

|
≤

h , d(cid:63)
s) = 1,
h(cid:48) ≤

h+1, . . . , d(cid:63)
s
∈ S
∀
H, π(cid:63)
h (a1

where dπ
π(cid:63)
h (a1
h + 1
way, we can argue that π(cid:63)
For Lossh, we have that
Lossh = ∑

|

s) = 1,
h (a1

|

∑
a

(cid:12)
(cid:12)
(cid:12)
(cid:12)
∈A
(cid:18) (cid:12)
(cid:12)
(cid:12)
(cid:12)

s
∈S
= ∑
s

∈S
= ∑
s

∈S

G

G

(cid:100)dπE
h (s, a)

dπ
h (s)πh(a

(cid:12)
(cid:12)
(cid:12)
(cid:12)

s)

|

−

−

(cid:100)dπE
h (s, a1)

dπ
h (s)πh(a1

(cid:18) (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:100)dπE
h (s)

−

dπ
h (s)πh(a1

s)

|

(cid:12)
(cid:12)
(cid:12)
(cid:12)

s)

+ ∑
=a1
a
(cid:16)

+ dπ

h (s)

|
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:19)

dπ
h (s)πh(a

s)

|

(cid:17) (cid:19)

1

−

πh(a1

s)

|

dπ
h (s)

dπ
h (s).

+ ∑
s

B

∈S
+ ∑
s

∈S

B

argmin
πh

Lossh = argmin

πh

∑

G

s

∈S

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:100)dπE
h (s)

−

dπ
h (s)πh(a1

(cid:12)
(cid:12)
(cid:12)
(cid:12) −

s)

|

dπ
h (s)πh(a1

s).

|

Notice that dπ

h (s) is independent of πh, then we have that

By the tabular formulation, we can consider the above optimization problem for each s

G individually:

∈ S

argmin
s)

πh(a1

[0,1]

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:100)dπE
h (s)

−

dπ
h (s)πh(a1

(cid:12)
(cid:12)
(cid:12)
(cid:12) −

s)

|

dπ
h (s)πh(a1

s).

|

∈
For this one-dimension optimization problem, we can show that π(cid:63)
Lemma 7. Thus, we obtain that π(cid:63)
h (a1
H
−

s
∀
1, we consider Loss(cid:63)
h(cid:48)

s) = 1,

∈ S

h (a1
G is the optimal solution of Lossh.

|

|

|

h(cid:48) ≤
G. Then we have

Next, for each h + 1
1, π(cid:63)
s
h(cid:48)

s) = 1,

(a1

|

H

−

. Notice that we assume that for each h + 1

h(cid:48) ≤

≤

s) = 1 is the optimal solution by

≤
∀
∈ S
Loss(cid:63)
h(cid:48)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:100)dπE
h(cid:48)

= ∑
s

∈S

∑
a

∈A

(s, a)

d(cid:63)
h(cid:48)

−

(s, a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

29

(cid:54)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:100)dπE
h(cid:48)

(s, a)

−

d(cid:63)
h(cid:48)

(s)π(cid:63)
h(cid:48)

(a

(s, a1)

d(cid:63)
h(cid:48)

(s)π(cid:63)
h(cid:48)

(a1

|

−

s)

d(cid:63)
h(cid:48)

(s)

d(cid:63)
h(cid:48)

(s, a)

∑
a

∈A

B

s)

|
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ ∑
s
+ ∑
s

∈S

B

∈S

∈A
(cid:100)dπE
h(cid:48)

∑
a
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:100)dπE
h(cid:48)

= ∑
s
∈S
= ∑
s
∈S
= ∑
s
∈S
(cid:124)

G

G

G

(s)

d(cid:63)
h(cid:48)

−

(s)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:125)

(cid:123)(cid:122)
Term I
1, πh, π(cid:63)

(s)

.

(17)

+ ∑
s
∈S
(cid:124)

d(cid:63)
h(cid:48)

B
(cid:123)(cid:122)
Term II

(cid:125)

Here d(cid:63)
h+1, . . . , π(cid:63)
h(cid:48)
h(cid:48)
good states, the agent could visit good states. With the Bellman-ﬂow equation in (7), we have that

). Note that only through taking the expert action on
G,

is induced by (π1, . . . , πh

−

s

∀

∈ S

d(cid:63)
h(cid:48)

∑
a

(s) = ∑
s(cid:48)∈S
= ∑
s(cid:48)∈S

G

dπ
h (s(cid:48))πh(a

∈A
dπ
h (s(cid:48))πh(a1

|

s(cid:48))Pπ(cid:63) (cid:0)sh(cid:48)
|
s(cid:48))Pπ(cid:63) (cid:16)

sh(cid:48)

= s

sh = s(cid:48), ah = a(cid:1)
|
sh = s(cid:48), ah = a1(cid:17)

,

= s

|

where Pπ(cid:63)
= s
(s(cid:48), a(cid:48)) in time step h via policy π(cid:63)

(sh(cid:48)

|

h+1, . . . , π(cid:63)

sh = s(cid:48), ah = a1) refers to the transition probability of s in time step h(cid:48) by starting from

Term I = ∑

(cid:100)dπE
h(cid:48)

(s)

−

∑
s(cid:48)∈S

G

G

s

∈S

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

h(cid:48)−

1. Then we obtain that
h (s(cid:48))Pπ(cid:63) (cid:16)
dπ

= s

sh(cid:48)

|

sh = s(cid:48), ah = a1(cid:17)

πh(a1

s(cid:48))

|

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

sh = s(cid:48), ah = a1(cid:1) is independent of πh. Besides, as for
Notice that the conditional probability Pπ(cid:63) (cid:0)sh(cid:48)
|
G, the visitation probability of bad states in step h(cid:48)
each h(cid:48) = h + 1, h + 2,
s) = 1,
∈ S
· · ·
comes from two parts in step h. One is the visitation probability of bad states in step h. The other is the
probability of visiting good states and taking non-expert actions in step h. We obtain

1, π(cid:63)
h(cid:48)

= s
s

(a1

, H

−

∀

|

Term II = ∑

s
∈S
= ∑
s
∈S
Plugging Term I and Term II into Loss(cid:63)
h(cid:48)

B

h (s) + ∑
dπ
s(cid:48)∈S
h (s) + ∑
dπ
s(cid:48)∈S
in (17) yields

B

∑
=a1
a

dπ
h (s(cid:48))πh(a

s(cid:48))

|

dπ
h (s(cid:48))

(cid:16)

1

πh(a1

s(cid:48))

|

−

(cid:17)

.

G

G

Loss(cid:63)
h(cid:48)

h (s(cid:48))Pπ(cid:63) (cid:16)
dπ

G

sh(cid:48)

= s

|

sh = s(cid:48), ah = a1(cid:17)

πh(a1

(cid:100)dπE
h(cid:48)

(s)

dπ
h (s(cid:48))

G

∑
−
s(cid:48)∈S
(cid:16)
1

−

(cid:100)dπE
h(cid:48)

(s)

∑
−
s(cid:48)∈S
dπ
h (s(cid:48))πh(a1

G

G

(cid:12)
(cid:12)
= ∑
(cid:12)
(cid:12)
(cid:12)
s
∈S
+ ∑
s(cid:48)∈S
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
∑
s(cid:48)∈S

= ∑
s

−

∈S

G

G
B dπ

(cid:17)

πh(a1

s(cid:48))

|
h (s(cid:48))Pπ(cid:63) (cid:16)
dπ

s(cid:48)) + constant .

|

dπ
h (s)

+ ∑
s

∈S

B

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

s(cid:48))

|

s(cid:48))

|

sh = s(cid:48), ah = a1(cid:17)

πh(a1

sh(cid:48)

= s

|

Here constant = ∑
G dπ
s
∈S
proof of the base case. Then we have that

h (s) + ∑

s(cid:48)∈S

h (s(cid:48)) is independent of πh. This equation is similar to (16) in the

argmin
πh

= argmin
πh

Loss(cid:63)
h(cid:48)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∑

G

s

∈S

(cid:100)dπE
h(cid:48)

(s)

−

∑
s(cid:48)∈S

G

h (s(cid:48))Pπ(cid:63) (cid:16)
dπ

sh(cid:48)

= s

|

sh = s(cid:48), ah = a1(cid:17)

πh(a1

|

s(cid:48))

For this optimization problem, we can again use Lemma 10 to prove that

s

∀

∈ S

30

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) −
G, π(cid:63)

∑
s(cid:48)∈S
h (a1

G

dπ
h (s(cid:48))πh(a1

s(cid:48)).

|

s) = 1 is the unique

|

(cid:54)
global optimal solution. To check the conditions required by Lemma 10, we deﬁne

s

G(cid:12)
(cid:12)
(cid:12) ,

(cid:12)
(cid:12)
(cid:12)S
∀
G, A(s, s(cid:48)) = dπ

∈ S

G, c(s) = (cid:100)dπE
h(cid:48)
h (s(cid:48))Pπ(cid:63) (cid:16)

sh(cid:48)

(s),

sh = s(cid:48), ah = a1(cid:17)

,

= s

|

G, d(s(cid:48)) = dπ

h (s(cid:48)).

m = n =

s, s(cid:48)

∀

∈ S

s(cid:48)

∀

∈ S

Now we verify the conditions in Lemma 10. Following the same argument in the proof of base case, we have
that

G,

s, s(cid:48) ∈ S

∀

h (s(cid:48)) > 0, and Pπ(cid:63) (cid:16)
dπ

sh(cid:48)

= s

|

sh = s(cid:48), ah = a1(cid:17)

> 0.

Then we can obtain that A > 0 where > means element-wise comparison. Besides, on the one hand, we
have that

On the other hand, we have that

∑

c(s) = ∑

G

s

∈S

G

s

∈S

(cid:100)dπE
h(cid:48)

(s) = 1.

∑

∑
s(cid:48)∈S

A(s, s(cid:48)) = ∑

h (s(cid:48))Pπ(cid:63) (cid:16)
dπ

sh(cid:48)

= s

|

sh = s(cid:48), ah = a1(cid:17)

∑
s(cid:48)∈S

G

≤

∑
s(cid:48)∈S

G

dπ
h (s(cid:48))

1.

≤

G

s

∈S

s
To summarize, we obtain

∈S

G

G

∑

G

s

∈S

∑
s(cid:48)∈S

G

A(s, s(cid:48))

c(s).

≤

∑

G

s

∈S

For each s(cid:48) ∈ S

G, we further have that
A(s, s(cid:48)) = ∑

∑

s

∈S

G

s

G

∈S

H

≤

In equation (a), we use ∑
s
1, π(cid:63)
(cid:96) (a1
(cid:96)
s) = 1,
−
we obtain that π(cid:63)
h + 1
h(cid:48) ≤

|
h (a1
1.
−
Finally, we consider Loss(cid:63)
H = ∑

Loss(cid:63)

≤

H

|

h (s(cid:48))Pπ(cid:63) (cid:16)
dπ

sh = s(cid:48), ah = a1(cid:17) (a)

= dπ

h (s(cid:48)) = d(s(cid:48)).

sh(cid:48)

= s

|

G Pπ(cid:63) (cid:0)sh(cid:48)
∈S
s
∈ S
∀
s) = 1,
s
∀

∈ S

|

= s

sh = s(cid:48), ah = a1(cid:1) = 1 due to the assumption that for each h + 1

≤
G. Thus, we have veriﬁed the conditions in Lemma 10. By Lemma 10,
for each time step h(cid:48), where

G is the unique optimal solution of Loss(cid:63)
h(cid:48)

H. Recall the deﬁnition that

VH =

s

{

∈ S

, (cid:100)dπE

H (s) > 0

}

. Then we derive

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∑
a

∈A
∑
a
(cid:12)
(cid:12)
(cid:12)
(cid:12)

G

s
∈S
= ∑
s
∈S
= ∑
s
∈VH
(cid:124)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:100)dπE
H (s, a)

d(cid:63)
H(s, a)

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:100)dπE
H (s, a)

−

d(cid:63)
H(s, a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∈A
(cid:100)dπE
H (s)

−
(cid:123)(cid:122)
Term I

d(cid:63)
H(s, a1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:125)

+ ∑
s
∈VH
(cid:124)

d(cid:63)
H(s)

B

+ ∑
s
∈S
∑
=a1
a
(cid:123)(cid:122)
Term II

d(cid:63)
H(s, a)

+

∑
G and s/
∈VH
(cid:123)(cid:122)
Term III

d(cid:63)
H(s)

(cid:125)

+ ∑
s
∈S
(cid:124)

B

(cid:123)(cid:122)
Term IV

(cid:125)

d(cid:63)
H(s)

.

s
∈S
(cid:124)

(cid:125)

Here d(cid:63)
H is induced by (π1, . . . , πh
G,
s

∀

∈ S

1, πh, π(cid:63)

h+1, . . . , π(cid:63)

H). With the Bellman-ﬂow equation in (7), we have

−

∑
a

H(s) = ∑
d(cid:63)
s(cid:48)∈S
= ∑
s(cid:48)∈S

G

dπ
h (s(cid:48))πh(a

s(cid:48))Pπ(cid:63)

(sH = s

∈A
dπ
h (s(cid:48))πh(a1

|
s(cid:48))Pπ(cid:63)

|

(sH = s

|

sh = s(cid:48), ah = a)

|
sh = s(cid:48), ah = a1).

31

(cid:54)
Accordingly, we have

Term I = ∑
∈VH

s

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:100)dπE
H (s)

−

∑
s(cid:48)∈S

G

h (s(cid:48))Pπ(cid:63)
dπ

(sH = s

|

sh = s(cid:48), ah = a1)π(cid:63)

H(a1

s)πh(a1

|

(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

s(cid:48))

|

Term II = ∑
s(cid:48)∈S

G

πh(a1

s(cid:48))

|

(cid:32)

∑
∈VH

s



h (s(cid:48))Pπ(cid:63)
dπ

(sH = s

|

sh = s(cid:48), ah = a1)

(cid:16)

1

π(cid:63)

H(a1

s)

|

−

(cid:33)

(cid:17)

,



and

and

πh(a1

Term III = ∑
s(cid:48)∈S
G, the visitation
With the assumption that for time step h(cid:48) = h + 1, h + 2,
probability of bad states in step H comes from two parts in step h. By a similar argument with the previous
analysis of Loss(cid:63)
h(cid:48)

sh = s(cid:48), ah = a1)

∑
G and s/

h (s(cid:48))Pπ(cid:63)
dπ

1, π(cid:63)
h(cid:48)

(sH = s

, we get

s) = 1,

∈ S

∈VH

(a1

 .

· · ·

, H

s(cid:48))



−

∈S

∀

s

|

|

|

G

s

Term IV = ∑

h (s) + ∑
dπ
s(cid:48)∈S
h (s) + ∑
dπ
s(cid:48)∈S

G

G

s
∈S
= ∑
s

B

B

∑
=a1
a

dπ
h (s(cid:48))πh(a

s(cid:48))

|

dπ
h (s(cid:48))

(cid:16)

1

πh(a1

s(cid:48))

|

−

(cid:17)

.

∈S
Plugging Term I, II, III, and IV into Loss(cid:63)
H yields that
h (s(cid:48))Pπ(cid:63)
dπ

(cid:100)dπE
H (s)

Loss(cid:63)

(sH = s

sh = s(cid:48), ah = a1)π(cid:63)

H(a1

|

s)πh(a1

|

(cid:12)
(cid:12)
(cid:12)
(cid:12)

s(cid:48))

|

h (s(cid:48))Pπ(cid:63)
dπ

(sH = s

|

sh = s(cid:48), ah = a1)π(cid:63)

H(a1

(cid:19)

s)

|

+ constant,

h (s(cid:48)), which is independent of πh. This equation is similar to (16)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

s

H = ∑
∈VH
∑
s(cid:48)∈S
s

−

−

∑
s(cid:48)∈S
(cid:18)
s(cid:48))

G

G

πh(a1

|
h (s) + ∑

∑
∈VH

s

where constant = ∑
in the proof of the base case. Then we have that
Loss(cid:63)
H

s(cid:48)∈S

G dπ

B dπ

∈S

argmin
πh

= argmin
πh

−

∑
s(cid:48)∈S

G

(cid:26)

s

∑
∈VH
πh(a1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:100)dπE
H (s)

s(cid:48))

|

(cid:18)

∑
∈VH

s

−

G

∑
s(cid:48)∈S
h (s(cid:48))Pπ(cid:63)
dπ

h (s(cid:48))Pπ(cid:63)
dπ

(sH = s

|

sh = s(cid:48), ah = a1)π(cid:63)

H(a1

s)πh(a1

|

(cid:12)
(cid:12)
(cid:12)
(cid:12)

s(cid:48))

|

(sH = s

|

sh = s(cid:48), ah = a1)π(cid:63)

H(a1

(cid:19)(cid:27)

.

s)

|

For this optimization problem, we can again use Lemma 10 to prove that
optimal solution. To check the conditions in Lemma 10, we deﬁne
(cid:12)
(cid:12)
(cid:12)S
∀
G, A(s, s(cid:48)) = dπ
h (s(cid:48))Pπ(cid:63)
dπ

∈ VH, c(s) = (cid:100)dπE
h (s(cid:48))Pπ(cid:63)

(sH = s

(sH = s

H (s),

G(cid:12)
(cid:12)
(cid:12) ,

, n =

|VH|
∈ VH, s(cid:48)
∈ S

m =

s(cid:48)

∀

∀

s

s

|

∈ S
G, d(s(cid:48)) = ∑
∈VH

s

sh = s(cid:48), ah = a1)π(cid:63)
H(a1
s).

sh = s(cid:48), ah = a1)π(cid:63)

H(a1

|

s),

|

|

s

∀

∈ S

G, π(cid:63)

h (a1

|

s) = 1 is the unique

Following the same argument in the proof of base case, we have that A > 0. On the one hand, we have

∑
∈VH

s

c(s) = ∑
∈VH

s

(cid:100)dπE
H (s) = 1.

32

(cid:54)
On the other hand,
∑
s(cid:48)∈S
Therefore, we obtain

∑
∈VH

s

G

A(s, s(cid:48))

≤

∑
∈VH

s

∑
s(cid:48)∈S

G

h (s(cid:48))Pπ(cid:63)
dπ

(sH = s

|

sh = s(cid:48), ah = a1)

≤

∑
s(cid:48)∈S

G

dπ
h (s(cid:48))

1.

≤

∑
∈VH

s

∑
s(cid:48)∈S

G

A(s, s(cid:48))

c(s).

≤

∑
∈VH

s

G,

We further obtain

s(cid:48) ∈ S
∀
∑
∈VH

s

A(s, s(cid:48)) = ∑
∈VH

s

h (s(cid:48))Pπ(cid:63)
dπ

(sH = s

|

sh = s(cid:48), ah = a1)π(cid:63)

H(a1

s) = d(s(cid:48)).

|

Thus we have veriﬁed the conditions in Lemma 10. By Lemma 10, we obtain π(cid:63)
unique optimal solution of minπh Loss(cid:63)
Lossh + ∑H
optimal solution of argminπh
proof is done.

H. Therefore, we prove that π(cid:63)
h(cid:48)=h+1 Loss(cid:63)
h(cid:48)

G is the
h (a1
G is the unique
s) = 1,
. Thus, we ﬁnish the induction proof and the whole

s) = 1,
|
s
∀

h (a1

∈ S

∈ S

∀

s

|

A.3 Proof of Proposition 2

Figure 4: A simple example to show that TV-AIL’s objective in (5) is non-convex.

Proof of Proposition 2. To prove that the objective in (5) could be non-convex, we provide an instance shown
in Figure 4. In particular, there are 5 states (s1, s2, s3, s4, s5) and two actions (a, a). Each arrow shows a
deterministic transition. The initial state is s1 and planning horizon is 2. Assume a is the expert action and
there is only one expert trajectory: (s1, a)

(s2, a). We can calculate the empirical distribution:

→

1 (s1, a) = 1, (cid:100)dπE
(cid:100)dπE

2 (s2, a) = 1.

Let us use the following notations: x := π1(a

s1) and y := π2(a

s2). In time step h = 1, we have

|

In time step h = 2, we have

=

x

|

1

|

−

+

|
dπ
1 (s, a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:100)dπE
1 (s, a)

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Loss1 = ∑
(s,a)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

dπ
1 (s1, a)

(cid:100)dπE
1 (s1, a)

dπ
1 (s1, a)

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
= 2(1

x).

−

−
1

|

x

0

|

−

−

(cid:100)dπE
1 (s1, a)

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

dπ
2 (s, a)

−

(cid:100)dπE
2 (s, a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Loss2 = ∑
(s,a)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

dπ
2 (s2, a)

(cid:100)dπE
2 (s2, a)

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

dπ
2 (s2, a)

−

(cid:100)dπE
2 (s2, a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

33

9Non-convexs1s3s4s2s5References101.01.01.01.01.01.0expert action (reward = 1)non-expert action (reward = 0)Thus, we have that

xy

=
|
= 1

−

+
1
−
|
xy + x

|
−

y)
−
xy + 1

−
−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

dπ
2 (s3, a)

−
x(1

(cid:100)dπE
2 (s3, a)

dπ
2 (s3, a)

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
+ (1
0
|
x = 2(1

x)
xy).

−
−

(cid:100)dπE
2 (s3, a)

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Furthermore, we can compute that

(cid:20)

f (x, y) = Loss1 + Loss2 = 2 (2

x

−

−

xy) .

f (x, y) =

∇

2 f (x, y) =

2

−

−
(cid:20) 0
2

2y

−
2x

2
−
0

,

(cid:21)

(cid:21)

.

∇
2 f (x, y) is not a PSD, we claim that f (x, y) is non-convex w.r.t. (x, y).

−

Since

∇

A.4 Proof of Claim in Example 1

In this part, we formally state and prove the claim in Example 1.

Claim 1. Consider the MDP and expert dataset in Example 1. Suppose that πAIL is an optimal solution of (5), then
s) = 1,
we have for each time step h

[2], πAIL

s) = πE

s1, s2

(a

s

.

∈

h

|

h (a

|

∀

∈ {

}

Proof of Claim 1. First of all, recall that there are three states (s1, s2, s3) and two actions (a, a), where a is
the expert action and a is the non-expert action. In particular, s1 and s2 are good states while s3 is a bad
absorbing state. Furthermore, H = 2 and ρ = (0.5, 0.5, 0.0). The agent is provided with 2 expert trajectories:
tr1 = (s1, a)

(s1, a) and tr2 = (s1, a)

(s2, a).

Let us compute the empirical state-action distribution of expert policy. In time step h = 1, we have

→

→

In time step h = 2, we have

1 (s1, a) = 1, (cid:100)dπE
(cid:100)dπE
1 (s2, a) = 0, (cid:100)dπE
(cid:100)dπE
(cid:100)dπE
1 (s3, a) = 0.

1 (s1, a) = 0,
1 (s2, a) = 0,

2 (s1, a) = 0.5, (cid:100)dπE
(cid:100)dπE
2 (s2, a) = 0.5, (cid:100)dπE
(cid:100)dπE
(cid:100)dπE
1 (s3, a) = 0.

1 (s1, a) = 0,
1 (s2, a) = 0,

Deﬁne the single-stage loss function in time step h as
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Lossh = ∑

dπ
h (s, a)

(cid:100)dπE
h (s, a)

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

Let us ﬁrst investigate the optimal solution π(cid:63)

(s,a)

∈S×A

π(cid:63)

2 ∈

argmin
π2

Loss2 = argmin

2 in the second time step, i.e.,
(cid:12)
(cid:12)
(cid:12)
(cid:12)

dπ
2 (s, a)

∑

−

(cid:100)dπE
2 (s, a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

π2

(s,a)

∈S×A

where dπ
have that

2 is computed by (π1, π2). Note that we will focus on the case where π1 is optimal. To start with, we

Loss2 =

(cid:12)
(cid:12)
(cid:12)
(cid:12)

dπ
2 (s1, a)

−

(cid:100)dπE
2 (s1, a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

dπ
2 (s2, a)

−

(cid:100)dπE
2 (s2, a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

34

+

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)

0.5

dπ
2 (s1, a)

dπ
2 (s2, a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
−
(cid:12)
(cid:12)dπ
2 (s1)π2(a
s1)
(cid:12)
(cid:12)
(cid:12)dπ
(cid:12)
+
(cid:12)
(cid:12)dπ
2 (s1)π2(a
(cid:12)
+ dπ
2 (s1)(1

|
2 (s1)π2(a

(cid:12)
(cid:12)
(cid:100)dπE
2 (s1, a)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)dπ
2 (s2)π2(a
(cid:12)
(cid:12)
(cid:12) +
(cid:12)
(cid:12)
(cid:12)dπ
(cid:12)
(cid:12)
(cid:12) +
0.0
(cid:12)
(cid:12)
(cid:12)dπ
2 (s2)π2(a
(cid:12)
(cid:12)
(cid:12) +
s1)) + dπ
2 (s2)(1

−
π2(a

−
0.5

−
s1)

|
s1)

|
2 (s2)π2(a

|

−
s2)

|
s2)

−

|

=

=

(cid:100)dπE
2 (s2, a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

dπ
2 (s3, a)

−

(cid:100)dπE
2 (s3, a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

0.5

−
s2)

(cid:12)
(cid:12)
(cid:12) +
0.0
(cid:12)
(cid:12) + dπ
(cid:12)

(cid:12)
(cid:12)dπ
(cid:12)
2 (s3)

−
0.5

2 (s3)π2(a

(cid:12)
(cid:12)
(cid:12)

0.0

s3)

|

−

|
−
s2)).
π2(a
|
2 (s2) and dπ

−
2 (s1), dπ

2 (s3) are independent of π2. Then, we

Note that π2 is the optimization variable, while dπ
obtain that
π(cid:63)

Loss2

2 ∈

argmin
π2

= argmin
π2

(cid:26) (cid:12)
(cid:12)dπ
(cid:12)

2 (s1)π2(a

(cid:12)
(cid:12)
(cid:12) +

0.5

s1)

|

−

(cid:12)
(cid:12)dπ
(cid:12)

2 (s2)π2(a

s2)

(cid:12)
(cid:12) + dπ
(cid:12)

2 (s3)

0.5

−

|
(cid:27)

s1)) + dπ

+ dπ

2 (s1)(1
(cid:12)
(cid:12)dπ
(cid:12)

π2(a

−
|
2 (s1)π2(a

= argmin
π2

s1)

|

−

2 (s2)(1
(cid:12)
(cid:12)
(cid:12) −

0.5

π2(a

s2))

−
|
dπ
2 (s1)π2(a

(cid:12)
(cid:12)dπ
(cid:12)

s1) +

|

Note that we now have two free optimization variables: π2(a
Based on this observation, we have

|

2 (s2)π2(a

s2)

|

−
s1) and π2(a

|

0.5

(cid:12)
(cid:12)
(cid:12) −

dπ
2 (s2)π2(a

s2).

|

s2), which are independent.

π(cid:63)
2 (a1

s1)

|

∈

argmin
s1)

π2(a1

[0,1]

2 (s1)π2(a

s1)

|

−

0.5

|

s2)

π(cid:63)
2 (a1

∈
argmin
s2)
∈
s1) and we want to argue that π2(a

2 (s2)π2(a

π2(a1

[0,1]

∈

|

|

|

s2)

0.5

−

(cid:12)
(cid:12)dπ
(cid:12)
(cid:12)
(cid:12)dπ
(cid:12)

(cid:12)
(cid:12)
(cid:12) −
(cid:12)
(cid:12)
(cid:12) −

dπ
2 (s1)π2(a

dπ
2 (s2)π2(a

s1),

s2).

|

|

s1) = 1 is the optimal solution. We can directly
We ﬁrst consider π(a
|
prove this claim for the speciﬁc conﬁguration in Example 1 but we have a more powerful lemma in
s1) = 1 is the unique globally optimal solution.
Appendix C. In particular, Lemma 10 claims that π2(a
Similarly, we also have that π(cid:63)

s2) = 1. This ﬁnishes the proof in time step h = 2.

|

|

In the following part, we check the conditions required by Lemma 10. We apply Lemma 10 with
s) > 0
0.5 = c1, where the equality holds if and only if
2 (a

2 (s1) and d1 = dπ
2 (s1) > 0. Besides, dπ
≤
s2) = 1. By Lemma 10, we have that π(cid:63)

m = n = 1, c1 = 0.5, a11 = dπ
and hence we have a11 = dπ
π(cid:63)

2 (s1). Lemma 1 implies that
2 (s1)

s1) = 1, π(cid:63)

s1) = 1.

1 (a1

s1, s2

, π(cid:63)

2 (a

2 (a

∈ {

∃

}

s

|

Then we consider the policy optimization in time step h = 1. With Lemma 8, we have that

|

|

|

2 (a

|

π(cid:63)

1 ∈

argmin
π1

Loss1 + Loss2 = argmin

π1

H
∑
h=1

∑
(s,a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

dπ
h (s, a)

−

(cid:100)dπE
h (s, a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

. Notice that we have proved that π(cid:63)

2 (a1

s1) = 1, π(cid:63)

2 (a1

|

|

s2) = 1. Then,

where dπ
we obtain that

2

2 is computed by π1 and πAIL
(cid:12)
(cid:12)
(cid:12)dπ
(cid:12)
(cid:12)
(cid:12) +
s1)P1(s1

Loss2 =

=

−

0.5

2 (s1)

(cid:12)
(cid:12)dπ
(cid:12)
(cid:12)
(cid:12)dπ
1 (s1)π1(a
(cid:12)
|
(cid:12)
(cid:12)dπ
1 (s1)π1(a
(cid:12)
+
+ dπ
1 (s1)π1(a
(cid:12)
(cid:12)
(cid:12)0.25π1(a

|

= 2

2 (s2)

0.5

−
s1, a) + dπ

2 (s3)

(cid:12)
(cid:12) + dπ
(cid:12)
1 (s2)π1(a

|
s1)P1(s2
|
s1)P1(s3
|
s1) + 0.25π1(a

|

s1, a) + dπ

|
s1, a) + dπ

|
1 (s2)π1(a
1 (s2)π1(a
(cid:12)
(cid:12)
(cid:12) + 0.5(1
0.5

|

s2)

|

−

s2)P1(s1

s2, a)

|
s2)P1(s2
|
s2)P1(s3

−
s2, a)

|
s2, a)

(cid:12)
(cid:12)
(cid:12)

0.5

(cid:12)
(cid:12)
(cid:12)

0.5

−

|
π1(a

−

s1)) + 0.5(1

|

π1(a

|

−

s2))

35

= (1.0

= 2.0

−

0.5π1(a
s1)
π1(a

|

s1)

0.5π1(a
s2),

−
π1(a

−

|

−

|

s2))

|

−

0.5π1(a

s2)

|

−

0.5π1(a

|

s2) + 1.0

s1) = 1.0 and π1(a
which has a unique globally optimal solution at π1(a
|
(cid:12)
(cid:16)
(cid:12) + ρ(s1)
s1)
(cid:12)
π1(a

ρ(s1)π1(a
|
(cid:12)
(cid:12)
(cid:12) + 0.5(1

−
s1)) + 0.5

−
0.5π1(a

Loss1 =

|
π1(a

1 (s1)

s1)

=

1

|

−

|

(cid:12)
(cid:12)dπ
(cid:12)
(cid:12)
(cid:12)
(cid:12)1
= 2

−

|
s1),

π1(a

|

−

s2) = 1.0. For Loss1, we have
s1)

+ ρ(s2)

(cid:17)

which has a globally optimal solution at π1(a
s1) = π1(a
π1(a
that π(cid:63)
1 ∈
time step h = 1.

s2) = 1. By Lemma 5, we have that
s2) = 1 is the unique globally optimal solution of the joint objective Loss1 + Loss2. Recall
s2) = 1.0. This ﬁnishes the proof in

Loss1 + Loss2. Hence it holds that π(cid:63)

|
s1) = π(cid:63)
1 (a

s1) = 1 and π1(a

argminπ1

1 (a

|

|

|

|

|

Finally, by the standard analysis of dynamic programming [Bertsekas, 2012], we know that π(cid:63) = (π(cid:63)

1 , π(cid:63)
2 )

is identical to πAIL = (πAIL

1

, πAIL
2

), where πAIL is the optimal solution to the original objective in (5).

In Example 1, the estimator in the last time step happens to equal the true distribution, i.e., (cid:100)dπE
2 (s1, a) =
dπE
2 (s1, a) and (cid:100)dπE
2 (s2, a). Therefore, we can prove that πE
H is the unique globally optimal
solution of (5). We remark that in general, we cannot prove that the optimal solution of the last time step is
πE
H.

2 (s2, a) = dπE

A.5 Proof of Theorem 4

Proof of Theorem 4. According to Equation (1), we have that

(cid:12)
(cid:12)V(πE)
(cid:12)

−

V(πAIL)

(cid:12)
(cid:12)
(cid:12) =

(cid:12)
(cid:12)
(cid:12)
(cid:12)

H
∑
h=1

(s,a)

∑

dπE
h (s, a)rh(s, a)

dπAIL
h

−

(s, a)rh(s, a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

From Proposition 1, we have that for any h
and πAIL never visit bad states and for any h
gap is upper bounded by the state-action distribution discrepancy in the last time step.

s) = πE
|
h (s, a) = dπAIL

1], πAIL
h
1], dπE

G. Therefore, πE
(s, a). As a result, the policy value

s) = 1,

−
−

∈
∈

h (a1

∈ S

(a1

∀

s

|

h

∈S×A
[H
[H

(cid:12)
(cid:12)V(πE)
(cid:12)

−

V(πAIL)

(cid:12)
(cid:12)
(cid:12) =

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∑

dπE
H (s, a)rH(s, a)

(s,a)

∈S×A

dπAIL
H (s, a)rH(s, a)

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

On the one hand, since rH(s, a)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∈
∑
(s,a)

[0, 1], we have that

dπE
H (s, a)rH(s, a)

−

dπAIL
H (s, a)rH(s, a)

(cid:12)
(cid:12)
(cid:12)
(cid:12) ≤

1.

On the other hand, by triangle inequality, we have
(cid:12)
(cid:12)
(cid:12)
(cid:12)

dπE
H (s, a)rH(s, a)

∑

dπAIL
H (s, a)rH(s, a)

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(s,a)

∈S×A
∑

≤

(s,a)

∈S×A

(cid:12)
(cid:12)
(cid:12)
(cid:12)

dπE
H (s, a)

(cid:100)dπE
H (s, a)

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

(cid:100)dπE
H (s, a)

−

dπAIL
H (s, a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

In the following part, we want to prove that the optimality of πAIL implies that ∑(s,a) |
∑(s,a) |

dπE
H (s, a)

(cid:100)dπE
H (s, a)

−

|

.

(cid:100)dπE
H (s, a)

−

dπAIL
H (s, a)

| ≤

36

By Lemma 6, it holds that

πAIL

H ∈

argmin
πH

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∑

(s,a)

∈S×A

(cid:100)dπE
H (s, a)

−

dπ
H(s, a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

where dπ
(πE

1 , . . . , πE
H

H is computed by (πAIL
1) and thus dπAIL

H (s) = dπE

, . . . , πAIL
H
−
H (s),

1

−

πAIL

H ∈

argmin
πH

This implies that

s

∀

∈ S
∑

(s,a)

∈S×A

. Then, we arrive at

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:100)dπE
H (s, a)

−

dπE
H (s, a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

1, πH). From Proposition 1, we know that (πAIL

1

, . . . , πAIL
−

H

1) =

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∑

(s,a)

∈S×A

(cid:100)dπE
H (s, a)

−

dπAIL
H (s, a)

(cid:12)
(cid:12)
(cid:12)
(cid:12) ≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∑

(s,a)

∈S×A

(cid:100)dπE
H (s, a)

−

dπE
H (s, a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

Then we obtain
(cid:12)
(cid:12)V(πE)
(cid:12)

−

V(πAIL)

(cid:12)
(cid:12)
(cid:12) ≤

min

(cid:26)

1, 2 ∑
(s,a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

dπE
H (s, a)

(cid:100)dπE
H (s, a)

−

(cid:27)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= min

(cid:26)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1, 2 ∑
s

∈S

dπE
H (s)

(cid:100)dπE
H (s)

−

(cid:27)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

(18)

Finally, we apply [Han et al., 2015, Theorem 1] to upper bound the estimation error in (18):

E

(cid:104)(cid:12)
(cid:12)V(πE)
(cid:12)

−

V(πAIL)

(cid:105)

(cid:12)
(cid:12)
(cid:12)

≤

(cid:26)

1, 2E

min

(cid:20)(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:100)dπE
H (

)

·

−

dπE
H (

(cid:21) (cid:27)

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

)

·

≤

(cid:26)

(cid:114)

min

1, 2

(cid:27)

1

.

|S| −
N

A.6 Proof of Theorem 6

To prove Theorem 6, we need to re-build the optimality condition by a stage-coupled analysis. This part is
stated in Proposition 5. Before we present Proposition 5, we mention a useful property of TV-AIL’s objective
on RBAS MDPs.

Lemma 3. Consider RBAS MDPs satisfying Assumption 1. Suppose that πAIL is an optimal solution of (5), then
(cid:100)dπE
πAIL and πE achieve the same loss, i.e., ∑H
h (cid:107)1 = ∑H

dπAIL
h −

(cid:100)dπE
h (cid:107)1.

dπE
h −

h=1 (cid:107)

h=1 (cid:107)

Refer to Appendix C.2.1 for the proof. Note that πAIL may be different with πE in the last time step on
RBAS MDPs. The following proposition demonstrates that the distance between the approximately optimal
solution and the exactly optimal solution can be properly controlled.

Proposition 5. For each tabular and episodic MDP satisfying Assumption 1, deﬁne the candidate policy set Πopt =

Π :

h

[H],

π
∈
{
that π
optimality condition almost surely:

s) > 0
∀
Πopt is an ε-optimal solution (refer to Deﬁnition 1). For any N

∈ S

G, πh(a1

∈

∈

∃

}

s

|

≥

. Given the expert state-action distribution estimation (cid:100)dπE

H , suppose
1, we have the following approximate

c(π)

(cid:32) H
∑
h=1

h
1
∑
−
(cid:96)=1

s

∈S

dπ
(cid:96) (s)

(cid:16)

1

∑

G

π(cid:96)(a1

s)

|

−

where c(π) > 0 is deﬁned as

(cid:17)

+ ∑
s
∈Sπ

(cid:18)

dπ
H(s)

min

{

1, (cid:100)dπE

H (s)/dπE

H (s)

(cid:19)(cid:33)

πH(a1

s)

|

} −

ε,

(19)

≤

c(π) :=

min

(cid:110)

Pπ (cid:16)

sh = s

s(cid:96) = s(cid:48), a(cid:96) = a1(cid:17)(cid:111)

.

Here Pπ (cid:0)sh = s
which is jointly determined by the transition function and policy π. In addition,

|
s(cid:96) = s(cid:48), a(cid:96) = a1(cid:1) is the visitation probability of s in time step by starting from s(cid:48), a(cid:48) in time step (cid:96),

H,s,s(cid:48)∈S

(cid:96)<h

≤

≤

|

1

G

(cid:26)

s

Sπ =

G : πH(a1

s)

|

≤

min

{

∈ S

37

1, (cid:100)dπE

H (s)/dπE

H (s)

(cid:27)

.

}

Proof of Proposition 5 is rather technical and is deferred to Appendix C.2.2. We explain Proposition 5
by connecting it with Proposition 1. In particular, if ε = 0, we can show that the optimality condition in
G, since c(π) > 0
Proposition 5 reduces to that in Proposition 1. To see this, for each h
and dπ
1], while there may exist many optimal solutions
in the last step policy optimization (corresponding to the second term in (19)).

h (s) > 0, we must have πh(a1

s) = 1 for all h

1] and s

∈ S

[H

[H

−

−

∈

∈

|

Equipped with Proposition 5, we can obtain the horizon-free sample complexity for the approximately

optimal solution of TV-AIL in Theorem 6.

Proof of Theorem 6. Given the estimation (cid:100)dπE , we consider TV-AIL’s objective.

min
π

H
∑
h=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∑

(s,a)

∈S×A

dπ
h (s, a)

−

(cid:100)dπE
h (s, a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

Suppose π is an ε-optimal solution (refer to Deﬁnition 1). First, we construct an optimal solution πAIL based
on Proposition 1:

• By Proposition 1, we have

h

∀

∈

[H

−

1], s

∈ S

G, πAIL
h

(a1

|

s) = πE

h (a1

s).

• For the last time step H, we deﬁned a set of states

|
H (s) < dπE
G : (cid:100)dπE

H (s)
}
\ UH, πAIL

. The policy in the
H (a1

s) = 1. In

|

G

s

∀

∈ S

s

UH :=
{
H (s)/dπE
.

∈ S
H (s) and

last time step is deﬁned as
H (a1

G, πAIL

a word,

s

∀
s) = min

H (a1
∈ UH, πAIL
|
H (s)/dπE
(cid:100)dπE
{

s) = (cid:100)dπE

s

H (s), 1
• For simplicity of analysis, we also deﬁne the policy on bad states, although πAIL never visit bad states.

∈ S

∀

}

|

h

∀

∈

[H], s

B, πAIL
h

(

·|

∈ S

s) = πh(

s).

·|

Next, we verify that such deﬁned πAIL is an optimal solution of (5). According to Proposition 1, it sufﬁces

to show that πH achieves the optimality. According to Lemma 6, we need to prove that

πAIL

H ∈

min
πH

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∑
(s,a)

dπ
H(s, a)

(cid:100)dπE
H (s, a)

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

where dπ
Hence, we omit details.

H is computed by (πAIL

1

, . . . , πAIL
−

H

1). In fact, the argument here is the same with that in Proposition 3.

Now we consider the imitation gap of π.

By (18) in the proof of Theorem 4, we have that

V(πE)

V(π) = V(πE)

−

V(πAIL) + V(πAIL)

V(π).

−

−

(20)

2 ∑
s
Then we consider the policy value gap between πAIL and π. With the dual form of policy value in (1), we
get that

V(πAIL)

dπE
H (s)

(cid:100)dπE
H (s)

V(πE)

(21)

−

≤

−

∈S

.

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

V(πAIL)

V(π) =

−

≤

H
1
∑
−
h=1

H
1
∑
−
h=1

(cid:16)

∑
(s,a)

dπAIL
h

(s, a)

−

(cid:17)

dπ
h (s, a)

rh(s, a) + ∑
(s,a)

(cid:16)

dπAIL
H (s, a)

−

(cid:17)

dπ
H(s, a)

rH(s, a)

(cid:13)
(cid:13)dπAIL
(cid:13)

h

)

(

,

·

·

−

dπ
h (

)

,

·

·

(cid:16)

(cid:13)
(cid:13)
(cid:13)1

+ ∑
(s,a)

dπAIL
H (s, a)

−

(cid:17)

dπ
H(s, a)

rH(s, a),

(22)

where we use dπ
h (
,
·
G : πH(a1

Sπ =
of
(22), we have

∈ S

{

s

·

) to explicitly denote the state-action distribution induced by π. Recall the deﬁnition

s)

|

≤

min

{

1, (cid:100)dπE

H (s)/dπE

H (s)

introduced in Proposition 5. For the second term in

}}

(cid:16)

∑
(s,a)

dπAIL
H (s, a)

−

(cid:17)

dπ
H(s, a)

rH(s, a)

38

dπAIL
H (s, a1)

(cid:17)

dπ
H(s, a1)

−

(cid:16)

G

(cid:16)

= ∑
s
∈S
= ∑
s
∈S
= ∑
s
∈S
(cid:13)
(cid:13)dπAIL
(cid:13)
H (

(cid:16)

G

G

≤

dπAIL
H (s)πAIL

H (a1

s)

|

−

dπ
H(s)πH(a1

s)

dπAIL
H (s)

(cid:17)

dπ
H(s)

−

πAIL

H (a1

|
s) + ∑
s

G

∈S

(cid:17)

(cid:16)

dπ
H(s)

πAIL

H (a1

s)

|

−

πH(a1

(cid:17)

s)

|

dπ
H(

)

·

−

(cid:13)
(cid:13)dπAIL
(cid:13)
H (

=

dπ
H(

)

·

−

|
∑
H (a1
(cid:16)

G,πAIL

dπ
H(s)

(cid:13)
(cid:13)
(cid:13)1

(cid:13)
(cid:13)
(cid:13)1

)

·

)

·

+

s:s
∈S
+ ∑
s
∈Sπ

(cid:16)

dπ
H(s)

πAIL

H (a1

s)

|

−

πH(a1

(cid:17)

s)

|

πH(a1

−

(cid:17)

,

s)

|

s)

πH (a1
≥
|
H (a1
πAIL

s)

|
s)

|

where we use dπ
h (
into the policy value gap yields

·

) to explicitly denote the state distribution induced by π. Plugging the above inequality

V(πAIL)

V(π)

−

≤

(cid:13)
(cid:13)dπAIL
(cid:13)
H (

H
1
∑
−
h=1
(cid:13)
(cid:13)dπAIL
(cid:13)
H (

+

)

,

·

·

−

dπ
H(

)

·

−

dπ
H(

(cid:13)
(cid:13)
(cid:13)1

)

·

,

)

(cid:13)
(cid:13)
(cid:13)1
·
·
+ ∑
s
∈Sπ

(cid:16)

dπ
H(s)

πAIL

H (a1

s)

|

−

πH(a1

(cid:17)

.

s)

|

,

·

·

·

)

)

≤

−

−

(cid:13)
(cid:13)dπAIL
(cid:13)
H (

(cid:13)
(cid:13)dπAIL
(cid:13)
H (

With Lemma 4, we have that
H
1
∑
−
h=1
H
1
∑
−
h=1
H
∑
h=1
H
∑
h=1
H
∑
h=1

h
1
∑
−
(cid:96)=1
h
1
∑
−
(cid:96)=1

(cid:13)
(cid:13)dπAIL
(cid:13)
H (

dπ
(cid:96) (

dπ
(cid:96) (

=

=

−

≤

E

E

2

∼

∼

)

·

)

)

s

s

·

·

dπ
H(

)

,

·

·

(cid:13)
(cid:13)
(cid:13)1

(cid:13)
(cid:13)dπAIL
(cid:13)
H (

+

dπ
H(

)

·

−

(cid:13)
(cid:13)
(cid:13)1

)

·

dπ
H(

(cid:13)
(cid:13)
(cid:13)1

)

·

+

dπ
H(

(cid:13)
(cid:13)
(cid:13)1

)

·

+

H
1
∑
−
h=1
H
1
∑
−
h=1

E

s

dπ
H (

)

·

∼

(cid:104)(cid:13)
(cid:13)πAIL
(cid:13)

h

(

s)

·|

−

πh(

s)

·|

(cid:105)

(cid:13)
(cid:13)
(cid:13)1

(cid:13)
(cid:13)dπAIL
(cid:13)
H (

+

dπ
H(

)

·

−

(cid:13)
(cid:13)
(cid:13)1

)

·

E

(cid:104)(cid:13)
(cid:13)πAIL
(cid:13)

h

(

s)

·|

−

πh(

s)

·|

(cid:105)

(cid:13)
(cid:13)
(cid:13)1

s

dπ
H (

)

·

∼

(cid:104)(cid:13)
(cid:13)πAIL
(cid:13)

(cid:96)

s)

(

·|

−

π(cid:96)(

s)

·|

(cid:105)

(cid:13)
(cid:13)
(cid:13)1

+

E

H
1
∑
−
h=1

s

dπ
H (

)

·

∼

(cid:104)(cid:13)
(cid:13)πAIL
(cid:13)

h

(

s)

πh(

s)

·|

−

·|

(cid:105)

(cid:13)
(cid:13)
(cid:13)1

(cid:104)(cid:13)
(cid:13)πAIL
(cid:13)

(cid:96)

(

s)

·|

−

π(cid:96)(

s)

·|

(cid:105)

(cid:13)
(cid:13)
(cid:13)1

.

Then we have that

V(πAIL)

V(π)

2

≤

−

H
∑
h=1

h
1
∑
−
(cid:96)=1

E

s

dπ
(cid:96) (

)

·

∼

(cid:104)(cid:13)
(cid:13)πAIL
(cid:13)

(cid:96)

(

s)

·|

−

π(cid:96)(

s)

·|

(cid:105)

(cid:13)
(cid:13)
(cid:13)1

+ ∑
s
∈Sπ

(cid:16)

dπ
H(s)

πAIL

H (a1

s)

|

−

πH(a1

(cid:17)

.

s)

|

(23)

Notice that πAIL agrees with π on bad states and we obtain
(cid:13)
(cid:13)πAIL
(cid:13)


(cid:104)(cid:13)
(cid:13)πAIL
(cid:13)

= ∑
s

dπ
(cid:96) (s)

(cid:13)
(cid:13)
(cid:13)1

π(cid:96)(

dπ
(cid:96) (

s)

s)

−

E

·|

·|

∼

(cid:105)

(

G

(cid:96)

(cid:96)

)

s

·

s)

(

·|

−

π(cid:96)(

s)

·|

(cid:13)
(cid:13)
(cid:13)1

dπ
(cid:96) (s)



(cid:12)
(cid:12)πAIL
(cid:12)

(cid:96)

(a1

s)

|

−

π(cid:96)(a1

s)

|

(cid:12)
(cid:12) + ∑
(cid:12)

a

∈A\{



π(cid:96)(a

s)



|

a1

}

∈S
= ∑
s
∈S
= 2 ∑
s

G

∈S

G

dπ
(cid:96) (s)

(cid:16)

1

π(cid:96)(a1

−

(cid:17)

.

s)

|

39

In the penultimate inequality, we use the fact that

(cid:96)

[H

s) = 1. Then we have that

V(πAIL)

V(π)

4

≤

−

H
∑
h=1

h
1
∑
−
(cid:96)=1

s

dπ
(cid:96) (s)

(cid:16)

1

∑

G

−

∀
∈
π(cid:96)(a1

s)

|

1], s

−
(cid:17)
+ ∑
s
∈Sπ

G, πAIL
(cid:96)

(a1

∈ S

(cid:16)

dπ
H(s)

|
H (a1

πAIL

πH(a1

s)

|

−

(cid:17)

.

s)

|

(24)

Then we consider the second term in (24). For the last time step H, notice that by construction, we have
πAIL

. Then we get

s) = min

H (s)/dπE
(cid:100)dπE

H (s), 1

H (a1

|

{

∈S

}

∑
∈Sπ
Plugging (25) to (23) yields

dπ
H(s)

s

(cid:16)

πAIL

H (a1

πH(a1

s)

|

−

(cid:17)

s)

|

≤

∑
∈Sπ

s



dπ
H(s)

min






(cid:100)dπE
H (s)
dπE
H (s)

, 1



 −



πH(a1

s)

 .

|

(25)

V(πAIL)

V(π)

4

≤

−

H
∑
h=1

h
1
∑
−
(cid:96)=1

∑

G

s

∈S

dπ
(cid:96) (s)

(cid:16)

1

π(cid:96)(a1

s)

|

−

(cid:17)

+ ∑
s
∈Sπ

Subsequently, we can apply Proposition 5 and get that



dπ
H(s)

min






(cid:100)dπE
H (s)
dπE
H (s)

, 1



 −



πH(a1

s)

 .

|

V(πAIL)

V(π)

−



4



H
∑
h=1

h
1
∑
−
(cid:96)=1

4
c(π)

ε .

≤

≤

∑

G

s

∈S

dπ
(cid:96) (s)

(cid:16)

1

π(cid:96)(a1

s)

|

−

(cid:17)

+ ∑
s
∈Sπ



dπ
H(s)

min






(cid:100)dπE
H (s)
dπE
H (s)

, 1



 −





πH(a1

s)





|

We proceed to upper bound the imitation gap. With (20) and (21), we have

V(πE)

= V(πE)
(cid:40)

−

−

E [V(π)]
(cid:104)
E

V(πAIL)
(cid:34)

min

1, 2E

≤

(cid:26)

= min

1, 2E

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∑
s
∈S
(cid:20)(cid:13)
(cid:13)
(cid:100)dπE
H (
(cid:13)
(cid:13)

(cid:105)

(cid:104)

+ E

V(πAIL)
(cid:35)(cid:41)

dπE
H (s)

(cid:100)dπE
H (s)

−

(cid:105)

V(π)

4
c(π)

ε

−

+

)

dπE
H (

)

+

4
c(π)

ε,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:21)(cid:27)

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

−
where the expectation is taken over the randomness in collecting N expert trajectories. Finally, we apply
[Han et al., 2015, Theorem 1] to upper bound the estimation error in the ﬁrst term:
(cid:41)

(cid:40)

·

·

(cid:114)

4
c(π)

ε, 2

|S| −
N

1

+

4
c(π)

ε

.

V(πE)

−

E [V(π)]

≤

min

1 +

B Proof of Results in Section 5

B.1 Proof of Proposition 3

Proof of Proposition 3. As we have analyzed, for any tabular and episodic MDP satisfying Assumption 2, we
have that dπ

, h

s

h (s) = ρ(s),

∀
H
∑
h=1

∈ S
∑

(s,a)

argmin

Π

π

∈

= argmin
π

Π

∈

H
∑
h=1

∈S×A
∑

(s,a)

∈S×A

∈

[H]. Then we obtain
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:100)dπE
h (s, a)

dπ
h (s, a)

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

ρ(s)πh(a

s)

|

−

(cid:100)dπE
h (s, a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

40

= argmin
π

Π

∈

= argmin
π

Π

∈

= argmin
π

Π

∈

H
∑
h=1

H
∑
h=1
H
∑
h=1

∑
s

∈S
∑
s

∈S
∑
s

∈S





(cid:12)
(cid:12)
(cid:12)
(cid:12)

ρ(s)πh(a1

s)

|

−

(cid:100)dπE
h (s, a1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ ∑
a

(cid:12)
(cid:12)
(cid:12)
(cid:12)

a1

}

ρ(s)π(a

s)

|

−

(cid:100)dπE
h (s, a)





(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)

ρ(s)πh(a1

ρ(s)πh(a1

|

|

s)

−

(cid:100)dπE
h (s)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ ρ(s)

∈A\{
(cid:16)

1

−

s)

−

(cid:100)dπE
h (s)

(cid:12)
(cid:12)
(cid:12)
(cid:12) −

ρ(s)πh(a1

πh(a1

s)

|

(cid:19)

.

s)

|

(cid:17)(cid:19)

∈
πAIL

[H] and s
(cid:12)
(cid:12)
(cid:12)
(cid:12)

min
πh

h ∈

∈ S
ρ(s)πh(a1

We see that the above multi-stage policy optimization reduces to H independent state-action distribution
matching problems: for each h
, we solve

For this optimization problem, we introduce the notation

πAIL
Lemma 8, we have that the optimal solution set is
h
c
hand, for any state s
h (i.e., the complement set of

{

∈ W

πAIL
h

(a1

|

s) = argmin
πh(a1

s)

[0,1]

|

∈

s)

|

−

(cid:100)dπE
h

(cid:12)
(cid:12)
(cid:12)
(cid:12) −

ρ(s)πh(a1

s).

|

(26)

: (cid:100)dπE
[(cid:100)dπE

s
{
(a1

Wh =
s) : πAIL

h (s) < ρ(s)
∈ S
h (s)/ρ(s), 1]
(
s)
|
Wh), the problem in (26) reduces to
(cid:100)dπE
h (s)

2ρ(s)πh(a1

s).

∈

·|

h

}

−

|

. Then, with

}
. On the other

In this case, it is easy to see that the unique optimal solution is πh(a1
point in Proposition 3 is ﬁnished.

|

s) = 1. Hence, the proof of the ﬁrst

Next, we continue to prove the second and third points in Proposition 3. To start with, we note that

V(πE)

−

V(πAIL) =

=

=

H
∑
h=1

H
∑
h=1
H
∑
h=1

(s,a)

∑
s

∈S
∑
s

∈S

∑

(cid:16)

dπE
h (s, a)

−

dπAIL
h

(s, a)

(cid:17)

rh(s, a)

∈S×A
dπE
h (s, a1)

dπAIL
h

(s, a1)

−

dπE
h (s)

−

ρ(s)πAIL

h

(a1

s).

|

(27)

According to the ﬁrst point in Proposition 3, we further ﬁnd that among all optimal solutions, the largest
s) = (cid:100)dπE
c
imitation gap is obtained at πAIL
h.
Accordingly, the largest imitation gap is

h with πAIL

h and πAIL
1

h (s)/ρ(s),

s) = 1,

∈ W

∈ W

(a1

(a1

∀

∀

s

s

|

|

h

h

V(πE)

−

V(πAIL) =

H
∑
h=1

∑
∈Wh

s

dπE
h (s)

−

ρ(s)πAIL

h

(a1

s) =

|

H
∑
h=1

∑
∈Wh

s

dπE
h (s)

(cid:100)dπE
h (s).

−

(28)

(cid:100)dπE
h (s) with the estimation error. Notice that for each time

In the sequel, we connect the term ∑s
−
∈Wh
(cid:100)dπE
h (s) = 1. Then we have that
step h
dπE
(cid:100)dπE
h (s)
h (s)

h (s) = ∑
(cid:100)dπE

dπE
h (s) = ∑s

[H], ∑s

∈S

∈

dπE
h (s)

∈S
∑
∈Wh

s

dπE
h (s).

−

s

∈W

c
h

Furthermore, we obtain
(cid:13)
(cid:13)
(cid:13)
(cid:13)

H
∑
h=1

(cid:100)dπE
h −

dπE
h

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

=

=

H
∑
h=1

H
∑
h=1

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∑
s
∈S

 ∑
∈Wh

s

dπE
h (s)

(cid:100)dπE
h (s)

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

h (s) + ∑
(cid:100)dπE

s

∈W



(cid:100)dπE
h (s)

−

dπE
h (s)



c
h

dπE
h (s)

−

41

= 2

H
∑
h=1

∑
∈Wh

s

dπE
h (s)

(cid:100)dπE
h (s),

−

where the penultimate equality follows the deﬁnition
to (28), we get that

Wh =

s

{

∈ S

: (cid:100)dπE

h (s) < ρ(s) = dπE

h (s)

(29)

. Finally, back

}

V(πE)

−

V(πAIL) =

H
∑
h=1

∑
∈Wh

s

dπE
h (s)

−

(cid:100)dπE
h (s) =

1
2

(cid:13)
(cid:13)
(cid:13)
(cid:13)

H
∑
h=1

(cid:100)dπE
h −

dπE
h

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

.

Taking the expectation over the randomness in collecting the dataset ﬁnishes the proof.

B.2 Proof of Theorem 7
Proof of Theorem 7. First, we consider the small sample regime where N (cid:45)
. To prove this lower bound,
we draw a connection between the (cid:96)1-norm-based estimation error and the missing mass [Good, 1953,
McAllester and Ortiz, 2003, Rajaraman et al., 2020]. Speciﬁcally, we construct a multinomial distribution Q(cid:48)
as follows.

|X |

Q(cid:48) = (cid:0)Q(cid:48)(1), . . . , Q(cid:48)(
1
+ 1

, . . . ,

=

(cid:18)

|X |

1), Q(cid:48)(

|X | −
1
+ 1

|X |

, 1

−

)(cid:1)

|X |
|X | −
|X |

1
+ 1

(cid:19)

.

We consider the regime N (cid:45)
estimator (cid:98)Q:

|X |

. Hence, there exists a constant c > 0 such that N

c

|X |

≤

. Recall the

where N(i) = ∑j
Then we have that

I(Xj = i) denotes the number that the symbol i is observed in N i.i.d. samples (X1, . . . , XN).

(cid:98)Q(i) =

N(i)
N

,

(cid:13)
(cid:13)
(cid:13)Q(cid:48)

(cid:13)
(cid:13)
(cid:13)1

(cid:98)Q

=

−

≥

=

(cid:12)
(cid:12)
(cid:12)Q(cid:48)(i)

(cid:12)
(cid:12)
(cid:12)Q(cid:48)(i)

−

−

(cid:98)Q(i)

(cid:98)Q(i)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

I

{

N(i) = 0

}

Q(cid:48)(i)I

{

N(i) = 0

.

}

|X |∑
i=1

|X |∑
i=1

|X |∑
i=1

We note that the term in RHS is called missing mass in the statistical literature, which is deﬁned as the
probability mass of symbols unobserved in the dataset (X1, . . . , XN) [Good, 1953, McAllester and Ortiz,
2003]. Then we have that

E

(cid:104)(cid:13)
(cid:13)
(cid:13)Q(cid:48)

(cid:105)

(cid:13)
(cid:13)
(cid:13)1

(cid:98)Q

−

(cid:34)

E

|X |∑
i=1

Q(cid:48)(i)I

{

N(i) = 0

(cid:35)

}

|X |∑
i=1

|X |∑
i=1

|X |∑
i=1

Q(cid:48)(i)P (N(i) = 0)

Q(cid:48)(i) (cid:0)1

Q(cid:48)(i) (cid:0)1

−

−

Q(cid:48)(i)(cid:1)N

Q(cid:48)(i)(cid:1)c

|X | .

≥

=

=

≥

42

The last inequality follows 0 < 1

Q(cid:48)(i)

−

≤
(cid:105)

E

(cid:104)(cid:13)
(cid:13)
(cid:13)Q(cid:48)

(cid:98)Q

−

(cid:13)
(cid:13)
(cid:13)1

1 and N

c

. Next, we get that

|X |

≤
Q(cid:48)(i) (cid:0)1

Q(cid:48)(i)(cid:1)c

|X |

−

|X |∑
i=1

≥

1

1

≥

=

|X |−
∑
i=1

|X |−
∑
i=1

Q(cid:48)(i) (cid:0)1

Q(cid:48)(i)(cid:1)c

|X |

−

(cid:18)

1
+ 1

1

−

(cid:19)c

|X |

1
+ 1

(cid:19)c

|X |

|X |

1
+ 1
(cid:19)c

|X |

|X |
(cid:18)
1
+ 1

(cid:18)

= |X | −
|X |

1
+ 1

= |X | −
|X |

(a)

1
+ 1 ·

1

−

|X |

|X |

+ 1

|X |
c

e−

|X | −
|X |
1
3ec .
In the inequality (a), we use the fact that (1 + 1/
Then we get that

≥
(b)

≥

|X |

)c

|X | ≤

ec and the inequality (b) follows that

2.

|X | ≥

E

(cid:104)(cid:13)
(cid:13)
(cid:13)Q

(cid:105)

(cid:13)
(cid:13)
(cid:13)1

(cid:98)Q

−

≥

E

(cid:104)(cid:13)
(cid:13)
(cid:13)Q(cid:48)

−

(cid:105)

(cid:13)
(cid:13)
(cid:13)1

(cid:98)Q

≥

1
3ec

(cid:37) 1,

max
Q
∈Q

which completes the proof in the small sample regime. The lower bound in the large sample regime can be
obtained directly from [Kamath et al., 2015, Lemma 8].

B.3 Proof of Proposition 4

Proof of Proposition 4. According to (27) in the proof of Proposition 3, we have that

∑
∈Wh
Note that the optimal solution is not unique on the lower bound instances. Taking expectation with respect
to the uniform selection of πAIL yields that

V(πAIL) =

ρ(s)πAIL

dπE
h (s)

V(πE)

(a1

s).

−

−

|

h

s

H
∑
h=1

V(πE)

E

−

πAIL

∼

Unif(ΠAIL)

(cid:104)

V(πAIL)

(cid:105)

= E

πAIL

∼

Unif(ΠAIL)

dπE
h (s)

−

ρ(s)πAIL

h

(a1

(cid:35)

s)

|

(cid:105)(cid:21)

(cid:104)

πAIL
h

(a1

s)

|

πAIL
h

(a1

s)

|

∼

Unif([ (cid:100)dπE
h (s)/ρ(s),1])


(cid:100)dπE
h (s)/ρ(s) + 1
2



ρ(s)

−

(cid:34) H
∑
h=1

s

∑
∈Wh
ρ(s)E

=

=

H
∑
h=1

∑
∈Wh

s

H
∑
h=1

s

(cid:20)

dπE
h (s)

−


dπE

h (s)

∑
∈Wh
∑
∈Wh

s

=

1
2

H
∑
h=1

dπE
h (s)

(cid:100)dπE
h (s),

−

43

where in the last equation we use the fact that dπE

V(πE)

E

−

πAIL

∼

Unif(ΠAIL)

(cid:104)

V(πAIL)

(cid:105)

=

h (s) = ρ(s). Combing with (29), we have that
1
2

(cid:100)dπE
h (s) =

dπE
h (s)

(cid:100)dπE
h −

dπE
h

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
4

−

H
∑
h=1

H
∑
h=1

∑
∈Wh

s

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

.

We further take the expectation over the randomness in collecting expert trajectories on both sides.

(cid:104)

E

E

V(πE)

−

πAIL

∼

Unif(ΠAIL)

(cid:104)

V(πAIL)

(cid:105)(cid:105)

=

1
4

H
∑
h=1

E

(cid:20)(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:100)dπE
h −

dπE
h

(cid:21)

.

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

Finally, we apply the lower bounds of (cid:96)1-risk of the empirical distribution (cid:100)dπE (refer to ?? and Theorem 7) to
obtain the desired result.

C Technical Lemmas

C.1 Basic Technical Lemmas

Lemma 4. For any tabular and episodic MDP, considering two policies π and π(cid:48), let dπ
h (
state distribution and state-action distribution induced by π in time step h, respectively. Then we have that

) and dπ
h (

·

·

·

,

) denote the

•

•

dπ
h (

)

·

(cid:107)

−

dπ(cid:48)
h (

)

·

(cid:107)1 ≤

∑h

1
−
(cid:96)=1

E

s

dπ
h (

)

,

·

·

(cid:107)

−

dπ(cid:48)
h (

)

,

·

·

(cid:107)1 ≤ (cid:107)

dπ(cid:48)(cid:96) (

∼
dπ
h (

)

·

−

)[(cid:13)
(cid:13)π(cid:96)(

·
dπ(cid:48)
h (

)

·

s)

·|
−
(cid:107)1 + E

s)(cid:13)

(cid:13)1] when h

π(cid:48)(cid:96)(

·|

2.

≥

dπ(cid:48)h (

)

s

∼

·
dπ
1 (

(cid:2)(cid:13)
(cid:13)πh(

s)

·|

s)(cid:13)
(cid:13)1

(cid:3).

π(cid:48)h(

−
·|
(cid:107)1 =

)

·

Proof. For the ﬁrst statement, it is direct to obtain that
for any (cid:96) where 1 < (cid:96)

dπ(cid:48)
1 (
h, we will prove the following recursion:

−

(cid:107)

)

·

ρ(

)

·

−

ρ(

)

·

(cid:107)

(cid:13)
(cid:13)dπ
(cid:13)
(cid:96) (

)

≤
dπ(cid:48)
(cid:96) (

)

(cid:13)
(cid:13)
(cid:13)1 ≤

(cid:13)
(cid:13)dπ
(cid:13)

(cid:96)

1(

)

−
−
With Bellman-ﬂow equation in (7), we have

−

·

·

·

dπ(cid:48)
(cid:96)
−

1(

)

·

(cid:13)
(cid:13)
(cid:13)1

+ E

s

∼

dπ(cid:48)(cid:96)
−

1(

)

·

(cid:2)(cid:13)
(cid:13)π(cid:96)

s)

1(

·|

−

−

π(cid:48)(cid:96)

−

1(

·|

2,

≥

(cid:107)1 = 0. When h
s)(cid:13)
(cid:13)1

(cid:3) .

1(s(cid:48))π(cid:96)

1(a(cid:48)

s(cid:48))P(cid:96)

1(s

|

−

|

−

s(cid:48), a(cid:48))

−

∑
(s(cid:48),a(cid:48))

dπ(cid:48)
(cid:96)
−

1(s(cid:48))π(cid:48)(cid:96)

1(a(cid:48)

|

−

s(cid:48))P(cid:96)

1(s

|

−

s(cid:48), a(cid:48))

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

dπ
(cid:96)
−

)

(cid:13)
(cid:13)
(cid:13)1
·
dπ(cid:48)
(cid:96) (s)

(cid:12)
(cid:12)
(cid:12)

dπ(cid:48)
(cid:96) (

−

)

(cid:13)
(cid:13)dπ
(cid:13)
(cid:96) (
−
·
(cid:12)
= ∑
(cid:12)dπ
(cid:12)
(cid:96) (s)
s
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∈S
= ∑
s

∈S
= ∑
s

∑
(s(cid:48),a(cid:48))

∑
(s(cid:48),a(cid:48))

dπ(cid:48)
(cid:96)
−
(cid:12)
(cid:12)dπ
(cid:12)

∈S
+ ∑
(s(cid:48),a(cid:48))
∑
∑
s
(s(cid:48),a(cid:48))
∈S
∑
+ ∑
s
(s(cid:48),a(cid:48))

(cid:96)

∈S

≤

(cid:16)

dπ
(cid:96)
−

1(s(cid:48))

dπ(cid:48)
(cid:96)
−

−

1(s(cid:48))

(cid:17)

1(a(cid:48)

π(cid:96)

−

|

s(cid:48))P(cid:96)

1(s

|

−

s(cid:48), a(cid:48))

1(s(cid:48)) (cid:0)π(cid:96)

1(a(cid:48)

s(cid:48))

|

−

s(cid:48))(cid:1) P(cid:96)

1(a(cid:48)

|

−

1(s

|

−

s(cid:48), a(cid:48))

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1(s(cid:48))

−
1(s(cid:48)) (cid:12)

−

dπ(cid:48)
(cid:96)
−

dπ(cid:48)
(cid:96)
−

1(s(cid:48))

1(a(cid:48)

s(cid:48))P(cid:96)

1(s

|

−

|

−

s(cid:48), a(cid:48))

(cid:12)π(cid:96)

1(a(cid:48)

s(cid:48))

|

−

π(cid:48)(cid:96)

−

1(a(cid:48)

|

−

s(cid:48))(cid:12)

(cid:12) P(cid:96)

1(s

|

−

s(cid:48), a(cid:48))

π(cid:48)(cid:96)

−
(cid:12)
(cid:12)
(cid:12) π(cid:96)

(cid:13)
(cid:13)dπ
(cid:13)

(cid:96)

=

dπ(cid:48)
(cid:96)
−
Applying the recursion with

1(

−

−

)

·

1(

)

·
dπ
1 (

(cid:13)
(cid:13)
(cid:13)1
)

·

(cid:107)

+ E

(cid:2)(cid:13)
(cid:13)π(cid:96)

1(

s)

π(cid:48)(cid:96)

1(

s)(cid:13)
(cid:13)1

(cid:3) ,

s

∼
dπ(cid:48)
1 (

·

)

−

dπ(cid:48)(cid:96)
1(
·|
−
(cid:107)1 = 0 ﬁnishes the proof of the ﬁrst statement.
)

−

·|

−

·

−

44

,

,

)

)

(cid:12)
(cid:12)
(cid:12)

−

dπ(cid:48)
h (

(cid:13)
(cid:13)
(cid:13)1
·
·
dπ(cid:48)
h (s, a)

Next, we continue to prove the second statement.
(cid:13)
(cid:13)dπ
(cid:13)
h (
·
·
−
(cid:12)
= ∑
(cid:12)dπ
(cid:12)
h (s, a)
(s,a)
(cid:12)
= ∑
(cid:12)dπ
(cid:12)
(s,a)
(cid:12)
= ∑
(cid:12)
(cid:12)
(s,a)
∑
(s,a)
(cid:13)
(cid:13)dπ
(cid:13)
h (

(cid:12)
(cid:12)
(cid:12) πh(a

h (s)πh(a

dπ(cid:48)
h (s)

dπ(cid:48)
h (s)

dπ
h (s)

(cid:12)
(cid:12)dπ
(cid:12)

dπ(cid:48)
h (

πh(a

h (s)

+ E

s)

−

≤

−

=

−

(cid:16)

(cid:17)

)

)

|

|

|

s

(cid:13)
(cid:13)
(cid:13)1

·

·

−

dπ(cid:48)
h (s)π(cid:48)h(a

(cid:12)
(cid:12)
(cid:12)

s)

|

s) + dπ(cid:48)

h (s) (cid:0)πh(a

s)

|

−

π(cid:48)h(a

s)(cid:1)(cid:12)
(cid:12)
(cid:12)

|

s) + ∑
(s,a)
(cid:2)(cid:13)
(cid:13)πh(

dπ(cid:48)h (

)

·

∼

h (s) (cid:12)
dπ(cid:48)

(cid:12)πh(a

s)

π(cid:48)h(a

s)(cid:12)
(cid:12)

|

|
−
s)(cid:13)
(cid:13)1

(cid:3) ,

s)

·|

−

π(cid:48)h(

·|

which proves the second statement.

Lemma 5. Consider the optimization problem: minx
Suppose that 1) there exists k
[m], j

= k, x(cid:63) is the optimal solution to minx

[m] such that x(cid:63) is the unique optimal solution to minx

→
[0,1]n fk(x); 2) for each j
[0,1]n fj(x). Then, x(cid:63) is the unique optimal solution to minx

i=1 fi(x), where fi : [0, 1]n

[0,1]n f (x) := ∑m

∈

∈

∀

∈

∈

i

R,

[m].

∈

Proof. Since x(cid:63) is the unique optimal solution to minx
fk(x). Furthermore, for each j
that

[m], j

∈

∈

= k, recall that x(cid:63) is the optimal solution to minx

[0,1]n fk(x), we have that

[0, 1]n, x

x

∀

∈

∈
[0,1]n f (x).
= x(cid:63), fk(x(cid:63)) <
[0,1]n fj(x). We have

∈

∈

Then we derive that

x

∀

∈

j
∀
∈
[0, 1]n, x

= k,

= x(cid:63), fj(x(cid:63))
[m], j
= x(cid:63), f (x(cid:63)) < f (x) and x is the unique optimal solution to minx

[0, 1]n, x

fj(x).

≤

∈

∀

x

[0,1]n f (x).

∈

Lemma 6. Consider the optimization problem minx1,
optimal solution, then

···
i is the optimal solution to minxi F(xi) := f (x(cid:63)
1,
Proof. The proof is based on contradiction. Suppose that the original statement is not true. There exists
(cid:101)xi (cid:54)

, xn). Suppose that x(cid:63) = (x(cid:63)
1,
n).

i such that

n) is the

,xn f (x1,

[n], x(cid:63)

= x(cid:63)

, x(cid:63)

, x(cid:63)

, xi,

· · ·

· · ·

· · ·

· · ·

∈

∀

i

F((cid:101)xi) < F(x(cid:63)
i ).

Consider (cid:101)x = (x(cid:63)
1,

, x(cid:63)
which contradicts the fact that x(cid:63) = (x(cid:63)
1,
the original statement is true.

· · ·

· · ·

· · ·

, (cid:101)xi,
· · ·
f (x(cid:63)
1,

, x(cid:63)

n) which differs from x(cid:63) in the i-th component. Then we have that
, (cid:101)xi,

n) = F((cid:101)xi) < F(x(cid:63)

i ) = f (x(cid:63)
1,
n) is the optimal solution to minx1,

n),
,xn f (x1,

, x(cid:63)
i ,

, x(cid:63)

, x(cid:63)

· · ·

· · ·

· · ·

···

, xn). Hence,

· · ·

Lemma 7. For any constants a, c
problem minx

≥
[0,1] f (x), then x(cid:63) = 1 is the optimal solution.

0, we deﬁne the function f (x) =

∈

ax

c

|

−

| −

ax. Consider the optimization

Proof. We assume that x(cid:63) = 1 is not the optimal solution. There exists (cid:101)x(cid:63)
That is

∈

[0, 1) such that f ( (cid:101)x(cid:63)) < f (x(cid:63)).

a (cid:101)x(cid:63)

c

− |

−

a

|

| −

+ a < 0,

c
|
a (cid:101)x(cid:63)

a (cid:101)x(cid:63)
−
> a

−

a (cid:101)x(cid:63). On the other hand, according to the inequality that

which implies that
p

a
c
|
−
for p, q

p

q

q

|

| − |

| ≤ |

−

|

c

| − |
∈

|
−
R, we have
a
c

| ≤ |
where the last equality follows that (cid:101)x(cid:63) < 1. We construct a contradiction. Therefore, the original statement is
true.

| − |

−

−

−

−

|

|

c

a (cid:101)x(cid:63)

a (cid:101)x(cid:63)

a

= a

a (cid:101)x(cid:63),

45

(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
Lemma 8. For any constants a, c > 0, we deﬁne the function f (x) =
problem minx
set is [c/a, 1].

ax. Consider the optimization
[0,1] f (x), If x(cid:63) is an optimal solution, then x(cid:63) > 0. Furthermore, if c < a, then the optimal solution

| −

ax

−

∈

c

|

Proof. To begin with, we prove the ﬁrst statement. The proof is based on contradiction. We assume that
x = 0 is the optimal solution. We compare the function value on x = 1 and x = 0.

where the strict inequality follows that a, c > 0. We obtain that f (1) < f (0), which contradicts with the
assumption that x = 0 is the optimal solution. Therefore, the original statement is true and we ﬁnish the
proof.

f (0)

f (1) = c + a

−

c

− |

−

a

|

> 0,

Then we prove the second statement. It is easy to see that

(cid:40)

c

f (x) =

2ax

x
x

−
c

−

[0, c
a ),
[ c
a , 1].

∈
∈

f (x) is continuous piece-wise linear function. f (x) is strictly decreasing when x
[c/a, 1]. Therefore, we can get that the optimal solutions are x(cid:63)
when x

∈
[c/a, 1].

[0, c/a) and is constant

∈

∈

Lemma 9. For any constants a > 0 and c
f (1) = 2a(min
min

, we have f (x)

c/a, 1

{

}

−

0, we deﬁne the function f (x) =

≥
{

c/a, 1

x).

} −

ax

c

|

−

| −

ax. For any x

≤

Proof. We consider two cases: c
f (x) = c
min
hand, when c < a, the function f (x) at [0, 1] is formulated as

a and c < a. When c
= 1, f (x)

≥
c/a, 1
{

2ax. For any x

≥
f (1) = 2a(1

≤

−

−

}

−

a, the function f (x) at [0, 1] is formulated as
x). On the other
c/a, 1

x) = 2a(min

{

} −

(cid:40)

c

f (x) =

2ax

x
x

−
c

−

[0, c
a ),
[ c
a , 1].

∈
∈

For any x
proof.

min

{

≤

c/a, 1

}

= c/a, f (x)

−

f (1) = 2a(c/a

x) = 2a(min

−

c/a, 1

{

} −

x). Therefore, we ﬁnish the

×

∈

Rm

n, c

[n], ∑m

Lemma 10. Consider that A = (aij)
j

∈
i=1 aij = dj. Consider the following optimization problem:
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
∑
j=1
Then x(cid:63) = 1 is the unique optimal solution, where 1 is the vector that each element is 1.

min
[0,1]n
∈

Rn where aij > 0, ∑m

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) −

m
∑
i=1

n
∑
j=1

f (x) :=

d(cid:62)x =

Rm, d

ci −

(cid:107)1 −

aijxj

Ax

−

∈

∈

(cid:107)

c

x

i=1 ci ≥

djxj.

∑m

i=1

∑n

j=1 aij and for each

Proof. For x = (x1,

· · ·

, xn), the function f (x) is formulated as

f (x) =

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

m
∑
i=1

ci −

n
∑
j=1

aijxj

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) −

n
∑
j=1

djxj.

The proof is based on contradiction. We assume that the original statement is not true and there exists
x = (x1,
= 1. We
, xn)
· · ·
construct (cid:101)x = ((cid:101)x1,

[n] denote some index where xk (cid:54)

= 1 such that x is the optimal solution. Let k

, (cid:101)xn)

∈

· · ·

∈

[0, 1]n in the following way.
k

[n]

(cid:101)xj = xj,

j

∀

∈

\ {

,

}

(cid:101)xk = 1.

We compare the function value of x and (cid:101)x.
(cid:32)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

f (x) =

m
∑
i=1

f ((cid:101)x)

−

ci −

n
∑
j=1

aij (cid:101)xj

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) −

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ci −

n
∑
j=1

aijxj

(cid:33)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

dk(1

xk)

−

−

46

(cid:54)
<

m
∑
i=1

(aik(1

xk))

−

−

dk(1

−

xk) = 0.

Here the strict inequality follows the statement that there exists i(cid:63)
n
∑
j=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) −

n
∑
j=1

n
∑
j=1

ai(cid:63) j (cid:101)xj

ai(cid:63) j (cid:101)xj

ai(cid:63) jxj

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ci(cid:63)

ci(cid:63)

ci(cid:63)

−

−

<

−

(cid:33)

(cid:32)

∈
(cid:32)

[m] such that

(cid:33)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
= i(cid:63), with the inequality that

n
∑
j=1

ai(cid:63) jxj

ci(cid:63)

−

−

We will prove this statement later. As for i
a, b

R, we obtain that

[m], i

= ai(cid:63)k(1

xk).

−

a

|

| − |

b

| ≤ |

a

b

|

−

for

∈

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) −

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

aij (cid:101)xj

ci −

n
∑
j=1

n
∑
j=1
Hence the strict inequality holds and we construct (cid:101)x such that f ((cid:101)x) < f (x), which contradicts with the
assumption that x is the optimal solution. Therefore, we prove that the original statement is true and ﬁnish
the proof.

= aik(1

n
∑
j=1

n
∑
j=1

ci −

ci −

ci −

aij (cid:101)xj

aijxj

aijxj

xk).

−

−

(cid:33)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:33)

(cid:32)

∈
(cid:12)
(cid:32)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) ≤

Now we proceed to prove the statement that there exists i(cid:63)
(cid:12)
n
(cid:12)
∑
(cid:12)
(cid:12)
(cid:12)
j=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) −

n
∑
j=1

n
∑
j=1

ai(cid:63) j (cid:101)xj

ai(cid:63) jxj

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ci(cid:63)

ci(cid:63)

ci(cid:63)

<

−

−

−

(cid:32)

∈

[m] such that

(cid:33)

(cid:32)

ai(cid:63) j (cid:101)xj

ci(cid:63)

−

−

n
∑
j=1

ai(cid:63) jxj

(cid:33)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

We also prove this statement by contradiction. We assume that for all i
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) ≥

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) −

ci −

ci −

ci −

aij (cid:101)xj

aij (cid:101)xj

aijxj

n
∑
j=1

n
∑
j=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:33)

(cid:32)

According to the inequality that

[m],

i

∀

∈

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ci −

n
∑
j=1

a

|

b

a

| − |
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) −

| ≤ |
(cid:12)
(cid:12)
(cid:12)
ci −
(cid:12)
(cid:12)

−
n
∑
j=1

aij (cid:101)xj

b

|

for a, b
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

aijxj

Furthermore, consider the inequality
(b

0. Hence we have that

a)b

−

≤

−

n
∑
j=1
R, we have
n
∑
j=1

∈
(cid:32)
ci −

aij (cid:101)xj

∈
(cid:32)

(cid:33)

[m],

ci −

n
∑
j=1

aijxj

(cid:33)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:32)

ci −

−

n
∑
j=1

aijxj

(cid:33)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

a

|

| − |

b

| ≤ |

a

b

|

−

for a, b

∈

R. Notice that the equality holds iff

(cid:32)

i

∀

∈

[m], (aik(1

xk))

−

ci −

(cid:33)

aijxj

0.

≤

n
∑
j=1

Since (aik(1

−

xk)) > 0, we obtain that

This implies that

i

∀

∈

[m], ci −

n
∑
j=1

aijxj ≤

0.

m
∑
i=1

ci ≤

m
∑
i=1

n
∑
j=1

aijxj <

m
∑
i=1

n
∑
j=1

aij ≤

m
∑
i=1

ci,

where the strict inequality follows that xk < 1 and aij > 0. The last inequality follows the assumption of
i=1 ci < ∑m
Lemma 10. Here we ﬁnd a contradiction that ∑m

i=1 ci and hence the original statement is true.

Lemma 11. Under the same conditions in Lemma 10, for any x

[0, 1]n, we have that

where x(cid:63) = 1, which is the vector that each element is 1.

f (x)

−

f (x(cid:63))

n
∑
j=1

≥

∈
aij}

min
i
∈

[m]{

(1

−

xj),

Proof. Recall that f (x) = ∑m

i=1 |

ci −

∑n
j=1 aijxj| −

∑n

j=1 djxj. We ﬁrst claim that when x

[0, 1]n, ci −

∈

47

(cid:54)
∑n

j=1 aijxj < 0 does not hold simultaneously for all i

[m]. We prove this claim via contradiction. Assume

that there exists x

∈

[0, 1]n such that ci −
ci <

m
∑
i=1

∑n

∈
j=1 aijxj < 0,
m
(1)
∑
i=1

n
∑
j=1

aijxj

≤

∀
m
∑
i=1

∈
n
∑
j=1

aij

(2)

≤

m
∑
i=1

ci.

i

[m]. Then we have that

The inequality (1) follows that A > 0 and x
of Lemma 11. Thus we constructs a contradiction, which implies that the original claim is true.

[0, 1]n and the inequality (2) follows that original assumption

∈

Let xp:q be the shorthand of (xp, xp+1,

n. With telescoping, we have that

· · ·
f (x(cid:63)) =

p

≤

, xq) for any 1
n
∑
j=1

f (x(cid:63)
1:j

−

1, xj:n)

−

q

≤
≤
f (x(cid:63)

1:j, xj+1:n).

f (x)

−
1, xj:n) and f (x(cid:63)
, xn ∈

· · ·

Note that f (x(cid:63)
1:j
−
, x(cid:63)
x(cid:63)
1, xj+1,
1,
j
−
Notice that Fj(t) is also a continuous piece-wise linear function.

· · ·

[0, 1], we deﬁne one-variable function Fj(t) = f (x(cid:63)
1:j

1:j, xj+1:n) only differ in the j-th variable. For each j

−

∈
1, t, xj+1:n),

[n], with ﬁxed
[0, 1].

t

∀

∈

On the one hand, Fj(t) is differentiable at any interior point t0 and it holds that

(cid:40)(cid:32)

I

m
∑
i=1

j
1
−
∑
k=1

aikx(cid:63)

n
∑
k=j+1

−

dj +

F(cid:48)j (t0) =

ci −

min
i
∈
0 does not hold simultaneously for all
i=1 aij. On the other hand, the number of boundary points of Fj(t) is m at most. Let
[xj, x(cid:63)
j ]. With fundamental theorem of calculus,

aijt0 −
∑n
j=1 aijxj ≤

k −
[0, 1]n, ci −

The last inequality follows that
[m] and dj = ∑m
i
∈
nj
b1
j , b2
j denote the boundary point of Fj(t) when t
, b
j ,
· · ·
we have that

aij ≤ −

aij}

aikxk

< 0

[m]{

∈

∈

∀

x

.

(cid:33)

(cid:41)

f (x)

−

f (x(cid:63)) =

=

=

n
∑
j=1
n
∑
j=1

n
∑
j=1

f (x(cid:63)
1:j

−

1, xj:n)

−

f (x(cid:63)

1:j, xj+1:n)

Fj(xj)

−

Fj(x(cid:63)
j )



Fj(xj)

Fj(b1

j ) +

−

nj−
1
∑
k=1

Fj(bk
j )

−

Fj(bk+1
j

) + Fj(b

nj
j )

F(x(cid:63)
j )



−



(cid:90) bk+1
j





(cid:90) b1
j

xj

n
∑
j=1

=

−

F(cid:48)j (t)dt +

n
∑
j=1

≥

min
i
∈

[m]{

aij}

(cid:16)

x(cid:63)
j −

xj

nj−
1
∑
k=1

(cid:17)

=

bk
j

n
∑
j=1

F(cid:48)j (t)dt +



F(cid:48)j (t)dt



(cid:90) x(cid:63)
j
nj
j

b

min
i
∈

[m]{

aij}

(cid:0)1

−

(cid:1) .

xj

C.2 Proof of Technical Lemmas in Appendix

C.2.1 Proof of Lemma 3

Proof. For h, h(cid:48) ∈
(a1
h
we have that
|
∀
Furthermore, notice that for any time step h

h(cid:48), we use πh:h(cid:48)
≤
G, πAIL
1], s
h
−

[H], h
[H

∈ S

∈

denote the shorthand of (πh, πh+1,

). From Proposition 1,
, πh(cid:48)
s) = 1. Hence, πAIL and πE never visit bad states.

s) = πE

· · ·

[H], Lossh only depends on π1:h. Therefore, we have

h (a1

|

H
1
∑
−
h=1

Lossh(πE).

H
1
∑
−
h=1

∈

Lossh(πAIL) =

48

It remains to prove that Lossh(πAIL) = Lossh(πE). From Lemma 6, we have that

πAIL

H ∈

argmin
πH

Lossh := ∑
(s,a)

(cid:100)dπE
H (s, a)

−

dπ
H(s, a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

where dπ

H is computed by πAIL
1:H
−

πAIL

H ∈

argmin
πH

1. Then, we have that
∑
s

(cid:100)dπE
H (s, a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

dπE
H (s)πH(a

∑
a
∈S
∑

s

s

G

∈S
∑

G

∈S

∈A
(cid:12)
(cid:12)
(cid:100)dπE
H (s)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:100)dπE
H (s)

= argmin
πH

= argmin
πH

(cid:12)
(cid:12)
(cid:12)
(cid:12)

s)

|

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) −

dπE
H (s)πH(a1

dπE
H (s)πH(a1

s)

s)

|

|

−

−

+ dπE

H (s)

(cid:16)

1

−

πH(a1

(cid:17)

s)

|

dπE
H (s)πH(a1

s).

|

H (s, a) = 0,

In the penultimate equality, we use the facts that 1) for each s
(cid:98)PπE
H (s) = dπE
}
is independent of πH. Since the optimization variables πH(a1
can view the above optimization problem for each πH(a1

; 2) for each s

∈ A \ {

B, (cid:100)dπE

∈ S

a1

∀

a

∈ S

G, we have (cid:100)dπE

H (s, a1) = (cid:100)dπE

H (s) = 0. The last equality follows that dπE
∈ S

s) for different s

H (s), and
H (s)
G are independent, we

|

πAIL

H (a1

s)

|

∈

argmin
s)

πH (a1

|

∈

[0,1]

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:100)dπE
H (s)

−

s) individually.

|

dπE
H (s)πH(a1

(cid:12)
(cid:12)
(cid:12)
(cid:12) −

s)

|

dπE
H (s)πH(a1

s).

|

By Lemma 7, we have that πE
dπE
H −

|
(cid:100)dπE
H (cid:107)1. Combing the above steps, we ﬁnish the proof.

H(a1

(cid:107)

s) = 1 is an optimal solution. Therefore, we have that

dπAIL
H −

(cid:107)

(cid:100)dπE
H (cid:107)1 =

C.2.2 Proof of Proposition 5

Proof. Suppose that πAIL is an optimal solution to (5). Since π is ε-optimal, we have that

where f (π) denotes the state-action distribution matching loss, i.e.,

f (π)

f (πAIL)

−

ε,

≤

f (π) =

H
∑
h=1

∑
(s,a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

dπ
h (s, a)

−

(cid:100)dπE
h (s, a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

By Lemma 3, it holds that f (πAIL) = f (πE). Furthermore, with the decomposition of f (π), we have

f (π)

f (πAIL) = f (π)

f (πE) =

Lossh(π)

Lossh(πE)

ε,

−

−
where Lossh refers to the one-stage state-action distribution matching loss. For any h, h(cid:48) ∈
we use πh:h(cid:48)
have

denote the shorthand of (πh, πh+1,

h(cid:48),
). Note that Lossh only depends on π1:h and thus we

[H] with h

, πh(cid:48)

· · ·

≤

−

≤

(30)

H
∑
h=1

We deﬁne the policy candidate set Πopt =
h=1 Lossh(π1:h)
π
following key composition by telescoping:

Πopt. We will analyze ∑H

∈

{
−

H
∑
h=1

Lossh(π1:h)

ε.

Lossh(πE

1:h)

≤

−
Π :
[H],
s
π
∀
∃
Lossh(πE
1:h) for any π

∈

∈

h

∈ S
∈

G, πh(a1
s) > 0
Πopt. For each h

|

and assume that
[H], we have the

}
∈

Lossh(π1:h)

−

Lossh(πE

1:h) =

h
∑
(cid:96)=1

Lossh(π1:(cid:96), πE

(cid:96)+1:h)

Lossh(π1:(cid:96)

1, πE

(cid:96):h).

−

−

(31)

49

In the following part, we consider two cases: Case I: h < H and Case II: h = H.

First, we consider Case I and focus on the term Lossh(π1:(cid:96), πE

consider two situations: (cid:96) = h and (cid:96) < h.

(cid:96)+1:h)

Lossh(π1:(cid:96)

−

−

1, πE

(cid:96):h). In Case I, we

• When (cid:96) = h, we consider the term Lossh(π1:h)

1, πE
h )
−
differ in the policy in time step h. Hence, we take the policy in time step h as variable and focus on

h ). Note that π1:h and (π1:h

Lossh(π1:h

1, πE

−

−

where g(πh) = Lossh(π1:h) and g(πE

g(πh) = ∑
(s,a) |

(cid:100)dπE
h (s, a)

−

g(πh)

−
h ) = Lossh(π1:h
dπ
h (s, a)

−

|

g(πE
h ),
1, πE
h ). We formulate g(πh) = Lossh(π1:h) as

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:100)dπE
h (s, a)

dπ
h (s)πh(a

|

−

s)

(cid:100)dπE
h (s, a1)

dπ
h (s)πh(a1

s)

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ ∑
s

B

∑
a

∈S
+ dπ
h (s)

∈A
(cid:16)

1

dπ
h (s, a)

πh(a1

−

(cid:100)dπE
h (s)

dπ
h (s)πh(a1

s)

|

−

+ dπ

h (s)

(cid:16)

1

πh(a1

s)

|

−

|
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= ∑
s
∈S
= ∑
s
∈S
= ∑
s

∈S

G

G

G

∑
a
∈A
(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)

s)

|
(cid:17)(cid:19)

dπ
h (s)

(cid:17)(cid:19)

+ ∑
s
+ ∑
s

B

∈S

B

∈S
dπ
h (s).

Note that dπ

h (s) is independent of the policy in time step h. Then we have that

g(πh)

−

g(πE

(cid:100)dπE
h (s)

−

dπ
h (s)πh(a1

dπ
h (s)πh(a1

(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)

s

h ) = ∑
G
(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)

∈S

(cid:12)
(cid:12)
(cid:12)
(cid:12) −

s)

|
(cid:12)
(cid:12)
(cid:12)
(cid:12) −

(cid:19)

s)

|
(cid:19)

−
G, we may apply Lemma 7 and obtain that

−

(cid:100)dπE
h (s)

dπ
h (s)πE

h (a1

s)

|

dπ
h (s)πE

h (a1

s)

|

.

For each s

∈ S

g(πh)

−

g(πE

h ) = Lossh(π1:h)

1, πE
h )

0.

≥

(32)

−
(cid:96)+1:h)

Lossh(π1:h

−
Lossh(π1:(cid:96)

• When (cid:96) < h, we consider the term Lossh(π1:(cid:96), πE

(cid:96)+1:h)
(cid:96):h) only differ in the policy in time step (cid:96). Therefore, we take the policy in time step (cid:96) as

(cid:96):h). Notice that (π1:(cid:96), πE

1, πE

1, πE

−

−

and (π1:(cid:96)
−
variable and focus on

where g(π(cid:96)) = Lossh(π1:(cid:96), πE
g(π(cid:96)) = ∑
(s,a) |

(cid:96)+1:h) and g(πE
(cid:100)dπE
h (s, a)

−

dπ
h (s, a)

|

g(πE

(cid:96) ),
g(π(cid:96))
−
(cid:96) ) = Lossh(π1:(cid:96)

1, πE

(cid:96):h). We can calculate g(π(cid:96)) as

−

G

= ∑
s
∈S
(a)
= ∑
s
∈S
(b)
= ∑
s

∈S

G

G

(cid:100)dπE
h (s, a)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∑
a
∈A
(cid:12)
(cid:12)
(cid:100)dπE
h (s, a1)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:100)dπE
h (s)

−

dπ
h (s)πh(a

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

s)

|

dπ
h (s, a1)

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

dπ
h (s)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ ∑
s

∈S

B

+ ∑
s

B

∈S
dπ
h (s).

dπ
h (s, a)

∑
a

∈A

B

+ ∑
s

∈S
dπ
h (s)

h (s, a) and dπ

Here dπ
for all s
Proposition 1, with Bellman-ﬂow equation in (7), we have

s) = 1
G. This is the difference with the result in the previous case. Similar to the proof of

(cid:96)+1:h), so equality (a) and (b) follow that πh(a1

h (s) are decided by (π1:(cid:96), πE

∈ S

|

s

∀

∈ S

G, dπ

h (s) = ∑
s(cid:48)∈S

∑
a

∈A

dπ
(cid:96) (s(cid:48))π(cid:96)(a

|

s(cid:48))Pπ (cid:0)sh = s

|

s(cid:96) = s(cid:48), a(cid:96) = a(cid:1)

50

Notice that the conditional probability Pπ (cid:0)sh = s
the visitation probability on bad states in time step h, we have

|

= ∑
s(cid:48)∈S

G

dπ
(cid:96) (s(cid:48))π(cid:96)(a1

s(cid:48))Pπ (cid:16)

sh = s

s(cid:96) = s(cid:48), ah = a1(cid:17)

.

|
s(cid:96) = s(cid:48), ah = a1(cid:1) is independent of π(cid:96). Besides, for

|

∑

B

s

∈S

h (s) = ∑
dπ
s(cid:48)∈S
= ∑
s(cid:48)∈S

B

B

(cid:96) (s) + ∑
dπ
s(cid:48)∈S
(cid:96) (s) + ∑
dπ
s(cid:48)∈S

∑

dπ
(cid:96) (s(cid:48))π(cid:96)(a

s)

|

a

∈A\{
dπ
(cid:96) (s(cid:48))

a1
}
(cid:16)
1

G

G

π(cid:96)(a1

s(cid:48))

|

−

(cid:17)

.

(cid:100)dπE
h (s)

g(π(cid:96)) = ∑

Plugging the above two equations into g(π(cid:96)) yields that
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
s
∈S
+ ∑
s(cid:48)∈S

∑
−
s(cid:48)∈S
(cid:96) (s(cid:48)) + ∑
dπ
s(cid:48)∈S
(cid:96) (s) is independent of the policy in time step (cid:96) and we have
g(πE
(cid:96) )

dπ
(cid:96) (s(cid:48))π(cid:96)(a1

Note that dπ

s(cid:48))Pπ (cid:16)

dπ
(cid:96) (s(cid:48))

π(cid:96)(a1

sh = s

s(cid:48))

−

(cid:17)

(cid:16)

1

|

|

|

G

G

G

.

B

g(π(cid:96))
(cid:32)

−
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

G

∑

s
∈S
(cid:32)

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:100)dπE
h (s)

(cid:96) (s(cid:48))Pπ (cid:16)
dπ

sh = s

|

s(cid:96) = s(cid:48), a(cid:96) = a1(cid:17)

π(cid:96)(a1

s(cid:48))

|

−

∑
s(cid:48)∈S

G

∑

∑
s(cid:48)∈S
For this function, we can use Lemma 11 to prove that

(cid:100)dπE
h (s)

sh = s

(cid:96) (s(cid:48))Pπ (cid:16)
dπ

−

−

∈S

|

G

G

s

s(cid:96) = s(cid:48), a(cid:96) = a1(cid:17)

πE
(cid:96) (a1

|

s(cid:96) = s(cid:48), a(cid:96) = a1(cid:17)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∑
s(cid:48)∈S

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) −
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) −

s(cid:48))

∑
s(cid:48)∈S

G

(cid:33)

dπ
(cid:96) (s(cid:48))π(cid:96)(a1

s(cid:48))

|

G

dπ
(cid:96) (s(cid:48))πE

(cid:96) (a1

(cid:33)

.

s(cid:48))

|

(cid:110)

−

g(π(cid:96))

g(πE
(cid:96) )

∑
≥
s(cid:48)∈S
= ∑
s(cid:48)∈S
To check the conditions required by Lemma 11, we deﬁne

(cid:96) (s(cid:48))Pπ (cid:16)
dπ
Pπ (cid:16)

min
G
s

min
G
s

sh = s

∈S

∈S

(cid:110)

|

G

G

|

s(cid:96) = s(cid:48), a(cid:96) = a1(cid:17)(cid:111) (cid:16)
(cid:16)

sh = s
s(cid:96) = s(cid:48), a(cid:96) = a1(cid:17)(cid:111)

dπ
(cid:96) (s(cid:48))

1

(cid:17)

π(cid:96)(a1

s(cid:48))

|

−

1

−

π(cid:96)(a1

s(cid:48))

|

(cid:17)

.

(33)

s

G(cid:12)
(cid:12)
(cid:12) ,

(cid:12)
(cid:12)
(cid:12)S
∀
G, A(s, s(cid:48)) = dπ

∈ S

G, c(s) = (cid:100)dπE
(cid:96) (s(cid:48))Pπ (cid:16)

h (s),

sh = s

s(cid:96) = s(cid:48), a(cid:96) = a1(cid:17)

,

|

G, d(s(cid:48)) = dπ

(cid:96) (s(cid:48)).

m = n =

s, s(cid:48)

∀

∈ S

s(cid:48)

∀

∈ S

Remember that π
G, Ph(s(cid:48)|

S

∈

s, a1) > 0, we have that

Πopt. With the reachable assumption (refer to Assumption 1) that

h

∀

∈

[H], s, s(cid:48) ∈

s, s(cid:48)

∀

∈ S

G, dπ

(cid:96) (s(cid:48)) > 0, Pπ (cid:16)

sh = s

|

s(cid:96) = s(cid:48), a(cid:96) = a1(cid:17)

> 0.

Then we can obtain that A > 0 where > means element-wise comparison. Besides, we have that

c(s) = 1

∑

G

s

∈S

∑

≥

∑
s(cid:48)∈S

(cid:96) (s(cid:48))Pπ (cid:16)
dπ

s(cid:96) = s(cid:48), a(cid:96) = a1(cid:17)

sh = s

|

= ∑
s

∈S

G

A(s, s(cid:48)).

∑
s(cid:48)∈S

G

For each s(cid:48) ∈ S

G

G

∈S

s
G, we further have that
(cid:96) (s(cid:48))Pπ (cid:16)
∑
A(s, s(cid:48)) = ∑
dπ

G

s

∈S

G

s

∈S

s(cid:96) = s(cid:48), a(cid:96) = a1(cid:17)

= dπ

(cid:96) (s(cid:48)) = d(s(cid:48)).

sh = s

|

51

Thus, we have veriﬁed the conditions in Lemma 11 and (33) is true. From (33), we have that

g(π(cid:96))

−

g(πE
(cid:96) )

(cid:110)

Pπ (cid:16)

sh = s

min
G
s

∈S
(cid:110)

Pπ (cid:16)

sh = s

|

dπ
(cid:96) (s(cid:48))

(cid:16)

1

G

−

|

s(cid:96) = s(cid:48), a(cid:96) = a1(cid:17)(cid:111)
s(cid:96) = s(cid:48), a(cid:96) = a1(cid:17)(cid:111) ∑
s(cid:48)∈S
(cid:17)

π(cid:96)(a1

s(cid:48))

.

|

G

≥

∑
s(cid:48)∈S
min
≥
G
s,s(cid:48)∈S
= c(cid:96),h ∑
s(cid:48)∈S
Pπ (cid:0)sh = s

dπ
(cid:96) (s(cid:48))

(cid:16)

1

(cid:17)

π(cid:96)(a1

s(cid:48))

|

−

dπ
(cid:96) (s(cid:48))

(cid:16)

1

(cid:17)

π(cid:96)(a1

s(cid:48))

|

−

G

Here c(cid:96),h = mins,s(cid:48)∈S
(cid:96) < h,

G

{

s(cid:96) = s(cid:48), a(cid:96) = a1(cid:1)

}

> 0. In conclusion, we have proved that for each

|

Lossh(π1:(cid:96), πE

(cid:96)+1:h)

Lossh(π1:(cid:96)

1, πE

(cid:96):h)

−

≥

−

c(cid:96),h ∑
s(cid:48)∈S

G

dπ
(cid:96) (s(cid:48))

(cid:16)

1

π(cid:96)(a1

s(cid:48))

|

−

(cid:17)

.

(34)

Then for Case I where h < H, we combine the results in (32) and (34) to obtain

Lossh(π1:h)

Lossh(πE

1:h)

−
Lossh(π1:(cid:96), πE

=

h
∑
(cid:96)=1

(cid:96)+1:h)

Lossh(π1:(cid:96)

1, πE

(cid:96):h)

−

−

= Lossh(π1:h)

Lossh(π1:h

1, πE

h ) +

−

−

h
1
∑
−
(cid:96)=1

Lossh(π1:(cid:96), πE

(cid:96)+1:h)

Lossh(π1:(cid:96)

1, πE

(cid:96):h)

−

−

Lossh(π1:(cid:96), πE

(cid:96)+1:h)

Lossh(π1:(cid:96)

1, πE

(cid:96):h)

−

−

h
1
∑
−
(cid:96)=1
h
1
∑
−
(cid:96)=1

≥

≥

c(cid:96),h ∑
s(cid:48)∈S

dπ
(cid:96) (s(cid:48))

(cid:16)

1

π(cid:96)(a1

s(cid:48))

|

−

(cid:17)

.

G

(35)

The penultimate inequality follows (32) and the last inequality follows (34).
Second, we consider Case II where h = H. By telescoping, we have that

Lossh(π1:H)

Lossh(πE

1:H)

−

=

H
∑
(cid:96)=1

Lossh(π1:(cid:96), πE

(cid:96)+1:H)

Lossh(π1:(cid:96)

1, πE

(cid:96):H)

−

−

= Lossh(π1:H)

Lossh(π1:H

1, πE

H) +

−

−

H
1
∑
−
(cid:96)=1

Lossh(π1:(cid:96), πE

(cid:96)+1:H)

Lossh(π1:(cid:96)

1, πE

(cid:96):H).

−

−

(36)

Similar to Case I, we also consider two situations: (cid:96) = H and (cid:96) < H. We ﬁrst consider the situation where
(cid:96) < H, which is similar to the corresponding part in Case I.

• When (cid:96) < H, we consider Lossh(π1:(cid:96), πE
to that in Case I. Note that (π1:(cid:96), πE
take the policy in time step (cid:96) as variable and focus on

(cid:96)+1:H)
(cid:96)+1:H) and (π1:(cid:96)

−

−

Lossh(π1:(cid:96)
1, πE

(cid:96):H). The following analysis is similar
(cid:96):H) only differ in the policy in time step (cid:96). We

−

1, πE

where g(π(cid:96)) = Lossh(π1:(cid:96), πE

g(πE

g(π(cid:96))
(cid:96) ),
−
(cid:96)+1:h) and g(πE
(cid:96) ) = Lossh(π1:(cid:96)
−
(cid:12)
(cid:12)
(cid:12)
(cid:12)
g(π(cid:96)) = ∑
+ ∑
(cid:12)
(cid:12)
(cid:12)
(cid:12)
s
∈S
(cid:96)+1:h). With Bellman-ﬂow equation in (7), it holds that

(cid:96):h). Similarly, we have

(cid:100)dπE
h (s)

dπ
h (s).

dπ
h (s)

1, πE

−

∈S

G

s

B

s(cid:48))Pπ (cid:16)

sH = s

|

s(cid:96) = s(cid:48), ah = a1(cid:17)

,

Here dπ

h (s) is computed by (π1:(cid:96), πE

s

∀

∈ S

G, dπ

h (s) = ∑
s(cid:48)∈S

G

dπ
(cid:96) (s(cid:48))π(cid:96)(a1

|

52

∑

h (s) = ∑
dπ

B

s

B

∈S

s

∈S

(cid:96) (s) + ∑
dπ
s(cid:48)∈S

dπ
(cid:96) (s(cid:48))

(cid:16)

1

π(cid:96)(a1

s(cid:48))

|

−

(cid:17)

.

G

g(π(cid:96)) = ∑

Plugging the above two equations into g(π(cid:96)) yields that
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
s
∈S
+ ∑
s

∑
−
s(cid:48)∈S
(cid:96) (s) + ∑
dπ
s(cid:48)∈S

dπ
(cid:96) (s(cid:48))π(cid:96)(a1

(cid:100)dπE
h (s)

dπ
(cid:96) (s(cid:48))

−

∈S

(cid:16)

1

|

G

G

G

B

π(cid:96)(a1

(cid:17)

.

s(cid:48))

|

Notice that dπ

(cid:96) (s) is independent of the policy in time step (cid:96) and we have
g(πE
(cid:96) )

s(cid:48))Pπ (cid:16)

s(cid:96) = s(cid:48), a(cid:96) = a1(cid:17)

sH = s

|

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

g(π(cid:96))
(cid:32)

−
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

G

∑

s
∈S
(cid:32)

=

(cid:100)dπE
h (s)

(cid:96) (s(cid:48))Pπ (cid:16)
dπ

sH = s

|

s(cid:96) = s(cid:48), a(cid:96) = a1(cid:17)

π(cid:96)(a1

s(cid:48))

|

−

∑
s(cid:48)∈S

G

(cid:96) (s(cid:48))Pπ (cid:16)
dπ

G

sH = s

|

s(cid:96) = s(cid:48), a(cid:96) = a1(cid:17)

(cid:96) (a1
πE

|

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∑

G

∈S

(cid:100)dπE
h (s)

−

∑
s(cid:48)∈S

−

s

For this type function in RHS, we can use Lemma 11 to prove that

∑
s(cid:48)∈S

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) −
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) −

s(cid:48))

∑
s(cid:48)∈S

G

(cid:33)

dπ
(cid:96) (s(cid:48))π(cid:96)(a1

s(cid:48))

|

G

dπ
(cid:96) (s(cid:48))πE

(cid:96) (a1

(cid:33)

.

s(cid:48))

|

g(π(cid:96))

−

g(πE
(cid:96) )

∑
≥
s(cid:48)∈S
= ∑
s(cid:48)∈S

(cid:110)

(cid:110)

min
G
s

∈S

min
G
s

∈S

G

G

(cid:96) (s(cid:48))Pπ (cid:16)
dπ
Pπ (cid:16)

sH = s

sH = s

|

s(cid:96) = s(cid:48), a(cid:96) = a1(cid:17)(cid:111) (cid:16)
(cid:16)

1

s(cid:96) = s(cid:48), a(cid:96) = a1(cid:17)(cid:111)

dπ
(cid:96) (s(cid:48))

|

(cid:17)

π(cid:96)(a1

s(cid:48))

|

−

1

−

π(cid:96)(a1

s(cid:48))

|

(cid:17)

.

(37)

To check the conditions in Lemma 11, we deﬁne

s

G(cid:12)
(cid:12)
(cid:12) ,

(cid:12)
(cid:12)
(cid:12)S
∀
G, A(s, s(cid:48)) = dπ

∈ S

G, c(s) = (cid:100)dπE
(cid:96) (s(cid:48))Pπ (cid:16)

h (s),

sH = s

s(cid:96) = s(cid:48), a(cid:96) = a1(cid:17)

,

|

G, d(s(cid:48)) = dπ

(cid:96) (s(cid:48)).

m = n =

s, s(cid:48)

∀

∈ S

s(cid:48)

∀

∈ S

Similar to the analysis in Case I, we obtain that A > 0 and
∑

(cid:96) (s(cid:48))Pπ (cid:16)
dπ

c(s) = 1

sH = s

∑

s(cid:96) = s(cid:48), a(cid:96) = a1(cid:17)

|

s

∀

∈S
s(cid:48)

G

∈ S

G, ∑
s

∈S

G

∑
≥
s(cid:48)∈S
A(s, s(cid:48)) = ∑

∈S

G

G

s

G

s

∈S

(cid:96) (s(cid:48))Pπ (cid:16)
dπ

sh = s

|

s(cid:96) = s(cid:48), a(cid:96) = a1(cid:17)

= ∑
s
∈S
= dπ

A(s, s(cid:48)),

∑
s(cid:48)∈S
(cid:96) (s(cid:48)) = d(s(cid:48)).

G

G

Thus, we have veriﬁed the conditions in Lemma 11 and (37) is true. From (37), we get

g(π(cid:96))

−

g(πE
(cid:96) )

Here c(cid:96),H = mins,s(cid:48)∈S

G

{

min
G
s

G

≥

∈S
(cid:110)

∑
s(cid:48)∈S
min
≥
G
s,s(cid:48)∈S
= c(cid:96),H ∑
s(cid:48)∈S
Pπ (cid:0)sH = s

G

Lossh(π1:(cid:96), πE

(cid:96)+1:H)

−

(cid:110)

Pπ (cid:16)

sH = s

|

s(cid:96) = s(cid:48), a(cid:96) = a1(cid:17)(cid:111)
s(cid:96) = s(cid:48), a(cid:96) = a1(cid:17)(cid:111) ∑
s(cid:48)∈S
(cid:17)

π(cid:96)(a1

s(cid:48))

.

−

|

Pπ (cid:16)

sH = s

|

dπ
(cid:96) (s(cid:48))

(cid:16)

1

dπ
(cid:96) (s(cid:48))

(cid:16)

1

(cid:17)

π(cid:96)(a1

s(cid:48))

|

−

dπ
(cid:96) (s(cid:48))

(cid:16)

1

(cid:17)

π(cid:96)(a1

s(cid:48))

|

−

G

s(cid:96) = s(cid:48), a(cid:96) = a1(cid:1)
1, πE

|
Lossh(π1:(cid:96)

}
(cid:96):H)

. In summary, for (cid:96) < H, we have proved that

≥

c(cid:96),H ∑
s(cid:48)∈S

G

dπ
(cid:96) (s(cid:48))

(cid:16)

1

π(cid:96)(a1

s(cid:48))

|

−

(cid:17)

.

(38)

• When (cid:96) = H, we consider the term Lossh(π1:H)
more complicated. Note that π1:H and (π1:H

−
1, πE

Lossh(π1:H
H). The analysis in this situation is
H) only differs in the policy in the last time step H.

−

1, πE

−

−

53

Take the policy in time step H as variable and we focus on

where g(πH) = Lossh(π1:H) and g(πE
Lossh(π1:H) as

g(πE

g(πH)
H) = Lossh(π1:H

−

H),
1, πE

−

g(πH) = ∑
∈S

s

G

(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:100)dπE
h (s)

−

dπ
h (s)πH(a1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

s)

|

+ dπ

h (s)

(cid:16)

1

πH(a1

s)

|

−

(cid:17)(cid:19)

dπ
h (s).

+ ∑
s

∈S

B

Note that dπ

h (s) is independent of the policy in time step H and we have that

g(πH)

−

g(πE

H) = ∑

s

∈S

G
(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:100)dπE
h (s)

−

dπ
h (s)πH(a1

dπ
h (s)πH(a1

s)

(cid:19)

|
(cid:19)

(cid:12)
(cid:12)
(cid:12)
(cid:12) −

s)

|
(cid:12)
(cid:12)
(cid:12)
(cid:12) −

(cid:100)dπE
h (s)

−

dπ
h (s)πE

H(a1

s)

|

dπ
h (s)πE

H(a1

s)

|

.

H). Similarly, we can formulate g(πH) =

Given estimation (cid:100)dπE

π = ∅. Here
c

Sπ ∩ S

h (s), we divide the set of good states into two parts. That is
Sπ =
min

s)

s

Sπ ∪ S
S
. Therefore, we have that
(cid:19)

G =

c
π and

{
g(πE

g(πH)

−

|
≤
(cid:100)dπE
h (s)

s

G, πH(a1
(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)

∈ S
H) = ∑
∈Sπ
(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)
−
+ ∑
s

c
π

∈S

(cid:124)

(cid:100)dπE
h (s)
(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:100)dπE
h (s)

−

1, (cid:100)dπE

{
dπ
h (s)πH(a1

h (s)/dπE
H (s)
(cid:12)
(cid:12)
(cid:12)
(cid:12) −

s)

−

}}
dπ
h (s)πH(a1

dπ
h (s)πE

H(a1

s)

|

−

|
(cid:12)
(cid:12)
(cid:12)
(cid:12) −

s)

|
(cid:19)

H(a1

s)

|

dπ
h (s)πE
(cid:12)
(cid:12)
(cid:12)
(cid:12) −

s)

dπ
h (s)πH(a1

s)

|

(cid:19)

(cid:125)

dπ
h (s)πE

H(a1

s)

|

(cid:19)

.

(cid:125)

dπ
h (s)πH(a1
|
(cid:123)(cid:122)
T1(s)
(cid:12)
(cid:12)
(cid:12)
(cid:12) −

s)

|

H(a1
(cid:123)(cid:122)
T2(s)

(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)

−
(cid:124)

(cid:100)dπE
h (s)

−

dπ
h (s)πE

By Lemma 7, we have that ∑s

∈S
H)

g(πE

c

π T1(s) + T2(s)
(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)

≥
(cid:100)dπE
h (s)

≥

−

g(πH)

−

0. Then we have that

dπ
h (s)πH(a1

dπ
h (s)πH(a1

s

∑
∈Sπ
(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12) −

s)

|
(cid:12)
(cid:12)
(cid:12)
(cid:12) −

(cid:19)

s)

|
(cid:19)

.

(cid:100)dπE
h (s)

dπ
h (s)πE

H(a1

s)

|

dπ
h (s)πE

H(a1

s)

|

For each s
(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:100)dπE
h (s)
(cid:18)

−
∈ Sπ, we aim to apply Lemma 9 to prove that
(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)

dπ
h (s)πH(a1

dπ
h (s)πH(a1

s)

−

(cid:19)

−

−

|

s)

(cid:12)
(cid:12)
(cid:12)
(cid:12) −
|
h (s)/dπE

H (s)

(cid:19)

πH(a1

} −

s)

|

.

2dπ

h (s)

≥

1, (cid:100)dπE

min

{

(cid:100)dπE
h (s)

dπ
h (s)πE

H(a1

−

(cid:12)
(cid:12)
(cid:12)
(cid:12) −

s)

|

dπ
h (s)πE

H(a1

(cid:19)

s)

|

To check the conditions required by Lemma 9, we deﬁne

c = (cid:100)dπE

h (s), a = dπ

h (s), x = πH(a1

s).

|

It is easy to see that c
Assumption 1) that

h

the deﬁnition of

0. Since π

∀

≥
[H

1],
Sπ, we have that x = πH(a1

∈
s, s(cid:48) ∈ S

−

∈

∀

Πopt, combined with the reachable assumption (refer to
G, Ph(s(cid:48)|
h (s) > 0. According to
h (s)/dπ
s)
=
≤
|

s, a1) > 0, we have that a = dπ
h (s)/dπE

1, (cid:100)dπE

1, (cid:100)dπE

H (s)

h (s)

min

min

} ≤

}

{

{

54

min

. We have veriﬁed the conditions in Lemma 9 and obtain that

dπ
h (s)πH(a1

−

(cid:12)
(cid:12)
(cid:12)
(cid:12) −

s)

|

dπ
h (s)πH(a1

(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

(cid:19)

|

s)

(cid:19)

(cid:100)dπE
h (s)

−

dπ
h (s)πE

H(a1

(cid:12)
(cid:12)
(cid:12)
(cid:12) −

s)

|

dπ
h (s)πE

H(a1

(cid:19)

s)

|

1, c/a
{
(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)

}
(cid:100)dπE
h (s)
(cid:18)

= 2dπ

h (s)

min

(cid:18)

min

{

{

1, (cid:100)dπE

h (s)/dπ

h (s)

πH(a1

} −

s)

|

2dπ

h (s)

≥

1, (cid:100)dπE

h (s)/dπE

H (s)

πH(a1

} −

where the last inequality follows that
H) yields that
g(πH)

g(πE

−

g(πH)

g(πE

H)

−

≥

In summary, we have proved that

2 ∑
s
∈Sπ

s

∀

∈ S

H (s)

dπ
h (s). Plugging the above inequality into

≥

(cid:18)

dπ
h (s)

min

{

1, (cid:100)dπE

h (s)/dπE

H (s)

πH(a1

s)

|

} −

(cid:19)

.

Lossh(π1:H)

Lossh(π1:H

1, πE
H)

−

≥

−

(cid:18)

dπ
h (s)

min

{

2 ∑
s
∈Sπ

1, (cid:100)dπE

h (s)/dπE

H (s)

πH(a1

s)

|

} −

(cid:19)

.

(39)

(cid:19)

s)

,

|
G, dπE

In Case II where h = H, with (36), (38), and (39), we have that
1:H)

Lossh(π1:H)

Lossh(πE

−

−
(cid:18)

= Lossh(π1:H)

≥

2 ∑
s
∈Sπ

dπ
h (s)

Lossh(π1:H

1, πE

H) +

−

H
1
∑
−
(cid:96)=1

Lossh(π1:(cid:96), πE

(cid:96)+1:H)

Lossh(π1:(cid:96)

1, πE

(cid:96):H)

−

−

1, (cid:100)dπE

h (s)/dπE

H (s)

min

{
Pπ (cid:0)sH = s

|

s(cid:96) = s(cid:48), a(cid:96) = a1(cid:1)

}

πH(a1

s)

|

} −

(cid:19)

+

H
1
∑
−
(cid:96)=1

c(cid:96),H ∑

dπ
(cid:96) (s)

(cid:16)

1

−

π(cid:96)(a1

(cid:17)

,

s)

|

(40)

G

s

∈S

. The last inequality follows (38) and (39).

where c(cid:96),H = mins,s(cid:48)∈S

G

{

Finally, we combine the results in (35) and (40) to obtain that

f (π)

f (πE)

−

Lossh(π1:h)

Lossh(πE

1:h)

−

H
∑
h=1
H
1
∑
−
h=1
H
1
∑
−
h=1

=

=

≥

+ 2 ∑
s
∈Sπ
h
1
∑
−
(cid:96)=1

H
∑
h=1

=

H
∑
h=1

≥

−
h
1
∑
−
(cid:96)=1

Lossh(π1:h)

−

Lossh(πE

1:H) + Lossh(π1:h)

Lossh(πE

1:H)

−

h
1
∑
−
(cid:96)=1

c(cid:96),h ∑

dπ
(cid:96) (s)

(cid:16)

1

(cid:17)

π(cid:96)(a1

s)

|

−

s

G
∈S
(cid:18)

dπ
h (s)

1, (cid:100)dπE

h (s)/dπE

H (s)

πH(a1

s)

|

} −

(cid:19)

+

H
1
∑
−
(cid:96)=1

min

{

c(cid:96),H ∑

dπ
(cid:96) (s)

(cid:16)

1

(cid:17)

π(cid:96)(a1

s)

|

−

G

s

∈S

(cid:18)

dπ
h (s)

min

{

1, (cid:100)dπE

h (s)/dπE

H (s)

πH(a1

s)

|

} −

(cid:19)

,

. The penultimate inequality follows (35) and (40). In

}

c(cid:96),h ∑

dπ
(cid:96) (s)

(cid:16)

1

(cid:17)

π(cid:96)(a1

s)

s

G

−
|
s(cid:96) = s(cid:48), a(cid:96) = a1(cid:1)
Pπ (cid:0)sh = s
where c(cid:96),h = mins,s(cid:48)∈S
Πopt, we have
summary, we prove that for any π
f (πE)

f (π)

|
∈

∈S

{

G

+ 2 ∑
s
∈Sπ

c(cid:96),h ∑

dπ
(cid:96) (s)

(cid:16)

1

G

s

∈S

π(cid:96)(a1

s)

|

−

(cid:17)

+ 2 ∑
s
∈Sπ

(cid:18)

dπ
h (s)

min

{

55

1, (cid:100)dπE

h (s)/dπE

H (s)

(cid:19)

πH(a1

s)

|

} −

c(π)

≥

(cid:32) H
∑
h=1

h
1
∑
−
(cid:96)=1

s

dπ
(cid:96) (s)

(cid:16)

1

∑

G

(cid:17)

π(cid:96)(a1

s)

|

−

Here c(π) = min1
that

≤

(cid:96)<h

≤

∈S
H c(cid:96),h = min1

(cid:96)<h

≤

≤

H,s,s(cid:48)∈S

{

dπ
h (s)

+ ∑
s
∈Sπ
Pπ (cid:0)sh = s
G

(cid:18)

min

1, (cid:100)dπE

h (s)/dπE

H (s)

{

πH(a1

s)

|

} −

(cid:19)(cid:33)

.

s(cid:96) = s(cid:48), a(cid:96) = a1(cid:1)

|

. Since π

}

∈

Πopt, it holds

f (π)

c(π)

≥

f (πE)
h
1
∑
−
(cid:96)=1

−
(cid:32) H
∑
h=1

dπ
(cid:96) (s)

(cid:16)

1

∑

G

π(cid:96)(a1

s)

|

−

s

∈S

(cid:17)

+ ∑
s
∈Sπ

(cid:18)

dπ
H(s)

min

{

1, (cid:100)dπE

h (s)/dπE

H (s)

πH(a1

s)

|

} −

(cid:19)(cid:33)

.

Combined with (30), we obtain

c(π)

(cid:32) H
∑
h=1

h
1
∑
−
(cid:96)=1

s

∈S

dπ
(cid:96) (s)

(cid:16)

1

∑

G

π(cid:96)(a1

s)

|

−

which completes the whole proof.

(cid:17)

+ ∑
s
∈Sπ

(cid:18)

dπ
H(s)

min

{

1, (cid:100)dπE

h (s)/dπE

H (s)

πH(a1

s)

|

} −

(cid:19)(cid:33)

ε,

≤

C.2.3 Proof of Lemma 12

Proof of Lemma 12. Lemma 12 is a direct consequence of the regret bound of online gradient descend [Shalev-
Shwartz, 2012]. To apply such a regret bound, we need to verify that 1) the iterate norm
(cid:107)2 has an upper
(cid:107)2 also has an upper bound. The ﬁrst point is easy to show, i.e.,
bound; 2) the gradient norm
∈ CTV =
. For the second point, we have
}

(cid:107)∇c f (t)(c)
by the condition that c

S × A →

|S||A|

(cid:107)2 ≤

1, 1]

ch :

(cid:112)

−

H

(cid:107)

{

c

[

c
(cid:107)
that

(cid:13)
(cid:13)

(cid:13)∇c f (t)(c)

(cid:13)
(cid:13)
(cid:13)2

=

≤

≤

(cid:118)
(cid:117)
(cid:117)
(cid:116)

(cid:118)
(cid:117)
(cid:117)
(cid:116)

H
∑
h=1

H
∑
h=1

(cid:118)
(cid:117)
(cid:117)
(cid:116)

H
∑
h=1
2√H,

≤

(cid:18)

∑

dπ(t)
h

(s, a)

(cid:100)dπE
h (s, a)

−

(cid:19)2

(cid:16)

dπ(t)
h

(s, a)

(cid:18)

(cid:17)2

+

(cid:100)dπE
h (s, a)

(cid:19)2

(s,a)

∈S×A

2 ∑
(s,a)

∈S×A

(cid:18)(cid:13)
(cid:13)dπ(t)
(cid:13)

h

2

(cid:13)
(cid:13)
(cid:13)1

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:100)dπE
h

(cid:19)

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

where the ﬁrst inequality follows (a + b)2
0

1.

x
Then, invoking Corollary 2.7 in [Shalev-Shwartz, 2012] with B = (cid:112)
≤

≤

≤

2(a2 + b2) and the second inequality is based on that x2

if

x

|

≤ |

H

|S||A|

and L = 2√H ﬁnishes the

proof.

D Discussion

D.1 Optimization Procedures for TV-AIL

Here we discuss optimization procedures for TV-AIL. Recall the objective of TV-AIL:

H
∑
h=1

min
Π
π
∈

∑

(s,a)

∈S×A

dπ
h (s, a)

|

(cid:100)dπE
h (s, a)

,

|

−

(41)

Two optimization approaches can solve the above state-action distribution matching problem. First, we can
utilize linear programming to solve the above optimization problem exactly [Syed et al., 2008, Rajaraman
et al., 2021b]. The main idea is to relax the optimization variable from π to dπ, and solve the matching

56

problem in the space of state-action distributions. In particular, we notice that the optimization objective and
Bellman-ﬂow constraints are linear with respect to dπ. Thus, linear programming is applicable. Finally, we
recover the optimal policy from the solved state-action distribution. Please see [Syed et al., 2008, Rajaraman
et al., 2021b] for details.

Second, we can utilize gradient-based methods to solve this optimization problem approximately. This
type of optimization approach is widely used in practice [Ho and Ermon, 2016, Ghasemipour et al., 2019, Ke
et al., 2020, Kostrikov et al., 2019]. We utilize the min-max formulation for TV-AIL.

max
c
∈CTV
CTV is the set of functions ch :

min
Π
π
∈

H
∑
h=1

E

(s,a)

dπ
h

∼

[ch(s, a)]

E

−

[ch(s, a)] ,

(s,a)

(cid:100)dπE
h

∼

(42)

1, 1]. Then our target is to solve the saddle point of the
where
above min-max problem. By the dual representation of policy value in (1), we see that the outer problem is to
ch(s, a). For the inner optimization problem, we
maximize the policy value of π given the reward function
can use online gradient descent methods [Shalev-Shwartz, 2012] so that we can ﬁnally reach an approximate
saddle point. Formally, let us deﬁne the objective f (t)(c):

S × A →

−

−

[

∑
(s,a)
where π(t) is the optimized policy at iteration t. Then the update rule for c is:
c(t+1) :=

(cid:100)dπE
h (s, a)

f (t)(c(t))

ch(s, a)

dπ(t)
h

(s, a)

η(t)

c(t)

−

(cid:16)

(cid:17)

,

,

(cid:18)

(cid:19)

H
∑
h=1

where η(t) > 0 is the stepsize to be chosen later, and
PCTV (c) := argminz
z

∈CTV (cid:107)

−

c

(cid:107)2. The above procedure is outlined in Algorithm 1.

∇

−
PCTV is the Euclidean projection on the set

PCTV

(43)

CTV, i.e.,

Algorithm 1 TV-AIL via Gradient-based Method

, number of iterations T, step size η(t), and initialization c(1).

Input: expert demonstrations
1: Obtain the estimation (cid:100)dπE
h
2: for t = 1, 2,
π(t)
3:
Compute the state-action distribution dπ(t)

D
in (4).

, T do

· · ·

←

(cid:16)

4:
5: Update c(t+1) :=
6: end for
7: Compute the mean state-action distribution dh(s, a) = ∑T
8: Compute πh(a
Output: policy π.

dh(s, a)/ ∑a dh(s, a) for all h

h
f (t)(c(t))

PCTV

η(t)

c(t)

←

∇

s)

−

∈

|

solve the optimal policy with the reward function

c(t) up to an error of εopt.

−
for π(t) for all h
(cid:17)

∈
with f (t)(c) deﬁned in (43).

[H].

t=1 dπ(t)

h

(s, a)/T for all h

[H], (s, a)

.

∈ S × A

∈

[H], (s, a)

.

∈ S × A

For the optimization problem in Line 3 of Algorithm 1, we can use value iteration or policy gradient
methods [Agarwal et al., 2020]. Speciﬁcally, if we use value iteration, it is clear that εopt = 0 and this
procedure can be done in H iterations for episodic MDPs. In the sequel, we demonstrate that by Algorithm 1,
the state-action distribution matching problem in TV-AIL can be solved approximately.

Proposition 6. Fix ε
error εopt ≤
have

ε/2, the number of iterations T (cid:37)

(0, H). Consider Algorithm 1 with π being the output policy. Assume that the optimization
/(8T). Then we

H2/ε2, and the step size η(t) := (cid:112)

∈

|S||A|

|S||A|

(cid:13)
(cid:13)
(cid:13)
(cid:13)

H
∑
h=1

dπ
h −

(cid:100)dπE
h

(cid:13)
(cid:13)
(cid:13)
(cid:13)1 ≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)

H
∑
h=1

min
Π
π
∈

dπ
h −

(cid:100)dπE
h

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

+ ε.

Before we prove Proposition 6, we ﬁrst state the key lemma as follows.

57

Lemma 12. Consider Algorithm 1, then we have

where

f (t) (cid:16)

c(t)(cid:17)

T
∑
t=1

−

min
c
∈CTV

T
∑
t=1

f (t)(c)

≤

(cid:113)

2

2H

T,

|S||A|

f (t)(c) =

H
∑
h=1

∑

(s,a)

∈S×A

ch(s, a)((cid:100)dπE

h (s, a)

dπ(t)
h

(s, a)).

−

Please refer to Appendix C.2.3 for the proof. Basically, Lemma 12 is a direct consequence of the regret
bound of online gradient descent [Shalev-Shwartz, 2012]. Now, we proceed to prove Proposition 6. The
analysis here is based on the proof strategy of [Syed and Schapire, 2007, Theorem 2]. Here we list main proof
procedures and interested readers are referred to [Syed and Schapire, 2007] for details.

Proof of Proposition 6. With the dual representation of (cid:96)1-norm and the celebrated min-max theorem Bertsekas
[2016], we have

(cid:13)
(cid:13)
(cid:13)
(cid:13)

H
∑
h=1

min
Π
π
∈

dπ
h −

(cid:100)dπE
h

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

=

−

min
c
∈CTV

max
Π
π
∈

H
∑
h=1

∑

ch(s, a)

(cid:18)

(s,a)

∈S×A

(cid:100)dπE
h (s, a)

dπ
h (s, a)

−

(cid:19)

.

Furthermore, we have that

min
c
∈CTV

max
Π
π
∈

H
∑
h=1

(s,a)

∑

ch(s, a)

(cid:18)

(cid:100)dπE
h (s, a)

dπ
h (s, a)

−

(cid:19)

∈S×A
(cid:32)
1
T

T
∑
t=1

(cid:33) (cid:18)

c

(t)
h (s, a)

(cid:100)dπE
h (s, a)

dπ
h (s, a)

−

(cid:19)

(cid:18)

c

(t)
h (s, a)

(cid:100)dπE
h (s, a)

dπ
h (s, a)

−

(cid:19)

.

(cid:18)

(cid:100)dπE
h (s, a)

dπ(t)
h

−

(cid:19)

(s, a)

+ εopt.

H
∑
h=1

≤

max
Π
π
∈

∑

(s,a)

1
T

1
T

≤

≤

T
∑
t=1

max
Π
π
∈

∈S×A
H
∑
h=1

(s,a)

∑

∈S×A

T
∑
t=1

H
∑
h=1

∑

(s,a)

∈S×A

c

(t)
h (s, a)

In the last inequality, in iteration t, π(t) is the approximately optimal policy regarding reward function
with an optimization error of εopt. Applying Lemma 12 yields that

c(t)

−

1
T

T
∑
t=1

H
∑
h=1

∑

(cid:18)

c

(t)
h (s, a)

(cid:100)dπE
h (s, a)

−

dπ(t)
h

(s, a)

(cid:19)

(s,a)

1
T

T
∑
t=1

∈S×A
H
∑
h=1

(s,a)

H
∑
h=1

H
∑
h=1

∑

(s,a)

∈S×A
∑

(s,a)

∈S×A

≤

min
c
∈CTV

= min
c
∈CTV

= min
c
∈CTV

∑

ch(s, a)

(cid:18)

(cid:100)dπE
h (s, a)

dπ(t)
h

−

(cid:19)

(s, a)

+ 2H

(cid:114)

2

|S||A|
T

∈S×A

(cid:32)

ch(s, a)

(cid:100)dπE
h (s, a)

ch(s, a)

(cid:18)

(cid:100)dπE
h (s, a)

−

−

1
T

T
∑
t=1

dπ(t)
h

(cid:33)

(s, a)

+ 2H

(cid:114)

2

|S||A|
T

(cid:19)

+ 2H

(cid:114)

2

dπ
h (s, a)

|S||A|
T

.

Note that π is induced by the mean state-action distribution, i.e., πh(a
dh(s, a) = 1/T

(s, a). Based on Proposition 3.1 in [Ho and Ermon, 2016], we have that dπ

s) = dh(s, a)/ ∑a dh(s, a), where
h (s, a) =

t=1 dπ(t)
∑T

|

h

·

58

dh(s, a), and hence the last equation holds. Then we get that
(cid:18)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

H
∑
h=1

min
Π
π
∈

dπ
h −

(cid:100)dπE
h

(cid:13)
(cid:13)
(cid:13)
(cid:13)1 ≥ −

H
∑
h=1

min
c
∈CTV
(cid:13)
(cid:13)
(cid:13)
(cid:13)

H
∑
h=1

dπ
h −

=

∑

ch(s, a)

(cid:100)dπE
h (s, a)

(s,a)
∈S×A
(cid:13)
(cid:13)
(cid:13)
(cid:13)1 −

(cid:100)dπE
h

2H

(cid:114)

2

|S||A|
T

−

εopt,

dπ
h (s, a)

−

(cid:114)

2

(cid:19)

2H

−

|S||A|
T

−

εopt

where the last step again utilizes the dual representation of (cid:96)1-norm. When εopt ≤
we see that

ε/2 and T

32

≥

|S||A|

H2/ε2,

(cid:13)
(cid:13)
(cid:13)
(cid:13)

H
∑
h=1

dπ
h −

(cid:100)dπE
h

(cid:13)
(cid:13)
(cid:13)
(cid:13)1 ≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)

H
∑
h=1

min
Π
π
∈

dπ
h −

(cid:100)dπE
h

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

+ ε.

The proof is ﬁnished.

D.2 Addressing Sample Barrier Issue of TV-AIL

This section discusses how to address the sample barrier issue of TV-AIL on RBAS MDPs. That is, in the
last time step, TV-AIL may make a wrong decision because there is no future guidance. We propose two
(cid:91)
dπE
approaches. The ﬁrst approach is to add a one-stage terminal loss
H+1(cid:107)1. As a consequence,
TV-AIL can exactly recover the expert policy in the ﬁrst H time steps. We note that this approach is widely
applied in dynamic-programming-based algorithms [Bertsekas, 2012]. Another approach is to directly
override the policy of TV-AIL in the ﬁnal time step by a BC’s policy. In this way, we can prove that the
), which is the one-step imitation gap of a BC policy. Notably,
imitation gap bound becomes
the improved bound for TV-AIL is better than that of BC in the whole sample regime.

dπ
H+1 −

(min

/N

|S|

O

1,

{

}

(cid:107)

D.3 Performance of Other AIL Methods

In this part, we present experiment results for three representative AIL methods FEM, GTAL and GAIL, on
RBAS MDPs and the lower bound instances (refer to Assumption 2). In particular, FEM [Abbeel and Ng,
2004] and GTAL [Syed and Schapire, 2007] perform state-action distribution matching with (cid:96)2-norm-based
and (cid:96)∞-norm-based divergences, respectively. Besides, GAIL [Ho and Ermon, 2016] minimizes the state-
action distribution discrepancy with the JS divergence. First, Table 7 summarizes the imitation gaps with
different horizons on a RBAS MDP. We clearly see that the imitation gaps of FEM, GTAL and GAIL do not
increase when the horizon grows, which is similar to TV-AIL. Second, we evaluate TV-AIL, FEM, GTAL and
GAIL on the lower bound instance with different horizons; see the results in Table 8 and Table 9. In both
large sample regime (Table 8) and small sample regime (Table 9), we observe that the imitation gaps of all
methods increase when the horizon grows. In summary, we observe that FEM, GTAL and GAIL exhibit
similar algorithmic behaviors to TV-AIL.

Table 7: Imitation gap on the RBAS MDP with N = 1.

H = 100

H = 500

H = 1000

H = 2000

TV-AIL 0.69
FEM 0.58
0.80
GTAL
0.94
GAIL

0.00
0.00
0.00
0.00

±
±
±
±

0.70
0.57
0.81
0.95

0.00
0.00
0.00
0.00

±
±
±
±

0.71
0.58
0.81
0.95

0.00
0.00
0.19
0.00

±
±
±
±

0.71
0.58
0.74
0.95

0.00
0.00
0.32
0.00

±
±
±
±

59

Table 8: Imitation gap on the lower bound instance with N = 1.

H = 100

H = 500

H = 1000

H = 2000

TV-AIL 49.50
FEM 49.50
50.05
GTAL
49.50
GAIL

0.01
0.00
4.93
0.00

±
±
±
±

247.50
247.50
250.52
247.50

0.00
0.00
3.96
0.00

±
±
±
±

495.00
495.00
495.05
495.00

0.01
0.00
4.95
0.00

±
±
±
±

990.00
990.00
989.06
990.00

0.00
0.00
4.85
0.00

±
±
±
±

Table 9: Imitation gap on the lower bound instance with N = 100.

H = 100

H = 500

H = 1000

H = 2000

TV-AIL 18.23
FEM 18.18
21.52
GTAL
18.27
GAIL

1.63
1.63
2.22
1.62

±
±
±
±

94.30
92.85
94.63
91.71

6.85
7.06
7.83
7.55

±
±
±
±

179.80
192.21
188.02
184.34

16.31
15.89
15.06
14.34

±
±
±
±

360.05
378.15
373.98
371.09

38.21
30.52
30.82
30.64

±
±
±
±

E Experiment Details

E.1 Experiment Details on MuJoCo

For MuJoCo tasks, all experiments are run with 5 random seeds. We use the expert dataset collected by
the trained online SAC [Haarnoja et al., 2018] with 1 million steps. We use the deterministic policy as the
expert policy, which is common in the literature [Ho and Ermon, 2016]. The expert policy values are listed in
Table 10.

For MuJoCo tasks, we implement BC according to [Li et al., 2022]. The implementation of TV-AIL is
based on an existing AIL algorithm DAC [Kostrikov et al., 2019] (https://github.com/google-research/
google-research/tree/master/value_dice). Instead of the KL-divergence in DAC, TV distance is consid-
ered in TV-AIL. To this end, the tanh activation function is applied in the last layer of the discriminator.

Table 10: Policy value of the expert policy on Hopper with different horizons.

Horizon

H=100 H=500 H=1000 H=2000

Policy value

221.76

1533.83

3202.19

6496.92

E.2 Experiment Details on Tabular MDPs

For tabular MDPs, all experiments are run with 20 random seeds. The detailed task information is listed in
Table 11. For RBAS MDPs, we consider that the initial state distribution is uniform over good states. For the
lower bound instances (refer to Assumption 2), we consider that the initial state distribution is uniform over
all states. By construction, the policy value of the expert policy is H.

BC directly estimates the expert policy from expert demonstrations via (3). For TV-AIL, we run Algo-
rithm 1 to obtain an approximate solution. In our experiments, an adaptive step size [Orabona, 2019] is
implemented for TV-AIL:

η(t) =

(cid:113)

D

∑t

i=1

(cid:13)
(cid:13)

∇c f (i) (cid:0)c(i)(cid:1)(cid:13)

(cid:13)

2
2

,

60

Table 11: Information about tabular MDPs.

Tasks

Number of states Number of actions Horizon

Number of expert trajectories

RBAS MDPs (Table 3)

RBAS MDPs (Table 4)

Lower bound instance (Table 8)

Lower bound instance (Table 9)

20

20

100

100

2

2

2

2

1000

[1, 4, 7, 10]

[10, 100, 1000, 2000]

[10, 100, 1000, 2000]

1

1

[10, 100, 1000, 2000]

100

where D = (cid:112)
step size [Orabona, 2019].

|S||A|

2H

is the diameter of the set

CTV. Notice that Proposition 6 still holds with this adaptive

61

