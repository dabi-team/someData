Partial Wasserstein Covering

Keisuke Kawano, Satoshi Koide, Keisuke Otaki
Toyota Central R&D Labs., Inc.
{kawano, koide, otaki}@mosk.tytlabs.co.jp

1
2
0
2

c
e
D
8

]

G
L
.
s
c
[

3
v
6
8
8
0
0
.
6
0
1
2
:
v
i
X
r
a

Abstract

We consider a general task called partial Wasserstein covering with the goal of providing information on what
patterns are not being taken into account in a dataset (e.g., dataset used during development) compared with another
dataset(e.g., dataset obtained from actual applications). We model this task as a discrete optimization problem with
partial Wasserstein divergence as an objective function. Although this problem is NP-hard, we prove that it satisﬁes
the submodular property, allowing us to use a greedy algorithm with a 0.63 approximation. However, the greedy al-
gorithm is still inefﬁcient because it requires solving linear programming for each objective function evaluation. To
overcome this inefﬁciency, we propose quasi-greedy algorithms that consist of a series of acceleration techniques,
such as sensitivity analysis based on strong duality and the so-called C-transform in the optimal transport ﬁeld. Ex-
perimentally, we demonstrate that we can efﬁciently ﬁll in the gaps between the two datasets and ﬁnd missing scene
in real driving scenes datasets.

1 Introduction

A major challenge in real-world machine learning applications is coping with mismatches between the data distribution
obtained in real-world applications and those used for development. Regions in the real-world data distribution that are
not well supported in the development data distribution (i.e., regions with low relative densities) result in potential risks
such as a lack of evaluation or high generalization error, which in turn leads to low product quality. Our motivation is
to provide developers with information on what patterns are not being taken into account when developing products by
selecting some of the (usually unlabeled) real-world data distribution, also referred to as application dataset,1 to ﬁll
in the gaps in the development dataset. Note that the term “development” includes choosing models to use, designing
subroutines for fail safe, and training/testing models.

Our research question is formulated as follows. To resolve the lack of data density in development datasets, how
and using which metric can we select data from application datasets with a limited amount of data for developers to
understand? One reasonable approach is to deﬁne the discrepancy between the data distributions and select data to
minimize this discrepancy. The Wasserstein distance has attracted signiﬁcant attention as a metric for data distribu-
tions [1, 2]. However, the Wasserstein distance is not capable of representing the asymmetric relationship between
application datasets and development datasets, i.e., the parts that are over-included during development increase the
Wasserstein distance.

In this paper, we propose partial Wasserstein covering (PWC) that selects a limited amount of data from the ap-
plication dataset by minimizing the partial Wasserstein divergence [3] between the application dataset and the union
of the development dataset and the selected data. PWC, as illustrated in Fig. 1, selects data from areas with fewer
development data than application data in the data distributions (lower-right area in the blue distribution in Fig. 1)
while ignoring areas with sufﬁcient development data (upper-middle of the orange distribution). We also highlight the
data selected through an anomaly detection method, LOF [4], where irregular data (upper-right points) were selected,
but the major density difference was ignored. Furthermore, we show the selection obtained using an active learning
method, coreset with a k-center [5], where the data are chosen to improve the accuracy rather than ﬁll the gap in terms
of the distribution mismatch.

Our main contributions are summarized as follows.

1More precisely, we call a ﬁnite set of data sampled from the real-world data distribution application dataset in this study.

1

 
 
 
 
 
 
Figure 1: Concept of PWC. PWC extracts some data from an unlabeled application dataset by minimizing the par-
tial Wasserstein divergence between the application dataset and the union of the selected data and a development
dataset. PWC focuses on regions in which development data are lacking compared with the application dataset, whereas
anomaly detection extracts irregular data, and active learning selects to improve accuracy.

• We propose PWC that extracts data that are lacking in a development dataset from an application dataset by

minimizing the partial Wasserstein divergence between an application dataset and the development dataset.

• We prove that PWC is a maximization problem involving a submodular function whose inputs are the set of

selected data. This allows us to use a greedy algorithm with a guaranteed approximation ratio.

• Additionally, we propose fast heuristics based on sensitivity analysis and the Sinkhorn derivative for an acceler-

ated computation.

• Experimentally, we demonstrate that compared with baselines, PWC extracts data lacking in the development

data distribution from the application distribution more efﬁciently.

The remainder of this paper is organized as follows. Section 2 brieﬂy introduces the partial Wasserstein divergence,
linear programming (LP), and submodular functions. In Sect. 3, we detail the proposed PWC, fast algorithms for
approximating the optimization problem, and some theoretical results. Section 4 presents a literature review of related
work. We demonstrated the PWC in some numerical experiments in Sect. 5.1. In Sect. 6, we present our conclusions.

2 Preliminaries

In this section, we introduce the notations used throughout this paper. We then describe partial Wasserstein divergences,
sensitivity analysis for LP, and submodular functions.

Notations Vectors and matrices are denoted in bold (e.g., x and A), where xi and Aij denote the ith and (i, j)th
elements, respectively. To clarify the elements, we use notations such as x = (f (i))i and A = (g(i, j))ij, where f
and g specify the element values depending on their subscripts. (cid:104)·, ·(cid:105) denotes the inner product of matrices or vectors.
1n ∈ Rn is a vector with all elements equal to one. For a natural number n, we deﬁne [[n]] := {1, · · · , n}. For a ﬁnite
set V , we denote its power set as 2V and its cardinality as |V |. The L2-norm is denoted as (cid:107) · (cid:107). The delta function is
denoted as δ(·). R+ is a set of real positive numbers, and [a, b] ⊂ R denotes a closed interval. I[·] denotes an indicator
function (zero for false and one for true).

In this paper, we consider the partial Wasserstein divergence [6, 3, 7] as an objec-
Partial Wasserstein divergence
tive function. Partial Wasserstein divergence is designed to measure the discrepancy between two distributions with

2

Partial Wasserstein coveringOriginal data distributionAnomaly detection (LOF)Active learning (coreset)Application datasetDevelopment datasetSelected datadifferent total masses by considering variations in optimal transport. Throughout this paper, datasets are modeled as
empirical distributions that are represented by mixtures of delta functions without necessarily having the same total
mass. Suppose two empirical distributions (or datasets) X and Y with probability masses a ∈ Rm
+, which
are denoted as X = (cid:80)m
j=1 bjδ(y(j)). Without loss of generality, the total mass of Y is greater
than or equal to that of X (i.e., (cid:80)m

i=1 aiδ(x(i)) and Y = (cid:80)n

+ and b ∈ Rn

i=1 ai = 1 and (cid:80)n

j=1 bj ≥ 1).

Based on the deﬁnitions above, we deﬁne the partial Wasserstein divergence as follows:2

PW 2(X, Y ) := min

(cid:104)P, C(cid:105) , where

P∈U (a,b)

U (a, b) = {P ∈ [0, 1]m×n | P1n = a, P(cid:62)1m ≤ b},

(1)

where Cij := (cid:107)x(i) − y(j)(cid:107)2 is the transport cost between x(i) and y(j), and Pij is the amount of mass ﬂowing from
x(i) to y(j) (to be optimized). Unlike the standard Wasserstein distance, the second constraint in Eq. (1) is not deﬁned
with “=”, but with “≤”. This modiﬁcation allows us to treat distributions with different total masses. The condition
P(cid:62)1m ≤ b indicates that the mass in Y does not need to be transported, whereas the condition P1n = a indicates that
all of the mass in X should be transported without excess or deﬁciency (just as in the original Wasserstein distance).
This property is useful for the problem deﬁned below, which treats datasets with vastly different sizes.

To compute the partial Wasserstein divergence, we must solve the minimization problem in Eq.(1). In this paper,
we consider the following two methods. (i) LP using simplex method. (ii) Generalized Sinkhorn iteration with entropy
regularization (with a small regularization parameter ε > 0) [8, 9].

As will be detailed later, an element in mass b varies when adding data to the development datasets. A key to our
algorithm is to quickly estimate the extent to which the partial Wasserstein divergence will change when an element in
b varies. If we compute PW 2 using LP, we can employ a sensitivity analysis, which will be described in the following
paragraph. If we use generalized Sinkhorn iterations, we can use automatic differentiation techniques to obtain a partial
derivative with respect to bj (see Sect. 3.3).

LP and sensitivity analysis The sensitivity analysis of LP plays an important role in our algorithm. Given a variable
x ∈ Rm and parameters c ∈ Rm, d ∈ Rn, and A ∈ Rn×m, the standard form of LP can be written as follows:
min c(cid:62)x, s.t. Ax ≤ d, x ≥ 0. Sensitivity analysis is a framework for estimating changes in a solution when
the parameters c, A, and d of the problem vary. We consider a sensitivity analysis for the right-hand side of the
constraint (i.e., d). When dj changes as dj + ∆dj, the optimal value changes by y∗
j ∆dj if a change ∆dj in dj lies
within (dj, dj), where y∗ is the optimal solution of the following dual problem corresponding to the primal problem:
max d(cid:62)y s.t. A(cid:62)y ≥ c, y ≥ 0. We refer readers to [10] for the details of calculating the upper bound dj and the
lower bound di.

Submodular function Our covering problem is modeled as a discrete optimization problem involving submodular
functions, which are a subclass of set functions that play an important role in discrete optimization. A set function
φ : 2V → R is called a submodular iff φ(S ∪ T ) + φ(S ∩ T ) ≤ φ(S) + φ(T ) (∀S, T ⊆ V ). A submodular
function is monotone iff φ(S) ≤ φ(T ) for S ⊆ T . An important property of monotone submodular functions is that
a greedy algorithm provides a (1 − 1/e) ≈ 0.63-approximate solution to the maximization problem under a budget
constraint |S| ≤ K [11]. The contraction ˜φ : 2V → R of a (monotone) submodular function φ, which is deﬁned as
˜φ(S) := φ(S ∪ T ) − φ(T ), where T ⊆ V , is also a (monotone) submodular function.

3 Partial Wasserstein covering problem

3.1 Formulation

As discussed in Sect. 1, our goal is to ﬁll in the gap between the application dataset Dapp by adding some data from
a candidate dataset Dcand to a small dataset Ddev. We consider Dcand = Dapp (i.e., we copy some data in Dapp and add

2The partial optimal transport problems in the literature contain a wider problem deﬁnition than Eq.(1) as summarized in Table 1 (b) of [3], but

this paper employs this one-side relaxed Wasserstein divergence corresponding to Table 1 (c) in [3] without loss of generality.

3

them into Ddev) in the above-mentioned scenarios, but we herein consider the most general formulation. We model this
task as a discrete optimization problem called the partial Wasserstein covering problem.

Given a dataset for development Ddev = {y(j)}Ndev

a dataset containing candidates for selection Dcand = {s(j)}Ncand
the following optimization:3

j=1, a dataset obtained from an application Dapp = {x(i)}Napp

i=1 , and
j=1 , where Napp ≥ Ndev, the PWC problem is deﬁned as

−PW 2(Dapp, S ∪ Ddev) + PW 2(Dapp, Ddev)

(2)

max
S⊆Dcand
s.t. |S|≤K

We select a subset S from the candidate dataset Dcand under the budget constraint |S| ≤ K ((cid:28) Ncand), and then add
that subset to the development data Ddev to minimize the partial Wasserstein divergence between the two datasets Dapp
and S ∪ Ddev. The second term is a constant with respect to S, which is included to make the objective non-negative.
In Eq. (2), we must specify the probability mass (i.e., a and b in Eq. (1)) for each data point. Here, we employ a
uniform mass, which is a natural choice because we do not have prior information regarding each data point. Specif-
ically, we set the weights to ai = 1/Napp for Dapp and bj = 1/Ndev for S ∪ Ddev. With this choice of masses, we
can easily show that PW 2(Dapp, S ∪ Ddev) = W 2(Dapp, Ddev) when S = ∅, where W 2(Dapp, Ddev) is the Wasserstein
distance. Therefore, based on the monotone property (Sect. 3.2), the objective value is non-negative for any S.

The PWC problem in Eq.(2) can be written as a mixed integer linear program (MILP) as follows. Instead of using
the divergence between Dapp and S ∪ Ddev, we consider the divergence between Dapp and Dcand ∪ Ddev. Hence, the
mass b is an (Ncand + Ndev)-dimensional vector, where the ﬁrst Ncand elements correspond to the data in Dcand and the
remaining elements correspond to the data in Ddev. In this case, we never transport to data points in Dcand \ S, meaning
we use the following mass that depends on S:

bj(S) =

(cid:40) I[s(j)∈S]
Ndev
,

1
Ndev

,

if 1 ≤ j ≤ Ncand
if Ncand + 1 ≤ j.

(3)

As a result, the problem is an MILP problem with an objective function (cid:104)P, C(cid:105) w.r.t. S ⊆ Dcand and P ∈ U (a, b(S))
with |S| ≤ K and ai = 1/Napp.

One may wonder why we use the partial Wasserstein divergence in Eq.(2) instead of the standard Wasserstein
distance. This is because the asymmetry in the conditions of the partial Wasserstein divergence enables us to extract
only the parts with a density lower than that of the application dataset, whereas the standard Wasserstein distance
becomes large for parts with sufﬁcient density in the development dataset. Furthermore, the guaranteed approximation
algorithms that we describe later can be utilized only for the partial Wasserstein divergence version.

3.2 Submodularity of the PWC problem

In this section, we prove the following theorem to guarantee the approximation ratio of the proposed algorithms.

Theorem 1. Given the datasets X = {x(i)}Nx
φ(S) = −PW 2(X, S ∪ Y ) + PW 2(X, Y ) is a monotone submodular function.

i=1 and Y = {y(j)}Ny

j=1, and a subset of a dataset S ⊆ {s(j)}Ns

j=1,

To prove Theorem 1, we reduce our problem to the partial maximum weight matching problem [12], which is

known to be submodular. First, we present the following lemmas.

Lemma 1. Let Q be the set of all rational numbers, and m and n be positive integers with n ≤ m. Consider A ∈ Qm×n
and b ∈ Qm. Then, the extreme points x∗ of a convex polytope deﬁned by the linear inequalities Ax ≤ b are also
rational, meaning x∗ ∈ Qn.

Proof of Lemma 1. It is well known [10] that any extreme point of the polytope is obtained by (i) choosing n
inequalities from Ax ≤ b and (ii) solving the corresponding n-variable linear system Bx = ˜b, where B and ˜b are the

3We herein consider the partial Wasserstein divergence between the unlabled datasets, because the application and candidate datasets are usually

unlabeled. If they are labeled, we can use the labels information as in [2].

4

corresponding sub-matrix/vector (when replacing ≤ with =). As all elements in B−1 and ˜b are rational, 4 we conclude
that any extreme points are rational.

Lemma 2. Let l, m, and n be positive integers and Z = [[n]]. Given a positive-valued m-by-n matrix R > 0, the
following set function ψ : 2Z → R is a submodular function:

ψ(S) =

(cid:104)R, P(cid:105) ,

max
P∈U ≤(1m/m,b(S))
where bj(S) = I[j∈S]

l

∀j ∈ [[n]].

(4)

Here, U ≤(a, b(S)) is a set deﬁned by replacing the constraint P1n = a in Eq. (1) with P1n ≤ a.

Proof of Lemma 2. For any ﬁxed S ⊆ Z, there is an optimal solution P∗ of ψ(S), where all elements P ∗
ij are
rational because all the coefﬁcients in the constraints are in Q (∵ Lemma 1). Thus, there is a natural number M ∈ Z+
that satisﬁes ψ(S) = 1
mlM maxP∈UQ(S) (cid:104)R, P(cid:105) where the set UQ(S) is the set of possible transports when the transport
mass is restricted to the rational numbers Q.

UQ(S) = (cid:8)P ∈ Zm×n

+

P(cid:62)1m ≤ mM · I[j ∈ S] · 1n

| P1n ≤ lM · 1m,
(cid:9) .

(5)

The problem of ψ(S) can be written as a network ﬂow problem on a bipartite network following Sect 3.4 in
[9]. Since the elements of P are integers, we can reformulate the problem of ψ(S) using maximum-weight bipar-
tite matching (i.e., an assignment problem) on a bipartite graph whose vertices are generated by copying the ver-
tices lM and mM times with modiﬁed edge weights. Therefore, G(V ∪ W, E) is a bipartite graph, where V =
{v1,1, . . . , v1,lM , . . . , vm,1, . . . , vm,lM } and W = {w1,1, . . . , w1,mM , . . . , wn,1, . . . , wn,mM }. For any i ∈ [[m]], j ∈
[[n]], the transport mass Pij ∈ Z+ is encoded by using the copied vertices {vi,1, . . . , vi,lM } and {wj,1, . . . , wj,mM },
mlM Rij > 0 to represent Rij. The partial maximum weight match-
and the weights of the edges among them are
ing function for a graph with positive edge weights is a submodular function [12]. Therefore, ψ(S) is a submodular
function.
Lemma 3. If |S| ≥ l, there exists an optimal solution P∗ in the maximization problem of ψ satisfying P∗1n = 1m
m
(i.e., P∗ := arg maxP∈U ≤(

m ,b(S)) (cid:104)R, P(cid:105) = arg maxP∈U (

m ,b(S)) (cid:104)R, P(cid:105)).

1m

1m

1

i,j P ∗

Proof of Lemma 3. Given an optimal solution P∗, suppose there exists i(cid:48) satisfying (cid:80)n

m − (cid:80)n
ij < 1. We denote the gap as δ := 1
j=1 bj(S) ≥ 1. Hence, there must exist j(cid:48) such that (cid:80)m

that (cid:80)
(cid:80)n
0. We also deﬁne ˜δ = min{δ, δ(cid:48)} > 0. Then, a matrix P(cid:48) = (P ∗
However, Rij > 0 leads to (cid:104)R, P(cid:48)(cid:105) > (cid:104)R, P∗(cid:105), which contradicts P∗ being the optimal solution. Thus, (cid:80)n
1
m , ∀i.

m . This implies
i(cid:48)j > 0. Based on the assumption of |S| ≥ l, we have
ij(cid:48) < bj(cid:48). We denote the gap as δ(cid:48) := bj(cid:48) − (cid:80)m
ij(cid:48) >
ij + ˜δI[i = i(cid:48), j = j(cid:48)])ij is still a feasible solution.
ij =

j=1 P ∗
i=1 P ∗

i(cid:48)j < 1

j=1 P ∗

j=1 P ∗

i=1 P ∗

Proof of Theorem 1. Given Cmax = maxi,j Cij + γ, where γ > 0, the following is satisﬁed:

φ(S) = (−Cmax + max

P∈U (a,b(S))

(cid:104)R, P(cid:105))

− (−Cmax + max

P∈U (a,b(∅))

(cid:104)R, P(cid:105)),

(6)

where R = (Cmax − Cij)ij > 0, m = Nx, n = Ns + Ny, and l = Ny. Here, |S ∪ Y | ≥ Ny = l and Lemma 3
yield φ(S) = ψ(S ∪ Y ) − ψ(Y ). Since φ(S) is a contraction of the submodular function ψ(S) (Lemma 2), φ(S) =
−PW 2(X, S ∪ Y ) + PW 2(X, Y ) is a submodular function.

We now prove the monotonicity of φ. For S ⊆ T , we have U (a, b(S)) ⊆ U (a, b(T )) because b(S) ≤ b(T ). This

implies that φ(S) ≤ φ(T ).

Finally, we prove Proposition 1 to clarify the computational intractability of the PWC problem.

4This immediately follows from the fact that B−1 = ˜B/ det B, where ˜B is the adjugate matrix of B and det B is the determinant of B. Since

B ∈ Qn×n, ˜B and det B are both rational.

5

Proposition 1. The PWC problem with marginals a and b(S) is NP-hard.

Proof of Proposition 1. The decision version of the PWC problem, denoted as PWC(a, b(S), C, K, x), asks whether
a subset S, |S| ≤ K such that φ(S) ≤ x ∈ R exists. We discuss a reduction to the PWC from the well-known NP-
complete problems called set cover problems [13]. Given a family G = {Gj ⊆ I}M
j=1 from the set I = [[N ]] and an
integer k, SetCover(G, I, k) is a decision problem for answering Yes iff there exists a sub-family G(cid:48) ⊆ G such that
|G(cid:48)| ≤ k and ∪G∈G(cid:48)G = I. Our reduction is deﬁned as follows. Given SetCover(G, I, k) for i ∈ [[N ]] and j ∈ [[M ]],
we set ai = 1, bj(S) = |Gj| · I[j ∈ S], Cij = I[i ∈ Gj], K = k, and x = 0. Based on this polynomial reduction, the
given SetCover instance is Yes iff the reduced PWC instance is Yes. This reduction means that the decision version of
PWC is NP-complete and now Proposition 1 holds.

3.3 Algorithms

MILP and greedy algorithms The PWC problem in Eq. (2) can be solved directly as an MILP problem. We refer to
this method as PW-MILP. However, this is extremely inefﬁcient when Napp, Ndev, and Ncand are large.

As a consequence of Theorem 1, a simple greedy algorithm provides a 0.63 approximation because the problem is a
type of monotone submodular maximization with a budget constraint |S| ≤ K [11]. Speciﬁcally, this greedy algorithm
selects data from the candidate dataset Dcand sequentially in each iteration to maximize φ. We consider two baseline
methods, called PW-greedy-LP and PW-greedy-ent. PW-greedy-LP solves the LP problem using the simplex
method. PW-greedy-ent computes the optimal P(cid:63) using Sinkhorn iterations [8].

Even for the greedy algorithms mentioned above, the computational cost is still high because we need to calculate
the partial Wasserstein divergences for all candidates on Dapp in each iteration, yielding a computational time complex-
ity of O(K·Ncand·CP W ). Here, CP W is the complexity of the partial Wasserstein divergence with the simplex method
for the LP or Sinkhorn iteration.

To reduce the computational cost, we propose heuristic approaches called quasi-greedy algorithms. A high-level
description of the quasi-greedy algorithms is provided below. In each step of greedy selection, we use a type of sensi-
tivity value that allows us to estimate how much the partial Wasserstein divergence varies when adding candidate data,
instead of performing the actual computation of divergence. Speciﬁcally, if we add a data point s(j) to S, denoted as
T = {s(j)} ∪ S, we have b(T ) = b(S) + ej
, where ej is a one-hot vector with the corresponding jth element
Ndev
activated. Hence, we can estimate the change in the divergence for data addition by computing the sensitivity with
respect to b(S). If efﬁcient sensitivity computation is possible, we can speed up the greedy algorithms. We describe
the concrete methods for each of the approaches, simplex method, and Sinkhorn iterations in the following paragraphs.

Quasi-greedy algorithm for the simplex method When we use the simplex method for the partial Wasserstein
divergence computation, we employ a sensitivity analysis for LP. The dual problem of partial Wasserstein divergence
computation is deﬁned as follows.

max
f ∈RNapp ,g∈RNcand+Ndev

(cid:104)f , a(cid:105) + (cid:104)g, b(S)(cid:105) ,

s.t. gj ≤ 0, fi + gj ≤ Cij, ∀i, j,

(7)

j ∆bj, where g∗

where f and g are dual variables. Using sensitivity analysis, the changes in the partial Wasserstein divergences can be
estimated as g∗
j is an optimal dual solution of Eq. (7), and ∆bj is the change in bj. It is worth noting that
∆bj now corresponds to I[s(j) ∈ S] in Eq. (3), and a smaller Eq. (7) results in a larger Eq. (2). Thus, we propose a
heuristic algorithm that iteratively selects s(j∗) satisfying j∗ = arg minj∈[[Ncand]],s(j) /∈S g∗(t)
is
the optimal dual solution at t. We emphasize that we have − 1
j = φ({s(j)}∪S)−φ(S) as long as bj −bj ≥ 1/Ndev
g∗
Ndev
holds, where bj is the upper bound obtained from the sensitivity analysis, leading to the same selection as that of the
greedy algorithm. The computational complexity of the heuristic algorithm is O(K ·CP W ) because we can obtain g∗
by solving the dual simplex. It should be noted that we add a small value to bj when bj = 0 to avoid undeﬁned values
in g. We refer to this algorithm as PW-sensitivity-LP and summarize the overall algorithm in Algorithm 1.

at iteration t, where g∗(t)

j

j

6

Algorithm 1: Data selection for the PWC problem with sensitivity analysis (PW-sensitivity-LP)

1: Input: Dapp, Ddev, Dcand
2: Output: S
3: S ← {}, t ← 0
4: while |S| < K do

5:

6:

Calculate PW 2(Dapp, S ∪ Ddev) , and obtain g∗(t)
j∗ = arg minj∈[[Ncand]],s(j) /∈S g∗(t)
S ← S ∪ {s(j∗)}
t ← t + 1

j

j

7:
8:
9: end while

from the sensitivity analysis.

For computational efﬁciency, we use the solution matrix P from the previous step for the initial value in the simplex
algorithm in the current step. Any feasible solution P ∈ U (a, b(S)) is also feasible because P ∈ U (a, b(T )) for all
S ⊆ T . The previous solutions of the dual forms f and g are also utilized for the initialization of the dual simplex and
Sinkhorn iterations.

Faster heuristic using C-transform The heuristics above still require solving a large LP in each step, where the
number of dual constraints is Napp × (Ncand + Ndev). Here, we aim to reduce the size of the constraints in the LP to
Napp × (|S| + Ndev) (cid:28) Napp × (Ncand + Ndev) using C-transform (Sect.3.1 in [9]).

To derive the algorithm, we consider the dual Eq.(7). Considering the fact that bj(S) = 0 for s(j) /∈ S, we ﬁrst
solve the smaller LP problem by ignoring j such that s(j) /∈ S, whose system size is Napp × (|S| + Ndev). Let the
optimal dual solution of this smaller LP be (f ∗, g∗), where f ∗ ∈ RNapp , g∗ ∈ R|S|+Ndev. For each s(j) /∈ S, we consider
an LP in which s(j) is added to S. Instead of solving each LP such as PW-greedy-LP, we approximate the optimal
solution using a technique called the C-transform. More speciﬁcally, for each j such that s(j) /∈ S, we only optimize
gj and ﬁx the other variables to be the dual optimal above. This is done by gC
:= min{0, mini∈[[Ndev]] Cij − f ∗
i }.
j
Note that this is the largest value of gj, satisfying the dual constraints. This gC
j gives the estimated increase in the
objective function when s(j) /∈ S is added to S. Finally, we select the instance j∗ = arg minj∈[[Ncand]],s(j) /∈S gC
j . As
shown later, this heuristic experimentally works efﬁciently in terms of computational times. We refer to this algorithm
as PW-sensitivity-Ctrans.

Quasi-greedy algorithm for Sinkhorn iteration When we use the generalized Sinkhorn iterations, we can easily
obtain the derivative of the entropy-regularized partial Wasserstein divergence ∇j := ∂PW 2
(cid:15) (Dapp, S ∪ Ddev)/∂bj,
where PW 2
(cid:15) denotes the partial Wasserstein divergence, using automatic differentiation techniques such as those pro-
vided by PyTorch [14]. Based on this derivative, we can derive a quasi-greedy algorithm for the Sinkhorn iteration
as j∗ = arg minj∈[[Ncand]],s(j) /∈S ∇j (i.e., we replace Lines 6 and 7 of Algorithm 1 with this formula). Because the
derivative can be obtained with the same computational complexity as the Sinkhorn iterations, the overall complexity
is O(K · CP W ). We refer this algorithm PW-sensitivity-ent.

4 Related work

Instance selection and data summarization Tasks for selecting an important portion of a large dataset are common
in machine learning. Instance selection involves extracting a subset of a training dataset while maintaining the target
task performance. Accoring to [15], two types of instance selection methods have been studied: model-dependent meth-
ods [16, 17, 18, 19, 20] and label-dependent methods [21, 22, 23, 24, 25, 26]. Unlike instance selection methods, PWCs
do not depend on either model or ground-truth labels. Data summarization involves ﬁnding representative elements in
a large dataset [27]. Here, the submodularity plays an important role [28, 29, 30, 31]. Unlike data summarization, PWC
focuses on ﬁlling in gaps between datasets.

7

Anomaly detection Our covering problem can be considered as the task of selecting a subset of a dataset that is not
included in the development dataset. In this regard, our method is related to anomaly detectors e.g., LOF [4], one-class
SVM [32], and deep learning-based methods [33, 34]. However, our goal is to extract patterns that are less included
in the development dataset, but have a certain volume in the application, rather than selecting rare data that would be
considered as observation noise or infrequent patterns that are not as important as frequent patterns.

Active learning The problem of selecting data from an unlabeled dataset is also considered in active learning. A
typical approach usees the output of the target model e.g., the entropy-based method [35, 36], whereas some methods
are independent of the output of the model e.g., the k-center-based [5], and Wasserstein distance-based approach [37].
In contrast, our goal is ﬁnding unconsidered patterns during development, even if they do not contribute to improving
the prediction performance by directly adding them to the training data; developers may use these unconsidered data to
test the generalization performance, design subroutines to process the patterns, redevelop a new model, or alert users
to not use the products in certain special situations. Furthermore, we emphasize that the submodularity and guaranteed
approximation algorithms can be used only with the partial Wasserstein divergence, but not with the vanilla Wasserstein
distance.

Facility location problem (FLP) and k-median The FLP and related k-median [38] are similar to our problem
in the sense that we select data S ⊆ Dcand to minimize an objective function. FLP is a mathematical model for
ﬁnding a desirable location for facilities (e.g., stations, schools, and hospitals). While FLP models facility opening
costs, our data covering problem does not consider data selection costs, making it more similar to k-median problems.
However, unlike the k-median, our covering problem allows relaxed transportation for distribution, which is known
as Kantorovich relaxation in the optimal transport ﬁeld [9]. Hence, our approach using partial Wasserstein divergence
enables us to model more ﬂexible assignments among distributions beyond na¨ıve assignments among data.

5 Experiments

In our experiments, we considered a scenario in which we wished to select some data in the application dataset Dapp for
the PWC problem (i.e., Dapp = Dcand). For PW-*-LP and PW-sensitivity-Ctrans, we used IBM ILOG CPLEX
20.01 [39] for the dual simplex algorithm, where the sensitivity analysis for LP is also available. For PW-*-ent, we
computed the entropic regularized version of the partial Wasserstein divergence using PyTorch [14] on a GPU. We
computed the sensitivity using automatic differentiation. We set the coefﬁcient for entropy regularization (cid:15) = 0.01 and
terminated the Sinkhorn iterations when the divergence did not change by at least maxi,j Cij×10−12 compared with the
previous step, or the number of iterations reached 50,000. All experiments were conducted with an Intel® Xeon® Gold
6142 CPU and an NVIDIA® TITAN RTX™ GPU.

5.1 Comparison of algorithms

First, we compare our algorithms, namely, the greedy-based PW-greedy-*, sensitivity-based PW-sensitivity-*,
and random selection for the baseline. For the evaluation, we generated two-dimensional random values following a
normal distribution for Dapp and Ddev, respectively. Figure 2 (left) presents the partial Wasserstein divergences be-
tween Dapp and S ∪ Ddev while varying the number of the selected data when Napp = Ndev = 30 and Fig. 2 (right)
presents the computational times when K = 30 and Napp = Ndev = Ncand varied. As shown in Fig. 2 (left),
PW-greedy-LP, and PW-sensitivity-LP select exactly the same data as the global optimal solution obtained
by PW-MILP. As shown in Fig. 2 (right), considering the logarithmic scale, PW-sensitivity-* signiﬁcantly
reduces the computational time compared with PW-MILP, while the na¨ıve greedy algorithms do not scale. In par-
ticular, PW-sensitivity-ent can be calculated quickly as long as the GPU memory is sufﬁcient, whereas its
CPU version (PW-sensitivity-ent(CPU)) is signiﬁcantly slower. PW-sensitivity-Ctrans is the fastest
among the methods without GPUs. It is 8.67 and 3.07 times faster than PW-MILP and PW-sensitivity-LP,
K), where S∗
respectively. For quantitative evaluation, we deﬁne the empirical approximation ratio as φ(SK)/φ(S∗
K
is the optimal solution obtained by PW-MILP when |S| = K. Figure 3 shows the empirical approximation ratio

8

Figure 2: Partial Wasserstein divergence PW 2(Dapp, S ∪ Ddev) when data are sequentially selected and added to S
(left). Computational time, where colored areas indicate standard deviations (right).

Figure 3: Approximation ratios evaluated empirically for K = 15 and 50 random instances.

(Napp = Ndev = 30, K = 15) for 50 different random seeds. Both PW-*-LP achieve approximation ratios close to
one, whereas PW-*-ent and PW-sensitivity-Ctrans have slightly lower approximation ratios5.

5.2 Finding a missing category

For the quantitative evaluation of the missing pattern ﬁndings, we herein consider a scenario in which a category (i.e.,
label) is less included in the development dataset than in the application dataset6. We employ subsets of the MNIST
dataset [40], where the development dataset contains 0 labels at a rate of 0.5%, whereas all labels are included in the
application data in equal ratios (i.e., 10%). We randomly sampled 500 images from the validation split for each Dapp
and Ddev. We employ the squared L2 distance on the pixel space as the ground metric for our method. For baseline
methods, we employed LOF [4] novelty detection provided by scikit-learn [41] with default parameters. For the LOF,
we selected top-K data according to the anomaly scores. We also employed active learning approaches based on
entropies of the prediction [35, 36] and coreset with the k-center and k-center greedy [5, 42]. For the entropy-based
method, we train a LeNet-like neural network using the training split, and then select K data with high entropies of the
predictions from the application dataset. LOF and coreset algorithms are conducted on the pixel space.

We conducted 10 trials using different random seeds for the proposed algorithms and baselines, except for the
PW-greedy-* algorithms, because they took over 24h. Figure 4 presents a histogram of the selected labels when
K = 30, where x-axis corresponds to labels of selected data (i.e., 0 or not 0) and y-axis shows relative frequencies of
selected data. The proposed methods (i.e., PW-*) extract more data corresponding to the label 0 (0.71 for PW-MILP)
than the baselines. We can conclude that the proposed methods successfully selected data from the candidate dataset
Dcand(= Dapp) to ﬁll in the missing areas in the distribution.

5An important feature of PW-*-ent is that they can be executed without using any commercial solvers.
6Note that in real scenarios, missing patterns do not always correspond to labels.

9

0102030# selected data0.000.010.02Partial WassersteindivergencePartial Wasserstein divergence100200300400500# data (Napp=Ndev=Ncand)101102103Computational time [s]Computational TimePW-MIPPW-greedy-LPPW-greedy-entPW-sensitivity-LPPW-sensitivity-CtransPW-sensitivity-entPW-sensitivity-ent(CPU)randomPW-greedy-LPPW-greedy-entPW-sensitivity-LPPW-sensitivity-CtransPW-sensitivity-entrandom0.70.80.91.0Approximation ratioFigure 4: Relative frequency of missing category in selected data. The black bars denote the standard deviations.

5.3 Missing scene extraction in driving datasets

Finally, we demonstrate PWC in a realistic scenario, using driving scene images. We adopted two datasets, BDD100K [43]
and KITTI (Object Detection Evaluation 2012) [44] as the application and development datasets, respectively. The ma-
jor difference between these two datasets is that KITTI (Ddev) contains only daytime images, whereas BDD100k (Dapp)
contains both daytime and nighttime images. To reduce computational time, we randomly selected 1,500 data points
for the development dataset from the test split of the KITTI dataset and 3,000 data points for the application dataset
from the test split of BDD100k. To compute the transport costs between images, we calculated the squared L2 norms
between the feature vectors extracted by a pretrained ResNet with 50 layers [45] obtained from Torchvision [46]. Be-
fore inputting the images into the ResNet, each image was resized to a height of 224 pixels and then center-cropped to
a width of 224, followed by normalization. As baseline methods, we adopted the LOF [4] and coreset with the k-center
greedy [5]. Figure 5 presents the obtained top-3 images. One can see that PWC (PW-sensitivity-LP) selects the
major pattern (i.e., nighttime scenes) that is not included in the development data, whereas LOF and coreset mainly
extracts speciﬁc rare scenes (e.g., a truck crossing a street or out-of-focus scene). The coreset selects a single image of
nighttime scenes; however, we emphasize that providing multiple images is essential for the developers to understand
what patterns in the image (e.g., nighttime, type of cars, or roadside) are less included in the image.

The above results indicate that the PWC problem enables us to accelerate updating ML-based systems when the
distributions of application and development data are different because our method does not focus on isolated anomalies
but major missing patterns in the application data, as shown in Fig. 1 and Fig. 5. The ML workﬂow using our method
can efﬁciently ﬁnd such patterns and allow developers to incrementally evaluate and update the ML-based systems
(e.g., test the generalization performance, redevelop a new model, and designing a special subroutine for the pattern).
Identifying patterns that are not taken into account during development and addressing them individually can improve
the reliability and generalization ability of ML-based systems, such as image recognition in autonomous vehicles.

6 Conclusion

In this paper, we proposed the PWC, which ﬁlls in the gaps between development datasets and application datasets
based on partial Wasserstein divergence. We also proved the submodularity of the PWC, leading to a greedy algo-
rithm with the guaranteed approximation ratio. In addition, we proposed quasi-greedy algorithms based on sensitivity
analysis and derivatives of Sinkhorn iterations. Experimentally, we demonstrated that the proposed method covers the
major areas of development datasets with densities lower than those of the corresponding areas in application datasets.
The main limitation of the PWC is scalability; the space complexity of the dual simplex or Sinkhorn iteration is at
least O(Napp · Ndev), which might require approximation, e.g., stochastic optimizations [47], slicing [3] and neural
networks [48]). As a potential risk, if infrequent patterns in application datasets are as important as frequent patterns,
our proposed method may ignore some important cases, and it may be desirable to use our covering with methods
focusing on fairness.

10

Methods0.00.20.40.60.8Relative frequencyPW-MILPPW-sensitivity-LPPW-sensitivity-CtransPW-sensitivity-entk-centerk-center-greedyactive-entLOFrandom(a) Partial Wassersein covering

(b) LOF

(c) Coreset (k-center greedy)

Figure 5: Covering results when the application dataset (BDD100k) contains nighttime scenes, but the development
dataset (KITTI) does not. PWC (PW-sensitivity-LP) extracts the major difference between the two datasets (i.e.,
nighttime images), whereas LOF and coreset (k-center greedy) mainly extract some rare cases.

References

[1] Martin Arjovsky, Soumith Chintala, and L´eon Bottou. Wasserstein generative adversarial networks. In Doina Precup and
Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of
Machine Learning Research, pages 214–223. PMLR, 06–11 Aug 2017.

[2] David Alvarez-Melis and Nicolo Fusi. Geometric dataset distances via optimal transport.

In H. Larochelle, M. Ranzato,
R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems 33, volume 33, pages
21428–21439. Curran Associates, Inc., 2020.

[3] Nicolas Bonneel and David Coeurjolly. Spot: sliced partial optimal transport. ACM Transactions on Graphics (TOG), 38(4):1–

13, 2019.

[4] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J¨org Sander. Lof: identifying density-based local outliers. In

Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data, pages 93–104, 2000.

[5] Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set approach. In International

Conference on Learning Representations, 2018.

[6] Alessio Figalli. The optimal partial transport problem. Archive for rational mechanics and analysis, 195(2):533–560, 2010.

[7] Laetitia Chapel, Mokhtar Alaya, and Gilles Gasso. Partial optimal transport with applications on positive-unlabeled learning.

In Advances in Neural Information Processing Systems 33, pages 2903–2913, 2020.

[8] Jean-David Benamou, Guillaume Carlier, Marco Cuturi, Luca Nenna, and Gabriel Peyr´e. Iterative bregman projections for

regularized transportation problems. SIAM Journal on Scientiﬁc Computing, 37(2):A1111–A1138, 2015.

[9] Gabriel Peyr´e and Marco Cuturi. Computational optimal transport: With applications to data science. Foundations and

Trends® in Machine Learning, 11(5-6):355–607, 2019.

[10] Robert J Vanderbei. Linear programming, volume 3. Springer, 2015.

[11] George L Nemhauser, Laurence A Wolsey, and Marshall L Fisher. An analysis of approximations for maximizing submodular

set functions—i. Mathematical programming, 14(1):265–294, 1978.

11

[12] Amotz Bar-Noy and George Rabanca. Tight approximation bounds for the seminar assignment problem. In International

Workshop on Approximation and Online Algorithms, pages 170–182. Springer, 2016.

[13] Richard M Karp. Reducibility among combinatorial problems.

In Complexity of computer computations, pages 85–103.

Springer, 1972.

[14] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin,
Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K¨opf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style,
high-performance deep learning library. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alch´e-Buc,
Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8024–8035, 2019.

[15] J Arturo Olvera-L´opez, J Ariel Carrasco-Ochoa, J Francisco Mart´ınez-Trinidad, and Josef Kittler. A review of instance selec-

tion methods. Artiﬁcial Intelligence Review, 34(2):133–143, 2010.

[16] Peter Hart. The condensed nearest neighbor rule (corresp.). IEEE transactions on information theory, 14(3):515–516, 1968.

[17] GL Ritter. An algorithm for a selective nearest neighbor decision rule. IEEE Trans. Information Theory, 21(11):665–669,

1975.

[18] Chien-Hsing Chou, Bo-Han Kuo, and Fu Chang. The generalized condensed nearest neighbor rule as a data reduction method.

In Proceedings of the 18th International Conference on Pattern Recognition, volume 2, pages 556–559. IEEE, 2006.

[19] Dennis L Wilson. Asymptotic properties of nearest neighbor rules using edited data. IEEE Transactions on Systems, Man, and

Cybernetics, (3):408–421, 1972.

[20] Fernando V´azquez, J Salvador S´anchez, and Filiberto Pla. A stochastic approach to wilson’s editing algorithm. In Iberian

conference on pattern recognition and image analysis, pages 35–42. Springer, 2005.

[21] D Randall Wilson and Tony R Martinez. Reduction techniques for instance-based learning algorithms. Machine Learning,

38(3):257–286, 2000.

[22] Henry Brighton and Chris Mellish. Advances in instance selection for instance-based learning algorithms. Data Mining and

Knowledge Discovery, 6(2):153–172, 2002.

[23] Jos´e C Riquelme, Jes´us S Aguilar-Ruiz, and Miguel Toro. Finding representative patterns with ordered projections. Pattern

Recognition, 36(4):1009–1018, 2003.

[24] James C Bezdek and Ludmila I Kuncheva. Nearest prototype classiﬁer designs: An experimental study. International Journal

of Intelligent Systems, 16(12):1445–1473, 2001.

[25] Huan Liu and Hiroshi Motoda. On issues of instance selection. Data Mining and Knowledge Discovery, 6(2):115–130, 2002.

[26] Barbara Spillmann, Michel Neuhaus, Horst Bunke, El˙zbieta Pekalska, and Robert PW Duin. Transforming strings to vector
spaces using prototype selection. In Joint IAPR international workshops on statistical techniques in pattern recognition (SPR)
and structural and syntactic pattern recognition (SSPR), pages 287–296. Springer, 2006.

[27] Mohiuddin Ahmed. Data summarization: a survey. Knowledge and Information Systems, 58(2):249–273, 2019.

[28] Marko Mitrovic, Ehsan Kazemi, Morteza Zadimoghaddam, and Amin Karbasi. Data summarization at scale: A two-stage
submodular approach. In Proceedings of the 35th International Conference on Machine Learning, pages 3596–3605. PMLR,
2018.

[29] Hui Lin and Jeff Bilmes. A class of submodular functions for document summarization. In Proceedings of the 49th Annual

Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 510–520, 2011.

[30] Baharan Mirzasoleiman, Morteza Zadimoghaddam, and Amin Karbasi. Fast distributed submodular cover: Public-private data

summarization. In Advances in Neural Information Processing Systems 29, pages 3594–3602, 2016.

[31] Sebastian Tschiatschek, Rishabh K Iyer, Haochen Wei, and Jeff A Bilmes. Learning mixtures of submodular functions for
image collection summarization. In Advances in Neural Information Processing Systems 27, pages 1413–1421, 2014.

[32] Bernhard Sch¨olkopf, Robert C Williamson, Alexander J Smola, John Shawe-Taylor, John C Platt, et al. Support vector method
for novelty detection. In Advances in Neural Information Processing Systems 12, volume 12, pages 582–588. Citeseer, 1999.

[33] Ki Hyun Kim, Sangwoo Shim, Yongsub Lim, Jongseob Jeon, Jeongwoo Choi, Byungchan Kim, and Andre S Yoon. Rapp:
Novelty detection with reconstruction along projection pathway. In International Conference on Learning Representations,
2019.

[34] Jinwon An and Sungzoon Cho. Variational autoencoder based anomaly detection using reconstruction probability. Special

Lecture on IE, 2(1):1–18, 2015.

12

[35] Alex Holub, Pietro Perona, and Michael C Burl. Entropy-based active learning for object recognition. In 2008 IEEE Computer

Society Conference on Computer Vision and Pattern Recognition Workshops, pages 1–8. IEEE, 2008.

[36] Luiz FS Coletta, Moacir Ponti, Eduardo R Hruschka, Ayan Acharya, and Joydeep Ghosh. Combining clustering and active

learning for the detection and learning of new image classes. Neurocomputing, 358:150–165, 2019.

[37] Changjian Shui, Fan Zhou, Christian Gagn´e, and Boyu Wang. Deep active learning: Uniﬁed and principled method for query

and training. In International Conference on Artiﬁcial Intelligence and Statistics, pages 1308–1318. PMLR, 2020.

[38] Charles S ReVelle and Ralph W Swain. Central facilities location. Geographical analysis, 2(1):30–42, 1970.

[39] IBM ILOG CPLEX Optimization Studio. Cplex user’s manual. Vers, 12:207–258, 2013.

[40] Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010.

[41] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg,
J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python.
Journal of Machine Learning Research, 12:2825–2830, 2011.

[42] Olivier Bachem, Mario Lucic, and Andreas Krause. Practical coreset constructions for machine learning. arXiv preprint

arXiv:1703.06476, 2017.

[43] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht Madhavan, and Trevor Darrell.
Bdd100k: A diverse driving dataset for heterogeneous multitask learning. In IEEE/CVF Conference on Computer Vision and
Pattern Recognition, June 2020.

[44] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. International

Journal of Robotics Research, 2013.

[45] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of

the IEEE Conference on Computer Vision and Pattern Recognition, pages 770–778, 2016.

[46] S´ebastien Marcel and Yann Rodriguez. Torchvision the machine-vision package of torch. In Proceedings of the 18th ACM

International Conference on Multimedia, pages 1485–1488, 2010.

[47] Genevay Aude, Marco Cuturi, Gabriel Peyr´e, and Francis Bach. Stochastic optimization for large-scale optimal transport.

arXiv preprint arXiv:1605.08527, 2016.

[48] Yujia Xie, Minshuo Chen, Haoming Jiang, Tuo Zhao, and Hongyuan Zha. On scalable and efﬁcient computation of large scale

optimal transport. In International Conference on Machine Learning, pages 6882–6892. PMLR, 2019.

[49] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in Neural Information Process-

ing Systems 26, pages 2292–2300, 2013.

13

A Reformulation of Eq.(2) as a MILP

The MILP formulation of Eq.(2) with marginal masses a and b(S) can be deﬁned based on the partial Wasserstein
divergence in Eq.(1), i.e., the marginal mass b(S) depends on which subset S ⊆ Dcand is selected. Precisely, the
marginal mass of Dapp side (i.e., source) is ai = 1
. The mass of Dcand ∪ Ddev side (i.e., target) is now Eq.(3),
meaning that bj(S) represents the mass can be transported into j. This parameterization enables us to deﬁne Eq.(2) as
MILP.

Napp

Let yj ∈ {0, 1}, j ∈ [[Ncand]] be the 0-1 decision variable. The value yj = 1 means the data s(j) ∈ Dcand is
included in S (i.e., s(j) ∈ S), and yj = 0 means s(j) /∈ S. Continuous decision variables Pij ∈ [0, 1] represent
transportation masses between Dapp and Dcand ∪ Ddev for each data pair i ∈ [[Napp]] and j ∈ [[Ncand + Ndev]]. The
minimization problem among two sets with marginal masses a and b(S) is written as the following MILP problem:

min

P∈[0,1]Napp×(Ncand+Ndev ),{yj }

(cid:104)P, C(cid:105)

Ncand
j=1

1
Napp

yj
Ndev

1
Ndev

for all i ∈ [[Napp]]

for all 1 ≤ j ≤ Napp

for all Napp + 1 ≤ j ≤ Napp + Ndev

subject to

(cid:88)

Pij =

j∈[[Ncand+Ndev]]

(cid:88)

Pij ≤

i∈[[Napp]]
(cid:88)

i∈[[Napp]]
(cid:88)

Pij ≤

yj ≤ K

j∈[[Ncand]]

yj ∈ {0, 1}
Pij ∈ [0, 1]

for all j ∈ [[Ncand]]
for all i ∈ [[Napp]], j ∈ [[Ncand + Ndev]]

(8)

(9)

(10)

(11)

(12)

(13)

(14)

Constraints (9)-(11) ensures the requirements of the partial Wasserstein divergence of Eq. (1). Note that Constraints

(10) and (12) represent that we select up to K data from Dcand. Constraints (13)-(14) deﬁne decision variables.

B Pseudocodes of proposed algorithms

This section explains the evaluated methods in the main text; the greedy-based algorithm PW-greedy-*, sensitivity-
based ones PW-sensitivity-*, and two baselines random.

B.1 Greedy-based algorithms

We explain the proposed greedy-based algorithm PW-greedy-LP, which is based on LP formulation of the partial
Wasserstein divergence. The pseudocode is given below. For each insertion iteration up to K, we compute all the partial
Wasserstein divergences when s(j) for j ∈ [[Ncand]] only if s(j) /∈ S. We select a new data s(j) if this addition maximizes
the selection objective (i.e., minimize the distance among two datasets).

14

Algorithm 2: Greedy data selection for the PWC problem (PW-greedy-LP)

1: Input: Dapp, Ddev, Dcand
2: Output: S
3: S ← {}
4: while |S| < K do
5:

Compute dj := PW 2(Dapp, S ∪ {s(j)} ∪ Ddev) for each j ∈ [[Ncand]] only if s(j) /∈ S

(cid:46) Iteration up to K data

6:

Select j(cid:63) = arg minj∈[[Ncand]],s(j) /∈S dj
S ← S ∪ {s(j)}

7:
8: end while

(cid:46) O(Ncand · CPW ) in each iteration
(cid:46) (1). each divergence requires O(CPW )
(cid:46) (2). Ncand times to compute {dj}

Note that PW-greedy-ent is obtained by just replacing the LP part of computing PW 2(·, ·) with the (partial)

Sinkhorn iterations of computing the entropy regularized Partial Wasserstein divergence PW 2

(cid:15) (·, ·).

B.2 C-transform-based sensitivity algorithm

Developed sensitivity-based algorithms (e.g., PW-sensitivity-LP) reduces computational costs to evaluate all
the divergence dj in greedy-based algorithms to estimate the importance of j-th data by their sensitivity values. In
the LP-based method, such values are obtained by sensitivity analyses functions included in the LP solver. Further,
gradients of the partial Wasserstein divergences can be applicable when the entropy-regularized PWC is used. Note
that these methods are explained in the main text (Algorithm 2).

Here, we give further details particularly when the C-transform is adopted to accelerate sensitivity-based algo-
rithms. The pseudocode is given below. For each iteration up to K, we compute the dual optimal solutions f (cid:63), g(cid:63) with
LP-formulation of system size Ncand × (|S| + Ndev), instead of using the full size Ncand × (Ndev + Ndev) for practical
acceleration. Once dual optimal solutions are computed by a solver, we could estimate the effect of adding a new data
s(j) using the computed solutions as they are also feasible even if s(j) is added to the next step LP formulation. The
effect of adding s(j) can be estimated by C-transform as we discussed (see Chapter 3 in [9] as well) and this esti-
mation is apparently possible in constant time (see Line 6). In conclusion, this C-transform-based algorithm has the
same computational complexity with those of PW-sensitivity-LP, this computational trick accelerates the total
computation times as we evaluated in experiments.

Algorithm 3: Data
selection
(PW-sensitivity-Ctrans)

for

the

PWC problem with

sensitivity

analysis

and C-transform

1: Input: Dapp, Ddev, Dcand
2: Output: S
3: S ← {}
4: while |S| < K do
5:

where f (cid:63) ∈ RNapp, g(cid:63) ∈ R|S|+Ndev

Compute PW 2(Dapp, S ∪Ddev) with LP of system size Ncand ×(|S|+Ndev), and obtain the optimal dual f (cid:63), g(cid:63),

6:

7:

gC
j

Compute C-transforms: For each j ∈ [[Ncand]] if s(j), we evaluate the effect of adding s(j) with the value
:= min{0, mini∈[[Ndev]] Ci,j − f ∗
j(cid:63) = arg minj∈[[Ncand]],s(j) /∈S gC
j
S ← S ∪ {s(j(cid:63))}

i }.

8:
9: end while

15

B.3 Sinkhorn algorithms

The entropy regularized Wasserstein divergence [49] has attracted much attention because it can be computed in high
speed on GPUs by some simple matrix multiplier, named the Sinkhorn iterations. Entropy regularized Wasserstein
distance is formally deﬁned as

W 2

(cid:15) (X, Y ) = min

P∈U (a,b)

(cid:104)P, C(cid:105) − (cid:15)H(P),

U (a, b) = {P ∈ [0, 1]m×n | P1n = a, P(cid:62)1m = b},

H(P) = −

m
(cid:88)

n
(cid:88)

i=1

j=1

Pij(log Pij − 1),

(15)

(16)

(17)

where (cid:15) > 0. An important property of the Sinkhorn iterations is differentiable with respect to the inputs and con-
straints, for example, ∂W 2
(cid:15) (X, Y )/∂bj can be obtained by the automatic differentiation tech-
niques. It is noteworthy that we add a small value to bj for the computation of the derivative. We can also employ
Sinkhorn-like iterations on GPUs to solve the entropy regularized partial Wasserstein divergences [8].

(cid:15) (X, Y )/∂X and ∂W 2

The (partial) derivatives ∂W 2

(cid:15) . For ex-
ample in PW-greedy-LP, we can replace PW 2 with PW 2
(cid:15) , leading another proposed method PW-greedy-ent. Further,
instead of selecting j(cid:63) = arg minj∈[[Ncand]],s(j) /∈S dj, where dj := PW 2(Dapp, S ∪ {s(j)} ∪ Ddev), we could select j(cid:63)
that minimizes the partial derivatives of PW 2, leading another proposed method PW-sensitivity-ent.

(cid:15) (X, Y )/∂bj can be computed after evaluating PW 2

(cid:15) (X, Y )/∂X and ∂W 2

C Details of Experiments

C.1 LeNet-like network

def __init__(self):

class Net(nn.Module):

super(Net, self).__init__()
self.conv1 = nn.Conv2d(1, 32, 3) # 28x28x32 -> 26x26x32
self.conv2 = nn.Conv2d(32, 64, 3) # 26x26x64 -> 24x24x64
self.pool = nn.MaxPool2d(2, 2) # 24x24x64 -> 12x12x64
self.dropout1 = nn.Dropout2d()
self.fc1 = nn.Linear(12 * 12 * 64, 128)
self.dropout2 = nn.Dropout2d()
self.fc2 = nn.Linear(128, 10)

We show the PyTorch code of LeNet-like network used for the entropy-based active learning approach in MNIST
experiment. The network is trained by SGD optimizer, where learning rate is 0.001 and momentum is 0.9 to minimize
the cross entropy loss function during 20 epochs.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20

x = F.relu(self.conv1(x))
x = self.pool(F.relu(self.conv2(x)))
x = self.dropout1(x)
x = x.view(-1, 12 * 12 * 64)
x = F.relu(self.fc1(x))
x = self.dropout2(x)
x = self.fc2(x)
return x

def forward(self, x):

C.2 Finding a completely missing category

In Sec. 5.2, we consider the situation, where the frequency of a label in the development dataset is very rare compared
to that in the application dataset. We herein consider another situation, where a label is completely missing in the
development dataset. Fig. 6 illustrates the relative frequency of missing category in selected data. In this situation,
partial Wasserstein covering methods (i.e., PW-(cid:63)) outperform the baselines including the active learning methods (i.e.,
k-center, k-center-greedy, and active-ent) and the anomaly detection method (i.e., LOF).

16

Figure 6: Relative frequency of completely missing category in selected data (MNIST dataset). The black bars denote
the standard deviations.

Figure 7: Relative frequency of missing category in selected data (CIFAR-10 dataset). The black bars denote the
standard deviations.

C.3 Finding a missing category on CIFAR-10

We perform the missing category ﬁnding experiment using CIFAR-10 dataset. We herein consider airplane label (the
ﬁrst label in the CIFAR-10) as the missing label. As the ground metrics, we employ L2-norm between the feature
vectors extracted by a neural network. We obtain the neural network from torchhub7, and train using the subset of the
training data of CIFAR-10 from scratch using the Adam optimizer8 during 10 epochs. For the subset of the training
data, we remove the missing category data so that the percentage of the missing category would be the same as the
development data (i.e., 0.5%). The other settings are the same as in Sec.5.2. Fig. 7 illustrates the results indicating that
our covering algorithms outperform the baseline, while PW-sensitivity-ent failed to ﬁnd the missing category.
This could be due to the Sinkhorn iterations not sufﬁciently converged.

C.4 Missing scene extraction in real driving datasets varying number of data

In the Sec. 5.3, we showed the results of 1,500 data for the development dataset, and 3,000 data for the application
dataset. We herein show some additional results varying number of data. We also show top-8 images for each. These
result show that partial Wasserstein covering successfully extract the major differences (i.e., night scenes).

D Discussion

D.1 General partial OT and unbalanced OT

We only consider the formulation of the partial Wasserstein divergence (Eq. 1), while the other formulations of the op-
timal transport can be considered. We note that the general partial OT [6] for discrete distributions is actually the same
as Eq.(1) when a(cid:62)1n ≤ b(cid:62)1m. A covering problem using the unbalanced OT (i.e., minP (cid:104)P, C(cid:105) + τ1D1(P1m|a) +
τ2D2(P(cid:62)1n|b)) can be considered, while the submodularity may not be hold in this case. We also emphasize that we
need to adjust the strength of the penalty term (i.e., τ1, τ2) to select data points within the budgets.

7https://github.com/chenyaofo/pytorch-cifar-models
8We employed the default parameters in PyTorch for the Adam optimizer.

17

Methods0.00.20.40.60.8Relative frequencyPW-MILPPW-sensitivity-LPPW-sensitivity-CtransPW-sensitivity-entk-centerk-center-greedyactive-entLOFrandomMethods0.00.10.20.30.40.50.6Relative frequencyPW-MILPPW-sensitivity-LPPW-sensitivity-CtransPW-sensitivity-entk-centerk-center-greedyactive-entLOFrandom(a) Partial Wassersein covering (PW-sensitivity-LP)

(b) LOF

(c) Coreset (k-center greedy)

(d) Random

Figure 8: Development dataset 500 and application dataset 1,000

(a) Partial Wassersein covering (PW-sensitivity-LP)

(b) LOF

(c) Coreset (k-center greedy)

(d) Random

Figure 9: Development dataset 1,000 application dataset 2,000

18

(a) Partial Wassersein covering (PW-sensitivity-LP)

(b) LOF

(c) Coreset (k-center greedy)

(d) Random

Figure 10: Development dataset 1,500 application dataset 3,000

19

