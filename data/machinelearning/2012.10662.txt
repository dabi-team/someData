1
2
0
2

r
a

M
8
1

]
E
S
.
s
c
[

2
v
2
6
6
0
1
.
2
1
0
2
:
v
i
X
r
a

Configuring Test Generators using Bug Reports: A Case Study of
GCC Compiler and Csmith

Md Rafiqul Islam Rabin
University of Houston
Houston, TX, USA
mrabin@uh.edu

Mohammad Amin Alipour
University of Houston
Houston, TX, USA
maalipour@uh.edu

ABSTRACT
The correctness of compilers is instrumental in the safety and relia-
bility of other software systems, as bugs in compilers can produce
executables that do not reflect the intent of programmers. Such
errors are difficult to identify and debug. Random test program
generators are commonly used in testing compilers, and they have
been effective in uncovering bugs. However, the problem of guiding
these test generators to produce test programs that are more likely
to find bugs remains challenging.

In this paper, we use the code snippets in the bug reports to
guide the test generation. The main idea of this work is to extract
insights from the bug reports about the language features that are
more prone to inadequate implementation and using the insights to
guide the test generators. We use the GCC C compiler to evaluate
the effectiveness of this approach. In particular, we first cluster the
test programs in the GCC bugs reports based on their features. We
then use the centroids of the clusters to compute configurations for
Csmith, a popular test generator for C compilers. We evaluated this
approach on eight versions of GCC and found that our approach
provides higher coverage and triggers more miscompilation failures
than the state-of-the-art test generation techniques for GCC.

CCS CONCEPTS
â€¢ Software and its engineering â†’ Software testing and de-
bugging; Compilers.

KEYWORDS
testing; clustering; compiler; evaluation

ACM Reference Format:
Md Rafiqul Islam Rabin and Mohammad Amin Alipour. 2021. Configuring
Test Generators using Bug Reports: A Case Study of GCC Compiler and
Csmith. In Proceedings of The 36th ACM/SIGAPP Symposium on Applied
Computing - Software Verification and Testing Track (SAC-SVTâ€™21). ACM,
New York, NY, USA, 10 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn

1 INTRODUCTION
Compilers are key parts of software development infrastructure
that translate high-level programs understandable by developers to

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SAC-SVTâ€™21, March, 2021, Virtual, Republic of Korea
Â© 2021 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

low-level programs that machines can execute. Developers rely on
compilers to build, profile, and debug the programs; therefore any
bugs in compilers threaten the integrity of software development
process. For this reason, researchers have been focused on testing
compilers to uncover bugs, [1, 9, 27].

Testing compilers is particularly difficult due to their sheer size
and complexity. Testing such massive, sophisticated systems is a
non-trivial task, and researchers and developers still can find bugs
in them [4]. Random testing, also know as fuzzing, is a common,
lightweight approach for generating test programs for compilers
[20]. Many programming languages have specialized random test
generators that use the language grammar and language-specific
heuristics to produce test programs in those languages; Csmith [27]
for C, jsFunFuzz [25] for JavaScript, and Go-Fuzz [11] for Go are
good examples of such test generators that have been successful in
helping developers to find hundreds of bugs in various compilers
and interpreters.

Mature test generators, like Csmith, are configurable and allow
developers to guide the test generation through configurations.
Prior studies have proposed different techniques to exploit con-
figurations to generate more effective test programs. For example,
swarm testing [14] configures Csmith such that test programs con-
tain a random subset of the language features, focused random
testing [1] uses statistical analysis of the programs generated by
Csmith and their coverage to suggest Csmith configurations that
target specific blocks in GCC, or HiCOND [5] analyzes on the his-
torical test programs to create Csmith configurations that are more
likely to find bugs.

In this paper, we propose K-Config, an approach that uses the
bugs reported by users to propose configurations for Csmith. More
specifically, K-Config clusters the programs in the bug reports
by X-Means algorithm [23] and uses the centroids of clusters as
a basis for proposing configurations for Csmith. We implemented
K-Config for GCC C compiler and Csmith test generator. We col-
lected 5, 960 failing test programs from the GCC Bugzilla 1 and
Testsuite 2. We performed an extensive experiment to evaluate the
effectiveness of K-Config on eight versions of GCC. We compared
it with swarm testing [14], HiCOND [5] and the default configura-
tion of Csmith [27].

To the best of our knowledge, the other comparable techniques
do not use the information available in the bug reports in the gen-
eration of new test programs. K-Config is the first attempt in that
direction [24]. The reasoning it uses is that the code snippets in
the bug reports that triggered bugs earlier are more likely to be of

1https://gcc.gnu.org/bugzilla/
2https://github.com/gcc-mirror/gcc/blob/master/gcc/testsuite/

 
 
 
 
 
 
SAC-SVTâ€™21, March, 2021, Virtual, Republic of Korea

Md Rafiqul Islam Rabin and Mohammad Amin Alipour

Figure 1: The workflow of test generation in K-Config approach.

test program. In other words, all initial test programs must be gen-
erated by Csmith. Therefore, the test programs that are not created
by Csmith, like the ones available in the bug reports, cannot be used
by HiCOND. Second, the particle swarm optimization technique
used in HiCOND requires a large initial test suite of failing and
passing test programs generated by Csmith. Unfortunately, as GCC
matures, it becomes harder for Csmith to trigger failures in GCC
hence, which negatively impacts HiCOND applicability on newer,
more stable versions of GCC.
Contributions. This paper makes the following contributions.

â€¢ We propose an approach for computing configuration of the
Csmith test generator by processing the code snippets in the
bug reports.

â€¢ We perform a large-scale case study on the effectiveness of
our approach on eight versions of GCCâ€”a large, complex C
compiler.

Paper Organization. The paper is organized as follows. Section 2
describes the proposed K-Config approach. Section 3 and Section 4
describe the experimental setting and evaluation setup. Section 5
presents the results of the experiments. Section 6 discusses the
results. Section 7 presents the related works. Section 8 describes
the main threats to validity. Finally, section 9 concludes the paper.

2 APPROACH
Figure 1 depicts the overall workflow of our approach that consti-
tutes the following three main steps: (1) extracting configurable
test features from the initial test suite, (2) using the test features
to cluster test programs into similar groups, and (3) generating
configurations based on the centroid of clusters. We describe each
step in the rest of this section.

2.1 Extracting configurable test features
Test features. We use the term test features, or features for short,
similar to Groce et al. [13]: â€œTest features are basic compositional
units used to describe what a test does (and does not) involve.â€ Test
features are essentially building blocks of test cases. For example, in
testing libraries, the API calls are the functions; in grammar-based
testing, the production rules in the grammar can be features; and,
in testing compilers or interpreters, programming structures can
be test features. In Csmith, test features are the same as C language
features, therefore, in the rest of the paper, we use terms test feature,
language feature, and feature interchangeably.
Configurable test generator. A test generator essentially com-
poses new test programs with or based on the test features. Some

Figure 2: An example of bug report (#11492) in GCC Bugzilla.

interest to developers. We believe bug reports contain insights that
can be extracted to improve the test generation for compilers.

The results of our experiments suggest that K-Config could find
up to 96 miscompilations, while swarm finds up to 70 miscompila-
tions, and the default configurations couldnâ€™t find any miscompi-
lations. Moreover, the coverage of K-Config is higher than other
techniques in all versions of GCC compilers in our experiment. Our
results also show that the coverage of regression test programs is
still higher than any automatically generated test suite. It suggests
that despite the advances in generating test programs for compilers,
there is still a wide gap between the effectiveness of the generated
test programs, and the small, regression test suites.

The intuition behind K-Config is that the language features
that have participated in previous bugs are likely to participate in
new bugs as well. Similar intuition has been used in LangFuzz [15]
and HiCOND [5]. LangFuzz transplants fragments of failing test
programs to generate new test programs for JavaScript. Although
similar in the intuition, instead of manipulating existing test pro-
grams, K-Config uses configurations to guide the test generator
to generate new test programs. HiCOND uses the configuration of
Csmith generated test programs to optimize and propose diverse
test configurations. While K-Config and HiCOND both are similar
in the sense that they use an initial test suite to infer effective con-
figurations for Csmith, they differ in two main ways. First, HiCOND
requires the configuration of the Csmith in the generation of each

Configuring Test Generators using Bug Reports: A Case Study of GCC Compiler and Csmith

SAC-SVTâ€™21, March, 2021, Virtual, Republic of Korea

Figure 3: Number of test programs in Seed test suite for each feature.

test generators allow users to use configuration files or command-
line options to customize test generation by modifying the test
generation parameters. We call such test generators configurable.
Csmith [27] is a configurable test generator for C compilers. Csmith
then uses these parameters to decide which features to include in
the test programs.
Controllable test features in Csmith. Csmith allows choosing
the C programming constructs to be included in the test programs
through command-line options. The order and number of the con-
structs are, however, chosen randomly and developers cannot con-
trol themâ€”mainly because Csmith is a random generator that uses
grammar to generate test programs. As of Csmith version 2.3.0 3,
the Csmith provides 32 configurable features in the form of â€“ft
and â€“no-ft command-line options, where ğ‘“ ğ‘¡ denotes a feature,
and â€“ft indicates inclusion and â€“no-ft indicates exclusion of ğ‘“ ğ‘¡
in the generation of the test programs. Figure 3 shows the list of
available controllable features in Csmith. These features describe a
wide range of language features from programming constructs in C
language, e.g., compound_assignment for compound assignments,
and float for floating-point data types. Different features may im-
pact the behavior of the compiler under test in different ways. For
example, Groce et al. [13] observed that pointer manipulation in a
test program can suppress certain class of bugs in the C compilers,
as most compilers would conservatively avoid certain optimizations
for pointers in the input program, consequently, masking potential
bugs in those optimizations.
Extracting features. In the context of this paper, we only consider
test programs that have been reported in the GCC bug reports.

3https://embed.cs.utah.edu/csmith/

Figure 2 shows an example of bug report (#11492) in GCC Bugzilla 4.
We use the number of occurrences of features in each test program
to create a feature vector for the test program. For this, we use
ClangAST 5 to extract the abstract syntax tree (AST) of the programs
in the initial failing test suite. Feature vectors are normalized to
the range of [0, 1] using min-max normalization [22], i.e., ğ‘§ğ‘– ğ‘— =
ğ‘¥ğ‘– ğ‘— âˆ’ min(ğ‘¥.ğ‘— )
, where ğ‘¥ğ‘– ğ‘— is the value of the ğ‘—ğ‘¡â„ feature of the
max(ğ‘¥.ğ‘— ) âˆ’ min(ğ‘¥.ğ‘— )
ğ‘–ğ‘¡â„ test program ğ‘¥ğ‘– = (ğ‘¥ğ‘–1, ..., ğ‘¥ğ‘–ğ‘›), ğ‘¥.ğ‘— is the list of ğ‘—ğ‘¡â„ feature from
all test programs, and ğ‘§ğ‘– ğ‘— is the corresponding min-max normalized
value of ğ‘¥ğ‘– ğ‘— .

2.2 Clustering test programs
To identify the clusters, we use X-Means clustering algorithm [23]
that estimates the optimal number of clusters in the underlying
distribution of data. The X-Means clustering is an unsupervised
machine learning algorithm that performs clustering of unlabeled
data without the need for presetting the number of clusters. Each
cluster is represented by a centroid that has a minimum distance to
the data points of the cluster. Since the feature vectors contain the
value in the range of [0, 1], the values in the centroids would be a
number between 0 and 1 as well.

2.3 Generating test programs
Our implementation of K-Config uses the centroid in X-Means
clustering [23] to suggest configurations for the test generator.
To generate configurations, we use the corresponding value of a
feature in the cluster as the probability of including the feature

4https://gcc.gnu.org/bugzilla/show_bug.cgi?id=11492
5https://clang.llvm.org/docs/IntroductionToTheClangAST.html

SAC-SVTâ€™21, March, 2021, Virtual, Republic of Korea

Md Rafiqul Islam Rabin and Mohammad Amin Alipour

in a test program. The feature is more dominant in the test pro-
grams if the corresponding value of a feature is closer to 1, and
conversely, if the corresponding value is closer to 0, it denotes that
the feature is less prevalent in the test programs. Therefore, in this
approach, for each cluster, we create a configuration wherein the
probability of inclusion of a feature in the test program is equal to
its corresponding value in the centroid of that cluster. For instance,
suppose the centroid of a cluster is (0.1, 0.7) for features f1 and f2,
respectively, the configuration generation algorithm in K-Config,
whenever called, it includes â€“f1 with probability 0.1 and â€“no-f1
with probability 0.9, similarly it would â€“f2 with 0.7 probability
and â€“no-f2 with 0.3 probability. It is in contrast with the swarm
testing [14] that uses the simplest form of fair coin-toss probability
(i.e., 0.5) for the inclusion of a feature.

Algorithm 1 describes the process of generating new test
programs using K-Config in more detail. Given a testing bud-
get ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ğµğ‘¢ğ‘‘ğ‘”ğ‘’ğ‘¡ and a set of centroids ğ¶ğ‘†, the algorithm calls
ğ¶ğ‘œğ‘›ğ‘“ ğ‘–ğ‘”ğºğ‘’ğ‘› in round-robin fashion until the test budget expires.
Procedure ğ¶ğ‘œğ‘›ğ‘“ ğ‘–ğ‘”ğºğ‘’ğ‘› takes a centroid ğ¶ âˆˆ ğ¶ğ‘† and generates a
new configuration. In generating a new configuration, ğ¶ğ‘œğ‘›ğ‘“ ğ‘–ğ‘”ğºğ‘’ğ‘›
chooses to include feature ğ‘“ğ‘– with a random probability ğ‘ğ‘– where ğ‘“ğ‘–
is represented by the element ğ‘ğ‘– in the centroid ğ¶. Finally, ğ‘‡ ğ‘† will
have all the generated failure-inducing test programs.

Algorithm 1: K-Config

ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ğµğ‘¢ğ‘‘ğ‘”ğ‘’ğ‘¡ â† Testing budget (i.e. 10,000 test count);
ğ¶ğ‘† â† Set of centroids;
ğ‘‡ ğ‘† â† {};

while ğ‘ ğ‘ğ‘’ğ‘›ğ‘¡ğµğ‘¢ğ‘‘ğ‘”ğ‘’ğ‘¡ â‰¤ ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ğµğ‘¢ğ‘‘ğ‘”ğ‘’ğ‘¡ do

forall centroid ğ¶ âˆˆ ğ¶ğ‘† do

ğ‘ğ‘œğ‘›ğ‘“ ğ‘–ğ‘” â† ğ¶ğ‘œğ‘›ğ‘“ ğ‘–ğ‘”ğºğ‘’ğ‘›(ğ¶)
ğ‘¡ğ‘’ğ‘ ğ‘¡ â† ğ¶ğ‘ ğ‘šğ‘–ğ‘¡â„(ğ‘ğ‘œğ‘›ğ‘“ ğ‘–ğ‘”)
if doesFail(test,GCC) then

ğ‘‡ ğ‘† â† ğ‘‡ ğ‘† âˆª ğ‘¡ğ‘’ğ‘ ğ‘¡

end

end

end

Function ConfigGen(ğ¶):
ğ‘“ ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ğ‘  â† âˆ…
forall value ğ‘ âˆˆ ğ¶ do

ğ‘Ÿğ‘ğ‘›ğ‘‘ğ‘ƒğ‘Ÿğ‘œğ‘ â† [0 : 1]
if randProb â‰¤ c then
ğ‘“ ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ğ‘ .ğ‘ğ‘¢ğ‘¡ (1)

else

ğ‘“ ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ğ‘ .ğ‘ğ‘¢ğ‘¡ (0)

end

end
return

3 EXPERIMENTAL SETTING
This section discusses the experimental setting we used for the
K-Config approach.
Reference test suite. A reference test suite is a collection of test
programs that exhibit interesting behaviors, e.g., high code coverage
or triggering failures. We use the GCC regression test suite that
has 5, 960 parsable C code snippets. These code snippets have been
collected from the confirmed bug reports in GCC Bugzilla 1 and
Testsuite 2. The code snippets in the bug reports are small and
usually do not include main functions. We call this test suite Seed
henceforth. We, hereby, use terms regression test suite and Seed
test suite interchangeably.

Figure 3 shows the number of test programs in Seed test suite
for each feature. It shows that the distribution of test features in
failing test programs is not uniform. Features such as comma op-
erators, global variables, and pointers occurred more frequently
than features such as unary plus operator (+), safe math, or int8. We
observe that all Csmith configurable test features appear in one or
more failing test programs in the regression test suite. Each feature
has been present in 6 to 3871 test programs. Note that it is difficult
to identify the role of individual features in the failures in the large,
complex systems such as GCC, without substantial simplification
of the test [29] and close inspection of the program execution.
Test generation tool. We use Csmith [27] that is an open-source
automatic test generation tool for C compilers. Given a set of C
language features as options, Csmith can generate a random C
program that contains those features. The programs generated by
Csmith are closed; that is, all variables are initialized in the source
code by Csmith and they do not require any external inputs for the
execution. We use Csmith 2.3.0 3 in our experiments.
Test generation techniques. We compare the effectiveness of
configurations proposed by K-Config with the effectiveness of
configurations in three comparable techniques: (1) Csmith default
configuration, (2) swarm configurations, and (3) HiCOND configura-
tions. In the default configuration, we use the default configuration
of Csmith wherein all C language features are enabled by default.
Swarm configurations are configurations that the Csmith features
are enabled by a fair coin-toss as in [14]. HiCOND configurations
are created by particle swarm optimization as in [5] that attempts
to optimize the effectiveness of Csmith by systematically exploring
the bug-finding capability of configurations in Csmith.
Configuration generation strategies. We evaluate K-Config
with two configuration generation strategies: round-robin, and
weighted. In the round-robin strategy, centroids of the clusters are
used in a round-robin fashion to generate test programs. Round-
robin strategy ignores the size of clusters. The weighted K-Config
strategy is similar to the round-robin, except the number of test
programs generated by each centroid is proportional to the size
of the cluster that the centroid represents. In weighted K-Config,
larger clusters will have more test programs generated.
GCC versions under test. We use eight versions of GCC to eval-
uate the effectiveness of the K-Config approach: GCC 4.3.0, 4.8.2,
5.4.0, 6.1.0, 7.1.0, 8.1.0, 9.1.0, and GCC trunk (as of 09/03/2019).
The official releases are mature and have been widely in use for
building various programs and operating systems, while the trunk

Configuring Test Generators using Bug Reports: A Case Study of GCC Compiler and Csmith

SAC-SVTâ€™21, March, 2021, Virtual, Republic of Korea

No Optimization (-O0) High Optimization (-O3)

Failure?

Compiler crashes

Compiler crashes

Compiler crashes

Compiler doesnâ€™t crash

Compiler doesnâ€™t crash

Compiler crashes

Outputs are identical for different optimization

Outputs are different for different optimization

False

True

True

False

True

Table 1: Possible failures in the experiment.

Figure 4: Differential testing in optimization level.

version contains the newest experimental features and, as a result,
is not as stable as the official releases.

4 EVALUATION SETUP
This section discusses the evaluation setup we used for the K-
Config approach.
Test oracles. The common practice for testing compilers is dif-
ferential testing [19, 27]. That is, a test program is compiled and
executed by two or more versions of compilers, or two or more
optimization levels, and the results are compared. The differential
test oracle specifies the result of the output of the compiled pro-
grams by all compilers and optimization levels must be the same.
Undefined behaviors [27] in the C language can complicate this
process, however, Csmith does the best effort to avoid the undefined
behaviors in C.

We use differential testing to evaluate the behavior of the com-
pilers on the test programs. Figure 4 shows the differential testing
in optimization level. More specifically, we compile a program with
different levels of optimization and compare the output of the exe-
cuted programs. Due to the large scale of the experiment, and to
optimize the experimentation time, we only use the lowest -O0 and
highest -O3 optimization levels. We do not include the intermedi-
ate optimization levels -O1 and -O2, as most optimization passes
that a compilation of a program can invoke can be captured in -O3.
Note that although almost all optimization passes are enabled in
-O3, there may be cases that the interactions of viable, competing
optimizations may put compiler in a position to choose ones over
another one, hence not exercising some of the passes that would
have been otherwise exercised in -O1 or -O2 levels. Our experiment

does not capture such cases, and we speculate that such cases would
be rare and hence negligible.
Failure types. We expect three types of compiler failures in the
compilation of a program: (1) Crash when compiler terminates the
compilation abruptly with a crash report on screen, (2) Timeout
when compiler gets into non-termination on the compilation of a
program, and (3) Miscompilation when the executable generated by
the compiler produced different results on different optimizations.
We check the behavior of the compiler (i.e., exit code) to identify
the crashes. We use 10 seconds timeout in compiling test programs.
We identify miscompilation by comparing the output of executables
generated by the compiler on different optimization levels.
Experiment parameters. Overall we evaluate five configuration
generation for Csmith: Default, Swarm, HiCOND, K-Config-
Round-Robin and K-Config-Weighted. Default denotes the de-
fault configuration that includes all the features by default. Swarm
represents the swarm testing configuration where the probability
of inclusion of a feature in a test program is 0.5, same as the fair
coin-toss approach [14]. HiCOND denotes the test configurations
computed from the history-guided approach proposed by Chen et
al. [5]. In K-Config, this probability is equal to the corresponding
value in the centroid of the cluster. For example, if the correspond-
ing value for volatile in the centroid of a cluster is 0.2, there is
20% chance that K-Config to use â€“volatile and 80% chance that
it includes â€“no-volatile in the configuration. K-Config-Round-
Robin adopts the round-robin strategy with uniform weights for all
clusters, while K-Config-Weighted uses a weighted strategy based
on the size of clusters. Note that we do not evaluate the regression
test suite, as only a few of its test programs have main functions
that can generate an executable.
Clustering parameters. We use the implementation of X-Means
algorithm in pyclustring data mining library [21] with default
hyperparameters. We use the default Bayesian information crite-
rion (BIC) [10] for the splitting type to approximate the number
of clusters. The stop condition for each iteration is 0.025; the algo-
rithm stops processing whenever the maximum value of change
in centers of clusters is less than this value. Moreover, the default
distance metric used is the sum of squared errors (SSE) [16] which
is the distance between data points and its centroid.
Test budget. We create 10, 000 test programs for each approach
and evaluate the effectiveness of the test (i.e., bug finding, coverage,
and the number of distinct bugs in each approach). To account for
the random effects in the approaches, we run each experiment three
times, average them, and round the values to the closest integer. We
use 10 seconds as the timeout for GCC to compile a test program. A
small experiment with 6 hours test budget and 30 seconds timeout
yielded similar observations. Overall, we evaluated the techniques
for over 1, 500 hours (approximately 62 days).

5 RESULTS
In this section, we present the results of our experiments. Table
2 depicts the number of failures in each experiment. In this table,
the column C0 and C3 denote the number of test programs that
crashed only in -O0 and only in -O3, respectively, and C03 denotes
the number of test programs that crashed in both. Similarly, the
column T0, T3, and T03 represent the number of test programs

SAC-SVTâ€™21, March, 2021, Virtual, Republic of Korea

Md Rafiqul Islam Rabin and Mohammad Amin Alipour

GCC

Setting

4.3.0

4.8.2

5.4.0

6.1.0

7.1.0

8.1.0

9.1.0

trunk

Default
Swarm
HiCOND
K-Config-Round-Robin
K-Config-Weighted

Default
Swarm
HiCOND
K-Config-Round-Robin
K-Config-Weighted

Default
Swarm
K-Config-Round-Robin
K-Config-Weighted

Default
Swarm
K-Config-Round-Robin
K-Config-Weighted

Default
Swarm
K-Config-Round-Robin
K-Config-Weighted

Default
Swarm
K-Config-Round-Robin
K-Config-Weighted

Default
Swarm
K-Config-Round-Robin
K-Config-Weighted

Default
Swarm
K-Config-Round-Robin
K-Config-Weighted

C0

0
256
0
187
133

0
56
0
45
26

0
0
0
0

0
0
0
0

0
0
0
0

0
0
0
0

0
0
0
0

0
0
0
0

C3

C03

T0

T3 T03 MC

1708
349
2584
519
329

0
65
0
61
35

0
0
0
0

0
0
0
0

0
0
0
0

0
0
0
0

0
0
0
0

0
0
0
0

0
61
0
49
20

0
4
0
5
2

0
0
0
0

0
0
0
0

0
0
0
0

0
0
0
0

0
0
0
0

0
0
0
0

242
1159
386
1256
2003

10
1218
9
1269
2266

11
1224
1276
2003

13
1253
1298
2005

12
1135
1193
1939

13
1132
1190
1939

13
1143
1209
1965

45
1410
1359
2150

0
63
1
45
33

0
18
0
16
4

0
5
6
0

0
4
6
0

0
6
9
4

0
6
8
4

0
6
8
4

0
5
7
4

1017
1374
990
1101
947

1250
1410
1366
1198
993

1249
1429
1217
973

1247
1396
1188
961

1247
1490
1270
1018

1247
1478
1269
1017

1247
1468
1250
991

1215
1200
1099
805

16
55
16
68
29

0
52
0
73
21

0
61
81
25

0
64
82
26

0
66
86
28

0
66
88
27

0
69
95
26

0
70
96
26

Table 2: Result for 10,000 test programs.

that encountered timeouts only in -O0, only in -O3, and in both,
respectively. Finally, the column MC indicates the total number of
test programs that produced miscompilations.

5.1 Comparison with Default and Swarm
In Table 2, 10, 000 test programs of K-Config-Round-Robin, on
average, triggered up to 96 miscompilations, while Swarm triggered
up to 70 miscompilations, on average. The default configuration of
Csmith (Default) triggered 16 miscompilations in GCC-4.3.0 but
did not find any miscompilations in other versions of GCC.

The Default found 1708 crashes in GCC-4.3.0 compared to 755
by K-Config-Round-Robin and 666 by Swarm. However, the De-
fault did not find any crashes in GCC-4.8.2, where the K-Config-
Round-Robin found 111 crashes and the Swarm found 125 crashes.
In all cases, K-Config-Round-Robin triggered the highest num-

ber of miscompilations compared to Default and Swarm.

5.2 Comparison with HiCOND
HiCOND uses particle swarm optimization to search for configura-
tions that can find more bugs. We received the configurations of
HiCOND for GCC-4.3.0 from the authors. In GCC-4.3.0, HiCOND
finds the highest number of crashes on -O3, while swarm followed

by K-Config-Round-Robin discover the highest number of crashes
under -O1. K-Config-Round-Robin finds the highest number of mis-
compilations. Configurations proposed by HiCOND do not find any
crashes in GCC-4.8.2, as it is highly optimized for bugs in GCC-4.3.0.
Moreover, HiCOND relies on a set of bugs generated by Default
to search for the optimal configurations. Default does not trigger
any failures on GCC-4.8.2, we, therefore, could not compute new
configurations using the approach in [5] for GCC-4.8.2 and beyond.

5.3 Number of distinct bugs
To identify the number of distinct bugs we used the correcting
commits heuristic that has been used in previous studies [3, 7].
This heuristic uses the commit that a test program switched from
failing to passing as the proxy for the minimum number of distinct
bugs. Note that in our results, there are test programs that still
exhibit miscompilation characteristics on the latest versions of the
GCC, therefore, this heuristic can not be used for them. Due to
the submission deadline, we only compute the distinct bugs for
Swarm and K-Config-Round-Robin in GCC-4.8.2. For the crash
and miscompilation of failing test programs, the numbers of distinct
bugs detected by the techniques are: 3 only by K-Config-Round-
Robin, 3 only by Swarm, and 4 by both.

5.4 Effectiveness of cluster weighting strategies
We evaluated the effectiveness of K-Config for two cluster weight-
ing strategies: K-Config-Round-Robin that ignores the size of clus-
ters, and K-Config-Weighted which generates test programs pro-
portional to the size of clusters. The results in Table 2 show that K-
Config-Round-Robin was more effective than K-Config-Weighted
in finding bugs. K-Config-Round-Robin could trigger twice as
more miscompilations, and thrice as more crashes than K-Config-
Weighted. The potential reason can be that larger clusters represent
prominent combinations of features in the bug reports, it is likely
that developers already noticed them and addressed them. There-
fore, test programs generated based on K-Config-Weighted would
not lead to new failures. Due to the poor performance of K-Config-
Weighted, we excluded the K-Config-Weighted from subsequent
experiments to save computing time.

5.5 Effectiveness of individual clusters
To measure the effectiveness of individual clusters in K-Config-
Round-Robin, we count the number of failures triggered by the test
programs generated based on their centroid configuration. Figure 5
shows the number of crashes and miscompilations triggered by
each cluster. We excluded the timeouts from the figure due to their
sheer numbers. The ğ‘¥-axis in this figure denotes the clusters, and
the ğ‘¦-axis denotes the number of miscompilation and crash failures
triggered by each cluster.

Among 134 clusters, 50 did not contribute to finding any failures,
29 triggered only one crash, 18 triggered only one miscompilation,
while there are configurations with 20 to 26 (crash or miscompila-
tion) failures. Table 3 shows the top-5 most effective configurations
in triggering failures (crash and miscompilation combined).

Configuring Test Generators using Bug Reports: A Case Study of GCC Compiler and Csmith

SAC-SVTâ€™21, March, 2021, Virtual, Republic of Korea

GCC

4.3.0

4.8.2

5.4.0

6.1.0

7.1.0

Setting

Seed

Default

Swarm

HiCOND

# Statement

# Branch

# Function

169838âœ“âœ“

182429âœ“âœ“ 10933âœ“âœ“

133700

140404

134721

138581

148677

139570

9181

9475

9252

K-Config-Round-Robin

141165âœ“

148910âœ“

9499âœ“

Seed

Default

Swarm

HiCOND

214054âœ“âœ“

210324âœ“âœ“ 17601âœ“âœ“

177269

183123

176646

171935

180203

171637

15464

15688

15460

K-Config-Round-Robin

183736âœ“

180686âœ“

15734âœ“

Seed

Default

Swarm

256300âœ“âœ“

251049âœ“âœ“ 21949âœ“âœ“

207342

216008

199413

210644

19088

19431

K-Config-Round-Robin

216438âœ“

211039âœ“

19436âœ“

Seed

Default

Swarm

275888âœ“âœ“

269575âœ“âœ“ 23865âœ“âœ“

222485

232007

214760

226936

20398

20848

K-Config-Round-Robin

232990âœ“

227865âœ“

20899âœ“

Seed

Default

Swarm

294633âœ“âœ“

291922âœ“âœ“ 25131âœ“âœ“

237252

248171

233134

247166

21389

21953âœ“

K-Config-Round-Robin

248654âœ“

247621âœ“

21953âœ“

Table 4: Number of statements, branches, and functions cov-
ered in test suites. (âœ“âœ“denotes the test suite with the high-
est coverage. âœ“denotes the test suite with the second highest
coverage.)

writing, we did not extract the coverage information for most re-
cent versions of GCC, i.e., 8.1.0, 9.1.0, and trunk, due to difficulties
stemming from recent changes in the file structure in GCC project.
The coverage shows that, in the measured GCC versions, the regres-
sion test suite has higher coverage than the generated test suites.
In the generated test suite, the K-Config-Round-Robin has higher
coverage than others (Default, Swarm, and HiCOND).

5.7 The probability of features
In the swarm testing [14], the probability of inclusion of a test
feature in the test program depends on the fair coin-toss for all test
features. Therefore, the probability is the same for all test features,
it is 0.5. Figure 6, on the other hand, shows the distributions of prob-
abilities per test feature in the K-Config-Round-Robin approach.
The mean and median of the probabilities of all features are less
than 0.5. However, 28 out of 32 features have been in one or more
configurations with probability 1. In more than 25% of configu-
rations, 75% or more of features are excluded. It highlights the
importance of a few test features that dominate the test programs
generated by those clusters.

Figure 5: Number of failures in each configuration.

49

107

80

60

44

# Crash # MC # Total

16

10

26

12

10

22

13

8

21

Index

Features in Centroid

comma-operators,

compound-
arrays,
assignment,
embedded-assigns, pre-decr-
operator, post-incr-operator, unary-plus-
longlong, float, math64, muls,
operator,
volatiles, volatile-pointers, global-variabless,
builtins

bitfields, consts, divs, embedded-assigns, pre-
incr-operator, pre-decr-operator, post-decr-
operator, unary-plus-operator, jumps, long-
long, int8, uint8, float, math64, muls, safe-
math, packed-struct, paranoid, structs, unions,
volatiles, volatile-pointers, const-pointers,
global-variabless, builtins

argc, arrays, bitfields, compound-assignment,
consts, divs, embedded-assigns, pre-incr-
operator,
post-incr-
pre-decr-operator,
operator, post-decr-operator, unary-plus-
operator,
inline-
function, muls, safe-math, packed-struct,
paranoid, structs, unions, volatiles, volatile-
pointers, const-pointers, global-variabless,
builtins

int8, uint8, float,

jumps,

argc, arrays, bitfields, comma-operators,
embedded-
divs,
compound-assignment,
assigns,
post-decr-
post-incr-operator,
operator, unary-plus-operator, uint8, float,
math64, inline-function, muls, packed-struct,
structs, unions, volatiles, volatile-pointers,
global-variabless, builtins

comma-operators,
compound-assignment,
embedded-assigns, unions, volatiles, global-
variabless, builtins

12

8

20

8

12

20

Table 3: Top-5 centroids for failure-inducing test programs

5.6 Coverage across test suites
Table 4 shows the number of covered statements, branches, and
functions on GCC versions 4.3.0, 4.8.2, 5.4.0, 6.1.0, and 7.1.0 for the
Seed, Default, Swarm, and K-Config-Round-Robin test suites.
We computed the coverage with -O3 optimization. At the time of

SAC-SVTâ€™21, March, 2021, Virtual, Republic of Korea

Md Rafiqul Islam Rabin and Mohammad Amin Alipour

the gap still is very wide. Note that we only could compile the test
programs in the regression test suite without linking them because
they did not accompany a main function.
Difference with deep learning-based approaches. This ap-
proach is different from (deep) learning-based approaches such as
DeepSmith [9] that build a generative model for the test programs
of the programs. Moreover, learning-based techniques face two
challenges. First, learning-based approaches require many test
programs with millions of tokens to train a model. Second, learning-
based approaches tend to converge to a restrictive language model
of test program that overly restricts the type of test programs that
can be produced [12]. K-Config instead uses the configuration
of test generators to guide testing which is less constrained than
the generation of test programs in learning-based approaches. In
particular, K-Config only specifies the programming constructs
that should be present in the generated test programs, and the
order or number of those constructs are determined by the test
generator.
Limiting requirements. There are two main limitations to the
application of K-Config. First, it assumes that a stable test genera-
tor exists. Second, it requires a set of failing test programs. GCC
compiler has been under development for decades and the bug
reports are available. Moreover, GCC has Csmith [27], a mature
and well-engineered test generator, that allowed us to evaluate the
effectiveness of K-Config in testing GCC compilers.

7 RELATED WORK
Several approaches for testing compilers have been proposed; for
example, code mutation [15], random test generation [7], metamor-
phic testing [8, 17, 18, 26], learning-based testing [9], and swarm
testing [14], to name few. These approaches either generate test
programs from scratch by grammar [27] and learning [9], or they
create new test programs by manipulating [15] or transforming the
existing test programs, e.g., [17].

Swarm testing [14] randomly chooses a subset of features avail-
able to generate new test cases. The generated test cases are very
diverse and the evaluation result shows that this approach out-
performs Csmithâ€™s default configuration in both code coverage
and crash bug finding. SPE [30] where authors enumerate a set
of programs with different variable usage patterns. The generated
diverse test cases exploit different optimization and the evaluation
result shows that the skeletal program enumeration has confirmed
bugs in all tested compilers. Two more related studies in this area
are LangFuzz [15] and Learn&Fuzz [12]. The LangFuzz approach
extracts code fragments from a given code sample that triggered
past bugs and then apply random mutation within a pool of frag-
ments to generate test inputs. On the other hand, the Learn&Fuzz
approach uses the learnt seq2seq model to automate the generation
of an input grammar suitable for PDF objects using different sam-
pling strategies. Both approaches have revealed several previously
unknown bugs in popular compilers and interpreters.

Several approaches to accelerate the speed of test selection and
triage have been proposed; for example, [7], [3], [2], [6]. Chen et al.
[7] evaluate the impact of several distance metrics on test case selec-
tion and prioritization. Chen et al. [3] proposed LET where authors

Figure 6: Distribution of probability of inclusion for individ-
ual GCC features in K-Config-Round-Robin.

6 DISCUSSION
K-Config-Round-Robin and Swarm are complementary ap-
proaches in proposing configurations: Swarm chooses the
configurations randomly, while K-Config-Round-Robin takes
into account the bug reports in determining the configurations.
K-Config-Round-Robin generally finds more failures and provides
higher code coverage than Swarm. Moreover, the number of
distinct bugs detected by the techniques are: 4 by both, 3 only by
K-Config-Round-Robin, and 3 only by Swarm.

The particle swarm optimization technique used in HiCOND
requires a considerable amount of failing and passing test programs
generated by Csmith to find an optimal configuration. Unfortu-
nately, as GCC matures, it becomes harder for Csmith to trigger a
failure in itâ€”e.g., in Table 2, compare the number of failures trig-
gered by Default Csmith in GCC 4.3.0 and 4.8.2. Therefore, HiCOND
loses its applicability to newer, more stable versions of GCC. No-
tice that in [5], HiCOND is only trained on an older version (i.e.
GCC-4.3.0), and in the cross-version experiment is tried in other
versions (Section IV-F in [5]).
Searching around the lamp posts. K-Config analyzes features
of failing test programs to create new configurations for a test
generator. The result of our experiment shows that K-Config could
find up to 96 miscompilations in GCC, and outperforms Swarm
and HiCOND in triggering miscompilation failures.

It suggests that analysis of the regression test suite can enable de-
signing techniques to guide random test generators such as Csmith.
The result indicates that processing failing test programs can pro-
vide insights into the regions of code that are susceptible to bugs. It
might suggest that many bug fixes are incomplete [28], but we are
unable to verify the similarity between root causes of the bugs that
K-Config triggers and the test programs of regression test suite.
Seed vs. generated test suite. The coverage of GCC for the test
programs in the regression test suite shows the power of small,
directed test programs. Although the size of the regression test
suite was smaller than the test suites generated by Csmith, they sig-
nificantly outperformed those test suites in the coverage of GCC in
terms of statements, branches, and functions. The coverage of GCC
for the regression test suite can serve as a benchmark to measure
the shortcomings of the generated test suites. From this experiment,

Configuring Test Generators using Bug Reports: A Case Study of GCC Compiler and Csmith

SAC-SVTâ€™21, March, 2021, Virtual, Republic of Korea

use machine learning to schedule the test inputs. This learning-to-
test approach has two steps: learning and scheduling. In learning
steps, LET extracts a set of features from the past bug triggering test
cases and then trains a capability model to predict the bug trigger-
ing probability of the test programs, and trains another time model
to predict the execution time of the test programs. In scheduling
steps, LET ranks the target test programs based on the probability
of bug triggering in unit time. The evaluation result shows that
the scheduled test inputs significantly accelerate compiler testing.
Another example in this area is COP [6] where authors predict the
coverage information of compilers for test inputs and prioritize
test inputs by clustering them according to the predicted coverage
information. The result shows that COP significantly outperforms
state-of-the-art acceleration approaches in test acceleration.

8 THREATS TO VALIDITY
In this section, we describe several threats to validity for our study.
Internal Validity. Despite our best effort, some bugs may exist
in the tools and scripts used in this paper. However, we used the
well-tested programs in the implementation and evaluation of K-
Config to reduce the chance of mistakes. We have taken care to
ensure that our results are unbiased, and have tried to eliminate the
effects of variability by repeating the experiments multiple times.
External Validity. We evaluated the K-Config approach on
Csmith and several GCC versions, therefore, the extent to which
it generalizes to other compilers and test generators is quite
unknown. In this approach, the feature set is restricted to the ones
that can be translated to Csmith configuration options, therefore
the feature space is also limited. Additionally, K-Config relies on
the features that have triggered compiler bugs in the past, therefore
the effectiveness may decrease in future GCC versions for the bug
fixes. Another potential issue can be that the bug reports to build
the Seed test suite may not be representative of all GCC bugs and
can impact the clusters.

9 CONCLUSION
This paper proposed K-Config, a test configuration generation
approach, that uses the code snippets in the bug reports to guide
the test generation. Given a Seed test suite and a configurable test
generator, K-Config computes configurations for the test generator.
We extensively evaluated K-Config with GCC regression test
suite and Csmith on eight GCC versions. The results suggest that the
configurations computed by K-Config approach can help Csmith
to trigger more miscompilation failures than the state-of-the-art
approaches. The results signify the benefits of analyzing bug reports
in generation of new test programs. Our source code for this work
is publicly available at https://github.com/mdrafiqulrabin/kconfig/.

10 ACKNOWLEDGEMENT
We thank Junjie Chen and Guancheng Wang for computing the
number of distinct bugs and helping with the experiments related
to HiCOND.

REFERENCES
[1] Alipour, M. A., Groce, A., Gopinath, R., and Christi, A. Generating focused
random tests using directed swarm testing. In Proceedings of the 25th International

Symposium on Software Testing and Analysis (New York, NY, USA, 2016), ISSTA
2016, ACM, pp. 70â€“81.

[2] Chen, J. Learning to accelerate compiler testing.

In Proceedings of the 40th
International Conference on Software Engineering: Companion Proceeedings (New
York, NY, USA, 2018), ICSE â€™18, ACM, pp. 472â€“475.

[3] Chen, J., Bai, Y., Hao, D., Xiong, Y., Zhang, H., and Xie, B. Learning to priori-
tize test programs for compiler testing. In Proceedings of the 39th International
Conference on Software Engineering (Piscataway, NJ, USA, 2017), ICSE â€™17, IEEE
Press, pp. 700â€“711.

[4] Chen, J., Patra, J., Pradel, M., Xiong, Y., Zhang, H., Hao, D., and Zhang, L. A

survey of compiler testing. ACM Comput. Surv. 53, 1 (Feb. 2020).

[5] Chen, J., Wang, G., Hao, D., Xiong, Y., Zhang, H., and Zhang, L. History-
guided configuration diversification for compiler test-program generation. In
2019 34th IEEE/ACM International Conference on Automated Software Engineering
(ASE) (2019), IEEE, pp. 305â€“316.

[6] Chen, J., Wang, G., Hao, D., Xiong, Y., Zhang, H., Zhang, L., and XIE, B.
IEEE Transactions on

Coverage prediction for accelerating compiler testing.
Software Engineering (2018), 1â€“1.

[7] Chen, Y., Groce, A., Zhang, C., Wong, W.-K., Fern, X., Eide, E., and Regehr, J.
Taming compiler fuzzers. In Proceedings of the 34th ACM SIGPLAN Conference on
Programming Language Design and Implementation (New York, NY, USA, 2013),
PLDI â€™13, ACM, pp. 197â€“208.

[8] Chen, Y., Su, T., Sun, C., Su, Z., and Zhao, J. Coverage-directed differential testing
of jvm implementations. In Proceedings of the 37th ACM SIGPLAN Conference on
Programming Language Design and Implementation (New York, NY, USA, 2016),
PLDI â€™16, ACM, pp. 85â€“99.

[9] Cummins, C., Petoumenos, P., Murray, A., and Leather, H. Compiler fuzzing
through deep learning. In Proceedings of the 27th ACM SIGSOFT International
Symposium on Software Testing and Analysis (New York, NY, USA, 2018), ISSTA
2018, ACM, pp. 95â€“105.

[10] Fraley, C., and Raftery, A. E. How many clusters? which clustering method?
answers via model-based cluster analysis. The computer journal 41, 8 (1998),
578â€“588.

[11] Go-fuzz: randomized testing for go. https://github.com/dvyukov/go-fuzz/.
[12] Godefroid, P., Peleg, H., and Singh, R. Learn&fuzz: Machine learning for
input fuzzing. In Proceedings of the 32Nd IEEE/ACM International Conference on
Automated Software Engineering (Piscataway, NJ, USA, 2017), ASE 2017, IEEE
Press, pp. 50â€“59.

[13] Groce, A., Zhang, C., Alipour, M. A., Eide, E., Chen, Y., and Regehr, J. Help,
help, iâ€™m being suppressed! the significance of suppressors in software testing.
In 2013 IEEE 24th International Symposium on Software Reliability Engineering
(ISSRE) (2013), IEEE, pp. 390â€“399.

[14] Groce, A., Zhang, C., Eide, E., Chen, Y., and Regehr, J. Swarm testing. In
Proceedings of the 2012 International Symposium on Software Testing and Analysis
(New York, NY, USA, 2012), ISSTA 2012, ACM, pp. 78â€“88.

[15] Holler, C., Herzig, K., and Zeller, A. Fuzzing with code fragments. In Proceed-
ings of the 21st USENIX Conference on Security Symposium (Berkeley, CA, USA,
2012), Securityâ€™12, USENIX Association, pp. 38â€“38.

[16] Kwedlo, W. A clustering method combining differential evolution with the
k-means algorithm. Pattern Recognition Letters 32, 12 (2011), 1613â€“1621.
[17] Le, V., Afshari, M., and Su, Z. Compiler validation via equivalence modulo
inputs. In Proceedings of the 35th ACM SIGPLAN Conference on Programming
Language Design and Implementation (New York, NY, USA, 2014), PLDI â€™14, ACM,
pp. 216â€“226.

[18] Le, V., Sun, C., and Su, Z. Finding deep compiler bugs via guided stochastic
program mutation. In Proceedings of the 2015 ACM SIGPLAN International Con-
ference on Object-Oriented Programming, Systems, Languages, and Applications
(New York, NY, USA, 2015), OOPSLA 2015, ACM, pp. 386â€“399.

[19] McKeeman, W. M. Differential testing for software. Digital Technical Journal 10,

1 (1998), 100â€“107.

[20] Miller, B. P., Fredriksen, L., and So, B. An empirical study of the reliability of

unix utilities. Communications of the ACM 33, 12 (1990), 32â€“44.

[21] Novikov, A. PyClustering: Data mining library. Journal of Open Source Software

4, 36 (apr 2019), 1230.

[22] Pedregosa, F., Varoqaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel,
O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J.,
Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E.
Scikit-learn: Machine learning in Python. Journal of Machine Learning Research
12 (2011), 2825â€“2830.

[23] Pelleg, D., and Moore, A. X-means: Extending k-means with efficient estimation
of the number of clusters. In In Proceedings of the 17th International Conf. on
Machine Learning (2000), Morgan Kaufmann, pp. 727â€“734.

[24] Rabin, M. R. I., and Alipour, M. A. K-config: Using failing test cases to generate

test cases in gcc compilers. In ASE Late Breaking Results (2019).

[25] Ruderman, J. Introducing jsfunfuzz. https://www.squarefree.com/2007/08/02/

introducing-jsfunfuzz/, 2007.

[26] Sun, C., Le, V., and Su, Z. Finding compiler bugs via live code mutation. In
Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented

SAC-SVTâ€™21, March, 2021, Virtual, Republic of Korea

Md Rafiqul Islam Rabin and Mohammad Amin Alipour

Programming, Systems, Languages, and Applications (New York, NY, USA, 2016),
OOPSLA 2016, ACM, pp. 849â€“863.

[27] Yang, X., Chen, Y., Eide, E., and Regehr, J. Finding and understanding bugs in c
compilers. In Proceedings of the 32Nd ACM SIGPLAN Conference on Programming
Language Design and Implementation (New York, NY, USA, 2011), PLDI â€™11, ACM,
pp. 283â€“294.

[28] Yin, Z., Yuan, D., Zhou, Y., Pasupathy, S., and Bairavasundaram, L. How do
fixes become bugs? In Proceedings of the 19th ACM SIGSOFT Symposium and the

13th European Conference on Foundations of Software Engineering (New York, NY,
USA, 2011), ESEC/FSE â€™11, ACM, pp. 26â€“36.

[29] Zeller, A., and Hildebrandt, R. Simplifying and isolating failure-inducing

input. IEEE Transactions on Software Engineering 28, 2 (2002), 183â€“200.

[30] Zhang, Q., Sun, C., and Su, Z. Skeletal program enumeration for rigorous com-
piler testing. In Proceedings of the 38th ACM SIGPLAN Conference on Programming
Language Design and Implementation (New York, NY, USA, 2017), PLDI 2017,
ACM, pp. 347â€“361.

