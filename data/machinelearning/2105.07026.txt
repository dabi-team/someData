A Monotone Approximate Dynamic Programming Approach for the
Stochastic Scheduling, Allocation, and Inventory Replenishment Problem:
Applications to Drone and Electric Vehicle Battery Swap Stations

Amin Asadia, Sarah Nurre Pinkleya,

aDepartment of Industrial Engineering, University of Arkansas
4207 Bell Engineering, Fayetteville, AR 72701

1
2
0
2

y
a
M
4
1

]

C
O
.
h
t
a
m

[

1
v
6
2
0
7
0
.
5
0
1
2
:
v
i
X
r
a

Abstract

There is a growing interest in using electric vehicles (EVs) and drones for many applications. However, battery-
oriented issues, including range anxiety and battery degradation, impede adoption. Battery swap stations are one
alternative to reduce these concerns that allow the swap of depleted for full batteries in minutes. We consider the
problem of deriving actions at a battery swap station when explicitly considering the uncertain arrival of swap
demand, battery degradation, and replacement. We model the operations at a battery swap station using a ﬁnite
horizon Markov Decision Process model for the stochastic scheduling, allocation, and inventory replenishment
problem (SAIRP), which determines when and how many batteries are charged, discharged, and replaced over
time. We present theoretical proofs for the monotonicity of the value function and monotone structure of an
optimal policy for special SAIRP cases. Due to the curses of dimensionality, we develop a new monotone ap-
proximate dynamic programming (ADP) method, which intelligently initializes a value function approximation
using regression. In computational tests, we demonstrate the superior performance of the new regression-based
monotone ADP method as compared to exact methods and other monotone ADP methods. Further, with the tests,
we deduce policy insights for drone swap stations.

Keywords: Electric Vehicles and Drones; Battery Swap Station; Markov Decision Processes; Battery
Degradation; Monotone Policy and Value Function; Regression-based Initialization; Approximate Dynamic
Programming

1. Introduction

Electric vehicles (EVs) and drones hold great promise for revolutionizing transportation and supply chains. The
United States Department of Energy [1] reports that EVs can reduce oil dependence and carbon emissions, but
vehicle adoption is hindered by range anxiety, purchase price, recharge times, and battery degradation [2, 3].
Drone applications have increased in recent years; many organizations are using drones or undergoing testing
to use drones for different purposes, including, but not limited to, delivery [4, 5, 6, 7], transportation [8], and
agriculture [9]. However, drone use is restricted by short ﬂight times, long battery recharge times, and battery
degradation [10, 11, 12]. An option to overcome these barriers for EVs and drones is a battery swap station. A
battery swap station is a physical location that enables the automated or manual exchange of depleted batteries for
full batteries in a matter of seconds to a few minutes.

Swap stations have many beneﬁts including their ability to help reduce battery degradation. Battery degra-
dation, or, more speciﬁcally, battery capacity degradation, is the act of the battery capacity decreasing over time
with use. Each recharge and use of a battery causes a battery to degrade. Degraded battery capacity means EVs
and drones have shorter maximum ﬂight times and ranges. Thus, an interesting aspect of managing battery swap
stations is that both battery charge and battery capacity are needed; however, the recharging and use of battery
charge is the exact cause of battery capacity degradation. Thus, this presents a unique problem where recharging
batteries, which enables the system to operate in the short-term, is harmful for long-term operation. Although all
recharging causes degradation, the regular-rate charging used at swap stations reduces the speed in which batteries
degrade as compared to fast-charging [13, 14]. This increases battery lifespans and causes less environmental

Email addresses: asadi@email.uark.edu (Amin Asadi), snurre@uark.edu (Sarah Nurre Pinkley)

1

 
 
 
 
 
 
waste from disposal. In spite of this beneﬁt, swap stations still must determine when to recharge and replace
batteries.

The beneﬁts of battery swap stations are not restricted to decreasing battery degradation. Swap stations also
alleviate range anxiety by allowing users to swap their batteries in a couple of minutes. Furthermore, battery swap
stations are projected to be a key component within a smart grid through the use of battery-to-grid (B2G) and
vehicle-to-grid (V2G). B2G and V2G enable a charged battery to discharge the stored energy back to the power
grid [15]. In practice, several companies such as Toyota Tsusho and Chubu Electric Power in Japan [16, 17],
NIO and State Grid Corporation in China [18], and Nissan and E.ON in the UK [19] have installed or plan to
install V2G technology. Swap stations can also reduce the purchase price barrier through a business plan where
the swap station owns and leases the high-cost batteries [20]. For the many organizations seeking to use drones, a
set of continuously operating drones is often vital. However, continuous operation is difﬁcult because the realized
ﬂight time of a drone is often less than the recharge time [11, 12]. Thus, automated drone battery swap stations
are a promising option because no downtime for recharging is necessary. Given the beneﬁts and applications of
swap stations, we examine the problem of optimally managing a battery swap station when considering battery
degradation.

We model the operations at a battery swap station using the new class of stochastic scheduling, allocation,
and inventory replenishment problems (SAIRPs). In SAIRPs, we decide on the number of batteries to recharge,
discharge, and replace over time when faced with time-varying recharging prices, time-varying discharge revenue,
uncertain non-stationary demand over time, and capacity-dependent swap revenue. SAIRPs consider the key
interaction between battery charge and battery capacity and link the use and replenishment (recharging) actions of
charge inventory with the degradation and replenishment needs of battery capacity inventory. Battery charge and
capacity are linked because each recharge and discharge of a battery causes the battery capacity to degrade, and
the level of battery capacity directly limits the maximum amount of battery charge. To replenish battery capacity,
we must determine when and how many batteries to replace over time. For SAIRPs, the combination of battery
charge and battery capacity is necessary to satisfy non-stationary, stochastic demand over time.

We model the problem as a ﬁnite horizon MDP model allowing us to capture the non-stationary elements of
battery swap stations over time, including mean battery swap demand, recharging price, and discharging revenue
[21]. The MDP’s state space is two-dimensional, indicating the total number of fully charged batteries and the
average capacity of all batteries at the station. The action of the model is two-dimensional. The ﬁrst dimension
indicates the total number of batteries to recharge or discharge. The second dimension indicates the total number
of batteries to replace. The selected action results in an immediate reward, equal to proﬁt, comprised of capacity-
dependent revenue from battery swaps, revenue from discharging batteries back to the power grid, cost from
recharging batteries, and cost from replacing batteries. The system transitions to a new state according to a
discrete probability distribution representing battery swap demand over time, the current state, and the selected
action. For our MDP model of the stochastic SAIRP, we seek to determine an optimal policy that maximizes the
expected total reward, which is equal to the station’s expected total proﬁt. A standard solution method for solving
MDPs is backward induction (BI) [21]. We solve a set of modest-sized SAIRPs using BI to provide a baseline
for comparing the approximate solution methods; however, as Asadi and Nurre Pinkley [22] showed, BI is not
effective for deriving optimal policies for realistic-sized SAIRPs.

Stochastic SAIRPs suffer from the curses of dimensionality, thus, we investigate theoretical properties of
the problem to inform more efﬁcient solution methods. We prove that the stochastic SAIRP has a monotone
non-decreasing value function in the ﬁrst, second, and both dimensions of the state. We also prove that general
SAIRPs violate the sufﬁcient conditions for the existence of a monotone optimal policy in the second dimension
of the state. However, if the number of battery replacements in each decision epoch is constrained to be less than
a constant upper bound, we prove there exists a monotone optimal policy for the second dimension of the state in
the stochastic SAIRP.

To overcome the curses of dimensionality, we exploit these theoretical results and investigate efﬁcient solution
methods. We investigate methods that exploit our proven monotone structure, including Monotone Backward
Induction (MBI) [21] and monotone approximate dynamic programming (ADP) algorithms. First, we examine
Jiang and Powell’s [23] Monotone Approximate Dynamic Programming (MADP) algorithm which exploits the
monotonicity of the value function. Next, we propose a new regression-based Monotone ADP algorithm, which
we denote MADP-RB. In our MADP-RB, we build upon the foundation of MADP and introduce a regression-
based approach to intelligently initialize the value function approximation.

We design a comprehensive set of experiments using Latin hypercube sampling (LHS). We compare the perfor-
mance of ADP methods with the BI and MBI for the LHS’s generated scenarios of a modest size. Experimentally,
we show our regression-based ADP generates near-optimal solutions for modest SAIRPs. Besides, using the same
LHS scenarios, we solve large-scale SAIRPs with our proposed ADP algorithms. We demonstrate that our pro-

2

posed ADP approaches can overcome the inherent curses of dimensionality of SAIRPs that BI, and MBI failed to
succeed.

Main Contributions. The main contributions of this work are as follows: (i) we demonstrate that stochastic
SAIRPs violate the sufﬁcient conditions for the optimality of a monotone policy in the second dimension of the
state and prove the existence of a monotone optimal policy for the second dimension of state when an upper bound
is placed on the number of batteries replaced in each decision epoch; (ii) we prove the monotone structure for the
MDP value function; (iii) we propose a regression-based monotone ADP method by utilizing the theoretical
structure of the MDP optimal value function to intelligently approximate the initial value function and make
updates in each iteration; (iv) we computationally demonstrate the superior performance of our regression-based
monotone ADP algorithm and deduce managerial insights about managing battery swap stations.

The remainder of the paper is organized as follows. In Section 2, we outline literature relevant to our modeling
approach, solution approaches, and EV and drone applications. In Section 3, we formally deﬁne the stochastic
scheduling, allocation, and inventory replenishment problem as a two-dimensional Markov Decision Process. In
Section 4, we present theoretical results for the stochastic SAIRP. In Section 5, we present solution methods and
outline the monotone ADP algorithm with regression-based initialization to solve stochastic SAIRP instances. In
Section 6, we present results and insights from computational tests of the solution methods and realistic instances
of the stochastic SAIRP. We summarize the contributions in Section 7 and provide opportunities for future work.

2. Literature Review

There is growing interest surrounding electric vehicles (EVs) and drones in industry and academia. We proceed by
discussing the relevant literature pertaining to (i) the EV and drone swap station application; (ii) the background
knowledge for the proposed approach using aspects of optimal timing and reliability, inventory management,
and equipment replacement problems; (iii) the scientiﬁc works that explain the lithium-ion battery degradation
process; and (iv) ADP approaches that address the curses of dimensionality. To the best of our knowledge, no
past research has derived the structure of the optimal policy and value function for the scheduling, allocation,
and inventory replenishment problem nor solved the realistic-sized instances of SAIRPs to derive insights for
managing the operations at a battery swap station faced with battery degradation.

Swap stations were initially introduced for EVs and thus have a more extensive research base. However, there
is growing interest surrounding drone battery swap stations. We ﬁrst examine the work on managing the internal
operations of a battery swap station that are most similar to the model presented in this paper and Asadi and Nurre
Pinkley [22]. Widrick et al. [24] develop an inventory control MDP for a swap station that only considers the
number of batteries to recharge and discharge over time but excludes battery capacity levels, degradation, and
replacement. They prove the existence of a monotone optimal policy only when the demand is governed by a non-
increasing discrete distribution (e.g., geometric). Nurre et al. [25] also consider determining the optimal charging,
discharging, and swapping at a swap station using a deterministic integer program that excludes uncertainty.
Worley and Klabjan [26] examine an EV swap station with uncertainty and seek to determine the number of
batteries to purchase and recharge over time. Note, purchasing batteries is fundamentally different from battery
replacement in SAIRPs. Worley and Klabjan [26] examine the one-time purchase of batteries to open a swap
station and do not consider purchasing decisions over time. Contrarily, we assume the initial number of batteries
at the swap station is previously determined and instead consider replacing batteries over time. Sun et al. [27]
propose a constrained MDP model for determining an optimal charging policy at a single battery swap station and
examine the tradeoffs between quality of service for customers and energy consumption costs.

Other research considers a mix of long-term strategic and short-term operational swap station decisions.
Schneider et al. [28] consider a network of swap stations that seeks to determine the long-term number of charging
bays and batteries to locate at each station and the short-term number of batteries to recharge over time. Schneider
et al. [28] do consider charging capacity; however, their use of capacity indicates the number of batteries that
can be recharged at one time in the station and do not model battery capacity. Kang et al. [29] propose the EV
charging management problem, which determines the optimal locations for a network of swap stations and fur-
ther determines the charging policy for each location. Their deﬁnition of charging policy only considers charging
and excludes discharging or replacement. Excluding the explicit charging actions over time, Zhang et al. [30]
determine the number of batteries that are necessary for swapping over time. For further studies in the area of EV
operations management, we refer the reader to a review by Shen et al. [31].

A common limitation of the aforementioned research is that it fails to account for battery degradation. To
the best of our knowledge, there are very few articles that consider battery degradation. Asadi and Nurre Pinkley
[22] are the ﬁrst to introduce stochastic SAIRPs for managing battery swap stations with degradation. However,

3

they do not theoretically analyze this problem class, do not introduce intelligent approximate dynamic solution
methods that exploit the theoretical results, and do not provide insights from solving realistic-sized SAIRPs.

Others have examined battery degradation in a deterministic setting without any uncertainty [32, 10, 33].
Sarker et al. [34] consider the problem of determining the next day operation plan for a battery swap station under
uncertainty. They do consider battery degradation; however, they solely penalize battery degradation with a cost
in the objective and do not link it to a reduction in operational capabilities.

Others have examined battery swap stations from different perspectives. Researchers have examined how to
ﬁnd the optimal number and location of swap stations in a system [35, 36, 37]. Extending this idea further, Yang
and Sun [38] look to locate swap stations and route vehicles through the swap stations. Others have examined how
to locate and/or operate swap stations that are coordinated with green power resources [39], stabilize uncertainties
from wind power [40], or coordinate with the power grid [41].

Our research is related to optimal timing and reliability problems. There is a rich literature on ﬁnding the
optimal timing of decisions to maximize systems’ lifespan and reliability. For instance, researchers maximize the
expected quality-adjusted life years by ﬁnding the optimal timing of living-donor liver transplantation [42], biopsy
test [43, 44], and replacement of an Implantable Cardioverter Deﬁbrillator generator [45]. There are two options
for the actions in these works (e.g., transplant/wait, take/skip the biopsy test, replace/not replace). However, our
action determines the number of batteries to recharge/discharge and replace in each epoch because it is not a
single battery that enables the station to operate. Instead, it a set of batteries that enables operation, which creates
a signiﬁcantly larger action space that is dependent on the number of batteries at the station. Similar to our work
is that of [46], which determines the optimal timing and duration of a degrading repairable system. There is
extensive research in the nexus of optimization, reliability, and systems maintenance. We refer the reader to the
recent review paper by [47] for further study.

Our research can be placed under the umbrella of inventory management and equipment replacement prob-
lems with stochastic elements. There is a large research base examining these types of problems under different
characteristics. We proceed by reviewing a small sample of this body of knowledge by focusing on foundational
work and research most similar to the scope of this paper. Researchers have extensively studied inventory prob-
lems, including those with stochastic demand [48], two- and multi-echelon supply chains [49, 50], and multiple
products [51]. A desirable feature of the solutions to inventory problems is that the optimal policy has a simple
structure. A classic example of such an optimal policy is the (s, S ) policy that indicates to order up to S units
when the inventory level drops below s [52]. Others have examined more sophisticated inventory problems which
include scheduling production [53, 54, 55, 56], performing maintenance or replacement [57], and ordering spare
parts for maintenance [58, 59]. Additionally, researchers have examined perishable inventory that degrades over
time [60] or inventory that can be recycled or remanufactured in a closed-loop supply chain [61, 62, 63].

The proposed work is distinct from this previous literature as it links the actions of recharging batteries to the
actions that must be taken for replacing battery capacity. No prior work includes the counter-intuitive property that
the act of maintaining the system in the short term (e.g., through recharging batteries which can be analogous to
short-term maintenance or short-term inventory replenishment) is harmful for long-term performance (e.g., future
need to replace equipment or replenish other types of inventory).

A novel component of our work is the consideration of battery degradation within the decision-making process.
Battery degradation is most traditionally measured based on calendar life or cycles, where a cycle consists of one
use and one recharge [13, 64]. Using physical experiments, simulation, and mathematical modeling, researchers
aim to capture the rate of battery degradation for different batteries and conditions such as temperature and depth
of discharge [65, 66, 64, 67, 68]. We approximate battery degradation using a linear degradation factor derived
from the work of [13] and [64], as is consistent with other research using a linear forecast [69, 70, 71].

Our MDP model suffers from the curses of dimensionality due to the very large size of all MDP elements
together, including state and action spaces, transition probability, and reward. Approximate dynamic programming
(ADP) is a method that has had great success in determining near-optimal policies for large-scale MDPs [72].
Researchers have used ADP methods to solve problems in energy, healthcare, transportation, resource allocation,
and inventory management [73, 74, 75, 76, 77, 78, 79, 80, 81]. Jiang and Powell [23] propose a monotone ADP
algorithm that is speciﬁcally designed for problems with monotone value functions. In this paper, we prove that
the value function of the stochastic SAIRP has a non-decreasing monotone structure. Hence, we utilize Jiang and
Powell’s [23] monotone ADP algorithm and enhance it by adding a regression-based initialization.

4

3. Problem Statement

In this section, we present and model the Markov Decision Process (MDP) model of the scheduling, allocation, and
inventory replenishment problem (SAIRP) that considers stochastic demand for swaps over time, non-stationary
costs for recharging depleted batteries, non-stationary revenue from discharging, and capacity-dependent swap
revenue. The MDP model captures the dynamic average battery capacity over time, the associated replacement
policies, and the interaction between battery charge and battery capacity at a battery swap station. We note, this
model was originally presented in Asadi and Nurre Pinkley [22]; however, we believe it is necessary to provide
the reader with the formal problem deﬁnition to enable understanding of the main theoretical and algorithmic
contributions that follow. We use a ﬁnite horizon MDP to capture the high variability of data over time, including
the mean demand for battery swaps, the price for recharging batteries, and the revenue earned from discharging
batteries back to the power grid. The uncertainty in the system is the stochastic demand for battery swaps (i.e.,
exchange of a depleted battery for a fully-charged battery). We model this uncertainty (stochastic demand) using
the random variable, Dt, for each time period t. These random variables are explicitly used to calculate the
transition probabilities. The objective is to maximize the expected total reward of the swap station and determine
optimal policies which dictate how many batteries to recharge, discharge, and replace over time. For our model,
the expected total reward equals the expected total proﬁt calculated as the revenue from satisfying demand and
discharging batteries to the power grid minus the costs from recharging and replacing batteries.

We formulate our MDP model with the following elements. We deﬁne T as the ﬁnite set of decision epochs,
which are the discrete periods in time in which decisions are made. By deﬁning N as the terminal epoch, T =
{1, . . . , N − 1}, N < ∞.

We denote the two-dimensional state of the system at time t, st = (s1

t ) ∈ S = (S 1 × S 2), as the total number
t ∈ S 2, at the swap station. In the
t , we only consider that batteries are either fully charged or depleted. The number of full batteries at
t , is an integral value between 0 and M, where M is the total number of batteries in the station, thus,

t ∈ S 1, and the average capacity of all batteries, s2

t , s2

of fully charged batteries, s1
design of s1
time t, s1
S 1 = {0, 1, 2, . . . , M}.

We use an aggregated MDP in which we track the discretized average battery capacity rather than a disag-
gregated MDP, which tracks each battery capacity individually, to reduce the curses of dimensionality from the
second dimension of the state. The disaggregated MDP severely suffers from the curse of dimensionality as the
state space’s size grows exponentially as the number of batteries increases. We discretize the average battery ca-
pacity where S 2 = {0, θ, θ + ε, θ + 2ε, . . . , 1}, in which θ equals the lowest acceptable average battery capacity and
ε in the discretized capacity increment. State zero in S 2 is an absorbing state representing that the average battery
capacity dropped below θ. To discourage the station from allowing the battery capacity to drop below θ thereby
resulting in lower quality batteries at the station, we disallow charging, discharging, swapping, and replacement
= 0, only
when in this absorbing state. Hence, the set of feasible actions when in an absorbing state, s2
includes no recharge/discharge and no replacement. We note, with this aggregated modeling proposed by Asadi
and Nurre Pinkley [22], the problem size and complexity are reduced, which is not always necessary when using
approximate solution methods. However, the aggregated model allows us to benchmark the performance of new
and existing approximate solution methods and analyze larger SAIRP instances. Further, we previously showed
that the results do not signiﬁcantly change with aggregation [22].

t < θ or s2
t

We denote the two-dimensional action to represent the number of batteries to recharge/discharge, a1

t , and
the number of batteries to replace, a2
t , at time t. In our aggregated MDP model, there is no known difference
between the capacity of batteries as we only track the average capacity of all batteries. In reality, swap stations,
applying the aggregated MDP, may track/not track the capacity of each battery. If, consistent with the model, the
swap station does not track individual battery capacity values, we assume the speciﬁc batteries that are selected
to be recharged/replaced or discharged/swapped are arbitrarily selected from the set of empty and fully-charged
batteries, respectively. However, if the swap station does track the individual battery capacity value, we assume
that the station selects to recharge/discharge and swap batteries with the highest capacity values and selects to
replace batteries with the lowest capacity values. With this selection mechanism, individual battery capacity values
will be closer to the average battery capacity of the system and, thus, further emphasizes the aggregated modeling
decision. Regarding the ﬁrst dimension of the action, a1
t , we attribute a positive value to the number of batteries
to recharge, a1+
. To clarify the distinction
t
t
between recharging and discharging actions, we deﬁne positive recharging, a1+
, and discharging actions, a1−
,
t
with Equations (1) and (2). We note that only dealing with the positive number of batteries that are recharged or
discharged using Equations (1) and (2) is helpful to clarify the forthcoming state transition, probability transitions,
and reward calculations.

, and a negative value to the number of batteries to discharge, a1−

t

5

a1+
t

=

a1−
t

=

(cid:40)

(cid:40)

a1
t
0

|a1
t |
0

if a1
t ≥ 0,
otherwise,

if a1
t < 0,
otherwise.

(1)

(2)

The action a1

t represents both the number of batteries that are recharged, when a1

t is positive, and the number
of batteries that are discharged, when a1
t is negative. We designed the action in this way as it is not beneﬁcial to
recharge and discharge at the same epoch, as they will cancel each other out and cause the capacity to degrade.
Thus, we select one value for a1
t for each time t, and state, st. Depending on whether the selected action is
positive or negative indicates whether recharging or discharging will occur. We denote the number of plug-ins in
the station as Φ. We assume all plug-ins are capable of supplying energy from the grid to recharge batteries and
receiving energy from batteries discharged using Battery to Grid [15]. We deﬁne the ﬁrst dimension of action as
t , Φ)}, which limits the number of discharged batteries by
t ∈ A1
a1
t
t , −Φ)) and limits
the minimum of the number of plug-ins and the number of full batteries (−min(s1
the number of recharged batteries by the minimum of the number of plug-ins and the number of depleted batteries
t }, we only allow
that were not replaced.
depleted batteries to be replaced at each epoch t which arrive in epoch t + 1 with full charge and capacity. We
st ) ⊆ (A1
deﬁne Ast
t ) as the set of feasible actions for the state st at time t. In our model, the
= 0, only includes no recharge/discharge and no
set of feasible actions when in an absorbing state, s2
replacement; i.e., A(s1

In the second dimension of the action space, a2

t , −Φ), . . . , 0, . . . , min(M − s1

t , Φ) = max(−s1

= {0, . . . , M − s1

= {max(−s1

t < θ or s2
t

= {(0, 0)}.

= (A1
st

t ∈ A2
t

t × A2

t − a2

× A2

In Figure (1), we display the timing of the operations at the swap station including recharging, discharging,
replacing, and swapping between epochs t and t + 1. We assume that the time between two consecutive epochs
is sufﬁcient to recharge or discharge a battery completely. In our model, we could preemptively recharge, dis-
charge or replace batteries for future time periods. Therefore, depleted (full) batteries selected for recharging
(discharging) in epoch t are fully charged (depleted) at the start of epoch t + 1. When stochastic demand for a
battery swap arrives in epoch t, we can swap up to the number of fully-charged batteries in our inventory which
equals the number of fully-charged batteries at the start of t minus the number of discharged batteries. We subtract
the fully-charged batteries assigned to be discharged as they are unavailable for swapping until the next decision
epoch.

t ,0)

Replace a2
t
batteries

Recharge a1+
t
batteries

Start of
epoch t

Recharged a1+
t

discharged a1−
batteries complete

t

and

Start of
epoch t + 1

# Fully charged
and average capacity
t , s2
t )

(s1

Discharge a1−
batteries

t

Demand Dt occurs
in epoch t, satisﬁed
if fully charged
batteries available

New replaced a2
t
batteries arrive

Figure 1: Diagram outlining the timing of events for the SAIRP model.

# Fully charged and
average capacity
t+1, s2

t+1)

(s1

Transition probabilities indicate the likelihood of transitioning between states when considering the uncertainty
of the system. In our MDP model, the uncertainty in the system is the stochastic demand for battery swaps (i.e.,
exchange of a depleted battery for a fully-charged battery) at each decision epoch t, Dt. The amount of satisﬁed
swap demand in epoch t equals min{Dt, s1
t } wherein the second term indicates the number of full batteries
that are not already discharging at t. We outline the state transition for the ﬁrst dimension of the state in Equation
(3) which determines the number of full batteries in epoch t +1 based on the number of full, recharged, discharged,
6

t − a1−

replaced, and swapped batteries in epoch t.

+ a1+

s1
t+1

= s1
t

+ a2
t

t − a1−
t }.

t − a1−

t − min{Dt, s1

(3)
The second state transitions according to Equation (4), which determines the future average capacity in t + 1
based on the current average capacity and the number of full, recharged, discharged, and replaced batteries in
epoch t. We assume that all batteries swapped at time t have a capacity equal to the average capacity of the
batteries at the swap station. We justify the assumption with the following logic. Batteries previously swapped in
epoch t1 < t, which are in use outside of the station between t1 and t and need to be swapped again in epoch t, have
a capacity similar to the average station capacity at t when the swap station is used regularly (i.e., t − t1 is small).
We deﬁne δC to represent the amount of battery capacity degradation from one battery cycle. We adopt the cycle-
based degradation measure [70, 13] and assume that batteries do not degrade when not in use. Further, without
loss of generality, we attribute the degradation from a full cycle to the recharge/discharge portion of the cycle. We
use round() to represent that Equation (4) returns values in the discretized state space, S 2, with ε precision.

g2(s1

t , s2

t , a1

t , a2
t )

=

s2
t+1

=

round

(cid:18) (s2

t − δC)(a1+

t

+ a1−

+ s2
t ) + a2
t
M

t (M − a1+

t − a1−

t − a2
t )

(cid:19)
.

(4)

t > 0) batteries by the reduced average capacity (s2

In the ﬁrst term in the numerator of Equation (4), we multiply the summation of the number of recharged (a1+

t >
0) and discharged (a1−
t − δC) due to the recharging/discharging
actions. The second term adds the a2
t replaced batteries with 100% capacity. The third term maintains the same
capacity for batteries not recharged, discharged, and replaced. These terms are all averaged over the M batteries
in the swap station. The system enters the absorbing state 0 ∈ S 2 when the average capacity is less than θ. To
discourage entrance into this absorbing state, no recharging, discharging, swapping, or replacement is allowed.
This setting ensures that swap stations should take appropriate actions before allowing the average capacity to
drop below θ. Thus, the transition of the second dimension of the state is precisely deﬁned with Equation (5).

f 2(s1

t , s2

t , a1

t , a2

t ) = s2
t+1

(cid:40)

=

t , s2

t , a1

t , a2
t )

g2(s1
0

t , s2
if g2(s1
otherwise.

t , a1

t , a2

t ) ≥ θ,

(5)

In Equation (6), we deﬁne the probability of transitioning from state st = (s1

t , s2

t ) in epoch t to the state

t

t



+a2
t

ps1

, a2

+a1+

t − j1

t , s2

, a1−
t

t −a1−

t , a2

t , a1+

p( j1, j2 | s1

t ) is taken.

j = ( j1, j2) in epoch t + 1 when action at = (a1


qs1
0
We deﬁne p j = P(Dt = j) and qu = (cid:80)∞
j=u p j = P(Dt ≥ u). Each probability in Equation (6) depends on the
+ a1+
t − j1 (see Equation (3)). When no batteries are swapped
fully charged batteries at epoch t + 1. Instead, if all available
t − a1−
t
fully charged batteries at epoch t + 1,

number of batteries swapped, i.e., s1
t
+ a2
in epoch t, the station still has s1
t
t
fully-charged batteries in epoch t are swapped, the station will have a2
t
which are the result of the a2

+ a1+
t
recharged batteries in epoch t.

t < j1 ≤ s1
t
t , a2
t , s2
t , a1
t ),
+ a1+
and j2 = f 2(s1
t

+ a1+
if a2
t
j2 = f 2(s1
if j1 = a2
t
otherwise.

+ a2
t
+ a1+

t − a1−

t − a1−
t

t replaced and a1+

t , a2
t ),

+ a1+

t −a1−

+ a2
t

t , a1

t ) =

t , s2

t − j1

+a1+

and

+a2
t

(6)

t

t

In Equation (6), the probability of transitioning to another state is non-zero only when Equation (5) is satisﬁed.
When the transition probability is P(Dt = s1
t − a1−
t − j1), the demand for swaps is less than or equal
t
to the number of full batteries available for swapping (as in condition 1 of Equation (6)). Alternatively, when
the demand for swaps is greater than the number of available full batteries, the state transitions according to the
t − a1−
t − j1). If Equation (5) is not satisﬁed, j1 is lower than the total
cumulative probability P(Dt ≥ s1
t
+ a1+
t ), or j1 exceeds than the maximum number of fully charged
number of batteries recharged and replaced (a2
t
t − a1−
batteries, s1
, the probability of transition is zero.
t
t

+ a1+

+ a1+

+ a1+

+ a2
t

+ a2
t

+ a2
t

To clarify the transition probability function, we illustrate using an example. Consider the case when at epoch
= 80), the average
t, the swap station has 80 full-batteries and 20 depleted batteries in inventory (i.e., M = 100, s1
t
= a1+
= 10) and replace
battery capacity equals 0.85, and we take the action to recharge 10 batteries (i.e., a1
t
t
= 5). For this example, we assume recharging or discharging for one time period results in
5 batteries (i.e., a2
t
a capacity degradation equal to 0.01 (i.e., δC = 0.01) and the discretized capacity increment is also 0.01 (i.e.,
ε = 0.01). If there is no demand for battery swaps (i.e., Dt = 0), at epoch t + 1 the station will have 95 full batteries
with a discretized average capacity equal to 0.86. Thus, the probability of transitioning to a state with more than
95 full batteries or an average capacity not equal to 0.86 is zero. Contrarily, if the demand for swaps is 80 or more
(i.e., Dt ≥ 80), then all full batteries in inventory will be swapped and the number of full batteries at at epoch t + 1
7

+ a1+
t

= 10 + 5 = 15. Thus, the probability of transitioning to a state with less than 15 full batteries is
equals a2
t
zero. Further, the probability of transitioning to a state with exactly 15 full batteries and average capacity equal to
= 80. Lastly, consider the case that we transition
0.86 indicates that demand for swaps met or exceeded s1
to a state with 30 full batteries and average capacity equal to 0.86. The 30 full batteries is between the minimum,
= 15
a2
t
t − j1 = 80 + 5 + 10 − 0 − 30 = 65
batteries arrive at the end of t indicating that we swapped s1
t
batteries at epoch t. As follows, the probability of transitioning to this state equals the probably that demand for
swaps equals 65, i.e., P(Dt = 65).

= 95 number of full batteries; thus, we know that a2
t

= 15, and maximum, s1
t

t − a1−

t − a1−
t

t − a1−
t

+ a1+
t

+ a1+
t

+ a1+

+ a1+

+ a2
t

+ a2
t

The actions taken seek to maximize the expected total reward. The expected total reward depends on the
immediate reward earned at each epoch. Speciﬁcally, the immediate reward is the proﬁt earned. In our setting,
swap stations earn revenue from swapping and/or discharging fully-charged batteries and incur costs to recharge
and/or replace depleted batteries. We calculate the immediate reward at epoch t according to the state of the
system st = (s1
t+1). Speciﬁcally, the
immediate reward is calculated according to Equation (7),

t ), and the future state st+1 = (s1

t ), the taken action at = (a1

t+1, s2

t , a2

t , s2

rt(st, at, st+1) = ρs2
t − s1

t

(s1
t

+ a2
t

+ a1+

t − a1−

t − s1

t+1) − Kta1+

t

+ Jta1−

t − Lta2
t ,

(7)

+ a1+

+ a2
t

t − a1−

where s1
t+1 equals the number of batteries swapped and the time-dependent recharging
t
cost, discharging revenue, and replacement cost are deﬁned as Kt, Jt, and Lt, respectively. We note that SAIRPs
consider two aspects of a battery, charge and capacity. In this model, the fully-charged/empty batteries are not
necessarily full-capacity as they might already be degraded due to the previous recharge/discharge actions. Thus,
the average capacity of batteries can take a value less than 100%. We assume the realized swap revenue depends
on the current average capacity. Thus, we deﬁne ρs2
to be the capacity-dependent revenue per battery swapped in
Equation (8).

t

= β

ρs2

t

(cid:33)

(cid:32)

1 + s2

t − θ
1 − θ

= β(1 + s2
1 − θ

t − 2θ)

.

(8)

We set β ≥ maxt∈T Jt to ensure the swap station is proﬁtable with each battery swapped (i.e., the swap revenue
is no less than the maximum recharging cost). We use the average capacity of batteries as the indicator of the
quality of batteries in the station when developing the revenue per swap function. Revenue per battery swap is a
linear function of the average capacity of batteries in the station. This setting ensures that the stations can gain
higher revenue when the average capacity is higher. It also provides an incentive for swap stations to replace
batteries for higher revenue and beneﬁts customers by receiving higher quality batteries. In the design of Equation
(8), when the average capacity is at the lowest operational value (s2
equals β,
t
which is at least equal to the maximum price paid for recharging batteries. When the swap station has an average
= 2β which equates to a higher revenue earned due to higher customer
battery capacity equal to 1, s2
t
satisfaction from swapping a higher quality battery. Hence, in our design, the revenue per swap has a value
between [β, 2β] depending on the average capacity of batteries in the station at time t. We calculate the terminal
reward in Equation (9) as the potential revenue from swapping all remaining fully charged batteries provided the
average battery capacity is at least θ.

= θ), the revenue per swap ρs2

= 1, then ρs2

t

t

(cid:40)

rN(sN) =

s1
N

ρs2
0

N

s2
N ≥ θ,
if
otherwise.

(9)

Using the probability transition function and the immediate reward, we deﬁne the immediate expected reward

in Equation (10).

rt(st, at) =

(cid:88)

(cid:104)

st+1∈S

pt(st+1 | st, at)(ρs2

t

(s1
t

+ a2
t

+ a1+

t − a1−

t − s1

t+1))

(cid:105)

− Kta1+

t

+ Jta1−

t − Lta2
t .

(10)

We deﬁne the decision rules, dt(st) : st → Ast , as a function of the current state and time. Our decision
rules determine the selected action at ∈ Ast when the system is in st at decision epoch t ∈ T . In our problem
setting, we use deterministic Markovian decision rules because we choose which action to take provided we know
the current state [21]. A policy π consists of a sequence of decision rules (dπ
2(s2), . . . , dπ
N−1(sN−1) for all
decision epochs. The expected total reward of policy π, denoted υπ
N(s1) when the system starts in state s1 at time
t=1 is calculated according to Equation (11).

1(s1), dπ

8

N(s1) = Eπ
υπ
s1


N−1(cid:88)


t=1



rt(st, at) + rN(sN)

.

(11)

In Section 5, we describe our solution methodology to ﬁnd optimal/near-optimal solutions to maximize the

expected total reward of the stochastic SAIRPs.

4. Theoretical Results

In this section, we prove theoretical properties regarding the structure of the optimal SAIRP policy and value func-
tion. First, we show that the stochastic SAIRPs violate the sufﬁcient conditions for the optimality of a monotone
policy in the second dimension of the state. Second, we prove the existence of a monotone optimal policy for
the second dimension of the state in a special case of the SAIRP. Lastly, we prove the monotonicity of the value
function when considering the ﬁrst, second, and both dimensions of the state. In the remainder of this section, we
present the main theorems and point the reader to the appendices for the formal mathematical proofs.

4.1. Monotone Policy

Our investigation in proving the structure of an optimal policy for the SAIRP is motivated by the desire to exploit
efﬁcient algorithms that require less computational effort to ﬁnd optimal policies and increase the ability to solve
larger problem instances [21]. Widrick et al. [24] examined the problem of managing a battery swap station when
only considering battery charge, or equivalently the ﬁrst dimension of our MDP model. They proved the existence
of a monotone optimal policy when demand is governed by a non-increasing discrete distribution. In our investi-
gation of the second dimension, our intuition was that monotonicity would be preserved for the stochastic SAIRP.
Informally, this equates to the optimal policy indicating to replace more batteries when the average capacity is
lower. However, in Lemma 1, we prove a counter-intuitive result that, in general, the sufﬁcient conditions for
the optimality of a monotone policy in the second dimension of the state do not exist for the stochastic SAIRPs.
Instead, we are able to prove the existence of a monotone optimal policy for the second dimension of the state
when an upper bound is placed on the number of batteries replaced at the swap station in each decision epoch.

First, we formally deﬁne a monotone optimal policy. A non-increasing monotone policy π has the property
that for any si, s j ∈ S with si ≤ s j (for multi-dimensional states, please see the partial ordering deﬁnition in
t (s j) for each t = 1, . . . , N − 1 [21]. The
Deﬁnition 2 of Appendix 7.2), there exist decision rules dπ
sufﬁcient conditions for the existence of a monotone optimal policy in the second dimension of the state are as
follows [21].

t (si) ≥ dπ

1. rt(s2

t , a2
2. qt(k | s2

3. rt(s2

t , a2
4. qt(k | s2

t ∈ A(cid:48).

t for all a2

t ) is non-decreasing in s2

t ) is non-decreasing in s2
t , a2
t ) is a subadditive function on S 2 × A(cid:48).
t , a2

t ) is subadditive on S 2 × A(cid:48) for every k ∈ S 2.

t for all k ∈ S 2 and a ∈ A(cid:48).

5. rN(sN) is non-decreasing in s2
N.

Where A(cid:48) includes all possible actions for the second dimension of the action space. Speciﬁcally, A(cid:48) = {∪st∈S A2
st
We note that qt(k | s, a) = (cid:80)∞
second dimension, we have qt(k | s2

}.
j=k pt( j | s, a), which is the sum of the probabilities from k to ∞, in general. For the

t , a2
t ).
In Lemma 1, we prove that one of the aforementioned conditions is not satisﬁed for stochastic SAIRPs. In
Theorem 1, we are able to prove that a monotone optimal policy in the second dimension of the state does exist
when there is an upper bound on the number of batteries replaced in each decision epoch. We refer the reader to
Appendix 7.1 for full details of the proof of Lemma 1 and Theorem 1.

j2=k pt( j2 | s2

t ) = (cid:80)∞

t , a2

Lemma 1. The stochastic SAIRPs violate the sufﬁcient conditions for the optimality of a monotone policy in the
second dimension of the state.

Theorem 1. There exist optimal decision rules d∗
t : S → Ast for the stochastic SAIRP which are monotone non-
increasing in the second dimension of the state for t = 1, . . . , N − 1 if there is an upper-bound U on the number
of batteries replaced at each decision epoch where U = Mε
, when M is the number of batteries at the swap
2(1−s2
t )
station and ε is the discretized increment in capacity.

9

We provide an example to explain the optimality of the monotone policy in the second dimension of the state.
Consider a swap station with M = 100 batteries, a discretized capacity increment ε = 0.01, and a replacement
threshold θ = 0.8. The monotone policy is optimal when the maximum number of batteries replaced per epoch,
U, is between 2 and 50. The speciﬁc value between 2 and 50 depends on the value of the average capacity. If the
= 50. We note that when
average capacity s2
t
t = 1, then U = 100(0.01)
= ∞ meaning there is no limit on the number of replaced batteries. However, as the
s2
2(1−1)
average capacity is already at the highest value of 1, it is not advantageous for swap stations to incur the cost for
replacing a full capacity battery after which the average capacity will remain at 1. Although there is a restriction
on the number replaced in each epoch, there are no restrictions on the consistent replacement of batteries over
multiple consecutive decision epochs.

= 0.99, then U = 100(0.01)
2(1−0.99)

= 0.8, then U = 100(0.01)
2(1−0.8)

= 2 whereas if s2
t

4.2. Monotone Value Function

We now investigate the structure of the value function for stochastic SAIRPs. Although proving that a value
function has a monotone structure is a weaker result than proving the structure of an optimal policy, it enables the
application of computationally efﬁcient solution methods. We prove that the MDP value function for the stochastic
SAIRP is monotone non-decreasing in the ﬁrst, second, and both dimensions. These results directly motivate our
selection of efﬁcient approximate dynamic programming algorithms.

A value function V(s) is monotone non-decreasing in state s, if for any si, s j ∈ S with si ≤ s j we have
V(si) ≤ V(s j) for any given action in any decision epoch t [82, 23]. The MDP value function for the stochastic
SAIRP is given in Equation (12) which is comprised of the immediate expected reward (as given by Equation
(7)) and the transition probabilities (as given by Equation (6)). In Theorem 2, we show that the value function is
monotone in s2
t . This means for any given action, in each decision epoch t, as the average capacity increases, the
MDP value function will not decrease.

Theorem 2. The MDP value function of the stochastic SAIRP is monotonically non-decreasing in s2
t .

In Theorem 3, we prove that the value function is monotone in s1

t . This means for any given action in each
decision epoch t, as the number of fully-charged batteries increases, the MDP value function will not decrease. If
the demand for the MDP model is governed by a non-increasing discrete distribution, this result is implied from
the result of Widrick et al. [24]. However, we strengthen the result as we do not require a non-increasing discrete
distribution in Theorem 3.

Theorem 3. The MDP value function of the stochastic SAIRP is monotonically non-decreasing in s1
t .

When considering both dimensions simultaneously, in Theorem 4, we prove that the value function is mono-

tone in (s1

t , s2
t ).

Theorem 4. The MDP value function of the stochastic SAIRP is monotonically non-decreasing in (s1

t , s2
t ).

In a multi-dimensional setting, we need to deﬁne the concepts of partial ordering and partially non-decreasing
function, which are given by Deﬁnitions 2 and 3 in Appendix 7.2. We also refer the reader to Appendix 7.2 for
full details of the proofs of Theorems 2, 3, and 4.

5. Solution Methodology

This section presents the solution methods used to solve the stochastic Scheduling, Allocation, and Inventory
Replenishment Problem (SAIRP). First, we brieﬂy describe the dynamic programming solution methods with the
backward induction (BI) approach to provide exact solutions when the problem is not large-scale. Next, we present
the approximate dynamic programming methods to overcome the curses of dimensionality and yield high-quality
solutions for the stochastic SAIRPs.

5.1. Exact Solution Method: Dynamic Programming

Backward induction (BI) is an exact solution method to ﬁnd optimal policies for the Markov Decision Process
(MDP) problems [21]. Our goal is to ﬁnd the optimal policy π∗ that maximizes the expected total reward given
by Equation (11). We attribute the optimal value function, V ∗
t (st), to the optimal policy. We calculate the optimal
value based on cumulative values of taking the best actions onward from decision epoch t to N when in state st at

10

time t (see Equation (12)). We use Bellman equations as presented in Equation (12) to ﬁnd optimal policies and
corresponding optimal value functions for t = 1, . . . , N − 1 and st ∈ S .

Vt(st) = max
at∈Ast





rt(st, at) +



pt( j | st, at)ut+1( j)


.

(cid:88)

j∈S

(12)

The BI algorithm starts from t = N and sets VN(sN) = rN(S N) according to Equation (9). Then, it ﬁnds the
actions that maximize Vt(st) for every state st moving backward in time (t = N − 1, . . . , 1) using Equation (13).
The optimal expected total reward over the time horizon is V ∗
1 (s1) where s1 is the state of the system at the ﬁrst
decision epoch.

a∗
st,t

= arg maxat∈Ast





rt(st, at) +

(cid:88)

j∈S

pt( j|st, at)ut+1( j)





.

(13)

Now, we clarify terms used in the remainder of this paper. A sample path of demand is the collection of a
realized demand (uncertainty element) per time period generated from a given probability distribution. A sample
path of state is comprised of the collection of consecutive visited states, one per time period. To calculate the
visited states, we need the decision rule returned by a solution method for all states or visited states over time,
the sample path of demand, and the present state. Using this information, we use the state transition functions
(Equations (3) and (5)) to calculate the sample path of state. A sample path of policy is the set of consecutive
decision rules of the visited states of the system. We use the term instance to refer to an example of stochastic
SAIRP, speciﬁcally when we discuss the size of stochastic SAIRPs. The term scenario is used to refer to exam-
ples within our space-ﬁlling designed experiment that include different values for parameters. These values are
generated such that to cover the designed experiment space (see Section 6.3).

5.2. Approximate Dynamic Programming Solution Methods

In this section, we outline our monotone approximate dynamic programming algorithm with regression-based
initialization (MADP-RB). Approximate dynamic programming is a proven solution method that overcomes the
curses of dimensionality [72]. Using the foundation of the monotone approximate dynamic programming algo-
rithm proposed by Jiang and Powell [23], we make enhancements by exploiting our theoretical results to intelli-
gently approximate the initial value function approximation and update the approximation with each algorithmic
iteration.

), O(M2), O(M4N 1−θ

ε ), and O(MN 1−θ

The stochastic SAIRP suffers from the curses of dimensionality considering the size of all MDP elements
together. Asadi and Nurre Pinkley [22] showed that the size of the the state space, the action space, the transition
probability function, and the optimal policy are O( M(1−θ)
ε ), respectively,
ε
where M, N, ε, and θ are the number of batteries, the time horizon, the capacity increment, and the replacement
threshold, respectively. For instance, the size of the transition probability function is O(1015) for a realistic-sized
problem with M = 100 batteries, planning over a one month time horizon in one hour increments N = 744 with
discretized battery capacity in increments of ε = 0.001. Due to these large sizes, standard MDP solution methods,
such as backward induction (BI), were ineffective in solving realistic-sized instances of the stochastic SAIRP
[22]. Although there are many different ADP algorithms and approaches [72], there is no standard method to
link the best algorithm to solve any particular problem. However, using the problem structure is good practice
when developing efﬁcient and effective algorithms. As we proved in Theorems 2, 3, and 4, the value function is
monotonically non-decreasing in both dimensions, s1
t and s2
t . Hence, it is reasonable to utilize and enhance the
monotone approximate dynamic programming algorithm proposed by Jiang and Powell [23]. This algorithm has
already shown promising performance for several application areas [23]. We proceed by outlining the core steps
of [23]’s monotone approximate dynamic programming (MADP) algorithm while highlighting our additions and
changes to create the monotone approximate dynamic programming algorithm with regression-based initialization
(MADP-RB). To aid with the explanation, in Algorithm (1), we outline the MADP and underline the enhancements
for our MADP-RB. First, we introduce the notation necessary for the ADP algorithms in Table (1).

5.2.1. Monotone ADP with Regression-Based Initialization

In this section, we describe the core steps of the MADP algorithm proposed by Jiang and Powell [23] and our
enhancement using regression-based initialization in Algorithm (1). We display our enhancements in Algorithm
(1) with underlines to make it more clear for the reader. We proceed by explaining the implementation of the

11

Table 1: Notation used in the ADP algorithms

Notation

Description

maxIteration+1

The maximum number of regression-based initialization iterations

M

T

uiter
t

(st)

τ

V n
t (st)
n
t (st)
V
t (sn
ˆυn
t )
t (sn
zn
t )

The starting number of batteries used for the small SAIRPs solved using BI

The time horizon in the small SAIRP

The optimal value of being in state st at iteration iter and time t

The maximum number of core ADP iterations

The optimal value of being in state st at time t for iteration n

The approximate value of being in state st at time t for iteration n
The observed value of state sn

t at time t for iteration n

The smoothed value of being in state st at time t for iteration n

algorithm.

The Monotone ADP with Regression-Based Initialization (MADP-RB) has two main steps. In the ﬁrst step,
we intelligently initialize the value function approximation using a linear regression function. The coefﬁcients
of the regression function are derived from feeding the optimal solutions of small SAIRPs. The second step is
the core MADP algorithm that consists of updating the approximated values of visited and non-visited states over
time through an iterative process. The states are visited over time at each iteration using the information of the
present state, realized uncertainty, and taken action. The approximated value of the visited state is updated based
on the observed value and previous approximated value. At each iteration, the monotonicity operator updates the
approximated value of the non-visited states over time. We proceed by explaining each step in detail.

The ﬁrst step of the MADP algorithm is to initialize the value function approximation for all decision epochs
such that the monotonicity of the value function is preserved. Commonly, this is done by assigning a constant
0
t (st) for all st ∈ S and t = 1, . . . N − 1. However, using 0 or any constant value fails to exploit
value, e.g., 0, to V
how the monotone value function changes based on state and time. Thus, our enhancement to the MADP algorithm
is to intelligently approximate the initial value function approximation by exploiting the monotonicity of the value
function. To do so, we iteratively calculate the optimal value function for small but increasing problem instances.
Then, we use linear regression to approximate the initial value function for larger problem instances. To further
explain this enhancement, we outline how these steps can be applied to the stochastic SAIRP.

In the stochastic SAIRP, as M and T increase, the problem suffers from the curses of dimensionality. For
instance, the transition probability is O(1015) for a realistic-sized problem when M = 100, N = 744, and ε = 0.001.
Thus, we ﬁrst optimally solve small instances of the stochastic SAIRP with small numbers of batteries M (cid:28) M
and time periods T (cid:28) T . We repeat this step by slowly increasing the number of batteries by one until we reach
a user deﬁned maximum number of iterations (or until it is computationally infeasible to optimally solve small
instances with BI). For each instance, we determine the optimal value function and the associated trends based on
changes in the state, time, and number of batteries. From the value functions for smaller instances of the problem,
we use linear regression on the decision epoch (t), the state of the system (st), and the number of batteries in the
station (M) to approximate the initial value function approximation for larger problem instances. See lines 1-6 in
Algorithm (1) that describe this regression-based enhancement which are new and distinct from Jiang and Powell
[23]. We complete the initialization phase in line 7, where we set the approximate value for the terminal epoch for
all states and all core iterations n = 1, . . . , τ to the terminal reward (see Equation (9)).

The algorithm iteratively proceeds in accordance with [23] in lines 8-12. With each iteration, we select the best
action starting from a random initial state (line 10) and move forward in time (line 11) using a sample observation
n−1
t+1 ( j) (line 12). Speciﬁcally, we pick the best action
of uncertainty and the approximate value of the future state, V
(line 12) using Equation (15) and store the current observed value using Equation (16). In Equation (16), we use
the previous approximation of the future states, V

n−1
t+1 (st+1), as the approximation of E(Vt+1 | st, at).
(cid:27)
n−1
t+1 (st+1)

rt(st, at) + V

(cid:26)

.

an
st,t

= arg max
at∈Ast

t (sn
ˆυn

t ) = max
at∈Ast

{rt(st, at) + E(Vt+1 | st, at)}

In line 13, we use a combination of the current value function approximation, V

n−1
t

(sn

t ), and the current ob-

12

(15)

(16)

Algorithm 1 Monotone Approximate Dynamic Program with Regression-based Initilization

1: Initialize M batteries and T time periods, where M (cid:28) M and T (cid:28) T .
2: Set iteration counter iter = 0.
3: for iter ≤ MaxIterations do
4:

uiter
t

(st) ← Use backward induction to ﬁnd the value function for the problem with M + iter
batteries and T time periods, where M (cid:28) M and T (cid:28) T .

0
t (st) using linear regression on the combined uiter

t

(st) for all states at t = 1, . . . , N − 1.

n
N (s) = rN (s) for s ∈ S and n = 1, . . . , τ

5: end for
6: Initialize V

7: Set V
8: Set n = 1 n ≤ τ
9: Select initial state S n
1
10: for t = 1, . . . , N − 1 do
11:
12:

Sample an observation of the uncertainty, Dt, determine optimal action an
Smooth the new observation with the previous value,

t and future value ˆυn

t (sn
t ).

t (sn
zn

t ) = (1 − αn)V

n−1
t

(sn

t ) + αn ˆυn

t (sn
t )

(14)

Perform value function monotonicity projection operator as in Jiang and Powell (2015)
Determine next state, S n

t+1

13:
14:
15: end for
16: Increment n = n + 1

t (sn

t ) to calculate the smoothed value of being in state st, zn

served value, ˆυn
t ). This combination is weighted
based on a stepsize function, αn. Traditional stepsize functions, including 1/n [72] and harmonic [72, 83, 79],
usually smooth the value function using pure observations for early iterations and gradually put less weight on the
observation and put more on the approximations as the number of iterations increases.

t (sn

Next, we apply the monotonicity projection operator as deﬁned in Jiang and Powell [23] (line 14). We note
that the algorithm stores the value of all the states, and the monotonicity operator adjusts the value of non-visited
states according to the value of the visited state. It ensures that no approximated value violates the monotonicity
of the value function. For instance, consider the visited state st and an arbitrary state (cid:101)st such that st ≤ (cid:101)st and
V t(st) > V t((cid:101)st). The monotonicity operator increases the approximation for (cid:101)st up to V t(st) and preserves the
monotonicity property of the value function. Similarly, we decrease the approximation for lower states (i.e.,
(cid:101)st ≤ st) with higher value approximations than the visited state. Lastly, the algorithm moves forward in time to
the next decision epoch until the last decision epoch, wherein a new iteration begins. Every new iteration starts
from an arbitrary state and steps forward in time until the input number of iterations are completed.

5.2.2. Stepsize Function

Finding a good stepsize is a problem-dependent procedure that requires empirical experiments [72]. A stepsize
function, αn is used to scale the current observed value and (1 − αn) is used to scale the current value function
approximation. Because the value function approximation is often initially set to constant values and therefore
is not informative, many stepsizes start with higher αn values that emphasize the observed value. Then, as more
iterations are conducted, αn is decreased in order to place a greater emphasis on the value function approximation.
We note that, a stepsize function needs to satisfy the three basic conditions given by Powell [72] to guarantee
convergence. A basic example of such stepsize function is 1/n. Our preliminary experiments show that using the
1/n stepsize function is not appropriate as the rate of decreasing αn over iterations is too fast for SAIRPs. Instead,
we use the harmonic stepsize function [72] that uses a user-deﬁned parameter, w, to controls the rate of decrease
over iterations. In Equation (17), we provide the formal deﬁnition of the harmonic stepsize function.

w
w + n − 1
Additionally, we use the Search-Then-Converge (STC) stepsize rule that can control the rate of decrease in αn
by appropriately setting the parameter values [72]. Hence, the STC function is suitable for cases like ours that
need an extended learning phase [72]. The STC rule was initially proposed by Darken and Moody [84]; however,
we use the generalized STC formula given by George and Powell [85] presented in Equation (18).

αn =

(17)

αn = α0

(cid:18)

(cid:18)

(cid:19)

µ2
n

+ µ1

(cid:19)
+ µ1 + nζ − 1

µ2
n

13

(18)

The harmonic and STC stepsize functions follow the common process of weighting the observation higher
earlier and decreasing this weighting with each iteration. The harmonic and STC stepsize functions are classiﬁed
as deterministic as their values do not change based on the observations. In contrast, adaptive stepsize functions
are sensitive to changes in the observations. To broaden our investigation, we also use the adaptive stepsize ﬁrst
introduced in George and Powell [85] in Section 6.

6. Computational Results

In this section, we present the results and insights from computational experiments on different SAIRP instances.
First, we present the data used to solve modest and realistic-sized SAIRP instances. We clarify how we distinguish
modest, realistic-sized, and small SAIRP instances. We denote SAIRP instances as modest if they are optimally
solvable using backward induction (BI) (i.e., 7 batteries total and only a one-week time horizon). We denote
SAIRP instances as realistic-sized if they include larger numbers of batteries and the system is considered for
longer time horizons (i.e., 100 batteries over a one-month time horizon). Due to the curses of dimensionality,
the realistic-sized instances are not optimally solvable using BI. Within our monotone approximate dynamic pro-
gramming with regression-based initialization solution procedure, we initially solve small SAIRP instances that
are optimally solvable using BI in a matter of minutes (i.e., 2, 3, or 4 batteries over a one-week time horizon).
After explaining the data for these instances, we proceed by explaining in detail the regression-based initialization
used in MADP-RB, the Latin hypercube sampling (LHS) designed experiments, solution method comparison, and
solutions/insights for both the modest and realistic-sized SAIRPs.

6.1. Explanation of Data

For the computational results, we use realistic data representing the costs to recharge, discharge, and replace a
battery, the demand, and the battery degradation rate. To avoid redundancy in presenting similar insights for both
EVs and drones, we focus on drones. First, we present the associated data, and then we show the results of a
comprehensive set of experiments in Sections 6.3 and 6.4.

First, we set parameters associated with drone battery costs. We set the cost to recharge a battery using the
historical power prices from the Capital Region, New York area in 2016 [86]. We use the time frame with the
highest total power price for the modest and realistic-sized instances with the time horizon of a week and a month,
respectively. Hence, as displayed in Figure (2), we select December 12-18 and the month of December for the
modest and realistic-sized problems, using dashed and solid lines, respectively. We multiply these historical time-
varying power prices by the maximum capacity of a battery to calculate the non-stationary, time-varying costs to
recharge a battery. We assume a drone battery has a maximum capacity equal to 400 Wh as in the DJI Spreading
Wing S1000 battery [87]. We assume the revenue earned from discharging a battery back to the grid is equal to
the charge price. Consistent with level 2 or 3 battery charging [88, 64, 89], we assume a depleted (full) battery
takes one hour (i.e., time between two consecutive decision epochs) of recharging (discharging) to become full
(depleted). We use the purchase price for batteries to calculate the cost of battery replacement. Assuming the price
per kWh of a battery is approximately $235 [90], we set the replacement cost of a drone battery to be $100. This
calculated replacement cost serves as the baseline price, which is used and varied in the Latin hypercube designed
experiments in Section 6.3.

In the absence of real data representing the number of customers demanding swaps at the station over time,
we use the methodology of Asadi and Nurre Pinkley [22], [24], and [25] to derive the mean demand at the swap
station over time. We assume the mean demand, λt, is equivalent to the historical arrival of customers at Chevron
gas stations [91]. Using λt, we assume the demand follows a Poisson distribution where t is the hour of the day.
We scale λt to be in line with the number of batteries in the problem instance. Let λ(cid:48)
t be the scaled demand for
M(cid:48) number of batteries. Because λt is originally used for M = 7, we calculate λ(cid:48)
t values by multiplying M(cid:48)/7 by λt.
In Figure (3), we display the mean demand by hour over a one-week time horizon for the modest instances with
M = 7. For longer time horizons, we assume the mean arrival of demand repeats every week.

We use existing studies to calculate the battery degradation rate per cycle. Although there are several factors
that inﬂuence the battery degradation process, research states that the capacity fading has a linear behavior, espe-
cially in the ﬁrst 500 cycles [68, 67, 13, 64]. We note that the standard number of cycles for a drone Lithium-ion
battery is 300 to 500 (δC ≈ 0.1%) [92]. We select a higher value for the degradation rate to account for the elements
that accelerate the degradation process, such as temperature from continuous use and recharging in swap stations.
Thus, we select δC = 2% as the baseline degradation rate and vary this value in our computational experiments
to capture how changes in the degradation rate impact the policy and performance of the system. We note, the
model is robust in that future experiments can be conducted for different baseline δC values to represent different
14

Figure 2: Power price ﬂuctuations over December (Realistic-sized SAIRP) and the week of Dec. 12-18 (Modest Size SAIRP), 2016 in the
Capital Region, New York.

Figure 3: Mean demand for swaps over time.

degradation characteristics. In general, the industry-accepted battery end of life value is 80% of capacity. As
follows, we set the replacement threshold θ = 80% [71, 93]. We note that s2
t < 80% is equivalent to the absorbing
= {(0, 0)}. Hence, we can only replace batteries
state of the system s2
t
when the average capacity of batteries is not less than 80%.

= 0 wherein the feasible action set is A(s1

t ,0)

6.2. Regression-Based Initialization

In this section, we explain our regression-based method to initialize the MADP-RB intelligently. We examine
the empirical experiments of small SAIRPs and detect that the value function of the optimal policies Vt(st) is a
function of the decision epoch (t), the state of the system (st), and the number of batteries in the station (M).
To clarify, we display the optimal values Vt(st) of scenario 6 from the Latin hypercube designed experiments
presented in Section 6.3. We display two consecutive decision epochs in Figure (4). In this example, we compute
and show the optimal Vt(st) when we solve small SAIRPs with M = 2, 3, 4.

As shown in Figure (4), the horizontal axis denotes the states, and the vertical axis shows the corresponding
values. Among various techniques to estimate or forecast V 0
t (st), we want a fast and simple method to generate
and assign the initial approximations. Therefore, we propose using the linear regression function presented in
Equation (19) to initialize the approximated value function, V

0
t (st).

0
t (st) = h0 + h1 M + h2s1
t

V

+ h3s2
t

+ h4t

(19)

Having solved the small SAIRPs, we ﬁnd the appropriate values for h0, h1, h2, h3, and h4. In Sections 6.3 and 6.4,
we demonstrate how the intelligent initial value function approximation leads to superior results.

6.3. Latin Hypercube Designed Experiments for Modest SAIRPs

In this section, we perform a Latin hypercube sampling designed experiment that is used to assess the quality of
the solution methods and to deduce insights for the battery swap station application. A Latin hypercube sampling
(LHS) designed experiment is a space-ﬁlling design with broad application in computer simulation [94]. Using

15

Figure 4: An instance of the optimal values over states in two consecutive decision epochs

the LHS conﬁguration of Asadi and Nurre Pinkley [22], with the test set of 40 scenarios generated to cover the
design space of parameters, we ﬁrst quantify the performance of MBI, MADP, and MADP-RB.

In prior work, Asadi and Nurre Pinkley [22] was able to optimally solve 40 modest LHS scenarios using BI.
We run all computational tests using a high-performance computer with four shared memory quad Xeon octa-
core 2.4 GHz E5-4640 processors and 768GB of memory. Even on a high-performance computer, the largest
scenario Asadi and Nurre Pinkley [22] is able to optimally solve using BI is with M = 7, ε = 0.001, and N = 168
which represents a full week of operations where each decision epoch is one hour. Using these modest scenarios,
herein, we are speciﬁcally concerned with quantifying the speed and performance of MBI, MADP, and MADP-
RB against a known optimal policy and optimal expected total reward. The factors and their associated lower
and upper bounds are deﬁned as in Table (3) (we refer the reader to Asadi and Nurre Pinkley [22] for a complete
justiﬁcation for how these low and high values are calculated).

Memory Intensive Processing vs. Compute Intensive Processing. We can implement BI and MBI in two
ways. In the ﬁrst way, which we denote ‘Memory Intensive’, we calculate and store all of the transition probability
values, so they are readily available throughout the execution of the algorithm. In the second way, which we
denote ‘Compute Intensive’, we do not store any probability values and instead calculate each probability value
when needed for calculations through the execution of the algorithm. As is indicated in their names, the Memory
Intensive way requires more memory and the Compute Intensive way requires more time as it needs to calculate
probability values numerous times. We implement and use both ways to solve the modest SAIRP instances on
a high-performance computer (HPC). We note, on this HPC, we have different options. If we want to run the
algorithm for longer, we have up to 72 hours of computational time available per run with access to only 768 GB
of memory using four shared memory quad Xeon octa-core 2.4 GHz E5-4640 processors. However, if we want to
use more memory, we have access to 3 TB of memory (four shared memory Intel(R) Xeon(R) CPU E7-4860 v2
2.60GHz processors) but only 6 hours of computational time. Thus, our results reﬂect these limitations.

In Table (2), we report the memory used and computation times for BI and MBI by instance and method. We
use red to highlight the instances where we either exceeded the memory or time available and thus, do not ﬁnd
a solution. As is evident by the results, BI and MBI are not tractable solution methods for even modest-sized
SAIRPs. MBI does outperform BI; however, the computation time is signiﬁcant even for M = 10. Although not
shown in Table (2), when M ≥ 12, MBI exceeds the 72-hour time limit. We note that our approximate solution
methods do not run into memory issues. Hence, we move forward with the faster processing method, Memory
Intensive, to solve SAIRPs hereafter.

Table 2: Time and memory used for different size of SAIRPs using memory intensive and compute intensive methods.

Memory Intensive

Compute Intensive

Memory
Used (GB)

BI Computation MBI Computation

Time (h)

Time (h)

Memory
Used (GB)

BI Computation MBI Computation

Time (h)

Time (h)

830
1120
2030
> 3072

3.8
> 6
> 6
> 6

3.6
> 6
> 6
> 6

0.2
0.2
0.2
0.2

29.2
52.6
> 72
> 72

7.5
11.3
16.3
24.8

Size

M = 7
M = 8
M = 9
M = 10

We computationally test MBI for the general case when no limits are placed on how many batteries are replaced
per decision epoch and the MADP and MADP-RB methods with the harmonic, Search-Then-Converge (STC), and
adaptive stepsize functions. For the core ADP procedure, we run the scenarios for τ = 500000 iterations. For these

16

stepsize functions, based on extensive computational experiments, we present the most favorable results. For the
harmonic stepsize function, Powell [72] recommends that αn < 0.05 in the last iteration (τ = 500000). Thus,
we set ατ = 0.05 and w = 25000. We note, our tests show that the average optimality gap increases when w is
below 25000, thus we use w = 25000. For setting the STC parameters, we let α0 = 1 to put the total weight on
the observation at the beginning of the procedure. This results in α1 = 1. We test six values for µ1 = 10, 100,
500, 600, 1000, and 10000, ﬁve values for µ2 = 10, 100, 1000, 10000, and 100000, and 3 values for ζ = 0.6,
0.7, and 0.8 to adjust the tuple of parameters (µ1, µ2, ζ). Over these 90 experiments, we observe that (µ1, µ2, ζ) =
(600, 1000, 0.7) yields the best result in terms of the average optimality gap. Thus, we use this tuple of parameter
values for the STC stepsize. For the adaptive stepsize function, we use the setting presented in Powell [72]. In the
adaptive stepsize, we need to estimate the bias and its variance. For smoothing the observation and approximation
of the bias and its variance, Powell [72] recommends using a harmonic stepsize that tends to a value between 0.05
and 0.1 in the last iteration. Consistent with this recommendation, we use the harmonic stepsize function with
w = 25000. For the MADP-RB, we set M = 2, T = T = 168 hours, MaxIterations = 2 (i.e., 3 iterations are used as
the input for the linear regression and value of iter can be 0, 1, and 2), and use BI to optimally solve the scenarios
for 2, 3, and 4 batteries. We then plug these results into Equation (19) to set the initial approximation as in line 6
of Algorithm (1).

Table 3: Factors with associated low and high values for use in the Latin hypercube designed experiment.

Factor

Low High

Basic revenue per swap (β)

Replacement cost Lt
Battery degradation factor (δC)

1

2

0.005

3

100

0.02

0

Later, we will show that adding the regression-based initialization to MADP signiﬁcantly improves the quality
of solutions. One can argue that initialization using any monotone value function leads to the same result. We
tested this argument using a set of arbitrary monotone value functions and report the best-founded function. We
call the algorithm with this arbitrary monotone value function initialization MADP-M. We note that in MADP, the
0
t (st) = 0 ∀st ∈ S . In MADP-M, the monotone value function
initial value function is a constant and equals to V
t ) + k(N − t) where k is a constant. The ﬁrst term is a monotone
used for initialization is V
value function that equals the revenue generated from swapping all of the full batteries when in state st at time t.
The second term is a non-negative term with an opposite relationship with time, which means the value of being
in every state, st, is higher at earlier decision epochs. The logic behind adding the non-negative term is as follows.
The value of being in state st, Vt(st), accumulates the reward from time t onward to the end of the time horizon,
so we may expect to gain a higher reward (proﬁt) over a longer time. Although Vt(st) is not always decreasing in
t, the general trend is observed in the optimal values of small SAIRPs. Our tests show that MADP-M with k ≤ 1
outperforms MADP. Then, we test k = 0.1, 0.2, . . . , 1 and observe that MADP-M with k = 0.5 yields the best
result.

t (st) = (cid:0) β(1+s2

(cid:1)(s1

t −2θ)

1−θ

In Table (4), we summarize the 40 scenarios and the results comparing the approximate methods to backward
induction. Speciﬁcally, we calculate an average optimality gap over all scenarios, comparing MADP (monotone
approximate dynamic programming), MADP-M (monotone approximate dynamic programming algorithm with
arbitrary monotone value function initialization), MADP-RB (monotone approximate dynamic programming al-
gorithm with regression-based initialization), and Monotone Backward Induction (MBI) to the optimal expected
total reward calculated using BI. In Equation (20) for each scenario of MADP, MADP-M, and MADP-RB, we
input the expected total reward of the last iteration into the expected reward of the approximated method. Then,
we can calculate the average and maximum optimality gaps over 40 scenarios given by the last two rows of Table
(4).

Optimality Gap =

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Expected Reward BI - Expected Reward Approximated Method
Expected Reward BI

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

∗ 100%

(20)

Immediately evident from Table (4) is that MADP-RB led to signiﬁcantly smaller average and maximum
optimality gaps than Jiang and Powell’s [23] MADP, MADP-M, and MBI. Overall, we ﬁnd that MADP-M outper-
forms MADP in regards to the average optimality gap. This demonstrates that initializing an ADP approach, even
with an arbitrarily monotone value function does result in beneﬁt. However, we further ﬁnd that when considering
both the average and maximum optimality gaps, MADP-RB signiﬁcantly outperforms MADP-M. This demon-
strates the signiﬁcant beneﬁt of the regression-based initialization. We proceed by further analysis of MADP and
17

MADP-RB.

We note, MBI is a reasonable approximation for scenarios with high replacement costs, but it does not provide
competitive optimality gaps when replacement cost is low. However, when we limit the number of batteries
replaced in each epoch in accordance with Theorem 1, the optimality gap is 0.00%. MBI is smarter than BI
and can save computational time when searching for the best policies. In other words, we do not need to loop
over all the actions when using the monotonicity property of the optimal policy. However, as we showed, BI and
even MBI are not computationally tractable for realistically sized instances of SAIRPs, which necessitates using
approximate solution methods.

Table 4: Optimality Gap (%) of MADP, MADP-M, MADP-RB, and MBI

Harmonic Stepsize

STC Stepsize

Adaptive Stepsize

β
1.03
1.08
1.13
1.20
1.23
1.26
1.32
1.39
1.41
1.48
1.53
1.56
1.60
1.68
1.71
1.76
1.83
1.88
1.94
1.95
2.00
2.07
2.15
2.16
2.22
2.27
2.32
2.37
2.40
2.50
2.51
2.56
2.64
2.68
2.71
2.77
2.83
2.90
2.91
2.96

Lt
45
82
61
69
89
47
98
94
87
36
68
28
57
31
62
44
55
51
66
73
83
20
90
41
79
8
25
74
7
14
24
34
39
54
3
16
21
77
96
12

δc
0.009
0.011
0.005
0.009
0.008
0.010
0.019
0.011
0.014
0.006
0.016
0.018
0.010
0.017
0.006
0.017
0.016
0.019
0.012
0.019
0.012
0.013
0.013
0.007
0.015
0.007
0.017
0.014
0.018
0.008
0.015
0.009
0.010
0.016
0.007
0.012
0.013
0.015
0.020
0.005

Scenario
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
Avg Gap
Max Gap

MADP MADP-M MADP-RB
8.26
6.66
12.10
13.68
6.27
12.14
1.26
8.28
8.62
2.20
9.72
12.06
9.15
22.34
12.76
8.38
15.37
19.06
9.20
15.61
3.17
7.24
1.25
2.22
10.10
14.45
6.18
15.21
5.25
10.13
1.14
0.36
1.84
3.00
7.55
10.00
3.99
2.01
9.15
11.59
4.27
1.95
0.38
52.87
12.19
3.44
2.02
31.29
12.45
7.37
8.99
67.51
4.37
45.07
13.87
8.27
15.65
69.38
7.05
68.12
2.08
54.56
1.64
41.41
7.55
34.27
8.06
11.83
7.69
34.05
6.64
69.50
2.29
64.86
5.16
4.20
11.55
7.51
7.42
66.65
7.09
23.52
15.65
69.50

27.11
14.14
43.50
26.08
14.80
27.76
12.11
13.49
6.93
25.41
17.28
15.04
27.51
30.02
26.79
19.14
17.36
11.38
17.12
11.22
16.41
60.56
13.40
33.37
7.42
64.60
59.25
8.40
68.69
65.68
59.93
49.50
52.19
23.60
31.16
68.79
69.68
8.18
10.27
59.05
30.86
69.68

MADP MADP-M MADP-RB
10.45
8.57
10.58
12.19
7.54
17.28
3.00
10.24
7.88
0.53
10.95
13.51
9.32
22.8
10.82
7.97
14.55
18.87
9.58
15.97
1.60
6.56
0.06
0.78
11.84
15.74
4.92
15.29
4.62
10.78
0.20
0.32
1.13
2.39
6.57
10.51
3.06
2.35
8.47
10.74
3.95
3.06
1.59
54.87
11.46
2.44
3.07
29.17
12.03
7.45
8.91
70.09
5.03
40.87
12.80
7.62
15.22
72.84
2.48
67.59
1.12
54.09
2.67
42.90
6.50
36.56
8.13
12.74
7.45
33.44
6.62
68.35
3.12
63.10
5.48
3.62
11.18
7.61
6.93
67.87
6.82
23.74
15.22
72.84

30.67
16.09
47.01
30.10
18.62
30.10
13.73
16.33
9.39
33.78
18.05
15.37
31.56
28.94
33.69
20.91
17.63
13.74
18.23
13.51
19.50
53.56
15.41
27.58
10.11
68.96
59.72
8.27
72.46
68.96
48.80
36.87
49.33
23.82
31.22
71.81
69.44
9.62
10.91
63.61
31.94
72.46

MADP MADP-M MADP-RB
8.77
7.71
2.40
3.90
7.38
8.31
3.34
8.81
1.73
0.15
9.59
12.37
0.71
4.17
2.68
2.56
3.31
6.35
3.47
14.75
0.94
1.43
6.11
6.46
1.46
14.18
10.45
14.52
1.19
8.63
2.88
3.62
1.83
1.46
1.00
1.32
0.17
2.91
0.82
1.76
0.01
3.29
0.39
54.10
2.06
0.27
0.94
33.40
1.55
3.08
8.79
70.85
5.13
44.52
2.95
3.34
15.47
73.61
7.38
68.68
3.00
51.93
0.25
38.64
7.73
32.55
9.42
12.34
7.69
33.40
6.29
66.33
3.33
63.29
1.14
1.10
1.51
1.17
7.69
68.09
4.07
21.23
15.47
73.61

20.00
7.67
11.69
19.84
4.61
19.66
9.07
7.28
4.33
26.50
13.51
12.79
20.77
23.95
15.92
14.72
13.29
8.32
11.17
9.27
11.45
56.97
9.88
35.00
5.07
66.91
44.85
4.45
75.11
64.64
59.03
49.92
37.41
19.71
33.33
68.49
67.79
4.58
7.05
65.77
26.54
75.11

MBI

0.00
0.09
6.05
0.00
0.00
0.00
0.00
0.00
0.00
7.25
0.00
5.95
0.00
11.05
0.00
0.00
0.00
0.00
0.00
0.00
0.00
53.97
0.00
30.69
0.00
47.85
49.80
0.00
74.43
43.56
56.76
30.92
25.82
7.20
53.11
56.91
58.57
0.00
0.00
17.20
15.93
74.43

As is expected, ADP methods take signiﬁcantly less time than BI and MBI, which take on average 3.8 hours
and 3.6 hours, respectively. The computational time of MADP-RB includes three major operations in Algorithm
(1): (i) obtaining the data for the regression function in steps 1-5; (ii) deriving the regression function in step 6;
and (iii) calculating the monotone ADP results in steps 7-18. The average computational time for operations (i),
(ii), and (iii) are 902 seconds (0.25 hours), less than 1 second, and 1025 seconds (0.28 hours), respectively. We
note that (iii) is equivalent to the average computational time of MADP without the regression-based initialization.
In other words, if we execute the initialization steps ofﬂine, there is no difference between the computational time
of MADP and MADP-RB as ﬁnding the coefﬁcients of the regression function takes less than a second. As
the problem instance of the SAIRP increases signiﬁcantly, the required memory and time for BI and MBI also
increase signiﬁcantly. However, MADP and MADP-RB enable us to solve realistic-sized SAIRP instances, as we
demonstrate in Section 6.4.

Next, to compare the two ADP methods, we quantify a measure of convergence. One way to evaluate an ADP
method is to focus not only on the ending performance but also on the convergence with each iteration. Thus, we
calculate the average optimality gap over all iterations, gs
m, for each method m and scenario s. In this calculation,
we average the optimality gap for all iterations (τ) as in Equation (21).

gs
m

= 1
τ

τ(cid:88)

i=1

(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)Expected total reward at iteration i − Optimal expected total reward
Optimal expected total reward

∗ 100%

(21)

In Figure (5), we show the expected total reward (value) as a function of the number of iterations for MADP
and MADP-RB with the three different stepsizes. The rows of Figure (5) correspond to the harmonic, STC, and
adaptive stepsizes, respectively. The columns correspond to the scenarios which result in the low, average, and
high gs
m in the LHS where m includes MADP with the three stepsizes. Within each subﬁgure, we display three lines

18

Figure 5: Expected Total Reward Convergence of MADP and MADP-RB Using Different Stepsize Functions Vs Optimal Expected Reward.

showing the progression of the value by iteration for MADP and MADP-RB and the optimal value for the state
s1, which corresponds to the initial state of the swap station when all batteries are full and new with full capacity.
From the subﬁgures, we observe that MADP-RB consistently outperforms MADP in regards to convergence.

Now, we explain the possibility of convergence to a value greater than optimal (see Figures (5)-(I), (5)-(II),
and (5)-(III)). The approximated value depends on two factors: (i) the previous approximation value and (ii) the
present observation value. These values are smoothed using a stepsize function and set equal to the approximation
value for the next iteration. Hence, as either value can be greater than the optimal value in any iteration, the
smoothed value can take a value higher than the optimal value. For example, suppose the initial approximation is
too high. In that case, the smoothed value can be higher than the expected value of being in any state.

We also calculate the average optimality gap over iterations over scenarios for each method m using Equation
(22). In this equation, m = MADP and MADP-RB, and the total number of scenarios in our Latin Hypercube
designed experiments is 40.

gm =

(cid:80)40

s=1 gs
m
40

(22)

In Table (5), we display the average optimality gap over iterations over scenarios for MADP and MADP-RB
using different stepsize functions given by Equation (22). As shown in Table (5), the average optimality gap
associated with MADP-RB for all stepsizes is remarkably lower than MADP. This indicates that MADP-RB is
able to more quickly converge to the optimal value as compared to MADP.

Table 5: Comparison between convergence over iterations using MADP and MADP-RB.

Average Optimality Gap over Iterations over Scenarios (%)

MADP

MADP-RB

Harmonic

STC Adaptive

Harmonic

STC Adaptive

36.72

40.84

32.58

6.89

8.53

5.90

The required computation effort for both ADP approaches depends on, in part, the number of iterations, τ.
Thus, we analyze how the value function changes by iteration, which will inform our choice of τ for future
experiments. In Figure (5), we observe the value function plateaus before τ = 500000. Furthermore, we notice
that both methods do not signiﬁcantly change after τ = 100000 in many scenarios. When examining all scenarios
solved by MARP-RB, we calculate the percentage difference between the value at τ = 100000 and τ = 500000.
For the harmonic stepsize, the average percentage difference is 1.03% and the maximum percentage difference is
19

8.86%. We obtained similar values for the STC and adaptive stepsize and scenarios; however, to avoid redundancy,
we proceed with an analysis of the harmonic stepsize.

As the value does not always convey the operational changes in the policy which decision makers implement,
we proceed by analyzing the changes in policy at iteration 100000 and 500000. We use Equation (23) to compute
the percentage difference in the policies using at iteration 500000 and 100000 when 500 sample paths of realized
demand generated. With this equation, we calculate the percentage difference between the summation of the
recharging/discharging and replacement actions as we decrease the number of iterations for each scenario. We
then average this value over all scenarios. Speciﬁcally, in Equation (23), TCa
s represent the total number
of actions, recharging/discharging (a = a1) and replacements (a = a2), for scenario s using τ = 500000 and
τ = 100000, respectively. That is, TCa1
s denote the total number of recharging/discharging actions
and TCa2
s are the total number of replacements for scenario s using 500000 and 100000 iterations,
respectively. Our results show the average percentage difference in the number of recharged/discharged and
replaced batteries over 40 scenarios are 6% and 5%, respectively.

s and TCa2

s and TCa1

s and TCa

Average (%) difference for action a over all scenarios = 1
40

(cid:12)(cid:12)(cid:12)TCa

s − TCa
s
TCa
s

(cid:12)(cid:12)(cid:12)

40(cid:88)

s=1

∗ 100%.

(23)

Thus, as the changes in the value and policies are not signiﬁcant, we use τ = 100000 in our future computational
experiments for solving realistic-sized SAIRPs in Section 6.4.

Exploiting the monotonicity property does signiﬁcantly improve the quality of solutions and convergence
of ADP methods. To demonstrate this result, we run two versions of standard approximate value iteration (AVI)
without and with regression-based initialization, AVI and AVI-RB, respectively. Adding the monotonicity property
to AVI and AVI-RB converts them to MADP and MADP-RB, respectively. In Table (6), we present a summary
comparing AVI, MADP, AVI-RB, and MADP-RB using the harmonic stepsize function and 500000 iterations. In
this table, we present the average optimality gap of AVI, MADP, AVI-RB, and MADP-RB over all 40 scenarios of
our Latin hypercube designed experiments (see Table (4) for full details of these 40 scenarios). As is evident by
these results, using the monotonicity property decreases the average optimality gap. However, we observe that the
regression-based initialization is even more impactful than the monotonicity operator. By adding the monotonicity
operator, the average optimality gap decreases by 10.31% (AVI to MADP) but by adding the regression-based
initialization, the average optimality gap decreases by 26.42% (AVI to AVI-RB). The lowest average optimality
gap occurs with MADP-RB that has both the monotonicity property and regression-based initialization.

Table 6: Comparison between the average optimality gap of different approximate solution methods.

Approximate Method

AVI

MADP AVI-RB MADP-RB

Average Optimality Gap (%)

41.17% 30.86% 14.75%

7.09%

6.4. Monotone ADP Results and Performance for Realistic-sized SAIRPs

In this next set of computational experiments, we focus on the ability to solve and deduce insights from realistic-
sized stochastic SAIRPs. We proceed by determining the parameters, summarizing the results, including the
expected total reward and computational times, presenting sample paths of policies, and analyzing the relationship
between the outputs and inputs of the experiments.

For the test instances, we solve all 40 scenarios of the designed experiment presented in Section 6.3 using the
realistic data summarized in Section 6.1. Speciﬁcally, we consider 100 batteries, a one-month time horizon with
= (λt)(M(cid:48)/7) where M(cid:48) = 100 and λt is
each decision epoch representing one hour, and scaled mean demand λ(cid:48)
t
the original mean arrival of demand at time t in line with the original modest-sized problem with M = 7. Due to
the curses of dimensionality, BI and MBI are not capable of solving these large problems; thus, we focus on the
performance of MADP and MADP-RB with τ =100000 iterations.

In Table (7), we present the expected total reward and required computational time for the MADP and MADP-
RB methods with harmonic, Search-Then-Converge (STC), and adaptive stepsizes. The average computational
time of MADP-RB is only 8 minutes longer than MADP for all stepsize functions as a result of executing the
regression-based initialization (see steps 1-6 in Algorithm (1)). We highlight the highest expected reward for each
row of Table (7). We acknowledge that the highest expected total reward does not always indicate the lowest

20

optimality gap; however, we use this as a metric of comparison in the absence of being able to determine the
optimal solution and value for these realistic-sized SAIRPs.

The results are very clear and consistent. The MADP-RB value is always greater than MADP for all stepsize
functions and scenarios. Within MARP-RB, we observe that harmonic, STC, and adaptive stepsize generate the
highest expected total reward in 11, 3, and 26 scenarios, respectively.

Table 7: Results of realistic-sized SAIRPs for the latin hypercube designed experiment.

MADP Expected Total Reward ($)

MADP-RB Expected Total Reward ($)

Scenario
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40

Avg CPU
Time (hours)

Harmonic
3468.3
5404.1
9324.8
5397.1
4783.1
4824.0
3779.6
4479.5
2087.0
8461.4
3684.1
2888.5
6248.1
3253.2
9983.3
4446.9
4789.0
3958.3
5855.0
4090.8
7545.6
5454.8
5629.3
10593.2
4963.0
51874.6
5224.7
6366.9
39244.6
25188.0
5373.0
9216.7
9929.9
5920.8
1,00038.0
18743.7
8067.0
7726.4
5699.1
68536.0

8.5

STC
2791.7
4900.6
4219.4
3890.3
4923.2
2363.0
4415.5
3401.0
2546.6
8736.6
3385.4
2842.8
4061.9
2578.2
9573.4
3631.8
3875.1
3634.7
4561.5
3676.9
5562.7
5539.8
5707.7
10698.7
5349.2
48577.1
5512.7
5006.7
34101.7
24640.2
5971.0
10299.8
9783.1
4409.0
98060.7
17610.1
9426.7
5789.2
5683.8
63309.2

8.5

Adaptive
3639.3
4773.9
8788.3
5836.5
4986.2
5443.4
4422.3
3521.3
2730.5
7297.4
3749.2
3357.9
5994.1
3426.1
8814.8
4474.7
5372.4
4291.3
6722.5
2051.8
7116.9
5699.1
7111.5
11037.8
7186.8
50855.3
4699.4
6514.1
37832.0
26046.0
6832.8
10055.9
10293.0
6452.9
98254.7
16362.2
12138.2
8285.0
5168.2
62951.7

8.6

Harmonic
6964.3
7073.1
15443.0
8684.0
10308.5
7545.4
5269.7
8128.5
6848.6
22060.2
6581.5
5903.6
11620.6
7176.3
16484.4
7177.7
7804.2
6900.5
10514.8
7415.2
10685.1
33221.8
11848.4
34805.9
10505.9
84745.6
16876.2
11356.8
71877.0
79376.8
33432.0
44189.8
29615.5
11341.8
122397.0
73562.5
60294.6
12137.7
11029.9
112542.0

8.6

STC Adaptive
7884.1
7549.9
16491.2
9710.6
11079.1
8797.8
5212.2
9486.3
7780.6
20229.4
7889.3
5900.4
12950.7
7573.4
16614.3
7268.3
8787.8
7260.1
12124.5
7529.3
13478.4
32379.8
12811.4
32399.8
11664.4
85354.7
17203.0
12748.8
71396.5
80233.1
32693.5
43355.7
28786.7
13178.0
124779.0
73060.5
59488.7
14674.7
11158.6
112073.0

7303.6
7622.2
15840.8
9645.6
10597.4
8123.5
5176.2
9336.6
7446.5
19707.0
7463.9
6052.8
12228.4
7258.9
16326.9
7417.1
8601.2
6998.8
10892.3
7501.5
12764.6
31557.5
12005.5
32215.2
10074.7
83125.1
16771.0
12461.2
70920.1
77325.9
31564.1
42772.4
28922.0
12115.3
122683.0
72612.0
58483.0
14070.3
10982.9
111153.0

8.6

8.7

In addition to examining the expected total reward of the two ADP methods, we examine sample paths of the
policies when 500 sample paths of realized demand are generated. Our results show that the average percentage
of demand met levels off around when the number of sample paths is greater than 200. As a result and to be
conservative, for all scenarios, we calculate the average of demand met for 500 sample paths. We observe that the
average of demand met for 500 sample paths over all the scenarios under MADP-RB is 25%, 29%, and 19% more
than MADP with harmonic, STC, and adaptive stepsize functions, respectively. The optimal policies are similar
across these three stepsize functions, thus, we proceed by presenting further analysis for MADP-RB policy using
the harmonic stepsize function.

From the sample paths, we observe three typical behaviors within all LHS scenarios. In Figure (6), we dis-
played examples of the ﬂuctuations of the average capacity for these three categories when the realized demand
equals mean demand. In Type 1, the average capacity remains over 95% percent throughout the time horizon (see
Figure (6a). This is achieved with many replacement actions. The scenarios that exhibit Type 1 behavior have low
replacement costs and high revenue per swap (including scenarios 26, 29, 30, 35, 36, 40, 37, and 22). In Type
2 (see Figure (6b)), the average capacity does not stay as high as Type 1, but is maintained above 85% for the
entire time horizon except the very end of the month. For 500 sample paths of demand, on average, in scenarios
with Type 1 behavior, we observe replacement in 60% of the epochs and when a replacement occurs, 15% of the
batteries are replaced. In contrast, in scenarios with Type 2 behavior, replacement occurs in 41% of the decision
epochs and 7% of the batteries are replaced when a replacement occurs. The scenarios that exhibit Type 2 be-
havior have average replacement cost and revenue per swap values (includes scenarios 31, 27, 32, 24, 33, 10, and
15). In these scenarios, it is beneﬁcial to recharge batteries to ensure demand is met, but the station allows battery
capacity to degrade and does not replace batteries as frequently. Finally, in Type 3 (see Figure (6c)), the average
battery capacity consistently decreases over the time horizon to the replacement threshold of 80%. Fewer batteries
21

are replaced as, on average, we observe replacement in 11% of the decision epochs and only 3% of batteries are
replaced when a replacement occurs. This demonstrates a reduced priority for maintaining a high battery capacity.
At epochs when batteries are replaced, a small number of batteries are replaced at a time and are done to ensure
the minimum capacity is maintained. The scenarios that exhibit Type 3 behavior have high replacement costs and
low swapping revenue values.

Figure 6: Sample paths of average capacity over time horizon for three types of LHS scenario when realized demand equals means demand.

Next, we dig into a speciﬁc scenario to compare and contrast the policies for the modest and realistic-sized
instances. Our intent in this analysis is to determine whether similar actions are taken for the same scenario when
considering more batteries over a long time horizon. We observed this is not the case, thereby justifying the
need to solve realistic-sized instances. To demonstrate our observations, we present results for scenario 15 that
has meaningful differences with regard to the replacement actions, charging actions, and amount of demand met
between the modest and realistic-sized instances. In the realistic-sized problem, on average, we observe replace-
ment in 27% of the decision epochs, and when replacement occurs 4% of the batteries are replaced. However, the
derived policy in the modest problem consists of no replacement actions. With regard to charging, on average, we
see charging actions in 97% and 40% of decision epochs for the realistic-sized and modest problems, respectively.
With more frequent charging and replacement actions in the realistic-sized problem, the percentage of met demand
is higher than the modest problem. The derived policy for the realistic-sized problem satisﬁes 76% of demand,
which is signiﬁcantly higher than 54% of the demand met for the modest problem. From this analysis, while we
must solve a ﬁnite horizon MDP to capture the time-varying elements, it is necessary to be able to computationally
solve instances with longer time horizons and a larger number of batteries that mimic reality in order to determine
the general operating policies.

In Figure (7), we present a sample path of the policy when realized demand equals the mean demand for the
realistic-sized SAIRP associated with scenario 15. We observe that more batteries are replaced before epochs
with high power prices (e.g., 53% on day 12 before the high power price days of December 12-18), which is
consistent among Type 2 scenarios. When this replacement occurs, the battery capacity is higher, which results in
higher swapping revenue to offset to higher costs to recharge batteries. Overall, we observe a consistent trend for
recharging batteries and meeting demand during the time horizon. The number of full batteries is kept between
40% and 60% of the total number of batteries during the middle of the day and recharging is conducted to raise the
number of full batteries to 90% over night. We show in Figure (8) that the policy meets 100% of demand during
off-peak epochs and more than 50% during peak epochs.

7. Conclusions

We examined a stochastic scheduling, allocation, and inventory replenishment problem (SAIRP) for a battery
swap station where there is a direct link between the inventory level and necessary recharging and replacement
actions for battery charge and battery capacity. Speciﬁcally, the direct link is that the act of recharging a battery
to enable short-term operation is the exact cause for long-term battery capacity degradation. This creates a unique
problem where trade-offs between recharging and replacing batteries must be analyzed. We utilized a Markov
Decision Process (MDP) model for a battery swap station faced with battery degradation and uncertain arrival of
demand for swaps. In the MDP model of the stochastic SAIRP, we determine the optimal policy for charging,
discharging, and replacing batteries over time when faced with non-stationary charging prices, non-stationary

22

Figure 7: Sample path of the policy for scenario 15 when realized demand equals mean demand.

Figure 8: Demand and met demand for scenario 15 based on the sample path when realized demand equals mean demand.

discharging revenue, and capacity-dependent swap revenue. We prove theoretical properties for the MDP model,
including the existence of an optimal monotone policy for the second dimension of the state when there is an upper
bound placed on the number of batteries replaced in each decision epoch. Further, we prove the monotonicity of
the value function. Given these results, we solved the stochastic SAIRP using both backward induction (BI) and
monotone backward induction (MBI). However, we run into the curses of dimensionality as we are unable to ﬁnd
an optimal policy for realistic-sized instances.

To overcome these curses, we propose a monotone approximate dynamic programming (ADP) solution method
with regression-based initialization (MADP-RB). The MADP-RB builds upon the monotone ADP (MADP) algo-
rithm, which is ﬁtting for problems with monotone value functions [23]. In our MADP-RB, we initialize the value
function based on an intelligent approximation using regression. We used a Latin hypercube designed experiment
to test the performance of MBI, MADP, and MADP-RB. In the designed experiment, we examine both modest-
sized problem instances that are optimally solvable using BI and realistic-sized instances. Overall, we observed
that MADP-RB resulted in the greatest performance in terms of computational time, optimality gap, convergence,
and expected total reward.

Our investigation into SAIRPs opens the avenue for many opportunities for future work. In terms of the model
and solution method, researchers should examine state reduction/aggregation approaches and quantify how they
impact the quality of the solutions and computational time. Future research should consider different mechanisms
within a disaggregated MDP model that captures individual battery states and actions. Furthermore, it is valuable
to solve the disaggregated MDP using approximate solution methods and compare the insights with the presented
aggregated MDP. Moreover, it is interesting to study discretizing the state of charge of batteries and allow partial
recharging decisions.
For the battery swap station application, future work should consider different types
of charging options, multiple swap station locations, and multiple classes of demand dictated by how long the
battery must operate to satisfy the demand (e.g., how far a drone must ﬂy).

23

Acknowledgement

This research is supported by the Arkansas High Performance Computing Center which is funded through multiple
National Science Foundation grants and the Arkansas Economic Development Commission.

24

APPENDICES

7.1. Monotonicity of the Second Dimension of the State

In this Appendix, in Lemma 1, we prove that the stochastic SAIRPs violate the sufﬁcient conditions for the
optimality of a monotone policy in the second dimension of the state. Then, in Theorem 1, we prove the special
conditions under which a monotone policy in the second dimension of the state is optimal. First, we state Lemma 1
and then prove Lemmas 2, 3, 4, and 5 that we use to prove Lemma 1 and Theorem 1.

Lemma 1. The stochastic SAIRPs violate the sufﬁcient conditions for the optimality of a monotone policy in the
second dimension of the state.

Lemma 2. If s2

t ≥ (cid:101)s2

t and a2

t , a1

t , s1

t are constant then j2 ≥ (cid:101)j2.

t , a2

Proof. Using Equation (4), the second state of the system transitions from s2
t to (cid:101)j2 when action
at = (a1
t ) is taken. In Equation (4), we use the round() function to return values in the discretized state space.
Without loss of generality, we examine the state transition for the second state of the system without the round()
function, as the round() function does not alter the validity of the arguments. Thus, we have

t to j2 and from (cid:101)s2

j2 = s2

t+1

(cid:101)j2 = (cid:103)s2
t+1

(cid:104)

(cid:20)

= 1
M
= 1
M

t (M − a2
s2

t ) + a2

t − δC(a1+

t

t (M − a2
(cid:101)s2

t ) + a2

t − δC(a1+

t

+ a1−
t )

+ a1−
t )

(cid:105)

(cid:21)

,

.

(24)

We start from j2 ≥ (cid:101)j2 and reduce it to an always true statement.

(cid:104)

1
M

t (M − a2
s2
t (M − a2
s2

t ) + a2
t ) + a2

t

(cid:20)

j2 ≥ (cid:101)j2 ⇔
(cid:105)
t − δC(a1+
+ a1−
(cid:101)s2
≥ 1
t (M − a2
t )
M
t − δC(a1+
t ) + a2
+ a1−
t ) ≥ (cid:101)s2
t (M − a2
t
t ) ≥ (cid:101)s2
t (M − a2
t (M − a2
s2
t ).

t ) + a2

t − δC(a1+

t − δC(a1+

t

(cid:21)
+ a1−
t )

t
+ a1−

t ) ⇔

⇔

t ≤ M, (M − a2
As a2
non-negative value and get s2

t ≥ (cid:101)s2

t which is always true.

t ) is always non-negative. Thus, we can divide both sides of the last inequality by this

(cid:3)

(cid:107)

Lemma 3. If s2

t , a1

t , and θ are ﬁxed, then replacing less than

ξ = (cid:106) δC(a1+

t

causes a transition to absorbing state η.

+ a1−

t ) − M(s2
1 − s2
t

t − θ)

Proof. Transition from state s2

t to absorbing state η means s2

t+1 < θ. From Equation (24) we know that

s2
t+1

(cid:104)

= 1
M

Therefore,

t (M − a2
s2

t ) + a2

t − δC(a1+

t

(cid:105)
+ a1−
t )

.

s2
t+1

= j2 < θ ⇔

1
M

[s2

t (M − a2
t (M − a2
s2
t )a2
(1 − s2

t

t

t − δC(a1+
t ) + a2
t − δC(a1+
t ) + a2
t < Mθ + δC(a1+
t
Mθ + δC(a1+
t
1 − s2
t
+ a1−

δC(a1+
t

a2
t <

a2
t <

+ a1−

t )] < θ ⇔

+ a1−
+ a1−
+ a1−

t ) < Mθ ⇔
t ) − Ms2
t ⇔
t ) − Ms2
t

⇔

t ) − M(s2
1 − s2
t

t − θ)

.

25

Because a2

t is an integer, the upper bound can be derived using the ﬂoor function.
t < ξ = (cid:106) δC(a1+
a2

t − θ)

+ a1−

(cid:107)
.

t

t ) − M(s2
1 − s2
t

(cid:3)
In this Appendix, when we use the transition probability and reward functions with only the second dimension
t are

of the state and action as arguments, we assume that the ﬁrst dimension of the system is ﬁxed (i.e., s1
constants).
Lemma 4. For the stochastic SAIRP, qt(k | s2

t and a1

t , a2
t ) is a deterministic function equal to
(cid:40)

0
pt( j2 | s2

t , a2

t ) = 1

if
if

j2 < k,
j2 ≥ k.

qt(k | s2

t , a2

t ) =

Proof. As deﬁned in Section 3,

qt(k | s, a) =

∞(cid:88)

j=k

pt( j | s, a).

Hence, when the ﬁrst dimension of the system is ﬁxed, we have

qt(k | s2

t , a2

t ) =

∞(cid:88)

j2=k

pt( j2 | s2

t , a2
t ).

From the second state transition function given by Equation (5), the value of second state in the future, s2

j2, does not depend on Dt. From Lemma 3, if we start from s2
the absorbing state j2 = η, if

t and take action a2

=
t , the capacity will transition to

t+1

or it will transition to state j2 ≥ θ, if

a2
t <

(cid:107)

(cid:106)
ξ

(cid:107)

(cid:106)
ξ

a2
t ≥
Thus, this is a deterministic transition with into two intervals, (cid:2)η, j2(cid:3) and ( j2, 1(cid:3). If k ∈ ( j2, 1(cid:3), then qt(k | s2
equals zero. However, if k ∈ (cid:2)η, j2(cid:3), then k ≤ j2 and qt(k | s2

t ) equals 1.

t , a2

.

t , a2
t )

(cid:3)

We use the notation we deﬁne in Deﬁnition 1 in Lemma 5, Lemma 1, and Theorem 1.

Deﬁnition 1. The second dimension of the state transitions to the following state when starting from s2
action a2
t .

t and taking

j2
A

(cid:104)

= 1
M

t (M − a2
s2

t ) + a2

t − δC(a1+

t

+ a1−
t )

(cid:105)

.

(25)

The second dimension of the state transitions to the following state when starting from (cid:101)s2

t and taking action a2
t .

(cid:20)

j2
B

= 1
M

t (M − a2
(cid:101)s2

t ) + a2

t − δC(a1+

t

(cid:21)

.

+ a1−
t )

(26)

The second dimension of the state transitions to the following state when starting from s2

t and taking action (cid:101)a2
t .

(cid:20)

j2
C

= 1
M

s2
t (M − (cid:101)a2

t ) + (cid:101)a2

t − δC(a1+

t

(cid:21)

.

+ a1−
t )

(27)

The second dimension of the state transitions to the following state when starting from (cid:101)s2

t and taking action (cid:101)a2
t .

(cid:20)

j2
D

= 1
M

(cid:101)s2
t (M − (cid:101)a2

t ) + (cid:101)a2

t − δC(a1+

t

(cid:21)

.

+ a1−
t )

(28)

Lemma 5. If s2

t ≥ (cid:101)s2

t and a2

t ≥ (cid:101)a2

t , then j2

A ≥ max( j2

B, j2

C) ≥ min( j2
26

B, j2

C) ≥ j2

D .

Proof. Based on Deﬁnition 1, we need to show j2
follows we prove j2
with the claim and reducing it to an always true statement.

A ≥ j2

A ≥ j2

A ≥ j2

B ≥ j2

D, j2

C, j2

B, j2

A has the greatest value and j2
C ≥ j2

D has the lowest value. Thus, as
D. We proceed with a proof by cases starting

D, and j2

i.

A ≥ j2
j2
D

(cid:104)

1
M

t (M − a2
s2

t ) + a2

t − δC(a1+

t

A ≥ j2
j2
D ⇔
(cid:20)
(cid:105)
1
M

≥

+ a1−
t )

t (M − (cid:101)a2
(cid:101)s2

t ) + (cid:101)a2

t − δC(a1+

t

(cid:21)

⇔

+ a1−
t )

t (M − a2
s2

t ) + a2

t

+ a1−

t − δC(a1+
t a2
t − s2
Ms2
t
t ) + a2

t − (cid:101)s2

M(s2

+ a2

t ) + (cid:101)a2

t − δC(a1+

t

+ a1−

t ) ⇔

t ) ≥ (cid:101)s2

t − M (cid:101)s2
t

t (M − (cid:101)a2
+ (cid:101)s2

t (cid:101)a2

t − (cid:101)a2

t ≥ 0 ⇔

t (1 − s2

t ) − (cid:101)a2

t (1 − (cid:101)s2

t ) ≥ 0.

We know a2

t (1 − s2

t ) ≥ (cid:101)a2

t (1 − s2

t ), so it sufﬁces to show that

M(s2

t − (cid:101)s2

t ) + (cid:101)a2

t (1 − s2

t ) − (cid:101)a2

t (1 − (cid:101)s2

t ) ≥ 0 ⇔

M(s2

t − (cid:101)s2

t ) + (cid:101)a2

t ((cid:101)s2

t − s2

t ) ≥ 0 ⇔

(M − (cid:101)a2

t )(s2

t − (cid:101)s2

t ) ≥ 0.

Because M ≥ (cid:101)a2

t and s2

t ≥ (cid:101)s2

t the last statement is always non-negative and we prove our claim that j2

A ≥ j2
D.

ii.

A ≥ j2
j2
B

(cid:104)

1
M

t (M − a2
s2

t ) + a2

t − δC(a1+

t

+ a1−
t )

t (M − a2
(cid:101)s2

t ) + a2

t − δC(a1+

t

(cid:21)

⇔

+ a1−
t )

A ≥ j2
j2
B ⇔
(cid:20)
(cid:105)
1
M
t (M − a2

t ) ≥ (cid:101)s2

≥

t (M − a2
s2

t ) + a2

t

+ a1−

t − δC(a1+
t (M − a2
s2

t ) ≥ (cid:101)s2

t (M − a2
t ).

t ) + a2

t − δC(a1+

t

+ a1−

t ) ⇔

Because M ≥ (cid:101)a2

t we have

The last statement is always true and we prove our claim that j2

A ≥ j2
B.

s2
t ≥ (cid:101)s2
t .

iii.

A ≥ j2
j2
C

(cid:104)

1
M

t (M − a2
s2

t ) + a2

t − δC(a1+

t

s2
t (M − (cid:101)a2

t ) + (cid:101)a2

t − δC(a1+

t

(cid:21)

⇔

+ a1−
t )

t (M − a2
s2

t ) + a2

t (M − (cid:101)a2

t − δC(a1+

t

+ a1−

t ) ⇔

A ≥ j2
j2
C ⇔
(cid:20)
(cid:105)
1
M
t ) ≥ s2

≥

+ a1−
t )

+ a1−

t ) + a2
t ≥ s2
t − M + (cid:101)a2
t ) + a2

t − a2

t

t − δC(a1+
t (M − a2
s2

t (M − a2
s2

t ) + (cid:101)a2
t ) + (cid:101)a2

t ⇔

t ≥ 0 ⇔

t (M − (cid:101)a2
t ) + a2

t − (cid:101)a2

s2
t ( (cid:101)a2

t − (cid:101)a2

t ≥ 0 ⇔

(1 − s2

t )(a2

t − (cid:101)a2

t ) ≥ 0.

Because (1 − s2
A ≥ j2
j2
C.
So far we know j2
B ≥ j2
show j2

D and j2

t ) ≥ 0 and (a2

t − (cid:101)a2

t ) ≥ 0 the last statement is always non-negative and we prove our claim that

A ≥ max( j2
C ≥ j2

C) ≥ min( j2
D. First, we show j2

C) and j2
B, j2
B ≥ j2
D.

B, j2

A ≥ j2

D. Now, we show min( j2

B, j2

C) ≥ j2

D. It sufﬁces to

iv.

B ≥ j2
j2
D

B ≥ j2
j2

D ⇔

27

(cid:20)

1
M

t (M − a2
(cid:101)s2

t ) + a2

t − δC(a1+

t

+ a1−
t )

(cid:21)

≥

(cid:20)

1
M

t (M − (cid:101)a2
(cid:101)s2

t ) + (cid:101)a2

t − δC(a1+

t

(cid:21)

⇔

+ a1−
t )

t (M − a2
(cid:101)s2

t ) + a2

+ a1−

t ) ≥ (cid:101)s2

t (M − (cid:101)a2

t − δC(a1+

t

+ a1−

t ) ⇔

t

t − δC(a1+
t (M − a2
(cid:101)s2

t ) + (cid:101)a2
t ) + (cid:101)a2

t ⇔

t ) + a2

t ≥ (cid:101)s2
t ) + (a2

t ( (cid:101)a2
(cid:101)s2

t − a2

t (M − (cid:101)a2

t − (cid:101)a2

t ) ≥ 0 ⇔

(1 − (cid:101)s2

t )(a2

t − (cid:101)a2

t ) ≥ 0.

t ) ≥ 0 and (a2

t − (cid:101)a2

t ) ≥ 0 the last statement is always non-negative and we prove our claim that

Because (1 − (cid:101)s2
B ≥ j2
j2
D.
C ≥ j2
j2
D

v.

(cid:20)

1
M

s2
t (M − (cid:101)a2

t ) + (cid:101)a2

t − δC(a1+

t

+ a1−
t )

t (M − (cid:101)a2
(cid:101)s2

t ) + (cid:101)a2

t − δC(a1+

t

(cid:21)

⇔

+ a1−
t )

C ≥ j2
j2
D ⇔
(cid:20)
(cid:21)
1
M

≥

s2
t (M − (cid:101)a2

t ) + (cid:101)a2

+ a1−

t ) ≥ (cid:101)s2

t (M − (cid:101)a2

t − δC(a1+

t

+ a1−

t ) ⇔

t

t − δC(a1+
s2
t (M − (cid:101)a2

t ) + (cid:101)a2

t ≥ (cid:101)s2

t (M − (cid:101)a2

t ) + (cid:101)a2
t ) + (cid:101)a2

t ⇔

s2
t (M − (cid:101)a2

t ) ≥ (cid:101)s2

t (M − (cid:101)a2

t ) ⇔

s2
t ≥ (cid:101)s2
t .

The last statement is always true and we prove our claim that j2
B, j2
D. From i, ii, iii, iv, and v, we prove j2
min( j2

A ≥ max( j2

C) ≥ j2

B, j2

C ≥ j2
C) ≥ min( j2

B, j2

C) ≥ j2
D.

D. From iv and v, we conclude that

(cid:3)
Using Lemmas 2, 3, 4, and 5, we prove in Lemma 1 that the stochastic SAIRPs violate the sufﬁcient conditions
for the optimality of a monotone policy in the second dimension of the state. Then, in Theorem 1, we prove the
special conditions under which a monotone policy is optimal in the second dimension of the state.

Lemma 1. The stochastic SAIRPs violate the sufﬁcient conditions for the optimality of a monotone policy in the
second dimension of the state.

Proof. First, we need to show that at least one of the following conditions is not always satisﬁed for our problem.
t ) is not subadditive on S 2 × A(cid:48) for every k ∈ S 2
We will prove that condition 4 is not satisﬁed; That is, qt(k | s2
where A(cid:48) is the set of possible actions in the second dimension, independent of the state of the system.

t , a2

1. rt(s2
If s2

t , a2
t ≥ (cid:101)s2

t for all a2
t , a2

t ) is non-decreasing in s2
t , it sufﬁces to show that rt(s2
t ) = β(1 + s2
1 − θ

t , a2

rt(s2

t ∈ A(cid:48).
t ) ≥ r2

t ((cid:101)s2

t , a2

t ). From Equations (7) and (8) we know

t − 2θ)

(cid:104)

min{Dt, s1

t − a1−
t }

(cid:105)

− Kta1+

t

+ Jta1−

t − Lta2
t .

We start with our claim, rt(s2

t , a2

t ) ≥ rt((cid:101)s2

t , a2

t ), and reduce it to an always true statement.

(cid:104)

t −2θ)

β(1+s2
1−θ

min{Dt, s1

t − a1−
t }

(cid:105)

− Kta1+
t

+ Jta1−

t − a1−
t }

(cid:105)

− Kta1+
t

+ Jta1−

t − Lta2

t ⇔

t ) ⇔
(cid:104)
min{Dt, s1

rt(s2
t , a2
t − Lta2
β(1+s2
t −2θ)
1−θ

t , a2
t −2θ)

t ) ≥ rt((cid:101)s2
t ≥ β(1+ (cid:101)s2
1−θ
≥ β(1+ (cid:101)s2
t −2θ)
1−θ ⇔
t ≥ (cid:101)s2
s2
t .

2. qt(k | s2
t ≥ (cid:101)s2
If s2

t ) is non-decreasing in s2

t , a2
t , we seek to show

t for all k ∈ S 2 and a ∈ A(cid:48).

qt(k | s2

t , a2

t ) ≥ qt(k | (cid:101)s2

t , a2
t ).

It is sufﬁcient to show that if j2
in Lemma 2 and prove our claim.

A ≥ j2

B from Deﬁnition 1, then qt(k | s2

t , a2

t ) ≥ qt(k | (cid:101)s2

t , a2

t ). We show j2

A ≥ j2
B

28

3. rt(s2

t ) is a subadditive function on S 2 × A(cid:48).
t , a2

t , a2
For rt(s2
batteries is less when the average capacity is greater. Consider, s2

t ) to be subadditive, it means the incremental effect on the expected total reward of replacing less
t . It sufﬁces to show that

t and a2

t ≥ (cid:101)a2

t ≥ (cid:101)s2

rt(s2

t , a2

t ) + rt((cid:101)s2

t , (cid:101)a2

t ) ≤ rt(s2

t , (cid:101)a2

t ) + rt((cid:101)s2

t , a2

t ) ⇔

P(Dt = j)

t − 2θ)

β(1 + s2
1 − θ

(cid:104)
min{ j, s1

t − a1−
t }

(cid:105)

− Kta1+

t

+ Jta1−

t − Lta2
t

P(Dt = j)

t − 2θ)

β(1 + (cid:101)s2
1 − θ

(cid:104)

min{ j, s1

(cid:105)
t − a1−
t }

− Kta1+

t

+ Jta1−

t − Lt (cid:101)a2

t ≤

P(Dt = j)

t − 2θ)

β(1 + s2
1 − θ

(cid:104)
min{ j, s1

t − a1−
t }

(cid:105)

− Kta1+

t

+ Jta1−

t − Lt (cid:101)a2
t

P(Dt = j)

t − 2θ)

β(1 + (cid:101)s2
1 − θ

(cid:104)
min{ j, s1

t − a1−
t }

(cid:105)

− Kta1+

t

+ Jta1−

t − Lta2
t .

∞(cid:88)

j=0

∞(cid:88)

j=0
∞(cid:88)

+

j=0

∞(cid:88)

j=0

+

As the right-hand side and the left-hand side of the inequality are the same, we prove our claim.

4. qt(k | s2

t , a2

t ) is subadditive on S 2 × A(cid:48) for every k ∈ S 2. It sufﬁces to show that

qt(k | s2

t , a2

t ) + qt(k | (cid:101)s2

t , (cid:101)a2

t ) ≤ qt(k | (cid:101)s2

t , a2

t ) + qt(k | s2

t , (cid:101)a2
t ),

for every k ∈ S 2 if s2

t ≥ (cid:101)s2

t and a2

t ≥ (cid:101)a2
t .

From Lemma 4,

(cid:40)

qt(k | s2

t , a2

t ) =

0
pt( j2 | s2

t , a2

t ) = 1

if
if

j2 < k,
j2 ≥ k.

t transfers to j2

From Lemmas 3 and 4, the starting state s2
t , respectively. If
the number of batteries replaced is greater than a threshold ξ, then the future state is at least the capacity
threshold, θ. Otherwise, we will transfer to the absorbing state, η. Similarly, the starting state (cid:101)s2
t transfers
B or j2
to j2
t , respectively. If the number of batteries replaced is greater than a
threshold
(cid:101)ξ = (cid:106) δC(a1+

D, if we take action a2

C, if we take action a2

t or (cid:101)a2

A or j2

t − θ)

+ a1−

t or (cid:101)a2

(cid:107)
,

t

t ) − M((cid:101)s2
1 − (cid:101)s2
t

then the future state is at least θ. Otherwise, we will transfer to the absorbing state, η.
Recall in Lemma 5 and Deﬁnition 1 we have:

(cid:104)

(cid:20)

(cid:20)

(cid:20)

j2
A

= 1
M

j2
B

j2
C

j2
D

= 1
M
= 1
M
= 1
M

and

t (M − a2
s2

t ) + a2

t − δC(a1+

t

t (M − a2
(cid:101)s2

t ) + a2

t − δC(a1+

t

s2
t (M − (cid:101)a2

t ) + (cid:101)a2

t − δC(a1+

t

(cid:101)s2
t (M − (cid:101)a2

t ) + (cid:101)a2

t − δC(a1+

t

(cid:105)
+ a1−
t )

,

(cid:21)
+ a1−
t )

(cid:21)
+ a1−
t )

(cid:21)
+ a1−
t )

,

,

,

A ≥ max( j2
j2

B, j2

C) ≥ min( j2

B, j2

C) ≥ j2
D.

To show the subadditivity property of the qt(k | s2
t , a2
(cid:105)
C), j2
intervals. Then, we show that if k ∈
the subadditivity condition is not satisﬁed.
A

t ) for every k ∈ S 2, we divide S 2 space into ﬁve

max( j2

B, j2

(cid:16)

29

i. 0 ≤ k ≤ j2
D

ii.

D < k ≤ min( j2
j2

B, j2
C)

qt(k | s2

t , a2

t ) + qt(k | (cid:101)s2

t , a2

t ) + qt(k | s2

t , (cid:101)a2

t ) ⇔

t , (cid:101)a2
t ) ≤ qt(k | (cid:101)s2
1 + 1 ≤ 1 + 1.

qt(k | s2

t , a2

t ) + qt(k | (cid:101)s2

t , (cid:101)a2

t ) ≤ qt(k | (cid:101)s2

t , a2

t ) + qt(k | s2

t , (cid:101)a2

t ) ⇔

iii. min( j2

B, j2

C) < k ≤ max( j2

B, j2
C)

1 + 0 ≤ 1 + 1.

qt(k | s2

t , a2

t ) + qt(k | (cid:101)s2

t , (cid:101)a2

t ) ≤ qt(k | (cid:101)s2

t , a2

t ) + qt(k | s2

t , (cid:101)a2

t ) ⇔

iv. max( j2

B, j2

C) < k ≤ j2
A

1 + 0 ≤ 1 + 0.

qt(k | s2

t , a2

t ) + qt(k | (cid:101)s2

t , (cid:101)a2

t ) ≤ qt(k | (cid:101)s2

t , a2

t ) + qt(k | s2

t , (cid:101)a2

t ) ⇔

v.

j2
A < k

1 + 0 (cid:2) 0 + 0.

qt(k | s2

t , a2

t ) + qt(k | (cid:101)s2

t , a2

t ) + qt(k | s2

t , (cid:101)a2

t ) ⇔

t , (cid:101)a2
t ) ≤ qt(k | (cid:101)s2
0 + 0 ≤ 0 + 0.

Due to the result of part iv, we can not conclude that qt(k | s2

t , a2

t ) is subadditive on S 2 × A(cid:48) for every k ∈ S 2.

5. rN(sN) is non-decreasing in s2
N.
N ≥ (cid:102)s2

Consider s2

N, it sufﬁces to show that rN(sN) ≥ rN((cid:102)sN) where:

(cid:40)

rN(sN) =

s1
N

ρs2
0

N

s2
N ≥ θ,
if
otherwise.

(29)

We examine the three following cases.

i. s2

N ≥ (cid:102)s2

N ≥ θ

ii. s2

N ≥ θ > (cid:102)s2

N

rN(sN) ≥ rN((cid:102)sN) ⇔
s1
s1
N ⇔
N ≥ ρ(cid:102)s2
ρs2

N

N

β(1 + s2

N − 2θ)

1 − θ

s1
N ≥

N − 2θ)

β(1 + (cid:102)s2
1 − θ

s1
N ⇔

s2
N ≥ (cid:102)s2
N.

rN(sN) ≥ rN((cid:102)sN) ⇔

ρs2

N

s1
N ≥ 0 ⇔

β(1 + s2

N − 2θ)

1 − θ

β(1 + s2

N − 2θ)

s1
N ≥ 0 ⇔

s1
N ≥ 0 ⇔

1 − θ
1 + s2
N − 2θ ≥ 1 + θ − 2θ = 1 − θ and 1 − θ ≥ 0 we can conclude that:

N − 2θ ≥ 0.

Because 1 + s2

N − 2θ ≥ 0.

1 + s2
30

iii. θ > s2

N ≥ (cid:102)s2

N

rN(sN) ≥ rN((cid:102)sN) ⇔
0 ≥ 0.

We show that we can not prove the subadditivity condition and in turn, we proved that the stochastic SAIRPs

violate the sufﬁcient conditions for the optimality of a monotone policy in the second dimension of the state.

(cid:3)

Theorem 1. There exist optimal decision rules d∗
t : S → Ast for the stochastic SAIRP which are monotone non-
increasing in the second dimension of the state for t = 1, . . . , N − 1 if there is an upper-bound U on the number
of batteries replaced at each decision epoch where U = Mε
, when M is the number of batteries at the swap
2(1−s2
t )
station and ε is the discretized increment in capacity.

Proof. We prove this monotonicity claim by showing the aforementioned ﬁve conditions are satisﬁed [21]. When
we ﬁx the ﬁrst dimension of the state and action, in Lemma 1, we show that conditions i, ii, iii, and v are true.
Thus, it sufﬁces to prove condition iv is true in order to show the existence of monotone optimal decision rules for
the second dimension. First, we prove the following claim and use it to show the subadditivity condition and in
turn, monotonicity.

Claim 1.

A and j2
j2
Proof. We know j2

C represents the same point if a2
A and j2

C represents the same point if j2

t − (cid:101)a2

t ≤ U.

C ≤ ε

2 due to the precision in rounding. Thus,

A − j2
ε
2

⇔

A − j2
j2

C ≤

(cid:104)

1
M

t (M − a2
s2

t ) + a2

+ a1−
t )

(cid:105)

−

(cid:20)

1
M

s2
t (M − (cid:101)a2

t − δC(a1+

t

+ a1−
t )

(cid:21)

≤

ε
2

⇔

t − δC(a1+
(cid:20)

t

1
M

t (M − a2
s2
(cid:20)

1
M

t ) + (cid:101)a2
(cid:21)

≤

≤

⇔

ε
2

t − (cid:101)a2
t
(cid:21)

ε
2

⇔

t − (M − (cid:101)a2

t ) + a2

t (−a2
s2
t
(cid:20)

+ (cid:101)a2

t ) + a2

t − (cid:101)a2
t
(cid:21)
t )(1 − s2
t )

≤

(a2

t − (cid:101)a2

1
M

ε
2

⇔

(a2

t − (cid:101)a2

t ) ≤

Mε
2(1 − s2
t )

.

Let (cid:101)a2
t

= 0 to get the least upper-bound for the number of batteries to be replaced, we will have

a2
t ≤

Mε
2(1 − s2
t )

.

(cid:3)

t ) is subadditive on S 2 × A(cid:48) for every k ∈ S 2.

With the case that j2
A

= j2

C, we have

C ≥ j2
Now, we revisit the condition, we seek to show that qt(k | s2
It sufﬁces to show that

= j2

j2
A

B ≥ j2
D.
t , a2

qt(k | s2

t , a2

t ) + qt(k | (cid:101)s2

t , (cid:101)a2

t ) ≤ qt(k | (cid:101)s2

t , a2

t ) + qt(k | s2

t , (cid:101)a2
t ),

for every k ∈ S 2 if s2
We divide S 2 space into four intervals and show the condition is satisﬁed for every interval.

t and a2

t ≥ (cid:101)a2
t .

t ≥ (cid:101)s2

i. 0 ≤ k ≤ j2
D

qt(k | s2

t , a2

t ) + qt(k | (cid:101)s2

t , a2

t ) + qt(k | s2

t , (cid:101)a2

t ) ⇔

t , (cid:101)a2
t ) ≤ qt(k | (cid:101)s2
1 + 1 ≤ 1 + 1.

31

ii.

D < k ≤ j2
j2
B

iii.

B < k ≤ j2
j2
C

= j2
A

iv.

j2
A < k

qt(k | s2

t , a2

t ) + qt(k | (cid:101)s2

t , a2

t ) + qt(k | s2

t , (cid:101)a2

t ) ⇔

t , (cid:101)a2
t ) ≤ qt(k | (cid:101)s2
1 + 0 ≤ 1 + 1.

qt(k | s2

t , a2

t ) + qt(k | (cid:101)s2

t , a2

t ) + qt(k | s2

t , (cid:101)a2

t ) ⇔

t , (cid:101)a2
t ) ≤ qt(k | (cid:101)s2
1 + 0 ≤ 1 + 0.

qt(k | s2

t , a2

t ) + qt(k | (cid:101)s2

t , a2

t ) + qt(k | s2

t , (cid:101)a2

t ) ⇔

t , (cid:101)a2
t ) ≤ qt(k | (cid:101)s2
0 + 0 ≤ 0 + 0.

We conclude that qt(k | s2

t ) is subadditive on S 2 × A(cid:48) for every k ∈ S 2. As all conditions are valid, we
deduce that there exists monotone optimal decision rules in the second dimension of the state for the stochastic
SAIRP when there is an upper-bound U on the number of batteries replaced at each decision epoch.

t , a2

(cid:3)

32

7.2. Monotonicity of Value Functions

In this Appendix, we prove that the MDP value function for the stochastic SAIRP is monotone non-decreasing
in the ﬁrst, second, and both dimensions of the state. In Theorems 2 and 3, we show the monotonicity of the
value function regarding s2
t , respectively, and in Theorem 4 we prove that the value function is monotone
in (s1
t ). To prove these theorems, we need to ensure that four conditions are satisﬁed as given by Papadakia and
Powell [82] and Jiang and Powell [23]. These two articles use different notation, thus, for clarity, we deﬁne S as
the state space, A as the action space, and the transition function as f : S × A → S . To prove the theorems, we
ﬁrst state key deﬁnitions used.

t and s1

t , s2

Deﬁnition 2. Partial ordering operator (cid:22) on the N-dimensional set S is deﬁned as s (cid:22) s(cid:48) for any s, s(cid:48) ∈ S , if
s(i) ≤ s(cid:48)(i) for all i ∈ {1, 2, . . . N} [82].

Deﬁnition 3. An N-dimensional real-valued function F is partially non-decreasing on the set S , if for all s−, s+ ∈
S where s− (cid:22) s+, we have F(s−) ≤ F(s+) [82].

Theorem 2. The MDP value function of the stochastic SAIRP is monotonically non-decreasing in s2
t .

Proof. Fixing the ﬁrst dimension of the state space, the MDP value function is monotone if the following four
conditions are satisﬁed [82, 23].

1. For e (cid:23) 0 we have f (s + e, a) (cid:23) f (s, a) for all a ∈ A [82]. Equivalently, if every s2

t , (cid:101)s2

t ∈ S 2 with s2

t ≥ (cid:101)s2
t ,

action at = (a1

t , a2

t ) ∈ A is taken, and demand equals Dt, the second state transition function f 2 satisﬁes:

f 2(s1

t , s2

t , a1

t , a2

t ) ≥ f 2(s1

t , (cid:101)s2

t , a1

t , a2
t ).

From Equation (24), if the beginning state is s2
t , a1
equals (cid:101)j2. Because a2

t , s1

t are constant, using Lemma 2 we prove our claim as

t , then f 2 equals j2, and if the beginning state is (cid:101)s2

t , then f 2

f 2(s1

t , s2

t , a1

t , a2

t ) = j2 ≥ (s1

t , (cid:101)s2

t , a1

t , a2

t ) = (cid:101)j2.

2. The one period cost function rt(s, a) is partially non-decreasing in s ∈ S for all a ∈ A, t = 0, 1, . . . , N − 1
t ) ∈ A, then

t , if we take action at = (a1

t ∈ S 2 with s2

t ≥ (cid:101)s2

t , a2

t , (cid:101)s2

[82]. It is sufﬁces to show for every t < N, s2
the one period reward function satisﬁes

We show that we can reduce Equation (30) to an always true statement.

rt(s1

t , s2

t , a1

t , a2

t , Dt) ≥ rt(s1

t , (cid:101)s2

t , a1

t , a2

t , Dt).

(cid:104)

t −2θ)

β(1+s2
1−θ

min{Dt, s1

t − a1−
t }

(cid:105)

rt(s1
− Kta1+
t

t , a1
t , s2
+ Jta1−

t , a2
t , a1
t , Dt) ⇔
(cid:104)
min{Dt, s1

t − a1−
t }

t , (cid:101)s2
t −2θ)

t , a2
t − Lta2
β(1+s2
t −2θ)
1−θ

t , Dt) ≥ rt(s1
t ≥ β(1+ (cid:101)s2
1−θ
≥ β(1+ (cid:101)s2
t −2θ)
1−θ ⇔
t ≥ (cid:101)s2
s2
t .

(30)

(cid:105)

− Kta1+
t

+ Jta1−

t − Lta2

t ⇔

3. The terminal cost function rN(s) is partially non-decreasing in s ∈ S [82]. It sufﬁces to show that for every

N ∈ S 2 with s2

N, (cid:102)s2
s2
We proved this claim in condition (5) of Lemma 1 and Theorem 1.

N, that rN(sN) ≥ rN((cid:102)sN).

N ≥ (cid:102)s2

4. For each t < N, s2

t and Dt+1 are independent [23].

In our model, demand is a random variable and does not depend on the current state or action, including s2
t .

As all the conditions are valid for the stochastic SAIRP, we can conclude that the value function is monotone in
s2
t .

(cid:3)

Theorem 3. The MDP value function of the stochastic SAIRP is monotonically non-decreasing in s1
t .

33

Proof. Fixing the second dimension of the state space, the MDP value function is monotone if the following four
conditions are satisﬁed [82, 23].

1. For e (cid:23) 0 we have f (s + e, a) (cid:23) f (s, a) for all a ∈ A [82]. It sufﬁces to show for every s1

t ∈ S 1 with
t ) ∈ A and demand equals Dt, the ﬁrst state transition function f 1

t , (cid:101)s1

t , if we take action at = (a1

t , a2

t ≥ (cid:101)s1
s1
satisﬁes

f 1(s1

t , s2

t , a1

t , a2

t , Dt) ≥ f 1((cid:101)s1

t , s2

t , a1

t , a2

t , Dt).

Using Equation (3), if the beginning state is s1
equals (cid:101)j1. It sufﬁces to show that

t , then f 1 equals j1, and if the starting state is (cid:101)s1

t , then f 1

j1 ≥ (cid:101)j1 ⇔

s1
t

+ a2
t

+ a1+

t − a1−

t − min{Dt, s1

t } ≥ (cid:101)s1
t

+ a2
t

+ a1+

t − a1−

t − a1−

t } ⇔

t − a1−
t − a1−

t − min{Dt, (cid:101)s1
t − a1−
t }.

t } ≥ (cid:101)s1
Three cases can happen for the stochastic demand Dt, and we show that we can reduce Equation (31) to an
always true statement in all cases.

t − min{Dt, (cid:101)s1

t − min{Dt, s1
s1

(31)

i. s1

t − a1−

t ≥ (cid:101)s1

t − a1−

t > Dt

ii. s1

t − a1−

t ≥ Dt ≥ (cid:101)s1

t − a1−
t

iii. Dt > s1

t − a1−

t ≥ (cid:101)s1

t − a1−
t

s1
t − Dt ≥ (cid:101)s1

t − Dt ⇔

s1
t ≥ (cid:101)s1
t .

s1
t − Dt ≥ (cid:101)s1
t − a1−
s1

t − ((cid:101)s1
t ≥ Dt.

t − a1−

t ) ⇔

t − (s1
s1

t − a1−

t − a1−

t ) ⇔

t ) ≥ (cid:101)s1
t ≥ a1−
a1−
t

t − ((cid:101)s1
.

From i, ii, and iii, we conclude that j1 ≥ (cid:101)j1. Thus,

f 1(s1

t , s2

t , a1

t , a2

t , Dt) ≥ f 1((cid:101)s1

t , s2

t , a1

t , a2

t , Dt).

2. The one period cost function rt(s, a) is partially non-decreasing in s ∈ S for all a ∈ A, t = 0, 1, . . . , N − 1
t ∈ S 1 with s1
t ) ∈ A, then

t , if we take action at = (a1

t ≥ (cid:101)s1

t , a2

t , (cid:101)s1

[82]. It sufﬁces to show for every t < N, s1
the one period reward function satisﬁes

Using Equation (7), we reduce the following statement to an always true statement.

rt(s1

t , s2

t , a1

t , a2

t , Dt) ≥ rt((cid:101)s1

t , s2

t , a1

t , a2

t , Dt).

ρs2

t

(min{Dt, s1

t − a1−

rt(s1
t }) − Kta1+

t , a1
t , s2
+ Jta1−

t , a2
t − Lta2

t , Dt) ≥ rt((cid:101)s1
t ≥ ρs2

t , a2
t , a1
t , s2
(min{Dt, (cid:101)s1

t , Dt) ⇔
t − a1−

t

t

t }) − Kta1+

t

The second dimension is ﬁxed, so we could cancel out ρs2
Similar to part 1, three cases can happen.

t

min{Dt, s1

t − a1−

t } ≥ min{Dt, (cid:101)s1

t − a1−
t }.
from both sides.

i. s1

t > Dt

t ≥ (cid:101)s1

t − a1−

t − a1−
We can reduce Equation (32) to Dt ≥ Dt which is always true.
t − a1−
t − a1−
t
We can reduce Equation (32) to Dt ≥ (cid:101)s1

t ≥ Dt ≥ (cid:101)s1

t − a1−

ii. s1

t which is true for this case.

34

+ Jta1−

t − Lta2

t ⇔

(32)

iii. Dt > s1

t − a1−

t ≥ (cid:101)s1

t − a1−
t

We can reduce Equation (32) to s1

t ≥ (cid:101)s1

t which is always true.

So, we can conclude that

rt(s1

t , s2

t , a1

t , a2

t , Dt) ≥ rt((cid:101)s1

t , s2

t , a1

t , a2

t , Dt).

3. The terminal cost function rN(s) is partially non-decreasing in s ∈ S [82]. It sufﬁces to show that for every

N ∈ S 1 with s1

N, (cid:102)s1
s1
As the second state is ﬁxed equal to s2

N ≥ (cid:102)s1

N, that rN(sN) ≥ rN((cid:102)sN).

N, two possible cases can happen.

i. s2

N < θ

ii. s2

N ≥ θ.

rN(sN) ≥ rN((cid:102)sN) ⇔
0 ≥ 0.

rN(sN) ≥ rN((cid:102)sN) ⇔

ρs2

N

s1
N ≥ ρs2

N

(cid:102)s1
N.

ρs2

N

is not a function of the ﬁrst dimension, so we can cancel it out from both sides.

Thus, we conclude that

s1
N ≥ (cid:102)s1
N.

rN(sN) ≥ rN((cid:102)sN).

4. For each t < N, s1

t and Dt+1 are independent [23].

In our model, demand is a random variable and does not depend on the current state or action, including s1
t .

As all the conditions are valid for the stochastic SAIRP, we conclude that the value function is monotone in s1
t .

(cid:3)
If there exists a monotone optimal policy, then Theorem 3 is implied. Widrick et al. [24] proved that there
t only when demand is governed by a non-increasing discrete distribution.

exists a monotone optimal policy in s1
Thus, Theorem 3 provides a stronger result, as it does not depend on the probability distribution of demand.

Theorem 4. The MDP value function of the stochastic SAIRP is monotonically non-decreasing in (s1

t , s2
t ).

Proof. The MDP value function is monotone if the following four conditions are satisﬁed [82, 23].

1. For e (cid:23) 0 we have f (s + e, a) (cid:23) f (s, a) for all a ∈ A [82]. Using Deﬁnition 2 for partially ordered functions,
t ) ∈ A

it sufﬁces to show that for every st, (cid:101)st ∈ (S 1 ×S 2) with s1
and demand equals Dt, then the ith state transition function f i satisﬁes

t , if we take action at = (a1

t and s2

t ≥ (cid:101)s2

t ≥ (cid:101)s1

t , a2

We show the relationship between transition functions for each dimension.

f i(st, at, Dt) ≥ f i((cid:101)st, at, Dt) ∀i = 1, 2.

i.

ii.

t , then f 1 equals (cid:101)j1. We proved the claim when s2

t , then f 1 equals j1 and if the starting
t ≥ θ, in part 1 of Theorem 3. If
t , as we are in the absorbing state, the only possible action is at = (0, 0) that leads to
t , the only allowed action is at = (0, 0) because it is the only

f 1(st, at, Dt) ≥ f 1((cid:101)st, at, Dt)
Using Equation (3), we can state that if the beginning state is s1
state is (cid:101)s1
t ≥ (cid:101)s2
θ > s2
t ≥ (cid:101)j1 = (cid:101)s1
j1 = s1
feasible action for both s2
f 2(st, at, Dt) ≥ f 2((cid:101)st, at, Dt)
Using Equation (24), we reduce f 2(st, at, Dt) ≥ f 2((cid:101)st, at, Dt) to an always true statement:

t . Similarly, if s2
t and (cid:101)s2

t ≥ θ > (cid:101)s2
t . Thus, j1 = s1

t ≥ (cid:101)j1 = (cid:101)s1
t .

t ≥ (cid:101)s2

f 2(st, at, Dt) ≥ f 2((cid:101)st, at, Dt) ⇔

35

(cid:104)

1
M

t (M − a2
s2

t ) + a2

t − δC(a1+

t

(cid:105)
+ a1−
t )

≥

t (M − a2
(cid:101)s2

t ) + a2

t − δC(a1+

t

(cid:21)
+ a1−
t )

⇔

t (M − a2
s2

t ) + a2

t

+ a1−

t − δC(a1+
t (M − a2
s2

t ) ≥ (cid:101)s2

t ) ≥ (cid:101)s2

t (M − a2
t ).

t ) + a2

t − δC(a1+

t

+ a1−

t ) ⇔

(cid:20)

1
M
t (M − a2

Because M ≥ (cid:101)a2

t , we know

s2
t ≥ (cid:101)s2
t .

The last statement is always true and we prove that f 2(st, at, Dt) ≥ f 2((cid:101)st, at, Dt).

2. The one period cost function rt(s, a) is partially non-decreasing in s ∈ S for all a ∈ A, t = 0, 1, . . . , N − 1
[82]. Using Deﬁnition 2 for partially ordered functions, it sufﬁces to show that for every st, (cid:101)st ∈ (S 1 × S 2)
with s1

t ) ∈ A, then the one period reward function satisﬁes

t , if we take action at = (a1

t and s2

t ≥ (cid:101)s1

t ≥ (cid:101)s2

t , a2

Using Equation (7), we reduce the following statement to an always true statement.

rt(st, at, Dt) ≥ rt((cid:101)st, at, Dt).

rt(st, at, Dt) ≥ rt((cid:101)st, at, Dt) ⇔

(min{Dt, s1

t − a1−

t }) − Kta1+

t

ρs2

t

+ Jta1−

t − Lta2

t ≥ ρ (cid:101)s2

t

(min{Dt, (cid:101)s1

t − a1−

t }) − Kta1+

t

+ Jta1−

t − Lta2

t ⇔

(cid:18) β(1 + s2
1 − θ

t − 2θ)

(cid:19)

(min{Dt, s1

t − a1−

t }) ≥

(cid:18) β(1 + (cid:101)s2
1 − θ

t − 2θ)

(cid:19)
(min{Dt, (cid:101)s1

t − a1−

t }) ⇔

(1 + s2

t − 2θ)(min{Dt, s1

t − a1−

t }) ≥ (1 + (cid:101)s2

t − 2θ)(min{Dt, (cid:101)s1

t − a1−

t }).

(33)

Three cases can happen for the stochastic demand Dt. We show that we can reduce Equation (33) to an
always true statement in all cases.

i. Dt > s1

t ≥ (cid:101)s1

t − a1−
t
t − 2θ) ≥ (1 + (cid:101)s2

t − a1−
Because (1 + s2
that Equation (33) is always true.
t − a1−
We know (1 + s2

t − a1−
t
t − 2θ) ≥ (1 + (cid:101)s2

t ≥ Dt ≥ (cid:101)s1

ii. s1

t − 2θ), thus it sufﬁces to show that

t − 2θ) ≥ 0 and min{Dt, s1

t − a1−

t } ≥ min{Dt, (cid:101)s1

t − a1−

t } ≥ 0, we can conclude

(1 + s2

t − 2θ)(min{Dt, s1

t }) ≥ (1 + s2

t − 2θ)(min{Dt, (cid:101)s1

t − a1−

t }) ⇔

t − a1−
t − a1−

min{Dt, s1

t } ≥ min{Dt, (cid:101)s1
t − a1−

Dt ≥ (cid:101)s1

.

t

t − a1−

t } ⇔

iii. s1

The last statement is always true for this case.
t − a1−
Using the same approach as the previous part, we can reduce Equation (33) to Dt ≥ Dt which is always
true.

t − a1−

t ≥ (cid:101)s1

t > Dt

(1 + s2

t − 2θ)(min{Dt, s1

t }) ≥ (1 + s2

t − 2θ)(min{Dt, (cid:101)s1

t − a1−

t }) ⇔

t − a1−
t − a1−

min{Dt, s1

t } ≥ min{Dt, (cid:101)s1

t − a1−

t } ⇔

So, we conclude that

Dt ≥ Dt.

rt(st, at, Dt) ≥ rt((cid:101)st, at, Dt).

3. The terminal cost function rN(s) is partially non-decreasing in s ∈ S [82]. It sufﬁces to show that for every
N, that rN(sN) ≥ rN((cid:102)sN). Using Equation (29), three cases can

N and s2

N ≥ (cid:102)s2

N ≥ (cid:102)s1

sN, (cid:102)sN ∈ (S 1 × S 2) with s1
happen.

36

i. s2

N ≥ (cid:102)s2

N ≥ θ.

Because β(1+s2
1−θ

t −2θ)

≥ β(1+ (cid:101)s2
1−θ

t −2θ)

ii. s2

N ≥ θ > (cid:102)s2

N

iii. θ > s2

N ≥ (cid:102)s2

N

rN(sN) ≥ rN((cid:102)sN) ⇔
s1
N ≥ ρ(cid:102)s2

(cid:102)s1
N.

ρs2

N

N

and s1

N ≥ (cid:102)s1

N the last statement is true.

rN(sN) ≥ rN((cid:102)sN) ⇔
s1
N ≥ 0.

ρs2

N

rN(sN) ≥ rN((cid:102)sN) ⇔
0 ≥ 0.

4. For each t < N, (s1

t , s2

t ) and Dt+1 are independent [23].

In our model, demand is a random variable and does not depend on the current state or action, including
(s1

t , s2
t ).

As all the conditions are valid for the stochastic SAIRP, we conclude that the value function is monotone in (s1

t , s2
t ).
(cid:3)

37

7.3. Algorithmic and Experimental Parameter Settings

Parameter

Value

Description

M
M
M
T
T
T
θ
ε
maxIteration+1
τ
τ
w
µ1
µ2
ζ
α0

The starting number of batteries used for the small SAIRPs solved using BI
2
The number of batteries in the modest-sized SAIRPs
7
The number of batteries in the realistic-sized SAIRPs
100
The time horizon (number of hours) in the small SAIRP
168
The time horizon (number of hours) in the modest-sized SAIRPs
168
The time horizon (number of hours) in the realistic-sized SAIRPs
744
The replacement threshold
80%
The capacity increment used in discretizing the 2nd dimension of the state
0.001
3
The maximum number of regression-based initialization iterations
500000 The maximum number of core ADP iterations in the modest-sized SAIRPs
100000 The maximum number of core ADP iterations in the realistic-sized SAIRPs
25000
600
1000
0.7
1

The harmonic stepsize parameter
The STC stepsize parameter
The STC stepsize parameter
The STC stepsize parameter
The STC stepsize parameter

38

References

[1] United States Department of Energy, EV everywhere grand challenge road to success, last accessed on April 1, 2021 at https:

//www.energy.gov/sites/prod/files/2014/02/f8/eveverywhere road to success.pdf (January 2014).

[2] Z. Rezvani, J. Jansson, J. Bodin, Advances in consumer electric vehicle adoption research: A review and research agenda, Transportation

Research Part D: Transport and Environment 34 (2015) 122–136.

[3] S. Saxena, C. L. Floch, J. MacDonald, S. Moura, Quantifying EV battery end-of-life through analysis of travel needs with vehicle

powertrain models, Journal of Power Sources 282 (2015) 265–276.

[4] CBS NEWS, Amazon unveils futuristic plan: Delivery by drone, last accessed on April 1, 2021 at http://www.cbsnews.com/

news/amazon-unveils-futuristic-plan-delivery-by-drone/ (December 2013).

[5] E. Weise, UPS tested launching a drone from a truck for deliveries, USA Today, last accessed on April 1, 2021 at https:

//www.usatoday.com/story/tech/news/2017/02/21/ups-delivery-top-of-van-drone-workhorse/
98057076/ (February 2017).
Successful
at

[6] DHL,
2021
successful trial integration dhl parcelcopter logistics chain.html (May 2016).

integration
1,
logistics
http://www.dhl.com/en/press/releases/releases 2016/all/parcel ecommerce/

parcelcopter

on April

of DHL

accessed

chain,

trial

into

last

[7] T. Wallace, Royal Mail wants to use drones and driverless trucks,

last accessed on April 1, 2021 at The Telegraph: http:

//www.telegraph.co.uk/technology/11984099/Royal-Mail-wants-to-use-drones-and-driverless-
trucks.html (November 2015).

[8] B. Mutzabaugh, Drone taxis?

Dubai plans roll out of self-ﬂying pods,

last accessed on April 1, 2021 at https:

//www.usatoday.com/story/travel/flights/todayinthesky/2017/02/13/dubai-passenger-carrying-
drones-could-flying-july/97850596/ (February 2017).

[9] J. Jensen, Agricultural drones: How drones are revolutionizing agriculture and how to break into this booming market, UAV Coach, last

accessed on April 1, 2021 at: https://uavcoach.com/agricultural-drones/ (April 2019).

[10] S. Park, L. Zhang, S. Chakraborty, Battery assignment and scheduling for drone delivery businesses, in: IEEE/ACM International

Symposium on Low Power Electronics and Design, Taipei, Taiwan, 2017, pp. 1–6.

[11] D. James, 14 drones with the best ﬂight times, last accessed on April 1, 2021 at http://www.dronesglobe.com/guide/long-

flight-time/ (March 2020).

[12] Drones Etc., Phantom 3 batteries – Tips and Tutorial, last accessed on April 1, 2021 at https://www.dronesetc.com/blogs/

news/30238721-phantom-3-batteries-tips-and-tutorial (June 2015).

[13] G. Lacey, T. Jiang, G. Putrus, R. Kotter, The effect of cycling on the state of health of the electric vehicle battery, in: 48th International

Universities’ Power Engineering Conference, Dublin, Ireland, 2013, pp. 1–7.

[14] M. Shirk, J. Wishart, Effects of electric vehicle fast charging on battery life and vehicle performance, in: SAE Technical Paper, SAE

2015 World Congress & Exhibition, SAE International, Detroit, MI, 2015, pp. 1–13.

[15] B. Dunn, H. Kamath, J.-M. Tarascon, Electrical energy storage for the grid: A battery of choices, Science 334 (6058) (2011) 928–935.
[16] Nuvve, Toyota Tsusho and Chubu Electric Power announce Japan’s ﬁrst ever V2G project using Nuvve’s technology, last accessed
on March 24, 2021 at https://nuvve.com/toyota-tsusho-and-chubu-electric-power-announce-japans-
first-ever-v2g-project-using-nuvves-technology/ (March 2021).

[17] Nuvve, Vehicle-To-Grid technology, last accessed on March 24, 2021 at https://nuvve.com/technology/ (March 2021).
[18] P. Zhang, NIO signs agreement with state grid subsidiary to build 100 charging and battery swap stations by 2021, last accessed
on March 23, 2021 at https://cntechpost.com/2020/12/14/nio-national-grid-subsidiary-to-build-100-
charging-battery-swap-stations-by-2021/ (December 2020).

[19] Inside EVs, E.ON and Nissan are launching new V2G trial project in the UK, last accessed on March 24, 2021 at https:

//insideevs.com/news/437785/eon-nissan-v2g-trial-project-uk/ (August 2020).

[20] H.-Y. Mak, Y. Rong, Z.-J. M. Shen, Infrastructure planning for electric vehicles with battery swapping, Management Science 59 (7)

(2013) 1557–1575.

[21] M. L. Puterman, Markov decision processes: Discrete stochastic dynamic programming, 1st Edition, John Wiley & Sons, Hoboken, New

Jersey, 2005.

[22] A. Asadi, S. Nurre Pinkley, A stochastic scheduling, allocation, and inventory replenishment problem for battery swap stations, Trans-

portation Research Part E: Logistics and Transportation Review 146 (2021) 102212.

[23] D. R. Jiang, W. B. Powell, An approximate dynamic programming algorithm for monotone value functions, Operations Research 63 (6)

(2015) 1489–1511.

[24] R. S. Widrick, S. G. Nurre, M. J. Robbins, Optimal policies for the management of an electric vehicle battery swap station, Transportation

Science 52 (1) (2018) 59–79.

[25] S. G. Nurre, R. Bent, F. Pan, T. C. Sharkey, Managing operations of plug-in hybrid electric vehicle (PHEV) exchange stations for use

with a smart grid, Energy Policy 67 (2014) 364–377.

[26] O. Worley, D. Klabjan, Optimization of battery charging and purchasing at Electric Vehicle battery swap stations, in: IEEE Vehicle

Power and Propulsion Conference, Chicago, IL, 2011, pp. 1–4.

[27] B. Sun, X. Tan, D. H. K. Tsang, Optimal charging operation of battery swapping stations with QoS guarantee, in: IEEE International

Conference on Smart Grid Communications (SmartGridComm), Venice, Italy, 2014, pp. 13–18.

[28] F. Schneider, U. W. Thonemann, D. Klabjan, Optimization of battery charging and purchasing at electric vehicle battery swap stations,

Transportation Science 52 (5) (2018) 1211–1234.

[29] Q. Kang, J. Wang, M. Zhou, A. C. Ammari, Centralized charging strategy and scheduling algorithm for electric vehicles under a battery

swapping scenario, IEEE Transactions on Intelligent Transportation Systems 17 (3) (2016) 659–669.

[30] L. Zhang, S. Lou, Y. Wu, L. Yi, B. Hu, Optimal scheduling of electric vehicle battery swap station based on time-of-use pricing, in:

IEEE PES Asia-Paciﬁc Power and Energy Engineering Conference, Hong Kong, 2014, pp. 1–6.

[31] Z.-J. M. Shen, B. Feng, C. Mao, L. Ran, Optimization models for electric vehicle service operations: A literature review, Transportation

Research Part B: Methodological 128 (2019) 462–477.

[32] O. Kwizera, S. G. Nurre, Using drones for delivery: A two-level integrated inventory problem with battery degradation and swap stations,

in: Proceedings of the Industrial and Systems Engineering Research Conferences, Orlando, FL, 2018, pp. 1–6.

[33] X. Tan, G. Qu, B. Sun, N. Li, D. H. K. Tsang, Optimal scheduling of battery charging station serving electric vehicles based on battery

swapping, IEEE Transactions on Smart Grid 10 (2) (2019) 1372–1384.

39

[34] M. Sarker, H. Pand˘zi´c, M. Ortega-Vazquez, Optimal operation and services scheduling for an electric vehicle battery swapping station,

IEEE Transactions on Power Systems 30 (2) (2015) 901–910.

[35] S. M. Shavarani, M. G. Nejad, F. Rismanchian, G. Izbirak, Application of hierarchical facility location problem for optimization of a
drone delivery system: a case study of Amazon prime air in the city of San Francisco, The International Journal of Advanced Manufac-
turing Technology 95 (9) (2018) 3141–3153.

[36] J. Kim, B. D. Song, J. R. Morrison, On the scheduling of systems of UAVs and fuel service stations for long-term mission fulﬁllment,

Journal of Intelligent & Robotic Systems 70 (1) (2013) 347–359.

[37] I. Hong, M. Kuby, A. T. Murray, A range-restricted recharging station coverage model for drone delivery service planning, Transportation

Research Part C: Emerging Technologies 90 (2018) 198–212.

[38] J. Yang, H. Sun, Battery swap station location-routing problem with capacitated electric vehicles, Computers and Operations Research

55 (2015) 217–232.

[39] F. Pan, R. Bent, A. Berscheid, D. Izraelevitz, Locating PHEV exchange stations in V2G, in: First IEEE International Conference on

Smart Grid Communications, Gaithersburg, MD, USA, 2010, pp. 173–178.

[40] Y. Gao, K. Zhao, C. Wang, Economic dispatch containing wind power and electric vehicle battery swap station, in: IEEE PES Transmis-

sion and Distribution Conference and Exposition, Orlando, FL, 2012, pp. 1–7.

[41] Q. Dai, T. Cai, S. Duan, F. Zhao, Stochastic modeling and forecasting of load demand for electric bus battery-swap station, IEEE

Transactions on Power Delivery 29 (4) (2014) 1909–1917.

[42] O. Alagoz, L. M. Maillart, A. J. Schaefer, M. S. Roberts, The optimal timing of living-donor liver transplantation, Management Science

50 (10) (2004) 1420–1430.

[43] J. Chhatwal, O. Alagoz, E. S. Burnside, Optimal breast biopsy decision-making based on mammographic features and demographic

factors, Operations Research 58 (6) (2010) 1577–1591.

[44] J. Zhang, B. T. Denton, H. Balasubramanian, N. D. Shah, B. A. Inman, Optimization of prostate biopsy referral decisions, Manufacturing

& Service Operations Management 14 (4) (2012) 529–547.

[45] A. Khojandi, L. M. Maillart, O. A. Prokopyev, M. S. Roberts, T. Brown, W. W. Barrington, Optimal implantable cardioverter deﬁbrillator

(ICD) generator replacement, INFORMS Journal on Computing 26 (3) (2014) 599–615.

[46] S. Bloch-Mercier, Monotone markov processes with respect to the reversed hazard rate ordering: an application to reliability, Journal of

Applied Probability 38 (1) (2001) 195–208.

[47] B. de Jonge, P. A. Scarf, A review on maintenance optimization, European Journal of Operational Research 285 (3) (2020) 805–824.
[48] E. L. Porteus, Foundations of Stochastic Inventory Theory, Stanford University Press, Stanford, CA, 2002.
[49] A. J. Clark, H. Scarf, Optimal policies for a multi-echelon inventory problem, Management Science 6 (4) (1960) 475–490.
[50] A. J. Clark, An informal survey of multi-echelon inventory theory, Naval Research Logistics 19 (4) (1972) 621–650.
[51] G. A. DeCroix, A. Arreola-Risa, Optimal production and inventory policy for multiple products under resource constraints, Management

Science 44 (7) (1998) 950–961.

[52] H. Scarf, The optimality of (S , s) policies in the dynamic inventory problem, in: Arrow, Karlin, Suppers (Eds.), Mathematical Methods

in the Social Sciences, Stanford University Press, Stanford, CA, 1960, pp. 196–202.

[53] E. V. D. Laan, M. Salomon, Production planning and inventory control with remanufacturing and disposal, European Journal of Opera-

tional Research 102 (2) (1997) 264–278.

[54] M. ElHafsi, Optimal integrated production and inventory control of an assemble-to-order system with multiple non-unitary demand

classes, European Journal of Operational Research 194 (1) (2009) 127–142.

[55] A. K. Maity, One machine multiple-product problem with production-inventory system under fuzzy inequality constraint, Applied Soft

Computing 11 (2) (2011) 1549–1555.

[56] M. Golari, N. Fan, T. Jin, Multistage stochastic optimization for production-inventory planning with intermittent renewable energy,

Production and Operations Management 26 (3) (2017) 409–425.

[57] A. V. Horenbeek, J. Bur´e, D. Cattrysse, L. Pintelon, P. Vansteenwegen, Joint maintenance and inventory optimization systems: A review,

International Journal of Production Economics 143 (2) (2013) 499–508.

[58] A. H. Elwany, N. Z. Gebraeel, Sensor-driven prognostic models for equipment replacement and spare parts inventory, IIE Transactions

40 (7) (2008) 629–639.

[59] M. Rausch, H. Liao, Joint production and spare part inventory control strategy driven by condition based maintenance, IEEE Transactions

on Reliability 59 (3) (2010) 507–516.

[60] S. Nahmias, Perishable inventory theory: A review, Operations Research 30 (4) (1982) 680–708.
[61] L. B. Toktay, L. M. Wein, S. A. Zenios, Inventory management of remanufacturable products, Management Science 46 (11) (2000)

1412–1426.

[62] S. X. Zhou, Z. Tao, X. Chao, Optimal control of inventory systems with multiple types of remanufacturable products, Manufacturing &

Service Operations Management 13 (1) (2011) 20–34.

[63] K. Govindan, H. Soleimani, D. Kannan, Reverse logistics and closed-loop supply chain: A comprehensive review to explore the future,

European Journal of Operational Research 240 (3) (2015) 603–626.

[64] H. Ribbernick, K. Darcovich, F. Pincet, Battery life impact of vehicle-to-grid application of electric vehicles, in: 28th International

Electric Vehicle Symposium and Exhibition, Vol. 2, Korean Society of Automotive Engineers, Goyang, Korea, 2015, pp. 1535–1545.

[65] G. L. Plett, Recursive approximate weighted total least squares estimation of battery cell total capacity, Journal of Power Sources 196 (4)

(2011) 2319–2331.

[66] M. Abe, K. Nishimura, E. Seki, H. Haruna, T. Hirasawa, S. Ito, T. Yoshiura, Lifetime prediction for heavy-duty industrial lithium-ion

batteries that enables highly reliable system design, Hitachi Review 61 (6) (2012) 259–263.

[67] A. Hussein, Capacity fade estimation in electric vehicle li-ion batteries using artiﬁcial neural networks, IEEE Transactions on Industry

Applications 51 (3) (2015) 2321–2330.

[68] M. Dubarry, C. Truchot, B. Y. Liaw, K. Gering, S. Sazhin, D. Jamison, C. Michelbacher, Evaluation of commercial lithium-ion cells
based on composite positive electrode for plug-in hybrid electric vehicle applications. Part II. degradation mechanism under 2C cycle
aging, Journal of Power Sources 196 (23) (2011) 10336–10343.

[69] B. Xu, A. Oudalov, A. Ulbig, G. Andersson, D. S. Kirschen, Modeling of lithium-ion battery degradation for cell life assessment, IEEE

Transactions on Smart Grid 9 (2) (2018) 1131–1140.

[70] A. Abdollahi, N. Raghunathan, X. Han, B. Pattipati, B. Balasingam, K. R. Pattipati, Y. Bar-Shalom, B. Card, Battery health degradation

and optimal life management, in: IEEE AUTOTESTCON, National Harbor, MD, USA, 2015, pp. 146–151.

[71] E. Wood, M. Alexander, T. H. Bradley, Investigation of battery end-of-life conditions for plug-in hybrid electric vehicles, Journal of

40

Power Sources 196 (11) (2011) 5147–5154.

[72] W. B. Powell, Approximate Dynamic Programming: Solving the Curses of Dimensionality, 2nd Edition, John Wiley & Sons, New York,

NY, USA, 2011.

[73] D. Bertsimas, R. Demir, An approximate dynamic programming approach to multidimensional knapsack problems, Management Science

48 (4) (2002) 550–565.

[74] W. B. Powell, H. Topaloglu, Approximate dynamic programming for large-scale resource allocation problems, INFORMS TutORials in

Operations Research (2005) 123–147.

[75] H. P. Simao, J. Day, A. P. George, T. Gifford, J. Nienow, W. B. Powell, An approximate dynamic programming algorithm for large-scale

ﬂeet management: A case application, Transportation Science 43 (2) (2009) 178–197.

[76] A. Erdelyi, H. Topaloglu, Approximate dynamic programming for dynamic capacity allocation with multiple priority levels, IIE Trans-

actions 43 (2) (2010) 129–142.

[77] M. S. Maxwell, M. Restrepo, S. G. Henderson, H. Topaloglu, Approximate dynamic programming for ambulance redeployment, IN-

FORMS Journal on Computing 22 (2) (2010) 266–281.

[78] M. C¸ imen, C. Kirkbride, Approximate dynamic programming algorithms for multidimensional inventory optimization problems, IFAC

Proceedings Volumes 46 (9) (2013) 2015–2020.

[79] J. Meissner, O. V. Senicheva, Approximate dynamic programming for lateral transshipment problems in multi-location inventory systems,

European Journal of Operational Research 265 (1) (2018) 49–64.

[80] M. C¸ imen, C. Kirkbride, Approximate dynamic programming algorithms for multidimensional ﬂexible production-inventory problems,

International Journal of Production Research 55 (7) (2017) 2034–2050.

[81] A. Nasrollahzadeh, A. Khademi, M. E. Mayorga, Real-time ambulance dispatching and relocation, Manufacturing and Service Opera-

tions Management 20 (3) (2018) 467–480.

[82] K. Papadakia, W. B. Powell, Monotonicity in multidimensional Markov decision processes for the batch dispatch problem, Operations

Research Letters 35 (2) (2007) 267–272.

[83] A. J. Rettke, M. J. Robbins, B. J. Lunday, Approximate dynamic programming for the dispatch of military medical evacuation assets,

European Journal of Operational Research 254 (3) (2016) 824–839.

[84] C. Darken, J. Moody, Towards faster stochastic gradient search, in: Neural Information Processing Systems, Vol. 4, Morgan Kaufmann,

San Mateo, CA, 1992, pp. 1009–1016.

[85] A. P. George, W. B. Powell, Adaptive stepsizes for recursive estimation with applications in approximate dynamic programming, Machine

Learning 65 (1) (2006) 167–198.

[86] National Grid, Hourly electric supply charges,

last accessed on April 1, 2021 at https://www.nationalgridus.com/

niagaramohawk/business/rates/5 hour charge.asp (2016).

[87] DJI, DJI Spreading Wings S1000 specs, last accessed on April 1, 2021 at https://www.dji.com/spreading-wings-s1000/

spec (2019).

[88] K. Morrow, D. Karner, J. Francfort, Plug-in hybrid electric vehicle charging infrastructure review, Tech. rep., U.S. Department of Energy,

Idaho National Laboratory (November 2008).

[89] Tesla, Supercharger, last accessed on April 1, 2021 at https://www.tesla.com/supercharger (2017).
the month: Driven by Tesla, battery prices cut
[90] J. Romm, Chart of

last accessed on April 1,
2021 at https://archive.thinkprogress.org/chart-of-the-month-driven-by-tesla-battery-prices-
cut-in-half-since-2014-718752a30a42/ (January 2017).

in half since 2014,

[91] Nexant, Inc., Air Liquide, Argonne National Laboratory, Chevron Technology Venture, Gas Technology Institute, National Renewable
Energy Laboratory, Paciﬁc Northwest National Laboratory, TIAX LLC, H2A hydrogen delivery infrastructure analysis models and
conventional pathway options analysis results, Online, last accessed on April 1, 2021 at https://www.energy.gov/sites/prod/
files/2014/03/f9/nexant h2a.pdf (May 2008).

[92] Battery University, BU-801b: How to deﬁne battery life, last accessed on April 1, 2021 at: https://batteryuniversity.com/
learn/article/how to define battery life#:˜:text=The%20service%20life%20of%20a,elevated%
20temperatures%20also%20induces%20stress. (August 2017).

[93] U. K. Debnath, I. Ahmad, D. Habibi, Quantifying economic beneﬁts of second life batteries of gridable vehicles in the smart grid,

International Journal of Electrical Power and Energy Systems 63 (2014) 577–587.

[94] D. Montgomery, Design and analysis of experiments, 8th Edition, John Wiley & Sons, New Jersey, 2008.

41

