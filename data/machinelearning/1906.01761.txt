Generalized Linear Rule Models

Dennis Wei 1 Sanjeeb Dash 1 Tian Gao 1 Oktay G ¨unl ¨uk 1

9
1
0
2

n
u
J

5

]

G
L
.
s
c
[

1
v
1
6
7
1
0
.
6
0
9
1
:
v
i
X
r
a

Abstract

This paper considers generalized linear models
using rule-based features, also referred to as rule
ensembles, for regression and probabilistic clas-
siﬁcation. Rules facilitate model interpretation
while also capturing nonlinear dependences and
interactions. Our problem formulation accord-
ingly trades off rule set complexity and predic-
tion accuracy. Column generation is used to op-
timize over an exponentially large space of rules
without pre-generating a large subset of can-
didates or greedily boosting rules one by one.
The column generation subproblem is solved
using either integer programming or a heuris-
tic optimizing the same objective.
In experi-
ments involving logistic and linear regression,
the proposed methods obtain better accuracy-
complexity trade-offs than existing rule ensem-
ble algorithms. At one end of the trade-off, the
methods are competitive with less interpretable
benchmark models.

1. Introduction

Decision rules have served as important building blocks
for supervised learning models. They are often combined
via logical operations into decision lists (Rivest, 1987;
Angelino et al., 2017; Yang et al., 2017) and rule sets
(Lakkaraju et al., 2016; Wang et al., 2017), models whose
appeal stems from the human-interpretability of their con-
stituent rules.

In this paper, we consider models that are linear combina-
tions of decision rules, also referred to as rule ensembles,
within the framework of generalized linear models (GLM)
(McCullagh & Nelder, 1989). Rule ensembles retain inter-
pretability while allowing modelling ﬂexibility since rules
are able to capture nonlinear dependences and interactions.
Our problem formulation accordingly trades off predic-

1IBM Research, Yorktown Heights, NY, USA. Correspon-

dence to: Dennis Wei <dwei@us.ibm.com>.

Proceedings of the 36 th International Conference on Machine
Learning, Long Beach, California, PMLR 97, 2019. Copyright
2019 by the author(s).

tive accuracy against the complexity of the rule ensemble,
measured in terms of the number of rules as well as their
lengths, i.e., the number of elementary conditions on indi-
vidual variables in the conjunction. We make use of GLMs
to address the real-valued prediction tasks of regression
and probabilistic classiﬁcation, where class probability es-
timates are desired.

The main challenge in learning rule-based models is due to
the exponential size of the space of rules, corresponding to
all possible conjunctions of the input features. Predominant
approaches include pre-selecting a (often large) subset of
candidate rules and greedy optimization in which rules are
added one by one but not revised. The former includes op-
timization methods that select from among the candidates
(Friedman & Popescu, 2008; Lakkaraju et al., 2016; Wang
et al., 2017) while the latter includes sequential covering
(Cohen, 1995; Clark & Boswell, 1991; F¨urnkranz et al.,
2014) and boosting.

Our main contribution herein is to propose an approach
that avoids both of the above alternatives. The technique
of column generation (CG) is used to intelligently search
the space of rules and produce useful ones as needed, as
opposed to using a large, ﬁxed set of candidates. Instead
of boosting, a GLM is re-ﬁt as rules are generated, which
allows existing rules to be reweighted and even discarded.
The CG subproblem is formulated as an integer program
(not of exponential size) and is solved using either integer
programming or a heuristic that targets the same objective.
We also discuss a non-CG algorithm that uses only ﬁrst-
degree rules, i.e., those with a single condition, and op-
tionally numerical features as-is. This latter algorithm pro-
duces a kind of generalized additive model (GAM) (Hastie
& Tibshirani, 1990).

Experiments are presented involving the two most com-
mon cases of GLMs, logistic and linear regression. The
proposed methods are seen to yield better performance-
complexity trade-offs than existing rule ensemble algo-
rithms. At the performance-maximizing end of the trade-
off, the methods are competitive with less interpretable
benchmark models such as tree ensembles and nonlinear
support vector machines (SVM). The trade-off curves also
suggest that substantially simpler models are often avail-
able at a relatively small cost in performance.

 
 
 
 
 
 
1.1. Related Work

Further details on binarization are given in Section 5.

Generalized Linear Rule Models

Of the rule ensemble algorithms that have been proposed,
the most closely related to the current proposal is RuleFit
(Friedman & Popescu, 2008), which also ﬁts linear models
to a set of rules. This set however is predetermined by ﬁrst
inducing a large number (hundreds) of decision trees from
the data and then extracting rules corresponding to nodes of
the trees. As will be seen in Section 5, this approach yields
more complex rule ensembles for the same performance.
R¨uckert & Kramer (2006) also propose ﬁtting linear mod-
els to an increasing set of rules but the rules are generated
in an unsupervised manner.

Boosting algorithms, which do not modify previously
added rules, are represented by SLIPPER (Cohen & Singer,
1999), MLRules and ENDER (Dembczy´nski et al., 2010).
Other algorithms include HKL (Jawanpuria et al., 2011),
which is also able to effectively optimize over an exponen-
tial number of conjunctions but relies on a regularizer with
a special group structure. While often cited as a rule en-
semble method, Weiss & Indurkhya (2000) actually learn
ensembles of disjunctive normal forms (DNF), which are
more akin to tree ensembles.

Even though branch-and-bound and CG have been used be-
fore in ML, e.g., for boosting (Demiriz et al., 2002) and in
particular for rule learning (Angelino et al., 2017; Rudin &
Ertekin, 2018), we believe our work is the ﬁrst application
of CG to a nonlinear optimization problem in ML. Our CG
approach is inspired by its recent use in learning disjunc-
tive/conjunctive normal form rules (Dash et al., 2018), with
similar beneﬁts. While that work applied CG to a linear
program, here we have a (convex) nonlinear problem. For
nonlinear problems, even though the general framework
has been discussed for OR problems (Garcia et al., 2003),
there are only a few practical applications, mostly nonlinear
variants of the VRP (Bornd¨orfer et al., 2013; Fortz et al.,
2010). We also note that whether CG can be successfully
applied to MINLPs has been posed as an open problem in
a 2018 Dagstuhl Seminar (Bonami et al., 2018).

2. Generalized Linear Rule Models

We consider the standard supervised learning problem of
predicting a target variable Y ∈ Y using input features
X = (X1, . . . , Xd), given a training dataset of i.i.d. sam-
ples (xi, yi), xi = (xi1, . . . , xid), i = 1, . . . , n. The output
space Y may be discrete or continuous. We assume that all
features Xj have been binarized to take values in {0, 1}.
For categorical features, this is achieved through the usual
“one-hot” coding into indicators Xj = x for all categories
x as well as their negations Xj (cid:54)= x. Numerical features
are binarized through bi-directional comparisons to a set of
thresholds, e.g., Xj ≤ 1, Xj ≤ 2.3 and Xj > 1, Xj > 2.3.

In this section, we recall aspects of generalized linear mod-
els (GLMs) and introduce notation needed to deﬁne them
over a feature space of rules. Let K denote the set of con-
junctions of X to be considered. We defer discussion of
the choice of K to Section 3 but note that it is not necessary
to limit its size. Denote by Ak the variable corresponding
to conjunction k ∈ K, and aik ∈ {0, 1} the value taken
by Ak in instance i. Let k = 0 be the index of the empty
conjunction A0 ≡ 1.

For a GLM, we posit that Y conditioned on X follows an
exponential family distribution given by

pY | X(y | x) = h(y) exp (ηy − Φ(η)) ,

(1)

where the canonical parameter η is a linear combination of
the conjunctions Ak of X,

η =

(cid:88)

k∈K

βkAk,

(2)

and Φ(η) is the log-partition function. The terms with
nonzero coefﬁcients βk deﬁne an ensemble of rules map-
ping conjunctions to real values βk, which are then linearly
combined. The distribution may have parameters in addi-
tion to η (e.g., the variance in the Gaussian case) but these
are either assumed known or their estimation can be sep-
arated from that of η. The prediction function is given by
the conditional mean of Y ,

ˆy(X) = E[Y | X] = Φ(cid:48)(η),

(3)

where the second equality holds for (1).

The coefﬁcients βk in (2) are determined by minimizing the
negative log-likelihood corresponding to (1) on the training
data. Since K is potentially large, a sparse solution is es-
sential and is obtained through (cid:96)1 regularization. The opti-
mization problem is therefore
(cid:34)
(cid:33)

(cid:32)

(cid:35)

(cid:88)

Φ

βkaik

− yi

βkaik

+

λk|βk|.

(cid:88)

(cid:88)

n
(cid:88)

1
n

min
β

i=1

k∈K

k∈K

k∈K

(4)
The factor h(y) in (1) is not a function of β and is omitted.
Each regularization parameter λk depends on the number
of literals of conjunction k as speciﬁed later. It is a prop-
erty of exponential families that the log-partition function
Φ(η) is convex. By afﬁne composition property of convex
functions the problem (4) is therefore convex.

We specialize the foregoing to the two most common cases
of GLMs, logistic and linear regression. For logistic re-
gression, the log-partition function Φ(η) = log(1 + eη).
Substituting this into (4), the quantity in square brackets
becomes the familiar expression
(cid:16)

(cid:17)(cid:17)

log

1 + exp

(cid:16)
(−1)yi (cid:88)

βkaik

,

(5)

k∈K

Generalized Linear Rule Models

where yi ∈ {0, 1}. For linear regression, Φ(η) = η2/2 and
the bracketed quantity becomes (after adding back y2

i /2)

(cid:16)

1
2

yi −

(cid:88)

k∈K

(cid:17)2

.

βkaik

(6)

3. Model Instantiations

We discuss two instantiations of generalized linear rule
models (GLRM). In Section 3.1, the set of conjunctions
K is restricted to ﬁrst-degree or singleton conjunctions,
i.e., those with a single condition on an individual feature.
Section 3.2 considers the general case with no restriction
on K.

3.1. Generalized Additive Model Using First-Degree

Rules

In the ﬁrst case, the conjunctions Ak correspond to the bi-
narized features Xj themselves. In terms of the original
unbinarized features, conditions are placed on only one fea-
ture at a time and so the resulting GLRM is free of inter-
action terms. On the other hand, if features are binarized
as discussed at the beginning of Section 2, then the GLRM
is a type of generalized additive model (GAM), i.e., a sum
of univariate functions. For numerical features, ﬁrst-degree
rules correspond to step functions, which can be linearly
combined into arbitrary piecewise-constant functions with
discontinuities at the binarization thresholds. For categori-
cal features, any function can be realized.

Friedman & Popescu (2008) discuss a similar type of model
obtained by restricting their decision trees to depth 1 (de-
cision stumps). There are two differences however. First,
the singleton rules herein are systematically enumerated as
opposed to generated by a randomized tree induction pro-
cedure. Second, from every pair of complementary single-
ton rules (e.g., Xj ≤ 1, Xj > 1), we remove one member
as otherwise the pair together with the empty conjunction
A0 ≡ 1 are collinear. The results in Section 5 suggest that
this removal of linearly dependent rules contributes toward
sparser, simpler rule ensembles.

In addition to ﬁrst-degree rules, following Friedman &
Popescu (2008) we may also include in the feature space
any numerical features as they are, without binarization.
This model variant is also evaluated in Section 5.

3.2. General Rule Ensemble Using Column Generation

We now let K be the set of all possible conjunctions of X to
obtain rule ensembles with no restrictions. Since K is now
exponentially large, it is intractable even to enumerate the
variables in problem (4) (unless the feature dimension d is
very small). We exploit the technique of column generation
(CG) to tackle this problem.

Column generation was originally developed to solve lin-
ear programs (LPs) with a very large number of columns
(Gilmore & Gomory, 1961; Conforti et al., 2014). Using
the fact that optimal solutions of LPs are sparse, the main
idea is to ﬁrst solve a restricted problem with a small num-
ber of candidate columns and then generate some of the
missing columns based on the optimal dual solution of this
restricted problem. Using the dual solution, one can com-
pute the marginal beneﬁt (or, partial derivative) of introduc-
ing a missing column to the restricted problem. If partial
derivative for the most promising missing column is non-
negative, then the procedure terminates. The crucial com-
ponent of this approach is to formulate a column generation
problem that can search through all of the missing columns
without complete enumeration.

We next describe how to adopt this idea to solve the gen-
eralized linear model (4). To derive the column generation
k − β−
subproblem, it is helpful to express βk as βk = β+
for β+

k ≥ 0. Problem (4) becomes

k , β−

k

min
β+,β−≥0

1
n

n
(cid:88)

i=1

(cid:34)

(cid:32)

Φ

(cid:88)

(β+

k − β−

k )aik

(cid:33)

k∈K

(cid:35)

− yi

(cid:88)

k∈K

(β+

k − β−

k )aik

(cid:88)

+

k∈K

λk(β+

k + β−

k ).

(7)

Suppose that a restricted version of (7) has been solved
for a subset S ⊂ K of the set of conjunctions, yielding
k = (β±
β±
k )∗ for k ∈ S. We extend this to a solution for
(7) by setting β±
k = 0 for k /∈ S and wish to determine
the optimality of the extended solution. Since (7) is also
a convex problem with non-negativity constraints, a neces-
sary and sufﬁcient condition of optimality is for the partial
derivatives of the objective with respect to β±
k to be zero
if β±
k = 0. This condition is
true for k ∈ S due to optimality for the restricted problem.
For k /∈ S, β±
k = 0 and we are thus required to check
non-negativity of the derivatives.
The partial derivative w.r.t. β+

k > 0 and non-negative if β±

k of the objective in (7) is

1
n

n
(cid:88)

i=1

(cid:32)

(cid:34)
Φ(cid:48)

(cid:88)

k(cid:48)∈K

(cid:33)

(cid:35)

βk(cid:48)aik(cid:48)

aik − yiaik

+ λk

=

1
n

n
(cid:88)

(cid:0)ˆy(xi) − yi

(cid:1)aik + λk =

i=1

1
n

n
(cid:88)

i=1

riaik + λk,

using (2) and (3) to obtain the ﬁrst equality and deﬁning the
prediction residual ri = ˆy(xi) − yi in the second. The par-
tial derivative with respect to β−
k is the same except that the
residuals are negated. Non-negativity of all partial deriva-
tives can thus be determined by solving the pair of prob-

Generalized Linear Rule Models

lems

min
k∈K

±

1
n

n
(cid:88)

riaik + λk.

(8)

i=1
If both optimal values in (8) are non-negative, then all
derivatives are indeed non-negative and we conclude that
the extended solution is optimal for (7). On the other hand,
if the objective in (8) is negative for some k /∈ S and the
‘+’ sign (say), then the partial derivative w.r.t. β+
k is nega-
tive and β+
k can be added to the restricted problem (i.e., k
added to S) to potentially improve the current solution.

We now use the fact that K is a set of conjunctions to avoid
solving (8) by enumeration. This involves encoding a con-
junction by the set of participating features and relating the
values aik to the feature values xij. Let zj ∈ {0, 1} repre-
sent whether feature j is selected in a conjunction. We as-
sume that the regularization parameter λk is an afﬁne func-
j zj
tion of the degree of the conjunction, λk = λ0 + λ1
with λ0, λ1 ≥ 0 (other afﬁne functions of zj are possi-
ble). Deﬁne ¯xij = 1 − xij, I+ = {i : ri > 0}, and
I− = {i : ri < 0}. Then (8) can be reformulated as

(cid:80)

min
a,z

±

1
n

s.t. ai +

n
(cid:88)

i=1

d
(cid:88)

riai + λ0 + λ1

d
(cid:88)

j=1

zj

¯xijzj ≥ 1,

ai ≥ 0,

i ∈ I+

(9)

j=1
ai + zj ≤ 1,
zj ∈ {0, 1},

i ∈ I−,
j = 1, . . . , d,

j : ¯xij = 1

where the subscript k has been dropped in favor of encod-
ing by {zj}. The constraints in (9) ensure that ai acts as the
conjunction of the selected xij’s. For i ∈ I+, ai = 1 only
if all selected features have xij = 1 (¯xijzj = 0 ∀j), other-
wise ai = 0 since riai is minimized in the objective. For
i ∈ I−, ri < 0, ai is maximized, and the corresponding
constraint enforces the same behavior for ai.

Therefore, our column generation algorithm alternates be-
tween solving the restricted log-likelihood problem (4) and
searching for new columns by solving (9) for both signs.
We initialize the restricted set S to be the set of ﬁrst-degree
rules discussed in Section 3.1, optionally including original
numerical features as well. The algorithm terminates with
a certiﬁcate that problem (4) is solved to optimality if the
optimal value of problem (9) is non-negative for both signs.
For practical reasons we also have a secondary termination
criteria that depends on the total number of column gener-
ation iterations and the total CPU time spent.

This algorithm is guaranteed to terminate in ﬁnite time as
there are only a ﬁnite number of candidate columns (con-
junctions) and the column generation procedure would not
generate the same conjunction more than once as the par-
tial derivative of the conjunctions in the restricted problem

are guaranteed to be non-negative. We note that ﬁnite ter-
mination is not guaranteed for models with different reg-
ularization parameters, for example for (cid:96)2-regularization,
as repeating a conjunction with a non-zero weight would
improve the optimal value of (7) by simply splitting the
weight of an original conjunction into two equal parts.
After splitting the ﬁrst term in (7) would stay the same
whereas the regularization penalty would decrease.

Once the column generation algorithm terminates, we solve
the log-likelihood problem (4) one last time to de-bias the
solution. In this ﬁnal run we restrict conjunctions to the
ones with β±
k > 10−5 in the last round and we drop the
regularization term in the objective.

4. Column Generation Approaches

The column generation subproblem (9) is solved using ei-
ther integer programming (IP) or a heuristic algorithm. We
have implemented our column generation procedure in Java
using LibLinear (Fan et al., 2008) to solve the regularized
logistic regression problem (4) and using Cplex callable li-
brary (version 12.7.1) to solve the integer program (9) for
column generation. As LibLinear package only allows sim-
ple (cid:96)1-regularization (as opposed to using different weights
λk that depend on the complexity of the conjunction k), we
scale the aik values by 1/λk.

The proposed heuristic algorithm performs a limited search
of the rule space, proceeding in order of increasing con-
junction degree D = (cid:80)
j zj from 1 up to a maximum
Dmax. To describe the heuristic, we deﬁne the children
of a conjunction as those conjunctions that involve one ad-
ditional feature, i.e., have one additional zj = 1 in terms
of the representation in (9). At each degree D, only those
conjunctions that are children of a chosen parent (of degree
D − 1) are evaluated. These children are evaluated for two
purposes: 1) To determine whether any improve upon the
incumbent solution, deﬁned as the best solution observed
thus far; 2) to select a parent conjunction for the next high-
est degree. Evaluation for the ﬁrst purpose is based only
on the objective value achieved in (9), while evaluation for
the second also considers a lower bound on future objective
values, as discussed next.

We illustrate the objective value and lower bound calcula-
tions for degree 1 conjunctions, i.e., children of the initial
empty conjunction. Higher degrees are analogous. Ob-
jective values of children can be computed via their incre-
ments and decrements relative to the value of the parent.
For D = 1, setting zj = 1 forces ai = 0 for i such that
xij = 0 (¯xij = 1). The change in value of child j is thus

∆v(j) = λ1 −

(cid:88)

i∈I+

ri ¯xij −

(cid:88)

i∈I−

ri ¯xij.

(10)

Generalized Linear Rule Models

A lower bound on future objective values resulting from
setting zj = 1, i.e., values of descendants of child j, can be
obtained by optimistically assuming that with the addition
of one more feature (at a further cost of λ1), all positive ri
can be eliminated from the objective function in (9) while
no negative ri are eliminated beyond those due to setting
zj = 1 itself. Expressed in the same relative terms as (10),
this lower bound is

LB(j) = 2λ1 −

(cid:88)

ri −

(cid:88)

ri ¯xij.

(11)

i∈I+

i∈I−

To determine the parent for the next degree, we ﬁrst elimi-
nate all children of the current parent whose lower bounds
LB(j) are not less than the value of the incumbent solu-
tion, since these cannot lead to improvement. Any remain-
ing children are evaluated using the average of ∆v(j) and
LB(j) and the child with the lowest such average is se-
lected. The motivation is to consider not only the children’s
current values but also a crude but easily computed estimate
of potential future values. Other convex combinations of
∆v(j) and LB(j) have not been explored.

Once a new parent conjunction is chosen, corresponding to
setting a zj = 1, two operations are performed to reduce
the dimensions of the problem and to render it in the same
form as for D = 1. First, indices (rows) i where ¯xij = 1
are removed since ai is forced to zero as noted above. Sec-
ond, setting zj = 1 may make other features j(cid:48) redundant
and these can be removed (by setting zj(cid:48) = 0). For exam-
ple, if binary feature j corresponds to one category of an
original categorical feature, then we may set zj(cid:48) = 0 for
other categories of the same feature because the conjunc-
tion of j and j(cid:48) would be identically zero. We refer to Su
et al. (2016) for a fuller discussion of these redundancies.

We have explored additional variations of the heuristic al-
gorithm as discussed in Appendix A.

5. Numerical Evaluation

We report on numerical experiments involving both logis-
tic regression (5) (i.e., classiﬁcation) and linear regression
(6). We tested the 4 methods discussed in Section 3: lo-
gistic/linear regression on singleton rules (without CG, ab-
breviated LR1), logistic/linear regression on general rules
(with CG, abbreviated LRR), and the same two with the
addition of any numerical features originally present in the
data (LR1N, LRRN). Column generation is done using the
heuristic in Section 4 but we also report a preliminary result
using an IP version of LRR (LRRI). We compared these
methods to RuleFit (Friedman & Popescu, 2008), which
is the closest existing method, and speciﬁcally a Python
implementation1 because the original R code is no longer

1https://github.com/christophM/rulefit

supported. We also tried to test HKL (Jawanpuria et al.,
2011) but encountered a no longer supported toolbox as
well as numerical problems. Beyond rule ensembles, we
also compared to gradient boosted classiﬁcation/regression
trees (GBT) and support vector machines (SVM) with ra-
dial basis function (RBF) kernels. These are less inter-
pretable models intended to provide a benchmark for pre-
diction performance. 10-fold cross-validation (CV) is used
to estimate all test performance metrics.

Categorical and numerical features were binarized as de-
scribed at the beginning of Section 2, using sample deciles
as thresholds for numerical features. To control for the ef-
fect of discretization, numerical features were discretized
using the same quantile thresholds for all rule- and tree-
based methods in the comparison. Excluded from this treat-
ment are SVMs and the variant of RuleFit that uses numer-
ical features in addition to rules (abbreviated RuleFitN).

5.1. Classiﬁcation

For classiﬁcation, we used the same 16 datasets considered
in (Dash et al., 2018), which also appeared in other recent
works on rule-based models (Su et al., 2016; Wang et al.,
2017). One of these datasets comes from the recent FICO
Explainable Machine Learning Challenge (FICO, 2018).

In the ﬁrst experiment, we evaluated the performance-
complexity trade-offs of the four proposed methods (LR1,
LRR, LR1N, LRRN) as well as RuleFit. For performance
metrics, we report both accuracy and Brier score (i.e., mean
squared error (MSE)), the latter a well-known metric for
probabilistic outputs (Hern´andez-Orallo et al., 2012) as
produced by logistic regression-based models. To measure
rule ensemble complexity, we consider not only the number
of rules (with nonzero coefﬁcients) but also their lengths in
terms of number of conditions. Speciﬁcally we deﬁne the
weight of a rule similarly to the regularization parameter λk
as 1+w (cid:80)
j zj, where we take w = 0.2 as the weight on the
degree. Coefﬁcients corresponding to numerical features
receive a weight of 1. For consistency with this deﬁnition,
the parameters λk used by all methods in this comparison
are set proportional to the weights, i.e., with λ1/λ0 = 0.2.
By varying the remaining free parameter λ0, we sweep out
trade-offs between performance and complexity. RuleFit
has an additional parameter, the mean tree size, that is rec-
ommended for tuning in (Friedman & Popescu, 2008). We
have done so based on test set results, which gives RuleFit
a slight advantage.

Figure 1 shows the resulting trade-offs with Brier score for
4 of the 16 datasets. The other 12 plots as well as those
for accuracy are in Appendix B. Pareto-efﬁcient points,
i.e., those not dominated by points with both lower Brier
score and lower complexity, have been connected with line
segments for the sole purpose of visualization. RuleFit ob-

Generalized Linear Rule Models

(a) Pima Indians diabetes

(b) FICO Explainable Machine Learning
Challenge

(c) MAGIC gamma telescope

(d) Musk molecules

Figure 1. Trade-offs between Brier score and weighted number of rules. Pareto efﬁcient points are connected by line segments. Hori-
zontal and vertical bars represent standard errors in the means.

tains inferior trade-offs on most of the tested datasets. Even
in cases such as Figure 1c where RuleFit eventually attains
a lower Brier score, the initial part of the trade-off is worse.
Note that the variants employing numerical features gener-
ally fare better, as expected. Figure 1c indicates that IP CG
(LRRI) can improve upon the heuristic in some cases.

include GBT and SVM. For GBT, the maximum tree depth
was tuned and the number of trees was also determined via
a stopping criterion on a validation set, up to a maximum
of 500 trees. For SVM, the regularization parameter C and
kernel width γ were tuned and Platt scaling (Platt, 1999)
was used to calibrate the output scores as probabilities.

Next we discuss the differences between LR1(N) and
LRR(N). A general observation is that LRR, which uses
CG to produce higher-degree rules, tends to achieve bet-
ter trade-offs on larger datasets, as exempliﬁed by musk in
Figure 1d. This can be explained by the fact that LRR ef-
fectively considers a much larger feature space than LR1.
If the training sample size is sufﬁcient to support this, then
the greater power of the higher-degree rules found by LRR
generalizes to test data. The presence of strong interac-
tions in the data also favors LRR. On the other hand, on
smaller datasets such as in Figure 1a, LRR may overﬁt and
achieve a worse trade-off relative to LR1.
In Figure 1b,
LRR outperforms LR1 but the advantage disappears for
LRRN compared to LR1N, a pattern that also occurs on
other datasets. In Figure 1c, the advantage of LRR(N) lies
in attaining a slightly lower minimal Brier score.

In a second experiment, we aim to maximize performance
(minimize Brier score or maximize accuracy) by perform-
ing nested CV on the training set to select λ0 and applying
the resulting model to the test set. Since performance is
now the primary criterion, we broaden the comparison to

Table 1 shows the resulting mean Brier scores while Ta-
ble 2 shows the weighted number of rules at which the Brier
scores were achieved. To help summarize these results, we
report the mean rank of each method as well as Friedman
tests on these mean ranks, as suggested by a reviewer and
following (Demˇsar, 2006). The overall conclusion is that
when tuned for maximum performance, the proposed meth-
ods, especially LRRN, compete well with benchmark mod-
els and do so using signiﬁcantly fewer rules than RuleFit.

For Table 1, the Friedman statistic is 12.41, corresponding
to a p-value of 0.082 using the F -distribution approxima-
tion. Hence the null hypothesis of no signiﬁcant differences
is rejected at the 0.10 level but not at 0.05. A post-hoc test,
comparing all other algorithms to LRRN and correcting for
multiple comparisons using Holm’s procedure, shows that
the only signiﬁcant difference at the 0.10 level is with LR1,
i.e., when excluding both higher-degree rules and numeri-
cal features. The mean ranks also show that LRR(N) out-
performs LR1(N) although most of the differences are not
statistically signiﬁcant.

01020304050weighted rules0.160.180.200.220.24Brier scoreLR1LR1NLRRLRRNRuleFitRuleFitN020406080100120140weighted rules0.1700.1750.1800.1850.1900.1950.200Brier scoreLR1LR1NLRRLRRNRuleFitRuleFitN0255075100125150175200weighted rules0.100.110.120.130.140.150.160.170.18Brier scoreLR1LR1NLRRLRRNLRRIRuleFitRuleFitN0255075100125150175200weighted rules0.020.040.060.08Brier scoreLR1LR1NLRRLRRNRuleFitRuleFitNGeneralized Linear Rule Models

Table 1. Mean test Brier scores (standard error in parentheses). Best values in bold.

dataset

banknote
heart
ILPD
ionosphere
liver
pima
tic-tac-toe
transfusion
WDBC

adult
bank-mkt
gas
magic
mushroom
musk
FICO

mean rank

LR1

LRR

RuleFit

LR1N

LRRN

RuleFitN

GBT

SVM

2.58 (0.75) E-3
1.33 (0.09) E-1
1.86 (0.06) E-1
7.26 (0.97) E-2
2.50 (0.12) E-1
1.63 (0.09) E-1
1.62 (0.30) E-2
1.66 (0.05) E-1
2.52 (0.61) E-2

1.05 (0.01) E-1
1.00 (0.00) E-1
3.93 (0.41) E-3
1.11 (0.01) E-1
5.05 (1.75) E-7
2.53 (0.31) E-2
1.79 (0.01) E-1

2.58 (0.98) E-3
1.33 (0.10) E-1
1.82 (0.05) E-1
6.63 (0.95) E-2
2.47 (0.09) E-1
1.66 (0.09) E-1
1.71 (0.41) E-2
1.55 (0.04) E-1
1.76 (0.38) E-2

1.05 (0.01) E-1
7.79 (0.07) E-2
3.40 (0.35) E-3
1.09 (0.01) E-1
2.02 (1.10) E-7
1.37 (0.09) E-2
1.79 (0.02) E-1

3.46 (0.99) E-3
1.22 (0.09) E-1
2.50 (0.00) E-1
6.31 (1.39) E-2
2.55 (0.11) E-1
1.62 (0.09) E-1
3.78 (2.58) E-7
1.62 (0.09) E-1
2.01 (0.35) E-2

1.07 (0.01) E-1
1.73 (0.00) E-1
4.19 (0.49) E-3
1.06 (0.02) E-1
0.00 (0.00) E-7
2.02 (0.11) E-2
1.80 (0.02) E-1

7.43 (1.94) E-5
1.34 (0.10) E-1
1.86 (0.06) E-1
7.07 (1.11) E-2
2.57 (0.13) E-1
1.61 (0.08) E-1
1.62 (0.30) E-2
1.53 (0.03) E-1
2.08 (0.61) E-2

9.78 (0.09) E-2
1.00 (0.00) E-1
3.99 (0.33) E-3
1.10 (0.01) E-1
5.05 (1.75) E-7
3.00 (0.49) E-2
1.78 (0.02) E-1

1.54 (0.95) E-3
1.32 (0.09) E-1
1.82 (0.05) E-1
6.99 (1.09) E-2
2.50 (0.11) E-1
1.65 (0.08) E-1
1.71 (0.41) E-2
1.55 (0.03) E-1
1.41 (0.34) E-2

9.78 (0.09) E-2
7.77 (0.07) E-2
4.40 (0.34) E-3
1.09 (0.02) E-1
2.02 (1.10) E-7
1.20 (0.10) E-2
1.78 (0.01) E-1

1.53 (0.83) E-3
1.25 (0.10) E-1
2.50 (0.00) E-1
4.95 (1.30) E-2
2.48 (0.16) E-1
1.66 (0.11) E-1
3.78 (2.58) E-7
1.66 (0.11) E-1
2.62 (0.44) E-2

9.01 (0.09) E-2
9.99 (0.00) E-2
3.70 (0.30) E-3
9.33 (0.17) E-2
0.00 (0.00) E-7
1.76 (0.12) E-2
1.79 (0.01) E-1

4.50 (1.04) E-3
1.33 (0.08) E-1
1.78 (0.03) E-1
6.98 (1.08) E-2
2.47 (0.06) E-1
1.66 (0.07) E-1
8.22 (1.60) E-3
1.60 (0.03) E-1
3.09 (0.37) E-2

1.04 (0.01) E-1
7.98 (0.07) E-2
5.72 (0.50) E-3
9.56 (0.17) E-2
4.69 (2.51) E-4
4.27 (0.33) E-2
1.80 (0.01) E-1

3.58 (2.47) E-4
1.26 (0.12) E-1
1.94 (0.02) E-1
3.96 (1.18) E-2
2.35 (0.08) E-1
1.57 (0.10) E-1
1.53 (0.35) E-2
1.68 (0.03) E-1
1.63 (0.26) E-2

1.11 (0.01) E-1
8.94 (0.06) E-2
4.11 (0.46) E-3
9.34 (0.15) E-2
3.10 (1.73) E-4
1.30 (0.38) E-2
1.88 (0.01) E-1

5.81

4.12

4.84

4.75

3.56

3.59

5.31

4.00

Table 2. Mean weighted number of rules (standard error in parentheses) corresponding to Table 1. Best values in bold.

dataset

LR1

LRR

RuleFit

LR1N

LRRN

RuleFitN

banknote
heart
ILPD
ionosphere
liver
pima
tic-tac-toe
transfusion
WDBC

adult
bank-mkt
gas
magic
mushroom
musk
FICO

mean rank

32.3 (0.8)
13.4 (2.7)
14.6 (3.7)
114.4 (28.5)
28.9 (5.8)
22.1 (2.3)
21.6 (0.0)
15.2 (4.3)
145.7 (18.0)

87.1 (1.6)
0.0 (0.0)
483.7 (8.0)
93.1 (2.2)
24.7 (0.6)
263.0 (39.3)
92.6 (5.9)

54.9 (2.4)
7.6 (0.7)
11.6 (1.4)
122.8 (35.4)
16.9 (2.6)
25.9 (1.4)
78.2 (6.3)
17.6 (1.1)
271.4 (13.6)

83.2 (5.3)
68.9 (3.6)
694.8 (2.1)
202.1 (13.5)
30.1 (1.6)
1255.7 (25.4)
72.9 (5.1)

57.8 (0.7)
34.3 (0.9)
0.0 (0.0)
1007.3 (12.0)
66.7 (11.7)
64.8 (1.1)
1640.7 (99.2)
64.8 (1.1)
809.4 (89.6)

102.4 (4.6)
0.0 (0.0)
2663.1 (235.9)
496.7 (5.9)
1308.9 (207.5)
1152.1 (298.9)
239.8 (2.5)

16.4 (0.4)
14.3 (2.1)
14.7 (4.9)
130.3 (23.7)
25.7 (5.7)
12.1 (1.0)
21.6 (0.0)
24.3 (2.5)
86.1 (12.6)

85.8 (2.4)
0.0 (0.0)
950.2 (12.9)
97.9 (2.9)
24.7 (0.6)
313.9 (101.6)
81.0 (5.2)

50.6 (3.3)
6.0 (0.8)
11.3 (1.3)
122.6 (44.1)
14.0 (2.0)
18.7 (2.9)
78.2 (6.3)
12.4 (1.9)
248.4 (28.0)

104.7 (4.3)
61.8 (3.6)
1145.2 (25.6)
219.9 (21.5)
30.1 (1.6)
1348.3 (50.0)
51.8 (2.7)

1124.9 (67.7)
59.4 (2.4)
0.0 (0.0)
983.4 (145.2)
89.7 (36.8)
183.8 (34.9)
1640.7 (99.2)
183.8 (34.9)
562.3 (83.3)

719.9 (58.6)
0.2 (0.0)
2920.8 (125.7)
947.4 (7.0)
1308.9 (207.5)
2000.3 (314.8)
183.4 (3.0)

2.31

3.19

4.66

2.56

2.94

5.34

In Table 2, it is clear that RuleFit produces much more (3-
4 times) complex classiﬁers than all four proposed meth-
ods. The Friedman statistic of 34.01 (p-value ∼ 10−6) is
signiﬁcant as expected. Post-hoc comparisons to LRRN
conﬁrm that RuleFit and RuleFitN are signiﬁcantly more
complex, this time at the 0.05 level and again with Holm’s
correction. Note that including original numerical features
does not signiﬁcant increase classiﬁer complexity (compar-
ing LR1 with LR1N and LRR with LRRN).

5.2. Regression

For regression, we experimented with an additional 8
datasets, 7 of which are drawn from previous works on
rule ensembles (Friedman & Popescu, 2008; Dembczy´nski
et al., 2010) and the UCI repository (Dua & Karra Taniski-
dou, 2017). The last dataset comes from the Medical Ex-
penditure Panel Survey (MEPS) (Agency for Healthcare
Research and Quality, 2018) of the US Department of
Health and Human Services, speciﬁcally panel 19 from the
year 2015. The task is to predict the annual healthcare ex-
penditure of individuals based on demographics and self-
reported medical conditions.

The same two experiments are conducted for regression,
using the linear regression variants (6) of both the proposed
approaches as well as RuleFit. The coefﬁcient of deter-
mination R2 is chosen as the performance metric and the
model complexity metric is the same as in Section 5.1.

In Figure 2, we show trade-offs between R2 and weighted
number of rules for 3 of the datasets; the other 5 can be
found in Appendix B. RuleFit is a stronger competitor in
regression due to sometimes achieving higher R2 at higher
complexities. On 4 of 8 datasets, the curves for RuleFit(N)
remain below their LRR(N) counterparts as in Figure 2a.
On another 2 datasets, the curves cross at moderate to high
complexities as in Figure 2b, while in Figure 2c, RuleFit
obtains much higher R2. Among the proposed methods,
the advantage of LRR(N) vs. LR1(N) is generally larger in
regression than in classiﬁcation (cf. Figure 1), indicating
the beneﬁt of generating higher-degree rules. A possible
explanation may be that interaction terms matter more in
regression where the output range is wider.

Tables 3 and 4 show the results of selecting parameter λ0
through nested CV to maximize R2. The same overall con-

Generalized Linear Rule Models

(a) Communities and Crime

(b) Wine Quality

(c) Bike Sharing

Figure 2. Trade-offs between coefﬁcient of determination R2 and weighted number of rules.

Table 3. Mean test R2 (%, standard error in parentheses). Best values in bold.
GBT
LR1N
LR1

RuleFitN

RuleFit

LRRN

LRR

51.2 (1.2)
77.9 (4.1)
63.2 (0.5)
70.2 (0.4)
60.6 (1.7)
17.8 (0.6)
32.3 (1.0)
16.4 (1.5)

51.2 (1.3)
76.4 (4.2)
69.1 (0.5)
73.3 (0.4)
63.9 (1.3)
45.2 (1.3)
35.5 (1.2)
16.4 (1.4)

51.1 (1.1)
84.3 (2.3)
83.0 (0.4)
75.2 (0.3)
64.1 (1.5)
25.3 (3.4)
33.6 (0.9)
15.3 (1.6)

55.4 (1.3)
78.8 (3.5)
63.3 (0.5)
72.7 (0.3)
61.3 (2.2)
17.8 (0.6)
32.9 (0.9)
16.6 (1.5)

55.5 (1.3)
78.2 (3.7)
68.9 (0.5)
75.6 (0.3)
65.7 (1.2)
46.1 (1.2)
35.6 (1.4)
16.5 (1.4)

54.7 (1.3)
84.3 (2.2)
83.0 (0.4)
76.6 (0.3)
63.5 (1.6)
49.2 (7.9)
32.6 (1.1)
14.5 (1.4)

51.8 (1.0)
79.3 (3.5)
83.9 (0.3)
75.6 (0.4)
63.9 (1.2)
29.3 (0.8)
38.0 (1.1)
15.7 (1.5)

SVM

56.7 (1.0)
82.0 (2.7)
54.3 (0.5)
76.4 (0.4)
54.5 (1.0)
7.0 (0.3)
36.5 (0.8)
5.5 (0.6)

6.75

4.88

4.50

5.00

3.00

3.50

3.38

5.00

dataset

abalone
boston
bike
california
crime
parkinsons
wine
MEPS

mean rank

Table 4. Mean weighted number of rules (standard error in parentheses) corresponding to Table 3. Best values in bold.

dataset

LR1

LRR

RuleFit

LR1N

LRRN

RuleFitN

abalone
boston
bike
california
crime
parkinsons
wine
MEPS

mean rank

69.4 (1.0)
56.3 (3.0)
69.7 (1.0)
82.1 (0.6)
237.4 (2.8)
2.4 (0.0)
75.7 (2.2)
99.4 (2.2)

141.7 (13.0)
96.1 (21.3)
287.1 (14.7)
219.8 (2.9)
76.8 (5.8)
136.5 (5.9)
188.0 (10.9)
80.6 (7.4)

90.1 (3.5)
432.3 (41.1)
558.3 (29.6)
315.3 (11.3)
161.7 (3.7)
14.0 (2.0)
112.2 (7.9)
115.1 (3.3)

74.9 (0.9)
50.2 (3.1)
70.7 (0.9)
78.7 (0.7)
130.9 (16.5)
2.4 (0.0)
61.8 (1.7)
104.9 (2.7)

144.1 (13.8)
97.1 (24.9)
285.0 (16.0)
204.4 (3.8)
55.1 (5.9)
134.1 (4.5)
189.8 (9.5)
78.9 (7.6)

83.4 (2.1)
285.8 (18.2)
568.9 (28.0)
447.2 (15.9)
168.5 (5.0)
389.8 (120.2)
108.2 (11.6)
107.2 (3.1)

2.31

3.75

4.62

1.94

3.50

4.88

clusion as in Section 5.1 holds for LRRN in particular,
namely that it yields highly competitive R2 values while
using fewer rules than RuleFit. The Friedman statistic for
Table 3 is 13.63 (p-value 0.046), and in post-hoc compar-
isons to LRRN, the only signiﬁcant difference (0.05 level)
is again with LR1. Among the proposed methods, advan-
tages due to CG and/or numerical features in Figure 2 carry
over into Table 3. In Table 4, RuleFit(N) again yields more
complex solutions on average, although the difference is
not as large as in Table 2. The Friedman statistic is 16.16
(p-value 0.002); however, no post-hoc comparisons with
LRRN (which occupies a middle position) as the reference
show statistically signiﬁcant differences.

6. Discussion

The numerical results in Section 5 may raise the question
of the interpretability of rule ensembles with hundreds of
rules. First we note that because of the shape of many of

the trade-off curves, with steep improvement at low com-
plexity followed by a ﬂatter region, it may be possible to
obtain substantially simpler models, with say a few tens
of rules, that are not too far from maximum performance.
Apart from model simpliﬁcation, the fact that a rule en-
semble is also a linear model facilitates model inspection,
for example by focusing on the rules corresponding to the
largest coefﬁcients βk. Friedman & Popescu (2008) discuss
a slightly more reﬁned measure of rule importance, which
is also used to assess the importance of (original) input fea-
tures. Based on the discussion in Section 3, we also suggest
a division between singleton rules and linear terms on the
one hand, and higher-degree rules on the other. Since the
former constitute a GAM, their effect can be summarized
visually by univariate plots. The higher-degree rules can be
ranked and the interactions studied further as described in
(Friedman & Popescu, 2008).

The following extensions are suggested for future work: 1)
We have used a ﬁxed binarization of the features to facili-

020406080100120140weighted rules0.400.450.500.550.600.650.70R-squaredLR1LR1NLRRLRRNRuleFitRuleFitN0255075100125150175200weighted rules0.200.250.300.350.40R-squaredLR1LR1NLRRLRRNRuleFitRuleFitN0255075100125150175200weighted rules0.30.40.50.60.70.8R-squaredLR1LR1NLRRLRRNRuleFitRuleFitNGeneralized Linear Rule Models

tate formulation of the column generation subproblem (9)
as an IP. However, if CG is done using a heuristic, it may be
possible to reﬁne the binarization with each CG iteration.
Doing so may improve results, particularly for regression.
2) In the experiments in Section 5, we have ﬁxed the ratio
λ1/λ0 to match the deﬁnition of rule weight. One may also
vary this ratio to encourage or discourage longer rules, or
tune it to the level of interaction present in a dataset.

Acknowledgements

We thank Karthikeyan Natesan Ramamurthy for help with
the MEPS dataset.

References

Dembczy´nski, K., Kotłowski, W., and Słowi´nski, R. EN-
DER: a statistical framework for boosting decision rules.
Data Mining and Knowledge Discovery, 21(1):52–90,
Jul 2010.

Demiriz, A., Bennett, K. P., and Shawe-Taylor, J. Linear
programming boosting via column generation. Mach.
Learn., 46(1–3):225–254, January 2002.

Demˇsar, J. Statistical comparisons of classiﬁers over mul-
tiple data sets. J. Mach. Learn. Res., 7:1–30, January
2006.

Dua, D. and Karra Taniskidou, E. UCI machine learn-
ing repository, 2017. URL http://archive.ics.
uci.edu/ml.

Agency for Healthcare Research and Quality. Medical
Expenditure Panel Survey (MEPS). http://www.
ahrq.gov/data/meps.html, 2018. Last accessed
2019-01.

Fan, R.-E., Chang, K.-W., Hsieh, C.-J., Wang, X.-R., and
Lin, C.-J. LIBLINEAR: A library for large linear clas-
siﬁcation. Journal of Machine Learning Research, 9:
1871–1874, 2008.

Angelino, E., Larus-Stone, N., Alabi, D., Seltzer, M., and
In
Rudin, C. Learning certiﬁably optimal rule lists.
Proceedings of the 23rd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining
(KDD), pp. 35–44, 2017.

Bonami, P., Gleixner, A. M., Linderoth, J., and Misener,
R. Designing and implementing algorithms for mixed-
integer nonlinear optimization. In Report from Dagstuhl
Seminar 18081, 2018.

Bornd¨orfer, R., L¨obel, A., Reuther, M., Schlechte, T., and
Weider, S. Rapid branching. Public Transport, 5:3–23,
01 2013. doi: 10.1007/s12469-013-0066-8.

Clark, P. and Boswell, R. Rule induction with CN2:
Some recent improvements. In Proceedings of the Eu-
ropean Working Session on Machine Learning (EWSL),
pp. 151–163, 1991.

Cohen, W. W. Fast effective rule induction. In Proc. Int.

Conf. Mach. Learn. (ICML), pp. 115–123, 1995.

Cohen, W. W. and Singer, Y. A simple, fast, and effective
rule learner. In Proc. Conf. Artif. Intell. (AAAI), pp. 335–
342, 1999.

Conforti, M., Cornuejols, G., and Zambelli, G. Integer pro-

gramming. Springer, 2014.

Dash, S., G¨unl¨uk, O., and Wei, D. Boolean decision
In Proc. Thirty-second
rules via column generation.
Conference on Neural Information Processing Systems
(NeurIPS), 2018.

FICO. FICO Explainable Machine Learning Challenge.
https://community.fico.com/community/
xml, 2018. Last accessed 2019-05.

Fortz, B., Labbe, M., and Poss, M. A branch-and-cut-and-
price framework for convex MINLP applied to a stochas-
tic network design problem. In Proc. European Work-
shop on MINLP, pp. 131–139, 2010.

Friedman, J. H. and Popescu, B. E. Predictive learning via
rule ensembles. Annals of Applied Statistics, 2(3):916–
954, Jul 2008.

F¨urnkranz, J., Gamberger, D., and Lavraˇc, N. Foundations

of Rule Learning. Springer-Verlag, Berlin, 2014.

Garcia, R., Marin, A., and Patriksson, M. Column gener-
ation algorithms for nonlinear optimization, I: Conver-
gence analysis. Optimization, 52(2):171–200, 2003.

Gilmore, P. C. and Gomory, R. E. A linear programming
approach to the cutting-stock problem. Operations Re-
search, 9:849–859, 1961.

Hastie, T. J. and Tibshirani, R. J. Generalized Addi-
ISBN

tive Models. Chapman and Hall/CRC, 1990.
9780412343902.

Hern´andez-Orallo, J., Flach, P., and Ferri, C. A uni-
ﬁed view of performance metrics: Translating threshold
choice into expected classiﬁcation loss. J. Mach. Learn.
Res., 13:2813–2869, October 2012.

Jawanpuria, P., Nath, J. S., and Ramakrishnan, G. Efﬁ-
cient rule ensemble learning using hierarchical kernels.
In Proc. Int. Conf. Mach. Learn. (ICML), 2011.

Generalized Linear Rule Models

Lakkaraju, H., Bach, S. H., and Leskovec, J. Interpretable
decision sets: A joint framework for description and pre-
diction. In Proc. ACM SIGKDD Int. Conf. Knowl. Disc.
Data Mining (KDD), pp. 1675–1684, 2016.

McCullagh, P. and Nelder, J. Generalized Linear Models,

Second Edition. Chapman & Hall, 1989.

Platt, J. C. Probabilistic outputs for support vector ma-
chines and comparisons to regularized likelihood meth-
ods. In Advances in Large Margin Classiﬁers, pp. 61–74,
1999.

Rivest, R. L. Learning decision lists. Machine Learning, 2

(3):229–246, 1987.

R¨uckert, U. and Kramer, S. A statistical approach to rule
learning. In Proc. Int. Conf. Mach. Learn. (ICML), pp.
785–792, 2006.

Rudin, C. and Ertekin, S¸ . Learning customized and op-
timized lists of rules with mathematical programming.
Mathematical Programming Computation, 10(4):659–
702, Dec 2018.

Su, G., Wei, D., Varshney, K. R., and Malioutov, D. M.
Learning sparse two-level Boolean rules. In Proc. IEEE
Int. Workshop Mach. Learn. Signal Process. (MLSP), pp.
1–6, September 2016.

Wang, T., Rudin, C., Doshi-Velez, F., Liu, Y., Klampﬂ, E.,
and MacNeille, P. A Bayesian framework for learning
rule sets for interpretable classiﬁcation. Journal of Ma-
chine Learning Research, 18(70):1–37, 2017.

Weiss, S. M. and Indurkhya, N. Lightweight rule induc-
tion. In Proc. Int. Conf. Mach. Learn. (ICML), pp. 1135–
1142, 2000.

Yang, H., Rudin, C., and Seltzer, M. Scalable Bayesian
rule lists. In Proc. Int. Conf. Mach. Learn. (ICML), pp.
1013–1022, 2017.

A. Variations on the Column Generation

B.2. Regression

Generalized Linear Rule Models

Figure 7 shows the trade-off between R2 and weighted
rules for all 8 regression datasets.

Heuristic

We have explored the following three variations of the
heuristic algorithm for column generation described in Sec-
tion 4:

1. The algorithm can return the best K solutions that it
ﬁnds instead of a single incumbent solution, poten-
tially reducing the number of CG iterations needed.
We have observed however that these solutions tend to
correspond to very similar conjunctions and are hence
highly correlated. By allowing multiple such columns
to enter together, sparsity suffers because the (cid:96)1 reg-
ularization in (4) has difﬁculty favoring sparse linear
combinations of highly correlated columns over dense
ones. For this reason we kept K = 1.

2. The algorithm can be generalized to a beam search by
considering the children of B > 1 parent conjunctions
at each degree instead of a single parent. The best B
children according to a combination of metrics (10)
and (11) are then chosen to become the next parents.
To date however, we have not found setting B > 1 to
be beneﬁcial.

3. The algorithm can be terminated early once a solution
with negative objective value is found since any such
solution corresponds to a descent direction for prob-
lem (7). Termination can be immediate or occur after
the current degree. While early termination speeds up
each CG iteration, the number of iterations tends to
increase because the generated columns are of lower
quality.

B. Additional Numerical Results

B.1. Classiﬁcation

Figures 3–6 show trade-offs between Brier score and
weighted rules and between accuracy and weighted rules
for all 16 classiﬁcation datasets.

Tables 5 and 6 show mean test accuracies and correspond-
ing complexities when the methods are optimized for accu-
racy. For Table 5, the Friedman statistic computed from the
mean ranks is 9.74 with a p-value of 0.202, indicating no
statistically signiﬁcant differences in accuracy among the
methods. For Table 6, the Friedman statistic is 44.61 (p-
value ∼ 10−12). Post-hoc comparisons with LRRN as the
reference show that RuleFit and RuleFitN are signiﬁcantly
more complex at the 0.05 level using Holm’s step-down
procedure.

Generalized Linear Rule Models

Table 5. Mean test accuracies (%, standard error in parentheses). Best values in bold.

dataset

LR1

LRR

RuleFit

LR1N

LRRN

RuleFitN

GBT

SVM

banknote
heart
ILPD
ionosphere
liver
pima
tic-tac-toe
transfusion
WDBC

99.8 (0.1)
80.9 (1.6)
71.0 (1.1)
91.2 (1.2)
61.2 (2.0)
75.5 (1.6)
98.3 (0.4)
76.7 (0.3)
97.0 (0.6)

99.7 (0.1)
84.3 (2.0)
70.8 (0.5)
91.2 (1.3)
59.1 (2.2)
75.1 (1.4)
98.0 (0.6)
79.0 (0.9)
97.9 (0.5)

84.9 (0.2)
adult
88.7 (0.0)
bank-mkt
99.5 (0.0)
gas
84.9 (0.3)
magic
mushroom 100.0 (0.0)
96.8 (0.5)
musk
73.8 (0.3)
FICO

84.9 (0.2)
90.0 (0.1)
99.6 (0.1)
85.4 (0.3)
100.0 (0.0)
98.4 (0.1)
73.8 (0.2)

99.6 (0.1)
83.3 (1.3)
71.5 (0.1)
93.4 (1.5)
58.0 (2.2)
75.5 (1.9)
100.0 (0.0)
75.5 (1.9)
97.9 (0.4)

84.8 (0.2)
88.7 (0.0)
99.5 (0.1)
86.7 (0.2)
100.0 (0.0)
97.6 (0.3)
73.8 (0.2)

100.0 (0.0)
81.3 (1.8)
70.6 (0.9)
91.7 (1.1)
60.0 (2.6)
77.7 (1.3)
98.3 (0.4)
78.7 (0.7)
97.2 (0.7)

85.8 (0.1)
88.7 (0.0)
99.6 (0.0)
85.1 (0.3)
100.0 (0.0)
96.1 (0.7)
74.0 (0.2)

99.9 (0.1)
84.6 (1.9)
70.8 (0.7)
90.9 (1.6)
58.0 (2.7)
75.8 (1.6)
98.0 (0.6)
79.3 (1.0)
98.2 (0.4)

85.9 (0.1)
90.1 (0.1)
99.5 (0.1)
85.4 (0.2)
100.0 (0.0)
98.4 (0.2)
73.9 (0.2)

99.9 (0.1)
83.3 (2.0)
71.7 (1.1)
94.3 (1.3)
58.6 (2.1)
74.7 (1.9)
100.0 (0.0)
74.7 (1.9)
96.8 (0.5)

87.0 (0.2)
88.7 (0.0)
99.6 (0.0)
87.5 (0.2)
100.0 (0.0)
97.8 (0.2)
74.0 (0.2)

99.7 (0.1)
82.3 (1.8)
71.8 (0.2)
91.2 (1.8)
57.1 (2.5)
75.9 (1.9)
99.1 (0.2)
76.6 (0.3)
95.6 (0.6)

84.8 (0.2)
89.9 (0.1)
99.4 (0.1)
87.2 (0.2)
99.9 (0.1)
94.5 (0.5)
73.3 (0.2)

99.9 (0.1)
82.6 (1.4)
71.7 (0.2)
94.9 (1.4)
58.8 (2.7)
77.1 (2.0)
98.3 (0.4)
76.9 (0.3)
98.1 (0.4)

84.8 (0.1)
88.7 (0.0)
99.5 (0.1)
87.4 (0.2)
100.0 (0.0)
97.6 (0.7)
72.4 (0.4)

mean rank

5.25

4.31

4.84

4.16

3.78

3.69

5.75

4.22

Table 6. Mean weighted number of rules (standard error in parentheses) corresponding to Table 5. Best values in bold.

dataset

banknote
heart
ILPD
ionosphere
liver
pima
tic-tac-toe
transfusion
WDBC

adult
bank-mkt
gas
magic
mushroom
musk
FICO

mean rank

LR1

LRR

RuleFit

LR1N

LRRN

RuleFitN

32.3 (0.8)
13.4 (2.7)
14.6 (3.7)
114.4 (28.5)
28.9 (5.8)
22.1 (2.3)
21.6 (0.0)
15.2 (4.3)
145.7 (18.0)

87.1 (1.6)
0.0 (0.0)
483.7 (8.0)
93.1 (2.2)
24.7 (0.6)
263.0 (39.3)
92.6 (5.9)

47.2 (3.4)
5.7 (0.6)
38.1 (25.5)
85.2 (22.6)
20.8 (4.7)
27.7 (1.8)
67.1 (3.5)
17.8 (1.1)
283.6 (10.3)

91.5 (4.8)
68.6 (9.5)
678.2 (17.6)
177.2 (17.5)
18.2 (0.9)
1002.0 (71.8)
65.9 (3.4)

57.8 (0.7)
34.3 (0.9)
0.0 (0.0)
1022.6 (64.9)
66.7 (11.7)
64.8 (1.1)
1640.7 (99.2)
64.8 (1.1)
809.4 (89.6)

425.5 (35.0)
0.0 (0.0)
2663.1 (235.9)
496.7 (5.9)
927.9 (58.6)
1796.4 (326.1)
239.8 (2.5)

16.4 (0.4)
14.3 (2.1)
14.7 (4.9)
130.3 (23.7)
25.7 (5.7)
12.1 (1.0)
21.6 (0.0)
24.3 (2.5)
86.1 (12.6)

85.8 (2.4)
0.0 (0.0)
950.2 (12.9)
97.9 (2.9)
24.7 (0.6)
313.9 (101.6)
81.0 (5.2)

47.7 (1.6)
5.2 (0.4)
1.9 (1.9)
150.7 (49.3)
34.7 (14.2)
15.5 (2.3)
67.1 (3.5)
11.9 (1.4)
228.4 (30.3)

94.2 (6.2)
83.6 (4.9)
1259.4 (45.8)
196.2 (25.0)
18.2 (0.9)
1079.7 (78.4)
56.2 (4.1)

1124.9 (67.7)
59.4 (2.4)
2106.0 (30.3)
1225.6 (81.3)
89.7 (36.8)
3211.5 (83.3)
1640.7 (99.2)
3211.5 (83.3)
562.3 (83.3)

719.9 (58.6)
0.2 (0.0)
2920.8 (125.7)
1656.0 (11.7)
927.9 (58.6)
2000.3 (314.8)
183.4 (3.0)

2.25

2.88

4.75

2.38

3.06

5.69

Generalized Linear Rule Models

(a) banknote

(b) heart

(c) ILPD

(d) ionosphere

(e) liver

(f) pima

(g) tic-tac-toe

(h) transfusion

Figure 3. Trade-offs between Brier score and weighted number of rules on classiﬁcation datasets. Pareto efﬁcient points are connected
by line segments. Horizontal and vertical bars represent standard errors in the means.

01020304050607080weighted rules0.000.020.040.060.080.10Brier scoreLR1LR1NLRRLRRNRuleFitRuleFitN02468101214weighted rules0.120.140.160.180.20Brier scoreLR1LR1NLRRLRRNRuleFitRuleFitN05101520253035weighted rules0.170.180.190.200.210.220.23Brier scoreLR1LR1NLRRLRRNRuleFitRuleFitN020406080weighted rules0.040.060.080.100.120.140.160.18Brier scoreLR1LR1NLRRLRRNRuleFitRuleFitN051015202530weighted rules0.220.240.260.280.300.320.34Brier scoreLR1LR1NLRRLRRNRuleFitRuleFitN01020304050weighted rules0.160.180.200.220.24Brier scoreLR1LR1NLRRLRRNRuleFitRuleFitN020406080100weighted rules0.000.050.100.150.20Brier scoreLR1LR1NLRRLRRNRuleFitRuleFitN051015202530weighted rules0.1450.1500.1550.1600.1650.170Brier scoreLR1LR1NLRRLRRNRuleFitRuleFitNGeneralized Linear Rule Models

(a) WDBC

(b) adult

(c) bank-marketing

(d) gas

(e) magic

(f) mushroom

(g) musk

(h) FICO

Figure 4. Trade-offs between Brier score and weighted number of rules on classiﬁcation datasets. Pareto efﬁcient points are connected
by line segments. Horizontal and vertical bars represent standard errors in the means.

0255075100125150175200weighted rules0.010.020.030.040.05Brier scoreLR1LR1NLRRLRRNRuleFitRuleFitN020406080100120140weighted rules0.090.100.110.120.130.14Brier scoreLR1LR1NLRRLRRNRuleFitRuleFitN020406080100weighted rules0.0760.0780.0800.0820.084Brier scoreLR1LR1NLRRLRRNRuleFitRuleFitN0255075100125150175200weighted rules0.0000.0250.0500.0750.1000.1250.1500.175Brier scoreLR1LR1NLRRLRRNRuleFitRuleFitN0255075100125150175200weighted rules0.100.110.120.130.140.150.160.170.18Brier scoreLR1LR1NLRRLRRNLRRIRuleFitRuleFitN01020304050weighted rules0.000.020.040.060.08Brier scoreLR1LR1NLRRLRRNRuleFitRuleFitN0255075100125150175200weighted rules0.020.040.060.08Brier scoreLR1LR1NLRRLRRNRuleFitRuleFitN020406080100120140weighted rules0.1700.1750.1800.1850.1900.1950.200Brier scoreLR1LR1NLRRLRRNRuleFitRuleFitNGeneralized Linear Rule Models

(a) banknote

(b) heart

(c) ILPD

(d) ionosphere

(e) liver

(f) pima

(g) tic-tac-toe

(h) transfusion

Figure 5. Trade-offs between accuracy and weighted number of rules on classiﬁcation datasets. Pareto efﬁcient points are connected by
line segments. Horizontal and vertical bars represent standard errors in the means.

010203040506070weighted rules8486889092949698100% accuracyLR1LR1NLRRLRRNRuleFitRuleFitN01234567weighted rules7476788082848688% accuracyLR1LR1NLRRLRRNRuleFitRuleFitN0.001000.000750.000500.000250.000000.000250.000500.000750.00100weighted rules67686970717273% accuracyLR1LR1NLRRLRRNRuleFitRuleFitN020406080weighted rules7580859095% accuracyLR1LR1NLRRLRRNRuleFitRuleFitN051015202530weighted rules52545658606264% accuracyLR1LR1NLRRLRRNRuleFitRuleFitN05101520253035weighted rules687072747678% accuracyLR1LR1NLRRLRRNRuleFitRuleFitN020406080weighted rules707580859095100% accuracyLR1LR1NLRRLRRNRuleFitRuleFitN051015202530weighted rules767778798081% accuracyLR1LR1NLRRLRRNRuleFitRuleFitNGeneralized Linear Rule Models

(a) WDBC

(b) adult

(c) bank-marketing

(d) gas

(e) magic

(f) mushroom

(g) musk

(h) FICO

Figure 6. Trade-offs between accuracy and weighted number of rules on classiﬁcation datasets. Pareto efﬁcient points are connected by
line segments. Horizontal and vertical bars represent standard errors in the means.

0255075100125150175200weighted rules9293949596979899% accuracyLR1LR1NLRRLRRNRuleFitRuleFitN020406080100120140160weighted rules767880828486% accuracyLR1LR1NLRRLRRNRuleFitRuleFitN020406080100weighted rules88.889.089.289.489.689.890.090.2% accuracyLR1LR1NLRRLRRNRuleFitRuleFitN0255075100125150175200weighted rules7580859095100% accuracyLR1LR1NLRRLRRNRuleFitRuleFitN0255075100125150175200weighted rules7476788082848688% accuracyLR1LR1NLRRLRRNLRRIRuleFitRuleFitN0510152025weighted rules9092949698100% accuracyLR1LR1NLRRLRRNRuleFitRuleFitN0255075100125150175200weighted rules889092949698% accuracyLR1LR1NLRRLRRNRuleFitRuleFitN020406080100120140weighted rules7172737475% accuracyLR1LR1NLRRLRRNRuleFitRuleFitNGeneralized Linear Rule Models

(a) abalone

(b) boston

(c) bike

(d) california

(e) crime

(f) parkinsons

(g) wine

(h) MEPS

Figure 7. Trade-offs between coefﬁcient of determination R2 and weighted number of rules on regression datasets. Pareto efﬁcient
points are connected by line segments. Horizontal and vertical bars represent standard errors in the means.

0255075100125150175weighted rules0.350.400.450.500.55R-squaredLR1LR1NLRRLRRNRuleFitRuleFitN0255075100125150175weighted rules0.50.60.70.80.9R-squaredLR1LR1NLRRLRRNRuleFitRuleFitN0255075100125150175200weighted rules0.30.40.50.60.70.8R-squaredLR1LR1NLRRLRRNRuleFitRuleFitN0255075100125150175200weighted rules0.40.50.60.70.8R-squaredLR1LR1NLRRLRRNRuleFitRuleFitN020406080100120140weighted rules0.400.450.500.550.600.650.70R-squaredLR1LR1NLRRLRRNRuleFitRuleFitN0255075100125150175200weighted rules0.20.30.40.50.60.7R-squaredLR1LR1NLRRLRRNRuleFitRuleFitN0255075100125150175200weighted rules0.200.250.300.350.40R-squaredLR1LR1NLRRLRRNRuleFitRuleFitN020406080100120140weighted rules0.050.100.150.200.250.30R-squaredLR1LR1NLRRLRRNRuleFitRuleFitN