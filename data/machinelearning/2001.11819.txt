0
2
0
2

n
a
J

2
2

]
L
P
.
s
c
[

1
v
9
1
8
1
1
.
1
0
0
2
:
v
i
X
r
a

Joint Distributions for TensorFlow Probability

DAN PIPONI†, DAVE MOORE† & JOSHUA V. DILLON, Google Research

A central tenet of probabilistic programming is that a model is speciﬁed exactly once in a canonical
representation which is usable by inference algorithms. We describe JointDistributions, a family
of declarative representations of directed graphical models in TensorFlow Probability.

1 INTRODUCTION

TensorFlow Probability JointDistributions enable a variety of idioms for probabilistic model speci-
ﬁcation while providing a standardized interface to inference algorithms. Our design thesis is that
there is no single best way to specify joint distributions. Some users may prefer an imperative-
style “probabilistic program,” while others may prefer a declarative representation of graphical
model structure. JointDistribution invites subclasses to implement diﬀerent semantics for speci-
fying models, some of which we discuss below. Since subclasses share a common output contract,
inference algorithms can access them interchangeably and are isolated from the details of model
speciﬁcation.

While many frameworks separate the low-level distribution abstraction from higher-level prob-
abilistic modeling tools, TFP joint distributions are TensorFlow Distributions [Dillon et al. 2017]:
the JointDistribution contract is a reﬁnement of the Distribution contract. Joint distributions pro-
vide sample and log_prob methods, which respectively draw a joint sample and compute a joint log-
density. This enables ﬂexible APIs that work seamlessly with both single and joint distributions.
Joint distributions naturally extend the concepts of event, batch, and sample shape that organize
the Tensor dimensions of Distribution values.

Working in TensorFlow oﬀers the potential to exploit hardware acceleration for vectorized
sampling and inference. JointDistributions support manual and automatic vectorization of mod-
els, making it easy to explore new inference paradigms such as massively multi-chain MCMC
[Hoﬀman and Ma 2019] and multi-sample variational bounds [Burda et al. 2015; Mnih and Rezende
2016; Tucker et al. 2018].

2 THE JOINT DISTRIBUTION INTERFACE

JointDistribution extends the existing interface of TensorFlow Distributions [Dillon et al. 2017].
The Distribution abstraction provides a consistent interface to a large library of probability distri-
butions, of which most are deﬁned over Tensors: scalars, vectors, matrices, or batches (replications)
thereof. By contrast, a joint distribution is deﬁned over a structure of Tensors: a Python list, tuple,
dict, or a nested combination of these with tensors at the leaves. For example, a joint distribution’s
sample may be a list of tensors, s = [a, b], or we may alternatively choose to deﬁne the same distri-
bution with dictionary-valued samples, s = {'a': a, 'b': b}. Because hardware-accelerated vector-
ization is generally most eﬃcient in the SIMD setting where all threads execute the same control
ﬂow, our current implementation assumes that the structure itself is the same across all execu-
tions.1

1The extension to stochastic control ﬂow is interesting future work.

†The ﬁrst two authors contributed equally.
Author’s address: Dan Piponi†, Dave Moore† & Joshua V. Dillon, Google Research, tfprobability@tensorflow.org.

 
 
 
 
 
 
2

Dan Piponi†, Dave Moore† & Joshua V. Dillon

2.1 Structured shapes
Recall that TensorFlow Distribution [Dillon et al. 2017] methods adhere to a “Tensor-in, Tensor-out”
design. Each Distribution conceptually partitions a Tensor’s shape into three groups. From left to
right: sample shape indexes independent and identically distributed draws, batch shape indexes
independent but non-identical draws, and event shape describes a single draw from the underlying
distribution. JointDistributions extend this design to structures: each Tensor within the structure
is a sample from a conditional distribution, and its shape is interpreted by that distribution under
the existing contract.

Just as in Dillon et al. [2017], properties such as dtype, batch_shape, event_shape are unchang-
ing for the lifetime of the joint distribution object. In joint distributions these properties become
structures, matching the structure of the sample.2 For example, where tensor-valued distributions
have a dtype, such as tf.float32, that speciﬁes the type of their samples, the dtype of a joint dis-
tribution also speciﬁes its structure; for example, jd.dtype = {'a': tf.int32, 'b': float64} speciﬁes
the number of model variables and their names and individual dtypes, and also that samples are
dictionary-valued.

2.2 Probabilistic computations

Many methods of tensor-valued Distributions straightforwardly generalize to joint distributions.
For a joint distribution jd, x = jd.sample() returns a sampled structure of tensors x in one-to-one
correspondence with jd.dtype. Given a structure x, jd.log_prob(x) returns the Tensor-valued joint
log-density of that structure. Most other probabilistic computations generalize similarly, though
not all extend to all joint distributions. For example, joint distributions only implement an analytic
mean, standard deviation and entropy in the special case where their components are independent.
Joint distributions also add a new method, sample_distributions, which takes a sample_shape and
optional value, runs the model forward, returning the resulting structures of sampled conditional
distributions ds and values xs. This allows users to access ﬁner-grained computations, for example
computing every local conditional log-density as [d.log_prob(x)for (d, x)in zip(ds, xs)]. The op-
tional value argument conditions the forward sampling on a partially speciﬁed joint value; that is,
a structure some of whose leaves are None. For example, to compute the posterior predictive distri-
butions over observables we might set latent values to posterior samples obtained from inference
but leave their children unspeciﬁed, to be generated by forward sampling.

Like all TensorFlow distributions, gradients of log_prob and sample (for reparameterizeable distri-
butions) with respect to any model parameters are available by reverse-mode automatic diﬀerenti-
ation. Gradient-based inference methods such as Hamiltonian Monte Carlo [Hoﬀman and Gelman
2014; Neal 2011] and black-box variational inference [Kucukelbir et al. 2017; Ranganath et al. 2014]
are immediately applicable, although users are of course also free to build custom samplers or more
structured variational models.

3 JOINT DISTRIBUTION FLAVORS
Users construct joint distributions by instantiating one of several subclass ‘ﬂavors’, each of which
provides a diﬀerent interface to a common underlying representation. This ‘let a thousand ﬂowers
bloom’ philosophy allows for experimentation in interface design targeting distinct audiences and
use cases. In this section we describe three ﬂavors of joint distribution currently implemented in
TFP, acknowledging that this is only a small slice of the possibility space.

2Section 4.1 discusses some subtleties particular to batch shapes.

Joint Distributions for TensorFlow Probability

3

All ﬂavors described in this paper specify a joint distribution by a sequence of conditional dis-

tributions. By the chain rule of probability,

P(X1, X2, . . . , Xn) = P(X1)P(X2|X1) . . . P(Xn |X1, . . . Xn−1)

(1)

any joint distribution can be written in this form. We leave the exploration of joint distributions
speciﬁed in other ways for future work.

Under the hood, the sample sites of a joint distribution are addressed using a ﬂattening of its
nested structure into a sequential list representation. Each ﬂavor implements the method ds, xs =
jd._flat_sample_distributions(sample_shape, values=None) which returns lists of distributions and
values in a canonical order, as well as the methods struct = jd._model_unflatten(list_) and list_ =
jd._model_flatten(struct) which convert between the canonically ordered internal representation
and the user-visible structure.

s

m

s ∼ InverseGamma(3, 2)
m ∼ Normal(0, 1)
x ∼ Normal(m, s)

x

simple_model = (

tfd . J o i n t D i s t r i b u t i o n Se q ue n ti a l([

tfd . InverseGamma ( concentration =3. ,
# s
# m

tfd . Normal ( loc =0. , scale =1.) ,
lambda m , s : tfd . Normal (

scale =2.) ,

loc =m , scale = s ) ])

# x

Fig. 1. A simple model with conjugate priors on the parameters of a Normal distribution, as a graphical
model (left), generative process (center), and JointDistributionSequential (right).

3.1 JointDistributionSequential
The ﬁrst ﬂavor of joint distribution we consider is JointDistributionSequential. It is constructed
by providing a list of the distributions in the model, using functions to represent distributions
conditioned on earlier variables.

Consider the example in Figure 1. Both s and m are independent and are represented by the ﬁrst
two elements in the list. The variable x is conditioned on both s and m, expressed using a lambda
expression with two arguments. Note that the distributions of s and m are not given names in the
code (except in comments). Arguments to the function are matched with earlier distributions by
working up through the list, one element at the time. So the argument m refers to the previous
element and s refers to the element two before.

On sampling, JointDistributionSequential draws from each component distribution in turn. When
it encounters a function representing a conditional distribution it provides, as arguments, the ap-
propriate samples generated earlier during the process.

When computing the log-density for a given structure [s, m, x], the log-density for each ele-
ment is computed using the corresponding distribution object. For conditional distributions, the
given elements are provided as arguments to the lambda expression. This process is carried out
for all three component distributions and following Equation 1, the sum of the log-densities is
returned.

Sample shape. When calling the sample() method of a JointDistributionSequential users
3.1.1
can also provide a sample_shape argument (see Sec. 4) to draw multiple samples. This requires
each component distribution in our models to draw multiple samples. In our example above, the
JointDistributionSequential object supplies the sample_shape argument to the distribution for s.
Now s contains a tensor of samples. Similarly m will contain a tensor of samples. Because Normal

4

Dan Piponi†, Dave Moore† & Joshua V. Dillon

treats tensor-valued parameters as a request for a batch of samples (see Section 4.1), the distri-
bution for x automatically generates a tensor with a shape matching that of s and m. This means
the second normal distribution doesn’t need to be provided with a sample_shape argument. The
JointDistributionSequential object can automatically infer that it doesn’t need to provide this ar-
gument because the third distribution is a distribution function rather than a distribution object.

Often we wish to repeatedly draw variables from a distribution within a joint distribution, cor-
responding to plate notation for graphical models. For this we provide the Sample class that draws
multiple samples as a single event. We also oﬀer the Independent class to allow the reorganization
of batches as individual sample events.

The probabilistic matrix factorization model [Mnih and Salakhutdinov 2008] demonstrates a

nested plate structure.

U

users

R

items

V

Ui j ∼ Normal(0, σU )
Vik ∼ Normal(0, σV )

Rjk ∼ Normal

Ui jVik, σ

dist = tfd . J o i n t D i s t r i b u t i o n S e qu e nt ia l(

[ tfd . Sample ( tfd . Normal ( loc =0. , scale = user_trait_sc al e ) ,

sample_shape =[ n_factors , n_users ]) ,

tfd . Sample ( tfd . Normal ( loc =0. , scale = item_trait_sc al e ) ,

sample_shape =[ n_factors , n_items ]) ,

lambda v , u : tfd . Independent (

tfd . Normal ( loc = tf . matmul (u , v , adjoint_a = True ) ,

# U

# V

# R

i
Õ

!

scale = o b s e r v a t i o n _ n o i s e_ s ca le) , r e i n t e r p r e t e d _ b a t c h _n d im s=2) ])

Fig. 2. Probabilistic matrix factorization

Both u and v are matrices of i.i.d. normal variables so we use Sample to draw these as single
events. The product of these matrices is a parameter to the third Normal distribution. Because this
matrix is interpreted as a batch parameter we get a batch of normal variates. As we wish to treat
the entire matrix as a single event we use the Independent distribution to indicate that the two
matrix dimensions should be considered as event dimensions.

3.2 JointDistributionNamed

simple_model = tfd. JointDistributionNamed( dict(

m=tfd. Normal (loc =0. , scale =1.),
s=tfd. InverseGamma( concentration=3. , scale =2.) ,
x=lambda m, s: tfd. Normal (loc=m, scale=s)))

When using JointDistributionSequential the individual distributions are not named and distribu-
tions are matched with function arguments implicitly. JointDistributionNamed provides explicit
naming. The user provides a dictionary of distributions and functions similar to the list provided to
JointDistributionSequential. Sampling and log-density computation is also similar, using a topolog-
ical sort to determine a suitable order to process the distributions. When sampled, JointDistributionNamed
returns a dictionary of named element samples.

 
Joint Distributions for TensorFlow Probability

5

3.3 JointDistributionCoroutine
Python provides a type of coroutine [Van Rossum and Eby 2005]. A type of function known as
a generator, once called, “yields” values. A value yielded by a yield expression or statement is
returned to the caller. Unlike a standard function, the generator remains in an idle state and the
caller can subsequently wake it and transfer control back to it, optionally providing a return value
for the yield expression with which the generator continues.

We use this to provide another way to allow users to write models. Consider Figure 1 again:

def simple ():

Root = tfd. JointDistributionCoroutine.Root
m = yield Root(tfd. Normal (loc =0. , scale =1.))
s = yield Root(tfd. InverseGamma( concentration=3. , scale =2.))
x = yield tfd. Normal (loc=m, scale=s)

simple_model = tfd. JointDistributionCoroutine( simple_model)

(For now, ignore the Roots.) When simple yields its ﬁrst distribution it isn’t immediately assigned to
the variable m. It is returned to the caller and when the generator is resumed, the caller determines
what value is sent back and assigned to m.

When sampling, the caller samples each yielded distribution and resumes the generator with the
sample. When computing the log-density of a given structure, the log-density for each element is
computed according to the yielded distributions. Instead of sampling the distributions, each given
element is provided to the resumed generator.

The JointDistributionSequential object automatically infers whether a distribution is condi-
tioned on an earlier random variable and uses this to decide when to supply a distribution with
a sample_shape argument. JointDistributionCoroutine is unable to make this inference so we use
the Root function to wrap distributions that require a sample_shape argument. These correspond to
nodes in a graphical model with no parent nodes.

3.3.1 Latent Dirichlet Allocation. Figure 3 shows a simpliﬁed latent Dirichlet model. The variable
n is the number of words in a document. If we represent multiple documents as rows of an array,
each row would have a diﬀerent length, requiring a ragged array. This is not well suited to SIMD
parallelism. Instead, because LDA treats a document as a bag of words, we use z and w to represent
the number of occurrences of each topic and word respectively, giving ﬁxed width arrays.

topics

words

θ

α

z

n

w

β

def lda_model () :

Root = tfd . J o i n t D i s t r i b u t i o n Co r ou t in e. Root
n = yield Root ( tfd . Poisson ( rate = avg_doc_length ) )
theta = yield Root ( tfd . Dirichlet ( concentration =

tfp . util . T r a n s f o r m e d V a r ia bl e( alpha , tfb . Softplus () )) )

z = yield tfd . Multinomial ( total_count =n , probs = theta )
w = yield tfd . Independent (

tfd . Multinomial ( total_count =z , logits = tf . Variable ( beta ) ) ,
r e i n t e r p r e t e d _ b a t ch _ nd i ms=1)

lda = tfd . J o i n t D i s t r i b u t i o n C or o ut i ne( lda_model )

3. Latent Dirichlet Allocation

Fig.
JointDistributionCoroutine model (right). Note that α, β are learnable via Type-II MLE; see Sec. 5.

plated

graph

(LDA)

(left)

and

corresponding

as

a

In our implementation using JointDistributionCoroutine we wrap the distributions for n and
theta with Root because they have no parent nodes. When drawing samples for w we treat z as a
batch of counts so we have one Multinomial draw for each topic. But we wish the batch to be treated
not as separate samples but as the array of counts for a single document, so we use Independent to
group these samples together as a single event.

6

Dan Piponi†, Dave Moore† & Joshua V. Dillon

4 VECTORIZED SAMPLING AND INFERENCE
Deep learning frameworks such as TensorFlow [Abadi et al. 2016], PyTorch [Paszke et al. 2019],
and JAX [Bradbury et al. 2018] enable seamless use of vectorized accelerator hardware, such as
GPUs, TPUs, and CPU SIMD instruction sets (e.g., SSE, AVX). The ability to run thousands of
Markov chains or sample thousands of particles simultaneously opens up new paradigms for infer-
ence algorithms and diagnostics (e.g., Hoﬀman and Ma [2019]), and to scale probabilistic modeling
by vectorizating over batches of data.

Joint distributions support vectorized operations, including sampling and log-density evalua-
tion: jd.sample(sample_shape=N) returns N independent realizations of the joint distribution, repre-
sented as a structure in which each component Tensor has a leading dimension of size N. The log
density jd.log_prob(jd.sample(N)) has shape [N], i.e., one joint log-density for each possible world
realized in the sample. Vectorized execution of joint distributions enables multiple MCMC chains,
multi-sample variational bounds, and multiple optimizations from diﬀerent initializations.

4.1 Batch semantics
In addition to drawing and evaluating i.i.d. samples as discussed above, it is often valuable to
work with a batch of independent but diﬀerently-parameterized distributions. By default, the joint
distribution generalization of jd.batch_shape is (like other properties in Sec. 2.1) is just the structure
aggregating batch shapes of the individual conditional distributions. Allowing components to have
diﬀerent, but broadcastable, batch shapes can save memory by sharing some local values between
multiple (conceptual) batches of the entire joint distribution. This local, per-distribution deﬁnition
of batch shape also has the interesting consequence that batch_shape may depend on sample_shape,
since i.i.d. samples of root variables will deﬁne batches of (non-identical) conditional distributions
over their children.

An alternative that can be easier to reason about is to deﬁne the batch shape of a joint distribu-
tion as a global property: not a structure, but a single shape describing the number of distinct joint
distributions represented. This removes dependence between batch shape and sample shape and
can simplify model implementation by eliminating the need to use Independent to align local batch
shapes across components. We have implemented these semantics as part of an experimental set of
AutoBatched joint distributions, which also incorporate automatic vectorization as deﬁned below.

4.2 Specifying vectorized models

A common feature of deep PPLs, including Pyro and Edward2 [Bingham et al. 2019; Tran et al.
2018], is that exploiting vectorization requires taking care to specify models that vectorize correctly.
Consider this innocent-looking JointDistributionCoroutine model:

z = yield tfd. Normal ( loc=0. , scale =[1. , 2., 3.])
x = yield tfd. Normal ( loc=0. , scale =1.)
y = yield tfd. Normal ( loc=z[:2] + x, scale =1.)

# z. shape ==[3]
# x. shape ==[]
# y. shape ==[2]

This is a valid probabilistic program; its execution produces a joint sample from p(z, x, y) with
shape ([3], [], [2]). However, this model speciﬁcation is not valid under vectorized execution. We
woud expect the shape of N joint samples to be ([N , 3], [N ], [N , 2]), but in fact, given vectorized
inputs the z[:2] on the last line will return a result of shape [2, 3] rather than the [N , 2]. Further-
more, neither of those shapes can be added to x having vectorized shape [N ] to produce a valid loc.
Also we should have annotated that z and x are root variables that must be sampled N times (as
discussed above), while the conditional distribution on the downstream y will already have batch
shape N and so needs to be sampled only once.

Joint Distributions for TensorFlow Probability

7

Joint distributions support two approaches to ensuring that models vectorize correctly. Under
manual vectorization, the user takes responsibility for specifying a model that is valid under vec-
torized execution. In this case they would annotate z and x as Root nodes (discussed above), and
apply more careful indexing hints in the last line: tfd.Normal(z[..., :2])+ x[..., None], 1.).

Since manual vectorization is often quite cumbersome, joint distributions also support auto-
matic vectorization in which the model is treated as a speciﬁcation of the sampling process for a
single possible world instead of being directly executed with vectorized inputs. This is automated
using tf.vectorized_map3 [Agarwal 2019] which lifts each tensor operation to one that correctly
preserves the batch dimension, allowing naïvely-written models like the above to execute correctly
in parallel. Although manual vectorization may still be preferred when models include operations
not supported by vectorized_map, or when custom broadcasting allows for additional performance
optimizations, we are excited that automatic vectorization can in many cases greatly provide a
more user-friendly approach to vectorizing probabilistic models.

5 LEARNABLE PARAMETERS

All TFP Distributions (including JointDistribution) are trivially "learnable." By "learnable" we mean
that the distribution is implicitly a function of some mutable data container (tf.Variable) which in
turn can be updated without necessitating reinstantiation of the Distribution object. For example,

loc = tf. Variable (0. , name= ' loc ' )
scale = tfp.util. TransformedVariable(2. , tfp. bijectors .Exp (), name= ' scale ' )
jd = tfd. JointDistributionSequential([

tfd. InverseGamma( concentration=3. , scale= scale),
tfd.Normal (loc=loc , scale =100.) ])

jd. trainable_variables
# ==> (<tf. Variable ' scale : 0 ' shape =() dtype= float32 , numpy =0.6931472 > ,
#
<tf. Variable ' loc : 0 ' shape =() dtype =float32 , numpy =0.0 >)
jd. log_prob ([1. , 0.])
# ==> <tf.Tensor : shape =(), dtype= float32 , numpy = −6.1378145 >
loc. assign (−7.)
scale .assign (0.25)
jd. trainable_variables
# ==> (<tf. Variable ' scale : 0 ' shape =() dtype= float32 , numpy = −1.3862944 > ,
#
<tf. Variable ' loc : 0 ' shape =() dtype =float32 , numpy =−7.0>)
jd. log_prob ([1. , 0.])
# ==> <tf.Tensor : shape =(), dtype= float32 , numpy = −10.62859 >

Notice that accessing jd.trainable_variables recurses through all dependencies to ﬁnd tf.Variable

s. The tfp.util.TransformedVariable serves as a constrained proxy of an otherwise unconstrained
tf.Variable and in this case ensures the scale is non-negative.

All Distributions make the contractual commitment to defer reading input arguments until
necessity demands. Among other beneﬁts, this enables declaring Distributions outside functions
which might otherwise update mutable data inputs. For example, maximum likelihood estimation
via gradient descent is simply, tfp.math.minimize(lambda: −tf.reduce_sum(jd.log_prob([p, m])),
num_steps, tf.optimizers.Adam(), jd.trainable_variables). For do-it-yourself tfp.math.minimize, see
Appendix A.

6 COMPOSITION OF JOINT DISTRIBUTIONS
Since JointDistributions are Distributions parameterized by Distributions, recursive composition
of JointDistributions (“nesting”) is automatic. For example,

3Inspired by vmap in JAX. [Bradbury et al. 2018]

8

Dan Piponi†, Dave Moore† & Joshua V. Dillon

def _inner ():

Root = tfd. JointDistributionCoroutine.Root
x0 = yield Root(tfd. Bernoulli (probs =0.25) )
x1 = yield Root(tfd. Normal (loc=0. , scale =1.))

jd = tfd. JointDistributionNamed(dict(

a=tfd. JointDistributionCoroutine( _inner ),
b=tfd. JointDistributionSequential((

tfd. Poisson ( rate =2.) ,
tfd. Gamma( concentration=2. , rate =1.))),

))
jd. sample ()
# ==> { ' b ' : (<tf.Tensor : shape =(), dtype= float32 , numpy =1.0 > ,
#
#
#

' a ' : (<tf.Tensor : shape =(), dtype= int32, numpy =1>,

<tf.Tensor : shape =(), dtype= float32 , numpy =0.89002216 >) ,

<tf.Tensor : shape =(), dtype= float32 , numpy = −0.55625236 >)}

In Python 3 it is also possible to nest JointDistributionCoroutine using yieldfrom.

7 RELATED WORK

Like existing deep PPLs such as Pyro [Bingham et al. 2019] and Edward2 [Tran et al. 2018], JointDistribution
s enable specifying probabilistic models within a hardware-accelerated diﬀerentiable program-
ming environment. Our focus is less on the particular speciﬁcation language; rather, JointDistribution
is a framework that uniﬁes multiple speciﬁcation ﬂavors under a shared backend interface, nat-
urally extending the Distributions API. Indeed, it would be feasible to build JointDistribution
ﬂavors that use Edward2 or even Pyro (up to TensorFlow-PyTorch translation) as the speciﬁcation
language.

Unlike Edward2, JD ﬂavors have diﬀerent sample/batch semantics.4 Our insight is to plumb
sample_shape only into the PGM roots and otherwise let shape propagate “organically” among de-
scendants. Additionally, some JD ﬂavors provide automatic batching capability.

JointDistributions also provide capability similar to eﬀect-handling in Edward2 and Pyro [Moore and Gorinova

2018; Phan et al. 2019]. Sampling from user deﬁned distributions is handled in the JointDistribution
base class where the result is interceptable.

Following its introduction in TFP, PyMC4 adopted a similar interface to JointDistributionCoroutine.

PyMC4 will be built on top of TensorFlow Probability [Kochurov et al. 2019].

Perhaps the most direct analogue to JointDistributions in modern PPLs are generative func-
tions in Gen [Cusumano-Towner et al. 2019]. Generative functions also support multiple model
speciﬁcation ﬂavors backed by a common interface to inference algorithms. Gen’s ‘choice maps’
and the value structures of joint distributions are roughly analogous. Unlike generative functions,
we have not augmented JointDistributions with any inherent concept of return values or built-in
proposal distributions, and do not currently support models with stochastic control ﬂow. Con-
versely, joint distributions support eﬃcient vectorized inference, which was not a design goal of
Gen. Exploring the connections between these abstractions may be an interesting direction for
future development.

8 ACKNOWLEDGEMENTS
We would like to acknowledge the contributions of Junpeng Lao, Alexey Radul, Brian Patton, Matt
Hoﬀman, Christopher Suter, Pavel Sountsov, and the TFP Team. We also thank Sharad Vikram and
Rif A. Saurous for their review of this work.

4We suspect Pyro also diﬀers from our sample shape propagation design but lacked time to verify this suspicion.

Joint Distributions for TensorFlow Probability

9

REFERENCES
Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M., et al. (2016).

Tensorﬂow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467.

Agarwal, A. (2019). Static automatic batching in tensorﬂow. In International Conference on Machine Learning, pages 92–101.
Bingham, E., Chen, J. P., Jankowiak, M., Obermeyer, F., Pradhan, N., Karaletsos, T., Singh, R., Szerlip, P., Horsfall, P., and
Goodman, N. D. (2019). Pyro: Deep universal probabilistic programming. The Journal of Machine Learning Research,
20(1):973–978.

Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., and Wanderman-Milne, S. (2018).

JAX:

composable transformations of Python+NumPy programs.

Burda, Y., Grosse, R., and Salakhutdinov, R. (2015). Importance weighted autoencoders. arXiv preprint arXiv:1509.00519.
Cusumano-Towner, M. F., Saad, F. A., Lew, A. K., and Mansinghka, V. K. (2019). Gen: a general-purpose probabilistic pro-
gramming system with programmable inference. In Proceedings of the 40th ACM SIGPLAN Conference on Programming
Language Design and Implementation, pages 221–236. ACM.

Dillon, J. V., Langmore, I., Tran, D., Brevdo, E., Vasudevan, S., Moore, D., Patton, B., Alemi, A., Hoﬀman, M., and Saurous,

R. A. (2017). Tensorﬂow distributions. arXiv preprint arXiv:1711.10604. https://arxiv.org/abs/1711.10604.

Hoﬀman, M. D. and Gelman, A. (2014). The no-u-turn sampler: adaptively setting path lengths in hamiltonian monte carlo.

Journal of Machine Learning Research, 15(1):1593–1623.

Hoﬀman, M. D. and Ma, Y. (2019). Langevin dynamics as nonparametric variational inference.
Kochurov, M., Carroll, C., Wiecki, T., and Lao, J. (2019). Pymc4: Exploiting coroutines for implementing a probabilistic

programming framework. NeurIPS Workshop on Program Transformations for Machine Learning.

Kucukelbir, A., Tran, D., Ranganath, R., Gelman, A., and Blei, D. M. (2017). Automatic diﬀerentiation variational inference.

The Journal of Machine Learning Research, 18(1):430–474.

Mnih, A. and Rezende, D. J. (2016). Variational inference for monte carlo objectives. arXiv preprint arXiv:1602.06725.
Mnih, A. and Salakhutdinov, R. R. (2008). Probabilistic matrix factorization. In Platt, J. C., Koller, D., Singer, Y., and Roweis,

S. T., editors, Advances in Neural Information Processing Systems 20, pages 1257–1264. Curran Associates, Inc.

Moore, D. and Gorinova, M. I. (2018). Eﬀect handling for composable program transformations in edward2. arXiv preprint

arXiv:1811.06150.

Neal, R. M. (2011). Mcmc using hamiltonian dynamics. In Handbook of Markov Chain Monte Carlo, pages 139–188. Chapman

and Hall/CRC.

Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. (2019).
In Advances in Neural Information Processing

Pytorch: An imperative style, high-performance deep learning library.
Systems, pages 8024–8035.

Phan, D., Pradhan, N., and Jankowiak, M. (2019). Composable eﬀects for ﬂexible and accelerated probabilistic programming

in numpyro. arXiv preprint arXiv:1912.11554.

Ranganath, R., Gerrish, S., and Blei, D. (2014). Black box variational inference. In Artiﬁcial Intelligence and Statistics, pages

814–822.

Tran, D., Hoﬀman, M. W., Moore, D., Suter, C., Vasudevan, S., and Radul, A. (2018). Simple, distributed, and accelerated

probabilistic programming. In Advances in Neural Information Processing Systems, pages 7598–7609.

Tucker, G., Lawson, D., Gu, S., and Maddison, C. J. (2018). Doubly reparameterized gradient estimators for monte carlo

objectives. arXiv preprint arXiv:1810.04152.

Van Rossum, G. and Eby, P. J. (2005). Pep 342–coroutines via enhanced generators. Python Enhancement Proposals.

https://www.python.org/dev/peps/pep-0342/ .

10

Dan Piponi†, Dave Moore† & Joshua V. Dillon

A DIY GRADIENT DESCENT
The convenience function tfp.math.minimize is a wrapper made simple by TF’s automatic diﬀeren-
tiation. Its (one-step) behavior is roughly equivalent to the following code.

def make_fit_op(loss_fn , optimizer , trainable_variables):

@tf.function
def fit_op ( ∗ args , ∗ ∗ kwargs ):

with tf.xla. experimental. jit_scope ( compile_ops=True):

with tf. GradientTape( watch_accessed_variables=False ) as tape:

tf.nest. map_structure(tape.watch , trainable_variables)
loss = loss_fn ( ∗ args , ∗ ∗ kwargs )

grads = tape.gradient (loss , trainable_variables)
optimizer . apply_gradients(list(zip(tf.nest. flatten ( grads),

tf.nest. flatten ( trainable_variables))))

return loss

return fit_op

Which, for example, can be used as:

def negloglik ():

x, y = next( train_iter)
return −tf.reduce_sum(model (x).log_prob (y))

fit_op = make_fit_op( negloglik , tf. optimizers.Adam (), model . trainable_variables)
loss = [ fit_op () for i in range( num_steps )]

Notice fit_op’s use of tf.function and tf.xla.experimental.jit_scope. The former “lifts” eagerly
executed code into graph mode wherein the latter makes numerous optimizations, including op
fusion and constant folding.

