1
2
0
2

y
a
M
5
1

]

V
C
.
s
c
[

1
v
3
1
1
7
0
.
5
0
1
2
:
v
i
X
r
a

A LARGE VISUAL, QUALITATIVE AND QUANTITATIVE DATASET
OF WEB PAGES

A PREPRINT

Christian Mejia-Escobar
Central University of Ecuador
P.O. Box 17-03-100
Quito, Ecuador
cimejia@uce.edu.ec

Miguel Cazorla
Institute for Computer Research
University of Alicante
P.O. Box 99. 03080, Spain
miguel.cazorla@ua.es

Ester Martinez-Martin
Institute for Computer Research
University of Alicante
P.O. Box 99. 03080, Spain
ester@gcloud.ua.es

May 18, 2021

ABSTRACT

The World Wide Web is not only one of the most important platforms of communication and infor-
mation at present, but also an area of growing interest for scientiﬁc research. This motivates a lot
of work and projects that require large amounts of data. However, there is no dataset that integrates
the parameters and visual appearance of Web pages, because its collection is a costly task in terms
of time and effort. With the support of various computer tools and programming scripts, we have
created a large dataset of 49,438 Web pages. It consists of visual, textual and numerical data types,
includes all countries worldwide, and considers a broad range of topics such as art, entertainment,
economy, business, education, government, news, media, science, and environment, covering differ-
ent cultural characteristics and varied design preferences. In this paper, we describe the process of
collecting, debugging and publishing the ﬁnal product, which is freely available. To demonstrate the
usefulness of our dataset, we expose a binary classiﬁcation model for detecting error Web pages, and
a multi-class Web subject-based categorization, both problems using convolutional neural networks.

Keywords Convolutional neural networks · dataset · webshots · deep learning · Web categorization

1

Introduction

Imagining today’s world without the Internet is difﬁcult since human activities such as commerce, education, entertain-
ment, social interaction, and many more have their digital version. It is the tool that has allowed much of these activities
to take place despite the paralysis caused by the recent pandemic. Internet is commonly associated with the Web, both
terms are intimately related but are very different concepts. The ﬁrst one refers to the big network of networks, i.e., the
infrastructure, whereas the second one refers to the content, formed by Web sites, which are a collection of Web pages
on a speciﬁc topic and linked to each other [1].

Since its invention in the 1990s, the World Wide Web, or just Web, has revolutionized access to large amounts of data
and information for individuals and organizations in all areas. Factors such as ease of use (user-friendly interface),
popularity, and increasing connectivity have made the Web a ﬁeld of interest for the business sector and scientiﬁc
research. Among the main domains of development are the following:

• Web design and redesign: identiﬁcation of metrics and guidelines to support the work of beginners and experts.
• Analysis of aesthetics and quality of Web pages.
• Optimization and improvement of the indexing of Web pages by search engines as Google and Bing [2].
• Categorization Web: classiﬁers and recommenders systems, Web directories and crawlers [3].
• Security: detection of illegitimate Web sites (phishing).
• Accessibility without limitations, regardless of knowledge, skills and technology.

 
 
 
 
 
 
A Large Visual, Qualitative and Quantitative Dataset of Web Pages

A PREPRINT

• Programming: automatic code generation.

• Machine Learning and Artiﬁcial Intelligence projects.

• Challenges of performance of algorithms and competitions of the best Web sites.

All of the above topics demand an essential resource: a large amount of data for analysis and testing tasks. Currently
there is no large Web-related dataset that includes both visual representation and attributes of Web pages. Therefore, we
make the following contributions:

• A free and available dataset of 49,438 Web pages from all countries worldwide and classiﬁed in the following
topics: arts and entertainment, business and economy, education, government, news and media, and science
and environment. The dataset combines different types of data: images, text, and numbers, which represent
the visual aspect of the full Web page (webshot), and qualitative and quantitative attributes, respectively.

• A workﬂow supported by programming languages and computer tools to automatize most of the process of
collecting, organizing, and debugging links, webshots, and parameter extraction. This methodology can be
adapted to other problems where the acquisition of a lot of data is needed.

• A deep convolutional neural network to detect error Web pages using only their screenshots. This allowed us
to debug our image dataset after the HTML code is no longer available. It can also be useful in other situations,
e.g. for the job of crawlers, indexers and search engines by skipping analysis of HTML content, or webmasters
could be notiﬁed of problems and hacking through periodic screenshots.

The rest of this paper is organized as follows: a review of similar works; a description of our dataset and the methodology
used for its creation, debugging, and publication; a statistical analysis of the Web page parameters; and a case study on
Web categorization to demonstrate the practical use of the dataset presented here.

2 Related work

The availability of a large amount of data is the current need for research and development in areas related to the Web.
We have generated an extensive dataset of Web pages, including features of various types: text, numbers, and images.
First, we reviewed the literature and analyzed the existing datasets, whose main properties are in Table 1.

For ease of comparison, we have divided the state-of-the-art datasets into two groups according to size: small (less than
1000 instances) and large. Other relevant properties are the topic, data types, its owner, purpose, and availability.

DeBoer et al. [1] use a tiny dataset, only visual and collected for categorization within four classes (news, hotels,
conferences, and celebrities). These categories are quite different from each other, so the categorization problem is of
less complexity. There is no link to download the screenshots.

Lopez et al. [7] and [9] have datasets with more Web pages, including their respective images and links (URLs).
However, these images are not screenshots but elements of the Web page. The URL is used to download the images
from HTML code and analyze them for categorization. Although there are more categories than in the previous work,
they are still very different topics. In both cases, no download link.

Reinecke et al. [6] is the most relevant dataset among the smaller ones. It could be a useful resource for small-scale
research and development works. It covers several countries in the world, various topics and is available for download.
However, it is strictly visual and insufﬁcient for current needs, and its purpose is more oriented to aesthetics analysis
and classiﬁcation.

The Computer Incident Response Center Luxembourg (CIRCL) is a government initiative created to respond to computer
security threats and incidents. CIRCL [10] offers a dataset of more than 400 screenshots of veriﬁed or potential phishing
Web sites. Also, an extensive dataset with more than 37000 images is available [11], corresponding to screenshots of
Web sites belonging to the Dark-Web, the problematic facet of the Web associated with cybercrime, hate, and extremism
[15]. Both datasets can be easily downloaded; however, because the images represent fakes or hidden Web pages would
have limited applications.

ImageNet [5], the most popular of the image databases, includes millions of images organized according to the WordNet
hierarchy1 (https://wordnet.princeton.edu). The Web sites section has 1840 screenshots from different countries
and languages without categorization. Some screenshots appear cropped, and download requires registration and
authorization.

1A large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets).

2

A Large Visual, Qualitative and Quantitative Dataset of Web Pages

A PREPRINT

Table 1: State-of-the-art datasets.

Owner & year

Size

De Boer
et al., 2011

Reinecke
et al., 2014

López
et al., 2017

Small:
60 screenshots

Small:
430 screenshots

Small:
280 Web pages

López
et al., 2019

Small:
365 Web pages

Topic
News, hotels,
conferences, and
celebrities

Generic

Food, animals,
fashion, nature,
home and vehicles
Food, vehicles,
animals, fashion,
home design and
landscape

Data type

Images
database

Images
database
URL and
images extracted
from HTML

Purpose
Aesthetics and thematic
classiﬁcation with
Machine Learning
Aesthetics
classiﬁcation
Thematic
classiﬁcation with
Machine Learning

URL and
images extracted
from HTML

Thematic
classiﬁcation with
Machine Learning

CIRCL, 2019

ImageNet, 2009

Small:
460 screenshots

Large:
1840 screenshots

Phishing

Generic

Nordhoff
et al., 2018

Large:
80901 screenshots

Generic

CIRCL, 2019

Large:
37500 screenshots

University of
Alicante, 2019

Large:
8950 labeled
screenshots

Onion Website
(Hidden Web,
no indexed)

Good and
bad design

Images
database

Images
database

URL,
metrics
and images

Images
database

Labeled
images
dataset

Analysis of
security events
Resource for
image and vision
research ﬁeld

Aesthetics and
Web design

Analysis of
security events

Aesthetics Web
categorization

The dataset created by the University of Alicante [12] collects 8950 screenshots of Web pages for analysis and evaluation
of the quality of Web design. Half of the images come from the Awwwards site (https://www.awwwards.com), so
they have been labeled with "good design", whereas the other half extracted from yellow pages, labeled with "bad
design". This dataset serves the academic work of the institution.

Nordhoff et al. [8] stands out because it has achieved a larger number of Web pages. However, they come from only 44
countries, the parameters are purely aesthetics, and the image download is not direct. In contrast, our dataset takes into
account all countries worldwide, includes parameters related to Web page structure, is general-purpose, and available
for download.

According to the above, it is not possible to take advantage of existing data sources, so our purpose is to create from
scratch an extensive and available dataset. It incorporates the visual representation of the Web page through a webshot,
complemented with qualitative and quantitative parameters extracted from the underlying HTML source code, so that a
Web page is better characterized. Because manual data collection is a complex task and requires too much time and
human effort, we have automatized most of the process by writing several programs in Python and R.

3 Description of the dataset

The dataset has been designed to combine visual, textual and numerical elements, which are presented in Table 2 and
detailed below.

3

A Large Visual, Qualitative and Quantitative Dataset of Web Pages

A PREPRINT

Element
Webshot
Name
URL
Country
Continent
Category
Time
Bytes
Images
Script_ﬁles
CSS_ﬁles
Tables
iframes
Style_tags
Img_bytes
Img_width
Img_height

Qualitative

Type
Visual
Text
Text

Table 2: Structure of the dataset.
Description
Web page entire screenshot in JPG format
Identiﬁcation given to the webshot
Link to locate and display a Web page
National origin of Web page
Region grouping countries
Main thematic of Web page
Web page’s source code download time
Size in bytes of Web page’s source code
Number of images from Web page
Number of executable ﬁles of Web page
Number of ﬁles to layout a Web page
Number of table tags in the source code
Number of iframe tags in the source code
Number of style tags in the source code
Webshot size in bytes
Webshot width in pixels
Webshot height in pixels

Quantitative

A webshot is a digital image of the entire Web page; unlike a screenshot, which may appear cropped because its
dimensions exceed the viewing device, forcing the user to scroll. The name given to the webshot is a key element that
follows a convention to identify the Web page’s source, category, and country. It is also the link between the image and
the qualitative and quantitative parameters. A URL (Uniform Resource Locator) is the Web page’s address together the
recovery mechanism (http / https). It is placed in the address bar of the browsers, which are the programs to display
the content to the user. We have collected URLs worldwide to cover different cultural characteristics and preferences,
so the dataset includes attributes related to the geographic location such as country and continent. The Web pages
collected belong to the following categories: Arts and Entertainment, Business and Economy, Education, Government,
News and Media, and Science and Environment. We have considered these categories since they are part of the Web
directory used here and explained in section 4.1.2.

We have added the following quantitative parameters, which give us an overview of the structure and quality of a Web
page:

• Download time: users want to wait as little as possible to view a Web page [13], which means reducing the

source code download time.

• Size: the larger the size in bytes, the slower the download and display of the Web page.

• Images: the images will increase the download time. A Web page is not more attractive because it has more

images, a balance between all types of information is recommendable.

• Scripts: they are external ﬁles to get more complex functionality to the Web page. It is convenient to reduce

their quantity because they increase the network trafﬁc and download time.

• CSS ﬁles: style ﬁles cause an extra load and delay the display of the Web page, ideally there should be one.

• Tables: are often used to structure the Web page’s content; however, it is discouraged due to appropriate

elements such as "div" tags.

• iFrames: insert a Web page inside another one, which is not a good practice currently.

• Style tags: are not recommended since there are CSS ﬁles.

Finally, we have the image size in bytes, as well as the dimensions (width and height) in pixels of each webshot. Fig. 1
presents a small sample of the dataset showing a case of each category.

4

A Large Visual, Qualitative and Quantitative Dataset of Web Pages

A PREPRINT

Figure 1: A small sample of the dataset, including one example of each category.

4 Methodology

The workﬂow developed to produce the dataset is represented in Fig. 2. Following, we describe each stage included in
the methodology, supported by various techniques, procedures, and computer tools.

Figure 2: Methodology created to obtain the Web pages dataset.

4.1 Data Acquisition

The ﬁrst step is to collect URLs worldwide related to the given categories. Then, each of the URLs will allow us to
download the HTML code, extract quantitative and qualitative parameters by scraping, and take a screenshot of the
entire Web page (webshot).

In order to obtain a larger number of URLs, we have used two ways of ﬁnding information on the Web: "Searching"
and "Browsing". Searching requires the user to translate a need for information into queries, whereas Browsing is a
basic and natural human activity, occurring in an information environment where information objects are visible and

5

A Large Visual, Qualitative and Quantitative Dataset of Web Pages

A PREPRINT

organized [14]. Below, we describe in detail how both techniques allowed us to collect URLs, and based on them,
extract parameters and capture webshots, all through various Python and R scripts.

4.1.1 URL collection by Searching

The Google search engine asks for words or phrases related to the topic of interest. To avoid the repetitive task of typing
the query into the Google search page and manually retrieving the response URLs, we have automatized the process
through a Python script2, where:

1. The country name and its Internet code are extracted iteratively from a plain text ﬁle3.

2. The search query has the following structure:

’site:’ + countrycode + ’ business OR economy OR marketing OR computers OR internet OR construction OR ﬁnancial OR industry OR shopping OR

restaurant’ + ’ ext:html’

OR, site and ext are operators or reserved words that can be used in query phrases within the Google search
engine. The "OR" operator concatenates several search words related to the category. The "site" operator
speciﬁes the geographic top-level Internet domain assigned for each country, e.g. ".es" for Spain. The
"ext:html" operator allows us to obtain results exclusively with this extension.

3. The request returns the Google results page with the ﬁrst 100 links. We have deﬁned it like this to achieve an

approximately uniform distribution of Web pages according to country and category.

4. Web page links are extracted by automatically scanning the source code of the results page (scraping),

generating a text ﬁle that contains the URLs and their attributes: country, continent and category.

For the rest of the categories, the query phrases are:

’site:’ + countrycode + ’ arts OR entertainment OR dance OR museums OR theatre OR literature OR artists OR galleries’ + ’ ext:html’

’site:’ + countrycode + ’ education OR academy OR university OR college OR school’ + ’ ext:html’

’site:’ + countrycode + ’ government OR military OR presidency ’ + ’ ext:html’

’site:’ + countrycode + ’ news OR media OR magazine OR radio OR television OR newspaper ’ + ’ ext:html’

’site:’ + countrycode + ’ science OR environment OR archaeology’ + ’ ext:html’

4.1.2 URL collection by Browsing

This technique uses a Web Directory, a specialized Web site consisting of a catalog of links to other Web sites. Build,
maintain, and organization by categories and subcategories is done by human experts, unlike search engines that
do it automatically. To include a URL, specialists perform a review, analysis, and evaluation process to verify the
requirements determined by the Web Directory.

Today a few Web Directories have survived the popularity of search engines like Google. We can highlight Best of
the Web (BOTW) (Fig. 3), one of the most recognized by its quality, global reach, a wide range of categories and
subcategories, level of trafﬁc (visits per month), reliability, the number of links, and demanding requirements.

2https://osf.io/k6yrx
3https://osf.io/yrmx8

6

A Large Visual, Qualitative and Quantitative Dataset of Web Pages

A PREPRINT

Figure 3: BOTW Web Directory (https://botw.org/)

Instead of a query or search phrase, it is necessary to know the hierarchical structure of the directories and subdirectories
until the URL of interest. We have taken advantage of the organization by countries and categories deﬁned by BOTW
Web Directory, e.g. for Greece:

https://botw.org/top/Regional/Europe/Greece/Arts-and-Entertainment/
https://botw.org/top/Regional/Europe/Greece/Business-and-Economy/
https://botw.org/top/Regional/Europe/Greece/Education/
https://botw.org/top/Regional/Europe/Greece/Government/
https://botw.org/top/Regional/Europe/Greece/News-and-Media/
https://botw.org/top/Regional/Europe/Greece/Science-and-Environment/

We collect the URLs published within each category through a Python script4 that:

1. Reads iteratively the name of each country from a ﬂat text ﬁle5.
2. Sets the path corresponding to the category, which will always have the same structure, only the country’s

name changes:
’https://botw.org/top/Regional/’ + countryname + ’/Science_and_Environment/’

3. A connection to the formed Web address is realized, which obtains the source code of the results page to

extract the URLs of each category.

4. The links are stored in a text ﬁle6 that can be opened in a spreadsheet where a ﬁlter is applied to select only

those links belonging to a country in particular.

4.1.3 Parameters collection by Scraping

After collecting and storing the URLs from the two sources described above, we implemented a Python script7 to: a)
sequentially read the URL links stored in the text ﬁle8; b) make a connection via browser to each of these links; and c)
download and analyze the source code of the Web page, obtaining the parameters speciﬁed in the dataset: download
time in seconds, total size in bytes, number of images, script ﬁles, CSS ﬁles, tables, iFrames tags, and style tags. This
script includes the library for Web scraping named Beautiful Soup, which allows us to deﬁne and extract the mentioned
parameters from the HTML code of each Web page.

4.1.4 Webshots collection

We take advantage of Webshot and PhantomJS packages to create a R script9 that: reads each URL from text ﬁles
generated in sections 4.1.1 and 4.1.2, takes a snapshot of the entire Web page, and saves it as JPG image.

4https://osf.io/73sc2
5https://osf.io/ve986
6https://osf.io/hjwgm
7https://osf.io/de78f
8https://osf.io/gk3p2
9https://osf.io/6pmyb

7

A Large Visual, Qualitative and Quantitative Dataset of Web Pages

A PREPRINT

The name assigned to the image is an unique identiﬁer that allows us to know the source, category, and country to
which the Web page belongs, e.g. B2Netherlands_791.jpg indicates a screenshot of a Web page obtained through
the technique of Browsing, belonging to category number two (Business and Economy) and that it comes from the
Netherlands. The number after the underscore only establishes a sequential order. Note that such identiﬁer is the key to
attach the webshots to their respective qualitative and quantitative parameters. In this way, it was possible to link two
different storage media, i.e. a parameters datasheet and an images folder.

Additionally, this script includes functions to obtain both webshot size in bytes and webshot dimensions (width and
height) in pixels, so we get a better characterization of the Web page. All images are available for viewing and
downloading10.

4.2 Dataset debugging

During the automatic data collection, some events did not allow downloading the HTML code or capturing the webshot,
caused by: request for manual acceptance of cookies and SSL certiﬁcates, error messages as HTTP 403 Forbidden,
HTTP 404 Not Found, HTTP 406 Not Acceptable, HTTP 909 Denied permission, and exceed timeout.

We used exception handling inside the scripts to avoid interruptions during the execution of the programs. When an
error occurred, the ﬁelds associated with the parameters or webshot were assigned the value "-1". Thus, the programs
could continue their execution, and the inexistence of webshots or parameters was solved.

For the ﬁnal dataset, we have considered only URLs that have their respective webshot, as this is the most relevant
element of our work. However, after a brief visual review of the webshots in the dataset, several error Web pages were
detected, e.g. Web sites under construction, maintenance, domain offer, suspended account, page not found, browser
incompatibility, virus or phishing risks. Some of them shown in Fig. 4.

Figure 4: A sample of the error Web pages.

The webshots above are not useful for the dataset, so we decided to select and remove these images that appeared in
the automatic collection. Although the size of the ﬁnal dataset will be smaller, we will obtain a cleaner dataset. Given
that the URLs connections corresponding to these webshots did not return HTTP 403 or HTTP 404 error messages,
nor did the HTML code contain phrases like "suspended account" or "page under construction", text analysis was not
possible. We need implement an image analyzer to avoid manual and visual veriﬁcation of thousands of webshots,
which consumes too much time and effort. We used a Convolutional Neural Network (CNN), the state-of-the-art tool in
computer vision, to detect error Web pages, and then separate them into a dedicated folder, all automatically.

Here we present an automatic detection of error Web pages based exclusively on their webshots. It consists of
determining if a Web page belongs to a "VALID" category or, by the contrary, to an "ERROR" category, i.e. a binary
classiﬁcation problem. To tackle it, we followed the methodology shown in Fig. 5.

10https://osf.io/7ghd2/?view_only=0bf99589809e4e88b0aa0602c8060b46

8

A Large Visual, Qualitative and Quantitative Dataset of Web Pages

A PREPRINT

Figure 5: Methodology for detecting error Web pages.

4.2.1 Data selection

The main resource for automatic learning is data. In our case, images that will be the input for a training process
that tries, iteratively, to obtain a known output ("valid" or "error"), and if an acceptable accuracy is reached, to make
predictions.

The training process requires images associated with their respective category: valid or error. Since our dataset consists
of two groups of images (Browsing and Searching), we selected the webshots of the smallest subset (Browsing) to
perform an exhaustive visual inspection and classify the images manually, obtaining the results shown in Table 3. Once
the neural network model is adjusted, it will classify each webshot in the largest subset (Searching) as valid or error.

Table 3: Dataset for binary classiﬁcation (Browsing webshots).

Category
Arts & Entertainment
Business & Economy
Education
Government
News & Media
Science & Environment
Total

Browsing
Webshots Valid Error

447
1058
419
730
458
497
3609

397
892
368
669
394
462
3182

50
166
51
61
64
35
427

The dataset for training an error Web pages detection model has a total of 3609 images, 427 error webshots and 3182
valid webshots, which have been uploaded to Google Drive in separated folders, called "VALID" and "ERROR" (Fig. 6).

9

A Large Visual, Qualitative and Quantitative Dataset of Web Pages

A PREPRINT

Figure 6: Directory structure of the dataset.

We take advantage of Google Colaboratory, a free platform that offers powerful hardware and requires no installation
or setup, supports Python through an online notebook, and includes the packages and libraries to facilitate automatic
learning such as Tensorﬂow, Keras, Sklearn, and others. Next, a description of the developed code11.

The initial step is the connection to the data source where the images of our dataset have been stored within folders
and subfolders, named as the categories (Fig. 6). Such denomination facilitates the labeling of the images with their
corresponding category. A pair of instructions are needed to access Google Drive:

from google.colab import drive
drive.mount(’/content/drive’)

4.2.2 Split data

One of the tasks that characterizes automatic learning is the division of the data. Because we have only 3609 images, we
consider two subsets: training and validation (Table 4). The training subset contains the largest amount of images (80%)
and is used for learning and ﬁtting the model’s parameters, whereas the validation subset (20%) is used to evaluate the
capacity of the model, which will serve to adjust its hyperparameters.

Table 4: Dataset split for training and validation.
Subset Webshots Valid Error Percentage
2545
Training
637
Validation
Total
3182

80%
20%
100%

2886
723
3609

341
86
427

Although the most convenient is a balanced dataset, that is, an equal number of error cases and valid cases, we used all
the images in order to obtain a better generalization. To automatically divide into training and validation folders, is
useful to install and import the split-folders12 package. It is necessary to specify the images directory, output directory,
and the proportion to split (80% and 20%, respectively).

splitfolders.ratio(’/content/drive/My Drive/DATASET’,

output=’/content/drive/My Drive/SPLIT’, seed=1337,
ratio=(.8,.2), group_prefix=None)

The result is a new directory structure. Within the "SPLIT" folder, the "train" and "val" folders are created, and within
each of these, the "ERROR" and "VALID" folders.

4.2.3 Data pre-processing

The images need to be prepared before modeling. First, we normalize the pixel values (integers between 0 and 255)
to a scale between 0 and 1, using an image processing utility. The ImageDataGenerator class from Keras framework
divides all pixels values by the maximum pixel value (255).

train_datagen = ImageDataGenerator(rescale=1./255)

Second, the images have different dimensions (width and height), so they are all resized to 256x256 pixels by setting
the target_size parameter of the ﬂow_from_directory method. This operation is performed in groups of 32 images
(batch_size) that are labeled for binary classiﬁcation (class_mode) according to the folder where they are stored (valid
and error) within of training directory.

11https://colab.research.google.com/drive/1mzmm0C-WDNopOlEtRm7VVA6_8eHuYoVH?usp=sharing
12https://pypi.org/project/split-folders/

10

A Large Visual, Qualitative and Quantitative Dataset of Web Pages

A PREPRINT

train_generator = train_datagen.flow_from_directory(

train_data_dir,
target_size=(256,256),
batch_size=32,
shuffle=False,
class_mode=’binary’)

In this way, the small values of both pixels and dimensions help speed up the training process. The above code applies
to the validation data, only the directory changes.

4.2.4 Create model

The model’s architecture is based on the convolutional neural network proposed by Liu et al. [16] to detect malicious
Web sites. Since this is a similar problem, we just applied minor adaptations. The model’s structure (Fig. 7) is composed
of two parts:

Figure 7: CNN architecture.

• Convolutional basis: for automatic extraction of features. The input image is resized to 256x256 pixels and
separated into 3 RGB color channels (Red, Green, Blue). Then, it is processed by 3 convolutional layers with
their respective activation functions (ReLU, Rectiﬁed Linear Unit) and max-pooling layers. The ﬁrst two
convolutions use 32 ﬁlters (kernels) while the third convolution has 64, with a size of 3x3, in contrast to the
pool size is 2x2.

• Binary classiﬁer: features are received in a ﬂattened form by a fully connected layer that applies dropout to
avoid overﬁtting. The sigmoid function generates the prediction as a probability value between 0 and 1. If the
value is greater than 0.5, the Web page is valid, otherwise, an error Web page.

Keras provides functions to implement this model from scratch in a simple way, just adding in sequence the convolutional,
activation, pooling, dropout, ﬂatten, and dense layers, and specifying their respective parameters. For example, the ﬁrst
convolutional block has the following instructions:

model = Sequential()
model.add(Conv2D(32, (3, 3), input_shape=(256, 256, 3)))
model.add(Activation(’relu’))
model.add(MaxPooling2D(pool_size=(2, 2)))

Meanwhile, the classiﬁer part:

model.add(Flatten())
model.add(Dense(64))
model.add(Activation(’relu’))
model.add(Dropout(0.5))
model.add(Dense(1))
model.add(Activation(’sigmoid’))

Finally, the model.summary() instruction allows us to verify the structure in detail (Fig. 8).

11

A Large Visual, Qualitative and Quantitative Dataset of Web Pages

A PREPRINT

Figure 8: Model’s layers summary.

4.2.5 Train and ﬁt model

Before starting the training, we must explicitly deﬁne the hyperparameters required by the neural network for binary
classiﬁcation.

model.compile(loss=’binary_crossentropy’,
optimizer=’rmsprop’, metrics=[’acc’])

Through Keras, we can establish the loss function that will be minimized by the optimization algorithm and the
classiﬁcation accuracy as the metric that will be collected and reported by the model.

history = model.fit(
train_generator,
steps_per_epoch=train_samples // batch_size,
epochs=epochs,
validation_data=val_generator,
validation_steps=validation_samples // batch_size,
callbacks=[checkpoint])

After a few hours of computation, 20 iterations (epochs) of the training dataset (2886 images) have been executed; each
iteration consists of 90 groups (steps_per_epoch) of 32 images (batch_size). The accuracy achieved is 96.6% (iteration
20), while in the validation stage the accuracy is 97.16% (iteration 16). The evolution of the process is summarized in
the graphs of the training and validation curves (Fig. 9).

12

A Large Visual, Qualitative and Quantitative Dataset of Web Pages

A PREPRINT

Figure 9: Accuracy and loss in training and validation phases.

The training and validation phases have reached a high level of accuracy, both progressing to the same level, which is
desirable. The model ﬁts very well with the images provided, but how it will behave with new images (generalization).
This concern is answered by analyzing the difference between training and validation losses. The last one, despite
oscillating, does not separate so much from the other one until iteration 17, after which, they start to separate, with the
possibility of overﬁtting. Therefore, the model is saved with the accuracy and parameters of iteration number 16. We
can say that the model is capable of acceptably distinguishing error Web pages and valid Web pages and thereby move
to the prediction phase.

4.2.6 Predictions

The images from the largest set (Searching) of our dataset become the input of the already trained and validated model.
We used google.colab library to select and upload from local drive the ﬁle (webshot) with click on “Choose Files”
button.

from google.colab import files
uploaded = files.upload()

Once the ﬁle to be 100% uploaded, it will be pre-processed using keras.preprocessing and NumPy libraries to transform
the image into an array with a suitable shape and normalized pixel values for the model, which makes the prediction.

img = image.load_img(path, target_size=(256, 256))
x = image.img_to_array(img)/255.
x = np.expand_dims(x, axis=0)
images = np.vstack([x])
classes = model.predict(images)

Fig. 10 shows the result for two images selected one at a time. The resized webshot is displayed and the prediction is a
probability value between 0 and 1, less than 0.5, so the category assigned is ERROR (left side), and a case of a valid
Web page, with a probability value very close to 1 (right side).

13

A Large Visual, Qualitative and Quantitative Dataset of Web Pages

A PREPRINT

Figure 10: Prediction of error and valid Web page.

In addition to making predictions one by one, more important for our purpose is to generate predictions for groups of
images. We just select a list of ﬁles using the choose button. As our dataset is organized by topic categories, we can
select all images in one category, e.g. "Arts and Entertainment". An extract of the results is shown in Fig. 11.

Figure 11: Prediction for a group of Web pages.

This list of predictions is passed to a spreadsheet, then by means of a ﬁlter, the Web pages of the error category are
selected and saved as a text format ﬁle (list.txt). This ﬁle is the input to execute a command that moves all images from
their original folder to the "ERROR" folder. This command-line runs in Windows (PowerShell interface), although it is
easily adaptable to different operating systems such as Linux.

cat list.txt | ForEach {mv $_ ERROR}

As result of the prediction, 822 error Web pages y 7747 valid Web pages. Once the images were classiﬁed and separated,
the visual veriﬁcation was much faster, and we were able to manually establish the successes and fails of the classiﬁer.
Thus we identiﬁed 1214 real error Web pages and 7355 real valid Web pages. The same procedure is carried out for the
remaining images in the other categories. The results are summarized in Table 5.

Table 5: Results of the binary Web categorization.

Category

Webshots

Valid
(Prediction)

Searching
Error
(Prediction)

Arts &
Entertainment
Business &
Economy
Education

Government

News &
Media
Science &
Environment
Total

8569

8699

8742

8088

7747

8004

8083

7363

11574

10597

8893

54565

8137

49931

Valid
(Real)

Error
(Real)

Accuracy

7355

1214

94.68%

7546

7524

1153

1218

93.79%

92.80%

6685

1403

90.85%

9650

1924

90.80%

822

695

659

725

977

756

4634

7496

46256

1397

8309

92.34%

92.47%

14

A Large Visual, Qualitative and Quantitative Dataset of Web Pages

A PREPRINT

We used the Confusion Matrix to evaluate and know how accurate our model is. This table enables us to compare reality
and prediction, and based on the successes and fails, calculates an accuracy value. E.g. for "Arts and Entertainment"
category, the classiﬁer predicted 822 error Web pages, however, it failed in 32 instances. There were 7747 predictions
of valid Web page, but 424 were incorrect. These values are placed in Table 6 and substituted into the formula for
accuracy.

Table 6: Confusion Matrix for the "Arts and Entertainment" category.

Prediction
Error Valid
424
790
7323
32

Real

Error
Valid

Accuracy =

790+7323

790+32+424+7393 = 0.9468 × 100% = 94.68%

The classiﬁer reached an accuracy of 94.68% for this category, which is good considering the small number of images
that were part of the training process. Table 7 shows the respective confusion matrix for each category and a total matrix
indicating an accuracy of 92.47% for the entire Searching dataset.

Table 7: Confusion matrix for the rest of categories and overall result.

Business and Economy

Education

Prediction
Error Valid
499
654
7505
41

Real

Error
Valid

Government

Prediction
Error Valid
709
694
6654
31

Real

Error
Valid

Science and Environment

Prediction
Error Valid
661
736
7476
20

Real

Error
Valid

Prediction
Error Valid
594
624
7489
35

Real

Error
Valid

News and Media

Prediction
Error Valid
1006
918
9591
59

Real

Error
Valid

Overall

Prediction
Error Valid
3893
4416
46038
218

Real

Error
Valid

After running the automatic error Web pages detection, the debugging is complete. The composition and ﬁnal size of
our dataset is shown in Table 8. Combining Browsing and Searching techniques, we have achieved to collect 49438
valid Web pages that occupy approx. 17 GB.

Table 8: Composition and size of the ﬁnal dataset.

Category

Arts & Entertainment
Business & Economy
Education
Government
News & Media
Science & Environment
Total

Browsing
Webshots
397 (147 MB)
892 (300 MB)
368 (126 MB)
669 (253 MB)
394 (237 MB)
462 (193 MB)
3182 (1.22 GB)

Searching
Webshots
7355 (2.58 GB)
7546 (2.48 GB)
7524 (2.64 GB)
6685 (2.47 GB)
9650 (3.19 GB)
7496 (2.63 GB)
46256 (15.99 GB)

Total

7752
8438
7892
7354
10044
7958
49438

15

A Large Visual, Qualitative and Quantitative Dataset of Web Pages

A PREPRINT

4.3 Publication of the dataset

All products generated in this work are public and available through OSF13 (Open Science Foundation), a free and open
platform to support, disseminate and enable collaboration of scientiﬁc research. The OSF Web site offers a user-friendly
interface to manage everything related to the project, as shown in Fig. 12.

Figure 12: Web Pages Dataset Project in OSF.

The hierarchical organization of the project consists of: a) the dataset divided in the visual part (images in categories),
and the textual and numerical part (datasheets); b) the code developed; and c) support documentation. The project is
public14 and their resources can be selected and viewed within the same page and downloaded through the button on
the top bar.

5 Statistical analysis

Both sources of the Web pages: Browsing and Searching, are compared through a statistical study supported by the
R programming language. Before, we should note that the calculation of statistical indicators and creation of graphs
excluded outliers due to the high heterogeneity of the variables included in Table 2. These values are far from those
considered common and may cause distortions in mathematical and visual analysis. By using the well-known rule "1.5
times the Interquartile Range", outliers can be identiﬁed and omitted. For this reason, the number of values of each of
the variables may differ.

5.1 Qualitative parameters

Although we tried to obtain an uniformly distributed set of URLs with respect to the categories, the errors cited in
the debugging section, caused the results shown in Fig. 13a. In Searching, there is less imbalance, in contrast to
Browsing, where Web pages related to business, economy and government predominate, possibly due to a greater need
for dissemination and economic capacity to register their Web pages in a paid service.

13https://osf.io/
14https://osf.io/7ghd2/

16

A Large Visual, Qualitative and Quantitative Dataset of Web Pages

A PREPRINT

(a)

(b)

Figure 13: Distribution of the qualitative parameters about Web pages: a) category and b) continent.

The geographical location of the Web pages is mostly in Europe and Asia, both for Browsing and Searching. Such
continents agglomerate a larger number of countries. Moreover, the economic potential could explain why they are at
the top. (Fig. 13b).

5.2 Quantitative parameters

The variability of Searching is more evident for the number of characters in a URL (Fig. 14a), since a wider range
(the difference between the minimum and maximum) and a higher mean and standard deviation. In both graphs, the
values accumulate more at the bottom of the variable and are less frequent at the top, so there is a tail to the right. This
behavior is desirable when reading or typing a URL in a browser, and is an indicator that there are not too many levels
to reach a particular Web page.

The download time of the source code of the Web pages shown in Fig. 14b has a similar behavior for Browsing and
Searching. In both cases, although there is considerable variability due to the width of the range and standard deviation,
the values are around 20 milliseconds and mostly low, which is a beneﬁt for the user who wants to view the Web page
in the shortest time possible. According to the errors cited in the debugging section, 254 of 3182 URLs belonging
to Browsing were not available, i.e. 7.98%; while in Searching, 4079 of 46256, i.e. 8.82%, were not accessible to
download the source code, and hence the extraction of the quantitative parameters was not possible.

The behavior of the size in bytes (Fig. 14c) is almost identical between Browsing and Searching, where the average of
the Web pages is approximately 50 KB. Both the variability and the tails of the distributions are practically the same,
being favorable for a quick view of the Web page, which is in direct relation to a small size. Although there are still
Web pages with a considerable size, which may be due to graphic elements or external objects linked to page.

In Fig. 14d, close to 60% of Web pages do not include images within their source code. It seems that the pages present
textual information exclusively. However, they might include images through CSS style ﬁles, which is a good practice
[13]. Although the range of number of images is wide, the average is low, 2 or 3 images per Web page, so there is a
tendency to use a few images within a Web page in order to make it lighter.

Non-scripted Web pages exceed 50% in both cases. Scripts are add-on programs that provide additional functions to
Web pages. However, their use may cause incompatibilities with browsers and make the page more complex and heavy.
In Fig. 14e, the trend is to minimize the presence of scripts, 3 scripts per Web page on average.

Fig. 14f shows that approximately 50% of the Web pages do not use Cascading Style Sheets (CSS) ﬁles, whose use is
recommended as a good practice in Web design. The ideal amount would be one style ﬁle per Web page. The average
for Browsing and Searching is 5 and 4, respectively, not too far away. The tendency is towards low values, but there are
some cases with many style ﬁles, being prejudicial to the agile display of the Web page.

The graphs in Fig. 14g have a very similar aspect. The majority of Web pages (about 85%) no longer use tables within
the source code. Tables were generally used to structure the content; however, this practice has been replaced with the
"div" tag, achieving a more elegant and professional design.

Over 85% of Web pages do not use iFrames. For Browsing and Searching, the bars in the graph are grouped on the
left (Fig. 14h). We can deduce that the embedding of another document in the current HTML document through the
"iFrame" tag is disappearing, as there are now better options.

More than half of the Web pages (close to 60%) no longer use "style" tags in their source code. Both graphs in Fig. 14i
have bars that decrease towards the right. The trend is to minimize the number of such tags, as it is more appropriate to
use CSS ﬁles. Thus, the source code of a Web page does not extend too much.

17

A Large Visual, Qualitative and Quantitative Dataset of Web Pages

A PREPRINT

(a)

(c)

(e)

(g)

(i)

(k)

(b)

(d)

(f)

(h)

(j)

(l)

Figure 14: Distribution of the quantitative parameters about Web pages

18

A Large Visual, Qualitative and Quantitative Dataset of Web Pages

A PREPRINT

The webshot size is decisive in determining how much space our dataset will consume on a storage device. In Fig. 14j,
the size ﬂuctuates over a wide range of values, with an average of about 300 KB. Most of the values are concentrated in
low sizes, but with a considerable presence of images with medium and high size. This behavior would require not only
a good amount of space but a pre-processing of the images for Machine Learning and Deep Learning applications.

The Searching and Browsing graphs are quite similar for the webshot width (Fig. 14k). In both cases, the ﬁrst bar,
where the minimum is, signiﬁcantly predominates as the default screenshot sets a width of 992 pixels. The average
value is very close to the minimum since almost all images were captured with this default value (about 85%); however,
there are also images with a wider width, especially in Searching with a maximum of almost 10000 pixels.

In the case of the height variable (Fig. 14l), the minimum value prevails too, although to a lesser degree (about 28%),
which coincides with the default value set by the screenshot, i.e. 744 pixels. Unlike the previous case, there is a less
unbalanced distribution of values, with a wider variability in which the highest accumulation occurs up to 5000 pixels, a
considerable accumulation between 5000 and 10000 pixels, and ﬁnally, images with a height of up to almost 50000
pixels have been obtained. Considering the width and height, most of the Web pages have a vertical layout. These
parameters are closely related to the resolution or quality of the image. The more pixels, the more resolution and quality
the image has, but it demands more storage space.

Table 9: Summary of statistical indicators for quantitative parameters.

Parameter

URL length
Time (ms)
Size (KB)
Images
Scripts
CSS ﬁles
Tables
iFrames
Style tags
Size (KB)
Width (px)
Height (px)

Browsing
Min. Max. Mean
31.73
161
18.72
72.9
50.49
200.89
2.05
15
3.35
20
5.38
30
0.25
15
0.16
14
1.25
15
302.72
954.23
1058.8
6814
3859
49658

14
1.25
0
0
0
0
0
0
0
15.75
992
744

Searching
Std. Dev. Min. Max. Mean
69.84
22.33
46.82
2.51
3.04
4.19
0.39
0.18
0.8
273.92
1052.3
3560

12.59
17.39
44.33
3.48
5.1
8.07
1.05
0.62
2.37
200.18
388.7
4293

200
73.3
176.87
17
17
22
15
15
15
846.77
9954
49894

21
1.9
0
0
0
0
0
0
0
13.59
992
744

Std. Dev.
30.99
15.83
39.58
4.14
4.43
5.77
1.45
0.6
1.6
180.25
379.6
3918

Finally, Table 9 summarizes the main statistical indicators for the quantitative parameters of the Web pages, both for
Browsing and Searching set.

6 Case study: Multi-class categorization of Web pages

The Web is a global communication platform where the volume of information available is enormous, grows rapidly, and
covers any topic. Search and recovery service providers are constantly working on improving mechanisms to manage
this information effectively. Classiﬁcation is a basic technique to deal with this problem. Since doing it manually is not
practical, the automatic classiﬁcation of Web pages is the recommended method and has motivated several research and
development works.

The classiﬁcation of Web pages, also called Web categorization, determines whether a Web page or Web site belongs
to a category in particular. For example, judging whether a page is about "arts", "business", or "sports" is an instance
of subject classiﬁcation [4]. This is usually done by analyzing both the textual content and underlying HTML code.
However, the visual appearance is also an important part of a Web page, and many topics have a distinctive visual
appearance, e.g., Web design blogs have a highly designed visual appearance, whereas newspaper sites will have a lot
of text and images [1].

Here, we present an automatic categorization of Web pages according to topic or subject and based exclusively on their
visual appearance. We take advantage of the dataset generated in this work, formed by images (webshots) belonging to
6 categories: arts and entertainment, business and economy, education, government, news and media, and science and
environment. Therefore, the problem becomes a multi-class categorization.

We implemented a model of Deep Learning with a Convolutional Neural Network (CNN). In essence, a learning process
with the webshots collected in order to achieve an acceptable accuracy and then make predictions. We hope to capture
features (difﬁcult to identify manually) that can distinguish categories, predict to which of them a Web page would

19

A Large Visual, Qualitative and Quantitative Dataset of Web Pages

A PREPRINT

belong, analyze the difﬁculty of the topic classiﬁcation of the Web pages and verify if there are particular patterns for
each category.

It is important to highlight that the following results have been selected from a series of various experiments, where
different models and architectures were tested using the whole dataset and parts of it. The developed code, as well as
the weights of the adjusted deep learning model, are publicly accessible15.

The best results were obtained with the Transfer Learning technique and the images of the Browsing dataset. This
may be since in a Web directory such as BOTW, the Web pages go through a rigorous registration process under the
supervision of human specialists, so the Web pages have a better distinction and categorization.

The dataset for the multi-class categorization is constituted only by the webshots of the set of Browsing, which has been
organized and split according to Table 10. For the training process, we have a balanced amount of data, i.e. the same
number of images for each category, to avoid possible preferences. The category with fewer images (Education) was the
basis for randomly selecting the same number of images in the other categories. One image of 3.68 MB and resolution
of 992x30154 was discarded because the Python imaging library does not open larger images to avoid malicious attacks.
So, each category has 367 images, a total of 2202 images, 80% for training, whereas 20% remaining is destined for
validation (both sets randomly selected).

Table 10: Dataset split for multi-class categorization.

Category
Arts & Entertainment
Business & Economy
Education
Government
News & Media
Science & Environment
TOTAL

Webshots Dataset Train (80%) Val. (20%)

397
892
368
669
394
462
3182

367
367
367
367
367
367
2202

293
293
293
293
293
293
1758

74
74
74
74
74
74
444

The images are stored within the directory structure displayed in Fig. 15. Within the main folder of the dataset the
division in training and validation, and the sub-folders represent the categories, which have been named as the topics
considered in this work.

Figure 15: Directory structure by categories for training and validation.

After organizing the images, a pre-processing step is convenient to normalize the image’s pixel values (integers between
0 and 255) to the scale of values between 0 and 1; and resize to 224x224 pixels recommended for the model, because
the images in the dataset have different dimensions (width and height). Both are a common practice that helps speed up
the process of training.

In the training phase, several models were tested with a variety of options to achieve greater accuracy. The ﬁnal model
exploits the Transfer Learning technique using ResNet [17], a competitive CNN pre-trained on the ImageNet dataset
(above 14 million images belonging to 1000 categories) and winner of ImageNet challenge in 2015.

Despite more up-to-date models, ResNet is still very popular for Transfer Learning implementations. We used ResNet-50
that is 50 layers deep, whose convolutional basis is kept for feature extraction, while the classiﬁer part is replaced by a
new one that will predict the probabilities for 6 classes corresponding to our categories (Fig. 16).

15https://osf.io/8zfh2

20

A Large Visual, Qualitative and Quantitative Dataset of Web Pages

A PREPRINT

Figure 16: Model’s architecture based on ResNet-50.

Only the classiﬁer layers are trained on our dataset. After 500 iterations (epochs) of the whole training set (1758 images)
in groups of 32 images (batch_size), an accuracy of 94.26% was obtained and 40.38% in the validation phase. The
evolution of the process is summarized in the following training and validation curve graphs (Fig. 17).

Figure 17: Accuracy and loss in training and validation phases.

A suitable solution to this problem must meet: a) high training accuracy; b) validation and training curves very close to
each other; and c) small difference between the validation and training error. According to the graphs, only the ﬁrst item
is accomplished, so the model learned very well but does not for generalization, i.e. to classify new images acceptably.
Although we increased the data, tuned the hyperparameters, and applied regularization techniques such as dropout,
neither accuracy is improved nor overﬁtting is signiﬁcantly reduced.

21

A Large Visual, Qualitative and Quantitative Dataset of Web Pages

A PREPRINT

For a better understanding of the results, Fig. 18 displays the confusion matrix with the validation data.

Figure 18: Confusion matrix for validation data.

The model is correct in most cases if we focus on the categories of arts and entertainment, government, and news and
media; however, the number of successes is low. This test takes the validation data, a total of 444 images, achieving
an accuracy of 38.29%, according to the confusion matrix. For remaining categories, the model gets signiﬁcantly
confused. The classiﬁcation of these categories is a very hard problem. Nowadays, the composition of Web pages is
becoming more complex, and the content has a high variability of visual features, even within the same category.

7 Conclusions

We combined different types of data (text, numbers, and images) in a single and extensive dataset about Web pages.
Although the data is organized separately, the qualitative and quantitative parameters are linked to the webshots through
an alphanumeric identiﬁer whose name allows us to know the Web page’s source, category, and country of origin.

We were able to collect 3609 webshots from Browsing (Table 4) and 54565 from Searching (Table 5), a total of 58174
webshots; however, the ﬁnal dataset was reduced to 49438 due to the elimination of Web pages with error messages.
This type of non-valid pages is equivalent to 15%, a signiﬁcant value that reﬂects a problem on the Internet that affects
webmasters, search engines, and users in general. To address this problem, more efﬁcient debugging processes are
needed. As an approach, we implemented an automatic detection of error Web pages based on a CNN model from
scratch achieving acceptable accuracy.

The methodology described for the generation of the Web pages dataset could be adapted without inconvenience in
several problems that involve the collection, organization, analysis, and publication of large amounts of data. Much of
the workﬂow was automatized using Python and R scripts that allowed us to collect URLs and their webshots. By using
scraping, several parameters have been extracted from each Web page. In this way, thousands of Web pages, indexed by
Google and BOTW Web Directory, organized by categories (Arts and Entertainment, Business and Economy, Education,
Government, News and Media, Science and Environment), and geographical location (country and continent), are
available for public use.

The statistical analysis of the quantitative parameters: download time, size, number of images, scripts, CSS ﬁles, number
of tables, iFrames, and style tags, show similar behavior. These variables have a very heterogeneous distribution, high
variability, and tend to be strongly concentrated in the low values (Table 9). This suggests that Web design follows an
implicit rule of optimization of all these parameters, since the higher their values, the longer the download and display
time of the page would increase, causing the consequent user’s discomfort.

Regarding the multi-class CNN model, the results showed that the automatic categorization of Web pages based
exclusively on visual appearance (webshot) is a highly complex problem. Our dataset has proven to be difﬁcult to
classify; the difﬁculty increases when the categories cover a wide range of topics (like the case presented here). In
addition, within each topic there is also a lot of variability in the visual aspect of the images.

Although it is not possible to reliably distinguish between the categories in the dataset using only the webshots, the
successes of the Deep Learning model for Web categorization, especially for government and, arts and entertainment
categories, allow us to presume that it is feasible to identify distinctive visual patterns, which may be part of a next work.

22

A Large Visual, Qualitative and Quantitative Dataset of Web Pages

A PREPRINT

A high level of accuracy has not been achieved; however, it can serve as a baseline for future research. We suppose that
increasing the dataset, preprocessing the images to have the same size and resolution, cropping and scaling the Web
pages, could improve the results.

8 Future work

Our work can motivate the following developments:

• Extending the dataset provided, increasing the categories, URLs, webshots, and other qualitative and quantita-

tive parameters.

• Achieving a higher level of automation in the process of collecting, organizing, and capturing webshots.

• Considering alternative URLs sources, that is, a search engine other than Google for Searching, and a Web

directory other than BOTW for Browsing.

• Improving the accuracy achieved in multi-class categorization of Web pages with deeper and more recent

convolutional neural networks.

References

[1] V. De Boer, M. Van Someren, and T. Lupascu, “Classifying web pages with visual features,” WEBIST 2010 -
Proceedings of the 6th International Conference on Web Information Systems and Technology, vol. 1, pp. 245–252,
2010, doi: 10.5220/0002804102450252.

[2] M. Du, Y. Han, and L. Zhao, “A Heuristic Approach for Website Classiﬁcation with Mixed Feature Extractors,”
IEEE 24th International Conference on Parallel and Distributed Systems (ICPADS), 2018, doi: 10.1109/IC-
PADS.2018.00028.

[3] S. Lassri, H. Benlahmar, and A. Tragha, “Machine Learning for Web Page Classiﬁcation: A Survey,” International

Journal of Information Science & Technology (iJIST), vol. 3, no. 5, pp. 38-50, 2019.

[4] X. Qi and B. D. Davison, “Web page classiﬁcation: Features and algorithms,” ACM Computing Surveys, vol. 41, no.

2, 2009, doi: 10.1145/1459352.1459357.

[5] J. Deng, W. Dong, R. Socher, L. J. Li, K. Li, and L. Fei-Fei, “ImageNet: A Large-Scale Hierarchical Image
Database,” IEEE conference on computer vision and pattern recognition CVPR2009, pp. 248-255, 2009. [Online].
Available: http://www.image-net.org/

[6] K. Reinecke and K. Z. Gajos, “Quantifying visual preferences around the world,” Proceedings of the SIGCHI
Conference on Human Factors in Computing Systems, CHI-14, pp. 11–20, 2014, doi: 10.1145/2556288.2557052.

[7] D. López-Sánchez, J. M. Corchado, and A. G. Arrieta, “A CBR system for image-based webpage classiﬁcation:
Case representation with convolutional neural networks,” FLAIRS 2017 -Proceedings of the 30th International
Florida Artiﬁcial Intelligence Research Society Conference, pp. 483–488, 2017.

[8] M. Nordhoff, T. August, N. A. Oliveira, and K. Reinecke, “A Case for Design Localization: Diversity of Website

Designs in 44 Countries,” pp. 1-12, 2018, doi: 10.1145/3173574.3173911.

[9] D. López-Sánchez, A. G. Arrieta, and J. M. Corchado, “Visual content-based web page categorization
with deep transfer learning and metric learning,” Neurocomputing, vol. 338, pp. 418–431. 2019, doi:
10.1016/j.neucom.2018.08.086.

[10] V. Falconieri, “CIRCL - Computer Incident Response Center Luxembourg,” Security made in Lëtzebuerg
(SMILE), g.i.e. 16, bd d’Avranches L-1160 Luxembourg Grand-Duchy of Luxembourg, 2019. [Online]. Available:
https://www.circl.lu/opendata/circl-phishing-dataset-01/

[11] V. Falconieri, “CIRCL - Computer Incident Response Center Luxembourg,” Security made in Lëtzebuerg
(SMILE), g.i.e. 16, bd d’Avranches L-1160 Luxembourg Grand-Duchy of Luxembourg, 2019. [Online]. Available:
https://www.circl.lu/opendata/circl-ail-dataset-01/

[12] Institute for Computer Research, University of Alicante, 2019, P.O. Box 99. 03080, Alicante, Spain.

[13] A. B. King, “Website Optimization: Speed, Search Engine & Secrets,” O’Reilly Publications, EEUU, ISBN-13:

978-0596515089, 2008.

[14] I. Ruthven and D. Kelly, “Interactive Information Seeking, Behaviour and Retrieval,” London: Facet Publishing,

320 pp., ISBN: 978-1856047074, 2011.

23

A Large Visual, Qualitative and Quantitative Dataset of Web Pages

A PREPRINT

[15] T. Fu, A. Abbasi, and H. Chen, “A focused crawler for Dark Web forums,” Journal of the American Society for

Information Science and Technology, vol. 61, no. 6, pp. 1213-1231, 2010, doi: 10.1002/asi.21323.

[16] D. Liu, J. Lee, W. Wang, and Y. Wang, “Malicious Websites Detection via CNN based Screenshot Recognition,”
International Conference on Intelligent Computing and its Emerging Applications (ICEA), pp. 115-119, 2019, doi:
10.1109/ICEA.2019.8858300.

[17] K. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual Learning for Image Recognition,” IEEE Conference on

Computer Vision and Pattern Recognition (CVPR), pp. 770-778, 2016, doi: 10.1109/CVPR.2016.90.

24

