1
2
0
2

y
a
M
5
2

]

C
O
.
h
t
a
m

[

2
v
7
5
2
3
1
.
4
0
0
2
:
v
i
X
r
a

A Lagrange-Newton Algorithm for Sparse
Nonlinear Programming∗

Chen Zhao †, Naihua Xiu ‡, Houduo Qi §, Ziyan Luo ¶

May 26, 2021

Abstract

The sparse nonlinear programming (SNP) problem has wide applications in signal and image
processing, machine learning, pattern recognition, ﬁnance and management, etc. However, the
computational challenge posed by SNP has not yet been well resolved due to the nonconvex and
discontinuous ℓ0-norm involved. In this paper, we resolve this numerical challenge by developing
a fast Newton-type algorithm. As a theoretical cornerstone, we establish a ﬁrst-order optimality
condition for SNP based on the concept of strong β-Lagrangian stationarity via the Lagrangian
function, and reformulate it as a system of nonlinear equations called the Lagrangian equations.
The nonsingularity of the corresponding Jacobian is discussed, based on which the Lagrange-
Newton algorithm (LNA) is then proposed. Under mild conditions, we establish the locally
quadratic convergence and the iterative complexity estimation of LNA. To further demonstrate
the eﬃciency and superiority of our proposed algorithm, we apply LNA to solve two speciﬁc
application problems arising from compressed sensing and sparse high-order portfolio selection,
in which signiﬁcant beneﬁts accrue from the restricted Newton step in LNA.

Key words. Sparse nonlinear programming, Lagrange equation, The Newton method, Locally
quadratic convergence, Application

AMS subject classiﬁcations. 90C30, 49M15, 90C46

1

Introduction

In this paper, we are mainly concerned with the following sparse nonlinear programming (SNP)
problem:

min
x∈Rn f (x),

s.t. h(x) = 0, x

S,

∈

(1)

{

∈

→
Rn :
k

where f : Rn
S :=
x
of x that counts the number of nonzero components of x. By denoting Ω :=
the feasible set of problem (1) is abbreviated as Ω

Rm are twice continuously diﬀerentiable functions,
k0 is the ℓ0-norm
(0, n) and
x
Rn : h(x) = 0
,
}
∈
S and the optimal solution set can be written

R and h := (h1, . . . , hm)⊤ : Rn
x
k0 ≤

is the sparse constraint set with integer s

∩
∗This research was partially supported by the National Natural Science Foundation of China (11771038, 11971052,

k
x

→

∈

}

{

s

12011530155) and Beijing Natural Science Foundation (Z190002).

†Department of Mathematics, Beijing Jiaotong University, Beijing 100044, China; 14118409@bjtu.edu.cn.
‡Department of Mathematics, Beijing Jiaotong University, Beijing 100044, China; nhxiu@bjtu.edu.cn.
§School of Mathematics, University of Southampton, Southampton SO17 1BJ, UK; hdqi@soton.ac.uk.
¶Corresponding author; Department of Mathematics, Beijing Jiaotong University, Beijing 100044, China;

zyluo@bjtu.edu.cn.

1

 
 
 
 
 
 
as arg minx∈Ω∩S f (x). The SNP problem has wide applications ranging from linear and nonlinear
compressed sensing [12, 14] in signal processing, the sparse portfolio selection [17, 34] in ﬁnance, to
variable selection [19,23] and sparse principle component analysis [3,42] in high-dimensional statistical
analysis and machine learning, etc. Unfortunately, due to the intrinsic combinatorial property in S,
the SNP problem is generally NP-hard, even for the simple convex quadratic objective function [24].
To well resolve the computational challenge resulting from S, eﬀorts have been made in two main-
streams in the literature. The ﬁrst mainstream is “relaxation” approach, with a rich variety of relax-
ation schemes distributed in [10, 15, 18], just name a few. The second one is the “greedy” approach
that tackles the involved ℓ0-norm directly, with a large number of algorithms tailored for SNP with
the feasible set S merely (i.e., Ω = Rn), see, e.g., the ﬁrst-order algorithms [5,25,32], and second-order
algorithms with the Newton-type steps interpolated [9,36–38], etc. As the ﬁrst-order information such
as gradients are used in ﬁrst-order greedy algorithms, linear rate convergence results are established
as one can expect. While beneﬁtting from the second-order information such as Hessian matrices,
the aforementioned second-order greedy algorithms are witnessed in the numerical experiments with
superior performance in terms of fast computation speed and high solution accuracy. Besides the
notable computational advantage that observed numerically, in a very recent work [39], Zhou et al.
propose a new algorithm called Newton Hard-Thresholding Pursuit (NHTP) with cheap Newton steps
in a restricted fashion and rigorously establish the quadratic convergence rate.

In sharp contrast to the fruitful computational algorithms for nonlinear programming with the
single sparse constraint set S, a small portion of research on greedy algorithms is addressed for general
SNP over S intersecting with some additional constraint set. The limited works are distributed
in [2, 20–22, 30]. It is noteworthy that these algorithms are mostly gradient-based, and no quadratic
convergence rate can be expected. To make up such a deﬁciency, the appealing theoretical and
computational properties of NHTP [39] inspires us to develop a quadratic convergent Newton-type
method for SNP when Ω is characterized by nonlinear equality constraints as presented in problem
(1).

The main contributions of this paper are summarized as below:

(i) The strong β-Lagrangian stationarity is introduced to characterize the optimality condition for
SNP, and an equivalent characterization of such a stationary point is built which is accessible
to performing the Newton method.

(ii) The crucial Jacobian nonsingularity of the underlying system is well addressed under some
mild assumptions, and the essential linear system in each iteration is reduced to be of size
(s + m)

(s + m), a signiﬁcant dimension reduction beneﬁtting from the intrinsic sparsity.

×

(iii) The resulting Lagrange-Newton algorithm (LNA) is shown to possess the locally quadratic con-
vergence, and to gain high eﬃciency numerically for speciﬁc application problems including
compressed sensing and sparse portfolio selection.

The remainder of this paper is organized as follows. In Section 2, the optimality condition in terms
of Lagrangian stationarity is established. In Section 3, an equivalent Lagrangian equation system is
proposed and the nonsingularity of its Jacobian is discussed. The framework of LNA and its locally
quadratic convergence are elaborated in Section 4. Two well-known applications are analyzed in
Section 5. Extensive numerical experiments are conducted in Section 6. Conclusions are made in
Section 7.

For convenience, the following notations will be used throughout the paper. For any given positive
be the cardinality of J that
T be its complementary set. The collection of

integer n, denote [n] :=
{
counts the number of elements in J, and J c := [n]

. For an index set J

1, . . . , n

[n], let

⊆

J

}

|

|

\

2

{

}

J

J

and

= 0

∈
∈

Js(x) :=

[n] : xi 6
Rn indexed by T . For the matrix A

Rn, denote
all index sets with cardinality s in [n] is deﬁned by
J
. Given x
|
∈
R|T | as the
supp(x) :=
i
. We deﬁne xT ∈
Rm×n, deﬁne AI,J as a submatrix whose
subvector of x
rows and columns are respectively indexed by I and J. In particular, we write AT as its submatrix
consisting of columns indexed by T and AT,· as its submatrix consisting of rows indexed by T . Given
2g(x) at x, denote
a twice continuously diﬀerentiable function g with its gradient
g(x) and Hessian
2g(x))I,J . The Euclidean norm of a vector x is denoted by
∇T g(x) := (
∇
x
k
k

Js :=
⊆
∈ Js : supp(x)
∈

, and the spectral norm of a matrix A is denoted by

2
I,J g(x) := (
∇

[n] :
|
J
}

g(x))T and

A
k

= s

∇

∇

∇

⊆

{

}

{

k

.

2 Lagrangian Stationarity

This section is devoted to the optimality conditions for (1) in terms of the Lagrangian stationarity,
which will build up the theoretical fundamentals to our new proposed algorithm in the sequel.
Firstly, we consider the projection on sparse set S. For any given nonempty closed set Q

∈

Rn, deﬁne the projection operator ΠQ(x) := arg miny∈Q k
any x
y
k
projection operator ΠS admits an explicit formula as follows: for any z
∈
we have

−

x

Rn and
2. Recall from [2,29] that the
ΠS(z),

Rn, and for any π

⊆

∈

πti =

zti ,
0,

[s],
i
otherwise,

∈

(2)

|

{

. . .

≥ |

satisﬁes

zt1| ≥

t1, . . . , tn}

. Utilizing the sparse projection ΠS, Beck and Eldar
where
[1, Theorem 2.2] introduced and characterized an optimality condition for SNP with single sparse
contraint set S: if x∗ is a global minimizer, then for any L > Lf , x∗ = ΠS(x∗
f (x∗)/L), where Lf
S by means
is the Lipschitz constant of
f (x∗)/L), where Ω′ is a nonempty closed and convex set. Limitation follows
of x∗ = ΠΩ′∩S(x∗
) has no explicit expression. This motivates us to introduce the following Lagrangian
when ΠΩ′∩S(
·
stationarity.

f . Later, Lu [21] extended such a result to the case of Ω′

− ∇

− ∇

∇

∩

Deﬁnition 2.1. Given x∗
problem (1) if there exists a Lagrangian multiplier y∗

∈

Rn and β > 0, x∗ is called a strong β-Lagrangian stationary point of

(cid:26)
ztn |

Rm such that

∈
∇xL(x∗, y∗)),

x∗ = ΠS(x∗
h(x∗) = 0,

β

−

(cid:26)

where L(x, y) := f (x)
y

Rm.

∈

y, h(x)
i

− h

is the Lagrangian function associated with (1) for any x

By employing (2) of ΠS, one can easily rewrite system (3) as follows.

Rm, denote Γ∗ := supp(x∗) and q∗ =
Lemma 2.2. Given x∗
∈
is a strong β-Lagrangian stationary point of problem (1) with y∗ if and only if

Rn and y∗

∈

∇xL(x∗, y∗). Then x∗

S, h(x∗) = 0,

x∗

∈

q∗
β
(Γ∗)c
q∗ = 0,

k

(cid:26)

k∞ <

|

x∗

|(s) & q∗

Γ∗ = 0,

if

if

x∗
x∗

k

k

k0 = s,
k0 < s,

(4)

(5)

where supp(x) :=

i

{

∈

[n] : xi 6

= 0

}

and x(s) is the sth largest component of x.

The following assumption is introduced, followed by optimality analysis.

Assumption 2.3. Given x
Γ = supp(x).

Ω

∩

∈

S, rank(

∇Γh(x)) = m, where

3

h(x) := (

h1(x), . . . ,

∇

∇

hm(x))⊤,

∇

(3)

S and

∈

Theorem 2.4 (First-order necessary optimality condition). Suppose that x∗ is a local minimizer of
Rm such that x∗ is a strong
(1) and Assumption 1 holds at x∗. Then there exists a unique y∗
β-Lagrangian stationary point of problem (1) for any β

∈
(0, ˆβ), where

∈

|(s)

c (cid:13)
)
(cid:13)
(cid:13)

∗

∗

|x
q∗
(cid:13)
(Γ
(cid:13)
(cid:13)
+

,
∞

,

∞

if

x∗

k0 = s and q∗

(Γ∗)c

k

= 0,

otherwise.

(6)

ˆβ := 




Proof. Since x∗ is a local minimizer of (1.1), for any J

∈ Js(x∗), x∗ is also a local minimizer of

(PJ ) min

x∈Rn f (x),

s.t. h(x) = 0, xJ c = 0.

Rn is the
Assumption 1 implies that
∈
jth column in the identity matrix. It means that linear independent constraint qualiﬁcation (LICQ)
Rm and a unique
holds at x∗ for (PJ ). Thus for any given J
zJ

J c are linearly independent, where ej ∈

Rn−s such that

hi(x∗), i

[m], ej, j

∇

∈

∈

∈ Js(x∗), there exists a unique yJ
h(x∗))⊤yJ +
zJ
j ej.

(7)

∈

f (x∗) = (

∇

∇

Xj∈J c

x∗
Case I: When
k
calculations yield

k0 = s, we have

Js(x∗) =

Γ∗

{

}

. Set J = Γ∗ in (7), and let y∗ = yΓ

∗

. Direct

q∗
Γ∗ =

f (x∗)

∇
(cid:0)

h(x∗))⊤y∗

(
∇

−

Γ∗ = 
(cid:1)


Xi∈(Γ∗)c

∗

zΓ
i ei

Γ∗

= 0 and q∗

(Γ∗)c = zΓ

∗

,

from which (4) holds for all β
Case II: When
form

k0 < s,

x∗

k

(0, ˆβ).
=

∗

∈
|Js(x∗)
|
C
∇Γ∗ f (x∗) = (
∇J\Γ∗ f (x∗) =
∇J c f (x∗) =






∇Γ∗ h(x∗))⊤yJ ,
∇J\Γ∗ h(x∗))⊤yJ ,
∇J c h(x∗))⊤yJ + zJ.

s−Γ
n−Γ∗ is ﬁnite. For any J

∈ Js(x∗), rewrite (7) into the block

(8)

By virtue of Assumption 1, the ﬁrst equation in (8) indicates that yJ ’s coincide for all J
which we assign to y∗. It then leads to q∗
second equation in (8) that q∗
2.2.

∈ Js(x∗),
Γ∗) = (Γ∗)c, it follows from the
(Γ∗)c = 0. Thus, q∗ = 0. This completes the proof by utilizing Lemma

Γ∗ = 0. As

J∈Js(x∗)(J

S

\

Theorem 2.5 (First-order suﬃcient optimality condition). Let f be a convex function and h be an
aﬃne function. Given β > 0, suppose that x∗ is a strong β-Lagrangian stationary point of (1) with
the Lagrangian multiplier y∗
k0 < s,
then x∗ is a global minimizer of (1).

k0 = s, then x∗ is a local minimizer of (1); If

Rm. If

x∗

x∗

∈

k

k

Proof. Under the hypotheses on f and h, the Lagrangian function L(x, y) is convex with respect to
x. It follows that

L(x, y∗)

L(x∗, y∗) +

≥

q∗, x
h

−

x∗

,
i

x

∀

∈

Ω

∩

S.

(9)

∈

Ω

Using the facts x, x∗
Lagrangian stationary point with y∗, if
f (x)
≥
that for any x
that

(x∗, δ)

∈ N

(Ω

∩

∩

∩

f (x∗). Thus x∗ is a global minimizer. If

S, we have L(x, y∗) = f (x), L(x∗, y∗) = f (x∗). Since x∗ is a strong β-
S,
q∗, x
k0 < s,
= 0 from (5). Then for any x
h
x∗
k0 = s, there exists a suﬃciently small δ > 0 such
k
S), x(Γ∗)c = 0 and hence (x
x∗)(Γ∗)c = 0. By invoking (4), it yields

x∗

x∗

−

Ω

∈

∩

k

i

−

q∗, x
h

−

x∗

i

=

q∗
Γ∗ , (x
h

−

x∗)Γ∗

+

q∗
(Γ∗)c, (x
h

i

−

x∗)(Γ∗)c

i

= 0.

4

6
Thus for any x
(1).

(x∗, δ)

(Ω

∩

∩

∈ N

S), f (x)

≥

f (x∗), which implies that x∗ is a local minimizer of

Remark 2.6. Consider problem (1). (i) By virtue of Theorem 2.4 and Lemma 2.2, we summarize
the relations among the strong β-Lagrangian stationarity (Strong-β-LS), B-KKT point and C-KKT
point in [28, Deﬁnition 3.1], S-stationarity (S-stat) and M-stationarity (M-stat) in [8, Deﬁnition 4.1]
as below.

Local minimizer

Assumption 2.3
=
⇒

Strong-β-LS

(0, ˆβ)

β

∀

∈

M-stat

= S-stat

⇐

⇐⇒

C-KKT point

⇐

= B-KKT point

m

(ii) Given a feasible solution x, we can show that Assumption 2.3, the restricted Robinson constraint
qualiﬁcation (R-RCQ) in [27, Deﬁnition 3.1], and the cardinality constraints linear independence
constraint qualiﬁcation (CC-LICQ) in [8, Deﬁnition 3.11] are equivalent. They are stronger than the
restricted linear independent constraint qualiﬁcation (R-LICQ) in [28, Deﬁnition 2.4] when
k0 = s.
x
Learning from [28], together with (i) of this remark, we obtain that R-LICQ could also ensure the
existence of strong β-Lagrangian stationary point, but hardly could guarantee the uniqueness of the
corresponding Lagrangian multiplier as stated in Theorem 2.4.

k

3 Lagrangian Equations and Jacobian Nonsingularity

In this section, we will present an equivalent reformulation for the Lagrangian stationarity in terms
of nonlinear equations, and discuss the Jacobian nonsingularity of the resulting equation system.

3.1 Lagrangian Equations

The optimality conditions in terms of the strong β-Lagrangian stationary point, as established in
Theorems 2.4 and 2.5, provide a way of solving (1). As one knows that ΠS(
) is not diﬀerentiable,
·
the main challenge is how to tackle such a non-diﬀerentiability. By exploiting the special structure
possessed by the projection operator ΠS, we propose a diﬀerentiable reformulation of the deﬁnitional
expression of the strong β-Lagrangian stationary point (3), using a ﬁnite sequence of Lagrangian
equations.

Deﬁnition 3.1. Given x
of sparse projection index sets of u by

∈

∈

S, y

Rm and β > 0, denote u := x

β

∇xL(x, y). Deﬁne the collection

−

T(x, y; β) :=

|
T(x, y; β), deﬁne the corresponding Lagrangian equation as

∈

∈

{

}

T

∈ Js :

ui| ≥ |

uj|

,

i
∀

T,

j
∀

T c

.

For any given T

∈

∇xL(x, y))T
(
F (x, y; T ) := 
xT c
h(x)



−

= 0.





(10)

(11)

As one can see, the function F (x, y; T ) in (11) is diﬀerentiable with respect to x and y once T is

selected. Moreover, we have the following equivalent relationship between (11) and (3).

S, y∗
Theorem 3.2. Given x∗
of (1) with the Lagrangian multiplier y∗ if and only if for any T
Meanwhile, T(x∗, y∗; β) =

Rm and β > 0, x∗ is a strong β-Lagrangian stationary point
T(x∗, y∗; β), F (x∗, y∗; T ) = 0.

∈

∈

∈
Js(x∗).

5

T c = 0 and hence supp(x∗)

Proof. By invoking the proof Lemma 4 in [39], we can obtain the equivalent relationship directly.
T(x∗, y∗; β),
Now, we prove the rest part of the theorem. If follows from Theorem 3.2 that for any T
∈
F (x∗, y∗; T ) = 0. Thus, x∗
∈ Js(x∗). The
⊆
arbitrariness of T leads to the inclusion T(x∗, y∗; β)
⊆ Js(x∗).
Js(x∗)
⊆
T(x∗, y∗; β). If
x∗
∇xL(x∗, y∗) = 0 by invoking (5), and hence u∗ = x∗
∇xL(x∗, y∗) =
β
k
−
∈ Js(x∗), it is easy to verify that for any i
T c,
u∗
x∗. For any T
u∗
x∗
x∗
=
,
=
i | ≥ |
i |
j |
j |
∈
|
T(x∗, y∗; β); If
Γ∗
Js(x∗) =
k0 = s, then
. By virtue
which indicates that T
|
}
{
∇xL(x∗, y∗))j|
u∗
Γ∗,
Γ∗ and any j /
u∗
|(s) > β
of (4), we have that for any i
.
=
(
=
i |
j |
|
|
∈
T(x∗, y∗; β). This
Thus, Γ∗
Js(x∗)
⊆
∈
completes the proof.

∈
|(s) > 0 and
x∗
x∗
i | ≥ |

In a word, in both cases, we can conclude

T and any j
x∗

T . It then yields that T

It now suﬃces to show

k0 < s, then

T(x∗, y∗; β).

x∗

∈

∈

k

|

|

|

|

3.2 Jacobian Nonsingularity

In this subsection, let x∗ be a strong β-Lagrangian stationary point with y∗. To handle the Lagrangian
T(x∗, y∗; β), it is crucial to discuss the nonsingularity of the
equation (11) for a given index set T
Jacobian of F (x, y; T ) with respect to (x, y) in a neighborhood of (x∗, y∗), namely,

∈

∇(x,y)F (x, y; T ) = 


2
xxL(x, y))T,· −
(
∇

IT c,·

h(x)

−∇

∇T h(x))⊤
(
0
0

∈





R(n+m)×(n+m),

(12)

m

Pi=1

where

2
xxL(x, y) =
∇

2f (x)

∇

−

2hi(x) is Hessian matrix of L(x, y) with respect to x, and I is

yi∇

the identity matrix. It is worth mentioning that since T is related to (x, y), the conventional Jacobian
of F (x, y; T ) may diﬀer with (12) if we treat T as a function of (x, y). However, as T may vary as
(x, y) changes, we will update such an index set T in our proposed iterative algorithm adaptively.
Two additional assumptions are stated as below.
∇T h(x∗)) = m, for any T
Assumption 1′ rank(
Assumption 3.3. (Second-order optimality condition) For any T
positive deﬁnite restricted to the null space of

xxL(x∗, y∗))T,T is
2
∈ Js(x∗), (
∇

∈ Js(x∗).

∇T h(x∗), i.e.,

d⊤

xxL(x∗, y∗)
2
∇
(cid:1)
(cid:0)
∈ Js and (x, y)
Given any T
the nonsingularity of

S
∇(x,y)F (x, y; T ) and that of

×

∈

T,T d > 0,

0

= d

N(

∇T h(x∗)) :=

d

Rs :

∇T h(x∗)d = 0

.

∈

∀
Rm, elementary row operations yield the equivalence between

∈

{

}

G(x, y; T ) :=

2
xxL(x, y))T,T −
(
∇
−∇T h(x)

(cid:20)

∇T h(x))⊤
(
0

(cid:21) ∈

R(s+m)×(s+m).

(13)

Thus, we call G(x, y; T ) the reduced Jacobian of F (x, y; T ). Furthermore, when Assumptions 1′ and 3.3
hold, we can directly obtain the desired nonsingularity at (x∗, y∗) as stated in the following theorem.

Theorem 3.4. Let x∗ be a strong β-Lagrangian stationary point with y∗. If Assumptions 1′ and 3.3
hold, then

∇(x,y)F (x∗, y∗; T ) is nonsingular for each index set T

T(x∗, y∗; β).

∈

The rest of this subsection is devoted to the nonsingularity of G(x, y; T ) when (x, y) are suﬃ-
ciently close to (x∗, y∗), by employing the achieved nonsingularity of G(x∗, y∗; T ) and the following
assumption.

Assumption 3.5.

2f and

∇

2hi (i

∇

∈

[m]) are Lipschitz continuous near x∗.

6

6
such that for any z, ˆz

∈ N

(z∗, δ∗

The locally Lipschitz continuity in Assumption 3.5 allows us to ﬁnd positive constants δ∗
0 ) with z∗ := (x∗; y∗), we have
L1k
− ∇xL(ˆx, ˆy)
2L(ˆx, ˆy)
L2 k
− ∇

k∇xL(x, y)
2L(x, y)
k∇

k ≤
= 0 (since the trivial case x∗ = 0 is not desired in practice) be a strong β-Lagrangian stationary

0, L1, L2

ˆz
k
ˆz
k

(15)

(14)

k ≤

−

−

z

z

.

,

Let x∗
point with y∗. We can deﬁne

δ∗
1 :=

min
i∈Γ∗ |

β max

x∗
i | −
√2(1 + βL1)

i∈(Γ∗)c |

q∗
i |

,

where Γ∗, q∗ are deﬁned in Lemma 2.2. By employing Lemma 2.2, one can easily verify that δ∗
since x∗

= 0. Denote

1 > 0

S(z∗; δ∗) :=

N

z

{

∈

δ∗ := min

0, δ∗
δ∗
1}
S,
Rn+m : x

{

∈

,

z

k

−

z∗

k

< δ∗

.

}

(16)

(17)

Lemma 3.6. Let x∗ be a strong β-Lagrangian stationary point with y∗. Denote z∗ := (x∗; y∗). If
Assumption 3.5 holds, then for any z := (x; y)

S(z∗; δ∗), we have

∈ N
T(x∗, y∗; β) and Γ∗

T(x, y; β)

⊆
k0 = s, then

supp(x)

T,

∀

∩

T

∈

⊆

T(x, y; β).

(18)

Particularly, if

x∗

k

supp(x)
}

{

= T(x, y; β) = T(x∗, y∗; β) =

Γ∗

.

}

{

Proof. Since x∗ is a strong β-Lagrangian stationary point with y∗, we have F (x∗, y∗; T ) = 0,
T(x∗, y∗; β) =
supp(x) and q =

∈
S(z∗; δ∗), denote Γ =

∈ N

T

∀

|

∈

Γ∗ and any j

(Γ∗)c, we have

Js(x∗) from Theorem 3.2. Consider any given z = (x; y)
∇xL(x, y). For any i
∈
xj −
βqi| − |
xi −
x∗
x∗
xi −
i | − |
i | − |
x∗
t | −
x∗
t | −

x∗
qi −
j | −
|
√2βL1k
z
√2(1 + βL1)δ∗

βqj|
xj −
z∗

q∗
i | −
z∗

k −
q∗
t | −

≥ |
(14)

min
t∈Γ∗ |

t∈(Γ∗)c |

β max

qj −
β max

√2

k −

≥

−

−

≥

β

β

k

z

|

q∗
j |

|

β

q∗
j | −
q∗
t |

t∈(Γ∗)c |

min
t∈Γ∗ |
0.

≥

This indicates that i

T and hence

∈

Furthermore, we have

Γ∗

T,

⊆

T

∀

∈

T(x, y; β).

T(x, y; β)
Next we claim that Γ∗ is also a subset of Γ. If not, there exists an index i0 ∈
x∗
x∗
t | ≥

⊆ Js(x∗) = T(x∗, y∗; β).

x∗
i0 | ≥

x∗)i0 |

min
t∈Γ∗ |

k ≥ k

z∗

(x

=

−

−

x

k

z

|

δ∗,

(19)

(20)

Γ∗

\

Γ. Then

which is a contradiction to z

k ≥ |

−
S(z∗; δ∗). Thus,

∈ N

Γ∗

Γ.

⊆

Summarizing (19), (20) and (21), we get (18). Particularly, if
Γ∗
{
of the desired assertion.

k
. Utilizing (19), (20) and (21) again, together with the fact

}

Finally, the desired nonsingularity in a given neighborhood is stated.

7

(21)
Js(x∗) =
s, we immediately get the rest

= s and

Γ∗

|

|

x∗
k0 = s, then
Γ
| ≤

|

6
6
Theorem 3.7. Let x∗ be a strong β-Lagrangian stationary point with y∗. If Assumptions 1′, 3.3 and
3.5 hold, then there exist constants ˜δ∗
(0, δ∗] and M ∗

) such that for any z := (x; y)

(0, +

S(z∗; ˜δ∗) with z∗ := (x∗; y∗), the reduced Jacobian matrix G(x, y; T ) is nonsingular and

∈

∈

∞

∈

N

G−1(x, y; T )

k ≤

k

M ∗,

T

∀

∈

T(x, y; β).

(22)

T(x∗, y∗; β) =

Proof. By invoking Theorems 3.2 and 3.4, we can get the nonsingularity of G(x∗, y∗; T ) for all
T(x∗, y∗; β) from Lemma
T
⊆
T(x, y; β). Furthermore, it
3.6 immediately yields the nonsingularity of G(x∗, y∗; T ) for each T
follows from (15) that for any T

S(z∗; δ∗), the inclusion T(x, y; β)

Js(x∗). For any z

T(x, y; β),

∈ N

∈

∈

∈
G(x∗, y∗; T )

G(x, y; T )

k

−
; T ) is Lipschitz continuous near z∗ for any given T

k ≤ k∇

− ∇

k ≤

k

−
T(x, y; β). Thus, there

z

L2k

z∗

,

(23)

2L(x, y)

2L(x∗, y∗)

(z∗; δT ), G−1(x, y; T ) exists and

∈

G−1(x, y; T )

k ≤

k

which indicates that G(
,
·
exists δT > 0 and MT > 0 such that for any z
MT . Set

·

∈ N

˜δ∗ := min
{

δ∗,

{

, and M ∗ := max

δT }T ∈Js(x∗)}
(z∗; ˜δ∗), G(x, y; T ) is nonsingular and

T ∈Js(x∗){

MT }

.

(24)

G−1(x, y; T )

k ≤

k

M ∗, for

It follows readily that for any z
all T

T(x, y; β).

∈

∈ N

4 The Lagrange-Newton Algorithm

In this section, we propose a Newton Algorithm for solving Lagrangian equation (11) of problem
(1) which is named as Lagrange-Newton Algorithm (LNA), and analyze the convergence rate of the
algorithm.

4.1 LNA Framework

Given β > 0, let (xk, yk)

By employing the relationship between the strong β-Lagrangian stationary point and the Lagrangian
equations as stated in Theorem 3.2, the basic idea behind our algorithm is: solve the Lagrangian
equation F (x, y; T ) = 0 iteratively by using the Newton method, and update the involved index set
T accordingly from T(x, y; β) by deﬁnition in each iteration. Details on the algorithm are as below.
Rm be the current iteration.
Index Set Selection: Choose one index set Tk from T(xk, yk; β) deﬁned as in (10). This can be
safely accomplished by picking the indices of the ﬁrst s largest elements (in magnitude) in xk
∇xL(xk, yk).
β
The Newton Step: The classical Newton equation is

−

×

∈

S

∇(x,y)F (xk, yk; Tk)(xk+1

−

xk; yk+1

yk) =

−

−

F (xk, yk; Tk).

After simple calculations, (25) can be rewritten as

xk+1
T c
k

= 0;

G(xk, yk; Tk)






xk+1
Tk
yk+1(cid:21)

(cid:20)

=

2
−∇Tk f (xk) + (
xxL(xk, yk))Tk,.xk
∇
h(xk)
(cid:20)
− ∇

h(xk)xk

,

(cid:21)

from which a signiﬁcant dimension reduction is attained, from (n + m)
(s + m).
Under the conditions presented in Theorem 3.7, G(xk, yk; Tk) is nonsingular, and hence the next

(n + m) to (s + m)

×

×

(25)

(26)

8

iteration can be obtained from the unique solution of (26), which can be solved in a direct way if
s + m is small or by employing the conjugate gradient (CG) method when s + m is relatively large.
As indicated, the low computational cost of the Newton step is greatly beniﬁcial from the intrinsic
sparisity, especially when s
Stopping Criterion: Given the current iteration triplet (xk, yk, Tk), to measure how far xk is from
being a strong β-Lagrangian stationary point, the following quantity is adopted

≪

n.

ηβ(xk, yk; Tk) :=

F (xk, yk; Tk)
k

k

+ max
i∈T c

k n

max

∇xL(xk, yk))i|−|
(

xk

|(s)/β, 0

.

(cid:17)o

(27)

(cid:16)|

The ﬁrst term on the right-hand side of (27) is to measure the residual of the Lagrangian equation
system, and the second term is to testify

∇xL(xk, yk)
(cid:13)
(cid:1)
(cid:0)
(cid:13)
(cid:13)

T c
k (cid:13)
(cid:13)
(cid:13)

1
β |

xk

|(s),

∞ ≤

an inequality comes from (10) and (11) after simple manipulations. The stopping criterion is then
designed in terms of ηβ(xk, yk; Tk).
The algorithmic framework is now summarized as follows.

Algorithm 1 Lagrange-Newton Algorithm (LNA) for (1)
Step 0. (Initialization) Give β > 0 and ǫ > 0, choose (x0, y0)
Step 1. (Index Set Selection) Choose Tk ∈
Step 2. (Stopping Criterion) If ηβ(xk, yk; Tk)
ǫ, then stop. Otherwise, go to Step 3.
Step 3. (The Newton Step) Update (xk+1, yk+1) by (26), set k = k + 1 and go to Step 1.

∈
T(xk, yk; β) by (10).

Rm and set k = 0.

≤

×

S

4.2 Locally Quadratic Convergence

The locally quadratic convergence of LNA is shown to be inherited from the classic Newton method,
armed with the essential invariance property of index sets as stated in Lemma 3.6. Speciﬁcally, we
have

Theorem 4.1. Given β > 0, suppose x∗ is a strong β-Lagrangian stationary point of (1) with y∗. If
Assumptions 1′, 3.3 and 3.5 hold. Let ˜δ∗ and M ∗ be deﬁned as in (24), and T(x∗, y∗; β) be deﬁned
as in (10), respectively. Denote z∗ := (x∗; y∗). Suppose that the initial point z0 := (x0; y0) of LNA
satisﬁes z0
generated by
LNA is well-deﬁned and for any k

S(z∗, δ) with δ = min
{
0,

zk := (xk; yk)
}

. Then the sequence

1
M ∗L2 }

∈ N

˜δ∗,

{

≥
zk = z∗ with quadratic convergence rate, namely

(i)

lim
k→∞

zk+1

k

z∗

−

k ≤

M ∗L2
2

zk
(cid:13)
(cid:13)

−

z∗

2

.

(cid:13)
(cid:13)

(ii)

lim
k→∞

F (zk; Tk) = 0 with quadratic convergence rate, namely

F (zk+1; Tk+1)

k ≤

k

M ∗L2

L2

1 + 1

p
λH

F (zk; Tk)
k

k

2,

where λH := min

Tk∈Js(x∗)

λmin

∇zF (z∗; Tk)⊤
(cid:0)

∇zF (z∗; Tk)
.
(cid:1)

9

(iii)

lim
k→∞
when

ηβ(zk; Tk) = 0 with ηβ(zk+1; Tk+1)

M ∗L2

≤

L2

1 + 1
k

p

zk

z∗

k

−

2 and LNA will terminate

log2

4δ2M ∗L2
(cid:16)

2

k



≥

L2

1 + 1/ǫ

p

denotes the smallest integer no less than t.





where

t

⌉

⌈

(cid:17)



,





Proof. By employing Theorem 3.7, we know that the sequence
from the nonsingularity of G(xk, yk; Tk) for all k

0.

zk

{

}

generated by LNA is well-deﬁned

(i) Choose T0 ∈

≥
T(x0, y0; β). Lemma 3.6, together with Theorem 3.2, yields x∗
T c
0
following from (26) in Algorithm 1, we also have x1
T c
0
obtain

= 0. Meanwhile,
= 0. With some routine work, one can further

(26)
=

≤

k

k

z∗

z1
−
x1
T0
y1 (cid:21) − (cid:20)
(cid:13)
(cid:20)
(cid:13)
(cid:13)
M ∗L2
(cid:13)
2

z0
(cid:13)
(cid:13)

x∗
T0
y∗ (cid:21)(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

z∗

−

(cid:13)
(cid:13)

(22)

≤

M ∗

z0

≤ (cid:13)
(cid:13)

G(x0, y0; T0)

x1
T0
y1 (cid:21) − (cid:20)

(cid:18)(cid:20)

z∗

δ
2

.

<

(cid:13)
(cid:13)

x∗
T0
y∗ (cid:21)(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
−
2

(28)

(29)

Similar reasons allow us to sequentially get

zk+1

k

z∗

−

k ≤

M ∗L2
2

2

z∗

and zk

zk
(cid:13)
(cid:13)

−

(cid:13)
(cid:13)

S(z∗,

∈ N

δ
2k ).

Hence lim
k→∞

zk = z∗ with quadratic convergence rate.

(ii) Similar to the case k = 1, Lemma 3.6 and Theorem 3.2 also yield x∗
T c

(29). After basic manipulations, we have

k+1

= 0 for any k

1 from

≥

Fβ(zk+1; Tk+1)
k

k

2

(28)

≤

(L2

1 + 1)

M ∗L2

2

(cid:18)

2 (cid:19)

k

zk

z∗

4.

k

−

(30)

Additionally, the index set property in Lemma 3.6 ensures

T(zk; β)

Tk ∈

⊆

T(z∗; β) =

Js(x∗),

k

∀

≥

0

which further leads to F (z∗; Tk) = 0 from Theorem 3.2, and the nonsingularity of
Theorem 3.4. Thus, λH > 0 and

∇zF (z∗; Tk) from

zk

k

z∗

2

k

−

≤

≤

=

1
λH k∇zF (z∗; Tk)(zk
2
λH k∇zF (z∗; Tk)(zk
2
F (zk; Tk)
λH k
k

2,

2

z∗)
k

−

z∗) + o(
k

zk

−

z∗

2

)
k

k

−

(31)

where the last equality is from F (z∗; Tk) = 0. Combining with (30), we have

F (zk+1; Tk+1)

k ≤

k

zk

z∗

2

k

≤

M ∗L2

L2

1 + 1

p
λH

F (zk; Tk)
k

k

2.

M ∗L2

L2

1 + 1

p
2

k

−
∇xL(xk, yk))j | − |
(

xk

(iii) Denote ζk := max
j∈T c
k {

max

{|

|(s)/β, 0

}

, for any given k

≥

1. We claim that

ζk ≤

L1k

zk

z∗

,

k

−

k

∀

≥

1.

10

(32)

1, we consider the following two cases.

For each given k
Case I: If

k

x∗

∇xL(x∗, y∗) = 0. It follows from the deﬁnition of ζk that
z∗

≥
k0 < s, then
∇xL(xk, yk))T c
ζk ≤ k
(
k0 = s, we have T(zk; β) =
∇xL(x∗, y∗))Tk = 0. Besides, since
≥

k k ≤ k∇xL(xk, yk)
Γ∗
Γk}
{
xk
|(s) > 0, there exists ik ∈
|
1, it follows from the deﬁnition of T(xk, yk; β) that for any j

− ∇xL(x∗, y∗)
from Lemma 3.6. Thus, for any k

|
k = (Γ∗)c,
T c

Tk such that

xk
ik |

L1k

k ≤

zk

x∗

=

−

k

}

{

k

.

Case II: If
and (
any k

1, Tk = Γ∗
xk
|(s). For

≥
=
|

β

∇xL(xk, yk))j |
(

|

=

≤

∈
xk
∇xL(xk, yk))j |
β(
j −
|
xk
min
i −
i∈Tk{|
xk
∇xL(xk, yk))ik |
β(
ik −
xk
|(s) + β
k

β(

∇xL(xk, yk))i|}

≤ |

≤ |
xk
|(s)/β, 0

.

∇xL(xk, yk))Tk k
(
∇xL(xk, yk))Tk k
,
(

(33)

j
∀

∈

T c
k , which means

This implies that max
{|
∇xL(xk, yk))Tk k
ζk ≤ k
(
∇xL(xk, yk))Tk −
(
ζk ≤ k

∇xL(xk, yk))j | − |
(
. Together with (

} ≤ k
∇xL(x∗, y∗))Tk = 0, we have
zk
∇xL(x∗, y∗))Tk k ≤
(

L1k

−

z∗

,

k

k

∀

≥

1.

This shows the claim in (32). Combining with (30) and (28), we further get that for k

1,

≥

ηβ(xk, yk; Tk) =

F (zk; Tk)
+ ζk
k
k
M ∗L2
L2
1 + 1

≤

≤

M ∗L2

p
2

L2

1 + 1

q

k

zk−1

k
zk−1

z∗

−

2 +

k
2.

z∗

−

k

M ∗L2L1
2

k

zk−1

z∗

2

k

−

(34)

In addition, by virtue of (29), we obtain ηβ(xk, yk; Tk)
ηβ(xk, yk; Tk)
desired. This completes the proof.

ǫ in LNA, it suﬃces to have

δ2M

≤

∗

≤
L2√L2
22k−2

1+1

∗

δ2M

L2√L2
22k−2

1+1

. To meet the stopping criterion

ǫ, which leads to the bound of k as

≤

5 Applications

Two selected SNP problems arising from some important applications are considered to demonstrate
the eﬀectiveness of our proposed Lagrange-Newton algorithm.

5.1 Compressed Sensing

Compressed sensing (CS) [13] has been widely applied in signal and image processing [14], machine
learning [41], statistics [26], etc. A more general framework is considered, where some noise-free
observations are allowed and added as hard constraints into the standard CS model, taking the form
of

min

where A

∈

R(p−m)×n, C

Rm×n, b

∈

∈

b

2,

Ax

1
2 k
−
Rp−m and d

k

s.t. Cx = d, x

S,

∈

Rm. Set

∈

(35)

f (x) =

1
2 k

Ax

b

k

−

2 and h(x) = Cx

d.

−

11

The Lagrangian function of (35) is

L(x, y) =

1
2 k

Ax

2

b

k

−

−

y⊤(Cx

d),

−

for any x

S and y

∈

∈

Rm. Direct calculations lead to

h(x) = C,

2f (x) =

∇

∇xL(x, y) = A⊤(Ax
xxL(x, y) = A⊤A,
2
∇

∇

b)

C⊤y,

−
−
2h(x) = 0,

2L(x, y) =

∇

A⊤A
C

−

(cid:20)

−

C⊤
0 (cid:21)

.

(36)

Since

) are constant, Assumption 3.5 holds automatically everywhere. To ensure
Assumptions 1′ and 3.3 hold, we introduce the following assumption on the input matrices A and C.

) and

∇

∇

2h(
·

∇





2f (
·

Assumption 5.1. For any index set T

∈ Js, AT is full column rank and CT is full row rank.
Rn+m and β > 0, T(x, y; β)

Suppose that Assumption 5.1 holds. Note that for any (x, y)

⊆ Js.
∇T h(x)) = rank(CT ), we can conclude that Assumption 1′ holds everywhere once
T AT , it is positive deﬁnite in

Together with rank(
CT is full row rank for all T
∈ Js. Similarly, since
the entire space Rs once AT is full column rank. Thus, Assumption 3.3 follows.

T,T = A⊤
2
xxL(x, y)
∇
(cid:1)
(cid:0)
It is worth mentioning that Assumption 5.1 is actually a mild condition for problem (35). Indeed,
the full column rankness of AT is the so-called s-regularity introduced by Beck and Eldar [1] which
has been widely used in the CS community, and limiting the number of hard constraints will make
the full row rankness of CT accessible (here m is no more than s and hence s + m
2s). Under
Assumption 5.1, we have the following optimality conditions for problem (35).

≤

∈

Proposition 5.2. Assume that the feasible set of problem (35) is nonempty and Assumption 5.1 holds.
Then the optimal solution set S∗
cs of (35) is nonempty. Furthermore, for any strong β-Lagrangian
stationary point x∗ of (35), it is either a strictly local minimizer if
k0 = s or a global optimal
solution otherwise.

x∗

k

Proof. The nonemptiness of S∗
∪J∈Js Rn

J . Then the rest of the assertion follows from Theorem 2.5 and [28, Theorem 4.2].

cs follows from the Frank-Wolfe Theorem and the observation S =

With the above optimality results, we can apply LNA to solve (35) eﬃciently, since the linear
2s and the algorithm will have a fast quadratic

system in each iteration is of size no more than 2s
convergence rate, as stated in the following proposition.

×

Proposition 5.3. Suppose that Assumption 5.1 holds. For given β > 0, let x∗ be a strong β-
Lagrangian stationary point of (35) with y∗. Suppose that the initial point z0 of sequence
gen-
and ˜δ∗ is deﬁned as in (24). Then for
erated by LNA satisﬁes z0
any k

∈ NS(z∗, δ) where δ = min
{

˜δ∗, 1

zk

0,

}

{

}

≥

(i)

lim
k→∞

zk = z∗ with quadratic convergence rate, i.e.,

zk+1

k

z∗

−

k ≤

1
2 k

zk

z∗

2.

k

−

(ii) LNA terminates with accuracy ǫ when k

log2(cid:16)4δ2√k[A⊤A,−C⊤]k2+1/ǫ(cid:17)
2

.

(cid:25)

≥ (cid:24)

Proof. Since
L2 = 1/M ∗ with M ∗ deﬁned as in (24). Similarly, from (36), we also have (14) at any z
. By employing Theorem 4.1, we can obtain the desired assertions.
L1 =

2L(x, y) is constant and hence (15) holds everywhere for any L2 > 0. Thus, take
Rn+m with

[A⊤A,

C⊤]

∇

∈

k

−

k

12

5.2 Sparse High-Order Portfolio Selection

In real ﬁnancial markets, returns have often been found to be skewed and extreme events observed
to be frequent which can be measured by skewness and kurtosis. Thus, based on Markowitz’s mean-
variance (MV) portfolio model, several studies consider the high-order portfolio selection with only a
limited number of assets, i.e., the mean-variance-skewness-kurtosis model with cardinality constraint
Rn is the vector of portfolio
Rn is the return vector of n assets and x
(MVSKC). Suppose that ˜r
weights. The MVSKC model, which is ﬁrst introduced in [7], takes the form of

∈

∈

min
s.t.

−
e⊤x = 1,

λ1x⊤µ + λ2x⊤Σx
s.
x
k0 ≤

k

λ3x⊤Φ(x

−

⊗

x) + λ4x⊤Ψ(x

x

⊗

⊗

x),

(37)

Here µ = E(˜r) is the mean return vector, Σ = E(rr⊤) is the covariance matrix, Φ = E(r(r⊤
is the co-skewness, Ψ = E(r(r⊤
return vector and
⊗
moments of the portfolio return. Set

r⊤))
µ the centered
, 4, are parameters to balance the four

⊗
the Kronecker product, λi > 0, i = 1,

r⊤)) is the co-kurtosis matrix, with r := ˜r

r⊤

· · ·

⊗

−

⊗

f (x) =

−

λ1x⊤µ + λ2x⊤Σx

λ3x⊤Φ(x

−

⊗

x) + λ4x⊤Ψ(x

x

⊗

⊗

x), h(x) = e⊤x

1.

−

It is obvious that the objective function f is nonconvex and twice continuously diﬀerentiable. The
corresponding Lagrangian function associated with problem (37) is

L(x, y) = f (x)

y(e⊤x

1),

−

x

∀

∈

S, y

R.

∈

−

Utilizing Lemma 1 in [33], we have

h(x) = e⊤,
2h(x) = 0
∇
∇
∇xL(x, y) =
λ1µ + 2λ2Σx
−
−
2
2f (x) =
xxL(x, y) = 2λ2Σ
∇
∇
e
2L(x, y) =
−
0 (cid:21)

2f (x)
e⊤

∇

∇

(cid:20)

−
.

−

3λ3Φ(x

⊗
6λ3Φ(I

x) + 4λ4Ψ(x

⊗
x) + 12λ4Ψ(I

x

ye,

−
x),

x)

⊗

(38)

⊗
x

⊗

⊗






It is easy to verify that Assumption 1′ holds directly for any x

diﬀerentiable and
required Assumption 3.3 hold for problem (37), we introduce the following assumption.

) is continuously
) is constant, Assumption 3.5 automatically holds near x∗. To make the

2f (
·

2h(
·

Rn. Since

∇

∇

∈

Assumption 5.4. λi > 0, i = 1,
to the set

Rn : e⊤d = 0

d

.

{

∈

}

, 4, satisfy 4λ4(2λ2 −

· · ·

1) > λ2

3, and Σ is positive deﬁnite restricted

Learning from (38), for any d

d

Rn : e⊤d = 0

, we have

}

∈ {
xxL(x, y)d = d⊤(2λ2Σ
2
∇

∈

d⊤

6λ3Φ(I
x) + 12λ4Ψ(I
⊗
6λ3r⊤x + 12λ4(r⊤x)2))rr⊤)d

⊗

⊗

x

x))d

−
= d⊤E((2λ2 −
≥

dT E(rr⊤)d > 0,

where the second equality is from the deﬁnitions of Σ, Φ and Ψ, and the ﬁrst inequality is from
Assumption 5.4. Thus, Assumption 3.3 is valid.

Note that the condition in Assumption 5.4 is mild in real-world instances of sparse portfolio,
since the covariance matrix is always positive deﬁnite. Under Assumption 5.4, we have the following
optimality conditions for problem (37) by Theorem 2.4 and [28, Theorem 4.2].

13

Proposition 5.5. (i) Suppose that x∗ is a local minimizer of (5.4), then there exists a unique y∗
such that for any β
(6).

Rm
(0, ˆβ), x∗ is a strong β-Lagrangian stationary point, where ˆβ is deﬁned as in

∈

∈

(ii) Assume that Assumption 5.4 holds and x∗ is a strong β-Lagrangian stationary point of (37),

then x∗ is a strictly local minimizer of (37).

With the above optimality results, we can apply LNA to solve (37) eﬃciently. It is noteworthy that
the computational cost per iteration is super low since we just need to handle s + 1 linear equations
in each iteration, and the algorithm will have a fast quadratic convergence rate as stated below.

Proposition 5.6. Suppose that Assumption 5.4 holds. For given β > 0, let x∗ be a strong β-
Lagrangian stationary point of (37) with y∗. Then LNA for (37) has locally quadratic convergence
rate as stated in Theorem 4.1.

6 Numerical Experiments

This section reports numerical results of LNA in compressed sensing problem and sparse high-order
portfolio selection on both synthetic and real data. All experiments were conducted by using MATLAB
(R2018a) on a laptop of 8GB memory and Inter(R) Core(TM) i5 1.8Ghz CPU. We terminate our
10−6 where ηβ(xk, yk; T ) is deﬁned as (27) or k reaches 1000.
method at kth step if ηβ(xk, yk; Tk)

≤

6.1 Compressed Sensing

The aim of this subsection is to compare LNA with six state-of-the-art methods for compressed sensing
problem (35), including HTP [16]1, NIHT [6]2, GP [4]2, OMP [31, 32]2, CoSaMP [25]3 and SP [11]3.

6.1.1 Testing examples

We generate the sensing matrix

is normalized to
= 1 to make it consistent with the algorithms used in [4, 6, 16]. The true signal x∗ and the

in the same way as [35, 40]. Each column of

A

A

kAjk
measurement

are produced by the following pseudo MATLAB codes:

B

x∗ = zeros(n, 1), Γ = randperm(n), x∗(Γ(1 : s)) = randn(s, 1),

=

B

x∗.

A

Then, we randomly choose m =
0.1s
objective function. See the following pseudo MATLAB code for details:

as C in (35). The rest part of

rows of

A

⌈

⌉

composes A in the

A

J = randperm(p), J1 = J(1 : m), J2 = J(m + 1 : end);
A =

(J1), C =

(J2), d =

(J1), b =

(J2).

A

B

A

B

Example 6.1 (Gaussian matrix). Let
A ∈
being identically and independently generated from the standard normal distribution.

Rp×n be a random Gaussian matrix with each column

1HTP is available at: https:// github. com/ foucart/ HTP.
2NIHT, GP and OMP are available at

https:// www. southampton. ac. uk/ engineering/ about/ staff/
tb1m08. page$# $software. We use the version sparsify 0 5 in which NIHT, GP and OMP are called hard l0 Mterm,

greed gp and greed omp.
3CoSaMP and

SP are

available

at:

http:// media. aau. dk/ null_ space_ pursuits/ 2011/ 07/

a-few-corrections-to-cosamp-and-sp-matlab. html.

14

Example 6.2 (Partial DCT matrix). Let
(DCT) matrix generated by

A ∈

Rp×n be a random partial discrete cosine transform

Aij = cos(2π(j

−

1)ψi),

i = 1, . . . , p,

j = 1, . . . , n

where ψi (i = 1, . . . , m) is uniformly and independently sampled from [0, 1].

6.1.2 Numerical comparisons

We set the maximum number of iterations and the tolerance as 1000 and 10−6, respectively, in all of
the six comparison methods mentioned above. The initializations are set to be x0 = 0, y0 = 0 and
β = 5/n for LNA. For comparison purpose, HTP, NIHT, GP, OMP, CoSaMP and SP as tested in this
section are all initialized with the origin in their default setups.

We say a recovery of a method is successful if

, where x is the solution
produced by this method. The corresponding success rate is deﬁned as the percentage of the number
of successful recovery instances over all trials.

< 0.01

−

x

k

k

k

k

x∗

x∗

Firstly, we run 500 independent trials with ﬁxed n = 256, p =

at diﬀerent sparsity levels s
from 6 to 36. The corresponding success rates are illustrated in Fig. 1. One can observe that LNA
always yielded the highest success rate for each s under both Example 6.1 and Example 6.2, while a
lowest success rate is generated in GP. For example, when s = 20 for Gaussian matrix, 85% successful
recoveries are guaranteed in our method, which performed much better than other methods, whose
success rates are all less than 60%.

n/4

⌉

⌈

Next, we implement 500 independent trials by varying p =

when
n = 256, s =
in Fig. 2, which indicates that the larger m is, the easier the problem becomes
to be solved. Again, LNA outperformed the others in the success rate for each s, and GP still came
the last.

0.1, 0.12, . . . , 0.3

0.05n

in r

∈ {

rn

}

⌉

⌉

⌈

⌈

,

⌉

⌉

⌉

⌈

⌈

⌈

x

x∗

n/4

, s =

0.05n

0.01n

We now examine these algorithms with higher dimensions n between 5000 and 25000 with 50 trials
in the framework of Example 6.1, to compare their speed of
when p =
convergence and the accuracy of solutions. The average absolute error
and CPU time are
presented in Table 1 and Table 2, respectively. One can see that the highest accurate recovery can be
obtained in LNA with the least CPU time for most cases. Although OMP and HTP rendered solutions
as accurate as those by LNA when n is small, they presented some shortcomings by comparing to
LNA. In OMP, the accuracy cannot be guaranteed when n is large, and some inaccurate ones were
20000 particularly, which implies that OMP only worked well
produced when s =
when the solution is very sparse. On the other hand, the CPU time consumed by HTP is booming
over n. For example, when n = 25000 and s =
, 5.99 seconds by LNA against 159.38 seconds
0.05n
by HTP. Moreover, even though NIHT is the fastest one among the six methods, its accuracy is much
worse than others as it is stable at achieving the solutions with accuracy of order 10−7. That is to
say, the superiority of LNA becomes more obvious in the trade oﬀ of high accuracy and convergence
speed with high dimensional data.

and n

0.05n

−

≥

k

k

⌉

⌈

⌈

⌉

As stated in Theorem 4.1, LNA is a local method. Therefore, We conduct numerical experiments
with randomly generated initial points for CS problems to see how the initial points would aﬀect LNA.
To proceed, we apply LNA into solving Examples 6.1 and 6.2 with n = 10000, p =
.
⌉
We run the LNA under 50 diﬀerent initial points which are randomly generated from the uniform
distribution, namely, (x0, y0) ∼ U[0, 1]. The absolute error
, the number of iterations and
CPU time are plotted in Fig. 3, where the x-axis stands for the 50 initial points. One can see that all
the results stabilize at a certain level, which indicates that LNA is not sensitive to the choices of the
initial points for CS problems.

0.05n

, s =

n/4

x∗

−

x

k

k

⌈

⌈

⌉

15

1

0.8

0.6

0.4

0.2

0

1

0.8

0.6

0.4

0.2

e
t
a
R
s
s
e
c
c
u
S

e

t

a
R
s
s
e
c
c
u
S

(b) PartialDCT Matrix

HTP
NIHT
GP
OMP
CoSaMP
SP
LNA

(a) Gaussian Matrix

HTP
NIHT
GP
OMP
CoSaMP
SP
LNA

e
t
a
R
s
s
e
c
c
u
S

1

0.8

0.6

0.4

0.2

0

10

14

18

22

s

26

30

34

10

14

18

22

s

26

30

34

n/4

, s

⌉

⌈

∈ {

6, 8, . . . , 36

.

}

(b) PartialDCT Matrix

HTP
NIHT
GP
OMP
CoSaMP
SP
LNA

Figure 1: Success rates. n = 256, p =

(a) Gaussian Matrix

HTP
NIHT
GP
OMP
CoSaMP
SP
LNA

1

0.8

0.6

0.4

0.2

e

t

a
R
s
s
e
c
c
u
S

0
0.1

0.15

0.2
m/n

0.25

0.3

0
0.1

0.15

0.2
m/n

0.25

0.3

Figure 2: Success rates. n = 256, s =

0.05n

, p =

⌉

rn

⌈

⌉

⌈

with r

∈ {

0.1, 0.12, . . . , 0.3

.

}

10-14

(a) Gaussian matrix

r
o
r
r

E

2.9488

r
e
t
I

20

16

e
m
T

i

0.8772

10

20

30

40

50

10

20

30

40

50

10

20

30

40

50

10-14

(b) Partial DCT matrix

r
o
r
r

E

2.7347

r
e

t
I

13

11

e
m
T

i

0.6628

10

20

30

40

50

10

20

30

40

50

10

20

30

40

50

Figure 3: Eﬀects of initial points for Examples 6.1 and 6.2.

16

 
 
 
 
Table 1: Average absolute error

x

k

−

x∗

k

for Example 6.1.

s

n

LNA

HTP

NIHT

GP

OMP

CoSaMP SP

0.01n

⌈

⌉

0.05n

⌈

⌉

5000
10000
15000
20000
25000

5000
10000
15000
20000
25000

2.71e-15
4.86e-15
6.52e-15
8.87e-15
1.04e-14

1.14e-14
2.80e-14
3.91e-14
5.22e-14
6.30e-14

3.13e-15
5.70e-15
7.39e-15
9.97e-15
1.21e-14

1.11e-14
2.28e-14
3.70e-14
4.77e-14
6.12e-14

2.03e-8
2.27e-8
2.92e-8
4.37e-8
3.95e-8

1.63e-7
3.30e-7
3.06e-7
4.03e-7
3.75e-7

4.01e-15
7.04e-15
1.06e-14
1.37e-14
1.72e-14

1.49e-14
2.97e-14
5.02e-14
5.83e-14
7.74e-14

2.78e-15
4.80e-15
6.82e-15
9.34e-15
1.16e-14

1.08e-14
2.42e-14
4.31e-14
5.15e-04
6.30e-04

1.41e-14
2.15e-14
2.98e-14
4.08e-14
4.44e-14

4.10e-14
7.75e-14
1.10e-13
1.34e-13
1.82e-13

1.41e-14
2.15e-14
2.98e-14
4.08e-14
4.44e-14

4.10e-14
7.75e-14
1.10e-13
1.34e-13
1.82e-13

Table 2: Average CPU time (in seconds) for Example 6.1.

s

n

LNA HTP

NIHT GP

OMP

CoSaMP SP

0.01n

⌈

⌉

0.05n

⌈

⌉

5000
10000
15000
20000
25000

5000
10000
15000
20000
25000

0.06
0.25
0.60
1.08
1.77

0.14
0.59
1.50
3.03
5.99

0.62
4.02
13.07
32.08
111.94

0.80
5.71
18.90
49.10
159.38

0.21
0.83
1.93
3.50
6.11

0.62
2.42
5.47
11.24
17.49

1.92
14.07
46.88
110.07
230.27

2.18
15.07
50.41
118.35
239.70

0.30
2.28
7.70
18.16
37.43

1.73
13.66
46.82
111.40
174.21

0.57
0.28
1.01
2.13
4.11

1.44
13.87
47.41
127.37
217.07

0.04
0.18
0.78
1.34
2.57

0.93
5.36
18.23
55.21
96.38

17

6.2 Sparse High-Order Portfolio Selection

This subsection is devoted to comparing LNA with successive convex approximation algorithm (SCA)
[33] in sparse high-order portfolio selection problem (37) on real data sets.

6.2.1 Testing examples

Example 6.3. (Portfolio data sets) The data sets used in our experiments are selected from the
Standard and Poor’s 500 (USA) (S&P 500 for short)6. Firstly, we randomly select 100 socks from
S&P 500 Index components and randomly choose 500 continuous trading days from 2012-12-01 to
2018-12-01. Then the selected data is normalized to raise precision of the model, and the sample
moments7 are computed. To be fair enough, we adopt the choices of model parameters in (37) from [7]
with λ1 = 1, λ2 = ξ/2, λ3 = ξ(ξ + 1)/6, λ4 = ξ(ξ + 1)(ξ + 2)/24, where ξ = 5, 10 is the risk aversion
parameter. Direct calculations certify that these parameters satisfy the condition in Assumption 5.4.
Additionally, the sparsity level s for ξ = 5, 10 will be varying among
to generate a
total of 10 testing instances.

5, 10, 15, 20, 25

{

}

6.2.2 Numerical comparisons

For portfolio data sets in Example 6.3, we found that diﬀerent initial points lead to diﬀerent output
solutions. This is reasonable since the objective function is nonconvex and LNA is a locally conver-
gence method. Note that there are various ways to ﬁnd an initial point near to strong β-Lagrangian
stationary point, for instance, some ﬁrst-order gradient descent methods and convex relaxation meth-
ods. For simplicity, we initialize LNA with the origin (x0, y0) = (0, 0) which is appropriate to our
testing examples, and β = 1. For comparison purpose, SCA is called with the initial point x0 = 0,
10−3 for ξ = 5, 10 respectively. The sparsity
10−3, 3
along with other parameters as α = 0.2, ρ = 4
of a solution x generated by SCA will be recorded by ˆs := min
. Table 3
records s, ˆs, the objective function value (f-value) and CPU time when ξ = 5, 10 respectively.

x
|(i) ≥

t
i=1 |

x
k1}

0.99

t :

×

×

{

k

P

Table 3: ˆs, f-value and CPU time (in seconds) for Example 6.3.

ξ = 5

ξ = 10

s

5
10
15
20
25

ˆs

55
55
55
55
55

f-value

CPU time
LNA SCA LNA SCA
50.55
-1.11
45.48
-3.08
45.43
-2.81
47.06
-4.27
46.62
-4.35

15.55
39.53
26.80
23.74
34.30

-1.50
-1.50
-1.50
-1.50
-1.50

ˆs

88
88
88
88
88

f-value

CPU time
LNA SCA LNA SCA
-0.70
-1.55
-1.78
-2.80
-3.01

95.73
46.66
77.15
75.58
23.96

-1.44
-1.44
-1.44
-1.44
-1.44

144.92
216.47
186.79
194.80
117.98

As one can see from Table 3, LNA outperforms SCA in computational time for all testing instances,
and attains smaller f-value than that of SCA when s > 5. Speciﬁcally, LNA provides s-sparse solutions
while SCA fails. Furthermore, as s grows, f-value decreases in LNA, which indicates the trend to the
true minimum in some sense, against that almost no improvement of f-value in SCA.

6 http:// cran. r-project. org/ web/ packages/ portfolioBacktest/ vignettes
7 http:// www. mathworks. com/ matlabcentral/ fileexchange/ 47839-co_ moments-m.

18

7 Conclusion

In this paper, we have designed a second-order greedy algorithm named the Lagrange-Newton Al-
gorithm (LNA) for the sparse nonlinear programming (SNP) problem with sparsity and nonlinear
equality constraints, based on the strong β-Lagrangian stationarity and Lagrangian equations. The
resulting LNA has shown to be eﬀective, with local quadratic convergence rate and low iterative
complexity from the theoretical perspective, and good computational superiority from the numerical
perspective.

There are also some issues that remain to be further investigated. As LNA is a second-order local
method with heavy reliance on the initial points in general, the ﬁrst issue is whether a line search
scheme would be equipped for LNA, attempting to achieve global convergence and to accelerate the
algorithm. Another more general issue would be whether we can extend LNA to more general sparse
optimization models with equality and inequality constraints. We leave these in our future research.

Acknowledgement

We would like to thank AE and two referees for their valuable comments to improve our paper, and
Dr. Shenglong Zhou for his great support on the numerical experiments.

References

[1] Beck, A., Eldar, Y.C.: Sparsity constrained nonlinear optimization: Optimality conditions and

algorithms. SIAM Journal on Optimization 23(3), 1480–1509 (2013)

[2] Beck, A., Hallak, N.: On the minimization over sparse symmetric sets: projections, optimality

conditions, and algorithms. Mathematics of Operations Research 41(1), 196–223 (2015)

[3] Beck, A., Vaisbourd, Y.: The sparse principal component analysis problem: Optimality conditions
and algorithms. Journal of Optimization Theory and Applications 170(1), 119–143 (2016)

[4] Blumensath, T., Davies, M.E.: Gradient pursuits. IEEE Transactions on Signal Processing 56(6),

2370–2382 (2008)

[5] Blumensath, T., Davies, M.E.: Iterative hard thresholding for compressed sensing. Applied and

Computational Harmonic Analysis 27(3), 265–274 (2009)

[6] Blumensath, T., Davies, M.E.: Normalized iterative hard thresholding: Guaranteed stability and

performance. IEEE Journal of Selected Topics in Signal Processing 4(2), 298–309 (2010)

[7] Boudt, K., Lu, W., Peeters, B.: Higher order comoments of multifactor models and asset alloca-

tion. Finance Research Letters 13, 225 – 233 (2015)

[8] ˇCervinka, M., Kanzow, C., Schwartz, A.: Constraint qualiﬁcations and optimality conditions for
optimization problems with cardinality constraints. Mathematical Programming 160(1), 353–377
(2016)

[9] Chen, J., Gu, Q.: Fast Newton hard thresholding pursuit for sparsity constrained nonconvex
optimization. In: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, pp. 757–766 (2017)

19

[10] Chen, X., Ge, D., Wang, Z., Ye, Y.: Complexity of unconstrained l2 −

matical Programming 143(1-2), 371–383 (2014)

lp minimization. Mathe-

[11] Dai, W., Milenkovic, O.: Subspace pursuit for compressive sensing signal reconstruction. IEEE

transactions on Information Theory 55(5), 2230–2249 (2009)

[12] Donoho, D.L.: Compressed sensing. IEEE Transactions on Information Theory 52(4), 1289–1306

(2006)

[13] Elad, M.: Sparse and Redundant Representations. Springer, New York (2010)

[14] Elad, M., Figueiredo, M.A., Ma, Y.: On the role of sparse and redundant representations in

image processing. Proceedings of the IEEE 98(6), 972–982 (2010)

[15] Fan, J., Li, R.: Variable selection via nonconcave penalized likelihood and its Oracle properties.

Journal of the American Statistical Association 96(456), 1348–1360 (2001)

[16] Foucart, S.: Hard thresholding pursuit: an algorithm for compressive sensing. SIAM Journal on

Numerical Analysis 49(6), 2543–2563 (2011)

[17] Gao, J., Li, D.: Optimal cardinality constrained portfolio selection. Operations Research 61(3),

745–761 (2013)

[18] Gotoh, J.y., Takeda, A., Tono, K.: DC formulations and algorithms for sparse optimization

problems. Mathematical Programming 169(1), 141–176 (2018)

[19] Koh, K., Kim, S.J., Boyd, S.: An interior-point method for large-scale ℓ1-regularized logistic

regression. Journal of Machine Learning Research 8, 1519–1555 (2007)

[20] Kyrillidis, A., Becker, S., Cevher, V., Koch, C.: Sparse projections onto the simplex. In: Proceed-
ings of the 30th International Conference on Machine Learning, Atlanta, Georgia, USA, 2013,
vol. 28, pp. 235–243 (2013)

[21] Lu, Z.: Optimization over sparse symmetric sets via a nonmonotone projected gradient method.

arXiv preprint arXiv:1509.08581 (2015)

[22] Lu, Z., Zhang, Y.: Sparse approximation via penalty decomposition methods. SIAM Journal on

Optimization 23(4), 2448–2478 (2013)

[23] Misra, J.: Interactive exploration of microarray gene expression patterns in a reduced dimensional

space. Genome Research 12(7), 1112–1120 (2002)

[24] Natarajan, B.K.: Sparse approximate solutions to linear systems. SIAM Journal on Computing

24(2), 227–234 (1995)

[25] Needell, D., Tropp, J.A.: CoSaMP: Iterative signal recovery from incomplete and inaccurate

samples. Applied and Computational Harmonic Analysis 26(3), 301–321 (2009)

[26] Negahban, S.N., Ravikumar, P., Wainwright, M.J., Yu, B., et al.: A uniﬁed framework for high-
dimensional analysis of m-estimators with decomposable regularizers. Statistical Science 27(4),
538–557 (2012)

[27] Pan, L., Luo, Z., Xiu, N.: Restricted Robinson constraint qualiﬁcation and optimality for
cardinality-constrained cone programming. Journal of Optimization Theory and Applications
175(1), 104–118 (2017)

20

[28] Pan, L., Xiu, N., Fan, J.: Optimality conditions for sparse nonlinear programming. Science China

Mathematics 60(5), 759–776 (2017)

[29] Pan, L., Xiu, N., Zhou, S.: On solutions of sparsity constrained optimization. Journal of the

Operations Research Society of China 3(4), 421–439 (2015)

[30] Pan, L., Zhou, S., Xiu, N., Qi, H.D.: Convergent iterative hard thresholding for sparsity and
nonnegativity constrained optimization. Paciﬁc Journal of Optimization 13(2), 325–353 (2017)

[31] Pati, Y.C., Rezaiifar, R., Krishnaprasad, P.S.: Orthogonal matching pursuit: Recursive function
In: Proceedings of 27th Asilomar

approximation with applications to wavelet decomposition.
conference on signals, systems and computers, IEEE, pp. 40–44 (1993)

[32] Tropp, J.A., Gilbert, A.C.: Signal recovery from random measurements via orthogonal matching

pursuit. IEEE Transactions on Information Theory 53(12), 4655–4666 (2007)

[33] Wang, J., Deng, Z., Zheng, T., So, A.M.C.: Sparse high-order portfolios via proximal dca and

sca. arXiv preprint arXiv:2008.12953 (2020)

[34] Xu, F., Lu, Z., Xu, Z.: An eﬃcient optimization approach for a cardinality-constrained index

tracking problem. Optimization Methods and Software 31, 258–271 (2016)

[35] Yin, P., Lou, Y., He, Q., Xin, J.: Minimization of ℓ1−2 for compressed sensing. SIAM Journal

on Scientiﬁc Computing 37(1), 536–563 (2015)

[36] Yuan, X., Li, P., Zhang, T.: Gradient hard thresholding pursuit. Journal of Machine Learning

Research 18(166), 1–43 (2018)

[37] Yuan, X., Liu, Q.: Newton greedy pursuit: A quadratic approximation method for sparsity-
constrained optimization. In: Proceedings of IEEE Conference on Computer Vision and Pattern
Recognition, pp. 4122–4129 (2014)

[38] Yuan, X., Liu, Q.: Newton-type greedy selection methods for ℓ0-constrained minimization. IEEE

Transactions on Pattern Analysis and Machine Intelligence 39(12), 2437–2450 (2017)

[39] Zhou, S., Xiu, N., Qi, H.: Global and quadratic convergence of Newton hard-thresholding pursuit.

Journal of Machine Learning Research (2021)

[40] Zhou, S., Xiu, N., Wang, Y., Kong, L., Qi, H.D.: A null-space-based weighted ℓ1 minimization
approach to compressed sensing. Information and Inference: A Journal of the IMA 5(1), 76–102
(2016)

[41] Zhou, T., Tao, D., Wu, X.: Manifold elastic net: a uniﬁed framework for sparse dimension

reduction. Data Mining and Knowledge Discovery 22(3), 340–371 (2011)

[42] Zou, H., Hastie, T., Tibshirani, R.: Sparse principal component analysis. Journal of Computa-

tional & Graphical Statistics 15(2), 265–286 (2006)

21

