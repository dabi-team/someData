MQDD: Pre-training of Multimodal Question Duplicity Detection for
Software Engineering Domain

Jan Pašek, Jakub Sido, Miloslav Konopík, Ondˇrej Pražák
{pasekj,sidoj,konopik,ondfa}@kiv.zcu.cz

NTIS – New Technologies for the Information Society,
Department of Computer Science and Engineering,
Faculty of Applied Sciences, University of West Bohemia, Plzeˇn, Czech Republic

Abstract

This work proposes a new pipeline for leverag-
ing data collected on the Stack Overﬂow web-
site for pre-training a multimodal model for
searching duplicates on question answering
websites. Our multimodal model is trained on
question descriptions and source codes in mul-
tiple programming languages. We design two
new learning objectives to improve duplicate
detection capabilities. The result of this work
is a mature, ﬁne-tuned Multimodal Question
Duplicity Detection (MQDD) model, ready to
be integrated into a Stack Overﬂow search sys-
tem, where it can help users ﬁnd answers for
already answered questions. Alongside the
MQDD model, we release two datasets re-
lated to the software engineering domain. The
ﬁrst Stack Overﬂow Dataset (SOD) represents
a massive corpus of paired questions and an-
swers. The second Stack Overﬂow Duplicity
Dataset (SODD) contains data for training du-
plicate detection models.

1

Introduction

The beneﬁts of Question-Answer (QA) networks
for software developers such as the Stack Overﬂow
website are widely exploited by professionals and
beginners alike during the software creation pro-
cess. Many solutions to various problems, short tu-
torials, and other helpful tips can be found on these
networks. However, access to this valuable source
of information highly depends on users’ ability to
In our paper, we intro-
search for the answers.
duce a multimodal method for detecting duplicate
questions. Apart from the primary use to prevent
posting duplicate questions, this technique can be
directly used for better search. When users are
posting an already answered question, they can get
the answer immediately without the necessity to
wait until someone else links the duplicate post or
answers their question.

The duplicate question detection task aims to
classify whether two questions share the same in-

tent. In other words, if two questions are duplicates,
they relate to the same answer. The duplicate de-
tection task is quite challenging since the classiﬁer
needs to distinguish tiny semantic nuances that can
signiﬁcantly change the desired answer.

The posts in the QA networks for software de-
velopment mix natural language and source code
snippets. The great success of neural networks for
Natural Language Processing (NLP) encourages us
to employ these methods for our task where natural
language is intermixed with source codes. We sup-
port our decision with the ﬁndings of Hindle et al.
(2012), who show that the source codes carry even
less entropy than the English language.

Current state-of-the-art NLP methods build on
large pre-trained models based on the Trans-
former architecture (Vaswani et al., 2017). The
Transformer-based models such as BERT (Devlin
et al., 2018), GPT (Brown et al., 2020), RoBERTa
(Liu et al., 2019), or T5 (Raffel et al., 2019) are
usually pre-trained on massive unlabeled corpora
and applied to a task with much less training data
afterward. We follow this idea and introduce the
pre-training phase into our solution.

In our work, we try to leverage the transfer learn-
ing techniques to build a duplicate question de-
tection (Wang et al., 2020) model for question-
answering platforms such as Stack Overﬂow1 or
Quora2. Since both platforms are extensively used
to ask programming-related questions, we aim
to pre-train a multimodal encoder for a program-
ming language (PL) and natural language (NL).
To achieve the best possible results, we design
duplicate-detection-speciﬁc pre-training objectives
(see Section 3.3).

Since the source code snippets present in the
Stack Overﬂow questions may be relatively long,
we choose to base our model on the Longformer
architecture (Beltagy et al., 2020); whose modiﬁed

1https://stackoverflow.com
2https://quora.com/

2
2
0
2

r
a

M
9
2

]
L
C
.
s
c
[

2
v
3
9
0
4
1
.
3
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
attention scheme scales linearly with the sequence
length. The resulting model with ≈146M parame-
ters is ﬁrstly pre-trained on a large semi-supervised
corpus of Stack Overﬂow questions and answers.
For detailed information about the dataset and pre-
training, see Section 3.

Afterward, in Section 4, we ﬁne-tune the ob-
tained model on the duplicate detection task and
compare our model with CodeBERT (Feng et al.,
2020), which represents another NL-PL multi-
modal encoder. We also compare our model to
a randomly initialized Longformer (Beltagy et al.,
2020) and pre-trained RoBERTa (Liu et al., 2019)
to see whether the pre-training of both models
brings a signiﬁcant improvement of the achieved
results. The previously described experiments are
visualized in Figure 1. At the end of this paper, we
explore how well our model generalizes to other
tasks by applying our model to the CodeSearchNet
dataset (Husain et al., 2019) in Section 5.

Our main contributions are: 1) We release a ﬁne-
tuned Multimodal Question Duplicity Detection
(MQDD) model for duplicate question detection.
The model is mature enough to be deployed to
Stack Overﬂow, where it can automatically link
duplicate questions and, therefore, improve users’
ability to search for desired answers. Furthermore,
we release the pre-trained version of the encoder,
so other researchers may reuse the most computa-
tionally intensive phase of our model training. 2)
We present and explore the effect of entirely new
pre-training objectives specially designed for du-
plicate detection. 3) We release a Stack Overﬂow
Dataset (SOD) that can be used for pre-training
models in a software engineering domain. Further-
more, we release a novel Stack Overﬂow Duplicity
Dataset (SODD) for duplicate question detection,
enabling other researchers to follow up on our work
seamlessly.

2 Related Work

Deep-learning models have become more common
in many new domains in recent years. The main
advantage lies in processing a large volume of doc-
uments and beneﬁting from them. Software devel-
opment is not an exception. The naturally collected
massive amount of data in software management
systems, issue tracker tools, and versioning systems
makes the software development an ideal domain to
apply deep models to increase work effectiveness.
Developers from Microsoft have introduced a

system called DeepCodeReviewer, which recom-
mends the proper review from a repository of his-
torical peer reviews from their database (Gupta and
Sundaresan, 2018). Ullah et al. (2019) have out-
performed the existing techniques in source code
authorship attribution using deep learning meth-
ods to express coding styles’ features for each pro-
grammer and attribute the actual authors. Tufano
et al. (2018) use learned vector embedding to cre-
ate vector representations of source codes to detect
similarities in code. Yin and Neubig (2017); Balog
et al. (2017) utilize RNN for source code generation
from natural language. Authors of Commit2Vec
(Cabrera Lozoya et al., 2021) use analysis of code
structure before and after repository commit to re-
veal a change in the security of the source code.
Liu et al. (2020) generate commit messages from
code changes. Deng et al. (2020) experiment with
SQL from natural language generation. Another
representative of source code-oriented models is
the Codex (Chen et al., 2021), a massive pre-trained
model designated for source code generation. Its
slightly modiﬁed form is also integrated with the
GitHub Copilot3 system, a digital pair program-
mer. Wang et al. (2021) introduces a generative
model called CodeT5, capable of solving multiple
tasks thanks to converting all problems into a uni-
ﬁed sequence-to-sequence form. Sun et al. (2022)
translate source codes into a natural language to
retrieve similar code snippets.

In NLP, many recent successful models follow
the BERT (Devlin et al., 2018) architecture and
use the Transformer encoder (Vaswani et al., 2017)
to produce contextual representations of input to-
kens. These contextual embeddings (Peters et al.,
2018; McCann et al., 2017) can then be used for
various tasks, including the classiﬁcation of entire
sequences (Reimers and Gurevych, 2019) or indi-
vidual tokens (Liu et al., 2021; Sun et al., 2019b).
Such success can probably be attributed to a well-
used attention mechanism (Bahdanau et al., 2014),
which allows the model to capture contextual infor-
mation from the entire sequence being processed.
In deep learning, the correct choice of the train-
ing objective signiﬁcantly impacts the accuracy of
a ﬁnal system. Adapting the pre-training phase and
ﬁnding a proper objective allows the model to ex-
ploit useful features from large, naturally labeled
data enabling future gains on a downstream task.
An example of a model that tries to improve the

3https://copilot.github.com

Figure 1: A visualization of the pipeline of our experiments. The upper part of the ﬁgure shows the construction
of our SOD and SODD datasets and their usage for pre-training and ﬁne-tuning our MQDD model. The lower part
of the ﬁgure visualizes the pre-training of the CodeBERT done by Feng et al. (2020).

pre-training objectives is ELECTRA (Clark et al.,
2020), which employs a generator-discriminator
setup. RoBERTa (Liu et al., 2019) slightly modiﬁes
the Masked Language Modeling (MLM) objective
and abandons the Next Sentence Prediction (NSP).
Further work then tries to modify the used neural
network architecture and thus target various prob-
lems. For example, Longformer (Beltagy et al.,
2020) and Reformer (Kitaev et al., 2020) models
signiﬁcantly modify the attention mechanism to
mitigate the O(N 2) complexity of a vanilla atten-
tion.

With increasing computational capability and
model complexity, we can observe more multilin-
gual pre-training goals to achieve better results
– mBERT (Devlin et al., 2018) and SlavicBERT
(Arkhipov et al., 2019) create a joint model for
representing several different languages. Unlike
multilingual models, multimodal models process
inputs from entirely different domains. For ex-
ample, VilBERT (Lu et al., 2019) and VideoBERT
(Sun et al., 2019a) process inputs consisting of both
the natural language and video.

Another possible usage of multimodal encoders
is to process inputs consisting of source codes and
natural texts. Such models can produce contextual

embeddings of source codes (Chen and Monperrus,
2019) directly applicable to downstream tasks such
as programming language detection, code similar-
ity, code search, or code ﬁxing (Le et al., 2020).
Furthermore, such a multimodal encoder can be
incorporated in a Seq2Seq setup, in which it can,
for example, generate source code or its documen-
tation.

The CuBERT (Kanade et al., 2020a) is an ex-
ample of a multimodal encoder for Python source
codes and texts. It is pre-trained on 7.4M source
ﬁles. The model outperforms BiLSTM (Schus-
ter and Paliwal, 1997; Kanade et al., 2020b) and
randomly initialized Transformer (Vaswani et al.,
2017) approach in ﬁve different tasks, including
classiﬁcation of variable misuse, wrong binary
operator usage, swapped operands, and function-
docstring match. Another representative of mul-
timodal source code encoders is the CodeBERT
model (Feng et al., 2020) pre-trained on a multi-
lingual corpus of source codes from six different
programming languages. The CodeBERT builds
upon the RoBERTa (Liu et al., 2019) and follows
the generator-discriminator approach laid out in
ELECTRA (Clark et al., 2020). Besides the re-
placed token detection (RTD) learning objective

SODStack OverflowPre-trainingMQDD (pre-trained)Longformer(random)MQDD (duplicates)Longformer (duplicates)RoBERTaCodeSearchNetPre-trainingCodeBERT (pre-trained)CodeBERT (duplicates)RoBERTa (duplicates)Duplicate Detection Fine-tuningSODDGitHubfrom ELECTRA, the authors employ the tradi-
tional MLM objective. The resulting model shows
superior results in code search, natural language-
programming language (NL-PL) probing, and doc-
umentation generation.

Our work differs from the previous multimodal
source code encoders in the following points: 1)
Our model is trained using novel pre-training ob-
jectives targeting speciﬁcally the duplicate detec-
tion task. 2) Unlike the CuBERT, explicitly desig-
nated for Python and CodeBERT, pre-trained on
six different programming languages, our model is
capable of processing inputs from an arbitrary pro-
gramming language. Processing every possible pro-
gramming language is crucial for deploying such a
model to real-world question-answering platforms.
3) Our MQDD model employs a Transformer-
based architecture with an attention scheme scaling
linearly with sequence length. This enables the
model to be integrated into systems that require
processing long sequences in a reasonable time.

3 Model Pre-training

This section describes the pre-training procedure,
including the construction of the new dataset from
the Stack Overﬂow, the deﬁnition of the learning
objectives, and the model itself.

3.1 Stack Overﬂow Dataset

For the pre-training, we construct our Stack Over-
ﬂow Dataset (SOD), created from the Stack Over-
ﬂow data dump4. The original data source5 con-
tains around 45M of posts (questions + answers)
exported in an XML format. The questions repre-
sent approximately 17.7M thereof. To construct the
dataset, we take all question-answer pairs, extract
the textual and source code parts and apply dif-
ferent pre-processing on both (for pre-processing
details, see appendix A). A result of the pre-
processing procedure are tuples (Qt, Qc, At, Ac)
containing pre-processed texts (t) and codes (c)
from both the questions (Q) and answers (A).

Afterward, we construct the training set by tak-
ing 2-combinations of the pre-processed tuples, re-
sulting in 6 different input pair types described in
Section 3.3. The acquired input pairs (x1, x2) are
further processed in batches of 100 examples. For

4Available at: https://archive.org/download/

stackexchange.

Order
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
-

Tag
javascript
java
c#
php
python
html
css
c++
sql
c
objective-c
r
ruby
swift
bash
total

Percentage
10,95
9,88
8,04
7,95
6,32
6,18
4,28
4,15
3,42
2,29
1,93
1,36
1,32
1,05
0,8
69,92

Table 1: The table presents a tag-based analysis of the
percentage of individual programming languages in the
SOD dataset. The table shows the 15 most frequent pro-
gramming languages included in the dataset. Together
they form ≈70% of all the examples. The remaining
30% are then made up of less popular programming
languages or speciﬁc technologies.

each pair in the batch, we sample one negative ex-
ample by choosing a random text or code xr from
the batch buffer and use it as a replacement for the
second element in the pair. This results in adding
pair (x1, xr) to the training set.

Subsequently, we tokenize the input pairs and
store them in TFRecords6 to speed up data pro-
cessing during the training. The resulting dataset
contains 218.5M examples and can be downloaded
from our GitHub repository https://github.
com/kiv-air/StackOverflowDataset. A de-
tailed description of the dataset’s structure is pro-
vided in appendix D. Furthermore, Tables 1 and
2 present a detailed analysis of the programming
languages included in the corpus and dataset size,
respectively.

3.2 Tokenization

Before extracting the input pairs, we employ the
(Qt, Qc, At, Ac) tuples to train a joint tokenizer for
both the source codes and English texts. We use
the Word Piece tokenizer (Schuster and Nakajima,
2012), whose vocabulary size is typically set to
a value between 10K-100K subword tokens. In

5Data dump was downloaded in June 2020. Therefore, all

6https://www.tensorflow.org/tutorials/

the stated information is valid to this date.

load_data/tfrecord

Statistic
average # of characters
average # of tokens
average # of words
# of characters
# of tokens
# of words

QC
846
298
83

QT
519
130
89

AC
396
140
44

AT
369
92
60

Total
-
-
-

16.1B 13.5B 6.6B 9.6B 45.8B
3.4B 2.3B 2.4B 13.8B
5.7B
6.2B
2.3B 0.7B 1.6B
1.6B

Table 2: Detailed statistics of the released Stack Overﬂow Dataset (SOD). The table shows the average number of
characters, tokens, and words in different source codes present in questions (QC) or answers (AC) and texts present
in questions (QT) or answers (AT). Besides the average statistics, the table provides a total count of tokens, words,
or characters. To calculate the statistics related to token counts, we utilized the tokenizer presented in Section 3.2,
whereas we employed a simple space tokenization for the word statistics.

our work, we set the vocabulary size to 50K sub-
word tokens, which is large enough to encompass
both the textual and code tokens while preserving
a reasonable size of the embedding layer. When
constructing the dataset, we ignore all tokens that
occur less than ﬁve times in the dataset.

3.3 Pre-training Objectives

Similarly to BERT (Devlin et al., 2018), we employ
a Masked Language Modeling (MLM) task during
the pre-training phase. The MLM objective aims to
reconstruct original tokens from intentionally mod-
iﬁed input sequences. The modiﬁcation replaces
randomly selected tokens with a special [MASK]
token or any other token from the dictionary.

Besides the MLM, we introduce two Stack Over-
ﬂow dataset-speciﬁc tasks dealing with multimodal
data. The ﬁrst task is called Question-Answer (QA),
and it aims is to classify whether the input pair orig-
inates from a question-answer relationship. The
individual elements of the input pair can be either
a natural language text or a programming language
snippet. Therefore, we work with the following
input pair types:

• Question text - Answer code (Qt-Ac)

• Question code - Answer code (Qc-Ac)

• Question text - Answer code (Qt-Ac)

• Question code - Answer text (Qc-At)

The second Stack Overﬂow-related task is called
Same Post (SP). Similarly to the QA task, the SP
works with input pairs of natural language and
source code snippets. However, unlike the QA task,
SP classiﬁes whether the elements of the input pair
come from the same post (a post represents either
a question or an answer). The resulting possible
input pair types are the following:

• Answer Text - Answer Code (At-Ac)

• Question Text - Question Code (Qt-Qc)

We designed these learning objectives speciﬁ-
cally to achieve the best possible result on our tar-
get task - duplicate detection (Section 4). We pre-
sume that employing these tasks requiring a deep
understanding of the multimodal input helps us out-
perform similar models such as CodeBERT (Feng
et al., 2020). Furthermore, our learning objectives
require comparing and matching the semantics of
both the textual input and the source code that can
be leveraged on downstream tasks such as code
search (Heyman and Cutsem, 2020; Sachdev et al.,
2018; Arwan et al., 2015).

3.4 Model Description

We choose to employ the architecture of the Long-
former model (Beltagy et al., 2020) for its atten-
tion mechanism that scales linearly with the input
sequence length. This addresses the fact that the
processed input sequences (mainly the source code)
may contain several hundreds of tokens. Process-
ing such long sequences with the vanilla attention
mechanism used in the Transformer (Vaswani et al.,
2017) can be computationally exhausting.

We use the Hugging Face’s Transformers (Wolf
et al., 2020) model with approximately 146M pa-
rameters (for more details on the model, see ap-
pendix B).

On top of the base model, we build two dif-
ferent classiﬁcation heads. The ﬁrst head, deal-
ing with the MLM task, takes the input tokens’
contextual embeddings as its input. It means the
MLM head works with the matrix E ∈ RN ×H ,
where H is the hidden size and N is the length
of the input sequence. MLM prediction is ob-
tained by passing the matrix through a linear
layer so that MLMoutput = E × Wmlm, where

Wmlm ∈ RH×|V |, and |V | represents the size of
the vocabulary.
In other words, the model pro-
duces a probability distribution over the vocabulary
for each of the input tokens, including the masked
ones. To optimize the weights, we further calculate
a cross-entropy loss over the network’s prediction.

The second head classiﬁes whether an input pair
represents a question-answer pair and whether
both inputs originate from the same post. To
achieve this, the head takes the contextual embed-
ding of the special [CLS] token ([CLS] ∈ RH )7.
The vector is then transformed using a linear
layer with ReLu (Nair and Hinton, 2010) used as
an activation function - QA_SPintermediate =
relu([CLS]×Wqa_sp1), where Wqa_sp1 ∈ RH×D
and D represents a dimensionality of the interme-
diate layer. In the end, the Question-Answer/Same
Paragraph (QA/SP) head output is obtained us-
ing another linear layer - QA_SPoutput =
QA_SPintermediate × Wqa_sp2, where Wqa_sp2 ∈
RD×2. Put differently, the QA/SP head is a multi-
label classiﬁer with two output neurons. The ﬁrst
one represents a probability of the input pair origi-
nating from the same post. The second one repre-
sents the probability of the input pair originating
from the question-answer relationship. To optimize
the weights with respect to our QA/SP objectives,
we compute a binary cross-entropy loss over the
two output neurons.

3.5 Pre-training Procedure

We optimize our model using Adam optimizer
(Kingma and Ba, 2014) with a learning rate of
1e−5 while employing both linear warmup and lin-
ear decay to zero. The linear warmup is conﬁgured
to reach the target learning rate in 45K batches.
The pre-training is carried out on two Nvidia A100
GPUs and two AMD EPYC 7662 CPU cores with
a batch size of 64 examples.

We perform a single iteration over the whole
dataset (≈ 220M examples) with such a conﬁgu-
ration while trimming the sequences to a sequence
length of 256 tokens. Afterward, we set the se-
quence length to 1024 tokens and train the model
for additional 10M examples, enabling us to train
positional embeddings for longer sequences.

7The [CLS] token is an artiﬁcial token added at the begging

4 Duplicate Question Detection

Following the pre-training phase, this section fo-
cuses on applying the obtained model to the task
of duplicate detection. In the ﬁrst part, we describe
the construction of a new dataset for duplicate de-
tection. The next part presents how we integrate
the pre-trained model into a two-tower neural net-
work. At the end of this section, we describe the
concluded experiments and present the results.

4.1 Stack Overﬂow Duplicity Dataset

Similarly to the pre-training phase, we employ the
Stack Overﬂow data dump to assemble the Stack
Overﬂow Duplicity Dataset (SODD). The data
contain approximately 491K pairs of questions
marked to be duplicated by the page’s users. To
replenish the dataset with negative samples, we
employ randomly chosen questions and similar
questions retrieved using ElasticSearch8. More
speciﬁcally, we sample three random questions and
retrieve six similar questions for each duplicate
pair. The similarities are retrieved either based
on a full-text similarity of the question’s body
or associated tags. However, each question can
be included in the dataset at most once. The
resulting dataset consists of approximately 1.4M
(x1, x2, y),
examples
represented by triplets
where x1 and x2 represent the questions and y ∈
{duplicate, text_similar, tag_similar, different}
represents the label. Although the dataset differen-
tiates between different and similar questions, all
of our experiments treat both of these classes the
same. In other words, our experiments perform a
binary classiﬁcation task. For more information
about the dataset size, see Table 3.

Type

Train

Dev Test Total

Different
Similar
Duplicates
Total

64K 32K 646K
550K
62K 30K 618K
526K
191K
22K 11K 224K
1.2M 148K 73K 1.4M

Table 3: Stack Overﬂow Duplicity Dataset (SODD)
size summary.

The question pairs acquired from the Stack Over-
ﬂow are stored in the HTML format. Therefore,
we employ a BeautifulSoup9 library to re-

8https://www.elastic.co
9https://beautiful-soup-4.readthedocs.

for sequence classiﬁcation tasks.

io/en/latest/

move unwanted HTML markup and separate nor-
mal text from source code snippets. Besides,
we pre-process the source code stripping all in-
line comments and newline characters. More-
over, we substitute the ﬂoating-point and integer
numbers with placeholder tokens. Similarly to
the source codes, we replace numbers and date/-
time information with placeholder tokens and re-
move newlines and punctuation in the textual part
of the dataset. The resulting dataset can be ob-
tained from our repository https://github.com/
kiv-air/StackOverflowDataset. For a detailed
description of the dataset structure, see appendix
E.

4.2 Model

We employ a variant of a two-tower neural network
to adapt our pre-trained model to the duplicate de-
tection task. Our setup (Figure 2) encodes both
questions separately using the same pre-trained en-
coder, obtaining representations of the questions
(xe1, xe2 ∈ Rd). The representations are then con-
catenated (xe = [xe1; xe2]) and transformed using
a linear layer with ReLu activation (Nair and Hin-
ton, 2010), as stated in equation 1.

xL = max(0, xeWL + bL)

(1)

xH = sof tmax(xLWH + bH )

(2)

At the top of our duplicate detection model, there
is a classiﬁcation head consisting of a linear layer
with two neurons, whose activation is further trans-
formed using a softmax function, (Bridle, 1990) as
shown in equation 2.

An alternative approach would be to jointly pass
both questions into the encoder and build a classiﬁ-
cation head at the top. However, our architecture of
the two-tower model allows the representations of
the whole corpus to be pre-computed and indexed
in a fast vector space search library such as Faiss10
(Johnson et al., 2019) (see the future work in Sec-
tion 7). Thanks to that, it is possible to compute
only the representation of the newly posted ques-
tion and run a quick search inside the vector space.
This is much faster than running the model for each
pair of questions composed of a new question and
the others in the corpus.

10https://github.com/facebookresearch/

faiss

Figure 2: The neural network model architecture used
for duplicate question detection. The encoder blocks
in the ﬁgure share the same weights and represent ei-
ther an MQDD, CodeBERT (Feng et al., 2020), or
RoBERTa (Liu et al., 2019).

4.3 Experimental Setup - Duplicate Detection

Similarly to the pre-training phase, we use the
Adam optimizer (Kingma and Ba, 2014) with a
learning rate set to 6.35e−6 to train the model
on a computation node with two cores of AMD
EPYC 7662 CPU and two Nvidia A100 GPUs. In
each experiment, we train the model for 24 hours
with a batch size of 96 examples and observe the
progress of cross-entropy loss, accuracy, and F1
score. The hyperparameters were set based on 30
hyperparameter-search experiments conducted us-
ing the Weights & Biases (Biewald, 2020) sweeps
service11. For detailed information about the hy-
perparameter setting, refer to appendix C.

To evaluate the effectiveness of our pre-training
objectives, we compare our model with the Code-
BERT (Feng et al., 2020), RoBERTa (Liu et al.,
2019), and randomly initialized Longformer (Belt-
agy et al., 2020). The comparison experiments also
utilize the architecture depicted in Figure 2, where
we only replace the encoder with the model being
compared. The training setup for the comparison
experiment is identical to the setup described above.
It means that we ﬁne-tune the models for 24 hours
on the same hardware.

4.4 Results

As evaluation metrics, we use an F1 score and
accuracy. We summarize the results of our exper-
iments in Table 4, where the achieved results are
stated with 95% conﬁdence intervals. From the

11https://docs.wandb.ai/guides/sweeps

<CLS>text1<SEP>code1<SEP><CLS>text2<SEP>code2<SEP>contatenationlinear + relulinear + softmaxencoderencoderresults, we can see that our model signiﬁcantly out-
performed all alternative approaches. For further
discussion on the results, see Section 6.

Model
MQDD
CodeBERT
RoBERTa
Longformer†

Accuracy
74.83 ± 0.10
70.44 ± 0.12
70.16 ± 0.19
67.31 ± 0.12

F1 Score
75.10 ± 0.10
70.70 ± 0.13
70.51 ± 0.22
67.71 ± 0.19

Table 4: Summary of duplicate detection experiment re-
sults stated with 95% conﬁdence intervals. The † sign
marks randomly initialized models. As we expected,
our model outperformed the other tested approaches.
For a broader discussion of the results and an inter-
esting observation that the CodeBERT does not bring
a statistically signiﬁcant improvement over the basic
RoBERTa, see Section 6.

(Liu et al., 2019), and randomly initialized Long-
former (Beltagy et al., 2020).

For all of the experiments, we employ the
AutoModelForSequenceClassification
class from the Hugging Face’s Transformers (Wolf
et al., 2020) library. The models for sequence
classiﬁcation come with an in-build classiﬁcation
head that operates over the pooled output of the
base model.

Similarly to the duplicate detection experiments,
we perform the ﬁne-tuning on two NVidia A100
GPUs for 24 hours with a batch size of 64 exam-
ples. For optimization, we also employ the Adam
(Kingma and Ba, 2014) optimizer with a learning
rate of 1e−5. Furthermore, we utilize learning
rate warmup during the ﬁrst 256 batches and apply
linear learning rate decay to zero.

5 Generalization to Other Tasks

5.3 Results

To explore how well our model generalizes to other
tasks, we choose the code search task. The in-
formation retrieval seems to be close to our pre-
training tasks. Therefore, we conjecture that the
acquired results can be promising. For all the exper-
iments, we use the CodeSearchNet dataset (Husain
et al., 2019) containing approximately 2.3M ex-
amples from six different programming languages
extracted from GitHub repositories.

5.1 Domain-Speciﬁc Pre-Training

Since our model is pre-trained on Stack Overﬂow
data signiﬁcantly different from the CodeSearch-
Net extracted from GitHub, we employ a domain-
speciﬁc pre-training to adapt our model to the target
domain.

We employ the masked language modeling
(MLM) learning objective for the domain-speciﬁc
pre-training. We perform 20 iterations over the
CodeSearchNet dataset following the same experi-
mental setup as described in Section 3.5.

5.2 Experimental Setup – Code Search

To ﬁne-tune our model on the CodeSearchNet
dataset (Husain et al., 2019), we utilize its pre-
processed version from the authors of CodeBERT
(Feng et al., 2020) since it comes with negative
examples, unlike the original dataset distribution.
In our experiments, we train a separate model for
each of the six available programming languages
and compare our results with the results obtained
using the CodeBERT (Feng et al., 2020), RoBERTa

In the case of the code search task, we use the
F1 score metric to evaluate the achieved results.
The complete summary of the results with 95%
conﬁdence intervals can be found in Table 5. The
results show that both the CodeBERT (Feng et al.,
2020) and RoBERTa (Liu et al., 2019) signiﬁcantly
outperform our model in the code search task.

6 Discussion

As the results stated in Sections 4.4 and 5.3 sug-
gest, our model excels in detecting duplicates but
lags in source code retrieval. We expected the dom-
inance of our model in the duplication detection
task. However, an interesting observation is that
the pre-training of the CodeBERT initialized us-
ing the RoBERTa’s weights does not bring any
improvement when applied to the duplicate detec-
tion. On the other hand, it is very surprising that
our MQDD model does not perform comparably
well as the CodeBERT on the code search since our
pre-training objectives require the model to build a
deep understanding of the processed source code.
This can be explained by the fact that the datasets
used for pre-training of both models have very dif-
ferent characteristics. The SOD does not contain
source code from a constrained set of six program-
ming languages (see Table 1), as in the case of
the CodeBERT. Therefore, our model may produce
representations of all programming languages in
average quality. In contrast, the CodeBERT may
produce high-quality representations in the six pro-
gramming languages it was pre-trained on but very

Model
MQDD
CodeBERT
RoBERTa
Longformer†

Go
95.33 ± 0.04
96.68 ± 0.06
95.94 ± 0.06
66.62 ± 0.14

Java
80.11 ± 0.15
83.75 ± 0.06
81.58 ± 0.23
66.51 ± 0.24

JavaScript
70.09 ± 0.48
83.42 ± 0.06
80.35 ± 0.25
66.71 ± 0.15

PHP
85.58 ± 0.16
88.50 ± 0.03
86.78 ± 0.09
66.68 ± 0.06

Python
84.14 ± 0.48
88.25 ± 0.12
86.02 ± 0.11
66.71 ± 0.10

Ruby
82.77 ± 0.31
87.22 ± 0.31
84.06 ± 0.20
66.74 ± 0.15

Table 5: Results summary of code search experiments in six different programming languages. The F1 score is
stated in percents with 95% conﬁdence intervals. The best results in each language are highlighted in bold. The
† sign marks randomly initialized models. The results show that the CodeBERT shows better results than our
MQDD model. As discussed in Section 6, we conjecture that this might be caused by a negative transfer effect.
Nevertheless, our model is speciﬁcally designed for duplicate question detection, and therefore, we do not place
much importance on the poorer results in the code search task.

bad representations of the other programming lan-
guages. This would also explain why CodeBERT
does not perform so well on duplicates; it excels
in processing the six programming languages but
fails to generalize to other abundantly contained
languages in the Stack Overﬂow dataset.

However, the offered explanation does not cover
that RoBERTa, whose pre-training dataset did not
contain any source code, outperforms our model
in the code search task. We speculate that this can
be caused by the MQDD model being trapped in
its local optimum due to its pre-training designed
especially for the duplicate detection. This can
make it difﬁcult to get out of this local optimum
when ﬁne-tuned on a slightly different dataset and
task. This phenomenon is often referred to as a
negative transfer (Rosenstein et al., 2005; Zhang
et al., 2020) and can be caused, among other things,
by the discrepancy between the pre-training and
ﬁne-tuning domains.

Given that our research aimed to build a model
designed directly for the detection of duplicates
on platforms such as Stack Overﬂow or Quora, it
can be stated that the results we achieve are sat-
isfactory. Our resulting model far exceeds the re-
sults achieved by competitive work on a task that
can be perceived as more demanding due to the
need to process a general source language and
distinguish seemingly insigniﬁcant semantic nu-
ances. For example, questions "How to implement
a producer-consumer in Java" and "How to im-
plement a producer-consumer in C++" must be
identiﬁed as different since the answers would sig-
niﬁcantly differ.

7 Future Work

Our work opens up further opportunities to build
on our current research. First of all, it would be
interesting to explore methods that would eliminate

the effect of negative transfer and thus allow the
use of our pre-trained model in other tasks.

Furthermore, the follow-up work can integrate
our model into a production-ready duplicate detec-
tion system employing a fast vector space search
library such as Faiss.

The proposed system can be further extended by
a duplicate detection model that jointly processes
both questions allowing the attention mechanism to
attend across both inputs. Such a model can poten-
tially achieve better results and be deployed along
with our two-tower-based model. Our two-tower
model would then be used to ﬁlter out candidate
duplicate questions. Afterward, the cross-attention
model could verify that the candidate questions are
indeed duplicates more accurately.

8 Conclusion

This work presents a new pre-trained BERT-
like model
that detects duplicate threads on
programming-related discussion platforms. Based
on the Longformer architecture,
the presented
model is pre-trained on our novel pre-training ob-
jectives (QA and SP) that aim to target the dupli-
cate detection task. The comparison with the com-
petitive CodeBERT model shows that our model
outperforms other approaches, suggesting the ef-
fectiveness of our learning objectives. Furthermore,
we investigated the generalization capabilities of
our model by applying it to a code retrieval task.
In this task, it turned out that our model does not
exceed the results achieved with either CodeBERT
or the more general RoBERTa model. We attribute
these ﬁndings to the signiﬁcant differences between
our pre-training dataset and the evaluation dataset
for the code search task. Therefore, we consider
our model an excellent choice for solving duplicate
detection. However, it seems to be too specialized
to solve other tasks well.

Our models are publicly available for research
purposes in our Hugging Face12 and GitHub13
repositories.

Acknowledgments

This work has been supported by Grant No. SGS-
2022-016 Advanced methods of data process-
ing and analysis. Computational resources were
supplied by the project "e-Infrastruktura CZ" (e-
INFRA CZ LM2018140 ) supported by the Min-
istry of Education, Youth and Sports of the Czech
Republic.

References

Mikhail Arkhipov, Maria Troﬁmova, Yuri Kuratov, and
Alexey Sorokin. 2019. Tuning multilingual trans-
formers for language-speciﬁc named entity recogni-
tion. In Proceedings of the 7th Workshop on Balto-
Slavic Natural Language Processing, pages 89–93,
Florence, Italy. Association for Computational Lin-
guistics.

Achmad Arwan, Siti Rochimah, and Rizky Januar
Akbar. 2015. Source code retrieval on stackover-
In 2015 3rd International Confer-
ﬂow using lda.
ence on Information and Communication Technol-
ogy (ICoICT), pages 295–299.

Dzmitry Bahdanau, Kyunghyun Cho, and Y. Bengio.
2014. Neural machine translation by jointly learning
to align and translate. ArXiv, 1409.

M Balog, AL Gaunt, M Brockschmidt, S Nowozin, and
D Tarlow. 2017. Deepcoder: Learning to write pro-
In International Conference on Learning
grams.
Representations (ICLR 2017). OpenReview. net.

Iz Beltagy, Matthew E. Peters, and Arman Cohan.
2020. Longformer: The long-document transformer.
CoRR, abs/2004.05150.

Lukas Biewald. 2020.
weights and biases.
wandb.com.

tracking with
Experiment
Software available from

John S. Bridle. 1990. Probabilistic interpretation of
feedforward classiﬁcation network outputs, with re-
lationships to statistical pattern recognition. In Neu-
rocomputing, pages 227–236, Berlin, Heidelberg.
Springer Berlin Heidelberg.

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,

12https://huggingface.co/UWB-AIR
13https://github.com/kiv-air/MQDD

Clemens Winter, Christopher Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin
Chess, Jack Clark, Christopher Berner, Sam Mc-
Candlish, Alec Radford, Ilya Sutskever, and Dario
Amodei. 2020. Language models are few-shot learn-
ers. CoRR, abs/2005.14165.

Rocío Cabrera Lozoya, Arnaud Baumann, An-
tonino Sabetta, and Michele Bezzi. 2021. Com-
mit2vec: Learning distributed representations of
code changes. SN Computer Science, 2(3):1–16.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Yuan, Henrique Ponde de Oliveira Pinto, Jared
Kaplan, Harrison Edwards, Yuri Burda, Nicholas
Joseph, Greg Brockman, Alex Ray, Raul Puri,
Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
Girish Sastry, Pamela Mishkin, Brooke Chan, Scott
Gray, Nick Ryder, Mikhail Pavlov, Alethea Power,
Lukasz Kaiser, Mohammad Bavarian, Clemens Win-
ter, Philippe Tillet, Felipe Petroski Such, Dave Cum-
mings, Matthias Plappert, Fotios Chantzis, Eliza-
beth Barnes, Ariel Herbert-Voss, William Hebgen
Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie
Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,
William Saunders, Christopher Hesse, Andrew N.
Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan
Morikawa, Alec Radford, Matthew Knight, Miles
Brundage, Mira Murati, Katie Mayer, Peter Welin-
der, Bob McGrew, Dario Amodei, Sam McCandlish,
Ilya Sutskever, and Wojciech Zaremba. 2021. Evalu-
ating large language models trained on code. CoRR,
abs/2107.03374.

Zimin Chen and Martin Monperrus. 2019. A litera-
ture study of embeddings on source code. CoRR,
abs/1904.03061.

Kevin Clark, Minh-Thang Luong, Quoc V. Le, and
Christopher D. Manning. 2020. ELECTRA: pre-
training text encoders as discriminators rather than
generators. CoRR, abs/2003.10555.

Xiang Deng, Ahmed Hassan Awadallah, Christopher
Meek, Oleksandr Polozov, Huan Sun, and Matthew
Richardson. 2020. Structure-grounded pretraining
for text-to-sql. arXiv preprint arXiv:2010.12773.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. BERT: pre-training of
deep bidirectional transformers for language under-
standing. CoRR, abs/1810.04805.

Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-
aocheng Feng, Ming Gong, Linjun Shou, Bing Qin,
Ting Liu, Daxin Jiang, and Ming Zhou. 2020. Code-
bert: A pre-trained model for programming and nat-
ural languages. CoRR, abs/2002.08155.

Anshul Gupta and Neel Sundaresan. 2018. Intelligent
code reviews using deep learning. In Proceedings of
the 24th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining (KDD’18)
Deep Learning Day.

Geert Heyman and Tom Van Cutsem. 2020. Neu-
ral code search revisited: Enhancing code snippet
retrieval through natural language intent. CoRR,
abs/2008.12193.

Abram Hindle, Earl Barr, Zhendong Su, Mark Gabel,
and Premkumar Devanbu. 2012. On the naturalness
of software. Proceedings - International Conference
on Software Engineering, pages 837–847.

Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis
Allamanis, and Marc Brockschmidt. 2019. Code-
searchnet challenge: Evaluating the state of seman-
tic code search. CoRR, abs/1909.09436.

Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.
IEEE

Billion-scale similarity search with GPUs.
Transactions on Big Data, 7(3):535–547.

Aditya Kanade, Petros Maniatis, Gogul Balakrishnan,
and Kensen Shi. 2020a. Pre-trained contextual em-
bedding of source code. CoRR, abs/2001.00059.

Aditya Kanade, Petros Maniatis, Gogul Balakrishnan,
and Kensen Shi. 2020b. Pre-trained contextual em-
bedding of source code. CoRR, abs/2001.00059.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
International

method for stochastic optimization.
Conference on Learning Representations.

Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.
2020. Reformer: The efﬁcient transformer. CoRR,
abs/2001.04451.

Triet Huynh Minh Le, Hao Chen, and Muhammad Ali
Babar. 2020. Deep learning for source code model-
ing and generation: Models, applications and chal-
lenges. CoRR, abs/2002.05442.

Shangqing Liu, Cuiyun Gao, Sen Chen, Nie Lun Yiu,
and Yang Liu. 2020. Atom: Commit message gener-
ation based on abstract syntax tree and hybrid rank-
ing. IEEE Transactions on Software Engineering.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized BERT pretraining ap-
proach. CoRR, abs/1907.11692.

Zihan Liu, Feijun Jiang, Yuxiang Hu, Chen Shi, and
Pascale Fung. 2021. NER-BERT: A pre-trained
CoRR,
model for low-resource entity tagging.
abs/2112.00405.

Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan
Lee. 2019. Vilbert: Pretraining task-agnostic visi-
olinguistic representations for vision-and-language
tasks. CoRR, abs/1908.02265.

Bryan McCann, James Bradbury, Caiming Xiong, and
Richard Socher. 2017. Learned in translation: Con-
textualized word vectors. CoRR, abs/1708.00107.

Vinod Nair and Geoffrey E. Hinton. 2010. Rectiﬁed
linear units improve restricted boltzmann machines.
In Proceedings of the 27th International Conference
on International Conference on Machine Learning,
ICML’10, page 807–814, Madison, WI, USA. Om-
nipress.

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
In Proceedings of the 2018 Confer-
resentations.
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long Papers), pages
2227–2237, New Orleans, Louisiana. Association
for Computational Linguistics.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. 2019. Exploring the limits
of transfer learning with a uniﬁed text-to-text trans-
former. CoRR, abs/1910.10683.

Nils Reimers and Iryna Gurevych. 2019. Sentence-
bert: Sentence embeddings using siamese bert-
networks. CoRR, abs/1908.10084.

Michael Rosenstein, Zvika Marx, Leslie Kaelbling,
and Thomas Dietterich. 2005. To transfer or not to
transfer.

Saksham Sachdev, Hongyu Li, Sifei Luan, Seohyun
Kim, Koushik Sen, and Satish Chandra. 2018. Re-
trieval on source code: A neural code search. In Pro-
ceedings of the 2nd ACM SIGPLAN International
Workshop on Machine Learning and Programming
Languages, MAPL 2018, page 31–41, New York,
NY, USA. Association for Computing Machinery.

M. Schuster and K.K. Paliwal. 1997. Bidirectional re-
current neural networks. IEEE Transactions on Sig-
nal Processing, 45(11):2673–2681.

Mike Schuster and Kaisuke Nakajima. 2012. Japanese
In 2012 IEEE Interna-
and korean voice search.
tional Conference on Acoustics, Speech and Signal
Processing (ICASSP), pages 5149–5152.

Chen Sun, Austin Myers, Carl Vondrick, Kevin Mur-
phy, and Cordelia Schmid. 2019a. Videobert: A
joint model for video and language representation
learning. CoRR, abs/1904.01766.

Chi Sun, Luyao Huang, and Xipeng Qiu. 2019b.
Utilizing BERT for aspect-based sentiment anal-
ysis via constructing auxiliary sentence. CoRR,
abs/1903.09588.

Weisong Sun, Chunrong Fang, Yuchen Chen, Guan-
hong Tao, Tingxu Han, and Quanjun Zhang. 2022.
Code search based on context-aware code transla-
tion. arXiv preprint arXiv:2202.08029.

Michele Tufano, Cody Watson, Gabriele Bavota, Mas-
similiano Di Penta, Martin White, and Denys Poshy-
vanyk. 2018. Deep learning similarities from dif-
In 2018
ferent representations of source code.

IEEE/ACM 15th International Conference on Min-
ing Software Repositories (MSR), pages 542–553.
IEEE.

Farhan Ullah, Junfeng Wang, Sohail Jabbar, Fadi Al-
Turjman, and Mamoun Alazab. 2019. Source code
authorship attribution using hybrid approach of pro-
gram dependence graph and deep learning model.
IEEE Access, 7:141987–141999.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. CoRR, abs/1706.03762.

Liting Wang, Li Zhang, and Jing Jiang. 2020. Dupli-
cate question detection with deep learning in stack
overﬂow. IEEE Access, 8:25964–25975.

Yue Wang, Weishi Wang, Shaﬁq R. Joty, and Steven
C. H. Hoi. 2021. Codet5: Identiﬁer-aware uniﬁed
pre-trained encoder-decoder models for code under-
standing and generation. CoRR, abs/2109.00859.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander M. Rush. 2020.
Transformers: State-of-the-art natural language pro-
cessing. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing:
System Demonstrations, pages 38–45, Online. Asso-
ciation for Computational Linguistics.

Pengcheng Yin and Graham Neubig. 2017. A syntactic
neural model for general-purpose code generation.
In Proceedings of the 55th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 440–450.

Wen Zhang, Lingfei Deng, and Dongrui Wu. 2020. A
survey on negative transfer. CoRR, abs/2009.00909.

A Dataset Pre-processing

C Duplicate Detection Hyperparameters

The data retrieved from the Stack Overﬂow data
dump contain an HTML markup that needs to be
pre-processed before being used to train a neural
network. Furthermore, the natural language and
source code snippets are mixed in a single HTML
document, so we need to separate those two parts.
We use the BeautifulSoup14 library to ex-
tract the textual data from the HTML markup.
To do so, we remove all content enclosed in
<code></code> tags and strip all the remain-
ing HTML tags. Afterward, we remove all newline
characters and multiple subsequent space charac-
ters induced by stripping the HTML tags.

On the other hand, while pre-processing
the code snippets, we ﬁrst extract all con-
tent from <pre><code></code></pre> us-
ing the BeautifulSoup library and throw away
the rest. Afterward, we remove the newlines and
multiple spaces, as in the case of the textual part.

B Longformer Model Conﬁguration

The implementation of the Longformer model
that we employ in the pre-training is the
transformers.LongformerModel15 from
HuggingFace Transformers library. Below, we pro-
vide a detailed listing of the model’s parameters.

• attention_probs_dropout_prob = 0.1

• attention_window = 256

• hidden_act = gelu

• hidden_dropout_prob = 0.1

• hidden_size = 768

• initializer_range = 0.02

• intermediate_size = 3072

• layer_norm_eps = 1e-12

• max_position_embeddings = 1026

• num_attention_heads = 12

• num_hidden_layers = 12

• position_embedding_type = absolute

• vocab_size = 50256

• intermediate_layer_dim (D) = 1000

14https://www.crummy.com/software/

BeautifulSoup/bs4/doc/

15https://huggingface.co/docs/
transformers/model_doc/longformer#
transformers.LongformerModel

For ﬁne-tuning our MQDD model on the duplicate
detection task, we employ the Adam optimizer with
an initial learning rate of 6.35e−6. We train the
model on sequences of 256 subword tokens with a
batch size of 100 examples. Additionally, we use an
L2 normalization with a normalization factor set to
0.043. Another regularization method we employ
is the dropout with the following conﬁguration:

• attention dropout in the Longformer = 0.2

• hidden dropout in the Longformer = 0.5

• dropout at the ﬁrst linear layer of the classiﬁ-

cation head = 0.26

• dropout at the second linear layer of the clas-

siﬁcation head = 0.2

D Stack Overﬂow Dataset Structure

The Stack Overﬂow Dataset (SOD) consists of a
metadata ﬁle and several data ﬁles. Each line of the
metadata ﬁle (dataset_meta.csv) contains a
JSON array with the following information:

• question_id - identiﬁer of the question in
format <id>-<page> (in our case the
page = stackoverflow)

• answer_id - identiﬁer of the answer in
format <id>-<page> (in our case the
page = stackoverflow)

• title - title of the question

• tags - tags associated with the question

• is_accepted - boolean ﬂag indicating whether
the answer represents an accepted answer for
the question

The dataset export is organized in such a way
that i-th row in the metadata ﬁle corresponds to
training examples located on the i-th row in the data
ﬁles. There are six different data ﬁle types, each
comprising training examples of different input
pair types (described in Section 3.3). A complete
list of the data ﬁle types follows:

• dataset_AC_AT.csv - code from an an-

swer with text from the same answer

• dataset_QC_AC.csv - code from a ques-

tion with code from a related answer

• dataset_QC_AT.csv - code from a ques-

tion with text from a related answer

• second_post - HTML formatted data of the
second question (contains both text and code
snippets)

• ﬁrst_author - username of the ﬁrst question’s

author

• second_author - username of the second

question’s author

• label - label determining the relationship of

the two questions

0. duplicates
1. similar based on full-text search
2. similar based on tags
3. different
4. accepted answer

• page - Stack Exchange page from which
to

the questions originate (always
stackoverflow)

set

As one can see, our dataset contains accepted
answers as well. Although we are not using them
in our work, we included them in the dataset to
open up other possibilities of using our dataset.

• dataset_QC_QT.csv - code from a ques-

tion with text from the same question

• dataset_QT_AC.csv - text from a ques-

tion with code from a related answer

• dataset_QT_AT.csv - text from a ques-

tion with text from a related answer

Each row in the data ﬁle then represents a
single example whose metadata can be obtained
from a corresponding row in the metadata ﬁle.
A training example is represented by a JSON ar-
ray containing two strings. For example, in the
dataset_QC_AC.csv, the ﬁrst element in the
array contains code from a question, whereas the
second element contains code from the related an-
swer. It shall be noted that the dataset export does
not contain negative examples since they would
signiﬁcantly increase the disk space required for
storing the dataset. The negative examples must
be randomly sampled during pre-processing, as
discussed in Section 3.1.

Since the resulting dataset

takes up a lot
of disk space, we split
the individual data
ﬁles and the metadata ﬁle into nine smaller
ones.
for exam-
ple, dataset_meta_1.csv and corresponding
dataset_QC_AT_1.csv can then be found in
the repository.

Therefore, ﬁles such as,

E Stack Overﬂow Duplicity Dataset

Structure

The published SODD dataset is split into train/de-
v/test splits and is stored in parquet16 ﬁles com-
pressed using gzip. The data can be loaded using
the pandas17 library using the following code snip-
pet:

1 !pip3 install pandas pyarrow
2
3 import pandas as pd
4
5 d=pd.read_parquet(’<file>.parquet.gzip’)

The dataframe loaded using the snippet above

contains the following columns:

• ﬁrst_post - HTML formatted data of the ﬁrst
question (contains both text and code snip-
pets)

16https://parquet.apache.org/

documentation/latest/

17https://pandas.pydata.org

