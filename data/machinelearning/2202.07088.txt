Scaling up Ranking under Constraints for Live Recommendations
by Replacing Optimization with Prediction

Yegor Tkachenko 1 Wassim Dhaouadi 2 Kamel Jedidi 1

2
2
0
2

b
e
F
4
1

]

R

I
.
s
c
[

1
v
8
8
0
7
0
.
2
0
2
2
:
v
i
X
r
a

Abstract

Many important multiple-objective decision prob-
lems can be cast within the framework of ranking
under constraints and solved via a weighted bi-
partite matching linear program. Some of these
optimization problems, such as personalized con-
tent recommendations, may need to be solved in
real time and thus must comply with strict time re-
quirements to prevent the perception of latency by
consumers. Classical linear programming is too
computationally inefﬁcient for such settings. We
propose a novel approach to scale up ranking un-
der constraints by replacing the weighted bipartite
matching optimization with a prediction problem
in the algorithm deployment stage. We show em-
pirically that the proposed approximate solution
to the ranking problem leads to a major reduction
in required computing resources without much
sacriﬁce in constraint compliance and achieved
utility, allowing us to solve larger constrained
ranking problems real-time, within the required
50 milliseconds, than previously reported.

1. Introduction

Many important multiple-objective decision problems can
be cast within the framework of ranking under constraints.
Content delivery platforms, such as TikTok, Pinterest, In-
stagram, Facebook news feed, and Google search engine,
are prominent examples. Such platforms constantly have to
select and prioritize content from a large dynamic library to
show within the restrictions of available display space and
consumer’s attention span, across millions of users (Ansari
et al., 2000; Covington et al., 2016; Mnih & Salakhutdinov,
2008). The recommendations need to balance multiple ob-
jectives – the primary objective of optimizing revenue or
its proxy, as well as auxiliary objectives, such as diversity,
ethics, recency, and other properties of the recommended

1Columbia University 2Stanford University. Correspondence

to: Yegor Tkachenko <ytkachenko21@gsb.columbia.edu>.

Copyright 2022 by the author(s).

content (Celis et al., 2017; White, 2000). To avoid the per-
ception of latency, ranking must be returned in less than 100
milliseconds (Miller, 1968), which, given the time required
to send and receive information online, means ranking has to
be computed within 50 milliseconds (Zhernov et al., 2020).

The problem of ranking under constraints in recommender
settings can be formulated as a weighted bipartite matching
linear program (Singh & Joachims, 2018; Biega et al., 2018).
The formulation can handle varied constraints and ﬂexible
preferences over item-rank combinations. However, classi-
cal linear programming is too computationally inefﬁcient to
meet the latency requirement for live deployment.

We address this problem of speed. We propose a scalable
algorithm for ranking under constraints based on statistical
sampling and a dual formulation of the weighted constrained
bipartite matching program (Shah et al., 2017; Mehta, 2012).
In particular, we propose to replace the problem of online
optimization with a prediction problem. We solve the dual
program for optimal shadow prices on a sample of users in
ofﬂine settings, where speed is not critical. We then train a
model to predict users’ optimal shadow prices from users’
characteristics. In online settings, the model can predict the
shadow prices directly based on observed user covariates,
without having to solve the time-consuming optimization
problem, allowing us to quickly compute the ranking for any
such user. We demonstrate empirically that the proposed
approximate solution to the ranking problem leads to a sub-
stantial reduction in required computing resources without
much sacriﬁce in constraint compliance and achieved utility,
allowing us to solve larger constrained ranking problems
real-time, within the required 50 milliseconds, than previ-
ously reported (Zhernov et al., 2020). The code to reproduce
the results in this paper is made available on GitHub.1

Signiﬁcance Our fast constrained ranking framework can
empower ethical personalization at scale across online con-
tent platforms. It could also tackle time-sensitive assignment
and matching problems beyond content recommendations,
e.g., robot swarm task allocation. Further, our results show
the power of the idea of predicting optimization solutions,
which could ﬁnd many uses in AI research and practice.

1https://github.com/computationalmarketing/scalable_

ranking_under_constraints

 
 
 
 
 
 
Scaling up Ranking under Constraints for Live Recommendations

2. Optimization framework

Consider an example where a recommender system ranks m
items (e.g., movies) to show to a given user. Assume that the
decision maker’s utility is maximized by showing items in
the order that maximizes user’s utility – so decision maker’s
utility mirrors user’s utility. This is in line with probability
ranking principle, which states that, for optimal retrieval,
documents should be ranked in order of the probability of
relevance or usefulness (Robertson, 1977). Let U be an
m × m utility matrix that captures decision maker’s pref-
erences. Uij gives decision maker’s utility from assigning
item i to position j in the ranking. In other words, each
row in this matrix represents scores that item i would get
by being placed in different ranking positions j. A common
assumption here is that the utility of an item should decline /
be discounted with the lower position in the ranking because
the user is less likely to be exposed to an item when it is
displayed lower in the list (Singh & Joachims, 2018).

2.1. Optimal assignment

An assignment of items to positions in the ranking can be
captured by an m × m permutation matrix P ∈ P (P is
the space of permutation matrices): Pij = 1 if item i is
assigned to position j, Pij = 0 otherwise (each item is
assigned to a single rank, and each rank holds a single item;
in other words, all rows and columns sum to one; P T 1 = 1
and P 1 = 1). We can write down the utility from a given
assignment as a sum over element-wise product between U
and P : (cid:80)
j UijPij. This sum is equal to the trace of the
i
dot product of the two matrices tr(U T P ) = (cid:80)
j UijPij.
i
The goal is to ﬁnd P that maximizes tr(U T P ). More for-
mally, we have the following optimization problem, also
known as the weighted bipartite matching problem or the
assignment problem:

(cid:80)

(cid:80)

The Hungarian algorithm gives an optimal solution (Ed-
monds & Karp, 1972; Tomizawa, 1971; Kuhn, 1955; 1956).

2.2. Adding constraints

Ranking under constraints problem can be formulated as an
instance of the constrained bipartite graph matching pro-
gram, following Singh & Joachims (2018); Biega et al.
(2018). Classical formulation of the problem is as follows:

This program (eq. (2)) is different from the previous one
(eq. (1)) only in that K constraints have been added: an
m×m matrix Ak together with a scalar bk track if P satisﬁes
a k-th constraint. Ak can be interpreted as an auxiliary
utility matrix corresponding to an alternative objective. The
formulation can accommodate a variety of constraints, such
as quotas by item type (e.g., protected category), constraints
on inverse-rank-weighted average of item characteristics
(e.g., release recency), etc. Addition of the constraints,
while P is restricted to be an integer permutation matrix,
renders this combinatorial optimization problem intractable.

2.3. Primal program

To proceed, we remove the integer requirement, constraining
P to be a doubly stochastic matrix.

tr(U T P )

max
P
s.t. tr(AT

k P ) ≥ bk ∀ k ∈ 1 : K

(3)

P ∈ Rm×m, P 1 = 1, P T 1 = 1, P ≥ 0.

This convex relaxation of the integer problem (eq. (3)) can
be solved using linear programming (Bubeck, 2014). The
solution P may be fractional, but integer solutions can be ob-
tained via Birkhoff-von Neumann decomposition (Birkhoff,
1940). We refer to this formulation as the primal program.

2.4. Dual program

We also consider the dual formulation of the weighted bipar-
tite graph matching problem (Roth et al., 1993; Shah et al.,
2017; Mehta, 2012), obtained by application of the duality
theorem (Boyd & Vandenberghe, 2004) – the dual program:

λT b + αT 1 + βT 1

max
λ,α,β

s.t. U +

(cid:88)

λkAk + 1αT + β1T ≤ 0

(4)

Here λk is a scalar shadow price corresponding to constraint
k (λ is a vector of all K shadow prices). Shadow prices have
rich interpretation. For example, when constraint k speciﬁes
the minimum number of items from a protected category
among top n items, λk can be interpreted as a ‘boost’ in
terms of utility that has to be applied to protected category
items for them to be shifted up in the ranking to satisfy con-
straint k. Thus, shadow prices λ can be informative about
decision maker’s preferences and the cost of the constraint.
Shadow prices α and β correspond, respectively, to column
and row sum constraints on P .

tr(U T P )

max
P

P ∈ P.

(1)

k
λ ≥ 0.

tr(U T P )

max
P
s.t. tr(AT

k P ) ≥ bk ∀ k ∈ 1 : K

P ∈ P.

(2)

The optimized permutation ranking matrix can be obtained
by applying the Hungarian algorithm to the modiﬁed utility
matrix S = U + (cid:80)
k λkAk for a given set of items (Shah

Scaling up Ranking under Constraints for Live Recommendations

et al., 2017). This suggests another view of λ as optimal
weights in a linear combination of primary (U ) and auxil-
iary (Ak) utility matrices – such that the resulting modiﬁed
utility matrix S directly yields optimal assignment upon
application of the Hungarian algorithm. Thus, while the pri-
mary program searches directly for doubly stochastic matrix
P capturing optimal assignment, the dual program searches
for λ capturing the optimal combination of utility matrices
U and Ak behind multiple objectives to yield the optimal
assignment indirectly. The latter method proves to be useful
for speeding up the computation.

3. Speeding up ranking under constraints

Off-the-shelf solvers struggle to solve the above linear pro-
grams in real time (under 50 milliseconds) in general. For
large numbers of items and constraints (≥ 500 ranked ob-
jects and ≥ 5 constraints), even custom solvers relying on
special problem structure fail (Zhernov et al., 2020).

Algorithm 1 Scalable personalized constrained ranking

Input: For each ranking instance l (user), utility matrix
U (l), constraint arrays A(l)
k , b(l)
for l in Ofﬂine Train Set do

k , and covariates X (l).

Solve dual linear prog. (eq. (4)) for U (l), A(l)
Store optimal λ(l).

k , and b(l)
k .

end for
Train prediction model f (Xtrain) → λtrain based on
Ofﬂine Train Set.
for l in Online Test Set do
Predict ˆλ(l) ← f (X (l)).
Compute ˆS(l) = U (l) + (cid:80)
k
Determine an optimal assignment P (l) based on ˆS(l)
via Hungarian method, greedy 1/2-approximation al-
gorithm, via identity permutation if ˆS(l) is inverse
Monge, or via sorting + identity permutation in case
of special permuted inverse Monge structure. Alterna-
tively, an optimal assignment could be approximated
with a neural net prediction.

k A(l)
ˆλ(l)
k .

3.1. Prediction instead of optimization

end for

To address computational inefﬁciencies of linear program-
ming, we propose to replace real-time optimization with
prediction. We ﬁrst solve the dual program to obtain op-
timal shadow prices on a sample of users ofﬂine, where
speed is not critical. We can then train a supervised learning
model to predict users’ optimal shadow prices from users’
characteristics. In online settings, for a new user we have
not seen before, the model can predict the optimal shadow
prices ˆλ based on observed covariates, without solving the
time-consuming optimization problem. We can then directly
compute the adjusted utility matrix ˆS = U + (cid:80)
ˆλkAk for
k
the new user and obtain the optimal permutation matrix from
ˆS afterwards. Algorithm 1 presents our proposed procedure.

Layers of approximation Note that all of the discussed
practical constrained optimization formulations only pro-
vide approximate solutions, ﬁrst and foremost stemming
from the convex relaxation. In case of the primal prob-
lem relaxation in eq. (3), approximation arises because we
need to move from doubly-stochastic matrix to an integral
permutation matrix via Birkhoff-von Neumann decomposi-
tion. Finding such a decomposition with minimum number
of terms is an NP-hard problem in itself and admits only
heuristic solutions, with the commonly used greedy heuristic
having complexity O(m2) (Duff & Koster, 2001; Dufoss´e
& Uc¸ar, 2016). It results in, at most, m2 distinct permu-
tation matrices, which can then be sampled based on this
decomposition. Constraints are satisﬁed only asymptoti-
cally. In case of the dual formulation in eq. (4), the source
of approximation arises because the Hungarian algorithm
run on S returns one of the valid extremal solutions from
Birkhoff polytope (Schrijver, 2003), and thus also satis-
ﬁes primal constraints asymptotically. That is, the use of

shadow prices λ obtained as a solution to eq. (4) to com-
pute S = U + (cid:80)
k λkAk can result in multiple constraint-
compliant and constraint-non-compliant assignments that
are tied in utility. In practice, we can often break ties heuristi-
cally in favor of auxiliary objectives (constraint compliance)
by computing S = U + (cid:80)
k(1 + ε)λkAk, where ε > 0 is a
tuning parameter. Figure 1 gives an example of solving the
dual program and handling the case of multiple solutions.

The statistical estimation of λ adds an additional layer of
approximation, however, simple prediction algorithms can
give us good estimates under mild assumptions. For ex-
ample, a k-nearest neighbor regressor yields a consistent
estimate of E[λ|X = x], where X denotes user covariates,
given a large number of i.i.d. observations, a large enough
number of neighbors used in the prediction, and some tech-
nical conditions (Devroye et al., 1994). Intuitively, with
enough data, we can predict λ arbitrarily well, limited only
by the expressive power of covariates and systematic noise.

Taking note of these technical issues, we show empirically
in Section 4 that we can consistently obtain constraint-
compliant solutions with minimal loss of optimality.

3.2. Special problem structure for further speed up

An optimal assignment permutation matrix P (m × m)
can be computed from S in the most general settings via
the Hungarian algorithm with worst-case run-time com-
plexity O(m3) (Edmonds & Karp, 1972; Tomizawa, 1971;
Bougleux et al., 2017). If m is large, this computational step
can become a bottleneck and so we would like to ﬁnd a faster
algorithm. One approach that has been proposed is to use

Scaling up Ranking under Constraints for Live Recommendations

R3 R4
1
2
2
3
3
3
0
0






I1
I2
I3
I4






R1
0
0
1
0

R2
0
0
0.6
0

R3
0
0
0.5
0

R4
0
0
0.4
0






I1
I2
I3
I4

A1 =

tr(AT

1 P ) ≥ 0.7






R1 R2
4
5
3
5
3
3
1
2

U =






R1 R2
1
0
0
0
0
1
0
0

P =

R3 R4
0
0
0
1
0
0
1
0






I1
I2
I3
I4

λ1 = 4.0

S = U + λ1A1 =






R1
5
5
7
2

R2
4
3
5.4
1

R3
2
3
5
0

R4
1
2
4.6
0






I1
I2
I3
I4

Figure 1. Problem formulation example. U is a utility matrix. Uij gives utility a decision maker obtains from placing item i in rank
position j (e.g., as predicted by a recommender system). P is a permutation matrix. Pij = 1 if item i is assigned to position j, and
Pij = 0 otherwise. We want to ﬁnd an optimal assignment, subject to constraint tr(AT
1 P ) ≥ 0.7. Here A1 is an auxiliary utility matrix
indicating items with some valued attribute, the utility decaying as item gets ranked lower. The constraint sets the minimum acceptable
level of auxiliary utility. The only way to satisfy the constraint here is to place item 3 in rank 1. In more complex cases, we have to resort
to formal optimization to solve the dual program. λ1 = 4 is the shadow price returned by the optimization program and S = U + λ1A1
is the adjusted utility matrix that we can ﬁnd an optimal assignment from. Critically, there is a tie here. Presented P gives the optimal
assignment satisfying the constraint, with tr(ST P ) = 14. However, a different (i, j) assignment (2, 1), (1, 2), (3, 3), (4, 4) gives the
same total adjusted utility but violates the constraint. This shows that a dual program solution need not be unique and only satisﬁes
constraints asymptotically. Fortunately, we can often heuristically break the tie in favor of the constraint-compliant solution by computing
S = U + (cid:80)

k(1 + ε)λkAk, where ε > 0 (e.g., ε = 10−4). We treat ε as a tuning parameter.

a greedy 1/2-approximation algorithm (Avis, 1983; Preis,
1999; Gusﬁeld, 1992) with lower worst-case complexity
O(m2). However, the weight of its matching can be as low
as 1/2 of the maximum weight computed by the Hungarian
algorithm. Luckily, we can get the best of both worlds in
terms of speed and guaranteed matching optimality if we
restrict S to possess a special structure.

3.2.1. FIXED DISCOUNTING AND REARRANGEMENT

INEQUALITIES

Consider a ‘ﬁxed discounting’ formulation U = uγT and
Ak = akγT for all k, with γ, u, ak ∈ Rm. Vectors u and
ak give primary and auxiliary utilities of items, independent
of assigned rank. γ is a discount vector along the ranks.
k λkAk = (u + (cid:80)
Then S = U + (cid:80)
k λkak)γT = sγT ,
where s = u + (cid:80)
k λkak. We seek a permutation matrix
P ∈ P that maximizes tr(ST P ). Intuitively, the lower the
item’s rank, the lower is its observation probability, so u
and ak utilities should be discounted proportionately (Singh
& Joachims, 2018).2

Let parenthesis indexing denote descending ordering of
the sequences so that s(1) ≥ s(2) ≥ · · · ≥ s(m) and

2For example, discounted cumulative gain (DCG) approach
expresses discounting factor as γi = 1/ log2(1+i) with i ∈ 1 : m,
which is non-increasing (J¨arvelin & Kek¨al¨ainen, 2002; Singh &
Joachims, 2018; Biega et al., 2018). Simple discounting γi = di
for d ∈ (0, 1] also ﬁts the proﬁle.

γ(1) ≥ γ(2) ≥ · · · ≥ γ(m), for s, γ that are in arbitrary
order. Hardy et al. (1952) have proven the following rear-
rangement inequality applicable to our case:

m
(cid:88)

i=1

s(i)γ(i) ≥

m
(cid:88)

i=1

siγ(i) ≥

m
(cid:88)

i=1

s(m−i+1)γ(i).

(5)

It follows immediately from this inequality that when two
sequences s and γ are sorted in a descending order, then
identity permutation matrix P = Im×m, from among all
permutation matrices, maximizes sT P γ = tr(γsT P ).

Discounting vector γ is determined by the decision maker
and it makes intuitive sense to use discounting that is in the
descending order (we will also assume γ > 0). However,
s = u + (cid:80)
k λkak need not be in the descending order.
To handle this, deﬁne argsort(x) = Rx ∈ P such that
Rxx is in descending order. Once we have obtained S =
(u + (cid:80)
k λkak)γT , we can look at its ﬁrst column S[:, 1] =
γ1(u + (cid:80)
k λkak). If γ1 > 0, Rs = argsort(S[:, 1]) =
argsort(u + (cid:80)
k λkak). Then RsS = sort(s)γT and, by re-
arrangement inequality, Im×m = argmaxP ∈P tr(ST RT
s P ).
It follows that RT
s = argmaxP ∈P tr(ST P ).
Thus, if γ > 0 and is in descending order, we only need
to sort S on its ﬁrst column to recover optimal assignment
via identity permutation. This gives us an O(m log m) time
complexity algorithm, in contrast to O(m3) worst time com-
plexity of the Hungarian algorithm and O(m2) worst time
complexity of the greedy algorithm, making this method

s Im×m = RT

Scaling up Ranking under Constraints for Live Recommendations

more scalable and the intentional restriction on the decision
problem with ﬁxed discounting an attractive approach.

algorithms for special types of unbalanced Monge matrices
have been studied by Aggarwal et al. (1992).

The presented optimality proof via rearrangement inequality
explains the great performance of the greedy sorting algo-
rithm observed (but left without an explanation) by Shah
et al. (2017) in the context of their Reference CTR model,
which is a form of ﬁxed discounting model presented above.
Notably, the same ﬁxed discounting formulation appears in
a different context as a necessary condition for operation of
the custom efﬁcient solver by Zhernov et al. (2020).

4. Experiments

In this section, we perform empirical evaluation of the pro-
posed algorithm for ranking under constraints in the recom-
mender system settings. We test the algorithm on recom-
mendations of (1) movies and (2) news documents. In both
cases, we use real data to train a recommender system.

3.2.2. GENERALIZATION AND ALTERNATIVES

4.1. Problem setup

Monge property is a generalization of rearrangement in-
If S is (weak) inverse
equalities (Holstermann, 2017).
Monge, then optimal permutation matrix is an identity ma-
If S is inverse Monge after
trix (Burkard et al., 1996).
sorting, it is called permuted Monge (H¨utter et al., 2020).
Fixed discounting problem structure yields Monge structure
– for s and γ non-increasing, S = sγT is inverse Monge
(Burkard, 2007). However, S can be inverse Monge with-
out the discounting structure, as Monge condition is more
general. If Monge condition is not satisﬁed, we can use
the greedy 1/2-approximation algorithm (Avis, 1983; Preis,
1999; Gusﬁeld, 1992) with worst-case complexity O(m2)
before using the O(m3) Hungarian algorithm. There is also
research on using neural nets to approximate the Hungarian
algorithm optimal assignment with complexity O(m2) or
O(m) (depending on the neural net type) (Lee et al., 2018).
See Appendix A for an extended discussion.

3.3. Unbalanced case

To simplify exposition, we have presented the results for
square U and Ak matrices. Unbalanced case (rectangular U
and Ak, m1 items and m2 rank positions, m1 > m2 – some
items remain unassigned) can also be accommodated. The
dual program can be augmented to tackle a different number
of vertices in a bipartite graph, relaxing the restriction on
the permutation matrix from equality to inequality (Mehta,
2012) (see Equation (6)). Optimal assignment can be found
based on rectangular Sm1×m2. The Hungarian algorithm
can be applied with complexity O(m1m2
2) (Bougleux et al.,
2017). Greedy 1/2-approximation algorithm can be applied
with complexity O(m1m2) (Preis, 1999). In the ﬁxed dis-
counting formulation, we can apply the algorithm without
change, taking elements from the main (i = j) diagonal
after sorting S on the ﬁrst column with worst time complex-
ity O(m1 log m1), as discussed earlier. In case of general
unbalanced inverse Monge matrices, Vaidyanathan (2013)
proposed a shortest path O(m1m2) algorithm. Burkard et al.
(1996) mention an alternative algorithm based on dynamic
programming with complexity O((m1 − m2 + 1)m2), but
do not provide implementation details. Optimal assignment

We want to compute an optimized ranking of items (movies
or news documents) for each user, maximizing the utility
from the recommended items, while ensuring compliance
with constraints, such as constraints on amount of exposure
by movies/news documents across different topic areas.

To set up the optimization, we need to predict utility for
all possible user-item combinations. On each data set, we
train an embedding-based neural net, which predicts item
utility to the user as a non-linear function of user and item
embeddings. See Appendix B for training details. The pre-
dicted utility is in [1, 5] range for both movie and news data
sets. Learned user embeddings constitute user covariates X.
(Our proposed method uses covariates to predict λ (see Al-
gorithm 1).) For each user, the trained recommender system
outputs a vector u ∈ Rm1 of utilities over available items.
We construct utility matrix U over item-rank combinations
as U = uγT , where γ = 1/ log2(j + 1) for each rank
j ∈ 1 : m2 (m1 ≥ m2), as in the discounted cumulative
gain (DCG) framework (J¨arvelin & Kek¨al¨ainen, 2002; Singh
& Joachims, 2018; Biega et al., 2018). γ captures exposure
from being placed in rank position i (i.e., a discount on
utility as we go down in the rank). All constraint matrices
have the form Ak = akγT with ak ∈ Rm1 . Constraints are
data-speciﬁc and are discussed in more detail below.

Given this ﬁxed discounting problem structure, we can
use the efﬁcient O(m1 log m1) method for recovery of
optimal assignment from the adjusted utility matrix S =
U +(cid:80)
k λkAk, as discussed.3 However, shadow prices λ are
unknown. We compare the following strategies for comput-
ing λ – in terms of speed, achieved utility, and probability
of constraint compliance on a holdout set of users:

• No optimization: No accounting for constraints (λ =

0), showing top items by utility (benchmark).

• Optimal lambda: Dual optimization program (eq. (4))
is solved for each holdout user to get λ (time-intensive).

3Per earlier discussion, we actually compute S = U + (cid:80)
k(1 +
ε)λkAk to favor constraint-compliant solutions in case of ties. We
select ε that minimizes train subset constraint violation probability
– from the candidate set {0} ∪ {i · 10−j | i ∈ 1 : 9, j ∈ 1 : 4}.

Scaling up Ranking under Constraints for Live Recommendations

Unbalanced primal
tr(U T P )

max
P
s.t. tr(AT

k P ) ≥ bk ∀ k ∈ 1 : K

Unbalanced dual
λT b + αT 1m2 + βT 1m1

max
λ,α,β

s.t. U + (cid:80)

kλkAk + 1m1 αT + β1T

m2 ≤ 0

⇒

(6)

P ∈ Rm1×m2 , P 1m2 ≤ 1, P T 1m1 = 1, P ≥ 0

λ ≥ 0, β ≤ 0

• Mean lambda: An average shadow price vector ¯λ
across users in the train set is used to compute the rank-
ing (i.e., an intercept-only covariate-free predictor).

• KNeighbors lambda: K-nearest neighbor regressor,4
trained on the train set, is used to predict personalized
ˆλ from user’s covariates. This is our proposed method.

For that reason, we perform optimization experiments on
1,000 users sampled from the full data – so that we can
compute both scalable and time-intensive optimal solutions
within a reasonable amount of time. Train set contains 750
users and holdout set contains 250 users (randomly split).

4.2.2. YOW NEWS

Across experiments, in order to solve eq. (4), we use CBC
(COIN-OR Branch-and-Cut) solver within CVXPY library
(Diamond & Boyd, 2016), which is capable of efﬁciently
handling a large number of constraints. Following Zhernov
et al. (2020), all reported running times include exclusively
time spent solving the optimization problem on users in the
test set and exclude time spent on data generation, reading,
and pre-processing, as well as time spent on any ofﬂine
computation that can be done beforehand and need not be
completed live. All experiments are implemented in Python
and are run on the same uniform hardware.5

YOW news data contains evaluations of ∼6 thousand news
documents by 24 users. Speciﬁcally, data contains ∼10
thousand relevance scores assigned by users, a subset of
all possible user-document evaluations. The data set also
contains document tags, which allow us to classify news
documents into a set of topics. Based on tags, we classify
each document using a set of binary indicators character-
izing the topics: (1) Science and Technology, (2) Health,
(3) Business, (4) Entertainment, (5) World, (6) Politics, (7)
Sport, and (8) Environment. Train set contains 18 users and
holdout set contains 6 users, the split being random.

4.2. Evaluation

4.2.3. SCENARIOS

We perform evaluation on MovieLens 25M6 and YOW
news7 recommendation data sets.

4.2.1. MOVIELENS

MovieLens data contains 25 million ratings for 62 thousand
movies by 162 thousand users. There is information on
how each movie scores on a variety of tags in terms of
relevance. We classify each movie as being in top 5% of
movies on the following tags: (1) Gay character, (2) Race
issues, (3) Freedom of speech, and (4) Science ﬁction. We
also know the year of release for each movie. In training the
recommender system, we use the full data. In optimization
experiments, as one of the benchmarks, we need to solve
the time-intensive full dual optimization program for each
holdout user (‘optimal lambda’), which is time-consuming.

4We use scalable ball-tree k-nearest neighbor regressor (Pe-
dregosa et al., 2011), where neighbor points are weighted by the
inverse of their Euclidean distance to the point for which the pre-
diction is made, so closer neighbors have greater inﬂuence. We
use k = 10 nearest neighbors, following Nigsch et al. (2006).

5All evaluations are performed on the same machine (2.7 GHz

Quad-Core Intel Core i5-6400).

6https://grouplens.org/datasets/movielens/25m/
7https://users.soe.ucsc.edu/˜yiz/papers/data/

YOWStudy/

k = a(l)

The optimization is on a per-user basis, its complexity
driven by the number of items, rank positions, and con-
straints. We consider three optimization scenarios, where
we rank (a) top 50, (b) top 500, and (c) 1,000 items from
among the 1,000 highest-utility items for each user, while
imposing data-speciﬁc constraints. For each user l, we
have u(l) ∈ R1000, γ ∈ R50, R500, R1000, U (l) = u(l)γT ,
a(l) ∈ R1000, A(l)
k γT . a(l)
k is a binary vector captur-
ing whether each of 1,000 considered items for user l be-
longs to topic k (in MovieLens data, it also contains (scaled)
delta of the movie release year relative to 1990). b contains
corresponding constraint scalars, selected to promote rec-
ommendation diversity while ensuring program feasibility.
b ∈ R5 for MovieLens and b ∈ R8 for YOW news. Table 1
shows speciﬁc constraints. The mean per-user optimization
strategy performance across scenarios and the associated
conﬁdence intervals are estimated on the holdout user set.

4.3. Results

Figure 2 shows the constraint compliance and computing
time across different optimization strategies on MovieLens
and YOW news data sets. ‘Optimal lambda’ strategy tends
to yield the best constraint compliance of all strategies, but

Scaling up Ranking under Constraints for Live Recommendations

(a) MovieLens data (K = 5 constraints).

(b) YOW news data (K = 8 constraints).

Figure 2. Algorithm computation time vs. constraint compliance on MovieLens and YOW news data sets. Results are averages across
holdout users in three scenarios, where we rank (a) top 50, (b) top 500, and (c) 1,000 items from among the 1,000 highest-utility items for
each user, subject to constraints. Error bars indicate two standard errors of the mean (nmovielens = 250 and nyow = 6 per algorithm-problem
size combination; some are not visible because the error is small). Dashed line indicates 50 millisecond real-time latency requirement.

does not meet latency requirements.8 At the same time, we
see that the proposed prediction-based approach ‘KNeigh-
bors lambda’ performs close to optimal in terms of con-
straint compliance and is within the 50 millisecond latency
requirement – even when ranking all 1,000 items under 5 or
more constraints. Importantly, Zhernov et al. (2020) report
inability to solve problems of such size (≥ 500 ranked ob-
jects and ≥ 5 constraints) in real time. This highlights the
speed advantage of prediction-based methods and suggests
they are an attractive approach to solving large ranking prob-
lems in real time. ‘Mean lambda’ method underperforms
compared to ‘KNeighbors lambda’ approach in terms of
achieved constraint compliance on MovieLens data, suggest-
ing that capturing user heterogeneity can be advantageous.
‘Mean lambda’ and ‘KNeighbors lambda’ achieve similar
constraint compliance on YOW news data set. Under no
optimization, we get the worst constraint compliance. Algo-
rithm differences in achieved utility were small in magnitude
and mostly not signiﬁcant,9 in line with ﬁndings that the
price of imposing diversity constraints is often low (Bandi
& Bertsimas, 2021). See Appendix C for regression output.

5. Related work

Many methods exist for ranking under multiple objectives
(Singh & Joachims, 2018; Yang & Stoyanovich, 2017;
Zehlike et al., 2017; Celis et al., 2017; Asudeh et al., 2019;
Biega et al., 2018; Radlinski et al., 2008). A common ap-
proach, called ranking under constraints, is to consider al-

8Note that the optimal strategy does not always achieve 100%
constraint compliance – this is in line with the notion that even the
optimal solution is approximate, as discussed earlier.

9No optimization MovieLens utility was signiﬁcantly lower.

ternative objectives as constraints on the primary objective.

Singh & Joachims (2018); Biega et al. (2018) have consid-
ered formulation of the ranking under constraints in recom-
mender setting as a weighted bipartite matching program,
which can handle varied constraints and ﬂexible preferences
over arbitrary item-ranking combinations. However, their
proposed linear program formulation is too computation-
ally inefﬁcient to meet the strict latency requirement for
live deployment. Zhernov et al. (2020) have constructed a
dedicated solver to speed up solution of such a linear pro-
gram, when it has a special structure. However, they have
reported that their approach cannot meet requirements of
real-time performance for large decision problems (for ex-
ample, ≥ 500 ranked objects together with ≥ 5 constraints),
whereas our proposed algorithm meets the 50 millisecond
latency requirement when solving problems of similar and
larger sizes, as we have demonstrated. Their algorithm also
breaks down when a special structure of ﬁxed discount-
ing along the ranks is absent, whereas our algorithm offers
speed-ups even in the absence of such structure.

Others have proposed algorithms that may be faster, but are
not general enough to handle ﬂexible preferences and varied
constraints: Asudeh et al. (2019) require ranking function
adjustment by the user; Celis et al. (2017); Zehlike et al.
(2017) deal with top-n style ranking / constraints; Yang &
Stoyanovich (2017) restrict the problem to a narrow set of
speciﬁc fairness measures.

Shah et al. (2017) have shown that the dual formulation
of the weighted bipartite matching problem can be ﬁrst
solved ofﬂine on a statistical sample of users – to estimate
‘average’ ˆλ based on aggregate U and Ak for that sample.
These shadow prices can then be used to arrive at a ranking

AlgorithmKNeighborslambdaMeanlambdaNooptimizationOptimallambdaProblemsize505001000100101102103104105Milliseconds0.00.20.40.60.81.0Constraintcomplianceprobability50ms.100101102103104105Milliseconds0.20.40.60.81.0Constraintcomplianceprobability50ms.Scaling up Ranking under Constraints for Live Recommendations

Table 1. Inequality constraints in MovieLens and YOW news experiments. Constraints on % exposure are in terms of the total exposure:
(cid:80)
j 1/ log2(j + 1). We consider three scenarios, where we rank (a) top 50, (b) top 500, and (c) 1,000 items from among 1,000

j γj = (cid:80)

highest-utility items for each user. Constraint scalars bk differ across scenarios to ensure problem feasibility.

(a) Constraints on required % of exposure by topic and exposure-weighted movie release year in MovieLens recommendations data set.

NO.

1
2
3
4
5

TR(AT P )

INEQ.

TOP 50 bk

TOP 500 bk

TOP 1000 bk

GAY CHARACTER (QUEER) MOVIES - % TOTAL EXPOSURE
RACIAL ISSUES MOVIES - % TOTAL EXPOSURE
FREEDOM OF SPEECH MOVIES - % TOTAL EXPOSURE
SCIENCE FICTION MOVIES - % TOTAL EXPOSURE
AVERAGE[(MOVIE i RELEASE YEAR - 1990)×γj/100]

≥
≥
≥
≥
≥

10%
10%
10%
10%
0

5%
5%
5%
5%
0

1.5%
1.5%
1.5%
1.5%
0

(b) Topic characteristics and constraints on required % of exposure by topic in YOW news recommendations data set.

NO.

TR(AT P )

INEQ.

TOP 50 bk

TOP 500 bk

TOP 1000 bk % DOCS. IN DATA

1
2
3
4
5
6
7
8

SCIENCE AND TECH. - % TOTAL EXPOSURE
HEALTH - % TOTAL EXPOSURE
BUSINESS - % TOTAL EXPOSURE
ENTERTAINMENT - % TOTAL EXPOSURE
WORLD - % TOTAL EXPOSURE
POLITICS - % TOTAL EXPOSURE
SPORT - % TOTAL EXPOSURE
ENVIRONMENT - % TOTAL EXPOSURE

≥
≥
≤
≤
≤
≤
≤
≥

30%
20%
10%
10%
10%
10%
10%
5%

30%
20%
10%
10%
10%
10%
10%
5%

20%
15%
20%
20%
20%
20%
20%
2%

15.6%
9.6%
10.1%
14.1%
15.5%
9.2%
3.6%
1.9%

in online settings, without having to again solve the lin-
ear program, offering a substantial speed advantage. Note
that their solution does not guarantee the constraints are
met on per-user basis, but instead only on average across
many users. The idea of using sampling-based estimates of
shadow prices for online matching has also been discussed
in earlier works (Mehta, 2012). Our idea of replacing op-
timization with prediction builds on and generalizes this
stream of research. Speciﬁcally, instead of learning a single
set of shadow prices for a user population, we propose train-
ing a model to predict personalized optimal shadow prices
based on user covariates, capturing user heterogeneity.

6. Conclusion and future research directions

In this work, we propose a scalable algorithm for rank-
ing under constraints, based on a dual formulation of the
weighted bipartite matching program. Speciﬁcally, we pro-
pose to replace online optimization with prediction. We
solve for optimal shadow prices on a sample of users in
ofﬂine settings, where speed is not critical. We then train a
model to predict users’ optimal shadow prices from users’
characteristics. In online settings, the model can predict
the shadow prices based on observed covariates, without
solving the time-consuming optimization problem, allowing
us to quickly compute the ranking for any such user. We
show empirically that the proposed approximate solution
to the ranking problem leads to a substantial reduction in
required computing resources, meeting real-time 50 mil-

lisecond latency requirement, without much sacriﬁce in
constraint compliance and achieved utility, allowing us to
solve larger constrained ranking problems real-time than
previously reported (≥ 500 ranked objects and ≥ 5 con-
straints) (Zhernov et al., 2020). Our method thus enables
the deployment of the constrained ranking to new large-
scale problems, where latency matters. We elucidate the
role of rearrangement inequality / Monge problem structure
in achieving the speed-ups.

Future directions There is a place for more research on
fast unconstrained optimal assignment methods (eq. (1)),
which our proposed algorithm depends on when computing
ranking from the adjusted utility matrix S. The structuring
of the problem in terms of Monge matrices, a generalization
of rearrangement inequalities, speeds up the solution. How-
ever, most research has assumed arrays are precisely Monge.
This condition may be hard to satisfy, e.g., because of noise.
Efﬁcient statistical estimation of the best Monge approxima-
tion to a matrix that is not strictly Monge (H¨utter et al., 2020)
could enable faster approximate ranking on a larger problem
set, warranting more research. Outside of decision problems
with Monge structure, there have been promising results on
approximating the Hungarian algorithm solution with neu-
ral net predictions (Lee et al., 2018). This deep learning
approach to solving combinatorial optimization problems
deserves further attention. In general, the prediction of op-
timization solutions as a way to reduce computation time
appears to be a promising research direction.

Scaling up Ranking under Constraints for Live Recommendations

Acknowledgements

This work was supported The Sanford C. Bernstein & Co.
Center for Leadership and Ethics at Columbia Business
School.

References

Aggarwal, A., Bar-Noy, A., Khuller, S., Kravets, D., and
Schieber, B. Efﬁcient minimum cost matching using
In STACS, pp. 583–592. IEEE
quadrangle inequality.
Computer Society, 1992.

Ansari, A., Essegaier, S., and Kohli, R. Internet recommen-
dation systems. Journal of Marketing Research, 37(3),
2000.

Asudeh, A., Jagadish, H., Stoyanovich, J., and Das, G. De-
In ACM SIGMOD, pp.

signing fair ranking schemes.
1259–1276, 2019.

Avis, D. A survey of heuristics for the weighted matching

problem. Networks, 13(4):475–493, 1983.

Bandi, H. and Bertsimas, D. The price of diversity. arXiv

preprint arXiv:2107.03900, 2021.

Biega, A. J., Gummadi, K. P., and Weikum, G. Equity of
attention: Amortizing individual fairness in rankings. In
ACM SIGIR, pp. 405–414, 2018.

Birkhoff, G. Lattice theory, volume 25. American Mathe-

matical Soc., 1940.

Bougleux, S., Ga¨uz`ere, B., and Brun, L. A Hungarian
algorithm for error-correcting graph matching. In GbRPR,
pp. 118–127. Springer, 2017.

Chade, H., Eeckhout, J., and Smith, L. Sorting through
search and matching models in economics. Journal of
Economic Literature, 55(2):493–544, 2017.

Covington, P., Adams, J., and Sargin, E. Deep neural net-
works for YouTube recommendations. In ACM RECSYS,
pp. 191–198, 2016.

Derigs, U., Goecke, O., and Schrader, R. Monge sequences
and a simple assignment algorithm. Discrete Applied
Mathematics, 15(2-3):241–248, 1986.

Devroye, L., Gyorﬁ, L., Krzyzak, A., and Lugosi, G. On
the strong universal consistency of nearest neighbor re-
gression function estimates. The Annals of Statistics, pp.
1371–1385, 1994.

Diamond, S. and Boyd, S. CVXPY: A Python-embedded
modeling language for convex optimization. Journal of
Machine Learning Research, 17(83):1–5, 2016.

Duff, I. S. and Koster, J. On algorithms for permuting
large entries to the diagonal of a sparse matrix. SIAM
Journal on Matrix Analysis and Applications, 22(4):973–
996, 2001.

Dufoss´e, F. and Uc¸ar, B. Notes on Birkhoff–von Neumann
decomposition of doubly stochastic matrices. Linear
Algebra and its Applications, 497:108–115, 2016.

Edmonds, J. and Karp, R. M. Theoretical improvements in
algorithmic efﬁciency for network ﬂow problems. Jour-
nal of the ACM, 19(2):248–264, 1972.

Fortin, D. and Rudolf, R. Weak Monge arrays in higher
dimensions. Discrete Mathematics, 189(1-3):105–115,
1998.

Goodfellow, I., Bengio, Y., and Courville, A. Deep Learning.

Boyd, S. and Vandenberghe, L. Convex optimization. Cam-

MIT Press, 2016.

bridge University Press, 2004.

Bubeck, S. Convex optimization: Algorithms and complex-

ity. arXiv preprint arXiv:1405.4980, 2014.

Gusﬁeld, D. Handbooks in Operations Research and Man-
agement Science, volume 3, chapter 8: Design (with
Analysis) of Efﬁcient Algorithms. Elsevier, 1992.

Burkard, R., Dell’Amico, M., and Martello, S. Assignment

problems: Revised reprint. SIAM, 2012.

Hardy, G. H., Littlewood, J. E., and P´olya, G. Inequalities.

Cambridge University Press, 1952.

Burkard, R. E. Monge properties, discrete convexity and
applications. European Journal of Operational Research,
176(1):1–14, 2007.

Burkard, R. E., Klinz, B., and Rudolf, R. Perspectives
of Monge properties in optimization. Discrete Applied
Mathematics, 70(2):95–161, 1996.

Celis, L. E., Straszak, D., and Vishnoi, N. K. Ranking with
fairness constraints. arXiv preprint arXiv:1704.06840,
2017.

Holstermann, J. A generalization of the rearrangement
inequality. Mathematical Reﬂections, 5:503–507, 2017.

H¨utter, J.-C., Mao, C., Rigollet, P., Robeva, E., et al. Esti-
mation of Monge matrices. Bernoulli, 26(4):3051–3080,
2020.

J¨arvelin, K. and Kek¨al¨ainen, J. Cumulated gain-based evalu-
ation of IR techniques. ACM TOIS, 20(4):422–446, 2002.

Kingma, D. P. and Ba, J. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980, 2014.

Scaling up Ranking under Constraints for Live Recommendations

Kuhn, H. W. The Hungarian method for the assignment
problem. Naval Research Logistics Quarterly, 2(1-2):
83–97, 1955.

Kuhn, H. W. Variants of the Hungarian method for assign-
ment problems. Naval Research Logistics Quarterly, 3
(4):253–258, 1956.

Shah, P., Soni, A., and Chevalier, T. Online ranking with
constraints: A primal-dual algorithm and applications
to web trafﬁc-shaping. In ACM SIGKDD, pp. 405–414.
ACM, 2017.

Singh, A. and Joachims, T. Fairness of exposure in rankings.

In ACM SIGKDD, pp. 2219–2228. ACM, 2018.

Lee, M., Xiong, Y., Yu, G., and Li, G. Y. Deep neural
IEEE
networks for linear sum assignment problems.
Wireless Communications Letters, 7(6):962–965, 2018.

Tomizawa, N. On some techniques useful for solution of
transportation network problems. Networks, 1(2):173–
194, 1971.

Mehta, A. Online matching and ad allocation. Theoretical

Computer Science, 8(4):265–368, 2012.

Miller, R. B. Response time in man-computer conversational
transactions. In Proceedings of the Fall Joint Computer
Conference, pp. 267–277, 1968.

Mnih, A. and Salakhutdinov, R. R. Probabilistic matrix

factorization. In NeurIPS, pp. 1257–1264, 2008.

Nigsch, F., Bender, A., van Buuren, B., Tissen, J., Nigsch,
E., and Mitchell, J. B. Melting point prediction employ-
ing k-nearest neighbor algorithms and genetic parameter
optimization. Journal of Chemical Information and Mod-
eling, 46(6):2412–2422, 2006.

Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V.,
Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,
Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cour-
napeau, D., Brucher, M., Perrot, M., and Duchesnay, E.
Scikit-learn: Machine learning in Python. Journal of
Machine Learning Research, 12:2825–2830, 2011.

Preis, R. Linear time 1/2-approximation algorithm for max-
imum weighted matching in general graphs. In STACS,
pp. 259–269. Springer, 1999.

Radlinski, F., Kleinberg, R., and Joachims, T. Learning
diverse rankings with multi-armed bandits. In ICML, pp.
784–791, 2008.

Robertson, S. E. The probability ranking principle in IR.

Journal of Documentation, 1977.

Roth, A. E., Rothblum, U. G., and Vande Vate, J. H. Sta-
ble matchings, optimal assignments, and linear program-
ming. Mathematics of Operations Research, 18(4):803–
828, 1993.

Schrijver, A. Combinatorial optimization: Polyhedra and ef-
ﬁciency, volume 24. Springer Science & Business Media,
2003.

Sethumadhavan, S. A survey of Monge properties. PhD
thesis, Cochin University of Science and Technology,
India, 2009.

Vaidyanathan, B. Faster strongly polynomial algorithms for
the unbalanced transportation problem and assignment
problem with monge costs. Networks, 62(2):136–148,
2013.

Vince, A. A rearrangement inequality and the permuta-
hedron. The American Mathematical Monthly, 97(4):
319–323, 1990.

White, D. M. The requirement of race-conscious evalua-
tion of LSAT scores for equitable law school admissions.
Berkeley La Raza LJ, 12:399, 2000.

Yang, K. and Stoyanovich, J. Measuring fairness in ranked

outputs. In ACM SSDBM, pp. 1–6, 2017.

Zehlike, M., Bonchi, F., Castillo, C., Hajian, S., Megahed,
M., and Baeza-Yates, R. FA*IR: A fair top-k ranking
algorithm. In ACM CIKM, pp. 1569–1578, 2017.

Zhernov, A., Dvijotham, K. D., Lobov, I., Calian, D. A.,
Gong, M., Chandrashekar, N., and Mann, T. A. The Node-
Hopper: Enabling low latency ranking with constraints
via a fast dual solver. In ACM SIGKDD, pp. 1285–1294,
2020.

A. Problem structure for further speed up

We now discuss generalization of and alternatives to ﬁxed
discounting problem structure for speeding up maximum
weight assignment on the adjusted utility matrix S.

A.1. Monge structure

Rearrangement inequalities have been extended from a prod-
uct of sequence elements in eq. (5) to supermodular func-
tions of variable pairs (Holstermann, 2017; Vince, 1990;
Chade et al., 2017), where identity permutation still guaran-
tees optimal assignment. In terms of matrices that we work
with, this generalization is equivalent to an observation that
if S is an (inverse) Monge matrix, then optimal permutation
matrix P does not depend on S and has the form of an iden-
tity matrix P = Im×m (Sethumadhavan, 2009; Burkard
et al., 1996; 2012). A matrix U is called inverse Monge
when for 1 ≤ i1 < i2 ≤ m and 1 ≤ j1 < j2 ≤ m:

Scaling up Ranking under Constraints for Live Recommendations

U [i1, j1] + U [i2, j2] ≥ U [i1, j2] + U [i2, j1]. For u and
γ non-increasing, U = uγT is inverse Monge (Burkard,
2007).10 What conditions guarantee S = U + (cid:80)
k λkAk
(λk ≥ 0) is inverse Monge too? As we have shown, ﬁxed
discounting S = (u + (cid:80)
k λkak)γT ensures that S, after
sorting on the ﬁrst column, is represented as a dot product
of two non-increasing vectors and is thus inverse Monge, so
identity permutation extracts an optimal assignment. Ma-
trix that is Monge after sorting is called a permuted Monge
matrix (H¨utter et al., 2020).

When ﬁxed discounting is not available as a decision prob-
lem structure, we still have some options. An important
property of inverse Monge matrices is that they are closed
under several operations (Burkard et al., 1996). Consider
two inverse Monge matrices C, D ∈ Rm×m and two vec-
tors α, β ∈ Rm. Then the following matrices are inverse
Monge as well: (1) transpose C T ; (2) τ C for τ ≥ 0; (3)
sum C + D; and (4) matrix F , where Fij = Cij + αi + βj.
Properties (2) and (3) imply that a linear combination with
non-negative coefﬁcients of inverse Monge matrices is it-
self an inverse Monge array. Thus, if U and all Ak are
inverse Monge, then, because λk ≥ 0, S will also be inverse
Monge, so optimal assignment problem could be obtained
as an identity permutation (O(m) worst time complexity).
These properties enable efﬁcient solution of additional con-
strained ranking problems. For example, this framework
accommodates utility and/or constraint matrices of αi + βj
variety for arbitrary α, β ∈ Rm, which do not fall within
the ﬁxed discounting framework prevalent in the literature.
When Monge structure is not available, we can check if the
matrix is weak Monge (Fortin & Rudolf, 1998; Derigs et al.,
1986; Burkard et al., 1996) – the most general condition we
are aware of sufﬁcient for the identity permutation to yield
an optimal assignment.

A.2. Other approaches

Without Monge condition, we can use the greedy 1/2-
approximation algorithm (Avis, 1983; Preis, 1999; Gusﬁeld,
1992) with worst-case complexity O(m2) before using the
O(m3) Hungarian algorithm. However, as mentioned ear-
lier, the weight of its matching can be as low as 1/2 of the
maximum weight computed by the Hungarian algorithm.
There exist specialized conditions, known as box inequali-
ties, which are related to the Monge property, such that when
the matching weight matrix satisﬁes these conditions, the as-
signment obtained via the greedy algorithm is in fact optimal
– these have been successfully used to address the shortest su-
perstring problem in biology (Gusﬁeld, 1992; Burkard et al.,

10For 1 ≤ i1 ≤ i2 ≤ m and 1 ≤ j1 ≤ j2 ≤ m, let
u[i1] ≥ u[i2], γ[j1] ≥ γ[j2]. U = uγT . Inverse Monge property
u[i1]γ[j1] + u[i2]γ[j2] ≥ u[i1]γ[j2] + u[i2]γ[j1] is equivalent to
(u[i1] − u[i2])(γ[j1] − γ[j2]) ≥ 0, which always holds because
u[i1] ≥ u[i2] and γ[j1] − γ[j2] ≥ 0. Thus, U is inverse Monge.

2012). However, their specialized form makes them non-
trivial to apply in typical ranking-under-constraints settings.
There is also research on using neural nets to approximate
Hungarian algorithm optimal assignment – this approach
could be attempted if no special structure in the problem
is present, but a fast solution with complexity O(m2) or
O(m) (which depends on the neural net type) is desirable
(Lee et al., 2018).

B. Recommender system training

Recommender systems that we train on the movie and news
recommendation data sets to predict user-item utility are
of the matrix factorization ﬂavor (Mnih & Salakhutdinov,
2008), where the dot product operation from the traditional
matrix factorization framework is replaced with a neural
net computation, similar to Covington et al. (2016). Each
user and each item in the data set is assigned an embedding
vector of dimension 20 (user embeddings are denoted eu
and item embeddings are denoted ev). Additionally, each
user and each item is assigned a vector of dimension 5,
where each element is an intercept term for each of the
observed rating/relevance values: {1, 2, 3, 4, 5} (user and
item intercepts are, respectively, denoted gu and gv).

Based on these embedding and intercept vectors, for a given
user-item combination, the deep net outputs the probabilities
for each rating level. First, user and item embeddings are
concatenated into a single vector and then processed by a
neural net with a hidden layer of dimension 15, followed by
rectiﬁed nonlinearity and dropout of 0.1 (Goodfellow et al.,
2016), outputting 5 deterministic utilities corresponding to
different rating values. User and item intercept values are
added to these deterministic utilities, for the corresponding
ratings. Probabilities are obtained by passing the deter-
ministic utilities through a softmax transformation. Point
prediction is obtained as a probability-weighted sum of
{1, 2, 3, 4, 5} rating values.

The parameters of the neural net are optimized using Adam
mini batch gradient descent algorithm (Kingma & Ba, 2014)
with learning rate 0.01. We train the model for 5 epochs (5
passes through all training data) with a mini batch of size
200 (each iteration, a gradient update is computed based on
200 sampled observations). When training the net, we use
cross entropy loss based on predicted rating probabilities
and observed ratings to compute the gradients.

C. Algorithm performance

Tables 2 and 3 show analysis of algorithm performance.

Scaling up Ranking under Constraints for Live Recommendations

Table 2. Regression analysis of algorithm performance: MovieLens data (K = 5 constraints).

OPTIMAL LAMBDA (VS. KNEIGHBORS)

MEAN LAMBDA (VS. KNEIGHBORS)

NO OPTIMIZATION (VS. KNEIGHBORS)

PROBLEM SIZE: 1000 (VS. 50)

PROBLEM SIZE: 500 (VS. 50)

INTERCEPT

OBSERVATIONS
R2
ADJUSTED R2

NOTE:

PERFORMANCE METRIC

LOG10 COMP. TIME (MS.)
3.64∗∗∗
(3.59 , 3.69)
-0.39∗∗∗
(-0.41 , -0.37)
-1.28∗∗∗
(-1.31 , -1.24)
1.10∗∗∗
(1.06 , 1.14)
0.84∗∗∗
(0.80 , 0.88)
0.19∗∗∗
(0.16 , 0.22)

CONSTRAINT COMPL. PROB.
0.02∗∗∗
(0.01 , 0.03)
-0.15∗∗∗
(-0.16 , -0.14)
-0.82∗∗∗
(-0.83 , -0.80)
0.08∗∗∗
(0.07 , 0.10)
0.08∗∗∗
(0.07 , 0.09)
0.90∗∗∗
(0.89 , 0.92)

UTILITY

-0.00
(-0.79 , 0.78)
-0.00
(-0.79 , 0.78)
0.86∗∗
(0.08 , 1.65)
442.77∗∗∗
(442.05 , 443.50)
234.23∗∗∗
(233.82 , 234.63)
54.75∗∗∗
(54.26 , 55.23)

3,000
0.95
0.95

3,000
0.86
0.86

3,000
1.00
1.00
P<0.05; ∗∗∗

P<0.01
STANDARD ERRORS ARE HETEROSCEDASTICITY ROBUST (HC3)

P<0.1; ∗∗

∗

Table 3. Regression analysis of algorithm performance: YOW News data (K = 8 constraints).

OPTIMAL LAMBDA (VS. KNEIGHBORS)

MEAN LAMBDA (VS. KNEIGHBORS)

NO OPTIMIZATION (VS. KNEIGHBORS)

PROBLEM SIZE: 1000 (VS. 50)

PROBLEM SIZE: 500 (VS. 50)

INTERCEPT

OBSERVATIONS
R2
ADJUSTED R2

NOTE:

LOG10 COMP. TIME (MS.)
3.42∗∗∗
(3.09 , 3.75)
-0.37∗∗∗
(-0.49 , -0.25)
-1.31∗∗∗
(-1.65 , -0.98)
1.03∗∗∗
(0.70 , 1.36)
0.77∗∗∗
(0.46 , 1.08)
0.37∗∗∗
(0.14 , 0.60)

72
0.94
0.94

PERFORMANCE METRIC

CONSTRAINT COMPL. PROB.

UTILITY

0.07
(-0.02 , 0.16)
-0.00
(-0.10 , 0.10)
-0.34∗∗∗
(-0.49 , -0.19)
0.21∗∗∗
(0.12 , 0.30)
-0.03
(-0.14 , 0.08)
0.84∗∗∗
(0.75 , 0.93)

72
0.56
0.52

0.17
(-7.55 , 7.89)
0.02
(-7.72 , 7.76)
0.38
(-7.36 , 8.11)
435.53∗∗∗
(428.57 , 442.49)
231.51∗∗∗
(227.06 , 235.95)
55.70∗∗∗
(50.85 , 60.56)

72
1.00
1.00
P<0.05; ∗∗∗

P<0.01
STANDARD ERRORS ARE HETEROSCEDASTICITY ROBUST (HC3)

P<0.1; ∗∗

∗

