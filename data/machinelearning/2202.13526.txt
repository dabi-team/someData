Sparse Graph Learning with Eigen-gap for Spectral
Filter Training in Graph Convolutional Networks

Jin Zeng
Dept. of Software Eng.
Tongji University
Shanghai, China
zengjin@tongji.edu.cn

Saghar Bagheri
Dept. of EECS
York University
Toronto, Canada
sagharb@yorku.ca

Yang Liu
WICT
Peking University
Beijing, China
1800011037@pku.edu.cn

Gene Cheung
Dept. of EECS
York University
Toronto, Canada
genec@yorku.ca

Wei Hu
WICT
Peking University
Beijing, China
forhuwei@pku.edu.cn

2
2
0
2

b
e
F
8
2

]

G
L
.
s
c
[

1
v
6
2
5
3
1
.
2
0
2
2
:
v
i
X
r
a

Abstract—It is now known that the expressive power of graph
convolutional neural nets (GCN) does not grow inﬁnitely with
the number of layers. Instead, the GCN output approaches a
subspace spanned by the ﬁrst eigenvector of the normalized
graph Laplacian matrix with the convergence rate characterized
by the “eigen-gap”: the difference between the Laplacian’s ﬁrst
two distinct eigenvalues. To promote a deeper GCN architecture
with sufﬁcient expressiveness, in this paper, given an empiri-
cal covariance matrix ¯C computed from observable data, we
learn a sparse graph Laplacian matrix L closest to ¯C−1 while
maintaining a desirable eigen-gap that slows down convergence.
Speciﬁcally, we ﬁrst deﬁne a sparse graph learning problem with
constraints on the ﬁrst eigenvector (the most common signal)
and the eigen-gap. We solve the corresponding dual problem
greedily, where a locally optimal eigen-pair is computed one at
a time via a fast approximation of a semi-deﬁnite programming
(SDP) formulation. The computed L with the desired eigen-gap
is normalized spectrally and used for supervised training of GCN
for a targeted task. Experiments show that our proposal produced
deeper GCNs and smaller errors compared to a competing
scheme without explicit eigen-gap optimization.

Index Terms—Sparse graph learning, graph convolutional

networks, graph signal processing

I. INTRODUCTION

Given an underlying graph structure, graph convolutional
networks (GCN) [1] performs graph ﬁltering and point-wise
non-linear operation (e.g., ReLU) in a sequence of neural
layers for a range of tasks, such as graph signal interpola-
tion, denoising, and node classiﬁcation [1]–[3]. It has been
observed, however, that the expressive power of GCN does
not grow inﬁnitely with the number of layers; for a given task,
a GCN starts to oversmooth as the number of layers grows,
resulting in stagnant or even worsening performance [4], [5].
This undesirable phenomenon has been empirically observed
[6] and studied analytically recently [7], [8].

Speciﬁcally, [7] proved that the GCN output approaches a
subspace spanned by the ﬁrst eigenvector of the normalized
graph Laplacian matrix ˜L with the convergence rate character-
ized by the “eigen-gap”1: the difference between the ﬁrst and

Corresponding author: Wei Hu (forhuwei@pku.edu.cn). Gene Cheung ac-
knowledges the support of the NSERC grants RGPIN-2019-06271, RGPAS-
2019-00110.

1The relationship between convergence rate and eigen-gap of a matrix is
found also in Perron-Frobenius theorem for a discrete-time Markov chain [9]
and the power iteration method in numerical linear algebra [10].

second dominant eigenvalues2 of matrix P = I− ˜L—typically
the ﬁrst two eigenvalues of ˜L. Given that sparser graphs in
general have smaller eigen-gaps (e.g., a complete unweighted
graph has maximum eigen-gap of 2), [7] showed that for
random Erdö-Rényi graphs with edge probability p, sparser
graphs (smaller p) converge to the aforementioned subspace
at a slower pace. Similar in approach to achieve graph sparsity,
[8] randomly removed edges from a pre-chosen graph topology
in layers during training, resulting in more expressiveness in
the trained GCNs.

Orthogonally, graph signal processing (GSP) studies dis-
crete signals residing on combinatorial graphs [11], [12]. A
key assumption in GSP is that an underlying similarity graph
capturing pairwise correlations is available as input, before
spectral ﬁlters are designed and applied to signals on top
based on spectral graph theory [13]. Given a training set of
graph signal observations generated from the same statistical
model, there exist many graph learning algorithms [14]–[16]
that compute a most likely sparse inverse covariance matrix
(interpreted as a generalized graph Laplacian matrix), which is
subsequently used for GSP processing like compression [17],
[18], denoising [19], [20] and interpolation [21].

In this paper, leveraging a recent spectral graph learning
scheme [22], we learn a sparse graph from empirical data with
a tunable eigen-gap, in order to promote expressiveness in the
trained GCN and optimize performance. Unlike works [7],
[8] that indirectly induced eigen-gaps by sparsifying graphs
heuristically in the nodal domain, we directly engineer an
eigen-gap by optimizing one eigen-pair of the graph Laplacian
at a time in the spectral domain.

Speciﬁcally, we ﬁrst formulate a sparse graph learning
problem similar to graphical lasso (GLASSO) [14], but with
additional constraints on the ﬁrst eigenvector (the most com-
mon signal) and the eigen-gap. We solve the corresponding
dual problem greedily, where each locally optimal eigen-
pair is computed via a fast approximation of a semi-deﬁnite
programming relaxation (SDR) [23]. The learned graph Lapla-
cian L with the desired eigen-gap is normalized spectrally
and used for supervised training of GCN at a targeted task.
Experimental results show that when the pre-set eigen-gap was

2A dominant eigenvalue is the largest eigenvalue in magnitude.

 
 
 
 
 
 
smaller, the optimal number of GCN network layers increased.
Further, compared to [8], our computed graph achieved smaller
predictive error, since by creating an eigen-gap in the spectral
domain, we avoided random dropping of strong correlation
edges that are important for graph ﬁltering.

II. PRELIMINARIES

A. Graph Deﬁnitions

A graph G(V, E, W) is deﬁned by a set of N nodes V =
{1, . . . , N }, edges E = {(i, j)}, and an adjacency matrix W.
Wi,j ∈ R is the edge weight if (i, j) ∈ E, and Wi,j = 0
otherwise. Self-loops may exist, in which case Wi,i ∈ R+ is
the weight of the self-loop for node i. Degree matrix D has
j Wi,j , ∀i. A combinatorial graph
diagonal entries Di,i =
Laplacian matrix L is deﬁned as L , D − W, which is
positive semi-deﬁnite (PSD) for a positive graph [12]. If self-
loops exist, then the generalized graph Laplacian matrix L,
deﬁned as L , D − W + diag(W), is typically used.

P

B. Hilbert Space Deﬁnitions

We ﬁrst deﬁne a vector space S of real, symmetric matrices
in RN ×N . We next deﬁne an inner product h·, ·i for two
matrices A, B ∈ S as

hA, Bi = tr(B⊤A) =

Aij Bij.

(1)

Xi,j

Assuming Cauchy sequence convergence, the vector space
endowed with an inner product is a Hilbert Space H [24]. We
focus on a subspace H+ ⊂ H that contains PSD matrices, i.e.,
H+ = {A ∈ H | A (cid:23) 0}. It can be easily proven that H+ is
a convex cone [25].

C. Graph Convolutional Network

W , W + IN and

For a given graph G(V, E, W), a GCN [1] associated with
D ,
G is deﬁned as follows. Denote by
D + IN the adjacency and degree matrices augmented with
self-loops, respectively. The augmented normalized Laplacian
D−1/2, and we set
D−1/2
[26] is deﬁned by
P , IN −
L. Let L, C ∈ N+ be the layer and channel sizes,
respectively. With weights Θ(l) ∈ RC×C, l ∈ {1, . . . , L}, the
GCN is deﬁned by f = fL ◦ · · · ◦ f1 where fl : RN ×C 7→
RN ×C is deﬁned by fl(X) , ReLU(PXΘ(l)).

L , IN −

W

f

f

e

e

e

e

e

D. Convergence to Invariant Subspace

Several studies [4], [5] have reported that the expressive
power of GCN does not grow with layer number; node
representations become indistinguishable (known as over-
smoothing) when many layers are stacked in the GCN, leading
to performance degradation. To understand this undesirable
phenomenon, [7] provided theoretical conditions under which
the output of GCN exponentially converges to the invariant
subspace, which is the eigenspace corresponding to the lowest
frequency of the graph Laplacian matrix and has “no informa-
tion” other than connected components and node degrees.

Given deﬁnitions in Sec. II-C, let λ1 ≤ · · · ≤ λN be the
eigenvalues of P. Suppose G has M connected components,

deﬁne λ , maxn=1,...,N −M |λn| < 1 and λN −M+1 = · · · =
λN = 1. With initial value X0, the distance between the output
of l-th layer X(l) and the invariant subspace M satisﬁes

dM(X(l)) ≤ (sλ)ldM(X0),
(2)
where s , supl=1,...L sl and sl is the maximum singular value
of Θ(l). In particular, dM(X(l)) exponentially converges to
0 if sλ < 1. Thus, the gap between λ and λN should be
minimized to slow down convergence to the invariant subspace
and avoid oversmoothing.

III. SPECTRAL GRAPH LEARNING

We propose a spectral graph learning scheme to enhance
GCN’s expressiveness and optimize its performance. Given
an empirical covariance matrix ¯C estimated from data, we
compute matrix C that has a desired eigen-gap and a pre-
speciﬁed last eigenvector u. u is also the ﬁrst eigenvector of
to-be-constructed graph Laplacian L = C−1, and is set to
the mean signal of the training data—the most likely signal
assuming a Gaussian Markov random ﬁeld (GMRF) generation
model [27]. We ﬁrst describe a projection operator Proj( ¯C)
projecting ¯C onto a convex cone H+
u,κ of real symmetric
matrices sharing the same last eigenvector u with a desired
eigen-gap κ. We then use Proj(·) in a modiﬁed GLASSO
formulation [14] to derive an iterative optimization algorithm.
A. Computing Last Eigen-Pair (λN , u)

Given an empirical covariance matrix ¯C, we ﬁrst project ¯C
onto the rank-1 eigen-component U = uu⊤ to maximize the
inner product h ¯C, Ui. The resulting residual signal EN −1 is
EN −1 = ¯C − h ¯C, UiU.
(3)
It can be easily proven that h ¯C, Ui ≥ 0 [25]. Thus, the last
eigen-pair of C is (λN , vN ) = (h ¯C, Ui, u).
B. Computing Next Eigen-Pair (λN −1, vN −1)

Next, we compute the (N −1)-th eigen-pair (λN −1, vN −1).
We seek a vector vN −1 that maximizes the following inner
product:

vN −1 = arg max

v

hEN −1, vv⊤i,

s.t.

u⊤v = 0
kvk2 = 1

(cid:26)

. (4)

The constraints require eigenvectors of a real symmetric
matrix C to be orthonormal. Objective in (4) is equivalent to
tr(v⊤EN −1v), which is quadratic and convex, given EN −1
can be proven to be PSD [25]. Thus, the maximization (4) is
non-convex and NP-hard.

1) SDP Relaxation: Instead of solving (4) directly, we relax
it via semi-deﬁnite programming relaxation (SDR) [23] as
follows. Deﬁne ﬁrst V = vv⊤. Adding constraint tr(V) = 1
would imply kvk2 = 1. Rank-1 constraint V = vv⊤ implies
V (cid:23) 0 but not vice versa. Thus, by relaxing V = vv⊤ to the
less stringent V (cid:23) 0, we get the following SDR formulation:

max
V

hEN −1, Vi,

hU, Vi = 0
tr(V) = 1
V (cid:23) 0

s.t. 




.

(5)

(5) has a linear objective and a set of linear constraints plus
a PSD cone constraint, and thus is an semi-deﬁnite program-
ming (SDP) problem. SDR is commonly used to approximate
quadratically constrained quadratic programs (QCQP) [28],
where (4) is a special case. SDP can be solved in polynomial
time using off-the-shelf solvers [23]. Given computed solution
V to (5), we compute its rank-1 approximation vv⊤, where
v is the last eigenvector of V corresponding to the largest
eigenvalue. v is then the next eigenvector vN −1 for C.

2) Fast Approximation: Because matrix variable V in (5)
has size N × N , for large graphs this is still expensive. Thus,
we perform a fast approximation when N is large. We ﬁrst
approximate EN −1 with its rank-1 approximation ee⊤, where
e is the last eigenvector3 of EN −1. We then formulate the
following problem:

max
v

e⊤v,

s.t.

u⊤v = 0
kvk2
2 ≤ 1

(cid:26)

.

(6)

The ﬁrst constraint is the same as the ﬁrst constraint is (4).
The second constraint is a relaxation of kvk2 = 1 in (4), so
that the feasible space is a convex set.

Then, we rewrite the constrained problem (6) into the

corresponding unconstrained version:

min
v

−e⊤v + γv⊤uu⊤v + Φ1(v)

(7)

where γ > 0 is a penalty weight parameter, and Φ1(v) is a
convex function in v deﬁned as

Φ1(v) =

(cid:26)

if kvk2

2 ≤ 1

0
∞ o.w.

.

(8)

Denote by Θ(v) = −e⊤v + γv⊤uu⊤v. Thus, the objective
in (7) is composed of two functions: i) Θ(v) is convex and
differentiable w.r.t. v with gradient ∇Θ(v) = −e + 2γuu⊤v,
and ii) Φ1(v) is convex and non-differentiable. One can
thus optimize (7) using proximal gradient (PG) [30]. For
initialization, we set the ﬁrst solution to be v0 = e/kek2
2.

3) Compute Eigenvalue λN −1: Given vN −1, we compute

the corresponding eigenvalue λN −1 as

λN −1 = max

λN − κ, hEN −1, vN −1v⊤
(cid:0)

N −1i
(cid:1)

(9)

where κ > 0 is a chosen parameter so that there is a desired
eigen-gap between the last two eigenvalues λN and λN −1.
Residual signal is now EN −2 = EN −1 − λN −1vN −1v⊤
N −1.

C. Computing Remaining Eigen-Pairs (λi, vi)

Computing remaining eigen-pairs (λi, vi) for i ∈ {N −
2, . . . , 1} is a straightforward extension. Given residual signal
Ei = Ei+1 − λi+1vi+1v⊤
i+1, we ﬁrst compute its rank-1
approximation Ei ≈ ee⊤, where e is the last eigenvector
of Ei, again computed in linear time using LOBPCG. We
formulate a similar optimization to (7) as

min
v

−e⊤v + γv⊤YY⊤v + Φ1(v)

(10)

3Extreme eigenvectors can be computed efﬁciently using fast algorithms

like LOBPCG [29].

where Y ∈ RN ×(N −i) is composed of last eigenvector u and
previously computed eigenvectors {vj}N −1
j=i+1 as columns, i.e.,

Y =

u vN −1

. . . vi+1

.

(11)

Optimal solution vi to (10) can be computed using PG again.

(cid:2)

(cid:3)

The corresponding eigenvalue λi is

λi = min

λi+1, hEi, viv⊤
i i
(cid:0)
(cid:1)

.

(12)

(12) ensures that the computed eigenvalues satisfy λi ≤ λi+1.

D. GLASSO Formulation

The previous eigen-component computation constitutes a
projection C = Proj( ¯C)—one can prove this operator is in-
deed idempotent, i.e., Proj(Proj( ¯C)) = Proj( ¯C) [25]. Next, we
formulate the following GLASSO-like optimization problem
to estimate a graph Laplacian matrix L [31]:

min
L−1∈H+

u,κ

Tr(L ¯C) − log det L + ρ kLk1

(13)

where ρ > 0 is a shrinkage parameter for the l1-norm. The
only difference from GLASSO is that (13) has an additional
constraint L−1 ∈ H+
u,κ is a set of matrices with the last
eigenvector being u and the desirable eigen-gap κ.

u,κ. H+

We solve (13) iteratively using our projection operator
Proj(·) and a variant of the block Coordinate descent (BCD)
algorithm in [32]. Speciﬁcally, similarly done in [22], we solve
the dual of GLASSO as follows. Note ﬁrst that the l1-norm
in (13) can be written as

kLk1 = max

Tr(LU)

(14)

kUk∞≤1
where kUk∞ is the maximum absolute value element of the
symmetric matrix U. Hence, the dual problem of GLASSO
that seeks an estimated covriance matrix C = L−1 is

min
C∈H+

u,κ

− log det C,

s.t. kC − ¯Ck∞ ≤ ρ

(15)

where C = ¯C + U implies that the primal and dual variables
−1 [15]. To solve (15), we update
are related via L = ( ¯C + U)
one row-column pair of C in (15) in each iteration following
optimization procedure in [15].

Our algorithm to solve (13) is thus as follows. We minimize
the GLASSO terms in (13) by solving its dual (15)—iteratively
updating one row / column of C. We then project C to
H+
u,κ using our projection operator. We repeat these two steps
till convergence. Note that both steps are computed using
covariance C directly, and thus inversion to graph Laplacian
L = C−1 is not necessary until convergence, when we output
a solution.

IV. GRAPH CONVOLUTIONAL NETWORKS TRAINING
We employ the learned generalized graph Laplacian L as an
underlying topology to train GCNs [1]. The graph convolution
at the l-th layer outputs the following:

X(l+1) = ReLU(PX(l)Θ(l)),

(16)

where P is the normalized and self-loop-augmented adjacency
matrix, X(l) and Θ(l) are the input feature and network
parameters at the l-th layer.

To compute P, we need to normalize the given generalized
graph Laplacian L learned in Section III, so that the conver-
gence theorem in [7] is applicable. However, the normalization
adopted in GCN [1] is not applicable since it changes the order
of engineered eigen-gaps. Instead, we spectrally normalize L
via

(L − 1/λN IN ),

P = IN − ˆL, ˆL =

2
µmax
where 1/λN is the smallest eigenvalue of L, so that the
smallest eigenvalue of ˆL equals to 0 and the largest eigenvalue
of P equals to 1, then dM(X) does not vanish after graph
convolution. µmax is the maximum eigenvalue for all L with
different eigen-gaps, so that the eigenvalues of P is normalized
into range [0, 2], and the order of the eigen-gap is preserved.

(17)

V. EXPERIMENTS
We conducted experiments using the METR-LA [33] dataset
to validate our proposed scheme that relieves over-smoothing
and increases the optimal GCN layer number with smaller
eigen-gaps. Further, by comparing against recent proposal
DropEdge [8], our proposed scheme achieved larger optimal
layer numbers and lower prediction errors.

A. Dataset and Evaluation Metric
Dataset. The METR-LA dataset [33] used in the experiments
contains trafﬁc speed data in four months (from March 1st
2012 to June 30th 2012) from 207 sensors in the Los Angeles
County. The sensors sampled the speed data every 5 minutes.
Our task is to predict the current trafﬁc speed using historical
speed data from 50 minutes ago to 5 minutes ago as the input
feature. In our experiments, we randomly sampled 70% data
as training dataset, 20% as validation dataset and 10% as test
dataset.
Evaluation Metric. We use Mean Squared Error (MSE) as
our evaluation metric deﬁned as:

MSE(x, ˆx) =

N
i=1 (xi − ˆxi)2
N

P
where x = [x1, . . . , xN ] are the ground truth values, and ˆx =
[ˆx1, . . . , ˆxN ] are the predicted values.

(18)

B. Experimental Settings

For graph learning, we computed the empirical covariance
matrix ¯C with all the observations in the training data, and
used the mean signal in the training data as the ﬁrst eigenvector
of Laplacian L (last eigenvector u of computed covariance
C). Sparsity parameter ρ in (13) was set
to 10−4. The
GCN training was implemented in Pytorch and was optimized
by Adam [34]. Our GCN model was consisted of L GCN
blocks (L = [1, 9]) and two linear layers. Each GCN block
contained a graph convolutional layer, a BatchNorm layer and
an activation layer (leaky ReLU).

C. Comparison among Different Eigen-gaps

set

we

For

graph

learning,

eigen-gap
as
the
κ ∈ {1.0, 3.0, 5.0, 7.0, 8.0} and compared against
solution produced without eigen-gap modiﬁcation, which had
eigen-gap 8.5083. Fig. 1 shows the results for GCN training
using graph Laplacian matrices learned with eigen-gap κ and
the original eigen-gap. Results with 1 layer were omitted
since the values were too large. As shown in Fig. 1, smaller
eigen-gap relieved over-smoothing during the GCN training
and achieved larger optimal number of layers. For example,
the optimal result was achieved with
for eigen-gap 1.0,
layer 6, while for larger eigen-gaps, the optimal results were
reached at layer 1. The MSE results are also shown in Table I,
where the optimal results are highlighted in bold font.

0.0135

0.0130

0.0125

0.0120

s
s
o

l

0.0115

0.0110

0.0105

0.0100

0.0095

gap = 1.0
gap = 3.0
gap = 5.0
gap = 7.0
gap = 8.0
gap = 8.5038

1

2

3

4

5
num_layers

6

7

8

9

Fig. 1. MSE results with different layer sizes of GCN models using Laplacian
matrices learned with different eigen-gaps.

D. Comparison with State-of-the-art Methods

We compared our method with recent DropEdge [8]. Same
experimental settings were used for DropEdge with drop rate
p = {0, 0.3, 0.5}. The MSE results are shown in Fig. 2. We
observe that our method had better performance in terms of
slowing down over-smoothing and achieving lower prediction
error. Speciﬁcally, DropEdge achieved the optimal results
at
the second layer with MSE 0.0107, while our method
increased the optimal layer number to 6 with gap 1.0, and
achieved lower MSE 0.0094.

TABLE I
MSE RESULTS WITH DIFFERENT LAYER SIZE OF GCN MODELS USING
LAPLACIAN MATRICES LEARNED WITH DIFFERENT EIGEN-GAPS.

layers

gap = 1

gap = 3

gap = 5

gap = 7

gap = 8

gap = 8.5083

1
2
3
4
5
6
7
8
9

0.00953
0.00949
0.00948
0.0095
0.00954
0.0094
0.0097
0.00944
0.00969

0.00948
0.00955
0.0096
0.00982
0.00976
0.00965
0.00993
0.00967
0.01001

0.00954
0.00961
0.00992
0.00956
0.00991
0.00988
0.00983
0.01054
0.01012

0.00946
0.00968
0.00983
0.00981
0.00993
0.01054
0.01079
0.01084
0.0116

0.0096
0.00981
0.01032
0.01086
0.01156
0.01142
0.01209
0.01177
0.01347

0.01046
0.01021
0.01156
0.01205
0.01247
0.01268
0.01317
0.0125
0.01322

s
s
o

l

t
s
e
t

0.020

0.018

0.016

0.014

0.012

0.010

droprate = 0
droprate = 0.3
droprate = 0.5
gap = 1.0

1

2

3

4

5
num_layers

6

7

8

9

Fig. 2. MSE results with different layer sizes of GCN models using DropEdge
[8] with different drop rates, compared with our proposal with gap = 1.0.

VI. CONCLUSION

To improve the expressivenesss of a graph convolutional
network (GCN), we propose to learn a graph Laplacian
matrix from data in the spectral domain, such that a target
eigen-gap (the difference between the Laplacian’s two distinct
dominant eigenvalues) can be directly engineered. Speciﬁcally,
we compute each eigen-pair greedily via a fast approximation
of a semi-deﬁnite programming (SDP) formulation. The eigen-
gap slows down the inevitable convergence to the subspace
spanned by the ﬁrst eigenvector of the Laplacian, and leads
to more optimal depth layers (more expressiveness) and lower
loss function values during supervised GCN training. Com-
pared to a recent work [8] that drops edges randomly to
improve graph sparsity, our proposal produced deeper GCNs
and lower loss function values.

REFERENCES

[1] T. N. Kipf and M. Welling, “Semi-supervised classiﬁcation with graph
convolutional networks,” Proc. Int. Conf. Learn. Rep., April 2017.
[2] F. Hu, Y. Zhu, S. Wu, L. Wang, and T. Tan, “Hierarchical graph
convolutional networks for semi-supervised node classiﬁcation,” in
Proceedings of the Twenty-Eighth International Joint Conference on
Artiﬁcial Intelligence, (IJCAI), 2019.

[3] T. H. Do, D. M. Nguyen, and N. Deligiannis, “Graph auto-encoder
for graph signal denoising,” in ICASSP 2020-2020 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP).
IEEE, 2020, pp. 3322–3326.

[4] Q. Li, Z. Han, and X.-M. Wu, “Deeper insights into graph convolu-
tional networks for semi-supervised learning,” in Thirty-Second AAAI
conference on artiﬁcial intelligence, 2018.

[5] G. Li, M. Muller, A. Thabet, and B. Ghanem, “Deepgcns: Can gcns
go as deep as cnns?” in Proceedings of the IEEE/CVF International
Conference on Computer Vision, 2019, pp. 9267–9276.

[6] T. Kipf and M. Welling, “Semi-supervised classiﬁcation with graph
convolutional networks,” in International Conference on Learning Rep-
resentations, Toulon, France, April 2017.

[7] K. Oono and T. Suzuki, “Graph neural networks exponentially lose
expressive power for node classiﬁcation,” in International Conference
on Learning Representations, 2020.

[8] Y. Rong, W. Huang, T. Xu, and J. Huang, “Dropedge: Towards deep
graph convolutional networks on node classiﬁcation,” in International
Conference on Learning Representations, 2020.

[9] C. Meyer, Matrix Analysis and Applied Linear Algebra. SIAM, 2010.
[10] G. Golub and C. F. V. Loan, Matrix Computations (Johns Hopkins
Studies in the Mathematical Sciences). Johns Hopkins University Press,
2012.

[11] A. Ortega, P. Frossard, J. Kovacevic, J. M. F. Moura, and P. Van-
dergheynst, “Graph signal processing: Overview, challenges, and ap-
plications,” in Proceedings of the IEEE, vol. 106, no.5, May 2018, pp.
808–828.

[12] G. Cheung, E. Magli, Y. Tanaka, and M. Ng, “Graph spectral image
processing,” in Proceedings of the IEEE, vol. 106, no.5, May 2018, pp.
907–930.

[13] F. Chung, Spectral Graph Theory. American Mathematical Society,

1996.

[14] J. Friedman, T. Hastie, and R. Tibshirani, “Sparse inverse covariance
estimation with the graphical lasso,” Biostatistics (Oxford, England),
vol. 9, pp. 432–41, 08 2008.

[15] O. Banerjee and L. Ghaoui, “Model selection through sparse max
likelihood estimation,” Journal of Machine Learning Research, vol. 9,
08 2007.

[16] H. Egilmez, E. Pavez, and A. Ortega, “Graph learning from data under
Laplacian and structural constraints,” in IEEE Journal of Selected Topics
in Signal Processing, vol. 11, no.6, July 2017, pp. 825–841.

[17] W. Hu, G. Cheung, A. Ortega, and O. Au, “Multi-resolution graph
Fourier transform for compression of piecewise smooth images,” in IEEE
Transactions on Image Processing, vol. 24, no.1, January 2015, pp. 419–
433.

[18] W. Hu, G. Cheung, and A. Ortega, “Intra-prediction and generalized
graph Fourier transform for image coding,” in IEEE Signal Processing
Letters, vol. 22, no.11, November 2015, pp. 1913–1917.

[19] J. Pang and G. Cheung, “Graph Laplacian regularization for inverse
imaging: Analysis in the continuous domain,” in IEEE Transactions on
Image Processing, vol. 26, no.4, April 2017, pp. 1770–1785.

[20] C. Dinesh, G. Cheung, and I. V. Baji´c, “Point cloud denoising via feature
graph laplacian regularization,” IEEE Transactions on Image Processing,
vol. 29, pp. 4143–4158, 2020.

[21] F. Chen, G. Cheung, and X. Zhang, “Fast amp; robust

image in-
terpolation using gradient graph laplacian regularizer,” in 2021 IEEE
International Conference on Image Processing (ICIP), 2021, pp. 1964–
1968.

[22] S. Bagheri, G. Cheung, A. Ortega, and F. Wang, “Learning sparse graph
Laplacian with K eigenvector prior via iterative glasso and projection,”
in ICASSP 2021 - 2021 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), 2021, pp. 5365–5369.
[23] B. Gartner and J. Matousek, Approximation Algorithms and Semideﬁnite

Programming. Springer, 2012.

[24] M. Vetterli, J. Kovavcevi´c, and V. Goyal, Foundations of Signal
Processing. Cambridge University Press, 2014. [Online]. Available:
https://books.google.ca/books?id=5WA5nQAACAAJ

[25] S. Bagheri, “Learning sparse graph Laplacian with K eigenvector
prior via iterative glasso and projection,” in Yorkspace Library, 2021.
[Online]. Available: http://hdl.handle.net/10315/38692

[26] F. Wu, A. Souza, T. Zhang, C. Fifty, T. Yu, and K. Weinberger,
“Simplifying graph convolutional networks,” in Proceedings of the 36th
International Conference on Machine Learning.
PMLR, 2019, pp.
6861–6871.

[27] H. Rue and L. Held, Gaussian Markov random ﬁelds: theory and

applications. CRC Press, 2005.

[28] Z.-q. Luo, W.-k. Ma, A. M.-c. So, Y. Ye, and S. Zhang, “Semideﬁnite
relaxation of quadratic optimization problems,” IEEE Signal Processing
Magazine, vol. 27, no. 3, pp. 20–34, 2010.

[29] A. Knyazev, “Toward the optimal preconditioned eigensolver: Locally
optimal block preconditioned conjugate gradient method,” SIAM journal
on scientiﬁc computing, vol. 23, no. 2, pp. 517–541, 2001.

[30] N. Parikh and S. Boyd, “Proximal algorithms,” in Foundations and

Trends in Optimization, vol. 1, no.3, 2013, pp. 123–231.

[31] R. Mazumder and T. Hastie, “The graphical

lasso: New insights
and alternatives,” Electron. J. Statist., vol. 6, pp. 2125–2149, 2012.
[Online]. Available: https://doi.org/10.1214/12-EJS740

[32] S. Wright,
151,

“Coordinate
vol.
1,
no.
https://doi.org/10.1007/s10107-015-0892-3

descent
3–34,

pp.

algorithms,” Math. Program.,
[Online]. Available:

2015.

[33] Y. Li, R. Yu, C. Shahabi, and Y. Liu, “Diffusion convolutional recurrent
neural network: Data-driven trafﬁc forecasting,” in International Con-
ference on Learning Representations (ICLR ’18), 2018.

[34] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
in International Conference on Learning Representations, 2015.

 
