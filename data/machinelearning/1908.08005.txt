Consistent Feature Construction with Constrained
Genetic Programming for Experimental Physics

No¨elie Cherrier∗†, Jean-Philippe Poli∗, Maxime Defurne† and Franck Sabati´e†
∗CEA, LIST, 91191, Gif-sur-Yvette cedex, France.
†Irfu, CEA, Universit´e Paris-Saclay, 91191, Gif-sur-Yvette cedex, France.

9
1
0
2

g
u
A
7
1

]
E
N
.
s
c
[

1
v
5
0
0
8
0
.
8
0
9
1
:
v
i
X
r
a

Abstract—A good feature representation is a determinant
factor to achieve high performance for many machine learning
algorithms in terms of classiﬁcation. This is especially true for
techniques that do not build complex internal representations of
data (e.g. decision trees, in contrast to deep neural networks).
To transform the feature space, feature construction techniques
build new high-level features from the original ones. Among
these techniques, Genetic Programming is a good candidate
to provide interpretable features required for data analysis in
high energy physics. Classically, original features or higher-level
features based on physics ﬁrst principles are used as inputs for
training. However, physicists would beneﬁt from an automatic
and interpretable feature construction for the classiﬁcation of
particle collision events.

Our main contribution consists in combining different aspects
of Genetic Programming and applying them to feature construc-
tion for experimental physics. In particular, to be applicable to
physics, dimensional consistency is enforced using grammars.

Results of experiments on three physics datasets show that
the constructed features can bring a signiﬁcant gain to the
is
classiﬁcation accuracy. To the best of our knowledge,
the ﬁrst time a method is proposed for interpretable feature
construction with units of measurement, and that experts in
high-energy physics validate the overall approach as well as the
interpretability of the built features.

it

Index Terms—feature construction, grammar-guided genetic

programming, high-energy physics, interpretability

I. INTRODUCTION

The performance of many supervised machine learning
(ML) algorithms depends on the list of features provided with
the training examples. Inappropriate representation of data
may indeed lead to limited performance, while carefully se-
lected features may improve the performance of the subsequent
learning algorithm [1]. This is especially true for algorithms
that have a limited ability to build an optimal internal represen-
tation of data. For instance, classiﬁers such as decision trees
or more generally rule extraction algorithms represent the data
as hyper-rectangles, which is not optimal in many applications
but has the advantage of producing explainable outputs. In
contrast, “black box” algorithms such as deep neural networks
build a much more complex internal representation of data,
which is often more efﬁcient but discards any explanation
for the ﬁnal output [2]. Yet the readability of features is a
requirement in several applications. A full transparency of the
model is for instance essential to its acceptance in sensitive
applications such as national security, medical diagnoses or
banking activities. Additionally,
the readability of features
enables supervision and re-usability by experts of the ﬁeld,

which is needed in particular in the case of high-energy
physics (HEP) experiments. We are interested in particular
HEP experiments: using different accelerator facilities across
the world, particles are collided with each other to probe
their internal structure and/or search for physics beyond the
Standard Model. Detectors are placed around the collision site
for the reconstruction of the ﬁnal state of the collision, i.e.
the determination of the charge, mass and momentum of all
particles after collision.

We focus here on distinguishing signal events from back-
ground in several HEP experiments. The algorithms must be
trained on labeled Monte-Carlo simulated data, to be later
applied to unlabeled real data from the experiments. Although
the simulation programs are very sophisticated, they may not
exactly reﬂect the reality (background processes may not be
all included, modeled physics are not exactly the same, etc.).
An interpretable classiﬁcation model is therefore preferable
to better analyze the predictions on real data. Based on the
use of physics ﬁrst principles such as momentum and energy
conservation, physicists manually add high-level variables to
the initial set of input features to improve the classiﬁcation,
whether it is based or not on machine learning algorithms.
Automating the construction of such features would enable the
discovery of new variables of interest that are interpretable
to the experts. One would be able to build an entirely
interpretable classiﬁer, which permits a deeper performance
analysis while applying the trained classiﬁer to real unlabeled
data.

We develop an algorithm derived from Genetic Program-
ming (GP) to build automatically high-level features that are
consistent with physics laws and that help the classiﬁcation
task. Next section reviews existing techniques for feature
construction, and then details methods to constrain GP. The
section afterwards presents our derived GP method adapted to
experimental physics applications. Experiments and compar-
isons are conducted then. We ﬁnally conclude the paper and
give perspectives.

II. RELATED WORK

A. Feature construction

Feature engineering is the general processing step that
transforms the feature space to make it better suited to the
learning task desired output. Several methods perform feature
engineering indirectly, like Support Vector Machines (SVM)
[3] or Deep Neural Networks [4]. Dimensionality reduction

©2019 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including
reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or
reuse of any copyrighted component of this work in other works. DOI: 10.1109/CEC.2019.8789937

 
 
 
 
 
 
algorithms such as Principal Component Analysis or Linear
Discriminant Analysis also build new features which are often
complex non-linear combinations of the primary ones and are
not interpretable.

Both feature selection and feature construction methods
split into three categories. Firstly, ﬁlters rank the features
independently of any classiﬁer. Secondly, wrappers use the
score obtained by a classiﬁcation algorithm. Finally, embedded
methods gather algorithms that perform feature construction
at the same time as the optimization, such as SVM. We are
interested only in the ﬁrst two categories.

A review of several feature construction methods can be
found in [5]. The methods detailed in the rest of this sec-
tion construct new numerical features according to a list of
allowed transformations on the dataset. The new features are
combinations of operators applied on the original features, and
can consequently be represented by trees in which the nodes
are the transformations and the leaves are the original fea-
tures. These numerical feature construction methods received
increased attention in the past years.

A number of methods use tree-based algorithms to explore
the search space. For instance, FICUS [6] is a general frame-
work for feature construction, generalizing several previous
works in the ﬁeld. It searches new numerical features using
information gain in a decision tree, given an input dataset
and a list of allowed transformations. Likewise, Cognito [7]
is based on a transformation tree aiming at recommending
a series of transformations. The tree is explored depending
on the current performances obtained within the branches.
In [8], the authors continue this work using reinforcement
learning to learn the optimal exploration strategy. However,
this last method requires to train with several datasets before
application of the optimal strategy on the desired dataset.

Another class of feature construction methods gathers evo-
lutionary methods. Although Particle Swarm Optimization
(PSO) is already widely used in feature selection, few works
use PSO for feature construction. Reference [9] associates one
particle with one constructed feature, the dimensionality being
the number of original features. However, this method does
not include the choice of operators to combine the features.
Reference [10] expands the dimensionality to include the
selection of operators. They propose two representations: pair
and array representation, to enable the evolution to optimize
the operators as well as the features used. However, the space
of features that can be constructed with these methods is
limited, since only binary operators can be included. Moreover,
the algorithm combines the features in a way that one cannot
represent a constructed feature by a balanced binary tree.

Genetic Programming (GP) is a popular method among the
feature construction methods. A survey on applications of GP
to classiﬁcation can be found in [11]. GP is an evolutionary
computation technique that evolves tree representations of
computer programs [12]. In the context of feature construction,
this refers to a combination of several features thanks to a list
of allowed operators (e.g. +, −, ×, ÷). The main steps are as
follows:

1) Start from a random initial population
2) Evaluate its individuals with some ﬁtness function
3) Loop until termination criterion:

a) Perform selection
b) Perform mutation and crossover to get to the next

generation of individuals
c) Evaluate new individuals
4) Return individual with highest ﬁtness

In wrapper methods, the ﬁtness of the individual representing
an evolved feature set is the score of a predictive model using
these features [13]. In ﬁlter methods, the ﬁtness measure can
be for instance the information gain [14], [15], entropy [16],
or the Fisher criterion [17]. A comparison of wrapper and
ﬁlter approaches in the case of genetic programming is drawn
in [18]. GP based feature construction methods can construct
one feature to add to the original feature set [14], [19], or
several at a time [13], [20], etc.

Finally, Grammatical Evolution (GE), very similar to GP,

has been successfully applied to feature construction [21].

We choose to adapt GP algorithm to our problem. Its main
asset is its ﬂexibility: one can build one or several features
at a time, choose the maximum complexity of the features,
etc. Furthermore, there is abundant literature about techniques
to add constraints to GP such as the respect of dimensional
consistency (of physical units and of the feature dimensions).
Several techniques have been developed to incorporate domain
knowledge to constrain the evolution phase.

B. Constrained Genetic Programming

Our goal is to come up with high-level features that respect
physics laws, in order to be interpretable and reusable by
the experts. A review underlines the difﬁculty to incorporate
domain knowledge in feature construction methods [5].

In his early work in GP, Koza [12] uses syntactic constraints
to enforce the production of valid individuals. He imposes a
root operator, or restrict the crossover operation according to
the operator types for instance. Standard GP actually requires
closure, namely that any individual tree can be considered as a
subtree of another tree. To keep valid individuals during the en-
tire evolution when constraining the construction process, one
must modify the evolution functions. In Strongly Typed GP
(STGP) [22], [23], the crossover and mutation operators are
restricted according to the constraints put on the construction
process. The author of [22] deﬁnes type labels and associates
one to each original feature. Every operator has then a list of
accepted data types (e.g. vectors, matrices, angles, etc.) and a
return type.

Constraints in GP can also be expressed by grammars: the
technique is called Grammar-Guided Genetic Programming
(GGGP) [24]. In GGGP, an individual is a tree derived from
the grammar, called a derivation tree, from which one can infer
a GP-regular expression tree. The evolution uses the same
operators than in standard GP, but applies on the derivation
trees instead of the expression tree. However, the crossover
and mutation operators are constrained so that the offspring
respects the grammar.

Reference [25] states that Context-Free Grammar (CFG)
reduce the search space by expressing syntactic constraints
on admissible individuals. Industrial applications begin to
appear at this point. In [26], the authors perform a multi-
objective evolution to favor dimensionally consistent solutions,
but non-valid individuals are still present in the population.
In [27], a CFG constrains the evolution to produce only
individuals representing dimensionally valid expressions, to
discover empirical physics laws from numerical experiments.
Both applications modify the initialization procedure, which
can raise an error if there is not a terminal (i.e. leave of the
tree, which can be either an original feature or a constant) for
each type.

Another way of using grammars in the evolution is linear
GGGP. The individuals are represented as strings of integers,
which encode a derivation sequence from the grammar. The
derivation leads to a derivation tree and then to a syntax tree.
The evolution is performed on the string of integers with
a genetic algorithm. The most widely used linear GGGP is
Grammatical Evolution (GE) [28]. Several articles use GE for
feature construction [21], [29], [30]. They deﬁne a simple
grammar including the operators (e.g. +, −, ×, ÷) used in
the construction. However, they do not take advantage of the
potential of grammars to enforce constraints on the shape of
the tree or on the validity of the use of certain operators on
some variables. Their grammar is called a universal grammar.
For physics applications, the domain knowledge involves
in particular dimensionality analysis, to ensure that the con-
structed features are dimensionally consistent. For this pur-
pose, dimensional analysis can be expressed as a CFG, as
proven in [27]. The authors of [31] use symbolic regression,
to extract physics laws from
containing a GP algorithm,
experimental data. However,
they ensure the dimensional
consistency of the built expressions the well-chosen scalar
parameters without the use of grammars.

Finally, a number of methods use Probabilistic Model-
Building Algorithms (PMBA) [32]. The idea is to maintain
a distribution estimation over the search space. An initial
probability distribution is deﬁned on the grammar, and then
the ﬁtnesses of the individuals are used to update these
probabilities [33]. Their objective is to guide the search.

To deal with HEP applications, we propose to adapt the

grammar of [27] to handle dimensional consistency.

III. GENETIC PROGRAMMING ADAPTED TO
EXPERIMENTAL PHYSICS

In this section, we detail how to take advantage of the
methods listed in the previous part to come up with a feature
construction algorithm adapted to HEP applications. Firstly,
we focus on the deﬁnition of a grammar suitable for di-
mensional consistency of physical quantities, and the coupled
probability distribution. Then we detail the evolution methods
for our Probabilistic GGGP algorithm.

A. Grammar deﬁnition

In experimental physics, particle detectors measure numer-
ical values that are then reconstructed into variables such as

< s t a r t > : : = <E> | <A> | <F>

<E> : : = <E> + <E> | <E> − <E> | <E> * <F>

| <E> / <F> | S q r t (<E2>)

| <termE>

<A> : : = <A> + <A> | <A> − <A> | <A> * <A>
| A s i n (<F>)

| Atan (<F>)

| Acos(<F>)
| <termA>

<F> : : = <F> + <F> | <F> − <F> | <F> * <F>
| <E> / <E> | <A> / <A> | <F> / <F>
| Cos(<A>)
| <termF>

| S i n (<A>)

| Tan(<A>)

<E2> : : = <E2> + <E2> | <E2> − <E2>

| <E2> * <F> | <E2> / <F>
| <termE2>
| S q u a r e (<E>)

Fig. 1: Production rules of the grammar used for Higgs
dataset (E: Energy; E2: squared Energy; A: Angle; F: Float).
<termX> means either a constant or base feature of type X.

momenta (M eV /c), energies (M eV ), angles (rad), durations
(ns), lengths (cm), etc. Features constructed by a GP algorithm
should be dimensionally consistent to be independent of the
system of measurement of the base features, and to ensure their
interpretability. For instance, combinations such as adding or
subtracting energies and lengths are forbidden.

We deﬁne a CFG similar to the one in [27], while adding
additional restrictions on the types that can be mixed. For
instance, we do not allow the creation of any combination
of energies and lengths, or angles and durations, etc., since
these combinations are less common in the HEP domain.
Figure 1 shows an example of the grammar used for one
particular dataset in our study. For the sake of comprehension,
this grammar is a simpliﬁed version of the one used in the
experiments. The GGGP can be seen in our case as a STGP,
the types being the ones described in the grammar: a ﬁxed
number of types such as Energy, Angle, Distance, Float, etc.
In our study, the operations used are the same between the
different datasets, and are chosen among standard operators
used in HEP.

B. Transition matrix deﬁnition

We go further in the guidance of the evolution process to
obtain physically interpretable and consistent features. HEP
theory indeed involves a number of typical formulas. For
instance, one can study the conservation of energy and mo-
mentum in a collision of particles to infer the intermediate
states, or reconstruct the collision vertex with geometrical
operations. An automated classiﬁer could beneﬁt from such
useful features. An idea is then to guide the construction of
the trees through the choice of a probability distribution over
the grammar, to favor the construction of formulas that are
similar to those used in HEP. We choose not to update the
probabilities during the evolution contrary to the strategy in
[33], to avoid converging too quickly and to guide the search to
build physically sound features by following recurrent patterns
in physics formulas.

TABLE I: Snippet of the transition matrix: the probability to
choose an operator among the rows according to the previous
operator in the columns.

Energy
−
0.225
0.225
0.25
0.2
0.1

+
+ 0.1
− 0.1
× 0.1
÷ 0.1
√
0.6

+
−
×
2

y
g
r
e
n
E

2
y
g
r
e
n
E

√

Energy2
−

+

2

0.8
0.1
0.07
0.03
0

0.7
0.25
0.05
0

0.4
0.15
0.05
0.4

0.2
0.07
0.03
0.7

(a)

(b)

Fig. 2: (a) Example of derivation tree and (b) Example of
expression tree.

More speciﬁcally, we put constraints on the transitions
between the operators. For instance, a square root in physics
is very often followed by a sum of squares. In this way, we
also forbid a square operator followed by a square root and
conversely, to simplify the trees. This particular constraint
could actually appear in the grammar itself, but it would lead
to a more complex and less readable grammar. A probability
transition matrix between a few operators is shown in Table I.
To this matrix one must add the initial probability distribution
Pinit for the choice of the ﬁrst operator. For instance, a
square root of a squared energy is much more likely to
appear in HEP formulas than the division of a time by a
cosine. The probabilities are chosen manually to emulate the
frequencies of operators in HEP formulas. In the experiments,
we compare the chosen probabilities to the uniform case where
all operations can be selected with the same probability.

C. Evolution methods

From the grammars and transition probabilities, one can de-
rive a derivation tree and then the standard GP expression tree.
Examples of a derivation tree and its associated expression tree
for HEP are displayed on Figure 2.

We actually combine both techniques of GE and GP by
taking advantage of standard GP evolution methods applied
to expression trees, while adding a few methods replicating

the effects of those in GE. For single feature construction,
an individual
is one expression tree. For multiple feature
construction, an individual is a ﬁxed-length list of n > 1
expression trees.

An initial population is ﬁrstly evaluated, then evolved ac-
cording to a (µ + λ)-strategy. For each individual in the pop-
ulation, mutation can be applied with a probability Pmutation
and crossover with a probability Pcrossover. The offspring is
then evaluated, and the selection is performed over the whole
parent population and the offspring, which differs from the
standard (µ + λ)-strategy but has the advantage of keeping in
the population some efﬁcient features.

In the following paragraphs, we detail the evolution methods
used in our work for both single and multiple feature construc-
tion. In the case of single feature construction, an individual is
a single tree. For multiple feature construction, an individual
is represented by a list of n trees.

1) Initialization: Whether for constructing one tree (i.e.
one feature) or a ﬁxed number n > 1 of trees (i.e. n
features), the trees are built the same way: they are generated
using the ramped half and half initialization [12]. The process
of choosing the operators is however altered to respect the
grammars. A type t and the ﬁrst operator (returning type t) are
selected under the Pinit distribution. Then, while the condition
on current depth is not satisﬁed, the tree keeps growing. The
possible operators are selected according to the grammar. The
transition matrix then deﬁnes a distribution probability among
the remaining possible operators according to the parent node.
Finally, the leaves of the tree are randomly chosen among the
set of base features of the proper type and constants. After the
population is initialized, the evolution process begins with a
series of mutations, crossovers, evaluations and selections.

2) Mutation and crossover: Both crossover and mutation
apply on the expression trees themselves, whether several
features are constructed at the same time or only one. Re-
gardless of the applied transformation, a node is picked in
the tree following a probability distribution that depends on
the previous scores achieved by the node in its tree. At each
generation, a weight is indeed assigned to each node of every
tree in the population. Each node contains the information
of its best gain on accuracy obtained within the tree, even if
other branches of the tree changed. The list of the gains is
then inversed and normalized to get the weights used to select
the node during mutation/crossover. In this way, the weights
sum to 1, with a small shift to let a chance for the best nodes
to be mutated anyway. When mutating or mating trees, the
weights are used to select the less efﬁcient nodes with higher
probability.

The mutation method is randomly picked among three
existing techniques. Each of these techniques is modiﬁed to
be compliant with the grammars:

• Uniform mutation: a node is selected in the tree, then
the subtree is entirely regenerated from that node while
making sure that
the dimensional consistency is still
respected in particular at the root.

startESqrtE2pt_taupt_tauE2+E2SquareESquareEE/FtanAtheta_tau+SquareSquare/pt_taupt_tautantheta_tauSqrt• Node replacement: a node is selected and replaced with

any dimensionally compliant node.

• Insertion: from a selected node, a new subtree is inserted
that have the original subtree(s) of the mutated node as
child nodes. The inserted subtree is generated so that the
grammar is also respected at the connections with the
original tree.

The transition matrix is used each time a new tree or subtree
needs to be generated to support the interpretability.

The crossover operation is the standard GP one-point
crossover, assuming that the exchange of the two subtrees is
compatible with the grammar, i.e. that the two roots of the
subtrees are of the same type.

In multiple feature construction, mutation is applied on one
or several features of the selected individual. Crossover is
the standard genetic algorithm one-point crossover, with no
modiﬁcation of the trees themselves.

The individuals created through these operations must then
be evaluated and selected with the parent population to form
the next generation.

3) Evaluation and selection: A repeated tournament selec-
tion among three individuals constitutes the next generation
population. The tournament size of three was chosen exper-
imentally as a good compromise between randomness and
quick convergence.

To evaluate an individual, expression trees are converted to
numerical features, by computing the function they represent.
In our implementation, a division by zero would create a
missing value. If the feature is not well constructed and
the whole column is invalid, then the individual is given a
ﬁtness score of zero. Two main approaches exist to evaluate
individuals in feature construction. Wrapper methods evaluate
the score (e.g. the accuracy) of a classiﬁer trained on the set
containing the constructed feature(s). In this case, the score is
obtained through a k-fold cross-validation on the training set.
Filter methods use ranking measures that are independent of
any predictor.

In the experiments, we compare among others the perfor-
mances obtained with several classiﬁers and with one ﬁlter
ﬁtness measure.

IV. EXPERIMENTS

In this section, we present the datasets used for the experi-
ments, the experimental settings, and then several comparisons
of methods and settings. Finally, we discuss the results.

A. Datasets

We use three datasets taken from three different and unre-
lated experiments. The Higgs dataset is unbalanced, while the
two other datasets are more balanced.

1) Higgs dataset: ATLAS is a particle physics experiment
at CERN. In 2014, the collaboration opened a Kaggle chal-
lenge1 with the objective to automatically detect the Higgs
boson decaying into two τ -leptons in Monte-Carlo simulated

data. The data has then been publicly released on the Open
Data platform of CERN2. The dataset [34] consists in more
than 800000 simulated events, including about 280000 signal
(positive) instances, the rest being background. 17 primitive
features per event are available, including notably the trans-
verse momentum pT (in the xy-plane), φ angle (between the
x and y-axes) and θ angle (between the z-axis and the xy-
plane) of several particles. Besides, we remove 13 high-level
features also provided in the dataset since our objective is to
automatically make new features from the basic ones.

2) DVCS dataset: The experimental Hall B of the Jeffer-
son Laboratory conducts experiments on nuclear and particle
physics. The CLAS12 collaboration notably studies Deeply
Virtual Compton Scattering (DVCS), to probe the internal
structure of the proton. The objective of this dataset is to dis-
criminate between two event types: DVCS and π0-production
acting as background for the DVCS study. The dataset consists
in Monte-Carlo simulated data. We train on about 25000
examples including 15000 DVCS (signal) events and the rest
pion production events. The events are selected so that exactly
three particles have been detected. Features of these three
particles include their 3-momentum, vertex position, charge,
deposit energy, etc. A total of 36 features is available to train
the model.

3) τ → µ + µ + µ dataset (τ µ3): The Large Hadron
Collider beauty experiment (LHCb) searches for new physics
at CERN. A Kaggle challenge3 took place in 2015 to design
a model able to ﬁnd the decay τ − → µ+µ−µ− among
background events. The particularity of this dataset is that
signal events are simulated data, since this decay is not
supposed to happen according to the Standard Model [35].
Observation of this decay would mean a violation of the latter
and consequently a sign of new physics. Background events
are taken from real experiments at LHCb. The total dataset
includes more than 67000 instances of which 40000 are signal
events. 46 features describe each event, including features on
the three muons and the reconstructed τ -lepton. Features of
the latter are therefore removed since they are derived from
the three muons. It should be noted that the dataset originally
comes with an agreement dataset and a correlation dataset, to
ensure a few constraints are respected for the further physics
analysis. We do not use these side datasets in our work.

B. Experimental settings

We systematically present the results of our experiments
by showing the mean and standard deviation over at least 20
independent runs for each method and dataset. The settings are
as follows: we evolve a population of 500 individuals over 150
generations. The individuals can consist in one or several trees,
i.e. features. We set the probability of mutation and crossover
both to 0.6, which means that individuals can undergo both
mutation and crossover during the same generation. The train-
ings are done on two thirds of the datasets, through a 3-fold

1www.kaggle.com/c/higgs-boson

2http://opendata.cern.ch/record/328
3www.kaggle.com/c/ﬂavours-of-physics

TABLE II: Gains in accuracy (in %) obtained on three test
datasets constructing one feature. First p-value compares the
best of PGGGP and GGGP to simple GP. Second p-value
compares the best of PGGGP and GGGP to PSO when
applicable.

baseline
PSO
simple GP
GGGP
PGGGP
p-value 1
p-value 2

Higgs
75.46
0.54 ± 0.15
1.92 ± 0.11
2.23 ± 0.68
2.10 ± 0.72
10−17
0.010

DVCS
66.95

τ µ3
82.24

0.52 ± 0.27
0.86 ± 0.27
0.87 ± 0.40

0.58 ± 0.36
0.43 ± 0.28
0.54 ± 0.37

10−4

0.661

TABLE III: Gains obtained on the Higgs dataset constructing
from one to six features. The p-values are not signiﬁcant here
hence not shown.

GGGP
PGGGP

GGGP
PGGGP

1 feature
2.23 ± 0.68
2.10 ± 0.72
4 features
2.92 ± 0.28
2.92 ± 0.27

2 features
2.24 ± 0.18
2.31 ± 0.15
5 features
3.16 ± 0.29
3.16 ± 0.26

3 features
2.61 ± 0.16
2.53 ± 0.25
6 features
3.24 ± 0.31
3.24 ± 0.25

cross-validation on the training sets. The gains in accuracy are
computed on the test sets, i.e. one third of the datasets.

C. Performance comparisons

1) Comparison of methods: In this paragraph, the ﬁtness
function is the mean accuracy of a XGBoost classiﬁer over a
3-fold cross-validation on the training sets.

We ﬁrst compare the performances on the three datasets of
three methods constructing one feature: simple GP (without
any added constraint), GGGP with the grammar adapted to
the dataset, and PGGGP with the same adapted grammar and
an empirically designed transition matrix. Table II shows the
gains in accuracy for the different methods and datasets, as
well as the p-values from a Welchs t-test to compare PGGGP
to simple GP. The baseline accuracy is obtained by training
XGBoost on the base feature set. In addition, we also present
in Table II the results and statistical test of the PSO algorithm
from [10] (array representation) applied on the Higgs dataset.
The performances vary with the datasets used. For the
DVCS dataset, the PGGGP improves the score of the XGBoost
classiﬁer by 0.87%, which is comparable to the method
without probabilities and signiﬁcantly better than the GP
with no constraints at all. For the τ µ3 dataset, the PGGGP
algorithm achieves performance similar to the unconstrained
GP but slightly better than the GP without probabilities, with
a gain of 0.54%. However, for the Higgs dataset, the PGGGP
method is overcome by the GGGP without transition matrix
but the constrained methods are still signiﬁcantly better than
the simple GP. Moreover, the three GP methods signiﬁcantly
outperform the PSO algorithm.

To go further in the study and try to understand the discrep-
ancies in the performances of the different methods on the
Higgs dataset, we perform several experiments constructing

TABLE IV: Gains on accuracies obtained on the Higgs dataset
constructing one feature with different ﬁtness functions, with
the associated p-values comparing simple GP to PGGGP.

DT
KNN
NB
entropy

baseline
66.92
71.18
62.95
75.46

simple GP
2.74 ± 0.19
1.09 ± 0.35
4.11 ± 8.55
0.07 ± 0.12

PGGGP
2.96 ± 0.58
1.52 ± 0.66
6.49 ± 1.51
0.11 ± 0.10

p-value
0.035
0.001
0.119
0.181

n > 1 features. Table III shows the gains in accuracy obtained
on the Higgs dataset while constructing one to six features
comparing our GGGP algorithm with or without the use of
the transition matrix. Although the GGGP algorithm performs
the best when constructing a low number of features, the gap
between the GGGP and the PGGGP algorithms reduces as the
number of features increases, and the PGGGP algorithm over-
comes the GGGP algorithm when constructing six features.

2) Comparison of ﬁtness functions: In previous paragraphs,
the experiments were conducted with the score of XGBoost
as the ﬁtness function. We assess the utility of our PGGGP
also when using other ﬁtness functions. In the experiments, we
compare the performances obtained with different classiﬁers:
a single decision tree (DT), the k-nearest neighbors algorithm
(KNN) and the naive bayes algorithm (NB) as wrapper evalu-
ators, and with the entropy of [18] as a ﬁlter ﬁtness measure.
Table IV compares the accuracies obtained with these dif-
ferent ﬁtness measures and gives the p-values from a Welch’s
t-test. In the case of the entropy measure, the scores are
computed with a XGBoost classiﬁer trained on the datasets
created by the evolutionary algorithm. Using any ﬁtness func-
tion, our probability-aided GGGP algorithm improves the base
score compared to the simple unconstrained GP algorithm. The
improvement is signiﬁcant for the DT and the KNN. These
results show the robustness of our algorithm to several ﬁtness
functions.

D. Discussion

We observe in Table II that the method with probabilities
reaches lower performances than the GGGP for the Higgs
dataset,
in particular when constructing one feature only.
Actually, the GGGP comes up with a feature that achieves
a gain of +3.82%:

(cid:114)

(cid:16)

plep
t

missingtE + plep

t + pτ
t

(cid:17)

+

(cid:113)

m2

H0 + missingtE2 + pτ

t

2

(cid:0)cos (cid:0)φlep − φτ (cid:1) + 2(cid:1)2

cos4 (cid:0)θlep − θτ (cid:1)

(1)
In HEP, physicists may perform cuts on hand-crafted high-
level features to isolate signal events from background events,
thus building a rule of inference. The feature (1) above is
mathematically correct, but does not have an intuitive meaning
to the physicists. However, it is independent of the system
of measurement and can therefore be used to replace the
usual features of the physicists. Although the ratio is not
familiar to the physicists, the elementary components can be
interpretable. The numerator indeed resembles a transverse
energy/momentum balance. In the denominator, the elements
the θ and φ angles
are more interpretable. As expected,

Fig. 3: Histogram of the feature constructed by the GGGP
algorithm.

Fig. 4: Learning curve for constructing 6 features on Higgs
dataset. The mean and standard deviation of the best ﬁtness
of the population are shown at each generation.

are not mixed since they are deﬁned in orthogonal planes.
From energy/momentun conservation, a strong correlation
exists between the direction of the lepton and the τ -lepton.
Therefore, comparing the two angles of the two particles
makes completely sense. Because the angles are deﬁned within
] − π; π],
the comparison is much easier with the cosine
function.

Figure 3 shows the histogram of the feature, visually
proving its discriminating power. The physicists widely use
the invariant mass as a high-level feature to improve the
classiﬁcation. The invariant mass is a 4-momentum balance
between two particles, here the tau and the lepton. Adding
this invariant mass to the original dataset increases the score
by 2.91%. With the GGGP, we overcome the invariant mass
and provide a better performing feature which is independent
of the system of units, adding 3.82% to the accuracy.

However, the GGGP with transition matrix does not achieve
this level of performance, probably because the ratio between
an energy and a series of cosines is set to be very unlikely.
When constructing several features at a time, the performances
of the two methods become in fact similar (see Table III).
The GGGP without probabilities is then confronted to a
bigger search space without guidance, whereas the GGGP with
transition matrix gets more space to construct interpretable and
performing features all at once. For instance, the GGGP using
the transition matrix builds ﬁve features achieving a gain of
+3.71% on the accuracy:

cos (cid:0)φlep − φτ (cid:1)
cos (cid:0)θlep − θτ (cid:1)
cos (cid:0)φmissing − φlep(cid:1)

pleading
T

(cid:88)

pjets
T −

(cid:16)

Emissing

T

+ plep
T

(cid:17)2

m2

H 0 +

(cid:16)
plep
T + pτ

T

(cid:17)2

(2)

(3)

(4)

(5)

(6)

The ﬁrst and second features (2) (3) are exact elementary
components of the single feature constructed by the GGGP.
The third (4) is another geometrical combination of the angles
of the missing transverse energy (most likely corresponding
to a neutrino) and the lepton, which both come from the
same τ lepton. It should be noted that the θ and φ angles are
almost never mixed in the output features, proving that the GP
algorithm inferred that these two angles belong to two different
planes. The two last features (5) (6) are energy balances in
the transverse space. These ﬁve constructed features are more
interpretable features than the one from the simple GGGP,
and are also independent of the system of measurement and
therefore reusable in HEP analyses.

Finally, it seems that forcing the algorithm to build in-
terpretable features is not beneﬁcial when constructing only
one feature. Therefore, the GGGP manages to compress the
information of several interpretable features into one, whereas
the GGGP aided by the transition matrix is less efﬁcient.
Actually, the best features built by the PGGGP in the context
of single feature construction are very complex and exploit
the maximum allowed depth of the trees. However, having big
trees impairs the interpretability of the associated features. In
this context, the preference of PGGGP over GGGP for the
interpretability of the features is not obvious.

However, the transition matrix seems to help the GGGP

algorithm to converge faster (see Figure 4).

On the DVCS dataset, probabilities also seem to help
the convergence and performance of the GGGP algorithm.
However, the statistical tests to prove the difference between
GGGP and PGGGP results are never signiﬁcant regardless of
the dataset. PGGGP and GGGP algorithms are actually very
similar in their conception, and only differ by the choice of
the probabilities to construct the trees. It is therefore normal
not to observe any signiﬁcant variation in the distributions of
the results. Only slight improvements can be observed.

050100150200250300MeV0.0000.0020.0040.0060.0080.0100.0120.0140.016backgroundsignal020406080100120140160generation0.7550.7600.7650.7700.7750.7800.7850.7900.795accuracy6 featuresPGGGPGGGPFinally,

the performance gains on the τ µ3 dataset are
lower than those on the other two datasets. This can be
explained by the design of the dataset itself: providing many
high-level variables (we partly discarded) and missing basic
features such as the φ-angle essential for our experiment,
this dataset might not be suited for our study. With the
features provided in the dataset, more complex non-linear and
therefore non-interpretable combinations are probably more
efﬁcient to increase the accuracy of a classiﬁer. This can
also explain the better score of the simple GP, which is not
constrained and then allowed to apply complex operations such
as trigonometric functions on all variables. The features built
by our GGGP, with or without probabilities, are already quite
complex: the most performing individuals have a height h ≥ 6.
However, we still manage to increase the accuracy score by
1.48% constructing one feature with PGGGP.

Finally, one of our objectives being the interpretability of the
built features, it is difﬁcult to provide an accurate comparison
with other methods that do not focus on interpretability or
consistency of features, or that have different structure for
built features. Therefore, the comparison can be only on the
numerical scores. Within evolutionary methods other than GP,
we tested a PSO algorithm, which performs poorly on our
datasets. We also considered tree-based algorithms such as
Cognito or FICUS, but the lack of open source code and
implementation details in the papers prevented us to draw a
fair comparison. Besides, the datasets used in other studies do
not include units of measurement. Few benchmarks with units
of measurement exist, since the data are often anonymized.
However, our method is useful for many real-world problems.

V. CONCLUSION AND PERSPECTIVES

In this paper, we have presented an adaptation of a new
Grammar-Guided Genetic Programming algorithm to dimen-
sional consistency and physics laws, aided with probability
transition matrices. Compared to a simple Genetic Program-
ming algorithm without constraints, we have shown that this
method signiﬁcantly improves the accuracy score of several
classiﬁers for two high-energy physics datasets from com-
pletely different experimental setups. The possibilities to build
new features for the third dataset are limited due to the lack of
base features. However, for the three datasets, our interpretable
GGGP-based feature construction algorithm brings a signiﬁ-
cant improvement on the classiﬁcation accuracy especially for
datasets with a low baseline score. We discussed the trade-
off dilemma between performances and interpretability raised
by the application of the transition matrix. The comparison
between GGGP and PGGGP in gain in accuracies mainly
relies on the data on which the experiment is conducted and on
the number of features that are constructed. The interpretability
for physics experts is deﬁnitely better with PGGGP, but the
score can be better with GGGP (without probabilities) because
of the need to compress information into a small number
of features. Therefore, we observe that constructing several
features enables the PGGGP to ﬁll the gap with the GGGP
while being more interpretable.

In future work, we plan to automatically extract the prob-
abilities of the transition matrix from physics formulas taken
from books or articles. Another perspective would be to
enforce the impact of probabilities as we go deeper in the
generation of the tree: this could help building consistent
elementary components while enabling more complex com-
binations at the root of the tree, imitating the high performing
feature obtained by the GGGP on the Higgs dataset.

ACKNOWLEDGEMENT

We want to thank Marouen Baalouch for his precious help
during the experiments, Francesco Bossu and Herv´e Moutarde
for their help understanding High-Energy Physics and the
datasets, and more generally the CLAS12 collaborators for
the simulation software.

REFERENCES

[1] G. H. John, R. Kohavi, and K. Pﬂeger, “Irrelevant Features and the
Subset Selection Problem,” in Machine Learning Proceedings 1994.
Elsevier, 1994, pp. 121–129.

[2] L. H. Gilpin, D. Bau, B. Z. Yuan, A. Bajwa, M. Specter, and L. Kagal,
“Explaining Explanations: An Approach to Evaluating Interpretability
of Machine Learning,” arXiv:1806.00069 [cs, stat], May 2018, arXiv:
1806.00069.

[3] V. N. Vapnik, The Nature of Statistical Learning Theory. New York,

NY, USA: Springer-Verlag New York, Inc., 1995.

[4] Y. Bengio, “Deep learning of representations: Looking forward,” CoRR,

vol. abs/1305.0445, 2013.

[5] P. Sondhi, “Feature Construction Methods: A Survey,” p. 8, 2009.
[6] S. Markovitch and D. Rosenstein, “Feature generation using general
constructor functions,” Machine Learning, vol. 49, no. 1, pp. 59–98,
Oct 2002.

[7] U. Khurana, D. Turaga, H. Samulowitz, and S. Parthasrathy, “Cognito:
Automated feature engineering for supervised learning,” in 2016 IEEE
16th International Conference on Data Mining Workshops (ICDMW),
Dec 2016, pp. 1304–1307.

[8] U. Khurana, H. Samulowitz, and D. S. Turaga, “Feature engineering
learning,” CoRR, vol.

for predictive modeling using reinforcement
abs/1709.07150, 2017.

[9] B. Xue, M. Zhang, Y. Dai, and W. N. Browne, “PSO for feature
construction and binary classiﬁcation.” ACM Press, 2013, p. 137.
[10] Y. Dai, B. Xue, and M. Zhang, “New Representations in PSO for
Feature Construction in Classiﬁcation,” in Applications of Evolutionary
Computation, ser. Lecture Notes in Computer Science. Springer, Berlin,
Heidelberg, Apr. 2014, pp. 476–488.

[11] P. Espejo, S. Ventura, and F. Herrera, “A Survey on the Application of
Genetic Programming to Classiﬁcation,” IEEE Transactions on Systems,
Man, and Cybernetics, Part C (Applications and Reviews), vol. 40, no. 2,
pp. 121–144, Mar. 2010.

[12] J. R. Koza, Genetic programming: on the programming of computers by
means of natural selection, ser. Complex adaptive systems. Cambridge,
Mass: MIT Press, 1992.

[13] K. Krawiec, “Genetic Programming-based Construction of Features for
Machine Learning and Knowledge Discovery Tasks,” p. 15, 2002.
[14] F. E. B. Otero, M. M. S. Silva, A. A. Freitas, and J. C. Nievola,
“Genetic Programming for Attribute Construction in Data Mining,” in
Proceedings of the 6th European Conference on Genetic Programming,
ser. EuroGP’03. Berlin, Heidelberg: Springer-Verlag, 2003, pp. 384–
393.

[15] M. G. Smith and L. Bull, “Genetic Programming with a Genetic Al-
gorithm for Feature Construction and Selection,” Genetic Programming
and Evolvable Machines, vol. 6, no. 3, pp. 265–281, Sep. 2005.
[16] K. Neshatian and M. Zhang, “Genetic Programming and Class-Wise
Orthogonal Transformation for Dimension Reduction in Classiﬁcation
Problems,” in Genetic Programming, ser. Lecture Notes in Computer
Science. Springer Berlin Heidelberg, 2008, pp. 242–253.

[17] H. Guo, L. B. Jack, and A. K. Nandi, “Feature generation using genetic
programming with application to fault classiﬁcation,” IEEE Transactions
on Systems, Man, and Cybernetics, Part B (Cybernetics), vol. 35, no. 1,
pp. 89–99, Feb. 2005.

[18] K. Neshatian, M. Zhang, and P. Andreae, “A Filter Approach to
Multiple Feature Construction for Symbolic Learning Classiﬁers Using
Genetic Programming,” IEEE Transactions on Evolutionary Computa-
tion, vol. 16, no. 5, pp. 645–661, Oct. 2012.

[19] H. Firpi, E. Goodman, and J. Echauz, “On Prediction of Epileptic
Seizures by Computing Multiple Genetic Programming Artiﬁcial Fea-
tures,” in Genetic Programming, ser. Lecture Notes in Computer Science.
Springer Berlin Heidelberg, 2005, pp. 321–330.

[20] B. Tran, M. Zhang, and B. Xue, “Multiple feature construction in
IEEE, Dec. 2016,

classiﬁcation on high-dimensional data using GP.”
pp. 1–8.

[21] D. Gavrilis, I. G. Tsoulos, and E. Dermatas, “Selecting and constructing
features using grammatical evolution,” Pattern Recognition Letters,
vol. 29, no. 9, pp. 1358–1365, Jul. 2008.

[22] D. J. Montana, “Strongly Typed Genetic Programming,” Evol. Comput.,

vol. 3, no. 2, pp. 199–230, Jun. 1995.

[23] T. D. Haynes, D. A. Schoenefeld, and R. L. Wainwright, “Type Inheri-
tance in Strongly Typed Genetic Programming,” in Advances in Genetic
Programming 2, chapter 18. MIT Press, 1996, pp. 359–376.

[24] R. McKay, N. Hoai, P. Whigham, Y. Shan, and M. O’Neill, “Grammar-
based Genetic Programming: a survey,” Genetic Programming and
Evolvable Machines, vol. 11, pp. 365–396, Sep. 2010.

[25] F. Gruau, “Advances in Genetic Programming,” P. J. Angeline and K. E.
Kinnear, Jr., Eds. Cambridge, MA, USA: MIT Press, 1996, pp. 377–
394.

[26] M. Keijzer and V. Babovic, “Dimensionally Aware Genetic Program-
ming,” in Proceedings of the 1st Annual Conference on Genetic and
Evolutionary Computation - Volume 2, ser. GECCO’99, San Francisco,
CA, USA, 1999, pp. 1069–1076.

[27] A. Ratle and M. Sebag, “Grammar-guided genetic programming and
dimensional consistency: application to non-parametric identiﬁcation in
mechanics,” Applied Soft Computing, vol. 1, no. 1, pp. 105–118, Jun.
2001.

[28] M. O’Neill and C. Ryan, “Grammatical evolution,” IEEE Transactions

on Evolutionary Computation, vol. 5, no. 4, pp. 349–358, Aug. 2001.

[29] P. Miquilini, R. C. Barros, V. V. d. Melo, and M. P. Basgalupp,
“Enhancing discrimination power with genetic feature construction: A
grammatical evolution approach,” in 2016 IEEE Congress on Evolution-
ary Computation (CEC), Jul. 2016, pp. 3824–3831.

[30] S. Yazdani, J. Shanbehzadeh, and E. Hadavandi, “MBCGP-FE: A
modiﬁed balanced cartesian genetic programming feature extractor,”
Knowledge-Based Systems, vol. 135, pp. 89–98, Nov. 2017.

[31] M. Schmidt and H. Lipson, “Distilling Free-Form Natural Laws from
Experimental Data,” Science, vol. 324, no. 5923, pp. 81–85, Apr. 2009.
[32] Y. Shan, R. I. McKay, D. Essam, and H. A. Abbass, “A Survey
of Probabilistic Model Building Genetic Programming,” in Scalable
Optimization via Probabilistic Modeling. Berlin, Heidelberg: Springer
Berlin Heidelberg, 2006, vol. 33, pp. 121–160.

[33] A. Ratle and M. Sebag, “Avoiding the Bloat with Stochastic Grammar-
based Genetic Programming,” arXiv:cs/0602022, Feb. 2006, arXiv:
cs/0602022.

[34] C. Adam-Bourdarios, G. Cowan, C. Germain, I. Guyon, B. Kegl, and
D. Rousseau, “Learning to discover: the Higgs boson machine learning
challenge - Documentation,” 2014.

[35] LHCb collaboration, “Search for the lepton ﬂavour violating decay
τ − → µ−µ+µ−,” Journal of High Energy Physics, vol. 2015, no. 2,
Feb. 2015, arXiv: 1409.8548.

