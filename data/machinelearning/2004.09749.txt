Parametric Programming Approach for More Powerful and General
Lasso Selective Inference

1
2
0
2

b
e
F
2
2

]
L
M

.
t
a
t
s
[

3
v
9
4
7
9
0
.
4
0
0
2
:
v
i
X
r
a

Vo Nguyen Le Duy
Nagoya Institute of Technology and RIKEN
duy.mllab.nit@gmail.com

Ichiro Takeuchi
Nagoya Institute of Technology and RIKEN
takeuchi.ichiro@nitech.ac.jp

Abstract

1 Introduction

Selective Inference (SI) has been actively
studied in the past few years for conduct-
ing inference on the features of linear models
that are adaptively selected by feature selec-
tion methods such as Lasso. The basic idea
of SI is to make inference conditional on the
selection event. Unfortunately, the main lim-
itation of the original SI approach for Lasso
is that the inference is conducted not only
conditional on the selected features but also
on their signs — this leads to loss of power
because of over-conditioning. Although this
limitation can be circumvented by consider-
ing the union of such selection events for all
possible combinations of signs, this is only
feasible when the number of selected features
is suﬃciently small. To address this com-
putational bottleneck, we propose a para-
metric programming-based method that can
conduct SI without conditioning on signs
even when we have thousands of active fea-
tures. The main idea is to compute the
continuum path of Lasso solutions in the
direction of the selected test statistic, and
identify the subset of the data space corre-
sponding to the feature selection event by
following the solution path. The proposed
parametric programming-based method not
only avoids the aforementioned computa-
tional bottleneck but also improves the per-
formance and practicality of SI for Lasso in
various respects. We conduct several exper-
iments to demonstrate the eﬀectiveness and
eﬃciency of our proposed method.

Proceedings of the 24th International Conference on Artiﬁ-
cial Intelligence and Statistics (AISTATS) 2021, San Diego,
California, USA. PMLR: Volume 130. Copyright 2021 by
the author(s).

Reliable machine learning (ML), which is the prob-
lem of assessing the reliability of data-driven knowl-
edge obtained by ML algorithms, is one of the most
important issues in the ML community. Among vari-
ous approaches for reliable ML, selective inference (SI)
has been recognized as a new promising approach for
assessing the statistical reliability of data-driven hy-
potheses selected by complex data analysis algorithms.

SI was ﬁrst introduced as a statistical inference tool
for the features selected by Lasso (Tibshirani, 1996).
Although various properties of Lasso have been ex-
tensively studied in the past decades (see, e.g., Hastie
et al. (2015)), exact statistical inference such as com-
puting p-values or conﬁdence intervals for adaptively
selected features by Lasso has only recently begun to
be actively studied in the context of SI (Lee et al.,
2016; Fithian et al., 2014; Liu et al., 2018).

The main idea of SI is to make inference for the se-
lected features conditional on the selection event, lead-
ing to exact valid inference on adaptively selected fea-
tures by Lasso is possible in the sense that p-values for
proper false positive rate control or conﬁdence inter-
vals with proper coverage guarantees can be obtained.
After the seminal work (Lee et al., 2016), conditional
inference-based SI has been actively studied and ap-
plied to various problems (Bachoc et al., 2014; Fithian
et al., 2014, 2015; Choi et al., 2017; Tian and Taylor,
2018; Chen and Bien, 2019; Hyun et al., 2018; Ba-
choc et al., 2018; Charkhi and Claeskens, 2018; Loftus
and Taylor, 2014; Loftus, 2015; Panigrahi et al., 2016;
Tibshirani et al., 2016; Yang et al., 2016; Suzumura
et al., 2017; Tanizaki et al., 2020; Duy et al., 2020b,a;
Sugiyama et al., 2020).

Existing works and their drawbacks. Let A be
a random variable indicating the set of the selected
features by applying Lasso on any random data sam-
ple and s be their signs. Then in the seminal work
(Lee et al., 2016), the authors showed that the selec-
tion event {A = Aobs, s = sobs} is characterized as

 
 
 
 
 
 
Parametric Programming Approach for More Powerful and General Lasso Selective Inference

a polytope in the data space, where Aobs and sobs
are the corresponding observations (see §2 for detailed
setup), leading to the sampling distribution of the test-
statistic in the form of a truncated Normal distribu-
tion. However, it is well-known that conditioning on
the signs leads to low statistical power because of over-
conditioning, which is widely recognized as a major
drawback of the current Lasso SI approach.

Lee et al. (2016) also discussed the solution to over-
come the drawback by conducting conditional infer-
ences without sign event {A = Aobs}, which can be
characterized by 2|Aobs| polytopes.
If the number of
selected features |Aobs| is moderate (e.g., up to 15), it
is feasible to consider all aﬃne constraints of all these
2|Aobs| polytopes. However, if |Aobs| is large, it is in-
feasible to enumerate the whole aﬃne constraints for
exponentially increasing number of polytopes.

Recently, Liu et al. (2018) have proposed two ap-
proaches, in which the problem settings are diﬀerent
from Lee et al. (2016), to improve the power. How-
ever, in their ﬁrst approach, it is only applicable when
the number of features p is smaller than the number
of instances n. In the second approach, they still con-
sider an exponentially large number of all possible sign
vectors. This paper is motivated by Section 6 of Liu
et al. (2018) in which they provide a recipe for con-
structing more powerful conditional SI methods.
In
the other direction, Tian and Taylor (2018) and Ter-
ada and Shimodaira (2019) proposed methods using
randomization. A drawback of these randomization-
based approaches including simple data-splitting ap-
proach is that further randomness is added in both
feature selection and inference stages.

Many machine learning tasks involve careful tuning of
a regularization parameter λ that controls the balance
between an empirical loss term and a regularization
term, e.g., commonly by cross-validation (CV). How-
ever, most of the current Lasso SI methods assume a
pre-speciﬁed λ and ignore the fact that λ is selected
based on the data because the selection event of cross-
validation iis diﬃcult to characterize. Loftus (2015)
and Markovic et al. (2017) proposed solutions to in-
corporate CV event. However, the former requires ad-
ditional conditioning on all intermediate models which
leads to loss of power and the latter considers a ran-
domization version of CV instead of the vanilla CV.

Lee et al. (2016). The polytope-based SI is applica-
ble when the selection event can be characterized as a
polytope in the data space. Otherwise, the only way
is to consider extra conditions, e.g., sign conditioning,
so that the over-conditioned event is characterized as
a polytope, which leads to loss of statistical power. In
contrast, with the proposed PP-based SI, it is possible
to characterize the selection event even if they cannot
be described by a polytope.

• We introduce a method to compute the contin-
uum path of Lasso solutions in the direction of in-
terest, which is sub-sequently used to identify the ex-
act sampling distribution of the test statistic with
the minimum amount of conditioning. Therefore,
the PP-based SI can fundamentally resolve the over-
conditioning problem, which was a major concern
in polytope-based SI, to achieve the high statistical
power. Although the concept of PP has been used in
various problems (Osborne et al., 2000; Efron and Tib-
shirani, 2004; Hastie et al., 2004; Rosset, 2005; Bach
et al., 2006; Rosset and Zhu, 2007; Tsuda, 2007; Lee
and Scott, 2007; Garrigues and Ghaoui, 2008; Takeuchi
et al., 2009; Karasuyama and Takeuchi, 2010; Hocking
et al., 2011; Karasuyama et al., 2012; Lei, 2019), this
is the ﬁrst study that introduces a piecewise-linear PP
approach for characterizing the selection events in SI.

• Furthermore, by using PP-based SI, we can perform
SI with minimal conditioning for regularization param-
eter selection by cross-validation, which is complicated
and was not possible with polytope-based SI. Besides,
we show that our proposed method is general and can
be applied in several settings as well as other selection
models such as elastic net and interaction model.

Figure 1 shows the schematic illustration of the pro-
posed method. For reproducibility, our implementa-
tion is available at

https://github.com/vonguyenleduy/parametric lasso
selective inference

2 Problem Statement

To formulate the problem, we consider a random re-
sponse vector

Y = (Y1, ..., Yn)(cid:62) ∼ N(µ, Σ),

(1)

Contribution. Our contributions are as follows:

• In this paper, we propose a new SI approach based
on parametric-programming (PP) (Ritter, 1984; Allgo-
wer and George, 1993; Gal, 1995; Best, 1996), which
we call PP-based SI, for resolving several major limi-
tations of the seminal polytope-based SI proposed by

where n is the number of instances, µ is modeled as
a linear function of p features x1, ..., xp ∈ Rn, and
Σ ∈ Rn×n is a covariance matrix which is known or
estimable from independent data. The goal is to sta-
tistically quantify the signiﬁcance of the relation be-
tween the features and response while properly con-
trolling the false positive rate. To achieve the goal,

Vo Nguyen Le Duy, Ichiro Takeuchi

Figure 1: Schematic illustration of the proposed method. By applying Lasso on the observed data yobs, we
obtain the observed active set Aobs. The statistical inference for each selected feature is conducted conditional
on the subspace Y whose data has the same active set as yobs. We introduce a parametric programing method
for characterizing the conditional data space Y by searching on the parametrized line.

the authors in Lee et al. (2016) have proposed a prac-
tical SI framework, in which a subset of features is
ﬁrst “selected” by the Lasso, and the inferences are
then conducted for each selected feature.

used for initial hypothesis generating process. This
is achieved by considering the sampling distribution
of the test statistic η(cid:62)
j Y conditional on the selection
event, i.e.,

Feature selection and its selection event. Given
an observed response vector yobs ∈ Rn sampled from
the model (1), the Lasso optimization problem is given
by

ˆβ = arg min

β∈Rp

1
2

(cid:107)yobs − Xβ(cid:107)2

2 + λ(cid:107)β(cid:107)1,

(2)

where X ∈ Rn×p is a feature matrix, and λ ≥ 0 is
a regularization parameter. Since the Lasso produces
sparse solutions, the active set selected by applying
the Lasso to yobs is deﬁned as

Aobs = A(yobs) = {j : ˆβj (cid:54)= 0}.

(3)

Then, the event that the Lasso active set for a random
vector Y is the same as yobs is written as

(cid:8)A(Y ) = A(yobs)(cid:9) .

(4)

Statistical inference for the selected feature.
The selected jth coeﬃcient is written as ˆβj = η(cid:62)
j yobs
by deﬁning

ηj = XAobs

(cid:0)X (cid:62)

Aobs

XAobs

(cid:1)−1

ej,

(5)

where ej ∈ R|Aobs| is a basis vector with a 1 at position
jth. For the inference on the jth selected feature, we
consider the following statistical test

j Y | (cid:8)A(Y ) = A(yobs), q(Y ) = q(yobs)(cid:9) ,
η(cid:62)

(7)

j )Y with c = Σηj(η(cid:62)

j Σηj)−1.
where q(Y ) = (In − cη(cid:62)
The second condition q(Y ) = q(yobs) indicates the
component that is independent of the test statistic for
a random vector Y is the same as the one for yobs. The
q(Y ) corresponds to the component z in the seminal
paper (see Lee et al. (2016), Sec 5, Eq 5.2 and Theorem
5.2).

Once the selection event is identiﬁed, we can easily
compute the pivotal quantity

F Z
j µ,η(cid:62)
η(cid:62)

j Σηj

(η(cid:62)

j Y ) |

(cid:110)

A(Y ) = A(yobs), q(Y ) = q(yobs)

(cid:111)

,

j µ, variance η(cid:62)

(8)
which is the c.d.f. of the truncated Normal distribution
with mean η(cid:62)
j Σηj, and the truncation
region Z which is calculated based on the selection
event. The pivotal quantity is crucial for calculating
p-value and conﬁdence interval. Based on the pivotal
quantity, we can consider selective type I error or se-
lective p-value (Fithian et al., 2014) in the form of

P selective

j

= 2 min{πj, 1 − πj},

(9)

where πj = 1 − F Z
sense that

0,η(cid:62)

j Σηj

(η(cid:62)

j Y ), which is valid in the

H0,j : η(cid:62)

j µ = 0 vs. H1,j : η(cid:62)

j µ (cid:54)= 0.

(6)

ProbH0,j

(cid:0)P selective

j

< α(cid:1) = α, ∀α ∈ [0, 1].

Since the hypothesis is generated from the data, se-
lection bias exists.
In order to correct the selection
bias, we have to remove the information that has been

Furthermore, to obtain 1−α conﬁdence interval for any
α ∈ [0, 1], by inverting the pivotal quantity in Equa-
tion (8), we can ﬁnd the smallest and largest values of

Data Space ℝnyobsLasso 𝒜(yobs)Proposed MethodObserved Active Set𝒜obs={1,3,5}Parametrized
Liney(z)=a+bzConditional Data Space 
𝒴={y∈ℝn∣𝒜(y)=𝒜(yobs),q(y)=q(yobs)}{1,3,5}{1,3,5}{3}{2}{1,6}Parametric Programming Approach for More Powerful and General Lasso Selective Inference

η(cid:62)
j µ such that the value of pivotal quantity remains
in the interval (cid:2) α

(cid:3) (Lee et al., 2016).

2 , 1 − α

2

However, the main challenge is that characterizing
A(Y ) = A(yobs) in Equation (7) is intractable because
we have to consider 2|A(yobs)| possible sign vectors. To
overcome this issue, Lee et al. (2016) consider infer-
ence conditional not only on the selected features but
also on their signs. Unfortunately, additionally consid-
ering the signs leads to low statistical power because
of over-conditioning.

In the next section, we will
introduce a method
for identifying the minimum amount of condition-
ing (cid:8)A(Y ) = A(yobs), q(Y ) = q(yobs)(cid:9), which leads
to high statistical power. The main idea is to com-
pute the path of Lasso solutions in the direction of
interest ηj. By focusing on the line along ηj, we can
skip majority of the polytopes that do not aﬀect the
truncated Normal sampling distribution because they
do not intersect with this line.
In other words, we
can skip majority of combinations of signs that never
appear when applying Lasso to the data on the line.

3 Proposed Method

In this section, we propose a parametric programming
approach for characterizing conditioning event in (7).
The schematic illustration is shown in Figure 1.

3.1 Conditional Data Space Characterization

Let us deﬁne the set of y ∈ Rn which satisﬁes the
conditions in Equation (7) as

Y = {y ∈ Rn | A(y) = A(yobs), q(y) = q(yobs)}.

(10)
According to the second condition, the data in Y is
restricted to a line (see Sec 6 in Liu et al. (2018), and
Fithian et al. (2014)). Therefore, the set Y can be
re-written, using a scalar parameter z ∈ R, as

Y = {y(z) = a + bz | z ∈ Z} ,

(11)

where a = q(yobs), b = Σηj(η(cid:62)

j Σηj)−1, and
Z = (cid:8)z ∈ R | A(y(z)) = A(yobs)(cid:9) .

(12)

Now, let us consider a random variable Z ∈ R and
its observation zobs ∈ R, which satisfy Y = a + bZ
and yobs = a + bzobs. The conditional inference in
(7) is re-written as the problem of characterizing the
sampling distribution of

Z | {Z ∈ Z} .

(13)

Since Z ∼ N(0, η(cid:62)
j Σηj) under the null hypothesis,
the law of Z | Z ∈ Z follows a truncated Normal

distribution. Once the truncation region Z is identi-
ﬁed, the pivotal quantity in Equation (8) is equal to
F Z
(Z), and can be easily obtained. Thus, the
remaining task is to characterize Z.

j Σηj

0,η(cid:62)

Characterization of truncation region Z. Let
us
introduce the optimization problem (2) with
parametrized response vector y(z) for z ∈ R as

ˆβ(z) = arg min

β∈Rp

1
2

(cid:107)y(z) − Xβ(cid:107)2

2 + λ(cid:107)β(cid:107)1.

(14)

The subdiﬀerential of the (cid:96)1-norm at ˆβ(z) is deﬁned
as follows:

∂(cid:107) ˆβ(z)(cid:107)1 = s(z) :

(cid:40)

sj(z) = sign( ˆβj(z))
sj(z) ∈ [−1, 1]

if ˆβj(z) (cid:54)= 0
if ˆβj(z) = 0

,

where we denote s(z) = sign( ˆβ(z)). Then, for any z
in R, the optimality condition is given by

X (cid:62) (cid:16)

X ˆβ(z) − y(z)

(cid:17)

+ λs(z) = 0,

(15)

s(z) ∈ ∂(cid:107) ˆβ(z)(cid:107)1. To construct the truncation region
Z in Equation (12), we have to 1) compute the en-
tire path of ˆβ(z), and 2) identify the set of intervals
of z on which A(y(z)) = A(yobs). However, it seems
intractable to compute ˆβ(z) for inﬁnitely many val-
ues of z ∈ R. Our main idea to overcome this diﬃ-
culty is to propose a parametric programming method
for eﬃciently computing a ﬁnite number of “transition
points” at which the active set changes.

3.2 A Piecewise Linear Homotopy

We now derive the main technique. We show that ˆβ(z)
is a piecewise linear function of z. To make the nota-
tion lighter, we write Az = A(y(z)), and we denote
the set of inactive features as Ac
z.
Lemma 1. Consider two real values z(cid:48) and z (z(cid:48) > z).
z, |sj(z(cid:48))| < 1 for all
Suppose |sj(z)| < 1 for all j ∈ Ac
If ˆβAz (z) and
j ∈ Ac
XAz is invertible.
ˆβAz(cid:48) (z(cid:48)) have the same active set and the same signs,
then we have

z(cid:48), and X (cid:62)
Az

ˆβAz (z(cid:48)) − ˆβAz (z) = ψAz (z) × (z(cid:48) − z),
(z) × (z(cid:48) − z),
(z) = γAc

(z(cid:48)) − λsAc

z

z

λsAc

z

(16)

(17)

where ψAz (z) = (X (cid:62)
Az
X (cid:62)
Ac
z

b − X (cid:62)
Ac
z

XAz ψAz (z).

XAz )−1X (cid:62)
Az

b, and γAc

z

(z) =

Proof. From the optimality conditions of the Lasso,
we have

X (cid:62)
Az
Az(cid:48) XAz(cid:48)

ˆβAz (z) − X (cid:62)
Az

XAz
ˆβAz(cid:48) (z(cid:48)) − X (cid:62)

y(z) + λsAz (z) = 0, (18)
Az(cid:48) y(z(cid:48)) + λsAz(cid:48) (z(cid:48)) = 0. (19)

X (cid:62)

Vo Nguyen Le Duy, Ichiro Takeuchi

Then, by subtracting (18) from (19) and Az = Az(cid:48), we
have

ˆβAz (z(cid:48)) − ˆβAz (z) = (X (cid:62)
Az
= (X (cid:62)
Az
= (X (cid:62)
Az

XAz )−1X (cid:62)
Az
XAz )−1X (cid:62)
Az
XAz )−1X (cid:62)
Az

(y(z(cid:48)) − y(z))
(a + bz(cid:48) − a − bz)
b × (z(cid:48) − z).

Thus, we achieve Equation (16). Next, from the opti-
mality conditions of the Lasso, we also have

−X (cid:62)
Ac
z
XAz(cid:48)

XAz

ˆβAz (z) + X (cid:62)
Ac
z
ˆβAz(cid:48) (z(cid:48)) + X (cid:62)
Ac
z(cid:48)

−X (cid:62)
Ac
z(cid:48)

y(z) = λsAc
y(z(cid:48)) = λsAc

z

(z),
z(cid:48) (z(cid:48)).

(20)

(21)

Similarly, by subtracting (20) from (21) and Az = Az(cid:48),
we can easily achieve Equation (17).

Remark 1. In this paper, we assume the uniqueness
of the Lasso solution ˆβ(z) for all z ∈ R as well as
|sj(z)| < 1 for all j ∈ Ac
z and the invertibility of
X (cid:62)
XAz . These assumptions are justiﬁed by assuming
Az
the columns of X are in general position (Tibshirani,
2013). Parametric programming methods for handling
the rare cases where these assumptions are not satis-
ﬁed have been studied, e.g., in Best (1996), and can be
applied to our problem setup. In practice, when the
design matrix is not in general position, it is also com-
mon to introduce an additional ridge penalty term, re-
sulting in the elastic net (Zou and Hastie, 2005). Our
proposed method can be extended to the elastic net
case (see Appendix 6.2.1 for the details).

Computation of the transition point. From
Lemma 1, the solution ˆβ(z) is a linear function of z
until z reaches a transition point at which either an
element of ˆβ(z) becomes zero or a component of s(z)
becomes one in absolute value. We now introduce how
the transition point is identiﬁed.

z

be a real value such that
Lemma 2. Let z
Then, Az(cid:48) = Az,
maxj∈Ac
|sj(z)| < 1.
z(cid:48) |sj(z(cid:48))| < 1, and s(z) = s(z(cid:48)) for any real
maxj∈Ac
value z(cid:48) in the interval [z, z + tz), where z + tz is the
value of transition point,

tz = min (cid:8)t1
(cid:32)

(cid:9) ,
(cid:33)

z, t2
z
ˆβj(z)
ψj(z)

t1
z = min
j∈Az

−

(cid:18)

λ

t2
z = min
j∈Ac
z

++
sign(γj(z)) − sj(z)
γj(z)

(22)

(23)

(24)

.

++

,

(cid:19)

Here, we use the convention that for any m ∈ R,
(m)++ = m if m > 0, and (m)++ = ∞ otherwise.

Proof. From Equation (16), we can see that ˆβAz (z) is
a function of z. For a real value z, there exists t1
z such

Algorithm 1 parametric lasso SI
Input: X, yobs, λ, [zmin, zmax]
1: Compute Lasso solution and obtain observed Aobs for

data (X, yobs)

4:

5:

2: for each selected feature j ∈ Aobs do
Compute ηj ← Equation (5)
3:
Compute a and b ← Equation (11)
ˆβ(z), Az ← compute solution path (X, λ, a, b,
[zmin, zmax])
Truncation region Z ← {z : Az = Aobs}
P selective
dence interval of βj)

← Equation (9) (and/or selective conﬁ-

6:

7:

j

8: end for
Output: {P selective

j

intervals of βj, j ∈ Aobs)

}j∈Aobs (and/or selective conﬁdence

z

that for any real value z(cid:48) in [z, z + t1
z), all elements of
ˆβAz(cid:48) (z(cid:48)) remain the same signs with ˆβAz (z). Similarly,
(z) is a func-
from Equation (17), we can see that sAc
tion of z. Then, for a real value z, there exists t2
z such
that for any real value z(cid:48) in [z, z + t2
z), all elements of
z(cid:48) (z(cid:48)) are smaller than 1 in absolute value. Finally,
sAc
by taking tz = min{t1
z}, we obtain the interval in
which the active set and signs of Lasso solution remain
the same. The remaining task is how to compute t1
z
z and t2
z. We defer the detailed derivations of t1
and t2
z
to the Appendix 6.1.

z, t2

3.3 Algorithm

In this section, we show the detailed algorithm of our
proposed parametric programming method. In Algo-
rithm 1, for feature selection step, we just simply ap-
ply Lasso to the data (X, yobs), and obtain the active
set Aobs. Then, we conduct SI for each selected fea-
ture. For testing βj, j ∈ Aobs, we ﬁrst obtain the di-
rection of interest ηj, which can be easily computed
as in Equation (5). Second, the main task is to com-
pute the solution path of ˆβ(z) in Equation (14) for the
parametrized response vector y(z), where, note that,
the parametrized solution ˆβ(z) are diﬀerent among dif-
ferent j ∈ Aobs since the direction of interest ηj de-
pends on j. This task can be done by Algorithm 2.
Finally, after having the path, we can easily obtain
truncation region Z which is used to compute selec-
tive p-value or selective conﬁdence interval.

In Algorithm 2, a sequence of transition points are
computed one by one. The algorithm is initialized at
zk = zmin, k = 0. At each zk, the task is to ﬁnd
the next transition point zk+1, where the active set
changes. This task can be done by computing the
step size in Algorithm 3. This step is repeated until

Parametric Programming Approach for More Powerful and General Lasso Selective Inference

Algorithm 2 compute solution path
Input: X, λ, a, b, [zmin, zmax]
1: Initialization: k = 0, zk = zmin, T = zk
2: while zk < zmax do
3:

4:

5:

y(zk) = a + bzk
tzk , ˆβ(zk), Azk ← compute step size(X, y(zk), λ)
zk+1 = zk + tzk , T = T ∪ {zk+1}
(zk+1 is the value of the next transition point)
k = k + 1
6:
7: end while
Output: { ˆβ(zk)}zk∈T , {Azk }zk∈T

Algorithm 3 compute step size
Input: X, y(z), λ
1: Compute primal/dual Lasso solution ˆβ(z), ˆs(z) for

data (X, y(z))

2: Obtain active set Az = {j : ˆβj(z) (cid:54)= 0}
3: Compute ψAz (z), γAc
z (z) ← Lemma 1
4: t1
5: tz = min{t1
Output: tz, ˆβ(z), Az

z, t2
z}

z, t2

z ← Equations (23) and (24) in Lemma 2

zk > zmax. The algorithm returns the sequences of
Lasso solutions and transition points.

Choice of [zmin, zmax]. Under the normality, very
positive and negative values of z does not aﬀect the
inference. Therefore, it is reasonable to consider range
of values, e.g., [−20σ, 20σ] (Liu et al., 2018), where σ
is the standard deviation of the sampling distribution
of test statistic.

3.4 Characterization of CV-based Tuning

Parameter Selection Event

In this section, we introduce a new way to character-
ize the minimal selection event that λ is chosen based
on the data, e.g., via cross-validation, which is compli-
cated and thus none of the currently available Lasso SI
methods can handle. Given a set of regularization pa-
rameter candidates Λ, we denote V(yobs) = λobs ∈ Λ
is the event that λobs is selected when performing val-
idation on yobs. The conditional inference on selected
feature j when applying Lasso on {X, yobs} is then
deﬁned as

where ZCV = {z ∈ R | A(y(z)) = A(yobs), V(y(z)) =
V(yobs)}. We now can easily construct Z1 = {z ∈ R |
A(y(z)) = A(yobs)} by using the proposed method in
previous parts. The remaining task is to identify Z2 =
{z ∈ R | V(y(z)) = V(yobs)}. Finally, ZCV = Z1 ∩ Z2.

For notational simplicity, we consider the case where
the data is divided into training and validation
sets, and the latter is used for selecting λ. The
following discussion can be easily extended to cross-
Let us re-write {X, yobs} =
validation scenario.
(cid:8)(Xtrain Xval)(cid:62) ∈ Rn×p, (yobs
val )(cid:62) ∈ Rn(cid:9) .
For
λ ∈ Λ, the Lasso problem on parametrized training
response vector is written as

train yobs

ˆβλ(z) ∈ arg min
β∈Rp

1
2

(cid:107)ytrain(z) − Xtrainβ(cid:107)2

2 + λ(cid:107)β(cid:107)1.

The validation error is deﬁned as Eλ(z) = 1
2 (cid:107)yval(z) −
Xval ˆβλ(z)(cid:107)2
2. Then, we can re-deﬁned Z2 = {z ∈
R | Eλobs(z) ≤ Eλ(z) for any λ ∈ Λ}. Since ˆβλ(z)
is a piecewise-linear function of z and yval(z) is a
linear function of z, the validation error Eλ(z) is a
picecewise-quadratic function of z. Now,
for each
λ ∈ Λ, we have a corresponding picecewise-quadratic
function of z. Finally, we can identify Z2 by ﬁnd-
ing the intervals of z in which the validation error
Eλobs(z) corresponding to λobs is minimum among a
set of picecewise-quadratic functions.

3.5 The Generality of the Proposed Method

Since we can eﬃciently compute the path of Lasso solu-
tions, our proposed method is ﬂexible and can be easily
extended to various respects. In Liu et al. (2018), the
main limitations are their method can not be applied
when p > n, or requires huge computation time. With
our method, all these limitations are resolved. We pro-
vide detailed discussions and solutions in Appendices
6.2.2 and 6.2.3. Besides, we also apply the proposed
method to other respects, which can not be solved by
the methods in Lee et al. (2016) and Liu et al. (2018),
including characterizing the minimum amount of con-
ditioning in elastic net (Zou and Hastie, 2005) (Ap-
pendix 6.2.1), marginal model (Appendix 6.2.4), and
interaction model (Appendix 6.2.5).

j Y | {A(Y ) = A(yobs),
η(cid:62)

V(Y ) = V(yobs), q(Y ) = q(yobs)}.

(25)

4 Experiment

The conditional data space in (11) with validation se-
lection event is re-deﬁned as

Y = {y(z) = a + bz | z ∈ ZCV},

(26)

In this section, we will demonstrate the performance
of the proposed method. Here, we present the main
results. Several additional experiments can be found
in Appendix 6.3.

Vo Nguyen Le Duy, Ichiro Takeuchi

4.1 Experimental Setup

We executed the code on Intel(R) Xeon(R) CPU E5-
2687W v4 @ 3.00GHz.

Methods for comparison. We show the false posi-
tive rates (FPRs), true positive rates (TPRs) and con-
ﬁdence intervals (CIs) for the following cases of condi-
tional inferences:

• TN-A: conditional inference without sign condition-
ing, which is mainly focused in this paper,

j Y | (cid:8)A(Y ) = Aobs, q(Y ) = q(yobs)(cid:9) .
η(cid:62)

• TN-As: conditional inference with additional sign
conditioning, which is mainly focused in Lee et al.
(2016),

(a) FPR

(b) TPR

(c) CI demonstration

(d) Length of CI

j Y | (cid:8)A(Y ) = Aobs, s = sobs, q(Y ) = q(yobs)(cid:9) ,
η(cid:62)

Figure 2: Results of false positive rate (FPR) control,
true positive rate (TPR) and conﬁdence interval (CI).

where s is the sign vector of Lasso solutions on Y , and
sobs is the sign vector of the Lasso solutions on yobs.

We also show the FPRs, TPRs and CIs of data split-
ting (DS) method (Cox, 1975), which is the commonly
used procedure for the purpose of selection bias correc-
tion. In this approach, the data is randomly divided
in two halves — one half is used for model selection
and the other half is used for inference.

Synthetic data generation. We generated n out-
comes as yi = x(cid:62)
i β + εi, i = 1, ..., n, where xi ∼
N(0, Ip) in which p = 5, and εi ∼ N(0, 1). Here, we
assume that the variance of the noise is known.
In
practice, the variance can be estimated from indepen-
dent data. We set the regularization parameter λ = 1
and signiﬁcance level α = 0.05. We used Bonferroni
correction to account for the multiplicity in all the ex-
periments. If we test m selected features (hypotheses)
at the same time, then the Bonferroni correction would
test each individual hypothesis at α∗ = α/m. For the
FPR experiments, all elements of β were set to 0 and
we set n ∈ {100, 200, 300, 400, 500}. For the TPR ex-
periments, the ﬁrst two elements of β were set to 0.25.
We ran 100 trials for each n ∈ {50, 100, 150, 200}, and
we repeated this experiments 10 times. For the exper-
iments of CIs, we set n = 100, p = 10, and the ﬁrst 5
elements of β were set to 0.25.

Deﬁnition of TPR.
In SI, we only conduct sta-
tistical testing when there is at least one hypothesis
discovered by the algorithm. Therefore, the deﬁnition
of TPR, which can be also called conditional power, is
as follows:

TPR =

# correctly detected & rejected
# correctly detected

,

where # correctly detected is the number of truly pos-
itive features selected by the algorithm (e.g., Lasso)
and # rejected is the number of truly positive features
whose null hypothesis is rejected by SI.

4.2 Numerical Results

The results of FPRs, TPRs and CIs. The re-
sults of FPR and TPR are shown in Figures 2a and
2b. In three cases, the FPRs are properly controlled
under the signiﬁcance level α. Regarding the TPR
comparison, it is obvious that TN-A has the highest
power. In regard to CI experiments, we note that the
number of selected features between Lasso and DS can
be diﬀerent. Therefore, for a fair comparison, we only
consider the features that are selected in both meth-
ods. In our experiments, since 9 features were selected
by the Lasso in the cases of TN-A and TN-As while
only 8 features were selected in the case of DS, we only
show the 95% CI of the features that are selected in
both cases in Figure 2c. The lengths of CI obtained
by TN-A are almost the shortest. We repeated this
experiment 100 times and showed the boxplot of the
lengths of the conﬁdence intervals in Figure 2d.
In
summary, the CI results are consistent with the TPR
results, i.e., TN-A has the shortest length of CI which
indicates it has the highest power.

The results when accounting CV selection
event. We also demonstrate the TPRs and the
lengths of CIs between the case when λ = 20 is ﬁxed
and λ is selected from the set Λ1 = {2−1, 20, 21} or
Λ2 = {2−10, 2−9, ..., 29, 210}. We show that the TPR
tends to decrease when increasing the size of Λ as

Parametric Programming Approach for More Powerful and General Lasso Selective Inference

Figure 3: Demonstration of TPR when accounting
cross-validation selection event.

Figure 4: Demonstration of CI length when consider-
ing cross-validation selection event.

shown in Figure 3. This is due to the fact that when we
increase the size of Λ, we have to condition on more
information which leads to shorter truncation inter-
val and results low TPR. The TPR results are consis-
tent with the CI results shown in Figure 4 in which
the length of CI is longer when increasing the size of
Λ. Besides, we also conducted TPR comparison be-
tween our method and the over-conditioning version
proposed in Loftus (2015). The results are shown in
Figure 5. Our method has higher power since we can
characterize minimum amount of conditioning.

The eﬃciency of the proposed method.
In Lee
et al. (2016), the authors mentioned the naive way
to remove sign conditioning by enumerating all possi-
ble combination of signs 2|Aobs| which is only feasible
when |Aobs| is small. On the left-hand side of Figure
6, we show the eﬃciency of our method compared to
the naive way of removing sign conditioning. On the
right-hand side of Figure 6, the Lasso SI without con-
ditioning on signs can be done even when n = 10, 000,
p = 10, 000 and thousands of features are selected
while the naive way can not ﬁnish the task in realistic

(a) Λ1 = {2−1, 20, 21}

(b) Λ2 = {2−10, ..., 210}

Figure 5: TPR comparison with the existing method
(Loftus, 2015) when accounting CV selection event.

Figure 6: Eﬃciency of the proposed method. With our
method, Lasso SI without conditioning on signs can be
done even when thousands of features are selected.

time. We also additionally show the eﬃciency of our
method compare to two methods in Liu et al. (2018),
which we call TN-(cid:96)1 and TN-Custom. The details of
these two methods are shown in Appendix 6.2.3. In
general, to perform these two methods, we still need to
naively enumerate all possible combinations of signs.
The results are shown in Figure 7.

One might wonder how we can circumvent the compu-
tational bottleneck of exponentially increasing num-
ber of polytopes. Our experience suggests that, by
focusing on the the line along the test-statistic in data
space, we can skip majority of the polytopes that do
not aﬀect the truncated Normal sampling distribution
because they do not intersect with this line. In other
words, we can skip majority of combinations of signs
that never appear.

In Figure 8, we show the boxplot of the actual number
of intervals of z that we encountered on the line when
constructing the truncation region Z. This indicates
that the number of polytopes intersecting the line z
that we need to consider is much smaller than 2|Aobs|,
which is considered in Lee et al. (2016)—this is the
reason why the proposed approach can resolve all ma-
jor limitations of the current SI method, making Lasso
SI more powerful and practical.

We did not compare the computational time between
the proposed method TN-A and the over-conditioning
version TN-As because TN-As is obviously faster than
TN-A but it has lower power than TN-A. Our main
purpose is to demonstrate that the proposed method

Vo Nguyen Le Duy, Ichiro Takeuchi

Table 1: Results on high-dimensional real-world bioin-
formatics related datasets.

Figure 7: Comparison between the proposed method
and methods in Liu et al. (2018), in which an expo-
nentially increasing number of all possible sign combi-
nations are still required.

Dataset 3

133

5787

n

89

76

Dataset 1

Dataset 2

5787

5144

p

|Aobs| Avg. Time (s)

600

621

660

0.374

0.344

0.342

ran 1,200 trials for each n ∈ {100, 200, 300, 400}. We
conﬁrmed that our method still maintains good per-
formance on FPR control. The results are shown in
Appendix 6.3.

4.3 Results on Real-World Datasets

We demonstrate the eﬃciency of the proposed method
by applying it on high-dimensional real-world bioin-
formatics related datasets, which is available at http:
//www.coepra.org/CoEPrA_regr.html. In datasets 1
and 3, n is the number of nona-peptides. Each amino
acid in a nona-peptide is described by 643 descrip-
tors, for a total of p = 643 × 9 = 5787 descriptors.
In dataset 2, n is the number of octa-peptides. Each
amino acid in a octa-peptide is described by 643 de-
scriptors, for a total of p = 643 × 8 = 5144 descriptors.
For these experiments, we used elastic net instead of
Lasso to obtain large Aobs. The extension of the pro-
posed method for elastic net is presented in Appendix
6.2.1. The results are shown in Table 1. The time
shown in the table is the average time to compute p-
value for a selected feature.

5 Conclusion

In this paper, we have proposed a general method for
characterizing the selection event of Lasso SI by in-
troducing piecewise-linear parametric programing ap-
proach. With the proposed method, we can conduct a
powerful SI by conditioning only on the selected fea-
tures without the need of enumerating all possible sign
vectors. Besides, we also introduced a new way to
charactering the cross-validation based tuning param-
eter selection. The proposed method not only over-
comes the drawbacks of current Lasso SI methods but
also improves the performance and practicality of SI
for Lasso in various respects. Our idea is general and
can be applied to circumvent several drawbacks of all
the methods that are based on the current SI frame-
work. We conducted experiments on both synthetic
and real-world datasets to demonstrate the eﬀective-
ness and eﬃciency of our proposed method.

Figure 8: Number of encountered intervals on the line.

not only has high statistical power but also has prac-
tically computational costs.

We note that, in the worst-case, the complexity of the
proposed method still grows exponentially. This is a
common issue in other parametric programming ap-
plications such as regularization paths. However, for-
tunately, it has been well-recognized that this worst
case rarely happens in practice, and our experiments
suggest that this also applies to PP-based SI.

The robustness of the proposed method in
terms of the FPR control. We demonstrate the
robustness of our method in terms of the FPR control
by considering the following cases:

• Non-normal noise: we consider the noise following
Laplace distribution, skew normal distribution (skew-
ness coeﬃcient 10), and t20 distribution.

• Unknown σ2: we also consider the case when the
variance is estimated from the data.

We generated n outcomes as yi = x(cid:62)
i β+εi, i = 1, ..., n,
where p = 5, xi ∼ N(0, Ip), and εi follows Laplace dis-
tribution, skew normal distribution, or t20 distribution
with zero mean and standard deviation was set to 1.
In the case of estimated σ2, εi ∼ N(0, 1). We set all
elements of β to 0, and set λ = 0.5. For each case, we

Parametric Programming Approach for More Powerful and General Lasso Selective Inference

Acknowledgements

This work was partially supported by MEXT KAK-
ENHI (20H00601, 16H06538), JST CREST (JP-
MJCR1502), RIKEN Center for Advanced Intelligence
Project, and RIKEN Junior Research Associate Pro-
gram.

References

E. L. Allgower and K. George. Continuation and path

following. Acta Numerica, 2:1–63, 1993.

F. R. Bach, D. Heckerman, and E. Horvits. Consider-
ing cost asymmetry in learning classiﬁers. Journal
of Machine Learning Research, 7:1713–41, 2006.

F. Bachoc, H. Leeb, and B. M. P¨otscher. Valid conﬁ-
dence intervals for post-model-selection predictors.
arXiv preprint arXiv:1412.4605, 2014.

F. Bachoc, G. Blanchard, and P. Neuvial. On the post
selection inference constant under restricted isome-
try properties. Electronic Journal of Statistics, 12
(2):3736–3757, 2018.

M. J. Best. An algorithm for the solution of the para-
metric quadratic programming problem. Applied
Mathemetics and Parallel Computing, pages 57–76,
1996.

A. Charkhi and G. Claeskens. Asymptotic post-
selection inference for the akaike information crite-
rion. Biometrika, 105(3):645–664, 2018.

S. Chen and J. Bien. Valid inference corrected for out-
lier removal. Journal of Computational and Graph-
ical Statistics, pages 1–12, 2019.

Y. Choi, J. Taylor, and R. Tibshirani. Selecting the
number of principal components: Estimation of the
true rank of a noisy matrix. The Annals of Statistics,
45(6):2590–2617, 2017.

D. R. Cox. A note on data-splitting for the evalua-
tion of signiﬁcance levels. Biometrika, 62(2):441–
444, 1975.

V. N. L. Duy, S. Iwazaki, and I. Takeuchi. Quan-
tifying statistical signiﬁcance of neural network
representation-driven hypotheses by selective infer-
ence. arXiv preprint arXiv:2010.01823, 2020a.

V. N. L. Duy, H. Toda, R. Sugiyama, and I. Takeuchi.
Computing valid p-value for optimal changepoint
by selective inference using dynamic programming.
arXiv preprint arXiv:2002.09132, 2020b.

W. Fithian, J. Taylor, R. Tibshirani, and R. Tibshi-
rani. Selective sequential model selection. arXiv
preprint arXiv:1512.02565, 2015.

T. Gal. Postoptimal Analysis, Parametric Program-
ming, and Related Topics. Walter de Gruyter, 1995.

P. Garrigues and L. Ghaoui. An homotopy algorithm
for the lasso with online observations. Advances in
neural information processing systems, 21:489–496,
2008.

T. Hastie, S. Rosset, R. Tibshirani, and J. Zhu. The
entire regularization path for the support vector ma-
chine. Journal of Machine Learning Research, 5:
1391–415, 2004.

T. Hastie, R. Tibshirani, and M. Wainwright. Statisti-
cal learning with sparsity: the lasso and generaliza-
tions. CRC press, 2015.

T. Hocking, j. P. Vert, F. Bach, and A. Joulin. Clus-
terpath: an algorithm for clustering using convex
fusion penalties. In Proceedings of the 28th Interna-
tional Conference on Machine Learning, pages 745–
752, 2011.

S. Hyun, K. Lin, M. G’Sell, and R. J. Tibshirani. Post-
selection inference for changepoint detection algo-
rithms with application to copy number variation
data. arXiv preprint arXiv:1812.03644, 2018.

M. Karasuyama and I. Takeuchi. Nonlinear regular-
ization path for quadratic loss support vector ma-
chines. IEEE Transactions on Neural Networks, 22
(10):1613–1625, 2010.

M. Karasuyama, N. Harada, M. Sugiyama, and
I. Takeuchi. Multi-parametric solution-path al-
gorithm for instance-weighted support vector ma-
chines. Machine Learning, 88(3):297–330, 2012.

G. Lee and C. Scott. The one class support vector
machine solution path. In Proc. of ICASSP 2007,
pages II521–II524, 2007.

J. D. Lee, D. L. Sun, Y. Sun, and J. E. Taylor. Ex-
act post-selection inference, with application to the
lasso. The Annals of Statistics, 44(3):907–927, 2016.

J. Lei. Fast exact conformalization of the lasso using
piecewise linear homotopy. Biometrika, 106(4):749–
764, 2019.

K. Liu, J. Markovic, and R. Tibshirani. More power-
ful post-selection inference, with application to the
lasso. arXiv preprint arXiv:1801.09037, 2018.

B. Efron and R. Tibshirani. Least angle regression.

J. R. Loftus. Selective inference after cross-validation.

Annals of Statistics, 32(2):407–499, 2004.

arXiv preprint arXiv:1511.08866, 2015.

W. Fithian, D. Sun, and J. Taylor. Optimal

ference after model selection.
arXiv:1410.2597, 2014.

in-
arXiv preprint

J. R. Loftus and J. E. Taylor. A signiﬁcance test for
forward stepwise model selection. arXiv preprint
arXiv:1405.3920, 2014.

Vo Nguyen Le Duy, Ichiro Takeuchi

regression procedures. Journal of the American Sta-
tistical Association, 111(514):600–620, 2016.

K. Tsuda. Entire regularization paths for graph data.
In In Proc. of ICML 2007, pages 919–925, 2007.

F. Yang, R. F. Barber, P. Jain, and J. Laﬀerty. Selec-
tive inference for group-sparse linear models. In Ad-
vances in Neural Information Processing Systems,
pages 2469–2477, 2016.

H. Zou and T. Hastie. Regularization and variable
selection via the elastic net. Journal of the royal
statistical society: series B (statistical methodology),
67(2):301–320, 2005.

J. Markovic, L. Xia, and J. Taylor. Unifying ap-
proach to selective inference with applications to
cross-validation. arXiv preprint arXiv:1703.06559,
2017.

M. R. Osborne, B. Presnell, and B. A. Turlach. A
new approach to variable selection in least squares
problems. IMA Journal of Numerical Analysis, 20
(20):389–404, 2000.

S. Panigrahi, J. Taylor, and A. Weinstein. Bayesian
post-selection inference in the linear model. arXiv
preprint arXiv:1605.08824, 28, 2016.

K. Ritter. On parametric linear and quadratic pro-
gramming problems. mathematical Programming:
Proceedings of the International Congress on Math-
ematical Programming, pages 307–335, 1984.

S. Rosset. Following curved regularized optimization
solution paths. In Advances in Neural Information
Processing Systems 17, pages 1153–1160, 2005.

S. Rosset and J. Zhu. Piecewise linear regularized solu-
tion paths. Annals of Statistics, 35:1012–1030, 2007.

K. Sugiyama, V. N. L. Duy, and I. Takeuchi. More
powerful and general selective inference for stepwise
feature selection using the homotopy continuation
approach. arXiv preprint arXiv:2012.13545, 2020.

S. Suzumura, K. Nakagawa, Y. Umezu, K. Tsuda, and
I. Takeuchi. Selective inference for sparse high-order
interaction models. In Proceedings of the 34th Inter-
national Conference on Machine Learning-Volume
70, pages 3338–3347. JMLR. org, 2017.

I. Takeuchi, K. Nomura, and T. Kanamori. Non-
parametric conditional density estimation using
piecewise-linear solution path of kernel quantile re-
gression. Neural Computation, 21(2):539–559, 2009.

K. Tanizaki, N. Hashimoto, Y. Inatsu, H. Hontani,
and I. Takeuchi. Computing valid p-values for image
segmentation by selective inference. In Proceedings
of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 9553–9562, 2020.

Y. Terada and H. Shimodaira. Selective inference after
variable selection via multiscale bootstrap. arXiv
preprint arXiv:1905.10573, 2019.

X. Tian and J. Taylor. Selective inference with a ran-
domized response. The Annals of Statistics, 46(2):
679–710, 2018.

R. Tibshirani. Regression shrinkage and selection via
the lasso. Journal of the Royal Statistical Society:
Series B (Methodological), 58(1):267–288, 1996.

R. J. Tibshirani. The lasso problem and uniqueness.
Electronic Journal of statistics, 7:1456–1490, 2013.

R. J. Tibshirani, J. Taylor, R. Lockhart, and R. Tib-
shirani. Exact post-selection inference for sequential

Parametric Programming Approach for More Powerful and General Lasso Selective Inference

6 Appendix

6.1 Detailed Proof for Lemma 2

From Equation (16), we can see that ˆβAz (z) is a function of z. For a real value z, there exists t1
for any real value z(cid:48) in [z, z + t1
Equation (17), we can see that sAc
any real value z(cid:48) in [z, z + t2
tz = min{t1
remaining task is to compute t1

z such that
z), all elements of ˆβAz(cid:48) (z(cid:48)) remain the same signs with ˆβAz (z). Similarly, from
z such that for
z(cid:48) (z(cid:48)) are smaller than 1 in absolute value. Finally, by taking
z}, we obtain the interval in which the active set and signs of lasso solution remain the same. The

(z) is a function of z. Then, for a real value z, there exists t2

z), all elements of sAc

z, t2

z

We ﬁrst show how to derive t1

z and t2
z.
z. From Equation (16), we have

To guarantee ˆβAz (z(cid:48)) and ˆβAz (z) have the same signs,

ˆβAz (z(cid:48)) − ˆβAz (z) = ψAz (z) × (z(cid:48) − z).

sj(z(cid:48)) = sj(z),

∀j ∈ Az.

(27)

For a speciﬁc j ∈ Az, we consider the following cases:

• If ˆβj(z) > 0, then ˆβj(z(cid:48)) = ˆβj(z) + ψj(z) × (z(cid:48) − z) > 0.

– If ψj(z) > 0, then z(cid:48) − z > −

ˆβj (z)
ψj (z) (This inequality always holds since the left hand side is positive

while the right hand side is negative).

– If ψj(z) < 0, then z(cid:48) − z < −

ˆβj (z)
ψj (z) .

• If ˆβj(z) < 0, then ˆβj(z(cid:48)) = ˆβj(z) + ψj(z) × (z(cid:48) − z) < 0.

– If ψj(z) > 0, then z(cid:48) − z < −

– If ψj(z) < 0, then z(cid:48) − z > −

ˆβj (z)
ψj (z) .
ˆβj (z)
ψj (z) (This inequality always holds since the left hand side is positive

while the right hand side is negative).

Finally, for satisfying the condition in Equation (27),

z(cid:48) − z < min
j∈Az

(cid:33)

(cid:32)

−

ˆβj(z)
ψj(z)

= t1
z.

++

We next show how to derive t2

z. From Equation (17), we have
(z(cid:48)) − λsAc

(z) = γAc

λsAc

z

z

z

(z) × (z(cid:48) − z).

To guarantee (cid:107)λsAc

z

(z(cid:48))(cid:107)∞ = (cid:107)λsAc

z

(z) + γAc

z

(z) × (z(cid:48) − z)(cid:107)∞ < λ,

−λ < λsj(z) + γj(z) × (z(cid:48) − z) < λ,

∀j ∈ Ac
z.

(28)

For a speciﬁc j ∈ Ac

z, we have the following cases:

• If γj(z) > 0, then −λ−λsj (z)

• If γj(z) < 0, then λ−λsj (z)

γj (z) < z(cid:48) − z < λ−λsj (z)
γj (z) < z(cid:48) − z < −λ−λsj (z)

γj (z)

γj (z)

.

.

Note that the ﬁrst inequalities of the above two cases always hold since the left hand side is negative while the
right hand side is positive). Then, for satisfying the condition in Equation (28),

z(cid:48) − z < min
j∈Ac
z

(cid:18)

λ

sign(γj(z)) − sj(z)
γj(z)

(cid:19)

= t2
z.

++

Finally, we can compute tz by taking tz = min (cid:8)t1

z, t2
z

(cid:9).

Vo Nguyen Le Duy, Ichiro Takeuchi

6.2 Derivations of the Proposed Method for Various Settings

6.2.1 Elastic Net

In some cases, the lasso solutions are unstable. One way to stabilize them is to add an (cid:96)2 penalty to the objective
function, resulting in the elastic net (Zou and Hastie, 2005). Therefore, we extend our proposed method and
provide detailed derivation for testing the selected features in elastic net case. We now consider the optimization
problem with parametrized response vector y(z) for z ∈ R as follows

ˆβ(z) = arg min

β∈Rp

1
2n

(cid:107)y(z) − Xβ(cid:107)2

2 + λ(cid:107)β(cid:107)1 +

1
2

δ(cid:107)β(cid:107)2
2.

For any z in R, the optimality condition is given by

X (cid:62) (cid:16)

1
n

X ˆβ(z) − y(z)

(cid:17)

+ λs(z) + δ ˆβ(z) = 0, s(z) ∈ ∂(cid:107) ˆβ(z)(cid:107)1.

(29)

(30)

Similar to lasso case, to construct the truncation region Z, we have to 1) compute the entire path of ˆβ(z) in
Equation (29), and 2) identify a set of intervals of z on which A(y(z)) = A(yobs).
Lemma 3. Let us consider two real values z(cid:48) and z (z(cid:48) > z). If ˆβAz (z) and ˆβAz(cid:48) (z(cid:48)) have the same active set
and the same signs, then we have

ˆβAz (z(cid:48)) − ˆβAz (z) = ψAz (z) × (z(cid:48) − z),
(z) × (z(cid:48) − z),
(z) = γAc

(z(cid:48)) − λsAc

z

z

λsAc

z

where ψAz (z) = (X (cid:62)
Az

XAz + nδI|Az|)−1X (cid:62)
Az

b, and γAc

z

(z) = 1

n (X (cid:62)
Ac
z

b − X (cid:62)
Ac
z

XAz ψAz (z)).

Proof. From the optimality conditions of the elastic net (30) , we have

(X (cid:62)
Az

XAz + nδI|Az|) ˆβAz (z) − X (cid:62)
Az

(X (cid:62)

Az(cid:48) XAz(cid:48) + nδI|Az(cid:48) |) ˆβAz(cid:48) (z(cid:48)) − X (cid:62)

y(z) + nλsAz (z) = 0,
Az(cid:48) y(z(cid:48)) + nλsAz(cid:48) (z(cid:48)) = 0.

(31)

(32)

(33)

(34)

By substracting (33) from (34) and Az = Az(cid:48), we have

ˆβAz (z(cid:48)) − ˆβAz (z) = (X (cid:62)
Az
= (X (cid:62)
Az
= (X (cid:62)
Az

XAz + nδI|Az|)−1X (cid:62)
Az
XAz + nδI|Az|)−1X (cid:62)
Az
XAz + nδI|Az|)−1X (cid:62)
Az

(y(z(cid:48)) − y(z))
(a + bz(cid:48) − a − bz)
b × (z(cid:48) − z).

Thus, we achieve Equation (31). Similarly, we can write the optimality conditions with XAc
easily obtain Equation (32).

z

for z and z(cid:48), and

Now, we can see that ˆβAz (z) and sAc
(z) are functions of z. Then, for a real value z, there exists tz such that for
any real value z(cid:48) in [z, z + tz), all elements of ˆβAz(cid:48) (z(cid:48)) remain the same signs with ˆβAz (z), and all elements of
z(cid:48) (z(cid:48)) are strictly smaller than 1 in absolute value. The value of tz can be computed by Lemma 2 as in lasso
sAc
case.

z

6.2.2 Full Target Case

In the full target case, as discussed in Liu et al. (2018), the data is used to choose the interesting features but
it is not used for summarizing the relation between the response and the selected features. Therefore, we can
always use all the features to deﬁne the direction of interest

where ej ∈ Rp is a zero vector with one at its jth coordinate. The conditional inference is deﬁned as

j Y | (cid:8)j ∈ A(Y ), q(Y ) = q(yobs)(cid:9) .
η(cid:62)

(35)

ηj = X(X (cid:62)X)−1ej,

Parametric Programming Approach for More Powerful and General Lasso Selective Inference

In Liu et al. (2018), the authors proposed a solution to conduct conditional inference for a speciﬁc case when
p < n, and there is no solution for the case when p > n. With the proposed parametric programming method,
we can solve this problem. We ﬁrst re-write the conditional inference in (35) as the problem of characterizing
the sampling distribution of

Z | {Z ∈ Z} where Z = {z ∈ R | j ∈ A(y(z))}.

(36)

The y(z) in (36) is deﬁned as in (11). Then, to identify Z, we only need to obtain the path of Lasso solution ˆβ(z)
as we proposed in §3, and simply check the intervals in which j is an element of the active set corresponding to
ˆβ(z) along the path. Finally, after having Z, we can easily compute the selective p-value or selective conﬁdence
interval.

6.2.3 Stable Partial Target Case

In the stable partial target case, as discussed in Liu et al. (2018), we only allow stable features to inﬂuence the
formation of the test-statistic. The stable features are those with very strong signals and we would not to miss
out. We will choose a set Hobs of stable features. Then, for any j ∈ Hobs, j ∈ Aobs,

ηj = XHobs(X (cid:62)

Hobs

XHobs)−1ej.

And, for any j (cid:54)∈ Hobs, j ∈ Aobs,

We next show how to construct Hobs according to Liu et al. (2018).

ηj = XHobs∪{j}(X (cid:62)

Hobs∪{j}XHobs∪{j})−1ej.

In this case, Hobs is the lasso active set
Stable target formation by setting higher value of λ (TN-(cid:96)1).
but with a higher value of λ than the one was used to select Aobs. We denote Hobs = H(yobs), the conditional
inference is then deﬁned as

j Y | (cid:8)j ∈ A(Y ), H(Y ) = H(yobs), q(Y ) = q(yobs)(cid:9) .
η(cid:62)

(37)

The main drawback of the method in Liu et al. (2018) is that they have to consider all 2|Hobs| sign vectors, which
requires huge computation time when |Hobs| is large. With our piecewise-linear homotopy computation, we can
easily overcome this drawback. We ﬁrst re-write the conditional inference in (37) as the problem of characterizing
the sampling distribution of

Z | {Z ∈ Z} where Z = {z ∈ R | j ∈ A(y(z)), H(y(z)) = H(yobs)}.

(38)

We now can easily identify Z = Z1 ∩ Z2, where Z1 = {z ∈ R | j ∈ A(y(z))} which is the same with full target
case, and Z2 = {z ∈ R | H(y(z)) = H(yobs)} which we can simply obtain by using the proposed method in §3
of the main paper.

Stable target formation by setting a cutoﬀ value c (TN-Custom).
setting a cutoﬀ value c for choosing βj such that |βj| ≥ c 1. The set Hobs is deﬁned as

In this case, we choose Hobs by

where βj = e(cid:62)
formulated as

j (X (cid:62)

Aobs

Hobs = {j ∈ Aobs, |βj| ≥ c} ,

XAobs )−1X (cid:62)

Aobs

yobs. We denote Hobs = H(Aobs) ⊂ Aobs, the conditional inference is then

η(cid:62)

j Y | {H(A(Y )) = H(Aobs), A(Y ) = Aobs} .

(39)

The main drawback of the method in Liu et al. (2018) is that they still require conditioning on {A(Y ) = Aobs},
which is computationally intractable when |Aobs| is large because the enumeration of 2|Aobs| sign vectors is
required. With our proposed method, we can easily overcome this drawback.

1We note that our formulation is slightly diﬀerent but more general than the one in Liu et al. (2018).

Vo Nguyen Le Duy, Ichiro Takeuchi

6.2.4 Marginal Model

In the case of marginal model, we can always decide a priori to investigate the marginal relationship between the
column j of feature matrix X and the observed response vector yobs if j is selected. The conditional inference
is deﬁned as

j Y | (cid:8)j ∈ A(Y ), q(Y ) = q(yobs)(cid:9) ,
η(cid:62)

(40)

where ηj = Xj(X (cid:62)
target case. The only diﬀerence between marginal model case and full target case is the formulation of ηj.

j Xj)−1ej. The solution for conducting this conditional inference is the same with the full

6.2.5

Interaction Model

Firstly, we apply Lasso on {X, yobs} to obtain the active set Aobs = A(yobs). Next, we construct a feature
matrix for interaction model as

Xinter = (XiXj)i,j∈Aobs,i<j ∈ Rn×d,

where d = 0.5|Aobs|(|Aobs| − 1). Then, the Lasso optimization problem for the interaction model is given by

ˆβ = arg min

β∈Rd

1
2

(cid:107)yobs − Xinterβ(cid:107)2

2 + λ(cid:107)β(cid:107)1.

Let us denote Ainter = Ainter(yobs) be the active set of the interaction model with yobs, the conditional inference
on the jth selected feature in Ainter is deﬁned as

η(cid:62)
j Y | {j ∈ Ainter(Y ), A(Y ) = A(yobs), q(Y ) = q(yobs)},

(41)

interXinter)−1ej in which ej ∈ Rd. We note that Ainter(Y ) is diﬀerent from A(Y ) which is
where ηj = Xinter(X (cid:62)
the active set when we apply Lasso on data {X, Y }. By restricting the response vector to a line as in (11), the
conditional inference in (41) is re-deﬁned as

Z | {Z ∈ Z} where Z = {z ∈ R | j ∈ Ainter(y(z)), A(y(z)) = A(yobs)}.

From now on, the process of identifying Z is straightforward which is based on the method we proposed in §3 of
the main paper and the extension for full target case in the Appendix.

6.3 Additional Experiments.

For the experiments, we executed the code on Intel(R) Xeon(R) CPU E5-2687W v4 @ 3.00GHz.

Eﬃciency of the proposed method. We checked the computation time of our extension for elastic net when
applying on synthetic data. The results are shown in Figure 9.

Figure 9: Computation time of our proposed method in elastic net case.

Parametric Programming Approach for More Powerful and General Lasso Selective Inference

Figure 10: The robustness of the proposed method in terms of the FPR control.

(a) TN-Full

(b) TN-A

(c) TN-As

(d) TN-Marginal

(e) TN-(cid:96)1

(f) TN-Custom

(g) TN-Interaction

(h) TN-Validation

Figure 11: Uniform QQ-plot of the pivotal quantity.

The robustness of the proposed method in terms of the FPR control. We applied our proposed
method to the case when the data follows Laplace distribution, skew normal distribution (skewness coeﬃcient
10), and t20 distribution. We also conducted experiments when σ2 is also estimated from the data. We generated
i β + εi, i = 1, ..., n, where p = 5, xi ∼ N(0, Ip), and εi follows Laplace distribution, skew
n outcomes as yi = x(cid:62)
normal distribution, or t20 distribution with zero mean and standard deviation was set to 1.
In the case of
estimated σ2, εi ∼ N(0, 1). We set all elements of β to 0, and set λ = 0.5. For each case, we ran 1,200 trials for
each n ∈ {100, 200, 300, 400}. The FPR results are shown in Figure 10.

Vo Nguyen Le Duy, Ichiro Takeuchi

Uniformity veriﬁcation of the pivotal quantity. We generated n = 100 outcomes as yi = x(cid:62)
i β + εi,
i = 1, ..., n, where p = 5, xi ∼ N(0, Ip), and εi ∼ N(0, 1). We set the ﬁrst two elements of β to 2, and set λ = 5.
We applied our method and ran 1,200 trials for each case of conditioning: TN-Full, TN-A, TN-As, TN-Marginal
(marginal model), TN-(cid:96)1, TN-Custom, TN-Interaction (interaction model), and TN-Validation (considering
validation selection event). For stable partial target formation, to identify Hobs, we set the value of higher λ to
15 in the case of TN-(cid:96)1, and cutoﬀ value c is set to 1 in the case of TN-Custom. We set Λ = {2−1, 20, 21} and
performed 5-fold cross-validation in the case of TN-Validation. The results are shown in Figure 11.

