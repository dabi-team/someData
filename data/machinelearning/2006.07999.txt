0
2
0
2

n
u
J

4
1

]
L
M

.
t
a
t
s
[

1
v
9
9
9
7
0
.
6
0
0
2
:
v
i
X
r
a

Support Estimation
with Sampling Artifacts and Errors

Eli Chien
ECE
UIUC
ichien3@illinois.edu

Olgica Milenkovic
ECE
UIUC
milenkov@illinois.edu

Angelia Nedich
School of Electrical, Computer and Energy Engineering
Arizona State University
Angelia.Nedich@asu.edu

Abstract

The problem of estimating the support of a distribution is of great importance in
many areas of machine learning, computer science, physics and biology. Most of
the existing work in this domain has focused on settings that assume perfectly ac-
curate sampling approaches, which is seldom true in practical data science. Here
we introduce the ﬁrst known approach to support estimation in the presence of
sampling artifacts and errors where each sample is assumed to arise from a Pois-
son repeat channel which simultaneously captures repetitions and deletions of
samples. The proposed estimator is based on regularized weighted Chebyshev ap-
proximations, with weights governed by evaluations of so-called Touchard (Bell)
polynomials. The supports in the presence of sampling artifacts are calculated
using discretized semi-inﬁte programming methods. The estimation approach is
tested on synthetic and textual data, as well as on GISAID data collected to ad-
dress a new problem in computational biology: mutational support estimation in
genes of the SARS-Cov-2 virus. In the later setting, the Poisson channel captures
the fact that many individuals are tested multiple times for the presence of viral
RNA, thereby leading to repeated samples, while other individual’s results are not
recorded due to test errors. For all experiments performed, we observed signiﬁ-
cant improvements of our integrated methods compared to those obtained through
adequate modiﬁcations of state-of-the-art noiseless support estimation methods.
Our code will be released upon acceptance.

1 Introduction

Estimating the support size of a discrete distribution is an important theoretical and data processing
problem [1, 2]. In computer science, this task frequently arises in large-scale database mining and
network monitoring where the objective is to estimate the types of database entries or IP addresses
from a limited number of observations [3, 4, 5]. In machine learning, support estimation is used
to bound the number of clusters in clustering problems encountered in semi-supervised or active
learning [6, 7, 8, 9]. In life sciences, support estimation arises when estimating population sizes
or increases in population sizes [10]. The most challenging practical support estimation issues are
encountered in the “small sample set” regime in which one has only a limited number of observations
for a distribution with a large support. In such a setting, classical maximum likelihood frequency
techniques are known to perform poorly [11]. It is for this sampling regime that the estimation

Preprint. Under review.

 
 
 
 
 
 
problem has received signiﬁcant attention from both the theoretical computer science and machine
learning community, as well as researchers from various computational data processing areas [12,
13, 14, 15, 5, 16, 17, 18].

×

By now, a number of efﬁcient and near-optimal support estimation techniques has been reported in
the literature [19, 20, 21, 22, 23, 24, 25]. All these methods traditionally use the assumption that
the samples are observed without errors. In practice, sampling artifacts and noise are ubiquitous,
especially when dealing with data acquired from biological and medical science experiments. As a
motivating example, consider the problem of estimating the number of mutations in a viral genome
(such as that of SARS-Cov-19) during the early stages of an outbreak. Viral RNA/DNA is usually of
10, 000 and testing is time consuming and expensive, and additionally hampered by privacy
length
issues. Consequently, a small number of sequenced genomes may be available when trying to deter-
mine in a timely manner if the virus is mutating at a high rate and therefore potentially dangerous
to the population (Note that large body of work reports mutational rates of viruses as indicators of
their virulence and potential to cause epidemic and pandemic outbreaks [26, 27].). In this particular
case, the actual alphabet size is known and equal to the length of the genome, but not all genomic
sites are subject to mutations. Furthermore, sequencing errors introduce counting artifacts, and so
do sampling biases which are caused by some individuals being tested multiple times (e.g., health
workers [28]) or not tested at all. To address these issues, we propose a novel noisy support esti-
mation problem under the Poisson repeat channel [29] model. The Poisson repeat channel models
both deletion and repetitions of particular mutational sites, and is adequate for capturing unknown
sampling biases and sequencing error phenomena. To the best of our knowledge, this work is the
ﬁrst to consider the noisy support estimation problem where symbols are potentially repeated or
missed. The only other line of work addressing a similar problem was reported in [30], with a focus
on Good-Turing distribution estimation in the presence of sample insertion errors.

We address the noisy small sample support estimation problem under the Poisson repeat channel
by using novel regularized weighted Chebyshev approximation techniques. Weighted polynomial
approximation techniques are largely unknown in the machine learning community [31] since all
previous approximation based methods proposed so far have only focused on unweighted and noise-
less settings [20, 22, 24, 25]. As will be shown in our subsequent analysis, exponential smoothing
and “noise-compensating” weights play a major role in improving the performance of polynomial
methods as well as making them computationally tractable. Within this framework, the Mhaskar-
Saff theorem and extensions thereof presented in the work are of great importance [31, 32, 33]. In
addition, our regularization term arises from consideration of the variance of the estimators, as the
weighted Chebyshev approximation component only takes into account the bias. Hence, solving our
regularized weighted Chebyshev approximation problem is equivalent to jointly optimizing the bias
and variance of the estimator. In addition, we show that Touchard (Bell) polynomials [34] naturally
arise when incorporating the Poisson repeat channel into the model. To numerically solve the un-
derlying optimization problem we use discretized semi-inﬁnite programming (SIP) techniques. We
prove that the solution of discretized SIP converges to the true unique optimal solution for the noisy
support estimation problem. Through extensive experiments on both synthetic and real-world data,
we show that our methods are able to accurately estimate the support size under the Poisson repeat
channel.

Prior work Noiseless support estimation methods operating in the small sample regime can be
roughly grouped into two categories [19, 20, 21, 22, 23, 24, 25]. The ﬁrst line of works [19, 21, 23]
makes use of the maximum likelihood principle. While [21] constructs estimators based on the Pro-
ﬁle Maximum Likelihood (PML) [35], the work reported in [19] focuses on Sequence Maximum
Likelihood (SML) estimators [36]. The main advantage of ML-based methods is that they easily
generalize to many other estimation tasks. For example, the authors of [21] showed that a single
method may be used for entropy estimation, support coverage and distance to uniformity analysis.
However, most ML-based estimators require large computational resources [23, 20]. To address
the computational issue, a sophisticated approximate PML technique that reduces the computational
complexity of support estimation at the expense of some performance loss was proposed in [23].
On the other hand, the second line of works [20, 22, 24, 25] formulates support estimation as an
approximation problem. The underlying methods, which we henceforth refer to as approximation-
based methods, design estimators by minimizing the worst case risk. In particular, [20] uses shifted
and scaled Chebyshev polynomials of the ﬁrst kind to construct efﬁcient estimators. In contrast, the
authors of [25] suggest disposing of minmax estimators and implementing a data ampliﬁcation tech-

2

nique with analytical performance guarantees. The aforementioned estimator is based on polynomial
smoothing [37] related to approximation techniques. All described approximation-based estimators
are computational efﬁcient, with the exception of [24], as reported in [25]. Sampling artifacts lead
to non-iid observations which are very different from Markovian models discussed in [38].

The paper is organized as follows. In Section 2, we introduce the relevant notation and the sup-
port estimation problem under the Poisson repeat channel. We also describe a class of estimators
termed polynomial class estimators. In Section 3, we outline our analysis of polynomial class esti-
mators and describe our main results needed to overcome the technical challenges associated with
regularized weighted minmax polynomial approximations. Section 4 is devoted to experimental ver-
iﬁcations and testing, both on synthetically generated data and real-world data. Our real world data
includes Shakespeare’s plays, used to illustrate the performance of our methods, and a collection of

4, 100 SARS-Cov-2 viral genomes retrieved from GISAID [39, 40].

∼

2 Problem formulation and a new class of polynomial estimators

Let P = (p1, p2, . . .) be a discrete distribution over some countable alphabet and let X1, . . . , Xn
be i.i.d. samples drawn according to the distribution P . The problem of interest is to estimate the
i 1{pi>0} where 1A stands for the indicator of the event A. When
support size, deﬁned as S(P ) =
clear from the context we use S instead of S(P ) to avoid notational clutter. We make the assumption
R+, i.e.,
that the minimum nonzero probability of the distribution P is at least 1/k, for some k
1
k . Furthermore, we let Dk denote the space of all probability distribution
inf
|
∈
1
satisfying inf
Dk. A sufﬁcient statistics for X1, . . . , Xn
k . Clearly, S
is the empirical distribution (i.e., histogram) N = (N1, N2, . . .), where Ni =

p > 0
p

} ≥
P
|

p > 0

n
j=1 1{Xj =i}.

} ≥

P

k,

≤

∈

∈

∈

P

P

∀

p

{

{

The Poisson repeat channel. For each sample index 1
n, the Poisson repeat channel with
≤
parameter η outputs Ri copies of the sample Xi, where Ri
P oisson(η) is a Poisson distributed
random variable with mean η. For simplicity of analysis, the variables Ri are assumed to be i.i.d.
and the corresponding channel memoryless; the value Ri = 0 indicates that a sample input Xi has
been deleted. We also deﬁne the empirical distribution of the output sequence of the Poisson channel
n
as N ′ = (N ′
j=1 Rj1{Xj =i}. Throughout the paper, we assume that the
parameter η is known although it is possible to learn it simultaneously with the support.

2, ...), where N ′

1, N ′

i =

P

∼

≤

i

The focal point of our analysis is to upper bound the minmax risk under normalized squared loss
2

2

R∗(n, k) = inf
ˆS

sup
P ∈Dk

E



ˆS(N )
k

S

−

and inf
ˆS

sup
P ∈Dk

E





!

ˆS(N ′)
k

S

−

which correspond to the case without and with Poisson repeat channel, respectively.







(1)

!

,





P

We focus on the case including the Poisson repeat channel, as a similar analysis can be easily per-
formed for the case without Poisson repeats. We seek a support estimator ˆS that minimizes

sup
P ∈Dk

E



ˆS(N ′)
k

S

−

2



!

= sup

P ∈Dk "

E2

ˆS(N ′)
k

S

−

!

+ var

ˆS(N ′)
k

S

−

.

!#





The ﬁrst term within the supremum captures the expected bias of the estimator ˆS. The second term
represents the variance of the estimator ˆS. Hence, “good” estimators are required to balance out the
worst-case contributions of the bias and variance.

N, we say that
We deﬁne a class of polynomial based estimators as follows. Given a parameter L
an estimator ˆS is a polynomial class estimator with the parameter L (i.e., a P oly(L) estimator) if it
takes the form ˆS =

i gL(Ni), where gL is deﬁned as

∈

P

gL(j) =

ajj! + 1,
1,

L

if j
≤
otherwise.

(2)

(cid:26)

Here, aj
estimator ˆS with its corresponding coefﬁcients a, and deﬁne a family of estimators

1, since this choice ensures that gL(0) = 0. One can associate an

−

∈

R, and a0 =

P oly(L) =

a

RL+1

∈

a0 =

|

−

1

.

(cid:8)

3

(cid:9)

 
 
 
 
 
Next, we show that the problem of minimizing worst-case risk within the class P oly(L) can be cast
as a regularized exponentially weighted Chebyshev approximation problem [31].

3 Estimator analysis

We start by analyzing the minmax risk under the Poisson channel model through Poissonization
arguments. These assert that the number of samples drawn is Poisson distributed N
P oisson(n)
and that the counts Ni
P oisson(λi) are independent, where λi = npi. Poissonization was also
used in [20, 24] to derive the following tight upper bound on the minmax risk.

∼

∼

Lemma 3.1 (Lemma 1 in [20]). Let R∗
Then, for any β > 1 we have

P (n, k) be the minmax risk under the Poissonized model.

R∗(n, k)

≤

R∗
1

−

P ((1

−
exp(
−

β)n, k)
nβ2/2)

.

(3)

Note that it is straightforward to show that Lemma 3.1 is also valid under the Poisson repeat channel.
Let
be the set of symbols with positive probability. A simple calculation reveals
=
|
that for any ˆS

P oly(L), one has

λℓ > 0

L

{

}

ℓ

∈

ˆS(N ′)
k

S

−

2

=

(cid:19)

E

(cid:18)

≤

1
k2

1
k2

E

gL(N ′
i )

i∈L

(cid:26) X

(cid:18)

E

gL(N ′
i )

i∈L

(cid:26) X

(cid:18)

1

1

−

−

(cid:19)
2

(cid:19)

2

+

E

g(N ′
i )

i6=j∈L
X

(cid:18)

E

1

g(N ′
i )

(cid:19)

(cid:18)

1

−

(cid:19)(cid:27)

−

+ (S

1)

−

E

g(N ′
i )

i∈L (cid:18)
X

(cid:18)

2

1

−

(cid:19)(cid:19)

,
(cid:27)

(4)

where the inequality we use Cauchy-Bunyakowski-Schwarz inequality for the cross terms. Note
that the ﬁrst term captures the variance while the second term is related to the bias. An interesting
observation is that the objective function above is symmetric in the parameters λi. The little-known
problem of establishing when the optima of such constrained symmetric functions is achieved for
the case that all parameters are equal has been studied in [41].
We ﬁrst analyze the bias term E (g(N ′
i )
have N ′

1). By the deﬁnition of the Poisson repeat channel, we

P oisson(ηNi), which allows us to write E (g(N ′
i )

1) as

−

−

i ∼

h=0
X

L

=

l=0
X

∞

P(Ni = h)

P(N ′

i = l

|

L

l=0
X
∞

e−λialηl

h=0
X

Ni = h)

all!

=

L

∞

λh
i
h!

e−λi

(ηh)l
l!

×

e−ηh

all!

×

(λie−η)h
h!

hl

×

−η

(cid:19)

l=0
X

(cid:18)
h=0
X
e−λie
e−λie−η = e−λi(1−e

−η )

alηlM (l)
N ∗
i

(0),

(5)

L

l=0
X

i

is the moment generating function (MGF) of

(0) has a closed form expression of the form of Touchard (Bell) polynomials [34] in

i ∼
(0) to denote the lth derivative of the MGF at 0. It is worth not-

where MN ∗
P oisson(λie−η). We use M (l)
N ∗
i
ing that M (l)
N ∗
i
λie−η. More speciﬁcally, we have M (l)
denotes the Stir-
N ∗
i
ling number of the second kind, counting the number of partitions of a set of size l into r disjoint
nonempty subsets. Following a procedure similar to the one used for the bias, one can be shown that
the term corresponding to the variance equals

(λie−η)r, where

the random variable N ∗

(0) =

l
r=0

P

(cid:8)

(cid:8)

(cid:9)

(cid:9)

l
r

l
r

E

g(N ′
i )

(cid:18)

2

1

−

(cid:19)

= e−λi(1−e

−η )

L

l=0
X

l ηll!M (l)
a2
N ∗
i

(0).

(6)

4

By plugging (5), (6) into (4) and taking the supremum over Dk, we have

sup
P ∈Dk

E

(cid:18)

ˆS(N ′)
k

S

−

2

(cid:19)

sup
k ,n], ℓ∈L

λℓ∈[ n

≤

1
k2

≤

E

i∈L

(cid:26) X
−η )

(cid:18)
L

1
k

e−λ(1−e

sup
λ∈[ n

≤

sup
k ,n], ℓ∈L

λℓ∈[ n

ˆS(N ′)
k

S

−

2

(cid:19)

E

(cid:18)
2

gL(N ′
i )

+ (S

1

−

(cid:19)

−

1)

E

g(N ′
i )

l ηll!M (l)
a2

N ∗(0) +

i∈L (cid:18)
X
e−λ(1−e

−η )

(cid:18)
L

1

−

2

(cid:19)(cid:19)

(cid:27)
2

alηlM (l)

N ∗ (0)

,
(cid:27)

k ,n] (cid:26)
(cid:18)
P oisson(λe−η). The inequality (7) is due to the increase in the domain over which
where N ∗
we take the supremum. The inequality (8) follows from (4). The last inequality is a consequence
of
k and the fact that all terms in the summation are nonnegative. Hence we have to
minimize an objective with respect to the coefﬁcients a1, ..., aL according to

l=0
X

l=0
X

= S

|L|

≤

∼

(cid:19)

(7)

(8)

(9)

inf
a∈P oly(L)

sup
λ∈[ n

k ,n] (cid:26)

e−λ(1−e

−η )

1
k

l=0
X

L

l ηll!M (l)
a2

N ∗(0) +

e−λ(1−e

−η )

L

alηlM (l)

N ∗(0)

2

.

(10)

(cid:18)

l=0
X

(cid:19)

(cid:27)

For the case that the Poisson repeat channel is not present, one only needs to adjust the exponential
weights and substitute ηlM (l)

N ∗ (0) with λl. The resulting optimization problem reads as

inf
a∈P oly(L)

sup
λ∈[ n

k ,n] (cid:26)

e−λ
k

(cid:18)

L

L

l λll!
a2

+

e−λ

alλl

l=0
X

(cid:19)

(cid:18)

l=0
X

2

.
(cid:27)

(cid:19)

(11)

Note that both (10) and (11) are of the form of a regularized weighted Chebychev approximation
problem. For simplicity, we ﬁrst focus on the noiseless case (11), as similar but more tedious
arguments may be used for the noisy case (10).

If we ignore the ﬁrst term in (11), the optimization problem reads as

inf
a∈P oly(L)

sup
λ∈[ n

k ,n] (cid:18)

L

2

e−λ

alλl

l=0
X

(cid:19)

inf
a∈P oly(L)

⇔

sup
λ∈[ n

L

e−λ

alλl

.

(12)

k ,n] (cid:12)
(cid:12)
(cid:12)
(cid:12)

l=0
X

(cid:12)
(cid:12)
(cid:12)
(cid:12)

L
The term e−λ
l=0 alλl corresponds to the bias of the estimator. It is straightforward to see that the
optimal choice of a for the two problems are the same. Problem (12) is an exponentially weighted
Chebyshev approximation problem [42]. Note that one can further upper bound (12) as follows

P

inf
a∈P oly(L)

sup
λ∈[ n

L

e−λ

alλl

l=0
X

(cid:12)
(cid:12)
(cid:12)
(cid:12)

k ,n] (cid:12)
(cid:12)
(cid:12)
(cid:12)

e− n

k

≤

inf
a∈P oly(L)

sup
λ∈[ n

L

alλl

,

(13)

l=0
X

k ,n] (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

resulting in a standard Chebyshev approximation problem with a solution of the form of scaled and
shifted Chebyshev polynomials. Despite the fact that the authors of [20] obtained the coefﬁcients
of the Chebyshev estimator using a different interval in the supremum that account for the variance,
(13) along with our extensive simulation results show that ignoring the exponential weights results
in a worse bound on the risk and practical performance.

The ﬁrst term 1
k

L
l=0 e−λa2

l λll!

, which corresponds to the variance, may be rewritten as

L

(cid:16)P
l λll!
e−λa2

(cid:17)

= aT M(λ)a ,

1
k

2
M(λ), M(λ) ,

a

||

||

e−λ
k

Diag(λ00!, λ11!, ..., λLL!).

||

||

(cid:19)

l=0
X
.

(cid:18)
Clearly,
M(λ) is a valid norm, and consequently, the ﬁrst term in (11) may be viewed as a regu-
larizer. For the case including the Poisson repeat channel, since the sum of Touchard polynomials is
still a polynomial, the bias term in (10) also represents an exponentially weighted Chebyshev approx-
imation problem. The variance term may be written as the weighted norm of a with a weight matrix
N ∗ (0), 1!η1M (1)
1
k e−λ(1−e
N ∗ (0)). The resulting problem (10) is
once again an regularized weighted Chebyshev approximation problem.

N ∗(0), ..., L!ηLM (L)

−η) Diag(0!η0M (0)

5

Solving problems (10) and (11).
Solving problem (10) and (11) directly appears to be difﬁcult, so
we instead resort to numerically solving the epigraph formulation of the semi-inﬁnite programs (10)
and (11) and proving that the numerical solution is asymptotically consistent.

Once again, we start with the case that excludes the Poisson repeat channel (11). The epigraph
formulation of (11) is a semi-inﬁnite program of the form ([43], Chapter 6.1)

min
t,a∈P oly(L)

t

s.t.

1
k

L

L

2

e−λa2

l λll!

+

e−λ

alλl

t,

λ

∀

∈

≤

[

n
k

, n].

(14)

(cid:18)

(cid:26)

l=0
X

(cid:19)
There are many algorithms that can be used to numerically solve (14): the discretization and central
cutting plane method, the KKT and SQP reductions [44, 45]. For simplicity, we focus on the
discretization method. For this purpose, we ﬁrst form a grid of the interval [ n
k , n] involving s points,
denoted by Grid([ n
k , n], s). Problem (14) represents an LP with inﬁnitely many quadratic constraints,
which is not solvable. Hence, instead of solving (14), we focus on the relaxed problem

l=0
X

(cid:27)

(cid:18)

(cid:19)

min
t,a∈P oly(L)

t

s.t.

1
k

(cid:26)

(cid:18)

L

L

2

e−λa2

l λll!

+

e−λ

alλl

l=0
X

(cid:19)

(cid:18)

l=0
X

(cid:19)

(cid:27)

t,

λ

∀

∈

≤

Grid([

n
k

, n], s).

(15)

As will be discussed in greater detail later, the solution of the relaxed problem is asymptotically
, the optimal values of the
consistent with the solution of the original problem (i.e., as s
objectives of the original and relaxed problem are the same). Problem (15) is an LP with a ﬁnite
number of quadratic constraints that may be solved using standard optimization tools. Unfortunately,
the number of constraints scales with the length of the grid interval, which in the case of interest
is linear in n. This appears as an undesirable feature of the approach, but can be easily mitigated
through the following theorem which demonstrates that an optimal solution of the problem may be
found over an interval of length proportional to the signiﬁcantly smaller value of log k (k/ log k . n
is needed for accurate estimation [20]). We relegate the proof of this result to the Supplement.
Theorem 3.2. For any a

P oly(L), L =

and c0 = 0.558, let

c0 log k

→ ∞

∈

⌊

⌋

g(a, λ) =

Then, we have

1
k

L

(cid:18)

l=0
X

e−λa2

l λll!

+

e−λ

alλl

(cid:19)

(cid:18)

l=0
X

L

2

.

(cid:19)

g(a, λ) =

sup
λ∈[ n

k ,n]

(

k ,6.5L] g(a, λ)

supλ∈[ n
g(a, n
k )

if n
if n

6.5L
k ≤
k > 6.5L.

Remark 3.1. In weighted approximation theory [31], the problem of bounding the interval over
which the supremum is achieved is a topic of signiﬁcant interest, with many important results readily
available. For example, if we ignore the regularization term, we can directly use the Mhaskar-
Saff theorem [32, 33] (Theorem 7.1 in the Supplement) to reduce the length of the interval in the
supremum to π
2 L. Our Theorem 3.2 shows that even when a regularization term is present, we
can still restrict the length of the interval to 6.5L. Our proof differs from that of the more general
Mhaskar-Saff theorem, since we exploit the speciﬁc structure of the problem. It remains an open
problem to extend the approach of [31] used in our proof to account for more general weights.

Using the previous derivations, we arrive at the following optimization problem

min
t,a∈P oly(L)

t

s.t.

1
k

(cid:26)

L

(cid:18)

l=0
X

e−λa2

l λll!

+

e−λ

alλl

L

2

(cid:19)

(cid:18)

l=0
X

(cid:19)

(cid:27)

t,

λ

∀

∈

≤

Grid([

n
k

, 6.5L], s).

(16)

Since L =
tion interval in (16) is proportional to log k and thus the (16) can be solved efﬁciently.

for the case excluding the Poisson repeat channel, the length of the optimiza-

c0 log k

⌊

⌋

For the case including the Poisson repeat channel, using the same arguments as above, we have

min
t,a∈P oly(L)

t

s.t.

λ

∀

∈

Grid([

n
k

e−λ(1−e

−η)

1
k

(cid:26)

L

l=0
X

l ηll!M (l)
a2

N ∗(0) +

e−λ(1−e

−η )

(cid:18)

L

l=0
X

2

alηlM (l)

N ∗(0)

(cid:19)

(cid:27)

t

≤

(17)

, CL], s),

6

where C > 0 is a constant. Unfortunately, it is very hard to precisely characterize C. Nevertheless,
we ﬁnd that in practice, the choice C = 2 works well with L =
. Furthermore, since the
Poisson repeat channel introduces an average of η replicas of each samples, the “cut-off” value L
for gL in (2) is set to be η times larger than the corresponding value without Poisson repeats.

ηc0 log k

⌋

⌊

Convergence of the discretized method For the case of objective functions and constraints that
are “well-behaved” (see [46] and [47]), as s grows, the solution of the relaxed semi-inﬁnite program
approaches the optimal solution of the original problem. We use the above results in conjunction
with a number of properties of our objective SIP to establish the claim in the following theorem
whose proof is delegated to the Supplement.
Theorem 3.3. Let s be the number of uniformly placed grid points on the interval (16) or (17),
and let d , 6.5L− n
0, the optimal objective
value td of the discretized SIP (16) or (17) (with η > 0) converges to the optimal objective value
of the original SIP t⋆. Moreover, the optimal solution is unique. The convergence rate of td to t⋆
equals O(d2). If the optimal solution of the SIP is a strict minimum of order one (i.e., if t
C
the discretized SIP also converges to an optimal solution with rate O(d2).

≥
for some constant C > 0 and for all feasible neighborhoods of a⋆), then the solution of

be the length of the discretization interval. As d

a⋆

s−1

→

t⋆

−

−

a

||

||

k

4 Experiments

Next, we compare our estimator, referred to as the Regularized Weighted Chebyshev (RWC) method,
with the Good-Turing (GT) estimator, the WY estimator of [20], the PJW estimator described in [23]
and the HOSW estimator of [25]. We do not compare our method with the estimators introduced
in [19, 24] due to their high computational complexity [20, 25].

log(i + 1)

k , the Zipf distributions with pi
∝

Synthetic data experiments. We ﬁrst evaluate the maximum risk under normalized squared loss
of all listed estimators over six different distributions without Poisson repeats: the uniform distribu-
tion with pi = 1
i−α, and α equal to 1.5, 1, 0.5 or 0.25, and the
∝
Benford distribution with pi
log(i). We choose the support sizes for the Zipf and
Benford distribution so that the minimum nonzero probability mass is roughly 10−6. We run the
estimator 100 times to calculate the risk. For solving (16), we use a grid with s = 1000 points in the
interval [ n
. The GT method used for comparison ﬁrst estimates the
total probability of seen symbols (e.g., sample coverage) according to ˆC = 1
h1/n, and then esti-
mates the support size according to ˆSGT = ˆSc/ ˆC; here, ˆSc stands for the simple counting estimator.
Note that h1 equals the number of different alphabet symbols observed only once in the n samples.
Detailed ﬁndings are presented in the Supplement.

k , 6.5L], and L =

0.558 log k

−

−

⌊

⌋

Figures 1(a) and 1(b) indicate that the RWC estimator has a signiﬁcantly better worst case perfor-
mance compared to all other methods when tested on the above distributions, provided that n
0.2k.
Also, both the RWC and WY estimators have signiﬁcantly better error exponents compared to GT,
PJW and HOSW. Interestingly, we ﬁnd the the worst case risk with normalization (1/k)2 tends to
severely bias the results towards a near-uniform distribution. We mitigate this issue by changing the
normalization from (1/k)2 to (1/S)2, which was also done in [25]. We repeat the experiment using
the normalization (1/S)2, corresponding to what we refer to as the RWC-S estimator. A detailed
description of this algorithm and its analysis is available in the Supplement. Figures 1(c) and 1(d)
illustrate that the RWC-S estimator signiﬁcantly outperforms all other estimators.

≥

Next, we turn our attention to the case of the Poisson repeat channel (PRC). Our RWC-S-prc esti-
mator requires solving (17) with 1/k replaced by 1/ ˆSc, and setting C = 2, L =
⌋
and s = 1000. Since the noisy support estimation problem is new there is no standard benchmark
to compare it with. This is why we consider the performance of RWC-S-prc, the naive counting
estimator and two simple modiﬁcations of the WY estimator, since those offer the best performance
in the noiseless setting. The WY-naive method ﬁrst divides the empirical counts N ′ by η and then
applies the WY estimator. This is intuitive since each symbol is repeated η times in expectation. The
WY-prc method involves modifying the coefﬁcients in the WY estimator with Touchard polynomial
multipliers. Note that this modiﬁcation does not take into account the exponential weighting and
regularization term that we introduced for both the noiseless and noisy setting. In the experiments,
since the replication rate is small in practice. As we can see from Fig-
we choose η from

0.558η log k

0.5, 1, 1.5

⌊

{

}

7

ures 1(e), 1(f) and 1(g), our method signiﬁcantly outperforms all other methods. Notably, WY-prc
performs poorly for η > 1 while WY-Naive performs poorly for η < 1.

Worst case over the selected set of distributions
100

Worst case over the selected set of distributions
10

Worst case over the selected set of distributions
100

Worst case over the selected set of distributions
10

RWC
GT
WY
PJW
HOSW

2

k

y
b

d
e
z

i
l

a
m
r
o
n
E
S
M

10-5

0

2

4

6

8

n

10
105

2

k

y
b

d
e
z

i
l

a
m
r
o
n
E
S
M

C
W
R

f
o

i

n
a
G

8

6

4

2

0

0

RWC
GT
WY
PJW
HOSW

2
S
y
b

d
e
z

i
l

a
m
r
o
n
E
S
M

10-1

10-2

10-3

RWC-S
GT
WY
PJW
HOSW

2
S
y
b

d
e
z

i
l

a
m
r
o
n
E
S
M

-

S
C
W
R

f
o

i

n
a
G

8

6

4

2

RWC-S
GT
WY
PJW
HOSW

2

4

6

8

n

10-4

0

10
105

2

4

6

8

n

0

0

10
105

2

4

6

8

n

10
105

(a) Worst case MSE/k2.

(b) Gain of RWC.

(c) Worst case MSE/S2.

(d) Gain of RWC-S.

Support estimation results for Hamlet

RWC
RWC-S
WY
GT
PJW
HOSW

Worst case, PRC 

 = 0.5

100

Worst case, PRC 

 = 1

RWC-S-prc
Naive
WY
WY-naive
WY-prc

2
S
y
b

d
e
z

i
l

a
m
r
o
n
E
S
M

RWC-S-prc
Naive
WY
WY-naive
WY-prc

1010

105

100

2
S
y
b

d
e
z

i
l

a
m
r
o
n
E
S
M

Worst case, PRC 

 = 1.5

RWC-S-prc
Naive
WY
WY-naive
WY-prc

100

2
S
y
b
d
e
z

i
l

a
m
r
o
n
E
S
M

10-1

10-2

10-3

102

100

10-2

2
S
y
b

d
e
z

i
l

a
m
r
o
n
E
S
M

10-4

0

0.5

1

1.5

2

n

10-5

0

2.5

106

(e) PRC η = 0.5

0.5

1

1.5

2

n

(f) PRC η = 1

10-5

0

2.5

106

0.5

1

1.5

2

n

10-4

0

2.5

106

0.5

1

1.5
n (samples)

2

2.5

3
104

(g) PRC η = 1.5

(h) Hamlet MSE/S2

Figure 1: The y-axes of (a), (c), (e), (f), and (g) are in log scale. Figures (a)-(d) show demon-
strate that our methods outperform state-of-the-art techniques in the noiseless setting. Figures (e)-(f)
demonstrate that our method outperforms all other methods in the presence of Poisson repeats. Fig-
ure (h) illustrates the superiority of our methods on noiseless real-world data.

Real-world data experiments. We start by estimating the number of distinct words in selected
books, as suggested in [20, 19]. We use Hamlet, Othello, Macbeth and King Lear for our compar-
ative study, with the results presented in Figure 1(h) (for Hamlet) and the Supplement (all other
plays). In the experiments, we randomly sampled words from the text with replacement and used
the obtained counts to estimate the number of distinct words. For simplicity, we set k to be equal
the total number of words. For example, as the number of words in Hamlet equals 30, 364, we set
k = 30, 364. As may be clearly seen, our methods signiﬁcantly outperform all other competitive
techniques both in terms of convergence rate and the accuracy of the estimated support.

To estimate the mutational support of the SARS-Cov-2 virus in the presence of sampling artifacts,
we ﬁrst create the histogram of mutations in sequenced genomes, using a reference corresponding to
Patient 1 (the ﬁrst infected individual that was sequenced). The mutational support of a population
of individuals equals the size of the union of the individual supports. The datasets used in the study
were retrieved from the GISAID repository [39, 40] on 04-14-2020, and they pertain to European
patients only. The analysis of datasets acquired from Asia and North America is relegated to the
Supplement. We conduct three experiments: ﬁrst, we examine the results of the noiseless support
estimation methods (Table 3); next, we manually corrupt the samples by Poisson repeats with η =
0.5, 1, 1.5 (Table 1 and the Supplement for η = 1, 1.5). A good noisy support estimation method
should produce a support close to that of its noiseless counterpart, and in this setting our method
shows superior performance compared to other techniques. Finally, we also report the results of
noisy support estimation on the unperturbed data (Table 2). Note that the naive estimator gives a
lower bound for the true support size, and the results of WY-Naive for η = 0.5 are erroneous. On the
other hand, WY-prc produces estimates that violate the known maximum support size or negative
entries for η = 1.5, as reported in the Supplement. A more detailed discussion of the relevant
biological ﬁndings is also available in the Supplement.

References

[1] R. A. Fisher, A. S. Corbet, and C. B. Williams, “The relation between the number of species
and the number of individuals in a random sample of an animal population,” The Journal of
Animal Ecology, pp. 42–58, 1943.

[2] B. Efron and R. Thisted, “Estimating the number of unseen species: How many words did

shakespeare know?” Biometrika, vol. 63, no. 3, pp. 435–447, 1976.

8

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Table 1: Support estimation with synthetic Pois-
son repeats. We report the relative difference
(in %) of the results for the noisy and noiseless
counterparts (the closer the value to 0, the better
the performance).

Genomic region

η = 0.5

Table 2: Results of the noisy support estimation
method directly applied to the real-world data.
We underline the results that are obviously false
(i.e., those violating the maximum support con-
straint, taking negative values or values smaller
than the results of the naive estimator).

RWC-S-prc Naïve WY-Naïve WY-prc

Genomic region

η = 0.5

Maximum support

ORF1a
ORF1b
S
ORF3a
ORF6
ORF7a
ORF8
N
ORF10
All

-30.5
-28.4
-30.8
-32.5
-31.1
-33.6
-22.4
-24
7.7
-29.6

-50.6
-50.9
-51.6
-49.5
-55.8
-54.5
-46.9
-45.3
-40
-50.5

-116.5
-120
-122.9
-96.1
-93.3
-90.1
-83
-121.9
-61.9
-114.7

-64.7
-63.6
-64.7
-70.6
-68.6
-68.7
-58.5
-56.6
14.3
-64

ORF1a
ORF1b
S
ORF3a
ORF6
ORF7a
ORF8
N
ORF10
All

RWC-S-prc Naïve WY-Naïve WY-prc

2346
1106
553
210
106
139
56
267
46
4955

911
477
246
99
52
66
32
139
30
2118

465
230
104
70
42
55
28
74
28
1156

1262
665
353
112
64
80
37
188
32
2867

13203
8087
3822
828
186
366
366
1260
117
29132

[3] S. Raskhodnikova, D. Ron, A. Shpilka, and A. Smith, “Strong lower bounds for approximating
distribution support size and the distinct elements problem,” SIAM Journal on Computing,
vol. 39, no. 3, pp. 813–842, 2009.

[4] Z. Bar-Yossef, T. Jayram, R. Kumar, D. Sivakumar, and L. Trevisan, “Counting distinct el-
ements in a data stream,” in International Workshop on Randomization and Approximation
Techniques in Computer Science. Springer, 2002, pp. 1–10.

[5] M. Charikar, S. Chaudhuri, R. Motwani, and V. Narasayya, “Towards estimation error guar-
antees for distinct values,” in Proceedings of the nineteenth ACM SIGMOD-SIGACT-SIGART
symposium on Principles of database systems. ACM, 2000, pp. 268–279.

[6] I. Chien, C. Pan, and O. Milenkovic, “Query k-means clustering and the double Dixie cup
problem,” in Advances in Neural Information Processing Systems, 2018, pp. 6650–6659.

[7] H. Ashtiani, S. Kushagra, and S. Ben-David, “Clustering with same-cluster queries,” in Ad-

vances in Neural Information Processing Systems, 2016, pp. 3216–3224.

[8] I. Chien, H. Zhou, and P. Li, “HS2 : Active learning over hypergraphs,” in International Con-

ference on Artiﬁcial Intelligence and Statistics, 2019.

[9] E. Chien, A. M. Tulino, and J. Llorca, “Active learning in the geometric block model,” in

Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 2020.

[10] I. Good and G. Toulmin, “The number of new species, and the increase in population coverage,

when a sample is increased,” Biometrika, vol. 43, no. 1-2, pp. 45–63, 1956.

[11] A. Orlitsky, N. P. Santhanam, and J. Zhang, “Always good turing: Asymptotically optimal

probability estimation,” Science, vol. 302, no. 5644, pp. 427–431, 2003.

[12] J. Acharya, H. Das, A. Jafarpour, A. Orlitsky, and S. Pan, “Competitive closeness testing,” in

Proceedings of the 24th Annual Conference on Learning Theory, 2011, pp. 47–68.

[13] L. Paninski, “Estimation of entropy and mutual information,” Neural computation, vol. 15,

no. 6, pp. 1191–1253, 2003.

[14] J. Bunge and M. Fitzpatrick, “Estimating the number of species: a review,” Journal of the

American Statistical Association, vol. 88, no. 421, pp. 364–373, 1993.

[15] Z. Bar-Yossef, R. Kumar, and D. Sivakumar, “Sampling algorithms: lower bounds and appli-
cations,” in Proceedings of the thirty-third annual ACM symposium on Theory of computing.
ACM, 2001, pp. 266–275.

[16] T. Batu, L. Fortnow, R. Rubinfeld, W. D. Smith, and P. White, “Testing that distributions are
close,” in Foundations of Computer Science, 2000. Proceedings. 41st Annual Symposium on.
IEEE, 2000, pp. 259–269.

[17] M. R. Nelson, D. Wegmann, M. G. Ehm, D. Kessner, P. S. Jean, C. Verzilli, J. Shen, Z. Tang,
S.-A. Bacanu, D. Fraser et al., “An abundance of rare functional variants in 202 drug target
genes sequenced in 14,002 people,” Science, p. 1217876, 2012.

9

[18] A. Keinan and A. G. Clark, “Recent explosive human population growth has resulted in an

excess of rare genetic variants,” Science, vol. 336, no. 6082, pp. 740–743, 2012.

[19] P. Valiant and G. Valiant, “Estimating the unseen: improved estimators for entropy and other
properties,” in Advances in Neural Information Processing Systems, 2013, pp. 2157–2165.

[20] Y. Wu and P. Yang, “Chebyshev polynomials, moment matching, and optimal estimation of the

unseen,” The Annals of Statistics, vol. 47, no. 2, pp. 857–883, 2019.

[21] J. Acharya, H. Das, A. Orlitsky, and A. T. Suresh, “A uniﬁed maximum likelihood approach
for estimating symmetric properties of discrete distributions,” in International Conference on
Machine Learning, 2017, pp. 11–21.

[22] Y. Wu and P. Yang, “Sample complexity of the distinct elements problem,” Mathematical Statis-

tics and Learning, vol. 1, no. 1, pp. 37–72, 2018.

[23] D. S. Pavlichin, J. Jiao, and T. Weissman, “Approximate proﬁle maximum likelihood,” arXiv

preprint arXiv:1712.07177, 2017.

[24] Y. Han, J. Jiao, and T. Weissman, “Local moment matching: A uniﬁed methodology for sym-
metric functional estimation and distribution estimation under wasserstein distance,” arXiv
preprint arXiv:1802.08405, 2018.

[25] H. Yi, A. Orlitsky, A. T. Suresh, and Y. Wu, “Data ampliﬁcation: A uniﬁed and competitive ap-
proach to property estimation,” in Advances in Neural Information Processing Systems, 2018,
pp. 8834–8843.

[26] T. Hoenen, D. Safronetz, A. Groseth, K. Wollenberg, O. Koita, B. Diarra, I. Fall, F. Haidara,
F. Diallo, M. Sanogo et al., “Mutation rate and genotype variation of ebola virus from mali
case sequences,” Science, vol. 348, no. 6230, pp. 117–119, 2015.

[27] R. M. Ribeiro, H. Li, S. Wang, M. B. Stoddard, G. H. Learn, B. T. Korber, T. Bhattacharya,
J. Guedj, E. H. Parrish, B. H. Hahn et al., “Quantifying the diversiﬁcation of hepatitis c virus
(hcv) during primary infection: estimates of the in vivo mutation rate,” PLoS pathogens, vol. 8,
no. 8, 2012.

[28] E. O.-O. Max Roser, Hannah Ritchie and J. Hasell, “Coronavirus pandemic (covid-19),” Our

World in Data, 2020, https://ourworldindata.org/coronavirus.

[29] M. Cheraghchi and J. Ribeiro, “Sharp analytical capacity upper bounds for sticky and related

channels,” IEEE Transactions on Information Theory, vol. 65, no. 11, pp. 6950–6974, 2019.

[30] F. Farnoud, O. Milenkovic, and N. P. Santhanam, “Small-sample distribution estimation over
IEEE, 2009,

sticky channels,” in 2009 IEEE International Symposium on Information Theory.
pp. 1125–1129.

[31] D. S. Lubinsky, “A survey of weighted polynomial approximation with exponential weights,”

Surveys in Approximation Theory, vol. 3, pp. 1–105, 2007.

[32] H. Mhaskar and E. Saff, “Extremal problems for polynomials with exponential weights,” Trans-

actions of the American Mathematical Society, vol. 285, no. 1, pp. 203–234, 1984.

[33] ——, “Where does the sup norm of a weighted polynomial live?” Constructive Approximation,

vol. 1, no. 1, pp. 71–91, 1985.

[34] J. Touchard et al., “Sur les cycles des substitutions,” Acta Mathematica, vol. 70, pp. 243–297,

1939.

[35] A. Orlitsky, N. P. Santhanam, K. Viswanathan, and J. Zhang, “On modeling proﬁles instead of
values,” in Proceedings of the 20th conference on Uncertainty in Artiﬁcial Intelligence. AUAI
Press, 2004, pp. 426–435.

[36] J. Aldrich et al., “R.A. Fisher and the making of maximum likelihood 1912-1922,” Statistical

science, vol. 12, no. 3, pp. 162–176, 1997.

[37] A. Orlitsky, A. T. Suresh, and Y. Wu, “Optimal prediction of the number of unseen species,”

Proceedings of the National Academy of Sciences, vol. 113, no. 47, pp. 13 283–13 288, 2016.

[38] Y. Han, J. Jiao, C.-Z. Lee, T. Weissman, Y. Wu, and T. Yu, “Entropy rate estimation for markov
chains with large state space,” in Advances in Neural Information Processing Systems, 2018,
pp. 9781–9792.

10

[39] S. Elbe and G. Buckland-Merrett, “Data, disease and diplomacy: Gisaid’s innovative contribu-

tion to global health,” Global Challenges, vol. 1, no. 1, pp. 33–46, 2017.

[40] Y. Shu and J. McCauley, “Gisaid: Global initiative on sharing all inﬂuenza data–from vision

to reality,” Eurosurveillance, vol. 22, no. 13, 2017.

[41] W. C. Waterhouse, “Do symmetric problems have symmetric solutions?” The American Math-

ematical Monthly, vol. 90, no. 6, pp. 378–387, 1983.

[42] J. C. Mason and D. C. Handscomb, Chebyshev polynomials. Chapman and Hall/CRC, 2002.
[43] S. Boyd and L. Vandenberghe, Convex optimization. Cambridge university press, 2004.
[44] M. López and G. Still, “Semi-inﬁnite programming,” European Journal of Operational Re-

search, vol. 180, no. 2, pp. 491–518, 2007.

[45] R. Reemtsen and S. Görner, “Numerical methods for semi-inﬁnite programming: a survey,” in

Semi-inﬁnite programming. Springer, 1998, pp. 195–275.

[46] R. Reemtsen, “Discretization methods for the solution of semi-inﬁnite programming problems,”

Journal of Optimization Theory and Applications, vol. 71, no. 1, pp. 85–103, 1991.

[47] G. Still, “Discretization in semi-inﬁnite programming: the rate of convergence,” Mathematical

programming, vol. 91, no. 1, pp. 53–69, 2001.

[48] E. A. Rakhmanov, “On asymptotic properties of polynomials orthogonal on the real axis,”

Sbornik: Mathematics, vol. 47, no. 1, pp. 155–193, 1984.

[49] L. Mousavizadeh and S. Ghasemi, “Genotype and phenotype of covid-19: Their roles in patho-

genesis,” Journal of Microbiology, Immunology and Infection, 2020.

[50] R. C. Edgar, “Muscle: a multiple sequence alignment method with reduced time and space

complexity,” BMC bioinformatics, vol. 5, no. 1, p. 113, 2004.

11

Supplement

5 Proof of Theorem 3.2

To prove the result, we need to show that
in g equals

λ

∀

≥

6.5L, ∂

∂λ g(a, λ) < 0. The derivative of the ﬁrst term

∂
∂λ

1
k

(cid:18)

L

l=0
X

e−λa2

l λll!

=

(cid:19)

1
k

(cid:18)

(

l
λ −

1)e−λa2

l λll!

.
(cid:19)

L

l=0
X

Clearly, the right hand side in the above expression is negative for all λ > L. The second term of
the derivative equals

∂
∂λ

L

2

e−λ

alλl

(cid:18)

l=0
X
L

(cid:19)

= 2

e−λ

alλl

(cid:18)

l=0
X
L

−

(cid:19)(cid:18)

= 2e−2λ

alλl

(cid:18)

l=0
X

(cid:19)(cid:18)

l=0
X

L

l=0
X

l
λ

alλl

(cid:19)

L

e−λ

alλl + e−λ

L

l=0
X

(

l
λ −

1)alλl

.
(cid:19)

To analyze the two terms of the derivative, we introduce the vectors y, z, 1 and the diagonal matrix
D according to

y = (a0λ0, a1λ1, ..., aLλL)T ,
L
λ −

0
λ −
1 = (1, 1, ..., 1)T ,

1
λ −

1), ..., (

z = ((

1), (

1))T ,

Consequently, we have

Dii = (

−

1 +

i

1

)

(i
1)!
λ(i−1) .
−

−
λ

∂
∂λ

Therefore,

∂
∂λ

1
k

L

e−λa2

l λll!

=

e−λ
k

yT Dy,

(cid:18)

l=0
X
L

e−λ

(cid:19)

2

alλl

= 2e−2λyT 1zT y = e−2λyT (1zT + z1T )y.

(cid:18)

l=0
X

(cid:19)

∂
∂λ

g(a, λ) = e−2λyT

eλ
k

(cid:18)

D + (1zT + z1T )

.y

(cid:19)

(cid:16)

To show that ∂

∂λ g(a, λ) < 0 for all polynomials of degree L whenever λ > CL, we show that the
eλ
k D + (1zT + z1T )
is negative-deﬁnite whenever λ > CL, for some constant C > 0. It
matrix
sufﬁces to show that the sum of the maximum eigenvalues of eλ
(cid:17)
k D and (1zT +z1T ) is negative, since
eλ
k D is a diagonal matrix. Thus, we turn our attention to determining the maximum eigenvalues of
these two matrices. For eλ
eλ
k

eλ
i!
λi ,
2k
2 . When λ > L, it is clear that i!
λi is decreasing in i, for

k D, the maximum eigenvalue satisﬁes

max
i∈{0,1,...,L}
1 + i
λ )

i!
λi ≤ −

min
i∈{0,1,...,L}

1 +

i
λ

−

(cid:18)

(cid:19)

1

since for λ > 2L, one has (
i

0, 1, ..., L

, so that

−

∈ {

}

≤ −

min
i∈{0,1,...,L}

i!
λi =

L!
λL ≥

L
eλ

(cid:18)

L

.

(cid:19)

12

The last inequality is a consequence of Stirling’s formula, which asserts that n!
the above expressions, we obtain

≥

( n
e )n. Combining

eλ
k

max
i∈{0,1,...,L}

1 +

−

(cid:18)

i
λ

(cid:19)

i!
λi ≤ −

eλ
2k

L

.

L
eλ

(cid:19)

(cid:18)

Next, we derive an upper bound on maximum eigenvalue of the second matrix. The i, j entry of the
matrix (1zT + z1T ) equals i+j−2
2, and all these values are negative when λ > L. Moreover, it
is clear that the matrix of interest has rank equal to 2. Therefore, the matrix has exactly two nonzero
eigenvalues.

λ −

(1zT + z1T ). All entries of A are positive whenever λ > L. By Gershgorin’s theorem,
Let A =
we can upper bound the maximum eigenvalues of the matrix A by its maximum row sum. It is
obvious that the maximum row sum equals

−

Moreover, the trace of A equals

2(L + 1)

L(L + 1)
2λ

.

−

2(L + 1)

L(L + 1)
λ

.

−

This implies that the minimum eigenvalue of A is lower bounded by
plies that the maximum eigenvalue of (1zT + z1T ) is upper bounded by L(L+1)
Summing up the two previously derived upper bounds gives

−

2λ

L(L+1)
2λ

, which directly im-
.

h(λ) ,

eλ
2k

−

L
eλ

(cid:18)

L

(cid:19)

+

L(L + 1)
2λ

,

whenever λ > 2L. Note that h(λ) < 0 is equivalent to

L

<

L(L + 1)
2λ
(cid:18)
log(L) + log(L + 1) + log(k)

L
eλ

(cid:19)

eλ
2k

⇔
The function λ + log(λ)

L log(L) + L < λ + log(λ)

L log(λ).

(18)

−

−

L log(λ) is nondecreasing in λ whenever λ > L since

−

d
dλ

(λ + log(λ)

L log(λ)) = 1

−
λ
L+1
c0 . Using log(x + 1)
λ > CL where C > 2, the sufﬁcient condition for (18) to hold is

, we also have log(k)

c0 log(k)
⌋

−

−

≤

⌊

.

≤

x, which

L

1

L + 1

c0 −

L log(L) + L < CL + log(CL)

L log(CL).

−

By the deﬁnition of L =
holds

1. Hence

x

∀

≥

∀

log(L) + L +

Rearranging terms leads to

C

(cid:18)

−

log(C)

2

−

−

1
c0 (cid:19)

L + log(C) >

1
c0

.

Sufﬁcient conditions that ensure that the above inequality holds are log(C)

1
c0 ) > 0. The ﬁrst condition implies C

2
C = 6.5, for which the ﬁrst condition is also satisﬁed. This completes the proof.

≥

−

e

1

−
c0 = 6.0021, while the second condition holds with

−

≥

1
c0 and (C

log(C)

6 Proof of Theorem 3.3

The proof consists of two parts. In the ﬁrst part, we establish the conditions for convergence, while
in the second part, we determine the convergence rate. For simplicity, we present the proofs for the
case without Poisson repeats. We then outline how the analysis can be modiﬁed to account for the
repeats.

13

6.1 Proof of convergence

We start by introducing the relevant terminology. Let Π
let f be a continuous functional on Π. Assume that B
a continuous mapping from Π into
equipped with the supremum norm

C

RL+1 be a closed set of parameters, and
R is compact and that g : Π
(B) is
(B) is the space of continuous functions over B

⊂
⊂

7→ C

(B), where
C
|| · ||∞. For each D
g(c, x)
Π
|
∈

c

{

B let

0, x

D

}

∈

⊂

≤

M (D) =

denote the set of feasible points of the optimization problem

Assuming that M (D)

=

, let

∅

and deﬁne the level set

min f (c) over c

M (D).

∈

µ(D) = inf

c

f (c)
|

{

M (D)
}

,

∈

Level(c0, D) =

c

∈
We also make the following two assumptions:
Assumption 6.1 (Fine grid). Let N0 = N
of B with Bi
Bi+1, i

{

⊂

∈

f (c)

Π
|

≤

f (c0)

} ∩

M (D).

. There exists a sequence
{
N0, for which limi→∞ h(Bi, B) = 0, such that

∪ {

}

0

of compact subsets

Bi

}

h(Bi, B) = sup
x∈B

inf
y∈Bi ||

x

y

.

||

−

Assumption 6.2 (Bounded level set). M (B) is nonempty, and there exists a c0 ∈
the level set Level(c0, B0) is bounded and hence compact in RL+1.
Theorem 6.3 (Convergence of the discretized method, Theorem 2.1 in [46]). Under assumptions 6.1
and 6.2, the solution of the discretized problem converges to the optimal solution. More formally,
we have

M (B) such that

µ(Bi)
lim
i→∞
If c∗ is the unique optimal solution of the original problem, and c∗
discretized relaxation with grid Bi, then

µ(Bi+1)
≤
µ(Bi) = µ(B).

µ(B),

t
∀

≤

∈

N0

i is the optimal solution of the

c∗

lim
i→∞ ||

c∗
i ||2 = 0.

−

It is straightforward to see that our chosen grid is arbitrary ﬁne. Hence, we only need to prove that
there exists a c0 such that the level set Level(c0, D) is bounded.
Let c = (a; t) and note that in our setting, f (c) = t. Rewrite g(c, λ) in matrix form as

where

g(c, λ) = aT M(λ)a + aT ΛΛT a

t,

−

Λ , e−λ(λ0, λ1, ..., λL)T .

1. Obviously, ΛΛT is posi-
Note that only a1, ...aL are allowed to vary since we ﬁxed a0 =
tive semi-deﬁnite and the previously introduced M(λ) is positive deﬁnite for all λ > 0. Since the
constraints on g in (16) are positive deﬁnite with respect to a1, ...aL, g is coercive in a1, ...aL. Fur-
thermore, for any given t, the set of feasible coefﬁcients a1, ...aL is bounded. Therefore, given a t0,
the level set Level(c0, B0) is bounded. This ensures that Assumption 6.2 holds for our optimization
problem.
Next, we prove the uniqueness of the optimal solution c⋆. Note that proving this result is equivalent
to proving the uniqueness of a⋆. Hence, we once again refer to the original minmax formulation of
our problem,

−

inf
a:a0=−1

sup
k ,6.5L]

λ∈[ n

aT (M(λ) + ΛΛT )a , inf

a:a0=−1

sup
k ,6.5L]

λ∈[ n

hλ(a)

(19)

14

6
Clearly,
[ n
k , 6.5L]. Taking the supremum over λ preserves strict convexity since

[ n
k , 6.5L], the function hλ(a) is strictly convex since (M(λ) + ΛΛT )

0,
(0, 1), one has

≻

∈

λ

∀

θ
∀

∈

λ

∀

∈

hλ(θx + (1

θ)y)

−

θhλ(x) + (1

θ)hλ(y)

−

λ∈[ n

sup
k ,6.5L]
< sup
λ∈[ n

k ,6.5L]
sup
k ,6.5L]

≤

sup
k ,6.5L]
k ,6.5L] hλ(a) is strictly convex, which consequently implies the uniqueness of a⋆ and

θhλ(x) +

θ)hλ′ (y).

λ′∈[ n

λ∈[ n

(1

−

Hence supλ∈[ n
hence c⋆.

For the case of samples passed through a Poisson channel, it is not hard to see that the constraints
are again strictly convex in a, where one only need to replace M(λ), Λ by

e−λ(1−e

1
k
e−λ(1−e

−η) Diag(0!η0M (0)

N ∗(0), 1!η1M (1)

N ∗ (0), ..., L!ηLM (L)

N ∗ (0))

−η )(η0M (0)

N ∗ (0), η1M (1)

N ∗(0), ..., ηLM (L)

N ∗ (0))T

respectively. Thus, a similar analysis is possible and the details are omitted. The proof above along
with the previous observation prove the convergence result of Theorem 3.3.

6.2 Proof for the convergence rate

In what follows, and for reasons of simplicity, we omit the constraint a0 =
−
The described proof only requires small modiﬁcations to accommodate a0 =
Recall that we used Bd to denote the grid with grid spacing d. In order to use the results in [47], we
require the convergence assumption below.
Assumption 6.4. Let ¯c be a local minimizer of an SIP. There exists a local solution cd of the
discretized SIP with grid Bd such that

1 in the SIP formulation.

1.

−

This assumption is satisﬁed for the SIP of interest as shown in the ﬁrst part of the proof.
Assumption 6.5. The following hold true:

cd

||

−

¯c

|| →

0.

There is a neighborhood ¯U of ¯c such that the function ∂2

∂λ2 g(c, λ) is continuous on ¯U

B.

×

The set B is compact, nonempty and explicitly given as the solution set of a set of inequal-
ities, B =

, where I is a ﬁnite index set and vi

C2(B).

vi(λ)

R

λ

•

•

For any ¯λ

{

|

∈
B, the vectors ∂

≤

•

∈
Recall that our objective is of the form

0, i
I
∈
}
∂λ vi(¯λ), i

i

I

|

∈

∈ {

vi(¯λ) = 0

}

are linearly independent.

∈

where

g(c, λ) = aT M(λ)a + aT ΛΛT a

t,

−

Λ , e−λ(λ0, λ1, ..., λL)T , c = (a; t),

M(λ) ,

e−λ
k

Diag(λ00!, λ11!, ..., λLL!).

It is straightforward to see that the ﬁrst condition in Assumption 6.5 holds. For the second condition,
recall that B = [ n
,
}
n
6.5L). Since we only have one variable v1, it is also easy to see that the third
v1(λ) = (λ
k )(λ
condition is met.
Assumption 6.6. The set B satisﬁes Assumption 6.5 and all the sets Bd contain the boundary points
n
k , 6.5L.

k , 6.5L]. Hence, the second condition can be satisﬁed by choosing I =
−

−

{

1

15

This assumption also clearly holds for the grid of choice. Note that it is crucial to include the
boundary points for the proof in [47] to be applicable.
Assumption 6.7.
there exists a vector ξ such that

B, where ¯U is a neighborhood of ¯c. Moreover,

cg(c, λ) is continuous on ¯U

∇

×

cg(¯c, λ)T ξ

∇

1,

λ

∀

∈

B.

≤ −

Note that

cg(c, λ) = [

∇

∇

ag(c, λ);

tg(c, λ)] and

∇
ag(c, λ) = 2(M(λ) + ΛΛT )a.

∇

λ

Also note that
of the same direction as [
inequality

∈

∀

−

B, M(λ) + ΛΛT is positive deﬁnite. Hence choosing ξ to be colinear with and
aT 1]T , as well as of sufﬁciently large norm will allow us to satisfy the

cg(¯c, λ)T ξ

1,

λ

B.

∇

≤ −
Hence, Assumption 6.7 holds as well. The next results follow from the above assumptions and
observations, and the results in [47].
Lemma 6.8 (Corollary 1 in [47]). Let td be the optimal objective value of the discretized SIP used
for support estimation with the grid Bd, and let t⋆ be the optimal objective value for the original SIP.
Since Assumptions 6.4,6.5,6.6,6.7 hold, then for some c3 > 0 and d sufﬁciently small, we have

∈

∀

t⋆

0

≤

−

td

≤

c3d2.

Consequently, td
Lemma 6.9 (Theorem 2 in [47]). Assume that all assumptions in Lemma 6.8 hold. If there exists a
constant c4 > 0 such that

t⋆ with a convergence rate of O(d2).

→

then for sufﬁciently small d and σ > 0 we have

t

¯t

−

≥

c

c4||

−

¯c

,

||

c

∀

∈

M (B)

¯U ,

∩

cd

||

¯c
|| ≤

−

σd2.

This result implies that if ¯c is also a strict minimum of order one, then the solution of the discretized
SIP converges to that of the the original SIP with rate O(d2). For the Poisson repeat channel, the
constraints are also strictly convex in a. Therefore, a similar analysis is possible and the details are
omitted once again. Combining these results completes the proof.

7 Theoretical results supporting Remark 3.1

The result described in the main text follows from Theorem 6.2 in [31], originally proved in [32, 33]
and [48].
Theorem 7.1 (Theorem 6.2 from [31]). Let W (x) = exp(
Q : R
) is even, convex, diverging for x

Q(x)) be a weight function, where

−
, and such that

[0,

7→

∞

→ ∞
0 = Q(0) < Q(x),

x

= 0.

∀

L, not identical to zero, one has

Then, for any polynomial P (x) of degree

sup
x∈R |

P (x)W (x)
|

=

≤

sup
x∈[−ML,ML] |

P (x)W (x)
|

,

sup
x∈R\[−ML,ML] |

P (x)W (x)
|

<

sup
x∈[−ML,ML] |

P (x)W (x)
|

.

Here, ML stands for the Mhaskar-Rakhmanov-Saff (MSF) number, which is the smallest positive
root of the integral equation

L =

2
π

0
Z

1

MLtQ′(MLt)

√1

t2

−

dt.

(20)

In our setting, the weight equals exp(
Thus, we can restrict our optimization interval to [ n
optimal interval reduces to [ n

−
2 L + n
k ].

k , π

x). Solving (20) gives us an MSF number equal to ML = π

2 L.
k ]. If there is no regularization term, the

2 L + n

k , π

16

6
8 Construction of the RWC-S estimator

We introduce the optimization problem needed for minimizing the risk E
arguments once again establish that

2

S− ˆS
S

(cid:16)

(cid:17)

. Poissonization

S

E

2

=

ˆS
−
S !

1
S2

L

e−λi a2

l λl

il!

L

L

+

e−λi

alλl
i

e−λj

alλl
j

l=0
X
Taking the supremum over Dk, one can further upper bound the risk as

l=0
X

i6=j∈L (cid:18)
X

(cid:26) X

i∈L (cid:18)

(cid:19)

.
(cid:19)(cid:27)

(cid:19)(cid:18)

l=0
X

L

L

e−λia2

l λl

il!

+

e−λi

alλl
i

e−λj

alλl
j

(cid:19)
L

i6=j∈L (cid:18)
X
2

l=0
X

(cid:19)(cid:18)

l=0
X

(cid:19)(cid:27)

1
S2

L

L

l=0
X

i∈L (cid:18)

(cid:26) X
e−λa2

l λll!

sup
k ,n], ℓ∈L

λℓ∈[ n

≤

≤

sup
λ∈[ n

k ,n] (cid:26)

1
S

sup
λ∈[ n

≤

k ,n] (cid:26)

1
ˆSc (cid:18)

(cid:18)

l=0
X
L

+

e−λ

alλl

(cid:19)

(cid:18)

l=0
X
L

(cid:19)

(cid:27)
2

e−λa2

l λll!

+

e−λ

alλl

,
(cid:27)

l=0
l=0
X
X
where the last inequality is due to the fact that ˆSc
S. Note that the only difference between (21)
and (11) is in terms of changing 1/k to 1/ ˆSc in the ﬁrst term. In view of Theorem 3.2, (21) is
optimized by the solution of the following problem:

≤

(cid:19)

(cid:18)

(cid:19)

(21)

min
t,a∈P oly(L)

t

s.t.

1
ˆSc (cid:18)

(cid:26)

L

l=0
X

e−λa2

l λll!

+

e−λ

alλl

L

2

(cid:19)

(cid:18)

l=0
X

(cid:19)

(cid:27)

t,

λ

∀

∈

≤

Grid([

n
k

, 6.5L], s).

(22)

9 Detailed description of the SARS-Cov-2 genomic data

Organization of the SARS-Cov-2 genome. A breakdown of the genomic structure of SARS-Cov-
2 is shown in Figure 2, and described in detail in more detail in [49]. SARS-Cov-2 comprises the
following open reading frames: ORF1a and ORF1b, spike (S), membrane (M), envelope (E), and
nucleocapsid (N), as well as ORF 10. Since all these ORFs encode proteins that have different roles
in the process of evading the immune system of the host, we perform the mutational support analysis
in the presence of sampling artifacts for each individual region. Note that the overall support is the
sum of the mutational supports of all ORF regions.

Figure 2: Organization of the SARS-Cov-2 genome (Source: Wikipedia).

Data acquisition. We used data from the GISAID EpiCov repository [40] which contains se-
quenced viral strains collected from patients across the world. We downloaded the data aggregated
by the date of 04-14-2020. We ﬁltered out all incomplete genomic datasets, resulting in a total of

17

 
8, 893 samples. To obtain the mutation counts, we ﬁrst used the alignment software MUSCLE [50]
with respect to the reference of Patient 1, published under the name Wuhan-Hu-1 and collected at
the Central Hospital of Wuhan, December 2019 (GenBank accession number MN909847). For each
aligned pair of samples we generate a list containing the positions in the reference genome in which
the patient aligned to the reference has a mutation. The mutation proﬁle lists are aggregated pro-
ducing a mutation histogram for each of the viral genome regions depicted in Figure 2. To counter
alignment artifacts caused by sequencing errors, we removed all gaps encountered in the preﬁxes
and sufﬁxes and sufﬁciently long gaps (> 10 nts) within the alignments.

For the experiments, we partitioned the mutation histograms based on geographic location (Asia/
North America/ Europe). Since the number of samples across the three regions varies signiﬁcantly,
we subsampled the sequence sets to arrive at 636 samples from Asia and 1774 samples from Europe
and North America.

Interpretation of the results.
The mutational supports without Poisson repeats may be used
to estimate how quickly SARS-Cov-2 as well as any other virus mutates early on in the infection.
The Poisson repeat model, as previously mentioned, accounts for resampling of patients that may
have tested positive in a previous round or that may have been exposed to new sources of infection.
Erroneous samples are accounted for through deletions. The mean value of the Poisson repeats is
chosen to accommodate the fact that most individuals are sequenced once or not at all, but that
certain high-risk groups (such as health workers) may be subject to multiple tests.

Remark The estimators are based on the assumption that the symbol counts are independent and
identically distributed, which may clearly not be the case when analyzing viral mutations (i.e., mu-
tations at difference locations in the genomes may and are expected to be correlated). However, it
gives good performance even if the i.i.d. assumption is violated. See experiments on Shakespeare’s
plays where the i.i.d. assumption does not hold for example.

10 Additional experimental results

18

Uniform

100

Benford

RWC
GT
WY
PJW
HOSW

2

4

6

8

n

10
105

(a) Uniform distribution.

100

Zipf(1)

RWC
GT
WY
PJW
HOSW

2

4

6

8

n

10
105

(d) Zipf(1) distribution.

Uniform

RWC-S
GT
WY
PJW
HOSW

100

2

k

y
b

d
e
z

i
l

a
m
r
o
n
E
S
M

10-2

10-4

10-6

10-8

0

2

k

y
b

d
e
z

i
l

a
m
r
o
n
E
S
M

10-5

0

100

2
S
y
b

d
e
z

i
l

a
m
r
o
n
E
S
M

10-2

10-4

10-6

10-8

0

2

k

y
b

d
e
z

i
l

a
m
r
o
n
E
S
M

2

k

y
b

d
e
z

i
l

a
m
r
o
n
E
S
M

2
S
y
b

d
e
z

i
l

a
m
r
o
n
E
S
M

RWC
GT
WY
PJW
HOSW

2

4

6

8

n

10
105

(b) Benford distribution.

10-5

0

100

2

k

y
b

d
e
z

i
l

a
m
r
o
n
E
S
M

101

100

10-1

10-2

10-3

Zipf(1.5)

RWC
GT
WY
PJW
HOSW

0

4

2

6

10
105
(c) Zipf(1.5) distribution.

n

8

Zipf(0.5)

100

Zipf(0.25)

RWC
GT
WY
PJW
HOSW

2

k

y
b

d
e
z

i
l

a
m
r
o
n
E
S
M

RWC
GT
WY
PJW
HOSW

10-5

100

0

4

2

6

10
105
(e) Zipf(0.5) distribution.

n

8

Benford

RWC-S
GT
WY
PJW
HOSW

10-5

0

6

4

2

10
105
(f) Zipf(0.25) distribution.

n

8

100

2
S
y
b

d
e
z

i
l

a
m
r
o
n
E
S
M

10-1

10-2

10-3

10-4

0

Zipf(1.5)

RWC-S
GT
WY
PJW
HOSW

2

4

6

8

n

10
105

2

4

6

8

n

10
105

10-5

0

2

4

6

8

n

10
105

(g) Uniform distribution.

(h) Benford distribution.

(i) Zipf(1.5) distribution.

100

Zipf(1)

RWC-S
GT
WY
PJW
HOSW

2
S
y
b

d
e
z

i
l

a
m
r
o
n
E
S
M

10-5

0

2

4

6

8

n

10
105

(j) Zipf(1) distribution.

2
S
y
b

d
e
z

i
l

a
m
r
o
n
E
S
M

100

10-2

10-4

10-6

Zipf(0.5)

RWC-S
GT
WY
PJW
HOSW

100

10-2

10-4

2
S
y
b

d
e
z

i
l

a
m
r
o
n
E
S
M

Zipf(0.25)

RWC-S
GT
WY
PJW
HOSW

0

6

4

2

10
105
(k) Zipf(0.5) distribution.

n

8

10-6

0

6

2

4

10
105
(l) Zipf(0.25) distribution.

n

8

Figure 3: The ﬁgures plot the MSE of all estimators considered on each tested distributions without
Poisson repeats. The y-axis is on the log scale.

19

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
12

10

8

6

4

n
o
i
t
a
m

i
t
s
e
t
r
o
p
p
u
S

2

0

105

Uniform

104

10

Benford

RWC-S
GT
WY
PJW
HOSW
Ground Truth

2

4

6

8

n

10
105

n
o
i
t
a
m

i
t
s
e
t
r
o
p
p
u
S

8

6

4

2

0

0

RWC-S
GT
WY
PJW
HOSW
Ground Truth

2

4

6

8

n

10
105

(a) Uniform distribution.

(b) Benford distribution.

104

10

Zipf(1)

105

Zipf(0.5)

n
o
i
t
a
m

i
t
s
e
t
r
o
p
p
u
S

6

5

4

3

2

1

RWC-S
GT
WY
PJW
HOSW
Ground Truth

RWC-S
GT
WY
PJW
HOSW
Ground Truth

n
o
i
t
a
m

i
t
s
e
t
r
o
p
p
u
S

8

6

4

2

0

0

103

Zipf(1.5)

RWC-S
GT
WY
PJW
HOSW
Ground Truth

0

2

6

4

10
105
(c) Zipf(1.5) distribution.

n

8

105

Zipf(0.25)

RWC-S
GT
WY
PJW
HOSW
Ground Truth

n
o
i
t
a
m

i
t
s
e
t
r
o
p
p
u
S

n
o
i
t
a
m

i
t
s
e
t
r
o
p
p
u
S

7

6

5

4

3

2

1

8

7

6

5

4

3

2

2

4

6

8

n

10
105

(d) Zipf(1) distribution.

0

4

2

6

10
105
(e) Zipf(0.5) distribution.

n

8

0

4

2

6

10
105
(f) Zipf(0.25) distribution.

n

8

Figure 4: The ﬁgures plot the mean and variance of the estimators for the tested distributions without
Poisson repeats. The y-axis is on the log scale.

102

100

10-2

10-4

2
S
y
b

d
e
z

i
l

a
m
r
o
n
E
S
M

10-6

0

Uniform  =0.5

RWC-S-prc
Naive
WY
WY-naive
WY-prc

102

100

10-2

2
S
y
b

d
e
z

i
l

a
m
r
o
n
E
S
M

Benford  =0.5

RWC-S-prc
Naive
WY
WY-naive
WY-prc

102

100

10-2

2
S
y
b

d
e
z

i
l

a
m
r
o
n
E
S
M

Zipf(1.5)  =0.5

RWC-S-prc
Naive
WY
WY-naive
WY-prc

0.5

1

1.5

2

n

10-4

0

2.5

106

0.5

1

1.5

2

n

10-4

0

2.5

106

0.5

1

1.5

2

n

2.5

106

(a) Uniform distribution.

(b) Benford distribution.

(c) Zipf(1.5) distribution.

102

100

10-2

2
S
y
b

d
e
z

i
l

a
m
r
o
n
E
S
M

10-4

0

Zipf(1)  =0.5

RWC-S-prc
Naive
WY
WY-naive
WY-prc

102

100

10-2

10-4

2
S
y
b

d
e
z

i
l

a
m
r
o
n
E
S
M

Zipf(0.5)  =0.5

RWC-S-prc
Naive
WY
WY-naive
WY-prc

102

100

10-2

10-4

2
S
y
b

d
e
z

i
l

a
m
r
o
n
E
S
M

Zipf(0.25)  =0.5

RWC-S-prc
Naive
WY
WY-naive
WY-prc

0.5

1

1.5

2

n

10-6

0

2.5

106

0.5

1

1.5

2

n

10-6

0

2.5

106

0.5

1

1.5

2

n

2.5

106

(d) Zipf(1) distribution.

(e) Zipf(0.5) distribution.

(f) Zipf(0.25) distribution.

Figure 5: The ﬁgures plot the MSE of all estimators considered on each tested distributions with
Poisson repeats, η = 0.5. The y-axis is on the log scale.

20

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
100

Uniform  =1

100

Benford  =1

100

Zipf(1.5)  =1

RWC-S-prc
Naive
WY
WY-naive
WY-prc

2
S
y
b

d
e
z

i
l

a
m
r
o
n
E
S
M

RWC-S-prc
Naive
WY
WY-naive
WY-prc

2
S
y
b

d
e
z

i
l

a
m
r
o
n
E
S
M

RWC-S-prc
Naive
WY
WY-naive
WY-prc

10-5

0

0.5

1

1.5

2

n

10-5

0

2.5

106

0.5

1

1.5

2

n

10-5

0

2.5

106

0.5

1

1.5

2

n

2.5

106

(a) Uniform distribution.

(b) Benford distribution.

(c) Zipf(1.5) distribution.

100

Zipf(1)  =1

RWC-S-prc
Naive
WY
WY-naive
WY-prc

100

10-2

10-4

2
S
y
b

d
e
z

i
l

a
m
r
o
n
E
S
M

Zipf(0.5)  =1

RWC-S-prc
Naive
WY
WY-naive
WY-prc

100

10-2

10-4

2
S
y
b

d
e
z

i
l

a
m
r
o
n
E
S
M

Zipf(0.25)  =1

RWC-S-prc
Naive
WY
WY-naive
WY-prc

2
S
y
b

d
e
z

i
l

a
m
r
o
n
E
S
M

2
S
y
b

d
e
z

i
l

a
m
r
o
n
E
S
M

10-5

0

0.5

1

1.5

2

n

10-6

0

2.5

106

0.5

1

1.5

2

n

10-6

0

2.5

106

0.5

1

1.5

2

n

2.5

106

(d) Zipf(1) distribution.

(e) Zipf(0.5) distribution.

(f) Zipf(0.25) distribution.

Figure 6: The ﬁgures plot the MSE of all estimators considered on each tested distributions with
Poisson repeats, η = 1. The y-axis is on the log scale.

102

100

10-2

10-4

2
S
y
b

d
e
z

i
l

a
m
r
o
n
E
S
M

10-6

0

Uniform  =1.5

RWC-S-prc
Naive
WY
WY-naive
WY-prc

105

100

10-5

2
S
y
b

d
e
z

i
l

a
m
r
o
n
E
S
M

Benford  =1.5

RWC-S-prc
Naive
WY
WY-naive
WY-prc

1010

105

100

2
S
y
b

d
e
z

i
l

a
m
r
o
n
E
S
M

Zipf(1.5)  =1.5

RWC-S-prc
Naive
WY
WY-naive
WY-prc

0.5

1

1.5

2

n

10-10

0

2.5

106

0.5

1

1.5

2

n

10-5

0

2.5

106

0.5

1

1.5

2

n

2.5

106

(a) Uniform distribution.

(b) Benford distribution.

(c) Zipf(1.5) distribution.

105

100

10-5

2
S
y
b

d
e
z

i
l

a
m
r
o
n
E
S
M

10-10

0

Zipf(1)  =1.5

Zipf(0.5)  =1.5

Zipf(0.25)  =1.5

RWC-S-prc
Naive
WY
WY-naive
WY-prc

2
S
y
b

d
e
z

i
l

a
m
r
o
n
E
S
M

100

10-5

RWC-S-prc
Naive
WY
WY-naive
WY-prc

2
S
y
b

d
e
z

i
l

a
m
r
o
n
E
S
M

100

10-5

RWC-S-prc
Naive
WY
WY-naive
WY-prc

0.5

1

1.5

2

n

2.5

106

0

0.5

1

1.5

2

n

2.5

106

0

0.5

1

1.5

2

n

2.5

106

(d) Zipf(1) distribution.

(e) Zipf(0.5) distribution.

(f) Zipf(0.25) distribution.

Figure 7: The ﬁgures plot the MSE of all estimators considered on each tested distributions with
Poisson repeats, η = 1.5. The y-axis is on the log scale.

21

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
6000

5000

4000

3000

2000

1000

e
z
s

i

t
r
o
p
p
u
s

d
e
t
a
m

i
t
s
E

Support estimation results for Hamlet

5000

e
z
s

i

t
r
o
p
p
u
s

d
e
t
a
m

i
t
s
E

4000

3000

2000

1000

RWC
RWC-S
WY
GT
PJW
HOSW
Ground Truth

Support estimation results for Othello

RWC
RWC-S
WY
GT
PJW
HOSW
Ground Truth

0

0

0.5

1

1.5
n (samples)

2

2.5

3
104

0

0

0.5

1

1.5
n (samples)

2

2.5

3
104

(a)

(b)

100

2
S
y
b

d
e
z

i
l

a
m
r
o
n
E
S
M

10-1

10-2

10-3

10-4

0

Support estimation results for Hamlet

RWC
RWC-S
WY
GT
PJW
HOSW

0.5

1

1.5
n (samples)

2

2.5

3
104

(e)

100

2
S
y
b

d
e
z

i
l

a
m
r
o
n
E
S
M

10-1

10-2

10-3

10-4

0

Support estimation results for Othello

RWC
RWC-S
WY
GT
PJW
HOSW

0.5

1

1.5
n (samples)

2

2.5

3
104

(f)

4000

3500

3000

2500

2000

1500

1000

e
z
s

i

t
r
o
p
p
u
s

d
e
t
a
m

i
t
s
E

500

0

100

2
S
y
b

d
e
z

i
l

a
m
r
o
n
E
S
M

10-1

10-2

10-3

10-4

0

Support estimation results for Macbeth

RWC
RWC-S
WY
GT
PJW
HOSW
Ground Truth

5000

e
z
s

i

t
r
o
p
p
u
s

d
e
t
a
m

i
t
s
E

4000

3000

2000

1000

Support estimation results for King Lear

RWC
RWC-S
WY
GT
PJW
HOSW
Ground Truth

0.5

1
n (samples)

1.5

2
104

0

0

0.5

1

1.5
n (samples)

2

2.5

3
104

(c)

(d)

Support estimation results for Macbeth

RWC
RWC-S
WY
GT
PJW
HOSW

0.5

1
n (samples)

1.5

2
104

(g)

100

2
S
y
b

d
e
z

i
l

a
m
r
o
n
E
S
M

10-1

10-2

10-3

10-4

0

Support estimation results for King Lear

RWC
RWC-S
WY
GT
PJW
HOSW

0.5

1

1.5
n (samples)

2

2.5

3
104

(h)

Figure 8: The results obtained by aggregation over 100 independent trials. The ﬁrst row of the
ﬁgures shows the mean and standard deviation of the estimators, while the second row of the ﬁgures
shows the MSE normalized by S2.

22

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Genomic region RWC-S Naive WY GT PJW HOSW Maximum support

ORF1a
ORF1b
S
ORF3a
E
M
ORF6
ORF7a
ORF8
N
ORF10
All

1746
858
438
166
26
81
90
116
49
221
39
3830

911
477
246
99
15
51
52
66
32
139
30

2022 1082 1010
598
1046 549
309
277
563
108
108
204
16
43
25
64
63
85
61
158
105
76
157
131
43
42
53
175
146
274
40
35
21

2118 4529 2660 2500

2770
1346
700
272
30
103
118
156
60
336
21
5912

13203
8087
3822
828
228
669
186
366
366
1260
117
29132

Table 3: Results of the noiseless support estimation method on samples from Europe.

Genomic region

η = 0.5

η = 1

η = 1.5

ORF1a
ORF1b
S
ORF3a
E
M
ORF6
ORF7a
ORF8
N
ORF10
All

RWC-S-prc Naïve WY-Naïve WY-prc RWC-S-prc Naïve WY-Naïve WY-prc RWC-S-prc Naïve WY-Naïve WY-prc

-30.5
-28.4
-30.8
-32.5
-76.9
-25.9
-31.1
-33.6
-22.4
-24
7.7
-29.6

-50.6
-50.9
-51.6
-49.5
-60
-49
-55.8
-54.5
-46.9
-45.3
-40
-50.5

-116.5
-120
-122.9
-96.1
-76
-82.4
-93.3
-90.1
-83
-121.9
-61.9
-114.7

-64.7
-63.6
-64.7
-70.6
-76
-61.2
-68.6
-68.7
-58.5
-56.6
14.3
-64

-21.3
-17.9
-18.5
-18.1
-26.9
-19.8
-22.2
-24.1
-16.3
-14.5
7.7
-19.5

-28.9
-28.9
-30.1
-27.3
-33.3
-29.4
-30.8
-31.8
-28.1
-25.9
-20
-28.8

-54.7
-54.9
-57.7
-51.5
-44
-42.4
-50.5
-50.4
-39.6
-49.6
42.9
-53.5

-38
-37.5
-41
-39.2
-48
-35.3
-41
-42.7
-32.1
-33.2
66.7
-37.7

-15.9
-12.2
-12.6
-11.4
-15.4
-11.1
-16.7
-17.2
-10.2
-10
7.7
-13.8

-17.2
-17
-17.5
-16.2
-20
-15.7
-19.2
-18.2
-15.6
-15.1
-10
-16.9

-25.7
-22.9
-25
-28.4
-28
-21.2
-29.5
-29
-18.9
-16.4
85.7
-24.1

-31.2
-17
-16.2
-36.8
-12
3.5
-25.7
-24.4
-9.4
-34.3
71.4
-24.7

Table 4: Support estimation with synthetic Poisson repeats on samples from Europe. We report the
relative difference (in %) of the results for the noisy and noiseless counterparts (the closer the value
to 0, the better the performance).

Genomic region

η = 0.5

η = 1

η = 1.5

Maximum support

ORF1a
ORF1b
S
ORF3a
E
M
ORF6
ORF7a
ORF8
N
ORF10
All

RWC-S-prc Naive WY-Naive WY-prc RWC-S-prc Naive WY-Naive WY-prc RWC-S-prc Naive WY-Naive WY-prc
105101
67043
44332
5467
84
685
543
632
196
16493
-225
240351

3273
1649
888
299
32
133
153
187
82
508
91
7295

2022
1046
563
204
25
85
105
131
53
274
21
4529

3277
1718
967
273
23
96
124
151
61
450
29
7169

911
477
246
99
15
51
52
66
32
139
30
2118

911
477
246
99
15
51
52
66
32
139
30
2118

465
230
104
70
15
45
42
55
28
74
28
1156

911
477
246
99
15
51
52
66
32
139
30
2118

2346
1106
553
210
32
94
106
139
56
267
46
4955

1262
665
353
112
15
59
64
80
37
188
32
2867

4030
1908
949
325
55
135
153
202
79
429
69
8334

3173
1465
720
265
41
114
132
173
67
337
54
6541

13203
8087
3822
828
228
669
186
366
366
1260
117
29132

Table 5: Results of the noisy support estimation method directly applied to the real-world data on
samples from Europe. We underline the results that are obviously false (i.e., those violating the
maximum support constraint, taking negative values or values smaller than the results of the naive
estimator).

23

Genome region

RWC-S

Naive

WY

GT

PJW

HOSW

Maximum support

ORF1a
ORF1b
S
ORF3a
E
M
ORF6
ORF7a
ORF8
N
ORF10
All

1752
624
353
171
63
51
3
214
339
93
17
3680

835
316
188
93
36
31
3
109
340
60
11
2022

2139
762
455
185
82
65
3
234
330
115
17
4387

1543
747
375
186
185
55
Inf
633
340
70
15
4149

897
335
213
107
38
31
3
117
340
74
13
2168

2752
983
551
242
84
72
5
301
316
132
20
5458

13203
8087
3822
828
228
669
186
366
366
1260
117
29132

Table 6: Results of the noiseless support estimation method on samples from Asia.

Genome region

η = 0.5

η = 1

η = 1.5

ORF1a
ORF1b
S
ORF3a
E
M
ORF6
ORF7a
ORF8
N
ORF10
All

RWC-S-prc Naive WY-Naive WY-prc RWC-S-prc Naive WY-Naive WY-prc RWC-S-prc Naive WY-Naive WY-prc

-35.3
-41.8
-39.1
-36.8
-38.1
-31.4
-66.7
-42.1
-5
-24.7
-70.6
-34.4

-55.7
-56.3
-55.9
-55.9
-61.1
-54.8
-66.7
-57.8
-0.6
-45
-54.5
-46.4

-122.7
-104.9
-105.9
-89.2
-98.8
-92.3
-66.7
-91.5
-2.4
-99.1
-70.6
-104

-69.6
-76.8
-76.5
-70.3
-73.2
-69.2
-66.7
-73.1
3
-65.2
-70.6
-66.2

-26.1
-22.9
-21.5
-26.3
-25.4
-23.5
-33.3
-30.8
0.6
-16.1
-17.6
-22.6

-32.8
-33.9
-33
-32.3
-36.1
-32.3
-33.3
-35.8
0
-25
-27.3
-27.4

-61.7
-59.7
-59.3
-50.8
-56.1
-53.8
-33.3
-55.1
2.7
-47.8
-35.3
-54.8

-44.1
-47
-45.3
-44.3
-46.3
-46.2
-33.3
-47.9
3
-33.9
-41.2
-41.2

-18.8
-15.4
-14.7
-16.4
-19
-15.7
-33.3
-19.2
0.3
-9.7
-11.8
-15.7

-19.8
-19.9
-19.7
-19.4
-19.4
-19.4
-33.3
-21.1
0
-15
-18.2
-16.4

-32.3
-34.5
-34.9
-30.8
-35.4
-32.3
-33.3
-33.8
3.6
-22.6
-23.5
-30

-19.9
-29.8
-33.8
-3.2
-28
-21.5
-33.3
-23.5
-8.5
-19.1
-11.8
-21.8

Table 7: Tests of the noisy support estimation methods against synthetic Poisson repeats introduced
into samples from Asia. We report the relative difference (in %) compared to the results of the
noiseless counterparts (the closer the value to 0, the better the estimator).

Genome region

η = 0.5

η = 1

η = 1.5

Maximum support

ORF1a
ORF1b
S
ORF3a
E
M
ORF6
ORF7a
ORF8
N
ORF10
All

RWC-S-prc Naive WY-Naive WY-prc RWC-S-prc Naive WY-Naive WY-prc RWC-S-prc Naive WY-Naive WY-prc
183124
29256
4799
2764
560
401
3
4372
-11001
1015
44
215337

3508
1060
627
257
114
79
3
314
352
170
22
6506

4138
1578
809
314
105
81
3
409
346
163
33
7979

3864
1072
654
207
100
76
3
265
324
162
16
6743

1254
372
223
112
47
38
3
135
340
68
11
2603

2139
762
455
185
82
65
3
234
330
115
17
4387

835
316
188
93
36
31
3
109
340
60
11
2022

835
316
188
93
36
31
3
109
340
60
11
2022

2401
832
465
210
74
58
3
269
339
113
21
4785

835
316
188
93
36
31
3
109
340
60
11
2022

289
192
110
79
28
25
3
90
340
41
11
1208

3278
1144
621
266
90
70
3
346
341
137
26
6322

13203
8087
3822
828
228
669
186
366
366
1260
117
29132

Table 8: Results of noisy support estimation methods applied directly on samples from Asia. We
underline the results that are obviously false (i.e., those that violate the maximum support size
constraint, those that produce negative results and estimates smaller than the results of the Naive
estimator).

Genome region

RWC-S

Naive

WY

GT

PJW

HOSW

Maximum support

ORF1a
ORF1b
S
ORF3a
E
M
ORF6
ORF7a
ORF8
N
ORF10
All

1509
727
375
134
25
44
33
269
44
227
16
3403

804
403
209
81
15
28
21
135
29
138
10
1873

1765
912
515
161
24
52
43
285
45
272
16
4090

944
437
237
86
33
38
32
587
30
174
20
2618

980
442
229
97
15
32
23
142
36
185
14
2195

2374
1245
636
214
29
59
46
384
51
331
16
5385

13203
8087
3822
828
228
669
186
366
366
1260
117
29132

Table 9: Results for the noiseless support estimation methods applied to samples from North Amer-
ica.

24

Genome region

η = 0.5

η = 1

η = 1.5

ORF1a
ORF1b
S
ORF3a
E
M
ORF6
ORF7a
ORF8
N
ORF10
All

RWC-S-prc Naive WY-Naive WY-prc RWC-S-prc Naive WY-Naive WY-prc RWC-S-prc Naive WY-Naive WY-prc

-28.4
-30.3
-33.9
-30.6
-72
-27.3
-33.3
-43.1
-22.7
-27.3
-68.8
-31.1

-51
-49.6
-52.6
-50.6
-53.3
-50
-52.4
-57.8
-44.8
-49.3
-50
-51.1

-117.5
-115.5
-122.3
-96.9
-70.8
-86.5
-93
-90.2
-93.3
-101.1
-68.8
-112.4

-64.1
-65
-67.6
-69.6
-70.8
-63.5
-69.8
-73
-57.8
-68.4
-68.8
-65.9

-20.1
-19.5
-18.4
-17.2
-24
-15.9
-18.2
-31.2
-15.9
-15.4
-25
-20.2

-29.2
-28
-30.1
-27.2
-33.3
-25
-33.3
-34.8
-24.1
-27.5
-30
-29.2

-54.7
-56.1
-60
-49.7
-41.7
-46.2
-51.2
-54.4
-35.6
-50.4
-37.5
-54.7

-38
-40.2
-42.3
-37.9
-45.8
-36.5
-39.5
-48.4
-22.2
-37.1
-37.5
-39.6

-14.4
-13.9
-12.8
-11.2
-16
-11.4
-9.1
-20.1
-9.1
-7.9
-12.5
-13.8

-17.2
-16.4
-17.7
-16
-20
-17.9
-19
-20
-13.8
-15.9
-20
-17.1

-25
-26.8
-28.9
-25.5
-25
-26.9
-30.2
-34
-6.7
-22.8
-18.8
-26.3

-15.2
-57
-19.2
-30.4
-20.8
-15.4
-20.9
-18.2
8.9
-16.9
-18.8
-25.8

Table 10: Tests of the noisy support estimation methods against synthetic Poisson repeats introduced
into samples from North America. We report the relative difference (in %) compared to the results
of the noiseless counterparts (the closer the value to 0, the better the estimator).

Genome region

η = 0.5

η = 1

η = 1.5

Maximum support

ORF1a
ORF1b
S
ORF3a
E
M
ORF6
ORF7a
ORF8
N
ORF10
All

RWC-S-prc Naïve WY-Naïve WY-prc RWC-S-prc Naïve WY-Naïve WY-prc RWC-S-prc Naïve WY-Naïve WY-prc
95895
55948
50434
3656
72
235
252
4729
168
8471
47
219907

2802
1391
793
240
30
77
62
381
93
408
22
6299

3502
1535
799
253
50
71
54
535
70
469
28
7366

1114
558
305
91
15
34
26
165
32
157
10
2507

2851
1474
901
213
22
60
52
317
61
372
15
6338

1765
912
515
161
24
52
43
285
45
272
16
4090

804
403
209
81
15
28
21
135
29
138
10
1873

804
403
209
81
15
28
21
135
29
138
10
1873

407
206
82
58
15
24
17
113
22
95
10
1049

2005
924
471
167
31
51
39
345
52
287
18
4390

2716
1202
608
209
39
60
46
448
61
369
23
5781

804
403
209
81
15
28
21
135
29
138
10
1873

13203
8087
3822
828
228
669
186
366
366
1260
117
29132

Table 11: Results of noisy support estimation methods applying directly to North America samples.
We underline the results that are obviously false (i.e. violating maximum support, being negative
and smaller than the results of Naive estimator).

25

