Testing Group Fairness via Optimal Transport Projections

Nian Si1 Karthyek Murthy 2 Jose Blanchet1 Viet Anh Nguyen1 3

1
2
0
2

n
u
J

2

]
L
M

.
t
a
t
s
[

1
v
0
7
0
1
0
.
6
0
1
2
:
v
i
X
r
a

Abstract
We present a statistical testing framework to de-
tect if a given machine learning classiﬁer fails to
satisfy a wide range of group fairness notions.
The proposed test is a ﬂexible, interpretable, and
statistically rigorous tool for auditing whether
exhibited biases are intrinsic to the algorithm or
due to the randomness in the data. The statis-
tical challenges, which may arise from multi-
ple impact criteria that deﬁne group fairness and
which are discontinuous on model parameters,
are conveniently tackled by projecting the empir-
ical measure onto the set of group-fair probabil-
ity models using optimal transport. This statis-
tic is efﬁciently computed using linear program-
ming and its asymptotic distribution is explicitly
obtained. The proposed framework can also be
used to test for testing composite fairness hy-
potheses and fairness with multiple sensitive at-
tributes. The optimal transport testing formula-
tion improves interpretability by characterizing
the minimal covariate perturbations that elimi-
nate the bias observed in the audit.

1. Introduction

Algorithmic decisions are commonly conceived to have the
potential of being more objective than a human’s decisions,
since they are generated by logical instructions and the
rules of algebra. However, recent studies indicate that this
may not be the case. For example, an algorithm which
helps the US criminal justice system to predict recidi-
vism rates has been shown to falsely give a higher risk for
African-Americans than white Americans (Chouldechova,
2017; MultiMedia LLC, 2016). Similar biases are exhib-
ited against female candidates in a hiring-help system de-
veloped by Amazon AI (Dastin, 2018) and an ad-targeting
algorithm used by Google (Datta et al., 2015).

1Department of Management Science & Engineering, Stanford
University 2Engineering Systems and Design pillar, Singapore
University of Technology and Design 3VinAI Research, Vietnam.
Correspondence to: Nian Si <niansi@stanford.edu>.

Proceedings of the 38 th International Conference on Machine
Learning, PMLR 139, 2021. Copyright 2021 by the author(s).

A natural ﬁrst explanation for the reported algorithmic bi-
ases is that the data used to train the algorithms may already
be corrupted by human biases (Buolamwini & Gebru,
2018; Manrai et al., 2016). Deeper inquests have re-
vealed insights on how common learning procedures in-
trinsically perpetuate the biases and potentially introduce
fresh ones. The usual practice of training by minimiz-
ing empirical risk, while geared towards yielding predic-
tions that are best when averaged over the entire popu-
lation, often under-represents minority subgroups in the
datasets. Moreover, even though certain sensitive attributes
are forbidden by law to be used in the algorithm, the
strong correlations between the sensitive attributes and
other features potentially lead to biases in predictions.
As reported in the studies in (Grgic-Hlaca et al., 2016;
Garg et al., 2019; Barocas & Selbst, 2016; Black et al.,
2020; Kleinberg et al., 2018; Lipton et al., 2018), merely
masking the sensitive attributes does not address the prob-
lem.

The aforementioned biases and their impacts have sparked
substantial interests in the pursuit of algorithmic fair-
ness
(Berk et al., 2018; Chouldechova & Roth, 2020;
Corbett-Davies et al., 2017; Mehrabi et al., 2019). Testing
whether a given machine learning algorithm is fair emerges
as a question of ﬁrst-order importance. In turn, designing
this test for a wide range of group fairness notions (dis-
cussed in the sequel) is the main task of this paper.

Our proposed statistical hypothesis testing framework (test-
ing framework for short) allows the auditors to systemati-
cally determine whether the biases exhibited in the audit
procedure, if any, are intrinsic to the algorithm or due to the
randomness in data. Moreover, our framework can be im-
plemented as a black-box, without knowing the exact struc-
ture of the classiﬁcation algorithm used.

For settings where sensitive attributes are not explic-
itly used as input to classiﬁcation, fairness is measured
on impact either at a group level or at an individual
level (Barocas & Selbst, 2016). Group fairness notions
seek to measure the differences in impacts across dif-
ferent groups and constitute the prominent means of as-
sessing discrimination associated with group member-
ships.
Individual fairness, on the other hand, seeks to
assess if similar users are treated similarly (Dwork et al.,

 
 
 
 
 
 
Testing Group Fairness via Optimal Transport Projections

2012; John et al., 2020; Xue et al., 2020). Common ex-
amples of group fairness notions include disparate im-
pact
(Zafar et al., 2017), demographic parity (statisti-
cal parity) (Calders & Verwer, 2010), equality of oppor-
tunity (Hardt et al., 2016), equalized odds (Hardt et al.,
2016), etc. The speciﬁc choice is usually driven by the
philosophical, sociological or legal constraints binding the
application considered.

Our testing framework applies to a generic notion of group
fairness which encompasses all of the above speciﬁc group
fairness notions as examples. This unifying approach can
also be used in contexts requiring the use of different
fairness notions simultaneously and settings with multiple
groups.

Since a single fairness criterion among two groups can be
reduced to testing the equality in two sample conditional
means, one may consider employing a Welch’s t-test or
a permutation test. Further, a suitable adjustment of the
randomness in sample sizes, as in DiCiccio et al. (2020);
Tramer et al. (2017) can be applied. Some other existing
methods such as Besse et al. (2018) also only apply to one-
dimensional criterion. Extensions to multiple impact cri-
teria are not immediate, as is the equalized odds case cri-
terion (Hardt et al., 2016), or in the presence of multiple
groups. Algorithmic approaches, such as in (Saleiro et al.,
2018), lack the control of the type-I (false positive error).
In contrast, the framework proposed here is applicable un-
der general multiple impact criteria and controls the type-I
error exactly.

The statistical challenges, which may arise from the pres-
ence of multiple impact criteria, are conveniently handled
in our framework by utilizing the machinery of optimal
transport projections. This involves computing the test
statistic by projecting, or in other words, optimally trans-
porting the empirical measure to the set of probability mod-
els which satisfy the group fairness notion (or notions) con-
sidered. This gives a measure of plausibility of the clas-
siﬁer in satisfying the fairness criterion under the data-
generating distribution and the fairness hypothesis is duly
rejected if the test statistic exceeds a suitable threshold de-
termined by the signiﬁcance level. This threshold is de-
termined from the limiting distribution of the test statistic
obtained as one of the main results of this paper.

Performing statistical inference with a projection criterion
is prevalent in statistics: Owen (2001) serves as a com-
prehensive reference for projections, or proﬁle functions,
that are computed based on likelihood ratio metrics or
the Kullback-Liebler divergence. Blanchet et al. (2019)
and Cisneros-Velarde et al. (2020) study statistical infer-
ence with optimal transport projections. Recently, optimal
transport divergences (Villani, 2008) become an attractive
tool in many recent machine learning studies, including

missing data imputation, geodesic PCA, point embeddings,
and repairing data with Wasserstein barycenters for training
fair classiﬁers (Silvia et al., 2020; Gordaliza et al., 2019;
Zehlike et al., 2020).

Taskesen et al. (2021) uses optimal transport projections to
test a smooth relaxation of the equal opportunity criterion
called probabilistic fairness; see Pleiss et al. (2017). This
relaxation is required in Taskesen et al. (2021) to overcome
the discontinuities in the classiﬁcation boundaries which
create technical complications when computing the optimal
transport projections. Further, the resulting test statistic in-
volves a non-convex optimization problem which is difﬁ-
cult to compute. In contrast, our work resolves the tech-
nical challenges arising from the discontinuities in classiﬁ-
cation boundaries. Moreover, our test statistic is the opti-
mal value of a linear program, whose optimal solution of-
fers interpretability by characterizing the minimal covari-
ate perturbations that eliminate the bias observed in the au-
dit. We emphasize that addressing boundary discontinu-
ities is not simply a technical improvement. As we dis-
cuss in Section 3.3, our results show different qualitative
behaviors both in the scaling and the interpretation of the
optimal transport projection in terms of group fairness us-
ing optimal transport projections. In addition to enabling
exact, computationally tractable, and interpretable fairness
assessment for general deterministic classiﬁers, the techni-
cal analysis serves as a stepping stone for statistical infer-
ence in estimation tasks such as quantile regression which
involve discontinuous estimating equations.

The main contributions are summarized as follows:

(1) We develop a statistical hypothesis test for assessing
group fairness as per a generic notion that includes com-
monly used fairness criteria as special cases. The test is
computationally tractable and interpretable. Besides being
applicable to settings involving multiple groups, our frame-
work is also applicable to any classiﬁer algorithms, includ-
ing but not limited to the logistic regression, SVMs, kernel
methods, and nearest neighbors.

(2) We develop an extension of the statistical test for the
testing problem with composite null hypothesis, addressed
here as ǫ-fairness.

(3) The framework facilitates the exact use of fairness cri-
terion, thus obviating the need to invoke relaxations in the
absence of smoothness in the impact criteria deﬁning group
fairness. The resulting qualitative difference, in terms of
the rate of convergence for resulting optimal transport pro-
jections, is previously unreported and could be of interest
from the technical standpoint of analysing proﬁle functions
with discontinuous score functions.

The remainder of the paper is structured as follows. In Sec-
tion 2, we introduce a generic notion of group fairness and

Testing Group Fairness via Optimal Transport Projections

discuss the theory of optimal transport. Section 3 details
the proposed statistical test for the simple fairness null hy-
pothesis. Section 4 extends our approach to composite hy-
potheses. Section 5 discusses computational methods as-
sociated with the test. Numerical experiments presented in
Section 6 serve to demonstrate the efﬁcacy of the test. All
technical proofs are relegated to Appendix A.

}

,
⇒

Notations. We use
.
to denote the dual norm of
k · k
k · k∗
and (x)+ = max
.
We denote (x)− = min
x, 0
x, 0
}
{
{
p
and a.s.
We use
to denote convergence in distribu-
−→
−→
tion, in probability and convergence almost surely, respec-
tively. The support of the distribution of X is represented
and
by supp(X). We use [n] to denote the set
Rm
.
0
}
δ(x,a,y) denotes a Dirac measure on a ﬁxed point (x, a, y).

+ to denote the positive orthant

1, 2, . . . , n
Rm
+ : x

{
∈

}
≥

x

{

2. Problem Setup and Preliminaries

}

{

C

=

0, 1

X ⊂

X → Y

. For simplicity, we consider

Throughout this paper we consider the classiﬁcation set-
tings in which the deterministic classiﬁer
maps
:
Rd to output labels in the set
the input features from
. Evaluation of fairness is considered with re-
Y
spect to a sensitive attribute A taking values in a ﬁnite set
where A = 1
A
can be taken to identify the reference group. The statistical
test developed in this paper and the main results are ap-
plicable more generally to settings involving a non-binary
sensitive attribute (or) multiple sensitive attributes. Most
notions of group fairness are stated in terms of the joint
distribution Q of (X, A, Y ), where X is the vector of input
features, A is the sensitive attribute, and Y is the class label
of a random sample from the population.

0, 1

A

=

{

}

2.1. Notions of Group Fairness

A general reference to fairness notions can be found in
Makhlouf et al. (2020, Table 14). The statistical notion of
group fairness that we consider, encapsulated in Deﬁnition
1 below, is stated ﬂexibly to include commonly used no-
tions such as equal opportunity (Hardt et al., 2016), predic-
tive equality (Corbett-Davies et al., 2017), equalized odds
(Hardt et al., 2016), and statistical parity (Dwork et al.,
2012), etc., as special cases. This ﬂexibility is achieved
by stating the deﬁnition in terms of a tuple (U, φ), where
U is an Rs-valued random vector completely dependent on
(A, Y ) and φ : Rs
Rm is a function chosen to dis-
cern the differences in performance of the classiﬁer across
groups. We address (U, φ) as the discerning tuple.

Rs

→

×

Deﬁnition 1 (Generic notion of group fairness). A classi-
ﬁer
is fair with respect to the discerning
tuple (U, φ) under a probability distribution Q if

X → {

0, 1

C

}

:

EQ[

C

(X)φ(U, EQ[U ])] = 0.

(1)

{C
{C

Note that
, and thus equation (1)
(X) = I
(X) = 1
C
}
φ(U, EQ[U ])] = 0. At
can be seen as EQ[I
(X) = 1
}
the ﬁrst glance, equation (1) seems asymmetric as it only
considers the positive prediction label
(X) = 1. How-
ever, it is easy to check that EQ[φ(U, EQ[U ])] = 0 in all
the group fairness notions in Examples 1 - 6. By tak-
ing the difference, we get the symmetric guarantee that
EQ[I

φ(U, EQ[U ])] = 0.

(X) = 0

C

{C

}

Various useful notions of fairness can be obtained by vary-
ing the choice of (U, φ) as illustrated in Examples 1 - 6
below. We take

in Examples 1 - 4.

0, 1

=

A

{

}

Example 1 (Equal opportunity (Hardt et al., 2016)). A
satisﬁes the equal opportunity
classiﬁer
X → {
criterion relative to a distribution Q if

0, 1

C

}

:

A = 1, Y = 1)

Q (
C
−

(X) = 1
Q (
C

|
(X) = 1

A = 0, Y = 1) = 0.

|

This criterion coincides with condition (1) with the choice
U = (I(1,1)(A, Y ); I(0,1)(A, Y )), where I(a,y)(A, Y ) de-
notes the indicator I(A = a, Y = y), and

φ : (U, EQ[U ])

U1

7→

EQ[U1] −

U2
EQ[U2]

.

(2)

Example 2 (Predictive equality (Corbett-Davies et al.,
2017)). A classiﬁer
satisﬁes the predic-
tive equality criterion relative to a distribution Q if

X → {

0, 1

C

}

:

A = 0, Y = 0)

Q (
C
−

(X) = 1
Q (
C

|
(X) = 1

A = 1, Y = 0) = 0.

|

This criterion coincides with condition (1) with the choice
U = [I(1,0)(A, Y ); I(0,0)(A, Y )] and φ takes the same form
as the function (2).

:

C

Example 3 (Equalized odds (Hardt et al., 2016)). A
classiﬁer
satisﬁes the equalized
0, 1
odds criterion relative to a distribution Q if it satisﬁes
both equal opportunity and predictive equality criteria.
This criterion coincides with condition (1) with U =
[I(1,1)(A, Y ); I(0,1)(A, Y ); I(1,0)(A, Y ); I(0,0)(A, Y )] and

X → {

}

φ : (U, EQ[U ])

U1
EQ[U1] −

U2
EQ[U2]

;

U3
EQ[U3] −

U4
EQ[U4]

.

7→

h

i
Example 4 (Statistical parity (Dwork et al., 2012)). A clas-
satisﬁes the statistical parity crite-
siﬁer
rion relative to a distribution Q if

X → {

0, 1

C

}

:

Q (
C

(X) = 1

A = 1)

|

Q (
C

−

(X) = 1

|

A = 0) = 0.

This criterion coincides with condition (1) with the choice
U = [I1(A); I0(A)] and φ takes the same form as the func-
tion (2).

Testing Group Fairness via Optimal Transport Projections

If the sensitive attribute takes multiple values or there are
multiple sensitive attributes, we can still deﬁne the associ-
ated fairness notions, which correspond to different choices
of (U, φ).

Example 5 (Equal opportunity with a non-binary sensitive
attribute). Let
0, 1, 2, . . . , k
0, 1
{
probability measure Q if

X →
satisﬁes the equal opportunity criterion relative to a

. A classiﬁer

A

=

C

{

}

}

:

,

·

Z

) is a metric on

If c(
) is the type-1 Wasser-
, then Wc(
·
·
stein distance; see Villani (2008, Chapter 6). The quantity
Wc(Q1, Q2) can be interpreted as the least transportation
cost incurred in transporting mass from Q1 to Q2, when
the cost of transporting unit mass from location z
to
is given by c(z, z′).
location z′

∈ Z

∈ Z

Throughout the paper, we assume that the function c is de-
composable as

A = t, Y = 1)

Q (
C

(X) = 1
Q (
C

|
(X) = 1

−

A = 0, Y = 1) = 0

|

t
∀

∈ A\{

0

.

}

This criterion coincides with condition (1) with the choice
U = (I(0,1)(A, Y ); I(1,1)(A, Y ); . . . I(k,1)(A, Y )) and φ =
(φ1, φ2, . . . , φt) with

φt : (U, EQ[U ])

Ut

7→

EQ[Ut] −

U1
EQ[U1]

t
∀

∈ A\{

0

.

}

Example 6 (Equal opportunity with multiple sensitive
attributes). Suppose we have K sensitive attributes,
. A clas-
A1, A2, . . . , AK, all taking values in a superset
siﬁer
satisﬁes the equal opportunity crite-
}
rion relative to a probability measure Q if

X → {

0, 1

A

C

:

At = 1, Y = 1)

Q (
C

(X) = 1
Q (
C

|
(X) = 1

−

At = 0, Y = 1) = 0

|

t
∀

∈

[K].

This criterion coincides with condition (1) with the choice

Ut = I(1,1)(At, Y ) and Ut+K = I(0,1)(At, Y )

t
∀

∈

[K]

and φ = (φ1, φ2, . . . , φt) with

φt : (U, EQ[U ])

Ut

7→

EQ[Ut] −

Ut+K
EQ[Ut+K]

t
∀

∈

[K].

2.2. Optimal Transport and the Wasserstein Distance

We next introduce the notion of optimal transport costs, of
which Wasserstein distances is a special case. Let P (
)
denote the set of all probability distributions on

,

Z
X ×

Z

A × Y
Deﬁnition 2 (Optimal transport costs, Wasserstein dis-
tances). Given a lower semicontinuous function c :

Z ×
], the optimal transportation cost Wc(Q1, Q2)
) is given by,

P (

∞

Z →
between any two distributions Q1, Q2 ∈
Wc(Q1, Q2) = min

Eπ [c (Z, Z ′)] ,

Z

.

[0,

π

Π(Q1,Q2)

∈

where Π(Q1, Q2) is the set of all joint distributions of
(Z, Z ′) such that the law of Z = (X, A, Y ) is Q1 and that
of Z ′ = (X ′, A′, Y ′) is Q2.

c ((x, a, y), (x′, a′, y′))
a

= ¯c(x, x′) +

a′

+

y

y′

,

|

∞ · |

∞ · |

−
for some ¯c :
] satisfying (i) ¯c(x, x′) = 0
∞
if and only if x = x′ and (ii) ¯c(x, x′) = ¯c(x′, x) for all
0 = 0.
. In the above expression, we interpret
x, x′
∈ X
Examples of ¯c(
·

) that are useful in our context include

X × X →

∞×

[0,

−

,

·

|

¯c(x, x′) =

x

k

x′

,

k

−

(3a)

and also

(3b)

¯c(x, x′) = k(x, x)

2k(x, x′) + k(x′, x′),

X × X →

−
R is a suitable reproducing kernel.
where k :
Another useful example of ¯c(
) is speciﬁed in terms of the
·
discrete metric suitable for use in the presence of discrete
categorical features: Suppose that the feature vector X =
(XD, XC ), with XD denoting the set of discrete features
Rd1 and XC denoting
taking values in a countable set D
the set of continuous features taking values in Rd2. We have
d1 + d2 = d. In this instance, it is feasible to restrict the
transportation to elements in D

Rd2 by considering

⊂

×

=

¯c ((xD, xC ), (x′D, x′C ))
x′C k
+ δI
xC −
xD, x′D}
I
{{
∞ ·

+

k

xD, x′D} ⊂

{{
* D, xD 6

D, xD 6
,

= x′D}

= x′D}
for some δ > 0.
Further, we allow the cost func-
tion to be dependent on the sensitive attribute. Follow-
ing the same line, Hui et al. (2021) recently demonstrates
a test power gain by tuning properly a sensitive-attribute-
dependent transportation cost function.

(4)

3. Test For Simple Null Hypothesis via

Optimal Transport

Recall that P (
tions on
,

Z

Z
X × A × Y

. Let

) denotes the set of all probability distribu-

=

Q

{

∈

F

P (

Z

) : EQ[

C

(X)φ(U, EQ[U ])] = 0

}

be the collection of distributions under which the classi-
) is fair, as deemed by Deﬁnition 1. Given N in-
ﬁer
N
i=1 from a distribution P of
dependent samples

(
·

C

xi, ai, yi}

{

Testing Group Fairness via Optimal Transport Projections

(X, A, Y ), we are interested in the statistical test with the
hypotheses

Proposition 1 (Primal reformulation). The projection dis-
(ˆPN ) is equal to the optimal value of a linear pro-
tance
gram. More speciﬁcally, we have

P

H0 : P

∈ F

.

against

H1 : P
H0 being that the classiﬁer

6∈ F

) is
With the null hypothesis
(
C
·
fair, our statistical test will detect the failure of
) in meet-
ing the fairness criterion (in Deﬁnition 1) under the data
generating distribution. To develop a suitable test statistic,
N
let ˆPN = N −
i=1 δ(xi,ai,yi) denote the empirical mea-
sure of the samples obtained from a distribution P
).
P
We deﬁne the projection of ˆPN onto

P (

(
·

as

Z

∈

C

1

F

(ˆPN ) , inf
∈F

Q

P

Wc(Q, ˆPN )

=

(cid:26)

inf Wc(Q, ˆPN )
s.t. EQ[

C

(X)φ(U, EQ[U ])] = 0.

(5)

D

(ˆPN ) ,

minp

pid(xi)

s.t.

p

1
N

Xi
[N ]
∈
[0, 1]N ,
∈
2
[1

−

C

[N ]
Xi
∈
=

(xi)]φ(ui, EˆPN [U ])pi

−

C

[N ]
Xi
∈

(xi)φ(ui, EˆPN [U ]).

(7)

(ˆPN ) =

P






Naturally, one may study the above linear program by con-
sidering its dual formulation. Deﬁne the following function

We adopt the statistical hypothesis framework: for a pre-
speciﬁed signiﬁcance level α,

max
Rm
γ
∈

(

1
i
N
∈
d(xi) + [1

[N ] γ⊤φ(ui, EˆPN [U ])
C
(xi)] γ⊤φ(ui, EˆPN [U ])

(xi)+

2

P

−

C

,

− )

reject

H0 if sN > η1
where sN is a test statistic that depends on the projection
(ˆPN ), and η1
distance
100% quantile
α is the (1
−
of a limiting distribution.

α)

α,

×

−

P

−

(cid:0)

where recall the notation that (x)− = min
{
duality of linear programming asserts that

x, 0

(cid:1)
. Strong
}
(ˆPN ) and

P

(ˆPN ) are dual to each other.

D
Proposition 2 (Strong duality). Strong duality holds, i.e.,

(ˆPN ) =

(ˆPN ).

P

D

3.1. Linear Programming Formulation for Projection

3.2. Asymptotic Behavior of the Projection Distance

Our aim here is to reformulate the inﬁnite dimensional pro-
jection formulation (5) as a ﬁnite dimensional linear pro-
gram. For this purpose, let us deﬁne d :

] as

[0,

X →

∞

d(x) , inf

¯c(x, x′) : x′
{

,

C

∈ X

(x′) = 1

,

(x)
}

− C

(6)

which gives a measure of distance to the region with clas-
siﬁer label different from that at x, and d(x) = 0 means
that x on the decision boundary. The value d(x) is read-
ily computed for commonly used classiﬁers such as linear
classiﬁers (as shown in the proof of Lemma 1 below in the
supplement) and kernelized classiﬁers. In the case of a clas-
siﬁer deﬁned in terms of kernels, say as in,

(x) = I

C

n

i=1
X

αik(xi, x) + b

,

0

!

≥

one may use the transportation cost (3b) and d(x) admits a
closed-form expression

n

d(x) =

αik(xi, x) + b

i=1
X

2

!

/(α⊤Kα),

where K is an n

×

n matrix with entries Ki,j = k(xi, xj).

The goal of this subsection is to study the limiting behav-
(ˆPN ) as the sample size N
ior of the projection distance
P
increases. Proposition 2 implies that it is sufﬁcient to ex-
(ˆPN ). To present the
amine the asymptotic behavior of
regularity assumptions under which the limiting behavior
can be unravelled, we set µ = EP[U ] and deﬁne Φ as

D

Φ :

X →

R, Φ(X) = (2

(X)

C

−

1)d(X).

Assumption 1 (Continuous density and derivatives). There
exists η > 0 such that the below conditions are satisﬁed:

a) The probability distribution of Φ(X) has a positive
v, v), i.e.,
v, v).
(
−

continuous density f (
·
v, u)) =
[
P(Φ(X)

) in the interval (
u
v f (ν)dν for u

−
∈

−

∈
b) For every u

−
R

∈

supp(U ), the function φ(u, z) has a
continuous derivative (Jacobian matrix) φz(u, z) in
k2 < v. In ad-
the neighborhood z satisfying
−
dition, Σ1 , EP[φ(U, µ)φ(U, µ)⊤
0.
|

µ
d(X) = 0]

≻

k

z

Assumption 2 (Continuous conditional probability). For
the case where supp(U ) is a ﬁnite set, the conditional prob-
ability P(U = u
Φ(X) = t) is continuous around t = 0
for every u

|
supp(U ).

∈

 
 
Testing Group Fairness via Optimal Transport Projections

We are now ready to state the main result of this section
concerning the asymptotic behavior of the projection dis-
tance.
(ˆPN )). Suppose that
Theorem 1 (Limit theorem for
are independently obtained from
Xn, Un}
X1, U1}
{
the distribution P and that Assumptions 1 and 2 are satis-
ﬁed. Then under the null hypothesis

, ...,

D

{

γ⊤V

1
2

−

N

× D

(ˆPN )

⇒

max
Rm
γ
∈

(cid:26)
where S = f (0)Σ1, V
∼ N
matrix of φ(U, µ)
(X)+ EP [φz(U, µ)
C
C
notes the convergence in distribution.

=

1
2

(cid:27)

V ⊤S−

1V,

(0, Σ), Σ is the covariance
de-
(X)] U , and

⇒

H0,
γ⊤Sγ

The ﬁnite cardinality of the outcome space of U in As-
sumption 2 is not restrictive: Theorem 1 still holds under
inﬁnite cardinality under an equivalent assumption. Details
can be found in Appendix A. For the fairness notions in
Examples 1 - 4, we report in Corollary 1 below the speciﬁc
closed-form limit distributions obtained from Theorem 1.
Corollary 1. Suppose that φ(U, µ) = U1
with U1, U2
satisfying U1U2 = 0 (with probability 1), as in Examples
1, 2, and 4. Then we have the limiting distribution,

µ1 −

U2
µ2

V ⊤S−

1V /2

=

2f (0) (µ2

2EP[U1|

σ2χ2(1)
d(X) = 0] + µ2

1EP[U2|

d(X) = 0])

,

where χ2(1) is a chi-squared distribution with one degree
of freedom and

σ2 =var

{C
+ U2EP [U1C
For Example 3, we have

(X) (µ2U1 −
(X)]
−

µ1U2)
U1EP [U2C

(X)]
}

.

V ⊤S−

1V /2

2f (0) (µ2

2EP[U1|

1χ1(1)2
σ2

d(X) = 0] + µ2

2χ2(1)2
σ2

1EP[U2|

d(X) = 0])

=

+

2f (0) (µ2

4EP[U3|
1(1) and χ2

d(X) = 0] + µ2

3EP[U4|
2(1) are two independent chi-squared

d(X) = 0])

where χ2
distributions with one degree of freedom and

.

σ2

σ2

1 =var
{C
+ U2EP [U1C
2 =var
{C
+ U4EP [U3C

(X) (µ2U1 −
(X)]
−
(X) (µ4U3 −
(X)]
−

µ1U2)
U1EP [U2C
µ3U4)
U3EP [U4C

(X)]
}

,

(X)]
}

.

Assumption 1a) is satisﬁed for a broad class of classiﬁca-
tion models interesting in practice. Lemma 1 below identi-
ﬁes that Assumption 1a) is satisﬁed even if there are some
discrete features.

+

C

→

⊂

ℓ(θ⊤X)

(X) = I
{

where ℓ(
·
ℓ(x) > τ > limx

∞
→−∞
= 0, we have that Assumption 1.a) is satisﬁed.

≥
) is a continuous and increasing function with
ℓ(x) and θ = (θD, θC )

Lemma 1. Suppose that X = (XD, XC), where XD takes
Rd1, d = d1 + d2 and XC is
values in a ﬁnite subset D
an Rd2-valued random vector whose conditional distribu-
XD = x has a positive density in Rd2 for every
tion XC |
) be given by (3a) or (4).
D. Let the cost function ¯c(
x
·
∈
Further, if the classiﬁer is written as
τ
}
limx
with θC 6
(ˆPN ) as in Proposition 2, Theorem 1 re-
With
(ˆPN ) as a test statistic
veals that one can use sN , N
× P
H0. In particular, for a prespeciﬁed signiﬁcance
to reject
α denote the (1
level α
100%
∈
quantile of the generalized chi-squared distribution given
1V /2. Speciﬁc computation and esti-
by the law of V ⊤S−
mation procedures required to compute the statistic sN and
α are discussed in Section 5.
the quantile η1

(0, 1), let η1

(ˆPN ) =

α)

×

−

D

P

−

−

3.3. The Structure of the Wasserstein Projection

In this subsection, we characterize the projection measure
Q and provide a heuristic justiﬁcation for the convergence
rate of Theorem 1. Note that the proof of Proposition 1 also
leads to an ε-optimizer sequence for (5).

Proposition 3 (ε-optimizer). Suppose d(xi) < +
every i
(6) with x = xi, for i
solution of the problem (7). Then, the measure

for
i be an ε-optimizer obtained by solving
p⋆
N
i=1 be an optimal
i }

[N ]. Let xε

[N ]. Let

∞

∈

∈

{

Qε ,

1
N

N

(1

−

p⋆
i )δ(xi,ai,yi) + p⋆

i δ(xε

i ,ai,yi)

i=1
X
is an ε-optimizer of the problem (5).

i when p⋆
i

Proposition 3 indicates in the optimal transportation plan,
the transporter moves mass from xi to xε
= 0.
Since the optimal solution of a linear programming prob-
lem occurs at corner points, most of p⋆
i should be either
zero or one. Further, the proof of Theorem 1 shows that
p⋆
N
the number of non-zero values in
i=1 is of the or-
i }
{
der Op(N 1/2) and for each non-zero p⋆
i , the moving dis-
1/2) under the null hy-
tance d(xi) is of the order Op(N −
1).
pothesis. Therefore,
This statistical phenomenon is due to the discontinuity of
the estimating function
(X)φ(U, EQ[U ]) in X, where the
transporter is able to move a small amount of probability
mass, but the move results in a signiﬁcant change of the
value for the estimating function around the discontinu-
1) convergence rate is in contrast
ity region. The Op(N −
to the rate in Blanchet et al. (2019), Taskesen et al. (2021)
and Cisneros-Velarde et al. (2020), where the estimating
function is assumed to be continuous. For the continu-
ous estimating function, it is optimal to move every point

(ˆPN ) is of the order Op(N −

D

C

6
Testing Group Fairness via Optimal Transport Projections

1/2) con-
1/2) distances, which results in a Op(N −
Op(N −
vergence rate. Therefore, let us emphasize again the key
qualitative difference between our contributions and those
of Taskesen et al. (2021). A statistical noise gives the em-
pirical appearance of unfairness in two ways: (A) small
statistical ﬂuctuations around all data points; (B) a small
sub-population with large outcome ﬂuctuations around the
decision boundary. Taskesen et al. (2021) studied scenario
(A) and our paper studies scenario (B).

4. Test For Composite Null Hypothesis via

Optimal Transport

In settings where the notion of exact group fairness be-
comes restrictive or unattainable, it becomes attractive to
test whether the deviation from fairness, if any, from a
given group’s viewpoint is not more than a prespeciﬁed
small extent. The question of verifying ǫ-fairness, from an
one-sided perspective, can be similarly formulated as fol-
lows. Following Section 3, we deﬁne

Q : EQ[

{

C

(X)φ(U, EQ[U ])]

,

ǫ
}

≤

Fǫ =
Rm

+ is a tolerance level prespeciﬁed by the fair-
where ǫ
ness auditor. We are interested in the statistical test with
the hypotheses

∈

A suitable statistical test for this formulation will serve the
) in meeting the one-
purpose of detecting failure of
sided fairness condition within an ǫ tolerance. Similar to
Problem 5, we deﬁne the projection of ˆPN onto

(
·

C

Fǫ as

Pǫ(ˆPN ) ,

(cid:26)

inf Wc(Q, ˆPN )
s.t. EQ[

(X)φ(U, EQ[U ])]

C

ǫ.

≤

4.1. Linear Programming Formulation for Projection

Proposition 4 (Primal reformulation). The projection dis-
Pǫ(ˆPN ) is equal to the optimal value of a linear pro-
tance
gram. More speciﬁcally, we have

(9)

pid(xi)

respective dual function,
Dǫ(ˆPN ) ,
max
Rm
+ −
γ
∈

γ⊤ǫ +

1
N

γ⊤φ(ui, EˆPN [U ])
C

(xi)

[N ] n
Xi
∈
2

−

C

+

d(xi) + (1

(xi)) γ⊤φ(ui, EˆPN [U ])

−

.

(cid:0)
Proposition 5 (Strong duality). Strong duality holds, i.e.,
Pǫ(ˆPN ) =
4.2. Asymptotic Behavior of the Projection Distance

Dǫ(ˆPN ).

(cid:1)

o

Dǫ(ˆPN ) as the sample size N
We next study the limit of
tends to inﬁnity. In order to state the theorem, let us in-
troduce notation for asymptotic stochastic ordering. We
say that a sequence of random elements
1 satis-
ﬁes An .D B if for every continuous and bounded non-
decreasing function g,

An}n

{

≥

E [g(An)]

E [g(B)] .

lim sup
n

→∞

≤
X1, U1}

Theorem 2. Suppose that
are in-
dependently obtained from the distribution P and that As-
sumptions 1 and 2 are satisﬁed. Then under the null hy-
pothesis

Xn, Un}

, ...,

{

{

H0,
× Dǫ(ˆPN ) .D max

γ

1
2

γ⊤V

γ⊤Sγ

,

(10)

∈

−

Rm
+ (cid:26)
(0, Σ), and Σ is the covariance
(X)] U. In particular, if

(cid:27)

where S = f (0)Σ1, V
of φ(U, µ)
(X) + EP [φz(U, µ)
C
φ(U, µ) is one-dimensional (m = 1), we have

∼ N

C

1
2

1
2

.

1V 2I
{

0

}

V

(cid:27)

=

≥

−

S−

γ⊤V

γ⊤Sγ

max
Rm
γ
+ (cid:26)
∈
Dǫ(ˆPN ) as in Proposition 5, Theorem 2
Pǫ(ˆPN ) =
With
× Pǫ(ˆPN ) as a test
reveals that one can use sN (ǫ) , N
α, deﬁned by the (1
H0 and η1
statistic to reject
×
−
100% quantile of the right hand side bounding variable in
(10), as a threshold. We then follow the same hypothesis
testing procedure deﬁned in Section 3. Since Theorem 2
only provides a stochastic upper-bound, we actually use a
conservative quantile and the type I error is less than or
equal to the desired signiﬁcance level α asymptotically.

α)

−

Pǫ(ˆPN )

minp

s.t.

p

1
N

Xi
[N ]
∈
[0, 1]N
∈
2
(1

−

[N ]
Xi
∈

+

=






(xi))φ(ui, EˆPN [U ])pi

C

5.1. Computations of the Test Statistic

5. Computation and Estimation Procedures

(xi)φ(ui, EˆPN [U ])

N ǫ.

≤

C

[N ]
Xi
∈

Based on Proposition 1, we propose a sorting-based algo-
) (that
rithm for computing
is, m = 1). The steps involve transporting points which are
close to the decision boundary and have signiﬁcant con-
tributions towards improving fairness if prediction labels

(ˆPN ) for one-dimensional φ(
·

P

As in Section 3,

Pǫ(ˆPN ) is amenable to be studied via the

H0 : P

∈ Fǫ

against

H1 : P

6∈ Fǫ.

(8)

N

Testing Group Fairness via Optimal Transport Projections

are ﬂipped. The exact steps are described in Algorithm
1. Note that the algorithm requires only information on
, instead of the whole functional
.

[N ]
}
{C
structure of the classiﬁer
C

(xi), d(xi) : i

∈

f (0) and Σ1, i.e.,

ˆf (0) =

1
N h

Algorithm 1 Computing

{

d(xi),

1: Input: Data
2: Output: the optimal value
3: Let s
4: For i

[N ] C
i
∈
[N ], compute

C

(ˆPN ) for one-dimensional φ(
P
·
(xi), φ(ui, EˆPN [U ])
}
(ˆPN ).
(xi)φ(ui, EˆPN [U ]);

N
i=1.

P

)

d(xi)−

1(1

2

C

−

(xi))φ(ui, EˆPN [U ])sgn(s);

5: Sort t1, . . . , tN in descending order, where t(i) denotes
the i-th largest one and let d(i) be the corresponding
distance;

← −
∈
ti ←

P

s

;

|

← |

←

1 to T do

if t(i)d(i) < s then
s

6: Initialize V = 0 and let s
7: for i
8:
s
9:
else
10:
V
11:
end if
12:
13: end for
14: Output

V + t−

V /N .

(ˆPN )

←

←

−

P

←

t(i)d(i) and V

V + d(i);

←

1
(i) s and break;

K

[N ]
Xi
∈
K

(cid:18)
Φ(xi)
h

Φ(xi)
h

(cid:19)

, and

φ

ui, EˆPN [U ]

φ

ui, EˆPN [U ]

⊤

(cid:1)

(cid:0)

(cid:1)

,

(cid:19)

(cid:0)

K

Φ(xi)
h

ˆΣ1 = Xi

[N ]

∈

(cid:18)

(cid:18)

(cid:19)

[N ]
Xi
∈
where h > 0 is the bandwidth parameter, and K(
) is a
·
kernel function that is symmetric and integrates to one. By
combining the above two estimates, an empirical estimate
for S, denoted by ˆS is computed via,

1
N h

K

[N ]
Xi
∈

(cid:18)

Φ(xi)
h

(cid:19)

φ

ui, EˆPN [U ]

φ

ui, EˆPN [U ]

⊤ .

(cid:1)

(cid:0)

(cid:0)

(cid:1)
1/5),
Under some mild conditions, by choosing h = O(N −
2/5); see, for example, Härdle
we have
(1990, Theorem 4.2.1) and Tsybakov (2008, Proposition
1.7). By combining the empirical covariance estimator for
Σ, we obtain a quantile estimate ˆη1

= O(N −

α.

ˆS

−

S

k

k

−

6. Numerical Experiments

With the output
P
tation of the test statistic sN = N
To obtain
gorithm 1 as in

(ˆPN ) returned by Algorithm 1, compu-
(ˆPN ) is immediate.
× P
Pǫ(ˆPN ) similarly, one may modify Line 3 in Al-

Our experiments use the following three datasets: Arrhyth-
mia (Dua & Graff, 2017), COMPAS (MultiMedia LLC,
2016) and Drug (Fehrman et al., 2017). The details of the
datasets are provided in Appendix B.2.

+

(xi)φ(ui, EˆPN [U ])

N ǫ



−

.

C

s

← − 

[N ]
Xi
∈

∈




With sgn(0) = 0 assigning ti = 0 for all i
[N ] in Step
4, we take 0/0 = 0 in Line 11 in Algorithm 1 in order to
Pǫ(ˆPN ). It is easy to see that the time complexity
obtain
of Algorithm 1 is the same of the time complexity of the
sorting algorithm, which is generally O(N log N ). For in-
stances where m > 1, one may solve either problem (7)
or problem (9) with a standard linear program solver to
Pǫ(ˆPN ), which is
obtain the respective values
P
also solvable in polynomial time. Therefore, our hypothe-
sis test is more computationally efﬁcient than the test pro-
posed in Taskesen et al. (2021), which requires solving a
non-convex optimization problem.

(ˆPN ) or

5.2. Computations of the Quantile of the Limiting

Distributions

We use the conditional density estimator and the Nadaraya-
Watson estimator (Tsybakov, 2008, Section 1) to estimate

In the ﬁrst experiment, we test the fairness of the Tikhonov-
regularized logistic and SVM classiﬁers by the equal op-
portunity criterion. We randomly split 70%-30% of the
data as a train-test set. Figure 1 reports the test statistics,
fairness rejection threshold, and the accuracy of the clas-
siﬁer. Figure 1(a) shows the result of regularized logistics
classiﬁer in COMPAS dataset, while Figure 1(b) shows the
result of SVM classiﬁer in the Drug dataset. We observe
that a strong regularization only reduces the test statistics
very mildly, and the Wasserstein projection tests suggest
we reject the fair null hypothesis even when the regular-
ization power is sufﬁciently large, which presents a dif-
ferent phenomenon from the probabilistic fairness test re-
sults shown in Taskesen et al. (2021). We here provide a
heuristic explanation for this difference. Consider a lo-
.
gistic classiﬁer
}
Since regularization usually induces shrinkage, the regular-
ized classiﬁer could be approximated by
1/(1 +
, and large regularization power cor-
exp(
0.5
≥
(x) no matter
responds to small ε. Note that
how small ε > 0 is. However, for the probabilistic notion,
Cε will output approximately equal probabilities for both
labels, which tends to be probabilistic fair when ε is very

−
≥
Cε(x) = I
{

(x) = I
{

Cε(x) =

1/(1 + exp(

εθ⊤x))

θ⊤x))

0.5

−

C

C

}

Testing Group Fairness via Optimal Transport Projections

Table1. Rejection percentage of the Naive SVM and the method
in Donini et al. (2018) at the signiﬁcance level α = 0.05 accord-
ing to the equal opportunity criterion using our test.

Arrhythmia COMPAS

Drug

Naive SVM
Donini et al. (2018)

68.4%
11.6%

100%
16.6%

30.1%
21.6%

Table2. Rejection percentage of the Naive SVM and the method
in Donini et al. (2018) at the signiﬁcance level α = 0.05 accord-
ing to the equalized odds criterion using our test.

Arrhythmia COMPAS

Drug

Naive SVM
Donini et al. (2018)

75.1%
13.7%

100%
21.7%

30.5%
17.2%

Table3. Rejection percentage of the Naive SVM and the method
in Donini et al. (2018) at the signiﬁcance level α = 0.05 accord-
ing to the equal opportunity criterion using Welch’s test.

Arrhythmia COMPAS

Drug

Naive SVM
Donini et al. (2018)

76.1%
14.0%

100%
16.1%

35.5%
23.0%

More experiments are conducted in Appendix B.1 to em-
pirically validate the convergence result in Theorem 1 and
our proposed hypothesis test method.

Acknowledgement

Material in this paper is based upon work supported by
the Air Force Ofﬁce of Scientiﬁc Research under award
number FA9550-20-1-0397. Additional support is grate-
fully acknowledged from NSF grants 1915967, 1820942,
and 1838576 and Singapore Ministry of Education’s AcRF
grant MOE2019-T2-2-163.

small. The experiment thus demonstrates probabilistic fair-
ness does not imply the exact fairness in general.

e
u
a
v

l

c
i
t
s
i
t
a
t
S

10

8

6

4

2

0.670

0.665

0.660

0.655

0.650

0.645

0.640

y
c
a
r
u
c
c
A

0

20

40

60

80

100

(a) Regularized logistics classiﬁer in COMPAS

e
u
a
v

l

c
i
t
s
i
t
a
t
S

1.050

1.025

1.000

0.975

0.950

0.925

0.900

y
c
a
r
u
c
c
A

0.814

0.812

0.810

0.808

0.806

0.804

0.802

0.800

5

10

15

(b) SVM classiﬁer in Drug dataset

Figure1. Test statistics and accuracy of regularized classiﬁers on
test data with a rejection threshold. The green line is the test statis-
tics; the pink dashed line is the rejection threshold at the signiﬁ-
cance level α = 0.05; the purple line is the test accuracy; λ de-
notes the regularization parameter, where larger λ means stronger
regularization power.

In the second experiment, we compare a fair algorithm
proposed in Donini et al. (2018) with a naive SVM clas-
siﬁer (parametrized by the ridge regularization λ) in three
datasets: Arrhythmia, COMPAS and Drug. We randomly
split 70%-30% of the data as a train-test set and we repli-
cate this procedure 1,000 times. We will test the fairness
in terms of the equal opportunity and equalized odds cri-
teria, and for the equal opportunity criteria, we will further
show results using Welch’s test (Welch’s test is not applica-
ble for multi-dimensional equalized odds criteria). Tables
1 and 2 show a rejection percentage of the naive SVM and
the method in Donini et al. (2018) at the signiﬁcance level
α = 0.05 in those 1,000 replications using our test accord-
ing to the equal opportunity and equalized odds criteria,
respectively. Table 3 shows the test results using Welch’s
test according to the equal opportunity criterion. Our test
results demonstrate that the method in Donini et al. (2018)
has a signiﬁcantly lower rejection rate, which means it is
substantially more fair than the naive method.

Testing Group Fairness via Optimal Transport Projections

Appendix A. Proofs

We prove Theorems 1 and 2 under a more general assumption below.
Assumption 2′ (Continuous conditional measure). For the case where supp(U ) is potentially an inﬁnite set, the cost
function c is decomposable as

and the following conditions are satisﬁed:

c ((x, u), (x′, u′)) = ¯c(x, x′) +

∞ · k

u

u′

,

k

−

a) the moments EP

U

b) for z such that

k

k

2
2 and EP

2
2, EP
φ(U, µ)
k
k2 < v, the derivative φz(
·

µ

k

k
z

k

−

φz(U, µ)

k2 are ﬁnite.

) satisﬁes,

where EP[M (U )] < +

.
∞

φz(u, z)
k

−

φz(u, µ)

k2 ≤

M (u)

z
k

µ

k2 ,

−

(A.1)

Φ(X) = t converges in terms of the type 1-Wasserstein

c) The (regular) conditional probability measure ν t of φ(U, µ)
|
R with P(Φ(X)
0: i.e., there exist a set B

distance as t

→

⊂
lim
0
t
→

W1 (νt, ν0) 1

B) = 1 and ε0 > 0 such that

B

}

∈

= 0

∈
t

{

and supt
φ(U, µ)
B EP[
k
∈
function being a metric.

k

2+ǫ0
2

Φ(X)=t] is ﬁnite, where type 1-Wasserstein distance W1(
·

|

,

) is Wc(
·

·

,

·

) with the cost

Remark 1. If supp(U ) is a ﬁnite set, and U is completely dependent on (X, Y ), the simpler Assumption 2 is equivalent
to Assumption 2′.

Appendix A.1. Proofs of Section 3

Proof of Proposition 1. Since the cost to move U is +
measure Q such that

, we have EQ[U ] = EˆPN [U ]. Then, consider any probability

∞

and let π be the optimal coupling between ˆPN and Q. Because ˆPN is the empirical measure, the coupling π can be written
as π = 1
N

δ(xi,ui). For any value ε > 0, construct now the measure

[N ] πi ⊗

i
∈

EQ[

(X)φ(U, EˆPN [U ])] = 0,

C

P

where the mass pi is set to

Qε =

1
N

(1

−

[N ]
Xi
∈

pi)δ(xi,ui) + piδ(xε

i ,ui),

(A.2)

ZX1−C(xi )
i is an ε-optimizer of the problem inf x′

and xε

pi =

πi(dx) = πi(X1

−C

(xi))

∈

[0, 1]

[N ]

i
∀

∈

X1−C(x) c(xi, x′). Then, it is easy to see that

∈

EQε[

C

(X)φ(U, EˆPN [U ])] = EQ[

(X)φ(U, EˆPN [U ])] = 0

C

and that

EQε [

(X)φ(U, EˆPN [U ])]

=

=

C
1
N 



1
N 



pi)
C

−

(1

[N ]
Xi
∈

(xi)φ(ui, EˆPN [U ]) + pi(1

(xi))φ(ui, EˆPN [U ])



− C

2

C

−

(1

[N ]
Xi
∈

(xi))φ(ui, EˆPN [U ])pi +

C

[N ]
Xi
∈

(xi)φ(ui, EˆPN [U ])


.





Testing Group Fairness via Optimal Transport Projections

Since d(xi)
xik ≤ k
Since ε can be chosen arbitrarily, this implies that

+ ε for any x

xik

≤ k

xε
i −

−

x

X1

−C

∈

(xi), this implies that 1
N

[N ] pid(xi)

≤

W (Q, ˆPN ) + ε.

i
∈

P

(ˆPN )

P

≥

s.t.

p

min

pid(xi)

1
N

Xi
[N ]
∈
[0, 1]N
∈
2
(1

−

[N ]
Xi
∈





pi}

(xi))φ(ui, EˆPN [U ])pi =

C

−

C

[N ]
Xi
∈

(xi)φ(ui, EˆPN [U ]).

(A.3)

N
i=1 satisfying the constraints in the linear programming (A.3), we can construct the measure
On the other hand, for any
Qε according to (A.2). Since Qε is a feasible solution of the primal problem (5), we have the other direction of the
inequality.

{

Proof of Proposition 2. Case 1: d(xi) < +

[N ]. The primal problem has a feasible solution (pi = 0 if
(X) = 0; pi = 1, otherwise) and is bounded, thus it has an optimal solution and the strong duality holds. By the strong

for i

∞

∈

C
duality, we have

(ˆPN ) =

P






Then, we have

max

1
N 

s.t. αi ≤

αi −

Xi
[N ]
∈
0
i
∀
2
(1
−

αi + γ⊤

(xi)φ(ui, EˆPN [U ])

C




[N ]
∈
(xi)) γ⊤φ(ui, EˆPN [U ])
C

≤


d(xi)

[N ].

i
∀

∈

which gives the desired results.

(cid:0)

(cid:1)

αi =

d(xi) + (1

2

C

−

(xi)) γ⊤φ(ui, EˆPN [U ])

− ,

Case 2:

i
∃

∈

[N ] such that d(xi) = +

. The primal problem is equivalent to

∞
1
N

min

s.t.

pid(xi),

Xi
[N ]
∈
[0, 1]N ,
∈

p
pi = 0 for d(xi) = +
2

(1

∞

−

C

[N ]
Xi
∈

(ˆPN ) =

P






with the convention that if the problem is infeasible, the optimal value of the minimization problem is +
dual problem

(xi))φ(ui, EˆPN [U ])pi =

(xi)φ(ui, EˆPN [U ]),

−

C

[N ]
Xi
∈

max

1
N 

s.t. αi ≤

αi −

αi + γ⊤

C

(xi)φ(ui, EˆPN [U ])




∞

Xd(xi)<+
0
(1

−

C

for d(xi) < +
2

∞
(xi)) γ⊤φ(ui, EˆPN [U ])


d(xi)

for d(xi) < +

.
∞

≤

(A.4)

(A.5)

. We have the

∞

(A.6)

Since the problem (A.6) is also feasible, if it is bounded, then the strong duality holds. If the problem (A.6) is unbounded,
. Finally, because
the primal problem (A.5) is infeasible, which means the primal and the dual both have optimal value +

∞

d(xi) + (1

2

C

−

(xi)) γ⊤φ(ui, EˆPN [U ])

− = 0 for d(xi) = +

,
∞

we have the optimal value of problem (A.6) equals to

(cid:0)

(ˆPN ).

D

(cid:1)

Proof of Lemma 1. Since XC
density in R for every x

∈

(cid:12)
(cid:12)

XD = x has positive density in Rd2 for every x

D. Therefore, θ⊤X has a density

D, we have θ⊤C XC

XD = x has positive

∈

(cid:12)
(cid:12)

fθ⊤X (x) =

D
Xv
∈

pvf

θ⊤
C XC

v

(x

−

θ⊤Dv) > 0,

(cid:12)
(cid:12)

Testing Group Fairness via Optimal Transport Projections

where pv = P (XD = v) and f

θ⊤
C XC

v

(
·

) denotes the conditional density of θ⊤C XC

XD = x.

Further, let w = ℓ−

1(τ ). For the cost function ¯c(
·

(cid:12)
(cid:12)

) given by (3a), we have by Hölder inequality

(cid:12)
(cid:12)

d(x) = inf

θ⊤x′=w k

x

x′

k

−

=

θ

1
−
∗ |

k

k

θ⊤x

w

.

|

−

Therefore, PΦ has a continuous density f (
·
) given by (4), when d(x) < δ, we have

fθ⊤X (
k

) =

k∗

k

θ

θ

k∗ × ·

+ w) with f (0) > 0.

For the cost function ¯c(
·
¯c(x, x′) =

d(x) = inf

θ⊤x′=w

xD =x′

D,θ⊤

inf
C x′=w

θ⊤
D xD

−

¯c(x, x′) =

inf
θ⊤
C x′=w
−

xC −

x′C k

=

θC k

1
−
∗ |

k

θ⊤x

w

.

|

−

D xD k
θ⊤

The last equality is again due to Hölder inequality. Therefore, PΦ has a continuous density f (
·
+ w) with f (0) > 0, which completes the proof.

·

) =

θC k∗

fθ⊤X (
k

θC k∗ ×

k

Lemmas A2 and A3 are useful for the proof of Theorem 1, whose proofs are presented in Appendix A.3.
Lemma A2. Suppose Assumption 2′ is enforced. Then, we have

√N

EˆPN

φ

U, EˆPN [U ]

(X)

C

EP [φ(U, µ)
C

−

(X)]

⇒ N

(0, cov (EP [φz(U, µ)
C

(X)] U + φ(U, µ)

C

(X))).

Lemma A3. Suppose Assumption 1 and 2′ are enforced. Then, we have
(cid:3)

(cid:1)

(cid:0)

(cid:1)

(cid:0)

(cid:2)

√N EˆPN

γ⊤φ

U, EˆPN [U ]

−

(cid:20)(cid:16)

uniformly over

(cid:0)
k2 ≤
We are now ready to prove Theorem 1.

B.

γ

k

(cid:1)

+ √N d(X)
(cid:17)

−

C

(X)

(cid:21)

p

−→ −

1
2

f (0)EP

γ⊤φ (U, µ)

h (cid:0)

γ⊤φ (U, µ)

0

}

≥

2

I
{

(cid:1)

d(X) = 0

,

i

(cid:12)
(cid:12)
(cid:12)

Proof of Theorem 1. Recall that

(ˆPN ) = max
Rm

γ

∈

D

1
N 


d(xi) + (1

[N ]
Xi
∈

(cid:0)

2

C

−

(xi)) γ⊤φ(ui, EˆPN [U ])

− + γ⊤

C

(cid:1)

= max
γ

Rm 


∈

1

N

[N ]
Xi
∈

γ⊤

C

(xi)φ(ui, EˆPN [U ]) +

(xi)φ(ui, EˆPN [U ])






1
N



d(xi)

[N ]
Xi
∈

(cid:0)

γ⊤φ(ui, EˆPN [U ])

−

C

−

(xi) +

d(xi) + γ⊤φ(ui, EˆPN [U ])

− (1

(cid:1)

(cid:0)

(cid:1)

− C

.

(xi))






We ﬁrst rescale γ

γ√N and thus

N

←
(ˆPN )
D
= √N max
Rm

γ

∈

γ⊤EˆPN

φ

U, EˆPN [U ]

(X)

+

C

(cid:8)

√N d(X)

EˆPN

(cid:1)

(cid:0)

(cid:2)
γ⊤φ(U, EˆPN [U ])
(cid:17)

−

(cid:3)

C

(X) +

√N d(X) + γ⊤φ(U, EˆPN [U ])
(cid:17)
(cid:16)

− (1

− C

(X))

.

(cid:21)(cid:27)

−

(cid:20)(cid:16)
To ease the notation, we denote λi = φ

1
√N
1
2
1
2

−

i=1 (cid:16)
X
f (0)EP

f (0)EP

−→ −

p

=

ui, EˆPN [U ]

. By Lemma A3, we have

(cid:0)

N

(cid:1)

−

γ⊤λi + N 1/2d(xi)
(cid:17)

− (1

(xi))

− C

γ⊤φ (U, µ)

2

h (cid:0)

(cid:1)
2
γ⊤φ (U, µ)

h (cid:0)

(cid:1)

γ⊤φ (U, µ)

γ⊤φ (U, µ)

I
{

I
{

d(X) = 0

d(X) = 0

,

i

i

0

0

}

}

≥

≥

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Testing Group Fairness via Optimal Transport Projections

and similarly, we have

1
√N

N

γ⊤λi + N 1/2d(xi)
(cid:17)

i=1 (cid:16)
X

− (1

(xi))

− C

p

−→ −

1
2

Therefore, we have

f (0)EP

γ⊤φ (U, µ)

h (cid:0)

γ⊤φ (U, µ) < 0

2

I
{

(cid:1)

d(X) = 0

.

i

}

(cid:12)
(cid:12)
(cid:12)

√N EˆPN

√N d(X)

(cid:20)(cid:16)
f (0)EP

1
2

p

−→ −

We denote

−

γ⊤φ(U, EˆPN [U ])
(cid:17)
d(X) = 0

2

γ⊤φ (U, µ)

−

C

.

i

h (cid:0)

(cid:1)

(cid:12)
(cid:12)
(cid:12)

(X) +

√N d(X) + γ⊤φ(U, EˆPN [U ])
(cid:17)
(cid:16)

− (1

− C

(X))

(cid:21)

VN = √N EˆPN
N

φ

U, EˆPN [U ]

(X)

, and

C

MN (γ) =

1
√N

(cid:2)

(cid:0)
−

(cid:1)
(cid:3)
γ⊤λi + N 1/2d(xi)
(cid:17)

−

(X)
}

C

+

γ⊤λi + N 1/2d(xi)
(cid:17)
(cid:16)

− (1

− C

(X))

.

(cid:21)

i=1 (cid:20)(cid:16)
X

To proceed, we rely on the following lemma.

Lemma A4. Suppose Assumption 1 is enforced. Then, for every ε > 0, there exists N0 > 0 and b
all N

N0,

≥

(0,

∈

) such that for

∞

P

sup
k2>b
γ

k

(cid:8)

γ⊤VN + MN (γ)

> 0

(cid:9)

ε.

! ≤

The proof of Lemma A4 is furnished in Appendix A.3. Notice that
when N

N0,

≥

(ˆPN )

D

≥

0 (choosing γ = 0), Lemma A4 implies that

P

N

D

(

By Lemmas A2 and A3, we have

(ˆPN ) = sup
k2≤

γ

k

γ⊤VN + MN (γ)

b

(cid:8)

(cid:9)

ε.

1

−

) ≥

sup
γ
k2≤

b

k

γ⊤VN + MN (γ)

(cid:8)

(cid:9)

⇒

=

sup
γ
k2≤

k

sup
γ
k2≤

k

γ⊤V

b (cid:26)

γ⊤V

b (cid:26)

1
2

1
2

−

−

f (0)E

γ⊤φ (U, µ)

γ⊤Sγ

h (cid:0)
,

(cid:27)

d(X) = 0

i(cid:27)

2

(cid:1)

(cid:12)
(cid:12)
(cid:12)

where

and V is normally distributed with mean zero and covariance matrix

h

(cid:12)
(cid:12)
(cid:12)

i

S = f (0)EP

φ (U, µ) φ (U, µ)⊤

d(X) = 0

,

By the arbitrariness of ε, we have the desired result:

cov (EP [φz(U, µ)
C

(X)] U + φ(U, µ)

C

(X)) .

N

× D

(ˆPN )

⇒

γ⊤V

sup
γ (cid:26)

1
2

−

γ⊤Sγ

.

(cid:27)

This completes the proof.

 
Testing Group Fairness via Optimal Transport Projections

Appendix A.2. Proofs of Section 4

The proofs of Propositions 4 and 5 are not presented because they follow the same lines as the proofs of Propositions 1 and
2.

Proof of Theorem 2. Let ǫ∗ = EP [
we have

C

N

Dǫ(ˆPN )

(X)φ (U, EP[U ])]. By following the similar arguments with the proof of Theorem 1,

(xi))φ(ui, EˆPN [U ]) +

2

C

−

−

(1

γ⊤

γ⊤ǫ +

Xi
[N ]
∈
U, EˆPN [U ]

1
N 

φ

(cid:0)
(cid:8)
γ⊤φ(U, EˆPN [U ]) + √N d(X)
(cid:17)

EˆPN

−
γ⊤VN + √N γ⊤ (ǫ∗

C

(cid:1)

(cid:0)

(cid:2)

γ

= N sup
Rm
+



= √N sup

Rm
+

∈

γ

∈

EˆPN

(cid:20)(cid:16)

= sup
Rm
γ
+ n

∈

ǫ) + MN (γ)

.

−

o

(X)

ǫ

+

} −

(cid:3)(cid:1)
(X) +

−

C

C

[N ]
Xi
∈

(xi)φ(ui, EˆPN [U ])











γ⊤φ(U, EˆPN [U ]) + √N d(X)
(cid:17)
(cid:16)

− (1

− C

(X))

(cid:21)(cid:27)

Similarly, we still have

γ⊤VN + MN (γ)

γ⊤V

⇒

1
2

−

γ⊤Sγ,

uniformly over

γ : γ

Rm
+ ,

γ
k

k2 ≤

∈

B

. Therefore, we must enforce γ⊤ (ǫ∗

ǫ) = 0 here. Then, we have

−

(cid:8)

N

Dǫ(ˆPN )

⇒

γ

∈

(cid:9)
max
Rm
+ ,γ⊤(ǫ∗

γ⊤V

1
2

−

ǫ)=0

−

(cid:26)

γ⊤Sγ

γ⊤V

max
Rm
γ
+ (cid:26)
∈

(cid:22)

(cid:27)

1
2

−

γ⊤Sγ

.

(cid:27)

This completes the proof.

Appendix A.3. Proofs of Technical Results

Proof of Lemma A2. By adding and subtracting the term EˆPN [φ(U, µ)
C

(X)], we ﬁnd

EˆPN
= EˆPN

φ(U, EˆPN [U ])
C
φ(U, EˆPN [U ])
(cid:2)
C
(cid:2)

(X)
−
(X)
(cid:3)
−

EP [φ(U, µ)
C
(X)
φ(U, µ)

C

(X)]
+ EˆPN [φ(U, µ)
C

(X)]

EP [φ(U, µ)
C

−

(X)]

(A.7)

Under Assumption 2′ and the fundamental theorem of calculus, the ﬁrst term in the right-hand side of (A.7) becomes

(cid:3)

EˆPN

φ(U, EˆPN [U ])
C

(X)

−

φ(U, µ)

C

(X)

= EˆPN

(cid:2)

Thanks to Assumption 2′, we have that

(cid:3)

1

0
(cid:20)Z

φz

U, µ + t

EˆPN [U ]

µ

−

EˆPN [U ]

µ

−

C

(cid:0)

(cid:0)

(cid:1)(cid:1) (cid:0)

(cid:1)

(X)dt

.

(cid:21)

1

0

(cid:20)Z

EˆPN
(cid:13)
(cid:13)
(cid:13)
(cid:13)

EˆPN

−

0
(cid:20)Z
EˆPN [M (U )]

1
2

≤

φz

U, µ + t

EˆPN [U ]

µ

−

EˆPN [U ]

µ

−

C

(X)dt

(cid:21)

(cid:0)

1

(cid:0)
φz (U, µ)

(cid:1)(cid:1) (cid:0)

EˆPN [ψ(U )]

µ

−

C

(X)dt

(cid:1)

2
(cid:21)(cid:13)
(cid:13)
(cid:13)
(cid:13)

EˆPN [U ]

(cid:0)
µ

−

(cid:1)

2
2 ,
(cid:13)
(cid:13)

(cid:13)
(cid:13)

whenever

EˆPN [U ]

µ

−

(cid:13)
(cid:13)

2 < εµ. Then, notice that we have
(cid:13)
(cid:13)

√N EˆPN [M (U )]

1
2

lim
N
→∞

EˆPN [ψ(U )]

µ

−

2
2 = 0 almost surely,
(cid:13)
(cid:13)

(A.8)

(cid:13)
(cid:13)

Testing Group Fairness via Optimal Transport Projections

and

EˆPN

1

0
(cid:20)Z

φz (U, µ)

EˆPN [U ]

µ

−

C

(X)dt

(cid:0)

(cid:1)

= EˆPN [φz (U, µ)

(cid:21)

= (EP [φz (U, µ)

(X)]

EˆPN [U ]

C
(cid:0)
(X)] + op (1))

C

By multiplying √N to both sides of equation (A.7), we have

µ

−
}
(cid:1)
EˆPN [ψ(U )]

(cid:0)

µ

.

−

(cid:1)

√N

φ

(cid:0)

EˆPN
= √N (EP [φz (U, µ)
= √N EˆPN
h
(0, Σ),

EP

(cid:0)

(cid:2)

(cid:2)

⇒ N

C

φz (U, µ)

U, EˆPN [U ]

(X)

−
C
(X)] + op (1))

(cid:1)

(cid:3)

EP [φ(U, µ)
C
µ

EˆPN [U ]

−

(X)]

+ EˆPN [φ(U, µ)
C

(cid:1)

(X)

(U

C

(cid:1)
(cid:0)
µ) + φ(U, µ)
C

−

(X)

−

EP

φ(U, µ)

C

(cid:3)

(cid:2)

(cid:3)i

(X)]

−
(X)

(X)] + op(1)

EP [φ(U, µ)
C

+ op(1)

where Σ is the covariance matrix of EP [φz(U, µ)
C

(X)] U + φ(U, µ)

(X), namely

C
(X)] U + φ(U, µ)

(X)) .

C

Σ = cov (EP [φz(U, µ)
C

This completes the proof.

Proof of Lemma A3. Step 1: we ﬁrst show

√N EˆPN

−

(cid:20)(cid:16)

uniformly over

γ⊤φ

U, EˆPN [U ]

+ √N d(X)

−

(cid:0)
k2 ≤

γ
k

(cid:1)
B. When

EˆPN [U ]

√N EˆPN

(cid:13)
(cid:13)
U, EˆPN [U ]

γ⊤φ

(X)

C

−

√N EˆPN

(cid:17)
µ

(cid:21)
2 < εµ, we have
(cid:13)
(cid:13)
+ √N d(X)

(X)

−

−

1/2

N −

≤

where the events

−

(cid:20)(cid:16)
γ

k2

k

N

(cid:0)
φ

(cid:2)(cid:13)
(cid:13)

i=1
X
(cid:0)
Ei are deﬁned by
γ
Ei =

(cid:1)

(cid:17)

ui, EˆPN [ψ(U )]

φ (ui, µ)

−

(cid:1)

{Ei}

,

(cid:3)

2 I
(cid:13)
(cid:13)

γ⊤φ (U, µ) + √N d(X)

−

(cid:17)

−

(cid:20)(cid:16)

C

(X)

(cid:21)

p
−→

0,

C

√N EˆPN

−

−

(cid:20)(cid:16)

γ⊤φ (U, µ) + √N d(X)

−

(cid:17)

C

(X)

(cid:21)

(cid:21)

{ k
By a similar derivation with the proof of Lemma A2, we have

k2 (
k

φ (ui, µ)

k2 + (
k

φz (ui, µ)

k2 + M (ui)εµ) εµ)

√N d(xi)
}

.

≥

1/2

N −

γ

k2

k

N

N

φ
i=1 h(cid:13)
X
(cid:13)
1
φz

0
i=1 (cid:20)Z
X
√N

(cid:0)
EˆPN [U ]

(cid:0)

−

µ

(cid:1)

= k

γ
k2
√N

≤ k

γ

k2

ui, EˆPN [U ]

(cid:0)

−

(cid:1)

φ (ui, µ)

ui, µ + t

EˆPN [U ]

µ

−

{Ei}

2 I
−
(cid:13)
(cid:13)
EˆPN [U ]

(cid:0)

EˆPN [φz (U, µ) I

(cid:1)(cid:1) (cid:0)
{Ei}

] +

1
γ
2 k

k2

i

−

µ

dt

(cid:1)

(cid:1)

I
{Ei}

(cid:21)

√N EˆPN [M (U )]

Since I

{Ei} →

0 almost surely and EP[φz(U, µ)] < +

(cid:0)

, we have

∞

uniformly over

γ
k

k2 ≤

B. By combining

(cid:0)

γ

k

k2

√N

EˆPN [U ]

EˆPN [φz (U, µ) I

{Ei}

]

p
−→

0,

µ

−

(cid:1)

1
γ
2 k

k2

√N EˆPN [M (U )]

EˆPN [U ]

uniformly over

γ
k

k2 ≤

B, we ﬁnish step 1.

(cid:13)
(cid:13)

0 almost surely,

µ

−

2
2 →

(cid:13)
(cid:13)

EˆPN [U ]

µ

−

(cid:13)
(cid:13)

2
2 .
(cid:13)
(cid:13)

Testing Group Fairness via Optimal Transport Projections

Step 2: We claim that

√N EP

−

(cid:20)(cid:16)

γ⊤φ (U, µ) + √N d(X)

−

(cid:17)

1
2

−→ −

C

(X)

(cid:21)

Notice that for any c > 0, we have

f (0)E

γ⊤φ (U, µ)

2

I
{

γ⊤φ (U, µ)

0

}

≥

(cid:1)

d(X) = 0

.

i

(cid:12)
(cid:12)
(cid:12)

h (cid:0)

(X)

√N EP

−

γ⊤φ (U, µ) + √N d(X)

−

(cid:17)

C

(cid:20)(cid:16)
+
∞

EP

0
Z

c/√N

= √N

= √N

0
Z
+√N

+

∞

c/√N

Z

γ⊤φ (U, µ) + √N d(X)

−

(cid:20) (cid:16)
EP

−

(cid:20) (cid:16)

EP

−

(cid:20) (cid:16)

(cid:17)
γ⊤φ (U, µ) + √N d(X)

(cid:12)
(cid:12)
(cid:12)
−
(cid:12)

(cid:17)
γ⊤φ (U, µ) + √N d(X)

(cid:12)
(cid:12)
(cid:12)
−
(cid:12)

(cid:17)

(cid:21)
−

d(X) (2

(X)

C

−

1) = t

dPΦ(t).

(cid:21)

Φ(X) = t

dPΦ(t)

(cid:21)

Φ(X) = t

dPΦ(t)

(cid:21)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(A.9)

(A.10)

We ﬁrst analyze the ﬁrst term in (A.9). By Assumption 1.a), when N is sufﬁcient large such that c/√N < v, we have

√N

= √N

c/√N

c/√N

0
Z

0
Z

EP

EP

−

(cid:20) (cid:16)

−

(cid:20) (cid:16)

γ⊤φ (U, µ) + √N d(X)

−

Φ(X) = t

dPΦ(t)

(cid:17)
γ⊤φ (U, µ) + √N d(X)
(cid:17)

−

(cid:21)

Φ(X) = t

f (t)dt.

(cid:21)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

By changing of the variable s = √N t , we have

√N

c/√N

EP

−

γ⊤φ (U, µ) + √N dB(X)
(cid:17)
1/2s

Φ(X) = N −

−

(cid:20) (cid:16)
γ⊤φ (U, µ) + s

−

Φ(X) = t

f (t)dt

(cid:12)
(cid:12)
(cid:12)
f (N −
(cid:12)

(cid:21)

1/2s)ds

c

0
Z
EP

=

0
Z

−

(cid:12)
(cid:12)
(cid:12)
By Assumption 1.c), we have for any ε > 0, any 0 < c < +

h (cid:0)

(cid:1)

i

, there exists N0, such that for N > N0 and s

c,

≤

∞

EP

γ⊤φ (U, µ) + s

−

Φ(X) = N −

(cid:12)
(cid:12)
(cid:12)
≤ k

≤ k

γ

γ

−

h (cid:0)
k∗

EP

φ (U, µ)

(cid:13)
(cid:13)
W1
(cid:13)

h
φ (U, µ)
|

k∗

(cid:16)

(cid:1)

(cid:12)
(cid:12)
Φ(X) = N −
(cid:12)

|
Φ(X) = N −

1/2s

−
1/2s, φ (U, µ)
|

i

Φ(X) = 0

(cid:13)
(cid:13)
(cid:13)

ε.

≤

(cid:17)

1/2s

EP

i

−
h (cid:0)
EP [ φ (U, µ)
|

γ⊤φ (U, µ) + s

−
Φ(X) = 0]

−

Φ(X) = 0

(cid:1)

(cid:12)
(cid:12)
(cid:12)

i(cid:12)
(cid:12)
(cid:12)

Therefore, by taking ε

0, we have

↓

c

EP

c

0
(cid:12)
Z
(cid:12)
(cid:12)
(cid:12)
−

0
Z

h (cid:0)
EP

γ⊤φ (U, µ) + s

h (X)

− I
{

−

τ

≥

}

(cid:1)
γ⊤φ (U, µ) + s

h (X)
− I
{

(cid:1)

−

h (cid:0)

Φ(X) = N −

1/2s

f (N −

1/2s)ds

(cid:12)
(cid:12)
(cid:12)
τ
}
≥

Φ(X) = 0

i
f (N −

1/2s)ds

(cid:12)
(cid:12)
(cid:12)

i

p
−→

0.

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Testing Group Fairness via Optimal Transport Projections

Then, the basic algebra and the mean value theorem for integrals give us

−

c

EP

0
Z

c

=

EP

0
Z

=f (ξ)EP

h (cid:0)

h (cid:0)

γ⊤φ (U, µ) + s

−

Φ(X) = 0

f (N −

1/2s)ds

(cid:1)
−

(cid:12)
(cid:12)
(cid:12)

d(X) = 0

i
f (N −

1/2s)ds

γ⊤φ (U, µ) + s

−
c

γ⊤φ (U, µ) + s

(cid:1)

(cid:12)
(cid:12)
(cid:12)

i
d(X) = 0

−

0
(cid:20)Z

−

(cid:16) (cid:0)
min(c,γ

⊤

φ(U,µ)

|

(cid:1)
Φ(X)=0)

(cid:12)
(cid:12)
(cid:12)

−

ds

(cid:21)

(cid:17)

γ⊤φ (U, µ) + s

=f (ξ)EP

"Z
0
f (0)EP

1
2

→ −

min
{

c, γ⊤φ (U, µ)
}

h

(cid:16)
x, 0

[0, N −

where ξ

}
We then deal with the second term in (A.9). Let

1/2c] and (x)+ = max
{

∈

.

(cid:0) (cid:0)
γ⊤φ (U, µ) +

I
{

(cid:1)

γ⊤φ (U, µ)

0

}

≥

d(X) = 0

ds

#

γ⊤φ (U, µ)

(cid:0)

(cid:12)
(cid:12)

γ⊤φ (U, µ)

(cid:1)
0

≥

}

c

−

I
{

+

(cid:17)

(cid:1)

d(X) = 0

,

(A.11)

(cid:12)
(cid:12)

i

For any c

≥

0, we have

+

∞

√N

Mγ = ess sup

0

t
≥

EP

γ⊤φ (U, µ)

h(cid:12)
(cid:12)

(cid:12)
(cid:12)

2+ǫ0

|

Φ(X)=t

.

i

EP

γ⊤φ (U, µ) + √N t

−

Φ(X) = t

dPΦ(t)

(cid:17)

(cid:12)
(cid:12)
(cid:12)
γ⊤φ (U, µ)
(cid:12)

(cid:21)

√N t

Φ(X) = t

dPΦ(t)

γ⊤φ (U, µ) I
{

−

(cid:20) (cid:16)
EP

c/√N
+

∞

c/√N

Z

+

∞

Z
√N

√N

c/√N (cid:18)
Z
ǫ0
−
√N

(cid:16)
√N

(cid:17)

−

ǫ0

(cid:16)

(cid:17)

≥ −

≥ −

≥ −

≥ −

1+ǫ0

h
1
√N t (cid:19)
1
t1+ǫ0
+

∞

+

∞

c/√N

Z

Mγ

c/√N

Z

EP

γ⊤φ (U, µ)

h(cid:0)
γ⊤φ (U, µ)

EP[

(cid:12)
1
(cid:12)
dPΦ(t).
t1+ǫ0

(cid:12)
(cid:12)

≥

(cid:12)
(cid:12)
2+ǫ0 I
(cid:12)
{

i

γ⊤φ (U, µ)

≥

(cid:1)
2+ǫ0

|

Φ(X) = t]dPΦ(t)

√N t

Φ(X) = t

dPΦ(t)

(cid:12)
(cid:12)
(cid:12)

i

We pick ε > 0 such that PΦ (
·

) has density in [0, ε]. Then, we have

√N
(cid:16)

= Mγ

ǫ0

−

Mγ

(cid:17)
√N

ǫ0

−

(cid:16)

(cid:18)(cid:16)

(cid:17)
√N

√N

Mγ

≤

= Mγ

 Z
ε
ǫ0

−

(cid:17)

−

ǫ0

(cid:18)(cid:16)

(cid:17)
0, we have

+

∞

c/√N

Z

1
t1+ǫ0

f (t)dt

+

∞

1
t1+ǫ0

f (t)dPΦ(t) +

1
t1+ǫ0

f (t)dt

!

ε

c/√N

Z
√N /c

ǫ0

f (ξ)

(cid:17)

(cid:19)

1
ε1+ǫ0
1
ε1+ǫ0

+

+

1
ǫ0
(cid:16)
1
cǫ0 ǫ0

ǫ0

−

√N

(cid:17)
f (ξ)

(cid:16)

,

(cid:19)

where ξ

∈

(c/√N , ε). By taking ε

↓

√N

lim inf
+
N
∞
→

+

∞

c/√N

Z

EP

−

(cid:20) (cid:16)

γ⊤φ (U, µ) + √N t

−

Φ(X) = t

dPΦ(t)

Mγf (0)
cǫ0 ǫ0

.

≥ −

(cid:17)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:21)

Finally, by taking c

+

↑

, we conclude step 2.

∞

Testing Group Fairness via Optimal Transport Projections

Step 3: We then apply weak law of triangular arrays Durrett (2019, Theorem 2.2.11). We need to check

N

E

For condition (A.12a), we have

×

P

h

−
"(cid:18)(cid:16)

γ⊤φ (U, µ) + √N d(X)

(X) > √N

−

−

(cid:16)

(cid:16)
γ⊤φ (U, µ) + √N d(X)

2

−

C

(cid:17)
(X)

C

0.

# →

(cid:19)

(cid:17)

0, and

→

(cid:17)i

(A.12a)

(A.12b)

N P

N P

E

−

(cid:18)

−

(cid:16)

γ⊤φ (U, µ) + √N d(xi)
(cid:17)

(X) > √N

−

C

(cid:19)

≤

≤

γ⊤φ (U, µ)

√N

≥
2+ǫ0

(cid:16)
γ⊤φ (U, µ)
ǫ0

√N

(cid:1)

h(cid:0)

(cid:17)

i

≤

Mγ

√N

0.

ǫ0 →

For condition (A.12b), we have

(cid:16)

(cid:17)

(cid:16)

(cid:17)

γ⊤φ (U, µ) + √N d(X)

−

2

−
"(cid:18)(cid:16)

E

E

≤

=

γ⊤φ (U, µ)

2

I
{

γ⊤φ (U, µ)

h(cid:0)
+

∞

EP

(cid:1)

γ⊤φ (U, µ)

2

I

(X)

C

#

(cid:19)

(cid:17)
√N d(X)
}

≥

i
√N t

γ⊤φ (U, µ)

≥

n

h(cid:0)

(cid:1)

) has density in [0, ε]. Then, we have

Φ(X) = t

dPΦ(t).

o(cid:12)
(cid:12)
(cid:12)

i

0
Z
We pick ε > 0 such that PΦ (
·
+

∞

EP

γ⊤φ (U, µ)

2

I

γ⊤φ (U, µ)

0
Z

ε

=

EP

h(cid:0)
γ⊤φ (U, µ)

2

(cid:1)
I

n
γ⊤φ (U, µ)

√N t

≥

√N t

Φ(X) = t

dPΦ(t)

o(cid:12)
(cid:12)
Φ(X) = t
(cid:12)

i
f (t)dt

0
Z

+

ε
Z

+

h(cid:0)
∞

EP

n
γ⊤φ (U, µ)

(cid:1)

2

I

γ⊤φ (U, µ)

≥

h(cid:0)

n

(cid:1)

≥

o(cid:12)
(cid:12)
√N t
(cid:12)

i
Φ(X) = t

o(cid:12)
(cid:12)
(cid:12)

dPΦ(t).

i

For the ﬁrst term (A.13), we have

ε

0
Z

EP

γ⊤φ (U, µ)

h(cid:0)

(cid:1)

2

I
{

γ⊤φ (U, µ)

≥

[0, ε]. For the second term (A.14) we have

√N t

Φ(X) = t

f (t)dt

(cid:12)
(cid:12)
(cid:12)

i

M 2/(2+ǫ0)

γ

εf (ξ),

≤

where ξ

∈

(A.13)

(A.14)

+

∞

EP

γ⊤φ (U, µ)

2

I
{

γ⊤φ (U, µ)

≥

√N t

Φ(X) = t

dPΦ(t)

(cid:1)
γ⊤φ (U, µ)

2+ǫ0 Φ(X) = t

ǫ0

(cid:1)
√N t

(cid:16)

(cid:17)

i

(cid:12)
(cid:12)
(cid:12)
dPΦ(t)

i

≤

≤

By taking ε

0, we have

↓

+

∞

0
Z

ε
Z

ε
Z

h(cid:0)

+

∞

EP

h(cid:0)

M0
√N ε

0.

ǫ0 →

(cid:16)

(cid:17)

EP

γ⊤φ (U, µ)

h(cid:0)

(cid:1)

2

I
{

γ⊤φ (U, µ)

≥

√N t

Φ(X) = t

dPΦ(t)

(cid:12)
(cid:12)
(cid:12)

i

0.

→

Testing Group Fairness via Optimal Transport Projections

We then apply Durrett (2019, Theorem 2.2.11) to obtain the weak law for each γ.

Step 4: We establish the Lipschitz continuity of

for

γ
k

k2 ≤

B, which ensures the tightness. For any γ1, γ2 satisfying

√N EP

−

(cid:20)(cid:16)

γ⊤φ (U, µ) + √N d(X)

−

C

(X)

(cid:21)
B and

γ2k2 ≤
k

B, we have

(cid:17)
γ1k2 ≤
k

√N

EP

√N

−

−

γ⊤1 φ (U, µ) + √N d(X)
(cid:17)
(X)
}

(cid:20)(cid:16)
γ⊤2 φ (U, µ) + √N d(X)
(cid:17)
(X)I

φ (U, µ)

C

C

−

EP
(cid:12)
(cid:12)
(cid:12)
(cid:12)
−
(cid:20)(cid:16)
γ1 −
k

γ2k2 k

k2 C

−

≤

(X)

(cid:21)

(cid:21)(cid:12)
(cid:12)
(cid:12)
φ (U, µ)
(cid:12)

B

n

k

k2 ≥

√N d(X)

.

o

By following similar lines with steps 2 and 3, we have

√N EˆPN

φ (U, µ)
k

k2 C

(X)I

B

φ (U, µ)
k

k2 ≥

√N d(X)

h

n

p
−→

f (0)EP

φ (U, µ)
k

k

2
2

oi

h

Then, by Billingsley (2013, Theorem 7.5), we have the desired uniform convergence result.

d(X) = 0

.

i

(cid:12)
(cid:12)
(cid:12)

Proof of Lemma A4. Due to E

φ (U, µ) φ (U, µ)⊤

d(X) = 0

E

min

(cid:12)
(cid:12)
c0, γ⊤φ (U, µ)
(cid:12)

i

≻

0, there exists δ > 0 and c0 ∈

(0, +

) such that

∞

γ⊤φ (U, µ)

> δ.

(cid:2)

(cid:8)

(cid:9) (cid:12)
(cid:12)

(cid:12)
(cid:3)
(cid:12)

k

inf
k2=1
γ
k2=1 E

E

γ⊤φ (U, µ)

> 0,

(cid:12)
(cid:12)
γ⊤φ (U, µ)

(cid:12)
(cid:12)
. For any ε > 0, there exists N1 > 0 and b′ < +

,
∞

(cid:12)
VN k2 ≥
P(
(cid:12)
k

(cid:12)
b′) < ε/2,
(cid:12)

for any N > N0. Recalling Lemma A3 and equation (A.11), there exists N0 > N1 such that

h

inf
k2=1
γ

k

for all

γ

k2 = 1. And

k

since the unit circle is compact. Let δ = inf
such that

γ

k

P

γ :

∃

γ
k

k2 = b such that MN (γ)

(cid:18)
for any N > N0. Then, we have

≥

Let b = 4b′/δ. We have

Notice that for any

γ

k2 > b,

k

1
4

≥ −

E

min

bc0,

γ⊤φ (U, µ)

γ⊤φ (U, µ)

< ε/2

(cid:2)

(cid:8)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:9) (cid:12)
(cid:12)

(cid:19)

(cid:12)
(cid:3)
(cid:12)

bc0,

γ⊤φ (U, µ)

γ⊤φ (U, µ)

min

E

inf
k2=b
γ
k
(cid:2)
b2
inf
E
k2=1
γ

k

(cid:8)
min

(cid:12)
c0,
(cid:12)

(cid:12)
γ⊤φ (U, µ)
(cid:12)

(cid:9) (cid:12)
(cid:12)

(cid:12)
(cid:3)
γ⊤φ (U, µ)
(cid:12)

> b2δ.

(cid:2)

(cid:8)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:9) (cid:12)
(cid:12)

(cid:3)

(cid:12)
(cid:12)

P

sup
k2=b
γ

k

MN (γ)

≥ −

bb′

!

< ε/2.

(A.15)

(A.16)

MN (γ)

γ
k2
k
b

≤

MN

b
γ
k

k2

γ

(cid:18)

≤

(cid:19)

γ
k2
k
b

sup
k2=b
γ

k

MN (γ).

By combining inequalities (A.15) and (A.16), we have

P (

γ :

∃

γ

k2 > b, such that MN (γ)

k

γ

≥ − k

k2 b′) < ε/2.

 
Testing Group Fairness via Optimal Transport Projections

P

sup
k2>b
γ

k

γ⊤VN + MN (γ)

> 0

(cid:8)

(cid:9)

!

k

P

sup
k2>b {k
γ
P(
VN k2 ≥
k
ε.

γ

k2 k

b′) + P (

> 0

VN k2 + MN (γ)
}
γ

γ :

∃

k

!
k2 > b, such that MN (γ)

γ

≥ − k

k2 b′)

Therefore,

≤

≤

≤

This completes the proof.

Appendix B. Additional Details for Numerical Experiments

Appendix B.1. Validation of the Hypothesis Test

In this section, we empirically validate the convergence result in Theorem 1 and our proposed hypothesis test method. we
use a simple logistic classiﬁer in the form

1

(x) = I

C




1 + exp

θ⊤x

τ

≥

.




−

(cid:16)
(cid:17)
. We denote w =



Then, the decision boundary is
example in Taskesen et al. (2021). Let

x : θ⊤x =

n

log

−


1
τ −

1

(cid:0)

(cid:1)o

log

−

1
τ −

1

. Then, we borrowed the

(cid:0)

(cid:1)

Moreover, conditioning on (A, Y ), the feature X follows a Gaussian distribution of the form

p11 = 0.4, p01 = 0.1, p10 = 0.4, p00 = 0.1.

A = 1, Y = 1
A = 0, Y = 1

A = 1, Y = 0

A = 0, Y = 0

X
X

X

X

|
|

|

|

∼N
∼N

∼N

∼N

−

([6, 0], [3.5, 0; 0, 5]),
2, 0], [5, 0; 0, 5]),
([

([6, 0], [3.5, 0; 0, 5]),

4, 0], [5, 0; 0, 5]).

([

−

The true distribution P is thus a mixture of Gaussian. A simple algebraic calculation indicates that a logistic classiﬁer with
) denotes the density
θ = (0, 1)⊤ and τ = 0.5 is fair with respect to the equal opportunity criterion in Example 1. Let ϕ(
·
of the standard normal distribution and we denote µay and Σay to be the conditional mean and variable deﬁned above,
respectively. For any θ, the density of θ⊤X becomes

θ⊤Σayθ

1/2

−

payϕ

θ⊤Σayθ

1/2

−

0,1
Xa,y
∈{

2 (cid:16)
}

(cid:17)

(cid:18)(cid:16)

(cid:17)

(cid:16)

θ⊤x

−

θ⊤µay

.

(cid:17)(cid:19)

And thus the density of Φ (
·

) becomes

f (z) =

θ
k

k∗

θ⊤Σayθ

1/2

−

payϕ

θ⊤Σayθ

1/2

−

0,1
Xa,y
∈{

2 (cid:16)
}

(cid:17)

(cid:18)(cid:16)

(cid:17)

(cid:16)

(z

θ
k

k∗

+ w)

−

θ⊤µay

.

(cid:17)(cid:19)

By Bayes’ formula, we have

d(X)=0 = f (0)−

1

θ⊤Σayθ

pay

|

(cid:16)

(cid:17)

1/2

−

θ

k

k∗

payϕ

θ⊤Σayθ

(cid:18)(cid:16)

1/2

−

(cid:17)

(cid:16)

w

−

θ⊤µay

(cid:17)(cid:19)

 
 
Testing Group Fairness via Optimal Transport Projections

(a) N = 30

(b) N = 100

(c) N = 500

Figure2. Empirical distribution N × D(ˆP
with different sample sizes N .

N

) over 2,000 replications (histogram) versus the limiting Chi-square distribution (blue curve)

}

0, 1

and y

0, 1
∈ {
30, 100, 500

for a
N
}
compare the empirical distribution of N
ﬁnite-sample empirical estimates are closed to the theoretical limiting distributions even when N is as small as 30.

d(X) = 0]. In the ﬁrst experiments, we generate
d(X)=0 = E[I(a,y)(A, Y )
|
(ˆPN ). We replicate this process for 2,000 times and
(ˆPN ) with the limiting distribution deﬁned in Theorem 1. Figure 2 shows that

i.i.d. samples from P and then calculate N

, where pay
}

× D

× D

∈ {

∈ {

|

In the second experiments, we show that our proposed Wasserstein projection hypothesis test has the desired coverage
i.i.d. samples from P and compute the estimate ˆS deﬁned in
property. We generate N
Section 5.2 and the empirical covariance using the sample data. For the kernel estimator ˆS, we use the standard Gaussian
1/5, where the results listed below are not sensitive to the constant. We repeat the
kernel and choose the bandwidth h = N −
procedure for 2,000 replications and report the rejection probability at different signiﬁcant values of α
0.1, 0.05, 0.01
in Table 4. We can observe that when N > 100, the rejection probability is closed to the desired level α.

30, 100, 500, 1000, 2000
}

∈ {

∈ {

}

Table4. Comparison of the null rejection probabilities of probabilistic equal opportunity tests with different signiﬁcance levels α and
test sample sizes N .

α

0.10

0.05

0.01

N = 30
N = 100
N = 500
N = 1000
N = 2000

0.2875
0.0945
0.0895
0.0900
0.0870

0.2255
0.0540
0.0450
0.0430
0.0460

0.1415
0.0250
0.0085
0.0065
0.0080

Appendix B.2. The Description of Datasets

Followings show brief descriptions of datasets: Arrhythmia, COMPAS and Drug (Fehrman et al., 2017) provided in Sec-
tion 6.

• Arrhythmia is from UCI repository1, where the aim of this data set is to distinguish between the presence and absence
of cardiac arrhythmia and classify it in one of the 16 groups. The dataset consists of 452 samples and we use the ﬁrst
12 features among which the gender is the sensitive feature. For our purpose, we construct binary labels between
‘class 01’ (‘normal’) and all other classes (different classes of arrhythmia and unclassiﬁed ones).

• COMPAS (Correctional Offender Management Proﬁling for Alternative Sanctions)2 is a commerical tool used by
judges, probation and parole ofﬁcers to estimate a criminal defendant’s likelihood to re-offend algorithmically. The
COMPAS dataset contains the criminal records within 2 years after the decision. We use race (African-American and
Caucasian, which accounts for 5278 samples) as the sensitive attribute.

1https://archive.ics.uci.edu/ml/datasets/arrhythmia
2https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis

Testing Group Fairness via Optimal Transport Projections

• Drug (Fehrman et al., 2017) contains answers of 1885 participants on their use of 17 legal and illegal drugs. We
concern the cannabis usage as a binary problem, where the label is ‘Never used’ VS ‘Others’ (‘used’). There are 12
features including age, gender, education, country, ethnicity, NEO-FFI-R measurements, impulsiveness measured by
BIS-11 and sensation seeing measured by ImpSS. Among those, we choose ethnicity (black vs others) as the sensitive
attribute.

References

Barocas, S. and Selbst, A. D. Big data’s disparate impact. California Law Review, 104:671–732, 2016.

Berk, R., Heidari, H., Jabbari, S., Kearns, M., and Roth, A. Fairness in criminal justice risk assessments: The state of the

art. Sociological Methods & Research, pp. 0049124118782533, 2018.

Besse, P., del Barrio, E., Gordaliza, P., and Loubes, J.-M. Conﬁdence intervals for testing disparate impact in fair learning.

arXiv preprint arXiv:1807.06362, 2018.

Billingsley, P. Convergence of Probability Measures. John Wiley & Sons, 2013.

Black, E., Yeom, S., and Fredrikson, M. Fliptest: fairness testing via optimal transport.

In Proceedings of the 2020

Conference on Fairness, Accountability, and Transparency, pp. 111–121, 2020.

Blanchet, J., Kang, Y., and Murthy, K. Robust Wasserstein proﬁle inference and applications to machine learning. Journal

of Applied Probability, 56(3):830–857, 2019.

Buolamwini, J. and Gebru, T. Gender shades: Intersectional accuracy disparities in commercial gender classiﬁcation. In

Conference on Fairness, Accountability and Transparency, pp. 77–91, 2018.

Calders, T. and Verwer, S. Three naive Bayes approaches for discrimination-free classiﬁcation. Data Mining and Knowl-

edge Discovery, 21(2):277–292, 2010.

Chouldechova, A. Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. Big Data, 5

(2):153–163, 2017.

Chouldechova, A. and Roth, A. A snapshot of the frontiers of fairness in machine learning. Communications of the ACM,

63(5):82–89, 2020.

Cisneros-Velarde, P., Petersen, A., and Oh, S.-Y. Distributionally robust formulation and model selection for the graphical

lasso. In International Conference on Artiﬁcial Intelligence and Statistics, pp. 756–765. PMLR, 2020.

Corbett-Davies, S., Pierson, E., Feller, A., Goel, S., and Huq, A. Algorithmic decision making and the cost of fairness.
In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp.
797–806, 2017.

Dastin, J. Amazon scraps secret AI recruiting tool that showed bias against women. San Fransico, CA: Reuters. Retrieved

on October, 9:2018, 2018.

Datta, A., Tschantz, M. C., and Datta, A. Automated experiments on ad privacy settings: A tale of opacity, choice, and

discrimination. Proceedings on Privacy Enhancing Technologies, 2015(1):92–112, 2015.

DiCiccio, C., Vasudevan, S., Basu, K., Kenthapadi, K., and Agarwal, D. Evaluating fairness using permutation tests. In
Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 1467–
1477, 2020.

Donini, M., Oneto, L., Ben-David, S., Shawe-Taylor, J. S., and Pontil, M. Empirical risk minimization under fairness

constraints. In Advances in Neural Information Processing Systems, pp. 2791–2801, 2018.

Dua, D. and Graff, C. UCI machine learning repository, 2017. URL http://archive.ics.uci.edu/ml.

Durrett, R. Probability: Theory and Examples. Cambridge University Press, 2019.

Testing Group Fairness via Optimal Transport Projections

Dwork, C., Hardt, M., Pitassi, T., Reingold, O., and Zemel, R. Fairness through awareness. In Proceedings of the 3rd

Innovations in Theoretical Computer Science Conference, pp. 214–226, 2012.

Fehrman, E., Muhammad, A. K., Mirkes, E. M., Egan, V., and Gorban, A. N. The ﬁve factor model of personality and

evaluation of drug consumption risk. In Data Science, pp. 231–242. Springer, 2017.

Garg, S., Perot, V., Limtiaco, N., Taly, A., Chi, E. H., and Beutel, A. Counterfactual fairness in text classiﬁcation through

robustness. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, pp. 219–226, 2019.

Gordaliza, P., Barrio, E. D., Fabrice, G., and Loubes, J.-M. Obtaining fairness using optimal transport theory. In Proceed-

ings of the 36th International Conference on Machine Learning, pp. 2357–2365, 2019.

Grgic-Hlaca, N., Zafar, M. B., Gummadi, K. P., and Weller, A. The case for process fairness in learning: Feature selection

for fair decision making. In NIPS Symposium on Machine Learning and the Law, volume 1, pp. 2, 2016.

Härdle, W. Applied Nonparametric Regression. Cambridge University Press, 1990.

Hardt, M., Price, E., Price, E., and Srebro, N. Equality of opportunity in supervised learning. In Advances in Neural

Information Processing Systems 29, pp. 3315–3323, 2016.

Hui, Y., Xie, J., Blanchet, J., and Glynn, P. Empirical optimal transport projections with non-symmetric costs. preprint,

2021.

John, P. G., Vijaykeerthy, D., and Saha, D. Verifying individual fairness in machine learning models. In Conference on

Uncertainty in Artiﬁcial Intelligence, pp. 749–758. PMLR, 2020.

Kleinberg, J., Ludwig, J., Mullainathan, S., and Rambachan, A. Algorithmic fairness. In AEA Papers and Proceedings,

volume 108, pp. 22–27, 2018.

Lipton, Z., McAuley, J., and Chouldechova, A. Does mitigating ML’s impact disparity require treatment disparity? In

Advances in Neural Information Processing Systems, pp. 8125–8135, 2018.

Makhlouf, K., Zhioua, S., and Palamidessi, C. On the applicability of ML fairness notions.

arXiv preprint

arXiv:2006.16745, 2020.

Manrai, A. K., Funke, B. H., Rehm, H. L., Olesen, M. S., Maron, B. A., Szolovits, P., Margulies, D. M., Loscalzo, J., and
Kohane, I. S. Genetic misdiagnoses and the potential for health disparities. New England Journal of Medicine, 375(7):
655–665, 2016.

Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., and Galstyan, A. A survey on bias and fairness in machine learning.

arXiv preprint arXiv:1908.09635, 2019.

MultiMedia LLC. Machine Bias, 2016. Available at https://www.propublica.org/article/machine-bias-risk-assessments-in-

criminal-sentencing.

Owen, A. B. Empirical Likelihood. CRC Press, 2001.

Pleiss, G., Raghavan, M., Wu, F., Kleinberg, J., and Weinberger, K. Q. On fairness and calibration. In Advances in Neural

Information Processing Systems, pp. 5680–5689, 2017.

Saleiro, P., Kuester, B., Hinkson, L., London, J., Stevens, A., Anisfeld, A., Rodolfa, K. T., and Ghani, R. Aequitas: A bias

and fairness audit toolkit. arXiv preprint arXiv:1811.05577, 2018.

Silvia, C., Ray, J., Tom, S., Aldo, P., Heinrich, J., and John, A. A general approach to fairness with optimal transport. In

Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pp. 3633–3640, 2020.

Taskesen, B., Blanchet, J., Kuhn, D., and Nguyen, V. A. A statistical test of probabilistic fairness. Accepted to ACM

Conference on Fairness, Accountability, and Transparency, 2021.

Tramer, F., Atlidakis, V., Geambasu, R., Hsu, D., Hubaux, J.-P., Humbert, M., Juels, A., and Lin, H. Fairtest: Discovering
In 2017 IEEE European Symposium on Security and Privacy

unwarranted associations in data-driven applications.
(EuroS&P), pp. 401–416. IEEE, 2017.

Testing Group Fairness via Optimal Transport Projections

Tsybakov, A. B. Introduction to Nonparametric Estimation. Springer, 2008.

Villani, C. Optimal Transport: Old and New, volume 338. Springer, 2008.

Xue, S., Yurochkin, M., and Sun, Y. Auditing ML models for individual bias and unfairness. In International Conference

on Artiﬁcial Intelligence and Statistics, pp. 4552–4562. PMLR, 2020.

Zafar, M. B., Valera, I., Gomez Rodriguez, M., and Gummadi, K. P. Fairness beyond disparate treatment & disparate
impact: Learning classiﬁcation without disparate mistreatment. In Proceedings of the 26th International Conference on
World Wide Web, pp. 1171–1180, 2017.

Zehlike, M., Hacker, P., and Wiedemann, E. Matching code and law: achieving algorithmic fairness with optimal transport.

Data Mining and Knowledge Discovery, 34(1):163–200, 2020.

