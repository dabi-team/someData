1
2
0
2

g
u
A
0
3

]
E
S
.
s
c
[

2
v
7
3
4
3
0
.
2
1
9
1
:
v
i
X
r
a

Early Prediction for Merged vs Abandoned Code
Changes in Modern Code Reviews

Khairul Islama, Touﬁque Ahmedb, Rifat Shahriyara, Anindya Iqbala, and Gias
Uddinc
aBangladesh University of Engineering and Technology, bUniversity of California, Davis
and cUniversity of Calgary

Abstract

Context: The modern code review process is an integral part of the current

software development practice. Considerable eﬀort is given here to inspect code

changes, ﬁnd defects, suggest an improvement, and address the suggestions of

the reviewers. In a code review process, several iterations usually take place

where an author submits code changes and a reviewer gives feedback until is

happy to accept the change. In around 12% cases, the changes are abandoned,

eventually wasting all the eﬀorts.

Objective: In this research, our objective is to design a tool that can predict

whether a code change would be merged or abandoned at an early stage to

reduce the waste of eﬀorts of all stakeholders (e.g., program author, reviewer,

project management, etc.) involved. The real-world demand for such a tool was

formally identiﬁed by a study by Fan et al. [1].

Method: We have mined 146,612 code changes from the code reviews of three

large and popular open-source software and trained and tested a suite of super-

vised machine learning classiﬁers, both shallow and deep learning-based. We

consider a total of 25 features in each code change during the training and

testing of the models. The features are divided into ﬁve dimensions: reviewer,

author, project, text, and code.

Results: The best performing model named PredCR (Predicting Code Re-

view), a LightGBM-based classiﬁer achieves around 85% AUC score on average

and relatively improves the state-of-the-art [1] by 14-23%.

In our extensive

Preprint submitted to Information and Software Technology

September 1, 2021

 
 
 
 
 
 
empirical study involving PredCR on the 146,612 code changes from the three

software projects, we ﬁnd that (1) The new features like reviewer dimensions

that are introduced in PredCR are the most informative. (2) Compared to the

baseline, PredCR is more eﬀective towards reducing bias against new develop-

ers. (3) PredCR uses historical data in the code review repository and as such

the performance of PredCR improves as a software system evolves with new and

more data.

Conclusion: PredCR can help save time and eﬀort by helping developers/code

reviewers to prioritize the code changes that they are asked to review. Project

management can use PredCR to determine how code changes can be assigned to

the code reviewers (e.g., select code changes that are more likely to be merged

for review before the changes that might be abandoned).

Keywords: Code Review, Patch, Early Prediction, Merged, Abandoned

1. Introduction

Code review is a practice where a developer submits his/her code to a peer

(referred to as ‘reviewer’) to judge the eligibility of the written code to be

included in the main project code-base. Code review helps remove errors and

issues at the early stage of development. As such, code review can reduce

bugs very early and improve software quality in a cost-eﬀective way. A code

review process has some distinct steps (see Figure 1). The process starts when

a developer introduces a code change by creating a patch or revision. The

developer or the project moderator assigns a reviewer to examine this change

request [2]. The reviewer inspects the code, discusses any possible improvement,

and often suggests ﬁxes. After the review, the developer may provide a new

patch or revision addressing the review comments and generate a new review

iteration. This process repeats until either the reviewer accepts the changes and

it gets merged to the project, or the reviewer rejects the code changes and it

gets abandoned [3]. Such a workﬂow is facilitated by diﬀerent automated code

review tools such as Gerrit [4].

2

Figure 1: Workﬂow of a modern code review process

Substantial eﬀorts are spent by code reviewers to review a patch thoroughly,

to make code changes, and to analyze comments/suggestions made by the au-

thors. If a change is abandoned after some iterations, it causes signiﬁcant waste

of time and resources both for the code reviewer and the code author. Indeed,

we found that around 12% of code changes are abandoned in our mined data in

three large and popular open-source software projects (see Table 3 in Section 3).

Therefore, if we can predict early whether a code change would be merged or

abandoned in the long run, we can reduce the waste in eﬀort and time by both

code reviewers and authors. The prediction has to come as early as possible so

that the reviewers can use it to prioritize which code change to review next. On

the other hand, the management can analyze the cause of an ongoing review

process with negative predictions and intervene to save resources.

The real-world demand for a tool to early predict the future merge/abandon

chance of a code change was previously identiﬁed by a study by Fan et al. [1].

They surveyed 59 developers from three popular open-source software communi-

ties (Eclipse, LibreOﬃce, and GerritHub) and asked them whether they needed

a tool to predict early if a code change will be merged or abandoned in the

future. The developers agreed that they need a tool to early predict whether a

change would be merged/abandoned in the future. Developers pointed out that

this will (1) help prioritize code changes to review, (2) increase their conﬁdence

in merging the changes, and (3) reduce the resources wasted due to abandoned

changes.

3

A number of techniques and tools are developed in recent years to assist

code reviews with such early prediction. Jeong et al.

[3] proposed a model to

predict patch acceptance in the Bugzilla system. Gousios et al.

[5] predicted

pull request acceptance on GitHub. They calculated features when the pull

request has been closed or merged. The most recent early prediction model is

a shallow learning Random Forest model developed by Fan et al.

[1]. They

compared their performance with Jeong et al. [3] and Gousios et al. [5] and

showed better results at predicting merged changes.

Unfortunately, all the above approaches suﬀer from one or more of the follow-

ing shortcomings: (1) Jeong et al. [3] used programming language-speciﬁc key-

words as features but did not use any historical data, which can oﬀer more con-

texts to predict the likelihood of future merged/abandoned states. (2) Gousios

et al. [5] predict just before a pull request is merged/abandoned, which might be

too late to save eﬀorts because a code review can span over multiple iterations

and interactions between the code author and the code reviewer. Intuitively,

the sooner we can predict (in this cycle of iterations), the more eﬀorts and time

we can hope to save for both stakeholders. (3) Fan et al.

[1] do not use any

reviewer or project-related dimensions, which can oﬀer useful insights that are

more speciﬁc to a reviewer or a project. (4) All the three models also suﬀer

from bias against new authors, i.e., pull requests from new authors could be un-

fairly predicted as most likely to be abandoned due to lack of data. Therefore,

software developers and code reviewers can beneﬁt from a more robust tool that

can more reliably predict whether a code change would be merged or abandoned

in the future.

In this paper, we have conducted an empirical study on the feasibility of

developing a better classiﬁcation model by addressing the above limitations of

prior works. Using Gerrit API, we have mined 146,612 code changes from the

code reviews of three large and popular open-source software projects (Eclipse,

LibreOﬃce, and GerritHub). Each code change has information of whether it

is merged or abandoned - this is our target variable. For each code change,

we compute 25 features from ﬁve dimensions: reviewer, author, project, text,

4

and code. We then train and test ﬁve shallow machine learning models and

one deep neural network model on the dataset. We ﬁnd that a LightGBM-

based model oﬀers the best overall performance. We name the model PredCR.

In an empirical study involving PredCR, we answer the following ﬁve research

questions:

RQ1. Can our proposed early prediction model PredCR outperform

the state-of-the-art baselines? This validates the contributions of our work,

compared to the prior works (Section 4.1). The most recent model on the early

prediction of merged code changes was developed by Fan et al. [1]. Their model

outperforms previous works (Jeong et al. [3], Gousios et al. [5], etc.). We have

compared our model performance with the state-of-the-art by reproducing their

work. We found that PredCR relatively improves the AUC score by 14-23%.

The normalized improvements [1, 6] are 44 - 54%. Therefore, our developed

early prediction model PredCR oﬀers considerable performance improvement

over state-of-the-art baseline (i.e., Fan et al. [1]).

RQ2. How eﬀective is each feature dimension in our proposed ap-

proach? This investigates how each feature dimension in PredCR performs.

We have found that (Section 4.2) on average the AUC scores in models on the

reviewer, author, project, text, and code dimensions are 77%, 67%, 58%, 53%,

and 57% respectively. So previous experience-related features have much impact

on the code change outcome. Also, when dimensions are used all together the

average AUC score is around 85%. This validates that PredCR beneﬁts from

using all the dimensions.

RQ3. How well does the model handle bias against new authors?

As we noted before, state-of-the-art tools to predict early merge/abandoned

changes suﬀer from bias against new authors. One of our goals while designing

PredCR was to reduce such bias so that we can facilitate better onboarding of

new reviewers and authors into the software ecosystem. We have used historical

data to predict merged code changes whereas new authors have little prior

records in the system. We ﬁnd (Section 4.3) that PredCR achieves on average

5

78.7% AUC score for new authors. This relatively improves the AUC scores by

21-30% compared to the state-of-the-art [1].

RQ4. How well does our approach work while using multiple revi-

sions? In real life, code changes generally go through multiple revisions before

ﬁnally getting merged or abandoned. Intuitively, it is more diﬃcult to predict

the merge/abandoned change of a code change if we are only looking at the

ﬁrst revision, compared to the last revision. As such, it would be beneﬁcial to

ﬁnd whether and how PredCR can improve its prediction accuracy as we add

more revisions to it over time. This research question leads to exploring how

well PredCR performs when predictions are updated at each new revision of

the same code change. We ﬁnd (Section 4.4) that if features related to prior

revisions are added to the feature set, 6-15% relative improvements are achieved

in terms of the AUC score in the last revision compared to the ﬁrst. Therefore,

PredCR achieves better performance during the latter stages of a revision chain.

RQ5. How well does the model improve over time? As a software project

evolve, it can have more data to train over time. Therefore, it is important to

understand whether PredCR is able to improve its prediction accuracy, as a

project evolves. We thus sliced a project data by time into 11 folds, where

fold 0 contains the earliest data and fold 10 contains the most recent data. We

ﬁnd (Section 4.5) that PredCR gives 5-9% better AUC scores in the second

half of the folds (i.e., folds 5-10) than the ﬁrst half of the folds (i.e., folds 0-4).

Therefore, the performance of PredCR for an evolving software project improves

over time, as we have access to more data of the software.

Our tool can help reviewers manage their review works better. It can also as-

sist project management to make decisions regarding resource allocation. Code

changes with the possibility of being merged into the main codebase can be

given more focus than those predicted as abandoned. PredCR also extracts

features to understand change intent: bug ﬁx, feature implementation or refac-

toring (Section 3.2.4). In practice, bug ﬁx changes have more importance than

feature implementations and feature implementations have more importance

6

than refactoring. So reviewers can use PredCR to label more important code

changes (e.g., bug ﬁxes). Then prioritize changes that are more important and

have better merge probability. Table 1 summarizes the contributions we have

made in this work. The usage scenarios of our proposed tool are as below:

Table 1: Research contributions made in this work

Topic

Research Contribution

Research Advancement

Prioritizing

Our work shows considerable

Predicting

outcome

of

code

review

re-

performance improvement com-

changes has been highlighted

quests

pared to the state-of-the-art [1]

by many prior studies [3] [7] [5]

in early predicting outcome of

[8] [1] [9]. Our study will help

code changes.

to reduce the diﬃculties pro-

grammers are facing in the rapid

growth of software projects.

Reducing

We have shown that PredCR

Code review approach for new-

prediction

can reduce the prediction bias

comers is diﬀerent [10]. So care-

bias

against new authors is most cases

ful approach is necessary so that

compared to the state-of-the-art

such a prediction model does not

[1].

discourage them from contribut-

ing. Our study will help the com-

munity by reducing this bias.

Update

We have presented an adjusted

Compared to prior arts which

prediction

approach that can update pre-

calculates code change related

at multiple

diction at the submission of new

features only at initial submis-

revisions

revision for a code change so that

sion [1] or just before closing [11]

eﬀorts at later revisions are rec-

[5]

[8], our approach adds the

ognized.

ﬂexibility to also consider sub-

sequent revisions. This is more

useful as it scores based on the

latest patch before any review

has started on it.

7

• Without PredCR: Bob is a developer in a large project team. His respon-

sibility is to review submitted code changes by other developers. With the

expansion of the projects, the number of code changes he has to review has

increased too. He inspects the code changes serially by the order of submis-

sion time or randomly. However, it is diﬃcult for him to keep the focus on

reviewing so many code changes. Also, code changes with better quality are

often taking much longer to merge into the project for falling behind in the

queue. Some of the code changes are being abandoned even after his eﬀort

and time. Also after giving some initial reviews in a code change, he has to

go through it again to check if the author has improved it in a later revision.

• With PredCR: Bob and his team adopt our tool. The tool predicts the

merged probability of code changes that are assigned to him. Now he can

prioritize the code changes based on their probability of getting merged. So he

can focus more on those with a better chance of getting merged in the future.

He can also use the tool features to ﬁlter more important changes (e.g., bug

ﬁxes) and prioritize only them. Also, there would be less delay for the better

code changes, as they will be reviewed and accepted earlier. As such, Bob now

can spend less time on code changes that will likely be abandoned in the long

run. Moreover, the tool updates the prediction with each new revision/patch

submission of the same code change. This helps Bob reﬁne his decision to

prioritize code changes for review, e.g., a code author may radically improve

a new version of a code that was previously predicted to be abandoned by

PredCR. With new data, PredCR can update its prediction that the updated

code has now more chance of getting merged than abandoned. This will help

Bob to then focus more eﬀort on the new code changes during reviews.

Replication Package. https://github.com/khairulislam/Predict-Code-Changes.

Paper Organizations. The rest of the paper is organized as follows. Section

2 presents the prior works related to ours. Section 3 presents the data collection

process, studied features, research questions, and evaluation metrics. Section 4

presents the answers to the research questions presented in the previous section.

8

Section 5 discusses the major themes of our study results and highlights the

ﬁnding of our study. Then in Section 6, we have presented the threats to the

validity of our work. And Section 7 has the concluding remarks.

2. Related Work

In this section, we have presented the prior works related to our study. We

have discussed their motivations, working setups, features used, and limitations.

Table 2 shows the summary of those works and our comparison with them.

2.1. Early Prediction in Code Reviews

Jeong et al.

[3] focused on predicting patch acceptance at any state of

revisions. They suggested that patches predicted as accepted can be auto-

accepted and authors can use it before submitting a patch to get feedback

on it. Also, reviewers can use it to predict patch quality. Jiang et al.

[11]

conducted a study on the Linux kernel and examined the relationship between

patch characteristics and patch reviewing/integration time. Kamei et al.

[15]

built a change risk model based on characteristics of a software change to predict

whether or not the change will lead to a defect. However, this doesn’t predict

whether the change will be eventually merged or abandoned. Gousios et al.

[5] predicted acceptance of pull requests. To obtain an understanding of pull

request usage and to analyze the factors that aﬀect such development.

Hellendoorn et al. [12] used natural language processing techniques to com-

pute how similar a code change is to previous ones. They then predicted whether

it will be approved based on the review outcomes of similar ones. Thongta-

nunam et al.

[8] investigated the characteristics of patches that: (i) do not

attract reviewers, (ii) are not discussed, and (iii) receive slow initial feedback.

They calculated features just before the code change was closed and predicted

acceptance for it at that moment. Gerede et al.

[14] focused on predicting

whether or not a code change would be subject to a revision request by any of

its reviewers.

9

Table 2: Comparison of our paper with related works

Topic

Our works

Prior study

Comparison

Early Pre-

Our goal is to predict early

Predict whether a patch

Our goal is to predict merge

diction in

whether a code change

will be accepted [3, 5, 12],

probability early before any

Code Re-

will be merged or aban-

will need more than one

review starts.

Similar to

views

doned to prioritize

re-

submission to be accepted

Fan et al. [1].

views and to reduce waste

[13, 14], will

fail to at-

of eﬀorts on abandoned

tract reviewers [8], will be

changes.

closed earlier than others

[9]. Early prediction of a

code change being merged

[1].

Review

Gerrit code review tool

Bugzilla [3], Linux kernel

As all features available on

tool used

[11], Github [5, 12, 9], Ger-

one tool, might not be avail-

rit [1, 14, 13]

able on another, our work

on Gerrit can not be com-

pared directly with all of

them.

Feature

Reviewer, author, project,

Code or patch [3][11] [13] [5]

We have focused on more

dimen-

text and code related fea-

[1], bug report [3], project

recent performance of au-

sions

used

tures. Experience related

[5, 8, 1], author [5] [1], re-

thors and reviewers. All

features were calculated

view [11] [8], text [8] [1], re-

features presented by us

using more recent data

viewer [8]

in Section 3.2 are available

(past 60 days).

from the creation of the

code change.

Program

We have not used any

Jeong et al.

[3], Hellen-

PredCR can be used on

language

language-dependent

doorn et al.

[12], and

any project using the Ger-

depen-

dency

features

Huang et al.

[13] used

rit tool as it is language-

features dependent on Java

independent.

language.

Fan et al.

[1] predicted whether a code change will be merged or aban-

doned as soon as it was submitted. Their main objective was to prioritize the

code review process by early predicting code changes that are more likely to be

merged. They compared their works with Jeong et al. [3], Gousios et al. [5], and

show state-of-the-art performance. Zhao et al. [9] proposed a learning-to-rank

10

(LtR) approach to recommending pull requests that can be quickly reviewed

by reviewers. Diﬀerent from a binary model for predicting the decisions of pull

requests, their ranking approach complements the existing list of pull requests

based on their likelihood of being quickly merged or rejected. Huang et al. [13]

proposed a method to predict the time-cost in code review before a submission

is accepted. They focused on predicting whether a submission will be accepted

on the ﬁrst submission and whether it will take more than 10 submissions.

Our target is to predict the merged probability of a code change request as

soon as it is submitted before any review has come. This is similar to the work

by Fan et al. [1].

2.2. Review tool used

Jeong et al.

[3] used the Bugzilla system in Firefox and the Mozilla Core

projects. Gousios et al. [5], Zhao et al. [9], Hellendoorn et al. [12] worked with

pull requests in GitHub projects. Jiang et al. [11] worked on the Linux kernel

which is supported by Git repositories. Thongtanunam et al.

[8],Huang et al.

[13], Gerede et al. [14], and Fan et al. [1] worked on open source projects using

the Gerrit tool. We have also worked with the Gerrit tool.

2.3. Feature dimensions

Jeong et al. [3] used patch metadata, patch content, and bug report related

features. Bug report-related features are very speciﬁc to the Bugzilla system

they worked on. However, they do not use any historical data in the feature

set. Shin et al. [16] showed that without historical data fault prediction models

usually have low performance. Gousios et al. [5] used pull request, project, and

developers’ characteristics-related features. Both Jeong et al.[3] and Gousios

et al.

[5] used some features (time after open) which are not available when

the ﬁrst patch is submitted. Gousios et al.

[5] also used review activities in

previous revisions in the feature set (num comments, num participants). They

calculated features at the time a pull request has been closed or merged.

11

Jiang et al. [11] grouped the features into six dimensions: experience, email,

review, patch, commit, and development. The review group is related to re-

view participation in the prior patches. The email feature contains information

related to prior patches. Thus many of the features are not available when

submitting the ﬁrst patch. Kamei et al.

[15] grouped the features into diﬀu-

sion, size, purpose, history, experience dimensions. Thongtanunam et al.

[8]

extracted patch metrics in ﬁve dimensions: patch properties, history, past in-

volvement of an author, past involvement of reviewers, and review environment.

Their history feature is related to review activities in prior patches of the patch

set. They calculated the features just before the code change was merged or

abandoned. Fan et al. [1] grouped the features into ﬁve dimensions: code, ﬁle

history, owner experience, collaboration network, and text. All of these fea-

tures are available when the ﬁrst revision of the code change request is being

submitted.

We have grouped our features into ﬁve dimensions: reviewer, author, project

text, code. All of those features are calculated after the ﬁrst revision is created.

2.4. Programming language dependency

Jeong et al.

[3] used Java language-speciﬁc keywords in their feature set

to predict patch acceptance. Hellendoorn et al.

[12] trained and tested their

language models on pull requests that only contain java ﬁles. Huang et al.

[13] used code modifying features and code coupling features which are java

language-dependent. Therefore, they ﬁltered out any changes from their dataset

which contained any non-java ﬁle. These works are programming language-

dependent, so can not be used for projects of diﬀerent languages. Other previous

works discussed [1, 8, 11, 5], don’t have a programming-language dependency.

Our work doesn’t use any language-speciﬁc features. So it is programming

language-independent.

12

3. Empirical Study Setup

In this section, we have described how we have collected the data from Gerrit

projects and preprocessed them before using them in the experiment. Then we

have explained the features extracted from the dataset, which we have grouped

into ﬁve dimensions. We have presented the rationale and explained how the

features were calculated. Then, we have described our evaluation metrics to

measure the prediction performance. Finally, we have presented the research

questions we shall answer in our work.

3.1. Data collection and Preprocessing

We have used the REST API provided by Gerrit systems to collect data

from three Gerrit projects LibreOﬃce, Eclipse, and GerritHub. The miner was

created following the approach presented by Yang et al. [17]. We have collected

changes with the status “merged” or “abandoned”. We have mined a total

of 61062, 113427, and 61989 raw code changes respectively from LibreOﬃce,

Eclipse, and GerritHub respectively within the time period mentioned in Table

3.1.

To ﬁlter out the inactive/dead sub-projects, we have selected sub-projects

with at least 200 merged code changes. Hence, 4, 64, and 48 sub-projects were

left respectively from LibreOﬃce, Eclipse, and GerritHub. We have removed

code changes where subjects contain the word “NOT MERGE” or “IGNORE”

since these will eventually be abandoned. We have also removed changes where

the reviewers are the same as the owners. Some changes didn’t have patchset

data available anymore, we have also excluded them. The same preprocessing

steps are applied to all three projects. Table 3 presents statistics of the ﬁnally

collected dataset. We have also collected registration dates for each developer

account. It was later used during feature extraction for the computing expe-

rience of the developer. In case of missing values on the date of registration,

we have ﬁlled them by linearly interpolating them based on the existing dates

and account id. For example, if account id 3 has registration date missing and

13

the closest previous and next account ids are 1 and 5 with registration dates

01-01-2018 and 01-05-2018. Account id 3 will be assigned 01-03-2018 as the

registration date.

Table 3: Statistics of collected data

Project

Time period

Changes

Merged Abandoned

LibreOﬃce

2012.03.06 – 2018.11.29

56,241

51,410(91%)

4,831(9%)

Eclipse

2012.01.01 – 2016.12.31

57,351

48,551(85%)

8,800(15%)

GerritHub

2016.01.03 – 2018.11.29

33,020

29,367(89%)

3,653(11%)

Total

146,612

129,328(88%)

17,284(12%)

3.2. Studied features

We have extracted a total of 25 features from the dataset. All features

are calculated when the code change is initially submitted (same as Fan et al.

[1]). Gousios et al.

[5], Jiang et al.

[11], Thongtanunam et al.

[8] calculated

all features at the time when a change has been closed. However, the review

process has already been ﬁnished by then and no remedy is eﬀective at that

point. Our main goal is to predict the possibility of merging/abandonment

for code changes as early as possible. For this reason, we have not used the

following dimensions: history (Thongtanunam et al.

[8]), review (Jiang et al.

[11]), commit (Jiang et al.

[11]). These are not available at the initial stage.

Also, in Section 4.4 we have shown that by only adding revision numbers to

the feature list, PredCR can give signiﬁcant performance when the prediction

is updated after submission of each new revision.

Some features were not available in the Gerrit system. For example: bug

report information (Jeong et al.[3]), email (Jiang et al. [11]). When calculating

past record-related features, we have generally considered recent performances

(in the last 7 or 60 days). Fan et al.

[1] added ’recent’ preﬁx to features that

were calculated in the last 120 days. Our approach thus is more restrictive in

14

terms of feature history. Table 4 shows our ﬁnally selected feature list and the

rationale behind choosing those. We discuss the features and dimensions below.

Table 4: List of features. The dimensions which we have used, but were not used by state-of-

the-art [1] are highlighted as bold. The features for which we did not ﬁnd prior studies using

them, are highlighted as bold too.

Dimension Rationale

Feature Name

Reviewers number and their past record aﬀect

Reviewer

change outcome [18] [8].

Experienced programmer has low defect

probability [20]. Developer’s experience

Author

signiﬁcantly impacts on change outcome [11] [5].

More active developers have a better chance at

merging patches [21].

avg reviewer experience

avg reviewer review count [8] [19]

num of reviewers [8] [11]

num of bot reviewers

author merge ratio [1]

author experience

author merge ratio in project [1]

total change number [1] [19]

author review number [1] [11]

author changes per week [8]

Large workload results in less review

project changes per week [8]

Project

participation [19].

changes per author

Project’s receptiveness aﬀects change outcome [5] project merge ratio

Well

explained

descriptions

better

draw

description length [1]

Text

attention[18]

Intent of a code change is related to the kind of

feedback it receives.[22]

is bug ﬁxing [15, 8, 1]

is feature [8, 1]

is documentation [8, 1]

Modifying more directories is usually defect-

modiﬁed directories [8]

prone [20].

Scattered changes are more prone to defects [23]. modify entropy [15]

Code

Larger changes are more defect-prone [24].

Touching many ﬁles is more defect-prone [25] [24].

lines added [3] [1]

lines deleted [3] [1]

ﬁles modiﬁed [5]

ﬁles added [1]

ﬁles deleted [1]

subsystem num [1]

15

3.2.1. Feature Dimension 1. Reviewer

Num of reviewers is the number of human reviewers found in the reviewer

list of the code change. This feature was previously used by Thongtanunam

et al.

[8] and Jiang et al.

[11]. Num of bot reviewers are the number of bot

tools added to the reviewer’s list. As these accounts don’t actively participate

in review discussion but perform diﬀerent analyses on the patch set, we have

kept their number separately. Whether an account is a bot, is determined by

checking whether the account name is ’do not use’ or it contains any of the

following words ’bot’, ’chatbot’, ’ci’, ’jenkins’, or the project name. We have

calculated a reviewer’s experience by the number of years s/he is registered in

this system. We have calculated that using the diﬀerence of the revision upload

date and the reviewer’s date of registration in this project. This value is then

averaged by the number of reviewers, which is feature avg reviewer experience.

A reviewer’s review count is found by calculating the number of closed (merged

or abandoned) changes, in the last 60 days, where that a particular reviewer

was involved in the reviewer list. This value is then averaged by the number

of reviewers, which is feature avg reviewer review count. Thongtanunam et al.

[8] introduced similar features that calculated prior patches that a reviewer has

reviewed or authored.

3.2.2. Feature Dimension 2. Author

We have used the recent changes in a 60-day window when calculating au-

thor merge ratio, author review number, author merge ratio in project,

changes per week. When calculating the merge ratio, if there are no ﬁnished

changes of this author, then a default merge ratio of 0.5 is given. Author merge ratio

is the ratio of merged changes among all ﬁnished changes created by this au-

thor. Author review number is the number of changes where the author is in

the reviewers’ list. Author merge ratio in project is the author’s merge ratio in

the corresponding sub-project. This sub-project name comes with the ”project”

key in code change response, so we have kept it in this way. Changes per week

is the number of closed changes each week for this author in the last 60 days.

16

Author experience is calculated following the same way as the reviewer expe-

rience, i.e., taking the diﬀerence between the current revision upload date and

the author’s date of registration in years. Total change number is the number

of changes created by this author.

3.2.3. Feature Dimension 3. Project

We have calculated all project-related features in a 60 days window.

Project changes per week feature is calculated using the number of changes

closed every 7 days among the past 60 days for this sub-project. Changes per author

is the number of closed changes per author in the last 60 days. Project merge ratio

is the ratio of merged and closed changes in the last 60 days for this sub-project.

If the project doesn’t have any ﬁnished changes yet, the default merge ratio of

0.5 is given.

3.2.4. Feature Dimension 4. Text

These features are calculated on the change description provided for the

code change. The aim is to identify the purpose of the code change. The

description is provided in the subject of the code change when it is created.

Description length is the number of words present in the change description.

The other three features have binary values, i.e., 0 or 1. We have marked a code

change as documentation if the change description contains ”doc”, ”copyright”,

or ”license”. Similarly, we categorize it as bug ﬁxing if the change description

contains ”bug”, ”ﬁx” or ”defect”. Other changes are marked as a feature. These

are done following Thongtanunam et al. [8].

3.2.5. Feature Dimension 5. Code

This section refers to the features which are related to the changes made in

The source code. Modiﬁed directories refer to the number of directories modiﬁed

by this code change. It is calculated by extracting the bottom directories from

ﬁle paths. Similarly, subsystem num is the number of subsystems (the top

directory in the ﬁle path) modiﬁed in the change. Modify entropy is a feature
n
k=1(pk ∗
previously proposed by Kamei et al.[15]. Entropy is deﬁned as − P

17

log2pk), where n is the number of the ﬁles modiﬁed and pk is the proportion of

lines modiﬁed among total modiﬁed lines in this change.Other features such as

ﬁles added, ﬁles deleted, ﬁles modiﬁed, and lines added, lines deleted are self-

explanatory. Most of these source code features have also been used in prior

studies [15, 5, 8, 1].

3.3. Performance Metrics

We use a total of seven metrics to report and compare the performance of

PredCR against the baselines in our three datasets. The metrics can be broadly

divided into two categories: Standard Performance Metrics and Improvement

Analysis Metrics. All the metrics except one (cost-eﬀectiveness) are used from

Python scikit-learn library. The metrics are deﬁned below.

3.3.1. Standard Performance Metrics

We report ﬁve standard performance metrics:(1) AUC, (2) Cost-Eﬀectiveness,

(3) Precision, (4) Recall, and (5) F1-score.

AUC. Area Under the Curve (AUC) of the Receiver Operating Characteristic

(ROC) is a widely used performance measure for prediction models. For our

case, the AUC score calculates the probability that PredCR prioritizes merged

code changes more than abandoned code changes. Following related literature

on the early prediction of merged/abandoned code reviews, we use the AUC

score to determine the best-performing models.

Cost-Eﬀectiveness (ER@K%). Cost-eﬀectiveness is used to measure perfor-

mance given a cost limit. As in practice, developers can only review a limited

number of changes, our target is to correctly predict as many merged cases

as possible within that limit. Following prior studies

[2, 1], we have used

EﬀectivenessRatio@K% (ER@K% in short), which evaluates the percentage of

merged code changes in the top K% code changes(sorted by decreasing order

of merge probability) predicted as ”Merged”.

This also helps evaluate how well our model can prioritize the code changes.

A larger eﬀectiveness ratio means the model better prioritizes code changes that

18

will eventually be merged. The state-of-the-art [1] used this metric for the same

purpose. Xia et al. [2] used this metric to evaluate the prioritization of blocking

bugs. Jiang et al.

[7] also used this to evaluate the ranking of personalized

defect prediction. The authors of these works used prediction probability from

the model to prioritize.

By denoting the number of merged changes and the number of changes in

top K% as Nmk and Nk, respectively, we get,

ER@K% =

Nmk
Nk

(1)

We have used ER@20% as the default cost-eﬀective metrics. In Section 5.1.1,

we have shown PredCR performance when K is varied from 10 to 90. Note that

using K at 100 doesn’t have any signiﬁcance, as when choosing top 100% code

changes, the proportion of merged changes and all changes are constant for a

test set, irrespective of the model.

Precision. The proportion of changes that are correctly labeled among all pre-

dicted examples of that class. For merged and abandoned classes, we presented

this metric as P(M) and P(A).

P (M ) =

T P
T P + F P

, P (A) =

T N
T N + F N

(2)

Recall. The proportion of changes that are correctly labeled among changes

that actually belong to that class. For merged and abandoned classes, we pre-

sented this metric as R(M) and R(A).

R(M ) =

T P
T P + F N

, R(A) =

T N
T N + F P

(3)

Recall is diﬀerent from precision in this regard, precision means the per-

centage of results that are relevant. On the other hand, recall refers to the

percentage of total relevant results correctly classiﬁed by our algorithm.

F1-Score. The harmonic means of precision and recall. For merged and aban-

doned classes we presented this metric as F1(M) and F1(A).

F 1(M ) =

2 ∗ P (M ) ∗ R(M )
P (M ) + R(M )

(4)

19

F 1(A) =

2 ∗ P (A) ∗ R(A)
P (A) + R(A)

(5)

3.3.2. Improvement Analysis Metrics

We report two metrics: (1) Relative Improvement (RIMPR), and (2) Nor-

malized Improvement (NIMPR).

Relative Improvement (RIMPR). By relative improvement, we mean the

relative change between the two scores. Instead of simply calculating the diﬀer-

ence it is better because it considers the diﬀerence relative to the old value. For

example, improving a score from 20% to 40% is only a 20% increase in score.

But only calculating the diﬀerence misses the fact that the new score is double

the previous score. However, the improvement here is 100% which clearly shows

that fact. Improvement is calculated as follows,

Relative Improvement (RIM P R) =

new score − old score
old score

(6)

We report this metrics name as RIMPR throughout the rest of the paper.

Normalized Improvement (NIMPR). Normalized improvement is a mea-

sure proposed by Costa et al.

[6] to evaluate the improvement between two

methods in terms of an evaluation metric. The same metrics have been used

by Fan et al.

[1] to highlight improvements over baselines in prioritizing code

changes for reviewers. It takes room for improvement into consideration. For

example: let us consider accuracy is improved from 80% to 85% and F1 score

is improved from 90% to 95%. In both cases, the improvement is 5%, but nor-

malized improvement is 25% and 50%, respectively. In the latter case, the room

for improvement was only 10%. Hence, a 5% improvement here has much more

impact. We have used the short form of this metric as NIMPR.

N ormalized Improvement (N IM P R) =

new score − old score
1 − old score

(7)

20

3.4. Experimentation setup and approach

We have used the longitudinal data setup, previously used by Fan et al. [1].

Previous works have used similar setups to ensure only using past data to predict

future events. Rakha et al.[26] used a similar approach in retrieving duplicate

issue reports. Bangash et al.[27] used time-aware evaluation in cross-project

defect prediction.

For each project, the selected code changes are ﬁrst sorted in increasing

order of creation time. Then they are divided into 11 non-overlapping windows

of equal size. Instead of traditional ten-fold cross-validation, this approach is

followed to ensure that no future data is used during training.

In the ﬁrst fold, the model is trained using window 1 and tested on win-

dow 2.

In the second fold, the model is trained using windows 1 and 2 and

tested on window 3. Similarly, in the last fold(10), the model is trained on

windows 1-10 and tested on window 11. At each stage, we have calculated the

AUC, ER@20%, precision, recall, and F1 scores for merged and abandoned code

changes. Then we have computed the average of the metrics across ten-folds

for both merged and abandoned code changes. Kaggle kernels were used to run

all experiments. They provide an Intel(R) Xeon(R) CPU with 16 Gigabytes

of Ram, 4 CPU cores, and a 2.20GHz processor. We have used Python as the

programming language. Due to the stochastic nature of the machine learning

models, it is recommended to run a model multiple times and take the average

for ﬁnal performance reporting. In our case, for each model, each experimen-

tation is rerun ten times and the average result is reported to ensure stable

model performance. This means that for each model we did longitudinal 10-fold

cross-validation 10-times and then took the average. During each of the runs,

we did hyperparameter tuning.

Model selection process. First, we have used StandardScaler to ﬁt and trans-

form the features of each project. Then to ﬁnd the best model, we have used six

machine learning classiﬁers GradientBoosting [28], RandomForest [29], Extra-

Trees [30], LogisticRegression, LightGBM [31] and Deep Neural Network(DNN).

21

Except for LightGBM and DNN, all other classiﬁers are imported from the
scikit-learn library. The LightGBM classiﬁer used is taken from lightgbm 1
library. The DNN model was created using the keras2 library.

Handling class imbalance. As this dataset is an imbalanced one, we have con-

sidered class imbalance when training the models. We have balanced the classi-

ﬁcation loss, by setting the classiﬁer parameter class weight to ’balanced’. This

uses the values of the target column to automatically adjust weights inversely

proportional to class frequencies in the input data. This way class imbalance is

taken into consideration when calculating loss. Hence, we have set class weight

= ’balanced’ for all of these classiﬁers. Except for GradientBoosting, which

automatically handles class imbalance by constructing successive training sets

based on incorrectly classiﬁed examples [28].

Randomness across different runs. To introduce randomness across dif-

ferent runs, we set solver = ’saga’ for LogisticRegression (suggested by scikit-

learn documentation). And subsample=0.9, subsample freq=1, random state =

numpy.random.randint(seed=2021) for LightGBM (this will subsample 90% of

the train data each time). Otherwise, these two models produce the same re-

sults after each run, and rerunning them ten times doesn’t have a meaning. The

DNN model maintains random results because it initializes to random weights.

The other models had their random state kept to default ’None’ during model

initialization. We also manually validated whether each run is creating diﬀerent

results.

Deep Neural Network (DNN) Architecture. We have used the deep neural

network model to investigate whether neural networks would outperform other

machine learning classiﬁers. The network architecture is shown in ﬁgure 2. It

contained three dense layers. The input dense layer contains 25 relu units, one

for each feature. Then we have added a dropout layer with a 10% dropout rate,

1https://lightgbm.readthedocs.io/en/latest/index.html
2https://keras.io/

22

InputLayer

input:

[(None, 25)]

output:

[(None, 25)]

Dense

input:

(None, 25)

output:

(None, 25)

Dropout

input:

(None, 25)

output:

(None, 25)

Dense

input:

(None, 25)

output:

(None, 16)

Dropout

input:

(None, 16)

output:

(None, 16)

Dense

input:

(None, 16)

output:

(None, 1)

Figure 2: DNN model architecture

this would randomly drop 10% of the incoming values, which will help reduce

overﬁt on the training data. Then another dense layer with 16 relu units. Then

another dropout layer with a 10% dropout rate. The output layer contains

one sigmoid node to convert input values within 0 to 1. This is the merged

probability predicted by the model. We have used the ‘adam’ optimizer and

‘binary crossentropy’ loss. The number of epochs was set to 10 (increasing or

decreasing epoch more reduced test performance) during model training.

Parameter-tuning. We have also used grid search to hyper-tune the pa-

rameters for each model and presented their best performance. We tuned

n estimators and max depth for RandomForest and ExtraTrees classiﬁer, n estimators,

and learning rate for GradientBoosting and LightGBM, max iter for LogisticRe-

gression. For the DNN, we tried varying the number of layers, dropout rate,

optimizers, loss (those available in Keras tool for binary classiﬁcation), and the

number of nodes.

The best-performing one was chosen based on the AUC score, as it is men-

tioned [1] as the most important evaluation metrics to prioritize code changes

23

for reviewers. The chosen model is then used to compare our longitudinal cross-

validation performance with the state-of-the-art. The same classiﬁer is later

used to answer our other research questions. The hyper-tuning results for the

chosen model are discussed in Section 5.1.3.

Reproducing state-of-the-art baseline. To compare our work with the

state-of-the-art [1], we have followed the steps presented in their work and also
publicly shared git repository 3 to reproduce it. We have preprocessed our

dataset using the same steps as them. Then calculated their features. How-

ever, we found a bug in their feature calculations, which calculates the status

of some code changes which have not been closed yet. The bug found in au-

thor feature.py - class AuthorPersonalFeatures - def extract features, separates

code changes by the creation date of the current code change(for which they

are calculating features). However, some of those code changes are merged after

the current change was created. When calculating merge ratios, these changes

were not excluded. Thus merge ratios now may contain information about the

future of those code changes, potentially leaking the target label(merged or

abandoned). We have ﬁxed this issue by excluding those open changes when

calculating any kind of merge ratio (merged ratio, recent merged ratio, subsys-

tem merged ratio). We have shared both implementations (original and ﬁxed)
in our shared repository 4.

We have used a RandomForest classiﬁer with class weight=’balanced’ as

their model, it is equivalent to their usage of RandomForest from the Weka

tool with α = 1 in cost ratio. We have hyper-tuned their model and found the

best results when n estimators are 500 and max depth is 5. This hyper-tuned

model is then used on their feature set calculated from the same dataset as

us, following the longitudinal cross-validation setup. So our results are directly

comparable.

3https://github.com/YuanruiZJU/EarlyPredictionReview
4https://github.com/khairulislam/Predict-Code-Changes.

24

4. Empirical Study Results

In this section, we answer ﬁve research questions:

RQ1. Can our proposed approach outperform the state-of-the-art?

RQ2. How eﬀective is PredCR when only one feature dimension is used?

RQ3. How well does the model handle bias against new authors?

RQ4. How well does our approach work while using multiple revisions?

RQ5. How well does the model improve over time?

Answering RQ1 will show how PredCR performs compared to the state-of-

the-art works. It will also validate PredCR’s eﬀectiveness in the early prediction

of merged code changes. RQ2 will highlight the performance of each feature

dimension used in PredCR. This research question will also validate whether

PredCR beneﬁts from using all features rather than using a subset of them.

RQ3 will explore whether PredCR has any bias against new authors. We have

used author experience-related features, so this may introduce bias against new

authors. By answering this research question we have explained how we have

handled it and what impact it had on the performance of PredCR. RQ4 is

intended to explore if PredCR is able to improve its prediction ability with sub-

sequent revisions. And ﬁnally, RQ5 investigates if PredCR has any advantage of

using a longer period of time. With time, training data can be enriched as new

code changes are available in a project and hence performance improvement is

expected.

4.1. Can our proposed approach outperform the state-of-the-art? (RQ1)

4.1.1. Motivation

To validate the performance of PredCR, we plan to compare our approach

and performance with the state-of-the-art. The work of Fan et al.

[1] is con-

sidered state-of-the-art on the early prediction of merged code changes. Their

model outperforms the previous works (Jeong et al. [3], Gousios et al. [5], etc.)

25

Table 5: Performance for diﬀerent classiﬁers across the three projects

Model

LightGBM

DNN

Random Forest

GradientBoosting (GBT)

ExtraTrees

Logistic Regression

AUC

LibreOﬃce Eclipse GerritHub

86.0

84.3

85.3

85.2

85.4

85.6

81.2

82.9

83.6

82.8

83.3

77.0

84.6

84.0

83.5

82.7

83.6

79.3

with respect to most of the metrics. So if PredCR is able to outperform their

models in a similar setup, its superiority and applicability will be established.

4.1.2. Approach

The approach described in Section 3.4 is used here to evaluate this research

question.

4.1.3. Results

Table 5 shows the results for selecting the best classiﬁer. AUC is chosen

as it is suggested to be the best metric for this task [1]. The best results for

each project are in bold. We have found that LightGBM has the best

overall performance across the three projects. Which isn’t surprising

as LightGBM has previously shown better performance than similar gradient

boosting decision trees [31]. LightGBM showed an AUC score of 86.0, 84.3, and

84.6 for the three projects LibreOﬃce, Eclipse, and GerritHub, respectively.

We, therefore, have picked LightGBM as the underlying model in

our PredCR tool and used it throughout the remaining parts of the

paper.

Table 6, shows the results of our best model and its comparison with the

state-of-the-art [1]. PredCR outperforms the state-of-the-art in all cases.

26

Our average AUC is around 85%, where the state-of-the-art is around

71.4%. We note that we ﬁnd a slightly lower performance for Fan et al. [1]

model compared to its performance as reported in the Fan et al. paper. This

can be due to dataset diﬀerences and adding a ﬁx in their feature calculation.

Table 6: Longitudinal cross-validation test results with comparison

Project

Approach AUC ER@20%

Merged

Abandoned

F1(M) P(M) R(M) F1(A) P(A) R(A)

LibreOﬃce

Eclipse

GerritHub

Ours

86.0

99.3

94.1

95.9

92.4

48.3

42.2

58.7

Fan et al [1]

70.2

96.3

86.7

91.1

82.8

31.7

26.4

42.2

Ours

84.3

97.5

92.3

92.9

91.8

57.7

56.0

60.0

Fan et al [1]

69.7

93.9

81.6

89.2

75.6

36.1

29.2

50.0

Ours

84.6

99.0

92.0

95.3

88.9

50.0

42.2

62.5

Fan et al [1]

74.4

98.2

82.9

93.9

74.8

33.4

24.1

59.0

As deﬁned in Section 3.3, we calculate the relative improvement (RIMPR)

and normalized improvement (NIMPR) [6, 1] of PredCR over the state-of-the-

art baseline (i.e., Fan et al. [1]). We present the improvement of PredCR over

the baseline in Table 7 using four metrics AUC, ER@20%, F1(M), and F1(A).

We ﬁnd that (1) PredCR improves the AUC scores by around 14-23% com-

pared to state-of-the-art [1] and the normalized improvements are around 46-

58%. (2) The ER@20% in state-of-the-art was already around 96% on average.

However, PredCR still provides around 44-81% normalized improvement. (3) In

terms of f1 score for merged changes, PredCR provides around 9-13% relative

improvements and 42-58% normalized improvements. Though the state-of-the-

art [1] were already signiﬁcant for merged code changes. (4) For abandoned

changes, PredCR improves f1 score by a large margin. It gives around 50-60%

relative improvements and 24-34% normalized improvements. Considering only

12% of the code changes are abandoned, diﬃculties in accurately predicting the

fate of abandoned code changes are signiﬁcantly higher.

Table 8 shows the importance of our studied features while running the longi-

tudinal cross-validation process. It was calculated using the feature importances

27

Table 7: Improvements (RIMPR and NIMPR) of PredCR over the baseline (Fan et al. [1])

Project

Metric

RIMPR NIMPR

AUC

LibreOﬃce

ER@20%

F1(M)

F1(A)

AUC

Eclipse

ER@20%

F1(M)

F1(A)

AUC

GerritHub

ER@20%

F1(M)

F1(A)

22.8

3.11

8.53

52.4

20.9

5.06

13.1

59.8

13.7

0.81

8.56

49.7

53.7

81.1

55.6

24.3

48.2

59.0

58.2

33.8

39.8

44.4

41.5

24.9

attribute provided by the LightGBM classiﬁer for each project during the lon-

gitudinal cross-validation process and averaged over all runs. The importance

of the top three features for each project is in bold. The review and project

dimensions added in PredCR were not used by Fan et al. [1], but Table 8 shows

that they have a signiﬁcant impact on prediction performance.

28

Table 8: List of features with importance in PredCR

Dimension Feature Name

Feature Importance

LibreOﬃce Eclipse GerritHub

Reviewer

Author

avg reviewer experience

avg reviewer review count [8] [19]

num of reviewers [8] [11]

num of bot reviewers

author merge ratio [1]

author experience

author merge ratio in project [1]

total change number [1] [19]

author review number [1] [11]

author changes per week [8]

project changes per week [8]

Project

changes per author

Text

Code

project merge ratio

description length [1]

is bug ﬁxing [8] [1]

is feature [8] [1]

is documentation [8] [1]

modiﬁed directories [8]

subsystem num [15],

modify entropy [15]

lines added [3, 1]

lines deleted [3, 1]

ﬁles modiﬁed [5]

ﬁles added [32, 1]

ﬁles deleted [32, 1]

9.67

10.9

3.64

2.84

4.72

9.25

1.71

7.23

7.61

4.91

7.55

5.43

2.93

3.53

0.30

0.38

0.18

2.07

2.83

2.47

4.44

3.33

1.24

0.48

0.30

6.73

8.18

4.94

0.60

2.68

8.07

3.77

8.40

8.55

5.39

7.00

6.28

4.43

3.79

0.33

0.84

0.28

0.98

5.90

2.02

5.24

3.32

1.24

0.80

0.25

10.0

8.98

7.73

1.11

4.24

8.30

1.53

7.21

7.87

7.02

7.00

4.94

5.35

2.64

0.17

1.19

0.24

0.96

3.39

2.42

3.93

3.21

1.37

0.72

0.06

RQ1. Can our proposed approach PredCR outperform the

state-of-the-art baseline? Our PredCR tool is based on the LightGBM

model, which on average, outperforms the state-of-the-art [1] by 19% in

terms of AUC score. If we compare the normalized improvement (NIMPR)

metric, PredCR outperforms the state-of-the-art [1] by 48% in terms of AUC
29
score. PredCR outperforms by 10% for merged and by 54% for abandoned

changes (in terms of F1-score). The most informative two features in

PredCR are avg reviewer experience and avg reviewer review count which

belong to the Reviewer dimension, none of which were used by Fan et al. [1].

4.2. How eﬀective is PredCR when only one feature dimension is used? (RQ2)

4.2.1. Motivation

We have described the features we have used in Section 3.2. In this research

question, we have investigated how much performance each feature dimension

used in PredCR achieves alone. This will also validate whether PredCR beneﬁts

from using all those feature dimensions or not.

4.2.2. Approach

We have used the same longitudinal ten-fold cross-validation on all projects.

We have worked ﬁrst with all dimensions and later trained and tested the clas-

siﬁer for one feature dimension only. Then reported the performance metrics.

4.2.3. Results

Table 9 shows PredCR performance using all feature dimensions and single

feature dimension. The best results for each dimension are in bold. The average

AUC on models trained on all dimensions, reviewer, author, project, text, and

code dimensions are 85%, 77%, 67%, 58%, 53%, 57%.

In terms of the AUC

score, PredCR on average improves reviewer, author, project, text,

and code models by 10%, 27%, 46%, 60%, and 49% respectively.

Except for the reviewer dimension, all other dimensions have poor performance

for abandoned code changes.

We have presented two examples to demonstrate the importance of the re-

In project
viewer dimension and how it aﬀects the change request outcome.
LibreOﬃce, for change id 658905, the author was facing build failures because

one of the pipeline tests was failing. The reviewer mentioned that the test

failed not because of the author’s change.

If he would have uploaded a new

revision of this patch, the tests might have run successfully. The author later
abandoned this change and created another change 662036 for the same issue.

This was later merged successfully with further help from the reviewer. Clearly,

5https://gerrit.libreoffice.org/c/core/+/65890
6https://gerrit.libreoffice.org/c/core/+/66203

30

Table 9: Performance of PredCR for all features and in each feature dimension

Project

Dimension

AUC ER@20% F1(M) F1(A)

LibreOﬃce

Eclipse

GerritHub

All dimensions

86.0

99.3

94.1

48.1

Reviewer

Author

Project

Text

Code

81.3

67.7

50.8

52.5

53.7

97.9

96.1

91.2

92.6

92.6

92.2

90.7

76.8

73.2

81.0

42.9

25.1

11.5

14.9

14.5

All dimensions

84.3

97.5

92.3

57.7

Reviewer

Author

Project

Text

Code

75.9

65.3

58.1

55.1

55.9

93.3

92.4

90.0

87.7

88.4

91.5

81.6

78.5

74.0

76.1

54.2

31.5

25.5

24.2

24.6

All dimensions

84.6

99.0

91.7

49.3

Reviewer

Author

Project

Text

Code

72.7

69.3

66.4

52.4

61.2

95.3

97.5

96.8

90.2

95.8

86.8

83.6

77.7

58.2

74.8

35.1

26.9

26.2

19.2

22.5

the reviewer’s experience directly inﬂuenced the outcome of these changes. In
project GerritHub change id 7455197, the reviewer suggested that the change

made by the author was unnecessary since there was a better alternative. The

experienced reviewer knew about this method, but the author did not. After

reviewer’s suggestion he abandoned the change.

7https://review.opendev.org/#/c/745519/

31

To demonstrate the importance of the author dimension, we show an example
from our dataset below. In LibreOﬃce change id 40718 the author gives a ﬁx

for several bugs. The reviewer compliments the author for ﬁxing this critical

problem. The author is experienced in this project and had been working for

more than 1 year, with a 0.98 merge ratio in this project. At the time of this

code change, he was making around 7 code changes per week and also actively

reviewing other code changes.

Similarly, here is an example of the project dimension. In LibreOﬃce change
id 40718, the code change is made for the ’core’ sub-project. At the time this

code change was created, this sub-project had around 103 code changes per

week, a merge ratio of 0.87, and on average 8 code changes per developer.

The author was making around 7 code changes per week, so he was a regular

developer on that project. We see his code changes get merged with minimal

review.

And, the following example shows the importance of text dimension.
In
project GerritHub change id 7455199, the change description says, ”add brctl

command for neutron-linuxbridge image”. The number of words would be 6.

And following the approach of Thongtanunam et al.

[8] this code change will

be labeled as a feature.

The next one demonstrates the importance of the source code-related dimen-
sion. For example, LibreOﬃce change id 40718 is a medium-size code change.

The author made 71 line additions and 4 deletions across 6 ﬁles. So this is easier

for the reviewers to inspect. We see it gets merged with minimal review. These

examples demonstrate the importance of using PredCR with diverse features.

8https://gerrit.libreoffice.org/c/core/+/4071
9https://review.opendev.org/#/c/745519/

32

RQ2. How eﬀective is PredCR when only one feature dimension

is used? The reviewer dimension has the best average AUC score of 77%

across projects for a single dimension. Also, this dimension has moderate

performance on abandoned code changes. Author dimension achieved 67%

AUC score on average. Project dimension achieved on average 58% AUC

score, but there is a signiﬁcant diﬀerence in score between LibreOﬃce and

GerritHub. Text and code dimensions achieved around 53% and 57% AUC

scores, so their impact is close. Using all dimensions together improved our

AUC scores by 10-60%. This validates that PredCR beneﬁts from the use of

all features, compared to its subset.

4.3. How well does the model handle bias against new authors? (RQ3)

4.3.1. Motivation

Table 8 shows high importance of author-related features on PredCR. For a

new author, it is more likely to consider him/her as inexperienced and predict

a lower possibility for merging. For example, Fan et al. [1] faced a considerable

bias against new authors. Changes made by new authors were mostly being

predicted as abandoned. Hence, they had to propose an adjustment approach.

They predicted code changes made by new authors using a model that is only

trained on code changes by new authors. They used another model trained on

all code changes for experienced contributors. We also need to evaluate how

much bias PredCR might have for the new authors.

4.3.2. Approach

We have labeled authors with less than ten code changes as new authors

following Fan et al.

[1]. Test dataset in each fold of the longitudinal ten-

fold cross-validation only contains new authors. Fan et al.’s [1] results were

reproduced using their adjusted approach as they suggested.

33

4.3.3. Results

Table 10 shows the comparison of PredCR performance with state-of-the-art

[1]. Our ﬁndings from the table are as below,

1. PredCR’s average AUC score across all projects is 85% and for new au-

thors, it is 78.7%. So the performance drop in PredCR for this case is

small, considering new authors have either none or few past records.

2. In terms of AUC scores PredCR improves over Fan et al.’s [1] adjusted

approach for LibreOﬃce, Eclipse, and GerritHub projects by 26%, 31%,

and 21%. The normalized improvements are 43%, 47% and 40%.

3. In terms of ER@20%, PredCR provides 5-17% relative improvements.

4. For metrics related to merged code changes, PredCR under-performs in

terms of F(M) and R(M). This is because PredCR has less bias against

abandoned code changes.

5. For metrics related to abandoned code changes, PredCR signiﬁcantly out-

performs in terms of F(A) and R(A). But under-performs in terms of

P(A). However, this shows that Fan et al.’s [1] adjusted approach has a

considerable bias towards merged code changes.

Table 10: Performance on changes created by new authors

Project

Approach

AUC ER@20%

Merged

Abandoned

LibreOﬃce

Ours

Fan et al. [1]

Ours

Eclipse

Fan et al.[1]

Ours

GerritHub

Fan et al.[1]

78.3

62.1

78.9

60.6

78.9

65.0

F1(M) P(M) R(M) F1(A) P(A) R(A)

94.4

83.2

89.9

76.6

91.1

86.9

52.9

92.3

41.6

49.6

37.1

85.2

85.3

76.0

97.2

13.9

49.7

8.31

71.5

87.2

61.2

54.0

42.7

75.8

79.4

68.7

98.2

9.70

54.3

5.38

64.6

89.8

51.9

49.3

36.5

78.9

84.9

75.6

97.1

11.3

41.6

6.81

We have concluded that in Fan et al.’s [1] original approach, the bias to expe-

rienced authors was introduced by using many features related to the author’s

34

past records. For the new authors, these feature values are mostly zero and

thus cause a bias against them increasing the likelihood of predicting them as

abandoned. Even the adjusted approach ends up having a bias towards merged

code changes. To reduce such bias, we have decided not to use the collaborative

dimension which considers the collaborative history between author and review-

ers. This could have resulted in a decrease in overall performance. However,

our addition of features related to reviewer and project dimensions makes up

for that deﬁciency and also improves the overall model performance.

RQ3. How well does the model handle bias against new authors?

PredCR achieved on average 78.7% AUC score in the longitudinal

cross-validation test for new authors, where the state-of-the-art [1] achieved

around 63%. PredCR gives a more balanced prediction for both classes,

while still maintaining a better AUC score. Also, our model performance for

new authors (78.7% AUC) is not far behind the overall model performance

(85% AUC).

4.4. How well does our approach work while using multiple revisions? (RQ4)

4.4.1. Motivation

So far we have trained and tested with only the initial submission of code.

But in real life, a code change generally goes through several revisions before

ﬁnally getting merged or abandoned. Each revision contains updated ﬁles based

on reviews received in the previous revisions. Thus an outcome predicted based

on the ﬁrst revision might be improper for later revisions. The prediction model

needs to be able to update prediction given a code change when a new revision is

pushed. Besides the initial submission, the stakeholders can still be signiﬁcantly

beneﬁted if a good prediction is available after early-stage revisions.

Many of the changes are not ready for review during the initial submission.

The reasons can be: (i) build failure, (ii) pipeline test failure, (iii) work in

progress, (iv) merge conﬂict, (v) unintentionally included changes, and (vi)

dependent on any other change. For this, the author has to push more patches.

35

Multiple patches are already uploaded before the review even starts. A merge

prediction made only on the ﬁrst patch would miss any of these cases. For
example, in project Eclipse, for change-id 16741210, the initial patch was labeled

work in progress. The second patch faced a build fail. On the third patch, it

was labeled as ready-for-review and ﬁnally was merged after the eighth patch.
In project LibreOﬃce, for change-id 10037311, it took the author ﬁve patches

to ﬁx build fails. Only then the change was ready for review and ﬁnally was

merged at the sixth patch.

4.4.2. Approach

We have designed two adjusted approaches for the merge prediction of a

code change in revision rounds. In the ﬁrst approach, we have added only the

review number to the existing feature set. This approach does not train on

any previous activities within the patchset. In the second approach, we have

added features related to reviews and other activities of previous revisions in

the feature set. Similar approach was followed by Gousios et al.

[5], Jiang et

al.

[11] and Thongtanunam et al.

[8]. In both approaches, we have used the

longitudinal data setup during validation. Change features are sorted according

to their creation time.

We have used two diﬀerent approaches because they will show how PredCR

performs with or without considering review activities from previous revisions.

One important diﬀerence is that for this approach our features are

calculated right after a new revision is uploaded. So that we can give

updated predictions on the code change before reviewers have to do

any review. Both Gousios et al. [5] and Thongtanunam et al. [8] calculated

features just before the pull request or the code change is closed. However

reviews are already done at that point, so predicting at that point would not

be helpful for reviewers.

10https://git.eclipse.org/r/c/platform/eclipse.platform.swt/+/167412
11https://gerrit.libreoffice.org/c/core/+/100373

36

4.4.3. Results

Table 11 shows the test results with the ﬁrst approach. Column RIMPR

and NIMPR show the improvement and normalized improvement [6, 1] in the

AUC scores at the last revision compared to the ﬁrst. Here we have added

’revision number’ in the feature set so that the model knows at which stage of

review this code change belongs. Jiang et al. [11] used patch no when predicting

whether a patchset will be accepted in the git repository of the Linux kernel.

Note that this result is not comparable with the one shown in Table 6 because

the test set is diﬀerent. However, the average AUC is still signiﬁcant.

’Total’ presents results when the test fold contains all changes of that fold.

’First revision’ presents the result when the test fold only contains changes at

their ﬁrst revision. The last revision means when the code change was ﬁnally

merged or abandoned.

’Last revision’ presents the result when the test fold

only contains changes at that revision. Table 11 shows that in all cases the

AUC score has improved in the last revision. That is expected because the fate

of the code change is almost set at that time.

Table 11: AUC(%) for multiple revisions with revision number (Approach 1)

Project

Total First revision Last revision RIMPR NIMPR

LibreOﬃce

Eclipse

GerritHub

85.2

77.8

82.0

86.2

83.7

85.1

92.5

86.1

86.5

7.7

2.9

1.6

47

15

9.4

For the second approach we have used revision number [11], weighted approval score,

avg delay between revisions [8], and number of messages as extra features. Weighted

approval score is calculated at each revision by adding label values of previous re-

visions multiplying by the fraction of current revision no and current revision no

+ 1. This will add more weight to the labels in the later revisions.

Avg delay between revisions is calculated in days. Table 12 shows the average

test AUC scores achieved during the experiments. Column RIMPR and NIMPR

show the improvement and normalized improvement[6, 1] of the AUC scores at

37

the last revision compared to the ﬁrst.

Table 12: AUC(%) for multiple revisions with previous revision related features (Approach 2)

Project

Total First revision Last revision RIMPR NIMPR

LibreOﬃce

Eclipse

GerritHub

88.2

79.5

82.6

86.2

83.7

85.1

98.8

89.0

90.0

15

6.1

5.8

92

33

33

Overall AUC scores and AUC scores in the last revision both have improved

in this approach. During the ﬁrst revision, these previous revision-related fea-

tures do not exist. However, this result shows that adding previous revision-

related features can improve prediction performance in later revisions. Also we

have found, for changes with only one revision the AUC scores are 86%, 85.2%,

and 83.5% in project LibreOﬃce, GerritHub, and Eclipse. But for changes with

multiple revisions, their AUC scores at the last revision (when the change was

ﬁnally closed) are 98.6%, 89.4%, and 86% respectively. But since our primary

goal is to give better results during the initial submission, we have not focused

too much on this point.

RQ4. How well does our approach work while using multiple

revisions? PredCR achieves around 78-88% AUC score when predictions

are updated at the submission of each new revision. PredCR can improve

prediction at the last revision up to 8%, compared to the prediction

performance at the ﬁrst revision without using previous revision

activity-related features. Using previous revision activity-related features can

improve the performance up to 15%. So PredCR can be adjusted with

signiﬁcant results to update predictions at later revision.

4.5. How well does the model improve over time? (RQ5)

4.5.1. Motivation

In real-life scenarios, the number of changes will keep increasing over time.

Hence, the model can be trained on a larger dataset. But it is important to

38

validate whether increasing the size of the training dataset will increase the

performance of PredCR.

4.5.2. Approach

We have followed a longitudinal ten-fold cross-validation setup to calculate

model performance in each project. As explained in the approach of RQ1,

this validation setup ensures no future data is used during training. The code

changes are sorted by their time of creation and the model trained on past data

is used to predict future code changes. The performance of subsequent folds

of validation presents the outcome of the model over time. Therefore, we have

used the results achieved in each fold of the longitudinal cross-validation setup

performed in RQ1, to validate whether PredCR performance improves in later

folds.

4.5.3. Results

Table 13 shows the prediction performance of the model during each fold

by AUC score. The results show that the performance does not monotonically

increase over time. However, the performance in the last half is better on aver-

age than that in the ﬁrst half. In the last fold, both LibreOﬃce and GerritHub

achieved the best results. Eclipse achieved the best AUC score in the 6th fold.

Average AUC score for LibreOﬃce, Eclipse, and GerritHub in fold 1-5 are re-

spectively 82%, 82%, and 81.4%. And in fold 6-10 they are 89.2%, 86.0%, and

87.9%. So AUC scores on average improved 9%, 5%, and 8% in the latter half

of the longitudinal cross-validations.

RQ5. How well does the model improve over time? The longitudinal

cross-validation setup sorts data by time and after each fold, one more

window is added to the training data, so train data size increases too. In this

real-world scenario, PredCR has improved 5-9% in terms of AUC scores in

the latter half of the fold. This validates that, in an active project, with the

passage of time, PredCR will be able to improve its performance as more

changes are created.

39

Table 13: AUC score in each fold

Fold LibreOﬃce Eclipse GerritHub

1

2

3

4

5

6

7

8

9

82.6

80.7

81.0

80.3

87.0

87.5

88.2

89.6

90.9

10

91.8

5. Discussions

76.6

82.1

81.2

86.9

86.1

88.6

84.3

85.6

87.1

84.9

86.4

72.9

80.6

79.7

87.5

84.8

85.5

89.6

88.4

91.1

In this section, we ﬁrst oﬀer more detailed insights into the performance of

PredCR by analyzing the performance based on hyper-parameters and run-time

(Section 5.1). We then discuss the implications of PredCR and our study ﬁnd-

ings to the ﬁeld of software engineering practitioners and research in Section 5.2.

5.1. A Deeper Dive Into PredCR Performance

In Section 5.1.1, we ﬁrst analyze the eﬀectiveness of PredCR based on the

presence of more/fewer code changes.

In Section 5.1.2, we report how much

time PredCR takes to train. In Section 5.1.3, we report how the performance of

PredCR changes based on diﬀerent values of hyper-parameters. We have used

the PredCR in Section 4 after the hyper-parameter tuning. In Section 5.1.4 we

discuss our model results after excluding each dimension from the feature set.

Finally, Section5.1.5 shows the eﬀorts developers spent per code changes in our

dataset.

40

5.1.1. Eﬀectiveness of PredCR with gradual increase in code changes

In this section, we will investigate the performance of PredCR with an in-

creased percentage of inspected code changes. Since reviewing code changes is a

costly and time-consuming task, it is not feasible to inspect all the reviews. Like

previous studies, we use 20 as the default value for K in ER@K%. To observe

the performance of PredCR with increased K, we increase the value from 10 to

90 and repeat the experiment. Table 14 presents that PredCR outperforms Fan

et al. [1] for all the projects at every K value. Though the ER is supposed to

decrease as K increases (the number of abandoned changes increases in the top

K% of the list), still PredCR performs well.

Table 14: ER@20% for diﬀerent K

K

LibreOﬃce

Eclipse

GerritHub

Ours Fan et al[1] Ours Fan et al[1] Ours Fan et al[1]

10

20

30

40

50

60

70

80

90

99.5

99.2

98.9

98.6

98.1

97.8

97.4

96.6

95.7

97.3

96.3

95.1

94.4

93.7

92.8

92.0

90.0

89.7

97.9

97.5

96.8

96.4

95.8

95.2

94.4

93.3

91.8

95.2

93.9

93.1

92.4

91.4

90.5

89.4

88.1

85.9

99.4

99.0

98.6

98.0

97.3

96.8

96.2

95.5

94.3

98.2

98.2

97.3

96.3

95.8

94.7

93.8

92.5

91.2

5.1.2. Time eﬃciency

In this section, we discuss the time needed to train the model and its pre-

diction time. If the model takes too long to predict, then the reviewers would

not get the updated predictions in time, thus discouraging them from applying

it. Moreover, new changes keep coming, and it would be challenging to update

the model if it takes too long. Our used environment provides 16 Gigabytes

41

of Ram, 4 CPU cores, and a 2.20GHz Intel Xeon CPU. In Table 15, we have

presented model training times in seconds. For LibreOﬃce, Eclipse, and Ger-

ritHub, PredCR training across all 10 folds takes on average 3.56, 2.39, and

2.37 seconds. Where the state-of-the-art [1] takes on average 6.97, 14.2, and

6.60 seconds.

Table 15: Model training time (seconds) in each fold

Fold

Libreoﬃce

Eclipse

GerritHub

Ours Fan’s Ours Fan’s Ours Fan’s

1

2

3

4

5

6

7

8

9

10

1.64

2.13

2.69

3.07

3.30

3.78

4.19

4.61

5.02

5.17

1.99

3.09

4.20

5.24

6.32

7.58

8.65

9.75

10.9

12.0

1.22

1.65

1.82

1.98

2.21

2.46

2.71

3.08

3.22

3.54

2.85

5.29

7.68

10.1

12.5

15.3

18.0

20.7

23.4

25.8

1.28

1.77

1.79

2.06

2.38

2.49

2.87

2.90

2.94

3.24

1.99

3.01

4.07

5.04

6.00

7.10

8.18

9.13

10.2

11.3

Average

3.56

6.97

2.39

14.2

2.37

6.60

5.1.3. Impact of Hyper tuning of PredCR

Hyper-tuning the parameters of the selected classiﬁer is needed to ensure best

model performance during practical use [33]. We have hyper-tuned the selected

LightGBM classiﬁer with varying the number of estimators and learning rate.

The results are shown in Table 16. The best parameters are n estimators= 500

and learning rate= 0.01 based on AUC score.

42

Table 16: Hyper-tuning of PredCR

Project

n estimators

learning rate AUC F1(M) F1(A)

LibreOﬃce

Eclipse

GerritHub

100

100

500

500

100

100

500

500

100

100

500

500

0.1

84.8

0.01

85.6

94.9

91.7

48.5

43.4

0.1

83.3

96.3

48.8

0.01

86.0

0.1

84.0

0.01

83.4

94.2

92.6

90.9

48.1

57.6

55.2

0.1

83.1

93.9

59.4

0.01

84.3

0.1

83.8

0.01

83.9

92.3

93.3

89.1

57.7

51.6

44.2

0.1

82.8

95.8

56.2

0.01

84.6

92.0

50.0

5.1.4. Impact of excluding each dimension

This section presents the importance of each feature dimension. We exclude

one feature at a time and rerun the experiment. From Table 17 we can see that

both reviewer and author dimensions have signiﬁcant impacts on the model per-

formance. However, removing the author dimension still gives around 82% AUC

score. So if potential bias against new authors becomes a concern, removing this

dimension will not make the model unusable.

5.1.5. Developer eﬀort spent for changes

In this section, we show how much eﬀort the developers on average spent on

code changes. We consider its duration in days, the number of messages, and

the number of revisions as eﬀort. Duration is measured as the number of days

43

Table 17: Performance of PredCR after excluding one feature dimension at a time

Project

Excluded Dimension AUC ER@20% F1(M) F1(A)

Reviewer

Author

LibreOﬃce

Project

Text

Code

Reviewer

Author

Project

Text

Code

Reviewer

Author

Project

Text

Code

Eclipse

GerritHub

70.0

82.4

85.9

85.9

85.7

68.4

81.1

84.0

84.1

83.6

73.3

82.8

83.5

84.4

84.5

96.9

98.4

99.2

99.2

99.1

94.1

96.4

97.3

97.5

97.1

97.9

98.5

98.9

98.9

98.9

91.7

93.5

93.8

94.0

93.9

84.1

92.9

91.9

92.2

92.3

84.6

91.3

91.6

91.8

91.8

26.8

45.7

46.8

47.9

47.6

33.4

56.4

57.3

57.3

57.1

30.7

47.8

47.7

49.4

49.5

spent from the creation of the code change till it gets merged or abandoned.

We found there were occasional large values of these metrics and removing such

outliers as noises is a standard statistical process [34]. Following Tukey et al.

[34], we have removed values outside these two ranges: (a) Lower limit: ﬁrst

quartile - 1.5 * IQR (b) Upper limit: third quartile + 1.5 * IQR. Where IQR

(Inter Quartile Range) is calculated by subtracting the ﬁrst quartile from the

third. After removing the outliers, we calculated the mean of those values

and presented them in Table 18. From Table 18 we see, abandoned changes

are generally taking more time to close, have fewer messages and revisions per

change. These stats are consistent with the results found by Wang et al.

[35]

who investigated in detail why code changes get abandoned.

44

Table 18: Developer eﬀort spent on the code changes

Project

Approach Duration in days Messages Revisions

Total

LibreOﬃce

Merged

Abandoned

Total

Eclipse

Merged

Abandoned

Total

GerritHub

Merged

Abandoned

0.90

0.89

0.98

2.11

2.09

2.30

1.60

1.57

1.90

5.81

5.87

5.21

8.75

9.18

6.43

9.16

9.37

7.50

2.28

2.36

1.42

2.24

2.36

1.62

2.00

2.04

1.68

5.1.6. Cross Project Performance

For new projects, there might not be enough data to start giving predictions.

In those cases, models pre-trained on other projects might be useful during the

initial stage of the project. Here we have evaluated PredCR performance in

cross-project settings. We have trained the model on a complete dataset of one

project and tested it on a complete dataset from another project.

Table 19: Performance of PredCR across projects

Source Project Target Project AUC ER@20% F1(M) F1(A)

LibreOﬃce

Eclipse

GerritHub

Eclipse

GerritHub

LibreOﬃce

GerritHub

LibreOﬃce

Eclipse

64.3

66.9

77.5

76.6

79.2

81.1

95.1

92.5

97.5

97.8

98.3

95.9

85.2

88.7

84.6

88.0

84.4

87.6

23.0

32.2

30.6

36.0

30.7

58.2

From Table 19 we see PredCR maintains around considerable performance

45

even across projects. So PredCR pre-trained on other projects can be eﬀectively

used for new projects. Notice that this result is not comparable to the ones from

Section 4 as it doesn’t follow longitudinal cross-validation.

5.2. Implications of Findings

As described in Section 1, the basic usage scenario of our tool is to provide

early warnings to authors, reviewers, and the management about review itera-

tions that will eventually be abandoned. As such, PredCR can be eﬀective for

the diverse major stakeholders in software engineering: (1) Project Manager

and leads to prioritize code review and code change eﬀorts based on the recom-

mendation from PredCR, (2) Software Developers to save time and eﬀorts

by focusing on code changes that will most likely be merged (as predicted by

PredCR), and (3) Software Engineering Researchers to further investigate

useful features like reviewer dimension in relevant early prediction tools.

Project Manager. This tool can provide beneﬁts to software project man-

agement.

If the management can predict the negative outcome of a review-

iteration early, they may analyze the cause and take necessary steps if required.

Multiple reasons may act behind the abandonment of changes such as resource

allocation, job environment, eﬃciency mismatch between the author and the

reviewer, and even their relations. Some of these may be addressed well by the

early intervention of the management and thus revert the result of a particular

review-iteration. Thus the company may save lots of time and resources.

Indeed, code review is a very important aspect of modern software engineer-

ing. Large software companies, as well as Open Source projects, are practicing

it with care. Researchers are trying to generate insight from large repositories

of code review and try to bring eﬃciency in the process to save the cost of

production. In this work, we study a relatively under-studied problem of pre-

dicting whether a code change will eventually be merged or abandoned at an

early stage of the code review process. We design a machine learning model to

apply carefully selected features generated as a result of communication between

the developers and the reviewers. Our developed tool PredCR is expected to

46

save wastage of eﬀort or help to recover from being abandoned by inviting early

intervention of the management.

Software Developers. The code review process requires serious eﬀort and

also is time-consuming. The authors and reviewers involved in a review iteration

are likely to get frustrated if they see that their eﬀort goes in vain, i.e., a patchset

has to be abandoned wasting their eﬀorts for quite some time. If they get an

early indication from our tool that their current review process is going in a

negative direction, they may become cautious, seek external/management help,

or at least be prepared mentally. If the management makes a decision early,

their eﬀorts would be saved. Thus it would beneﬁt the practitioners.

Software Engineering Researchers. Prior SE researchers followed diﬀer-

ent approaches including statistical methods, parametric models, and machine

learning (ML) methods for software eﬀort and duration prediction [36], software

cost prediction [37], software fault or defect prediction [38, 39], etc. Search-based

peer reviewer recommendation [40] is another related area. Diﬀerent prediction

models were introduced in the SE (Software Engineering) domain such as pre-

dicting the question quality [41] and response time [42] in Stack Overﬂow. In

these models, the authors exploit the interactions among users in diﬀerent con-

texts related to software engineering. Bosu et al.

[43] identiﬁed factors that

lead to useful code reviews. Some prior research suggests that a higher number

of reviewers reduces the number of bugs and increases the probability of accept-

ability [44, 45]. The experience of the reviewers sometimes leads to useful code

changes [45, 21].

6. Threats to Validity

Threats to internal validity refers to errors in our implementation. We

have cross-checked all data mined to ensure the data used is valid and contains

all changes available within that period. We have also removed changes for

which full data was not available (i.e. some old changes were missing patch data

from Gerrit response). We have rerun the pre-processing steps several times

47

to ensure the same statistics of the ﬁnal dataset. We have removed changes

for which the outcome is obvious (i.e. changes labeled “WIP” or “DO NOT

MERGE” etc). So that the dataset only contains changes for which prediction

is needed. To make the comparisons compatible with the state-of-the-art, we

have followed the process presented in their work and reproduced their feature

set and experimentation. Our experiments follow the longitudinal setup, which

prevents past data to be used in training. This same setup has been followed

by prior works [1, 3] in similar scenarios. As the dataset is an imbalanced one,

we need to prevent the model from being biased on the majority class. We

have balanced classiﬁcation loss to counter the data imbalance problem. We

have also shown the eﬀect of hyper-tuning on model performance. Then made

comparisons with the state-of-the-art [1] using all metrics presented by them.

Threats to external validity refers to the generalization of our tool. For

PredCR, it is validated by our test results for unseen data (Section 4.1). Also

despite having large feature importance for experience-related features, for new

contributors, PredCR still outperforms the state-of-the-art (Section 4.3). So

there is no threat to use this model even developers who are new or past tracks

are missing. Also with the increase of the training dataset, we have shown

a positive impact in test results in later half folds of our longitudinal cross-

validation result ( Section 4.5). Even when prediction is updated for each revi-

sion, PredCR shows signiﬁcant performance (Section 4.4). With diﬀerent num-

bers of K, PredCR will still show better predictions than the state-of-the-art

(Section 5.1.1). PredCR takes little time to train and test (Section 5.1.2) which

validates its practical usability. We have also shown in Section 5.1.6 PredCR

pre-trained on a project can still perform well for external projects. Even within

projects, we found there are sub-projects from diﬀerent domains and new sub-

projects keep getting added to the project over time. PredCR still maintains

a signiﬁcant overall performance. However, the prioritization given by PredCR

doesn’t imply the importance of the code change or how fast it will be closed.

So users need to be cautious when using PredCR in such scenarios.

Threats to construct validity refers to the suitability of our evaluation

48

metrics. We have used the evaluation metrics following prior works in the same

domain. Both AUC and cost-eﬀectiveness have been widely used in the pre-

diction models of software engineering studies [1, 7, 2]. We have presented

precision, recall, and f1-scores for both classes so that model performance for

both of them can be investigated. Also, the metrics have been calculated after

averaging over multiple runs of the experimentation. So we believe there is little

threat to the validity of our work in practice.

7. Conclusion

Modern code review is an integral part to ensure the quality and timely

delivery of software systems. Unfortunately, around 12% of the code changes in

a software system are abandoned, i.e, they are not merged to the main code base

of the software system. As such, any tool to detect such abandoned changes

well in advance can assist software teams with reduced time and eﬀort (e.g., by

prioritizing code changes for review that is most likely going to be merged). In

this paper, we present a tool named PredCR that can predict at an early stage

of a code review iteration whether a code change would be merged or abandoned

eventually. This tool is developed using a LightGBM-based classiﬁer following a

supervised learning approach including features related to the reviewer, author,

project, text, and code changes and a dataset of 146,612 code changes from three

Gerrit open source projects. PredCR outperforms the state-of-the-art tool by

Fan et al. [1] by 14-23% (in terms of AUC score) and achieves around 85% AUC

score on average. We have conducted an empirical study on the applicability

of PredCR. We ﬁnd that the new features like reviewer dimensions that are

introduced in PredCR are the most informative. We also ﬁnd that compared

to the baseline, PredCR is more eﬀective towards reducing bias against new

developers. PredCR uses historical data in the code review repository and as

such the performance of PredCR improves as a software system evolves with

new and more data. Therefore, PredCR oﬀers more accuracy over the state-

of-the-art baseline to early predict merged/abandoned code changes in diverse

49

use cases. As such, PredCR can help to reduce the waste of time and eﬀorts

of all stakeholders (e.g., program author, reviewer, project management, etc.)

involved in code reviews with early prediction, which can be used to prioritize

eﬀorts and time during the triaging of code changes for reviews.

References

[1] Y. Fan, X. Xia, D. Lo, S. Li, Early prediction of merged code changes to prioritize

reviewing tasks, Empirical Software Engineering (2018) 1–48.

[2] X. Xia, D. Lo, E. Shihab, X. Wang, X. Yang, Elblocker: Predicting blocking

bugs with ensemble imbalance learning, Information and Software Technology 61

(2015) 93–106.

[3] G. Jeong, S. Kim, T. Zimmermann, K. Yi, Improving code review by predicting

reviewers and acceptance of patches, Research on software analysis for error-free

computing center Tech-Memo (ROSAEC MEMO 2009-006) (2009) 1–18.

[4] A. Bacchelli, C. Bird, Expectations, outcomes, and challenges of modern code

review, in: 2013 35th International Conference on Software Engineering (ICSE),

IEEE, 2013, pp. 712–721.

[5] G. Gousios, M. Pinzger, A. v. Deursen, An exploratory study of the pull-based

software development model, in: Proceedings of the 36th International Conference

on Software Engineering, ACM, 2014, pp. 345–355.

[6] C. Costa, J. Figueiredo, A. Sarma, L. Murta, Tipmerge: recommending devel-

opers for merging branches, in: Proceedings of the 2016 24th ACM SIGSOFT

International Symposium on Foundations of Software Engineering, 2016, pp. 998–

1002.

[7] T. Jiang, L. Tan, S. Kim, Personalized defect prediction,

in:

2013 28th

IEEE/ACM International Conference on Automated Software Engineering

(ASE), Ieee, 2013, pp. 279–289.

[8] P. Thongtanunam, S. McIntosh, A. E. Hassan, H. Iida, Review participation in

modern code review, Empirical Software Engineering 22 (2) (2017) 768–817.

[9] G. Zhao, D. A. da Costa, Y. Zou, Improving the pull requests review process

using learning-to-rank algorithms, Empirical Software Engineering 24 (4) (2019)

2140–2170.

50

[10] V. Kovalenko, A. Bacchelli, Code review for newcomers: is it diﬀerent?, in: Pro-

ceedings of the 11th International Workshop on Cooperative and Human Aspects

of Software Engineering, 2018, pp. 29–32.

[11] Y. Jiang, B. Adams, D. M. German, Will my patch make it? and how fast? case

study on the linux kernel, in: 2013 10th Working Conference on Mining Software

Repositories (MSR), IEEE, 2013, pp. 101–110.

[12] V. J. Hellendoorn, P. T. Devanbu, A. Bacchelli, Will they like this?: Evaluating

code contributions with language models, in: Proceedings of the 12th Working

Conference on Mining Software Repositories, IEEE Press, 2015, pp. 157–167.

[13] Y. Huang, N. Jia, X. Zhou, K. Hong, X. Chen, Would the patch be quickly

merged?, in: International Conference on Blockchain and Trustworthy Systems,

Springer, 2019, pp. 461–475.

[14] C¸ . E. GEREDE, Z. Mazan, Will it pass? predicting the outcome of a source code

review, Turkish Journal of Electrical Engineering & Computer Sciences 26 (3)

(2018) 1343–1353.

[15] Y. Kamei, E. Shihab, B. Adams, A. E. Hassan, A. Mockus, A. Sinha, N. Ubayashi,

A large-scale empirical study of just-in-time quality assurance, IEEE Transactions

on Software Engineering 39 (6) (2013) 757–773.

[16] Y. Shin, R. Bell, T. Ostrand, E. Weyuker, Does calling structure information

improve the accuracy of fault prediction?, in: Mining Software Repositories, 2009.

MSR’09. 6th IEEE International Working Conference on, IEEE, 2009, pp. 61–70.

[17] X. Yang, R. G. Kula, N. Yoshida, H. Iida, Mining the modern code review repos-

itories: A dataset of people, process and product, in: Proceedings of the 13th

International Conference on Mining Software Repositories, 2016, pp. 460–463.

[18] P. C. Rigby, M.-A. Storey, Understanding broadcast based peer review on open

source software projects,

in: 2011 33rd International Conference on Software

Engineering (ICSE), IEEE, 2011, pp. 541–550.

[19] O. Baysal, O. Kononenko, R. Holmes, M. W. Godfrey, Investigating technical

and non-technical factors inﬂuencing modern code review, Empirical Software

Engineering 21 (3) (2016) 932–959.

[20] A. Mockus, D. M. Weiss, Predicting risk of software changes, Bell Labs Technical

Journal 5 (2) (2000) 169–180.

51

[21] O. Baysal, O. Kononenko, R. Holmes, M. W. Godfrey, The inﬂuence of non-

technical factors on code review, in: Reverse Engineering (WCRE), 2013 20th

Working Conference on, IEEE, 2013, pp. 122–131.

[22] S. Wang, C. Bansal, N. Nagappan, A. A. Philip, Leveraging change intents for

characterizing and identifying large-review-eﬀort changes, in: Proceedings of the

Fifteenth International Conference on Predictive Models and Data Analytics in

Software Engineering, 2019, pp. 46–55.

[23] A. E. Hassan, Predicting faults using the complexity of code changes, in: 2009

IEEE 31st international conference on software engineering, IEEE, 2009, pp. 78–

88.

[24] R. Moser, W. Pedrycz, G. Succi, A comparative analysis of the eﬃciency of change

metrics and static code attributes for defect prediction, in: Proceedings of the

30th international conference on Software engineering, 2008, pp. 181–190.

[25] N. Nagappan, T. Ball, A. Zeller, Mining metrics to predict component failures, in:

Proceedings of the 28th international conference on Software engineering, 2006,

pp. 452–461.

[26] M. S. Rakha, C.-P. Bezemer, A. E. Hassan, Revisiting the performance evalua-

tion of automated approaches for the retrieval of duplicate issue reports, IEEE

Transactions on Software Engineering 44 (12) (2017) 1245–1268.

[27] A. A. Bangash, H. Sahar, A. Hindle, K. Ali, On the time-based conclusion stability

of cross-project defect prediction models, Empirical Software Engineering 25 (6)

(2020) 5047–5083.

[28] J. H. Friedman, Stochastic gradient boosting, Computational Statistics & Data

Analysis 38 (4) (2002) 367–378.

[29] L. Breiman, Random forests, Machine learning 45 (1) (2001) 5–32.

[30] P. Geurts, D. Ernst, L. Wehenkel, Extremely randomized trees, Machine learning

63 (1) (2006) 3–42.

[31] G. Ke, Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye, T.-Y. Liu,

Lightgbm: A highly eﬃcient gradient boosting decision tree, in: Advances in

neural information processing systems, 2017, pp. 3146–3154.

[32] P. Weißgerber, D. Neu, S. Diehl, Small patches get in!, in: Proceedings of the

52

2008 international working conference on Mining software repositories, 2008, pp.

67–76.

[33] P. Probst, A.-L. Boulesteix, B. Bischl, Tunability: Importance of hyperparameters

of machine learning algorithms., J. Mach. Learn. Res. 20 (53) (2019) 1–32.

[34] J. W. Tukey, et al., Exploratory data analysis, Vol. 2, Reading, Mass., 1977.

[35] Q. Wang, X. Xia, D. Lo, S. Li, Why is my code change abandoned?, Information

and Software Technology 110 (2019) 108–120.

[36] M. Shepperd, G. Kadoda, Using simulation to evaluate prediction techniques [for

software], in: Software Metrics Symposium, 2001. METRICS 2001. Proceedings.

Seventh International, IEEE, 2001, pp. 349–359.

[37] M. Jorgensen, M. Shepperd, A systematic review of software development cost

estimation studies, IEEE Transactions on software engineering 33 (1) (2007).

[38] T. Hall, S. Beecham, D. Bowes, D. Gray, S. Counsell, A systematic literature re-

view on fault prediction performance in software engineering, IEEE Transactions

on Software Engineering 38 (6) (2012) 1276–1304.

[39] M. Shepperd, D. Bowes, T. Hall, Researcher bias: The use of machine learning

in software defect prediction, IEEE Transactions on Software Engineering 40 (6)

(2014) 603–616.

[40] A. Ouni, R. G. Kula, K. Inoue, Search-based peer reviewers recommendation in

modern code review, in: Software Maintenance and Evolution (ICSME), 2016

IEEE International Conference on, IEEE, 2016, pp. 367–377.

[41] A. Baltadzhieva, G. Chrupa la, Predicting the quality of questions on stackover-

ﬂow, in: Proceedings of the International Conference Recent Advances in Natural

Language Processing, 2015, pp. 32–40.

[42] J. Goderie, B. M. Georgsson, B. van Graafeiland, A. Bacchelli, Eta: Estimated

time of answer predicting response time in stack overﬂow, in: Mining Software

Repositories (MSR), 2015 IEEE/ACM 12th Working Conference on, IEEE, 2015,

pp. 414–417.

[43] A. Bosu, M. Greiler, C. Bird, Characteristics of useful code reviews: An empirical

study at microsoft, in: Mining Software Repositories (MSR), 2015 IEEE/ACM

12th Working Conference on, IEEE, 2015, pp. 146–156.

[44] G. Bavota, B. Russo, Four eyes are better than two: On the impact of code

53

reviews on software quality, in: Software Maintenance and Evolution (ICSME),

2015 IEEE International Conference on, IEEE, 2015, pp. 81–90.

[45] O. Kononenko, O. Baysal, L. Guerrouj, Y. Cao, M. W. Godfrey, Investigating code

review quality: Do people and participation matter?, in: Software Maintenance

and Evolution (ICSME), 2015 IEEE International Conference on, IEEE, 2015,

pp. 111–120.

54

