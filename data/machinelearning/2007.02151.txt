0
2
0
2

l
u
J

4

]

G
L
.
s
c
[

1
v
1
5
1
2
0
.
7
0
0
2
:
v
i
X
r
a

(1) 2020

Submitted pp; Published mm/dd

Variational Policy Gradient Method for
Reinforcement Learning with General Utilities

Junyu Zhang
Department of Industrial and Systems Engineering
University of Minnesota
Minneapolis, Minnesota, 55455

Alec Koppel
Computational and Information Sciences Directorate
US Army Research Laboratory
Adelphi, MD 20783

Amrit Singh Bedi
Computational and Information Sciences Directorate
US Army Research Laboratory
Adelphi, MD, USA 20783

Csaba Szepesvari
Department of Computer Science
DeepMind/University of Alberta
Princeton, NJ 08544

Mengdi Wang
Department of Electrical Engineering
Center for Statistics and Machine Learning
Princeton University/Deepmind
Princeton, NJ 08544

zhan4393@umn.edu

alec.e.koppel.civ@mail.mil

amrit0714@gmail.com

szepesva@ualberta.ca

mengdiw@princeton.edu

Abstract
In recent years, reinforcement learning (RL) systems with general goals beyond a cumulative sum
of rewards have gained traction, such as in constrained problems, exploration, and acting upon
prior experiences. In this paper, we consider policy optimization in Markov Decision Problems,
where the objective is a general concave utility function of the state-action occupancy measure,
which subsumes several of the aforementioned examples as special cases. Such generality invalidates
the Bellman equation. As this means that dynamic programming no longer works, we focus on
direct policy search. Analogously to the Policy Gradient Theorem Sutton et al. (2000) available for
RL with cumulative rewards, we derive a new Variational Policy Gradient Theorem for RL with
general utilities, which establishes that the parametrized policy gradient may be obtained as the
solution of a stochastic saddle point problem involving the Fenchel dual of the utility function. We
develop a variational Monte Carlo gradient estimation algorithm to compute the policy gradient
based on sample paths. We prove that the variational policy gradient scheme converges globally to
the optimal policy for the general objective, though the optimization problem is nonconvex. We
also establish its rate of convergence of the order O(1/t) by exploiting the hidden convexity of
the problem, and proves that it converges exponentially when the problem admits hidden strong
convexity. Our analysis applies to the standard RL problem with cumulative rewards as a special
case, in which case our result improves the available convergence rate.

c(cid:13)2020 Junyu Zhang, Alec Koppel, Amrit Singh Bedi, Csaba Szepesvari, and Mengdi Wang.

 
 
 
 
 
 
Zhang, Koppel, Bedi, Szepesvari, and Wang

1. Introduction

The standard formulation of reinforcement learning (RL) is concerned with ﬁnding a policy that
maximizes the expected sum of rewards along the sample paths generated by the policy. The additive
nature of the objective function creates an attractive algebraic structure which most eﬃcient RL
algorithms exploit. However, the cumulative reward objective is not the only one that has attracted
attention. In fact, many alternative objectives made appearances already in the early literature on
stochastic optimal control and operations research. Examples include various kinds of risk-sensitive
objectives Kallenberg (1994); Borkar and Meyn (2002); Yu et al. (2009); Mannor and Tsitsiklis
(2011), objectives to maximize the entropy of the state visitation distribution Hazan et al. (2018),
the incorporation of constraints Derman and Klein (1965); Altman (1999); Achiam et al. (2017), and
learning to “mimic” a demonstration Schaal (1997); Argall et al. (2009).

In this paper, we consider RL with general utility functions, and we aim to develop a principled
methodology and theory for policy optimization in such problems. We focus on utility functions that
are concave functionals of the state-action occupancy measure, which contains many, although not
all, of the aforementioned examples as special cases.The general (or non-standard Kallenberg (1994))
utility is a strict generalization of cumulative reward, which itself can be viewed as a linear functional
of the state-action occupancy measure, and as such, is a concave function of the occupancy measures.
When moving beyond cumulative rewards, we quickly run into technical challenges because of
the lack of additive structure. Without additivity of rewards, the problem becomes non-Markovian
in the cost-to-go Tak´acs (1966); Whitehead and Lin (1995). Consequently, the Bellman equation
fails to hold and dynamic programming (DP) breaks down. Therefore, stochastic methods based
upon DP such as temporal diﬀerence Sutton (1988) and Q-learning Watkins and Dayan (1992); Ross
(2014) are inapplicable. The value function, the core quantity for RL, is not even well deﬁned for
general utilities, thus invalidating the foundation of value-function based approach to RL.

Due to these challenges, we consider direct policy search methods for the solution of RL problems
deﬁned by general utility functions. We consider the most elementary policy-based method, namely
the Policy Gradient (PG) method Williams (1992). The idea of policy gradient methods is that to
represent policies through some policy parameterization and then move the parameters of a policy in
the direction of the gradient of the objective function. When (as typical) only a noisy estimate of the
gradient is available, we arrive at a stochastic approximation method Robbins and Monro (1951);
Kiefer et al. (1952). In the classical cumulative reward objectives, the gradient can be written as
the product of the action-value function and the gradient of the logarithm of the policy, or policy
score function Sutton et al. (2000). State-of-the-art RL algorithms for the cumulative reward setting
combine this result with other ideas, such as limiting the changes to the policies S Kakade (2002);
Schulman et al. (2015, 2017), variance reduction Kakade (2002); Papini et al. (2018); Xu et al. (2019),
or exploiting structural aspects of the policy parameterization Wang et al. (2019); Agarwal et al.
(2019); Mei et al. (2020).

As mentioned, these approaches crucially rely on the standard PG Theorem Sutton et al. (2000),
which is not available for general utilities. Compounding this challenge is the fact that the action-value
function is not well-deﬁned in this instance, either. Thus, how and whether the policy gradient can
be eﬀectively computed becomes a question. Further, due to the problem’s nonconvexity, it is an
open question whether an iterative policy improvement scheme converges to anything meaningful: In
particular, while standard results for stochastic approximation would give convergence to stationary
points Borkar (2009), it is unclear whether the stationary points give reasonable policies. Therefore,
we ask the question:

Is policy search viable for general utilities,
when Bellman’s equation, the value function, and dynamic programming all fail?

We will answer the question positively in this paper. Our contributions are three-folded:

2

Variational Policy Gradient

• We derive a Variational Policy Gradient Theorem for RL with general utilities which establishes
that the parametrized policy gradient is the solution to a stochastic saddle point problem.

• We show that the Variational Policy Gradient can be estimated by a primal-dual stochastic
approximation method based on sample paths generated by following the current policy Arrow
et al. (1958). We prove that the random error of the estimate decays at order O(1/
n) that
also depends on properties of the utility, where n is the number of episodes .

√

• We consider the non-parameterized policy optimization problem which is nonconvex in the
policy space. Despite the lack of convexity, we identify the problem’s hidden convexity, which
allows us to show that a variational policy gradient ascent scheme converges to the global
optimal policy for general utilities, at a rate of O(1/t), where t is the iteration index. In the
special case of cumulative rewards, our result improves upon the best known convergence rate
O(1/
t) for tabular policy gradient Agarwal et al. (2019), and matches the convergence rate of
variants of the algorithm such as softmax policy gradient Mei et al. (2020) and natural policy
gradient Agarwal et al. (2019). In the case where the utility is strongly concave in occupancy
measures (e.g., utilities involving Kullback-Leiber divergence), we established the exponential
convergence rate of the variational gradient scheme.

√

Related Work. Policy gradient methods have been extensive studied for RL with cumulative
returns. There is a large body of work on variants of policy-based methods as well as theoretical
convergence analysis for these methods. Due to space constraints, we defer a thorough review to
Supplement A.

Notation. We let R denote the set of reals. We also let (cid:107) · (cid:107) denote the 2-norm, while for matrices
we let it denote the spectral norm. For the p-norms (1 ≤ p ≤ ∞), we use (cid:107) · (cid:107)p. For any matrix B,
(cid:107)B(cid:107)∞,2 := max(cid:107)u(cid:107)∞≤1 (cid:107)Bu(cid:107)2. For a diﬀerentiable function f , we denote by ∇f its gradient. If f
is nondiﬀerentiable, we denote by ˆ∂f the Fr´echet superdiﬀerential of f ; see e.g. Drusvyatskiy and
Paquette (2019).

2. Problem Formulation

Consider a Markov decision process (MDP) over the ﬁnite state space S and a ﬁnite action space A.
For each state i ∈ S, a transition to state j ∈ S occurs when selecting action a ∈ A according to a
conditional probability distribution j ∼ P(·|a, i), for which we deﬁne the short-hand notation Pa(i, j).
Let ξ be the initial state distribution of the MDP. We let S denote the number of states and A the
number of actions. The goal is to prescribe actions based on previous states in order to maximize
some long term objective. We call π : S → P (A) a policy that maps states to distributions over
actions, which we subsequently stipulate is stationary. In the standard (cumulative return) MDP,
the objective is to maximize the expected cumulative sum of future rewards Puterman (2014), i.e.,

V π(s) := E

max
π

(cid:34) ∞
(cid:88)

t=0

γtrstat

(cid:12)
(cid:12)
(cid:12)
(cid:12)

i0 = s, at ∼ π(·|st), t = 0, 1, . . .

,

∀s ∈ S.

(2.1)

(cid:35)

with reward rstat ∈ R revealed by the environment when action at is chosen at state st.

In this paper we consider policy optimization for maximizing general objective functions that are

not limited to cumulative rewards. In particular, we consider the problem

R(π) := F (λπ)

max
π

(2.2)

where λπ is known as the cumulative discounted state-action occupancy measure, or ﬂux under policy
π, and F is a general concave functional. Denote ∆S
A and L as the set of policy and ﬂux respectively,

3

Zhang, Koppel, Bedi, Szepesvari, and Wang

then λπ is given by the mapping Λ : ∆S

A (cid:55)→ L as

λπ
sa = Λsa(π) :=

∞
(cid:88)

t=0

(cid:16)

γt · P

st = s, at = a

(cid:12)
(cid:12)
(cid:12) π, s0 ∼ ξ

(cid:17)

for ∀a ∈ A, ∀s ∈ S .

(2.3)

Similar to the LP formulation of a standard MDP, we can write (2.2) equivalently as an optimiza-

tion problem in λ (see Zhang et al. (2020a)), giving rise to

max
λ

F (λ)

s.t.

(cid:88)

a∈A

(I − γP (cid:62)

a )λa = ξ, λ ≥ 0,

(2.4)

where λa = [λ1a, · · · , λSa](cid:62) ∈ RA is the a-th column of λ and ξ is the initial distribution over the
state space S. The constraints require that λ be the unnormalized state-action occupancy measure
corresponding to some policy. In fact, it is well known that a policy π inducing λ can be extracted
from λ using the mapping Π : L (cid:55)→ ∆S
A as π(a|s) = Πsa(λ) :=

for all a, s.

(cid:80)

λsa
a(cid:48) ∈A λsa(cid:48)

Problem (2.2) contains the original MDP problem as a special case. To be speciﬁc, when
(cid:12)
F (λ) = (cid:104)r, λ(cid:105) with r ∈ RSA as the reward function, then F (λ) = (cid:104)λ, r(cid:105) = E(cid:2) (cid:80)∞
(cid:12) π, s0 ∼ ξ(cid:3).
This means that (2.4) is a generalization of (2.1), and reduces to the dual LP formulation of standard
MDP for this (linear) choice of F (·) Kallenberg (1983). We focus on the case where F is concave,
which makes (2.4) a concave (hence, convenient) maximization problem. Next we introduce a few
examples that arise in practice for incentivizing safety, exploration, and imitation, respectively.

t=0 γtrstat

Example 2.1 (MDP with Constraints or Barriers). In discounted constrained MDPs the goal
is to maximize the total expected discounted reward under a constraint where for some cost function
c : S × A → R, the total expected discounted cost incurred by the chosen policy is constrained
from above. Letting r denote the reward function over S × A, the underlying optimization problem
becomes

max
π

r := Eπ
vπ

(cid:34) ∞
(cid:88)

t=0

(cid:35)

γtr(st, at)

s.t.

c := Eπ
vπ

(cid:34) ∞
(cid:88)

(cid:35)
γtc(st, at)

≤ C.

t=0

(2.5)

As is well known, a relaxed formulation is

max
λ

F (λ) := (cid:104)λ, r(cid:105) − β · p((cid:104)λ, c(cid:105) − C)

s.t.

(I − γP (cid:62)

a )λa = ξ, λ ≥ 0.

(2.6)

(cid:88)

a∈A

where p is a penalty function (e.g., the log barrier function).

Example 2.2 (Pure Exploration). In the absence of a reward function, an agent may consider
the problem of ﬁnding a policy whose stationary distribution has the largest “entropy”, as this should
facilitate maximizing the speed at which the agent explores its environment Hazan et al. (2018):

R(π) := Entropy(¯λπ),

max
π

(2.7)

where ¯λπ is the normalized state visitation measure given by ¯λπ
sa for all s. Various
entropic measures are possible, but the simplest is the negative log-likelihood: Entropy(¯λπ) =
− (cid:80)
s
Another example, when d state-action features φ(s, a) ∈ Rd are available, is to cover the entire

s ]. As is well known, this entropy is (strongly) concave.

s = (1 − γ) (cid:80)

s log[¯λπ
¯λπ

a λπ

feature space by maximizing the smallest eigenvalue of the covariance matrix:

max
π

R(π) := σmin

(cid:32)

Eπ

(cid:34) ∞
(cid:88)

t=1

γtφ(st, at)φ(st, at)(cid:62)

.

(cid:35)(cid:33)

(2.8)

4

Variational Policy Gradient

In (2.8), observe that Eπ[(cid:80)∞

t=1 γtφ(st, at)φ(st, at)(cid:62)] = (cid:80)

sa λπ

sa · φ(s, a)φ(s, a)(cid:62). By Rayleigh

principle, it is again a concave function of λ.

Example 2.3 (Learning to mimic a demonstration). When demonstrations are available, they
may be employed to obtain information about a prior policy in the form of a state visitation
distribution ¯µ. Remaining close to this prior can be achieved by minimizing the Kullback-Liebler
(KL) divergence between the state marginal distribution of λ and the prior ¯µ stated as

F (λ) = KL

(cid:16)

(1 − γ)

(cid:88)

(cid:17)

λa||¯µ

a

(2.9)

which, when substituted into (2.4), yields a method for ensuring some baseline performance. We
further note that in place of KL divergence, one can also use other convex distances such as
Wasserstein, total variation, or Hellinger distances.

Additional instances may be found in Zhang et al. (2020a). With the setting clariﬁed, we shift

focus to developing an algorithmic solution to (2.4), that is, to solve for policy π.

3. Variational Policy Gradient Theorem

To handle the curse of dimensionality, we allow parametrization of the policy by π = πθ, where
θ ∈ Θ ⊂ Rd is the parameter vector. In this way, we can narrow down the policy search problem to
within a d-dimensional parameter space rather than the high-dimensional space of tabular policies.
The policy optimization problem then becomes

max
θ∈Θ

R(πθ) := F (λπθ )

(3.1)

where F is the concave utility of the state-action occupancy measure λ(θ) := λπθ , Θ ⊂ Rd is a
convex set. We seek to solve for the policy maximizing the utility as in (3.1) using gradient ascent
over the parameter space Θ. Note that (3.1) is simply (2.2) with parameterization θ of policy π
substituted. We denote by ∇θR(πθ) the parameterized policy gradient of general utility.

First, recall the policy gradient theorem for RL with cumulative rewards Sutton et al. (2000).
Let the reward function be r. Deﬁne V (θ; r) := (cid:104)λ(θ), r(cid:105), i.e., the total expected discounted reward
under the reward function r and the policy πθ. The Policy Gradient Theorem states that

∇θV (θ; r) = Eπθ

(cid:34) ∞
(cid:88)

t=0

γtQπθ (st, at; r) · ∇θ log πθ(at|st)

,

(3.2)

(cid:35)

where Qπ(s, a; r) := Eπ(cid:2) (cid:80)
t γtr(st, at) | s0 = s, a0 = a, at ∼ π(· | st)(cid:3). Unfortunately, this elegant
result no longer holds when we consider a general function instead of cumulative rewards: The policy
gradient theorem relies on the additivity of rewards, which is lost in our problem. For future reference,
we denote Qπ(s, a; z) := Eπ(cid:2) (cid:80)
t γtzstat | s0 = s, a0 = a, at ∼ π(· | st)(cid:3) where z is any “function”
of the state-action pairs (z ∈ RSA). Moreover, V (θ; z) is deﬁned similarly. These deﬁnitions are
motivated by subsequent eﬀorts to derive an expression for the gradient of (3.1).

3.1 Policy Gradient of R(πθ)

Now we derive the policy gradient of R(πθ) with respect to θ. By the chain rule, the gradient of
F (λ(θ)) := F (λπθ ), using the deﬁnition of R(πθ), yields (assuming diﬀerentiability of F, λ):

∇θR(πθ) =

(cid:88)

(cid:88)

s∈S

a∈A

∂F (λ(θ))
∂λsa

· ∇θλsa(θ).

(3.3)

5

Zhang, Koppel, Bedi, Szepesvari, and Wang

To directly use the chain rule, one needs the partial derivatives ∂F (λ(θ))
and ∇θλsa(θ). Unfortunately,
neither of them is easy to estimate. The partial gradient ∂F (λ(θ))
is a function of the current
state-action occupancy measure λπθ . One might attempt to estimate the measure λπθ and then
evaluate the gradient map ∂F (λ(θ))
. However, estimates of distributions over large spaces tend to
converge very slowly Tsybakov (2008).

∂λsa

∂λsa

∂λsa

As it turns out, a viable alternate route is to consider the Fenchel dual F ∗ of F . Recall that
F ∗(z) = inf λ(cid:104)λ, z(cid:105) − F (λ), where we use (cid:104)x, y(cid:105) := x(cid:62)y (since F is concave, the dual is deﬁned using
inf, instead of sup). As is well known, for F concave, under mild regularity conditions, the bidual
(dual of the dual) of F is equal to F . This forms the basis of our ﬁrst result, which states that the
steepest policy ascent direction of (3.1) is the solution to a stochastic saddle point problem. The
proofs of this and subsequent results are given in the supplementary material.

Theorem 3.1 (Variational Policy Gradient Theorem). Suppose F is concave and continuously
diﬀerentiable in an open neighborhood of λπθ . Denote V (θ; z) to be the cumulative value of policy πθ
when the reward function is z, and assume ∇θV (θ; z) always exists. Then we have
(cid:26)

(cid:27)

V (θ; z) + δ∇θV (θ; z)(cid:62)x − F ∗(z) −

(cid:107)x(cid:107)2

.

(3.4)

∇θR(πθ) = lim
δ→0+

argmax
x

inf
z

δ
2

Therefore, to estimate ∇θR(πθ) we require the cumulative return V (θ; z) of the function z, its
associated “vanilla” policy gradient (3.2), and the gradient of the Fenchel dual of F at z. These
ingredients are combined via (3.4) to obtain a valid policy gradient for general objectives. Next, we
discuss how to estimate the gradient using sampled trajectories.

3.2 Estimating the Variational Policy Gradient

Theorem 3.1 implies that one can estimate ∇θR(πθ) by solving a stochastic saddle point problem.
Suppose we generate n i.i.d. episodes of length K following πθ, denoted as ζi = {s(i)
k=1. Then
we can estimate V (θ; z) and ∇V (θ; z) for any function z by

k , a(i)

k }K

˜V (θ; z) :=

∇ ˜V (θ; z) :=

1
n

1
n

n
(cid:88)

i=1

(cid:88)

i=1

V (θ; z; ζi) :=

1
n

n
(cid:88)

K
(cid:88)

i=1

k=1

γk · z(s(i)

k , a(i)
k ),

∇θV (θ; z; ζi) :=

1
n

n
(cid:88)

K
(cid:88)

(cid:88)

i=1

k=1

a∈A

γk · Q(s(i)

k , a; z)∇θπθ(a|s(i)
k ).

(3.5)

For a given value of K, the error introduced by “truncating” trajectories at length K is of order
γK/(1 − γ), which quickly decays to zero for γ < 1. Plugging in the obtained estimates into (3.4)
gives rise to the sample-average approximation to the policy gradient:

ˆ∇θR(πθ; δ) := argmax

x

(cid:26)

inf
(cid:107)z(cid:107)∞≤(cid:96)F

−F ∗(z) + ˜V (θ; z) + δ∇θ ˜V (θ; z)(cid:62)x −

(cid:27)

,

(cid:107)x(cid:107)2

δ
2

(3.6)

where (cid:96)F is deﬁned in the next theorem. Therefore, any algorithm that solves problem (3.6) will serve
our purpose. A MC stochastic approximation scheme, i.e., Algorithm 1, is provided in Appendix B.1.

Theorem 3.2 (Error bound of policy gradient estimates). Suppose the following holds:
(i) domF = RSA, there exists (cid:96)F such that max{(cid:107)∇F (λ)(cid:107)∞ : (cid:107)λ(cid:107)1 ≤ 2
(ii) F is LF -smooth under L1 norm, i.e., (cid:107)∇F (λ) − ∇F (λ(cid:48))(cid:107)∞ ≤ LF (cid:107)λ − λ(cid:48)(cid:107)1.
(iii) F ∗ is ((cid:96)F ∗ )-Lipschitz with respect to the L∞ norm in the set {z : (cid:107)z(cid:107)∞ ≤ 2(cid:96)F , F ∗(z) > −∞}.
(iv) There exists C with (cid:107)∇θπ(·|s)(cid:107)∞,2 ≤ C, where ∇θπ(·|s) = [∇θπ(1|s), · · · , ∇θπ(A|s)].
Let ˆ∇θR(πθ) := limδ→0+

ˆ∇θR(πθ; δ). Then

1−γ } ≤ (cid:96)F .

E[(cid:107) ˆ∇θR(πθ) − ∇θR(πθ)(cid:107)2] ≤ O

(cid:18) C 2((cid:96)2

F (cid:96)2
F + L2
n(1 − γ)4

F ∗ )

(cid:19)

+

C 2L2
F
n(1 − γ)6

+ O(γK).

6

Variational Policy Gradient

Remarks.

√

(1) Theorem 3.2 suggests an O(1/
n) error rate, proving that the variational policy gradient -
though more complicated than the typical policy gradient that takes the form of a mean - can be
eﬃciently estimated from ﬁnite data.
(2) Although the variable z is high dimensional, our error bound depends only on the properties of
F .
(3) We assumed for simplicity that Q values are known. In practice, they can estimated by, e.g., an
additional Monte Carlo rollout on the same sample path or temporal diﬀerence learning. As long as
the estimator for Q(s, a; z) is unbiased and upper bouded by O( (cid:107)z(cid:107)∞
(4) For the case of cumulative rewards, we have F (λ) = (cid:104)r, λ(cid:105), so that (cid:96)F = (cid:107)r(cid:107)∞, (cid:96)F ∗ =0, LF =0.
Therefore E[(cid:107) ˆ∇θR(πθ) − ∇θR(πθ)(cid:107)2] ≤ O

1−γ ), the result will not change.

(cid:18)

(cid:19)

.

C2(cid:107)r(cid:107)2
∞
n(1−γ)4

Special cases of ∇θR(πθ). We further explain how to obtain the variational policy gradient
for several special cases of R, including constrained MDP, maximal exploration, and learning from
demonstrations. See Appendix B.2 for more details.

4. Global Convergence of Policy Gradient Ascent

In this section, we analyze policy search for the problem (3.1), i.e., maxθ∈Θ R(πθ) via gradient ascent:

θk+1 = argmax

θ∈Θ

R(πθk )+(cid:10)∇θR(πθk ), θ−θk(cid:11)−

1
2η

(cid:107)θ−θk(cid:107)2 = Proj Θ

(cid:8)θk + η∇θR(πθk )(cid:9)

(4.1)

where Proj Θ{·} denotes Euclidean projection onto Θ, and equivalence holds by the convexity of Θ.

4.1 No spurious ﬁrst-order stationary solutions.

We study the geometry of the (possibly) nonconvex optimization problem (3.1). When F is a linear
function of λ, and the parameterization is tabular or softmax, existing theory of cumulative-return
RL problems have shown that every ﬁrst-order stationary point of (3.1) is globally optimal – see
Agarwal et al. (2019); Mei et al. (2020).

In what follows, we show that the problem (3.1) has no spurious extrema despite of its nonconvexity,
for general utility functions and policy parametrization. Speciﬁcally, to generalize global optimality
attributes of stationary points of (3.1) from (2.1), we exploit structural aspects of the relationship
between occupancy measures and parameterized families of policies, namely, that these entities are
related through a bijection. This bijection, when combined with the fact that (3.1) is concave in λ,
and suitably restricting the parameterized family of policies, is what we subsequently describe as
“hidden convexity.” For these results to be valid, we require the following regularity conditions.

Assumption 4.1. Suppose the following holds true:
(i). λ(·) forms a bijection between Θ and λ(Θ), where Θ and λ(Θ) are closed and convex.
(ii). The Jacobian matrix ∇θλ(θ) is Lipschitz continuous in Θ.
(iii). Denote g(·) := λ−1(·) as the inverse mapping of λ(·). Then there exists (cid:96)θ > 0 s.t. (cid:107)g(λ) −
g(λ(cid:48))| ≤ (cid:96)θ|||λ − λ(cid:48)||| for some norm ||| · ||| and for all λ, λ(cid:48) ∈ λ(Θ).

In particular, for the direct policy parametrization, also known as the “tabular” policy case, we
have λ(θ) := Λ(π) where Λ is deﬁned in (2.3). When ξ is positive-valued, Assumption 4.1 is true for
the tabular policy case (as established in Appendix H).

Theorem 4.2 (Global optimality of stationary policies). Suppose Assumption 4.1 holds, and
F is a concave, and continuous function deﬁned in an open neighbourhood containing λ(Θ). Let θ∗
be a ﬁrst-order stationary point of problem (3.1), i.e.,
∃u∗ ∈ ˆ∂(F ◦ λ)(θ∗),

(cid:104)u∗, θ − θ∗(cid:105) ≤ 0

∀θ ∈ Θ.

(4.2)

s.t.

for

7

Zhang, Koppel, Bedi, Szepesvari, and Wang

Then θ∗ is a globally optimal solution of problem (3.1).

Theorem 4.2 provides conditions such that, despite of nonconvexity, local search methods can
ﬁnd the global optimal policies. Since we aim at general utilities, we naturally separated out the
convex and non-convex maps in the composite objective and our conditions for optimality rely on the
properties of these. In a recent paper, Bhandari and Russo (2020) proposed some suﬃcient conditions
under which a result similar to Theorem 4.2 holds in the setting of the standard, cumulative total
reward criterion. Their conditions are (i) the policy class is closed under (one-step, weighted) policy
improvement and that (ii) all stationary points of the one-step policy improvement map are global
optima of this map. It remains for future work to see the relationship between our conditions and
these conditions: They appear to have rather diﬀerent natures.

4.2 Convergence analysis

Now we analyze the convergence rate of the policy gradient scheme (4.1) for general utilities.

Assumption 4.3. There exists L > 0 such that the policy gradient ∇θR(πθ) is L-Lipschitz.

The objective R(πθ) is nonconvex in θ, so one might expect that gradient schemes converge to
stationary solutions at a standard O(1/
t) convergence rate Shapiro et al. (2014). Remarkably, the
policy optimization problem admits a convex nature if we view it in the space of λ, as long as F is
concave. By exploiting this hidden convexity, we establish an O(1/t) convergence rate for solving RL
with general utilities. Further, we show that, when the utility F is strongly concave, the gradient
ascent scheme converges to the globally optimal policy exponentially fast.

√

Theorem 4.4 (Convergence rate of parameterized policy gradient iteration). Let Assump-
tions 4.1 and 4.3 hold. Denote Dλ := maxλ,λ(cid:48)∈λ(Θ) |||λ − λ(cid:48)||| as deﬁned in Assumption 4.1(iii). Then
the policy gradient update (4.1) with η = 1/L satisﬁes for all k
θD2
λ
k + 1

R(πθ∗ )−R(πθk ) ≤

4L(cid:96)2

.

Additionally, if F (·) is µ-strongly concave with respect to the ||| · ||| norm, we have
(cid:16)

(cid:17)k

1

R(πθ∗ )−R(πθk ) ≤

1−

(R(πθ∗ )−R(πθ0)) .

1+L(cid:96)2

θ/µ

The exponential convergence result of Theorem 4.4 implies that, when a regularizer like Kullback-
Leiber divergence is used, policy gradient method converges much faster. In other words, policy
search with general utilities can actually be easier than the typical, cumulative-return problem.

Finally, we study the case where policies are not parameterized, i.e., θ = π. The next theorem

establishes a tighter convergence rate than what Theorem 4.4 already implies.

Theorem 4.5 (Convergence rate of tabular policy gradient iteration). Let θ = π and
λ(θ) = Λ(π). Let Assumption 4.3 hold and assume that ξ is positive-valued. Then the iterates
generated by (4.1) with η = 1/L satisfy for all k ≥ 1 that
20L|S|
(1 − γ)2(k + 1)

R(π∗) − R(πk) ≤

(cid:13)
(cid:13)dπ∗
(cid:13)

ξ /ξ

(cid:13)
2
(cid:13)
(cid:13)

∞

·

.

The case of cumulative rewards. Let us consider the well-studied special case where F is a linear
functional, i.e., R(π) = V π [cf. (2.1)] is the typical cumulative return. In this case, we have L = 2γA
(1−γ)3
(Agarwal et al. (2019)). Now in order to obtain an (cid:15)-optimal policy ¯π such that V π∗
−V ¯π ≤ (cid:15), the
(cid:17) iterations according to Theorem 4.5. This
ξ /ξ(cid:13)
2
gradient ascent update requires O
(cid:13)
∞
(cid:17) iteration complexity proved by Agarwal et al.
ξ /ξ(cid:13)
2
bound is strictly smaller than the O
(cid:13)
∞
(2019) for tabular policy gradient. The improvement from O(1/(cid:15)2) to (1/(cid:15)) comes from the fact that,
although the policy optimization problem is nonconvex, our analysis exploits its hidden convexity in
the space of λ.

(1−γ)5(cid:15)
SA
(1−γ)6(cid:15)2

· (cid:13)
(cid:13)dπ∗
(cid:13)
(cid:13)dπ∗

(cid:16) SA

(cid:16)

8

Variational Policy Gradient

5. Experiments
Now we shift to numerically validating our methods and theory on OpenAI Frozen Lake Brockman
et al. (2016). Throughout, additional details may be found in Appendix C.

Figure 1: PG estimation via Alg.
1 Cosine similarity between
PG estimates ˆxt generated
by Algorithm 1 after t sam-
ples and the ground truth
x(cid:63), which consistently con-
verges to near 1 across dif-
ferent instances (E.g.
(2.1)
-
(2.3)) when t becomes
large. For comparison, we
also include the convergence
of PG estimates from RE-
INFORCE for cumulative re-
turns.

(a) Entropy vs. # episodes

(b) World & occupancy dist. (En-
tropy)

Figure 2: Results for maximum entropy exploration: In Fig. 2(a), to
quantify exploration, we present the entropy of ﬂux λ over train-
ing index n for our approach, as compared with the entropy of a
uniform random policy. Fig. 2(b)(bottom) visualizes the world
model (holes in the lake have null entropy, as they terminate the
episode), the lower middle layer displays the occupancy measure
associated with a uniformly random policy, the upper-middle vi-
sualizes the pseudo-reward z∗ deﬁned by the Fenchel dual of the
entropy (2.7) – see Appendix B.2. Lastly, on top we visualize the
occupancy measure associated with the max entropy policy, which
better covers the space than a uniformly random policy.

Policy Gradient (PG) Estimation. First we investigate the use of Theorem 3.1 and Algorithm
1 (Appendix B.1) for PG estimation, for several instances of the general utility. We also compare
it with the gradient estimates computed by REINFORCE for cumulative returns. Speciﬁcally, in
Figure 1 we illustrate the convergence of gradient estimates, measured using the cosine similarity
between xn (running estimate based on n episodes) and the true gradient x∗ (which is evaluated
using brute force Monte Carlo rollouts – see Appendix C.2). The cosine similarity converges to 1
across diﬀerent instances, providing evidence that Algorithm 1 yields consistent gradient estimates
for general utilities.
PG Ascent for Maximal Entropy Exploration. Next, we consider maximum entropy exploration
(2.7) using algorithm (4.1), with softmax parametrization. First, we display the evolution of the
entropy of the normalized occupancy measure over the number of episodes in Fig. 2(a). Then, we
visualize the world model in Fig. 2(b)(bottom). Moreover, the lower middle is the occupancy measure
associated with a uniformly random policy, the upper-middle layer visualizes the ”pseudo-reward” z∗
computed as the Fenchel dual of the entropy (2.7) – see Appendix B.2, which is null at the holes
and positive otherwise. We use a diﬀerent color to denote that its values are not likelihoods. The
occupancy measure obtained by policy gradient ascent with gradient estimated by Algorithm 1 at
the end of training is in Figure 2(b)(top) – observe the maximal entropy policy achieves signiﬁcantly
better coverage of the state space than the uniformly random policy.
PG Ascent for Avoiding Obstacles. Suppose our goal is to navigate the Frozen Lake and avoid
obstacles. We consider imposing penalties to avoid costly states [cf. (2.6)] via a logarithmic barrier
(B.3), and by applying variational PG ascent, we obtain an optimal policy whose resulting occupancy
measure is depicted in Fig. 3(a)(top). For comparison, we consider optimizing the standard expected
cumulative return (2.1), whose state occupancy measure is given in Fig. 3(a)(middle). Observe that
imposing log penalties yields policies whose probability mass is concentrated away from obstacles
(dark green). Further, we display in Fig. 3 the reward 3(b) and cost 3(c) accumulation during test

9

Zhang, Koppel, Bedi, Szepesvari, and Wang

(a) World & occupancy dist.
(CMDP)

(b) Reward vs. # episodes

(c) Cost vs. # episodes

Figure 3: Results for avoiding obstacles. Fig. 3(a)(bottom) depicts the world model of OpenAI Frozen Lake with
augmentation to include costly states, e.g., obstacles: C represents costly states, F is the frozen lake, H is the
hole, and G is the goal. We consider softmax policy parameterization, and visualize the occupancy measure
associated with REINFORCE for the cumulative return (2.1) in the middle layer, and the relaxed CMDP (2.6)
via a logarithmic barrier (B.3) at the top.The policy obtained via barriers avoids visiting costly states, in

contrast to the middle. Fig. 3(b) and Fig. 3(c) show the reward/cost accumulated during test trajectories over
training index for Algorithm 1. Observe that the reward/cost curves behave diﬀerently as the penalty parameter
β varies: observe that without any constraint imposition (which implies β = 0 in red), one achieves the highest
reward, but incurs the most costs, i.e., hits obstacles most often. Larger β imposes more penalty, and hence
β = 4 incurs lowest cost and lowest reward. Other instances are also shown for β = 1 and β = 2.

trajectories as a function of the iteration index for the PG ascent (4.1) for the cumulative return (2.1)
as compared with a logarithmic barrier imposed to solve (2.6) for diﬀerent penalty parameters β.

6. Broader Impact

While RL has a great number of potential applications, our work is of foundational nature and as
such, the application of the ideas in this paper can have both broad positive and negative impacts.
However, this paper is purely theoretical, as we do not aim at any speciﬁc application, there is nothing
we can say about the most likely broader impact of this work that would go beyond speculation.

References

Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In
Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 22–31.
JMLR. org, 2017.

Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. Optimality and approximation
with policy gradient methods in Markov decision processes. arXiv preprint arXiv:1908.00261, 2019.

Eitan Altman. Constrained Markov decision processes, volume 7. CRC Press, 1999.

Brenna D Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. A survey of robot learning

from demonstration. Robotics and autonomous systems, 57(5):469–483, 2009.

K.J. Arrow, L. Hurwicz, and H. Uzawa. Studies in Linear and Non-Linear Programming, volume II
of Stanford Mathematical Studies in the Social Sciences. Stanford University Press, Stanford,
December 1958.

Jalaj Bhandari and Daniel Russo. Global optimality guarantees for policy gradient methods. arXiv

arXiv:1906.01786, 2019.

Jalaj Bhandari and Daniel Russo. Global optimality guarantees for policy gradient methods. working

paper, 2020. URL https://djrusso.github.io/docs/policy_grad_optimality.pdf.

10

Variational Policy Gradient

Shalabh Bhatnagar, Richard Sutton, Mohammad Ghavamzadeh, and Mark Lee. Natural actor-critic

algorithms. Automatica, 45(11):2471–2482, 2009.

Vivek S Borkar. Stochastic approximation: A dynamical systems viewpoint. Cambridge University

Press, 2008.

Vivek S. Borkar. Stochastic Approximation: A Dynamical Systems Viewpoint, volume 77. Wiley,

2009.

Vivek S Borkar and Sean P Meyn. Risk-sensitive optimal control for Markov decision processes with

monotone cost. Mathematics of Operations Research, 27(1):192–209, 2002.

Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and

Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.

Jingjing Bu, Afshin Mesbahi, Maryam Fazel, and Mehran Mesbahi. LQR through the lens of ﬁrst

order methods: Discrete-time case. arXiv preprint arXiv:1907.08921, 2019.

Cyrus Derman and Morton Klein. Some remarks on ﬁnite horizon Markovian decision models.
Operations Research, 13(2):272–278, April 1965. URL https://doi.org/10.1287/opre.13.2.272.

Dmitriy Drusvyatskiy and Courtney Paquette. Eﬃciency of minimizing compositions of convex

functions and smooth maps. Mathematical Programming, 178(1-2):503–558, 2019.

Maryam Fazel, Rong Ge, Sham M Kakade, and Mehran Mesbahi. Global convergence of policy
gradient methods for the linear quadratic regulator. In International Conference on Machine
Learning, pages 1467–1476, 2018.

Jerzy A. Filar, L. C. M. Kallenberg, and Huey-Miin Lee. Variance-penalized Markov decision
processes. Mathematics of Operations Research, 14(1):147–161, 1989. URL http://pubsonline.
informs.org/doi/abs/10.1287/moor.14.1.147.

Elad Hazan, Sham M Kakade, Karan Singh, and Abby Van Soest. Provably eﬃcient maximum

entropy exploration. arXiv preprint arXiv:1812.02690, 2018.

Ying Huang and L. C. M. Kallenberg. On ﬁnding optimal policies for Markov decision chains:
A unifying framework for mean-variance-tradeoﬀs. Mathematics of Operations Research, 19(2):
434–448, 1994. URL http://pubsonline.informs.org/doi/abs/10.1287/moor.19.2.434.

Sham M Kakade. A natural policy gradient. In Advances in neural information processing systems,

pages 1531–1538, 2002.

Sham M Kakade, Shai Shalev-Shwartz, and Ambuj Tewari. Regularization techniques for learning

with matrices. Journal of Machine Learning Research, 13(Jun):1865–1890, 2012.

L C M Kallenberg. Linear Programming and Finite Markovian Control Problems. CWI Mathematisch

Centrum, 1983.

L. C. M. Kallenberg. Survey of linear programming for standard and nonstandard Markovian control

problems. Part I: Theory. Zeitschrift f¨ur Operations Research, 40(1):1–42, 1994.

Jack Kiefer, Jacob Wolfowitz, et al. Stochastic estimation of the maximum of a regression function.

The Annals of Mathematical Statistics, 23(3):462–466, 1952.

Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in Neural Information

Processing Systems, pages 1008–1014, 2000.

11

Zhang, Koppel, Bedi, Szepesvari, and Wang

Vijaymohan R Konda and Vivek S Borkar. Actor-critic–type learning algorithms for Markov Decision

Processes. SIAM Journal on Control and Optimization, 38(1):94–123, 1999.

Boyi Liu, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural proximal/trust region policy optimization

attains globally optimal policy. arXiv preprint arXiv:1906.10306, 2019.

Shie Mannor and John N Tsitsiklis. Mean-variance optimization in Markov decision processes. In
Proceedings of the 28th International Conference on International Conference on Machine Learning,
pages 177–184, 2011.

Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. On the global convergence

rates of softmax policy gradient methods. arXiv preprint arXiv:2005.06392, 2020.

Jorge Nocedal and Stephen Wright. Numerical optimization. Springer Science & Business Media,

2006.

Matteo Papini, Damiano Binaghi, Giuseppe Canonaco, Matteo Pirotta, and Marcello Restelli.

Stochastic variance-reduced policy gradient. arXiv preprint arXiv:1806.05618, 2018.

Matteo Pirotta, Marcello Restelli, and Luca Bascetta. Adaptive step-size for policy gradient methods.

In Advances in Neural Information Processing Systems, pages 1394–1402, 2013.

Matteo Pirotta, Marcello Restelli, and Luca Bascetta. Policy gradient in Lipschitz Markov Decision

Processes. Machine Learning, 100(2-3):255–283, 2015.

Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John

Wiley & Sons, 2014.

Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical

statistics, pages 400–407, 1951.

Sheldon M Ross. Introduction to stochastic dynamic programming. Academic press, 2014.

J Langford S Kakade. Approximately optimal approximate reinforcement learning. In ICML, pages

267–274, 2002.

Stefan Schaal. Learning from demonstration. In Advances in neural information processing systems,

pages 1040–1046, 1997.

John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International conference on machine learning, pages 1889–1897, 2015.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy

optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczy´nski. Lectures on stochastic programming:

modeling and theory. SIAM, 2014.

Richard S Sutton. Learning to predict by the methods of temporal diﬀerences. Machine learning, 3

(1):9–44, 1988.

Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. In Advances in neural information
processing systems, pages 1057–1063, 2000.

L Tak´acs. Non-Markovian processes. In Stochastic Process: Problems and Solutions, pages 46–62.

Springer, 1966.

12

Variational Policy Gradient

Alexandre B Tsybakov. Introduction to nonparametric estimation. Springer Science & Business

Media, 2008.

Lingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural policy gradient methods: Global

optimality and rates of convergence. arXiv preprint arXiv:1909.01150, 2019.

Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279–292, 1992.

Steven D Whitehead and Long-Ji Lin. Reinforcement learning of non-Markov decision processes.

Artiﬁcial intelligence, 73(1-2):271–306, 1995.

Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement

learning. Machine learning, 8(3-4):229–256, 1992.

Pan Xu, Felicia Gao, and Quanquan Gu. Sample eﬃcient policy gradient methods with recursive

variance reduction. arXiv preprint arXiv:1909.08610, 2019.

Y.-L. Yu, Y. Li, D. Schuurmans, and Cs. Szepesv´ari. A general projection property for distribution

families. In Advances in Neural Information Processing Systems, 2009.

Junyu Zhang, Amrit Singh Bedi, Mengdi Wang, and Alec Koppel. Cautious reinforcement learning

via distributional risk in the dual domain. arXiv preprint arXiv:2002.12475, 2020a.

Junyu Zhang, Mingyi Hong, Mengdi Wang, and shuzhong Zhang. Generalization bounds for stochastic

saddle point problems. arXiv preprint arXiv:2006.02067, 2020b.

Kaiqing Zhang, Alec Koppel, Hao Zhu, and Tamer Ba¸sar. Global convergence of policy gradient

methods to (almost) locally optimal policies. arXiv preprint arXiv:1906.08383, 2019.

13

Zhang, Koppel, Bedi, Szepesvari, and Wang

Supplementary Material for
“Variational Policy Gradient Method
for Reinforcement Learning with General Utilities”

Appendix A. Related Work

We provide a more extension discussion for the context of this work. Firstly, when closed-form
expressions for the optimizer of a function are unavailable, solving optimization problems requires
iterative schemes such as gradient ascent Nocedal and Wright (2006). Their convergence to global
extrema is predicated on concavity and the tractability of computing ascent directions. When the
objective takes the form of an expected value of a function parameterized by a random variable,
stochastic approximations are required Robbins and Monro (1951); Kiefer et al. (1952). The PG
Theorem mentioned above gives a speciﬁc form for obtaining ascent directions with respect to a
parameterized family of stationary policies via trajectories in a Markov decision process, when the
objective is the expected cumulative return Sutton et al. (2000), which gives rise to the REINFORCE
algorithm.

The convergence of policy search for the expected cumulative return has been studied extensively
in recent years. Under general parameterizations the problem becomes nonconvex. Hence, early
work focused on asymptotic convergence to stationarity Pirotta et al. (2015) by invoking dynamical
systems Borkar (2008). In actor-critic Konda and Borkar (1999); Konda and Tsitsiklis (2000), one
replaces the Monte Carlo rollout of the Q function with a temporal diﬀerence estimator Sutton
(1988), and its asymptotic stability follows similar logic Bhatnagar et al. (2009). Another line of
work focused on only on per-step value increase, i.e., policy improvement bounds Pirotta et al. (2013,
2015). Recent interest has been on structural results that yield convergence to global optimality:
when state transitions are linear Fazel et al. (2018); Bu et al. (2019)), the policy parameterization is
direct (tabular) Bhandari and Russo (2019); Agarwal et al. (2019), function approximation error can
be quantiﬁed S Kakade (2002); Liu et al. (2019). Clever step-size rules have also been designed to
ensure convergence to second-order stationary points under general settings Zhang et al. (2019).

These results, however, are restricted to the expected cumulative return, a linear functional of the
state-action occupancy measure, and hence do not apply to general concave functionals of the form
considered in this work. Early works in operations research consider nonstandard utilities Huang and
Kallenberg (1994), motivated by certain variance-penalizations which may also be written as concave
functionals of occupancy measures Filar et al. (1989). Similar in spirit to this work is Kallenberg
(1994), as it also puts occupancy measures at the center of its conceptual development. These
works develop dynamic programming approaches for tabular settings, and hence are not scalable to
problems with large spaces. More recently, maximizing the entropy of the state visitation distribution
has been considered Hazan et al. (2018), a special case of the concave utilities we study. Moreover,
the authors develop a model-based iteratively policy update, which requires explicit knowledge of
the transition probability matrix. By contrast, in this work we prioritize model-free approaches for
possibly large spaces via the fusion of direct policy search and parameterization over a family of
policies.

Appendix B. Supplementary materials of Section 3

B.1 A Monte Carlo Algorithm for solving (3.6)

Note that any algorithm that solves problem (3.6) will serve our purpose. Therefore, we provide a
Monte Carlo method that alternates between stochastic primal and dual updates as an example,
stated in Algorithm 1, in which the projection operator onto the set {z : (cid:107)z(cid:107)∞ ≤ (cid:96)F } is denoted as

14

Variational Policy Gradient

Proj (cid:96)F {z}. For any z, z(cid:48) = Proj (cid:96)F {z} is deﬁned as

z(cid:48)
i =






−(cid:96)F ,
zi,
(cid:96)F ,

if zi ∈ (−∞, −(cid:96)F ),
if zi ∈ [−(cid:96)F , (cid:96)F ],
if zi ∈ ((cid:96)F , +∞).

It is worth noting that we have we omit the term δ∇ ˜V (θ; z)(cid:62)x when computing the gradient w.r.t.

Algorithm 1 Monte Carlo Variational Policy Gradient Estimation
Require: a diﬀerentiable policy parametrization πθ, stepsizes αt, βt > 0, initial points x = 0, z = 0.

A constant (cid:96)F .
policy parameter θ ∈ Rd
Generate episodes ζi = {(sk, ak)} from i = 1, · · · , n following πθ(a|s)
For t = 0, 1, 2, ... until some stopping criterion is met:

Sample (sk, ak) from the data set
Update

zt+1 ← Proj (cid:96)F

(cid:26)

zt −

αt
1 − γ

1sk,ak + αt∇F ∗(zt)

(cid:27)

xt+1 ← xt + βt

(cid:34)

(cid:88)

a∈A

Qπθ (sk, a; zt) · ∇θπθ(a|sk) − xt

(cid:35)

(B.1)

(B.2)

Output: the last iterate x

z in (B.1). Note that for the iterates xt are all well bounded, then δ∇ ˜V (θ; zt)(cid:62)xt = O(δ), which is
negligible when δ → 0.

B.2 Special cases of policy gradient computation

We give several examples of the policy gradient for special cases of the general utility in (3.1).

Linear utility The simplest, where F (λ) = (cid:104)λ, r(cid:105) [cf. (2.1)], we have F ∗(z) = 0 if z = c · r for
some scalar c and F ∗(z) = ∞ otherwise. In this case z∗ = r and Theorem 3.1 recovers the known
policy gradient theorem for the risk-neutral MDP (2.1), that is ∇θR(πθ) = ∇θV (θ; r).
Constrained MDPs By contrast, in Example 2.1, i.e., when a constraint Eπ [(cid:80)∞
t=0 γtc(st, at)] ≤ C
on the accumulation of costs c(st, at) is present, and we may enforce it approximately with a log
barrier by deﬁning

R(πθ) = (cid:104)r, λ(θ)(cid:105) + β log (C − (cid:104)c, λ(θ)(cid:105)) = V (θ; r) + β log (C − V (θ; c)) ,

(B.3)

where β is a regularization parameter, in which case the policy gradient takes the form

∇R(πθ) = ∇θV (θ; r) − β

∇θV (θ; c)
C − V (θ; c)

.

Estimating the policy gradient R of constrained MDP consists of estimating two policy gradients
∇θV (θ; c) and ∇θV (θ; r) and accumulated reward V (θ; c).
Minimum eigenvalue For case (2.8), deﬁne Φ(λπθ ) = (cid:80)
symmetric and positive semideﬁnite, since λπθ ≥ 0. By using Rayleigh principle, we have

sa · φ(s, a)φ(s, a)(cid:62). Then Φ(λπθ ) is

s,a λπθ

R(πθ) = σmin (Φ(λπθ )) = min
(cid:107)u(cid:107)=1

u(cid:62)Φ(λπθ )u = min
(cid:107)u(cid:107)=1

(cid:88)

s,a

sa |φ(s, a)(cid:62)u|2.
λπθ

(B.4)

15

Zhang, Koppel, Bedi, Szepesvari, and Wang

which is the minimum of a family of linear function in λ. Let v(1), ..., v(k) be a group of orthonormal
bases of the eigenspace of Φ(λπθ ) corresponding to the minimum eigenvalue. Then deﬁne k vectors
as r(i)(s, a) = |φ(s, a)(cid:62)v(i)|2, ∀s, a, i = 1, ..., k. Then the Fr´echet superdiﬀerential of R at θ is

ˆ∂θR(πθ) =

(cid:110)

∇θV (θ; r) : r ∈ conv(r(1), ..., r(k))

(cid:111)

,

where conv(·) denotes the convex hull of a group of vectors. When the multiplicity of the minimum
eigenvalue is 1, then R(·) is diﬀerentiable at this point and ˆ∂θR(·) = {∇θR(·)}.

Entropy maximization For the entropy (2.7), its Fenchel dual takes the form

F ∗(z) = −

(cid:88)

exp (cid:8) −

sa

zsa
1 − γ

− 1(cid:9).

Learning to mimic a distribution For the KL divergence to a prior µ in (2.9), we have

(cid:40)

F ∗(z) =

− (cid:80)
−∞

s µs exp (cid:8) − zs1

1−γ − 1(cid:9) if

zsa1 = zsa2 ∀s ∈ S, a1, a2 ∈ A,

otherwise.

Appendix C. Additional Details of Experiments

C.1 Details of Environment

OpenAI Frozen Lake is a ﬁnite-state action problem. The standard state consists of {S, F, H, G},
to which we add an additional state C which is visualized in Fig. 3(a). At each step, an agent
selects an action a ∈ A, which consists of one of four directions (up, down, left, right), which may
be enumerated as {1, . . . , 4}. The reward is null at all Frozen F spaces, the start S location, and
the Holes H in the lake. If the agent enters a hole, the episode terminates, and hence null reward is
accumulated for this trajectory. The only positive reward is 1 and may be obtained when reaching
the goal state G. Our augmentation is that costly states C have been added, which incur reward
−0.4 to represent, for instance, obstacles. We note that only for the cumulative return and its
constrained variants, or other utilities that are deﬁned in terms of the problem’s inherent reward do
these quantities matter. That is, for the entropy maximization problem, there is no reward associated
with any state in the usual sense. The MDP transition model is unknown and deﬁned by the OpenAI
environment, a simulation oracle that provides state-action-reward triples.

Throughout all experiments, for simplicity, we considered a softmax policy parameterization. For
a(cid:48) eθsa(cid:48) ) for θ ∈ R|S|×|A|. For

this parameterization, the policy takes the form πθ(s | a) = eθsa /((cid:80)
the Frozen lake environment in this paper, we have |S| = 16 and |A| = 4.

C.2 Computing the True Policy Gradient

For comparison, we compute the true policy gradient by using a baseline approach based on the chain
rule and a variant of REINFORCE Sutton et al. (2000): the second factor on the right-hand side
of (3.3) is exactly computed using REINFORCE ∇θλsa(θ), whereas the ﬁrst, ∂F (λ(θ))
, is computed
using an additional Monte Carlo rollout. We denote as x∗ the result of this procedure and use it
as ground truth. In Figure 4(a) we display the evolution of its norm diﬀerence (cid:107)ˆx(cid:63)
n−1(cid:107) as the
sample size n increases. That it approaches null with the sample size implies that this brute force
Monte Carlo variant of REINFORCE is convergent, and hence is a reasonable benchmark comparator.

n − ˆx(cid:63)

∂λsa

C.3 Details about Maximum Entropy Exploration

For this problem instance, i.e., (2.7) from Example 2.2, we also consider the state space deﬁned by
Frozen Lake, but note that the reward as deﬁned by the environment is now a moot point. This is

16

Variational Policy Gradient

(a) Convergence of x(cid:63)

Figure 4: Fig. 4(a) displays the convergence of a generalization of REINFORCE-based gradient
n−1(cid:107) as the number of processed

estimator for (3.3) in terms of its diﬀerence (cid:107)ˆx(cid:63)
trajectories n increases, which converges to null, certifying ˆx(cid:63)

n − ˆx(cid:63)

n as a baseline.

because each state contributes positive entropy, with the exception of the holes in the lake, which
terminate the episode. We visualize this setup at the bottom layer of Fig. 2(b). The lower middle
layer visualizes the occupancy measure associated with a uniform policy. Moreover, the upper middle
layer visualizes the “pseudo-reward” z for each point in the state space. This quantity is computed in
terms of the Fenchel dual of the entropy – see Appendix B.2, and the occupancy measure associated
with the output of Algorithm 1 at the end of training is visualized at the top layer. To obtain this
result, we run it for 105 total episodes, and for each episode we evaluate the entropy using (2.7). We
consider a constant step-size α = 0.01, β = 0.1, and η = 0.001 throughout this experiment.

C.4 Details about the Constrained Markov Decision Process

In this subsection, we elaborate upon the implementation of Example 2.1, speciﬁcally, (2.6) and its
approximation using a logarithmic barrier as detailed in (B.3). We consider the problem of navigating
through the FrozenLake environment as shown in Fig. 3(a)(bottom): we seek to reach the goal state
G (reward = 1) from the starting location S (reward = 0), navigating along F frozen spaces (reward
= 0), while avoiding locations marked C (reward = −0.2) that denote costly states (obstacles) and
H holes.

We consider two approaches to the problem: ﬁrst, we focus on optimizing the standard expected
cumulative return (2.1), whose associated state occupancy measure is given in Fig. 3(a)(middle);
second, we consider imposing constraints to avoid costly states [cf. (2.6)] via a logarithmic barrier
(B.3), whose resulting occupancy measure is depicted in Fig. 3(a)(top). Bluer/yellower colors denote
higher/lower likelihoods, respectively. We observe that imposing constraints yields policies whose
probability mass is concentrated away from constraints and instead along paths from the start to the
goal. Thus, Algorithm 1 combined with a policy search scheme (4.1) may be used to solve CMDPs.
This trend is corroborated in Fig. 3, which depicts the reward 3(b) and cost 3(c) accumulation
during test trajectories as a function of training index for Algorithm 1 for the cumulative return
(2.1) as compared with a logarithmic barrier imposed to solve CMDP (2.6) for diﬀerent penalty
parameters β. We may observe that without imposing any constraint (β = 0 in red), one achieves
the highest reward, but incurs the most costs, i.e., hits obstacles most often, a form of “reckless
boldness.” Larger β means higher penalty for the constraints, and hence β = 4 incurs lower cost and
lower reward. We further added the curves for β = 1 and β = 2 for comparison.

For all results reported in Fig. 3, we run the algorithm for 10K total training steps in the form
of episodes. For each episode, we run a number of evaluation (test) trajectories in order to determine

17

Zhang, Koppel, Bedi, Szepesvari, and Wang

their merit, both in terms of reward and cost accumulation. Put more simply, we evaluate the
performance averaged over a few test trajectories as a function of episode number and report its
average over last 20 episodes to show the trend. This is to illuminate policy improvement in its
various forms (reward/cost accumulation) during training. Moreover, the algorithm is run with
constant step-size η = 0.1 throughout this experiment.

Appendix D. Proof of Theorem 3.1

Proof. First note that for any z ∈ RSA, x ∈ Rd, we have

V (θ; z) = (cid:104)z, λ(θ)(cid:105) ,
∇θV (θ; z)(cid:62)x = (cid:104)z, ∇θλ(θ)x(cid:105) ,

(D.1)

where ∇θλ(θ) is the SA × d Jacobian matrix, the ﬁrst identity holds by deﬁnition, and the second
holds by directly diﬀeretiating the ﬁrst indentity and product it with x.

Consider the saddle point problem in (3.4) for ﬁxed 0 < δ < 1. Let G be any constant such that

(cid:107)∇F (λ(θ))(cid:107)∞ < G. Deﬁne

(x∗(δ), z∗(δ))

:= argmaxx argmin(cid:107)z(cid:107)∞≤G

(cid:8)V (θ; z) + δ∇θV (θ; z)(cid:62)x − F ∗(z) − δ

2 (cid:107)x(cid:107)2(cid:9).

(D.2)

Note in (D.2) we added the auxiliary constraint set {z : (cid:107)z(cid:107)∞ ≤ G}, and later we will show
that this constraint is inactive for all δ suﬃciently small. We will also show that (x∗(δ), z∗(δ)) are
bounded for all δ suﬃciently small.

By the ﬁrst-order stationarity condition, we have

x∗(δ) = ∇θV (θ; z∗(δ)).

Note that ∇θV (θ; ·) is a linear function of z, thus there exists B > 0 such that (cid:107)∇θV (θ; z)(cid:107) ≤ B for
all z ∈ {(cid:107)z(cid:107)∞ ≤ G}. And consequently (cid:107)x∗(δ)(cid:107) ≤ B for all δ > 0.

For all x ∈ {(cid:107)x(cid:107) ≤ 2B}, we have

lim
δ→0+

λ(θ) + δ∇θλ(θ)x = λ(θ).

Therefore, there exists some small δ0 > 0, such that for all δ < δ0, the vector λ(θ) + δ∇θλ(θ)x belongs
to the neighborhood on which F is diﬀerentiable and

(cid:107)∇F (λ(θ) + δ∇θλ(θ)x)(cid:107)∞ < G,

∀ x ∈ {x : (cid:107)x(cid:107) ≤ 2B}.

In this case, we consider the unconstrained solution, for (cid:107)x(cid:107) ≤ 2B, deﬁned by

z∗(x; δ) := argmin

z

V (θ; z) + δ∇θV (θ; z)(cid:62)x − F ∗(z) = ∇F (cid:0)λ(θ) + δ∇θλ(θ)x(cid:1),

and observe that the unconstrained solution satisﬁes (cid:107)z∗(x; δ)(cid:107)∞ < G, and consequently the constraint
(cid:107)z(cid:107)∞ ≤ G is not active. Therefore, for δ < δ0, we can equivalently rewrite (D.2) as

x∗(δ)

:= argmax
(cid:107)x(cid:107)≤2B

min
z

(cid:26)

V (θ; z) + δ∇θV (θ; z)(cid:62)x − F ∗(z) −

(cid:27)

(cid:107)x(cid:107)2

δ
2

(D.3)

= argmax
(cid:107)x(cid:107)≤2B

F (λ(θ) + δ∇θλ(θ)x) −

δ
2

(cid:107)x(cid:107)2 ,

Recall that we showed (cid:107)x∗(δ)(cid:107) ≤ B, therefore the constraint (cid:107)x(cid:107) ≤ 2B is also inactive and removable.
Therefore x∗(δ) is equivalent to the unconstrained min-max solution, for all δ suﬃciently small, and

18

Variational Policy Gradient

Fenchel duality together with the ﬁrst-order stationarity condition implies

x∗(δ) = argmax

x

(cid:26)

inf
z

V (θ; z) + δ∇θV (θ; z)(cid:62)x − F ∗(z) −

(cid:27)

(cid:107)x(cid:107)2

δ
2

= ∇θλ(θ)(cid:62)∇F (cid:0)λ(θ) + δ∇θλ(θ)x∗(δ)(cid:1).

By using the fact that ∇F is continuous at λ(θ) and x∗(δ) is bounded, by letting δ → 0 on both
sides, we get

lim
δ→0+

x∗(δ) = lim
δ→0+

∇θλ(θ)(cid:62)∇F (cid:0)λ(θ) + δ∇θλ(θ)x∗(δ)(cid:1)

= ∇θλ(θ)(cid:62)∇F (cid:0)λ(θ)(cid:1)
= ∇R(θ) ,

where the last equality uses the chain rule.

Appendix E. Proof of Theorem 3.2

Proof. First, let us denote the expression in (3.4) for ﬁxed 0 < δ < 1 as

(x∗(δ), z∗(δ)) = argmax

x

argmin
(cid:107)z(cid:107)∞≤(cid:96)F

V (θ; z) + δ∇θV (θ; z)(cid:62)x − F ∗(z) −

δ
2

(cid:107)x(cid:107)2,

(E.1)

and its approximation with empirically estimated value functions and their gradients in (3.5) as

(ˆx(δ), ˆz(δ)) = argmax

x

argmin
(cid:107)z(cid:107)∞≤(cid:96)F

˜V (θ; z) + δ∇θ ˜V (θ; z)(cid:62)x − F ∗(z) −

δ
2

(cid:107)x(cid:107)2.

(E.2)

Then we decompose the entity E

(cid:20)(cid:13)
ˆ∇θR(πθ) − ∇θR(πθ)
(cid:13)
(cid:13)

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

into three terms by adding and subtract-

ing (i) x∗(δ) and (ii) ˆx(δ), which we then establish depends on the diﬀerence between (iii) ˆz(δ) and
z∗(δ). Taken together with computing the limit of the right-hand side as δ → 0 we obtain the result.
Each of these steps is analyzed independently, whose estimation errors are derived in the following
lemma.

Lemma E.1. Consider (x∗(δ), z∗(δ)) and (ˆx(δ), ˆz(δ)) as deﬁned in (E.1)-(E.2), respectively. Under
the technical conditions stated in Theorem 3.2, their respective estimation errors satisfy:

(i)

(ii) E

(cid:13)
(cid:13)
(cid:13)x∗(δ) − ∇θR(πθ)
(cid:20)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)x∗(δ) − ˆx(δ)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
2(cid:21)

= O(δ2).

≤ 2C2(cid:107)z∗(δ)(cid:107)2

(1−γ)4

∞

(cid:16) γ2K
(1−γ)2 + 1

n

·

(cid:17)

+ 2C2

(1−γ)4 · E

(cid:104)
(cid:107)z∗(δ) − ˆz(δ)(cid:107)2
∞

(cid:105)

.

(iii) E (cid:2)(cid:107)ˆz(δ) − z∗(δ)(cid:107)2

∞

(cid:3) ≤ O

(cid:16) L2
n(1−γ)2 + L2

F

F ∗

F (cid:96)2
n + L2

F δ2+LF δ
n

(cid:17)

.

Combining the three steps and the fact that (cid:107)z∗(δ)(cid:107)∞ ≤ (cid:96)F yields
(cid:19)

E

(cid:107)ˆx(δ) − ∇θR(θ)(cid:107)2(cid:105)
(cid:104)

≤ O

(cid:18) C 2((cid:96)2

F + L2
F (cid:96)2
n(1 − γ)4

F ∗ )

+

C 2L2
F
n(1 − γ)6

+ O(δ2 + δ/n + γK).

Let δ → 0, we get

E

(cid:20)(cid:13)
ˆ∇θR(πθ) − ∇θR(πθ)
(cid:13)
(cid:13)

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

≤ O

(cid:18) C 2((cid:96)2

F + L2
F (cid:96)2
n(1 − γ)4

F ∗ )

(cid:19)

+

C 2L2
F
n(1 − γ)6

+ O(γK).

19

Zhang, Koppel, Bedi, Szepesvari, and Wang

Lemma E.1(i) - (iii) is proved in the next subsection. For the ease of notation, we will simply
denote x∗ and ˆx instead of x∗(δ) and ˆx(δ). Similarly, we denote z∗ and ˆz instead of z∗(δ) and ˆz(δ).

E.1 Preliminary Technicalities

Linearity property. The functions Q, V and ∇θV are linear in the reward function. Namely, for
any α, α(cid:48) ∈ R and r, r(cid:48) ∈ R|S||A|,

α∇θV (θ; r) + α(cid:48)∇θV (θ; r(cid:48)) = ∇θV (θ; αr + α(cid:48)r(cid:48)).

Similar identities holds for Qπθ (s, a; ·) and V (θ; ·). For the stochastic estimators ∇θ ˜V (θ; r; ζ), it is
straightforward to check that the linearity property is still true.
Upperbounding Q and V . Given an arbitrary reward function r, the upper bounds of Q and V
functions are

|Qπθ (s, a; r)| ≤

and

|V (θ; r)| ≤

(cid:107)r(cid:107)∞
1 − γ

(cid:107)r(cid:107)∞
1 − γ

.

Uniform upperbounds for estimators. Given any sample path ζ = {(sk, ak)}K
˜V (θ; z; ζ) and ∇θ ˜V (θ; z; ζ) are upper bounded by

k=0, the estimators

˜V (θ; z; ζ)| ≤

(cid:107)z(cid:107)∞
1 − γ

and

(cid:107)∇θ ˜V (θ; z; ζ)(cid:107) ≤

C(cid:107)z(cid:107)∞
(1 − γ)2 .

Consequently, as the sample averages of ˜V (θ; z; ζi) and ∇θ ˜V (θ; z; ζi), we also have

| ˜V (θ; z)| ≤

(cid:107)z(cid:107)∞
1 − γ

and

(cid:107)∇θ ˜V (θ; z)(cid:107) ≤

C(cid:107)z(cid:107)∞
(1 − γ)2

(E.3)

(E.4)

for any set of sample paths {ζi}n

i=1.

Proof. For ˜V (θ; z; ζ), for any z,

| ˜V (θ; z; ζ)| =

K
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
k=0

γk · z(sk, ak)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

K
(cid:88)

k=0

γk(cid:107)z(cid:107)∞ ≤

(cid:107)z(cid:107)∞
1 − γ

For ∇θ ˜V (θ; z; ζ), for any z,

(cid:107)∇θ ˜V (θ; z; ζ)(cid:107) =

≤

≤

≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

K
(cid:88)

(cid:88)

γk · Q(sk, a; z)∇θπθ(a|sk)

(cid:88)

Q(sk, a; z)∇θπθ(a|sk)

a∈A

γk · max

(cid:107)u(cid:107)∞≤ (cid:107)z(cid:107)∞
1−γ

(cid:107)πθ(·|sk)u(cid:107)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

k=1

a∈A
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

γk ·

K
(cid:88)

k=1

K
(cid:88)

k=1
C(cid:107)z(cid:107)∞
(1 − γ)2 .

20

Variational Policy Gradient

E.2 Proof of Lemma E.1(i).

Consider the problem (E.1). First let us ignore the requirement that (cid:107)z(cid:107)∞ ≤ (cid:96)F . For this series of
unconstrained problem, Theorem 3.1 suggests that

lim
δ→0+

x∗(δ) = ∇θR(πθ).

Consequently, limδ→0+ λ(θ) + δ∇θλ(θ)x∗(δ) = λ(θ). Because (cid:107)λ(θ)(cid:107)1 = (1 − γ)−1, ∃δ0 > 0 s.t. when
δ < δ0 we have

According to condition (i) of this theorem, we have

(cid:107)λ(θ) + δ∇θλ(θ)x∗(δ)(cid:107)1 ≤

2
1 − γ

.

(cid:107)∇F (λ(θ) + δ∇θλ(θ)x∗(δ))(cid:107)∞ ≤ (cid:96)F .
It is worth noting that z∗(δ) = ∇F (λ(θ) + δ∇θλ(θ)x∗(δ)) is also the solution to the unconstrained
version of (E.1). Therefore we have (cid:107)z(cid:107)∞ ≤ (cid:96)F , so that we can add this to the constraint without
changing the optimal solutions. By the intermediate result in the proof of Theorem 3.1, we have
x∗(δ) = ∇θλ(θ)(cid:62)∇F (cid:0)λ(θ) + δ∇θλ(θ)x∗(δ)(cid:1).

Consequently, by the Lipschitz continuity of ∇F , we have

(cid:13)
(cid:13)x∗(δ) − ∇θR(θ)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

=

≤ (cid:107)∇θλ(θ)(cid:62)(cid:107)∞,2 ·

(cid:13)
(cid:13)

(cid:13)∇θλ(θ)(cid:62)∇F (cid:0)λ(θ) + δ∇θλ(θ)x∗(δ)(cid:1) − ∇θλ(θ)(cid:62)∇F (cid:0)λ(θ)(cid:1)(cid:13)
(cid:13)∇F (cid:0)λ(θ) + δ∇θλ(θ)x∗(δ)(cid:1) − ∇F (cid:0)λ(θ)(cid:1)(cid:13)
∞,2 ·

(cid:13)
(cid:13)
2
(cid:13)δ∇θλ(θ)x∗(δ)
(cid:13)
(cid:13)
(cid:13)
1

2
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)

(cid:13)
(cid:13)

∞

≤ LF (cid:107)∇θλ(θ)(cid:62)(cid:107)2

= O(δ2).

as stated in Lemma E.1(i).
x∗(δ) → ∇θR(πθ).

In the last step, we used the fact that x∗(δ) is bounded because

E.3 Proof of Lemma E.1(ii).

By the ﬁrst order stationarity condition of the problems (E.1)-(E.2), we know

x∗ = ∇θV (θ; z∗)

and

ˆx = ∇θ ˜V (θ; ˆz).

Consider the norm-diﬀerence between the preceding quantities:

E

(cid:104)(cid:13)
(cid:13)x∗ − ˆx(cid:13)
(cid:13)

2(cid:105)

≤ 2E

(cid:104)

(cid:107)∇θV (θ; z∗) − ∇θ ˜V (θ; z∗)(cid:107)2(cid:105)

+ 2E

(cid:104)

(cid:107)∇θ ˜V (θ; z∗) − ∇θ ˜V (θ; ˆz)(cid:107)2(cid:105)

.

(E.5)

To bound the term E

(cid:104)

(cid:107)∇θV (θ; z∗) − ∇θ ˜V (θ; z∗)(cid:107)2(cid:105)

, recall the deﬁnition (3.5):

∇ ˜V (θ; z) :=

1
n

(cid:88)

i=1

∇θV (θ; z; ζi) =

1
n

n
(cid:88)

K
(cid:88)

(cid:88)

i=1

k=1

a∈A

γkQ(s(i)

k , a; z)∇θπθ(a|s(i)
k ).

Consider the ﬁrst term on the right-hand side of (E.5). Add and subtract E

(cid:105)
(cid:104)
∇θ ˜V (θ; z∗)

and use

the fact that E

(cid:104)

∇θ ˜V (θ; z∗)

(cid:105)

= ∇θV (θ; z∗), i.e., the bias-variance decomposition identity, to write

E

=

2(cid:21)

(cid:20)(cid:13)
(cid:13)
(cid:13)∇θV (θ; z∗) − ∇θ ˜V (θ; z∗)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∇θV (θ; z∗) − E
(cid:13)

(cid:104)
∇θ ˜V (θ; z∗)

(cid:105)(cid:13)
2
(cid:13)
(cid:13)

+ E

(cid:20)(cid:13)
(cid:13)∇θ ˜V (θ; z∗) − E
(cid:13)

(cid:104)

∇θ ˜V (θ; z∗)

2(cid:21)

(cid:105)(cid:13)
(cid:13)
(cid:13)

.

(E.6)

21

Zhang, Koppel, Bedi, Szepesvari, and Wang

For the ﬁrst (squared bias) term on the right-hand side of (E.6), denote dπ
ξ (s)| ≤ γK
s|π, s0 ∼ ξ). Then it is straightforward that (cid:80)

ξ,K(s) − dπ

s |dπ

ξ,K(s) = (1−γ) (cid:80)K
1−γ . As a result, we know

t=0 γtProb(st =

(cid:13)
(cid:13)∇θV (θ; z∗) − E
(cid:13)

(cid:104)

∇θ ˜V (θ; z∗)

(cid:105)(cid:13)
2
(cid:13)
(cid:13)

(E.7)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

(cid:0)dπ

ξ (s) − dπ

ξ,K(s)(cid:1) (cid:88)

Qπθ (s, a; z∗)∇θπθ(a|s)

1
(1 − γ)2

1
(1 − γ)2

1
(1 − γ)2

1
(1 − γ)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:32)

s

(cid:88)

s

(cid:32)

(cid:88)

s

(cid:32)

(cid:88)

s

|dπ

ξ (s) − dπ

ξ,K(s)| ·

|dπ

ξ (s) − dπ

ξ,K(s)| ·

a

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

(cid:88)

a

(cid:88)

a

|dπ

ξ (s) − dπ

ξ,K(s)| ·

(cid:107)∇θπ(·|s)(cid:107)2

∞,2 · (cid:107)z∗(cid:107)2
∞

(1 − γ)4

C 2(cid:107)z∗(cid:107)2
∞
(1 − γ)6 γ2K.

(cid:32)

(cid:88)

s

=

=

=

≤

≤

≤

(cid:13)
Qπθ (s, a; z∗)∇θπθ(a|s)
(cid:13)
(cid:13)

(cid:33)2

(cid:13)
Qπθ (s, a; z∗)∇θπθ(a|s)
(cid:13)
(cid:13)

(cid:33)2

(cid:33)2

max
(cid:107)u(cid:107)∞≤ (cid:107)z∗ (cid:107)∞

1−γ

(cid:107)∇θπ(·|s)u(cid:107)

(cid:33)2

|dπ

ξ (s) − dπ

ξ,K(s)|

Next, we consider the second (variance) term on the right-hand side of (E.6). By substituting (3.5)
in for ∇θ ˜V (θ; z∗) to rewrite it in terms of trajectories ζi, we have

E

(cid:20)(cid:13)
(cid:13)∇θ ˜V (θ; z∗) − E
(cid:13)

(cid:104)

∇θ ˜V (θ; z∗)

2(cid:21)

(cid:105)(cid:13)
(cid:13)
(cid:13)

(cid:104)
∇θ ˜V (θ; z∗; ζi)

2(cid:21)

(cid:105)(cid:13)
(cid:13)
(cid:13)

=

≤

≤

(cid:20)(cid:13)
(cid:13)∇θ ˜V (θ; z∗; ζi) − E
(cid:13)
2(cid:21)
(cid:20)(cid:13)
(cid:13)
(cid:13)∇θ ˜V (θ; z∗; ζi)
(cid:13)
(cid:13)
(cid:13)

E

E

1
n
1
n
C 2(cid:107)z∗(cid:107)2
∞
n(1 − γ)4 .

The ﬁrst inequality comes from crudely upper-bounding the bias by the estimator itself. The last
equality uses (E.3).

Now, returning focus to the second term in the bound (E.5), by the linearity of the stochastic

estimators with respect to the diﬀerential and (E.4), we have

(cid:13)
(cid:13)∇θ ˜V (θ; z∗) − ∇θ ˜V (θ; ˆz)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

=

(cid:13)
(cid:13)
2
(cid:13)∇θ ˜V (θ; z∗ − ˆz)
(cid:13)
(cid:13)
(cid:13)

≤

C 2(cid:107)z∗ − ˆz(cid:107)2
∞
(1 − γ)4

.

Taking the expectation after squaring both sides yields

E

(cid:20)(cid:13)
(cid:13)
(cid:13)∇θ ˜V (θ; z∗) − ∇θ ˜V (θ; ˆz)
(cid:13)
(cid:13)
(cid:13)

2(cid:21)

≤

C 2
(1 − γ)4

E (cid:2)(cid:107)z∗ − ˆz(cid:107)2

∞

(cid:3) .

(E.8)

Combining inequalities (E.5), (E.6), (E.7), (E.8), (E.8) yields

E

(cid:104)(cid:13)
(cid:13)x∗ − ˆx(cid:13)
(cid:13)

2(cid:105)

≤

2C 2(cid:107)z∗(cid:107)2
∞
(1 − γ)4

·

(cid:18) γ2K

(1 − γ)2 +

(cid:19)

1
n

+

2C 2
(1 − γ)4 · E

(cid:104)

(cid:107)z∗ − ˆz(cid:107)2
∞

(cid:105)

.

which is as stated in Lemma E.1(ii).

22

Variational Policy Gradient

E.4 Proof of Lemma E.1(iii).

In this section we will apply the generalization bound for stochastic saddle points from Zhang et al.
(2020b) to bound the term E[(cid:107)ˆz − z∗(cid:107)2
∞]. To achieve this, we need a compact feasible region for x.
Note that for problems (E.1) and (E.2), the solutions x∗ and ˆx has the form

x∗ = ∇θV (θ; z∗)

and

ˆx = ∇θ ˜V (θ; ˆz).

Due to (E.4) and the constraint that (cid:107)z(cid:107)∞ ≤ (cid:96)F , we have (cid:107)x∗(cid:107) ≤ C(cid:107)z∗(cid:107)∞
(1−γ)2 and thus
(cid:107)ˆx(cid:107) ≤ C(cid:96)F
(1−γ)2 will not change
the solutions of problems (E.1) and (E.2). Formally speaking, we will then apply the theory of Zhang
et al. (2020b) to the following pair of constrained problems:

(1−γ)2 with probability 1. Therefore, adding a constraint that (cid:107)x(cid:107) ≤ C(cid:96)F

(1−γ)2 ≤ C(cid:96)F

(x∗, z∗) = argmax

x∈X

argmin
z∈Z

V (θ; z) + δ∇θV (θ; z)(cid:62)x − F ∗(z) −

δ
2

(cid:107)x(cid:107)2,

and

(ˆx, ˆz) = argmax

x∈X

argmin
z∈Z

˜V (θ; z) + δ∇θ ˜V (θ; z)(cid:62)x − F ∗(z) −

δ
2

(cid:107)x(cid:107)2.

(E.9)

(E.10)

with X = {x : (cid:107)x(cid:107) ≤ C(cid:96)F
same solution, and problems (E.2) and (E.10) share the same solution.
Finally, similar to the proof of (E.7), for any x ∈ X and z ∈ Z

(1−γ)2 } and Z = {z : (cid:107)z(cid:107)∞ ≤ (cid:96)F }. The problems (E.1) and (E.9) share the

V (θ; z) + δ∇θV (θ; z)(cid:62)x − E

(cid:104) ˜V (θ; z; ζi) + δ∇θ ˜V (θ; z; ζi)(cid:62)x

(cid:105)

= O

(cid:18) γK
1 − γ

(cid:19)

.

For the simplicity of discussion, let us assume that K is large enough so that we can ignore the
bias. Therefore problem (E.10) can be viewed as an empirical version of the problem (E.9)
O

(cid:17)

(cid:16) γK
1−γ

with negligible bias. To apply the theory of Zhang et al. (2020b), deﬁne

Ψζ(x, z) := ˜V (θ; z; ζ) + δ∇θ ˜V (θ; z; ζ)(cid:62)x − F ∗(z) −

δ
2

(cid:107)x(cid:107)2.

Then for any sample path ζ, Ψζ satisﬁes the following set of properties:

• Ψζ(·, z) is µx-strongly concave under L2 norm. And Ψζ(x, ·) is µz-strongly convex under the

L∞ norm. In other words, for ∀x, x(cid:48) ∈ X and z, z(cid:48) ∈ Z,

(cid:40)

Ψζ(x(cid:48), z) ≥ Ψζ(x, z) + (cid:104)u, x(cid:48) − x(cid:105) + µx
Ψζ(x, z(cid:48)) ≤ Ψζ(x, z) + (cid:104)v, z(cid:48) − z(cid:105) − µz

2 (cid:107)x(cid:48) − x(cid:107)2, u ∈ ∂xΨζ(x, z),
2 (cid:107)z(cid:48) − z(cid:107)2
v ∈ ∂zΨζ(x, z).

∞,

In our case, it is clear that µx = δ. Due to Theorem 3 of Kakade et al. (2012), µz = L−1
F .

• The feasible regions X and Z are compact convex sets. For every ζ, there exist constants

(cid:96)x(ξ, z) and (cid:96)z(ξ, x) s.t.

(cid:40)

|Ψζ(x(cid:48), z) − Ψζ(x, z)| ≤ (cid:96)x(ζ, z)(cid:107)x(cid:48) − x(cid:107),
∀x, x(cid:48) ∈ X and y ∈ Y,
|Ψζ(x, z(cid:48)) − Ψζ(x, z)| ≤ (cid:96)z(ζ, x)(cid:107)z(cid:48) − z(cid:107)∞, ∀z, z(cid:48) ∈ Z and x ∈ X .

In our case, we gave (cid:96)z(ζ, x) = sup {(cid:107)u(cid:107)1 : z ∈ Z, u ∈ ∂zΨζ(x, z)} = 1
(cid:96)x(ζ, z) = supx∈X (cid:107)∇xΨζ(x, z)(cid:107) = O(δ). Consequently,

1−γ + (cid:96)F ∗ + O(δ) and

(cid:40)

((cid:96)w
((cid:96)w

x )2 := supz∈Z
z )2 := supx∈X

E(cid:2)(cid:96)2
E(cid:2)(cid:96)2

x(ζ, z)(cid:3) = O(δ2),
z(ζ, x)(cid:3) = O((cid:96)2

F ∗ + 1

(1−γ)2 + δ2).

23

Zhang, Koppel, Bedi, Szepesvari, and Wang

With the above two properties, Theorem 1 of Zhang et al. (2020b) indicates that

µz
2

E (cid:2)(cid:107)ˆz − z∗(cid:107)2

∞

(cid:3) ≤

√
2

2

n

(cid:18) ((cid:96)w
x )2
µx

·

+

(cid:19)

.

((cid:96)w
z )2
µz

With the detailed parameters substituted in the above inequality, we have

E (cid:2)(cid:107)ˆz − z∗(cid:107)2

∞

(cid:3) ≤ O

(cid:18)

L2
F
n(1 − γ)2 +

L2

F (cid:96)2
F ∗
n

+

L2

F δ2 + LF δ
n

(cid:19)

as stated in Lemma E.1(iii).

Appendix F. Proof of Theorem 4.2

Proof. Let θ∗ be a ﬁrst-order stationary solution of (3.1). When F is concave and locally Lipschitz
continuous in a neighbourhood containing λ(Θ), we can compute the Fr´echet superdiﬀerential of
F ◦ λ at θ∗ by the chain rule, see Drusvyatskiy and Paquette (2019). That is

ˆ∂(F ◦ λ)(θ∗) = [∇θλ(θ∗)](cid:62) ∂F (λ∗)

where ∂F (λ∗) denotes the set of supergradients of the concave function F at λ∗. Then there exists
w∗ ∈ ∂F (λ∗) ∈ RSA such that u∗ := [∇θλ(θ∗)](cid:62)w∗ ∈ ˆ∂(F ◦ λ)(θ∗) as in (4.2). It follows from (4.2)
that

(cid:104)w∗, ∇θλ(θ∗)(θ − θ∗)(cid:105) ≤ 0,

for ∀θ ∈ Θ.

(F.1)

For any λ ∈ λ(Θ), we let θ := g(λ) such that λ = λ(θ). Therefore, by adding and subtracting
∇θλ(θ∗)θ inside the inner product we have

(cid:104)w∗, λ − λ∗(cid:105) = (cid:104)w∗, λ(θ) − λ(θ∗)(cid:105)

(F.2)

= (cid:104)w∗, ∇θλ(θ∗)(θ − θ∗)(cid:105) + (cid:104)w∗, λ(θ) − λ(θ∗) − ∇θλ(θ∗)(θ − θ∗)(cid:105)
≤ 0 + (cid:107)w∗(cid:107)(cid:107)λ(θ) − λ(θ∗) − ∇θλ(θ∗)(θ − θ∗)(cid:107).

where in the last inequality we group terms and apply Cauchy-Schwartz. Note that the Jacobian matrix
∇θλ(θ) is Lipschitz continuous. Denote the Lipschitz constant by Lλ, i.e., (cid:107)∇θλ(θ) − ∇θλ(θ(cid:48))(cid:107) ≤
Lλ(cid:107)θ − θ(cid:48)(cid:107) for all θ, θ(cid:48) ∈ Θ. Then,

(cid:107)λ(θ) − λ(θ∗) − ∇θλ(θ∗)(θ − θ∗)(cid:107) ≤

Lλ
2

(cid:107)θ − θ∗(cid:107)2.

By Assumption 4.1, we know

(cid:107)θ − θ∗(cid:107)2 = (cid:107)g(λ) − g(λ∗)(cid:107)2 ≤ (cid:96)2

θ|||λ − λ∗|||2.

Substituting the above inequalities into (F.2) yields

(cid:104)w∗, λ − λ∗(cid:105) ≤

Lλ(cid:96)2
θ
2

(cid:107)w∗(cid:107)|||λ − λ∗|||2

∀λ ∈ λ(Θ).

(F.3)

Note that (F.3) holds for arbitrary λ ∈ λ(Θ). Therefore, since λ(Θ) is assumed to be convex
(Assumption 4.1(i)), we can also substitute λ with (1 − α)λ∗ + αλ, α ∈ [0, 1] into the above equation,
which yields

α(cid:104)w∗, λ − λ∗(cid:105) ≤

θα2
Lλ(cid:96)2
2

(cid:107)w∗(cid:107)|||λ − λ∗|||2

∀λ ∈ L, ∀α ∈ [0, 1].

24

Variational Policy Gradient

Divide both sides of the preceding expression by α and take α → 0+ gives

(cid:104)w∗, λ − λ∗(cid:105) ≤ lim
α→0+

θα

Lλ(cid:96)2
2

(cid:107)w∗(cid:107)|||λ − λ∗|||2 = 0

∀λ ∈ λ(Θ).

Recall that the following problem is concave in λ:

max
λ

F (λ)

s.t.

λ ∈ λ(Θ),

therefore we conclude that λ∗ is the global optimal solution. Then θ∗ = g(λ∗) is the globally optimal
solution of the nonconvex optimization problem (3.1).

Appendix G. Proof of Theorem 4.4

G.1 Proof of sublinear convergence

Proof. First, the Lipschitz continuity in Assumption 4.3 indicates that

(cid:12)F (λ(θ)) − F (λ(θk)) − (cid:104)∇θF (λ(θk)), θ − θk(cid:105)(cid:12)
(cid:12)

(cid:12) ≤

L
2

(cid:107)θ − θk(cid:107)2.

Consequently, for any θ ∈ Θ we have the ascent property:

F (λ(θ)) ≥ F (λ(θk)) + (cid:104)∇θF (λ(θk)), θ − θk(cid:105) −

L
2

(cid:107)θ − θk(cid:107)2 ≥ F (λ(θ)) − L(cid:107)θ − θk(cid:107)2.

(G.1)

The optimality condition in the policy update rule (4.1) then yields

F (λ(θk+1)) ≥ F (λ(θk)) + (cid:104)∇θF (λ(θk)), θk+1 − θk(cid:105) −

L
2

(cid:107)θk+1 − θk(cid:107)2

= max
θ∈Θ

F (λ(θk)) + (cid:104)∇θF (λ(θk)), θ − θk(cid:105) −

L
2

(cid:107)θ − θk(cid:107)2

(a)
≥ max
θ∈Θ

F (λ(θ)) − L(cid:107)θ − θk(cid:107)2

(b)
≥ max
α∈[0,1]

(cid:8)F (λ(θα)) − L(cid:107)θα − θk(cid:107)2 : θα = g(αλ(θ∗) + (1 − α)λ(θk))(cid:9) .

(G.2)

Here, step (a) is due to (G.1) and step (b) uses the convexity of λ(Θ). Now, we proceed to analyze
the right-hand side of (G.2). First, by the concavity of F and the fact that λ ◦ g = id, we have

F (λ(θα)) = F (αλ(θ∗) + (1 − α)λ(θk)) ≥ αF (λ(θ∗)) + (1 − α)F (λ(θk)).

Moreover, by the Lipschitz continuity assumption of g, we have

(cid:107)θα − θk(cid:107)2 = (cid:107)g(αλ(θ∗) + (1 − α)λ(θk)) − g(λ(θk))(cid:107)2
2
θ|||λ(θ∗) − λ(θk)|||
θD2
λ.

≤ α2(cid:96)2
≤ α2(cid:96)2

(G.3)

Substituting the above two inequalities into the right-hand side of (G.2), we get

F (λ(θ∗)) − F (λ(θk+1))

≤ min
α∈[0,1]

≤ min
α∈[0,1]

(cid:8)F (λ(θ∗)) − F (λ(θα)) + L(cid:107)θα − θk(cid:107)2 : θα = g(αλ(θ∗) + (1 − α)λ(θk))(cid:9)

(1 − α)(cid:0)F (λ(θ∗)) − F (λ(θk))(cid:1) + α2L(cid:96)2

θD2
λ .

(G.4)

25

Zhang, Koppel, Bedi, Szepesvari, and Wang

Let αk = F (Λ(π∗))−F (Λ(πk))
αk ≤ 1.

θD2
λ

2L(cid:96)2

≥ 0, which is the minimizer of the RHS of (G.4) as long as it satisﬁes

Now, we claim the following: If αk ≥ 1 then αk+1 < 1. Further, if αk < 1 then αk+1 ≤ αk. The

two claims together mean that (αk)k is decreasing and all αk are in [0, 1) except perhaps α0.

To prove the ﬁrst of the two claims, assume αk ≥ 1. This implies that F (Λ(π∗)) − F (Λ(πk)) ≥
θD2

λ. Hence, choosing α = 1 in (G.4), we get

2L(cid:96)2

which implies that αk+1 ≤ 1/2 < 1.

To prove the second claim, we plug αk into (G.4) to get

F (λ(θ∗)) − F (λ(θk)) ≤ L(cid:96)2

θD2
λ

F (λ(θ∗)) − F (λ(θk+1)) ≤

(cid:18)

1 −

F (λ(θ∗)) − F (λ(θk))

(cid:19)

4L(cid:96)2

θD2
λ

(F (λ(θ∗)) − F (λ(θk))),

which shows that αk+1 ≤ αk as required.

Now, by our preceding discussion, for k = 1, 2, . . . the previous recursion holds. Using the

deﬁnition of αk, we rewrite this in the equivalent form
αk
2

αk+1
2
By rearranging the preceding expressions and algebraic manipulations, we obtain

αk
2

1 −

≤

(cid:17)

(cid:16)

·

.

2
αk+1

≥

1
(cid:0)1 − αk

2

(cid:1) · αk

2

=

2
αk

+

1
1 − αk
2

≥

2
αk

+ 1.

For simplicity assume that α0 < 1 also holds. Then, 2
αk
F (λ(θ∗)) − F (λ(θ0))
1 + F (λ(θ∗))−F (λ(θ0))
4L(cid:96)2

F (λ(θ∗)) − F (λ(θk)) ≤

≥ 2
α0

· k

θD2
λ

≤

4L(cid:96)2
θD2
λ
k

.

+ k, and consequenlty

A similar analysis holds when α0 > 1. Combining these two gives that F (λ(π∗)) − F (λ(πk)) ≤ 4L(cid:96)2
no matter the value of α0, which proves the result.

k+1

θD2

λ

G.2 Proof of exponential convergence

When the strong concavity of F is available, we further provide the exponential convergence result.

Proof. We start from (G.2) whose proof requires no assumption on strong concavity of F , which is

F (λ(θk+1)) ≥ max
α∈[0,1]

(cid:8)F (λ(θα)) − L(cid:107)θα − θk(cid:107)2 : θα = g(αλ(θ∗) + (1 − α)λ(θk))(cid:9) .

(G.5)

By the µ-strong concavity of F , we have

F (λ(θα)) = F (αλ(θ∗) + (1 − α)λ(θk)) ≥ αF (λ(θ∗)) + (1 − α)F (λ(θk)) +

2
α(1 − α)|||λ(θ∗) − λ(θk)|||

.

µ
2

By the Lipschitz continuity of g, we know that

(cid:107)θα − θk(cid:107) = (cid:107)g(αλ(θ∗) + (1 − α)λ(θk)) − g(λ(θk))(cid:107) ≤ α(cid:96)θ|||λ(θ∗) − λ(θk)|||

Substituting the above two inequalities into the right-hand side of (G.5), we get

F (λ(θ∗)) − F (λ(θk+1))

(G.6)

≤ min
α∈[0,1]

≤ min
α∈[0,1]

(cid:8)F (λ(θ∗)) − F (λ(θα)) + L(cid:107)θα − θk(cid:107)2 : θα = g(αλ(θ∗) + (1 − α)λ(θk))(cid:9)

(1 − α)(cid:0)F (λ(θ∗)) − F (λ(θk))(cid:1) − α

(cid:18) 1 − α
2

(cid:19)

µ − L(cid:96)2

θα

2
|||λ(θ∗) − λ(θk)|||

26

Variational Policy Gradient

Suppose we choose ¯α =
with modulus 1 − ¯α as

1

θ/µ < 1 such that (cid:0) 1− ¯α

2 µ − L(cid:96)2

1+L(cid:96)2

θ ¯α(cid:1) = 0. Then we have a contraction

F (λ(θ∗)) − F (λ(θk+1)) ≤ (1 − ¯α)F (λ(θ∗)) − F (λ(θk)).

Consequently, for any k ≥ 1, we have

F (λ(θ∗)) − F (λ(θk)) ≤ (1 − ¯α)k (cid:0)F (λ(θ∗)) − F (λ(θ0))(cid:1) .

which can be translated into iteration complexity by ﬁxing (cid:15) and initialization θ0, and solving for the
minimal k such that F (λ(θ∗)) − F (λ(θk)) ≤ (cid:15). Doing so is an algebraic exercise which results in

O

(cid:18) 1
¯α

log

(cid:18) F (λ(θ∗)) − F (λ(θ0))
(cid:15)

(cid:19)(cid:19)

= O

(cid:18) L(cid:96)2
θ
µ

log

(cid:18) 1
(cid:15)

(cid:19)(cid:19)

Appendix H. Validating Assumption 4.1 for tabular policy case

For the tabular policy case, the following Proposition holds true and hence the Assumption 4.1 is
satisﬁed in this case.

Proposition H.1. Suppose ξs > 0 for ∀s ∈ S. Then the following hold:

(i). The mappings Π and Λ form a pair of bijections between the convex sets ∆S

A and L;

(ii). ∃Lλ > 0 s.t. (cid:107)∇Λ(π) − ∇Λ(π(cid:48))(cid:107) ≤ Lλ(cid:107)π − π(cid:48)(cid:107), ∀π, π(cid:48) ∈ ∆S
A;

(iii). For all λ, λ(cid:48) ∈ L, we have

(cid:107)Π(λ) − Π(λ(cid:48))(cid:107)2 ≤ 2

(cid:88)

(cid:16) (cid:88)

(λ(cid:48)

sa − λsa)2 + (

(cid:88)

sa − λsa)2(cid:17)
λ(cid:48)

/(cid:0) (cid:88)

(cid:1)2
.

λsa

s

a

a

a

Consequently, (cid:107)Π(λ) − Π(λ(cid:48))(cid:107) ≤ 2

mins ξs

(cid:107)λ − λ(cid:48)(cid:107)1

Proof.
Proof of (i): The equations Π ◦ Λ = idL and Λ ◦ Π = id∆S
or Appendix A of Zhang et al. (2020a).

A

are standard. See, e.g., Altman (1999)

Proof of (ii): For the existence of the Lλ-Lipschitz constant of the gradient ∇Λ, note that the t-th
term of the inﬁnite sum

Λsa(π) =

(cid:18)

γt · P

∞
(cid:88)

t=0

st = s, at = a

(cid:19)

π, s0 ∼ ξ

(cid:12)
(cid:12)
(cid:12)
(cid:12)

is a (t + 1)-th order polynomial. Therefore, Λsa(π) can actually be deﬁned for any π even if π /∈ ∆S
A,
as long as this inﬁnite series of polynomial of π converges absolutely. Note that for ∀π ∈ ∆S
A, since
0 ≤ P(cid:0)st = s, at = a (cid:12)
(cid:12) π, s0 ∼ ξ(cid:1) ≤ 1 this inﬁnite series is absolutely convergent. Because we have
0 < γ < 1, even if we slightly purterb the π within a neighbourhood of it (not necessarily in ∆S
A after
purterbation), the inﬁnite series is still absolutely convergent. This indicates that Λsa is inﬁnitely
continuously diﬀerentiable in an open neighbourhood containing ∆S
A, then due to the compactness of
∆S

A, we are able to argue that there exists a Lλ s.t. ∇Λ is Lλ-Lipschitz continuous within ∆S
A.

27

Zhang, Koppel, Bedi, Szepesvari, and Wang

Proof of (iii): Now, we provide the calculation of the Lipschitz constant of Π. For the ease of
sa. Then for ∀λ, λ(cid:48) ∈ L and ∀(s, a) ∈ S × A,

a∈A λsa and µ(cid:48)

s = (cid:80)

a∈A λ(cid:48)

notation, let us deﬁne µs = (cid:80)
it holds that

Πsa(λ) − Πsa(λ(cid:48)) =

=

=

−

λsa
µs
(cid:18) λsa
µs

λ(cid:48)
sa
µ(cid:48)
s
λ(cid:48)
sa
µs

−

(cid:19)

+

1
µs

(λsa − λ(cid:48)

sa) +

−

(cid:18) λ(cid:48)
sa
µs
s − µs
µsµ(cid:48)
s

µ(cid:48)

(cid:19)

λ(cid:48)
sa
µ(cid:48)
s

λ(cid:48)
sa.

Consequently, we can compute the norm diﬀerence of the preceding expression and apply the triangle
inequality:

(cid:107)Π(λ) − Π(λ(cid:48))(cid:107)2 =

(cid:88)

(cid:88)

s∈S

a∈A

(Πsa(λ) − Πsa(λ(cid:48)))2

(H.1)

≤ 2

≤ 2

(cid:88)

s∈S

(cid:88)

s∈S

(cid:88)

a∈A

1
µ2
s
(cid:32)

1
µ2
s

(cid:88)

a∈A

(λsa − λ(cid:48)

sa)2 + 2

(cid:88)

(cid:88)

s∈S

a∈A

s − µs)2
(µ(cid:48)
s(µ(cid:48)
µ2
(cid:33)

s)2 (λ(cid:48)

sa)2

(λsa − λ(cid:48)

sa)2 + (µ(cid:48)

s − µs)2

,

where the last inequality follows because (cid:107)x(cid:107)2
p-norm). Finally, note that µs ≥ ξs > 0, we have

2 ≤ (cid:107)x(cid:107)2

1 holds for any vector x (here, (cid:107) · (cid:107)p denotes the

(cid:107)Π(λ) − Π(λ(cid:48))(cid:107)2 ≤ 2

(cid:88)

s∈S

1
µ2
s

≤

≤

2
mins ξ2
s

4
mins ξ2
s

(cid:32)

(cid:88)

(λsa − λ(cid:48)

sa)2 + (µ(cid:48)

s − µs)2

(cid:33)

a∈A
(cid:32)

(cid:88)

(cid:88)

s∈S

a∈A

(cid:107)λ − λ(cid:48)(cid:107)2
1

(λsa − λ(cid:48)

sa)2 + (cid:0) (cid:88)

|λsa − λsa(cid:48)|(cid:1)2

(cid:33)

a∈A

Take the square root of both sides completes the proof.

Appendix I. Proof of Theorem 4.5

Proof. To prove this theorem, it suﬃces to observe that (G.2) is still true with θ = π, λ(θ) = Λ(π)
and g(λ) = Π(λ). Therefore, (G.2) can be translated as

F (Λ(πk+1)) ≥ max
α∈[0,1]

(cid:8)F (Λ(πα)) − L(cid:107)πα − πk(cid:107)2 : πα = Π(αΛ(π∗) + (1 − α)Λ(πk))(cid:9) .

(I.1)

By the concavity of F and the fact that Λ ◦ Π = id, we have

F (Λ(πα)) = F (αΛ(π∗) + (1 − α)Λ(πk)) ≥ αF (Λ(π∗)) + (1 − α)F (Λ(πk)).

(I.2)

28

Variational Policy Gradient

For the inequality (G.3), we can derive a tighter bound by the following argument:

(cid:107)πα − πk(cid:107)2 = (cid:107)Π(αΛ(π∗) + (1 − α)Λ(πk)) − Π(Λ(πk))(cid:107)2
sa − λsa)2 + (cid:0) (cid:88)

≤ α2 (cid:88)

(λ∗

(cid:88)

(cid:32)

(I.3)

(cid:33)

λ∗
sa −

(cid:88)

(cid:1)2

λsa

a

(cid:32)

(cid:0) (cid:88)

a

a

λ∗
sa

(cid:1)2

+ (cid:0) (cid:88)

(cid:1)2

λsa

(cid:33)

a

a

(cid:0) (cid:80)

s

(cid:1)2

1
a λsa
1
(cid:1)2
a λsa
ξ (s)(cid:1)2
(cid:0)dπk

(cid:0) (cid:80)

(cid:0)dπ∗

≤ 4α2 (cid:88)

s

= 4α2 (cid:88)

s

= 4α2|S| + 4α2 (cid:88)

≤ 4α2|S| + 4α2|S|

(cid:18)

≤ 4α2|S| ·

ξ (s)(cid:1)2

(cid:33)2

+ (cid:0)dπk
ξ (s)(cid:1)2
(cid:32) dπ∗
ξ (s)
dπk
ξ (s)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

s

(cid:13)
dπ∗
(cid:13)
ξ
(cid:13)
(cid:13)
dπk
(cid:13)
ξ
1 + (1 − γ)−2 (cid:13)
(cid:13)dπ∗
(cid:13)

∞

ξ /ξ

(cid:19)

(cid:13)
2
(cid:13)
(cid:13)

∞

≤

5α2|S|
(1 − γ)2

(cid:13)
(cid:13)dπ∗
(cid:13)

ξ /ξ

(cid:13)
2
(cid:13)
(cid:13)

∞

Denote D := 5|S|
(1−γ)2
(I.1), we get

(cid:13)
(cid:13)dπ∗
(cid:13)

ξ /ξ

(cid:13)
2
(cid:13)
(cid:13)

∞

F (Λ(π∗)) − F (Λ(πk+1))

. Substituting the above two inequalities into the right-hand side of

≤ min
α∈[0,1]

(cid:8)F (Λ(π∗)) − F (Λ(πα)) + L(cid:107)πα − πk(cid:107)2 : πα = Π(αΛ(π∗) + (1 − α)Λ(πk))(cid:9)

≤ min
α∈[0,1]

(1 − α)(cid:0)F (Λ(π∗)) − F (Λ(πk))(cid:1) + LDα2 .

(I.4)

Note that (I.4) diﬀers from (G.4) by replacing (cid:96)2
almost identical to that of Theorem 4.4 and hence we omit the proof.

θD2

λ with D. The latter proof of Theorem 4.5 is

29

