2
2
0
2

n
u
J

6
2

]

V
C
.
s
c
[

3
v
2
6
4
8
0
.
6
0
2
2
:
v
i
X
r
a

Recursive Neural Programs: Variational Learning of
Image Grammars and Part-Whole Hierarchies

Ares Fisher, Rajesh P. N. Rao
Paul G. Allen School of Computer Science and Engineering
University of Washington, Seattle
{aresf, rao}@cs.washington.edu

Abstract

Human vision involves parsing and representing objects and scenes using structured
representations based on part-whole hierarchies. Computer vision and machine
learning researchers have recently sought to emulate this capability using capsule
networks, reference frames and active predictive coding, but a generative model
formulation has been lacking. We introduce Recursive Neural Programs (RNPs),
which, to our knowledge, is the ﬁrst neural generative model to address the part-
whole hierarchy learning problem. RNPs model images as hierarchical trees of
probabilistic sensory-motor programs that recursively reuse learned sensory-motor
primitives to model an image within different reference frames, forming recursive
image grammars. We express RNPs as structured variational autoencoders (sVAEs)
for inference and sampling, and demonstrate parts-based parsing, sampling and
one-shot transfer learning for MNIST, Omniglot and Fashion-MNIST datasets,
demonstrating the model’s expressive power. Our results show that RNPs provide
an intuitive and explainable way of composing objects and scenes, allowing rich
compositionality and intuitive interpretations of objects in terms of part-whole
hierarchies.

1

Introduction

Human visual cognition relies heavily on hierarchical relationships between objects and their parts.
For example, a human face can be modeled as a hierarchical tree of parts, each part’s relative position
speciﬁed within a local reference frame: eyes, nose, mouth etc. positioned within the face’s reference
frame, the parts of an eye (eyebrow, eyelid, iris, pupil etc.) positioned within the eye’s reference
frame, and so on. To emulate such a capability, a computer vision system needs to not only learn
what a part looks like (shapes, contours, colors etc. as in current deep convolutional networks) but
also the relative transformation of the part within a local reference frame, and do this recursively in
order to compose a human face (or a Picasso painting).

Beyond vision, nested structure and hierarchical parts-based decompositions are ubiquitous in human
attributes such as natural language (texts, chapters, paragraphs, sentences, words, characters) and
complex behaviors (cooking a recipe, driving to work, etc.). Such recursive modeling confers the
important property of compositionality [16]: the same building blocks can be hierarchically and
recursively composed into an endless variety of possible patterns, allowing an agent to "imagine"
novel conﬁgurations of parts (e.g., for creating new solutions to problems), and recognize new
conﬁgurations of known parts for zero-shot generalization. The challenge lies in learning a model of
the parts and their transformations that is recursive and composable. Existing approaches for parsing
tree-structured data [1, 16, 7, 6, 18, 19] are either not recursive [1, 18], not compositional [19], not
generative [7, 6], or not differentiable [16]. Indeed, the lack of a smooth “program space" has been a
challenge in this regard.

 
 
 
 
 
 
We introduce recursive neural programs (RNPs), which address this problem by creating a fully
differentiable recursive tree representation of sensory-motor programs. Our model builds on past
work on Active Predictive Coding Networks [3] in using state and action networks but is fully
generative, recursive, and probabilistic, allowing a structured variational approach to inference and
sampling of neural programs. The key differences between our approach and existing approaches
are: 1) Our approach can be extended to arbitrary tree depth, creating a "grammar" for images that
can be recursively applied 2) our approach provides a sensible way to perform gradient descent in
hierarchical “program space,” and 3) our model can be made adaptive by letting information ﬂow
from children to parents in the tree, e.g., via prediction errors [11, 3].

2 Recursive Neural Programs

t , ak

t , ak

state(zk

policy(zk

t+1 = f k

t+1 = f k

t ), and an action transition function (policy) ak

We describe a 2-level Recursive Neural Program (RNP), though the architecture can be generalized
to more levels. Consider the problem of parsing an image of a digit at two levels (k = {1, 2}) of an
abstraction tree (ﬁg. 1), e.g., in terms of larger parts and smaller strokes (henceforth referred to as parts
and sub-parts). A top-level program (k = 2) generates the digit in terms of parts and a bottom-level
program (k = 1) generates each large part as a sequence of smaller parts and their transformations
within the larger part’s reference frame. Each program is expressed as an interaction between two
recurrent functions, a state-transition function (or state-based forward model) that predicts the next
state zk
t ) (ﬁg. 1b,
ﬁg. 2, algorithm 1; in this paper, we assume actions correspond to transformations of parts). This
is similar to the next-state and policy functions in a partially observable Markov decision process
(POMDP [12]).
A program at tree depth k, represented by the state vector zk
t , generates a ﬁxed-length sequence of
τ k lower level states Sk−1 = {zk−1
τ k }.
The state zk
t that corresponds to a stroke or other image
feature, then transformed according to g(ˆxk
t ) to place it on a “canvas” (here a refers to parameters
of an afﬁne transform on a grid, where g is the bilinear interpolation function [10]. The transformed
images are added together at each time step, such that each step increasingly approximates the target
image represented by zk (ﬁg. 2b). This method allows us to reuse the same strokes with different
transformations. For example, if zk
t represents a 4, Sk−1 can represent three straight lines, and T k−1
are the transformations that orient and place them in the conﬁguration of a 4 (ﬁg. 1a).
The above model can be made recursive, with generation performed in a depth-ﬁrst manner: each zk
t
, ..., zk−1
generates the program for a sequence {zk−1
t terminates. Here we
use the decoded patches {ˆxk−1
t (similar to other
predictive coding models [11, 3]).

1
t can be decoded into an image patch ˆxk

τ k } and their transformations T k−1 = {ak−1

τ k−1} as accumulated evidence to update zk

t+1 begins after zk

τ k−1}. zk

, ..., ˆxk−1

, ..., ak−1

, ..., zk−1

t , ak

1

1

1

2.1 Model architecture

t , ak

state and f 2

t ), where ˆe is the input to the f k

In a two-level RNP (ﬁg. 1, ﬁg. 2), the top-level program z2 parameterizes two recurrent neural
networks (RNNs) f 2
policy via a hypernetwork [4] (ﬁg. 2b,c). The hypernet H is an MLP
with seven heads, ﬁve of which generate parameters for the level-speciﬁc networks: an encoder
(3-layer MLP), ˆe = E k(ˆxk
policy networks; two recurrent
networks f k
policy, with hidden size |z|, and their initial hidden states; and two decoders
t , ak
(3-layer MLP’s): ˆxk
t )
translates the hidden state of f k
t+1 (scaling, translation, rotation and shear)
that transform ˆxk
0 to initialize the
sequence generation. More implementation and training details are in the Supplementary Material
section.

t+1. The remaining two heads provide initialization values ˆxk

t+1 = Dk(zk) generates an image patch from zk, and ak

policy into parameters ak

t+1 = T k(hpolicy(zk

state and f k

state and f k

0, ak

We train the model described above by exploiting the end-to-end differentiability of the architec-
ture, minimizing the reconstruction loss between all transformed sub-parts and the target image ˆx,
regularized by the reconstruction at the level of parts:

2

Figure 1: Image Parsing as a Sequence of Transformed Primitives. (a) A 4 can be constructed
by generating three identical straight lines (black circles) and transforming them according to
parameters a to place them in the appropriate location. (b) Left: An MNIST digit decomposed
into an abstraction tree of parts, each of which is further decomposed into smaller sub-parts. Right:
Schematic representation of a parsing tree produced by a recursive neural program. The digit is
described as a “program,” represented by the vector zk, which generates functions f k
policy
to construct the digit by generating parts and transforming them according to the action vector ak
(position, scale, ...) within the digit’s reference frame. Each part is in turn described by a program
zk−1, which generates smaller parts transformed according to ak−1 within that part’s reference frame.

state and f k

L =(cid:107)

τ 2
(cid:88)

τ 1
(cid:88)

g(

t2=1

t1=1

g(ˆx1
t1

, a1
t1

), a2
t2

) − x(cid:107)2

2 +

1
τ 2 (cid:107)

τ 2
(cid:88)

t2=1

g(ˆx2, a2
t2

) − xpatch
t2

(cid:107)2
2

(1)

where τ 2 and τ 1 are the number of level-2 and level-1 time steps respectively, x is the target image
and xpatch
) (i.e. zooming in instead of
t2
scaling down). We note that RNPs can be trained one depth at a time to decrease training time and
resources.

is the image patch generated by transforming x with g−1(a2
t2

To allow probabilistic sampling of programs, we can express an RNP as a structured variational
auto-encoder [14] (VAE) to learn an approximate posterior q(zK|x) ≈ p(zK|x) of an image x given
prior p(zK) ∼ N (0, 1), where zK is the highest level state vector. We therefore use an encoder
network to parameterize the approximate posterior q(zK|x) and regularize eq. (1) with the KL(q||p)
term.

3

Figure 2: Recursive Neural Program for Parsing Images. (a) Parsing tree for a digit, same as
ﬁg. 1b. (b) Representation of parsing tree in (a) with a neural network, where zk is generated by
an encoder network. At time t, the network receives the most recent estimate of the part/sub-part
and ak
t−1 as input, and generates a prediction of the next transformed part or sub-part. Transformed
parts and sub-parts are summed at their respective levels. Purple lines indicate recurrence. In our
implementation, the top level k receives the output of the program k − 1 as input (dotted purple line)
as opposed to its own output (dashed purple line). (c) The RNP module consists of a hypernet H,
which parameterizes f k
policy and auxiliary networks to perform the computation shown in (b).

state, f k

3 Results

We ﬁrst demonstrate how our RNPs can recursively parse input images of MNIST digits [17],
Omniglot characters [16] and Fashion-MNIST objects [20] into parts and sub-parts. We then
characterize the embedding space of state vectors at two levels and show how learned representations
at various tree-depths can be composed to generate previously unseen image types.

3.1

Image parsing into parts and sub-parts

We trained RNPs to reconstruct MNIST digits and Omniglot characters as two-level generative
programs. An encoder network was trained to map the input image to the top-level program
(embedding vector) z2. As described above, z2 parameterizes f 1
policy via a hypernetwork
H, and z1 is the latent code corresponding to the parts (larger patches, 6x6px - 12x12px; ﬁg. 1b). z1
is then passed through the same hypernetwork H to synthesize sub-parts (smaller patches, 1.5x1.5px
- 4x4px; ﬁg. 2b). We force the network to learn a part-wise representation by constraining each
part to be smaller than its parent, therefore requiring a sequence of steps to reconstruct it. Figure 3
shows examples of MNIST digits (ﬁg. 3a), Omniglot characters (ﬁg. 3b) and Fashion-MNIST objects

state and f 1

4

Figure 3: Hierarchical Parts-Based Decomposition by the Model after Learning: Parsing of
(a) four MNIST digits, (b) four Omniglot characters and (c) four Fashion-MNIST objects by the
model. Two levels of the hierarchical representation are shown, parts generated by z2 (left; each part
is denoted by a different color) and sub-parts generated by z1 (right, bordered boxes, each sub-part is
denoted by a different color). Order: blue → red → green → orange. Each bordered box shows the
output of a program generated by z1 to construct a part as a combination of sub-parts transformed
and placed within the reference frame of the part.

(ﬁg. 3c) generated by RNPs, with reconstructions at the level of parts (untiled-) and sub-parts (tiled
images).

3.2 Topography of neural programs

A notable challenge in optimizing and representing probabilistic programs has been the absence of a
continuous program space that can be interpretably manipulated. As we use the same hypernetwork
H to generate programs at all levels, we should expect that programs at different tree depths inhabit
different areas of |z|-dimensional space, i.e. programs representing digits cluster separately from
programs representing parts. Analyzing the embedding space of z2 and z1 vectors that represent the
trained data (MNIST digits or Omniglot characters) reveals that z2 and z1 “neural program” vectors
do cluster separately (ﬁg. 4a,b).
To test the expressiveness of our model, we investigated the space between learned z2 and z1 program
clusters by linearly interpolating in the latent “neural program" space occupied by the z2 and z1
vectors. Sampling from regions between clusters produced programs that generated novel images
(ﬁg. 5), showing that the model can exploit the latent structure of the program embedding space to
synthesize previously unseen patterns by combining the learned parts.

3.3 Compositionality and transfer learning

Compositionality is a main goal of our architecture. With a generative model over programs, we are
able to sample program space in regions outside those representing the trained data (ﬁg. 4). This can
be demonstrated by interpolating between clusters in {z2, z1} (ﬁg. 4c), or sampling randomly from
z2 ∼ N (0, I) (ﬁg. 5). Figure 5 shows that the model can generate novel characters by synthesizing
learned primitives in different, often novel, combinations of parts.

We further tested the compositional ability of our model in a transfer learning task. We trained RNPs
on all MNIST classes but one (7 or 8) and on the Omniglot transfer dataset. By adjusting the weights

5

Figure 4: Topography of Neural Program Space. (a) t-SNE clustering of z2 and z1 vectors in a
model trained on MNIST. A representative image is shown for each cluster. (b) Example clusters of
sampled images from z2 (left column) and z1 (remaining columns) for a model trained on Omniglot.
(c) Example linear interpolations in z space from the center of one cluster (leftmost image) to the
center of another cluster (rightmost image) show novel generated images from neural programs in the
intermediate space. Left: MNIST, right: Omniglot.

of the encoder network (but not the decoder hypernetwork H), RNPs were able to synthesize parts
for the unseen class (ﬁg. 6).

4 Conclusion

In this paper, we introduced Recursive Neural Programs (RNPs), a new model for differentiably
learning tree-structured data as sensory-motor sequences in a way that allows ﬂexible composition of
learned primitives using a recursive “grammar.” We demonstrated our model’s ability to generate
images using a hierarchy of parts and their transformations. Our architecture can also be applied to
learning in arbitrary domains, such as audio, video and other dynamical processes such as motor
behavior.

There are several potential directions for future research. Using the same hypernetwork at different
levels allows natural recursion, but limits the expressive power of the model. This can be addressed
by learning different hypernetworks for different levels. Hypernetworks describing different data
modalities (e.g. audio, visual, etc.) could be combined to generate richer multi-modal neural
programs, provided constraints on the size of the primary network are taken into account [2]. Training
deep RNPs across levels and across time steps can be challenging. This could be addressed by
training RNPs at different depths in parallel. Another potential area for improvement is replacing
bilinear interpolation used for transformation of image primitives, which can result in poor quality
gradients, with smoother functions to sample images (e.g. [15]). Finally, message passing between

6

Figure 5: Sampling from the Prior. Sampling of z2 from N (0, 1) for a model trained on MNIST
digits (left), Omniglot characters (middle) and Fashion MNIST objects (right). As in (ﬁg. 3), part
order is blue → red → green → orange.

Figure 6: Transfer Learning. RNPs trained on a training set of classes (see text) are able to explain
novel examples from unseen classes and synthesize the parts for MNIST digits (a,b) and Omniglot
characters (c).

nodes at different tree depths could allow for bidirectional information ﬂow: predictions from parents
to children, and belief updates from children to parents (using, e.g., prediction errors). We intend to
explore such predictive coding-based architectures for RNPs in future work.

Acknowledgments

We thank Dimitrios Gklezakos for his help with hypernetworks, and Preston Jiang for feedback
on probabilistic aspects of the model. This material is based upon work supported by the Defense
Advanced Research Projects Agency (DARPA) under Contract No. HR001120C0021, a Weill
Neurohub Investigator award, a grant from the Templeton World Charity Foundation, and a Cherng
Jia & Elizabeth Yun Hwang Professorship to RPNR. The opinions expressed in this publication are
those of the authors and do not necessarily reﬂect the views of the funders.

References

[1] S. M. Ali Eslami et al. “Attend,

Infer, Repeat: Fast Scene Understanding with
Generative Models”. In: Advances in Neural Information Processing Systems. 2016.
URL: https : / / proceedings . neurips . cc / paper / 2016 / hash /
52947e0ade57a09e4a1386d08f17b656-Abstract.html.

[2] Tomer Galanti and Lior Wolf. “On the Modularity of Hypernetworks”. In: arXiv:2002.10006
[cs, stat] (Nov. 2020). arXiv: 2002.10006. URL: http://arxiv.org/abs/2002.10006.
[3] Dimitrios C. Gklezakos and Rajesh P. N. Rao. Active Predictive Coding Networks: A Neural
Solution to the Problem of Learning Reference Frames and Part-Whole Hierarchies. en.
preprint. Neuroscience, Jan. 2022. DOI: 10.1101/2022.01.20.477125.

7

[4] David Ha, Andrew Dai, and Quoc V. Le. “HyperNetworks”. In: arXiv:1609.09106 [cs] (Dec.

2016). arXiv: 1609.09106. URL: http://arxiv.org/abs/1609.09106.

[5] Kaiming He et al. “Deep Residual Learning for Image Recognition”. In: 2016, pp. 770–778.
URL: https : / / openaccess . thecvf . com / content _ cvpr _ 2016 / html / He _ Deep _
Residual_Learning_CVPR_2016_paper.html.

[6] Geoffrey Hinton. “How to represent part-whole hierarchies in a neural network”. en. In:
arXiv:2102.12627 (Feb. 2021). DOI: 10 . 48550 / arXiv . 2102 . 12627. URL: https : / /
arxiv.org/abs/2102.12627v1.

[7] Geoffrey Hinton, Sara Sabour, and Nicholas Frosst. “Matrix Capsules with EM Routing”. en.

In: ICLR (2018), p. 15.

[8] Michael Innes. Don’t Unroll Adjoint: Differentiating SSA-Form Programs. Tech. rep. Publica-
tion Title: arXiv e-prints ADS Bibcode: 2018arXiv181007951I Type: article. Oct. 2018. URL:
https://ui.adsabs.harvard.edu/abs/2018arXiv181007951I.

[9] Michael Innes et al. Fashionable Modelling with Flux. Tech. rep. arXiv:1811.01457.
arXiv:1811.01457 [cs] type: article. arXiv, Nov. 2018. DOI: 10.48550/arXiv.1811.01457.
URL: http://arxiv.org/abs/1811.01457.

[10] Max Jaderberg et al. “Spatial Transformer Networks”. In: Advances in Neural Information
Processing Systems. 2015. URL: https : / / proceedings . neurips . cc / paper / 2015 /
hash/33ceb07bf4eeb3da587e268d663aba1a-Abstract.html.

[11]

Jiang, Preston L, Gklezakos, Dimitrios, and Rajesh P. N. Rao. Dynamic Predictive Coding with
Hypernetworks | bioRxiv. 2021. URL: https://www.biorxiv.org/content/10.1101/
2021.02.22.432194v2.abstract.

[12] Leslie Pack Kaelbling, Michael L. Littman, and Anthony R. Cassandra. “Planning and acting
in partially observable stochastic domains”. en. In: Artiﬁcial Intelligence 101.1-2 (May 1998),
pp. 99–134. ISSN: 00043702. DOI: 10. 1016/S0004 - 3702(98) 00023 - X. URL: https:
//linkinghub.elsevier.com/retrieve/pii/S000437029800023X.

[13] Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. Tech. rep.
arXiv:1412.6980. arXiv:1412.6980 [cs] type: article. arXiv, Jan. 2017. DOI: 10.48550/arXiv.
1412.6980. URL: http://arxiv.org/abs/1412.6980.

[14] Diederik P. Kingma and Max Welling. “Auto-Encoding Variational Bayes”.

In:
arXiv:1312.6114 [cs, stat] (May 2014). arXiv: 1312.6114. URL: http : / / arxiv . org /
abs/1312.6114.

[15] Sylwester Klocek et al. “Hypernetwork Functional Image Representation”. en. In: Artiﬁcial
Neural Networks and Machine Learning – ICANN 2019: Workshop and Special Sessions. 2019.
ISBN: 978-3-030-30493-5. DOI: 10.1007/978-3-030-30493-5_48.

[16] Brenden M. Lake, Ruslan Salakhutdinov, and Joshua B. Tenenbaum. “Human-level concept
learning through probabilistic program induction”. In: Science 350.6266 (Dec. 2015), pp. 1332–
1338. DOI: 10.1126/science.aab3050. URL: https://www.science.org/doi/full/
10.1126/science.aab3050.

[17] LeCun et al., Yann. Gradient-based learning applied to document recognition | IEEE Journals
& Magazine | IEEE Xplore. 1998. URL: https://ieeexplore.ieee.org/document/
726791.

[18] Volodymyr Mnih et al. “Recurrent Models of Visual Attention”. In: Advances in Neural
Information Processing Systems. 2014. URL: https://proceedings.neurips.cc/paper/
2014/hash/09c6c3783b4a70054da74f2538ed47c6-Abstract.html.

[19] Richard Socher et al. “Parsing Natural Scenes and Natural Language with Recursive Neural

Networks”. en. In: Jan. 2011. URL: https://openreview.net/forum?id=SyEeunWObH.

[20] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: a Novel Image Dataset for
Benchmarking Machine Learning Algorithms. Tech. rep. arXiv:1708.07747. arXiv:1708.07747
[cs, stat] type: article. arXiv, Sept. 2017. DOI: 10.48550/arXiv.1708.07747. URL: http:
//arxiv.org/abs/1708.07747.

8

Supplementary Material

Model algorithm

state, dec1

policy, dec1

t2−1)
t2−1)

Algorithm 1 Two-level image generation
1: z2 ← Encoder(x)
state, f 1
2: enc1, f 1
3: a1
0 ← init(z2)
0, z1
4: for t2 = 1 : τ2 do
state(a1
= f 2
z1
t2−1, z1
5:
t2
policy(a1
a1
= f 2
t2−1, z1
6:
t2
ˆx2 = dec1
state(z1
)
7:
t2
(cid:46) Bottom-level loop
state, f 0
enc0, f 0
a0
0, z0
0 ← init(z1)
for t1 = 1 : τ1 do
z0
t1−1, z0
= f 1
state(a0
t1
t1−1, z0
policy(a0
= f 1
a0
t1
state(z0
t = dec0
ˆx1
)
t1
t , a0
t ← p1 + g(ˆx1
p1
t )
t ← p2 + g(p1, a1
p2
t )

8:
9:
10:
11:
12:
13:
14:
15:

t1−1)
t1−1)

policy, dec0

policy ← H(z2)

state, dec0

policy ← H(z1)

(cid:46) transformed reconstruction

Model and training details

Parameters

RNPs consist of a hypernet H that generates the parameters of an autoregressive network for a level k.
All hypernetworks used in this study are 6-layer MLP’s (64 units, elu activations), with seven heads,
parameterizing networks of the same size (except to reﬂect different sizes of |z|). All networks for a
given k consisted of fully connected layers of 64 units, except the RNNs f k
policy, which
retained the dimensionality of |z|.

state and f k

The encoder network consisted of ﬁve ResNet blocks (32 channels) [5] and four fully connected
layers (64 units).

Training

We trained all models using the ADAM optimizer [13] with a learning rate of 4e-5, which reliably
showed convergence. We trained our models for 200 epochs, except on the Omniglot dataset where
we trained for 400 epochs. We used |z| = 32 on MNIST and Fashion-MNIST, and |z| = 96 for
Omniglot. Models were trained on a single GPU (Nvidia Quadro RTX 6000).

4.0.1 Software

All experiment code was written in Julia using Flux.jl [9] and Zygote.jl [8]. Code is publicly available
at https://github.com/FishAres/RNP.

9

