1
2
0
2

g
u
A
1
1

]
S
D
.
s
c
[

1
v
6
9
6
5
0
.
8
0
1
2
:
v
i
X
r
a

Correlation Clustering with
Asymmetric Classiﬁcation Errors∗

Jafar Jafarov†
University of Chicago

Sanchit Kalhan†
Northwestern University

Konstantin Makarychev†
Northwestern University

Yury Makarychev†
Toyota Technological Institute at Chicago

Abstract

In the Correlation Clustering problem, we are given a weighted graph G with its edges
labelled as “similar” or “dissimilar” by a binary classiﬁer. The goal is to produce a clustering
that minimizes the weight of “disagreements”: the sum of the weights of “similar” edges across
clusters and “dissimilar” edges within clusters. We study the correlation clustering problem
under the following assumption: Every “similar” edge e has weight we
[αw, w] and every
∈
“dissimilar” edge e has weight we
1 and w > 0 is a scaling parameter). We
give a (3+2 loge(1/α)) approximation algorithm for this problem. This assumption captures well
the scenario when classiﬁcation errors are asymmetric. Additionally, we show an asymptotically
matching Linear Programming integrality gap of Ω(log 1/α).

αw (where α

≥

≤

1

Introduction

C

In the Correlation Clustering problem, we are given a set of objects with pairwise similarity infor-
mation. Our aim is to partition these objects into clusters that match this information as closely
as possible. The pairwise information is represented as a weighted graph G whose edges are la-
belled as “positive/similar” and “negative/dissimilar” by a noisy binary classiﬁer. The goal is to
that minimizes the weight of edges disagreeing with this clustering: A positive
ﬁnd a clustering
edge is in disagreement with
, if its endpoints belong to distinct clusters; and a negative edge
if its endpoints belong to the same cluster. We call this objective the
is in disagreement with
MinDisagree objective. The MinDisagree objective has been extensively studied in literature since it
was introduced by Bansal, Blum, and Chawla [2004] (see e.g., Charikar et al. [2003], Demaine et al.
[2006], Ailon et al. [2008], Pan et al. [2015], Chawla et al. [2015]). There are currently two standard
models for Correlation Clustering which we will refer to as (1) Correlation Clustering on Complete
Graphs and (2) Correlation Clustering with Noisy Partial Information. In the former model, we
assume that graph G is complete and all edge weights are the same i.e., G is unweighted. In the
latter model, we do not make any assumptions on the graph G. Thus, edges can have arbitrary

C

C

∗The conference version of this paper appeared in the proceedings of ICML 2020.
†Equal contribution. Jafar Jafarov and Yury Makarychev were supported in part by NSF CCF-1718820 and NSF
TRIPODS CCF-1934843. Sanchit Kalhan and Konstantin Makarychev were supported in part by NSF TRIPODS
CCF-1934931.

1

 
 
 
 
 
 
weights and some edges may be missing. These models are quite diﬀerent from the computa-
tional perspective. For the ﬁrst model, Ailon, Charikar, and Newman [2008] gave a 2.5 approxi-
mation algorithm. This approximation factor was later improved to 2.06 by Chawla, Makarychev,
Schramm, and Yaroslavtsev [2015]. For the second model, Charikar, Guruswami, and Wirth [2003]
and Demaine, Emanuel, Fiat, and Immorlica [2006] gave an O(log n) approximation algorithm,
they also showed that Correlation Clustering with Partial Noisy Information is as hard as the
Multicut problem and, hence, O(log n) is likely to be the best possible approximation for this
In this paper, we show how to interpolate between these two models for Correlation
problem.
Clustering.

≤

We study the Correlation Clustering problem on complete graphs with edge weights. In our
model, the weights on the edges are constrained such that the ratio of the lightest edge in the graph
1. Thus, if w is the weight of the heaviest positive
to the heaviest positive edge is at least α
edge in the graph, then each positive edge has weight in [αw, w] and each negative edge has weight
greater than or equal to αw. We argue that this model – which we call Correlation Clustering with
Asymmetric Classiﬁcation Errors – is more adept at capturing the subtleties in real world instances
than the two standard models. Indeed, the assumptions made by the Correlation Clustering on
Complete Graphs model are too strong, since rarely do real world instances have equal edge weights.
In contrast, in the Correlation Clustering with Noisy Partial Information model we can have edge
weights that are arbitrarily small or large, an assumption which is too weak. In many real world
instances, the edge weights lie in some range [a, b] with a, b > 0. Our model captures a larger family
of instances.

Furthermore, the nature of classiﬁcation errors for objects that are similar and objects that are
dissimilar is quite diﬀerent. In many cases, a positive edge uv indicates that the classiﬁer found
some actual evidence that u and v are similar; while a negative edge simply means that the classiﬁer
could not ﬁnd any such proof that u and v are similar, it does not mean that the objects u and v
are necessarily dissimilar. In some other cases, a negative edge uv indicates that the classiﬁer found
some evidence that u and v are dissimilar; while a positive edge simply means that the classiﬁer
could not ﬁnd any such proof. We discuss several examples below. Note that in the former case,
a positive edge gives a substantially stronger signal than a negative edge and should have a higher
weight; in the latter, it is the other way around: a negative edge gives a stronger signal than a
positive edge and should have a higher weight. We make this statement more precise in Section 1.1.
The following examples show how the Correlation Clustering with Asymmetric Classiﬁcation
Errors model can help in capturing real world instances. Consider an example from the paper on
Correlation Clustering by Pan, Papailiopoulos, Oymak, Recht, Ramchandran, and Jordan [2015].
In their experiments, Pan et al. [2015] used several data sets including dblp-2011 and ENWiki-
2013 1. In the graph dblp-2011, each vertex represents a scientist and two vertices are connected
with an edge if the corresponding authors have co-authored an article. Thus, a positive edge with
weight w+ between Alice and Bob in the Correlation Clustering instance indicates that Alice and
Bob are coauthors, which strongly suggests that Alice and Bob work in similar areas of Computer
Science. However, it is not true that all researchers working in some area of computer science have
co-authored papers with each other. Thus, the negative edge that connects two scientists who do
not have an article together does not deserve to have the same weight as a positive edge, and thus
can be modeled as a negative edge with weight w− < w+.

Similarly, the vertices of the graph ENWiki-2013 are Wikipedia pages. Two pages are connected

1These data sets are published by Boldi and Vigna [2004], Boldi et al. [2011, 2004, 2014]

2

with an edge if there is a link from one page to another. A link from one page to the other is a
strong suggestion that the two pages are related and hence can be connected with a positive edge
of weight w+, while it is not true that two similar Wikipedia pages necessarily should have a link
from one to the other. Thus, it would be better to join such pages with a negative edge of weight
w− < w+.

Consider now the multi-person tracking problem. The problem is modelled as a Correlation
Clustering or closely related Lifted Multicut Problem Tang et al. [2016, 2017] on a graph, whose
vertices are people detections in video sequences. Two detections are connected with a positive or
negative edge depending on whether the detected people have similar or dissimilar appearance (as
well as some other information). In this case, a negative edge (u, v) is more informative since it
signals that the classiﬁer has identiﬁed body parts that do not match in detections u and v and
thus the detected people are likely to be diﬀerent (a positive edge (u, v) simply indicates that the
classiﬁer was not able to ﬁnd non-matching body parts).

The Correlation Clustering with Asymmetric Classiﬁcation Errors model captures the examples
we discussed above. It is instructive to consider an important special case where all positive edges
= w−. If we were to use the state
have weight w+ and all negative edges have weight w− with w+
of the art algorithm for Correlation Clustering on Complete Graphs on our instance for Correla-
tion Clustering with Asymmetric Classiﬁcation Errors (by completely ignoring edge weights and
looking at the instance as an unweighted complete graph), we would get a Θ(max(w+/w−, w−/w+))
approximation to the MinDisagree objective. While if we were to use the state of the art algorithms
for Correlation Clustering with Noisy Partial Information on our instance, we would get a O(log n)
approximation to the MinDisagree objective.
Our Contributions. In this paper, we present an approximation algorithm for Correlation Clus-
tering with Asymmetric Classiﬁcation Errors. Our algorithm gives an approximation factor of
1/α. Consider the scenario discussed above where all positive edges have weight w+
A = 3 + 2 loge
and all negative edges have weight w−. If w+
w−, our algorithm gets a (3 + 2 loge w+/w−)
approximation; if w+

w−, our algorithm gets a 3-approximation.

≥

≤

Deﬁnition 1. Correlation Clustering with Asymmetric Classiﬁcation Errors is a variant of Cor-
relation Clustering on a Complete Graph. We assume that the weight we of each positive edge lies
in [αw, w] and the weight we of each negative edge lies in [αw,

(0, 1] and w > 0.

), where α

∞

∈

We note here that the assumption that the weight of positive edges is bounded from above
is crucuial. Without this assumption (even if we require that negative weights are bounded from
above and below), the LP gap is unbounded for every ﬁxed α (this follows from the integrality gap
example we present in Theorem 1.3).

The following is our main theorem.

Theorem 1.1. There exists a polynomial time A = 3 + 2 loge 1/α approximation algorithm for
Correlation Clustering with Asymmetric Classiﬁcation Errors.

We also study a natural extension of our model to the case of complete bipartite graphs. That is,
the positive edges across the biparition have a weight between [αw, w] and the negative edges across
the bipartition have a weight of at least αw. Note that the state-of-the-art approximation algorithm
for Correlation Clustering on Unweighted Complete Bipartite Graphs has an approximation factor
of 3 (see Chawla et al. [2015]).

3

6
Theorem 1.2. There exists a polynomial time A = 5 + 2 loge 1/α approximation algorithm for
Correlation Clustering with Asymmetric Classiﬁcation Errors on complete bipartite graphs.

Our next result shows that this approximation ratio is likely best possible for LP-based al-
gorithms. We show this by exhibiting an instance of Correlation Clustering with Asymmetric
Classiﬁcation Errors such that integrality gap for the natural LP for Correlation Clustering on this
instance is Ω(log 1/α).

Theorem 1.3. The natural Linear Programming relaxation for Correlation Clustering has an in-
tegrality gap of Ω(log 1/α) for instances of Correlation Clustering with Asymmetric Classiﬁcation
Errors.

Moreover, we can show that if there is an o(log(1/α))-approximation algorithm whose running
time is polynomial in both n and 1/α, then there is an o(log n)
approximation algorithm for the
general weighted case2(and also for the MultiCut problem). However, we do not know if there
approximation algorithm for the problem whose running time is polynomial in
is an o(log(1/α))
−
in 1/α. The existence of such an algorithm does not imply that there is an
n and exponential
o(log n)

approximation algorithm for the general weighted case (as far as we know).

−

We show a similar integraplity gap result for the Correlation Clustering with Asymmetric Clas-

−

siﬁcation Errors on complete bipartite graphs problem.

Theorem 1.4. The natural Linear Programming relaxation for Correlation Clustering has an in-
tegrality gap of Ω(log 1/α) for instances of Correlation Clustering with Asymmetric Classiﬁcation
Errors on complete bipartite graphs.

Throughout the paper, we denote the set of positive edges by E+ and the set of negative edges
by E−. We denote an instance of the Correlation Clustering problem by G = (V, E+, E−). We
denote the weight of edge e by we.

1.1 Ground Truth Model

In this section, we formalize the connection between asymmetric classiﬁcation errors and asymmet-
ric edge weights. For simplicity, we assume that each positive edge has a weight of w+ and each
negative edge has a weight of w−. Consider a probabilistic model in which edge labels are assigned
T ) be the ground truth clustering of the vertex set V . The
by a noisy classiﬁer. Let
classiﬁer labels each edge within a cluster with a “+” edge with probability p+ and as a “
” edge
” edge with
with probability 1
probability q− and as a “+” edge with probability 1
q−) are the
classiﬁcation error probabilities. We assume that all classiﬁcation errors are independent.

p+; it labels each edge with endpoints in distinct clusters as a “
p+) and (1

q−. Thus, (1

1 , . . . C ∗

∗ = (C ∗

−
−

−

−

−

−

C

We note that similar models have been previously studied by Bansal et al. [2004], Elsner and Schudy

[2009], Mathieu and Schudy [2010], Ailon et al. [2013], Makarychev et al. [2015] and others. How-
q−),
ever, the standard assumption in such models was that the error probabilities, (1
are less than a half; that is, p+ > 1/2 and q− > 1/2. Here, we investigate two cases (i) when

p+) and (1

−

−

2The reduction to the general case works as follows. Consider an instance of Correlation Clustering with arbitrary
weights. Guess the heaviest edge e that is in disagreement with the optimal clustering. Let we be its weight, and
set w = n2we, and α = 1/n4. Then, assign new weights to all pairs of vertices in the graph. Keep the weights of
all edges with weight in the range [αw, w]. Set the weights of all edges with weight greater than w to w and the
weights of all edges with weight less than αw (including missing edges) to αw.

4

p+ < 1/2 < q− and (ii) when q− < 1/2 < p+. We assume that p+ + q− > 1, which means that the
classiﬁer is more likely to connect similar objects with a “+” than dissimilar objects or, equivalently,
that the classiﬁer is more likely to connect dissimilar objects with a “
” than similar objects. For
instance, consider a classiﬁer that looks for evidence that the objects are similar: if it ﬁnds some
evidence, it adds a positive edge; otherwise, it adds a negative edge (as described in our examples
dblp-2011 and ENWiki-2013 in the Introduction). Say, the classiﬁer detects a similarity between
two objects in the same ground truth cluster with a probability of only 30% and incorrectly detects
similarity between two objects in diﬀerent ground truth clusters with a probability of 10%. Then,
it will add a negative edge between two similar objects with probability 70%! While this scenario is
not captured by the standard assumption, it is captured by case (i) (here, p+ = 0.3 < 1/2 < q− = 0.9
and p+ + q− > 1).

−

Consider a clustering

with both endpoints in the same cluster by In+(
C
edges and negative edges with endpoints in diﬀerent clusters by Out+(
C
Then, the log-likelihood function of the clustering

of the vertices. Denote the sets of positive edges and negative edges
), respectively, and the sets of positive
), respectively.

) and Out−(
C

) and In−(
C

is,

C

C

p+)

(1

×

−
Y(u,v)∈Out+(C)

q−)

×

Y(u,v)∈Out−(C)

q−)|Out+(C)|(q−)|Out

−

(C)|

q−

(cid:17)

ℓ(G;

C

) = log

= log

(cid:16) Y(u,v)∈In+(C)
(p+)|In+(C)|(1
log p+ +

)
|
log p+ +

p+

(1
×
−
Y(u,v)∈In−(C)
p+)|In

−

−
In−(
)
C
|
|
log q−
E−
|

|

(C)|

log(1

−

(cid:17)

constant expression

=

=

(cid:16)
In+(
C
|
E+
|
(cid:16)

|

(1

·

−
p+) +

−
Out+(
C
|
(cid:16)

(cid:17)
q−) +

log(1

Out+(
)
|
C
|
p+
q− +

log

)
|

1

−
In−(
C
|

log

)
|

Out−(
C
|
q−

−

MinDisagree objective

.

p+

1

−

(cid:17)

log q−

)
|

}

{z

|
1−q− and w− = log q−
Let w+ = log p+
Out+(
– equals w+
+w−
)
C
|
|
In−(
is the number of negative edges disagreeing with
)
and
with
|
C
|
E+
|
(cid:16)

1−p+ . Then, the negative term –
Out+(
. Note that
)
C
|
|

Now observe that the ﬁrst term in the expression above –

In−(
C
|

)
|

(cid:16)

C

|

– does
. It only depends on the instance G = (V, E+, E−). Thus, maximizing the log-

|

|

(cid:17)

E−
|

log q−

{z
Out+(
C
|

)
|

log p+

1−q− +

}
In−(
C
|

)
|

log q−
1−p+

is the number of positive edges disagreeing

.

C
log p+ +

(cid:17)

not depend on
likelihood function over

C

is equivalent to minimizing the following objective

C

w+(# disagreeing “+”edges) + w−(# disagreeing “

−

”edges).

Note that we have w+ > w− when p+ < 1/2 < q− (case (i) above); in this case, a “+” edge
” edge. Similarly, we have w− > w+ when q− < 1/2 < p+ (case

” edge gives a stronger signal than a “+” edge.

gives a stronger signal than a “
(ii) above); in this case, a “

−

−

2 Algorithm

In this section, we present an approximation algorithm for Correlation Clustering with Asymmetric
Classiﬁcation Errors. The algorithm ﬁrst solves a standard LP relaxation and assigns every edge
a length of xuv (see Section 2.1). Then, one by one it creates new clusters and removes them from
the graph. The algorithm creates a cluster C as follows. It picks a random vertex p, called a pivot,
[0, 1]. Then, it adds the pivot p and all
among yet unassigned vertices and a random number R

∈

5

Algorithm 1 Approximation Algorithm
input An instance of Correlation Clustering with Asymmetric Weights G = (V, E+, E−, we).

Initialize t = 0 and Vt = V .
= ∅ do
while Vt
Pick a random pivot pt
∈
Choose a radius R uniformly at random in [0, 1].
Create a new cluster St; add the pivot pt to St.
for all u

Vt do

Vt.

∈
if f (xptu)

Add u to St.

≤

R then

end if
end for
Let Vt+1 = Vt

end while

output clustering

St and t = t + 1.

\

= (S0, . . . , St−1).

S

vertices u with f (xpu)
deﬁne below. We give a pseudo-code for this algorithm in Algorithm 1.

R to C, where f : [0, 1]

→

≤

[0, 1] is a properly chosen function, which we

Our algorithm resembles the LP-based correlation clustering algorithms by Ailon et al. [2008]
and Chawla et al. [2015]. However, a crucial diﬀerence between our algorithm and above mentioned
algorithms is that our algorithm uses a “dependant” rounding. That is, if for two edges pv1 and
R at some step t of the algorithm then both v1 and v2 are
pv2, we have f (xpv1)
added to the new cluster St. The algorithms by Ailon et al. [2008] and Chawla et al. [2015] make
decisions on whether to add v1 to St and v2 to St, independently. Also, the choice of the function
f is quite diﬀerent from the functions used by Chawla et al. [2015]. In fact, it is inﬂuenced by the
paper by Garg, Vazirani, and Yannakakis [1996].

R and f (xpv2)

≤

≤

2.1 Linear Programming Relaxation

In this section, we describe a standard linear programming (LP) relaxation for Correlation Clus-
tering which was introduced by Charikar, Guruswami, and Wirth [2003]. We ﬁrst give an integer
programming formulation of the Correlation Clustering problem. For every pair of vertices u and
v, the integer program (IP) has a variable xuv
, which indicates whether u and v belong to
0, 1
}
the same cluster:

∈ {

• xuv = 0, if u and v belong to the same cluster; and
• xuv = 1, otherwise.

We require that xuv = xvu, xuu = 0 and all xuv satisfy the triangle inequality. That is, xuv + xvw
xuw.

≥

Every feasible IP solution x deﬁnes a partitioning

= (S1, . . . , ST ) in which two vertices u and
v belong to the same cluster if and only if xuv = 0. A positive edge uv is in disagreement with this
partitioning if and only if xuv = 1; a negative edge uv is in disagreement with this partitioning if
and only if xuv = 0. Thus, the cost of the partitioning is given by the following linear function:

S

wuvxuv +

Xuv∈E+

Xuv∈E−

6

wuv(1

xuv).

−

6
subject to

min

wuvxuv +

Xuv∈E+

Xuv∈E−

wuv(1

xuv).

−

xuv + xvw

xuw
≤
xuv = xvu
xuu = 0
xuv

[0, 1]

∈

for all u, v, w

for all u, v

for all u

for all u, v

V

V

V

V

∈

∈

∈

∈

Algorithm 2 One iteration of Algorithm 1 on triangle uvw

Figure 1: LP relaxation

Pick a random pivot p
.
}
Choose a random radius R with the uniform distribution in [0, 1].
Create a new cluster S. Insert p in S.
for all a

u, v, w

u, v, w

∈ {

do

p
} \ {
R then

}

∈ {
if fα(xpa)

Add a to S .

≤

end if
end for

We now replace all integrality constraints xuv

in the integer program with linear
0, 1
}
[0, 1] . The obtained linear program is given in Figure 1. In the paper, we refer

∈ {

constraints xuv
to each variable xuv as the length of the edge uv.

∈

3 Analysis of the Algorithm

The analysis of our algorithm follows the general approach proposed by Ailon, Charikar, and Newman
[2008]. Ailon et al. [2008] observed that in order to get upper bounds on the approximation factors
of their algorithms, it is suﬃcient to consider how these algorithms behave on triplets of vertices.
Below, we present their method adapted to our settings. Then, we will use Theorem 3.1 to analyze
our algorithm.

3.1 General Approach: Triple-Based Analysis

Consider an instance of Correlation Clustering G = (V, E+, E−) on three vertices u, v, w. Suppose
, respectively. We shall call this
that the edges uv, vw, and uw have signs σuv, σvw, σuw
instance a triangle (u, v, w) and refer to the vector of signs σ = (σvw, σuw, σuv) as the signature of
the triangle (u, v, w).

∈ {±}

Let us now assign arbitrary lengths xuv, xvw, and xuw satisfying the triangle inequality to the
edges uv, vw, and uw and run one iteration of our algorithm on the triangle uvw (see Algorithm 2).

7

We say that a positive edge uv is in disagreement with S if u
S. Similarly, a negative edge uv is in disagreement with S if u, v

v
probability that the edge (u, v) is in disagreement with S given that w is the pivot.

S and v /
∈
S. Let cost(u, v

∈
∈

∈

S and
S or u /
∈
w) be the

|

cost(u, v

w) =

|

Pr(u
Pr(u

(

S, v /
∈
S, v
∈

∈
∈

S, v

S or u /
∈
p = w),
S

|

S

|

∈

p = w),

if σuv = “+”;
”.
if σuv = “

−

Let lp(u, v

w) be the LP contribution of the edge (u, v) times the probability of it being

removed, conditioned on w being the pivot.

|

lp(u, v

w) =

|

xuv
(1

·
−

(

Pr(u
xuv)

·

∈
Pr(u

S or v

S
∈
|
S or v

p = w),
S

∈

|

∈

p = w),

if σuv = “+”;
”.
if σuv = “

−

We now deﬁne two functions ALGσ(x, y, z) and LP σ(x, y, z). To this end, construct a triangle

(u, v, w) with signature σ edge lengths x, y, z (where xvw = x, xuw = y, xuv = z). Then,

ALGσ(x, y, z) = wuv
LP σ(x, y, z) = wuv

·

·

cost(u, v

lp(u, v

|

w) + wuw

|
w) + wuw

cost(u, w

v) + wvw

cost(v, w

·
lp(u, w

|
v) + wvw

·
lp(v, w

u);

|

·

|

·

u).

|

We will use the following theorem from the paper by Chawla, Makarychev, Schramm, and Yaroslavtsev
[2015] (Lemma 4) to analyze our algorithm. This theorem was ﬁrst proved by Ailon, Charikar, and Newman
[2008] but it was not stated in this form in their paper.

Theorem 3.1 (see Ailon et al. [2008] and Chawla et al. [2015]). Consider a function fα with
fα(0) = 0.
) and edge lengths x, y,
∈ {±}
and z satisfying the triangle inequality, we have ALGσ(x, y, z)
ρLP σ(x, y, z), then the approxi-
mation factor of the algorithm is at most ρ.

If for all signatures σ = (σ1, σ2, σ3) (where each σi

≤

3.2 Analysis of the Approximation Algorithm

Proof of Theorem 1.1. Without loss of generality we assume that the scaling parameter w is 1. We
0.169, we
use diﬀerent functions for α
deﬁne fα(x) as follows (see Figure 2):

0.169. Let A = 3 + 2 loge 1/α. For α

0.169 and α

≤

≥

≤

fα(x) =

e−Ax,

1
1,

−

(cid:26)

and, for α

≥

0.169, we deﬁne fα(x) as follows:

x < 1

if 0
≤
otherwise;

2 −

1
2A ;

fα(x) =

0,
1−α
3 ,

1,




if x < 1
A
if 1
x < 1
2 −
1
1
if x
2A
2 −

A ≤
≥

1
2A

Our analysis of the algorithm relies on Theorem 3.1. We will show that for every triangle
(u1, u2, u3) with edge lengths (x1, x2, x3) (satisfying the triangle inequality) and signature σ =
(σ1, σ2, σ3), we have



ALGσ(x1, x2, x3)

LP σ(x1, x2, x3).

A

·

≤

(1)

Therefore, by Theorem 3.1, our algorithm gives an A-approximation.

8

Without loss of generality, we assume that x1 ≤

denote the other two elements of
1, 2, 3
}
{
(the edge opposite to ui), wi = wei, xi = xuj uk , yi = fα(xi), and

x3. When i
x2 ≤
by k and j, so that j < k. For i

1, 2, 3
is ﬁxed, we will
}
, let ei = (uj, uk)
1, 2, 3
}

∈ {
∈ {

·
Observe that (1) is equivalent to the inequality w1t1 + w2t2 + w3t3 ≥

−

ti = A

lp(uj, uk

cost(uj, uk

ui).
|

ui)
|

inequality always holds.

0. We now prove that this

Lemma 3.2. We have

We express each ti in terms of xi’s and yi’s.

w1t1 + w2t2 + w3t3 ≥

0

(2)

Claim 3.3. For every i

Proof. If σi = “+”, then

∈ {

ti =

, we have
1, 2, 3
}
A(1
A(1

yj)xi
yj)(1

(

−
−

(yk
xi)

yj),
(1

−

−
−

yk),

if σi = “+”
”
if σi = “

−

−
−

ti = A

lp(uj, uk

·
= Axujuk ·
= Axi
·
= Axi(1

ui)
|
Pr(uj
∈
Pr(fα(xk)
yj)

≤
(yk

−

−

cost(uj, uk
ui)
−
|
S
S or uk

|
R or fα(xj)

∈

≤

−

yj),

−

≥

≤

≥

p = ui]
R)

Pr(uj
−
∈
Pr(fα(xk)

S, uk /
∈

S or uj /
∈
∈
R < fα(xj) or fα(xj)

S, uk

S

p = ui]
|
R < fα(xk))

≤

where we used that yk = fα(xk)

fα(xj) = yj (since xk

xj and fα(x) is non-decreasing).

If σi = “

”, then similarly to the previous case, we have

−
ti = A

·
= A(1

= A(1

= A(1

−

ui)
lp(uj, uk
|
xujuk )
xi)
xi)

−
Pr(uj
∈
Pr(fα(xk)
(1

yj)

−

·

·

−

·

−

−

≤
(1

cost(uj, uk

ui)
|
S or uk

S

|
R or fα(xj)

∈

yk).

−

p = ui)
R)

Pr(uj
−
∈
Pr(fα(xk)

≤

−

≤

S, uk

S

p = w)

∈
|
R, fα(xj)

R)

≤

We say that edge ei pays for itself if ti

0. Note that if all edges e1, e2, e3 pay for themselves

then the desired inequality (2) holds. First, we show that all negative edges pay for themselves.

≥

Claim 3.4. If σi = “

−

”, then ti

Proof. By Claim 3.3, ti = A(1
1
xj

≥
xk, we get

yk. If xk

1
2 −

−

−

0.

≥
yj)(1

≤

1

2A then yk = 1, and the inequality trivially holds. If xk < 1

xi)

1

−

−

−

yk. Thus, we need to show that A(1

yj)(1

xi)

−
≥
−
1
2A , then using

2 −

here we used the triangle inequality xk + xj

A >

1

1
2xk ≥

−

1

1
xk
−
−
xi. Thus

xj ≥

1

1

−

,

xi

A(1

yj)(1

xi)

−

−

≥

yk)(1

xi)

1

−

≥

yk.

−

−

≥
A(1

9

We now show that for short edges ei, it is suﬃcient to consider only the case when σi = “+”.

Speciﬁcally, we prove the following claim.
Claim 3.5. Suppose that xi < 1
obtained from σ by changing the sign of σi to “

2 −

”.

−

1
2A . If (2) holds for σ with σi = “+”, then (2) also holds for σ′

Proof. To prove the claim, we show that the value of ti is greater for σ′ than for σ. That is,

A(1

yj)xi

(yk

−

−

−

yj) < A(1

yj)(1

xi)

(1

−

−

−

yk).

−

Note that the values of tj and tk do not depend on σi and thus do not change if we replace σ with
σ′. Since fα is non-decreasing and xj

xk, we have yj

yk. Hence,

xi <

1
2 −

1
2A

=

+

1
2A −

1
A ≤

≤
1
2

≤
1
2

+

1
2A −

(1
A(1

yk)
yj)

.

−
−

Thus,

Therefore,

as required.

2A(1

−

yj)xi < A(1

yj) + 1

yj

−

−

2(1

−

yk).

−

A(1

yj)xi

(yk

−

−

−

yj) < A(1

yj)(1

xi)

(1

−

−

−

yk),

−

Unlike negative edges, positive edges do not necessarily pay for themselves. We now prove that

positive edges of length at least 1/A pay for themselves.

Claim 3.6. If σi = “+” and xi

1/A, then ti

0.

≥

≥

Proof. We have,

ti = A(1

yj)xi

(yk

yj)

(1

−

≥

yj)

−

(yk

−

−

−

−

yj) = 1

yk

−

≥

0.

We now separately consider two cases α

0.169 and α

0.169.

≥

≤

3.3 Analysis of the Approximation Algorithm for α

0.169

≤

First, we consider the case of α

0.169.

≤

0.169. We ﬁrst show that if x3 < 1

1
2A , then all three edges e1, e2,

2 −

Proof of Lemma 3.2 for α
and e3 pay for themselves.
1
Claim 3.7. If x3 < 1
2A , then ti

≤

Proof. Since x3 < 1
We show that ti

≥
yj = e−Axj

−

yk

−

0 for every i.

≥

2 −
1
2A , for every i

1, 2, 3
2 −
}
0 for all i. Fix i. If σi = “
−
e−A(xk−xj)

e−Axk = e−Axj

∈ {

1

−

(cid:16)

where the ﬁrst inequality follows from the inequality 1
from the triangle inequality. Thus, ti = A(1

yj)xi

−
(yk

−

−

−

≤
yj)

0.

≥

10

we have xi < 1
”, then, by Claim 3.4, ti

1
2A and thus yi

2 −

≡

0. If σi = “+”, then

−

fα(xi) = 1

e−Axi.

≥

≤

(cid:17)
e−Axj A(xk

≤

e−x

xj)

e−Axj Axi = A(1

≤

−
x, and the second inequality follows

−

yj)xi,

1

2 −

We conclude that if x3 < 1

2A , then (2) holds. The case x3 < 1

1
2A is the most interesting
case in the analysis; the rest of the proof is more technical. As a side note, let us point out that
e−Ax or
Theorem 1.1 has dependence A = 3 + 2 loge 1/α because (i) fα(x) must be equal to C
a slower growing function so that Claim 3.7 holds (ii) Theorem 3.1 requires that fα(0) = 0, and
3
ﬁnally (iii) we will need below that 1
2A

−
From now on, we assume that x3 ≥
≥
2A , then x2 ≥

1
(cid:0)
2 −
1
A and thus, by Claims 3.4 and 3.6, all ti
0 and t3 ≥

x1 ≥
3). Hence, t2 ≥
x2 ≥
t1 = 0 and inequality (2) holds. Therefore, it remains to show that inequality (2) holds when

1
2 −
1
2A and, consequently, y3 = f (x3) = 1. Observe that if
0 and we are done. Similarly, if
≥
0; additionally, y2 = y3 = 1. Thus

1
A , then all xi
1
2 −

1
A (since A

2 −

α.

≤

−

≥

f

(cid:1)

1

x1 <

1
A

,

x2 <

1
2 −

1
2A

, and x3 ≥

1
2 −

1
2A

.

By Claim 3.5, we may also assume that σ1 = “+” and σ2 = “+”. Since α
and

0.169, we have A > 5

≤

x3 −

x2 ≥

x1 ≥
Thus, by Claims 3.4 and 3.6, t2 ≥
e1 is a positive edge and thus w1 ≤

>

1
A

1
A

1
1
1
2 −
2A
2 −
−
(cid:19)
(cid:18)
0. Hence, w2t2 + w3t3 ≥
0 and t3 ≥
1. Therefore, it is suﬃcient to show that

and x3 ≥

1
2A

>

1
A

.

α(w2 + w3). Also, recall that

Now we separately consider two possible signatures σ = (“+”, “+”, “+”) and σ = (“+”, “+”, “

−

”).

First, assume that σ = (“+”, “+”, “+”). We need to show that

t1 ≥ −

α(t2 + t3).

(3)

A(1

y2)x1 −

−

(1

−

y2)

≥

α

(1

(cid:18)

y1) + (y2 −

−

y1)

−

A(1

−

y1)x2 −

Here, we used that y3 = 1. Note that x2 ≥
e−A( 1

1

1

1

x3 −
2A )

2 − 3

x1 ≥
= e− 3

1
2 −
2 −loge

1
2A −
α + 3

1

A = 1
1

2 −
2 = e− loge

1
α = α.

Thus, (1
σ = (“+”, “+”, “+”), it is suﬃcient to show that

y2) + α(1

y1)

≤

−

−

αy2 + 2α(1

(cid:17)

y2 ≤

−

−
−
(cid:16)
y1) + α(y2 −

A(1

y1)x3

.

−

(cid:19)
3
2A . Therefore,

αy2 + 2α(1

y1)

A(1

≤

−

−

y2)x1 + αA(1

y1). To ﬁnish the analysis of the case

y1)x2 + αA(1

y1)x3.

−

−

−

This inequality immediately follows from the following claim (we simply need to add up (4) and
(5) and multiply the result by α).

Claim 3.8. For c = 0.224, we have

(2
c)(1
y2 + c(1

−

Proof. Since c

2

−

≥

loge

1
0.169 ≥

2

−

loge

−

≤

y1)
y1)

A(1

A(1

−

y1)x2; and
y1)x3.

≤

−
1
α (recall that α

−

0.169), we have

≤

(4)

(5)

2

c

−

≤

loge

1
α

=

A
2 −

3
2 ≤

Ax2.

11

Therefore, (4) holds. We also have,

c

≤

0.169 + loge

1
0.169

+ 1

e

−

≤

α + loge

1
α

+ 1

e.

−

Thus, e

α

−

≤

A
2 −

1
2 −

c

c. Therefore,

Ax3 −
≤
e−1 (Ax3 −

c)

1

−

≥

αe−1 = 1

where we used that x2 < 1
and x1 < 1
A it follows that

2 −

1
2A and y2 = fα(x2) = 1

e−A( 1

2 − 1

2A )

y2,

≥

(6)

e−Ax2. Observe that from inequalities (6)

−

−

y2 ≤

f

1

−

(cid:18)

1
A

(cid:16)

(cid:17)(cid:19)

(Ax3 −

c)

(1

y1)(Ax3 −

−

≤

c),

which implies (5).

(7)

(8)

(9)

(10)

Now, assume that σ = (“+”, “+”, “

”). We need to prove the following inequality,

−
y2)

A(1

−

≤

y2)x1 + αA(1

y1)(x2 + 1

x3).

−

−

(1

−

y2) + α(1

y1 + 1

−

−

As before,

(1

−

y2) + α(1

y1 + 1

y2)

−

≤

−

α + α(1

y1 + 1

y2)

−

≤

−

α + 2α(1

y1).

−

On the other hand,

A(1

−

y2)x1 + αA(1

y1)(x2 + 1

x3)

−

−

αA(1

αA(1

y1)(1
y1)(1

−

−

≥

≥

x3)

−

−
1

x1 + x1 + x2 −
x1)
1
A

(cid:19)

αA(1

≥
= α(1

y1)

−
(cid:18)
y1)(A

−

−

−
1)

where the second inequality is due to the triangle inequality, and the third inequality is due to
1
x1 < 1
α = e−1(A

A . Finally, observe that 1

2e−1 loge

3). We get,

y1)(A

(1

3)

≤
α(1

y1)(A

1)

≤

−
α + 2α(1

−
y1).

−

≥
Combining (8), (9), and (10), we get (7). This concludes the case analysis and the proof of
Theorem 1.1 for the regime α

0.169.

−

−

−

≤
3.4 Analysis of the Approximation Algorithm for α

We now consider the case when α

0.169. Observe that for α

≥

≥

0.169

≥
0.169

and

A = 3 + 2 loge(1/α)

6α + 3

α)2

(1

−
3α

−

≥

1

α
−
3 ≤

2α
1 + α

12

(11)

(12)

Proof of Lemma 3.2 for α
≥
Claims 3.4 and 3.6, all ti
≥
0 for σi = “ + ”. This combined with Claim 3.4 imply all ti
ing yi = 0 and thus, ti
1/A (since A
we are done. Similarly, if x2 ≥
additionally, we have y2 = y3 = 1. Thus, t1 = 0 and we are done.

0.169. Observe that if x1 ≥
0 and we are done. Moreover, if x3 < 1

1/A and thus, by
A then all xi < 1/A imply-
0 and
0;

3). Hence, t2 ≥

≥
0 and t3 ≥

2A , then x2 ≥

1
2 −

1
A , then all xi

≥

≥

≥

1

Therefore, we will assume below that

x1 <

1
A

, x2 <

1
2 −

1
2A

, x3 ≥

1
A

.

Furthermore, by Claim 3.5, we may assume σ1 = “+” and σ2 = “+”. We consider four cases:
1/(2A), and
1/2

1/A, x3 < 1/2

1/(2A), (iii) x2 ≥

−

−

1/(2A), (ii) x2 < 1/A, x3 ≥
−
1/(2A).
−

1

A , x3 ≥

1
2 −

1
2A . Then y1 = 0, y2 = (1−α)/3, y3 = 1. By Claims 3.4
0, we are done. So we will assume below

0, and e2, e3 pay for themselves. If t1 ≥

1/A, x3 ≥

1/2
(i) x2 ≥
(iv) x2 < 1/A, x3 < 1/2
Consider the case x2 ≥
and 3.6, t2, t3 ≥
that t1 < 0. Then,

w1t1 + w2t2 + w3t3 ≥

1

·

t1 + αt2 + αt3

(13)

(recall that we assume that e1 is a positive edge and thus w1 ≤

1).

Now we separately consider two possible signatures σ = (“+”, “+”, “+”) and σ = (“+”, “+”, “

−
First, assume that σ = (“+”, “+”, “+”). Because of (13), to prove (2) it is suﬃcient to show

”).

(1

y2) + α + αy2 ≤

−

A(1

−

y2)x1 + αAx2 + αAx3

From (11) it follows that

which implies (15) due to x3 ≥

1
2 −

1 + α

≤
1
2A

α)2

(1

−
3

+ α(A

1)

−

1 + α

α)2

(1

−
3

≤

+ 2αAx3

(14)

(15)

Observe that from (15) together with triangle inequality and y2 = 1−α

1

α it follows that

1 + α

(1

−

≤

α)y2 + A(1

y2)x1 −

−

−
αAx1 + αAx1 + αAx2 + αAx3

3 ≤

which is equivalent to (14).

Now, assume that σ = (“+”, “+”, “

”). Because of (13), to prove (2) it is suﬃcient to show

(1

y2) + α + α(1

A(1

−

≤

y2)x1 + αAx2 + αA(1

x3)

−

(16)

−
y2)

−

−
From (11) and y2 = 1−α
3

it follows that

1 + 2α

α)2

(1

−
3

≤

+ αA

≤

y2(1 + α) + αA

Since y2 ≤

1

−

α,

(1 + 2α)

≤

(1 + α)y2 + A(1

y2)x1 −

−

αAx1 + αA,

13

Hence, using the triangle inequality,

1 + 2α

≤

(1 + α)y2 + A(1

y2)x1 −

αAx1 + αA + αAx1 + αAx2 −

−

αAx3.

which is equivalent to (16).
Consider the case x2 < 1
t1, t2 < 0. Then,

1

1
A , x3 ≥
2 −
w1t1 + w2t2 + w3t3 ≥

2A . Then y1 = y2 = 0, y3 = 1. Observe that t3 ≥
t2 + αt3.

t1 + 1

1

·

·

0 and

(17)

(recall that we assume that e1, e2 are positive edges and thus w1, w2 ≤
x3 ≥

1
2A we have

1
2 −

−
From (18), we get that if (2) holds for σ with σ3 = “

−

Ax3 ≥

A(1

x3)

1.

from σ by changing the sign of σ3 to “+”. Thus without loss of generality σ3 = “
need to consider σ = (“+”, “+”, “

”). Then, because of (17), to prove (2) it is suﬃcient to show

−

”, then (2) also holds for σ′ obtained
” and we only

−

1). Furthermore, since

(18)

−

From (11) it follows that

which is equivalent to

1 + 1 + α

≤

Ax1 + Ax2 + αA(1

x3).

−

A

≥

5 + α
α + 1

2 + α

≤

αA + (1

α)(

−

A
2 −

1
2

).

Observe that from (20) together with triangle inequality and x3 ≥

1
2 −

1
2A it follows that

2 + α

≤

αA + (1

−

α)Ax3 = Ax3 + αA(1

x3)

−

≤

Ax1 + Ax2 + αA(1

x3).

−

(19)

(20)

1
A , x3 < 1
Consider the case x2 ≥
only need to consider σ = (“+”, “+”, “+”). Then by Claim 3.6, t2, t3 ≥
w1t1 + w2t2 + w3t3 ≥
Thus,

1
2A . Then y1 = 0, y3 = (1−α)/3. By Claim 3.5 we
0 then
1.

0. Let us assume that t1 < 0. Since e1 is a positive edge, we have w1 ≤

0. Thus, if t1 ≥

2 −

We need to show that the right hand side in the above inequality is non-negative. Replace t1, t2,
and t3 with the expressions from Claim 3.3. Now to obtain (2), it is suﬃcient to prove that

w1t1 + w2t2 + w3t3 ≥

1

·

t1 + αt2 + αt3

Observe that since x3 ≥

y3 −
1
A we have

y2 + αy3 + αy2 ≤

A(1

−

y2)x1 + αAx2 + αAx3

Inequalities (22) and (12) imply

2α

(1

−

≤

α)y2 + 2αAx3.

(1 + α)y3 ≤

(1

−

α)y2 + 2αAx3.

14

(21)

(22)

(23)

1
2A . Then y1 = y2 = 0. By Claim 3.5 we only need to

0 and we are done. Thus we assume x1 < y3/A which implies t1 < 0.

Observe that from (23) together with triangle inequality and y2 ≤

1

α it follows that

−
αAx1 + αAx1 + αAx2 + αAx3

−

−

(1

α)y2 + A(1

y2)x1 −

(1 + α)y3 ≤
which is equivalent to (21).
Consider the case x2 < 1
consider σ = (“+”, “+”, “+”). Then by Claim 3.6, t3 ≥
y3/A then t1, t2 ≥
We consider two diﬀerent regimes: (i) x2 ≥
First, assume that x2 ≥

A , x3 < 1

If x1 ≥

2 −

y3/A which implies t2 ≥
w1t1 + w2t2 + w3t3 ≥

0.

y3/A and (ii) x2 < y3/A.
0. Then,

t1 + αt2 + αt3

1

·

(recall that we assume that e1 is a positive edge and thus w1 ≤

Because of (24), to prove (2) it is suﬃcient to show

1).

Observe that by (12) and y3 = (1−α)/3 we have

y3 + αy3 ≤

Ax1 + αAx2 + αAx3

(24)

(25)

(1 + α)y3 ≤

2α

2αAx3 ≤

≤

αAx3 + αAx1 + αAx2 ≤

Ax1 + αAx2 + αAx3

1
A and the third inequality follows from triangle

where the second inequality follows from x3 ≥
inequality.

Now, assume that x2 < y3/A which implies t2 < 0. Then,

t2 + αt3
(recall that we assume that e1, e2 are positive edges and thus w1, w2 ≤

w1t1 + w2t2 + w3t3 ≥

Because of (26), to prove (2) it is suﬃcient to show

t1 + 1

1

·

·

Observe that by (12) and x3 ≥
4α

2y3 ≤

Ax1 + Ax2 + αAx3

1
A

1).

(26)

(27)

(1 + α)Ax3 ≤
where the last inequality follows from triangle inequality.

2y3 ≤

1 + α ≤

1 + α

≤

Ax1 + Ax2 + αAx3

This concludes the case analysis and the proof of Theorem 1.1 for the regime α

0.169.

≥

4 Better approximation for values of α appearing in practice

We note that the choice of function f (x) in Theorem 1.1 is somewhat suboptimal. The best
function fopt(x) for our analysis of Algorithm 1 can be computed using linear programming (with
high precision). Using this function fopt, we can achieve an approximation factor Aopt better than
= 1).3 While
the approximation factor Athm = 3 + 2 loge 1/α guaranteed by Theorem 1.1 (for α
asymptotically Athm/Aopt
0, Aopt is noticeably better than Athm for many values of α
(10−8, 0.1)). We list approximation factors Athm
that are likely to appear in practice (say, for α
∈
and Aopt for several values of α in Table 1; we also plot the dependence of Athm and Aopt on α in
Figure 3.

1 as α

→

→

3It is also possible to slightly modify Algorithm 1 so that it gets approximation Aopt without explicitly computing

f . We omit the details here.

15

6
Table 1: Approximation factors Athm and Aopt for diﬀerent α-s.

loge

1/α

1/α Athm Aopt

0
1.61
2.30
3.91
4.61
6.21
6.91
8.52
10
15
20

1
5
10
50
100
500
1000
5 000
22 026.5
106
108

×
×

3.3
4.9

3
6.22
7.61
10.82
12.21
15.43
16.82
20.03
23
33
43

3
4.32
4.63
6.07
6.78
8.69
9.62
11.9
14.2
22.6
31.3

5 Analysis of the Algorithm for Complete Bipartite Graphs

Proof of Theorem 1.2. The proof is similar to the proof of Theorem 1.1. Without loss of generality
we assume that the scaling parameter w is 1. Deﬁne f (x) as follows

f (x) =

e−Ax,

1
1,

−

(cid:26)

x < 1

if 0
≤
otherwise

2 −

1
2A

where A = 5 + 2 loge 1/α. Our analysis of the algorithm relies on Theorem 3.1. Since in the proof
of Theorem 3.1, we assumed that all edges are present, let us add missing edges (edges inside parts)
to the bipartite graph and assign them weight 0; to be speciﬁc, we assume that they are positive
edges. (It is important to note that Theorem 3.1 is true even when edges have zero weights). We
will still refer to these edges as ‘missing edges’.

We will show that for every triangle (u1, u2, u3) with edge lengths (x1, x2, x3) (satisfying the

triangle inequality) and signature σ = (σ1, σ2, σ3), we have

ALGσ(x1, x2, x3)

A

LP σ(x1, x2, x3)

(28)

≤
Therefore, by Theorem 3.1, our algorithm gives an A-approximation. In addition to Theorem 3.1
we use Claims 3.3, 3.4, 3.5, 3.6 and 3.7. Recall that proofs of these claims rely on f being non-
decreasing which is satisﬁed by the above choice. Observe that (28) is equivalent to

·

3

witi

≥

0.

(29)

i=1
X
1
A and thus, by Claims 3.4 and 3.6, all ti

1
A , then all xi
Observe that if x1 ≥
≥
1
1
1
A (since A > 3), then t2 ≥
are done. Similarly, if x2 ≥
2A ≥
2 −
y2 = y3 = 1, thus t1 = 0 and we are done. Furthermore, if x3 < 1
and thus, by Claim 3.7, all ti
x2 < 1
1
2A , and x3 ≥
We have (here we use that A

2 −
2A . Further, by the triangle inequality x2 ≥

1
2A
0 and we are done. Therefore, we will assume below that x1 < 1
A ,
A−3
2A .

0 and we
0; additionally,

2A then all xi < 1

2 −
x1 ≥

0 and t3 ≥

A−1
2A −

x1 ≥

x3 −

1
2 −

2 −

5),

≥

≥

1

1

≥
A

x1 ≤

1
A ≤

3
−
2A ≤

A

1
−
2A −

x1 ≤

x2 <

A

1
−
2A ≤

x3 ≤

x1 + x2.

16

We will use below that

eA(x2−x1)

≥

eA( A−1

2A −2x1) = e2+log 1

α −2Ax1 = e2(1−Ax1)/α

1/α.

≥

By Claim 3.5, we may also assume that σ1 = “+” and σ2 = “+” (and since we assume that
missing edges are positive). By Claims 3.4 and 3.6, t2 ≥
0 (edges e2 and e3 pay for
0, we are done. So we will assume below that t1 < 0. Since G is a complete
themselves). If t1 ≥
bipartite graph, a triangle (u1, u2, u3) contains either (i) no edges or (ii) two edges. In case (i) we
have w1 = w2 = w3 = 0 and (29) holds trivially. In case (ii) if e1 is the missing edge then w1 = 0 and
0, (29) holds trivially. It remains to consider three signatures σ = (“+”, “+”, “
”),
since t2, t3 ≥
◦
σ = (“+”, “
” denotes a missing edge (which by our
”) where “
”, “
”, “+”) and σ = (“+”, “
◦
◦
◦
assumption above is a positive edge).

0 and t3 ≥

−

First, assume that σ = (“+”, “+”, “
Ax1) and t2 = A(1
−
w1t1 + w2t2 + w3t3 ≥
t1 + αt2 > 0 or, equivalently, eAx2(αt2 + t1)
we get

”). By Claim 3.3, t1 = A(1
◦
y2) = e−Ax1(Ax2 −
y1)x2 −
t1 + αt2 (here we used that t1 ≤

(1
−
1). Since e3 is missing, w3 = 0. We have,
0). So it suﬃces to prove that
0 and t2 ≥
x1,

0. Using that eA(x2−x1)

1/α and x2 ≥

y2)x1−

A−1
2A −

y2) =

(1

−

−

−

−

≥

≥

e−Ax2(1

eAx2(αt2+t1) = αeA(x2−x1)(Ax2−
as required.

1)

(1

−

−

Ax1)

≥

1
α ·

α

·

A

(cid:16)

(cid:0)

A

1
−
2A −

x1

1

(cid:17)

−

(cid:1)

+Ax1−

1 =

A

5

−
2

> 0,

Now, assume that σ = (“+”, “

”, “+”). Now we have t1 =
◦

−

t3 = A(1

y1)x3 −
We prove that t1 + αt3 ≥
A−1
x3 ≥
2A , we get

y1) = Ae−Ax1x3 −

(y2 −
0 or, equivalently, eAx2(αt3 + t1)

(e−Ax1

−

−

−

e−Ax2(1
e−Ax2) = e−Ax1(Ax3 −
≥

0. Using that eA(x2−x1)

Ax1) (as before) and
1) + e−Ax2.

1/α and

≥

eAx2(αt3 + t1) = α

eA(x2−x1)(Ax3 −
(1
1) + α

(cid:0)
(Ax3 −

−

−

1) + 1

(1

Ax1)

−

−
Ax1) > Ax3 −

(cid:1)

A

1
−
2 −

2

0,

≥

2

≥

as required.

≥

Finally, assume that σ = (“+”, “
and t3 = A(1
y1)(1
that eAx2(αt3 + t1)
eAx2(αt3+t1) = α

x3)
−
−
−
0. We have,

≥
AeA(x2−x1)(1

x3)

(1

−

”, “
◦
y2) = Ae−Ax1(1

−

”). Now we have t1 =

Ax1) (as before)
e−Ax2. As in the previous case, we prove

−

−

e−Ax2(1

x3)

−

−

1

(1

−

−

Ax1)

≥

α

AeA(x2−x1)(1

x1 −

−

x2)

−

1

(1

−

−

Ax1)

.

−

−

(cid:0)

(cid:1)

(cid:0)

F (x1,x2)

(cid:1)

Denote the expression on the right by F (x1, x2). We now show that for a ﬁxed x1, F (x1, x2) is an
}
increasing function of x2 when x2 ∈
A(1

2A ). Indeed, we have

= αAeA(x2−x1)

αAeA(x2−x1)

x1, A−1

[ A−1

{z

A

A

1

1

1

1

|

x2)

∂F (x1, x2)
∂x2

1
A −

−

−
2A

−

(cid:17)

(cid:17)

(cid:16)

(cid:16)

2A −
x1 −
> 0.

−
3

= αAeA(x2−x1)

A

(cid:0)
·

−
2

−

≥

(cid:1)

17

We conclude that

F (x1, x2)

F

α

≥

≥

(cid:18)
A

·

x1,

A

1
−
2A −

1
α ·

·

1

−

(cid:18)

x1

=

α

AeA(˜x2−x1)(1

−

˜x2)

x1 −
A + 1

1

−

(1

−

−

Ax1)

α

(cid:1)

−

1 + Ax1 ≥

(cid:17)(cid:12)
A + 1
(cid:12)
(cid:12)
2 −

˜x2= A−1

2A −x1

2 > 0.

(1

−

−

Ax1) =

2 −

A

(cid:19)

−
2A

(cid:0)
α

(cid:16)

1

−

(cid:19)

This concludes the case analysis and the proof of Theorem 1.2.

6

Integrality Gap

In this section, we give a Θ(log 1/α) integrality gap example for the LP relaxation presented in
Section 2.1. Notice that in the example each positive edge has a weight of w+ and each negative
edge has a weight of w− with w+

w−.

≥

Proof of Theorem 1.3. Consider a 3-regular expander G = (V, E) on n = Θ((α2 log2 α)−1) vertices.
E; otherwise u and v are dissimilar. That
We say that two vertices u and v are similar if (u, v)
is, the set of positive edges E+ is E and the set of negative edges E− is V
E. Let w+ = 1
and w− = α.

×

∈

V

\

Lemma 6.1. The integrality gap of the Correlation Clustering instance Gcc = (V, E+, E−) de-
scribed above is Θ(log 1/α).

Proof. Let d(u, v) be the shortest path distance in G. Let ε = 2/ log3 n. We deﬁne a feasible metric
LP solution as follows: xuv = min(εd(u, v), 1).

Let LP + be the LP cost of positive edges, and LP − be the LP cost of negative edges. The
E. There are 3n/2 positive edges
LP cost of every positive edge is ε since d(u, v) = 1 for (u, v)
in Gcc. Thus, LP+ < 3n/ log3 n. We now estimate LP −. For every vertex u, the number of
vertices v at distance less than t is upper bounded by 3t because G is a 3-regular graph. Thus,
the number of vertices v at distance less than 1/2 log3 n is upper bounded by √n. Observe that
the LP cost of a negative edge (u, v) (which is equal to α(1
xuv)) is positive if and only if
d(u, v) < 1/2 log3 n. Therefore, the number of negative edges with a positive LP cost incident on
any vertex u is at most √n. Consequently, the LP cost of all negative edges is upper bounded by
αn

3
2 = Θ(n/ log 1/α). Hence,

−

∈

LP

≤

Θ(n/ log 1/α) + 3n/ log3 n = Θ(n/ log 1/α).

Here, we used that log n = Θ(log 1/α).

We now lower bound the cost of the optimal (integral) solution. Consider an optimal solution.

There are two possible cases.

1. No cluster contains 90% of the vertices. Then a constant fraction of positive edges in the

expander G are cut and, therefore, the cost of the optimal clustering is at least Θ(n).

2. One of the clusters contains at least 90% of all vertices. Then all negative edges in that cluster
m = Θ(n2) such edges.

are in disagreement with the clustering. There are at least
Their cost is at least Ω(αn2).

0.9n
2

−

(cid:0)

(cid:1)

18

We conclude that the cost of the optimal solution is at least Θ(n) and, thus, the integrality gap

is Θ(log(1/α)).

We note that in this example log(1/α) = Θ(log n). However, it is easy to construct an integrality
Θ(log n). To do so, we pick the integrality gap example constructed
gap example where log(1/α)
above and create k
n disjoint copies of it. To make the graph complete, we add negative edges
with (fractional) LP value equal to 1 to connect each copy to every other copy of the graph. The
new graph has kn

n vertices. However, the integrality gap remains the same, Θ(log 1/α).

≪

≫

≫

Now we give a Θ(log 1/α) integrality gap example when G is a complete bipartite graph.

Proof of Theorem 1.4. The proof is very similar to that of Theorem 1.3. We start with a 3-regular
bipartite expander G = (L, R, E) on n = Θ((α2 log2 α)−1) vertices (e.g., we can use a 3-regular
bipartite Ramanujan expander constructed by Marcus, Spielman, and Srivastava [2013]). Then
we deﬁne a correlation clustering instance as follows: Gcc = (L, R, E+, E−) where E+ = E and
E− = (L
E; let w+ = 1 and w− = α. The proof of Lemma 6.1 can be applied to Gcc; we only
need to note that if a cluster contains at least 90% of the vertices, then there are at least Θ(n2)
edges of Gcc between vertices in the cluster. It follows that the integrality gap is Ω(log(1/α)).

R)

×

\

References

Nir Ailon, Moses Charikar, and Alantha Newman. Aggregating inconsistent information: ranking

and clustering. Journal of the ACM (JACM), 55(5):23, 2008.

Nir Ailon, Yudong Chen, and Huan Xu. Breaking the small cluster barrier of graph clustering. In

International Conference on Machine Learning, pages 995–1003, 2013.

Nikhil Bansal, Avrim Blum, and Shuchi Chawla. Correlation clustering. Machine learning, 56(1-3):

89–113, 2004.

Paolo Boldi and Sebastiano Vigna. The WebGraph framework I: Compression techniques. In Proc.

of the Thirteenth International World Wide Web Conference, pages 595–601, 2004.

Paolo Boldi, Bruno Codenotti, Massimo Santini, and Sebastiano Vigna. Ubicrawler: A scalable

fully distributed web crawler. Software: Practice & Experience, 34(8):711–726, 2004.

Paolo Boldi, Marco Rosa, Massimo Santini, and Sebastiano Vigna. Layered label propagation: A
multiresolution coordinate-free ordering for compressing social networks. In Proceedings of the
International Conference on World Wide Web, pages 587–596, 2011.

Paolo Boldi, Andrea Marino, Massimo Santini, and Sebastiano Vigna. BUbiNG: Massive crawling
for the masses. In Proceedings of the Companion Publication of the International Conference on
World Wide Web, pages 227–228, 2014.

Moses Charikar, Venkatesan Guruswami, and Anthony Wirth. Clustering with qualitative informa-

tion. In IEEE Symposium on Foundations of Computer Science. Citeseer, 2003.

Shuchi Chawla, Konstantin Makarychev, Tselil Schramm, and Grigory Yaroslavtsev. Near optimal
LP rounding algorithm for correlation clustering on complete and complete k-partite graphs. In
Proceedings of the Symposium on Theory of Computing, pages 219–228, 2015.

19

Erik D Demaine, Dotan Emanuel, Amos Fiat, and Nicole Immorlica. Correlation clustering in

general weighted graphs. Theoretical Computer Science, 361(2-3):172–187, 2006.

Micha Elsner and Warren Schudy. Bounding and comparing methods for correlation clustering
beyond ilp. In Proceedings of the Workshop on Integer Linear Programming for Natural Langauge
Processing, pages 19–27. Association for Computational Linguistics, 2009.

Naveen Garg, Vijay V Vazirani, and Mihalis Yannakakis. Approximate max-ﬂow min-(multi) cut

theorems and their applications. SIAM Journal on Computing, 25(2):235–251, 1996.

Konstantin Makarychev, Yury Makarychev, and Aravindan Vijayaraghavan. Correlation clustering

with noisy partial information. In Conference on Learning Theory, pages 1321–1342, 2015.

Adam W. Marcus, Daniel A. Spielman, and Nikhil Srivastava.

Interlacing families I: Bipartite
Ramanujan graphs of all degrees. In Proceedings of the Symposium on Foundations of Computer
Science, pages 529–537, 2013.

Claire Mathieu and Warren Schudy. Correlation clustering with noisy input. In Proceedings of the

Symposium on Discrete Algorithms, pages 712–728, 2010.

Xinghao Pan, Dimitris Papailiopoulos, Samet Oymak, Benjamin Recht, Kannan Ramchandran,
In Advances in Neural

and Michael I Jordan. Parallel correlation clustering on big graphs.
Information Processing Systems, pages 82–90, 2015.

Siyu Tang, Bjoern Andres, Mykhaylo Andriluka, and Bernt Schiele. Multi-person tracking by
multicut and deep matching. In European Conference on Computer Vision, pages 100–111, 2016.

Siyu Tang, Mykhaylo Andriluka, Bjoern Andres, and Bernt Schiele. Multiple people tracking by
lifted multicut and person re-identiﬁcation. In Proceedings of the Conference on Computer Vision
and Pattern Recognition, pages 3539–3548, 2017.

A Proof of Theorem 3.1

For the sake of completeness we include the proof of Theorem 3.1 (see Ailon et al. [2008] and
Chawla et al. [2015]).

Proof of Theorem 3.1. Our ﬁrst task is to express the cost of violations made by Algorithm 1 and
the LP weight in terms of ALGσ(
) and LP σ(
), respectively. In order to do this, we consider the
·
·
cost of violations made by the algorithm at each step.

Consider step t of the algorithm. Let Vt denote the set of active (yet unclustered) vertices at
Vt denote the pivot chosen at step t. The algorithm chooses a set
the start of step t. Let w
St, the constraint
St
E− is satisﬁed or violated right after step t. Speciﬁcally,
imposed by each edge of type (u, v)
if (u, v) is a positive edge, then the constraint (u, v) is violated if exactly one of the vertices u, v is

Vt as a cluster and removes it from the graph. Notice that for each u

E+

⊆

∈

∈

∈

∪

20

in St. If (u, v) is a negative constraint, then it is violated if both u, v are in St. Denote the weight
of violated constraints at step t by ALGt. Thus,

ALGt =

X(u,v)∈E+
u,v∈Vt

1 (u

wuv

·

∈

St, v

6∈

St or u

St, v

6∈

∈

St) +

1 (u

wuv

·

St, v

St) .

∈

∈

X(u,v)∈E−
u,v∈Vt

Similarly, we can quantify the LP weight removed by the algorithm at step t, which we denote
St. Thus,

by LPt. We count the contribution of all edges (u, v)

E− such that u

St or v

E+

∈

∪

∈

∈

LPt =

X(u,v)∈E+
u,v∈Vt

wuvxuv

1(u

·

∈

St or v

St) +

∈

X(u,v)∈E−
u,v∈Vt

wuv(1

xuv)

·

−

1(u

∈

St or v

St)

∈

Note that the cost of the solution produced by the algorithm is the sum of the violations across
t ALGt. Moreover, as every edge is removed exactly once from the
all steps, that is ALG =
t LPt. We will charge the cost of the violations of the algorithm at
graph, we can see that LP =
P
step t, ALGt, to the LP weight removed at step t, LPt. Hence, if we show that E[ALGt]
ρE[LPt]
for every step t, then we can conclude that the approximation factor of the algorithm is at most ρ,
since

P

≤

E[ALG] = E

ALGt

E

LPt

= ρ

LP.

ρ

·

≤

·

(cid:20) X
(cid:21)
) which are deﬁned in Section 3.1.
) and lp(
We now express ALGt and LPt in terms of cost(
·
·
This will allow us to group together the terms for each triplet u, v, w in the set of active vertices
and thus write ALGt and LPt in terms of ALGσ(
) and LP σ(
), respectively.
·
·
For analysis, we assume that for each vertex u

V , there is a positive (similar) self-loop, and

(cid:20) X

(cid:21)

t

t

thus we can deﬁne cost(u, u
p = w) = 0 and lp(u, u
S

w) and lp(u, u
Pr(u

|
w) = xuu

·

|
∈

|

|

|

w) formally as follows: cost(u, u
S

p = w) = 0 (recall that xuu = 0).

|

w) = Pr(u

S, u

6∈

∈

∈

E[ALGt

Vt] =

|

1
Vt
|

w∈Vt
| X

X(u,v)∈E
u,v∈Vt

(cid:18)

wuv

·

cost(u, v

|

w)

=

(cid:19)

1
Vt
2
|

u,v,w∈Vt
| X
u6=v

wuv

·

cost(u, v

|

w)

(30)

E[LPt

Vt] =

|

1
Vt
|

w∈Vt
| X

wuv

·

lp(u, v

|

w)

=

(cid:19)

1
Vt
2
|

u,v,w∈Vt
| X
u6=v

X(u,v)∈E
u,v∈Vt

(cid:18)

wuv

·

lp(u, v

w)

|

(31)

w) are counted twice. Now adding the contribution of terms cost(u, u

We divide the expressions on the right hand side by 2 because the terms cost(u, v

w) and
w)
lp(u, v
(both equal to 0) to (30) and (31), respectively and grouping the terms containing u, v and w
together, we get,

w) and lp(u, u

|

|

|

|

E[ALGt

Vt] =

|

=

1
Vt
6
|
1
Vt
6
|

wuv

·

cost(u, v

|

w) + wuw

·

cost(u, w

v) + wwv

·

|

cost(w, v

|

u)

(cid:19)

u,v,w∈Vt (cid:18)
| X

ALGσ(x, y, z)

u,v,w∈Vt
| X

21

and

E[LPt

Vt] =

|

=

1
Vt
6
|
1
Vt
6
|

wuv

·

lp(u, v

|

w) + wuw

lp(u, w

·

|

v) + wwv

lp(w, v

·

|

u)

(cid:19)

u,v,w∈Vt (cid:18)
| X

LP σ(x, y, z)

u,v,w∈Vt
| X

Thus, if ALGσ(x, y, z)

triangle inequality, then E[ALGt
ﬁnishes the proof.

|

≤

ρLP σ(x, y, z) for all signatures and edge lengths x, y, z satisfying the
E[LP ] which

Vt], and, hence, E[ALG]

E[LPt

Vt]

ρ

ρ

≤

·

|

≤

·

22

1.1

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

)
x
(
α
f

0.01

1
0
0.

1
.
0

1
.
0

1
0
0
.
0

1
0
0
.
0

fα for α = 0.001
fopt for α = 0.001
fα for α = 0.01
fopt for α = 0.1
f for α = 0.1
fopt for α = 0.1

0.1

−

0

0.25

0.5
x

0.75

1

.
Figure 2: This plot shows functions fα(x) used in the proof of Theorem 1.1 for α
0.001, 0.01, 0.1
}
Additionally, it shows optimal functions fopt(x) (see Section 4 for details). Note that every function
fα(x), including fopt(x), has a discontinuity at point τ = 1/2

τ , fα(x) = 1.

1/2A; for x

∈ {

−

≥

23

Athm
Aopt

A

45

40

35

30

25

20

15

10

5

0

0

10
loge

1/α

20

Figure 3: Plots of approximation factors Athm and Aopt.

24

