2
2
0
2

n
u
J

5

]
L
C
.
s
c
[

1
v
1
7
8
1
1
.
6
0
2
2
:
v
i
X
r
a

OfÔ¨Çine RL for Natural Language Generation with
Implicit Language Q Learning

Charlie Snell
UC Berkeley
csnell22@berkeley.edu

Ilya Kostrikov
UC Berkeley
kostrikov@berkeley.edu

Yi Su
UC Berkeley
suyi@berkeley.edu

Mengjiao Yang
UC Berkeley
sherryy@berkeley.edu

Sergey Levine
UC Berkeley
svlevine@berkeley.edu

Abstract

Large language models distill broad knowledge from text corpora. However, they
can be inconsistent when it comes to completing user speciÔ¨Åed tasks. This issue can
be addressed by Ô¨Ånetuning such models via supervised learning on curated datasets,
or via reinforcement learning. In this work, we propose a novel ofÔ¨Çine RL motivated
method, implicit language Q-learning (ILQL), designed for use on language models,
that combines both the Ô¨Çexible utility optimization framework of traditional RL
algorithms with supervised learning‚Äôs ability to leverage existing data and its
simplicity and stability. Our method, based on dynamic programming, employs
a blend of value conservatism alongside an implicit dataset support constraint in
learning value functions, which are then used to guide language model generations
towards maximizing utility. In addition to empirically validating ILQL, we present
a detailed empirical analysis of situations where ofÔ¨Çine RL can be useful in natural
language generation settings, demonstrating how it can be a more effective utility
optimizer than prior approaches for end-to-end dialogue, and how it can effectively
optimize high variance reward functions based on subjective judgement, such as
whether to label a comment as an example of toxic speech or not.

1

Introduction

Large language models can acquire a remark-
able amount of knowledge from large noisy
corpora and can be applied to a wide range of
language-based tasks. However, such models
are not designed to optimize any user-speciÔ¨Åed
utility, instead requiring considerable trial-and-
error to design prompts that ‚Äúcoerce‚Äù the models
into producing desirable outcomes [1, 2, 3]. In
essence, standard unsupervised language model
training only solves part of the problem, being
effective at distilling down knowledge in large
corpora, but relatively clumsy when applying
this knowledge to solve user-speciÔ¨Åed tasks.
Reinforcement learning (RL) in principle can
provide an effective framework for steering lan-
guage models toward user speciÔ¨Åed tasks as long

Figure 1: OfÔ¨Çine RL differs from supervised learning
in that it learns to maximize user-speciÔ¨Åed rewards from
suboptimal data with reward labels.

Code at https://sea-snell.github.io/ILQL_site/

Preprint. Under review.

Task Specific ‚ÄúExpert‚Äù DatasetSuboptimal Dataset with RewardsThanks! Please return to shop with us!Thank you for shopping with us have a nice day!FinetuningOffline RLTask LM (ùõë)Havea<a>aniceHaveniceday‚Ä¶‚Ä¶Value ModelHavea<a>0.10.30.0nice0.9‚Ä¶‚Ä¶Pretrained LM+10-1+1Thanks! Please return to shop with us!Thank you for shopping with us have a nice day!Good bye.Please don‚Äôt come back. Please don‚Äôt shop with us ever again.Pretrained LMsupervised finetuningoffline RL (this paper) 
 
 
 
 
 
as it could be represented using some utility functions; however, as outlined in Figure 2 contemporary
methods suffer from high systems complexity and can require expensive human interaction, we
therefore need several conditions to make RL practical: (1) Easy to use: the underlying learning
algorithm and workÔ¨Çow should be simple, stable, and scalable; (2) Able to optimize user speciÔ¨Åed
rewards: the algorithm should be able to steer a language model toward maximizing any user-deÔ¨Åned
reward functions, from high-level task goals (e.g., book a Ô¨Çight) to low-level linguistic subtleties
(e.g., avoiding rude or toxic speech); (3) Practical in interactive settings: the system should be able
to handle a variety of tasks, from generating text with desired properties to sequential turn-taking in
settings such as dialogue tasks; (4) Able to leverage existing data: it is beneÔ¨Åcial for such system to
directly utilize the large quantities of existing internet data, avoiding expensive and time-consuming
online human interactions; (5) Temporally compositional: Ô¨Ånally, the method should be able to
attain signiÔ¨Åcant improvement over the average behavior in the data ‚Äì not merely copying the best
behaviors in the dataset, but actually distilling out underlying patterns in the relationship between
rewards, task dynamics, and language to produce near optimal generations, even when the dataset
demonstrates only mediocre performance on the task.
OfÔ¨Çine RL provides a learning paradigm (Fig-
ure 1) that combines both supervised learning‚Äôs
ability to leverage existing data (criteria 4) with
the general utility optimization power of on-
line reinforcement learning methods (criteria
2, 3, 5) [4, 5, 6, 7, 8, 9, 10]. However, prior
ofÔ¨Çine RL approaches for language tasks are
either based on dynamic programming, which
enjoy the temporal compositionality but suffer
from high systems complexity, hyper-parameter
instability, and slow training times [11, 12, 13]
(meets criteria 5, fails 1), or methods based on
conditional imitation or dataset value learning that are simple and stable to train, but do not have
the ‚Äùstitching‚Äù beneÔ¨Åt of RL (meets criteria 1, fails 5) [8, 14, 15, 16, 17, 18]. Motivated by all these
criteria, we design a novel ofÔ¨Çine RL method based on dynamic programming with an implicit dataset
support constraint [5], that enjoys greater stability, fewer training time dependencies, and a more
Ô¨Çexible decoding process than prior approaches (see Sections 4 and 6.3). SpeciÔ¨Åcally, we Ô¨Åne-tune a
transformer language model to predict the state-action Q function and the state value function V at
each token. During training we perform iterative policy improvement by Ô¨Åtting value function to an
upper-expectile of the Q function. At inference time, we use our learned value functions to perturb
the log probabilities of a standard language model, steering the language model towards producing
outputs that maximize a speciÔ¨Åc utility (see Figure 3).
Our main contribution is twofold: 1) a novel ofÔ¨Çine RL algorithm, ILQL, for language models,
that employs a stable optimization process that can Ô¨Çexibly learn high-performing policies from
sub-optimal data in arbitrary sequential decision making settings, thus meeting each of the conditions
laid out above; and 2) a detailed empirical analysis, not only demonstrating ILQL‚Äôs ability to Ô¨Çexibly
adapt to many different utility functions more consistently and more stably than prior approaches,
but also ILQL‚Äôs unique ability to optimize stochastic or subjective reward functions and its ability to
discover optimal behavior in the face of sub-optimal or unusual data distributions.

Figure 2: ILQL meets each of the Ô¨Åve criteria for prac-
tical NLP RL methods we outline in Section 1.

2 Related Work
A number of prior works have explored combining online RL methods with language models for
natural language tasks such as machine translation or summarization [19, 20, 21, 22, 23, 24, 25, 26,
27, 28]. These works have demonstrated that RL can be a good tool for steering language models
towards satisfying utility functions. However, when it comes to settings that require multiple steps of
human interaction, e.g., dialogue, these methods can quickly become impractical [11, 29].
OfÔ¨Çine RL addresses this shortcoming by removing all need for environment interaction or user
simulators, instead operating purely on static datasets of prior human interaction. Several prior
works have applied ofÔ¨Çine RL to NLP and more broadly sequence generation problems [12, 11, 13,
14, 7, 8]. The most closely related to our work are those methods based on approximate dynamic
programming [12, 13, 11, 30]. While all these works present promising ofÔ¨Çine RL methods for
NLP tasks, none of them provide a method that achieves the simplicity, stability, and ease-of-use
aspect at the level of supervised learning. For example, Verma et al. and Jang et al. [11, 30] deÔ¨Åne

2

Method / CriteriaEasy to UseAble to Optimize User Specified RewardsPractical in Interactive SettingsAble to Leverage Existing DataTemporally CompositionalSupervised Learning (BC)Filtered Fine Tuning (%BC)Online RLILQL (ours)their action space at the ‚Äúper-utterance‚Äù level [11], resulting in expensive decoding processes during
training [31]; and while Jaques et al. [12, 13] remove this issue by deÔ¨Åning actions at the ‚Äúper-token‚Äù
level, the ofÔ¨Çine RL algorithm proposed requires querying likelihoods from a language model at RL
training time. This compounds complexity and its potential sources of error, and leads to increased
computational cost during training. Our proposed method instead operates both at the ‚Äúper-token‚Äú
level and trains in a fully self-contained way, without the need to simulate generation at training
time. This is achieved by combining an implicit dataset support constraint [5] with a novel policy
extraction method that takes advantage of the discrete ‚Äúper-token‚Äú action space. The result of these
design considerations is a simple, stable, and effective method that is easy for NLP practitioners to
pick up and apply to a variety of language-based tasks; in Section 6.3 we demonstrate our method‚Äôs
effectiveness in meeting these criteria through a series of ablations and comparisons.
Much prior work on steering language models towards desired behavior has done so without an
explicit utility function, instead either carefully curating Ô¨Ånetuning datasets [32, 33, 34], or designing
subtle decoding heuristics [35, 36, 37, 38, 39, 40, 41, 42]. A more closely related line of work uses
classiÔ¨Åers to guide LMs towards generating desired textual attributes [16, 43, 15, 17, 44, 18, 45].
These methods are closely related to the prior work on ofÔ¨Çine RL. In RL parlance, such methods
could be considered ‚Äúpolicy extraction‚Äù methods with Monte Carlo value estimates. This can be
interpreted as taking a single step of policy improvement which, though often effective [46], is
known to be suboptimal as compared to full dynamic programming methods (i.e., full Q-learning
or actor-critic) [5]. We will demonstrate empirically in Section 5 that our ofÔ¨Çine RL method can
lead to signiÔ¨Åcant improvements in Ô¨Ånal performance as compared to such ‚Äúsingle step‚Äù approaches,
particularly when the training data is highly suboptimal for the desired task.

3 Preliminaries: Language Generation as a Reinforcement Learning Task

Token-level POMDP.
In this work, we formalize language generation tasks as a partially observable
Markov decision process (POMDP). We deÔ¨Åne the POMDP M at the token level with M =
(S, A, O, T , Z, ¬µ0, R, Œ≥). We deÔ¨Åne the agent‚Äôs observation ht ‚àà O as a history of tokens with
ht = {t0, t1, t2, t3, ...tt‚àí1}; the action space at = tt ‚àà A is the set of possible next-tokens in our
vocabulary which includes the special end-of-turn token aend (see Figure 3).
In ofÔ¨Çine RL, the goal is to learn the optimal policy œÄ that achieves
Value-based ofÔ¨Çine RL.
highest discounted cumulative reward from a static dataset D that was produced by some potentially
suboptimal behavior policy œÄŒ≤. In this work, we build on the implicit Q-learning (IQL) algorithm [5],
which approximates the Bellman optimality equation constrained to in-dataset actions

Q‚àó(s, a) = R(s, a) + Œ≥

max
a(cid:48),s.t. œÄŒ≤ (a(cid:48)|s(cid:48))>0

Q‚àó(s(cid:48), a(cid:48)).

Instead of directly implementing the support constraint, IQL approximates the maximization on the
right-hand side of the constrained Bellman operator with expectile regression:
LV (œà) = E(s,a) ‚àºD[LœÑ

2 (QÀÜŒ∏(s, a) ‚àí Vœà(s))]

(1)

where LœÑ
maximum. Then this approximation can be used to estimate TD-targets for the Q-networks:

2 (u) = |œÑ ‚àí 1(u < 0)|u2. Increasing the hyperparameter œÑ , more closely approximates the

LQ(Œ∏) = E(s,a,s(cid:48)) ‚àºD[(R(s, a) + Œ≥Vœà(s(cid:48)) ‚àí QŒ∏(s, a))2].

(2)

IQL was designed for fully observable MDPs. However, in Section 4.1, we discuss how we adapt this
formulation to the POMDP setting described above using sequence models.

Supervised learning baselines.
In line with RL nomenclature, we denote Ô¨Ånetuning on curated
or Ô¨Åltered data as %BC and Ô¨Ånetuning on unÔ¨Åltered data as BC (also referred to as œÄŒ≤). See
Appendix A.3 for Ô¨Åltering details.

4

Implicit Language Q-Learning

Our main technical contribution implicit language Q-learning (ILQL), an ofÔ¨Çine RL algorithm for
NLP tasks. ILQL is speciÔ¨Åcally designed to enable simple and efÔ¨Åcient training of language models
with user-speciÔ¨Åed reward functions, with a workÔ¨Çow that is similar to standard supervised learning.
ILQL builds on the IQL algorithm, extending it to the token-level POMDP that deÔ¨Ånes NLP tasks
via the following modiÔ¨Åcations: (i) it integrates with sequence models to handle partially observable
language generation tasks (Section 4.2); (ii) it utilizes a novel policy extraction method that directly

3

Figure 3: A diagram of our Implicit Language Q Learning algorithm. Left: ILQL training involves three
transformers, each of which is Ô¨Ånetuned from a standard pretrained model: (1) A œÄŒ≤ model, Ô¨Ånetuned with
standard supervised learning. (2) A value function model, with Q and V on two separate heads; the value
functions are trained with Bellman backups using a combination of conservatism and and an implicit dataset
support constraint. (3) A target value network, which is a Polyak moving average of (2). Right: At inference time,
we use our learned value functions to perturb the log probabilities of œÄŒ≤ towards utility maximizing behavior.
perturbs the behavior policy œÄŒ≤ with our learned value functions, rather than training a separate actor
œÄ, signiÔ¨Åcantly improving performance and stability on NLP tasks (Section 4.1) and (iii) it adds a
conservatism loss term [6] to the Q-function, Ô¨Åxing a calibration issue in the policy extraction step.
Figure 3 provides an overview of our method.

4.1 Adapting Implicit Q-Learning to Language Models

Implicit value function learning. Like IQL, our method learns both a value function and a Q-
function, which bootstrap off each other through Bellman backups with implicit maximization
through an expectile loss. This recursive process of Ô¨Åtting Q and V corresponds to iterative policy
improvement subject to an implicit dataset support constraint, speciÔ¨Åed by the expectile used to Ô¨Åt V.
Due to parameter sharing, we combine Eqn. 1 and 2 into a single loss function:

LQ,V (Œ∏) = EœÑ ‚àºD

(cid:34) T

(cid:88)

i=0

(R(hi, ai) + Œ≥VŒ∏(hi+1) ‚àí QŒ∏(hi, ai))2 + LœÑ

(cid:35)
2 (QÀÜŒ∏(hi, ai) ‚àí VŒ∏(hi))

In contrast to IQL, we sample sequences of tokens instead of individual transitions to handle partial
observability, such that for each time step, the values are predicted based on a full history.

Policy extraction.
IQL [5] uses AWR policy extraction [47], which distills the Q-function into a
policy with a weighted log-likelihood loss, with weights given by eŒ≤( ÀÜQ‚àíV ). However, as we discuss
in the Section 6, we found this somewhat unstable to train on language models, likely due to the
high-variance gradients induced by the advantage weights. Fortunately, the value learning procedure
in IQL is independent of policy extraction, so instead of attempting to train a model to represent
the optimal policy, we use the learned Q and V values to directly perturb samples from a model
Ô¨Ånetuned via supervised learning to model œÄŒ≤ (see Figure 3). To this end, we compute a modiÔ¨Åed
likelihood for each token by adding its advantage Q(h, a) ‚àí V (h) to its logits under the œÄŒ≤ model,
with a multiplier Œ≤. We can then renormalize these pseudo-logits and sample them, resulting in
the implicit policy œÄ(a|h) ‚àù œÄŒ≤(a|h)eŒ≤(Q(h,a)‚àíV (h)) = exp(log(œÄŒ≤(a|h)) + Œ≤(Q(h, a) ‚àí V (h))).
This does not require training a separate actor, only a behavioral model œÄŒ≤, which can be trained with
the standard and stable supervised Ô¨Ånetuning objective.
However, na√Øvely performing policy extraction in this way can perform poorly due to over-smoothed
probabilities in œÄŒ≤ that may be nonzero for extremely unlikely tokens. In this case, samples from
œÄŒ≤ may be out of distribution for Q and V , and might have erroneous values. To Ô¨Åx this calibration
issue, we can either further decrease the probability of low probability actions in œÄŒ≤ by performing
top-p Ô¨Åltering or tuning a temperature parameter, or we can explicitly push down OOD Q-values
during training. We implement the latter by adding a small amount of NLL loss to the Q values,
which corresponds to the additional loss terms introduced by CQL [6] with a uniform KL regularizer.
Since ILQL actions are discrete tokens, as opposed to the original CQL method [5], which operates
on continuous action spaces, this CQL loss term is no more expensive than, and in fact equivalent to,
a standard cross-entropy loss at the token level. We Ô¨Ånd that both of these approaches often work in
practice, but prefer the latter, Ô¨Ånding that it requires less tuning for policy extraction at inference time.

4

Transformer Value FunctionHi!</a>Howareyou?</s>I‚Äômwell‚Ä¶,.!‚Ä¶How‚Äôs</a>What‚Ä¶HiHelloWhat‚Ä¶<a>1.261.011.06Q0Q1Q2V0V1V2‚Ä¶‚Ä¶‚Ä¶0.971.001.12Q3Q4Q5V3V4V5OkNotI‚Äômdoingokwell,.</a>TrainingVt fits to upper expectile of Qt:Qt fits to mean of rt + ùõæVt+1:Task DataùõëŒ≤ TransformerHi!</a>Howareyou?</s>I‚Äômwell‚Ä¶<a>!</a>I‚Äômwell</a>‚Ä¶HiTarget NetHi!‚Ä¶<a>CopyTarget ValuesIQL: Implicit BackupCQL LossùõëŒ≤ Transformer</a>Howareyou?</s>Ok‚Ä¶ln P3OkNotI‚ÄômOk‚Ä¶Q3OkNotI‚Äôm0.97V3ùú∑()‚Ä¶Transformer Value Function</a>Howareyou?</s>‚Ä¶Ok‚Ä¶OkNotI‚ÄômDeployment2. Perturb logprobs with value function3. sample from perturbed distribution1. Fetch LM logprobsKey:Q(                  ,      ) ‚Üí Haveaniceday0.9V(                   ) ‚Üí 0.7Haveaniceùõë(                   ) ‚Üí Haveanicedayr (                   ,       ) ‚Üí Haveaniceday+1.0SLFigure 4: Left: an abstract depiction of an MDP where SARSA fails to discover the optimal policy. Right: A
notional illustrative example where we might expect full ‚Äúmulti-step‚Äù RL methods (such as ILQL) to perform
signiÔ¨Åcantly better than ‚Äúsingle-step‚Äù methods such as SARSA or Ô¨Åltered supervised learning methods that
simply imitate high-scoring examples. In this example, good utterances tend to start with ‚ÄúThe movie was...‚Äù,
while bad utterances start with ‚ÄúThe movie wasn‚Äôt...‚Äù However, the very best examples also start with ‚ÄúThe movie
wasn‚Äôt...‚Äù, requiring multi-step planning or multiple steps of policy improvement to derive effective strategies.
Due to the temporal structure of this example, methods that implement just a single step of policy improvement
will fail to produce maximally positive sentiment outputs. While this example may appear somewhat contrived,
we see in our experiments that multi-step RL methods do lead to improvements in a number of more real settings.

Our full loss function is therefore:

Lc

Q,V (Œ∏) = LQ,V (Œ∏) ‚àí Œ±EœÑ ‚àºD log

(cid:18)

eQŒ∏(si,ai)
a(cid:48)‚ààA eQŒ∏(si,a(cid:48))

(cid:80)

(cid:19)

In early experiments, we found that decoding using the CQL regularized value functions alone,
without œÄŒ≤, required careful tuning of the CQL weight Œ±. When the CQL regularized value function
is combined with œÄŒ≤ for policy extraction, it mitigates this issue with hyper parameter sensitivity, and
simply setting the CQL weight Œ± to an arbitrary small value less than 1 typically works well.

4.2 Architectures for Implicit Language Q-Learning

We use GPT-2 small as the base model for all transformers. Our value function transformer has
three MLP heads: two independently initialized and trained Q heads and one V head. Each head
has two layers, with a hidden dimension twice that of the transformer‚Äôs embedding dimension. Our
target Q value is parameterized as the minimum prediction of both Polyak averaged target Q heads:
ÀÜQ = min(Q1, Q2) [48]. Just as in standard language modeling, the transformer‚Äôs auto-regressive
causal masking enables us to perform Bellman updates over entire sequences in parallel.

5 Proof of Concept: Multi-Step OfÔ¨Çine RL on Wordle

The lack of easily conÔ¨Ågurable task settings and reliable simulated evaluations has arguably down
progress in applying sophisticated RL algorithms to language and dialogue tasks [49, 50, 51].
To address this, we present the Wordle game [52] as an easy-to-use but challenging objective
benchmark task to test the capabilities of ofÔ¨Çine RL algorithms. While this task does not require
handling compositional natural language, it still retains many of the challenges of RL with sequence
models. In this section, we use this task to explore situations where we would expect ofÔ¨Çine RL to
lead to signiÔ¨Åcant improvement over simpler methods based on supervised learning or single-step
improvement (e.g., SARSA-style or Ô¨Åltered supervised learning methods).

Multi-step RL: a motivating example. General value-based RL methods based on solving the
Bellman equation described in Section 3 can be viewed as iteratively improving the policy: each
update sets the current value Q(ht, at) to be the reward plus the maximum possible next time step
value according to the current value function. This is in contrast to ‚Äúsingle-step‚Äù update methods,
which do not recursively update the value function to account for maximization at future time steps,
instead only learning to estimate the value of the behavior policy (i.e., the policy that collected
the dataset), and then at test-time greedily selecting the action that maximizes this value. Classic
examples of such methods use Monte Carlo regression or SARSA [53] to train the value function,
and then greedily choose actions at test-time, though a number of different methods of this sort have
been proposed for guided language generation in the literature [16, 43, 15, 17, 44, 18, 45].
Since ILQL performs multiple steps of policy improvement, it can be strictly better than SARSA
when the underlying data is highly sub-optimal. One particular such case corresponds to the notional
task in Figure 4, in which the optimal sequence of actions requires one to go through a state that is

5

The moviewasn‚Äôt good.wasIt was fantastic! A true experience!I disliked every minute.a good time.Goal: produce outputs that maximally activate a sentiment classifierreward: +1.0reward: +0.5SARSA: Q(s, a)=+0.5ILQL: Q(s, a)=+0.5reward: -1.0SARSA: Q(s, a)=-6.4ILQL: Q(s, a)=+1.0SARSAILQLILQLS2LavaGoalS0S1R=0R = 0.5R = 1R = -1Abstract MDP DiagramThe movie wasn‚Äôt good. It was utterly mind blowing, creative, and inspiring!The movie wasn‚Äôt good. It was a masterpiece, an experience, a lifetime!The movie wasn‚Äôt good. I wanted to fall asleep by the end.The movie wasn‚Äôt good. All the characters were one dimensional, and it lacked creativity.The movie was good.The movie was solid. It entertained.10% of data+1.0 reward each45% of data-1.0 reward each45% of data0.5 reward eachDataset CompositionSARSAFigure 6: We empirically validate the setting depicted in Figure 4 on a Wordle task. Left: a visualization of
the synthetic dataset distribution we constructed to demonstrate the beneÔ¨Åts of ILQL‚Äôs multiple steps of policy
improvement over ‚Äúsingle step‚Äù methods, such as SARSA. Right: a plot showing that ILQL‚Äôs Q function learns
to more often assign higher Q values to optimal actions than SARSA.
also frequented by sub-optimal examples. In this case, SARSA will learn to take actions that appear
safer according to the dataset ‚Äî such as the transition ‚ÄúThe movie‚Äù ‚Üí ‚Äúwas‚Äù in Figure 4 -‚Äì whereas
full (‚Äúmulti-step‚Äù) RL methods would recover the optimal policy. We demonstrate this phenomenon
empirically on the Wordle game below.

Wordle dataset. Our Wordle task is designed to allow us to use data from real humans in a sequen-
tial decision-making setting while still enabling objective simulated evaluation and the Ô¨Çexibility to
compose datasets with different properties, thus providing an effective benchmark for validating a
variety of approaches. Wordle is a word guessing game; the agent gets 6 turns to guess a 5 letter
word randomly selected from a vocabulary, and the environment responds with one of three ‚Äúcolors‚Äù
for each letter in the guessed word: ‚Äúblack‚Äù meaning the guessed letter is not in the environment‚Äôs
word, ‚Äúyellow‚Äù meaning the guessed letter is in the word but not in the right location, and ‚Äúgreen‚Äù
meaning the guessed letter is in the right location. We give a reward of -1 for each incorrect guess
and a reward of 0 for a correct guess, at which point environment interaction ends; the agent‚Äôs goal is
therefore to guess the correct word in as few turns as possible, a task for which computing optimal
behavior has previously been proven to be an intractable NP-Hard problem [52].
While Wordle may appear distinct from natural language tasks, it shares a number of high-level
properties with more complex language domains, making it well suited for a Ô¨Årst evaluation of
NLP-focused RL methods. Like dialogue, the game has non-deterministic dynamics and a sequential
turn-based structure. However, while Wordle does not require any understanding of word semantics
or grammatical structure, this property makes it easy to evaluate policies objectively, providing
complementary beneÔ¨Åts to more realistic (but more subjective and harder to evaluate) tasks, such as
dialogue. Additionally, Wordle not only enables training on datasets of naturalistic, human games
scraped from Twitter, but also on datasets synthesized by hand-crafted policies of various skill-levels.

method Wordle Score
-2.13 ¬± 0.03
ILQL
-2.23 ¬± 0.03
SARSA
-2.38 ¬± 0.03
%BC
-2.61 ¬± 0.03
BC
-1.75 ¬± 0.02
œÄoptimal

Synthetic Wordle task. Having motivated multiple steps of
policy improvement in Figure 4 and introduced the Wordle
task, we now describe the synthetic Wordle dataset which we
construct to demonstrate how a single step of policy improve-
ment can fail catastrophically in comparison to multiple steps
of improvement. We create such a distribution in Wordle, by
synthesizing a dataset consisting of a mixture of three distinct
policies, each representing one of the paths through the MDP
in Ô¨Ågure 4: (1). œÄoptimal, a high-performing policy which my-
opically selects the word with the highest information gain,
representing the path from S0 ‚Üí Goal. (2). œÄadversarial, which behaves the same as œÄoptimal for the
Ô¨Årst two actions (S0 ‚Üí S1) and then randomly repeats these Ô¨Årst two words at every subsequent
action (S0 ‚Üí Lava). (3). œÄsuboptimal, which selects any random word from the game vocabulary
50% of the time, and the other 50% randomly selects a word that meets all known letter constraints,
representing the path from S0 ‚Üí S1. The relative performance of these policies is ordered according
to œÄoptimal > œÄsuboptimal > œÄadversarial. We construct our dataset with 9% of the data coming from
œÄoptimal, 45.5% from œÄsuboptimal, and 45.5% from œÄadversarial.
Evaluating ILQL on the synthetic Wordle task. Measuring the predicted Q values from our
models, in Figure 6 (right) we observe that ILQL assigns higher values to actions corresponding
to the paths towards ‚Äúmisleading states‚Äù (i.e. S2) than those to the ‚Äúgoal states‚Äù (i.e. S1), whereas

Figure 5: Comparing ILQL to baselines
on Wordle human data. Even on realistic
human data, ILQL outperforms ‚Äúsingle
step‚Äù SARSA.

6

represents S0=>Goalaverage reward: -2.647~9% of datarepresents S0=>Lavaaverage reward: -6.0~45.5% of data*repeats the first two words from optimal policyrepresents S0=>S1average reward: -4.262~45.5% of dataWe synthesize a dataset with trajectories from 3 different Wordle policies.We see a dramatic difference between ILQL and SARSA on this dataset.Fraction of Q Values Where [ S0 => S2 ] > [ S0 => S1 ]SARSA shows the exact opposite preference, conÔ¨Årming both our hypothesis that this type of MDP
would be amenable to multiple steps of policy improvement, and that ILQL as an algorithm is able to
perform such policy improvement.
Validating on natural Wordle data. While the synthetic setting explored above was speciÔ¨Åcally
designed to demonstrate a dramatic difference between ILQL and SARSA, the Ô¨Åndings still transfer
to more realistic settings. In Table 5, we demonstrate ILQL outperforming SARSA on a natural
dataset of Wordle games scraped from Twitter (see Appendix A.4 for details).

6 Natural Language Experiments

Next, we evaluate ILQL on two realistic language tasks. We Ô¨Årst identify scenarios in which one
might expect ofÔ¨Çine RL to be particularly beneÔ¨Åcial: (1) tasks that demand repeated interactions,
such as dialogue; (2) data that is highly sub-optimal under its utility function; (3) settings with highly
stochastic rewards based on subjective human judgement (e.g., avoiding toxic language). Taking into
account these three scenarios, we evaluate ILQL on (1) a goal-directed question asking task based
on Visual Dialogue [54], where achieving high rewards on a diverse set of metrics during repeated
interactions is desirable, and (2) a forum comment generation task based on Reddit Comments with
highly subjective and noisy reward functions (toxicity ratings or upvotes). For general experiment
details see Appendix A.3.

6.1 Evaluating Diverse Rewards on Visual Dialogue

Visual Dialogue dataset. We use the Visual Dia-
logue dataset [54] to evaluate our algorithm‚Äôs ability
to optimize many different reward functions in com-
plex dialogue settings. The task involves a question
asking agent and a question answering agent, the latter
of which is presented with an image and tasked with
answering the former‚Äôs questions about the image. In-
stead of using this task as a question answering task, we
follow Das et al. [55] and train our agents to ask ques-
tions, with rewards based on how well the ground-truth
image can be predicted from the resulting dialogue. For
evaluation, we use the model from Das et al. [55] as our
environment simulator. To allow our agents to operate
entirely in the space of natural language, we treat the
image embedding as part of the reward function, using
the (separately pre-trained) supervised model proposed
by Das et al. [55] to predict the image embedding from
the dialogue. See Figure 7 for example dialogues in
this domain. We chose this environment speciÔ¨Åcally
because (1) it has been previously studied in the con-
text of RL [55]; (2) as a dialogue game, automated
evaluation is more reliable than other tasks; and (3) the
Q&A structure enables some temporal compositional-
ity (i.e. the answer to one question may prompt new
more speciÔ¨Åc questions).
Visual Dialogue task. The agent receives a reward
of -1 for each turn in which the ground truth image
is sufÔ¨Åciently difÔ¨Åcult to predict from the dialogue,
otherwise the agent receives a reward of 0 and the envi-
ronment ends interaction. For details on the task setup,
see Appendix A.5. Since the Visual-Dialogue dataset
was largely designed for supervised learning agents,
the data is already near optimal for the original task.
However, if we shift the reward function such that the
data is no longer optimal, we can observe very large
improvements from ofÔ¨Çine RL. We therefore use this
domain to demonstrate ofÔ¨Çine RL‚Äôs Ô¨Çexibility to adapt to different reward functions. We consider
three reward functions: ‚Äústandard‚Äù, ‚Äúy/n‚Äù, and ‚Äúconservative y/n‚Äù. ‚ÄúStandard‚Äù is simply the reward

Figure 7: Dialogues from agents optimized for
different reward functions on the Visual Dia-
logue task. The questions are qualitatively differ-
ent depending on the reward function. The ‚Äústan-
dard‚Äù agent asks many yes/no question, whereas
adding an exact string match penalty for yes/no
questions prevents many of such questions from
being asked, and adding a more conservative
yes/no penalty prevents all of such questions.

7

Image Caption: A man, with other people in the background, is sitting at a picnic table with a cell phone in front of him while he uses a fork and knife to cut a dessert on a plate.Questioner: is man young?Answerer: yesQuestioner: is he wearing glasses?Answerer: noQuestioner: is there food on table?Answerer: yesQuestioner: is there food on table?Answerer: yesImage Caption: A person flying a kite over a sand dune.Questioner: what color is kite?Answerer: it's black and whiteQuestioner: is person male or female?Answerer: can't tellQuestioner: what is person wearing?Answerer: i can't tellQuestioner: what color is sand?Answerer: dark brownQuestioner: is person flying kite flying in air?Answerer: yesQuestioner: what color is kite?Answerer: blackImage Caption: Tour buses are lined up on the street waiting for people.Questioner: how many buses?Answerer: 2Questioner: what color are buses?Answerer: white and redQuestioner: how many people?Answerer: 2Questioner: what gender are people?Answerer: 1 is male and 1 is femaleQuestioner: what are they wearing?Answerer: 1 is wearing shorts and other is wearing shorts and shirtQuestioner: what color is their hair?Answerer: dark brownStandard Reward Function:Yes/No Penalty:Conservative Yes/No Penalty:method
ILQL
SARSA
%BC
BC

standard
-5.22 ¬± 0.13
-5.14 ¬± 0.13
-5.07 ¬± 0.13
-5.25 ¬± 0.13

y/n
-5.69¬±0.13
-6.19¬±0.15
-7.48 ¬± 0.21
-10.85 ¬± 0.27

conservative y/n
-6.57 ¬± 0.18
-7.77 ¬± 0.20
-9.13 ¬± 0.22
-15.16 ¬± 0.35

train/eval
standard
y/n
conservative y/n

standard
-5.22 ¬± 0.13
-5.41 ¬± 0.12
-5.29 ¬± 0.13

y/n
-11.12 ¬± 0.30
-5.69 ¬± 0.13
-5.42 ¬± 0.13

conservative y/n
-14.97 ¬± 0.36
-8.24 ¬± 0.22
-6.57 ¬± 0.18

Table 1: Left: Comparing ILQL to baselines on various Visual Dialogue reward settings. ILQL is able to
Ô¨Çexibly optimize for many different rewards, even those for which the data is highly sub-optimal (e.g. BC
performance). Right: Evaluating each ILQL agent on all other reward functions. Agents generally perform
worse on reward functions for which they were not trained.
described above and detailed in Appendix A.5. ‚Äúy/n‚Äù penalizes the agent for asking questions that
produce yes or no answers. It assigns a reward of -2 each time the other speaker says ‚Äúyes‚Äù or ‚Äúno‚Äù in
addition to the ‚Äústandard‚Äù reward. This is challenging, because while the data contains many yes/no
questions, the goal is not for the agent itself to avoid those words, but rather avoid saying things that
will cause the other speaker to use them. Not all simple questions produce literal ‚Äúyes/no‚Äù answers,
so our third reward further penalizes all brief responses, such as ‚ÄúI can‚Äôt tell‚Äù, ‚Äúno it isn‚Äôt‚Äù, ‚Äúyes it is‚Äù,
‚ÄúI don‚Äôt know‚Äù. This reward function assigns a -2 reward to a set of low-information responses using
a handful of conservative string matching heuristics, detailed in Appendix A.5.

Results on optimizing diverse rewards We demonstrate that ILQL learns a policy distinct from
the dataset behavior policy and can optimize many different rewards in Table 1 left, where we can see
that ILQL is able to outperform baselines on most of our Visual Dialogue reward functions. Agents
optimized for each reward function are quantitatively different as well: in Table 1 right, we see
that ofÔ¨Çine RL agents trained on one reward function are generally suboptimal on others. Figure 7
demonstrates this qualitatively: without the ‚Äúyes/no‚Äù penalty, our policies tend to ask many ‚Äúyes/no‚Äù
questions, but under its presence policies tend to ask more color or number based questions instead.
Even when the underlying data is highly sub-optimal for a given reward function, ILQL is able to
determine the desired behavior.

6.2 Subjective Rewards on Reddit Comments

method
ILQL
SARSA
%BC
BC

toxicity
0.0¬±0.0
0.0¬±0.0
-0.74¬±0.07
-3.51¬±0.13

upvotes real
9.83¬±0.04
6.23¬±0.15
7.06¬±0.14
4.87¬±0.16

upvotes model
10.0¬±0.0
10.0¬±0.0
7.86¬±0.13
4.87¬±0.16

Reddit comments dataset. To evalu-
ate our agents on minimally curated
and maximally diverse open-domain text
with highly stochastic reward functions
based on subjective human judgement,
Figure 8: A comparison of ILQL against baselines on the
we train our ofÔ¨Çine RL algorithm on a
various Reddit comments reward functions.
ILQL manages
large dataset of 4 million Reddit com-
ments from 1; our agents are given a par-
to never generate undesirable comments on 2 out of 3 reward
functions, whereas Ô¨Åne-tuning on Ô¨Åltered data occasionally does.
ent comment or post as context and then
trained to produce replies that satisfy one of two rewards: ‚Äútoxicity‚Äù and ‚Äúupvotes real‚Äù. Given that
this is open internet text, the data contains much toxic language, so for our ‚Äútoxicity‚Äù reward, we
train our agents to satisfy a toxicity Ô¨Ålter. This Ô¨Ålter gives a reward of -10 for toxic comments, -5 for
moderately toxic, and 0 for non-toxic. Our second reward function incentivizes generating comments
that would receive a positive number of upvotes: giving a reward of +10 for the positive case and 0
for the negative. We automatically evaluate our upvote agents with a Ô¨Ånetuned RoBERTa-base model,
that predicts whether a comment will receive positive upvotes. We train this model on a held-out split
of the data (see Appendix A.6 for more details on our reward models). We train agents on both the
ground truth upvotes (denoted ‚Äúupvotes real‚Äù) and on this upvote model‚Äôs predicted reward (denoted
‚Äúupvotes model‚Äù).

Results on optimizing noisy rewards.
In natural language tasks, we may need to optimize stochas-
tic, high-variance reward functions based on subjective judgement, such as whether a Reddit comment
should be Ô¨Çagged as toxic, or whether it is likely to receive an upvote. Such stochastic settings
should be expected when multiple users with different opinions are providing reward labels. OfÔ¨Çine
RL, by design, is robust to environment stochasticity, and therefore should be able to optimize such
noisy reward environments. We use the Reddit toxicity and upvote tasks to study how well ILQL
in particular can handle such settings. As we can see in Table 8, ILQL is surprisingly able to get a
perfect or near-perfect score on these more subjective settings, whereas more standard approaches,
such as Ô¨Åltered Ô¨Ånetuning on only non-toxic or only positive upvote comments (%BC), perform
signiÔ¨Åcantly worse (i.e. generates more comments Ô¨Çagged as toxic or predicted to have negative
upvotes). We have additional complementary experiments studying this effect in appendix A.8.

1https://www.kaggle.com/code/danofer/reddit-comments-scores-nlp/notebook

8

Figure 9: Two toxic comments incidentally generated by the Ô¨Åltered Ô¨Åne-tuning model. ILQL assigns negative
advantages to many tokens, thus demonstrating how ILQL is more effectively able to avoid such generations.

max score
-5.69¬±0.13
-5.89¬±0.14
-7.35 ¬± 0.17
-5.57¬±0.13

method
ILQL
ILQL (utterance)
SARSA (utterance)
CHAI

œÉ w.r.t hparams
0.42
0.51
0.21
1.11
Table 2: On the VisualDialogue ‚Äúy/n‚Äù reward, we compare per-token ILQL to per-utterance ILQL, per-utterance
SARSA, and CHAI [11]. We observe that per-token ILQL is generally much faster at inference time than
per-utterance methods. All evaluations were performed on a single T4 GPU. All baseline implementations build
on the same core code for sampling utterances, with a handful of method speciÔ¨Åc runtime optimizations in each
case.

inference time per-dialogue (sec)
5.10¬±0.12
22.1¬±0.47
20.38¬±0.41
12.13 ¬± 0.25

6.3

ILQL Ablations

method
ILQL
CQL
œà
SARSA
DT
ILQL (AWR)
%BC
BC

max score
-5.69¬±0.13
-7.32¬±0.17
-10.05¬±0.18
-6.19 ¬± 0.15
-6.70 ¬± 0.17
-5.96¬±0.13
-7.48 ¬± 0.21
-10.85 ¬± 0.27

We aim to understand which components of
ILQL enable both good results and greater ease-
of-use than prior ofÔ¨Çine RL approaches for lan-
guage tasks. In particular, we validate ILQL‚Äôs
main design decisions: the choice of a per-token
action space, our value learning method, and our
policy extraction strategy.
We evaluate these comparisons on the Visual
Dialogue ‚Äúyes/no‚Äù reward because (1) it is im-
portant to compare ofÔ¨Çine RL methods on a chal-
lenging and realistic sequential decision prob-
lem like dialogue, and (2) since the Visual Dia-
logue data is already ‚Äúnear-expert‚Äù for the ‚Äústandard‚Äù reward, the ‚Äúyes/no‚Äù reward is better able to
differentiate between methods.

Figure 10:
ILQL out-performs baseline ofÔ¨Çine-RL
methods on the Visual Dialogue ‚Äúy/n‚Äù reward, while
also having less hyperparameter sensitivity than other
top performing methods. We show the best hyperparam-
eter setting found for each method.

œÉ w.r.t hparams
0.42
1.98
0.60
0.27
1.15
2.82
0.72
-

Ablations on per-token vs. per-utterance actions. We hypothesize that, since a per-token Q
function enables an efÔ¨Åcient search through the utterance action space, performing ofÔ¨Çine RL at the
token level, rather than the utterance level, can yield less expensive inference and better performance.
We compare ILQL to: (1) ILQL (utterance): a per-utterance adaptation of ILQL that is nearly
identical to ILQL, except that it removes the conservatism loss term and performs Bellman backups
at the utterance level instead of the token level; (2) SARSA (utterance): a per-utterance version of
SARSA, which is identical to (1) with œÑ = 0.5; and (3) CHAI [11]: an adaptation of CQL for use on
language models at the per-utterance action level. For policy extraction, each of these baselines uses
EMAQ [29], where we sample N utterances from a learned behavior policy and then re-rank with the
Q function.
We see in Table 2 that per-token ILQL outperforms per-utterance ILQL and SARSA, while also
running inference ‚àº4x faster on a single T4 GPU. When tuned well, we see that CHAI can slightly,
but not signiÔ¨Åcantly, outperform ILQL. However, CHAI is much less stable with respect to hyper-
parameters than ILQL and is >2x slower at inference time. Additionally, training CHAI involves
an expensive preprocessing step of sampling multiple utterances for all actions in the training data,
greatly hindering ease of use.

Ablations on choice of OfÔ¨Çine RL algorithm. We compare ILQL to four other RL methods: a
per-token version of CQL, an adaptation of the œà-learning as proposed by Jaques et al. [12, 13],
decision transformer (DT) [8], and SARSA [16, 43, 15, 17, 44, 18, 45]. In Table 10, we see that

9

0.0this-0.9horrible-0.3site-1.1   .0.2on0.1comments-0.5your-0.9censor-0.1they0.1And-0.6he-1.0was-1.2a-1.0   dead-2.0thinking0.5out-0.6skull-0.6your-1.6ripped0.1He-1.0   man-2.1   .ILQL per-token advantages for toxic comments generated by filtered finetuning modeladvantage:token:advantage:token:ILQL signiÔ¨Åcantly outperforms baselines, and also has the second lowest hyper-parameter variance,
just behind SARSA, conÔ¨Årming our hypothesis that ILQL can provide both high relative performance
and training stability.

Ablations on policy extraction strategies. While ILQL largely follows the design of IQL [5], in
adapting it to sequence models, we design a novel policy extraction strategy, as described in Section 4.
In the next experiment, we compare this extraction procedure to the standard AWR-based [47] policy
extraction method used in IQL [5] and a number of other ofÔ¨Çine RL algorithms [56, 57]. We expect
that our approach should generally yield better performance, while also being easier to tune.
We apply AWR extraction to our best performing ILQL value function, denoting this as ILQL (AWR).
Table 10 demonstrates that ILQL is both more stable and better at extracting good performance from
a given value function than the well established AWR-style extraction. Additionally, AWR extraction
requires tuning Œ≤ at training time rather than at inference time, generally decreasing Ô¨Çexibility and
increasing the time and effort spent tuning parameters and re-training.

7 Conclusion

We proposed ILQL, an ofÔ¨Çine RL method for effectively steering language generation to fulÔ¨Åll a
variety of desirable conversational behaviors. Through experiments ranging from word games and
goal-directed question asking to optimizing upvotes and minimizing toxic language, ILQL shows
that ofÔ¨Çine RL can serve as a strong alternative to the method landscape of language generation
dominated by language model Ô¨Ånetuning on manually Ô¨Åltered datasets and classiÔ¨Åer guidance. We
hope the positive results from ILQL will inspire more work on ofÔ¨Çine RL for dialogue, and lead to
more controllable language models that directly optimize user-speciÔ¨Åed utility functions for a wide
range of tasks in text generation. Additionally, as any utility optimization method can be used to
aid or harm, we hope these future works consider ethical uses of ofÔ¨Çine RL. Our method also has
its limitations: for example, ILQL may not prove effective when datasets are extremely suboptimal.
OfÔ¨Çine RL would also not be ideal in settings that require distributional constraints, such as fairness.

Acknowledgements

This research was supported by an Amazon-BAIR Commons project, as well as the OfÔ¨Åce of Naval
Research. We thank Dilek Hakkani-tur, Alexandros Papangelis, and Ruiqi Zhong for advice and
feedback.

References

[1] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig.
Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language
processing, 2021.

[2] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.
Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz
Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.

[3] Sewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Noisy channel language

model prompting for few-shot text classiÔ¨Åcation, 2021.

[4] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. OfÔ¨Çine reinforcement learning:

Tutorial, review, and perspectives on open problems, 2020.

[5] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. OfÔ¨Çine reinforcement learning with implicit

q-learning, 2021.

[6] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for
ofÔ¨Çine reinforcement learning. In Proceedings of the 34th International Conference on Neural
Information Processing Systems, NIPS‚Äô20, Red Hook, NY, USA, 2020. Curran Associates Inc.

[7] Michael Janner, Qiyang Li, and Sergey Levine. OfÔ¨Çine reinforcement learning as one big
sequence modeling problem. In Advances in Neural Information Processing Systems, 2021.

10

[8] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter
Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning
via sequence modeling, 2021.

[9] Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea

Finn, and Tengyu Ma. Mopo: Model-based ofÔ¨Çine policy optimization, 2020.

[10] Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel :

Model-based ofÔ¨Çine reinforcement learning, 2020.

[11] Siddharth Verma, Justin Fu, Mengjiao Yang, and Sergey Levine. Chai: A chatbot ai for

task-oriented dialogue with ofÔ¨Çine reinforcement learning, 2022.

[12] Natasha Jaques, Judy Hanwen Shen, Asma Ghandeharioun, Craig Ferguson, Agata Lapedriza,
Noah Jones, Shixiang Shane Gu, and Rosalind Picard. Human-centric dialog training via ofÔ¨Çine
reinforcement learning, 2020.

[13] N. Jaques, S. Gu, D. Bahdanau, J. M. Hernandez-Lobato, R. E. Turner, and D. Eck. Sequence
tutor: Conservative Ô¨Åne-tuning of sequence generation models with kl-control. International
Conference on Machine Learning (ICML), 2017.

[14] Charlie Snell, Sherry Yang, Justin Fu, Yi Su, and Sergey Levine. Context-aware language

modeling for goal-oriented dialogue systems, 2022.

[15] Ari Holtzman, Jan Buys, Maxwell Forbes, Antoine Bosselut, David Golub, and Yejin Choi.

Learning to write with cooperative discriminators, 2018.

[16] Kevin Yang and Dan Klein. FUDGE: Controlled text generation with future discriminators.
In Proceedings of the 2021 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies. Association for Computational
Linguistics, 2021.

[17] Jiwei Li, Will Monroe, and Dan Jurafsky. Learning to decode for future success, 2017.

[18] Ben Krause, Akhilesh Deepak Gotmare, Bryan McCann, Nitish Shirish Keskar, ShaÔ¨Åq Joty,
Richard Socher, and Nazneen Fatema Rajani. GeDi: Generative discriminator guided sequence
generation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages
4929‚Äì4952, Punta Cana, Dominican Republic, November 2021. Association for Computational
Linguistics.

[19] Marc‚ÄôAurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level

training with recurrent neural networks, 2015.

[20] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang
Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah,
Melvin Johnson, Xiaobing Liu, ≈Åukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo,
Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason
Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey
Dean. Google‚Äôs neural machine translation system: Bridging the gap between human and
machine translation, 2016.

[21] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive

summarization, 2017.

[22] Yuxiang Wu and Baotian Hu. Learning to extract coherent summary via deep reinforcement

learning, 2018.

[23] Pradyumna Tambwekar, Murtaza Dhuliawala, Lara J. Martin, Animesh Mehta, Brent Harrison,
and Mark O. Riedl. Controllable neural story plot generation via reward shaping. In Proceedings
of the Twenty-Eighth International Joint Conference on ArtiÔ¨Åcial Intelligence. International
Joint Conferences on ArtiÔ¨Åcial Intelligence Organization, aug 2019.

[24] Jiatao Gu, Kyunghyun Cho, and Victor O. K. Li. Trainable greedy decoding for neural machine

translation, 2017.

[25] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin,
Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton,
Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano,
Jan Leike, and Ryan Lowe. Training language models to follow instructions with human
feedback, 2022.

11

[26] Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec
Radford, Dario Amodei, and Paul Christiano. Learning to summarize from human feedback,
2020.

[27] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei,
Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences,
2019.

[28] Florian B√∂hm, Yang Gao, Christian M. Meyer, Ori Shapira, Ido Dagan, and Iryna Gurevych.
Better rewards yield better summaries: Learning to summarise without references, 2019.
[29] Seyed Kamyar Seyed Ghasemipour, Dale Schuurmans, and Shixiang Shane Gu. Emaq:

Expected-max q-learning operator for simple yet effective ofÔ¨Çine and online rl, 2020.

[30] Youngsoo Jang, Jongmin Lee, and Kee-Eung Kim. GPT-critic: OfÔ¨Çine reinforcement learning
In International Conference on Learning

for end-to-end task-oriented dialogue systems.
Representations, 2022.

[31] Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On
the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021
ACM Conference on Fairness, Accountability, and Transparency, FAccT ‚Äô21, page 610‚Äì623,
New York, NY, USA, 2021. Association for Computing Machinery.

[32] Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston.

Personalizing dialogue agents: I have a dog, do you have pets too?, 2018.

[33] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a

machine really Ô¨Ånish your sentence?, 2019.

[34] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don‚Äôt know: Unanswerable

questions for squad, 2018.

[35] Clara Meister, Tim Vieira, and Ryan Cotterell. If beam search is the answer, what was the

question?, 2020.

[36] Jungo Kasai, Keisuke Sakaguchi, Ronan Le Bras, Dragomir Radev, Yejin Choi, and Noah A.

Smith. Beam decoding with controlled patience, 2022.

[37] Clara Meister, Tiago Pimentel, Gian Wiher, and Ryan Cotterell. Typical decoding for natural

language generation, 2022.

[38] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation, 2018.
[39] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural

text degeneration, 2019.

[40] Sourya Basu, Govardana Sachitanandam Ramachandran, Nitish Shirish Keskar, and Lav R.

Varshney. Mirostat: A neural text decoding algorithm that directly controls perplexity, 2020.

[41] Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, and Richard Socher.

Ctrl: A conditional transformer language model for controllable generation, 2019.

[42] Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, and Jacopo

Staiano. Discriminative adversarial search for abstractive summarization, 2020.

[43] Marjan Ghazvininejad, Xing Shi, Jay Priyadarshi, and Kevin Knight. Hafez: an interactive
poetry generation system. In Proceedings of ACL 2017, System Demonstrations, pages 43‚Äì48,
Vancouver, Canada, July 2017. Association for Computational Linguistics.

[44] R√©mi Leblond, Jean-Baptiste Alayrac, Laurent Sifre, Miruna Pislar, Lespiau Jean-Baptiste,
Ioannis Antonoglou, Karen Simonyan, and Oriol Vinyals. Machine translation decoding beyond
beam search. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language
Processing, pages 8410‚Äì8434, Online and Punta Cana, Dominican Republic, November 2021.
Association for Computational Linguistics.

[45] Ashutosh Baheti, Alan Ritter, Jiwei Li, and Bill Dolan. Generating more interesting responses

in neural conversation models with distributional constraints, 2018.

[46] David Brandfonbrener, William F. Whitney, Rajesh Ranganath, and Joan Bruna. OfÔ¨Çine rl

without off-policy evaluation, 2021.

[47] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression:

Simple and scalable off-policy reinforcement learning, 2019.

12

[48] Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in

actor-critic methods, 2018.

[49] Haoming Jiang, Bo Dai, Mengjiao Yang, Tuo Zhao, and Wei Wei. Towards automatic
evaluation of dialog systems: A model-free off-policy evaluation approach. arXiv preprint
arXiv:2102.10242, 2021.

[50] Jan Deriu, Alvaro Rodrigo, Arantxa Otegi, Guillermo Echegoyen, Sophie Rosset, Eneko Agirre,
and Mark Cieliebak. Survey on evaluation methods for dialogue systems. ArtiÔ¨Åcial Intelligence
Review, 54(1):755‚Äì810, 2021.

[51] Amanda Cercas Curry, Helen Hastie, and Verena Rieser. A review of evaluation techniques for
social dialogue systems. In Proceedings of the 1st ACM SIGCHI International Workshop on
Investigating Social Interactions with ArtiÔ¨Åcial Agents, pages 25‚Äì26, 2017.

[52] Daniel Lokshtanov and Bernardo Subercaseaux. Wordle is np-hard, 2022.

[53] Richard S Sutton, Andrew G Barto, et al. Introduction to reinforcement learning. 1998.

[54] Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jos√© M. F. Moura, Devi

Parikh, and Dhruv Batra. Visual dialog, 2016.

[55] Abhishek Das, Satwik Kottur, Jos√© M. F. Moura, Stefan Lee, and Dhruv Batra. Learning

cooperative visual dialog agents with deep reinforcement learning, 2017.

[56] Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. Awac: Accelerating online

reinforcement learning with ofÔ¨Çine datasets, 2020.

[57] Ziyu Wang, Alexander Novikov, Konrad Zolna, Jost Tobias Springenberg, Scott Reed, Bobak
Shahriari, Noah Siegel, Josh Merel, Caglar Gulcehre, Nicolas Heess, and Nando de Freitas.
Critic regularized regression, 2020.

[58] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal

policy optimization algorithms, 2017.

[59] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy

maximum entropy deep reinforcement learning with a stochastic actor, 2018.

[60] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.

Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.

[61] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of

deep bidirectional transformers for language understanding, 2018.

[62] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,

Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2017.

[63] Felix A. Gers, J√ºrgen Schmidhuber, and Fred Cummins. Learning to Forget: Continual

Prediction with LSTM. Neural Computation, 12(10):2451‚Äì2471, 10 2000.

[64] Justin Fu, Aviral Kumar, OÔ¨År Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for

deep data-driven reinforcement learning, 2020.

[65] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach, 2019.

Checklist

1. For all authors...

(a) Do the main claims made in the abstract and introduction accurately reÔ¨Çect the paper‚Äôs

contributions and scope? [Yes] see Section 6

(b) Did you describe the limitations of your work? [Yes] see Section 7
(c) Did you discuss any potential negative societal impacts of your work? [Yes] see

Section 7

(d) Have you read the ethics review guidelines and ensured that your paper conforms to

them? [Yes] I‚Äôve read it and our paper obeys these guidelines.

2. If you are including theoretical results...

13

(a) Did you state the full set of assumptions of all theoretical results? [N/A] We have no

theoretical results.

(b) Did you include complete proofs of all theoretical results? [N/A] We have no proofs.

3. If you ran experiments...

(a) Did you include the code, data, and instructions needed to reproduce the main experi-
mental results (either in the supplemental material or as a URL)? [Yes] see supplemental
(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they

were chosen)? [Yes] see Appendix

(c) Did you report error bars (e.g., with respect to the random seed after running experi-

ments multiple times)? [Yes] see Section 6

(d) Did you include the total amount of compute and the type of resources used (e.g., type

of GPUs, internal cluster, or cloud provider)? [Yes] see Appendix

4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
(a) If your work uses existing assets, did you cite the creators? [Yes] We cite all datasets

and models used in Section 6.

(b) Did you mention the license of the assets? [No] They are open sourced.
(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]

See supplemental for code.

(d) Did you discuss whether and how consent was obtained from people whose data you‚Äôre

using/curating? [N/A] All assets used are open source.

(e) Did you discuss whether the data you are using/curating contains personally identiÔ¨Åable
information or offensive content? [Yes] We use potentially toxic internet text for some
experiments; we discuss this in Section 6.

5. If you used crowdsourcing or conducted research with human subjects...

(a) Did you include the full text of instructions given to participants and screenshots, if

applicable? [N/A] We do not conduct experiments with human subjects.

(b) Did you describe any potential participant risks, with links to Institutional Review
Board (IRB) approvals, if applicable? [N/A] We do not conduct experiments with
human subjects.

(c) Did you include the estimated hourly wage paid to participants and the total amount
spent on participant compensation? [N/A] We do not conduct experiments with human
subjects.

14

A Appendix

A.1

JustiÔ¨Åcations for OfÔ¨Çine RL in Dialogue

Dialogue tasks are one of the most rich and interactive settings in NLP, and as we will argue, these
properties make it an ideal target for applying ofÔ¨Çine RL. RL in general presents an elegant and
highly desirable utility optimization framework for sequential decision making settings, such as
dialogue. However, solving realistic interactive tasks with online RL requires either repeated real-
world interaction or building a realistic simulator of the environment. In the case of dialogue, such
online interaction means communicating with real humans, which may be impractically expensive
and time-consuming with contemporary sample-inefÔ¨Åcient online RL methods [58, 59], and building
a realistic simulator of human responses may be largely intractable in sufÔ¨Åciently rich or complex
dialogue settings. OfÔ¨Çine RL, on the other hand, avoids both of these heavy requirements, by, just as
many recent breakthroughs in the Ô¨Åeld of NLP [2, 60, 61], operating purely on previously collected
data, which is wildly available on the internet in general. OfÔ¨Çine-RL therefore presents an ideal
approach for Ô¨Çexibly steering language models towards the successful completion of dialogue tasks
in a way that effectively leverages existing data, just as supervised learning does.

A.2 Full Token-Level POMDP Formulation

We expand on the POMDP deÔ¨Ånition presented in Section 3. In order to apply RL to interactive
language settings, we need to formalize dialogue generation and other NLP tasks as a partially
observable Markov decision processes (POMDP). We deÔ¨Åne the POMDP M at the token level with
M = (S, A, O, T , Z, ¬µ0, R, Œ≥). We deÔ¨Åne the agent‚Äôs observation ht ‚àà O to be a history of tokens
with ht = {t0, t1, t2, t3, ...tt‚àí1}; the action space at = tt ‚àà A is deÔ¨Åned to be the set of possible
next-tokens in our vocabulary, which includes the special end-of-turn token aend. The agent‚Äôs policy
then corresponds to a mapping œÄ : O ‚Üí P(A). Many tasks such as dialogue have an underlying state
st that goes beyond just the sequence history, which can encompass things like the speaker‚Äôs mental
state. The environment transitions T (¬∑|st, at) are deÔ¨Åned as a function of this st. In particular, in
domains, such as dialogue, the dynamics are trivial within the agent‚Äôs utterance (the selected token is
deterministically appended to the history), but when the policy outputs a special ‚Äúend of turn‚Äù token,
the other speaker gets a turn, which is subsequently appended to the history. In other tasks, where
the goal is to generate a single utterance, such as generating a summary or a single Reddit comment,
the episode ends when the policy produces the end token. The agent receives a reward, deÔ¨Åned by
R(st, at) ‚Üí R, after each action taken. However, in all the settings we consider, the agent receives
non-zero reward rt only after producing an ‚Äúend of turn‚Äù token, rather than densely at every token in
the agent‚Äôs utterances.
While some prior works [11, 30] have considered actions at the utterance level, deÔ¨Åning decision
processes at the token level can yield a more effective search over the exponentially large utterance
action space, simply by selecting tokens with high estimated values. Typically, searching over a
per-utterance action space requires a Monte Carlo process of sampling multiple full utterances and
then re-ranking with estimated values, which can generally bring additional computational complexity
at both training and inference time. In Section 6.3, we demonstrated the effectiveness of learning at
the token level through an ablation study that compares with learning at the utterance level.

A.3 General Experiment Details

Here we outline architecture and hyper-parameter details of all our models and baselines.

ILQL experiment details. We run all of our experiments on GPT-2 small transformer architectures,
with the supervised learning policy on one transformer and Q and V heads on a separate one. The
target Q network is also on a separate transformer. In all our experiments we initialize with GPT-2
pre-trained weights, except in the case of Wordle, where we initialize randomly. Additionally, Wordle
uses a different token set: the set of 26 characters, plus an additional token for each ‚Äúcolor‚Äù. We train
all RL baselines with double-Q learning, using two separate heads on the same transformer model as
the two Q-functions. Our target Q networks are Polyak-averaged with decay factor 0.005 for both
the transformer and the Q function head. We use Œ≥ = 0.99 for all ofÔ¨Çine-RL experiments. All value
function heads are two layer MLPs with hidden dimension twice that of the transformer‚Äôs embedding
dimension. Our MLPs used ReLU non-linearities and no dropout. We used a learning rate of 1e-4
on the Reddit and Visual Dialogue tasks and 1e-5 on the Wordle task. We used no weight decay in
training any of our models, and we used a dropout of 0.1 inside the transformer (i.e. the same dropout
setting that was used for pretraining GPT2). We trained all Wordle models with a batch size of 1024,

15

all Visual Dialogue models with a batch size of 64, and all Reddit models with a batch size of 32.
We always truncate token sequences to length 1024, except on Reddit tasks, in which we truncate to
length 512.
Except on the Reddit comment task, we train ILQL on each of œÑ = {0.7, 0.8, 0.9}, and we also
evaluate each on Œ≤ = {4, 8, 16}. On the Reddit comment tasks, we only train with œÑ = 0.6 and
evaluate on Œ≤ = {1, 2, 4, 8, 16, 32}. On all tasks, we report the setting with the greatest performance.
For the NLL (CQL) loss term applied to ILQL, we used a weight Œ± of 1.0 on all VisualDialogue
experiments, 0.25 on all Reddit Comment experiments, and 0.0001 on Wordle. These values were
tuned by hand. Generally, we Ô¨Ånd this loss parameter to not be too critical to performance; we tune it
a little at Ô¨Årst for each task and then don‚Äôt worry about it.
All ILQL models and all baselines were trained on a single GPU until convergence. Training never
exceeded three days.

Evaluation details. During evaluation, we use greedy decoding to generate utterances on all tasks
and baselines, except the Reddit Comments tasks, where we sample instead. All experiments are
evaluated on 1024 task-instances from an unseen evaluation set. We use our BC baseline model as
œÄŒ≤ for guiding ILQL‚Äôs perturbation-based policy extraction.

BC baselines. We train BC baselines with the same optimization parameters (i.e., weight decay,
dropout, learning rate, batch size) and initialization as ILQL. We use early stopping: when the
validation loss exceeds the training loss, we stop training. Unlike our ILQL value function models, we
use a linear head on top of the transformer to parameterize our BC policy, as is standard for language
model Ô¨Ånetuning. The only difference between our BC and standard language model training is that
instead of Ô¨Ånetuning the model to predict the whole sequence of states and actions, we only Ô¨Ånetune
the model to predict the agent‚Äôs own actions or utterances.

%BC baselines. For %BC baselines we report the performance of the best model out of several
different percentages: {10%, 30%, 50%} for Wordle, {10%, 20%, 30%} for Visual Dialogue, and for
Reddit, since our rewards are discrete, we deÔ¨Åne %BC to mean just training on the data-points with
the maximum reward label. We use the same hyper-parameters as our BC baselines for training these
models.

Decision transformer baseline. Our decision transformer baseline follows from Chen et
al. [8], except we initialize with pretrained GPT2 weights. All hyperparameters are identi-
cal to those used to train our BC baselines. To evaluate decision transformer on our Vi-
sual Dialogue ‚Äúy/n‚Äù reward, we swept over a broad range of conditional reward-to-go values:
{‚àí11, ‚àí10, ‚àí9, ‚àí8, ‚àí7, ‚àí6, ‚àí5, ‚àí4, ‚àí3, ‚àí2, ‚àí1, 0}. We report the setting with the best perfor-
mance.

SARSA baselines. SARSA baselines are implemented as ILQL with œÑ = 0.5 and all other hyper-
parameters are identical to those used with ILQL as described above. Except on the Reddit comment
tasks, we evaluate all SARSA models on Œ≤ = {4, 8, 16}, and report the setting with the greatest per-
formance. On the Reddit comment tasks, we show the best performance from Œ≤ = {1, 2, 4, 8, 16, 32}.

Per-utterance ILQL. For ‚ÄúILQL (utterance)‚Äù, we train models with œÑ = {0.7, 0.8, 0.9}. We
also evaluate each model with number of EMAQ-style [29] samples chosen from N={4, 8, 16}.
We report the setting with the best task performance. ‚ÄúSARSA (utterance)‚Äù is a special case of
‚ÄúILQL (utterance)‚Äù with œÑ = 0.5, which we evaluate on each of N={4, 8, 16} and report the setting
with the best performance. The architecture for ‚ÄúILQL (utterance)‚Äù and ‚ÄúSARSA (utterance)‚Äù is
largely identical to that of per-token ILQL, with the main difference being that Bellman backups are
performed at the utterance level instead of the token-level. As a result of this difference, Q-heads
map to a scalar at the end of an utterance instead of a vector at every token with length equal to the
size of the vocabulary.

CHAI baseline. Our CHAI baseline is adopted from Verma et al. [11]. The tasks we consider
only require utterance actions; no axuiliary actions, like the price proposal action required by that of
Verma et al. [11]‚Äôs bargaining task. We therefore only adopt the components from CHAI relevant
to utterance level actions. In order to compute CHAI‚Äôs CQL loss at the utterance level, we need to
sample counterfactual utterances for each action in the training data, which can be highly expensive
and can greatly slow training. Following Verma et al. [11], we amortize this cost at the risk of inducing
some bias by caching 5 counterfactual samples for each action in the training data as a preprocessing
step. In our case, this preprocessing step took over 30 hours to execute on a V-100 GPU for the full

16

the Visual Dialogue training set. As in all our other experiments, we train two Q networks [48], where
our target Q value is parameterized as the minimum of both Polyak averaged target Q heads. We train
our CHAI models with a batch size of 16 and otherwise all other hyperparameters are identical to
those used with ILQL. We train models with CQL Œ± = {0.1, 1.0, 10.0}, and we evaluate each model
with the number of EMAQ-style [29] samples chosen from N={4, 8, 16}. We report the setting with
the best performance.
Per-token dynamic programming baselines. For our per-token CQL and œà-learning baselines in
Table 10, we tuned the CQL loss weight with Œ± = {0.1, 1.0, 10.0}, and the œà-learning reward scale
with c = {0.1, 1.0, 10.0}. For each baseline agent, we evaluated using ILQL‚Äôs policy extraction with
Œ≤ = {4, 8, 16} and also evaluated by greedily selecting tokens with the Q function by itself. We
report the setting with the best performance for each baseline.
Our implementation of per-token CQL is identical to ILQL with the only exception being that for
per-token CQL the loss function is deÔ¨Åned as:

LQ,V (Œ∏) = EœÑ ‚àºD

(cid:34) T

(cid:88)

i=0

(R(hi, ai) + Œ≥ max
at+1‚ààA

QÀÜŒ∏(hi+1, at+1) ‚àí QŒ∏(hi, ai))2

(cid:35)

Our implementation of œà-learning is adapted from Jaques et al. [12, 13] for use on transformer
language models [62, 60] instead of RNNs [63]. The architecture is identical to that of ILQL, the
main difference is in the loss function:
Ô£Æ

LQ,V (Œ∏) = EœÑ ‚àºD

Ô£∞

LŒ¥(

T
(cid:88)

i=0

R(hi, ai)
c

+ log(œÄŒ≤(hi, ai)) + Œ≥ log(

(cid:88)

exp QÀÜŒ∏(hi+1, at+1)) ‚àí QŒ∏(hi, ai))

Ô£ª

at+1‚ààA

Ô£π

Where œÄŒ≤ is our BC baseline model: a transformer language model trained with supervised learning.
And LŒ≥ deÔ¨Ånes the Huber loss; we use Œ≥ = 1 in our experiments.
In both baselines, we also Ô¨Åt a value function head to the mean of the Q functions, as in ILQL
with œÑ = 0.5. Additionally, for both baselines, aside from the parameters mentioned, all other
parameters are identical to those used with ILQL, as described above. The only exception being that
for œà-learning, we used a learning rate of 1e-5 instead of 1e-4 due to training instability with the
higher learning rate.
We generally found œà-learning to be highly unstable to train in our experiments, often producing
incomprehensible outputs. It is possible that the baseline could work better with even more careful
tuning.

AWR extraction abalation details. For our ‚ÄúILQL (AWR)‚Äù ablation, we extracted a policy with
AWR extraction using the best performing ILQL value function out of those trained with œÑ =
{0.7, 0.8, 0.9}. We performed AWR extraction from this value function using 3 different settings
for beta: Œ≤ = {4, 8, 16}. Using the same value function, we performed ILQL extraction with
œÑ = {4, 8, 16}. For each, we reported the setting with the best performance.

A.4 Wordle Task Details

Environment details. Our agents observe the game-state as a history of alternating sequences of
5 letter tokens followed by 5 color tokens. Unlike the actual Wordle game, we do not prevent the
agent from generating words that aren‚Äôt in the vocabulary (i.e., the agent is free to produce any
sequence of 5 letters). For our synthetic experiments, we chose to use the full word list given at
https://gist.github.com/cfreshman/a03ef2cba789d8cf00c08f767e0fad7bdue to its relatively large size
and its use in the actual Wordle game. However, the environment can be conÔ¨Ågured with any provided
list of 5 letter words.

Wordle Twitter dataset details. We outline the details of our natural Wordle dataset scraped from
Twitter, introduced in Section 5. Due to Wordle‚Äôs popularity, we have access to a large amount
of natural human data for Wordle (speciÔ¨Åcally 214,930 games), scraped from tweets 2. Existing
ofÔ¨Çine RL benchmarks are composed of purely synthetic data [64], and as a result, it may be unclear
how well ofÔ¨Çine RL algorithms work on more natural data distributions. We therefore present

2we use the wordle tweet dataset provided here: https://www.kaggle.com/code/benhamner/wordle-1-

6/notebook

17

this human Wordle dataset as a more naturalistic ofÔ¨Çine RL task. While the scraped Tweets don‚Äôt
display the actual words used by human players, only the sequence of transition colors given by the
environment, we can retroÔ¨Åt valid words onto these tweets to produce a dataset of full trajectories.
Note that the words we retroÔ¨Åt may not necessarily be the natural words human players would have
used, but the dataset still represents the average performance of human players, since the number
of turns remains unchanged in this retroÔ¨Åtting process. Additionally, this retroÔ¨Åtting allows us to
further multiply the size of the dataset, since typically several different sequences of words can be
valid for a given tweet. We can also partially control the difÔ¨Åculty level of the task and dataset by
specifying the size and composition of the vocabulary used to retroÔ¨Åt words onto Tweets. In our
Wordle human experiments in Table 5, we use a random subset of 200 words from the word list given
at https://gist.github.com/cfreshman/a03ef2cba789d8cf00c08f767e0fad7b.

A.5 Visual Dialogue Task Details

Here we detail the task setup and reward functions used in our Visual Dialogue experiments in
Section 6.1. We use Das et al.‚Äôs code 3 to produce generations and to predict image embeddings
from the provided supervised learning answer and question bots, respectively. We integrate these
components of Das et al.‚Äôs codebase into ours by wrapping the relevant functionality of Das et al.‚Äôs
codebase in a Ô¨Çask webserver interface that is then queried by our system.
As described in Section 6.1, our ‚Äústandard‚Äù reward function gives a reward of -1 for each turn in
which the true image is sufÔ¨Åciently difÔ¨Åcult to predict from the dialogue, otherwise the agent receives
a reward of 0 and the environment interaction ends. We Ô¨Årstly formalize this notion of ‚ÄúsufÔ¨Åciently
difÔ¨Åcult to predict‚Äù.
The standard reward is based on the relative percentile ranking of the ground truth image‚Äôs distance
from the predicted embedding among a set of images taken from the evaluation set. We give a -1
reward to our agent for every turn in which (1 ‚àí pt) < (1 ‚àí p0) ‚àó 0.5, where pt is the ground truth
image‚Äôs percentile rank at dialogue turn t and p0 is the ground truth image‚Äôs percentile rank at the
beginning of the dialogue, when only the image caption is observed. Otherwise, the agent gets a
reward of 0 and the episode ends. This condition effectively rewards the agent once the ground truth
image is preferred over 50% of the images that were preferred over it at initialization. The agent
should learn to ask as many good questions as possible to get the episode to successfully end as early
as possible. In initial experiments, we found it took a very long time to train good value functions
for rewards based on the absolute Euclidean distance alone, as used by Das et al. [55], so to make it
faster to iterate, we used the relative distance formulation described above.
Our ‚Äúy/n‚Äù reward adds, on top of the ‚Äústandard‚Äù reward, a reward of -2 for every question that results
in a response that exactly matches the strings ‚Äúyes‚Äù or ‚Äúno‚Äù.
Our ‚Äúconservative y/n‚Äù reward instead aims to provide a more conservative, higher-recall lower-
precision penalty to any question which might be a yes/no question. This accounts for the fact
that people often answer yes/no questions with longer phrases (e.g., ‚ÄúIt appears so‚Äù). This reward
function provides a reward of -2 if any of the following words are sub-strings of the response to the
agent‚Äôs question: ‚Äúnot‚Äù, ‚Äúdon‚Äôt‚Äù, ‚Äúcan‚Äôt‚Äù, ‚Äúcannot‚Äù, ‚Äúfairly‚Äù, ‚Äúcould‚Äù, ‚Äúthink so‚Äù, ‚Äúokay‚Äù, ‚Äúmaybe‚Äù,
‚Äúyes‚Äù, ‚Äúno‚Äù, ‚Äúlooks‚Äù, ‚Äúappears‚Äù, ‚Äútell‚Äù, ‚Äúmostly just‚Äù. All of these were determined by hand to be
words/phrases that occur often in answers to questions that are effectively yes/no questions.

A.6 Reddit Reward Model Details

We outline the details of our reward functions for the Reddit tasks presented in Section 6.2.
Toxicity Reward. Our toxicity Ô¨Ålter reward uses OpenAI‚Äôs API 4, which provides a free toxicity
Ô¨Ålter, meant for developers building applications off the GPT3 API to use to block toxic inputs or
generations. We assign a reward of -10 for comments labeled as toxic (scored as 2), -5 for comments
labeled as moderately toxic (scored as 1), and 0 for comments labeled as non-toxic (scored as 0).

Upvote Model Reward. Our upvote reward function is Ô¨Ånetuned from RoBERTa-base [65] with
a learning rate of 1e-5 and a batch size of 64. Since our reward functions are binary, we train with
binary cross entropy loss. Like our value function heads in ILQL, we predict the reward as a scalar
from a 2-layer MLP on top of the RoBERTa transformer, with hidden dimension twice that of the
transformer, ReLU non-linearity, and no dropout. We truncate token sequences to maximum length

3https://github.com/batra-mlp-lab/visdial-rl
4https://openai.com/api/

18

toxicity
0.0¬±0.0
-5.00¬±0.00
-5.00¬±0.00
Table 3: Evaluating each Reddit ILQL agent on all other reward functions. Agents trained on one
reward function are less optimal on rewards for which they were not trained.

upvotes model
9.07¬±0.09
9.83¬±0.04
10.0¬±0.0

train/eval
toxicity
upvotes gold
upvotes model

method
ILQL
SARSA
%BC
BC

toxicity
0.0¬±0.0
0.0¬±0.0
-0.74¬±0.07
-3.51¬±0.13

noised toxicity
0.0¬±0.0
0.0¬±0.0
-1.61¬±0.11
-3.48¬±0.15

upvotes real
9.83¬±0.04
6.23¬±0.15
7.06¬±0.14
4.87¬±0.16

upvotes model
10.0¬±0.0
10.0¬±0.0
7.86¬±0.13
4.87¬±0.16

Table 4: A comparison of ILQL against baselines on the various Reddit comments reward functions. ILQL
manages to never generate undesirable comments on 3 out of 4 reward functions, whereas Ô¨Åne-tuning on Ô¨Åltered
data occasionally does.

256. At inference time, we predict a reward of +10 if the model‚Äôs reward logit is ‚â• 0 and a reward of
0 otherwise. We used binary (positive or negative) rewards for upvotes instead of the more natural
cardinal numeric representation, because different sub-reddits can have drastically different upvote
counts depending on the sub-reddit‚Äôs population, and our binarization (positive or negative upvotes)
is invariant to these differences in scale. However, this binarization is not the only normalization that
we could have used to overcome this issue.

A.7 Evaluating Reddit Agents on Different Rewards

To complement Table 1 right, comparing Visual Dialogue agents on all Visual Dialogue reward
functions, we also present Table 3, showing how different Reddit agents perform when evaluated
on different rewards. Here we Ô¨Ånd the same result, that agents trained on one reward function are
generally less optimal for others, further conÔ¨Årming ILQL as an effective utility optimizer on natural
language tasks.

A.8 Noisy Rewards

In Ô¨Ågure 11, we present a more detailed visual explanation for why ofÔ¨Çine RL outperforms Ô¨Åltered
Ô¨Ånetuning on our Reddit tasks in section 6.2. As our results in Table 8 show, ofÔ¨Çine RL consistently
outperforms Ô¨Ånetuning on Ô¨Åltered data on this task. We hypothesize that this is due to ofÔ¨Çine RL‚Äôs
ability to effectively reason about the inherently stochastic and subjective rewards functions present in
these tasks. In these high-variance reward settings, simply Ô¨Åltering or curating datasets for exclusively
high-reward examples can fail to produce desirable outputs, since such Ô¨Åltered Ô¨Ånetuning approaches
do not make the model aware of the reward uncertainty. Put another way, training on curated datasets
that exclude low-reward examples doesn‚Äôt teach the model about what not to generate, whereas
models that are aware of the reward, such as Q-learning, directly learn to relate actions to their
expected reward values, averaging out uncertainty and stochasticity. This can enable models to avoid
outputs that have low but non-trivial probability of undesirable outcomes (e.g., toxicity). OfÔ¨Çine RL
in this sense is able to Ô¨Ånd the safest outputs, whereas Ô¨Åne-tuning on Ô¨Åltered data does not explicitly
express such a preference.
To further test this hypothesis, we artiÔ¨Åcially add further noise to the toxicity reward function. The
standard toxicity reward assigns all comments one of three reward values: 0, indicating the comment
is non-toxic; -5, indicating the comment is moderately toxic; -10, indicating the comment is highly
toxic. We now relabel the reward for all comments originally given a -5 reward, randomly to either 0
or 10 with equal probability.
Since some of the moderately toxic comments get relabeled with reward=0, they would be included in
the %BC training set, whereas ofÔ¨Çine RL should learn to represent uncertainty about such comments
and thus push away from the stochastic ‚Äúmiddle ground‚Äù of this reward function. In Table A.8 in
the ‚Äúnoised toxicity‚Äù column, we see that our ofÔ¨Çine RL agents learn to never generate toxic outputs
despite the additional noise, and in Figure 9 we can see qualitatively that ofÔ¨Çine RL assigns low
advantages to potentially negative or toxic words/phrases that were incidentally generated by the
%BC model. All of this goes to support our hypothesis that the advantage of ofÔ¨Çine RL over Ô¨Åltered
supervised learning in these settings lies in its improved ability to handle reward uncertainty.

19

Figure 11: A visual explanation of ofÔ¨Çine RL‚Äôs ability to optimize high variance reward functions
based on subjective judgement, such as whether to label a comment as an example of toxic speech or
not. Finetuning on Ô¨Åltered data accidentally generalizes into producing undesirable outputs, whereas
ofÔ¨Çine RL is able to Ô¨Ånd the ‚Äúsafe‚Äù outputs. Top: In the case of stochastic rewards, ofÔ¨Çine RL learns
to avoid the highly stochastic regions of the action space, whereas Ô¨Åltered Ô¨Ånetuning will explicitly
learn to imitate undesirable outputs that were stochastically given a positive reward in the training
data, thus leading to suboptimal behavior. Right: In the case of non-stochastic but sharp-boundary
reward functions, ILQL is still able to integrate into its Q values uncertainty about actions near the
sharper parts of the reward function‚Äôs decision boundary, thus avoiding these regions. Finetuning on
Ô¨Åltered data expresses no such preference and thus risks generalizing into occasionally producing
undesirable outputs.

A.9 Trading Off Output Diversity for Optimization

An advantage of our novel policy extraction mechanism is that we can Ô¨Çexibly tune the parameter Œ≤
at inference time, directly trading off between random generation and optimality. As discussed in
Section 12, this parameter controls a constraint on our policy‚Äôs deviation from the data distribution.
As we increase Œ≤, the resulting policy will be more strongly inÔ¨Çuenced by the Q-function, and as we
decrease Œ≤, it should approach the data distribution. Beyond the risk in diverging too far from the
data, another potential downside to increasing Œ≤ is that the resulting policy distribution will become
more deterministic. In some settings, such as chit-chat dialogue, we may desire policies capable of
producing diverse and interesting outputs, so such a deterministic, highly-optimized agent would be
undesirable.
We demonstrate in Figure 12, using the Reddit Toxicity task, how varying Œ≤ can modulate the diversity
of the generations produced by our policy, as measured by its entropy, at the cost of a small decrease
in performance. We show that as we increase Œ≤, while the policy‚Äôs performance generally increases,
the entropy decreases and subsequently so does the interestingness and diversity of the language
model‚Äôs outputs. At inference time, we can tune this parameter to trade-off optimization for output
diversity as we desire.

20

Offline-RL PolicyNon-stochastic RewardStochastic RewardFiltered Finetuning PolicyOffline-RL PolicyFiltered Finetuning PolicyKey:=  expected reward decision boundary=  comment labeled as non-toxic=  comment labeled as toxic=  policy probability densityKey:=  reward decision boundary= non-toxic comment=  toxic comment=  policy probability densityFigure 12: By varying Œ≤ at inference time, we can interpolate the trade-off between ofÔ¨Çine RL
"optimization" and output diversity.

A.10 Visual Dialogue Example Dialogues and Histogram

In Figures 13, 14,and 15 we present a set of selected representative example dialogues produced by
our best performing ILQL agent on each reward. In Figure 16 we present a histogram showing often
different agents generate yes/no questions as judged by their respective ‚Äúy/n‚Äù or ‚Äúy/n conservative‚Äù
reward functions.

A.11 Comprehensive Result Tables

In Tables 5, 6,and 7 we present comprehensive results for all hyper-parameter settings for all baselines
on all tasks.

21

comment: yescomment: lolcomment: editcomment: http://i.imgur.com/e2O.gifcomment: I and all of you know me...comment: Yep. The Oatmeal on the right is trying to deny [this](http://imgur.com/Ojc4M)comment: it's not the main menu, it's the fridgecomment: I did what Idra did. Deep down they told me, I said "I'm not cool" and all I could do was leave.comment: All I see in its place is a frame-by-frame summary of its most relevant comments. So having no thought whatsoever of this, I'm happy.Figure 13: Example dialogues produced by the best performing ILQL agent on the Visual Dialogue
‚Äústandard‚Äù reward.

22

Image Caption: A man, with other people in the background, is sitting at a picnic table with a cell phone in front of him while he uses a fork and knife to cut a dessert on a plate.Questioner: is man young?Answerer: yesQuestioner: is he wearing glasses?Answerer: noQuestioner: is there food on table?Answerer: yesQuestioner: is there food on table?Answerer: yesImage Caption: A man with a wide eyed look with an apron on in a kitchen.Questioner: is man young?Answerer: i would say he isQuestioner: what color is man's hair?Answerer: brownQuestioner: is he wearing glasses?Answerer: noQuestioner: is he wearing apron?Answerer: i can't tellQuestioner: is he wearing apron?Answerer: i can't tellQuestioner: is there anything on his head?Answerer: yes, it's UNK and UNKQuestioner: is he looking at something?Answerer: he is looking at cameraQuestioner: is there anything else on counter?Answerer: yes, there are some papers and papersQuestioner: is there anything on counter?Answerer: yes, there are some papers and papersImage Caption: Young adults with surfboards standing on pathway near beach.Questioner: how many people are there?Answerer: 2Questioner: are they male or female?Answerer: 1 male and 1 femaleQuestioner: are they wearing wetsuits?Answerer: yesQuestioner: what color are they?Answerer: 1 is black and other is whiteQuestioner: are they wearing swimsuits?Answerer: yes they areQuestioner: are they wearing wetsuits?Answerer: yes they areQuestioner: are they wearing sunglasses?Answerer: no they are notQuestioner: are there any other people around?Answerer: no there are notQuestioner: are there any animals?Answerer: no animalsQuestioner: is it sunny?Answerer: yes it isImage Caption: Colorful vehicles in roadway near large building in city.Questioner: how many cars are there?Answerer: i see 3Questioner: are there any people?Answerer: yes, there are peopleQuestioner: are there any cars parked?Answerer: yes, there are carsexample standard reward Visual DialoguesFigure 14: Example dialogues produced by the best performing ILQL agent on the Visual Dialogue
‚Äúy/n‚Äù reward.

23

Image Caption: Yellow and blue passenger train rounding a curve.Questioner: how many cars on train?Answerer: i can only see 1Questioner: is it day or night?Answerer: dayQuestioner: is it sunny or cloudy?Answerer: sunnyQuestioner: is there anything else on tracks?Answerer: no, just birdsQuestioner: how many birds are there?Answerer: 2Questioner: what color are birds?Answerer: blackQuestioner: what color is sky?Answerer: blueQuestioner: what time of day is it?Answerer: it is daytimeQuestioner: what time of year is it?Answerer: i can't tellQuestioner: what else can you see?Answerer: trees, sky, sky, skyImage Caption: A person flying a kite over a sand dune.Questioner: what color is kite?Answerer: it's black and whiteQuestioner: is person male or female?Answerer: can't tellQuestioner: what is person wearing?Answerer: i can't tellQuestioner: what color is sand?Answerer: dark brownQuestioner: is person flying kite flying in air?Answerer: yesQuestioner: what color is kite?Answerer: blackImage Caption: Little girl reaching through fence to pet a giraffe.Questioner: how old is girl?Answerer: maybe 3Questioner: what color is her hair?Answerer: dark brownQuestioner: what is she wearing?Answerer: i can only see her from waist upQuestioner: what color is her hair?Answerer: dark brownQuestioner: what is her eye color?Answerer: i can't tell, it's covered with white and black and redQuestioner: what is fence made of?Answerer: woodQuestioner: what color is fence?Answerer: it's metal fence with black metal fenceQuestioner: is giraffe full grown?Answerer: yesImage Caption: a number of people in a field with a kite in the airQuestioner: how many people are there?Answerer: i see 4Questioner: what color is kite?Answerer: it's hard to tell, but it looks blackQuestioner: are they male or female?Answerer: both are maleQuestioner: what are they wearing?Answerer: they are wearing shorts and t shirtsQuestioner: what color are shorts?Answerer: blackexample y/n reward Visual DialoguesFigure 15: Example dialogues produced by the best performing ILQL agent on the Visual Dialogue
‚Äúconservative y/n‚Äù reward.

24

Image Caption: Tour buses are lined up on the street waiting for people.Questioner: how many buses?Answerer: 2Questioner: what color are buses?Answerer: white and redQuestioner: how many people?Answerer: 2Questioner: what gender are people?Answerer: 1 is male and 1 is femaleQuestioner: what are they wearing?Answerer: 1 is wearing shorts and other is wearing shorts and shirtQuestioner: what color is their hair?Answerer: dark brownQuestioner: what color is their shirt?Answerer: blackQuestioner: what color is their pants?Answerer: blackQuestioner: what color is their shirt?Answerer: blackQuestioner: what color is their pants?Answerer: blackImage Caption: a living room with a table couch guitar and some picturesQuestioner: what color is couch?Answerer: brownQuestioner: what kind of pictures are on couch?Answerer: 1 is UNK other is not sureQuestioner: what color is guitar?Answerer: blackQuestioner: what color is table?Answerer: brownQuestioner: what color is table?Answerer: brownQuestioner: what color is guitar?Answerer: blackQuestioner: what color are walls?Answerer: whiteImage Caption: Little girl reaching through fence to pet a giraffe.Questioner: what is little girl wearing?Answerer: she is wearing long sleeved shirt and jeansQuestioner: what color is fence?Answerer: it's metal fence with black metal fenceQuestioner: what is giraffe doing?Answerer: it is just standing thereImage Caption: Pictures of living room, dining room, kitchen and bedroom.Questioner: how many people are there?Answerer: 2Questioner: what are they doing?Answerer: 1 is on top of other 1 is on other side of fridgeQuestioner: what color is fridge?Answerer: silverexample conservative y/n reward Visual DialoguesFigure 16: Top: Histogram of the fraction of yes/no questions asked per dialogue by Visual Dialogue
agents trained on the ‚Äúy/n‚Äù reward. Here yes/no questions are determined by the same exact match
heuristic used by the ‚Äúy/n‚Äù reward. ILQL agents ask fewer questions triggered as being yes/no than
baselines. The best performing agent is used for all methods. Bottom: Histogram of the fraction
of yes/no questions asked per dialogue by Visual Dialogue agents trained on the ‚Äúconservative
y/n‚Äù reward. Here yes/no questions are determined by the same exact match heuristic used by the
‚Äúconservative y/n‚Äù reward. ILQL agents ask fewer questions triggered as being yes/no than baselines.
The best performing agent is used for all methods.

25

model
ILQL œÑ = 0.7, Œ≤ = 4
ILQL œÑ = 0.7, Œ≤ = 8
ILQL œÑ = 0.7, Œ≤ = 16
ILQL œÑ = 0.8, Œ≤ = 4
ILQL œÑ = 0.8, Œ≤ = 8
ILQL œÑ = 0.8, Œ≤ = 16
ILQL œÑ = 0.9, Œ≤ = 4
ILQL œÑ = 0.9, Œ≤ = 8
ILQL œÑ = 0.9, Œ≤ = 16
SARSA Œ≤ = 4
SARSA Œ≤ = 8
SARSA Œ≤ = 16
10%BC
20%BC
30%BC
BC
CQL Œ± = 0.1, Œ≤ = 4
CQL Œ± = 0.1, Œ≤ = 8
CQL Œ± = 0.1, Œ≤ = 16
CQL Œ± = 0.1, Œ≤ = ‚àû
CQL Œ± = 1.0, Œ≤ = 4
CQL Œ± = 1.0, Œ≤ = 8
CQL Œ± = 1.0, Œ≤ = 16
CQL Œ± = 1.0, Œ≤ = ‚àû
CQL Œ± = 10.0, Œ≤ = 4
CQL Œ± = 10.0, Œ≤ = 8
CQL Œ± = 10.0, Œ≤ = 16
CQL Œ± = 10.0, Œ≤ = ‚àû
œà c = 0.1, Œ≤ = 4
œà c = 0.1, Œ≤ = 8
œà c = 0.1, Œ≤ = 16
œà c = 0.1, Œ≤ = ‚àû
œà c = 1.0, Œ≤ = 4
œà c = 1.0, Œ≤ = 8
œà c = 1.0, Œ≤ = 16
œà c = 1.0, Œ≤ = ‚àû
œà c = 10.0, Œ≤ = 4
œà c = 10.0, Œ≤ = 8
œà c = 10.0, Œ≤ = 16
œà c = 10.0, Œ≤ = ‚àû
DT R = ‚àí11
DT R = ‚àí10
DT R = ‚àí9
DT R = ‚àí8
DT R = ‚àí7
DT R = ‚àí6
DT R = ‚àí5
DT R = ‚àí4
DT R = ‚àí3
DT R = ‚àí2
DT R = ‚àí1
DT R = 0
ILQL (AWR) œÑ = 0.7, Œ≤ = 4
ILQL (AWR) œÑ = 0.7, Œ≤ = 8
ILQL (AWR) œÑ = 0.7, Œ≤ = 16
ILQL (utterance) œÑ = 0.7, N = 4
ILQL (utterance) œÑ = 0.7, N = 8
ILQL (utterance) œÑ = 0.7, N = 16
ILQL (utterance) œÑ = 0.8, N = 4
ILQL (utterance) œÑ = 0.8, N = 8
ILQL (utterance) œÑ = 0.8, N = 16
ILQL (utterance) œÑ = 0.9, N = 4
ILQL (utterance) œÑ = 0.9, N = 8
ILQL (utterance) œÑ = 0.9, N = 16
SARSA (utterance) N = 4
SARSA (utterance) N = 8
SARSA (utterance) N = 16
CHAI Œ± = 0.1, N = 4
CHAI Œ± = 0.1, N = 8
CHAI Œ± = 0.1, N = 16
CHAI Œ± = 1.0, N = 4
CHAI Œ± = 1.0, N = 8
CHAI Œ± = 1.0, N = 16
CHAI Œ± = 10.0, N = 4
CHAI Œ± = 10.0, N = 8
CHAI Œ± = 10.0, N = 16

standard
-5.23¬±0.13
-5.22¬±0.13
-5.28¬±0.13
-5.30¬±0.12
-5.40¬±0.13
-5.38¬±0.13
-5.35¬±0.13
-5.40¬±0.13
-5.45¬±0.13
-5.20¬±0.13
-5.14¬±0.13
-5.18¬±0.13
-5.24¬±0.12
-5.07¬±0.13
-5.16¬±0.12
-5.25¬±0.13
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
Table 5: All hyper-parameter settings evaluated across all Visual Dialogue tasks, abalations, and
baselines. The best performing setting for each baseline of abalation is bolded. Œ≤ = ‚àû refers to
greedily selecting actions with just the Q function. ILQL is generally better performing and more
stable than all baseline approaches.

conservative y/n
-7.64¬±0.21
-7.05¬±0.19
-6.77¬±0.18
-7.97¬±0.21
-6.85¬±0.18
-6.65¬±0.18
-7.41¬±0.20
-6.72¬±0.18
-6.57¬±0.18
-9.12¬±0.24
-8.09¬±0.21
-7.77¬±0.20
-9.67¬±0.26
-9.13¬±0.22
-10.52¬±0.25
-15.16¬±0.35
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-

y/n
-6.65¬±0.18
-5.92¬±0.15
-5.69¬±0.13
-6.88¬±0.18
-6.28¬±0.16
-5.99¬±0.15
-7.01¬±0.19
-6.35¬±0.16
-6.17¬±0.15
-6.87¬±0.18
-6.39¬±0.16
-6.19¬±0.15
-7.48¬±0.21
-8.91¬±0.24
-9.10¬±0.22
-10.85¬±0.27
-10.08¬±0.21
-10.97¬±0.21
-12.92¬±0.23
-10.74¬±0.17
-7.84¬±0.20
-7.53¬±0.19
-7.45¬±0.18
-7.32¬±0.17
-11.76¬±0.29
-11.81¬±0.30
-11.84¬±0.30
-11.81¬±0.30
-11.59¬±0.18
-11.26¬±0.17
-11.42¬±0.17
-11.51¬±0.16
-10.05¬±0.18
-10.05¬±0.18
-10.05¬±0.18
-10.05¬±0.18
-11.35¬±0.18
-10.99¬±0.17
-10.95¬±0.17
-10.78¬±0.17
-10.57 ¬± 0.24
-10.58 ¬± 0.24
-10.53 ¬± 0.24
-10.49 ¬± 0.24
-10.40 ¬± 0.23
-10.42 ¬± 0.23
-10.30 ¬± 0.23
-10.30 ¬± 0.23
-9.83 ¬± 0.22
-9.54 ¬± 0.21
-8.15 ¬± 0.19
-6.70 ¬± 0.17
-5.96¬±0.13
-11.75¬±0.23
-12.11¬±0.22
-7.08¬±0.17
-6.04¬±0.15
-5.89¬±0.14
-7.12¬±0.17
-6.15¬±0.15
-5.91¬±0.14
-7.01¬±0.17
-6.12¬±0.15
-5.93¬±0.14
-7.82¬±0.18
-7.39¬±0.17
-7.35¬±0.17
-7.18¬±0.18
-6.18¬±0.15
-5.62¬±0.13
-7.03¬±0.17
-5.87¬±0.14
-5.57¬±0.13
-8.43¬±0.18
-8.24¬±0.16
-8.28¬±0.15

26

noised toxicity
-1.89¬±0.12 / 260.34¬±10.35
-0.99¬±0.09 / 202.09¬±8.99
-0.53¬±0.07 / 156.85¬±7.80
-0.23¬±0.05 / 73.12¬±4.88
-0.03¬±0.02 / 22.05¬±1.51
0.00¬±0.00 / 9.24¬±0.16
-1.72¬±0.12 / 233.17¬±9.89
-1.06¬±0.10 / 187.36¬±8.82
-0.41¬±0.06 / 108.76¬±6.53
-0.13¬±0.03 / 41.61¬±3.52
0.00¬±0.0 / 1.09¬±0.38
0.00¬±0.00 / 0.00¬±0.00
-1.61¬±0.11 / 90.75¬±3.29
-3.48¬±0.15 / 126.54¬±5.26

method
ILQL œÑ = 0.6, Œ≤ = 1
ILQL œÑ = 0.6, Œ≤ = 2
ILQL œÑ = 0.6, Œ≤ = 4
ILQL œÑ = 0.6, Œ≤ = 8
ILQL œÑ = 0.6, Œ≤ = 16
ILQL œÑ = 0.6, Œ≤ = 32
SARSA Œ≤ = 1
SARSA Œ≤ = 2
SARSA Œ≤ = 4
SARSA Œ≤ = 8
SARSA Œ≤ = 16
SARSA Œ≤ = 32
%BC
BC
method
ILQL œÑ = 0.6, Œ≤ = 1
ILQL œÑ = 0.6, Œ≤ = 2
ILQL œÑ = 0.6, Œ≤ = 4
ILQL œÑ = 0.6, Œ≤ = 8
ILQL œÑ = 0.6, Œ≤ = 16
ILQL œÑ = 0.6, Œ≤ = 32
SARSA Œ≤ = 1
SARSA Œ≤ = 2
SARSA Œ≤ = 4
SARSA Œ≤ = 8
SARSA Œ≤ = 16
SARSA Œ≤ = 32
%BC
BC

toxicity
-2.15¬±0.10 / 289.97¬±10.92
-1.16¬±0.08 / 222.18¬±9.68
-0.47¬±0.05 / 151.46¬±7.86
-0.12¬±0.02 / 79.20¬±5.38
-0.01¬±0.01 / 15.46¬±1.76
0.00¬±0.00 / 2.00¬±0.35
-1.60¬±0.09 / 240.21 ¬± 10.10
-0.96¬±0.07 / 199.04¬±9.24
-0.34¬±0.04 / 111.13¬±6.76
-0.07¬±0.019 / 37.59¬±3.47
0.00¬±0.00 / 4.38¬±0.54
0.00¬±0.00 / 0.03¬±0.01
-0.74¬±0.07 / 80.84¬±3.19
-3.51¬±0.13 / 137.70¬±5.82
upvotes real
6.29¬±0.15 / 39.11¬±1.99
7.01¬±0.14 / 21.47¬±1.56
7.47¬±0.14 / 7.91¬±0.63
9.05¬±0.09 / 1.55¬±0.07
9.73¬±0.05 / 0.55¬±0.14
9.83¬±0.04 / 0.18¬±0.02
6.23¬±0.15 / 33.75¬±1.64
4.66¬±0.16 / 15.81¬±0.82
0.81¬±0.09 / 2.38¬±0.28
0.09¬±0.03 / 0.00¬±0.00
0.09¬±0.03 / 0.00¬±0.00
0.09¬±0.03 / 0.00¬±0.00
7.06¬±0.14 / 100.77¬±4.15
4.87¬±0.16 / 127.65¬±5.48
Table 6: All hyper-parameter settings evaluated across all Reddit Comment tasks, abalations, and
baselines. The best performing setting for each baseline of abalation is bolded. The left result in
each cell the the agent‚Äôs average reward and the right result is the estimated entropy of the agent‚Äôs
policy, measured in nats. As Œ≤ is turned up, performance increases, but the entropy (or diversity) of
the policy‚Äôs outputs decreases. ILQL generally learns better performing and more diverse policies
than baselines.

upvotes model
8.38¬±0.12 / 25.15¬±1.43
9.53¬±0.07 / 7.82¬±0.88
9.99¬±0.01 / 1.38¬±0.09
10.00¬±0.00 / 0.78¬±0.02
10.00¬±0.00 / 0.54¬±0.02
10.00¬±0.00 / 0.22¬±0.02
7.82¬±0.13 / 28.80¬±1.87
8.96¬±0.10 / 9.34¬±0.64
9.93¬±0.03 / 0.43¬±0.09
10.00¬±0.00 / 0.01¬±0.01
10.00¬±0.00 / 0.00¬±0.00
10.00¬±0.00 / 0.00¬±0.00
7.86¬±0.13 / 97.51¬±3.90
4.87¬±0.16 / 127.65¬±5.48

method
ILQL œÑ = 0.7, Œ≤ = 4
ILQL œÑ = 0.7, Œ≤ = 8
ILQL œÑ = 0.7, Œ≤ = 16
ILQL œÑ = 0.8, Œ≤ = 4
ILQL œÑ = 0.8, Œ≤ = 8
ILQL œÑ = 0.8, Œ≤ = 16
ILQL œÑ = 0.9, Œ≤ = 4
ILQL œÑ = 0.9, Œ≤ = 8
ILQL œÑ = 0.9, Œ≤ = 16
SARSA Œ≤ = 4
SARSA Œ≤ = 8
SARSA Œ≤ = 16
10%BC
30%BC
50%BC
BC

score
-2.23¬±0.03
-2.18¬±0.03
-2.18¬±0.03
-2.13¬±0.02
-2.13¬±0.03
-2.31¬±0.03
-2.30¬±0.03
-2.26¬±0.03
-2.36¬±0.03
-2.24¬±0.03
-2.26¬±0.03
-2.23¬±0.03
-2.93¬±0.06
-3.01¬±0.06
-2.38¬±0.03
-2.61¬±0.03

Table 7: All hyper-parameter settings and baselines evaluated on the human Wordle dataset scraped
from Tweets (see Section A.4). The best performing setting for each baseline of abalation is bolded.
ILQL is generally better performing than baselines.

27

