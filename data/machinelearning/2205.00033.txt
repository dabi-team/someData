A Human-Centric Perspective on Fairness and Transparency in
Algorithmic Decision-Making
Jakob Schoeffer
jakob.schoeffer@kit.edu
Karlsruhe Institute of Technology (KIT)
Germany

2
2
0
2

r
p
A
9
2

]
I

A
.
s
c
[

1
v
3
3
0
0
0
.
5
0
2
2
:
v
i
X
r
a

ABSTRACT
Automated decision systems (ADS) are increasingly used for con-
sequential decision-making. These systems often rely on sophis-
ticated yet opaque machine learning models, which do not allow
for understanding how a given decision was arrived at. This is not
only problematic from a legal perspective, but non-transparent sys-
tems are also prone to yield unfair outcomes because their sanity
is challenging to assess and calibrate in the first place—which is
particularly worrisome for human decision-subjects. Based on this
observation and building upon existing work, I aim to make the
following three main contributions through my doctoral thesis: (a)
understand how (potential) decision-subjects perceive algorithmic
decisions (with varying degrees of transparency of the underlying
ADS), as compared to similar decisions made by humans; (b) evalu-
ate different tools for transparent decision-making with respect to
their effectiveness in enabling people to appropriately assess the
quality and fairness of ADS; and (c) develop human-understandable
technical artifacts for fair automated decision-making. Over the
course of the first half of my PhD program, I have already addressed
substantial pieces of (a) and (c), whereas (b) will be the major focus
of the second half.

CCS CONCEPTS
• Human-centered computing → Human computer interac-
tion (HCI); • Computing methodologies → Machine learn-
ing; • Information systems → Decision support systems.

KEYWORDS
Algorithmic decision-making, explanations, fairness

ACM Reference Format:
Jakob Schoeffer. 2022. A Human-Centric Perspective on Fairness and Trans-
parency in Algorithmic Decision-Making. In CHI Conference on Human
Factors in Computing Systems Extended Abstracts (CHI ’22 Extended Ab-
stracts), April 29–May 5, 2022, New Orleans, LA, USA. ACM, New York, NY,
USA, 6 pages. https://doi.org/10.1145/3491101.3503811

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
CHI ’22 Extended Abstracts, April 29–May 5, 2022, New Orleans, LA, USA
© 2022 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-9156-6/22/04.
https://doi.org/10.1145/3491101.3503811

1 CONTEXT AND MOTIVATION
Automated decision-making has become ubiquitous in many do-
mains such as hiring [32], bank lending [56], grading [49], and polic-
ing [23], among others. As automated decision systems (ADS) are
used to inform increasingly high-stakes decisions, understanding
their inner workings is of utmost importance—and undesirable be-
havior can become a problem of societal relevance. The underlying
motives of adopting ADS are manifold: they range from cost-cutting
to improving performance and enabling more robust and objective
decisions [22, 32, 45]. One widespread assumption is that ADS can
also avoid human biases in the decision-making process [32]. In
fact, if properly designed, ADS can be a valuable tool for breaking
out of vicious patterns of stereotyping and contributing to social
equity, for instance, in the realms of recruitment [8, 30], health care
[20, 57], or financial inclusion [39]. However, ADS are typically
based on artificial intelligence (AI)—particularly machine learning
(ML)—techniques, which, in turn, generally rely on historical data.
If, for instance, this underlying data is biased (e.g., because certain
socio-demographic groups were favored in a disproportionate way),
an ADS will pick up and perpetuate existing patterns of unfairness
[18]. Prominent examples of such behavior from the recent past
are race and gender stereotyping in job ad delivery on Facebook
and LinkedIn [25], as well as the discrimination of black people in
the realm of facial recognition [4] and recidivism prediction [1]. To
that end, in recent years, a significant body of research has been
devoted to detecting and mitigating unfairness in ADS—particularly
from the techno-centric perspective of the AI/ML communities [2].
Yet, most of this work has focused on formalizing the concept of
fairness and enforcing certain statistical equity constraints, often
without explicitly taking into account the opinions, feelings, and
perceptions of the people affected by such automated decisions.

Partly as a response to multiple known instances of adverse
behavior by ADS in high-stakes decision-making settings, both
researchers and lawmakers have started to demand that decision-
subjects be presented with more information about the inner work-
ings of algorithms [55]. In fact, the EU General Data Protection
Regulation (GDPR)1, for instance, requires the disclosure of “the
existence of automated decision-making, including [. . . ] meaningful
information about the logic involved [. . . ]” to data subjects. Beyond
that, however, such regulations generally remain vague and little
actionable, which often results in deficient adoption, as noticed in
the context of bank lending.2 Moreover, while the general need
for transparency appears obvious, it is also understood that there
exists no “one-size-fits-all” explanation technique that addresses all

1https://eur-lex.europa.eu/eli/reg/2016/679/oj (last accessed: October 10, 2021)
2https://algorithmwatch.org/en/poland-credit-loan-transparency/ (last accessed: Oc-
tober 10, 2021)

 
 
 
 
 
 
CHI ’22 Extended Abstracts, April 29–May 5, 2022, New Orleans, LA, USA

Jakob Schoeffer

desiderata of different stakeholders of algorithmic decision-making
[3, 14, 35]. This inevitably results in certain stakeholders’—often
the system designers’—goals being (implicitly or explicitly) priori-
tized over more vulnerable groups, such as the decision-subjects,
when choosing a means to facilitate “transparency” of ADS [42].
Additionally, it remains an open question (among many others)
how to evaluate the effectiveness of explanations with respect to
desiderata that are somewhat difficult to measure; such as morality,
ethics, responsibility [35], and others.

In this doctoral thesis, I aim to address selected open questions
around fairness and transparency in algorithmic decision-making
from the perspective of the most vulnerable stakeholders: decision-
subjects. In Section 2, I summarize key related work and highlight
specific research gaps. Next, in Section 3, I derive open research
questions and outline how I plan to fill the previously identified
gaps. After that, in Section 4, I summarize my previous results
and contributions to date; and I lay out specific next steps in my
dissertation in Section 5. To conclude, I briefly address my long-term
goals.

2 KEY RELATED WORK AND RESEARCH

GAPS

In this section, I summarize key related work along the dimensions
of algorithmic fairness, explainable AI, and perceptions of fairness
and trustworthiness, and I highlight certain research gaps that I
address in my doctoral thesis.

Algorithmic fairness. Fairness is a contested concept [44].
Mehrabi et al. [43], similarly to existing laws and regulations, define
fairness in the context of decision-making as the “absence of any
prejudice or favoritism towards an individual or a group based on
their intrinsic or acquired traits.” Generally, the (mostly AI/ML-
driven) algorithmic fairness literature distinguishes individual from
group fairness definitions [43]. A widely-used notion of individual
fairness is fairness through awareness (FTA) [15]. FTA says that an
algorithm is fair if it gives similar predictions to similar individuals.
As stated by Dwork et al. [15], the main challenge with this notion
is defining an appropriate distance metric—for instance, to mea-
sure differences in qualification between individuals. Regarding
group fairness metrics, popular notions include demographic parity
[6] (“predictions should be independent of sensitive information”)
and equalized odds/equal opportunity [21] (“predictive error rates
should be equal for all demographic groups”). Most often, these
statistical notions are then enforced as constraints during the train-
ing phase of the associated ML model (in-processing techniques
[43]); meaning that predictive performance (e.g., accuracy) still re-
mains the primary objective. There are, however, several limitations
to this: first, most existing approaches to fairness-aware ADS are
specifically designed for (often binary) classification tasks, whereas
fairness-aware regression is still relatively under-researched. Fur-
thermore, error rate-based fairness notions—as well as the whole
idea of optimizing for predictive performance in ADS—rely on the
availability of ground-truth labels; for instance, whether or not an
applicant actually paid back a loan. This is a strong assumption that
is often not met in real-world decision-making scenarios [33, 51].
Specifically, if ground-truth labels are not (or selectively) available
[33], then maximizing for accuracy subject to fairness constraints

seems counterintuitive and is, in fact, sub-optimal [26]. Lastly, and
perhaps most importantly, it has been shown that many notions
of fairness are mutually incompatible [9, 29]. For instance, demo-
graphic parity will generally be at odds with individual fairness.
This implies that there cannot be a technical fairness definition that
universally works for everyone and every use case. Hence, it is
important to consider alternative fairness criteria, such as human
perceptions of fairness, as well.

Explainable AI. Despite being a popular topic of current research,
explainable AI (XAI) is a natural consequence of designing ADS and,
as such, has been around at least since the 1980s [40]. Its importance,
however, keeps rising as increasingly sophisticated (and opaque)
AI techniques are used to inform ever more consequential deci-
sions. Common approaches to XAI are (a) employing “white-box”
ML models that are interpretable by design (e.g., logistic regres-
sion), or (b) providing post-hoc explanations (e.g., LIME [48] or
SHAP [41]). Explainability is not only required by law (e.g., GDPR,
ECOA3); Eslami et al. [17], for instance, have shown that users’
attitudes towards algorithms change when transparency is altered.
In fact, both quantity and quality of explanations matter: Kulesza
et al. [31] explored the effects of soundness and completeness of
explanations on end users’ mental models and suggest, among oth-
ers, that oversimplification is problematic. Recent findings from
Langer et al. [34], on the other hand, suggest that in certain cases
it might make sense to withhold pieces of information in order to
not evoke negative reactions. Either way, even in the presence of
explanations, people sometimes rely too heavily on system sugges-
tions [5], a phenomenon sometimes referred to as automation bias
[13, 19]. Eventually, latest research cautions that explanations can
also have negative consequences—either through intentional decep-
tion of certain stakeholders [10] or even unsuspecting downstream
effects [16]. In fact, a major share of prior work in this area has
been looking at issues of opaque ADS through the lens of powerful
stakeholders, such as decision-makers, and how to benefit them
through explanations. There are still significant contributions to be
made with respect to designing (possibly combinations of) explana-
tions that meet the needs of more vulnerable groups (e.g., decision-
subjects). Generally, as pointed out by Langer et al. [35], many prior
studies have gathered mixed or inconclusive empirical evidence
regarding the general effectiveness of explanations—which again
demands follow-up work, for instance within specific communities
or subgroups of a population. Finally, as recent XAI literature seems
to have been favoring post-hoc techniques over “white-box” ML,
it should be noted that explaining highly-nonlinear ML models in
a post-hoc fashion can lead to somewhat ambiguous insights or
recommendations that violate common social norms such as “do
not penalize good attributes” [58].

Perceptions of fairness and trustworthiness. Due to the contested
nature of the fairness concept [44] as well as shortcomings of the
typical AI/ML approaches to algorithmic fairness listed previously,
researchers (primarily from HCI) have turned to more empirical
and human-centric ways of assessing fairness and transparency of
ADS. For instance, Binns et al. [3] and Dodge et al. [14] compare

3Equal Credit Opportunity Act: https://www.consumer.ftc.gov/articles/0347-your-
equal-credit-opportunity-rights (last accessed: October 10, 2021)

Fairness and Transparency in Algorithmic Decision-Making

CHI ’22 Extended Abstracts, April 29–May 5, 2022, New Orleans, LA, USA

Table 1: Overview of my dissertation-related academic work to date.

Contribution to References

Venue

RQ1

RQ2

RQ3

Schoeffer et al. [53]
Schoeffer et al. [52]
Schoeffer et al. [54]

TExSS Workshop @ ACM IUI 2021
Hawaii International Conference on System Sciences 2022 (HICSS-55)
Under review at ACM CHI 2022

Schoeffer and Kuehl [50]

Poster @ ACM CSCW 2021

Schoeffer et al. [51]

ACM COMPASS 2021

human fairness perceptions of ADS for four distinct post-hoc ex-
planation styles. Their works suggest differences in effectiveness
of individual explanation styles—however, they also note that there
does not seem to be a single best approach to explaining automated
decisions. Lee [36] compares perceptions of fairness and trustwor-
thiness depending on whether the decision-maker is a person or
an algorithm in the context of managerial decisions. Their findings
suggest that, among others, people perceive automated decisions
as less fair and trustworthy for tasks that require typical human
skills. An interesting finding by Lee et al. [37] suggests that fairness
perceptions decline for some people when gaining an understand-
ing of an algorithm if their personal fairness concepts differ from
those of the algorithm. Regarding trustworthiness, Kizilcec [28],
for instance, concludes that it is important to provide the right
amount of transparency for optimal trust effects, as both too much
and too little transparency can have undesirable effects. One major
weakness of related work is addressed by Lee and Rich [38], who
point out that prior studies have mostly recruited respondents from
Amazon Mechanical Turk [47], which has predominantly white
participants [24]. Moreover, it is often argued that one goal of expla-
nations should be to facilitate positive perceptions (e.g., fairness and
trustworthiness) towards ADS—which implicitly assumes that the
respective ADS is fair and trustworthy, to begin with. Examining
which explanations allow for calibrated perceptions is a significant
research gap. It will also be important to empirically measure novel
“flavors” of fairness; for instance, based on established constructs
from other disciplines like psychology.

3 RESEARCH OBJECTIVES
Based on research gaps identified in the previous section, I aim to
contribute towards answering the following three main research
questions through my doctoral thesis:

RQ1 How do (potential) decision-subjects perceive consequential
algorithmic decisions, primarily with respect to fairness; and
how do explanations impact these perceptions?

RQ2 How and when can explanations enable (potential) decision-
subjects to appropriately assess the quality (e.g., fairness) of
ADS?

RQ3 Considering that in many real-world decision-making sce-
narios we do not have access to ground-truth labels, how can
we design fair and transparent ADS that—unlike traditional
ML approaches—do not rest on this assumption?

I aim to investigate RQ1 through an experimental study conducted
with online study participants. Contrary to existing work, I provide

combinations of different explanation styles to (potential) decision-
subjects, thereby simulating different levels of transparency. I study
perceptions of fairness and trustworthiness for the specific use case
of bank lending, and I also analyze the moderating effects of people’s
AI literacy on their perceptions. Based on findings from RQ1, I aim
to analyze for RQ2 whether people’s perceptions are calibrated—
meaning that they should be high if and only if the underlying ADS
is fair and trustworthy. To that end, I aim to conduct a random-
ized online experiment as well. Finally, regarding RQ3, I propose
a paradigm shift from making accurate predictions to making good
decisions. Specifically, I aim to (a) make an algorithmic contribu-
tion to the fairness- and transparency-aware ADS toolbox; and (b)
evaluate empirically how (potential) decision-subjects perceive this
“white-box” artifact compared to traditional ML approaches with
post-hoc explanations.

4 RESULTS AND CONTRIBUTIONS TO DATE
In this section, I briefly summarize main results and contributions
of my dissertation-related academic work to date. An overview of
peer-reviewed accepted and in-submission work is given in Table
1.

Regarding RQ1. For understanding people’s perceptions towards
ADS better, I have already conducted two studies. The first one
[53, 54] examines the effects of ADS transparency on people’s per-
ceptions of informational fairness and trustworthiness. To that end,
I have designed and conducted a randomized online experiment
around automated loan decisions with 400 participants recruited
through Prolific [46]. Participants were assigned one of four trans-
parency treatments (between-subject) and asked multiple questions
based on established constructs from the information systems [7]
and organizational justice [11] literature. Please refer to [53] for
samples of the employed questionnaires as well as the precise study
setup including explanations. Through analyzing group differences
and estimating a full structural equation model (SEM), I found that
an increase in transparency leads to an increase in both informa-
tional fairness and trustworthiness perceptions. It is interesting
to note, however, that informational fairness appears to act as a
mediator between transparency and trustworthiness perceptions.
Additionally, I found evidence that people with higher AI literacy
tend to perceive ADS as more informationally fair and trustworthy
than people with little or no knowledge in this field. In the second
study [52], I have similarly conducted an online experiment (200
participants, recruited through Prolific) to compare perceptions
between human-made and automated decisions in the context of
lending. Study participants were randomly assigned one of two

CHI ’22 Extended Abstracts, April 29–May 5, 2022, New Orleans, LA, USA

Jakob Schoeffer

(cid:143)

Fair ADS

Unfair ADS

Input Influence

Sensitivity

Case-Based

Demographic

Input Influence

Sensitivity

Case-Based

Demographic

Figure 1: Blueprint of preliminary study setup regarding RQ2.

treatments (human vs. automated), where both groups were shown
identical explanations about the decision-making logic—either em-
ployed by a human being or an ADS. Interestingly, and contrary
to some prior works’ findings, I found that automated decisions
are perceived as fairer than human-made decisions in the given
context. Based on an analysis of qualitative responses, it appears
that people particularly appreciate the absence of subjectivity in
ADS as well as their data-driven approach. Again, please refer to
the full publication [52] for further details.

that is readily interpretable by decision-subjects and allows for
meaningful recourse (as opposed to many nonlinear ML models).
Through extensive experiments on synthetic and real-world data, I
have shown that the proposed method is fair in the sense that it (a)
assigns the desirable outcome to the most qualified individuals, and
(b) removes the effect of stereotypes in decision-making, thereby
outperforming traditional classification algorithms. Finally, in [51],
I have also shown theoretically that my method is consistent with
the prominent individual fairness notion of FTA (see Section 2).

Regarding RQ2. Based on the observations that (a) explanations
can be misused to deceive certain groups of stakeholders (deliber-
ately or unintentionally), and thus, (b) facilitating positive percep-
tions should not be an unconditional goal of providing explanations,
I have introduced the desideratum of appropriate fairness perceptions
[50]. This desideratum is satisfied if people’s fairness perceptions
towards an ADS are positive if and only if the system is actually
fair. A violation of this desideratum (e.g., when perceptions are
positive despite the ADS issuing unfair decisions) likely indicates
that people are not (sufficiently) enabled to appropriately assess the
fairness of the respective ADS. I further introduce in [50] a novel
study design to gauge the effectiveness of given explanation styles
with respect to this desideratum. I aim to spend a major share of
the second half of my PhD program on conducting this study (see
also Section 5).

Regarding RQ3. As outlined in Section 2, the general approach
of in-processing fairness-aware ML techniques is to enforce cer-
tain statistical fairness constraints while maximizing for predic-
tive performance. This approach, however, is sub-optimal when
ground-truth labels are unavailable [27]. In fact, in many real-world
settings, we only have access to data with imperfect labels, as the
result of (potentially biased) human-made decisions. In [51], I pro-
pose a novel ranking-based decision system that does not learn
to mimic biased decisions but (a) incorporates only useful infor-
mation from historical decisions, and (b) accounts for unwanted
correlation between sensitive (e.g., gender) and legitimate features.
Specifically, I introduce a decision criterion based on weighted
distances between individual data points and a so-called North
Star, which represents the (potentially hypothetical) observation
with the highest possible qualification towards a given outcome.
By taking into account known (or easily identifiable) monotonic
relationships between legitimate features and the outcome, the
proposed decision criterion can be seen as a “white-box” approach

5 NEXT STEPS AND OUTLOOK
Based on my results and contributions to date, most work remains
to be done around RQ2. In fact, after presenting and gathering
feedback on the proposed idea and study setup of [50] at CSCW
2021, I intend to design and carefully evaluate a suitable experiment.
A blueprint of the preliminary study setup is depicted in Figure 1.
Specifically, I aim to conduct a between-subject experiment where
study participants are randomly shown either outcome from a fair
or an unfair ADS. Then, they are provided with a specific type of
explanation regarding the decision-making logic. The four explana-
tion styles from Figure 1 are taken from Binns et al. [3], but they
could readily be replaced and/or extended by others. Finally, I pro-
pose to measure fairness perceptions based on Colquitt and Rodell
[12], and compare the results between the fair and unfair ADS treat-
ments. If perceptions are not substantially different, this indicates
that a given explanation style does not satisfy the desideratum of
appropriate fairness perceptions. A key challenge in designing this
experiment will be to align people’s personal “fairness concepts”
with the employed notion of fairness when constructing the fair
and unfair ADS. I aim to accomplish this through supplementary
upstream experiments as well as through focusing on a specific
community of people. I generally think that this work will benefit
tremendously from feedback of peers as well as more experienced
HCI researchers.

Next, regarding RQ3, I plan to empirically evaluate my technical
artifact [51] with respect to people’s perceptions and understanding.
I hypothesize that people appreciate the “white-box” approach—
specifically, the fact that legitimate features are monotonically re-
lated to the final outcome; for instance, that higher GRE scores will
never decrease the chances of being admitted to grad school. (Note
that this is not guaranteed for nonlinear ML models.) I plan to test
this hypothesis, among others, in a lab study setting. Depending on
the volume of this remaining work, I may have additional time to

Fairness and Transparency in Algorithmic Decision-Making

CHI ’22 Extended Abstracts, April 29–May 5, 2022, New Orleans, LA, USA

examine techniques for making my approach from [51] applicable
to regression settings as well.

With this dissertation at the intersection of HCI, computer sci-
ence, and information systems, I aim to contribute towards solving
issues of unfairness and non-transparency in algorithmic decision-
making, from the perspective of the most vulnerable stakeholders:
decision-subjects. After finishing my PhD program, it is my utmost
desire to continue conducting research on this important topic—
either in a postdoctoral researcher role or as a researcher in industry.
My long-term career goal is to become a full professor.

REFERENCES
[1] Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2016. Machine
bias. https://www.propublica.org/article/machine-bias-risk-assessments-in-
criminal-sentencing.

[2] Solon Barocas, Moritz Hardt, and Arvind Narayanan. 2018. Fairness and machine

learning. (2018). http://www.fairmlbook.org

[3] Reuben Binns, Max Van Kleek, Michael Veale, Ulrik Lyngs, Jun Zhao, and Nigel
Shadbolt. 2018. ’It’s reducing a human being to a percentage’ Perceptions of
justice in algorithmic decisions. In Proceedings of the 2018 CHI Conference on
Human Factors in Computing Systems. 1–14.

[4] Joy Buolamwini and Timnit Gebru. 2018. Gender shades: Intersectional accu-
racy disparities in commercial gender classification. In Conference on Fairness,
Accountability, and Transparency. PMLR, 77–91.

[5] Adrian Bussone, Simone Stumpf, and Dympna O’Sullivan. 2015. The role of
explanations on trust and reliance in clinical decision support systems. In 2015
International Conference on Healthcare Informatics. IEEE, 160–169.

[6] Toon Calders, Faisal Kamiran, and Mykola Pechenizkiy. 2009. Building classifiers
with independency constraints. In 2009 IEEE International Conference on Data
Mining Workshops. IEEE, 13–18.

[7] Lemuria Carter and France Bélanger. 2005. The utilization of e-government
services: Citizen trust, innovation and acceptance factors. Information Systems
Journal 15, 1 (2005), 5–25.

[8] Aaron Chalfin, Oren Danieli, Andrew Hillis, Zubin Jelveh, Michael Luca, Jens
Ludwig, and Sendhil Mullainathan. 2016. Productivity and selection of human
capital with machine learning. American Economic Review 106, 5 (2016), 124–127.
[9] Alexandra Chouldechova. 2017. Fair prediction with disparate impact: A study
of bias in recidivism prediction instruments. Big Data 5, 2 (2017), 153–163.
[10] Michael Chromik, Malin Eiband, Sarah Theres Völkel, and Daniel Buschek. 2019.
Dark patterns of explainability, transparency, and user control for intelligent
systems. In IUI Workshops, Vol. 2327.

[11] Jason A Colquitt, Donald E Conlon, Michael J Wesson, Christopher O L H Porter,
and K Yee Ng. 2001. Justice at the millennium: A meta-analytic review of 25 years
of organizational justice research. Journal of Applied Psychology 86, 3 (2001), 425.
[12] Jason A Colquitt and Jessica B Rodell. 2015. Measuring justice and fairness.

(2015).

[13] Maria De-Arteaga, Riccardo Fogliato, and Alexandra Chouldechova. 2020. A
case for humans-in-the-loop: Decisions in the presence of erroneous algorithmic
scores. In Proceedings of the 2020 CHI Conference on Human Factors in Computing
Systems. 1–12.

[14] Jonathan Dodge, Q Vera Liao, Yunfeng Zhang, Rachel KE Bellamy, and Casey
Dugan. 2019. Explaining models: An empirical study of how explanations impact
fairness judgment. In Proceedings of the 24th International Conference on Intelligent
User Interfaces. 275–285.

[15] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard
Zemel. 2012. Fairness through awareness. In Proceedings of the 3rd Innovations in
Theoretical Computer Science Conference. 214–226.

[16] Upol Ehsan and Mark O Riedl. 2021. Explainability pitfalls: Beyond dark patterns

in explainable AI. arXiv preprint arXiv:2109.12480 (2021).

[17] Motahhare Eslami, Kristen Vaccaro, Min Kyung Lee, Amit Elazari Bar On, Eric
Gilbert, and Karrie Karahalios. 2019. User attitudes towards algorithmic opacity
and transparency in online reviewing platforms. In Proceedings of the 2019 CHI
Conference on Human Factors in Computing Systems. 1–14.

[18] Stefan Feuerriegel, Mateusz Dolata, and Gerhard Schwabe. 2020. Fair AI: Chal-
lenges and opportunities. Business & Information Systems Engineering 62 (2020),
379–384.

[19] Kate Goddard, Abdul Roudsari, and Jeremy C Wyatt. 2014. Automation bias:
Empirical results assessing influencing factors. International Journal of Medical
Informatics 83, 5 (2014), 368–375.

[20] Thomas Grote and Philipp Berens. 2020. On the ethics of algorithmic decision-

making in healthcare. Journal of Medical Ethics 46, 3 (2020), 205–211.

[21] Moritz Hardt, Eric Price, and Nati Srebro. 2016. Equality of opportunity in
supervised learning. In Advances in Neural Information Processing Systems. 3315–
3323.

[22] Jeanne G Harris and Thomas H Davenport. 2005. Automated decision making

comes of age. MIT Sloan Management Review 46, 4 (2005), 2–10.

[23] Will Douglas Heaven. 2020. Predictive policing algorithms are racist. They need

to be dismantled. MIT Technology Review (2020).

[24] Paul Hitlin. 2016. Research in the crowdsourcing age: A case study. (2016).
[25] Basileal Imana, Aleksandra Korolova, and John Heidemann. 2021. Auditing
for discrimination in algorithms delivering job ads. In Proceedings of the Web
Conference 2021. 3767–3778.

[26] Niki Kilbertus, Manuel Gomez Rodriguez, Bernhard Schölkopf, Krikamol Muan-
det, and Isabel Valera. 2020. Fair decisions despite imperfect predictions. In
International Conference on Artificial Intelligence and Statistics. 277–287.
[27] Niki Kilbertus, Mateo Rojas-Carulla, Giambattista Parascandolo, Moritz Hardt,
Dominik Janzing, and Bernhard Schölkopf. 2017. Avoiding discrimination through
causal reasoning. In Advances in Neural Information Processing Systems. 656–666.
[28] René F Kizilcec. 2016. How much information? Effects of transparency on trust
in an algorithmic interface. In Proceedings of the 2016 CHI Conference on Human
Factors in Computing Systems. 2390–2395.

[29] Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. 2016.

Inherent
trade-offs in the fair determination of risk scores. arXiv preprint arXiv:1609.05807
(2016).

[30] Sami Koivunen, Thomas Olsson, Ekaterina Olshannikova, and Aki Lindberg. 2019.
Understanding decision-making in recruitment: Opportunities and challenges for
information technology. Proceedings of the ACM on Human-Computer Interaction
3, GROUP (2019), 1–22.

[31] Todd Kulesza, Simone Stumpf, Margaret Burnett, Sherry Yang, Irwin Kwan, and
Weng-Keen Wong. 2013. Too much, too little, or just right? Ways explanations
impact end users’ mental models. In 2013 IEEE Symposium on Visual Languages
and Human Centric Computing. IEEE, 3–10.

[32] Nathan R Kuncel, David M Klieger, and Deniz S Ones. 2014. In hiring, algorithms

beat instinct. Harvard Business Review (2014).

[33] Himabindu Lakkaraju, Jon Kleinberg, Jure Leskovec, Jens Ludwig, and Sendhil
Mullainathan. 2017. The selective labels problem: Evaluating algorithmic predic-
tions in the presence of unobservables. In Proceedings of the 23rd ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining. 275–284.
[34] Markus Langer, Kevin Baum, Cornelius J König, Viviane Hähne, Daniel Oster,
and Timo Speith. 2021. Spare me the details: How the type of information about
automated interviews influences applicant reactions. International Journal of
Selection and Assessment (2021).

[35] Markus Langer, Daniel Oster, Timo Speith, Holger Hermanns, Lena Kästner,
Eva Schmidt, Andreas Sesing, and Kevin Baum. 2021. What do we want from
explainable artificial intelligence (XAI)?—A stakeholder perspective on XAI and
a conceptual model guiding interdisciplinary XAI research. Artificial Intelligence
296 (2021), 103473.

[36] Min Kyung Lee. 2018. Understanding perception of algorithmic decisions: Fair-
ness, trust, and emotion in response to algorithmic management. Big Data &
Society 5, 1 (2018), 1–16.

[37] Min Kyung Lee, Anuraag Jain, Hea Jin Cha, Shashank Ojha, and Daniel Kusbit.
2019. Procedural justice in algorithmic fairness: Leveraging transparency and
outcome control for fair algorithmic mediation. Proceedings of the ACM on
Human-Computer Interaction 3, CSCW (2019), 182:1–182:26.

[38] Min Kyung Lee and Katherine Rich. 2021. Who is included in human perceptions
of AI? Trust and perceived fairness around healthcare AI and cultural mistrust. In
Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems.
1–14.

[39] Bruno Lepri, Jacopo Staiano, David Sangokoya, Emmanuel Letouzé, and Nuria
Oliver. 2017. The tyranny of data? The bright and dark sides of data-driven
decision-making for social good. In Transparent Data Mining for Big and Small
Data. Springer, 3–24.

[40] Clayton Lewis and Robert Mack. 1982. The role of abduction in learning to use a

computer system. (1982).

[41] Scott M Lundberg and Su-In Lee. 2017. A unified approach to interpreting
model predictions. In Proceedings of the 31st International Conference on Neural
Information Processing Systems. 4768–4777.

[42] Arunesh Mathur, Gunes Acar, Michael J Friedman, Elena Lucherini, Jonathan
Mayer, Marshini Chetty, and Arvind Narayanan. 2019. Dark patterns at scale:
Findings from a crawl of 11K shopping websites. Proceedings of the ACM on
Human-Computer Interaction 3, CSCW (2019), 1–32.

[43] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram
Galstyan. 2021. A survey on bias and fairness in machine learning. ACM Com-
puting Surveys (CSUR) 54, 6 (2021), 1–35.

[44] Deirdre K Mulligan, Joshua A Kroll, Nitin Kohli, and Richmond Y Wong. 2019.
This thing called fairness: Disciplinary confusion realizing a value in technology.
Proceedings of the ACM on Human-Computer Interaction 3, CSCW (2019), 1–36.
[45] Sue Newell and Marco Marabelli. 2015. Strategic opportunities (and challenges)
of algorithmic decision-making: A call for action on the long-term societal effects

CHI ’22 Extended Abstracts, April 29–May 5, 2022, New Orleans, LA, USA

Jakob Schoeffer

of ‘datification’. The Journal of Strategic Information Systems 24, 1 (2015), 3–14.
[46] Stefan Palan and Christian Schitter. 2018. Prolific.ac—A subject pool for online
experiments. Journal of Behavioral and Experimental Finance 17 (2018), 22–27.
[47] Gabriele Paolacci, Jesse Chandler, and Panagiotis G Ipeirotis. 2010. Running
experiments on Amazon Mechanical Turk. Judgment and Decision making 5, 5
(2010), 411–419.

[48] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. “Why should I
trust you?” Explaining the predictions of any classifier. In Proceedings of the 22nd
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.
1135–1144.

[49] Adam Satariano. 2020. British grading debacle shows pitfalls of automating
government. The New York Times (2020). https://www.nytimes.com/2020/08/20/
world/europe/uk-england-grading-algorithm.html

[50] Jakob Schoeffer and Niklas Kuehl. 2021. Appropriate fairness perceptions? On
the effectiveness of explanations in enabling people to assess the fairness of
automated decision systems. In Companion Publication of the 24th ACM Confer-
ence on Computer Supported Cooperative Work and Social Computing (CSCW ’21
Companion).

[51] Jakob Schoeffer, Niklas Kuehl, and Isabel Valera. 2021. A ranking approach to fair
classification. In COMPASS ’21: Proceedings of the 4th ACM SIGCAS Conference on
Computing and Sustainable Societies.

[52] Jakob Schoeffer, Yvette Machowski, and Niklas Kuehl. 2021. Perceptions of
fairness and trustworthiness based on explanations in human vs. automated
decision-making. In 55th Hawaii International Conference on System Sciences 2022
(HICSS-55).

[53] Jakob Schoeffer, Yvette Machowski, and Niklas Kuehl. 2021. A study on fairness
and trust perceptions in automated decision making. In Joint Proceedings of the
ACM IUI 2021 Workshops.

[54] Jakob Schoeffer, Yvette Machowski, and Niklas Kuehl. 2021. “There is not enough
information”: On the effects of transparency on perceptions of informational
fairness and trustworthiness in automated decision making. Preprint (2021).
[55] Megha Srivastava, Hoda Heidari, and Andreas Krause. 2019. Mathematical
notions vs. human perception of fairness: A descriptive approach to fairness
for machine learning. In Proceedings of the 25th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining. 2459–2468.

[56] Sian Townson. 2020. AI can make bank loans more fair. Harvard Business Review

(2020).

[57] Stefano Triberti, Ilaria Durosini, and Gabriella Pravettoni. 2020. A “third wheel”
effect in health decision making involving artificial entities: A psychological
perspective. Frontiers in Public Health 8 (2020).

[58] Serena Wang and Maya Gupta. 2020. Deontological ethics by monotonicity shape
constraints. In International Conference on Artificial Intelligence and Statistics.
PMLR, 2043–2054.

