IronMan: GNN-assisted Design Space Exploration in High-Level
Synthesis via Reinforcement Learning

Nan Wu
nanwu@ucsb.edu
UC Santa Barbara
Santa Barbara, CA, USA

Yuan Xie
yuanxie@ucsb.edu
UC Santa Barbara
Santa Barbara, CA, USA

Cong Hao
callie.hao@ece.gatech.edu
Georgia Institute of Technology
Atlanta, GA, USA

1
2
0
2
c
e
D
8

]

R
A
.
s
c
[

2
v
8
3
1
8
0
.
2
0
1
2
:
v
i
X
r
a

ABSTRACT
Despite the great success of High-Level Synthesis (HLS) tools, we
observe several unresolved challenges: 1) the high-level abstraction
of programming styles in HLS conceals optimization opportuni-
ties; 2) existing HLS tools do not provide flexible trade-offs among
different objectives and constraints; 3) the actual quality of the
resulting RTL designs is hard to predict. To this end, we propose an
end-to-end framework, IronMan. The primary goal is to enable a
flexible and automated design space exploration (DSE), which can
provide either optimized solutions under user-specified constraints,
or Pareto trade-offs among different objectives (e.g., resource types,
area, and latency). IronMan consists of three components: GPP
(a highly accurate graph-neural-network-based performance pre-
dictor), RLMD (a reinforcement-learning-based DSE engine that
explores the optimized resource allocation strategy), and CT (a
code transformer that assists RLMD and GPP by extracting data
flow graphs from original HLS C/C++). Experimental results show
that, 1) GPP achieves high prediction accuracy, reducing prediction
errors of HLS tools by 10.9× in resource usage and 5.7× in tim-
ing; 2) RLMD obtains optimized or Pareto solutions outperforming
genetic algorithm and simulated annealing by 12.7% and 12.9%,
respectively; 3) IronMan can find optimized solutions perfectly
matching various DSP constraints, with 2.54× fewer DSPs and up
to 6× shorter latency than those of HLS tools. IronMan is also up
to 400× faster than meta-heuristic techniques and HLS tools.

CCS CONCEPTS
• Hardware → Electronic design automation.

KEYWORDS
High-Level Synthesis; Graph Neural Network; Reinforcement Learn-
ing; Design Space Exploration

ACM Reference Format:
Nan Wu, Yuan Xie, and Cong Hao. 2021. IronMan: GNN-assisted Design
Space Exploration in High-Level Synthesis via Reinforcement Learning.
In Proceedings of the Great Lakes Symposium on VLSI 2021 (GLSVLSI ’21),
June 22–25, 2021, Virtual Event, USA. ACM, New York, NY, USA, 6 pages.
https://doi.org/10.1145/3453688.3461495

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
GLSVLSI ’21, June 22–25, 2021, Virtual Event, USA
© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-8393-6/21/06. . . $15.00
https://doi.org/10.1145/3453688.3461495

1 INTRODUCTION
High-Level Synthesis (HLS) benefits ASIC and FPGA design automa-
tion by enabling automated transformation from behavioral descrip-
tions in high-level languages (C/C++, etc.) to RTL-level designs.
In addition to widely used commercial HLS tools for FPGA [18]
and ASIC [1], recent efforts focus on improving RTL design qual-
ity [2, 22], performance and resource prediction [8, 19, 20], design
space exploration (DSE) [12], etc.

Despite great achievement shown by previous efforts, there are
several crucial challenges unaddressed. 1) Higher level abstrac-
tions in HLS can obstruct optimization opportunities. The
structured HLS coding style, such as loops and function calls, hin-
ders advanced or fined-grained performance and resource optimiza-
tion. Meanwhile, the irregular logic, cascaded and imperfect loops
in HLS programs usually require manual or complicated code trans-
formations to improve hardware implementation performance [7].
Table 1 demonstrates a simple multiplication-accumulation func-
tion using a for-loop. To explore trade-offs between the DSP usage
and the number of clock cycles (latency), typical ways are to use
unroll pragmas or manual loop-tiling, as line 1-4. However, when
the loop boundary (e.g., 8) is not divisible by the DSP constraint (e.g.,
3), it results in a partial unrolling as line 4, introducing undesired
latency increment (from 4 to 8) and worsening the critical path (CP)
timing (from 5ns to 7.4ns). The nested loops further complicate this
problem (imagine a 5-layer nested loop with a DSP constraint of 17).
Motivated by the necessity of better performance and more flexible
optimization choices, we propose a code transformer (CT). CT
easily allows to use directives, such as allocation and resource prag-
mas, to conduct finer-grained DSEs for resource and performance,
as line 7-11 in Table 1.

2) HLS tools do not always provide the best solution, nor
automatically provide trade-offs (Pareto solutions). Existing
DSE approaches as well as commercial HLS tools do not provide
flexible trade-offs among different objectives and constraints (e.g.,
different types of resources), and they usually sacrifice design la-
tency for less resource, or vise versa [12]. In contrast, one potential
alternative is to trade one type of resource for another (e.g., LUT and
DSP in FPGA) while maintaining the latency, which is unexplored
and only can be done through tedious manual efforts. An example
shown in Fig. 2 explores fine-grained trade-offs between LUTs and
DSPs: first, the HLS default solution is not on the Pareto frontier;
second, there is a large design space for finding the Pareto solutions,
and thus the DSE for Pareto solutions is non-trivial. Notably,
the solution space grows exponentially even for a binary selection
of DSP/LUT for each multiplication, which is further complicated
by different data precisions (bitwidth). Motivated by the necessity

 
 
 
 
 
 
Figure 1: The overall framework, IronMan.

Orig. Code: for (int i=0; i<8; i++) sum += a[i]∗b[i];

Figure 2: Pareto solutions between DSPs and LUTs on an
FPGA, achieved by specifying certain multiplications using
LUTs instead of DSPs. The input DFG has 200 operations.

and difficulty of flexible and fine-grained DSE, we propose a deep
reinforcement learning (RL) based DSE tool, RLMD.

3) The real quality of the resulting RTL designs is hard
to predict, especially for irregular data paths. Most existing
model-based predictors target well-structured data flows, such as
perfect and nested loops with high-level directives [19, 21]. As
such, these predictors and high level DSE tools are not suitable
for irregular logic and data paths (especially for timing estimation).
While machine-learning-based predictions are feasible [3, 8, 20],
they often requires abundant features after design synthesis and/or
implementation. Fortunately, the inherent graph structure of data
flow graphs (DFGs) provides a promising opportunity to exploit the
representative power of graph neural networks (GNNs) [4, 6, 11].
Motivated by the necessity of DSE and high-accuracy prediction
for irregular data paths and the intrinsic graph structure of DFGs,
we propose a GNN-based HLS performance predictor, GPP,
enabling RLMD for DSE on arbitrary DFGs.

By seamlessly integrating the three aforementioned components,
we propose an end-to-end framework, namely IronMan, as de-
picted in Fig. 1. IronMan has two major goals: 1) to enable a
flexible and automated DSE, aiming to explore various trade-offs
among different objectives such as resource types and latency; 2) to
provide an accurate RTL design performance predictor, which does
not require any additional features except the original DFG, support-
ing both regular and irregular data paths. We briefly introduce the
components and summarize our contributions as follows.
• GPP: a highly accurate GNN-based performance predictor for
HLS designs, including resource utilization (DSP/LUT) and crit-
ical path (CP) timing. It predicts the actual performance after
physical synthesis (placement and routing) rather than the syn-
thesized results by HLS tools, and can generalize to unseen DFGs.
• RLMD: a deep RL-based multi-objective DSE engine for re-
source allocation in HLS. Assisted by GPP, RLMD explores
optimized resource allocation strategy under user-specified con-
straints. The objectives include minimizing resource utilization,
optimizing CP timing and/or minimizing DFG computation la-
tency. RLMD also provides Pareto solutions among different
objectives, which are unavailable in HLS tools.

• CT: a code transformer that extracts DFGs from original HLS
C/C++ and re-generates synthesizable code with HLS directives
optimized by RLMD. CT reveals concealed optimization opportu-
nities for achieving higher parallelism, and enables flexible and
finer-grained DSE under user-specified constraints.

• IronMan: while each proposed component alone can contribute
to the HLS community (performance prediction, DSE, code trans-
formation), we integrate them into a framework, IronMan, and
demonstrate the end-to-end benefits on real-world benchmarks.

Method
Original
unroll (factor=8, complete)
unroll (factor=4)
unroll (factor=3)
unroll + allocation (limit=3) ∗
Code Transform (CT)
CT + allocation (limit=2)
CT + allocation (limit=3)
⊲ CT + resource (5 Mul_LUT)
⊲ CT + resource (4 Mul_LUT)
⊲ CT + resource (3 Mul_LUT)

1
2
3
4
5
6
7
8
9
10
11

Cycles DSP LUTs CP (ns)

17
2
4
8
4
2
5
4
2
2
2

1
8
4
3
6
8
3
6
2
2
3

75
100
87
109
168
100
196
168
1742
1741
1461

4.07
5.04
4.83
7.44
8.76
5.03
9.91
8.54
4.24
4.01
3.98

∗ HLS pragmas do not always behave as expected.

Table 1: Approaches to meeting DSP constraints (e.g., ≤
3). This work explores CT+resource approaches (line 9-11),
which achieve the best latency under the constraint. An al-
ternative to constrain DSP is to use #pragma HLS allocation
instance=mul, while increasing latency (line 7,8).

• Experimental results show that, 1) GPP achieves high prediction
accuracy in actual resource, reducing prediction errors of HLS
tools by 10.9× in resource usage and 5.7× in CP timing; 2) RLMD
obtains optimized or Pareto solutions, outperforming the genetic
algorithm and simulated annealing by 12.7% and 12.9%, respec-
tively; 3) on real-case benchmarks, IronMan finds solutions
satisfying various DSP constraints, with 2.54× fewer DSPs and
up to 6× shorter latency than those of HLS tools, while being up
to 400× faster than heuristic algorithms and HLS tools.

2 OVERALL FRAMEWORK
The overall framework of IronMan is shown in Fig. 1. The inputs
are HLS C/C++ code and user-specified constraints. The output is
the re-generated code with optimized HLS directives, either meet-
ing the user-specified constraints (e.g., resource or latency), or
providing Pareto solutions among different optimization objectives.
CT extracts the intrinsic DFGs from the intermediate representa-
tions (IRs) of HLS tools to release more optimization opportunities,
and then re-generates synthesizable C/C++ code with optimized
directives. Fig. 3 (a) exemplifies how CT re-generates C++ code,
with the extracted DFG in (b). Each intermediate operator may have
various bit-width, e.g., ⟨12⟩ means a 12-bit data precision.

GPP, a GNN-based performance predictor, estimates the ac-
tual resource usage after physical synthesis of DFGs. GNNs [4, 6, 11]
are adopted for three reasons. 1) DFGs are graphs, which are natu-
rally suitable for GNNs to learn the underlying information from
graph structures. 2) DFGs vary in topologies and sizes, and in order
to generalize predictions to unseen graphs, it is necessary to use in-
ductive GNNs [4] to learn fixed-size graph embeddings. 3) IronMan
runs inferences of trained GNN models during execution, which is
orders of magnitude faster than running HLS tools.

of 3, using #pragma HLS resource variable=⟨var⟩ core=Mul_LUT.
Notably, such a finer-grained DSE of IronMan is enabled by CT.

3 PROPOSED GPP AND RLMD
3.1 GPP
The key role of a GNN is to extract adequate information of node
types, graph topology and connectivity within a large DFG, and
encode the information into low-dimension vector representations
that can be used for downstream tasks.

Node Feature Vector. In a DFG, each node is encoded into a
10-dimension node feature vector, as the example shown in Fig. 3(d).
The 1𝑠𝑡 to 4𝑡ℎ dimension use one-hot representations to encode the
node types, including input nodes, intermediate nodes/operations
(additions and multiplications), and output nodes. The 5𝑠𝑡 to 9𝑡ℎ
dimension encode the data precision of an intermediate operation,
which in this work ranges from INT2 to INT32. We use a binary
representation to encode the precision minus one, so the bit-width
can be expressed in 5 bits. The 10𝑡ℎ dimension indicates whether
an HLS directive #pramga HLS resource is applied to this node. Note
that such an encoding scheme can be easily extended to support
more types of nodes/operations or pragmas.

Graph Embedding. We employ three GNN models of the same
structure to separately predict LUT/DSP usage and CP timing, as
illustrated in the left part of Fig. 4. For each GNN model, the inputs
are adjacency matrices and node feature matrices of DFGs. The
first two layers are graph convolutional [6], with 64 and 128 units
respectively and ReLu activations. In each graph convolutional
layer, the node embedding is updated by aggregating feature vectors
from its neighbors, and one node can receive information from
farther nodes by stacking multiple layers. Next, the learned node
embeddings are summarized by a mean pooling to create a graph
representation (i.e., a 128 × 1 vector). This representation is then
passed to a feed-forward network with three fully connected layers
and leaky ReLu (𝛼 = 0.1) activations to generate a graph embedding
(i.e., a 64 × 1 vector). The last layer is the output, involving a single
unit with ReLu activation to provide the prediction result.

Integration with RLMD. To integrate with RLMD, we combine
the three embedding vectors that focus on different characteristics
of DFGs into one graph embedding, shown as the 192 × 1 vector
in Fig. 4. Finally, the graph embedding vector is concatenated with
the meta data of the input DFG, and passed to RLMD as its inputs.
The DFG meta data include the size of the DFG (i.e., the number
of input/intermediate/output nodes and the number of edges) and
the number of multiplications in this DFG. Given predictions of
LUT/DSP/CP, solutions generated by RLMD can be quickly evalu-
ated, providing feedback to further improve the policy of RLMD.

3.2 RLMD
RL Formulation. The resource allocation problem in HLS, as a
typical RL [13] problem, can be formulated as a Markov Decision
Process (MDP), with four key components.
• States: the set of possible states. In this problem, a state can be

every possible partially assigned DFGs.

• Actions: the set of eligible actions under a state. In this problem,
given the current state and the currently considered node of the
DFG, the action is whether to assign the directive to this node.

Figure 3: An example of IronMan solution. (a) The original
HLS code and transformed code with resource pragma, indi-
cating the importance of CT for IronMan. (b) HLS default
solution (4 DSPs and a latency of 3); (c) HLS solution with
naive constraints (2 DSPs while increasing latency from 3 to
4); (d) IronMan solution (2 DSPs and an unchanged latency).

RLMD, an RL-based DSE engine, takes DFGs, their corre-
sponding graph embeddings, and user-specified constraints as in-
puts, to make endeavors for optimal resource allocation strategy.
RL is adopted for two main reasons. 1) The design space grows
exponentially with the size of DFGs, different graph topologies, and
various data precision. RL has been widely applied for proactive DSE
in computer systems [17], and a well pre-trained agent can general-
ize to new problems by minimal fine-tuning efforts. 2) By carefully
defining reward functions, RL agents can achieve multi-objective
optimization automatically, getting rid of manual efforts to craft
useful heuristics. An informative and well-crafted state representa-
tion will significantly benefit the learning process in RL problems,
motivating the integration of GPP and RLMD.Consequently, the
graph embeddings enable RLMD to generalize across different DFG
topologies, and GPP largely accelerates the training process of
RLMD by quickly evaluating solutions generated by RLMD.

As a case study of IronMan, the specific problem solved is
to find a resource allocation solution that strictly meets the
DSP constraint, or to find Pareto solutions between DSPs and
LUTs on FPGAs, without sacrificing computation latency. For
simplicity, the DFGs only have additions and multiplications, where
RLMD decides whether to assign the directive #pramga HLS resource
core=Mul_LUT to each multiplication, to minimize LUTs within
DSP constraints. As shown in Fig. 3, (c) naively uses #pragma HLS
allocation instance=mul limit=2 to enforce the usage of two DSPs; (d)
is the solution of IronMan with 2 DSPs and an unchanged latency

Figure 4: GPP and RLMD structure. GPP encodes information of the DFG adjacency and node features, to make predictions of
LUT/DSP/CP. RLMD outputs a binary probability distribution 𝜋 (𝑎𝑡 |𝑠𝑡 ) of whether to use LUTs for multiplication computation
on the current node, and a scalar as the state-value function.

• State transition: given a state and an action, the probability dis-

tribution of next states.

• Reward: the immediate reward of taking an action in a state. In
this problem, the reward is 0 for all intermediate actions, with an
exception for the last action where the reward is the evaluation
of the fully assigned DFG subject to user-specified constraints.
Specifically, the state at time step 𝑡 is 𝑠𝑡 , a concatenation of fea-
tures including a 192 × 1 graph embedding vector that describes
the current status of the DFG, the ID of current node to assign
a directive, metadata of the DFG, and the DSP constraint (either
user-specified or automatically generated for Pareto solution explo-
ration). The action 𝑎𝑡 is a valid assignment of a directive to the 𝑡𝑡ℎ
node, i.e., whether to use LUTs for multiplication on this node. The
reward 𝑟𝑡 is defined as a negative weighted sum of predicted LUTs,
CP timing, and the difference between predicted and target DSPs:

𝑟𝑡 =

(cid:26)−𝛼𝐿𝑈𝑇𝑝 − 𝛽 |𝐷𝑆𝑃𝑡𝑎𝑟𝑔𝑒𝑡 − 𝐷𝑆𝑃𝑝 | − 𝜆𝐶𝑃𝑝,
0,

𝑡 = 𝑇
0 < 𝑡 < 𝑇

.

(1)

where 𝛼, 𝛽 and 𝜆 are hyper-parameters.

At the initial state 𝑠0, all the multiplication nodes in a DFG are
unassigned. At each time step 𝑡, the RL agent observes the current
state 𝑠𝑡 , takes an action 𝑎𝑡 , receives a reward 𝑟𝑡 +1 and arrives at a
new state 𝑠𝑡 +1. The nodes are assigned with directives sequentially
based on their node IDs. Given 𝑇 multiplication nodes in total, the
final state 𝑠𝑇 corresponds to a DFG completely assigned with proper
directives. The goal is to maximize the expected rewards received.
RLMD Training. We adopt the actor-critic method with Monte-
Carlo learning [13]: the actor aims to learn an optimal policy
𝜋𝜃 (𝑎𝑡 |𝑠𝑡 ) parameterized by 𝜃 , which is a probability distribution of
valid actions under the current state; the critic approximates the
state-value function 𝑉 (𝑠𝑡 ) = E𝜋 [(cid:205)𝑇 −𝑡
𝑘=0 𝛾𝑘𝑟𝑡 +𝑘 |𝑠𝑡 ] by parameters 𝑤,
which is an estimate of total rewards starting from state 𝑠𝑡 to 𝑠𝑇
following policy 𝜋. The 𝛾 ∈ (0, 1] is the discount factor. As shown
in the right part of Fig. 4, there are shared parameters in the actor
𝜋𝜃 and the critic 𝑉𝑤, and for clarity we denote 𝜃 and 𝑤 separately.
By Monte-Carlo learning, the parameters are updated once only
after one complete episode (i.e., one complete assignment process
of a DFG), leading to the updates as follows:

𝛿𝑖 = 𝛾𝑇 −𝑖𝑟𝑇 − 𝑉𝑤 (𝑠𝑖 ),

𝑇
∑︁

𝛿𝑖 ∇𝑤𝑉𝑤 (𝑠𝑖 ),

𝑖=1
𝛿𝑖 ∇𝜃 log 𝜋𝜃 (𝑎𝑖 |𝑠𝑖 ),

Δ𝑤 ∝
𝑇
∑︁

Δ𝜃 ∝

𝑖=1

(2)

(3)

(4)

where 𝑇 is the total time steps in one episode. Through repeated
episodes (i.e., sequences of states, actions, and rewards), the actor
learns optimized policy that will maximize cumulative rewards.

Our ultimate goal is to enable RLMD to generate higher-quality
results and transfer knowledge across various DFGs as it gains ex-
perience from exploring resource allocation strategies on more and
more DFGs. Thus, we formally formulate the overall optimization
objective function as:

J (𝜃, 𝑤, 𝐺) =

1
𝐾

∑︁

𝑔∈𝐺

E𝑔,𝑙 ∼𝜋𝜃 [𝑅𝑔,𝑙 ],

(5)

where J (𝜃, 𝑤, 𝐺) measures the total expected rewards over all
training DFGs. The dataset 𝐺 has 𝐾 different DFGs, each of which
is denoted as 𝑔. 𝑅𝑔,𝑙 is the episode reward (i.e., 𝑟𝑇 in Eq.(1)) under the
resource allocation 𝑙 on the DFG 𝑔. To get better exploration during
training, we apply 𝜖-greedy algorithm for action selections [13].

RLMD Fine-tuning. Given an unseen DFG, the simplest way
is to directly apply the pre-trained RLMD for inference, which can
generate a solution within a second. When higher quality solutions
are expected, the pre-trained RLMD can be further finetuned on
this particular DFG. The fine-tuning step provides the flexibility
to trade off between a quick solution using the pre-trained RLMD
(which has learned rich knowledge of resource allocation strategies
on other DFGs) and a longer yet better one for a particular DFG.

4 EXPERIMENT
4.1 Experiment Setup
Dataset Generation. To train GPP and RLMD, we build a dataset of
both synthetic and real-case DFGs, which are generated by our CT.
For synthetic DFGs, we randomly generate 47 different topologies,
each of which has 100 to 200 operations (i.e., intermediate nodes) of
either multiplication or addition. Upon each distinct topology, we
generate 100 sets of directives, specifying a subset of multiplications
to be implemented by LUTs rather than DSPs. This makes up 4700
(i.e., 47 × 100) different synthetic DFGs. For real-case DFGs, 8 bench-
marks from MachSuite [10], CHStone [5] and PolyBench/C [9] are
considered: gemm, kernel_2mm, kernel_durbin (small, large), spmv,
stencil3d (small, large), and kernel_adi. Similarly, we randomly gen-
erate 100 sets of directives per benchmark, making up 800 real-case
DFGs. The ground-truth (actual) resource usage (LUT/DSP) and CP
timing are synthesized by Vivado HLS [18] and implemented by
Vivado [15] targeting Xilinx Ultra96 part xc7z020clg484.

Figure 5: GPP predictions on resource utilization (LUTs and DSPs), and critical path timing (CP timing).

4.2 Evaluation
Baselines. To evaluate GPP, we compare with the commercial tool
Vivado HLS [18] and the learning-based performance predictor,
Pyramid [8]. To evaluate IronMan, we compare with genetic algo-
rithm (GA) [16], simulated annealing (SA) [14], and the solutions
provided by Vivado HLS [18]. Notably, none of the state-of-the-art
DSE methods for HLS [12] can do such a fine-grained resource
allocation as we proposed in this work, which is enabled by our CT.
GPP vs. HLS tool and Pyramid [8]. GPP is evaluated on both
synthetic and real-case DFGs. Fig. 5 compares GPP predictions
with HLS synthesis reports regarding LUT, DSP and CP timing.
For LUT usage, the mean absolute percentage errors (MAPEs) of
GPP on synthetic and real-case DFGs are 7.4% and 9.2%, whereas
the MAPEs of Vivado HLS are 122.4% and 92.2%, respectively. For
DSP usage, the prediction accuracy is measured by root-mean-
square error (RMSE) since MAPE is not applicable when the ground
truth appears to be zero. GPP achieves 5.6 and 2.1 in RMSE for
synthetic and real-case DFGs, while Vivado HLS reaches 26.9 and
19.7, respectively. For CP timing, the MAPEs of GPP are 4.2% and
4.6% on synthetic and real-case DFGs, whereas the MAPEs of Vivado
HLS are 7.7% and 42.1%. On average, GPP reduces the prediction
error of Vivado HLS by 10.9× in resource and 5.7× in timing.

Pyramid [8] is also an ML-based framework for resource and
timing prediction. The major difference between GPP and Pyramid
is the features required for predictions. Pyramid needs 72 features
from HLS reports as inputs, which enforce the running of HLS to
get VHDL designs, possibly consuming hours for large designs;
whereas GPP can make high-accuracy predictions simply from raw
DFGs (within a second). Pyramid considers four ML models and an
ensemble of these four, none of which includes graphical structure.
The reported results show that the averaged prediction error of a
single ML model is 17.8% for resource and 17.3% for timing, with
the ensemble reaching 5.5% for resource and 4.1% for timing.

RLMD vs. GA/SA. Fig. 6 compares RLMD with GA and SA
regarding the Pareto solutions between LUTs and DSPs. Obviously,
RLMD outperforms GA and SA by a large margin. Given the same
number of DSPs, RLMD can find solutions reducing the LUT usage
by 12.7% and 12.9%, compared with SA and GA. After fine-tuning,
additional 11.6% reduction in LUT utilization is achieved.

These promising results show great potentials of applying RL
for DSE in HLS. Through trials and interactions with GPP and
user-specified constraints, RLMD is able to gradually understand
which directive should be assigned to which node, and proactively
learn proper resource allocation strategies by balanced exploration
and exploitation. In contrast, one underlying assumption in GA is
that the offspring of two strong individuals among a population is
often stronger, which is not the case in DSE for HLS problems, thus
reducing its effectiveness. Similarly, SA is a probabilistic technique
and uses meta-heuristic aiming to approximate the global optima,

Figure 6: Pareto solutions found by RLMD, SA and GA on
synthetic DFGs, with unchanged latency.

Training Process. To demonstrate the generalization capability
of IronMan across different DFGs and applications, GPP and RLMD
are trained on part of DFGs from the dataset and evaluated on rest of
them. The training set consists of 41 different topologies and 4 real-
case benchmarks (kernel_durbin and stencil3d), totally involving
4500 distinct DFGs.

GPP is trained via regression to minimize the mean squared loga-
rithmic errors for DSPs and CP timing, and the mean absolute errors
for LUTs, respectively. The supervised learning process and GNN
models enable GPP to identify features and necessary information
to generalize performance prediction and graph embeddings across
different DFGs. In terms of hyper-parameter selection, the GPP is
trained over 200 epochs with batchsize as 32. The Adam optimizer is
applied with an initial learning rate of 0.01 decaying exponentially.
Once GPP is trained, it is integrated with RLMD and the training
process of RLMD can be started. To train RLMD, we provide tuples
in the form of [𝐷𝐹𝐺𝑖𝑛𝑑𝑒𝑥 , 𝐷𝑆𝑃𝑡𝑎𝑟𝑔𝑒𝑡 ] to the RL agent. The optimiza-
tion goal is to maximize the average cumulative rewards on all of
the tuples, so that the agent can learn resource allocation strategies
under different DSP constraints and across different DFGs. There
have 1125 different tuples in total, and each tuple appears 8 times
during the training process, amounting to 9,000 episodes. As for
fine-tuing the RLMD on a particular DFG, we additionally conduct
500 episodes of training with various target DSPs. The parameters
in RLMD are also learned by Adam optimizer with the learning
rate of 0.008. We empirically set the discount rate 𝛾 = 0.95, and
the exploration rate 𝜖 = 0.08 decaying exponentially. In the reward
function, we have 𝛼 = 0.002, 𝛽 = 5, and 𝜆 = 0.02.

Figure 7: IronMan performance on real-case benchmarks. IronMan meets DSP constraints for 39 out of 40 cases, and meets
all 40 after fine-tuning, while SA, GA and Vivado HLS meet for 2, 6, and 0 cases, respectively.
which ignores past experiences and searches solutions to some
extent hinging on randomness, thus not always reliable.

IronMan vs. All. As depicted in Fig. 7, IronMan is fully eval-
uated by comparing with SA, GA and Vivado HLS on four real-case
benchmarks, whose sizes are 2 − 4× larger than synthetic DFGs. To
showcase IronMan capable to perfectly satisfy user specifications
without sacrificing latency, we specify different DSP constraints
within 20% to 80% of the maximal number of DSPs for each case.

Among four real-case benchmarks with 40 different DSP con-
straints in total, IronMan is able to meet 39 (97.5%) of them, and
can further improve to 40 (100%) by fine-tuning. Whereas SA, GA
and Vivado HLS only meet the constraints for 2 (5%), 6 (15%) and
0 cases, respectively. Specifically, IronMan on average consumes
98.5% of the targeted DSPs (improved to 99.3% with fine-tuning),
whereas those found by SA, GA and Vivado HLS use 1.29×, 1.43×,
and 2.54× targeted DSPs, respectively. Not only can IronMan meet
user-specified constraints much more accurately than its counter-
parts and HLS tools, but it always maintains the shortest latency
whereas Vivado HLS results in an increased latency by up to 6×.

Reducing DSPs without sacrificing latency is at the cost of in-
creased LUTs, because the DSP resource is often more critical in
FPGAs while LUTs are more adequate. By perfectly satisfying DSP
constraints, IronMan slightly increases the LUT usage by 1.2% and
3.0%, compared with SA and GA; with further fine-tuning, Iron-
Man achieves additional 8.9% LUT reduction, resulting in 7.7% and
5.9% lower LUT usage than SA and GA.

Execution Time. During inference, i.e., being applied on real
applications, IronMan only takes a few seconds for prediction and
solution generation. Vivado HLS takes tens of minutes to synthesize
C++ code, and up to hours to get the exact resource usage after
implementation. SA and GA take hours in average, as they struggle
to closely meet the DSP constraint, and cannot generalize across
different DFGs or DSP constraints. The fine-tuning for RL agent can
balance between a quick solution using the pre-trained model and a
longer yet better one for a particular DFG, which is optional and the
number of episodes is adjustable regarding users’ requirements.

5 CONCLUSION
IronMan is an end-to-end framework, aiming to help HLS tools
generate higher quality solutions under user-specified constraints,
or perform more flexible DSEs to provide Pareto solutions that are
not currently supported by HLS tools. IronMan is equipped with
a GNN-based performance predictor GPP, an RL-based DSE engine

RLMD, and a code transformer. Independently, GPP achieves high
prediction accuracy; RLMD obtains Pareto solutions surpassing GA
and SA. Integrated, IronMan is capable to find optimized solutions
perfectly matching various DSP constraints, with up to 400× faster
than the heuristic algorithms and HLS tools.

REFERENCES
[1] Cadance. Accessed:

Synthe-
https://www.cadence.com/en_US/home/tools/digital-design-and-

Stratus High-Level

Cadance

2021.

sis.
signoff/synthesis/stratus-high-level-synthesis.html.

[2] Jason Cong et al. 2012. Optimizing memory hierarchy allocation with loop

transformations for high-level synthesis. In 49th DAC.

[3] Steve Dai et al. 2018. Fast and accurate estimation of quality of results in high-

level synthesis with machine learning. In FCCM.

[4] Will Hamilton et al. 2017. Inductive representation learning on large graphs. In

NeurIPS.

[5] Yuko Hara et al. 2009. Proposal and quantitative analysis of the CHStone bench-
mark program suite for practical C-based high-level synthesis. JIP 17 (2009),
242–254.

[6] Thomas N Kipf and Max Welling. 2017. Semi-supervised classification with graph

convolutional networks. ICLR (2017).

[7] Johannes de Fine Licht et al. 2018. Transformations of High-Level Synthesis

Codes for High-Performance Computing. arXiv:1805.08288 (2018).

[8] Hosein Mohammadi Makrani et al. 2019. Pyramid: Machine Learning Framework
to Estimate the Optimal Timing and Resource Usage of a High-Level Synthesis
Design. In 29th FPL.

[9] Louis-Noël Pouchet and Tomofumi Yuki. 2016. PolyBench/C - the Polyhedral

Benchmark suite. http://web.cs.ucla.edu/~pouchet/software/polybench/.
[10] Brandon Reagen et al. 2014. MachSuite: Benchmarks for Accelerator Design and

Customized Architectures. In IISWC.

[11] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele
Monfardini. 2008. The graph neural network model. IEEE Transactions on Neural
Networks 20, 1 (2008), 61–80.

[12] Benjamin Carrion Schafer and Zi Wang. 2019. High-level synthesis design space

exploration: Past, present and future. IEEE TCAD (2019).

[13] Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An intro-

duction. MIT press.

[14] Peter JM Van Laarhoven and Emile HL Aarts. 1987. Simulated annealing. In

Simulated annealing: Theory and applications. Springer, 7–15.

[15] Vivado. Accessed: 2021. Vivado Design Suite - HLx Editions. https://www.xilinx.

com/products/design-tools/vivado.html.

[16] Darrell Whitley. 1994. A genetic algorithm tutorial. Statistics and computing 4, 2

(1994), 65–85.

[17] Nan Wu and Yuan Xie. 2021. A Survey of Machine Learning for Computer

Architecture and Systems. arXiv preprint arXiv:2102.07952 (2021).

[18] Xilinx. Accessed: 2021. Xilinx Vivado High-Level Synthesis. https://www.xilinx.

com/products/design-tools/vivado/integration/esl-design.html.

[19] Jieru Zhao et al. 2017. COMBA: A comprehensive model-based analysis frame-

work for high level synthesis of real applications. In ICCAD.

[20] Jieru Zhao et al. 2019. Machine learning based routing congestion prediction in

fpga high-level synthesis. In DATE.

[21] Guanwen Zhong et al. 2016. Lin-analyzer: a high-level performance analysis tool

for FPGA-based accelerators. In 53nd DAC.

[22] Wei Zuo et al. 2013. Improving high level synthesis optimization opportunity

through polyhedral transformations. In FPGA.

