2
2
0
2

b
e
F
4

]

G
L
.
s
c
[

6
v
6
8
3
8
0
.
6
0
9
1
:
v
i
X
r
a

Journal of Machine Learning Research 23 (2022) 1-26

Submitted 11/21; Published 1/22

Inherent Tradeoffs in Learning Fair Representations

Han Zhao∗
University of Illinois at Urbana-Champaign

Geoffrey J. Gordon
Carnegie Mellon University

Editor: Maya Gupta

HANZHAO@ILLINOIS.EDU

GGORDON@CS.CMU.EDU

Abstract
Real-world applications of machine learning tools in high-stakes domains are often regulated to
be fair, in the sense that the predicted target should satisfy some quantitative notion of parity with
respect to a protected attribute. However, the exact tradeoff between fairness and accuracy is not
entirely clear, even for the basic paradigm of classification problems. In this paper, we characterize
an inherent tradeoff between statistical parity and accuracy in the classification setting by providing
a lower bound on the sum of group-wise errors of any fair classifiers. Our impossibility theorem
could be interpreted as a certain uncertainty principle in fairness: if the base rates differ among
groups, then any fair classifier satisfying statistical parity has to incur a large error on at least
one of the groups. We further extend this result to give a lower bound on the joint error of any
(approximately) fair classifiers, from the perspective of learning fair representations. To show
that our lower bound is tight, assuming oracle access to Bayes (potentially unfair) classifiers, we
also construct an algorithm that returns a randomized classifier which is both optimal (in terms of
accuracy) and fair. Interestingly, when the protected attribute can take more than two values, an
extension of this lower bound does not admit an analytic solution. Nevertheless, in this case, we
show that the lower bound can be efficiently computed by solving a linear program, which we term
as the TV-Barycenter problem, a barycenter problem under the TV-distance.

On the upside, we prove that if the group-wise Bayes optimal classifiers are close, then learning
fair representations leads to an alternative notion of fairness, known as the accuracy parity, which
states that the error rates are close between groups. Finally, we also conduct experiments on
real-world datasets to confirm our theoretical findings.
Keywords: Algorithmic fairness, representation learning, information theory

1. Introduction

With the prevalence of machine learning applications in high-stakes domains, e.g., criminal judge-
ment, medical testing, online advertising, etc., it is crucial to ensure that the automated decision
making systems do not propagate existing bias or discrimination that might exist in historical
data (Barocas and Selbst, 2016; Berk et al., 2018). Among many recent proposals for achieving
different notions of algorithmic fairness (Zemel et al., 2013; Dwork et al., 2012; Zafar et al., 2015;
Hardt et al., 2016; Zafar et al., 2017), learning fair representations has received increasing attention
due to recent advances in learning rich representations with deep neural networks (Edwards and

*. This work is an extended version of an earlier paper with the same title appearing in NeurIPS 2019 by the same

authors.

©2022 Han Zhao, Geoffrey J. Gordon.

License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
http://jmlr.org/papers/v23/21-1427.html.

 
 
 
 
 
 
ZHAO AND GORDON

Storkey, 2015; Louizos et al., 2015; Madras et al., 2018; Zhang et al., 2018; Beutel et al., 2017;
Song et al., 2019; Zhao et al., 2019c; Chi et al., 2021). In fact, a line of work has proposed to learn
group-invariant representations with adversarial learning techniques in order to achieve statistical
parity, also known as the demographic parity in the literature. This line of work dates at least back
to Zemel et al. (2013) where the authors proposed to learn predictive models that are independent
of the group membership attribute. At a high level, the underlying idea is that if representations of
instances from different groups are similar to each other, then any predictive model on top of them
will certainly make decisions independent of group membership.

On the other hand, it has long been observed that there is an underlying tradeoff between
accuracy and statistical parity. In particular, it is easy to see that in an extreme case where the
group membership coincides with the target variable to predict, a call for exact statistical parity will
inevitably remove the perfect predictor (Hardt et al., 2016). Empirically, it has also been observed
that a tradeoff exists between accuracy and fairness in binary classification (Zliobaite, 2015). Clearly,
methods based on learning fair representations are also bound by such inherent tradeoff between
accuracy and fairness. But before attempting to develop an algorithm to achieve a particular goal on
fairness, it is natural to ask:

Q1 How does the fairness constraint trade for accuracy? Without further assumptions on the data

generating distributions, what is the exact price any fair classifiers have to pay for fairness?

Q2 Furthermore, given the underlying distribution, can we construct an algorithm to return the

optimal (in terms of accuracy) fair classifier?

Q3 Will learning fair representations help to achieve other notions of fairness besides the statistical
parity? If yes, what is the fundamental limit of accuracy that we can hope to achieve under
such constraint?

To answer the above questions, through the lens of information theory, in this paper we provide
the first result that quantitatively characterizes the tradeoff between demographic parity and the
sum of group-wise accuracy across different population groups. Specifically, when the base rates
differ between groups, we provide a tight information-theoretic lower bound on the joint error across
these groups. Our lower bound is algorithm-independent so it holds for all methods that satisfy
statistical parity. We also extend this result to prove a lower bound on the joint accuracy for any fair
classifiers, and generalize it to the case where the protected attribute can take any finite number of
values. Interestingly, when the number of groups defined by the protected attribute is more than two,
we can no long obtain an analytic lower bound. Nevertheless, we show that the lower bound can be
efficiently computed by solving a linear program, which we term as the TV-Barycenter problem, a
barycenter problem under the TV-distance. To show that our lower bound is tight, assuming oracle
access to Bayes (potentially unfair) classifiers, we derive an algorithm that returns a randomized
classifier which is both optimal (in terms of accuracy) and fair.

When only approximate statistical parity is achieved, we present a family of lower bounds to
quantify the tradeoff of accuracy introduced by such approximate constraint. As a side contribution,
our proof technique is simple but general, and we expect it to have broader applications in other
learning problems using adversarial techniques, e.g., unsupervised domain adaptation (Ganin et al.,
2016; Zhao et al., 2019b), privacy-preservation under attribute inference attacks (Hamm, 2017; Zhao
et al., 2019a) and multilingual machine translation (Johnson et al., 2017; Zhao et al., 2020).

2

INHERENT TRADEOFFS IN LEARNING FAIR REPRESENTATIONS

To complement our negative results, we show that if the (potentially unfair) Bayes optimal
classifiers across different groups are close, then learning fair representations helps to achieve an
alternative notion of fairness, i.e., the accuracy parity (Buolamwini and Gebru, 2018), which states
that the error rates are close between different groups. Empirically, we conduct experiments on a
real-world dataset to corroborate both our positive and negative results. We believe our theoretical
insights contribute to better understanding of the tradeoff between accuracy and different notions of
fairness, and they are also helpful in guiding the future design of representation learning algorithms
to achieve algorithmic fairness.

2. Preliminaries

We first introduce the notation used throughout the paper and formally describe the problem setup.
We then briefly discuss some information-theoretic concepts that will be used in our analysis.

2.1 Notation

{

}

}

0, 1

∈ {

∈ X ⊆
=
∈ Y

Rd is the input vector, A
0, 1

We consider a general classification setting where there is a joint distribution µ over the triplet
1 is the protected attribute, e.g.,
T = (X, A, Y), where X
is the target output. Lower case letters x, a and y are used
race, gender, etc., and Y
to denote the instantiation of X, A and Y, respectively. Let
be a hypothesis class of predictors
from input to output space. Throughout the paper, we focus on the setting where the classifier cannot
directly use the sensitive attribute A to form its prediction. However, note that even if the classifier
does not explicitly take the protected attribute A as input, this fairness through blindness mechanism
can still be biased due to the redundant encoding issue (Barocas et al., 2017). To keep the notation
uncluttered, for a
, we use µa to mean the conditional distribution of µ given A = a. We
use µ(Y) to denote the marginal distribution of Y from a joint distribution µ over Y and some other
random variables. With slight abuse of notation, occasionally we also use Y(cid:93)µ to denote the marginal
distribution of Y from the joint distribution µ, i.e., projection of µ onto the Y coordinate.

∈ {

0, 1

H

}

For an event E, µ(E) denotes the probability of E under µ. In particular, in the literature of
BR(µ, µ(cid:48)) :=
to denote the difference of the base rates between two distributions µ and

fair machine learning, we call µ(Y = 1) the base rate of distribution µ and we use ∆
µ(Y = 1)
|
µ(cid:48) over the same sample space.

µ(cid:48)(Y = 1)

−

|

, we define g(cid:93)µ := µ

Given a feature transformation function g :
to feature space

that maps instances from the input space
X → Z
1 to be the induced (pushforward) distribution of
X
Z
).
, g(cid:93)µ(E(cid:48)) := µ(g−
µ under g, i.e., for any event E(cid:48) ⊆ Z
E(cid:48)}
x
The zero-one entropy of A (Grünwald et al., 2004, Section 3.5.3) is denoted as H0-1(A) :=
Pr(A = a). Furthermore, we use Fµ to represent the cumulative distribution
1
∈{
function of µ, i.e., for z

R, Fµ(z) := Prµ((

1(E(cid:48))) = µ(

∞, z]).

∈ X |

maxa

g(x)

g−

−

∈

0,1

{

◦

}

∈

−

2.2 Group Fairness

Given a joint distribution µ, the error of a predictor h under µ is defined as Errµ(h) := Prµ(Y
=
h(X)). To make the notation more compact, we may drop the subscript µ when it is clear from
the context. In this work we focus on group fairness where the group membership is given by the

1. Our main results could be extended to the case where A can take finitely many values. We show this extension in

Section 3.2

3

(cid:54)
ZHAO AND GORDON

sensitive attribute A. Even in this context there are many possible definitions of fairness (Narayanan,
2018), and in what follows we provide a brief review of the ones that are mostly relevant to this work.

Definition 1 (Demographic Parity) Given a joint distribution µ, a classifier (cid:98)Y satisfies demo-
graphic parity if (cid:98)Y is independent of A.

Demographic parity reduces to the requirement that µ0( (cid:98)Y = 1) = µ1( (cid:98)Y = 1), i.e., positive outcome
is given to the two groups at the same rate. When exact equality does not hold, we use the absolute
difference between them as an approximate measure:

Definition 2 (DP Gap) Given a joint distribution µ, the demographic parity gap of a classifier (cid:98)Y is
∆

DP( (cid:98)Y) :=

µ0( (cid:98)Y = 1)

µ1( (cid:98)Y = 1)

|

−

.

|

Demographic parity is also known as statistical parity, and it has been adopted as definition of
fairness in a series of seminal works (Calders et al., 2009; Edwards and Storkey, 2015; Johndrow
et al., 2019; Kamiran and Calders, 2009; Kamishima et al., 2011; Louizos et al., 2015; Zemel et al.,
2013; Madras et al., 2018). However, as we shall quantify precisely in Section 3, demographic parity
may reduce the accuracy that we hope to achieve, especially in the common scenario where the base
rates differ between two groups, e.g., µ0(Y = 1)
= µ1(Y = 1). In light of this, an alternative
definition is accuracy parity:

Definition 3 (Accuracy Parity) Given a joint distribution µ, a classifier h satisfies accuracy parity
if Errµ0(h) = Errµ1(h).

In the literature, a break of accuracy parity is also known as disparate mistreatment (Zafar et al., 2017).
Again, when h is a binary classifier, accuracy parity reduces to µ0(h(X) = Y) = µ1(h(X) = Y).
Different from demographic parity, the definition of accuracy parity does not eliminate the perfect
predictor when Y = A when the base rates differ between two groups. When costs of different error
types matter, more refined definitions exist:

|

|

Definition 4 (Equalized Odds (Hardt et al., 2016)) Given a joint distribution µ, a classifier h
satisfies equalized odds if µ0(h(X) = 1

Y = y) = µ1(h(X) = 1

Y = y),

0, 1

y

.

|

∀

∈ {

}

Y = 1) = µ1(h(X) = 1

Equalized odds essentially requires equal true positive and false positive rates between different
groups. Furthermore, Hardt et al. (2016) also defined true positive parity, or equal opportunity, to be
Y = 1) when the positive outcome is more desirable in
µ0(h(X) = 1
certain applications. For example, in school admission, the cost of denying a competent candidate is
considerably higher than the other way around. Last but not least, predictive rate parity, also known
as test fairness (Chouldechova, 2017), asks for equal chance of positive outcomes across groups
given predictions:

|

|

Definition 5 (Predictive Rate Parity) Given a joint distribution µ, a probabilistic classifier h sat-
isfies predictive rate parity if µ0(Y = 1

h(X) = c) = µ1(Y = 1

h(X) = c),

[0, 1].

c

|

∀

∈

A closely related notion of predictive rate parity is known as statistical calibration. Formally, a
classifier h is said to be calibrated if µ(Y = 1
[0, 1], i.e., if we look at
|
the set of data that receive a predicted probability of c by h, we would like c-fraction of them to be
positive instances according to Y (Pleiss et al., 2017). Hence, it is clear to see that if a classifier h is
calibrated across different subgroups, then it also satisfies predictive rate parity.

h(X) = c) = c,

∈

∀

c

4

(cid:54)
INHERENT TRADEOFFS IN LEARNING FAIR REPRESENTATIONS

In the special case when h is a deterministic binary classifier that only takes value in

,
}
Chouldechova (2017) showed an intrinsic incompatibility between predictive rate parity and equalized
odds:

0, 1

{

Theorem 6 (Chouldechova (2017)) Assume µ0(Y = 1)
classifier h :
that is not perfect, i.e., h(X)
X → {
parity cannot hold simultaneously.

0, 1

}

= µ1(Y = 1), then for any deterministic
= Y, equalized odds and predictive rate

Similar incompatibility result for probabilistic classifier has also been proved by Kleinberg et al.
(2016), where the authors showed that for any non-perfect predictors, statistical calibration and
equalized odds cannot be achieved simultaneously if the base rates are different across groups.

2.3 f -divergence

Introduced by Ali and Silvey (1966) and Csiszár (1964, 1967), f -divergence, also known as the
Ali-Silvey distance, is a general class of statistical divergences to measure the difference between
two probability distributions

over the same probability space.

and

P

Q
and
is absolutely continuous w.r.t.

Definition 7 ( f -divergence) Let
assume
that is strictly convex at 1 and f (1) = 0, the f -divergence of
(cid:18) d
d

) := E

P (cid:28) Q

P (cid:107) Q

Q
Q

D f (

P

P

(cid:20)

Q

f

P
Q

be two probability distributions over the same space and
R
). Then for any convex function f : (0,∞)
(

→

is defined as

from

Q
(cid:19)(cid:21)

P

.

(1)

The function f is called the generator function of D f (

).

· (cid:107) ·

=

P (cid:107) Q

) = 0 iff

Different choices of the generator function f recover popular statistical divergence as special cases,
e.g., the KL-divergence. From Jensen’s inequality it is easy to verify that D f (
0 and
almost surely. Note that f -divergence does not necessarily leads to
D f (
a distance metric, and it is not symmetric in general, i.e., D f (
) provided
. We list some common choices of the generator function f and their
that
corresponding properties in Table 1. Notably, Khosravifard et al. (2007) proved that among all the
f -divergences, total variation is the only f -divergence that serves as a metric, i.e., satisfying the
triangle inequality.

P (cid:28) Q

Q (cid:28) P

= D f (

P (cid:107) Q

P (cid:107) Q

Q (cid:107) P

and

Q

≥

P

)

)

3. Tradeoff between Fairness and Accuracy

In this section we take a slight detour from learning fair representations to first provide general
results on the tradeoff between fairness and accuracy that applies to any fair classifiers. As we
briefly discussed in Section 2.2, it is impossible to have imperfect predictor that is both statistically
calibrated and verifies equalized odds when the base rates differ between two groups. On the other
hand, while it has long been observed that demographic parity may eliminate perfect predictor (Hardt
et al., 2016), and previous work has empirically verified that tradeoff exists between accuracy and
demographic parity (Calders et al., 2009; Kamiran and Calders, 2009; Zliobaite, 2015) on various
datasets, so far a quantitative characterization on the exact tradeoff between accuracy and various
notions of group fairness is still missing in the classification setting. In this section, we seek to
answer the following intriguing and important question:

5

(cid:54)
(cid:54)
(cid:54)
ZHAO AND GORDON

Table 1: List of different f -divergences and their corresponding properties. DKL(
the KL-divergence of
P
Symm. stands for symmetric and Tri. stands for triangle inequality.

P (cid:107) Q
)/2 is the average distribution of

from

:= (

and

M

Q

Q

+

P

P

) denotes
.
and

Q

)

Name

P (cid:107) Q

D f (
Kullback-Leibler DKL(
P (cid:107) Q
Reverse-KL
DKL(
Q (cid:107) P
Jensen-Shannon DJS(
Q
P
Squared Hellinger H2(
,
Q
P
Total Variation
dTV(
,
Q
P

,

)
)
) := 1
2 (DKL(
ş
) := 1
(√d
2
) := supE |P

P (cid:107)M
√d
P −
(E)

) + DKL(
)2
(E)

Q
− Q

|

Q(cid:107)M

t log t
log t
−
)) t log t
(1
t

|

−
√t)2/2
−
/2
1
−

|

(t + 1) log( t+1

(cid:55)
(cid:55)
2 ) (cid:51)
(cid:51)
(cid:51)

(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:51)

Generator f (t)

Symm. Tri.

In the setting of classification, what is the minimum error that any fair algorithm has
to incur, and how does this error depend on the coupling between the target and the
protected attribute?

In what follows we shall first provide a simple example to illustrate this general tradeoff. This
example will give readers a flavor the kind of impossibility result we are interested in obtaining. We
then proceed to formally present a family of information-theoretic lower bounds on the accuracy
that hold for all algorithms, even if only approximate statistical parity is satisfied. We conclude this
section by some discussions on the implications of our results.

A Simple Example As a warm-up, let us consider an example to showcase the potential tradeoff
between statistical parity and accuracy in binary classification. But before our construction, it should
be noted that the error Errµ( (cid:98)Y) bears an intrinsic lower bound for any classifier (cid:98)Y = h(X), i.e.,
the noise in the underlying data distribution µ, e.g., the Bayes error rate. Hence to simplify our
discussion, in this example we shall construct distributions such that the Bayes error rates are 0, i.e.,
for a
, there exists a ground-truth labeling function h∗a such that Y = h∗a (X) on µa. Realize
that such simplification will only make it harder for us to prove lower bound on Errµa since there
exists classifiers that are perfect.

∈ {

0, 1

}

, let the
Example 1 (A bijection between the target and the protected attribute) For a
. Let Y = a be a constant. Hence by
marginal distribution X(cid:93)µa be a uniform distribution over
construction, on the joint distribution, we have Y = A hold. Now for any fair predictor (cid:98)Y = h(X),
the statistical parity asks (cid:98)Y to be independent of A. However, no matter what value h(x) takes, we
always have

1. Hence for any predictor h :

h(x)

h(x)

∈ {

0, 1

0, 1

0,1

+

{

}

}

1

:

|

|

|

−

Errµ0(h) + Errµ1(h) =

h(0)

+

0

|

−

1
2 |

h(1)

+

0

|

−

1
2 |

h(0)

1

|

−

+

h(1)

1

|

−

X → {

}
1
2 |

| ≥
1
2 |
1
≥
2
= 1.

+

1
2

This shows that for any fair predictor h, the sum of the errors of h on both groups has to be at least 1.
On the other hand, there exists a trivial unfair algorithm that makes no error on both groups by also
, h∗(x) = 0 if A = 0 else h∗(x) = 1.
taking the protected attribute into consideration:

0, 1

x

∀

∈ {

}

6

INHERENT TRADEOFFS IN LEARNING FAIR REPRESENTATIONS

3.1 An Accuracy Lower Bound for Fair Classifiers

In this subsection we generalize the above simple example to general cases without making explicit
assumptions on the underlying data generating distributions. Essentially, every prediction function
induces the following Markov chain:

X

g
−→

Z

h
−→

(cid:98)Y,

where g is the feature transformation, h is the classifier on feature space, Z is the feature and (cid:98)Y is the
predicted target variable by h
g. Note that simple models, e.g., linear classifiers, are also included
by specifying g to be the identity map. With this notation, we first state the following theorem that
quantifies an inherent tradeoff between fairness and accuracy.

◦

Theorem 8 Let (cid:98)Y = h(g(X)) be a predictor. If (cid:98)Y satisfies demographic parity, then Errµ0(h
Errµ1(h

BR(µ0, µ1).

g)

∆

◦

≥

g) +

◦

Remark It is worth pointing out that Theorem 8 holds for any representation function g and
classifier h, as long as the final predictor (cid:98)Y satisfies demographic parity. In particular, by restricting
g to be the identity function, we see that the lower bound also holds for any classifier that directly
acts on the original input data. We choose the current presentation using a composition function
h

g only for consistency with the rest of the paper.
Next, ∆

◦

BR(µ0, µ1) is the difference of base rates across groups, and it achieves its maximum
value of 1 iff there exists a bijection between Y and A, e.g., Y = A. On the other hand, if Y is
independent of A, then ∆
BR(µ0, µ1) = 0 so the lower bound does not make any constraint on the
joint error. Hence, the lower bound could also be understood as an uncertainty principle in fairness,
stating in general that when the difference of base rates is large, then any fair algorithm has to incur a
large error on at least one of the subgroups. Lastly, although Theorem 8 asks for exact demographic
parity, in Section 5 we shall extend the above theorem when only approximate demographic parity is
met, via learning fair representations.

Note that from Example 1, we can see this lower bound is tight, in the sense that there exist
problem instances where the equality is verified. Second, Theorem 8 applies to all possible feature
transformation g and predictor h. In particular, if we choose g to be the identity map, then Theorem 8
says that when the base rates differ, no algorithm can achieve a small joint error on both groups,
and it also recovers the previous observation that demographic parity can eliminate the perfect
predictor (Hardt et al., 2016). Third, the lower bound in Theorem 8 is insensitive to the marginal
distribution of A, i.e., it treats the errors from both groups equally. As a comparison, let α := µ(A =
g) could still be
1), then Errµ(h
g) + αErrµ1(h
g),
small even if the minority group suffers a large error. More formally, for the joint error Errµ(h
we have the following corollary hold:

g). In this case Errµ(h

α)Errµ0(h

g) = (1

−

◦

◦

◦

◦

◦

Corollary 9 Let (cid:98)Y = h(g(X)) be a predictor. If (cid:98)Y satisfies demographic parity, then the joint error
has the following lower bound: Errµ( (cid:98)Y)
H0-1(A)

BR(µ0, µ1).

∆

≥

·

Compared with the lower bound in Theorem 8, the lower bound of the joint error in Corollary 9
additionally depends on the zero-one entropy of A. In particular, if the marginal distribution of A is
skewed, then H0-1(A) will be small, which means that fairness will not reduce the joint accuracy
too much. This corollary further implies that when the demographic subgroups are imbalanced in
the overall population, the joint accuracy is not an ideal metric to look at, since it may hide the

7

ZHAO AND GORDON

potentially large drop in accuracy of the minority group. In particular, by the pigeonhole principle,
the following corollary holds:

Corollary 10 If the predictor (cid:98)Y = h(g(X)) satisfies demographic parity, then max
g), Errµ1(h

BR(µ0, µ1)/2.

g)

∆

◦

} ≥

Errµ0(h

{

◦

In words, this means that for fair predictors in the demographic parity sense, at least one of the
subgroups has to incur an error of at least ∆
BR(µ0, µ1)/2, which further emphasizes the fundamental
role of the difference of base rates, ∆

BR(µ0, µ1), in the tradeoff between fairness and accuracy.

Proofs of Theorem 8, Corollary 9 Before we present the proof, we first present a useful lemma
that lower bounds the prediction error by the total variation distance.

Lemma 11 Let (cid:98)Y = h(X) be a predictor, then for a

0, 1

}

∈ {

, dTV(µa(Y), µa( (cid:98)Y))

Errµa (h).

≤

Proof For a

0, 1

}

∈ {

, because both µa(Y) and µa( (cid:98)Y) are Bernoulli distributions, we have:

µa(Y = 1)
dTV(µa(Y), µa( (cid:98)Y)) =
|
= (cid:12)
(cid:12)Eµa [Y]
−
Eµa [
Y
≤
−
= Errµa (h),

|

µa(h(X) = 1)
−
Eµa [h(X)](cid:12)
(cid:12)
h(X)
]

|

|

where the last equality holds because Prµa (Y
Now we are ready to prove Theorem 8:
Proof [Proof of Theorem 8] First of all, we show that if (cid:98)Y = h(g(X)) satisfies demographic parity,
then:

] when Y, (cid:98)Y

h(X)

∈ {

0, 1

−

Y

}

|

|

.

= (cid:98)Y) = Eµa [

dTV(µ0( (cid:98)Y), µ1( (cid:98)Y)) = max (cid:8)

µ0( (cid:98)Y = 0)

µ1( (cid:98)Y = 0)

|
µ0( (cid:98)Y = 1)
µ( (cid:98)Y = 1

|

=

=

|

|

−
µ1( (cid:98)Y = 1)

−
A = 0)

|
µ( (cid:98)Y = 1

−

A = 1)

= 0,

|

|

µ0( (cid:98)Y = 1)

,

|

|

−

µ1( (cid:98)Y = 1)

(cid:9)

|

where the last equality follows from the definition of demographic parity. Now from Table 1, dTV(
is symmetric and satisfies the triangle inequality, we have:

,

·

)

·

dTV(µ0(Y), µ1(Y))

dTV(µ0(Y), µ0( (cid:98)Y)) + dTV(µ0( (cid:98)Y), µ1( (cid:98)Y)) + dTV(µ1( (cid:98)Y), µ1(Y))

≤
= dTV(µ0(Y), µ0( (cid:98)Y)) + dTV(µ1( (cid:98)Y), µ1(Y)).

(2)

The last step is to bound dTV(µa(Y), µa( (cid:98)Y)) in terms of Errµa (h

g) for a

◦

0, 1

}

∈ {

using Lemma 11:

dTV(µ0(Y), µ0( (cid:98)Y))

Errµ0(h

◦

≤

g),

dTV(µ1(Y), µ1( (cid:98)Y))

Errµ1(h

g).

◦

≤

Combining the above two inequalities and (2) completes the proof.

We now provide the proof of Corollary 9 on the lower bound of the joint error.

8

(cid:54)
INHERENT TRADEOFFS IN LEARNING FAIR REPRESENTATIONS

Proof [Proof of Corollary 9] To simplify the notation used in the proof, define ε := Errµ( (cid:98)Y),
ε0 := Errµ0( (cid:98)Y) and ε1 := Errµ1( (cid:98)Y). Let α := Prµ(A = 0). By Theorem 8, we know that
ε0 + ε1 ≥

BR(µ0,µ1). By definition of the joint error:

∆

ε = αε0 + (1
αε0 + (1
α)∆

≥
= (1

α)ε1
α)(∆

−
BR(µ0,µ1)
−
BR(µ0, µ1) + (2α

−

ε0)
1)ε0.

−
−

Similarly, we can also lower bound the joint error by:

Now we discuss in two cases. If α

ε

α∆

BR(µ0, µ1) + (1

2α)ε1.

−

1/2, considering the second inequality yields:

≥

≤

ε

≥

α∆

BR(µ0, µ1) + (1

2α)ε1 ≥

−

α∆

BR(µ0, µ1).

If α > 1/2, using the first inequality we have:

ε

(1

α)∆

BR(µ0, µ1) + (2α

≥
Combining the above two cases leads to:

−

1)ε0 ≥

−

(1

−

α)∆

BR(µ0, µ1).

ε

≥
completing the proof.

min

α, 1

α

} ·

−

{

∆

BR(µ0, µ1) = H0-1(A)

∆

BR(µ0, µ1),

·

It is not hard to show that our lower bound in Theorem 8 is tight. To see this, consider the
case A = Y, where the lower bound achieves its maximum value of 1. Now consider a constant
0, which clearly satisfies demographic parity by definition. But in this
predictor (cid:98)Y
g) = 1, hence
case either Errµ0(h
Errµ0(h

g) = 0 or Errµ0(h
1, achieving the lower bound.

≡
g) = 1, Errµ1(h

g) = 0, Errµ1(h

g) + Errµ1(h

1 or (cid:98)Y

g)

≡

◦

◦

◦

◦

To conclude this section, we point out that the choice of total variation in the lower bound is not
unique. As we will see shortly in Section 5, similar lower bounds could be attained using specific
choices of the general f -divergence with some desired properties.

≡

◦

◦

3.2 Extension to Multiple Subgroups under Multi-class Classification

= 2) with binary protected attribute (

The analytical tradeoff lower bound in Theorem 8 mainly works for the setting of binary classification
= 2). Hence it is natural to ask whether such lower
(
|Y |
bounds also exist in the general setting where the target variable Y is a discrete random variable that
takes m
2 values and the protected attribute A is also a categorical random variable that takes
2 different values. In what follows we shall provide an extension of Theorem 8 to this general
n
setting, although in this case we can no longer obtain an analytical characterization of the lower
bound. However, as we shall see shortly, the exact lower bound could still be efficiently computed
by solving a linear program.

|A|

≥

≥

We first introduce some additional notation that will be used in this section. For a positive
K to denote the K-dimensional
1, . . . , K

N∗, we use [K] to denote the set

. We use ∆

integer K

∈

{

}

9

ZHAO AND GORDON

Figure 1: An example of the TV-Barycenter problem in (3) where n = 5 and m = 3. The optimal
that minimizes
solution vector q∗ corresponds to the barycenter in the convex hull of
the sum of (cid:96)1 distances between p∗ and pi, i

p1, . . . , p5}

[5].

{

∈

probability simplex, i.e., p
[n],
the corresponding marginal distribution of the target label Y could then be described by a m-
m, i.e., µi(Y) = pi. With these notation, we could then formally establish
dimensional vector pi ∈
the following optimization problem:

[K] pi = 1. For each subgroup A = i, i

∈

∈

∈

∆

∈

i

∆K if p

RK

+ and ∑

(TV-Barycenter) :

min
q

1
2

n
∑
i=1 (cid:107)

q

pi(cid:107)1 =

−

1
2

n
∑
i=1

m
∑
j=1 |

(pi)j −

qj|

subject to q

∆

m : q

0,

≥

∈

m
∑
j=1

qj = 1

(3)

∈

{

We use OPT(
[n]) to denote the optimal value of the optimization problem in (3). As the name
pi}i
suggests, the above optimization problem computes the barycenter (under the (cid:96)1 distance) q∗ of
the set of vectors
m corresponds to the marginal label distribution of
[n], where each pi ∈
Y within the group A = i. Clearly, the TV-Barycenter problem is a linear program, and hence its
optimal solution could be efficiently computed in polynomial time.

pi}i

∆

{

∈

We now state the extension of Theorem 8 using the optimal solution to the TV-Barycenter

problem:

Theorem 12 Define pi to be the probability mass vector of µi(Y):
∈
Prµi (Y = j). Let (cid:98)Y = h(X) be a predictor. If (cid:98)Y satisfies demographic parity, then ∑n
i=1 Errµi (h)
OPT(

[n], j

[m], (pi)j =

≥

∈

∀

i

pi}i

{

∈

[n]).

10

(1,0,0)(0,1,0)(0,0,1)q∗p1p2p3p4p5INHERENT TRADEOFFS IN LEARNING FAIR REPRESENTATIONS

+

1
2

1
2

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

p1 −

p1 + p2
2

Remark To see that Theorem 12 is indeed a generalization of Theorem 8, note that when there are
only two groups, i.e., n = 2, we can readily read off the optimal solution q∗ as: q∗ = (p1 + p2)/2
by realizing that the objective function is fully decomposable, and OPT(

pi}i
(cid:13)
(cid:13)
p2(cid:107)1 = dTV(µ1(Y), µ2(Y)).
(cid:13)
(cid:13)1
Furthermore, when m = 2, dTV(µ1(Y), µ2(Y)) = ∆
BR(µ1, µ2). In general when n > 2, we cannot
expect to have an analytic solution of OPT(
[n]), but nevertheless it can be computed efficiently
∈
by solving a linear program. From this perspective, Theorem 12 builds an equivalent connection
between the tradeoff problem in fairness and the barycenter problem in TV-distance.
Proof [Proof of Theorem 12] For (cid:98)Y = h(X), let C(i)
and (cid:98)Y under µi,
Hence,

be the confusion matrix between Y
= Prµi (Y = j, (cid:98)Y = j(cid:48)).

[n]. By definition of the confusion matrix, we have C

p1 + p2
2

p1 −

p2 −

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

[n]) is

1
2 (cid:107)

pi}i

Rm
+

(i)
jj(cid:48)

=

∈

∈

∀

{

{

×

m

∈

i

(Y

= (cid:98)Y) = 1

Pr
µi

−

(Y = (cid:98)Y = j) = 1

Pr
µi

tr(C(i)).

−

m
∑
j=1

On the other hand, consider dTV(µi(Y), µi( (cid:98)Y)), we have

(Y = j)

Pr
µi

Pr
µi

−

( (cid:98)Y = j)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

dTV(µi(Y), µi( (cid:98)Y)) =

=

≤

1
2

1
2

1
2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

m
∑
j=1

m
∑
j=1

m
∑
j=1

C

(i)
j(cid:48) j

C

(i)
jj(cid:48) −

∑
[m]
j(cid:48)∈
∑
=j
j(cid:48)(cid:54)
tr(C(i)) = Pr
µi

(i)
jj(cid:48)

1
2

+

C

∑
[m]
j(cid:48)∈
m
∑
j=1

(Y

C

∑
=j
j(cid:48)(cid:54)
= (cid:98)Y).

= 1

−

(C

(i)
jj(cid:48) −

C

(i)
j(cid:48) j )

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

m
∑
j=1

=

1
2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
j(cid:48) j = ∑

(i)

C

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∑
=j
j(cid:48)(cid:54)
(i)
jj(cid:48)

j

=j(cid:48)

Next, since (cid:98)Y satisfies demographic parity, so (cid:98)Y
⊥
Combining the above two arguments together yields

A, which means ν := µ1( (cid:98)Y) =

= µn( (cid:98)Y).

· · ·

inf
(cid:98)Y=h(X)

∑
[n]

∈

i

Errµi ( (cid:98)Y) = inf

(cid:98)Y=h(X)

inf
(cid:98)Y=h(X)

≥

= inf

i

i

dTV(µi(Y), µi( (cid:98)Y))

(Y

Pr
µi

= (cid:98)Y)

∑
[n]
∈
∑
[n]
∈
∑
[n]
dTV(µi(Y), ν)

dTV(µi(Y), ν)

(cid:98)Y=h(X)

≥

inf
∆
ν
m
∈

= min
∆
q
m
∈
= OPT(

i

i
∈
∑
[n]
n
∑
i=1 (cid:107)
pi}i

{

∈
1
2

q

pi(cid:107)1

−

[n]),

∈

completing the proof.

11

(cid:54)
(cid:54)
(cid:54)
(cid:54)
ZHAO AND GORDON

Algorithm 1 Optimal fair classifier
Input: Oracle access to h∗0 and h∗1, the Bayes optimal classifiers over µ0 and µ1
Output: A randomized optimal fair classifier h∗Fair :
1: Compute α := Prµ0(Y = 1) and β := Prµ1(Y = 1). Without loss of generality assume α
2: For (x, a), randomly sample s
3: Construct h∗Fair(x, a) as

U(0, 1), the uniform distribution between (0,1)

X × A → Y

∼

h∗Fair(x, a) :=






a = 0 :

a = 1 :

(cid:40)

(cid:40)

0
1
0

1

return h∗Fair

4. An Optimal Fair Classifier

If h∗0(x) = 0 or h∗0(x) = 1 and s > α+β
If h∗0(x) = 1 and s
If h∗1(x) = 0 and s > α
2(1
If h∗1(x) = 1 or h∗1(x) = 0 and s

α+β
2α

β
β)

−
−

≤

2α

α
2(1

β
β)

−
−

≤

β

≥

(4)

Theorem 8 provides an information-theoretic lower bound on the sum of group-wise errors for any
fair classifiers. From the proof of Theorem 8, it is clear that the same lower bound also holds for fair
classifiers that can have explicit access to the protected attribute A. To see this, consider a special
case where the input X contains a redundant attribute that is a synonym of the protected attribute
A. Since Theorem 8 holds for any distribution µ over the triplet (X, A, Y), this simple observation
implies that our lower bound also holds for fair classifiers that take the protected attribute A as an
input explicitly.

Although in the last section we briefly mention the tightness of Theorem 8 by constructing
problem instances and fair classifiers where the equality verifies, it is still unclear whether it is
possible to construct an algorithm such that:

1. For any distribution µ over (X, A, Y), the algorithm returns a (possibly randomized) fair

classifier h∗Fair.

2. The returned fair classifier h∗Fair is optimal, in the sense that it verifies the lower bound in

Theorem 8: Errµ0(h∗Fair) + Errµ1(h∗Fair) = ∆

BR(µ0, µ1).

It should be noted that in general it is relatively easy to construct a trivial classifier that is fair
in the demographic parity sense. For example, any constant classifier that always outputs 0 or 1
is always fair for any distribution µ over (X, A, Y), but this classifier is not optimal in the sense
of achieving the best possible accuracy. On the other hand, the problem of learning an optimal
fair classifier is at least as hard as learning a Bayes optimal classifier, which we formally define as
follows.

Definition 13 (Bayes Optimal Classifier) Given random variables (X,A) and a target variable
= h(X, A)) with h∗(x, a) = 1 iff
Y, the Bayes optimal classifier is h∗ := arg minh(x,a) Pr(Y
E[Y
) to denote the restriction of h∗ on
1/2 otherwise 0. We also use h∗a (
X = x, A = a]
A = a, respectively, i.e., h∗a (

) := h∗(

, a).

≥

|

·

·

·

12

(cid:54)
INHERENT TRADEOFFS IN LEARNING FAIR REPRESENTATIONS

Figure 2: The decision tree diagram of the (randomized) optimal fair classifier h∗Fair. The optimal
fair classifier h∗Fair has oracle access to h∗0 and h∗1, the (possibly unfair) Bayes optimal classifier
on µ0 and µ1, respectively. α, β are the base rates over µ0 and µ1, i.e, α := Prµ0(Y = 1) and
β := Prµ1(Y = 1).

) is the corresponding Bayes optimal classifier on the group A = a for a

Namely, h∗a (
.
}
With the definition of Bayes optimal classifier, we can formally argue that learning an optimal fair
classifier is at least as hard as learning a Bayes optimal classifier by the following reduction.

∈ {

0, 1

·

,

·

·

·

·

·

·

X × Y

, let h(cid:48)(

A Reduction Given a distribution µ(cid:48) over
) be the Bayes optimal classifier over µ(cid:48).
We can create a problem instance of learning the optimal fair classifier by constructing a distribution
µ over (X, A, Y) with µA=0 = µA=1 = µ(cid:48). Now because µA=0 = µA=1 = µ(cid:48), it is clear to see that
the classifier h∗(
) satisfies demographic parity. Furthermore,
, 1) = h(cid:48)∗(
the optimality of h(cid:48)∗ over µ(cid:48) implies the optimality of h∗ over µ as well. This shows that an oracle
call to the problem of learning the optimal fair classifier over µ can be used to solve the problem of
learning a Bayes optimal classifier over µ(cid:48), by a restriction of the returned h∗ to either h∗0 or h∗1.

) where h∗(

, 0) = h∗(

Surprisingly, the other direction is also true. More specifically, in what follows we shall present
an algorithm to construct a randomized classifier that is both fair and optimal, given oracle access to
h∗0 and h∗1, i.e., the group-wise Bayes optimal classifiers. We list the algorithm in Algorithm 1 and
the corresponding decision diagram of h∗Fair in Figure 2.
Theorem 14 For any distribution µ over (X, A, Y) such that YA=0 = h∗0(X) and YA=1 = h∗1(X),
the classifier h∗Fair constructed by Algorithm 1 satisfies demographic parity and is optimal, i.e.,
Errµ0(h∗Fair) + Errµ1(h∗Fair) = ∆
Before we present the proof of Theorem 14, we first briefly discuss one implication of its assumption
that YA=0 = h∗0(X) and YA=1 = h∗1(X). Essentially, this assumption says that there exists a perfect
but potentially unfair classifier over µ. Hence in this case Errµ0 + Errµ1 exactly corresponds to the
price paid by enforcing demographic parity. In what follows we provide the proof for this theorem.

BR(µ0, µ1).

13

<latexit sha1_base64="U0zHtceDq2H6lumRra0YM3bEbTM=">AAAEUXicfZNNb9NAEIa3TYCS8tHCkYtFhFQkFCVV+bhUqoBK4VZCk1ZKomq9HidL9sPdXTdxVv4PXOFvceKncGOdBiVZC1ayNJrnnXnt8U6YMKpNs/lra7tSvXP33s792u6Dh48e7+0/6WmZKgJdIplUlyHWwKiArqGGwWWiAPOQwUU4+VDwixtQmkpxbrIEhhyPBI0pwcalegezVwF+ebVXbzaaixOUg9YyqKPlObvar7weRJKkHIQhDGvdbzUTM7RYGUoY5LVBqiHBZIJH0HehwBz00C5eNw9euEwUxFK5R5hgkV2vsJhrnfHQKTk2Y+2zIvkvZsY893OFi86DxdlgTmykZHqzouhbTvdTE78bWiqS1IAgt18RpywwMigGG0RUATEscwEmirpBBGSMFSbGjb9kkOC5zGsbBhxPQEnJ3ZywIMCcvYApkZxjEdkBFVFuB0VtGNpPuUenQEdj81cQ26kvmM1WcObDLFvBzIfz+QrOS77T/3jqRCwpwcx+KWOjANYU576i3V6jbZ+CUu462cENVpBoyqTwBJHbn7UGH/0GEcRwXTQgxT1211Ac+xbXTpPb4zWJpzidJavfcuo7dDor2Mlzt2gtf63KQe+w0XrTOPp8VD95v1y5HfQMPUcHqIXeohPURmeoiwj6ir6h7+hH5WfldxVVt2+l21vLmqdo41R3/wBPW4N1</latexit>(x,a)<latexit sha1_base64="paY6A3F4YlXTd5kQwfo95WElB4c=">AAAEUXicfZNNb9NAEIa3SaAl5aOFIxeLCIlT5KAWeomogErhVkKTVkqiar0eJ0v2w91dN3Es/weu8Lc48VO4sU6DkqwFK1kazfPOvPZ4J4gZ1cb3f+1UqrV793f3HtT3Hz56/OTg8Glfy0QR6BHJpLoKsAZGBfQMNQyuYgWYBwwug+mHgl/egtJUiguTxjDieCxoRAk2NtXHXtvz310fNPymvzxeOWitggZanfPrw+rxMJQk4SAMYVjrQcuPzSjDylDCIK8PEw0xJlM8hoENBeagR9nydXPvpc2EXiSVfYTxltnNigxzrVMeWCXHZqJdViT/xcyE526ucNG5tzxbzIqNlExvVxR9y+lBYqKTUUZFnBgQ5O4rooR5RnrFYL2QKiCGpTbARFE7CI9MsMLE2PGXDGK8kHl9y4DjKSgpuZ0TFgSYtRcwI5JzLMJsSEWYZ8OiNgiyT7lDZ0DHE/NXEGUzVzCfr+HchWm6hqkLF4s1XJR8Z//x1LFYUYJZ9qWMjQLYUFy4ik5ng3ZcCkrZ65QNb7GCWFMmhSMI7f5sNPjoNgghgpuiASnusb2Gou1a3FhNnrU3JI7ibB6vf8uZ69DtrmE3z+2itdy1Kgf9183Wm+bR56PG6fvVyu2h5+gFeoVa6C06RR10jnqIoK/oG/qOflR/Vn/XUK1yJ63srGqeoa1T2/8DruODTA==</latexit>a=0?yesnoyesno<latexit sha1_base64="22DS5TdTZj2XoHY45VyJV+l6gcg=">AAAEWHicfZPbbtNAEIa3TaBtOLXlkhuLCKlwETlVOdxEVEClcFdCT1ISovV6nKyyB2d33cSx/BzcwmPB07BOg5KsBSNZGs33z/z2eieIGdXG939tbVeq9+7v7O7VHjx89PjJ/sHhlZaJInBJJJPqJsAaGBVwaahhcBMrwDxgcB2MPxb8+haUplJcmDSGPsdDQSNKsLGl/mjgf3t1NHvptbzm+8F+3W/4i/DKSXOZ1NEyzgcHlde9UJKEgzCEYa27TT82/QwrQwmDvNZLNMSYjPEQujYVmIPuZ4u3zr0XthJ6kVT2EcZbVNc7Msy1TnlglRybkXZZUfwXMyOeu7XCRefeIjaYFRspmd7sKOaWy93ERO/6GRVxYkCQu6+IEuYZ6RXn64VUATEstQkmitqD8MgIK0yM/QslgxjPZV7bMOB4DEpKbs8JCwLM2guYEsk5FmHWoyLMs17RGwTZ59yhU6DDkfkriLKpK5jNVnDmwjRdwdSF8/kKzku+0/946lgsKcEs+1rGRgGsKS5cRbu9RtsuBaXsdcp6t1hBrCmTwhGEdo3WBnxyB4QQwaQYQIp7bK+haLkWE6vJs9aaxFGczeLVbzlzHTqdFezkuV20prtW5eTquNF80zj5clI//bBcuV30DD1HR6iJ3qJT1Ebn6BIRNEHf0Q/0s/K7iqo71b076fbWsucp2ojq4R8ifIR7</latexit>h⇤0(x)=1?<latexit sha1_base64="nZbHrRj0a42z4weNUavI8dIY5ZQ=">AAAEgXicfZPbbhMxEIbdNkAppxQuubGIkFpRRZuqHCRUUQGVwl0pTVupG0Ve72xj1YeN7TQHa1+Ep+EW3oC3wZsEJXEEI600mu+f+bW2J8k5MzaKfq+tb1Tu3L23eX/rwcNHj59Ut5+eG9XXFFpUcaUvE2KAMwktyyyHy1wDEQmHi+TmU8kvbkEbpuSZHeXQFuRasoxRYn2pUz0wsWECt3aiPdzY3cMm5tDDcaYJdTHheZfgVzhOwJLC7U8LxYdOtRbVo0ng1aQxS2poFied7Y3XcapoX4C0lBNjrhpRbtuOaMsoh2Ir7hvICb0h13DlU0kEmLab/F6BX/pKijOl/SctnlQXOxwRxoxE4pWC2K4JWVn8F7NdUYS10sUUeBJLzIutUtwsd5RzV8tXfZu9azsm874FSad/kfU5tgqXF4FTpoFaPvIJoZr5g8C0S/y5W39dKwY5Gatia8lAkBvQSgl/TkRS4N5ewoAqIYhMXcxkWri47E0S96UI6ADYddf+FWRuEAqGwzkchnA0msNRCMfjORyv+A7+42lyOaOUcPdtFVsNsKA4CxXN5gJthhS09s/JxbdEQ24YVzIQpH7fFgZ8DgekkEGvHEDLd+yfoTwMLXpeU7jDBUmgOB7m82s5Dh1OT+fwtCj8ojXCtVpNzvfrjTf1g68HtaOPs5XbRM/RC7SDGugtOkJNdIJaiKLv6Af6iX5VNiq7laiyP5Wur816nqGlqLz/A0/zk4Y=</latexit>s⇠U(0,1),sa+b2a?<latexit sha1_base64="7yRzwoHrqlgDYe85Js9Qiz51Ldg=">AAAETHicfZNNb9NAEIa3dSklfLVw5GIRIXGKHFQ+LpUqoFK4taFpKyVRtV6Pm1X2w91dN3FW/gVc4W9x539wQ0is06Aka8FKlkbzvDOvPd6JM0a1iaIfG5vB1p3tuzv3GvcfPHz0eHfvyZmWuSLQI5JJdRFjDYwK6BlqGFxkCjCPGZzH4w8VP78BpakUp6bIYMjxlaApJdi41El0uduMWtH8hPWgvQiaaHGOL/eC14NEkpyDMIRhrfvtKDNDi5WhhEHZGOQaMkzG+Ar6LhSYgx7a+ZuW4QuXScJUKvcIE86zqxUWc60LHjslx2akfVYl/8XMiJd+rnLRZTg/a8yJjZRMr1dUfevpfm7Sd0NLRZYbEOT2K9KchUaG1UzDhCoghhUuwERRN4iQjLDCxLjJ1wwyPJNlY82A4zEoKbmbExYEmLMXMCGScywSO6AiKe2gqo1j+6n06ATo1cj8FaR24gum0yWc+rAolrDw4Wy2hLOa7+Q/njoTC0ows5/r2CiAFcWpr+h0VmjHp6CUu052cIMVZJoyKTxB4lZnpcFHv0ECKVxXDUh1j901FAe+xbXTlPZgReIpjqbZ8rcc+Q7d7hJ2y9ItWttfq3pw9qrVftPaP9lvHr5frNwOeoaeo5eojd6iQ9RBx6iHCAL0BX1F34Lvwc/gV/D7Vrq5sah5itbO1vYf0QKD+w==</latexit>0yesno<latexit sha1_base64="7yRzwoHrqlgDYe85Js9Qiz51Ldg=">AAAETHicfZNNb9NAEIa3dSklfLVw5GIRIXGKHFQ+LpUqoFK4taFpKyVRtV6Pm1X2w91dN3FW/gVc4W9x539wQ0is06Aka8FKlkbzvDOvPd6JM0a1iaIfG5vB1p3tuzv3GvcfPHz0eHfvyZmWuSLQI5JJdRFjDYwK6BlqGFxkCjCPGZzH4w8VP78BpakUp6bIYMjxlaApJdi41El0uduMWtH8hPWgvQiaaHGOL/eC14NEkpyDMIRhrfvtKDNDi5WhhEHZGOQaMkzG+Ar6LhSYgx7a+ZuW4QuXScJUKvcIE86zqxUWc60LHjslx2akfVYl/8XMiJd+rnLRZTg/a8yJjZRMr1dUfevpfm7Sd0NLRZYbEOT2K9KchUaG1UzDhCoghhUuwERRN4iQjLDCxLjJ1wwyPJNlY82A4zEoKbmbExYEmLMXMCGScywSO6AiKe2gqo1j+6n06ATo1cj8FaR24gum0yWc+rAolrDw4Wy2hLOa7+Q/njoTC0ows5/r2CiAFcWpr+h0VmjHp6CUu052cIMVZJoyKTxB4lZnpcFHv0ECKVxXDUh1j901FAe+xbXTlPZgReIpjqbZ8rcc+Q7d7hJ2y9ItWttfq3pw9qrVftPaP9lvHr5frNwOeoaeo5eojd6iQ9RBx6iHCAL0BX1F34Lvwc/gV/D7Vrq5sah5itbO1vYf0QKD+w==</latexit>0<latexit sha1_base64="N7HM93DDSyf7d3J2xz/yDe1gV1Q=">AAAETHicfZNNb9NAEIa3dSklfLVw5GIRIXGKYlQ+LpUqoFK4taFpKyVRtV6Pm1X2w91dN3FW/gVc4W9x539wQ0is06Aka8FKlkbzvDOvPd6JM0a1abd/bGwGW3e27+7ca9x/8PDR4929J2da5opAj0gm1UWMNTAqoGeoYXCRKcA8ZnAejz9U/PwGlKZSnJoigyHHV4KmlGDjUifR5W6z3WrPT1gPokXQRItzfLkXvB4kkuQchCEMa92P2pkZWqwMJQzKxiDXkGEyxlfQd6HAHPTQzt+0DF+4TBKmUrlHmHCeXa2wmGtd8NgpOTYj7bMq+S9mRrz0c5WLLsP5WWNObKRker2i6ltP93OTvhtaKrLcgCC3X5HmLDQyrGYaJlQBMaxwASaKukGEZIQVJsZNvmaQ4ZksG2sGHI9BScndnLAgwJy9gAmRnGOR2AEVSWkHVW0c20+lRydAr0bmryC1E18wnS7h1IdFsYSFD2ezJZzVfCf/8dSZWFCCmf1cx0YBrChOfUWns0I7PgWl3HWygxusINOUSeEJErc6Kw0++g0SSOG6akCqe+yuoTjwLa6dprQHKxJPcTTNlr/lyHfodpewW5Zu0SJ/rerB2atW9Ka1f7LfPHy/WLkd9Aw9Ry9RhN6iQ9RBx6iHCAL0BX1F34Lvwc/gV/D7Vrq5sah5itbO1vYf1OqD/A==</latexit>1yesno<latexit sha1_base64="4NR8U0kOmOibd3LTx7myZgcX1Mg=">AAAEWHicfZPbbtNAEIa3TaBtOLXlkhuLCKlwEcVVOdxEVEClcFdCT1ISovV6nKyyB2d33cSx/BzcwmPB07BOg5KsBSNZGs33z/z2eieIGdWm2fy1tV2p3ru/s7tXe/Dw0eMn+weHV1omisAlkUyqmwBrYFTApaGGwU2sAPOAwXUw/ljw61tQmkpxYdIY+hwPBY0owcaW+qOB/+3V0eyl1/L894P9erPRXIRXTvxlUkfLOB8cVF73QkkSDsIQhrXu+s3Y9DOsDCUM8lov0RBjMsZD6NpUYA66ny3eOvde2EroRVLZRxhvUV3vyDDXOuWBVXJsRtplRfFfzIx47tYKF517i9hgVmykZHqzo5hbLncTE73rZ1TEiQFB7r4iSphnpFecrxdSBcSw1CaYKGoPwiMjrDAx9i+UDGI8l3ltw4DjMSgpuT0nLAgway9gSiTnWIRZj4owz3pFbxBkn3OHToEOR+avIMqmrmA2W8GZC9N0BVMXzucrOC/5Tv/jqWOxpASz7GsZGwWwprhwFe32Gm27FJSy1ynr3WIFsaZMCkcQ2jVaG/DJHRBCBJNiACnusb2GouVaTKwmz1prEkdxNotXv+XMdeh0VrCT53bRfHetysnVccN/0zj5clI//bBcuV30DD1HR8hHb9EpaqNzdIkImqDv6Af6WfldRdWd6t6ddHtr2fMUbUT18A8mboR8</latexit>h⇤1(x)=1?<latexit sha1_base64="N7HM93DDSyf7d3J2xz/yDe1gV1Q=">AAAETHicfZNNb9NAEIa3dSklfLVw5GIRIXGKYlQ+LpUqoFK4taFpKyVRtV6Pm1X2w91dN3FW/gVc4W9x539wQ0is06Aka8FKlkbzvDOvPd6JM0a1abd/bGwGW3e27+7ca9x/8PDR4929J2da5opAj0gm1UWMNTAqoGeoYXCRKcA8ZnAejz9U/PwGlKZSnJoigyHHV4KmlGDjUifR5W6z3WrPT1gPokXQRItzfLkXvB4kkuQchCEMa92P2pkZWqwMJQzKxiDXkGEyxlfQd6HAHPTQzt+0DF+4TBKmUrlHmHCeXa2wmGtd8NgpOTYj7bMq+S9mRrz0c5WLLsP5WWNObKRker2i6ltP93OTvhtaKrLcgCC3X5HmLDQyrGYaJlQBMaxwASaKukGEZIQVJsZNvmaQ4ZksG2sGHI9BScndnLAgwJy9gAmRnGOR2AEVSWkHVW0c20+lRydAr0bmryC1E18wnS7h1IdFsYSFD2ezJZzVfCf/8dSZWFCCmf1cx0YBrChOfUWns0I7PgWl3HWygxusINOUSeEJErc6Kw0++g0SSOG6akCqe+yuoTjwLa6dprQHKxJPcTTNlr/lyHfodpewW5Zu0SJ/rerB2atW9Ka1f7LfPHy/WLkd9Aw9Ry9RhN6iQ9RBx6iHCAL0BX1F34Lvwc/gV/D7Vrq5sah5itbO1vYf1OqD/A==</latexit>1<latexit sha1_base64="kHewOSv231Bl2gfIOve7qfzt1hM=">AAAEhXicfZPbThsxEIYNpIXSA6G97I1VVAkkQFlEKTcI1BYpvaOUABIbIa93NrHwYWM75GDtq/Rpetve923qDUFJHLUjrTQ73z/zy6ck58zYWu3PwuJS5cnT5ZVnq89fvHy1Vl1/fWlUV1NoUMWVvk6IAc4kNCyzHK5zDUQkHK6Su88lv7oHbZiSF3aQQ1OQlmQZo8T60m310MSGCdzYrG1HW9vYxBw6OM40oS4mPG8TvIPjBCwp3N5m9PizVRzfVjdqu7VR4PkkGicbaBxnt+tLH+JU0a4AaSknxtxEtdw2HdGWUQ7Fatw1kBN6R1pw41NJBJimGy2xwO99JcWZ0v6TFo+q0x2OCGMGIvFKQWzbhKws/ovZtijCWuliCjyKGebFViluZjvKufPlm67NDpuOybxrQdKHVWRdjq3C5WHglGmglg98QqhmfiMwbRO/+dYf2ZxBToaqWJ0xEOQOtFLC7xORFLi3l9CjSggiUxczmRYuLnuTxH0tAtoD1mrbR0HmeqGg35/AfggHgwkchHA4nMDhnG/vP54ml2NKCXff57HVAFOKi1BRr0/RekhBa3+dXHxPNOSGcSUDQerf3NSAL+GAFDLolANoeY/9NZRHoUXHawp3NCUJFKf9fHIsp6HD+fkEnheFf2hR+Kzmk8u93ehgd//b/sbJp/GTW0Fv0Tu0iSL0EZ2gOjpDDUTRD/QT/UK/K8uVncp+5eBBurgw7nmDZqJy/BfIVJQV</latexit>s⇠U(0,1),sa b2(1 b)?yes<latexit sha1_base64="N7HM93DDSyf7d3J2xz/yDe1gV1Q=">AAAETHicfZNNb9NAEIa3dSklfLVw5GIRIXGKYlQ+LpUqoFK4taFpKyVRtV6Pm1X2w91dN3FW/gVc4W9x539wQ0is06Aka8FKlkbzvDOvPd6JM0a1abd/bGwGW3e27+7ca9x/8PDR4929J2da5opAj0gm1UWMNTAqoGeoYXCRKcA8ZnAejz9U/PwGlKZSnJoigyHHV4KmlGDjUifR5W6z3WrPT1gPokXQRItzfLkXvB4kkuQchCEMa92P2pkZWqwMJQzKxiDXkGEyxlfQd6HAHPTQzt+0DF+4TBKmUrlHmHCeXa2wmGtd8NgpOTYj7bMq+S9mRrz0c5WLLsP5WWNObKRker2i6ltP93OTvhtaKrLcgCC3X5HmLDQyrGYaJlQBMaxwASaKukGEZIQVJsZNvmaQ4ZksG2sGHI9BScndnLAgwJy9gAmRnGOR2AEVSWkHVW0c20+lRydAr0bmryC1E18wnS7h1IdFsYSFD2ezJZzVfCf/8dSZWFCCmf1cx0YBrChOfUWns0I7PgWl3HWygxusINOUSeEJErc6Kw0++g0SSOG6akCqe+yuoTjwLa6dprQHKxJPcTTNlr/lyHfodpewW5Zu0SJ/rerB2atW9Ka1f7LfPHy/WLkd9Aw9Ry9RhN6iQ9RBx6iHCAL0BX1F34Lvwc/gV/D7Vrq5sah5itbO1vYf1OqD/A==</latexit>1no<latexit sha1_base64="7yRzwoHrqlgDYe85Js9Qiz51Ldg=">AAAETHicfZNNb9NAEIa3dSklfLVw5GIRIXGKHFQ+LpUqoFK4taFpKyVRtV6Pm1X2w91dN3FW/gVc4W9x539wQ0is06Aka8FKlkbzvDOvPd6JM0a1iaIfG5vB1p3tuzv3GvcfPHz0eHfvyZmWuSLQI5JJdRFjDYwK6BlqGFxkCjCPGZzH4w8VP78BpakUp6bIYMjxlaApJdi41El0uduMWtH8hPWgvQiaaHGOL/eC14NEkpyDMIRhrfvtKDNDi5WhhEHZGOQaMkzG+Ar6LhSYgx7a+ZuW4QuXScJUKvcIE86zqxUWc60LHjslx2akfVYl/8XMiJd+rnLRZTg/a8yJjZRMr1dUfevpfm7Sd0NLRZYbEOT2K9KchUaG1UzDhCoghhUuwERRN4iQjLDCxLjJ1wwyPJNlY82A4zEoKbmbExYEmLMXMCGScywSO6AiKe2gqo1j+6n06ATo1cj8FaR24gum0yWc+rAolrDw4Wy2hLOa7+Q/njoTC0ows5/r2CiAFcWpr+h0VmjHp6CUu052cIMVZJoyKTxB4lZnpcFHv0ECKVxXDUh1j901FAe+xbXTlPZgReIpjqbZ8rcc+Q7d7hJ2y9ItWttfq3pw9qrVftPaP9lvHr5frNwOeoaeo5eojd6iQ9RBx6iHCAL0BX1F34Lvwc/gV/D7Vrq5sah5itbO1vYf0QKD+w==</latexit>0ZHAO AND GORDON

Proof [Proof of Theorem 14] We first show that h∗Fair is fair in the demographic parity sense. Let
(cid:98)Y = h∗Fair(X, A), α := Pr(Y = 1
A = 1). Without loss of
generality, we assume α

A = 0) and β := Pr(Y = 1

β.

|

|

For A = 0, consider the probability Prµ0( (cid:98)Y = 1). Note that by construction in Algorithm 1,

≥

h∗Fair(X, 0) = 0 whenever h∗0(X) = 0, so

(cid:18)

Pr
µ0

( (cid:98)Y = 1) = Pr
µ0

h∗0(X) = 1, S

(cid:19)

α + β
2α

≤

= Pr
µ0

(h∗0(X) = 1)

·

(cid:18)

Pr

S

(cid:19)

α + β
2α

≤

= Pr
µ0

(Y = 1)

α + β
2α

·

= α

α + β
2α

·

=

α + β
2

.

Similarly, for A = 1, recall that by construction, h∗Fair(X, 1) = 1 whenever h∗1(X) = 1, so

(cid:18)

(cid:18)

Pr
µ1

( (cid:98)Y = 1) = Pr
µ1

h∗1(X) = 1

h∗1(X) = 0

S

∧

≤

∨

α
2(1
(cid:18)

−
−

(h∗1(X) = 1) + Pr (h∗1(X) = 0)

Pr

S

·

≤

(cid:19)(cid:19)

β
β)
α
2(1

(cid:19)

β
β)

−
−

= Pr
µ1

= Pr
µ1

(Y = 1) + Pr
µ1
α
2(1

β)

−

·

= β + (1

(Y = 0)

β
β)

=

−
−

·

β
β)

α
−
2(1
−
α + β
2

,

where in the proof above we use the fact that S
∼
that µ0( (cid:98)Y) = µ1( (cid:98)Y) so (cid:98)Y = h∗Fair(X, A) is fair.

U(0, 1) is drawn independently of X. This shows

Next, we prove that h∗Fair is optimal. For A = 0,

Errµ0( (cid:98)Y) = Pr(h∗Fair(X, 0)

= Y

|

A = 0) = Pr(h∗Fair(X, 0)

= h∗0(X)

A = 0).

|

However, due to the construction of h∗Fair in Algorithm 1, h∗Fair(X, 0)
h∗0(X) = 1 while h∗Fair(X, 0) = 0, so

= h∗0(X) could only happen if

Pr(h∗0(x) = 1

A = 0)

|

·

= h∗0(X)

|
= h∗0(x)

A = 0)

A = 0)

|

Pr(h∗Fair(x, 0) = 0, h∗0(x) = 1

A = 0)

Pr(h∗Fair(x, 0)

Errµ0( (cid:98)Y) = Pr(h∗Fair(X, 0)
= ∑
x
= ∑
x
= ∑
x
= ∑
x
(cid:18)

S >

Pr

(cid:18)

(cid:19)

α + β
2α

Pr(h∗Fair(x, 0) = 0

|
h∗0(x) = 1, A = 0)

|

(cid:19)

·

Pr(h∗0(x) = 1

A = 0)

|

Pr(h∗0(x) = 1

A = 0)

|

α + β
2α
α + β
2α

(cid:19)

∑
x

α

·

·

1

1

=

=

=

(cid:18)

α

−

−
β

−
2

.

14

(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
INHERENT TRADEOFFS IN LEARNING FAIR REPRESENTATIONS

Similarly, for A = 1,

Errµ1( (cid:98)Y) = Pr(h∗Fair(X, 1)

= Y

|

A = 1) = Pr(h∗Fair(X, 1)

= h∗1(X)

A = 1).

|

Again, due to the construction of h∗Fair in Algorithm 1, h∗Fair(X, 1)
h∗1(X) = 0 while h∗Fair(X, 0) = 1, so

= h∗1(X) could only happen if

= h∗0(X)

|
= h∗1(x)

A = 1)

A = 1)

|

Pr(h∗Fair(x, 1)

Errµ0( (cid:98)Y) = Pr(h∗Fair(X, 1)
= ∑
x
= ∑
x
= ∑
x
= ∑
x

Pr

(cid:18)

S

Pr(h∗Fair(x, 1) = 1

|
(cid:19)

Pr(h∗Fair(x, 1) = 1, h∗1(x) = 0

A = 1)

|
h∗1(x) = 0, A = 1)

Pr(h∗1(x) = 0

A = 1)

|

·

Pr(h∗1(x) = 0

A = 1)

|

β
β)

−
−

α
2(1
≤
∑
x

(1

−

β)

β
β) ·
β
β) ·

=

=

=

α
2(1
α
2(1
α

−
−
−
−
β

−
2

.

Pr(h∗1(x) = 0

A = 1)

|

·

Combining the two cases above shows that Errµ0(h∗Fair) + Errµ1(h∗Fair) = α
which completes the proof.

−

β = ∆

BR(µ0, µ1),

As a simple corollary of Theorem 14, we can now strengthen Theorem 8 as follows:

Corollary 15 Let (cid:98)Y = h(X, A) be a predictor that satisfies demographic parity, then inf (cid:98)Y Errµ0(h) +
Errµ1(h) = ∆

BR(µ0, µ1).

Note that one difference between Corollary 15 and Theorem 8 is that the fair predictor in Corollary 15
is allowed to have explicit access to the protected attribute A during decision making. This is
necessary for our construction of the optimal fair classifier in Algorithm 1. It remains an open
question whether it is possible to construct an optimal fair classifier to achieve the lower bound in
Theorem 8 without having explicit access to the protected attribute, as in many practical applications
of high stakes the automated decision making process is regulated to not directly use the protected
attribute A (GDPR, Article 22 Paragraph 4).

5. Approximate Fairness via Learning Fair Representations

In the last section we show that there is an inherent tradeoff between fairness and accuracy when a
predictor exactly satisfies demographic parity. In practice we may not be able to achieve demographic
parity precisely. Instead, a line of recent algorithms (Edwards and Storkey, 2015; Louizos et al.,
2015; Beutel et al., 2017; Adel et al., 2019; Zhao et al., 2019c) build an adversarial discriminator that
takes as input the feature vector Z = g(X), and the goal is to learn fair representations such that it is
hard for the adversarial discriminator to infer the group membership from Z, typically by solving

15

(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
ZHAO AND GORDON

a minimax objective between the feature encoder and the adversarial discriminator (Edwards and
Storkey, 2015; Beutel et al., 2017; Zhang et al., 2018; Zhao et al., 2019c; Chi et al., 2021).

In these applications, due to the limit on the capacity of the adversarial discriminator, only
approximate demographic parity can be achieved in practice. Hence it is natural to ask what is the
tradeoff between fair representations and accuracy in this scenario? In this section we shall answer
this question by generalizing our previous analysis with f -divergence to prove a family of lower
bounds on the joint target prediction error. Our results also show how approximate DP helps to
reconcile but not remove the tradeoff between fairness and accuracy. Before we state and prove the
main results in this section, we first introduce the following lemma by Liese and Vajda (2006) as a
generalization of the data processing inequality for f -divergence:

Lemma 16 (Liese and Vajda (2006)) Let ∆(

. Then for any f -divergence D f (
and
)

over

, D f (κ

κ

), any stochastic kernel κ :
D f (

).

Q

X

P (cid:107)

Q

P (cid:107) Q

· (cid:107) ·
≤

Z
P

X →

Z

Z

) be the space of all probability distributions over
∆(
), and any distributions

Roughly speaking, Lemma 16 says that data processing cannot increase discriminating infor-
). It is well-known in
,
P
) form a bounded distance metric over the space of
), H2(
) are all

) := (cid:112)
mation. Define dJS(
DJS(
information theory that both dJS(
,
·
probability distributions (Wu, 2017, Chapter 4). Realize that dTV(
f -divergence. The following corollary holds:

,
Q
P
) and H(

) := (cid:112)

) and DJS(

) and H(

H2(

Q

Q

Q

P

P

·

·

·

·

·

·

·

·

·

,

,

,

,

,

,

Corollary 17 Let h :

be a classifier, and g(cid:93)µa be the pushforward distribution of µa by g,

0, 1

. Let (cid:98)Y = h(g(X)) be the predictor, then all the following inequalities hold:

Z → Y

a

∀

}

∈ {
1. dTV(µ0( (cid:98)Y), µ1( (cid:98)Y))

dTV(g(cid:93)µ0, g(cid:93)µ1)

≤

2. H(µ0( (cid:98)Y), µ1( (cid:98)Y))

H(g(cid:93)µ0, g(cid:93)µ1)

≤

3. dJS(µ0( (cid:98)Y), µ1( (cid:98)Y))

dJS(g(cid:93)µ0, g(cid:93)µ1)

≤

Now we are ready to present the following main theorem of this section:

Theorem 18 Let (cid:98)Y = h(g(X)) be the predictor where h :
ing on the feature space. Assume dJS(g(cid:93)µ0, g(cid:93)µ1)
H(µ0(Y), µ1(Y)), then the following three inequalities hold:

≤

}
dJS(µ0(Y), µ1(Y)) and H(g(cid:93)µ0, g(cid:93)µ1)

Z → {

0,1

is any classifier act-

≤

1. Total variation lower bound:

Errµ0(h

◦

g) + Errµ1(h

g)

◦

≥

dTV(µ0(Y), µ1(Y))

dTV(g(cid:93)µ0, g(cid:93)µ1).

−

2. Jensen-Shannon lower bound:

Errµ0(h

◦

g) + Errµ1(h

(cid:0)

g)

◦

≥

dJS(µ0(Y), µ1(Y))

3. Hellinger lower bound:

Errµ0(h

◦

g) + Errµ1(h

(cid:0)

g)

◦

≥

H(µ0(Y), µ1(Y))

16

dJS(g(cid:93)µ0, g(cid:93)µ1)(cid:1)2

/2.

H(g(cid:93)µ0, g(cid:93)µ1)(cid:1)2

/2.

−

−

INHERENT TRADEOFFS IN LEARNING FAIR REPRESENTATIONS

Remark All the three lower bounds in Theorem 18 imply a tradeoff between the joint error across
demographic subgroups and learning group-invariant feature representations. When g(cid:93)µ0 = g(cid:93)µ1,
due to the data-processing principle, we also have µ0( (cid:98)Y) = µ1( (cid:98)Y), and all three lower bounds get
larger. In this case, we have

(cid:26)

max

dTV(µ0(Y), µ1(Y)),

1
2

d2
JS(µ0(Y), µ1(Y)),

H2(µ0(Y), µ1(Y))

(cid:27)

1
2

= dTV(µ0(Y), µ1(Y))

= ∆

BR(µ0, µ1),

and this reduces to Theorem 8. We now present the proof for Theorem 18.
Proof [Proof of Theorem 18] We prove the three inequalities respectively. The total variation lower
bound follows the same idea as the proof of Theorem 8 and the inequality dTV(µ0( (cid:98)Y), µ1( (cid:98)Y))
dTV(g(cid:93)µ0, g(cid:93)µ1) from Corollary 17. To prove the Jensen-Shannon lower bound, realize that dJS(
,
·
a distance metric over probability distributions. Combining with the inequality dJS(µ0( (cid:98)Y), µ1( (cid:98)Y))
dJS(g(cid:93)µ0, g(cid:93)µ1) from Corollary 17, we have:

≤
) is

≤

·

dJS(µ0(Y), µ1(Y))

≤

dJS(µ0(Y), µ0( (cid:98)Y)) + dJS(g(cid:93)µ0, g(cid:93)µ1) + dJS(µ1( (cid:98)Y), µ1(Y)).

Now by Lin’s lemma (Lin, 1991, Theorem 3), for any two distributions
d2
JS(
,
P
bound:

, we have
). Combine Lin’s lemma with Lemma 11, we get the following lower

dTV(

and

Q

Q

Q

≤

P

P

)

,

(cid:113)

Errµ0(h

g) +

◦

(cid:113)

Errµ1(h

g)

◦

≥

dJS(µ0(Y), µ1(Y))

dJS(g(cid:93)µ0, g(cid:93)µ1).

−

Apply the AM-GM inequality, we can further bound the L.H.S. by

(cid:113)

2(cid:0)Errµ0(h

g) + Errµ1(h

◦

(cid:113)

g)(cid:1)

◦

≥

Errµ0(h

g) +

◦

(cid:113)

Errµ1(h

g).

◦

dJS(µ0(Y), µ1(Y)), taking a square at both sides then
Under the assumption that dJS(g(cid:93)µ0, g(cid:93)µ1)
completes the proof for the second inequality. The proof for Hellinger’s lower bound follows exactly
as the one for Jensen-Shannon’s lower bound, except that instead of Lin’s lemma, we need to use the
fact that H2(
),

√2H(

dTV(

≤

)

)

.

,

,

,

,

P

Q

≤

P

Q

≤

P

Q

∀P

Q

As a simple corollary of Theorem 18, the following result shows how approximate DP (in terms
of the DP gap) helps to reconcile the tradeoff between fairness and accuracy, by controlling the
divergence between different groups of representations:

Corollary 19 Let g :
any classifier h :
∆
(cid:101).

BR(µ0, µ1)

−

Z → Y

be a feature transformation. If dTV(g(cid:93)µ0, g(cid:93)µ1)

X → Z

, the DP gap ∆

DP(h

g)

◦

≤

(cid:101), and Errµ0(h

◦

≤

g) + Errµ1(h

(cid:101), then for
g)

◦

≥

In a sense Corollary 19 means that in order to lower the joint error, the DP gap of the predictor cannot
be too small. Of course, since the above inequality is a lower bound, it only serves as a necessary
condition for small joint error. Hence an interesting question would be to ask whether it is possible
to have a sufficient condition that guarantees a small joint error such that the DP gap of the predictor
is no larger than that of the perfect predictor, i.e., ∆

BR(µ0, µ1).

17

ZHAO AND GORDON

6. Fair Representations Lead to Accuracy Parity

In the previous sections we prove a family of information-theoretic lower bounds that demonstrate an
inherent tradeoff between fair representations and joint error across groups. A natural question to ask
then, is, what kind of parity can fair representations bring us? To complement our negative results,
in this section we show that learning group-invariant representations help to reduce discrepancy of
errors across groups.

First of all, since we work under the stochastic setting where µa is a joint distribution over
X and Y conditioned on A = a, then any function mapping h :
will inevitably incur
an error due to the noise existed in the distribution µa. In the case of binary classification, this
be the Bayes
error is also known as the Bayes error. Formally, for a
optimal classifier on µa. Now define the noise of distribution µa (the Bayes error on µa) to be
]. We are now ready to present the following
nµa
theorem:

= h∗a (X)) = Eµa [

:= Prµa (Y

, let h∗a :

X → Y

X → Y

h∗a (X)

∈ {

0, 1

−

Y

}

|

|

Theorem 20 (Error Decomposition Theorem) For any hypothesis
ing inequality holds:

h :

H (cid:51)

X → Y

, the follow-

(cid:12)
(cid:12)Errµ0(h)

Errµ1(h)(cid:12)
(cid:12)

−

≤

(nµ0 + nµ1) + dTV(µ0(X), µ1(X))
+ min (cid:8)Eµ0[
h∗0 −
h∗0 −

], Eµ1[

h∗1|

|

|

](cid:9) .

h∗1|

|

|

≤

−

](cid:9) .

h∗1|

h∗1|

h∗0 −

h∗0 −

], Eµ1[

Errµ1(h)(cid:12)
(cid:12)

Remark Theorem 20 upper bounds the discrepancy of accuracy across groups by three terms:
the sum of group-wise noise, the distance of representations across groups and the discrepancy of
the Bayes optimal classifiers. In an ideal setting where both distributions are noiseless, i.e., same
individuals in the same group are always treated similarly, the upper bound simplifies to the latter
two terms:
(cid:12)
(cid:12)Errµ0(h)

dTV(µ0(X), µ1(X)) + min (cid:8)Eµ0[
If we further require that the optimal decision functions h∗0 and h∗1 are close to each other, i.e.,
optimal decisions are insensitive to the group membership, then Theorem 20 implies that a sufficient
condition to guarantee accuracy parity is to find group-invariant representation that minimizes
dTV(µ0(X), µ1(X)). Note, however, we should also pay attention to ensure that the first term as well
as the third term in the upper bound do not increase drastically when learning the representations, as
a change of the data representation will also change the noise term as well as the distance between the
optimal decision functions based on the representations. We now present the proof for Theorem 20:
Proof [Proof of Theorem 20] First, we show that for a
, Errµa (h) cannot be too large if h is
close to h∗a . Note that since we are focusing on binary classification problems, for any two classifiers
].
h, h(cid:48), Pr(h(X)
(cid:12)
Eµa [
(cid:12)Errµa (h)
|
|
Eµa [(cid:12)
(cid:12)
|
Eµa [
Y

Eµa [
|
h(X)
| − |
] = nµa,
where both inequalities are due to the triangle inequality. Next, we bound (cid:12)
(nµ0 + nµ1) + (cid:12)

= h(cid:48)(X)) = E[
Eµa [

(cid:12)Errµ0(h)
Eµ1[

Errµ1(h)(cid:12)
](cid:12)
h∗1(X)
(cid:12) .

|
Y
−
h∗a (X)

h(cid:48)(X)
](cid:12)
(cid:12) =

h∗a (X)
(cid:12)
(cid:12)]

Errµ1(h)(cid:12)
(cid:12)

(cid:12)
(cid:12)Errµ0(h)

−
h∗a (X)

h(X)
h(X)

|
h(X)

−
h∗a (X)

h∗0(X)

(cid:12)Eµ0[

h(X)

h(X)

h(X)

h(X)

(cid:12) by:

∈ {

0, 1

−

−

−

≤

≤

−

−

−

Y

}

|

|

|

]

|

]

|

]

|

|

|

|

−

|

−

|

|

−

≤

−

−

18

(cid:54)
(cid:54)
INHERENT TRADEOFFS IN LEARNING FAIR REPRESENTATIONS

In order to show this, define ε a(h, h(cid:48)) := Eµa [

h(X)

|
h(X)

−

Eµ1[

|

−

] so that

h(cid:48)(X)
|
(cid:12) = (cid:12)
](cid:12)

−
h∗1(X)

(cid:12)ε0(h, h∗0)

ε1(h, h∗1)(cid:12)
(cid:12).

−

To bound (cid:12)

(cid:12)
(cid:12)Eµ0[

|

h(X)

(cid:12)ε0(h, h∗0)

−
(cid:12)
(cid:12)ε0(h, h∗0)

]

h∗0(X)
−
|
ε1(h, h∗1)(cid:12)
ε1(h, h∗1)(cid:12)

−

|
h∗a (X)

0, 1

| ∈ {

h(X)

(cid:12), realize that
−
}
|
(cid:12) = (cid:12)
ε0(h, h∗1) + ε0(h, h∗1)
(cid:12)ε0(h, h∗0)
(cid:12) + (cid:12)
ε0(h, h∗1)(cid:12)
(cid:12)
(cid:12)ε0(h, h∗0)
ε0(h∗0, h∗1) + dTV(µ0(X), µ1(X)),
ε1(h, h∗1)(cid:12)

(cid:12) = (cid:12)

≤

−

−

h

−
(cid:12)ε0(h, h∗1)

. On one hand, we have:

ε1(h, h∗1)(cid:12)
(cid:12)
ε1(h, h∗1)(cid:12)
(cid:12)

−

=
(cid:12)µ0(
= dTV(µ0, µ1). Similarly, by subtracting and adding back ε1(h, h∗0)
(cid:12)ε0(h, h∗0)

ε1(h∗0, h∗1) + dTV(µ0(X), µ1(X)).

−
ε1(h, h∗1)(cid:12)
(cid:12)

= 1)

h∗1|

h∗1|

µ1(

−

−

−

h

|

|

−

≤

≤
where the last inequality is due to (cid:12)
(cid:12)ε0(h, h∗1)
1)(cid:12)
µ0(E)
(cid:12)
instead, we can also show that (cid:12)

supE |

µ1(E)

−

≤

|

Combine the above two inequalities yielding:
ε1(h, h∗1)(cid:12)
(cid:12)

(cid:12)
(cid:12)ε0(h, h∗0)

min

−

≤

{

ε0(h∗0, h∗1), ε1(h∗0, h∗1)

+ dTV(µ0(X), µ1(X)).

}

Incorporating the sum of group-wise noise back to the above inequality by using one more triangle
inequality then completes the proof.

7. Empirical Results

Our theoretical results on the lower bound imply that over-training the feature transformation function
to achieve group-invariant representations will inevitably lead to large joint errors. On the other hand,
our upper bound also implies that group-invariant representations help to achieve accuracy parity.
To verify these theoretical implications, in this section we conduct experiments on a real-world
benchmark dataset, the UCI Adult dataset, to present empirical results with various metrics.

Dataset The Adult dataset contains 30,162/15,060 training/test instances for income prediction.
Each instance in the dataset describes an adult from the 1994 US Census. Attributes include gender,
education level, age, etc. In this experiment we use gender (binary) as the sensitive attribute, and we
preprocess the dataset to convert categorical variables into one-hot representations. The processed
data contains 114 attributes. The target variable (income) is also binary: 1 if
50K/year otherwise 0.
For the sensitive attribute A, A = 0 means Male otherwise Female. In this dataset, the base rates
across groups are different: Pr(Y = 1
A = 1) = 0.113. Also,
the group ratios are different: Pr(A = 0) = 0.673.

A = 0) = 0.310 while Pr(Y = 1

≥

|

|

Experimental Protocol To validate the effect of learning group-invariant representations with
adversarial debiasing techniques (Zhang et al., 2018; Madras et al., 2018; Beutel et al., 2017), we
perform a controlled experiment by fixing the baseline network architecture to be a three hidden-layer
feed-forward network with ReLU activations. The number of units in each hidden layer are 500, 200,
and 100, respectively. The output layer corresponds to a logistic regression model. This baseline
without debiasing is denoted as NoDebias. For debiasing with adversarial learning techniques, the
adversarial discriminator network takes the feature from the last hidden layer as input, and connects
it to a hidden-layer with 50 units, followed by a binary classifier whose goal is to predict the sensitive

19

ZHAO AND GORDON

attribute A. This model is denoted as AdvDebias. Compared with NoDebias, the only difference of
AdvDebias in terms of objective function is that besides the cross-entropy loss for target prediction,
the AdvDebias also contains a classification loss from the adversarial discriminator to predict the
sensitive attribute A. In the experiment, all the other factors are fixed to be the same between these
two methods, including learning rate, optimization algorithm, training epoch, and also batch size. To
see how the adversarial loss affects the joint error, the demographic parity as well as the accuracy
parity, we vary the coefficient ρ for the adversarial loss between 0.1, 1.0, 5.0 and 50.0.

Results and Analysis The experimental results are listed in Table 2. Note that in the table
could be understood as measuring an approximate version of accuracy parity, and
Errµ0 −
Errµ1|
|
similarly ∆
DP( (cid:98)Y) measures the closeness of the classifier to satisfy demographic parity. From the
table, it is then clear that with increasing ρ, both the overall error Errµ (sensitive to the marginal
distribution of A) and the joint error Errµ0 + Errµ1 (insensitive to the imbalance of A) are increasing.
As expected, ∆
DP( (cid:98)Y) is drastically decreasing with the increasing of ρ. Furthermore,
Errµ1|
is also gradually decreasing, but much slowly than ∆
DP( (cid:98)Y). This is due to the existing noise in the
data as well as the shift between the optimal decision functions across groups, as indicated by our
upper bound. To conclude, all the empirical results are consistent with our theoretical findings.

Errµ0 −

|

Table 2: Adversarial debiasing on demographic parity, joint error across groups, and accuracy parity.

NoDebias
AdvDebias, ρ = 0.1
AdvDebias, ρ = 1.0
AdvDebias, ρ = 5.0
AdvDebias, ρ = 50.0

Errµ

0.157
0.159
0.162
0.166
0.201

Errµ0 + Errµ1
0.275
0.278
0.286
0.295
0.360

Errµ1

(cid:12)
(cid:12)Errµ0 −
0.115
0.116
0.106
0.106
0.112

(cid:12)
(cid:12) ∆

DP( (cid:98)Y)

0.189
0.190
0.113
0.032
0.028

8. Related Work

Tradeoff between Fairness and Accuracy Although it has long been empirically observed that
there is an inherent tradeoff between accuracy and statistical parity in both classification and
regression problems (Calders et al., 2009; Zafar et al., 2015; Zliobaite, 2015; Corbett-Davies et al.,
2017; Zhao et al., 2019c; Zhao, 2021), precise characterizations on such tradeoffs are less explored.
Menon and Williamson (2018, Proposition 8) explored such tradeoff in terms of the fairness frontier
function under the context of cost-sensitive binary classification. In this work the fair machine
learning problem is reduced to learning a classifier which optimizes a difference between cost-
sensitive risks, one with respect to the target variable and one with respect to the sensitive variable.
Zhao and Gordon (2019) proved a lower bound of accuracy on both the sum of group-wise errors
as well as the joint error that has to be incurred by any fair algorithm satisfying statistical parity.
In this paper, assuming oracle access to Bayes optimal classifiers, we also give an algorithm to
construct an optimal fair classifier that can verify the lower bound. Furthermore, we also extend the
preliminary result in Zhao and Gordon (2019) for binary classification and binary protected attribute
to the general multi-class classification setting where the protected attribute can take more than two
values, i.e., there are more than two groups defined by the protected attribute. Recently, Chzhen et al.

20

INHERENT TRADEOFFS IN LEARNING FAIR REPRESENTATIONS

(2020) and Le Gouic et al. (2020) concurrently derived an analytic bound to characterize the price of
statistical parity in regression when the learner can take the sensitive attribute explicitly as an input
for (cid:96)2 loss. In this case, the lower bound is given by the optimal transportation distance from two
group distributions to a common one, characterized by the W2 barycenter. Our result complements
this line of works for the classification setting, where we show that the price paid by a fair classifier
for general multi-class classification problems with a categorical protected attribute is given by the
so-called TV-Barycenter problem.

On the upside, under certain data generative assumptions of the sampling bias, there is a
line of recent works showing that fairness constraints could instead improve the accuracy of the
predictor (Dutta et al., 2020; Blum and Stangl, 2020). In particular, Blum and Stangl (2020) prove
that if the observable data are subject to labeling bias, then the Equality of Opportunity constraint
could help recover the Bayes optimal classifier. Note that this does not contradict with our results,
since in this work we do not make any assumptions on the underlying training distributions, and we
mainly focus on statistical parity and accuracy parity, rather than equalized odds.

Regularization Techniques The line of work on fairness-aware learning through regularization
dates at least back to Kamishima et al. (2012), where the authors argue that simple deletion of
sensitive features in data is insufficient for eliminating biases in automated decision making, due
to the possible correlations among attributes and sensitive information (Lum and Johndrow, 2016).
In light of this, the authors proposed a prejudice remover regularizer that essentially penalizes the
mutual information between the predicted goal and the sensitive information. In a more recent
approach, Zafar et al. (2015) leveraged a measure of decision boundary fairness and incorporated it
via constraints into the objective function of logistic regression as well as support vector machines.
As discussed in Section 2, both approaches essentially reduce to achieving demographic parity
through regularization.

Fair Representations
In a pioneer work, Zemel et al. (2013) proposed to preserve both group and
individual fairness through the lens of representation learning, where the main idea is to find a good
representation of the data with two competing goals: to encode the data for accuracy maximization
while at the same time to obfuscate any information about membership in the protected group.
Due to the power of learning rich representations offered by deep neural nets, recent advances in
building fair automated decision making systems focus on using adversarial techniques to learn
fair representation that also preserves enough information for the prediction vendor to achieve his
accuracy (Edwards and Storkey, 2015; Louizos et al., 2015; Beutel et al., 2017; Zhang et al., 2018;
Adel et al., 2019; Song et al., 2019; Zhao et al., 2019c). Madras et al. (2018) further extended this
approach by incorporating reconstruction loss given by an autoencoder into the objective function to
preserve demographic parity, equalized odds, and equal opportunity.

9. Discussion and Conclusion

In this paper we theoretically and empirically study the important problem of quantifying the tradeoff
between accuracy and statistical parity in algorithmic fairness. Specifically, we prove a novel lower
bound to characterize the tradeoff between statistical parity and the joint accuracy across different
population groups when the base rates differ between groups. In particular, our results imply that,
in general, any method aiming to satisfy statistical parity admits an information-theoretic lower
bound on the joint error. This holds even only approximate statistical parity is met. In light of this

21

ZHAO AND GORDON

impossibility result, under the statistical parity constraint, we can only hope to design algorithms
that achieve the accuracy lower bound. To this end, assuming oracle access to the potentially unfair
Bayes classifiers, we construct an algorithm that returns a randomized classifier, and we prove that
this randomized classifier is both optimal (in terms of accuracy) and fair.

When the number of groups defined by the protected attribute is more than two, we can no
longer obtain an analytic form of the lower bound. Nevertheless, we show that it can be efficiently
computed by solving a linear program in polynomial time, which we term as the TV-Barycener
problem. This finding also builds a connection between the tradeoff problem in algorithmic fairness
and the barycenter problem (under the TV-distance) in optimal transport. Complementary to our
negative results, we also show that learning fair representations leads to accuracy parity if the Bayes
optimal classifiers across different groups are close. Our theoretical findings are also confirmed
empirically on a real-world dataset. We believe our results take an important step towards better
understanding the tradeoff between accuracy and different notions of fairness.

Acknowledgments

HZ and GG would like to acknowledge support from the DARPA XAI project, contract #FA87501720152
and a Nvidia GPU grant. HZ would also like to thank support from a Facebook research award.
The authors are very grateful to the anonymous reviewers for the suggestions on improving the
presentation of this work.

22

INHERENT TRADEOFFS IN LEARNING FAIR REPRESENTATIONS

References

Tameem Adel, Isabel Valera, Zoubin Ghahramani, and Adrian Weller. One-network adversarial

fairness. In 33rd AAAI Conference on Artificial Intelligence, 2019.

Syed Mumtaz Ali and Samuel D Silvey. A general class of coefficients of divergence of one
distribution from another. Journal of the Royal Statistical Society: Series B (Methodological), 28
(1):131–142, 1966.

Solon Barocas and Andrew D Selbst. Big data’s disparate impact. Calif. L. Rev., 104:671, 2016.

Solon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness in machine learning. NIPS Tutorial,

2017.

Richard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth. Fairness in crim-
inal justice risk assessments: The state of the art. Sociological Methods & Research, page
0049124118782533, 2018.

Alex Beutel, Jilin Chen, Zhe Zhao, and Ed H Chi. Data decisions and theoretical implications when

adversarially learning fair representations. arXiv preprint arXiv:1707.00075, 2017.

Avrim Blum and Kevin Stangl. Recovering from biased data: Can fairness constraints improve
accuracy? In Symposium on Foundations of Responsible Computing (FORC), volume 1, 2020.

Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial
gender classification. In Conference on fairness, accountability and transparency, pages 77–91.
PMLR, 2018.

Toon Calders, Faisal Kamiran, and Mykola Pechenizkiy. Building classifiers with independency
constraints. In 2009 IEEE International Conference on Data Mining Workshops, pages 13–18.
IEEE, 2009.

Jianfeng Chi, Yuan Tian, Geoffrey J Gordon, and Han Zhao. Understanding and mitigating accuracy

disparity in regression. In International Conference on Machine Learning, 2021.

Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism

prediction instruments. Big data, 5(2):153–163, 2017.

Evgenii Chzhen, Christophe Denis, Mohamed Hebiri, Luca Oneto, and Massimiliano Pontil. Fair

regression with wasserstein barycenters. arXiv preprint arXiv:2006.07286, 2020.

Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. Algorithmic deci-
sion making and the cost of fairness. In Proceedings of the 23rd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, pages 797–806. ACM, 2017.

Imre Csiszár. Eine informationstheoretische ungleichung und ihre anwendung auf beweis der
ergodizitaet von markoffschen ketten. Magyer Tud. Akad. Mat. Kutato Int. Koezl., 8:85–108, 1964.

Imre Csiszár. Information-type measures of difference of probability distributions and indirect

observation. studia scientiarum Mathematicarum Hungarica, 2:229–318, 1967.

23

ZHAO AND GORDON

Sanghamitra Dutta, Dennis Wei, Hazar Yueksel, Pin-Yu Chen, Sijia Liu, and Kush Varshney. Is there
a trade-off between fairness and accuracy? a perspective using mismatched hypothesis testing. In
International Conference on Machine Learning, pages 2803–2813. PMLR, 2020.

Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through
awareness. In Proceedings of the 3rd innovations in theoretical computer science conference,
pages 214–226. ACM, 2012.

Harrison Edwards and Amos Storkey. Censoring representations with an adversary. arXiv preprint

arXiv:1511.05897, 2015.

Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Lavi-
olette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks.
The Journal of Machine Learning Research, 17(1):2096–2030, 2016.

GDPR. General data protection regulation. URL https://gdpr-info.eu/art-22-gdpr/.

[Online; accessed 13-May-2021].

Peter D Grünwald, A Philip Dawid, et al. Game theory, maximum entropy, minimum discrepancy

and robust bayesian decision theory. Annals of statistics, 32(4):1367–1433, 2004.

Jihun Hamm. Minimax filter: Learning to preserve privacy from inference attacks. The Journal of

Machine Learning Research, 18(1):4704–4734, 2017.

Moritz Hardt, Eric Price, Nati Srebro, et al. Equality of opportunity in supervised learning. In

Advances in neural information processing systems, pages 3315–3323, 2016.

James E Johndrow, Kristian Lum, et al. An algorithm for removing sensitive information: application
to race-independent recidivism prediction. The Annals of Applied Statistics, 13(1):189–220, 2019.

Melvin Johnson, Mike Schuster, Quoc V Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil
Thorat, Fernanda Viégas, Martin Wattenberg, Greg Corrado, et al. Google’s multilingual neural
machine translation system: Enabling zero-shot translation. Transactions of the Association for
Computational Linguistics, 5:339–351, 2017.

Faisal Kamiran and Toon Calders. Classifying without discriminating. In 2009 2nd International

Conference on Computer, Control and Communication, pages 1–6. IEEE, 2009.

Toshihiro Kamishima, Shotaro Akaho, and Jun Sakuma. Fairness-aware learning through regulariza-
tion approach. In 2011 IEEE 11th International Conference on Data Mining Workshops, pages
643–650. IEEE, 2011.

Toshihiro Kamishima, Shotaro Akaho, Hideki Asoh, and Jun Sakuma. Fairness-aware classifier
with prejudice remover regularizer. In Joint European Conference on Machine Learning and
Knowledge Discovery in Databases, pages 35–50. Springer, 2012.

Mohammadali Khosravifard, Dariush Fooladivanda, and T Aaron Gulliver. Confliction of the
IEICE Transactions on Fundamentals of

convexity and metric properties in f-divergences.
Electronics, Communications and Computer Sciences, 90(9):1848–1853, 2007.

24

INHERENT TRADEOFFS IN LEARNING FAIR REPRESENTATIONS

Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-offs in the fair determi-

nation of risk scores. arXiv preprint arXiv:1609.05807, 2016.

Thibaut Le Gouic, Jean-Michel Loubes, and Philippe Rigollet. Projection to fairness in statistical

learning. arXiv e-prints, pages arXiv–2005, 2020.

Friedrich Liese and Igor Vajda. On divergences and informations in statistics and information theory.

IEEE Transactions on Information Theory, 52(10):4394–4412, 2006.

Jianhua Lin. Divergence measures based on the shannon entropy. IEEE Transactions on Information

theory, 37(1):145–151, 1991.

Christos Louizos, Kevin Swersky, Yujia Li, Max Welling, and Richard Zemel. The variational fair

autoencoder. arXiv preprint arXiv:1511.00830, 2015.

Kristian Lum and James Johndrow. A statistical framework for fair predictive algorithms. arXiv

preprint arXiv:1610.08077, 2016.

David Madras, Elliot Creager, Toniann Pitassi, and Richard Zemel. Learning adversarially fair and
transferable representations. In International Conference on Machine Learning, pages 3381–3390,
2018.

Aditya Krishna Menon and Robert C Williamson. The cost of fairness in binary classification. In

Conference on Fairness, Accountability and Transparency, pages 107–118, 2018.

Arvind Narayanan. Translation tutorial: 21 fairness definitions and their politics. In Proc. Conf.

Fairness Accountability Transp., New York, USA, 2018.

Geoff Pleiss, Manish Raghavan, Felix Wu, Jon Kleinberg, and Kilian Q Weinberger. On fairness and
calibration. In Advances in Neural Information Processing Systems, pages 5680–5689, 2017.

Jiaming Song, Pratyusha Kalluri, Aditya Grover, Shengjia Zhao, and Stefano Ermon. Learning
controllable fair representations. In Artificial Intelligence and Statistics, pages 2164–2173, 2019.

Yihong Wu. Lecture notes on information-theoretic methods for high-dimensional statistics. Lecture

Notes for ECE598YW, 16, 2017.

Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Fairness

constraints: Mechanisms for fair classification. arXiv preprint arXiv:1507.05259, 2015.

Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Fair-
ness beyond disparate treatment & disparate impact: Learning classification without disparate
mistreatment. In Proceedings of the 26th International Conference on World Wide Web, pages
1171–1180. International World Wide Web Conferences Steering Committee, 2017.

Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations.

In International Conference on Machine Learning, pages 325–333, 2013.

Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating unwanted biases with adversarial
learning. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, pages
335–340. ACM, 2018.

25

ZHAO AND GORDON

Han Zhao. Costs and benefits of wasserstein fair regression. arXiv preprint arXiv:2106.08812, 2021.

Han Zhao and Geoff Gordon. Inherent tradeoffs in learning fair representations. Advances in neural

information processing systems, 32:15675–15685, 2019.

Han Zhao, Jianfeng Chi, Yuan Tian, and Geoffrey J. Gordon. Adversarial privacy preservation under

attribute inference attack. arXiv preprint arXiv:1906.07902, 2019a.

Han Zhao, Remi Tachet des Combes, Kun Zhang, and Geoffrey J Gordon. On learning invariant
representation for domain adaptation. In International Conference on Machine Learning, 2019b.

Han Zhao, Amanda Coston, Tameem Adel, and Geoffrey J. Gordon. Conditional learning of fair

representations. arXiv preprint arXiv:1910.07162, 2019c.

Han Zhao, Junjie Hu, and Andrej Risteski. On learning language-invariant representations for
universal machine translation. In International Conference on Machine Learning, pages 11352–
11364. PMLR, 2020.

Indre Zliobaite. On the relation between accuracy and fairness in binary classification. arXiv preprint

arXiv:1505.05723, 2015.

26

