1
2
0
2

t
c
O
7
2

]

G
L
.
s
c
[

1
v
5
9
2
4
1
.
0
1
1
2
:
v
i
X
r
a

A Subgame Perfect Equilibrium Reinforcement Learning
Approach to Time-inconsistent Problems

Nixie S. Lesmana
Chi Seng Pun
School of Physical and Mathematical Sciences
Nanyang Technological University
Singapore

Editor: TBA

nixiesap001@e.ntu.edu.sg
cspun@ntu.edu.sg

Abstract

In this paper, we establish a subgame perfect equilibrium reinforcement learning (SPERL)
framework for time-inconsistent (TIC) problems. In the context of RL, TIC problems are
known to face two main challenges: the non-existence of natural recursive relationships
between value functions at diﬀerent time points and the violation of Bellman’s principle of
optimality that raises questions on the applicability of standard policy iteration algorithms
for unprovable policy improvement theorems. We adapt an extended dynamic programming
theory and propose a new class of algorithms, called backward policy iteration (BPI), that
solves SPERL and addresses both challenges. To demonstrate the practical usage of BPI
as a training framework, we adapt standard RL simulation methods and derive two BPI-
based training algorithms. We examine our derived training frameworks on a mean-variance
portfolio selection problem and evaluate some performance metrics including convergence
and model identiﬁability.

Keywords: Time Inconsistency, Reinforcement Learning, Consistent Planning, Intraper-
sonal Game, Subgame Perfect Equilibrium, Training Algorithms, Mean-Variance Analysis

1. Introduction

Time-inconsistent (TIC1) performance criterion arises as a result of decision-theoretic plan-
ning in order to reﬂect more closely human’s preferences that are prone to bounded rational-
ity (see Simon (1955); Simon et al. (2008)), biases, and fallacies. In these preference models,
decision alternatives are evaluated using various psychological principles that include but
are not limited to present bias, loss aversion, nonlinear probability weighting, and projection
bias; see the (cumulative) prospect theory in behavioral economics developed in Kahneman
and Tversky (1979a); Tversky and Kahneman (1992). As a result of these biases, TIC can

1. In this paper, the abbreviation TIC could refer to time-inconsistent as an adjective or time inconsistency

as a noun, whichever is appropriate.

1

 
 
 
 
 
 
then be described as a situation in which a plan, consisting of current and future actions,
could be optimal today but might not be optimal in the future. In the context of dynamic
game theory, TIC, also known as dynamic inconsistency, manifests itself through the vio-
lation of Bellman’s principle of optimality (BPO) by the dominant player (i.e. the future);
see Simaan and Cruz (1973). Examples include endogenous habit formation in economics
(Fuhrer (2000)) and mean-variance criteria in ﬁnance (Markowitz (1952)).

Recently, TIC criterion has grown in prevalence alongside artiﬁcial intelligence (AI) prolifer-
ation, as more and more AI agents are centered around human, e.g., assistive, autonomous,
and humanoid robots. However, modelling realistic human’s preferences is not the only
cause of TIC. In fact, any eﬀorts to modify a time-consistent (TC) optimization/control
criterion may result in TIC. One of the largest contributors to such AI advances is arguably
the ﬁeld of reinforcement learning (RL). An RL agent aims to solve decision-making/control
problems, but with methods distinguishable from analytical control solvers, resulting in a
In particular, RL training methods are often drawn from
broader scope of capabilities.
animal learning psychology, where it is a common conception that on top of goal-directed
behaviour, it is also important to encode seemingly unrelated behaviour to help achieve
the speciﬁed goal. Such an encoding can range from simple reward engineering to the ﬁeld
of (optimal) reward design with some involved modiﬁcations of reward functions as well
as the control criterion itself. These modiﬁcations, while maybe inspired by human’s cog-
nition, are often practically motivated to overcome a designer’s limited ability to observe
all intricacies related to the environment and capture them into a single goal functional
at initialization. For instance, modiﬁcations are necessary when bounded resources cause
an RL agent to behave unexpectedly, failing to achieve the intended behavior encoded in
the original goal functional, or when designer wants to encode abilities to promote adap-
tivity and autonomy; e.g., Schmidhuber (1991); Chentanez et al. (2004); Bellemare et al.
(2016); Achiam and Sastry (2017). An interesting crossover between the two is shown by
Fedus et al. (2019), where behaviorally-motivated (TIC) hyperbolic-agent can also serve
as a practically-motivated auxiliary task to improve performance against TC exponential-
agent in several domains. It is noteworthy that an RL framework for the problems under a
TIC criterion, e.g., hyperbolic discounting (see Laibson (1997); Frederick et al. (2002)), en-
hances the modelling feasibility, where psychological principles can be introduced to reﬂect
the agent’s intrinsic objective beyond the expected utility theory.

In his seminal work, Strotz (1955) summarizes two types of decision makers, who tackle
with TIC seriously: (i) a pre-committer, who is TIC-aware but chooses to focus on the
planning solely at the initial time point by determining her plan once at the beginning and
committing to it throughout the planning horizon, and (ii) a sophisticated agent, who is
also TIC-aware but unable or unwilling to pre-commit and thus chooses consistent planning
as a compromise by ﬁnding a plan that is optimal at any given time in consideration of
her future disobedience. From an optimization and control perspective, a pre-commiter
solves a globally optimal solution to the TIC problem, while a sophisticated agent solves a
intrapersonal equilibrium solution.

2

Contrary to the given terminologies, there are several circumstances in which we prefer con-
sistent plans than globally optimal ones. First, by deﬁnition, the so-called globally optimal
plans are only optimal at initial time and state disregarding the fact that such plan might
not be optimal when assessed at future time point. The main drawback for pre-committers
is their commitment to a plan determined at the beginning while sticking with it through-
out. Especially for a stochastic environment, as time evolves, it could deviate from what the
agent anticipated at the beginning, resulting in the degeneracy of a pre-commitment plan,
and the problem is more pronounced when the planning horizon is long. Empirical evidence
reveals that in their natural state (i.e. without relying on commitment devices), humans
tend to reassess their plan at some future times and states making them prone to future
deviations for a lack of self control; see Kahneman and Tversky (1979b); Bondt and Thaler
(1985). Therefore, in the event when commitment devices are unavailable or not useful,
being (time-)consistent is a rational choice. For instance, let us consider an online learning
agent that is acting on behalf of human. In this case, we want our agent to keep its plan open
for re-evaluation at future times and states to anticipate some changes in the environment
and update its belief or information set accordingly. Consistent plans are robust2 under
such future re-evaluations, while globally optimal plans are not. Second, there is lack of a
pivotal tool to identify pre-commitment plans. In the context of stochastic controls, BPO
violation renders standard DP techniques inapplicable. Although an embedding technique
can be employed for some speciﬁc criteria such as mean-variance, initiated by Li and Ng
(2000) and later adapted to RL by Wang and Zhou (2020), such technique is diﬃcult to
extend to general problem speciﬁcations, e.g., state-dependent problems. In fact, the em-
bedding technique is mainly useful in handling the TIC source from the nonlinearity with
respect to the reward-to-go, while a more general approach is to formulate and attack the
problem with McKean–Vlasov DP; see Pham and Wei (2017); Lei and Pun (2020). Similar
approaches to such a technique can also be found in the RL context, such as Mannor and
Tsitsiklis (2011); Evans et al. (2016). These approaches also involve a TC reformulation to
the original problem of searching globally optimal solution to a speciﬁc TIC criterion but
in an approximate or less formal sense. The remaining alternative is then to use trajectory
enumeration techniques that apply for more general TIC sources. However, such techniques
are not eﬃcient and quickly become intractable in cases such as stochastic environments.

The problem of identifying consistent plans in (ﬁnite-horizon) TIC control has been explored
extensively since the seminal work of Strotz (1955). The current popularity of consistent
plans in TIC control literature can be attributed to its formalism as intrapersonal equilib-
ria, in which an agent’s selves at diﬀerent times (while with the same terminal time) are
considered to be self-interested players of a game and a consistent plan is characterized by
a subgame perfect equilibrium (SPE) of the game, where no selves have the incentive to
deviate. Such formalism was initiated through the study of classical (continuous-time) Ram-
sey model with a non-exponential discount function in a series of papers; see Ekeland and
Lazrak (2006); Ekeland and Pirvu (2008); Ekeland and Lazrak (2010). Similar works that

2. Here, robustness refers strictly to the agent’s TIC sources (which does not concern the risk and un-
certainty coming from the external environment). TIC sources are an agent’s internal attributes and
thus, assumed to be known at initialization when agent is TIC-aware. Sophisticated agent exploits this
knowledge to devise its ”robust” consistent plan, while precommiter does not.

3

adopt game-theoretic line of reasoning, but less formally, can be found in Pollak (1968);
Phelps and Pollak (1968); Peleg and Yaari (1973); Barro (1999); Luttmer and Mariotti
(2003), where diﬀerent application problems in both discrete- and continuous-time setting
are considered. In an eﬀort to unify the game-theoretic views, Bj¨ork and Murgoci (2014)
proposes an extended DP theory in the context of discrete-time TIC stochastic controls.
In this work, the authors derive a Bellman equation like system for relatively general TIC
criterion to characterize an equilibrium value function and then solve the system recur-
sively backward to obtain the corresponding SPE policy in various application examples. A
continuous-time extension to this general theory is proposed in Bj¨ork et al. (2017), where a
system of Hamilton–Jacobi–Bellman (HJB) equations is derived; if solvable, solution to such
system can be obtained by the use of the partial diﬀerential equation (PDE) tools (see Lei
and Pun (2021)). Standard procedure can then be applied for practical implementation of
the SPE policy derived as above that is, by estimating the parameters in the modelled state
dynamics (i.e. stochastic diﬀerential equations, transition probabilities) and substituting
these estimates into the analytically derived SPE policy form. Despite the close connection
between DP and RL, the exploration of SPE policy and extended DP remains scarce in
RL literature, except for a few related works on speciﬁc tasks, whose subtle diﬀerence from
ours is illustrated technically in Section 2.3 below.

In this paper, we formalize the search of SPE policy as a reinforcement learning (SPERL)
problem under general (task-invariant) TIC objectives and limit the scope of our study
to ﬁnite-horizon, discrete-time problems. In an RL context, TIC problems are known to
exhibit two challenges: (i) the non-existence of natural (action-)value recursion given ﬁxed
policy, and (ii) the questionable applicability of standard policy iteration algorithms due to
unprovable policy improvement theorems (PIT). We propose a new class of algorithms called
backward policy iteration (BPI) that addresses both challenges. First, by extending Bj¨ork
and Murgoci (2014)’s value recursion to action-value recursion, we obtain TIC-adjusted
temporal-diﬀerence (TD)-based policy evaluation method that recursively links Q-functions
at diﬀerent time points. Second, by applying SPE notion of optimality to structure our
policy search, we show that BPI is lexicographically monotonic; this result is parallel to
PIT in standard RL problems. Building on this monotonicity result, we can obtain the
theoretical guarantees for BPI, such as ﬁnite termination/convergence and characterization
of converged policy as SPE policy.

Our primary contribution is to propose BPI as a SPERL solver and study its analyti-
cal properties as mentioned above. To address some intractability issues of BPI, we further
investigate the adaptation of standard RL simulation methods to BPI. We explore both tab-
ular and function approximation cases and derive training algorithms, similar to Q-learning
and Deterministic Actor-Critic, which in course reveals both contrasts and similarities be-
tween BPI-based and standard PI-based algorithms. We consider a mean-variance analysis
application to exemplify how the derived algorithms can be used in practice and evaluate
some performance metrics, including convergence and model identiﬁability.

The remainder of this paper is organized as follows. Section 2 formulates a general TIC
problem for our RL study. The concept of SPE is elaborated in Section 2.2, while the

4

related works of both SPE-alike and non-SPE solutions are reviewed in Section 2.3. Section
3 lays out the theoretical foundations to our proposed RL framework for training SPE
policies structured along the line of policy iteration, namely policy evaluation and policy
improvement, where the new BPI algorithm for both discrete and continuous state-action
spaces is proposed. Section 4 incorporates the standard RL simulation methods into BPI
and speciﬁes how to adapt BPI’s key rules. Section 5 elaborates the training algorithms
under the proposed framework with a well-known ﬁnancial example of dynamic portfolio
selection. Section 6 concludes.

2. Preliminaries

In this section, we will deﬁne the subgame perfect equilibrium reinforcement learning
(SPERL) problem that we are addressing and provide some backgrounds on its two central
concepts: the ﬁnite-horizon TIC-RL problems and the SPE notion of optimality.

2.1 Finite-Horizon TIC Control Problems

.
Let T
= {0, 1, . . . , T − 1} be a discrete time set of decision epochs with ﬁnite time horizon
T ∈ Z+. A ﬁnite-horizon TIC-RL problem is then deﬁned as policy search in ﬁnite-horizon
TIC MDP which consists of the standard ﬁnite-horizon tuple (T , {Xt}t∈T , {Ut}t∈T , P ) and
a TIC performance criterion, each of which will be detailed in the following.

t,x(·)

For each t ∈ T , we assume general state-spaces Xt and action-spaces Ut and deﬁne tran-
.
sition probability measures pu
= P (Xt+1 = ·|Xt = x, Ut = u) on Xt+1. We note on
the stationary transition assumption imposed here. Let us denote by ΠM D the set of all
.
deterministic Markovian policies π
= {πt : t ∈ T }, where πt : Xt → Ut, ∀t. Here onward, we
will restrict our attention to the policies in the set ΠM D. To ease the notational burden, we
introduce some sequential notations to denote the subprocesses and subsets corresponding
to a subsequence of T .

Deﬁnition 1 (Policy Sequences). We deﬁne some sequential notations for policies π ∈
ΠM D and their truncations,

1. ∀k, n ∈ T , k ≤ n, n

k π

.
= {πt : k ≤ t ≤ n}.

2. When the last time index n = T − 1, we shorten our notation by kπ

3. When the start time index k = 0, we shorten our notation by nπ

.
= T −1
k π.
.
= n

0 π and in

particular π

.
= T −1

0 π.

Note that the right superscript of policy sequences is reserved to distinguish diﬀerent policies
and the right subscript is to indicate the action at the corresponding time. Similarly for
.
= {t, t + 1, . . . , T − 1} and by tΠM D the set of all
the set notations, we denote by tT
deterministic Markovian policies tπ.

5

Given a decision epoch t and an observed current system state xt ∈ Xt, tπ prescribes an
action selection πt(xt) ∈ Ut which then drives the transition of our MDP to the next system
state xt+1 ∈ Xt+1 according to pπ

.
= P (Xt+1 = xt+1|Xt = xt, Ut = πt(xt)).

t,x(xt+1)

In standard ﬁnite-horizon RL problems, a TC performance criterion takes the form

V π
std,τ (y)

.
= Eτ,y

(cid:34)T −1
(cid:88)

t=τ

Rt(X π

t , πt(X π

t )) + F(X π
T )

(cid:35)

.
= Eτ,y

(cid:34) T

(cid:88)

(cid:35)

Rπ
t

,

t=τ

(1)

where (τ, y = X π
t ) represents the current/evaluation time and state and the superscript π
indicates the policy being followed. We note that the latter presentation is more commonly
.
= Rt(Xt, Ut) for
found in RL literature to account for random intermittent rewards Rt
.
= F(XT ) emitted by the environment upon hitting
t ∈ T and random terminal reward RT
the state-action pair (Xt, Ut) at time t < T and state XT at time t = T .

A SPERL problem instead considers the following TIC performance criterion of the form

V π
τ (y)

.
= Eτ,y

(cid:34)T −1
(cid:88)

t=τ

Rτ,t (y, X π

t , πt(X π

t )) + Fτ (y, X π
T )

(cid:35)

+ Gτ (y, Eτ,y[X π

T ]) .

(2)

As compared to the TC criterion in (1), we can observe some notable diﬀerences which
make up the TIC sources3 of the criterion (2): (i) the dependency of reward functions R
and F on the current time and state, (τ, y), and (ii) the appearance of term Gτ (y, z) that
is non-linear in the z-variable.

.
= H(τ, y, F(XT ))

Remark 2 (Random rewards). It is possible to incorporate random rewards into the TIC
performance criterion above by imposing additional assumption on the reward functions
.
R and F. For instance, we can deﬁne Rτ,t(y, Xt, Ut)
= H(τ, y, Rt)
.
and Fτ (y, XT )
= H(τ, y, RT ) where H can be considered as some TIC
transformation of raw rewards and is required to be deterministic. We may then ﬁt some
popular TIC rewards into this form: H(τ, y, R)
1+h(T −τ ) in hyperbolically discounted
.
= γ
y R in state-dependent problems for some constants h and γ.
problems or H(τ, y, R)
However, random rewards are rarely considered in TIC control literature due to their focus
on analytical solutions. Thus, for most derivations in this paper, we will stick to the form
in (2) and revisit the random reward case as a short remark in Section 4.

.
= H(τ, y, Rt(Xt, Ut))

.
=

R

2.1.1 BPO Violation and TIC

We now describe the issue of BPO violation under TIC that lead to the splitting of globally
optimal and locally optimal policies, which eventually motivates the SPE notion of optimal-
ity. Let us introduce some notations for a generic criterion that could be (1) or (2). Under
the standard notion of optimality, a policy search given a criterion aims to ﬁnd a (globally)
optimal policy at the beginning time 0: π∗0 .
0 (x0). Let us also consider

= arg maxπ∈ΠM D V π

3. We refer readers to (Bj¨ork and Murgoci, 2014, Sections 1.2, 7-9) for domain-speciﬁc examples of each
source which include non-exponential discounting for type (i) and variance-related term for type (ii).

6

local problems Pτ,y indexed by the initial time τ ∈ T and state y. Similarly, we may obtain
under the standard notion of (local) optimality, π∗τ .

= arg maxπ∈τ ΠM D V π

τ (y).

Under standard TC criterion as in (1), the optimal solutions for the local problem Pτ,y and
the global problem P0,x0 are linked by BPO which states

sπ∗0 = sπ∗τ ,
In fact, (3) also implies that sπ∗t = sπ∗τ for any t ≤ τ and s ∈ τ T .
In other words,
globally optimal and locally optimal solutions are identical and in this case, the so-called
pre-commitment policy is also SPE from a game-theoretic perspective.

∀τ ∈ T , s ∈ τ T .

(3)

However, under the TIC criterion (2), the BPO relation (3) no longer holds, causing the split
between the locally optimal policy π∗τ and the globally optimal policy π∗0. The globally
optimal policy π∗0 is called the pre-commitment policy and it is usually found by means
other than DP, which is no longer applicable without BPO. Moreover, π∗0 is not SPE. The
BPO violation can be viewed as the consequence of conﬂicting objectives in the collection
of local problems {Pt,x : t ∈ T , x ∈ Xt}, motivating the game-theoretic reformulation of
TIC problems as an intrapersonal game, which will be detailed in the next subsection. The
intrapersonal equilibrium (i.e. SPE) solution, if exists, recovers the BPO relation (3) by
modifying how we deﬁne π∗ for all local problems.

Remark 3. In the literature on TIC problems, there is the third class of ‘optimality’ other
than the pre-commitment and the SPE notions, namely dynamically optimality; see Pedersen
and Peskir (2016). A dynamically optimal policy is constructed by the collection of locally
: t ∈ T }, where π∗t is the optimal pre-
optimal solutions at all time points, i.e. {π∗t
t
commitment policy to the local problem Pt,x. However, this formation does not exploit the
linkage among the local problems and such a construction needs to be justiﬁed. A closely
related concept regarding the latter is called time consistency in eﬃciency; see Cui et al.
(2017); Pun and Ye (2021). However, since dynamically optimal policies are generally lack
of interpretation power and theoretical guarantees, we focus on the SPE policies.

2.2 SPE Notion of Optimality

Given the ﬁnite-horizon discrete-time TIC criterion in (2), SPERL’s objective is to ﬁnd a
SPE policy, which will be deﬁned shortly after a few deﬁnitions and notational assumptions.

Deﬁnition 4 (Delaying Operator). We denote by u ⊕t π the concatenation between the use
of action u ∈ Ut at any given time t ∈ T and the delayed use of policy t+1π. Hereafter, we
adopt a convention that u ⊕t
Deﬁnition 5 (Action-Value Functions). Given any ﬁxed policy π ∈ ΠM D and its cor-
responding value function V π
t (x) deﬁned in (2) for t ∈ T , we deﬁne (policy-dependent)
action-value function

mπ for m ≤ t is simply an action u ∈ Ut at time t ∈ T .

(4)
Remark 6. In Deﬁnition 5, the policy tπ does not play a role and the policy t+1π is ﬁxed,
while u ∈ Ut is the action variate at time t with the state x.

t (x, u)

(x).

Qπ

.
= V u⊕tπ
t

7

Deﬁnition 7 (SPE Policy). A policy π∗ ∈ ΠM D is an SPE policy if

Qπ∗

t (x, π∗

t (x)) ≥ Qπ∗

t (x, u),

∀t ∈ T , x ∈ Xt, u ∈ Ut.

(5)

SPERL’s search objective is inspired by the intrapersonal equilibria characterization of time-
consistent plans that aim to solve the conﬂicts between the objectives in the local problem
set {Pt,x : t ∈ T , x ∈ Xt} by reformulating them as an SPE search in a sequential subgames
played by self-interested T -indexed players, which goes as follows:

At each round t, only player t is allowed to move by choosing a strategy in the
form of a mapping πt : Xt → Ut. Player t’s objective is to maximize his/her
.
= V πt⊕tπ
expected payoﬀ Qπ
t = x) and
t
has perfect information on the future players’ strategies t+1π.

(x). Player t can observe (t, X π

t (x, πt)

The SPE of the game above can be found by applying backward induction, where π∗
tained at each inductive step, resulting in SPE strategies of each player {π∗
We can easily verify that this strategy set realizes the condition in (5).

T −1, π∗

t is ob-
T −2, . . . , π∗
0}.

Remark 8 (Markovian assumption on SPE policies). By deﬁnition, an SPE policy is
Markovian, which implies that for each t ∈ T , the past (including past players’ policies
t−1π) does not inﬂuence how player t acts.

2.3 Related Works on TIC Problems in RL

After reviewing some notions of optimality, it is convenient at this stage to discuss the
related works with some technical comments before we end this section. Table 1 compares
between the TC and TIC problems in RL and the subclasses of optimality under the TIC
problems, which were discussed in Section 1. We remark here that the SPE concept is
investigated in Lattimore and Hutter (2014) for general discounting RL problems, which
provides a detailed account on the advantages of SPE policies. However, they do not focus
on solving the SPE policy search problem.

Table 1: Diﬀerent classes of RL problems and how they are attempted.

Criterion
Optimality

Update Rule

Convergence
guarantee
Task-invariant Yes

TC (w/ BPO)
BPO promises dy-
namic optimality
Policy Improvement
(PolImp)
Monotonicity
(w/ PIT)

TIC (w/o BPO)

Globally optimal plan
(Precommitment)
Embed the problem to
the one w/ PolImp
Monotonicity for the
auxiliary one
No

Consistent plan
(SPE revives BPO)
SPE-improving rule
(Deﬁnition 13 below)
Lex-monotonicity
(Theorem 25 below)
Yes

Reference

Sutton and Barto (2018) Wang and Zhou (2020)

This paper

8

It should also be noted that there are a number of works attempting a TIC-RL problem
from the perspective of learning behaviour or eﬃciency instead of optimality. Hence, each
of their algorithms may learn a kind of ‘optimality’ based on a pre-speciﬁed behaviour of
the agent and it is not clear whether the converged policy falls into any class of optimality
in Table 1. Hence, in the following two paragraphs, we categorize some related works into
two streams based on their search of SPE-alike or non-SPE policies.

SPE-alike Policy Search in RL We highlight some diﬀerences between ours and some
related prior works that have mentioned sophisticated, locally optimal, or SPE solutions.
Evans et al. (2016) consider the learning of sophisticated behaviour under hyperbolic dis-
counted criterion. The construction of their update rules is based on TC reformulation
to learn pre-commitment plans to which sophistication is heuristically encoded afterwards.
Though the sophisticated behavior is an exact analogy of the SPE policy, this work does not
clarify whether its heuristic encoding suﬃciently characterizes sophistication (actually ter-
minates at/converges to SPE policy). Moreover, due to its TC reformulation, their method
does not apply to our more general TIC criterion (which includes state-dependency). In
Tamar and Mannor (2013); Tamar et al. (2016), actor-critic and temporal-diﬀerence (TD)-
based algorithms are proposed to learn a locally optimal policy under variance-related crite-
ria, respectively. These works are by far the closest to our approach for some similarities in
their derivation of value recursions to the extended DP theory, but the relation of the learnt
policies to SPE policy remains obscure for two reasons: (i) SPE policy is deﬁned under de-
terministic notion of optimality, while both algorithms use stochastic policy representation;
(ii) both works adopt gradient-based methods shifting the optimization landscape to that
of policy parameters.

Non-SPE Policy Search in RL We brieﬂy review some solution alternatives that do
not fall into either class of aforementioned solutions. Recently, TIC is often handled from
a tool-/task-oriented angle, that is by overcoming the diﬃculties of applying speciﬁc tools
(derived under TC criteria) to a speciﬁc TIC task through clever maneuvers of task-speciﬁc
properties. Though such approaches may be eﬀective in handling some speciﬁc TIC crite-
ria, their applicability to other TIC tasks remains unknown. Moreover, the tool-oriented
handling of diﬃculties may cause the lost of “optimality” of the obtained solutions. For
instance, in the context of RL, Q-learning (Watkins and Dayan (1992)) is a globally-optimal
policy search tool given TC criteria under deterministic notion of optimality. A popular
RL algorithm designed for hyperbolic-discounted criterion is µAgent (see Kurth-Nelson and
Redish (2010) and Fedus et al. (2019)), that learns through a shared representation of Q-
learning like agents. However, the policy learnt by such modiﬁed Q-learning as part of
µAgent has an unknown theoretical “optimality”4. For the above reasons, we will pivot on
the control/optimization perspective to preserve the main characteristics of TIC, without
relying on speciﬁcations of tasks or tools.

4. For empirical studies relevant to uncovering the ”optimality” property of hyperbolic-discounted RL

algorithms, see Kurth-Nelson and Redish (2010).

9

3. SPERL Framework

In this section, we will lay out some foundations to our proposed framework for training SPE
policies structured along the line of policy iteration. We divide our discussion into two parts:
policy evaluation and policy improvement. In regards of policy evaluation, we will derive a
recursive system satisﬁed by the TIC Q-function given ﬁxed policy π, as deﬁned in (4). At
this stage, we have not applied any game-theoretic concepts and instead focus on the validity
of the recursive evaluation scheme. We will elaborate on how to use these π-dependent Q-
functions to structure our search for an SPE policy in our policy improvement. Borrowing
the SPE notion of optimality, we will propose a new class of policy iteration algorithms,
called backward policy iteration (BPI), which possess desirable analytical properties such
as monotonicity (i.e. policy improvement theorem-alike) and convergence to SPE policy in
both discrete and continuous state-action spaces.

3.1 Policy Evaluation (PolEva)

In this subsection, we set up a recursive evaluation of expected TIC rewards given a ﬁxed
policy π. Unlike in the case of TC rewards, there is no natural recursive equations between
TIC value functions; be it state-value or action-value functions. We apply the techniques,
which were used to derive the extended Bellman equations in Bj¨ork and Murgoci (2014), to
obtain a backward inductive policy evaluation (PolEva) scheme.

First, we deﬁne a few adjustment functions that will help tracking the non-stationary
changes in the Q-recursion.

Deﬁnition 9 (Adjustment Functions). Given any ﬁxed policy π ∈ ΠM D, we deﬁne the
following (policy-dependent) adjustment functions.

f π,τ,y
(x, u)
t
gπ
t (x, u)
rπ,τ,m,y
t

(x, u)

.
= Et,x
.
= Et,x
.
= Et,x

)(cid:3) ,

(cid:2)Fτ (y, X u⊕tπ
T
(cid:3) ,
(cid:2)X u⊕tπ
T
(cid:16)
(cid:104)
y, X u⊕t
Rτ,m
m

m−1π

, πm(X u⊕t

m

m−1π

(cid:17)(cid:105)
)

(6)

(7)

(8)

for t ∈ T , τ ∈ tT , m ∈ τ T , y ∈ Xτ , x ∈ Xt, and u ∈ Ut.

Remark 10. Again, we note that by Deﬁnitions 5 and 9, for each player t, its action-value
function and adjustment functions are independent of past players’ policies t−1π.

Now, we are ready to present the main result of this subsection, from which we later set up
our TIC PolEva scheme.

Proposition 11 (Policy-dependent TIC Q-recursion). Let π ∈ ΠM D be any ﬁxed policy
and Q, f, g, r deﬁned as in Deﬁnitions 5 and 9. Then, the following holds for every ﬁxed
t ∈ T , τ ∈ tT , m ∈ τ T , y ∈ Xτ , x ∈ Xt, and u ∈ Ut,

10

1. the adjustment function rπ,τ,m,y satisfy the equations

rπ,τ,m,y
t
rπ,t,t,y
t
rπ,T −1,T −1,y
T −1

(cid:2)rπ,τ,m,y
t+1

(x, u) = Et,x
(x, u) = Et,x [Rt,t(y, x, u)] , for t < T − 1,
(x, u) = ET −1,x [RT −1,T −1(y, X u

t+1, πt+1(X u

(X u

T , u)] ;

t+1))(cid:3) , for m (cid:54)= t, t < T − 1,(9)
(10)

2. the adjustment function f π,τ,y satisfy the equations

(x, u) = Et,x

f π,τ,y
t
f π,τ,y
T −1 (x, u) = ET −1,x [Fτ (y, X u

t+1 (X u

(cid:2)f π,τ,y

T )] ;

t+1, πt+1(X u

t+1))(cid:3) , for t < T − 1,

3. the adjustment function gπ satisfy the equations

(cid:2)gπ
gπ
t (x, u) = Et,x
T −1(x, u) = Et,x [X u
gπ

t+1(X u
T ] ;

t+1, πt+1(X u

t+1))(cid:3) , for t < T − 1,

4. the action-value function Qπ satisﬁes the equations

(11)

(12)

(13)

(14)

(15)

Qπ

t (x, u) = rπ,t,t,x

t

(x, u) + Et,x

(cid:2)Qπ

t+1(X u

t+1, πt+1(X u

t+1))(cid:3)

(cid:40) T −1
(cid:88)

−

(cid:16)

Et,x

(cid:104)
π,t+1,m,X u
r
t+1

t+1

(X u

t+1, πt+1(X u

(cid:105)
t+1))

(16)

− rπ,t,m,x
t

(x, u)

(cid:41)

(cid:17)

(cid:110)

m=t+1
(cid:104)
Et,x
−
− (cid:8)Et,x

π,t+1,X u
t+1

f
(cid:2)Gt+1(X u

t+1

(X u

t+1, πt+1(X u

t+1))
t+1, πt+1(X u

(cid:105)

− f π,t,x
t

(x, u)

(cid:111)

t+1, gπ

t+1(X u
(x, u) + f π,T −1,x

T −1

t+1)))(cid:3) − Gt (x, gπ
T −1(x, u)).

t (x, u))(cid:9) , for t < T − 1,
(17)

(x, u) + GT −1(x, gπ

Qπ

T −1(x, u) = rπ,T −1,T −1,x

T −1

Proof. This proof is similarly developed as in Bj¨ork and Murgoci (2014). The boundary
values (10), (11), (13), (15), and (17) can be easily computed from Deﬁnitions 5 and 9.

1. For the r recursion and m (cid:54)= t, we have

(cid:104)

rπ,τ,m,y
t

(x, u) = Et,x

= Et,x

m−1π

Rτ,m(y, X u⊕t
(cid:20)
Et+1,X u

m
(cid:20)
Rτ,m(y, X

t+1

, πm(X u⊕t

m

m−1π

(cid:105)
))

(by (8))

m−1
t+1 π
m

, πm(X

m−1
t+1 π
m

))

(cid:21)(cid:21)

(by the tower rule)

= Et,x

(cid:2)rπ,τ,m,y

t+1

(X u

t+1, πt+1(X u

t+1))(cid:3) .

(by (8))

2. For the f recursion and t < T − 1, by (6) and the tower rule, we similarly have

f π,τ,y
t

(x, u) = Et,x
= Et,x

)(cid:3) = Et,x
(cid:2)Fτ (y, X u⊕tπ
T
(cid:2)f π,τ,y
t+1, πt+1(X u
t+1 (X u

t+1))(cid:3) .

(cid:104)

Et+1,X u

t+1

(cid:2)Fτ

(cid:0)y, X t+1π

T

(cid:1)(cid:3)(cid:105)

3. For the g recursion and t < T − 1, by (7) and the tower rule, we similarly have

gπ
t (x, u) = Et,x

(cid:2)X u⊕tπ
T

(cid:3) = Et,x

(cid:104)
Et+1,X u

t+1

(cid:2)X t+1π
T

(cid:3)(cid:105)

= Et,x

11

(cid:2)gπ

t+1(X u

t+1, πt+1(X u

t+1))(cid:3) .

4. For the Q recursion and t < T − 1, by (4) and the tower rule, we have

Qπ

t (x, u) = Et,x[Rt,t(x, x, u)] + Et,x [Qπ

t+1(X u

t+1, πt+1(X u

t+1))] − Et,x [Qπ

t+1(X u

t+1, πt+1(X u

t+1))]

T −1
(cid:88)

+

m=t+1

rπ,t,m,x
t

(x, u) + f π,t,x

t

(x, u) + Gt (x, gπ

t (x, u))

= Et,x[Rt,t(x, x, u)] + Et,x [Qπ
(cid:20)
Et+1,Xu

T −1
(cid:88)

Et,x

−

(cid:20)

t+1

t+1(X u

t+1, πt+1(X u

t+1))]

Rt+1,m(X u

t+1, X

m−1
t+1 π
m

, πm(X

m−1
t+1 π
m

(cid:21)(cid:21)

))

+

T −1
(cid:88)

rπ,t,m,x
t

(x, u)

m=t+1

m=t+1
(cid:104)

− Et,x

Et+1,Xu

t+1

− Et,x [Gt+1(X u

t+1, gπ

(cid:2)Ft+1(X u
t+1(X u

)(cid:3)(cid:105)
t+1, X t+1π
T
t+1)))] + Gt (x, gπ
t+1, πt+1(X u

+ f π,t,x
t

(x, u)

t (x, u)) ,

which gives the right-hand side (RHS) of (16) by noting (6)-(8).

The SPERL PolEva scheme follows trivially by making the updates of f, g, r and Q ﬂow
backward from t = T − 1 to 0; see Algorithm 1.

Algorithm 1: TIC-TD Policy Evaluation (PolEva)

Input : t+1π
Output: Qπ

t (x, u), ∀x, u

1 for k ← T − 1 to t do
2

for τ ← T − 1 to k do

3

4

5

6

7

8

for m ← T − 1 to τ do
Compute rπ,τ,m,y

k

end
Compute f π,τ,y

k

(x, u), ∀x ∈ Xk, u ∈ Uk, y ∈ Xτ by (9)-(11);

(x, u), ∀x ∈ Xk, u ∈ Uk, y ∈ Xτ by (12)-(13);

end
Compute gπ
Compute Qπ

9
10 end

k (x, u), ∀x ∈ Xk, u ∈ Uk by (14)-(15);
k (x, u), ∀x ∈ Xk, u ∈ Uk by (16)-(17);

Next, we will justify the validity of Algorithm 1 for computing Qπ
t (x, u). Firstly, note
that the t-indexed adjustment functions in the RHS of (16) have been computed in the
same iteration k = t by lines 2-8. We can verify the validity of the adjustment functions
computation scheme by observing that in the non-boundary cases, the (t + 1)-indexed
functions inside the expectations in the RHS of (9), (12), and (14) have all been computed
in the previous iteration k = t + 1 such that an expectation over Xt+1 can be computed.
For the boundary cases, the functions inside the expectations i.e. R, F are known such that
expectations can be computed, either over the deterministic variable in (11) or the random
XT in (13) and (15). Secondly, we observe that the (t + 1)-indexed functions inside the
expectations in the RHS of (16) have all been computed in the previous iteration k = t + 1
such that the expectation over Xt+1 can then be computed. Finally, by Proposition 11, we
have shown that (4) holds.

12

In the subsequent sections, we will refer to the integrands in the RHS of Proposition 11 as
DP targets deﬁned as follows

ξr
t (x, u, τ, m, y; t+1π)

ξf
t (x, u, τ, y; t+1π)

ξg
t (x, u; t+1π)

ξQ
t (x, u; t+1π)

.
=

.
=

.
=

.
=





(cid:40)

(cid:40)

(cid:40)

T , u),

RT −1,T −1(y, X u
Rt,t(y, x, u),
rπ,τ,m,y
(X u
t+1
Fτ (y, X u
T )
f π,τ,y
t+1, πt+1(X u
t+1 (X u
X u
T
t+1(X u
gπ
rπ,t,t,x
t
rπ,t,t,x
t

t+1, πt+1(X u
(x, u) + f π,t,x
t
t+1(X u
(x, u) + Qπ

if m = τ = t = T − 1,
if m = τ = t, ∀t < T − 1,
if m (cid:54)= t, ∀t < T − 1,

t+1, πt+1(X u

t+1)),

if t = T − 1,
otherwise,

if t = T − 1,
otherwise,

t+1))

t+1))

(x, u) + Gt(x, gπ
t+1, πt+1(X u

t+1)) − (∆rπ

t (x, u))

t + ∆f π

t + ∆gπ

t ),

(18)

(19)

(20)

if t = T − 1,
otherwise,

(21)

where

∆rπ
t

.
=

T −1
(cid:88)

(cid:16)

m=t+1

rπ,t+1,m,X u

t+1(X u

t+1, πt+1(X u

t+1)) − rπ,t,m,x

t

(x, u)

(cid:17)

,

∆f π
t
∆gπ
t

.
π,t+1,X u
= f
t+1
.
= Gt+1(X u

t+1

(X u
t+1, πt+1(X u
t+1(X u
t+1, gπ

t+1, πt+1(X u

t+1)) − f π,t,x

t

(x, u),

t+1))) − Gt(x, gπ

t (x, u)).

3.2 Policy Improvement (PolImp)

This subsection is mainly concerned about adapting the SPE notion of optimality to con-
struct an SPE-improvement scheme. We deliberately deviate from the standard policy
improvement (PolImp) scheme due to unprovable PIT under TIC5. Intuitively, we hypoth-
esize that the cause of such PIT failure lies in the deﬁnition of ‘improvement’ itself such
that the SPE notion of optimality should be accompanied by a corresponding change in the
deﬁnition of a ‘better’ policy.

To aid our result presentation in the later part of this section, we ﬁrst deﬁne several SPE-
improvement relations on the (tail) truncation of policies.

5. This has been shown by Sobel (1982) through counterexample, particularly showing that ∀s,

V π(cid:48)

(s) ≥ V π(s) (cid:54)⇒ V δ·π(cid:48)

(s) ≥ V δ·π(s).

While his work considers inﬁnite-horizon variance-related criterion, ﬁnite-horizon RL problems are often
rewritten as inﬁnite-horizon problems with time-extended state-space; see Harada (1997). Moreover, as
his argument relies mainly on BPO violation, it also applies to our case.

13

Deﬁnition 12 (SPE-Improving Rules on Truncated Policy Sequences).

kπ(cid:48) (cid:23)eq kπ ⇔

kπ(cid:48) ∼eq kπ ⇔

kπ(cid:48) (cid:31)eq kπ ⇔

(cid:16)

(cid:16)

(cid:16)

∀x ∈ Xk, Qπ(cid:48)

k (x, π(cid:48)

∀x ∈ Xk, Qπ(cid:48)

k (x, π(cid:48)

∃x ∈ Xk, Qπ(cid:48)

k (x, π(cid:48)

k(x)) ≥ Qπ(cid:48)
k(x)) = Qπ(cid:48)
k(x)) > Qπ(cid:48)

k (x, πk(x)) ∧ k+1π(cid:48) (cid:23)eq k+1π
(cid:17)

k (x, πk(x)) ∧ k+1π(cid:48) ∼eq k+1π

,

(cid:17)

,

k (x, πk(x)) ∧ kπ(cid:48) (cid:23)eq kπ

(cid:17)

.

Then, we deﬁne our SPE-improving rules on the set of policies ΠM D.

Deﬁnition 13 (SPE-Improving Rules). Let us deﬁne the following relations on the set of
all policies ΠM D such that for any arbitrary policies π(cid:48), π ∈ ΠM D,

π(cid:48) (cid:23)eq π ⇔

π(cid:48) ∼eq π ⇔

π(cid:48) (cid:31)eq π ⇔

(cid:16)

(cid:16)

(cid:16)

∀k, ∀x ∈ Xk, Qπ(cid:48)

k (x, π(cid:48)

∀k, ∀x ∈ Xk, Qπ(cid:48)

k (x, π(cid:48)

k(x)) ≥ Qπ(cid:48)
k(x)) = Qπ(cid:48)

∃k, ∃x ∈ Xk s.t. Qπ(cid:48)

k (x, π(cid:48)

k(x)) > Qπ(cid:48)

k (x, πk(x)) ∧ k+1π(cid:48) (cid:23)eq k+1π

k (x, πk(x)) ∧ k+1π(cid:48) ∼eq k+1π
(cid:17)

k (x, πk(x)) ∧ kπ(cid:48) (cid:23)eq kπ

(cid:17)

(cid:17)

,

,

.

These rules are inspired by the Nash Equilibrium (NE) concept of game-theory; speciﬁcally,
we can interpret the relation (cid:23)eq as a PolImp rule by the following statement:

“Player t’s strategy π(cid:48)

t is said to be better if playing the strategy π(cid:48)

t improves t’s utility,

given other players have the same belief about π(cid:48) (cid:23)eq π and thus play π(cid:48)

−t.”

We note that as compared to the NE concept, our SPE-improving rule restricts the set of
player t’s opponents from {0, 1, . . . , t − 1, t + 1, . . . , T − 1} to {t + 1, . . . , T − 1}, which is
implied by Remark 8 on the concept of SPE solution.

The two SPE-improving rules deﬁned above are related by the following equivalence result.

Proposition 14. Consider two arbitrary policies π, π(cid:48) ∈ ΠM D. Then, the following holds

π(cid:48) (cid:23)eq π ⇔ ∀k, kπ(cid:48) (cid:23)eq kπ,
π(cid:48) ∼eq π ⇔ ∀k, kπ(cid:48) ∼eq kπ.

(22)

(23)

Proof. First, we show that (22) holds. By Deﬁnition 13, π(cid:48) (cid:23)eq π is equivalent to

∀k, ∀x ∈ Xk, Qπ(cid:48)

k (x, π(cid:48)

k(x)) ≥ Qπ(cid:48)

k (x, πk(x)) ∧ (k+1π(cid:48) (cid:23)eq k+1π),

which by Deﬁnition 12, is equivalent to ∀k, k+1π(cid:48) (cid:23)eq k+1π. We can show (23) similarly by
replacing the relation (cid:23)eq with ∼eq.

14

Now, we introduce backward policy iteration (BPI) algorithm that can achieve these SPE-
improving rules; see Algorithm 2.

Algorithm 2: Backward Policy Iteration

: π0 (cid:54)= ∅
Input
Output : π∗ = π(cid:48)
Initialize: π(cid:48) ← π0, π ← ∅;
1 while not stable (π(cid:48) (cid:54)= π) do
2

Update π ← π(cid:48);
for k ← T − 1 to 0 do

3

4

5

6

7

8

9

10

11

1. Policy Evaluation (PolEva);
Compute Qπ(cid:48)

k (x, u), ∀x ∈ Xk, u ∈ Uk by TIC-TD (Algorithm 1);

2. Policy Improvement (PolImp);
for x ∈ Xk do

if ∃u(cid:48) ∈ Uk s.t. Qπ(cid:48)

k (x, u(cid:48)) > Qπ(cid:48)
k(x) ← u(cid:48) (arbitrarily);

Assign π(cid:48)

k (x, πk(x)) then

else

Assign π(cid:48)

k(x) ← πk(x);

end

end

end

12
13 end

Subsequently, we refer to diﬀerent parts of BPI as follows,

• TIC-TD: the recursive method developed in Section 3.1 for computing Q-values with
adjustments; see Algorithm 1. Since BPI integrates PolEva and PolImp steps at
all k ∈ T , Algorithm 1 is also integrated into this PolEva-PolImp-loop such that
k , f π(cid:48)
rπ(cid:48)

k at the previous iteration k = t + 1 can be reused to compute Qπ(cid:48)

k , Qπ(cid:48)

k , gπ(cid:48)

.

t

• PolEva-specs: every elements in the PolEva block that includes time-extended, π(cid:48)-
t (x, u)) and the use of TIC-TD-based computation.

based evaluation criteria (i.e. Qπ(cid:48)

• termination: the while-condition in the outer-loop, i.e. π(cid:48) = π.

• non-termination: the if -condition inside PolImp block.

• consistent tie-break: the element in the else-block; with consistent tie-break rule, non-

termination-condition characterizes termination.

• strictly-improving: Qπ(cid:48)

t (x, u(cid:48)) > Qπ(cid:48)

t (x, u) for any old and new actions u, u(cid:48).

• action-specs: the choice of new action u(cid:48) that is arbitrary strictly-improving one.

Next, we probe some analytical properties of the BPI algorithm with an aim to obtain
convergence results; speciﬁcally, to see whether the algorithm converges and if it does, to

15

determine the property of its converged policy. We start by showing that the BPI algorithm
satisﬁes the (weak) SPE-improving rule in Deﬁnition 13.

Proposition 15. Let π, π(cid:48) be the old and new policies obtained through BPI. Then,

∀k ∈ T , kπ(cid:48) (cid:23)eq kπ

and thus, π(cid:48) (cid:23)eq π.

Proof. We ﬁrst note that

∀k ∈ T , ∀x ∈ Xk, Qπ(cid:48)

k (x, π(cid:48)

k(x)) ≥ Qπ(cid:48)

k (x, πk(x)),

implied by the PolEva step and action search step in BPI; see Algorithm 2.

Next, we show that (24) holds. We will use (25) and backward induction to show

∀x ∈ Xk, Qπ(cid:48)

k (x, π(cid:48)

k(x)) ≥ Qπ(cid:48)

k (x, πk(x)) ⇒ kπ(cid:48) (cid:23)eq kπ.

(Base step) For the base case of k = T − 1, the premise in (26) states

(24)

(25)

(26)

∀x ∈ XT −1, Qπ(cid:48)

T −1(x, π(cid:48)

T −1(x)) ≥ Qπ(cid:48)

T −1(x, πT −1(x)),

which is equivalent to T −1π(cid:48) (cid:23)eq T −1π by Deﬁnition 12 and thus, the statement (26) holds.

(Inductive step) Suppose that the statement (26) holds for k = t + 1. Hence, by (25) for
k = t + 1, we have

Then, at k = t, the premise of (26) states that

t+1π(cid:48) (cid:23)eq t+1π.

∀x ∈ Xt, Qπ(cid:48)

t (x, π(cid:48)

t(x)) ≥ Qπ(cid:48)

t (x, πt(x)).

(27)

(28)

Combining (27) and (28), we have tπ(cid:48) (cid:23)eq tπ by Deﬁnition 12 and have thus shown that
(26) holds for k = t.

Finally, we can combine what we have from (25) and the previously shown (26) to show
that (24) holds; π(cid:48) (cid:23)eq π is directly implied by Proposition 14.

Note that Proposition 15 is an implication of π(cid:48)-based PolEva-specs, which manifests it-
self through the backward update direction in BPI. Unfortunately, being (weakly) SPE-
improving is still insuﬃcient to establish monotonicity and this is due to its non-transitivity.
To deal with this issue, we will deﬁne a lexicographical order on the policy set ΠM D. Before
that, we present more properties pertaining to the relation and implication between the old
and new policies obtained through BPI.

First, we present two results that characterize BPI’s rules of strictly-improving action-specs
and consistent tie-break.

16

Corollary 16. Let π, π(cid:48) be the old and new policies obtained through BPI. Then, ∀t ∈
T , x ∈ Xt,

( ∃u ∈ Ut s.t. Qπ(cid:48)

t (x, u) > Qπ(cid:48)

t (x, πt(x)) ) ⇒ π(cid:48)

t(x) (cid:54)= πt(x).

(29)

Proof. Direct implication of strictly-improving action-specs.

Corollary 17. Let π, π(cid:48) be the old and new policies obtained through BPI. Then, ∀t ∈
T , x ∈ Xt,

Qπ(cid:48)

t (x, π(cid:48)

t(x)) = Qπ(cid:48)

(t, πt(x)) ⇒ π(cid:48)

t(x) = πt(x).

(30)

Proof. Direct implication of consistent tie-break rule; if the premise of (30) holds, then the
search must have entered the else-block and the conclusion follows.

Then, we establish three lemmas concerning about the relations between two policies ob-
tained through adjoint iterations of the BPI algorithm.

Lemma 18. Let π, π(cid:48) be the old and new policies obtained through BPI. Then, for any
k ∈ T ,

kπ(cid:48) ∼eq kπ ⇔ kπ(cid:48) = kπ.

Proof. (⇒) We will use mathematical induction to show that for any k ∈ T ,

kπ(cid:48) ∼eq kπ ⇒ kπ(cid:48) = kπ.

(31)

(32)

As our base case, we set k = T − 1. First, note that by Deﬁnition 12, the premise of
(32) is equivalent to ∀x ∈ XT −1, Qπ(cid:48)
T −1(x, πT −1(x)), which implies that
∀x ∈ XT −1, π(cid:48)

T −1(x)) = Qπ(cid:48)
T −1(x) = πT −1(x) by Corollary 17 and thus, we have shown T −1π(cid:48) = T −1π.

T −1(x, π(cid:48)

Next, we start our inductive argument: if relation (32) applies for k = t + 1, then it also
applies for k = t. By assumption, we have

t+1π(cid:48) ∼eq t+1π ⇒ t+1π(cid:48) = t+1π.

At case k = t, by Deﬁnition 12, the premise of (32) is equivalent to

(i) ∀x ∈ Xt, Qπ(cid:48)
(ii) t+1π(cid:48) ∼eq t+1π.

t (x, π(cid:48)

t(x)) = Qπ(cid:48)

t (x, πt(x));

By applying the assumption (33), condition (35) implies

t+1π(cid:48) = t+1π.

Then, by Corollary 17, condition (34) implies

∀x ∈ Xt, π(cid:48)

t(x) = πt(x).

17

(33)

(34)

(35)

(36)

(37)

The conclusion that (32) applies for k = t follows from (36) and (37).

(⇐) We will next show the converse by induction that for any k ∈ T

kπ(cid:48) = kπ ⇒ kπ(cid:48) ∼eq kπ.

(38)

As our base case at k = T −1, we can rewrite the premise as ∀x ∈ XT −1, π(cid:48)
which implies ∀x ∈ XT −1, Qπ(cid:48)
have T −1π(cid:48) ∼eq T −1π.

T −1(x) = πT −1(x),
T −1(x, πT −1(x)) and by Deﬁnition 12, we

T −1(x)) = Qπ(cid:48)

T −1(x, π(cid:48)

Next, we start our inductive argument: if relation (38) applies for k = t + 1, then it also
applies for k = t. By assumption, we have

t+1π(cid:48) = t+1π ⇒ t+1π(cid:48) ∼eq t+1π.

(39)

At case k = t, the premise tπ(cid:48) = tπ can be written as ∀x ∈ Xt, π(cid:48)
that

t(x) = πt(x), which implies

(40)
By Deﬁnition 12, the conclusions of (39) and (40) imply tπ(cid:48) ∼eq tπ and thus, we have shown
(38) holds for k = t.

t (x, πt(x)).

∀x ∈ Xt, Qπ(cid:48)

t (x, π(cid:48)

t(x)) = Qπ(cid:48)

Lemma 19. Let π, π(cid:48) be the old and new policies obtained through BPI. In the event of
non-termination, then there exists k ∈ T such that the following hold

1. kπ(cid:48) (cid:23)eq kπ;
2. ∃x ∈ Xk, Qπ(cid:48)

k (x, π(cid:48)

k(x)) > Qπ(cid:48)

k (x, πk(x)).

Proof. First, the ﬁrst claim is always true by Proposition 15, which says

For the second claim, by Deﬁnition 12, (41) implies

∀k, kπ(cid:48) (cid:23)eq kπ.

∀k ∈ T , ∀x ∈ Xk, Qπ(cid:48)

k (x, π(cid:48)

k(x)) ≥ Qπ(cid:48)

k (x, πk(x)).

(41)

(42)

Since non-termination is assumed, we must have π(cid:48) (cid:54)= π such that ∃k, x ∈ Xk and

k(x) (cid:54)= πk(x) ⇒ Qπ(cid:48)
π(cid:48)
⇒ Qπ(cid:48)

k (x, π(cid:48)
k (x, π(cid:48)

k(x)) (cid:54)= Qπ(cid:48)
k(x)) > Qπ(cid:48)

k (x, πk(x))
k (x, πk(x)).

(by Corollary 17)

(by (42))

Hence, the second claim follows.

Lemma 20. Let π, π(cid:48) be the old and new policies obtained through BPI. In the event of
non-termination, then ∃k∗ ∈ T s.t. the following holds
∃x ∈ Xk∗, Qπ(cid:48)

k∗(x, πk∗(x)),

k∗(x, π(cid:48)

(43)

k∗(x)) > Qπ(cid:48)
k∗π,
k∗+1π.

k∗π(cid:48) (cid:23)eq
k∗+1π(cid:48) ∼eq

18

(44)

(45)

Proof. Let k∗ be the largest index in the set of k’s realizing the two claims in Lemma 19,
i.e.

k∗ = max{k ∈ T : ∃x ∈ Xk, Qπ(cid:48)

k (x, π(cid:48)

k(x)) > Qπ(cid:48)

k (x, πk(x)) ∧ kπ(cid:48) (cid:23)eq kπ},

where the set is not empty by Lemma 19. This deﬁnition means that for any k > k∗,

(cid:16)

¬

∃x ∈ Xk, Qπ(cid:48)

k (x, π(cid:48)

k(x) > Qπ(cid:48)

k (x, πk(x)) ∧ kπ(cid:48) (cid:23)eq kπ

(cid:17)

and by Deﬁnition 12, the above is equivalent to

¬(kπ(cid:48) (cid:31)eq kπ).

(46)

(47)

Then, it is clear that (43) and (44) hold by the conditions in the set of (46). Suppose
that (45) is not true, i.e. ¬ (k∗+1π(cid:48) ∼eq k∗+1π). By the second condition in the set of (46),
(k∗+1π(cid:48) (cid:31)eq k∗+1π), contradicting (47). Therefore, we have shown the existence of k∗ by
construction and the result follows.

3.2.1 Lex-Monotonicity

To characterize the monotonicity of the equilibrium policies, we introduce the lexicographi-
cal structure. Under which, we aim to show that two adjoint policies obtained through BPI
are strictly monotonic, namely lex-monotonic.

In this section, we study the continuous state-action space setting and deﬁne three notations
in which, while the subsequent results can be easily reduced to discrete setting by intro-
ducing the parallel deﬁnitions as in the remark below and thus the proof for the discrete
setting is almost identical. In this regard, diﬀerent settings manifest themselves through
diﬀerent lexicographic representations of a policy.

Deﬁnition 21 (Basis of Policy Bπ). For any ﬁxed π ∈ ΠM D, we represent the basis
of a policy Bπ by a T -dimensional vector function, whose (k + 1)th entry is deﬁned as
(Bπ)k : Xk (cid:55)→ R given by (Bπ)k(x) = Qπ

k (x, πk(x)) for k ∈ T .

Deﬁnition 22 (Element-wise order >e). Let a, b be two functions where dom(a) = dom(b).
Then, we say a >e b if a(x) ≥ b(x), ∀x ∈ dom(a) and ∃x∗ s.t. a(x∗) > b(x∗).

Deﬁnition 23 (Lexicographic order >lex). For any two policies π, π(cid:48) ∈ ΠM D, let k∗ be
the largest index k ∈ T such that (Bπ)k (cid:54)= (Bπ(cid:48))k. Then, we say Bπ >lex Bπ(cid:48) if (Bπ)k∗ >e
(Bπ(cid:48))k∗.

Remark 24. When the state space is discrete, the deﬁnitions above can be revised accord-
ingly to accommodate the analyses below. Speciﬁcally, for each π ∈ ΠM D, we deﬁne Bπ as
Rd-vector with d = (cid:80)

k|Xk|, whose entries are ﬁlled as follows.

1. For k ∈ T , order state indices for x ∈ Xk and ﬁx this order across updates.

19

2. Then, ﬁll in entries of the Rd-vector Bπ with (Bπ)k,x

state order determined above in ascending time order, i.e. the ( (cid:80)k−1
entry of Bπ is (Bπ)k,x, where jk(x) is the index of x in Xk.

.
= Qπ

k (x, πk(x)), according to the
i=0 |Xi|+jk(x))-th

We also denote by (Bπ)k = (Qπ
are ﬁlled according to a pre-ﬁxed state order for any ﬁxed k (aligned with the ﬁrst point).

k (x, πk(x)))x∈Xk the Xk-dimensional vector, whose entries

Then the element-wise order can be deﬁned for two vectors similarly as follows: we say
vectors a >e b if aj ≥ bj for any j and there is a j∗ such that aj∗ > bj∗. Subsequently, the
deﬁnition of lexicographic order in the vector case simply follows Deﬁnition 23.

Note that the lexicographical order in Deﬁnition 23 is a variant of the conventional lexi-
cographical order by the use of functions/vectors in place of scalars and the corresponding
operator >e. In the subsequent analyses, we will use the deﬁnitions above to show that BPI
is lexicographically monotonic, where the SPE-improving property obtained in Proposition
15 becomes a suﬃcient condition. Such lex-monotonicity result is parallel to the policy
improvement theorem (PIT) in standard RL approaches.

We will here onward refer to the k∗ deﬁned in Lemma 20 as our lex-index, indicating a
particular time index, after which all later time-state pairs have obtained their SPE policies.
We are now ready to establish our lex-monotonicity result.

Theorem 25 (Lex-monotonicity). Let π, π(cid:48) be the old and new policies obtained through
BPI. In the event of non-termination,

Bπ(cid:48)

>lex Bπ.

Proof. Let k∗ be the lex-index that satisﬁes (43)-(45) in Lemma 20. By Lemma 18, (44)
implies

which then implies

k∗+1π(cid:48) = k∗+1π,

∀k ≥ k∗, Qπ(cid:48)

k (x, u) = Qπ

k (x, u), ∀(x, u) ∈ Xk × Uk.

(48)

(49)

Claim 1. ∀k ≥ k∗ + 1, (Bπ(cid:48))k = (Bπ)k

Consider any k ≥ k∗ + 1. By (48), we have π(cid:48)
we have

k(·) = πk(·). By substituting them into (49),

Qπ(cid:48)

k (x, π(cid:48)

k(x)) = Qπ

k (x, πk(x)), ∀x ∈ Xk,

which says that (Bπ(cid:48))k and (Bπ)k are equal for any k ≥ k∗ + 1 that proves Claim 1.

Claim 2. At k = k∗, (Bπ(cid:48))k∗ >e (Bπ)k∗

20

Set x∗ to be one x that realizes (43). Then, we have

Qπ(cid:48)

k∗(x∗, π(cid:48)

k∗(x∗)) > Qπ(cid:48)

k∗(x∗, πk∗(x∗)) = Qπ

k∗(x∗, πk∗(x∗)).

(by (49))

Together with (45), we have (Bπ(cid:48))k∗ >e (Bπ)k∗ by the deﬁnition of >e.

Finally, from Claims 1 and 2, we conclude that Bπ(cid:48) >lex Bπ.

We note that by the strict lex-monotonicity result in Theorem 25, we have shown that the
mapping B is one-to-one, i.e. Bπ(cid:48) = Bπ ⇔ π(cid:48) = π, on the set of policies encountered in
BPI’s update. This property is central to the analysis in Section 3.2.3, where value-based
arguments are used to describe the movement of policy across updates.

3.2.2 Discrete State-Action Space (Policy-based Analysis)

Under a discrete state-action setting, we leverage the lex-monotonicity result above to show
that the BPI (Algorithm 2) terminates in ﬁnite steps.

Theorem 26 (Finite Termination). Assuming discrete state-action spaces, the BPI (Algo-
rithm 2) terminates in ﬁnite time.

Proof. First, by Theorem 25, we have that for any π, π(cid:48) consecutive policies in BPU,

Bπ(cid:48)

>lex Bπ.

(50)

This implies that the BPI visits diﬀerent basis across updates and speciﬁcally, there won’t
be any cycling of basis by the transitivity of lex-order.

Secondly, by assumption of discrete state-action spaces, we have a ﬁnitely many possible
policies to visit. Moreover, since our space of basis is completely spanned by BΠ, we also
have ﬁnitely many basis to visit, i.e. |BΠ|< ∞.

From these two observations, ﬁnite termination directly follows.

We now present the result concluding that the converged policy from BPI is a SPE policy.

Theorem 27. Let π, π(cid:48) be the old and new policies obtained through BPI. If π(cid:48) = π, then

∀t ∈ T , ∀x ∈ Xt, Qπ

t (x, πt(x)) ≥ Qπ

t (x, u), ∀u ∈ Ut.

Proof. Suppose otherwise, then the following must be true

∃t ∈ T , ∃x ∈ Xt, s.t. ∃u ∈ Ut, Qπ

t (x, u) > Qπ

t (x, πt(x)).

(51)

(52)

21

We focus on one such t

.
= t∗ and by assumption of this theorem (i.e. π(cid:48) = π), we have

By applying (53) to (52), we obtain

t∗π(cid:48) = t∗π.

∃x ∈ Xt∗, s.t. ∃u ∈ Ut∗, Qπ(cid:48)

t∗ (x, u) > Qπ(cid:48)

t∗ (x, πt∗(x)).

(53)

(54)

t∗(x∗) (cid:54)= πt∗(x∗) which
Now focus on one such x
contradicts the theorem assumption of π(cid:48) = π. Thus, we conclude that our supposition is
false and that (51) must hold.

.
= x∗. By Corollary 16, this implies π(cid:48)

Since we have shown that the BPI algorithm will converge in ﬁnite time, we have thus
shown that this policy at convergence is a SPE policy by Theorem 27. Note that this ﬁnite-
termination result and all the results derived beforehand hold for arbitrary action-specs
since the main argument is to permute over the whole policy space ΠM D that is discrete by
assumption. Unfortunately, the proof of Theorem 27 does not claim about the convergence
rate nor does it extend to more complicated setting such as continuous state-action space.
Permuting argument will treat any action-specs similarly, concluding the convergence rate
to be at most the number of permutation there is. Such analysis is not tight since diﬀerent
action-specs would lead to diﬀerent rates. The limitation of policy-based analysis is even
clearer through the continuous state-action case when permuting over inﬁnite-dimensional
spaces (i.e. (cid:13)
(cid:13) = ∞) will not conclude anything about ﬁnite termination/convergence.

(cid:13)ΠM D(cid:13)

3.2.3 Continuous State-Action Spaces (Value-based Analysis)

In this subsection, we will obtain convergence guarantees for continuous state-action spaces
by applying value-based analysis, that is to show convergence by showing B(cid:48) = B rather
than π(cid:48) = π. These two termination conditions are interchangable as long as we have one-
to-one mapping B, which through Theorem 25 has been shown to apply for any algorithm
belonging to BPI class. As described in the preceding subsection, the insuﬃciency of policy-
based analysis is caused by its permutative argument that is used to maintain generality
on the choice of action-specs. In value-based analysis, we are no longer able to keep this
generality. In what follows, we will explore three important action-specs in RL, verify that
each belongs to BPI class, and derive new convergence results for each.

Full-sweep argmax. We specify our action-specs to

π(cid:48)
k(x) ← arg max

u∈Ut

Qπ(cid:48)

t (x, u) (arbitrarily)

for any x ∈ Xt and retain the non-termination-condition of BPI, i.e.
∃u(cid:48) ∈ Uk, s.t. Qπ(cid:48)

k (x, u(cid:48)) > Qπ(cid:48)

k (x, πk(x)).

(55)

(56)

Thus, full-sweep argmax belongs to BPI class and all the preceding analyses apply. In what
follows, we will use value-based analysis to derive some convergence results.

22

To ease notation, we deﬁne the mappings qt+1π

t,x

: Ut,x → R and qt,x,u : t+1ΠM D → R by

qt+1π
t,x (u)

.
= qt,x,u (t+1π)

.
= Qπ

t (x, u).

Moreover, for each iteration i ∈ N and any ﬁxed (t, x) ∈ T × Xt, we deﬁne q(i)
qt+1π(i)
t,x
respectively.

t (x), indicating the current action-values and the current action,

(u) and u(i)
t,x

.
= π(i)

t,x(u)

.
=

Let us now consider the case when there are non-unique local-maximizers t+1π∗ such that
given a ﬁxed t, x, we will have multiple limiting action-value functions q(∞)
t,x . This will pose
an issue in the update of u(i)
t,x, whose convergence requires a well-deﬁned limiting action-
value q(∞)
t,x . In the subsequent analyses, we address such an issue by showing the existence of
an iteration index i∗
t+1, at which termination to a unique local-optima t+1π(∞) is guaranteed
to happen. Once we have such i∗
t+1, we will be dealing with a unique limiting
action-value q(∞)
and by noting that locally optimal actions with respect to this
t,x
unique action-value are well-deﬁned, the analysis on the update of u(i)
t,x can follow naturally.

.
= qt+1π(∞)
t,x

t+1, ∀i ≥ i∗

Assumption 28. At each iteration i ∈ N, for any pair (k, x) ∈ T × Xk, there exists the
global optimum of q(i)

k,x(·) over Uk, i.e. there is a map u(i)(k, x) ∈ arg maxu∈Uk q(i)

k,x(u).

Assumption 28 is related to the compactness of Uk and the continuity of q(i)
which is linked to the problem speciﬁcation.

k,x : Uk → R,

Theorem 29 (Finite Termination). Suppose that Assumption 28 holds. There exists i∗
such that for all i ≥ i∗ and t ∈ T ,

∀x ∈ Xt,

(cid:13)
(cid:13)Q(i+1)
(cid:13)

t

(x, π(i+1)
t

Moreover, we have i∗ = 1.

(x)) − Q(i)

t (x, π(i)

(cid:13)
(cid:13)
(cid:13) = 0.
t (x))

(57)

Proof. Assume any initial policy π(0). We note that showing (57) is equivalent to showing
that given i∗ = 1, ∀k ∈ T ,

∀x ∈ Xk,

(cid:13)
(cid:13)q(i∗+1)
(cid:13)

k,x

(u(i∗+1)
k,x

) − q(i∗)

(cid:13)
k,x (u(i∗)
(cid:13)
(cid:13) = 0.
k,x )

(Base step.) At the base case k = T − 1, we have

T −1,x(u) = q(i)
q(i+1)

T −1,x(u) = q∗

T −1,x(u), ∀x ∈ XT −1, ∀u ∈ UT −1

(58)

(59)

By Assumption 28, a global optimum exists. Since the full-sweep argmax action-specs
q(i+1)
prescribes u(i+1)
T −1,x(u) at i = 0, by consistent tie-break, we must
have u(2)

T −1,x, ∀x ∈ XT −1. Combining this with (59), we have shown (58).

T −1,x ← arg maxu∈UT −1

T −1,x = u(1)

23

(Inductive step.) Set i = 1. Suppose that (57) holds for k = t + 1, we have t+1π(i+1) =
t+1π(i). By Lemma 18, this implies

∀x ∈ Xt, u ∈ Ut, q(i+1)

t,x

(u) = q(i)

t,x(u).

(60)

Let us now consider any arbitrary x ∈ Xt. By Assumption 28, (60), and our action-specs,

u(i)
t,x = arg max

u∈Ut

q(i)
t,x(u) = arg max

u∈Ut

q(i+1)
t,x

(u) = u(i+1)

t,x

.

Therefore,

(cid:16)

(cid:13)
(cid:13)q(i+1)
(cid:13)

t,x

(cid:17)

u(i+1)
t,x

− q(i)
t,x

(cid:16)

u(i)
t,x

(cid:17)(cid:13)
(cid:13)
(cid:13) =

(cid:13)
(cid:13)q(i+1)
(cid:13)

t,x

(cid:17)

(cid:16)

u(i)
t,x

− q(i)
t,x

(cid:16)

u(i)
t,x

(cid:17)(cid:13)
(cid:13)
(cid:13) = 0

(61)

showing that (58) holds for k = t.

By Theorem 29, we have Bπ(2) = Bπ(1). By one-to-one B, we have BPI’s termination
condition π(2) = π(1). We may then apply Theorem 27 to conclude termination to (global)
SPE-policy in just two iterations.

Next, we reveal the fact that full-sweep argmax is infeasible in practice under continu-
ous Uk and while discretization techniques may be applied, the argmax computation will
In such sit-
quickly become intractable as the discretization dimension of Uk increases.
uation, local search methods are often desirable to trade-oﬀ performance (i.e. allowing
termination/convergence to local 6 SPE-policy) for tractability.

Deﬁnition 30 (λ-Local SPE-Policy). Any policy π ∈ ΠM D is a local SPE-policy if it
satisﬁes

∀k ∈ T , x ∈ Xk, Qπ

k (x, πk(x)) ≥ Qπ

k (x, u), ∀u ∈ Nk(πk(x), λ),

where Nk(πk(x), λ) is the neighbourhood of πk(x) with the set radius λ > 0, i.e.
.
= {u ∈ Uk :

|u − πk(x)|< λ}

Nk(πk(x), λ)

.

Local-sweep argmax. To capture this localized search aim, we modify the full-sweep
argmax action-specs as

π(cid:48)
k(x) ← arg max

u∈N (πk(x),λ)

Qπ(cid:48)

t (x, u) (arbitrarily)

(62)

which needs to be accompanied by a modiﬁcation to BPI’s non-termination-condition to

∃u(cid:48) ∈ N (πk(x), λ) s.t. Qπ(cid:48)

k (x, u(cid:48)) > Qπ(cid:48)

k (x, πk(x))

(63)

6. The ‘locality’ here refers to the value-optimization landscape as a function of action variable u at a ﬁxed
time t and state x and thus, is irrelevant to the ‘locally optimal’ plan terminology of SPE policy, where
’locality’ refers to the sequential structure.

24

We note that (63) is necessary to characterize its termination policy. Suppose that we retain
(56) and the global optimum u(cid:48) ∈ Uk is not in the current neighborhood N (πk(x), λ), we may
encounter the situations where (i) we have non-unique solution to the argmax problem in
(62) and no termination happens due to inconsistent choices of solution happening in every
consecutive iterations such that if-condition is always satisﬁed, or (ii) we have termination
either when there is no non-uniqueness issue or by coincidental outputting of the same
solution in consecutive iterations in presence of non-uniqueness issue, which then gives
the wrong conclusion that else-condition has been satisﬁed and that the converged policy
is a global SPE policy. Both cases are not desirable to our analysis. Moreover, the non-
termination-condition (63) has an additional advantage of requiring the PolEva computation
only up to ∀u ∈ N (πk(x), λ) which can reduce the computational burden in each iteration.
Without (63), Corollaries 16 and 17 are countered by situation (ii) and (i), respectively. By
modifying non-termination-condition to (63), we can change the premise of Corollary 16 as

∃u ∈ N (πt(x), λ) s.t. Qπ(cid:48)

t (x, u) > Qπ(cid:48)

t (x, πt(x))

(64)

and recover both corollaries. The results up to Theorem 25 then apply as they rely solely
on these two corollaries. In what follows, we will show ﬁnite termination with value-based
analysis, which can be validated once Theorem 25 applies by retaining the injectivity of B.
However, we need a stronger assumption than Assumption 28.

Assumption 31. At each iteration i ∈ N, for any pair (k, x) ∈ T ×Xk, q(i)
and bounded over u ∈ Uk. Moreover, Uk is compact.

k,x(u) is continuous

Theorem 32 (Finite Termination). Suppose that Assumption 31 holds.

For any ﬁxed t ∈ T , if for each k ∈t+1 T ,

∃i∗

k < ∞ s.t. ∀i ≥ i∗

k, ∀x ∈ Xk,

(cid:13)
(cid:13)Q(i+1)
(cid:13)

k

(cid:16)

x, π(i+1)
k

(cid:17)

(x)

(cid:16)

− Q(i)
k

x, π(i)

k (x)

(cid:17)(cid:13)
(cid:13)
(cid:13) = 0,

then, for any ﬁxed x ∈ Xt,

∃i∗

t,x < ∞ s.t. ∀i ≥ i∗

t,x,

(cid:13)
(cid:13)Q(i+1)
(cid:13)

t

(x, π(i+1)
t

(x)) − Q(i)

t (x, π(i)

(cid:13)
(cid:13)
(cid:13) = 0.
t (x))

(65)

(66)

Moreover, if for each t ∈ T , i∗

t,x is bounded in x ∈ Xt

7, then the following holds

∃i∗ < ∞ s.t. ∀i ≥ i∗,

(cid:13)
(cid:13)Q(i+1)
(cid:13)

t

(cid:16)

x, π(i+1)
t

(x)

(cid:17)

(cid:16)

− Q(i)
t

x, π(i)

t (x)

(cid:17)(cid:13)
(cid:13)
(cid:13) = 0, ∀t ∈ T , x ∈ Xt. (67)

Proof. By Lemma 20, (65) means that at iteration i ≥ i∗
implies t+1π(i) = t+1π(∞) and correspondingly,

t+1, the lex-index k∗ ≤ t and it

∀x ∈ Xt, u ∈ Ut,

q(i+1)
t,x

(u) = q(i)

t,x(u) = q(∞)

t,x (u)

(68)

7. To iterate some instances when this assumption is met: (i) discrete Xt, (ii) as in Theorem 29, and (iii)

distance of initialization u(0)

t,x to the corresponding local optima u(∞)

t,x is bounded in x.

25

Suppose k∗ < t, we can set i∗
∀i ≥ i∗

t+1 and for any ﬁxed x ∈ Xt, we have
(cid:16)

(cid:17)

(cid:13)
(cid:13)q(i+1)
(cid:13)

t,x

(cid:16)
u(i+1)
t,x

− q(i)
t,x

u(i)
t,x

(cid:17)(cid:13)
(cid:13)
(cid:13) ≤

t,x = i∗

t+1 < ∞, ∀x ∈ Xt, thus showing (66). Otherwise (k∗ = t),

(cid:17)

(cid:17)

t,x

(cid:13)
(cid:13)q(i+1)
(cid:13)
(cid:13)
(cid:13)q(i+1)
(cid:13)
(cid:13)
(cid:16)
(cid:13)q(∞)
(cid:13)

t,x

t,x

(cid:16)
u(i+1)
t,x
(cid:16)
u(i+1)
t,x
(cid:17)

u(i+1)
t,x

=

=

− q(i+1)
t,x

(cid:16)
u(i)
t,x
(cid:16)

(cid:17)(cid:13)
(cid:13)
(cid:13) +
(cid:17)(cid:13)
(cid:13)
(cid:13)

− q(i+1)
t,x
(cid:16)
u(i)
t,x

− q(∞)
t,x

u(i)
t,x
(cid:17)(cid:13)
(cid:13)
(cid:13) ,

(cid:16)

(cid:13)
(cid:13)q(i+1)
(cid:13)

t,x

u(i)
t,x

(cid:17)

− q(i)
t,x

(cid:16)

u(i)
t,x

(cid:17)(cid:13)
(cid:13)
(cid:13)

where the second and third equations hold by (68). It thus remains to show that ∃i∗
t+1, ∞) s.t. ∀i ≥ i∗
[i∗

t,x,

t,x ∈

(cid:16)

(cid:13)
(cid:13)q(∞)
(cid:13)

t,x

(cid:17)

u(i+1)
t,x

− q(∞)
t,x

(cid:16)

u(i)
t,x

(cid:17)(cid:13)
(cid:13)
(cid:13) = 0.

(69)

First, we note that since k∗ = t, ∃x ∈ Xt where the sequence

increasing. By Assumption 31, such sequence is bounded above by sup
and thus, convergent. This then implies

(cid:110)

q(∞)
t,x
(cid:110)

(cid:111)

(cid:16)

(cid:17)

: i ≥ 0

u(i)
t,x
q(∞)
t,x (u) : u ∈ Ut

is
(cid:111)

(cid:16)

(cid:13)
(cid:13)q(∞)
(cid:13)

t,x

(cid:17)

u(i+1)
t,x

− q(∞)
t,x

(cid:16)

u(i)
t,x

(cid:17)(cid:13)
(cid:13)
(cid:13) = 0.

lim
i→∞

(70)

Next, we will show that the limit in (70) is attained at some ﬁnite iteration i∗
otherwise, the accumulation point

t,x. Suppose

sup

(cid:110)

q(∞)
t,x

(cid:17)

(cid:16)

u(i)
t,x

: i ≥ i∗

t+1

(cid:111)

= sup






q(∞)
t,x (u) : u ∈

(cid:91)

(cid:16)

(cid:17)
u(i)
t,x, λ

N

i≥i∗

t+1






is never attained. Since by Assumption 31, Ut,x is bounded,

(cid:91)

(cid:16)

(cid:17)
u(i)
t,x, λ

=

N

(cid:91)

(cid:16)

t,x − λ, u(i)
u(i)

(cid:17)
t,x + λ

i≥i∗

t+1

i≥i∗

t+1

must also be bounded. Moreover, by Assumption 31, q(∞)
extreme value theorem that (cid:83)

is open.

N

(cid:17)
u(i)
t,x, λ

(cid:16)

i≥i∗

t+1

t,x is continuous and implies by the

(cid:110)

(cid:111)

t+1)

. Therefore, ∃i∗ ≥ [i∗

We ﬁrst consider the case when the sequence

u(i)
t,x : i ≥ i∗
(cid:13)
(cid:13)
(i∗
T −1,x − u(i∗)
(cid:13)u(i∗+1)
(cid:13)
(cid:13)
to u
(cid:13) and
t,x
u(i∗+2)
T −1,x ∈ N (cid:0)u(i∗), λ(cid:1). This leads to a contradiction as in iteration i∗ + 1, u(i∗+2) should
have been chosen then instead of u(i∗+1).
(cid:110)

is non-monotonic with respect
T −1,x − u(i∗)

(cid:13)
(cid:13)u(i∗+2)
(cid:13)

t+1, ∞) s.t.

(cid:13)
(cid:13)
(cid:13) >

T −1,x

T −1,x

t+1

u(i)
t,x : i ≥ i∗

t+1

(cid:111)
. Then, limi→∞ u(i)

t,x exists and

Consider next bounded and monotonic

∀(cid:15) > 0, ∃i∗ ∈ [i∗

t+1, ∞) s.t. ∀i ≥ i∗,

26

(cid:13)
(cid:13)u(i+1)
(cid:13)

t,x − u(i)

t,x

(cid:13)
(cid:13)
(cid:13) < (cid:15).

Set (cid:15) = λ

2 . Thus, we must have
(cid:13)
(cid:13)
(cid:13)u(i∗+2)
(cid:13)
(cid:13)
(cid:13) ≤
(cid:17)
u(i∗)
t,x , λ

− u(i∗)
t,x

∈ N

t,x

t,x

(cid:16)

t,x

(cid:13)
(cid:13)u(i∗+2)
(cid:13)
. If u(i∗+2)
t,x

− u(i∗+1)
t,x

(cid:13)
(cid:13)
(cid:13) +

(cid:13)
(cid:13)u(i∗+1)
(cid:13)

t,x

− u(i∗)
t,x

(cid:13)
(cid:13)
(cid:13) < λ,

such that u(i∗+2)
the non-monotonic case to conclude contradiction. Otherwise, we must have limi→∞ u(i)
t,x =
u(i∗+1)
being open. Therefore,
t,x
our supposition must be false: the 0-limit in (70) must be attained at some ﬁnite iteration
t,x ≥ i∗
i∗

t+1 which concludes that (66) holds.

, we may apply similar argument as in

which contradicts (cid:83)

(cid:54)= u(i∗+1)
t,x

(cid:17)
u(i)
t,x, λ

(cid:17)
u(i)
t,x, λ

∈ (cid:83)

i≥i∗

i≥i∗

N

N

t+1

t+1

(cid:16)

(cid:16)

Showing (67) is equivalent to showing that (65) holds for all k ∈ T . We prove the latter by
induction.

(cid:16)

(cid:17)

T −1,x

u(i+1)
T −1,x

(Base case.) At k = T − 1, (68) holds for all i ≥ 0 such that for any ﬁxed x ∈ XT −1,
(cid:17)(cid:13)
(cid:17)(cid:13)
(cid:13)
(cid:13)
(cid:13) .
(cid:13) =
T = 0, we can apply the proof for (66) (i.e. k = t) to show that i∗

(cid:13)
(cid:13)q(i+1)
(cid:13)
By setting i∗
any ﬁxed x ∈ XT −1. Since we have i∗
T −1 = sup{i∗
i∗

T −1,x < ∞ for
T −1,x bounded in x ∈ XT −1 by assumption, we can set

T −1,x : x ∈ XT −1} < ∞ to conclude that (65) holds for k = T − 1.

(cid:13)
(cid:13)q(∞)
(cid:13)

u(i)
T −1,x

u(i)
T −1,x

u(i+1)
T −1,x

− q(∞)

− q(i)

T −1,x

T −1,x

T −1,x

(cid:16)

(cid:16)

(cid:17)

(cid:16)

(Inductive step.) Suppose (65) holds for k ∈t+1 T , this is exactly the assumption for (66),
to which we have shown the existence of i∗
t,x < ∞ for any ﬁxed x ∈ Xt. Similarly by the
assumption of i∗
t,x : x ∈ Xt} < ∞ thus showing
that (65) holds for k = t.

t,x bounded in x ∈ Xt, we can set i∗

t = sup{i∗

Finally, by noting that i∗

0 ≥ i∗

1 ≥ . . . ≥ i∗

T −1, we can set i∗ = i∗

0 to show (67).

We will next re-derive a result similar to Theorem 27, whose conclusion becomes unprovable
once (54) is no longer the premise of Corollary 16 (i.e. modiﬁed to (64)). To rectify this
issue, we will modify its conclusion to reﬂect the λ-local SPE policy; see Deﬁnition 30.

Theorem 33 (Converged policy is λ-local SPE-policy). If π(cid:48) = π, then

∀t ∈ T , ∀x ∈ Xt, Qπ

k (x, πk(x)) ≥ Qπ

k (x, u), ∀u ∈ N (πk(x), λ).

(71)

Proof. Suppose otherwise,

∃k, x s.t. ∃u ∈ N (πk(x), λ), Qπ

k (x, u) > Qπ

k (x, πk(x)).

Focus on one such k. By assumption that π(cid:48) = π, we have kπ(cid:48) = kπ which then implies

∃x ∈ Xk s.t. ∃u ∈ N (πk(x), λ), Qπ(cid:48)

k (x, u) > Qπ(cid:48)

k (x, πk(x)).

By modiﬁed Corollary 16 with (64), this implies that π(cid:48)
π(cid:48) = π. Thus, supposition is false and (71) must hold.

k(x) (cid:54)= πk(x) which contradicts

27

3.3 Chapter Summary

Through this section, we have introduced the BPI as a new class of policy iteration algo-
rithms to learn SPE policy under ﬁnite-horizon TIC objective speciﬁed in Section 2. We
obtained monotonicity results for general state-action case that circumvent the use of PIT.
In Section 3.2.2, we dealt with discrete state-action and proved the correctness of BPI in
converging to a (global) SPE policy by policy-based analysis that is, by permuting over all
possible policies in the discrete search space ΠM D. In Section 3.2.3, we generalize this result
to continuous state-action, where we turn to value-based analysis as permutative argument
no longer applies. This necessitates further speciﬁcation of BPI’s action-specs, which we
exempliﬁed through through cases: (i) full-sweep argmax, and (ii) local-sweep argmax. In
each case, we proved convergence to (global/local) SPE policy.

Next, we highlight several deﬁning rules of BPI that have played a central role in our
analyses in general and across diﬀerent action-specs.

π(cid:48)-based PolEva-specs. This rule captures the game-theoretic nature of BPI and mainly
distinguishes BPI from standard RL, in which π-based PolEva-specs is used. In particular,
it contributes in establishing lex-monotonicity as a suﬃcient condition through Proposition
Intuitively, lex-monotonicity guarantees that if at indexes in k∗+1T , the distance to
15.
the (global/local) SPE-policy contracts, then at the lex-index k∗ it must also contract (by
strictly-improving action-specs). Note that by deﬁnition, the lex-index k∗ is determined
with π(cid:48). For instance, in full-sweep argmax, the lex-index k∗ for the old-new policy pair
(π(0), π(1)) is 0 and not T − 1. This guarantee is related to the speed of convergence, which
in our case consists of two components: (i) how fast is the lex-index k∗ moving to 0, and
(ii) how long does it take to ﬁnish updating for one particular k∗. The faster the update at
k∗, the more advantageous is this rule over π-based PolEva. For instance, in the extreme
case when full-sweep argmax is used, BPI converges in just two iterations while π-based
PolEva can only be guaranteed to converge in T iterations. Referring to Theorem 32, BPI
attains 0-limit at some ﬁnite iteration i∗
0 = i∗
In contrast, π-based
PolEva will only reﬂect a current iteration’s changes in future policies in the next iteration,
i.e. i∗

1 = . . . = i∗

T −1 = 1.

t+1 + 1, ∀t < T − 1.

t = i∗

Strictly-improving action-specs. This rule imposes strict SPE-improving update in
each iteration, preventing stagnancy unless it is SPE-optimal as described by Corollary 16.
This rule is especially important in characterizing non-termination as strict lex-monotonicity
i.e. B(cid:48) >lex B. This allows the use of B(cid:48) = B to characterize termination which serves
as the basis of value-based analysis in Section 3.2.2. Across diﬀerent action-specs, this
rule is tightly connected to the non-termination-condition. To illustrate, we may revisit
the local-sweep argmax case, where the strictly-improving rule alone is insuﬃcient to es-
tablish strict lex-monotonicity unless accompanied with a corresponding modiﬁcation of
non-termination-condition.

28

t

Consistent tie-break. This rule prevents each player t’s oscillation of policies when the
values Qt+1π(cid:48)
(x, πt(x)) are equal and contributes to lex-monotonicity as suﬃcient conditions
in the form of Corollary 17 and Lemma 18. The need of such rule in SPERL is motivated
by the dependence of each players’ SPE-optimality on the choice of other players. Consider
the case when at some iteration i ∈ N, SPE policy π(i) has been found such that by BPI,
it remains to go through one more iteration to reach termination, i.e. π(i+1) = π(i). Now,
suppose that there is a player t + 1 which from its perspective, the action u(cid:48) and u have
the same values. Without consistent tie-break rule, t + 1 may shift his action choice, i.e.
t+1(x) = u and π(i+1)
π(i)
t+1 (x) = u(cid:48), which by the adjustment terms in (16),

t+1π(i+1) ∼eq t+1π(i) (cid:54)⇒ Qt+1π(i+1)

t

(x, u) = Qt+1π(i)

t

(x, u),

∀x ∈ Xt, u ∈ Ut.

(72)

Once we have non-equality (i.e. the conclusion of (72)), such shift will break the SPE-
optimality of π(i) and will cause earlier players in tT to re-adjust to a diﬀerent SPE-
policy. This process can then repeat itself causing the algorithm to never terminate. More-
over, by noting that re-adjustment may not happen at once, e.g., local-sweep argmax,
force-terminating the algorithm may lead to a non-SPE-optimal policy. In contrast, stan-
dard RL approaches do not usually impose such rule since the dependence of t-agent’s
evaluation Qπ
t (x, u) to the players in t+1T in standard RL problems is fully encoded by
Qπ
t+1(·, u) without adjustment terms. Thus, as long as the action
choices are (locally/globally) argmax, Qπ
t (x, u) is invariant to the choice of πt+1 and (72)
will never happen. Finally, we note that in SPERL, if we can ensure a unique solution
to any argmax operation, (72) can also be prevented; for instance, in local-sweep argmax,
when λ is suﬃciently small or in full-sweep argmax, or when we have unique global SPE
policy.

t+1(·, πt+1(·)) = maxu Qπ

Remark 34 (Performance of the converged SPE policy). As illustrated in the paragraph
above, each action choice u(cid:48) matters to which SPE (if non-unique) a search algorithm will
converge to. And while BPI’s consistent tie-break rule is supported by game-theoretic argu-
ments, in reality, diﬀerent choices of u(cid:48) may aﬀect the actual control performance.

One drawback of the algorithms covered in this section is the assumed full-sweep over the
state-spaces {Xt : t ∈ T } that is unrealistic in practice. In the next section, we will propose
several SPERL training algorithms that relax such an assumption.

4. Training Algorithms

In this section, we will focus on relaxing the full-sweep assumptions on the state-spaces {Xt :
t ∈ T } by incorporating standard RL simulation methods into BPI. We consider three types
of methods, namely (i) tabular Q-learning, (ii) Q-learning with function approximators,
and (iii) gradient-based methods. For each method, we will ﬁrst set up new prediction
objectives that build on BPI’s PolEva step with particular attention drawn to the training
of adjustment functions. Then, we specify how to adapt BPI’s key rules, speciﬁcally π(cid:48)-
based PolEva-specs and consistent tie-break, while noting that strictly-improving action-
specs automatically applies by the default setup.

29

4.1 Tabular Q-learning

Here, we derive a SPERL version of the standard ﬁnite-horizon Q-learning presented in
Harada (1997). Consider a SPERL agent that consists of T child agents and deﬁne tabular
representations ˆQt, ˆft, ˆgt, ˆrt for each agent t i.e.

(x, u),

t

ˆQt(x, u) ≈ Qt+1π(cid:48)
ˆrt(x, u, τ, m, y) ≈ rt+1π(cid:48),τ,m,y
ˆft(x, u, τ, y) ≈ f t+1π(cid:48),τ,y
ˆgt(x, u) ≈ gt+1π(cid:48)

t

t

t

(x, u).

(x, u),

(x, u),

(73)

(74)

(75)

(76)

The superscript π(cid:48) denotes the SPERL agent’s policy obtained after applying BPI with π.
We can then apply the TIC-TD PolEva derived in Section 3.1 and obtain a bootstrapped
version of the DP targets deﬁned in (18)-(21) as follows

ξr
t (x, u, τ, m, y)

.
=






RT −1,T −1(y, XT , u),
Rt,t(y, x, u),
ˆrt+1(Xt+1, π(cid:48)

t+1(Xt+1), τ, m, y),

if m = τ = t = T − 1,
if m = τ = t, ∀t < T − 1,
if m (cid:54)= t, ∀t < T − 1,

(cid:40)

(cid:40)

ξf
t (x, u, τ, y)

.
=

ξg
t (x, u)

.
=

Fτ (y, XT ),
ˆft+1(Xt+1, π(cid:48)

t+1(Xt+1), τ, y),

if t = T − 1,
otherwise,

XT ,
ˆgt+1(Xt+1, π(cid:48)

t+1(Xt+1)),

if t = T − 1,
otherwise,

(77)

(78)

(79)

(cid:40)

ˆrt(x, u, t, t, x) + ˆft(x, u, t, x) + Gt(x, ˆgt(x, u)),
ˆrt(x, u, t, t, x) + ˆQt+1(Xt+1, π(cid:48)

t+1(Xt+1)) − (∆ˆrt + ∆ ˆft + ∆ˆgt),

if t = T − 1,
otherwise,

(80)

ξQ
t (x, u)

.
=

where

.
=

∆ˆrt

T −1
(cid:88)

(cid:0)ˆrt+1(Xt+1, π(cid:48)

t+1(Xt+1), t + 1, m, Xt+1) − ˆrt(x, u, t, m, x)(cid:1) ,

m=t+1

∆ ˆft
∆ˆgt

.
= ˆft+1(Xt+1, π(cid:48)
.
= Gt+1(Xt+1, ˆgt+1(Xt+1, π(cid:48)

t+1(Xt+1), t + 1, Xt+1) − ˆft(x, u, t, x),

t+1(Xt+1))) − Gt(x, ˆgt(x, u)).

Finally, we follow a generalized policy iteration to perform BPI update i.e. ∀t, x,

π(cid:48)
t(x) ← arg max

u∈Ut

ˆQt(x, u), (with consistent tie-break )

30

(81)

(82)

(83)

(84)

where ˆQt(x, u) is used in place of the unknown Qπ(cid:48)
t (x, u). Supposing the use of on-policy
training, we note some similarities between (84) and the local-sweep argmax (62) in that
for any ﬁxed t, x, the values of ˆQt(x, u) can only be accurate on the actions visited in the
set of simulated trajectories which are analogous to N (πt(x), λ). We further note that the
consistent tie-break rule is imposed explicitly in (84). We summarize the discussion into
SPERL Q-learning algorithm above; see Algorithm 4 in Appendix A.

Remark 35 (Sampling for τ, m, y.). To make our approach more scalable, in Algorithm
4 in Appendix A, we identify which τ, m, y are relevant to the prediction of ˆQt(x, u) for a
ﬁxed t, x, u. For instance, consider the parameters τ, y in ˆft(x, u; τ, y). Referring to (80), we
want our estimated ˆft(x, u; τ, y) to be accurate at τ = t, y = x. By TIC-adjusted TD-based
PolEva-specs for ˆft prediction, we then need accurate estimates of ˆfk(Xk, Uk; τ = t, y = x)
for k ≥ t + 1. By inverting this observation ﬁxing the k instead, we can then derive the
importance region τ ≤ k − 1 and correspondingly y ∈ ∪τ ≤k−1Xτ .

4.2 Q-learning with Function Approximation

This subsection focuses on addressing the drawback of tabular representations that are
usually limited to small, discrete state-action spaces by adapting the use of function ap-
proximators. Here, we adapt the steps used by Sutton and Barto (2018) in extending the
standard (inﬁnite-horizon) Q-learning (see Watkins and Dayan (1992)) to handle inﬁnite-
dimensional state-action spaces. Consider w-parameterized approximators Qw
t , gw
t
and set each agent t’s prediction objective analogous to Bellman-error minimization i.e.
minimizing J(ϕw
t )
t (·) − ˆϕt(·; w(t; ϕ))(cid:107)Dϕ
for ϕ ∈ {Q, f, r, g}, over the parameter
space Wϕ ⊂ Rdϕ where w(t; ϕ) takes values on. The weighted-norm (cid:107) ·(cid:107)Dϕ
is deﬁned on
.
the input space of each ϕ such that DQ
= Xt × Ut × tT × Yt, and
t
.
Dr
= Xt × Ut × tT × Mt × Yt where with a little abuse of notation, Xt, Ut, tT , Mt, Yt now
t
represents arbitrary density functions deﬁned on the full spaces X , U, T , T , X as a measure
of approximation quality.

.
= Xt × Ut, Df
t

t = Dg

.
= (cid:107)ϕπ(cid:48)

t , f w

t , rw

t

t

.
= ρ({0, . . . , t}), and Mt

Intuitively, due to dim(w(t; ϕ)) being much smaller than the dimension of the ϕ’s full
input space, we want our approximation to be accurate at some important regions at the
sacriﬁce of irrelevant regions. Since J(Qw
t ) is an analog of what we have in standard RL,
we focus instead on the adjustment functions’ prediction objective, speciﬁcally in dealing
.
with the τ, m, y in ˆf and ˆr. As in Remark 35, we can ﬁrst attempt to set Yt
= ρ(∪0≤τ ≤tXτ ),
.
tT
= ρ({t, . . . , T −1}) for some density function ρ(·) that measures
the relative importance of any points/regions in the full input space in approximating ˆϕt
or solving w∗(t; ϕ) for ϕ ∈ {f, r}. However, such aggregation may seem unnatural in some
cases; for instance, setting ρ(·) for tT and Mt to be uniform is a natural choice under such
aggregation but that is essentially saying that each element τ or m contributes uniformly
to w∗(t; f ) or w∗(t; r).

To address this issue, we introduce weight tables w(t, τ ; f ) and w(t, τ, m; r) and modify our
approximators such that ˆft(x, u, y; w(t, τ ; f )) ≈ f π(cid:48)
t (x, u, τ, y) and ˆrt(x, u, y; w(t, τ, m; r)) ≈

31

rπ(cid:48)
t (x, u, τ, m, y). According to this setup, our TD-based prediction objectives are as follows:

J(f w
t )
J(rw
t )
J(gw
t )
J(Qw
t )

.
= (cid:107)E[ξf
.
= (cid:107)E[ξr
.
= (cid:107)E[ξg
.
= (cid:107)E[ξQ

t (·, ·, τ, ·; π(cid:48))] − ˆf (·; w(t, τ ; f ))(cid:107)Df
t (·, ·, τ, m, ·; π(cid:48))] − ˆr(·; w(t, τ, m; r))(cid:107)Dr
t (·, ·; π(cid:48))] − ˆg(·; w(t; g))(cid:107)Dg
t (·, ·; π(cid:48))] − ˆQ(·; w(t; Q))(cid:107)DQ

t,τ

t,τ

t,τ

t,τ

where DQ

t,τ = Dg

t,τ

.
= Xt × Ut and Df

t,τ = Dr
t,τ

.
= Xt × Ut × Xτ .

The prediction objectives abovecan then be solved using any least-squares solver. Once we
have our approximate action-value function Qw, we can then apply BPI’s PolImp rules. In
the case of discrete action spaces, these rules can be speciﬁed similarly as in (84). With con-
tinuous action spaces, our choice of approximator needs to be restricted to ensure feasible
computation of local-optima. This is possible, e.g., when model-based approximators are
available or when domain knowledge allows the identiﬁcation of twice-diﬀerentiable linear
features that are amenable to direct argmax solving. These restrictions are however unde-
sirable as they restrict the addressable class of problems. Therefore, in the next subsection,
we present gradient-based methods that are applicable to broader problem settings.

4.3 Gradient-based Methods: Deterministic Policy Gradient and Actor-Critic

Gradient-based methods are common tools in standard RL to deal with continuous action
spaces, which aim to train a parameterized policy separate from the action-value estimates.
Here, we will derive a SPERL version of deterministic8 policy gradient along the line of
Silver et al. (2014). We consider a ﬁnite-horizon θ-parameterized policy πθ where we
assume separate policy representation ˆπt(x; θ(t)) for each agent t. Such a separation is
consistent with π(cid:48)-based PolEva-specs in BPI, where each agent t is only allowed to vary
its policy πt while assuming ﬁxed future players’ policies at t+1π(cid:48). For each agent t, we
incorporate BPI’s PolImp by applying simple chain rules to ∇θ(t)Qt+1π(cid:48)
(x, πt(x; θ(t))) and
obtain the corresponding deterministic gradient-ascent rule, i.e.

t

θl+1(t) = θl(t) + α∇θ(t)Qπl+1

(xt, ˆπt(xt; θl(t)))
= θl(t) + α∇θ ˆπt(xt; θ)|θ=θl(t)∇uQπl+1

t

t

(xt, u)|u=ˆπt(xt;θl(t)).

(85)

We note the similarities of (85) to local-sweep argmax rule (62), except for here, optimization
is done over Θt instead of Ut and λ ↓ 0. We can also observe that while consistent tie-break
rule does not explicitly appear anywhere in (85), it is implicitly imposed by letting λ ↓ 0.
For the gradient-ascent rule (85) to be implementable in practice, the true action-value
.
gradient ∇uQt+1π(cid:48)
= {ˆπt(·; θ(cid:48)(t)) : ∀t ≥ t + 1}, must
be approximated. We follow Silver et al. (2014) to instead approximate Qt+1π(cid:48)
(x, u) to

(x, u), given continuation policy t+1π(cid:48)

t

t

8. The choice to present deterministic instead of a stochastic version is made to avoid much deviation from

the control-theoretic deﬁnition of SPE policy in Section 2.

32

which the results from Section 4.2 can be applied and SPERL Deterministic Actor-Critic
algorithm can be obtained; see Algorithms 5–9 in Appendix A, where we also discuss about
the choice of critic approximator, the critic parameter update, and the use of replay buﬀer.

4.4 Chapter Summary

In this section, we have adapted standard RL simulation methods into BPI, addressing the
main drawback of the version presented in Section 3. Two SPERL training algorithms were
derived for two diﬀerent model assumptions, discrete and continuous state-action spaces.
The adaptation of TIC-adjusted TD-based methods to evaluate policy and some training
procedures were discussed. We emphasize that the key to adapting BPI’s rules is to realize
the π(cid:48)-based PolEva-specs. In all three subsections, we have demonstrated that once we
have a prediction framework, such rule can be integrated seamlessly into all methods we
consider by simply imposing a backward policy update direction; see for instance, Algorithm
4 in Appendix A. While a thorough investigation on the training algorithms is not the focus
of this paper, we exemplify our insights into the training under the SPERL framework with
a ﬁnancial example in the next section.

5. An Illustrative Example: Dynamic Mean-Variance Portfolio Selection

This section focuses on illustrating an end-to-end derivation of training algorithm under the
SPERL framework with an application of dynamic mean-variance (MV) portfolio selection.

5.1 Problem Formulation

We consider a portfolio management problem with a ﬁxed investment horizon Tinv < ∞
.
that can be discretized into T > 0 decision periods in T
= {0, 1, . . . , T − 1}. We denote
by ∆t the timestep or the length of each period such that Tinv = T ∆t. For simplicity,
we assume a market environment consisting of one risky and one riskless asset. Given a
standard one-dimensional Brownian motion {Wt : 0 ≤ t ≤ T }, our risky asset price follows

St+∆t − St = St(µ∆t + σ

(cid:112)

∆Wt),

∀t ∈ T

(86)

with S0 = s0 > 0, µ ∈ R, and σ > 0 denoting the initial price at t = 0, annualized
mean return, and annualized stock volatility, respectively. The riskless asset has a constant
annualized interest rate rann. > 0.

t ∈ R deﬁnes her wealth at time t and agent’s action ut ∈ R signiﬁes
An agent’s state X u
how much wealth she puts into the risky asset with the remaining wealth X u
t − ut being
invested into riskless asset. Given the above market environment model, the wealth process
can then be described by the following stationary linear dynamics
t+1 = (1 + r)X u

t + ut(Yt+1 − r),

∀t ∈ T

(87)

X u

33

with normalized wealth at time 0, i.e. X0 = 1, period rate r = rann.∆t, and {Yt}t∈T a
sequence of i.i.d random variables with the following attributes

E[Yt] = µ∆t, Var(Yt) = σ2∆t,

∀t ∈ T .

(88)

The agent’s objective is to select a dynamic portfolio π = {π0, π1, . . . , πT −1} = {u0, u1, . . . , uT −1}
that strikes the best balance between the expected value (reward) of the terminal wealth
E[XT ] and the variance (risk) of the terminal wealth Var(XT ). Hence, the performance
criterion at time t ∈ T takes the form

V π
t (x)

.
= Et,x[X π

T ] −

γ
2

Vart,x(X π

T ) = Et,x

(cid:104)

X π

T −

γ
2

(X π

T )2(cid:105)

+

γ
2

(Et,x[X π

T ])2 ,

(89)

which by the general form in (2), we have

Gτ (y, x) =

γ
2

x2, Fτ (y, x) = x −

γ
2

x2

(90)

implying the existence of G-type of TIC. Since we have continuous state-action spaces, we
will apply the SPERL Deterministic Actor-Critic algorithm proposed in Section 4.3 to train
an SPE policy that solves (89). We remark here that in this example, our only unknowns
are the transition model parameters in (88), which also deﬁnes our risky asset model (86).

5.2 Model-based Function Approximators

In the next subsection, we describe both policy and critic approximators that we use to
train our algorithm.

5.2.1 Critic Approximators

To address the estimation of Qπ(cid:48)
t (x, u) in the gradient-ascent rule (85), we derive model-
based linear representations for both ˆQt(x, u) and ˆgt(x, u). Referring to the boundary
conditions at t = T − 1,

Qπ

T −1(x, u)

gπ
T −1(x, u)

.
= ET −1,x[X u

γ
2
= ET −1,x [(1 + r)x + (YT − r)u] −

VarT −1,x[X u
T ]

T ] −

γ
2
Var[YT ]u2

γ
2
T ] = ET −1,x [(1 + r)x + (YT − r)u]

= (1 + r)x + (E[YT ] − r)u −
.
= ET −1,x[X u
= (1 + r)x + (E[YT ] − r)u

VarT −1,x [(1 + r)x + (YT − r)u]

(by (87))

(91)

(by (87))

(92)

and noting the linear-quadratic setting of this example, for any arbitrary policy π, Qπ and
gπ will have the following form

t (x, u) = Atu2 + Btu + Ctx + Dt
Qπ
gπ
t (x, u) = atu + btx + ct

(93)

(94)

34

We can thus set our critic approximators according to (93)-(94) i.e.

ˆQt(x, u; w(t))
ˆgt(x, u; w(t))

.
= w3(t; Q)u2 + w2(t; Q)u + w1(t; Q)x + w0(t; Q)
.
= w2(t; g)u + w1(t; g)x + w0(t; g)

(95)

(96)

5.2.2 Policy Approximators

The obtained forms in (93)-(94) can further give us clues to set up model-based policy
approximators; in particular, we observe that the action-value gradient ∇uQπ
t (x, u) is in-
dependent of the state x, i.e.

∇uQπ(cid:48)

t (xt, u) = ∇uQπ(cid:48)

t (˜xt, u), ∀xt, ˜xt ∈ Xt, xt (cid:54)= ˜xt,

(97)

which means that no matter what state agent t is in, any state xt will give the same signal
about what direction of improvement (towards the optimal action) to take. Such indiﬀerence
to xt can then be exploited to set state-invariant policy approximators, i.e. ∀t ∈ T ,
.
= θ(t),

(98)

∀x ∈ Xt

ˆπt(x)

5.3 Training Procedures

In this subsection, we specify in detail how we train the approximators above as outlined
in Algorithm 3 below.

5.3.1 Trajectory Generation

We refer to lines 3–11 in Algorithm 3 as experience collection step. Experiences here are
collected at every iteration l in the form of wealth trajectories of length T with initial state
X0 normalized to 1. At each time t and given the corresponding wealth Xt, we sample the
next state Xt+1 from a MarketEnv simulator under a uniform exploratory policy π(l)
t,λ-unif
that samples action Ut ∼ Unif(πt(Xt; θ(l)(t)) − λ, πt(Xt; θ(l)(t)) + λ). We note that such
exploration schedule is possible since our training is oﬄine (i.e. the environment that we
interact with is a simulator and not the real market). All B generated trajectories are then
stored into the replay buﬀer D in a tupled form as speciﬁed in Section 4.3.

5.3.2 Critic Training

At t = T − 1, we follow the steps in Algorithm 5 in Appendix A to solve the prediction
problems

ˆQT −1(x, u; w(T − 1)) ≈ Qπ(cid:48)
ˆgT −1(x, u; w(T − 1)) ≈ gπ(cid:48)

T −1(x, u)

T −1(x, u)

.
= ET −1,x[X u
.
= ET −1,x[X u
T ].

T ] −

γ
2

VarT −1,x[X u
T ],

35

Algorithm 3: SPERL Dynamic MV Portfolio Selection

Input : MarketEnv(µ, σ, r, x0, ∆t, T, γ), Hyperparameters(L, B, λ, κ, αw, αθ, . . .)
Output: Approximate SPE-policy πθ

1 Initialize critic parameters w, actor parameters θ, replay memory D ← ∅;
2 for l ← 0 to L do
3

for b ← 1 to B do
Set X0 ← 1;
Generate trajectory X0, U0, X1, U1, . . . , XT −1, UT −1, XT ∼ π(l)
for t ← 0 to T − 1 do
for τ ← t to 0 do

λ-unif;

D ← D ∪ {(t, τ, Xt, Ut, Xτ , Xt+1)}

end

end

end
for t ← T − 1 to 0 do

if t = T − 1 then
Initialize Ξg
Sample mini-batch ˜D·,· ∼ Replay(·, ·, D, κ) ;
for (t, τ, x, u, y, X x,u) ∈ ˜D·,· do

·,· ← ∅;

·,·, ΞQ

Transform (x, u, X x,u) to (1, u, X 1,u) ;
ξg
t ← X 1,u;
Set Ξg

·,· ∪ (·, 1, u, ξg

·,· ← Ξg

t );

(cid:80)

end
Solve w∗ ← arg minw
Update w2(t; g) ← w2(t; g) + αw (w∗
Update w1(t; g) ← (1 + r) ;
for (t, τ, x, u, y, X x,u) ∈ ˜D·,· do

(ξg

Ξg
·,·

t − ˆgt(x, u; w))2 (with ALS);

2 − w2(t; g)) (with EMA) ;

Transform (x, u, X x,u) to (1, u, X 1,u) ;
ξQ
2 (X 1,u)2 + γ
t ← X 1,u − γ
2 ˆg2
·,· ∪ (·, 1, u, ξQ
·,· ← ΞQ
Set ΞQ
t );

t (1, u; w(t; g));

end
Solve w∗ ← arg minw
Update w2,3(t; Q) ← w2,3(t; Q) + αw
Update w1(t; Q) ← (1 + r);

ΞQ
·,·

(cid:80)

(cid:16)

(cid:17)2

ξQ
t − ˆQt(x, u; w)
(cid:0)w∗

(with ALS);
2,3 − w2,3(t; Q)(cid:1) (with EMA);

else

Update w1,2,3(t; g), w1,2,3(t; Q) following (105)-(109) ;

end
θ(t) ← θ(t) + αθ∇u ˆQt(1, u; w(t; Q))|u=θ(t);

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

end

36
37 end

36

Note that since we only have two unknown model parameters by (88), some critic parameters
can be ﬁxed by (91)-(92) at

w0(T − 1; Q) = w0(T − 1; g) = 0,
w1(T − 1; Q) = w1(T − 1; g) = (1 + r)

to let the critic training focus on the remaining unknown parameters,

w2(T − 1; g) ≈ (E[YT ] − r) = µ∆t − r,
w2(T − 1; Q) ≈ (E[YT ] − r) = µ∆t − r,
γ
σ2∆t.
2

w3(T − 1; Q) ≈ −

Var[YT ] = −

γ
2

(99)

(100)

(101)

(102)

Parametric Recursions Moreover, by noting that the approximation scheme for (100)-
(102) has learnt all our unknowns, we can exploit the same model knowledge and assump-
tions to perform parametric recursions. This technique reduces the problem of estimating
{Qπ(cid:48)
T −1 by converting the remaining T − 1 estimations to simple
t
computation problems.

: t ∈ T } to only Qπ(cid:48)

Let us ﬁrst recover some statistics about our state dynamics from the estimated critics in
(100)-(102) that is to be used in the subsequent parametric recursion derivation,

VarT −1,x[X u

T ] ≈

2
γ

(ˆgT −1(x, u; w(T − 1)) − ˆQT −1(x, u; w(T − 1))),

ET −1,x[X u

T ] ≈ ˆgT −1(x, u; w(T − 1)).

(103)

(104)

Next, we rewrite the Q-recursion from Proposition 11, after keeping only the g-term by MV
TIC-source speciﬁcations with Qπ(cid:48) replaced by their approximators ˆQ as follows

(cid:104) ˆQt+1(X u
(cid:2)w(cid:48)

ˆQt(x, u; w(cid:48)(t)) = Et,x

t+1, θ(cid:48)(t + 1); w(cid:48)(t + 1))

(cid:105)

−

γ
2

Vart,x

(cid:2)ˆgt+1(X u

−

−

(w(cid:48)

t+1]

(cid:2)w(cid:48)

= w(cid:48)

= w(cid:48)

Vart,x

1(t + 1; g))2Vart,x [X u

2(t + 1; g)θ(cid:48)(t + 1) + w(cid:48)

2(t + 1; Q)θ(t + 1) + w(cid:48)
t+1 + w(cid:48)
1(t + 1; g)X u

2(t + 1; Q)θ(cid:48)(t + 1) + w(cid:48)

3(t + 1; Q)θ(cid:48)(t + 1)2 + w(cid:48)

= Et,x
γ
2
3(t + 1; Q)θ(cid:48)(t + 1)2 + w(cid:48)
γ
2
3(t + 1; Q)θ(cid:48)(t + 1)2 + w(cid:48)
γ
−
2
= (cid:0)w(cid:48)
1(t + 1; Q) − w(cid:48)
3(t + 1; Q)θ(cid:48)(t + 1)2 + w(cid:48)
+ w(cid:48)
Applying similar steps as the above, we obtain the following
ˆgt(x, u; w(cid:48)(t)) = Et,x
= Et,x
= w(cid:48)
= w(cid:48)

1(t + 1; g)X u
1(t + 1; g)Et,x [X u
1(t + 1; g)ET −1,x [X u

2(t + 1; g)θ(cid:48)(t + 1) + w(cid:48)
2(t + 1; g)θ(cid:48)(t + 1) + w(cid:48)

t+1 + w(cid:48)
t+1] + w(cid:48)
T ] + w(cid:48)

t+1, θ(cid:48)(t + 1); w(cid:48)(t + 1))(cid:3)

1(t + 1; g)2VarT −1,x [X u
T ]

2(t + 1; Q)θ(cid:48)(t + 1) + w(cid:48)

2(t + 1; Q)θ(cid:48)(t + 1) + w(cid:48)

1(t + 1; g)2(cid:1) ˆgT −1(x, u; w(cid:48)(T − 1)) + w(cid:48)

2(t + 1; g)θ(cid:48)(t + 1) + w(cid:48)

(cid:2)ˆgt+1(X u
(cid:2)w(cid:48)

w(cid:48)

t+1, θ(cid:48)(t + 1); w(cid:48)(t + 1))(cid:3)
t+1 + w(cid:48)

0(t + 1; Q)(cid:3)

1(t + 1; Q)X u
0(t + 1; g)(cid:3)

(by (93)-(94))

1(t + 1; Q)Et,x [X u

t+1] + w(cid:48)

0(t + 1; Q)

(by deterministic coeﬃcients)

1(t + 1; Q)ET −1,x [X u

T ] + w(cid:48)

0(t + 1; Q)

(by stationary transitions (87))

1(t + 1; g)2 ˆQT −1(x, u; w(cid:48)(T − 1))
(by (103)-(104))

0(t + 1; Q).

0(t + 1; g)(cid:3)
(by (94))
0(t + 1; g) (by deterministic coeﬃcients)

0(t + 1; g)

= w(cid:48)

2(t + 1; g)θ(cid:48)(t + 1) + w(cid:48)

0(t + 1; g) + w(cid:48)

1(t + 1; g)ˆgT −1(x, u; w(cid:48)(T − 1)).

(by (104))

(by stationary transitions (87))

37

By matching coeﬃcients on the LHS and RHS in the last line in each parametric recursion
derivation, we obtain a formula9 to replace lines 13–20 in Algorithm 5 for t < T − 1,

1(t + 1; g))w1(T − 1; g) + w2

1(t + 1; g)w1(T − 1; Q),

1(t + 1; g) (w2(T − 1; Q) − w2(T − 1; g)) ,

w1(t; Q) ← (w1(t + 1; Q) − w2
w2(t; Q) ← w1(t + 1; Q)w2(T − 1; g) + w2
w3(t; Q) ← w2
1(t + 1; g)w3(T − 1; Q),
w1(t; g) ← w1(t + 1; g)w1(T − 1; g),
w2(t; g) ← w1(t + 1; g)w2(T − 1; g).

(105)

(106)

(107)

(108)

(109)

To see the use of the formulas above, please refer to line 33 in Algorithm 3.

5.3.3 Actor Training

To train our policy parameters {θ(t) : t ∈ T }, we will adopt the gradient-ascent rule in (85).
By substituting the state-invariant approximator in (98) and replacing the true Qπ(cid:48)
t (x, u)
with its current estimate ˆQt(x, u), we obtain

θ(t) ← θ(t) + αθ

(cid:88)

x∈ ˜Dt,·

∇u ˆQt(x, u; w(cid:48)(t))|u=θ(t),

∀t ∈ T .

(110)

State-invariant Policy By exploiting the state-invariance property in (97), we can sim-
plify the rule (110) while improving the accuracy of the update direction.

Let us revisit the rule before substitution of critic estimates,

θ(t) ← θ(t) + αθ

(cid:88)

x∈ ˜Dt,·

∇uQπ(cid:48)

t (x, u)|u=θ(t),

∀t ∈ T .

(111)

By the independence of action-value gradient to x, we have

∇uQπ(cid:48)

t (x, u) ∝ ∇uQπ(cid:48)

t (x, u),

(cid:88)

x∈ ˜Dt,·

which allows us to arbitrarily choose any x ∈ ˜Dt,· and substitute the latter into (111).
However, such an arbitrary substitution may no longer apply when ˆQt(x, u) is used in place
of Qπ(cid:48)
t (x, u) as the deterministic actor-critic prescribes due to the possible discrepancy in
the accuracy of ˆQt(x, u) at diﬀerent x ∈ ˜Dt,·, making the choice of x matters. We deal with
this issue by focusing our critic estimation to one particular x that we simply set to 1.

In what follows, we discuss how focusing critic approximation to x = 1 necessitates modiﬁ-
cations to Algorithms 5-9 or the methods detailed in Section 5.3.2.

9. The parameters w(cid:48)

0(t; g), w(cid:48)

0(t; Q) are irrelevant to the value of ∇u ˆQt(x, u) and have thus been omitted.

38

• At t = T −1, our modiﬁcation mainly happens inside Algorithms 8-9 concerning how to
avoid “throwing away” the samples collected with x (cid:54)= 1 in estimating ˆQt(1, u). This
can be done by transforming each collected experience tuple (x, u, X x,u) to (1, u, X 1,u)
according to (87), i.e. X 1,u .
= X x,u − (1 + r)(x − 1). We then adjust our TD-targets
ξg
t , ξQ
t by substituting any x with 1. This discussion is summarized into lines 17–
19 in Algorithm 3. Moreover, once we no longer care about x, keeping an estimate
importance distribution of XT −1 is no longer necessary. This warrants the use of
all 3-tuples (x, u, X x,u) from any period t in estimating ˆQT −1(1, u) that we indicate
by dropping the subscripts t from all mini-batches notation; for instance, compare
between line 15 in Algorithm 3 and line 2 in Algorithm 9.

• Next, still at t = T −1, we record some changes in the number of trainable parameters
for both ˆQT −1(1, u) and ˆgT −1(1, u) as we collapse the coeﬃcients of x, i.e. w1(T −1; g)
and w1(T − 1; Q) into intercepts; see lines 22 and 30 in Algorithm 3. We can then
disentangle the merged parameters w0(T − 1; ·) and w1(T − 1; ·) by applying (99)
noting that r is a known parameter; see lines 23 and 31 in Algorithm 3.

• At t < T − 1, our trainable parameters stay the same: w1(t; Q), w2(t; Q), w3(t; Q) for
Q and w1(t; g), w2(t; g) for g. Since we have recovered all the necessary parameters at
t = T − 1 to perform parametric recursion, no modiﬁcations to the derived formula
(105)-(109) are required; see line 33 in Algorithm 3.

5.3.4 Improving Training Stability

In this subsection, we group the training components in Algorithm 3 that deal particularly
with in-training stability issues.

Replay Speciﬁcations. Here, we specify a replay technique to regulate the mini-batch
sampling in line 15 of Algorithm 3 that will be used in solving the least-squares problems in
lines 21 and 29. At each iteration l, we separate current experiences (referring to the new
batch generated by lines 3–11) from past experiences. We include all current experiences
into the mini-batch ˜D(l)
·,· and then, sample randomly without replacement from past expe-
riences in κ : 1 proportion to the size of the current experiences. Hyperparameter involved
in this replay technique is the resample constant κ.

Least-squares Solver. To solve the argmin functions in lines 21 and 29, we will use a
type of regression that has been modiﬁed to account for the special attributes of noise model
(87) that breach the assumption of residuals independence to input variables in ordinary
least squares (OLS), causing severe instability issues in critic parameter estimation.

For each ϕ ∈ {g, Q}, we consider the corresponding OLS regression model for ϕt(x, u; w)
ξϕ = φϕ · wϕ + eϕ
(112)
with ξϕ, φϕ, and eϕ representing target variable, input variables, and residuals, respectively.
To illustrate the aforementioned noise attributes, we focus on ϕ = g, where ξg(x, u) =

39

˜ET −1,x[X u
the one-sample estimator X x,u to mini-batch estimator ˜ET −1,x[X u
implementation of line 21 in Algorithm 3. We then compute the following

T ]. Note that in the above, we have re-deﬁned the target variable deﬁnitions from
T ] to reﬂect the actual

(eg)2(x, u) = Var[ξg(x, u)] = Var[˜ET −1,x[X u
= Var[u˜E[(Y − r)]] = u2 σ2∆Wt
Nx,u

,

T ]] = Var[˜E[(1 + r)x + u(Y − r)]]

(113)

where Nx,u represents the number of sampled tuples (x, u, X x,u) used in estimating wg.
Referring back to (96), we have φg = (x, u, 1) and thus, the homoscedasticity requirement
on the residuals eg is only met when the number of samples in the mini-batch Nx,u ≈ ∞;
this is unrealistic in practice.

To mitigate the aforementioned heteroscedasticity’s eﬀect on training stability, we propose
an adaptive correction to our OLS regression model, namely adaptive least squares (ALS),
by performing the following steps; see Sterchi and Wolf (2017) for empirical evidence.

1. For each ϕ ∈ {g, Q}, rewrite the original OLS model in (112) as

ols = φϕ
ξϕ

ols · wϕ

t + eϕ

ols

(114)

and denote by ˆξϕ

ols the ﬁtted solution.

2. Derive model-based features φe for the OLS squared residuals (eϕ

ols)2 as exempliﬁed

in (113); thus, φeg

= (u2) and φeQ

= (u2, u4).

3. Perform OLS regression on the target-input variables ((ξϕ

ols− ˆξϕ
) without ﬁtting
any intercepts and denote by (ˆeϕ
ols)2 the ﬁtted residual values. As we may get 0 or
negative ﬁtted values due to some noisy estimates, we proceed by keeping only the
positive ﬁtted values.

ols)2, φeϕ

4. Transform the original target-input variable in (114) by

(ξϕ

als, φϕ

als) ←





ξϕ
ols
(ˆeϕ
ols)2

(cid:113)

,

φϕ
ols
(ˆeϕ
ols)2

(cid:113)



 .

5. Perform OLS regression with the transformed target-input variables (ξϕ

als, φϕ

als) without

ﬁtting any intercepts.

Smoothing Regularization. Finally, to tame the variance10 of (mini-batch) critic es-
timation at each iteration l, we apply exponential moving average (EMA) by setting the
critic learning rate α(l)
w = 2/(l + 1); see lines 22 and 30 in Algorithm 3.

10. This technique of slowing the update of parameters is commonly used in standard RL with function
approximation to ensure TD-error remains small across iterations; see Fujimoto et al. (2018) for instance.

40

5.4 Experiments

In this subsection, we perform simulation study, where we deploy our algorithm in two
diﬀerent types of MarketEnv with annualized mean µ ∈ {20%, −20%}, annualized volatility
σ = 30%, and annualized risk-free rate rann. = 2%. We normalize initial wealth x0 to 1, set
the investment horizon Tinv. to 1 year with timestep ∆t = 1/100, and ﬁx the mean-variance
criterion parameter γ = 1.2. In each MarketEnv, we evaluate our algorithm by its ﬁnancial
performance and learning curves of both critic parameters w and policy parameter θ.

5.4.1 Training Setup

For both experiments, we set the total training episodes L = 5000, trajectory generation
size B = 5, exploratory policy parameter λ = 1.5, and resample constant κ = 1. We note
that such setup of B and κ then implies a mini-batch size | ˜D·,·|= 1000 after appending past
experiences and including samples from all time periods t < T − 1 as speciﬁed in Section
5.3.3. We initialize our critic parameters w near the true analytical parameters and actor
parameters θ to 0. We ﬁx the learning rate for actor parameter update αθ = 2 and use
EMA learning rate α(l)
w = 2/(l + 1) for our critic parameter update.

5.4.2 Results and Discussions

Financial Performance For evaluation purpose, at each iteration l, a new price trajec-
tory (diﬀerent from the one used in training our actor-critic parameters) is generated from
MarketEnv. A non-randomized policy π(l) is then used to generate a wealth trajectory
from which the terminal wealth X (l)
is recorded. In Figures 1 and 2, we plot the learn-
T
ing curves of sample mean and sample standard deviation (stdev) of terminal wealth XT
that are computed by aggregating X (l)
T over 50 non-overlapping episodes. From these two
ﬁgures, we can observe that our algorithm converges in ≈ 20 aggregated episodes in both
MarketEnv setups. Moreover, the mean and stdev of return at convergence, i.e. (35%, 50%)
in MarketEnv(µ = 20%) and (45%, 60%) in MarketEnv(µ = −20%), are within a reasonable
range of Sharpe ratio.

Parameter Learning Curves
In the interest of model parameter identiﬁcation, we
record the learning curves of critic parameters w(T − 1); see Figures 3 and 4. First, we
clarify that we only present the learning curves at t = T − 1 because with the use of
parametric recursion technique, one directly links the learning performance for earlier time
periods to the last time period t = T − 1. We compare the learning curve of our proposed
algorithm (‘EMA’) to the ground truth (‘TRUE’) that is computed by substituting the
MarketEnv parameters µ, σ to (100)-(102). Similarly as in the terminal wealth curve, we
observe that convergence happens in about 20 aggregated episodes. Moreover, to illustrate
how our smoothing choice stabilizes the noisiness of w(T − 1) updates, we also include the

41

Figure 1: Sample mean and stdev of terminal wealth (µ = 20%)

Figure 2: Sample mean and stdev of terminal wealth (µ = −20%)

parameter learning curve of a contending stabilization moving average technique over past
20 periods, MA(q = 20), which clearly fall short of EMA.

Optimal Strategy Finally, in the last plot of both Figures 3 and 4, we present the
learning curves of policy πT −1(·) that concurrently represents our actor parameter θ(T − 1)
by the state-invariant approximator 97. We compare the policy at convergence with the
ground truth (‘TRUE’) where as derived in Bj¨ork and Murgoci (2014),

u∗
t =

(µ∆t − r)2
γσ2∆t

(1 + r)T −(t+1),

∀t ∈ T .

Thus, we conclude that the gradient-based update will converge to the ground truth in
about 40 aggregated episodes. To further illustrate how this result translates to other time
periods, we provide the learning curves of u∗

in Appendix B.

t at t = 0, T −1
2

42

Figure 3: Critic and actor parameter learning curves at t = T − 1 under MarketEnv(µ = 20%)

5.5 Chapter Summary

In this section, we applied SPERL deterministic actor-critic training framework and spec-
iﬁed training procedures that suit the problem speciﬁcations. In particular, we have used

43

Figure 4: Critic and actor parameter learning curves at t = T − 1 under MarketEnv(µ = −20%)

model-based function approximators and perform model-based reductions to both actor
and critic training problems. We note some connections to control-based approaches in the
derivations of parametric recursion formulas (105)-(109), that are analogous to the deriva-
tions of analytic equilibrium control except for our use of action-value function Q in place of

44

value function V . This is natural under the restrictive problem speciﬁcations that we made
at the beginning, i.e. (88) being our only unknowns, which implies that all the remaining
model assumptions are at our disposal. The usage of these assumptions can be observed
at each step of converting Q-recursion to parametric recursion. While such reduction can
help us achieve sample and training eﬃciency, using too many model assumptions may be
undesirable in practice, especially in more complex domain, where model is generally un-
available. In such an event, we have no choice but to get stuck in the ﬁrst step of conversion
and use model-free training as presented in Section 4. This observation illustrates SPERL
advantage over the current analytic equilibrium control approaches.

6. Conclusions and Future Works

In this paper, we studied the search of SPE policy in ﬁnite-horizon TIC problems as a RL
problem, which forms the proposed SPERL framework. By drawing insights from the ex-
tended DP theory, we proposes a new class of policy iteration algorithm, which we refer to
as BPI, as a SPERL solver. We further conducted detailed analyses on BPI’s update rules
and correspondingly showed some desirable properties, such as update (lex-)monotonicity
and convergence to SPE policy, which in turn address some existing challenges in TIC-RL
domain. To demonstrate how BPI can be used in practice, we discussed several ways of pair-
ing BPI with standard RL simulation methods, resulting in two main training frameworks:
SPERL Q-learning and SPERL deterministic actor-critic. We then illustrate a full train-
ing algorithm derivation under the SPERL deterministic actor-critic framework through
a mean-variance analysis example. The experimental results are plausible and show the
eﬃciency of the proposed algorithms.

Noting that some training and implementation details are still left to generalities, promising
future research directions are to investigate on these training matters, examining on more
complex domain problems, and benchmarking with other TIC-RL algorithms, especially
those that do not belong to either globally optimal or SPE policy class. Moreover, the
learning algorithms in this paper provide a practical solution towards the search of SPE
policy other than the analytical solution, especially when the latter is not available in
practice or a model-free environment. Since TIC is considered as a key feature to better
revealing human’s preferences, it will be interesting to explore applications of this SPERL
framework.

45

Appendix A. SPERL Training Algorithms for General TIC Problems

A.1 SPERL Q-learning

Algorithm 4: SPERL Q-learning

: Env, Hyperparameters(α, (cid:15))

Input
Output : Approximate SPE Q-function ˆQt(x, u), ∀t, x, u
Initialize: ˆQ, ˆr, ˆg, ˆf ; π(cid:48) ← ∅; πt(x) ← arg maxu ˆQ(x, u), ∀t, x

1 while π(cid:48) (cid:54)= π do
2

Update π ← π(cid:48);
Choose X0 randomly;
Generate trajectory X0, U0, X1, U1, . . . , XT −1, UT −1, XT ∼ π(cid:15)-greedy ;
for t ← T − 1 to 0 do

for τ ∈ {t, t − 1, . . . , 0}, y ∈ {Xt, Xt−1, . . . , X0} do

for m ← t to T − 1 do

Compute ξr
ˆrt(Xt, Ut, τ, m, y) ← ˆrt(Xt, Ut, τ, m, y) + α(ξr

t (Xt, Ut, τ, m, y) by (77);

t ← ξr

t − ˆrt(Xt, Ut, τ, m, y));

end
Compute ξf
t (Xt, Ut, τ, y) by (78);
ˆft(Xt, Ut, τ, y) ← ˆft(Xt, Ut, τ, y) + α(ξf

t ← ξf

t − ˆft(Xt, Ut, τ, y));

t (Xt, Ut) by (79);

end
Compute ξg
t ← ξg
ˆgt(Xt, Ut) ← ˆgt(Xt, Ut) + α(ξg
t ← ξQ
Compute ξQ
ˆQt(Xt, Ut) ← ˆQt(Xt, Ut) + α(ξQ
Compute u(cid:48) ← arg maxu ˆQt(Xt, u);
if ˆQt(Xt, u(cid:48)) > Qt(Xt, πt(Xt)) then

t (Xt, Ut) by (80);

t − ˆgt(Xt, Ut));

t − ˆQt(Xt, Ut));

t(Xt) ← u(cid:48)
π(cid:48)
else
π(cid:48)
t(Xt) ← πt(Xt);
end

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

end

24
25 end

The case of random rewards.
in Remark 2, we can sample Rt and modify our trajectory generation in line 4 to

In the presence of random rewards as shortly remarked

X0, U0, R1, X1, U1, . . . , XT −1, UT −1, RT , XT .

Since these random rewards are the strict attributes of adjustment function r, it remains
to modify its target computation in line 8 to account for Rt that is, by assigning

ξr
t ← H(τ, y, Rt)

46

A.2 SPERL Deterministic Actor-Critic

Algorithm 5: SPERL Deterministic Actor-Critic

Input : Env, Hyperparameters(α, (cid:15))
Output: Approximate SPE-policy πθ

1 Initialize critic parameters w, actor parameters θ, replay memory D ← ∅;
2 for l ← 0 to L do
3

for b ← 1 to B do

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

Choose X0 randomly;
Generate trajectory X0, U0, X1, U1, . . . , XT −1, UT −1, XT ∼ πθ
for t ← 0 to T − 1 do
for τ ← t to 0 do

(cid:15)-greedy ;

D ← D ∪ {(t, τ, Xt, Ut, Xτ , Xt+1)}

end

end

end
for t ← T − 1 to 0 do
for τ ← t to 0 do

for m ← t to T − 1 do

w(t, τ, m; r) ← UPDATE-r(w, α, θ, t, τ, m, D);

end
w(t, τ ; f ) ← UPDATE-f (w, α, θ, t, τ, D);

end
w(t; g) ← UPDATE-g(w, α, θ, t, D);
w(t; Q) ← UPDATE-Q(w, α, θ, t, D);
(cid:80)

(cid:16)

θ(t) ← θ(t) + αθ

˜Dt,·

∇θ ˆπt(x; θ)|θ=θ(t)∇u ˆQ(x, u; w(t; Q))|u=ˆπt(x;θ(t))

(cid:17)

;

end

22
23 end

We discuss in SPERL context several implementation essentials that are common in dealing
with function approximators and critic estimation.

Choice of Critic Approximator.
In Algorithms 5–7, we have incorporated tabularized
weight representations for f, r; see Section 4.2 for details. Beyond this, we do not restrict
how input space should be segregated or aggregated. For instance, neural networks and
linear approximators can both be used to represent ˆft(x, u, y; w) depending on how amenable
are the prediction problems at hand.

Critic Parameter Update. We illustrate how critic parameters w are updated by the
last two lines in Algorithms 6–9. For instance, we refer to Algorithm 9. To solve the
arg min function in line 21, any least-squares solvers, such as stochastic gradient descent,
Batch gradient descent, or simple regression, can be used. Line 22 then provides some
ﬂexibility to incorporate smoothening of parameter updates, i.e. αw < 1.

47

5

6

7

8

5

6

7

8

Algorithm 6: UPDATE-r

Input : w, α, θ, t, τ, m, D
Output: w(cid:48)(t, τ, m; r)

1 Initialize Ξt,τ ← ∅;
2 Sample mini-batch ˜Dt,τ ∼ Replay(t, τ, D);
3 for (t, τ, x, u, y, X x,u) ∈ ˜Dt,τ do
4

if m = t then

ξr
t ← Rτ,t(y, x, u)
else
t ← ˆrt+1(X x,u, ˆπt+1(X x,u; θ(t + 1)), y; w(t + 1, τ, m; r))
ξr
end
Set Ξt,τ ← Ξt,τ ∪ (t, τ, x, u, y, ξr

t );

9
10 end
11 Solve w∗ ← arg minw
Ξt,τ
12 w(cid:48)(t, τ, m; r) ← w(t, τ, m; r) + α (w∗ − w(t, τ, m; r));

t − ˆrt(x, u, y; w))2;

(ξr

(cid:80)

Algorithm 7: UPDATE-f
Input : w, α, θ, t, τ, D
Output: w(cid:48)(t, τ ; f )
1 Initialize Ξt,τ ← ∅;
2 Sample mini-batch ˜Dt,τ ∼ Replay(t, τ, D);
3 for (t, τ, x, u, y, X x,u) ∈ ˜Dt,τ do
4

if t = T − 1 then

ξf
t ← Fτ (y, X x,u)
else
ξf
t ← ˆft+1(X x,u, ˆπt+1(X x,u; θ(t + 1)), y; w(t + 1, τ ; f ))
end
Set Ξt,τ ← Ξt,τ ∪ (t, τ, x, u, y, ξf

t );

9
10 end
11 Solve w∗ ← arg minw
12 w(cid:48)(t, τ ; f ) ← w(t, τ ; f ) + α (w∗ − w(t, τ ; f ));

ξf
t − ˆft(x, u, y; w)

(cid:80)

Ξt,τ

(cid:16)

(cid:17)2

;

48

Algorithm 8: UPDATE-g
Input : w, α, θ, t, D
Output: w(cid:48)(t; g)
1 Initialize Ξt,· ← ∅;
2 Sample mini-batch ˜Dt,· ∼ Replay(t, ·, D);
3 for (t, τ, x, u, y, X x,u) ∈ ˜Dt,· do
4

5

6

7

8

if t = T − 1 then
ξg
t ← X x,u
else
ξg
t ← ˆgt+1(X x,u, ˆπt+1(X x,u; θ(t + 1)); w(t + 1; g))
end
Set Ξt,· ← Ξt,· ∪ (t, x, u, ξg

t );

9
10 end
11 Solve w∗ ← arg minw
12 w(cid:48)(t; g) ← w(t; g) + α (w∗ − w(t; g));

(ξg

(cid:80)

Ξt,·

t − ˆgt(x, u; w))2;

Algorithm 9: UPDATE-Q

Input : w, α, θ, t, D
Output: w(cid:48)(t; Q)
1 Initialize Ξt,· ← ∅;
2 Sample mini-batch ˜Dt,· ∼ Replay(t, ·, D);
3 for (t, τ, x, u, y, X x,u) ∈ ˜Dt,· do
4

if t = T − 1 then

ξQ
t ← ˆrt(x, u, x; w(t, t, t; r)) + ˆft(x, u, x; w(t, t; f )) + Gt(x, ˆgt(x, u; w(t; g)))

else

Set ∆ˆrt ← 0;
for m ← t + 1 to T − 1 do

∆ˆrt ← ∆ˆrt + ˆrt+1(X x,u, ˆπt+1(X x,u; θ(t + 1)), X x,u; w(t + 1, t + 1, m; r))

−ˆrt(x, u, x; w(t, t, m; r));

end
∆ ˆft ← ˆft+1(X x,u, ˆπt+1(X x,u; θ(t + 1)), X x,u; w(t + 1, t + 1; f ))

− ˆft(x, u, x; w(t, t; f ));

∆ˆgt ← Gt+1(X x,u, ˆgt+1(X x,u, ˆπt+1(X x,u; θ(t + 1)); w(t + 1; g))

−Gt(x, ˆgt(x, u; w(t; g));

ξQ
t ← ˆrt(x, u, x; w(t, t, t; r)) + ˆQt+1(X x,u, ˆπt+1(X x,u; θ(t + 1)); w(t + 1; Q))

−(∆ˆrt + ∆ ˆft + ∆ˆgt);

5

6

7

8

9

10

11

12

13

14

15

16

17

18

end
Set Ξt,· ← Ξt,· ∪ (t, x, u, ξQ

19
20 end
21 Solve w∗ ← arg minw
22 w(cid:48)(t; Q) ← w(t; Q) + αw (w∗ − w(t; Q));

(cid:80)

Ξt,·

(cid:16)

t );

ξQ
t − ˆQt(x, u; w)

(cid:17)2

;

49

Replay Buﬀer.
In the case of noisy input-target pairs to be used in critic estimation,
keeping a replay buﬀer can help stabilize training by replaying past experiences. In our
example algorithms, experiences are collected in the form of tuple (t, τ, x, u, y, X x,u), where
X x,u denotes the next state encountered after hitting state x and acting u. The notation
X x,u marks our use of the stationary transition probability assumption in Section 2.1.
To contrast with the state-action-reward-state action (SARSA) experiences collection in
standard RL context, we need to collect additional information about t for our ﬁnite-horizon
model and τ, y = xτ for our adjustment functions r, f ; see Algorithms 6 and 7 for illustration
on how these information (especially the latter) are used. Any replay techniques can then
be used on the pool of experiences D in place of the function “Replay” in Algorithms 6-9.
In the case of on-policy sampling, we can simply replay the latest collected data in D.

Appendix B. Experimental Results of Mean-Variance Analysis

Figure 5: Actor learning curve (µ = 20%)

Figure 6: Actor learning curve (µ = −20%)

50

References

Joshua Achiam and Shankar Sastry. Surprise-based intrinsic motivation for deep reinforce-

ment learning. arXiv:1703.01732, March 2017.

Robert J. Barro. Ramsey meets Laibson in the neoclassical growth model. The
Quarterly Journal of Economics, 114(4):1125–1152, November 1999. doi: 10.1162/
003355399556232.

G. Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and R´emi
Munos. Unifying count-based exploration and intrinsic motivation. In Proceedings of the
30th International Conference on Neural Information Processing Systems 29 (NIPS’16),
page 1479–1487, Barcelona, Spain, December 2016.

Tomas Bj¨ork and Agatha Murgoci. A theory of Markovian time-inconsistent stochastic
control in discrete time. Finance and Stochastics, 18(3):545–592, June 2014. doi: 10.
1007/s00780-014-0234-y.

Tomas Bj¨ork, Mariana Khapko, and Agatha Murgoci. On time-inconsistent stochastic
control in continuous time. Finance and Stochastics, 21(2):331–360, March 2017. doi:
10.1007/s00780-017-0327-5.

Werner F. M. Bondt and Richard Thaler. Does the stock market overreact? The Journal

of Finance, 40(3):793–805, July 1985. doi: 10.1111/j.1540-6261.1985.tb05004.x.

Nuttapong Chentanez, Andrew Barto, and Satinder Singh.

Intrinsically motivated rein-
forcement learning. In L. Saul, Y. Weiss, and L. Bottou, editors, Advances in Neural
Information Processing Systems 17 (NIPS 2004), volume 17. MIT Press, 2004.

Xiangyu Cui, Duan Li, and Xun Li. Mean-variance policy for discrete-time cone-constrained
markets: Time consistency in eﬃciency and the minimum-variance signed supermartin-
gale measure. Mathematical Finance, 27(2):471–504, April 2017. doi: 10.1111/maﬁ.12093.

Ivar Ekeland and Ali Lazrak. Being serious about non-commitment: Subgame perfect

equilibrium in continuous time. arXiv:math/0604264, April 2006.

Ivar Ekeland and Ali Lazrak. The golden rule when preferences are time inconsistent.
doi: 10.1007/

Mathematics and Financial Economics, 4(1):29–55, November 2010.
s11579-010-0034-x.

Ivar Ekeland and Traian A. Pirvu.

Investment and consumption without commit-
ment. Mathematics and Financial Economics, 2(1):57–86, July 2008. doi: 10.1007/
s11579-008-0014-6.

Owain Evans, Andreas Stuhlm¨uller, and Noah D. Goodman. Learning the preferences
In Proceedings of the AAAI Conference on Artiﬁcial

of ignorant, inconsistent agents.
Intelligence, volume 30 of AAAI’16, pages 323–329, Phoenix, Arizona, February 2016.

51

William Fedus, Carles Gelada, Yoshua Bengio, Marc G. Bellemare, and Hugo Larochelle.
Hyperbolic discounting and learning over multiple horizons. arXiv: 1902.06865, February
2019.

Shane Frederick, George Loewenstein, and Ted O’donoghue. Time discounting and time
preference: A critical review. Journal of Economic Literature, 40(2):351–401, June 2002.
doi: 10.1257/jel.40.2.351.

Jeﬀrey C. Fuhrer. Habit formation in consumption and its implications for monetary-policy
models. American Economic Review, 90(3):367–390, June 2000. doi: 10.1257/aer.90.3.
367.

Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error
in actor-critic methods. In Jennifer Dy and Andreas Krause, editors, Proceedings of the
35th International Conference on Machine Learning, volume 80 of Proceedings of Machine
Learning Research, pages 1587–1596. PMLR, July 2018. URL https://proceedings.
mlr.press/v80/fujimoto18a.html.

Daishi Harada. Reinforcement learning with time. In AAAI-97 Proceedings, pages 577–582,

1997.

Daniel Kahneman and Amos Tversky. Prospect theory: An analysis of decision under risk.

Econometrica, 47(2):263, March 1979a. doi: 10.2307/1914185.

Daniel Kahneman and Amos Tversky. Intuitive prediction: Biases and corrective proce-

dures. TIMS Studies in Management Science, 12:313–327, 1979b.

Zeb Kurth-Nelson and A. David Redish. A reinforcement learning model of precommitment
in decision making. Frontiers in Behavioral Neuroscience, 4:1–13, December 2010. doi:
10.3389/fnbeh.2010.00184.

David Laibson. Golden eggs and hyperbolic discounting. The Quarterly Journal of Eco-

nomics, 112(2):443–478, May 1997. doi: 10.1162/003355397555253.

Tor Lattimore and Marcus Hutter. General time consistent discounting. Theoretical Com-

puter Science, 519:140–154, January 2014. doi: 10.1016/j.tcs.2013.09.022.

Qian Lei and Chi Seng Pun.

An extended McKean–Vlasov dynamic program-
ming approach to robust equilibrium controls under ambiguous covariance matrix.
SSRN.com/abstract=3581429, April 2020. doi: 10.2139/ssrn.3581429.

Qian Lei and Chi Seng Pun. Nonlocal fully nonlinear parabolic diﬀerential equations arising

in time-inconsistent problems. arXiv: 2110.04237, October 2021.

Duan Li and Wan-Lung Ng. Optimal dynamic portfolio selection: Multiperiod mean-
variance formulation. Mathematical Finance, 10(3):387–406, July 2000. doi: 10.1111/
1467-9965.00100.

Erzo G. J. Luttmer and Thomas Mariotti. Subjective discounting in an exchange economy.

Journal of Political Economy, 111(5):959–989, October 2003. doi: 10.1086/376954.

52

Shie Mannor and John N. Tsitsiklis. Mean-variance optimization in Markov decision pro-
cesses. In Proceedings of the 28th International Conference on International Conference
on Machine Learning (ICML’11), pages 177–184, Bellevue Washington USA, June 2011.

Harry Markowitz. Portfolio selection. The Journal of Finance, 7(1):77–91, March 1952.

doi: 10.1111/j.1540-6261.1952.tb01525.x.

Jesper Lund Pedersen and Goran Peskir. Optimal mean-variance portfolio selection.
10.1007/

Mathematics and Financial Economics, 11(2):137–160, June 2016.
s11579-016-0174-8.

doi:

Bezalel Peleg and Menahem E. Yaari. On the existence of a consistent course of action
when tastes are changing. The Review of Economic Studies, 40(3):391, July 1973. doi:
10.2307/2296458.

Huyˆen Pham and Xiaoli Wei. Dynamic programming for optimal control of stochastic
McKean–Vlasov dynamics. SIAM Journal on Control and Optimization, 55(2):1069–
1101, January 2017. doi: 10.1137/16m1071390.

E. S. Phelps and R. A. Pollak. On second-best national saving and game-equilibrium growth.

The Review of Economic Studies, 35(2):185, April 1968. doi: 10.2307/2296547.

Robert A. Pollak. Consistent planning. The Review of Economic Studies, 35(2):201, April

1968. doi: 10.2307/2296548.

Chi Seng Pun and Zi Ye. Optimal multi-period transaction-cost-aware long-only portfolio

and its time consistency in eﬃciency. Working paper at NTU Singapore, 2021.

J¨urgen Schmidhuber. Adaptive conﬁdence and adaptive curiosity. Technical report, Institut
fur Informatik, Technische Universitat Munchen, Arcisstr. 21, 800 Munchen 2, 1991.

David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Ried-
miller. Deterministic policy gradient algorithms. In Proceedings of the 31st International
Conference on International Conference on Machine Learning (ICML’14), volume 32,
pages 387–395, Beijing, China, June 2014. PMLR, JMLR.org.

M. Simaan and J. B. Cruz. On the Stackelberg strategy in nonzero-sum games. Jour-
nal of Optimization Theory and Applications, 11(5):533–555, May 1973. doi: 10.1007/
bf00935665.

Herbert A. Simon. A behavioral model of rational choice. The Quarterly Journal of Eco-

nomics, 69(1):99, February 1955. doi: 10.2307/1884852.

Herbert A. Simon, Massimo Egidi, Riccardo Viale, and Robin Marris. Economics, Bounded
Rationality and the Cognitive Revolution. Edward Elgar Publishing Limited, 2008. ISBN
978-1-85278-425-6.

Matthew J. Sobel. The variance of discounted Markov decision processes. Journal of Applied

Probability, 19(4):794–802, December 1982. doi: 10.2307/3213832.

53

Martin Sterchi and Michael Wolf. Weighted least squares and adaptive least squares: Fur-
ther empirical evidence.
In Vladik Kreinovich, Songsak Sriboonchitta, and Van-Nam
Huynh, editors, Robustness in Econometrics, volume 692 of Studies in Computational
Intelligence, pages 135–167. Springer, Cham, February 2017. ISBN 978-3-319-50741-5.
doi: 10.1007/978-3-319-50742-2 9.

Robert H. Strotz. Myopia and inconsistency in dynamic utility maximization. The Review

of Economic Studies, 23(3):165–180, December 1955. doi: 10.2307/2295722.

Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT

Press Ltd, 2 edition, November 2018. ISBN 0262039249.

Aviv Tamar and Shie Mannor. Variance adjusted actor critic algorithms. arXiv: 1310.3697,

October 2013.

Aviv Tamar, Dotan Di Castro, and Shie Mannor. Learning the variance of the reward-to-go.

The Journal of Machine Learning Research, 17(1):361–396, March 2016.

Amos Tversky and Daniel Kahneman. Advances in prospect theory: Cumulative represen-
tation of uncertainty. Journal of Risk and Uncertainty, 5(4):297–323, October 1992. doi:
10.1007/bf00122574.

Haoran Wang and Xun Yu Zhou. Continuous-time mean-variance portfolio selection: A re-
inforcement learning framework. Mathematical Finance, 30(4):1273–1308, October 2020.
doi: 10.1111/maﬁ.12281.

Christopher J. C. H. Watkins and Peter Dayan. Q-learning. Machine Learning, 8(3-4):

279–292, May 1992. doi: 10.1007/bf00992698.

54

