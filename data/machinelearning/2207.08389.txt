The short version of this work is accepted at ACM/IEEE CASESâ€™22 â€“ https://esweek.org/cases

MLGOPerf: An ML Guided Inliner to Optimize Performance
Amir H. Ashouriâˆ— Mostafa Elhoushi Yuzhe Hua Xiang Wang Muhammad Asif Manzoor
Bryan Chan Yaoqing Gao

Huawei Technologies, Heterogeneous Compiler Lab
Toronto, Canada

2
2
0
2

l
u
J

9
1

]
L
P
.
s
c
[

2
v
9
8
3
8
0
.
7
0
2
2
:
v
i
X
r
a

ABSTRACT
For the past 25 years, we have witnessed an extensive applica-
tion of Machine Learning to the Compiler space; the selection and
the phase-ordering problem. However, limited works have been
upstreamed into the state-of-the-art compilers, i.e., LLVM, to seam-
lessly integrate the former into the optimization pipeline of a com-
piler to be readily deployed by the user. MLGO was among the
first of such projects and it only strives to reduce the code size of a
binary with an ML-based Inliner using Reinforcement Learning.

This paper presents MLGOPerf; the first end-to-end framework
capable of optimizing performance using LLVMâ€™s ML-Inliner. It em-
ploys a secondary ML model to generate rewards used for training
a retargeted Reinforcement learning agent, previously used as the
primary model by MLGO. It does so by predicting the post-inlining
speedup of a function under analysis and it enables a fast train-
ing framework for the primary model which otherwise wouldnâ€™t
be practical. The experimental results show MLGOPerf is able to
gain up to 1.8% and 2.2% with respect to LLVMâ€™s optimization at
O3 when trained for performance on SPEC CPU2006 and Cbench
benchmarks, respectively. Furthermore, the proposed approach pro-
vides up to 26% increased opportunities to autotune code regions
for our benchmarks which can be translated into an additional 3.7%
speedup value.

CCS CONCEPTS
â€¢ Software and its engineering â†’ Compilers; â€¢ Computing
methodologies â†’ Machine learning.

KEYWORDS
Compilers, Reinforcement Learning, Deep Learning, LLVM, Inlining

1 INTRODUCTION
Compilers have gone a long way to emit efficient and high per-
formance code given high-level programming languages. Break-
throughs in compiler technology are essential to making program-
ming mainstream [26]. State-of-the-art compiler frameworks such
as LLVM and GCC are able to translate the high-level programming
languages into an (almost) architecture-independent Intermediate
Representation (IR) in which a sequence of optimization passes can
be applied to optimize a code segment, iteratively. These optimiza-
tion passes can be applied at the granularity of Function, Call Graph,
Loop, or Module level [34]. Gradually and tentatively, compiler de-
signers come up with a set of fixed-length optimization pipelines,
known as standard optimization levels, i.e., -O{1,2,3,s,z}, that
on average have shown benefits to optimizing performance (speed)

or code size. For instance, LLVMâ€™s O3 has around 160 passes in its
pipeline, performing a number of optimizations at different levels of
the granularity from which around 60 passes are unique and several
sub-sequences of passes are repeated in the optimization sequence
[5]. The idea behind producing such a standardized optimization
sequence is to enable users with an option that performs on average
good [24].

In the past couple of decades, several attempts have been made to
specialize the standard optimization levels given a code segment of
interest as they are not always the optimal solution [1, 5, 7, 14, 41].
These approaches rely on leveraging applications of Machine Learn-
ing (ML) and an automatic tuning methodology, possibly using an
autotuner [3, 30], to speed up the search in the vast compiler op-
timization space [6]. Although in many cases researchers have
achieved meaningful results, these approaches are often limited
by the use of external/third-party components outside a compiler
framework to derive intelligence for it. Fortunately, we have been
witnessing more and more proposed works, tackling the above
problems by adding end-to-end frameworks [17, 19, 25, 36, 45],
however, to the best of our knowledge, there has been only a single
recent work, namely MLGO [54], which does so by refactoring
an optimization pass, i.e, function inlining, to bring an ML-based
guidance into the optimization pass, as a standalone component.
MLGO is also upstreamed into LLVM trunk 1.

Function Inlining is one of the fundamental compiler optimiza-
tions used by many state-of-the-art compiler frameworks; when
applied to a function, it examines each call site within the func-
tion and decides whether or not to inline the function body of the
callee into the caller. Not only does inlining eliminate the function
call overhead at the call site, it also expands the scope of intra-
procedural analyses of subsequent passes and enables additional
optimizations [46, 52]. When performed incorrectly, however, in-
lining can cause code size increase, which can lead to performance
loss due to instruction cache misses. Therefore, an ideal inlining
heuristic must either improve performance without incurring unac-
ceptable code bloat, or reduce code size while avoiding substantial
performance loss. Constructing an ideal inlining heuristic has been
shown to be a complex problem and the rate at which the number
of choices would grow is known to be at least at an exponential
increase [6, 52].

Presently, MLGO provides code size reduction to the LLVMâ€™s
inline optimization and we based our proposed work on adapting a
framework which provides performance optimization for it. Our
work, MLGOPerf, proposes the aforementioned contribution to
the community, by leveraging two ML models to optimize inline

*Corresponding author: amirh.ashouri@huawei.com.

1https://lists.llvm.org/pipermail/llvm-dev/2020-April/140763.html

1

 
 
 
 
 
 
optimization for speed. MLGOPerf, increases the tunable code-
regions subsequent to the inline pass under O3 and we show in the
experimental results that it can provide an added speedup value
with respect to the MLGOâ€™s and the vanilla inliner. MLGOPerf
proposes the following contributions:

(1) We propose an ML model, namely IR2Perf, to predict the
post-inlining function speedup of a caller with respect to its
baseline. IR2Perf leverages a number of handcrafted features
from LLVM IR and it correlates the speedup outcome to the
changes in IR as a result of function inlining. The model
enables us to rapidly generate Rewards needed to train the
existing Reinforcement Learning (RL) agent used for LLVMâ€™s
ML-Inliner infrastructure without the need to execute each
program thousands of times.

(2) We add an extra couple of features to better define the be-
havior of callee functions and to boost the accuracy of the
existing LLVM ML-Inliner RL agent.

(3) At model training, we utilize the newly generated rewards
from IR2Perf and revised the RL agent to optimize for perfor-
mance rather than code size. Finally, at model deployment,
we provide a pretrained model to be built with LLVM CMake
system and used with LLVMâ€™s ML-Inliner for inference with-
out the need for IR2Perf.

The rest of the paper organizes as follow. Section 2 discusses the
state-of-the-art. Section 3 provides details on our proposed method
and how the two ML models interact with each other in Sections 3.1
and 3.2, respectively. Section 4 showcases the experimental results.
Finally, in Section 5 we reveal the current shortcomings, challenges,
and propose some future work.

2 RELATED WORK
Compiler autotuning has become an extensively researched area
in the past two decades [6], more so with the introduction of the
application of Machine Learning (ML) in a number of early ap-
proaches, i.e, optimization space reduction [15], estimating un-
rolling factor [32], scheduling [38], etc. However, the space has
gradually shifted towards tackling two major problems, both of
which remains open problems, namely (1) the optimization selec-
tion problem [1, 7, 11, 23, 41] and, (2) the phase ordering problem
[5, 18, 33, 39, 55].

The bulk of the approaches are tackling the above problems as
a black-box optimization; by means of an autotuning [3, 9, 14, 21,
30] strategy paired with an ML model, which derives an iterative
compilation [1, 5, 7, 8, 11, 27, 40, 53] methodology to speed up the
search.

Recently, we have also witnessed the applications of Deep Learn-
ing in the mix. For instance, using Deep Reinforcement Learning
[51] has become a popular method. On the optimization side, au-
thors tackle the problem of finding the optimal vectorization [25],
providing a high-level optimization environment for compiler re-
search [18], and an autotuner [42]. Additionally, by leveraging
Natural Language Processing (NLP) [56], one can generate an em-
bedding or simply a characterization of the source-level software
using representative features [2, 10, 16]. However, there have been
a number of works addressing the aforementioned problems with
an end-to-end framework and as such, they are tailored towards

bringing the autotuning into the compiler [17â€“19, 22, 25, 36, 45].
These methods still leverage a high-level optimization engine which
wraps around the identification of beneficial passes to apply given a
segment of code and none has managed to propose a standalone op-
timization pass capable of deriving decisions by means of inference
from an ML model built-into the compiler.

Rotem and Cummins [45] propose an ML framework, leveraging
Decision Trees (DS), to provide profile-guided Optimization (PGO)
to branch probabilities. The authors use handcrafted features for
both basic blocks and branches to characterize a code segment and
employ XGBoost [13] library to generate their DS. This work is rel-
evant to ours only because it does the aforementioned methodology
built into LLVM and once trained, it can provide PGO inferences
without the need to actually executing a number of iterations of
PGO profiling.

Phothilimthana et al. [43] propose a two-level autotuner to tune
graph-level and subgraph-level optimizations across multiple com-
pilation stages. The authors provide a joint optimization method-
ology for a number of tensor parameters and operations mostly
leveraged in production ML compilers, including tile size, tensor lay-
outs, operator fusion, and code generation. This work is orthogonal
to our proposed work.

Trofin et al. [54] propose MLGO which was the first of a kind to
provide an ML-guided optimization for a pass, i.e., inline optimiza-
tion 2. The work has been upstreamed into LLVM and provides
guidance to the inline optimization as to whether or not a call
site should be inlined, provided that the generated code size is
minimized. Our proposed work, MLGOPerf, is similar to MLGO
in the sense that we leverage the inlining infrastructure already
upstreamed in LLVM repository, however, we adapt a framework
to derive decisions to optimize performance rather than a reduction
in code size.

Optimizing the execution-time performance of compiler passes
is an inherently more difficult problem and we attempt to do so
by proposing a second ML model, i.e., IR2Perf, which is able to
predict the speedup of a function after it was passed through the
function inlining optimization. Leveraging IR2Perf, we generate a
fitness function, namely, Rewards to train an RL agent that learns
the complex behavior of providing an inlining decision given a
call site. Recent studies have suggested using a semi-supervised
learning [31] and using loss as self-supervision [48] to guide the
learning of an RL agent with success. We believe, a supervised
reward learning process has merits in helping the ML-Inliner agent
to learn its optimal policy. Once trained, MLGOPerf is able to pro-
vide standalone predictions to LLVMâ€™s ML-Inliner and to optimize
segments of the code that are (1) running faster and additionally,
(2) contain an enhanced number of inter-procedural opportunities
down the pipeline of LLVMâ€™s O3 which may translate into a faster
code.

3 PROPOSED METHODOLOGY
Recently, Upstream LLVM Inliner has benefited from MLGO [54] for
which the user can leverage an ML-Inliner pass to derive Inlining
decisions for call sites. The current ML-Inliner approach is targeted
towards code size reduction and, although function inlining is not

2The repository has since provided an added option for register allocation.

2

Table 1: MLGOPerf Phases

Phases
IR2Perf Training
RL Model Training
MLGOPerf Deployment

IR2Perf
Training
Inference
-

RL Model
-
Training
Inference

3.2 MLGOPerf Phases
As a result of leveraging two ML models, our proposed work is
a multi-phased methodology and thus, in this section we provide
insights to better demonstrate the functionalities of our approach.
Table 1 showcase the specific interaction between the two models
within the three phases. Furthermore, Figure 2 depicts the three
phases in higher detail. We discuss the three phases in the following
subsections.

IR2Perf Training. In this phase, we design an autotuning
3.2.1
methodology that controls the Inlining decisions made at the gran-
ularity of the call sites of a function. We do so by leveraging an
autotuner [30], an OpenTuner [3] derivative. This allows us to
generate meaningful training data that captures the behavior of
function inlining when it decides to inline a call site into its respec-
tive caller together with its Function and Module execution times.
Additionally, we develop a number of relevant function features,
as a pass, and register it subsequent to the Inline optimization into
the O3 pipeline to collect our training data.

Let ğœ” be a characterization vector of features of a function having
at minimum one call site. This vector represents ğ‘™ variables to
account for the intermediate representation of the function when
collected post-inlining. We feed the collected features together
with the measured execution time and the corresponding inlining
configuration into our ML model. IR2Perf is a Deep Regression
model, when trained, designed to predict the relative speedup of
a function under analysis wrt O3 when an inlining configuration
was applied.

3.2.2 RL Model Training. Reinforcement Learning (RL) is a class
of Machine Learning which tries to find an optimal policy for a
Markov Decision Process (MDP) that is defined by the tuple of
< ğ‘†, ğ´,ğ‘‡ , ğ‘… > where ğ‘† is the state space, ğ´ represents action space,
ğ‘… is the reward function that an agent receives by doing action a
from state s to sâ€™, and ğ‘‡ is the transition probability at time t from
state s to sâ€™: ğ‘‡ğ‘ (ğ‘ , ğ‘  â€²) = ğ‘ƒğ‘Ÿ (ğ‘ ğ‘¡ +1 = ğ‘  â€²|ğ‘ ğ‘¡ = ğ‘ , ğ‘ğ‘¡ = ğ‘) [51]. The goal of
RL training is for its agent to learn an optimal policy to maximize
its reward function. In this work, we formulate the tuple as follows:

(1) State ğ‘†: Current visiting call site
(2) Action ğ´: Defines a bool variable whether or not to inline

the call site

(3) Transition ğ‘‡ : A deterministic function in our context which
updates the call graph upon taking an action over the current
call site and switching to visit the next call site

(4) Reward ğ‘…: In this work, it is defined as the function execution

speedup with respect to the baseline

As stated in MLGO [54], training the original method for Speed
has difficulties for runtime measurement of a large body of bench-
marks and a more complex problem with respect to the training

Figure 1: MLGOPerf Highlevel Flow

the first candidate among compiler passes for directly increasing the
performance of running codes [44, 46, 52], we believe there is merit
in designing a flow in which the ML-Inliner targets a performance
gain when decides whether or not to inline a call site. An added
benefit of this approach is the new opportunities proper function
inlining can provide for subsequent compiler passes within the O3
pipeline.

There are three major challenges and limitations to achieving
such behavior: (1) The current Reinforcement Learning (RL) infras-
tructure designed to reproduce MLGO 3 only tackles the reduction
of the code size of a function and we need to adapt the methodology
to optimize performance instead. (2) It relies on a set of rewards
to guide the training of its policy trajectory[47]. (3) training such
RL agent requires thousands of evaluations or observations and at
each iteration, a reward value is needed to learn the profitability
of the action. Therefore, using the actual execution time cannot
be a practical solution; this is especially a problem for real-world
applications, e.g., SPEC, which takes tens of minutes per round of
execution. To this end, we design a second ML model, i.e., IR2Perf,
to help alleviate the above challenges. Figure 1 depicts the system-
atic view of the interaction between the two ML models in our
work.

3.1 IR2Perf to the Rescue
At the granularity of call sites, ML-Inliner decides whether or not
to inline the function body of the callee() into the caller()â€™s. It
does so by using the existing Inference path under MLInlineAdvisor:
InlineAdvisor. It collects a number of features, including Callee-
BasicBlockCount, CallSiteHeight, NodeCount, etc., to provide an
inference to the advisor. However, in order to train such a model,
one requires rewards to be fed, at each iteration of its training
pipeline, to guide the trajectory of its policy decisions when it takes
an action[54]. We provide IR2Perf model at the granularity of Func-
tion level for our caller() to predict the speedup of the whole
function as a proxy to generate rewards for the RL model.

3https://github.com/google/ml-compiler-opt

3

Figure 2: MLGOPerf Three Phases
4

source.cppInlinersource.oopt -O3 pipelineSubsequent passes of -O3Callee()  FTsRL TrainerTraining Model Inline  Callsite?RewardIR2Perf Inference Caller() FTssource.cppInlinersource.oopt -O3 pipelineSubsequent passes of -O3Callee()  FTsRL ReleaseDeployed ModelInline  Callsite?Y/NMLGOPerf::RLTrainingMLGOPerf::DeploymentMLGOPerf::IR2PerfTrainingsource.cppInlinersource.oopt -O3 pipelineSubsequent passes of -O3Caller()  FTsIR2Perf  Model CollectFTsFunction FeaturesMeasurementsBinary ExecutionAutotunerDataPreprocessingFeedback/StatsDeep RegressionModel Finegrained Tuning at Inlining CallsitesExperiment StatsCollectFTsFunction Features Algorithm 1 Training Inliner RL Model using IR2Perf

1: procedure FunctionSpeedup
2:

for Function f in Module do
ğ¹ğ‘‡ğ‘  â† getFunctionFeatures()
ğ‘“ ğ‘¢ğ‘›ğ‘ğ‘…ğ‘’ğ‘¤ğ‘ğ‘Ÿğ‘‘ â† ğ‘–ğ‘›ğ‘“ ğ‘’ğ‘Ÿ (ğ¹ğ‘‡ğ‘ )
ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ğ‘…ğ‘’ğ‘¤ğ‘ğ‘Ÿğ‘‘ â† ğ‘ğ‘ğ‘ğ‘’ğ‘›ğ‘‘ (ğ‘“ ğ‘¢ğ‘›ğ‘ğ‘…ğ‘’ğ‘¤ğ‘ğ‘Ÿğ‘‘)

âŠ² IR2Perf Inference

3:

4:

5:

6:

12:

13:

14:

15:

16:

end for
return totalReward

7:
8: end procedure
9:
10: procedure CallsiteInline
11:

initialize policy ğœ‹ğ›¾ randomly
for iteration i in Training do

ğ‘ ğ‘– â† ğ‘†ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘ (0,ğ¼ ) (ğ‘‡ğ‘Ÿğ‘ğ‘–ğ‘›ğ‘–ğ‘›ğ‘”ğ·ğ‘ğ‘¡ğ‘)
Compile and Get IR with policy ğœ‹ğ›¾ +ğœğ‘ ğ‘–
ğ‘… â† ğ¹ğ‘¢ğ‘›ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘†ğ‘ğ‘’ğ‘’ğ‘‘ğ‘¢ğ‘ (ğ‘€ğ‘œğ‘‘ğ‘¢ğ‘™ğ‘’)
Update policy ğ›¾ for using Equation 1

âŠ² RL Model Training

end for

17:
18: end procedure

for code size. Unlike a code size reduction objective, we cannot
decouple performance by means of a Commutative and Associative
properties in an optimization exploration strategy. Therefore, we
plan to address this issue by leveraging IR2Perf. There are a number
of methods to train such RL agent which uses Proximal Policy Opti-
mization (PPO) [47] algorithm. Algorithm 1 provides an insight into
the training flow for which we update the policy of our RL agent
using Equation 1. Similar to MLGO, we rely on total reward as the
summation of all the rewards generated using IR2Perf. It is worth
noting that using IR2Perf, we could also generate partial rewards
per function at each time ğ‘¡, but we did not follow this method.

ğ›¾ = ğ›¾ + ğ›¼

1
ğ‘›

ğ‘–=1{Î£ğ‘¡ğ‘›
Î£ğ‘›

ğ‘¡ =0ğ‘…ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ğ‘– âˆ‡ğ›¾ğ‘™ğ‘œğ‘”ğœ‹ğ›¾ (ğ‘ğ‘–,ğ‘¡ |ğ‘ ğ‘–,ğ‘¡ )}

(1)

where ğ›¼ is the learning rate at which the policy ğ›¾ is updated. The
RL agent tries to maximize the total reward of its policy when
receiving from IR2Perf when it applies an action (ğ‘ğ‘–,ğ‘¡ |ğ‘ ğ‘–,ğ‘¡ ).

3.2.3 MLGOPerf Deployment. After our RL model was trained for
sufficient iterations [35]. we unplug IR2Perf and build LLVM with
the pretrained RL model. The point at which we stop the training
can be determined by occasional evaluation of the performance
of the RL model, observing the trajectory of the loss function, or,
by means of a training budget constraint, i.e., number of iterations
or allocated time. This step is similar to the Release Mode step in
MLGO [54].

4 EXPERIMENTAL RESULTS
In this section, we provide the experimental results of our proposed
work. From the perspective of Compiler Engineers and in order
to fine-tune the quality of the predictions, one has to reproduce
the first two phases stated in Section 3.2 and, from the userâ€™s per-
spective, the third and the final phase is of relevance where we
deploy the pretrained model and build it together with LLVM in-
frastructure. At deployment, there is no need to leverage neither
IR2Perf nor RL Model since IR2Perf is no longer needed to generate

5

rewards, and also, the RL model can be compiled and built AOT
(Ahead Of Time) using saved_model_cli tool 4.

4.1 IR2Perf Model
As we discussed in Section 3.2.1, we develop a pass and register
it subsequent to the Inline optimization in O3 pipeline to collect
the handcrafted features corresponding to a function of interest. In
recent years, there have been a number of approaches [2, 10, 16]
proposed to provide an embedding of the Intermediate Representa-
tion (IR) of the program for which we could have used but instead,
in this work, we decided to directly characterize the space using
static features of IR and we show them in Table 2. The overhead
caused by collecting these features is negligible.

We collect a total of 370K different inlining configurations from
10 SPEC CPU2006 benchmarks using our autotuner for which we
tuned the inlining parameter at the granularity of call site. For
practicality, we used the train dataset for SPEC 2006 as opposed
to the ref dataset; this limits the runtime of our training samples
to be within a minute each on average with a few, i.e., 403.gcc,
462.libquantum, etc., being on the shorter execution time range.
Note that the evaluations in Section 4.4 are still performed with the
ref dataset. This process takes around 5 days. Execution time of a
function call can be too short to allow for a reliable measurement
and to this end, we exclude speedup values higher than our exclu-
sion threshold. In this work, we use 3Ã— as the exclusion threshold
hyperparameter. Additionally, we filter out recorded function over-
heads of less than than 1% by Perf to avoid a noisy estimate in our
training data.

One challenge here is to measure the exact CPU time the program
takes on every call of a certain function. We estimate this with the
following:

ğ¹ğ‘¢ğ‘›ğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘¡ğ‘–ğ‘šğ‘’ =

(Total Program Runtime) âˆ— ğ‘‡ğ¹ğ‘¢ğ‘›ğ‘
ğ‘ğ¹ğ‘¢ğ‘›ğ‘

(2)

where ğ‘‡ğ¹ğ‘¢ğ‘›ğ‘ is the percentage of time the program spent on this
function and ğ‘ğ¹ğ‘¢ğ‘›ğ‘ the number of times this function was called
during the execution.

Subsequently, we define the function speedup as the true label

for IR2Perf model as:

ğ¹ğ‘¢ğ‘›ğ‘ğ‘ ğ‘ğ‘’ğ‘’ğ‘‘ğ‘¢ğ‘ =

ğ¹ğ‘¢ğ‘›ğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘¡ğ‘–ğ‘šğ‘’ (ğµğ‘ğ‘ ğ‘’)
ğ¹ğ‘¢ğ‘›ğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘¡ğ‘–ğ‘šğ‘’ (ğ‘‹ )

(3)

where ğ¹ğ‘¢ğ‘›ğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘¡ğ‘–ğ‘šğ‘’ (ğµğ‘ğ‘ ğ‘’) is the execution time of the function with
no inlining configuration and ğ¹ğ‘¢ğ‘›ğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘¡ğ‘–ğ‘šğ‘’ (ğ‘‹ ) , the same function
having an inlining configuration of X in the set of all inlining
configurations generated by our autotuner. Total program runtime
and the ğ‘‡ğ¹ğ‘¢ğ‘›ğ‘ are collected with Linux perf and ğ‘ğ¹ğ‘¢ğ‘›ğ‘ is collected
with profiling instrumentation and llvm-profdata tool. In this
work, all runtime measurements are done single-threaded, and we
use numactl tool to bind the workloads to a unique CPU core. After
each measurement run, we flush the system page cache to avoid
any perturbance into the collection of our training data.

The goal is to include diverse and meaningful inlining configura-
tions. Each tuning iteration provides us, depending on the number
of call sites, a number of data points which can be added to our

4https://developers.googleblog.com/2017/03/xla-tensorflow-compiled.html

Table 2: IR2Perf Model Features

Table 3: IR2Perf Model Architecture

Static Feature Name
InstructionPerBlock
SuccessorPerBlock
AvgNestedLoopLevel
InstrPerLoop
BlockWithMultipleSuccecorsPerLoop
CallsNo
IsLocal

No
1
2
3
4
5
6
7
8 MaxLoopDepth
9 MaxDomTreeLevel
CallerHeight
10
CallUsage
11
12
IsRecursive
13 NumCallsiteInLoop
14
EntryBlockFreq
15 MaxCallsiteBlockFreq
16 NoOfInstructions=â€™Retâ€™
17 NoOfInstructions=â€™fmulâ€™
18 NoOfInstructions=â€™fdivâ€™
19 NoOfInstructions=â€™faddâ€™
20 NoOfInstructions=â€™fsubâ€™

training data. Many of the function characteristics are correlated to
one another in complex ways and also, with the target metric, i.e.,
function speedup. Thus, we perform a preprocessing stage to better
represent the data for training our model. It is well-known that the
optimal function inlining problem has an exponential increase in
optimization space [6, 52]. Therefore, generating sufficient training
data which captures as many permutations of combinations of in-
linig parameters still remains a pain point. We apply a dimension
reduction of features space since it is experimentally observed as
the feature space increases, so does the average distance between
points. Multidimensional datasets, similar to ours, are subject to
rarity [29] and to this end, we employ Principal Component Analy-
sis (PCA) setting PC to 7 to reduce the dimensionality of our feature
space while preserving the information of our training data with
minimal loss [20].

The preprocessing is done as the following: (0) We compute the
global and function speedups of each inlining configuration against
its baseline, i.e, O3 (1) We then remove redundant data points from
the training data at this stage and normalize the dataset, followed by
(3) applying PCA and additionally, as a standard practice, we store
the objects of our feature scaling and PCA process to transform our
test data into the same scale determined by the training set used to
train IR2Perf.

IR2Perf is a Regression model for which there are four linear
fully-connected layers followed by an activation function. Here, we
use Leaky RelU rather than ReLU as we experimentally observe a
slight benefit in the accuracy of the model.

4.2 IR2Perf Accuracy
The training is done using leave-one-out cross-validation for which
we exclude one benchmark from the training data and use it later

Layer
No

1

2

3

4

Layer (type)

Output Shape

Linear-FC1
Leaky_Relu-1
Linear-FC2
Leaky_Relu-2
Linear-FC3
Leaky_Relu-3
Linear-FC4

[-1, 1, 128]
[-1, 1, 128]
[-1, 1, 256]
[-1, 1, 256]
[-1, 1, 32]
[-1, 1, 32]
[-1, 1, 1]

Table 4: IR2Perf Cross Validation Accuracy

NO. Benchmark

401_bzip2
456_hmmer
462_libquantum
464_h264ref
445_gobmk
470_lbm
458_sjeng
429_mcf
433_milc
482_sphinx3

1
2
3
4
5
6
7
8
9
10
Geometric Mean

Prediction
Error
(MSE)
9.1%
9.5%
15.7%
19.1%
17.5%
9.8%
13.5%
12.2%
13.9%
11.2%
12.8%

as our test data. This ensures no data leakage occurs. Table 4 rep-
resents the accuracy of IR2Perf for each benchmark. In this work,
we use Mean Squared Error (MSE) loss function and we observe
that on average, IR2Perf achieved an error rate of 12.8% among the
SPEC CPU2006 benchmarks with a best 9.1% on 401.bzip2. Figure
3 depicts a number of IR2Perf ML accuracy evaluations. Specifi-
cally, Figure 3a represents the loss function of the first epoch of our
training. As can be seen, the model converges rapidly with a few
iterations of batches. Figure 3 showcases the training graph when
401.bzip2 was excluded from the training data. Subsequently, Fig-
ure 3c shows the prediction accuracy after the model was tested
on 401.bzip2. Finally, Figure 3d depicts the prediction accuracy
of IR2Perf, i.e., 19.1% when 464.h264ref was excluded from the
training set (training figure is not shown here for conciseness).
The prediction accuracy of IR2Perf is established by the process
mentioned above and we use the pretrained model with the lowest
error-rate found by means of cross-validation. We chose SPEC as
the main suite for data collection as it provides real-world com-
plex function-level code segments which can benefit IR2Perf to
characterize the behaviour of inlining.

4.3 MLGOPerf Training
Once the efficient accuracy of IR2Perf is established, we infer from
it to generate rewards for our RL agent trainer. In this stage, i.e.,
phase 2 of Figure 2, we deploy IR2Perf into MLGOPerf::RLTraining
pipeline, and given an incoming function in a module, we collect

6

(a) Training Loss
(First epoch)

(b) Training Accuracy
(401.bzip2 was held out)

(c) Prediction Accuracy of
401.bzip2 hold-out on 3b

(d) Prediction Accuracy of
464.h264ref as hold-out
(training figure not shown)

Figure 3: IR2Perf Model Evaluation

the callerâ€™s function feature to be fed as input to IR2Perf. This will
guide the RL agentâ€™s policy as to how well an inlining configuration
potentially performs given its callee function representation which
we also collect and feed into the trainer. In the original version of
MLGO [54], authors propose 11 callee features to be used with the
RL trainer 5, we add two more relevant features we believe have
added representational value, namely (1) Block Frequency and (2)
Loop Level. Our early results show benefits to the final MLGOPerf
model when we employ our two handcrafted features to the mix
by around 1%. These two will be used together with the previously
introduced 11 features and they characterize the caller function for
the RL agent.

We train the RL agent using the default hyperparameters men-
tioned in the original MLGO work, except enabling rewards to be
normalized as suggested by [47]. Rapidly generating rewards using
IR2Perf, training the RL agent takes around 4 days using an NVIDIA
Tesla P100 PCIe 12GB and an Intel(R) Xeon(R) CPU E7-8890 v4 @
2.20GHz on Linux 18.04.1 LTS when trained on module samples
taken from SPEC CPU2006. Note that IR2Perf reward generator is
not limited to any benchmark it was trained on or cross-validated
with, as it can liberally infer the function speedup of any unseen
function when only fed the IR features defined under Table 2 and
thus, we successfully eliminate the need to collect any runtime
metrics, i.e, execution time, and to largely accelerate the training
process.

4.4 MLGOPerf Deployment
We deploy the MLGOPerf under the third and final phase of our
proposed work. Similar to MLGO, the deployed model when com-
piled AOT is emitted as a library which can be invoked when we
feed as input the 13 callee features of any unseen applications.
Note that at this phase, we no longer need to use IR2Perf as it has
already done its job to help train the RL agent. The overhead of
collecting these callee features is minimal and MLInliner frame-
work can leverage the decision made by the RL model to decide
whether or not to inline a call site to optimize the performance of
an application under analysis. Similar to Section 4.1, all run time
measurements are done single-threaded, and we use numactl tool
to bind the workloads to a unique CPU core and we made sure we

5Specifically, we use LLVM 12 as at commit 1dad9d42 and MLGO Github repository as
at commit dac1b149

flush the system page cache after every measurement to avoid any
protuberance into the training data. The experiments are measured
on an ARMv8.2-A (Kunpeng 920) architecture @ 2.6GHz on Linux.
We run each benchmark five times, having flushed the system page
cache after every run, and we use a trimmed mean method to drop
the minimum and the maximum measurements and perform an
average of the remaining three measurements. Additionally, we
collect the variance between the measurements and we make sure
to repeat the measurements for every instance of evaluation having
a variance of more than 2%.

4.5 Performance Improvement
Table 5 shows our experimental results using SPEC CPU2006 with
the ref dataset (Table 5a) and Cbench (Table 5b). As can be seen,
MLGOPerf on average achieves 1.8% and 2.2% speedup against
LLVMâ€™s O3 on SPEC CPU2006 and Cbench, respectively. Addition-
ally, we compare the performance of MLGOPerf with respect to
MLGO and on average it outperforms MLGO by 3.1% and 4.1% on
SPEC2006 CPU and Cbench, respectively. The average variance
between the runs is measured to be around 0.3% to 0.4% on SPEC
and 0.43% to 0.86% on Cbench. As expected, there is a slight increase
in the code size of the optimized binaries MLGOPerf generates, and
that is measured as 12% and 16% against LLVMâ€™s O3 and MLGO on
Cbench and 17.8% and 23.4% for SPEC CPU2006. MLGOPerfâ€™s is
trained to optimize performance as its objective and it is reasonable
to observe an increase in the code size, especially compared with
MLGO which was solely trained to optimize code size.

4.6 Performance Enablement with Autotuning
As discussed earlier, an added benefit of utilizing an optimized func-
tion inlining pass is the enablement of tunable opportunities for
subsequent passes which may translate into an increased perfor-
mance. In this section, we experimentally explore this phenomenon
by designing an autotuning methodology to detect whether or
not MLGOPerf provides an increased code-region opportunity to
the code segments of interest and that if it can lead to finding
more optimal configurations. We leverage our autotuner to explore

7

Table 5: Performance improvement against LLVMâ€™s O3 and MLGO

Benchmark

401.bzip2
403.gcc
429.mcf
445.gobmk
456.hmmer
458.sjeng
462.libquantum
464.h264ref
471.omnetpp
433.milc
444.namd
453.povray
470.lbm
482.sphinx3
Geomean

Speedup
wrt O3
1.052
1.022
1.009
1.030
0.997
1.003
1.040
1.068
1.004
1.021
0.992
0.997
1.020
0.993
1.018

(a) SPEC CPU2006 (w/ ref dataset) Performance Improvement

Measured
Variance
0.966
0.921
1.021
0.249
0.040
0.354
1.856
0.620
1.107
0.566
0.530
0.416
0.025
0.676
0.434%

Speedup
wrt MLGO
1.072
1.054
1.031
1.044
1.020
1.040
1.051
1.088
0.999
0.999
1.015
1.035
1.004
0.992
1.031

Measured
Variance
0.594
0.004
1.242
0.135
0.077
0.031
0.029
0.782
1.091
0.486
0.016
0.022
0.005
0.070
0.086%

Code size
Increase wrt O3
1.131
1.227
1.047
1.097
1.227
1.318
1.257
1.389
1.146
1.297
1.002
1.237
1.025
1.167
1.178

Code size
Increase wrt MLGO
1.248
1.411
1.077
1.104
1.273
1.373
1.428
1.312
1.198
1.276
1.018
1.418
1.031
1.225
1.235

(b) Cbench Performance Improvement

Benchmark

automotive_bitcount
automotive_qsort1
automotive_susan_c
automotive_susan_e
automotive_susan_s
bzip2d
bzip2e
consumer_jpeg_c
consumer_jpeg_d
consumer_lame
consumer_tiff2bw
consumer_tiff2rgba
consumer_tiffdither
consumer_tiffmedian
network_dijkstra
network_patricia
office_stringsearch1
security_blowfish_d
security_blowfish_e
security_rijndael_d
security_rijndael_e
security_sha
telecom_adpcm_c
telecom_adpcm_d
telecom_CRC32
Geomean

Speedup
wrt O3
1.002
1.000
1.020
1.015
1.002
1.025
1.014
1.002
1.041
1.006
1.027
1.051
1.006
1.012
1.005
1.000
0.996
1.005
1.059
1.042
1.037
1.134
1.001
1.001
1.065
1.022

Measured
Variance
0.18%
0.14%
0.32%
0.28%
0.29%
0.70%
0.53%
0.41%
0.22%
0.13%
0.30%
0.38%
0.20%
0.12%
1.20%
0.35%
0.50%
0.75%
0.13%
0.64%
0.07%
1.63%
0.13%
0.15%
0.44%
0.3%

Speedup
wrt MLGO
1.005
1.003
1.122
1.044
1.008
1.012
1.041
1.005
1.335
1.007
1.023
1.110
1.014
1.056
0.991
0.999
1.000
1.002
1.029
1.039
1.035
1.132
1.001
1.000
1.066
1.041

8

Measured
Variance
0.17%
0.12%
1.02%
1.71%
0.23%
0.70%
0.53%
0.37%
1.36%
0.10%
0.32%
1.71%
0.11%
0.16%
1.30%
0.29%
0.91%
1.73%
1.80%
0.21%
0.13%
0.09%
0.21%
0.19%
0.57%
0.4%

Code size
Increase wrt O3
1.000
1.000
1.349
1.349
1.349
1.173
1.113
1.045
1.037
1.181
1.298
1.294
1.190
1.048
1.000
1.007
1.000
1.000
1.412
1.412
1.009
1.009
0.996
1.000
1.000
1.121

Code size
Increase wrt MLGO
1.000
1.000
1.567
1.567
1.567
1.239
1.189
1.200
1.198
1.137
1.356
1.351
1.146
1.048
1.000
1.052
1.000
1.000
1.390
1.390
1.009
1.009
0.996
1.000
1.000
1.161

Table 6: MLGOPerf Enhanced Autotuning Code Region Opportunities

Cbench

automotive_bitcount
automotive_qsort1
automotive_susan_c
automotive_susan_e
automotive_susan_s
bzip2d
bzip2e
consumer_jpeg_c
consumer_jpeg_d
consumer_tiff2bw
consumer_tiff2rgba
consumer_tiffdither
consumer_tiffmedian
network_dijkstra
network_patricia
office_rsynth
security_blowfish_d
security_blowfish_e
security_pgp_d
security_pgp_e
security_rijndael_d
security_rijndael_e
security_sha
telecom_adpcm_c
telecom_adpcm_d
telecom_CRC32
telecom_gsm
Geomean

O3
Autotuning
Speedup

1.027714
1.009412
1.038951
1.031977
1.001988
1.15753
1.032093
1.040332
1.031342
1.004812
1.047902
1.012297
0.973255
1.078947
1.015152
0.998958
1.001764
1.001314
1.019659
1.039591
1.040965
1.018811
1.009674
1.006329
1.039636
1.003663
1.010018
1.025198

MLGO

MLGOPerf

Tunable
Regions

Autotuning
Speedup

Tunable
Regions

Autotuning
Speedup

20
32
116
116
116
637
637
1049
1074
641
633
640
741
13
12
152
18
18
955
955
22
22
10
7
7
4
115
â€”

1.019832
1.008607
1.036704
1.026087
1.065891
1.100431
1.026258
1.017417
1.014804
1.018229
1.122697
1.004719
1.001938
1.087719
1.015152
1.001032
1.000441
1.002632
1.017919
1.038804
1.048175
1.02481
1.004434
1.004211
1.039636
1.001217
1.009991
1.027676

20
32
112
112
112
580
580
891
885
619
611
614
715
13
12
147
18
18
929
929
22
22
10
7
7
4
112
â€”

1.02781
1.009123
1.037121
1.033349
1.146078
1.165478
1.033333
1.043764
1.017778
1.017452
1.123338
1.007853
0.988845
1.061404
1.008772
1.018398
0.998679
1.001314
1.036332
1.04023
1.04694
1.029064
1.101781
1.002101
1.038986
1.003663
1.007286
1.037887

Tunable Regions
wrt
O3
1.000
1.000
1.621
1.621
1.621
1.173
1.173
1.148
1.069
1.409
1.433
1.409
1.443
1.692
1.000
1.007
1.000
1.000
1.379
1.379
1.136
1.136
1.300
1.000
1.000
1.250
1.026
1.218

Tunable
Regions
20
32
188
188
188
747
747
1204
1148
903
907
902
1069
22
12
153
18
18
1317
1317
25
25
13
7
7
5
118
â€”

wrt
MLGO
1.000
1.000
1.679
1.679
1.679
1.288
1.288
1.351
1.297
1.459
1.484
1.469
1.495
1.692
1.000
1.041
1.000
1.000
1.418
1.418
1.136
1.136
1.300
1.000
1.000
1.250
1.054
1.260

code-region opportunities for loop-unroll and loop-vectorize
by tuning unroll-count âˆˆ {0, 2, 4, 8} and interleave-count âˆˆ {1, 2, 4}.
Iteratively, our autotuner uses the list of opportunities to deter-
mine optimal configurations by using different search techniques.
Then, the compiler uses the configurations suggested by the au-
totuner and generates a new executable file. We run the emitted
executable and provide the execution times to the autotuner as
feedback which will be leveraged by the autotuner to generate a
new set of configurations based on the feedback. We repeat this
tuning process until the stop criteria are satisfied. For practicality,
we set the iteration number to 120 per benchmark and we showcase
the results for Cbench. Similar to the other experimental results in
this work, we used a trimmed mean of runs per benchmark at each
iteration. The geometric mean of the measured variance between
the runs is recorded at 0.164%.

Table 6 presents this evaluation. We report O3, MLGO, and our
proposed work in the scenario where we start by tuning the avail-
able parameters and record the number of code region opportu-
nities the autotuner finds suitable to tune. Columns Autotuning
Speedup represent the best found configuration at the end of the

tuning given our budget. The speedup values are all normalized
to LLVMâ€™s O3 and it reveals the following. (1) MLGOPerf enables
enhanced autotuning opportunities for subsequent passes. These
values are reported under the final two columns as MLGOPerf pro-
vides an increased number of up to 21.8% and 26% for tunable code
regions against LLVMâ€™s O3 and MLGO, respectively. (2) Under-
standably, not all enhanced opportunities will translate into higher
performance values. As the autotuner explores the optimization
space, many of the combinations do not lead to any better outcome,
however, we experimentally observe that using MLGOPerf, the rate
at which the former translates into higher speedup values is higher.
These values are 2.5%, 2.7%, and 3.7% for LLVMâ€™s O3, MLGO, and
MLGOPerf, respectively.

5 DISCUSSION
MLGOPerf is the first step towards an ML-guided type of compiler
optimizations targeting performance and although it provides bene-
fits to the function inlining and its subsequent optimization passes,
it is still in its infancy and we are distant from an ideal standalone
compiler pass that derives optimal decisions using ML. Function

9

inlining is a unique optimization which can easily slow down the
performance of an executable; no inlining or aggressive inlining
both can hurt the performance of a running code and there is a
fine line to walk through between the two extremes. There are a
number of challenges and shortcomings we would like to mention
here.

5.1 Challenges

Function vs. Global speedup. MLGOPerf uses the predicted function

speedup values of IR2Perf as a proxy to guide the training of its
RL agent towards an optimal function inlining that benefits the
global speedup of program. However, we experimentally notice
that on 15% of the cases in our training data, these two metrics
are contracting one another in a sense that an increase in function
speedup led to a decrease in global speedup. This is a common
challenge of optimizing applications using a finer-grained perfor-
mance metric for which we eluded to in Section 3.2.2 regarding
performance decoupling. There are several factors involved in this
phenomenon, i.e., cache hierarchy, memory-bound workloads, hard-
ware microarchitecture design, etc., and are outside the scope of
this work [52].

Multi-objective optimization. MLGOPerf attempts to optimize the
performance of a code segment with intelligent inlining decisions
derived by ML. However, in a number of cases, we experimentally
observe that the emitted code size is also increased. Although this is
outside the scope of the current work, an enhanced function inlining
may also benefit from taking into account both of the objectives in
its exploration strategy by identifying the Pareto optimality [49].
Similar strategies are employed for a number of adjacent problems;
i.e, Multicore Embedded Systems [4, 50] and Compiler autotuning
[9, 28, 37].

Compiler optimization space. It is an inherently difficult problem
to identify the optimal set of optimization passes given a code seg-
ment. There are a plethora of permutations of optimizations which
can be applied to increase the performance of a running code and
the problem quickly becomes an NP-hard problem [6]. Additionally,
the optimization space is unbounded as there is no decision bound-
ary for the length of an optimization sequence one can add to the
optimization pipeline. Similar to Halting problem [12], a general
algorithm to solve the selection and the phase-ordering problem
for all possible program-input pairs cannot exist.

Compiler performance prediction. Another fundamentally diffi-
cult problem with MLGOPerf and any other performance prediction
framework is the fact that predicting the CPU time/cycles are archi-
tecture and compiler dependent. There are many unknown factors in
place to affect the performance of a binary and at the same time, we
always, at best, measure a noisy estimate of the performance met-
rics and for these reasons, predicting the performance or speedup
remains a difficult problem.

Software characterization. MLGOPerf leverages our handcrafted
features to characterize a segment of a code. However, this process
has an unwanted discretization noise which can be reduced by
means of finer-grained embedding which is outside the scope of
this work. We acknowledge that one of the toughest challenges

in the compiler space is the lack of an ideal characterization of
software in a way that ML models can be applied and we are no
exception here.

6 CONCLUSION
In this paper, we presented MLGOPerf, the first end-to-end frame-
work capable of optimizing performance using ML-Inliner. We
experimentally demonstrated that using MLGOPerf, we are able to
achieve up to 1.8% and 2.5% performance speedup over LLVMâ€™s O3
on SPEC CPU 2006 and Cbench while expanding the horizons of
code-regions opportunities for subsequent passes up to 26% which
can add further 3.7% speedup to the emitted binary at the end of
the O3 pipeline. Future works will be focused around generalizing
a flow for which other compiler optimizations can be tackled in a
seamless manner.

REFERENCES
[1] Felix Agakov, Edwin Bonilla, John Cavazos, BjÃ¶rn Franke, Grigori Fursin,
Michael FP Oâ€™Boyle, John Thomson, Marc Toussaint, and Christopher KI Williams.
2006. Using machine learning to focus iterative optimization. In Proceedings of the
International Symposium on Code Generation and Optimization. IEEE Computer
Society, 295â€“305.

[2] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2019. code2vec: Learn-
ing distributed representations of code. Proceedings of the ACM on Programming
Languages 3, POPL (2019), 1â€“29.

[3] Jason Ansel, Shoaib Kamil, Kalyan Veeramachaneni, Jonathan Ragan-Kelley,
Jeffrey Bosboom, Una-May Oâ€™Reilly, and Saman Amarasinghe. 2014. Opentuner:
An extensible framework for program autotuning. In Proceedings of the 23rd
international conference on Parallel architectures and compilation. 303â€“316.
[4] G. Ascia, V. Catania, M. Palesi, and D. Patti. Jan.. A system-level framework for
evaluating area/performance/power trade-offs of VLIW-based embedded systems.
In Design Automation Conference, 2005. Proceedings of the ASP-DAC 2005. Asia
and South Pacific, Vol. 2. 940â€“943 Vol. 2.

[5] Amir H. Ashouri, Andrea Bignoli, Gianluca Palermo, Cristina Silvano, Sameer
Kulkarni, and John Cavazos. 2017. MiCOMP: Mitigating the Compiler Phase-
Ordering Problem Using Optimization Sub-Sequences and Machine Learning.
ACM Trans. Archit. Code Optim. 14, 3, Article 29 (Sept. 2017), 28 pages. https:
//doi.org/10.1145/3124452

[6] Amir H. Ashouri, William Killian, John Cavazos, Gianluca Palermo, and Cristina
Silvano. 2018. A survey on compiler autotuning using machine learning. ACM
Computing Surveys (CSUR) 51, 5 (2018), 1â€“42. https://doi.org/10.1145/3197978
[7] Amir Hossein Ashouri, Giovanni Mariani, Gianluca Palermo, Eunjung Park, John
Cavazos, and Cristina Silvano. 2016. COBAYN: Compiler Autotuning Framework
Using Bayesian Networks. ACM Trans. Archit. Code Optim. (TACO) 13, 2, Article
21 (June 2016), 25 pages. https://doi.org/10.1145/2928270

[8] Amir H. Ashouri, Gianluca Palermo, John Cavazos, and Cristina Silvano. 2018.
Automatic Tuning of Compilers Using Machine Learning. Springer. https://doi.
org/10.1007/978-3-319-71489-9

[9] Amir Hossein Ashouri, Vittorio Zaccaria, Sotirios Xydis, Gianluca Palermo, and
Cristina Silvano. 2013. A framework for Compiler Level statistical analysis over
customized VLIW architecture. In VLSI-SoC. 124â€“129. https://doi.org/10.1109/
VLSI-SoC.2013.6673262

[10] Tal Ben-Nun, Alice Shoshana Jakobovits, and Torsten Hoefler. 2018. Neural
code comprehension: A learnable representation of code semantics. Advances in
Neural Information Processing Systems 31 (2018).

[11] FranÃ§ois Bodin, Toru Kisuki, Peter Knijnenburg, Mike Oâ€™Boyle, and Erven Rohou.
1998. Iterative compilation in a non-linear optimisation space. In Workshop on
Profile and Feedback-Directed Compilation.

[12] Leslie Burkholder. 1987. The halting problem. ACM SIGACT News 18, 3 (1987),

48â€“60.

[13] Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A scalable tree boosting system.
In Proceedings of the 22nd acm sigkdd international conference on knowledge
discovery and data mining. 785â€“794.

[14] Yang Chen, Shuangde Fang, Yuanjie Huang, Lieven Eeckhout, Grigori Fursin,
Olivier Temam, and Chengyong Wu. 2012. Deconstructing iterative optimization.
ACM Transactions on Architecture and Code Optimization (TACO) 9, 3 (2012), 21.
[15] KD Cooper, PJ Schielke, and D Subramanian. 1999. Optimizing for reduced
code space using genetic algorithms. ACM SIGPLAN Notices (1999).
http:
//dl.acm.org/citation.cfm?id=314414

10

abs/1801.06378

[38] Eliot Moss, Paul Utgoff, John Cavazos, Doina Precup, D Stefanovic, Carla Brodley,
and David Scheeff. 1998. Learning to schedule straight-line code. Advances in
Neural Information Processing Systems 10 (1998), 929â€“935. http://books.nips.cc/
papers/files/nips10/0929.pdf

[39] Ricardo Nobre, LuÃ­s Reis, and JoÃ£o M. P. Cardoso. 2018. Impact of Compiler Phase
Ordering When Targeting GPUs. In Euro-Par 2017: Parallel Processing Workshops,
Dora B. Heras and Luc BougÃ© (Eds.). Springer International Publishing, Cham,
427â€“438.

[40] E Park, J Cavazos, and MA Alvarez. 2012. Using graph-based program character-
ization for predictive modeling. Proceedings of the International Symposium on
Code Generation and Optimization (2012), 295â€“305. http://dl.acm.org/citation.
cfm?id=2259042

[41] E Park, J Cavazos, and LN Pouchet. 2013. Predictive modeling in a polyhedral
optimization space. International journal of parallel programming (2013), 704â€“750.
http://link.springer.com/article/10.1007/s10766-013-0241-1

[42] Sunghyun Park, Salar Latifi, Yongjun Park, Armand Behroozi, Byungsoo Jeon, and
Scott Mahlke. 2022. SRTuner: Effective Compiler Optimization Customization by
Exposing Synergistic Relations. In 2022 IEEE/ACM International Symposium on
Code Generation and Optimization (CGO). IEEE, 118â€“130.

[43] Phitchaya Mangpo Phothilimthana, Amit Sabne, Nikhil Sarda, Karthik Srini-
vasa Murthy, Yanqi Zhou, Christof Angermueller, Mike Burrows, Sudip Roy,
Ketan Mandke, Rezsa Farahani, et al. 2021. A Flexible Approach to Autotuning
Multi-Pass Machine Learning Compilers. In 2021 30th International Conference
on Parallel Architectures and Compilation Techniques (PACT). IEEE, 1â€“16.
[44] Aleksandar Prokopec, Gilles Duboscq, David Leopoldseder, and Thomas
WÃ¯rthinger. 2019. An optimization-driven incremental inline substitution al-
gorithm for just-in-time compilers. In 2019 IEEE/ACM International Symposium
on Code Generation and Optimization (CGO). IEEE, 164â€“179.

[45] Nadav Rotem and Chris Cummins. 2021. Profile Guided Optimization without
Profiles: A Machine Learning Approach. arXiv preprint arXiv:2112.14679 (2021).
[46] Robert W Scheifler. 1977. An analysis of inline substitution for a structured

programming language. Commun. ACM 20, 9 (1977), 647â€“654.

[47] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347
(2017).

[48] Evan Shelhamer, Parsa Mahmoudieh, Max Argus, and Trevor Darrell. 2016. Loss
is its own reward: Self-supervision for reinforcement learning. arXiv preprint
arXiv:1612.07307 (2016).

[49] Anderson Faustino da Silva, Bernardo NB de Lima, and Fernando Magno QuintÃ£o
Pereira. 2021. Exploring the space of optimization sequences for code-size reduc-
tion: insights and tools. In Proceedings of the 30th ACM SIGPLAN International
Conference on Compiler Construction. 47â€“58.

[50] Cristina Silvano, William Fornaciari, Gianluca Palermo, Vittorio Zaccaria, Fab-
rizio Castro, Marcos Martinez, Sara Bocchio, Roberto Zafalon, Prabhat Avasare,
Geert Vanmeerbeeck, et al. 2011. Multicube: Multi-objective design space ex-
ploration of multi-core architectures. In VLSI 2010 Annual Symposium. Springer,
47â€“63.

[51] Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An intro-

duction. MIT press.

[52] Theodoros Theodoridis, Tobias Grosser, and Zhendong Su. 2022. Understanding
and exploiting optimal function inlining. In Proceedings of the 27th ACM Inter-
national Conference on Architectural Support for Programming Languages and
Operating Systems. 977â€“989.

[53] A Tiwari, C Chen, and J Chame. 2009. A scalable auto-tuning framework for
compiler optimization. In Parallel & Distributed Processing, 2009. IPDPS 2009. IEEE
International Symposium on. 1â€“12. http://ieeexplore.ieee.org/xpls/abs{_}all.jsp?
arnumber=5161054

[54] Mircea Trofin, Yundi Qian, Eugene Brevdo, Zinan Lin, Krzysztof Choromanski,
and David Li. 2021. Mlgo: a machine learning guided compiler optimizations
framework. arXiv preprint arXiv:2101.04808 (2021).

[55] Huanting Wang, Zhanyong Tang, Cheng Zhang, Jiaqi Zhao, Chris Cummins,
Hugh Leather, and Zheng Wang. 2022. Automating reinforcement learning
architecture design for code optimization. In Proceedings of the 31st ACM SIGPLAN
International Conference on Compiler Construction. 129â€“143.

[56] Tom Young, Devamanyu Hazarika, Soujanya Poria, and Erik Cambria. 2018.
Recent trends in deep learning based natural language processing. ieee Computa-
tional intelligenCe magazine 13, 3 (2018), 55â€“75.

[16] Chris Cummins, Zacharias V Fisches, Tal Ben-Nun, Torsten Hoefler, Michael FP
Oâ€™Boyle, and Hugh Leather. 2021. Programl: A graph-based program representa-
tion for data flow analysis and compiler optimizations. In International Conference
on Machine Learning. PMLR, 2244â€“2253.

[17] C. Cummins, P. Petoumenos, Z. Wang, and H. Leather. 2017. End-to-End Deep
Learning of Optimization Heuristics. In 2017 26th International Conference on
Parallel Architectures and Compilation Techniques (PACT). 219â€“232. https://doi.
org/10.1109/PACT.2017.24

[18] Chris Cummins, Bram Wasti, Jiadong Guo, Brandon Cui, Jason Ansel, Sahir
Gomez, Somya Jain, Jia Liu, Olivier Teytaud, Benoit Steiner, et al. 2021. Compiler-
Gym: Robust, Performant Compiler Optimization Environments for AI Research.
arXiv preprint arXiv:2109.08267 (2021).

[19] Dibyendu Das, Shahid Asghar Ahmad, and Venkataramanan Kumar. 2020. Deep
Learning-based Approximate Graph-Coloring Algorithm for Register Allocation.
In 2020 IEEE/ACM 6th Workshop on the LLVM Compiler Infrastructure in HPC
(LLVM-HPC) and Workshop on Hierarchical Parallelism for Exascale Computing
(HiPar). IEEE, 23â€“32.

[20] Gustavo Deco and Dragan Obradovic. 1996. An information-theoretic approach to

neural computing. Springer Science & Business Media.

[21] G Fursin, Y Kashnikov, and AW Memon. 2011. Milepost gcc: Machine learning
enabled self-tuning compiler. International journal of parallel programming 39, 3
(2011), 296â€“327. http://link.springer.com/article/10.1007/s10766-010-0161-2
[22] G Fursin, C Miranda, and O Temam. 2008. MILEPOST GCC: machine learning
based research compiler. GCC Summit (2008). https://hal.inria.fr/inria-00294704/
[23] Grigori Fursin and Olivier Temam. 2009. Collective optimization. In International
Conference on High-Performance Embedded Architectures and Compilers. Springer,
34â€“49.

[24] Kyriakos Georgiou, Craig Blackmore, Samuel Xavier-de Souza, and Kerstin Eder.
2018. Less is More: Exploiting the Standard Compiler Optimization Levels for
Better Performance and Energy Consumption. arXiv preprint arXiv:1802.09845
(2018).

[25] Ameer Haj-Ali, Nesreen K Ahmed, Ted Willke, Yakun Sophia Shao, Krste
Asanovic, and Ion Stoica. 2020. Neurovectorizer: End-to-end vectorization with
deep reinforcement learning. In Proceedings of the 18th ACM/IEEE International
Symposium on Code Generation and Optimization. 242â€“255.

[26] M Hall, D Padua, and K Pingali. 2009. Compiler research: the next 50 years.

Commun. ACM (2009). http://dl.acm.org/citation.cfm?id=1461946

[27] K Hoste and L Eeckhout. 2008. Cole: compiler optimization level exploration.
In Proceedings of the 6th annual IEEE/ACM international symposium on Code
generation and optimization. 165â€“174. http://dl.acm.org/citation.cfm?id=1356080
[28] Kenneth Hoste and Lieven Eeckhout. 2008. Cole: compiler optimization level
exploration. In Proceedings of the 6th annual IEEE/ACM international symposium
on Code generation and optimization. 165â€“174.

[29] Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An

introduction to statistical learning. Vol. 112. Springer.

[30] Michael Kalyan, Xiang Wang, Ahmed Eltantawy, and Yaoqing Gao. 2019. Code
Region Based Auto-Tuning Enabled Compilers. In Workshop on the Intersection of
High Performance Computing and Machine Learning (HPCaML). ACM.

[31] Ksenia Konyushkova, Konrad Zolna, Yusuf Aytar, Alexander Novikov, Scott Reed,
Serkan Cabi, and Nando de Freitas. 2020. Semi-supervised reward learning for
offline reinforcement learning. arXiv preprint arXiv:2012.06899 (2020).

[32] A Koseki. 1997. A method for estimating optimal unrolling times for nested loops.
In Parallel Architectures, Algorithms, and Networks, 1997.(I-SPANâ€™97) Proceedings.,
Third International Symposium on. 376â€“382. http://ieeexplore.ieee.org/xpls/
abs{_}all.jsp?arnumber=645123

[33] S Kulkarni and J Cavazos. 2012. Mitigating the compiler optimization phase-
ordering problem using machine learning. ACM SIGPLAN Notices (2012). http:
//dl.acm.org/citation.cfm?id=2384628

[34] Chris Lattner. 2008. LLVM and Clang: Next generation compiler technology. In

The BSD conference, Vol. 5.

[35] Marlos C Machado, Marc G Bellemare, Erik Talvitie, Joel Veness, Matthew
Hausknecht, and Michael Bowling. 2018. Revisiting the arcade learning en-
vironment: Evaluation protocols and open problems for general agents. Journal
of Artificial Intelligence Research 61 (2018), 523â€“562.

[36] Sandya Mannarswamy and Dibyendu Das. 2022. Learning to Combine Instruc-

tions in LLVM Compiler. arXiv preprint arXiv:2202.12379 (2022).

[37] Thierry Moreau, Anton Lokhmotov, and Grigori Fursin. 2018. Introducing Re-
QuEST: an Open Platform for Reproducible and Quality-Efficient Systems-ML
Tournaments. CoRR abs/1801.06378 (2018). arXiv:1801.06378 http://arxiv.org/

11

