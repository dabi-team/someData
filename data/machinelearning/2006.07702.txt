Noname manuscript No.
(will be inserted by the editor)

Low-Rank Factorization for Rank Minimization with
Nonconvex Regularizers

April Sagan · John E. Mitchell

1
2
0
2

r
a

M
8
2

]

C
O
.
h
t
a
m

[

2
v
2
0
7
7
0
.
6
0
0
2
:
v
i
X
r
a

Received: date / Accepted: date

Abstract Rank minimization is of interest in machine learning applications
such as recommender systems and robust principal component analysis. Min-
imizing the convex relaxation to the rank minimization problem, the nuclear
norm, is an eﬀective technique to solve the problem with strong performance
guarantees. However, nonconvex relaxations have less estimation bias than
the nuclear norm and can more accurately reduce the eﬀect of noise on the
measurements.

We develop eﬃcient algorithms based on iteratively reweighted nuclear
norm schemes, while also utilizing the low rank factorization for semideﬁ-
nite programs put forth by Burer and Monteiro. We prove convergence and
computationally show the advantages over convex relaxations and alternating
minimization methods. Additionally, the computational complexity of each
iteration of our algorithm is on par with other state of the art algorithms, al-
lowing us to quickly ﬁnd solutions to the rank minimization problem for large
matrices.

Keywords Rank Minimization · Matrix Completion · Nonconvex Regulariz-
ers · Semideﬁnite Programming

This work was supported in part by National Science Foundation under Grant Number
DMS-1736326.

A. Sagan
E-mail: aprilsagan1729@gmail.com

J. Mitchell
E-mail: mitchj@rpi.edu

 
 
 
 
 
 
2

1 Introduction

April Sagan, John E. Mitchell

We consider the rank minimization problem with linear constraints formulated
as

min
X∈S n
subject to

rank(X) + φ(X)

A(X) = b

X (cid:23) 0

where S n denotes the set of symmetric n×n matrices, A : S n → Rm is an linear
map, b ∈ Rm is the measurement vector, and φ(X) is an L- smooth function.
A common example is matrix completion, in which the linear constraint is
PΩ(M ) = PΩ(X), where Ω is the set of indices (i, j) of known points in the
matrix, and PΩ : Rm×n → Rm×n is the projection onto the set of matrices
which the entry (i, j) vanishes for all (i, j) /∈ Ω. Formally, we deﬁne PΩ as
(cid:40)
0
Xij

(i, j) /∈ Ω
(i, j) ∈ Ω

PΩ(X)ij =

2 ||PΩ(X − M )||2

Additionally, in the presence of noise, we can penalize the constraint by adding
φ(X) = β
F to the objective function, with a parameter β.
Solving the rank minimization problem directly is impractical due to the rank
function being non-convex and highly discontinuous. In practice, it is common
to instead minimize the convex relaxation to the rank function known as the
nuclear norm, which is deﬁned as the sum of the singular values of the matrix,
or in the case of positive semideﬁnite matrices, the trace.

trace(X)

min
X
subject to A(X) = b

X (cid:23) 0
The nuclear norm, denoted by ||X||∗ = (cid:80)n

i=1 σi(X) where σi(X) is the
ith singular value of X, is the tightest convex relaxation, and in the case of
matrix completion on an n by n matrix known to be at most rank r, it has
been shown to exactly recover the original matrix with high probability if
at least Cnr log(n) entries are observed, for an absolute constant C, under
the assumption that the original matrix satisﬁes the incoherence property [5].
However, minimizing the nuclear norm is not always the best approach. As
observed in the similar problem of l0 norm minimization, the convex relaxation,
the l1 norm, introduces an estimation bias [33]. Consider the following rank
minimization problem:

min
X∈Rm×n

||X||∗ +

β
2

||PΩ( ˜M − X)||2
F

where ˜M is a low rank matrix, M , plus Gaussian noise. As we show in Section
2, the minimizer to the expected value of the nuclear norm regularized formu-
lation is
mn . The bias of this formulation comes from the

pβ+1 M , where p = |Ω|

pβ

Low-Rank Factorization for Rank Minimization with Nonconvex Regularizers

3

nuclear norm not only minimizing the smallest singular values, which corre-
spond to the noise, but also the largest singular values, which correspond to
the signal.

Another common approach to ﬁtting a low rank matrix to a set of mea-
surements is rank constrained optimization, wherein one attempts to ﬁnd a
rank r matrix that minimizes an objective function.

min
X∈Rm×n

||A(X) − b||2 subject to rank(X) = r

The most common approach utilizes the low rank factorization X = U V T for
U ∈ Rm×r and V ∈ Rn×r

min
U ∈Rm×r,V ∈Rn×r

||A(U V T ) − b||2

Because r is typically much smaller than the size of the matrix, this greatly
reduces the number of variables.

In addition to ﬁnding a matrix of a given rank, this technique can be used
in nuclear norm minimization as well [22][18][23]. The nuclear norm can be
characterized as follows:

||X||∗ = min
U,V
subject to

(cid:0)||U ||2

F + ||V ||2
F

(cid:1)

1
2

X = U V T

and so, to minimize a weighted sum of the nuclear norm and a quadratic loss
function, we can minimize the following

min
U ∈Rm×r,V ∈Rn×r

1
2

(cid:0)||U ||2

F + ||V ||2
F

(cid:1) +

β
2

||A(U V T ) − b||2

1.1 Contributions

In this paper, we consider the following general relaxation to the rank mini-
mization

n
(cid:88)

i=1

min
X

subject to

ρ(λi(X)) + φ(X)

A(X) = b

X (cid:23) 0

(1)

where λi(X) denotes the ith eigenvalue of X. We impose the following assump-
tions on all ρ throughout the paper.

Assumption 1 For a function ρ : [0, ∞) → [0, ∞),

(i) ρ is concave
(ii) ρ is monotonically increasing
(iii) ρ(0) = 0

4

–

April Sagan, John E. Mitchell

Table 1: Examples of typical concave relaxations used in sparse optimization
and their supergradients. For each regularizer, γ is a positive parameter. For
SCAD, we take β > 1, and for the Schatten-p norm, 0 < p ≤ 2. Each of these
functions satisﬁes Assumption 1.

Trace Inverse[8]

ρ(x)
1 − γ

γ+x

Capped l1 norm [29]

min(γx, 1)

LogDet [7] [17]

Schatten-p Norm [12]

SCAD[6]

Laplace[26]





log(x + γ)
p
2

(x + γ)

γx
−x2+2γαx−γ2
2(α−1)

γ2(α+1)
2

x ≤ γ

γ ≤ x ≤ αγ

x > αγ






1 − e−γx

∂ρ(x)
γ
(γ+x)2






x < 1
γ,
γ
[0, γ] x = 1
γ
x ≥ 1
0,
γ

γ
γ+x
p
2λ (x + γ)

p
2 −1

γ
αγ−x
(α−1)
0

x ≤ γ
γ ≤ x ≤ βγ

x > αγ

γe−γx

(iv) For all x ∈ [0, ∞), every subgradient of ρ is ﬁnite. Because ρ is concave, it

is suﬃcient to say

lim
x→0+

sup
w∈∂ρ(x)

w = κ < +∞

Additionally, we may also impose one or both of the following two assumptions:

Assumption 2 The function ρ(x) is strictly concave on [0, ∞).

Assumption 3 The function ρ(x) is diﬀerentiable on [0, ∞).

Examples of functions meeting these assumptions that are commonly used
as surrogates to the l0 norm are shown in Table 1. For each of the functions
listed with the exception of the Shatten-p norm and the LogDet relaxation,
the derivative approaches 0 for large values of x, which would expect to greatly
reduce the estimation bias.

To simplify notation, when applied to a positive semideﬁnite matrix, the
+ → R+ is the sum of the regularizer ρ applied to the eigenvalues

function ρ : S n
of the matrix. That is,

ρ(X) =

n
(cid:88)

i

ρ(λi(X))

In this paper, we show by construction that for any regularizer meeting
Assumption 1, the optimization problem (1) can be posed as a bi-convex opti-
mization problem. Our bi-convex formulation serves as an abstraction of that
presented by Mohan and Fazel [17], and can be used to derive similar iterative
reweighted problems. Using our abstraction, we are able to utilize the low-
rank factorization method for solving SDPs proposed by Burer and Monteiro
[3] in order to reduce the number of variables to O(nr) where r is an upper

Low-Rank Factorization for Rank Minimization with Nonconvex Regularizers

5

bound on the rank of the matrix, and extend the results to rectangular ma-
trices as well. We derive algorithms based on the low rank factorization and
prove convergence.

1.2 Previous Works on Nonconvex Approaches to Rank Minimization

In order to more closely approximate the rank of a matrix, Fazel et. al. pro-
posed the LogDet heuristic for positive semideﬁnite rank minimization [7].
Instead of a convex function, the authors use the following smooth, concave
function as a surrogate for the rank function.

log(det(X + γI)) =

n
(cid:88)

i=1

log(λi(X) + γ)

where γ is a positive parameter. While nonconvex, the authors put forwards
a Majorize-Minimization (MM) algorithm to ﬁnd a local optimum. At each
iteration, the ﬁrst order Taylor expansion centered at the previous iterate
is solved as a surrogate function. The algorithm is simpliﬁed to solving the
following SDP at each iteration.

X (k+1) =argmin

(cid:104)W (k), X(cid:105)

X

subject to A(X) = b

X (cid:23) 0

where W (k) = (X (k−1) + δI)−1. We can view this algorithm as an iterative
reweighting of the nuclear norm. The iterative reweighted scheme was later
generalized by Mohan and Fazel [17] to minimize a class of surrogate functions
known as the smooth Schatten-p function, deﬁned as

fq(X) = Tr(X + γI)

p
2 =

n
(cid:88)

i=1

(λi(X) + γ)

p
2

for 0 < p ≤ 2. The weight matrix for the Schatten-p function is W (k) =
(X (k−1) + γI)
2 −1. Mohan and Fazel extend the algorithm for non square
matrices by solving

p

X (k+1) =argmin

(cid:104)W (k), X T X(cid:105)

X

subject to

A(X) = b

(2)

where W (k) = (X (k−1)T
X (k−1) + γI)−1 at each iteration. The authors prove
asymptotic convergence of the iterative reweighted algorithm for 0 ≤ p ≤ 1.
While this algorithm does give superior computational results, it can be very
time consuming in the positive semideﬁnite case and will not scale well for
large problems. We show in Section 5 how this can be improved by taking
advantage of the low rank property of X.

6

April Sagan, John E. Mitchell

In recent years, many functions have been proposed as alternative non-
convex surrogates to the rank function in addition to the logdet heuristic.
Zhang et. al.[34] proposed minimizing the truncated nuclear norm for a general
matrix X ∈ Rm×n, deﬁned for a ﬁxed constant r as

||X||r,∗ =

min(m,n)
(cid:88)

i=r+1

σi(X)

where σi(X) denotes the ith largest singular value. If we consider the large
singular values to represent the signal and the small singular values the noise,
as in the case of noisy image reconstruction, then this minimizes only the noise.
The idea of minimizing a concave function of the eigenvalues has been
generalized by Lu et. al. [4] [15], to any monotonically increasing and Lipschitz
diﬀerentiable function. These works consider an unconstrained problem with
a general loss function φ(X).

minX

min(m,n)
(cid:88)

i=1

ρ(σi(X)) + φ(X)

As with the LogDet algorithm, one can derive an MM algorithm using the
ﬁrst order Taylor expansion about the objective function. The authors include
a proximal term. At each iteration, the authors propose solving the following
problem

X k+1 =min

=min

min(m,n)
(cid:88)

i=1

min(m,n)
(cid:88)

i=1

wiσi(X) + (cid:104)∇φ(X k), X − X k(cid:105) +

µ
2

||X − X k||

wiσi(X) +

µ
2

||X − Y ||

γ(σi(X k)). Much like the popular Singular
where Y = X k−∇φ(X k) and wi = ρ(cid:48)
Value Thresholding method put forth by Cai, Cand`es, and Shen [11], this has
a closed form involving the shrinkage operator deﬁned as St(Σ) = Diag(Σii −
ti)+. The authors prove that the subproblem has a closed form solution

X k+1 = U Sγw(Σ)V T

where U ΣV T is the singular value decomposition of Y .

The shrinkage operator, however, requires computing the singular value
decompositon of a possibly very large matrix, which can be time consuming
and ineﬃcent even when only the top few singular values are needed. Simi-
lar algorithms presented by Yao et. al. address this problem by showing one
only needs to ﬁnd the singular value decomposition of a much smaller matrix,
making the method suitable for large scale problems. [31], [32]

Low-Rank Factorization for Rank Minimization with Nonconvex Regularizers

7

2 Equivalent Biconvex Formulation

It was shown by Mohan and Fazel [17] that the LogDet heuristic can be refor-
mulated as a bi-convex problem with an additional variable W as follows

min
X,W

subject to

(cid:104)X, W (cid:105) + γtrace(W ) − log det(W )

A(X) = b

X (cid:23) 0

I (cid:23) W (cid:23) 0

(3)

This allowed the authors to reformulate the MM algorithm outlined in equation
(2) as an alternating method, which was of use when showing convergence of
the algorithm. We now show that an extension of this reformulation can be
used for any surrogate to the rank function satisfying Assumption 1.

Proposition 1 For a function ρ satisfying Assumption 1, consider the fol-
lowing bi-convex semideﬁnite program

min
X,W

subject to

(cid:104)X, W (cid:105) + G(W ) + φ(X)

A(X) = b

X (cid:23) 0

κI (cid:23) W (cid:23) 0

(4)

where κ = sup ∂ρ(0), the function G : Sn
satisﬁes the following condition:

+ → R deﬁned as G(W ) = (cid:80) g(λi(W ))

∂g(w) = {−x : w ∈ ∂ρ(x)}.

(5)

Any KKT point X ∗ of the general nonconvex relaxation (1) can be used to
construct a KKT point (X ∗, W ∗) of (4) where W ∗ ∈ ∂ρ(X ∗) . Likewise, for
any (X ∗, W ∗) pair that is a KKT point of (4), X ∗ is a KKT point of (1) and
W ∗ ∈ ∂ρ(X ∗).

Remark 1 In previous works, it has been shown that the rank minimization
problem (1) is equivalent to the following semideﬁnite program with comple-
mentarity constraints:

min
X,U

subject to

n − trace(U ) + φ(X)

(cid:104)X, U (cid:105) = 0

A(X) = b

X (cid:23) 0

0 (cid:22) U (cid:22) I

(6)

Intuitively, the eigenvalues of the matrix I − U are the l0 norm of the eigen-
values of X, which implies that n − trace(U ) is the rank of X [21,20,13]. Shen

8

April Sagan, John E. Mitchell

and Mitchell [21] studied the problem when the complementarity constraint is
relaxed as a penalty term.

min
X,U

subject to

n − trace(U ) + γ(cid:104)X, U (cid:105) + φ(X)

A(X) = b

X (cid:23) 0

0 (cid:22) U (cid:22) I

(7)

The penalty formulation is a biconvex semideﬁnite program in the form of (4),
with W = 1
γ trace(W ). This is equivalent to the semideﬁnite
program (1) with ρ(x) being the capped l1 norm, min( 1

γ U and G(W ) = − 1

γ x, 1)

We want to work with the derivative of the inverse of the derivative of
ρ(x), but this is only deﬁned as stated if ργ(x) satisﬁes Assumptions 2 and 3.
Under only Assumption 1, we deﬁne the function

q(t) := inf{x ∈ [0, ∞) : t ∈ ∂ργ(x)}.

(8)

Note that if t ≥ κ then t ∈ ∂ρ(0), so q(t) = 0 for t ≥ κ. The function q(t) is
deﬁned for t > β, since ρ(x) is concave; q(β) is also deﬁned if β is attained.
We let J denote the domain of q(t) and ¯J := {w ∈ J : w ≤ κ}. Note that
q(t) is lower semicontinuous; it is continuous if Assumptions 2 and 3 hold, in
which case it is the inverse function of the derivative of ρ(x) for t ∈ ¯J. We can
now deﬁne the function g : J → [0, ∞) as

g(w) :=

(cid:90) κ

w

q(t)dt.

(9)

Lemma 1 The function g(w) is decreasing and convex on its domain J. It is
strictly convex for w ≤ κ if Assumption 3 holds. It is diﬀerentiable for w ≤ κ
if Assumption 2 holds.

Lemma 2 For each x ∈ [0, ∞), there exists w ∈ ∂ρ(x) such that

Further, the subdiﬀerential is given by

− x ∈ ∂g(w).

∂g(w) = {−x : w ∈ ∂ρ(x)}.

If ρ(x) also satisﬁes Assumptions 2 and 3 then

g(cid:48)((ρ(cid:48))−1(x)) = −x.

Example 1 Let ρ(x) be the continuous nondiﬀerentiable function

ρ(x) =






4x
if 0 ≤ x ≤ 2
6x − x2 if 2 ≤ x ≤ 3
9

if x ≥ 3

(10)

(11)

(12)

Low-Rank Factorization for Rank Minimization with Nonconvex Regularizers

9

which is nondiﬀerentiable at x = 2 and is only strictly concave for x ∈ [2, 3].
We have β = 0 and κ = 4. Then

and

Further,

q(t) =






3 − 1
2
0

2 t if 0 ≤ t ≤ 2
if 2 ≤ t < 4
if t ≥ 4

g(t) =






9 + 1
2(4 − t)
0

4 t2 − 3t if 0 ≤ t ≤ 2
if 2 ≤ t ≤ 4
if t ≥ 4

∂g(w) =






[−∞, −3] if w = 0
{ 1
2 w − 3} if 0 < w ≤ 2
if 2 ≤ w < 4
{−2}
if w = 4
[−2, 0]
if w > 4
{0}

The lack of strict concavity on the two line segments leads to the two intervals
of subgradients ∂g(w) for w = 0 and w = 4. The nondiﬀerentiability at x =
2 leads to multiple values of w having the same set of subgradients ∂g(w),
namely {2} for 2 ≤ w < 4.

Proofs of lemmas

Proof Proof of Lemma 1:

Monotonicity of g(w) follows from the nonnegativity of q(t).
To show convexity, we consider w1 < w2, with w1, w2 ∈ J, and 0 ≤ λ ≤ 1.

We have

g(λw1+ (1 −λ)w2) = (cid:82) κ
= λ (cid:82) κ
w1

λw1+(1−λ)w2
q(t)dt + (1 − λ) (cid:82) κ
w2
+ (1 − λ) (cid:82) w2

q(t)dt

λw1+(1−λ)w2

q(t)dt

q(t)dt − λ (cid:82) λw1+(1−λ)w2
w1

q(t)dt

≤ λg(w1) + (1 − λ)g(w2)

− λ(λw1 + (1 − λ)w2 − w1) g(λw1 + (1 − λ)w2)
+ (1 − λ)(w2 − λw1 + (1 − λ)w2) g(λw1 + (1 − λ)w2)
from monotonicity of q(t)

= λg(w1) + (1 − λ)g(w2),

so g(w) is convex.

If Assumption 3 holds then q(t) is strictly decreasing for β < w1 ≤ κ, so

the inequality above holds strictly, so g(w) is strictly convex.

If Assumption 2 holds then q(t) is continuous on J, so g(w) is diﬀerentiable.

Proof Proof of Lemma 2:

Since g(w) is convex, the subdiﬀerential of g(w) for a slope w ∈ J is deﬁned

April Sagan, John E. Mitchell

10

as

∂g(w) = {ξ : ξh ≤ g(w + h) − g(w) ∀ w + h ∈ J}

= {ξ : ξh ≤ − (cid:82) w+h

w

q(t)dt ∀ w + h ∈ J}

= {ξ : ξh ≤ −hq(w + h) ∀ w + h ∈ J}
from monotonicity of q(t)

= {ξ : ξ ≤ −q(w + h) ∀ h > 0, w + h ∈ J}

∩ {ξ : ξ ≥ −q(w + h) ∀ h < 0, w + h ∈ J}

= {ξ : ξ ≤ −x ∀ x ∈ [0, ∞) with w + h ∈ J ∩ ∂f (x), h > 0}

∩ {ξ : ξ ≥ −x ∀ x ∈ [0, ∞) with w + h ∈ J ∩ ∂f (x), h < 0}

= { −x : w ∈ ∂f (x)}

from concavity of ρ(x).

It follows that given x ∈ [0, ∞), we can choose ¯w ∈ ∂f (x), and we will have
−x ∈ ∂g( ¯w).

If Assumptions 2 and 3 hold then ∂ρ(x) = {ρ(cid:48)(x)}, x is the unique point
with derivative ρ(cid:48)(x), and g(w) is diﬀerentiable from Lemma 1. Setting ¯w =
ρ(cid:48)(x), the Fundamental Theorem of Calculus implies that

g(cid:48)(ρ(cid:48)(x)) = −q(ρ(cid:48)(x)) = −x,

as required.

Before proving Proposition 1, we consider the following lemma.
Lemma 3 Let X be a positive deﬁnite matrix. Let G(W ) = (cid:80)n
be a convex function for any matrix W ∈ Sn
˜W is a minimizer of:

i=1 g(λi(W ))
+. Let κ be a positive constant. If

minW ∈Sn
subject to 0 (cid:22) W (cid:22) κI

(cid:104)X, W (cid:105) + G(W )

+

Then ˆW is also a minimizer with the same objective value, where

ˆW =

n
(cid:88)

i=1

λn−i+1( ˜W )vivT
i

and vi is the eigenvector of X corresponding to the ith largest eigenvalue.

Proof First, note the ˆW is a feasible point and G( ˆW ) = G( ˜W ), as the two
matrices have the same eigenvalues.

The proof relies on the Hoﬀman-Wielandt inequality [10], which states that

for any symmetric matrices A and B,

||A − B||2

F ≥ ||λ(A) − λ(B)||2

Low-Rank Factorization for Rank Minimization with Nonconvex Regularizers

11

where λ(A) denotes the vector of eigenvalues of A in descending order. When
applied to the matrices X and − ˜W , we have

||X − (− ˜W )||2

F ≥

n
(cid:88)

i=1

(cid:0)λi(X) − λi(− ˜W )(cid:1)2

=

n
(cid:88)

i=1

(cid:0)λi(X) + λn−i+1( ˜W )(cid:1)2

.

Expanding these terms gives us the following:

||X||2

F + || ˜W ||2

F + 2(cid:104)X, ˜W (cid:105) ≥ ||λ(X)||2 + ||λ( ˜W )||2 + 2

n
(cid:88)

i=1

λi(X)λn−i+1( ˜W )

Using the fact that the Frobenius norm of a matrix is the norm of the eigen-
values, and using the simultaneous diagonalizability of ˆW and X, we have:

(cid:104)X, ˜W (cid:105) ≥

n
(cid:88)

i=1

λi(X)λn−i+1( ˜W ) = (cid:104)X, ˆW (cid:105)

So, ˆW is a feasible point with an objective value no more than that of ˜W , and
is also a minimizer.

Additionally, we present the technical lemma about the gradient of the
objective function in (1), which is paramount when deriving algorithms and
optimality conditions. First and second derivatives of the eigenvalue function
have been studied extensively by Mangus [16] and Andrew et. al. [2].

Lemma 4 Let vi denote the eigenvector corresponding to the ith eigenvalue
of X. If λi(X) is a simple eigenvalue,

d
dX

λi(X) = vivT
i

(13)

If λi(X) = λi+1(X) = · · · = λi+k(X), then

d
dX

k
(cid:88)

j=0

λi+j(X) =

k
(cid:88)

j=0

vi+jvT

i+j

Lemma 4 allows us to easily compute the subgradient of the objective function.

(cid:26)

∂ρ(X) =

V diag(y1, y2, . . . yn)V T

(cid:12)
(cid:12)
(cid:12)
(cid:12)

yi ∈ ∂ρ(λi(X))

(14)

(cid:27)

where V denotes the matrix of eigenvectors of X. We can now prove Proposi-
tion 1.

12

April Sagan, John E. Mitchell

Proof We start by considering KKT points of (4). The feasible pair (X, W ) is
a KKT point if there exists a subgradient Z of G(W ) such that

0 (cid:22) W ⊥ X + Z + Y (cid:23) 0

0 (cid:22) X ⊥

(cid:88)

µiAi + W + ∇φ(X) (cid:23) 0

i
0 (cid:22) Y ⊥ κI − W (cid:23) 0

(15a)

(15b)

(15c)

By Lemma 4, if W has eigenvectors V W and eigenvalues w1, w2, . . . , wn, then
V W diag(z1, z2, . . . , zn)V W T (cid:12)

zi ∈ ∂g(w)

∂G(W ) =

(cid:26)

(cid:27)

.

(cid:12)
(cid:12)
(cid:12)

We start by claiming that X and W (and hence Z and Y ) are simultane-
ously diagonalizable by citing Lemma 3. Equation (15c) shows that Y and W
are simultaneously diagonalizable. Hence X, W , Y , and Z are all simultane-
ously diagonalizable, and the KKT conditions (15a) and (15c) simplify to the
following.

0 ≤ λi(W ) ⊥ λi(X) + λi(Z) + λi(Y ) ≥ 0
0 ≤ λi(Y ) ⊥ κ − λi(W ) ≥ 0

∀i = 1, ..., n

∀i = 1, ..., n

(16a)

(16b)

If 0 < λi(W ) < κ, we have that λi(Y ) = 0, and so equations (16a) and (16b)
are satisﬁed if λi(X) + λi(Z) = 0. By construction of g from Lemma 2, there
exists wi ∈ ∂ρ(λi(X)) and zi ∈ −∂g(wi) such that λi(Z) = zi, λi(W ) = wi is
a solution.

When the upper bound on the eigenvalue of W is an active constraint,
i.e. when λi(W ) = κ, then there exists zi ∈ ∂g(κ) such that λi(X) + zi ≤ 0.
Because ∂g(κ) = {0}, λi(X) = 0, which is to say λi(W ) ∈ ∂ρ(λi(X)).
Finally, we consider when λi(W ) = 0. Equation (16a) becomes

λi(X) ≥ −zi

for some zi ∈ ∂g(0). By Lemma 2, we have that 0 ∈ ∂ρ(−zi). Because ρ is
concave and nondecreasing, if 0 ∈ ∂ρ(x1), then 0 ∈ ∂ρ(x2) for all x2 ≥ x1, and
so λi(W ) = 0 ∈ ∂ρ(λi(X)).

We can now say that, in general, any KKT point satisﬁes λi(W ) ∈ ∂ρ(λi(X))

for i = 1, .., n, and by Lemma 4, W ∈ ∂ρ(x). The KKT conditions for (1) state
that there exists a U ∈ ∂ρ(x) such that

0 (cid:22) X ⊥

(cid:88)

µiAi + U + ∇φ(X) (cid:23) 0

With the assignment U = W , it is clear that if (X, W ) is a KKT point of (4),
then X is a KKT point of (1).

Conversely, consider any X that is a KKT of (1) with dual variable µ.
Then, the assignment W ∈ ∂ρ(x) and Y = 0 satisfy (15b) and (15c). By
Lemma 2, we have that there exists a Z ∈ ∂G(W ) such that Z = −X and
(15a) is satisﬁed.

Low-Rank Factorization for Rank Minimization with Nonconvex Regularizers

13

Table 2: Function G(W ) and constants κ that satisfy the conditions in Propo-
sition 1 for various concave relaxations of the rank function.

Trace Inverse
Capped l1 norm
LogDet

Schatten-p Norm
SCAD

Laplace

2

∂g(w)
γ + w− 1
−1
γ
γ − γ
w
(cid:1) 1
p−1

γ − (cid:0) w
p
−αγ + (α − 1)w
γ log( w
γ )

− 1

G(W )

√

trace(γW − 2

W )

−1
γ trace(W )
γtrace(W ) − γ log det(W )
trace(γW − 2−p
trace( α−1
λi(W )
γ

p
p−2 )
p W
2 W 2 − αγW )
(log( λi(W )

) − 1)

(cid:80)
i

γ

κ
1
γ
γ
1
p
2 −1

p
2γ γ
γ

γ

Such a function is shown for various choices of nonconvex regularizers in Table
2, and can be easily veriﬁed by showing that equation (5) holds. We note that
the function G(W ) is used primarily for theoretical analysis and derivation of
algorithms. In practice, one only needs the function ρ(cid:48)(x).

2.1 Low-Rank factorization

While the MM algorithm is eﬃcient in the non-symmetric case, with each
iteration having closed form updates which can be calculated in O(nm2) time,
the algorithm is not scalable in the positive semideﬁnite case, as it needs to
solve a semideﬁnite program at each iteration. Instead, we take advantage of
the low rank factorization for semideﬁnite programs as presented by Burer
and Monteiro [3] and utilized to solve the nuclear norm minimization problem
by Tasissa and Lai [25]. Let r be an upper bound on the rank of the matrix
we seek to reconstruct. Then, if X is positive semideﬁnite, we have that there
exists a matrix P ∈ Rn×r such that X = P P T .

min
P ∈Rn×r,W ∈S n
+

subject to

(cid:104)P P T , W (cid:105) + G(W ) + φ(P P T )

A(P P T ) = b, 0 (cid:22) W (cid:22) κI

(17)

While X is replaced with a variable of drastically reduced size, W is left as
a positive semideﬁnite matrix of size n. To reduce the size of W , we propose
minimizing the rank of P T P instead of the rank of P P T .

min
P ∈Rn×r,W ∈S r
+

subject to

(cid:104)P T P, W (cid:105) + G(W ) + φ(P P T )

A(P P T ) = b, 0 (cid:22) W (cid:22) κI

(18)

Intuitively, this should be equivalent due to the fact that the non-zero
eigenvalues of P P T are equivalent to the nonzero eigenvalues of P T P . We
prove this intuition in the following proposition.

14

April Sagan, John E. Mitchell

Proposition 2 Let P ∗ ∈ Rn×r have the singular value decomposition P ∗ =
(cid:80)r

i . If (P ∗, Wn) is an optimizer of (17), then

i=1 viuT

i σP

W ∗

n =

n
(cid:88)

i=1

i vivT
λW

i =

r
(cid:88)

i=1

i vivT
λW

i + κ

n
(cid:88)

i=r+1

vivT
i ,

and if (P ∗, Wr) is an optimizer of (18), then W ∗
more, (P ∗, W ∗
of (17).

i=1 λW
r ) is an optimizer of (18) if and only if (P ∗, W ∗

r = (cid:80)r

i uiuT
i . Further-
n ) is an optimizer

Proof We start by proving that W ∗
n and W ∗
r have the eigenvalue decomposi-
tions stated in the proposition. By the same reasoning as in Proposition 1, any
matrix W ∈ ∂ρ(P ∗P ∗T ) is an optimizer to the convex semideﬁnite program:

min
W ∈S n
+
subject to

(cid:104)P ∗P ∗T , W (cid:105) + G(W )

0 (cid:22) W (cid:22) κI

i λW

i=1(σP

i )2vivT

So, if P ∗P ∗T = (cid:80)r
W ∗
∂ρ(cid:0)(σP
W ∗

n has the eigendecompositon (cid:80)r

n ) such that
i vivT
i ∈
i )2(cid:1) and κ = sup ∂ρ(0). Likewise, (P ∗, Wr) is an optimizer of (18), then

i , then there is a minimizer (P ∗, W ∗
i + (cid:80)n

i , where λW

i=r+1 κvivT

is an optimizer, where λW

r = (cid:80)r
Next, we will show that if (∆P, ∆Wr) was a feasible descent direction
r ), then we can construct a feasible direction for (17) at
n ), and vice versa. If (∆P, ∆Wr) was a feasible descent direction, then,

in (18) at (P ∗, W ∗
(P ∗, W ∗
there exists a subgradient Zr ∈ ∂G(W ∗

i ∈ ∂ρ(cid:0)(σP

i=1 λW

i uiuT
i

i )2(cid:1).

r ) such that

2(cid:104)P ∗W ∗

r + ∇φ(P ∗P ∗T )P ∗, ∆P (cid:105) + (cid:104)P ∗T P ∗ + Zr, ∆Wr(cid:105) < 0.

(19)

We claim that (∆P, ∆Wn) is a descent direction in (17) with

∆Wn = VrU T ∆WrU V T
r ,

where U ∈ Rr×r is the matrix whose columns are the eigenvectors of Wr, and
V ∈ Rn×r is the matrix whose columns are the ﬁrst r eigenvectors of Wn. First
note that, P ∗Wr = (cid:80)r
n P ∗, and ∆P is a feasible direction
i = W ∗
in (17).

i=1 viuiλW

i σP

Next, consider the gradient of the objective of (17) with respect to W ,

P ∗P ∗T + ∇G(W ∗

n ) =

r
(cid:88)

i=1

vivT
i

(cid:0)σP

i + zi

(cid:1) +

n
(cid:88)

i=r+1

vivT

i (zκ)

where zi ∈ ∂g(λW
i ) and zκ ∈ ∂g(κ). Speciﬁcally, we chose zi = λi(Zr), Zr
be the r by r matrix with eigenvectors U and eigenvalues z1, . . . , zr so that
Zr ∈ G(Wr). By Lemma 2, 0 ∈ ∂g(κ), and so the rank r matrix

Zn := VrU T (cid:0)P ∗T P ∗ + Zr

(cid:1)U V T

r

Low-Rank Factorization for Rank Minimization with Nonconvex Regularizers

15

is a subgradient of G with respect to Wn. Consider the inner product of the
gradient of the objective of (17) with respect to Wn and the proposed descent
direction for Wn.

(cid:104)P ∗P ∗T + Zn, ∆Wn(cid:105) = (cid:104)VrU T (cid:0)P ∗T P ∗ + Zr

(cid:1)U V T

r , ∆Wn(cid:105)

=(cid:104)(cid:0)P ∗T P ∗ + Zr
=(cid:104)(cid:0)P ∗T P ∗ + Zr

r ∆WnVrU T (cid:105)

(cid:1), U V T
(cid:1), ∆Wr(cid:105)

Combining these facts gives us that (∆P, ∆Wn) is a descent direction:

2(cid:104)W ∗
=2(cid:104)P ∗W ∗

n P ∗ + ∇φ(P ∗P ∗T )P ∗, ∆P (cid:105) + (cid:104)P ∗P ∗T + Zn, ∆Wn(cid:105)

r + ∇φ(P ∗P ∗T )P ∗, ∆P (cid:105) + (cid:104)P ∗T P ∗ + Zr, ∆Wr(cid:105) < 0

The proof of the other direction is similar.

2.2 Extension to Nonsymmetric Matrices

To extend these methods to general nonsymmetric matrices X ∈ Rm×n, we
can minimize the rank of PSD matrix X T X, as done by Mohan and Fazel [17].
However, this is computationally ineﬃcient as each iteration requires ﬁnding
the eigendecomposition of X T X. With this in mind, we put forth a separate
extension in which we minimize the rank of the following auxiliary variable

Z =

(cid:21)

(cid:20)G X T
X B

It was shown by Liu et. al. that for any X, there exists G and B such that
rank(X) = rank(Z) and Z (cid:23) 0 [14]. We can thus solve the following minimiza-
tion problem

min
Z,W

subject to

(cid:104)Z, W (cid:105) + G(W ) + φ(X)

Z =

A(X) = b
(cid:21)

(cid:23) 0

(cid:20)G X T
X B

0 (cid:22) W (cid:22) κI

(20)

While ineﬃcient on its own due to the matrix W being (m + n) × (m + n), this
formulation allows us to utilize the Burer-Monteiro approach which allowed
us to eﬃciently solve the semideﬁnite case in Algorithm 1. We utilize the
same upper bound r on the rank of X as before and introduce the matrix
P ∈ R(m+n)×r such that Z = P P T . We decompose P into Pm and Pn such

that P =

so that X = PmP T

n . As before, we minimize the rank of

(cid:21)

(cid:20) Pn
Pm

16

April Sagan, John E. Mitchell

P T P = P T

mPm + P T

n Pn.

min
W,Pm,Pn

subject to

(cid:104)P T

mPm + P T

n Pn, W (cid:105) + G(W ) + φ(PmP T
n )

A(PmP T

n ) = b, 0 (cid:22) W (cid:22) κI

(21)

We note that for the special case of minimizing the nuclear norm, W = I, we
have the well known alternating minimization method when using a quadratic
loss function [22][18][23] as follows:

||Pn||2

F + ||Pm||2

F +

β
2

||A(PmP T

n ) − b||2.

(22)

min
Pm,Pn

3 Algorithms

In most practical applications, we expect noise in our measurements, and thus
an equality constraint may not be practical. For the algorithms in this section,
we restrict our focus to the problem of rank minimization with a quadratic
loss function, φ(X) = β
F , and no linear constraints. Utilizing the
low-rank factorization technique, for the case of non symmetric matrices, we
seek to minimize

2 ||A(X) − b||2

min
X,W
subject to

(cid:104)P T

mPm + P T
0 (cid:22) W (cid:22) κI

n Pn, W (cid:105) + G(W ) + β

2 ||A(PmP T

n ) − b||2

(23)

3.1 Alternating Methods for Rectangular Matrices

While the formulation for rectangular matrices could be solved by simply using
Algorithm 1, we propose an ADMM algorithm wherein we alternate over the
variables Pm, Pn, and W . By doing so, the subproblems in Pm and Pn are
strongly convex. The subproblems are as follows:

m = argminPm (cid:104)P T
P k

β
mPm, W (cid:105) +
2
β
||A(PmP T
2
The gradients of which can be calculated as

P k
n = argminPn (cid:104)P T

n Pn, W (cid:105) +

||A(PmP T

n ) − b||2
F

n ) − b||2
F

(24)

∇PmF (Pm, Pn, W ) = PmW + βA∗(A(PmP T
∇Pn F (Pm, Pn, W ) = PnW + βA∗(A(PmP T

n ) − b)Pn
n ) − b)T Pm

where F (Pm, Pn, W ) is the objective function of (23).

The update for W is derived from Proposition 1, and is similar to that of

other iteratively reweighted methods [7] [17] [12].

W k = ∇ρ(P kT

P k)

Low-Rank Factorization for Rank Minimization with Nonconvex Regularizers

17

Because we are minimizing the rank of the the smaller matrix P T P , this
update is calculated in O(r3) operations.

Algorithm 1 Alternating Minimization for Rank Minimization with a Gen-
eral Nonconvex Regularizer (GenAltMin)

Input: A, b
Output: Stationary point X of (23)

Initialization :P 0 = rand(n, r), W 0 = I.

1: for k = 1, .., do
2:

Solve

3:

Solve

m = argminPm (cid:104)P T
P k

mPm, W (cid:105) +

n = argminPn (cid:104)P T
P k

n Pn, W (cid:105) +

β
2

||A(PmP T

n ) − b||2
F

β
2

||A(PmP T

n ) − b||2
F

[V k, Σk] = eig(P k T P k)

4:
5: W k = V kρ(cid:48)(Σk)V K T
6:
Check for Convergence
7: end for
8: return X

3.2 Alternating Steepest Descent

For alternating minimization without a regularizer, it has been shown com-
putationally eﬀective to, instead of solving subproblems to optimality, take
one step in the gradient direction at each iteration [24]. For the Pn and Pm
updates, we can calculate the steepest descent step size. Let dm and dn de-
note the gradient in the Pm and Pn subproblems. Then, the steepest descent
step sizes tm and tn for each subproblem respectively are can be calculated as
follows

tm =

tn =

β(cid:104)A(dmP T

n ), A(PmP T

β||A(dmP T

n )||2 + 2(cid:104)dT

β(cid:104)A(PmdT

n ), A(PmP T

n ) − b(cid:105) + 2(cid:104)P T
mdm, W (cid:105)
n ) − b(cid:105) + 2(cid:104)P T
n dn, W (cid:105)

mdm, W (cid:105)

n dn, W (cid:105)

β||A(PmdT

n )||2 + 2(cid:104)dT

Note that the step sizes can be calculated with O((m + n)r2 + r|Ω|) compu-
tations. Because solving W to optimality is computationally inexpensive by
comparison, we update W in the same way as in Algorithm 1. The parameters
β and γ are also updated in the previously mentioned way.

3.3 Convergence

Each of the algorithms presented in this section is guaranteed to converge by
the main result in [28]. Xu and Yin show convergence of coordinated block

18

April Sagan, John E. Mitchell

Algorithm 2 Alternating Steepest Descent with General Nonconvex Regu-
larizer (GenASD)

Input: A, b
Output: Stationary point X of (23)

Initialization :P 0

n = rand(n, r), W 0 = I.

m P k−1
n
n )−b(cid:105)+2(cid:104)P T

mdm,W (cid:105)

T

) − b)P k−1

n

mdm,W (cid:105)

T

) − b)T P k
m
n dn,W (cid:105)

n
n )−b(cid:105)+2(cid:104)P T
n dn,W (cid:105)

1: for k = 1, .., do
2:

6:

5:

4:

3:

n )||2+2(cid:104)dT

m W + βA∗(A(P k−1
n ),A(PmP T

m = P k−1
dk
m = β(cid:104)A(dmP T
tk
β||A(dmP T
m = P k−1
m − tk
mdk
P k
m
n W + βA∗(A(PmP k−1
n = P k−1
dk
n ),A(PmP T
n = β(cid:104)A(PmdT
tk
β||A(PmdT
n = P k−1
ndk
n − tk
P k
7:
n
n Pn + P T
[V k, Σk] = eig(P T
8:
9: W k = V kρ(cid:48)(Σk)V K T
10:
11:
12: end for
13: return X = PmP T
n

n )||2+2(cid:104)dT

mPm)

βk+1 = min(cid:0)1.2βk, βmax), γk+1 = max(cid:0)0.8γk, γmin)
Check for Convergence

descent algorithms to solve nonconvex optimization problems of the following
form:

min
x∈X

F (x1, . . . , xs) ≡ f (x1, . . . , xs) +

si(xi)

(25)

s
(cid:88)

i=1

Denote

and

i (xi) = f (xm
f k

1 , . . . , xk

i−1, xi, xk−1

i+1 , . . . xk−1

s

)

X k

i (xi) = X (xm

1 , . . . , xk

i−1, xi, xk−1

i+1 , . . . xk−1

s

).

Xu and Yin analyze three types of updates:

xk
i = argmin
xi∈X k
i

f k
i (xi) + ri(xi)

xk
i = argmin
xi∈X k
i

f k
i (xi) +

Lk−1
i
2

||xk−1

i − xk−2

i

||2 + ri(xi)

(26)

(27)

xk
i = argmin
xi∈X k
i

(cid:104)∇f k

i (ˆxk−1
i

), xi(cid:105) +

Lk−1
i
2

||xi − ˆxk−1

i

||2 + ri(xi)

(28)

i

where ˆxk−1

i −xk−2

i = xk−1

i +wk(xk−1

), and wk ≥ 0 is the extrapolation weight.
The authors assume that F is continuous, bounded, and has a minimizer.
i depending on the type of update
i must be strongly convex, and for the
i -Lipshitz diﬀerentiable. For the
i need not even

Additionally, they make assumptions on f k
used. For the standard update (26), f k
proximal linear update (28), ∇f k
proximal update (27), no additional assumptions are made; f k
be convex.

i must be Lk

Low-Rank Factorization for Rank Minimization with Nonconvex Regularizers

19

In both of the algorithms presented in this section, the W update is solved
to optimality, and thus G(W ) is required to be strongly convex. As shown in
Lemma 1, this is satisﬁed for any diﬀerentiable regularizer satisfying Assump-
tion 1.

In Algorithm 1, we utilize the standard update, and so our objective func-
tion must be strongly convex. Because the quadratic loss function is block
convex in both Pm and Pn, it typically samples a small portion of the ma-
m, W k(cid:105) and
trix and will not be strongly convex. However, the terms (cid:104)P T
(cid:104)P T
n , W k(cid:105) are strongly convex so long as W k is full rank. Assumption 2
is then necessary to ensure convergence, as strong concavity in ρ ensures ρ is
strictly increasing and that that 0 /∈ ∂ρ(x) for any ﬁnite x.

mP T

n P T

Lastly, because ∇PmF and ∇PnF are linear, Algorithm 2 converges.
While the capped l1 norm is non-diﬀerentiable, meaning none of the algo-
rithms in this section are guaranteed to converge when using it as the regu-
larizer, one can modify the algorithms slightly so that it does converge as in
Shen and Mitchell [21]. The authors utilize the proximal linear update for W
as follows:

W k+1 = proj
0(cid:22)W (cid:22)I

(cid:18)

W k +

(cid:19)
1
Lk (X k+1 + γI) + wk(W k − W k−1)

When this update is used in any of the algorithms in this section, convergence
is guaranteed without assuming diﬀerentiablity of the regularizer.

4 Numerical Results

Algorithms 1 and 2 were implemented in MATLAB R2018b, and the source
code to run the algorithms and reproduce every result in this section is publicly
available at github.com/april1729/GenAltMin. The numerical experiments
were conducted on a Dell Laptop running Windows 10 with 16 GB of ram and
an Intel Core i3-4030U CPU @ 1.90 GHz.

4.1 Synthetic Data for Rectangular Matrices

We now test Algorithms 1 and 2 utilizing synthetically generated low rank
matrices with additive Gaussian noise. Throughout this section, we generate
a matrix of size m by n with rank r and noise parameter d by the following
Matlab command:

M = randn(m,r) * randn(r,n) + d * randn(m,n)

Figures 1a and 1b show the Relative Frobenius Norm Error (RFNE) of the
solution recovered by the nuclear norm and by the trace inverse regularizer
with varying percentages of known data, along with the relative Frobenius
norm of the noise matrix as a baseline. We plot these results for a 300 by 200
matrix and a 1000 by 500 matrix, each averaged over 10 randomly generated

20

April Sagan, John E. Mitchell

(a) m = 300, n = 200, r = 5, d = 0.05

(b) m = 1000, n = 500, r = 10, d = 0.1

Fig. 1: RFNE of the matrix recovered from Algorithm 2 using both the nuclear
norm and trace inverse regularizer for varying amounts of data known, along
with the RFNE of the noise.

(a) m = 300, n = 200, r = 5,
d = 0.05, p = 0.2

(b) m = 1000, n = 500, r = 10,
d = 0.1, p = 0.2

Fig. 2: RFNE for varying amounts of data known for Algorithm 2 for both the
nuclear norm and trace inverse regularizer, along with the RFNE of the noise.
The two ﬁgures show the results for diﬀerent trade oﬀ parameters.

instances. In both ﬁgures, the trace inverse is able to outperform the baseline
when only 20% of the data is available. Note that in each case, the trace inverse
regularizer outperforms the nuclear norm.

To show that the superiority of the nonconvex regularizer is not just for
certain choices of β, we show how each method performs for values of β between
10−3 and 10 for the smaller problem and 10−4 and 1 for the larger problem
in ﬁgures 2a and 2b respectively. When the parameter is diﬀered by an orders
of magnitude, the results for the trace inverse regularizer are hardly aﬀected,
while the accuracy of the optimal solution to the nuclear norm problem varies
a signiﬁcant amount. In fact, every value of β for the trace inverse regularizer
outperformed the optimal value of β for the nuclear norm regularizer.

In order to illustrate the estimator bias of the nuclear norm formulation
compared to nonconvex approaches, we plot the singular values of the recon-

20406080100Percentage of Data Known00.020.040.060.080.1RFNETrace InverseNuclear NormNoisy matrix20406080100Percentage of Data Known00.010.020.030.040.050.060.070.080.09RFNETrace InverseNuclear NormNoisy matrix10-210010210400.20.40.60.81RFNETrace InverseNuclear Norm10-210010210400.10.20.30.40.50.60.70.8RFNETrace InverseNuclear NormLow-Rank Factorization for Rank Minimization with Nonconvex Regularizers

21

structed matrix utilizing both the trace inverse regularizer and the nuclear
norm, along with the singular values of the original matrix. We show this plot
for varying values of β of for a 300 by 200 matrix with rank 5 in Figure 3. We
plot the ﬁrst r singular values and the next r singular values on a diﬀerent
scale, where r is the rank of the matrix being recovered.

For values of β that are smaller than 0.01, the solution is the zero matrix,
and for values of β larger than 0.1, the solution is not the correct rank. As
expected, there is a very small range in which we obtain a matrix with the
correct rank. Additionally, when the nuclear norm algorithm gives a matrix
with the correct rank, the singular values reconstructed using the nuclear norm
are noticeably smaller. This is due to the fact that the nuclear norm puts equal
weight on minimizing each singular value, including the ones that should not
be zero. So, by increasing β, the singular values that are supposed to be zero
become larger, and by decreasing β, the singular values that are not supposed
to be zero become too small.

By contrast, the top r singular values for the matrix reconstructed with
Algorithm 2 are approximately equal to the singular values of the original
matrix. For values of β less than 0.01 in the ﬁrst case and 0.001 in the second
case, the solution to the trace inverse formulation is the correct rank. As
opposed to the convex relaxation, the nonconvex method has a suﬃciently
large range of β that give a matrix of the correct rank.

While this shows that the nonconvex formulations are signiﬁcantly more
robust to the choice of β, one may wonder if the added parameter controlling
the curvature of the regularizer, γ, may contribute to more variability with
parameter choices. Figure 4 shows the RFNE for choices of γ distributed be-
tween 0.03125 and 256. Surprisingly, the ﬁgure shows that for a large range of
choices of γ, the results are identical. It is only at γ = 0.125 that the nonconvex
formulation loses the stability it usually has. This behavior is expected due to
the fact that the trace inverse regularizer converges to the rank function as γ
approaches 0. For values of γ larger than the smallest non-zero singular value
of the original low rank matrix (roughly 200), the trace inverse formulation
behaves more similarly to the nuclear norm, which one could also expect as the
derivative of the nonconvex regularizer is approximately a constant for large
values of γ.

Due to the remarkable consistency of the algorithm for varying choices of
γ, parameter tuning is not an issue in practice. Ideally, the choice of γ would
be approximately half of the largest nonzero singular value of the original low
rank matrix so that the gradient of the regularizer is small for the top r singular
values. While this quantity cannot be directly measured with incomplete noisy
data, it can be (very roughly) approximated as follows:

γ =

1
√
rp

2

||PΩ( ˜M )||F

where r is a rough estimate of the rank of the matrix. Note that, unlike rank
constrained optimization methods which rely heavily on the rank of the matrix

22

April Sagan, John E. Mitchell

Fig. 3: Singular value distribution for the matrices recovered utilizing Algo-
rithm 2 with the trace inverse regularizer and nuclear norm regularizer with
m = 300, n = 200, r = 5, d = 0.05, and p = 0.2.

to be recovered being known exactly, Figure 4 indicates that our method will
perform well even when the estimate of the rank is oﬀ by orders of magnitude.

Before moving on to larger, real data sets, we demonstrate the diﬀerence
in speed between Algorithm 1 and Algorithm 2. Figures 5a and 5b plot the
convergence of the two algorithms on matrices that are 300 by 200 and 1000
by 500 respectively. First, note that in both ﬁgures the two methods converge
to the same local optima, suggesting one need not worry about the diﬀerence
in quality of the output between the two algorithms.

Low-Rank Factorization for Rank Minimization with Nonconvex Regularizers

23

(a) m = 300, n = 200, r = 5
p = 0.3, d = 0.1

(b) m = 1000, n = 500, r = 5
p = 0.1, d = 0.05

Fig. 4: RFNE of the matrix recovered by Algorithm 2 utilizing the trace norm
regularizer with values of γ between 2−4 and 28 in the left plot, and between
21 and 213 in the right plot, along with the RFNE of the noisy matrix and the
optimal value to the nuclear norm minimization problem.

(a) m = 300, n = 200, r = 5
p = 0.4, d = 0.05

(b) m = 1000, n = 500, r = 15
p = 0.2, d = 0.01

Fig. 5: Convergence of Algorithm 1 and Algorithm 2. The RFNE and cumu-
lative runtime is recorded at each iteration.

For the smaller case, while clear that taking only one step converges faster
than solving the subproblems to optimality, they both converge in under 2 sec-
onds. When solving the subproblems to optimality, however, only 4 iterations
are needed to converge. In the larger case, the diﬀerence is much more appar-
ent. GenASD still converges in less than half of a second, where as solving the
subproblems to optimality takes about 17 seconds.

We compare our algorithm to three other common matrix completion algo-
rithms. The algorithm presented by Yao et. al. [30], Fast Nonconvex Low-Rank
Matrix Learning (FaNCL), is the only other work we know of that solves (1)
with iterations having computational complexity O(r|Ω|). The authors utilize
nonconvex regularizers similar to the ones discussed in this paper, and use sin-
gular value thresholding with iteratively reweighted thresholds. The FaNCL

1001020.0150.020.0250.030.0350.040.0450.050.0550.06RFNETrace InverseNuclear NormNoise Matrix10010110210310400.020.040.060.080.10.120.140.160.18RFNETrace InverseNuclear NormNoise Matrix10-310-210-1100101Time (s)10-210-1100RFNEAlternating Steepest DescentAlternating Minimization10-210-1100101102Time (s)10-210-1100RFNEASDAM24

April Sagan, John E. Mitchell

Table 3: Comparison of four diﬀerent matrix completion algorithms on ran-
domly generated low rank matrices. The algorithm LMaFit reconstructs a
matrix of a given rank k. The table shows the results when the algorithm is
given the exact rank(k = r) and an incorrect rank (k = 2r).

GenASD

FaNCL

FPC

LMaFit

r

noise

p

5
5
5
5
10
10
10
10

5
5
5
5
10
10
10
10

0.05
0.05
0.1
0.1
0.05
0.05
0.1
0.1

0.1
0.1
0.3
0.3
0.1
0.1
0.3
0.3

0.1
0.3
0.1
0.3
0.1
0.3
0.1
0.3

0.05
0.1
0.05
0.1
0.05
0.1
0.05
0.1

Trace
Inverse

0.0234
0.0089
0.0399
0.018
0.7321
0.0094
0.7726
0.0193

0.031
0.0188
0.1723
0.0994
0.8952
0.0207
0.8675
0.1139

Capped
L1 Norm

SCAD

Log
Sum
m=300, n=200
0.0994
0.0128
0.0906
0.0203
0.2853
0.0156
0.3515
0.0233
m=1000, n=500
0.0493
0.023
0.1503
0.0894
0.4071
0.0273
0.5028
0.1013

0.0546
0.0092
0.0476
0.0181
0.1742
0.0098
0.1942
0.0195

0.039
0.0197
0.1216
0.057
0.0795
0.0222
0.1544
0.063

0.0232
0.0089
0.0402
0.018
0.0476
0.0093
0.091
0.0193

0.0311
0.0188
0.0947
0.0582
0.0424
0.0207
0.1231
0.0623

k=r

k=2r

0.2374
0.0171
0.2573
0.0334
0.6683
0.0202
0.6349
0.0428

0.1436
0.0484
0.3023
0.1061
0.5504
0.0684
0.5626
0.1563

0.0273
0.0089
0.3616
0.018
1.1706
0.0093
0.8075
0.0193

0.0314
0.0188
0.0943
0.0566
0.0475
0.0208
0.1258
0.0622

0.2892
0.1035
0.3027
0.1039
0.8913
0.1267
0.8429
0.1679

0.2074
0.1352
0.2946
0.1333
0.6989
0.1612
0.7173
0.2003

algorithm was later improved upon in [32] by incorporating a momentum term
for faster convergence. We only compare to the earlier work as that was the
code we had available.

We also compare to FPC, which solved the nuclear norm minimization
problem [19], and LMaFit, which solves the rank constrained problem [27].
Because LMaFit requires an estimate of the rank, we show results when the
algorithm is given the correct rank and a rank twice as large as the original
matrix to demonstrate the advantage of a rank minimization approach.

With minor exceptions, the algorithm presented in this paper, FaNCL,
and LMaFit when given the correct rank all give approximately the same
quality result. GenAltMin solves the problem faster than FaNCL in every
case. Although GenAltMin and FaNCL take approximately the same amount
of time per iteration, singular value thresholding methods take signiﬁcantly
more iterations. Our algorithm outperforms FPC for reasons discussed earlier
in this section, and also LMaFit when the rank is not well known.

4.2 Collaborative Filtering

Perhaps the most widely known application of rank minimization is the Net-
ﬂix Problem, wherein the goal is to predict how a user would rate a movie
based on how she rated other movies, along with how other users with similar
taste rated said movie. To formulate this as a matrix completion problem, we
have a sparse matrix whose columns correspond to diﬀerent movies and whose
rows correspond to diﬀerent users, with the entries of the matrix being how

Low-Rank Factorization for Rank Minimization with Nonconvex Regularizers

25

Table 4: NMAE utilizing Algorithm 2 with the trace inverse regularizer and
with the nuclear norm regularizer, along with LMaFit

MovieLens100k

Fold TI
1
2
3
4
5
avg

0.1724
0.1719
0.1702
0.1715
0.1732
0.1719

NN
0.1812
0.1799
0.1785
0.1789
0.1822
0.1802

LmaFit TI
0.1800
0.1775
0.1781
0.1787
0.1788
0.1786

MovieLens1m
NN
0.1695
0.1699
0.1695
0.1703
0.1691
0.1697

0.1683
0.1676
0.1682
0.1685
0.1678
0.1681

Jester

LmaFit TI
0.1820
0.1811
0.1825
0.1824
0.1815
0.1819

0.1570
0.1577
0.1572
0.1572
0.1574
0.1573

NN
0.1607
0.1610
0.1604
0.1603
0.1612
0.1607

LmaFit
0.1600
0.1601
0.1596
0.1602
0.1601
0.1600

a user rated a speciﬁc movie. We expect that if every entry of this matrix
was observed, the matrix would be low rank because the number of factors
contributing to how much someone enjoys a movie is far less than the total
number of movies or users in the data set.

We utilize Algorithm 4.2 and LMaFit on the MovieLens100k and Movie-
Lens1m datasets [1], and the Jester dataset [9]. Both MovieLens datasets con-
sist of ratings on various movies, rated from 1 to 5, and the Jester dataset
consists of ratings on jokes, rated -10 to 10. The MovieLens100k dataset
has 1,000 users, 1,700 movies, and 100,000 measurements, the MovieLens1m
dataset has 6,000 users, 4,000 movies, and 1 million measurements, and the
Jester dataset has 24,983 users, 101 jokes, and 689,000 measurements. Note
that while the movie lens datasets are both very sparse (approximately 5%),
the Jester dataset has 27% of all possible ratings.

For each dataset, we separate the data into ﬁve partitions, and for each
partition we use the remaining four partitions to ﬁnd a low rank matrix, and
the ﬁfth partition to test our results. In Table 4, we report the normalized
mean absolute error (NMAE), deﬁned as

NMAE =

1
nratings

(cid:88)

i

|yi − ˜yi|
ymax − ymin

where nratings is the total number of ratings used in the testing set, y is the
measurements from the dataset, ˜y are the predictions from the low rank matrix,
and ymax and ymin are the maximum and minimum ratings for the dataset (5
and 1 for the MovieLens dataset, and -10 and 10 for the Jester dataset). In
each case, we use 10 as the upper bound on the rank. We found that the
NMAE for LMaFit is minimized when constrained to a rank 1 matrix, which
is what is reported.

In every fold in each of the three datasets, Algorithm 4.2 utilizing the
trace norm regularizer outperforms the nuclear norm regularizer and LMaFit.
To gain insight as to why the trace inverse regularizer outperforms the other
methods, we examine the singular value distribution of the resulting low rank
matrix. The singular values for the matrices recovered from the MovieLens1M
dataset withholding fold 5 is shown for each method in Figure 6. Comparing
the trace inverse to the nuclear norm, the ﬁrst singular value of the matrix
recovered with the trace inverse regularizer is larger, and the rest are smaller,

26

April Sagan, John E. Mitchell

Fig. 6: Singular value decomposition for the matrix recovered from the Movie-
Lens1M dataset withholding fold 5.

which is expected because the trace inverse puts more weight on minimizing
smaller singular value and less weight on minimizing larger singular values. Be-
cause the ratings matrix is close to a rank one matrix, penalizing the largest
singular value is disadvantageous because we expect it to be large. Addition-
ally, as opposed to the result from LMaFit, the remaining 9 singular values
are nonzero. This demonstrates the advantage of rank minimization methods
over rank constrained methods: while we may want to put more emphasis on
the ﬁrst singular value, the remaining singular values are still important. In a
rank constrained paradigm, there is no way to both keep singular values and
also minimize them.

5 Conclusions

We have shown that the problem of minimizing the rank of a matrix using
nonconvex regularizers can be posed as a bi-convex semideﬁnite optimization
problem. By doing so, we were able to derive eﬃcient algorithms using a low
rank factorization and show convergence.

The methods are shown to be computationally superior to methods based
oﬀ of the nuclear norm relaxation, and that the estimator bias is drastically
reduced by using nonconvex regularizers. We show that the quality of the result
from our algorithm hardly changes when either of the parameters are changed
by multiple orders of magnitude. Additionally, we show that our method is
faster than other existing methods based oﬀ of nonconvex regularizers.

References

1. Movielens. https://grouplens.org/datasets/movielens/. Accessed: 2019-11-21
2. Andrew, A., Chu, K., Lancaster, P.: Derivatives of eigenvalues and eigenvectors of matrix
functions. SIAM Journal on Matrix Analysis and Applications 14(4), 903–926 (1993).
DOI 10.1137/0614061. URL https://doi.org/10.1137/0614061

12345678910Index0200040006000800010000120001400016000Singular ValueNuclear NormTrace InverseLMaFitLow-Rank Factorization for Rank Minimization with Nonconvex Regularizers

27

3. Burer, S., Monteiro, R.: A nonlinear programming algorithm for solving semideﬁ-
nite programs via low-rank factorization. Mathematical Programming 95(2), 329–
357 (2003). DOI 10.1007/s10107-002-0352-8. URL http://dx.doi.org/10.1007/
s10107-002-0352-8

4. C. Lu J. Tang, S.Y., Lin, Z.: Generalized nonconvex nonsmooth low-rank minimization.
Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern
Recognition (2014). DOI 10.1109/CVPR.2014.526

5. Cand`es, E., Tao, T.: The power of convex relaxation: Near-optimal matrix completion.
IEEE Trans. Inf. Theor. 56(5), 2053–2080 (2010). DOI 10.1109/TIT.2010.2044061.
URL https://doi.org/10.1109/TIT.2010.2044061

6. Fan, J., Li, R.: Variable selection via nonconcave penalized likelihood and its oracle
properties. Journal of the American Statistical Association 96(456), 1348–1360 (2001).
URL http://www.jstor.org/stable/3085904

7. Fazel, M., Hindi, H., Boyd, S.P.: Log-det heuristic for matrix rank minimization with ap-
plications to hankel and euclidean distance matrices. Proceedings of the 2003 American
Control Conference, 2003. 3, 2156–2162 vol.3 (2003)

8. Geman, D., Chengda Yang: Nonlinear image recovery with half-quadratic regularization.

IEEE Transactions on Image Processing 4(7), 932–946 (1995)

9. Goldberg, K., Roeder, T., Gupta, D., Perkins, C.: Eigentaste: A constant time collabora-
tive ﬁltering algorithm. Inf. Retr. 4(2), 133–151 (2001). DOI 10.1023/A:1011419012209.
URL https://doi.org/10.1023/A:1011419012209

10. Hoﬀman, A.J., Wielandt, H.W.: The variation of the spectrum of a normal matrix.
Duke Math. J. 20(1), 37–39 (1953). DOI 10.1215/S0012-7094-53-02004-3. URL https:
//doi.org/10.1215/S0012-7094-53-02004-3

11. J. Cai, E.C., Shen, Z.: A singular value thresholding algorithm for matrix completion.
SIAM Journal on Optimization 20(4), 1956–1982 (2010). DOI 10.1137/080738970. URL
http://dx.doi.org/10.1137/080738970

12. Lai, M.J., Xu, Y., Yin, W.: Improved iteratively reweighted least squares for uncon-
strained smoothed lq minimization. SIAM Journal on Numerical Analysis 51(2), 927–
957 (2013). DOI 10.1137/110840364. URL https://doi.org/10.1137%2F110840364
13. Li, Q., Qi, H.d.: A sequential semismooth newton method for the nearest low-rank
correlation matrix problem. SIAM Journal on Optimization 21(4), 1641–1666 (2011).
DOI 10.1137/090771181. URL https://doi.org/10.1137/090771181

14. Liu, Z., Vandenberghe, L.: Interior-point method for nuclear norm approximation with
application to system identiﬁcation. SIAM Journal on Matrix Analysis and Applications
31(3), 1235–1256 (2010). DOI 10.1137/090755436. URL https://doi.org/10.1137/
090755436

15. Lu, C., Zhu, C., Xu, C., Yan, S., Lin, Z.: Generalized singular value thresholding. arXiv

abs/1412.2231 (2014). URL http://arxiv.org/abs/1412.2231

16. Magnus, J.: On diﬀerentiating eigenvalues and eigenvectors. Econometric Theory 1(2),
179–191 (1985). DOI 10.1017/s0266466600011129. URL http://dx.doi.org/10.1017/
s0266466600011129

17. Mohan, K., Fazel, M.: Iterative reweighted least squares for matrix rank minimization.
2010 48th Annual Allerton Conference on Communication, Control and Computing
(Allerton) (2010). DOI 10.1109/allerton.2010.5706969. URL http://dx.doi.org/10.
1109/allerton.2010.5706969

18. Rennie, J.D.M., Srebro, N.: Fast maximum margin matrix factorization for collabo-
In: Proceedings of the 22nd International Conference on Machine
rative prediction.
Learning, ICML ’05, p. 713–719. Association for Computing Machinery, New York, NY,
USA (2005). DOI 10.1145/1102351.1102441. URL https://doi.org/10.1145/1102351.
1102441

19. S. Ma, D.G., Chen, L.: Fixed point and Bregman iterative methods for matrix rank

minimization. Mathematical Programming 128, 321–353 (2009)

20. Sagan, A., Shen, X., Mitchell, J.E.: Two Relaxation Methods for Rank Minimization
Problems. Journal of Optimization Theory and Applications 186(3), 806–825 (2020).
DOI 10.1007/s10957-020-01731-

21. Shen, X., Mitchell, J.: A penalty method for rank minimization problems in symmetric
matrices. Computational Optimization and Applications 71(2), 353–380 (2018). DOI
10.1007/s10589-018-0010-6. URL http://dx.doi.org/10.1007/s10589-018-0010-6

28

April Sagan, John E. Mitchell

22. Srebro, N., Rennie, J.D.M., Jaakkola, T.S.: Maximum-margin matrix factorization. In:
Proceedings of the 17th International Conference on Neural Information Processing
Systems, NIPS’04, p. 1329–1336. MIT Press, Cambridge, MA, USA (2004)

23. T. Hastie R. Mazumder, J.D.L., Zadeh, R.: Matrix completion and low-rank svd via fast
alternating least squares. Journal of Machine Learning Research 16(104), 3367–3402
(2015). URL http://jmlr.org/papers/v16/hastie15a.html

24. Tanner, J., Wei, K.: Low rank matrix completion by alternating steepest descent meth-
ods. Applied and Computational Harmonic Analysis 40 (2015). DOI 10.1016/j.acha.
2015.08.003

25. Tasissa, A., Lai, R.: Exact reconstruction of euclidean distance geometry problem using
low-rank matrix completion. IEEE Transactions on Information Theory 65(5), 3124–
3144 (2019). DOI 10.1109/tit.2018.2881749. URL http://dx.doi.org/10.1109/tit.
2018.2881749

26. Trzasko, J., Manduca, A.: Highly undersampled magnetic resonance image reconstruc-
tion via homotopic (cid:96)0 -minimization. IEEE Transactions on Medical Imaging 28(1),
106–121 (2009)

27. Wen, Z., Yin, W., Zhang, Y.: Solving a low-rank factorization model for matrix com-
pletion by a nonlinear successive over-relaxation algorithm. Mathematical Program-
ming Computation 4(4), 333–361 (2012). DOI 10.1007/s12532-012-0044-1. URL
https://doi.org/10.1007/s12532-012-0044-1

28. Xu, Y., Yin, W.: A block coordinate descent method for regularized multiconvex opti-
mization with applications to nonnegative tensor factorization and completion. SIAM
Journal on Imaging Sciences 6(3), 1758–1789 (2013). DOI 10.1137/120887795. URL
https://doi.org/10.1137/120887795

29. Y. Lou, P.Y., Xin, J.: Point source super-resolution via non-convex l1 based methods. J.
Sci. Comput. 68(3), 1082–1100 (2016). DOI 10.1007/s10915-016-0169-x. URL https:
//doi.org/10.1007/s10915-016-0169-x

30. Yao, Q., Kwok, J., Zhong, W.: Fast low-rank matrix learning with nonconvex reg-

ularization.
10.1109/icdm.2015.9. URL http://dx.doi.org/10.1109/icdm.2015.9

2015 IEEE International Conference on Data Mining (2015). DOI

31. Yao, Q., Kwok, J.T., Gao, F., Chen, W., Liu, T.Y.: Eﬃcient inexact proximal gradient
algorithm for nonconvex problems. Proceedings of the Twenty-Sixth International Joint
Conference on Artiﬁcial Intelligence (2017). DOI 10.24963/ijcai.2017/462. URL http:
//dx.doi.org/10.24963/ijcai.2017/462

32. Yao, Q., Kwok, J.T., Wang, T., Liu, T.: Large-scale low-rank matrix learning with
nonconvex regularizers. IEEE Transactions on Pattern Analysis & Machine Intelligence
41(11), 2628–2643 (2019). DOI 10.1109/TPAMI.2018.2858249

33. Zhang, C.H.: Nearly unbiased variable selection under minimax concave penalty. The

Annals of Statistics 38(2), 894–942 (2010)

34. Zhang, D., Hu, Y., Ye, J., Li, X., He, X.: Matrix completion by truncated nuclear norm
regularization. 2012 IEEE Conference on Computer Vision and Pattern Recognition
pp. 2192–2199 (2012). DOI 10.1109/CVPR.2012.6247927

