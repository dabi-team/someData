1
2
0
2

y
a
M
0
2

]

C
O
.
h
t
a
m

[

1
v
6
1
7
9
0
.
5
0
1
2
:
v
i
X
r
a

A STOCHASTIC COMPOSITE AUGMENTED LAGRANGIAN
METHOD FOR REINFORCEMENT LEARNING∗

YONGFENG LI† , MINGMING ZHAO‡ , WEIJIE CHEN§ , AND ZAIWEN WEN¶

Abstract. In this paper, we consider the linear programming (LP) formulation for deep rein-
forcement learning. The number of the constraints depends on the size of state and action spaces,
which makes the problem intractable in large or continuous environments. The general augmented
Lagrangian method suﬀers the double-sampling obstacle in solving the LP. Namely, the conditional
expectations originated from the constraint functions and the quadratic penalties in the augmented
Lagrangian function impose diﬃculties in sampling and evaluation. Motivated from the updates
of the multipliers, we overcome the obstacles in minimizing the augmented Lagrangian function by
replacing the intractable conditional expectations with the multipliers. Therefore, a deep parameter-
ized augment Lagrangian method is proposed. Furthermore, the replacement provides a promising
breakthrough to integrate the two steps in the augmented Lagrangian method into a single con-
strained problem. A general theoretical analysis shows that the solutions generated from a sequence
of the constrained optimizations converge to the optimal solution of the LP if the error is controlled
properly. A theoretical analysis on the quadratic penalty algorithm under neural tangent kernel
setting shows the residual can be arbitrarily small if the parameter in network and optimization
algorithm is chosen suitably. Preliminary experiments illustrate that our method is competitive to
other state-of-the-art algorithms.

Key words. Deep reinforcement learning; Augmented Lagrangian method; Linear program-

ming; Oﬀ-policy learning

AMS subject classiﬁcations. 49L20, 90C15, 90C26, 90C40, 93E20

1. Introduction. With the recent development and empirical successes of the
neural networks, deep reinforcement learning has been resoundingly applied in many
industrial ﬁelds, such robotics and control, economics, automatic driving, etc. The
focus of reinforcement learning (RL) is solving a Markov decision process (MDP)
which models the framework for interactions between the agent and the environment.
In real-world problems, the issues such as high dimensional or/and continuous state
and action spaces, partially observed state informations, limited and noised samples,
unknown dynamics and etc., make RL signiﬁcantly harder than general deep learning
tasks.

A fundamental conclusion in RL is that the Bellman operator is contractive in
value function space and the optimal value function is the unique ﬁxed point of the
Bellman operation [7]. It is not easy to solve the Bellman equation greedily in large
state and action spaces or when the function parameterization is used because of the
non-smoothness and highly non-convexity. Instead of formulating such a ﬁxed point
iteration, value-based RL algorithms concentrate on minimizing the Bellman residual,
which is a square loss in general, to approximate the optimal value function with
stochastic gradient descent methods. However, there are still two troublesome issues

∗Submitted to the editors DATE.
Funding: Research supported in part by the NSFC grant 11831002 and by Beijing Academy of

Artiﬁcial Intelligence (BAAI).

†Beijing International Center

(yongfengli@pku.edu.cn).

for Mathematical Research, Peking University, CHINA

‡Beijing International Center

for Mathematical Research, Peking University, CHINA

(mmz102@pku.edu.cn).

§Academy for Advanced Interdisciplinary Studies, Peking University, CHINA(baoz@pku.edu.cn).
¶Corresponding author. Beijing International Center for Mathematical Research, Peking Univer-

sity,CHINA(wenzw@pku.eu.cn).

1

 
 
 
 
 
 
2

YONGFENG LI, MINGMING ZHAO, WEIJIE CHEN, ZAIWEN WEN

in this alternative formulation. One is the conditional expectation over the next state
suﬀers from the double-sampling obstacle in gradient formulation. In other words, the
objective function is a square of a conditional expectation of value functions, thus to
construct the unbiased gradient estimation in a stochastic behavior, two independent
sets of samples of the next transition state are required for each pair of state and
action. However, only one single transition state sample is observed in sampling cases
or model-free setting. This obstacle has oppressed the designs of RL algorithms in
practice. The other is that the max operator in the optimal Bellman equation exposes
the non-smoothness and diﬃculty of the optimization in complex environments. To
overcome the former challenge, several methods are proposed in recent researches.
Wang et al., [25] takes an additional variable to track the inner expectation and
proposes a class of stochastic compositional gradient descent algorithms by using
noisy sample gradients. Borrowing extra randomness from the future is proposed in
[26]. Conditioned on the assumption that the transition kernel can be described with
a stochastic diﬀerential equation, they propose two algorithms in which the training
trajectories are shown to be close to the one of the unbiased SGD. Deep Q-Network
(DQN) [16, 11] concentrates on the Bellman equation of the state and action value
function and predicts the value for the next state with the target parameter which
is periodically synchronized to the training parameters by minimizing the Bellman
residual. Under the oﬀ-policy setting, DQN performs SGD and achieves more than
human players level in Atari Games [2, 4]. As for the second problem that the max
operator in Bellman equation may be unavailable or expensive, an additional actor
network is introduced in [9, 14] to adapt to large or/and continuous environments.
By adding the scaled log-policy to the one-step reward in [23], the max operator is
slightly converted into a soft formulation. A smoothed Bellman equation is introduced
in [7] and it is reformulated as a novel primal-dual optimization problem.

The optimal Bellman equation can be interpreted as a linear programming (LP)
problem with respect to the state value function [3, 17]. The uniqueness of the ﬁxed
point of the Bellman operator guarantees the existence of the solution of the LP
problem. The dual form of the LP problem can also be viewed as an interpretation
of maximizing the cumulative reward in policy space. As the transition kernel of the
MDP may be unavailable, such as in model-free setting, the dual problem is rarely
considered since its constraint functions are hard to estimate. Consequently, the
equivalency between the primal and dual LP problems, as well as their relationship
to the optimal policy, provide a broad breakthrough for the design of RL algorithm.
By formulating the primal and dual LPs into an equivalent saddle-point problem, a
stochastic primal-dual algorithm is proposed in [24] to ﬁnd a near optimal policy in
tabular case. For large or even continuous state and action spaces, the bilinear struc-
ture of the Lagrangian function is destroyed because of the parameterization, the
general convergence result for the parametrized saddle-point problem no longer holds
due to the lack of convex-concavity. Furthermore, the large bias of sample estimations
and under-ﬁtted functions bring great disturbance to the stability of numerical per-
formance. To promote the local convexity and duality, a path regularization is added
to the objective function in [6] to restrict the feasible domain of the value function to
be a ball guided by the behavior policy, and the general Bellman residual is extended
to the multi-step version for a better bias-variance trade oﬀ. For sample eﬃciency,
the authors exploit stochastic dual ascent method and solve a sequence of partial
Lagrangian functions to update the value function for each policy. To overcome the
path dependency and time consuming in [6], the path regularization is replaced with
a quadratic penalty term with respect to the Bellman residual in [5] and a sequence of

A STOCHASTIC COMPOSITE ALM FOR REINFORCEMENT LEARNING

3

stochastic gradient updates for primal and dual variables are performed alternately.
In this work, we propose a novel stochastic optimization method for the primal
LP problem derived from the algorithmic framework of the augmented Lagrangian
method (ALM). The optimal policy can be recovered from a speciﬁc relationship
with the multipliers [5, 6]. Speciﬁcally, a weighted augmented Lagrangian function
rather than the standard form is constructed based on a weighted summation of the
quadratic penalty terms deﬁned by the multipliers and the constraints. The weights
are functions on state and action spaces. They enable the sampling process and are
not required in practice. As the constraint functions include conditional expectations,
the quadratic terms induce a challenge to the calculation of the weighted augmented
Lagrangian function, as well as a double sampling obstacle in its gradient estima-
tion. Actually, it is claimed in the appendix of [6] that minimizing the augmented
Lagrangian function itself is quite diﬃcult.
Our main contributions are as follows:

• Motivated by the updates of the multipliers in ALM, we overcome these is-
sues by replacing the intractable conditional expectations in the weighted
augmented Lagrangian function with the multipliers. The replacement gets
rid of the those diﬃculties without modifying the one-step reward, introduc-
ing smoothed Bellman equation and extra computations as in [7, 23, 26].
More crucially, it motivates a promising breakthrough to equivalently refor-
mulate the two steps in ALM into a single constrained optimization problem.
For practical consideration, we construct a quadratic penalty function for the
constrained problem and employ the semi-gradient update with a target net-
work for primal variables. Afterwards, a practical and stochastic optimization
method, dubbed as SCAL, is developed.

• We establish theoretical analysis for the proposed algorithm. The algorithm
can be regarded as an inexact augmented Lagrangian method. The error is
generated from two parts: minimizing the augmented Lagrangian function
and the update of multiplier. If these two error terms can be controlled prop-
erly, the algorithm converges to the optimal solution of the LP problems.
Then we present a speciﬁc analysis under neural tangent kernel (NTK) set-
ting, where the variables behave as linear functions, so that the inner loop
is similar to the optimization of convex functions. It gives a bound of the
error between an outer iteration of our algorithm and an exact augmented
Lagrangian update. Furthermore, we get that the residual can be arbitrarily
small if the network is suﬃciently wide and the parameters in algorithm is
chosen properly.

• To demonstrate the learning capability of SCAL, we take an ablation study by
investigating the eﬀects of the double-sampling solution on gradient variance
and varying the number of rollout steps, and perform comprehensive numer-
ical experiments. The results imply that our algorithm has a competitive
learning potential.

The paper is organized as follows. In section 2, we clarify the LP formulations
in RL, then introduce the ALM framework. In section 3, we analyze the diﬃculties
in evaluating the parameterized augmented Lagrangian function and its gradient,
and present an optimization algorithm which is equivalent to ALM but easier to
calculate. We further propose a single constrained optimization problem in section
4 to integrate the two steps in ALM. Some theoretical conclusions are introduced in
section 5. The implementation details are explained in section 6. Comprehensive
numerical experiments with several state-of-the-art RL algorithms are provided in

YONGFENG LI, MINGMING ZHAO, WEIJIE CHEN, ZAIWEN WEN

4

section 7.

2. Preliminaries. We consider the standard reinforcement learning setting char-
acterized by an inﬁnite-horizon discounted MDP. Formally, a MDP is described by
the tuple (S, A, P, r, ρ0, γ). We assume that S and A represent the ﬁnite state and
action spaces, respectively. P : S × A × S → R indicates the transition dynamics
by a conditional distribution over the state and action spaces, r : S × A → R is the
bounded reward function. The initial state s0 of the MDP obeys the distribution
ρ0 : S → R, and γ ∈ (0, 1) is the discount factor.

2.1. LP Formulation. Generally, we denote the total expected reward as the
expectation of the cumulative discounted reward of a trajectory induced by the policy
π:

(2.1)

η(π) = Eπ

(cid:34) ∞
(cid:88)

(cid:35)
γtr(st, at)

,

t=0

where s0 ∼ ρ0, at ∼ π(·|st), st+1 ∼ P (·|st, at). The optimization problem is

(2.2)

where

max
π∈Π

η(π),

(cid:40)

Π =

π|

(cid:88)

a∈A

π(a|s) = 1, π(a|s) ≥ 0, ∀a ∈ A, s ∈ S

.

(cid:41)

The optimal state value function is the maximal expected cumulative discounted
reward from state s, i.e.,

(2.3)

V ∗(s) = max
π∈Π

Eπ

(cid:34) ∞
(cid:88)

t=0

γtr(st, at)

(cid:35)
(cid:12)
(cid:12)
(cid:12)s0 = s

.

Respectively, the optimal Bellman equation states that

(2.4)

V ∗(s) = (T V ∗)(s) = max

{r(s, a) + γEs(cid:48)|s,a[V ∗(s(cid:48))]}, ∀s,

a

which is a ﬁxed point equation of V ∗(s). As the Bellman operator T is contractive,
the state value function V ∗(s) in (2.3) is the unique solution of (2.4). Furthermore,
the optimal Bellman equation can be interpreted as solving the LP problem [3, 17]:

(2.5)

(P) : min

V

(cid:88)

s

ρ0(s)V (s),

s.t. V (s) ≥ r(s, a) + γEs(cid:48)|s,a[V (s(cid:48))], ∀s, a.

The optimal policy π∗ can be obtained from the solution of (2.5) via

(2.6)

π∗(a|s) =

(cid:40)

1 a = arg maxa(cid:48) r(s, a(cid:48)) + γEs(cid:48)|s,a[V ∗(s(cid:48))],
0

o.w.

Actually, any policy that assigns positive probability only to the maximizers (i.e.,
actions) of the right side at state s can be regarded as an optimal policy π∗. We

A STOCHASTIC COMPOSITE ALM FOR REINFORCEMENT LEARNING

5

just denote the deterministic case (2.6) for simplicity. In fact, a closer relation to the
optimal policy π∗ comes from an equivalent form of the dual of the LP problem

(2.7)

(D) : max
x≥0

s.t.

(cid:88)

s,a
(cid:88)

a

r(s, a)w(s, a)x(s, a),

w(s(cid:48), a)x(s(cid:48), a) −

(cid:88)

s,a

γP (s(cid:48)|s, a)w(s, a)x(s, a) = ρ0(s(cid:48)), ∀s(cid:48),

where the weight function w is a distribution with respect to the state and action
spaces, i.e.,

(cid:88)

s∈S,a∈A

w(s, a) = 1.

The only diﬀerence from the standard dual formulation is that the variables x(s, a) car-
ries the weight w(s, a). Actually, x can be viewed as the multipliers using a weighted
inner product other than the standard case in Euclidean space. Moreover, the weight
function w(s, a) enables us to perform a sampling process from a replay buﬀer in large
and complex environments. In practice, the explicit form of w(s, a) is not required as
it simply characterizes the sampling behavior. It can be veriﬁed that [5, 6]

(2.8)

(cid:88)

s,a

w(s, a)x∗(s, a) =

1
1 − γ

, and π∗(a|s) =

w(s, a)x∗(s, a)
a(cid:48) w(s, a)x∗(s, a(cid:48))

(cid:80)

,

where x∗ is the solution of (2.7).

Generally, the goal of RL is computing the ﬁxed point of the optimal Bellman
equation or the maximizer of (2.2). Alternatively, we can focus on the LP problem
(2.5) or (2.7) in an equivalent way. However, as the transition probability P (s(cid:48)|s, a)
is usually not accessible in real-world problems or model-free setting, the summation
over s, a in the constraint function of the dual form (2.7) limits the estimation from
sampling process. Therefore, the primal LP problem (2.5) is considered in most cases.

2.2. The Classic ALM. To simplify the following derivation, we consider an
equivalent primal LP formulation with equality constraints by introducing an addi-
tional slack variable. Namely, we reformulate (2.5) as

(2.9)

min
h≥0,V

(cid:88)

s

ρ0(s)V (s),

s.t. V (s) = r(s, a) + γEs(cid:48)|s,a[V (s(cid:48))] + h(s, a), ∀s, a.

If a∗ is the optimal action in state s, then the connection between the optimal policy
and the optimal slack variable is,

(2.10)

(cid:40)

π∗(a|s) = 1, h∗(s, a) = 0, a = a∗,
π∗(a|s) = 0, h∗(s, a) > 0, a (cid:54)= a∗.

We denote x(s, a) as the multiplier of the constraints in (2.9). For simplicity, we
introduce a Z-function as

Zµ(V, h, x, s, a) = x(s, a) + µ (cid:0)h(s, a) + r(s, a) + γEs(cid:48)|s,a[V (s(cid:48))] − V (s)(cid:1) ,

6

YONGFENG LI, MINGMING ZHAO, WEIJIE CHEN, ZAIWEN WEN

and deﬁne

zµ(V, h, x, s, a, s(cid:48)) = x(s, a) + µ (h(s, a) + r(s, a) + γV (s(cid:48)) − V (s)) ,

where µ > 0 is the penalty parameter. Since

(2.11)

we have

(2.12)

P (s(cid:48)|s, a) = 1,

(cid:88)

s(cid:48)

Zµ(V, h, x, s, a) = Es(cid:48)|s,a[zµ(V, h, x, s, a, s(cid:48))].

That is to say, Z-function is a conditional expectation of s(cid:48). For the LP problem
(2.9), the weighted augmented Lagrangian function is deﬁned as

(2.13)

Lµ(V, h, x) =

(cid:88)

s

ρ0(s)V (s) +

1
2µ

(cid:88)

s,a

w(s, a)[Zµ(V, h, x, s, a)]2.

We use superscript k for all variables to denote the iteration index. At the k-th

iteration, ALM can be written as

(2.14)

(2.15)

hk+1, V k+1 = arg min
h≥0,V

Lµ(V, h, xk),

xk+1(s, a) = Zµ(V k+1, hk+1, xk, s, a), ∀s, a.

The general ALM performs these two steps repeatedly until some terminal conditions
are satisﬁed. A general pseudo-code of ALM is given in Algorithm 2.1. Obviously,

Algorithm 2.1 The Augmented Lagrangian Method
1: Choose initial points V 0, h0, x0, set µ, w.
2: for k = 1, 2, ... do
3:

update V k+1, hk+1 from the optimization (2.14);
update the multiplier xk+1(s, a) for each s, a with (2.15);

4:
5: end for

the variable size of the optimization (2.14) is the same as that of the state space S.
Moreover, when the state and action spaces are extremely large or even continuous,
the element-wise update in (2.15) is hard to compute. Thus, the general ALM is
impractical due to the computational obstacle.

3. A Deep Parameterized ALM. In this part, we ﬁrst consider a parame-
terized formulation to approximate the unaccessible functions in ALM and analyze
its challenges in sampling and evaluation. Then we propose an approximate function
which is easy to compute and present an algorithm for solving the deep parameterized
ALM.

3.1. Parameterization and Strategies for Double-Sampling. We take the
deep neural networks to approximate the value function, slack function and multipli-
ers. Suppose that V , h and x are parameterized by Vφ, hψ and xθ, respectively. The
non-negative property of hψ is automatically guaranteed by the network. For simplic-
ity, we directly replace Vφ, hψ, xθ in the following discussion by φ, ψ and θ without
extra explanation.

A STOCHASTIC COMPOSITE ALM FOR REINFORCEMENT LEARNING

7

Let the superscripts φk, ψk and θk, represent the parameters of the networks in the
k-th iteration. Respectively, the weighted augmented Lagrangian function in (2.13)
is denoted as Lk
µ(φ, ψ) is
an expectation of the squared Z-function over states and actions, and the Z-function
itself is a conditional expectation over the next state. Thus, the computation of
Lk
µ(φ, ψ)
with respect to φ satisﬁes

µ(φ, ψ) is impractical from sampling process. Furthermore, the gradient of Lk

µ(φ, ψ). According to the deﬁnition, the second term in Lk

∇φLk

µ(φ, ψ) −

(cid:88)

s

ρ0(s)∇φVφ(s)

(cid:88)

w(s, a)Zµ(φ, ψ, θk, s, a) (cid:0)γEs(cid:48)|s,a[∇φVφ(s(cid:48))] − ∇φVφ(s)(cid:1)

s,a
(cid:88)

s,a,s(cid:48)

pw(s, a, s(cid:48))Zµ(φ, ψ, θk, s, a) (γ∇φVφ(s(cid:48)) − ∇φVφ(s)) ,

(3.1)

=

=

where pw(s, a, s(cid:48)) = w(s, a)P (s(cid:48)|s, a) can be viewed as a probability function of tran-
sition tuples. The last equation follows from (2.11). The right-hand term in (3.1)
contains two expectations of the next state s(cid:48) for each pair (s, a): one is the gradient
of the value function at s(cid:48) and the other comes from the deﬁnition of the Z-function.
µ(φ, ψ), independent samples of the next state s(cid:48)
Thus, to estimate the gradient ∇φLk
are required for evaluating ∇φVφ(s(cid:48)) and approximating Zµ(φ, ψ, θk, s, a) separately,
while only one s(cid:48) is observed from the pair (s, a) in sampling case. This arises the
well-known double-sampling obstacle.

To overcome the challenges of estimating Lk

µ(φ, ψ) and its gradient, we consider
approximating the intractable function Zµ(φ, ψ, θk, s, a) with an additional function
yτ (s, a). The weight of the network τ is trained to approximate the Z-function with
a weighted least square regression, i.e.,

(3.2)

min
τ

(cid:88)

s,a

w(s, a) (cid:0)Zµ(φ, ψ, θk, s, a) − yτ (s, a)(cid:1)2

.

By adding a term unrelated to τ as

(cid:88)

s,a,s(cid:48)

pw(s, a, s(cid:48))zµ(φ, ψ, θk, s, a, s(cid:48))2 −

w(s, a)Zµ(φ, ψ, θk, s, a)2

(cid:88)

s,a

to the objective function of (3.2) and using the relationships in (2.11) and (2.12), we
can equivalently transform the optimization problem (3.2) into

(3.3)

min
τ

(cid:88)

s,a,s(cid:48)

pw(s, a, s(cid:48)) (cid:0)zµ(φ, ψ, θk, s, a, s(cid:48)) − yτ (s, a)(cid:1)2

,

which enables stochastic gradient optimization by taking samples on the transition
tuples (s, a, r, s(cid:48)). Consequently, an approximation of Lk

µ(φ, ψ) is deﬁned as

(3.4)

˜Lk

µ(φ, ψ, τ ) =

=

ρ0(s)Vφ(s) +

ρ0(s)Vφ(s) +

1
µ

1
µ

(cid:88)

s
(cid:88)

s

(cid:88)

w(s, a)yτ (s, a)Zµ(φ, ψ, θk, s, a)

s,a
(cid:88)

s,a,s(cid:48)

pw(s, a, s(cid:48))yτ (s, a)zµ(φ, ψ, θk, s, a, s(cid:48)),

8

YONGFENG LI, MINGMING ZHAO, WEIJIE CHEN, ZAIWEN WEN

which is a summation of ordinary expectations over initial states and transition tu-
ples. Therefore, its value and gradient are relatively easy to compute. Suppose that
yτ (s, a) is the solution of (3.2) with respect to φ, ψ, then the gradients ∇φ ˜Lk
µ(φ, ψ, τ )
and ∇ψ ˜Lk
µ(φ, ψ), respectively. Further-
more, if (φk+1, ψk+1) is the minimizer of Lk
µ(φ, ψ), then the corresponding function
yτ k+1(s, a) is identical to the multiplier xθk+1(s, a). In other words, it implies that
the optimization problem (3.2) is equivalent to the multiplier update in (2.15). These
properties provide a particular strategy on solving the deep parameterized ALM.

µ(φ, ψ, τ ) are exactly the same as those of Lk

3.2. A Deep Parameterized ALM. Based on the proposed strategy for han-
dling double-sampling obstacle in Lk
µ(φ, ψ), we can directly take the multiplier xθ
to substitute the additional function yτ in (3.3), as well as in (3.4). Generally, by
denoting

˜Lk

µ(φ, ψ, θ) =

(cid:88)

s

ρ0(s)Vφ(s) +

1
µ

(cid:88)

s,a,s(cid:48)

the deep parameterized ALM performs

pw(s, a, s(cid:48))xθ(s, a)zµ(φ, ψ, θk, s, a, s(cid:48)),

φk+1, ψk+1 = arg min
φ,ψ
θk+1 = arg min

θ

˜Lk

µ(φ, ψ, θk),
(cid:88)

pw(s, a, s(cid:48))[zµ(φk+1, ψk+1, θk, s, a, s(cid:48)) − xθ(s, a)]2.

s,a,s(cid:48)

The multiplier update is similar as the derivation from (3.2) to (3.3). Practically, we
can approximate these two minimization problems with single or multiple stochastic
gradient steps. Namely, at k-th iteration, starting from φk
0 = φk, ψk
0 = ψk and
0 = θk, the following three operations are performed at each inner step t:
θk

(3.5)

(3.6)

(3.7)

φk
t+1 = φk
ψk
t+1 = ψk
t+1 = θk
θk

t − βφ · ˆ∇φ ˜Lk
t , θk
t , ψk
t − βψ · ˆ∇ψ ˜Lk
t+1, ψk
t − βθ · (cid:0)xθ(s, a) − zµ(φk

µ(φk
µ(φk

t+1),
t , θk
t , ψk

t+1),
t , θk, s, a, s(cid:48))(cid:1) ∇θxθ(s, a),

where βφ, βψ, βθ are the step sizes. The inner iteration terminates until some stop
t are assigned to φk+1, ψk+1 and
t , ψk
criterion holds, and the last iterators φk
θk+1, respectively.

t and θk

4. A Stochastic Composite Augmented Lagrangian Algorithm. The mul-
tiplier xθ(s, a) may not ﬁt the Z-function well in the deep parameterized ALM since
the number of samples is limited and the step (3.7) is inexact. We observe that
the performance is not stable in practice and the approximate function ˜Lk
µ(φ, ψ, θ)
is often not possible to generate desirable update direction for the primal variable
φ and slack variable ψ. Therefore, the iteration (3.5)-(3.7) is highly sensitive to the
hyper-parameters and usually results in an unstable performance.

4.1. A Quadratic Penalty Optimization. Based on the connections between
µ(φ, ψ), we can incorporate the two steps in parameterized ALM with

˜Lk
a single constrained optimization. We consider the following constrained problem

µ(φ, ψ, θ) and Lk

(4.1)

min
φ,ψ,θ

˜Lk

µ(φ, ψ, θ), s.t. xθ(s, a) = Zµ(φ, ψ, θk, s, a), ∀s, a.

It is worth noting that, the coeﬃcient in the second term of ˜Lk
diﬀerent from that of the augmented Lagrangian function (2.13), i.e., 1

µ(φ, ψ, θ) is 1

µ , which is
2µ . However,

A STOCHASTIC COMPOSITE ALM FOR REINFORCEMENT LEARNING

9

˜Lk
µ(φ, ψ, θ) is equivalent to the augmented Lagrangian function of the primal LP
problem (2.5) when its objective function is multiplied with two. Actually, multiplying
any positive scalar for the LP problem does not change the solution of (2.5). That is
to say, the constrained optimization (4.1) is an integration of the two steps in ALM.
We can simultaneously accomplish the updates of the primal, slack and multiplier
variables by solving a single constrained optimization problem.

In order to approximately solve (4.1), we consider penalizing the constraints into

the objective function to form a quadratic penalty optimization, i.e.,

min
φ,ψ,θ

β,µ(φ, ψ, θ) := ˜Lk
˜Lk

µ(φ, ψ, θ)

(4.2)

+

β
2

(cid:88)

s,a

w(s, a) (cid:2)xθ(s, a) − (cid:0)Zµ(φ, ψ, θk, s, a)(cid:1)(cid:3)2

,

where β > 0. We employ the semi-gradient for φ, speciﬁcally, the term r(s, a) +
γ (cid:80)
s(cid:48) P (s(cid:48)|s, a)Vφ(s(cid:48)) in Z-function is treated as a target. This is a bootstrapping
way originated from temporal-diﬀerence learning and is commonly used in many RL
algorithms, such as in [16, 24]. Therefore, we introduce a target value network Vφtarg
as the usage in DQN. Additionally, we also replace xθk in (4.2) with a target multiplier
network xθtarg which shares the same structure as that of xθ. We denote

(φ, ψ, s, a, s(cid:48)) = xθtarg (s, a) + µ (hψ(s, a) + r(s, a) + γVφtarg (s(cid:48)) − Vφ(s)) ,

ztarg
µ
Z targ
µ

(φ, ψ, s, a) = Es(cid:48)|s,a[ztarg

µ

(φ, ψ, s, a, s(cid:48))].

Then the objective function in (4.2) is reformulated as

(4.3)

ˆLk

β,µ(φ, ψ, θ)

(cid:88)

:=

ρ0(s)Vφ(s) +

s
β
2

+

(cid:88)

s,a

1
µ

(cid:88)

s,a,s(cid:48)

pw(s, a, s(cid:48))xθ(s, a)ztarg

µ

(φ, ψ, s, a, s(cid:48))

w(s, a) (cid:2)xθ(s, a) − (cid:0)Z targ

µ

(φ, ψ, s, a)(cid:1)(cid:3)2

.

By adding a term unrelated to φ, θ and ψ as

(cid:88)

s,a,s(cid:48)

pw(s, a, s(cid:48))[xθtarg (s, a) + µ (r(s, a) + γVφtarg (s(cid:48)))]2

(cid:88)

−

s,a

(cid:32)

(cid:34)

w(s, a)

xθtarg (s, a) + µ

r(s, a) + γ

P (s(cid:48)|s, a)Vφtarg (s(cid:48))

(cid:35)(cid:33)2

(cid:88)

s(cid:48)

to the last square function of (4.3), we can rearrange it as

ˆLk

β,µ(φ, ψ, θ)

(cid:88)

=

ρ0(s)Vφ(s) +

s

+

β
2

(cid:88)

s,a,s(cid:48)

1
µ

(cid:88)

s,a,s(cid:48)

pw(s, a, s(cid:48))xθ(s, a)ztarg

µ

(φ, ψ, s, a, s(cid:48))

pw(s, a, s(cid:48)) (cid:0)xθ(s, a) − ztarg

µ

(φ, ψ, s, a, s(cid:48))(cid:1)2

,

which can be easily evaluated with states and transitions sampled from ρ0 and pw(s, a, s(cid:48)),
respectively. In the k-th iteration, we need to solve the following optimization problem

(4.4)

φk+1, ψk+1, θk+1 = arg min
φ,ψ,θ

ˆLk

β,µ(φ, ψ, θ)

10

YONGFENG LI, MINGMING ZHAO, WEIJIE CHEN, ZAIWEN WEN

with appropriate hyper-parameters.

4.2. Stochastic Optimization. We next consider the sampling process and
give a stochastic algorithm. To be clear in advance, as the explicit form of the tran-
sition probability P (s(cid:48)|s, a) is usually unknown in practice, our derivation is con-
structed in the model-free setting. Therefore, to promote the data eﬃciency in an
online RL setting, an experience replay buﬀer D is adopted to store the initial states
{s0} and the transition tuples {(si, ai, ri, s(cid:48)
i)} encountered in each episode. At each
step, we randomly get a batch of initial state samples {s0,i}b
i=1 and transition tuples
{(si, ai, ri, s(cid:48)
i=1 (b is the batch size) from the replay buﬀer D, in which the samples
are assumed to be sampled from the probability ρ0(s) and pw(s, a, s(cid:48)), respectively.
We denote

i)}b

µ

(φk, ψk, si, ai, s(cid:48)

ei =β (cid:0)ztarg
gi =∇φVφk (s0,i) − (xθk (si, ai) + µei) ∇φVφk (si),
qi =(xθk (si, ai) + µei)∇ψhψk (si, ai),

i) − xθk (si, ai)(cid:1) ,

(4.5)

mi =

1
µ

(cid:0)ztarg

µ

(φk, ψk, si, ai, s(cid:48)

i) − µei

(cid:1) ∇θxθk (si, ai),

and update the parameters as

(4.6)

φk+1 =φk −

ψk+1 =ψk −

θk+1 =θk −

αφ
b

αψ
b

b
(cid:88)

i=1

b
(cid:88)

i=1

gi,

qi,

αθ
b

b
(cid:88)

i=1

mi,

where αφ, αθ, αψ > 0 are step size. We assign the current parameters θk and φk
to the targets θtarg and φtarg for every T iterations. The update rules in (4.6) is a
type of Jacobi iteration. Alternatively, we can apply Gauss-Seidel iteration. However,
the gradient estimation in deep neural network composes the forward and backward
propagation. Thus, in consideration of the computational cost, Jacobi iteration is
more practical.

In conclusion, we instantiate our method named SCAL in Algorithm 4.1, com-
bined with an experience replay D with a behavior policy (Line 4-5). Intuitively, the
multiplier xθk is approaching xθ∗ as the algorithm iterates. Therefore, we can exploit
the policy obtained in previous iteration, i.e., xθk , as the behavior policy πb. Lines
6-7 correspond to the stochastic gradient descents in (4.6).

5. Theoretical Analysis.

5.1. General convergence. In this section, we analyze the convergence of the
iteration (4.1). It is worth recalling the fact that the update (4.1) is equivalent to an
approximated augmented Lagrangian update (2.14)-(2.15) upto a scaling. Multiplying
the objective function in (2.5) by a constant does not change the optimal solution of
(2.5). We consider the diﬀerence between exact augmented Lagrangian update and

A STOCHASTIC COMPOSITE ALM FOR REINFORCEMENT LEARNING

11

Algorithm 4.1 A Stochastic Composite ALM (SCAL)
1: Initialize the points φ0, ψ0, θ0. Set step size αφ, αψ, αθ ∈ (0, 1).
2: Set φtarg = φ0 and θtarg = θ0;
3: for k = 0, ..., K do
4:

add a new transition (s, a, r, s(cid:48)) into D by executing the behavior policy πb.
sample {(si, ai, ri, s(cid:48)
compute {gi}b
update φk+1, ψk+1 and θk+1 using (4.6) ;
if k % T == 0 then

i=1 and {s0,i}b
i=1 and {mi}b

i=1from D;
i=1 using (4.5);

i)}b
i=1, {qi}b

Set φtarg = φk+1 and θtarg = θk+1;

5:
6:
7:
8:
9:

end if
10:
11: end for

the iteration (4.1). Deﬁne the error terms

(cid:15)k
L = Lµ(Vφk+1, hψk+1, xθk ) − inf
h≥0,V

Lµ(V, h, xθk ),

(cid:15)k
x =

(cid:88)

s,a

w(s, a) (cid:0)xθk+1(s, a) − Zµ(Vφk+1, hψk+1 , xθk , s, a)(cid:1)2

.

L and (cid:15)k

Notice that the error terms (cid:15)k
x contain the optimization error and parameter-
ization error. The ﬁrst part is generated from the optimization algorithm and the
second part is due to the parametrization way to represent variables. We show that if
the error terms are bounded well, the update (4.1) converges to the optimal solution
of (2.7).

The following lemma gives a connection between the proximal point method and
It shows that Zµ(V, h, x, s, a) is a solution of an

augmented Lagrangian method.
optimization problem.

Lemma 5.1. If V ∗, h∗ = arg minh≥0,V Lµ(V, h, x(cid:48)), then the term Zµ(V ∗, h∗, x(cid:48), s, a)

is an optimal solution of the following problem

max
x

s.t.

(cid:88)

s,a
(cid:88)

s,a

(5.1)

r(s, a)w(s, a)x(s, a) −

1
2µ

(cid:88)

s,a

w(s, a)(x(s, a) − x(cid:48)(s, a))2

(δs(cid:48)(s) − γP (s(cid:48)|s, a))w(s, a)x(s, a) = ρ0(s(cid:48)), ∀s(cid:48)

x(s, a) ≥ 0, ∀s, a.

The optimal value of problem (5.1) is equal to

(5.2)

Fµ(V ∗, h∗, x(cid:48)) := Lµ(V ∗, h∗, x(cid:48)) −

1
2µ

(cid:88)

s,a

w(s, a)x(cid:48)(s, a)2.

Proof. The KKT condition of the problem (5.1) can be written as

x(s, a) = x(cid:48)(s, a) + µ

h(s, a) + r(s, a) +

(cid:32)

γP (s(cid:48)|s, a)V (s(cid:48)) − V (s)

, ∀s, a,

(cid:33)

(cid:88)

s(cid:48)

(5.3)

(cid:88)

(δs(cid:48)(s) − γP (s(cid:48)|s, a))w(s, a)x(s, a) = ρ0(s(cid:48)), ∀s(cid:48),

s,a

x(s, a) ≥ 0, h(s, a) ≥ 0, ∀s, a,

12

YONGFENG LI, MINGMING ZHAO, WEIJIE CHEN, ZAIWEN WEN

where V and h are the dual variables. Then, x∗ is an optimal solution if and only
if there exists variables V and h such that x∗, V and h satisfy the condition (5.3).
Suppose that V ∗, h∗ = arg minh≥0,V Lµ(V, h, x(cid:48)). We have the optimality condition

ρ0(s(cid:48)) −

(cid:88)

w(s, a)Zµ(V ∗, h∗, x(cid:48), s, a)(δs(cid:48)(s) − γP (s(cid:48)|s, a)) = 0, ∀s(cid:48)

s,a
(cid:34)

(cid:32)

µh(x, a) =

−x(cid:48)(s, a) − µ

r(s, a) +

γP (s(cid:48)|s, a)V (s(cid:48)) − V (s)

(cid:33)(cid:35)

(cid:88)

s(cid:48)

.

+

(5.4)

Let

x(s, a) = Zµ(V ∗, h∗, x(cid:48), s, a)

(cid:34)

(cid:32)

=

x(cid:48)(s, a) + µ

r(s, a) +

γP (s(cid:48)|s, a)V (s(cid:48)) − V (s)

≥ 0.

(cid:33)(cid:35)

+

(cid:88)

s(cid:48)

Then the KKT condition (5.3) holds, i.e., x(s, a) = Zµ(V ∗, h∗, x(cid:48), s, a) is an optimal
solution of (5.1).

Let x∗ = Zµ(V ∗, h∗, x(cid:48), s, a), then we obtain

Fµ(V ∗, h∗, x(cid:48))
(cid:88)

ρ0(s)V (s) +

=

s
(cid:88)

=

ρ0(s)V (s) +

1
2µ

1
µ

s,a
(cid:88)

s,a

s

−

1
2µ

(cid:88)

s,a

w(s, a)(x∗(s, a) − x(cid:48)(s, a))2

(cid:88)

w(s, a)x∗(s, a)2 −

1
2µ

(cid:88)

s,a

w(s, a)x(cid:48)(s, a)2

w(s, a)x∗(s, a)(x∗(s, a) − x(cid:48)(s, a))

(cid:88)

=

s,a

r(s, a)w(s, a)x∗(s, a) −

1
2µ

(cid:88)

s,a

w(s, a)(x∗(s, a) − x(cid:48)(s, a))2,

where the last inequality is due to the ﬁrst equation in (5.4) and the deﬁnition of
Zµ(V ∗, h∗, x(cid:48), s, a), which completes the proof.

The next lemma shows the noise between one exact augmented Lagrangian update
and the corresponding approximated update can be bounded by the two error terms
x and (cid:15)k
(cid:15)k
L.

Lemma 5.2. Suppose that θk is generated by the iteration (4.1). Deﬁne ˆV k, ˆhk =

arg minh≥0,V Lµ(V, h, xθk ) and let

ˆxk+1(s, a) = Zµ( ˆV k, ˆhk, xθk , s, a).

Then we have

(cid:88)

s,a

w(s, a)(xθk+1(s, a) − ˆxk+1(s, a))2 ≤ 2(cid:15)k

x + 4µ(cid:15)k
L.

Proof. Let ¯xk+1 = Zµ(Vφk+1, hψk+1, xθk , s, a) and Fµ(V, h, x) is deﬁned in (5.2).

It is easy to show that Fµ(V, h, x) be a concave function with respect to x and

[∇xFµ(V, h, x)](s, a) =

1
µ

ω(s, a) (Zµ(V, h, x, s, a) − x(s, a)) .

A STOCHASTIC COMPOSITE ALM FOR REINFORCEMENT LEARNING

13

It follows that for any κ(s, a),

(5.5)

Fµ(Vφk+1, hψk+1, xθk )

+

1
µ

(cid:88)

s,a

w(s, a)(¯xk+1(s, a) − xθk (s, a))(κ(s, a) − xθk (s, a))

≥ Fµ(Vφk+1, hψk+1, κ) ≥ inf
h≥0,V

Fµ(V, h, κ).

According to Lemma 5.1, the value inf h≥0,V Fµ(V, h, κ) is equal to the optimal value
of the following problem

max
x

s.t.

(cid:88)

s,a
(cid:88)

s,a

(5.6)

r(s, a)w(s, a)x(s, a) −

1
2µ

(cid:88)

s,a

w(s, a)(x(s, a) − κ(s, a))2

(δs(cid:48)(s) − γP (s(cid:48)|s, a))w(s, a)x(s, a) = ρ0(s(cid:48)), ∀s(cid:48)

x(s, a) ≥ 0, ∀s, a.

Since ˆxk+1(s, a) satisﬁes the constraints of problem (5.6), it implies

(5.7)

inf
h≥0,V

Fµ(V, h, κ) ≥

(cid:88)

s,a

r(s, a)w(s, a)ˆxk+1(s, a)

−

1
2µ

(cid:88)

s,a

w(s, a)(ˆxk+1(s, a) − κ(s, a))2.

A similar argument yields

(5.8)

inf
h≥0,V

Fµ(V, h, xθk ) =

(cid:88)

s,a

r(s, a)w(s, a)ˆxk+1(s, a)

−

1
2µ

(cid:88)

s,a

w(s, a)(ˆxk+1(s, a) − xθk (s, a))2.

Combining (5.6), (5.7) and (5.8), we obtain

(cid:15)k
L = Lµ(Vφk+1 , hψk+1, xθk ) − inf
h≥0,V
= Fµ(Vφk+1 , hψk+1, xθk ) − inf
h≥0,V

Lµ(V, h, xθk )

Fµ(V, h, xθk )

(cid:88)

w(s, a)(¯xk+1(s, a) − xθk (s, a))(κ(s, a) − xθk (s, a))

≥ −

1
µ

−

1
2µ

+

1
2µ

s,a
(cid:88)

s,a
(cid:88)

s,a

w(s, a)(ˆxk+1(s, a) − κ(s, a))2

w(s, a)(ˆxk+1(s, a) − xθk (s, a))2

=

1
µ

−

(cid:88)

w(s, a)(κ(s, a) − xθk (s, a))(ˆxk+1(s, a) − ¯xk+1(s, a))

s,a
1
2µ

(cid:88)

s,a

w(s, a)(κ(s, a) − xθk (s, a)).

14

YONGFENG LI, MINGMING ZHAO, WEIJIE CHEN, ZAIWEN WEN

Since κ is arbitrary, taking κ = ˆxk+1 − ¯xk+1 + xθk gives

(cid:15)k
L ≥

1
2µ

(cid:88)

s,a

w(s, a)(ˆxk+1(s, a) − ¯xk+1(s, a))2.

It follows that

(cid:88)

w(s, a)(xθk+1(s, a) − ˆxk+1(s, a))2

s,a
(cid:88)

≤ 2

w(s, a)(xθk+1(s, a) − ¯xk+1(s, a))2

s,a

(cid:88)

+2

w(s, a)(¯xk+1(s, a) − ˆxk+1(s, a))2

s,a
x + 4µ(cid:15)k
L,

≤ 2(cid:15)k

which completes the proof.

We next give the main convergence theorem.
Theorem 5.3. If the error terms (cid:15)k

k (cid:15)k
L and (cid:15)k
∞, then xθk generated by iteration (4.1) converges to the optimal solution of (2.5).

x satisfy that (cid:80)

L < ∞ and (cid:80)

k (cid:15)k

x <

Proof. We ﬁrst claim that iteration (4.1) is equalvalent to one iteration in the

augmented Lagrangian method under parameterization, i.e.,

(5.9)

φk+1, ψk+1 = arg min
φ,ψ

θk+1 = arg min

θ

1
2

s
(cid:88)

s,a

(cid:88)

ρ0(s)Vφ(s) +

1
2µ

(cid:88)

s,a

w(s, a)[Zµ(Vφ, hψ, xθk , s, a)]2,

w(s, a) (cid:0)Zµ(φ, ψ, θk, s, a) − xθ(s, a)(cid:1)2

.

The constraint in (4.1) is the second update in (5.9). If we replace x by the constraints
in ˜Lk
µ(φ, ψ, θ), the optimization problem is the ﬁrst step in update (5.9). According to
Lemmas 5.1 and 5.2, the iterations (5.9) are equal to the proximal point method with
x + 4µ(cid:15)k
noise 2(cid:15)k
L. It follows from [18, Theorem 1] that xθk converges to the optimal
solution of (2.5).

5.2. Convergence analysis under neural tangent kernel. In this subsec-
tion, we analyze the convergence of the scheme (4.2) under neural tangent kernel
setting. For convenience, we consider the discrete case (state and action space is dis-
crete). The conclusions for the continuous case can be proven similarly under proper
assumptions. Neural tangent kernel is a basic tool to understand the approximation
of neural network.
It has been used to analyze the convergence of RL algorithms
under neural network parametrization.

We next parametrize the variables by the following two-layer neural networks:

(5.10)

(5.11)

(5.12)

Vφ(s) =

√

hψ(s, a) =

√

xθ(s, a) =

√

1
mV

1
mh

1
mx

mV(cid:88)

φ1,i · σ(φT

2,is),

i=1

mh(cid:88)

i=1

mx(cid:88)

i=1

ψ1,i · σ(ψT

2,i(s, a)),

θ1,i · σ(θT

2,i(s, a)),

A STOCHASTIC COMPOSITE ALM FOR REINFORCEMENT LEARNING

15

where mV , mh, mx are integers, σ = max(0, x) is the ReLU function. The parameters
are randomly initialized by either the uniform distribution or normal distribution, i.e.,

(5.13)

φ1,i, θ1,i ∼ U({1, −1}), ψ1,i = 1,
φ2,i ∼ N (0, Ids /ds), ψ2,i, θ1,i ∼ N (0, Idsa /dsa),

where ds is the dimension of s and dsa is the dimension of (s, a).
In particular,
φ1,i, ψ1,i, θ1,i are ﬁxed and we only optimize the parameter φ2,i, ψ2,i, θ2,i. Let φ =
(φ2,i)mV
i=1. The corresponding random initialization
parameters are denoted by φ0, ψ0, θ0. The expectation under the distribution (5.13)
is denoted by E0.

i=1 and θ = (θ2,i)mV

i=1, ψ = (ψ2,i)mV

Deﬁne the following function classes

F V

RV ,mV

= {

√

F h

Rh,mh

= {

√

F x

Rx,mx

= {

√

1
mV

1
mh

1
mh

and let

mV(cid:88)

φ1,i · 1{(φ0

2,i)T s≥0}φT

2,is : (cid:107)φ − φ0(cid:107) ≤ RV },

i=1

mh(cid:88)

i=1

mh(cid:88)

i=1

ψ1,i · 1{(ψ0

2,i)T (s,a)≥0}ψT

2,i(s, a) : (cid:107)ψ − ψ0(cid:107) ≤ Rh},

θ1,i · 1{(θ0

2,i)T (s,a)≥0}θT

2,i(s, a) : (cid:107)θ − θ0(cid:107) ≤ Rx},

Ξ = {ζ : (cid:107)φ − φ0(cid:107) ≤ RV , (cid:107)ψ − ψ0(cid:107) ≤ Rh and (cid:107)θ − θ0(cid:107) ≤ Rx}.

For simplicity, we assume that (cid:107)(s, a)(cid:107) ≤ 1, which follows that (cid:107)s(cid:107) ≤ 1. On Ξ, we
have

|Vφ(s) − Vφ0(s)| ≤

√

1
mV

mV(cid:88)

i=1

|φ1,i| · |σ(φT

2,is) − σ((φ0

2,i)T s)|

(5.14)

≤

√

1
mV

mV(cid:88)

i=1

|φ1,i| · |φT

2,is − (φ0

2,i)T s| ≤

√

1
mV

mV(cid:88)

i=1

(cid:107)φ2,i − (φ0

2,i)(cid:107) ≤ R1/2
V .

Similarly, we have |hψ(s) − hψ0 (s)| ≤ R1/2
obtain that

h

and |xθ(s) − xθ0(s)| ≤ R1/2

x . We can also

(cid:107)∇φVφ(s)(cid:107)2 =

1
mV

mV(cid:88)

i=1

1{(φ2,i)T s≥0} · (cid:107)s(cid:107)2 ≤ 1.

We also yields (cid:107)∇ψhψ(s, a)(cid:107) ≤ 1 and (cid:107)∇θxθ(s, a)(cid:107) ≤ 1. Consider that (V, h, x) is in
the Hilbert space H with inner product

(cid:104)(V, h, x), (V (cid:48), h(cid:48), x(cid:48))(cid:105)H =

(cid:88)

s,a

w(s, a) (cid:0)V (s)2 + x(s, a)2 + h(s, a)2(cid:1) .

The corresponding norm in H is denoted by (cid:107) · (cid:107)H. Deﬁne

(5.15)

˜Lk

β,µ(V, h, x) :=

(cid:88)

s

ρ0(s)V (s) +

1
2µ

(cid:88)

s,a

w(s, a)x(s, a)Zµ(V, h, xθk , s, a)

+

β
2

(cid:88)

s,a

w(s, a) [x(s, a) − (Zµ(V, h, xθk , s, a))]2 .

16

YONGFENG LI, MINGMING ZHAO, WEIJIE CHEN, ZAIWEN WEN

Here, we use the same notation as (4.2). It is easy to distinguish them according to
variables. Let ˜V k, ˜hk, ˜xk = arg minh≥0,V,x ˜Lk
β,µ(V, h, x). The following lemma shows
properties of ˜Lk

β,µ(V, h, x).
Lemma 5.4. If β > 1

4µ , the following claims hold.

(i) ˜Lk

β,µ(V, h, x) is a restricted strongly convex function with respect to V, h, x, i.e.,

there exits a constant c such that

β,µ(V, h, x) − ˜Lk
˜Lk

β,µ( ˜V k, ˜hk, ˜xk) ≥

dist ((V, h, x), X ∗)2 ,

c
2

where X ∗ is the set of optimal solutions of ˜Lk
is L-Lipschitz continuous.
2µ (1 − 1

β,µ(V, h, x) is a 1

(ii) ˜Lk

V, h.
Proof. The function ˜Lk

β,µ(V, h, x). The gradient of ˜Lk

β,µ(V, h, x)

4µβ )-stongly convex function with respect to x for ﬁxed

˜Lk
β,µ(V, h, x) is L-Lipschitz continuous. To show the convexity of ˜Lk
only need to show that the quadratic part is nonnegtive. If β > 1
the quadratic term in ˜Lk
β,µ(V, h, x), we obtain

β,µ(V, h, x) is a quadratic function. Hence, the gradient of
β,µ(V, h, x), we
4µ , by rearranging

1
2µ

(cid:88)

s,a

w(s, a)x(s, a)Zµ(V, h, xθk , s, a) +

β
2

(cid:88)

s,a

w(s, a) [x(s, a) − (Zµ(V, h, xθk , s, a))]2

=

β
2

(cid:88)

s,a

(cid:40)(cid:20)

w(s, a)

(1 −

1
2µβ

)x(s, a) − Zµ(V, h, xθk , s, a)

+

(cid:21)2

1
4µβ

(1 −

1
4µβ

)x(s, a)2

(cid:41)

≥

1
2µ

(1 −

1
4µβ

)

(cid:88)

s,a

w(s, a)x(s, a)2 ≥ 0.

Hence, ˜Lk
value of the Hessian of ˜Lk
strongly convex with respect to x.

β,µ(V, h, x) is restricted strongly convex and c is the smallest positive eigen-
4µβ )-

β,µ(V, h, x). It also implies that ˜Lk

β,µ(V, h, x) is 1

2µ (1 − 1

We make the following assumption about the optimal points ˜V k, ˜hk, ˜xk and the

distribution w(s, a).
Assumption 1.
any k.

(i) It holds that ˜V k ∈ F V

RV ,mV

, ˜hk ∈ F h

Rh,mh

, ˜xk ∈ F x

Rx,mx

for

Ew1{|vT s|<τ } ≤ cτ holds, where c is a constant and Ewf = (cid:80)

(ii) For any unit vector u, v and nonnegtive scalar τ , Ew1{|uT (s,a)|<τ } ≤ cτ and
s,a w(s, a)f (s, a).
Assumption 1 (i) is commonly used in the literature and Assumption 1 (ii) is on the
regularity of the distribution w. For convenience, let ζ = (φ, ψ, θ), Γ = (V, h, x),
R = max{RV , Rh, Rx} and m = min{mV , mh, mx}. The following method is used to
minimize (4.2). Let ζ k,0 = ζ k and

ζ k,t+1 = Πζ∈Ξ

(cid:16)

ζ k,t − αt∇ζ ˜Lk

β,µ(Vφk,t , hψk,t , xθk,t )

(cid:17)

,

(5.16)

ζ k+1 =

1
T

T
(cid:88)

t=1

ζ k,t.

The scheme (5.16) is actually a projected gradient descent method. Comparing with
the algorithm in Section 4, the scheme (5.16) has an additional projection and average

A STOCHASTIC COMPOSITE ALM FOR REINFORCEMENT LEARNING

17

operator, which is easy for analysis. We also ignore the stochastic part, which is not
hard to analyze by the method in [15]. In the following part, we show the convergence
of the scheme (5.16). Deﬁne the linearization function

V 0
φ (s) =

√

h0
ψ(s, a) =

√

x0
θ(s, a) =

√

1
mV

1
mh

1
mx

mV(cid:88)

φ1,i · 1{(φ0

2,i)T s≥0}φT

2,is,

i=1

mh(cid:88)

i=1

mx(cid:88)

i=1

ψ1,i · 1{(ψ0

2,i)T (s,a)≥0}ψT

2,i(s, a),

θ1,i · 1{(θ0

2,i)T (s,a)≥0}θT

2,i(s, a).

The following lemma gives the local linearization property.

Lemma 5.5. Suppose that Assumption 1 holds and ζ ∈ Ξ. Then we have the

following estimation:

E0,w(Vφ(s) − V 0

φ (s))2 = O

E0,w(hψ(s, a) − h0

ψ(s.a))2 = O

(cid:17)

R3

R3

V

V m−1/2
(cid:17)

hm−1/2

h

,

,

E0,w(xθ(s, a) − x0

E0

(cid:12)
(cid:12)
(cid:12)

β,µ(Vφ, hψ, xθ) − ˜Lk
˜Lk

β,µ(V 0

φ , h0

θ(s, a))2 = O
(cid:12)
(cid:12)
(cid:12) = O

ψ, x0
θ)

E0(cid:107)∇ζ ˜Lk

β,µ(Vφ, hψ, xθ) − ∇ζ ˜Lk

β,µ(V 0

φ , h0

ψ, x0

θ)(cid:107)2

2 = O

(cid:17)

R3
xm−1/2
,
x
R3m−1/2 + R2m−1/4(cid:17)
R3m−1/2(cid:17)

,

,

(cid:16)

(cid:16)

(cid:16)

(cid:16)

(cid:16)

where E0,w represent the expectation under the distribution (5.13) and w.

Proof. The ﬁrst three estimations follows from [15, Lemma D.1]. The main dif-
ference is that ψ1,i is always equal to 1, but it does not change the arguments. By
Lemma 5.4, it yields

(5.17)

θ)(cid:107)2
H

φ , h0

ψ, x0
β,µ(V 0
β,µ(Vφ0 , hψ0, xθ0 )(cid:107)2
β,µ(Vφ0, hψ0, xθ0 )(cid:107)2

(cid:107)∇Γ ˜Lk
≤2(cid:107)∇Γ ˜Lk
≤2(cid:107)∇Γ ˜Lk
≤4L(cid:107)(Vφ0, hψ0 , xθ0 )(cid:107)2

ψ, x0

H + 2(cid:107)∇Γ ˜Lk
H + 2L(cid:107)(V 0

β,µ(V 0
φ , h0
ψ, x0
β,µ(0, 0, 0)(cid:107)2

θ) − ∇Γ ˜Lk
φ , h0
θ) − (Vφ0 , hψ0, xθ0 )(cid:107)2
H
H + 6LR,

H + 4(cid:107)∇Γ ˜Lk

β,µ(Vφ0 , hψ0, xθ0)(cid:107)2
H

where the last inequality uses the bound of V, h, x in (5.14). It implies

E0(cid:107)∇Γ ˜Lk

β,µ(V 0

φ , h0

ψ, x0

θ)(cid:107)2

H = O (R) .

18

YONGFENG LI, MINGMING ZHAO, WEIJIE CHEN, ZAIWEN WEN

It follows from Lemma 5.4 and [1, Theorem 18.15] that

β,µ(Vφ, hψ, xθ) − ˜Lk
˜Lk
(cid:68)

β,µ(V 0

φ , h0

(cid:12)
ψ, x0
(cid:12)
θ)
(cid:12)

β,µ(V 0

φ , h0

ψ, x0

θ), (Vφ, hψ, xθ) − (V 0

φ , h0

ψ, x0
θ)

(cid:69)

H

(cid:12)
(cid:12)
(cid:12)

E0(cid:107)(Vφ, hψ, xθ) − (V 0

ψ, x0

φ , h0

θ)(cid:107)2
H
θ)(cid:107)(cid:107)(Vφ, hψ, xθ) − (V 0

φ , h0

ψ, x0

θ)(cid:107)H

β,µ(V 0

φ , h0

ψ, x0

E0(cid:107)(Vφ, hψ, xθ) − (V 0

θ)(cid:107)2
H

ψ, x0
φ , h0
(cid:17) 1
2 (cid:0)E0(cid:107)(Vφ, hψ, xθ) − (V 0

φ , h0

ψ, x0

θ)(cid:107)2
H

(cid:1) 1

2

β,µ(V 0

φ , h0

ψ, x0

θ)(cid:107)2
H

+

E0

≤ E0

∇Γ ˜Lk

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
L
2
≤ E0(cid:107)∇Γ ˜Lk
L
2
E0(cid:107)∇Γ ˜Lk
L
2
(cid:16)

+

≤

+

(cid:16)

= O

E0(cid:107)(Vφ, hψ, xθ) − (V 0
R3m−1/2 + R2m−1/4(cid:17)

.

φ , h0

ψ, x0

θ)(cid:107)2
H

We next prove the last estimation. We can obtain

E0(cid:107)∇ζ ˜Lk

β,µ(Vφ, hψ, xθ) − ∇ζ ˜Lk
∇Γ ˜Lk

(cid:16)

≤ 2E0(cid:107)∇ζ(Vφ, hψ, xθ)T ·

+2E0(cid:107) (cid:0)∇ζ(Vφ, hψ, xθ) − ∇ζ(V 0

≤ 2E0(cid:107)∇ζ(Vφ, hψ, xθ)(cid:107)2

∗(cid:107)∇Γ ˜Lk

+2E0(cid:107)∇ζ(Vφ, hψ, xθ) − ∇ζ(V 0

θ)(cid:107)2
2

φ , h0

ψ, x0
β,µ(V 0
β,µ(Vφ, hψ, xθ) − ∇Γ ˜Lk
θ)(cid:1)T
· ∇Γ ˜Lk
β,µ(Vφ, hψ, xθ) − ∇Γ ˜Lk
θ)(cid:107)2

∗(cid:107)∇Γ ˜Lk

φ , h0

φ , h0

ψ, x0

ψ, x0

β,µ(V 0

φ , h0

ψ, x0
θ)

(cid:107)2
2

(cid:17)

β,µ(V 0
β,µ(V 0
φ , h0

φ , h0
φ , h0
ψ, x0

ψ, x0
θ)(cid:107)2
2
θ)(cid:107)2
ψ, x0
H
θ)(cid:107)2
H,

β,µ(V 0

:= I1 + I2,

where the second inequality is due to the triangle inequality and the H¨older inequality
and (cid:107) · (cid:107)∗ is deﬁned by
(5.18)

(cid:107)∇ζ(Vφ, hψ, xθ)(cid:107)2

∗ =

(cid:88)

w(s, a) (cid:2)(cid:107)∇φVφ(s)(cid:107)2 + (cid:107)∇ψhψ(s)(cid:107)2 + (cid:107)∇θxθ(s)(cid:107)2(cid:3) ≤ 3.

s,a

Using the relationship (5.18), we get

I1 ≤ 6E0(cid:107)∇Γ ˜Lk

β,µ(Vφ, hψ, xθ) − ∇Γ ˜Lk
ψ, x0

φ , h0

β,µ(V 0
θ)(cid:107)2

H = O

θ)(cid:107)2
ψ, x0
φ , h0
H
R3m−1/2(cid:17)
(cid:16)

.

≤ 6LE0(cid:107)(Vφ, hψ, xθ) − (V 0

According to the arguments in [15, Lemma D.2], we have

(5.19)

E0(cid:107)∇ζ(Vφ, hψ, xθ) − ∇ζ(V 0

φ , h0

ψ, x0

θ)(cid:107)2

∗ = O

(cid:16)

Rm−1/2(cid:17)

,

E0(cid:107)∇ζ(Vφ, hψ, xθ) − ∇ζ(V 0

φ , h0

ψ, x0

θ)(cid:107)2

∗(cid:107)(Vφ0, hψ0 , xθ0)(cid:107)2

H = O

Rm−1/2(cid:17)
(cid:16)

.

Combining (5.17) and (5.19) yields

I2 = O

(cid:16)

R2m−1/2(cid:17)

.

which completes the proof.

A STOCHASTIC COMPOSITE ALM FOR REINFORCEMENT LEARNING

19

The next lemma gives the convergence of the scheme (5.16).
Lemma 5.6. Suppose that Assumption 1 holds and β > 1

4µ . Let ζ k,t = (φk,t, ψk,t, θk,t)

be a sequence generated by the projected gradient descent method (5.16) with αt = α ≤
1√

. Then we have

3L

(5.20)

(cid:16) ˜Lk
β,µ(Vφk+1 , hψk+1, xθk+1) − ˜Lk
E0
R2T −1 + R5/2m−1/4 + R3m−1/2(cid:17)
(cid:16)

.

=O

(cid:17)
β,µ( ˜V k+1, ˜hk+1, ˜xk+1)

Furthermore, it holds that

(5.21)

E0,w(xθk+1(s, a) − ˜xk+1(s, a))2 = O

(cid:16)

R2T −1 + R5/2m−1/4 + R3m−1/2(cid:17)

.

Proof. Since V 0
ψ, x0

φ , h0

β,µ(V 0

˜Lk

φ (s), h0

ψ(s, a), x0

θ(s, a) is linear with respect to φ, ψ, θ, respectively,

θ) is convex for ζ by Lemma 5.4. We also have

(cid:107)∇ζ ˜Lk
≤(cid:107)∇ζ(V 0

≤(cid:107)∇ζ(V 0
≤3L2(cid:107)(V 0

β,µ(V 0
φ , h0
φ , h0
φ , h0

φ , h0
ψ, x0
ψ, x0
ψ, x0

θ) − ∇ζ ˜Lk
ψ, x0
(cid:16)
∇Γ ˜Lk
θ)T ·
∗(cid:107)∇Γ ˜Lk
θ)(cid:107)2
θ) − (V 0
√

β,µ(V 0
β,µ(V 0
ψ(cid:48), x0

φ(cid:48), h0

β,µ(V 0
φ , h0
φ , h0
θ(cid:48))(cid:107)2

ψ(cid:48), x0
θ(cid:48))(cid:107)2
φ(cid:48), h0
θ) − ∇Γ ˜Lk
φ(cid:48), h0
β,µ(V 0
ψ, x0
θ) − ∇Γ ˜Lk
φ(cid:48), h0
β,µ(V 0
ψ, x0
H ≤ 3L2(cid:107)ζ − ζ (cid:48)(cid:107)2
2.

(cid:17)

(cid:107)2

ψ(cid:48), x0
ψ(cid:48), x0

θ(cid:48))
θ(cid:48))(cid:107)2
H

3L-Lipschitz continuous gradient. According to [1,

Hence, ˜Lk
β,µ(V 0
Theorem 18.15], it implies

φ , h0

ψ, x0

θ) has

˜Lk

β,µ(V 0

φk,t+1 , h0

ψk,t+1, x0

θk,t+1) − ˜Lk

β,µ(V 0

≤∇ζ ˜Lk

β,µ(V 0

φk,t , h0

ψk,t , x0

θk,t )T (ζ k,t+1 − ζ k,t) +

(cid:107)ζ k,t+1 − ζ k,t(cid:107)2
2.

θk,t )

ψk,t , x0
φk,t , h0
√
3L
2

By the convexity of ˜Lk

β,µ(V 0
˜Lk
β,µ(V 0
≤∇ζ ˜Lk

φ , h0
ψ, x0
φk,t , h0

θ), we have
ψk,t , x0

β,µ(V 0

φk,t , h0

ψk,t , x0

θk,t ) − ˜Lk

β,µ(V 0
φ∗ , h0
θk,t )T (ζ k,t − ζ ∗),

ψ∗ , x0

θ∗ )

(5.22)

(5.23)

(5.24)

where ζ ∗ = (φ∗, ψ∗, θ∗) is an optimal solution, i.e., (V 0
φ∗ , h0
According to the property of projection operator, we get

ψ∗ , x0

θ∗ ) = ( ˜V k+1, ˜hk+1, ˜xk+1).

(5.25)

(ζ ∗ − ζ k,t+1)T (cid:16)

ζ k,t − αt∇ζ ˜Lk

β,µ(Vφk,t , hψk,t , xθk,t ) − ζ k,t+1(cid:17)

≥ 0

Combining (5.23), (5.24) and (5.25) yields
θk,t+1) − ˜Lk
φk,t+1, h0

ψk,t+1, x0

β,µ(V 0

˜Lk

β,µ(V 0

θ∗ )

ψ∗ , x0
φ∗ , h0
√
3L
2

(5.26)

≤

1
αt

+

(cid:16)

≤∇ζ ˜Lk

β,µ(V 0

φk,t , h0

ψk,t , x0

θk,t )T (ζ k,t+1 − ζ ∗) +

(cid:107)ζ k,t+1 − ζ k,t(cid:107)2
2

√

3L
2
θk,t ) − ∇ζ ˜Lk

(ζ k,t − ζ k,t+1)T (ζ k,t+1 − ζ ∗) +

(cid:107)ζ k,t+1 − ζ k,t(cid:107)2
2

∇ζ ˜Lk

β,µ(V 0

φk,t , h0

ψk,t , x0

β,µ(Vφk,t , hψk,t , xθk,t )

(cid:17)T

(ζ k,t+1 − ζ ∗)

≤

(cid:0)(cid:107)ζ k,t − ζ ∗(cid:107)2 − (cid:107)ζ k,t+1 − ζ ∗(cid:107)2(cid:1)

1
α
+ (cid:107)∇ζ ˜Lk

β,µ(V 0

φk,t , h0

ψk,t , x0

θk,t ) − ∇ζ ˜Lk

β,µ(Vφk,t , hψk,t , xθk,t )(cid:107)(cid:107)ζ k,t+1 − ζ ∗(cid:107).

20

YONGFENG LI, MINGMING ZHAO, WEIJIE CHEN, ZAIWEN WEN

By the convexity of ˜Lk

β,µ(V 0

φk,t+1, h0

ψk,t+1, x0

θk,t+1) and Lemma 5.5, we obtain

(cid:16) ˜Lk

E0

β,µ(V 0

φk+1, h0

ψk+1 , x0

θk+1) − ˜Lk

(cid:17)
β,µ( ˜V k+1, ˜hk+1, ˜xk+1)

≤

1
T

T
(cid:88)

t=1

E0

(cid:16) ˜Lk

β,µ(V 0

φk,t , h0

ψk,t , x0

θk,t ) − ˜Lk

(cid:17)
β,µ( ˜V k+1, ˜hk+1, ˜xk+1)

≤

4R
T

T
(cid:88)

(cid:16)

E0(cid:107)∇ζ ˜Lk

β,µ(V 0

φk,t , h0

ψk,t , x0

θk,t ) − ∇ζ ˜Lk

β,µ(Vφk,t , hψk,t , xθk,t )(cid:107)2(cid:17) 1

2

+

(cid:107)ζ k,0 − ζ ∗(cid:107)2

t=1
1
αT
R2T −1 + R5/2m−1/4(cid:17)
(cid:16)

,

=O

where the second inequality is due to (5.26) and H¨older inequality. It follows from
Lemma 5.5 that

(cid:16) ˜Lk
(cid:16) ˜Lk

E0

β,µ(Vφk+1, hψk+1, xθk+1 ) − ˜Lk

≤E0

φk+1, h0

θk+1 ) − ˜Lk

β,µ(V 0
ψk+1, x0
(cid:16) ˜Lk
β,µ(Vφk+1 , hψk+1, xθk+1) − ˜Lk
+ E0
R2T −1 + R5/2m−1/4 + R3m−1/2(cid:17)
(cid:16)

= O

,

(cid:17)
β,µ( ˜V k+1, ˜hk+1, ˜xk+1)
(cid:17)
β,µ( ˜V k+1, ˜hk+1, ˜xk+1)

β,µ(V 0

φk+1, h0

ψk+1, x0

(cid:17)
θk+1)

which completes the proof of (5.20).
By Lemma 5.4 (i), we have

E0dist (cid:0)(cid:0)Vφk+1, hψk+1, xθk+1

(cid:1) , X ∗(cid:1)2

= O

(cid:16)

R2T −1 + R5/2m−1/4 + R3m−1/2(cid:17)

.

It means that there exists an optimal solution (V ∗, h∗, x∗) ∈ X ∗ such that
R2T −1 + R5/2m−1/4 + R3m−1/2(cid:17)

E0,w(xθk+1 (s, a) − x∗(s, a))2 = O

(cid:16)

.

By Lemma 5.4 (ii), x∗ is unique in X ∗, i.e. x∗ = ˜xk+1, which completes the proof of
(5.21).

The next lemma shows an estimation about the error between the quadratic

penalty method and augmented Lagrangian method.

Lemma 5.7. Suppose that Assumption 1 holds and β > 1

4µ . Deﬁne ˆV k, ˆhk =
arg minh≥0,V Lµ(V, h, xθk ) and let ˆxk+1(s, a) = [Zµ( ˆV k, ˆhk, xθk , s, a)]. Then we have

(5.27)

(5.28)

Zµ( ˜V k+1, ˜hk, xθk , s, a) = ξ1Zµ( ˆV k+1, ˆhk+1, xθk , s, a),
˜xk+1 = ξ2 ˆxk+1,

where ξ1 = 4µβ

4µβ−1 and ξ2 = 4µβ−2

4µβ−1 . Furthermore, it implies

E0,w(xθk+1(s, a) − ˆxk+1(s, a))2 = O

(cid:16)

R2T −1 + R5/2m−1/4 + R3m−1/2 + β−2R

(cid:17)

.

A STOCHASTIC COMPOSITE ALM FOR REINFORCEMENT LEARNING

21

Proof. Let x∗

V,h = arg minx ˜Lk

β,µ(V, h, x). The ﬁrst order optimality condition

gives

1
2µ

Zµ(V, h, xθk , s, a) + β (cid:2)x∗

V,h(s, a) − (Zµ(V, h, xθk , s, a))(cid:3) = 0.

It implies that

(5.29)

Deﬁne

x∗
V,h(s, a) = (1 −

1
2µβ

)Zµ(V, h, xθk , s, a).

M(V, h) := ˜Lk

β,µ(V, h, x∗

V,h) =

(cid:88)

s

ρ0(s)V (s) +

1
2µξ1

(cid:88)

s,a

w(s, a)[Zµ(V, h, xθk , s, a)]2.

The optimality condition of minh≥0,V M(V, h) is

ξ1ρ0(s(cid:48)) −

(cid:88)

w(s, a)Zµ(V ∗, h∗, x(cid:48), s, a)(δs(cid:48)(s) − γP (s(cid:48)|s, a)) = 0, ∀s(cid:48)

s,a
(cid:34)
−x(cid:48)(s, a) − µ

(cid:32)

r(s, a) +

µh(x, a) =

γP (s(cid:48)|s, a)V (s(cid:48)) − V (s)

(cid:88)

s(cid:48)

(cid:33)(cid:35)

.

+

Together with (5.4), we have

Zµ( ˜V k+1, ˜hk, xθk , s, a) = ξ1Zµ( ˆV k+1, ˆhk+1, xθk , s, a).

Combining with (5.29), we have

˜xk+1(s, a) = x∗

˜V k+1,˜hk+1(s, a) = (1 −

1
2µβ

)Zµ( ˜V k+1, ˜hk+1, xθk , s, a) = ξ2 ˆxk+1(s, a).

According to Lemma 5.6, we have

E0,w(xθk+1(s, a) − ˆxk+1(s, a))2

≤ 2E0,w(xθk+1(s, a) − ˜xk+1(s, a))2 + 2E0,w(˜xk+1(s, a) − ˆxk+1(s, a))2
= 2E0,w(xθk+1 (s, a) − ˜xk+1(s, a))2 + 2(1 − ξ2)2E0,w(ˆxk+1(s, a))2

(cid:16)

= O

R2T −1 + R5/2m−1/4 + R3m−1/2 + β−2R

(cid:17)

.

This completes the proof of Lemma 5.7.

Let S be the subgradient operator of − (cid:80)

s,a r(s, a)w(s, a)x(s, a) + 1Ω(x) with

respect to x, where

Ω = {x :

(cid:88)

s,a

(δs(cid:48)(s) − γP (s(cid:48)|s, a))w(s, a)x(s, a) = ρ0(s(cid:48)), ∀s(cid:48), x(s, a) ≥ 0, ∀s, a}.

Then S is a maximal monotone operator. Lemma 5.1 means that

Zµ( ˆV k, ˆhk, xθk , s, a) = T xθk := (I + µS)−1xθk ,

where ˆV k, ˆhk = arg minh≥0,V Lµ(V, h, xθk ). We next give the main theorem. It states
that the residual can be controlled if the parameters are chosen properly.

22

YONGFENG LI, MINGMING ZHAO, WEIJIE CHEN, ZAIWEN WEN

Theorem 5.8. Suppose that Assumption 1 holds and β > 1
(φk+1, ψk+1, θk+1) be a sequence generated by (5.16). Then we have that

4µ . Let ζ k+1 =

K
(cid:88)

E0,w (xθk (s, a) − T xθk (s, a))2

1
K

k=1
(cid:16)
R3/2T −1/2 + R7/4m−1/8 + R2m−1/4 + β−1R + K −1R

(cid:17)

,

=O

and

E0,w (xθK (s, a) − T xθK (s, a))2

(cid:16)

KR3/2T −1/2 + KR7/4m−1/8 + KR2m−1/4 + Kβ−1R + K −1R

(cid:17)

.

=O

Proof. According to Lemmas 5.1 and 5.7, we have

E0,w (xθk+1(s, a) − T xθk (s, a))2 = O

(cid:16)

R2T −1 + R5/2m−1/4 + R3m−1/2 + β−2R

(cid:17)

.

Let x∗ be the solution of the equation x∗ = T x∗, i.e., the optimal solution of the LP
(2.7). It follows from [18, Proposition 1] that

E0,w (x∗(s, a) − T xθk (s, a))2 + E0,w (xθk (s, a) − T xθk (s, a))2

≤E0,w (xθk (s, a) − x∗(s, a))2 .

It implies

E0,w (xθk (s, a) − T xθk (s, a))2

≤E0,w (xθk (s, a) − x∗(s, a))2 − E0,w (x∗(s, a) − T xθk (s, a))2
=E0,w (xθk (s, a) − x∗(s, a))2 − E0,w (xθk+1(s, a) − x∗(s, a))2

+ E0,w (xθk+1 (s, a) − x∗(s, a))2 − E0,w (x∗(s, a) − T xθk (s, a))2

=E0,w (xθk (s, a) − x∗(s, a))2 − E0,w (xθk+1(s, a) − x∗(s, a))2

+ E0,w (xθk+1 (s, a) − T xθk (s, a)) (xθk+1(s, a) + T xθk (s, a) − 2x∗(s, a))

≤E0,w (xθk (s, a) − x∗(s, a))2 − E0,w (xθk+1(s, a) − x∗(s, a))2

+ cR1/2 · (E0,w (xθk+1(s, a) − T xθk (s, a))2)

1
2 ,

where c is a constant. Taking summation over k, we obtain

K
(cid:88)

k=1

E0,w (xθk (s, a) − T xθk (s, a))2

E0,w (xθ1 (s, a) − x∗(s, a))2 + cR1/2 ·

1
K

1
K

≤

1
K

K
(cid:88)

k=1

(E0,w (xθk+1(s, a) − T xθk (s, a))2)

1
2

(cid:16)

=O

R3/2T −1/2 + R7/4m−1/8 + R2m−1/4 + β−1R + K −1R

(cid:17)

.

Furthermore, by the same arguments, we have

E0,w (xθk+1(s, a) − T xθk+1 (s, a))2 − E0,w (xθk (s, a) − T xθk (s, a))2

(cid:16)

=O

R3/2T −1/2 + R7/4m−1/8 + R2m−1/4 + β−1R

(cid:17)

.

A STOCHASTIC COMPOSITE ALM FOR REINFORCEMENT LEARNING

23

It implies that

K · E0,w (xθK (s, a) − T xθK (s, a))2

≤

K
(cid:88)

k=1

E0,w (xθk (s, a) − T xθk (s, a))2

K
(cid:88)

+

(K − k) · O

(cid:16)

k=1

R3/2T −1/2 + R7/4m−1/8 + R2m−1/4 + β−1R

(cid:17)

.

Hence, we have

E0,w (xθK (s, a) − T xθK (s, a))2

≤

1
K

K
(cid:88)

k=1

E0,w (xθk (s, a) − T xθk (s, a))2

(cid:16)

+ K · O

R3/2T −1/2 + R7/4m−1/8 + R2m−1/4 + β−1R

(cid:17)

(cid:16)

KR3/2T −1/2 + KR7/4m−1/8 + KR2m−1/4 + Kβ−1R + K −1R

(cid:17)

,

=O

which completes the proof.

6. Implementation Matters. In this section, we explain the structure of the

neural networks and clarify some strategies in our implementation.

6.1. Network Architecture. We describe the network structures for the value
network Vφ, the slack network hψ and the multiplier network xθ in this subsection. By
deﬁnition, the value network reads state information and outputs the corresponding
predicted values, while both the multiplier network and slack network take the state
as input and predict the values on all possible actions.

For the environments with continuous action spaces, a 3-layer fully connected
network of 64 units using tanh activator is considered for function approximation in
value network Vψ. The multiplier network xθ starts from a 2-layer MLP with 64 units
and tanh activator, then splits into two heads. The ﬁrst head generates a positive state
value, i.e., xθ,1(s), after a single fully connected layer and the second head similarly
predicts a mean vector xθ,2(s) which has the same dimension as action space. The
Gaussian policy is commonly adopted, however in practice, the actions are bounded
in a ﬁnite interval, i.e., A ⊆ [−¯a, ¯a]D and D is the dimension of the action space.
Therefore, we apply tanh function to the Gaussian samples u ∼ N (xθ,2(s), σ2), i.e.,
a = ¯a tanh(u), and derive the log-likelihood by employing the change of variables
formula for the bounded actions. The multiplier is composed as

(6.1)

xθ(s, a) = xθ,1(s) exp(log f (u|s) − 1T log(1 − tanh2(u))),

where f (u|s) is the Gaussian density function and σ > 0 is a hyper-parameter. As
hψ(s, a) ≥ 0, ∀s, a, we take a fully connected slack network to map the state s into a
vector hψ(s) with the same dimension as actions. The slack function is

(6.2)

hψ(s, a) = C(1 − exp(−(cid:107)a − hψ(s)(cid:107)2)).

where C > 0 is a hyper-parameter.

In discrete cases, e.g. Acrobot, CartPole, the diﬀerence from the continuous
environments mainly locates in the output layer of the second head in the multiplier

24

YONGFENG LI, MINGMING ZHAO, WEIJIE CHEN, ZAIWEN WEN

network, as well as in the slack network. The head directly maps the hidden variable
into an action distribution by a sof tmax activator. In other words, xθ,2(s) is a vector
which has the same dimension as the action space and

(6.3)

xθ(s, a) = xθ,1(s)xθ,2(s)T 1a.

where 1a is an indicator vector. Similarly, the slack network predicts

(6.4)

hψ(s, a) = Chψ(s)T 1a.

The network structure of the multiplier function xθ is illustrated in Fig 1. Addi-
tionally, if the states are not presented as a vector in the Euclidean space, but video
frames for example, then the fully connected layers are concatenated with multiple
convolutional layers in advance to embed the states.

state s

fully connected layers or CNN

fully connected layers

xθ,1(s)

fully connected layers

xθ,2(s)

Fig. 1. The multiplier network xθ with two heads.

6.2. Experimental Strategies. For practical computation considerations, we

also include several helpful and widely used strategies as follows.

• Proportional Sampling. In the above discussion, we propose a stochastic
gradient-based algorithm with a series of approximations to resolve the con-
strained LP problem. To enhance the control of the constraints in the dual
LP, we are supposed to encourage the points that violate the constraints. In
other words, when getting batch data from the replay buﬀer D, the transition
tuple (s, a, r, s(cid:48)) is sampled proportional to the empirical constraint violation,
i.e., [r + γVφtarg (s(cid:48)) − Vφ(s)]+ where φ is the current parameter of the value
network, other than uniformly sampling in general case.

• Adam Learning Rate Annealing. Based on Adam’s [13] excellent perfor-
mance in many stochastic optimization problems, we adopt it as our method
for all networks. The learning rate of Adam is annealed as the iteration going
on.

• Local Gradient Clipping. After each gradient computation, we clip each
component of the gradient (i.e., the gradient of each layer) such that the local
gradient norm does not exceed one. DQN implemented in OpenAI Baselines
[8] also executes such operation.

7. Experiments. To get a better understanding of the stability and eﬃciency of
our composite augmented Lagrangian algorithm, we make a comprehensive numerical
experiments including a series of explorations on algorithmic components and com-
parisons with some state-of-the-art algorithms. As the related works [6, 5, 24] do not
provide open-source code, we are not able to compare with them fairly. However, ac-
cording to their reported results, the performance of our method is competitive with
them.

A STOCHASTIC COMPOSITE ALM FOR REINFORCEMENT LEARNING

25

The interaction environments are provided from OpenAI Gym benchmark [4]. As
illustrated in [10], the implementation of the RL algorithms, as well as the preprocess-
ing of the environments, have a large aﬀect on numerical performance. For fairness,
our method is implemented in OpenAI Baselines [8] toolkit which provides suﬃcient
wrappers for all type of environments. For all ﬁgures below, the solid line reports the
average values and the shade area corresponds to the standard derivation over ﬁve
random seeds. Besides, the horizontal axis represents the number of the interactions
with the environment.

7.1. Ablation Study. To investigate the algorithmic components and the scal-
ability of SCAL, we perform ablation study in several control problems by detecting
two type of gradients of the weighted augmented Lagrangian function and varying the
number of rollout steps.

• The Eﬀect of Gradient Estimation. To understand the capability of the
proposed method in section 3.1 for the double sampling obstacle, we compare
the un-bias gradient estimation of Lµ,w(φ, θk), i.e.,

g1 = ∇φVφk (s0) + xθk (s, a) (cid:0)γ∇φVφk (s(cid:48)) − ∇φVφk (s)(cid:1) ,

with a simple gradient approximation, i.e.,

g2 = ∇φVφk (s0) + zµ(φk, ψk, θk, s, a, s(cid:48)) (cid:0)γ∇φVφk (s(cid:48)) − ∇φVφk (s)(cid:1) ,

in which the expectation Zµ(φk, ψk, θk, s, a) is replaced by single transition
sample. We call the corresponding gradient as un-bias and bias version,
respectively. We investigate the variance of these two diﬀerent gradients in a
classic control task Acrobot in Fig 2. Notably, the default transition dynamic
in Acrobot itself is deterministic. In this case, g2 is exactly an un-bias gradient
estimation of Lc,µ(φ, θk), and the left picture shows that the variance curves
of these two gradient estimations are similar. Furthermore, we add a small
white noise disturbance to the transition probability and the variance of two
gradients are shown in the right picture. In this time, a signiﬁcant variance
reduction illustrates that our algorithm works well in solving the double-
sampling obstacle. Moreover, for the environments in which state space is
large or/and the transition kernel is stochastic, the bias version may also
induce a horrible deviation to mislead the optimization.

Fig. 2. Two diﬀerent gradient estimations in deterministic and stochastic dynamics.

020000400006000080000100000Timesteps050100150200250var(|g|)Deterministicbiasun-bias020000400006000080000100000Timesteps050100150200250300350var(|g|)Stochasticbiasun-bias26

YONGFENG LI, MINGMING ZHAO, WEIJIE CHEN, ZAIWEN WEN

• The Eﬀect of Multi-steps. In temporal-diﬀerence algorithms and cases
where the reward function is highly sparse, multi-step setting is commonly
utilized for better bias-variance trade oﬀ [12, 21] and reward condensing,
respectively. The optimal Bellman equation in (2.4) is the one-step case
and it can be extended into multi-step version, as well as the corresponding
LP formulations (2.5) and (2.7). Therefore, SCAL can be applied to the
multi-step setting by modifying the one-step reward ri in transition tuple
(si, ai, ri, s(cid:48)

i) into

(7.1)

ri =

l−1
(cid:88)

j=0

γjr(si+j, ai+j),

where l is the length of looking ahead, and s(cid:48)
i is the state after l steps transition
from si. The parameter l determines how far into the future the algorithm can
reach. The bias of the Bellman residual is reduced as the lookahead length l
increases, while the variance is also enlarged unexpectedly. We test diﬀerent
values of l in two robotic locomotion control problems, HalfCheetah and Ant,
to investigate the inﬂuence on the performance. We plot the results in Fig
3. It reveals that slightly increasing the lookahead length is quite eﬃcient in
promoting the performance, while looking too far ahead induces unexpected
variance which may suppresses the learning of the algorithm.

Fig. 3. The comparison among diﬀerent lookahead length.

7.2. Inventory Control. We consider the problem of day-to-day control of an
inventory of a ﬁxed maximum size M in the face of uncertain demand1. It is a single
agent domain featuring discrete state and action space. In the evening of day t, the
agent must decide about the quantity at ∈ {0, 1, 2, ..., M } to be ordered for the next
day t + 1, in which st ∈ {0, 1, 2, ..., M } is the current size of the inventory. During the
day t + 1, some stochastic demand dt+1 arrives, and the next inventory is

(7.2)

st+1 = [min((st + at), M ) − dt+1]+,

in which {dt} is a sequence of independent and identically distributed (i.i.d.) integer-
valued random variables. The revenue on day t + 1 is deﬁned as
(7.3)
rt+1 = −K · 1at>0 − c · [min((st + at), M ) − st]+ − h · st + p · [min((st + at), M ) − st+1]+,

1Example 1 in https://sites.ualberta.ca/ szepesva/papers/RLAlgsInMDPs.pdf

0.00.20.40.60.81.0Timesteps1e601000200030004000500060007000Average RewardHalfCheetahmethodl=1l=5l=10l=200.00.20.40.60.81.0Timesteps1e6010002000300040005000Average RewardAntmethodl=1l=5l=10l=20A STOCHASTIC COMPOSITE ALM FOR REINFORCEMENT LEARNING

27

where K > 0 is a ﬁxed entry cost, c > 0 is the unit cost of purchasing, h > 0 is the
unit cost of holding an inventory and p > h is the unit cost of selling.

In our experiments, we consider the setting as follows: M = 100, K = 5, c = 2,
h = 2, p = 3 and the Poisson distribution P(8) is considered for the demand variables.
The lines in Fig 4 illustrate the average cumulative return over the last simulated 10
days. The result shows that SCAL reaches a stable solution with quite less interactions
than other two state-of-the-art RL algorithms, TRPO [19] and PPO [20].

Fig. 4. The Inventory Control.

7.3. Comparisons in Discrete Environments. To demonstrate the learning
ability, we compare SCAL with several popular RL algorithms, including DQN, two
policy gradient methods TRPO and PPO on two discrete control problems. We plot
the results in Fig 5. With the same number of interactions, our algorithm achieves
signiﬁcant better performance in learning capability and stability. Note that TRPO
and PPO are only applicable in on-policy setting which has a poor sample eﬃciency,
while both our algorithm and DQN are performed in an oﬀ-policy manner.

Fig. 5. Comparisons among discrete environments.

7.4. Comparisons in Continuous Control Problems. As DQN is unavail-
able in continuous environments, we just compare SCAL with TRPO and PPO on
several control tasks provided from OpenAI Gym benchmark [4] using the MuJoCo
simulator [22]. The diﬀerent dynamic properties and dimensions of these environ-
ments provide meaningful benchmarks to make a comparison for the algorithms. In

020000400006000080000100000Timesteps200000150000100000500000Average RewardInventorymethodSCALTRPOPPO020000400006000080000100000Timesteps255075100125150175200Average RewardCartPolemethodSCALTRPOPPODQN020000400006000080000100000Timesteps500400300200100Average RewardAcrobotmethodSCALTRPOPPODQN28

YONGFENG LI, MINGMING ZHAO, WEIJIE CHEN, ZAIWEN WEN

Fig 6, our method attains higher average reward with the same number of interactions
than the other two state-of-the-art algorithms.

Fig. 6. Comparisons among state-of-the-art algorithms.

8. Conclusion. With the help of the algorithmic property of ALM, we over-
come the diﬃculties in minimizing the weighted augmented Lagrangian function by
taking advantage of the multipliers. Our solution provides an opportunity to incor-
porate the subproblems in ALM into a single constrained problem. To develop a
practical optimization method, we penalize the constraints to formulate a quadratic
penalty problem and employ semi-gradient for the value function, so that a particular
stochastic gradient optimization is easily to be performed for the objective function.
The ablation study shows that the double-sampling solution signiﬁcantly reduces the
variance of the gradient estimation in environments with stochastic transition prob-
ability, and the algorithm can be easily extended into multi-step version with better
performance. The comparisons with other state-of-the-art RL algorithms also demon-
strate that our method has competitive learning ability in multiple environments.

Acknowledgement. The computational results were obtained at GPUs sup-
ported by the National Engineering Laboratory for Big Data Analysis and Applica-
tions and the High-performance Computing Platform of Peking University.

REFERENCES

[1] H. H. Bauschke, P. L. Combettes, et al., Convex analysis and monotone operator theory

in Hilbert spaces, vol. 408, Springer, 2011.

[2] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling, The arcade learning environ-
ment: An evaluation platform for general agents, Journal of Artiﬁcial Intelligence Re-
search, 47 (2013), pp. 253–279.

0.00.20.40.60.81.0Timesteps1e6100001000200030004000500060007000Average RewardHalfCheetahmethodSCALPPOTRPO0.00.20.40.60.81.0Timesteps1e605001000150020002500300035004000Average RewardHumanoidmethodSCALPPOTRPO0.00.20.40.60.81.0Timesteps1e6300020001000010002000300040005000Average RewardAntmethodSCALPPOTRPO0.00.20.40.60.81.0Timesteps1e650050100150Average RewardSwimmermethodSCALPPOTRPOA STOCHASTIC COMPOSITE ALM FOR REINFORCEMENT LEARNING

29

[3] D. P. Bertsekas, D. P. Bertsekas, D. P. Bertsekas, and D. P. Bertsekas, Dynamic
programming and optimal control, vol. 1, Athena scientiﬁc Belmont, MA, 1995.
[4] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and

W. Zaremba, Openai gym, arXiv:1606.01540, (2016).

[5] W. S. Cho and M. Wang, Deep primal-dual reinforcement learning: Accelerating actor-critic

using bellman duality, arXiv:1712.02467, (2017).

[6] B. Dai, A. Shaw, N. He, L. Li, and L. Song, Boosting the actor with dual critic,

arXiv:1712.10282, (2017).

[7] B. Dai, A. Shaw, L. Li, L. Xiao, N. He, Z. Liu, J. Chen, and L. Song, Sbeed: Convergent
reinforcement learning with nonlinear function approximation, in International Conference
on Machine Learning, 2018, pp. 1125–1134.

[8] P. Dhariwal, C. Hesse, O. Klimov, A. Nichol, M. Plappert, A. Radford, J. Schul-
man, S. Sidor, Y. Wu, and P. Zhokhov, Openai baselines. https://github.com/openai/
baselines, 2017.

[9] S. Fujimoto, H. van Hoof, and D. Meger, Addressing function approximation error in

actor-critic methods, arXiv:1802.09477, (2018).

[10] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and D. Meger, Deep rein-

forcement learning that matters, arXiv preprint arXiv:1709.06560, (2017).

[11] M. Hessel, J. Modayil, H. Van Hasselt, T. Schaul, G. Ostrovski, W. Dabney, D. Hor-
gan, B. Piot, M. Azar, and D. Silver, Rainbow: Combining improvements in deep
reinforcement learning, in Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018.
[12] M. J. Kearns and S. P. Singh, Bias-variance error bounds for temporal diﬀerence updates.,

in COLT, Citeseer, 2000, pp. 142–147.

[13] D. P. Kingma and J. Ba, Adam: A method for stochastic optimization, arXiv preprint

arXiv:1412.6980, (2014).

[14] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and
D. Wierstra, Continuous control with deep reinforcement learning, arXiv:1509.02971,
(2015).

[15] B. Liu, Q. Cai, Z. Yang, and Z. Wang, Neural proximal/trust region policy optimization

attains globally optimal policy, arXiv preprint arXiv:1906.10306, (2019).

[16] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., Human-level control
through deep reinforcement learning, Nature, 518 (2015), p. 529.

[17] M. L. Puterman, Markov decision processes: discrete stochastic dynamic programming, John

Wiley & Sons, 2014.

[18] R. T. Rockafellar, Augmented lagrangians and applications of the proximal point algorithm
in convex programming, Mathematics of operations research, 1 (1976), pp. 97–116.
[19] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, Trust region policy opti-
mization, in International Conference on Machine Learning, 2015, pp. 1889–1897.
[20] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, Proximal policy

optimization algorithms, arXiv:1707.06347, (2017).

[21] R. S. Sutton, A. G. Barto, et al., Reinforcement learning: An introduction, MIT press,

1998.

[22] E. Todorov, T. Erez, and Y. Tassa, Mujoco: A physics engine for model-based control, in
2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, IEEE, 2012,
pp. 5026–5033.

[23] N. Vieillard, O. Pietquin, and M. Geist, Munchausen reinforcement

learning,

arXiv:2007.14430, (2020).

[24] M. Wang, Randomized linear programming solves the discounted markov decision problem in

nearly-linear (sometimes sublinear) running time, arXiv:1704.01869, (2017).

[25] M. Wang, E. X. Fang, and H. Liu, Stochastic compositional gradient descent: algorithms
for minimizing compositions of expected-value functions, Mathematical Programming, 161
(2017), pp. 419–449.

[26] Y. Zhu, Z. Izzo, and L. Ying, Borrowing from the future: Addressing double sampling in

model-free control, arXiv:2006.06173, (2020).

