1
2
0
2

t
c
O
7

]

G
L
.
s
c
[

2
v
2
1
4
8
0
.
0
1
9
1
:
v
i
X
r
a

On the Sample Complexity of Actor-Critic Method
for Reinforcement Learning with Function Approximation

Harshat Kumar∗, Alec Koppel†, and Alejandro Ribeiro∗

October 11, 2021

Abstract

Reinforcement learning, mathematically described by Markov Decision Problems, may be approached
either through dynamic programming or policy search. Actor-critic algorithms combine the merits of
both approaches by alternating between steps to estimate the value function and policy gradient updates.
Due to the fact that the updates exhibit correlated noise and biased gradient updates, only the asymptotic
behavior of actor-critic is known by connecting its behavior to dynamical systems. This work puts forth
a new variant of actor-critic that employs Monte Carlo rollouts during the policy search updates, which
results in controllable bias that depends on the number of critic evaluations. As a result, we are able
to provide for the ﬁrst time the convergence rate of actor-critic algorithms when the policy search step
employs policy gradient, agnostic to the choice of policy evaluation technique. In particular, we establish
conditions under which the sample complexity is comparable to stochastic gradient method for non-convex
problems or slower as a result of the critic estimation error, which is the main complexity bottleneck.
These results hold for in continuous state and action spaces with linear function approximation for the
value function. We then specialize these conceptual results to the case where the critic is estimated
by Temporal Diﬀerence, Gradient Temporal Diﬀerence, and Accelerated Gradient Temporal Diﬀerence.
These learning rates are then corroborated on a navigation problem involving an obstacle, which suggests
that learning more slowly may lead to improved limit points, providing insight into the interplay between
optimization and generalization in reinforcement learning.

1 Introduction

Reinforcement learning (RL) is a form of adaptive control where the system model is unknown, and one
seeks to estimate parameters of a controller through repeated interaction with the environment [5, 41]. This
framework gained attention recently for its ability to express problems that exhibit complicated dependences
between action selection and environmental response, i.e., when the cost function or system dynamics are
diﬃcult to express analytically. This is the case in supply chain management [21], power systems [22],
robotic manipulation [23], and games of various kinds [46, 38, 14]. Although the expressive capability of RL
continues to motivate new and diverse applications, its computational challenges remain doggedly persistent.
More speciﬁcally, RL is deﬁned by a Markov Decision Process [37]: each time an agent, starting from
one state, selects an action, and then transitions to a new state according to a distribution Markov in the
current state and action. Then, the environment reveals a reward informing the quality of that decision.
The goal of the agent is to select an action sequence which yields the largest expected accumulation of
rewards, deﬁned as the value. Two dominant approaches to RL have emerged since its original conception
from Bellman [3]. The ﬁrst, dynamic programming [50], writes the value as the expected one-step reward
plus all subsequent rewards (Bellman equations), and then proceeds by stochastic ﬁxed point iterations [47].
Combining dynamic programming approaches with nonlinear function parameterizations, as in [48], may
cause instability.

∗Department of Electrical and Systems Engineering, University of Pennsylvania, Philadelphia, PA 19104
†Computational and Information Sciences Directorate, U.S. Army Research Laboratory, Adelphi, MD 20783

1

 
 
 
 
 
 
Critic Method
GTD (SCGD)
GTD (A-SCGD)
TD(0)
TD(0)

Convergence Rate
O (cid:0)(cid:15)−3(cid:1)
O (cid:0)(cid:15)−5/2(cid:1)
O (cid:0)(cid:15)−4(cid:1)
O (cid:0)(cid:15)−2(cid:1)

State-Action Space
Continuous
Continuous
Continuous
Finite

Smoothness Assumptions Algorithm

Assumption 6
Assumptions 6 and 7
None
None

Alg 2
Alg 3
Alg 4
Alg 4

Table 1: Rates of Actor Critic with Policy Gradient Actor updates and diﬀerent Critic only methods

On the other hand, the alternative approach, policy search [44], hypothesizes actions are chosen according
to a parameterized distribution. It then repeatedly revises those parameters according to stochastic search
directions. Policy search has gained popularity due to its ability scale to large (or continuous) spaces and
exhibit global convergence, although its variance and hence sample complexity, may be impractically large.
Also worth mentioning is Monte Carlo search (“guess and check”) [2, 20], which is essential to reducing large
spaces to only viable hypotheses.

In this work, we focus on methods that operate in the intersection of dynamic programming and policy
search called actor-critic [25, 24]. Actor-critic is an online form of policy iteration [5] that inherits the
ability of policy search to scale to large or continuous spaces, while reducing its number of queries to the
environment. In particular, policy gradient method repeatedly revises policy parameter estimates through
policy gradient steps. Owing to the Policy Gradient Theorem [44], the policy gradient is the product of
two factors: the score function and the Q function. One may employ Monte Carlo rollouts to acquire the
Q-estimates, which under careful choice of the rollout horizon, may be shown to be unbiased [34, 51]. Doing
so, however, requires an inordinate amount of querying to the environment in order to generate trajectory
data.

Actor-critic replaces Monte-Carlo rollouts for the Q-value by stochastic approximates of solutions to
Bellman equations, i.e., temporal diﬀerence (TD) [40] or gradient temporal diﬀerence (GTD) [45] steps.
Intuitively, this weaving together of the merits of dynamic programming and policy search yields comparable
scalability properties to policy search while reducing its sample complexity. However, the iteration (and
sample) complexity of actor-critic is noticeably absent from the literature, which is striking due to its
foundational role in modern reinforcement learning systems [30, 38], and the fact that eﬀorts to improve
upon it also only establish asymptotics [8]. This absence is due to the fact that actor-critic algorithms exhibit
two technical challenges: (i) their sample path is dependent (non i.i.d.), and (ii) their search directions are
biased.

In this work, we mitigate these challenges through (i) the use of a Monte Carlo rollout scheme to esti-
mate the policy gradient, given a value function estimate; and (ii) employing recently established sample
complexity results of policy evaluation under linear basis expansion. Doing so permits us to characterize
for the ﬁrst time the complexity of actor-critic algorithms under a few canonical settings and schemes for
the critic (policy evaluation) update. Our results hinge upon viewing policy search as a form of stochastic
gradient method for maximizing a non-convex function, where the ascent directions are biased. Moreover,
the magnitude of this bias is determined the number of critic steps. This perspective treats actor-critic a
form of two time-scale algorithm [9], whose asymptotic stability is well-known via dynamical systems tools
[26, 10]. To wield these approaches to establish ﬁnite-time performance, however, concentration probabilities
and geometric ergodicity assumptions of the Markov dynamics are required – see [10]. To obviate these com-
plications and exploit recent unbiased sampling procedures [34, 51], we focus on the case where independent
trajectory samples are acquirable through querying the environment.

Our main result establishes that actor-critic, independent of any critic method, exhibits convergence to
stationary points of the value function that are comparable to stochastic gradient ascent in the non-convex
regime. We note that a key distinguishing feature from standard non-convex stochastic programming is
that the rates are inherently tied to the bias of the search direction which is determined by the choice of
critic scheme. In fact, our methodology is such that a rate for actor-critic can be derived for any critic only
method for which a convergence rate in expectation on the parameters can be expressed. In particular, we

2

characterize the rates for actor-critic with temporal diﬀerence (TD) and gradient TD (GTD) critic steps.
Furthermore, we propose an Accelerated GTD (A-GTD) method derived from accelerations of stochastic
compositional (quasi-) gradient descent [49], which converges faster than TD and GTD.

In summary, for the continuous spaces, we establish that GTD and A-GTD converge faster than TD. In
particular, this introduces a trade oﬀ between the smoothness assumptions and the rates derived (see Table
1). TD has no additional smoothness assumptions, and it achieves a rate of O((cid:15)−4). This rate is analogous
to the non-convex analysis of stochastic compositional gradient descent. Adding a smoothness assumption,
GTD achieves the faster rate of O((cid:15)−3). By requiring an additional strong convexity assumption, we ﬁnd
that A-GTD achieves the fastest convergence rate of O((cid:15)−5/2). For the case of ﬁnite state action space, actor
critic achieves a convergence rate of O((cid:15)−2). Overall, the contribution in terms of sample complexities of
diﬀerent actor-critic algorithms may be found in Table 1.

We evaluate actor-critic with TD, GTD, and A-GTD critic updates on a navigation problem. We ﬁnd
that indeed A-GTD converges faster than both GTD and TD. Interestingly, the stationary point it reaches
is worse than GTD or TD. This suggests that the choice of critic scheme illuminates an interplay between
optimization and generalization that is less-well understood in reinforcement learning [13, 12]. Surprisingly,
we ﬁnd that TD converges faster than GTD which we postulate is an artifact of the selection of the feature
space parametrization. A detailed discussion on the results and implications can be found in section 7. The
remainder of the paper is organized as follows. Section 2 describes the problem of Reinforcement Learning
and characterizes common assumptions which we use in our analysis.
In section 3 we derive a generic
actor-critic algorithm from an optimization perspective and describe how the algorithm would be amended
given diﬀerent policy evaluation methods. The derivation of the convergence rate for generic actor-critic is
presented in section 4, and the speciﬁc analysis for Gradient, Accelerated Gradient, and vanilla Temporal
diﬀerence are characterized in sections 5 and 6.

2 Reinforcement Learning

In reinforcement learning (RL), an agent moves through a state space S and takes actions that belong to
some action set A, where the state/action spaces are assumed to be continuous compact subsets of Euclidean
space: S ⊂ Rq and A ⊂ Rp. Every time an action is taken, the agent transitions to its next state that depends
on its current state and action. Moreover, a reward is revealed by the environment. In this situation, the
agent would like to accumulate as much reward as possible in the long term, which is referred to as value.
Mathematically this problem deﬁnition may be encapsulated as a Markov decision process (MDP), which
is a tuple (S, A, P, R, γ) with Markov transition density P(s(cid:48) | s, a) : S × A → P(S) that determines the
probability of moving to state s(cid:48). Here, γ ∈ (0, 1) is the discount factor that parameterizes the value of a
given sequence of actions, which we will deﬁne shortly.

At each time t, the agent executes an action at ∈ A given the current state st ∈ S, following a possibly
stochastic policy π : S → P(A), i.e., at ∼ π(· | st). Then, given the state-action pair (st, at), the agent
t ∼ P(· | st, at) according to
observes a (deterministic) reward rt = R(st, at) and transitions to a new state s(cid:48)
a transition density that is Markov. For any policy π mapping states to actions, deﬁne the value function
Vπ : S → R as

Vπ(s) = Eat∼π(·|st),st+1∼P(·|st,at)

γtrt | s0 = s

,

(1)

(cid:18) ∞
(cid:88)

(cid:19)

t=0

which is a measure of the long term average reward accumulation discounted by γ. We can further deﬁne
the value Vπ : S × A → R conditioned on a given initial action as the action-value, or Q-function as
Qπ(s, a) = E(cid:0) (cid:80)∞
t=0 γtrt | s0 = s, a0 = a(cid:1). Given any initial state s0, the goal of the agent is to ﬁnd
the optimal policy π that maximizes the long-term return Vπ(s0), i.e., to solve the following optimization
problem

max
π∈Π

J(π) := Vπ(s0).

(2)

In this work, we investigate actor-critic methods to solve (2), which is a hybrid RL method that fuses key
properties of policy search and approximate dynamic programming. To ground the discussion, we ﬁrst derive

3

the canonical policy search technique called policy gradient method, and explain how actor-critic augments
policy gradient. Begin by noting that to address (2), one must search over an arbitrarily complicated
function class Π which may include those which are unbounded and discontinuous. To mitigate this issue,
we parameterize the policy π by a vector θ ∈ Rd, i.e., π = πθ, yielding RL tools called policy gradient methods
[24, 8, 15]. Under this speciﬁcation, the search over arbitrarily complicated function class Π to (2) may be
reduced to Euclidean space Rd, i.e., a vector-valued optimization maxθ∈Rd J(θ) := Vπθ (s0). Subsequently,
we denote J(πθ) by J(θ) for notational convenience. We ﬁrst make the following standard assumption on
the regularity of the MDP problem and the parameterized policy πθ, which are the same conditions as [52].

Assumption 1. Suppose the reward function R and the parameterized policy πθ satisfy the following con-
ditions:

(i) The absolute value of the reward R is bounded uniformly by UR, i.e., |R(s, a)| ∈ [0, UR] for any

(s, a) ∈ S × A.

(ii) The policy πθ is diﬀerentiable with respect to θ, and the score function ∇ log πθ(a | s) is LΘ-Lipschitz

and has bounded norm, i.e., for any (s, a) ∈ S × A,

(cid:107)∇ log πθ1(a | s) − ∇ log πθ2(a | s)(cid:107) ≤ LΘ · (cid:107)θ1 − θ2(cid:107),
(cid:107)∇ log πθ(a | s)(cid:107) ≤ BΘ,

for any θ.

for any θ1, θ2,

(3)

(4)

Note that the boundedness of the reward function in Assumption 1(i) is standard in policy search algo-
rithms [7, 8, 15, 53]. Observe that with R, we have the Q-function is absolutely upper bounded by UR/(1−γ),
since by deﬁnition

|Qπθ (s, a)| ≤

∞
(cid:88)

t=0

γt · UR =

UR
1 − γ

,

for any (s, a) ∈ S × A.

(5)

The same bound also applies for Vπθ (s) for any πθ and s ∈ S and thus for the objective J(θ) which is deﬁned
as Vπθ (s0), i.e.,

|Vπθ (s)| ≤

UR
1 − γ

,

for any s ∈ S,

|J(θ)| ≤

UR
1 − γ

.

(6)

We note that the conditions (3) and (4) have appeared in recent analyses of policy search [15, 35, 32], and are
satisﬁed by canonical policy parameterizations such as Boltzmann policy [25] and Gaussian policy [18]. For
example, for Gaussian policy1 in continuous spaces, πθ(· | s) = N (φ(s)(cid:62)θ, σ2), where N (µ, σ2) denotes the
Gaussian distribution with mean µ and variance σ2. Then the score function has the form [a−φ(s)(cid:62)θ]φ(s)/σ2,
which satisﬁes (3) and (4) if the feature vectors φ(s) have bounded norm, the parameter θ lies some bounded
set, and the action a ∈ A is bounded.

We now state the following assumptions which are required for the base results. Additional assumptions

on smoothness are speciﬁc to the critic only method used, and are detailed in Table 1.

Assumption 2. The random tuples (st, at, s(cid:48)
the Markov reward process independently across time.

t, a(cid:48)

t), t = 0, 1, . . . are drawn from the stationary distribution of

As pointed out by [17], the i.i.d. assumption does not hold in practice, but it is standard dealing with
convergence bounds in reinforcement learning. Although there has been an eﬀort to characterize the rate
of convergence in expectation for critic only methods with Markov noise [6], theses results do not result
in a bound of the desired form in Assumption 5. For this reason, we study speciﬁcally the case where
Assumption 2 holds, and leave the study of Markov noise for future work. The general conditions for
stability of trajectories with Markov dependence, i.e., negative Lyapunov exponents for mixing rates, may
be found in [29].

1We observe that in practice, the action space A is bounded, which requires a truncated Gaussian policy to be used over A,

as in [32].

4

Assumption 3. For any state action pair (s, a) ∈ S × A, the norm of the feature representation ϕ(s, a) is
bounded by a constant C2 ∈ R+.

Assumption 3 is easily implementable in practice by normalizing the feature representation. Because the
score function is bounded by BΘ (c.f. Assumption 1) and the reward function is bounded, we have that for
some constant C3 ∈ R+

(cid:107)J(θk)(cid:107) ≤ C3

(7)

for all (s, a) ∈ S × A.

Additionally, we assume that the estimate of the gradient ∇J(θ) conditioned on the ﬁltration Fk is

bounded by some ﬁnite variance σ2

Assumption 4. Let ˆ∇J(θ) be a possibly biased estimate ∇J(θ). There exists a ﬁnite σ2 such that,

E((cid:107) ˆ∇J(θ)(cid:107)|Fk) ≤ σ2.

(8)

Generally, the value function is nonconvex with respect to the parameter θ, meaning that obtaining a
globally optimal solution to (2) is out of reach unless the problem has additional structured properties, as
in phase retrieval [39], matrix factorization [27], and tensor decomposition [19], among others. Thus, our
goal is to design actor-critic algorithms to attain stationary points of the value function J(θ). Moreover, we
characterize the sample complexity of actor-critic, a noticeable gap in the literature for an algorithmic tool
decades old [25] at the heart of the recent innovations of artiﬁcial intelligence architectures [38].

3 From Policy Gradient to Actor-Critic

In this section, we derive actor-critic method [25] from an optimization perspective: we view actor-critic as
a way of doing stochastic gradient ascent with biased ascent directions, and the magnitude of this bias is
determined by the number of critic evaluations done in the inner loop of the algorithm. The building block
of actor-critic is called policy gradient method, a type of direct policy search, based on stochastic gradient
ascent. Begin by noting that the gradient of the objective J(θ) with respect to policy parameters θ, owing
to the Policy Gradient Theorem [44], has the following form:

(cid:90)

∇J(θ) =

∞
(cid:88)

s∈S,a∈A

t=0

γt · p(st = s | s0, πθ) · ∇πθ(a | s) · Qπθ (s, a)dsda

(9)

=

=

=

1
1 − γ

1
1 − γ
1
1 − γ

(cid:90)

s∈S,a∈A

(cid:90)

s∈S,a∈A

(1 − γ)

∞
(cid:88)

t=0

γt · p(st = s | s0, πθ) · ∇πθ(a | s) · Qπθ (s, a)dsda

ρπθ (s) · πθ(a | s) · ∇ log[πθ(a | s)] · Qπθ (s, a)dsda

· E(s,a)∼ρθ(·,·)

(cid:2)∇ log πθ(a | s) · Qπθ (s, a)(cid:3).

(10)

In the preceding expression, p(st = s | s0, πθ) denotes the probability of state st equals s given initial state
s0 and policy θ, which is occasionally referred to as the occupancy measure, or the Markov chain transition
density induced by policy π. Moreover, ρπθ (s) = (1 − γ) (cid:80)∞
t=0 γtp(st = s | s0, πθ) is the ergodic distribution
associated with the MDP for ﬁxed policy, which is shown to be a valid distribution [44]. For future reference,
we deﬁne ρθ(s, a) = ρπθ (s) · πθ(a | s). The derivative of the logarithm of the policy ∇ log[πθ(· | s)] is usually
referred to as the score function corresponding to the probability distribution πθ(· | s) for any s ∈ S.

Next, we discuss how (10) can be used to develop stochastic methods to address (2). Unbiased samples
of the gradient ∇J(θ) are required to perform the stochastic gradient ascent, which hopefully converges to a
stationary solution of the nonconvex maximization. One way to obtain an estimate of the gradient ∇J(θ) is
to evaluate the score function and Q function at the end of a rollout whose length is drawn from a geometric

5

If the Q function evaluation is unbiased, then the
distribution with parameter 1 − γ [52][Theorem 4.3].
stochastic estimate of the gradient ∇J(θ) is unbiased as well. We therefore deﬁne the stochastic estimate by

ˆ∇J(θ) =

1
1 − γ

ˆQπθ (sT , aT )∇ log πθ(aT |sT ).

(11)

We consider the case where the Q function admits a linear parametrization of the form ˆQπθ (s, a) = ξ(cid:62)ϕ(s, a),
which in the literature on policy search is referred to as the critic [25], as it “criticizes” the performance of
actions chosen according to policy π. Here ξ ∈ Ξ ⊂ Rp and ϕ : X × A → Rp is a (possibly nonlinear) feature
map such as a network of radial basis functions or an auto-encoder. Moreover, we estimate the parameter ξ
that deﬁnes the Q function from a policy evaluation (critic only) method after some TC(k) iterations, where
k denotes the number of policy gradient updates. Thus, we may write the stochastic gradient estimate as

ˆ∇J(θ) =

1
1 − γ

ξ(cid:62)
k ϕ(sT , aT )∇ log πθ(aT |sT ).

(12)

k ϕ(sT , aT ) | θ, s, a] = Q(s, a), then E[ ˆ∇J(θ) | θ] = ∇J(θ)
If the estimate of the Q function is unbiased, i.e., E[ξ(cid:62)
(c.f.
[52][Theorem 4.3]). Typically, critic only methods do not give unbiased estimates of the Q function;
however, in expectation the rate at which their bias decays is proportional to the number of Q estimation
steps. In particular, denote ξ∗ as the parameter for which the Q estimate is unbiased:

E[ξ(cid:62)

∗ ϕ(s, a)] = E[ ˆQπθ (s, a)] = Q(s, a).

(13)

Hence, by adding and subtracting the true estimate of the parametrized Q function to (12), we arrive at the
fact the policy search direction admits the following decomposition:

ˆ∇J(θ) =

1
1 − γ

(ξk − ξ∗)(cid:62)ϕ(sT , aT )∇ log πθ(aT |sT ) +

1
1 − γ

(ξ∗)(cid:62)ϕ(sT , aT )∇ log πθ(aT |sT ).

(14)

The second term in is the unbiased estimate of the gradient ∇J(x), whereas the ﬁrst deﬁnes the diﬀerence of
the critic parameter at iteration k with the true estimate ξ∗. For linear parameterizations of the Q function,
policy evaluation methods establish convergence in mean of the bias

E[(cid:107)ξk − ξ∗(cid:107)] ≤ g(k),

(15)

where g(k) is some decreasing function. We address cases where the critic bias decays at rate k−b for
b ∈ (0, 1], due to the fact that several state of the art works on policy evaluation may be mapped to the form
(15) for this speciﬁcation [49, 17]. We formalize this with the following assumption.

Assumption 5. The expected error of the critic parameter is bounded by O(k−b) for some b ∈ (0, 1], i.e.,
there exists constant L1 > 0 such that

E[(cid:107)ξk − ξ∗(cid:107)] ≤ L1k−b.

(16)

Recently, alternate rates have been established as O(log k/k); however, they concede that O(1/k) rates
may be possible [6, 54]. Thus, we subsume recent sample complexity characterizations of policy evaluation
as (15) in Assumption 5.

As such, (14) is nearly a valid ascent direction: it is approximately an unbiased estimate of the gradient
∇J(θ) since the ﬁrst term becomes negligible as the number of critic estimation steps increases. Based
upon this observation, we propose the following variant of actor-critic method [25]: run a critic estimator
(policy evaluator) for TC(k) steps, whose output is critic parameters ξTC (k). We denote the critic estimator
by Critic:N → Rp which returns the parameter ξTC (k) ∈ Rp after TC(k) ∈ N iterations. Then, simulate a
trajectory of length Tk, where Tk is geometrically distributed with parameter 1 − γ, and update the actor
(policy) parameters θ as:

θk+1 = θk + ηk ˆ∇J(θk) = θk + ηk

1
1 − γ

ξ(cid:62)
TC (k)ϕ(sTk , aTk )∇ log πθk (sTk , aTk |θk)

(17)

6

Algorithm 1 Generic Actor-Critic
Require:

s0 ∈ Rn, θ0, ξ0, stepsize {ηk}, Policy evaluation method Critic: N → Rp, γ ∈ (0, 1)

1: for k = 1, . . . do
2:
3:
4:
5:

ξTC (k) ← Critic(TC(k))
Draw Tk ∼ Geom(1 − γ)
(sTk , aTk ) ← rollout of πθk with length Tk
θk+1 ← θk + 1

1−γ ηkξ(cid:62)

TC (k)ϕ(sTk , aTk )∇ log πθk (sTk , aTk |θk)

We summarize the aforementioned procedure, which is agnostic to particular choice of critic estimator, as
Algorithm 1.
Examples of Critic Updates We note that Critic:N → Rp admits two canonical forms: temporal diﬀer-
ence (TD) [40] and gradient temporal diﬀerence (GTD)-based estimators [45]. The TD update for the critic
is given as

t, a(cid:48)
whereas for the GTD-based estimator for the critic, we consider the update

t) − ϕ(st, at)) ϕ(st, at) ,

δt = rt + γξ(cid:62)

t (ϕ(s(cid:48)

ξt+1 = ξt + αtδtϕ(st, at)

δt = rt + ξ(cid:62)

t (γϕ(s(cid:48)
ξt+1 = (1 − λαt)ξt − 2αtzt+1[γϕ(s(cid:48)

t) − ϕ(st, at)) ,
t, a(cid:48)

t, a(cid:48)

t) − ϕ(st, at)]

zt+1 = (1 − βt)zt + βtδt,

(18)

(19)

We further analyze a modiﬁcation of GTD updates proposed by [49] that incorporates an extrapolation
technique to reduce bias in the estimates and improve error dependency, which is distinct from accelerated
stochastic approximation with Nesterov Smoothing[31]. With y0 = 0 and zt deﬁned for t = 1, . . . , the
accelerated GTD (A-GTD) update becomes

ξt+1 = ξt − 2αt(γϕ(s(cid:48), a(cid:48)) − ϕ(s, a))yt
(cid:19)

zt+1 = −

− 1

ξt +

ξt+1

(cid:18) 1
βt

1
βt

(20)

yt+1 = (1 − βt)yt + βt(r(s, a) + z(cid:62)

t+1 (γϕ(s(cid:48), a(cid:48)) − ϕ(s, a))

Subsequently, we shift focus to characterizing the mean convergence of actor-critic method given any policy
evaluation method satisfying (15) in Section 4. Then, we specialize the sample complexity of actor-critic
to the cases associated with critic updates (18) - (20), which we respectively call Classic (Algorithm 4),
Gradient (Algorithm 2), and Accelerated Actor-Critic (Algorithm 3).

4 Convergence Rate of Generic Actor-Critic

In this section, we derive the rate of convergence in expectation for the variant of actor-critic deﬁned in
Algorithm 1, which is agnostic to the particular choice of policy evaluation method used to estimate the Q
function used in the actor update. Unsurprisingly, we establish that the rate of convergence in expectation
for actor-critic depends on the critic update used. Therefore, we present the main result in this paper for any
generic critic method. Thereafter, we specialize this result to two well-known choices of policy evaluation
previously described (18) - (19), as well as a new variant that employs acceleration (20).

We begin by noting that under Assumption 1, one may establish Lipschitz continuity of the policy gradient

∇J(θ) [52][Lemma 4.2].

Lemma 1 (Lipschitz-Continuity of Policy Gradient). The policy gradient ∇J(θ) is Lipschitz continuous
with some constant L > 0, i.e., for any θ1, θ2 ∈ Rd

(cid:107)∇J(θ1) − ∇J(θ2)(cid:107) ≤ L · (cid:107)θ1 − θ2(cid:107).

(21)

7

This lemma allows us to establish an approximate ascent lemma for a random variable Wk deﬁned by

Wk = J(θk) − Lσ2

∞
(cid:88)

j=k

η2
j ,

(22)

where J(θ) is deﬁned in (2), σ2 is deﬁned in Assumption 4, and L is the Lipshitz constant of the gradient
from Lemma 1. Unless otherwise stated, to alleviate notation, we denote ξk as short-hand for ξTC (k).

Lemma 2. Consider the actor parameter sequence deﬁned by Algorithm 1. The sequence {Wk} deﬁned in
(22) satisﬁes the inequality

E[Wk+1|Fk] ≥ Wk + ηk(cid:107)∇J(θk)(cid:107)2 − ηkCE[(cid:107)ξk − ξ∗(cid:107)|Fk]

(23)

where C = BΘC2C3/(1 − γ), with BΘ the bound on the score function as in Assumption 1, C2 the bound on
the feature map in Assumption 3, and C3 as the bound on the value function in (7).

Proof. By deﬁnition of Wk, we write the expression for Wk+1

Wk+1 = J(θk+1) − Lσ2

∞
(cid:88)

η2
j .

j=k+1

By the Mean Value Theorem, there exists ˜θk = λθk + (1 − λ)θk+1 for some λ ∈ [0, 1] such that

J(θk+1) = J(θk) + (θk+1 − θk)(cid:62)∇J(˜θk).

Substitute this expression for J(θk+1) in (24)

Wk+1 = J(θk) + (θk+1 − θk)(cid:62)∇J(˜θk) − Lσ2

∞
(cid:88)

η2
j .

j=k+1

Add and subtract (θk+1 − θk)(cid:62)∇J(θk) to the right hand side of (26) to obtain

Wk+1 = J(θk) + (θk+1 − θk)(cid:62) (cid:16)

(cid:17)
∇J(˜θk) − ∇J(θk)

+ (θk+1 − θk)(cid:62)∇J(θk) − Lσ2

∞
(cid:88)

η2
j .

j=k+1

(24)

(25)

(26)

(27)

By Cauchy Schwartz, we know (θk+1 − θk)(cid:62) (cid:16)
≥ −(cid:107)θk+1 − θk(cid:107)(cid:107)∇J(˜θk) − J(θk)(cid:107). Further,
∇J(˜θk) − ∇J(θk)
by the Lipschitz continuity of the gradient, we know (cid:107)∇J(˜θk) − ∇J(θk)(cid:107) ≤ L(cid:107)˜θk − θk(cid:107). Therefore, we have

(cid:17)

(θk+1 − θk)(cid:62) (cid:16)

∇J(˜θk) − J(θk)

(cid:17)

≥ −L(cid:107)˜θk − θk(cid:107) · (cid:107)θk+1 − θk(cid:107) ≥ −L(cid:107)θk+1 − θk(cid:107)2 ,

(28)

where the second inequality comes from substituting ˜θk = (1 − λ)θk+1 + λθk. We substitute this expression
into the deﬁnition of Wk+1 in (27) to obtain

Wk+1 ≥ J(θk) + (θk+1 − θk)(cid:62)∇J(θk) − L(cid:107)θk+1 − θk(cid:107)2 − Lσ2

∞
(cid:88)

η2
j .

j=k+1

(29)

Take the expectation with respect to the ﬁltration Fk, substitute the deﬁnition for the actor update (17)

E[Wk+1|Fk] ≥ J(θk) + E[θk+1 − θk|Fk](cid:62)∇J(θk) + −LE[(cid:107)ηk ˆ∇J(θk)(cid:107)2|Fk] − Lσ2

∞
(cid:88)

η2
j .

j=k+1

(30)

8

Together, with the fact that this update θk+1 − θk is the stepsize ηk times the estimate of the gradient which
has bounded variance (8) (Assumption 4), we obtain

E[Wk+1|Fk] ≥ J(θk) + E[θk+1 − θk|Fk](cid:62)∇J(θk) − Lσ2η2

k − Lσ2

∞
(cid:88)

η2
j .

j=k+1

(31)

The terms on the right hand side outside the expectation may be identiﬁed as Wk [cf. (22)] by deﬁnition,
which allows us to write

E[Wk+1|Fk] ≥ Wk + E[θk+1 − θk|Fk](cid:62)∇J(θk).
Therefore, we are left to show that the last term on the right-hand side of the preceding expression is “nearly”
an ascent direction, i.e.,

(32)

E[θk+1 − θk|Fk](cid:62)∇J(θk) ≥ ηk(cid:107)∇J(θk)(cid:107)2 − ηkCE[(cid:107)ξk − ξ∗(cid:107)2|Fk].

(33)

and how far from an ascent direction it is depends on the critic estimate bias E[(cid:107)ξk − ξ∗(cid:107)2|Fk]. From
Algorithm 1, the actor parameter update may be written as

θk+1 − θk =

1
1 − γ

ηkξ(cid:62)

k ϕ(sTk , aTk )∇ log π(sTk , aTk |θk).

(34)

(s, a) = ξ(cid:62)

Add and subtract ηkξ(cid:62)
∗ ϕ(sTk , aTk )∇ log π(sTk , aTk |θk)/(1 − γ) to (34) where ξ∗ is such that the estimate
ˆQπθk
(s, a) is unbiased. Hence, (cid:107)ξTk − ξ∗(cid:107) represents the distance between the critic
parameters corresponding to the biased estimate after k critic only steps and the true estimate of the Q
function.

∗ ϕ(s, a) = Qπθk

1
1 − γ

θk+1 − θk =

ηk(ξk − ξ∗)(cid:62)ϕ(sTk , aTk )∇ log π(sTk , aTk |θk) +

∗ ϕ(sTk , aTk )∇ log π(sTk , aTk |θk).
(35)
Here we recall (12) and (13) from the derivation of the algorithm, that is that the expected value of the
stochastic estimate given θ is unbiased. Therefore, by taking the expectation of (35) with respect to the
ﬁltration Fk, we obtain

ηkξ(cid:62)

1
1 − γ

E[θk+1 − θk|Fk] =

1
1 − γ

ηkE[(ξk − ξ∗)(cid:62)ϕ(sTk , aTk )∇ log π(sTk , aTk |θk)|Fk] + ηk∇J(θk).

(36)

Take the inner product with ∇J(θk) on both sides

E[θk+1 − θk|Fk](cid:62)∇J(θk) =

1
1 − γ

ηkE[(ξk − ξ∗)(cid:62)ϕ(sTk , aTk )∇ log π(sTk , aTk |θk)|Fk](cid:62)∇J(θk) + ηk(cid:107)∇J(θk)(cid:107)2
(37)

The ﬁrst term on the right-hand side is lower-bounded by the negative of its absolute value, i.e.,

E[θk+1 − θk|Fk](cid:62)∇J(θk) ≥

ηk|E[(ξk − ξ∗)(cid:62)ϕ(sTk , aTk )∇ log π(sTk , aTk |θk)|Fk](cid:62)∇J(θk)| + ηk(cid:107)∇J(θk)(cid:107)2.
(38)
Next, we apply Cauchy Schwartz to the ﬁrst term on the right-hand side of the previous expression, followed
by Jensen’s Inequality, and then Cauchy Schwartz again, to obtain

−1
1 − γ

E[θk+1−θk|Fk](cid:62)∇J(θk)

(39)

≥ −

≥ −

≥ −

1
1 − γ
1
1 − γ
1
1 − γ

ηk(cid:107)E[(ξk − ξ∗)(cid:62)ϕ(sTk , aTk )∇ log π(sTk , aTk |θk)| |Fk](cid:107) · (cid:107)∇J(θk)(cid:107) + ηk(cid:107)∇J(θk)(cid:107)2

ηkE[|(ξk − ξ∗)(cid:62)ϕ(sTk , aTk )|(cid:107)∇ log π(sTk , aTk |θk)(cid:107)| |Fk] · (cid:107)∇J(θk)(cid:107) + ηk(cid:107)∇J(θk)(cid:107)2

ηkE[(cid:107)(ξk − ξ∗)(cid:107) · (cid:107)ϕ(sTk , aTk )(cid:107) · (cid:107)∇ log π(sTk , aTk |θk)(cid:107)| |Fk] · (cid:107)∇J(θk)(cid:107) + ηk(cid:107)∇J(θk)(cid:107)2.

9

Because the score function ((cid:107)∇ log π(s, a|θ)(cid:107) ≤ BΘ), the feature map ((cid:107)ϕ(s, a)(cid:107) ≤ C2), and the gradient
((cid:107)∇J(θk)(cid:107) ≤ C3) are bounded, we deﬁne C be the product of these constants with 1/(1 − γ):

(cid:107)ϕ(sTk , aTk )(cid:107) · (cid:107)∇ log π(sTk , aTk |θk)(cid:107) · (cid:107)∇J(θk)(cid:107) ≤

BΘC2C3
1 − γ

=: C.

which my be substituted into (39) to write

E[θk+1 − θk|Fk](cid:62)∇J(θk) ≥ −CηkE[(cid:107)ξk − ξ∗(cid:107)|Fk] + ηk(cid:107)∇J(θk)(cid:107)2

Now, we can express this relationship in terms of Wk by substituting back into (32):

E[Wk+1|Fk] ≥ Wk − CηkE[(cid:107)ξk − ξ∗(cid:107)|Fk] + ηk(cid:107)∇J(θk)(cid:107)2

which is as stated in (23).

From(23) (Lemma 2), consider taking the total expectation

E[Wk+1] ≥ E[Wk] + ηkE[(cid:107)∇J(θk)(cid:107)2] − ηkCE[(cid:107)ξk − ξ∗(cid:107)].

(40)

(41)

(42)

(43)

This almost describes an ascent of the variable Wk. Because the norm of the gradient is non-negative, if the
term E[(cid:107)ξk − ξ∗(cid:107)] was removed, an argument could be constructed to show that in expectation, the gradient
converges to zero. Unfortunately, the term of the error in critic estimate complicates the picture. However,
by Assumption 5 (which is not really an assumption but rather a fundamental property of most common
policy evaluation schemes), we know that the error goes to zero in expectation as the number of critic steps
increases. Thus, we leverage this property to derive the sample complexity of actor-critic (Algorithm 1).

We now present our main result, which is the convergence rate of actor-critic method when the algorithm
remains agnostic to the particular chocie of critic scheme. We characterize the rate of convergence by the
smallest number K(cid:15) of actor updates k required to attain a value function gradient smaller (cid:15), i,.e.,

K(cid:15) = min{k :

inf
0≤m≤k

(cid:107)∇J(θm)(cid:107)2 < (cid:15)}.

(44)

Theorem 1. Suppose the step-size satisﬁes ηk = k−a for a > 0 and the critic update satisﬁes Assumption 5.
When the critic bias converges to null as O(k−1), i.e., b = 1 in (15), then TC(k) = k + 1 critic updates occur
per actor update. Alternatively, if the critic bias converges to null more slowly b ∈ (0, 1) then TC(k) = k
critic updates per actor update are chosen. Then the actor sequence deﬁned by Algorithm 1 satisﬁes

K(cid:15) ≤ O

(cid:16)

(cid:15)−1/(cid:96)(cid:17)

, where (cid:96) = min{a, 1 − a, b}

(45)

Minimizing over a yields actor step-size ηk = k−1/2. Moreover, depending on the rate b of attenuation of
the critic bias [cf. (15)], the resulting sample complexity is:

K(cid:15) ≤

(cid:40)

O (cid:0)(cid:15)−1/b(cid:1)
O (cid:0)(cid:15)−2(cid:1) .

if b ∈ (0, 1/2)
if b ∈ (1/2, 1]

(46)

Proof. Begin by substituting the deﬁnition for Wk [cf. (22)] into Lemma 2, i.e., (23) to write

E[J(θk+1)|Fk] − Lσ2

∞
(cid:88)

j=k+1

j ≥ J(θk) − Lσ2
η2

∞
(cid:88)

j=k

j + ηk(cid:107)∇J(θk)(cid:107)2 − ηkCE[(cid:107)ξk − ξ∗(cid:107)|Fk].
η2

(47)

The term Lσ2 (cid:80)∞

j=k+1 η2

j cancels from both sides. Take the total expectation

E[J(θk+1)] ≥ E[J(θk)] − Lσ2η2

k + ηk(cid:107)∇J(θk)(cid:107)2 − ηkCE[(cid:107)ξk − ξ∗(cid:107)].

(48)

10

Deﬁne Uk := J(θ∗) − J(θk) where θ∗ is the solution of (2) when the policy is parametrized by θ. By this
deﬁnition, we know that Uk is non-negative for all θk. Add J(θ∗) to both sides of the inequality and rearrange
terms

ηkE[(cid:107)∇J(θk)(cid:107)] ≤ E[Uk] − E[Uk+1] + Lσ2η2
Divide both sides by ηk and take the sum over {k − N, . . . , k} for some integer 1 < N < k

k + ηkCE[(cid:107)ξk − ξ∗(cid:107)].

(49)

k
(cid:88)

j=k−N

E[(cid:107)∇J(θj)(cid:107)2] ≤

k
(cid:88)

j=k−N

1
ηj

(E[Uj] − E[Uj+1]) + Lσ2

k
(cid:88)

ηj + C

k
(cid:88)

j=k−N

j=k−N

E[(cid:107)ξj − ξ∗(cid:107)].

(50)

Add and subtract 1/ηk−N −1E[Uk−N ] on the right hand side. This allows us to write

k
(cid:88)

j=k−N

E[(cid:107)∇J(θj)(cid:107)2] ≤

k
(cid:88)

j=k−N

(cid:19)

(cid:18) 1
ηj

−

1
ηj−1

E[Uj] −

1
ηk

E[Uk+1] +

1
ηk−N −1

E[Uk−N ]

k
(cid:88)

+ Lσ2

ηj + C

k
(cid:88)

E[(cid:107)ξj − ξ∗(cid:107)].

j=k−N

j=k−N

(51)

By deﬁnition of Uk, E[Uk+1] ≥ 0. Therefore we can omit it from the right hand side of (51). Further,
we know that J(θ∗) ≤ UR/(1 − γ) as a consequence from Assumption 1(i) [see (6)]. Hence we have Uk ≤
2UR/(1 − γ) =: C4 for all k. Substituting this fact into the preceding expression yields

k
(cid:88)

j=k−N

E[(cid:107)∇J(θj)(cid:107)2] ≤

k
(cid:88)

j=k−N

(cid:19)

(cid:18) 1
ηj

−

1
ηj−1

C4 +

1
ηk−N −1

C4 + Lσ2

k
(cid:88)

ηj + C

k
(cid:88)

E[(cid:107)ξj − ξ∗(cid:107)]. (52)

j=k−N

j=k−N

By unraveling the telescoping sum, the ﬁrst two terms are equal to C4/ηk

k
(cid:88)

j=k−N

E[(cid:107)∇J(θj)(cid:107)2] ≤

C4
ηk

+ Lσ2

k
(cid:88)

ηj + C

k
(cid:88)

j=k−N

j=k−N

E[(cid:107)ξj − ξ∗(cid:107)].

(53)

Substitute the ηk = k−a for the step size and apply the bound (15) in Assumption 5 using extended notation
ξk = ξTC (k)

k
(cid:88)

j=k−N

E[(cid:107)∇J(θj)(cid:107)2] ≤ C4ka + Lσ2

k
(cid:88)

j−a + CL1

k
(cid:88)

TC(j)−b.

j=k−N

j=k−N

(54)

We break the remainder of the proof into two cases due to the fact that the right-hand side of the preceding
expression simpliﬁes when b = 1, and is more intricate when 0 < b < 1. We focus on the later case ﬁrst.

Case (i): b ∈ (0, 1) Consider the case where b ∈ (0, 1). Set TC(k) = k. Substitute the integration rule,
namely that (cid:80)k

j=k−N j−a ≤ k1−a − (k − N − 1)1−a, into (54) to obtain:

k
(cid:88)

j=k−N

E[(cid:107)∇J(θj)(cid:107)2] ≤ C4ka +

Lσ2
1 − a

(cid:0)k1−a − (k − N − 1)1−a(cid:1) +

CL1
1 − b

(cid:0)k1−b − (k − N − 1)1−b(cid:1) .

(55)

Divide both sides by k and set N = k − 1

1
k

k
(cid:88)

j=1

E[(cid:107)∇J(θj)(cid:107)2] ≤ C4ka−1 +

Lσ2
1 − a

k−a +

CL1
1 − b

k−b.

Suppose k = K(cid:15) so that we may write

1
K(cid:15)

K(cid:15)(cid:88)

j=1

E[(cid:107)∇J(θj)(cid:107)2] ≤ O (cid:0)K a−1

(cid:15) + K −a

(cid:15) + K −b

(cid:15)

(cid:1) .

11

(56)

(57)

By deﬁnition of K(cid:15) [c.f. (44)], we have that E[(cid:107)∇J(θj)(cid:107)2] > (cid:15) for all j = 1, . . . , K(cid:15), so

(cid:15) ≤

1
K(cid:15)

K(cid:15)(cid:88)

j=1

E[(cid:107)∇J(θj)(cid:107)2] ≤ O (cid:0)K a−1

(cid:15) + K −a

(cid:15) + K −b

(cid:15)

Deﬁning (cid:96) = min{a, 1 − a, b}, the preceding expression then implies

which by inverting the expression, yields the sample complexity

K(cid:15) ≤ O((cid:15)−1/(cid:96)).

(cid:15) ≤ O(K −(cid:96)

(cid:15) ),

(cid:1) .

(58)

(59)

(60)

Case (ii): b = 1 Now consider the case where b = 1. Set TC(k) = k + 1. Again, using the integration rule,
and that (cid:80)k

j=k−N (j + 1)−1 ≤ log(k + 1) − log(k − N ), we substitute into (54) which yields

k
(cid:88)

j=k−N

E[(cid:107)∇J(θj)(cid:107)2] ≤ C4ka +

Lσ2
1 − a

(cid:0)k1−a − (k − N − 1)1−a(cid:1) + CL1 (log(k + 1) − log(k − N )) .

(61)

Divide both sides by k and ﬁx N = k − 1

1
k

k
(cid:88)

j=1

E[(cid:107)∇J(θj)(cid:107)2] ≤ C4ka−1 +

Lσ2
1 − a

k−a + CL1

log(k + 1)
k

.

Let k = K(cid:15) in the preceding expression, which then becomes
K(cid:15)(cid:88)

(cid:18)

E[(cid:107)∇J(θj)(cid:107)2] ≤ O

K a−1

(cid:15) + K −a

(cid:15) +

1
K(cid:15)

j=1

log(K(cid:15) + 1)
K(cid:15)

(cid:19)

.

Again, by deﬁnition of K(cid:15) [c.f. (44)], we have that E[(cid:107)∇J(θj)(cid:107)2] > (cid:15) for all j = 1, . . . , K(cid:15), so

(cid:15) ≤

1
K(cid:15)

K(cid:15)(cid:88)

j=1

E[(cid:107)∇J(θj)(cid:107)2] ≤ O

(cid:18)

K a−1

(cid:15) + K −a

(cid:15) +

log(K(cid:15) + 1)
K(cid:15)

(cid:19)

.

Optimizing over a, we have

On the other hand,

Fix (cid:96) = min{1/2, b}, then

which implies

This concludes the proof.

(cid:16)

(cid:15) ≤ O

K − 1

2

(cid:15)

(cid:17)

for b >

1
2

(cid:15) ≤ O (cid:0)K −b

(cid:15)

(cid:1) . for b ≤ 1/2

(cid:15) ≤ O(K −(cid:96)

(cid:15) ),

K(cid:15) ≤ O((cid:15)−1/(cid:96)).

(62)

(63)

(64)

(65)

(66)

(67)

(68)

triangular number, that is (cid:0)K(cid:15)+1

The analysis of Lemma 2 and Theorem 1 do not make any assumptions on the size of the state action
space. Additionally, the result describes the number of actor updates required. The number of critic updates
(cid:1). These results connect actor-critic algorithms
required is simply the K th
(cid:15)
with the behavior of stochastic gradient method for ﬁnding the root of a non-convex objective. Under
additional conditions, actor-critic with TD updates for the critic step attains a O((cid:15)−2) rate. However, under
milder conditions on the state and action spaces but more stringent smoothness conditions on the reward
function, using GTD updates for the critic yields O((cid:15)−3) rates. These results are formally derived in the
following subsections.

2

12

Remark 1. The convergence rate derived by Theorem 1 makes the assumption that the rate of convergence
for the critic only method is bounded by k−b for some b ∈ (0, 1). The resulting bound is characterized by the
minimum of of the actor stepsize (k−1/2) and the critic convergence rate. This means for critic only methods
which converge faster than k−1/2, such as k−1 or log k/k, the bound will be completely characterized by actor
bottleneck k−1/2. The proof holds identically except that the critic in the kth iteration will need to run k + 1
steps.

5 Rates of Gradient and Accelerated Actor-Critic

In this section, we show how Algorithm 1 can be applied to derive the rate of actor-critic methods using
Gradient Temporal Diﬀerence (GTD) as the critic update. Thus, we proceed with deriving GTD-style
updates through links to compositional stochastic programming [49] which is also the perspective we adopted
to derive rates in the previous section. Begin by recalling that any critic method seeks a ﬁxed point of the
Bellman evaluation operator:

(T πθ Q)(s, a) (cid:44) r(s, a) + γEs(cid:48)∈S [Q(s(cid:48), a(cid:48)) | s, a]

(69)

Since we focus on parameterizations of the Q function by parameter vectors ξ ∈ Rd with some ﬁxed feature
map ϕ which is learned a priori, the Bellman operator simpliﬁes

T πθ Qξ(s, a) = Es(cid:48),a(cid:48)∼πθ(s(cid:48))[r(s, a) + γξ(cid:62)ϕ(s(cid:48), a(cid:48))|s, a]

(70)

The solution of the Bellman equation is its ﬁxed point: T πQ(s, a) = Q(s, a) for all s ∈ S, a ∈ A. Thus, we
seek Q functions that minimize the (projected) Bellman error

min
ξ∈Ξ

(cid:107)ΠT πθ Qπθ

ξ − Qπθ

ξ (cid:107)2

µ =: F (ξ).

(71)

where Ξ ⊆ Rp is a closed and convex feasible set. The Bellman error quantiﬁes distance from the ﬁxed point
for a given Qξ. Here the projection and µ-norm are respectively deﬁned as

Π ˆQ = arg min
f ∈F

(cid:107) ˆQ − f (cid:107)µ ,

(cid:90)

(cid:107)Q(cid:107)2

µ =

Q2(s, a)µ(ds, da),

(72)

This parameterization of Q implies that we restrict the feasible set – which is in general B(S, A), the space
of bounded continuous functions whose domain is S × A – to be F = {Qξ : ξ ∈ Ξ ⊂ Rd} (as in [28]). Without
this parameterization, one would require searching over B(S, A), whose complexity scales with the dimension
of the state and action spaces [4], which is costly when dimensions are large, and downright impossible for
continuous spaces [36].

Under certain mild conditions drawing tools from functional analysis, we can deﬁne a projection over
a class of functions such that Π ˆQ = ˆQ. For example, Radial-Basis-Function (RBF) networks have been
shown to be capable of approximating arbitrarily well functions in Lp(Rr) [33, Theorem 1]. Further, neural
networks with one hidden layer and sigmoidal activation functions are known to approximate arbitrarily well
continuous functions on the unit cube [16, Theorem 1].

By the deﬁnition of the µ norm, we can write F [cf. (71)] as an expectation

As such, we replace the Bellman operator in (73) with (70) to obtain

F (ξ) = E[(T πθ Qξ − Qξ)2].

F (ξ) = Es,a∼πθ(s){(Es(cid:48),a(cid:48)∼πθ(s(cid:48))[r(s, a) + γξ(cid:62)ϕ(s(cid:48), a(cid:48))|s, a ∼ πθ(s)] − ξ(cid:62)ϕ(s, a))2}.

(73)

(74)

Pulling the last term into the inner expectation, F (ξ) can be written as the function composition F (ξ) =
(f ◦ g)(x) = f (g(x)), where f : R → R and g : Rp → R take the form of expected values

f (y) = E(s,a)[f(s,a)(y)] ,

g(ξ) = E(s(cid:48),a(cid:48))[g(s(cid:48),a(cid:48))(ξ) | s, a ∼ πθ(s)],

(75)

13

where

f(s,a)(y) = y2 ,

g(s(cid:48),a(cid:48))(ξ) = r(s, a) + γξ(cid:62)ϕ(s(cid:48), a(cid:48)) − ξ(cid:62)ϕ(s, a).

(76)

Because F (ξ) can be written as a nested expectations of convex functions, we can use Stochastic Compo-
sitional Gradient Descent (SCGD) for the critic update [49]. This requires the computation of the sample
gradients for both f and g in (75)

∇f(s,a)(y) = 2y ,

∇g(s(cid:48),a(cid:48))(ξ) = γϕ(s(cid:48), a(cid:48)) − ϕ(s, a).

(77)

The speciﬁcation of SCGD to the Bellman evaluation error (74) yields the GTD updates (19) deﬁned in
Section 3 – see [45] for further details. When GTD updates (19) are substituted into the Critic(k) step in
Algorithm 1, we obtain Algorithm 2. We now turn to establishing the convergence rate in expectation for
Algorithm 2 using Theorem 1. Doing so requires the conditions of [49][Theorem 3] to be satisﬁed, which we
subsequently state.

Assumption 6.

(i) The outer function f is continuously diﬀerentiable, the inner function g is continuous, the critic pa-
rameter feasible set Ξ is closed and convex, and there exists at least one optimal solution to problem
(71), namely ξ∗ ∈ Ξ

(ii) The sample ﬁrst order information is unbiased. That is,

E[g(s(cid:48)

0,a(cid:48)

0)(ξ) | s0, a0 ∼ πθ(s0)] = g(ξ)

(iii) The function E[g(ξ)] [cf. (76)] is Cg-Lipshitz continuous and the samples g(ξ) and ∇g(ξ) have bounded

second moments

E[(cid:107)∇g(s(cid:48)

0)(ξ)(cid:107)2 | s0, a0 ∼ πθ(s0)] ≤ Cg,

0,a(cid:48)

E[(cid:107)g(s(cid:48)

0)(ξ) − g(ξ)(cid:107)2] ≤ Vg

0,a(cid:48)

(iv) The f(s,a)(y) has a Lipschitz continuous gradient such that

E[(cid:107)∇f(s0,a0)(y)(cid:107)2] ≤ Cf

(cid:107)∇f(s0,a0)(y) − f(s0,a0)(¯y)(cid:107) ≤ Lf (cid:107)y − ¯y(cid:107)

for all y, ¯y ∈ R

(v) The projected Bellman error is strongly convex with respect to the critic parameter ξ in the sense that

there exists a λ such that

F (ξ) ≥ λ(cid:107)ξ − ξ∗(cid:107)2 ∀ξ ∈ Ξ

The ﬁrst part of Assumption 6(i) is trivially satisﬁed by the forms of f and g in (76). Assumption 6(ii)
requires that the state-action pairs used to update the critic parameter to be independently and identically
distributed (i.i.d.), which is a common assumption unless one focuses on performance along a single trajectory.
Doing so requires tools from dynamical systems under appropriate mixing conditions on the Markov transition
density [10, 1], which we obviate here for simplicity and to clarify insights. We note that the sample
complexity of policy evaluation along a trajectory has been established in [6], but remains open for policy
learning in continuous spaces. Moreover, i.i.d. sampling yields unbiasedness of certain gradient estimators
and second-moment boundedness which are typical for stochastic optimization [11]. We note that these
conditions come directly from [49] – here we translate them to the reinforcement learning context.

We further require F (ξ) to be strongly convex, so that [49][Theorem 3 and Theorem 7] hold. These
results establish the rate at which the critic parameter converges to the minimizer of (71) in expectation.
Substituting our full expression for F (ξ), the assumption requires

Es,a∼πθ(s)

(cid:104)(cid:0)Es(cid:48),a(cid:48)∼πθ(s(cid:48))[r(s, a) + ξ(cid:62)(γϕ(s(cid:48), a(cid:48)) − ϕ(s, a)) | s, a](cid:1)2(cid:105)

≥ λ(cid:107)ξ − ξ∗(cid:107)2,

(78)

14

Algorithm 2 Gradient Actor-Critic
Require:

s0 ∈ Rn, θ0, ξ0, stepsizes {αt} ⊂ R+, {βt} ⊂ (0, 1] which satisfy αt−1
βt

→ 0, {ηk}

Initialize z0 ← 0
for t = 0, . . . , TC(k) − 1 do

1: for k = 1, . . . do
2:
3:
4:
5:
6:

Sample st from the ergodic distribution
Draw action at ∼ πθk
Observe next state s(cid:48)
Observe reward rt
δt = rt + ξ(cid:62)
t (γϕ(s(cid:48)
zt+1 = (1 − βt)zt + βtδt
Update Critic:

t ∼ P(st, at, s(cid:48)
t)

t) − ϕ(st, at))

t, a(cid:48)

7:
8:
9:
10:

11:
12:
13:

ξt+1 = ξt − 2αtzt+1[γϕ(s(cid:48)

t, a(cid:48)

t) − ϕ(st, at)]

Draw Tk ∼ Geom(1 − γ)
(sTk , aTk ) ← rollout of πθk with length Tk
θk+1 ← θk + 1

1−γ ηkξ(cid:62)

TC (k)ϕ(sTk , aTk )∇ log πθk (sTk , aTk |θk)

for all ξ ∈ Ξ. This can be achieved if the reward function is oﬀset by a positive constant greater than the
maximum squared norm diﬀerence between ξ and ξ∗, and that the feasible set Ξ should be bounded. We
can now combine [49][Theorem 3] with Theorem 1 to establish the rate of actor-critic with GTD updates for
the critic, through connecting GTD and SCGD. We summarize the resulting method as Algorithm 2, which
we call Gradient Actor-Critic.
Corollary 1. Consider the actor parameter sequence deﬁned by Algorithm 2. If the stepsize ηk = k−1/2 and
the critic stepsizes are αt = 1/tσ and βt = 1/t2/3, then we have the following bound on K(cid:15) deﬁned in (44):
K(cid:15) ≤ O (cid:0)(cid:15)−3(cid:1) .

(79)

Proof. Here we invoke [49, Theorem 3] which characterizes the rate of convergence for the critic parameter

E[(cid:107)ξk − ξ∗(cid:107)2] ≤ O

(cid:16)

k−2/3(cid:17)

.

Applying Jensen’s inequality, we have

E[(cid:107)ξk − ξ∗(cid:107)]2 ≤ E[(cid:107)ξk − ξ∗(cid:107)2] ≤ O

(cid:16)
k−2/3(cid:17)

,

(80)

(81)

Taking the square root gives us

(82)
Therefore, b = −1/3 (c.f. Assumption 5) in Theorem 1, which determines the O (cid:0)(cid:15)−3(cid:1) rate on K(cid:15) in the
preceding expression.

E[(cid:107)ξk − ξ∗(cid:107)] ≤ O

.

(cid:16)

k−1/3(cid:17)

Corollary 1 establishes that actor-critic method with GTD updates for the policy evaluation step converge
to stationarity at a O((cid:15)−3) rate which is slower than the rate O((cid:15)−2) for TD(0) updates (18) in Corollary 4
for ﬁnite countable spaces, but faster than the O((cid:15)−4) rate for TD(0) updates for continuous spaces Corollary
3, as established in Section 6. Of course, this faster rate comes at the cost of additional regularity conditions
imposed by Assumption 6.

Unsurprisingly, with additional smoothness assumptions,

it is possible to obtain faster convergence
through accelerated variants of GTD. The corresponding actor-critic method with Accelerated GTD up-
dates (20) is given in Algorithm 3, which we call Accelerated Actor-Critic. The validity of accelerated rates,
aside from Assumption 6, requires imposing that the inner expectation has Lipschitz gradients and that
sample gradients have boundedness properties which are formally stated below.

15

s0 ∈ Rn, θ0, ξ0, stepsizes {αt} ⊂ R+, {βt} ⊂ (0, 1] which satisfy αt−1
βt

→ 0, {ηk}

Algorithm 3 Accelerated Actor-Critic
Require:

Initialize y0 ← 0
for t = 0, . . . , TC(k) − 1 do

1: for k = 1, . . . do
2:
3:
4:
5:
6:

Sample st from the ergodic distribution
Draw action at ∼ πθk
Observe next state s(cid:48)
Observe reward rt
Update Critic:

t ∼ P(st, at, s(cid:48)
t)

7:
8:

9:

10:
11:

12:

ξt+1 = ξt − 2αt(γϕ(s(cid:48), a(cid:48)) − ϕ(s, a))yt

Update auxiliary Critic parameters yt and zt

zt+1 = −

(cid:18) 1
βt

(cid:19)

− 1

ξt +

1
βt

ξt+1

yt+1 = (1 − βt)yt + βt(r(s, a) + z(cid:62)

t+1 (γϕ(s(cid:48), a(cid:48)) − ϕ(s, a))

Draw Tk ∼ Geom(1 − γ)
(sTk , aTk ) ← rollout of πθk with length Tk
θk+1 ← θk + 1

1−γ ηkξ(cid:62)

TC (k)ϕ(sTk , aTk )∇ log πθk (sTk , aTk |θk)

Assumption 7.

(i) There exists a constant scalar Lg > 0 such that

(cid:107)∇Es(cid:48),a(cid:48)∼πθ(s(cid:48))[g(ξ1)] − ∇Es(cid:48),a(cid:48)∼πθ(s(cid:48))[g(ξ2)](cid:107) ≤ Lg(cid:107)ξ1 − ξ2(cid:107), ∀ξ1, ξ2 ∈ Ξ

(ii) The sample gradients satisfy with probability 1 that

E (cid:2)(cid:107)∇g(ξ)(cid:107)4 | s0, a0

(cid:3) ≤ C 2

g , ∀ξ ∈ Ξ ,

E (cid:2)(cid:107)∇f (y)(cid:107)4(cid:3) ≤ C 2

f , ∀y ∈ Rd

With this additional smoothness assumption, sample complexity is reduced, as we state in the following

corollary.

Corollary 2. Consider the actor parameter sequence deﬁned by Algorithm 3. If the stepsize ηk = k−1/2 and
the critic stepsizes are αt = 1/tσ and βt = 1/t4/5, then we have the following bound on K(cid:15) deﬁned in (44):

K(cid:15) ≤ O

(cid:16)

(cid:15)−5/2(cid:17)

.

(83)

Proof. The proof is identical to the proof of Corollary 1 while invoking Theorem 7 from [49].

Corollary 2 establishes a O((cid:15)−5/2) sample complexity of actor-critic when accelerated GTD steps are
used for the critic update. This is the lowest complexity/fastest rate relative to all others analyzed in this
work for continuous spaces. However, this fast rate requires the most stringent smoothness conditions. In
the following section, we shift to the case where the critic is updated using vanilla TD(0) updates (18), which
is the original form of actor-critic proposed in [25].

16

Algorithm 4 Classic Actor-Critic
Require:

s0 ∈ Rn, θ0, ξ0, stepsizes {αt}, {ηk}

Initialize z0 ← 0
for t = 0, . . . , TC(k) − 1 do

1: for k = 1, . . . do
2:
3:
4:
5:
6:

Sample st from the ergodic distribution
Draw action at ∼ πθk
Observe next state s(cid:48)
Observe reward rt and compute temporal diﬀerence δt = rt + γξ(cid:62)
Update Critic [cf. (18)] (Q function estimate):

t ∼ P(st, at, s(cid:48)
t)

t (ϕ(s(cid:48)

t, a(cid:48)

t) − ϕ(st, at))

7:
8:

9:
10:
11:

ξt+1 = ξt + αtδtϕ(st, at)

Draw Tk ∼ Geom(1 − γ)
(sTk , aTk ) ← rollout of πθk with length Tk
θk+1 ← θk + 1

1−γ ηkξ(cid:62)

TC (k)ϕ(sTk , aTk )∇ log πθk (sTk , aTk |θk)

6 Sample Complexity of Classic Actor-Critic

In this section, we derive convergence rates for actor-critic when the critic is updated using TD(0) as in (18)
for two diﬀerent canonical settings: the case where the state space action is continuous (Sec. 6.1) and when
it is ﬁnite (Sec. 6.2). Both use TD(0) with linear function approximation in its unaltered form [40]. We
substitute (18) in for the Critic(k) step in Algorithm 1 to deﬁne Algorithm 4, which is the classical form of
actor-critic given in [25, 24], thus the name Classic Actor-Critic.

6.1 Continuous State and Action Spaces

The analysis for Continuous State Action space TD(0) with linear function approximation uses the analysis
from [17] to characterize the rate of convergence for the critic. Their analysis requires the following common
assumption.

Assumption 8. There exists a constant Ks > 0 such that for the ﬁltration Gt deﬁned for the TD(0) critic
updates, we have

where Mt+1 is deﬁned as

E[(cid:107)Mt+1(cid:107)2|Gt] ≤ Ks[1 + (cid:107)ξt − ξ∗(cid:107)2],

Mt+1 = (cid:0)rt + γξ(cid:62)

t ϕ(st+1, at+1) − ξ(cid:62)

t ϕ(st, at)(cid:1) ϕ(st, at) − b + A

where

b := Es,a∼π(s)[r(s, a)ϕ(s, a)], and A := Es,a∼π(s)[ϕ(s, a)(ϕ(s, a) − γϕ(s(cid:48), a(cid:48)))(cid:62)]

(84)

(85)

(86)

Assumption 8 is known to hold when the samples have uniformly bounded second moments, which is a

common assumption for convergence results[43, 42].

Corollary 3. Consider the actor parameter sequence deﬁned by Algorithm 4. Suppose the actor step-size is
chosen as ηk = k−1/2 and the critic step-size takes the form αt = 1/(t + 1)σ where σ ∈ (0, 1/2). Then, for
large enough k,

(87)

K(cid:15) ≤ O

(cid:16)

(cid:15)−2/σ(cid:17)

17

Proof. Here we invoke the TD(0) convergence result from [17, Theorem 3.1] which establishes that

E[(cid:107)ξt − ξ∗(cid:107)2] ≤ K1e−K2t1−σ

+

K3
tσ

(88)

for some constants which depend on σ ∈ (0, 1). When σ ≈ 0 is close to zero, the ﬁrst term dominates, which
permits us to write that

Applying Jensen’s inequality, we have

E[(cid:107)ξt − ξ∗(cid:107)2] ≤ O

(cid:19)

(cid:18) 1
tσ

E[(cid:107)ξt − ξ∗(cid:107)]2 ≤ E[(cid:107)ξt − ξ∗(cid:107)2] ≤ O

(cid:19)

.

(cid:18) 1
tσ

Taking the square root on both sides gives us

E[(cid:107)ξt − ξ∗(cid:107)] ≤ O

(cid:18) 1
tσ/2

(cid:19)

,

(89)

(90)

(91)

which means that the convergence rate statement of Assumption 5 is satisﬁed with parameter b = σ/2.
Because σ < 1/2, this specializes Theorem 1, speciﬁcally, (46) to case (i), which yields the rate

K(cid:15) ≤ O

(cid:16)

(cid:15)−2/σ(cid:17)

.

(92)

Thus the claim in Corollary 3 is valid.

The fastest convergence rate is attained for σ = 1/2, which is O (cid:0)(cid:15)−4(cid:1). Compared to GTD [cf. (19)], this
rate is slower. However, we underscore that the required conditions for Corollary 3 that uses TD updates
[cf. (18)] are looser. Speciﬁcally, the GTD rates given in Corollary 1 hinge upon strong convexity of the
projected Bellman error, which may hold for carefully chosen state-action feature maps, bounded parameter
spaces, and lower bounds on the reward. These conditions are absent for TD(0) critic updates.

We also note that Corollary 3 holds for k large enough, which is an artifact of the exponential term in [17,
Theorem 3.1]. Speciﬁcally, a remark states that the the exponential term converges faster when σ is close to
zero, but no additional guidance on optimal choice of σ is available. This limitation carries through to the
rate characterization in Corollary 3. In the next section, we will consider analysis of actor-critic with TD(0)
critic updates in the case where the state and action spaces are ﬁnite. As would be expected, this added
assumption signiﬁcantly improves the bound on the rate of convergence, i.e., reduces the sample complexity
needed for policy parameters that are within (cid:15) of stationary points of the value function.

6.2 Finite State and Action Spaces

In this section, we characterize the rate of convergence for the actor-critic deﬁned by Algorithm 4 when
the number of states and actions are ﬁnite, i.e., |S| = S < ∞ and |A| = A < ∞. This setting yields
faster convergence. A key quantity in the analysis of TD(0) in ﬁnite spaces is the minimal eigenvalue of the
covariance of the feature map φ(s, a) weighted by policy π(s), which is deﬁned as

ω = min








eig



(cid:88)

π(s)ϕ(s, a)ϕ(s, a)(cid:62)

(s,a)∈S×A










.

(93)

That ω exists is an artifact from the ﬁnite state action space assumption. (93) is used to deﬁne conditions
on the rate of step-size attenuation for TD(0) [cf. (18)] critic updates in [6, Theorem 2 (c)], which we invoke
to establish the iteration complexity of actor-critic in ﬁnite spaces. We do so next.

18

(94)

(95)

(96)

(97)

(98)

Corollary 4. Consider the actor parameter sequence deﬁned by Algorithm 4. Let the actor step-size satisfy
ηk = k−1/2 and the critic step-size decrease as αt = β/λ + t where β = 2/ω(1 − γ) and λ = 16/ω(1 − γ)2.
Then when the number of critic updates per actor update satisﬁes TC(k) = k + 1, the following convergence
rate holds

K(cid:15) ≤ O (cid:0)(cid:15)−2(cid:1)

Proof. We begin by invoking the TD(0) convergence result [6, Theorem 2 (c)]:

E[(cid:107)ξt − ξ∗(cid:107)2] ≤ O

(cid:18) K1

t + K2

(cid:19)

,

for some constants K1, K2 which depend on ω and σ. Applying Jensen’s inequality, we have

E[(cid:107)ξt − ξ∗(cid:107)]2 ≤ E[(cid:107)ξt − ξ∗(cid:107)2] ≤ O

(cid:18) K1

t + K2

(cid:19)

.

Taking the square root on both sides yields

E[(cid:107)ξt − ξ∗(cid:107)] ≤ O

(cid:32)

K −1/2
1
(t + K2)−1/2

(cid:33)

(cid:46) O(t−1/2),

which means that Assumption 5 is valid with critic convergence rate parameter b = 1/2. Therefore, we may
apply= Theorem 1 to obtain the rate

K(cid:15) ≤ O (cid:0)(cid:15)−2(cid:1)

as stated in Corollary 4.

7 Numerical Results

In this section, we compare the convergence rates of actor critic with diﬀerent critic only methods for a
navigation problem. We ﬁrst describe the RL problem we aim to solve, then we describe some practical
modiﬁcations of the algorithms and the evaluation metrics. Finally we conclude with a discussion.

7.1 Navigating around an obstacle

We consider the problem of a point agent starting at an initial state s0 ∈ R2 whose objective is to navigate
to a destination s∗ ∈ R2 while remaining in the free space at all time. The free space F ⊂ R2 is deﬁned by

The feature representation of the state is determined by a radial basis (Gaussian) kernel where

F :=

s ∈ R2(cid:12)
(cid:110)
(cid:12)
(cid:12)(cid:107)s(cid:107) ∈ [0.5, 4]

(cid:111)

.

κ(s, s(cid:48)) = exp

(cid:26) −(cid:107)s − s(cid:48)(cid:107)2
2
2σ2

(cid:27)

.

(99)

(100)

The p kernel points are chosen evenly on the [−5, 5] × [−5, 5] grid (see Figure 7.2 (a)) so that the the feature
representation becomes

ϕ(s) = (cid:2)κ(s, s1) κ(s, s2)

. . .

κ(s, sp)(cid:3)(cid:62)

,

(101)

which we scale to have norm equal to one. Given the state st, the action is sampled from a multivariate
Gaussian distribution with covariance matrix Σ = 0.5 · I2 and mean given by θ(cid:62)
k ϕ(st). We let the action
determine the direction in which the agent will move. As such, the state transition is determined by st =
st + 0.5at/(cid:107)at(cid:107). In this formulation, the norm of the actor parameter serves as an indicator as per how

19

(a)

(b)

Figure 1: Convergence rate of actor-critic with diﬀerent critic only methods. (a) Shows the estimate of
the gradient norm with respect to the number of policy gradient updates. (b) Shows the average reward.
Interestingly, the rate at which the algorithm converges relates to the quality of the stationary point achieved.

conﬁdent the agent is about taking a particular action. The larger the norm, the smaller the diﬀerence
between the direction of the mean and the sampled action.

Because the agent’s objective is to reach the target s∗ while remaining in F for all time, we want to
penalize the agent heavily for taking actions which result in the next step being outside the free space and
reward the function for being close to the target. As such, we deﬁne the reward function to be

rt+1 =






−11
−0.1
−1

if st+1 /∈ F
if (cid:107)st+1 − s∗(cid:107) < 0.5
otherwise .

(102)

The design of this reward function for the navigation problem is informed by the [52], which suggests that
the reward function should be bounded away from zero. In fact, to satify Assumption 7, the reward should
be oﬀset by a larger value. In this simulation, we allow for the agent to continue taking actions through the
obstacles. This formulation is similar to a car driving on a race track which has grass outside the track. The
car is allow is allowed to drive on the track, however it incurs a larger cost due to the substandard driving
conditions.

This particular formulation does not allow for generalization, so if the target of the agent, obstacle
location, or starting point of the agent was moved, the agent would have to start from scratch to learn a
new meaningful policy. We acknowledge that this is not ideal, however we emphasize that it is the rates
of convergence which are of interest in this exposition, not necessarily ﬁnding the best way to design the
navigation problem.

7.2 Algorithm Speciﬁcs

Due to limitations on computational time and computing capacity, we chose to make some minor modiﬁ-
cations to Algorithm 1. To begin, we do not sample the state action pair from the stationary distribution,
but rather run a rollout of length 200, and use the entire trajectory to update the critic. After running ten
critic rollout updates, we update the actor by similarly updating along the trajectory of rollout length 200.

20

020406080100120140160180200Policy Gradient Updates-0.200.20.40.60.811.21.41.6 Approx of Norm GradientTD(0)GTDA-GTD020406080100120140160180200Policy Gradient Updates-600-500-400-300-200-1000Average RewardTD(0)GTDA-GTDsolved(a)

(b)

Figure 2:
from learned policy after convergence.

(a)Visualization of the of state action space radial basis function grid. (b) Example trajectories

Again, we emphasize that the choice to deviate from our proposed algorithm is due to computational time
and computing capacity.

For all critic only methods, the actor update step ηt is chosen to be constant η = 10−4. For TD(0), we
let also let the critic stepsize be constant, namely αt = α = 0.05. For GTD, we let αt = t−1 and βt = t−2/3.
For Accelerated GTD, we set αt = t−1 and βt = t−4/5. Gamma is chosen to be γ = 0.9. We set the initial
position to be s0 = (2, 2) and the target to be s∗ = (−2, −2).

For each critic only method, we run the algorithm 100 times. We evaluate the policy every tenth actor
update by measuring the average accumulated reward ten trajectories of length 200. We measure convergence
by an approximation of the gradient norm. By (12), we know that

ξ(cid:62)
k ϕ(sT , aT )∇ log πθ(aT |sT ),

(103)

is a scaled, biased estimate of the gradient norm ˜∇J(θ) where the bias comes from the critic only method and
the scaling comes from the 1/(1−γ) factor. We project the norm of the critic parameter exceeding 20 to have
norm equal to 20. This, combined with the fact that the feature representation has norm equal to one allows
us to characterize convergence of the norm by looking exclusively at the score function ∇ log πθ(aT |sT ).
Recall that we are using a multivariate policy to determine the action. Our score function can be written as

∇ log πθ(aT |sT ) =

1
σ2 [aT − θ(cid:62)

k ϕ(st)]ϕ(sT ).

(104)

Again, because (cid:107)ϕ(sT )(cid:107) = 1, we can approximate the norm of the gradient by measuring the norm of the
diﬀerence in direction of the action at and the mean θ(cid:62)
k ϕ(sT ). Similar to the average reward, we take the
average norm over ten trajectories. Recall that the norm of the agent parameter serves as an indication
on how conﬁdent the agent is with its actions. The large the norm, the smaller the diﬀerence between the
agent’s action and the mean of the Gaussian distribution from which it was sampled. Therefore, when the
norm of the agent exceeds 100, the agent is no longer updated, but the evaluation continues. The plots
showing convergence on these two metrics are shown in Figures 7.2 (a) and (b). Example trajectories of the
agent after learning are shown in Figure 7.2 (b). A detailed discussion on the rates and their implications
are discussed in the following subsection.

21

-505-5-4-3-2-1012345-4-2024-5-4-3-2-101234TDGTDA-GTD7.3 Discussion

Recall that the analysis of Corollaries 1, 2, and 3 establish that the convergence rates for GTD, A-GTD,
and TD(0) are O((cid:15)−3), O((cid:15)−5/2), and O((cid:15)−4) respectively [also see Table 1]. This would suggest that TD(0)
would converge the slower than GTD, and GTD would converge slower than A-GTD. Figure 7.2 (a) shows
that while A-GTD does indeed converge faster than GTD, the rate of convergence for TD(0) does not act as
expected and therefore warrants a closer inspection. TD(0) not only converges faster than GTD, but it seems
to have a steeper slope than even A-GTD. One possible explanation for this comes from the fact that RBF
grid-type parametrization of the state action space might make the convergence behavior of actor-critic with
TD(0) updates act closer to the ﬁnite state action case. The rate ascertained by corollary 4 is O((cid:15)−2), which
is indeed the fastest rate obtained by actor-critic. This suggests that with additional assumptions on the
feature space representation, the convergence rate of TD(0) for linear function approximation in continuous
state action space might converge at the same rate derived its ﬁnite state action space counterpart.

Theorem 1 characterizes the rate of convergence to a stationary point of the Bellman optimality operator,
however it does not provide any guarantee on the quality of the stationary point. Figure 7.2 (b) captures a
trade-of convergence rate and quality of the stationary point. We say that rewards which are greater than
−180 are solved trajectories because these trajectory spend time in the destination region. A trajectory
which does not reach the destination region will have accumulated reward of −200 or less.

A-GTD which converges the fastest has the worst policy improvement, seeming to converge to a policy
which on average does not seem to be better than even the initial random policy. On the other hand,
GTD converges the slowest, and it consistently reaches the solved region. Unsurprisingly, TD(0) converges
slower than A-GTD, but faster than GTD, and its stationary point has a reward between the two. Taken
together, these theoretical and experimental results suggest a tight coupling between the choice of training
methodology and the quality of learned policies. Thus, just as the choice of optimization method, statistical
model, and sample size inﬂuence generalization in supervised learning, they do so in reinforcement learning.
There are a number of future directions to take this work. To begin, we can establish bounds on cases
where the samples are not i.i.d., but instead have Markovian noise. Second, we can characterize the behavior
of the variance and use such characterizations to accelerate training.

References

[1] Andr´as Antos, Csaba Szepesv´ari, and R´emi Munos. Fitted q-iteration in continuous action-space mdps.

In Advances in neural information processing systems, pages 9–16, 2008.

[2] John Asmuth and Michael L Littman. Approaching bayes-optimalilty using monte-carlo tree search. In

Proc. 21st Int. Conf. Automat. Plan. Sched., Freiburg, Germany, 2011.

[3] Richard Bellman. The theory of dynamic programming. Technical report, RAND Corp Santa Monica

CA, 1954.

[4] Richard Ernest Bellman. Dynamic Programming. Courier Dover Publications, 1957.

[5] Dimitri P Bertsekas. Dynamic Programming and Optimal Control, volume 1. 2005.

[6] Jalaj Bhandari, Daniel Russo, and Raghav Singal. A ﬁnite time analysis of temporal diﬀerence learning

with linear function approximation. arXiv preprint arXiv:1806.02450, 2018.

[7] Shalabh Bhatnagar, Mohammad Ghavamzadeh, Mark Lee, and Richard S Sutton. Incremental natural
actor-critic algorithms. In Advances in Neural Information Processing Systems, pages 105–112, 2008.

[8] Shalabh Bhatnagar, Richard Sutton, Mohammad Ghavamzadeh, and Mark Lee. Natural actor-critic

algorithms. Automatica, 45(11):2471–2482, 2009.

[9] Vivek S Borkar. Stochastic approximation with two time scales. Systems & Control Letters, 29(5):291–

294, 1997.

22

[10] Vivek S Borkar. Stochastic approximation: a dynamical systems viewpoint, volume 48. Springer, 2009.

[11] L´eon Bottou. Online learning and stochastic approximations. On-line learning in neural networks,

17(9):142.

[12] Olivier Bousquet and Andr´e Elisseeﬀ. Stability and generalization. Journal of machine learning research,

2(Mar):499–526, 2002.

[13] Justin A Boyan and Andrew W Moore. Generalization in reinforcement learning: Safely approximating

the value function. In Advances in neural information processing systems, pages 369–376, 1995.

[14] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, Schulman John, Tang Jie, and

Zaremba Wojciech. Openai gym. arXiv preprint arXiv:1606.01540, 2016.

[15] Dotan Di Castro and Ron Meir. A convergent online single-time-scale actor-critic algorithm. Journal

of Machine Learning Research, 11(Jan):367–410, 2010.

[16] George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control,

signals and systems, 2(4):303–314, 1989.

[17] Gal Dalal, Balazs Szorenyi, Gugan Thoppe, and Shie Mannor. Finite sample analysis of two-timescale
stochastic approximation with applications to reinforcement learning. arXiv preprint arXiv:1703.05376,
2017.

[18] Kenji Doya. Reinforcement learning in continuous time and space. Neural Computation, 12(1):219–245,

2000.

[19] Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points–online stochastic

gradient for tensor decomposition. In Conference on Learning Theory, pages 797–842, 2015.

[20] Sylvain Gelly and David Silver. Monte-carlo tree search and rapid action value estimation in computer

go. Artiﬁcial Intelligence, 175(11):1856–1875, 2011.

[21] Ilaria Giannoccaro and Pierpaolo Pontrandolfo. Inventory management in supply chains: a reinforce-

ment learning approach. International Journal of Production Economics, 78(2):153–161, 2002.

[22] Daniel R Jiang, Thuy V Pham, Warren B Powell, Daniel F Salas, and Warren R Scott. A comparison of
approximate dynamic programming techniques on benchmark energy storage problems: Does anything
In 2014 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning
work?
(ADPRL), pages 1–8. IEEE, 2014.

[23] Jens Kober and Jan Peters. Reinforcement learning in robotics: A survey. In Reinforcement Learning,

pages 579–610. Springer, 2012.

[24] Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms.

In Advances in Neural Information

Processing Systems, pages 1008–1014, 2000.

[25] Vijaymohan R Konda and Vivek S Borkar. Actor-critic–type learning algorithms for Markov decision

processes. SIAM Journal on Control and Optimization, 38(1):94–123, 1999.

[26] Harold J. Kushner and G. George Yin. Stochastic approximation and recursive algorithms and applica-

tions. Springer, New York, NY, 2003.

[27] Xingguo Li, Zhaoran Wang, Junwei Lu, Raman Arora, Jarvis Haupt, Han Liu, and Tuo Zhao. Symmetry,

saddle points, and global geometry of nonconvex matrix factorization. arXiv preprint, 2016.

[28] Hamid R Maei, Csaba Szepesv´ari, Shalabh Bhatnagar, and Richard S Sutton. Toward oﬀ-policy learning
control with function approximation. In Proceedings of the 27th International Conference on Machine
Learning (ICML-10), pages 719–726, 2010.

23

[29] Sean P Meyn and Richard L Tweedie. Markov chains and stochastic stability. Springer Science &

Business Media, 2012.

[30] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning.
In International Conference on Machine Learning, pages 1928–1937, 2016.

[31] Yurii E Nesterov. A method for solving the convex programming problem with convergence rate o (1/kˆ

2). In Dokl. akad. nauk Sssr, volume 269, pages 543–547, 1983.

[32] Matteo Papini, Damiano Binaghi, Giuseppe Canonaco, Matteo Pirotta, and Marcello Restelli. Stochastic
variance-reduced policy gradient. In International Conference on Machine Learning, pages 4026–4035,
2018.

[33] Jooyoung Park and Irwin W Sandberg. Universal approximation using radial-basis-function networks.

Neural computation, 3(2):246–257, 1991.

[34] Santiago Paternain. Stochastic Control Foundations of Autonomous Behavior. PhD thesis, University

of Pennsylvania, 2018.

[35] Matteo Pirotta, Marcello Restelli, and Luca Bascetta. Policy gradient in Lipschitz Markov Decision

processes. Machine Learning, 100(2-3):255–283, 2015.

[36] Warren B Powell. Approximate Dynamic Programming: Solving the curses of dimensionality, volume

703. John Wiley & Sons, 2007.

[37] Martin L Puterman. Markov Decision Processes: Discrete stochastic dynamic programming. 2014.

[38] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of Go without
human knowledge. Nature, 550(7676):354–359, 2017.

[39] Ju Sun, Qing Qu, and John Wright. A geometric analysis of phase retrieval. In Information Theory

(ISIT), 2016 IEEE International Symposium on, pages 2379–2383. IEEE, 2016.

[40] Richard S Sutton. Learning to predict by the methods of temporal diﬀerences. Machine learning,

3(1):9–44, 1988.

[41] Richard S Sutton, Andrew G Barto, et al. Reinforcement Learning: An Introduction. 2 edition, 2017.

[42] Richard S Sutton, Hamid R Maei, and Csaba Szepesv´ari. A convergent o(n) temporal-diﬀerence al-
gorithm for oﬀ-policy learning with linear function approximation. In Advances in neural information
processing systems, pages 1609–1616, 2009.

[43] Richard S Sutton, Hamid Reza Maei, Doina Precup, Shalabh Bhatnagar, David Silver, Csaba Szepesv´ari,
and Eric Wiewiora. Fast gradient-descent methods for temporal-diﬀerence learning with linear function
approximation. In International Conference on Machine Learning, pages 993–1000. ACM, 2009.

[44] Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods
for reinforcement learning with function approximation. In Advances in Neural Information Processing
Systems, pages 1057–1063, 2000.

[45] Richard S Sutton, Csaba Szepesv´ari, and Hamid Reza Maei. A convergent o (n) algorithm for oﬀ-
policy temporal-diﬀerence learning with linear function approximation. Advances in neural information
processing systems, 21(21):1609–1616, 2008.

[46] Gerald Tesauro. Temporal diﬀerence learning and td-gammon. 1995.

24

[47] John N Tsitsiklis. Asynchronous stochastic approximation and q-learning. Machine learning, 16(3):185–

202, 1994.

[48] John N Tsitsiklis and Benjamin Van Roy. Analysis of temporal-diﬀference learning with function ap-

proximation. In Advances in Neural Information Processing Systems, pages 1075–1081, 1997.

[49] Mengdi Wang, Ethan X Fang, and Han Liu. Stochastic compositional gradient descent: algorithms for
minimizing compositions of expected-value functions. Mathematical Programming, 161(1-2):419–449,
2017.

[50] Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279–292, 1992.

[51] Kaiqing Zhang, Alec Koppel, Hao Zhu, and Tamer Ba¸sar. Convergence and iteration complexity of
policy gradient method for inﬁnite-horizon reinforcement learning. IEEE Conference on Decision and
Control (to appear), Dec. 2019.

[52] Kaiqing Zhang, Alec Koppel, Hao Zhu, and Tamer Ba¸sar. Global convergence of policy gradient meth-
ods: A nonconvex optimization perspective. SIAM Journal on Control and Optimization (SICON)
(submitted), Jan 2019.

[53] Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Ba¸sar. Fully decentralized multi-agent
reinforcement learning with networked agents. In International Conference on Machine Learning, pages
5872–5881, 2018.

[54] Shaofeng Zou, Tengyu Xu, and Yingbin Liang. Finite-sample analysis for sarsa and q-learning with

linear function approximation. arXiv preprint arXiv:1902.02234, 2019.

25

