2
2
0
2

b
e
F
3
2

]
E
N
.
s
c
[

2
v
9
3
0
4
0
.
2
0
2
2
:
v
i
X
r
a

Using Genetic Programming to Predict and Optimize
Protein Function

Iliya Miralavy1,2*, Alexander Bricco1,3, Assaf Gilad1,4, and Wolfgang Banzhaf1,2

1BEACON Center of Evolution in Action,
2Department of Computer Science and Engineering
3Department of Biomedical Engineering,
4Department of Chemical Engineering,
1Michigan State University, East Lansing, Michigan, United States
*Email: miralavy@msu.edu

Abstract

Protein engineers conventionally use tools such as Directed Evolution to ﬁnd new proteins
with better functionalities and traits. More recently, computational techniques and espe-
cially machine learning approaches have been recruited to assist Directed Evolution, showing
promising results. In this paper, we propose POET, a computational Genetic Programming
tool based on evolutionary computation methods to enhance screening and mutagenesis in
Directed Evolution and help protein engineers to ﬁnd proteins that have better functionality.
As a proof-of-concept we use peptides that generate MRI contrast detected by the Chemi-
cal Exchange Saturation Transfer contrast mechanism. The evolutionary methods used in
POET are described, and the performance of POET in diﬀerent epochs of our experiments
with Chemical Exchange Saturation Transfer contrast are studied. Our results indicate
that a computational modelling tool like POET can help to ﬁnd peptides with 400% better
functionality than used before.

1 Introduction

Advances in computational techniques for learning and optimization have been extremely helpful
in peptide (sequences of amino acids) design and protein engineering. Proteins are the workhorses
of life, the machinery and active components necessary for allowing biological organisms to
survive in their environment. Throughout millions of years, biological evolution has found a vast
variety of proteins. Literature suggests ∼ 20, 000 non-modiﬁed proteins have been discovered in
human body so far following the hypothesis that one gene produces one protein [1]. However,
not all of the protein search space has been explored by natural evolution yet.
In Science,
protein structure and function prediction has been an active topic for structural biology and
computer science. Protein engineers mainly use the three methods of Rational Design, Directed
Evolution (DE) and De Novo design to ﬁnd their proteins of interest. Rational Design, creates
new molecules based on extensive prior knowledge of the 3D structure of proteins and their
properties. DE does not require as much prior knowledge for protein optimization and instead,
uses random mutagenesis and screening of variants to ﬁnd ﬁtter proteins. De novo design uses

1

 
 
 
 
 
 
computational means to design algorithms that learn from the 3D protein structure and their
folding mechanisms to synthesise novel proteins [2].

Computational approaches to the design of proteins have been examined since the early
1980s [3]. In that line of work, linear sequences of amino acids representing proteins in their
basic sequential information are given to algorithms as inputs in order to create models able to
predict their secondary properties and to predict features of their variants, such as inter-residue
distances, 3D structural shapes, and protein folding. Obtaining this information is crucial to
predicting protein function and creating new variants. Diﬀerent techniques have been applied to
this problem to date, some of which formulate it as a classiﬁcation problem where a classifying
algorithms aims to ﬁnd the protein class with respect to structural properties or traits of the
peptide under consideration. Others have taken a more continuous approach and predict numer-
ical values corresponding to the characteristics of a protein structure. In that case, the applied
techniques are trained to estimate unknown numerical properties of a protein, like inter-residue
distances or hydrophobicity levels.

1.1 Protein Engineering by Directed Evolution

As we said before, proteins and peptides play an extremely important and integral part in the life
cycle of organisms. They carry out all kinds of natural functions such as forming muscle tissues,
creating enzymes and hormones, and are the building blocks of food and bio-medicine. Looking
at a living cell as a factory would make proteins the workers. These complex structures are
formed from sequences of amino acids, which code for their structural and phenotypic properties
[4]. Evolution by natural selection has produced numerous protein wild-types through millions
of years, yet has only explored a fraction of the vast protein search space. There are a total of 20
amino acids that can code for proteins. To ﬁnd a peptide consisting of 10 amino acids in a search
space of 1020 amino acid sequences is a complex undertaking. Exploring this large search space
in order to discover new and possibly better protein variants is one of the activities of protein
engineering.

As protein engineers studied proteins to understand these complex structures better and
optimize their functionalities or even develop new protein functions they came up with DE,
now a common technique for reaching these goals [5]. The DE technique starts with a pool
of proteins with similar functionalities to the desired one and imitates mutation and natural
selection in order to create the next generation of ﬁtter proteins, ultimately optimizing them.
Since DE is performed in vitro, it is a time-consuming and costly approach that requires careful
monitoring and screening of massive numbers of mutant proteins in each generation.

In recent years, many researchers have started to use computational and Machine Learning
(ML) methods to generate models that can predict the phenotypic behavior of proteins, based
on their genetic and molecular makeup [6]. In contrast to wet lab experiments, however, using
computational models enables the exploration of more of the search space in a signiﬁcantly shorter
amount of time. These computational models need to be well-trained in order to be eﬀective
which requires a lot of data. Figure 1 shows a general overview of the diﬀerence between a
conventional DE and a Machine Learning-guided Directed Evolution (ML-DE).
In DE, after
monitoring, unimproved mutants are discarded, and only improved proteins get the chance to be
selected for diversity generation. Unfortunately, this approach often causes DE to get stuck in
local optima. Meanwhile, in ML-DE, computational models choose potential mutants allowing to
cover more of the search space and thereby increasing the chance of escaping from local optima.
Even though the state of the art has not reached this point, an ideal computational model should
accurately predict any protein function in a matter of seconds greatly reducing the need for wet
lab methods such as DE; it could therefore signiﬁcantly beneﬁt the world of science and medicine.

2

1.2 Genetic Programming

Genetic Programming (GP) [7] is an extension of the Genetic Algorithm (GA) [8], an algorithm
inspired by the Darwinian conception of natural evolution [9], in which computational problem-
solving models - usually in the form of computer programs - are evolved and optimized over
a repeated generational cycle. Although all GP algorithms follow a core inspired by natural
evolution, they come in various forms and representations. Normally, the GP process starts with
a population of random individuals representing unknown solutions to a given problem. These
individuals are evaluated by a pre-deﬁned ﬁtness function to measure how well they can solve
the problem. A selection mechanism is then used to choose parent individuals that will undergo
evolutionary operators (crossover and mutation) and form the next generation of population.
Usually, ﬁtter individuals have a better chance to get selected. During crossover, parts of the
representation of the selected parent individuals are combined to create one or more oﬀspring
individuals with details depending on the algorithm. The mutation operator randomly alters
parts of the newly created oﬀspring individuals. Sometimes, an additional survival selection step
ﬁlters out only the better oﬀspring for inclusion in the population. The entire cyclical process
continues until a termination condition, such as ﬁnding a model that satisﬁes user requirements,
is met.

Figure 1: Diﬀerence between conventional DE and ML-DE. a. Conventional DE starts with
parent protein undergoing mutagenesis to produce close variants. Detailed lab screening is done
to throw out the unimproved variants and select the improved ones for the next generation of the
mutagenesis. This process continues until a desirable mutant is found. b. Commonly in ML-DE,
sequence-function models replace the rigorous screening and selection task in conventional DE.
ML-DE is able to explore more of the search space in the same amount of time and has a lower
risk to get stuck in a local optimum.

3

1.3 Protein Optimization Engineering Tool

Some of the design principles in developing protein-function-predicting models include deter-
mining (i) how much and what type of information should be given to the system, (ii) to what
extent the results should be trusted, and ﬁnally, (iii) what type of problem solver should be used.
We propose Protein Optimization Engineering Tool (POET), a GP computational tool that can
aid DE in order to ﬁnd new proteins with better functionalities. The magnetic susceptibility or
Chemical Exchange Saturation Transfer (CEST) contrast of peptides is our goal here to show a
proof-of-concept of the eﬃcacy of the method.

CEST [10] is a Magnetic Resonance Imaging (MRI) contrast approach in which peptides with
exchangeable protons or molecules are saturated and detected indirectly through enhanced water
signals after transfer. The main advantage of CEST based proteins is that they can be encoded
into DNA and expressed in live cells and tissue, thus allowing tracking non-invasively with MRI
[11, 12, 13, 14]. POET here aims to aid DE by predicting better functioning CEST contrast
proteins and replacing the rigorous and costly task of screening and mutagenesis.

Figure 2: A high-level overview of POET. POET starts with training its sequence-function
models using a curated dataset of protein sequences and their respective CEST contrast values
(Model Training). Protein Optimization evolves a random pool of protein sequences to predict
ﬁtter variants by using previously trained models for evaluation. Diﬀerent colors for protein
sequences indicate that POET can start from a random pool and is able to ﬁnd ﬁtter proteins in
a diﬀerent search space from the starting pool. Predicted variants are evaluated in wet labs, and
their measured values are added back to the protein dataset to evolve ﬁtter CEST predicting
models. The gear sign on proteins diﬀerentiates between natural and computational protein
sequences.

Figure 2 gives a high-level overview of the POET structure. POET can be divided into the
three major phases of (i) model training, (ii) protein optimization & prediction, and (iii) wet lab
experiments. During model training, POET receives an abstract dataset of amino acid sequences
in FASTA format and a trait value corresponding here to their CEST contrast. POET uses GP to
learn valuable motifs in the protein sequences, assigns weights to them, and creates collections
of rules (motifs and weights) as its models.Given a pool of protein sequences to choose from,
an optimized POET model can then select potential proteins with regard to the best expected
CEST behavior. During this protein optimization & prediction phase, a set of random protein

4

sequences of arbitrary length (here set to 12) are chosen to go through mutagenesis to ﬁnd
highly ﬁt sequences. Evaluation is performed using the best previously trained POET model.
Once enough generations of mutation and selection have been done, POET chooses the proteins
that are ﬁttest in predicted function for the third phase, wet-lab experiments. During wet-lab
experiments, the predicted sequences are chemically synthesized, and their respective CEST
contrast is measured in MRI. This concludes one round of POET which we call an epoch. The
measured values are added to the protein dataset and the POET experiment continues for another
round. The addition of newly added data is expected to improve the dataset and the potency
of POET models to predict ﬁtter sequences in the next epoch. POET omits the limitation of
the costly and time-consuming monitoring of the peptides in each generation of DE and predicts
a set of proteins that show potential based on the previous results found in its already known
dataset.

The rest of the paper is structured as follows: Section 2 discusses the related literature,
Section 3 introduces materials and methods used for developing POET. In Section 4 reports on
our empirical ﬁndings and results. Section 5 summarizes and gives a brief perspective on next
steps and possible future research directions.

2 Related Works

The scientiﬁc literature indicates how practical computational approaches can be in protein
engineering. In the late 1990s, Simulated Annealing (SA) was adopted to predict and discover
the structure of a hyperthermophile variant of a protein that could easily bind to human proteins
[15]. In 2005, Sim et al. [16] used a fuzzy K-Nearest Neighbor (KNN) algorithm to predict how
accessible protein residues are to solvent molecules. They incorporated PSI-BLAST proﬁles
as feature vectors and showed accuracy improvements in comparison to neural networks and
support vector machines. Wagner et al. [17] tackled the same problem but by employing linear
support vector regression and showed the applicability of such computationally less expensive
method in predicting protein folding and relative solvent accessibility of amino acid residues. In
2020, Xu et al. [18] compared the accuracy performance of 44 diﬀerent ML techniques on four
public and four proprietary datasets, showing that diﬀerent datasets cause dissimilar performance
levels for the same algorithms; nonetheless, most ML techniques show good promise in the area.
Notably, Xu [19] used deep learning to predict inter-residue distance distribution and folding of
protein variants with fewer homologs (protein with the same ancestry) than commonly required.
DeepMind [20] introduced AlphaFold, which applies deep residual-convolutional networks with
dilation to predict protein inter-residue distances and beneﬁted from this intermediary data to
accurately predict protein shapes with a minimum TM-Score 1 of 0.7 on 24 out of 43 free modeling
domains outperforming the previous best methods.

Many research groups have shown that evolutionary approaches can be valuable in solving
the protein structure and function prediction challenge [21].
In 1997, Khimasia [22] deﬁned
this challenge as an NP-Hard optimization problem and, aside from performance evaluation,
showed how a Simple Genetic Algorithm (SGA) can be applied to evolve simple lattice-based
structure-predicting models. Rashidi [23] examined ﬁve variants of the GA for solving a simpliﬁed
protein structure prediction and applied three methods of (i) exhaustive structure generation,
(ii) hydrophobic-core directed macro-move, and (iii) a random stagnation recovery for enhancing
each of these algorithms. Koza [24] used GP and the idea of Automatically Deﬁned Functions
(ADFs) to predict the D-E-A-D box family of proteins. UniRep [25] applied sequence-based

1TM-Score is a metric to quantify similarities between topological structures of proteins. Scores lower than

0.17 correspond to unrelated proteins, while scores higher than 0.5 generally indicate the same fold

5

deep representation learning to generate an evolutionary, semantically, and structurally rich
representation of protein properties that heuristic models could incorporate to predict structures
[26, 27] utilizedLinear Genetic Programming
and functions of unseen proteins. Seehuus et al.
(LGP) [28] for motif discovery. Their results indicated a better accuracy for discovering motifs in
diﬀerent protein families than traditional tree GP. Fathi [29] proposed a GP approach to predict
peptide sequence cleavage by HIV protease, and Langdon [30] used Grow and Graft GP (GGGP)
to predict the RNA folding of molecules. Borro et al [31] employed Bayesian classiﬁcation to
extract features from protein structure and achieves an accuracy of 45% in predicting enzyme
classes. Leijoto [32] based their work on [31] and proposed an evolutionary system combining
GA with Support Vector Machines (SVM) for the same purpose, achieving an accuracy of 71%
and outperforming the previous classiﬁcation methods.

Wu et al.

[33] proposed an ML-DE technique in which a combination of KNN, Linear Re-
gression (LR), Decision Trees, Random Forests and Multi-Layer Perceptron (MLP) (from scikit
library [34]) approaches are used to generate sequence-function models able to predict how ﬁt a
protein is concerning a speciﬁed task. The best model is applied to evaluate proteins during an
exhaustive computational mutagenesis. They evaluated their proposed method by ﬁnding ﬁtter
human GB1 binding proteins and show improvements over conventional DE methods. Linder
[35] focused on improving the lack of diversity during mutagenesis for DNA and protein synthesis
while not sacriﬁcing ﬁtness. They proposed a diﬀerentiable generative network architecture in
which deep exploration networks are incorporated to generate diverse sequences by punishing
similar sequence motifs. They employed a variational auto-encoder to ensure the generated di-
versity does not result in loss of ﬁtness. They trained their system to design ﬁtter proteins with
regards to polyadenylation, splicing, transcription, and GFP 2 ﬂuorescence. In a performance
evaluation on the same testbed, their proposed architecture designed ﬁt sequences in 140 minutes
while the classical approach of SA would need 100 days to achieve the same results. In 2019,
Repecka et al.
[36] introduced ProteinGAN, a tool that uses Generative Adversarial Networks
to learn important regulatory rules from the semantically-rich amino acid sequence space and
predicts diverse and ﬁt new proteins. ProteinGAN was experimented with to design highly cat-
alytic enzymes, and the results show that 24% of their predicted enzyme variants are soluble
and are highly catalytic even after going through more than 100 mutations. Hawkins-Hooker et
al.
[37] emphasized how the availability of data regarding protein properties could be helpful
for computational algorithms and train their variational auto-encoders on a dataset consisting of
approximately 70000 enzymes similar to luxA bacterial luciferase. They used Multiple Sequence
Alignment (MSA) and raw sequence input for their system and show that MSA better predicts
distances in the 3D protein shape. They also diverged into predicting 30 new variants not origi-
nally in their dataset. Cao [38], and Samaga [39] applied neural networks to predict the stability
of proteins upon mutations. In 2021, Das et al. [40] utilized deep generative encoders and deep
learning classiﬁers to predict antimicrobials through simulating molecular dynamics.

Evolutionary algorithms have been employed in motif extraction, function prediction, drug
discovery, and directed evolution for a long time. Yakobayashi et al.
[41] applied a GA to
replace mutagenesis and screening in DE. They used a dataset of 24 peptides with six amino
acid sequences and their respective inhibitory activity levels. In their GA, each individual shared
the same sequence as a data point of the dataset. The population of individuals went through
recombination, while the ﬁtness evaluation happened in wet labs with protein synthesis. They
showed a 36% increase on average inhibitory levels of the population after six generations of
their experiment. Archetti et al.
[42] incorporated four variants of GP (Tree-GP, Tree-GP
with Linear Scaling for ﬁtness evaluation, Tree-GP with constant input values and Tree-GP
with dynamic ﬁtness evaluation) to predict oral bioavailability, median oral lethal dose and

2Green Fluorescent Protein

6

plasma-protein binding levels of drugs of the dataset available in [43]. They used RMSE as their
prediction error evaluation metric and compared their results with classic ML techniques (LR,
Least Square Regression, SVM, and MLP), showing better performance on the GP side. Seehuus
[26] proposed ListGP, a linear-genome GP to discover important motifs from protein sequences.
They employed the PROSITE dataset introduced in [44] which represents motifs as regular
expressions and contains information about the relevance between motifs and protein domains.
ListGP showed improvements compared to Koza-style GP with Automatically Deﬁned Functions
for the task of classifying 69 protein families at 99% conﬁdence interval level. Other evolutionary
algorithms such as Immune GA [45] and Multi-Objective Genetic Algorithm [46] have also been
applied to the problem of motif discovery before. Chang et al. [47] utilized a Modiﬁed Particle
Swarm Optimization (PSO) for discovering motifs in protein sequences. They translate amino
acid symbols into numbers using a one-to-one translation table. Their results for two protein
families of EGF and C2H2 Zinc Finger showed 96.9% and 99.5% accuracy, respectively.

Availability of the relevant data is critical for training or evolving protein structure predicting
models. Uniprot [48] is a massive dataset of protein amino acid sequences and their respective
names, functions, and various structural properties containing more than 500,000 reviewed and
more than 200,000,000 unreviewed entries. Proteomes 3 in this dataset belong to Bacteria,
Viruses, Archaea and Eukaryota. Brenda [49] is a dataset containing functional enzyme and
metabolism data. This dataset consists of more than 5 million data points for approximately
90000 enzymes in 13000 organisms. Structural information of proteins, their metabolic pathways,
enzyme structures, and enzyme classiﬁcations are among the data found in this dataset.

The methods and algorithms discussed above show promising results for applying compu-
tational approaches in protein engineering. These results aid protein engineers and computer
scientists by discovering unknown protein properties and showing how the challenges in the
ﬁeld could be computationally formulated. Furthermore, some contributions evaluate the per-
formance of diﬀerent computational algorithms on the same general problem. Computer-aided
protein engineering and optimization reduce the cost and time constraints of this line of research
and enable exploring parts of the protein search landscape not easily achievable before. In this
paper, we propose Protein Optimization Engineering Tool (POET), a GP tool to aid protein en-
gineers with ﬁnding potent protein variants concerning a speciﬁed function. Speciﬁcally, POET
can be compared to algorithms used for ML-DE. In the following subsection, we discuss directed
evolution and ML-DE in detail.

3 Materials and Methods

We employ GP as the computational problem solver of POET. In the following sections, diﬀerent
parts of POET are explained in more detail.

3.1 Representation and Models

The GP representation used in POET is a table of rules with four columns (Figure 3). Each rule
consists of a unique rule ID, a motif, a weight, and a status bit denoting whether the associated
motif has been previously found in protein sequences of the dataset or not. It is essential to track
the status of rules since only rules with the status of ”1” are expressed for model evaluation or
sequence prediction. Unexpressed rules with the status of ”0” might be altered by undergoing
recombinational operators in later generations and become useful once their motifs become less

3A complete set of expressed proteins by an organism

7

Figure 3: A simple POET model consisting of four example rules with sequences and values
obtained from an early POET model. Each symbol in the motif sequence represents one amino
acid. Rules 1, 3, and 4 have a status of 1 and are found in dataset. Rule 2 is not found in the
dataset and is not expressed for evaluation and prediction purposes. Weights can be negative or
positive ﬂoat values.

random and are found in the training dataset. POET models are initialized with constrained
random motifs and weights in the ﬁrst generation of evolution.

Evolved POET models predict the CEST contrast levels of a given protein sequence in the
manner described in Algorithm 1. First, the input protein sequence is searched for motifs avail-
able in the model’s rule table. Once the search is completed, the sum of the weights of the found
motifs represents the CEST contrast prediction made by the predicting model. POET always
prioritizes longer amino acid motifs over shorter ones. For example, if the protein sequence in
hand is ”TKW” and all ”T,” ”K,” and ”TK” motifs are present in a model table, ”TK” is prior-
itized over ”T” and ”K” rules and the position index points to ”W” ignoring both of the shorter
rules. As shown in the pseudo-code, the reverse sequence of the same motif is also evaluated for
each rule. This is an attempt to extract more meaningful information from the abstract sequence

8

space.

Algorithm 1: Pseudo-code for computing the predicted CEST contrast levels using a
POET model. length() represents a function returning the size of a given input string
array. This algorithm takes a protein sequence and a predicting model as input to output
the predicted CEST contrast value.

Data: sequence, model
Result: predictedCEST
predictedCEST ← 0 ;
position ← 0 ;
while position < length(sequence) ;
do

for rule : model.rules;
do

if status is 0;
then

continue;

/* Loop through sequence symbols */

/* Loop through model rules */

/* Unexpressed rule */

end
if length(rule.pattern) + position > length(sequence) then

continue;

/* sequence is not long enough */

end
motif ← rule.motif ;
reversedM otif ← reverse(motif ) ;
portion ← sequence[position : (position + length)] ;
if motif = portion or reversedM otif = portion;
then

predictedCEST ← rule.weight + 1 ;
position ← length + 1 ;
break ;

end

end
position ← position + 1;

end
return predictedCEST ;

/* motif is found */

3.2 Selection Mechanism and Evolutionary Operators used in the GP

Tournament selection [50] with elitism is used as the selection mechanism of POET. The indi-
vidual with the highest ﬁtness will always be selected for the next generation with no change
(elitism). The rest of the population undergoes a tournament selection in which a pool of ﬁve in-
dividuals is randomly chosen, and the two ﬁtter individuals are selected for performing crossover
and creating a new oﬀspring for the next generation.

Crossover: In the POET’s crossover mechanism, every two ﬁt parents chosen by tournament
selection are used to generate one new oﬀspring. All expressed rules (rules with a status bit of 1)
and 20% of unexpressed rules for both parents are selected to form the new oﬀspring. In order
to avoid bloat, if the total number of rules in the oﬀspring model table exceeds the maximum
allowed size (an arbitrary value representing how large POET models can get with respect to
number of rules), POET uses a shrink step to cut down the number of unexpressed rules and,

9

Figure 4: A simple crossover example in POET where the maximum allowed rule count for each
model is 5. All the parents’ expressed rules are selected to form the oﬀspring. Unexpressed
rules have a 20% chance of being selected while undergoing recombination.
In this example,
only unexpressed rule 2 of parent A is selected while the unexpressed rule 3 of parent B is
not. Recombination of rules forms a new oﬀspring table with size 7. Since the oﬀspring size
exceeds the allowed rule count by 2, two rules are removed during the model shrinking process.
Unexpressed rules are prioritized to be removed in such a case. Since after removing all the
unexpressed rules, the oﬀspring still does not have a legal size, the shortest expressed rule is
removed to form the ﬁnal oﬀspring

if necessary, some of the expressed rules (prioritizing removing shorter rules over more extended
rules) for the model to reach the permitted model size. At every step, model tables are sorted,
arranging from longer motifs to shorter ones to follow the same design principle of prioritizing

10

the discovery of longer motifs. Figure 4 illustrates a simple example of crossover in POET.

Algorithm 2: Pseudo-code of the mutation evolutionary operator of POET. ARM and
RRM happen on individual models, while CWM, APM, and RPM can mutate every
rule of an individual. Each of these mutational operators has a conﬁgurable mutation
rate.

Data: population
Result: population ;
for individual in population ;
do

if rand(0, 1) <ARM rate ;
then

individual ← ARM (individual);

end
if rand(0, 1) <RRM rate ;
then

individual ← RRM (individual);

end
for rule in individual.rules ;
do

if rand(0, 1) <CWM rate ;
then

rule ← CW M (rule);

end
if rand(0, 1) <APM rate ;
then

rule ← AP M (rule);

end
if rand(0, 1) <RPM rate ;
then

rule ← RP M (rule);

end

end

end

/* Mutated population */
/* Loop through population */

/* Add Rule Mutation */

/* Remove Rule Mutation */

/* Loop through individual rules */

/* Change Weight Mutation */

/* Add to Pattern Mutation */

/* Remove from Pattern Mutation */

Mutational Operators: Multiple mutational operators are used in POET, all of which
have a chance to increase the diversity of the models and help explore diﬀerent valleys of the
ﬁtness landscape:

1. Add Rule Mutation (ARM): Adds a randomly generated rule to the model.

2. Remove Rule Mutation (RRM): Randomly selects a rule and removes it from the model.

3. Change Weight Mutation (CWM): Alters the weight of a randomly selected rule of the
model by adding or subtracting a small random value (between 0 and 1) to/from it (equal
chance).

4. Add to Pattern Mutation (APM): Randomly selects a rule and adds a amino acid symbol

to its pattern.

11

5. Remove from Pattern Mutation (RPM): Randomly selects a rule and removes a symbol

from its motif.

POET uses Algorithm 2 to apply these mutational operators on individual models.

3.3 Evaluation of Models

Root Mean Square Error (RMSE) [51] is used as the metric to evaluate the ﬁtness of the POET
models. RMSE is an error measurement, and therefore lower values of it indicate accurate
predictions. Since error values are squared when using RMSE as the ﬁtness metric, models with
high error rates are punished more than those whose error rate is lower. The following equation
describes the RMSE formula:

RM SE =

(cid:114)

(cid:16)

Σn

i=1

1
n

(cid:17)2

di − fi

To better train POET models starting with a relatively small dataset, 10-fold cross-validation
[52] was used. In k-fold cross-validation, the dataset is divided into k groups, and each model
gets evaluated k times. Each time, a group is selected to act as the test set while the rest work
as the training set. This process continues until all groups are selected as the test set. The
individual’s ﬁtness is the average ﬁtness among all the k iterations. K-fold cross-validation helps
evolve more accurate and generalized models, especially if the dataset is small.

Algorithm 3: Evolving protein sequences using a trained POET sequence-function
rand choice() chooses a random element from a given list of elements.
model.
model.predict() predicts the ﬁtness of a given protein sequence using the best previ-
ously trained model. Starting from a random population of sequences increases the
novelty in prediction.

Data: model, symbols, generations
Result: population ;
population ← init random() ;
gen ← 0;
while gen < generations do

/* Predicted protein population */
/* Randomly initialize the population */

for sequence in population do
old sequence ← sequence;
random site ← rand(0, length(sequence);
random symbol ← rand choice(symbols);
sequence[random site] ← random symbol ;
if model.predict(sequence) < model.predict(old sequence) then

/* Alter a random site */

sequence ← old sequence ;

/* Keep the altered sequence */

end

end
gen ← gen + 1;

end
sort(population);
return(population);

12

Symbol Hydrophobicity Symbol Hydrophobicity

I
L
F
V
M
P
W
J
T
E
K

-0.31
-0.56
-1.13
0.07
-0.23
0.45
-1.85
0.17
0.14
2.02
0.99

Q
C
Y
A
S
N
D
R
G
H

0.58
-0.24
-0.94
0.17
0.13
0.42
1.23
0.81
0.01
0.96

Table 1: Table of amino acids and their respective hydrophobicity values [53].

3.4 Optimization and Prediction of Peptides that Produce High CEST

Contrast

The proposed system is a multi-epoch feedback system, here used to predict better proteins
concerning their CEST contrast level measured by MRI. As illustrated in Figure 2, in each epoch
of the experiment, a dataset containing protein sequences and their respective CEST contrast
values evaluated in MRI is given to POET. POET computationally evolves protein sequence-
function models over generations, and the ﬁttest model is selected for the next step. Then a
pool of randomly generated protein sequences is given to the ﬁttest POET model for evaluation.
A copy of the best sequence regarding predicted ﬁtness is saved without change (elitism), and
the pool of the protein variants undergo mutagenesis, with every mutant being only one symbol
away from their parent protein. If there is no improvement after mutation of a sequence, the
old sequence reverts, and no changes are applied to that individual. This process repeats for an
arbitrary number of generations. Afterward, the ﬁttest predicted proteins in the sequence pool
of the latest generation are chosen for wet-lab measurements. Finally, the new data points are
added to the dataset to improve the POET models’ accuracy and learn from previous epochs.
Algorithm 3 exhibits the details of this implementation.

3.5 External Knowledge on Soluble Proteins

Medical characteristics of protein engineering make it very important for CEST protein agents
to be soluble in water. Therefore, applying a simple hydrophobicity threshold improves ﬁnding
soluble proteins and better predictions. We use the data shown in Table 3.4. If the sum of the
hydrophobicity levels of amino acids in a peptide sequence is less than zero, we consider the
peptide insoluble and do not use that sequence in protein optimization and prediction (sequence
ﬁtness is set to zero).

3.6 Dataset

For the ﬁrst epoch of the experiment, a dataset containing only 36 data points derived from
available literature was used. After that, at least ten new data points were added in each epoch
dataset through wet-lab experiments based on the predicted proteins.
In the ﬁnal epoch of
the experiment, the dataset contained 163 data points of protein sequences and their respective

13

CEST contrast values, with some of the new variants being wild types. Table 2 shows the dataset
curated during POET epochs.

Sequence

KKKKKKKKKKKK
KSKSKSKSKSKS
KHKHKHKHKHKH
KGKGKGKGKGKG
KSSKSSKSSKSS
KGGKGGKGGKGG
KSSSKSSSKSSS
KGGGKGGGKGGG
RRRRRRRRRRRR
RSRSRSRSRSRS
RGRGRGRGRGRG
RHRHRHRHRHRH
RTRTRTRTRTRT
RTTRTTRTTRTT
RTTTRTTTRTTT
TTTTTTTTTTTT
TKTKTKTKTKTK
DTDTDTDTDTDT
ETETETETETET
TTKTTKTTKTTK
DTTDTTDTTDTT
ETTETTETTETT
TTTKTTTKTTTK
DTTTDTTTDTTT
ETTTETTTETTT
TTTTTKTTTTTK
DTTTTTDTTTTT
ETTTTTETTTTT
DSDSDSDSDSDS
DSSSDSSSDSSS
DSSSSSDSSSSS
KKRKKHKKGKKP
KKAKKKGKKHKK
KKGKKKGKKHKK
KKGKKKGKKPKK
MPRRRRSSSRPVRRRRR
PRVSRRRRRRGGRRRR
DWNNYLYQNLH
SYYWLWWHQQI
NWNWWGLSYLA
NQYSNWNKNYK
NENQWHYYWRQ
NGTLYLNNYYE
NSSNHSNNMPCQ
IRTYLRKRNSTQ

CEST Contrast
Value (3.6 ppm)
12.5
17
12.7
10.8
13.2
11.8
13
12.1
22
12.8
17.2
5.5
18.7
16.3
18.9
6.5
14.1
2.2
1.7
12.6
4
4.4
13.8
4
4
13.8
7.2
5.9
2.5
7
9.1
11.9
9.9
11.3
9.3

19

0
0
0
6.94
0
0
14.06
8.03

Sequence

GIFKTTKCKHNS
SNHKMSECRGLR
FNSNKITPTSNM
VNSDPSNGQMRD
LSNRRGREQYAG
QTATENSQMNSG
QTEHYENSARNS
KDRTSKPKRPWC
GRKRGAIWKDTK
CCWHNPKWRRTR
KYTKTRKQSSKA
RGKMPLRWMTRK
GNCPMKVCSPMG
VNLPMVMPNLRM
GPMPMNAKMKLC
KVIRYVVAPMKL
IKGMNIKMPTDQ
MWQMKWTRKTRE
HGRKWKRTKFDD
DKRKIKQKMWWG
RRMVNRTITRMW
RKHHGWRWEQWK
HWSTCTRTRTLS
WWWKPKREDFMK
HIKWRLTKGTRT
WDRTSTRPSSVL
KPWHGCASRTKR
KKRLHWIRWHCG
RKHHGWRWEQWK
WFGLQRHLKKKD
CHLKDLRKMGLR
KMWDWEQKKKWI
QRHDSHRHGLWL
LELKLGKRPMGW
GQRWLYKMKDSM

MWVKGMKHKKMK

LDHTWGKWGHQS
DKVCKIQKRKWH
MAALLYQHRLARR
KPCKWAGRACAK
CQLAWRPCAKAS
QCAGWVQKRQIQ
RRCQAQEFWLGA
GLIEARAMQQCC

CEST Contrast
Value (3.6 ppm)
7.61
5.98
5.29
4.15
7.08
3.64
1.09
8.67
12.75
18.46
22.48
17.14
8.89
4.53
5.81
8.94
9.95
16.24
15.49
10.86
15.01
12.53
17.1
6.58
16.08
13.46
16.19014
12.01536
13.59362
19.0715
10.1388
34.11149
7.543669
29.24477
11.86265

13.23495

11.50256
12.51172
3.528221
16.69529
19.532
23.37685
9.515134
2.704976

Sequence

NFLRAQRQCQKQ
MAMADAAAPMNA
AQCCQHRKGYMN
NRVTESVRNVKM
NVVVQRRNHHTS
VINKVISCPCVN
GGRVWEWNVAA
NNKCQVVAAFVM
VLTWSAVNNNVQ
ETNVRVKVVSES
NCGVNLVNAVGQ
HIAVVNWVNVGH
CNNIQGRNNSVW
VPNIQVKGSK
PVARKVVQICHP
VTRMTIQVKGSK
MAMADAAAPMNA
MKVAAAMAPKQV
PVVYKTVIQCCD
KVLWRMPAQIIQ
VSVVATGCVWET
AKCKVQSANVCK
VAWVMKAHVCTM
WDWEQKKKWI
ERQEEKIKKW
SDGSKIKDRD
SSDQDRDKWL
LLRLLGLVER
KEEVWLKWLI
KGKLDKDRNL
HDDKNKESDD
QERRDDILWD
KRIIEDDQLE
VCNRIEPLKPIL
LHSSQWLKVDHLL

VINKVISNPCVN

GNKKNWRWYKNR
ICLKSQPICGID
LWSDIKMKLKKT
NWRDCLSLIVPN
KMGKLIGIPVLK
NDISMCNKNNNW
VSLQCWELGPNK
TVSEPVMMVSVS

CEST Contrast
Value (3.6 ppm)
18.16368
3.051858
14.69376
3.683273
28.05341
8.109024
6.08351
5.401218
0
0
0
0
0
4.99326
18.63698
30.16713
6.965346
38.66109
4.333159
13.51793
14.33556
37.82153
4.201038
31.26163
20.79427
8.630329
16.82505
3.037419
13.42497
23.28052
7.479407
2.296864
15.19034
21.82239
18.17873

8.107188

14.71334
29.49547
49.37196
3.179373
47.83688
8.824446
15.70919
7.771304

Sequence

PVNRLGKMSKNR
VGSVKSGNLRMR
TSKSKKRMTAKK
ETNVRVKVVSES
EPSNLPKGMNEK
RLWNSGEGRGEN
ELNTGLVLVNWK
RPPMLNVVRVVG
KWVVRPRIRRLL
IGVLRSVKQTVR
VINKVISNPCVN
ETNVRVKVVSES
RLPKRVQGNVEK
GLGNQHVVVLGV
KVRCLVEARPSW
HLVVSPRVSWGC
IIRSPICCVSRV
DKRKIKQKMWWG
RKHHGWRWEQWK
EMRQWKWMWENA
PIKQIAWPIIEH
KMWDWEQKKKWI
ARNRKKIMMRWI
NAPWKHWRIINE
NKQRRMLSRERS
LSQQPRKRATWR
IRRWNDRIRITS
QCRAGAMPAMYV
PRSWEVKEKETM
PGGVRSNDLLEV

CEST Contrast
Value (3.6 ppm)
28.83256
26.22986
29.83349
5.707696
24.69024
12.26758
0
6.489129
14.62156
28.55074
8.516833
2.368102
30.61573
3.528919
8.194995
5.30282
12.89188
19.13841
20.00466
6.580628
13.6599
24.09143
29.01464
8.898582
28.52089
12.60838
13.90809
12.05809
18.27337
11.18031

Table 2: All the available data in the dataset used for all the epochs of POET including the
mock test data and the discovered protein sequences

4 Results

4.1 Experimental Setup

The experiments were performed for eight epochs. Table 3 shows a brief overview of the used
experimental parameters of POET for the case study. Experiments were conducted on Michigan
State University’s High-Performance Computing Center (HPCC) computers details of which are
available in [54]. Each of the experiment repeats were run in parallel using 2 HPCC CPU cores
(2.5GHz) and 8 gigabytes of RAM on a single node.

4.2 Evolution of Models per Epoch

In each epoch of the experiment, POET is run 50 times, each repeat evolving a population of 100
models over 10000 generations. The overall best model is used to predict the protein sequences
to be tested in a wet lab at the end of each epoch. Figure 5 illustrates the evolution of models for
all eight epochs. The drop in RMSE values in early generations is evident in all the experiments
and is due to starting with randomly initialized populations. The same evolutionary method is

14

Parameter
Population Size
Tournament Size
Max Rule Motif Length
Max Table Rule Count
Number of Generations
Unused Rule Crossover Rate
Mutation Rate

Value
100
5
9
100
5000
20%
16% for all types

Table 3: Experimental parameters used for the case study.

Figure 5: Evolution of the POET models over generations for 8 epochs of the POET experiment.
The ﬁtness metric is RMSE, and therefore, lower values indicate more accurate measurements.

used throughout all the epochs; however, the number of data points in each epoch varies from the
previous one due to the addition of new data at the end of each epoch. For example, for epoch 1,
the training dataset had only 36 data points making it easier for the algorithm to achieve lower
RMSE values by quickly over-ﬁtting the data. Meanwhile, 112 data points were available in the
dataset used for epoch 8, which explains higher RMSE values. Furthermore, in earlier epochs,
the 50 best models performed more similarly, while models performance varies more in the later
epochs.

Table 4 summarizes the performance of the best model among 50 repeats in generation
10000 for each epoch. Test and training ﬁtness values show a slight over-ﬁtting of the models.
Each POET model can have a maximum number of 100 rules which can be either expressed
or unexpressed. The best model of each Epoch approximately uses all of the possible 100 rule
space while almost 50% of those rules are expressed (on average around 44 rules out of 97 are
expressed). An increase in the RMSE level of the best model for each epoch is visible as the
epoch number increases (Training Fitness column) showing the diﬃculty of ﬁnding ﬁtter models

15

Figure 6: Improvement of the POET models in each epoch of the experiment. RMSE levels for
the best models of each epoch (blue) and the average RMSE of all the models in each epoch
(orange) are tested against all available data.

on more data points. This increase does not indicate that models of later epochs are less accurate
since the dataset used for evaluating each model is diﬀerent. To show RMSE improvements of the
trained models over epochs, it is interesting to see how well the same best models for each epoch
perform when tested against all available data. When tested against all available data (Best
Overall column), as the epoch number increases, the RMSE values for average of the models
decreases, showing that the trained models are improving on average in each epoch (Figure 6).
As for the best models of each epoch, RMSE levels slightly increase for epoch 2 and epoch 7
compared to epoch 1 and epoch 6, respectively. However, RMSE of the best model in epoch 8 is
lower than all previous epochs.

Figure 7: Predicted ranks (orange) of the protein sequences in a mock set versus their actual
rank (blue). The proteins under the red line were among the top 10. The check-marked bars
indicate top 10 protein sequences which were also predicted by the best model as top 10.

16

Epoch

Data
Points

E1
E2
E3
E4
E5
E6
E7
E8

42
51
61
71
82
92
102
112

Training
Fitness
(RMSE)
1.272
1.558
2.238
2.308
2.898
3.651
3.923
4.891

Test
Fitness
(RMSE)
1.307
1.576
2.258
2.326
2.919
3.674
3.944
4.916

Best
Overall
RMSE
10.189
11.001
10.185
10.096
8.974
8.247
8.686
7.845

Average
Overall
RMSE
16.583
15.882
13.276
12.786
11.531
11.349
10.646
9.954

Total
Rules #

Expressed
Rules #

97
97
96
96
97
96
97
97

44
45
45
44
44
44
44
44

Table 4: Comparison of performance of the overall best POET models at generation 10000 for
all the epochs.

In each epoch, after model training, the best POET model is used to choose ﬁtter proteins
in a large pool of artiﬁcially generated protein sequences.
In this process, all the generated
sequences are evaluated using the model and the top 10 sequences are subsequently selected to
be tested in wet labs. A mock test was designed to analyze this process on a small dataset of 43
protein sequences with actual measured CEST contrast values. None of the data points in this
set were used in the training of the best POET model (best model of E8). These data points
were given to the best POET model to evaluate and sort based on their predicted CEST contrast
values. For a perfect model, the order of proteins after sorting would be the same as the order
of these proteins sorted based on their actual CEST contrast values.

Figure 7 shows predicted order made by the best model of Epoch 8 in orange and their actual
order in the dataset in blue. Although not signiﬁcantly, the two series positively correlate with
Pearson correlation coeﬃcient value of 0.63. In each POET epoch, 10 new sequences are chosen
to be evaluated in wet labs. An interesting observation is that 5 out of the top 10 ﬁttest protein
sequences are among the top 10 predictions of the best model.

4.3 Improvement of CEST Contrast Proteins

An essential aspect of the design goal of POET is to replace parts of DE in order to enhance
the process by ﬁnding ﬁtter proteins in a shorter time with less cost. We measure the CEST
contrast levels (normalized MTR) during each epoch of our experiment. M T Rasym [55] is the
most common metric for evaluating CEST contrast protein agents and determines the signal
strength of the target proteins in an MRI environment and is calculated through the following
equation:

M T Rasym(+τ ) =

S−τ − S+τ
S0

in which S+τ and S−τ is the measured signal with RF saturation at two points of +τ and −τ
respectively. S0 is the exact measurement without RF saturation [55]. We normalized MTR
against K12, a peptide of 12 K (lysine) amino acids, to have a fair comparison across all cycles of
our experiments. This peptide was chosen due to its high MRI contrast value and its similarity
to the other reported results.

After the second cycle of our experiment, our predicted peptides, on average, had a higher
contrast value than K12. On cycle seven, POET predicted a protein with an approximately
400% increase in contrast to K12.

17

It is conventionally believed that a protein with high CEST contrast values should be posi-
tively charged, and it likely consists of multiple lysine amino acid [56]. However, since POET’s
exploration is not limited to speciﬁc parts of the search space, we managed to ﬁnd proteins that
do not follow this principle and yet have a high CEST contrast value. Details of this experiment
can be found in [57].

Figure 8: Frequency of discovered motifs in the best models of the 50 repeats of epoch 8. Due
to the simplicity and the high number of data points only motifs are shown that are common
between at least 10% of the models

Figure 8 demonstrates the most frequently observed motifs among the best models of the 50
repeats performed for epoch 8. 63 of the discovered motifs are common between at least 10% of
the models in epoch 8. Motifs with a single amino acid symbol are the most trivial to ﬁnd and
therefore are the most common motifs among the models. In addition, four motifs with a length
of higher than six symbols are found. All of these motifs are listed as the x-axis of Figure 8.

5 Discussion and Future Directions

The main contribution of this paper is POET, a tool based on GP to predict protein functions
from the abstract and yet semantically rich representation of amino acid sequences of proteins
and peptides. DE starts mutagenesis from proteins with analogous properties to the desired one.
Similar starting points make it highly likely for this process to get stuck in a local optimum.
Unlike DE, POET starts the prediction process from random sequences, enabling it to explore
diﬀerent areas of the protein search space possibly jumping between optima to evolve ﬁtter
models. Results indicate that it is possible to evolve ﬁtter CEST contrast predicting models over
epochs, with the best model of Epoch 1 having an RMSE score of 10.189 over all available data,
while the best model of Epoch 8 has an RMSE of 7.845 for the same dataset. Running a mock
experiment to evaluate the best model of Epoch 8 (Best model selected for predicting sequences
of the next epoch), this model was able to ﬁnd 5 of the top 10 CEST proteins. Comparing
the ranks predicted by the best model and the actual ranks of the proteins in the dataset with
respect to their CEST contrast showed a positive correlation with Pearson r value of 0.63.

18

Our ﬁndings demonstrate an improvement in the CEST levels of the predicted proteins. Dur-
ing the experiments, a protein with four times higher CEST contrast than K-12 was discovered.
Results on Figure 8 suggests that there are common motifs discovered between the best-evolved
models while some diversity persists. This diversity, along with the randomness of POET has
been beneﬁcial in our experiments by predicting a diverse range of protein sequences and guiding
us to explore sections of the search space, which was not possible before using conventional DE
methods.

Another contribution of this work is curating a rich dataset consisting of 163 proteins and
their respective CEST contrast values (for 3.6 PPM) during the eight epochs of the experiment.
As a side discovery, some of the predicted proteins indicated that ﬁtter proteins do not necessarily
follow conventional bio-engineering theories. This dataset is made available for researchers in
this manuscript.

The same algorithm with the same number of generations under-performs in evolving accurate
models as the dataset grows. POET is a modular system with the potential of improvement on
many fronts. For example, dynamic recombination rates and ﬁtness evaluation by considering
motif diversity in the population can be an excellent approach to improving models’ evolution
over generations. Incorporating ideas of active learning to enhance the prediction and selection
of the proteins to be tested in the wet labs could potentially improve the chances of ﬁnding ﬁtter
proteins in fewer epochs. Adding more structural information and proven natural rules to quickly
explore more extensive parts of the search space is also a viable future direction. Although the
focus of this research was on evolving CEST contrast proteins, the capabilities of POET is not
limited to evolving sequence-function models of this type. Therefore, it will be interesting to test
POET with datasets of diﬀerent protein functionalities.

Acknowledgements

Partial support for this work came from NIH under award nrs. 1R01EB030565-01, 1R01EB031008-
01 and P41-EB024495. Computational model evaluation was done on MSU’s HPCC system and
is gratefully acknowledged.

References

[1] E. A. Ponomarenko, E. V. Poverennaya, E. V. Ilgisonis, M. A. Pyatnitskiy, A. T. Kopylov,
V. G. Zgoda, A. V. Lisitsa, and A. I. Archakov, “The size of the human proteome: The
width and depth,” International Journal of Analytical Chemistry, vol. 2016, pp. 1–6, 2016.

[2] R. K. Singh, J.-K. Lee, C. Selvaraj, R. Singh, J. Li, S.-Y. Kim, and V. C. Kalia, “Protein
engineering approaches in the post-genomic era,” Current Protein & Peptide Science, vol. 19,
no. 1, 2017.

[3] H. Hellinga, “Computational protein engineering,” Nature Structural Biology, vol. 5, no. 7,

pp. 525–527, 1998.

[4] B. Alberts, A. Johnson, J. Lewis, D. Morgan, M. Raﬀ, K. Roberts, and P. Walter, Molecular

Biology of the Cell. W.W. Norton & Company, 2017.

[5] F. H. Arnold, “Design by directed evolution,” Accounts of Chemical Research, vol. 31, no. 3,

pp. 125–131, 1998.

19

[6] K. K. Yang, Z. Wu, and F. H. Arnold, “Machine-learning-guided directed evolution for

protein engineering,” Nature Methods, vol. 16, no. 8, pp. 687–694, 2019.

[7] J. R. Koza and R. Poli, “Genetic Programming,” in Search Methodologies, pp. 127–164,

Springer US, 2005.

[8] J. H. Holland, “Genetic Algorithms,” Scientiﬁc American, vol. 267, no. 1, pp. 66–73, 1992.

[9] C. Darwin, The Origin of Species. New York: PF Collier & Son, 1909.

[10] P. C. M. van Zijl and N. N. Yadav, “Chemical exchange saturation transfer (CEST): What
is in a name and what isn't?,” Magnetic Resonance in Medicine, vol. 65, no. 4, pp. 927–948,
2011.

[11] A. A. Gilad, A. Bar-shir, A. R. Bricco, Z. Mohanta, and M. T. McMahon, “Protein and
peptide engineering for cest imaging in the age of synthetic biology,” NMR in Biomedicine,
2022. accepted.

[12] R. D. Airan, A. Bar-Shir, G. Liu, G. Pelled, M. T. McMahon, P. C. M. van Zijl, J. W. M.
Bulte, and A. A. Gilad, “MRI biosensor for protein kinase a encoded by a single synthetic
gene,” Magnetic Resonance in Medicine, vol. 68, no. 6, pp. 1919–1923, 2012.

[13] A. A. Gilad, M. T. McMahon, P. Walczak, P. T. Winnard, V. Raman, H. W. M. van
Laarhoven, C. M. Skoglund, J. W. M. Bulte, and P. C. M. van Zijl, “Artiﬁcial reporter gene
providing MRI contrast based on proton exchange,” Nature Biotechnology, vol. 25, no. 2,
pp. 217–219, 2007.

[14] O. Perlman, H. Ito, A. A. Gilad, M. T. McMahon, E. A. Chiocca, H. Nakashima, and
C. T. Farrar, “Redesigned reporter gene for improved proton exchange-based molecular
MRI contrast,” Scientiﬁc Reports, vol. 10, no. 1, 2020.

[15] S. M. Malakauskas and S. L. Mayo, “Design, structure and stability of a hyperthermophilic

protein variant,” Nature Structural Biology, vol. 5, no. 6, pp. 470–475, 1998.

[16] J. Sim, S.-Y. Kim, and J. Lee, “Prediction of protein solvent accessibility using fuzzy k-

nearest neighbor method,” Bioinformatics, vol. 21, no. 12, pp. 2844–2849, 2005.

[17] M. Wagner, R. Adamczak, A. Porollo, and J. Meller, “Linear regression models for sol-
vent accessibility prediction in proteins,” Journal of Computational Biology, vol. 12, no. 3,
pp. 355–369, 2005.

[18] Y. Xu, D. Verma, R. P. Sheridan, A. Liaw, J. Ma, N. M. Marshall, J. McIntosh, E. C.
Sherer, V. Svetnik, and J. M. Johnston, “Deep dive into machine learning models for protein
engineering,” Journal of Chemical Information and Modeling, vol. 60, no. 6, pp. 2773–2790,
2020.

[19] J. Xu, “Distance-based protein folding powered by deep learning,” Proceedings of the Na-

tional Academy of Sciences, vol. 116, no. 34, pp. 16856–16865, 2019.

[20] A. W. Senior, R. Evans, J. Jumper, J. Kirkpatrick, L. Sifre, T. Green, C. Qin, A. ˇZ´ıdek,
A. W. R. Nelson, A. Bridgland, H. Penedones, S. Petersen, K. Simonyan, S. Crossan,
P. Kohli, D. T. Jones, D. Silver, K. Kavukcuoglu, and D. Hassabis, “Improved protein struc-
ture prediction using potentials from deep learning,” Nature, vol. 577, no. 7792, pp. 706–710,
2020.

20

[21] L. Siqueira and S. Venske, “Ab initio protein structure prediction using evolutionary ap-
proach: A survey,” Revista de Inform´atica Te´orica e Aplicada, vol. 28, no. 2, pp. 11–24,
2021.

[22] M. M. Khimasia and P. V. Coveney, “Protein structure prediction as a hard optimization
problem: The genetic algorithm approach,” Molecular Simulation, vol. 19, no. 4, pp. 205–
226, 1997.

[23] M. A. Rashid, M. T. Hoque, M. A. H. Newton, D. N. Pham, and A. Sattar, “A new genetic
algorithm for simpliﬁed protein structure prediction,” in AI 2012: Advances in Artiﬁcial
Intelligence (M. Thielscher and D. Zhang, eds.), (Berlin, Heidelberg), pp. 107–119, Springer,
2012.

[24] J. R. Koza and D. Andre, “Automatic discovery of protein motifs using genetic program-
ming,” in Evolutionary Computation: Theory and Applications, pp. 171–197, World Scien-
tiﬁc, 1999.

[25] E. C. Alley, G. Khimulya, S. Biswas, M. AlQuraishi, and G. M. Church, “Uniﬁed rational
protein engineering with sequence-based deep representation learning,” Nature Methods,
vol. 16, no. 12, pp. 1315–1322, 2019.

[26] R. Seehuus, A. Tveit, and O. Edsberg, “Discovering biological motifs with genetic program-
ming,” in Proceedings of the 2005 Conference on Genetic and Evolutionary Computation -
GECCO '05 (H. Beyer, ed.), ACM Press, 2005.

[27] R. Seehuus, “Protein motif discovery with linear genetic programming,” in Knowledge-Based
Intelligent Information and Engineering Systems (R. Khosla, R. J. Howlett, and L. C. Jain,
eds.), (Berlin, Heidelberg), pp. 770–776, Springer, 2005.

[28] M. F. Brameier and W. Banzhaf, Linear Genetic Programming. Springer, New York, 2007.

[29] A. Fathi and R. Sadeghi, “A genetic programming method for feature mapping to improve
prediction of HIV-1 protease cleavage site,” Applied Soft Computing, vol. 72, pp. 56–64,
2018.

[30] W. B. Langdon, J. Petke, and R. Lorenz, “Evolving better rnafold structure prediction,”
in Genetic Programming (M. Castelli, L. Sekanina, M. Zhang, S. Cagnoni, and P. Garc´ıa-
S´anchez, eds.), (Cham), pp. 220–236, Springer, 2018.

[31] L. C. Borro, S. R. M. Oliveira, M. E. B. Yamagishi, A. L. Mancini, J. G. Jardine, I. Mazoni,
E. H. dos Santos, R. H. Higa, P. R. Kuser, and G. Neshich, “Predicting enzyme class from
protein structure using bayesian classiﬁcation,” Genetics and Molecular Research, 2006.

[32] L. F. Leijˆoto, T. Assis De Oliveira Rodrigues, L. E. Z´aratey, and C. N. Nobre, “A genetic
algorithm for the selection of features used in the prediction of protein function,” in 2014
IEEE International Conference on Bioinformatics and Bioengineering, pp. 168–174, 2014.

[33] Z. Wu, S. B. J. Kan, R. D. Lewis, B. J. Wittmann, and F. H. Arnold, “Machine learning-
assisted directed protein evolution with combinatorial libraries,” Proceedings of the National
Academy of Sciences, vol. 116, no. 18, pp. 8852–8858, 2019.

[34] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,
P. Prettenhofer, R. Weiss, V. Dubourg, A. Passos, D. Cournapeau, M. Brucher, M. Per-
rot, and ´Edouard Duchesnay, “Scikit-learn: Machine learning in Python,” The Journal of
Machine Learning Research, 2011.

21

[35] J. Linder, N. Bogard, A. B. Rosenberg, and G. Seelig, “A generative neural network for
maximizing ﬁtness and diversity of synthetic DNA and protein sequences,” Cell Systems,
vol. 11, no. 1, pp. 49–62.e16, 2020.

[36] D. Repecka, V. Jauniskis, L. Karpus, E. Rembeza, I. Rokaitis, J. Zrimec, S. Poviloniene,
A. Laurynenas, S. Viknander, W. Abuajwa, O. Savolainen, R. Meskys, M. Engqvist, and
A. Zelezniak, “Expanding functional protein sequence spaces using generative adversarial
networks,” Nature Machine Intelligence, vol. 3, no. 4, pp. 324–333, 2021.

[37] A. Hawkins-Hooker, F. Depardieu, S. Baur, G. Couairon, A. Chen, and D. Bikard, “Gen-
erating functional protein variants with variational autoencoders,” PLOS Computational
Biology, vol. 17, no. 2, p. e1008736, 2021.

[38] H. Cao, J. Wang, L. He, Y. Qi, and J. Z. Zhang, “DeepDDG: Predicting the stability change
of protein point mutations using neural networks,” Journal of Chemical Information and
Modeling, vol. 59, no. 4, pp. 1508–1514, 2019.

[39] Y. B. L. Samaga, S. Raghunathan, and U. D. Priyakumar, “SCONES: Self-consistent neural
network for protein stability prediction upon mutation,” The Journal of Physical Chemistry
B, vol. 125, no. 38, pp. 10657–10671, 2021.

[40] P. Das, T. Sercu, K. Wadhawan, I. Padhi, S. Gehrmann, F. Cipcigan, V. Chenthamarakshan,
H. Strobelt, C. dos Santos, P.-Y. Chen, Y. Y. Yang, J. P. K. Tan, J. Hedrick, J. Crain, and
A. Mojsilovic, “Author correction: Accelerated antimicrobial discovery via deep generative
models and molecular dynamics simulations,” Nature Biomedical Engineering, vol. 5, no. 8,
pp. 942–942, 2021.

[41] Y. Yokobayashi, K. Ikebukuro, S. McNiven, and I. Karube, “Directed evolution of trypsin
inhibiting peptides using a genetic algorithm,” Journal of the Chemical Society, Perkin
Transactions 1, vol. 1, no. 20, p. 2435, 1996.

[42] F. Archetti, S. Lanzeni, E. Messina, and L. Vanneschi, “Genetic programming for compu-
tational pharmacokinetics in drug discovery and development,” Genetic Programming and
Evolvable Machines, vol. 8, no. 4, pp. 413–432, 2007.

[43] F. Yoshida and J. G. Topliss, “QSAR model for drug human oral bioavailability,” Journal

of Medicinal Chemistry, vol. 43, no. 13, pp. 2575–2585, 2000.

[44] N. Hulo, “Recent improvements to the PROSITE database,” Nucleic Acids Research, vol. 32,

no. 90001, pp. 134D–137, 2004.

[45] J. wei Luo and T. Wang, “Motif discovery using an immune genetic algorithm,” Journal of

Theoretical Biology, vol. 264, no. 2, pp. 319–325, 2010.

[46] M. Kaya, “Motif discovery using multi-objective genetic algorithm in biosequences,” in
Advances in Intelligent Data Analysis VII (M. R. Berthold, J. Shawe-Taylor, and N. Lavraˇc,
eds.), (Berlin, Heidelberg), pp. 320–331, Springer, 2007.

[47] B. C. H. Chang, A. Ratnaweera, S. K. Halgamuge, and H. C. Watson, “Particle swarm
optimisation for protein motif discovery,” Genetic Programming and Evolvable Machines,
vol. 5, no. 2, pp. 203–214, 2004.

[48] U. Consortium, “UniProt: a worldwide hub of protein knowledge,” Nucleic Acids Research,

vol. 47, no. D1, pp. D506–D515, 2018.

22

[49] A. Chang, L. Jeske, S. Ulbrich, J. Hofmann, J. Koblitz, I. Schomburg, M. Neumann-Schaal,
D. Jahn, and D. Schomburg, “BRENDA, the ELIXIR core data resource in 2021: new
developments and updates,” Nucleic Acids Research, vol. 49, no. D1, pp. D498–D508, 2020.

[50] J. Miller, L. Brad, and D. E. Goldberg, “Genetic algorithms, tournament selection, and the

eﬀects of noise.,” Complex Systems, vol. 9, no. 3, pp. 193–212, 1995.

[51] Willmott, C. J., and K. Matsuura, “Advantages of the mean absolute error (mae) over the
root mean square error (rmse) in assessing average model performance.,” Climate Research,
vol. 30, no. 1, pp. 79–82, 2005.

[52] Arlot, Sylvain, and A. Celisse, “A survey of cross-validation procedures for model selection,”

Statistics Surveys, vol. 4, pp. 40–79, 2010.

[53] G. D. Rose, A. R. Geselowitz, G. J. Lesser, R. H. Lee, and M. H. Zehfus, “Hydrophobicity
of amino acid residues in globular proteins,” Science, vol. 229, no. 4716, pp. 834–838, 1985.

[54] ICER, “HPCC Hardware Description,” 2022. https://icer.msu.edu/hpcc/hardware [Ac-

cessed: 02-04-2022].

[55] B. Wu, G. Warnock, M. Zaiss, C. Lin, M. Chen, Z. Zhou, L. Mu, D. Nanz, R. Tuura, and
G. Delso, “An overview of CEST MRI for non-MR physicists,” European Journal of Nuclear
Medicine and Molecular Imaging Physics, vol. 3, no. 1, 2016.

[56] C. T. Farrar, J. S. Buhrman, G. Liu, A. Kleijn, M. L. M. Lamfers, M. T. McMahon, A. A.
Gilad, and G. Fulci, “Establishing the lysine-rich protein CEST reporter gene as a CEST
MR imaging detector for oncolytic virotherapy,” Radiology, vol. 275, no. 3, pp. 746–754,
2015.

[57] A. Bricco, I. Miralavy, S. Bo, O. Perlman, C. T. Farrar, M. T. McMahon, W. Banzhaf, and
A. A. Gilad, “Protein Optimization Evolving Tool (POET) based on Genetic Programming,”
2022. Unpublished data.

23

