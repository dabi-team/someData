1
2
0
2

r
p
A
8

]

D
S
.
s
c
[

1
v
6
7
8
3
0
.
4
0
1
2
:
v
i
X
r
a

SerumRNN: Step by Step Audio VST
Eﬀect Programming

Christopher Mitcheltree1,2[0000−0002−2844−3650]
and Hideki Koike1[0000−0002−8989−6434]

1 Tokyo Institute of Technology, Tokyo 152-8550, Japan
christhetree@gmail.com
koike@c.titech.ac.jp
https://www.vogue.cs.titech.ac.jp/
2 Qosmo Inc, Tokyo 153-0051, Japan
https://qosmo.jp/

Abstract. Learning to program an audio production VST synthesizer is
a time consuming process, usually obtained through ineﬃcient trial and
error and only mastered after years of experience. As an educational and
creative tool for sound designers, we propose SerumRNN : a system that
provides step-by-step instructions for applying audio eﬀects to change
a user’s input audio towards a desired sound. We apply our system to
Xfer Records Serum: currently one of the most popular and complex
VST synthesizers used by the audio production community. Our results
indicate that SerumRNN is consistently able to provide useful feedback for
a variety of diﬀerent audio eﬀects and synthesizer presets. We demonstrate
the beneﬁts of using an iterative system and show that SerumRNN learns
to prioritize eﬀects and can discover more eﬃcient eﬀect order sequences
than a variety of baselines.

Keywords: Synthesizer Programming · Audio Eﬀects · VST · Sound
Design · Educational Machine Learning · Ensemble Modeling · Recurrent
Neural Networks · Convolutional Neural Networks.

1

Introduction and Background

Sound design is the process of using a synthesizer and audio eﬀects to create a
desired output sound, typically by leveraging virtual studio technology (VST) on
a computer. Often, the audio eﬀects applied to the synthesizer play the biggest
role in producing a desired sound. Sound design for the music industry is a very
diﬃcult task typically done by professionals with years of experience. Educational
tools are limited and beginners are usually forced to learn via trial and error or
from online resources created by others who typically also learned in a similar
way. This makes the learning curve for sound design very steep.

1.1 Serum

Serum is a powerful VST synthesizer made by Xfer Records [6] that can apply up
to 10 audio eﬀects to the audio it generates. Serum is currently one of the most

 
 
 
 
 
 
2

C. Mitcheltree and H. Koike

popular VST synthesizers in the audio production community and is routinely
used by hobbyists and professionals alike. We chose Serum because we wanted to
apply our research to a relevant, widely adopted, fully-featured synthesizer that
is challenging for humans to master and will therefore maximize the practicality
of our work.

1.2 Related Work

While applying AI to sound design is a relatively niche research area, there
has been some prior work on leveraging AI to program audio VSTs. K-means
clustering + tree-search [3] and evolutionary algorithms such as genetic algorithms
[17,19] and genetic programming [12] have been applied to this problem with
varying levels of success. Genetic algorithms have also been used to model audio
eﬀects directly [11]. However, these systems suﬀer from one or more of the
following problems:

– They are applied to toy VSTs with little practical use.
– They are incompatible with existing VST plugins.
– Their inference time is prohibitively long.
– They are black-boxes with uninterpretable results.

With the recent rise in deep learning and neural networks, there has also
been some related work using deep convolutional neural networks (CNNs) to
program audio VSTs [2,19]. InverSynth [2] is probably most similar to SerumRNN
since it also makes use of CNNs to program synthesizer parameters. However,
as mentioned previously, these neural systems approach the problem with a
one-shot, black-box process and focus more on synthesis rather than applying
eﬀects. This end-to-end approach replaces a user more than it augments them
and results in fewer opportunities to learn. Most users typically know how to
begin programming their desired sound via oscillator, attack, decay, sustain, and
release parameters, but have diﬃculty programming the relevant eﬀects due to
the sheer number of combinatorial possibilities.

Finally, there has also been research on using deep learning to model audio
eﬀects and / or applying it directly to raw audio [5,7,13,14]. These systems typi-
cally use interesting signal processing techniques and neural network architectures
from which we draw inspiration. However, they typically cannot be applied to
existing VST synthesizers making their usefulness for our goals limited.

Overall, we believe that when using an AI assisted system, the user’s sense
of ownership over their work should be preserved. As a result, SerumRNN is
inspired by step-by-step white-box automatic image post-processing systems [9]
and collaborative production tools [15,18] that can educate and augment a user
rather than aiming to replace them.

1.3 Contributions

In this paper we propose a system (SerumRNN ) that iteratively changes an input
audio towards the same timbre of a desired target audio by sequentially applying

SerumRNN: Step by Step Audio VST Eﬀect Programming

3

Fig. 1. Mel spectrogram progression of our system applying three eﬀects to a user’s
input audio.

audio eﬀects via the Serum VST synthesizer. As a result, the system provides a
sequence of interpretable intermediate steps. It uses, to the best of our knowledge,
a novel approach consisting of an ensemble of models working together: an eﬀect
selection model to determine which eﬀect to apply next to the input audio and
then a collection of eﬀect parameter models, one per supported eﬀect, to program
the selected eﬀect parameters. We demonstrate through extensive evaluation that
SerumRNN :

– Signiﬁcantly reduces the error between the input and target audio.
– Beneﬁts from applying eﬀects iteratively in a speciﬁc order.
– Learns which eﬀects are most important.
– Provides interpretable and valuable intermediate steps.
– Can discover more eﬃcient eﬀect order sequences than a variety of baselines.

An example of SerumRNN applying three steps to some input audio can be
seen in Figure 1. Audio examples can be listened to at https://bit.ly/serum_rnn.

2 Data Collection

Data collection and processing systems represent a signiﬁcant portion of the
software engineering required for our system. Training data for all models is
generated by rendering audio samples from Serum and then converting them
into spectrograms and cepstra. Due to the complexity of Serum and its eﬀects,
virtually inﬁnite amounts of training data can be generated.

2.1 Audio Rendering

Five commonly used and predominantly timbre altering eﬀects are chosen for
training data collection: multi-band compression, distortion, equalizer (EQ),
phaser, and hall reverb. Table 1 summarizes which Serum synthesizer parameters
are sampled for each supported eﬀect. Continuous parameters (knobs on the
Serum synthesizer) can be represented as ﬂoating-point numbers between zero
and one inclusively. Categorical parameters can be represented as one-hot vectors

4

C. Mitcheltree and H. Koike

Table 1. Parameters sampled from the Serum VST synthesizer.

Eﬀect

Parameter Name

Type

Sampled Values

[0.0, 1.0]
Continuous
Compressor Low-band Compression
[0.0, 1.0]
Continuous
Compressor Mid-band Compression
[0.0, 1.0]
Continuous
Compressor High-band Compression
Categorical 12 classes
Distortion Mode
[0.3, 1.0]
Continuous
Drive
Distortion
[0.50, 0.95]
High Frequency Cutoﬀ
Equalizer
Continuous
[0.0, 1.0]
High Frequency Resonance Continuous
Equalizer
[0.0, 0.4] and [0.6, 1.0]
Continuous
High Frequency Gain
Equalizer
[0.0, 1.0]
Continuous
LFO Depth
Phaser
[0.0, 1.0]
Continuous
Frequency
Phaser
[0.0, 1.0]
Continuous
Phaser
Feedback
[0.3, 0.7]
Continuous
Hall Reverb Mix
[0.0, 1.0]
Continuous
Hall Reverb Low Frequency Cutoﬀ
[0.0, 1.0]
Continuous
Hall Reverb High Frequency Cutoﬀ

of length C where C is the number of classes. Continuous parameter sampling
value ranges are occasionally limited to lie within practical, everyday use regions.
Furthermore, in order to apply the system to a variety of diﬀerent sounds, we
collect data from 12 diﬀerent synthesizer presets split into three groups (in increas-
ing order of complexity): Basic Shapes, Advanced Shapes, and Advanced Modulat-
ing Shapes. The Basic Shapes preset group consists of the single oscillator sine,
triangle, saw, and square wave default Serum presets. Next, the Advanced Shapes
preset group consists of the dry (no eﬀects) dual oscillator "LD Power 5ths", "SY
Mtron Saw", "SY Shot Dirt Stab", and "SY Vintage Bells" default Serum
presets. Finally, the Advanced Modulating Shapes preset group consists of the dry
dual oscillator "LD Iheardulike5ths", "LD Postmodern Talking", "SQ Busy
Lines", and "SY Runtheharm" default Serum presets. These ﬁnal four presets
also use intense modulations on top of their use of dual oscillators.

Since ﬁve diﬀerent eﬀects are supported by the system, there are 32 diﬀerent
combinations possible when applying a minimum of zero and a maximum of
ﬁve eﬀects to an input audio signal. For each of these combinations, a modiﬁed
automated VST rendering tool [8] is used to render and save 4000 mono audio
clips. Parameters are sampled randomly and uniquely from the permissible values
shown in Table 1 and eﬀect order when multiple eﬀects are used is randomized.
All audio samples are played and rendered for one second using a MIDI pitch
of C4, maximum note velocity, and a sampling rate of 44100 Hz. This process
is repeated for each of the 12 presets resulting in 128k audio clips per preset,
384k audio clips per preset group, and 1.536M audio clips total. We only sample
a single C4 pitch due to the inclusion of high quality timbre-preserving pitch
shifting algorithms in all commonly used digital audio workstations, thus allowing
users to easily warp a desired sound they would like to use as input to the system
to C4. This preprocessing step could also be done by the system automatically.

SerumRNN: Step by Step Audio VST Eﬀect Programming

5

2.2 Audio Processing

Audio clips are converted to Mel spectrograms using a Short-time Fourier trans-
form (STFT) with a hop length of 512 samples, a fast Fourier transform (FFT)
window length of 2048, a minimum and maximum frequency of 20 Hz and 16 kHz
respectively, and a Mel ﬁlter-bank of size 128. For each spectrogram the ﬁrst 30
Mel-frequency cepstral coeﬃcients (MFCCs), representing the smoothed spectral
envelope, are also computed and saved. Lastly, each spectrogram is converted
to decibels (dB). We use both spectrograms and cepstra in order to provide
the subsequent neural networks as much volume, pitch, timbre, and temporal
information as possible. Early prototypes of the system using only spectral or
cepstral features beneﬁted from including both.

Fig. 2. System diagram: model components are dashed rectangles, inputs are blue,
outputs are red, and the Serum synthesizer is green. Dotted arrows represent the
conceptual ﬂow of the system and solid arrows represent the ﬂow of tensors.

3 Modeling

Our system consists of an ensemble of models that work together to take iterative
steps to transform some input audio towards the timbre of some target audio.
First, an eﬀect selection model determines which eﬀect to apply next and then
a collection of ﬁve eﬀect parameter models, one per supported eﬀect, program
the synthesizer accordingly. Figure 2 depicts a diagram of the entire system
performing one step.

3.1 Eﬀect Parameter Models

The output values of eﬀect parameter models are used to program the parameters
of an eﬀect. They take two tensors as input: a Mel spectrogram and the corre-

6

C. Mitcheltree and H. Koike

Fig. 3. Distortion eﬀect parameter model architecture (not to scale).

sponding ﬁrst 30 MFCCs. Both of these tensors consist of two channels, the ﬁrst
representing the input audio and the second representing the target audio. The
MFCC tensor is normalized by feeding it through a batch normalization layer
that has learnable shift and scale parameters. Two almost identical convolutional
neural networks (CNNs) are used to extract features from each input. The Mel
spectrogram CNN consists of three convolutional layers each with a 3x3 stride,
4x4 max-pool layer, and 64, 128, and 128 ﬁlters respectively. The MFCCs CNN
is identical except, due to the smaller X input dimension, it uses 2x4 max-pool
layers. The ﬁnal max-pool layers of the two CNNs are ﬂattened and concatenated
together. This is then followed by two 128-dimensional fully connected (FC)
layers with a dropout rate [16] of 50% each. All layers use ELU activations [4].
The output layers and loss functions of an eﬀect parameter model vary
depending on what eﬀect is being modeled. Continuous parameters of an eﬀect,
where N is the number of continuous parameters, are represented as a single N -
dimensional fully connected layer with a linear activation. Categorical parameters
of an eﬀect, where C is the number of classes, are each represented as a C-
dimensional fully connected layer with a softmax activation. All output layers
are connected to the second fully connected layer.

As a result, the eﬀect parameter models for the compressor, EQ, phaser, and
hall reverb eﬀects have a single 3-dimensional continuous output layer and the
distortion model has two output layers: a 12-dimensional categorical output layer
and a 1-dimensional continuous output layer. The architecture of the distortion
eﬀect parameter model is shown in Figure 3.

3.2 Eﬀect Selection Model

The eﬀect selection model outputs a probability distribution of what eﬀect should
be applied next to the input audio. As shown in Figure 2, it takes as input
three sequences of the same length corresponding to the number of steps that
have been taken by the system. The ﬁrst two are sequences consisting of eﬀect

SerumRNN: Step by Step Audio VST Eﬀect Programming

7

parameter model inputs: Mel spectrograms and MFCCs. The third is a sequence
of 6-dimensional one-hot vectors representing which eﬀect was applied to the input
audio at each step. The ﬁrst ﬁve dimensions of the one-hot vector correspond to
the ﬁve supported eﬀects and the last dimension indicates the initial step of the
system when no eﬀect has been applied yet to the input audio.

Features are extracted from the Mel spectrogram and MFCC sequences using
time distributed CNNs identical in architecture to the ones in the eﬀect parameter
models, except each convolutional layer uses only half as many ﬁlters and there is
only one fully connected layer which is the output. The extracted feature vectors
are then concatenated with the one-hot vector eﬀect sequence and are then fed
through a 128-dimensional bi-directional long short-term memory (LSTM) layer.
This is followed by a 128-dimensional fully connected layer with an ELU activation
and 50% dropout applied to it, and lastly a 5-dimensional fully connected output
layer with a softmax activation.

3.3 Training

One eﬀect parameter model is trained per eﬀect for each preset group (therefore
15 in total) on the Cartesian product of all possible input audio and target audio
Mel spectrogram and MFCC pairs. This results in each model being trained
on approximately 1.2M data points. Categorical output layers are trained with
categorical cross-entropy loss and continuous output layers are trained with mean
squared error loss. Multiple losses due to multiple output layers, such as for the
distortion eﬀect parameter model, are weighted equally and a batch size of 128 is
used.

One eﬀect selection model is trained for each preset group (therefore three
in total) on unique sequences consisting of one to ﬁve steps (applied eﬀects)
generated randomly from the available audio clip Mel spectrograms and MFCCs.
To encourage eﬃciency, each eﬀect can only be applied once to the input audio.
Consequently, the shortest generated sequence is of length two since this represents
the original input audio and one eﬀect being applied and similarly, the longest
generated sequence is of length six which represents the original input audio and
the ﬁve supported eﬀects being applied. Approximately 500k data points are
generated for each preset group eﬀect selection model. A categorical cross-entropy
loss is used for training along with a batch size of 32.

All models are trained for 100 epochs with early stopping (8 epochs of patience)
and a validation and test split of 0.10 and 0.05 respectively. The Adam optimizer
[10] is used with a learning rate of 0.001.

4 Evaluation

We evaluate our system at three levels of granularity: the eﬀect parameter models,
the eﬀect selection models, and the entire system / ensemble of models as a whole.
Each of the three preset groups are evaluated separately and yield very similar
and consistent results. We display detailed results for the most challenging preset

8

C. Mitcheltree and H. Koike

group, Advanced Modulating Shapes, and place the ﬁgures and tables for the
other two preset groups in the Appendix. Since understanding how perceptually
close two audio samples are from metrics alone is diﬃcult, audio examples for
SerumRNN can be found at https://bit.ly/serum_rnn.

4.1 Audio Similarity Metrics

In order to eﬀectively evaluate our system, the distance between two audio clips
needs to be computed; speciﬁcally the similarity between their timbres. We make
use of a variety of six diﬀerent metrics to evaluate our models vigorously.

The ﬁrst three metrics are quite basic: the mean squared error (MSE), mean
absolute error (MAE), and log-spectral distance (LSD) between two magnitude
spectrograms. LSD is deﬁned as the average root mean square diﬀerence between
each frame of two log power spectrograms.

M SE =

1
mn

m
(cid:88)

n
(cid:88)

(Sij − ˆSij)2

i=1

j=1

M AE =

1
mn

m
(cid:88)

n
(cid:88)

i=1

j=1

| Sij − ˆSij |

LSD =

1
n

n
(cid:88)

i=1

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
m

m
(cid:88)

j=1

(Sij − ˆSij)2

(1)

(2)

(3)

S is an output decibel Mel spectrogram matrix, ˆS is the target decibel Mel
spectrogram matrix, m is the number of Mel ﬁlters, and n is the number of
frames.

The fourth metric (MFCCD) is the mean Euclidean distance between the
ﬁrst 30 MFCCs which represent the smoothed spectral envelope of an audio clip.

M F CCD =

1
n

n
(cid:88)

i=1

(cid:107)Ci − ˆCi(cid:107)2

(4)

C is a matrix of the ﬁrst 30 output audio MFCCs for each frame, ˆC is a matrix of
the ﬁrst 30 target audio MFCCs for each frame, and n is the number of frames.
The Pearson Correlation Coeﬃcient (PCC) can also be used to measure audio

reconstruction quality [2] and is deﬁned as

P CC =

cov(x, y)
σxσy

(5)

where x and y are 1-dimensional vectors, not matrices. We apply this metric
to decibel Mel spectrogram matrices S and ˆS by ﬂattening each one via row
concatenation.

SerumRNN: Step by Step Audio VST Eﬀect Programming

9

Our last and most robust metric is based oﬀ a multi-scale spectral loss [7]
that we have modiﬁed and we call it multi-scale spectral mean absolute error
(MSSMAE). Given an output audio clip and a target audio clip, magnitude
spectrograms (Si and ˆSi respectively) are calculated for several diﬀerent FFT
sizes i. We then also compute log Si and log ˆSi and deﬁne the metric as

M SSM AE =

1
w

(cid:88)

(M AE(Si, ˆSi) + αM AE(log Si, log ˆSi))

(6)

i

where w is the number of diﬀerent FFT sizes, α is a constant scaling term,
and MAE is deﬁned in Equation 2 as the mean squared error between two
spectrograms. We set α to 0.1 in our experiments to make the two MAE values
roughly equal since the log values are generally around one order of magnitude
larger. We use FFT sizes of (64, 128, 256, 512, 1024, 2048) making w = 6 and the
neighboring frames in the resulting spectrograms have 75% overlap. By calculating
multiple regular and logarithmic spectrograms with diﬀerent FFT sizes, two audio
clips can be compared along diﬀerent spatial-temporal resolutions.

For the MSE, MAE, LSD, MFCCD, and MSSMAE metrics, lower values
indicate a higher similarity between two audio clips whereas the opposite is true
for PCC values. MSE, MAE, PCC, and MSSMAE values are multiplied by 100
for better readability.

4.2 Eﬀect Parameter Models

Eﬀect parameter models are evaluated using the six metrics deﬁned in Section
4.1 on 1000 input and target audio pairs from their corresponding test data sets.
We observe from Table 2 that for almost every metric and eﬀect, the models
substantially decrease the distance between the input audio clip and the target
audio clip.

*It is worth noting that this trend appears to be less dramatic for the Phaser
eﬀect, but can be explained by the fact that spectrograms and cepstra do not
represent phase information well [7]. As a result, our error metrics might be less
representative of the Phaser eﬀect when compared to the other eﬀects.

4.3 Eﬀect Selection Model

We compare the eﬀect selection model, which we call SerumRNN, against two
baselines to showcase the advantage of 1: using an iterative system and 2:
considering the order of eﬀects.

The ﬁrst baseline is a non-iterative version of the eﬀect selection model
(called SerumRNN NI ) that uses an identical architecture and training process
except that only the ﬁrst step (the original input and target audio) of the Mel
spectrogram and MFCC input sequences is used. As a result, the sequence
of eﬀects to be applied to the input audio can be predicted all at once using
SerumRNN NI without iteratively modifying the audio in between steps using
the eﬀect parameter models.

10

C. Mitcheltree and H. Koike

Table 2. Eﬀect parameter models eval. metrics (Adv. Mod. Shapes preset group).

Eﬀect

Metric

Input Audio Output Audio

∆

∆%

Mean Error against Target Audio

Compressor MSE
MAE
LSD
MFCCD
PCC
MSSMAE

Distortion MSE
MAE
LSD
MFCCD
PCC
MSSMAE

Equalizer

Phaser *

MSE
MAE
LSD
MFCCD
PCC
MSSMAE

MSE
MAE
LSD
MFCCD
PCC
MSSMAE

Hall Reverb MSE
MAE
LSD
MFCCD
PCC
MSSMAE

1.86
10.50
10.04
108.53
82.01
71.64

4.49
14.57
13.90
146.04
74.67
94.33

1.48
8.76
8.64
92.52
84.90
64.05

0.86
6.89
7.06
72.50
86.05
51.53

1.28
8.32
8.32
80.49
83.87
60.87

0.82
6.43
6.52

-1.04 -56.06
-4.07 -38.75
-3.53 -35.10
64.48 -44.05 -40.59
5.90
4.84
86.85
45.57 -26.07 -36.39

0.78
6.21
6.41

-3.71 -82.66
-8.36 -57.40
-7.50 -53.92
59.76 -86.29 -59.08
80.42
7.70
5.75
65.35 -28.97 -30.72

0.80
6.29
6.49

-0.68 -45.92
-2.47 -28.24
-2.14 -24.82
64.17 -28.35 -30.64
1.82
1.55
86.45
0.78
0.50
64.55

0.77
6.31
6.59
64.42
85.58
47.33

-0.09 -10.15
-8.44
-0.58
-0.47
-6.67
-8.08 -11.14
-0.55
-0.47
-8.16
-4.20

0.48
4.89
5.04

-0.80 -62.60
-3.44 -41.28
-3.28 -39.43
46.21 -34.28 -42.59
89.47
6.68
5.60
38.56 -22.31 -36.65

Table 3. Eﬀect selection models prediction accuracy (Adv. Mod. Shapes preset group).

Eﬀect Selection Model

SerumRNN
SerumRNN NI
SerumCNN

Step Number (Number of Eﬀects for SerumCNN )
All

4

2

3

5

1

0.994 0.984 0.983 0.979
0.992
0.993

0.999
0.962 1.000
0.889
0.916

0.961
0.946

0.974
0.975

0.985
0.971
0.955

SerumRNN: Step by Step Audio VST Eﬀect Programming

11

The second baseline is the eﬀect parameter model CNN with a 5-dimensional
fully connected output layer and a sigmoid activation. It is trained with a binary
cross-entropy loss to predict all eﬀects at once without considering order. We call
this baseline SerumCNN.

The accuracy of all three models for eﬀect sequences of length one to ﬁve
is measured using their test data sets of approximately 25k data points and is
summarized in Table 3. We observe that all three models perform remarkably
well, with SerumRNN outperforming the other two for virtually all cases. Since
the RNN models predict eﬀects one by one, we believe accuracy is highest for the
ﬁrst and last step due to the ﬁrst step representing the most dramatic diﬀerence
between input and target audio clips and the last step being deducible given
the previous eﬀects since repeated eﬀects are not supported. The behavior of
SerumCNN is also expected: accuracy decreases the longer the eﬀect sequence is
since more eﬀects need to be predicted correctly.

4.4 Ensemble / Entire System

Similarly to the eﬀect parameter models, we evaluate the entire ensemble of
models (SerumRNN ) using the six metrics deﬁned in Section 4.1 on 1000 input
and target audio pairs. These pairs are sampled from the unseen test sets and
the target audio is the result of a sequence of one to ﬁve eﬀects (200 audio pairs
per sequence length) being applied to the input audio. Inference time for the
entire system (audio processing, eﬀect selection, parameter programming, and
audio rendering) is approximately 300 ms per eﬀect applied to the input audio.

Baselines We compare SerumRNN against ﬁve baselines. The ﬁrst two are
SerumRNN NI and SerumCNN from Section 4.3 which allow us to test for
any gains from using an iterative system and considering the order of eﬀects,
respectively. The next baseline is a perfect eﬀect selection model (Oracle) that
always selects the "correct" next eﬀect as is deﬁned by the eﬀect sequence that
created the target audio. This allows us to compare our systems to what should
be the technically "correct" solution. In order to test the beneﬁt of using an
eﬀect selection model in the ﬁrst place, we also have a Random baseline which
simply chooses a random eﬀect each time it is called. Finally, to test the beneﬁt
of applying the eﬀect parameter models iteratively, we include a baseline called
SerumCNN 1S. This baseline is identical to the SerumCNN baseline except
that the eﬀect parameter models are applied all at once to the input audio
(after all eﬀects have been predicted at once by SerumCNN ). This results in a
one-shot system with the fastest inference time possible, but no intermediate
steps. This baseline is also the most similar to existing neural network synthesizer
programming architectures of prior works in related literature [2,19].

All six systems use the same ﬁve eﬀect parameter models for each preset group.
Since repeated eﬀects are not supported, if an eﬀect selection model chooses an
eﬀect that has already been applied in a previous step, the next most probable
unused eﬀect is selected. As a result, all systems terminate at the latest after
applying the maximum of ﬁve supported eﬀects.

12

C. Mitcheltree and H. Koike

∆

r
o
r
r
E

E
A
M
S
S
M
n
a
e
M

−20

−25

−30

SerumRNN
SerumRNN NI
SerumCNN
Random

0

1

2

3

4

5

Number of "Mistake" Steps Tolerated

Fig. 4. Relationship between the tolerated number of mistake steps vs. the overall
error ∆ against the target audio (Adv. Mod. Shapes preset group).

Stopping Condition Before the systems can be evaluated, a stopping condition
must be deﬁned for SerumRNN, SerumRNN NI, SerumCNN, and Random.
Otherwise ﬁve eﬀects will always be applied to the input audio, even when the
target audio was created using only one eﬀect. We believe a reasonable stopping
condition is terminating the system once it makes a mistake, i.e. when the distance
between the input and target audio increases. However, there may be situations
where making a "mistake" is advantageous since the ideal sequence of steps to
the target audio can be non-monotonic. Therefore, we investigate in Figure 4 how
the number of tolerated bad steps aﬀects the resulting overall error reduction of
each system. From the plotted results we can conclude that the ideal stopping
condition is tolerating one step in the wrong direction before terminating.

Discussion Table 4 summarizes the evaluation results of SerumRNN compared
to the ﬁve baselines and Table 5 gives more granular evaluation details for
SerumRNN and the Advanced Modulating Shapes preset group.

All systems signiﬁcantly reduce the error between the input and target audio
which indicates that the eﬀect parameter models individually are robust and
highly eﬀective, even when applied in a random order. We believe using two
channel Mel spectrogram and MFCC inputs and training on the Cartesian product
of possible input and target audio combinations (as explained in Section 3.1)
helped achieve this robustness.

Perhaps unsurprisingly, SerumRNN consistently outperforms SerumRNN NI,
SerumCNN, and SerumCNN 1S for all metrics. This highlights the importance
of considering the order of eﬀects and using an iterative approach for both
eﬀect selection and eﬀect parameter programming. Performance of the one-shot
SerumCNN 1S system is also quite good considering it is up to ﬁve times
faster than any of the other systems. If inference speed is critically important, a
reasonable compromise can be made by choosing to use this system instead.

SerumRNN: Step by Step Audio VST Eﬀect Programming

13

We also note for SerumRNN that all ﬁve steps reduce the error between the
input and target audio clips, with the largest gains achieved in the ﬁrst two
steps and the smallest in the ﬁnal two steps. In contrast to this, for the Random
baseline (Table 6) error reduction is distributed essentially evenly between the
ﬁve steps. This indicates that the eﬀect selection model chooses the eﬀects in
order of importance for multi-eﬀect sequences, thus making the intermediate
steps of SerumRNN more educational for the user.

The most surprising result is that SerumRNN consistently outperforms the
Oracle baseline, especially for the most robust MSSMAE metric. This implies
that SerumRNN sometimes chooses eﬀects in an order that is more optimal for
the eﬀect parameter models than using the original eﬀect order that created
the target audio. We ﬁnd this result exciting because it suggests our neural
architecture and ensemble of models is able to extrapolate and discover solutions
outside of the scope of what would be considered "correct" by the training data.

From an auditory point of view, the reduction in error for each step of
SerumRNN can be heard very clearly with the ﬁnal output audio often being
indistinguishable from the target audio. We highly recommend listening to the
audio examples at https://bit.ly/serum_rnn.

Table 4. Comparison of overall mean error reduction for all systems.

Preset Group System

Mean Error ∆ against Target Audio
MSE MAE LSD MFCCD PCC MSSMAE

Advanced
Modulating
Shapes

Advanced
Shapes

Oracle
SerumRNN
SerumRNN NI
SerumCNN
Random
SerumCNN 1S

Oracle
SerumRNN
SerumRNN NI
SerumCNN
Random
SerumCNN 1S

Basic Shapes Oracle

SerumRNN
SerumRNN NI
SerumCNN
Random
SerumCNN 1S

-4.02 -9.45 -8.48
-8.36
-9.41
-4.04
-8.25
-9.20
-4.02
-8.18
-9.21
-3.97
-6.52
-7.45
-3.33
-8.10
-9.12
-3.96

-2.96 -8.09 -7.21
-7.17
-8.08
-2.97
-7.07
-7.98
-2.96
-7.05
-7.84
-2.92
-5.71
-6.39
-2.48
-6.83
-7.65
-2.88

-4.00
-8.19
-9.49
-4.08 -9.62 -8.35
-7.75
-9.02
-3.79
-7.25
-8.50
-3.45
-6.21
-7.20
-3.14
-7.36
-8.52
-3.56

14.80
-97.14
-96.17 15.53
15.26
-94.55
14.71
-93.72
11.96
-74.40
14.51
-92.68

-83.01
15.90
-83.34 16.87
16.79
-81.80
15.84
-81.21
14.20
-65.95
15.55
-78.40

-92.12
18.78
-94.08 20.03
19.23
-86.36
19.32
-81.01
15.14
-69.70
18.93
-82.60

-28.67
-32.53
-30.06
-28.84
-25.55
-28.50

-37.74
-41.24
-39.14
-38.89
-35.18
-37.80

-41.16
-42.32
-40.67
-41.95
-35.30
-40.06

14

C. Mitcheltree and H. Koike

Table 5. Eval. metrics for SerumRNN (Adv. Mod. Shapes preset group).

Mean Error against Target

Metric

Init. Final

∆

∆%

Mean Error ∆ per Step
4
1

3

2

5

MSE
MAE
LSD
MFCCD
PCC
MSSMAE

0.94
7.06
7.23

4.97
16.47
15.59

-4.04 -81.19
-9.41 -57.11
-8.36 -53.63
167.52 71.35 -96.17 -57.41
68.48 84.01
22.68
83.38 50.85 -32.53 -39.02

15.53

-0.86
-2.30
-1.95

-2.64
-5.49
-5.04

-0.38 -0.16 -0.10
-1.13 -0.48 -0.33
-0.97 -0.41 -0.27
-56.37 -23.26 -11.41 -5.12 -3.19
2.37
1.53
1.33
-9.47 -3.81 -2.23

5.81
6.23
-3.70 -15.63

Table 6. Evaluation metrics for the Random baseline (Adv. Mod. Shapes preset group).

Mean Error against Target

Metric

Init. Final

∆

∆%

1

Mean Error ∆ per Step
4

3

2

5

MSE
MAE
LSD
MFCCD
PCC
MSSMAE

1.66
9.05
9.10

4.99
16.49
15.62

-3.33 -66.71
-7.45 -45.15
-6.52 -41.72
167.81 93.41 -74.40 -44.34
68.24 80.20
17.52
83.16 57.60 -25.55 -30.73

11.96

-0.95
-2.00
-1.74

-0.84
-1.70
-1.47

-0.69
-0.85
-1.89
-1.98
-1.69
-1.78
-16.85 -19.98 -19.84 -19.26 -19.53
4.15
3.85
-8.74
-7.63

-0.81
-1.99
-1.70

1.73
-4.04

3.72
-6.34

4.00
-5.91

5 Conclusion

Overall, SerumRNN is consistently able to produce intermediate steps that bring
the input audio signiﬁcantly closer to the target audio while using a fully featured,
industry leading synthesizer VST plugin. SerumRNN also provides near real-time,
quantitative feedback about which eﬀects are the most important. In addition
to this, it can also ﬁnd eﬀect sequence orders that are potentially more optimal
than what the target audio was originally created with. With SerumRNN the
user can pick and choose which intermediate steps they would like to use and
can feed tweaked versions back into the system for additional ﬁne-tuning or to
learn more. We also noticed fun creative applications when our system produced
unexpected results or was given signiﬁcantly out of domain target audio.

Currently, the training data for SerumRNN is sampled randomly and thus
many of the generated sounds are not useful musically. As future work we would
like to improve the eﬃciency of this using adversarial techniques. We would also
like to evaluate the system’s usefulness and perceived audio similarity with a
user study and plan on packaging SerumRNN into a Max for Live [1] plugin to
make it easily accessible to anyone learning sound design.

We believe our research is just the tip of the iceberg for building AI powered
sound design tools. We can imagine a future where tools like ours might be able
to ﬁnd more eﬃcient and simpler methods of creating sounds, thus educating
students more eﬀectively and democratizing sound design.

SerumRNN: Step by Step Audio VST Eﬀect Programming

15

References

1. Ableton: Max for Live | Ableton (2020 (accessed November 19, 2020)), https:

//www.ableton.com/en/live/max-for-live/

2. Barkan, O., Tsiris, D., Katz, O., Koenigstein, N.: Inversynth: Deep estimation
of synthesizer parameter conﬁgurations from audio signals. IEEE/ACM Trans-
actions on Audio, Speech, and Language Processing 27(12), 2385–2396 (2019).
https://doi.org/10.1109/TASLP.2019.2944568

3. Cáceres, J.P.: Sound design learning for frequency modulation synthesis parameters

(2007)

4. Clevert, D.A., Unterthiner, T., Hochreiter, S.: Fast and accurate deep network

learning by exponential linear units (elus). CoRR abs/1511.07289 (2016)

5. Damskägg, E.P., Juvela, L., Thuillier, E., Välimäki, V.: Deep learning for tube am-
pliﬁer emulation. ICASSP 2019 - 2019 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP) pp. 471–475 (2019)

6. Duda, S.: Serum: Advanced Wavetable Synthesizer - Xfer Records (2020 (accessed

November 19, 2020)), https://xferrecords.com/products/serum

7. Engel, J., Hantrakul, L.H., Gu, C., Roberts, A.: Ddsp: Diﬀerentiable digital signal
processing. In: International Conference on Learning Representations (2020), https:
//openreview.net/forum?id=B1x1ma4tDr

8. Fedden, L.: RenderMan (2018 (accessed November 19, 2020)), https://github.com/

fedden/RenderMan

9. Hu, Y., He, H., Xu, C., Wang, B., Lin, S.: Exposure: A white-box
photo post-processing framework. ACM Trans. Graph. 37(2) (May 2018).
https://doi.org/10.1145/3181974, https://doi.org/10.1145/3181974

10. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. CoRR

abs/1412.6980 (2015)

11. Ly, E., Villegas, J.: Genetic reverb: Synthesizing artiﬁcial reverberant ﬁelds via

genetic algorithms. In: EvoMUSART (2020)

12. Macret, M., Pasquier, P.: Automatic design of sound synthesizers as pure data
patches using coevolutionary mixed-typed cartesian genetic programming. In: Pro-
ceedings of the 2014 Annual Conference on Genetic and Evolutionary Computation.
p. 309–316. GECCO ’14 (2014), https://doi.org/10.1145/2576768.2598303

13. Ramírez, M.A.M., Reiss, J.: End-to-end equalization with convolutional neural
networks. In: Proceedings of the 21st International Conference on Digital Audio
Eﬀects (DAFx-18) (2018)

14. Sheng, D., Fazekas, G.: A feature learning siamese model for intelligent control
of the dynamic range compressor. 2019 International Joint Conference on Neural
Networks (IJCNN) pp. 1–8 (2019)

15. Sommer, N., Ralescu, A.: Developing a machine learning approach to controlling

musical synthesizer parameters in real-time live performance. In: MAICS (2014)

16. Srivastava, N., Hinton, G.E., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.:
Dropout: a simple way to prevent neural networks from overﬁtting. J. Mach. Learn.
Res. 15, 1929–1958 (2014)

17. Tatar, K., Macret, M., Pasquier, P.: Automatic synthesizer preset generation with

presetgen. Journal of New Music Research 45, 124 – 144 (2016)

18. Thio, V., Donahue, C.: Neural loops. In: NeurIPS Workshop on Machine Learning

for Creativity and Design (2019)

19. Yee-King, M.J., Fedden, L., d’Inverno, M.: Automatic programming of vst
sound synthesizers using deep networks and other techniques. IEEE Transac-
tions on Emerging Topics in Computational Intelligence 2(2), 150–159 (2018).
https://doi.org/10.1109/TETCI.2017.2783885

16

C. Mitcheltree and H. Koike

A Appendix

Table 7. Eﬀect selection models prediction accuracy (Advanced Shapes and
Basic Shapes preset groups).

Preset Group

Eﬀect Sel. Model

Advanced Shapes SerumRNN

Basic Shapes

SerumRNN NI
SerumCNN

SerumRNN
SerumRNN NI
SerumCNN

Step Number (No. of Eﬀects for SerumCNN )
All

1

5

3

2

4

0.993 0.983 0.981 0.986
0.990
0.992

0.959
0.941

0.968
0.968

0.966 1.000
0.909
0.921

0.994 0.985
0.969
0.952

0.992 0.983 0.983 0.974 1.000 0.983
0.969
0.991
0.947
0.990

0.971 1.000
0.884
0.913

0.970
0.963

0.954
0.936

Table 8. Evaluation metrics for SerumRNN (Advanced Shapes preset group).

Mean Error against Target

Metric

Init. Final

∆

∆%

Mean Error ∆ per Step
4
1

3

2

5

MSE
MAE
LSD
MFCCD
PCC
MSSMAE

0.95
7.20
7.27

3.92
15.28
14.44

-2.97 -75.80
-8.08 -52.87
-7.17 -49.64
153.24 69.90 -83.34 -54.38
64.26 81.12
26.25
95.76 54.52 -41.24 -43.07

16.87

-0.76
-2.01
-1.68

-1.74
-4.48
-4.12

-0.35 -0.14 -0.06
-1.13 -0.51 -0.20
-0.96 -0.45 -0.17
-46.05 -20.51 -11.57 -5.89 -2.34
2.32
1.48
1.75
-5.82 -4.16 -2.29

5.63
-19.55 -11.00

7.59

Table 9. Evaluation metrics for SerumRNN (Basic Shapes preset group).

Mean Error against Target

Metric

Init. Final

∆

∆%

Mean Error ∆ per Step
4
1

3

2

5

MSE
MAE
LSD
MFCCD
PCC
MSSMAE

1.16
7.20
7.34

5.25
16.82
15.69

-4.08 -77.83
-9.62 -57.19
-8.35 -53.25
166.76 72.68 -94.08 -56.42
65.97 86.00
30.37
87.18 44.85 -42.32 -48.55

20.03

-0.93
-2.23
-1.86

-2.58
-5.80
-5.09

-0.39 -0.21 -0.08
-1.05 -0.66 -0.16
-0.92 -0.57 -0.18
-55.78 -22.05 -10.78 -6.57 -2.03
2.06
13.02
1.19
3.33
1.76
-3.46 -3.36 -1.95
-21.67 -13.33

