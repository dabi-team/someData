9
1
0
2

t
c
O
0
3

]

G
L
.
s
c
[

1
v
7
4
1
4
1
.
0
1
9
1
:
v
i
X
r
a

A Uniﬁed Framework for Data Poisoning Attack to
Graph-based Semi-supervised Learning

Xuanqing Liu
Department of Computer Science
UCLA
xqliu@cs.ucla.edu

Si Si
Google Research
sisidaisy@google.com

Xiaojin Zhu
Department of Computer Science
University of Wisconsin-Madison
jerryzhu@cs.wisc.edu

Yang Li
Google Research
liyang@google.com

Cho-Jui Hsieh
Department of Computer Science
UCLA
chohsieh@cs.ucla.edu

Abstract

In this paper, we proposed a general framework for data poisoning attacks to graph-
based semi-supervised learning (G-SSL). In this framework, we ﬁrst unify different
tasks, goals and constraints into a single formula for data poisoning attack in G-
SSL, then we propose two specialized algorithms to efﬁciently solve two important
cases — poisoning regression tasks under (cid:96)2-norm constraint and classiﬁcation
tasks under (cid:96)0-norm constraint. In the former case, we transform it into a non-
convex trust region problem and show that our gradient-based algorithm with
delicate initialization and update scheme ﬁnds the (globally) optimal perturbation.
For the latter case, although it is an NP-hard integer programming problem, we
propose a probabilistic solver that works much better than the classical greedy
method. Lastly, we test our framework on real datasets and evaluate the robustness
of G-SSL algorithms. For instance, on the MNIST binary classiﬁcation problem
(50000 training data with 50 labeled), ﬂipping two labeled data is enough to make
the model perform like random guess (around 50% error).

1

Introduction

Driven by the hardness of labeling work, graph-based semi-supervised learning (G-SSL) [1, 2, 3]
has been widely used to boost the quality of models using easily accessible unlabeled data. The core
idea behind it is that both labeled and unlabeled data coexist in the same manifold. For instance,
in the transductive setting, we have label propagation [1] that transfers the label information from
labeled nodes to neighboring nodes according to their proximity. While in the inductive case, a
graph-based manifold regularizer can be added to many existing supervised learning models to
enforce the smoothness of predictions on the data manifold [4, 5]. G-SSL has received a lot of
attention; many of the applications are safety-critical such as drug discovery [6] and social media
mining [7].

We aim to develop systematic and efﬁcient data poisoning methods for poisoning G-SSL models.
Our idea is partially motivated by the recent researches on the robustness of machine learning models
to adversarial examples [8, 9]. These works mostly show that carefully designed, slightly perturbed
inputs – also known as adversarial examples – can substantially degrade the performance of many
machine learning models. We would like to tell apart this problem from our setting: adversarial
attacks are performed during the testing phase and applied to test data [10, 11, 12, 13, 14, 15], whereas
data poisoning attack is conducted during training phase [16, 17, 18, 19, 20], and perturbations are

33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.

 
 
 
 
 
 
added to training data only. In other words, data poisoning attack concerns about how to imperceptibly
change the training data to affect testing performance. As we can imagine, this setting is more
challenging than testing time adversarial attacks due to the hardness of propagating information
through a sophisticated training algorithm.

Despite the efforts made on studying poisoning attack to supervised models [16, 17, 18, 19, 20], the
robustness of semi-supervised algorithms has seldom been studied and many related questions remain
unsolved. For instance, are semi-supervised learning algorithms sensitive to small perturbation of
labels? And how do we formally measure the robustness of these algorithms?

In this paper, we initiate the ﬁrst systematic study of data poisoning attacks against G-SSL. We mainly
cover the widely used label propagation algorithm, but similar ideas can be applied to poisoning
manifold regularization based SSL as well (see Appendix 4.2). To poison semi-supervised learning
algorithms, we can either change the training labels or features. For label poisoning, we show it
is a constrained quadratic minimization problem, and depending on whether it is a regression or
classiﬁcation task, we can take a continuous or discrete optimization method. For feature poisoning,
we conduct gradient-based optimization with group Lasso regularization to enforce group sparsity
(shown in Appendix 4.2). Using the proposed algorithms, we answer the questions mentioned above
with several experiments. Our contributions can be summarized as follows:

We propose a framework for data poisoning attack to G-SSL that 1) includes both classiﬁcation
and regression cases, 2) works under various kinds of constraints, and 3) assumes both complete
and incomplete knowledge of algorithm user (also called “victim”).
For label poisoning to regression task, which is a nonconvex trust region problem, we design a
specialized solver that can ﬁnd a global minimum in asymptotically linear time.
For label poisoning attack to classiﬁcation task, which is an NP-hard integer programming problem,
we propose a novel probabilistic solver that works in combination with gradient descent optimizer.
Empirical results show that our method works much better than classical greedy methods.
We design comprehensive experiments using the proposed poisoning algorithms on a variety of
problems and datasets.

•

•

•

•

In what follows, we refer to the party running poisoning algorithm as the attacker, and the party doing
the learning and inference work as the victim.

2 Related Work

Adversarial attacks have been extensively studied recently. Many recent works consider the test
time attack, where the model is ﬁxed, and the attacker slightly perturbs a testing example to change
the model output completely [9]. We often formulate the attacking process as an optimization
problem [10], which can be solved in the white-box setting. In this paper, we consider a different
area called data poisoning attack, where we run the attack during training time — an attacker can
carefully modify (or add/remove) data in the training set so that the model trained on the poisoned
data either has signiﬁcantly degraded performance [18, 16] or has some desired properties [21, 19].
As we mentioned, this is usually harder than test time attacks since the model is not predetermined.
Poisoning attacks have been studied in several applications, including multi-task learning [20], image
classiﬁcation [21], matrix factorization for recommendation systems [19] and online learning [22].
However, they did not include semi-supervised learning, and the resulting algorithms are quite
different from us.

To the best of our knowledge, [23, 24, 25] are the only related works on attacking semi-supervised
learning models. They conduct test time attacks to Graph Convolutional Network (GCN). In
summary, their contributions are different from us in several aspects: 1) the GCN algorithm is quite
different from the classical SSL algorithms considered in this paper (e.g. label propagation and
manifold regularization). Notably, we only use feature vectors and the graph will be constructed
manually with kernel function. 2) Their works are restricted to testing time attacks by assuming the
model is learned and ﬁxed, and the goal of attacker is to ﬁnd a perturbation to fool the established
model. Although there are some experiments in [24] on poisoning attacks, the perturbation is still
generated from test time attack and they did not design task-speciﬁc algorithms for the poisoning in
the training time. In contrast, we consider the data poisoning problem, which happens before the
victim trains a model.

2

3 Data Poisoning Attack to G-SSL

3.1 Problem setting

×

Rnl

Rnu

d and unlabeled data Xu ∈

We consider the graph-based semi-supervised learning (G-SSL) problem. The input include labeled
d, we deﬁne the whole features X = [Xl; Xu].
data Xl ∈
×
Denoting the labels of Xl as yl, our goal is to predict the labels of test data yu. The learner
applies algorithm
to label
Xl, yl, Xu}
{
A
propagation method, where we ﬁrst generate a graph with adjacency matrix S from Gaussian kernel:
2), where the subscripts xi(j) represents the i(j)-th row of X. Then the
xi −
Sij = exp(
graph Laplacian is calculated by L = D
is the degree matrix.
k=1 Sik}
The unlabeled data is then predicted through energy minimization principle [2]

to predict yu from available data

S, where D = diag

. Here we restrict

(cid:80)n
{

xj(cid:107)

A

−

−

γ

(cid:107)

min
ˆy

1
2

(cid:88)

i,j

Sij( ˆyi −

(cid:124)
ˆyj)2 = ˆy

L ˆy,

s.t.

ˆy:l = yl.

(1)

1Sulyl, where we deﬁne
The problem has a simple closed form solution ˆyu = (Duu −
Duu = D[0:u,0:u], Suu = S[0:u,0:u] and Sul = S[0:u,0:l]. Now we consider the attacker who wants
to greatly change the prediction result yu by perturbing the training data
by small amounts
Rnl is a vector. This
∆x, δy}
{
seems to be a simple problem at the ﬁrst glance, however, we will show that the problem of ﬁnding
optimal perturbation is often intractable, and therefore provable and effective algorithms are needed.
To sum up, the problem have several degrees of freedom:

d is the perturbation matrix , and δy ∈

respectively, where ∆x ∈

Xl, yl}

Suu)−

Rnl

{

×

•

•

•

•

•

Learning algorithm: Among all graph-based semi-supervised learning algorithms, we primarily
focus on the label propagation method; however, we also discuss manifold regularization method
in Appendix 4.2.
Task: We should treat the regression task and classiﬁcation task differently because the former
is inherently a continuous optimization problem while the latter can be transformed into integer
programming.
Knowledge of attacker: Ideally, the attacker knows every aspect of the victim, including training
data, testing data, and training algorithms. However, we will also discuss incomplete knowledge
scenario; for example, the attacker may not know the exact value of hyper-parameters.
What to perturb: We assume the attacker can perturb the label or the feature, but not both. We
made this assumption to simplify our discussion and should not affect our ﬁndings.
Constraints: We also assume the attacker has limited capability, so that (s)he can only make small
perturbations. It could be measured (cid:96)2-norm or sparsity.

3.2 Toy Example

We show a toy example in Figure 1 to motivate the data
poisoning attack to graph semi-supervised learning (let
us focus on label propagation in this toy example). In
this example, the shaded region is very close to node-i
and yet quite far from other labeled nodes. After running
label propagation, all nodes inside the shaded area will
be predicted to be the same label as node-i. That gives
the attacker a chance to manipulate the decision of all
unlabeled nodes in the shaded area at the cost of ﬂipping
just one node. For example, in Figure 1, if we change
node-i’s label from positive to negative, the predictions in
the shaded area containing three nodes will also change
from positive to negative.

Besides changing the labels, another way to attack is to per-
turb the features X so that the graph structure S changes
subtly (recall the graph structure is constructed based on
pair-wise distances). For instance, we can change the fea-
tures so that node i is moved away from the shaded region,
while more negative label points are moved towards the

3

Figure 1: We show a toy example that
illustrates the main idea of the poisoning
attack against SSL. By ﬂipping just one
training data from positive to negative,
the prediction of the whole shaded area
will be changed.

PositiveNegativeUnlabelBeforeAttackAfterAttackﬂipthelabelofnodeinodeishaded area. Then with label propagation, the labels of the shaded region will be changed from
positive to negative as well. We will examine both cases in the following sections.

3.3 A uniﬁed framework

The goal of poisoning attack is to modify the data points to maximize the error rate (for classiﬁcation)
or RMSE score (for regression); thus we write the objective as

(cid:16)

(cid:13)
(cid:13)
(cid:13)g

1
2

−

(D(cid:48)uu −

S(cid:48)uu)−

(cid:17)
1S(cid:48)ul(yl + δy)

h(yu)

−

(cid:13)
2
(cid:13)
(cid:13)
2

s.t.

D(cid:48), S(cid:48)
{

}

= Kerγ(Xl + ∆x). (2)

min
δy
∈R
∆x
∈R

1

2

To see the ﬂexibility of Eq. (2) in modeling different tasks, different knowledge levels of attackers or
different budgets, we decompose it into following parts that are changeable in real applications:

• R1/

dmax}

restricts the
R2 are the constraints on δy and ∆x. For example,
R1 =
δy(cid:107)2 ≤
{(cid:107)
makes the solution to
perturbation δy to be no larger than dmax; while
cmax}
δy(cid:107)0 ≤
R1 =
{(cid:107)
R2, besides (cid:96)2 regularization, we can also
have at most cmax non-zeros. As to the choices of
enforce group sparsity structure, where each row of ∆x could be all zeros.
) is the task dependent squeeze function, for classiﬁcation task we set g(x) = sign(x) since the
g(
·
labels are discrete and we evaluate the accuracy; for regression task it is identity function g(x) = x,
and (cid:96)2-loss is used.
) controls the knowledge of unlabeled data. If the adversary knows the ground truth very well,
h(
·
then we simply put h(yu) = yu; otherwise one has to estimate it from Eq. (1), in other words,
h(yu) = ˆyu = g(cid:0)(Duu −
Kerγ is the kernel function parameterized by γ, we choose Gaussian kernel throughout.
Similar to S, the new similarity matrix S(cid:48) is generated by Gaussian kernel with parameter γ,
except that it is now calculated upon poisoned data Xl + ∆x.
Although not included in this paper, we can also formulate targeted poisoning attack problem by
changing min to max and let h(yu) be the target.

1Sulyl

Suu)−

(cid:1).

•

•

•
•

•

There are two obstacles to solving Eq. 2, that make our algorithms non-trivial. First, the problem is
naturally non-convex, making it hard to determine whether a speciﬁc solution is globally optimal;
secondly, in classiﬁcation tasks where our goal is to maximize the testing time error rate, the objective
is non-differentiable under discrete domain. Besides, even with hundreds of labeled data, the domain
space can be unbearably big for brute force search and yet the greedy search is too myopic to ﬁnd a
good solution (as we will see in experiments).

In the next parts, we show how to tackle these two problems separately. Speciﬁcally, in the ﬁrst part,
we propose an efﬁcient solver designed for data poisoning attack to the regression problem under
various constraints. Then we proceed to solve the discrete, non-differentiable poisoning attack to the
classiﬁcation problem.

3.4 Regression task, (un)known label

We ﬁrst consider the regression task where only label poisoning is allowed. This simpliﬁes Eq. (2) as

min
2
≤
(cid:107)

dmax

δy

(cid:107)






1
2
1
2

(cid:13)
(cid:13)
(cid:13)(Duu −
(cid:13)
(cid:13)
(cid:13)(Duu −

−

−

Suu)−

1Sulδy

(cid:13)
2
(cid:13)
(cid:13)
2

Suu)−

1Sul(yl + δy)

yu

(cid:13)
2
(cid:13)
(cid:13)
2

−

(estimated label)

(3a)

(true label)

(3b)

1Sul. We can solve
Here we used the fact that ˆyu = Kyl, where we deﬁne K = (Duu −
dmaxv1 and v1 is the
Eq. (3a) by SVD; it’s easy to see that the optimal solution should be δy =
1Sul = U ΣV (cid:124). However, (3b) is less
top right sigular vector if we decompose (Duu −
straightforward, in fact it is a non-convex trust region problem, which can be generally formulated as

Suu)−

Suu)−

±

min
≤
(cid:107)

2

z

dmax

1
2

f (z) =

(cid:124)

z

Hz + g

(cid:124)

z, H is indeﬁnite.

(4)

(cid:107)
Our case (3b) can thus be described as H =
proposed a sublinear time solver that is able to ﬁnd a global minimum in

0 and g = K(cid:124)(yu −

K(cid:124)K

−

(cid:22)

ˆyu). Recently [26]
(M/√(cid:15)) time. Here

O

4

Algorithm 1: Trust region problem solver

Data: Vector g, symmetric indeﬁnite matrix H for problem min
Result: Approximate solution z∗.

0.5 g
g
(cid:107)

−

1 Initialize z0 =

and step size η;

(cid:107)
/* Phase I: iterate inside sphere (cid:107)zt(cid:107) < 1

< 1 do

2 while
3
4 end

zt(cid:107)
(cid:107)
zt+1 = zt −

η(Hzt + g);

/* Phase II: iterate on the sphere (cid:107)zt(cid:107) = 1

1

2 z(cid:124)Hz + g(cid:124)z.

z

(cid:107)

(cid:107)≤

1

*/

*/

5 zt(cid:48) = zt;
6 while t < max_iter do
7

9 end
10 Return zmax_iter

Choose αt(cid:48) by line search and do the following projected gradient descent on sphere;
zt(cid:48)+1 =

;

zt(cid:48)
zt(cid:48)

−
−

(cid:107)

αt(cid:48) (Id
αt(cid:48) (Id

zt(cid:48) z
zt(cid:48) z

−
−

(cid:124)
t(cid:48) )(Hzt(cid:48) +g)
(cid:124)
t(cid:48) )(Hzt(cid:48) +g)
(cid:107)

8

we propose an asymptotic linear algorithm based purely on gradient information, which is stated in
Algorithm 1 and Theorem 6. In Algorithm 1 there are two phases, in the following theorems, we
show that the phase I ends within ﬁnite iterations, and phase II converges with an asymptotic linear
rate. We postpone the proof to Appendix 1.
Theorem 1 (Convergent). Suppose the operator norm
with initialization z0 =
Algorithm 1 converge to the global minimum.
Lemma 1 (Finite phase I). Since H is indeﬁnite, λ1 = λmin(H) < 0, and v1 is the corresponding
eigenvector. Denote a(1) = a(cid:124)v1 is the projection of any a onto v1, let T1 be number of iterations in
phase I of Algorithm 1, then:

(cid:107)op = β, by choosing a step size η < 1/β
generated from
zt}
{

H
(cid:107)
3
g
, 0 < α < min(1, (cid:107)
(cid:107)
g(cid:124)Hg
|

). Then iterates

α g
g
(cid:107)

−

(cid:107)

|

T1 ≤

log(1

−

ηλ1)−

1(cid:104)

log (cid:0)

(cid:1)

1
ηλ1

−

log (cid:0) −

z(1)
0
ηg(1) −

1
ηλ1

−

(cid:1)(cid:105)
.

(5)

|
Theorem 2 (Asymptotic linear rate). Let
be an inﬁnite sequence of iterates generated by
Algorithm 1, suppose it converges to z∗ (guaranteed by Theorem 3), let λH,min and λH,max be the
smallest and largest eigenvalues of H. Assume that z∗ is a local minimizer then λH,min > 0 and
(cid:1), ¯α, σ are line
given r in the interval (r
∗
search parameters. There exists an integer K such that:

min (cid:0)2σ ¯αλH,min, 4σ(1

, 1) with r
∗

σ)β λH,min
λH,max

= 1

−

−

{

η

1
g(1)
|
zt}

f (zt+1)

f (z∗)

−

≤

r(cid:0)f (zt)

f (z∗)(cid:1)

−

for all t

K.

≥

3.5 Classiﬁcation task

As we have mentioned, data poisoning attack to classiﬁcation problem is more challenging, as we can
only ﬂip an unnoticeable fraction of training labels. This is inherently a combinatorial optimization
nl , and the
problem. For simplicity, we restrict the scope to binary classiﬁcation so that yl ∈ {−
1, +1
}
labels are perturbed as ˜yl = yl (cid:12)
1].
denotes Hadamard product and δy = [
1, . . . ,
1,
±
±
For restricting the amount of perturbation, we replace the norm constraint in Eq. (3a) with integer
constraint (cid:80)nl
I
cmax, where cmax is a user pre-deﬁned constant. In summary, the
i=1
{
ﬁnal objective function has the following form

δy, where

1
} ≤

δy[i]=

±

(cid:12)

−

(cid:13)
(cid:13)

1
2

1

δy

∈{

δy)(cid:1)

min
+1,
−

(cid:13)g(cid:0)K(yl (cid:12)
nl −
1Sul and g(x) = sign(x), so the objective function directly
where we deﬁne K = (Duu −
Suu)−
(cid:1) solutions, making it
relates to error rate. Notice that the feasible set contains around (cid:80)cmax
k=0
almost impossible to do an exhaustive search. A simple alternative is greedy search: ﬁrst initialize

(yu or ˆyu)

1
} ≤

cmax,

(cid:0)nl
k

δy[i]=

I
{

s.t.

(6)

i=1

−

−

}

,

(cid:88)nl

(cid:13)
2
(cid:13)
(cid:13)

5

δy = [+1, +1, . . . , +1], then at each time we select index i
such that the objective function (6) decreases the most. Next, we set δy[i] =
process multiple times until the constraint in (6) is met.

∈

[nl] and try ﬂip δy[i] = +1

1,
1. We repeat this

→ −

−

Doubtlessly, the greedy solver is myopic. The main reason is that the greedy method cannot explore
other ﬂipping actions that appear to be sub-optimal within the current context, despite that some
sub-optimal actions might be better in the long run. Inspired by the bandit model, we can imagine
this problem as a multi-arm bandit, with nl arms in total. And we apply a strategy similar to (cid:15)-greedy:
each time we assign a high probability to the best action but still leave non-zero probabilities to other
“actions”. The new strategy can be called probabilistic method, speciﬁcally, we model each action
1] = α. The new loss
δy =
function is just an expectation over Bernoulli variables

1 as a Bernoulli distribution, the probability of “ﬂipping” is P [δy =

−

±

(cid:26)

min
α

L

(α) :=

1
2

−

z

E
(1,α)

(cid:104)(cid:13)
(cid:13)g(cid:0)K(yl (cid:12)

z)(cid:1)

−

(yu or ˆyu)(cid:13)
(cid:13)

2(cid:105)

+

λ
2 · (cid:107)

α

2
2
(cid:107)

(cid:27)

.

(7)

∼B
Here we replace the integer constraint in Eq. 6 with a regularizer λ
2
2, the original constraint is
(cid:107)
reached by selecting a proper λ. Once problem (7) is solved, we craft the actual perturbation δy by
setting δy[i] =
To solve Eq. (7), we need to ﬁnd a good gradient estimator. Before that, we replace g(x) = sign(x)
with tanh(x) to get a continuously differentiable objective. We borrow the idea of “reparameterization
trick” [27, 28] to approximate

1 if α[i] is among the top-cmax largest elements.

(1, α) by a continuous random vector

2 (cid:107)

−

α

B

z (cid:44) z(α, ∆G) =

1 + exp

(cid:16) 1
τ

2
(cid:0) log α

1

−

α + ∆G

(cid:1)(cid:17) −

1

(
−

∈

1, 1),

(8)

g2 and g1,2
where ∆G ∼
controlling the steepness of sigmoid function: as τ
to a stair function. Plugging (8) into (7), the new loss function becomes

Gumbel(0, 1) are two Gumbel distributions. τ is the temperature
0, the sigmoid function point-wise converges

g1 −

→

iid
∼

(α) :=

L

1
2

(cid:104)(cid:13)
(cid:13)g(cid:0)K(yl (cid:12)

E
∆G

−

z(α, ∆G))(cid:1)

(yu or ˆyu)(cid:13)
(cid:13)

2(cid:105)

+

λ
2 · (cid:107)

α

2
2.
(cid:107)

−

(9)

Therefore, we can easily obtain an unbiased, low variance gradient estimator via Monte Carlo
sampling from ∆G = g1 −

g2, speciﬁcally

∂

(α)

L
∂α ≈ −

1
2

∂
∂α

(cid:13)

(cid:13)g(cid:0)K(yl (cid:12)

z(α, ∆G))(cid:1)

−

(yu or ˆyu)(cid:13)
2
(cid:13)

+ λα.

(10)

Based on that, we can apply many stochastic optimization methods, including SGD and Adam [29],
to ﬁnalize the process. In the experimental section, we will compare the greedy search with our
probabilistic approach on real data.

4 Experiments

In this
ing attack algorithms

section, we will
for

show the
effectiveness of our proposed data poison-
regression and classiﬁcation tasks on graph-based SSL.

4.1 Experimental settings and baselines

We conduct experiments on two regression and
two binary classiﬁcation datasets1. The meta-
information can be found in Table 4. We use a
Gaussian kernel with width γ to construct the
graph. For each data, we randomly choose nl
samples as the labeled set, and the rest are un-
labeled. We normalize the feature vectors by

Table 1: Dataset statistics. Here n is the total num-
ber of samples, d is the dimension of feature vector
and γ∗ is the optimal γ in validation. mnist17 is
created by extracting images for digits ‘1’ and ‘7’
from standard mnist dataset.

Task

n

d

Name

cadata
E2006

Regression
Regression

mnist17 Classiﬁcation
Classiﬁcation

rcv1

8,000
19,227
26,014
20,242

8
150,360
780
47,236

γ∗
1.0
1.0
0.6
0.1

1Publicly available at https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/

6

−

−

(y

(x

←

←

(cid:80)nu

(cid:113) 1
nu

i=1(yi −

ymin) so that y(cid:48)

ymin)/(ymax −

µ)/σ, where µ is the sample mean, and σ is the sample variance. For regression
x(cid:48)
data, we also scale the output by y(cid:48)
[0, 1]. To evaluate
the performance of label propagation models, for regression task we use RMSE metric deﬁned
ˆyi)2, while for classiﬁcation tasks we use error rate metric. For
as RMSE =
comparison with other methods, since this is the ﬁrst work on data poisoning attack to G-SSL,
we proposed several baselines according to graph centrality measures. The ﬁrst baseline is random
perturbation, where we randomly add Gaussian noise (for regression) or Bernoulli noise (for regres-
sion) to labels. The other two baselines based on graph centrality scores are more challenging, they
are widely used to ﬁnd the “important” nodes in the graph. Intuitively, we need to perturb “important”
nodes to attack the model, and we decide the importance by node degree or PageRank. We explain
the baselines with more details in the appendix.

∈

4.2 Effectiveness of data poisoning to G-SSL

In this experiment, we consider the white-box setting where the attacker knows not only the ground
truth labels yu but also the correct hyper-parameter γ∗. We thus apply our proposed label poisoning
algorithms in Section 3.4 and 3.5 to attack regression and classiﬁcation tasks, respectively. In
particular, we apply (cid:96)2 constraint for perturbation δy in the regression task and use the greedy method
in the classiﬁcation task. The results are shown in Figure 2, as we can see in this ﬁgure, for both

Figure 2: Top row: testing the effectiveness of poisoning algorithms on four datasets shown in
Table (4). The left two datasets are regression tasks, and we report the RMSE measure. The right two
datasets are classiﬁcation tasks in which we report the error rate. For each dataset, we repeat the same
attacking algorithm w.r.t. different nl’s. Bottom row: compare our poisoning algorithm with three
baselines (random noise, degree-based attack, PageRank based attack). We follow our convention
that dmax is the maximal (cid:96)2-norm distortion, and cmax is the maximal (cid:96)0-norm perturbation.

= 3 (this is very small compared with the norm of label

regression and classiﬁcation problems, small perturbations can lead to vast differences: for instance,
on cadata, the RMSE increases from 0.2 to 0.3 when applied a carefully designed perturbation
37.36); More surprisingly, on
δy(cid:107)
(cid:107)
mnist17, the accuracy can drop from 98.46% to 50% by ﬂipping just 3 nodes. This phenomenon
indicates that current graph-based SSL, especially the label propagation method, can be very
fragile to data poisoning attacks. On the other hand, using different baselines (shown in Figure 2,
bottom row), the accuracy does not decline much, this indicates that our proposed attack algorithms
are more effective than centrality based algorithms.

yl(cid:107) ≈
(cid:107)

Moreover, the robustness of label propagation is strongly related to the number of labeled data nl: for
all datasets shown in Figure 2, we notice that the models with larger nl tend to be more resistant to
poisoning attacks. This phenomenon arises because, during the learning process, the label information
propagates from labeled nodes to unlabeled ones. Therefore even if a few nodes are “contaminated”
during poisoning attacks, it is still possible to recover the label information from other labeled nodes.

7

0.02.55.07.5dmax0.1750.2000.2250.2500.2750.300RMSEcadatanl=500nl=1000nl=2000024dmax0.100.150.200.25RMSEE2006nl=100nl=300nl=7000.02.55.07.5cmax01020304050Errorrate(%)mnist17nl=50nl=100nl=2000.02.55.07.5cmax10203040Errorrate(%)rcv1nl=500nl=1000nl=3000024dmax0.200.220.240.260.28RMSEcadataPoisoningNoiseDegreePageRank0123dmax0.1000.1250.1500.1750.2000.225RMSEE2006PoisoningNoiseDegreePageRank0.02.55.07.5cmax01020304050Errorrate(%)mnist17PoisoningNoiseDegreePageRank0.02.55.07.5cmax1015202530Errorrate(%)rcv1PoisoningNoiseDegreePageRankHence this experiment can be regarded as another instance of “no free lunch” theory in adversarial
learning [30].

4.3 Comparing poisoning with and without truth labels

Figure 3: Comparing the effectiveness of label poisoning attack with and without knowing the ground
truth labels of unlabeled nodes yu. Interestingly, even if the attacker is using the estimated labels ˆyu,
the effectiveness of the poisoning attack does not degrade signiﬁcantly.

We compare the effectiveness of poisoning attacks with and without ground truth labels yu. Recall
that if an attacker does not hold yu, (s)he will need to replace it with the estimated values ˆyu. Thus
we expect a degradation of effectiveness due to the replacement of yu, especially when ˆyu is not
a good estimation of yu. The result is shown in Figure 3. Surprisingly, we did not observe such
phenomenon: for regression tasks on cadata and E2006, two curves are closely aligned despite that
attacks without ground truth labels yu are only slightly worse. For classiﬁcation tasks on mnist17
and rcv1, we cannot observe any difference, the choices of which nodes to ﬂip are exactly the same
(except the cmax = 1 case in rcv1). This experiment provides a valuable implication that hiding the
ground truth labels cannot protect the SSL models, because the attackers can alternatively use the
estimated ground truth ˆyu.

4.4 Comparing greedy and probabilistic method

Figure 4: Comparing the relative performance of three approximate solvers to discrete optimization
greedy).
problem (6). For clarity, we also show the relative performance on the right (probabilistic

−

In this experiment, we compare the performance of three approximate solvers for problem (6) in
Section 3.5, namely greedy and probabilistic methods. We choose rcv1 data as oppose to mnist17
data, because rcv1 is much harder for poisoning algorithm: when nl = 1000, we need cmax ≈
30
50%, whilst mnist17 only takes cmax = 5. For hyperparameters, we set
to make error rate
≈
, nl = 1000, γ∗ = 0.1. The results are shown in Figure 4, we can see that
cmax =
0, 1, . . . , 29
{
}
for larger cmax, greedy method can easily stuck into local optima and inferior than our probabilistic
based algorithms.

4.5 Sensitivity analysis of hyper-parameter

8

024dmax0.200.220.240.260.28RMSEcadataw/oyuw/yu0123dmax0.1000.1250.1500.1750.2000.225RMSEE2006w/oyuw/yu0.02.55.07.5cmax01020304050Errorrate(%)mnist17w/oyuw/yu0.02.55.07.5cmax10203040Errorrate(%)rcv1w/oyuw/yu0102030cmax10203040Errorrate(%)rcv1GreedyProbablistic0102030cmax−20246Relativeerrorrate(%)rcv1ProbablisticSince we use the Gaussian kernel to construct the
graph, there is an important hyper-parameter γ (ker-
nel width) that controls the structure of the graph
deﬁned in (1), which is often chosen empirically by
the victim through validation. Given the ﬂexibility of
γ, it is thus interesting to see how the effectiveness of
the poisoning attack degrades with the attacker’s im-
perfect estimation of γ. To this end, we suppose the
victim runs the model at the optimal hyperparameter
γ = γ∗, determined by validation, while the attacker
γ∗. We conduct
has a very rough estimation γadv ≈
this experiment on cadata when the attacker knows
or does not know the ground truth labels yu, the re-
sult is exhibited in Figure 5. It shows that when the
adversary does not have exact information of γ, it
will receive some penalties on the performance (in
RMSE or error rate). However, it is entirely safe to
choose a smaller γadv < γtruth because the perfor-
mance decaying rate is pretty low. Take Figure 5 for example, even though γadv = 1
8 γtruth, the
RMSE only drops from 0.223 to 0.218. On the other hand, if γadv is over large, the nodes become
more isolated, and thus the perturbations are harder to propagate to neighbors.

Figure 5: Experiment result on imperfect esti-
mations of γ∗.

5 Conclusion

We conduct the ﬁrst comprehensive study of data poisoning to G-SSL algorithms, including label
propagation and manifold regularization (in the appendix). The experimental results for regression
and classiﬁcation tasks exhibit the effectiveness of our proposed attack algorithms. In the future, it
will be interesting to study poisoning attacks for deep semi-supervised learning models.

Acknowledgement

Xuanqing Liu and Cho-Jui Hsieh acknowledge the support of NSF IIS-1719097, Intel faculty award,
Google Cloud and Nvidia. Zhu acknowledges NSF 1545481, 1561512, 1623605, 1704117, 1836978
and the MADLab AF COE FA9550-18-1-0166.

References

[1] Xiaojin Zhu and Zoubin Ghahramani. Learning from labeled and unlabeled data with label

propagation. 2002.

[2] Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. Semi-supervised learning using gaussian
ﬁelds and harmonic functions. In Proceedings of the 20th International conference on Machine
learning (ICML-03), pages 912–919, 2003.

[3] Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien. Semi-supervised learning (chapelle,
o. et al., eds.; 2006)[book reviews]. IEEE Transactions on Neural Networks, 20(3):542–542,
2009.

[4] Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization: A geometric
framework for learning from labeled and unlabeled examples. Journal of machine learning
research, 7(Nov):2399–2434, 2006.

[5] Vikas Sindhwani, Partha Niyogi, Mikhail Belkin, and Sathiya Keerthi. Linear manifold regular-

ization for large scale semi-supervised learning. 2005.

[6] Ping Zhang, Fei Wang, Jianying Hu, and Robert Sorrentino. Label propagation prediction of

drug-drug interactions based on clinical side effects. Scientiﬁc reports, 5:12339, 2015.

[7] Michael Speriosu, Nikita Sudan, Sid Upadhyay, and Jason Baldridge. Twitter polarity classiﬁ-
cation with label propagation over lexical links and the follower graph. In Proceedings of the
First workshop on Unsupervised Learning in NLP, pages 53–63. Association for Computational
Linguistics, 2011.

9

2−82−52−221γadv0.2160.2180.2200.222RMSEcadata,γtrue=1.0,nl=1000w/oyuw/yu[8] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial

examples. In ICLR, 2015.

[9] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199,
2013.

[10] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In

2017 IEEE Symposium on Security and Privacy (SP), pages 39–57. IEEE, 2017.

[11] Pin-Yu Chen, Yash Sharma, Huan Zhang, Jinfeng Yi, and Cho-Jui Hsieh. Ead: elastic-net
attacks to deep neural networks via adversarial examples. In Thirty-Second AAAI Conference
on Artiﬁcial Intelligence, 2018.

[12] Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420,
2018.

[13] Minhao Cheng, Jinfeng Yi, Huan Zhang, Pin-Yu Chen, and Cho-Jui Hsieh. Seq2sick: Evaluating
the robustness of sequence-to-sequence models with adversarial examples. arXiv preprint
arXiv:1803.01128, 2018.

[14] Nicolas Papernot, Patrick McDaniel, Ananthram Swami, and Richard Harang. Crafting adver-
sarial input sequences for recurrent neural networks. In Military Communications Conference,
MILCOM 2016-2016 IEEE, pages 49–54. IEEE, 2016.

[15] Minhao Cheng, Thong Le, Pin-Yu Chen, Jinfeng Yi, Huan Zhang, and Cho-Jui Hsieh. Query-
efﬁcient hard-label black-box attack: An optimization-based approach. In ICLR, 2019.

[16] Shike Mei and Xiaojin Zhu. Using machine teaching to identify optimal training-set attacks on

machine learners. In AAAI, 2015.

[17] Pang Wei Koh and Percy Liang. Understanding black-box predictions via inﬂuence functions.

arXiv preprint arXiv:1703.04730, 2017.

[18] Huang Xiao, Battista Biggio, Blaine Nelson, Han Xiao, Claudia Eckert, and Fabio Roli. Support

vector machines under adversarial label contamination. Neurocomputing, 160:53–62, 2015.

[19] Bo Li, Yining Wang, Aarti Singh, and Yevgeniy Vorobeychik. Data poisoning attacks on
In Advances in neural information processing

factorization-based collaborative ﬁltering.
systems, pages 1885–1893, 2016.

[20] Mengchen Zhao, Bo An, Yaodong Yu, Sulin Liu, and Sinno Jialin Pan. Data poisoning attacks

on multi-task relationship learning. 2018.

[21] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on

deep learning systems using data poisoning. arXiv preprint arXiv:1712.05526, 2017.

[22] Yizhen Wang and Kamalika Chaudhuri. Data poisoning attacks against online learning. arXiv

preprint arXiv:1808.08994, 2018.

[23] Hanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang, Jun Zhu, and Le Song. Adversarial

attack on graph structured data. In ICML, 2018.

[24] Daniel Zügner, Amir Akbarnejad, and Stephan Günnemann. Adversarial attacks on classiﬁcation

models for graphs. In KDD, 2018.

[25] Xiaoyun Wang, Joe Eaton, Cho-Jui Hsieh, and Felix Wu. Attack graph convolutional networks

by adding fake nodes. arXiv preprint arXiv:1810.10751, 2018.

[26] Elad Hazan and Tomer Koren. A linear-time algorithm for trust region problems. Mathematical

Programming, 158(1-2):363–381, 2016.

[27] Michael Figurnov, Shakir Mohamed, and Andriy Mnih. Implicit reparameterization gradients.

arXiv preprint arXiv:1805.08498, 2018.

[28] George Tucker, Andriy Mnih, Chris J Maddison, John Lawson, and Jascha Sohl-Dickstein.
Rebar: Low-variance, unbiased gradient estimates for discrete latent variable models. In NIPS,
2017.

[29] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980, 2014.

10

[30] Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry.

Robustness may be at odds with accuracy. 2018.

[31] Jorge Cadima and Ian T Jolliffe. Loading and correlations in the interpretation of principle

compenents. Journal of Applied Statistics, 22(2):203–214, 1995.

[32] Hui Zou, Trevor Hastie, and Robert Tibshirani. Sparse principal component analysis. Journal

of computational and graphical statistics, 15(2):265–286, 2006.

[33] Alexandre d’Aspremont, Francis Bach, and Laurent El Ghaoui. Optimal solutions for sparse
principal component analysis. Journal of Machine Learning Research, 9(Jul):1269–1294, 2008.

11

A Proof of Convergence

We show that our gradient based nonconvex trust region solver is able to ﬁnd a global minimum
efﬁciently. First recall the objective function

f (z) =

f (z∗) = min
≤
i=1 λiviv

z

(cid:107)

(cid:107)

1

2

(cid:124)

(cid:124)

z

z,

Hz + g

1
2
(cid:124)
i and rank λ1 ≤ · · · ≤

Suppose H has decomposition (cid:80)n
λn. We only
0
focus on “easy case”: in which case g(1) = g(cid:124)v1 (cid:54)
= 0 and v1 is the corresponding eigenvector of
λ1 = λmin(H). In opposite, the hard case g(1) = 0 is hardly seen in practice due to rounding error,
and to be safe we can also add a small Gaussian noise to g. To see the structure of solution, suppose
the solution is z∗, then by KKT condition, we can get the condition of global optima

λi ≤

≤ · · · ≤

λmin(H) < 0.

(11)

λ(1

(H + λId)z∗ + g = 0,
(cid:107)2) = 0,
0.

z∗
− (cid:107)
H + λId (cid:23)

(12)

By condition λ1 < 0, further if g(1)
which implies λ >
(cid:80)n
λi+λ )2 = 1.

i=1( g(i)

−

λ1. Immediately we know z∗ =

= 0, then λ

≥ −
−

= 1. Because g(1)
λ1 and
= 0,
z∗
(cid:107)
(cid:107)
1g and λ is the solution of
(H + λId)−

As a immediate application of (12), we can conclude the following lemma:
Lemma 2. When g(1)
the global minimum.

= 0 and λ1 < 0, among all stationary points if s(1)g(1)

≤

0 then s = z∗ is

Proof. We proof by contradiction. Suppose s is a stationary point and s(1)g(1)
0, according
to (12) if s is not a global minimum then the third condition in (12) should be violated, implying
that λ1 + λ < 0. Furthermore, for stationary point s, we know the gradient of Lagrangian is zero:
(H + λId)z∗ + g = 0. Projecting this equation onto v1 we get
(λ1 + λ)s(1) + g(1) = 0.
= 0; multiply both sides of Eq. 13 by s(1) we get s(1)g(1) > 0,

(13)

≤

= 0 we know s(1)
By condition g(1)
which is in contradiction to s(1)g(1)

0.

≤

We now consider the projected gradient descent update rule zt+1 = Prox
following assumptions:
Assumption 1. (Bounded step size) Step size η < 1/β, where β =

Assumption 2. (Initialize) z0 =

α g
g
(cid:107)

(cid:107)

−

, 0 < α < min (cid:0)1, (cid:107)

3
g
(cid:107)
g(cid:124)Hg
|

|

(cid:107)op.

H
(cid:107)
(cid:1).

2 (zt −

(cid:107)·(cid:107)

η

∇

f (zt)), with

Under these assumptions, we next show proximal gradient descent converges to global minimum
Theorem 3. Under proximal gradient descent update: zt+1 = Prox
Assumption 1 if z(i)
λ1 < 0, g(1)

f (zt)(cid:1), and
(cid:107)·(cid:107)
0. Combining with Assumption 2 and Lemma 2, if

= 0 then zt converges to a global minimum z∗.

0 then z(i)

(cid:0)zt −

t+1g(i)

t g(i)

∇

≤

≤

η

2

Proof. Notice the projection onto sphere will not change the sign of z(i)

sign (cid:0)z(i)

t+1g(i)(cid:1) = sign (cid:0)(1

ηλi)z(i)

t g(i)

−

−

t+1, so:
ηg(i)2(cid:1)

ηt < 1/
λn|
|
0, so z(i)
t g(i)

≤

ensures 1

ηλi > 0 for all i

[n]. From Assumption 2 we know z(i)
0 for all t. We complete the proof by combining it with Lemma 2.

−

∈

0 g(i) =

α g(i)2

g

(cid:107) ≤

(cid:107)

−

By careful analysis, we can actually divide the convergence process into two stages. In the ﬁrst stage,
the iterates
< 1; in the second stage the iterates stay on the unit ball
zt(cid:107)
(cid:107)
= 1. Furthermore, we can show that the ﬁrst stage ends with ﬁnite number of iterations. Before
zt(cid:107)
(cid:107)
that, we introduce the following lemma:

stay inside the sphere

zt}
{

12

(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
(cid:54)
Lemma 3. Considering the ﬁrst stage, when iterates
under the update rule zt+1 = zt −
have z
≥

zt}
{
f (zt), and under assumption that z
f (zt) (recall we deﬁne β as the operator norm of H).

are inside unit sphere

(cid:124)
t ∇

(cid:124)
t H

βz

∇

∇

η

zt(cid:107)2 ≤
(cid:107)
f (zt)
≤

1, i.e.
0, we will

t /(

ηg(i)), then by iteration rule zt+1 = (I

ηH)zt −

−

ηg,

−

f (zt)

(cid:124)
t ∇
t = z(i)
Proof. We ﬁrst deﬁne w(i)
projecting both sides on vi,

dividing both sides by

−

z(i)
t+1 = (1
ηg(i), we get

ηλi)z(i)

t −

−

ηg(i),

w(i)

t+1 = (1

ηλi)w(i)

t + 1,

−

solving this geometric series, we get:

w(i)

t = (1

suppose at t-th iteration we have w(i)

−
w(i)

ηλi)t(cid:0)w(i)

0 −

(cid:1) +

1
ηλi

1
ηλi

,

(14)

(15)

t ≥
1
ηλi ≥

w(i)

0 −

t+1, after plugging in Eq. (15) and noticing 0 < ηλi < 1

(1

−

ηλi)(cid:0)w(i)

0 −

(cid:1).

1
ηλi

(16)

Furthermore, from Assumption 2 we know that w(i)
w(i)
1/w(i)
0, equivalently 0 < ηλi ≤
0
leading to a contradiction, so it must hold that

1
ηλi ≤

0 −

0 = z(i)

t /(
then by Eq. (16) we know 1

ηg(i)) = α 1
g
(cid:107)

−

η

(cid:107)

2 ≥
ηλi ≥

−

0, if we have
ηλi ≤

⇔

1

0

1.

ηλi ≤
λi for j

w(i)

0 −

1
ηλi

> 0 and 1

−
At the same time, the eigenvalues are nondecreasing, λj ≥

ηλj ≤
Also recalling the initialization condition implies w(j)
1/ηλj and noticing λj ≥
λi
w(j)

= w(i)

−

1

1.

1
ηλj

1
ηλj ≥

w(i)

1
ηλi

0 −

0 −
Combining Eq. (17) with Eq. (18), we can conclude that if w(i)
also holds for index j > i
1
ηλj ≥

ηλj)(cid:0)w(j)

1
ηλj

0 −

0 −

0 −

w(j)

⇐⇒

(1

−

(cid:1)

w(j)

t ≥

i, which means

≥

(17)

0 = α 1
g
(cid:107)

η

= w(i)

0 , subtracting both sides by

2
(cid:107)

> 0.

(18)

w(i)

t+1 holds, then such relation

t ≥

w(j)

t+1 for j

i.

≥

(19)

[n] is the smallest coordinate index such that w(i∗)

Consider at any iteration time t, suppose i∗
w(i∗)
i

t+1, and hence wi
≥

t < wi

∈

t+1 holds for all i < i∗. By Eq. (19) we know and wi

i∗ (such a i∗ may not exist, but it doesn’t matter). By analyzing the sign of zt we know:
(cid:0)w(i)

= sign (cid:0)w(i)

(cid:0)z(i)

= sign

(cid:1)(cid:17)

(cid:1)(cid:17)

(cid:16)

(cid:16)

[n],

zi
t

sign

t −

z(i)
t+1

w(i)
t+1
the second equality is true due to Eq. (14), we know w(i)
We complete the proof by following inequalities:

w(i)
t

t −

t −
t > 0 for all i and t.

t ≥
(cid:1),

w(i)
t+1

i
∀

∈

t ≥
t+1 for any

wi

1
η

i∗
1
(cid:88)
−

i=1

1

λi∗
−
η

λiz(i)
(cid:124)

t (z(i)
t −
(cid:123)(cid:122)
0
≤

z(i)
t+1)
(cid:125)

+

1
η

n
(cid:88)

i=i∗

λiz(i)
(cid:124)

t (z(i)
t −
(cid:123)(cid:122)
0
≥

z(i)
t+1)
(cid:125)

i∗
1
(cid:88)
−

i=1

t (z(i)
z(i)

t −

z(i)
t+1) +

λi∗
η

n
(cid:88)

i=i∗

t (z(i)
z(i)

t −

z(i)
t+1)

(20)

(cid:124)
t A
z

∇

f (zt) =

≥

≥

λi∗
η

n
(cid:88)

i=1

t (z(i)
z(i)

t −

z(i)
t+1)

(cid:124)
t ∇
Where the last inequality follows from assumption in this lemma.

f (zt).

βz

≥

13

zt(cid:107)2 monotone increases. In fact, we have the following theorem:
(cid:107)

zt}
{
is in the region
zt(cid:107)2 < 1, such that proximal gradient update equals to
(cid:107)
f (zt), then under this update rule,

that are still inside the sphere, we will eventually

is monotone increasing.

By applying this lemma on the iterates
conclude that
Theorem 4. Suppose
{
plain GD: zt+1 = zt −
Proof. We prove by induction. First of all, notice
zt+1(cid:107)
(cid:124)
(cid:107)
2, it remains to show z
to prove
f (zt)
zt(cid:107)
t ∇
f (z0) = α2 g(cid:124)Hg

zt}
η
∇

zt+1(cid:107)

≥ (cid:107)

(cid:107)

2

2 =

≤

zt(cid:107)
(cid:107)
zt(cid:107)
(cid:107)
0. For t = 0 we note that

(cid:124)
t ∇

f (zt) + η2

2ηz

−

2

f (zt)

(cid:107)∇

2,
(cid:107)

(21)

(cid:124)
z
0 ∇

α

g
(cid:107)

(cid:107) ≤

0,

g
(cid:107)

2 −
(cid:107)

(cid:124)
where the last inequality follows from Assumption 2. Now suppose z
t
update rule zt = zt
(cid:124)
z
t ∇

f (zt
1 −
−
(cid:124)
f (zt) =z
t
−

1) we know:

f (zt

f (zt

2
(cid:107)

1∇

1A

(cid:107)∇

1)

1)

∇

−

−

η

η

−

−

−

−

(cid:124)
ηz
t
(cid:124)

f (zt

1)
(cid:125)

−

∇
(cid:123)(cid:122)
(1)

1∇

−

f (zt

1)

−

≤

0 and by

+ η2
(cid:124)

∇

f (zt

−

(cid:124)
1)
(cid:123)(cid:122)
(2)

A

f (zt

−

∇

.

1)
(cid:125)

From Lemma 4 we know (1)
(2)

f (zt

β

≤

(cid:107)∇

−

≥

f (zt

βzt
−
2, combining them together:
1)
(cid:107)
(cid:124)
z
t ∇

(cid:124)
βη)z
t

f (zt)

1∇

(1

≤

−

by choosing η < 1/β we proved z

−
(cid:124)
t ∇
(cid:124)
Due to induction rule we know that z
t ∇
increasing.

−
f (zt)

1∇
0.

≤
f (zt)

1) and recall β is the operator norm of A, we have

f (zt

1)

−

−

η(1

−

ηβ)

f (zt)

(cid:107)∇

2,
(cid:107)

(22)

0 holds for all t and moreover,

zt+1(cid:107)
(cid:107)

is monotone

≤

We can easily improve the results above, to show that phase I (where
terminate after ﬁnite number of iteration. This is formally described in the following proposition:
Proposition 1. (Finite phase I) Assuming λ1 < 0, suppose t∗ is the index that
zt∗+1(cid:107) ≥
(cid:107)

1, then t∗ is bounded by:

zt(cid:107) ≤
(cid:107)

1) will eventually

< 1 and

zt∗

(cid:107)

(cid:107)

t∗

≤

log(1

−

ηλ1)−

1(cid:104)

log (cid:0)

1
g(1)

η

|

−

|

(cid:1)

1
ηλ1

−

log (cid:0) −

z(1)
0
ηg(1) −

(cid:1)(cid:105)

.

1
ηλ1

(23)

Proof. This directly follows from:

t∗+1 = z(1)2
together with Eq. (15) immediately comes to Eq. (23).

η2g(1)2w(1)2

≤

1

t∗+1 ≤ (cid:107)

zt∗+1(cid:107)

2,

{

zt}

Lastly, it remains to show the converge rate in phase II, this is actually a standard manifold gradient
descent problem
Theorem 5. Let
then every accumulation point of

be an inﬁnite sequence of iterates generated by line search gradient descent,
is a stationary point of the cost function f .
Theorem 6. Let
be an inﬁnite sequence of iterates generated by line search gradient descent,
suppose it converges to z∗. Let λH,min and λH,max be the smallest and largest eigenvalues of the
Hessian at z∗. Assume that z∗ is a local minimizer then λH,min > 0 and given r in the interval
(cid:1), there exists an integer K such that:
(r
∗

min (cid:0)2σ ¯αλH,min, 4σ(1

, 1) with r
∗

σ)β λH,min
λH,max

zt}

zt}

= 1

{

{

−

−

f (zt+1)

f (z∗)

−

≤

r(cid:0)f (zt)

−

f (z∗)(cid:1),

for all t

K.

≥

Proof. See Theorem 4.3.1 and Theorem 4.5.6 in [Absil et al., 2009].

14

To apply Theorem 6, we need to check λH,min > 0. To do that we can directly calculate its value, by
the deﬁnition of Riemanndian Hessian, we have

Hess f (x) = Hess(f

◦
Hess (f
(cid:104)

Expx)(0x),

Expx)(0x)[ξ], ξ

.
(cid:105)

◦

Hess f (x)[ξ], ξ
(cid:104)

(cid:105)

=

Hess (f

=

(cid:105)

(cid:104)

◦

Rx)(0x)[ξ], ξ

= d2

dt2 f (Rx(tξ))

(cid:105)

(cid:12)
(cid:12)
(cid:12)t=0

,

Then for ξ

,
TxM

∈

Hess f (x)[ξ], ξ

(cid:104)
we then expand f (Rx(tξ)) to,

t2 + ξ(cid:124)Hx
x + tξ
(cid:107)
By differentiating t twice and set t = 0 (this can be done by software), we ﬁnally get

t + x(cid:124)Hx
·
2
2
(cid:107)

g(cid:124)x + g(cid:124)ξ
·
2
x + tξ
2
(cid:107)

f (Rx(tξ)) =

ξ(cid:124)Hξ

+

(cid:107)

2

t

.

·

(cid:105)
Taking x = z∗ into above equation, we get

Hess f (x)[ξ], ξ
(cid:104)

=

−

(cid:124)
x

Hx + ξ

(cid:124)

Hξ

(cid:124)

g

x.

−

(24)

(25)

(26)

(27)

On the other hand, by optimal condition (12), we have:

Hess f (z∗)[ξ], ξ

=

(cid:124)

Hz∗ + ξ

(cid:124)

Hξ

z∗

(cid:104)

(cid:105)

−

(cid:124)

z∗

(H + λId)z∗ + z∗

(cid:124)

g = 0 =

(cid:124)

Hz∗

z∗

⇒ −

(cid:124)

g

z∗

(cid:124)

g

z∗ = λ,

(28)

−

−

Hessf (z∗)[ξ], ξ
so
(cid:104)
(12):

(cid:105)

= λ + ξ(cid:124)Hξ

λ + λ1

≥

!
> 0. Where

!
> is guaranteed by gradient condition in

in “easy-case”, g(1)
Based on above discussion, we know λH,min ≥

= 0, so λ1 + λ

λ + λ1 > 0 and λH,max ≤

λ + λn.

(1) + g(1) = 0,

(λ1 + λ)z∗
= 0 and Hessian condition in (12) can be improved to λ1 + λ > 0.

(29)

B Supplementary Experiments on Trust Region Solver

λIn, where B

(0, 1), obviously λmin(H) = λ1 =

We sample an indeﬁnite random matrix by H = BB(cid:124)

−
λ. Afterwards we sample a vector g by gi

iid
∼
(0, 1).
N
it is totally ﬁne to ignore the hard case, because the probability is zero. By changing the value
, we plot the function value decrement with respect to number of
of λ in
iterations in Figure 6(left). As we can see, the iterates ﬁrst stay inside of the sphere (phase I) for
a few iterations and then stay on the boundary (phase II). To inspect how λ changes the duration
of phase I, we then plot the number of iterations it takes to reach phase II, under different λ values
shown in Figure 6(right). Recall in (23), number of iterations is bounded as a function of λ, which
can be further simpliﬁed to:

10, 30, 50, 70, 90, 110
{

1) and Bij

iid
∼ N

Rn

−

−

∈

(n

}

×

log(1 + λ
g(1)
|
|
log(1 + ηλ)

)

t∗

≤

=

log(1 + c1λ)
log(1 + c2λ)

,

(30)

where we set z(1)
log(1+c1λ)
log(1+c2λ) , we ﬁnd our bounds given by Lemma 6 is quite accurate.

0 = 0 to simplify the formula. By ﬁtting the data point with function T (λ) =

C Baselines

There are three baselines included in the experiments, namely random noise, degree-based poisoning
and PageRank-based poisoning. The ﬁrst baseline, random noise, is the simplest one. For continuous
(0, 1); for discrete label, we randomly
label, the perturbation is created by δy = dmax
1.
choose cmax indices
from
As to degree-based poisoning, we ﬁrst calculate the degree vector deg[i] = (cid:80)n
j=1 Sij of all
nodes, then for continuous label we load the perturbation weighted by degree, i.e.
=
δy[i]
|

k1, k2, . . . , kcmax}
{

(cid:15)
with (cid:15)
(cid:15)
(cid:107)
1, 2, . . . , n

and then set δy[ki] =

∼ N
}

(cid:107)
{

−

|

2

15

(cid:54)
(cid:54)
Figure 6: Left: Trust region experiment, we use solid lines to indicate iterations inside the sphere
and dash lines to indicate iterations on the sphere. By changing λ we can modify the function
curvature. Right: #Iteration it takes to reach sphere under different λ’s, we also ﬁt the curve by model
T = log(1+c1λ)

log(1+c2λ) derived in Eq. (23).

(cid:114)

dmax

(cid:80)n

deg2[i]
j=1 deg2[j] , and the sign of δy[i] is determined by the gradient of loss on δy at δy = 0.

Speciﬁcally:

sign(δy) = sign

(cid:32)

∂L(δy)
∂δy |

δy=0

(cid:33)

,

this makes sure that the direction is good enough to increase the prediction loss of SSL models. For
discrete label, we simply choose the largest cmax training labels to ﬂip: δy[i] =
1 if and only if
node-i has many neighboring nodes. This is to maximize the inﬂuence of perturbations.

−

Similarly, we can also a PageRank based poisoning attack, the only difference is that we use PageRank
to replace the degree score.

D Supplementary Experiments on Data Poisoning Attacks

In this section, we design more experiments on other cases that are not able show up in the main text.
The problem settings and datasets are the same as previous experiments.

D.1 Sparse and group sparse constraint

In reality, the adversary may only be able to perturb very small amount of data points, this requirement
renders sparse constraint. In speciﬁc, we consider

R1 =
R2 =

δy(cid:107)0 ≤
{(cid:107)
(cid:110) n
(cid:88)
I

i=1

cmax and

(cid:107)

δy(cid:107)2 ≤
cmax

,
dmax}
(cid:111)
.

(31)

∆x[i, :]
{

= 0

} ≤

The constraint on
row-wise group sparsity. Both
take regression task as an example to show the effectiveness.

R2 implies that only a limited number of data can be perturbed, so we enforce a
R2 can be added to regression/classiﬁcation tasks, below we

R1 and

For

R1, the optimization problem is essentially a sparse PCA problem

min
δy −

s.t.

(cid:13)
(cid:13)
(cid:13)(Duu −
cmax,

1
2
δy(cid:107)0 ≤

(cid:107)

Suu)−

1Sulδy

δy(cid:107)2 ≤
(cid:107)

(cid:13)
2
(cid:13)
(cid:13)
2
dmax.

(32)

For this kind of problem, many efﬁcient solvers were proposed during the past decades, including
threshold method [31], LASSO based method [32], or by convex relaxation [33].

In order to solve the sparse PCA problem, we adopt the LASSO based sparse PCA solver [32]. We
conduct the experiment on cadata and E2006 data, then plot the sparsity (measured by #nnz of

16

0255075100125150175100200300400500#Iteration to reach boundaryT=log(1+c1)log(1+c2)ExperimentTheory02004006008001000Iteration104103102101100101102Objective function=10=30=50=70=90=110321012x0.00.51.01.52.0Activation functionalpha=0.2alpha=0.6alpha=1.0ReLU(cid:54)
Figure 7: The effectiveness of (cid:96)0/(cid:96)2-mixed constraints in ﬁnding the sparse perturbations δy. For
cadata, we set nl = 1000, while for E2006 data nl = 300. The RMSE results of dense solutions (3a)
are marked with red dashed lines.

Figure 8: We ranked the perturbations ∆l by their (cid:96)2-norm, to see the decay rate of distortion.

δy) and corresponding RMSE in Figure 7. For comparison, we also include the RMSE when no (cid:96)0
sparsity constraint is enforced. Interestingly, we observe that the RMSE increases rapidly as δy is
relatively sparse, and later it gradually stabilizes before reaching the same RMSE of dense solution.
That is to say, when attackers have constraint on the maximal number of perturbation they could
make, our sparse PCA based solution is able to make a good trade-off between sparsity and RMSE.
(cid:107)2 and we
For
use proximal gradient descent to solve the optimization problem. By changing the hyper-parameter λ
we can indirectly change the group sparsity of ∆x. As above, we run the experiment on mnist17
data, result is shown in Figure 8.

R2, the har dconstraint is replaced with a group LASSO regularizer λ (cid:80)nl

∆l[i, :]

i=1 (cid:107)

D.2 Data poisoning attack on manifold regularization model

Apart from label propagation model for semi-supervised learning, our method can also be seamlessly
applied to manifold regularization method [4]. Manifold regularization based SSL solves the following
optimization problem

f ∗ = arg min

f

∈F

nl(cid:88)

i=1

(cid:96)(cid:0)f (xi), yi

(cid:1) + λ
(cid:107)

f

2 + β
(cid:107)

nl+nu(cid:88)

i,j=1

Sij

(cid:0)f (xi)

f (xj)(cid:1)2

,

−

(33)

where
function. The model family
networks. If we limit our scope to the linear case, then (33) has a closed form solution:

is the set of model functions. (xi, yi) is the i-th data pair in (X, y), (cid:96)(ˆy, y) is the loss
ranges from linear models f (x) = w(cid:124)x to very complex deep neural

F

F

w∗ = (X

(cid:124)
l Xl + λI + βX

(cid:124)

17

LX)−

1X

(cid:124)
l yl = P yl

(34)

02004006008001000#Non-zeroelements0.200.220.240.260.28RMSEDenseδycadataSparseδy50100150200250300#Non-zeroelements0.160.180.200.22RMSEDenseδyE2006Sparseδy0100200300400500k-largestdistortion01234Normofdistortion∥δx∥where Xl is the feature matrix of all labeled nodes, X is the feature matrix of labeled and unlabeled
nodes, L is the graph Laplacian.

Manifold regularization term in (34) enforces two nodes that are close to each other (i.e. large Sij)
to hold similar labels, and that is similar to the objective of label propagation. This motivates us to
extend our algorithms for attacking label propagation to attack manifold regularization based SSL.
As an example, we discuss poisoning attack to manifold regularization model for regression task,
where the problem can be formulated as

min
(cid:107)≤

δy

(cid:107)

dmax −

1
2 (cid:107)

XuP (yl + δy)

2
2.

yu(cid:107)

−

(35)

Clearly, Eq. (35) is again a non-convex trust region problem, and we can apply our trust region
problem solver to it. For experiment, we take regression task on cadata as an example, different
from label propagation, manifold regularization learns a parametric model fw(x) that is able to
generalize to unseen graph. So for manifold regularization we can do label poisoning attack in both
transductive and inductive settings. The experiment result is shown in Figure 9.

Figure 9: Experiment result of manifold regularization on cadata, here we set nl = 500, nu = 3500
and the rest ng = 4000 data are used for inductive learning.

In this experiment, we do both transductive setting, using the test set
and inductive setting, on a brand new set
Xind, yind}
{
can see that for both settings the label poisoning attack algorithm has equally good performance.

as in label propagation,
that never been accessed in training stage. We

Xu, yu}
{

18

01234dmax0.140.160.180.200.220.24RMSEcadataTransductiveInductive