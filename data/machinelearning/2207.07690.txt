Support Vector Machines with the Hard-Margin Loss:
Optimal Training via Combinatorial Benders’ Cuts

´Italo Santana1, Breno Serrano2, Maximilian Schiﬀer2,3, Thibaut Vidal1,4,5

1 Department of Computer Science, Pontiﬁcal Catholic University of Rio de Janeiro

2 TUM School of Management, Technical University of Munich

3 Munich Data Science Institute, Technical University of Munich

4 CIRRELT & Department of Mathematics and Industrial Engineering, Polytechnique Montr´eal

5 SCALE-AI Chair in Data-Driven Supply Chains

Abstract. The classical hinge-loss support vector machines (SVMs) model is sensitive to outlier

observations due to the unboundedness of its loss function. To circumvent this issue, recent studies

have focused on non-convex loss functions, such as the hard-margin loss, which associates a constant

penalty to any misclassiﬁed or within-margin sample. Applying this loss function yields much-needed

robustness for critical applications but it also leads to an NP-hard model that makes training diﬃcult,

since current exact optimization algorithms show limited scalability, whereas heuristics are not able to

ﬁnd high-quality solutions consistently. Against this background, we propose new integer programming

strategies that signiﬁcantly improve our ability to train the hard-margin SVM model to global optimality.

We introduce an iterative sampling and decomposition approach, in which smaller subproblems are used

to separate combinatorial Benders’ cuts. Those cuts, used within a branch-and-cut algorithm, permit to

converge much more quickly towards a global optimum. Through extensive numerical analyses on classi-

cal benchmark data sets, our solution algorithm solves, for the ﬁrst time, 117 new data sets to optimality

and achieves a reduction of 50% in the average optimality gap for the hardest datasets of the benchmark.

Keywords. Support vector machines, Hard margin loss, Branch-and-cut, Combinatorial Benders’

cuts, Sampling strategies

∗ Correspondence to: isantana@inf.puc-rio.br, thibaut.vidal@cirrelt.ca

2
2
0
2

l
u
J

5
1

]

G
L
.
s
c
[

1
v
0
9
6
7
0
.
7
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
1. Introduction

Support vector machines (SVMs) are among the most popular classiﬁcation models due to their

simplicity and solid theoretical foundations from statistical learning [34, 47, 54]. Application ﬁelds

of SVMs include, among others, image classiﬁcation, bioinformatics, handwritten digits recognition,

face detection, and generalized predictive control [6, 9, 16]. Beyond this, SVMs are regularly used

as elementary building blocks of sophisticated AutoML pipelines [21]. They achieve state-of-the-art

results for a variety of applications, especially for large-scale datasets [10, 13, 30].

In its simplest form, an SVM can be deﬁned as follows. Let (X, y) =

which each xi

∈

Rm is an m-dimensional feature vector, and each yi

Then, an SVM seeks a hyperplane

=

x
{

∈

H

Rm : w

·

x + b = 0

}

xi, yi

{
1, 1

be a training set in

}
is its associated class.

∈ {−

}

that optimizes the following objective:

min
w,b

1
2 ||

w

2 + C
||

n
(cid:88)

i=1

f (yi(w

·

xi + b)).

(1)

The ﬁrst term of Equation (1) acts as a regularization term and indirectly maximizes the margin of

the SVM [47], whereas the second term ensures ﬁdelity to the data and penalizes misclassiﬁed samples,

with coeﬃcient C balancing the two terms. Accordingly, the objective establishes a trade-oﬀ between

maximizing the hyperplane’s margin and minimizing the concomitant misclassiﬁcation error. The loss

function f varies with respect to the studied problem variant. In the classical SVM with hinge loss

(SVM-HL), we deﬁne

f (u) := fHinge(u) = max
{

0, 1

,
u
}

−

(2)

such that a misclassiﬁed sample, i.e., a sample for which yi(w

xi + b) < 1, directly increases the

·

objective value of Equation (1) proportionally to its error (see Figure 1).

However, while the classical convex SVM-HL permits fast training and scalability, it is also known to

lack robustness in the presence of misclassiﬁed samples or outliers since its loss function is unbounded.

As a drawback, the trained model can be severely aﬀected by these outliers, preventing its application

in several domains, especially for high-stakes decisions where robustness is critical [42].

In light of this, some works have considered the use of bounded but non-convex loss functions in

an attempt to gain robustness [see, e.g., 5, 29, 53]. In particular, the SVM with hard-margin loss

(SVM-HML) uses

f (u) := fHard(u) =

(cid:40)
1
0

if u < 1
otherwise,

(3)

such that any misclassiﬁed sample (or sample within the margin) leads to a unit penalty (see Figure 2),

1

therefore limiting the inﬂuence of misclassiﬁed samples on

and increasing classiﬁcation robustness.

H

Figure 1: Hinge loss function

Figure 2: Hard-margin loss function

However, this much-needed robustness comes at the expense of computational eﬃciency since

training SVM-HML requires solving a mixed-integer quadratic problem (MIQP) and is NP-hard [11].

This hardness, but more especially the inability to eﬃciently solve the SVM-HML, limits its current use.

Training SVM-HML models to global optimality is currently only achievable for very small datasets,

whereas current heuristics for training do not consistently ﬁnd high-quality hyperplanes.

In this work, we contribute towards addressing those challenges and pave the way toward more

eﬃcient global optimization algorithms. We propose new mixed-integer programming approaches to

train SVM-HML models to global optimality. We exploit the problem’s structure to devise eﬃcient

decomposition techniques, relying on subsets of the samples to generate Combinatorial Benders’ (CB)

cuts quickly. More speciﬁcally, the contribution of this paper is fourfold:

• We show that CB cuts can be successfully exploited to generate useful inequalities for SVM-HML.
• We introduce sampling strategies that permit to quickly generate a diversiﬁed pool of cuts. We

eﬀectively embed these cuts within a branch-and-cut algorithm, leading to an eﬃcient training

algorithm that can achieve global optimality.

• We conduct an extensive numerical campaign to measure the performance of our training

approach and the impact of important design choices. As seen in our experiments, this algorithm

signiﬁcantly improves the current status-quo regarding the solution of SVM-HML, solving for

the ﬁrst time 117 new data sets to optimality and achieving a reduction of 50% in the average

optimality gap over previous approaches for the hardest datasets of the benchmark.

• Generally, our study underlines the beneﬁts of applying cutting-edge mixed-integer programming

techniques to combinatorial optimization problems that arise when training non-convex machine

learning models.

2

-10 1 0 1 2 Margin yi(w·xi+b)HfHINGE-10 1 0 1 Margin yi(w·xi+b)HfHMThe remainder of this paper is organized as follows. Section 2 brieﬂy reviews the related literature.

Section 3 introduces the proposed methodology. Section 4 details our computational experiments, and

Section 5 concludes this paper.

2. Related Literature

A vast body of literature on SVMs exists, covering various topics such as applications, training

algorithms, and loss functions. For the sake of brevity, we focus on recent contributions to training

algorithms for SVM-HL as well as works on SVMs with non-convex loss functions, namely SVM-HML

and the SVM with ramp loss (SVM-RL).

The training problem for the classical SVM-HL can be cast and eﬃciently solved as a continuous

convex quadratic programming problem. Existing solution approaches typically detect and ﬁx violations

of ﬁrst-order optimality conditions, leading to a series of small subproblems with few variables. A

classical approach, called Sequential Minimal Optimization (SMO) is used in several state-of-the-art

implementations [10, 13, 30, 40] and consists of solving a restricted problem of only two variables at each

iteration. Still, some algorithms have also exploited larger subproblems [see, e.g., 20, 30, 48]. Training

algorithms for SVM-HL are quite diverse and base on data-selection concepts [32, 33], geometric

methods [34] and heuristics [51]. For a detailed presentation of algorithms for the SVM-HL and its

variants, we refer the reader to the surveys of Carrizosa and Morales [8], Cervantes et al. [9], and Wang

et al. [50], as well as to the book of Sch¨olkopf et al. [43].

Despite its widespread use, the sensitivity of SVM-HL to outliers has regularly raised obstacles

when dealing with critical applications. Consequently, a part of recent research has explored the

possibility of using non-convex loss functions to gain robustness. Brooks [5] focused on two non-convex

loss functions in particular: the SVM-HML [36, 37, 39] and the SVM-RL [14, 45, 46]. Both of these

functions are bounded, such that the contribution of each sample to the objective is limited. In the

SVM-RL model, any sample within the margin receives a linear penalty proportional to its distance to

the margin (a value between 0 and 2C), whereas any misclassiﬁed sample outside the margin receives a

ﬁxed penalty of 2C. In the SVM-HML, the penalty of any misclassiﬁed or within-margin sample is

simply ﬁxed to a constant C.

The solution algorithm proposed by Brooks [5] solves the training problem as a mixed-integer

quadratic programming (MIQP) using state-of-the-art branch-and-cut solvers. For the SVM-HML

and SVM-RL, the authors rely on indicator constraints representing logical implications between a

binary variable representing the status of each sample (misclassiﬁed or not) and a linear constraint

3

that evaluates its relative position from the separating hyperplane. However, it is well known that

such constraints can be reformulated in linear form using a “big-M” constant, but doing so without

carefully tuning the value of the M constant typically leads to an ineﬀective formulation with a weak

linear relaxation, impeding an eﬃcient solution by branch-and-cut [52].

For the SVM-RL setting, Huang et al. [29] explored the ramp loss function with (cid:96)1-penalty, resulting

in a piecewise linear programming problem. Later, Belotti et al. [3] compared diﬀerent formulations

for the logical constraints and concluded that aggressive bound-tightening techniques are necessary for

a successful solution approach. The strategy derived from their studies has been since implemented as

a standard routine for handling such constraints in the commercial solver CPLEX for MILP/MIQPs.

Finally, Baldomero-Naranjo et al. [2] tightened the M constants by solving sequences of continuous

problems and Lagrangian relaxations. For the SVM-HML, Poursaeidi and Kundakcioglu [41] proposed

hard-margin loss formulations within the context of multiple-instances classiﬁcation, a setting in which

class labels are deﬁned as sets. Finally, in [28], the hard-margin loss was transformed into a re-scaled

hinge loss function for imbalanced noisy classiﬁcation.

Concluding, only a few studies have attempted to improve the state-of-the-art training algorithms

for SVM-HML after the seminal work of Brooks [5], often by concentrating on the handling of the

logical constraints and the proper calibration of the M constants. Despite this progress, optimal

training remains limited to data sets counting a few hundred samples. To improve this status-quo, we

investigate a diﬀerent approach, which consists of the separation of CB cuts and their combination

with classical logical constraints to achieve a valid problem formulation with a better linear relaxation.

Moreover, to improve computational eﬃciency, we rely on sampling techniques for a fast generation of

diverse cuts.

3. Methodology

We ﬁrst recall the classical mathematical programming formulations for the SVM variants that are

of interest for our study, namely SVM-HL and SVM-HML, and then proceed with a description of our

algorithmic approach.

3.1. Descriptive formulations

The SVM with hinge loss (also known as soft-margin SVM) associates to misclassiﬁed and within-margin

samples a penalty that is proportional to their distance in relation to the “correctly classiﬁed margin”

of the found hyperplane (see Figure 1).

4

Let w

∈

Rm be a vector of real variables that represents the coordinates of the hyperplane, let b be

its intercept to the origin, and let ξi represent the misclassiﬁcation penalty of sample i. With these

notations, the training problem for SVM-HL can be mathematically formulated as:

min
w,b,ξ

1
2 ||

w

2 + C
||

n
(cid:88)

i=1

ξi

s.t. yi(w

ξi

≥

·
0,

xi + b)

1

ξi,

≥
−
1, . . . , n

.
}

i

∈ {

i

1, . . . , n

∈ {

}

(4)

(5)

(6)

2 and minimizes the total
||

2 ||

Objective (4) seeks a maximum margin separator through the term 1

w

misclassiﬁcation penalty C (cid:80)n

i=1 ξi, where C > 0 is a constant that balances both parts of the objective.

Moreover, Constraints (5) calculate the misclassiﬁcation penalty of each sample.

In the SVM with hard-margin loss, misclassiﬁed samples are associated a ﬁxed penalty cost. Binary

variables z are used to indicate whether a sample i is misclassiﬁed or within the margin (zi = 1), or

correctly classiﬁed (zi = 0). The SVM-HML can be formulated as follows:

min
w,b,z

1
2 ||

w

2 + C
||

n
(cid:88)

i=1

zi

s.t.

(zi = 0)

yi(w

·

⇒

xi + b)

1

≥

i

1, . . . , n

∈ {

zi

0, 1

∈ {

}

i

1, . . . , n

∈ {

}

}
.

(7)

(8)

(9)

In this formulation, the penalty term in the objective (7) is directly proportional to the number

of misclassiﬁed samples. Constraints (8), represented as logical constraints, ensure that zi = 0 only

if sample i is correctly classiﬁed. These constraints are strictly speaking not part of the semantic

of a MILP/MIQP, but they could be directly transformed into a linear constraint by using a big-M

constant and imposing yi(w

xi + b)

1

−

≥

·

M zi. The drawback of such a reformulation is that it leads

to a formulation that provides notably weaker linear-relaxation bounds, rendering branch-and-bound

algorithms relatively ineﬃcient.

3.2. Combinatorial Benders’ cuts

To optimally solve the SVM-HML, we propose a solution method based on Benders’ decomposition [4].

In its canonical form, this strategy exploits the structure of a mixed-integer linear program and splits

its variables into two subsets. The ﬁrst subset of integer or continuous variables (sometimes called

5

“complicating variables”) is selected in such a way that ﬁxing them either decomposes or reduces the

complexity of the resulting problem. The remaining variables should be continuous. The method then

works by decomposing the original problem into a master problem (MP), solved over the complicating

variables, and a subproblem (SP), solved as a linear program over the remaining continuous variables.

The algorithm iteratively produces an incumbent solution of the MP and uses the dual of the SP to

assess its feasibility. If the SP determines that the incumbent solution is feasible, then this information

is integrated into the MP in the form of an additional CB cut, which eliminates the infeasible incumbent

solution.

The studies of Hooker and Ottosson [27] and Codato and Fischetti [12] paved the way towards

eﬃcient applications of Benders’ decomposition to a broad class of mixed-integer programming problems

(MIPs) with logical constraints. Notably, the so-called CB approach leads among others to a new

solution paradigm for models of the following form:

min c

(cid:124)

z

s.t.

(zi = 0)

0, 1

z

y

∈ {

Y,

∈

(cid:124)

ai

y

di

≥

i

1, . . . , n

∈ {

}

(10)

(11)

(12)

(13)

⇒
n

}

with binary complicating variables z, continuous variables y in a polytope Y (i.e., respecting a set of

linear inequalities), and linear weights c (applied only on the coeﬃcients of z) to calculate the objective.

In a CB approach, Model (10–13) is reformulated as follows:

min c

(cid:124)

z

s.t.

(cid:88)

i∈S

zi

1

≥

z

0, 1
}

∈ {

n,

mis

S

∈ S

(14)

(15)

(16)

where

S

mis is the collection of all inclusion-minimal infeasible subsystems (MIS) of rows:

mis =

S



S


(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:124)

(cid:124)

y

y

ai
{
ai
{

di

di

i
∀
i
∀

∈

∈

≥

≥

S, y

ˆS, y

Y

Y

}

}

∈

∈

is infeasible




is feasible

ˆS
∀

⊂

S



.

(17)

Notably, this formulation no longer contains logical implications or big-M terms. However, it includes

6

an exponential number of constraints. For this reason, the CB approach consists of dynamically detecting

and adding Constraints (15). As in a classical Benders’ decomposition, the method alternates between

solving the master problem with only a subset of the constraints

mis found so far, and solving

S ⊂ S

a subproblem to identify new violated constraints that cut the incumbent solution of the master in

case of infeasibility. Finally, we observe that

S

mis does not need to be restricted to “inclusion-minimal”

subsets of rows to yield a valid formulation, but doing so signiﬁcantly reduces the number of constraints

in the set.

Despite its successful application in a variety of settings [1, 15, 19], the aforementioned CB framework

is not directly applicable to problems with objective functions that contain both complicating variables z

and continuous variables y. This is unfortunately the case for the SVM-HML, due to the occurrence of

the continuous variables w and b in the objective and the logical implications. To circumvent this issue,

we introduce a weaker set of CB cuts in conjunction with the original logical implication constraints to

design an eﬀective solution method. In this case, the cuts do not carry the burden of ensuring the

model’s validity, but nevertheless contribute to strengthening the formulation to obtain a better linear

relaxation and improve the solution process.

3.3. Combinatorial Benders’ cuts and the SVM-HML

We ﬁrst start by describing a direct application of CB to the SVM-HML and by analyzing its

shortcomings. This would lead to the following formulation:

n
(cid:88)

i=1

zi

min
W,z

s.t.

W 2 + C

1
2

(cid:88)

i∈S

zi

1

≥

n

z

0, 1
}

∈ {

with

mis
svm-hml(W ) =

S



S


(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

yi(w
{
yi(w
{

·

·

xi + b)

xi + b)

1

1

i
∀
i
∀

∈

∈

≥

≥

S,

ˆS,

w

||

2
||

w

||

2
||

≤

≤

S

mis
svm-hml(W )

∈ S

(18)

(19)

(20)

is infeasible




is feasible

ˆS
∀

⊂

S



.

(21)

W 2

}
W 2

}

As seen in this formulation, W appears in the objective and also characterizes the set of Benders’

cuts. Unfortunately, the resulting formulation can no longer be practically solved as a MILP or MIQP

due to this dependency. To remedy this issue, we leverage Property 1.

Property 1. Let Wub be a valid upper bound on W on any optimal solution of Problem (18–21).

Then,

mis
svm-hml(Wub) is also set of valid inequalities for the SVM-HML.

S

7

Proof. Consider S

mis
svm-hml(Wub). Due to the deﬁnition of

mis
svm-hml(Wub),

S
∈ S
is an infeasible subsystem. Given that all optimal solutions of Problem (18–21)

≥

∈

{

·

yi(w

xi + b)

1

i
∀

S

||

w

W 2

with

2
||
satisfy W
therefore at least one sample i in S must be misclassiﬁed, implying that (cid:80)

ub}
≤
Wub, then

xi + b)

S with

yi(w

2
||

W 2

i
∀

≥

≤

≤

w

∈

||

{

}

1

·

is also an infeasible subsystem, and

i∈S zi

1 is a valid inequality.

≥

With this property, the dependence upon parameter W can be avoided as soon as a valid upper

bound is known. The quality of the upper bound also impacts the strength of the valid inequalities

obtained. Given an initial feasible solution of the SVM-HML problem with value Φub (obtained, for

example, with a heuristic for this problem), we can use Wub = 2√Φub since the two terms of the

objective are positive.

Finally, we opted to further relax the CB cuts by using

}
ub in Equation (21). Our experimental analyses have shown that this permitted

∈ {

≤

W 2

Wub ≤
−

wj

Wub for j

1, . . . , m

instead of

w

||

2
||

≤

a faster cut separation with only a limited impact on the strength of the formulation. Overall, we

will use the resulting valid inequalities in combination with the original formulation and the tightened

bounds on the wj coeﬃcients, leading to the following model:

min
w,z

1
2 ||

w

2 + C
||

n
(cid:88)

i=1

zi

s.t. (zi = 0)

yi(w

xi + b)

·
Wub

wj

≤

⇒
Wub ≤
1
zi

≥

−
(cid:88)

i∈S

n

z

0, 1
}

∈ {

with

mis-ub
svm-hml =

S



S


(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1

≥

i

j

S

1, . . . , n

}
1, . . . , m

}

∈ {

∈ {

mis-ub
svm-hml

∈ S

(22)

(23)

(24)

(25)

(26)

yi(w

yi(w

{

{

·

·

xi + b)

xi + b)

1

1

i
∀
i
∀

∈

∈

≥

≥

S,

ˆS,

Wub ≤
−
Wub ≤
−

wj

wj

Wub ∀
j
j
Wub ∀

}

}

≤

≤

is infeasible




is feasible

ˆS
∀

⊂

S



. (27)

With this problem formulation in mind, we will focus on our general solution approach and the

separation of the CB cuts in the following.

3.4. General Solution Approach

Our solution approach unfolds in three steps:

Step 1. Finding an initial upper bound Wub;

Step 2. Solving a simpliﬁed formulation by branch-and-cut to obtain a cut set

mis-ub
svm-hml;

S

8

Step 3. Solving Problem (22–27) with the set of cuts identiﬁed in the previous step.

We note that the generation of the cuts is done in a separate phase (Step 2). Our computational

experiments have shown that it is more eﬀective to generate a set of cuts beforehand, and then allow

the MILP solver (CPLEX) to use its default settings when solving the complete model with these cuts,

instead of dynamically providing additional cuts as the search progresses. We now proceed with a

detailed description of each step of the algorithm.

Step 1 – Initial bound. We start by solving an SVM with hinge loss, cast as a continuous quadratic

program through Equations (4–6), and collect the resulting value of the variables (w(cid:48), b(cid:48), ξ(cid:48)) deﬁning

the hyperplane. With this hyperplane, we obtain an associated SVM-HML solution by setting zi =

ξi

(cid:100)

(cid:101)

and calculate the resulting Wub objective value.

Step 2 – Separation of the Combinatorial Benders’ cuts. Next, we consider Problem (22–27)

excluding the logical Constraints (23) while dynamically generating Constraints (25). After excluding

the logical constraints,

w

||

2 is free to take a value of 0, and therefore the ﬁrst term of the objective
||

vanishes. The resulting problem is a variant of the linear separator problem [31, 47], which we will

only use to generate CB cuts for the subsequent solution of the SVM-HML. To obtain the cuts, we

apply a branch-and-cut scheme on the following master problem:

Master: min

z

n
(cid:88)

i=1
(cid:88)

i∈S

zi

zi

1

≥

z

0, 1
}

∈ {

n.

S

∈ S

(28)

(29)

(30)

At each branching node, the linear relaxation of the master problem is solved and the set

0 of variables

I

with values zi = 0 are identiﬁed. Based on this set of variables, we deﬁne the following feasibility

subproblem to check if a violated CB cut exists:

Subproblem:

yi(w

xi + b)

1

≥

·
Wub ≤

−

wj

≤

Wub

j

∈ {

i

∈ I
1, . . . , m

0

.
}

(31)

(32)

If this subproblem admits a feasible solution, then no CB cut needs to be added. Otherwise, the

9

subproblem is infeasible and we can obtain an inclusion-minimal infeasible subsystem (MIS) of indices
from CPLEX, giving us a new set of variables ¯
I

. As seen in Algorithm 1,

which can be added to

S

this process is repeated at each node of the branch-and-cut tree until no additional cut can be found.

1

S ← ∅
2 repeat

Solve the linear relaxation of Problem (28–30) with

S

0 admits a feasible solution then

I

3

4

5

6

7

8

9

z

←
0

Indices such that zi = 0

←

I
if Problem (31–32) with

break

else
¯
I ←
S ← S ∪

¯
I

MIS from Problem (31–32)

Algorithm 1: Generation of Combinatorial Benders’ cuts on a given node

To avoid spending excessive time on the solution of this auxiliary problem and to generate a

diversiﬁed set of CB cuts, we rely on sampling strategies. Until a maximum time limit of TS, we

randomly select subsets of r = min
{

n/2, 50

}

samples, and apply the aforementioned branch-and-cut

and cut separation methodology. Only afterward we repeat this process in a ﬁnal run, considering all

the samples and hot starting with all the CB cuts already found. We stop the cut separation as soon

as a time limit TB is attained.

Step 3 – Solution of the SVM-HML. Finally, we proceed with solving the complete Problem (22–

27), using the union of the CB cuts that have been found in Step 2. These cuts are included as lazy

constraints to avoid any overhead due to the number of cuts. Furthermore, we warm start from the

solution found in Step 1, and use the default settings of CPLEX, except for the locally valid implied

bounds parameter, which is set to very aggressively as suggested in [3]. The solution approach is

run until it proves optimality or reaches a maximum time limit of TMax. Note that this time limit

encompasses all the steps of our method, such that it is possible to limit the total computational time

and compare our method with other algorithms.

4. Computational Experiments

We conduct extensive experimental analyses to evaluate the performance of the proposed approach,

denoted as CB-SVM-HML, on a diverse set of benchmark instances. As an experimental baseline for

10

this comparison, we rely on a reimplementation of the branch-and-cut approach of Brooks [5]. The goal

of our experiments is (i) to compare the performance of these algorithms in terms of computational

time and optimality gaps, and (ii) to evaluate the impact of the CB cuts and of the proposed search

strategies based on sampling.

4.1. Data and Experimental Setup

To evaluate the performance of our algorithm, we use the same benchmark as in Brooks [5], divided

into three sets: UCI, Type A, and Type B. The UCI set consists of 11 heterogeneous datasets from

the UCI machine learning repository [17], which were preprocessed by Brooks [5]. Table 1 lists the

number of samples (n) and features (m) of these datasets. Type-A and Type-B datasets have been

constructed by Brooks [5] using simulated data with a controlled number of outliers. Type-A datasets

contain outliers that are clustered together, and generally distant from the rest of the data points.

In contrast, Type-B datasets contain outliers that are more evenly distributed. Type-A and Type-B

sets both include 12 datasets with diﬀerent number of samples n =

60, 100, 200, 500

and features

}

{

m =

2, 5, 10
}

{

. Finally, for all of the considered benchmark datasets, Brooks [5] obtained ﬁve diﬀerent

instances with diﬀerent random data points. Overall, this gives us (11 + 12 + 12)

5 = 175 instances

×

to evaluate SVM-HML solution methods. Additionally, for each of these instances, we will consider

C

1, 10, 100, 1000, 10000
}

∈ {

each algorithm.

for the penalty factor as done in [5], giving a total of 875 instances for

Name

adult
australian
breast
bupa
german
heart
ionosphere
pima
sonar
wdbc
wpbc

n m

400
366
383
193
400
152
196
400
116
319
108

77
45
9
6
24
20
33
8
60
30
30

Table 1: Characteristics of the UCI datasets

All the algorithms considered in this study have been implemented in C++ and use the CPLEX 12.9

callable library. The experiments were run on an Intel Xeon E5-2620 2.1 GHz processor machine with

11

128 GB of RAM and CentOS Linux 7 (Core) operating system. All the source code and scripts needed

to reproduce these experiments are provided at https://github.com/vidalt/Hard-Margin-SVM.

4.2. Performance of CB-SVM-HML

In the ﬁrst set of experiments, we compare the results of the proposed CB-SVM-HML with those

of the baseline algorithm of Brooks [5]. We use the same experimental conditions, and therefore run

each algorithm until a time limit of TMax = 600 seconds for each instance and value of C. The time

dedicated to the separation of CB cuts in CB-SVM-HML is set to TB = TS = 30 seconds. In other

words, 5% of the total time is dedicated to the separation of CB on subsets of samples, and 5% of the

time on the complete problem. As shown in our sensitivity analyses in Section 4.3, this amount of

time already permits to separate a large number of cuts. Allocating more computational time for cut

separation did not further improve the overall search process.

Tables 2 to 6 report, for each algorithm over all C values, the number of instances solved to

optimality (# Opts), the average optimality gap (Gap (%)), and the average computational time in

seconds (Avg Time). In the last line, Overall provides the sum of all values for columns # Opts, and the

average of all values for columns Gap (%) and Avg Time. In Tables 3 and 4, the results of the instance

of Type A and Type B are aggregated per number of samples n

, whereas in
}
Tables 5 and 6, the results are aggregated according to the dimension of the feature space m

2, 5, 10
}
The detailed results for each instance are additionally provided in the same repository as the source code.

60, 100, 200, 500

∈ {

∈ {

.

Brooks [5]
# Opts Gap (%) Avg Time

CB-SVM-HML
# Opts Gap (%) Avg Time

Overall

149/275

33.32

295.31

152/275

20.70

280.17

Table 2: Performance comparison on the UCI instances

n

60
100
200
500

Brooks [5]
# Opts Gap (%) Avg Time

CB-SVM-HML
# Opts Gap (%) Avg Time

75/75
42/75
4/75
0/75

0.00
11.66
56.83
88.17

39.16

2.87
325.71
582.13
600.00

377.33

75/75
61/75
32/75
11/75

179/300

0.00
4.26
18.21
36.05

14.63

32.83
170.76
395.30
542.28

285.29

Overall

121/300

Table 3: Performance comparison on the Type-A instances – grouped by number of samples n

12

n

60
100
200
500

m

2
5
10

m

2
5
10

Brooks [5]
# Opts Gap (%) Avg Time

CB-SVM-HML
# Opts Gap (%) Avg Time

74/75
26/75
1/75
0/75

0.28
26.63
65.59
90.76

45.82

72.01
404.67
595.16
600.00

417.61

73/75
49/75
24/75
11/75

157/300

0.45
8.55
22.51
40.01

17.88

63.87
266.52
437.02
529.61

324.26

Overall

101/300

Table 4: Performance comparison on the Type-B instances – grouped by number of samples n

Brooks [5]
# Opts Gap (%) Avg Time

CB-SVM-HML
# Opts Gap (%) Avg Time

54/100
40/100
27/100

28.15
40.37
48.98

39.16

297.02
388.24
446.72

377.33

86/100
52/100
41/100

179/300

1.84
13.87
28.18

14.63

145.27
322.75
387.85

285.29

Overall

121/300

Table 5: Performance comparison on the Type-A instances – grouped by number of features m

Brooks [5]
# Opts Gap (%) Avg Time

CB-SVM-HML
# Opts Gap (%) Avg Time

51/100
26/100
24/100

32.14
48.06
57.26

45.82

305.96
455.49
491.38

417.61

83/100
47/100
27/100

157/300

1.60
16.13
35.90

17.88

148.83
358.51
465.42

324.26

Overall

101/300

Table 6: Performance comparison on the Type-B instances – grouped by number of features m

13

As seen in these experiments, CB-SVM-HML generally achieves better performance than the

baseline algorithm of Brooks [5]. In general, it solves more instances to optimality with the same

time limit (488/875 compared to 371/875) and achieves smaller optimality gaps (17.65% on average

compared to 39.61%). CB-SVM-HML also found optimal solutions for some instances with 500 samples.

Instances of this size could not be solved to proven optimality by previous approaches. Observing

the results for instances with a diﬀerent number of features m, we see that CB-SVM-HML performs

especially well on low-dimensional problems (i.e. when m

) since MIS are generally smaller in
2, 5
}

∈ {

this regime, leading to stronger CB cuts involving fewer variables. Generally, our method improved

upon the baseline for all values of m.

In terms of computational time, CB-SVM-HML achieves optimality or attains smaller gaps faster

than the baseline algorithm on all instances except those with n = 60 samples. For those small instances,

the diﬀerence of performance comes from our parametrization choices: the current cut-separation

algorithm uses at least 5% of the time (30 seconds) separating cuts on randomly-generated subproblems

including diﬀerent subsets of the samples. As a consequence, the method’s computational time is

bounded below by 30 seconds, whereas the baseline algorithm sometimes solves the complete problem

in shorter time for very small instances. One way to reduce computational eﬀort in those cases could

involve using a variable separation time that depends on the size of the instance, or setting a limit on

the number of subproblems considered in the sampling phase.

We complete this analysis in Figure 3 with a ﬁne-grained study of the optimality gaps of the

methods as a function of the number of samples n

in Type-A and Type-B instances.

∈ {
For each value of n and for each method, we represent the optimality gaps (over the 75 instances) as

60, 100, 200, 500
}

a boxplot. The boxes indicate the ﬁrst and third quartile, and the whiskers extend to 1.5 times the

interquartile range. Outliers that extend beyond this range are depicted as small dots.

As can be seen, CB-SVM-HML achieves better optimality gaps than the baseline algorithm for all

values of n. Especially when n = 500, the baseline method terminates with optimality gaps as high as

85% in most cases, whereas CB-SVM-HML achieves much smaller gaps. This conﬁrms the positive

impact of the CB cuts, which signiﬁcantly tighten the formulation and decrease the optimality gap.

4.3. Impact of the time dedicated to cut separation

Finally, we measure the impact of the amount of computational eﬀort dedicated to separating the

CB cuts on solution quality. We therefore compare eight diﬀerent conﬁgurations of CB-SVM-HML

with diﬀerent time budgets for TS and TB to evaluate the impact of CB-cuts separation on randomized

14

Figure 3: Optimality gaps on Type-A and Type-B instances as a function of the number of samples

sample subsets (during TS) as well as on the complete problem (during TB). For all conﬁgurations,

we ﬁx the total time limit to TMax to 600 seconds. Figure 4 shows the results of this experiment as

boxplots, where each plot corresponds to the optimality gaps obtained by a method conﬁguration on

an instance set. Additionally, we indicate the number of optimal solutions found by each conﬁguration

on top of each boxplot.

As can be seen, the conﬁguration (5%; 5%) (which is the reference conﬁguration used in Section 4.2)

achieved the best performance among all the considered conﬁgurations. Moreover, all the conﬁgurations

with combinatorial Benders’ cuts achieved better gaps and number of optimal solutions than the

baseline algorithm of Brooks [5]. Comparing conﬁguration (TS, TB) = (10%, 10%) with conﬁgurations

(20%, 0%) and (0%, 20%), we notice that allocating the complete separation-time budget to the complete

problem or to the randomized subproblems is not as eﬀective as dividing the available time between

these two approaches. Finally, allocating a larger amount of time to the separation process, as in

conﬁgurations (30%, 30%) and (40%, 40%), also leads to a performance deterioration since there is a

larger number of cuts, which leads to a heavier model, such that the remaining time to solve Step 3 is

insuﬃcient to obtain a good solution quality.

As seen in Table 7, the previous observations are conﬁrmed at 0.05 signiﬁcance level by paired-

samples Wilcoxon tests. All versions of CB-SVM-HML obtained optimality gaps which were signiﬁcantly

smaller than the baseline method of Brooks [5] without cuts. Moreover, the conﬁguration of CB-SVM-

HML with (TS, TB) = (5%, 5%) obtained better results than all conﬁgurations, except (20%, 20%) for

which no statistical diﬀerence was observed.

15

025507510060100200500Number of samples (n)Optimality gap (%)AlgorithmsBaselineCB−SVM−HML025507510060100200500AlgorithmsBaselineCB−SVM−HMLFigure3:OptimalitygapresultsofType-AandType-Binstancesgroupedbynforallvaluesofn.Especiallywhenn=500,thebaselinemethodterminateswithoptimalitygapsashighas85%inamajorityofthecases,whereasCB-SVM-HMLachievesmuchsmallergaps.ThisconﬁrmsthepositiveimpactoftheCBcuts,whichsigniﬁcantlytightentheformulationanddecreasetheoptimalitygap.4.3.ImpactoftheseparationtimebudgetFinally,wemeasuretheimpactoftheamountofcomputationaleﬀortdedicatedtoseparatingtheCBcutsonsolutionquality.WethereforecompareeightdiﬀerentconﬁgurationsofCB-SVM-HMLwithdiﬀerenttimebudgetsforTSandTBtomeasuretheimpactoftheeﬀortspentseparatingCB-cutsonrandomizedsamplesubsets(duringTS)aswellasonthecompleteproblem(duringTB).ThetotaltimelimitismaintainedﬁxedtoTMaxto600seconds.Figure4displaystheresultsofthisexperimentasboxplots,whereeachboxplotcorrespondstotheoptimalitygapsobtainedbyamethodonaninstanceset.Thenumberofoptimalsolutionsfoundbythemethodisalsoindicatedovereachboxplot.Asseenintheseexperiments,theconﬁguration(5%;5%)(whichisthereferenceconﬁgurationusedinSection4.2)achievedthebestperformanceamongalltheconsideredconﬁgurations.Moreover,alltheseconﬁgurationswithcombinatorialBenders’cutsachievedbetteroptimalitygapsandnumberofoptimalsolutionsthanthebaselinealgorithmwithoutthem.Comparingconﬁguration(TS,TB)=(10%,10%)withconﬁgurations(20%,0%)and(0%,20%),wenoticethatallocatingthecompleteseparationtime-budgettothecompleteproblemortotherandomizedsubproblemsisnotaseﬀectiveasdividingtheavailabletimebetweenthesetwoapproaches.Finally,allocatingalargeramountoftimetotheseparationprocess,asinconﬁgurations(30,30)and(40,40)leadstoaperformancedecreaseasthereisalargernumberofcuts(heaviermodel)andnolongerenoughtimeforStep3.AsseeninTable7,thepreviousobservationsareconﬁrmedat0.05signiﬁcancelevelbypaired-samplesWilcoxontests:allversionsofCB-SVM-HMLobtainedoptimalitygapswhichweresigniﬁcantly14Type-Ainstances:Type-Binstances:Number of samples (n)Figure 4: Optimality gaps achieved by CB-SVM-HML with diﬀerent values of TB and TS

Table 7: Signiﬁcance results of paired-samples Wilcoxon tests

Conﬁguration

(20%,0%)

(0%,20%)

(2%,2%)

(5%,5%)

(10%,10%)

(20%,20%)

(30%,30%)

(40%,40%)

vs

p

Baseline [5]

sign

vs

(5%,5%)

p

sign

<0.001
(cid:51)

<0.001
(cid:51)

<0.001
(cid:51)

<0.001
(cid:51)

<0.001
(cid:51)

<0.001
(cid:51)

<0.001
(cid:51)

–

–

<0.001
(cid:51)

<0.001
(cid:51)

<0.001
(cid:51)

0.796

<0.001
(cid:51)

<0.001
(cid:51)

<0.001
(cid:51)

<0.001
(cid:51)

16

1491211010255075100UCIType AType BOptimality gap (%)Baseline1521441100255075100UCIType AType BCB−SVM−HML: (20%;0%)1521521320255075100UCIType AType BCB−SVM−HML: (0%;20%)1521741510255075100UCIType AType BOptimality gap (%)CB−SVM−HML: (2%;2%)1521791570255075100UCIType AType BCB−SVM−HML: (5%;5%)1521741550255075100UCIType AType BCB−SVM−HML: (10%;10%)1521761560255075100UCIType AType BInstance setsOptimality gap (%)CB−SVM−HML: (20%;20%)1521721550255075100UCIType AType BInstance setsCB−SVM−HML: (30%;30%)1521741510255075100UCIType AType BInstance setsCB−SVM−HML: (40%;40%)5. Conclusion

In this study, we have introduced CB-SVM-HML, a mixed integer programming approach based

on combinatorial Benders’ cuts for optimally training the SVM-HML. CB-SVM-HML operates by (i)

generating an initial heuristic solution of the SVM-HML to obtain an upper bound on

w

||

, (ii) using

||

this bound to deﬁne separation subproblems that permit to separate CB cuts, and ﬁnally (iii) solving

the SVM-HML with these additional cuts. Through extensive experiments, we observed that this

methodology permits substantial advances in the solution of the SVM-HML, increasing our ability to

achieve optimal solutions and small optimality gaps. Our sensitivity analyses show that additionally

separating CB cuts on small randomized subproblems with fewer samples also permits substantial

performance improvements.

The ﬁndings of our study open many research avenues. First, we suggest pursuing methodological

developments on mixed-integer programming strategies for the SVM-HML. Indeed, non-convex separa-

tion problems such as the SVM-HML with natural “big-M” MILP formulations are archetypal in many

classiﬁcation models (see, e.g., [7, 18, 22, 24, 26, 35]), such that new developments realized on one

problem can trigger signiﬁcant advances for the others. Next, while this study focuses on algorithms

capable of proving optimality, research is still needed on eﬃcient heuristics that can scale up to much

larger data sets. Arguably, there is a need for both optimal algorithms and heuristics, since known

optimal solutions give critical information regarding our ability to solve training problems reliably.

Moreover, optimal or near-optimal solutions permit us to properly assess learning models without

confounding factors due to the possible errors and instabilities of the training algorithms [25]. Finally,

from a more general viewpoint, combinatorial optimization techniques can play an essential role in

many other learning tasks and models. Especially given the rising concerns over equity, privacy, and

transparency, sophisticated optimization strategies become necessary for challenging tasks related to

model compression, training, and explanations, among others [23, 38, 44, 49].

Acknowledgments

This research has been supported by CAPES-PROEX [grant number 88887.214468/2018-00], CNPq

[grant number 308528/2018-2], and FAPERJ [grant number E-26/202.790/2019] in Brazil. This support

is gratefully acknowledged.

17

References

[1] Akpinar, S., A. Elmi, T. Bekta¸s. 2017. Combinatorial Benders cuts for assembly line balancing

problems with setups. European Journal of Operational Research 259(2) 527–537.

[2] Baldomero-Naranjo, M., L.I. Mart´ınez-Merino, A.M. Rodr´ıguez-Ch´ıa. 2020. Tightening big Ms in

integer programming formulations for support vector machines with ramp loss. European Journal

of Operational Research 286(1) 84–100.

[3] Belotti, P., P. Bonami, M. Fischetti, A. Lodi, M. Monaci, A. Nogales-G´omez, D. Salvagnin. 2016.

On handling indicator constraints in mixed integer programming. Computational Optimization

and Applications 65(3) 545–566.

[4] Benders, J.F. 1962. Partitioning procedures for solving mixed variables programming problems.

Numersiche Mathematik 4 238–252.

[5] Brooks, J.P. 2011. Support vector machines with the ramp loss and the hard margin loss. Operations

Research 59(2) 467–479.

[6] Byun, H., S.W. Lee. 2002. Applications of support vector machines for pattern recognition:

a survey. International Workshop on Support Vector Machines. Springer, Berlin, Heidelberg,

213–236.

[7] Carrizosa, E., C. Molero-R´ıo, D. Romero Morales. 2021. Mathematical optimization in classiﬁcation

and regression trees. TOP 29(1) 5–33.

[8] Carrizosa, E., D.R. Morales. 2013. Supervised classiﬁcation and mathematical optimization.

Computers and Operations Research 40(1) 150–165.

[9] Cervantes, J., F. Garcia-Lamont, L. Rodr´ıguez-Mazahua, A. Lopez. 2020. A comprehensive survey

on support vector machine classiﬁcation: applications, challenges and trends. Neurocomputing

408 189–215.

[10] Chang, C.C., C.J. Lin. 2011. LIBSVM: a library for support vector machines. ACM Transactions

on Intelligent Systems and Technology 2(3).

[11] Chen, Chunhui, O. L. Mangasarian. 1996. Hybrid misclassiﬁcation minimization. Advances in

Computational Mathematics 5(1) 127–136.

18

[12] Codato, G., M. Fischetti. 2006. Combinatorial Benders’ cuts for mixed-integer linear programming.

Operations Research 54(4) 756–766.

[13] Collobert, R., S. Bengio. 2001. SVMTorch: Support vector machines for large-scale regression

problems. Journal of Machine Learning Research 1 143–160.

[14] Collobert, R., F. Sinz, J. Weston, L. Bottou. 2006. Trading convexity for scalability. Proceedings

of the 23rd International Conference on Machine Learning 201–208.

[15] Cˆot´e, J.F., M. Dell’Amico, M. Iori. 2014. Combinatorial Benders’ cuts for the strip packing

problem. Operations Research 62(3) 643–661.

[16] Cristianini, N., J. Shawe-Taylor. 2000. An introduction to support vector machines and other

kernel-based learning methods. Cambridge University Press, Cambridge.

[17] Dua, D., C. Graﬀ. 2017. UCI machine learning repository.

[18] Elizondo, D. 2006. The linear separability problem: Some testing methods. IEEE Transactions

on Neural Networks 17(2) 330–344.

[19] Erdo˘gan, G., M. Battarra, R. Wolﬂer Calvo. 2015. An exact algorithm for the static rebalancing

problem arising in bicycle sharing systems. European Journal of Operational Research 245(3)

667–679.

[20] Fan, R.E., P.H. Chen, C.J. Lin. 2005. Working set selection using second order information for

training support vector machines. Journal of Machine Learning Research 6 1889–1918.

[21] Feurer, M., A. Klein, K. Eggensperger, J. Springenberg, M. Blum, F. Hutter. 2015. Eﬃcient

and robust automated machine learning. Advances in Neural Information Processing Systems 28

2962–2970.

[22] Florio, A.M., P. Martins, M. Schiﬀer, T. Serra, T. Vidal. 2022. Optimal decision diagrams for

classiﬁcation. Tech. rep., arXiv:2205.14500.

[23] Forel, A., A. Parmentier, T. Vidal. 2022. Robust counterfactual explanations for random forests.

Tech. rep., arXiv:2205.14116.

[24] Fr´eville, A., S. Hanaﬁ, F. Semet, N. Yanev. 2010. A tabu search with an oscillation strategy for

the discriminant analysis problem. Computers & Operations Research 37(10) 1688–1696.

19

[25] Gribel, D., T. Vidal. 2019. HG-means: A scalable hybrid metaheuristic for minimum sum-of-squares

clustering. Pattern Recognition 88 569–583.

[26] Hanaﬁ, S., N. Yanev. 2011. Tabu search approaches for solving the two-group classiﬁcation

problem. Annals of Operations Research 183(1) 25–46.

[27] Hooker, N., G. Ottosson. 2003. Logic-Based Benders Decomposition. Mathematical Programming,

Series A 96 33–60.

[28] Huang, L.W., Y.H. Shao, J. Zhang, Y.T. Zhao, J.Y. Teng. 2019. Robust Rescaled Hinge Loss

Twin Support Vector Machine for Imbalanced Noisy Classiﬁcation. IEEE Access 7 65390–65404.

[29] Huang, X., L. Shi, J.A.K. Suykens. 2014. Ramp loss linear programming support vector machine.

Journal of Machine Learning Research 15(1) 2185–2211.

[30] Joachims, T. 1999. Making large-scale SVM learning practical. Advances in Kernel Methods. MIT

Press, Cambridge, MA, USA, 169–184.

[31] Kearns, M.J., U.V. Vazirani. 1994. An Introduction to Computational Learning Theory. MIT

Press, Cambridge, MA.

[32] Lee, Y.J., S.Y. Huang. 2007. Reduced support vector machines: a statistical theory. IEEE

Transactions on Neural Networks 18(1) 1–13.

[33] Loosli, G., S. Canu, L. Bottou. 2007. Training invariant support vector machines using selective

sampling. Large Scale Kernel Machines 2.

[34] Mavroforakis, M.E., S. Theodoridis. 2006. A geometric approach to support vector machine (SVM)

classiﬁcation. IEEE Transactions on Neural Networks 17(3) 671–682.

[35] Murthy, S. K., S. Kasif, S. Salzberg. 1994. A system for induction of oblique decision trees. Journal

of Artiﬁcial Intelligence Research (2) 2(1) 1–32.

[36] Orsenigo, C., C. Vercellis. 2003. Multivariate classiﬁcation trees based on minimum features

discrete support vector machines. IMA Journal of Management Mathematics 14(3) 221–234.

[37] Orsenigo, Carlotta, Carlo Vercellis. 2004. Discrete support vector decision trees via tabu search.

Computational Statistics and Data Analysis 47(2) 311–322.

20

[38] Parmentier, A., T. Vidal. 2021. Optimal counterfactual explanations in tree ensembles. M. Meila,

T. Zhang, eds., Proceedings of the 38th International Conference on Machine Learning. Proceedings

of Machine Learning Research, PMLR, Virtual, 8422–8431.

[39] P´erez-Cruz, F., A. Navia-V´azquez, A.R. Figueiras-Vidal, A. Art´es-Rodr´ıguez. 2003. Empirical

risk minimization for support vector classiﬁers. IEEE Transactions on Neural Networks 14(2)

296–303.

[40] Platt, J. 1999. Fast training of support vector machines using sequential minimal optimization.

Advances in Kernel Methods — Support Vector Learning. MIT Press, Cambridge, MA, 185–208.

[41] Poursaeidi, M.H., O.E. Kundakcioglu. 2014. Robust support vector machines for multiple instance

learning. Annals of Operations Research 216(1) 205–227.

[42] Rudin, C. 2019. Stop explaining black box machine learning models for high stakes decisions and

use interpretable models instead. Nature Machine Intelligence 1(5) 206–215.

[43] Sch¨olkopf, B., A.J. Smola, F. Bach. 2002. Learning with Kernels: Support Vector Machines,

Regularization, Optimization, and Beyond . MIT Press, Cambridge, MA.

[44] Serra, T., A. Kumar, S. Ramalingam. 2020. Lossless compression of deep neural networks.

E. Hebrard, N. Musliu, eds., CPAIOR 2020: Integration of Constraint Programming, Artiﬁcial

Intelligence, and Operations Research. Springer, 417–430.

[45] Shawe-Taylor, J., N. Cristianini, et al. 2004. Kernel methods for pattern analysis. Cambridge

university press.

[46] Shen, X., G.C. Tseng, X. Zhang, W.H. Wong. 2003. On ψ-learning. Journal of the American

Statistical Association 98(463) 724–734.

[47] Vapnik, V.N. 1998. Statistical Learning Theory. Wiley, New York.

[48] Vidal, T., D. Gribel, P. Jaillet. 2019. Separable convex optimization with nested lower and upper

constraints. INFORMS Journal on Optimization 1(1) 71–90.

[49] Vidal, T., M. Schiﬀer. 2020. Born-again tree ensembles. Hal Daum´e III, Aarti Singh, eds.,

Proceedings of the 37th International Conference on Machine Learning, Proceedings of Machine

Learning Research, vol. 119. PMLR, Virtual, 9743–9753.

21

[50] Wang, Q., Y. Ma, K. Zhao, Y. Tian. 2020. A comprehensive survey of loss functions in machine

learning. Annals of Data Science 1–26.

[51] Wang, W., Z. Xu. 2004. A heuristic training for support vector regression. Neurocomputing 61

259–275.

[52] Wolsey, L. A. 2020. Integer Programming. John Wiley & Sons, Ltd.

[53] Wu, Y., Y. Liu. 2007. Robust truncated hinge loss support vector machines. Journal of the

American Statistical Association 102(479) 974–983.

[54] Xu, H., C. Caramanis, S. Mannor. 2009. Robustness and regularization of support vector machines.

Journal of Machine Learning Research 10 1485–1510.

22

