0
2
0
2

v
o
N
0
1

]

G
L
.
s
c
[

1
v
9
9
3
5
0
.
1
1
0
2
:
v
i
X
r
a

Learning for Integer-Constrained Optimization
through Neural Networks with Limited Training

Zhou Zhou
Bradley Department of ECE, Virginia Tech
Blacksburg, VA, 24060
zhouzhou@vt.edu

Shashank Jere
Bradley Department of ECE, Virginia Tech
Blacksburg, VA, 24060
shashankjere@vt.edu

Lizhong Zheng
Department of EECS, Massachusetts Institute of Technology
Cambridge, MA, 02139
lizhong@mit.edu

Lingjia Liu
Bradley Department of ECE, Virginia Tech
Blacksburg, VA, 24060
ljliu@vt.edu

Abstract

In this paper, we investigate a neural network-based learning approach towards
solving an integer-constrained programming problem using very limited training.
To be speciﬁc, we introduce a symmetric and decomposed neural network structure,
which is fully interpretable in terms of the functionality of its constituent compo-
nents. By taking advantage of the underlying pattern of the integer constraint, as
well as of the afﬁne nature of the objective function, the introduced neural network
offers superior generalization performance with limited training, as compared to
other generic neural network structures that do not exploit the inherent structure
of the integer constraint. In addition, we show that the introduced decomposed
approach can be further extended to semi-decomposed frameworks. The intro-
duced learning approach is evaluated via the classiﬁcation/symbol detection task
in the context of wireless communication systems where available training sets
are usually limited. Evaluation results demonstrate that the introduced learning
strategy is able to effectively perform the classiﬁcation/symbol detection task in a
wide variety of wireless channel environments speciﬁed by the 3GPP community.

1

Introduction

Integer-constrained optimization has wide applications in many ﬁelds, including industrial production
planning, vehicle scheduling in transportation networks, and resource allocation for cellular networks
where the variables often represent ﬁnite decisions and countable quantities [1, 2]. In this paper, we
consider developing a neural network (NN)-based solver for the integer-constrained programming
problem in the following form:

minimize
x

f (Hx − y)

subject to xn ∈ A

,

(1)

where H is an Q × N matrix; y is the vector of dimension Q × 1 containing observed values; f (·)
is the objective function; x is the unknown vector of dimension N × 1 (xn is the nth element of

 
 
 
 
 
 
x); and A is the set containing the limited integer values that each xn can take. Without loss of
generality (WLOG), A is set to be {−2M − 1, −2M + 1, · · · , −1, 1, · · · , 2M − 1, 2M + 1}. This
integer-constrained deﬁnition of set A can be generalized to any arbitrary integers. Unlike unfolding
any optimization based algorithms to a deep neural network [3], our approach is motivated by the
intrinsic geometry of the integer-constraint A, and the embedded afﬁne-mapping H in the objective.

Integer-constrained optimization problems are generally NP-hard [2] due to the non-convex nature
of the set AN , where N is the dimension of x. An exhaustive search can be prohibitively expensive,
especially when N is large. Therefore, rather than focusing on conventional optimization-based
approaches, we alternatively consider a data-driven or learning-based classiﬁcation/detection frame-
work where the structural information that is inherent in the constraint can be utilized to improve
the learning “efﬁciency” in terms of the training data overhead and “effectiveness” in terms of the
generalization performance.

The process of our introduced neural network based solver is as follows:

• We ﬁrst deﬁne a probability residual-model for r = Hx − y where r is generated according

to a distribution P (r) which is determined based on the objective function f (·)1.

• Then, we generate K tuples of (x, r) according to the distributions U (AN ) and P (r),
where U (AN ) represents the uniform distribution deﬁned on AN . The resulting tuples can
be organized as,

(cid:110)(cid:16)

x(1), r(1)(cid:17)

,

(cid:16)

x(2), r(2)(cid:17)

, · · · ,

(cid:16)

x(K), r(K))

(cid:17)(cid:111)

.

For a ﬁxed H, y is generated using x and r via the residual relation y = Hx − r. For the
purpose of training a neural network, we have K tuples of (x, y):

(cid:110)(cid:16)

x(1), y(1)(cid:17)

(cid:16)

x(2), y(2)(cid:17)

,

, · · · ,

(cid:16)

x(K), y(K))

(cid:17)(cid:111)

.

(2)

• We train a neural network N using the tuples deﬁned in (2), where y(k) is the input to N

and x(k) is the associated output.

• After N is trained, we feed the observation vector y to N to generate a solution, ˆx, of (1).

To provide a meaningful and practical solution for a large dimensional x, we develop a parallel
solver in which N independent neural networks simultaneously estimate each entry of x. This is
accomplished by leveraging the concept of marginal estimation, tailored speciﬁcally to our chosen
probability model for the residue r. This decomposed approach can also be extended to obtain a
semi-decomposed framework, thus providing a middle ground between this decomposed approach
and the joint approach.

The innovation in the introduced decomposed estimator relies on the geometric structure of the
lattice spanned by the afﬁne mapping H as well as the step-size selected from A. By incorporating
such structural information into our neural network design, we can achieve superior generalization
performance with very limited training samples. The resulting structure of the designed neural
network also allows an analytical characterization of the classiﬁcation/detection error in terms of an
asymptotic bound. For evaluation, we utilize this optimization framework to perform the symbol
detection task in wireless communications systems, which we believe is a meaningful step towards the
interdisciplinary research in applying neural networks to solve practical problems of ﬁfth generation
(5G) networks [4].

In summary, the key contributions of this paper are summarized as follows:

• The introduced neural network architecture is symmetric and its structure is fully explainable.
The neural network aims to learn the marginal probability distribution function (PDF) of
each variable. Instead of performing an integral to compute the marginal distribution, the
introduced method simpliﬁes the posterior estimation by using structured classiﬁers.

• With a limited amount of training, the introduced learning approach can achieve better
generalization performance than that of generic neural networks having an arbitrary structure,
since the structural knowledge of the objective term and of the integer constraints are well
integrated in the design of the neural network.

1For instance, the Gaussian distribution can be used as P (r) for L2-norm minimization.

2

• We derive a theoretical upper-bound for the generalization error of our introduced neural
network. This provides performance guarantees to the introduced neural network in the
classiﬁcation/detection task.

2 Related Work and Background

As a special case of the objective function f (·), the quadratic loss/error term has been widely adopted
for characterizing residual “loss” or “error” terms and often appears as the second order term for
objective approximation [5, 6, 7]. When the decision variables are constrained to be integers, the
optimization becomes challenging since the integer constraint renders the problem non-convex. A
wide range of approaches have been introduced for this class of problems, such as branch and bound,
convex relaxation, etc. [8]. These approaches can be brieﬂy summarized into the two categories
of joint and decomposed methods. The joint solver approach operates by directly optimizing the
objective function over the parameter grid spanned by the Cartesian product of all variable entries.
At a given iteration, the variables are adjusted by changing an approximated bound of the overall
loss function. Under this approach, sphere decoding is considered as the near optimal solver in the
context of symbol detection for communication systems [9]. Theoretically, the joint solver yields
the optimal solution after all the branches are checked out. However, it places an extremely heavy
computational burden, making its scaling to a larger system unfeasible. The decomposed approach
is as an efﬁcient way to address the scalability issue [10]. It essentially avoids the overwhelming
complexity of the joint approach by breaking down the aggregated loss term into smaller composite
loss terms. However, in order to implement the decomposed approach, either an explicit separable
form of the objective function or an implicit coordinate transformation is required [11, 12].

In this paper, we use a neural network structure that can be deconstructed in order to solve the stated
optimization problem. Our method circumvents the obstacle of the features being non-separable in the
objective function and attempts to solve the optimization problem via a pure data-driven framework.
Related approaches that utilize the decomposed objective function can be found in [13, 14]. The
introduced learning framework leverages the structural information of the optimal residual probability
distribution as introduced in (2). Prior work on using neural networks as probability distribution
estimators can be found in [15, 12, 16, 17, 18]. However, in these frameworks, inference of the
posterior distribution typically requires training a substantially large number of neural network
parameters, thereby signiﬁcantly increasing the training overhead. Furthermore, the neural network
architectures considered in such frameworks usually do not consider the speciﬁc underlying structural
information inherent in the problem.

Related work on connecting neural networks to optimization techniques can be found in [19, 20, 21].
In these cases, neural networks are often used to assist the optimization step or reversely, the
optimization is considered as part of the neural network operations. As an example of the latter,
[21] introduced a formulation of integrating quadratic programming as neural network layers. The
output of the previous layers of the neural network are considered as encoded parameters of the
objective function and the associated constraints. Genetic algorithms are a class of random-based
evolutionary algorithms that can be applied to constrained optimization problems [22]. MATLAB’s
Optimization Toolbox provides an implementation of the genetic algorithm that can be used to
evaluate its performance in an integer-constrained optimization task such as the one in Equation (1).

The introduced method does not follow the concepts of unrolling or integration as in the existing
literature. Instead, its motivation is derived purely from a data-driven perspective. The task chosen to
evaluate the introduced neural network architecture is that of classiﬁcation/transmit symbol detection
in wireless communications systems. Symbol detection is the most important receiver processing
function in the physical layer (PHY) of a wireless communications system. Detecting symbols in
vastly different radio environments is one of the major challenges in ﬁfth generation (5G) wireless
systems and is critical in guaranteeing the reliability of communication under heterogeneous scenarios.
In fact, for linear transmission channels with Gaussian noise, the symbol detection task can be exactly
formulated as (1), where the function f is the L2-norm. Since the interference plus noise in 5G
applications can be highly non-Gaussian, it is desirable to identify solvers that are robust and adaptive
to any interference and noise environments. In neural network-based approaches, a well-trained model
can mitigate the non-Gaussian effect of interference from other users that is superimposed on the target
signal. This provides a strong motivation to investigate neural network-based approaches to solve the
classiﬁcation problem which is essentially tied to the integer-constrained optimization task studied

3

in this paper. DetNet [23] is a state-of-the-art neural network architecture for symbol detection.
Even though its performance is comparable to conventional model-based methods, it requires a
prohibitively large amount of ofﬂine training. MMNet [24] is another recently introduced neural
network-based symbol detection framework for (massive) MIMO (Multiple Input Multiple Output)
systems, that utilizes correlation in channel for accelerated training and has a lower computational
complexity compared to DetNet. Compared with existing neural network-based methods, we show
that our method provides a guaranteed generalization performance with limited training.

3 Structure-Aware Learning

Based on the probability model presented in (2), we ﬁrst consider the joint posterior probability
distribution. According to Bayes’s rule, the joint posterior probability distribution can be written as,

p (x1, x2, · · · , xN |y)
= p (x1|y) p (x2|x1, y) · · · p (xN |x1, · · · , xN −1, y)

To facilitate a decomposed approach for the maximum a posteriori estimation (MAP), we utilize the
following naive Bayesian approximation:

p(x1, x2, · · · , xN |y)

arg max

x1,x2,··· ,xN
≈ arg max

x1,x2,··· ,xN

p(x1|y)p(x2|y) · · · p(xN |y),

(3)

indicating that the joint MAP estimator can be approximated as the product of the individual marginal
MAP estimators.

3.1 Binary Integer Constraint

We now consider how to obtain the marginal MAP estimator arg maxxn p(xn|y) using a neural
network. For ease of discussion, we ﬁrst assume that the integer constraint is only restricted to a
binary set, i.e., A = {−1, 1}. We then learn a neural network Nn to approximate p(xn|y), where the
input of the neural network is y and the output is the probability mass of xn. Alternatively, we can
write the output as the following ratio,

L+− (y) :=

p (xn = 1|y)
p (xn = −1|y)

.

(4)

This is termed as the likelihood ratio (LR) between hypotheses {H0 : xn = 1} and {H1 : xn = −1}.
Within the neural network, we choose soft-max as the activation function for the ﬁnal output layer,

p (xn = 1|q) =

p (xn = −1|q) =

eq(cid:62)w1
eq(cid:62)w1 + eq(cid:62)w−1
eq(cid:62)w−1
eq(cid:62)w1 + eq(cid:62)w−1

,

,

where q represents the last hidden state and w1, w−1 are the weights connecting the hidden layer to
the output nodes for +1 and −1 respectively. We choose cross-entropy as the loss function used in
the training of Nn. The training set for Nn consists of the K tuples:
n , y(1)(cid:17)
x(1)

n , y(K)(cid:17)(cid:111)
x(K)

n , y(2)(cid:17)
x(2)

, · · · ,

(cid:110)(cid:16)

(5)

(cid:16)

(cid:16)

,

,

where x(k)

n is the nth entry of x(k), as deﬁned in (2).

3.2 General Integer Constraint

Next, we consider the extension of the binary constraint to a general integer constraint, i.e., A =
{−2M − 1, −2M + 1, · · · , −1, 1, · · · , 2M − 1, 2M + 1}. Leveraging the pattern of this integer
constraint, the neural network Nn is designed as shown in Fig. 1, where the “atomic decision neural
network” (ADNN) is the same as the binary integer classiﬁer/ MAP estimator introduced in Section
3.1. We see that the input of each ADNN is a “shifted” version of the observed signal vector y, that

4

Figure 1: The neural network structure for the posterior estimation of general integer constraint.

is shifted in the direction of hn (,i.e., the n-th column of H), with the size of the shift depending
on the step size between the elements of the integer set A. In other words, for the mth ADNN, the
output ratio represents an estimate of the LR between the hypotheses {H (−2m)
: xn = −2m + 1}
and {H (−2m)
: xn = −2m − 1}, where m = −M, −M + 1, · · · , +M . Intuitively, this shifting
1
process is motivated by the fact that

0

L(2m)

+− (y) = L+− (y + 2mhn) ,

(6)

+− (y) represents the LR between H (−2m)

where L(2m)
Once the LRs representing all the pairwise boundary decision probabilities in A are obtained, the
posterior marginal estimation of xn can be constructed as follows: Assume a constant p(xn =
−2M − 1|y), we have

and H (−2m)
1

0

.

p(xn = −2m + 1|y)

= p (xn = −2M − 1|y)

m
(cid:89)

m(cid:48)=M

L(−2m(cid:48))

+−

(y) .

(7)

This chain connection represented in (7) corresponds to the last layer of the neural network structure
depicted in Fig. 1.

We now consider how to efﬁciently train this structured neural network. As depicted in Fig. 1, it
can be observed that the essential component in this neural network is the ADNN. According to the
discussion in Section 3.1, we know the ADNN training set is based on a binary integer target. In
order to create a similar training set for the ADNN with an arbitrary integer constraint, we introduce
the following training set,

(cid:110)(cid:16)

+1, ˜y(1)
+

(cid:16)

(cid:17)

,

−1, ˜y(1)
−

(cid:17)

, · · · ,

(cid:16)
+1, ˜y(K)

−

(cid:17)

,

(cid:16)

−1, ˜y(K)

−

(cid:17)(cid:111)

where

˜y(k)
+ = y(k) − x(k)
˜y(k)
− = y(k) − x(k)

n hn + hn
n hn − hn.

(8)

(9)

A geometric illustration of this atomic decision under the general integer constraint is shown in Fig.
2. Suppose y = h1 ˜x1 + h2x2 + h3x3 + r, where ˜x1 = {+1, −1} and x2, x3 ∈ {−3, −1, +1, +3}.
The lattice spanned by h1 + h2x2 + h3x3 is represented by the red dots in the ﬁgure, while the blue
dots represent the lattice spanned by −h1 + h2x2 + h3x3. Due to the noise r, the observation y can
lie at any arbitrary location in this space. Accordingly, the ADNN generates the LR for the input y.

5

y+2M hn+(2M-2) hn-2M hnAtomic Decision Neural NetworkAtomic Decision Neural NetworkAtomic Decision Neural NetworkNN with weight sharing NodeLog p(xn=-2M-1)Log p(xn=-2M+1)Log p(xn=-2M+1)Log p(xn=-2M+3)Log p(xn=+2M-1)Log p(xn=+2M+1)p(xn=-2M+1)p(xn=-2M+3)p(xn=+2M+1)p(xn=-2M-1)___Figure 2: Decision boundary for binary integer constraint.

3.3 Semi-Decomposed Approach

Note that the aforementioned decomposed approach requires N neural networks for the approximation
of the marginal PDFs of the N entries of x. Furthermore, we can combine any number of entries
of x into groups such that the marginal PDFs of more than one variable are characterized and
approximated. For two entries xn and xn(cid:48), their marginal PDF is given by

According to Bayes’ rule, we then split this marginal PDF into two parts,

p (xn, xn(cid:48)|y) ,

xn ∈ A, xn(cid:48) ∈ A.

p (xn, xn(cid:48)|y) = p (xn|xn(cid:48), y) p (xn(cid:48)|y) .

(10)

(11)

Since p (xn|xn(cid:48), y) = p (xn|y − xn(cid:48)hn(cid:48)), we can learn the marginal PDF of xn and x(cid:48)
n respectively.
Subsequently, we assemble these two marginal PDFs to generate their joint distribution by using
(11). This approach also can be extended to obtain the marginal PDF of the combination of any
number of variables, extending up to the joint posterior distribution. This fact reveals that the ADNN
is the fundamental component in the construction of the marginal PDF of any arbitrary variable
combination.

3.4 Analytical Performance Characterization

By employing the ADNN, we can approximate the posterior PDF for any combination of variables.
Accordingly, we can analyze how the binary decision error of the ADNN can impact the classiﬁcation
performance of the entire neural network.

Theorem 1 Assume that the binary decision error of the ADNN is characterized by the Type-I error
α and the Type-II error β, deﬁned as:

α = Pr (L+−(y) < 1|xn = +1)
β = Pr (L+−(y) > 1|xn = −1) .

Then, the decision error for the entire neural network shown in Fig. 1 is bounded by

(cid:18) (cid:18)

Pr(e) ≤

1 −

(cid:19)

1
4M

(cid:19)ρ

(α + β)

,

(12)

where ρ ∈ [0, 1], M is deﬁned in the description of the set A.

Theorem (1) can be directly obtained by applying the union bound [25] to the probability of symbol
error Pr(e). As an example of a numerical evaluation of this result, assume that α = 0.05, β = 0.05,
M = 1 and ρ = 1. Then, the probability of error is upper bounded by Pr(e) < 0.075 which is only
marginally higher than either α or β, the binary decision errors of the individual ADNN. This result
also indicates to what extent the performance of the algorithm would degrade with an increase in the

6

size of the integer constraint. Asymptotically, as M becomes inﬁnitely large, the probability of the
decision error for the entire neural network will tend to (α + β)ρ.

Having derived the relation between the probability of decision error for the atomic decision neural
network (ADNN) and that for the entire neural network, we can further characterize the generalization
performance of our introduced neural network by utilizing the following well-known result from
PAC-Bayes learning theory. For any neural network, [26] gives the following generalization bound,

Lemma 2 For any training set K deﬁned in (2), having a sample size K, a corresponding neural
network N trained on K satisﬁes the following generalization bound, with probability at least (1 − δ)
for δ ∈ (0, 1),

L (N ) ≤ LK (N ) + 2RK (H) +

(cid:115)

log (cid:0) 1
δ
2n

(cid:1)

.

(13)

We deﬁne each term in Equation (13) as follows. For a given loss function (cid:96), the risk of N , L(N ), is
deﬁned as:

L(N ) := E((cid:96)(x, N (y)),

(14)

Here the expectation is taken over the joint distribution of the input-output space. Since this joint
distribution is unknown, we instead calculate the empirical risk LK(N ) based on the training dataset
as:

LK(N ) :=

(cid:88)

(cid:96)(xk, N (yk)).

(15)

1
K

k
The Rademacher complexity RK(H) is a measure of the richness of the class of neural networks H
such that N ∈ H, and is deﬁned as [26]:

RK(H) := E(cid:15)K ,K

(cid:34)

1
K

sup
N ∈H

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

K
(cid:88)

k=1

(cid:15)k(cid:96) (xk, N (yk))

(cid:35)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

(16)

where (cid:15)K := [(cid:15)1, (cid:15)2, · · · , (cid:15)k, · · · , (cid:15)K] is a vector of i.i.d. Rademacher random variables, such that
each (cid:15)k ∈ {−1, +1} with probabilities {− 1

2 }, respectively..

2 , + 1

Lemma 3 Let H represent the class of all feed-forward neural network-based classiﬁers with weights
w satisfying the norm constraint (cid:107)w(cid:107) ≤ B, i.e.,:

Then, the Rademacher complexity is bounded by [26]

H := {(cid:104)w, ·(cid:105) : (cid:107)w(cid:107) ≤ B}.

RK (H) ≤

J
(cid:89)

j=1

(LjBj) ·

(cid:18) B0R
√
K

+

2R
K

(cid:112)J log 2

(cid:19)

,

(17)

where J is the number of layers except the input layer, Bj is the radius of the weights at the jth layer;
the intermediate activation functions are assumed to be Lipschitz-continuous with Lipschitz constant

Lj and R is the radius of the input training data, i.e. R :=

(cid:113)

1
K

(cid:80)K

k=1 (cid:107)yk(cid:107)2.

Note that the loss function (cid:96)(·) used in the deﬁnition of LK(N ) in Lemma 1 is characterized in terms
of the Type-I and Type-II errors α and β respectively deﬁned in Theorem 1. Combining this relation
and the known upper bound on RK(H) from Lemma 2, we have the following result in Theorem 2.

Theorem 4 Let HA be the class of all atomic decision neural networks (ADNNs) and NA be a
particular ADNN. With cross-entropy used as the loss function (cid:96)(·), the generalization error is
bounded as follows:

Pr(e) ≤



(cid:18)



1 −

1
4M



(cid:19)

2RK(HA) +

(cid:115)

(cid:1)

log (cid:0) 1
δ
2K




ρ

− L (NA)





with probability at least δ, δ ∈ (0, 1).

7

We provide a sketch of the proof as follows: Using cross-entropy as the loss function (cid:96)(·), we begin
1(xk = +1) log(1 − αk) + 1(xk = −1) log(1 − βk) into (13),
by substituting LK(NA) = 1
K
where 1 represents the indicator function, 1 − αk and 1 − βk is the ADNN outputs for xk = +1 and
xk = −1 respectively. Recalling from Theorem 1 that the decision errors encountered during the
testing stage are deﬁned as α and β, we have

(cid:80)
k

1
K

(cid:88)

k

1(xk = +1) log(1 − αk) + 1(xk = −1) log(1 − βk)

≤

1
2

log(1 − α) +

1
2

log(1 − β).

Since α ≤ 1 and β ≤ 1, we can arrive at the inequality in Theorem (4) by using the bound:
α ≤ − log(1 − α) and β ≤ − log(1 − β).

4 Case Study – Classiﬁcation/Symbol Detection in MIMO Systems

For performance evaluation of the introduced neural network structure, we consider the task of
classiﬁcation/symbol detection in a multi-input-multi-output (MIMO) wireless communications
system. This task of symbol detection in a wireless communications context can be viewed as a
multi-label classiﬁcation problem with the constraint of limited availability of training datasets, due
to the physical implementation limitations of a real-world wireless communications system. In this
task, H is the wireless transmission channel which often satisﬁes a certain probability distribution
and y is the signal received at the receiver. The introduced solver is then utilized to detect the transmit
symbol of interest x, by utilizing different objective functions f (·).

4.1 Comparison with Generic Neural Network Architectures

To compare the performance of our introduced neural network-based solver to other generic neural
network structures, three alternate neural network structures are also developed as benchmarks. These
four networks are respectively denoted as A-Net, B-Net, C-Net and D-Net and are shown in Fig. 3.
It can be observed that the other generic neural networks have a larger number of free parameters
compared to the primary introduced method, A-Net. To be speciﬁc, B-Net is with a relaxation on the
output layer, which is designed to gauge whether a better output combination can be learned rather
than using the likelihood ratio chain as introduced in (7). The learning procedure for B-Net has two
steps: in the ﬁrst stage, we learn the atomic decision neural network as per previous discussions; in
the second stage, we add a output layer to combine all these learned atomic decision neural networks
and only learn the output MLP using the same training data that was used for the ADNN’s learning.
Note that in the second learning stage, we use multiple-class cross-entropy as the loss function. In
C-Net, we enforce weight sharing at the same locations as the ADNNs in B-Net, and then directly
learn this network. In D-Net, we further relax the weight sharing constraint and also learn its weights
the same way as C-Net. We ﬁrst consider the case of Gaussian noise where we set f (x) = (cid:107)x(cid:107)2. The
results comparing the generalization ability of these four neural networks are summarized in Table 1,
where the training set size K is set as 30. Subsequently, the learned neural network is tested 30000
times by using different y but the same H. It can be observed that the introduced structure-aware
decomposition-based neural network, A-Net, has the best performance in this classiﬁcation task. On
the other hand, the structures with greater number of free parameters (in a descending order from
B-Net to D-Net) have much lower generalization ability. This clearly demonstrates the efﬁciency as
well as the effectiveness of our introduced neural network design.

Table 1: Average testing and training success rate of the four neural networks using very limited
training set: K = 30, where the objective function is f (·) := (cid:107) · (cid:107). (one trial is counted as success
when the global optimum is found)

Method Train Success Rate Test Success Rate
A-Net
B-Net
C-Net
D-Net

> 0.99
> 0.99
> 0.99
> 0.99

0.88
0.84
0.70
0.60

8

Figure 3: Benchmark neural network structures for symbol detection evaluation.

n log(1 + x2

We also analyze the performance of the introduced neural network in the case where f (x) =
(cid:80)
n/ν) which is parameterized by the degree of freedom parameter ν. Accordingly, the
residual noise r is characterized by the Student’s-t distribution. This is often the case in real-world
wireless systems where r often represents sparse noise or sparking interference from other users
whose occurrence is unknown, thus rendering the distribution of r to be non-Gaussian [27]. The
results comparing the generalization ability of the four networks using the Student’s-t residual model
as shown in Table 2. This performance evaluation uses ν = 3, with the training set and testing set
sizes set to K = 30 and 30000 respectively.

Table 2: Average testing and training success rate of the four neural networks using f (·) = (cid:80)
(·)2

n/ν) with degrees of freedom parameter ν = 3.

n log(1+

Method Train Success Rate Test Success Rate
A-Net
B-Net
C-Net
D-Net

> 0.99
> 0.99
> 0.99
> 0.99

0.87
0.86
0.70
0.62

4.2 Theoretical Justiﬁcation

In this section, we provide experimental validation for the upper bound on the generalization error
obtained in Theorem 4 via simulations on the introduced neural network structure A-Net. As seen in
Fig. 4, the generalization error decreases with an increase in the size of the training set K, where
the trials are conducted over 100 wireless channel realizations H in accordance with the O( 1√
)
K
dependence evident from Theorem 4.

4.3 Comparison with State-of-the-Art Methods

For a complete comparison with state-of-the-art, the performance of the introduced structure-aware
decomposition-based neural network (A-Net) is compared with benchmark conventional optimization
methods. These include gradient-free optimization methods implemented from the open-source
‘NLOpt’ package [28], such as Subplex [29], Nelder-Mead Simplex [30], Principal Axis (PRAXIS)
[31], Constrained Optimization By Linear Approximations (COBYLA) [32], Divided Rectangles
algorithm (DIRECT) [33] and its locally-biased variant DIRECT-L [34]. These evaluations use a
random initialization with the number of iterations set to 1000. A-Net’s performance is also compared
against the genetic algorithm [35, 22], which was implemented using MATLAB’s Optimization

9

y+2hn-2 hnAtomic Decision Neural NetworkAtomic Decision Neural NetworkAtomic Decision Neural Networky+2hn-2 hnAtomic Decision Neural NetworkAtomic Decision Neural NetworkAtomic Decision Neural Networky+2hn-2 hnMLPy+2hn-2 hnMLPMLPMLPChainin (16)P(xn=+3)P(xn=+1)P(xn=-1)P(xn=-3)MLPP(xn=+3)P(xn=+1)P(xn=-1)P(xn=-3)MLPP(xn=+3)P(xn=+1)P(xn=-1)P(xn=-3)MLPP(xn=+3)P(xn=+1)P(xn=-1)P(xn=-3)( A-Net )( B-Net )( C-Net )( D-Net )Figure 4: Generalization performance with respect to K for the introduced neural network.

Toolbox. In addition, since the problem chosen is the speciﬁc task of symbol detection under a
wireless channel, we also evaluate a state-of-the-art neural network-based MIMO symbol detection
architecture, DetNet [23] that is based on unfolding iteration-based optimization algorithms. For a
fair comparison, the same training and test settings are employed with DetNet as those with A-Net.
In all of these above comparative evaluations across all trials, we set E{(cid:107)Hx(cid:107)2/(cid:107)Hx − y(cid:107)2} as 10
dB. We analyze two cases: where the noise r follows a i) Gaussian distribution and ii) a non-Gaussian
distribution (t-distribution with ν = 3). The two different objective functions mentioned previously
are considered as well, i.e., the L2-norm and the log-sum function.

The performance comparison is summarized in Table 3. We can clearly observe from Tables 1, 2
and 3 that A-Net has a higher average detection success rate when compared to either the standard
optimization methods or a state-of-the-art neural network-based symbol detector such as DetNet. This
performance improvement is primarily due to the fact that we are able to incorporate the structural
information in the design of A-Net.

Table 3: Performance comparison with non-linear optimization solvers.

(cid:80)

Methods

Symbol Detection Success Rate
(cid:107) · (cid:107)2
n/ν)
0.87
0.89
0.76
0.92
0.85
0.83
Genetic Algorithm 0.71
0.36

n log(1 + (·)2
0.85
0.84
0.74
0.92
0.82
0.78
0.74
0.32

Sbplx
Nelder-Mead
PRAXIS
COBYLA
DIRECT
DIRECT-L

DetNet

5 Conclusion and Future Work

In this paper, a neural network based-approach to solve an integer-constrained programming problem
was presented. The introduced neural network architecture leverages the following two features of the
optimization problem (1) : (i) the lattice structure of the integer constraint A (ii) the afﬁne mapping
inside the composition-type objective function f (Hx − y). Moreover, the objective function is
translated to its corresponding probabilistic model to generate the dataset for training the neural
network. In addition, a generalization performance bound for this neural network was also derived.

10

Experimental results veriﬁed the superior generalization ability of this method with limited training.
Meanwhile, it was also observed that the introduced NN-based solver outperforms state-of-the art
generic non-linear optimization solvers. Overall, the introduced structure-aware NN-based method is
a promising direction towards solving NP-hard integer-constrained optimization problems.

References

[1] Fred Glover. Future Paths for Integer Programming and Links to Artiﬁcial Intelligence. Com-

puters operations research, 13(5):533–549, 1986.

[2] Raymond Hemmecke, Matthias Köppe, Jon Lee, and Robert Weismantel. Nonlinear Integer
Programming. In 50 Years of Integer Programming 1958-2008, pages 561–618. Springer, 2010.

[3] John R Hershey, Jonathan Le Roux, and Felix Weninger. Deep Unfolding: Model-based

Inspiration of Novel Deep Architectures. arXiv preprint arXiv:1409.2574, 2014.

[4] NGMN Alliance. 5G White Paper. Next generation mobile networks, white paper, 1, 2015.

[5] Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro. Online Learning for Matrix
Factorization and Sparse Coding. Journal of Machine Learning Research, 11(Jan):19–60, 2010.

[6] Jason D Lee, Yuekai Sun, and Michael Saunders. Proximal Newton-type Methods for Convex
Optimization. In Advances in Neural Information Processing Systems, pages 827–835, 2012.

[7] Neal Parikh, Stephen Boyd, et al. Proximal Algorithms. Foundations and Trends® in Optimiza-

tion, 1(3):127–239, 2014.

[8] Alexander Schrijver. Theory of Linear and Integer Programming. John Wiley & Sons, 1998.

[9] Babak Hassibi and Haris Vikalo. On the Sphere-Decoding Algorithm I. Expected Complexity.

IEEE transactions on signal processing, 53(8):2806–2818, 2005.

[10] Claus C CarøE and Rüdiger Schultz. Dual Decomposition in Stochastic Integer Programming.

Operations Research Letters, 24(1-2):37–45, 1999.

[11] Stephen Boyd, Lin Xiao, Almir Mutapcic, and Jacob Mattingley. Notes on Decomposition

Methods. Notes for EE364B, Stanford University, pages 1–36, 2007.

[12] David Price, Stefan Knerr, Léon Personnaz, and Gérard Dreyfus. Pairwise Neural Network
Classiﬁers with Probabilistic Outputs. In Advances in neural information processing systems,
pages 1109–1116, 1995.

[13] Dimitris Bertsimas, Vishal Gupta, and Nathan Kallus. Data-driven Robust Optimization.

Mathematical Programming, 167(2):235–292, 2018.

[14] Dimitris Bertsimas and Aurélie Thiele. Robust and Data-driven Optimization: Modern Decision
In Models, methods, and applications for innovative decision

Making under Uncertainty.
making, pages 95–122. INFORMS, 2006.

[15] Michael D Richard and Richard P Lippmann. Neural Network Classiﬁers Estimate bayesian a

Posteriori Probabilities. Neural computation, 3(4):461–483, 1991.

[16] Benigno Uria, Iain Murray, and Hugo Larochelle. Rnade: The Real-valued Neural Autore-
gressive Density-estimator. In Advances in Neural Information Processing Systems, pages
2175–2183, 2013.

[17] Kenji Doya, Shin Ishii, Alexandre Pouget, and Rajesh PN Rao. Bayesian Brain: Probabilistic

Approaches to Neural Coding. MIT press, 2007.

[18] João Ferdinando Gomes de Freitas. Bayesian Methods for Neural Networks. PhD thesis,

University of Cambridge, 2003.

[19] A Cochocki and Rolf Unbehauen. Neural Networks for Optimization and Signal Processing.

John Wiley & Sons, Inc., 1993.

11

[20] Abdesselam Bouzerdoum and Tim R Pattison. Neural Network for Quadratic Optimization

with Bound Constraints. IEEE transactions on neural networks, 4(2):293–304, 1993.

[21] Brandon Amos and J Zico Kolter. Optnet: Differentiable Optimization as a Layer in Neural
Networks. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pages 136–145. JMLR. org, 2017.

[22] A. E. Eiben and James E. Smith. Introduction to Evolutionary Computing. Springer Publishing

Company, Incorporated, 2nd edition, 2015.

[23] Neev Samuel, Tzvi Diskin, and Ami Wiesel. Learning to detect. IEEE Transactions on Signal

Processing, 67(10):2554–2564, 2019.

[24] M. Khani, M. Alizadeh, J. Hoydis, and P. Fleming. Adaptive neural signal detection for massive

mimo. IEEE Transactions on Wireless Communications, pages 1–1, 2020.

[25] Thomas M Cover. Elements of information theory. John Wiley & Sons, 1999.

[26] Bruce Hajek and Maxim Raginsky. Statistical learning theory. Lecture Notes, 387, 2019.

[27] Communication algorithms via deep learning, 2018.

[28] Steven Johnson. The nlopt nonlinear-optimization package. Software Package, 2020.

[29] T Rowan. : Functional stability analysis of numerical algorithms. in: Ph. d. thesis, department

of computer sciences, university of texas at austin, 1990. Ph.D. Thesis, 1990.

[30] John A Nelder and Roger Mead. A simplex method for function minimization. The computer

journal, 7(4):308–313, 1965.

[31] Richard P Brent. Algorithms for minimization without derivatives. Courier Corporation, 2013.

[32] Michael JD Powell. A direct search optimization method that models the objective and constraint
functions by linear interpolation. In Advances in optimization and numerical analysis, pages
51–67. Springer, 1994.

[33] Donald R Jones, Cary D Perttunen, and Bruce E Stuckman. Lipschitzian optimization without
the lipschitz constant. Journal of optimization Theory and Applications, 79(1):157–181, 1993.

[34] Joerg M Gablonsky and Carl T Kelley. A locally-biased form of the direct algorithm. Journal

of Global Optimization, 21(1):27–37, 2001.

[35] A.H. Wright, M.D. Vose, K.A. De Jong, and L.M. Schmitt. Foundations of Genetic Algorithms,

chapter 1, pages 1–52. John Wiley Sons, Ltd, 2007.

12

