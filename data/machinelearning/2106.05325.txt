2
2
0
2

y
a
M
6
1

]

G
L
.
s
c
[

2
v
5
2
3
5
0
.
6
0
1
2
:
v
i
X
r
a

ZoPE: A Fast Optimizer for ReLU Networks
with Low-Dimensional Inputs

Christopher A. Strong1, Sydney M. Katz2,
Anthony L. Corso2, and Mykel J. Kochenderfer2

1 Department of Electrical Engineering, Stanford University
christopher_strong@berkeley.edu
2 Department of Aeronautics and Astronautics, Stanford University
{smkatz, acorso, mykel}@stanford.edu

Abstract. Deep neural networks often lack the safety and robustness
guarantees needed to be deployed in safety critical systems. Formal veri-
ﬁcation techniques can be used to prove input-output safety properties
of networks, but when properties are diﬃcult to specify, we rely on the
solution to various optimization problems. In this work, we present an
algorithm called ZoPE that solves optimization problems over the out-
put of feedforward ReLU networks with low-dimensional inputs. The
algorithm eagerly splits the input space, bounding the objective using
zonotope propagation at each step, and improves computational eﬃciency
compared to existing mixed-integer programming approaches. We demon-
strate how to formulate and solve three types of optimization problems:
(i) minimization of any convex function over the output space, (ii) mini-
mization of a convex function over the output of two networks in series
with an adversarial perturbation in the layer between them, and (iii) max-
imization of the diﬀerence in output between two networks. Using ZoPE,
we observe a 25× speedup on property 1 of the ACAS Xu neural network
veriﬁcation benchmark compared to several state-of-the-art veriﬁers, and
an 85× speedup on a set of linear optimization problems compared to
a mixed-integer programming baseline. We demonstrate the versatility
of the optimizer in analyzing networks by projecting onto the range of a
generative adversarial network and visualizing the diﬀerences between a
compressed and uncompressed network.

Keywords: Neural Network Veriﬁcation · Global Optimization · Convex
Optimization · Safety Critical Systems

1

Introduction

The incorporation of deep neural networks (DNNs) into safety critical systems
is limited by our ability to provide guarantees on their behavior [1], [2]. Neural
network veriﬁcation tools aim to provide these guarantees by proving whether a
network satisﬁes a given input-output property [3]. When input-output relation-
ships are diﬃcult to specify, analyzing a system may require the solution to an
optimization problem [4].

 
 
 
 
 
 
In this paper, we focus on solving optimization problems involving feedfor-
ward ReLU networks with low-dimensional inputs. Neural networks that control
dynamical systems from state estimates often have low input dimension. For
example, the ACAS Xu networks for aircraft collision avoidance have a ﬁve-
dimensional input [2]. Additionally, semantic perturbations to high dimensional
spaces can be analyzed through low dimensional networks [4]. When the input
space is low-dimensional, it can more easily be decomposed into smaller regions,
each deﬁning a simpler optimization problem. We leverage this insight by rapidly
dividing the input space into smaller regions that can be more tightly approxi-
mated, realizing a signiﬁcant performance gain and ﬁnding the optimal value to
a desired tolerance.

We consider the following three optimization problems, each of which is
motivated by an application related to verifying the behavior of safety critical
systems:

– Minimizing a convex function of the output of a network. This problem can
be used to reason about the actions of a control network [5], [6]. It can also be
used to evaluate a generative adversarial network (GAN), which is a network
architecture often used to model high-dimensional data distributions, by
calculating the recall metric [4], [7].

– Minimizing a convex function of the output of two networks in series subject
to an adversarial attack at the output of the ﬁrst network. This problem can
be used to consider adversarial attacks on the input of a network when the
input space is itself modeled by another network [8].

– Maximizing the diﬀerence between the outputs of two networks given the same
input. This problem can be used to compare a compressed and uncompressed
network.

Minimizing a convex function of the output can be used to solve many neural
network veriﬁcation problems [3], [9]. The other two problems have received less
attention in the literature.

In this work, we propose the Zonotope Propagation with Eagerness (ZoPE)
optimizer, which solves these optimization problems to a desired tolerance by (i)
eagerly breaking down the problem by splitting the input region, and (ii) relying
on zonotope propagation to reason about the output reachable set from each input
region. We consider a more “eager” solver to be one which spends less time on its
bounding functions before splitting. We evaluate the optimizer through runtime
comparisons and qualitative demonstrations. We solve four of the standard ACAS
Xu neural network veriﬁcation benchmarks, and compare to state-of-the-art neural
network veriﬁcation tools ERAN [10], nnenum [11], and Marabou [12]. On
property 1, which can be solved as a linear optimization problem over the output
of the network, we observe a speedup of over 25× compared to the next best tool.
We also evaluate the runtime of ZoPE on a batch of linear optimization problems
from Katz, Corso, Strong, and Kochenderfer [4] and compare against a baseline
that mirrors RefineZono’s approach to verifying the ACAS Xu benchmark [10].
We observe a speedup of 85×. Lastly, we demonstrate how ZoPE can be used as

a tool to evaluate a generative adversarial network (GAN) and how it can be
used to compare compressed to non-compressed networks.

There have been numerous recent works in the ﬁeld of neural network veriﬁca-
tion. These approaches often focus on networks with piecewise linear activation
functions, such as the rectiﬁed linear unit (ReLU), and frequently take the form
of a branch and bound search [13]. Our optimizer does the same. Many break the
veriﬁcation problem into subproblems by case-splitting on the activation function
or dividing the input domain [11], [12], [14]–[17]. A survey by Liu, Arnon, Lazarus,
Strong, Barrett, and Kochenderfer [3] compares these veriﬁcation algorithms.

Many neural network veriﬁcation tools can be extended to solve optimization
problems [9], [18]. Inspired by this idea, the proposed optimizer uses components
from several veriﬁers — it eagerly splits the input domain like ReluVal [16],
propagates zonotopes like DeepZ [19], combines zonotope propagation with
input splitting like RefineZono [20], and can optimize functions on the output
like MIPVerify [18]. The pieces we drew from these diﬀerent approaches were
chosen in order to eagerly break down the input space while still limiting the
overapproximation at each step. We expected rapidly splitting would have an ad-
vantage on networks with low-dimensional inputs that hadn’t been fully explored
by existing optimizers.

This paper contains the following contributions:

– A uniﬁed optimizer for three global optimization problems over low input
dimension ReLU networks. These problems are of interest for verifying safety
critical systems.

– A comparison of this new optimizer to existing veriﬁers and optimizers
demonstrating a signiﬁcant improvement against the state of the art when
optimizing aﬃne functions.

– Demonstrations of optimization problems which project onto the range of a

network and ﬁnd the maximum diﬀerence between two networks.

2 Background

In this section we introduce notation, discuss the standard neural network
veriﬁcation problem, and compare it to the optimization problems that we focus
on. We view a network f as representing a function

We will only consider feedforward ReLU networks.

f : Rnin → Rnout

Geometric objects and operations. We will make use of several geomet-
ric objects. The ﬁrst is a hyperrectangle, the generalization of a rectangle to
n-dimensional space, which is deﬁned by a center c ∈ Rn and a radius r ∈ Rn
such that

H = {x ∈ Rn | c − r (cid:22) x (cid:22) c + r}

where (cid:22) is the elementwise ≤ between two vectors.

Hyperrectangles are a special case of a more general class of geometric objects
called zonotopes, which can be deﬁned as an aﬃne transform of the unit hypercube.
A zonotope Z can be represented using matrix G ∈ Rn×m whose columns are
referred to as generators, and a vector c ∈ Rn which is the center of the zonotope
as

Z = {y ∈ Rn | y = Gx + c, −1 ≤ xi ≤ 1 ∀i = 1, . . . , m}
Zonotopes are a subset of polytopes, and have symmetry about their center.
Optimizing a linear function over a hyperrectangle or a zonotope can be done
analytically instead of by solving a linear program [21], [22].

We will also use the Minkowski sum between two sets X and Y deﬁned as

X ⊕ Y = {x + y | x ∈ X, y ∈ Y }

This can be visualized as padding one set with the other.

Zonotope Propagation. A vital component of our approach will be ﬁnding an
overapproximation of the output reachable set for a given input region. There
are a variety of techniques to ﬁnd symbolic or concrete descriptions of such a
set [3], [16], [19]. One approach, used in the neural network veriﬁcation tool
DeepZ [19], propagates zonotopes through a network layer by layer. After each
layer the respective zonotope is an overapproximation of the reachable set for
that layer. The new zonotope is formed elementwise, with overapproximation
introduced for any dimension in the input zonotope that can be both negative and
positive. For dimensions where this is true, an additional generator is introduced
into the zonotope. The cost of computing this overapproximation is linear in
the number of existing generators. We refer readers to the original paper, in
particular Theorem 3.1, for details on this procedure [19]. We will make use of
this algorithm in our optimizer, although in principal other overapproximate
output reachable sets could be used. Exploring these alternatives is a promising
direction for future work.

3 Optimization Problems

The ﬁeld of neural network veriﬁcation has focused on checking input-output
properties with yes or no answers. Formally, for input sets X and Y a neural
network veriﬁcation tool tells us whether the property

x ∈ X =⇒ y ∈ Y

(1)

holds [3]. Recent work has explored extending these tools to solve optimization
problems [9]. In this work, we would like to address several optimization problems
involving neural networks. In each problem we will only consider optimizing over
hyperrectangular or zonotopic input sets.

Minimizing a convex function on the range of a network. Our ﬁrst

problem of interest is to minimize a convex function on the output of a network.
We can write this problem as

minimize
x

g(f (x))

subject to x ∈ X

(2)

where g is a convex function. This can be used to solve a variety of neural network
veriﬁcation problems as deﬁned in equation (1). We can view the problem of
projecting onto the range of a network as a special case with

g(f (x)) = (cid:107)f (x) − y0(cid:107)

(3)

An example use case is when f is a generative adversarial network (GAN). By
solving this optimization problem we can ﬁnd the closest possible generated
image to a ground truth image.

Noise buﬀer. We would like to optimize over the output of two networks
in series with an adversarial perturbation applied between the two networks. This
can be formulated as

minimize
x,z

g(f2(f1(x) + z)))

subject to x ∈ X

z ∈ Z

(4)

where Z is a zonotope of allowed perturbations and f1 and f2 are our two
networks in series. The addition of z from the set Z can be viewed as padding
the output manifold of the ﬁrst network. We will limit g to be convex in this
work. For an example of its use, consider if f1 is a generative model and f2 is
a control network. By solving this optimization problem, we can evaluate the
behavior of the controller with inputs deﬁned by the generative model and subject
to adversarial perturbations. Of note, this noise buﬀer optimization problem
could also be put into the form of the ﬁrst optimization problem in equation (2)
by considering an augmented input space that parameterizes the noise, then
connecting those extra inputs to the intermediate layer with skip connections or
a larger network. However, this could substantially increase the input dimension,
so we focus on the framing of the problem given in equation (4) and leave a
comparison with the alternative framing for future work.

Network diﬀerence. A third optimization problem of interest is to deter-
mine how diﬀerent the output of two networks can be if they take in the same
input. We can write this as

maximize
x

(cid:107)f1(x) − f2(x)(cid:107)p

subject to x ∈ X

(5)

for (cid:96)p norm with p ≥ 1. For an example of its use, consider if f1 is a large network
and f2 is a smaller “compressed” network that attempts to mimic the behavior

of f1. By solving this optimization problem, we can evaluate how closely f1 and
f2 will match. The non-convexity of this problem comes both from the network’s
non-convexity and from the fact that we would like to maximize rather than
minimize a convex function.

4 Approach

Our proposed approach takes the form of a branch and bound search for the
optimum value. The components within this branch and bound search will vary
between optimization problems but share some common elements, including input
splitting and zonotope propagation. Below we ﬁrst sketch the general branch
and bound algorithm and then discuss how it can be applied to each of the
optimization problems of interest.

4.1 Optimization With Branch and Bound

Branch and bound is an approach to optimization which repeatedly breaks
down a problem into smaller sub-problems, bounding the optimal value of each
sub-problem as it goes, and using those bounds to prune regions of the search
space [23], [24]. Suppose we would like to minimize an objective over some
region. The branch and bound algorithm requires three functions: (i) Split, (ii)
UpperBound, and (iii) LowerBound. The function Split splits a problem into
multiple subproblems, LowerBound ﬁnds a lower bound on the optimal value
for a sub-problem, and upperBound(f)inds an upper bound on the optimal
value for a sub-problem. The algorithm maintains a priority queue of subproblems
ordered by their associated lower bound on the objective from LowerBound,
with highest priority given to the subproblem with the lowest lower bound. Some
or all subproblems will also have associated upper bounds on their optimal value
from UpperBound. At each step, the subproblem with lowest lower bound is
removed from the queue and split. Each new subproblem then has its lower bound
evaluated and is added back onto the queue. The new subproblems may have an
upper bound on their minimum objective evaluated as well, and those that don’t
inherit the upper bound of their parent subproblem.

The optimality gap at any point is given by the diﬀerence between the
lowest lower bound and the lowest upper bound across the open subproblems
(those in the priority queue). If the optimality gap ever falls below a tolerance
(cid:15) ≥ 0, the algorithm can return with a value within (cid:15) of the global optimum.
The subproblems with lower bound greater than the lowest upper bound are
eﬀectively pruned, as they will never be revisited in the search for the optimum.
If we would like to maximize instead of minimize an objective, we can reframe
the problem as minimizing the negative of the original objective. Many neural
network veriﬁcation tools can be viewed as performing a branch and bound search
for violations of a property [13].

In our case, the problem will correspond to an input set X that we would like
to optimize over, and the subproblems will be regions from this original set. In this

work we will only consider zonotope input sets, which includes hyperrectangles.
In order to solve the optimization problems described in section 3 with the
generic branch and bound algorithm, we will describe how to implement the
three functions required: (i) Split, (ii) UpperBound, and (iii) LowerBound.

4.2 Split, UpperBound, LowerBound

We will start by addressing Split, which will be common to each of the problems
we would like to solve. For a zonotope input set Zin ⊆ Rnin deﬁned by ngen
generators G ∈ Rnin×ngen and center c ∈ Rnin, we choose to split along the
generator with largest (cid:96)2 norm using Proposition 3 from the work of Althoﬀ,
Stursberg, and Buss [25]. This approach splits a zonotope into two zonotopes,
but these zonotopes may have a non-empty intersection. Their union will be
guaranteed to contain the original zonotope.

For a hyperrectangular input set, we choose the dimension with largest radius
and split the hyperrectangle halfway along that dimension into two hyperrectan-
gles. The interiors of the hyperrectangles will have an empty intersection. We
experimented with a simple gradient based splitting heuristic but did not see
an improvement to the performance. This may have been the result of the par-
ticular geometry of these networks. The computation required for the zonotope
propagation at each step depends on the number of network activation regions,
which are sets where the activation pattern of the network is constant, that
overlap with the current input region. As a result, we conjecture that a splitting
strategy which aims to mold the subregions to match the geometric structure of
the activation regions may be beneﬁcial. Other gradient or duality based input
splitting heuristics from neural network veriﬁcation tools may lead to better
splits and should be explored in the future [16], [26]. Since we rely on splitting
the input space, we expect our approach to scale poorly to high dimensions.

The approach to UpperBound will also be similar across our problems. For
the upper bound on the optimization problem over a region, we will evaluate the
objective for a single point in the region. As an achievable objective, this will
always upperbound the minimum achievable objective. We choose to evaluate
the center of our input region. We experimented with a ﬁrst order method to
choose the point to evaluate but found limited beneﬁt, and as a result chose to
keep the heuristic of using the center point for simplicity. The optimality gap
depends on two factors: the value of the achievable objective and the size of
the input region. The overapproximation from propagating the input region is
often more substantial, so choosing a better achievable objective does little to
improve runtime. As a result, even with a better heuristic there is a limit to the
performance gains from the LowerBound function. Many adversarial attacks
could be repurposed to perform some local exploration for this step [27], and the
tradeoﬀ between the runtime of the UpperBound function and the ability to
reduce the optimality gap sooner could be explored. For the noise buﬀer problem,
to ﬁnd an upperbound we hold the input to the ﬁrst network constant at the
cell’s center, leading to an output y1 from the ﬁrst network. To account for points

in the buﬀered region, we then optimize our objective over the second network
with input given by the padded region {y1} ⊕ Z.

Next, we will focus on LowerBound for each of the optimization problems,
which diﬀers depending on the problem type. This function must map from a
zonotopic or hyperrectangular input region X to a lower bound on the objective
value.

Minimizing a convex function on the range of a network. To lower
bound a convex function over the output, we ﬁrst propagate the input set X
to a zonotopic output set Zout with generator Gout ∈ Rnout×ngen and center
cout ∈ Rnout which overapproximates the true output reachable set for this region.
We then solve the convex program

minimize
z

g(z)

subject to z ∈ Zout

(6)

The constraint z ∈ Zout is a set of linear constraints which can be written by
introducing variables x ∈ Rngen to get

minimize
z,x

g(z)

subject to − 1 ≤ xi ≤ 1 i = 1, . . . , ngen
z = Goutx + cout

(7)

We will return the optimal value p∗ of this convex program as the lower bound.
If g is an aﬃne function g(y) = a(cid:62)y + b, then the solution is analytic and is

given by

p∗ = c(cid:62)

outa + (cid:13)

(cid:13)G(cid:62)

outa(cid:13)

(cid:13)1 + b

(8)

where G is the generator matrix for the zonotope and c is the center of the
zonotope [28]. Computing this expression will typically be much faster than
solving a convex program, giving a large speedup when optimizing an aﬃne
function.

Additionally, checking whether the output of a network is always contained
within a polytope P = {x | Ax ≤ b, A ∈ Rn×m, b ∈ Rn} can be accomplished
by maximizing the maximum violation of the polytope’s constraints. We will
denote the ith row of A as a(cid:62)
i . This problem could either be solved with the
above framework through n separate queries with the negative violation of the
ith constraint as the objective g(y) = −a(cid:62)
i y − bi, or through a single query with
g(y) = − maxi(a(cid:62)
i y − bi) This objective is the negative of a pointwise maximum
of aﬃne functions, so is concave. Fortunately, although g is concave, minimizing
g over a zonotope can be accomplished with one linear optimization per row of
A, each of which is analytical. As a result, checking whether the output of a
network is always contained within a polytope P can be performed through n
separate queries which solve a single linear optimization at each step, or through
a single query which solves n linear optimizations at each step.

Lastly, if we are projecting onto the range of a network with g(y) = (cid:107)y − y0(cid:107),
the choice of norm will aﬀect the complexity of the optimization problem over a
zonotope. For example, with (cid:96)1 or (cid:96)∞ norms this can be formulated as a linear
program, while for the (cid:96)2 norm it will be a quadratic program. Future work could
explore using faster projection algorithms instead of solving a convex program at
each step which may yield signiﬁcant speedups.

Noise buﬀer. We would like to optimize a function over two networks in series
with a buﬀer of allowed perturbations Z after the ﬁrst layer. This is equivalent
to taking the Minkowski sum of the output manifold of the ﬁrst network and the
buﬀer. We would like to ﬁnd a lower bound on the objective that will approach
the true objective as the input cell grows smaller. We ﬁrst propagate the cell
through the ﬁrst network to get a zonotope Z1out which overapproximates the
reachable set. We then take the Minkowski sum of this zonotope with our buﬀer
to get

Zbuﬀered = Z1out ⊕ Z = {z1out + z | z1out ∈ Z1out , z ∈ Z}

Since zonotopes are closed under Minkowski sums, the resulting object will still
be a zonotope [29].

Our problem now becomes trying to lower bound our function g on this
buﬀered set. As our input cell becomes small, Z1out does as well, and Zbuﬀered
approaches the size of the buﬀer. Since the buﬀered zonotope will not become
arbitrarily small, if we were to just propagate Zbuﬀered through the second
network, we would incur some steady state error in our lower bound. To avoid
this overapproximation, we can solve the optimization problem from the buﬀered
zonotope to the output exactly. If the dimension of the intermediate space is
low, we could apply the algorithm we have already given for optimizing convex
functions over a single network. If the dimension is high, we can use another
optimization strategy such as encoding the second network using mixed-integer
constraints as done by NSVerify, MIPVerify, and ERAN [10], [18], [30],
then adding the objective and solving the resulting optimization problem with
an oﬀ-the-shelf MIP solver such as Gurobi or GLPK. Since this approach nests
another full optimization problem over the second half of the network within
each step of the original branch and bound, we expect the runtime to scale poorly
as the size of the perturbation set Z and the complexity of the second network
grow, which may limit the use of the proposed approach for this type of analysis.
In summary, to get a lower bound we (i) overapproximate the set passing
through the ﬁrst network, then (ii) solve the resulting optimization problem over
the second network with input set given by a buﬀered zonotope.

Network diﬀerence. Our goal is to ﬁnd the maximum diﬀerence in the output
of two networks over an input region. Since we are maximizing a function, we
are interested in ﬁnding an upper bound on the objective over our input cell. We
start by propagating the input cell through the ﬁrst network to get Z1out and the
second network to get Z2out . We can then tightly overapproximate each of these
zonotopes as hyperrectangles H1 and H2 by ﬁnding their maximum and minimum

value in each elementary direction. Each of these operations can be performed
analytically. Once we have these two hyperrectangular overapproximations, we
are interested in solving

maximize
h1,h2

(cid:107)h1 − h2(cid:107)p

subject to h1 ∈ H1
h2 ∈ H2

(9)

whose optimal value will upper bound the true maximum distance in this region.
Let c1 and c2 be the centers of H1 and H2 and r1 and r2 be the radius of H1
and H2 in each elementary direction. An analytical solution to this optimization
problem is given by

h∗
1 = c1 + sign(c1 − c2) (cid:12) r1
h∗
2 = c2 + sign(c2 − c1) (cid:12) r2
d∗ = (cid:107)h∗
2(cid:107)p

1 − h∗

where (cid:12) represents elementwise multiplication and d∗ is the optimal value. See
Appendix A.1 for a derivation of this analytical solution. Returning d∗ as deﬁned
above will upper bound the objective function.

4.3

Implementation

Each of the approaches described in section 4.2 were implemented in a Julia
package.1 This repository also has code to reproduce the benchmarks on our
optimizer in section 5. The zonotope propagation and zonotope splitting is
performed with the LazySets library.2 For solving linear and mixed-integer linear
programs we use Gurobi and for solving other convex programs we use Mosek,
both of which have a free academic license.3 The implementation is modular and
is intended to be easily extended to solve other optimization problems.

5 Experimental Results

We apply ZoPE to a variety of problems, ﬁrst comparing its runtime to existing
solvers on the ACAS Xu benchmark and linear optimization problems. We then
showcase how it can be used to solve problems with more complex objectives.
In several of these experiments we use a conditional GAN trained to represent
images from a wing-mounted camera on a taxiing aircraft. The conditional GAN
has four inputs, two of which are the crosstrack position and heading while the
other two are latent inputs. We also use a state estimation network which takes as
input a 128-dimensional image of the taxiway and outputs the state of the aircraft.

1 Source is at https://github.com/sisl/NeuralPriorityOptimizer.jl.
2 Source is at https://github.com/JuliaReach/LazySets.jl.
3 Available at https://www.gurobi.com and https://www.mosek.com.

The GAN and state estimation network can be combined in series. All timing is
done on a single core of an Intel Xeon 2.20GHz CPU and with an optimality gap
of 1 × 10−4 unless otherwise speciﬁed. All queries use hyperrectangular input
sets; in future work it would be valuable to explore the runtime consequences
when splitting non-hyperrectangular input zonotopes as well.

5.1 ACAS Xu Benchmark

The ACAS Xu neural network veriﬁcation benchmark contains a set of properties
on networks trained to compress the ACAS Xu collision avoidance system and is
often used to benchmark veriﬁcation tools [2], [15]. We will consider properties 1,
2, 3, and 4 introduced by Katz, Barrett, Dill, Julian, and Kochenderfer [15]. We
compare to the neural network veriﬁcation tools Marabou [12], nnenum [11],
and ERAN [10], [19], [20], [31]. See Appendix A.2 for details on how each solver
was conﬁgured. Property 1 can be evaluated by maximizing a linear function,
while properties 2, 3, and 4 can be evaluated by minimizing the convex indicator
function to the output polytope associated with the property or by minimizing
the distance to the output polytope associated with the property. Viewed in
another way, property 1 can be solved by asking the question “Is the network
always contained in a polytope?” while property 2 can be solved by asking the
question “Does the network ever reach a polytope?” For property 1 each step
is analytical, while for properties 2, 3, and 4 at each step we apply a quick
approximate check for intersection, and if it is indeterminate we solve a linear
program. Each veriﬁcation tool was run on a single core.

Figure 1 shows the performance of the optimizer on four ACAS properties.
ZoPE achieves a speedup of about 25× on property 1. We remain competitive
with the other tools on properties 2, 3, and 4, where we may need to solve a
linear program at each step.

5.2 Optimizing Convex Functions

We ﬁrst evaluate ZoPE maximizing a linear objective. We run queries on a
network composed of the conditional GAN concatenated with the image-based
control network. This combined network was introduced by Katz, Corso, Strong,
and Kochenderfer [4] and has an input of two states and two latent dimensions.
The objective function corresponds to the control eﬀort. The baseline we compare
against divides the state dimensions into hyperrectangular cells, propagates a
zonotope through each cell with DeepZ’s approach, then uses the resulting
bounds to formulate a mixed-integer program and ﬁnd the optimum for that
cell. Since we run these queries sequentially, each mixed-integer program also has
a constraint that the objective should be larger than the best seen so far. The
strategy of interleaving splitting and MIP calls mirrors RefineZono’s approach
to verifying the ACAS Xu networks [10]. Table 1 shows more than an 85×
speedup of our approach over the baseline. The eﬃciency of ZoPE relies heavily
on the computational cost of ﬁnding bounds for the objective over a zonotope.
As a result, like with ACAS property 1 we see substantially better performance

s
e
c
n
a
t
s
n
I

d
e
v
l
o
S

s
e
c
n
a
t
s
n
I

d
e
v
l
o
S

Property 1

Property 2

40

20

0

40

20

10−2

100
Property 3

102

40

20

0

40

20

10−2

100
Property 4

102

NNENUM
ERAN
Marabou
ZoPE (ours)

0
10−4 10−2

100
Time (s)

102

0
10−3

10−1

101
Time (s)

103

Fig. 1: Comparison of Solvers on ACASXu Properties 1, 2, 3, and 4 with a 300
second timeout.

Table 1: Performance on linear optimization problems. 25 queries in diﬀerent
regions of the input space are run on a single network. The network was introduced
in Katz, Corso, Strong, and Kochenderfer [4] and consists of a conditional GAN
concatenated with an image-based controller. The performance of the MIP
approach with a variety of discretizations of the state space is shown. For example,
MIP 3 × 3 corresponds to an optimizer which for each query (i) discretizes the
input space into a 3 × 3 grid, then (ii) for each cell in the grid ﬁnds bounds on
each node using the approach of DeepZ, and (iii) solves the resulting MIP using
Gurobi.

Approach
MIP 3 × 3
MIP 5 × 5
MIP 10 × 10
MIP 15 × 15
ZoPE (ours)

Total Time (s)

3728
1171
1610
2473
13.5

than existing tools when optimizing an objective with only analytical operations
at each step.

Next, we demonstrate using the proposed optimizer to project an image onto
the output manifold of a conditional GAN. The GAN has a ﬁnite, convex support
for its latent variables. This allows us to project onto the range of the network,

True Images

Generated Images

Fig. 2: Closest generated images (bottom row) to a set of true images (top row)
with distance measured by the (cid:96)2 norm.

under some (cid:96)p norm, by minimizing the convex objective function in equation (3).
Figure 2 shows several images and their corresponding closest generated images
from the GAN. The visual similarity between the two rows gives some evidence
that the GAN is capturing the desired images in its output manifold. However, we
still see some slight diﬀerences between the images. The degree of these diﬀerences
can be used to measure how closely the GAN captures each training datapoint,
giving a recall metric to evaluate a GAN and inform hyperparameter choice, as
was done in Katz, Corso, Strong, and Kochenderfer [4]. Note that this analysis,
and the sense of “closeness” in this context, depends on the norm used for the
projection.

5.3 Maximum Distance Between Compressed and Original

Networks

By ﬁnding the maximum distance between the outputs of two networks as
described in section 4.2, we can evaluate how well a compressed network mimics
the behavior of an original uncompressed network. We validate this technique on
a large conditional GAN, with two input states to be conditioned on, two latent
dimensions, four layers with 256 ReLUs each, and a 128 dimensional output layer.
The second “compressed” network has the same input and output spaces, but
only two layers with 128 ReLUs each. We use a required optimality gap of 0.1.
The heatmap in ﬁgure 3 shows the maximum diﬀerence in the output of these
networks across a slice of the state space. These maximum diﬀerences, or an
approximation thereof, could be used to retrain the network in regions where the
diﬀerence is large.

6 Conclusion

In this work, we introduced an algorithm for solving a wide variety of optimization
problems on feedforward ReLU networks with low input dimension. The algorithm
relies on eagerly splitting the input space and making use of zonotope propagation
through the network to bound the optimum at each step. We observe a speedup
of 25× on property 1 of the ACAS Xu benchmark compared to several existing
veriﬁcation tools, and 85× on a linear optimization benchmark compared to a
mixed-integer programming baseline. We also demonstrate how the optimizer can

)
s
e
e
r
g
e
d
(

θ

20

0

−20

20

15

10

−10

−5

0
x (meters)

5

Fig. 3: The maximum output distance in L1 norm of two networks over the state
space.

be used to analyze how closely a GAN has learned to replicate its training data
and how it can be used to compare a compressed and uncompressed network.
The optimizer was implemented modularly and was made available as a Julia
package at https://github.com/sisl/NeuralPriorityOptimizer.jl so as to
ﬂexibly allow for a reader to explore solving other optimization problems. Any
non-convex objective which can be optimized over a zonotope can readily be
optimized in this framework, as was demonstrated in both our approach to check
whether the output of a network is contained within a polytope and to maximize
the distance between the output of two networks.

There are several major avenues for future work. The often prohibitive growth
of the runtime with the input dimension, depth, and width of the network remains
as a signiﬁcant challenge for this and other exact optimizers. One direction of
interest would be to develop more specialized lower bound functions for particular
problems. For example, faster intersection or projection algorithms may be applied
to some problems where our implementation solves a convex program at each step.
We could also incorporate and compare some of the optimizations that ERAN
makes use of; for example, mixing mixed-integer program solves in with the
splitting, tightening the propagated zonotopes, or propagating polytopes instead
of zonotopes. Another would be to consider how to scale up to high-dimensional
input spaces, and consider what a more eager splitting strategy looks like in
those contexts. Lastly, we could ﬁnd other optimization problems of interest over
neural networks that could be solved with the same or a similar framework.

Acknowledgments

We would like to acknowledge support from Eric Luxenberg, Haoze Wu, Gagan-
deep Singh, Chelsea Sidrane, Joe Vincent, Changliu Liu, Tomer Arnon, and
Katherine Strong.

Funding in support of this work is from DARPA under contract FA8750-18-
C-009, the NASA University Leadership Initiative (grant #80NSSC20M0163),
and the National Science Foundation Graduate Research Fellowship under Grant
No. DGE–1656518. Any opinions, ﬁndings, and conclusions or recommendations
expressed in this material are those of the authors and do not necessarily reﬂect
the views of DARPA, any NASA entity, or the National Science Foundation.

References

[1] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal, L.
Jackel, M. Monfort, U. Muller, J. Zhang, X. Zhang, J. Zhao, and K. Zieba, End
to end learning for self-driving cars, Technical Report. http://arxiv.org/abs/
1604.07316, 2016.

[2] K. D. Julian, M. J. Kochenderfer, and M. P. Owen, “Deep neural network
compression for aircraft collision avoidance systems,” AIAA Journal of Guidance,
Control, and Dynamics, vol. 42, no. 3, pp. 598–608, 2019.

[3] C. Liu, T. Arnon, C. Lazarus, C. Strong, C. Barrett, and M. J. Kochenderfer,
“Algorithms for verifying deep neural networks,” Foundations and Trends® in
Optimization, vol. 4, no. 3-4, pp. 244–404, 2021.

[4] S. M. Katz, A. L. Corso, C. A. Strong, and M. J. Kochenderfer, “Veriﬁcation of
image-based neural network controllers using generative models,” Digital Avionics
Systems Conference (DASC), 2021.

[5] K. D. Julian, R. Lee, and M. J. Kochenderfer, “Validation of image-based neural

network controllers through adaptive stress testing,” 2020.

[6] S. M. Katz, K. D. Julian, C. A. Strong, and M. J. Kochenderfer, “Generating
probabilistic safety guarantees for neural network controllers,” Machine Learning,
no. 2103.01203, 2021.

[7] T. Kynkäänniemi, T. Karras, S. Laine, J. Lehtinen, and T. Aila, “Improved
precision and recall metric for assessing generative models,” Advances in Neural
Information Processing Systems (NeurIPS), 2019.

[8] M. Mirman, T. Gehr, and M. Vechev, “Robustness certiﬁcation with generative
models,” ACM SIGPLAN International Conference on Programming Language
Design and Implementation, 2021.

[9] C. A. Strong, H. Wu, A. Zeljić, K. D. Julian, G. Katz, C. Barrett, and M. J.
Kochenderfer, “Global optimization of objective functions represented by ReLU
networks,” Machine Learning, no. 2010.03258, 2021.

[10] G. Singh, T. Gehr, M. Püschel, and M. T. Vechev, “Boosting robustness certiﬁca-
tion of neural networks.,” in International Conference on Learning Representa-
tions, 2019.

[11] S. Bak, H.-D. Tran, K. Hobbs, and T. T. Johnson, “Improved geometric path
enumeration for verifying relu neural networks,” in International Conference on
Computer-Aided Veriﬁcation, Springer, 2020, pp. 66–96.

[12] G. Katz, D. A. Huang, D. Ibeling, K. Julian, C. Lazarus, R. Lim, P. Shah, S.
Thakoor, H. Wu, A. Zeljić, et al., “The Marabou framework for veriﬁcation and
analysis of deep neural networks,” in International Conference on Computer-Aided
Veriﬁcation, Springer, 2019, pp. 443–452.

[13] R. Bunel, P Mudigonda, I. Turkaslan, P Torr, J. Lu, and P. Kohli, “Branch
and bound for piecewise linear neural network veriﬁcation,” Journal of Machine
Learning Research, vol. 21, no. 2020, pp. 1–39, 2020.

[14] R. Ehlers, “Formal veriﬁcation of piece-wise linear feed-forward neural networks,”
in International Symposium on Automated Technology for Veriﬁcation and Anal-
ysis, Springer, 2017, pp. 269–286.

[15] G. Katz, C. Barrett, D. L. Dill, K. Julian, and M. J. Kochenderfer, “Reluplex:
An eﬃcient smt solver for verifying deep neural networks,” in International
Conference on Computer-Aided Veriﬁcation, Springer, 2017, pp. 97–117.
[16] S. Wang, K. Pei, J. Whitehouse, J. Yang, and S. Jana, “Formal security analysis
of neural networks using symbolic intervals,” in U SEN IX Security Symposium,
2018, pp. 1599–1614.

[17] H. Wu, A. Ozdemir, A. Zeljic, A. Irfan, K. Julian, D. Gopinath, S. Fouladi,
G. Katz, C. S. Pasareanu, and C. W. Barrett, “Parallelization techniques for
verifying neural networks,” CoRR, vol. abs/2004.08440, 2020. arXiv: 2004.08440.
[18] V. Tjeng, K. Xiao, and R. Tedrake, “Evaluating robustness of neural networks
with mixed integer programming,” International Conference on Learning Repre-
sentations, 2017.

[19] G. Singh, T. Gehr, M. Mirman, M. Püschel, and M. T. Vechev, “Fast and eﬀective
robustness certiﬁcation.,” Advances in Neural Information Processing Systems
(NeurIPS), 2018.

[20] G. Singh, T. Gehr, M. Püschel, and M. Vechev, “An abstract domain for certifying
neural networks,” Proceedings of the ACM on Programming Languages, vol. 3,
no. POPL, pp. 1–30, 2019.

[21] S. Fujishige, Submodular Functions and Optimization. Elsevier, 2005.
[22] T. Kitahara and N. Sukegawa, “A simple projection algorithm for linear program-

ming problems,” Algorithmica, vol. 81, no. 1, pp. 167–178, 2019.

[23] E. L. Lawler and D. E. Wood, “Branch-and-bound methods: A survey,” Operations

Research, vol. 14, no. 4, pp. 699–719, 1966.

[24] M. J. Kochenderfer and T. A. Wheeler, Algorithms for Optimization. MIT Press,

2019.

[25] M. Althoﬀ, O. Stursberg, and M. Buss, “Reachability analysis of nonlinear systems
with uncertain parameters using conservative linearization,” in IEEE Conference
on Decision and Control (CDC), 2008, pp. 4042–4048.

[26] V. Rubies-Royo, R. Calandra, D. M. Stipanovic, and C. Tomlin, “Fast neural
network veriﬁcation via shadow prices,” arXiv preprint arXiv:1902.07247, 2019.
[27] X. Yuan, P. He, Q. Zhu, and X. Li, “Adversarial examples: Attacks and defenses
for deep learning,” IEEE Transactions on Neural Networks and Learning Systems,
vol. 30, no. 9, pp. 2805–2824, 2019.

[28] M. Althoﬀ and G. Frehse, “Combining zonotopes and support functions for
eﬃcient reachability analysis of linear systems,” in IEEE Conference on Decision
and Control (CDC), 2016, pp. 7439–7446.

[29] M. Althoﬀ, “On computing the minkowski diﬀerence of zonotopes,” arXiv preprint

arXiv:1512.02794, 2015.

[30] A. Lomuscio and L. Maganti, “An approach to reachability analysis for feed-

forward relu neural networks,” arXiv preprint arXiv:1706.07351, 2017.

[31] G. Singh, R. Ganvir, M. Püschel, and M. Vechev, “Beyond the single neuron
convex barrier for neural network certiﬁcation,” Advances in Neural Information
Processing Systems (NeurIPS), vol. 32, pp. 15 098–15 109, 2019.

A Appendix

A.1 Maximum Distance Between Points in Two Hyperrectangles

We would like to derive an analytical solution for the maximum distance given by
a p-norm with p ≥ 1 between two hyperrectangles H1 and H2. We will let c1 and
c2 be the centers of H1 and H2, and r1 and r2 be the radii of H1 and H2. The
maximum distance can be found by solving the following optimization problem

maximize
h1,h2

(cid:107)h1 − h2(cid:107)p

subject to h1 ∈ H1
h2 ∈ H2

The p-norm for ﬁnite p is deﬁned as

n
(cid:88)

(cid:107)x(cid:107)p = (

|(x)i|p)

1
p

i=1

We expand the objective of our maximization problem to be

n
(cid:88)
(

i=1

(|(h1)i − (h2)i|p))

1
p

1
p is monotonically increasing on the non-negative reals for p ≥ 1, we

and since x
can remove the power of 1

p giving us the equivalent problem

maximize
h1,h2

n
(cid:88)

i=1

(|(h1)i − (h2)i|p)

subject to h1 ∈ H1
h2 ∈ H2

(10)

Now we see that the constraints h1 ∈ H1 and h2 ∈ H2 apply independent
constraints to each dimension of h1 and h2. We also note that the objective can
be decomposed coordinate-wise. As a result, in order to solve this optimization
problem, we will need to solve n optimization problems of the form

maximize
(h1)i,(h2)i

subject to

|(h1)i − (h2)i|p

(c1)i − (r1)i ≤ (h1)i ≤ (c1)i + (r1)i
(c2)i − (r2)i ≤ (h2)i ≤ (c2)i + (r2)i

(11)

Since xp is monotonically increasing for p ≥ 1 we can equivalently maximize
|(h1)i − (h2)i|. We show an analytic form for the maximum by checking cases.
If (c2)i is larger than (c1)i, the maximum will be found by pushing (h2)i to its
upper bound and (h1)i to its lower bound. Conversely, if (h1)i is larger than

(h2)i, the maximum will be found by pushing (h1)i to its upper bound and (h2)i
to its lower bound. If (c1)i is equal to (c2)i, then we can arbitrarily choose one
to push to its lower bound and the other to push to its upper bound — we select
(h1)i to go to its upper bound and (h2)i to go to its lower bound. As a result we
have the optimal inputs

(h1)∗
(h2)∗

i = (c1)i + sign((c1)i − (c1)i)(r1)i
i = (c2)i + sign((c2)i − (c2)i)(r2)i

where the sign function is given by

sign(x) =

(cid:40)

1.0
x ≥ 0
−1.0 x < 0

Then, backtracking to our original problem and vectorizing gives us the analytical
solution to this optimization problem with optimal value d∗

h∗
1 = c1 + sign(c1 − c2) (cid:12) r1
h∗
2 = c2 + sign(c2 − c1) (cid:12) r2
d∗ = (cid:107)h∗
2(cid:107)p

1 − h∗

where the sign function is applied elementwise. This completes our derivation of
the analytical solution for the maximum distance between two points contained
in two hyperrectangles.

A.2 Veriﬁer Conﬁguration for the Collision Avoidance Benchmark

This section describes how each veriﬁer was conﬁgured for the collision avoid-
ance benchmark discussed in section 5.1. Table 2 summarizes the non-default
parameters for each solver and the location where the parameter was set. Both
NNENUM and ERAN by default make use of parallelization, and Marabou
has a parallel mode of operation, but for this experiment we restrict all tools
to a single core. We ran the experiments on a single core to try to separate the
aspects of how each solver was parallelized from what we viewed as the core
of its algorithmic approach. We expect ZoPE would parallelize well, especially
on more challenging problems. The hyperparameters we ran for ERAN may be
better suited for multiple cores than a single core, so further comparison could
explore these in more depth. Additionally, the timing results from the Veriﬁcation
of Neural Networks 2020 competition4 for several properties for ERAN were
slower than we expected from the change in hardware and the restriction to
a single core. Exploring the tool further, we observed that on several problem
instances it would return back a failed status before reaching a timeout. On these
same instances we saw that ERAN would ﬁnd several inputs that were almost
counter-examples, for example with a margin of 1 × 10−6 from violating the

4 https://sites.google.com/view/vnn20/vnncomp

property, ﬂag these as potential counter-examples, then move on. It is possible
that the root cause of the abnormalities we observed aﬀected timing results. On
problems where ERAN did return a status the results were consistent with the
ground truth.

The parameters were chosen based oﬀ of a mix of recommendations from
developers on their best conﬁguration for the collision avoidance benchmark or
existing documented settings for this benchmark. For example, ERAN’s param-
eters were based oﬀ of the VNN20 competition as found at https://github.com/
GgnDpSngh/ERAN-VNN-COMP/blob/master/tf_verify/run_acasxu.sh. The code
for for Marabou,5 NNENUM,6 ERAN,7 and our optimizer ZoPE8 is available
for free online.

Table 2: Non-Default Veriﬁer Parameters

Solver

Parameter

Marabou

Value

Location

split-threshold
INTERVAL_SPLITTING_FREQUENCY

Command line argument
1
1 GlobalConﬁguration.cpp ﬁle

NNENUM

ERAN

ZoPE

Settings.NUM_PROCESSES

1

acasxu_all.py ﬁle

use_parallel_solve
processes
domain
complete
timeout_milp
numproc

stop_gap
stop_frequency

True
1
deeppoly
True
10
1

1 × 10−4
1

__main__.py ﬁle
__main__.py ﬁle
Command line argument
Command line argument
Command line argument
Command line argument

acas_example.jl
acas_example.jl

5 https://github.com/NeuralNetworkVerification/Marabou
6 https://github.com/stanleybak/nnenum
7 https://github.com/eth-sri/eran
8 https://github.com/sisl/NeuralPriorityOptimizer.jl

