Automatic structured variational inference

1
2
0
2

b
e
F
0
1

]
L
M

.
t
a
t
s
[

3
v
3
4
6
0
0
.
2
0
0
2
:
v
i
X
r
a

Luca Ambrogioni1

Kate Lin2

Emily Fertig2

Sharad Vikram2

Max Hinne1

Marcel van Gerven1
Dave Moore2
1Department of Artiﬁcial Intelligence, Donders Institute for Brain, Cognition and Behaviour
Radboud University, Nijmegen, the Netherlands
2Google Research, San Francisco, CA, USA

Abstract

Stochastic variational inference oﬀers an at-
tractive option as a default method for dif-
ferentiable probabilistic programming. How-
ever, the performance of the variational ap-
proach depends on the choice of an appro-
priate variational family. Here, we introduce
inference
automatic structured variational
(ASVI), a fully automated method for con-
structing structured variational families, in-
spired by the closed-form update in conju-
gate Bayesian models. These convex-update
families incorporate the forward pass of the
input probabilistic program and can there-
fore capture complex statistical dependencies.
Convex-update families have the same space
and time complexity as the input probabilis-
tic program and are therefore tractable for
a very large family of models including both
continuous and discrete variables. We validate
our automatic variational method on a wide
range of low- and high-dimensional inference
problems. We ﬁnd that ASVI provides a clear
improvement in performance when compared
with other popular approaches such as the
mean-ﬁeld approach and inverse autoregres-
sive ﬂows. We provide an open source imple-
mentation of ASVI in TensorFlow Probability.

1

Introduction

The aim of probabilistic programming is to automate
every aspect of probabilistic inference in arbitrary prob-
abilistic models (programs), so that the user can focus
her attention on modeling. Stochastic gradient-based
variational methods have recently emerged as a power-
ful alternative to MCMC for inference in diﬀerentiable

Preprint version. Full paper available in the proceedings of
the 24th International Conference on Artiﬁcial Intelligence
and Statistics (AISTATS) 2021, San Diego, California, USA.
PMLR: Volume 130. Copyright 2021 by the author(s).

probabilistic programming languages (Wingate and
Weber, 2013; Kucukelbir et al., 2017; Tran et al., 2016;
Kucukelbir et al., 2015; Bingham et al., 2019). This
trend is a consequence of the increasing automatization
of variational inference (VI) techniques, which evolved
from requiring highly mathematically sophisticated and
model-speciﬁc tools, to generic algorithms that can be
applied to a broad class of problems without model-
speciﬁc derivations (Hoﬀman et al., 2013; Kingma and
Welling, 2013; Hernández-Lobato and Adams, 2015;
Ranganath et al., 2014). However, applications of VI
usually require the user to specify a parameterized
variational family. In general, it is relatively easy to au-
tomate the construction of the variational distribution
under a mean-ﬁeld approximation, where the approx-
imate posterior distribution factorizes as a product
of univariate distributions. However, while there is a
substantial body of model-speciﬁc research on struc-
tured variational families, few existing methods can
be used for automatically constructing an appropriate
scalable structured variational approximation for an
arbitrarily chosen probabilistic model. Instead, existing
methods either ignore most of the prior structure of
the model (e.g. ADVI with the multivariate Gaussian
distribution (Kucukelbir et al., 2017)) or require strict
assumptions such as local conjugacy (e.g. structured
stochastic VI (Hoﬀman and Blei, 2015)). In addition,
several of these methods require the use of ad-hoc gradi-
ent estimators or variational lower bounds (Tran et al.,
2015; Ranganath et al., 2016).

In this paper we introduce an automatic procedure
for constructing variational approximations that incor-
porate the structure of the probabilistic model, while
being ﬂexible enough to capture the distribution of the
observed data. The construction of these variational
approximations is fully automatic and the resulting
variational distribution has the same time and space
complexity as the input probabilistic program. The new
family of variational models, which we call the convex-
update variational family, interpolates the evidence
coming from the observed data with the feedforward
structure of the model prior. Speciﬁcally, the parame-
ters of the posterior distribution of each latent variable

 
 
 
 
 
 
Automatic structured variational inference

are a convex combination of the parameters induced
by the parents of that variable, and a term reﬂecting
the inﬂuence of the data. This mimics the evidence
update in the expectation parameters of conjugate ex-
ponential family models, where this posterior is given
in closed form. The convex-update variational family
can be trained using standard inference techniques and
gradient estimators and can therefore be used as a
drop-in replacement of the mean-ﬁeld approach in au-
tomatic diﬀerentiation stochastic VI. We refer to this
new form of fully automatic inference as automatic
structured variational inference (ASVI). A full imple-
mentation is open-sourced in TensorFlow Probability
as tfp.experimental.build_asvi_surrogate_posterior.

2 Preliminaries

VI is used to approximate the posterior over the la-
tent variables of a probabilistic program p(x) with a
member of a parameterized family of probability distri-
butions q(x; ψ). The vector of variational parameters
ψ is obtained by maximizing the evidence lower bound
(ELBO):

Eq(x;ψ)

(cid:20)

log

[ψ] =

L

−

q(x; ψ)

L (y

x) p(x)

|

(cid:21)

,

(1)

|

where L (y
x) is a likelihood function. The resulting
variational posterior (i.e. the maximum of this opti-
mization problem) depends on the choice of the pa-
rameterized family q(x; ψ) and is equal to the exact
posterior only when the latter is included in the family.
In this paper we restrict our attention to probabilistic
programs that are speciﬁed in terms of conditional prob-
abilities and densities chained together by deterministic
functions:

p(x) =

(cid:89)

j

ρj (xj

|

θj(πj)) ,

(2)

⊆ {

· | ·
xi

) is a family of probability distributions
where ρj (
and πj
i(cid:54)=j is a subset of parent variables such
that the resulting graphical model is a directed acyclic
graph (DAG). The vector-valued functions θj(πj) spec-
ify the value of the parameters of the distribution of
the latent variable xj given the values of all its parents.

}

2.1 Convex updates in conjugate models

Exponential family distributions play a central role
in Bayesian statistics as the only families that sup-
port conjugate priors, where the posterior is available
via an analytic expression (Diaconis and Ylvisaker,
1979). An exponential family distribution p(y) can be
parameterized by a vector of expectation parameters
µ = Ep(y)[T (y)], where T (y) is the vector of suﬃcient
statistics of the data. We can assign to these parameters
a conjugate prior distribution p(µ), which in turn is
parameterized by the prior expectation ¯µ0 = Ep(µ)[µ].
Upon observing N independently sampled datapoints,
under some boundary assumptions, it can be shown

that the posterior expectation parameters are a con-
vex combination of the prior parameters ¯µ0
and the
maximum likelihood estimator (Diaconis and Ylvisaker,
1979):

¯µ = λ

¯µ0 + (1

(3)
where λ is a vector of convex combination coeﬃcients
and
denotes the element-wise product. A derivation
of this result is given in Supplementary Material A.

µML ,

λ)

(cid:12)

(cid:12)

(cid:12)

−

For example, in a Gaussian model with known precision
τ and a Gaussian prior over the mean, the posterior
mean is given by

(cid:32)

(cid:33)

N
(cid:88)

¯µ =

¯µ0 +

τ0
τ0 + N τ

N τ
τ0 + N τ
where τ0 is the precision of the prior. This formula
shows that the posterior parameters are a trade-oﬀ
between the prior hyper-parameters and the values
induced by the data.

1
N

(4)

yn

n=1

,

3 Convex-update variational families

Consider the following probabilistic model:

(5)

|

|

|

θ) ,

p(x, y) = L(y
x)ρ(x
x) is a likelihood function and ρ (x

where L (y
θ)
is a prior distribution parameterized by a vector of
parameters θ. We do not assume the likelihood or the
prior to be in the exponential family; simply that the
parameters θ are deﬁned in a convex domain. We can
construct a convex-update parameterized variational
family by mimicking the form of the parameter update
rule in conjugate models:

|

|

λ

−

λ)

α) ,

θ + (1

q(x; λ, α) = ρ(x

(6)
(cid:12)
where λ is now a vector of learnable parameters with
entries ranging from 0 to 1 and α is a vector of learnable
parameters with the same range of possible values as θ.
In practice, it is convenient to express each λi as the
logistic sigmoid of a logit parameter, so that we can
perform unconstrained optimization in the logit space.

(cid:12)

In a model with a single latent variable, the convex-
update parameterization is over-parameterized and
equivalent to mean ﬁeld. However, the power of this
approach becomes evident in multivariate models con-
structed by chaining basic probability distributions
together. Consider a probabilistic program speciﬁed
in the form of Eq. (2). We can construct a structured
variational family by applying the convex-update form
to each latent conditional distribution in the model:

q(x; Λ, A) =

(cid:89)

(cid:16)

xj

ρj

j

αj
λj

| U

(cid:17)

[θj(πj)]

(7)

with Λ = (λ1, . . . , λJ ), A = (α1, . . . , αJ ) and the
convex update operator :
α
λ [θ] = λ

θ + (1

α .

(8)

λ)

U

(cid:12)

−

(cid:12)

The multivariate structured distributions induced by
this family have several appealing theoretical prop-

Ambrogioni, Lin, Fertig, Vikram, Hinne, Moore and van Gerven

import tensorflow as tf
import tensorflow_probability as tfp
tfd = tfp. distributions
num_timesteps = 20
innovation_scale = 0.1

@tfd. JointDistributionCoroutineAutoBatched
def brownian_motion ():

xt = 0.
for t in range( num_timesteps ):

xt = yield tfd. Normal (loc=xt ,

scale= innovation_scale )

# Omitted for brevity : definitions of variational
# parameters ‘lam ‘ and ‘alpha ‘ as ‘tf.Variable ‘s.
@tfd. JointDistributionCoroutineAutoBatched
def brownian_motion_surrogate_posterior ():

xt = 0.
for t in range( num_timesteps ):

xt = yield tfd. Normal (

loc =( lam[t][’loc ’] ∗ xt +

(1 - lam[t][’loc ’]) ∗ alpha[t][’loc ’]),
scale =( lam[t][’scale ’] ∗ innovation_scale +

(1 - lam[t][’scale ’]) ∗
alpha[t][’scale ’]))

Figure 1: TensorFlow Probability models for a discretized Brownian motion prior (left) and corresponding
structured variational surrogate (right). In TFP’s coroutine syntax, yielding a distribution samples a new random
variable. The surrogate program is a written-out version of the process implicitly (and automatically) deﬁned by
the automatic method in Figure 2. Note that it has the same control-ﬂow structure as the original model.

erties that justify their usage in structured inference
problems:

4 Discrete latent variables and

stochastic control ﬂow

1. The family always contains the original probabilis-
tic program (i.e. the prior distribution). This is
trivial to see as we can obtain the prior by setting
λ = 1. On the other hand, setting λ = 0 results in
the standard mean-ﬁeld approximation. Note that
none of the commonly used automatic structured
variational approaches share this basic property.

2. The family includes both the exact ﬁltering and
smoothing posterior of univariate linear Gaussian
time series models of the form

xt

∼ N

(cid:0)axt−1, σ2(cid:1) ,

yt

(cid:0)xt, ξ2(cid:1) .

(9)

∼ N

In this case, the ﬁltering conditional posterior is
given by the Kalman ﬁlter update:

¯µt+1 = (1
= (1

−
−

Kt)µt+1(xt) + Ktyt
Kt)axt + Ktyt ,

(10)

≤

≤

Kt

1 is the Kalman gain, correspond-
where 0
ing to λt in ASVI. The smoothing update has a
similar form where the data term is augmented
with an estimate integrating all future observa-
tions.

3. The convex-update family has a very parsimonious
parameterization compared to other structured
families. The number of parameters is 2P , where
P is the total number of parameters of the con-
ditional distributions. In contrast, the common
multivariate normal approach scales as P 2. How-
ever, this parsimonious parameterization implies
that the convex-update family cannot capture de-
pendencies that are not already present in the
prior of the probabilistic program. Speciﬁcally, the
convex-update family cannot model correlations
originating from colliding arrows in the directed
acyclic graph (‘explaining away’ dependencies).

While this paper is focused on diﬀerentiable proba-
bilistic programming, the pseudo-conjugate family can
be applied equally well to models with discrete latent
variables and combinatorial link functions. Consider a
conditional Bernoulli variable:
π) = ρ(π)b(1

(11)
where the array π collects the values of its parents.
Since the parameter ρ is deﬁned in a convex domain,
we can construct a convex-update variational posterior:

ρ(π))1−b ,

p(b

−

|

|

α

U

p(b

− U

π) =

λ [ρ(π)]b (1

λ [ρ(π)]))1−b .
α

(12)
It is easy to see that the same procedure can be applied
to binomial, categorical, multinomial and Poisson vari-
ables among others. Using discrete variables, we can
also construct convex-update families with stochastic
control ﬂows. For example, a binary latent b can de-
termine whether the stochastic computation proceeds
on one branch of the computation tree rather than
another. This has the nice feature of having an approx-
imate posterior program with the same stochastic ﬂow
structure of the prior program where the probability
of each gate is biased by the observed data.

5 Automatic structured variational

inference

Construction of the variational family (7) can be
straightforwardly automated by probabilistic program-
ming systems. A variational program is deﬁned by
running the input program under a nonstandard inter-
pretation: at each random variable site

xprior
j

(cid:16)

pj

xj

θj

(cid:16)

(πprior
j

(cid:17)(cid:17)

,

∼

|
the variational program instead samples from the
convex-update distribution (Eq. (6)),
(cid:17)
(cid:16)

(cid:17)

(cid:16)

xposterior
j

q

xj

θj

|

∼

πposterior

j

; λj, αj

,

Automatic structured variational inference

def build_asvi_surrogate_posterior (prior):

ConvexUpdateParams = namedtuple ([’lam ’, ’alpha ’])
q_vars = {}
@tfd. JointDistributionCoroutineAutoBatched
def asvi_surrogate_posterior ():

# Step the model to yield the first RV.
prior_gen = prior. _coroutine_fn ()
rv = next( prior_gen )
while True:

# Run model to termination .

# If this is a new RV , initialize variables .
if rv.name not in q_vars :

q_vars [rv.name] = {

k: ConvexUpdateParams (

lam= Variable ( random_init_like (v),

xform= Sigmoid ()),

alpha= Variable ( random_init_like (v),

xform=dist. bijectors [k]()))

for k, v in rv. parameters .items ()}
# Apply convex update for each parameter .
q_params = {}
for k, v in rv. parameters .items ():

lam = q_vars [rv.name ][k]. lam
alpha = q_vars [rv.name ][k]. alpha
q_params [k] = lam ∗ v + (1. - lam) ∗ alpha

# Step to the next RV.
q_sample = yield type(rv)(∗∗q_params )
rv = prior_gen .send( q_sample )

return asvi_surrogate_posterior

Figure 2: Python pseudocode for general-purpose auto-
mated construction of a structured variational family.
A full implementation is provided in TensorFlow Proba-
bility (tfp.experimental.build_asvi_surrogate_posterior).

where λj and αj are trainable variables. Because each
variable deﬁnition is rewritten ‘locally’, the variational
program has the same control-ﬂow structure, depen-
dence graph, and time complexity as the original model.
Local transformations of this kind may be implemented
by eﬀect handlers (Plotkin and Pretnar, 2009), a mech-
anism supported in multiple recent probabilistic pro-
gramming frameworks (Moore and Gorinova, 2018;
Bingham et al., 2019; Phan et al., 2019).

We provide an open-source implementation of ASVI
for joint distribution models in TensorFlow Probabil-
ity (TFP; Dillon et al., 2017), which uses an eﬀect-
handling-like mechanism (Python coroutines) to trans-
form an input joint distribution into a structured vari-
ational surrogate. Figure 2 shows a simpliﬁed version.

Our implementation operates in the default parame-
terization θ of each TFP distribution class, which is
typically similar (if not exactly equal) to the expecta-
tion parameterization; e.g., normal distributions expose
a location-scale parameterization. Constrained varia-
tional parameters (λj, αj) are deﬁned as diﬀerentiable
transformations of unconstrained variables, where each
distribution is annotated with appropriate constraining
transformations for its parameters. For example, the
softplus transformation f (x) = log(1+exp(x)) enforces
that scale parameters have positive values. For vari-

∼

ables such as log-normal distributions whose deﬁnition
involves a bijective transformation y = fψ(x) of a base
variable x
pφ(x), the convex-update family has an
analogous form and deﬁnes a convex update on the
union of the parameters θ = (φ, ψ).
Given a structured variational program, the ELBO
(1) may be estimated by Monte Carlo sampling and
optimized with stochastic gradient estimates, obtained
via the reparameterization trick for continuous-valued
variables and score-function estimator for discrete vari-
ables (Ranganath et al., 2014; Kucukelbir et al., 2017).

6 Related work

Most structured VI approaches require model speciﬁc
derivations and variational bounds. However, several
forms of model-agnostic structured variational distri-
butions have been introduced. The most commonly
used fully automatic approach is probably automatic
diﬀerentiation variational inference (ADVI) (Kucukel-
bir et al., 2017). The ADVI variational family is con-
structed by mapping the values of all latent variables to
an unbounded coordinate space based on the support
of each distribution. The variational distribution in this
new space is then parameterized as either a spherical
(mean ﬁeld) or a fully multivariate normal distribution.
While this approach is broadly applicable, it exploits
very little information from the original probabilistic
model and has scalability problems due to the cubic
complexity of Bayesian inference when the multivari-
ate distribution is used. Hierarchical VI accounts for
dependencies between latent variables by coupling the
parameters of their factorized distributions through a
joint variational prior (Ranganath et al., 2016). While
this method is very generic, it requires user input in or-
der to deﬁne the variational prior and it uses a modiﬁed
variational lower bound. Copula VI models the depen-
dencies between latent variables using a vine copula
function (Tran et al., 2015). In the context of prob-
abilistic programming, copula VI shares some of the
same limitations of hierarchical VI: it requires the ap-
propriate speciﬁcation of bivariate copulas and it needs
a specialized inference technique. The approach that
is closest to our current work is structured stochastic
VI (Hoﬀman et al., 2013). Similar to our approach, its
variational posteriors have the same conditional inde-
pendence structure as the input probabilistic program.
However, this method is limited to conditionally conju-
gate models with exponential family distributions. Fur-
thermore, the resulting ELBO is intractable and needs
to be estimated using specialized techniques. Similarly,
conjugate-Computation Variational Inference uses con-
jugate Bayesian updates in non-conjugate models with
locally conjugate components (Khan and Lin, 2017).
This diﬀers from our approach as we use the convex
updates for all the variables in the model.

Normalizing ﬂows are general deep learning methods

Ambrogioni, Lin, Fertig, Vikram, Hinne, Moore and van Gerven

designed to approximate arbitrary probability densi-
ties through a series of learnable invertible mappings
with tractable Jacobians (Rezende and Mohamed, 2015;
Kingma et al., 2016; Dinh et al., 2016; Kingma and
Dhariwal, 2018; Papamakarios et al., 2017; Kobyzev
et al., 2020). Flows have been used in stochastic VI as
very expressive and highly parameterized variational
families (Rezende and Mohamed, 2015; Kingma et al.,
2016). Most normalizing ﬂow architectures used in VI
do not incorporate the conditional independence struc-
ture of the prior probabilistic program. Structured
conditional continuous normalizing ﬂows are a new
class of normalizing ﬂows that have the conditional
independence structure of the true posterior (Weilbach
et al., 2020). These architectures are based on faithful
inversion (Webb et al., 2018) and implement the depen-
dency structure using sparse matrices. Conversely to
ASVI, this approach only exploits the graphical struc-
ture of the probabilistic program and therefore ignores
the speciﬁc form of the conditional distributions and
link functions.

Structured VI is most commonly applied in time series
models such as hidden Markov models and autoregres-
sive models. In these cases the posterior distributions
inherit strong statistical dependencies from the sequen-
tial nature of the prior. Structured VI for time series
usually use structured variational families that capture
the temporal dependencies, while being fully-factorized
in the non-temporal variables (Eddy, 1996; Foti et al.,
2014; Johnson and Willsky, 2014; Karl et al., 2016;
Fortunato et al., 2017). This diﬀers from our convex-
update families, where both temporal and non-temporal
dependencies are preserved.

7 Applications

We ﬁrst evaluate ASVI on a set of standardized
Bayesian inference tasks to compare ASVI to other
automated inference methods. We then apply ASVI to
a both a deep Bayesian smoothing and a deep genera-
tive modeling task, to demonstrate that ASVI scales to
large datasets while producing high quality posteriors.
The code needed to run ASVI and baselines on the
standardized tasks can be found in this repository.

7.1

Inference Gym tasks

Inference Gym (IG; Sountsov et al., 2020) is a Python
library for evaluating probabilistic inference algorithms.
It deﬁnes a set of standardized inference tasks and
provides implementations as TFP joint distribution
models (Piponi et al., 2020) and as Stan programs (Car-
penter et al., 2017), which are used to compute ‘ground
truth’ posterior statistics via MCMC.

For each task we evaluate ASVI against a suite of
baseline variational posteriors, including mean ﬁeld
(MF), inverse autoregressive ﬂows (Kingma et al., 2016;

Papamakarios et al., 2017) with eight hidden units
(Small IAF) and 512 hidden units (Large IAF), and
a multivariate normal posterior (MVN). Where possi-
ble, we also compare to an AR(1) posterior. For each
method the ELBO is optimized to convergence (up
to 100000 steps, although signiﬁcantly fewer were re-
quired in most cases) using the Adam optimizer, with
learning rate set by a hyperparameter sweep for each
task/method pair—this tuning was particularly impor-
tant for the IAF models. Further details are provided
in the supplement.

Time series. We consider two discretized stochastic
diﬀerential equation (SDE) models, deﬁned by condi-
tional densities of the following form:

xt+1
yt

∼ N

∼ N

(cid:0)xt + f (xt, t) dt, g(xt, t)2 dt(cid:1)
(cid:0)xt, σ2

(cid:1)

obs

(13)

(14)

where x represents the latent process with drift function
f and volatility function g, and observations y are taken
with noise standard deviation σobs. The ﬁrst model
describes Brownian (BR) motion without drift, and
the second model is a stochastic Lorenz dynamical
system (LZ). Full model speciﬁcations are provided in
the supplementary material. We also include variants
of both models that include global variables (BRG
and LZG) where the innovation and observation noise
scale parameters are unknown. We simulate each model
forward 30 steps and assume that noisy observations
are taken of the ﬁrst ten and last ten steps, and the
middle ten are unobserved.

Hierarchical regression models. We also evaluate
ASVI on two standard Bayesian hierarchical models:
Eight Schools (ES; Gelman et al., 2013), which models
the eﬀect of coaching programs on standardized test
scores, and Radon (R; Gelman and Hill, 2007), a
hierarchical linear regression model that predicts Radon
measurements taken in houses. Full descriptions are
provided in the supplementary material.

Results. Table 1 shows that ASVI and the large IAF
produce the best ELBOs; ASVI is competitive with
the large IAF on all tasks despite having far fewer
parameters. Similarly, in Table 2 we see that ASVI’s
estimates of posterior means and standard deviations
are consistently among the best relative to MCMC
ground truth. Mean ﬁeld fails to converge to the true
posterior estimate in all four models, indicating that
inclusion of prior structure in ASVI is needed to capture
the dependencies in the true posterior. Qualitatively,
we observe in Figure 3 that only the ASVI posterior
captures the ground truth posterior in the Lorenz task,
indicating that ASVI is able to capitalize on strong prior
structure when it exists. Figure 1 in the supplementary
material shows loss trajectories. For all but the Radon
model, ASVI tends to converge in fewer iterations than
the large IAF, the other highest-performing surrogate
posterior.

Automatic structured variational inference

network G:

(cid:0)G(x(t)), σ2(cid:1) .

yt ∼ N

(16)
In our examples, G is a generator which converts la-
tent vectors into RGB images. We tested two kinds
of pre-trained generators: A DCGAN trained on CI-
FAR10 and a DCGAN trained on FashionGEN (Rad-
ford et al., 2015) (see Supplementary Material C). In
the former case, the latent space is 100-dimensional,
while in the latter it is 120-dimensional. We consid-
ered two inference challenges: ﬁltering, where we aim
to remove the noise from a series of images generated
by a trajectory in the latent process, and bridging,
where we reconstruct a series of intermediate obser-
vations given the beginning (ﬁrst three time points)
and the end (last two time points) of a trajectory. In
both cases, we assume knowledge of the dynamical and
generative models, discretize the neural SDE using an
Euler–Maruyama scheme and backpropagate through
the integrator (Chen et al., 2018).

Figure 5 shows the ﬁltering performance of ASVI and
two baselines (mean ﬁeld, and linear Gaussian model;
see Supplementary Material C for the details) in a
bridging problem. The quantitative results (negative
ELBOs) are shown in Figure 4. ASVI always reaches
tighter lower bounds except in the Fashion bridge exper-
iment where it has slightly lower performance than the
linear coupling baseline. This tighter variational bound
results in discernibly higher quality ﬁltered images in
both CIFAR10 and Fashion, as shown in Figure 5. The
ﬁgure also shows several samples from the ASVI bridge
posterior. As expected, the generated images diverge
in the unobserved period and reconverge at the end.

7.3 Deep amortized generative modeling

Finally, we apply an amortized form of ASVI to a deep
generative modeling problem. The goal is to model the
joint distribution of a set of binary images y paired with
class labels l. We use a deep variational autoencoder
with three layers of latent variables z1, z2 and z3
coupled in a feed-forward fashion through ReLU fully-
connected networks:
z1
∼ N
z2
∼ N
z3
∼ N
y
∼ N
l
∼
and f 2

(0, 1.5)
(ReLU (f 1(z1)) , 0.25)
(ReLU (f 2(z2)) , 0.1)
(g1(z3), 0.1)

where f 1
are fully-connected two-layer networks
with ReLU activations, linear output units, and 25 and
75 hidden units respectively, while g1
are linear
layers. The amortized convex-update distribution has
the following form:

Categorical(Softmax(g2(z2)))

and g2

(17)

z1
z2
z3

∼ N
∼ N
∼ N

(α1(y), ξ1(y))
(λ2f 1(z1) + (1
(λ3f 2(z1) + (1

−
−

λ2)α2(y), ξ2(y))
λ3)α3(y), ξ3(y))

(18)

Figure 3: Qualitative results of the Lorenz system with
globals (LZG) experiment, showing each baseline’s es-
timate of the three latent dimensions in the Lorenz
model
two standard deviations. Only ASVI is able
to successfully infer the values of each latent series.

±

Figure 4: Quantitative results of the deep Bayesian
smoothing experiment in subsection 7.2. Shown is the
negative ELBO of ASVI, mean-ﬁeld, and linear cou-
pling at the ﬁnal training iteration.

7.2 Deep Bayesian smoothing with neural

SDEs

So far, we performed inference in simple time series
and hierarchical models with relatively low-dimensional
state spaces. We will now test the performance of ASVI
on a high-dimensional problem with complex nonlin-
earities parameterized by deep networks. As a latent
model, we use neural (discretized) stochastic diﬀerential
equations (SDE) (Chen et al., 2018; Li et al., 2020):

dx = F (x)dt + dB(t)
where F (x) is a nonlinear function parameterized by
a neural network and B(t) is standard multivariate
Brownian motion (see Supplementary Material C for
the details of the architecture). This latent process
generates noise-corrupted observations through a deep

(15)

6.356.406.456.506.55CIFAR×104Filter1.001.051.101.151.201.25×104BridgeASVIMean-ﬁeldLinearcoupling2.59502.59752.60002.60252.60502.6075Fashion×105ASVIMean-ﬁeldLinearcoupling4.6004.6254.6504.6754.7004.7254.7504.775×104Ambrogioni, Lin, Fertig, Vikram, Hinne, Moore and van Gerven

Table 1: Average ﬁnal negative ELBO values and their standard errors, computed over 15 simulations. Each
row corresponds to an Inference Gym task and each column is a posterior baseline. Boldface indicates the best
performance.

ASVI

Mean ﬁeld

Small IAF

Large IAF

MVN

AR(1)

BR
BRG
LZ
LZG
ES
R

−5.20 ± 0.11
−0.35 ± 0.10
34.29 ± 0.66
34.68 ± 0.15
36.50 ± 0.04
1079.8 ± 0.22

0.99 ± 0.49
−5.16 ± 0.31
1225.66 ± 4.30
119.67 ± 0.17
36.94 ± 0.04
1082.77 ± 0.59

−3.47 ± 0.24
−1.31 ± 0.23
1236.49 ± 12.47
112.64 ± 0.58
36.28 ± 0.04
1078.85 ± 0.21

−4.9 ± 0.12
0.14 ± 0.09
1242.11 ± 10.39
94.8 ± 0.14
36.26 ± 0.02
1078.52 ± 0.19

−3.7 ± 0.2
−2.27 ± 0.20
2153.08 ± 179.24
98.43 ± 0.09
36.58 ± 0.02
1090.96 ± 0.48

−4.01 ± 0.31
−2.08 ± 0.31
1271.36 ± 16.53
54.53 ± 2.1
N/A
N/A

Table 2: Mean and standard error of the absolute error in posterior mean (M) and standard deviation (SD)
parameters relative to MCMC ground truth, normalized by true SD, computed over 15 simulations. Boldface
indicates the best performance.

ASVI

Mean ﬁeld

Small IAF

Large IAF

MVN

AR(1)

BR

M
0.16 ± 0.03
SD 0.06 ± 0.02

BRG M 0.69 ± 0.07
SD 0.22 ± 0.02

0.14 ± 0.02
0.37 ± 0.01

0.81 ± 0.03
0.31 ± 0.01

0.1 ± 0.02
0.14 ± 0.03

0.77 ± 0.03
0.27 ± 0.02

0.12 ± 0.03
0.07 ± 0.02

0.71 ± 0.05
0.23 ± 0.02

0.09 ± 0.01
0.07 ± 0.01

0.74 ± 0.03
0.23 ± 0.02

0.24 ± 0.05
0.07 ± 0.01

0.80 ± 0.07
0.24 ± 0.02

LZ

M 0.36 ± 0.25
SD 0.47 ± 0.03

35.83 ± 0.00
0.94 ± 0.00

35.94 ± 0.11
0.92 ± 0.01

36.62 ± 0.07
0.81 ± 0.01

39.66 ± 0.00
0.67 ± 0.01

36.52 ± 0.18
0.83 ± 0.01

LZG M 0.15 ± 0.03
SD 0.39 ± 0.02

21.46 ± 0.04
1.51 ± 0.04

23.35 ± 0.16
1.51 ± 0.20

23.96 ± 0.06
0.80 ± 0.14

23.86 ± 0.08
2.59 ± 0.06

1.56 ± 0.24
2.59 ± 0.08

ES

R

M
SD

0.16 ± 0.03
0.07 ± 0.02

0.16 ± 0.02
0.05 ± 0.01

0.16 ± 0.04
0.07 ± 0.02

0.13 ± 0.04
0.08 ± 0.03

0.13 ± 0.04
0.06 ± 0.01

M 0.12 ± 0.02
0.15 ± 0.03
SD

0.24 ± 0.02
0.12 ± 0.01

0.17 ± 0.02
0.09 ± 0.01

0.15 ± 0.02
0.10 ± 0.02

0.42 ± 0.01
0.50 ± 0.02

N/A
N/A

N/A
N/A

Figure 5: Qualitative results of the deep Bayesian ﬁlters using ASVI, mean-ﬁeld, and linear coupling for CIFAR10
(left) and FashionGEN (right). The top row shows the ground truth images, the second row the noisy-corrupted
observations, and the remaining rows show the diﬀerent reconstructions.

12345678910tGroundtruth12345678910Obs.ASVIMeanﬁeldLineartAutomatic structured variational inference

Figure 6: The latent embeddings for the diﬀerent MNIST variants. Each element is a predicted sample together
with its corresponding label.

Table 3: Average and standard error of the RMSE between the posterior means and the ground truth functions,
computed over 15 simulations. Boldface indicates the best performance.

MNIST

kMNIST

fMNIST

no labels

labels

no labels

labels

no labels

labels

ASVI −134.6 ± 0.1 −99.7 ± 0.5 −252.4 ± 0.3 −200.3 ± 0.7 −151.1 ± 0.4 −116.4 ± 1.2
−158.2 ± 1.3
MF

−153.8 ± 0.2 −146.3 ± 0.5

−166.7 ± 0.2

−258.6 ± 0.8

−267.0 ± 0.3

−

where the mean vectors αk(y) are the activation
of the (8
2k)-th layer (post ReLU) of a fully-
connected 6-layer ReLU inference network (with layer
sizes (120, 100, 70, 50, 70, 2)) that takes the image y as
input. The scale parameter vectors ξk(y) were obtained
by connecting a linear layer to the (8
2k)-th layer,
followed by a softplus transformation. The details of all
architectures are given in Supplementary Material D.
The amortized family was parameterized by λ and the
weights and biases of the inference network. Note that
this amortization scheme is not fully automatic as it
require the choice of the inference networks. The mean-
ﬁeld baseline had the same form given in Eq. (18),
but with the expectation of the distribution fully deter-
mined by αk(y). We did not include a comparison with
the other baselines as it is computationally unfeasible
in this larger scale experiment.

−

We tested the performance of these deep variational
generative models on three computer vision datasets:
MNIST, FashionMNIST and KMNIST (LeCun et al.,
1998; Xiao et al., 2017; Clanuwat et al., 2018). Further-
more, we performed two types of experiment. In the
ﬁrst, images and labels were generated jointly. In the
second, only images were generated. Table 3 reports
the performance of ASVI and mean-ﬁeld baseline quan-
tiﬁed as the test set ELBO. The ELBO was estimated
by sampling the latent variables 20 times per test image
from the variational posterior. Mean ELBO and SEM
were obtained by repeating the estimation 20 times.

ASVI achieves higher performance in all experiments
for all datasets. Figure 6 shows a randomized selection
of images generated by the ASVI model together with
the corresponding label.

8 Discussion

We introduced an automatic algorithm for constructing
an appropriately structured variational family, given
an input probabilistic program. The resulting method
can be used on any probabilistic program speciﬁed by
a directed Bayesian network, and always preserves the
forward-pass structure of the input program. The main
limitation of the convex-update family is that it cannot
capture dependencies induced by colliding arrows in
the input graphical model. Consequently, in a model
such as a standard Bayesian neural network, where the
prior over the weights is decoupled, the convex-update
family is a mean-ﬁeld family. Despite this limitation,
our results demonstrate good performance (competitive
with a high-capacity IAF) on small Bayesian hierarchi-
cal models with colliding arrows in the prior graphical
model, through capturing the forward structure alone.
Designing structured surrogates that eﬃciently capture
the full posterior dependence structure is an interesting
direction for ongoing work.

First latent dimensionSecond latent dimension0123456789MNISTFirst latent dimensionChar. 0Char. 1Char. 2Char. 3Char. 4Char. 5Char. 6Char. 7Char. 8Char. 9KMNISTFirst latent dimensionT-shirt/topTrouserPulloverDressCoatSandalShirtSneakerBagAnkle bootfMNISTAmbrogioni, Lin, Fertig, Vikram, Hinne, Moore and van Gerven

References

D. Wingate and T. Weber. Automated variational in-
ference in probabilistic programming. ArXiv preprint
arXiv:1301.1299, 2013.

A. Kucukelbir, D. Tran, R. Ranganath, A. Gelman, and
D. M. Blei. Automatic diﬀerentiation variational in-
ference. The Journal of Machine Learning Research,
18(1):430–474, 2017.

D. Tran, A. Kucukelbir, A. B Dieng, M. Rudolph,
D. Liang, and D. M. Blei. Edward: A library for prob-
abilistic modeling, inference, and criticism. ArXiv
preprint arXiv:1610.09787, 2016.

A. Kucukelbir, R. Ranganath, A. Gelman, and D. Blei.
Automatic variational inference in Stan. In Advances
in Neural Information Processing Systems, 2015.
E. Bingham, J. P. Chen, M. Jankowiak, F. Obermeyer,
N. Pradhan, T. Karaletsos, R. Singh, P. Szerlip,
P. Horsfall, and N. D. Goodman. Pyro: Deep uni-
versal probabilistic programming. The Journal of
Machine Learning Research, 20(1):973–978, 2019.
M. D. Hoﬀman, D. M. Blei, C. Wang, and J. Pais-
ley. Stochastic variational inference. The Journal of
Machine Learning Research, 14(1):1303–1347, 2013.
D. P. Kingma and M. Welling. Auto-encoding varia-
tional Bayes. ArXiv preprint arXiv:1312.6114, 2013.
J. M. Hernández-Lobato and R. Adams. Probabilistic
backpropagation for scalable learning of Bayesian
neural networks.
In International Conference on
Machine Learning, pages 1861–1869, 2015.

R. Ranganath, S. Gerrish, and D. Blei. Black box
variational inference. In International Conference on
Artiﬁcial Intelligence and Statistics, 2014.

M. Hoﬀman and D. Blei. Stochastic structured vari-
ational inference. In International Conference on
Artiﬁcial Intelligence and Statistics, 2015.

D. Tran, D. Blei, and E. M. Airoldi. Copula varia-
tional inference. In Advances in Neural Information
Processing Systems, 2015.

R. Ranganath, D. Tran, and D. Blei. Hierarchical
variational models. In International Conference on
Machine Learning, 2016.

P. Diaconis and D. Ylvisaker. Conjugate priors for
exponential families. The Annals of Statistics, 7(2):
269–281, 1979.

G. Plotkin and M. Pretnar. Handlers of algebraic
eﬀects. In European Symposium on Programming.
Springer, 2009.

D. Moore and M. I. Gorinova. Eﬀect handling for
composable program transformations in Edward2.
arXiv preprint arXiv:1811.06150, 2018.

D. Phan, N. Pradhan, and M. Jankowiak. Com-
posable eﬀects for ﬂexible and accelerated proba-
bilistic programming in NumPyro. ArXiv preprint
arXiv:1912.11554, 2019.

J. V Dillon, I. Langmore, D. Tran, E. Brevdo, S. Va-
sudevan, D. Moore, B. Patton, A. Alemi, M. Hoﬀman,
and R. A. Saurous. Tensorﬂow distributions. ArXiv
preprint arXiv:1711.10604, 2017.

M. E. Khan and W. Lin. Conjugate-computation vari-
ational inference: Converting variational inference
in non-conjugate models to inferences in conjugate
models. In International Conference on Artiﬁcial
Intelligence and Statistics, 2017.

D. J. Rezende and S. Mohamed. Variational inference
with normalizing ﬂows. In International Conference
on Machine Learning, 2015.

D. P Kingma, T. Salimans, R. Jozefowicz, X. Chen,
I. Sutskever, and M. Welling. Improved variational
inference with inverse autoregressive ﬂow. In Ad-
vances in Neural Information Processing Systems,
2016.

L. Dinh, J. Sohl-Dickstein, and S. Bengio. Den-
arXiv preprint

sity estimation using real NVP.
arXiv:1605.08803, 2016.

D. P. Kingma and P. Dhariwal. Glow: Generative ﬂow
In Advances in

with invertible 1x1 convolutions.
Neural Information Processing Systems, 2018.

G. Papamakarios, T. Pavlakou, and I. Murray. Masked
autoregressive ﬂow for density estimation. In Ad-
vances in Neural Information Processing Systems,
2017.

I. Kobyzev, S. Prince, and M. Brubaker. Normalizing
ﬂows: An introduction and review of current methods.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 2020.

C. Weilbach, B. Beronov, F. Wood, and W. Harvey.
Structured conditional continuous normalizing ﬂows
for eﬃcient amortized inference in graphical models.
In International Conference on Artiﬁcial Intelligence
and Statistics, 2020.

S. Webb, A. Golinski, R. Zinkov, N. Siddharth, T. Rain-
forth, Y. W. Teh, and F. Wood. Faithful inversion
of generative models for eﬀective amortized infer-
ence. In Advances in Neural Information Processing
Systems, 2018.

S. R. Eddy. Hidden Markov models. Current Opinion

in Structural Biology, 6(3):361–365, 1996.

N. Foti, J. Xu, D. Laird, and E. Fox. Stochastic varia-
tional inference for hidden Markov models. In Ad-
vances in Neural Information Processing Systems,
2014.

M. Johnson and A. Willsky. Stochastic variational
inference for Bayesian time series models. In Inter-
national Conference on Machine Learning, 2014.
M. Karl, M. Soelch, Ju. Bayer, and P. van der Smagt.
Deep variational Bayes ﬁlters: Unsupervised learning
of state space models from raw data. arXiv preprint
arXiv:1605.06432, 2016.

Automatic structured variational inference

M. Fortunato, C. Blundell, and O. Vinyals. Bayesian
arXiv preprint

recurrent neural networks.
arXiv:1704.02798, 2017.

P. Sountsov, A. Radul, and contributors.

Inference

gym, 2020.

D. Piponi, D. Moore, and J. V. Dillon. Joint distri-
butions for TensorFlow Probability. ArXiv preprint
arXiv:2001.11819, 2020.

B. Carpenter, A. Gelman, M. D. Hoﬀman, D. Lee,
B. Goodrich, M. Betancourt, M. Brubaker, J. Guo,
P. Li, and A. Riddell. Stan: A probabilistic program-
ming language. Journal of Statistical Software, 76
(1), 2017.

A. Gelman, J. B. Carlin, H. S. Stern, D. B. Dunson,
A. Vehtari, and D. B. Rubin. Bayesian Data Analysis.
CRC Press, 3rd edition, 2013.

A. Gelman and J. Hill. Data Analysis using Regres-
sion and Multilevel/Hierarchical Models. Cambridge
University Press, 2007.

T. Q. Chen, Y. Rubanova, J. Bettencourt, and D. K.
Duvenaud. Neural ordinary diﬀerential equations. In
Advances in Neural Information Processing Systems,
2018.

X. Li, T. L. Wong, R. T.Q. Chen, and D. Duvenaud.
Scalable gradients for stochastic diﬀerential equa-
tions. arXiv preprint arXiv:2001.01328, 2020.

A. Radford, L. Metz, and S. Chintala. Unsuper-
vised representation learning with deep convolutional
generative adversarial networks. ArXiv preprint
arXiv:1511.06434, 2015.

Y. LeCun, L. Bottou, Y. Bengio, and P. Haﬀner.
Gradient-based learning applied to document recog-
nition. Proceedings of the IEEE, 86(11):2278–2324,
1998.

H. Xiao, K. Rasul, and R. Vollgraf. Fashion-MNIST: a
novel image dataset for benchmarking machine learn-
ing algorithms. arXiv preprint arXiv:1708.07747,
2017.

T. Clanuwat, M. Bober-Irizar, A. Kitamoto, A. Lamb,
K. Yamamoto, and D. Ha. Deep learning for
classical Japanese literature.
ArXiv preprint
arXiv:1812.01718, 2018.

Ambrogioni, Lin, Fertig, Vikram, Hinne, Moore and van Gerven

A Convex updates in conjugate models

Consider an exponential family likelihood and its expectation parameter:

where η is the natural parameter. The conjugate prior is

(cid:90)

µ(η) =

eη·T (x)−A(η)T (x)dx =

ηA(η) ,

∇

(19)

(20)
where h(
) is a base density, τ0 is the relevant prior natural parameter and n0 is the natural "prior count" parameter.
·
This also induces a prior p(µ
τ0, n0) over the expectation parameters µ. In Section 2.1, we parameterize this
prior p(µ

τ0, n0) using the prior expectations of the expectation parameters

τ0, n0) = h(τ, n0)eτ0·η−n0A(η) .

p(η

|

|

|

(cid:90)

¯µ0 =

µp(µ

|

τ0, n0)dµ =

(cid:90)

ηA(η)p(η

∇

|

τ0, n0)dη

(21)

The result in Eq. 3 follows from the fact that ¯µ0 = τ0/n0. To see this, we can integrate both sides of the trivial
identity:

(22)
and notice that (cid:82)
τ0, n0) vanishes at the boundary
values of the integral using the generalized divergence theorem (Diaconis, 1979). Using this result, we can use the
conjugate update rules τ = τ0 + (cid:80)

τ0, n0)dη = 0. This result can be proven when p(η

τ0, n0) = p(η

τ0, n0)(τ0

ηA(η))

ηp(η

ηp(η

n0

∇

∇

∇

−

|

|

|

|

τ0 + (cid:80)N

n T (xn) and n = n0 + N to obtain
n T (xn)
τ
n
(cid:18) n0

=

(cid:18)

n0
n0 + N

(cid:19) 1
N

1

−

n0 + N
(cid:19) τ0
n0

+

n0 + N

¯µ =

=

N
(cid:88)

n

T (xn)

(23)

(24)

(25)

λ)µM L
where N is the number of datapoints and µM L = 1
N
estimator of the expectation parameter.

= λ¯µ0 + (1

−

(cid:80)N

n T (xn) can be shown to be the maximum likelihood

B Details of the Inference Gym experiments

B.1

Inference Gym tasks

Time series models

As a ﬁrst application, we focus on timeseries models and SDEs. We used two models, both from the Inference
Gym (Sountsov et al., 2020).

The ﬁrst model (BR) is a Brownian motion without drift, governed by x(cid:48)(t) = wx(t), where wx(t) is a Gaussian
white noise process with scale σx. The value of xt is observed with noise standard deviation equal to σobs. The
data are generated with σx = 0.1 and σobs = 0.15. In the BRG model with global variables, both σx and σobs are
treated as random variables with LogNormal(loc=0, scale=2) priors.

The second model (LZ) is a stochastic Lorenz system (nonlinear SDE):

x(cid:48)(t) = 10(y(t)
y(cid:48)(t) = x(t)(28
z(cid:48)(t) = x(t)y(t)

−

−

x(t)) + wx(t)
z(t))
(8/3)z(t) + wz(t)

y(t) + wy(t)

−

(26a)

(26b)

(26c)
where wx(t), wy(t) and wz(t) are Gaussian white noise processes with standard deviation σ = 0.1. The value
of x(t) is observed with Gaussian noise with standard deviation σobs = 1.; y(t) and z(cid:48)(t are left unobserved.
When global variables are allowed (the LZG model), σ and σobs are treated as unknown random variables with
LogNormal(loc=-1., scale=1.) priors.

−

All processes were discretized with the Euler–Maruyama method ( dt = 0.01 for BR and dt = 0.02 for LZ) and
the transition probability was approximated as Gaussian (this approximation is exact for dt tending to 0). Each
model was integrated for 30 steps.

Automatic structured variational inference

Hierarchical models

Eight Schools (Gelman et al., 2013) models the eﬀect of coaching programs on standardized test scores, and is
speciﬁed as follows:

where i = 1, . . . , 8 indexes the schools, µ represents the prior average treatment eﬀect and τ controls the variance
between schools. The yi and σi are observed.
The Radon model (Gelman and Hill, 2007) is a Bayesian hierarchical linear regression model that predicts
measurements of Radon, a carcinogenic gas, taken in houses in the United States. The hierarchical structure is
reﬂected in the grouping of houses by county, and the model is speciﬁed as follows:

µ
log τ

θi
yi

(0, 100)
(5, 1)

N
(µ, τ 2)
(θi, σ2
i )

∼ N
log
∼

∼ N

∼ N

µ

τ

θi
β1, β2, β3
σ

yj

∼ N

∼ N

∼ N
∼ N

∼ N

∼ N

(0, 1)
+(0, 1)
(µ, τ 2)
(0, 1)
+(0, 1)
(β1zcj + β2xj + β3 ¯xcj + θcj , σ2)

(27)
(28)

(29)

(30)

(31)

(32)

(33)
(34)

(35)

(36)

where θi is the eﬀect for county i (with prior mean µ and standard deviation τ ) and the β are regression coeﬃcients.
The log Radon measurement in house j, yj, depends on the eﬀect θcj
for the county to which the house belongs,
(the log uranium measurement in county cj), xj (the ﬂoor of the house on which the
as well as features zcj
measurement was taken), and ¯xcj
+(0, 1) indicates a Normal
distribution with mean 0 and variance 1, truncated to nonnegative values.

(the mean ﬂoor by county, a contextual eﬀect).

N

B.2 Baselines

Mean Field ADVI

The ADVI (MF) surrogate posterior is constructed with the same procedure as the ASVI posterior, but using
only the α parameters, or equivalently, ﬁxing λ = 0. As with ASVI, therefore, the surrogate posterior for each
variable is in the same distribution family as its prior. This diﬀers slightly from Kucukelbir et al. (2017), in
which surrogate posteriors are always bijectively transformed normal distributions, although we have no reason to
believe that this diﬀerence is material to our experiments.

Inverse Autoregressive Flows

Inverse Autoregressive Flows (IAFs) are normalizing ﬂows which autoregressively transform a base distribution
(Kingma et al., 2016) with a masked neural network (Papamakarios et al., 2017). We build an IAF posterior
by transforming a standard Normal distribution with two sequential two-layer IAFs built with tfp.bijectors
.MaskedAutoregressiveFlow. The output of the ﬂow is split and restructured to mirror the support of the prior
distribution, and then constrained to the support of the prior (for example, by applying a sigmoid transformation
to constrain values between zero and one, or a softplus to constrain values to be positive). In our experiments, we
use two diﬀerent-sized IAF posteriors: the “Large” IAF has 512 hidden units in each layer and the “Small” IAF
has 8 hidden units in each layer.

Multivariate Normal

The MVN surrogate posterior is built by deﬁning a full-covariance Multivariate Normal distribution with trainable
mean and covariance, restructuring the support to the support of the prior, and constraining the samples to the
prior support if necessary.

Ambrogioni, Lin, Fertig, Vikram, Hinne, Moore and van Gerven

(a) Brownian motion (BR)

(b) Brownian motion with globals (BRG)

(c) Lorenz system (LZ)

(d) Lorenz system with globals (LZG)

(e) Eight Schools (E)

(f) Radon (R)

Figure 7: Training losses (negative ELBO values) for the ASVI, MF, Large IAF, and Small IAF baselines on the
Inference Gym tasks. Each posterior was trained with the Adam optimizer for 100000 steps.

AR(1)

The autoregressive model surrogate learns a linear Gaussian conditional between each pair of successive model
variables:

xt+1

∼ N

(Atxt + bt, Dt)

where each At and bt parameterize a learned linear transformation, and Dt is a learned diagonal variance matrix.
The linear Gaussian autoregression operates on unconstrained values, which may then be then pushed through
constraining transformations as required by the model. To stabilize the optimization, we omit direct dependence
on global variables, i.e., when xt is a global variable we ﬁx At = 0 (these are generally the ﬁrst few variables
sampled in each model).

B.3 Training details

For each of the following inference tasks and posterior baselines, we ﬁt a posterior using full-batch gradient
descent on a 1-sample Monte Carlo estimate of the ELBO. We use the Adam optimizer with a learning rates
selected by hyperparameter sweep: 1e-2 learning rate for ASVI, MF, and AR(1), 1e-3 for the MVN and Small
IAF, and 5e-5 for the Large IAF. Each posterior was trained for 100000 iterations; in Figure 7 and Figure 8, we
report the training curves for each posterior-task pair. We ﬁnd that ASVI successfully converges in all tasks; in
most cases, ASVI converges well before 100000 iterations, while MF, Large IAF, and Small IAF fail to converge
to a good solution in a few of the tasks.

C Details of the neural SDE experiment

C.1 Models

The function F (x) had the following form

(37)
where W2 and W1 were d
d matrices whose entries were sampled in each of the 5 repetitions from a centered
normal with SD equal to 0.2. Those matrices encodes the forward dynamical model and they were assumed to

F (x) = W2 tanh (x + tanh(W1x))

×

Automatic structured variational inference

(a) Brownian motion (BR)

(b) Brownian motion with globals (BRG)

(c) Lorenz system (LZ)

(d) Lorenz system with globals (LZG)

(e) Eight Schools (E)

(f) Radon (R)

Figure 8: Training losses (negative ELBO values) for the MVN and AR(1) baselines on the Inference Gym tasks.
Each posterior was trained with the Adam optimizer for 100000 steps.

be known during the experiment. This is a Kalman ﬁlter-like setting where the form of the forward model is
known and the inference is performed in the latent units. The neural SDE was integrated using Euler–Maruyama
integration with step size equal to 1 from t = 0 to t = 9. We trained the model by back-propagating though the
integrator.

We used two DCGAN generators as emission models. The networks were the DCGAN implemented in PyTorch.
In the CIFAR experiment, we used the following architecture:

ConvTranspose2d ( 1 0 0 , 6 4 ∗ 8 , 4 , 1 , 0 ,

b i a s=F a l s e ) ,

BatchNorm2d ( 6 4 ∗ 8 ) ,
ReLU( True )
ConvTranspose2d ( 6 4 ∗ 8 , 6 4 ∗ 4 , 4 , 2 , 1 ,

b i a s=F a l s e ) ,

BatchNorm2d ( n g f ∗ 4 ) ,
ReLU( True ) ,
ConvTranspose2d ( 6 4 ∗ 4 , 6 4 ∗ 2 , 4 , 2 , 1 ,

b i a s=F a l s e ) ,

BatchNorm2d ( 6 4 ∗ 2 ) ,
ReLU( True ) ,
ConvTranspose2d ( 6 4 ∗ 2 , 6 4 , 4 , 2 , 1 ,

b i a s=F a l s e ) ,

BatchNorm2d ( n g f ) ,
ReLU( True ) ,
ConvTranspose2 ( 6 4 , 4 , k e r n e l _ s i z e =1,

s t r i d e =1,
padding =0,
b i a s=F a l s e ) ,

Tanh ( )

The network pretrained on CIFAR was obtained from the GitHub repository: csinva/gan-pretrained-pytorch. The
FashionGEN network was downloaded from the pytorch GAN zoo repository. The architectural details are given
in Radford et al. (2015).

Ambrogioni, Lin, Fertig, Vikram, Hinne, Moore and van Gerven

C.2 Baselines

The ADVI (MF) baseline was obtained by replacing all the conditional Gaussian distributions in the probabilistic
program with Gaussian distributions with uncoupled trainable mean and standard deviation parameters. ADVI
(MN) was not computationally feasible in this larger scale experiment. Therefore, we implemented a a linear
Gaussian model whith conditional densities:

xt−1)
where the matrix W , and the vectors αt and σ2
t

q(xt

|

(cid:1) ,
(cid:0)W xt−1 + αt, σ2
∼ N
are learnable parameters.

t

(38)

D Details of the autoencoder experiment

We used the following architectures in our deep autoencoder experiments.

Decoder 1 (f 1(z1))
h i d d e n _ s i z e =25
L i n e a r ( l a t e n t _ s i z e 1 , h i d d e n _ s i z e )
ReLU ( )
L i n e a r ( h i d d e n _ s i z e ,

l a t e n t _ s i z e 2 )

Decoder 2 (f 2(z2))
h i d d e n _ s i z e = 75
L i n e a r ( l a t e n t _ s i z e 2 , h i d d e n _ s i z e )
ReLU ( )
L i n e a r ( h i d d e n _ s i z e ,

l a t e n t _ s i z e 3 )

Decoder 3 (g1(z3))
L i n e a r ( l a t e n t _ s i z e 3 ,

i ma ge _s iz e )

Decoder 4 (α(y))
L i n e a r ( l a t e n t _ s i z e 3 ,

i ma ge _s i z e )

l a t e n t _ s i z e 3 )

l o g sd output

Inference network (f 1(z1))
h i d d e n _ s i z e 1 =120
L i n e a r ( image_size , h i d d e n _ s i z e )
ReLU ( ) # For hidden u n i t s
# Latent mean output
# Latent
L i n e a r ( h i d d e n _ s i z e 1 ,
S o f t p l u s ( ) # For s t a n d a r d d e v i a t i o n output
h i d d e n _ s i z e 2 =70
L i n e a r ( l a t e n t _ s i z e 3 , h i d d e n _ s i z e 2 )
ReLU ( ) # For hidden u n i t s
# Latent mean output
L i n e a r ( h i d d e n _ s i z e 2 ,
# Latent
L i n e a r ( h i d d e n _ s i z e 2 ,
S o f t p l u s ( ) # For s t a n d a r d d e v i a t i o n
h i d d e n _ s i z e 3 =70
L i n e a r ( l a t e n t _ s i z e 2 , h i d d e n _ s i z e 3 )
ReLU ( ) # For hidden u n i t s
# La tent mean output
L i n e a r ( h i d d e n _ s i z e 3 ,
# La tent
L i n e a r ( h i d d e n _ s i z e 3 ,
S o f t p l u s ( ) # For s t a n d a r d d e v i a t i o n

l o g sd output

l o g sd output

l a t e n t _ s i z e 2 )

l a t e n t _ s i z e 2 )

l a t e n t _ s i z e 1 )

l a t e n t _ s i z e 1 )

