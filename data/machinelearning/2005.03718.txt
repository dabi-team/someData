A Gradient-Aware Search Algorithm for
Constrained Markov Decision Processes

Sami Khairy, Prasanna Balaprakash, Lin X. Cai

1

0
2
0
2

y
a
M
7

]

G
L
.
s
c
[

1
v
8
1
7
3
0
.
5
0
0
2
:
v
i
X
r
a

Abstractâ€”The canonical solution methodology for ï¬nite con-
strained Markov decision processes (CMDPs), where the objective
is to maximize the expected inï¬nite-horizon discounted rewards
subject to the expected inï¬nite-horizon discounted costs con-
straints, is based on convex linear programming. In this brief,
we ï¬rst prove that the optimization objective in the dual linear
program of a ï¬nite CMDP is a piece-wise linear convex function
(PWLC) with respect to the Lagrange penalty multipliers. Next,
we propose a novel two-level Gradient-Aware Search (GAS)
algorithm which exploits the PWLC structure to ï¬nd the optimal
state-value function and Lagrange penalty multipliers of a ï¬nite
CMDP. The proposed algorithm is applied in two stochastic con-
trol problems with constraints: robot navigation in a grid world
and solar-powered unmanned aerial vehicle (UAV)-based wireless
network management. We empirically compare the convergence
performance of the proposed GAS algorithm with binary search
(BS), Lagrangian primal-dual optimization (PDO), and Linear
Programming (LP). Compared with benchmark algorithms, it is
shown that the proposed GAS algorithm converges to the optimal
solution faster, does not require hyper-parameter tuning, and is
not sensitive to initialization of the Lagrange penalty multiplier.

Index Termsâ€”Constrained Markov Decision Process, Gradient
Aware Search, Lagrangian Primal-Dual Optimization, Piecewise
Linear Convex, Wireless Network Management

I. INTRODUCTION

M ARKOV decision processes (MDPs) are classical formal-

ization of sequential decision making in discrete-time
stochastic control processes [1]. In MDPs, the outcomes of
actions are uncertain, and inï¬‚uence not only immediate rewards,
but also future rewards through next states. Policies, which
are strategies for action selection, should therefore strike a
trade-off between immediate rewards and delayed rewards, and
be optimal in some sense. MDPs have gained recognition
in diverse ï¬elds such as operations research, economics,
engineering, wireless networks, artiï¬cial
intelligence, and
learning systems [2]. Moreover, an MDP is a mathematically
idealized form of the reinforcement learning problem, which is
an active research ï¬eld within the machine learning community.
In many situations however, ï¬nding the optimal policies with
respect to a single reward function does not sufï¬ce to fully
describe sequential decision making in problems with multiple
conï¬‚icting objectives [3], [4]. The framework of constrained
MDPs (CMDPs) is the natural approach for handling multi-
objective decision making under uncertainty [5].

Sami Khairy and Lin X. Cai are with the Department of Electrical and
Computer Engineering, Illinois Institute of Technology, Chicago, IL 60616,
USA. E-mail: skhairy@hawk.iit.edu, lincai@iit.edu.

Prasanna Balaprakash is afï¬liated with the Mathematics and Computer
Science Division and Leadership Computing Facility, Argonne National
Laboratory, Lemont, IL 60439, USA. E-mail: pbalapra@anl.gov.

Algorithmic methods for solving CMDPs have been ex-
tensively studied when the underlying transition probability
function is known [5]â€“[11], and unknown [12]â€“[26]. In the case
of ï¬nite state-action spaces with known transition probability
function, the solution for a CMDP can be obtained by solving
ï¬nite linear programs [5]â€“[7], or by deriving a Bellman
optimality equation with respect to an augmented MDP whose
state vector consists of two parts: the ï¬rst part is the state
of the original MDP, while the second part keeps track of
the cumulative constraints cost [8]â€“[11]. Linear programming
methods become computationally impractical at a much smaller
number of states than dynamic programming, by an estimated
factor of about 100 [1]. In practice, MDP-speciï¬c algorithms,
which are dynamic programming based methods, hold more
promise for efï¬cient solution [27]. While MDP augmentation
based methods provide a theoretical framework to apply
dynamic programming to constrained stochastic control, they
introduce continuous variables to the state space which rules
out practical tabular methods and make the design of a solution
algorithm challenging [28].

On the other hand, solution methods for the constrained
reinforcement learning problem, i.e., CMDPs with unknown
transition probability, are generally based on Lagrangian primal-
dual type optimization. In these methods, gradient-ascent is
performed on state-values at a fast time scale to ï¬nd the optimal
value function for a given set of Lagrangian multipliers, while
gradient-descent is performed on the Lagrangian multipliers at
a slower time scale. This process is repeated until convergence
to a saddle point. Existing works have explored the primal-
dual optimization approach in the tabular setting, i.e., without
function approximation [12]â€“[15], and with function approx-
imators such as deep neural networks [16]â€“[26]. While this
approach is appealing in its simplicity, it suffers from the typical
problems of gradient optimization on non-smooth objectives,
sensitivity to the initialization of the Lagrange multipliers, and
convergence dependency on the learning rate sequence [19],
[21], [29], [30]. If the learning rate is too small, the Lagrange
multipliers will not update quickly to enforce the constraint;
and if it is too high, the algorithm may oscillate around the
optimal solution. In practice, a sequence of decreasing learning
rates should be adopted to guarantee convergence [14], yet
we do not have an obvious method to determine the optimal
sequence, nor we can assess the quality of a solution in cases
where the objective is not differentiable at the optimal solution.
In this brief, we develop a new approach to solve ï¬nite
CMDPs with discrete state-action space and known probabil-
ity transition function. We ï¬rst prove that the optimization
objective in the dual linear program of a ï¬nite CMDP is

 
 
 
 
 
 
a piece-wise linear convex function (PWLC) with respect
to the Lagrange penalty multipliers. Next, we treat the dual
linear program of a ï¬nite CMDP as a search problem over
the Lagrange penalty multipliers, and propose a novel two-
level Gradient-Aware Search (GAS) algorithm, which exploits
the PWLC structure to ï¬nd the optimal state-value function
and Lagrange penalty multipliers of a CMDP. We empirically
compare the convergence performance of the proposed GAS
algorithm with binary search (BS), Lagrangian primal-dual
optimization (PDO), and Linear Programming (LP), in two
application domains, robot navigation in grid world and wireless
network management. Compared with benchmark algorithms,
we show that the proposed GAS algorithm converges to the
optimal solution faster, does not require hyper-parameter tuning,
and is not sensitive to initialization of the Lagrange penalty
multiplier.

The remainder of this paper is organized as follows. A
background of unconstrained and constrained MDPs is given
in Section II. Our proposed Gradient-Aware Search (GAS)
algorithm is proposed in Section III. Performance evaluation
of GAS in two application domains is presented in Section
IV, followed by our concluding remarks and future work in
Section V.

II. BACKGROUND AND RELATED WORKS

A. Unconstrained Markov Decision Processes

An inï¬nite horizon Markov Decision Process (MDP) with
discounted-returns is deï¬ned as a tuple (S, A, P, Î², R, Î³),
where S and A are ï¬nite sets of states and actions, respectively,
P : S Ã— A Ã— S â†’ [0, 1] is the modelâ€™s state-action-state
transition probabilities, and Î² : S â†’ [0, 1] is the initial
distribution over the states, R : S Ã— A â†’ R, is the reward
function which maps every state-action pair to the set of real
numbers R, and Î³ is the discount factor. Denote the transition
probability from state st = i to state st+1 = j if action
at = a is chosen by Pij(a) := P (st+1 = j|st = i, at = a).
The transition probability from state i to state j is therefore,
pij = P (st+1 = j|st = i) = (cid:80)
a Pij(a)Ï€(a|i), where Ï€(a|i)
is the adopted policy. The state-value function of state i under
policy Ï€, VÏ€(i), is the expected discounted return starting in
state i and following Ï€ thereafter,

t=1

(cid:80)

VÏ€(i) = (cid:80)âˆ

j,a Î³tâˆ’1P Ï€(st = j, at = a|s0 = i)R(j, a), âˆ€i âˆˆ S.
(1)
Let VÏ€ be the vector of state values, VÏ€(i), âˆ€i. The solution
of an MDP is a Markov stationary policy Ï€âˆ— which maximizes
the inner product (cid:104)VÏ€, Î²(cid:105) = (cid:80)

i VÏ€(i)Î²(i), i.e.,

max
Ï€

âˆ
(cid:88)

(cid:88)

t=1

j,a

Î³tâˆ’1P Ï€(st = j, at = a)R(j, a).

(2)

There exist several methods to solve (2), including linear
programming [2] and dynamic programming methods such
as value iteration and policy iteration [1]. Based on the linear
programming formulation, the optimal VÏ€âˆ— can be obtained

2

by solving the following primal linear program [2],

min
V (i)

Î²(i)V (i),

(cid:88)

âˆ€i

V (i) â‰¥ R(i, a) + Î³

(cid:88)

Pij(a)V (j), âˆ€(i, a) âˆˆ S Ã— A.

j

(3)
Linear program (3) has |S| variables and |S| Ã— |A| constraints,
which becomes computationally impractical to solve for MDPs
with large state-action spaces. On the contrary, dynamic
programming is comparatively better suited to handling large
state spaces than linear programming, and is widely considered
the only feasible way of solving (2) for large MDPs [1].
Of particular interest is the value iteration algorithm which
generates an optimal policy in polynomial time for a ï¬xed Î³
[27]. In the value iteration algorithm, the state-value function is
iteratively updated based on Bellmanâ€™s principle of optimality,

V k+1
Ï€

(i) = max
aâˆˆA(i)

[R(i, a) + Î³

(cid:88)

jâˆˆS

Pij(a)V k

Ï€ (j)], âˆ€i âˆˆ S. (4)

It has been shown that the non-linear map VÏ€ â†’ VÏ€ in
(4) is a monotone contraction mapping that admits a unique
ï¬xed point, which is the optimal value function VÏ€âˆ— [2]. By
obtaining the optimal value function, the optimal policy can be
derived using one-step look-ahead: the optimal action at state
i is the one that attains the equality in (4), with ties broken
arbitrarily.

B. Constrained Markov Decision Processes

In constrained MDPs (CMDPs), an additional immediate cost
function C : S Ã— A â†’ R is augmented, such that a CMDP is
deï¬ned by the tuple (S, A, P, Î², R, C, Î³) [5]. The state-value
function is deï¬ned as in (1). In addition, the inï¬nite-horizon
discounted-cost of a state i under policy Ï€ is deï¬ned as,

t=1

(cid:80)

CÏ€(i) = (cid:80)âˆ

j,a Î³tâˆ’1P Ï€(St = j, At = a|So = i)C(j, a), âˆ€i âˆˆ S.
(5)
Let CÏ€ be the vector of state costs, CÏ€(i), âˆ€i, and E âˆˆ R a
given constant which represents the constraint upper-bound.
The solution of a CMDP is a Markov stationary policy Ï€âˆ—
which maximizes (cid:104)VÏ€, Î²(cid:105) subject to a constraint (cid:104)CÏ€, Î²(cid:105) â‰¤ E,

max
Ï€

âˆ
(cid:88)

(cid:88)

t=1
âˆ
(cid:88)

j,a

(cid:88)

t=1

j,a

Î³tâˆ’1P Ï€(st = j, at = a)R(j, a),

Î³tâˆ’1P Ï€(st = j, at = a)C(j, a) â‰¤ E.

(6)

The canonical solution methodology for (6) is based on
convex linear programming [5]. Of interest is the following
dual linear program1 which can be solved for the optimal

1The primal linear program is deï¬ned over a convex set of occupancy
measures, and by the Lagrangian strong duality, (6) can be obtained. Interested
readers can ï¬nd more details about this formulation in Chapter 3 [5].

state-value function and Lagrange penalty multiplier Âµ,

min
V (i),Âµâ‰¥0

(cid:88)

âˆ€i

Î²(i)V (i) + ÂµE,

V (i) â‰¥ R(i, a) âˆ’ ÂµC(i, a) + Î³

(cid:88)

j

Pij(a)V (j),

âˆ€(i, a) âˆˆ S Ã— A.

(7)
Note that for a ï¬xed Lagrangian penalty multiplier Âµ, (7)
is analogous to (3), which can be solved using dynamic
programming and Bellmanâ€™s optimality equation [5],

[R(i, a) âˆ’ ÂµC(i, a) + Î³ (cid:80)

VÏ€âˆ— (i, Âµ) = max
aâˆˆA(i)

jâˆˆS Pij(a)VÏ€âˆ— (j, Âµ)], âˆ€i âˆˆ S.
(8)
This formulation has led to the development of multi time-
scale stochastic approximation based Lagrangian primal-dual
type learning algorithms to solve CMDPs with and without
value function approximation [12]â€“[26]. In such algorithms, Âµ
is updated by gradient descent at a slow time-scale, while the
state-value function is optimized on a faster time-scale using dy-
namic programming (model-based) or gradient-ascent (model-
free), until an optimal saddle point (VÏ€âˆ— (Âµâˆ—), Âµâˆ—) is reached.
These approaches however suffer from the typical problems
of gradient optimization on non-smooth objectives, sensitivity
to the initialization of Âµ, and convergence dependency on the
learning rate sequence used to update Âµ [19], [21], [29], [30].
As we will show in the next section, the optimization objective
in (7) is piecewise linear convex with respect to Âµ, and so it
is not differentiable at the cusps. This means that we cannot
assess the optimality of a solution because Âµâˆ— usually does
not have a zero gradient. This also means that convergence
to Âµâˆ— is dependent on specifying an optimal learning rate
sequence, which is by itself a very challenging task. Unlike
existing methods, our proposed algorithm explicitly tackles
these problems by exploiting problem structure, and so it does
not require hyper-parameter tuning or specifying an optimal
learning rate sequence.

III. A NOVEL CMDP FRAMEWORK

In this section, we describe our novel methodology to
solve ï¬nite CMDPs with discrete state-action space and
known probability transition function. For ease of notation, we
hereby consider the case of one constraint, yet our proposed
algorithm can be readily extended to the multi-constraint case
by successively iterating the method described hereafter over
each Lagrange penalty multiplier, one at a time.

Our proposed method works in an iterative two-level
optimization scheme. For a given Lagrange penalty multiplier
Âµ, an unconstrained MDP with a penalized reward function
Ë†R(i, a) = R(i, a) âˆ’ ÂµC(i, a), âˆ€i, a,
is speciï¬ed, and we
require the corresponding optimal value function VÏ€âˆ— (i, Âµ), âˆ€i
to be found using dynamic programming (8). Denote the
optimization objective in (7) as a function of Âµ by O(Âµ), i.e.,
O(Âµ) = (cid:80)
âˆ€i Î²(i)VÏ€âˆ— (i, Âµ) + ÂµE. Thus to evaluate O(Âµ), one
has to solve for VÏ€âˆ— (i, Âµ), âˆ€i ï¬rst. How to efï¬ciently search
for the optimal Âµâˆ—? To answer this question, we ï¬rst prove
that O(Âµ) is a piecewise linear convex function with respect
to Âµ, and design an efï¬cient solution algorithm next.

3

1, Â· Â· Â· , bi

1, Â· Â· Â· , ai

m âˆˆ R, and bi

Deï¬nition 1: Let ai

m âˆˆ R for
some positive integer i âˆˆ Z+. A Piecewise Linear2 Convex
(cid:9).
function (PWLC) is given by fi(Âµ) = maxl=1,Â·Â·Â· ,m
Theorem 1: Given that VÏ€âˆ— (i, Âµ), âˆ€i is the optimal value
function of an unconstrained MDP with a penalized reward
function Ë†R(i, a) = R(i, a) âˆ’ ÂµC(i, a), âˆ€i, a, Î²(i) is the
probability that the initial state is i, and E is a constant
representing the cost constraint upper bound in the original
CMDP, O(Âµ) = (cid:80)
âˆ€i Î²(i)VÏ€âˆ— (i, Âµ) + ÂµE is a PWLC function
with respect to Âµ.

lÂµ + bi
l

(cid:8)ai

Proof: For ease of exposition, we introduce the following
three properties of PWLC functions, and show how they can
be used to construct the proof,

(cid:8)(ai

1) fi(Âµ) + fj(Âµ) is PWLC for any i, j âˆˆ Z+. Notice
that for point-wise maximum, maxl{al} + maxk{ak} =
maxl,k{al + ak}. Hence, fi(Âµ) + fj(Âµ) = maxl,k
l +
k)(cid:9), which is PWLC.
l + bj
aj
k)Âµ + (bi
l +fj(Âµ)(cid:9) is PWLC for any i, j âˆˆ Z+. No-
(cid:8)ai
lÂµ+bi
2) maxl
(cid:110)
= maxl{al}+maxk{ak}
al+maxk{ak}
tice that maxl
l + fj(Âµ)(cid:9)
(cid:8)ai
lÂµ + bi
= maxl,k{al + ak}. Hence, maxl
(cid:111)
(cid:8)(ai
kÂµ+bj
ai
lÂµ+bi
= maxl
l +
= maxl,k
k}
k)(cid:9), which is PWLC.
aj
l + bj
k)Âµ + (bi
3) fi(Âµ) + cÂµ is PWLC. Notice that fi(Âµ) + cÂµ =
(cid:9), which
(cid:8)(ai
(cid:9) + cÂµ = maxl

l +maxk

l + c)Âµ + bi
l

lÂµ + bi
l

(cid:8)aj

(cid:111)

(cid:110)

(cid:8)ai
maxl
is PWLC.

Based on these properties, the proof proceeds as follows. Given
VÏ€âˆ— (i, Âµ), the recursive formula in (8) is expanded by iteratively
substituting for VÏ€âˆ— (i, Âµ) with the right hand side of (8). This
expansion allows us to deduce that VÏ€âˆ— (i, Âµ) has the following
functional form,

(cid:40)

VÏ€âˆ— (i, Âµ) = maxl

lÂµ + bi
ai

l + Î³maxk

(cid:110)

k Âµ + bi(cid:48)
ai(cid:48)

k + Î³maxm

(cid:8)ai(cid:48)(cid:48)

m Âµ + bi(cid:48)(cid:48)

(cid:41)
m + Â· Â· Â· (cid:9)(cid:111)

,

l, bi

l, ai(cid:48)

k , bi(cid:48)
k , Â· Â· Â· âˆˆ R which come from
for some constants ai
discounted rewards, costs, and their product with the transition
probability function. Based on successive application of proper-
ties 1 and 2, VÏ€âˆ— (i, Âµ), âˆ€i are PWLC functions with respect to
Âµ. Also, (cid:80)
âˆ€i Î²(i)VÏ€âˆ— (i, Âµ) is PWLC with respect to Âµ based
on property 1. Finally, O(Âµ) = (cid:80)
âˆ€i Î²(i)VÏ€âˆ— (i, Âµ) + ÂµE is a
PWLC function with respect to Âµ based on property 3.

To efï¬ciently ï¬nd Âµâˆ— and solve (7), we propose a novel
Gradient-Aware Search (GAS) algorithm which exploits the
piecewise linear convex structure. Our proposed GAS algorithm,
which is outlined in Algorithm 1, operates over two-loops.
For a given Lagrange multiplier ÂµÃ— (line 6) at an outer loop
iteration, the inner loop ï¬nds the optimal state-value function,
VÏ€âˆ— (i, ÂµÃ—), âˆ€i using value iteration, as well as the gradient
components of O(Âµ) with respect to ÂµÃ— (lines 7-16). Based on
this information, the outer loop exploits the PWLC structure
to calculate the next ÂµÃ— (line 17, lines 20-25). This process
continues until a termination criteria is met (lines 18-19). In
what follows, the proposed GAS algorithm is outlined in detail.
Inner loop: The value iterations of the inner loop are sub-
scripted with the index k. Let Q(i, a, Âµ), âˆ€i, a, denote the state-
action value function which is the expected discounted return

2Afï¬ne is more accurate but less common.

Input

Algorithm 1: Gradient-Aware Search CMDP Solver
: R(i, a), C(i, a), Pij(a),
âˆ€(i, a) âˆˆ S Ã— A
: V âˆ—(i), âˆ€i âˆˆ S, Âµâˆ—

Output
Algorithm Parameters : (cid:15), (cid:15)(cid:48)

1 Initialize: Âµ+ = M , Âµâˆ’ = 0, O(Âµ+), O(Âµâˆ’)

âˆ‚O

âˆ‚Âµ+ , âˆ‚O

âˆ‚Âµâˆ’ , Omin = âˆ

2 repeat
3

âˆ† = âˆ, k = 0, Vk(i) = 0, âˆ€i âˆˆ S,
Ï‰1
k(i, a) = 0, âˆ€(i, a) âˆˆ S Ã— A
Qk(i, a) = 0, âˆ€(i, a) âˆˆ S Ã— A
Find the intersection ÂµÃ— of the two lines deï¬ned by
(O(Âµ+), âˆ‚O
repeat

âˆ‚Âµ+ ) and (O(Âµâˆ’), âˆ‚O
âˆ‚Âµâˆ’ )

Qk(i, a, ÂµÃ—) = R(i, a) âˆ’ ÂµÃ—C(i, a)

+ Î³ (cid:80)

jâˆˆS Pij(a)Vk(j), âˆ€(i, a) âˆˆ S Ã— A
Qk(i, a, ÂµÃ—), âˆ€i âˆˆ S

Vk+1(i, ÂµÃ—) = max
aâˆˆA(i)
Q(i, a, ÂµÃ—), âˆ€i âˆˆ S

Ëœa(i) = argmax
aâˆˆA(i)
k+1(i, a) = C(i, a) + Î¶ (cid:80)
Ï‰1

jâˆˆS Pij(a)Ï‰1

k(j, Ëœa(j)),
âˆ€(i, a) âˆˆ S Ã— A

|Vk+1(i,ÂµÃ—)âˆ’Vk(i,ÂµÃ—)|
|Vk(i,ÂµÃ—)|

(cid:80)
i

âˆ† = 1
|S|
k = k + 1
until âˆ† < (cid:15);
Calculate O(ÂµÃ—), âˆ‚O
if

|Omin âˆ’ O(ÂµÃ—)| â‰¤ (cid:15)(cid:48) then
Break

âˆ‚ÂµÃ— based on (10) and (11)

if 0 â‰¤ âˆ‚O

if Omin > O(ÂµÃ—) then
Omin = O(ÂµÃ—)
âˆ‚ÂµÃ— < âˆ‚O
Âµ+ = ÂµÃ—, âˆ‚O
âˆ‚O
âˆ‚Âµâˆ’ < âˆ‚O
Âµâˆ’ = ÂµÃ—, âˆ‚O

âˆ‚Âµ+ then
âˆ‚Âµ+ = âˆ‚O
âˆ‚ÂµÃ—
âˆ‚ÂµÃ— < 0 then
âˆ‚Âµâˆ’ = âˆ‚O
âˆ‚ÂµÃ—

if

25
26 until forever;
27 Âµâˆ— = Âµ+
28 V âˆ—(i) = Vk+1(i), âˆ€i âˆˆ S

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

starting in state i, taking action a, and following Ï€ thereafter,
for a ï¬xed Âµ. Note that we drop the subscript Ï€ to avoid
notation clutter, but dependence on Ï€ remains unchanged. The
state-action value function at each step k can be represented
as a linear function of ÂµÃ—,

Qk(i, a, ÂµÃ—) = Ï‰0

k(i, a) âˆ’ Ï‰1

k(i, a)ÂµÃ—.

(9)

Qk(i, a, ÂµÃ—) = Ï‰0

k(i, a) (line 12). Given Ï‰0

In this representation, accumulated discounted rewards con-
tribute to Ï‰0
k(i, a) and accumulated discounted costs contribute
to Ï‰1
k(i, a), âˆ€a, and ÂµÃ—,
k(i, a), Ï‰1
the value function at state i can be obtained by Vk(i, ÂµÃ—) =
k(i, Ëœa)ÂµÃ— (line 10), where
k(i, Ëœa) âˆ’ Ï‰1
max
a
Ëœa denotes the action which attains the maximum state-value
Qk(i, a, ÂµÃ—)
among the set of actions at state i, Ëœa(i) = argmax
(line 11). Notice that in the inner loop, two functions are
estimated using value iterations, the total state-action value
function (line 8), and the state-action cost function (line 12).

a

4

Dynamic programming steps in the inner loop are performed
until the mean relative absolute error in the estimation of the
state value function is less than a preset accuracy threshold
(cid:15) (line 14-16). When the inner loop concludes, the optimal
VÏ€âˆ— (i, ÂµÃ—), Ï‰1

Ï€âˆ— (i, Ëœa), âˆ€i, are returned to the outer loop.

Outer loop: Given ÂµÃ—, and VÏ€âˆ— (i, ÂµÃ—), Ï‰1

Ï€âˆ— (i, Ëœa), âˆ€i from
the inner loop, O(ÂµÃ—) and the gradient of O(ÂµÃ—) with respect
to ÂµÃ—, can be obtained based on (line 17),

O(ÂµÃ—) =

(cid:88)

i

VÏ€âˆ— (i, ÂµÃ—)Î²(i) + ÂµÃ—E,

(10)

and,

âˆ‚O
âˆ‚ÂµÃ— =

âˆ‚
âˆ‚ÂµÃ—

=

=

âˆ‚
âˆ‚ÂµÃ—

âˆ‚
âˆ‚ÂµÃ—
(cid:88)

= âˆ’

(cid:88)

i
(cid:88)

i
(cid:88)

VÏ€âˆ— (i, ÂµÃ—)Î²(i) + ÂµÃ—E

max
a

QÏ€âˆ— (i, a, ÂµÃ—)Î²(i) + ÂµÃ—E

(11)

(cid:0)Ï‰0

k(i, Ëœa) âˆ’ Ï‰1

k(i, Ëœa)ÂµÃ—(cid:1)Î²(i) + ÂµÃ—E

i
Ï‰1

Ï€âˆ— (i, Ëœa)Î²(i) + E.

i

i Ï‰1

i Ï‰1

When âˆ‚O

âˆ‚ÂµÃ— is negative, i.e., (cid:80)

Ï€âˆ— (i, Ëœa)Î²(j) > E, the
current policy Ï€âˆ—(ÂµÃ—) is infeasible, which means ÂµÃ— should
be increased. On the other hand, when âˆ‚O
âˆ‚ÂµÃ— is non-negative,
i.e., (cid:80)
Ï€âˆ— (i, Ëœa)Î²(j) â‰¤ E, the current policy is feasible, yet
we can potentially obtain another feasible policy with larger
returns by decreasing ÂµÃ—. Notice that if ÂµÃ— is a cusp which lies
at the intersection point of two linear segments of the PWLC
function O(Âµ), âˆ‚O
âˆ‚ÂµÃ— does not exist in theory, although the
two one-sided derivatives (left and right) are well-deï¬ned. The
nonexistence of a derivative at cusps does not pose a challenge
to our proposed algorithm, because computations are accurate
up to the machineâ€™s ï¬‚oating point precision. To further clarify,
suppose ÂµÃ— is a cusp on the curve of O(Âµ). In practice, we
can only compute the gradient (11) at (ÂµÃ— Â± Ë†(cid:15)), where Ë†(cid:15) is
a very small precision loss due to rounding in ï¬‚oating point
arithmetic, which is at the order of 10âˆ’16 for double precision
as deï¬ned by the IEEE 754-2008 standard. This means that
computing the gradient at a point is essentially computing a
one-sided derivative, which always exists.

It is worth to mention that our proposed algorithm does not
perform gradient optimization (which suffers on non-smooth
objectives) but rather exploits the structure of the PWLC
objective to ï¬nd the optimal Âµâˆ—. The algorithm always retains
two values for Âµ, {Âµâˆ’, Âµ+}, where âˆ‚O
âˆ‚Âµ+ â‰¥ 0.
Âµâˆ’ along with âˆ‚O
âˆ‚Âµâˆ’ deï¬ne a line Lâˆ’, while Âµ+ along with
âˆ‚O
âˆ‚Âµ+ deï¬ne another line L+. Lâˆ’ and L+ intersect at a point
ÂµÃ—, which is passed to the inner loop. There are two possible
outcomes when running the inner loop with ÂµÃ—,

âˆ‚Âµâˆ’ < 0 and âˆ‚O

1) 0 â‰¤ âˆ‚O

âˆ‚ÂµÃ— < âˆ‚O

âˆ‚Âµ+ , in this case O(ÂµÃ—) < O(Âµ+) because
O(Âµ) is PWLC with respect to Âµ. The algorithm retains
ÂµÃ— and âˆ‚O
âˆ‚ÂµÃ— by setting, Âµ+ = ÂµÃ— and âˆ‚O
âˆ‚Âµ+ = âˆ‚O
âˆ‚ÂµÃ—
(lines 22-23).
âˆ‚Âµâˆ’ < âˆ‚O
âˆ‚O
âˆ‚ÂµÃ— < 0, in this case O(ÂµÃ—) < O(Âµâˆ’) because
O(Âµ) is PWLC with respect to Âµ. The algorithm retains

2)

ÂµÃ— and âˆ‚O
(lines 24-25).

âˆ‚ÂµÃ— by setting, Âµâˆ’ = ÂµÃ— and âˆ‚O

âˆ‚Âµâˆ’ = âˆ‚O
âˆ‚ÂµÃ—

The algorithm terminates when the absolute error between Omin
which is the minimum objective value found so far, and O(ÂµÃ—),
is less than an arbitrarily small positive number (cid:15)(cid:48) (lines 18-19).
Note that the initial Âµ+ is set to M , an arbitrarily large number
which attains âˆ‚O
âˆ‚Âµ+ â‰¥ 0. If no such M can be found, then (7) is
not feasible, i.e., O(Âµ) is not lower bounded and can be made
arbitrarily small O(Âµ) â†’ âˆ’âˆ by taking Âµ â†’ âˆ. Figure 1
below shows a visual illustration of how Algorithm 1 works in
practice. It is worth to mention that Algorithm 1 only evaluates
O(Âµ) at the points indicated by a green circle.

illustration of Algorithm 1. The proposed
Fig. 1: Visual
algorithm always retains two values for the Lagrange multiplier
Âµ, {Âµâˆ’, Âµ+}, where âˆ‚O
âˆ‚Âµâˆ’ < 0 and âˆ‚O
âˆ‚Âµ+ â‰¥ 0. Âµâˆ’ along with
âˆ‚Âµâˆ’ deï¬ne a line Lâˆ’, while Âµ+ along with âˆ‚O
âˆ‚O
âˆ‚Âµ+ deï¬ne another
line L+. Lâˆ’ and L+ intersect at a point ÂµÃ—, at which O(ÂµÃ—)
is evaluated (green circle). Based on this evaluation, Âµâˆ’, Âµ+
are updated and the process is repeated until the global minima
is reached. In this example, Âµ+

2 is the optimal Âµâˆ—.

3 = ÂµÃ—

IV. CASE STUDIES

In this section, we evaluate the efï¬cacy of the proposed
GAS algorithm in two different domains, robot navigation in
a grid world, and solar-powered UAV-based wireless network
management. In all the experiments, (cid:15) and (cid:15)(cid:48) are set to 10âˆ’10
unless otherwise speciï¬ed.

A. Grid World Robot Navigation

In the grid world robot navigation problem, the agent controls
the movement of a robot in a rectangular grid world, where
states represent grid points on a 2D terrain map. The robot starts
in a safe region in the bottom right corner (blue square), and
is required to travel to the goal located in the top right corner
(green square), as can be seen from Figure 3. At each time step,
the agent can move in either of four directions to one of the
neighbouring states. However, the movement direction of the
agent is stochastic and partially depends on the chosen direction.
Speciï¬cally, with probability 1 âˆ’ Î´, the robot will move in
the chosen direction, and uniformly randomly otherwise, i.e.,
with probability Î´/4 the robot will move to one of the four

5

neighboring states. At each time step, the agent receives a
reward of âˆ’1 to account for fuel usage. Upon reaching the
goal, the agent receives a positive reward of Ë†M >> 1. In
between the starting and destination states, there are a number
of obstacles (red squares) that the agent should avoid. Hitting
an obstacle costs the agent Ë†M . It is important to note that the
2D terrain map is built such that a shorter path induces higher
risk of hitting obstacles. By maximizing long-term expected
discounted rewards subject to long-term expected discounted
costs, the agent ï¬nds the shortest path from the starting state
to the destination state such that the toll for hitting obstacles
does not exceed an upper bound. This problem is inspired by
classic grid world problems in the literature [18], [28].

1) Experiment Results: We choose a grid of 20 Ã— 20 with a
total of 400 states. The start state is (2, 18), the destination is
(19, 18), Ë†M = 2/(1âˆ’Î³), Î´ = 0.05, Î³ = 0.99, and 30 obstacles
are deployed. We follow [28] in the choice of parameters, which
trades off high penalty for obstacle collisions and computational
complexity. O(Âµ) is plotted in Figure 2(a) over Âµ âˆˆ [0, 200].
It can be seen from Figure 2(a) that O(Âµ) is a PWLC function
as given by Theorem 1. It can be also seen that the global
minima is a non-differentiable cusp around 90 with unequal
one-sided derivatives.

In Figure 2(b), we compare the convergence performance
of the proposed GAS with binary search (BS), by plotting the
number of outer loop iterations versus an accuracy level (cid:15)(cid:48). The
initial search window for Âµ is [0, M ]3, and M is either 103 or
105. Given a value for Âµ, the inner loop evaluates (10) and its
gradient (11). The outer loop then determines the next Âµ either
based on the proposed GAS or BS. This iterative procedures
continues until the convergence criteria is met. Compared to
BS, the proposed GAS algorithm requires a smaller number
of outer loop iterations for all levels of accuracy thresholds (cid:15)(cid:48).
This is because GAS adaptively shrinks the search window by
evaluating points at the intersection of line segments with non-
negative and negative gradients, whereas BS blindly shrinks the
window size by 1
2 every iteration. These results demonstrate
that (cid:15)(cid:48) can be set arbitrarily small and M can be set arbitrarily
large without substantially impacting the convergence time.

In Figure 2(c) we compare the convergence performance
of the proposed GAS with the Lagrangian primal-dual opti-
mization (Lagrangian PDO), by plotting the total number of
value iterations, which is the sum of the number of value
iterations over all outer loop iterations, versus the learning
rate decay parameter in a log-log scale. In the Lagrangian
PDO, gradient descent on Âµ is performed along side dynamic
programming iterations on V (i), âˆ€i, which motivates us to
compare the convergence performance in terms of total number
of value iterations. In Lagrangian PDO, the update rule for Âµ
âˆ‚O
, where Îºk is the step size parameter.
is Âµk+1 = Âµk âˆ’ Îºk
âˆ‚Âµk
In our experiments, Îº0 is initially set
to 1 to speed up
descending towards the minima, and is decayed according to
Îºk+1 = Îºkeâˆ’Î¾T , where Î¾ is the learning rate decay parameter,
and T is the number of times âˆ‚O
changes signs. While such
âˆ‚Âµk

3M is problem speciï¬c. The ability to choose an arbitrarily high value
for M without negatively impacting convergence time is desirable because it
demonstrates the algorithmâ€™s capability to make adaptively large strides in the
search space towards the optimal solution.

ğœ‡ğ’ª(ğœ‡)ğœ‡!"ğœ‡!#ğ’ª(ğœ‡!")ğ’ª(ğœ‡!#)ğ’ª(ğœ‡$#)ğœ‡$#=ğœ‡!Ã—ğ’ª(ğœ‡&")ğ’ª(ğœ‡â€™#)ğœ‡â€™#=ğœ‡&Ã—ğœ‡&"=ğœ‡$Ã—choice of learning rate decay does not necessarily produce an
optimal sequence, it serves to expose the sensitivity of gradient
based optimization to the initial Âµ0 and decay parameter Î¾.
For every Î¾ âˆˆ [10âˆ’4, 1], Âµ0 is uniformly randomly initialized
100 times from [0, M ], and the mean number of total value
iterations is plotted in Figure 2(c). It can be seen from Figure
2(c) that the convergence of the Lagrangian PDO method is
sensitive to the initialization of Âµ0 which can be inferred from
the initial window size, and the decay rate parameter. For some
Î¾, Lagrangian PDO converges faster than GAS, but for other
values it lags behind. In practice, Î¾ is a hyper-parameter which
requires tuning, consuming extra computational resources. In
contrast, the proposed GAS algorithm works out of the box
without the need to specify a learning rate sequence or tune
any hyper-parameters.

In Table I, we compare the results obtained by the pro-
posed GAS algorithm at convergence with those obtained by
solving (7) using Linear Programming. We use the LP solver
provided by Gurobi, which is arguably the most powerful
mathematical optimization solver in the market. The opti-
mal Âµâˆ—, O(Âµâˆ—), mini|BE(i)|, Ei[|BE(i)|], and maxi|BE(i)|
are reported, where BE(i) is the Bellman error at state i
given by, BE(i) = VÏ€âˆ— (i, Âµâˆ—) âˆ’ max
[R(i, a) âˆ’ ÂµC(i, a) +
aâˆˆA(i)
Î³ (cid:80)
jâˆˆS Pij(a)VÏ€âˆ— (j, Âµâˆ—)], âˆ€i âˆˆ S. It can be observed that
Gurobiâ€™s LP solver converges to a sub-optimal solution with
a higher objective value compared with the proposed GAS
algorithm (recall that (7) is a minimization problem, hence
the lower O(Âµ) the better). This demonstrates that generic LP
solvers may struggle to solve CMDPs, which has motivated
the research community to develop CMDP speciï¬c algorithms.

TABLE I: Performance comparison with LP

Grid World
Robot Navigation

Solar-Powered UAV-
Based Wireless Network

Âµâˆ—
mini|BE(i)|
Ei[|BE(i)|]
maxi|BE(i)|
O(Âµâˆ—)

LP (Gurobi)
68.33333
0.0
9.89e-10
1.13e-07
15216.83

GAS
90.08102
3.11e-07
3.11e-07
3.11e-07
15184.57

LP (Gurobi)
0.030786
0.0
0.001788
1.160397
93.92830

GAS
0.030786
9.32e-09
9.32e-09
9.33e-09
93.92830

In Figure 3, ï¬lled contours maps for the value function are
plotted. Regions which have the same value have the same
color shade. The value function is plotted for four different
values of the cost constraint upper bound, along with the start
state (blue square), destination (green square), and obstacles
(red squares). With a tighter cost constraint (E = 5), a safer
policy is learned in which the agent takes a long path to go
around the obstacles as in Figure 3(d), successfully reaching
the destination P s
Ï€âˆ— = 99.90% of the times. When the cost
constraint is loose (E = 160), a riskier policy is learned in
which the agent takes a shorter path by navigating between the
obstacles as in Figure 3(a), successfully reaching the destination
P s

Ï€âˆ— = 76.85% of the times, based on 2000 roll outs.

B. Solar Powered UAV-Based Wireless Networks

As a second application domain, we study a wireless
network management problem, which is of interest to the

6

wireless networking research community. We consider a solar-
powered UAV-based wireless network consisting of one UAV
which relays data from network servers to N wireless devices.
Wireless devices are independently and uniformly deployed
within a circular geographical area A with radius Rc. The UAV
is deployed at the center of A, and is equipped with solar panels
which harvest solar energy to replenish its on-board battery
storage. Because solar light is attenuated through the cloud
cover, solar energy harvesting is highest at higher altitudes.
System time is slotted into ï¬xed-length discrete time units
indexed by t. During a time slot t, the solar-powered UAV
provides downlink wireless coverage to wireless devices on
the ground. The UAV is equipped with an intelligent controller
which at each time slot t, controls its altitude zt, its directional
antenna half power beamwidth angle Î¸t
B, and its transmission
power P t
TX in dBm, based on its battery energy state and
altitude, to maximize the wireless coverage probability for the
worst case edge user, while ensuring energy sustainability of
the solar-powered UAV. Note that the worst case edge user
is the user which is farthest from the UAV. Due to distance
dependent free-space path loss, the received signal at the edge
user from the UAV is heavily attenuated, resulting in a low
Signal to Noise (SNR) ratio and decreased coverage probability.
Thus, by maximizing the coverage probability of the edge user,
the communication performance is improved for every user in
the network, albeit not proportionally. This wireless network
control problem can be modeled as follows,

1) Communication Model: during time slot t, the antenna

gain can be approximated by [31],

Gt

Antenna =

(cid:40) 29000
B )2 ,
(Î¸t
g(Ï†),

Î¸t
B

2 â‰¤ Ï† â‰¤ Î¸t
otherwise,

B
2

(12)

(cid:17)Î·

(cid:16) 180

LoS = Î¶

Ï€ Ïˆt âˆ’ 15

dB = 10log10(Gt

where Î¸t
B is in degrees, Ï† is the sector angle, and g(Ï†) is
the antenna gain outside the main lobe. The antenna gain
in dB scale is Gt
Antenna). The air-to-ground
wireless channel can be modeled by considering Line-of-Sight
(LoS) and non-Line-of-Site (NLos) links between the UAV
and the users separately. The probability of occurrence of
LoS for an edge user during time slot t can be modeled by
P t
, where Î¶ and Î· are constant values
reï¬‚ecting the environment impact, and Ïˆt is the elevation
angle between the UAV and the edge user during time slot
t, Ïˆt = tanâˆ’1(zt/Rc) [31]. The probability of an NLoS
link is P t
LoS. The shadow fading for the
LoS and NLoS links during time slot t can be modeled
by normal random variables Î·t
LoS)2) and
LoS âˆ¼ N (ÂµLoS, (Ïƒt
N LoS)2), respectively, where the mean
N LoS âˆ¼ N (ÂµN LoS, (Ïƒt
Î·t
and variance are in dB scale and depend on on the elevation an-
gle and environment parameters, Ïƒt
LoS(Ïˆt) = k1exp(âˆ’k2Ïˆt),
Ïƒt
N LoS(Ïˆt) = g1exp(âˆ’g2Ïˆt) [31]. Accordingly, the coverage
probability for the edge user during time slot t, deï¬ned as the
probability the received SNR by an edge user is larger than a

N LoS = 1 âˆ’ P t

7

(a) The piecewise linear convex function O(Âµ)

(b) Comparison with Binary Search (BS)

(c) Comparison with Lagrangian PDO

Fig. 2: Performance comparison on the grid world robot navigation problem

(a) P s

Ï€âˆ— = 76.85%, E = 160

(b) P s

Ï€âˆ— = 79.65%, E = 40

(c) P s

Ï€âˆ— = 93.55%, E = 20

(d) P s

Ï€âˆ— = 99.90%, E = 5

Fig. 3: Filled contour maps of the value function and the learned control policy for the grid world robot navigation problem.
With a lower upper bound on the cost constraint (E), the policy is more risk-averse and achieves a higher probability of success
for reaching the goal P s

Ï€âˆ— . The shown path is one realization based on the learned policy.

threshold (SNRTh) is,
(cid:16) âˆ’P t

cov =P t
P t

LoSQ

T X âˆ’ Gt

dB + ÂµLoS + LdB + Pmin

(cid:17)

(cid:16) âˆ’P t

T X âˆ’ Gt

ÏƒLoS
dB + ÂµN LoS + LdB + Pmin

+ P t

N LoSQ

ÏƒN LoS
(13)
where Q(cid:0)x(cid:1) = 1 âˆ’ P (X â‰¤ x)
for a standard
random variable X with mean 0 and variance
normal
1, Pmin = 10log10(N0SNRTh), N0 is the noise ï¬‚oor
power, LdB is the distance-dependent path-loss, LdB =
10Î±log
, c is the speed of light, f0 is the
carrier frequency, and Î± is the path loss exponent.

(Rc)2+(zt)2

(cid:16) 4Ï€f0

âˆš

(cid:17)

c

2) UAV Energy Model: the attenuation of solar light passing
through a cloud is modeled by Ï†(dcloud) = eâˆ’ Ë†Î²cdcloud
, where
Ë†Î²c â‰¥ 0 denotes the absorption coefï¬cient modeling the optical
characteristics of the cloud, and dcloud is the distance that the
solar light travels through the cloud [32]. The solar energy
harvesting model in time slot t is,

Et

solar =

ï£±
ï£´ï£´ï£²

ï£´ï£´ï£³

zt+zt+1
2

Ï„ Ë†S Ë†Gâˆ†t,
Ï„ Ë†S Ë†GÏ†(zup âˆ’ zn)âˆ†t, zlow â‰¤ zt+zt+1
Ï„ Ë†S Ë†GÏ†(zup âˆ’ zlow)âˆ†t, zt+zt+1

â‰¥ zhigh

2
< zlow

2

< zhigh

(14)
where Ï„ is a constant representing the energy harvesting
efï¬ciency, Ë†S is the area of solar panels, Ë†G is the average solar
radiation intensity on earth, and âˆ†t is the time slot duration.
zhigh and zlow are the altitudes of upper and lower boundaries
of the cloud. Based on this model, the output solar energy
is highest above the cloud cover at zhigh, and it attenuates
exponentially through the cloud cover until zlow. During a
time-slot t, the UAV can cruise upwards or downwards at a

constant speed of vt
consumed for cruising or hovering during time slot t is,
âˆš

z, or hover at the same altitude. The energy

(cid:17)

Et

UAV =

(cid:32)

W 2/(
âˆš

(cid:33)

2ÏA)

âˆ†t

2Vh
z + Pstatic + P t

T X

+ (cid:0)W vt

vt
z =

zt+1 âˆ’ zt
âˆ†t

,

Vh =

(15)

(cid:1) âˆ†t,
(cid:115)

W
2ÏA

Here, W is the weight of the UAV, Ï is air density, and A
is the total area of UAV rotor disks. Pstatic is static power
consumed for maintaining the operation of UAV. It is worth
to mention that cruising upwards consumes more power than
cruising downwards or hovering. Denote the battery energy
storage of the UAV at the beginning of slot t by Bt. The
battery energy of the next slot is given by,

Bt+1 = max(cid:8)0, min{Bt + Et

solar âˆ’ Et

UAV, Bmax}(cid:9)

(16)

3) CMDP Formulation: this constrained control problem
of maximizing the wireless coverage probability for the worst
case edge user, while ensuring energy sustainability of the
solar-powered UAV can be modeled as a discrete-time CMDP
with discrete state-action spaces as follows. First, the altitude
of the UAV is discretized into Nz discrete units of zmaxâˆ’zmin
.
Let the set of possible UAV altitudes be Z = {zmin, zmin +
zmaxâˆ’zmin
, Â· Â· Â· }. In addition, the ï¬nite
Nz
battery energy of the UAV is discretized into Nb energy units,
where each energy unit is eu = BmaxÃ—60Ã—60
Joules. Let B âˆˆ
{0, eu, Â· Â· Â· , (Nb âˆ’ 1)eu} be the set of possible UAV battery
energy levels. Accordingly, the CMDP can be formulated as
follows,

, zmin + 2 zmaxâˆ’zmin

Nz

Nz

Nb

050100150200Lagrange penalty ()1520015400156001580016000()109107105103101Accuracy threshold 01020304050Outer Loop IterationsBS [0,103]BS [0,106]GAS [0,103]GAS [0,106]104103102101100Learning Rate Decay Parameter 104Mean Total Value IterationsGAS [0,103]GAS [0,105]Lagrangian PDO [0,103]Lagrangian PDO [0,105]1290013650144001515015900166501740018150189001290013650144001515015900166501740018150189005000250002500500075001000012500150001750037500300002250015000750007500150001) The state of the agent is the battery energy level and
UAV altitude, âˆ€st âˆˆ S, st = (Bt, zt), where Bt âˆˆ B
and zt âˆˆ Z. Thus S = B Ã— Z.

2) The agent controls the UAV vertical velocity vt

z âˆˆ Az,
the antenna transmission power P t
TX âˆˆ APT X , and the
half power beamwidth angle Î¸t
B âˆˆ AÎ¸B . Thus, âˆ€at âˆˆ A,
A = Az Ã— APT X Ã— AÎ¸B , where Az = {âˆ’4, 0, 4} m/s,
APT X = {34, 38} dBm, and AÎ¸B = {28o, 56o}.

3) The immediate reward is the wireless coverage probabil-
ity for the edge user during time slot t, R(st, at) = P t
cov.
4) The immediate cost is the change in the battery level,

C(st, at) = Bt âˆ’ Bt+1.

5) E = âˆ’âˆ†B, where âˆ†B is the minimum desired battery

8

(a) Comparison with binary search

energy increase over the initial battery energy.

6) Model dynamics (cid:8)P (st+1|st, at), âˆ€s, a(cid:9): battery evolu-
tion of the UAV is modeled as an M/D/1/Nb queue.
Energy arrival is according to a poisson process with rate
Î»t = Et
. Energy arrivals which see a full battery are
solar
eu
rejected, and do not further inï¬‚uence the system. The
ï¬nite battery size acts as a regulator on the queue size.
Energy departure is deterministic and depends on the
action taken by the controller. Hence, energy departure
rate is Âµt = Et
. Utilization factor of the battery is
= Et
therefore Ït
UAV(at) . Based on poisson energy
Et
arrivals, the probability of k arrivals during a one unit
energy departure given action at is taken,

U AV (at)
eu
B = Î»t
Âµt

solar

P {k energy arrivals|at} =

(Ït

B

B)keâˆ’Ït
k!

(17)

On the other hand, altitude state evolution is determinsitic
based on vt
z. Hence, the transition probability function
of the CMDP can be derived based on (17) and the
probability transition matrix of the embedded Markov
chain with action at for an M/D/1/Nb queue [33].
This wireless communication system exhibits a trade-off
between the altitude of the UAV and the coverage probability
for the edge user. When the UAV hovers at a higher altitude, it
can harvest more solar energy to replenish its on-board battery.
However, at higher altitudes, the wireless coverage probability
is worse due to signal attenuation. An optimal control policy
should be learned to maximize the coverage probability for the
edge user while ensuring the UAVâ€™s battery is not depleted.

4) Experiment Results:

simulation parameters for this
experiment are outlined in Table II. Based on the chosen
discretization levels Nz and Nb, the CMDP has |S| = 3025
states and |A| = 12. In Figures 4(b) and 4(c), we compare the
convergence performance of the proposed GAS with BS and the
Lagrangian PDO approach, respectively. As previously noted
from Figures 2(b) and 2(c), it can be see that the proposed
algorithm compares favourably to BS and Lagrangian PDO,
despite the increased problem size. From Table I, we can
observe that both the proposed GAS and Gurobiâ€™s LP solver
converge to the same Âµâˆ—, although GAS achieves a lower
Bellman error in the estimation of the value function.

(b) Comparison with Lagrangian approach

Fig. 4: Performance comparison on the solar powered UAV-
Based wireless network management problem

TABLE II: Simulation parameters for the solar powered UAV-
Based wireless network management problem

Parameter
Nz
Î±
Bmax
âˆ†t
f0
g(Ï†)
n0
Nb
Nz
SNIRTh
Ë†Î²c
zhigh, zlow
zmin, zmax
ÂµLOS , ÂµN LOS
Î³

Value
121
2.5
100 Wh
10s
2 GHz
0
âˆ’100dBm
25
121
5
0.01

Parameter
Nb
Ï„
âˆ†B
ËœS
ËœG
W
Ï
A
Pstatic
Rc
k1, k2

1.3, 0.7km âˆ†zmin, âˆ†zmax
0.5, 1.5km
1, 20dB
0.99

g1, g2
Î¶
Î·

Value
25
0.4
1.67 Wh
1m2
1367W/m2
39.2kg âˆ— m/s2
1.225kg/m3
0.18m2
5 watts
250 m
10.39, 0.05
âˆ’40m, 40m
29.06, 0.03
0.6
0.11

recharge its on-board battery, and then climbs down when the
battery is full to improve the coverage probability for the worst
case edge user, as can be seen from Figures 5(c) and 5(d). In
addition, Figure 5(b) shows the learned control policy for the
transmission power and half power beamwidth angle. When the
UAV is up to charge its battery, the lower transmission power
and smaller beamwidth angle are selected. On the other hand,
when the UAV is down to improve the coverage probability,
the higher transmission power and larger beamwidth angle
are selected. This is because with a smaller beamwidth angle,
the antenna gain is higher, and a lower transmission power is
required to counter the effects of large scale fading.

V. CONCLUSION

The learned policy by our proposed GAS algorithm is shown
in Figure 5. It can be seen from Figure 5(a) that the agent
learns an adaptive policy in which the UAV climbs up to

In this brief, we have proved that the optimization objective
in the dual linear programming formulation of a constrained
Markov Decision process (CMDP) is a piece-wise linear convex

109107105103101Accuracy threshold 01020304050Outer Loop IterationsBS [0,103]BS [0,106]GAS [0,103]GAS [0,106]104103102101100Learning Rate Decay Parameter 104105Mean Total Value IterationsGAS [0,103]GAS [0,105]Lagrangian PDO [0,103]Lagrangian PDO [0,105]9

(a) UAV altitude

(b) Tx power and beamwidth

(c) Battery energy evolution

(d) Edge user coverage probability

Fig. 5: Learned control policy for the solar powered UAV-Based wireless network management problem

function (PWLC) with respect to the Lagrange penalty multi-
plier. Based on this result, a novel two-level Gradient-Aware
Search (GAS) algorithm which exploits the PWLC structure
has been proposed to ï¬nd the optimal state-value function and
Lagrange penalty multiplier of a CMDP. We have applied the
proposed algorithm on two different problems, and compared
its performance with binary search, Lagrangian primal-dual
optimization, and linear programming. Compared with existing
algorithms, it has been shown that our proposed algorithm
converges to the optimal solution faster, does not require hyper-
parameter tuning, and is not sensitive to initialization of the
Lagrange penalty multiplier. In our future work, we will study
the extension of the proposed GAS algorithm to the model-free
reinforcement learning problem.

ACKNOWLEDGMENT

This work was supported in part by the NSF grants ECCS-
1554576 and ECCS-1610874. We gratefully acknowledge the
computing resources provided on Bebop, a high-performance
computing cluster operated by the Laboratory Computing
Resource Center at Argonne National Laboratory.

REFERENCES

[1] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.

MIT press, 2018.

[2] L. Kallenberg, â€œMarkov decision processes,â€ Lecture Notes. University

of Leiden, 2011.

[3] S. Mannor and N. Shimkin, â€œA geometric approach to multi-criterion
reinforcement learning,â€ Journal of machine learning research, vol. 5,
no. Apr, pp. 325â€“360, 2004.

[4] K. Van Moffaert and A. NowÃ©, â€œMulti-objective reinforcement learning
using sets of pareto dominating policies,â€ The Journal of Machine
Learning Research, vol. 15, no. 1, pp. 3483â€“3512, 2014.

[5] E. Altman, Constrained Markov decision processes. CRC Press, 1999,

vol. 7.

[6] D. Dolgov and E. Durfee, â€œStationary deterministic policies for con-
strained MDPs with multiple rewards, costs, and discount factors,â€ in
International Joint Conference on Artiï¬cial Intelligence, vol. 19, 2005,
p. 1326.

[7] A. Zadorojniy and A. Shwartz, â€œRobustness of policies in constrained
markov decision processes,â€ IEEE transactions on automatic control,
vol. 51, no. 4, pp. 635â€“638, 2006.

[8] A. B. Piunovskiy and X. Mao, â€œConstrained markovian decision
processes: the dynamic programming approach,â€ Operations research
letters, vol. 27, no. 3, pp. 119â€“126, 2000.

[9] R. C. Chen and G. L. Blankenship, â€œDynamic programming equations
for discounted constrained stochastic control,â€ IEEE transactions on
automatic control, vol. 49, no. 5, pp. 699â€“709, 2004.

[10] A. B. Piunovskiy, â€œDynamic programming in constrained markov decision
processes,â€ Control and Cybernetics, vol. 35, no. 3, p. 645, 2006.
[11] R. C. Chen and E. A. Feinberg, â€œNon-randomized policies for constrained
markov decision processes,â€ Mathematical Methods of Operations
Research, vol. 66, no. 1, pp. 165â€“179, 2007.

[12] P. Geibel and F. Wysotzki, â€œRisk-sensitive reinforcement learning applied
to control under constraints,â€ Journal of Artiï¬cial Intelligence Research,
vol. 24, pp. 81â€“108, 2005.

[13] P. Geibel and F. Wysotzki, â€œLearning algorithms for discounted MDPs
with constraints,â€ International Journal of Mathematics, Game Theory,
and Algebra, vol. 21, no. 2/3, p. 241, 2012.

[14] V. S. Borkar, â€œAn actor-critic algorithm for constrained markov decision
processes,â€ Systems & control letters, vol. 54, no. 3, pp. 207â€“213, 2005.
[15] F. V. Abad, V. Krishnamurthy, I. Baltcheva, and K. Martin, â€œSelf learning
control of constrained markov decision processesâ€“a gradient approach,â€
in IEEE Conference on Decision and Control, 2002.

[16] S. Bhatnagar, â€œAn actorâ€“critic algorithm with function approximation
for discounted cost constrained markov decision processes,â€ Systems &
Control Letters, vol. 59, no. 12, pp. 760â€“766, 2010.

[17] S. Bhatnagar and K. Lakshmanan, â€œAn online actorâ€“critic algorithm
with function approximation for constrained markov decision processes,â€
Journal of Optimization Theory and Applications, vol. 153, no. 3, pp.
688â€“708, 2012.

[18] C. Tessler, D. J. Mankowitz, and S. Mannor, â€œReward constrained policy

optimization,â€ arXiv preprint arXiv:1805.11074, 2018.

[19] Q. Liang, F. Que, and E. Modiano, â€œAccelerated primal-dual pol-
learning,â€ arXiv preprint

icy optimization for safe reinforcement
arXiv:1802.06480, 2018.

[20] M. Fu et al., â€œRisk-sensitive reinforcement learning: A constrained
optimization viewpoint,â€ arXiv preprint arXiv:1810.09126, 2018.
[21] J. Achiam, D. Held, A. Tamar, and P. Abbeel, â€œConstrained policy
optimization,â€ in Proceedings of the 34th International Conference on
Machine Learning-Volume 70.

JMLR. org, 2017, pp. 22â€“31.

[22] L. Prashanth and M. Ghavamzadeh, â€œActor-critic algorithms for risk-
sensitive reinforcement learning,â€ arXiv preprint arXiv:1403.6530, 2014.
[23] S. Paternain, M. Calvo-Fullana, L. F. Chamon, and A. Ribeiro, â€œSafe
policies for reinforcement learning via primal-dual methods,â€ arXiv
preprint arXiv:1911.09101, 2019.

[24] D. Ding, X. Wei, Z. Yang, Z. Wang, and M. R. JovanoviÂ´c, â€œProvably
efï¬cient safe exploration via primal-dual policy optimization,â€ arXiv
preprint arXiv:2003.00534, 2020.

[25] A. Ray, J. Achiam, and D. Amodei, â€œBenchmarking safe exploration in

deep reinforcement learning.â€

[26] S. Khairy, P. Balaprakash, L. X. Cai, and Y. Cheng, â€œConstrained deep
reinforcement learning for energy sustainable multi-uav based random
access iot networks with noma,â€ arXiv preprint arXiv:2002.00073, 2020.
[27] M. L. Littman, T. L. Dean, and L. P. Kaelbling, â€œOn the complexity
of solving markov decision problems,â€ arXiv preprint arXiv:1302.4971,
2013.

[28] Y. Chow, A. Tamar, S. Mannor, and M. Pavone, â€œRisk-sensitive and
robust decision-making: a cvar optimization approach,â€ in Advances in
Neural Information Processing Systems, 2015, pp. 1522â€“1530.

[29] Y. Chow, M. Ghavamzadeh, L. Janson, and M. Pavone, â€œRisk-constrained
reinforcement learning with percentile risk criteria,â€ The Journal of
Machine Learning Research, vol. 18, no. 1, pp. 6070â€“6120, 2017.
[30] Y. Liu, J. Ding, and X. Liu, â€œIPO: Interior-point policy optimization

under constraints,â€ arXiv preprint arXiv:1910.09615, 2019.

[31] M. Mozaffari, W. Saad, M. Bennis, and M. Debbah, â€œEfï¬cient deployment
of multiple unmanned aerial vehicles for optimal wireless coverage,â€
IEEE Communications Letters, vol. 20, no. 8, pp. 1647â€“1650, 2016.

[32] Y. Sun, D. Xu, D. W. K. Ng, L. Dai, and R. Schober, â€œOptimal
3D-trajectory design and resource allocation for solar-powered uav
communication systems,â€ IEEE Transactions on Communications, 2019.
[33] J.-M. Garcia, O. Brun, and D. Gauchard, â€œTransient analytical solution
of M/D/1/N queues,â€ Journal of applied probability, vol. 39, no. 4, pp.
853â€“864, 2002.

0102030405060Time (minutes)60080010001200Altitude (m)0102030405060Time (minutes)(38dBm,56o)(34dBm,28o)TxPower,Beamwidth0102030405060Time (minutes)0.00.20.40.60.81.0Normalized Battery Energy0102030405060Time (minutes)0.920.930.940.95Coverage Probability