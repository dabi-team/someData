A Gradient-Aware Search Algorithm for
Constrained Markov Decision Processes

Sami Khairy, Prasanna Balaprakash, Lin X. Cai

1

0
2
0
2

y
a
M
7

]

G
L
.
s
c
[

1
v
8
1
7
3
0
.
5
0
0
2
:
v
i
X
r
a

Abstract—The canonical solution methodology for ﬁnite con-
strained Markov decision processes (CMDPs), where the objective
is to maximize the expected inﬁnite-horizon discounted rewards
subject to the expected inﬁnite-horizon discounted costs con-
straints, is based on convex linear programming. In this brief,
we ﬁrst prove that the optimization objective in the dual linear
program of a ﬁnite CMDP is a piece-wise linear convex function
(PWLC) with respect to the Lagrange penalty multipliers. Next,
we propose a novel two-level Gradient-Aware Search (GAS)
algorithm which exploits the PWLC structure to ﬁnd the optimal
state-value function and Lagrange penalty multipliers of a ﬁnite
CMDP. The proposed algorithm is applied in two stochastic con-
trol problems with constraints: robot navigation in a grid world
and solar-powered unmanned aerial vehicle (UAV)-based wireless
network management. We empirically compare the convergence
performance of the proposed GAS algorithm with binary search
(BS), Lagrangian primal-dual optimization (PDO), and Linear
Programming (LP). Compared with benchmark algorithms, it is
shown that the proposed GAS algorithm converges to the optimal
solution faster, does not require hyper-parameter tuning, and is
not sensitive to initialization of the Lagrange penalty multiplier.

Index Terms—Constrained Markov Decision Process, Gradient
Aware Search, Lagrangian Primal-Dual Optimization, Piecewise
Linear Convex, Wireless Network Management

I. INTRODUCTION

M ARKOV decision processes (MDPs) are classical formal-

ization of sequential decision making in discrete-time
stochastic control processes [1]. In MDPs, the outcomes of
actions are uncertain, and inﬂuence not only immediate rewards,
but also future rewards through next states. Policies, which
are strategies for action selection, should therefore strike a
trade-off between immediate rewards and delayed rewards, and
be optimal in some sense. MDPs have gained recognition
in diverse ﬁelds such as operations research, economics,
engineering, wireless networks, artiﬁcial
intelligence, and
learning systems [2]. Moreover, an MDP is a mathematically
idealized form of the reinforcement learning problem, which is
an active research ﬁeld within the machine learning community.
In many situations however, ﬁnding the optimal policies with
respect to a single reward function does not sufﬁce to fully
describe sequential decision making in problems with multiple
conﬂicting objectives [3], [4]. The framework of constrained
MDPs (CMDPs) is the natural approach for handling multi-
objective decision making under uncertainty [5].

Sami Khairy and Lin X. Cai are with the Department of Electrical and
Computer Engineering, Illinois Institute of Technology, Chicago, IL 60616,
USA. E-mail: skhairy@hawk.iit.edu, lincai@iit.edu.

Prasanna Balaprakash is afﬁliated with the Mathematics and Computer
Science Division and Leadership Computing Facility, Argonne National
Laboratory, Lemont, IL 60439, USA. E-mail: pbalapra@anl.gov.

Algorithmic methods for solving CMDPs have been ex-
tensively studied when the underlying transition probability
function is known [5]–[11], and unknown [12]–[26]. In the case
of ﬁnite state-action spaces with known transition probability
function, the solution for a CMDP can be obtained by solving
ﬁnite linear programs [5]–[7], or by deriving a Bellman
optimality equation with respect to an augmented MDP whose
state vector consists of two parts: the ﬁrst part is the state
of the original MDP, while the second part keeps track of
the cumulative constraints cost [8]–[11]. Linear programming
methods become computationally impractical at a much smaller
number of states than dynamic programming, by an estimated
factor of about 100 [1]. In practice, MDP-speciﬁc algorithms,
which are dynamic programming based methods, hold more
promise for efﬁcient solution [27]. While MDP augmentation
based methods provide a theoretical framework to apply
dynamic programming to constrained stochastic control, they
introduce continuous variables to the state space which rules
out practical tabular methods and make the design of a solution
algorithm challenging [28].

On the other hand, solution methods for the constrained
reinforcement learning problem, i.e., CMDPs with unknown
transition probability, are generally based on Lagrangian primal-
dual type optimization. In these methods, gradient-ascent is
performed on state-values at a fast time scale to ﬁnd the optimal
value function for a given set of Lagrangian multipliers, while
gradient-descent is performed on the Lagrangian multipliers at
a slower time scale. This process is repeated until convergence
to a saddle point. Existing works have explored the primal-
dual optimization approach in the tabular setting, i.e., without
function approximation [12]–[15], and with function approx-
imators such as deep neural networks [16]–[26]. While this
approach is appealing in its simplicity, it suffers from the typical
problems of gradient optimization on non-smooth objectives,
sensitivity to the initialization of the Lagrange multipliers, and
convergence dependency on the learning rate sequence [19],
[21], [29], [30]. If the learning rate is too small, the Lagrange
multipliers will not update quickly to enforce the constraint;
and if it is too high, the algorithm may oscillate around the
optimal solution. In practice, a sequence of decreasing learning
rates should be adopted to guarantee convergence [14], yet
we do not have an obvious method to determine the optimal
sequence, nor we can assess the quality of a solution in cases
where the objective is not differentiable at the optimal solution.
In this brief, we develop a new approach to solve ﬁnite
CMDPs with discrete state-action space and known probabil-
ity transition function. We ﬁrst prove that the optimization
objective in the dual linear program of a ﬁnite CMDP is

 
 
 
 
 
 
a piece-wise linear convex function (PWLC) with respect
to the Lagrange penalty multipliers. Next, we treat the dual
linear program of a ﬁnite CMDP as a search problem over
the Lagrange penalty multipliers, and propose a novel two-
level Gradient-Aware Search (GAS) algorithm, which exploits
the PWLC structure to ﬁnd the optimal state-value function
and Lagrange penalty multipliers of a CMDP. We empirically
compare the convergence performance of the proposed GAS
algorithm with binary search (BS), Lagrangian primal-dual
optimization (PDO), and Linear Programming (LP), in two
application domains, robot navigation in grid world and wireless
network management. Compared with benchmark algorithms,
we show that the proposed GAS algorithm converges to the
optimal solution faster, does not require hyper-parameter tuning,
and is not sensitive to initialization of the Lagrange penalty
multiplier.

The remainder of this paper is organized as follows. A
background of unconstrained and constrained MDPs is given
in Section II. Our proposed Gradient-Aware Search (GAS)
algorithm is proposed in Section III. Performance evaluation
of GAS in two application domains is presented in Section
IV, followed by our concluding remarks and future work in
Section V.

II. BACKGROUND AND RELATED WORKS

A. Unconstrained Markov Decision Processes

An inﬁnite horizon Markov Decision Process (MDP) with
discounted-returns is deﬁned as a tuple (S, A, P, β, R, γ),
where S and A are ﬁnite sets of states and actions, respectively,
P : S × A × S → [0, 1] is the model’s state-action-state
transition probabilities, and β : S → [0, 1] is the initial
distribution over the states, R : S × A → R, is the reward
function which maps every state-action pair to the set of real
numbers R, and γ is the discount factor. Denote the transition
probability from state st = i to state st+1 = j if action
at = a is chosen by Pij(a) := P (st+1 = j|st = i, at = a).
The transition probability from state i to state j is therefore,
pij = P (st+1 = j|st = i) = (cid:80)
a Pij(a)π(a|i), where π(a|i)
is the adopted policy. The state-value function of state i under
policy π, Vπ(i), is the expected discounted return starting in
state i and following π thereafter,

t=1

(cid:80)

Vπ(i) = (cid:80)∞

j,a γt−1P π(st = j, at = a|s0 = i)R(j, a), ∀i ∈ S.
(1)
Let Vπ be the vector of state values, Vπ(i), ∀i. The solution
of an MDP is a Markov stationary policy π∗ which maximizes
the inner product (cid:104)Vπ, β(cid:105) = (cid:80)

i Vπ(i)β(i), i.e.,

max
π

∞
(cid:88)

(cid:88)

t=1

j,a

γt−1P π(st = j, at = a)R(j, a).

(2)

There exist several methods to solve (2), including linear
programming [2] and dynamic programming methods such
as value iteration and policy iteration [1]. Based on the linear
programming formulation, the optimal Vπ∗ can be obtained

2

by solving the following primal linear program [2],

min
V (i)

β(i)V (i),

(cid:88)

∀i

V (i) ≥ R(i, a) + γ

(cid:88)

Pij(a)V (j), ∀(i, a) ∈ S × A.

j

(3)
Linear program (3) has |S| variables and |S| × |A| constraints,
which becomes computationally impractical to solve for MDPs
with large state-action spaces. On the contrary, dynamic
programming is comparatively better suited to handling large
state spaces than linear programming, and is widely considered
the only feasible way of solving (2) for large MDPs [1].
Of particular interest is the value iteration algorithm which
generates an optimal policy in polynomial time for a ﬁxed γ
[27]. In the value iteration algorithm, the state-value function is
iteratively updated based on Bellman’s principle of optimality,

V k+1
π

(i) = max
a∈A(i)

[R(i, a) + γ

(cid:88)

j∈S

Pij(a)V k

π (j)], ∀i ∈ S. (4)

It has been shown that the non-linear map Vπ → Vπ in
(4) is a monotone contraction mapping that admits a unique
ﬁxed point, which is the optimal value function Vπ∗ [2]. By
obtaining the optimal value function, the optimal policy can be
derived using one-step look-ahead: the optimal action at state
i is the one that attains the equality in (4), with ties broken
arbitrarily.

B. Constrained Markov Decision Processes

In constrained MDPs (CMDPs), an additional immediate cost
function C : S × A → R is augmented, such that a CMDP is
deﬁned by the tuple (S, A, P, β, R, C, γ) [5]. The state-value
function is deﬁned as in (1). In addition, the inﬁnite-horizon
discounted-cost of a state i under policy π is deﬁned as,

t=1

(cid:80)

Cπ(i) = (cid:80)∞

j,a γt−1P π(St = j, At = a|So = i)C(j, a), ∀i ∈ S.
(5)
Let Cπ be the vector of state costs, Cπ(i), ∀i, and E ∈ R a
given constant which represents the constraint upper-bound.
The solution of a CMDP is a Markov stationary policy π∗
which maximizes (cid:104)Vπ, β(cid:105) subject to a constraint (cid:104)Cπ, β(cid:105) ≤ E,

max
π

∞
(cid:88)

(cid:88)

t=1
∞
(cid:88)

j,a

(cid:88)

t=1

j,a

γt−1P π(st = j, at = a)R(j, a),

γt−1P π(st = j, at = a)C(j, a) ≤ E.

(6)

The canonical solution methodology for (6) is based on
convex linear programming [5]. Of interest is the following
dual linear program1 which can be solved for the optimal

1The primal linear program is deﬁned over a convex set of occupancy
measures, and by the Lagrangian strong duality, (6) can be obtained. Interested
readers can ﬁnd more details about this formulation in Chapter 3 [5].

state-value function and Lagrange penalty multiplier µ,

min
V (i),µ≥0

(cid:88)

∀i

β(i)V (i) + µE,

V (i) ≥ R(i, a) − µC(i, a) + γ

(cid:88)

j

Pij(a)V (j),

∀(i, a) ∈ S × A.

(7)
Note that for a ﬁxed Lagrangian penalty multiplier µ, (7)
is analogous to (3), which can be solved using dynamic
programming and Bellman’s optimality equation [5],

[R(i, a) − µC(i, a) + γ (cid:80)

Vπ∗ (i, µ) = max
a∈A(i)

j∈S Pij(a)Vπ∗ (j, µ)], ∀i ∈ S.
(8)
This formulation has led to the development of multi time-
scale stochastic approximation based Lagrangian primal-dual
type learning algorithms to solve CMDPs with and without
value function approximation [12]–[26]. In such algorithms, µ
is updated by gradient descent at a slow time-scale, while the
state-value function is optimized on a faster time-scale using dy-
namic programming (model-based) or gradient-ascent (model-
free), until an optimal saddle point (Vπ∗ (µ∗), µ∗) is reached.
These approaches however suffer from the typical problems
of gradient optimization on non-smooth objectives, sensitivity
to the initialization of µ, and convergence dependency on the
learning rate sequence used to update µ [19], [21], [29], [30].
As we will show in the next section, the optimization objective
in (7) is piecewise linear convex with respect to µ, and so it
is not differentiable at the cusps. This means that we cannot
assess the optimality of a solution because µ∗ usually does
not have a zero gradient. This also means that convergence
to µ∗ is dependent on specifying an optimal learning rate
sequence, which is by itself a very challenging task. Unlike
existing methods, our proposed algorithm explicitly tackles
these problems by exploiting problem structure, and so it does
not require hyper-parameter tuning or specifying an optimal
learning rate sequence.

III. A NOVEL CMDP FRAMEWORK

In this section, we describe our novel methodology to
solve ﬁnite CMDPs with discrete state-action space and
known probability transition function. For ease of notation, we
hereby consider the case of one constraint, yet our proposed
algorithm can be readily extended to the multi-constraint case
by successively iterating the method described hereafter over
each Lagrange penalty multiplier, one at a time.

Our proposed method works in an iterative two-level
optimization scheme. For a given Lagrange penalty multiplier
µ, an unconstrained MDP with a penalized reward function
ˆR(i, a) = R(i, a) − µC(i, a), ∀i, a,
is speciﬁed, and we
require the corresponding optimal value function Vπ∗ (i, µ), ∀i
to be found using dynamic programming (8). Denote the
optimization objective in (7) as a function of µ by O(µ), i.e.,
O(µ) = (cid:80)
∀i β(i)Vπ∗ (i, µ) + µE. Thus to evaluate O(µ), one
has to solve for Vπ∗ (i, µ), ∀i ﬁrst. How to efﬁciently search
for the optimal µ∗? To answer this question, we ﬁrst prove
that O(µ) is a piecewise linear convex function with respect
to µ, and design an efﬁcient solution algorithm next.

3

1, · · · , bi

1, · · · , ai

m ∈ R, and bi

Deﬁnition 1: Let ai

m ∈ R for
some positive integer i ∈ Z+. A Piecewise Linear2 Convex
(cid:9).
function (PWLC) is given by fi(µ) = maxl=1,··· ,m
Theorem 1: Given that Vπ∗ (i, µ), ∀i is the optimal value
function of an unconstrained MDP with a penalized reward
function ˆR(i, a) = R(i, a) − µC(i, a), ∀i, a, β(i) is the
probability that the initial state is i, and E is a constant
representing the cost constraint upper bound in the original
CMDP, O(µ) = (cid:80)
∀i β(i)Vπ∗ (i, µ) + µE is a PWLC function
with respect to µ.

lµ + bi
l

(cid:8)ai

Proof: For ease of exposition, we introduce the following
three properties of PWLC functions, and show how they can
be used to construct the proof,

(cid:8)(ai

1) fi(µ) + fj(µ) is PWLC for any i, j ∈ Z+. Notice
that for point-wise maximum, maxl{al} + maxk{ak} =
maxl,k{al + ak}. Hence, fi(µ) + fj(µ) = maxl,k
l +
k)(cid:9), which is PWLC.
l + bj
aj
k)µ + (bi
l +fj(µ)(cid:9) is PWLC for any i, j ∈ Z+. No-
(cid:8)ai
lµ+bi
2) maxl
(cid:110)
= maxl{al}+maxk{ak}
al+maxk{ak}
tice that maxl
l + fj(µ)(cid:9)
(cid:8)ai
lµ + bi
= maxl,k{al + ak}. Hence, maxl
(cid:111)
(cid:8)(ai
kµ+bj
ai
lµ+bi
= maxl
l +
= maxl,k
k}
k)(cid:9), which is PWLC.
aj
l + bj
k)µ + (bi
3) fi(µ) + cµ is PWLC. Notice that fi(µ) + cµ =
(cid:9), which
(cid:8)(ai
(cid:9) + cµ = maxl

l +maxk

l + c)µ + bi
l

lµ + bi
l

(cid:8)aj

(cid:111)

(cid:110)

(cid:8)ai
maxl
is PWLC.

Based on these properties, the proof proceeds as follows. Given
Vπ∗ (i, µ), the recursive formula in (8) is expanded by iteratively
substituting for Vπ∗ (i, µ) with the right hand side of (8). This
expansion allows us to deduce that Vπ∗ (i, µ) has the following
functional form,

(cid:40)

Vπ∗ (i, µ) = maxl

lµ + bi
ai

l + γmaxk

(cid:110)

k µ + bi(cid:48)
ai(cid:48)

k + γmaxm

(cid:8)ai(cid:48)(cid:48)

m µ + bi(cid:48)(cid:48)

(cid:41)
m + · · · (cid:9)(cid:111)

,

l, bi

l, ai(cid:48)

k , bi(cid:48)
k , · · · ∈ R which come from
for some constants ai
discounted rewards, costs, and their product with the transition
probability function. Based on successive application of proper-
ties 1 and 2, Vπ∗ (i, µ), ∀i are PWLC functions with respect to
µ. Also, (cid:80)
∀i β(i)Vπ∗ (i, µ) is PWLC with respect to µ based
on property 1. Finally, O(µ) = (cid:80)
∀i β(i)Vπ∗ (i, µ) + µE is a
PWLC function with respect to µ based on property 3.

To efﬁciently ﬁnd µ∗ and solve (7), we propose a novel
Gradient-Aware Search (GAS) algorithm which exploits the
piecewise linear convex structure. Our proposed GAS algorithm,
which is outlined in Algorithm 1, operates over two-loops.
For a given Lagrange multiplier µ× (line 6) at an outer loop
iteration, the inner loop ﬁnds the optimal state-value function,
Vπ∗ (i, µ×), ∀i using value iteration, as well as the gradient
components of O(µ) with respect to µ× (lines 7-16). Based on
this information, the outer loop exploits the PWLC structure
to calculate the next µ× (line 17, lines 20-25). This process
continues until a termination criteria is met (lines 18-19). In
what follows, the proposed GAS algorithm is outlined in detail.
Inner loop: The value iterations of the inner loop are sub-
scripted with the index k. Let Q(i, a, µ), ∀i, a, denote the state-
action value function which is the expected discounted return

2Afﬁne is more accurate but less common.

Input

Algorithm 1: Gradient-Aware Search CMDP Solver
: R(i, a), C(i, a), Pij(a),
∀(i, a) ∈ S × A
: V ∗(i), ∀i ∈ S, µ∗

Output
Algorithm Parameters : (cid:15), (cid:15)(cid:48)

1 Initialize: µ+ = M , µ− = 0, O(µ+), O(µ−)

∂O

∂µ+ , ∂O

∂µ− , Omin = ∞

2 repeat
3

∆ = ∞, k = 0, Vk(i) = 0, ∀i ∈ S,
ω1
k(i, a) = 0, ∀(i, a) ∈ S × A
Qk(i, a) = 0, ∀(i, a) ∈ S × A
Find the intersection µ× of the two lines deﬁned by
(O(µ+), ∂O
repeat

∂µ+ ) and (O(µ−), ∂O
∂µ− )

Qk(i, a, µ×) = R(i, a) − µ×C(i, a)

+ γ (cid:80)

j∈S Pij(a)Vk(j), ∀(i, a) ∈ S × A
Qk(i, a, µ×), ∀i ∈ S

Vk+1(i, µ×) = max
a∈A(i)
Q(i, a, µ×), ∀i ∈ S

˜a(i) = argmax
a∈A(i)
k+1(i, a) = C(i, a) + ζ (cid:80)
ω1

j∈S Pij(a)ω1

k(j, ˜a(j)),
∀(i, a) ∈ S × A

|Vk+1(i,µ×)−Vk(i,µ×)|
|Vk(i,µ×)|

(cid:80)
i

∆ = 1
|S|
k = k + 1
until ∆ < (cid:15);
Calculate O(µ×), ∂O
if

|Omin − O(µ×)| ≤ (cid:15)(cid:48) then
Break

∂µ× based on (10) and (11)

if 0 ≤ ∂O

if Omin > O(µ×) then
Omin = O(µ×)
∂µ× < ∂O
µ+ = µ×, ∂O
∂O
∂µ− < ∂O
µ− = µ×, ∂O

∂µ+ then
∂µ+ = ∂O
∂µ×
∂µ× < 0 then
∂µ− = ∂O
∂µ×

if

25
26 until forever;
27 µ∗ = µ+
28 V ∗(i) = Vk+1(i), ∀i ∈ S

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

starting in state i, taking action a, and following π thereafter,
for a ﬁxed µ. Note that we drop the subscript π to avoid
notation clutter, but dependence on π remains unchanged. The
state-action value function at each step k can be represented
as a linear function of µ×,

Qk(i, a, µ×) = ω0

k(i, a) − ω1

k(i, a)µ×.

(9)

Qk(i, a, µ×) = ω0

k(i, a) (line 12). Given ω0

In this representation, accumulated discounted rewards con-
tribute to ω0
k(i, a) and accumulated discounted costs contribute
to ω1
k(i, a), ∀a, and µ×,
k(i, a), ω1
the value function at state i can be obtained by Vk(i, µ×) =
k(i, ˜a)µ× (line 10), where
k(i, ˜a) − ω1
max
a
˜a denotes the action which attains the maximum state-value
Qk(i, a, µ×)
among the set of actions at state i, ˜a(i) = argmax
(line 11). Notice that in the inner loop, two functions are
estimated using value iterations, the total state-action value
function (line 8), and the state-action cost function (line 12).

a

4

Dynamic programming steps in the inner loop are performed
until the mean relative absolute error in the estimation of the
state value function is less than a preset accuracy threshold
(cid:15) (line 14-16). When the inner loop concludes, the optimal
Vπ∗ (i, µ×), ω1

π∗ (i, ˜a), ∀i, are returned to the outer loop.

Outer loop: Given µ×, and Vπ∗ (i, µ×), ω1

π∗ (i, ˜a), ∀i from
the inner loop, O(µ×) and the gradient of O(µ×) with respect
to µ×, can be obtained based on (line 17),

O(µ×) =

(cid:88)

i

Vπ∗ (i, µ×)β(i) + µ×E,

(10)

and,

∂O
∂µ× =

∂
∂µ×

=

=

∂
∂µ×

∂
∂µ×
(cid:88)

= −

(cid:88)

i
(cid:88)

i
(cid:88)

Vπ∗ (i, µ×)β(i) + µ×E

max
a

Qπ∗ (i, a, µ×)β(i) + µ×E

(11)

(cid:0)ω0

k(i, ˜a) − ω1

k(i, ˜a)µ×(cid:1)β(i) + µ×E

i
ω1

π∗ (i, ˜a)β(i) + E.

i

i ω1

i ω1

When ∂O

∂µ× is negative, i.e., (cid:80)

π∗ (i, ˜a)β(j) > E, the
current policy π∗(µ×) is infeasible, which means µ× should
be increased. On the other hand, when ∂O
∂µ× is non-negative,
i.e., (cid:80)
π∗ (i, ˜a)β(j) ≤ E, the current policy is feasible, yet
we can potentially obtain another feasible policy with larger
returns by decreasing µ×. Notice that if µ× is a cusp which lies
at the intersection point of two linear segments of the PWLC
function O(µ), ∂O
∂µ× does not exist in theory, although the
two one-sided derivatives (left and right) are well-deﬁned. The
nonexistence of a derivative at cusps does not pose a challenge
to our proposed algorithm, because computations are accurate
up to the machine’s ﬂoating point precision. To further clarify,
suppose µ× is a cusp on the curve of O(µ). In practice, we
can only compute the gradient (11) at (µ× ± ˆ(cid:15)), where ˆ(cid:15) is
a very small precision loss due to rounding in ﬂoating point
arithmetic, which is at the order of 10−16 for double precision
as deﬁned by the IEEE 754-2008 standard. This means that
computing the gradient at a point is essentially computing a
one-sided derivative, which always exists.

It is worth to mention that our proposed algorithm does not
perform gradient optimization (which suffers on non-smooth
objectives) but rather exploits the structure of the PWLC
objective to ﬁnd the optimal µ∗. The algorithm always retains
two values for µ, {µ−, µ+}, where ∂O
∂µ+ ≥ 0.
µ− along with ∂O
∂µ− deﬁne a line L−, while µ+ along with
∂O
∂µ+ deﬁne another line L+. L− and L+ intersect at a point
µ×, which is passed to the inner loop. There are two possible
outcomes when running the inner loop with µ×,

∂µ− < 0 and ∂O

1) 0 ≤ ∂O

∂µ× < ∂O

∂µ+ , in this case O(µ×) < O(µ+) because
O(µ) is PWLC with respect to µ. The algorithm retains
µ× and ∂O
∂µ× by setting, µ+ = µ× and ∂O
∂µ+ = ∂O
∂µ×
(lines 22-23).
∂µ− < ∂O
∂O
∂µ× < 0, in this case O(µ×) < O(µ−) because
O(µ) is PWLC with respect to µ. The algorithm retains

2)

µ× and ∂O
(lines 24-25).

∂µ× by setting, µ− = µ× and ∂O

∂µ− = ∂O
∂µ×

The algorithm terminates when the absolute error between Omin
which is the minimum objective value found so far, and O(µ×),
is less than an arbitrarily small positive number (cid:15)(cid:48) (lines 18-19).
Note that the initial µ+ is set to M , an arbitrarily large number
which attains ∂O
∂µ+ ≥ 0. If no such M can be found, then (7) is
not feasible, i.e., O(µ) is not lower bounded and can be made
arbitrarily small O(µ) → −∞ by taking µ → ∞. Figure 1
below shows a visual illustration of how Algorithm 1 works in
practice. It is worth to mention that Algorithm 1 only evaluates
O(µ) at the points indicated by a green circle.

illustration of Algorithm 1. The proposed
Fig. 1: Visual
algorithm always retains two values for the Lagrange multiplier
µ, {µ−, µ+}, where ∂O
∂µ− < 0 and ∂O
∂µ+ ≥ 0. µ− along with
∂µ− deﬁne a line L−, while µ+ along with ∂O
∂O
∂µ+ deﬁne another
line L+. L− and L+ intersect at a point µ×, at which O(µ×)
is evaluated (green circle). Based on this evaluation, µ−, µ+
are updated and the process is repeated until the global minima
is reached. In this example, µ+

2 is the optimal µ∗.

3 = µ×

IV. CASE STUDIES

In this section, we evaluate the efﬁcacy of the proposed
GAS algorithm in two different domains, robot navigation in
a grid world, and solar-powered UAV-based wireless network
management. In all the experiments, (cid:15) and (cid:15)(cid:48) are set to 10−10
unless otherwise speciﬁed.

A. Grid World Robot Navigation

In the grid world robot navigation problem, the agent controls
the movement of a robot in a rectangular grid world, where
states represent grid points on a 2D terrain map. The robot starts
in a safe region in the bottom right corner (blue square), and
is required to travel to the goal located in the top right corner
(green square), as can be seen from Figure 3. At each time step,
the agent can move in either of four directions to one of the
neighbouring states. However, the movement direction of the
agent is stochastic and partially depends on the chosen direction.
Speciﬁcally, with probability 1 − δ, the robot will move in
the chosen direction, and uniformly randomly otherwise, i.e.,
with probability δ/4 the robot will move to one of the four

5

neighboring states. At each time step, the agent receives a
reward of −1 to account for fuel usage. Upon reaching the
goal, the agent receives a positive reward of ˆM >> 1. In
between the starting and destination states, there are a number
of obstacles (red squares) that the agent should avoid. Hitting
an obstacle costs the agent ˆM . It is important to note that the
2D terrain map is built such that a shorter path induces higher
risk of hitting obstacles. By maximizing long-term expected
discounted rewards subject to long-term expected discounted
costs, the agent ﬁnds the shortest path from the starting state
to the destination state such that the toll for hitting obstacles
does not exceed an upper bound. This problem is inspired by
classic grid world problems in the literature [18], [28].

1) Experiment Results: We choose a grid of 20 × 20 with a
total of 400 states. The start state is (2, 18), the destination is
(19, 18), ˆM = 2/(1−γ), δ = 0.05, γ = 0.99, and 30 obstacles
are deployed. We follow [28] in the choice of parameters, which
trades off high penalty for obstacle collisions and computational
complexity. O(µ) is plotted in Figure 2(a) over µ ∈ [0, 200].
It can be seen from Figure 2(a) that O(µ) is a PWLC function
as given by Theorem 1. It can be also seen that the global
minima is a non-differentiable cusp around 90 with unequal
one-sided derivatives.

In Figure 2(b), we compare the convergence performance
of the proposed GAS with binary search (BS), by plotting the
number of outer loop iterations versus an accuracy level (cid:15)(cid:48). The
initial search window for µ is [0, M ]3, and M is either 103 or
105. Given a value for µ, the inner loop evaluates (10) and its
gradient (11). The outer loop then determines the next µ either
based on the proposed GAS or BS. This iterative procedures
continues until the convergence criteria is met. Compared to
BS, the proposed GAS algorithm requires a smaller number
of outer loop iterations for all levels of accuracy thresholds (cid:15)(cid:48).
This is because GAS adaptively shrinks the search window by
evaluating points at the intersection of line segments with non-
negative and negative gradients, whereas BS blindly shrinks the
window size by 1
2 every iteration. These results demonstrate
that (cid:15)(cid:48) can be set arbitrarily small and M can be set arbitrarily
large without substantially impacting the convergence time.

In Figure 2(c) we compare the convergence performance
of the proposed GAS with the Lagrangian primal-dual opti-
mization (Lagrangian PDO), by plotting the total number of
value iterations, which is the sum of the number of value
iterations over all outer loop iterations, versus the learning
rate decay parameter in a log-log scale. In the Lagrangian
PDO, gradient descent on µ is performed along side dynamic
programming iterations on V (i), ∀i, which motivates us to
compare the convergence performance in terms of total number
of value iterations. In Lagrangian PDO, the update rule for µ
∂O
, where κk is the step size parameter.
is µk+1 = µk − κk
∂µk
In our experiments, κ0 is initially set
to 1 to speed up
descending towards the minima, and is decayed according to
κk+1 = κke−ξT , where ξ is the learning rate decay parameter,
and T is the number of times ∂O
changes signs. While such
∂µk

3M is problem speciﬁc. The ability to choose an arbitrarily high value
for M without negatively impacting convergence time is desirable because it
demonstrates the algorithm’s capability to make adaptively large strides in the
search space towards the optimal solution.

𝜇𝒪(𝜇)𝜇!"𝜇!#𝒪(𝜇!")𝒪(𝜇!#)𝒪(𝜇$#)𝜇$#=𝜇!×𝒪(𝜇&")𝒪(𝜇’#)𝜇’#=𝜇&×𝜇&"=𝜇$×choice of learning rate decay does not necessarily produce an
optimal sequence, it serves to expose the sensitivity of gradient
based optimization to the initial µ0 and decay parameter ξ.
For every ξ ∈ [10−4, 1], µ0 is uniformly randomly initialized
100 times from [0, M ], and the mean number of total value
iterations is plotted in Figure 2(c). It can be seen from Figure
2(c) that the convergence of the Lagrangian PDO method is
sensitive to the initialization of µ0 which can be inferred from
the initial window size, and the decay rate parameter. For some
ξ, Lagrangian PDO converges faster than GAS, but for other
values it lags behind. In practice, ξ is a hyper-parameter which
requires tuning, consuming extra computational resources. In
contrast, the proposed GAS algorithm works out of the box
without the need to specify a learning rate sequence or tune
any hyper-parameters.

In Table I, we compare the results obtained by the pro-
posed GAS algorithm at convergence with those obtained by
solving (7) using Linear Programming. We use the LP solver
provided by Gurobi, which is arguably the most powerful
mathematical optimization solver in the market. The opti-
mal µ∗, O(µ∗), mini|BE(i)|, Ei[|BE(i)|], and maxi|BE(i)|
are reported, where BE(i) is the Bellman error at state i
given by, BE(i) = Vπ∗ (i, µ∗) − max
[R(i, a) − µC(i, a) +
a∈A(i)
γ (cid:80)
j∈S Pij(a)Vπ∗ (j, µ∗)], ∀i ∈ S. It can be observed that
Gurobi’s LP solver converges to a sub-optimal solution with
a higher objective value compared with the proposed GAS
algorithm (recall that (7) is a minimization problem, hence
the lower O(µ) the better). This demonstrates that generic LP
solvers may struggle to solve CMDPs, which has motivated
the research community to develop CMDP speciﬁc algorithms.

TABLE I: Performance comparison with LP

Grid World
Robot Navigation

Solar-Powered UAV-
Based Wireless Network

µ∗
mini|BE(i)|
Ei[|BE(i)|]
maxi|BE(i)|
O(µ∗)

LP (Gurobi)
68.33333
0.0
9.89e-10
1.13e-07
15216.83

GAS
90.08102
3.11e-07
3.11e-07
3.11e-07
15184.57

LP (Gurobi)
0.030786
0.0
0.001788
1.160397
93.92830

GAS
0.030786
9.32e-09
9.32e-09
9.33e-09
93.92830

In Figure 3, ﬁlled contours maps for the value function are
plotted. Regions which have the same value have the same
color shade. The value function is plotted for four different
values of the cost constraint upper bound, along with the start
state (blue square), destination (green square), and obstacles
(red squares). With a tighter cost constraint (E = 5), a safer
policy is learned in which the agent takes a long path to go
around the obstacles as in Figure 3(d), successfully reaching
the destination P s
π∗ = 99.90% of the times. When the cost
constraint is loose (E = 160), a riskier policy is learned in
which the agent takes a shorter path by navigating between the
obstacles as in Figure 3(a), successfully reaching the destination
P s

π∗ = 76.85% of the times, based on 2000 roll outs.

B. Solar Powered UAV-Based Wireless Networks

As a second application domain, we study a wireless
network management problem, which is of interest to the

6

wireless networking research community. We consider a solar-
powered UAV-based wireless network consisting of one UAV
which relays data from network servers to N wireless devices.
Wireless devices are independently and uniformly deployed
within a circular geographical area A with radius Rc. The UAV
is deployed at the center of A, and is equipped with solar panels
which harvest solar energy to replenish its on-board battery
storage. Because solar light is attenuated through the cloud
cover, solar energy harvesting is highest at higher altitudes.
System time is slotted into ﬁxed-length discrete time units
indexed by t. During a time slot t, the solar-powered UAV
provides downlink wireless coverage to wireless devices on
the ground. The UAV is equipped with an intelligent controller
which at each time slot t, controls its altitude zt, its directional
antenna half power beamwidth angle θt
B, and its transmission
power P t
TX in dBm, based on its battery energy state and
altitude, to maximize the wireless coverage probability for the
worst case edge user, while ensuring energy sustainability of
the solar-powered UAV. Note that the worst case edge user
is the user which is farthest from the UAV. Due to distance
dependent free-space path loss, the received signal at the edge
user from the UAV is heavily attenuated, resulting in a low
Signal to Noise (SNR) ratio and decreased coverage probability.
Thus, by maximizing the coverage probability of the edge user,
the communication performance is improved for every user in
the network, albeit not proportionally. This wireless network
control problem can be modeled as follows,

1) Communication Model: during time slot t, the antenna

gain can be approximated by [31],

Gt

Antenna =

(cid:40) 29000
B )2 ,
(θt
g(φ),

θt
B

2 ≤ φ ≤ θt
otherwise,

B
2

(12)

(cid:17)η

(cid:16) 180

LoS = ζ

π ψt − 15

dB = 10log10(Gt

where θt
B is in degrees, φ is the sector angle, and g(φ) is
the antenna gain outside the main lobe. The antenna gain
in dB scale is Gt
Antenna). The air-to-ground
wireless channel can be modeled by considering Line-of-Sight
(LoS) and non-Line-of-Site (NLos) links between the UAV
and the users separately. The probability of occurrence of
LoS for an edge user during time slot t can be modeled by
P t
, where ζ and η are constant values
reﬂecting the environment impact, and ψt is the elevation
angle between the UAV and the edge user during time slot
t, ψt = tan−1(zt/Rc) [31]. The probability of an NLoS
link is P t
LoS. The shadow fading for the
LoS and NLoS links during time slot t can be modeled
by normal random variables ηt
LoS)2) and
LoS ∼ N (µLoS, (σt
N LoS)2), respectively, where the mean
N LoS ∼ N (µN LoS, (σt
ηt
and variance are in dB scale and depend on on the elevation an-
gle and environment parameters, σt
LoS(ψt) = k1exp(−k2ψt),
σt
N LoS(ψt) = g1exp(−g2ψt) [31]. Accordingly, the coverage
probability for the edge user during time slot t, deﬁned as the
probability the received SNR by an edge user is larger than a

N LoS = 1 − P t

7

(a) The piecewise linear convex function O(µ)

(b) Comparison with Binary Search (BS)

(c) Comparison with Lagrangian PDO

Fig. 2: Performance comparison on the grid world robot navigation problem

(a) P s

π∗ = 76.85%, E = 160

(b) P s

π∗ = 79.65%, E = 40

(c) P s

π∗ = 93.55%, E = 20

(d) P s

π∗ = 99.90%, E = 5

Fig. 3: Filled contour maps of the value function and the learned control policy for the grid world robot navigation problem.
With a lower upper bound on the cost constraint (E), the policy is more risk-averse and achieves a higher probability of success
for reaching the goal P s

π∗ . The shown path is one realization based on the learned policy.

threshold (SNRTh) is,
(cid:16) −P t

cov =P t
P t

LoSQ

T X − Gt

dB + µLoS + LdB + Pmin

(cid:17)

(cid:16) −P t

T X − Gt

σLoS
dB + µN LoS + LdB + Pmin

+ P t

N LoSQ

σN LoS
(13)
where Q(cid:0)x(cid:1) = 1 − P (X ≤ x)
for a standard
random variable X with mean 0 and variance
normal
1, Pmin = 10log10(N0SNRTh), N0 is the noise ﬂoor
power, LdB is the distance-dependent path-loss, LdB =
10αlog
, c is the speed of light, f0 is the
carrier frequency, and α is the path loss exponent.

(Rc)2+(zt)2

(cid:16) 4πf0

√

(cid:17)

c

2) UAV Energy Model: the attenuation of solar light passing
through a cloud is modeled by φ(dcloud) = e− ˆβcdcloud
, where
ˆβc ≥ 0 denotes the absorption coefﬁcient modeling the optical
characteristics of the cloud, and dcloud is the distance that the
solar light travels through the cloud [32]. The solar energy
harvesting model in time slot t is,

Et

solar =






zt+zt+1
2

τ ˆS ˆG∆t,
τ ˆS ˆGφ(zup − zn)∆t, zlow ≤ zt+zt+1
τ ˆS ˆGφ(zup − zlow)∆t, zt+zt+1

≥ zhigh

2
< zlow

2

< zhigh

(14)
where τ is a constant representing the energy harvesting
efﬁciency, ˆS is the area of solar panels, ˆG is the average solar
radiation intensity on earth, and ∆t is the time slot duration.
zhigh and zlow are the altitudes of upper and lower boundaries
of the cloud. Based on this model, the output solar energy
is highest above the cloud cover at zhigh, and it attenuates
exponentially through the cloud cover until zlow. During a
time-slot t, the UAV can cruise upwards or downwards at a

constant speed of vt
consumed for cruising or hovering during time slot t is,
√

z, or hover at the same altitude. The energy

(cid:17)

Et

UAV =

(cid:32)

W 2/(
√

(cid:33)

2ρA)

∆t

2Vh
z + Pstatic + P t

T X

+ (cid:0)W vt

vt
z =

zt+1 − zt
∆t

,

Vh =

(15)

(cid:1) ∆t,
(cid:115)

W
2ρA

Here, W is the weight of the UAV, ρ is air density, and A
is the total area of UAV rotor disks. Pstatic is static power
consumed for maintaining the operation of UAV. It is worth
to mention that cruising upwards consumes more power than
cruising downwards or hovering. Denote the battery energy
storage of the UAV at the beginning of slot t by Bt. The
battery energy of the next slot is given by,

Bt+1 = max(cid:8)0, min{Bt + Et

solar − Et

UAV, Bmax}(cid:9)

(16)

3) CMDP Formulation: this constrained control problem
of maximizing the wireless coverage probability for the worst
case edge user, while ensuring energy sustainability of the
solar-powered UAV can be modeled as a discrete-time CMDP
with discrete state-action spaces as follows. First, the altitude
of the UAV is discretized into Nz discrete units of zmax−zmin
.
Let the set of possible UAV altitudes be Z = {zmin, zmin +
zmax−zmin
, · · · }. In addition, the ﬁnite
Nz
battery energy of the UAV is discretized into Nb energy units,
where each energy unit is eu = Bmax×60×60
Joules. Let B ∈
{0, eu, · · · , (Nb − 1)eu} be the set of possible UAV battery
energy levels. Accordingly, the CMDP can be formulated as
follows,

, zmin + 2 zmax−zmin

Nz

Nz

Nb

050100150200Lagrange penalty ()1520015400156001580016000()109107105103101Accuracy threshold 01020304050Outer Loop IterationsBS [0,103]BS [0,106]GAS [0,103]GAS [0,106]104103102101100Learning Rate Decay Parameter 104Mean Total Value IterationsGAS [0,103]GAS [0,105]Lagrangian PDO [0,103]Lagrangian PDO [0,105]1290013650144001515015900166501740018150189001290013650144001515015900166501740018150189005000250002500500075001000012500150001750037500300002250015000750007500150001) The state of the agent is the battery energy level and
UAV altitude, ∀st ∈ S, st = (Bt, zt), where Bt ∈ B
and zt ∈ Z. Thus S = B × Z.

2) The agent controls the UAV vertical velocity vt

z ∈ Az,
the antenna transmission power P t
TX ∈ APT X , and the
half power beamwidth angle θt
B ∈ AθB . Thus, ∀at ∈ A,
A = Az × APT X × AθB , where Az = {−4, 0, 4} m/s,
APT X = {34, 38} dBm, and AθB = {28o, 56o}.

3) The immediate reward is the wireless coverage probabil-
ity for the edge user during time slot t, R(st, at) = P t
cov.
4) The immediate cost is the change in the battery level,

C(st, at) = Bt − Bt+1.

5) E = −∆B, where ∆B is the minimum desired battery

8

(a) Comparison with binary search

energy increase over the initial battery energy.

6) Model dynamics (cid:8)P (st+1|st, at), ∀s, a(cid:9): battery evolu-
tion of the UAV is modeled as an M/D/1/Nb queue.
Energy arrival is according to a poisson process with rate
λt = Et
. Energy arrivals which see a full battery are
solar
eu
rejected, and do not further inﬂuence the system. The
ﬁnite battery size acts as a regulator on the queue size.
Energy departure is deterministic and depends on the
action taken by the controller. Hence, energy departure
rate is µt = Et
. Utilization factor of the battery is
= Et
therefore ρt
UAV(at) . Based on poisson energy
Et
arrivals, the probability of k arrivals during a one unit
energy departure given action at is taken,

U AV (at)
eu
B = λt
µt

solar

P {k energy arrivals|at} =

(ρt

B

B)ke−ρt
k!

(17)

On the other hand, altitude state evolution is determinsitic
based on vt
z. Hence, the transition probability function
of the CMDP can be derived based on (17) and the
probability transition matrix of the embedded Markov
chain with action at for an M/D/1/Nb queue [33].
This wireless communication system exhibits a trade-off
between the altitude of the UAV and the coverage probability
for the edge user. When the UAV hovers at a higher altitude, it
can harvest more solar energy to replenish its on-board battery.
However, at higher altitudes, the wireless coverage probability
is worse due to signal attenuation. An optimal control policy
should be learned to maximize the coverage probability for the
edge user while ensuring the UAV’s battery is not depleted.

4) Experiment Results:

simulation parameters for this
experiment are outlined in Table II. Based on the chosen
discretization levels Nz and Nb, the CMDP has |S| = 3025
states and |A| = 12. In Figures 4(b) and 4(c), we compare the
convergence performance of the proposed GAS with BS and the
Lagrangian PDO approach, respectively. As previously noted
from Figures 2(b) and 2(c), it can be see that the proposed
algorithm compares favourably to BS and Lagrangian PDO,
despite the increased problem size. From Table I, we can
observe that both the proposed GAS and Gurobi’s LP solver
converge to the same µ∗, although GAS achieves a lower
Bellman error in the estimation of the value function.

(b) Comparison with Lagrangian approach

Fig. 4: Performance comparison on the solar powered UAV-
Based wireless network management problem

TABLE II: Simulation parameters for the solar powered UAV-
Based wireless network management problem

Parameter
Nz
α
Bmax
∆t
f0
g(φ)
n0
Nb
Nz
SNIRTh
ˆβc
zhigh, zlow
zmin, zmax
µLOS , µN LOS
γ

Value
121
2.5
100 Wh
10s
2 GHz
0
−100dBm
25
121
5
0.01

Parameter
Nb
τ
∆B
˜S
˜G
W
ρ
A
Pstatic
Rc
k1, k2

1.3, 0.7km ∆zmin, ∆zmax
0.5, 1.5km
1, 20dB
0.99

g1, g2
ζ
η

Value
25
0.4
1.67 Wh
1m2
1367W/m2
39.2kg ∗ m/s2
1.225kg/m3
0.18m2
5 watts
250 m
10.39, 0.05
−40m, 40m
29.06, 0.03
0.6
0.11

recharge its on-board battery, and then climbs down when the
battery is full to improve the coverage probability for the worst
case edge user, as can be seen from Figures 5(c) and 5(d). In
addition, Figure 5(b) shows the learned control policy for the
transmission power and half power beamwidth angle. When the
UAV is up to charge its battery, the lower transmission power
and smaller beamwidth angle are selected. On the other hand,
when the UAV is down to improve the coverage probability,
the higher transmission power and larger beamwidth angle
are selected. This is because with a smaller beamwidth angle,
the antenna gain is higher, and a lower transmission power is
required to counter the effects of large scale fading.

V. CONCLUSION

The learned policy by our proposed GAS algorithm is shown
in Figure 5. It can be seen from Figure 5(a) that the agent
learns an adaptive policy in which the UAV climbs up to

In this brief, we have proved that the optimization objective
in the dual linear programming formulation of a constrained
Markov Decision process (CMDP) is a piece-wise linear convex

109107105103101Accuracy threshold 01020304050Outer Loop IterationsBS [0,103]BS [0,106]GAS [0,103]GAS [0,106]104103102101100Learning Rate Decay Parameter 104105Mean Total Value IterationsGAS [0,103]GAS [0,105]Lagrangian PDO [0,103]Lagrangian PDO [0,105]9

(a) UAV altitude

(b) Tx power and beamwidth

(c) Battery energy evolution

(d) Edge user coverage probability

Fig. 5: Learned control policy for the solar powered UAV-Based wireless network management problem

function (PWLC) with respect to the Lagrange penalty multi-
plier. Based on this result, a novel two-level Gradient-Aware
Search (GAS) algorithm which exploits the PWLC structure
has been proposed to ﬁnd the optimal state-value function and
Lagrange penalty multiplier of a CMDP. We have applied the
proposed algorithm on two different problems, and compared
its performance with binary search, Lagrangian primal-dual
optimization, and linear programming. Compared with existing
algorithms, it has been shown that our proposed algorithm
converges to the optimal solution faster, does not require hyper-
parameter tuning, and is not sensitive to initialization of the
Lagrange penalty multiplier. In our future work, we will study
the extension of the proposed GAS algorithm to the model-free
reinforcement learning problem.

ACKNOWLEDGMENT

This work was supported in part by the NSF grants ECCS-
1554576 and ECCS-1610874. We gratefully acknowledge the
computing resources provided on Bebop, a high-performance
computing cluster operated by the Laboratory Computing
Resource Center at Argonne National Laboratory.

REFERENCES

[1] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.

MIT press, 2018.

[2] L. Kallenberg, “Markov decision processes,” Lecture Notes. University

of Leiden, 2011.

[3] S. Mannor and N. Shimkin, “A geometric approach to multi-criterion
reinforcement learning,” Journal of machine learning research, vol. 5,
no. Apr, pp. 325–360, 2004.

[4] K. Van Moffaert and A. Nowé, “Multi-objective reinforcement learning
using sets of pareto dominating policies,” The Journal of Machine
Learning Research, vol. 15, no. 1, pp. 3483–3512, 2014.

[5] E. Altman, Constrained Markov decision processes. CRC Press, 1999,

vol. 7.

[6] D. Dolgov and E. Durfee, “Stationary deterministic policies for con-
strained MDPs with multiple rewards, costs, and discount factors,” in
International Joint Conference on Artiﬁcial Intelligence, vol. 19, 2005,
p. 1326.

[7] A. Zadorojniy and A. Shwartz, “Robustness of policies in constrained
markov decision processes,” IEEE transactions on automatic control,
vol. 51, no. 4, pp. 635–638, 2006.

[8] A. B. Piunovskiy and X. Mao, “Constrained markovian decision
processes: the dynamic programming approach,” Operations research
letters, vol. 27, no. 3, pp. 119–126, 2000.

[9] R. C. Chen and G. L. Blankenship, “Dynamic programming equations
for discounted constrained stochastic control,” IEEE transactions on
automatic control, vol. 49, no. 5, pp. 699–709, 2004.

[10] A. B. Piunovskiy, “Dynamic programming in constrained markov decision
processes,” Control and Cybernetics, vol. 35, no. 3, p. 645, 2006.
[11] R. C. Chen and E. A. Feinberg, “Non-randomized policies for constrained
markov decision processes,” Mathematical Methods of Operations
Research, vol. 66, no. 1, pp. 165–179, 2007.

[12] P. Geibel and F. Wysotzki, “Risk-sensitive reinforcement learning applied
to control under constraints,” Journal of Artiﬁcial Intelligence Research,
vol. 24, pp. 81–108, 2005.

[13] P. Geibel and F. Wysotzki, “Learning algorithms for discounted MDPs
with constraints,” International Journal of Mathematics, Game Theory,
and Algebra, vol. 21, no. 2/3, p. 241, 2012.

[14] V. S. Borkar, “An actor-critic algorithm for constrained markov decision
processes,” Systems & control letters, vol. 54, no. 3, pp. 207–213, 2005.
[15] F. V. Abad, V. Krishnamurthy, I. Baltcheva, and K. Martin, “Self learning
control of constrained markov decision processes–a gradient approach,”
in IEEE Conference on Decision and Control, 2002.

[16] S. Bhatnagar, “An actor–critic algorithm with function approximation
for discounted cost constrained markov decision processes,” Systems &
Control Letters, vol. 59, no. 12, pp. 760–766, 2010.

[17] S. Bhatnagar and K. Lakshmanan, “An online actor–critic algorithm
with function approximation for constrained markov decision processes,”
Journal of Optimization Theory and Applications, vol. 153, no. 3, pp.
688–708, 2012.

[18] C. Tessler, D. J. Mankowitz, and S. Mannor, “Reward constrained policy

optimization,” arXiv preprint arXiv:1805.11074, 2018.

[19] Q. Liang, F. Que, and E. Modiano, “Accelerated primal-dual pol-
learning,” arXiv preprint

icy optimization for safe reinforcement
arXiv:1802.06480, 2018.

[20] M. Fu et al., “Risk-sensitive reinforcement learning: A constrained
optimization viewpoint,” arXiv preprint arXiv:1810.09126, 2018.
[21] J. Achiam, D. Held, A. Tamar, and P. Abbeel, “Constrained policy
optimization,” in Proceedings of the 34th International Conference on
Machine Learning-Volume 70.

JMLR. org, 2017, pp. 22–31.

[22] L. Prashanth and M. Ghavamzadeh, “Actor-critic algorithms for risk-
sensitive reinforcement learning,” arXiv preprint arXiv:1403.6530, 2014.
[23] S. Paternain, M. Calvo-Fullana, L. F. Chamon, and A. Ribeiro, “Safe
policies for reinforcement learning via primal-dual methods,” arXiv
preprint arXiv:1911.09101, 2019.

[24] D. Ding, X. Wei, Z. Yang, Z. Wang, and M. R. Jovanovi´c, “Provably
efﬁcient safe exploration via primal-dual policy optimization,” arXiv
preprint arXiv:2003.00534, 2020.

[25] A. Ray, J. Achiam, and D. Amodei, “Benchmarking safe exploration in

deep reinforcement learning.”

[26] S. Khairy, P. Balaprakash, L. X. Cai, and Y. Cheng, “Constrained deep
reinforcement learning for energy sustainable multi-uav based random
access iot networks with noma,” arXiv preprint arXiv:2002.00073, 2020.
[27] M. L. Littman, T. L. Dean, and L. P. Kaelbling, “On the complexity
of solving markov decision problems,” arXiv preprint arXiv:1302.4971,
2013.

[28] Y. Chow, A. Tamar, S. Mannor, and M. Pavone, “Risk-sensitive and
robust decision-making: a cvar optimization approach,” in Advances in
Neural Information Processing Systems, 2015, pp. 1522–1530.

[29] Y. Chow, M. Ghavamzadeh, L. Janson, and M. Pavone, “Risk-constrained
reinforcement learning with percentile risk criteria,” The Journal of
Machine Learning Research, vol. 18, no. 1, pp. 6070–6120, 2017.
[30] Y. Liu, J. Ding, and X. Liu, “IPO: Interior-point policy optimization

under constraints,” arXiv preprint arXiv:1910.09615, 2019.

[31] M. Mozaffari, W. Saad, M. Bennis, and M. Debbah, “Efﬁcient deployment
of multiple unmanned aerial vehicles for optimal wireless coverage,”
IEEE Communications Letters, vol. 20, no. 8, pp. 1647–1650, 2016.

[32] Y. Sun, D. Xu, D. W. K. Ng, L. Dai, and R. Schober, “Optimal
3D-trajectory design and resource allocation for solar-powered uav
communication systems,” IEEE Transactions on Communications, 2019.
[33] J.-M. Garcia, O. Brun, and D. Gauchard, “Transient analytical solution
of M/D/1/N queues,” Journal of applied probability, vol. 39, no. 4, pp.
853–864, 2002.

0102030405060Time (minutes)60080010001200Altitude (m)0102030405060Time (minutes)(38dBm,56o)(34dBm,28o)TxPower,Beamwidth0102030405060Time (minutes)0.00.20.40.60.81.0Normalized Battery Energy0102030405060Time (minutes)0.920.930.940.95Coverage Probability