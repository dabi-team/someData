2
2
0
2

n
a
J

5
2

]

G
L
.
s
c
[

1
v
2
2
2
0
1
.
1
0
2
2
:
v
i
X
r
a

EXPLANATORY LEARNING:
BEYOND EMPIRICISM IN NEURAL NETWORKS

Antonio Norelli
Sapienza University, CS dep.

Giorgio Mariani
Sapienza University, CS dep.

Luca Moschella†
Sapienza University, CS dep.

Andrea Santilli†
Sapienza University, CS dep.

Giambattista Parascandolo
OpenAI‡

Simone Melzi
Sapienza University, CS dep.

Emanuele Rodol`a
Sapienza University, CS dep.

ABSTRACT

We introduce Explanatory Learning (EL), a framework to let machines use ex-
isting knowledge buried in symbolic sequences – e.g. explanations written in
hieroglyphic – by autonomously learning to interpret them. In EL, the burden
of interpreting symbols is not left to humans or rigid human-coded compilers, as
done in Program Synthesis. Rather, EL calls for a learned interpreter, built upon a
limited collection of symbolic sequences paired with observations of several phe-
nomena. This interpreter can be used to make predictions on a novel phenomenon
given its explanation, and even to ﬁnd that explanation using only a handful of
observations, like human scientists do. We formulate the EL problem as a sim-
ple binary classiﬁcation task, so that common end-to-end approaches aligned with
the dominant empiricist view of machine learning could, in principle, solve it.
To these models, we oppose Critical Rationalist Networks (CRNs), which instead
embrace a rationalist view on the acquisition of knowledge. CRNs express sev-
eral desired properties by construction, they are truly explainable, can adjust their
processing at test-time for harder inferences, and can offer strong conﬁdence guar-
antees on their predictions. As a ﬁnal contribution, we introduce Odeen, a basic
EL environment that simulates a small ﬂatland-style universe full of phenomena
to explain. Using Odeen as a testbed, we show how CRNs outperform empiricist
end-to-end approaches of similar size and architecture (Transformers) in discov-
ering explanations for novel phenomena.

Explanation1:

Explanation:

Figure 1: The Odeen universe. A convenient environment to study and test the process of knowl-
edge discovery in machines. Like the night sky was for humans.

*Galileo did not sketch negative examples.

†Equal contribution ‡This work was done while GP was at the MPI for Intelligent Systems and ETH Z¨urich.
Correspondence to norelli@di.uniroma1.it

1

 
 
 
 
 
 
Explanatory Learning: Beyond Empiricism in Neural Networks

1

INTRODUCTION

Making accurate predictions about the future is a key ability to survive and thrive in a habitat. Living
beings have evolved many systems to this end, such as memory (McConnell, 1962), and several can
predict the course of complex phenomena (Taylor et al., 2012). However, no animal comes even
close to the prediction ability of humans, which stems from a unique-in-nature system.

At the core of this system lies an object called explanation, formed by the proposition of a language,
which has a remarkable property: it can be installed with ease into another human speaking the same
language, allowing to make predictions on new phenomena without ever having experienced them.
When the installation is successful, we say that the human has understood the explanation.

This process is key to the success of human beings. An individual can provide accurate predictions
for a multitude of phenomena without going through a painful discovery process for all of them,
but only needs an operating system – mastering a language – and someone who communicates
the relevant explanations; this way, the individual can focus on unexplained phenomena. When an
explanation is found for them, it is added to the existing shared collection, which we call knowledge.

How can we make machines take part in this orchestra? With this work, we try to shed new light
on this problem. Speciﬁcally, we propose a learning procedure to allow machines (i) to understand
existing explanations, in the sense described above, and (ii) create new explanations for unexplained
phenomena, much like human scientists do.

Our contribution in this sense is threefold:

i) We formulate the challenge of creating a machine that masters a language as the problem of learn-
ing an interpreter from a collection of examples in the form (explanation, observations). The only
assumption we make is this dual structure of data; explanations are free strings, and are not required
to ﬁt any formal grammar. This results in the Explanatory Learning (EL) framework described in
Sec. 2.

ii) We present Odeen, a basic environment to test EL approaches, which draws inspiration from the
board game Zendo (Heath, 2001). Odeen simulates the work of a scientist in a small universe of
simple geometric ﬁgures, see Figure 1. We present it in Sec. 3, and release it with this work2.

iii) We argue that the dominating empiricist ML approaches are not suitable for EL problems. We
propose Critical Rationalist Networks (CRNs), a family of models designed according to the epis-
temological philosophy pushed forward by Popper (1935). Although a CRN is implemented using
two neural networks, the working hypothesis of such a model does not coincide with the adjustable
network parameters, but rather with a language proposition that can only be accepted or refused in
toto. We will present CRNs in Sec. 4, and test their performance on Odeen in Sec. 5.

2 EXPLANATORY LEARNING

Humans do not master a language from birth. A baby can not use the message “this soap stings”
to predict the burning sensation caused by contact with the substance. Instead, the baby gradually
learns to interpret such messages and make predictions for an entire universe of phenomena (Schulz
et al., 2007). We refer to this state of affairs as mastering a language, and we aim to replicate it in a
machine as the result of an analogous learning process.

Using a batch of explanations paired with observations of several phenomena, we want to learn an
interpreter to make predictions about novel phenomena for which we are given explanations in the
same language. Going a step further, we also want to discover these explanations, when all we have
is a handful of observations of the novel phenomena. We ﬁrst describe the problem setup in the
sequel, comparing it to existing ML problems; then we detail our approach in Sec. 4.

1The explanation Four wandering stars having their period around a principal star is adapted from the
English translation of the Sidereus Nuncius (Galilei, 2016, page 9). First sketch on the left is compatible with
the rule since the fourth moon can be hidden by one of the other moons or by Jupyter itself.

2The Odeen dataset and all the code useful to reproduce the results discussed in this work can be found at

https://github.com/gladia-research-group/explanatory-learning

2

Explanatory Learning: Beyond Empiricism in Neural Networks

Problem setup. Formally, let phenomena P1, P2, P3, . . . be subsets of a universe U , which is a
large set with no special structure (i.e., all the possible observations U = {x1, . . . , xz}). Over a
universe U , one can deﬁne a language L as a pair (ΣL, IL), where ΣL is a ﬁnite collection of short
strings over some alphabet A, with |ΣL| (cid:29) |A|, and IL is a binary function IL : U × ΣL → {0, 1},
which we call interpreter. We say that a phenomenon Pi is explainable in a language L if there Explainability
exists a string e ∈ ΣL such that, for any x ∈ U , it occurs IL(x, e) = 1Pi(x), where 1Pi(x) is the
indicator function of Pi. We call the string e an explanation, in the language L, for the phenomenon
Pi.

deﬁnition

Our ﬁrst contribution is the introduction of a new class of machine learning problems, which we
refer to as Explanatory Learning (EL).

Consider the general problem of making a new prediction for a phenomenon P0 ⊂ U . In our setting,
this is phrased as a binary classiﬁcation task: given a sample x(cid:48) ∈ U , establish whether x(cid:48) ∈ P0 or
not. We are interested in two instances of this problem, with different underlying assumptions:

• The communication problem: we have an explanation. We are given an explana-
tion e0 for P0, in an unknown language L. This means that we do not have access to
an interpreter IL; e0 looks like Japanese to a non-Japanese speaker.
Instead, we are
also given other explanations {e1, . . . , en}, in the same language, for other phenomena
P1, . . . , Pn, as well as observations of them, i.e., datasets {D1, . . . , Dn} in the form
Di = {(x1, 1Pi(x1)), . . . , (xm, 1Pi(xm))}, with m (cid:28) |U |. Intuitively, here we expect
the learner to use the explanations paired with the observations to build an approximated
interpreter ˆIL, and then use it to make the proper prediction for x(cid:48) by evaluating ˆIL(x(cid:48), e0).
• The scientist problem: we do not have an explanation. We are given explanations
{e1, . . . , en} in an unknown language L for other phenomena P1, . . . , Pn and observa-
tions of them {D1, . . . , Dn}. However, we do not have an explanation for P0; instead, we
are given just a small set of observations D0 = {(x1, 1P0(x1)), . . . , (xk, 1P0(xk))} and
two guarantees, namely that P0 is explainable in L, and that D0 is representative for P0 in Representativity
L. That is, for every phenomenon P (cid:54)= P0 explainable in L there should exist at least a
xi ∈ D0 such that 1P0 (xi) (cid:54)= 1P (xi). Again, we expect the learner to build the interpreter
ˆIL, which should ﬁrst guide the search for the missing explanation e0 based on the clues
D0, and then provide the ﬁnal prediction through ˆIL(x(cid:48), e0).

deﬁnition

Several existing works fall within the formalization above. The seminal work of Angluin (1987) on
learning regular sets is an instance of the scientist problem, where ﬁnite automata take the role of
explanations, while regular sets are the phenomena. More recently, CLEVR (Johnson et al., 2017)
posed a communication problem in a universe of images of simple solids, where explanations are
textual and read like “There is a sphere with the same size as the metal cube”. Another recent ex-
ample is CLIP (Radford et al., 2021), where 400,000,000 captioned internet images are arranged in
a communication problem to train an interpreter, thereby elevating captions to the status of explana-
tions rather than treating them as simple labels3. With EL, we aim to offer a uniﬁed perspective on
these works, making explicit the core problem of learning an interpreter purely from observations.

Relationship with other ML problems. We brieﬂy discuss the relationship between EL and other
problems in ML, pointing to Sec. 6 for additional discussion on the related work.

EL can be framed in the general meta-learning framework. The learner gains experience over mul-
tiple tasks to improve its general learning algorithm, thus requiring fewer data and less computation
on new tasks. However, differently from current meta-learning approaches (Hospedales et al., 2020),
we are not optimizing for any meta-objective. Instead, we expect the sought generality to be a conse-
quence of implicitly deﬁning an interpreter through a limited set of examples rather than an explicit
goal to optimize for.

To many, the concept of explanation may sound close to the concept of program; similarly, the sci-
entist problem may seem a rephrasing of the fundamental problem of Inductive Logic Programming
(ILP) (Shapiro, 1981) or Program Synthesis (PS) (Balog et al., 2017). This is not the case. ILP
has the analogous goal of producing a hypothesis from positive/negative examples accompanied by

3This shift greatly improved the performance of their model, as discussed in (Radford et al., 2021, Sec. 2.3).

3

Explanatory Learning: Beyond Empiricism in Neural Networks

background knowledge. Yet, ILP requires observations to be expressed as logic formulas, a task
requiring a human; only then the ILP solver outputs an explanation in the form of a logic proposi-
tion, which in turn is interpreted by a human expert. With EL, data can be fed as-is without being
translated into logic propositions, and a learned interpreter plays the expert’s role. PS also admits
raw data as input, it yields a program as output, and replaces the expert with a handcrafted inter-
preter; still, the sequence of symbols produced by a PS system only makes sense to a human (who
designed the interpreter), not to the system itself. Instead, in EL, the interpreter is learned from data
rather than hardcoded. An empirical comparison demonstrating the beneﬁts of EL over PS is given
in Sec. 5.

Next we introduce Odeen, an environment and benchmark to experiment with the EL paradigm.

3 ODEEN: A PUZZLE GAME AS EXPLANATORY LEARNING ENVIRONMENT

Single game. The inset shows a typical situation
in a game of Odeen. The players look at a set of
structures made of simple geometric ﬁgures. Each
structure is tagged red or green according to a secret
rule, and the players’ goal is to guess this rule. In the
example, the rule can not possibly be “A structure
must contain at least one red square” since the ﬁfth
structure on the left does not contain a red square, but respects the rule (green tag). To win the game,
a player must prove to know the rule by correctly tagging a large set of new structures4. We made a
simpliﬁed interactive version of Odeen available at https://huggingface.co/spaces/gladia/odeen.

Odeen challenge. We can see each game of Odeen as a different phenomenon of a universe, where
each element is a sequence of geometric ﬁgures. In this universe, players are scientists like Galileo,
trying to explain the new phenomenon; see Figure 1. We can phrase the challenge for an Odeen
scientist in this way: make correct predictions for a new phenomenon given few observations of it
in addition to explanations and observations of some other phenomena. This is the essence of the
Odeen Explanatory Learning problem, see Figure 2 (A and B).

- Why do we need explanations and observations from phenomena different from the one of interest?
Indeed, we are able to play Odeen from the very ﬁrst game.

- We are able to do so only because we are -already- ﬂuent in the Odeen language, which is a subset
of English in the above case. We already have and understand all necessary concepts, such as being
“at the right of” something, but also being a “square” or “at least”. Otherwise, we would need
past explanations and observations to ﬁrst build this understanding. Before explaining the dynamic
of the Jupiter moons, Galileo learned what “Jupiter” is and what does it mean to “have a period
around” something from past explanations and examples provided to him by books and teachers.

In Odeen, consider the point of view of someone who
does not speak the language in which the rules are writ-
ten; an example of this is in the inset, where the secret
explanations are given in hieroglyphics rather than En-
glish. Such a player would not be able to tag any struc-
ture according to the secret rule, even if the latter is
given. However, assume the player has been watching
several games together with their secret rules. Reason-
ably, the player will grow an idea of what those strange
symbols mean. If the player then wins several Odeen
games, it would be strong evidence of mastering the
Odeen language.

4The solution of the inset game is at the end of this footnote. Odeen is inspired by the board game Zendo,
where players must explicitly guess the rule, known only to a master. In Zendo, players can also experiment by
submitting new structures to the master. Solution: At least one square at the right of a red pyramid.

4

Explanatory Learning: Beyond Empiricism in Neural Networks

Figure 2: Odeen Explanatory Learning problem. Given observations and explanations in an
unknown language for some phenomena (A), plus a few observations of a new phenomenon, explain
the latter and prove this knowledge by correctly tagging a large set of new samples (B). An empiricist
approach attempts to extract this knowledge from data (C, left); a rationalist one conceives data as
theory-laden observations, used to ﬁnd the true explanation among a set of conjectures (C, right).

Problem formulation. Each game of Odeen is a different phenomenon Pi of a universe U whose
elements x are sequences of geometric ﬁgures. The speciﬁc task is to make correct predictions for
a new phenomenon P0 (a new game) given: (i) a few observations D0 of P0 (tagged structures), in
conjunction with (ii) explanations {e1, . . . , en} and observations {D1, . . . , Dn} of other phenomena
(other games and their secret rules). More formally:

Let us be given s unexplained phenomena with k observations each, and n explained phenom-
ena with m observations each; let the n phenomena be explained in an unknown language, i.e.,
e1, . . . en are plain strings without any interpreter. The task is to make (cid:96) correct predictions for
each of the s unexplained phenomena.

We consider (cid:96) = 1176 (1% of structures); s = 1132; k = 32; m = 10K, 1K, 100; n = 1438 or 500.

Why not explicitly ask for the rule? Instead of requiring the player to reveal the secret explanation
explicitly, we follow the principle of zero-knowledge proofs (Blum et al., 1988). In our setting, this
is done by asking the player to correctly tag many unseen structures according to the discovered
rule. This makes it possible for any binary classiﬁcation method to ﬁt our EL environment without
generating text. A winning condition is then deﬁned by counting the correct predictions, instead of
a textual similarity between predicted and correct explanation, which would require the player to
guess word-by-word the secret rule. In fact, different phrasings with the same meaning should grant
a victory, e.g., “at least one pyramid pointing up and at most one pyramid pointing up” is a winning
guess for the secret rule “exactly one pyramid pointing up”5. A brute-force enumeration of all
equivalent phrasings, in turn, would not allow solutions like “exactly one one pyramid pointing up”,
where “one” is mistakenly repeated twice; intuitively, we want to accept this as correct and dismiss
the grammatical error. Similarly, a solution like “exactly one pointing up”, where “pyramid” is
omitted, should be accepted in a universe where only pyramids point up. We will reencounter these
examples in Sec. 5 when we discuss the key properties of our approach.

Dataset generation. Odeen structures are sequences of six elements including spaces, blues or reds,
squares or pyramids, the latter pointing up or down. The size of the universe is |U | = 76 = 117, 649
possible structures. We further created a small language with objects, attributes, quantiﬁers, logical
conjunctions, and interactions (e.g., “touching”, see Appendix A). The grammar generates ≈25k
valid rules in total. Each of the |U | structures is tagged according to all the rules. The tagging is
done by an interpreter implemented via regular expressions.

5The intuitive notion of meaning adopted here coincides with the pragmatic deﬁnition given by Peirce (1878,
Sec. II), which identiﬁes the meaning of an expression with the set of all conceivable practical consequences
that derive from its acceptance. We refer the reader to Kant and the Platypus for a readable discussion of this
view (Eco, 2000, Sec. 3.3), involving the ﬁrst description of horses given by Aztecs.

5

Explanatory Learning: Beyond Empiricism in Neural Networks

the task is to tag (cid:96) new structures for each of s unex-
Metrics. As described above,
plained games. An EL algorithm addressing this task encodes the predicted rule as an
(cid:96)-dimensional binary vector v per game (predicted vector), where vi = 1 means that
the i-th structure satisﬁes the predicted rule, and vi = 0 oth-
erwise (see inset). Let w∗ be the ground-truth vector, ob-
tained by tagging the (cid:96) structures according to the correct se-
cret rule. Then, the Hamming distance dH (v, w∗) measures
the number of wrong tags assigned by the EL algorithm; if
dH (v, w∗) < dH (v, wi), where wi (cid:54)= w∗ ranges over all the
possible ≈25k rules, then the predicted rule v made by the
algorithm is deemed correct.

Predicted vector v

According to this, the Nearest Rule Score (NRS) is the number of correctly predicted rules over
a total of s games. A second score, the Tagging Accuracy (T-Acc), directly counts the number of
correct tags averaged over s games; this is more permissive in the following sense. Consider two
different rules A and B sharing 99% of the taggings, and let A be the correct one; if an EL model
tags all the structures according to the wrong rule B, it still reaches a T-Acc of 99%, but the NRS
would be 0. An EL algorithm with these scores would be good at making predictions, but would be
based on a wrong explanation.

4 CRITICAL RATIONALIST NETWORKS

In principle, an EL problem like Odeen can be approached by training an end-to-end neural network
to predict ˆy = 1Pi(x(cid:48)), given as input a set of observations Di and a single sample x(cid:48) (see Figure 2
C, left). Such a model would assume that all the information needed to solve the task is embedded
in the data, ignoring the explanations; we may call it a “radical empiricist” approach (Pearl, 2021).
A variant that includes the explanations in the pipeline can be done by adding a textual head to the
network. This way, we expect performance to improve because predicting the explanation string can
aid the classiﬁcation task. As we show in the experiments, the latter approach (called “conscious
empiricist”) indeed improves upon the former; yet, it treats the explanations as mere data, nothing
more than mute strings to match, in a Chinese room fashion (Searle, 1980; Bender & Koller, 2020).

In the following, we introduce a “rationalist” approach to solve EL problems. This approach recog-
nizes the given explanations as existing knowledge, and focuses on interpreting them. Here theory
comes ﬁrst, while the data become theory-laden observations.

Learning model. Our Critical Rationalist Networks (CRNs) tackle the EL scientist problem intro-
duced in Sec. 2: to ﬁnd y = 1P0(x(cid:48)) given x(cid:48), D0, {D1, . . . , Dn}, {e1, . . . , en}. They are formed
by two independently trained models:

(i) A stochastic Conjecture Generator

CG : {(x, 1P (x))j}k

j=1 (cid:55)→ e ,

taking k ≤ |D0| pairs (x, 1P (x)) ∈ Di as input, and returning an explanation string e ∈ Σ as
output. CG is trained to maximize the probability that CG( ˜Di) = ei for all i = 1, . . . , n, where
˜Di ⊂ Di is a random sampling of Di, and | ˜Di| = k.
(ii) A learned Interpreter

I : (e, x) (cid:55)→ ˆy ,
which takes as input a string e ∈ Σ and a sample x ∈ U , to output a prediction ˆy ∈ {0, 1}. I is
trained to maximize the probability that I(ei, x) = 1Pi(x), with i = 1, . . . , n and (x, 1Pi(x)) ∈ Di.
At test time, we are given a trained CG and a trained I, and we must predict whether some x(cid:48) /∈ D0
belongs to P0 or not. The idea is to ﬁrst generate t conjectures by applying CG t times to the dataset
D0; then, each conjecture is veriﬁed by counting how many times the interpreter I outputs a correct
prediction over D0. The conjecture with the highest hit rate is our candidate explanation ˆe0 for P0.
Finally, we obtain the prediction ˆy(cid:48) as I(ˆe0, x(cid:48)). See Figure 3 (left) for a step-by-step pseudo code.

Remarks. The interpreter I is a crucial component of our approach. A poor I may fail to identify
e0 among the generated conjectures, or yield a wrong prediction y(cid:48) when given the correct e0. On

6

Explanatory Learning: Beyond Empiricism in Neural Networks

I

CG

EMP-R

Figure 3: Left: Test-time algorithm of CRNs. Right: CRNs are implemented using encoder-
decoder transformers blocks, details of the parameters in Appendix B. Right-top: I denotes the
interpreter model (rule encoder and label decoder). Right-bottom: The conjecture generator CG is
composed by blue blocks. The “radical empiricist” (EMP-R) is composed by orange blocks. The
“conscious empiricist” (EMP-C) baseline model consists of all the transformer blocks in the right-
bottom ﬁgure, board encoder with rule and label decoders (all the blue and orange blocks).

the other hand, we can work with a CG of any quality and safely return as output an unknown token,
rather than a wrong prediction, whenever e0 does not appear among the generated conjectures. The
role of CG is to trade-off performance for computational cost, and is controlled by the parameter t.
Larger values for t imply more generated conjectures, corresponding to exhaustive search if taken
to the limit (as done, e.g., in Radford et al. (2021)). This potential asymmetry in quality between CG
and I is tolerated, since the learning problem solved by CG is generally harder.

Secondly, although a CRN is implemented using neural networks, as we shall see shortly, its working
hypothesis does not coincide with a snapshot of the countless network’s parameters; rather, the
working hypothesis is but the small conjecture analyzed at a given moment. This way, the CRN
hypothesis is detached from the model and can only be accepted or refused in its entirety, rather
than being slightly adjusted at each new data sample (Figure 2 C, the hypotheses are in orange).

Implementation. Figure 3 (right) illustrates the architecture of CRNs, which we implement using
encoder-decoder transformers (Vaswani et al., 2017). The ﬁgure also shows the architecture of the
baseline methods EMP-R and EMP-C, corresponding to the end-to-end NN model and its variant
with a textual head, respectively. We refer to the Appendix for further details.

5 EXPERIMENTS

We extensively compared CRNs to the radical (EMP-R) and conscious (EMP-C) empiricist models
over the Odeen challenge, and analyzed several fundamental aspects.

Generalization power. The Odeen challenge directly addresses the generalization capability of
a given algorithm, by asking for explanations to unexplained phenomena. This is evaluated over
s = 1132 new games, where each game is given with k = 32 tagged structures (guaranteed to satisfy
a unique, yet unknown rule) and requires to correctly tag (cid:96) = 1176 unseen structures according
to the unknown rule. The training set are n = 1438 games with ground-truth explanations and
m = 1000 tagged structures per game. The test set does not include any rule equivalent to the
training rules. One important example is the bigram “exactly two”, which appears in the test set,
but was deliberately excluded from training; the training rules only contain “at least/most two” and
“exactly one”. The CRN guessed 40% of the 72 test rules with “exactly two”, while the empiricist
models (EMP-C, EMP-R) scored 4% and 0% respectively. The table below reports the full results.

7

Explanatory Learning: Beyond Empiricism in Neural Networks

The NRS of 77.7% denotes that the CRN discovered the
correct explanation for 880 out of 1132 new phenomena.
Using the same data and a similar number of learnable pa-
rameters, the empiricist models score 22.5% at most. Some
example games can be found in Appendix D.

MODEL

NRS

T-ACC

R-ACC

CRN
EMP-C
EMP-R

0.777
0.225
0.156

0.980
0.905
0.898

0.737
0.035
-

In the table, R-Acc measures how frequently an output explanation is equivalent to the correct one;
two rules A and B are equivalent if the tags assigned by the hard-coded interpreter to all the ∼117k
structures in U are the same for A and B.

As expected, the explanation predicted by the conscious empiricist model is rarely correct (R-Acc
3.5%), even when it tags some structures properly (NRS 22.5%); indeed, EMP-C gives no guar-
antee for the predicted explanation to be consistent with the tags prediction. Conversely, the CRN
consistently provides the correct explanation when it is able to properly tag the new structures (NRS
77.7%, R-Acc 73.7%). The 4% gap between the two scores is clariﬁed in the next paragraph.

Handling ambiguity and contradiction. One may reasonably expect that a CRN equipped with
the ground-truth interpreter used to generate the dataset, would perform better than a CRN with a
learned interpreter. Remarkably, this is not always the case, as reported in Table 1.

Table 1: Explanatory Learning vs Program Synthesis paradigm. Performance comparison of a
data-driven vs ground-truth interpreter in a CRN. The last column shows the tag prediction accuracy
of the learned I, when provided with the correct rule.

TRAIN DATA

10K STRUCT.
1K STRUCT.
100 STRUCT.

10K STRUCT.
1K STRUCT.
100 STRUCT.

1438 RULES
1438 RULES
1438 RULES

500 RULES
500 RULES
500 RULES

FULLY-LEARNED CRN HARDCODED I CRN LEARNED I

NRS

T-ACC

0.813
0.777
0.402

0.354
0.319
0.109

0.801
0.754
0.406

0.377
0.336
0.101

0.997
1.000
0.987

0.923
0.924
0.920

The better performance of the fully learned interpreter over the ground-truth one is due to its ability
to process ill-formed conjectures generated by the CG. The conjecture “at least one pointing up”
makes the hard-coded interpreter fail, since “pointing up” must always follow the word “pyramid”
by the grammar. Yet, in Odeen, pyramids are the only objects that point, and the learned I interprets
the conjecture correctly. Other examples include: “exactly one red block touching pyramid blue”
(“pyramid” and “blue” are swapped), or the contradictory “at least one two pyramid pointing up and
exactly one red pyramid”, which was interpreted correctly by ignoring the ﬁrst “one”. When the
learned interpreter is not very accurate, the negative effect of errors in tagging prevails.

Making sense out of ambiguous or contradictory messages6 is a crucial difference between a learned
interpreter vs a hardcoded one. As Rota (1991) reminds us, a concept does not need to be precisely
deﬁned in order to be meaningful. Our everyday reasoning is not precise, yet it is effective. “After
the small tower, turn right”; we will probably reach our destination, even when our best attempts at
deﬁning “tower”, as found, e.g., in the Cambridge dictionary, begin with “a tall, narrow structure...”.

Explainability. The predictions of a CRN are directly caused by a human-understandable expla-
nation that is available in the output; this makes CRNs explainable by construction. Further, CRNs
allow counterfactuals; one may deliberately change the output explanation with a new one to obtain
a new prediction. The bank ML algorithm spoke: “Loan denied”; explanation: “Two not paid loan
in the past and resident in a district with a high rate of insolvents”. With a CRN, we can easily
discard this explanation and compute a new prediction for just “Two not paid loan in the past”.

Importantly, by choosing a training set, we control the language used for explanations; i.e., we
explicit the biases that will steer the learning of generalizations (Mitchell, 1980). This allows a

6This is one of seven essential abilities for intelligence as found in GEB (Hofstadter, 1979, Introduction).

8

Explanatory Learning: Beyond Empiricism in Neural Networks

CRN to ignore undesirable patterns in the data (e.g., skin color) if these can not be expressed in
the chosen language. If the Odeen training set had no rule with “pointing up/down”, the learned
interpreter would see all equal pyramids, even with unbalanced training data where 90% of pyramids
point up.

On the contrary, current explainability approaches for NNs (end-to-end empiricist models) ei-
ther require some form of reverse engineering, e.g., by making sense out of neuron activa-
tions (Goh et al., 2021), or introduce an ad-hoc block to generate an explanation given the pre-
diction, without establishing a cause-effect link between the two (Hendricks et al., 2016; Hind
et al., 2019). This practice produces explanations that are not reliable and can be misleading
(Rudin, 2019), on the contrary CRNs’ explanations are faithful to what the model actually computes.

Adjustable thinking time. End-to-end models do
not exhibit a parameter to adjust their processing to the
complexity of the incoming prediction. By contrast,
CRNs have a test-time parameter t, corresponding to
the number of generated conjectures, which trades off
computational cost for performance. In the inset, we
plot the cumulative R-Acc score (y axis) against the
number t of generated conjectures (x axis). The curves
show that > 60% of correct explanations are found
within the ﬁrst 50 candidates, and > 80% are within
the ﬁrst 300. As a reference, a brute force exhaustive search would reach 100% over a search space
of 24, 794 possible explanations.

Prediction conﬁdence. As explained in Sec. 4, at test time the CRN selects the conjecture with
the highest hit rate among the ones generated by the CG. Alternatively, one may keep only the
conjectures coherent with all the structures in the table, returning an “unknown explanation” signal
if no such conjectures are found. If the interpreter is sufﬁciently accurate, this stricter condition
barely deteriorates the CRN performance, and it will never return a prediction based on a possibly
wrong explanation. For example, tested in a setting with n = 1438, m = 1000 (same as the
Generalization power paragraph), this stricter CRN discovers the correct explanation for 861 out of
1132 new phenomena (76%), and admits its ignorance on the other 271. Conversely, evaluating the
conﬁdence of an end-to-end neural network remains an open problem (Meinke & Hein, 2019).

6 RELATED WORK

Epistemology. The deep learning model we propose in this work, CRNs, is designed according
to the epistemological theory of critical rationalism advanced by Popper (1935), where knowledge
derives primarily from conjectures, criticized at a later stage using data. Deutsch (2011) remarks
that to make this critique effective, conjectures should not be adjustable but can only be kept or
rejected at each new data sample, as done in CRNs at test time. Only in this way we can discover
explanations with “reach”, namely that maintain predictive power in novel situations.

Machine learning. Explanatory Learning enriches the fundamental problem of modern program
synthesis (e.g., Balog et al., 2017; Ellis et al., 2020) by including the interpretation step among what
should be learned. As seen with a few examples (see the Handling ambiguity paragraph), a CRN
with learned I can exploit the ambiguity of language to impose new meaning on arbitrary substrates,
which Santoro et al. (2021) recognize as a fundamental trait of symbolic behavior. Recent literature
ﬁnds few yet remarkable approaches that ﬁt our EL paradigm, such as CLIP (Radford et al., 2021)
in the vision area, and Generate & Rank (Shen et al., 2021) for Math Word Problems in NLP.

The Odeen challenge continues the tradition of AI benchmarks set in idealized domains (Mitchell,
2021). Unlike CLEVR (Johnson et al., 2017) and ShapeWorld (Kuhnle & Copestake, 2017), Odeen
focuses on abduction rather than deduction. Unlike ARC (Chollet, 2019), Odeen is a closed en-
vironment providing all it takes to learn the language needed to solve it. Unlike the ShapeWorld
adaptation of Andreas et al. (2017), its score is measured in terms of discovered explanations rather
than sparse guessed predictions; further, the test and training set do not share any phenomenon.

9

Explanatory Learning: Beyond Empiricism in Neural Networks

Learning theory. Finally, we point out that the expression Explanatory Learning was previously
used by Aaronson (2013, Sec. 7) to argue about the necessity of a learning theory that models
“predictions about phenomena different in kind from anything observed”. The author pointed to the
work of Angluin (1987), who generalized the PAC model (Valiant, 1984) by moving the goal from
successful predictions to comprehensive explanations.

7 CONCLUSIONS

Recently, the attention on the epistemological foundations of deep learning has been growing. The
century-old debate between empiricists and rationalists about the source of knowledge persists, with
two Turing prizes on opposite sides; LeCun (2019) argues that empiricism still offers a fruitful re-
search agenda for deep learning, while Pearl (2021) supports a rationalist steering to embrace model-
based science principles. This new debate is relevant, since as Pearl notes, today we can submit the
balance between empiricism and innateness to experimental evaluation on digital machines.

Limitations and future directions. EL models the essential part of the knowledge acquisition pro-
cess, namely the interval that turns a mute sequence of symbols into an explanation with reach.
However, our modeling assumes a representative set of observations D0 to be given (the k = 32
structures of the new phenomenon). A more comprehensive explanatory model would allow the
player to do without these observations, and instead include an interaction phase with the environ-
ment where the D0 itself is actively discovered. We see this as an exciting direction for follow-ups.

Odeen has potential as a parametric environment to experiment with EL approaches. The structure
length (6 in this paper), the number of shapes (3), attributes (2), and the grammar speciﬁcations (see
Appendix A) can be easily tweaked to obtain either simpler or signiﬁcantly more complex environ-
ments. The design choices of this paper provide a good starting point; the resulting benchmark is
far from being saturated, but experimenting with different variants is a possibility for the future.

Finally, we expect CRNs to be more resilient than end-to-end models to adversarial attacks. For a
given data point x(cid:48) ∈ P0 classiﬁed correctly by an empiricist model, a small adversarial change on
D0 can ﬂip the prediction for x(cid:48) while remaining unnoticed. Conversely, suppose that a CRN made
the prediction for x(cid:48), and assume that the correct explanation was ranked as the 5th most likely by
the CG. The same attack on D0 will have the effect of moving the correct explanation lower in the
ranking; however, as long as it stays within the ﬁrst t conjectures (300 in this paper), it will always
be found by the interpreter as the correct solution.

7.1 ETHICS STATEMENT

The Explainability paragraph in Section 5 brieﬂy touches upon the topic of fairness in AI, pointing
to a possible way to safely work with biased data. To this end, our proposed CRN model can be
beneﬁcial. In particular, CRNs allow counterfactuals and do not need impractical balanced datasets
to avoid algorithmic discrimination in automated decision processes, but just a proper design of the
language describing the data (pointing-up/down example). While addressing these aspects is out of
scope for this paper and it certainly deserves deeper investigation, we do not foresee any potentially
harmful or inappropriate application of our methodology.

7.2 REPRODUCIBILITY STATEMENT

Training: The proposed CRN model is composed of two parts, a learnable interpreter I and a
conjecture generator CG. Their architecture is described in Figure 3 (right) and in Appendix B.
The training procedure, including the choice of hyperparameters, is also described in Appendix B.
Testing: The algorithm used at test time is fully described in Figure 3 (left). Data: One of the main
contributions of the paper is the introduction of a new dataset and benchmark, called Odeen. Its full
description is given in Section 3 (Problem formulation and Dataset generation paragraphs) and in
Appendix A. The latter section also includes a formal deﬁnition of the Odeen grammar. The metrics
used for evaluation in the Odeen benchmark are deﬁned in Section 3 (Metrics paragraph). A simple
interactive version of the Odeen game, to help the readers familiarize with the concept, is available
at https://huggingface.co/spaces/gladia/odeen.

10

Explanatory Learning: Beyond Empiricism in Neural Networks

ACKNOWLEDGMENTS

This work has been funded by the ERC Starting Grant no. 802554 (SPECGEO).

We want to thank the whole GLADIA group for the precious feedback at the beginning of the project,
and especially: Luca Cosmo, Marco Fumero (for having played the role of the devil’s advocate),
Michele Mancusi (for the early philosophical discussion), and Arianna Rampini (for the design of
the ﬁrst version of Odeen).

We would also like to thank Dario Abbondanza, Marco Esposito, Giacomo Nazzaro, and Angela
Norelli for the frequent and helpful discussions throughout the work, such as those about Zero-
Knowledge proofs and the semiotic theory of Charles S. Peirce, which sometimes lasted until 5
AM.

Finally, we would like to express a special thank you to Paolo Scattini. With his usual enthusiasm,
he introduced Antonio to Zendo one night at the Rome Go club, providing the spark that lit this long
journey.

REFERENCES

Scott Aaronson. Why philosophers should care about computational complexity. Computability:

Turing, G¨odel, Church, and Beyond, 261:327, 2013.

Jacob Andreas, Dan Klein, and Sergey Levine. Learning with latent language. arXiv preprint

arXiv:1711.00482, 2017.

Dana Angluin. Learning regular sets from queries and counterexamples. Inf. Comput., 75(2):87–106,
November 1987. ISSN 0890-5401. doi: 10.1016/0890-5401(87)90052-6. URL https://doi.
org/10.1016/0890-5401(87)90052-6.

M Balog, AL Gaunt, M Brockschmidt, S Nowozin, and D Tarlow. Deepcoder: Learning to write
programs. In 5th International Conference on Learning Representations, ICLR 2017-Conference
Track Proceedings, 2017.

Emily M. Bender and Alexander Koller. Climbing towards NLU: On meaning, form, and under-
standing in the age of data. In Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics, pp. 5185–5198, Online, July 2020. Association for Computational
Linguistics. doi: 10.18653/v1/2020.acl-main.463. URL https://aclanthology.org/
2020.acl-main.463.

Manuel Blum, Paul Feldman, and Silvio Micali. Non-interactive zero-knowledge and its applica-
tions. In Proceedings of the Twentieth Annual ACM Symposium on Theory of Computing, STOC
’88, pp. 103–112, New York, NY, USA, 1988. Association for Computing Machinery.
ISBN
0897912640. doi: 10.1145/62212.62222. URL https://doi.org/10.1145/62212.
62222.

Franc¸ois Chollet. On the measure of intelligence. arXiv preprint arXiv:1911.01547, 2019.

David Deutsch. The beginning of inﬁnity: Explanations that transform the world. Penguin UK,

2011.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep

bidirectional transformers for language understanding, 2019.

Umberto Eco. Kant and the platypus: Essays on language and cognition. HMH, 2000.

Kevin Ellis, Catherine Wong, Maxwell Nye, Mathias Sable-Meyer, Luc Cary, Lucas Morales,
Luke Hewitt, Armando Solar-Lezama, and Joshua B Tenenbaum. Dreamcoder: Growing gen-
eralizable, interpretable knowledge with wake-sleep bayesian program learning. arXiv preprint
arXiv:2006.08381, 2020.

Galileo Galilei.
Press, 2016.
SideriusNuncius.pdf.

Sidereus Nuncius, Or The Sidereal Messenger.

University of Chicago
URL http://people.reed.edu/˜wieting/mathematics537/

11

Explanatory Learning: Beyond Empiricism in Neural Networks

Gabriel Goh, Nick Cammarata, Chelsea Voss, Shan Carter, Michael Petrov, Ludwig Schubert, Alec
Radford, and Chris Olah. Multimodal neurons in artiﬁcial neural networks. Distill, 2021. doi:
10.23915/distill.00030. https://distill.pub/2021/multimodal-neurons.

Kory Heath. Zendo, 2001. URL http://www.koryheath.com/zendo/.

Lisa Anne Hendricks, Zeynep Akata, Marcus Rohrbach, Jeff Donahue, Bernt Schiele, and Trevor
Darrell. Generating visual explanations. In European conference on computer vision, pp. 3–19.
Springer, 2016.

Michael Hind, Dennis Wei, Murray Campbell, Noel CF Codella, Amit Dhurandhar, Aleksandra
Mojsilovi´c, Karthikeyan Natesan Ramamurthy, and Kush R Varshney. Ted: Teaching ai to explain
its decisions. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, pp.
123–129, 2019.

Douglas Hofstadter. G¨odel, Escher, Bach: an eternal golden braid, volume 13. Basic books New

York, 1979.

T. M. Hospedales, A. Antoniou, P. Micaelli, and A. J. Storkey. Meta-learning in neural networks:
ISSN

IEEE Transactions on Pattern Analysis & Machine Intelligence, may 2020.

A survey.
1939-3539. doi: 10.1109/TPAMI.2021.3079209.

Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and
Ross Girshick. CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Vi-
sual Reasoning. In Proc. CVPR, pp. 1988–1997, 2017.

Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.

Alexander Kuhnle and Ann Copestake. Shapeworld - a new test methodology for multimodal lan-

guage understanding, 2017.

Yann LeCun. The epistemology of deep learning. Institute for Advanced Studies https://www.
ias.edu/sites/default/files/video/lecun-ias-20190222.pdf; https:
//youtu.be/gG5NCkMerHU, 2019. Accessed: 2021–10-04.

James McConnell. Memory transfer through cannibalism in planarians. J. Neuropsychiat., 3:542–

548, 1962.

Alexander Meinke and Matthias Hein. Towards neural networks that provably know when they don’t

know. arXiv preprint arXiv:1909.12180, 2019.

Melanie Mitchell. Abstraction and analogy-making in artiﬁcial intelligence.

arXiv preprint

arXiv:2102.10717, 2021.

Tom M Mitchell. The need for biases in learning generalizations. Department of Computer Science,

Laboratory for Computer Science Research, 1980.

Judea Pearl. Radical empiricism and machine learning research. Journal of Causal Inference, 9(1):

78–82, 2021.

Charles Sanders Peirce. How to make our ideas clear. Popular Science Monthly, 12:286–302, 1878.

Karl Popper. The Logic of Scientiﬁc Discovery. Julius Springer, Hutchinson & Co, 1935.

Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. In Proc. ICML, 2021.

Gian-Carlo Rota. The pernicious inﬂuence of mathematics upon philosophy. Synthese, 88(2):165–

178, 1991. doi: 10.1007/BF00567744.

Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and

use interpretable models instead. Nature Machine Intelligence, 1(5):206–215, 2019.

12

Explanatory Learning: Beyond Empiricism in Neural Networks

Adam Santoro, Andrew Lampinen, Kory Mathewson, Timothy Lillicrap, and David Raposo. Sym-

bolic behaviour in artiﬁcial intelligence. arXiv preprint arXiv:2102.03406, 2021.

Laura E Schulz, Alison Gopnik, and Clark Glymour. Preschool children learn about causal structure

from conditional interventions. Developmental science, 10(3):322–332, 2007.

John R. Searle. Minds, brains, and programs. Behavioral and Brain Sciences, 3(3):417–424, 1980.

doi: 10.1017/S0140525X00005756.

Ehud Y Shapiro. Inductive inference of theories from facts. Yale University, Department of Com-

puter Science, 1981.

Jianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin Jiang, Ming Zhang, and Qun Liu. Generate &
rank: A multi-task framework for math word problems. arXiv preprint arXiv:2109.03034, 2021.

Alex H. Taylor, Rachael Miller, and Russell D. Gray. New caledonian crows reason about hidden

causal agents. PNAS, 109(40):16389–16391, 2012.

Leslie G Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134–1142, 1984.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st Inter-
national Conference on Neural Information Processing Systems, NIPS’17, pp. 6000–6010, Red
Hook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964.

13

Explanatory Learning: Beyond Empiricism in Neural Networks

APPENDIX

A FURTHER DETAILS ON THE ODEEN7 DATASET

Training set. The total number of rules produced by the Odeen grammar is 24,794. We consider
training sets varying from 500 to 1438 rules. We choose these rules such that each token and each
syntactic construct appears at least once; then, we uniformly select the others from the distribution.
We removed from the training set any rule containing the bigram exactly 2, as well as any rule
of the form at least 2 X and at most 2 X, equivalent to exactly 2 X. Each rule is
associated with a set of 100, 1,000, or 10,0000 labelled structures that unambiguously identify a
rule equivalence class.

Test set. We generate the 1,132 games that compose the test set the same way, with the additional
constraint of excluding the rules belonging to an equivalence class that is already in the training set.
In the test set 72 rules contain the bigram exactly 2. Rules in the test set are associated with
just 32 labelled structures. The ﬁrst 10 structures are chosen by searching pairs of similar structures
with different labels, following a common human strategy in Zendo. The remaining 22 structures
are selected to ensure the lack of ambiguity on the board.

(cid:104)RULE(cid:105)
(cid:104)PROP(cid:105)
(cid:104)PROP S(cid:105)
(cid:104)OBJ(cid:105)
(cid:104)QTY(cid:105)
(cid:104)SHAPE(cid:105)
(cid:104)REL(cid:105)
(cid:104)ORIEN(cid:105)
(cid:104)NUM(cid:105)
(cid:104)CONJ(cid:105)
(cid:104)COL(cid:105)

|= (cid:104)PROP S(cid:105) | (cid:104)PROP(cid:105) | (cid:104)PROP S(cid:105) (cid:104)CONJ(cid:105) (cid:104)PROP S(cid:105)
|= (cid:104)QTY(cid:105) (cid:104)OBJ(cid:105) (cid:104)REL(cid:105) (cid:104)OBJ(cid:105)
|= (cid:104)QTY(cid:105) (cid:104)OBJ(cid:105)
|= (cid:104)COL(cid:105) | (cid:104)SHAPE(cid:105) | (cid:104)COL(cid:105) (cid:104)SHAPE(cid:105)
|= at least (cid:104)NUM(cid:105) | exactly (cid:104)NUM(cid:105) | at most (cid:104)NUM(cid:105) | zero
|= pyramid (cid:104)ORIEN(cid:105) | pyramid | block
|= touching | surrounded by | at the right of
|= pointing up | pointing down
|= 1 | 2
|= and | or
|= red | blue

Figure 4: Grammar productions for the Odeen Language.

Formal deﬁnition of the Odeen grammar. The context-free grammar in Figure 4 deﬁnes all the
acceptable rules in Odeen. This grammar only formalizes which rules are syntactically correct.
Token names (e.g. red, 1 or touching) do not imply any rule meaning.

The hard-coded interpreter formalizes how to interpret the rules. Similarly to compilers, it tokenizes
and transforms the rule into an abstract syntax tree (AST). The interpreter then adds semantic infor-
mation to the AST, establishing the truth value of each node based on the truth value of its children
and the structure under evaluation.

The Odeen binary semantic representations. By simulating the process of scientiﬁc discovery,
Odeen offers a convenient simulation of a world described by a language. Besides the computational
tractability, the simplicity and adjustable size of the Odeen world allows us to explicit the whole
semantics of its language.

This semantics can be encoded in a binary semantic matrix S with the 24,794 rules ei on the rows
and the 117,649 structures xj on the columns. The sij element of this matrix is equal to 1 if the
structure xj complies with the rule ei and 0 otherwise, see inset in Section 3, Metrics paragraph.
Si∗, the 117,649-dimensional binary vector coinciding with the i-th row of S, fully represents the
meaning of rule ei in the Odeen world. Similarly, each structure xj is represented by the 24,794-
dimensional binary vector coinciding with the column S∗j of S.

7Odeen is the Rational alien in The Gods Themselves (1972), a novel by Isaac Asimov about a conspiracy

against Earth by the inhabitants of a parallel universe with different physical laws.

14

Explanatory Learning: Beyond Empiricism in Neural Networks

(a)

(b)

Figure 5: Hamming weight of the binary semantic representation of each rule (a) and each structure
(b). We sort them in descending order for visualization purposes.

(a)

(b)

Figure 6: PCA applied to the binary semantic representation of structures (a) and rules (b). We high-
light in red the structures that have two touching pyramids pointing down. To ease the visualization,
we pair every structure with a different string of characters. Each character replaces an element of
Odeen according to a well-deﬁned mapping. The rules distribution reﬂect what can be observed in
Figure 5a.

j=1

(5a) and {S∗j}24,794

In Figure 5, we analyze the distribution of the Hamming weights (i.e., the number of ones) in
{Si∗}117,649
(5b). We observe an asymmetry between the rule and struc-
i=1
ture distributions. On one hand, the semantic representation of a rule can be quite unbalanced, with
populated extremes of rules evaluating all structures with 1 (or 0) as shown in Figure 5a. On the
other hand, Figure 5b shows that the semantic representations of structures are very balanced; most
of them have around half zeros and half ones, with no structure with less than 10k or more than 14k
ones.
This balanced trend, along with the well separable PCA of {S∗j}24,794
(Figure 6a) suggests that
the chosen language produce representations that are effective in separating structures. Conversely,
the PCA of {Si∗}117,649
is much less homogeneous (Figure 6b). Here we can recognize two poles,
corresponding respectively to rules with all ones and all zeros. We believe that this analysis of the
binary semantic representations is only partial, and we leave further exploration for follow-up work.

j=1

i=1

15

Explanatory Learning: Beyond Empiricism in Neural Networks

B IMPLEMENTATION DETAILS

In this paragraph, we give the implementation details of the models proposed and depicted in Figure
3 (right). All the models are based on a Transformer block composed of 4 layers and 8 heads.
We used a hidden dimension of 256 for all the models except for the interpreter, where we used
a hidden dimension of 128. The models differ primarily by the type of transformer block used
(encoder/decoder), inputs and embeddings. In detail:

• TRANSFORMER LABEL DECODER. This is a transformer block used to predict a label
given a structure. The input structure is a sequence of six learned embeddings, one per
piece. We add a sinusoidal positional encoding to each embedding as in the original trans-
former implementation. The embedding size is 128 in the I and 256 in the Empiricist
models (EMP-C, EMP-R). We used the standard transformer encoder block and added a
special token [CLS] at the beginning of the structure like in Devlin et al. (2019) to perform
the classiﬁcation task.

• TRANSFORMER RULE DECODER. This is a transformer decoder block with embedding
size of 128 and sinusoidal positional encoding. This decoder block is used to generate the
rule by the EMP-C and CG models.

• TRANSFORMER BOARD ENCODER. This is a transformer encoder block used to encode
the (structure, label) pairs. The input is encoded a sequence of 32 learned embeddings,
one per structure-label pair. The size of each embedding is 256. We did not add positional
encodings, since the speciﬁc position of structure-label pairs among the 32 is not relevant.
This block is used in all the models.

• TRANSFORMER RULE ENCODER. This transformer encoder block is used in I to encode
the rule. Its implementation is analogous to the TRANSFORMER RULE DECODER, with
the only difference that it does not use causal attention since it is an encoder layer.

Table 2: Number of training epochs for each training regimen.

TRAINING REGIMEN

NUMBER OF EPOCHS

10K STRUCT.
1K STRUCT.
100 STRUCT.
10K STRUCT.
1K STRUCT.
100 STRUCT.

1438 RULES
1438 RULES
1438 RULES
500 RULES
500 RULES
500 RULES

2
20
200
6
58
576

Training Procedure. All the models are trained with a learning rate of 3 · 10−4 using Adam
(Kingma & Ba, 2017), a batch size of 512 and early-stop and dropout set to 0.1 to prevent overﬁtting.
We train all the models on randomly sampled sets of 32 (structure, label) pairs to prevent overﬁtting
on speciﬁc boards. Table 2 describes the number of epochs for each training regimen. Models are
trained to: predict the label of a structure give the board (EMP-R); predict the label of a structure
given the 32 pairs (structure, label) and the associated rule (EMP-C); predict the rule given the 32
pairs (CG); predict the label of a structure given a rule (I).

16

Explanatory Learning: Beyond Empiricism in Neural Networks

Table 3: T-Acc and NRS for different training regimens.

TRAIN DATA

MODEL

NRS

T-ACC

TRAIN DATA

MODEL

NRS

T-ACC

10K STRUCT.
1438 RULES

1K STRUCT.
1438 RULES

100 STRUCT.
1438 RULES

CRN
EMP-C
EMP-R

CRN
EMP-C
EMP-R

CRN
EMP-C
EMP-R

0.813
0.352
0.179

0.777
0.225
0.156

0.402
0.125
0.163

0.984
0.930
0.895

0.980
0.905
0.898

0.939
0.865
0.896

10K STRUCT.
500 RULES

1K STRUCT.
500 RULES

100 STRUCT.
500 RULES

CRN
EMP-C
EMP-R

CRN
EMP-C
EMP-R

CRN
EMP-C
EMP-R

0.354
0.095
0.068

0.319
0.088
0.084

0.109
0.057
0.117

0.932
0.869
0.863

0.930
0.874
0.876

0.883
0.823
0.872

C EFFICIENCY

Data efﬁciency
In the Odeen challenge, CRNs require less training data to match the performance
of empiricist models. For instance, in the case of 1438 rules at training, we see in Table 3 that
the CRN trained on 100 structures per rule (NRS= 40, 2%) still overcomes the performance of
empiricist models trained on a dataset 100 times bigger (NRS= 35.2% on 10k structures per rule).

Computational cost.
In this section, we discuss the computational cost at test time of the rational-
ist and empiricists approaches. Table 4 reports the costs of tagging s new structures, while 5 reports
the costs of explicitly predicting the textual rule. We evaluate the cost per game in two ways: i) by
counting the number of calls of each trained neural network and ii) by measuring the absolute time
in seconds of each method with the same hardware conﬁguration.

We refer to the ﬁrst quantity as the Computational Cost and parametrize it in terms of the main
blocks of the models. This value is independent of the batch size and the hardware adopted. As an
example, the cost of tagging the new structures for a CRN using 300 conjectures is given by:

300 · CG + 300 · b · I + s · I.

Where 300 · CG stands for the 300 beams used to get 300 conjectures from the conjecture generator
CG. Each conjecture (300) is then tested on all the board structures (b) by the interpreter I. Finally
I is called to apply the chosen conjecture on each new structure (s). As an upper bound, an ex-
haustive search algorithm (EXV SRC) uses no conjecture generator, and thus has to evaluate with I
all admissible rules (r) on each structure of the board. Conversely, the empiricist approach provide
label predictions through a single end-to-end model which is simply called s times. Concerning the
problem of inferring explicitly the textual rule, using more beams in the empiricists models does not
provide any increase in performance, i.e. the true rule is not a more probable proposition accessible
through a larger beam search.

We measured also the absolute time in seconds with the following hardware conﬁguration for all the
experiments: 1 single core hyper threaded Xeon CPU Processor with 2.2 Ghz, 2 threads; 12.7 GiB.
of RAM; a Tesla T4 GPU, with 320 Turing Tensor Core, 2,560 NVIDIA CUDA cores, and 15.7
GDDR6 GiB of VRAM.

Table 4: Computational cost of our models at test time to tag s new structures. In Odeen r=24,794,
b=32, s=1,176. Notice how CRNs offer a good balance between computational efﬁciency and per-
formance, this trade-off is regulated by a single parameter, the number of beams.

MODEL

COMPUTATIONAL COST

T (S) NRS

EXV SRC
CRN [300B]
CRN [10B]
EMP

r · b · I + s · I
300 · CG + 300 · b · I + s · I
10 · CG + 10 · b · I + s · I
s· EMP-R

47.9
0.79
0.43
0.15

0.99
0.81
0.35
0.35

17

Explanatory Learning: Beyond Empiricism in Neural Networks

Table 5: Computational cost of our models at test time to produce the textual rule in output

MODEL

COMPUTATIONAL COST

T (S)

R-ACC

EXV SRC
CRN [300B]
CRN [10B]
EMP [300B]
EMP [10B]
EMP [1B]

r · b · I
300 · CG + 300 · b · I
10 · CG + 10 · b · I
300· EMP-C
10· EMP-C
1· EMP-C

47.8
0.72
0.35
0.41
0.10
0.10

0.99
0.77
0.35
0.07
0.07
0.07

D ODEEN EXAMPLE GAMES

In this section we propose a collection of qualitative results showing a series of Odeen games from
the test set and how they are solved by the proposed models. For each model, we report the predicted
rule (only for EMP-C and the CRN), the accuracy on the structures labeling (T-acc), and a mark that
indicates whether the nearest rule is the correct one (NRS). All the models are trained on 1,438 rules
with 1,000 structures per rule.

Board 01
Golden Rule: “at least 2 pyramid pointing down”
CRN: “at least 2 pyramid pointing down”; T-acc 1.0 (cid:88)
EMP-C: “at least 1 pyramid touching touching”; T-acc: 0.76 (cid:55)
EMP-R: T-acc 0.72 (cid:55)

Board 04
Golden Rule: “at most 1 blue pyramid pointing up”
CRN: “zero blue or at most 1 blue pyramid pointing up”; T-acc 1.0 (cid:88)
EMP-C: “zero 1 blue touching or or”; T-acc: 0.89 (cid:55)
EMP-R: T-acc 0.92 (cid:88)

18

Explanatory Learning: Beyond Empiricism in Neural Networks

Board 09
Golden rule: “exactly 1 pyramid pointing up touching red pyramid pointing down”
CRN: “exactly 1 red pyramid pointing down touching pyramid pointing up”, T-acc 0.95 (cid:55)
EMP-C: “exactly 1 red at the right of and red”, T-acc: 0.80 (cid:55)
EMP-R: T-acc 0.79 (cid:55)

Board 25
Golden rule: “at least 2 red touching blue pyramid pointing down”
CRN: “at least 2 red touching blue pyramid pointing down”, T-acc 1.0 (cid:88)
EMP-C: “at least 2 red touching blue pyramid”, T-acc: 0.87 (cid:55)
EMP-R: T-acc 0.69 (cid:55)

Board 30
Golden rule: “exactly 1 blue pyramid touching blue block”
CRN: “exactly 1 blue pyramid touching blue block”, T-acc 1.0 (cid:88)
EMP-C: “exactly 1 blue pyramid touching block block”, T-acc: 0.97 (cid:88)
EMP-R: T-acc 0.79 (cid:55)

19

Explanatory Learning: Beyond Empiricism in Neural Networks

Board 75
Golden rule: “zero blue touching red pyramid”
CRN: “zero blue touching red pyramid”, T-acc 1.0 (cid:88)
EMP-C: “zero blue touching red”, T-acc: 0.85 (cid:55)
EMP-R: T-acc 0.91 (cid:88)

Board 97
Golden rule: “at most 1 red block touching red”
CRN: “at most 1 red block touching red”, T-acc 1.0 (cid:88)
EMP-C: “at most 1 red touching at the right of red”, T-acc: 0.98 (cid:88)
EMP-R: T-acc 0.93 (cid:55)

Board 103
Golden rule: “at most 1 blue pyramid pointing down touching red”
CRN: “at most 1 blue pyramid pointing down touching red”, T-acc 1.0 (cid:88)
EMP-C: “at most 1 blue pyramid pointing down touching red”, T-acc: 0.98 (cid:88)
EMP-R: T-acc 0.85 (cid:55)

20

