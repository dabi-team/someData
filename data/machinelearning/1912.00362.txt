9
1
0
2

c
e
D
1

]

G
L
.
s
c
[

1
v
2
6
3
0
0
.
2
1
9
1
:
v
i
X
r
a

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 0, NO. 0, AUGUST 2019

1

Fast Stochastic Ordinal Embedding with
Variance Reduction and Adaptive Step Size

Ke Ma, Member, IEEE, Jinshan Zeng, Jiechao Xiong, Qianqian Xu, Senior Member, IEEE,
Xiaochun Cao∗, Senior Member, IEEE, Wei Liu, Senior Member, IEEE, Yuan Yao∗

Abstract—Learning representation from relative similarity comparisons, often called ordinal embedding, gains rising attention in recent
years. Most of the existing methods are based on semi-deﬁnite programming (SDP), which is generally time-consuming and degrades
the scalability, especially confronting large-scale data. To overcome this challenge, we propose a stochastic algorithm called SVRG-
SBB, which has the following features: i) achieving good scalability via dropping positive semi-deﬁnite (PSD) constraints as serving a
fast algorithm, i.e., stochastic variance reduced gradient (SVRG) method, and ii) adaptive learning via introducing a new, adaptive step
size called the stabilized Barzilai-Borwein (SBB) step size. Theoretically, under some natural assumptions, we show the O( 1
T ) rate
of convergence to a stationary point of the proposed algorithm, where T is the number of total iterations. Under the further Polyak-
Łojasiewicz assumption, we can show the global linear convergence (i.e., exponentially fast converging to a global optimum) of the
proposed algorithm. Numerous simulations and real-world data experiments are conducted to show the effectiveness of the proposed
algorithm by comparing with the state-of-the-art methods, notably, much lower computational cost with good prediction performance.

Index Terms—Ordinal Embedding, SVRG, Non-Convex Optimization, Barzilai-Borwein (BB) Step Size, .

(cid:70)

1 INTRODUCTION

O RDINAL embedding aims to learn the representation

of data as points in a low-dimensional embedded
space. Here the “low-dimensional” means the embedding

• K. Ma is with the School of Computer Science and Technology, University
of Chinese Academy of Sciences, Beijing 100049, China, and with the
Artiﬁcial Intelligence Research Center, Peng Cheng Laboratory, Shenzhen
518055, China, and part of this work was performed when he was in
the Key Laboratory of Information Security, Institute of Information
Engineering, Chinese Academy of Sciences, Beijing 100093, China, and
in the School of Cyber Security, University of Chinese Academy of
Sciences, Beijing 100049, China. E-mail: make@ucas.ac.cn

•

•

J. Zeng is with the School of Computer Information Engineering, Jiangxi
Normal University, Nanchang, Jiangxi 330022, China, and part of this
work was performed when he was with the Department of Mathematics,
Hong Kong University of Science and Technology, Clear Water Bay,
Kowloon, Hong Kong. E-mail: jsh.zeng@gmail.com

J. Xiong is with the Tencent AI Lab, Shenzhen, Guangdong, China.
E-mail: jcxiong@tencent.com

• Q. Xu is with the Key Laboratory of Intelligent Information Processing,
Institute of Computing Technology, Chinese Academy of Sciences, Beijing
100190, China. E-mail: qianqian.xu@vipl.ict.ac.cn, xuqianqian@ict.ac.cn

• X. Cao is with the State Key Laboratory of Information Security, Institute
of Information Engineering, Chinese Academy of Sciences, Beijing,
100093, China, and with the Cyberspace Security Research Center, Peng
Cheng Laboratory, Shenzhen 518055, China, and with the School of Cyber
Security, University of Chinese Academy of Sciences, Beijing 100049,
China. Corresponding Author. E-mail: caoxiaochun@iie.ac.cn

• W. Liu is with the Tencent AI Lab, Shenzhen, Guangdong, China. E-mail:

wl2223@columbia.edu

• Y. Yao is with the Department of Mathematics, and by courtesy,
the Department of Computer Science and Engineering, Hong Kong
University of Science and Technology, Clear Water Bay, Kowloon, Hong
Kong. Corresponding Author. E-mail: yuany@ust.hk

Manuscript received September 05, 2018; revised June 12, 2019.

dimension is much smaller than the number of data points.
The distances between these points agree with a set of
relative similarity comparisons. Relative comparisons are
often collected via the participators who are asked to answer
questions like:

“Is the similarity between object i and j larger than the

similarity between l and k?”

The feedback of these questions provide us with a set
of quadruplets, i.e., (i, j, l, k) which indicates that the sim-
ilarity between object i and j is larger than the similarity
between l and k. These relative similarity comparisons are
the supervision information for ordinal embedding. Without
prior knowledge, the relative similarity comparisons always
involve all objects, and the number of potential quadruplets
could be O(n4). Even under the so-called “local” setting
where we restrict l = i, the triple-wise comparisons, (i, j, k),
also have the complexity O(n3).

The ordinal embedding problem was ﬁrstly studied by
[1], [2], [3], [4] in the psychometric society. In recent years, it
has drawn a lot of attention in machine learning [5], [6], [7],
[8], [9], [10], statistic ranking [11], [12], [13], artiﬁcial intel-
ligence [14], [15], information retrieval [16], and computer
vision [17], [18], etc.

Most of the ordinal embedding methods are based on the
semi-deﬁnite programming (SDP). Some typical methods
include the Generalized Non-Metric Multidimensional Scal-
ing (GNMDS) [19], Crowd Kernel Learning (CKL) [20], and
Stochastic Triplet Embedding (STE/TSTE) [21]. The main
idea of such methods is to formulate the ordinal embedding
problem into a convex, low-rank SDP problem with respect
to the Gram matrix of the embedding points. In order to
solve such a SDP problem, the traditional methods gener-
ally employ the projection gradient descent to satisfy the
positive semi-deﬁnite constraint, where the singular value
decomposition (SVD) is required at each iteration. This

 
 
 
 
 
 
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 0, NO. 0, AUGUST 2019

2

inhibits the popularity of this type of methods for large-
scale and online ordinal embedding applications.

To handle the large-scale ordinal embedding problem,
we reformulate the considered problem using the embed-
ding matrix instead of its Gram matrix. By taking ad-
vantage of this new non-convex formulation, the positive
semi-deﬁnite constraint is eliminated. Furthermore, we ex-
ploit the well-known stochastic variance reduced gradient
(SVRG) method to efﬁciently solve the developed formula-
tion, which is a fast stochastic algorithm proposed in [22].
Generally, step size, one essential hyper-parameter, should
be tuned in SVRG. It is a difﬁcult task in practice as the
Lipschitz constant is hard to estimate. To facilitate the use of
SVRG, Tan et al. [23] introduced the well-known, adaptive
step size called the Barzilai-Borwein (BB) step size [24],
and proved its linear convergence in the strongly convex
case. However, as shown in our simulations (see, Figure
1), the absolute value of the original BB step size varies
dramatically regarding the epoch number, when applied to
our developed ordinal embedding formulation. One major
reason is that our developed ordinal embedding model is
not strongly convex, and even non-convex. Thus, in such
setting, the denominator of BB step size might be very close
to zero, leading to the instability of the BB step size. We add
another positive term to the non-negative denominator of
BB step size which overcomes such instability of the original
BB step size. Similar to the original version, the new step
size is adaptive with almost the same computational cost.
More importantly, the new step size is more stable than the
original BB step size, and can be applied to more general
case beyond the strongly convexity assumption. Henceforth,
we call the new method as stabilized Barzilai-Borwein (SBB)
step size. By incorporating the SBB step size with SVRG,
we propose a new stochastic algorithm called SVRG-SBB for
efﬁciently solving the considered ordinal embedding model.
In summary, our main contributions can be shown as

follows:

• We propose a non-convex framework for the ordinal
embedding problem via considering the original embed-
ding variable rather than its Gram matrix. We get rid of
the positive semi-deﬁnite (PSD) constraint on the Gram
matrix, and thus, our proposed algorithm is SVD-free
and has better scalability than the existing convex ordinal
embedding methods.

• The introduced SBB step size can overcome the insta-
bility of the original BB which comes from the absence
of strongly convexity. More importantly, the proposed
SVRG-SBB algorithm outperforms most of the state-of-
the-art methods as shown by numerous simulations and
real-world data experiments, in the sense that SVRG-SBB
often signiﬁcantly reduces the computational cost.

• We establish O( 1

T ) convergence rate of SVRG-SBB in the
sense of converging to a stationary point, where T is the
total number of iterations. Such result is comparable with
the existing convergence results in the literature.

This paper is an extension of our conference work
[25], where we propose the basic SVRG-SBB method
which derives the adaptive step size. But there still exist
the
some limitations in our conference method. First,
original SVRG-SBB does not incorporate with mini-batch

Fig. 1: Step sizes along iterations of SVRG-SBB(cid:15) on the
synthetic data, where the dark yellow curve of ncvx-SVRG-
SBB0 is exactly the varying of the BB step size in this setting.

paradigm which provides a computationally efﬁcient
process than single point update. Second, we observe that
the well-known “local optimal” of non-convex problem
does not have serious impact on the embedding result. The
empirical success in non-convex ordinal embedding poses
a new problem that under what conditions the non-convex
stochastic algorithms may ﬁnd the global optima effectively.
We provide a possible answer of this question with the
help of the Polyak-Łojasiewicz (PL) condition. Finally, we
summarize the existing ordinal embedding method and
propose the generalized ordinal embedding framework
which generalizes the existing classiﬁcation-based methods
including GNMDS, CKL and STE/TSTE. We hope the new
framework will guide the future research directions.

Organization

The remainder of the paper is organized as follows. In
Section 2, we describe the mathematical formulation of
the generalized ordinal embedding problem. Section 3
shows the development of the SVRG-SBB algorithm for
non-convex ordinal embedding. Section 4 establishes
the convergence analysis of
the proposed algorithm.
Comprehensive experimental validation based on simulated
and real-world datasets are demonstrated in Section 5. We
conclude this paper in Section 6.

2 GENERALIZED ORDINAL EMBEDDING

Throughout the paper, we denote scalars, vectors, matrices
and sets as lower case letters (x), bold lower case letters
(x), bold capital letters (X) and calligraphy upper case
letters (X ). xi and xij denote the ith element of vector x
and (i, j) entry of matrix X, respectively. For any x ∈ Rp,
(cid:107)x(cid:107)2 denotes its (cid:96)2 norm. I n is the identity matrix with
size n × n and the subscript n would be omitted if there
is no confusion. For any X ∈ Rp×n, (cid:107)X(cid:107)F and rank(X)
denote the Frobenius norm and rank of X. vec(X) is the
vectorization operator on X by column. For any square
matrix G ∈ Rn×n, tr(G) is the trace of G. [n] is the set
of {1, . . . , n}. For any X ∈ Rp×n, G = X (cid:62)X is the
Gram matrix. For any (xi, xj) ∈ X × X where X ⊂ Rp,
dij = d(xi, xj) is the distance between xi and xj, and
D = {d2(xi, xi)} is the squared distance matrix of X.
Here the distance d : Rp × Rp → R+ depends on the

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 0, NO. 0, AUGUST 2019

3

embedded space. In case of the Euclidean space, we adopt
the Euclidean distance if not speciﬁed. E[·] represents the
expectation.

Let O = {o1, . . . , on} be a collection of objects, X ⊂ Rp
be a low-dimensional embedded space where p (cid:28) n, and
ψ∗ : O × O → R+ be a dissimilarity function of O
where ψ∗
ij is the dissimilarity measure between oi and oj.
The traditional multi-dimensional scaling (MDS) methods
embed O into X based on Ψ∗ = {ψ∗
ij, i, j ∈ [n], i (cid:54)= j}.
However, there is always a lack of a dissimilarity function
ψ∗ that can evaluate the objects O properly for real-world
applications, e.g., [15], [18], [26], [27]. As an alternative,
ordinal embedding methods incorporate human knowledge
into the loop and relax the requirement of Ψ∗.

By collecting a partially ordered set which assesses dis-
similarity on a relative scale by human, ordinal embedding
methods establish relative dissimilarity of X and obtain
embedding X = {xi : xi ∈ X , oi ∈ O, i ∈ [n]}

based on the dissimilarity comparisons. Speciﬁcally,
given a dissimilarity function φ : X × X → R+ and
φij = φ(xi, xj) is the dissimilarity between xi and xj, we
collect a set of quadruplets, that is,

Q = (cid:8)q | q = (i, j, l, k), (φij, φlk) ∈ Φ2(cid:9) ,

and deﬁne Φ2 as

Φ2 = {(φij, φlk) | φij < φlk, i, j, l, k ∈ [n],

i (cid:54)= j, l (cid:54)= k, (i, j) (cid:54)= (l, k)} .

(1)

(2)

Although the embedding X and Φ = {φij | i, j ∈ [n], i (cid:54)= j}
is unknown, human knowledge can help to determine φij <
φlk or not and generate Q. The goal of ordinal embedding
is to estimate X or Φ based on Q.

One common class of ordinal embedding methods tries
to formulate it as a classiﬁcation problem (generally, a
binary classiﬁcation problem, say, [9], [19], [20], [21], [28]).
Given an ordered quadruplet q = (i, j, l, k) and the associ-
ated ordered pair (φij, φlk), the corresponding label yq can
be deﬁned as follows

(cid:26)> 0, φij < φlk,
< 0, φij > φlk.

yq

(3)

Here we ignore the multi-class case, e.g. yq could be
{−1, 0, +1} and yq = 0 indicates that φij and φlk have
the same value. As it is exceptionally rare in the practical
applications and has no obvious improvement of the results
whether we include multi-class label or not, we only con-
sider the binary case in our generalized ordinal embedding
(GOE) problem.

Let YQ := {yq, q ∈ Q} be the corresponding label
set. Given an embedding candidate X and a classiﬁer
h : R+ × R+ → YQ, the empirical misclassiﬁcation error
can be deﬁned as follows

LQ,φ,h(X, YQ) =

1
|Q|

(cid:88)

q∈Q

(cid:96)(h(φ(xi, xj), φ(xl, xk), yq)),

(4)
where |Q| represents the cardinality of the set Q, and (cid:96) : R×
R → R+ ∪{0} is a speciﬁc loss function such as hinge loss or
logistic loss. Therefore, the GOE problem can be formulated
as the following minimization problem,

min
X∈Rp×n

LQ,φ,h(X, YQ).

(5)

In practice, the dissimilarity function φ is generally taken
as the squared Euclidean distance φ(xi, xj) = d2
ij = (cid:107)xi −
xj(cid:107)2
2, and the empirical loss (4) can be written as

LQ,φ,h(X, YQ) = LQ,h(D, YQ)

=

1
|Q|

(cid:88)

q∈Q

(cid:96)(h(d2

ij, d2

lk, yq)),

(6)

where D is the squared Euclidean distance matrix of X.

Besides (5), the following SDP based formulation of the
ordinal embedding is commonly used in the literature ( [19],
[20], [21]). Let G = X (cid:62)X be the Gram matrix of X. There
+, Sn
exists a bijection between the Gram matrix G ∈ Sn
+
is the n-dimensional positive semi-deﬁnite cone, the set of
all symmetric positive semideﬁnite matrices in Rn×n and
the squared Euclidean distance matrix D as d2
ij = (cid:107)xi −
xj(cid:107)2
2 = gii − 2gij + gjj, where gij is the (i, j) element of
G.

We change the variable D in empirical loss (6) as G

LQ,h(D, YQ) = LQ,h(G, YQ)

=

1
|Q|

(cid:88)

q∈Q

(cid:96)(h(gii − 2gij + gjj, gll − 2glk + gkk, yq)).

(7)
According to (5), (6) and (7), the ordinal embedding problem
can be formulated as the following SDP problem with
respect to G, i.e.,

min

LQ,h(G, YQ),

(8)

G∈Sn

+, rank(G)≤p
the positive semi-deﬁnite constraint G ∈ Sn
+ or G (cid:23) 0
comes from the fact that the Gram matrix G is positive
semi-deﬁnite matrix; the rank constraint comes from the
fact that rank(G) ≤ rank(X) ≤ min(n, p) = p. Note that
the formulation (8) is generally convex. However, the com-
putational complexity of such SDP problem is very high,
which degrades the scalability of this kind of methods. This
motivates us to directly obtain embedding X from (5).

3 DEVELOPMENT OF SVRG-SBB
Since (5) is an unconstrained optimization problem, SVD
and regularization parameter tuning are both avoided.
However, without any prior knowledge on O, the sample
complexity of Q is O(n4). Because of the expense of full
gradients and inverse of Hessian matrix computation in
each iteration, the traditional full batch optimization meth-
ods, i.e. gradient descent and (quasi-)Newton method, are
not suitable for solving such large-scale problem where n
would be larger than thousands. Instead of the full-batch
methods, we introduce the stochastic algorithm to solve
the non-convex problem (5). One open issue in stochastic
optimization is how to choose an appropriate step size in
practice. Traditional methods include that using a constant
step size to track the iterations, adopting a diminishing
step size to enforce convergence, or tuning a step size
empirically which can be time-consuming. Recently, [23]
proposed to use the Barzilai-Borwein (BB) method to au-
tomatically compute step sizes in SVRG for strongly convex
objective function. Their method is called “SVRG-BB”. In
the following part, we will analyze the existing problem of
BB method when it is adopted in the non-convex problem.

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 0, NO. 0, AUGUST 2019

4

Furthermore, we propose the stabilized Barzilai-Borwein
(SBB) step size which alleviates these issues and establish
the non-asymptotic convergence analysis of the proposed
“SVRG-SBB” algorithm.

3.1 The Existing Problems of Barzilai-Borwein Method

In machine learning and data mining, we often encounter
the unconstrained minimization problem (5) as a ﬁnite-sum
problem. Let f1, . . . , fn be a sequence of vector function as
fi : Rp → R, and our goal is to obtain an approximation
solution of the following ﬁnite-sum problem

min
x

f (x) =

1
n

n
(cid:88)

i=1

fi(x),

(9)

where n is the training sample size, and each fi is the cost
function or loss function corresponding to the ith training
sample. Regardless the convexity of f , the choice of step
size in stochastic optimization always depends on the Lip-
schitz constant of f , which is usually difﬁcult to estimate
in practice. BB step size has been incorporated with SVRG
in [23] but it is restricted to the case where their assump-
tions are each fi is convex, differentiable and f is strongly
convex. This assumptions are adopted due to the use of a
strongly-convex regularization such as the squared (cid:96)2-norm.
However, there are many important large-scale non-convex
optimization problems, such as neural network.

The original BB method, proposed by Barzilai and Bor-
wein in [24], has been proven to be very effective in solving
nonlinear optimization problems via gradient descent. One
possible choice of BB step size ηt is
(cid:107)(cid:52)xt(cid:107)2
(cid:52)x(cid:62)
t (cid:52)yt
where (cid:52)xt = xt − xt−1 and (cid:52)yt = ∇f (xt) − ∇f (xt−1).
Actually BB step size is a possible solution of the so-called
“secant equation”.

ηt =

(10)

As the original BB method approximates the inverse of
Hessian matrix of f at xt by 1
I, there exist some inherent
ηt
drawbacks toward extending the step size (10) to non-
convex optimization problems. If f is differentiable and µ-
strongly convex, it holds that

(cid:52)x(cid:62)

t (cid:52)yt ≥ µ(cid:107)(cid:52)xt(cid:107)2 > 0

(11)

which implies ηt is always positive. However, if f is differ-
entiable and convex, we have

(cid:52)x(cid:62)

t (cid:52)yt ≥ 0

(12)

and (10) might approach ∞ when the denominator of (10) is
extremely small. Furthermore, if the differentiable function
f is non-convex, the denominator of (10) might even be
negative that makes BB method fail.

Example 1. given a quadratic optimization problem

1
2

x(cid:62)Ax

f (x) :=

min
x∈Rp
where A is a diagonal matrix with {λi}p
i=1 as the diagonal
entries, we set p = 3, λ1 = 1, λ2 = 0, λ3 = −1. The initial
x0 = [0, 0, 1](cid:62) and by gradient descent with BB step size,
x1 = x0 − η0Ax0 = [0, 0, 1 + η0]. The corresponding value
of (10) is −1. If the initial x0 = [0, 1, 0](cid:62), the denominator of
(10) is 0.

(13)

3.2 SVRG-SBB for Ordinal Embedding

An intuitive way to overcome the ﬂaw of BB step size is to
keep the denominator of (10) positive and control the lower
bound of (cid:52)x(cid:62)
t (cid:52)yt in each iteration, which leads to our
proposed stabilized Barzilai-Borwein (SBB) step size shown
as follows

(cid:107)(cid:52)xt(cid:107)2

η(cid:15),t =

|(cid:52)x(cid:62)

t (cid:52)yt| + (cid:15)(cid:107)(cid:52)xt(cid:107)2

.

(14)

Actually, as shown by our latter convergence result, if the
Hessian of the objective function ∇2f (x) is nonsingular and
the magnitudes of its eigenvalues are lower bounded by
some positive constant µ, then we can take (cid:15) = 0. In this
case, we call the referred step size SBB0 henceforth. Even if
we have no information about the Hessian of the objective
function in practice, the SBB(cid:15) step size with an (cid:15) > 0 is just
a more conservative version of SBB0 step size.

From (14), if the gradient ∇fi is Lipschitz continuous
with constant L > 0 (i.e., (cid:107)∇fi(x) − ∇fi(y)(cid:107) ≤ L(cid:107)x −
y(cid:107), ∀ x, y ∈ Rp), then the SBB(cid:15) step size can be bounded as
follows

1
L + (cid:15)

≤ η(cid:15),t ≤

1
(cid:15)

,

(15)

where the lower bound is obtained by the L-Lipschitz
continuity of ∇f , and the upper bound is directly derived
by its speciﬁc form. Furthermore, if ∇2f (x) is nonsingular
and its eigenvalues have a lower bound µ > 0, the bound
of SBB0 becomes

1
L

≤ η0,t ≤

1
µ

.

(16)

The proposed SVRG-SBB algorithm is described in Al-
gorithm 1. As shown in Figure 1, SBB(cid:15) step size with a
positive (cid:15) can make it more stable when SBB0 step size
is unstable and varies dramatically. Moreover, SBB(cid:15) step
size usually changes signiﬁcantly only at the initial several
epochs, and then quickly gets very stable. This is mainly
because there are many iterations in an epoch of SVRG-
SBB, and the algorithm might close to a stationary point
after only a few epochs. After the initial epochs, the SBB(cid:15)
step sizes might be very close to the inverse of the objective
function curvature as (cid:15) is small enough.

The main difference between Algorithm 1 and the for-
mer version is that we adopt mini-batch in the inner loop.
Mini-batching is a useful strategy for large-scale optimiza-
tion problem, especially in multi-core and distributed set-
tings as it greatly helps one exploiting parallelism and
reducing the communication burden. When the mini-batch
size is 1, Algorithm 1 reduces to our former algorithm. To
incorporate mini-batches, we replace single sample gradient
update with sampling (with replacement) a subset It ⊂ [n]
with its cardinality |It| = b, where b ∈ N is the mini-batch
size. We update the xs
t by the modiﬁed SBB step size as
t − bη(cid:15),s−1us
xs
t , where ηt,s−1 is speciﬁed in (17) and
us

t+1 = xs
t is deﬁnes as

us

t =

1
b

(cid:88)

it∈It

(∇fit(xs

t ) − ∇fit(˜xs)) + ∇f (˜xs).

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 0, NO. 0, AUGUST 2019

5

Algorithm 1 SVRG-SBB for (9)

Input: (cid:15) ≥ 0, update frequency m, mini-batch size b, maximal
number of iterations S, initial step size η0 (only used in the
ﬁrst epoch), initial point ˜x0 ∈ Rp, and the number of training
samples n
for s = 0 to S − 1 do

gs+1 = ∇f (˜xs) =

1
n

n
(cid:88)

i=1

∇fi(˜xs)

if s > 0 then

(cid:52)xs = ˜xs − ˜xs−1, (cid:52)xs = gs+1 − gs,

η(cid:15),s =

1
m

·

(cid:107)(cid:52)xs(cid:107)2
|(cid:104)(cid:52)xs, (cid:52)ys(cid:105)| + (cid:15)(cid:107)(cid:52)xs(cid:107)2

(17)

end if
xs+1
for t = 0 to m − 1 do

0 = ˜xs

uniformly pick It ⊂ [n] with |It| = b
us+1

) − ∇fit (˜xs)(cid:1) + gs+1

(cid:0)∇fit (xs+1
(cid:80)
t = 1
b
it∈It
t − bη(cid:15),sxs+1
t+1 = xs+1
xs+1
end for
˜xs+1 = xs+1

t

t

m

end for

Output: xout is chosen uniformly from {{xs

t }m

t=1}S

s=1.

4 CONVERGENCE ANALYSIS OF SVRG-SBB
In this section, we ﬁrst establish a sublinear rate of con-
vergence (to a stationary point) of the proposed SVRG-SBB
under the mild smoothness condition, and then show the
linear rate of convergence (i.e., converging exponentially
fast to a global optimum) of its modular version under the
furthered Polyak-Łojasiewicz (PL) property [29].

4.1 Sublinear convergence under smoothness
Throughout this section, we assume that each fi in (9) is
L-smoothness with a constant L > 0, i.e., fi is continuously
differentiable and its gradient ∇fi is L-Lipschitz, shown as
follows:

(cid:107)∇fi(x) − ∇fi(y)(cid:107) ≤ L(cid:107)x − y(cid:107), ∀ x, y ∈ Rp.

(18)

The L-smoothness assumption is very general to derive the
convergence of an algorithm in literature (say, [30], [31], [32],
[33]). As shown in the supplementary material, all the loss
functions adopted in the experiments for GOE problem are
veriﬁed to satisfy this assumption.

In the following, we provide a key lemma that illustrates
the convergence behavior of the inner loop of Algorithm
1. To state this lemma, we ﬁrst deﬁne several positive
constants and sequences which are used in our analysis.
Given a positive sequence {βs}S−1
s=0 , for any 0 ≤ s ≤ S − 1,
we deﬁne

ρs := 1 + bη(cid:15),sβs + 2bη2

(cid:15),sL2,

and for any t = 0, . . . , m − 1,

ct,s := ρsct+1,s + bη2

(cid:15),sL3

with cm,s = 0, and

(19)

(20)

Γt,s := bη(cid:15),s

(cid:2)1 − ct+1,sβ−1

s − bη(cid:15),s(L + 2ct+1,s)(cid:3) .

(21)

Based on these sequences, we present the lemma as follows.

Lemma 1. Suppose that each fi is L-smoothness (i.e., satisfying
(18)). Let {xs+1
}m
t=1 be a sequence generated by Algorithm 1 at
the sth inner loop, s = 0, . . . , S − 1. Let b, m, (cid:15) and βs be chosen
such that

t

bη(cid:15),s(L + 2ct+1,s) + ct+1,sβ−1

s < 1, t = 0, . . . , m − 1, (22)

then

E[(cid:107)∇f (xs+1

t

)(cid:107)2] ≤

Rt,s+1 − Rt+1,s+1
Γt,s

,

where Rt,s+1 := E[f (xs+1

t

) + ct,s(cid:107)xs+1

t − ˜xs(cid:107)2].

Lemma 1 shows that the inner loop of SVRG-SBB would
decrease along the deﬁned Lyapunov function Rt,s, which
contains the function value sequence itself as well as a
t − ˜xs(cid:107)2. Particularly, R0,s+1 = E[f (˜xs)]
proximal term (cid:107)xs+1
and Rm,s+1 = E[f (˜xs+1)]. This lemma indicates that

E[f (˜xs)] − E[f (˜xs+1)] ≥

m−1
(cid:88)

t=0

Γt,sE[(cid:107)xs+1

t

(cid:107)2] ≥ 0.

If f is lower bounded (say, lower bounded by 0), the above
inequality demonstrates that the function value sequence
{f (˜xs)} converges in expectation. We present its proof latter
in Appendix A for the completeness.

Lemma 1 implies the decreasing property and thus func-
tional value convergence of the outer loop sequence gen-
erated by Algorithm 1. In the next, we provide an abstract
result on the convergence rate of Algorithm 1, whose proof
is presented latter in Appendix B.

Theorem 1. Suppose that each fi is L-smoothness (i.e., satis-
fying (18)). Let {xs
t }(s = 0, . . . S − 1, t = 0, . . . , m − 1) be
a sequence generated by Algorithm 1. Let b, m, (cid:15) and {βs}S
s=0
be chosen such that (22) holds for any s = 0, . . . , S − 1 and
t = 0, 1, . . . , m − 1. There holds

E[(cid:107)∇f (xout)(cid:107)2] ≤

f (˜x0) − f (x∗)
T γS

,

where T = mS is the total number iterations, x∗ is an optimal
solution to (9), and γS = min0≤s≤S−1 min0≤t≤m−1 Γt,s.

This theorem shows the O(1/T ) convergence rate of
SVRG-SBB method under certain conditions. Such rate is
consistent with that of the mini-batch SVRG method studied
in [31, Theorem 2] and faster than the convergence rate
T ) established in [31], [34]. Note that
of SGD as O(1/
the condition (22) is rather technical. In the following, we
provide several sufﬁcient conditions of (22). Speciﬁcally, we
take

√

βs = 4mbη2

(cid:15),sL3, s = 0, . . . , S − 1.

(23)

Theorem 2. Let {{xs
s=1 be a sequence generated by
Algorithm 1. Suppose that each fi is L-smoothness. For any given
(cid:15) > 0, if

t=1}S

t }m

b < min






m(cid:15)2
1 + (cid:112)1 + 4(cid:15)/L

(cid:17) ,

L2 (cid:16)

m(cid:15)
(cid:17)
1 + (cid:112)1 + 4L/(cid:15)

(cid:16)

2L






,

(24)

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 0, NO. 0, AUGUST 2019

6

then for the output xout of Algorithm 1, we have

E[(cid:107)∇f (xout)(cid:107)2] ≤

4(f (˜x0) − f (x∗))
T bη(cid:15),min

,

(25)

where η(cid:15),min := min0≤s≤S−1 η(cid:15),s, and x∗ is a global minimum.

The proof of Theorem 2 is presented in Appendix C.
Given an (cid:15) > 0 and an update frequency m (commonly
taken as the multiple times of the total sample size n), Theo-
rem 2 shows that the proposed SVRG method converges to
a stationary point at a sublinear rate if the objective function
is L-smoothness and the mini-batch size b is less than an
upper bound. When b = 1, the mini-batch version of SVRG-
SBB reduces to the stochastic version of SVRG-SBB. By (25),
the established rate of SVRG-SBB is the same with that of
SVRG studied in [31]. By (25) again, a larger b adopted in
the inner loop generally implies faster convergence, which is
also veriﬁed by our latter experiments. Note that the upper
bound of b in (24) is related to m and the ratio (cid:15)
L , and
it is monotone increasing with respect to both m and (cid:15)
L .
Particularly, when (cid:15)
L = 1, then the upper bound of b in (24)
5) , and if m is further taken as the multiple times of
is
sample size n (say, m = 2(1 +
5)n), in this case, the upper
bound of b is the sample size n. This implies that the choice
of mini-batch size b is generally very ﬂexible when (cid:15)
L ≈ 1.
Moreover, if the curvature of the objective function is lower
bounded by some µ > 0, then according to the proof of
Theorem 2 (in Appendix C), the parameter (cid:15) emerging in the
upper bound of b should be replaced by (cid:15)(cid:48) = µ + (cid:15), while
when µ is moderately large, the parameter (cid:15) can be even
taken as 0, and in this case, the upper bound of b becomes

m
2(1+

√

√

running K modules in Algorithm 2 is the same as that of
running K × S iterations of the original SVRG-SBB, i.e.,
Algorithm 1. However, by exploiting such modiﬁcation, we
can show latter the linear convergence of Algorithm 2 under
the furthered Polyak-Łojasiewicz (PL) property [29], whose
deﬁnition is stated as follows.

Deﬁnition 1. A continuously differentiable function f : X → R
is said to be λ-Polyak-Łojasiewicz with some constant λ > 0, if it
satisﬁes the following PL inequality

1
2

(cid:107)∇f (x)(cid:107)2 ≥ λ(f (x) − f (x∗)), ∀ x ∈ X

(26)

where x∗ = arg min

f (x).

x∈X

The PL inequality is widely used to derive the linear
convergence of the existing methods in literature (say, [31],
[35]). Some typical examples satisfying PL inequality in-
clude the strongly convex function, the square loss and
logistic loss commonly used in machine learning, and some
invex functions like f (x) = x2 + 3 sin2(x) [35]. According
to [35], the function f (x) = x2 + 3 sin2(x) is nonconvex and
satisﬁes the PL inequality with λ = 1
32 . For more examples,
we refer to [35] and references therein.
Theorem 3. Let {˜xk}K
k=1 be a sequence generated by Algorithm
2. Suppose that assumptions in Theorem 2 hold, and further that
f satisﬁes the Polyak-Łojasiewicz inequality (26) for some λ > 0.
If 1

2 λmSbη(cid:15),min ≥ ρ−1 for some ρ ∈ (0, 1), then we have

E[(cid:107)∇f (˜xk)(cid:107)2] ≤ ρ−k(cid:107)∇f (˜x0)(cid:107)2,

(27)

(cid:40)

b < min

√

m
1 + 4κ−1)

,

κ2(1 +

2κ(1 +

m
√

1 + 4κ)

(cid:41)

,

and

where κ := L
µ represents the condition number of the
objective function. Formally, we state these claims in the
following corollary.

Corollary 1. Under the conditions of Theorem 2, if the Hessian
∇2f (x) exists and µ is the lower bound of the magnitudes of
eigenvalues of ∇2f (x) for any bounded x, the convergence rate
(25) still holds for Algorithm 1 with (cid:15) replaced by µ + (cid:15). In
addition, if µ > 0, then we can take (cid:15) = 0, and (25) still holds for
SVRR-SBB0 with (cid:15) replaced by µ.

4.2 Linear Convergence under PL Property

Algorithm 2 SVRG-SBB for (9) with PL Property

Input: (cid:15) ≥ 0, update frequency m, mini-batch size b, maximal
number of iterations S in every SVRG-SBB module, initial
point ˜x0 ∈ Rp, and the number of modules K
for k = 1 to K do

˜xk = SVRG-SBB(˜xk−1, S, m, {η(cid:15),s}S−1
s=0 )

end for
Output: ˜xK

In the next, we consider the convergence of a modular
version of the proposed SVRG-SBB method, that is, let every
S iterates of SVRG-SBB be one module, then the output
is yielded after running several modules as described in
Algorithm 2. Note that the computational complexity of

E[f (˜xk) − f (x∗)] ≤ ρ−k(f (˜x0) − f (x∗)).

(28)

Theorem 3 shows that Algorithm 2 converges exponen-
tially fast to a global optimum if the objective function
satisﬁes the PL inequality. The proof of this theorem is
presented in Appendix D.

5 EXPERIMENTS

In this section, we conduct a series of comprehensive ex-
periments on synthetic data and real-world data to demon-
strate the effectiveness of the proposed algorithms. Four
objective functions including GNMDS [19], CKL [20], STE
and TSTE [21] are taken into consideration. We notice that
some new methods are proposed for ordinal embedding,
such as SOE/LOE [28] and MVE [9]. However, our main
contribution is the optimization algorithm, SVRG-SBB(cid:15) and
its mini-batch variant, which can also be applied to solve
SOE/LOE and MVE. Moreover, the objective functions of
SOE/LOE and MVE are very similar to GNMDS and STE. We
compare the performance of stochastic methods (including
SGD, SVRG, SVRG-SBB0, SVRG-SBB(cid:15), and SVRG-SBB(cid:15) mini-
batch) with that of deterministic method (projection gradient
descent) for solving convex and non-convex formulations of
these four objective functions.

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 0, NO. 0, AUGUST 2019

7

(a) GNMDS

(b) CKL

method

min

mean

max

std

method

min

mean

max

std

cvx
ncvx Batch
ncvx SGD
ncvx SVRG
ncvx SVRG-SBB0
ncvx SVRG-SBB(cid:15)-1
ncvx SVRG-SBB(cid:15)-5
ncvx SVRG-SBB(cid:15)-10
ncvx SVRG-SBB(cid:15)-20
ncvx SVRG-SBB(cid:15)-50
ncvx SVRG-SBB(cid:15)-100

-
4.3760
-
6.5280
0.5120
0.7260
0.4210
0.4010
0.3800
0.4423
0.7657

-
4.7466
-
7.9204
0.6398
0.9119
0.5298
0.4810
0.4581
0.5162
1.0431

(c) STE

-
5.4570
-
9.5780
0.8360
1.1550
0.6720
0.6140
0.5730
0.6427
1.3730

-
0.2966
-
0.8233
0.0719
0.1024
0.0615
0.0553
0.0535
0.0548
0.1405

cvx
ncvx Batch
ncvx SGD
ncvx SVRG
ncvx SVRG-SBB0
ncvx SVRG-SBB(cid:15)-1
ncvx SVRG-SBB(cid:15)-5
ncvx SVRG-SBB(cid:15)-10
ncvx SVRG-SBB(cid:15)-20
ncvx SVRG-SBB(cid:15)-100
ncvx SVRG-SBB(cid:15)-200

-
2.4600
1.8360
2.0620
0.5130
1.0180
0.6130
0.5490
0.5250
0.5172
1.1380

-
2.5075
2.4086
2.4075
0.7010
1.1720
0.7093
0.6484
0.6176
0.6013
1.3083

(d) TSTE

-
2.6490
3.4120
2.9720
1.1740
1.4290
0.8680
0.7920
0.7560
0.7433
1.7200

-
0.0346
0.3210
0.1910
0.1183
0.0929
0.0512
0.0499
0.0490
0.0478
0.1120

method

min

mean

max

std

method

min

mean

max

std

cvx
ncvx Batch
ncvx SGD
ncvx SVRG
ncvx SVRG-SBB0
ncvx SVRG-SBB(cid:15)-1
ncvx SVRG-SBB(cid:15)-5
ncvx SVRG-SBB(cid:15)-10
ncvx SVRG-SBB(cid:15)-20
ncvx SVRG-SBB(cid:15)-100
ncvx SVRG-SBB(cid:15)-200

-
3.4520
5.8640
2.7930
0.4580
0.9100
0.5350
0.4920
0.4610
0.4600
0.9259

-
3.5765
6.6043
3.2328
0.6644
1.0656
0.6354
0.5814
0.5511
0.5414
1.1346

-
3.7310
6.7690
3.9710
0.8630
1.3040
0.7700
0.7340
0.6740
0.6618
1.3449

-
0.0676
0.2123
0.2521
0.0880
0.0803
0.0492
0.0511
0.0447
0.0501
0.0854

kernel
ncvx Batch
ncvx SGD
ncvx SVRG
ncvx SVRG-SBB0
ncvx SVRG-SBB(cid:15)-1
ncvx SVRG-SBB(cid:15)-5
ncvx SVRG-SBB(cid:15)-10
ncvx SVRG-SBB(cid:15)-20
ncvx SVRG-SBB(cid:15)-50
ncvx SVRG-SBB(cid:15)-100

-
6.3860
5.7110
2.8340
0.4360
0.5100
0.2930
0.3070
0.3120
0.3319
0.4973

-
6.9228
7.9055
3.4525
0.7962
0.5935
0.3541
0.3590
0.4104
0.4421
0.5802

-
7.6280
9.2990
4.3140
4.3630
0.7660
0.4520
0.4740
0.5310
0.5641
0.7531

-
0.4334
0.9766
0.3741
0.8396
0.0628
0.0362
0.0412
0.0454
0.0468
0.0593

TABLE 1: Computational complexity (second) comparisons on the synthetic dataset. ‘-’ represents that the generalization error of the method
can not be lower than a predeﬁned threshold in our setting, e.g. errorgen ≤ 0.15, before the algorithm calls a ﬁxed number of IFO subroutine. As
the TSTE adopts the heavy-tail kernel, its objective function is always non-convex. We note the Gram matrix formulation of TSTE as “kernel”. The
variants of the mini-batch size verify the theoretical analysis. If the mini-batch size is large than some value, the computational efﬁciency of the
proposed SVRG method will get worse as b increasing.

5.1 Simulations

In this subsection, we use a small-scale synthetic dataset to
analyze the performance of these methods in an idealized
setting. Here, we provide sufﬁcient ordinal
information
the embedding X in Rp.
without noise to construct
One metrics are adopted to evaluate the generalization
of different algorithms. Furthermore, the computational
complexity is evaluated to illustrate the convergence
behavior of each optimization method.

Settings. The triplets of this dataset involve n = 100
i=1 ⊂ R10.
points in 10-dimension Euclidean space as {xi}100
These data points {xi}100
i=1 are independent and identically
distributed (i.i.d) random variables as xi ∼ N (µ, 1
20 I),
and I ∈ R10×10 is the identity matrix. As the convex
formulation needs the data points to satisfy the “zero
mean / centered assumption”: (cid:80)100
i=1 xi = 0, we set
µ = 0. The possible triple-wise similarity comparisons
are generated based on the Euclidean distances between
these samples. As it is known that the generalization error
of the estimated Gram matrix G can be bounded if the
triplets sample complexity is O(pn log n) [10], we randomly
choose |Q| = 10, 000 triplets as the training set and the test
set Q(cid:48) has the same number of random sampled triplets.
The regularization parameter and step size settings for the
convex formulation follow the default setting of the STE
/ TSTE implementation1. Note that we do not choose the
step size by line search or the halving heuristic for convex

1. http://homepage.tudelft.nl/19j49/ste/Stochastic Triplet Embedding.html

formulation. The embedding dimension is selected just to
be equal to p = 10 without variations.

Evaluation Metrics. The metrics that we evaluate various
algorithms include the generalization error and running
time. We split all triplets into a training set and a test
set. Suppose YQ(cid:48) is the true label of test set Q(cid:48) and the
estimated label set is ˆYQ(cid:48) . We adopt the learned embedding
X from partial triple comparisons set Q ⊂ [n]3 to estimate
the partial order of the unknown triplets. The percentage of
held-out triplets whose labels ˆYQ(cid:48) are consistence with the
true labels ˆYQ(cid:48) based on the embedding X is used as the
main metric for evaluating the generalization of embedding
1yq(cid:48) (cid:54)= ˆyq(cid:48) . The running time is the
X, errorgen = 1
|Q(cid:48)|

(cid:80)
q(cid:48)∈Q(cid:48)

time spend to make the test error smaller than 0.15

Competitors. We evaluate both convex and non-convex
formulations of four objective functions. We establish two
baselines as :

• the convex objective functions solved by projection
gradient descent whose results are denoted as “cvx”,
• the non-convex objection function solved by batch gra-
dient descent whose results are denoted as “ncvx”.

We compare the performance of SVRG-SBB(cid:15) and its
mini-batch variants with SGD, ﬁxed step size SVRG (called
SVRG for short henceforth) as well as the two batch
gradient descent methods. We compare these algorithms in
the “epoch” sense which means that each method executes
the gradient calculation by the same times in every epoch.

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 0, NO. 0, AUGUST 2019

8

(a) GNMDS

(b) CKL

(c) STE

(d) TSTE

Fig. 2: Generalization errors of SGD, SVRG, SVRG-SBB and batch methods on the synthetic dataset.

To be concrete, as SVRG and SVRG-SBB(cid:15) evaluate 2m + |Q|
times of (sub)gradient in each epoch, the batch and SGD
solutions query the same numbers of (sub)gradient. The
mini-batch SVRG-SBB(cid:15) evaluates 2|It| gradients in the
inner loop where |It| is the size of mini-batch. We reduce
the inner iteration of the mini-batch SVRG-SBB(cid:15) to m/|It|
for fair comparisons.

Results. In Figure 2, the x-axis is the number of gradient
calculation divided by the total number of training samples
|Q|. We set m = |Q| for SVRG, SVRG-SBB(cid:15) and its
min-batch variants. As a consequence, we evaluate the
generalization error of each optimization method 3 times
in each epoch. The y-axis represents the generalization
error. The results are based on 50 trials with different
initialization X 0. The median of generalization error over
50 trials with [0.25, 0.75] conﬁdence interval are plotted.
The whole experiment results, generalization error and the
computational complexity are shown in Figure 2 and Table
1.

We observe the following phenomena from Figure 2.
First of all, due to instability of SBB0 step size, the SVRG-
SBB0 cause the generalization error to increase. The nu-
merical ‘explosion’ of SBB0 step size leads to the failure
of gradient decent method. This disadvantage of SBB0 is
consistent with our theoretical results and insights. On the
other hand, the results of the proposed SBB(cid:15) and its mini-
batch variant do not vibrate during the whole process. This
is the main motivation of the proposed stabilized method.
Although the SBB(cid:15) method applies the conservative treat-
ment and sacriﬁces some efﬁciency, this trade-off is valuable
if the objective function does not hold the elegant properties,
namely the condition number of Hessian matrix is not
too large. Secondly, the SBB(cid:15) and its mini-batch variant
outperform the SVRG incorporated with ﬁxed step size by
clear margins. The SBB(cid:15) method can choose more appro-
priate step size. Thus we can run the algorithm without
adding much computational burden by tuning parameter.
Just a ﬁxed step size ensures the convergence of non-convex
SVRG [31] but this particular step size is related to the
Lipschitz constant of the objective function which is hard to
obtain in practical problems, especially when the objective
functions are non-convex. The SBB(cid:15) method is derived from
BB step size and the latter one is obtained form the so-
called “secant equation” which approximates the inverse of
Hessian matrix by the identity matrix multiplied the desired
step size. Such an approximation leads to the instability of
BB method in stochastic non-convex optimization as the

curvature condition (12) does not always hold. A possible
future direction is how to preserve the positive-deﬁniteness
of the inverse of the Hessian matrix without line search and
extremely laborious computation in stochastic paradigm
[36]. Moreover, all the stochastic methods including SGD,
SVRG and SVRG-SBB(cid:15) ((cid:15) = 0 or (cid:15) > 0) converge fast at the
initial several epochs and quickly get admissible results in
terms of the relatively small generalization error.

Table 1 shows the computational complexity achieved by
batch and stochastic gradient descent with its variants, SGD,
SVRR and SVRG-SBB(cid:15) for convex and non-convex objective
functions. All results are obtained with MATLAB R(cid:13) R2016b,
on a desktop PC with Windows R(cid:13) 7 SP1 64 bit, with 3.3 GHz
Intel R(cid:13) Xeon R(cid:13) E3-1226 v3 CPU, and 32 GB 1600 MHz DDR3
memory. It is clear to see that for all objective functions,
SVRG-SBB(cid:15) ((cid:15) = 0 or (cid:15) > 0) gains speed-up compared to the
other methods. The superiority of SVRG-SBB(cid:15) mini-batch
in terms of the CPU time can also be observed from Table
1. Speciﬁcally, the speedup of SVRG-SBB(cid:15) mini-batch over
SVRG is about at least 4 times for all four models.

5.2 Visualization of Eurodist Dataset

The second empirical study is to visualize some objects in
2d space based on their relative similarity comparisons.

Settings. The “eurodist” dataset describes the “driving”
distances (in km) between 21 cities of Europe, and is
available in the stats library of R. In this dataset, there are
21, 945 possible quadruplets (i.e. (i, j, l, k)) in total. Here
we adopt the “local” setting as only the triplets are utilized
where j = i. A triplet (i, j, k) represents a partial order as
d2
ij < d2
ik, which indicates that “the distance between cities
i and j is smaller than the distance between cities i and
k” and d2
ij is the road distance between cities i and j as
i, j ∈ {1, ..., 21}. The main task of this dataset is to visualize
the embedding of these 21 cities in 2-dimensional Euclidean
space. We randomly sample 2, 000 triplets as the training
set and the left are test set.

Competitors. We compare the ordinal embedding results of
all four models with the classical metric Multidimensional
Scaling (MDS) result. mMDS is also known as Principal
Coordinates Analysis (PCoA) or Torgerson–Gower Scaling.
It takes an input matrix giving dissimilarities between pairs
of objects and outputs a coordinate matrix. Here we employ
the road distance between a pair of two cities as their
dissimilarities and obtain 2d coordinates of each cities. Note

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 0, NO. 0, AUGUST 2019

9

four SVRG-SBB(cid:15) ((cid:15) ≥ 0) methods decay much faster than
those of SGD, SVRG and projection gradient descent at the
initial epochs. We also observe that the SVRG-SBB0 tends
to failure as the step size is extremely large (∼ 1035). The
SVRG-SBB(cid:15) step size can effectively avoid the occurrence of
similar situations.

5.4 Image Retrieval on SUN397

We apply the ordinal embedding method with the proposed
SVRG-SBB algorithm on a real-world dataset, i.e., SUN 397.
In the visual search task, we wish to see how the learned
representation or embedding characterizes the “relevance”
of
the same image category and the “discrimination”
of different image categories. Hence, we use the image
representation obtained by ordinal embedding for image
retrieval.

Settings. We evaluate the effectiveness of
the ordinal
embedding methods for image retrieval on the SUN397
dataset. SUN397 consists of about 108, 000 images from
397 scene categories.
In SUN397, each image has a
1, 600-dimensional feature vector extracted by principle
component analysis (PCA) from 12, 288-dimensional Deep
Convolution Activation Features [38]. We form the training
set by randomly sampling 1, 080 images from 18 categories
with 60 images in each category. Only the training set
is used for learning the representations from ordinal
constraints and a nonlinear mapping from the original
feature space to the embedded space. We denote the
mapping as M : R1600 → R18. The nonlinear mapping
M is used to predict the embedded images in R18, which
do not participate in the relative similarity comparisons.
We use Regularized Least Square (RLS) and RBF kernel
to solve the nonlinear mapping M. The test set consists
of 720 images randomly chosen from 18 categories with
40 images in each category. We use labels of training
images to generate the similarity comparisons. The ordinal
constraints are generated like [16]: we randomly sample
two images i, j which are from the same category and
choose image k from the left categories. As the semantic
similarity between i and j in the same class is larger than
the similarity between i and k in the different class, a triplet
(i, j, k) describes the relative similarity comparison. The
total number of such triplets is 70, 000. Wrong triplets are
then synthesized to simulate the human error in real-world
data. We randomly sample 5%, 10% and 15% triplets to
exchange the positions of j and k in each triplet (i, j, k).

Evaluation Metrics. To measure the effectiveness of
various ordinal embedding methods for visual search, we
consider three evaluation metrics, i.e., precision at top-K
positions (Precision@K), recall at top-K positions (Recall@K),
and Mean Average Precision (MAP). Given the mapped
feature X = {x1, x2, . . . , x720} ⊂ R18 of test images and an
image i ∈ [n] belonging to the class ci as a query, we sort the
images of training set according to the distances between
their embedded feature in R18 and xi in an ascending order
as Ri. True positives (TPK
i ) are images correctly labeled as
positives, which involve the images belonging to ci and
listed within the top K positions in Ri. False positives

(a) Convex Result

(b) Non-Convex Result

Fig. 3: Visualization of the Eurodist dataset.

that there is no perfect embedding in the 2-dimensional
space as the given distances are actually geodesic.

the Procrustes

Figure 3 displays

Results.
rotated
embedding results of MDS and ordinal embedding.
Obviously, the full, explicit distance information helps MDS
to gain a better visualization. ODE methods only adopt
partial, relative comparisons and their visualizations are
inferior to the competitor’s result. However, the non-convex
ordinal embedding methods still generate the reasonable
representations of these 21 cities. In the new coordinate
system, all the positions are not contrary to geographical
knowledge. Furthermore,
the stochastic paradigm in
SVRG-SBB(cid:15) dose not affect the quality of the embedding.

5.3 Music Artists Similarity Comparison

We implement all methods on a medium-scale real world
dataset, called music artist similarity dataset, which is
collected by [37] through a web-based survey.

In this dataset,

there are 412 music artists
Settings.
involved in triple-wise comparisons based on music
genre. The genre labels for all artists are gathered using
Wikipedia2, to distinguish nine music genres (rock, metal,
pop, dance, hip hop, jazz, country, gospel, and reggae). The
similarity comparisons are labeled by 1, 032 participants.
The number of triplets on the similarity between music
artists is 213, 472. A triplet (i, j, k) indicates that “music
artist i is more similar to artist j than artist k”. Speciﬁcally,
we use the data pre-processed by [21] via removing the
inconsistent triplets from the original dataset. There are
9, 107 triplets for N = 400 artists. We randomly sample 80
percent of the comparisons as the training set and the left
are the test data. The embedded dimension is p = 9 as the
number of genre category. All methods start with the same
initialization X 0 which is randomly generated.

Results.
Each curve in Figure 4 shows the trend of
test error of one method with respect to the epoch number.
We execute 50 trials of each optimization method for
the four objective functions. From Figure 4, SBB(cid:15) and its
mini-batch variant can signiﬁcantly speed up SVRG in
terms of epoch number. Specially, the test error curves of

2. https://www.wikipedia.org

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 0, NO. 0, AUGUST 2019

10

(a) GNMDS

(b) CKL

(c) STE

(d) TSTE

Fig. 4: Generalization errors of SGD, SVRG, SVRG-SBB and batch methods on the music artists dataset.

(a) GNMDS

5%

10%

MAP

0.0259
0.0474
0.3120
0.3460
0.4659
0.4861
0.4861
0.4867
0.4872

P

0.0712
0.1326
0.4606
0.4949
0.5783
0.5993
0.5995
0.5998
0.6005

MAP

0.0255
0.0386
0.1865
0.2112
0.2971
0.3085
0.3085
0.3083
0.3085

P

0.0694
0.1119
0.3359
0.3631
0.4408
0.4533
0.4536
0.4534
0.4535

(c) STE

5%

MAP

0.0257
0.0304
0.2820
0.5265
0.4637
0.6387
0.6362
0.6359
0.6358

P

0.0707
0.0881
0.4261
0.6302
0.5799
0.7147
0.7129
0.7127
0.7126

10%

MAP

0.0262
0.0306
0.1948
0.3783
0.0070
0.4584
0.4568
0.4566
0.4565

P

0.0722
0.0874
0.3391
0.5105
0.0555
0.5730
0.5719
0.5719
0.5717

method

cvx
ncvx Batch
ncvx SGD
ncvx SVRG
ncvx SVRG-SBB0
ncvx SVRG-SBB(cid:15)-1
ncvx SVRG-SBB(cid:15)-5
ncvx SVRG-SBB(cid:15)-10
ncvx SVRG-SBB(cid:15)-20

method

cvx
ncvx Batch
ncvx SGD
ncvx SVRG
ncvx SVRG-SBB0
ncvx SVRG-SBB(cid:15)-1
ncvx SVRG-SBB(cid:15)-5
ncvx SVRG-SBB(cid:15)-10
ncvx SVRG-SBB(cid:15)-20

method

cvx
ncvx Batch
ncvx SGD
ncvx SVRG
ncvx SVRG-SBB0
ncvx SVRG-SBB(cid:15)-1
ncvx SVRG-SBB(cid:15)-5
ncvx SVRG-SBB(cid:15)-10
ncvx SVRG-SBB(cid:15)-20

(b) CKL

5%

10%

MAP

0.0258
0.0376
0.4830
0.5445
0.4662
0.5645
0.5652
0.5653
0.5653

P

0.0704
0.1087
0.5926
0.6450
0.5793
0.6525
0.6532
0.6533
0.6533

(d) TSTE

MAP

0.0260
0.0338
0.1765
0.2149
0.2037
0.2805
0.2809
0.2810
0.2810

P

0.0711
0.0992
0.3192
0.3623
0.3547
0.4270
0.4273
0.4274
0.4274

method

cvx
ncvx Batch
ncvx SGD
ncvx SVRG
ncvx SVRG-SBB0
ncvx SVRG-SBB(cid:15)-1
ncvx SVRG-SBB(cid:15)-5
ncvx SVRG-SBB(cid:15)-10
ncvx SVRG-SBB(cid:15)-20

5%

10%

MAP

0.0257
0.0270
0.3864
0.7198
0.0070
0.8861
0.8898
0.8866
0.8846

P

0.0695
0.0736
0.5074
0.7746
0.0555
0.9034
0.9030
0.9000
0.8995

MAP

0.0261
0.0273
0.2381
0.5241
0.0070
0.6859
0.6865
0.6854
0.6847

P

0.0704
0.0751
0.3742
0.6221
0.0555
0.7431
0.7437
0.7432
0.7426

TABLE 2: Image retrieval performance (MAP and Precision@60) on SUN397

(a) GNMDS

(b) CKL

(c) STE

(d) TSTE

Fig. 5: Recall@K with 10% noise on SUN397.

(FPK
i ) refer to negative examples incorrectly labeled as
(cid:54)= i)
positives, which are the images belonging to cl(l
and listed within the top K in Ri. True negatives (TNK
i )
correspond to negatives correctly labeled as negatives,
which refer to the images belonging to cl(l (cid:54)= i) and listed
after the top K in Ri. Finally, false negatives (FNK
i ) refer to
positive examples incorrectly labeled as negatives, which
are relevant to the images belonging to class ci and listed

(cid:80)n

after the top K in Ri. We are able to deﬁne Precision@K and
TPK
Recall@K as: Precision@K = 1
i
i +FPK
TPK
n
i
i = 1
and Recall@K = 1
. These two
n
n
measurements are both single-valued metric based on the
top K ranking order of training images refered to the query
image. It is also desirable to consider the total order of
the training images which are in the same category as the

i = 1
n
TPK
i
i +FNK
i

i pK
(cid:80)n
i

i rK

(cid:80)n
i

(cid:80)n

TPK

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 0, NO. 0, AUGUST 2019

11

query image. By computing precision and recall at every
position in the ranked sequence for query xi, one can plot a
precision-recall curve, plotting precision pi(r) as a function
of recall ri. Average Precision (AP) computes the average
value of pi(r) over the interval from ri = 0 to ri = 1:
APi = (cid:82) 1
0 pi(ri)dri, which is the area under precision-recall
curve. This integral can be replaced with a ﬁnite sum over
every position s in the ranked sequence of the embedding:
APi = (cid:80)40
s=1 pi(s) · (cid:52)ri(s), where (cid:52)ri(s) is the change in
recall from items s − 1 to s. The MAP used in this paper is
deﬁned as MAP = 1
n

i=1 APi.

(cid:80)n

Results. The experiment results are shown in Table 2
and Figure 5. With K varying from 40 to 100, we observe
that non-convex SVRG-SBB(cid:15) consistently achieves the
superior Precision@K, Recall@K and MAP results against
the other methods with the same gradient calculation.
The results illustrate that SVRG-SBB(cid:15) is more suitable for
non-convex objective functions than SVRG-SBB0. SVRG-
SBB(cid:15) has a very promising potential in practice, because
it generates appropriate step sizes automatically while
running the algorithm and the result is robust. Moreover,
under our setting, all the ordinal embedding methods
achieve reasonable results for image retrieval. It illustrates
that high-quality relative similarity comparisons can be
used for learning meaningful representation of massive
data, thereby making it easier to extract useful information
in other applications.

6 CONCLUSIONS

In this paper, we propose a stochastic non-convex frame-
work for the ordinal embedding problem. A novel stochastic
gradient descent algorithm called SVRG-SBB is applied to
solve this non-convex formulation. The proposed SVRG-
SBB is a variant of SVRG method. It incorporate with the
so-called stabilized BB (SBB) step size, a new, stable and
adaptive step size introduced in this paper. The motivation
of the SBB step size is to overcome the instability of the orig-
inal BB step size when the strongly convexity is absent. We
also establish the O(1/T ) convergence rate of SVRG-SBB.
Such a convergence rate is comparable to the existing best
convergence results of SVRG in the literature. Furthermore,
we derive the analysis to mini-batch variants of SVRG-SBB.
We also analyze the PL function on which SVRG-SBB attains
linear convergence to the global optimum. A series of sim-
ulations and real-world data experiments are implemented
to demonstrate the effectiveness of the proposed SVRG-SBB
for the ordinal embedding problem. The proposed SVRG-
SBB outperforms most of the state-of-the-art methods from
the perspective of computational cost.

[4] ——,

“Nonmetric multidimensional

scaling: A numerical

method,” Psychometrika, vol. 29, no. 2, pp. 115–129, 1964.

[5] K. Jamieson and R. Nowak, “Low-dimensional embedding using
adaptively selected ordinal data,” Annual Allerton Conference on
Communication, Control, and Computing, pp. 1077–1084, 2011.
[6] N. Ailon, “An active learning algorithm for ranking from pairwise
preferences with an almost optimal query complexity,” Journal of
Machine Learning Research, vol. 13, no. 1, pp. 137–164, 2012.
[7] M. Kleindessner and U. Luxburg, “Uniqueness of ordinal embed-

ding,” The Conference on Learning Theory, pp. 40–67, 2014.

[8] E. Arias-Castro, “Some theory for ordinal embedding,” Bernoulli,

vol. 23, no. 3, pp. 1663–1693, 08 2017.

[9] E. Amid and A. Ukkonen, “Multiview triplet embedding: Learn-
ing attributes in multiple maps,” International Conference on Ma-
chine Learning, pp. 1472–1480, 2015.

[10] L. Jain, K. G. Jamieson, and R. Nowak, “Finite sample prediction
and recovery bounds for ordinal embedding,” Annual Conference
on Neural Information Processing Systems, pp. 2711–2719, 2016.
[11] B. McFee and G. Lanckriet, “Learning multi-modal similarity,”
Journal of Machine Learning Research, vol. 12, pp. 491–523, 2011.
[12] K. Jamieson and R. Nowak, “Active ranking using pairwise
comparisons,” Annual Conference on Neural Information Processing
Systems, pp. 2240–2248, 2011.

[13] Y. Lan, J. Guo, X. Cheng, and T. yan Liu, “Statistical consistency
of ranking methods in a rank-differentiable probability space,”
Annual Conference on Neural Information Processing Systems, pp.
1241–1249, 2012.

[14] H. Heikinheimo and A. Ukkonen, “The crowd-median algorithm,”
AAAI Conference on Human Computation and Crowdsourcing, pp. 69–
77, 2013.

[15] M. Wilber, S. Kwak, and S. Belongie, “Cost-effective hits for
relative similarity comparisons,” AAAI Conference on Human Com-
putation and Crowdsourcing, pp. 227–233, 2014.

[16] D. Song, W. Liu, R. Ji, D. A. Meyer, and J. R. Smith, “Top rank
supervised binary coding for visual search,” IEEE International
Conference on Computer Vision, pp. 1922–1930, 2015.

[17] C. Wah, G. V. Horn, S. Branson, S. Maji, P. Perona, and S. Belongie,
“Similarity comparisons for interactive ﬁne-grained categoriza-
tion,” IEEE Conference on Computer Vision and Pattern Recognition,
pp. 859–866, 2014.

[18] M. Wilber, I. Kwak, D. Kriegman, and S. Belongie, “Learning
concept embeddings with combined human-machine expertise,”
IEEE International Conference on Computer Vision, pp. 981–989, 2015.
[19] S. Agarwal, J. Wills, L. Cayton, G. R. Lanckriet, D. J. Kriegman, and
S. Belongie, “Generalized non-metric multidimensional scaling,”
International Conference on Artiﬁcial Intelligence and Statistics, pp.
11–18, 2007.

[20] O. Tamuz, C. Liu, O. Shamir, A. Kalai, and S. Belongie, “Adap-
tively learning the crowd kernel,” International Conference on Ma-
chine Learning, pp. 673–680, 2011.

[21] L. van der Maaten and K. Weinberger, “Stochastic triplet embed-
ding,” IEEE International Workshop on Machine Learning for Signal
Processing, pp. 1–6, 2012.

[22] R. Johnson and T. Zhang, “Accelerating stochastic gradient de-
scent using predictive variance reduction,” Annual Conference on
Neural Information Processing Systems, pp. 315–323, 2013.

[23] C. Tan, S. Ma, Y.-H. Dai, and Y. Qian, “Barzilai-borwein step
size for stochastic gradient descent,” Annual Conference on Neural
Information Processing Systems, pp. 685–693, 2016.

[24] J. Barzilai and J. M. Borwein, “Two-point step size gradient meth-
ods,” IMA journal of numerical analysis, vol. 8, no. 1, pp. 141–148,
1988.

[25] K. Ma,

J. Zeng,

J. Xiong, Q. Xu, X. Cao, W. Liu, and
Y. Yao, “Stochastic non-convex ordinal embedding with stabilized
barzilai-borwein step size,” AAAI Conference on Artiﬁcial Intelli-
gence, pp. 3738–3745, 2018.

REFERENCES

[1] R. N. Shepard, “The analysis of proximities: Multidimensional
scaling with an unknown distance function. i,” Psychometrika,
vol. 27, no. 2, pp. 125–140, 1962.

[2] ——, “The analysis of proximities: Multidimensional scaling with
an unknown distance function. ii,” Psychometrika, vol. 27, no. 3,
pp. 219–246, 1962.
J. B. Kruskal, “Multidimensional scaling by optimizing goodness
of ﬁt to a nonmetric hypothesis,” Psychometrika, vol. 29, no. 1, pp.
1–27, 1964.

[3]

[26] A. Ukkonen, “Crowdsourced correlation clustering with relative
distance comparisons,” IEEE International Conference on Data Min-
ing, pp. 1117–1122, 2017.

[27] B. Mason, L. Jain, and R. D. Nowak, “Learning low-dimensional
metrics,” Annual Conference on Neural Information Processing Sys-
tems, pp. 4142–4150, 2017.

[28] Y. Terada and U. Luxburg, “Local ordinal embedding,” Interna-

tional Conference on Machine Learning, pp. 847–855, 2014.

[29] B. Polyak, “Gradient methods for the minimization of function-
als,” USSR Computational Mathematics and Mathematical Physics,
vol. 3, no. 4, pp. 864 – 878, 1963.

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 0, NO. 0, AUGUST 2019

12

[30] Y. Nesterov, Introductory lectures on convex optimization: A basic

course. Springer Science & Business Media, 2004.

[31] S. J. Reddi, A. Hefny, S. Sra, B. Poczos, and A. Smola, “Stochas-
tic variance reduction for nonconvex optimization,” International
Conference on Machine Learning, pp. 314–323, 2016.

[32] K. M. Jinshan Zeng and Y. Yao, “Finding global optima in noncon-
vex stochastic semideﬁnite optimization with variance reduction,”
International Conference on Artiﬁcial Intelligence and Statistics, pp.
199–207, 2018.

[33] ——, “On global linear convergence in stochastic nonconvex op-
timization for semideﬁnite programming,” IEEE Transactions on
Signal Processing, vol. 67, no. 16, pp. 4261–4275, 2019.

[34] A. S. Nemirovsky and D. B. Yudin, Problem complexity and method

efﬁciency in optimization. Wiley, 1983.

[35] J. N. Hamed Karimi and M. Schmidt, “Linear convergence
of gradient and proximal-gradient methods under the polyak-
łojasiewicz condition,” Joint European Conference on Machine Learn-
ing and Knowledge Discovery in Databases, vol. 9851, pp. 795–811,
2016.

[36] X. Wang, S. Ma, D. Goldfarb, and W. Liu, “Stochastic quasi-newton
methods for nonconvex stochastic optimization,” SIAM Journal on
Optimization, vol. 27, no. 2, pp. 927–956, 2017.

[37] D. P. Ellis, B. Whitman, A. Berenzweig, and S. Lawrence, “The
quest for ground truth in musical artist similarity,” International
Society for Music Information Retrieval Conference, pp. 109–116, 2002.
[38] Y. Gong, L. Wang, R. Guo, and S. Lazebnik, “Multi-scale order-
less pooling of deep convolutional activation features,” European
Conference on Computer Vision, pp. 392–407, 2014.

[39] J. Dattorro, Convex Optimization & Euclidean Distance Geometry.

Meboo Publishing, 2005.

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 0, NO. 0, AUGUST 2019

APPENDIX

A. PROOF OF LEMMA 1
To prove Lemma 1, we need the following lemma, which establishes the bound of the norm of variance gradient us+1
Lemma 2 (Bounded E (cid:0)(cid:107)us+1

). Under assumptions of Lemma 1, the following holds

(cid:107)2(cid:1)

t

t

13

.

E

(cid:104)(cid:13)
(cid:13)xs+1
t

2(cid:105)

(cid:13)
(cid:13)

≤ 2E

(cid:104)(cid:13)
(cid:13)∇f (xs+1

t

2(cid:105)

)(cid:13)
(cid:13)

+

Proof. Let vs+1

t

:= 1
b

(cid:80)

E

(cid:104)(cid:13)
(cid:13)us+1
t

(cid:13)
(cid:13)

it∈It
2(cid:105)

(cid:0)∇fit

= E

≤ 2E

(cid:1) − ∇fit (˜xs)(cid:1)

(cid:0)xs+1
t
(cid:104)(cid:13)
t − (cid:0)∇f (cid:0)xs+1
(cid:13)vs+1
(cid:104)(cid:13)
)(cid:13)
(cid:13)∇f (xs+1
(cid:13)

2(cid:105)

t

t

+ 2E

E

2(cid:105)

(cid:104)(cid:13)
(cid:13)xs+1

t − ˜xs(cid:13)
(cid:13)

2L2
b
t + ∇f (˜xs). Thus,

.

2(cid:105)

(cid:1)(cid:13)
(cid:13)

, then us+1

t = vs+1

(cid:1) − ∇f (˜xs)(cid:1) + ∇f (cid:0)xs+1
t
(cid:104)(cid:13)
2(cid:105)
t − E (cid:2)vs+1
(cid:13)vs+1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:0)xs+1
t

(cid:0)∇fit

2
b2

it∈It

(cid:3)(cid:13)
(cid:13)

(cid:88)




E

t

(cid:104)(cid:13)
(cid:13)∇fit

(cid:0)xs+1
t

(cid:1) − ∇fit (˜xs)(cid:13)
(cid:13)

2(cid:105)

E

(cid:104)(cid:13)
(cid:13)xs+1

t − ˜xs(cid:13)
(cid:13)

2(cid:105)

,

(cid:1) − ∇fit (˜xs) − E (cid:0)vs+1

t

(cid:1)(cid:1)

2




(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

= 2E

(cid:104)(cid:13)
(cid:13)∇f (cid:0)xs+1

t

2(cid:105)

(cid:1)(cid:13)
(cid:13)

+

≤ 2E

≤ 2E

t

(cid:104)(cid:13)
(cid:13)∇f (cid:0)xs+1
(cid:104)(cid:13)
(cid:13)∇f (cid:0)xs+1

t

2(cid:105)

(cid:1)(cid:13)
(cid:13)

+

2(cid:105)

(cid:1)(cid:13)
(cid:13)

+

E

2
b
2L2
b

where the ﬁrst inequality holds for the basic inequality, i.e., (cid:107)a + b(cid:107)2 ≤ 2((cid:107)a(cid:107)2 + (cid:107)b(cid:107)2) for any two vectors of the same
sizes, the second inequality holds for the fact that it are drawn uniformly randomly and independently from {1, 2, . . . , n}
and noting that for a random variable ξ, E[(cid:107)ξ − E[ξ](cid:107)2] ≤ E[(cid:107)ξ(cid:107)2], and the ﬁnal inequality holds for the L-smoothness of
fit .

Based on this lemma, we prove Lemma 1 as follows.

Proof of Lemma 1. By the L-smoothness of each fi (implying the L-smoothness of f ), there holds

E (cid:2)f (cid:0)xs+1
t+1

(cid:1)(cid:3) ≤ E (cid:2)f (cid:0)xs+1

t

= E (cid:2)f (cid:0)xs+1

t

where the equality holds for

E

(cid:1)(cid:3) +

L
2
(cid:1)(cid:3) − bη(cid:15),sE

(cid:104)(cid:13)
(cid:13)xs+1

t+1 − xs+1

t

2(cid:105)

(cid:13)
(cid:13)

+ E (cid:2)(cid:10)∇f (cid:0)xs+1

t

(cid:1) , xs+1

t+1 − xs+1

t

(cid:11)(cid:3)

(cid:104)(cid:13)
(cid:13)∇f (cid:0)xs+1

t

2(cid:105)

(cid:1)(cid:13)
(cid:13)

+

Lb2η2
(cid:15),s
2

E

(cid:104)(cid:13)
(cid:13)xs+1
t

2(cid:105)

(cid:13)
(cid:13)

,

E (cid:2)(cid:104)∇f (cid:0)xs+1

t

(cid:1) , xs+1

t+1 − xs+1

t

(cid:105)(cid:3) = −bη(cid:15),sE (cid:2)(cid:10)∇f (cid:0)xs+1

t

(cid:1) , E (cid:2)xs+1

t

(cid:3)(cid:11)(cid:3) = −bη(cid:15),sE

(cid:104)(cid:13)
(cid:13)∇f (cid:0)xs+1

t

2(cid:105)

(cid:1)(cid:13)
(cid:13)

.

Moreover, note that
t+1 − ˜xs(cid:13)
(cid:13)

(cid:104)(cid:13)
(cid:13)xs+1

E

2(cid:105)

= E

= E

= E

≤ E

t

2(cid:105)

(cid:104)(cid:13)
(cid:13)xs+1
(cid:104)(cid:13)
(cid:13)xs+1
(cid:104)(cid:13)
(cid:13)xs+1
(cid:104)(cid:13)
(cid:13)xs+1

t − ˜xs(cid:13)
t + xs+1
t+1 − xs+1
(cid:13)
(cid:104)(cid:13)
2(cid:105)
t − ˜xs(cid:13)
(cid:13)
(cid:13)xs+1
t+1 − xs+1
+ E
(cid:13)
(cid:13)
2(cid:105)
(cid:104)(cid:13)
2(cid:105)
t − ˜xs(cid:13)
(cid:13)us+1
(cid:13)
t
(cid:104)(cid:13)
t − ˜xs(cid:13)
(cid:13)us+1
(cid:13)
t
2(cid:105)
(cid:104)(cid:13)
(cid:13)xs+1

t − ˜xs(cid:13)
(cid:13)

+ b2η2
(cid:15),s

+ b2η2
(cid:15),s

2(cid:105)

2(cid:105)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

E

E

+ bη(cid:15),sβ−1

s

= (1 + bη(cid:15),sβs) E

2(cid:105)

t

+ 2E (cid:2)(cid:10)xs+1

t+1 − xs+1
− 2bη(cid:15),sE (cid:2)(cid:10)∇f (cid:0)xs+1
(cid:20) 1
+ 2bη(cid:15),s
2βs
(cid:104)(cid:13)
(cid:13)∇f (xs+1

)(cid:13)
(cid:13)

2(cid:105)

E

E

t

t

, xs+1
(cid:1) , xs+1

t − ˜xs(cid:11)(cid:3)
t − ˜xs(cid:11)(cid:3)
βs
2(cid:105)
)(cid:13)
(cid:13)
2
(cid:104)(cid:13)
(cid:13)us+1
t

+

E

+ b2η2
(cid:15),s

(cid:104)(cid:13)
(cid:13)∇f (xs+1

t

2(cid:21)
t − ˜xs(cid:13)
(cid:13)
(cid:13)xs+1
(cid:13)
2(cid:105)

(cid:13)
(cid:13)

,

where the third equality holds for the iterate of the proposed SVRG method, i.e., xs+1
inequality holds for the Young’s inequality with some positive constant βs.

t+1 = xs+1

t − bη(cid:15),sus+1

t

, and the

Now consider the Lyapunov function

Rt,s+1 := E (cid:2)f (cid:0)xs+1

t

(cid:1)(cid:3) + ct,sE

where ct,s is speciﬁed in (20). By (A) and (A), there holds

(cid:104)(cid:13)
(cid:13)xs+1

t − ˜xs(cid:13)
(cid:13)

2(cid:105)

,

Rt+1,s+1 ≤ (1 + bη(cid:15),sβs) ct+1,sE
L
2

+ b2η2
(cid:15),s

ct+1,s +

(cid:18)

(cid:104)(cid:13)
(cid:13)xs+1
t − ˜xs
(cid:19)
(cid:104)(cid:13)
(cid:13)us+1
t

(cid:13)
(cid:13)

E

2(cid:105)

+ E (cid:2)f (cid:0)xs+1

t

(cid:1)(cid:3) .

2(cid:105)

(cid:13)
(cid:13)

− bη(cid:15),s

(cid:0)1 − ct+1,sβ−1

s

(cid:1) E

(cid:104)(cid:13)
(cid:13)∇f (cid:0)xs+1

t

2(cid:105)

(cid:1)(cid:13)
(cid:13)

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 0, NO. 0, AUGUST 2019

14

Plugging the bound of E

(cid:104)(cid:13)
(cid:13)us+1
t

2(cid:105)

(cid:13)
(cid:13)

established in Lemma 2 into the above inequality yields

Rt+1,s+1 ≤ E (cid:2)f (cid:0)xs+1

t

(cid:1)(cid:3) + ct,sE

(cid:104)(cid:13)
(cid:13)xs+1

t − ˜xs(cid:13)
(cid:13)

2(cid:105)

− bη(cid:15),s

(cid:20)

1 −

ct+1,s
βs

− 2bη(cid:15),s

(cid:18)

ct+1,s +

(cid:19)(cid:21)

L
2

E

(cid:104)(cid:13)
(cid:13)∇f (cid:0)xs+1

t

2(cid:105)

(cid:1)(cid:13)
(cid:13)

,

which concludes this lemma via the deﬁnition of Γt,s (21).
B. PROOF OF THEOREM 1
Proof. By Lemma 1, for any 0 ≤ s ≤ S − 1,

m−1
(cid:88)

t=0

E

(cid:104)(cid:13)
(cid:13)∇f (cid:0)xs+1

t

2(cid:105)

(cid:1)(cid:13)
(cid:13)

≤

R0,s+1 − Rm,s+1
γS

.

Noting that xs+1

0 = ˜xs, xs+1

m = ˜xs+1 and cm,s = 0, the above inequality implies
(cid:16)

(cid:104)

E [f (˜xs)] − E

f

m−1
(cid:88)

E

(cid:104)(cid:13)
(cid:13)∇f (cid:0)xs+1

t

2(cid:105)

(cid:1)(cid:13)
(cid:13)

≤

˜xs+1(cid:17)(cid:105)

.

γS

Summing over s from 0 to S − 1, the above inequality yields

t=0

1
mS

S−1
(cid:88)

m−1
(cid:88)

s=0

t=0

E

(cid:104)(cid:13)
(cid:13)∇f (cid:0)xs+1

t

2(cid:105)

(cid:1)(cid:13)
(cid:13)

≤

f

− f (x∗)

(cid:16)
˜x0(cid:17)
mSγS

.

Using the above inequality and the deﬁnition of xout in Algorithm 1, we conclude this theorem.

C. PROOF OF THEOREM 2
To prove this theorem, we need the following lemma.
Lemma 3. Given some positive integer l ≥ 2, then for any 0 < x ≤ 1

l , the following holds (1 + x)l ≤ elx ≤ 1 + 2lx.

Proof. Note that (1 + x)l = el·ln(1+x) ≤ elx,where the last inequality holds for ln(1 + x) ≤ x for any x ∈ (0, 1]. Thus, we
get the ﬁrst inequality. Let h(z) = 1 + 2z − ez for any z ∈ (0, 1]. It is easy to check that h(z) ≥ 0 for any z ∈ (0, 1]. Thus
we get the second inequality.

Based on this lemma, we show the proof of Theorem 2.

Proof of Theorem 2. To prove this theorem, it only sufﬁces to show that (22) holds under the choice of βs (23) and condition
(24) presented in this theorem. To achieve this, we ﬁrst provide two intermediate conditions implying (22),

bη(cid:15),sβs + 2bηs

bη(cid:15),sβs + bη(cid:15),sL <

(cid:15),sL2 ≤
1
2

1
m

,

,

(29)

(30)

and then show that (24) together with the choice of βs (23) imply (29)-(30).

(a) Here, we prove that (29) and (30) implies (22). According to (20) and the initial condition cm,s = 0, we can easily

check that for t = m − 1, . . . , 1,

((ρs)m−t − 1)η(cid:15),sL3
βs + 2η(cid:15),sL2
Noting that ρs > 1 by its deﬁnition (19), then for any t = 1, . . . , m − 1,

ct,s =

.

By (29) and Lemma 3,

ct,s ≤ c1,s <

((ρs)m − 1)η(cid:15),sL3
βs + 2η(cid:15),sL2

.

(ρs)m = (cid:0)1 + bη(cid:15),sβs + 2bη2

(cid:15),sL2(cid:1)m
≤ 1 + 2mbη(cid:15),s(βs + 2η(cid:15),sL2).

Plugging (32) into (31), and by the deﬁnition of (23) imply

By (33) and (30), there holds

This yields (22).

ct,s <

1
2

βs, t = 1, . . . , m, s = 0, . . . , S − 1.

bη(cid:15),s(L + 2ct+1,s) + ct+1,sβ−1

s < bη(cid:15),s(L + βs) + 1/2 < 1.

(31)

(32)

(33)

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 0, NO. 0, AUGUST 2019

15

(b) In the next, we show that (24) implies (29) and (30). By the deﬁnition of η(cid:15),s (17) and the L-smoothness of f , there

holds

Note that

1
mL

< η(cid:15),s <

1
m(cid:15)

.

(34)

bη(cid:15),sβs + 2bη2

(cid:15),sL2 = 4mb2η3

(cid:15),sL3 + 2bη2

(cid:15),sL2 ≤

(cid:34)

1
m

(cid:19)2

4L3
(cid:15)3 ·

(cid:18) b
m

+

2L2
(cid:15)2 ·

b
m

(cid:35)

<

1
m

,

where the ﬁrst inequality holds for (34), and the ﬁnal inequality follows from (24) (i.e., b
inequality yields (29). Similarly, note that

m <

√

(cid:15)2
1+4(cid:15)L−1) ). The above

L2(1+

bη(cid:15),sβs + bη(cid:15),sL = 4mb2η3

(cid:15),sL3 + bη(cid:15),sL ≤

(cid:19)2

4L3
(cid:15)3 ·

(cid:18) b
m

+

(cid:19)

L
(cid:15)

(cid:18) b
m

·

<

1
4

<

1
2

,

(35)

where the ﬁrst inequality holds for (34), and the ﬁnal inequality follows from (24) (i.e., b/m < (cid:15)/2L(1 + (cid:112)4L/(cid:15) + 1)). The
above inequality implies (30).

Furthermore, by (33) and the deﬁnitions of Γt,s (21) and βs (23), there holds

Γt,s ≥ bη(cid:15),s

(cid:21)
− bη(cid:15),s(L + βs)

≥

(cid:20) 1
2

1
4

bη(cid:15),s,

where the second inequality follows from (35). Thus,

γS :=

min
1≤t≤m,0≤s≤S−1

Γt,s ≥

1
4

b min

0≤s≤S−1

η(cid:15),s.

By Theorem 1, we conclude (25).

D. PROOF OF THEOREM 3

Proof. According to Theorem 2, the following holds begin

E[(cid:107)∇f (˜xk)(cid:107)2] ≤

4E[f (˜xk−1) − f (x∗)]
mSbη(cid:15),min

≤

2E[(cid:107)∇f (˜xk−1)(cid:107)2]
λmSbη(cid:15),min

≤ ρE[(cid:107)∇f (˜xk−1)(cid:107)2]

(36)

where the second inequality holds for the PL property. The above inequality implies (27). By the PL property of f again,

we have

This together with (36) yields (28).

E[(cid:107)∇f (˜xk)(cid:107)2] ≥ 2λE[f (˜xk) − f (x∗)].

Next, we provide some details including the convex and non-convex formulation of GOE, some discussion of the GOE
framework and the proof for the lemma, propositions and theorems we proposed in the main paper. The numbering of
equations and the reference follows that of the main paper.

E. Convex and Non-convex Formulation of GOE

First, we revise some existing classiﬁcation based ordinal embedding methods and verify that they are all the speciﬁc
cases of generalized ordinal embedding (5). As the dissimilarity functions in these method are all adopted as the squared
Euclidean distance (cid:107) · (cid:107)2

2, we adopt the matrix X ∈ Rp×n to represent the embedding X for description clarity.

First of all, we introduce the Gram matrix G to linearize the distance calculation. We assume the matrix X =

{x1, . . . , xn} ∈ Rp×n is centered at the origin as

and deﬁne the centering matrix C

n
(cid:88)

i=1

xi = 0,

C = I −

1
n

1 · 1(cid:62),

(37)

(38)

where 1 is a n-dimensional all-one column vector. With (37), we immediately have XC = X. We call the Gram matrix
G = X (cid:62)X is also “centered” if X is a centered matrix which satisﬁes (37), and it holds that

G = X (cid:62)X = C(cid:62)X (cid:62)XC = C(cid:62)GC.

(39)

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 0, NO. 0, AUGUST 2019

16

Given a centered matrix X, we can establish a bijection between the Gram matrix G and the squared Euclidean distance
matrix D as

G = −

1
2

CDC,

D = diag(G) · 1(cid:62) − 2G + 1 · diag(G)(cid:62),

(40a)

(40b)

where the diagonal of G composes of the column vector diag(G). We refer [39] for the further properties of the squared
Euclidean matrix D. The bijection (40) indicates that

d2(xi, xj) = gii − 2gij + gjj,
where xi is the ith column of X, gij is the (i, j) element of G. Then, we can express the partial order {(φij, φlk)} or
{(d2

lk)} as linear inequalities on the Gram matrix:

ij, d2

(41)

Then we rewrite the GOE problem (5) as

ij < d2
d2
lk
⇔ gii − 2gij + gjj < gll − 2glk + gkk.

arg min
+, rank(G)≤p

G∈Sn

LQ,h(G, YQ),

(42a)

(42b)

(8)

where Sn
+ is the n-dimensional positive semi-deﬁnite cone, the set of all symmetric positive semideﬁnite matrices in Rn×n;
the rank constraint comes from the fact that rank(G) ≤ rank(X) ≤ min(n, p) = p. With G = X (cid:62)X, X can be determined
if we obtained G up to a unitary transformation. Since the deviation between d2
lk which is the input
of classiﬁer h, can be calculated by the linear operations on G, and the constraints like (42b) are all linear, (8) is always
a convex optimization if we choose the convex classiﬁer h and the convex loss function (cid:96). This is the main advantage of
optimizing G instead of X. We note

ij and d2

ij − d2

lk, d2

∆qG = (cid:104)W q, G(cid:105) = tr(W qG)

= gii − 2gij + gjj − gll + 2glk − gkk

(43)

where W q is a n × n matrix and has zero entry everywhere except on the entries corresponding to q = (i, j, l, k) which
has the form

W q =

,

(44)

l
i
j
0
1 −1
1
0
0 −1
0

k

0
0


1
1 −1


i
j −1


0
l
0
k

and (cid:104)W q, G(cid:105) (cid:44) tr(W qG) = vec(W q)(cid:62)vec(G) for any compatible matrices.

The Gram matrix G is introduced by the well-known Generalized Non-metric Multidimensional Scaling (GNMDS) [19].
GNMDS obtains the embedding X by a “SVM”-type algorithm. It adopts the soft-margin classiﬁer and the hinge loss in
(8) as

minimize
{ξq}, G

(cid:88)

q∈Q

ξq,

subject to ∆qG ≤ 1 − ξq, ξq ≥ 0

G (cid:23) 0, rank(G) ≤ p,

n
(cid:88)

i,j=1

gij = 0,

where (cid:80)n

i,j=1 gij = 0 is the constraints for the ‘centered’ G as G = X (cid:62)X and

(cid:32) n
(cid:88)

(cid:33)(cid:62) (cid:32) n
(cid:88)

(cid:33)

xi

=

xi

0 =

n
(cid:88)

n
(cid:88)

x(cid:62)

i xj =

i=1

i=1

i=1

j=1

n
(cid:88)

n
(cid:88)

i=1

j=1

gij.

(45)

(46)

The Crowd Kernel Learning (CKL) [20], Stochastic Triplet Embedding (STE) and t-Distributed STE (TSTE) [21] solve the
GOE problem by employing probabilistic models

pckl
q =

exp(d2

pste
q =

exp(−d2

,

lk)

exp(d2
lk)
ij) + exp(d2
exp(−d2
ij)
ij) + exp(−d2

lk)

,

(47)

(48)

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 0, NO. 0, AUGUST 2019

and

ptste
q =

(cid:19)− α+1

2

(cid:18)

1 +

d2
ij
α

(cid:16)

1 +

d2
ij
α

(cid:17)− α+1

2

(cid:16)

+

1 + d2
lk
α

(cid:17)− α+1

2

with threshold t = 0.5 as the classiﬁers and the logistic loss like

minimize
G

(cid:88)

q∈Q

log(1 + kernel(∆qG))

subject to G (cid:23) 0, rank(G) ≤ p,

n
(cid:88)

i,j=1

gij = 0.

17

(49)

(50)

where kernel(·) can be adopted as the Gaussian kernel and the Student-t kernel with α degrees of freedom.

Although the semi-deﬁnite positive programming (8) is a convex optimization problem, there exist some disadvantages
on obtaining the embedding X from the Gram matrix G: (i) the positive semi-deﬁnite constraint on Gram matrix, G (cid:23) 0,
needs project G onto PSD cone S+, which is performed by the expensive singular value decomposition in each iteration
due to the subspace spanned by the non-negative eigenvectors satisﬁes the constraint, is a computational bottleneck of
optimization; (ii) the embedding dimension is p (cid:28) n and we hope that rank(G) ≤ p. If rank(G) (cid:29) p, the freedom degree
of G is much larger than X with over-ﬁtting. Although G is a global optimal solution of (8), the subspace spanned by
the largest p eigenvectors of G also produce less accurate embedding. We can tune the regularization parameter λ to
force {Gt}, t = 1, . . . , T generated by the optimization algorithms to be low-rank and cross-validation is the most utilized
technology. This also needs extra computational cost. In summary, projection and parameter tuning render gradient descent
methods computationally prohibitive for learning the embedding X with ordinal information Q. To overcome these
challenges, we will exploit the non-convex and stochastic optimization techniques for the ordinal embedding problem.
Here the non-convex formulation of ordinal embedding will replace the Gram matrix G with the distance matrix D which
directly solving the embedded matrix X

arg min
X∈Rp×n

LQ,h(X, YQ),

(5)

where the loss and the classiﬁer can be adopted as the same as the convex formulation. The instance for classier h in (5) is
∆qD(X)

∆qD(X) = d2

ij − d2
lk
= (cid:107)xi − xj(cid:107)2

2 − (cid:107)xl − xk(cid:107)2
2.

(51)

F. DISCUSSION OF GOE
The existed literatures [7], [8], [9], [19], [20], [21], [28] are all obtained the embedding X in the Euclidean space as it assume
that the embedding X is lack of prior knowledge of O or the true dissimilarity function ψ is unknown. For fair comparison,
we focus on embedding O into Euclidean space Rp by classifying the instances indexed by Q. That is to say, X ⊂ Rp and
the dissimilarity function φ of X is chosen as the squared Euclidean distance d2
2. It is a very
interesting direction to ﬁnd helpful constraints in O or the prior knowledge of the true dissimilarity function ψ in real
applications. We leave this as one of our future works.

ij = d2(xi, xj) = (cid:107)xi − xj(cid:107)2

In practice, the class label set YQ are typically obtained by the crowdsourcing system or questionnaire survey. The
comparison results are inferred by combining answers from multiple human annotators. So these ordinal constraints could
not be consistence with the true dissimilarity relationship of O, which make YQ contain noise. Here we focus on the
problem which is provided single class label yq with a quadruplet (i, j, l, k) to obtain the embedding. Using multiple
inconsistence labels {y1
q} to estimate the embedding X is one of our future works.

q , . . . , ys

The desired embedding dimension p is a parameter of the ordinal embedding. It is well known that there exists a perfect
embedding X estimated by any label set Y on the Euclidean distances in Rn−2, even for the noisy constraints. Here we
consider the low-dimensional setting where p (cid:28) n. The optimal or smallest p for noisy ordinal constraints YQ is another
future work. The choices of p in experiment section differ form the applications.

We notice that the label set YQ carries the distance comparison information about D, but D is invariant to the so-
called similarity transformations, isotonic transformations or Procrustes transformations. It means that the embedding X ∈ Rp×n
obtained by YQ is not unique as we rotate, reﬂect, translate, or scale X in Euclidean space and the new matrix X (cid:48)
also consist with the same constraints YQ. Without loss of generality, we assume the points x1, . . . , xn are centered at the
origin. Even disregarding similarity transformations, the ordinal embedding X is still not unique. Points of the embedding,
{x1, . . . , xn}, can be perturbed slightly without changing their distance ordering, and so without violating any constraints.
Kleindessner and von Luxburg [7] proved the long-standing conjecture that, under mind conditions, an embedding of a
sufﬁciently large number of objects which preserves the total ordering of pairwise distances between all objects must place
all objects to within ε of their correct positions, where ε → 0 as n → ∞. However, we focus on the dimension reduction

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 0, NO. 0, AUGUST 2019

18

setting as p (cid:28) n and O is always a ﬁnite set in real application. The embedding X or X could not be unique. Therefore,
we adopt the classiﬁcation metric to evaluate the quality of the estimated embedding.

F. LIPSCHITZ DIFFERENTIABILITY OF GOE
Note that the Lipschitz differentiability of the objective function is crucial for the establishment of the convergence rate of
SVRG-SBB in Theorem 1. In the following, we give a lemma to show that a part of aforementioned objective functions in
the GOE problem are Lipschitz differentiable.

Lemma 4. The ordinal embedding functions are Lipschitz differentiable for any bounded variable X.

Proof. Let

X q =

(cid:19)

(cid:18)X 1
X 2

=

where X 1 = [xT

i , xT

j ]T , X 2 = [xT

l , xT

k ]T and













xi
xj
xl
xk

(cid:18) M 1

M =

(cid:19)

=







M 2

I −I
I

−I







−I

I
I −I

where I d×d is the identity matrix.

The ﬁrst-order gradient of STE is

and the second-order Hessian matrix of STE is

∇qf ste(X) = 2

exp(d2
1 + exp(d2

ij(X) − d2

lk(X))

ij(X) − d2

lk(X))

M X q

(52)

(53)

(54)

(55)

M .

∇2

qf ste(X) =

4 exp(d2
[1 + exp(d2

lk(X))
lk(X))]2 M X qX T
qf ste(X) are bounded. So the eigenvalues of ∇2

ij(X) − d2
ij(X) − d2

q M +

2 exp(d2
1 + exp(d2

ij(X) − d2
ij(X) − d2

lk(X))
lk(X))

All elements of ∇2
qf ste(X) are bounded. By the deﬁnition of Lipschitz
continuity, STE has Lipschitz continuous gradient with bounded X. As the loss function of CKL is very similar to STE, we
omit the derivation of CKL.

The ﬁrst-order gradient of GNMDS is

∇qf gnmds(X) =

(cid:26)

0,
2M X q,

ij(X) + 1 − d2

if d2
otherwise,

lk(X) < 0,

and the second-order Hessian matrix of GNMDS is

∇2

qf gnmds(X) =

(cid:26) 0,
2M ,

ij(X) + 1 − d2

if d2
otherwise.

lk(X) < 0,

(56)

(57)

ij(X) + 1 − d2

If d2
lk(X) (cid:54)= 0 for all q ∈ Q, ∇qf gnmds(X) is continuous on {xi, xj, xl, xk} and the Hessian matrix
∇2
qf gnmds(X) has bounded eigenvalues. So GNMDS has Lipschitz continuous gradient in some quadruple set as
{xi, xj, xl, xk} ⊂ Rp×4. As the special case of q = (i, j, l, k) which satisﬁed d2
lk(X) = 0 is exceedingly
rare, we split the embedding X into pieces {xi, xj, xl, xk} and employ SGD, SVRG and SVRG-SBB to optimize the
objective function of GNMDS. The empirical results are showed in the experiment section.

ij(X) + 1 − d2

Note sij = α + d2

ij(X), and the ﬁrst-order gradient of TSTE is

∇qf tste(X) =

(cid:18) M 1X 1

α + 1
sij

(cid:19)

−

O

α−α(α + 1)
+ s− α+1

s− α+1

2

2

ij

lk





s− α+3
ij M 1X 1

2





s− α+3
lk M 2X 2

2

The second-order Hessian matrix of TSTE is

∇2

qf tste(X) =

α + 1
s2
ij

(cid:18) sijM 1 − 2M 1X 1X T

1 M 1

(cid:19)

−

O

α−α(α + 1)
+ s− α+1

s− α+1

2

2

ij

lk





s− α+3
ij M 1

2

(cid:33)





s− α+3
lk M 2

2

+

(cid:18)

α−α(α + 1)2
+ s− α+1

s− α+1

2

2

ij

lk

(cid:32)

(cid:19)2

s−(α+3)
ij M 1X 1X T

1 M 1

+

α−α(α + 1)(α + 3)

s− α+1

2

ij

+ s− α+1

2

lk





s− α+5
ij M 1X 1X T

2

1 M 1

s−(α+3)
lk M 2X 2X T

2 M 2

s− α+5
lk M 2X 2X T

2

2 M 2





(58)

(59)

IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 0, NO. 0, AUGUST 2019

19

The boundedness of eigenvalues of The loss function of ∇2
continuous gradient with bounded X.

pf tste(X) can infer that the TSTE loss function has Lipschitz

We focus on a special case of quadruple comparisons as i = l and {i, j, i, k} ⊂ [n]3 in the Experiment section. To verify
the Lipschitz continuous gradient of ordinal embedding objective functions with c = (i, j, i, k) as i = l, we introduction
the matrix A as



A =



I 0 0 0
0 I 0 I
0 0 0 I


 .

(60)

By chain rule for computing the derivative, we have

∇fijk(X) = A∇fijlk(X),
∇2fijk(X) = A∇2fijlk(X)AT .
where l = i. As A is a constant matrix and ∇2fijlk(X) is bounded, all elements of the Hessian matrix ∇2fijk(X) are
bounded. So the eigenvalues of ∇2fijk(X) is also bounded. The ordianl embedding functions of CKL, STE and TSTE with
triplewise compsrisons have Lipschitz continuous gradient with bounded X.

(61)

