0
2
0
2

r
a

M
4

]

G
L
.
s
c
[

1
v
9
8
1
2
0
.
3
0
0
2
:
v
i
X
r
a

Exploration-Exploitation in Constrained MDPs

Yonathan Efroni1

Shie Mannor1

Matteo Pirotta2

1Technion, Israel, 2Facebook AI Research

March 5, 2020

Abstract

In many sequential decision-making problems, the goal is to optimize a utility function while
satisfying a set of constraints on diﬀerent utilities. This learning problem is formalized through
Constrained Markov Decision Processes (CMDPs).
In this paper, we investigate the exploration-
exploitation dilemma in CMDPs. While learning in an unknown CMDP, an agent should trade-oﬀ
exploration to discover new information about the MDP, and exploitation of the current knowledge
to maximize the reward while satisfying the constraints. While the agent will eventually learn a good
or optimal policy, we do not want the agent to violate the constraints too often during the learning
process. In this work, we analyze two approaches for learning in CMDPs. The ﬁrst approach lever-
ages the linear formulation of CMDP to perform optimistic planning at each episode. The second
approach leverages the dual formulation (or saddle-point formulation) of CMDP to perform incre-
mental, optimistic updates of the primal and dual variables. We show that both achieves sublinear
regret w.r.t. the main utility while having a sublinear regret on the constraint violations. That being
said, we highlight a crucial diﬀerence between the two approaches; the linear programming approach
results in stronger guarantees than in the dual formulation based approach.

Contents

1 Introduction

1.1 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 Preliminaries

2.1 Finite-Horizon Constrained MDPs
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 The Learning Problem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3 Linear Programming for CMDPs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.4 Notations and Deﬁnitions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3 Upper Conﬁdence Bounds for CMDPs

4 Exploration Bonus for CMDPs

5 Optimistic Dual and Primal-Dual Approaches for CMDPs

5.1 Optimistic Dual Algorithm for CMDPs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.2 Optimistic Primal Dual approach for CMDPs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6 Conclusions and Summary

A Optimistic Algorithm based on Bounded Parameter CMDPs

A.1 Failure Events . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A.2 Optimism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A.3 Proof of Theorem 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

B Optimistic Algorithm based on Exploration Bonus

B.1 Failure Events . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
B.2 Optimism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
B.3 Proof of Theorem 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

C Constraint MDPs Dual Approach

C.1 Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
C.2 Failure Events . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
C.3 Proof of Theorem 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1

2
3

4
4
6
6
7

7

10

11
11
12

14

16
17
18
18

19
20
20
22

22
23
23
23

 
 
 
 
 
 
D Constraint MDPs Primal Dual Approach

D.1 Failure Events . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
D.2 Optimality and Optimism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
D.3 Proof of Theorem 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

E Bounds of On-Policy Errors

F Useful Lemmas

F.1 Online Mirror Descent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

G Useful Results from Constraint Convex Optimization

26
26
27
30

33

40
42

42

1 Introduction

Markov Decision Processes (MDPs) have been successfully used to model several applications, including
video games, robotics, recommender systems and many more. However, MDPs do not take into account
additional constrains that can aﬀect the optimal policy and the learning process. For example, while
driving, we want to reach our destination but we want to avoid to go oﬀ-road, overcome the speed limits,
collide with other cars [Garcıa and Fern´andez, 2015]. Constrained MDPs [Altman, 1999] extend MDPs
to handle constraints on the long term performance of the policy. A learning agent in a CMDP has to
maximize the cumulative reward while satisfying all the constraints. Clearly, the optimal solution of a
CMDP is diﬀerent than the one of an MDP when at least one constraint is active. Then, the optimal
policy, among the set of policies which satisﬁes the constraint, is stochastic.

In this paper, we focus on the online learning problem of CMDPs. While interacting with an unknown
MDP, the agent has to trade-oﬀ exploration to gather information about the system and exploration to
maximize the cumulative reward. Performing such exploration in a CMDP may be unsafe since may lead
to numerous violations of the constraints. Since the constraints depend on the long term performance
of the agent and the CMDP is unknown, the agent cannot exactly evaluate the constraints. It can only
exploit the current information to build an estimate of the constraints. The objective is thus to design
an algorithm with a small number of violations of the constraints.

Objective and Contributions. The objective of this technical report is to provide an extensive anal-
ysis of exploration strategies for tabular constrained MDPs with ﬁnite-horizon cost. Similar to [Agrawal and Devanur,
2019], we allow the agent to violate the constraints over the learning process but we require the cumu-
lative cost of constraint violations to be small (i.e., sublinear). Opposite to [Zheng and Ratliﬀ, 2020],
we consider the CMDP to be unknown, i.e., the agent does not know the transition kernel, the reward
function and the constraints.

The performance of the learning agent is measured through the regret, that accounts for the diﬀerence
in executing the optimal policy and the learning agent. We deﬁne two regrets: i) the regret w.r.t. to
the main objective (as in standard MDP), ii) the regret w.r.t. the constraint violations. These terms
account for both convergence to the optimal policy and cumulative cost for violations of the constraints.
We introduce and analyze the following exploration strategies:

OptCMDP leverages the ideas of UCRL2 [Jaksch et al., 2010]. At each episodes, it builds a set of plausible
CMDPs compatible with the observed samples, and plays the optimal policy of the CMDP with
the lowest cost (i.e., optimistic CMDP). To solve this planning problem, we introduce an extended
linear programming (LP) problem in the space of occupancy measures. The important property is
that there always exists a feasible solution of this extended LP.

OptCMDP-bonus merges the uncertainties about costs and transitions used by OptCMDP into an explo-
ration bonus. As a consequence, OptCMDP-bonus solves a single (optimistic) CMDP rather than
planning in the space of plausible CMDPs. This leads to a more computationally eﬃcient algorithm.
In fact, this planning problem can be solved through an LP with O(SAH) constraints and decision
variables, a factor O(S) smaller than the LP solved by OptCMDP.

OptDual-CMDP leverages the saddle-point formulation of constrained MDP [e.g., Altman, 1999]. It solves
this problem using an optimistic version of the dual projected sub-gradient algorithm (e.g., Beck
2017). At each episode, OptDual-CMDP solves an optimistic MDP deﬁned using the estimated
Lagrangian multiplier. Then, it uses the computed solution to update the Lagrange multipliers via

2

Algorithm

OptCMDP

OptCMDP-bonus

OptDual-CMDP

Reg

OptPrimalDual-CMDP

Reg

≤

Optimality Regret

Constraint Regret

Reg+ ≤
Reg+ ≤
O(cid:16)p
≤
e
(S
O(cid:16)p
e

N

√S

√S

H 4K(cid:17)
H 4K(cid:17)
H 2 + ρ2I)H 2K(cid:17)

O(cid:16)
e
O(cid:16)
e
(S
N
H 2 + ρ2I 2H 2)H 2K(cid:17) Reg

N

N

Reg+ ≤
O(cid:16)
e
Reg+ ≤
O(cid:16)
e
O(cid:16)(1 + 1
Reg
≤
e
O(cid:16)(1 + 1
ρ )√IS
e

≤

H 4K(cid:17)
H 4K(cid:17)

√S

√S

N

N
ρ )√IS

N

H 4K(cid:17)
H 4K + I√H 4K(cid:17)

N

Table 1: Summary of the regret bounds obtained in this work. Algorithms OptCMDP, OptCMDP-bonus,
OptDual-CMDP, OptPrimalDual-CMDP are formulated and analyzed in sections 3, 4, 5.1, 5.2, respectively.
The constant term, which is omitted from the table, of OptCMDP-bonus is signiﬁcantly worse than the
one of OptCMDP. Notice that diﬀerent types of regrets are bounded (see Section 2 for deﬁnitions).

projected sub-gradient. The main advantage of this algorithm needs to solve a simple optimistic
planning problem for MDPs (rather than for CMDPs).

OptPrimalDual-CMDP exploits a primal-dual algorithm to solve the saddle-point problem associated to
a CMDP. It performs incremental updates both on the primal and dual variables. It uses mirror
descent to update the Q-function (thus the policy) and projected subgradient descent to update the
Lagrange multipliers. Similarly to OptCMDP-bonus, this algorithm exploits an exploration bonus
for both cost and constraint costs. This allows to use a simple dynamic programming approach to
compute the Q-functions (no need to solve a constrained optimization problem).

For all the proposed algorithms, we provide an upper-bound to the regret and the cumulative con-
straint violations (see Tab. 1). While the incremental algorithms (OptDual-CMDP and OptPrimalDual-CMDP)
may be more amenable for practical applications, they present limitations from a theoretical perspective.
In fact, we were able to prove weaker guarantees for the Lagrangian approaches compared to UCRL-like
algorithms (i.e., OptCMDP and OptCMDP-bonus). While for UCRL-like algorithms we can bound the sum
of positive errors, for Lagrangian algorithms we were able to bound only the cumulative (signed) error.
This weaker term allows for “cancellation of errors” (see discussion in Sec. 2.2). Whether it is possible
to provide stronger guarantees is left as an open question. Despite this, we think that the analysis of
Lagrangian approaches is important since it is at the core of many practical algorithms. For example,
the Lagrangian formulation of CMDPs has been used in [Tessler et al., 2019, Paternain et al., 2019], but
never analyzed from a regret perspective.

1.1 Related Work

The problem of online learning under constraints (with guarantees) have been analyzed both in ban-
dits and in RL. Conservative exploration focuses on the problem of learning an optimal policy while
satisfying a constrained w.r.t. to a predeﬁned baseline policy. This problem can be seen as a speciﬁc
instance of CMDPs where the constraint is that the policy should perform (in the long run) better than a
predeﬁned baseline policy. Conservative exploration has been analyzed both in bandits [Wu et al., 2016,
Kazerouni et al., 2017, Garcelon et al., 2020a] and in RL [Garcelon et al., 2020b]. All these algorithms
are able to guarantee that the performance of the learning agent is at least as good as the one of the
baseline policy with high probability at any time.1 While they enjoy strong theoretical guarantees, they
performs poorly in practice since are too conservative. In fact, the idea of these algorithms is to build
budget (e.g., by playing the baseline policy) in order to be able to take standard exploratory actions.
Concurrently to this paper, [Zheng and Ratliﬀ, 2020] has extended conservative exploration to CMDP
with average reward objective. They assume that the transition functions are known, but the rewards
and costs (i.e., the constraints) are unknown. The goal is thus to guarantee that, at any time, the policy
executed by the agent satisﬁes the constraints with high probability. These requirement poses several
limitations. Similarly to [Garcelon et al., 2020b], they need to assume that the MDP is ergodic and
that the initial policy is safe (i.e., satisﬁes the constraints). Furthermore, despite the theoretical guaran-
tees, this approach is not practical due to these strong requirements/assumptions. Agrawal and Devanur
[2019] studied the exploration problem for bandits under constraints as well as bandits with knapsack
constraints [Badanidiyuru et al., 2013]. Algorithms OptCMDP and OptCMDP-bonus can be understood as

1To guarantee this the allow the performance of the learning agent to be α-away from the baseline performance.

3

generalizing their bandit setting to an CMDP setting. That being said, in the following we derive regret
bounds on a stronger type of regret relatively to Agrawal and Devanur [2019] (see Remark 1).

There are several approaches in the literature that have focused on (approximately) solving CMDPs.
These methods are mainly based on Lagrangian-formulation [Bhatnagar and Lakshmanan, 2012, Chow et al.,
2017, Tessler et al., 2019, Paternain et al., 2019] or constrained optimization [Achiam et al., 2017]. Lagrangian-
based methods formulate the CMDP optimization problem as a saddle-point problem and optimize
it using primal-dual algorithms. While these algorithms may eventually converge to the true policy,
they have no guarantees on the policies recovered during the learning process. Constrained Policy
Optimization (CPO) [Achiam et al., 2017] leverages the intuition behind conservative approaches [e.g.,
Kakade and Langford, 2002] to force the policy to improve overtime. This is a practical implementation
of conservative exploration where the baseline policy is updated at each iteration.

Another way to solve CMDPs and guarantee safety during learning is through Lyapunov func-
tions [Chow et al., 2018, 2019]. Despite the fact that some of these algorithms are approximately safe over
the learning process, analysing the convergence is challenging and the regret analysis is lacking. Other
approaches use Gaussian processes to model the dynamics and/or the value function [Berkenkamp et al.,
2017, Wachi et al., 2018, Koller et al., 2018, Cheng et al., 2019] in order to be able to estimate the con-
straints and (approximately) guarantee safety over learning.

A related approach is the literature about budget learning in bandits [e.g., Ding et al., 2013, Combes et al.,

2015]. In this setting, the agent is provided with a budget (known and ﬁx in advance) and the learning
process is stopped as soon as the budget is consumed. The goal is to learn how to eﬃciently handle the
budget in order to maximize the cumulative reward. A widely studied case of budget bandit is bandit
with knapsack [e.g., Agrawal and Devanur, 2014, Badanidiyuru et al., 2018]. In our setting, we do not
have a “real” concept of budget and the length of the learning process does not depend on the total cost
of constraint violations. This paper is also related to learning with fairness constraints [e.g., Joseph et al.,
2016]. Similarly to conservative exploration, fairness constraints can be sometimes formulated as a speciﬁc
instance of CMDPs.

2 Preliminaries

We start introducing ﬁnite-horizon Markov Decision Processes (MDPs) and their constrained version.
We deﬁne [N ] :=

for all N

1, . . . , N

N.

{

,
}

∈

2.1 Finite-Horizon Constrained MDPs

Finite Horizon MDPs. We consider ﬁnite-horizon MDPs with time-dependent dynamics [Puterman,
1994]. A ﬁnite-horizon constraint MDP is deﬁned by the tuple
are
M
the state and action spaces with cardinalities S and A, respectively. The non-stationary immediate cost for
[0, 1] with expectation ECh(s, a) = ch(s, a).
taking an action a at state s is a random variable Ch(s, a)
s, a), the probability of transitioning to state s′ upon taking action
The transition probability is ph(s′
N
a at state s at time-step h. The initial state in each episode is chosen to be the same state s1 and H
s′ : ph(s′
is the maximum number of non-zero
is the horizon. Furthermore,
N
transition probabilities across the entire state-action pairs.

:= maxs,a,h |{

, c, p, s1, H), where

s, a) > 0

and

= (

A

A

}|

∈

∈

S

S

|

|

,

A Markov non-stationary randomized policy π = (π1, π2, . . . , πH )
. We denote by ah ∼
A
∈

states to probabilities ∆A on the action set
at time h at state sh according to a policy π. For any h
function of a non-stationary policy π = (π1, . . . , πH ) is deﬁned as

[H] and (s, a)

∈

ΠMR where πi :

∆A maps
π(sh, h) := πh(sh), the action taken
, the state-action value

S →

∈ S × A

Qπ

h(s, a) = ch(s, a) + E

H

cl(sl, al)

|

"

sh = s, ah = a, π, p

#

s)Qπ

a πh(a

Xl=h+1
where the expectation is over the environment and policy randomness. The value function is V π

h (s) =
h(s, a). Since the horizon is ﬁnite, under some regularity conditions, [Shreve and Bertsekas,
1978], there always exists an optimal Markov non-stationary deterministic policy π⋆ whose value and
P
action-value functions are deﬁned as V ⋆
h (s, a) =
supπ Qπ
h(s, a). The Bellman principle of optimality (or Bellman optimality equation) allows to eﬃciently
compute the optimal solution of an MDP using backward induction:

h (s) = supπ V π

h(s, a) := Qπ⋆

h (s) := V π⋆

h (s) and Q⋆

|

V ⋆
h (s) = min
a∈A

ch(s, a) + Es′∼ph(·|s,a)[V ⋆

h+1(s′)]

(cid:8)

, Q⋆

h(s, a) = ch(s, a) + Es′∼ph(·|s,a)[V ⋆

h+1(s′)]

(1)

(cid:9)

4

∈ S

and V ⋆

h (s) = mina Q⋆

H+1(s) := 0 for any s

where V ⋆
greedy w.r.t. V ⋆
h, V π
functions Qπ

h is thus
h [e.g., Puterman, 1994]. Notice that by boundedness of the cost, for any h and (s, a), all
h , Q⋆

h + 1].
We can reformulate the optimization problem by using the occupancy measure [e.g., Puterman, 1994,
Altman, 1999]. The occupancy measure qπ of a policy π is deﬁned as the set of distributions generated
by executing the policy π in the ﬁnite-horizon MDP

[e.g., Zimin and Neu, 2013]:

h are bounded in [0, H

. The optimal policy π⋆

h(s, a), for all s

h, V ⋆

∈ S

−

h (s, a; p) := E[1{
qπ

sh = s, ah = a

M
s1 = s1, p, π] = Pr
{

} |

sh = s, ah = a

s1 = s1, p, π

|

.
}

RHSA where its (s, a, h) element is given by
For ease of notation, we deﬁne the matrix notation qπ(p)
qπ
h (s, a; p). This implies the following relation between the occupancy measure and the value of a policy:

∈

V π
1 (s1; p, c) =

qπ
h (s, a; p)ch(s, a) := cT qπ(p).

(2)

where c

∈

RHSA such that element (s, a, h) element is given by ch(s, a).

Proof. The value function V π

1 (s1; p, c) is given by the following equivalent relations.

Xh,s,a

H

E

"

Xh=1
H

ch(sh, ah)

|

s1 = s1, π, p

=

#

H

Xh=1

E [ch(sh, ah)

s1 = s1, π, p]

|

ch(s, a) Pr
{

sh = s, ah = a

s1 = s1, p, π

}

|

=

H

s,a
Xh=1 X

ch(s, a)qπ

h (s, a; p) = cT qπ(p),

s,a
Xh=1 X

where the ﬁrst relation holds by linearity of expectation.

Finite Horizon Constraint MDPs. A constraint MDP [Altman, 1999] is an MDP supplied with a
RSAH and αi ∈
[0, H]. The immediate ith constraint when
set of I constraints
taking an action a from state s at time-step h is random variable Di(s, a)
[0, 1] with expectation
E[Di,h(s, a)] = di,h(s, a). The expected cost of the ith constraint violation from state s at time-step h is
deﬁned as

i=1, where di ∈

di, αi}

∈

{

I

H

h (s; p, di) := E
V π
"

di,h′ (sh′, ah′ )

sh = s, p, π

|

.
#

Xh′=h

Similarly to (2), we can rewrite the constraint in terms of occupancy measure: V π
Notice that by boundedness of the constraint cost, for any h, i and (s, a), all functions Qπ
V π
h (s; di, p), Q⋆
ﬁnd a policy minimizing the cost while satisfying all the constraints. Formally,

i qπ(p).
h(s, a; di, p),
h + 1]. The objective of a CMDP is to

h (s, ; di, p) are bounded in [0, H

h(s, a; di, p), V ⋆

h (s; p, di) = dT

−

cT qπ(p)

π⋆

∈

arg min
π∈ΠMR
s.t. Dqπ(p)

α,

≤

(3)

where D

∈

RI×SAH and α

RI such that

∈

D = 

, α = 



,



dT
1
...
dT
I







α1
...
αI







The optimal value is the value of π⋆ from the initial state, i.e., V ⋆

1 (s1) := V π⋆

1 (s1; p, c).

Assumption 1 (Feasibility). The unknown CMDP is feasible, i.e., there exists an unknown policy π
ΠMR which satisﬁes the constraints. Thus, an optimal policy exists as well.

∈

It is important to stress that the optimal policy of a CMDP may be stochastic [e.g., Altman, 1999],
i.e., may not exist an optimal deterministic policy. In fact, due to the constraints, the Bellman optimality
principle, see Eq. 1, may not hold anymore. This means that we cannot leverage backward induction
and the greedy operator. Altman [1999] showed that it is possible to compute the optimal policy of a
constrained problem by using linear programming. We will review this approach in Sec. 2.3.

5

2.2 The Learning Problem.

We consider an agent which repeatedly interacts with a CMDP in a sequence of K episodes of ﬁxed length
H by playing a non-stationary policy πk = (π1k, . . . , πHk) where πhk :
∆A. Each episode k starts
from the ﬁxed initial state sk
1 = s1. The learning agent does not know the transition or reward functions,
and it relies on the samples (i.e., trajectories) observed over episodes to improve its performance over
time.

S →

The performance of the agent is measured using multiple objectives: i) the regret relatively to the
In sections 3 and 4 we analyze

value of the best policy, and ii) the amount of constraint violations.
algorithms with guarantees on the following type of regrets

Reg+(K; c) =

K

Xk=1

Reg+(K; d) = max
i∈[I]

[V πk

1 (s1; p, c)

V ⋆
1 (s1)]+

−

K

Xk=1

[V πk

1 (s1; p, di)

αi]+,

−

(4)

(5)

where [x]+ := max
{
of the constraints.

0, x
}

. The term Reg+(K; d) represents the maximum cumulative cost for violations

We later continue and analyze algorithms with reduced computational complexity in sections 5.1
[K] with respect to a weaker

and 5.2. For these algorithms, we supply regret guarantees for all K ′
measure of regrets deﬁned as follows.

∈

Reg(K; c) =

K

Xk=1

V πk
1 (s1; p, c)

V ⋆
1 (s1)

−

K

Reg(K; d) = max

i∈[I]"

Xk=1

V πk
1 (s1; p, di)

αi

.

#

−

(6)

(7)

Remark 1. Note that in our setting, the immediate regret V πk
V ⋆
1 (s1) might be negative since
policy πk might violate the constraints. For this reason, bounding the regret as Reg+(K; c) is stronger
than bounding Reg+(K; c) in the sense that the a bound on the ﬁrst implies a bound on the latter; but
not vice-versa.

1 (s1; p, c)

−

Similar relation holds between the two deﬁnitions of the constraint violations types of regret; a bound
on Reg+(K; d) implies a bound on Reg(K; d), but the opposite does not holds. In words, a bound on the
ﬁrst implies a bound on the absolute sum of constraint violations where the latter bounds the cumulative
constraint violations, and, thus, allows for “error cancellations”.

2.3 Linear Programming for CMDPs

In Sec. 2, we have seen that the cost criteria can be expressed as the expectation of the immediate cost
w.r.t. to the occupancy measure. The convexity and compactness of this space is essential for the analysis
of constrained MDPs. We refer the reader to [Altman, 1999, Chap. 3 and 4] for an analysis in inﬁnite
horizon problems.

We start stating two basic properties of an occupancy measure q.

In this section, we remove the
dependence on the model p to ease the notation. It is easy to see that the occupancy measure of any
policy π satisﬁes [e.g., Zimin and Neu, 2013, Bhattacharya and Kharoufeh, 2017]:

ph−1(s

|

s′, a′)qπ

h−1(s′, a′)

a
X

qπ
h (s, a) =

qπ
h (s, a)

≥

Xs′,a′
0

s
∀

∈ S

s, a

∀

. For h = 1 and an initial state distribution µ, we have that

qπ
1 (s, a) = π1(a

µ(s)

s)

|

·

s, a

∀

(8)

1 (s, a) = 1. As a consequence, by summing the ﬁrst constraint in (8) over s we have
[H]. Thus the qπ satisfying the constraints are probability measures.

) the space of occupancy measures.

for all h

[H]

1

}

\ {

∈

s,a qπ

Notice that
s,a qπ
that
We denote by ∆µ(

P

M
Since the set ∆µ(

h (s, a) = 1, for all h
P

∈

M

Please refer to [e.g., Puterman, 1994, Altman, 1999, Mannor and Tsitsiklis, 2005] for more details.

) can be described by a set of aﬃne constraints, we can state the following property.

6

Algorithm 1 OptCMDP

Require: δ
(0, 1)
∈
h(s, a) = 0, p0
Initialize: n0
for k = 1, ..., K do

h(s′

|

s, a) = 1/S and c0

h(s, a) = 0

ck and

dk as in (13)

Deﬁne
Compute the solution of (14) through the extended LP
dk
Execute πk and collect a trajectory (sk
i,h}i) for h
k
Update counters and empirical model (i.e., nk, ck, d

h, ck
h,

h, ak

[H]
, pk) as in (9)

∈

e

e

{

end for

Proposition 1. The set ∆µ(

M

) of occupancy measure is convex.

An important consequence of the linearity of the cost criteria and of the structure of ∆(

) is that
the original control problem can be reduced to a Linear Program (LP) where the optimization variables
are measures. Furthermore, optimal solutions of the LP deﬁne the optimal Markov policy through the
occupancy measure. In fact, a policy πq generates an occupancy measure q

) if

∆(

M

∈

M

πq
h(a

|

s) =

qh(s, a)
b qh(s, b)

,

(s, a, h)

∀

∈ S × A ×

[H].

The constrained problem (3) is equivalent to the LP:

P

qh(s, a)ch(s, a)

min
q

Xs,a,h

s.t.

qh(s, a)di,h(s, a)

Xs,a,h

αi

≤

[I]

i
∀

∈

qh(s, a) =

ph−1(s

a
X

Xs′,a′
q1(s, a) = µ(s)

a
X
qh(s, a)

0

≥

s′, a′)qh−1(s′, a′)

|

h

∀

∈

[H]

1

}

\ {

s
∀

∈ S

(s, a, h)

∀

∈ S × A ×

[H]

The constraint

s,a qh(s, a) = 1 is redundant.

2.4 Notations and Deﬁnitions.

P

∈

[H] and k

Throughout the paper, we use t
[K] to denote time-step inside an episode and the index
Fk includes all events (states, actions, and costs) until the end
of an episode, respectively. The ﬁltration
of the k-th episode, including the initial state of the k + 1 episode. We denote by nk
h(s, a), the number
of times that the agent has visited state-action pair (s, a) at the h-th step, and by X k, the empirical
average of a random variable X. Both quantities are based on experience gathered until the end of the
kth episode and are
Fk measurable. Since πk is
h (s, a; p). Furthermore, from
this deﬁnition we have that for any X which is

∈

Fk−1 measurable, so is qπk
Fk−1 measureable
qπk
h (s, a; p)X(s, a).

E[X(sk

h, ak
h)

| Fk−1] =

s,a
X

We use

O(X) to refer to a quantity that depends on X up to a poly-log expression of a quantity at
up to numerical constans or poly-log

most polynomial in S, A, K, H and δ−1. Similarly, . represents
factors. We deﬁne X

Y , max

.

≤

e

∨

X, Y
{

}

3 Upper Conﬁdence Bounds for CMDPs

We start by considering a natural adaptation of UCRL2 [Jaksch et al., 2010] to the setting of CMDPs

which we call OptCMDP (see Algorithm 1).

7

Let nk−1

h

(s, a) =

denote the number of times a pair (s, a) was observed
before episode k. At each episode, OptCMDP estimates the transition model, cost function and constraint
cost function by their empirical average:

h = a

P

(cid:16)

(cid:17)

h = s, ak
sk

k−1
k′=1 1

′

′

pk−1
h

(s′

|

s, a) =

ck−1
h

(s, a) =

[I],

i
∀

∈

k−1
d
i,h (s, a) =

P

P

P

k−1
k′=1 1

(cid:16)

′

′

h = s, ak
sk
nk−1
h

h = a, sk
1

(s, a)

′

h+1 = s′

′

∨
h = s, ak
sk
h = a

′

′

k−1
k′=1 ck

′

k−1
k′=1 dk

h · 1
(cid:16)
nk−1
(s, a)
h

1

′

′

∨
h = s, ak
sk
1

i,h · 1
(cid:16)
nk−1
(s, a)
h

∨

h = a

.

(cid:17)

(cid:17)

,

(cid:17)

(9)

Following the approach of optimism-in-the-face-of-uncertainty we would like to act with an opti-
mistic policy. To this end, we generalize the notion of optimism from the bandit setup presented
in [Agrawal and Devanur, 2019] to the RL setting. Speciﬁcally, we would like for our algorithm to satisfy
the following demands:

(a) Feasibility of π∗ for all episodes. The optimal policy π∗ should be contained in the feasible set in

every episode.

(b) Value optimism. The value of every policy should be optimistic relatively to its true value,
pk are the optimistic cost and model by which the algorithm

ck,

ck,

V π
1 (s1;
≤
calculates the value of a policy.

V π
1 (s1; c, p) where

pk)

e

e

e

e

Indeed, optimizing over a set which satisfy (a) while satisfying (b) results in an optimistic estimate
of V ⋆

1 (s1).
Similar to UCRL2, at the beginning of each episode k, OptCMDP constructs conﬁdence intervals for the

costs and the dynamics of the CMDP. Formally, for any (s, a)

we deﬁne

Bp

h,k(s, a) =

Bc

h,k(s, a) =

Bd

i,h,k(s, a) =

p(

s, a)

∈
(s, a)

·|
n
ck−1
e
h
k−1
i,h (s, a)

h
d

∆S :

s′
∀

,

p(

s, a)

−

∈ S
|
·|
h,k(s, a), ck−1
(s, a) + βc
βc
e
k−1
i,h (s, a) + βd

βd
i,h,k(s, a), d

h

−

−

h

s, a)

∈ S × A
pk−1
h

(
·|
h,k(s, a)
i

,

,

i,h,k(s, a)
i

| ≤

βp
h,k(s, a, s′)
o

,

(10)

where the size of the conﬁdence intervals is built using empirical Bernstein inequality [e.g., Audibert et al.,
2007, Maurer and Pontil, 2009] for the transitions and Hoeﬀding inequality for the costs:

βp
h,k(s, a, s′) .

s

Var

(s′

pk−1
h
nk−1
(cid:0)
h

|
(s, a)

h,k = βd
βc

i,h,k .

nk−1
h

s

1
(s, a)

∨

+

nk−1
h

1
(s, a)

1

∨

s, a)

1

(cid:1)

∨

1

(11)

|

(s′

s, a)

pk−1
h

= pk−1
h

(s′
where Var
plausible CMDPs associated with the conﬁdence intervals is then
(cid:1)
(cid:0)
Bc
i,h,k(s, a),
di,h ∈
to the optimization problem
e

h,k(s, a)
}

h,k(s, a),

. Once

pk−1
h

s, a)

s, a)

Bp

ph(

Bd

(s′

(1

−

∈

·|

·

|

|

s, a)) [e.g., Dann and Brunskill, 2015]. The set of

Mk =

,
,
S
A
∈
Mk been computed, OptCMDP ﬁnds a solution

ch(s, a)

M = (

p) :

d,

c,

{

e

e

e

e

(Mk, πk) =

h(s, a)qπ
ck

h (s, a;

p)

e

(

c, edi,
e

arg min
p)∈Mk, π∈ΠMR
e

s.t.

Xh,s,a

Xh,s,a

di,h(s, a)qπ
e

e
h (s, a;

p)

αi,

≤

i
∀

∈

[H]

(12)

e
While this problem is well-deﬁned and feasible, we can simplify it and avoid to optimize over the sets Bc
k
and Bd

e

k. We deﬁne

h(s, a) = ck−1
ck

h

(s, a)

−

βc
h,k(s, a)

and

dk
i,h(s, a) = d

k−1
i,h (s, a)

βd
i,h,k(s, a)

−

(13)

e

e

8

to be the lower conﬁdence bounds on the costs. Then, we can solve the following optimization problem

min
k , π∈ΠMR

p∈Bp
e

s.t.

Xh,s,a

ck
h(s, a)qπ

h (s, a;

p)

dk
i,h(s, a)qπ
e

e
h (s, a;

p)

αi,

≤

i
∀

∈

[H]

(14)

Consider a feasible solution M ′ = (
S
with dk as in (13) and still have a feasible solution. This holds since c′

Xh,s,a
, c′, d′, p′) and π′ of problem (12). We can replace c′ with ck and d′
dk componentwise.

ck and d′

A

e

e

,

≥

≥

We can now state some property of (14).

Proposition 2. The optimization problem (14) is feasible. Denote by πk the policy recovered solving (14)
and by

pk) the associated CMDP. Then, policy πk is optimismtic, i.e.,

Mk = (

dk,

ck,

,

,
A

S

f

V πk
1 (s1;
e

e

e

ck,

pk) :=

k qπk (
c⊤

pk)

≤

c⊤qπ⋆

(p) := V ⋆

1 (s1; c, p)

Proof. The proof of optimism is reported in Lem. 9 and the feasibility is proven in Lem. 10.

e

e

e

e

The extended LP problem. Problem (14) is similar to (3), the crucial diﬀerence is that the true costs
and dynamics are unknown. Since we cannot directly optimize this problem, we propose to rewrite (14) as
an extended LP problem by considering the state-action-state occupancy measure zπ(s, a, s′; p) deﬁned as
h (s, a; p). We leverage the Bernstein structure of Bp
zπ
h (s, a, s′; p) = ph(s′
h,k (see Eq. 10) to formulate
the extended LP over variable z:

s, a)qπ

|

min
z

s.t.

Xh,s,a,s′

Xh,s,a,s′

zh(s, a, s′)ch(s, a)

zh(s, a, s′)di,h(s, a)

αi

≤

zh(s, a, s′) =

zh−1(s′, a′, s)

Xa,s′

Xs′,a′
z1(s, a, s′) = µ(s)

Xa,s′
zh(s, a, s′)

zh(s, a, s′)

0

pk−1
h

(s′

|

≥

−

s, a) + βp

h,k(s, a, s′)

(cid:16)
zh(s, a, s′) +

−

pk−1
h

(s′

s, a)

|

−

y
(cid:17) X

βp
h,k(s, a, s′)

[I]

i
∀

∈

h

∀

∈

[H]

1

}

\ {

s
∀

∈ S

zh(s, a, y)

0

≤

(s, a, s′, h)
∀
(s, a, s′, h)
∀

∈ S × A × S ×

∈ S × A × S ×

zh(s, a, y)

0

≤

(s, a, s′, h)
∀

∈ S × A × S ×

[H]

[H]

[H]

(cid:16)

y
(cid:17) X
This LP has O(S2HA) constraints and O(S2HA) decision variables. Such an approach was also used
in Jin et al. [2019] in a diﬀerent context. Notice that Bp
k can be chosen by using diﬀerent concentration
inequalities, e.g., L1 concentration inequality for probability distributions. Rosenberg and Mansour [2019]
showed that even in that case we can formulate an extended LP.

Once we have computed z, we can recover the policy and the transitions as

h(s′
pk

|

s, a) =

z(s, a, s′)
y z(s, a, y)

and

πk(a

|

s) =

s′ z(s, a, s′)
b,s′ z(s, b, s′)

P
P

sProposition 2 shows that (a) and (b) are satisﬁed and the solution is optimistic. This allows us to

e

P

provide the following guarantees.

Theorem 3 (Regret Bounds for OptCMDP). Fix δ
K ′

[K] the following regret bounds hold

∈

(0, 1). With probability at least 1

δ for any

−

∈

Reg+(K ′; c)

≤

O

Reg+(K ′; d)

≤

√S
(cid:16)

√S

e
O

e

(cid:16)

H 4K + (√

N

N

H 4K + (√

N

N

9

+ H)H 2SA

,

(cid:17)
+ H)H 2SA

.

(cid:17)

Algorithm 2 OptCMDP-bonus

Require: δ
(0, 1)
∈
h(s, a) = 0, p0
Initialize: n0
for k = 1, ..., K do

h(s′

|

s, a) = 1/S and c0

h(s, a) = 0

ck and

h as in (16)

Compute exploration bonus bk
dk as in (15)
Deﬁne
Compute the solution of (17) through LP
dk
h, ak
Execute πk and collect a trajectory (sk
i,h}i) for h
k
Update counters and empirical model (i.e., nk, ck, d

h, ck
h,

e

e

{

[H]
, pk) as in (9)

∈

end for

4 Exploration Bonus for CMDPs

OptCMDP is an eﬃcient algorithm for exploration in constrained MDPs. An obvious shortcoming of
OptCMDP is its high computational complexity due to the solution of the extended LP with O(S2HA)
constraints and decision variables.
In this section, we present a bonus-based algorithm for explo-
ration in CMDPs that we call OptCMDP-bonus. This algorithm can be seen as a generalization of
UCBVI [Azar et al., 2017] to constrained MDPs. The main advantage of OptCMDP-bonus is that it
requires to solve a single CMDP. To this extent, it has to solve an LP problem with O(SAH) constraints
and decision variables.

At each episode k, OptCMDP-bonus builds an optimistic CMDP Mk := (

S

,

ck
h(s, a) = ck

h(s, a)

−

bk
h(s, a)

and

k
dk
i,h(s, a)
i,h(s, a) = d

−

dk, pk−1) where

ck,

,
A
bk
e
h(s, a),

e

(15)

k
while ck, d
about costs and transitions into a single exploration bonus. Formally,

and pk are the empirical estimates deﬁned in (9). The term bk

e

e

h integrates the uncertainties

where βr and βp are deﬁned as in (11). Then, OptCMDP-bonus solves the following optimization problem

bk
h(s, a)

≃

βr
h,k(s, a) + H

βp
h,k(s, a, s′)

Xs′

(16)

h(s, a)qπ
ck

h (s, a; pk−1)

i,h(s, a)qπ
dk
e

h (s, a; pk−1)

αi,

≤

[H]

i
∀

∈

(17)

min
π∈ΠMR

s.t.

Xh,s,a

Xh,s,a

e

≤

This problem can be solved using the LP described in Sec. 2.3.
optimistic policy, i.e., V πk
ck, pk)

V ⋆
1 (s1).

1 (s1;

In App. B.2, we show that πk is an

Theorem 4 (Regret Bounds for OptCMDP-bonus). Fix δ
e
any K ′
[K] the following regret bounds hold

∈

(0, 1). With probability at least 1

δ for

−

∈

Reg+(K ′; c)

Reg+(K ′; d)

√S

N

H 4K + S2H 4A(

N

H 4K + S2H 4A(

≤

O

e
O

≤

(cid:16)

√S

N

H + S)

,

(cid:17)

H + S)

.

N

The regret bounds of OptCMDP-bonus include the same

(cid:17)
term as of OptCMDP. How-
ever, the constant term in the regret bounds of OptCMDP-bonus has worst dependence w.r.t. S, H,
.
N
This suggests that in the limit of large state space the bonus-based approach for CMDPs have worse
performance relatively to the optimistic model approach.

H 4K

√S

N

O

(cid:16)

(cid:17)

(cid:16)

e

e

Remark 2. The origin of the worst regret bound comes from the larger bonus term (16) we need to
add to compensate on the lack of knowledge of the transition model. This bonus term, allows us to
replace the optimistic planning w.r.t. a set of transition models (as in OptCMDP) by using the empirical
transition model. However, it leads to a value function which is not bounded within [0, H] but within
√SH 2, H]. To circumvent this problem, a truncated Bellman operator has been used [e.g., Azar et al.,
[

−

10

2017, Dann et al., 2017]. The value of a policy π is thus deﬁned as:

Qπ
0, ˜ck
h(s, a; ˜ck, ¯pk−1) = max
h
V π
h (s; ˜ck, ¯pk−1) =
; ˜ck, ¯pk−1), πh(
h(s,
(cid:8)
·

h(s, a) + ¯pk−1

Qπ
h

· |

(
· |
.

s)
i

s, a)V π

h+1(
·

; ˜ck, ¯pk−1)

(cid:9)

However, plugging this idea into the CMDP problem (Sec. 2.3) is not simple. In particular, it is not clear
how to enforce truncation in the space of occupancy measures. Thus, reduction to LP seems problematic
to obtain. At the same time, using dynamic programming to solve CMDP is problematic due to the
presence of constraints (and the lack of Bellman optimality principle). We leave it for future work to
devise a polynomial algorithm to solve this problem, or establishing it is a “hard-problem” to solve. If
solved, it would result in an algorithm with similar performance to that of OptCMDP (up to polylog and
constant factors).

5 Optimistic Dual and Primal-Dual Approaches for CMDPs

In previous sections, we analyzed algorithms which require access to a solver of an LP with at least
Ω(SHA) decision variables and constraints. In the limit of large state space, solving such linear pro-
gram is expected to be prohibitively expensive in terms of computational cost. Furthermore, most of
the practically used RL algorithms [e.g., Achiam et al., 2017, Tessler et al., 2019] are motivated by the
Lagrangian formulation of CMDPs.

Motivated by the need to reduce the computational cost, we follow the Lagrangian approach to CMDPs
+, the

in which the dual problem to CMDP (3) is being solved. Introducing Lagrange multipliers λ
dual problem to (3) is given by

RI

∈

L∗ = max
λ∈RI
+

min
π∈∆S
A

cT qπ(p) + λT (Dqπ(p)

(cid:8)

α)

−

(cid:9)

(18)

With this in mind, a natural way to solve a CMDP is to use a dual sub-gradient algorithm [see e.g.,
Beck, 2017] or a primal-dual gradient algorithm. Viewing the problem in this manner, a CMDP can be
solved by playing a game between two-player; the agent π and the Lagrange multiplier λ. This process
is expected to converge to the Nash equilibrium with value L∗. Furthermore, strong duality is known to
hold for CMDP [e.g., Altman, 1999] and thus the expected value of this game is expected to converge
to L∗ = V ∗
1 (s1). This general approach is also followed in the line of works on online learning with
long-term constraints [e.g., Mahdavi et al., 2012, Yu et al., 2017]. There, the problem does not have a
decision horizon H nor state space as in our case.

As the environment is unknown, and the agents gathers its experience based on samples, the algorithm
should use an exploration mechanism with care. To handle the exploration, we use the optimism approach.
In the following sections, we formulate and establish regret bounds for optimistic dual and primal-dual
approaches to solve a CMDP. These algorithms are computationally easier than the algorithms of previous
sections. Unfortunately, the regret bounds obtained in this section are weaker. We establish bounds on
Reg(K; c) (resp. Reg(K; d)) instead of Reg+(K; c) (resp. Reg+(K; d)) as in previous section (see Sec. 2.2
for details).

5.1 Optimistic Dual Algorithm for CMDPs

We start by describing the optimistic dual approach for CMDPs. OptDual-CMDP is based upon the dual
projected sub-gradient algorithm (e.g., Beck [2017]). It can also be interpreted through the lens of online
learning. In this sense, we can interpret OptDual-CMDP as solving a two-player game in a decentralized
manner where the ﬁrst player (the agent, π) applies “be-the-leader” algorithm, and the second player
(the Lagrange multiplier, λ) uses projected gradient-descent.

Algorithm OptDual-CMDP (see Alg. 3) acts by performing two stages in each iteration. At the ﬁrst

stage it solves the following optimistic problem:

πk,

pk ∈

arg min
π∈ΠMR, p′∈Bp
k

ck +
(

DT

k λk)⊤qπ(p′)

λT
k α

−

dk,i and Bp

ck,

where
ﬁnding the optimal policy (denoted πk) of the following extended MDP
r+
αi)λk
h (s, a) =

k are the same as in Sec. 3 (refer to (10) and (13)). This problem corresponds to
, r+, p+) :
. Since this is an extended MDP and not

ck
h(s, a)+
e

Mk =

i,h(s, a)

M = (

i(dk

s, a)

Bp

A

S

{

,

e

−

i , p+
h (
·|

h,k(s, a)
}

∈

e

e

e

P

e

11

Algorithm 3 OptDual-CMDP

RI, λ1 = 0, Counters, empirical averages

Require: tλ =

q

pk ∈

, λ1 ∈

H2IK
ρ2
for k = 1, ..., K do
# Update Policy
πk,
arg minπ∈ΠMR, p′∈Bp
# Update Dual Parameters
e
Dk−1qπk (
(
λk+1 =
e
+
dk
Execute πk and collect a trajectory (sk
i,h}i) for h
e
k
Update counters and empirical model (i.e., nk, ck, d

λk + 1
tλ
h

k λk)⊤qπ(p′)

pk)
e
−

h, ck
h,

h, ak

λT
k α

ck +
(

DT

α)

−

e

{

i

k

[H]
, pk) as in (9)

∈

end for

a CMDP, we can use standard dynamic programming techniques. One possibility is to use the extended
LP similar to the one introduced in Sec. 3. Otherwise, we can use backward induction to compute Qk

Qk

h(s, a) = r+

h (s, a) +

min
h,k(s,a)

p′∈Bp

p′(s′

s, a) min

a′ Qk

h+1(s′, a′)

|

Xs′
arg mina Qk

h(s, a). To compute qπk

h (s, a) we can use Alg.

with Qk
3 in [Jin et al., 2019].

H+1(s, a) = 0 for all s, a. Then, πk

h(s)

∈

of the “optimistic” constraints: λk+1 =

At the second stage, OptDual-CMDP updates the Lagrange multipliers proportionally to the violation
Dkqπk (
(
The following assumption is standard for the analysis of dual projected sub-gradient method which
we make as well. This assumption is quite mild and demands a policy which satisfy the constraint with
equality exists. For example, a policy with zero constraint-cost (from state s1) exists this assumption
hold.

λk + 1
tλ
h

pk)

α)

−

e

e

i

+

.

Assumption 2 (Slater Point). We assume there exists an unknown policy π for which dT
all the constraints i

[I]. Set

i qπ(p) < αi for

∈

−
αi −
(cid:0)
The following theorem establishes guarantees for both the performance and the total constraint vio-

ρ =

(cid:1)

.

cT qπ(p)
mini=1,..,I

∗

cT qπ
(p)
dT
i qπ(p)

lation (see App. C for the proof).

Theorem 5 (Regret Bounds for OptDual-CMDP). For any K ′
hold

∈

[K] the regrets the following bounds

Reg(K ′; c)

≤

O

√S

N

Reg(K ′; d)

≤

e
O

((1 +

(cid:16)

(cid:18)

e

H 4K + ρ√H 2IK + (√

+ H)H 2SA

1
ρ

)

√IS

(cid:16)

H 4K + (√

N

N

N

(cid:17)
+ H)√IH 2SA

.
(cid:17)(cid:19)

See that the regret bounded in Theorem 5 is Reg and not Reg+ as in Sec. 3 and 4. This diﬀerence in
types of regret, as we believe, is not an artifact of the analysis. It can be directly attributed to bounds
from convex analysis [Beck, 2017]. Meaning, establishing a guarantee on Reg+, instead on Reg, for
OptDual-CMDP requires to improve convergence guarantees of dual projected gradient-descent.

Finally, we think that it may be possible to use exploration bonus instead of solving the extended

problem. However, we leave this point for future work.

5.2 Optimistic Primal Dual approach for CMDPs

In this section, we formulate and analyze OptPrimalDual-CMDP (Algorithm 4). This algorithm per-
forms incremental, optimistic updates of both primal and dual variables. Optimism is achieved by using
exploration bonuses (refer to Sec. 4).

Instead of solving an extended MDP as OptDual-CMDP, OptPrimalDual-CMDP evaluates the Q-functions
of both the cost and constraint cost w.r.t. the current policy πk by using the optimistic costs
dk,i and
the empirical transition model ¯pk. Note that the optimistic cost and constraint costs are obtained using

ck,

12

e

e

Algorithm 4 OptPrimalDual-CMDP

Require: tλ =

H2IK
ρ2
for k = 1, ..., K do

q

, tK =

q

2 log A

(H2(1+Iρ)2K) , λ1 ∈

RI, λ1 = 0, Counters, empirical averages

= Trun. Policy Evaluation(

ck, pk−1, πk)
dk
i , pk−1, πk)

h as in (16)

dk as in (15)

Compute exploration bonus bk
ck and
Deﬁne
# Policy Evaluation
Qπk
ck, pk−1)
e
Qπk
e

h (s, a;
e
[I],

∈

e

o

s,a,h

s,a,h = Trun. Policy Evaluation(
di,k, pk−1)
h (s, a;
(cid:9)

i=1 λk,iQπk

e
× S × A

do
h (s, a;
ck, pk−1) +
πk
h(a|s) exp(−tK Qk
Pa′ πk

i
(cid:8)
∀
n
# Policy Update
for
[H]
h, s, a
∀
∈
h(s, a) = Qπk
Qk
πk+1
s) =
h
end for
# Update Dual Parameters
λk + 1
λk+1 = max
tλ
λk+1, ρ1
λk+1 = min
n
{
dk
Execute πk and collect a trajectory (sk
i,h}i) for h
k
Update counters and empirical model (i.e., nk, ck, d

o
h, ck
h, ak
h,

h(s,a))
P
h(s,a′))

h(a′|s) exp(−tK Qk

Dk−1qπk (
(

h (s, a;

α), 0

pk)

(a

−

e

e

e

e

{

}

|

I

e

dk,i, pk−1)

[H]
, pk) as in (9)

∈

end for

the exploration bonus bk
(MD) [Beck and Teboulle, 2003] update on the weighted Q-function

h(s, a) deﬁned in Eq. 15 (see also Eq. 14). Then, it applies a Mirror Descent

Qk

h(s, a) = Qπk

h (s, a;

ck, pk−1) +

λk,iQπk

h (s, a;

dk,i, pk−1),

I

i=1
X

e

e

and updates the dual variables, i.e., the Lagrange multipliers λ, by a projected gradient step. Since we
optimize over the simplex and choose the Bregman distance to be the KL-divergence, the update rule of
MD has a close solution (see the policy update step in Alg. 4).

Importantly, in the policy evaluation stage OptPrimalDual-CMDP uses a truncated policy evaluation,
which prevents the value function to be negative (see Algorithm 5). This allows us to avoid the problems
experienced in OptCMDP-bonus when such truncation is not being performed.

λ : 0

Furthermore, diﬀerently then in OptDual-CMDP, in OptPrimalDual-CMDP we project the dual param-
eter to be within the set Λρ :=
. Such projection can be done eﬃciently. We remark
}
that such an approach was also applied in [Nedi´c and Ozdaglar, 2009] for convex-concave saddle-points
problems. The reason for restricting the set of Lagrange multipliers to Λρ for our needs is to keep Qk
bounded (if a component of λk diverges then Qk might diverge). On the other hand, we wish to keep
the set suﬃciently big- otherwise, we cannot supply guarantees on the constraint violations. The set Λρ
is suﬃcient to meet both these needs. We remark that projecting on Λρ′ with ρ′
ρ would also lead to
convergence guarantees by applying similar proof techniques.

λρ1

≤

≥

{

The computational complexity of OptPrimalDual-CMDP amounts to estimate the state-action value
functions Qπk
dk,i, pk−1) instead of solving an extended MDP as in OptDual-CMDP.
However, as the following theorem establishes, the reduced computational cost comes with a worse regrets
guarantees. As for OptDual-CMDP we assume a slater point exists (see Assumption 2).

ck, pk−1), Qπk

h (s, a;

h (s, a;

e

The following theorem establishes guarantees for both the performance and the total constraint vio-

e

lation (see App. D for the proof).

Theorem 6 (Regret Bounds for OptPrimalDual-CMDP). For any K ′
bounds hold

∈

[K] the regrets the following

Reg(K ′; c)

≤

O

H 4K +

H 4(1 + Iρ)2K + (√

N

N

+ H)H 2SA

Reg(K ′; d)

≤

e
O

√S
(cid:16)

(1 +

(cid:18)

e

√IS

p
N

1
ρ

)

(cid:16)

H 4K + (√

N

+ H)√IH 2SA

(cid:17)

13

(cid:17)
+ I√H 4K

.
(cid:19)

Algorithm 5 Truncated Policy Evaluation

lh(s, a),

ph(s′

|

s, a), πh(a

s)

|

Require:
s
∀
for

∈ S
∀
for

s, a, s′, h,
∀
, V π
H+1(s) = 0
h = H, .., 1 do
b
do
p) = max

∈ S × A

l,

b

lh(s, a) +

ph(

·|

s, a)

V π
h+1(
·

;

l,

p), 0

b

b

b

b

o

b
do
b
p) =

n

b
l,
;

Qπ
h

h(s,

·

p), πh(

· |

s)
i

s, a
∀
Qπ
h(s, a;
end for
b
for
s
∀
V π
h (s;
end for
b

∈ S
l,

end for
return

b
b
Qπ
h(s, a)

b

b

b

h,s,a

o

n

b

Observe that Theorem 6 has worst performance relatively to Theorem 5 w.r.t. the terms multiplying
the √K term. However, its constant term has similar performance to the constant term in Theorem 5.

6 Conclusions and Summary

In this work, we formulated and analyzed diﬀerent algorithms by which safety constraints can be combined
in the framework of RL by combining learning in CMDPs. We investigated both UCRL-like approaches
(Sec. 3 and 4) motivated by UCRL2 [Jaksch et al., 2010], as well as, optimistic dual and primal-dual
approaches, motivated by practical successes of closely related algorithms [e.g., Achiam et al., 2017,
Tessler et al., 2019]. For all these algorithms, we established regret guarantees for both the performance
and constraint violations.

Interestingly, although the dual and primal-dual approaches are nowadays more practically acceptable,
we uncovered an important deﬁciency of these methods; these have ‘weaker’ performance guarantees (Reg)
relatively to UCRL-like algorithms (Reg+). This fact highlights an important practical message if an
algorithm designer is interested in good performance w.r.t. Reg+. Furthermore, the primal-dual algorithm
(section 5.2), which is computationally easier, has worse performance relatively to the optimistic dual
algorithm (section 5.1). In light of these observations, we believe an important future venue is to further
study the computational-performance tradeoﬀ in safe RL. This would allow algorithm designers better
understanding into the types of guarantees that can be obtained when using diﬀerent types of safe RL
algorithms.

References

Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In ICML,

volume 70 of Proceedings of Machine Learning Research, pages 22–31. PMLR, 2017.

Shipra Agrawal and Nikhil R. Devanur. Bandits with concave rewards and convex knapsacks. In EC,

pages 989–1006. ACM, 2014.

Shipra Agrawal and Nikhil R Devanur. Bandits with global convex constraints and objective. Operations

Research, 67(5):1486–1502, 2019.

Eitan Altman. Constrained Markov decision processes, volume 7. CRC Press, 1999.
Jean-Yves Audibert, R´emi Munos, and Csaba Szepesv´ari. Tuning bandit algorithms in stochastic en-
vironments. In ALT, volume 4754 of Lecture Notes in Computer Science, pages 150–165. Springer,
2007.

Mohammad Gheshlaghi Azar, Ian Osband, and R´emi Munos. Minimax regret bounds for reinforcement
learning. In ICML, volume 70 of Proceedings of Machine Learning Research, pages 263–272. PMLR,
2017.

Ashwinkumar Badanidiyuru, Robert Kleinberg, and Aleksandrs Slivkins. Bandits with knapsacks. In
2013 IEEE 54th Annual Symposium on Foundations of Computer Science, pages 207–216. IEEE, 2013.
Ashwinkumar Badanidiyuru, Robert Kleinberg, and Aleksandrs Slivkins. Bandits with knapsacks. J.

ACM, 65(3):13:1–13:55, 2018.

Amir Beck. First-order methods in optimization, volume 25. SIAM, 2017.

14

Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for convex

optimization. Operations Research Letters, 31(3):167–175, 2003.

Felix Berkenkamp, Matteo Turchetta, Angela P. Schoellig, and Andreas Krause. Safe model-based rein-

forcement learning with stability guarantees. In NIPS, pages 908–918, 2017.

Shalabh Bhatnagar and K Lakshmanan. An online actor–critic algorithm with function approximation
for constrained markov decision processes. Journal of Optimization Theory and Applications, 153(3):
688–708, 2012.

Arnab Bhattacharya and Jeﬀrey P Kharoufeh. Linear programming formulation for non-stationary,

ﬁnite-horizon markov decision process models. Operations Research Letters, 45(6):570–574, 2017.

Qi Cai, Zhuoran Yang, Chi Jin, and Zhaoran Wang. Provably eﬃcient exploration in policy optimization.

arXiv preprint arXiv:1912.05830, 2019.

Richard Cheng, G´abor Orosz, Richard M. Murray, and Joel W. Burdick. End-to-end safe reinforcement
learning through barrier functions for safety-critical continuous control tasks. In AAAI, pages 3387–
3395. AAAI Press, 2019.

Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone. Risk-constrained reinforce-

ment learning with percentile risk criteria. J. Mach. Learn. Res., 18:167:1–167:51, 2017.

Yinlam Chow, Oﬁr Nachum, Edgar A. Du´e˜nez-Guzm´an, and Mohammad Ghavamzadeh. A lyapunov-

based approach to safe reinforcement learning. In NeurIPS, pages 8103–8112, 2018.

Yinlam Chow, Oﬁr Nachum, Aleksandra Faust, Mohammad Ghavamzadeh, and Edgar A. Du´e˜nez-
Guzm´an. Lyapunov-based safe policy optimization for continuous control. CoRR, abs/1901.10031,
2019.

Richard Combes, Chong Jiang, and Rayadurgam Srikant. Bandits with budgets: Regret lower bounds

and optimal algorithms. In SIGMETRICS, pages 245–257. ACM, 2015.

Christoph Dann and Emma Brunskill. Sample complexity of episodic ﬁxed-horizon reinforcement learning.

In NIPS, pages 2818–2826, 2015.

Christoph Dann, Tor Lattimore, and Emma Brunskill. Unifying pac and regret: Uniform pac bounds
In Advances in Neural Information Processing Systems, pages

for episodic reinforcement learning.
5713–5723, 2017.

Wenkui Ding, Tao Qin, Xu-Dong Zhang, and Tie-Yan Liu. Multi-armed bandit with budget constraint

and variable costs. In AAAI. AAAI Press, 2013.

Yonathan Efroni, Nadav Merlis, Mohammad Ghavamzadeh, and Shie Mannor. Tight regret bounds for

model-based reinforcement learning with greedy policies. arXiv preprint arXiv:1905.11527, 2019.

Yonathan Efroni, Lior Shani, Aviv Rosenberg, and Shie Mannor. Optimistic policy optimization with

bandit feedback. arXiv preprint arXiv:2002.08243, 2020.

Evrard Garcelon, Mohammad Ghavamzadeh, Alessandro Lazaric, and Matteo Pirotta. Improved algo-

rithms for conservative exploration in bandits. CoRR, abs/2002.03221, 2020a.

Evrard Garcelon, Mohammad Ghavamzadeh, Alessandro Lazaric, and Matteo Pirotta. Conservative

exploration in reinforcement learning. CoRR, abs/2002.03218, 2020b.

Javier Garcıa and Fernando Fern´andez. A comprehensive survey on safe reinforcement learning. Journal

of Machine Learning Research, 16(1):1437–1480, 2015.

Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement learning.

Journal of Machine Learning Research, 11(Apr):1563–1600, 2010.

Chi Jin, Tiancheng Jin, Haipeng Luo, Suvrit Sra, and Tiancheng Yu. Learning adversarial mdps with

bandit feedback and unknown transition. arXiv preprint arXiv:1912.01192, 2019.

Matthew Joseph, Michael J. Kearns, Jamie H. Morgenstern, and Aaron Roth. Fairness in learning:

Classic and contextual bandits. In NIPS, pages 325–333, 2016.

Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In ICML,

volume 2, pages 267–274, 2002.

Abbas Kazerouni, Mohammad Ghavamzadeh, Yasin Abbasi, and Benjamin Van Roy. Conservative con-

textual linear bandits. In NIPS, pages 3910–3919, 2017.

Torsten Koller, Felix Berkenkamp, Matteo Turchetta, and Andreas Krause. Learning-based model pre-

dictive control for safe exploration. In CDC, pages 6059–6066. IEEE, 2018.

Mehrdad Mahdavi, Rong Jin, and Tianbao Yang. Trading regret for eﬃciency: online convex optimization

with long term constraints. Journal of Machine Learning Research, 13(Sep):2503–2528, 2012.

15

Shie Mannor and John N. Tsitsiklis. On the empirical state-action frequencies in markov decision processes

under general policies. Math. Oper. Res., 30(3):545–561, 2005.

Andreas Maurer and Massimiliano Pontil. Empirical bernstein bounds and sample variance penalization.

arXiv preprint arXiv:0907.3740, 2009.

Angelia Nedi´c and Asuman Ozdaglar. Subgradient methods for saddle-point problems. Journal of opti-

mization theory and applications, 142(1):205–228, 2009.

Francesco Orabona. A modern introduction to online learning. arXiv preprint arXiv:1912.13213, 2019.
Santiago Paternain, Luiz F. O. Chamon, Miguel Calvo-Fullana, and Alejandro Ribeiro. Constrained

reinforcement learning has zero duality gap. In NeurIPS, pages 7553–7563, 2019.

Martin L Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley

& Sons, Inc., 1994.

Aviv Rosenberg and Yishay Mansour. Online convex optimization in adversarial markov decision pro-
cesses. In ICML, volume 97 of Proceedings of Machine Learning Research, pages 5478–5486. PMLR,
2019.

Steven E Shreve and Dimitri P Bertsekas. Alternative theoretical frameworks for ﬁnite horizon discrete-
time stochastic optimal control. SIAM Journal on control and optimization, 16(6):953–978, 1978.
Chen Tessler, Daniel J. Mankowitz, and Shie Mannor. Reward constrained policy optimization. In ICLR

(Poster). OpenReview.net, 2019.

Akifumi Wachi, Yanan Sui, Yisong Yue, and Masahiro Ono. Safe exploration and optimization of con-

strained mdps using gaussian processes. In AAAI, pages 6548–6556. AAAI Press, 2018.
Yifan Wu, Roshan Shariﬀ, Tor Lattimore, and Csaba Szepesv´ari. Conservative bandits.

In ICML,

volume 48 of JMLR Workshop and Conference Proceedings, pages 1254–1262. JMLR.org, 2016.

Hao Yu, Michael Neely, and Xiaohan Wei. Online convex optimization with stochastic constraints. In

Advances in Neural Information Processing Systems, pages 1428–1438, 2017.

Andrea Zanette and Emma Brunskill. Tighter problem-dependent regret bounds in reinforcement learning
without domain knowledge using value function bounds. In ICML, volume 97 of Proceedings of Machine
Learning Research, pages 7304–7312. PMLR, 2019.

Liyuan Zheng and Lillian J. Ratliﬀ. Constrained upper conﬁdence reinforcement learning. CoRR,

abs/2001.09377, 2020.

Alexander Zimin and Gergely Neu. Online learning in episodic markovian decision processes by relative

entropy policy search. In NIPS, pages 1583–1591, 2013.

A Optimistic Algorithm based on Bounded Parameter CMDPs

In this section, we establish regret guarantees for OptCMDP (Alg. 1). As a ﬁrst step, we recall the
algorithm and we formally states the conﬁdence intervals. The empirical transition model, cost function
and constraint cost functions are deﬁned as in (9). We recall that OptCMDP constructs conﬁdence intervals
for the costs and the dynamics of the CMDP. Formally, for any (s, a)

we deﬁne

Bp

h,k(s, a) =

Bc

h,k(s, a) =

Bd

i,h,k(s, a) =

p(

s, a)

∈
(s, a)

·|
n
ck−1
e
h
h
k−1
d
i,h (s, a)

∆S :

s′
∀

,

p(

s, a)

−

∈ S
|
·|
h,k(s, a), ck−1
(s, a) + βc
βc
e
k−1
i,h (s, a) + βd

βd
i,h,k(s, a), d

h

pk−1
h

(
·|
h,k(s, a)
i

,

−

−

where

h

βp
h,k(s, a, s′) := 2

s

Var

h,k = βd
βc

i,h,k :=

s

nk−1
h

(s′

s, a)

Lp
δ

|
(s, a)

1
(cid:1)

∨

pk−1
h
nk−1
(cid:0)
h
Lδ
(s, a)

1

∨

∈ S × A
s, a)

| ≤

βp
h,k(s, a, s′)
o

,

(19)

,

i,h,k(s, a)
i
14/3Lp
δ
(s, a)

nk−1
h

+

1

∨

(20)

with Lp

δ = ln

6SAHK
δ

, Lc

6SAH(I+1)K
δ

and Var

The set of plausible CMDPs associated with the conﬁdence intervals is then
Bc
h,k(s, a),
which M ⋆

(cid:17)
h,k(s, a)
}

i,h,k(s, a),

Bp

ph(

∈

·|

(cid:1)

(cid:0)

(cid:1)

|

(s′

s, a)

pk−1
h

= pk−1
h

(s′
|
Mk =
. In the next section, we deﬁne the good event under
e

·
M = (

|
p) :

pk−1
h

s, a)).

s, a)

,
A

−
,

(s′

(1

d,

c,

S

e

e

e

{

δ = 2 ln
(cid:16)
s, a)

ch(s, a)

∈

(cid:0)
Bd
di,h ∈
∈ Mk w.h.p.
e

e

16

A.1 Failure Events

Deﬁne the following failure events.

F p

k =

s, a, s′, h :
∃

ph(s′

|

|

s, a)

−

pk−1
h

(s′

|

s, a)

| ≥

n

F N

k =




s, a, h : nk−1
∃

h

(s, a)

1
2

≤

qπk
h (s, a

p)

|

−

H ln

Xj<k

βp
h,k(s, a, s′)
o

SAH

F c

k =

F d

k =

s, a, h :

|
s, a, h, i


∃
(cid:8)

ck
h(s, a)

[I] :

ch(s, a)

−
k
i,h(s, a)
d

| ≥

βc
h,k(s, a)

di,h(s, a)

βd
(cid:9)
i,h,k(s, a)
| ≥

∈
Furthermore, the following relations hold by standard arguments.

−

n

∃

|

•

•

K
k=1 F c

S

F d

k ∪

F cd

Let F cd =
k . Then Pr
argument on all s, a, all possible values of nk(s, a), all i
n(s, a) = 0 the bound holds trivially since C, Di ∈
Let F P =
of nk

k=1 F p
h(s, a), we have that

[0, 1].

≤

(cid:8)

(cid:9)

K

∈

S

where

Pr

|
(cid:8)

ph(s′

s, a)

|

−

pk−1
h

(s′

s, a)

| ≥

|

ǫ1

δ′′,

≤

(cid:9)

δ′ 




o

∈

δ′, by Hoeﬀding’s inequality, and using a union bound
[K]. Furthermore, for

[I] and k

k . Using Thm. 4 in [Maurer and Pontil, 2009], for every ﬁxed s, a, h, k and value

2Var

s, a)

(s′

pk−1
h
nk−1
h

|
(s, a)

(cid:16)

(cid:17)
1

∨

ln

2
δ′′

(cid:0)

(cid:1)

+

7 ln

2
δ′′
(s, a)
(cid:0)

.

1

1)

∨

(cid:1)
−

3(nk−1
h

ǫ1 = v
u
u
t
≥

h(s, a)

See that for any nk
0, 1

{

}

the bound holds trivially. This also implies that

2, we use Theorem 4 in [Maurer and Pontil, 2009], and for nk

h(s, a)

∈

where

Pr

|
(cid:8)

ph(s′

s, a)

|

−

pk−1
h

(s′

s, a)

| ≥

|

ǫ2

δ′′,

≤

(cid:9)

2Var

(cid:16)

s, a)

(s′

pk−1
h
nk−1
h

|
(s, a)

(cid:17)
1

∨

ln

2
δ′′

(cid:0)

(cid:1)

+

7 ln

2
δ′′
(s, a)
(cid:0)

,

1)

1

∨

(cid:1)
−

3(nk−1
h

ǫ2 = v
u
u
t

since ǫ1 ≤
and set δ′′ =

ǫ2. Applying union bound on all s, a, h, and all possible values of nk(s, a) and k

[K]
δ′. This analysis was also used in [Jin et al., 2019].

F P

∈

δ

′

Let F N =

•

(SAHK)2 we get that Pr
(cid:8)
≤

k . Then, Pr

K
k=1 F N

F N

≤

(cid:9)

δ′. The proof is given in [Dann et al., 2017, Cor. E.4].

S

Remark 3. Boundness of of immediate cost and constraints cost. Notice that we assumed that the
random variables Ch(s, a)
∈
Lemma 7 (Good event of OptCMDP). Setting δ′ = δ

[0, 1] and Di,h(s, a)

[0, 1] for any s, a, h.

δ where

G

∈

(cid:8)

(cid:9)

G = F c

F d

F p

3 then Pr
{
F N = F cd

} ≤
F p

F N .

When the failure events does not hold we say the algorithm is outside the failure event, or inside the good
event G which is the complement of G.

[

[

[

[

[

The fact F p holds conditioning on the good event implies the following result [e.g., Jin et al., 2019,

Lem. 8].

Lemma 8. Conditioned on the basic good event, for all k, h, s, a, s′ there exists constants C1, C2 > 0 for
which we have that

pk−1
h

(s′

s, a)

|

−

ph(s′

|

s, a)

= C1

where Lδ,p = ln

(cid:0)

(cid:12)
(cid:12)

6SAHK
δ

.

(cid:1)

(cid:12)
(cid:12)

s

17

s, a)Lδ,p

ph(s′
|
nk
h(s, a)

1

∨

+

C2Lδ,p
nk
h(s, a)

∨

,

1

A.2 Optimism

Recall that
ck deﬁned in (13).

D

∈

RI×SAH and α

∈

RI such that

D =

dk
1, . . . ,

dk
I

⊤

and α = [α1, . . . , αI ]⊤, with

dk and

h

i

e

e
e
Lemma 9 (Optimism). Conditioning on the good event, for any π there exists a transition model p′
e
k qπ(p′)
cT
for which (i)
Proof. Conditioning on the good event, the true model p is contained in Bp
c component-wise. Thus, setting p′ = p
on the good event

Dqπ(p), and , (ii)

Dkqπ(p′)

cT qπ(p).

D and

Bp

≤

≤

e

e

e

e

k. Furthermore, conditioned

Bp
k

∈

k we get

∈

Dk ≤
e

ck ≤

e

Dkqπ(p)
cT
k qπ(p)
e

Dkqπ(p′) =
cT
k qπ(p′) =
e
0 component-wise.
e

e

≤

Dqπ(p)
≤
cT qπ(p),

where we used the fact that qπ(p)

≥

Lemma 10 (π∗ is Feasible Policy.). Conditioning on the good event, π∗ is a feasible policy for any
k

[K], i.e.,

∈

Proof. Denote ΠD =
true model. Furthermore, let

π : Dqπ(p)
{

≤

∈

n
α
}
Πk

D =

π :
{

Dkqπ(p′)

α, p′

≤

Bp
k}

∈

π∗

π

∈

∆S

A :

Dkqπ(p′)

α, p′

≤

Bp
k

.

∈

o
as the set of policies which does not violate the constraint on the

e

be the set of policies which do not violate the constraint w.r.t. all possible models at episode k. Observe
that Πk

D is the set of feasible policies at episode k for OptCMDP.

e

Conditioning on the good event, by Lemma 9 Dqπ(p)

α implies that exists p′

Bp

k such that

≤

∈

ΠD ⊆

Πk
D.

(21)

Dkqπ(p′)

≤

α. Thus,

e

Since π⋆

∈

ΠD it implies that π⋆

Πk
D.

∈

From the two lemmas we arrive to the following important corollary

Corollary 11. Conditioning on the good event (i) V πk
V πk
1 (s1; c, p).

1 (s1;

ck,

pk)

≤

1 (s1), and, (ii) V πk
V ⋆

1 (s1;

ck,

pk)

≤

Proof. The following relations hold.

e

e

e

e

V ∗(s1) = min
π∈∆S
A

cT qπ(p)

π

|
∈
cT qπ(p)

ΠD

(cid:9)
∈

π

|

Πk
D

Dkqπ(p′)

|

≤

(cid:9)
α

(cid:8)

cT q

n

(cid:8)
min
A,p′∈Bp

k

π∈∆S

min
A,p′∈Bp

k

π∈∆S

≥

=

≥

π∈∆S

min
A,p′∈Bp

k

k qπ(p′)
cT
e

Dqπ(p′)

α

= V πk

1 (s1;

ck,

pk).

|

o

≤

n
The second relation holds by Lemma 10 and the forth relation holds by Lemma 9.
e

o

e

e

e

A.3 Proof of Theorem 3

In this section, we establish the following regret bounds for OptCMDP (see Alg. 1).

Theorem 3 (Regret Bounds for OptCMDP). Fix δ
the following regret bounds hold

∈

(0, 1). With probability at least 1

δ for any K ′

−

[K]

∈

Reg+(K ′; c)

≤

O

√S

N

H 4K + (√

N

Reg+(K ′; d)

≤

e
O

(cid:16)

√S
(cid:16)

e

H 4K + (√

N

N

18

+ H)H 2SA

,

(cid:17)
+ H)H 2SA

.

(cid:17)

Proof. We start by conditioning on the good event. By Lem. 7 it holds with probability at least 1
We now analyze the regret relatively to the cost c. The following relations hold for any K ′

−
[K].

δ.

∈

Regret+(K ′; c) =

[V πk

1 (s1; c, p)

V ∗
1 (s1; c, p)]+ ≤
V πk
1 (s1;

pk)

ck,

[V πk

1 (s1; c, p)

−

V πk
1 (s1;

ck,

pk)]+

e

e

Xk

−

−

Xk

V πk
1 (s1; c, p)

=

≤

Xk
O

(√S

N

e

H 4K + (√

N

+ H)H 2SA).
e

e

The second and third relations hold by optimism, i.e., Cor. 11. The forth relation holds by Lem. 29.

See that assumptions 1,2,3 of Lem. 29 are satisﬁed conditioning on the good event.
We now turn to prove the regret bound on the constraint violation. For any i

[I] and K ′

[K] the

∈

∈

following relations hold.

′

K

Xk=1

[V πk

1 (s1; di, p)

αi]+ =

−

′

K

Xk=1
K

′

−
≥0

V πk
1 (s1; di, p)



V πk
1 (s1;

dk
i ,

pk)

+ V πk

1 (s1;

dk
i ,

pk)



|
V πk
1 (s1; di)

{z
V πk
1 (s1;

−

e

e

dk
i ,

pk)

}

|

≤0
e
{z

e

αi



−

+



}

≤

Xk=1
O

(√S

H 4K + (√

N

N

+ H)H 2SA).
e

e

≤
The ﬁrst relation holds since V πk

α as the optimization problem solved in every episode is
feasible (see Lem. 10). Furthermore, by optimism V πk
V πk
1 (s1; di, p) (see the ﬁrst relation
of Lem. 9). The third relation holds by applying Lem. 29. See that assumptions (a), (b) and (c) of
Lem. 29 are satisﬁed conditioning on the good event (see also Lem. 8).

1 (s1;
e

1 (s1;

di,k,

pk)

≤

≤

e

e

e

dk
i ,

pk)

e

B Optimistic Algorithm based on Exploration Bonus

In this section, we establish regret guarantees for OptCMDP-bonus (see Alg. 2). The main advantage of this
algorithm w.r.t. OptCMDP is the computational complexity. While OptCMDP requires to solve an extended
CMDP through an LP with O(S2AH) constraints and decision variable, OptCMDP-bonus requires to ﬁnd
the solution of a single CMDP by solving an LP with O(SAH) constraints and variables.

At each episode k, OptCMDP-bonus builds an optimistic CMDP Mk := (

S

,

ck
h(s, a) = ck

h(s, a)

−

bk
h(s, a)

and

k
dk
i,h(s, a)
i,h(s, a) = d

−

dk, pk) where

ck,

,
A
bk
e
h(s, a),

e

k
while ck, d

and pk are the empirical estimates deﬁned in (9). The exploration bonus bk

e

e

h is deﬁned as

h(s, a) := βc
bk
:=bc

h,k(s, a)

h,k(s,a)

where βc and βp are deﬁned as in (20).

|

{z

}

+ H

βp
h,k(s, a, s′)

(22)

Xs′

:=bp

h,k(s,a)

|

{z

}

The policy by which we act at episode k is given by solving the following optimization problem

πk,

k qπ(pk−1)
cT

pk = arg min
π∈∆S
A
Dkqπ(pk−1)
e

s.t.

e

α

≤

D = [

where
much similar to the LP by which a CMDP is solved (Section 2.3).

I ]⊤ and
dk

dk
i is deﬁned as in (15). Solving this problem can be done by solving an LP,

dk
1 , . . . ,

e

e

e

Before supplying the proof of Theorem 4 we formally deﬁning the set of good events which we show
holds with high probability. Conditioning on the good, we establish the optimism of OptCMDP-bonus and
then regret bounds for OptCMDP-bonus.

e

e

19

B.1 Failure Events

We deﬁne the same set of good events as for OptCMDP (App. A.1). We restate this set here for convenience.

F p

k =

s, a, s′, h :
∃

ph(s′

|

|

s, a)

−

pk−1
h

(s′

|

s, a)

| ≥

n

F N

k =




s, a, h : nk−1
∃

h

(s, a)

1
2

≤

qπk
h (s, a

p)

|

−

H ln

F c

k =

F d

k =

s, a, h :

|
s, a, h, i


∃
(cid:8)

ck
h(s, a)

[I] :

|

∈

ch(s, a)

−
k
i,h(s, a)
d

| ≥

−

∃

n

Xj<k

βc
h,k(s, a)

di,h(s, a)

βd
(cid:9)
i,h,k(s, a)
| ≥

βp
h,k(s, a, s′)
o

SAH

δ′ 




o

−
δ where

As in App. A.1 the union of these events hold with probability greater than 1

δ.

Lemma 12 (Good event of OptCMDP-bonus). Setting δ′ = δ

3 then Pr
G
{

} ≤

G = F c

F d

F p

F N .

When the failure events does not hold we say the algorithm is outside the failure event, or inside the good
event G which is the complement of G.

[

[

[

Lemma 13. Conditioned on the basic good event, for all k, h, s, a, s′ there exists constants C1, C2 > 0
for which we have that

pk−1
h

(s′

s, a)

|

−

ph(s′

|

s, a)

= C1

where Lδ,p = ln

(cid:12)
(cid:12)

6SAHK
δ

B.2 Optimism

(cid:0)

.

(cid:1)

(cid:12)
(cid:12)

s

s, a)Lδ,p

ph(s′
|
nk
h(s, a)

1

∨

+

C2Lδ,p
nk
h(s, a)

∨

,

1

Lemma 14 (Per-State Optimism.). Conditioning on the good event, for any π, s, a, h, k, i
that

∈

[I] it holds

ch(s, a)

ch(s, a)

−

and

e

(ph −

pk−1
h

)(s′

|

s, a)V π

h+1(s′; c, p)

0,

≤

−

Xs′

dh(sh, ah)

−

dh(sh, ah)

(ph −

pk−1
h

)(s′

|

sh, ah)V π

h+1(s′; di, p)

0.

≤

−

Xs′

Proof. For any s, a, h, k, conditioning on the good event,

e

ch(s, a)

−

ch(s, a)

−

bc
h,k(s, a)

≤ |

ch(s, a)

ch(s, a)
|

bc
h,k(s, a)

−

0

≤

(23)

−
h,k(s,a)

≤βc

by the choice of the bonus bc

h,k.

Furthermore, for any s, a, h, k

|

{z

}

s, a)V π

h+1(c)

)(s′

|

s, a)

−

bp
h,k(s, a)
h+1(s′; di)
V π

pk−1
h

(cid:12)
(cid:12)
(cid:12)
(cid:12)
s, a)

)(s′

|

−

(cid:12)
(cid:12)
bp
h,k(s, a)

bp
h,k(s, a)

−

(ph −

≤

≤

Xs′
H

pk−1
h

)(

· |
pk−1
(ph −
h
(cid:12)
(cid:12)
Xs′

(ph −

(cid:12)
(cid:12)

2H

≤
= bp

Xs′ s
h,k(s, a)
−

(cid:12)
(cid:12)
s, a)Lp,δ
1

pk−1
(s′
h
|
nk
h(s, a)
bp
h,k(s, a) = 0,

∨

14Lp,δ

3

(nk

h(s, a)

1)

∨

−

1

−

(cid:0)

(cid:1)

bp
h,k(s, a)

(24)

+ H

20

where the forth relation holds conditioning on the good event, and the ﬁfth relation by the choice of the
bonus bp

h,k(s, a).

Combining (23) and (24) we get that

Repeating this analysis while replacing c,
e

di,k we conclude the proof of the lemma.

ch(s, a)

ch(s, a)

−

−

s, a)V π

h+1(
·

; c, p)

0.

≤

· |

)(

pk−1
h

(ph −
ck with di,

Lemma 15 (Optimism). Conditioning on the good event, for any π, s, h, k, i it holds that (i) V π
e
h (s; c, p), and, (ii) V π
V π

V π
h (s; di, p).

dk
i , pk)

h (s;

e

h (s;

ck, pk)

≤

e

Proof. For any k

≤
[K] we have that
e
V π(s1; c, p)

−

∈
V π(s1;

= E
"

ck, pk)
H
e
Xh=1

e

ch(sh, ah)

ch(sh, ah)

−

(ph −

−

pk−1
h

)(

· |

sh, ah)V π

h+1(
·

; c, p)

s1, π, pk−1

#

(cid:12)
(cid:12)
(cid:12)

where we used the value diﬀerence lemma (see Lem. 35). Applying the ﬁrst statement of Lem. 14 which
hold for any s, a, h, k (conditioning on the good event) we conclude the proof of the ﬁrst claim.

The second claim follows by the same analysis on the diﬀerence V π

using the value diﬀerence lemma and the second claim in Lem. 14.

h (s;

dk
i , pk−1)

−

V π
h (s; di, p), i.e.,

The following lemma shows that the problem solved by OptCMDP-bonus is always feasible. This lemma

e

follows the same idea used to prove the feasibility for OptCMDP (see Lem. 10).

Lemma 16 (π⋆ is Feasible Policy.). Conditioning on the good event, π⋆ is a feasible policy for any
k

[K], i.e.,

∈

π∗

∈

π

∈

∆S

A :

Dkqπ(pk−1)

α

.

≤

n
as the set of policies which does not violate the constraint on the

o

e

Proof. Denote ΠD =
true model. Furthermore, let

π : Dqπ(p)
{

≤

α
}
Πk

D =

π :

Dkqπ(pk−1)

{

α
}

≤

be the set of policies which do not violate the constraint w.r.t. all possible models at the kth episode.
e

Conditioning on the good event, by Lem. 15 Dqπ(p)

α implies that

α. Thus,

Dkqπ(pk−1)

≤

≤

Since π∗

∈

ΠD it implies that π∗

Πk
D.

∈

ΠD ⊆

Πk
D.

e

(25)

From the two lemmas we arrive to the following corollary as

Corollary 17. Conditioning on the good event (i) V πk
V πk
1 (s1; c, p).

1 (s1;

ck, pk−1)

≤

1 (s1), and, (ii) V πk
V ⋆

1 (s1;

ck, pk−1)

≤

Proof. The following relations hold.

e

e

V ∗(s1) = min
π∈∆S
A

cT qπ(p)

(cid:8)
cT qπ(p)

(cid:8)

cT qπ(p)

π

π

∈

∈

ΠD

(cid:9)

Πk
D

(cid:9)

Dkqπ(pk−1)

≤

|

|

|

α

k qπ(pk−1)
cT
e

Dkqπ(pk−1)

|

min
π∈∆S
A

≥

= min
π∈∆S
A

min
π∈∆S
A

≥

n

n

o

≤

α

= V πk

1 (s1;

ck, pk−1).

o

e
The second relation holds by Lem. 16 and the forth relation holds by Lem. 15.

e

e

21

B.3 Proof of Theorem 4

In this section, we establish the following regret bounds for OptCMDP-bonus algorithm.

Theorem 4 (Regret Bounds for OptCMDP-bonus). Fix δ
K ′

[K] the following regret bounds hold

∈

(0, 1). With probability at least 1

δ for any

−

∈

Reg+(K ′; c)

≤

O

√S

N

H 4K + S2H 4A(

N

Reg+(K ′; d)

≤

e
O

H 4K + S2H 4A(

N

N

H + S)

,

(cid:17)
H + S)

.

(cid:16)

√S
(cid:16)

(cid:17)

Unlike the proof of the OptCMDP-bonus algorithm (Thm. 3), the value function is not constraint to be
within [0, H] . However, since the bonus is bounded, the estimated value function is bounded in the range
√SH 2, H]. Although this discrepency, in the following we are able to reach similar dependence in
of [
√K. The fact the estimated value is bounded in OptCMDP-bonus diﬀerently then in OptCMDP results in
worse constant term as Thm. 4 exhibits (see Remark 2).

−

e

Proof. We start by conditioning on the good event. By Lem. 7, it holds with probability at least 1
We now analyze the regret relatively to the cost c. The following relations hold for any K ′

[K]:

∈

δ.

−

Reg+(K ′; c) =

[V πk

1 (s1; c, p)

Xk

=

V πk
1 (s1; c, p)

V ⋆
1 (s1; c, p)]+ ≤
V πk
1 (s1;

ck, pk−1)

Xk

V πk
1 (s1; c, p)

−

V πk
1 (s1;

ck, pk−1)

+

(cid:2)

(cid:3)

e

−

−

Xk

(cid:16)

√S

H 4K + S2H 4A(

≤

O

N

e
H + S)
N

.

(cid:17)

e

The second and third relations hold by optimism, see Cor. 17. The forth relation holds by Lem. 31.
See that assumptions 1,2,3 of Lem. 31 are satisﬁed conditioning on the good event. Assumption 4 of
Lem. 31 holds by the optimism of the value estimate (see Lem. 15). Assumption 5 of Lem. 31 holds by
Lem. 14.

We now turn to prove the regret bound on the constraint violation. For any i

following relations hold.

[I] and K ′

∈

[K] the

∈

V πk
1 (s1; di, p)

V

πk
1 (s1; di)

+ V

πk
1 (s1; di)

′

K

Xk=1

[V πk

1 (s1; di)

α]+ =

−

≤

K



−
≥0

Xk=1
K



|
V πk
1 (s1; di, p)

Xk=1

V πk
1 (s1;

}

|
dk
i , pk−1)

{z

−

α



−

+



}

≤0

{z

√S

H 4K + S2H 4A(

N

e
H + S)

.

(cid:17)

≤

O

N

(cid:16)
e
dk
i , pk−1)
1 (s1;

The ﬁrst relation holds since V πk

α as the optimization problem solved in every episode
is feasible, see Lem. 16. Furthermore, by optimism V πk
V πk
1 (s1; di, p) (see the ﬁrst relation
1 (s1;
of Lem. 15). The third relation holds by applying Lem. 31. See that assumptions 1,2,3 of Lem. 31 are
satisﬁed conditioning on the good event (see also Lem. 13).

dk
i ,

pk)

≤

≤

e

e

e

C Constraint MDPs Dual Approach

In this section, we establish regret guarantees for OptDual-CMDP by proving Theorem 5. Unlike both pre-
vious sections, OptDual-CMDP does not require an LP solver, but repeatedly solves MDPs with uncertainty
in their transition model.

Before supplying the proof of Theorem 5 we formally deﬁne the set of good events which we show holds
with high probability. Conditioning on the good, we establish the optimism of OptDual-CMDP and then
regret bounds for OptDual-CMDP. The regret bound of OptDual-CMDP relies on results from constraint
convex optimization with some minor adaptations which we establish in Appendix G.

22

C.1 Deﬁnitions

We introduce a notation that will be used across the proves of this section. Following this notation allows
us to apply generic results from convex optimization to the problem.

The optimistic and true constraints valuation are denoted by

The optimistic value, true value, and optimal value are denoted by

pk)

Dkqπk (
gk = (
gk = (Dqπk (p)
e
e
e

−

α)

−
α).

•

•

pk)

k qπk (
cT
fk =
fk = cT qπk
e
e
fopt = V ∗

e

1 (s1) = cT q∗.

C.2 Failure Events

We deﬁne the same set of good events as for OptDual-CMDP (Appendix A.1). We restate this set here for
convenience.

F p

k =

s, a, s′, h :
∃

ph(s′

|

|

s, a)

−

pk−1
h

(s′

|

s, a)

| ≥

n

F N

k =




s, a, h : nk−1
∃

h

(s, a)

1
2

≤

qπk
h (s, a

p)

|

−

H ln

F c

k =

F d

k =

s, a, h :

|
s, a, h, i


∃
(cid:8)

ck
h(s, a)

[I] :

|

∈

ch(s, a)

−
k
i,h(s, a)
d

| ≥

−

∃

n

Xj<k

βc
h,k(s, a)

di,h(s, a)

βd
(cid:9)
i,h,k(s, a)
| ≥

δ′ 




o

βp
h,k(s, a, s′)
o

SAH

As in Appendix A.1 the union of these events hold with probability greater than 1

δ.

−

Lemma 18 (Good event of OptDual-CMDP). Setting δ′ = δ

3 then Pr
{

G

} ≤

δ where

G = F c

F d

F p

F N .

When the failure events does not hold we say the algorithm is outside the failure event, or inside the good
event G which is the complement of G.

[

[

[

Lemma 19. Conditioned on the basic good event, for all k, h, s, a, s′ there exists constants C1, C2 > 0
for which we have that

pk−1
h

(s′

s, a)

|

−

ph(s′

|

s, a)

= C1

where Lδ,p = ln

(cid:12)
(cid:12)

6SAHK
δ

.

(cid:0)
C.3 Proof of Theorem 5

(cid:1)

(cid:12)
(cid:12)

s

s, a)Lδ,p

ph(s′
|
nk
h(s, a)

1

∨

+

C2Lδ,p
nk
h(s, a)

∨

,

1

In this section, we establish the following regret bound for OptDual-CMDP.

Theorem 5 (Regret Bounds for OptDual-CMDP). For any K ′

[K] the regrets the following bounds hold

Reg(K ′; c)

≤

O

Reg(K ′; d)

≤

e
O

√S
(cid:16)

N

((1 +

(cid:18)

e

∈
H 4K + ρ√H 2IK + (√

1
ρ

)

√IS
(cid:16)

N

H 4K + (√

N

+ H)H 2SA

N

(cid:17)
+ H)√IH 2SA

.
(cid:17)(cid:19)

We start by proving several useful lemmas on which the proof is based upon.

Lemma 20 (Dual Optimism). Conditioning on the good event, for any k

[K]

∈

fk −
e

fopt ≤ −

λT
k

gk

23

e

Proof. We have that

fopt = cT qπ

∗

(p)

≥

≥

=

=

∗

cT qπ

∗

(p) + λT
k (Dqπ
(p)
−
cT
k qπ(p′) + λT
k (

α)
Dkqπ(p′)

α)

−

pk) + λT
k (
e

Dkqπk (

e
pk)
−

α)

e

e

min
A,p′∈Pk

π∈∆S
k qπk (
cT
fk + λT
gk.
k
e
e
e

e
ck ≤

The ﬁrst relation holds since π∗ satisﬁes the constraint (Assumption 1) which implies that (Dqπ
0, and that λk ≥
the true model is contained in Bp

≤
0 by the update rule. The second relation holds since conditioning on the good event

(p)

α)

−

c.

k as well as

∗

Lemma 21 (Update Rule Recursion Bound). For any λ

e

∈

RI

+ and K ′

[K]

′

K

−
(cid:0)

Xk=1

N

gT
k λk

+

Xk=1

(cid:1)

gT
k λ

tλ
2 k

λ1 −

λ
k

2
2 +

1
2tλ

≤

Proof. For any λ

∈

RI

+ by the update rule we have that

e

e

∈
K

′

2

gkk
k

e

Xk=1

λk+1 −

k

λ
k

2
2 =

[λk +
k

λk +

≤ k

2
2

[λ]+k

1
gk]+ −
tλ
1
λ
gk −
e
tλ
k
2
2
gT
k (λk −
2 +
e
tλ

2
2

=

λk −
k

gkk
[K ′] and multiplying both sides by tλ/2 we get

λ) +

λ
k

e

e

1
t2
λ k

2.

tλ
2 k

−

λ1 −

λ
k

2
2 ≤

tλ
2 k
K

′

λK ′+1 −

λ
k

2
2 −

tλ
2 k
K

′

λ1 −

2
2

λ
k

≤

Xk=1

gT
k (λk −

λ) +

e

1
2tλ

Xk=1

N

N

−
(cid:0)

Xk=1

gT
k λk

+

Xk=1

(cid:1)

e

gT
k λ

tλ
2 k

λ1 −

λ
k

2
2 +

1
2tλ

≤

e

2.

gkk

k

e

′

K

Xk=1

2

gkk
k

e

Summing this relation for k

∈

Rearranging we get,

for any λ

RI

+.

∈

We are now ready to establish Theorem 5.

Proof. Plugging Lemma 20 into Lemma 21 we get

′

K

′

K

′

K

′

K

fopt

+

gT
k λ

gT
k λk

+

gT
k λ

Xk=1(cid:16)
Adding, subtracting

(cid:17)

Xk=1
e
K
k=1 gT
k λ,

′

Xk=1
K
k=1 fk and rearranging we get

Xk=1

e

e

(cid:1)

′

≤

−
(cid:0)

fk −
e

tλ
2 k

≤

λ1 −

λ
k

2
2 +

1
2tλ

′

K

Xk=1

2.

gkk

k

e

P
fopt) +

′

K

P
gT
k λ

′

K

Xk=1

(fk −

tλ
2 k

λ1 −

λ
k

≤

Xk=1
1
2tλ

2
2 +

tλ
2 k

λ1 −

2
2 +

λ
k

1
2tλ

≤

′

K

Xk=1
K

′

Xk=1

2 +

gkk
k

(gk −

′

K

Xk=1

′

K

gk)T λ +

(fk −

fk)

Xk=1

e
gkk
k

e

I

e
K ′

2 + v
u
u
t

i=1 
X

Xk=1

(gk,i −

gk,i)

!

e

2

e
k2 +
λ

k

′

K

Xk=1

(fk −

fk)

e

(26)

24

RI

for any λ

We now bound each term in (26). Notice that

+, where the last relation holds by Cauchy Schwartz inequality.
pk)

gk,i = V πk (s1;

dk,i,

∈

); it is a value function deﬁned on an MDP with immediate cost in [

αi ∈

[

−

−

Lc

δH, H] (where
Lc
δH, H]

−

e

e

Lδ = 2 ln
and α

∈

6SAH(I+1)K
δ

[0, H]. Thus, we have that
(cid:16)

(cid:17)

e

2 .

gkk
k

H 2IK
2tλ

.

1
2tλ

′

K

Xk=1

e

Applying Lemma 29 (see that assumptions (a), (b) and (c) hold conditioning on the good event), we

get that

′

K

Xk=1
K

′

Xk=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(fk −

e
(gk,i −

=

fk)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
gk,i)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

e

which implies that

′

K

Xk=1
K

′

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
=

(cid:12)
(cid:12)
Xk=1
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(V πk (s1; c, p)

V πk (s1;

−

b
(V πk (s1; di, p)

−

V πk (s1;

e

≤

O

ck, pk)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
e
(cid:12)
(cid:12)
dk,i, pk)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

e

√S
(cid:16)

N

H 4K + (√

N

+ H)H 2SA
(cid:17)

√S

N

H 4K + (√

N

+ H)H 2SA
(cid:17)

,

≤

O

e

(cid:16)

b

2

(gk,i −

gk,i)

!

≤

O

e

e

I

K ′

v
u
u
t

i=1 
X

Xk=1

√IS

N

(cid:16)

H 4K + (√

N

+ H)√IH 2SA

.

(cid:17)

H2IK

ρ2 we get

q

Plugging these bounds back into (26) and setting tλ =

′

K

Xk=1

(fk −

fopt) +

′

K

Xk=1

gT
k λ

2
2

λ
k
ρ

. (ρ + k

)√H 2IK +

H 4K + (√

√IS
N
(cid:16)
+ H)H 2SA

,

(cid:17)

+

√S
(cid:16)

N

H 4K + (√

N

for any λ

RI
+.

∈

N

+ H)√IH 2SA

(cid:17)

λ
k2
k

(27)

First claim of Theorem 5. Setting λ = 0 (see that λ

RI

+) in (27) we get

∈

′

K

Xk=1

V πk (s1; c, p)

V ∗(s1) =

−

′

K

Xk=1

Second claim of Theorem 5. Fix i

∈

e
[I] and let

fk −

fopt .

O

√S

N

H 4K + ρ√H 2IK + (√

N

+ H)H 2SA

.

(cid:17)

(cid:16)

λi =

ρei
0

(

′

K

k=1 gi,k]+ 6

[
otherwise,
P

= 0

where ei(i) = 1 and ei(j) = 0 for j
the deﬁnition,

= i, and ρ is given in Assumption 2. See that λi ∈

RI

+ and that, by

Setting λ = λi in (27) we get

λik
k

2
2 ≤

ρ2

+

(28)

H 4K + √H 2IK + (√

N

+ H)√IH 2SA

:= ǫ(K).

(cid:17)(cid:17)

25

′

K

Xk=1

(fk −

fopt) + ρ

′

K


Xk=1

√IS
N

gi,k


(1 + ρ)

(cid:16)

≤

O

e

(cid:16)

6
Since the bound holds for any i

[I] we get that

∈

′

K

(fk −

Xk=1

fopt) + ρ max

i∈[I]



′

K

Xk=1

gi,k


+

′

K

max
i∈[I]

Xk=1
K

′

(fk −

fopt) + ρ

′

K


Xk=1


(fk −

(fk −

=

=

Xk=1
K

′

Xk=1

Xk=1

fopt) + ρ max
i∈[I](cid:12)

(cid:12)
(cid:12)
(cid:12)

′
(cid:12)
K
(cid:12)
gk


fopt) + ρ

Xk=1





=

′

+

K

gi,k

gi,k


+

∞

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
≤

ǫ(K).

Now, by the convexity of the state-action frequency (see Proposition 1) function there exists a policy πK ′
K
which satisﬁes qπK′ (p) = 1
k=1 qπk (p)
K ′
we have that

K
k=1 qπk (p) for any K ′. Since both f and g are linear in 1
K ′

′

′

P

fopt) + ρ

′

K

(fk −

Xk=1

1
K ′ 



′

K

Xk=1





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

gk


+

(cid:13)
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)



Applying Corollary 44 and Theorem 42 we conclude that

= fπK′



−

fopt + ρ

gπK′

+

(cid:13)
(cid:2)
(cid:13)
(cid:13)

(cid:3)

2 ≤

(cid:13)
(cid:13)
(cid:13)

1
K ′ ǫ(K).

P

′

K

max
i∈[I]

Xk=1


gk


max
i∈[I]



≤





′

K

Xk=1

=

gk


+





for any K ′

[K].

∈

ǫ(K)
ρ

,

≤

′

K

Xk=1

gk


+

∞

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Remark 4 (Convexity of the RL Objective Function). Although it is common to refer to the objective
function in RL as non-convex, in the state action visitation polytope the objective is linear and, hence,
convex (however, the problem is constraint to the state action visitation polytope). Thus, we can use
Theorem 42 and Cor. 44 which are valid for constraint convex problems.

D Constraint MDPs Primal Dual Approach

Qπ

h(s, a;

ck, pk), Qπ

In this section we establish regret guarantees for OptPrimalDual-CMDP by proving Theorem 6. Unlike
for OptDual-CMDP, OptPrimalDual-CMDP requires an access to a (truncated) policy estimation algo-
dk,i, pk), i.e., the Q-function w.r.t. to the empirical tran-
rithm which returns
sition model and optimistic cost and constraint cost. This reduces the computational complexity of
OptPrimalDual-CMDP. However, it results in worse performance guarantees relatively to OptDual-CMDP.
Before supplying the proof of Theorem 6 we formally deﬁne the set of good events which we show holds
with high probability. Conditioning on the good, we establish the optimism of OptPrimalDual-CMDP and
then regret bounds for OptPrimalDual-CMDP. The regret bounds of OptPrimalDual-CMDP relies on results
from constraint convex optimization with some minor adaptations which we establish in Appendix G.

h(s, a;

e

b

e

D.1 Failure Events

We deﬁne the same set of good events as for UCRL-OptCMDP (Appendix A.1). We restate this set here
for convenience.

F p

k =

s, a, s′, h :
∃

ph(s′

|

|

s, a)

−

pk−1
h

(s′

|

s, a)

| ≥

n

F N

k =




s, a, h : nk−1
∃

h

(s, a)

1
2

≤

qπk
h (s, a

p)

|

−

H ln

F c

k =

F d

k =

s, a, h :

|
s, a, h, i


∃
(cid:8)

ck
h(s, a)

[I] :

|

∈

ch(s, a)

−
k
i,h(s, a)
d

| ≥

−

∃

n

Xj<k

βc
h,k(s, a)

di,h(s, a)

βd
(cid:9)
i,h,k(s, a)
| ≥

δ′ 




o

βp
h,k(s, a, s′)
o

SAH

As in Appendix A.1 the union of these events hold with probability greater than 1

δ.

−

26

Lemma 22 (Good event of OptPrimalDual-CMDP). Setting δ′ = δ

3 then Pr
G
{

} ≤

δ where

G = F c

F d

F p

F N .

When the failure events does not hold we say the algorithm is outside the failure event, or inside the good
event G which is the complement of G.

[

[

[

Lemma 23. Conditioned on the basic good event, for all k, h, s, a, s′ there exists constants C1, C2 > 0
for which we have that

pk−1
h

(s′

s, a)

|

−

ph(s′

|

s, a)

= C1

where Lδ,p = ln

(cid:12)
(cid:12)

6SAHK
δ

.

(cid:12)
(cid:12)

s

s, a)Lδ,p

ph(s′
|
nk
h(s, a)

1

∨

+

C2Lδ,p
nk
h(s, a)

∨

,

1

D.2 Optimality and Optimism

(cid:0)

(cid:1)

Lemma 24 (On Policy Optimality.). Conditioning on the good event, for any k

[K ′]

∈

′

K

fk + λT
k

Xk=1

e

gk −

fπ∗

−

λT
k gπ∗

≤

O

(

H 4(1 + Iρ)2K)

e

p

e

Proof. By deﬁnition,

fπ∗ + λT

k gπ∗ = V π

1 (s1; c, p) +

∗

I

i=1
X
I

∗

λk,iV π

1 (s1; di, p)

−

fk + λT
k

gk =

V πk
1 (s1;

ck, pk) +

λk,i

V πk
1 (s1;

dk,i, pk)

Let

e

e

b

e

i=1
X

e

b

I

I

λk,iαi

i=1
X

I

−

i=1
X

λk,iαi.

Qk

h(s, a) := Qπk

h (s, a;

ck, pk−1) +

λk,iQπk

h (s, a;

dk,i, pk−1)

V k
h (s1) :=

Qk
h

h(s,

), πk
.
e
hi

·

i=1
X

e

Applying the extended value diﬀerence lemma 34 we get that

′

K

fk + λT
k

Xk=1
′
e
K

gk −

fπ∗

−

λT
k gπ∗

e
V k
1 (s1)

∗

V π
1 (s1; c + λk

d, p)

−

Xk=1
K

H

E

Qk

h(sh,

Xk=1

Xh=1

(cid:2)(cid:10)

=

=

+

|
K

H

Xk=1

Xh=1

E






e
sh)

), πk
h(

· |

·

π∗
h(

· |

−

sh)

|

s1 = s1, π∗, p

(cid:11)

(i)

{z

(cid:3)

}

Qk

h(sh, ah)

−

ch(sh, ah)

I

−

i=1
X

λkdh,i(sh, ah)

ph(

· |

−

sh, ah)V k

h+1

(ii)

|

s1 = s1, π∗, p
.






}

To bound (i), we apply Lemma 26 while setting π = π∗.

{z

|

′

K

H

(i) =

E

Qk

h(sh,

Xk=1

Xh=1

(cid:2)(cid:10)

), πk
h(

· |

·

sh)

π∗
h(

· |

−

sh)

|

s1 = s1, π∗, p

.

H 4(1 + Iρ)2K,

(29)

(cid:11)

(cid:3)

p

27

To bound (ii), observe that by Lemma 25 for all s, a, h, k it holds that

Qk

h(s, a)

ch(s, a)

−

I

−

i=1
X

λkdh,i(s, a)

ph(

· |

−

s, a)V k

h+1 ≤

0.

This implies that

(ii)

0

≤

(30)

since (ii) is an expectation over negative terms. Combining (29) and (30) we conclude that

′

K

fk + λT
k

Xk=1

e

gk −

fπ∗

−

e

′

K

λT
k gπ∗ =

V k
1 (s1)

Xk=1

∗

V π
1 (s1; c + λk

−

d, p) .

H 4(1 + Iρ)2K.

p

e

Lemma 25 (Policy Estimation Optimism). Conditioning on the good event, for any s, a, h, k the following
bound holds

Qk

h(s, a)

ch(s, a)

−

I

−

i=1
X

λkdh,i(s, a)

ph(

· |

−

s, a)V k

h+1 ≤

0,

where

Qk

h(s, a) = Qπk

h (s, a;

ck, pk−1) +

V k
h (s) =

Qk

h(s,

h

·

), πk
h(
e

· |

.

s)
i

I

i=1
X

λk,iQπk

h (s, a;

dk,i, pk−1),

e

(31)

(32)

See that Qπk
(Algorithm 4).

h (s, a;

ck, pk−1), Qπk

h (s, a;

dk,i, pk−1) are deﬁned in the update rule of OptPrimalDual-CMDP

Proof. For all s, a, h, k the following relations hold.

e

e

Qk

h(s, a)

ch(s, a)

−

I

−

i=1
X
I

λkdh,i(s, a)

ph(

· |

−

s, a)V k

h+1

=Qπk

h (s, a;

ck, pk−1) +

λk,iQπk

h (s, a;

dk,i, pk−1)

e
ch(s, a)

−

I

−

i=1
X

where V πk
h (
·
Furthermore, see that

ck, pk−1) :=

;

i=1
X

λk,idh,i(s, a)

ph(

· |

−

e
s, a)

V πk
h+1(
·

;

ck, pk−1) +

I

i=1
X

λk,iV πk

h+1(
·

;

dk,i, pk−1)

!

,

(33)

Qπk

h (s,

h

;

·

ck, pk−1), πk
h(
·

, s)
i

e
, V πk
h (
·

;

dk,i, pk−1) :=

Qπk
h

h (s,

e
dk,i, pk−1), πk
h(
;
·

·

.

, s)
i

Qπk

e
ck, pk−1) = max
h (s, a;

e
h(s, a) + pk−1
ck

h

0,

(
·|

s, a)V πk

e

= max

max

≤

(cid:8)

0, ck−1
h
e
n
0, ck−1
h

(cid:8)
+ max

0,

(s, a)

−

bh,k−1(s, a)

(s, a)
bh,k−1(s, a)
bp
h,k−1(s, a) + pk−1

−

h

;

e
ck, pk)
h+1(
·
bp
h,k−1(s, a) + pk−1
(cid:9)
e

−

h

e

(
·|

s, a)V πk

h+1(
·

;

ck, pk)

o

e

(cid:9)
(
·|

s, a)V πk

h+1(
·

;

ck, pk)

,

(34)

o

since max
{

0, a + b

max
{

0, a

} ≤

0, b

. Similarly, for any i
}

∈

[I],
e

−

n
+ max
{

}

Qπk

h (s, a;

di,k, pk−1)

≤

e

max

0, d

k−1
i,h (s, a)

bh,k−1(s, a)

−

n
+ max

n

bp
h,k−1(s, a) + pk−1

h

0,

−

28

o
s, a)V πk
(
·|

h+1(
·

;

di,k, pk)

.

(35)

o

e

 
Plugging (34) and (35) into (33) we get

Qk

h(s, a)

ch(s, a)

ph(

s, a)V k

h+1

max

≤

−
0, ck−1
h

−

· |
(s, a)
bh,k−1(s, a)
bp
h,k−1(s, a) + pk−1

−

h

ch(s, a)

−
s, a)V πk

h+1(
·

(cid:9)
(
·|

max

0, d

k−1
i,h (s, a)

−

bh,k−1(s, a)

o

e
−

(cid:8)
+ max

0,

−

I

n
λk,i

+

+

i=1
X
I

i=1
X

(cid:16)

n

λk,i

max

0,

(cid:16)

n

;

ck, pk)

ph(

· |

−

o
dh,i(s, a)

s, a)V πk
h (
·

;

ck, pk−1)

e

(36)

(37)

(38)

bp
h,k−1(s, a) + pk−1

h

(
·|

s, a)V πk

h+1(
·

−

ph(

· |

−

s, a)V πk
h (
·

;

dk,i, pk−1)

.

(39)

(cid:17)

e

(cid:17)

;

di,k, pk)

o

e

We now show each of these terms is negative conditioning on the good event.

(36) = max

= max

(cid:8)

0, ck−1
(s, a)
h
−
ch(s, a), ck−1

h

−

max

(cid:8)
(−

≤

ch(s, a),

s

−
ch(s, a)
(cid:9)

ch(s, a)

bh,k−1(s, a)

−

bh,k−1(s, a)

)

(cid:9)

bh,k−1(s, a)

(s, a)

−

Lδ
nk−1
(s, a) −
h
0.

Furthermore, observe that

= max

{−

ch(s, a), 0

} ≤

−

bp
h,k−1(s, a) + pk−1
bp
h,k−1(s, a) +

h

≤ −

s, a)V πk

(
·|
(pk−1

h −

;

h+1(
·
ph)(s′
e

|

ck, pk)

s, a)

||

|

−

ph(
· |
V πk
h+1(s′;

s, a)V πk
h (
·

;

ck, pk−1)

ck, pk)
|

e

(pk−1

h −

|

ph)(s′

s, a)
|

|

e

Xs′
bp
h,k−1(s, a) + H

bp
h,k−1(s, a) + 2H

≤ −

≤ −

Xs′

pk
h(s′

s, a) ln

s

|
nk−1
h
h,k−1(s, a) = 0.

(s, a)
(cid:0)

2SAHK
δ′
1

∨

(cid:1)

+

14H ln

2SAHK
δ′

3(nk−1
h

(s, a)
(cid:0)

1)

1

(cid:1)
∨

−

=

−

h,k−1(s, a) + bp
bp
The second relation holds since V πk
Qπk
ck, pk−1), πk
[0, H] by the update
h(
·
h
rule (OptPrimalDual-CMDP uses truncated policy evaluation, see Algorithm 5). The third relation holds
conditioning on the good event. The forth relation holds by the choice of bp
h,k−1. Applying (40) we get
that

ck, pk) :=

h+1(s′;

h+1(s′,

(40)

i ∈

, s)

e

e

·

;

(37) = max

0,

bp
h,k−1(s, a) + pk−1

h

(
·|

−

s, a)V πk

n
max

ph(

s, a)V πk
h (
·

;

ck, pk−1),

;

ck, pk)

h+1(
·
· |
o
bp
h,k−1(s, a) + (pk−1

ph(

−

e

h −

≤

−
Similarly, we get that each term in the sums at (38),(39) is non-positive. Since λk ≥
e

· |

−

n

e

·|

that both (38)

0 and (39)

0. Thus, we establish that

s, a)V πk
h (
·

;

ck, pk−1)

ph)(

s, a)V πk
e

h+1(
·

;

ck, pk)

0.

≤

o

0 we conclude

≤

≤

Qk

h(s, a)

ch(s, a)

ph(

· |

−

−

s, a)V k

h+1 ≤

0.

Lemma 26 (OMD Term Bound). Conditioned on the good event, we have that for any π

K

H

Xk=1

Xh=1

(cid:2)(cid:10)

E

Qk

h(sh,

), πk
h(

· |

·

sh)

πh(

· |

−

sh)

|

s1 = s, π, p

≤

2H 4(1 + Iρ)2K log A.

(cid:11)

(cid:3)

p

Proof. This term accounts for the optimization error, bounded by the OMD analysis.

By standard analysis of OMD [Orabona, 2019] with the KL divergence used as the Bregman distance

(see Lemma 40) we have that for any s, h and for policy any π,

K

Qk
h(

· |

Xk=1

(cid:10)

s), πk
h(

· |

s)

−

πh(

· |

s)

≤

log A
tK

+

tK
2

(cid:11)

29

K

a
Xk=1 X

πk
h(a

|

s)(Qk

h(s, a))2

(41)

where tK is a ﬁxed step size.

By the form of Qk (31) we get that Qk

0 since it is a sum of positive terms (policy evaluation
is done with truncated policy evaluation, see Algorithm 4). Furthermore, we upper bound Qk for any
s, a, h, k as follows,

≥

Qk

h(s, a) := Qπk

h (s, a;

ck, pk−1) +

H + H

≤

I

e
λk,i ≤

i=1
X

H + HIρ.

I

i=1
X

λk,iQπk

h (s, a;

dk,i, pk−1)

e

The second relation holds by the fact that Qπk
(both
di,k ≤
ck,
the update rule).
e

ck, pk−1), Qπk
1, thus, an expectation over an H such terms is smaller than H) and the fact λk ≥

H by the update rule
0 (by

dk,i, pk−1)

h (s, a;

h (s, a;

≤

Plugging this bound into (41) we get that for any s, a, h

e

e

e

′

K

Xk=1
(cid:10)

Qk

h(s,

·

), πk
h(

· |

s)

πh(

· |

−

s)

≤

log A
tK

+

tKH 2(1 + Iρ)2K
2

.

(42)

(cid:11)

Thus, the following relations hold.

K

H

E

Qk

h(sh,

Xk=1

=

Xh=1
H
E

(cid:2)(cid:10)
K

Xh=1
H

"

E

Xk=1
(cid:10)
log A
tK

≤

Xh=1

(cid:20)

), πk
h(

· |

·

sh)

πh(

· |

−

sh)

|

s1 = s, π, p

(cid:11)

(cid:3)

Qk

h(sh,

), πk
h(

· |

·

sh)

πh(

· |

−

sh)

|

s1 = s, π, p

#

+ tKH 2K

|

s1 = s, π

=

(cid:21)

(cid:11)
H log A
tK

+

tKH 3(1 + Iρ)2K
2

.

See that the ﬁrst relation holds as the expectation does not depend on k. Thus, by linearity of
expectation, we can switch the order of summation and expectation. The second relation holds since (42)
holds for any s.

Finally, by choosing tK =

2 log A/(H 2(1 + Iρ)2K), we obtain

p

K

H

E

Qk

h(sh,

Xk=1

Xh=1

(cid:2)(cid:10)

), πk
h(

· |

·

sh)

πh(

· |

−

sh)

|

s1 = s, π, p

≤

2H 4(1 + Iρ)2K log A.

(43)

(cid:11)

(cid:3)

p

D.3 Proof of Theorem 6

In this section, we establish the following regret bound for OptPrimalDual-CMDP.

Theorem 6 (Regret Bounds for OptPrimalDual-CMDP). For any K ′
bounds hold

∈

[K] the regrets the following

Reg(K ′; c)

≤

O

Reg(K ′; d)

≤

e
O

√S

N

(1 +

H 4K +

H 4(1 + Iρ)2K + (√

N

+ H)H 2SA

√IS

p
N

1
ρ

)

(cid:16)

H 4K + (√

N

+ H)√IH 2SA

(cid:17)

(cid:17)
+ I√H 4K

.
(cid:19)

(cid:16)

(cid:18)

e

We start by proving several useful lemmas on which the proof is based upon.

Lemma 27 (Dual Optimism). Conditioning on the good event, for any k

[K ′]

∈

fk −
e

fopt ≤ −

λT
k

gk +

e

gk −

fπ∗

−

λT
k gπ∗

(cid:17)

e

fk + λT
k
(cid:16)

e

30

Proof. We have that

fopt = cT qπ

∗

(p)

∗

cT qπ
≥
= fπ∗ + λT
fk + λT
k

=

(p) + λT

k (Dqπ

∗

(p)

α)

−

k gπ∗
gk + fπ∗ + λT

λT
k

gk.

k gπ∗

−

fk −
e

The ﬁrst relation holds since π∗ satisﬁes the constraint (Assumption 1) which implies that (Dqπ
and that λk ≥

0 by the update rule.

e

e

e

∗

(p)

α)

0,

≤

−

We now state a lemma which corresponds to Lemma 21 from previous section.

Lemma 28 (Update Rule Recursion Bound Primal-Dual). For any λ
K ′

[K]

∈

λ

∈

∈

RI : 0

λ

≤

≤

ρ1

and

(cid:8)

(cid:9)

N

gT
k λk

+

gT
k λ

tλ
2 k

λ1 −

λ
k

2
2 +

1
2tλ

≤

′

K

2

gkk
k

′

K

−
(cid:0)

Xk=1
Proof. Similar proof to Lemma 21 while using the fact that projection to the set
is non-expansive operator as the operator [x]+.

Xk=1

Xk=1

e

e

e

(cid:1)

RI : 0

λ

∈

λ

≤

≤

ρ1

(cid:8)

(cid:9)

We are now ready to establish Theorem 6.

Proof. Applying Lemma 27 into Lemma 28 we get

′

K

′

K

fk −
′
e

Xk=1(cid:16)
K

fopt

+

gT
k λ

(cid:17)

Xk=1
K

′

gT
k λk

+

≤

≤

−
(cid:0)
e
λ1 −

Xk=1
tλ
2 k

(cid:1)
2
2 +

λ
k

Xk=1
1
2tλ

′

K

e
gT
k λ +

Xk=1

′

K
e

Xk=1

gkk

k

fk + λT
k

gk −

fπ∗

−

λT
k gπ∗

e
2 +

′

K

e
fk + λT
k

Xk=1

e

gk −

fπ∗

−

λT
k gπ∗.

Adding, subtracting

′

K
k=1 gT

k λ,

′

e
K
k=1 fk and rearranging we get

e

P
fopt) +

(fk −

′

K

Xk=1

tλ
2 k

λ
k

2
2 +

1
2tλ

≤

′

K

P
gT
k λ

Xk=1
K

′

Xk=1

+

′

K

Xk=1

+

′

K

Xk=1

e
2
2 +

tλ
2 k

≤

λ
k

1
2tλ

′

e
K

Xk=1

fk + λT
k

gk −

e

e

2 +

gkk
k

(gk −

gk)T λ +

′

K

Xk=1

′

K

Xk=1

(fk −

fk)

e

fk + λT
k

gk −

λT
k gπ∗

−

e
fπ∗

e

I

K ′

2

′

K

gkk
k

e
fπ∗

2 + v
u
u
t
λT
k gπ∗

−

i=1 
X

Xk=1

(gk,i −

gk,i)

!

k2 +
λ
k

e

(fk −

Xk=1

fk)

e

(44)

for any λ

We now bound each term in (44). Since

I
+, where the last relation holds by Cauchy Schwartz inequality.
gk ∈

H, H]

∈ R

−

[

1
2tλ

′

K
e

Xk=1

2

gkk
k

≤

H 2IK
2tλ

.

e

31

Applying Lemma 30 (see that assumptions (1),(2),(3) hold conditioning on the good event), we get

that

′

K

Xk=1
K

′

Xk=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(fk −

e
(gk,i −

=

fk)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
gk,i)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

e

which implies that

′

K

Xk=1
K

′

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
=

(cid:12)
(cid:12)
Xk=1
(cid:12)
(cid:12)
(cid:12)
(cid:12)

I

K ′

(V πk (s1; c, p)

V πk (s1;

−

b
(V πk (s1; di, p)

−

V πk (s1;

e

≤

=

ck, pk)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
dk,i, pk)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

e

√S

N

H 4K + (√

N

+ H)H 2SA

(cid:17)

O

e

(cid:16)

√S

N

H 4K + (√

N

+ H)H 2SA

,

(cid:17)

≤

O

e

(cid:16)

b

2

v
u
u
t

i=1 
X

Lastly, by Lemma 24,

Xk=1

(gk,i −

gk,i)

!

e

≤

O

e

(cid:16)

√IS

N

H 4K + (√

N

+ H)√IH 2SA

.

(cid:17)

Plugging these bounds back into (44) and setting tλ =

′

K

fk + λT
k

Xk=1

e

e

gk −

fπ∗

−

λT
k gπ∗ .

H 4(1 + Iρ)2K.

p

H2IK

ρ2 we get

q

′

K

(fk −

Xk=1
. (ρ + k

′

K

fopt) +

gT
k λ

Xk=1
)√H 2IK +

2
2

λ
k
ρ

+

√S

(cid:16)

H 4K + (√

N

N

for any 0

λ

≤

≤

ρ1.

√IS

H 4K + (√

N
(cid:16)
+ H)H 2SA

N

+ H)√IH 2SA
k
(cid:17)

+

H 4(1 + Iρ)2K,

λ
k2

(45)

(cid:17)

p

First claim of Theorem 6 . Fix λ = 0 which satisﬁes 0

λ

≤

≤

ρ1 in (45) we get

′

K

V πk (s1; c, p)

V ∗(s1) =

−

fk −

fopt

′

K

Xk=1

Xk=1

≤

O

(cid:16)

e

√S

N

H 4K +

H 4(1 + Iρ)2K + (√

N

+ H)H 2SA

.

(cid:17)

p
[I] and let

∈

Second claim of Theorem 6. Fix i

λi =

ρei
0

(

′

K

k=1 gi,k]+ 6

[
otherwise
P

= 0

where ei(i) = 1 and ei(j) = 0 for j
it holds that

= i, and ρ is given in Assumption 2 See that 0

λi ≤

≤

ρ1. Furthermore,

λik
k

2
2 ≤

ρ2

(46)

Set λ = λi in (45) we get

′

K

′

K

(fk −

Xk=1
. (1 + ρ)

fopt) + ρ

√IS
(cid:16)

N

gi,k

Xk=1
+


H 4K + (√

N

+ H)√IH 2SA

+

H 4(1 + Iρ)2K := ǫ(K)

(47)

(cid:17)

p

32

6
where we applied (46) in the second relation. Since the bound (47) holds for any i we get that

′

K

(fk −

Xk=1

fopt) + ρ max

i∈[I]



′

K

Xk=1

gi,k


+

′

K

max
i∈[I]

Xk=1
K

′

(fk −

fopt) + ρ

′

K


Xk=1


(fk −

(fk −

=

=

Xk=1
K

′

Xk=1

Xk=1

fopt) + ρ max
i∈[I](cid:12)

(cid:12)
(cid:12)
(cid:12)

′
(cid:12)
K
(cid:12)
gk


fopt) + ρ

Xk=1





=

′

+

K

gi,k

gi,k


+

∞

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
≤

ǫ(K).

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Now, by the convexity of the state-action frequency function (Proposition 1) there exists a policy πK ′
K
k=1 qπk (p)

K
k=1 qπk (p) for any K ′. Since both f and g are linear in 1
K ′

′

′

which satisﬁes qπK′ (p) = 1
K ′
we have that

P

′

K

(fk −

Xk=1

1
K ′ 



fopt) + ρ

′

K

Xk=1

gk


+

= fπK′

−





(cid:13)
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

Applying Corollary 44 and Theorem 42 we conclude that

P

1
K ′ ǫ(K).

fopt + ρ

gπK′

+

(cid:13)
(cid:2)
(cid:13)
(cid:13)

(cid:3)

2 ≤

(cid:13)
(cid:13)
(cid:13)

′

K

max
i∈[I]

Xk=1


gk


max
i∈[I]



≤





′

K

Xk=1

=

gk


+





for any K ′

[K].

∈

E Bounds of On-Policy Errors

ǫ(K)
ρ

,

≤

′

K

Xk=1

gk


+

∞

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Lemma 29 (On Policy Errors for Optimistic Model). Let lh(s, a),
optimistic cost. Let p be the true transition dynamics of the MDP and
dynamics. Let V π
model l, p and

h (s; l, p), V π
pk, respectively. Assume the following holds for all s, a, h, k
e

lk
h(s, a) be a a cost function, and its
pk be an estimated transition
pk) be the value of a policy π according to the cost and transition

h (s;

lk,

lk,

e

[K]:

∈

(a)

lk
h(s, a)

|

e
−

lh(s, a)
|

e

.

1
√nk−1

h

(s,a)

e

e

.

(b)

e
h(s′
pk
|

|

s, a)

−

ph(s′

s, a)
|

|

.

(c) nk−1
e
h

(s, a)

1
2

≤

j<k qπk

h (s, a

r
p)

|

ph(s′|s,a)
nk−1

(s,a)∨1

h

+

1
(s,a)∨1

.

nk−1

h

H ln SAH

δ′

.

−

Furthermore, let πk be the policy by which the agent acts at the kth episode. Then, for any K ′

P

[K]

∈

′

K

|

Xk=1

V πk
1 (s1; l, p)

V πk
1 (s1;

lk,

pk)

| ≤

−

e

e

√S
(cid:16)

N

O

e

H 4K + (√

N

+ H)H 2SA
(cid:17)

.

33

Proof. The following relations hold.

′

K

V πk
1 (s1; l, p)

V πk
1 (s1;

lk,

pk)
|

−

|

Xk=1
K

′

H

E[

′

Xk=1(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
E[

K

=

≤

(lh(sh, ah)

Xh=1
H

lh(sh, ah)

|

Xk=1

Xh=1

e

e
lk
h(sh, ah)) + (ph −
e
lk
h(sh, ah)

e
s1, p, πk]

| |

−

−

e

(i)

pk
h)(

· |

sh, ah)

V πk
h+1 |

s1, p, πk]

e

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

′

K

H

E[

|
+

Xk=1

Xh=1 Xs′

{z
(ph −

h)(s′
pk

|

sh, ah)

||

|

}
V πk
h+1(s′;

lk,

pk)

| |

s1, p, πk]

,

e

(ii)

e

e

e

{z
where the ﬁrst relation holds by the value diﬀerence Lem. 35. We now bound the terms (i) and (ii).

|

}

Bound on (i). To bound (i) we use the assumption (1) and get,

′

K

H

(i) .

Xk=1
K

′

Xh=1
H

Xk=1

Xh=1

=

E[

E[

1
(sh, ah) |

nk−1
h

s1, p, πk]

q

1

nk−1
h

(sk

h, ak

h) | Fk−1]

q

√SAH 2K + SAH

.

(cid:17)

≤

O

e

(cid:16)

The ﬁrst relation holds by assumption (a). The second relation holds since πk is the policy by which

the agent acts at episode k in the true MDP. The third relation holds by Lem. 36.

Bound on (ii). To bound (ii) use the fact that

V πk
h+1(s;

lk,

|

. H

pk)
|

(48)

for every s since the immediate cost is bounded in

lk
h(s, a)
e
e
|
|
wise up to constants, since the second term is bounded by

e

. lh(s, a) +

1
√nk−1

h

(s,a)

. lh(s, a) component-

(1). Thus,

O

e

(ii) . H

H

≤

= H

′

K

H

Xk=1
K

′

Xh=1
H

Xk=1
K

′

Xh=1
H

Xk=1
K

′

Xh=1
H

E[

s

1
nk
h(sh, ah)

E[

s

1
nk
h(sh, ah)

1

∨

1

∨

Xs′
√

N

p

sXs′

ph(s′

|

sh, ah) +

S
nk
h(sh, ah)

1 |

∨

s1, p, πk]

ph(s′

|

sh, ah) +

S
nk
h(sh, ah)

1 |

∨

s1, p, πk]

E[

s

1
nk
h(sh, ah)

1

∨

√

N

+

S
nk
h(sh, ah)

1 |

∨

s1, p, πk]

= H

E[

1
h, ak
h)

√

1

N

+

S
h, ak
h)

h(sk
nk

h(sk
nk

s

Xk=1

Xh=1
H 4K + √

. √S

∨
H 2SA + SH 3A

N

N

1 | Fk−1]

∨
H 4K + (√

N

√S

N

≤

O

(cid:16)

+ H)H 2SA

.

(cid:17)

The ﬁrst relation holds by plugging the bound (48) and assumption (b) into (ii). The second relation
e
holds by Jensen’s inequality. The third relation holds since p is a probability distribution. The forth
relation holds since πk is the policy with which the agent interacts with the true CMDP. The ﬁfth relation
holds by Lem. 36 (its assumption holds by assumption (c)).

Combining the bounds on (i) and (ii) we conclude the proof.

34

lk
Lemma 30 (On Policy Errors for Truncated Policy Estimation). Let lh(s, a),
h(s, a) be a a cost function,
and its optimistic cost. Let p be the true transition dynamics of the MDP and pk be an estimated
transition dynamics. Let V π
h (s; l, p) be the value of a policy π according to the cost and transition model
V π
lk, pk) be a value function calculated by a truncated value estimation (see
h (s;
l, p. Furthermore, let
[K]:
Algorithm 5) by the cost and transition model
b

lk, pk. Assume the following holds for all s, a, h, k

∈

e

e

lk
h(s, a)

1.

|

lh(s, a)
|

−

.

1
√nk−1

h

.

(s,a)

e

2.

e
pk
h(s′
|

|

s, a)

−

ph(s′

s, a)
|

|

.

3. nk−1
h

(s, a)

1
2

≤

j<k qπk

h (s, a

r
p)

|

H ln SAH

δ′

.

−

ph(s′|s,a)
nk−1
h

(s,a)∨1

+

1
(s,a)∨1

.

nk−1
h

Furthermore, let πk be the policy by which the agent acts at the kth episode. Then, for any K ′

P

′

K

V πk
1 (s1; l, p)

V πk
1 (s1;

−

|

Xk=1
b
Proof. The following relations hold.

lk, pk)

| ≤

e

√S
(cid:16)

N

O

e

H 4K + (√

N

+ H)H 2SA

.

(cid:17)

′

K

|

Xk=1
K

V πk
1 (s1; l, p)

V πk
1 (s1;

lk,

pk)
|

−

′

H

b

(lh(sh, ah)

e
ph(

e
sh, ah)

−

· |

=

E[

Xk=1(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Observe that

Xh=1

V πk
h+1 −

Qπk (sh, ah;

lk, pk)

s1, p, πk]

|

e

b

e

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

[K]

∈

(49)

(50)

Qπk (sh, ah;

lk, pk) = min

0,

lk
h(sh, ah)

pk
h(

· |

−

−

sh, ah)

V πk

,

−

n

o

where the ﬁrst relation holds by the extended value diﬀerence lemma 34. Plugging back to (50) we get

b

e

(50)

≤

′

K

H

E[

Xk=1

Xh=1

lh(sh, ah)

|

−

lk
h(sh, ah)

| |

s1, p, πk]

′
K
|

H

E[

+

Xk=1

s
Xh=1 X

(ph −

|

We now bound the terms (i) and (ii).

|

e

(i)

{z
h)(s′
pk

e

sh, ah)

||

|

V πk
h+1(s′;

}
lk,

pk)

| |

s1, p, πk]

,

b

e

e

(ii)

{z

}

Bound on (i). To bound (i) we use the assumption (1) and get,

′

K

H

(i) .

Xk=1
K

′

Xh=1
H

Xk=1

Xh=1

=

E[

E[

1
(sh, ah) |

nk−1
h

s1, p, πk]

q

1

nk−1
h

(sk

h, ak

h) | Fk−1]

q

√SAH 2K + SAH

.

(cid:17)

≤

O

e

(cid:16)

The ﬁrst relation holds by assumption (1). The second relation holds since πk is the policy by which

the agent acts at the kth episode at the true MDP. The third relation holds by Lemma 36.

35

Bound on (ii). To bound (ii) use the fact that

V πk
h+1(s;

lk,

|

. H

pk)
|

(51)

for every s since the immediate cost is bounded in

b

e

lk
h(s, a)
e
|
|

. lh(s, a)

1 +

1
√nk−1

h

(s,a)

≤

. lh(s, a)

component-wise up to constants, since the second term is bounded by

e

(1). Thus,

O

e

(ii) . H

H

≤

= H

′

K

H

Xk=1
K

′

Xh=1
H

Xk=1
K

′

Xh=1
H

Xk=1
K

′

Xh=1
H

E[

s

1
nk
h(sh, ah)

E[

s

1
nk
h(sh, ah)

1

∨

1

∨

Xs′
√

N

p

sXs′

ph(s′

|

sh, ah) +

S
nk
h(sh, ah)

1 |

∨

s1, p, πk]

ph(s′

|

sh, ah) +

S
nk
h(sh, ah)

1 |

∨

s1, p, πk]

E[

s

1
nk
h(sh, ah)

1

∨

√

N

+

S
nk
h(sh, ah)

1 |

∨

s1, p, πk]

= H

E[

1
h, ak
h)

√

1

N

+

S
h, ak
h)

h(sk
nk

h(sk
nk

s

Xk=1

Xh=1
H 4K + √

. √S

∨
H 2SA + SH 3A

N

N

1 | Fk−1]

∨
H 4K + (√

N

√S

N

≤

O

(cid:16)

+ H)H 2SA

.

(cid:17)

e
The ﬁrst relation holds by plugging the bound (51) and assumption (2) into (ii). The second relation
holds by Jensen’s inequality. The third relation holds since p is a probability distribution. The third
relation holds since πk is the policy with which the agent interacts with the true MDP p. The ﬁfth
relation holds by Lemma 36 (its assumption holds by assumption (3)).

Combining the bounds on (i) and (ii) we conclude the proof.

lk
Lemma 31 (On Policy Errors for Bonus Based Optimism). Let lh(s, a),
h(s, a) be a cost function, and
its optimistic cost. Let p be the true transition dynamics of the MDP and pk−1 be an estimated transition
dynamics. Let V π
lk, pk−1) be the value of a policy π according to the cost and transition
model l, p and

lk, pk−1, respectively. Assume the following holds for all s, a, s′, h, k

h (s; l, p), V π

h (s;

[K]:

e

∈

e

lk
h(s, a)

1.

|

e
−

lh(s, a)
|

.

1
(s,a)∨1

+

nk−1

h

s′ H

h

pk−1
nk−1

h

(s′|s,a)
(s,a)∨1

+

((nk−1

h

HS
(s,a)−1)∨1)

.

e
pk−1
h

(s′

2.

q

s, a)

|

−

ph(s′

|

s, a)

.

(cid:12)
(cid:12)
3. nk−1
h

(s, a)

1
2

≤

j<k qπk

h (s, a; p)

(cid:12)
(cid:12)

r

−

r

P
ph(s′|s,a)

H ln SAH

δ′

.

(nk−1

h

(s,a)−1)∨1

+

(nk−1

h

1

(s,a)−1)∨1

.

4. V πk

h (s;

5. lh(s, a)

e
−

V πk
h (s; l, p).

lk, pk−1)

P
≤
lk
h(s, a) + (ph(

s, a)

pk−1
h

(
· |

−

s, a))V π

h+1(
·|

l, p)

≥

0.

· |

Let πk be the policy by which the agent acts at episode k. Then, for any K ′

e

[K]

∈

′

K

Xk=1

V πk
1 (s1; l, p)

−

V πk
1 (s1;

lk, pk−1)

≤

O

√S

N

H 4K + S2H 4A(

N

(cid:16)

H + S)

.

(cid:17)

Proof. Denote for any s, h

e
V πk
h (s) = V πk
h (s;

e
lk, pk−1) and V πk

h (s) = V πk

h (s; l, p). The following relations

e

e

36

hold:

′

K

V πk
1 (s1)

V πk
1 (s1)

−

Xk=1
K

′

=

H

E

e
(lh(sh, ah)

Xk=1
K

′

Xh=1
H

h

E

Xk=1

Xh=1

h

≤

lh(sh, ah)

|

−

−

lk
h(sh, ah)) + (ph −
e
lk
h(sh, ah)
|

s1, p, πk

pk−1
h

)(

· |

sh, ah)

V πk
h+1

e

s1, p, πk
(cid:12)
(cid:12)
(cid:12)

i

i

(cid:12)
(cid:12)
(cid:12)

e

(i)

{z
(ph −

pk−1
h

)(s′

|

}
V πk
h+1(
·

|

; l, p)(s′)
|

sh, ah)

(cid:12)
(cid:12)

(ii)

s1, p, πk

i

(cid:12)
(cid:12)
(cid:12)

E
h Xs′
(cid:12)
(cid:12)

|
+

+

′

K

H

Xk=1

Xh=1

′

K
|

H

Xk=1

Xh=1

E

(ph −
h(cid:12)
(cid:12)
(cid:12)

pk−1
h

)(

· |

{z
sh, ah)(V πk

h+1(
·

;

lk, pk−1)

V πk
h+1(
·

−

(iii)

e

}
; l, p))

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

s1, p, πk

,

(52)

i

}

where the ﬁrst relation holds by the value diﬀerence lemma (see Lem. 35).

{z

|

V πk
Bound on (i) and (ii). Since 0
h+1(
·
we can bound both (i) and (ii) by the same analysis as in Lem. 29. Thus,

; l, p)(s)

≤

≤

H (the value of the true MDP is bounded in [0, H]),

(i) + (ii)

√S

N

≤

H 4K + (√

N

+ H)H 2SA.

Bound on (iii). Applying Lem. 32 we obtain the following bound

(iii) . S2H 4A(

N

H + S) + √

N

SH 5/2√A

sXk

(V πk

1 (s1)

−

V πk
1 (s1)).

Plugging the bounds on terms (i), (ii), and (iii) into (52) we get

′

K

V πk
1 (s1)

Xk=1
. √S

N

V πk
1 (s1)

−

e
H 4K + S2H 4A(

H + S) + √

SH 5/2√A

N

N

sXk
V πk
1 (s1) this bound has the form 0

H 4K + S2H 4A(

H + S)

N

N
SH 5/2√A.

Denoting X =

′

K

k=1 V πk

1 (s1)

P

Applying Lem. 38, by which X

−
a = √S
e
b = √

N
a + b2, we get

≤

e

(V πk

1 (s1)

−

V πk
1 (s1)).

e

a + b√X, where

X

≤

≤

′

K

Xk=1

V πk
1 (s1)

−

V πk
1 (s1) . √S

N

H 4K + S2H 4A(

N

H + S).

e

Lemma 32. Let the assumptions of Lem. 31 hold. Then, for any K ′

′

K

H

Xk=1

Xh=1

E

h(cid:12)
(cid:12)
(cid:12)

(ph −

pk−1
h

)(

· |

h, ak
sk

h)(V πk

h+1(
·

;

lk, pk−1)

V πk
h+1(
·

−

; l, p))

. S2H 4A(

N

H + S) + √

N

e
SH 5/2√A

sXk

37

[K]

∈

(cid:12)
(cid:12)
(cid:12)
1 (s1)

(V πk

| Fk−1

i

V πk
1 (s1)).

−

e

Proof. Denote for any s, h
hold:

h (s) = V πk
V πk

h (s;

lk, pk−1) and V πk

h (s) = V πk

h (s; l, p). The following relations

e
pk−1
h

e
V πk
h+1 −

sh, ah)(

· |

(ph −

pk−1
h

)(

e
· |

s, a)(

(ph −

pk−1
h

)(s′

(cid:12)
(cid:12)

ph(s′

s, a)

|
nk
h(s, a)

Xs′ p
q

(cid:12)
(cid:12)
(cid:12) e

H

E

"

Xk
=

)(

(ph −
t=1(cid:12)
X
(cid:12)
qπk
(cid:12)
h (s, a; p)
(cid:12)
(cid:12)
(cid:12)
qπk
h (s, a; p)
Xs′

Xk,h,s,a

Xk,h,s,a

qπk
h (s, a; p)

Xk,h,s,a

≤

.

|

s1, πk, p

|

#

V πk
h+1)
(cid:12)
(cid:12)
V πk
(cid:12)
h+1 −

e
s, a)

V πk
h+1)
(cid:12)
(cid:12)
(cid:12)
V πk
h+1(s′)
(cid:12)
(cid:12)
(cid:12)
(cid:12) e
(cid:12)
V πk
h+1(s′)

|

−

−

V πk
h+1(s′)
(cid:12)
(cid:12)
(cid:12)

+

Xk,h,s,a

V πk
h+1(s′)
(cid:12)
(cid:12)
(cid:12)

In the third relation we used assumption (2) of Lem. 31 as well as bounding

qπk
h (s, a; p)

H 2S2
nk
h(s, a)

.

(53)

|

}

(ii)

{z

}

. SH 2

(54)

(i)

{z

(cid:12)
(cid:12)
(cid:12) e

V πk
h+1(s)

−

V πk
h+1(s)
(cid:12)
(cid:12)
(cid:12)

V πk
h+1(s)
since
Note that V πk
e

[
−
∈
[0, H] as usual.
h+1(s)
∈
Term (ii) is bounded as follows

SH 2, H] by the assumption on its instantaneous cost (assumption (1) of Lem. 31).

(ii) = H 2S2

E

Xk,h

(cid:20)

1
h, ak
h(sk
nk

h) |

s1, πk, p

= H 2S2

E

(cid:21)

Xk,h

(cid:20)

1
h, ak
h(sk
nk

h) | Fk−1

(cid:21)

. H 4S3A,

(55)

by Lem. 37.

We now bound term (i) as follows.

(i)

≤

Xk Xs,a,h

qπk
h (s, a; p)

N

q

s′ ph(s′

|

s, a)(

V πk
h+1(s′)

V πk
h+1(s′))2

−

P

nk
h(s, a)
e

√

N

≤

= √

N

. √

N

s

Xk Xs,a,h

s

Xk Xs,a,h

SH 2√A

qπk
h (s, a; p)

qπk
h (s, a; p)

1
nk
h(s, a)

1
nk
h(s, a)

q

qπk
h (s, a; p)ph(s′

sXk Xs,a,h Xs′
qπk
h+1(s′, a; p)(

qπk
h+1(s, a; p)(V πk

sXk Xs′,a,h
h+1(s)

−

e

V πk
h+1(s))

sXk Xs,a,h

√

N

≤

SH 5/2√A

√

N

≤

SH 5/2√A

sXk

sXk

(V πk

1 (s1)

(V πk

1 (s1)

−

−

V πk
1 (s1)) +

e
V πk
1 (s1))

e

s, a)(

V πk
h+1(s′)

V πk
h+1(s′))2

−

|

V πk
h+1(s′)

e
V πk
h+1(s′))2

−

e

Xk,h,s,a

qπk
h (s, a; p)
(cid:12)
(cid:12)
(cid:12)

(ph −

ph)(

· |

s, a)(

V πk
h+1 −

e

V πk
h+1)
(cid:12)
(cid:12)
(cid:12)

N

· |

ph)(

+ √

s, a)(

s Xk,h,s,a

SH 5/2√A

V πk
h+1 −

qπk
h (s, a; p)
(ph −
(cid:12)
(cid:12)
(cid:12)
The ﬁrst relation holds by Jensen’s inequality while using the fact that ph(
s, a) has at most
· |
non-zero terms. The second relation holds by Cauchy-Schwartz inequality. The third relation fol-
s,a ph(s′
s, a)qh(s, a; p) =
|
V πk
V πk
h+1(s))2 .
h+1(s)
SH 2(V πk
0 due to optimism (assumption (4) of
P
−
Lem. 31). The ﬁfth relation holds by Lemma 33 (see that its assumption holds by assumption (5)). The
sixth relation holds by √a + b

N
lows from properties of the occupancy measure (see Eq. 8). In particular,

a qh+1(s′, a; p). The forth relation holds by applying Lem. 37 and bounding (

V πk
h+1(s)) due to (54) and V πk

V πk
h+1)
(cid:12)
(cid:12)
(cid:12)

V πk
h+1(s)

√a + √b.

h+1(s)

h+1(s)

(56)

P

≥

−

−

e

e

.

e

e

≤

38

Plugging the bounds on term (i), (55), and term (ii), (56), into (53) we get

Xk,h,s,a

ph)(

(ph −

qπk
h (s, a; p)
(cid:12)
(cid:12)
(cid:12)
SH 5/2√A
N

H 4S3A + √

≤

s, a)(

V πk
h+1 −

· |

e
(V πk
1 (s1)

V πk
h+1)
(cid:12)
(cid:12)
(cid:12)
V πk
1 (s1))

−

+ √

N

SH 5/2√A

s Xk,h,s,a

Denoting X =

a + b√X, where

P

k,h,s,a qπk

h (s, a; p)
(cid:12)
(cid:12)
(cid:12)

(ph −

e
ph)(

· |

s, a)(

V πk
h+1 −

e

.

V πk
h+1)
(cid:12)
(cid:12)
(cid:12)

sXk
qπk
h (s, a; p)
(ph −
(cid:12)
(cid:12)
(cid:12)
s, a)(

ph)(

· |

V πk
h+1 −

V πk
h+1)
(cid:12)
(cid:12)
(cid:12)

e

this bound has the form 0

X

≤

≤

a = H 4S3A + √

N

SH 5/2√A

sXk

(V πk

1 (s1)

V πk
1 (s1))

−

e

b = √

N

SH 5/2√A.

Applying Lem. 38, by which X

a + b2, we get

≤

Xk,h,s,a

ph)(

qπk
h (s, a; p)
(ph −
(cid:12)
(cid:12)
(cid:12)
SH 5/2√A
N

H 4S3A + √

S2H 4A(

N

H + S) + √

N

≤

≤

V πk
h+1)
(cid:12)
(cid:12)
(cid:12)
V πk
1 (s)) +

s, a)(

V πk
h+1 −

· |

e
(V πk
1 (s)

sXk
SH 5/2√A

−

e
(V πk

1 (s)

V πk
1 (s))

−

e

sXk

S2H 5A

N

Lemma 33. Let lh(s, a),
probabilities. Let V π
the cost and transition model l, p and

h (s) := V π

e

lh(s, a) be a cost function and its optimistic cost. Let p, p be two transition
lk, p) be the value of a policy π according to

h (s) := V π
V π

h (s; l, p) and

h (s;

l, pk, respectively. Assume that

e

e
ph(

s, a))V π

h+1 ≥

0,

(57)

· |

lh(s, a)

−

lh(s, a) + (ph(

e

· |

s, a)

−

for any s, a, h. Then, for any π and s

e

H

Xh=2

E

V π
h (sh)
h

≤

H

V π
1 (s)

(cid:16)

−

V π
h (sh)

|

−

s1 = s, π, p

e
V π
1 (s)

H

E

+ H

e

(cid:17)

Xh=1

h(cid:12)
(cid:12)
(cid:12)

i

(ph(

· |

sh, ah)

ph(

· |

−

sh, ah′))(

V π
h+1 −

e

|

V π
h+1)
(cid:12)
(cid:12)
(cid:12)

s1 = s, π, p

i

39

Proof. By deﬁnition

V π
1 (s)

−
V π
1 (s1)
e

V π
1 (s)
= E
h
+ E
h
V π
2 (s2)

= E
h
+ E
h
= E
V π
2 (s2)
h
+ E
h
+ E
h
V π
2 (s2)

(p1(

E

≥

h
+ E
h

(p1(

· |

l1(s1, a1)

p1(

s1, a1)

−
l1(s1, a1) + p1(

V π
2 (s2)

|

−

· |
−
V π
s1, a1)
2 −
s1 = s, π, P

e

· |

V π
2 |
V π
1 (s)
e

s1 = s, π, P

i
s1 = s, π, P

i

|

e

i
· |

l1(s1, a1)
e

l1(s1, a1) + (p1(

−
V π
2 (s2)
e

−
s1, a1)
e

|
p1(

s1 = s, π, P

i
s1, a1))(

· |
l1(s1, a1)

−

· |
l1(s1, a1) + (p1(

s1, a1)

p1(

· |

−

s1, a1))

V π
2 |

s1 = s, π, P

e

s1 = s, π, P

V π
2 )

V π
2 −
s1, a1)
e

|
p1(

−

· |

· |

i
s1, a1))V π
2 |

s1 = s, π, P

i

i

−
V π
2 (s2)
e

−
s1, a1)
e

|
p1(

−

· |

s1 = s, π, P

i
s1, a1))(

V π
2 −

V π
2 )

|

where the ﬁrst relation holds by the value diﬀerence lemma 35 and the last relation holds due to the
e
assumption 57.

i

Iterating on this relation we get that for any h

2, ..H

}

∈ {

s1 = s, π, P

,

(58)

V π
1 (s)
E

≥

V π
1 (s)

−
V π
h (sh)
e

h
h−1
+

E

Xh′=1

h

V π
h (sh)

|

−

s1 = s, π, P

e
(ph′(
· |

sh′, ah′)

ph′(

· |

−

i
sh′, ah′ ))(

V π
h′+1 −

V π
h′+1)

|

s1 = s, π, P

.

i

By summing this relation for h

2, ..H

∈ {

}

and rearranging we get

e

H

V π
1 (s)

V π
1 (s)

−

(cid:16)

H

≥

Xh=2
Thus,

e
V π
h (sh)

E

h

H

h−1

E

Xh=2

Xh′=1

h

−

(cid:17)

V π
h (sh)

|

−

e

s1 = s, π, P

.

i

(ph′(

· |

sh′, ah′)

ph′(

· |

−

sh′, ah′ ))(

V π
h′+1 −

V π
h′+1)

|

s1 = s, π, P

e

i

H

Xh=2

E

V π
h (sh)
h

≤

H

V π
1 (s)

(cid:16)

H

≤

V π
1 (s)
(cid:16)

≤

H

V π
1 (s)

(cid:16)

−

−

−

V π
h (sh)

|

−

s1 = s, π, P

i

H

h−1

E

Xh=2
H

Xh′=1

H

h(cid:16)

e
V π
1 (s)

(cid:17)

e
V π
1 (s)

+

+

(cid:17)

e
V π
1 (s)

Xh=2
H

Xh′=1
E

+ H

e

(cid:17)

Xh=1

h(cid:12)
(cid:12)
(cid:12)

E

(ph′(

h(cid:12)
(cid:12)
(cid:12)
(ph(

· |

(ph′(

· |

−

sh′ , ah′)

ph′(

· |

−

sh′, ah′))(

V π
h′+1 −

V π
h′+1)

(cid:17)

s1 = s, π, P

|

i

sh′, ah′ )

ph′ (

· |

−

sh′, ah′ ))(

e
V π
h′+1 −

· |

sh, ah)

ph(

· |

−

sh, ah′))(

e
V π
h+1 −

e

|

V π
h+1)
(cid:12)
(cid:12)
(cid:12)

V π
h′+1)
(cid:12)
(cid:12)
(cid:12)

s1 = s, π, P

|

i

s1 = s, π, P

.

i

F Useful Lemmas

We start stating the value diﬀerence lemma (a.k.a. simulation lemma). This lemma has been used in
several papers [e.g., Cai et al., 2019, Efroni et al., 2020]. The following lemma is central for the analysis
of OptPrimalDual-CMDP.

40

M

V π
1 (s1; c, p)
H
b
Xh=1
H

Qπ

hD

E

Xh=1

h

b

Lemma 34 (Extended Value Diﬀerence). Let π, π′ be two policies, and
and

H
h=1)
= (
h(s, a; c, p) be an approximation of the Q-function

ph}
{

ch}
{

H
h=1,

′ = (

,
A

M

S

H
h=1,

,

,

,

M

S
of policy π on the MDP

A

p′
h}
{

{

c′
h}
for all h, s, a, and let

H
h=1) be two MDPs. Let
V π
h (s; c, p) =

Qπ

′

V π
1 (s1; c′, p′) =

b

−

b

Qπ

h(s,

D

b

; c, p), πh(

· |

·

s)

. Then,

E

h(sh,

; c, p), π′

h(

· |

·

sh)

πh(

· |

−

sh)

E

|

s1, π′, p′

+

i

b
Qπ
h(sh, ah; c, p)

E

c′
h(sh, ah)

p′
h(
·|

−

sh, ah)

V π
h+1(
·

; c, p)

|

−

s1, π′, p′

i

where V π

1 (s; c′, p′) is the value function of π′ in the MDP

′

b

′.

M

The following lemma is standard [see e.g., Dann et al., 2017, Lem. E.15], and can be seen as a corollary

of the extended value diﬀerence lemma.

Lemma 35 (Value diﬀerence lemma). Consider two MDPs
(
S

H
ch}
h=1,
{
H
h=1). For any policy π and any s, h the following relation holds.

p′
h}
{

ph}

,
A

= (

M

A

S

{

,

,

,

H
h=1) and

′ =

M

H
c′
h=1,
h}
{
V π
h (s; c, p)

H

h (s; c′, p′)
V π

−

= E[

(ch(sh, ah)

Xh′=h

H

= E[

(c′

h(sh, ah)

Xh′=h

c′
h(sh, ah)) + (ph −

−

p′
h)(

· |

sh, ah)V π

h+1(
·

; c, p)

|

sh = s, π, p′]

ch(sh, ah)) + (p′

h −

ph)(

· |

sh, ah)V π

h+1(
·

; c′, p′)

|

sh = s, π, p].

−

The following lemmas are standard. There proof can be found in [Dann et al., 2017, Zanette and Brunskill,

2019, Efroni et al., 2019] (e.g., Efroni et al. 2019, Lem. 38).

Lemma 36. Assume that for all s, a, h, k

[K]

nk−1
h

(s, a) >

∈
1
2

qπk
h (s, a; p)

H ln

−

SAH
δ′

,

Xj<k

then

K

H

1
h, ak
(sk
h)
Lemma 37 (e.g., Zanette and Brunskill [2019], Lem. 13). Assume that for all s, a, h, k

(√SAH 2K + SAH)

1 | Fk−1

nk−1
h

# ≤

Xh=1

Xk=1

"s

O

∨

E

e

[K]

∈

nk−1
h

(s, a) >

qπk
h (s, a; p)

H ln

−

SAH
δ′

,

1
2

Xj<k

then

K

H

E

Xk=1

t=1
X

(cid:20)

1
t , ak
nk−1(sk
t )

1 | Fk−1

(cid:21)

∨

Lemma 38 (Consequences of Self Bounding Property). Let 0

SAH 2

.

≤

O

e
X

(cid:0)

≤

≤

(cid:1)

a + b√X where X, a, b

R. Then,

∈

Proof. We have that

Since X

≥

0 this implies that

X . a + b2.

b√X

X

−

a

−

≤

0.

b2 + 4a

√X

b
2

b
2

+

+

≤

≤

r

r

1
4
b2
4

+ √4a

≤

b + 2√a,

41

where we used the relation √a + b

√a + √b.

0 by squaring the two sides of the later inequality we get

≤

Since √X

≥

where in the second relation we used the relation (a + b)2

2a2 + 2b2.

≤

(b + 2√a)2

X

≤

≤

2b2 + 4a . b2 + a,

F.1 Online Mirror Descent

In each iteration of Online Mirror Descent (OMD) the following problem is solved:

xk+1 ∈

arg min
x∈C

gk, x

tKh

xki

−

+ Bω (x, xk) ,

(59)

where tK is a stepsize, and Bω (x, xk) is the bregman distance.

When choosing Bω (x, xk) as the KL-divergence, and the set C is the unit simplex OMD has the

following closed form,

xk+1 ∈

arg min
x∈C{

tKh∇

fk(xk), x

xki

−

+ dKL(x

||

,

xk)
}

The following lemma [Orabona, 2019, Theorem 10.4] provides a fundamental inequality which will be

used in our analysis.

Lemma 39 (Fundamental inequality of Online Mirror Descent). Assume gk,i ≥
0 for k = 1, ..., K and
i = 1, ..., d. Let C = ∆d. Using OMD with the KL-divergence, learning rate tK, and with uniform
initialization, x1 = [1/d, ..., 1/d], the following holds for any u

∆d,

∈

gt, xk −
h

u

i ≤

log d
tK

+

tK
2

K

Xk=1

K

d

Xk=1

i=1
X

xk,ig2
k,i

In our analysis, we will be solving the OMD problem for each time-step h and state s separately,

πk+1
h

s)

(
· |

∈

arg min
π∈∆A

tK

Qk

h(s,

), π

·

−

xk
h(

· |

s)

+ dKL(π

πk
h(

· |

||

s)).

(60)

Therefore, by adapting the above lemma to our notation, we get the following lemma,

(cid:10)

(cid:11)

Lemma 40 (Fundamental inequality of Online Mirror Descent for RL). Let tK > 0. Let π1
h(
uniform distribution for any h
∈
solving (60) separately for any k
π,

s) be the
[0, M ] for all s, a, h, k. Then, by
, the following holds for any stationary policy

[H] and s
[K], h

. Assume that Qk

∈ S
∈

[H] and s

h(s, a)

∈ S

· |

∈

∈

K

Xk=1
(cid:10)

Qk
h(

· |

s), πk
h(

· |

s)

πh(

· |

−

s)

≤

log A
tK

+

tKM 2K
2

(cid:11)

Proof. First, observe that for any k, h, s, we solve the optimization problem deﬁned in (60) which is
the same as (59). By the fact that the estimators used in our analysis are non-negative, we can apply
Lemma 39 separately for each h, s with gk = Qk
M 2 and

s) = 1 for all s concludes the result.

). Lastly, bounding (Qk

) and xk = πk

h(s, a))2

h(s,

h(s,

≤

·

·

a πk

h(a

|

P

G Useful Results from Constraint Convex Optimization

In this section we enumerate several results from constraint convex optimization which are central to
establish the bounds for the dual algorithms. To keep the generality of discussion, we follow results
from Beck [2017], Chapter 3, and consider a general constraint convex optimization problem

fopt = min

x∈X{

f (x) : g(x)

≤

0, Ax + b = 0

,
}

(61)

where g(x) := (g1(x), .., gI (x))T , and f, g1, .., gm : E
Rp. By deﬁning the vector of constraints
A

Rp×n, b

∈

∈

,
(
−∞

→

) are convex real valued functions,

∞

42

We deﬁne a value function associated with (61)

Furthermore, we deﬁne the dual problem to (61). The dual function is

v(u, t) = min

x∈X{

f (x) : g(x)

u, Ax + b = t
,
}

≤

where λ

Rm

+ , µ

∈

∈

q(λ, µ) = min
x∈X
Rp and the dual problem is

(cid:8)

L(x, λ, µ) = f (x) + λT g(x) + µT (Ax + b)

,

qopt = max

λ∈Rm

+ ,µ∈Rp{

(cid:9)

q(λ, µ) : (λ, µ)

dom(

−

.

q)
}

∈

(62)

Where dom(
of (62) by λ∗, µ∗.

−

q) =

(cid:8)

(λ, µ)

Rm

+ , µ

∈

∈

Rp : q(λ, µ) >

−∞

. Furthermore, denote an optimal solution

(cid:9)

We make the following assumption which will be veriﬁed to hold. The assumption implies strong

duality, i.e., qopt = fopt.

Assumption 3. The optimal value of (61) is ﬁnite and exists a slater point x such that g(x) < 0 and
exists a point

x + b = 0, where ri(X) is the relative interior of X.

ri(X) satifying A

x

∈

The following theorem is proved in Beck 2017.

b

b

Theorem 41 (Beck 2017, Theorem 3.59.). (λ∗, µ∗) is an optimal solution of (62) iﬀ

Where ∂f (x) denotes the set of all sub-gradients of f at x.

(λ∗, µ∗)

−

∈

∂v(0, 0).

Using this result we arrive to the following theorem, which is a variant of Beck 2017, Theorem 3.60.

Theorem 42. Let λ∗ be an optimal solution of the dual problem (62) and assume that 2
x satisfy A

x + b = 0 and

λ∗
k

k1 ≤

ρ. Let

e
then

e

Proof. Let

f (

x)

fopt + ρ

[g(

x)]+k∞ ≤

k

δ,

−

e

e
x)]+k∞ ≤

[g(
k

δ
ρ

.

e

v(u, t) = min

x∈X{

f (x) : g(x)

u, Ax + b = t
.
}

≤

Since (

λ∗, µ∗) is an optimal solution of the dual problem it follows by Theorem 41 that (

−

λ∗, µ∗)

∈

(63)

(64)

∂v(0, 0). Therefore, for any (u, 0)

−

dom(v)

∈

Set u =

u := [g(

x)]+. See that u

≥

e

e

Thus, (63) implies that

v(u, 0)

v(0, 0)

−
0 which implies that

≥ h−

λ∗, u

.

i

v(

u, 0)

v(0, 0) = fopt ≤

≤

f (

x).

e

f (

x)

fopt ≥ h−

−

λ∗,

u

.
i

e

We obtain the following relations.

(ρ

λ∗

k1)
k

− k

e
u
k∞ =

−k

≤ h−
= f (

u

e
λ∗
u
k∞ + ρ
k1k
k
λ∗, u
u
+ ρ
k∞
i
k
e
u
x)
fopt + ρ
k
e

−

k∞

e
k∞ ≤

δ,

e

e

δ
λ∗

− k

k1 ≤

2
ρ

δ,

where the last relation holds by (64). Rearranging, we get

by using the assumption 2

λ∗
k

k1 ≤

[g(

k

x)]+k∞ =
ρ.
e

u
k

k∞ ≤

ρ

43

Lastly, we have the following useful result by which we can bound the optimal dual parameter by the

properties of a slater point. This result is an adjustment of Beck 2017, Theorem 8.42.

Theorem 43. Let x

X be a point satisfying g(x) < 0 and Ax + b = 0. Then, for any λ, µ

∈

Rm

+ , µ

∈

Rp

∈
+ : q(λ, µ)

λ

∈

(cid:8)

M

≥

(cid:9)

λ
k1 ≤

k

f (x)
M
minj=1,..,m −

−

gj(x)

Proof. Let

By deﬁnition, for any λ, µ

SM we have that

(cid:8)

∈

SM =

λ

Rm

+ , µ

∈

∈

Rp

+ : q(λ, µ)

.

M

.

≥

(cid:9)

M

q(λ, µ)

f (x) + λT g(x) + µT (Ax + b)

≤
= min
x∈X
f (x) + λT g(x) + µT (Ax + b)

(cid:8)

≤

m

Therefore,

= f (x) +

λj gj(x).

j=1
X

m

−

j=1
X

λjgj(x)

f (x)

M,

−

≤

which implies that for any (λ, µ)

SM

∈
m

λj =

λ
k1 ≤

k

j=1
X

f (x)

M

−
minj=1,..,m(

gj(x))

−

(cid:9)

.

From this theorem we get the following corollary.

Corollary 44. Let x
solution. Then,

∈

X be a point satisfying g(x) < 0 and Ax + b = 0, andλ∗ be an optimal dual

λ∗

k

k1 ≤

f (x)
M
minj=1,..,m −

−

gj(x)

Proof. Since (λ∗, µ∗)

∈

Sfopt be an optimal solution of the dual problem (62).

44

