2
2
0
2

t
c
O
6
1

]
L
C
.
s
c
[

2
v
2
2
5
4
1
.
5
0
2
2
:
v
i
X
r
a

A Character-Level Length-Control Algorithm for
Non-Autoregressive Sentence Summarization

Puyuan Liu, Xiang Zhang, Lili Mou∗
Dept. Computing Science, Alberta Machine Intelligence Institute (Amii)
University of Alberta, Canada
∗Canada CIFAR AI Chair, Amii
{puyuan, xzhang23}@ualberta.ca, doublepower.mou@gmail.com

Abstract

Sentence summarization aims at compressing a long sentence into a short one that
keeps the main gist, and has extensive real-world applications such as headline
generation. In previous work, researchers have developed various approaches to
improve the ROUGE score, which is the main evaluation metric for summarization,
whereas controlling the summary length has not drawn much attention. In our work,
we address a new problem of explicit character-level length control for summariza-
tion, and propose a dynamic programming algorithm based on the Connectionist
Temporal Classiﬁcation (CTC) model. Results show that our approach not only
achieves higher ROUGE scores but also yields more complete sentences.1

1

Introduction

Sentence summarization aims at compressing a long sentence into a short one that keeps the main
gist of the input. It is an established setting of summarization, and has extensive real-world ap-
plications such as generating news headlines [26, 30, 8] and being a key component of document
summarization [40, 21]. In previous work, researchers have developed various approaches to improve
the ROUGE score, which is the main evaluation metric for summarization [17], whereas controlling
the summary length has not drawn much attention.

Recently, researchers argue that length control is the key to summarization, since it is typically
required by real-world applications [20]. Moreover, the ROUGE score is found to be sensitive to the
summary length [33, 32], and summarization systems can achieve higher scores by simply generating
longer output. Previous work mainly addresses length control in the word level, i.e., restricting the
summary length by a pre-deﬁned number of words [33, 18].

In this paper, we address explicit length control in the character level for summarization, which is a
different setting from previous work. In other words, we restrict the summary length by the number
of characters, such as letters, punctuation marks, and whitespaces. We observe that this is a more
realistic setting in real-world applications than word-level length control. For example, the headline
shown in a mobile app or web page is constrained by the screen width (roughly speaking, the number
of characters), rather than the number of words. Likewise, a commercial LED display allows a certain
number of characters; ideally, the text shown should ﬁt the character-level budget, or otherwise it
will be scrolling, making it difﬁcult to read. However, the character-level constraint is unlikely to
be satisﬁed if we only perform word-level control, despite the positive correlation between word
and character lengths. Moreover, character-level length control is a common evaluation setting for
summarization systems. For example, a sub-task in the DUC evaluation requires the maximum target
length to be 75 bytes [27]. Such an important setting, unfortunately, is not adequately addressed

1Our code, model, and output are released at: https://github.com/MANGA-UOFA/NACC

36th Conference on Neural Information Processing Systems (NeurIPS 2022).

 
 
 
 
 
 
in previous summarization studies. Our work largely bridges the gap between the methodological
summarization research and its evaluation.

We further observe that controlling the summary length by characters cannot be easily addressed by
previous approaches. For example, truncating is able to explicitly control the length, but the resulting
summary is incomplete; Takase and Okazaki [36] feed length embeddings into the model as input,
but such an approach cannot control the summary length in an explicit manner; and Schumann et
al. [33] perform constrained discrete optimization by selecting a certain number of words from the
source text as the output, but their generated summaries may vary to a large extent in terms of the
number of characters.

To this end, we propose NACC, a Non-Autoregressive summarization model with Character-level
length Control. We adopt a non-autoregressive approach because it generates all tokens in parallel
and is much faster than autoregressive models. More importantly, we observe that non-autoregressive
models predict the output words independently. Such predicted probabilities are thus local, which
provides us with the unique opportunity to design a dynamic programming algorithm to constrain
the summary length. Speciﬁcally, we formulate length control as a knapsack-like problem, where
the weight is the number of characters in a token and the value is the predicted probability of the
token. In this way, we are able to explicitly control the summary length at the character level, while
retaining the completeness of the output text.

We evaluated our NACC model on the Gigaword headline generation [10] and DUC2004 [27]
datasets in two settings: supervised and unsupervised. In the latter setting, NACC learns from the
pseudo-reference given by an unsupervised word-extraction method based on discrete search [33].
Experiments show that NACC establishes the state-of-the-art performance of non-autoregressive
summarization under various target lengths in both settings; NACC even outperforms autoregressive
Transformers [37] in the unsupervised setting, where the input and output have stronger correspon-
dence. These all conﬁrm the effectiveness of our length-control algorithm. Regarding inference
efﬁciency, we show that non-autoregressive models without length control are 10 times faster than
autoregressive ones; even with our length-control dynamic programming, NACC is still several times
more efﬁcient. Further, our NACC is capable of length-transfer generation, i.e., generating summaries
of different lengths from the training targets.

2 Related Work

Non-Autoregressive Generation. Non-autoregressive (NAR) models [12] predict target tokens in
parallel and thus enjoy a much higher inference efﬁciency than autoregressive (AR) ones, which
generate output tokens sequentially. NAR generation is more difﬁcult than AR, because there
lacks dependencies among the outputs. Previous work addresses the dependency issue by iterative
reﬁnement [16, 22, 3, 13] or structured decoding with the conditional random ﬁeld (CRF) [35, 34, 4].

The Connectionist Temporal Classiﬁcation (CTC) algorithm [11] addresses a common problem in
NAR generation, namely, token repetition, by merging consecutive identical tokens (unless separated
by an empty token). We follow our previous work [18] and adopt CTC for NAR summarization,
allowing empty tokens scattering over the entire sentence to generate a short summary.

NAR models are traditionally thought to generate worse-quality text than AR models. However,
we have the unique insight that NAR also brings new opportunities for designing length-control
algorithms: since the model predicts output independently, the decoding problem can be divided into
shared sub-problems for dynamic programming.

Text Summarization. Summarization systems can be generally categorized into two types: extrac-
tive and abstractive. Extractive methods output a summary by extracting important sentences or
clauses from the source text [5, 23, 15, 44], while abstractive methods generate summaries with new
expressions [25, 28, 9].

Depending on the availability of training data, we may categorize summarization systems into
supervised and unsupervised ones. Supervised methods typically follow a sequence-to-sequence
(seq2seq) paradigm [43, 19]. Recently, unsupervised summarization is drawing increasing attention
because it does not require parallel training data. Yang et al. [42] use the lead approach (selecting
the ﬁrst several words or sentences) as pseudo-groundtruth for seq2seq training. Alternatively, cycle
consistency is adopted as the training objective for unsupervised summarizaiton [38, 2]. Schumann et

2

al. [33] generate a summary by maximizing a heuristically deﬁned scoring function (involving ﬂuency
and semantic similarity) with word-level extraction. We consider both supervised and unsupervised
settings in our work.

Controlling the output length is crucial for deploying summarization systems to real-world ap-
In early extractive summarization research, truncating is adopted for fair compari-
plications.
son [23, 24, 15], but the resulting summary may not be complete sentences. As pointed out by [32, 33],
however, the length-control problem is not adequately addressed for abstractive summarization in the
neural era, probably because researchers do not want to hurt the completeness. For example, length
information [20, 36] and soft penalty [34] are adopted to encourage short summaries, but they cannot
control the length in an explicit way. Very recently, Schumann et al. [33] address the length-control
problem by extracting a certain number of source tokens as the summary. Our previous work [18]
designs a CTC-based algorithm that controls the number of output words explicitly. However, these
approaches cannot be applied to the character level. Our paper proposes a character-level length-
control algorithm that can explicitly constrain the number of characters within a given budget, which
is different from all previous work.

3 Approach

In this section, we ﬁrst introduce the CTC-trained NAR model for summarization (§3.1). Then, we
propose a dynamic programming algorithm to explicitly control the number of characters (§3.2).

3.1 A Non-Autoregressive Model for Summarization

In the summarization task, the goal is to compress a source text x = (x1, x2, . . . , xS) into a shorter
text y = (y1, y2, . . . , yT ), while preserving the key information.

Encoder-Only Architecture. Our work follows recent non-autoregressive summarization meth-
It is
ods [34, 18], utilizing the encoder-only Transformer [37] as the base model (Figure 1a).
essentially a stack of Transformer layers, the last of which predicts all words simultaneously. Thus, it
has a much higher inference efﬁciency than autoregressive models.
Formally, let E(i) ∈ RS×d be the contextual representation of the input x at the ith Transformer
layer, where S is the number of tokens in x and d is the dimension. Specially, E(0) is the word
and positional embeddings of input tokens. Each layer is a Transformer block [37] based on its
predecessor, which can be denoted by E(i) = Layer(i)(E(i−1)), for i ≥ 1.

At the last layer E(L), a softmax function is applied to predict the probability at every prediction slot
independently, given by Ps(·|x) = softmax(W e(L)
is a column
vector transposing the sth row of matrix E(L), and W is a weight matrix.

) for slots s = 1, · · · , S, where e(L)

s

s

Such an encoder-only architecture is able to capture the strong correspondence between the input
and output in the summarization task and outperforms encoder–decoder non-autoregressive models,
shown by our previous work [18].

CTC Training. Training the above encoder-only architecture requires padding the target with empty
tokens (cid:15); otherwise, the output will have the same length as the input, and cannot be a summary. Gu
et al. [12] pad (cid:15)s at the end of the target, but this hinders the input–output correspondence.

Instead, we follow our previous work [18] and train our model with the Connectionist Temporal
Classiﬁcation (CTC) [11], allowing (cid:15)s to scatter over the entire target summary as appropriate. During
inference, the CTC approach removes (cid:15)s to generate a shorter text as the summary. In addition,
CTC merges consecutive identical tokens that are not separated by (cid:15), because non-autoregressive
generation suffers from the word-repetition problem [12, 31]. We denote the CTC operation as Γ, for
example, Γ(aa(cid:15)(cid:15)abb(cid:15)) = aab.

For training, CTC maximizes the marginal likelihood of token sequences (including (cid:15)) that can be
reduced to the target text y:

P (y|x) =

(cid:88)

P (w|x) =

(cid:88)

S
(cid:89)

Ps(ws|x)

w:Γ(w)=y

w:Γ(w)=y

s=1

(1)

3

Figure 1: Overview of our NACC approach. Dashed yellow arrows refer to transitions that do not
increase the summary length, while solid blue arrows refer to the increase of length. Thick arrows
and blocks refer to the selected path by CTC. Due to the space limit, (cid:15) is omitted in the predicted
sentence, and we use (cid:15)(S) to denote a sequence of S-many (cid:15)s. The number demonstrates the value
(i.e., log-probability) of a word.

where P (w|x) is the probability of generating a token sequence w = w1, · · · , wS, and Ps(ws|x) is
given by the non-autoregressive model. Although a brute force summation in Eqn. (1) is intractable, it
can be efﬁciently computed by dynamic programming. We refer interested readers to Alex et al. [11]
for the details of the above marginalization in CTC training.

3.2 The Proposed Algorithm for Character-Level Length-Control Decoding

One of our main contributions is to propose a dynamic programming (DP) algorithm for character-
level length control. As mentioned in §1, controlling the number of characters in the output is
important for summarization, since real-world applications usually involve character-level length
constraints.

Our insight is that a non-autoregressive model predicts probabilities independently, so the length-
control problem can be divided into shared sub-problems, making DP algorithms feasible. We
formulate character-level length control as a knapsack-like problem. We treat the number of characters
in a word (plus one) as the weight,2 denoted by u(w) for the word w, and the predicted log-probability
as the value vs(w) = log Ps(w|x) for the prediction slot s. Our goal of character-level length-control
summarization can be formulated as

maximize
w1,··· ,wS

S
(cid:88)

s=1

vs(ws),

subject to

(cid:88)

u(y) < U

(2)

y∈y
y=Γ(w1,··· ,wS )

where U is the total length budget. Here, the value is the sum of the log-probability of every
generation slot including (cid:15), whereas the length is said in terms of the words of the CTC-reduced
sequence y = Γ(w1, · · · , wS).

We observe that handling every possible integer weight (i.e., length) as in a standard knapsack
algorithm may slow down the inference. Thus, we divide the lengths into buckets for efﬁcient
inference. Formally, let the lth bucket cover the length ranging from α · (l − 1) + 1 to α · l characters,
where α is a hyperparameter controlling the bucket size. We denote by ds,l = ds,l
the most
probable s-token sequence that is reduced to a summary in the lth length bucket. Specially, we let
ds,0 mean that the reduced summary has zero words.
The initialization of ds,l ﬁlls in the DP table for l = 0 and s = 1.

1 · · · ds,l
s

2For the purposes of research, we assume every word is appended with another character, namely, a
whitespace. In real applications, our algorithm can be applied to any measure of length, such as the display
width of a word in some font.

4

𝜖, -0.3US, -0.1Factory, -0.2N/A0[1,5][11,15][6,10][46,50]𝜖, -0.2us, -0.1𝜖𝜖, -0.5US, -0.2US maker,-0.3Factory has, -0.3𝜖𝜖𝜖,………Factory has, -0.5has, -0.2 𝜖𝜖𝜖𝜖,………Factory has a, -0.7a, -0.2              has, -0.4𝑤1:1𝑤1:2𝑤1:3𝑤1:4𝜖(𝑆),…………𝑤1:𝑆Generation slotsN/AN/A………………[41,45]Factory has a 2.3 % raise of orders in septemberTransformer……𝜖𝑤1𝑤2………Input: Factory orders for manufactured goods rose 2.3 percent in september, the commerce department said here thursday.Lengthbuckets𝑙(a) Model architecture             (b) Decoding algorithm                  …Embedding……………• For l = 0, we must have ds,0 = (cid:15) · · · (cid:15) (s-many), because l = 0 means no non-(cid:15) word has been

generated. (First row of Figure 1b.)

• For s = 1, we have

d1,l =




(cid:15),

argmax
w:u(w)∈[α·(l−1)+1,α·l]



v1(w),

if l = 0
if l > 0

(3)

Here, l = 0 is the same as the previous bullet item. For l > 0, we select the most probable word
for each length bucket according to the value v1(·), i.e., the predicted log-probability of the ﬁrst
generation slot. (First column of Figure 1b.)

The DP recursion is to compute ds,l based on a newly predicted token ws, assuming its top-left
sub-table is ﬁlled. This involves three scenarios:

• Case 1: ws = (cid:15). In this case, the new word is (cid:15). Thus, the index for generation slots increases
1 as the set containing

from s − 1 to s, but the summary length does not change. We denote D s,l
the candidate sequence, given by

1 = (cid:8)ds−1,l ⊕ (cid:15)(cid:9)
D s,l

(4)

where ⊕ denotes string concatenation. (See yellow dash arrows in Figure 1b.)

• Case 2: ws (cid:54)= (cid:15), but ws = ds−1,l

s−1 . In other words, the candidate non-(cid:15) word ws for the sth slot
is identical to the last token of ds−1,l. Since repeated tokens are merged during CTC decoding,
the output length index l is unchanged. We form this sequence as a set:
2 = (cid:8)ds−1,l ⊕ ds−1,l
D s,l

(5)

s−1

(cid:9)

(Also see yellow dash arrows in Figure 1b.)

• Case 3: ws (cid:54)= (cid:15) and ws (cid:54)= ds−1,l(cid:48)

for some l(cid:48) ≤ l. That is, ws is neither (cid:15) nor repetition, and
thus the summary length will be increased from bucket l(cid:48) to l. We denote this candidate set by

s−1

(cid:26)

D s,l

3 =

ds−1,l(cid:48)

⊕ ws :

(cid:16)

u(ws) +

(cid:88)

(cid:17)

u(d)

∈ [α · (l − 1) + 1, α · l],

(6)

d∈ds−1,l(cid:48)

ws (cid:54)= (cid:15), ws (cid:54)= ds−1,l(cid:48)

s−1

, and l(cid:48) ≤ l

(cid:27)

(See blue arrows in Figure 1b.)

Then, our DP ﬁnds the most probable sequence at each recursion step:

ds,l =

argmax
1 ∪D s,l

2 ∪D s,l

3

d∈D s,l

S
(cid:88)

s=1

vs(ds)

(7)

where ds is the sth token of a sequence d from the three candidate sets above.
Theorem 1. (1) If the bucket size α = 1 and consecutive repetitions are not merged, then dS,T is the
most probable sentence of T characters given by the S prediction slots. (2) If α (cid:54)= 1 or repeating
tokens are merged, our algorithm may not be exact. (See Appendix A for the proof.)

Discussion. Our DP algorithm is inspired by the standard 0-1 knapsack problem [1], but also differs
in several signiﬁcant ways. First, we merge consecutive tokens during CTC decoding; this establishes
some dependencies among different generation slots, and thus exact inference with DP is not possible.
Second, our value function is non-stationary, as it changes over time. We require that every slot
should select a token, either (cid:15) or a word. In both cases, the token’s value is added to the total
value. Therefore, our algorithm is compatible with negative values, namely, log-probabilities in our
application, because only the relative difference matters for the value function.

4 Experiments

4.1 Setup

Datasets. Our model is evaluated on the Gigaword headline generation [30] and DUC2004
datasets [27]. The Gigaword dataset comprises pairs of news articles and titles; we follow the

5

standard setting [30, 33, 18] and adopt the ﬁrst sentence of each news article as the input and consider
the news title as the summary. In total, the dataset contains 3.8M, 198K, and 1951 samples for
training, validation, and test, respectively. On the other hand, the DUC2004 dataset has 500 paired
samples and is designed for test only; its performance is obtained by models trained on Gigaword.

Metrics. We follow previous work [33, 30, 34] and evaluate the output by ROUGE scores [17]:
ROUGE-n evaluates n-gram overalpping, whereas ROUGE-L evaluates the longest common se-
quence. We follow the convention and adopt ROUGE F1 for the Gigaword dataset [38, 33, 18] and
ROUGE Recall for the DUC2004 dataset [6, 39].

Implementation Details. We use a Transformer encoder as the base model, which has 6 layers and 8
attention heads for each layer, following the settings in [37]. The dimensions are 512 and 2048 for the
attention and feed-forward modules, respectively. Each training batch contains samples amounting to
4K tokens. The learning rate is chosen from {1e-4, 5e-4} by validation, and we ran 100K gradient
updates for the unsupervised setting, but 400K updates for the supervised setting. Note that, in the
unsupervised setting, our model learns from an extractive search-based method [33], and thus its
training is relatively easy. By contrast, the supervised setting takes human-written sentences as the
training targets, which are more abstractive and difﬁcult to train with. All experiments were run on
an i9-9940X CPU and an RTX6000 GPU.

For our length-control algorithm, we adopt a bucket size of 4, and only consider the most probable 20
words for every generation slot (cf. ws in Eqn. 6) due to efﬁciency concerns.

4.2 Results and Analyses

Results on Gigaword Headline Generation. Table 1 presents the performance on the Gigaword
test set in both supervised and unsupervised settings. For the supervised setting, we train with the
groundtruth references, which contain 51.29 characters on average. In the unsupervised setting, the
training targets of machine learning models (Rows 9–13) are the output summaries given by an
unsupervised search-based method [33]; speciﬁcally, we adopt the standard 8-word setting, which
counts to 48.75 characters on average. Based on these, we set our length constraint to 50 characters.
We observe that our proposed algorithm is the only machine learning-based approach that can perform
explicit character-level length control. For fair comparison, we truncate the output of other methods
to satisfy the length constraint.

As seen, NACC even with truncating outperforms all other non-autoregressive (NAR) models [34, 29,
41] in both settings. Speciﬁcally, Su et al. [34] emit multiple end-of-sequence tokens at the end of
the output sequence to generate a shorter summary than the source text (Rows 1 & 9); Qi et al. [29]
propose to pretrain a summarization system in an autoregressive manner, and gradually adapt it to
the NAR setting (Rows 5 & 10); and Yang et al. [41]3 propose a two-step strategy of autoregressive
part-of-speech (POS) prediction and non-autoregressive summary generation (Rows 6 & 11). Our
NACC trained by CTC is able to surpass all these methods, demonstrating the effectiveness of
CTC training for summarization. Besides, NACC outperforms its search-based teacher model [33],
showing that learning can smooth out the search noise.

Equipped with the length-control algorithm, our NACC model has a signiﬁcant boost in terms of
ROUGE scores. In a fair comparison with the same base architecture, the length-control algorithm
alone improves NACC (truncate) by ~3 points. Our full model achieves an improvement of more
than 4 total ROUGE points compared with previous state-of-the-art NAR methods.

Regarding the inference efﬁciency, NACC with truncating is faster than previous NAR models. Even
with the length-control algorithm, our NACC is still in the same ballpark, being ~500x faster than the
search-based method.

Additionally, we design a variant of the unsupervised summarization method based on [33], where
we directly impose the character-level length constraint during each search step (Row 8). We ﬁnd
this approach outperforms truncating word-constrained search (Row 7), but is much worse than our
machine learning-based NACC with the length-control algorithm.

3Yang et al. [41] only provided execution commands in their GitHub repo, but no training code. We emailed

the authors, but have not obtained the code either. The reported results are based on our best replication.

6

Setting

Supervised

Unsupervised

#

1
2
3
4
5
6
7
8
9
10
11
12
13

NAR

Baseline

Search

NAR

Approach

Su et al. [34] (truncate)
Qi et al. [29] (truncate)
Yang et al. [41] (truncate)
NACC (truncate)
NACC (length control)
Lead-50 chars
Schumann et al. [33] (truncate)
Char-constrained search
Su et al. [34] (truncate)
Qi et al. [29] (truncate)
Yang et al. [41] (truncate)
NACC (truncate)
NACC (length control)

Len

38.43
27.98
35.37
34.15
34.40
49.03
45.45
44.05
45.24
44.54
49.37
47.77
47.03

R-1
32.28
31.69
28.85
33.12
33.66
20.66
24.98
25.30
24.65
24.31
21.70
25.79
27.45

ROUGE F1
R-L
R-2
14.21
30.56
30.05
12.52
27.00
6.45
31.34
13.93
31.79
13.73
19.30
7.08
23.18
9.08
9.25
23.43
22.98
8.64
22.48
7.66
20.13
4.60
23.75
8.94
25.14
8.87

∆R
0
-2.79
-14.75
1.34
4.74
-9.23
0.97
1.71
0
-1.82
-9.84
2,21
5.19

Time

0.016
0.019
–
0.011
0.017
–
9.573
17.324
0.017
0.019
–
0.012
0.025

Table 1: Performance on the Gigaword headline generation test set, where NAR stands for non-
autoregressive. Len: Average number of characters in the predicted summaries. R-1, R-2, R-L:
ROUGE-1, ROUGE-2, ROUGE-L. ∆R: The difference of total ROUGE (sum of R-1, R-2, and R-L)
in comparison with the (previous) state-of-the-art NAR summarization system [34]. Time: Average
inference time in seconds for one sample on an i9-9940X CPU and an RTX6000 GPU.

#

Approach

Search

1 Baseline
2
3
4
5
6
7

NAR

Lead-75 chars
Schumann et al. [33] (truncate)
Char-constrained search
Su et al. [34] (truncate)
Qi et al. [29] (truncate)
NACC (truncate)
NACC (length control)

ROUGE Recall

R-2
6.50
8.03
7.95
7.25
5.91
7.86
7.74

R-L
19.74
22.86
22.78
21.81
20.05
22.66
24.30

R-1
22.52
26.09
26.30
24.67
22.79
26.43
28.37

∆R
-4.97
3.25
3.30
0
-4.98
3.22
6.68

Time

–
30.362
31.540
0.017
0.018
0.012
0.030

Table 2: Results on DUC 2004 dataset.

Results on DUC2004. Table 2 shows the performance of NACC on DUC2004, where we constrain
the summary length to be at most 75 characters, following previous work [39, 2, 33]. Since the
Gigaword training summaries have very different lengths from DUC2004 summaries, it is not straight-
forward to evaluate NACC in the supervised setting, as this involves length transfer. Therefore, we
consider the unsupervised setting for DUC2004, where we use 13-word summaries (~80 characters)
from [33] as our training targets. (We will have length-transfer experiments in later analysis.)

Experiments show that NACC with length control again largely outperforms previous NAR models,
while retaining high inference efﬁciency. Results are consistent with the Gigaword experiment.

Human Evaluation. We additionally conducted human evaluations to investigate the effectiveness
of our approach. Due to the limit of time and resources, we mainly considered the overall quality
and completeness/ﬂuency, as they are the key of our work but may not be adequately captured by
automatic metrics such as ROUGE scores. For controlled comparison, we consider NACC with
truncating and NACC with the proposed length-control algorithm.

We invited three human annotators to compare the output summaries on each of the 150 randomly
selected samples from the Gigaword test set. We adopted the unsupervised setting in Table 1, where
models were trained with 8-word summaries generated by [33]. The two systems’ outputs for a

Overall quality

Completeness
& ﬂuency

Decoding
Truncate
Length control
Truncate
Length control

Ties
Wins
18% 44%
38% 44%
22% 36%
42% 36%

Loses
38%
18%
42%
22%

p-value

0.0001

0.0002

Table 3: Human evaluation comparing truncating and length-control decoding of our NACC approach
on 150 samples selected from the Gigaword headline generation dataset in the unsupervised setting.
The p-value is given by a two-sided binomial test.

7

Setting

Supervised

Unsupervised

#

1
2
3
4
5
6
7
8

Approach

AR

NAR

AR

NAR

Transformer (truncate)
Transformer (length control)
NACC (truncate)
NACC (length control)
Transformer (truncate)
Transformer (length control)
NACC (truncate)
NACC (length control)

Len

40.89
39.73
34.15
34.40
46.62
45.23
47.77
47.03

ROUGE F1
R-2
16.61
15.65
13.93
13.73
9.33
9.03
8.94
8.87

R-L
32.55
31.64
31.34
31.79
24.29
23.44
23.75
25.14

R-1
35.12
34.10
33.12
33.66
26.31
25.33
25.79
27.45

Time

0.093
0.095
0.011
0.017
0.092
0.095
0.012
0.025

Table 4: Comparing autoregressive (AR) and non-autoregressive (NAR) models on the Gigaword
headline generation test set. Our length-control algorithm requires the predicted probabilities to be
independent, and thus is not compatible with AR models.

Figure 2: Performance of NACC with differ-
ent bucket sizes.

Figure 3: Length-transfer performance of
NACC and Su et al. [34].

sample were blindly given to annotators in a random order; annotators then voted for the better
summary or a tie, regarding the overall quality and completeness/ﬂuency separately. We counted
the votes for each decoding method on the two evaluation criteria, and show the percentage of
wins/ties/loses in Table 3.

As seen, our length-control decoding has a dominant advantage over the truncating method in terms
of both the overall quality and completeness/ﬂuency. Further, this result is statistically signiﬁcant
based on a two-sided binomial test (ignoring ties), verifying that our length-control algorithm indeed
improves the quality and completeness of the predicted summaries.

Comparison with Autoregressive Models. We are curious about how our non-autoregressive NACC
is compared with autoregressive (AR) methods. Thus, we train a standard AR Transformer with
truncating and length-control decodings, and show results in Table 4.

As seen, our length-control algorithm is not compatible with the AR Transformer and hurts the
ROUGE scores (Rows 2 & 6). This is because our algorithm is based on dynamic programming and
requires model outputs to be local, so that the length-control problem can be divided into shared sub-
problems; however, the predicted probabilities of the AR Transformer depend on partial generation
at previous time steps. Note that this is not a disadvantage of our approach, but shows that NAR
generation provides unique opportunities for length control.

Admittedly, NACC has slightly lower ROUGE scores than AR Transformers in the supervised
setting (Rows 1–4), because human-written summaries are ﬂexible, causing difﬁculties for NAR
training. Nevertheless, our NACC achieves much higher inference efﬁciency, and outperforms all
previous NAR systems (recall Table 1).

In the unsupervised setting, our approach achieves higher performance than the AR Transformer,
which is a very strong result. This is because we learn from a search-based method that extracts source
words as the summary [33], and our CTC training—with blank tokens scattering over the whole
output sentence as appropriate—can capture such strong correspondence between input and output.
Moreover, the proposed length-control algorithm is able to maintain the summary completeness given
the length constraint, achieving better ROUGE scores than NACC with truncating.

8

13579Size0.0240.0260.0280.030Inference Time (s)ROUGETime141618202224Average ROUGE Score30405060708090Length Budget (chars)141618202224Average ROUGE ScoreSu et el. [30]NACCInput: singapore airline and delta air lines announced two differing strategies to upgrade their long-haul in-ﬂight
service for business travelers .
Reference: business travel : competing strategies ; crowded skies
Supervised Setting:
AR Transformer (no control): delta airlines upgrade service for business travel (50 characters)
NACC (no control):
NACC (length control):
Unsupervised Setting:
AR Transformer (no control): delta air lines differing strategies to upgrade their in-ﬂight service for business travelers
NACC (no control):
NACC (length control):

delta air lines differing strategies to upgrade long-haul in-ﬂight service for business travelers
delta air lines upgrade service business travelers

delta to upgrade long-haul in-ﬂight (36 characters)
delta air to upgrade long-haul in-ﬂight service (48 characters)

Table 5: Example summaries for Gigaword headline generation, where gray words are truncated for
fair comparison.

Analysis of the Length Bucket. Our dynamic programming is an approximate algorithm with a
α-sized length bucket (see Figure 1b and §3.2). Here, we investigate the effect of the bucket size in
terms of ROUGE scores (the arithmetic mean of R-1, R-2, and R-L) and inference efﬁciency in the
unsupervised setting where training targets are 8-word summaries from [33], following Table 1.

As seen in Figure 2, the ROUGE score continuously decreases with a larger bucket size (thick orange
curve). This not only conﬁrms the inexactness of our algorithm, but also shows that a small bucket
size does not hurt the performance much. On the other hand, the inference time decreases drastically
at the beginning (thin blue curve) because we have fewer dynamic programming steps; as the bucket
size increases, the inference time convergences to NACC without length control. Based on this
analysis, we set the bucket size to be 4 in our experiments.

Length-Transfer Generation. Our NACC model is capable of length-transfer generation, that is,
generating summaries of different lengths from the training targets. Such generation is important to
real-world applications where summaries of various lengths are needed for the same input, e.g., ﬁtting
different screen widths. Although generating a short enough summary may satisfy all possible length
constraints, a longer summary that better utilizes the length budget can preserve more information;
this is also reﬂected by ROUGE scores, which prefer longer summaries, as argued by [33].

Figure 3 compares the performance of NACC with Su et al. [34] when learning from 8-word
summaries in the unsupervised setting. When the inference length budget is less than training (x-axis
< 50), the ROUGE score of NACC decreases almost linearly with a decreasing length budget, but Su
et al. [34]’s approach degrades faster than ours. For x-axis > 50, we ﬁnd the soft penalty in [34] is
unable to generate longer summaries than trained (shown by the dashed orange line), whereas our
approach is able to utilize the increased length budget and achieve higher ROUGE scores.

Case Study. Table 5 shows example summaries generated by NACC and the AR Transformer on the
Gigaword test set.

As seen, a model without length control may generate a summary that happens to have the desired
length (AR Transformer in the supervised setting), a shorter summary (NACC in the supervised
setting), or a longer summary (both AR Transformer and NACC in the unsupervised setting). A
shorter summary fails to utilize the entire length budget, leading to less information conveyed,
whereas a longer summary requires truncating for explicit length control. Both cases are undesired.

By contrast, our proposed algorithm is able to generate a summary whose length is close to but less
than the length budget. The resulting summary is more complete than truncating, and better keeps the
key information.

5 Conclusion

In this work, we propose a Non-Autoregressive Summariation model with Character-level length
Control (NACC), which can explicitly control the number of characters in the predicted summaries.
Experiments show that our NACC approach not only achieves the state-of-the-art non-autoregressive
(NAR) performance on Gigaword and DUC2004 datasets under length constraints, but is several times
faster than autoregressive (AR) models and even outperforms an AR Transformer in the unsupervised

9

setting. Moreover, our approach is able to perform length-transfer generation, that is, generating
summaries of different lengths from the training set.

Limitation and Future Work. Our proposed length-control algorithm only works with non-
autoregressive models, which is a limitation (but not a disadvantage) of our work. We have clearly
stated this throughout the paper.

Our NACC approach—although largely outperforming previous NAR models and retaining high
inference efﬁciency—achieves lower ROUGE scores than AR models in the supervised setting.
This is the limitation of all NAR approaches in general, and can be addressed by future work. We
nevertheless outperform AR models in the unsupervised setting.

Acknowledgments

We thank all reviewers for their valuable comments. The research is supported in part by the Natural
Sciences and Engineering Research Council of Canada (NSERC) under grant No. RGPIN2020-04465,
the Amii Fellow Program, the Canada CIFAR AI Chair Program, a UAHJIC project, a donation from
DeepMind, and the Digital Research Alliance of Canada (alliancecan.ca).

References

[1] Rumen Andonov, Vincent Poirriez, and Sanjay Rajopadhye. Unbounded knapsack problem:
Dynamic programming revisited. European Journal of Operational Research, 123(2):394–407,
2000.

[2] Christos Baziotis, Ion Androutsopoulos, Ioannis Konstas, and Alexandros Potamianos. SEQ3:
Differentiable sequence-to-sequence-to-sequence autoencoder for unsupervised abstractive
sentence compression. In NAACL-HLT, pages 673–681, 2019.

[3] Ethan A. Chi, Julian Salazar, and Katrin Kirchhoff. Align-reﬁne: Non-autoregressive speech

recognition via iterative realignment. In NAACL-HLT, pages 1920–1927, 2021.

[4] Yuntian Deng and Alexander Rush. Cascaded text generation with Markov Transformers. In

NeurIPS, pages 170–181, 2020.

[5] Yue Dong, Yikang Shen, Eric Crawford, Herke van Hoof, and Jackie Chi Kit Cheung. Ban-
In EMNLP, pages 3739–3748,

ditSum: Extractive summarization as a contextual bandit.
2018.

[6] Bonnie Dorr, David Zajic, and Richard Schwartz. Hedge trimmer: A parse-and-trim approach
to headline generation. In Proc. NAACL-HLT Text Summarization Workshop, pages 1–8, 2003.

[7] J.-B. Durand, P. Goncalves, and Y. Guedon. Computational methods for hidden Markov tree
models-an application to wavelet trees. IEEE Transactions on Signal Processing, 52(9):2551–
2560, 2004.

[8] Katja Filippova, Enrique Alfonseca, Carlos A. Colmenares, Lukasz Kaiser, and Oriol Vinyals.

Sentence compression by deletion with LSTMs. In EMNLP, pages 360–368, 2015.

[9] Sebastian Gehrmann, Yuntian Deng, and Alexander M Rush. Bottom-up abstractive summariza-

tion. In EMNLP, pages 4098–4109, 2018.

[10] David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. English Gigaword. Linguistic Data

Consortium, 2003.

[11] Alex Graves, Santiago Fernández, Faustino Gomez, and Jürgen Schmidhuber. Connectionist
temporal classiﬁcation: Labelling unsegmented sequence data with recurrent neural networks.
In ICML, page 369–376, 2006.

[12] Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher. Non-

autoregressive neural machine translation. In ICLR, 2018.

[13] Chenyang Huang, Hao Zhou, Osmar R Zaïane, Lili Mou, and Lei Li. Non-autoregressive
translation with layer-wise prediction and deep supervision. In AAAI, pages 10776–10784,
2022.

[14] Frederick Jelinek. Statistical Methods for Speech Recognition. MIT Press, 1998.

10

[15] Mikael Kågebäck, Olof Mogren, Nina Tahmasebi, and Devdatt Dubhashi. Extractive summa-
rization using continuous vector space models. In Proc. Workshop on Continuous Vector Space
Models and their Compositionality, pages 31–39, 2014.

[16] Jason Lee, Elman Mansimov, and Kyunghyun Cho. Deterministic non-autoregressive neural

sequence modeling by iterative reﬁnement. In EMNLP, pages 1173–1182, 2018.

[17] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summariza-

tion Branches Out, pages 74–81, 2004.

[18] Puyuan Liu, Chenyang Huang, and Lili Mou. Learning non-autoregressive models from search

for unsupervised sentence summarization. In ACL, pages 7916–7929, 2022.

[19] Yixin Liu, Zi-Yi Dou, and Pengfei Liu. RefSum: Refactoring neural summarization. In NAACL,

pages 1437–1448, 2021.

[20] Yizhu Liu, Zhiyi Luo, and Kenny Zhu. Controlling length in abstractive summarization using a

convolutional neural network. In EMNLP, pages 4110–4119, 2018.

[21] Afonso Mendes, Shashi Narayan, Sebastião Miranda, Zita Marinho, André F. T. Martins, and
Shay B. Cohen. Jointly extracting and compressing documents with summary state representa-
tions. In NAACL-HLT, pages 3955–3966, 2019.

[22] Ning Miao, Hao Zhou, Lili Mou, Rui Yan, and Lei Li. CGMH: Constrained sentence generation

by Metropolis-Hastings sampling. In AAAI, pages 6834–6842, 2019.

[23] Rada Mihalcea and Paul Tarau. TextRank: Bringing order into text. In EMNLP, pages 404–411,

2004.

[24] Gabriel Murray, Steve Renals, and Jean Carletta. Extractive summarization of meeting record-
ings. In European Conference on Speech Communication and Technology, pages 593–596,
2005.

[25] Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, Caglar Gulcehre, and Bing Xiang. Ab-
stractive text summarization using sequence-to-sequence RNNs and beyond. In CoNLL, pages
280–290, 2016.

[26] Ani Nenkova, Kathleen McKeown, et al. Automatic summarization. Foundations and Trends®

in Information Retrieval, 5(2–3):103–233, 2011.

[27] Paul Over and James Yen. An introduction to DUC-2004: Intrinsic evaluation of generic news

text summarization systems. In Proc. Document Understanding Conference, 2004.

[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive

summarization. In ICLR, 2018.

[29] Weizhen Qi, Yeyun Gong, Jian Jiao, Yu Yan, Weizhu Chen, Dayiheng Liu, Kewen Tang,
Houqiang Li, Jiusheng Chen, Ruofei Zhang, Ming Zhou, and Nan Duan. Bang: Bridging
autoregressive and non-autoregressive generation with large scale pretraining. In ICML, pages
8630–8639, 2021.

[30] Alexander M. Rush, Sumit Chopra, and Jason Weston. A neural attention model for abstractive

sentence summarization. In EMNLP, pages 379–389, 2015.

[31] Chitwan Saharia, William Chan, Saurabh Saxena, and Mohammad Norouzi. Non-autoregressive

machine translation with latent alignments. In EMNLP, pages 1098–1108, 2020.

[32] Itsumi Saito, Kyosuke Nishida, Kosuke Nishida, Atsushi Otsuka, Hisako Asano, Junji Tomita,
Hiroyuki Shindo, and Yuji Matsumoto. Length-controllable abstractive summarization by
guiding with summary prototype. arXiv preprint arXiv:2001.07331, 2020.

[33] Raphael Schumann, Lili Mou, Yao Lu, Olga Vechtomova, and Katja Markert. Discrete opti-
mization for unsupervised sentence summarization with word-level extraction. In ACL, pages
5032–5042, 2020.

[34] Yixuan Su, Deng Cai, Yan Wang, David Vandyke, Simon Baker, Piji Li, and Nigel Collier.
Non-autoregressive text generation with pre-trained language models. In EACL, pages 234–243,
2021.

[35] Zhiqing Sun, Zhuohan Li, Haoqing Wang, Di He, Zi Lin, and Zhihong Deng. Fast structured

decoding for sequence models. In NeurIPS, pages 3011–3020, 2019.

11

[36] Sho Takase and Naoaki Okazaki. Positional encoding to control output sequence length. In

NAACL, pages 3999–4004, 2019.

[37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, pages 5998–6008,
2017.

[38] Yaushian Wang and Hung-Yi Lee. Learning to encode text as human-readable summaries using

generative adversarial networks. In EMNLP, pages 4187–4195, 2018.

[39] Peter West, Ari Holtzman, Jan Buys, and Yejin Choi. BottleSum: Unsupervised and self-
supervised sentence summarization using the information bottleneck principle. In EMNLP-
IJCNLP, pages 3752–3761, 2019.

[40] Jiacheng Xu and Greg Durrett. Neural extractive text summarization with syntactic compression.

In EMNLP-IJCNLP, pages 3292–3303, 2019.

[41] Kexin Yang, Wenqiang Lei, Dayiheng Liu, Weizhen Qi, and Jiancheng Lv. POS-constrained
parallel decoding for non-autoregressive generation. In ACL-IJCNLP, pages 5990–6000, 2021.
[42] Ziyi Yang, Chenguang Zhu, Robert Gmyr, Michael Zeng, Xuedong Huang, and Eric Darve.
Ted: A pretrained unsupervised summarization model with theme modeling and denoising. In
EMNLP, pages 1865–1874, 2020.

[43] Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. PEGASUS: Pre-training with

extracted gap-sentences for abstractive summarization. In ICML, pages 11328–11339, 2020.

[44] Jinming Zhao, Ming Liu, Longxiang Gao, Yuan Jin, Lan Du, He Zhao, He Zhang, and Gho-
lamreza Haffari. SummPip: Unsupervised multi-document summarization with sentence graph
compression. In SIGIR, pages 1949–1952, 2020.

Checklist

1. For all authors...

(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s

contributions and scope? [Yes]

(b) Did you describe the limitations of your work? [Yes] See Section 5.
(c) Did you discuss any potential negative societal impacts of your work? [N/A]
(d) Have you read the ethics review guidelines and ensured that your paper conforms to

them? [Yes]

2. If you are including theoretical results...

(a) Did you state the full set of assumptions of all theoretical results? [Yes] In Theorem 1.
(b) Did you include complete proofs of all theoretical results? [Yes] In Appendix A.

3. If you ran experiments...

(a) Did you include the code, data, and instructions needed to reproduce the main experi-
mental results (either in the supplemental material or as a URL)? [Yes] Footnote 1.
(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
were chosen)? [Yes] See Section 4.1 for the key setups and the codebase (Footnote 1)
for full details.

(c) Did you report error bars (e.g., with respect to the random seed after running experi-
ments multiple times)? [N/A] In our preliminary experiments, we found the results are
pretty robust.

(d) Did you include the total amount of compute and the type of resources used (e.g., type

of GPUs, internal cluster, or cloud provider)? [Yes] See Section 4.1.

4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
(a) If your work uses existing assets, did you cite the creators? [Yes] See Section 4.1.
(b) Did you mention the license of the assets? [N/A]
(c) Did you include any new assets either in the supplemental material or as a URL? [No]
(d) Did you discuss whether and how consent was obtained from people whose data you’re

using/curating? [N/A]

12

(e) Did you discuss whether the data you are using/curating contains personally identiﬁable

information or offensive content? [N/A]

5. If you used crowdsourcing or conducted research with human subjects...

(a) Did you include the full text of instructions given to participants and screenshots, if

applicable? [N/A]

(b) Did you describe any potential participant risks, with links to Institutional Review
Board (IRB) approvals, if applicable? [N/A] While we conducted a human evaluation
of machine learning systems, it was neither crowdsourced nor involving human subjects.
Instead, it was researchers studying machine learning systems’ outputs (i.e., subjects
are machine learning).

(c) Did you include the estimated hourly wage paid to participants and the total amount
spent on participant compensation? [N/A] Our human evaluation of machine learning
systems was done by in-lab research assistants; the research activities were part of their
job duties paid through regular salary.

13

A Proof of Theorem 1

Theorem 1. (1) If the bucket size α = 1 and consecutive repetitions are not merged, then dS,T is
the most probable sentence of T characters given by the S prediction slots. (2) If α (cid:54)= 1 or repeating
tokens are merged, our algorithm may not be exact.

Proof. [Part (1)] Our NACC is trained by the Connectionist Temporal Classiﬁcation (CTC) al-
gorithm [11], which merges repeated consecutive tokens and removes (cid:15)s in the output sequence.
Since the merging operation establishes dependencies between tokens in the output sequence, our
length-control algorithm is inexact.

In this part, we consider a variant of the CTC algorithm that does not merge repeated tokens but only
removes (cid:15)s; we denote this modiﬁed reduction operation by Γ(cid:48). For example, Γ(cid:48)(aa(cid:15)abb(cid:15)) = aaabb.
Our thus revised algorithm works as follows.
We denote (cid:101)ds,l = (cid:101)ds,l
that is reduced to a summary of length l.

s as the recursion variable, being the most probable s-token sequence

1 · · · (cid:101)ds,l

The initialization of (cid:101)ds,l is the same as the original length-control algorithm (§3.2), since the merging
operation is not involved here. However, the recursion involves only two cases:

• Case 1: ws = (cid:15). The recursion of this case is also the same (see Eqn. 4):
1 = (cid:8)
(cid:101)D s,l

(cid:101)ds−1,l ⊕ (cid:15)(cid:9)

• Case 2: ws (cid:54)= (cid:15). We have a set of candidate sequences:

(cid:26)

(cid:101)D s,l

2 =

(cid:101)ds−1,l(cid:48)

⊕ ws :

(cid:16)

u(ws) +

(cid:88)

(cid:17)

u(d)

= l, ws (cid:54)= (cid:15), and l(cid:48) < l

(cid:27)

d∈(cid:101)ds−1,l(cid:48)

(8)

(9)

This is analogous to Eqn. (6), where α = 1 (due to our theorem assumption). Also, the
condition ws (cid:54)= (cid:101)ds−1,l(cid:48)
in Eqn. (6) is dropped here because this algorithm variant does not
merge repeated tokens.

s−1

Then, the algorithm chooses the most probable candidate sequence as (cid:101)ds,l, given by

(cid:101)ds,l = argmax
1 ∪ (cid:101)D s,l

d∈ (cid:101)D s,l

2

S
(cid:88)

s=1

vs(ds)

(10)

Now we will prove that the algorithm is exact: suppose Ps,l := (cid:80)s
of (cid:101)ds,l, we have

i=1 vi((cid:101)ds,l

i ) is the log probability

Ps,l =

max
d1···ds:|Γ(cid:48)(d1···ds)|=l

vi(di)

(11)

s
(cid:88)

i=1

In other words, (cid:101)ds,l is the most probable s-token sequence that is reduced to length l. This is proved
by mathematical induction as follows.

Base Cases. For l = 0, the variable (cid:101)ds,0 can only be s-many (cid:15)s. The optimality in Eqn. (11) holds
trivially.

For s = 1 but l > 0, the algorithm chooses (cid:101)d1,l = argmax
d1:u(d1)=l

v1(d1). Therefore, P1,l =

max
d1:|Γ(cid:48)(d1)|=l

v1(d1), showing that Eqn. (11) is also satisﬁed with only one term in the summation.

Induction Step. The induction hypothesis assumes Ps−1,l(cid:48) =

max
d1···ds−1:|Γ(cid:48)(d1···ds−1)|=l(cid:48)

(cid:80)s−1

i=1 vi(di)

for every l(cid:48) < l. We will show that the algorithm ﬁnds the sequence (cid:101)ds,l with Ps,l =

max
d1···ds:|Γ(cid:48)(d1···ds)|=l

(cid:80)s

i=1 vi(di).

14

Word P1(·|x) P2(·|x)
0.3
0.4
0.2
0.1

0.1
0.6
0.05
0.25

I
am
a
(cid:15)

Table 6: A counterexample showing that our algorithm may be inexact if α (cid:54)= 1 or repeated tokens
are merged. Here, we set the vocabulary to be three words plus a blank token (cid:15).

According to Eqn. (10), the variable (cid:101)ds,l is the most probable sequence in (cid:101)D s,l

1 ∪ (cid:101)D s,l

2 . Thus, we have

Ps,l =

max
l(cid:48),ds:l(cid:48)+u(ds)=l
(cid:26)

{Ps−1,l(cid:48) + vs(ds)}

(cid:27)

= max

l(cid:48)

Ps−1,l(cid:48) +

max
ds:l(cid:48)+u(ds)=l

vs(ds)

max
d1···ds−1:|Γ(cid:48)(d1···ds−1)|=l(cid:48)

s−1
(cid:88)

vi(di) +

max
ds:l(cid:48)+u(ds)=l

vs(ds)

(cid:41)

= max

l(cid:48)

= max

l(cid:48)

(cid:40)





i=1


vi(di)


s
(cid:88)

i=1

max
d1···ds:
|Γ(cid:48)(d1···ds−1)|=l(cid:48)
|Γ(cid:48)(d1···ds)|=l
s
(cid:88)

=

max
d1···ds:|Γ(cid:48)(d1···ds)|=l

vi(di)

i=1

(12)

(13)

(14)

(15)

(16)

Here, (13) separates the max operation over l(cid:48) and ds; (14) is due to the induction hypothesis; (15)
holds because the two max terms in (14) are independent given l(cid:48), and thus the summations can be
grouped; and (16) further groups the two max operations with l(cid:48) eliminated. The last two lines are
originally proved in [14] and also used in [7].

[Part (2)] We now prove our algorithm may be inexact if α (cid:54)= 1 or repeated tokens are merged. We
show these by counterexamples.4

Suppose α (cid:54)= 1 and in particular we assume α = 2. We further assume repeated tokens are not
merged. Consider the example shown in Table 6. The length-control algorithm ﬁnds (cid:101)d1,1 = {“am”},
and then (cid:101)d2,2 = {“am I”} with the probability of 0.4 · 0.1 = 0.04, as the ﬁrst bucket covers the
length range [1, 2] and second [3, 4]. Here, we notice that two words are separated by a white space,
which also counts as a character). However, the optimum should be {“I am”}, which has a probability
of 0.3 · 0.6 = 0.18.

Now suppose repeated tokens are merged, and we further assume the length bucket α = 1 in this
counterexample. Again, this can be shown by Table 6: the algorithm ﬁnds d1,1 = {“I”} and
d1,2 = {“am”}, based on which we have d2,3 = {“I a”} with probability 0.3 · 0.05 = 0.015.
However, the optimum should be {“a I”} with probability 0.2 · 0.1 = 0.02.

The above theoretical analysis helps us understand when our algorithm is exact (or inexact). Empiri-
cally, our approach works well as an approximate inference algorithm.

4To make our counterexample intuitive, we work with probabilities, rather than log-probabilities.

15

