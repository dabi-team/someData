Safe Approximate Dynamic Programming Via
Kernelized Lipschitz Estimation

Ankush Chakrabarty1,:, Devesh K. Jha1, Gregery T. Buzzard2, Yebin Wang1, Kyriakos G. Vamvoudakis3

1

9
1
0
2

l
u
J

3

]

Y
S
.
s
s
e
e
[

1
v
1
5
1
2
0
.
7
0
9
1
:
v
i
X
r
a

Abstract—We develop a method for obtaining safe initial
policies for reinforcement learning via approximate dynamic
programming (ADP) techniques for uncertain systems evolving
with discrete-time dynamics. We employ kernelized Lipschitz
estimation and semideﬁnite programming for computing admis-
sible initial control policies with provably high probability. Such
admissible controllers enable safe initialization and constraint
enforcement while providing exponential stability of the equilib-
rium of the closed-loop system.

Index Terms—Semideﬁnite programming; Lipschitz constant
estimation; linear matrix inequalities; neural networks; policy
iteration; value iteration; kernel density estimation; approximate
dynamic programming; incremental quadratic constraints.

I. INTRODUCTION

Recent advances in the ﬁeld of deep and machine learning
has led to a renewed interest in using learning for control
of physical systems [1]. Reinforcement learning (RL) is a
learning framework that handles sequential decision-making
problems, wherein an ‘agent’ or decision maker learns a
policy to optimize a long-term reward by interacting with the
(unknown) environment. At each step, an RL agent obtains
evaluative feedback (called reward or cost) about the perfor-
mance of its action, allowing it to improve the performance
of subsequent actions [2], [3]. While RL has witnessed huge
success in recent times [4], [5], there are several unsolved
challenges which restricts use of these algorithms for industrial
systems. In most practical applications, control policies must
be designed to satisfy operational constraints. This leads to
the challenge that one has to guarantee constraint satisfaction
during learning and policy optimization. Therefore, initializing
with an unveriﬁed control policy is not ‘safe’ (in terms of
stability or constraint handling). In other words, using on-
line RL for expensive equipment or safety-critical applications
necessitates that the initial policy used for obtaining data for
subsequently improved policies must be at least stabilizing,
and generally, constraint-enforcing. The work presented in this
paper is motivated by this challenge. We present a framework
for deriving initial control policies from historical data that can
be veriﬁed to satisfy constraints and guarantee stability while
learning the optimal control policy on-line, from operational
data.

1Mitsubishi Electric Research Laboratories, Cambridge, MA, USA. Email:

{chakrabarty,devesh.jha,yebinwang}@merl.com

2Department of Mathematics, Purdue University, West Lafayette, IN, USA.

Email: buzzard@purdue.edu

3Daniel Guggenheim School of Aerospace Engineering, Georgia Institute

of Technology, Atlanta, GA, USA. Email: kyriakos@gatech.edu
:Corresponding author: A. Chakrabarty. Phone: +1 (617) 758-6175.

A successful RL method needs to balance a fundamental
trade-off between exploration and exploitation. One needs
to gather data safely (exploration) in order to best extract
information from this data for optimal decision-making (ex-
ploitation). One way to solve the exploration and exploitation
dilemma is to use optimistic initialization [6]–[9], but this
assumes the optimal policy is available until data is obtained
that proves otherwise. Such approaches have been applied
to robotics applications, where systems with discrete and
continuous state-action spaces [10], [11]. A limitation of these
methods is that, before the optimal policy is learned, the
agent is quite likely to explore actions that lead to violation
of the task-speciﬁc constraints as it aims to optimize the
cumulative reward for the task. This shortcoming signiﬁcantly
limits such methods to be applicable to industrial applications,
since this could lead to irreparable hardware damage or harm
human operators due to unexpected dynamics. Consequently,
safe learning focuses on learning while enforcing safety con-
straints. There are primarily two types of approaches to safe
RL and approximate/adaptive dynamic programming (ADP).
These include: modiﬁcation of the optimization criterion with
a safety component such as barrier functions by transforming
the operational constraints into soft constraints [12], [13]; and,
modifying the exploration process through the incorporation of
external system knowledge or historical data [14]. Our method
is amongst the latter class of methods, because our operational
constraints are hard constraints and softening them could lead
to intermittent failure modes.

High performance model-based control requires precise
model knowledge for controller design. However, it is well
known that for most applications, accurate model knowl-
edge is practically elusive due to the presence of unmodeled
dynamical interactions (e.g., friction, contacts, etc.). Recent
efforts tackle this issue by learning control policies from
operational (on-line) or archival data (off-line). Since the exact
structure of the nonlinearity may be unknown or not amenable
for analysis, researchers have proposed ‘indirect’ data-driven
controllers that employ non-parametric learning methods such
as Gaussian processes to construct models from operational
data [15], [16] to improve control policies on-line [17],
[18]. Conversely, ‘direct’ methods, such as those proposed
in [19]–[22], directly compute policies using a combination
of archival/legacy and operational input-output data without
constructing an intermediate model. For example, in [23], a
human expert was introduced into the control loop to conduct
initial experiments to ensure safety while generating archival
data. A common assumption in many of these approaches is
the availability of an initial control policy that is stabilizing

 
 
 
 
 
 
and robust to unmodeled dynamics. Designing such safe initial
control policies in a computationally tractable manner remains
an open challenge.

In this work, we present a formalism for synthesizing safe
initial policies for uncertain non-linear systems. We assume
the presence of historical/archival/legacy data, with which we
estimate Lipschitz constants for the unmodeled system dynam-
ics. The estimation of the Lipschitz constant is done via kernel
density estimation (KDE). The estimated Lipschitz constant is
used to design control policies via semideﬁnite programming
that can incorporate stability and constraint satisfaction while
searching for policies. We show that the proposed approach is
able to design feasible policies for different constrained tasks
for several systems while respecting all active constraints.
Our key insight is that information regarding the structure of
classes of unmodeled nonlinearities can be encapsulated using
only a few parameters, without knowing the exact form of the
nonlinearity. Therefore, it may not be necessary to model the
unknown component itself in order to compute a safe control
policy. For example,
the class of Lipschitz nonlinearities
(which constitute a large share of nonlinearities observed in
applications) can be described using only a few parameters:
the Lipschitz constants of the nonlinear components. Recent
work has investigated the utility of Lipschitz properties in
constructing controllers when an oracle is available [24] or in
designing models for prediction [25] with on-line data used for
controller reﬁnement [26] In this paper, we construct control
policies that respect constraints and certify stability (with high
probability) for applications where only off-line data is avail-
able, and no oracle is present. We do so through the systematic
use of multiplier matrices that enable the representation of
nonlinear dynamics through quadratic constraints [27], [28]
without requiring knowledge of the underlying nonlinearity.
The control policies can then be obtained by solving semidef-
inite programs. However, construction of multiplier matrices
for Lipschitz systems requires knowledge of the Lipschitz
constants, which are not always available, and therefore, must
be estimated. We refer to the estimation of Lipschitz constants
from data as Lipschitz learning. Historically, methods that
estimate the Lipschitz constant [29]–[31] do not provide
certiﬁcates on the quality of the estimate. Herein, we provide
conditions that, if satisﬁed, enable us to estimate the Lipschitz
constant of an unknown locally Lipschitz nonlinearity with
high probability. To this end, we employ kernel density estima-
tion (KDE), a non-parametric data-driven method that employs
kernels to approximate smooth probability density functions to
arbitrarily high accuracy. We refer to our proposed KDE-based
Lipschitz constant estimation algorithm as kernelized Lipschitz
learning.

Contributions: Compared to the existing literature on safe
learning, the contributions of the present paper are threefold.
First, we formulate an algorithm to construct stabilizing and
constraint satisfying policies for nonlinear systems without
knowing the exact form of the nonlinearity. Then we leverage a
kernelized Lipschitz learning mechanism to estimate Lipschitz
constants of the unmodeled dynamics with high probability;
and, ﬁnally we use a multiplier-matrix based controller design
based on Lipschitz learning from legacy data that forces

exponential stability on the closed-loop dynamics (with the
same probability as the kernelized Lipschitz learner).

2

Structure: The rest of the paper is structured as follows.
We present the formal motivation of our work in Section II.
Our kernelized Lipschitz learning algorithm is described in
Section III, and benchmarking of the proposed learner on
benchmark Lipschitz functions is performed. The utility of
Lipschitz learning in policy design via multiplier matrices is
elucidated in Section IV, and a numerical example demon-
strating the potential of our overall formalism is provided in
Section V. We provide concluding remarks and discuss future
directions in Section VI.

Notation: We denote by R the set of real numbers, R` as
the set of positive reals, and N as the set of natural numbers.
The measure-based distance between two measurable subsets
A and B of a metric space Rn equipped with the metric ρµ is
given by ρµpA, Bq “ µpA(cid:52)Bq, where µ is a measure on Rn
and A(cid:52)B is the symmetric difference pAzBq Y pBzAq. We
deﬁne a ball B(cid:15)pxq :“ ty : ρpx, yq ď (cid:15)u and the sum A ‘ (cid:15) :“
Ť
xPA B(cid:15)pxq. The complement of a set A is denoted by Ac.
The indicator function of the set A is denoted by 1A. A block
. For every v P Rn,
diagonal matrix is denoted by blkdiag
¨
?
vJv, where vJ is the transpose of v. The
we denote }v} “
sup-norm or 8-norm is deﬁned as }v}8 ﬁ suptPR }vptq}. We
denote by λminpP q and λmaxpP q as the smallest and largest
eigenvalue of a square, symmetric matrix P . The symbol ą pă
q indicates positive (negative) deﬁniteness and A ą B implies
A ´ B ą 0 for A, B of appropriate dimensions. Similarly, ľ
pĺq implies positive (negative) semi-deﬁniteness. The operator
norm is denoted }P } and is deﬁned as the maximum singular
value of P . For a symmetric matrix, we use the ‹ notation to
‹ c s. The symbol
imply symmetric terms, that is,
Pr denotes the probability measure.

” r a b

a b
bJ c

`

˘

“

‰

II. PROBLEM FORMULATION

A. Problem statement

Consider the following discrete-time nonlinear system,
xt`1 “ F pxt, utq, t P N

qt “ Cqxt,

where xt P Rnx , u “ ut P Rnu denote the state and the
control input of the system respectively.

For simplicity of exposition we will write

xt`1 “ Axt ` But ` Gφpqtq, t P N

qt “ Cqxt,

(1a)

(1b)

where the system matrices A, B, G and Cq have appropriate
dimensions. Denote by φ P Rnφ the system’s uncertainty, or
unmodeled nonlinearity, whose argument q “ qt P Rnq is
represented by a linear combination of the state. The origin is
an equilibrium state for (1); that is, φp0q “ 0.

The following assumptions and deﬁnition are now needed.

Assumption 1. The matrix B is known. The matrix G has
full column rank and is sparse and all entries are 0 or 1. Only
the non-zero element locations of G are known. The matrix
Cq and function φ are unknown.
l

We require the following deﬁnition to describe the class of

optimal control policy

nonlinearities considered in this paper.

Deﬁnition 1. A function f : X Ñ Rnx is Lipschitz continuous
in the domain X Ă Rnf if

}f px1q ´ f px2q} ď Lf }x1 ´ x2}

for some Lf ą 0 and all x1, x2 P X. We deﬁne the scalar

L˚

f “ inf
R`

tLf : condition (2) holdsu

(2)

(3)

as the Lipschitz constant of f in X. A function is globally
Lipschitz if (2) holds for X ” Rnf .
l

Assumption 2. The nonlinearity φ is globally Lipschitz con-
tinuous. That is,

}φpq1q ´ φpq2q} ď L˚

φ}q1 ´ q2}

(4)

for any q1, q2 P Rnq , and the global Lipschitz constant L˚
unknown.

φ is
l

Assumptions 1 and 2 imply that the linear component of
the true system (1) can be assumed, but the rest is unknown.
However, we do know the vector space through which the
nonlinearity enters the dynamics of (1), since the non-zero
locations of G are ﬂagged.

Remark 1. Assumption 1 is mild. For instance, one could
relax the assumption on G and take the unknown ˜G to be
the identity matrix. Then the nonlinearity would be ˜φpqq “
‰
“
J , i P I, with I the index set of non-
0
zero rows of G, so that ˜G ˜φ “ Gφ.
l

. . . φipqq

. . .

Given a control policy upxq, we deﬁne an inﬁnite horizon

cost functional given an initial state x0 P Rnx as

J px0, uq “

8ÿ

t“0

γt Upxt, upxtqq,

(5)

where U is a function with non-negative range, Up0, 0q “
0, and txku denotes the sequence of states generated by the
closed loop system

xt`1 “ Axt ` Bupxtq ` Gφ pCqxtq .

(6)

The scalar γ P p0, 1s is a forgetting/discount factor intended
to enable the cost to be emphasized more by current state and
control actions and lend less credence to the past.

Before formally stating our objective, we need to introduce

the following standard deﬁnition [1].

Deﬁnition 2. A continuous control policy up¨q : Rnx Ñ Rnu
is admissible on X Ă Rnx if it stabilizes the closed loop
system (6) on X and J px0, uq is ﬁnite for any x0 P X.

We want to design an optimal control policy that achieves

the optimal cost

`

˘
x0, u

J8px0q “ inf
uPU
for any x0 P Rnx . Here, U denotes the set of all admissible
control policies. In other words, we wish to compute an

(7)

J

,

3

(8)

`

˘
x0, u

.

J

u8 “ arg inf

uPU

Directly constructing such an optimal controller is very chal-
lenging for general nonlinear systems; this is further compli-
cated because the system (1) contains unmodeled/uncertain
dynamics. Therefore, we shall use adaptive/approximate dy-
namic programming (ADP): a class of iterative, data-driven
algorithms that generate a convergent sequence of control
policies whose limit is provably the optimal control policy
u8pxq.

Recall from [32], [33] that a necessary condition for con-
vergence of policy iteration methods (a sub-class of ADP)
is the availability of an initial admissible control policy
u0pxq, which is non-trivial to derive for systems with some
unmodeled dynamics. Therefore, our objective in this work
is to systematically derive an initial admissible control policy
using only partial model information via kernelized Lipschitz
learning and semideﬁnite programming. We also extend this
idea to handle the case when the control input is constrained.
In such cases, along with an admissible controller, we also
derive a domain of attraction of the controller within which the
control policy is guaranteed to satisfy input constraints and the
closed-loop system remains stable. We refer to the derivation
of admissible control policies with guaranteed stabilizability
and/or constraint enforcement as safe initialization for ADP: a
crucial property required for ADP algorithms to gain traction
in expensive industrial applications.

We invoke the assumption in [20], [21] regarding the
availability of legacy/archival/historical data generated by the
is, at design time,
system during prior experiments. That
we have a dataset D consisting of unique triples: state-
input pairs along with corresponding state update information.
Concretely, we have access to D “ txj, uj, x`
j“1. For each
txj, uj, x`
j u P D, we estimate the nonlinear term using (1);
that is,

j uN

`

˘

φpqjq “ G:

x`
j ´ Axj ´ Buj

,

where G: exists by Assumption 1. Note that we also need to
estimate the matrix Cq (see (1)) so that qj can be calculated
from xj. While estimating the exact elements of these matrices
is quite challenging, we can estimate the non-zero elements
in the matrices, which is enough to design safe initial control
policies, because the exact elements of Cq will be subsumed
within the Lipschitz constant.

Remark 2. The problem of estimating the sparsity pattern
of Cq is analogous to the problem of feature selection and
sparse learning, known as automatic relevance determination
(ARD) [34]. The basic idea in ARD is to give feature
weights some parametric prior densities; these densities are
subsequently reﬁned by maximizing the likelihood of the
data [34], [35]. For example, one can deﬁne hyperparameters
which explicitly represent the relevance of different inputs
to a machine learning algorithm w.r.t.
the desired output
(e.g., a regression problem). These relevance hyperparameters
determine the range of variation of parameters relating to a par-
ticular input. ARD can then determine these hyperparameters

during learning to discover which inputs are relevant.

l
We need the following assumption on the data tqj, φpqjqu,
without which one cannot attain the global Lipschitz constant
of the nonlinearity φp¨q with high accuracy.

Assumption 3. Let Q denote the convex hull of the samples
tqju. The Lipschitz constant of φp¨q in the domain Q is
identical to the global Lipschitz constant L˚
φ.
l

Assumption 3 ensures that the samples obtained from the
archival data are contained in a subregion of Rnq where the
nonlinearity φp¨q’s local Lipschitz constant is the same as its
global Lipschitz constant.

Example 1. Suppose φpqq “ 1.5 sinpqq. As long as the convex
hull of the samples tqu contain zero, the Lipschitz constant of
φ on the convex hull Q and on R are identical.
l

In the following section, we will leverage the dataset D to
estimate the Lipschitz constant of φp¨q using kernelized Lip-
schitz learning/estimation, and consequently design an initial
admissible linear control policy u0 “ K0x via semideﬁnite
programs. We will demonstrate how such an initial admissible
linear control policy ﬁts into a neural-network based ADP for-
mulation (such as policy iteration) to asymptotically generate
the optimal control policy u8pxq.
Remark 3. The control algorithm proposed in this paper is
a direct data-driven controller because no model of φp¨q is
identiﬁed in the controller design step.
l
Remark 4. Although we focus only on discrete-time systems,
our results hold for continuous-time systems with slight mod-
iﬁcations.
l
Remark 5.
If nφ ą 1, our proposed Lipschitz learning
algorithm will yield nφ Lipschitz constant estimates, one for
each dimension of φp¨q. To avoid notational complications, we
proceed (without loss of generality) with nφ “ 1. For larger
nφ, our algorithm can be used component-wise.
l

III. KERNELIZED LIPSCHITZ LEARNING

In this section, we provide a brief overview of kernel density
estimation (KDE) and provide a methodology for estimating
Lipschitz constants from data.

A. Empirical density of Lipschitz estimates

With the data tφpqjq, qjuN

mates of the global Lipschitz constant L˚

j“1, we obtain n P N underesti-
φ using

(cid:96)jk “

|φpqjq ´ φpqkq|
}qj ´ qk}

,

(9)

where k P t1, . . . , N uzj. The sequence t(cid:96)jku are empirical
samples drawn from an underlying univariate distribution L.
Clearly, the true distribution L has ﬁnite support; indeed, its
left-hand endpoint is a non-negative scalar (zero, if nq ą 1
but may be positive if nq “ 1) and its right-hand endpoint is
L˚
φ. This leads us to the key idea of our approach that is to
identify the support of the distribution L to yield an estimate
of the true Lipschitz constant of φp¨q.

4

Remark 6. Variants of the estimator (9) such as maxk (cid:96)jk have
been widely used in the literature to construct algorithms for
determining Lipschitz constants, see for example: [29], [30],
[36].
l
In the literature, common methods of tackling the support
estimation problem is by assuming prior knowledge about the
exact density of Lipschitz estimates [36] or using Strongin
overestimates of the Lipschitz constant [30]. However, we
avoid these overestimators because they are provably unreli-
able, even for globally Lipschitz functions [31, Theorem 3.1].
Instead, we try to ﬁt the density directly from local estimates
and the data in a non-parametric manner using KDE and
characteristics of the estimated density.

B. Plug-in support estimation

With a set of n underestimates t(cid:96)run

r“1, we generate an
estimate ˆLn of the true density L using a kernel density
estimator
nÿ

ˆ

˙

ˆLnp(cid:96)q “

1
nhn

K

(cid:96) ´ (cid:96)r
hn

,

(10)

r“1
where K : R Ñ R is a smooth function called the kernel
function and hn ą 0 is the kernel bandwidth. A plug-in
estimate of the support S of the true density L is

ˆSn :“ t(cid:96) P Rě0 : ˆLnp(cid:96)q ě βnu,

(11)

where βn is an element of a sequence tβnu that converges to
zero as n Ñ 8; this plug-in estimator was proposed in [37].

C. Implementation details

Implementing the plug-in estimator involves ﬁrst construct-
ing a KDE of L with n samples. Then, if one picks β ” βn
small enough, one can easily compute ˆS from (11). Then

ˆLφ :“ maxp ˆSnq.

(12)

This is a very straightforward operation with the avail-
like ksdensity (MATLAB) and the
ability of
KernelDensity tool in scikit-learn (Python). The
pseudocode is detailed herein in Algorithm 1.

tools

Algorithm 1 Kernelized Lipschitz Estimation
Require: Initial dataset, txk, φpCqxkquN
k“1
Require: Conﬁdence parameter, 0 ă β ! 1
1: tqk, φpqkqu Ð Estimate Cq via ARD
2: for k in 1, . . . , N do
3:
(cid:96) Ð append (cid:96)jk computed by (9)
4:
5: ˆLn Ð KDE with cross-validated K and h using t(cid:96)ru
6: ˆSn Ð compute using (11)
7: ˆLφ Ð maxp ˆSnq.

for j in t1, . . . , N uzk do

Remark 7. Note that
the true support S is a subset of
Rě0. Therefore, when computing the density estimate, this
information should be fed into the tool being used. For
example, in MATLAB, one has the option t’support’,
’positive’u. Essentially, this subroutine transforms the

data into the log-scale and estimates the log-density so that
upon returning to linear scale, one preserves positivity.

D. Theoretical guarantees

We formally describe the density L. We consider that the
samples q P Q are drawn according to some probability
distribution µ0 with support X. For any set S, suppose that
ş
µ0 can be written as µ0pSq “
S Ωpqq dµpqq, where µ is the
Lebesgue measure, and Ω is continuous and positive on X. Let
µX denote the product measure µ0 ˆ µ0 on X ˆ X. Since µ0 is
absolutely continuous with respect to the Lebesgue measure,
µX assigns zero mass on the diagonal tpq, qq : q P Xu. The
cumulative distribution function for L is then given by

ˆ"

˜Lpλq “ µX

pq1, q2q : q1 ‰ q2,

|φpq1q ´ φpq2q|
}q1 ´ q2}

*˙

ď λ

.

Since ˜L is non-decreasing, L exists almost everywhere by
Lebesgue’s theorem for differentiability of monotone func-
tions, and L’s support is contained within r0, L˚
φs because
of (9).

We investigate the worst-case sample complexity involved

in overestimating L˚

φ under the following mild assumption.

Assumption 4. The nonlinearity φp¨q is twice continuously
differentiable, that is, φp¨q P C2.
l

Lemma 1. Suppose that Assumptions 3 and 4 hold. Then there
exists some q˚ P Q such that }∇φpq˚q} “ L˚
φ.

1 , qk

2 q|{}qk

2 } Ñ L˚

Proof. Suppose tpqk
2 qu8
k“1 denotes a sequence of paired
samples in Q such that |φpqk
1 ´qk
1 q´φpqk
φ as k Ñ
8. Since Q is the convex hull of ﬁnitely many samples, it is
compact, so we can choose a subsequence of tqk
k“1 that
1 “ q8
converges to pq8
2 ,
1 q} ě L˚
then a Taylor expansion estimate implies }∇φpq8
φ.
Since L˚
φ is an upper bound of }∇φ} at any sample in Q,
}∇φpq8
1 . If q8
φ and q˚ “ q8
2 , then the result
follows by applying the mean value theorem to

2 u8
2 q where both limits are in Q. If q8

1 q} “ L˚

1 ‰ q8

1 , q8

1 , qk

`

2 ´ q8

1 ` tpq8

ϕptq “ φ pq8

1 qq ´ φpq8
1 q
for t P r0, 1s, for which ϕp0q “ 0 and ϕp1q “ L˚
1 ´ q8
φ}q8
2 }.
2 ´ q8
Also, dϕ{dt “
1 q. Since
}∇φ} ď L˚
1 }. Reordering
1 and q8
q8

2 ´ q8
φ, this implies |dϕ{dt| ď L˚
2 if needed, we have
`
1

pq8
2 ´q8

1 qq
φ}q8

1 ` tpq8

∇φpq8

ż

˘

˘

J

L˚

φ}q8

2 ´ q8

1 } “ ϕp1q “

ds ď L˚

φ}q8

2 ´ q8

1 }.

dϕ{dt

0

Hence, the rightmost inequality must be an equality, which
implies that dϕpsq{dt “ L˚
φ}q8
2 } for all s. That is, if
the Lipschitz constant is attained with q8
2 , then φp¨q
restricted to the segment connecting q8
2 is linear with
slope L˚

φ. This concludes the proof.

1 and q8

1 ‰ q8

1 ´ q8

Lemma 1 enables the worst-case complexity result described

in the following theorem.

Theorem 1. Let ϕ1pq1, q´1q “ |φpq1q ´ φpq´1q|{}q1 ´ q´1},
and suppose that Assumptions 3 and 4 hold. There exists C0 ą
0 such that for all sufﬁciently small δ ą 0 and and any set

5

j“1 of n uniform random samples in X, the probability

tqjun
that some pair q`, q´ P tqju gives the Lipschitz estimate
φ1pq`, q´q ě p1 ´ δqL˚

φ ´ C0δ

is at least 1 ´ (cid:15)pn, δq. Here (cid:15)pn, δq ď 3 expp´nκδ2nq´1q,
where κ is a constant depending on nq.

Proof. By Lemma 1, there exists at least one q˚ such that
}∇φpq‹q} “ L˚
φ. For the worst-case analysis, suppose this
occurs only at a single sample, q‹. A Taylor expansion at q‹
yields

φpq‹ ` qq “ φpq‹q ` ∇φpq‹qJq ` Rpqq,

(13)

where R is a remainder term with |Rpqq| ď CR}q}2 when
}q} ď η, for some CR ą 0 and η ą 0. Note that

∇φpq‹qJq “ }∇φpq‹q}}q} cos θ,

where θ is the angle between ∇φpq‹q and q. To obtain a good
estimate of }∇φpq‹q}, one needs to sample two points in this
neighborhood, one point q` with cos θ « 1 and a second
point q´ with cos θ « ´1. Each of these conditions deﬁnes a
cone. Regarding one of these cones in cylindrical coordinates
0 ď (cid:96) ď η and }y} ď χ(cid:96) for χ “ tan θ, we can integrate the
nq ´ 1 dimensional volume to get the volume of this cone as
C0χnq´1ηnq for some dimension-dependent constant C0. A
calculation shows that cos θ ě 1 ´ χ2{2 for small θ. With q`
and q´ as one sample from each cone, we have that q` ´ q´
is contained in the cone }y} ď χ(cid:96), and so we can use (13) to
approximate

|φpq‹ ` q`q ´ φpq‹ ` q´q|
“ |∇φpq‹qT pq` ´ q´q ` Rpq`q ´ Rpq´q|

ˆ

˙

ě }∇φpq‹q}}q` ´ q´}

1 ´

´ |Rpq`q ´ Rpq´q|.

χ2
2

Dividing by }q` ´ q´} and using the deﬁning property of q‹
gives

ˆ

˙

φ1pq`, q´q ě

1 ´

χ2
2

L˚

φ ´

|Rpq`q ´ Rpq´q|
}q` ´ q´}

.

(14)

` ` qK

` perpendicular and satisfying }qK

Subsequently, one can decompose q` “ q(cid:107)

`|. Identical arguments can be used to infer }qK
` ´ qK

`, with q(cid:107)
`
parallel to ∇φpq‹q and qK
`} ď
´} ď χ|q(cid:107)
χ|q(cid:107)
´|,
and hence, }q` ´ q´} ě }q}
´}. Since q`
and q´ are chosen from opposite cones, we have }q}
` ´ q}
´} “
}q}
´}. Using }qK} ď χ}q}} and cos θ ě 1 ´ χ2{2, we
˘
1 ´ χ2{2
have }q` ´ q´} ě p}q`} ` }q´}qp1 ´ χq
. Hence,

`} ` }q}

´} ´ }qK

` ´ q}

`

|Rpq`q ´ Rpq´q|
}q` ´ q´}

ď

ď

ď

CRp}q`}2 ` }q´}2q
p}q`} ` }q´}qp1 ´ χq p1 ´ χ2{2q
2CR maxt}q`}, }q´}u2
maxt}q`}, }q´}up1 ´ χq p1 ´ χ2{2q
2CRηp1 ` χ2q1{2
p1 ´ χq p1 ´ χ2{2q

.

By combining the aforementioned inequality with (14), one

obtains

´

ϕ1pq`, q´q ě

1 ´

¯

χ
2

L˚

φ ´

2CRηp1 ` χ2q1{2
p1 ´ χq p1 ´ χ2{2q

.

Set δ “ χ{2 and take η “ δ. Then there exists C0 ą 0 such
that for all sufﬁciently small δ,

ϕ1pq`, q´q ě p1 ´ δq L˚

φ ´ C0δ,

(15)

which implies that ϕ1 Ñ L˚

φ as δ Ñ 0.

From the assumption on uniformly drawn samples,

the

probability of sampling in one of the pχ, ηq cones is,

ż

0

η

κpχrqnq´1 dr “

κχnq´1ηnq
nq

for some κ ą 0 that depends on nq. Using δ “ η “ χ{2 and
absorbing the factors of 2 and 1{nq into κ yields κδ2nq´1.

Let X1˘ be the event of sampling at least one point in the
pχ, ηq cone as above, and let X0˘ be the event of sampling
nothing in the pχ, ηq cone. The probability of sampling at least
one of each of the points q` and q´ just described is,

1 ´ P pX0` X X0´q ´ P pX0` X X1´q ´ P pX1` X X0´q

ě 1 ´ p1 ´ 2κδ2nq´1qn ´ 2p1 ´ κδ2nq´1qn,

where the factor 2 before κ in the second term comes from
the fact that both cones are excluded and they are disjoint, and
the 2 before the third term comes by combining the ﬁnal two
terms in the ﬁrst expression.

By using the fact that for any (cid:15)1 P p0, 1q and n ą 0 the
inequality p1 ´ (cid:15)1qn ď expp´n(cid:15)1q holds, we can conclude
that,

1 ´ P pX0` X X0´q ´ P pX0` X X1´q ´ P pX1` X X0´q
ě 1 ´ expp´2nκδnq´1q ´ 2 expp´nκδnq´1q ě 1 ´ (cid:15),

for any given (cid:15) ą 0. The latter can be ensured by choosing n
large enough. This gives a lower bound on the probability of
obtaining (15) and hence, the desired result.

E. Benchmarking the Lipschitz estimator

Our Lipschitz estimator is tested on well-studied benchmark
examples studied previously in [25], [29]:
the benchmark
functions are described in Table I along with their domains and
true local Lipschitz constants. Note that all the functions are
not globally Lipschitz (e.g. φ2), not differentiable everywhere
in the special case of φ4, speciﬁcally
(e.g. φ1, φ4), and,
constructed to ensure that naive overestimation of L˚
φ using
Strongin methods provably fails [31]. To evaluate the proposed
Lipschitz estimator, we vary the number of data points N and
the conﬁdence parameter β. Over 100 runs, we report mean
˘ one standard deviation of the following quantities: the time
required by our learning algorithm, the estimated Lipschitz
constant ˆLφ, and the error ˆLφ ´ L˚
φ (which should be positive
when we overestimate L˚

φ).

The ﬁnal column of Table I reveals an important empirical
detail: all our estimates of L˚
φ are overestimates for β ď 0.01
and n ě 100. This is a critical advantage of our proposed
approach, because an overestimate will enable us to provide

TABLE I
KERNELIZED LIPSCHITZ LEARNING OF BENCHMARK FUNCTIONS

6

log10 β

Time [s]

ˆLφ (mean ˘ stdev)

OE;?

n

100
100
500
500

100
100
500
500

100
100
500
500

φ1 “ | cospπxq|, L˚
0.596 ˘ 0.127
0.610 ˘ 0.122
2.463 ˘ 0.432
2.438 ˘ 0.427
φ2 “ x ´ x3{3, L˚
0.455 ˘ 0.123
0.459 ˘ 0.114
1.656 ˘ 0.218
1.585 ˘ 0.208

0.556 ˘ 0.121
0.547 ˘ 0.124
1.826 ˘ 0.224
1.821 ˘ 0.221

´2
´4
´2
´4

´2
´4
´2
´4

´2
´4
´2
´4

φ3 “ sinpxq ` sinp2x{3q, L˚

(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)

φ “ 3.141 on r´π, πs
3.361 ˘ 0.093
3.528 ˘ 0.177
3.252 ˘ 0.083
3.369 ˘ 0.158
φ “ 1.000 on r´1, 1s
1.018 ˘ 0.011
1.030 ˘ 0.020
1.005 ˘ 0.004
1.010 ˘ 0.009
φ “ 1.667 on r3.1, 20.4s
1.780 ˘ 0.074
1.923 ˘ 0.166
1.684 ˘ 0.010
1.720 ˘ 0.002

(cid:88)
(cid:88)
(cid:88)
(cid:88)
φ “ 8.378 on r0, 1s
(cid:88)
(cid:88)
(cid:88)
(cid:88)

8.969 ˘ 0.262
9.401 ˘ 0.476
8.507 ˘ 0.046
8.707 ˘ 0.103

φ “ 3.0 on r´10, 10s

φ4 “ Hansen test function from [29], L˚

´2
´4
´2
´4

0.450 ˘ 0.117
0.488 ˘ 0.123
1.921 ˘ 0.138
1.923 ˘ 0.130

100
100
500
500
φ5 “ maxt1 ´ 3 sinpxq, expp´ sinpxqqu, L˚
100
100
500
500

0.546 ˘ 0.061
0.612 ˘ 0.066
1.893 ˘ 0.245
1.989 ˘ 0.230

´2
´4
´2
´4

3.139 ˘ 0.051
3.200 ˘ 0.087
3.043 ˘ 0.014
3.104 ˘ 0.024

(cid:88)
(cid:88)
(cid:88)
(cid:88)

; OE indicates an overestimate of the true Lipschitz constant, that is,
mintˆLφu ą L˚
φ .

stability and constraint satisfaction guarantees about the data-
driven controller, as we will discuss in subsequent sections.
Furthermore, the estimation error is small for every test run,
and as expected, the error increases as β decreases, because a
smaller value of β indicates the need for greater conﬁdence,
which results in more conservative estimates.

IV. SAFE INITIALIZATION IN ADP

In this section, we begin by reviewing a general ADP proce-
dure, and then explain how to safely initialize unconstrained,
as well as input-constrained ADP.

A. Unconstrained ADP

Recall the optimal value function given by (7) and the
optimal control policy (8). From the Bellman optimality princi-
ple, we know that the discrete-time Hamilton-Jacobi-Bellman
equations are given by

J8pxtq “ inf
uPU

pUpxt, upxtqq ` γJ8pxt`1qq ,

u8pxtq “ arg inf

uPU

pUpxt, upxtqq ` γJ8pxt`1qq ,

(16)

(17)

where J8pxtq is the optimal value function and u8pxtq
is the optimal control policy. The key operations in ADP
methods [32] involve setting an admissible control policy
u0pxq and then iterating the policy evaluation step

`

˘

Jk`1pxtq “ U

xt, ukpxtq

` γJk`1pxt`1q

(18a)

and the policy improvement step

uk`1pxtq “ arg min

U

up¨q

until convergence.

`

`

˘
xt, upxtq

˘
` γJk`1pxt`1q

(18b)

1) Semideﬁnite programming for safe initial control policy:

Recall the following deﬁnition.

Deﬁnition 3. The equilibrium point x “ 0 of the closed-
loop system (6) is globally exponentially stable with a decay
rate α if there exist scalars C0 ą 0 and α P p0, 1q such that
}xt} ď C0αpt´t0q}x0} for any x0 P Rnx .
l

Conditions for global exponential stability (GES) of the

equilibrium state, adopted from [38], is provided next.

Lemma 2. Let V p¨, ¨q : r0, 8q ˆ Rnx Ñ R be a continuously
differentiable function such that

γ1}x}2 ď V pt, xtq ď γ2}x}2

V pt ` 1, xt`1q ´ V pt, xtq ď ´p1 ´ α2qV pt, xtq,

(19a)

(19b)

for any t ě t0 and x P Rnx along the trajectories of the
system

x` “ ϕpxq,

(20)

where γ1, γ2, and α are positive scalars, and ϕp¨q is a
nonlinear function. Then the equilibrium state x “ 0 for the
system (20) is GES with decay rate α.
l

The following design theorem provides a method to con-
struct an initial linear stabilizing policy u0pxq “ K0x such
that the origin is a GES equilibrium state of the closed-loop
system (6).

Theorem 2. Suppose that Assumptions 1–2 hold, and that
there exist matrices P “ P J ą 0 P Rnxˆnx , K0 P Rnuˆnx ,
and scalars α P p0, 1q, ν ą 0 such that

Ψ ` ΓJMΓ ĺ 0,

(21)

where

Ψ “

Γ “

„

„

pA ` BK0qJP pA ` BK0q ´ α2P
GJP pA ` BK0q



„

Cq
0

0
I

, and M “

ν´1pL˚

φq2I

0



,

‹
GJP G

0
´ν´1I

.

Then the equilibrium x “ 0 of the closed-loop system (6) is
GES with decay rate α.

Proof. Let V “ xJP x. Then (19a) in Lemma 2 is satisﬁed
with γ1 “ λminpP q and γ2 “ λmaxpP q. Let ∆V “ V ` ´ V .
Note that

V ` “ px`qJP x`

“ ppA ` BK0qx ` GφqJ P ppA ` BK0qx ` Gφq
“ xJpA ` BK0qJP pA ` BK0qx

` 2xJpA ` BK0qJP Gφ ` φJGJP Gφ.

Therefore,
„
„


J

x
φ

Ψ

7



x
φ

“ xJpA ` BK0qJP pA ` BK0qx ´ α2xJP x

` 2xJpA ` BK0qJP Gφ ` φJGJP Gφ

“ V ` ´ α2V “ ∆V ` p1 ´ α2qV,

and

„

J

x
φ

ΓJMΓ

„



„

x
φ

“



J

q
φ

„



q
φ

M

`

“ ν

˘

.

pL˚

φq2qJq ´ φJφ
‰
“

x φ

J and its

Thus, pre- and post-multiplying (21) with
transpose, respectively, we get
`

∆V ` p1 ´ α2qV ` ν

pL˚

φq2qJq ´ φJφ

˘

ď 0.

By inequality (4) in Assumption 2 and recalling that φp0q “
φq2qJq ´ φJφ ě 0. Since ν ą 0, this implies
0, we get pL˚
∆V ` p1 ´ α2qV ď 0, which is identical to (19b).

Note that we do not need to know φp¨q to satisfy con-
ditions (21). Instead, Theorem 2 provides conditions that
leverage matrix multipliers similar to those described in [27].
We shall now provide LMI-based conditions for computing
the initial control policy K0, the initial domain of attraction
P and ν via convex programming.
Theorem 3. Fix α P p0, 1q and ˆLφ obtained via (12). If there
exist matrices S “ SJ ą 0, Y , and a scalar ν ą 0 such that
the LMI conditions

»

—
—
–

´α2S
0
AS ` BY
ˆLφCqS

ﬁ

‹
‹
´νI
‹
νGS ´S

‹
‹
‹

ﬃ
ﬃ
ﬂ ĺ 0

0

0 ´νI

(24)

are satisﬁed, then the matrices K0 “ Y S´1, P “ S´1 and
scalar ν satisfy the conditions (21) with the same α and ˆLφ.

`“

Proof. A congruence transformation of (24) with the matrix
P ν´1 I P I
and substituting S with P ´1
blkdiag
and Y with K0P ´1 yields

‰˘

»

—
—
–

´α2P
0
A ` BK0
ˆLφCq

‹
´ν´1I
G
0

‹
‹
´P

‹
‹
‹

0 ´νI

ﬁ

ﬃ
ﬃ
ﬂ ĺ 0.

Taking the Schur complement with the submatrices shown by
the guidelines in the above inequality, we get (22). Since ν ą
0, taking the Schur complement again yields (23) which can
be rewritten as

„



„



„



Ψ ´

0
I

ν´1I

0
I

J

`

Cq
0

pˆLφq2ν´1I

Cq
0

„



J

ĺ 0

which is exactly (21). Thus, the conditions (21) and (24) are
equivalent.

Empirically, we observe that our proposed kernelized Lip-
schitz learner typically provides overestimates of L˚
φ (see
Appendix). A beneﬁt of overestimating L˚
φ is that admissibility
of the control policy is ensured. This is demonstrated by the
following result.

8

(22)

(23)

(25)

„

´α2P
0

„



„

 „

 „

`

0
´ν´1I

pA ` BK0qJ C J
q
0

0
φν´1I
pA ` BK0qJP pA ` BK0q ´ α2P pA ` BK0qJP G
´ν´1I ` GJP G

GJP pA ` BK0q

GJ

P
0

ˆL2



pA ` BK0qJ C J
q
0
„

GJ



„

`

Cq
0

pˆLφq2ν´1I

Cq
0



J



J

ĺ 0

ĺ 0

Theorem 4. Let pP, K0, ν, αq be a feasible solution to the
conditions (21) with an overestimate of the Lipschitz constant
ˆLφ ą L˚
φ. Then pP, K0, ν, αq is also a feasible solution to the
conditions (21).
Proof. Let δL “ ˆLφ ´ L˚
φ. Since ˆLφ is an overestimator of
L˚
φ, δL ą 0. Since pP, K, ν, αq is a feasible solution to (21)
with ˆLφ, it satisﬁes
„
´ν´1pL˚
φ ` δLq2I
0

Ψ ` ΓJ

Γ ĺ 0,

0
I



which can be written as
„

Ψ ` ΓJMΓ ` ΓJ

l

´ν´1p2L˚

φδL ` δL2qI
0
jh



0
0

Γ ĺ 0.

n

:“δM

As ν ą 0, we infer that δM ĺ 0, hence ΓJδMΓ ĺ 0.
Therefore, Ψ`ΓJMΓ ĺ 0. Since the other conditions in (21)
are independent of L˚
φ, the other conditions are automatically
satisﬁed. This concludes the proof.

Theorem 4 indicates that if our learned ˆLφ is an overesti-
φ, and we use ˆLφ to obtain a safe stabilizing control
mate of L˚
policy, then this is also a safe stabilizing control policy for
the true system (1). Having a feasible solution to (21) with an
underestimator of L˚
φ is not sufﬁcient to guarantee a feasible
solution for the true Lipschitz constant, because δM may not
be negative semi-deﬁnite in that case. Of course, extremely
conservative overestimates of ˆLφ will result in conservative
control policies or result
in infeasibility. In our proposed
approach, we have observed that the conﬁdence parameter β
dictates the conservativeness of the overestimate; that is β Ñ 1
makes the estimate ˆLφ more conservative.

2) Safely initialized PI: We begin by proving the following

critical result.

Theorem 5. Let Upx, uq be deﬁned as in (5). If K0 is obtained
by solving (24) for ˆLφ ě L˚
φ, then the initial control policy
u0 “ K0x is an admissible control policy on Rnx .

Proof. Clearly, u0 is continuous, and (by Theorem 2 and 3) is
a stabilizing control policy for (1). It remains to show that the
cost induced by u0 is ﬁnite. Since u0 is stabilizing and ˆLφ ě
L˚
φ, we know that }xt} Ñ 0 as t Ñ 8, which implies u0 Ñ 0
and, by therefore, Upxt, utq Ñ 0 as t Ñ 8. Since Upxt, utq
converges to a ﬁnite limit, Upxt, utq is bounded for all t ě
ř
t1
0. Therefore, any partial sum
t“0 Upxt, utq is bounded and
monotonic; that is, J converges to a ﬁnite limit.

Admissibility of u0 for the speciﬁc linear quadratic regulator

(LQR) cost function follows directly from Theorem 5.

Corollary 1. Let

Upxt, utq “ xJ

t Qxt ` uJ

t Rut

for some matrices Q “ QJ ľ 0 and R “ RJ ą 0. Then the
initial control policy u0 “ K0x obtained by solving (24) is
an admissible control policy on Rnx .

Now that we know u0 “ K0x is an admissible control
policy, we are ready to proceed with the policy iteration
steps (18). Typically, an analytical form of Jk is not known a
priori, so we resort to a shallow neural approximator/truncated
basis expansion for ﬁtting this function, assuming Jk is
smooth for every k P N Y t8u. Concretely, we represent the
value function and cost functions as:

Jkpxq :“ ωJ

k ψpxq

(26)

where ψ0p¨q : Rnx Ñ Rn0 denotes the set of differentiable
basis functions (equivalently, hidden layer neuron activations)
and ω : Rn0 is the corresponding column vector of basis
coefﬁcients (equivalently, neural weights).

It is not always clear how to initialize the weights of the
neural approximators (26). Commonly, small random numbers
drawn from a uniform distribution are used [39], but there is
no safety guarantee associated with random initialization. We
propose initializing the weights as follows. Since our initial
Lyapunov function is quadratic, we include the quadratic terms
of the components of x to be in the basis ψpxq. Then we
can express the initial Lyapunov function xJP x obtained by
solving (24) with appropriate weights in the ψpxq, respectively,
setting all other weights to be zero. With the approximator
initialized as above, the policy evaluation step (18a) is replaced
by

`

˘
ψpxtq ´ γψpxt`1q

ωJ

k`1

“ U pxt, ukpxtqq ,

(27a)

from which one can solve for ωk`1 recursively via

`

ωk`1 “ ωk ´ ηkϕk

ωJ

k ϕk ´ U pxt, ukpxtqq

˘

,

where ηk is a learning rate parameter that is usually selected
to be an element from the sequence tηku Ñ 0 as k Ñ 8, and
ϕk “ ψpxtq ´ γψpxt`1q. Subsequently, the policy improve-
ment step (18b) is replaced by

uk`1 “ arg min

up¨q

`

U pxt, upxtqq ` γωJ

˘
k`1ψpxt`1q

.

This minimization problem is typically non-convex and there-
fore, challenging to solve to optimality. In some speciﬁc cases,
one of which is that the cost function is quadratic as described
in (25), the policy improvement step becomes considerably

simpler to execute, namely
γ
2

uk`1pxq “ ´

R´1BJ∇ψpxqJωk`1.

(27b)

This can be evaluated as R and B are known, and ψ is
differentiable and chosen by the user, so ∇ψ is computable.
Since we prove that u0 is an admissible control policy, we
can use arguments identical to [39, Theorem 3.2 and Theorem
4.1] to claim that if the optimal value function and the optimal
control policy are dense in the space of functions induced by
the basis function expansions (26), then the weights of the
neural approximator employed in the PI steps (27) converges
to the optimal weights; that is, the optimal value function J8
and the optimal control policy u8 are achieved asymptotically.
A pseudocode for implementation is provided next.

Algorithm 2 Safely Initialized PI for discrete-time systems
Require: Termination condition constant (cid:15)ac
Require: Historical data D
1: Estimate Lipschitz constant ˆLφ using Algorithm 1
Require: Compute stabilizing control gain K0 via SDP (24)
2: Fix admissible control policy u0pxq “ K0x
3: while }Jk ´ Jk´1} ě (cid:15)ac do
4:

Solve for the value Jkpxq using

Jk`1pxtq “ Upxt, ukpxtqq ` γJk`1pxt`1q.

5:

Update the control policy upk`1qpxq using

`

˘

uk`1pxtq “ arg min

Upxt, ukpxtqq ` γJk`1pxt`1q

.

up¨q

6:

k :“ k ` 1

B. Input-constrained ADP with safety

Herein, we tackle the case when the control input is to be
constrained, which is very common in practical applications.
We make the following assumption on the constraints.
Assumption 5. The control input u P U, where
(cid:32)
u P Rnu : ξJ

U “

(28)

(

i u ď 1

,

for i “ 1, . . . , nc, where nc is the number of input constraints,
and ξi P Rnu for every i.
l

Remark 8. The matrix inequality (28) deﬁnes a polytopic input
constraint set. Clearly, constraints of the form |u| ď ¯u can be
written as
ξi
ξi`1

1{¯u
¨ ¨ ¨
¨ ¨ ¨
¨ ¨ ¨ ´1{¯u ¨ ¨ ¨


1
1


0
0

u ď

u “

0
0

„

„



„

,

which is of the form (28).

l
Note that with any control policy u0 “ K0x, the constraint

set described in (28) is equivalent to the set
(
(cid:32)
x P Rnx : ξJ
i K0x ď 1

X “

,

(29)

for i “ 1, . . . , nc. Before we state the main design theorem,
we require the following result from [40, pp. 69].

is a subset of X if and only if

ξiK J

0 P ´1K0 ξJ

i ď 1

for i “ 1, . . . , nc.

9

(30b)

l

1) Constrained admissible initial control policy and invari-
ant set estimation: Since the control input is constrained,
we need to characterize an invariant set of the form EP
within which all control actions satisfy (28) and the following
stability certiﬁcate holds.

Deﬁnition 4. The equilibrium point x “ 0 of the closed-
loop system (6) is locally exponentially stable with a decay
rate α and a domain of attraction EP if there exist scalars
C0 ą 0 and α P p0, 1q such that }xt} ď C0αpt´t0q}x0} for
any x0 P EP .
l

A standard result for testing local exponential stability of

the equilibrium point adopted from [41] is provided next.
Lemma 4. Let V : r0, 8q ˆ EP Ñ R be a continuously
differentiable function such that the inequalities (19) hold for
any t ě t0 and x P EP along the trajectories of the system (6)
where γ1, γ2, and α are positive scalars. Then the equilibrium
x “ 0 for the system (6) is locally exponentially stable with a
decay rate α and a domain of attraction EP .
l

The following design theorem provides a method to con-
struct a stabilizing policy such that the origin is a locally
exponentially stable equilibrium of the closed-loop system
and constraint satisfaction is guaranteed within a prescribed
ellipsoid of attraction EP Ă X without knowing the nonlin-
earity φp¨q.
Theorem 6. Fix α P p0, 1q and ˆLφ. Suppose ˆLφ ě L˚
φ, and
there exist matrices S “ SJ ą 0, Y , and a scalar ν ą 0 such
that the LMI conditions (24) and


„
1
‹

ξJ
i Y
S

ľ 0

(31)

for every i “ 1, . . . , nc. Then, the equilibrium x “ 0 of the
closed-loop system (6) is locally exponentially stable with a
decay rate α and a domain of attraction EP deﬁned in (30a).
Furthermore, given that the initial state x0 P EP , then the
control actions ut satisfy the constraints (28) for all t ě 0.

Proof. From Theorem 2, we know that (19) holds. Taking
Schur complements of (31) yields (30b) which, by Lemma 3,
implies that EP Ă X and hence, the input constraints are
satisﬁed for all t ě 0 by the closed-loop system with policy
u “ K0x, because x0 P EP . Thus, all the conditions of
Lemma 4 are satisﬁed, which concludes the proof.

Remark 9. Note that the conditions (24) and (31) are LMIs in
S, Y , and ν for a ﬁxed ˆLφ. Therefore one can maximize the
volume of EP by solving a constrained convex program with
cost function ´ log |S| (the log-determinant of S) subject to
the constraints (24) and (31) while line searching for α. This
will reduce the conservativeness of the domain of attraction.

Lemma 3. The ellipsoid

EP “ tx P Rnx : xJP x ď 1u

2) Safely initialized input-constrained PI: By adopting
the work of [42]–[46] for input-constrained/actuator saturated

(30a)

ADP we choose a cost function of the form

ż

Upx, uq “ Qpxq ` 2

u

`

¯u tanh´1pυ{¯uq

˘J

R dυ,

(32)

0

where Qpxq : Rnx Ñ R is a positive deﬁnite function
satisfying Qp0q “ 0 and R ą 0.

We begin by demonstrating that the constrained policy is an

admissible policy on its domain of attraction.

10

Algorithm 3 Safely Initialized VI for discrete-time systems
Require: Termination condition constant (cid:15)ac
Require: Historical data D
1: Estimate Lipschitz constant ˆLφ using Algorithm 1
Require: Compute stabilizing control gain K0 via SDP (24)
2: Fix safe initial control policy u0pxq “ K0x
3: while }Jk ´ Jk´1} ě (cid:15)ac do
4:

Solve for the value Jkpxq using

Jk`1pxtq “ Upxt, ukpxtqq ` γJkpxt`1q.

Theorem 7. Let U be deﬁned as in (32). Then the initial
control policy u0 “ K0x obtained by solving (24) and (31) is
an admissible control policy on EP .

5:

Update the control policy upk`1qpxq using

uk`1pxtq “ arg min

up¨q

`
Upxt, ukpxtqq ` γJk`1pxt`1q

˘
.

Proof. By deﬁnition Qp0q “ 0. Also, the integrand in (32) is
zero when the upper limit is zero. Therefore, Up0, 0q “ 0. For
any x0 P EP , u0 is a stabilizing constrained control policy,
therefore, }xt} Ñ 0 and }ut} Ñ 0 as t Ñ 8. Hence, U Ñ 0
as t Ñ 8. The rest of the proof follows identically as in the
proof of Theorem 5.

Since the control policy is constrained, we can initialize
ADP safely using the neural approximator (26) as discussed
in the previous subsection. The policy evaluation step is given
by

˘

`

ωJ

k`1

ψpxtq ´ γψpxt`1q
`
ukpxtq

ż

“ Qpxtq ` 2

¯u tanh´1pυ{¯uq

˘J

R dυ

0
“ Qpxtq ` 2¯uuJR tanh´1pu{¯uq

»

` ¯u2 diagpRqJ

—
—
—
–

(33)

ﬁ

ﬃ
ﬃ
ﬃ
ﬂ ,

1{¯u2q
2{¯u2q

lnp1 ´ u2
lnp1 ´ u2
...
lnp1 ´ u2

nu {¯u2q

where u1, u2, u3,
¨ ¨ ¨ , unu are the individual components
of the vector u. Subsequently, the policy improvement step is
given by

ı

uk`1 “ ´¯u tanh

R´1BJ∇ψpxt`1qJωk`1

,

(34)

”

γ
2¯u

which satisﬁes the control constraints, since } tanhp¨q}8 ď 1.
Since the initial control policy is constrained and admissible,
one can use [43, Theorem 2] to prove convergence of the
value function and the control policy to the optimal using the
constrained policy iteration steps (33) and (34).

C. Remarks on on-policy VI and Q-learning

The value iteration algorithm (see Algorithm 3) does not
generally require an admissible control policy in order to
converge optimally using data. Although this is true in off-
is, when the updated control
policy implementations (that
policy is not used on-line),
in on-policy implementations,
a lack of stabilizing initial policies could result in unsafe
transient behavior unless the underlying system is open-loop
stable, leading to unsafe exploration during the initial data
collection phase.

Q-learning is a provably convergent direct optimal adaptive
control algorithm and model-free reinforcement learning tech-
nique [47]–[50]. Q-learning can be used to ﬁnd an optimal

6:

k :“ k ` 1

action-selection policy based on measurements of previous
state and action observations controlled using a sub-optimal
policy. In most of the existing work the reward/cost function
is manipulated to guarantee correction of the unsafe actions
in the learning phase. Our proposed method does not require
a corrective modiﬁcation of the reward/cost function on-line
for safety. Instead, historical data and solving SDPs based on
Lipschitz estimation is used to generate safe control policies
that enables safe data collection during on-policy Q-learning
implementation, because the states are guaranteed not
to
diverge with the initial policy (this divergence could happen
if the initial policy was unsafe).

V. NUMERICAL EXAMPLES

A. Nonlinear torsional pendulum

We demonstrate our proposed approach using the torsional

pendulum which is modeled by discretizing the system

9θ “ ω,

J 9ω “ u ´ M gl sin θ ´ fdω,

(35a)

(35b)

with mass M “ 0.333 Kg, length l “ 0.667 m, acceleration
due to gravity g “ 0.981 m/s2, friction factor fd “ 0.2,
and moment of inertia J “ 0.1975 Kg-m2. With Euler
discretization and a sampling time of τ “ 0.01 s, we get
a discrete-time model of the form (1) with
„
„



„





„



x “

θ
ω

, A “ I ` τ

1

0
0 ´fd

, B “ τ

0
1

, G “ τ

0
´1

.

We assume that the nonlinearity φ “ M gl sin θ{J is com-
pletely unknown; clearly φp¨q has a Lipschitz constant L˚
φ “
M gl{J “ 11.038, which is also unknown to us.

In the data collection phase, we initialize the system (35)
from ten different initial conditions in the space r´π, πs ˆ
r´2, 2s and collect data each 0.1 s, leading to a total dataset of
N “ 50 samples. Note that the initialization procedure of [43]
requires 400 data points, which is considerably more than ours,
and in that procedure, the original policy in the pre-training
phase is not guaranteed to be admissible. Automatic relevance
determination reveals that the nonlinearity only acts through
the second state, and the argument of the nonlinearity is q “ θ.
Proceeding as in Algorithm 1, we perform cross-validation
using an Epanechnikov kernel with bandwidth hn “ 0.05 and

11

Fig. 2.
Illustration of constrained-input value-iteration based ADP with safe
initialization. The top plot shows the variation of }x} over time. The middle
plot demonstrates that input constraints are satisﬁed for all t, and the bottom
plot demonstrates that the initial control policy was close to the optimal, but
learning was necessary to change the weights to the optimal values.

ř

sensitive to the Lipschitz estimate. However, based on the
xJQx ` uJRu
J subplot which shows the variation of
with time, there is a slight improvement of performance in
the β “ 0.1 (continuous red line) case compared to the
β “ 10´3 (dashed blue line) case since the Lipschitz estimate
in the former is closer to the true Lipschitz constant. As
expected, the cost incurred by the control policy ignoring the
nonlinearity (dotted black line) is by far the worst, since the
control actions required early on are of larger magnitude and
the tracking performance is severely compromised. Randomly
selecting weights also results in worse performance than our
proposed method, as the cost incurred is increased due to
oscillatory behaviour in the states and poor tracking in the
initial time frame. Summarily, this experiment demonstrates
the effectiveness of the proposed approach and its robustness
to Lipschitz estimate conservatism.

In Fig. 2, we demonstrate the on-policy value iteration algo-
rithm with safe initialization. All initial conditions converge to
the origin using our proposed approach. In constrast, randomly
initializing a policy and value as is typical in on-policy value
iteration results in the states initially diverging (not shown in
the plot) and poor performance before the rank condition is
reached for determining a least-squares solution to update the
neural weights.

Constrained scenario: We also test the scenario where the
control actions are constrained by |u| ď 1. In this case, we
use the cost functional deﬁned in (32) with ¯u “ 1, Q “ I2
and R “ 0.5. We begin by solving (24) and (31) with
ν “ 1 and α “ 0.95 to get P and K0, as in the previous
subsection. We also select the same basis functions (36). We
randomly initialize (using 20 random initial conditions) the
system (35) from within the domain of attraction of the initial
control policy, that is, from within the set txJP x ď 1u. We
know from Theorem 6 that this ensures that the initial control

Fig. 1. Comparison of states xt,
inputs ut, and cost function values
for unconstrained ADP with safe initialization for Lipschitz estimates with
increasing conﬁdence ˆLφpβ “ 0.1q “ 11.14 and ˆLφpβ “ 0.001q “ 12.29.
We also compare our work to an LQR controller that is known to work for
linear systems.

choose β “ 0.01. This yields the overestimate ˆLφ “ 11.511 ą
L˚
φ. Using this Lipschitz estimate, we solve (24) with α “ 0.95
and ν “ 1 for an initial value function xJP x and control
policy estimate K0x.

We construct a 2´11´1 value function neural approximator

with a set of polynomial basis functions
"

ψpx1, x2q “

x2
1
2
x3
1
3

,

,

x2
2
2
x3
2
3

, x1x2,

,

x2
1x2
2
2

,

x2
1x2
2
x4
1x4
2
4

,

x1x2
2
2

*

,

x4
1
4

,

x4
2
4

,

,

(36)

where x1, x2 denote the ﬁrst and second components of x,
respectively. Our initial weight vector is set to

“

ω0 “

2P11

2P22 P12 ` P21

0

¨ ¨ ¨

‰
0

J ,

where Pij is the pi, jqth element of P . We ﬁx the learning
rate at η “ 10´4 and the forgetting factor γ “ 0.95.

ř

Unconstrained scenario: We ﬁrst test the unconstrained
}Qx}1 ` uJRu, with
scenario, where the cost function is
Q “ I2 and R “ 0.5, and compare four initial policies
and value functions obtained via: (i) kernelized Lipschitz
learning with β “ 0.1; (ii) kernelized Lipschitz learning with
β “ 0.001; (iii) solving an algebraic Riccati equation and
ignoring the nonlinearity; and (iv) randomly initializing with
small weights from a normal distribution with small variance
and zero mean (which is by far the most common initializer).
The comparison study results are shown in Figure 1. We
observe that all of the methods (i)–(iv) listed above work, and
result in stabilizing control policies that result in the state of
the torsional pendulum to converge to its equilibrium. Inter-
estingly, both the Lipschitz constant estimates result in similar
trajectories implying that the SDPs (24) are not extremely

12

Fig. 3. Illustration of constrained-input policy-iteration based ADP with safe
initialization. The top plot shows the variation of }x} over time. The middle
plot demonstrates that input constraints are satisﬁed for all t, and the bottom
plot demonstrates that the initial control policy was close to the optimal, but
learning was necessary to change the weights to the optimal values.

policy will satisfy input constraints. Consequently, because the
policy improvement step is also guaranteeed to satisfy input
constraints and the initial policy is stabilizing, there are no
constraint violations, and the initialization is deemed safe.
The performance of the proposed algorithm is provided in
Figure 3. The convergence of }xt} to zero and the satisfaction
of input bounds are illustrated. Finally, we demonstrate the
convergence of the neural weights ωt, noting that learning
did occur, that is, the weights were not static throughout the
simulation (which would indicate that the initial policy was
optimal).

B. Large randomized linear system

In order to study the scalability of the proposed approach
on higher state-space dimensions, we randomly generate a 20-
state, 10-input linear system of the form xt`1 “ Axt ` But,
where B is known, and A is unknown. We randomly choose
A0 ‰ A to be a known matrix such that pA0, Bq is control-
lable. Clearly, the unmodeled component is φpxq “ pA´A0qx.
We assume we do not know G and Cq, so both are set to
identity matrices of appropriate dimensions. The initial dataset
is generated using a small random perturbation signal on the
unknown system and 500 data points are stored, from which
we compute a Lipschitz constant for φ as ˆLφ “ 0.43 (the true
L˚
φ “ 0.34) and a safe initial policy with β “ 0.001; this
is illustrated in Figure 4’s top left subplot; the shaded blue
area is the subgraph of ˆLn within the β-conﬁdent support.
The unknown system is simulated from 100 randomly selected
initial conditions in R20 following a normal distribution with
variance 2 and zero-mean. The cost function has the form (25)
with Q “ 5I20, R “ 2I10, and γ “ 0.95. In all cases, the
weights of the 20´210´10 neural approximator with learning
rate η “ 0.1 converges within a few seconds, and the system
stabilizes, in spite of A being unstable, as shown in Figure 4.

Fig. 4. Kernelized Lipschitz estimate, and evolution of states, inputs, and
weights for the unknown 20-state, 10-input linear system using on-policy
policy iteration.

VI. CONCLUSIONS

This work provides a methodology for constructing admis-
sible initial control policies for ADP methods using Lips-
chitz learning by using kernel density estimation and semi-
deﬁnite programming. Such admissible controllers enable safe
initialization, that is, with constraint satisfaction using only
historical data, which is necessary not only in policy iteration
methods, but also in value iteration and Q-learning for safely
obtaining initial data on-line for on-policy learning when the
underlying system is not open-loop stable. Simulations on a
discretized torsional pendulum model and a high-dimensional
linear system are provided to show the efﬁciency of our
approach. Future research efforts will focus on more general
costs and uncertain nonlinear safety constraints while ensuring
feasibility with a high probability in terms of regret.

ACKNOWLEDGMENTS

We would like to thank Drs. Mouhacine Benosman and
Piyush Grover at Mitsubishi Electric Research Laboratories,
Cambridge, MA, USA, for their time and helpful insights.
Kyriakos Vamvoudakis was supported in part by NSF under
grant Nos. CPS-1851588 and S&AS-1849198.

REFERENCES

[1] K. Vamvoudakis, P. Antsaklis, W. Dixon, J. Hespanha, F. Lewis,
H. Modares, and B. Kiumarsi, “Autonomy and machine intelligence in
complex systems: A tutorial,” in American Control Conference (ACC),
2015, July 2015, pp. 5062–5079.

[2] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction

(2nd Edition). MIT press Cambridge, 2018, vol. 1.

[3] D. Vrabie, K. G. Vamvoudakis, and F. L. Lewis, Optimal Adaptive
Control and Differential Games by Reinforcement Learning Principles,
ser. IET control engineering series.
Institution of Engineering and
Technology, 2013.

[4] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van
Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam,
M. Lanctot et al., “Mastering the game of go with deep neural networks
and tree search,” nature, vol. 529, no. 7587, p. 484, 2016.

[5] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang,
A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton et al., “Mastering
the game of go without human knowledge,” Nature, vol. 550, no. 7676,
p. 354, 2017.

[6] M. Gregor and J. Spalek, “The optimistic exploration value function,”
in 2015 IEEE 19th International Conference on Intelligent Engineering
Systems (INES), Sep. 2015, pp. 119–123.

[7] R. I. Brafman and M. Tennenholtz, “R-max-a general polynomial time
algorithm for near-optimal reinforcement learning,” Journal of Machine
Learning Research, vol. 3, no. Oct, pp. 213–231, 2002.

[8] P. Thomas, G. Theocharous, and M. Ghavamzadeh, “High conﬁdence
policy improvement,” in International Conference on Machine Learning,
2015, pp. 2380–2388.

[9] D. K. Jha, M. Zhu, Y. Wang, and A. Ray, “Data-driven anytime algo-
rithms for motion planning with safety guarantees,” in 2016 American
Control Conference (ACC).

IEEE, 2016, pp. 5716–5721.

[10] S. Levine, C. Finn, T. Darrell, and P. Abbeel, “End-to-end training of
deep visuomotor policies,” The Journal of Machine Learning Research,
vol. 17, no. 1, pp. 1334–1373, 2016.

[11] Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel, “Bench-
marking deep reinforcement learning for continuous control,” in Inter-
national Conference on Machine Learning, 2016, pp. 1329–1338.
[12] J. Achiam, D. Held, A. Tamar, and P. Abbeel, “Constrained policy
optimization,” in Proceedings of the 34th International Conference on
Machine Learning-Volume 70.

JMLR. org, 2017, pp. 22–31.

[13] Y. Chow, O. Nachum, E. Duenez-Guzman, and M. Ghavamzadeh, “A
lyapunov-based approach to safe reinforcement learning,” in Advances
in Neural Information Processing Systems, 2018, pp. 8092–8101.
[14] J. Garcıa and F. Fern´andez, “A comprehensive survey on safe reinforce-
ment learning,” Journal of Machine Learning Research, vol. 16, no. 1,
pp. 1437–1480, 2015.

[15] D. Romeres, M. Zorzi, R. Camoriano, and A. Chiuso, “Online semi-
parametric learning for inverse dynamics modeling,” in Proc. of the
IEEE Conf. Dec. and Ctrl, 2016, pp. 2945–2950.

[16] D. Romeres, D. K. Jha, A. D. Libera, W. Yerazunis, and D. Nikovski,
“Semiparametrical Gaussian processes learning of forward dynamical
models for navigating in a circular maze,” CoRR, vol. abs/1809.04993,
2018. [Online]. Available: http://arxiv.org/abs/1809.04993

[17] F. Berkenkamp, R. Moriconi, A. P. Schoellig, and A. Krause, “Safe
learning of regions of attraction for uncertain, nonlinear systems with
Gaussian processes,” Proc. of the IEEE Conf. Decision and Control, pp.
4661–4666, 2016.

[18] L. Hewing and M. N. Zeilinger, “Cautious model predictive control
using gaussian process regression,” CoRR, vol. abs/1705.10702, 2017.
[Online]. Available: http://arxiv.org/abs/1705.10702

[19] Y. Jiang, Y. Wang, S. A. Bortoff, and Z.-P. Jiang, “Optimal co-design of
nonlinear control systems based on a modiﬁed policy iteration method,”
IEEE Transactions on Neural Networks and Learning Systems, vol. 26,
no. 2, pp. 409–414, 2015.

[20] M. Tanaskovic, L. Fagiano, C. Novara, and M. Morari, “Data-driven
control of nonlinear systems: An on-line direct approach,” Automatica,
vol. 75, pp. 1–10, 2017.

[21] D. Piga, S. Formentin, and A. Bemporad, “Direct data-driven control of
constrained systems,” IEEE Transactions on Control Systems Technol-
ogy, vol. 26, no. 4, pp. 1422–1429, 2018.

[22] B. Kiumarsi, K. G. Vamvoudakis, H. Modares, and F. L. Lewis, “Optimal
and autonomous control using reinforcement learning: A survey,” IEEE
transactions on neural networks and learning systems, vol. 29, no. 6,
pp. 2042–2062, 2018.

[23] L. Fagiano and C. Novara, “Automatic crosswind ﬂight of tethered
wings for airborne wind energy: a direct data-driven approach,” IFAC
Proceedings Volumes, vol. 47, no. 3, pp. 4927–4932, 2014.

[24] A. Chakrabarty, V. C. Dinh, M. J. Corless, A. E. Rundell, S. H. Zak, G. T.
Buzzard et al., “Support vector machine informed explicit nonlinear
model predictive control using low-discrepancy sequences.” IEEE Trans.
Automat. Contr., vol. 62, no. 1, pp. 135–148, 2017.

[25] J.-P. Calliess, “Conservative decision-making and inference in uncer-
tain dynamical systems,” Ph.D. dissertation, PhD thesis, University of
Oxford, 2014.

[26] D. Limon, J. Calliess, and J. M. Maciejowski, “Learning-based nonlinear
model predictive control,” IFAC-PapersOnLine, vol. 50, no. 1, pp. 7769–
7776, 2017.

13

[27] A. Chakrabarty, M. J. Corless, G. T. Buzzard, S. H. ˙Zak, and A. E.
Rundell, “State and unknown input observers for nonlinear systems with
bounded exogenous inputs,” IEEE Transactions on Automatic Control,
vol. 62, no. 11, pp. 5497–5510, 2017.

[28] X. Xu, B. Ac¸ıkmes¸e, M. Corless, and H. Sartipizadeh, “Observer-based
output feedback control design for systems with incrementally conic
nonlinearities,” in Proc. of the American Control Conference (ACC),
2018, pp. 1364–1369.

[29] G. Wood and B. Zhang, “Estimation of the Lipschitz constant of a
function,” Journal of Global Optimization, vol. 8, no. 1, pp. 91–103,
1996.

[30] R. G. Strongin, “On the convergence of an algorithm for ﬁnding a global
extremum,” Engineering Cybernetics, vol. 11, pp. 549–555, 1973.
[31] P. Hansen, B. Jaumard, and S.-H. Lu, “On using estimates of Lipschitz
constants in global optimization,” Journal of Optimization Theory and
Applications, vol. 75, no. 1, pp. 195–200, 1992.

[32] F. L. Lewis, D. Vrabie, and K. G. Vamvoudakis, “Reinforcement
learning and feedback control: Using natural decision methods to design
optimal adaptive controllers,” IEEE Control Systems, vol. 32, no. 6, pp.
76–105, 2012.

[33] K. G. Vamvoudakis, H. Modares, B. Kiumarsi, and F. L. Lewis, “Game
theory-based control system algorithms with real-time reinforcement
learning: How to solve multiplayer games online,” IEEE Control Systems
Magazine, vol. 37, no. 1, pp. 33–52, Feb 2017.

[34] M. E. Tipping, “Sparse bayesian learning and the relevance vector
machine,” Journal of machine learning research, vol. 1, no. Jun, pp.
211–244, 2001.

[35] W. Chu and Z. Ghahramani, “Preference learning with gaussian pro-
cesses,” in Proceedings of the 22nd international conference on Machine
learning. ACM, 2005, pp. 137–144.

[36] J.-P. Calliess, “Lipschitz optimisation for Lipschitz interpolation,” in
American Control Conference (ACC), 2017, 2017, pp. 3141–3146.
[37] A. Cuevas and R. Fraiman, “A plug-in approach to support estimation,”

The Annals of Statistics, vol. 25, no. 6, pp. 2300–2312, 1997.

[38] H. K. Khalil, Nonlinear control. Pearson New York, 2015.
[39] D. Liu and Q. Wei, “Policy iteration adaptive dynamic programming
algorithm for discrete-time nonlinear systems,” IEEE Trans. on Neural
Networks and Learning Systems, vol. 25, no. 3, pp. 621–634, 2014.
[40] S. Boyd, L. El Ghaoui, E. Feron, and V. Balakrishnan, Linear matrix
inequalities in system and control theory. SIAM, 1994, vol. 15.
[41] V. C. Aitken and H. M. Schwartz, “On the exponential stability of
discrete-time systems with applications in observer design,” IEEE Trans-
actions on Automatic Control, vol. 39, no. 9, pp. 1959–1962, 1994.

[42] M. Abu-Khalaf and F. L. Lewis, “Nearly optimal control

laws for
nonlinear systems with saturating actuators using a neural network HJB
approach,” Automatica, vol. 41, no. 5, pp. 779–791, 2005.

[43] Q. Lin, Q. Wei, and B. Zhao, “Optimal control for discrete-time systems
with actuator saturation,” Optimal Control Applications and Methods,
vol. 38, no. 6, pp. 1071–1080, 2017.

[44] H. Zhang, Y. Luo, and D. Liu, “Neural-network-based near-optimal
control for a class of discrete-time afﬁne nonlinear systems with control
constraints,” IEEE Transactions on Neural Networks, vol. 20, no. 9, pp.
1490–1503, 2009.

[45] H. Modares, F. L. Lewis, and M.-B. Naghibi-Sistani, “Adaptive optimal
control of unknown constrained-input systems using policy iteration and
neural networks,” IEEE Transactions on Neural Networks and Learning
Systems, vol. 24, no. 10, pp. 1513–1525, 2013.

[46] K. G. Vamvoudakis, M. F. Miranda, and J. P. Hespanha, “Asymptoti-
cally stable adaptiveoptimal control algorithm with saturating actuators
and relaxed persistence of excitation,” IEEE Transactions on Neural
Networks and Learning Systems, vol. 27, no. 11, pp. 2386–2398, Nov
2016.

[47] C. J. C. H. Watkins and P. Dayan, “Q-learning,” Machine Learning,

vol. 8, no. 3, pp. 279–292, May 1992.

[48] J. N. Tsitsiklis, “Asynchronous

stochastic approximation and q-

learning,” Machine Learning, vol. 16, no. 3, pp. 185–202, Sep 1994.

[49] P. Mehta and S. Meyn, “Q-learning and Pontryagin’s minimum princi-
ple,” in Proc. of the 48th IEEE Conference on Decision and Control
(CDC).

IEEE, 2009, pp. 3598–3605.

[50] K. G. Vamvoudakis, “Q-learning for continuous-time linear systems:
A model-free inﬁnite horizon optimal control approach,” Systems &
Control Letters, vol. 100, pp. 14 – 20, 2017.

