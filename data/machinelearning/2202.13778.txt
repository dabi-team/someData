Preprint manuscript No.
(will be inserted by the editor)

Rule-based Evolutionary Bayesian Learning

Themistoklis Botsas,a Lachlan R. Mason,a,b Omar K. Matar,a,b Indranil Pana,b,c∗

aThe Alan Turing Institute
bImperial College London
cNewcastle University

Received: date / Accepted: date

2
2
0
2

b
e
F
8
2

]
L
M

.
t
a
t
s
[

1
v
8
7
7
3
1
.
2
0
2
2
:
v
i
X
r
a

Abstract In our previous work in Botsas et al. (2020), we
introduced the rule-based Bayesian Regression, a method-
ology that leverages two concepts: (i) Bayesian inference,
for the general framework and uncertainty quantiﬁcation
and (ii) rule-based systems for the incorporation of expert
knowledge and intuition. The resulting method creates a
penalty equivalent to a common Bayesian prior, but it also
includes information that typically would not be available
within a standard Bayesian context. In this work, we ex-
tend the aforementioned methodology with grammatical
evolution, a symbolic genetic programming technique that
we utilise for automating the rules’ derivation. Our moti-
vation is that grammatical evolution can potentially detect
patterns from the data with valuable information, equiva-
lent to that of expert knowledge. We illustrate the use of
the rule-based Evolutionary Bayesian learning technique
by applying it to synthetic as well as real data, and exam-
ine the results in terms of point predictions and associated
uncertainty.

Keywords Rule-based systems · Probabilistic program-
ming · Bayesian · Inference · Grammatical evolution

1 Introduction

The issues of inclusion of expert knowledge and opinion
into statistical contexts (O’Hagan, 2019), and interpretabil-
ity of generic machine learning models (Molnar, 2020),
have been widely studied. In our previous work Botsas
et al. (2020), we aimed to tackle them for regression prob-
lems by introducing the Rule-based Bayesian regression
methodology; ﬁrst, expert opinions were translated into a
rule base, i.e. simple IF-THEN statements, where knowl-
edge about associations between inputs and outputs as-
sumes a concrete form. Second, these rules where pack-
aged into a Bayesian context in a manner similar to a stan-
dard prior. Finally, machine learning or standard statistical
techniques were used as the main likelihood model. The

∗ Corresponding Author
E-mail: i.pan11@imperial.ac.uk

resulting methodology had the ability to introduce expert
knowledge into models, which was not typically possible
solely from standard Bayesian priors.

In this work, we aim to extend the context described
above by automating the ﬁrst step of the process, i.e. the
expert knowledge elicitation. In cases where expert knowl-
edge is limited or non-existent, the rule-based Bayesian
context can still be used by employing grammatical evolu-
tion (Ryan et al., 1998), a genetic programming technique
that uses the notion of a “grammar” in order to ﬁnd sim-
ple or more complex associations among the inputs and the
output variables. These associations not only provide use-
ful insights into the system, but they can also be used to
improve ﬁtting in a practical context, using the rule-based
Bayesian methodology.

The rest of the manuscript is organized as follows: In
Section 2 we present the main components of the general
rule-based Evolutionary Bayesian learning framework.
In Section 3, we validate and apply our methodology
using data from four applications: a simple linear model,
a one-dimensional advection simulator, and two real
datasets (one regarding carbon emissions, and another
focusing on the electrical output of a power plant). In
Section 4, we address the shortcomings and complications
of the methodology. Finally, in Section 5, we summarise
the main takeaways from our work and discuss possible
areas of focus for future research.

2 Methodology

The methodology comprises different algorithms and
techniques. The ﬁrst essential part is a rule-based system
that uses IF–THEN logic-based rules in order to quantify
practical knowledge; rule-based systems are described
in Section 2.1. In Section 2.2, we present Grammatical
Evolution, a genetic programming technique, and the main
innovation for the rule-based Bayesian learning method-
ology that we introduce in this paper. In Section 2.3, we
explain the different ways we pair the rule-based systems
(derived from Grammatical evolution) with a conventional

 
 
 
 
 
 
2

Botsas, Mason, Matar, Pan

Bayesian framework. Finally, these frameworks, along
with statistical or machine learning models, are used for
regression and classiﬁcation.

2.1 Rule-based systems

Rule-based systems are useful for introducing additional
information (usually derived from domain expertise) into a
model, on top of the general model structure and the data.
In that sense, they offer similar beneﬁts to Bayesian priors.
The issue of using the latter, especially in conjunction with
machine learning algorithms, is that Bayesian priors ad-
dress knowledge about the model’s parameters, while ex-
pert knowledge can usually refer to associations among in-
puts and outputs. Rule-based systems, either hand-crafted
or derived in some automated technique, on the other hand,
can easily describe and facilitate the inclusion of such in-
formation into a model.

Our rule-based deﬁnition includes systems that incor-
porate knowledge in the form of a rule base Rk, which can
be expressed as:

δkRk : if Ak

1 ⊕ Ak

2 ⊕ · · · ⊕ Ak

m then Ck

(1)

where δk is a dichotomous variable indicating the inclu-
sion of the kth rule in the system; Ak
i , i ∈ 1, 2, . . . , m, is
the value of the ith antecedent attribute (cause) in the kth
rule; l is the number of antecedent attributes used in the
kth rule; Ck is the consequent (effect) in the kth rule; and
⊕ ∈ {∨, ∧} represents the set of connectives (OR, AND
operations) in the rules.

For our methodology, we include a logical-operator-
based (AND, OR) combination of all the rules to give rise
to a composite rule base: i.e., βk = 1, ∀ k, and we use the
quantity:

Rcomp := R1 ⊕ R2 ⊕ · · · ⊕ Rn

This is a versatile framework that can address rules of dif-
ferent nature. In our context, for example, the antecedent
attributes can be functions of one input (e.g. an inequality
between an input and a summary statistic), or functions of
many inputs (e.g. an equality that compares two or more in-
puts). Similarly, the antecedent attribute can be a function
of the output (e.g. an inequality between the output and a
number) or even an equality that describes a full model be-
tween inputs and outputs. For more concrete examples see
Section 3.

2.2 Grammatical evolution

Grammatical evolution (Ryan et al., 1998) is a genetic
programming technique used for automatically generating
programs, i.e. sequences of instructions, based on some
syntax (a popular application being symbolic regression).
It is composed of three separate parts.

The ﬁrst is a user-speciﬁed grammar, associated with
the program’s syntax. This is the part that accounts for all

the possible symbolic results that can be derived through
combinations of expressions, operations and functions. It
allows the user to restrict the search space of all possi-
ble functions, and therefore injects some version of domain
knowledge. In general, a grammar consists of four compo-
nents: A non-terminal set N , a terminal set T , a start set S,
and a set of production rules P. In our context, the gram-
mar pre-deﬁnes the nature of the plausible rules (IF-THEN
statements) that the algorithm is allowed to produce and as-
sess. In practice, this refers to the nature of the antecedents
and consequents (including different ways of combining
expressions for more complex antecedent forms), as well
as the associations among them.

The second part is the cost or ﬁtness function. It refers
to the quality assessment of each proposed program. In
practice, it is a quantity that grammatical evolution is try-
ing to minimise in order to retrieve the best possible ex-
pression constrained by the grammar. For this work, the
choice of the cost function depends on the nature of the
rules, which we discuss more on the different applications
of the next section. In practice it can vary from something
trivial, like minimising the number of points that do not
abide with a rule, to more classic cost functions, such as
the least square error.

The ﬁnal part required for the full speciﬁcation of
grammatical evolution is an optimisation algorithm that
searches the space deﬁned by the grammar and attempts
to ﬁnd the program that minimises the cost function.
Given the symbolic nature of the problem and difﬁculty
with computing gradients for individual symbolic expres-
sions, this needs to be a population based meta-heuristic
algorithm, such as Evolutionary Strategy or Genetic
Algorithm. For this work, we used the evolutionary
strategy as described in Beyer and Schwefel (2002).
For the applications in Section 3 we use grammatical
evolution in order to derive a rule-base, which we then use
in combination with a Bayesian context, as described in
the next section.

2.3 Bayesian context

In a standard Bayesian context, the posterior density is pro-
vided by Bayes’ theorem:

p(β|x) =

p(x|β)p(β)
p(x)

,

(2)

where x are the data, β are the model parameters, p(x|β)
is the likelihood, and p(β) is the prior density. The likeli-
hood describes the data formation, while the prior deﬁnes
the distributional nature of the parameters before we take
data into consideration, and can potentially account for in-
corporation of expert knowledge. The marginal likelihood,
p(x), normalises the aforementioned density product in or-
der to make the posterior p(β|x) a proper density. The
latter is the main quantity of interest within a Bayesian
context and describes the updated knowledge about the
model parameters after the inclusion of both data and ex-
pert knowledge.

Rule-based regression

3

In some cases, calculating the marginal likelihood an-
alytically is difﬁcult or intractable, so, instead, we employ
specialised algorithms, such as Markov Chain Monte Carlo
(MCMC), that approximate the posterior density with the
help of the proportionality formula of Bayes’ theorem:

p(β|x) ∝ p(x|β)p(β).

(3)

As we described in Botsas et al. (2020), the incorpo-
ration of the rule-based systems into the Bayesian context
comes by modifying the prior. For the general case Equa-
tion 3 becomes:

p(β|x, r) ∝ p(x|β)p(β, r),

(4)

where r is a random variable associated with the rule-base.
The extended prior that is the joint distribution p(β, r)
combines the standard knowledge and distributional form
associated with the model parameters and the expert infor-
mation derived from the rule-base. In practice, this yields a
framework similar to a conventional Bayesian context with
β treated as hyperparameters of r, and, thus, the joint dis-
tribution can be computed, by employing the chain rule:

p(β, r) = p(r|β)p(β),

(5)

and substituting Equation (5) into Equation (4), which
yields:

p(β|x, r) ∝ p(x|β)p(r|β)p(β),

(6)

or, in case we include hyperparameters η, that account for
the structure of the rules r :

p(β|x, r, η) ∝ p(x|β)p(r|β, η)p(β)p(η).

(7)

The term p(r|β) (or, equivalently, p(r|β, η)) is very
general, and can take many different shapes and forms.
For the purposes of this work, we will examine two main
possibilities of useful structures and distributional forms,
associated with this quantity. The ﬁrst is the one primar-
ily used in Botsas et al. (2020): we start by pre-deﬁning
discretisations of rule-input values, based on the rule-base
antecedents. For each proposed set of parameters β during
the MCMC we compute the number of rule-input values,
for which the corresponding outputs violate the respective
consequents, and we divide it by the number of all rule-
input values. The random variable of this ratio corresponds
to r|β. We assign a probability density for this random
variable. We use a Beta distribution with parameters a and
b (Beta(a, b)) for two reasons. First, r|β can take values
within the interval [0, 1] (with 0 corresponding to no rule-
input values violating the respective rule and 1 to all values
violating it). Second, it is very intuitive to incorporate con-
ﬁdence in the rule-base, by adjusting the parameters of the
beta distribution, i.e. r|β ∼ Beta(1, 100) corresponds to
a strict rule, or ‘strong conﬁdence’ in the rule-base, r|β ∼
Beta(1, 5) to a non-strict rule, and r|β ∼ Beta(1, 1) is
equivalent to the non-rule-based approach.

The method described above can yield rules of the

form:

R1 : if x ≤ xr,
R2 : if x > xr,

then y ≥ yr,
then y < yr.

Two other variations are introduced here. The ﬁrst is
similar to the one described above and depicts the same
form of rules, with the main difference being that instead of
the proportion of rule-input values that violate the rule, we
calculate their total distance (sum of individual distances)
from the rule-boundary. In this case, r|β ∼ Exp(λ), where
the choice of the rate parameter λ in the Exponential distri-
bution is related to the rule-base conﬁdence: Large values
of λ correspond to very strict rules and lower values to less
strict ones.

The intuition behind the ﬁnal variation is to consider a
penalty by constructing a segmented regressor based on a
rule-base, which is independent of the main model used in
the likelihood. Initially, we split the data based on the rule
breakpoint (i.e. the boundary between the segments) and
we compute the rule-output values y(cid:48) for all inputs within
each segment. To clarify this, we illustrate the form of rules
that this method accommodates:

R1 : if x ≤ xr,
R2 : if x > xr,

then y(cid:48) = A1x + K1,
then y(cid:48) = A2x + K2.

For each MCMC sample, β is approximated ignoring the
r dependence. Then r|β is calculated based on the β
from the previous step. This procedure repeats for every
MCMC iteration, essentially rendering the r|β equivalent
to an Empirical Bayes prior.

In practice the vector of rule-outputs y(cid:48) takes the role

of r and Equation (6) becomes:

p(β|x, y(cid:48)) ∝ p(x|β)p(y(cid:48)|β)p(β),

where

y(cid:48)|β ∼ N (f (x, β∗), σr).

(8)

(9)

For the Equation above, f is a function of the data and
β∗. The latter is a point estimate representing the β distri-
bution’s peak (in a typical Empirical Bayes fashion). The
variance σr is a pre-deﬁned constant associated with the
conﬁdence in the rule-base, i.e. a larger variance corre-
sponds to non-strict rules and a smaller variance to strict
rules. In Figure 1 we indicate some prospective priors for
the three variations.

3 Applications

We now illustrate the use of the methodology by apply-
ing it to two synthetic and two real world applications. The
ﬁrst one involves a simple synthetic sub-sample of linear
data where we attempt to retrieve the original linear re-
lationship. The second comprises data from a simulator
of a one-dimensional advection equation (Bar-Sinai et al.,
2019) where we ﬁt a B-splines model. For the third, we use

4

Botsas, Mason, Matar, Pan

Fig. 1 Prospective priors for the three variations of the rule distributions. The ﬁrst row corresponds to the proportion rules (Beta distribution),
the second to the total distance (Exponential distribution) and the last one to the piece-wise regression rules (Normal distribution). The ﬁrst
column corresponds to very strict rules, the second to non-strict rules, and the priors in the last column make the rule-based variations equivalent
to the standard, non-rule cases. Further explanation of the priors is described in Section 2.3.

a multivariate linear regression model in order to ﬁt data
that involve CO emissions from gas turbines (Kaya et al.,
2019). Finally, for the fourth application we use a multi-
variate logistic regression model to perform classiﬁcation
of the full load electrical power output of a combined cycle
power plant (T¨ufekci, 2014).

The results are produced with a two-step process;
for the ﬁrst step, we use the gramEvol package from R
(Noorian et al., 2016) in order to perform the Grammatical
evolution optimisation, while, for the second step, the
rule-based Bayesian context in produced using the PyMC3
Python package (Salvatier et al., 2016). The source code
has been made available online.1

Fig. 2 Linear regression data.

3.1 Linear regression

In Section 3.1.1 we illustrate how the synthetic data were
produced from a simple linear model. Section 3.1.2 is re-
served for the standard Bayesian linear regression analysis.
In Section 3.1.3 we describe how grammatical evolution is
used in order to derive a rule-base. Finally, in Section 3.1.4
we present the analysis for the Bayesian linear regression,
which incorporates the rules derived from the previous sec-
tion.

For

the Bayesian analyses, we use a Metropo-
lis–Hastings MCMC (Hastings, 1970), with 2 chains
of 120 000 iterations each, from which the ﬁrst 20 000

1 https://github.com/themisbo/
Rule-based-Evol-Bayesian-learn

are burn-in. For the posterior plots we use thinning of
100. In total, 4000 posterior samples are used for the
results. For both the intercept and slope priors, we use
the same Gaussian distribution α, β ∼ N (0, 102), and
for the likelihood variance an Exponential distribution
σ ∼ Exp(1).

3.1.1 Data

We produce synthetic linear data, from which we use a
small mid-portion as the training set. As we show in the
next section this adds a signiﬁcant amount of uncertainty
to the system, and makes recovering the original linear re-
lationship much harder. The goal is to use the grammati-
cal evolution in order to extract the appropriate pattern un-

Rule-based regression

5

Table 1 Posterior means µ and standard deviations σ for the parame-
ters of the true values (True), Bayesian linear regression (BLR), pro-
portion rule-based Bayesian linear regression (Prb-BLR), and total
distance rule-based Bayesian linear regression (TDrb-BLR).

True

BLR

Metric

µ

α
β

1.00
2.00

σ

1.73
-

ˆµ

−2.2
2.53

ˆσ

5.74
1.3

Prb-BLR

TDrb-BLR

Metric

ˆµ

ˆσ

ˆµ

α
β

0.3
1.96

3.44 −1.14
2.29
077

ˆσ

3.76
0.85

Table 2 Evaluation metrics for the different linear models: The one
without rules (No rules), the one with the proportion rules (Pr. rules)
and the one with the total distance rules (T.D. rules).

Metric/Model
MSE
MAE
WAIC

No rules
48.97
5.71
249.71

Pr. rules
33.75
5.09
249.26

T.D. rules
41.85
5.37
249.19

Fig. 3 Samples from the posterior predictive distribution for
Bayesian Linear Regression.

prompted, and then use it as additional information through
the rule-based Bayesian framework.

We sample 500 random predictor-values within the
interval [0, 10], and we produce the corresponding labels
from the true regression line y = 1 + 2x + (cid:15), where
(cid:15) ∼ N (0, 32). From those points, we use as training only
those within the sub-interval [4, 5] which leaves 49 points
for the ﬁnal analysis. The outcome is shown in Figure 2.

3.1.2 Bayesian linear regression (BLR)

The results for the Bayesian linear regression model are
shown in Figure 3. Even though the mean posterior regres-
sion line is very close to the true regression line, due to
the limited information of the data the uncertainty ranges
in the left and right section of the ﬁgure (denoted by the
red lines) are signiﬁcantly large. The summary statistics
for the MAP (Maximum A Posteriori estimator) parame-
ters are shown in Table 1, while corresponding metrics are
included in Table 2.

Table 3 The grammar used for the production of the linear rules.

N = {expr, comp, xv, yv}
T = {x, y, >, <, ≤, ≥, 4.1, . . . , 11.0}
S =< expr >

R = Production rules:
< expr > :== (cid:80) (comp(x, xv) (cid:54)= (comp(y, yv))
< comp > :==> | < | ≤ | ≥
< xv > :== 4.1|4.15| . . . |4.9
< yv > :== 7.0|7.05| . . . |11.0.

3.1.3 Rules derivation (proportion)

In Botsas et al. (2020) we examined how expert knowl-
edge and intuition can be directly translated into a rule-
base, and then incorporated into the rule-based Bayesian
context. Here on the other hand, we aim to show how the
method can still be used, even without information from an
expert. For this, we employ the use of grammatical evolu-
tion. As explained in Section 2.2, we require three compo-
nents in order to perform grammatical evolution optimisa-
tion. We examine each of them individually.

We start with the user-deﬁned grammar and its compo-
nents in Table 3. In practice, our goal is to generate differ-
ent expressions expr, which count the points at opposite
quadrants. The goal, then, is to minimise expr. For this
we require the second component of grammatical evolu-
tion which is the cost function. Here we use the function
f (x) = x, which corresponds to direct minimisation of the
aforementioned expression.

As mentioned in Section 2.2, for the optimisation al-
gorithm required we use the evolution strategy from Beyer
and Schwefel (2002). In addition, for all the examples in
this work we use the default parameters from the gramEvol
package (Noorian et al., 2016): population size of 8, 25%
probability of randomly generated individuals in each gen-
eration, mutation chance of 10/(1 + population size) and
10, 000 iterations.

The ﬁnal rules produced are:

R1 : if x < 4.8,
R2 : if x ≥ 4.8,

then

then

y ≤ 10.65,

y > 10.65.

(10)

and the composite rule base (Rcomp) is given by

Rcomp := R1 ∧ R2.

The result is shown graphically in Figure 4. The values
of xv and yv chosen by the optimisation algorithm corre-
spond to the thick red vertical and horizontal lines. Given
the cost function, the evolution strategy seeks to minimise
the points that are not within the red shaded area.

3.1.4 Rule-based Bayesian linear regression (proportion)

from the previous

rule-based Bayesian regression,

section for
We use the rules
the
and also set
r|β ∼ Beta(1, 100), which indicates a high level of
our conﬁdence in the rules. For the effect of different Beta
priors, and therefore different levels of conﬁdence, as well

6

Botsas, Mason, Matar, Pan

Fig. 4 Depiction of the rule-base formulated by the Expressions 10
chosen by grammatical evolution.

Fig. 6 Depiction of the rule-base formulated by the Expressions 12
chosen by grammatical evolution.

uncertainty reduction. Table 2 presents the mean square er-
ror (MSE) and mean absolute error (MAE) for the MAP of
the two methods, as well as the Watanabe–Akaike informa-
tion criterion (WAIC) for the whole chain. We see that the
rule-based variation surpasses its conventional counterpart
in all metrics.

3.1.5 Rules derivation (total distance)

We now examine the same data set for rules based on the
total distance, rather than the proportion of the data that
violate the rules, as explained in Section 2.3. The grammar
is similar to the one from Section 3.1.3, with the exception
of < expr > which becomes:

< expr > :==

if else(comp(x, xv)! = comp(y, yv), yv, y).

(11)

Given the statement above, < expr > takes the value of
yv for the points where the rule is violated, and the actual
point value y otherwise.

The cost function takes the form of the residuals sums
of squares (RSS): f (x) = (cid:80)n
i=1 (yi − expri)2. Therefore,
for the points where the rule is violated, their distance from
the rule-boundary is added, and for those that the rule is not
violated there is zero increment.
The rules produced are:

R1 : if x < 4.85,
R2 : if x ≥ 4.85,

then y ≤ 11,

then y > 11,

(12)

which are very similar to the corresponding rules from Sec-
tion 3.1.3. This is also conﬁrmed from Figure 6.

3.1.6 Rule-based Bayesian linear regression (total
distance)

We use the rules from the previous section, and set r|β ∼
Exp(10), which corresponds to a relatively high level of
our conﬁdence.

Sampling from this version of rule-based Bayesian re-

gression is depicted in Algorithm 2.

The posterior plots are shown in Figure 7. The uncer-
tainty range is somewhere between the wide uncertainty of
the case without rules (Figure 3), and the narrow uncer-
tainty of the case with the proportion rules (Figure 5). A

Fig. 5 Posterior regression lines of the proportion variation of the
rule-based Bayesian regression.

as the variation that includes rule-related hyperparameters
see Botsas et al. (2020).

The analytical steps for sampling from the rule-based

Bayesian regression model are presented in Algorithm 1.

Algorithm 1: Analytical sampling steps

Construct discretisations of the rule-input values. Take n

equally-spaced points between nmin and nmid
(antecedent of the ﬁrst rule) and n equally-spaced points
between nmid and nmax (antecedent of the second rule);

for each MCMC iteration do

Sample new values of α and β;
Compute the outputs from the parameter values of step

1 and the discretisations of the rule-input values;
Calculate the number of these points that violate the

corresponding consequents;

Calculate the ratio of the number of points that violate

the rules over the number of all (2n) points;

Compute r|β (here ∼ Beta(1, b);
Calculate the un-normalised posterior as the product of
the prior, the likelihood and the quantity r|β from the
previous step;

end

The posterior results are shown in Figure 5. We can ob-
serve a slight difference for the line that corresponds to the
MAP, but the main contrast is the signiﬁcant reduction in
the posteriors’ uncertainty (denoted by the red lines). Sum-
mary statistics in Table 1 show that the means are slightly
closer to their true counterparts for the rule-based Bayesian
linear regression, while the variance estimates conﬁrm the

Rule-based regression

7

Algorithm 2: Analytical sampling steps

Construct discretisations of the rule-input values. Take n

equally-spaced points between nmin and nmid
(antecedent of the ﬁrst rule) and n equally-spaced points
between nmid and nmax (antecedent of the second rule);

for each MCMC iteration do

Sample new values of α and β;
Compute the outputs from the parameter values of step

1 and the discretisations of the rule-input values;

Inspect which of these points violate the corresponding

consequents;

Calculate the total distance (sum of individual euclidean

distances) of the points that violate the rules;

Compute r|β (here ∼ Exp(λ);
Calculate the un-normalised posterior as the product of
the prior, the likelihood and the quantity r|β from the
previous step;

end

Fig. 7 Posterior regression lines of the total distance variation of the
rule-based Bayesian regressions.

larger value of λ would move the result towards the lat-
ter. The corresponding summary statistics and metrics in-
cluded in Table 1 and Table 2 respectively show that the
performance of this rule-based Bayesian regression varia-
tion is on par with the one from the other rule-based vari-
ation, while both rule-based versions perform better than
the standard Bayesian regression in terms of all metrics.

3.1.7 Remarks

This application outlines the main motivation for this pa-
per; we managed to construct new models that exceed in
performance the standard method, by incorporating rules
that were automatically derived from the grammatical evo-
lution algorithms, using only grammars, in order to restrict
the search space. The algorithms managed to ﬁnd patterns
that were not obvious given the training data and the results
were slightly better in terms of the MAP, and signiﬁcantly
better in terms of uncertainty.

Fig. 8 One-dimensional velocity advection data.

forcing function is

∂u
∂t

+ u

∂u
∂x

= f (x, t; a, φ),

(13)

where u(x, t) is the velocity, x the position, t the time, and
f (x, t; a, φ) is the external forcing term with amplitude a
and phase φ.

For the Bayesian analyses that follow we ﬁt third-
degree B-spline models with 50 knots. Speciﬁcally, we
use a reparameterisation that uses the increments of the
splines (Kharratzadeh, 2017; Rochford, 2017) as:

ai = a0 + σa

i
(cid:88)

j=1

∆aj.

For the priors’ speciﬁcation we use a0 ∼ N (0, 0.12), σa ∼
Half Cauchy(0.1) and ∆aj ∼ N (0, 52), while the likeli-
hood variance is ﬁxed at 0.0022.

For sampling we use the PyMC3 (Salvatier et al.,
2016) sequential Monte Carlo (SMC) variation, which
is a mixture of the Transitional Markov Chain Monte
Carlo (TMCMC) (Ching and Chen, 2007) and Cascading
Adaptive Transitional Metropolis in Parallel (CATMIP)
(Minson et al., 2013) algorithms. We use 10 000 draws,
which in this implementation also corresponds to the
number of chains. Finally, for the posterior plots we use a
thinning of 10.

3.2.1 Data

The data for the second application are constructed from
a one-dimensional advection velocity equation (Bar-Sinai
et al., 2019) with amplitude a = 0.001 and phase φ = π.
We extract the data for three different snapshots tj (corre-
sponding to t = 1, 2, 3), before adding a Gaussian error
with a standard deviation of 0.002:

y = u(x, t) + (cid:15)
(cid:15) ∼ N (0, 0.0022).

3.2 One-dimensional velocity advection equation

The velocity advection equation governs transport of mo-
mentum by bulk motion. Its one-dimensional form, with a

The data consist of 96 points (32 values for each snapshot)
and are shown in a single plot in Figure 8 along with the
corresponding true curves. The point where the curvature
changes for all snapshots (x = π) corresponds to the black
dashed line.

8

Botsas, Mason, Matar, Pan

Table 4 Evaluation metrics for the different spline models.

Metric/Model Without rules
3.20 × 10−7
4.38 × 10−4
897.34

MSE
MAE
WAIC

With rules
2.68 × 10−7
4.05 × 10−4
899.39

Fig. 10 Depiction of the rule-base chosen by grammatical evolution.

Fig. 9 Posterior curves for the standard Bayesian regression. The
red, green and blue lines are derived from (thinned) samples of the
MCMC chain for t = 1, 2, and 3 respectively. The black lines de-
note the exact solutions.

Table 5 The grammar used for
dimensional velocity advection rules.

the production of

the one-

N = {expr, comp, num, uv}
T = {x, u1, u2, u3, >, <, 0.1, . . . , 4.0}
S =< expr >

R = Production rules:
< expr > :== (cid:80) ((x < num) (cid:54)= (comp(uv, uv))
< comp > :==> | <
< uv > :== 0.1|0.2| . . . |4.0.

3.2.2 Bayesian B-splines regression

The results of the standard Bayesian B-splines regression
are presented in Figure 9. We can observe a lot of overlap
among the posterior curves, especially in the left side of
the plot, where the curves that correspond to t = 2 and
t = 3 are clearly ﬂipped, while at the right side of the plot
the same curves seem to overlap almost entirely. Similarly
in the middle of the plot, the curvature of the curve that
corresponds to t = 1 changes earlier than expected (at x =
π). All these discrepancies are attributed to the Gaussian
error included in the data. In the next sections, we examine
whether we can use the methodology of this paper in order
to derive a better ﬁt even with the limited knowledge that
there should be only one point where the curvature of each
pair of curves changes.

3.2.3 Rules derivation

Once again we specify the requirements for grammatical
evolution, starting from the grammar in Table 5. The cost
function is f (x) = x.

This set-up has some major similarities with the one
in Section 3.1.3, in the sense that we are still attempting
to minimise the points in expr, while most of the three
components remain the same. The only one that changes is

Fig. 11 Posterior curves for the rule-based Bayesian regression. The
red, green and blue lines are (thinned) samples of the MCMC chain
for t = 1, 2, and 3 respectively. The black lines denote the exact so-
lutions and the red dashed vertical line denotes the rule changepoints.

the grammar to denote that, instead of counting the points
at the quadrants, our goal is to count the points where the
outputs that correspond to different time-steps are above
(or below) each other.

The rules produced by grammatical evolution are:

1 : if 2.6 ≤ x ≤ 2π,

R1 : if 0 ≤ x ≤ 2.6,
R(cid:48)
R2 : if 0 ≤ x ≤ 3.6,
R(cid:48)

2 : if 2.6 ≤ x ≤ 2π,

then u1 ≤ u2,

then u1 > u2,

then u2 ≤ u3,

then u2 > u3.

and the composite rule base (Rcomp) is given by
1 ∧ R2 ∧ R(cid:48)
2.

Rcomp := R1 ∧ R(cid:48)

The result is shown in Figure 10. According to the
rules, before the changepoint x = 2.6, u1 ≤ u2, and after
it u1 > u2. This corresponds to the dashed red line in the
plot. Similarly, before the green line changepoint x = 3.6,
u2 ≤ u3, and after it u2 > u3. It is obvious that gram-
matical evolution did not manage to produce the optimum
rules (which we know from the theory that correspond to
the black dashed line x = π), but the result was close. We
contribute this discrepancy to the fact that the data included
a fair amount of noise.

3.2.4 Rule-based Bayesian regression

For the rule-based Bayesian analysis we are going to use
the same rule conditional distribution as in Section 3.1.4,
speciﬁcally r|β ∼ Beta(1, 100).

Rule-based regression

9

The results are shown in Figure 9. The issue at the mid-
dle of the plot regarding the early curvature change of the
posterior plots that correspond to t = 2 still remains, but
the problems with the left and right edges of the plot have
been resolved; the overlap is reduced, the order is correct
and the posterior curves are much closer to their true coun-
terparts.

In Table 4 we include relevant metrics. Note that the
MSE and MAE are calculated using the MAP and with re-
spect to the true values (the ones that correspond to the
curves of the Figures) rather than the observed data (those
that correspond to the points of the Figures). Obviously
the standard Bayesian regression would yield a better MSE
than the rule-based variation if we evaluated the metrics at
the observed data points, since, for that case, the MSE is
implicitly minimised during training, but our goal here is
to try and incorporate any additional knowledge we have
in order to derive a result closer to reality. The two met-
rics mentioned above show that the rule-based Bayesian re-
gression performed better than the non-rule version, which
reafﬁrms the intuition from the Figures. It is interesting to
note that the WAIC indicates that the penalty for the point
performance increase was additional uncertainty.

Fig. 12 Scatterplot of ambient humidity (AH) and carbon monoxide
emissions (CO). The red points show the observed data (used for the
model ﬁtting) and the blue points the unobserved data (used for the
testing).

how the models generalise. Note that, as we describe in
Botsas et al. (2020), it is known that there is expert con-
sensus to expect a connection between AH and CO.

3.2.5 Remarks

3.3.2 Bayesian multivariate linear regression

Once again grammatical evolution managed to ﬁnd useful
rules, given the restrictions that we imposed. Even though
the rules were not optimal, and more extensive expert
knowledge would be beneﬁcial in this case (see Section
4.2 in Botsas et al. (2020)), there was still a performance
increase, which helped to model move towards to the true
solution.

3.3 Carbon monoxide (CO) emissions from gas turbines

For the third application, our aim is to predict CO emis-
sion levels of a gas turbine using a multivariate linear re-
gression model. For all the analyses that follow we run a
single Metropolis - Hastings chain with 100 000 draws, in
addition to a burn-in of 30 000 iterations and thinning of
100, which leaves 1000 samples for each analysis. Once
again we use the PyMC3 package (Salvatier et al., 2016).

3.3.1 Data

The dataset comes from a ﬁeld turbine and is described in
Kaya et al. (2019). We speciﬁcally use the section of the
data that correspond to year 2013. We select four of the
features to avoid strong correlations: the ambient temper-
ature AT , the ambient humidity AH, the air ﬁlter differ-
ence pressure AF DP and the gas turbine exhaust pressure
GT EP and focus on the CO emissions as a single out-
put. Our training set consists of the data where the AH is
over 95% of the available data set as shown in Figure 12,
replicating a condition where collection occurs during days
with very high humidity, which leaves 547 data points from
the original 7152. We use the rest of the data to examine

For the baseline model we will use multivariate linear re-
gression with parameters the coefﬁcients of all the features.
The model is:

CO = ATco ∗ AT + AHco ∗ AH + AF DPco ∗ AF DP +

GT EPco ∗ GT EP + b + (cid:15),

where the carbon monoxide emission level CO is the re-
sponse, ATco, AHco, AF DPco, and GT EPco are the co-
efﬁcients of the features that were described in the previous
section, b is the intercept and (cid:15) is Gaussian error with:

(cid:15) ∼ N (0, σ2).

We choose Gaussian distributions for the regression co-
efﬁcients and intercept, and Exponential for the standard
deviation:

ATco, AHco, . . . ∼ N (0, 102),
b ∼ N (0, 202),
σ ∼ Exp(1).

We are going to focus on the AH − CO and GT EP −
CO pairs. The corresponding scatterplots, along with the
posterior predictive samples are shown in Figure 13 and
Figure 14 respectively. In the former we can clearly see that
the uncertainty increases drastically away from the training
data, while in the latter the slope is slightly different to the
one implied by the data.

Evaluation metrics for all the models are included in

Table 6.

10

Botsas, Mason, Matar, Pan

3.3.3 Rules derivation (piece-wise regression)

The grammar for this case is slightly more complex than
the ones in the previous sections. It is presented in Ta-
ble 7. We need to take into account a few things about this
grammar. First, before we feed the data into the algorithm
we standardise them. Not only it is going to help with the
Bayesian sampling later, but, more importantly, it makes
the range −2 to 2 of < num > robust, since it can be used
regardless of feature (or response). Second, the algorithm
by default samples two different < expry >, one for when
the if statement of < expr > is satisﬁed and another one
when it is not.

We use a residual sum of squares (RSS) type of cost

Fig. 13 Scatterplot of AH and CO. The green points are the ob-
served and the blue points the unobserved data. The posterior predic-
tive samples of model without rules are shown in orange and mean in
dark red.

function:

f (x) =

N
(cid:88)

i=1

(CO − y(cid:48))2 ,

where N is the number of training data and y(cid:48) are the
rule-output values that occurred from < expr > as de-
scribed in Section 2.3.

We attempted to derive rules for all the different fea-
tures, therefore, we went through all the components of
< var > sequentially and run the algorithm again. The
results are shown in Figure 15.

Most of the rules are piece-wise linear, but

there
are some exceptions. For example the left piece of the
GT EP − CO pair is a second degree curve, and the rules
associated with the AT −CO pair are both constant. These
forms are permitted by the grammar, which, depending on
its nature, can allow for more restrictive or more ﬂexible
types of rules.

It is very important to note that all these forms are in no
way connected to the model that we are trying to ﬁt (in this
case a multivariate linear regression model). Regardless of
the complexity of the piece-wise models, their only pur-
pose is to add a penalty to the corresponding parameters,
and the ﬁnal model that we will derive in the next section
is going to be linear regardless of whether we apply rules
or not. The value of the penalty is going to be directly as-
sociated with the distance of the actual model (linear) from
the rule piece-wise model (which can have various forms).
We are going to focus on the rules associated with the
GT EP −CO (best in terms of the cost function) and AH−
CO (worst in terms of the cost function) pairs. We remind
the reader that AH is the only features for which we have
some information (i.e. the training data correspond to the
ones with high humidity).

The rules produced by the former pair (before rescal-

ing) are:

R1 : if GT EP ≤ 23.11,
R(cid:48)
1 : if GT EP > 23.11,

then y(cid:48) = GT EP 2,
then y(cid:48) = −0.7,

and for the latter:

R2 : if AH ≥ 97.16,
R(cid:48)
2 : if AH < 97.16,

then y(cid:48) = −0.05AH,
then y(cid:48) = −0.2.

Fig. 14 Scatterplot of GT EP and CO. The green points are the
observed and the blue points the unobserved data. The posterior pre-
dictive samples of model without rules are shown in orange and mean
in dark red.

Table 6 Evaluation metrics for the different models.

Metric/Model
MSE
MAE

No rules GT EP rules AH rules
2.19
1.25

1.36
0.99

0.83
0.78

Table 7 The grammar used for the production of the carbon monox-
ide rules.

N = {op, comp, var, num, numr, expr, expry}
T = {AT, AH, . . . , >, <, ≤, ≥, +, −, ∗, −5.0, . . . , 5.0}
S =< expr >

R = Production rules:
< expr > :== if else((comp(var, num)), expry, expry)
< expry > :== op(var, var)|op(var, numr)|numr)
< op > :== +| − |∗
< comp > :==> | < | ≤ | ≥
< var > :== AT |AH|AF DP |GT EP
< numr > :== −5| − 4.95| . . . |5
< num > :== −2| − 1.95| . . . |2.

Rule-based regression

11

Fig. 15 Depiction of the rule-bases chosen by grammatical evolution for each feature. The cost on top of each plot indicates the Residual
Sum of Squares between the training data (green points) and rule-regression curves (in red). The vertical dotted red lines indicate where the
piece-wise models change shape. The yellow line in the last plot is the extension of the rule in the area where there are no training data.

The composite rule bases (Rcomp) are given by

Rcomp := Ri ∧ R(cid:48)
i,

for

i = 1, 2.

3.3.4 Rule-based Bayesian multivariate linear regression
(piece-wise regression)

For both pairs we are going to use a Gaussian distribution
for y(cid:48)|β as described in the last variation of Section 2.3.
Speciﬁcally we use a distribution with mean the residual
sum of squares of the rule-output values and the response
and standard deviation 0.1, which indicates relatively high
levels of conﬁdence in the rules.

Sampling from the model that incorporates the above

is presented in Algorithm 3.

Algorithm 3: Analytical sampling steps

Split the data based on the feature (i) and the changepoint

(c) from the corresponding rule.;

Calculate the rule-output values from the antecedents of the
rules (y(cid:48) = f1(i) and y(cid:48) = f2(i)). ;
for each MCMC iteration do

Sample new values of the intercept and the coefﬁcients

of the features;

Compute y(cid:48)|β from the Gaussian with mean the RSS

of the rule-output and CO data and standard deviation
sd: y(cid:48)|β ∼ N (y(cid:48) − CO, sd2);

Calculate the un-normalized posterior as the product of
the prior, the likelihood and the quantity y(cid:48)|β from
the previous step;

end

Fig. 16 Scatterplot of AH and CO. The green points are the ob-
served and the blue points the unobserved data. The posterior predic-
tive samples of model without rules are shown in orange and mean in
dark red.

As explained in the previous section, we are going
to run two analyses for the different rules derived by
the grammatical evolution. The ﬁrst one regards the pair
GT EP − CO; the posterior results for the scatterplots,
after we incorporate the corresponding rules, are shown
in Figures 16 and 17. We can see that in both scatterpolts
the uncertainty has decreased substantially. That does not
seem to be very helpful for the AH − CO pair, since the
the resulting slope is quite different from the one implied
by the data.

The scatterplots for the AH − CO case are presented
in Figures 18 and 19. Once again, the uncertainty has de-

12

Botsas, Mason, Matar, Pan

Fig. 17 Scatterplot of GT EP and CO. The green points are the ob-
served and the blue points the unobserved data. The posterior predic-
tive samples of model without rules are shown in orange and mean in
dark red. The light blue lines indicate the piece-wise rule model and
the vertical dashed light blue line the changepoint.

Fig. 19 Scatterplot of GT EP and CO. The green points are the
observed and the blue points the unobserved data. The posterior pre-
dictive samples of model without rules are shown in orange and mean
in dark red.

3.3.5 Remarks

In this example we saw how the methodology can be used
with real data-sets. The piece-wise regression rules showed
that the rule-base selected by grammatical evolution is not
always helpful, but, instead, when the algorithm is coupled
with additional information and available domain knowl-
edge it can offer signiﬁcant improvements.

3.4 Full load electrical power output of a combined cycle
power plant

So far we have only dealt with regression problems. For the
fourth and ﬁnal application, our aim is to predict whether
the electrical output of a combined cycle power plant (gas
and steam turbines) is high or low; therefore we frame it as
a classiﬁcation problem and we tackle it using multivariate
logistic regression. For the analyses we run a Metropolis -
Hastings chain with 100 000 draws from the PyMC3 pack-
age (Salvatier et al., 2016), in addition to a 30 000 itera-
tions burn-in and thinning of 100. In total there are 1000
ﬁnal samples for the analysis.

3.4.1 Data

The data are derived from a combined cycle power plant,
described in T¨ufekci (2014). They consist of four features:
the ambient temperature AT , the ambient pressure AP , the
humidity RH and the vacuum V , and one output: the elec-
trical energy P E. We use the number 465 as a cut-off point
of the P E to create our label P Eclass, as shown by the
green horizontal line in Figure 20. Therefore, points where
P E ≥ 440 are given the value P Eclass = 0, while points
where P E < 440 are assigned the value P Eclass = 1. Our
training data are a sample of points where AT ≥ 25oC,
which is equivalent to collecting data during high tempera-
ture days (e.g. summer season). In total the training dataset
produced consists of 704 points for class 0 and 2065 points
for the class 1. We evaluate the models in the remaining

Fig. 18 Scatterplot of AH and CO. The green points are the ob-
served and the blue points the unobserved data. The posterior predic-
tive samples of model without rules are shown in orange and mean in
dark red. The light blue lines indicate the piece-wise rule model and
the vertical dashed light blue line the changepoint.

creased. The slope for the AH − CO pair is quite different
from the previous case, approaching the straight line from
the left-hand side of the AH − CO rule.

Examining the metrics in Table 6 we can see that, de-
spite the decrease in the uncertainty, the model with the
GT EP − CO rules performed signiﬁcantly worse than
the baseline case, even though it was the best pick from
the grammatical evolution algorithm in terms of the cost
function. The reason is that, even though the ﬁt is better
for the GT EP − CO pair, we should be mindful that the
main model is still a multivariate linear regressor, and the
ﬁt was poor for some of the other dimensions, thus, mak-
ing the overall ﬁt poor. On the contrary, the AH − CO
ﬁt ended up being signiﬁcantly better than the baseline ﬁt.
Note that AH was the one feature we already had infor-
mation before the analysis. This is a particularly important
result, since it shows the limitations of incorporating the
grammatical evolution algorithm into our method, and it
also indicates that it works better in combination with do-
main/expert information.

Rule-based regression

13

Table 9 Evaluation metrics for the different models after class bal-
ancing.

Metric/Model
Accuracy
AUC
Sensitivity

No rules AT rules
0.9506
0.9498
0.8007
0.5683
0.6318
0.1384

Table 10 The grammar used for the production of the power plant
rules.

N = {comp, var, num, expr}
T = {AT, AP, . . . , >, <, ≤, ≥, −1.4, . . . , 3.5}
S =< expr >

R = Production rules:
< expr > :== if else((comp(var, num)), 1, 0)
< comp > :==> | < | ≤ | ≥
< var > :== AT |AP |RH|V
< num > :== −1.4| − 1.35| . . . |3.5.

3.4.3 Rules derivation

The grammar is presented in Table 10. Again we need to
standardise the data for reasons explained in the previous
example.

The cost function is:

f (x) =

N
(cid:88)

i=1

(P Eclass! = y) ,

and the optimisation algorithm is evolution strategy.

We examine all the components of < var > sequen-
tially. The results are shown in Figure 21. Note that for
visualisation purposes we show the continuous version of
the output P E, but we use only the categorical version
P Eclass for the grammatical evolution algorithm.

We focus on the rule associated with the AT −P Eclass
pair. We know from Botsas et al. (2020) and T¨ufekci
(2014) that the ambient temperature has some association
with the electrical output. After rescaling, the rule is:

R1 : if AT < 25.34,
R(cid:48)
1 : if AT ≥ 25.34,

then y = 0,

then y = 1,

and the composite rule base (Rcomp) is given by

Rcomp := R1 ∧ R(cid:48)
i,

for

i = 1, 2.

3.4.4 Rule-based Bayesian multivariate logistic
regression

We use the rules from the previous section and set r|β ∼
Beta(1, 1000), i.e. a very high level of our conﬁdence in
the rules. Sampling is very similar to the one in Algo-
rithm 1. For inspecting whether a point violates the rule
we compare y with 0.5.

The results in Table 8 indicate that the version without
the rules performs better in terms of accuracy, whereas the
version with the AT rules has a higher sensitivity and is
also better in terms of the area-under-curve (AUC) metric.
In many real data problems, especially with imbalanced
classes, it might be worth applying this trade-off. In this

Fig. 20 Scatterplot of AT and P E. The red points show the ob-
served data (used for the model ﬁtting) and the blue points the unob-
served data (used for the testing). The green horizontal line indicates
where the class changes

Table 8 Evaluation metrics for the different models.

Metric/Model
Accuracy
AUC
Sensitivity

No rules AT rules
0.8857
0.9597
0.8805
0.8264
0.8746
0.6762

dataset, which consists of 6416 points for class 0 and 383
points for the class 1. Note that this is a signiﬁcantly imbal-
anced dataset, which adds to the complexity of the prob-
lem. Parts of the analysis that follows uses popular tech-
niques to tackle this issue directly.

3.4.2 Bayesian multivariate logistic regression

We use a multivariate logistic regression model with pa-
rameters the coefﬁcients of the features:

P Eclass = σ(ATco ∗ AT + APco ∗ AP + RHco ∗ RH+

Vco ∗ V + b) + (cid:15),

where P Eclass is the response, ATco, APco, RHco, and
Vco are the features coefﬁcients described in the previous
section, b is the intercept, σ is the sigmoid function and (cid:15)
is Gaussian error with:

(cid:15) ∼ N (0, σ2).

Similarly to the previous example, we choose Gaussian
distributions for the regression coefﬁcients and intercept,
and Exponential for the standard deviation of the Gaussian
error:

ATco, APco, . . . ∼ N (0, 102),
b ∼ N (0, 202),
σ ∼ Exp(1).

Evaluation metrics for all the models are shown in Ta-
ble 8 and Table 9. In the latter, we repeat the same analyses
after upsampling the minority class in the training dataset.

14

Botsas, Mason, Matar, Pan

Fig. 21 Depiction of the rule-bases chosen by grammatical evolution for each feature. The cost on top of each plot indicates the number of
training data (in green) that do not follow the rule. The vertical dashed red lines indicate the position of the rules. The horizontal dashed blue
line indicates the change of class; Gold points correspond to low and black points to high power output.

example, it could be important to know when the electrical
power output of a plant is very low and adjust our planning
accordingly. This problem approach would make sensitiv-
ity the most important metric.

The results are conﬁrmed in Figure 22, where, for the
majority of the plot, the ROC curve that corresponds to the
rule version is higher on the y (sensitivity) axis than the
no-rule counterpart, which means that for most thresholds
the model with the rules will predict more points of class
1 correctly. The area under the ROC curve is also visibly
larger in the rule version.

In Table 9 we have repeated the analysis, but with some
additional pre-processing in the training set in order to bal-
ance the classes. In terms of accuracy the model with the
AT rules performs marginally better, while the AUC and
sensitivity metrics indicate a signiﬁcant performance in-
crease for the model with the rules. In almost all metrics,
though, we see worse performance than the correspond-
ing results without the balancing (with the accuracy of the
model with the AT rules being the only exception). We
contribute this to the fact that the imbalance in the training
set (where there are more data-points with low electrical
output) is different than the imbalance in the testing set
(where there are more data-points with high electrical out-
put), and therefore balancing the former was not beneﬁcial.

3.4.5 Remarks

In the ﬁnal example we examined how the methodology
can be applied to classiﬁcation problems. We also saw that
the result with the rule versions might not be beneﬁcial in

Fig. 22 ROC curves for the mean posterior prediction of the case
without rules (in red) and with AT rules (in blue).

terms of speciﬁc metrics (e.g. accuracy), but could be in
others (e.g. speciﬁcity).

4 Discussion

With the applications of the previous section, we illustrated
how the methodology can help derive better results and/or
reduce the system uncertainty with the help of automat-
ically derived rules and an appropriate Bayesian context.
What is very apparent, though, is that one needs to be very
careful when using that methodology. Grammatical evolu-
tion attempts to ﬁnd patterns and associations amongst the
data that might not always be meaningful or helpful. This is
why this method is better used when combined with (even

Rule-based regression

15

weak) expert knowledge or domain information, for which
there is no real substitute.

Additionally,

there is a question concerning com-
plexity, i.e. how ﬂexible the grammar should be and,
consequently, how convoluted the derived rules can be,
and whether the ﬁnal patterns are so intricate that are
no longer worth the effort. In general, we believe that it
is better to try to derive rules that can reﬂect something
meaningful for the parameter associations, i.e. can be
connected or even help discover intuitions, and adjust the
grammar accordingly.

We also need to re-iterate limitations associated with
the rule-based Bayesian regression context with or without
the addition of grammatical evolution. These include the
computational complexity linked with the sampling tech-
nique, and the potentially complex shape of the rule-based
posterior.

Regarding the translation of the methodology into
code, we opted for a two-step process. Speciﬁcally, we
used the gramEvol package from R (Noorian et al., 2016)
to derive the rules, and the PyMC3 Python package (Sal-
vatier et al., 2016) to construct the Bayesian framework.
The modular nature and simplicity of the methodology
indicate that the requirements to implement it include
any grammatical evolution package like PonyGE2 Fenton
et al. (2017), and any probabilistic framework, such as
Stan (Stan Development Team, 2019), or TensorFlow
Probability (Abadi et al., 2016).

5 Conclusion

In this paper, we extended our
rule-based Bayesian
methodology of Botsas et al. (2020) by introducing a
grammatical evolution step, which automates the rule
discovery. We presented the general framework and used
the methodology in four applications, adopting different
statistical models. In the ﬁrst application, we derived
data from a linear model and we used a uni-variate linear
regression model, in the second, we used data from a
one-dimensional velocity advection equation and we ﬁt
third-degree B-splines, in the third, we used multivariate
linear regression models to predict the CO emissions from
a gas turbine, and, ﬁnally, in the third we used multivariate
logistic regression models to predict whether the electrical
output of a power plant was high or low.

We extended the rule-based Bayesian regression
framework with different variations of the penalty and
associated distribution. Other than the proportion of the
rule-inputs that violate the rules, modeled with a Beta
distribution, that was introduced in Botsas et al. (2020),
we proposed a penalty based on the total distance from the
rule-boundary, modeled with an Exponential distribution
and a piece-wise regression penalty with an associated
Gaussian distribution. We also presented how we can use
the methodology to perform a classiﬁcation task.

Future research should be focused in applying the
methodology to more complex real data applications,
where the challenges mentioned in Section 4 might

be more prominent, such as computational issues and
difﬁculty of assessing the performance of a grammatical
evolution-derived rule.

Acknowledgements This work was supported by Wave 1 of
The UKRI Strategic Priorities Fund under
the EPSRC Grant
EP/T001569/1, particularly the Digital Twins for Complex Engineer-
ing Systems theme within that grant and The Alan Turing Institute.
IP was partially supported by the NUAcT fellowship at Newcastle
University.

Conﬂict of interest

The authors declare that they have no conﬂict of interest.

References

Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean,
J., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al.
(2016). Tensorﬂow: A system for large-scale machine
In 12th {USENIX} Symposium on Operat-
learning.
ing Systems Design and Implementation ({OSDI} 16),
pages 265–283.

Bar-Sinai, Y., Hoyer, S., Hickey, J., and Brenner, M. P.
(2019). Learning data-driven discretizations for par-
tial differential equations. Proceedings of the National
Academy of Sciences, 116(31):15344–15349.

Beyer, H.-G. and Schwefel, H.-P. (2002).

Evolution
strategies–a comprehensive introduction. Natural com-
puting, 1(1):3–52.

Botsas, T., Mason, L. R., and Pan, I. (2020). Rule-based
bayesian regression. arXiv preprint arXiv:2008.00422.
Ching, J. and Chen, Y.-C. (2007). Transitional Markov
chain Monte Carlo method for Bayesian model updat-
ing, model class selection, and model averaging. Jour-
nal of Engineering Mechanics, 133(7):816–832.

Fenton, M., McDermott, J., Fagan, D., Forstenlechner, S.,
Hemberg, E., and O’Neill, M. (2017). Ponyge2: Gram-
matical evolution in python. In Proceedings of the Ge-
netic and Evolutionary Computation Conference Com-
panion, pages 1194–1201.

Hastings, W. K. (1970). Monte Carlo sampling methods
using Markov chains and their applications. Biometrika,
57(1):97–109.

Kaya, H., T¨ufekci, P., and Uzun, E. (2019). Predicting CO
and NOx emissions from gas turbines: novel data and a
benchmark PEMS. Turkish Journal of Electrical Engi-
neering & Computer Sciences, 27(6):4783–4796.
in
(2017).
https://github.com/milkha/Splines_in_
Stan/blob/master/splines_in_stan.pdf.

Kharratzadeh, M.

Splines

Stan.

Minson, S., Simons, M., and Beck, J. (2013). Bayesian
inversion for ﬁnite fault earthquake source models
I—Theory and algorithm. Geophysical Journal Inter-
national, 194(3):1701–1726.

Molnar, C. (2020). Interpretable machine learning. Lulu.

com.

16

Botsas, Mason, Matar, Pan

Noorian, F., de Silva, A. M., Leong, P. H., et al. (2016).
gramevol: Grammatical evolution in r. Journal of Statis-
tical Software, 71(1):1–26.

O’Hagan, A. (2019).

subjective but scientiﬁc.
73(sup1):69–81.

Expert knowledge elicitation:
The American Statistician,

Rochford, A. (2017). A PyMC3 port of Splines in Stan.
https://gist.github.com/AustinRochford/
d640a240af12f6869a7b9b592485ca15.

Ryan, C., Collins, J. J., and Neill, M. O. (1998). Grammat-
ical evolution: Evolving programs for an arbitrary lan-
In European Conference on Genetic Program-
guage.
ming, pages 83–96. Springer.

Salvatier, J., Wiecki, T. V., and Fonnesbeck, C. (2016).
Probabilistic programming in python using PyMC3.
PeerJ Computer Science, 2:e55.

Stan Development Team (2019). RStan: the R interface to

Stan. R package version 2.19.1.

T¨ufekci, P. (2014). Prediction of full load electrical power
output of a base load operated combined cycle power
International
plant using machine learning methods.
Journal of Electrical Power & Energy Systems, 60:126–
140.

