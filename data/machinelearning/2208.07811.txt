Towards Informed Design and Validation Assistance
in Computer Games Using Imitation Learning

Alessandro Sestini1,2, Joakim Bergdahl1, Konrad Tollmar1, Andrew D. Bagdanov2, Linus Gissl´en1
1SEED - Electronic Arts (EA)
2Universit`a degli Studi di Firenze
{jbergdahl, ktollmar, lgisslen}@ea.com
{alessandro.sestini, andrew.bagdanov}@uniﬁ.it

2
2
0
2

g
u
A
9
1

]
E
S
.
s
c
[

2
v
1
1
8
7
0
.
8
0
2
2
:
v
i
X
r
a

Abstract—In games, as in and many other domains, design
validation and testing is a huge challenge as systems are growing
in size and manual testing is becoming infeasible. This paper
proposes a new approach to automated game validation and
testing. Our method leverages a data-driven imitation learning
technique, which requires little effort and time and no knowledge
of machine learning or programming, that designers can use to
efﬁciently train game testing agents. We investigate the validity
of our approach through a user study with industry experts.
The survey results show that our method is indeed a valid
approach to game validation and that data-driven programming
would be a useful aid to reducing effort and increasing quality
of modern playtesting. The survey also highlights several open
challenges. With the help of the most recent literature, we
analyze the identiﬁed challenges and propose future research
directions suitable for supporting and maximizing the utility of
our approach.

Index Terms—Automated playtesting, imitation learning, game

design

I. INTRODUCTION

Modern games are often enormous and are growing expo-
nentially in size, complexity, and asset count on every iteration.
To ensure the game works as intended each time a level or
level area is created or modiﬁed, there is a need for gameplay
validation. Playtesting is one common procedure for assessing
the quality of games. The testers check whether the game
is completed, if it is fun and sufﬁciently challenging, or if
it has problems like bugs or glitches. The process of game
validation and testing is usually done either by an in-house
quality veriﬁcation (QV) team or by internal and external
designated human testers. However, manual playtesting does
not scale well with the size of the game and turnaround time
– the time between game design and feedback – is often long.
To be effective, game validation should be performed as
early as possible and ideally by those who actually create the
content that requires testing: the game creators. Note: game
validation in this context is the process of evaluating that
the game plays and “feels” like intended. For manual game
testing, this is often limiting as bringing in human playtesters
would be slow, expensive and inefﬁcient. With automated
game validation, it would be possible to bring in game playing
agents directly into the development phase.

Recently, automated playtesting and validation techniques
have been proposed to mitigate the need for manual validation

Fig. 1: Our proposed data-driven approach lets designers
perform automated game validation directly during design
phase. The training is interactive as the algorithm allows
designers to smoothly switch between designing the level,
providing gameplay demonstrations and getting feedback from
the trained testing agents.

in large games. Automated gameplay testing offers a fast and
relatively cheap solution for testing games at scale. This is
often done by crafting model-based bots and using them to
perform automated playtesting [1]. Another proposed solution
as seen in recent research that also highlights some industry
use-cases is to train self-learning agents with reinforcement
learning [2, 3, 4]. However, both of these approaches suffer
from a few drawbacks. Model-based agents, that typically
refers to bots with hand-crafted behaviors (i.e. achieved by
scripting), require a certain level of domain knowledge and
programming skills that game designers do not necessarily
have. Moreover, the lack of generalization in this method
might render the agents unusable if changes are made in the
environment. Reinforcement learning, on the other hand, can
learn to play the game without scripting, and even can be re-
trained when the environment change. However, reinforcement
learning is sample-inefﬁcient and arguably requires a high
level of expertise in machine learning to effectively be used,
e.g. for properly creating a well performing reward function.
Furthermore, reinforcement learning in general provides a low

 
 
 
 
 
 
level of controllability as the agents will try to exploit the
environment regardless of the intentions of the designer [5].
Additionally, making a game compliant with a reinforcement
learning training setup is a considerable engineering effort
potentially requiring intrusive changes to the game’s source
code.

Ideally for an effective design tool for game designers, it

should satisfy a set of identiﬁed requirements:

• imitation - the agent behaviors should be learned from
demonstration. In this way the designer can show, rather
than program, desired behaviors;

• efﬁciency - as a tool for real-time game validation, it
needs to be close to instantaneous, therefore training time
and required samples should be minimal;

• generalization - the method should be able to adapt to
reasonable small design changes without retraining;
• controllability - having full control over behaviors can
sometimes be crucial, especially when validating short
sequences in games; and

• personas - the method should mimic different players’
play-styles in order to lead to more meaningful testing;
and

• machine learning expertise - no machine learning knowl-
edge should be required (as this is not a standard tool in
game design and therefore not common knowledge).

In this paper, we propose a data-driven approach for cre-
ating agent behaviors for automated game validation visible
in Figure 1. Unlike methods where agents are manually
programmed/scripted, this method uses imitation learning, in
particular the DAgger algorithm [6], to clone the behavior
of the user. With this, we can leverage the expertise of the
game developers while at the same time not require additional
knowledge in programming or machine learning. The feedback
loop for the designers can therefore be instantaneous with
a much more efﬁcient creation process than having to wait
days and sometimes weeks for manual playtests. This study is
made to either validate or refute above claims. The research
questions we are focusing on are: can imitation learning be
used as an effective game design tool? What improvements
and research are needed to maximize its value as a tool? To
explore this we showcase the possible use-cases for which
level designers could use our data-driven approach. Further, we
also propose a survey that explores how our method satisﬁes
the requirements previously mentioned and gauges the interest
of such a tool among professional developers. Based on the
answers from the survey, we propose future research directions
for improving game validation through imitation learning.

II. RELATED WORK

Automated game testing and validation has been gaining
interest from both the research and video game development
communities over recent years [7]. Here, we review work from
the literature most related to our contribution.

A. Automated Playtesting

Several studies have investigated the use of AI techniques
for automated playtesting. Many of these works rely heavily
on classical, hand-scripted AI or random exploration [1, 8].
The Reveal-More algorithm, in addition, uses human demon-
strations to guide random exploration, although in simple
2D dungeon levels [9]. Mugrai et al. [10] developed an
algorithm to mimic human behavior to achieve more mean-
ingful gameplay testing results, but also to aid in the game
design process. Several approaches from the literature are
based on model-based automated playtesting. Iftikhar et al.
[11] proposed a model-based testing approach for automated,
black-box functional testing of platform games, and Lovreto
et al. [12] report their experience with developing model-based
automated tests for 16 mobile games. Other notable examples
of model-based testing can be found in [13, 14, 15, 16].
However, when dealing with complex 3D environments these
scripted or model-based techniques are not readily applicable
due to the high-dimensional state space involved.

In parallel, many other works have used reinforcement
learning (RL) to perform automated playtesting [17]. Zheng
et al. [18] proposed an on-the-ﬂy game testing framework
which leverages evolutionary algorithms, deep-RL and multi-
objective optimization to perform automated game testing.
Agarwal et al. [19] trained reinforcement learning agents to
perform automated playtesting in 2D side-scrolling games
producing visualizations for level design analysis. Bergdahl
et al. [20] propose a study on the usefulness of using RL
policies to playtest levels against model-based agents, while
Gordillo et al. [2] used intrinsic motivation to train many
agents to explore a 3D scenario with the aim of ﬁnding issues
and oversights. Finally, Sestini et al. [3] proposed the curiosity-
conditioned proximal trajectories algorithm with which they
test complex 3D game scenes with a combination of IL, RL
and curiosity driven exploration.

B. Imitation Learning

Imitation learning (IL) aims to distill a policy mimicking the
behavior of an expert demonstrator from a dataset of recorded
demonstrations. It is often assumed that demonstrations come
from an expert who is behaving near-optimally. Standard
approaches are based on behavioral cloning that mainly use
supervised learning [6, 21, 22]. Generative adversarial imita-
tion learning (GAIL) is an imitation learning technique which
is based on a generator-discriminator approach [23], as Finn
et al. [24] noticed that imitation learning is closely related
to the training of generative adversarial networks (GAN).
Based on ideas from GAIL, Peng et al. [25] proposed the
adversarial motion prior (AMP) algorithm, which is a GAN-
based imitation learning method that aims to increase stability
of adversarial approaches.

Few approaches have applied imitation learning to game
testing agents. The method in the previously cited Reveal-
More algorithm [9] uses demonstrations to guide exploration,
although it does not use a learning algorithm. Zhao et al.
[26] used an approach similar to behavioral cloning in which

gameplay agents learn behavioral policies from the game
designers. Harmer et al. [27] trained agents through a combi-
nation of imitation learning and reinforcement learning with
multi-action policies for a ﬁrst person shooter game. Tucker
et al. [28] used an inverse reinforcement learning technique for
training agents that can play a variety of games in the Atari
2600 game suite of the Arcade Learning Environment [29].
In the next section we describe why we think IL is a better
approach to game validation than the cited approaches.

III. PROPOSED METHOD

We propose an approach based on imitation learning to
showcase the potential of data-driven methods for direct game-
play validation to designers. In this section, we ﬁrst describe
why we decide to use IL, then we detail our IL setup and
training algorithm.

A. Method Comparison

Here we give a brief overview on the different common

approaches to automated testing.

Reinforcement Learning. RL for game validation is known
to be sample-inefﬁcient. For example, the work proposed by
Gordillo et al. [2] required an average of two days of training
before thorough coverage of a game scene was achieved. Even
if the sample inefﬁciency can be mitigated by combining RL
with IL similar to the curiosity-conditioned proximal trajectory
algorithm [3], it is still a challenge that must be addressed
for RL to be practically useful in production. Moreover, RL
typically leaves little control to the user over the ﬁnal behavior
of the policy as the trained agents will inherently exploit
the reward function, regardless of the designers intention [5].
We can alleviate this problem with reward shaping, but this
requires considerable knowledge of RL and machine learning
in general, making it unpractical to the general user. Even
with these mentioned attributes, RL still has some advantages.
If designers prefer exploitation and exploration over imitation
then RL is preferable. And similar to IL, it does not need any
programming knowledge to train.

Scripted Bots. With scripted bots we mean bots modelled
with programming methods, i.e. the standard way of creating
game AI bots. For programming autonomous bots, designers
need to have relatively high domain knowledge of the level
design, but also scripting skills to successfully drive the
behaviors of the bots. Moreover, even if they have very ﬁne
control over the bots ﬁnal behavior, the scripted agents will
quickly become sub-optimal, and even unusable, when they
have to face design changes in the environment. The setup
time also greatly depends on the complexity of the game and
game scene. For the reason listed above, we argue that this
approach does not satisfy the requirements listed in section I.

Imitation learning.
IL is more sample-efﬁcient than RL,
and allows for training agents in minutes or only few hours
[3]. With this method we only need to leverage the designer’s
domain knowledge of
thus effectively adding
the game,
humans-in-the-loop for major controllability. Not only can we

Fig. 2: Example of a semantic map used by the agent. Each
cube describes the semantic integer value of the type of object
at the corresponding position relative to the agent highlighted
in blue.

demonstrate the intended behavior, but also even correct it
using new demonstrations with little additional training time.
Compared to scripted behavior, IL provides the same level
of generalization as RL as the policy model should be able
to generalize in some extent to unseen observations. However,
compared to both RL and model-based bots, IL calls for no or
limited theory knowledge or programming skills as designers
only have to demonstrate what they want the IL agent to do.
With IL, we may mitigate some of the drawbacks mentioned
of the two previous methods.

B. Approach

Following the above argumentation, we developed a IL
approach not only to demonstrate our claims, but also to
showcase different use-cases for professional designers for
helping automated game validation.

Algorithm. We use an IL approach based on the DAgger
algorithm [6], visible in Figure 1. DAgger allows designers
to train agents interactively like they are actually playing
the game. The core of the algorithm is to let designers
seamlessly move between designing the level, providing game
demonstrations and getting feedback from trained testing
agents. Our approach allows real-time training as it requires
low training time and it is more sample-efﬁcient than the
baseline methods, as we see in Section IV. Designers can also
provide corrections to faulty agent behaviors, resulting in a
continuous and instantaneous feedback loop between designers
and agents. Providing corrections is as easy as taking the
controller back and playing the game one more time. Once
they are satisﬁed with the agents behavior, designers can
stop providing demonstrations and watch the agent validate
the game. Developers can then make any kind of design
changes and wait for agent feedback, without making any new
demonstrations.

State Space. For the approach to be effective it needs to be
as general as possible to adapt to many different game genres

Fig. 3: Overview of the neural network architecture used in
this work.

and scenarios that designers construct. Since 3D movements
often are crucial gameplay elements of any video games, we
focused on ﬁnding the best state representation taking this into
consideration. This approach, with minor observation tweak-
ing, will transfer well to similar, but contextually different
game modes.

For setting up a behavior, developers deﬁne a goal position
in the game environment. The spatial information of the agent
relative to the goal is composed of the R2 projections of the
agent-to-goal vector onto the XY and XZ planes as well
as their corresponding lengths normalized to the gameplay
area. We also include information about the agent indicating
whether it can jump, whether it is grounded or climbing as well
as any other auxiliary data which is expected to be relevant to
gameplay. The user also speciﬁes a list of entities and game
objects that the agent should be aware of, e.g. intermediate
goals, dynamic objects, enemies, and other assets that could be
used for achieving the ﬁnal goal. From these entities, the same
relative information is inferred as for the main goal position
relative to the agent. Lastly, the agent also has local perception.
A semantic map solution is used similar to the one found by
Sestini et al. [3] which is general and performant. We illustrate
an example of a semantic 3D occupancy map as input to the
networks in Figure 2. This map is a categorical discretization
of the space and elements around the agent, and each voxel in
the map carries a semantic integer value describing the type
of object at the corresponding game world position relative to
the agent. The size, resolution and shape of the semantic map
can be conﬁgured by the designers.

Neural Network. We build the neural network πθ with
parameters θ with the goal of being as general and reusable
as possible. First, all the information about the agent and goal
is passed into a linear layer producing the self-embedding
xa ∈ Rd, where d is the embedding size. The list of entities
is passed through a separate linear layer with shared weights
producing embeddings xei ∈ Rd, one for each entity ei in
the list. Each of these embedding vectors is concatenated

Fig. 4: Overview of the environment used in this study.

with the self-embedding, producing xaei = [xa, xei], with
xaei ∈ R2d. This list of vectors is then passed through a
transformer encoder with 4 heads and ﬁnal average pooling,
producing a single vector xt ∈ R2d. In parallel, the semantic
occupancy map M ∈ Rs×s×s is ﬁrst fed into an embedding
layer, transforming categorical representations into continuous
ones, and then into a 3D convolutional network. The output of
this convolutional network is a vector embedding xM ∈ Rd that
is ﬁnally concatenated with xt and passed through an MLP,
producing an action probability distribution. The complete
neural network architecture is shown in Figure 3. Given a set
of demonstrations:

D = {τi | τi = (si

0, ai

0, ..., si

T , ai

T ), i = 1, .., N },

we update the network following the objective:

arg max

θ

E(s,a)∼τ,τ ∼D[log πθ(a|s)].

(1)

This objective aims to mimic the expert behavior which is
represented by the dataset D.

IV. EXPERIMENTS

In order to analyze the usefulness of our proposed approach,
we conduct experiments demonstrating how IL can be uti-
lized in different game design use-cases. First, we deﬁne the
environment we use for our experiments, then we describe
quantitative results of our approach compared to RL.

A. Experimental Setup

Here we describe the environment and training setup used

in this work to test our proposed approach.

Environment. The environment used in this work is shown
in Figure 4. This is a 3D navigation environment with proce-
durally generated elements. The environment is purposefully
built like a mutable sandbox which can offer scenarios where
a designer’s level design workﬂow can be simulated. Users
can add and change the goal location, agent spawn positions,
layout of the level, location of intermediate goals and the
locations of dynamic elements in the map such as elevators.
In this environment, the agent has a set of 7 discrete actions:

Fig. 5: Screenshot of the “Validating Design Changes” use-
case.

move forward, move backward, turn right, turn left, jump,
shoot and do nothing. In addition, the agent can use some
interactable objects located around the map.

Training Setup. We compare our approach to two main
baselines: Simple-RL and Tuned-RL, both using the PPO
algorithm with identical hyperparameters [30]. Simple-RL uti-
lizes a naive reward function that gives a positive, progressive
reward based on the distance to the goal. For Tuned-RL, a
hand-crafted, dense reward function is used that is designed to
lead the agent along a path, reaching multiple sub-goals before
the ﬁnal main goal. We use the same setting for all subsequent
experiments. For each of the methods, we are interested in:
the success rate of the agent (i.e. how many times it reaches
the goal), training time (i.e. the time it takes to reach that
success rate), generalization success rate (i.e. the success rate
of the same agent in a different version of the environment),
and imitation metric (i.e. how close the trajectories made by
the agent are to the demonstrations). For the latter, we use
the 3D Frech´et distance between the agent trajectories and
the demonstrator trajectories. All training was performed on
a single machine with an NVIDIA GTX 1080 GPU with 8
GB VRAM, a 6-core Intel Xeon W-2133 CPU and 64 GB of
RAM.

B. Use-case Evaluation

In order to evaluate the performance of the approach, we
deﬁne a set of use-cases that resemble typical situations that
game designers face in their daily workﬂow. In Table I, we
report quantitative results for each use-case, comparing our
approach to the aforementioned RL baselines.

Use-case 1: Validating Design Changes. For the ﬁrst use-
case, we investigate if an IL agent in real-time can validate
the layout of a level. The goal in this experiment is to train
an agent to follow human demonstrations, then change the
level and see how the agent adapts to these changes. This will
give developers a glimpse into the usefulness of our proposed
approach when they are still in the design process: they can
see how players would adapt accordingly to modiﬁcations in

Fig. 6: Screenshot of the “Validating a Complex Trajectory”
use-case.

the level layout. In Figure 5, we show details of the use-case.
The agent is tasked to navigate to a goal hidden behind a door
that can be opened by interaction with a button. After training
the agent, we test it by removing and adding new elements to
the level.

We argue, with the support of Table I, that IL is more
suitable than RL for use-cases like this. IL is generally
signiﬁcantly faster than RL, and at the same time it gives the
user much more controllability over the ﬁnal agent behavior
as the agent can be guided via the demonstrations. As seen
in this table, we could improve controllability and training
time of the RL solution (e.g. Tuned-RL), but this requires
reward shaping that is a very difﬁcult task, especially for non-
experts in machine learning. Moreover, deﬁning a suitable
reward function requires many adjustment iterations to the
training setup that decreases the overall efﬁciency of the
system. Furthermore, it is evident from the table that IL gives
the same level of generalization as RL and it adapts easily to
slight variations of the same environment.

Use-case 2: Validating Complex Trajectories. For this use-
case, we train our agent to reproduce a complex trajectory
shown in Figure 6. The agent has multiple intermediate goals:

Use-case 1

Ours
Simple-RL 0.91 ± 0.02
Tuned-RL 0.95 ± 0.00

Success ↑
0.95 ± 0.00 0.02 h 0.96 ± 0.05
0.90 ± 0.00
5.00 h
0.90 ± 0.02
4.48 h

Time ↓ Generalization ↑ Imitation ↓

6.84 ± 0.34
8.37 ± 0.27
8.13 ± 0.20

Use-case 2

Time ↓ Generalization ↑ Imitation ↓
Success ↑
0.06 h
0.90 ± 0.02
Ours
-
Simple-RL 0.00 ± 0.00
18.18 h -
Tuned-RL 0.92 ± 0.03 13.56 h -

7.73 ± 1.10
28.11 ± 0.41
9.53 ± 1.37

Use-case 3

Success ↑
0.81 ± 0.08
Ours
Simple-RL 0.00 ± 0.00
Tuned-RL 0.86 ± 0.05 4.12 h

Time ↓ Generalization ↑ Imitation ↓
0.22 h 0.78 ± 0.03
16.49 h 0.00 ± 0.00
0.87 ± 0.03

46.07 ± 1.03
80.22 ± 0.53
16.79 ± 2.12

Fig. 7: Screenshot of the “Navigation” use-case.

use the elevator, interact with the button to create the bridge,
destroy a wall by shooting it and arrive at the goal location.
Here we are not interested in generalization, but only on the
ability to replicate the expert behavior as quickly as possible.
In Table I, we can see how our interactive approach is much
more suitable for our goals compared to the baselines. As the
table shows, it takes a lot of time for the RL agent to be trained,
which is problematic as efﬁciency is a key requirement. Even
the Tuned-RL baseline is much time consuming than our
approach. Moreover, the RL agent will exploit the environment
without taking into account the real intention of the designer.
An interesting ﬁnding here is that the RL agent has found a
different way to get to the goal location, which is not desirable
for this speciﬁc use-case but would be very useful for exploit
detection.

Use-case 3: Navigation. As previously mentioned, naviga-
tion is one of the fundamental aspects in modern video games.
However, the traditional scripted way to handle navigation
is to pre-bake and use navigation meshes in combination
with classical pathﬁnding algorithms. It is known that using
a navigation mesh becomes intractable in many practical
situations, and we thus must compromise between navigation
cost and quality. This is accomplished in practice by either
removing some of the navigation abilities or by pruning most
of the navigation mesh connections [31]. Moreover, every time
designers change something in the layout, the navigation mesh
needs to be regenerated. For this use-case, we consider a
complex navigation task where the agent has to navigate a
city that the designers have not completely ﬁnished yet. To
simulate this, we ﬁrst record demonstrations in an unﬁnished
city, and then we test the trained agents within a test city that
changes every 2 seconds until it reaches the goal location. A
screenshot of this use-case is visible in Figure 7. Similar to
the second use-case, we want the agent to always follow the
same path that is demonstrated.

Table I summarizes the results. Since the city environment
is large, training agents with RL would take a lot of time
compared to guiding them along the correct path with our
approach. Moreover, even if they enjoy a similar level of
generalization, the corresponding RL agents will try to exploit

TABLE I: Quantitative results of our experiments. We compare
our approach with two main baselines: Simple-RL, which
uses PPO algorithm [30] with a sparse reward function and
Tuned-RL which uses an hand-crafted and very dense reward
function. All the numbers refer to the mean and standard
deviation of 5 training runs. For a complete description of
the use-cases, see Section IV. Since in use-case 2 we are not
interested in generalization, we do not report the values for
that experiment.

and explore the environment ﬁnding different ways to arrive to
the goal location. For our purposes we not only need general-
ization, but also a certain level of imitation, controllability and
efﬁciency. The Tuned-RL baseline, however, achieves slightly
better results in this case. However, we must reiterate that
this baseline is infeasible to replicate by a game designer as
it needs a very speciﬁc reward function and many iterations.
Even then, the overall result is not very different from our
approach. As shown by the results from these experiments,
the combination of these elements is more easily achievable
with IL rather than RL.

With these analyses, we wanted to assess our hypothesis
detailed in Section III that data-driven modelling via IL is
the preferred solution for achieving interactive game design
and validation compared to the baselines. Next, we evaluate
our proposed approach using feedback from professional game
designers.

V. USER STUDY

We performed a user study in the form of an online
survey with professional game and level designers not only
to assess desirability of using our proposed approach, but also
to identify open opportunities for supporting automated game
validation.

A. Survey Description

Our methodology consists of qualitative data collection ap-
plying an online survey The data for the survey were collected
from professional game and level designers, recruited using
snowball sampling. Subjects were recruited among different
game studios of varying sizes, with different background
knowledge and workﬂows. Table II summarizes participant
details. The survey is composed of Likert questions with

YoE MLK
Genre(s)
ID Role
15
FPS
P01 Level Designer
11
FPS
P02 Level Designer
3
Racing
P03 Level Designer
6
RPG
P04 Level Designer
1
Racing
P05 Level Designer
4
RPG
P06 Level Designer
9
RPG
P07 Game Designer
3
Sport
P08 Game Designer
4
Sport
P09 Game Designer
1
Match3
P10 Game Designer
15
RPG
P11 Level Designer
21
P12 Level Designer
FPS
15
P13 Gameplay Designer RPG
RPG
P14 Game Designer
22
FPS, racing 10
P15 Level Designer
5
RPG
P16 Level Designer

Low
None
Very Low
High
Very Low
Very Low
Low
Very Low
Very Low
Very Low
Very Low
Low
Very Low
None
Very Low
Very Low

TABLE II: Summary of participants. Abbreviations: YoE
(years of experience), MLK (machine learning knowledge),
FPS (ﬁrst person shooter), RPG (role-playing game), TPS
(third person shooter).

some additional open questions. We also let participants add
whatever feedback they want to individual questions.

The survey was divided into four sections: the ﬁrst asks
participants of some background information; the second asks
them about their current game validation workﬂow, in partic-
ular if they use manual or automated playtesting; the third is
the main part of the survey, as it asks participants what they
think about our solution, if they would use it in their games,
what characteristics an agent/approach like this should have
to help them in their game and level design work, if they
think IL would help them create better games; and the fourth
contains optional questions about possible use-cases and future
directions they see that we had not considered.

Since one of the main focuses of this paper is to assess
what improvements and research are needed to maximize the
value of using such an approach, one important question in
section 3 of the survey asks participants to evaluate various
characteristics that an agent for automated content evaluation
should have. These characteristics are:

• Imitation: the agent can exactly replicate the demonstra-

tions;

• Generalization: the agent can adapt to different variations

of the same situation;

• Exploration: the agent can explore beyond the demon-

strations and ﬁnd bugs and issues;

• Personas: the agent can have different types of behaviors;
• Efﬁciency: the agent must use as few demonstrations as

possible;

• Controllability: having control over the agent behavior vs

complete autonomy;

• Feedback: the agent gives feedback when the amount of

demonstration data is enough;

• Fine tuning: behaviors can be ﬁne tuned after initial

training;

• Interpretability: the agent can inform me when and why

it fails.

B. Survey Results

We had a total of 16 accepted responses. The majority
(71.4%) of the participants have more than 5 years of experi-
ence in level design, with a median of 7.5 years. All the partic-
ipants were level, game or gameplay designers. We found that
the general machine learning knowledge of the participants is
from very low to low. 83.3% of the respondents do not use
automatic validation of their levels and they rely on external
people or systems for their level validation. 72.2% of designers
never rely on automated rather than manual playtesting and
only 38.9% have used at least one a scripted automated method
in their daily work. They work on different game projects and
game genres and they use different tools in their level design
workﬂow, but all respondents have knowledge of and use game
engine editors.

In Figure 8 are shown some of the most interesting results
we got from the survey. The general feeling is that designers
would very likely use a tool like this as “it deﬁnitely would
be useful to speed up the typically time consuming iterative
design process” and “a method like this can deﬁnitely speed up
iteration, which is one of the main things any designer spends
a lot of time”. Moreover, they can relate to the demonstrated
example as “it is not quite like how they are building their
levels, but it is not far off ” and “they think [the examples
shown are] realistic for some games such as platformers”.

Participants acknowledge the usefulness and the potential
of the demonstrated method, even if it will probably need
the proper adaptations for the speciﬁc game genre they are
working on. The 81.2% of the respondents agree that au-
tomated validation of the level directly in the game editor
is useful and 93.0% of them think the demonstrated agents
could be useful for assisting in their content validation. Many
respondents (41.1%) agree that a method like this adds value
compared to scripted agents and some of them (23.5%) think
it could completely replace scripting.

Some designers were skeptical about the complexity of the
examples shown by the survey and they would like to see what
happens in more complicated game mechanics. One participant
says that “[he is] wondering about how effective this would be
in genres that are more complicated mechanically”. This was
expected due to the preliminary nature of this study and that
we do not cover every genre but focus on certain gameplay.
However, a larger group (81.2%) though that complexity was
high enough which reinforces the notion that the environment
is relevant to a signiﬁcant part of the community. Designers
also describe the challenges of using such a tool in their daily
workﬂow: one argue that “human players play very differently
then bots, can be difﬁcult to only rely on AI when it comes
to testing” or “demonstrations take time, which would push
busy level designers to not use it. Demonstrating how to solve
speciﬁc issues (how to go around a car, or recover from a
mistake) should not be on the level designer”. Not all game
creators were fully convinced by the approach because “the
demonstration video showed a process that does not really
show a level designer anything new or unexpected”.

Generalization.
From the survey, it is clear that one of
the most important request is to have an agent that not only
imitates the expert behavior, but that is more representative
of the unpredictable nature of the players. Moreover, “demon-
strating how to solve speciﬁc issues should not be on the level
designer”. Humans can easily understand, given a previous
task demonstration, how to adapt to slight changes in the
environment or to recover from faulty situations which in
comparison is difﬁcult for machine learning models. With our
proposed general purpose neural network and state space we
mitigate this problem, but in many cases agents trained under
one set of demonstrations are not able to adapt to slightly
different situations [32]. Generalization as a subject in self-
learning agents has already been addressed in the literature,
but the state-of-the-art approaches are not readily applicable
for our purpose: most of them use either interactions with the
environment [23, 33, 34] or learn the inverse dynamic of the
agent [35]. Moreover, a recent study by Xu et al. [36] proves
that adversarial imitation learning algorithms do not generalize
more than standard behavioral cloning in some domains. A
possible future direction is to try data augmentation techniques
used in ofﬂine reinforcement
learning algorithms such as
the work done by [37]. Ofﬂine reinforcement learning has
several connections to behavioral cloning [38]. It learns a
policy using a pre-deﬁned dataset without direct interactions
with the environment, but in contrast with imitation learning
that supposes the data comes from an optimal policy, ofﬂine
reinforcement learning relies also to sub-optimal examples.
This lets trained agents to be more general and allows a
higher exploration level. With all this considered, we argue that
generalization is far from solved for this use case. Therefore,
we conclude that research in this direction is one of the more
effective way to improve the results of this approach.

Personas.
One feature requested recurrently by survey
participants is the possibility to train the model for different
behavior types, or so called personas. The aim is to have
various behaviors similar to the multi-modal nature of how
humans play,
in order to create more meaningful agents.
Research community has addressed the problem of creating
different personas many times, but exclusively in RL contexts.
Works from de Woillemont et al. [39], Roy et al. [40] and
Sestini et al. [41] have studied and explored how to learn
and combine different behavior styles with different reward
functions. This makes it challenging to train different styles
with only IL. The work by Peng et al. [25] is an example
on how to combine IL and RL to create different animation
styles. More research into training different playstyles with IL
only would allow designers to train more meaningful testing
agents to validate different gameplay.

At

Exploration.
the same time participants frequently
brought up the exploration aspect, i.e. the ability for the agent
to look beyond expert demonstrations in search for bugs and
overlooked issues. Exploration is a well known problem in
RL literature [42], but exploration in IL is, to our knowledge,
to a large extent a unexplored ﬁeld. This is mainly due to the

Fig. 8: Boxplot of some of the most important questions of the
survey. The boxes represent the area between the lower quartile
and the upper quartile. The red lines represent the median of
the values, while the whiskers represent the minimum and
maximum of the distributions, except for the outliers that are
visualized with black dots.

After describing their doubts, we asked participants to
evaluate each of the characteristics delineated in the previous
section and which are the most important ones to improve
the approach. Table III refers to the results. We claim that
these results bring much value to our research: not only do we
know that our initial hypothesis is supported by professional
designers, but we also know what they really want and why
they are skeptical of using such an approach. This is important
because, as we will see in Section VI, the values in Table
III form very precise research directions that we encourage
all of the game research community to consider in future
contributions to the video game industry.

VI. FUTURE WORK

Results in section V-B, and in particular Table III, show-
cased challenges of using IL as a game validation tool. Here
we propose research opportunities and future work that we
gained from interacting and interviewing experts in the ﬁeld.
With this we want to lower the bar for researcher who would
like to contribute and improve said approach, as this would
help drive the research, and therefore industry, forward.

conﬂict nature of an agent that must both learn to follow more
or less precisely the expert demonstrations as well as explore
beyond the optimal behavior. Moreover, we would not use
exploration to improve agents, but we would like to exploit
exploration to ﬁnd bugs. Sestini et al. [3] provide a good
example on how to leverage both IL and RL to train agents
to both follow demonstrated behaviors but also to explore in
search for overlooked issues. However, this type of solution
is still sample inefﬁcient to be used as an active game design
tool. Research in this direction would likely help the utility of
such tool for game design validation.

is one of the most

The usability aspect

Usability.
im-
portant for participants. This includes both providing useful
information to designers about the models they are training,
but also the ease-of-use of the tool. Recent techniques from
the explainable AI [43] research community applied to RL
could be used to address the challenge of interpreting and
explaining behaviors of the agent. One can use techniques
from game analytics research and gameplay visualization
techniques [3, 44]. At the same time, a sentiment found in
the survey is that to be a usable tool there is a need to
minimize the effort of the end-user. This includes getting
feedback (e.g. demonstration samples needed, when to stop
producing demonstrations), and more direct improvements to
use as few demonstration samples as possible. To address the
latter, few- and one-shot imitation learning is a recent active
topic that potentially can help to reduce samples required.
Works from Duan et al. [45] and Hakhamaneshi et al. [46]
speciﬁcally address the problem within IL, mainly exploiting
meta-learning techniques [47]. However, these state-of-the-art
approaches require many preliminary training iterations and it
is unclear how to make this in a game that still is not readily
stable and not yet ﬁnished. One way to both improve agents
quality and the usability of the tool is to leverage the already
cited ofﬂine reinforcement learning techniques [37, 48, 49].
We can leverage the recorder to not only store demonstrations,
but also all the other interactions that the agent does while
testing the level. In this way we can both improve the agents,
be more sample efﬁcient and improve generalization without
the need to directly use the environment.

Multi-Agent. Many designers noted that most of mod-
ern video games are multi-agent systems, and in order to
thoroughly test these environments we need to train multiple
interacting agents. However, most of the current research in
IL focuses on single agents learning from a single teacher.
Few examples of multi-agent imitation learning are: Harmer
et al. [27] use IL in a multi-agent game, but they do not
really address the problem of a multi-agent system, while
Le et al. [50] propose a joint approach that simultaneously
learns a latent coordination model along with the individual
policies. We believe there is still a long way to go for multi-
agent imitation learning, especially for this use-case, and we
encourage researchers to follow this research direction.

Mean Median Std
1.63
4.00
Imitation
4.56
0.73
7.00
Generalization 6.50
0.40
7.00
6.81
Exploration
1.31
6.00
5.87
Personas
1.21
5.50
Efﬁciency
5.50
1.68
4.00
Controllability 4.18
0.96
6.00
6.00
Feedback
1.50
6.00
Fine tuning
5.62
1.03
7.00
Interpretability 6.50

TABLE III: Results of characteristics questions. Participants
could give a value between [1, 7] to each of the category, where
1 means “not so important” and 7 means “very important”.

VII. CONCLUSION

The use of machine learning has recently gaining attention
in game industry for creating better quality games. However,
many works for creating autonomous agents from the literature
do not have an easily interpretable translation into an actual
design tool as they need to satisfy certain requirements that
usually the research community tends not to address.

In this paper, we ﬁrst claimed that data-driven programming
via imitation learning is a suitable approach for real-time
validation of game and level design. We proposed an imitation
learning approach and we investigated it focusing on three
different design validation use-cases. We demonstrated how
this type of approach can satisfy many of the requirements for
being an effective game design tool in comparison to simple
reinforcement learning and model-based scripted behaviors.
We also performed a user study with professional game and
level designers from different, and in many cases, disparate
game studios and game genres. We asked participants to assess
the desirability and opportunities of using such an approach
in their daily workﬂow. Moreover, we asked designers what
characteristics they would want from a data-driven tool for
creating autonomous agents that validate their design.

The user study showcased the desire of designers to have an
automated way to test and validate their games. Along with our
preliminary results we proved that the data-driven approach we
propose is a potential candidate for achieving such objectives.
However, the study also highlighted challenges and the gap
that exists between techniques from the literature and their
actual use in the game industry. For this reason, we proposed a
series of research directions that will help such an approach to
move from an impractical tool to an effective game design tool.
We expect that our recommendations will foster game research
community to contribute to or expand on this research.

REFERENCES
[1] S. Stahlke, A. Nova, and P. Mirza-Babaei, “Artiﬁcial players in the
design process: Developing an automated testing tool for game level and
world design,” in Proceedings of the Annual Symposium on Computer-
Human Interaction in Play, 2020, pp. 267–280.

[2] C. Gordillo, J. Bergdahl, K. Tollmar, and L. Gissl´en, “Improving
playtesting coverage via curiosity driven reinforcement learning agents,”
in 2021 IEEE Conference on Games (CoG), 2021, pp. 1–8.

[3] A. Sestini, L. Gissl´en, J. Bergdahl, K. Tollmar, and A. D. Bagdanov,
“CCPT: Automatic gameplay testing and validation with curiosity-
trajectories,” arXiv preprint arXiv:2202.10057,
conditioned proximal
2022.

[4] L. Gissl´en, A. Eakins, C. Gordillo, J. Bergdahl, and K. Tollmar,
“Adversarial reinforcement learning for procedural content generation,”
in 2021 IEEE Conference on Games (CoG), 2021, pp. 1–8.
(2016) Faulty reward functions in the wild.

[5] Open AI.

[Online].

Available: https://openai.com/blog/faulty-reward-functions/

[6] S. Ross, G. Gordon, and D. Bagnell, “A reduction of imitation learning
and structured prediction to no-regret online learning,” in 2011 Interna-
tional Conference on Artiﬁcial Intelligence and Statistics (ICAIS), 2011.
[7] M. Jacob, S. Devlin, and K. Hofmann, “It’s unwieldy and it takes
a lot of time. Challenges and opportunities for creating agents in
commercial games,” in 16th AAAI Conference on Artiﬁcial Intelligence
and Interactive Digital Entertainment, 2020.

[8] C. Holmg˚ard, M. C. Green, A. Liapis, and J. Togelius, “Automated
playtesting with procedural personas through MCTS with evolved
heuristics,” IEEE Transactions on Games, vol. 11, no. 4, pp. 352–362,
2018.

[9] K. Chang, B. Aytemiz, and A. M. Smith, “Reveal-more: Amplifying
human effort in quality assurance testing using automated exploration,”
in 2019 IEEE Conference on Games (CoG).

IEEE, 2019, pp. 1–8.

[10] L. Mugrai, F. Silva, C. Holmg˚ard, and J. Togelius, “Automated playtest-
ing of matching tile games,” in 2019 IEEE Conference on Games (CoG).
IEEE, 2019, pp. 1–7.

[11] S. Iftikhar, M. Z. Iqbal, M. U. Khan, and W. Mahmood, “An automated
model based testing approach for platform games,” in 2015 ACM/IEEE
18th International Conference on Model Driven Engineering Languages
and Systems (MODELS).

IEEE, 2015.
[12] G. Lovreto, A. T. Endo, P. Nardi, and V. H. Durelli, “Automated tests for
mobile games: An experience report,” in 17th Brazilian Symposium on
Computer Games and Digital Entertainment (SBGames).
IEEE, 2018.
[13] S. Stahlke, A. Nova, and P. Mirza-Babaei, “Artiﬁcial playfulness: A tool

for automated agent-based playtesting,” 2019.

[14] C. Schaefer, H. Do, and B. M. Slator, “Crushinator: A framework to-
wards game-independent testing,” in 2013 28th IEEE/ACM International
Conference on Automated Software Engineering (ASE).
IEEE, 2013.
[15] C.-S. Cho, K.-M. Sohn, C.-J. Park, and J.-H. Kang, “Online game testing
using scenario-based control of massive virtual users,” in 2010 The
12th International Conference on Advanced Communication Technology
(ICACT).

IEEE, 2010.

[16] G. Xiao, F. Southey, R. C. Holte, and D. Wilkinson, “Software testing
by active learning for commercial games,” in AAAI, 2005, pp. 898–903.
[17] C. Politowski, Y.-G. Gu´eh´eneuc, and F. Petrillo, “Towards auto-
mated video game testing: Still a long way to go,” arXiv preprint
arXiv:2202.12777, 2022.

[18] Y. Zheng, X. Xie, T. Su, L. Ma, J. Hao, Z. Meng, Y. Liu, R. Shen,
Y. Chen, and C. Fan, “Wuji: Automatic online combat game testing
using evolutionary deep reinforcement learning,” in 34th IEEE/ACM
International Conference on Automated Software Engineering, 2019.

[19] S. Agarwal, C. Herrmann, G. Wallner, and F. Beck, “Visualizing AI
playtesting data of 2D side-scrolling games,” in 2020 IEEE Conference
on Games (CoG), 2020, pp. 572–575.

[20] J. Bergdahl, C. Gordillo, K. Tollmar, and L. Gissl´en, “Augmenting
automated game testing with deep reinforcement learning,” in IEEE
Conference on Games (CoG), 2020.

[21] M. Bain and C. Sammut, “A framework for behavioural cloning.” in

Machine Intelligence 15, 1995, pp. 103–129.

the TAMER framework,” in Proceedings of

[22] W. B. Knox and P. Stone, “Interactively shaping agents via human
the 5th

reinforcement:
international conference on Knowledge capture, 2009, pp. 9–16.
[23] J. Ho and S. Ermon, “Generative adversarial imitation learning,” in
Proceedings of the 30st International Conference on Neural Information
Processing Systems (NIPS), 2016, pp. 4565–4573.

[24] C. Finn, P. Christiano, P. Abbeel, and S. Levine, “A connection between
learning, and

generative adversarial networks,
energy-based models,” arXiv preprint arXiv:1611.03852, 2016.

inverse reinforcement

[25] X. B. Peng, Z. Ma, P. Abbeel, S. Levine, and A. Kanazawa,
“AMP: adversarial motion priors for stylized physics-based character
control,” ACM Trans. Graph., vol. 40, no. 4, 2021. [Online]. Available:
http://doi.acm.org/10.1145/3450626.3459670

[26] Y. Zhao, I. Borovikov, F. D. M. Silva, A. Beirami, J. Rupert, C. Somers,
J. Harder, J. Kolen, J. Pinto, R. Pourabolghasem et al., “Winning isn’t
everything: Enhancing game development with intelligent agents,” IEEE
Transactions on Games, 2020.

[27] J. Harmer, L. Gissl´en, J. del Val, H. Holst, J. Bergdahl, T. Olsson,
K. Sj¨o¨o, and M. Nordin, “Imitation learning with concurrent actions

in 3d games,” in 2018 IEEE Conference on Computational Intelligence
and Games (CIG).

IEEE, 2018.

[28] A. Tucker, A. Gleave, and S. Russell, “Inverse reinforcement learning for
video games,” in Proceedings of NIPS Workshop on Deep Reinforcement
Learning, 2018.

[29] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling, “The arcade
learning environment: An evaluation platform for general agents,” Jour-
nal of Artiﬁcial Intelligence Research, vol. 47, pp. 253–279, 2013.
[30] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Prox-
imal policy optimization algorithms,” arXiv preprint arXiv:1707.06347,
2017.

[31] E. Alonso, M. Peter, D. Goumard, and J. Romoff, “Deep reinforcement
learning for navigation in AAA video games,” in Proceedings of
the 30th International Joint Conference on Artiﬁcial
Intelligence
(IJCAI-21), Z.-H. Zhou, Ed., 2021, pp. 2133–2139. [Online]. Available:
https://doi.org/10.24963/ijcai.2021/294

[32] S. Huang, N. Papernot,

I. Goodfellow, Y. Duan, and P. Abbeel,
“Adversarial attacks on neural network policies,” arXiv preprint
arXiv:1702.02284, 2017.

[33] J. Fu, K. Luo, and S. Levine, “Learning robust rewards with adverserial
inverse reinforcement learning,” in International Conference on Learn-
ing Representations, 2018.

[34] F. Torabi, G. Warnell, and P. Stone, “Behavioral cloning from obser-
vation,” in Proceedings of the 27th International Joint Conference on
Artiﬁcial Intelligence, 2018, pp. 4950–4957.

[35] J. Monteiro, N. Gavenski, R. Granada, F. Meneguzzi, and R. Barros,
“Augmented behavioral cloning from observation,” in 2020 International
Joint Conference on Neural Networks (IJCNN).
IEEE, 2020, pp. 1–8.
[36] T. Xu, Z. Li, and Y. Yu, “More efﬁcient adversarial imitation learn-
ing algorithms with known and unknown transitions,” arXiv preprint
arXiv:2106.10424, 2021.

[37] S. Sinha, A. Mandlekar, and A. Garg, “S4rl: Surprisingly simple self-
supervision for ofﬂine reinforcement learning in robotics,” in Conference
on Robot Learning. PMLR, 2022, pp. 907–917.

[38] S. Fujimoto and S. S. Gu, “A minimalist approach to ofﬂine reinforce-
ment learning,” Advances in neural information processing systems,
vol. 34, pp. 20 132–20 145, 2021.

[39] P. L. P. de Woillemont, R. Labory, and V. Corruble, “Conﬁgurable agent
with reward as input: a play-style continuum generation,” in 2021 IEEE
Conference on Games (CoG).

IEEE, 2021, pp. 1–8.

[40] J. Roy, R. Girgis, J. Romoff, P.-L. Bacon, and C. Pal, “Direct behavior
learning,” arXiv preprint

speciﬁcation via constrained reinforcement
arXiv:2112.12228, 2021.

[41] A. Sestini, A. Kuhnle, and A. D. Bagdanov, “Policy fusion for adaptive
and customizable reinforcement learning agents,” in 2021 IEEE Confer-
ence on Games (CoG).

IEEE, 2021, pp. 01–08.

[42] Y. Burda, H. Edwards, A. Storkey, and O. Klimov, “Exploration by
random network distillation,” in International Conference on Learning
Representations (ICLR), 2018.

[43] J. Druce, M. Harradon, and J. Tittle, “Explainable artiﬁcial intelligence
(XAI) for increasing user trust in deep reinforcement learning driven
autonomous systems,” arXiv preprint arXiv:2106.03775, 2021.

[44] G. Wallner and S. Kriglstein, “Visualization-based analysis of gameplay
data–a review of literature,” Entertainment Computing, vol. 4, no. 3, pp.
143–155, 2013.

[45] Y. Duan, M. Andrychowicz, B. Stadie, O. Jonathan Ho, J. Schneider,
I. Sutskever, P. Abbeel, and W. Zaremba, “One-shot imitation learning,”
Advances in neural information processing systems, vol. 30, 2017.
[46] K. Hakhamaneshi, R. Zhao, A. Zhan, P. Abbeel, and M. Laskin,
“Hierarchical few-shot imitation with skill transition models,” arXiv
preprint arXiv:2107.08981, 2021.

[47] C. Finn, P. Abbeel, and S. Levine, “Model-agnostic meta-learning
for fast adaptation of deep networks,” in International conference on
machine learning. PMLR, 2017, pp. 1126–1135.

[48] A. Kumar, A. Zhou, G. Tucker, and S. Levine, “Conservative Q-learning
for ofﬂine reinforcement learning,” Advances in Neural Information
Processing Systems, vol. 33, pp. 1179–1191, 2020.

[49] L. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin, P. Abbeel,
A. Srinivas, and I. Mordatch, “Decision transformer: Reinforcement
learning via sequence modeling,” Advances in neural information pro-
cessing systems, vol. 34, pp. 15 084–15 097, 2021.

[50] H. M. Le, Y. Yue, P. Carr, and P. Lucey, “Coordinated multi-agent
imitation learning,” in International Conference on Machine Learning.
PMLR, 2017, pp. 1995–2003.

