2
2
0
2

n
a
J

5
2

]
I

A
.
s
c
[

2
v
2
0
7
3
0
.
1
0
2
2
:
v
i
X
r
a

Learning Logic Programs
From Noisy Failures

John Wahlig

St Hilda’s College
Supervised by Dr Andrew Cropper

University of Oxford

A thesis submitted for the degree of

Master of Science Computer Science

Trinity 2021

 
 
 
 
 
 
Acknowledgements

I ﬁrst would like to thank Andrew Cropper for his incredible guidance

while supervising this project. His insight and expertise was immeasur-

able. I would next like to thank Rolf Morel for all of his help, advice, and

morale boosting conversations. I would also like to thank Brad Hunter
for his insightful discussions and for the pleasure of working beside him. I

give utmost thanks to my parents for aﬀording me this incredible oppor-

tunity and for their endless support and encouragement. Nothing I have

achieved would be possible without their generosity and sacriﬁce. I would
lastly like to thank Stephen, Brian, and Abby for inspiring me everyday

to be the best that I can be and Alba for just about everything.

Abstract

Inductive Logic Programming (ILP) is a form of machine learning (ML)

which in contrast to many other state of the art ML methods typically

produces highly interpretable and reusable models. However, many ILP

systems lack the ability to naturally learn from any noisy or partially
missclassiﬁed training data. We introduce the relaxed learning from fail-

ures approach to ILP, a noise handling modiﬁcation of the previously in-

troduced learning from failures (LFF) approach [13] which is incapable of

handling noise. We additionally introduce the novel Noisy Popper ILP sys-
tem which implements this relaxed approach and is a modiﬁcation of the

existing Popper system [13]. Like Popper, Noisy Popper takes a generate-

test-constrain loop to search its hypothesis space wherein failed hypothe-

ses are used to construct hypothesis constraints. These constraints are
used to prune the hypothesis space, making the hypothesis search more

eﬃcient. However, in the relaxed setting, constraints are generated in a

more lax fashion as to avoid allowing noisy training data to lead to hypoth-

esis constraints which prune optimal hypotheses. Constraints unique to
the relaxed setting are generated via hypothesis comparison. Additional

constraints are generated by weighing the accuracy of hypotheses against

their sizes to avoid overﬁtting through an application of the minimum de-

scription length. We support this new setting through theoretical proofs
as well as experimental results which suggest that Noisy Popper improves

the noise handling capabilities of Popper but at the cost of overall runtime

eﬃciency.

Contents

1 Introduction

1.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.2 Contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.3 Paper Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 Related Work

2.1 No Noise Handling . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.2 Set Covering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.3 Sampling Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.4 Branch and Bound . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.5 Neural Networks

2.6 Applications to Popper . . . . . . . . . . . . . . . . . . . . . . . . . .

3 Learning from Failures (LFF) Framework

3.1 Logic Programming Preliminaries . . . . . . . . . . . . . . . . . . . .
3.1.1 First Order Logic . . . . . . . . . . . . . . . . . . . . . . . . .

3.2 LFF Problem Setting . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.2.1 Declaration Bias

. . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . .
3.2.2 Hypothesis Contraints
3.2.3 Problem Setting . . . . . . . . . . . . . . . . . . . . . . . . . .

3.2.4 Generalizations and Specializations . . . . . . . . . . . . . . .

3.3 Hypothesis Constraints . . . . . . . . . . . . . . . . . . . . . . . . . .

3.3.1 Generalization Constraints . . . . . . . . . . . . . . . . . . . .
Specialization Constraints . . . . . . . . . . . . . . . . . . . .
3.3.2

3.3.3 Elimination Constraints

. . . . . . . . . . . . . . . . . . . . .

3.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1
1

4

5

7
7

8

9

10
10

11

12

12
12

14

15

16
18

20

21

22
23

24

25

i

CONTENTS

4 Relaxed LFF Framework

4.1 Relaxed LFF Problem Setting . . . . . . . . . . . . . . . . . . . . . .

4.2 Relaxed LFF Hypothesis Constraints . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .

4.2.1 Hypothesis Constraints Applications

4.3 Relaxed LFF Hypothesis Constraints with Hypothesis Size . . . . . .

4.3.1 Hypothesis Constraints with Hypothesis Size Applications

. .

4.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5 Noisy Popper Implementation

5.1 Normal Popper Implementation . . . . . . . . . . . . . . . . . . . . .

5.1.1 Hypothesis to ASP Encoding . . . . . . . . . . . . . . . . . .

5.1.2 Generalization Constraints . . . . . . . . . . . . . . . . . . . .
Specialization Constraints . . . . . . . . . . . . . . . . . . . .
5.1.3

5.1.4 Elimination Constraints

. . . . . . . . . . . . . . . . . . . . .

5.1.5 Banish Constraints . . . . . . . . . . . . . . . . . . . . . . . .

5.1.6 Normal Popper Worked Example . . . . . . . . . . . . . . . .
5.2 Anytime Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.3 Minimal Constraints

. . . . . . . . . . . . . . . . . . . . . . . . . . .

5.4 Sound Hypothesis Constraints . . . . . . . . . . . . . . . . . . . . . .

5.5 Sound Constraints with Hypothesis Size
. . . . . . . . . . . . . . . .
5.6 Noisy Popper Worked Example . . . . . . . . . . . . . . . . . . . . .

5.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6 Experimental Results

6.1 Noisy Popper vs. Normal Popper

. . . . . . . . . . . . . . . . . . . .
6.1.1 Experiment 1: East-West Trains . . . . . . . . . . . . . . . . .

6.1.2 Experiment 2: List Manipulations . . . . . . . . . . . . . . . .

6.1.3 Experiment 3: IGGP Problems

. . . . . . . . . . . . . . . . .

6.2 Noisy Popper Enhancements . . . . . . . . . . . . . . . . . . . . . . .
6.2.1 Experiment 1: East-West Trains . . . . . . . . . . . . . . . . .

6.2.2 Experiment 2: List Manipulations . . . . . . . . . . . . . . . .

6.2.3 Experiment 3: IGGP Problems

. . . . . . . . . . . . . . . . .

6.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7 Conclusions

7.1 Summary and Evaluation . . . . . . . . . . . . . . . . . . . . . . . .

7.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

ii

26

26

28
35

36

40

41

42

42

44

46
47

48

49

50
52

54

55

57
60

63

65

66
66

69

71

75
76

77

82

82

84

84

85

CONTENTS

Bibliography

iii

87

List of Figures

1.1 Michalski’s original east-west trains problem . . . . . . . . . . . . . .

3

6.1 East-West Trains predictive accuracy and learning time (in seconds) for
program h1 when varying percentage of noisy training data. Standard
error is depicted by bars. . . . . . . . . . . . . . . . . . . . . . . . . .

6.2 East-West Trains predictive accuracy and learning time (in seconds) for
program h2 when varying percentage of noisy training data. Standard
error is depicted by bars. . . . . . . . . . . . . . . . . . . . . . . . . .

68

69

6.3

IGGP Minimal Decay task predictive accuracy and time when varying

percentage of noisy training data. Standard error is depicted by bars.

75

6.4

IGGP RPS task predictive accuracy and time when varying percentage
. . . . . .
of noisy training data. Standard error is depicted by bars.

6.5 East-West Trains predictive accuracies and learning times of Noisy
Popper variants (in seconds) for program h1 when varying percentage
. . . . . .
of noisy training data. Standard error is depicted by bars.
6.6 East-West Trains predictive accuracies and learning times of Noisy
Popper variants (in seconds) for program h2 when varying percentage
. . . . . .
of noisy training data. Standard error is depicted by bars.

6.7 Predictive accuracies of maintained best programs for Noisy Popper
variants versus the number of programs generated by each system on

75

77

78

evens dataset with 5% training noise. Standard error is depicted by bars. 79

6.8

IGGP minimal decay task predictive accuracy and time of Noisy Pop-

per variants (in seconds) for when varying percentage of noisy training
. . . . . . . . . . . . . . . .
data. Standard error is depicted by bars.

82

6.9

IGGP rps task predictive accuracy and time of Noisy Popper variants

(in seconds) when varying percentage of noisy training data. Standard

error is depicted by bars. . . . . . . . . . . . . . . . . . . . . . . . . .

83

iv

Chapter 1

Introduction

1.1 Motivation

A major goal for AI is to achieve human-like intelligence through the imitation of our
cognitive abilities [23]. To this end, AI systems often aim to mimic our automatic

inductive capacity in which previous (background) knowledge and prior observations

are used to infer upon new observations [30] - a complex task having applications in

numerous domains such as image classiﬁcation [16] and autonomous navigation [5].
Notably, humans have the innate ability to ﬁlter out outlying or incorrect observa-

tions, naturally and accurately handling noisy data or incomplete sets of background

knowledge. Many machine learning (ML) systems have been implemented to achieve

this inductive behavior, capable of identifying patterns among millions of often noisy
datapoints. However, traditional ML methods such as neural networks are typically

incapable of expressing their models in forms which are easily comprehensible to

humans. Additionally, the knowledge learned by many of these systems lacks trans-

ferability and cannot be applied to similar problems. For example, an AI system such
as AlphaGo [44] which has learned to eﬀectively play the game of Go on a standard

19 × 19 size board may struggle greatly on a board of diﬀerent size. Without com-

prehensibility and transferability, these systems fail to achieve true levels of human

cognition [23, 31]. Inductive Logic Programming [32] however has been an approach
more capable of meeting these additional requirements.

Inductive logic programming (ILP) is a form of ML wherein a logic program which

deﬁnes a target predicate is learned given positive and negative examples of the

predicate and background knowledge (BK). The target predicate, BK, and examples
are represented as logical statements, typically as logic programs in a language such as

Prolog whose notation we will use throughout this paper. More precisely, BK deﬁnes

1

CHAPTER 1.

INTRODUCTION

2

predicates and ground-truth atoms that the system may use to deﬁne the target

predicate program. The aim of the system is to learn a program (or hypothesis)

that correctly generalizes as many examples as possible, i.e., entails as many positive
examples as possible and does not entail as many negative examples as possible.

Example 1 (ILP Problem) Consider Michalski’s trains problem consisting of 5

trains moving eastbound and 5 moving westbound. We will use this example through-
out the paper and Figure 1.1 below visually depicts the original problem. Each train

is comprised of a locomotive pulling a variable number of cars, each with distinct

characteristics such as length, number of wheels, shape, number of cargo loads, shape

of loads, etc. The goal of the problem is to identify a set of one or more rules which
distinguishes the eastbound trains from the westbound. For instance, a solution to

the original problem shown in Figure 1.1 would be the following rule: if a train has

a car which is short, has two wheels, and its roof is closed, then it is eastbound and

otherwise it is westbound. This problem is easily described in an ILP setting by
letting one set of trains, say eastbound, represent positive examples and westbound

trains represent negative examples. BK here deﬁnes each train and its characteristics

with logical predicates for length, number of wheels, shape, etc. Hypotheses to these

problems can be easily described using these logical predicates. For example, the rule
above would be written as:

eastbound(A) :- has car(A,B), short(B), two wheels(B), roof closed(B).

Here, the hypothesis is claiming that any eastbound train A must have a car B and
B must be short, have two wheels, and its roof must be closed. While most modern
ILP systems are able to eﬀectively learn classiﬁers to solve the original trains prob-

lem, more complicated variations have been used to compare system capabilities and

predictive accuracies.

A strong motivation behind ILP methods is the high level of comprehensibility pos-

sessed by logic programs that they return when compared to traditional ML models.

Programs described by programming languages possess semantics interpretable by

both humans and computers. This falls in line with Michie’s [28] strong criterion
for machine learning and highly explainable AI whereas traditional ML methods of-

ten focus solely on improving the weak criterion, i.e., predictive accuracy. Work in

this area of ultra-strong machine learning [36] has demonstrated how human under-

standing of a system can be improved by better understanding the program, thus

CHAPTER 1.

INTRODUCTION

3

Figure 1.1: Michalski’s original east-west trains problem

motivating this desire for comprehensibility. The symbolic nature of the logic pro-
grams not only increases their comprehensibility but also allows for ease of lifelong

learning and knowledge transfer [46, 27, 9, 10], an essential criteria for human-like AI.

Models can be easily reused and complex models can be built up from solving smaller

problems whose logic programs are simply placed in the BK. ILP also constitutes a
form of code synthesis having applications in code completion, automated program-

ming, and novel program invention. Additionally, unlike many traditional machine

learning approaches, ILP can generalize exceptionally well with just a few examples

[11].

At it’s core, the problem of ILP is eﬃciently searching the set of all possible hy-

potheses, the hypothesis space, which is typically very large. Current ILP systems

take many approaches to this problem including but not limited to set covering tech-

niques [40, 33, 1, 4, 45], meta-level encodings [14, 35, 20], and neural networks [17],
each with various tradeoﬀs. Simultaneously, as most machine learning methods must,

these systems often navigate the issue of noisy datasets (i.e., misclassiﬁed examples).

While some systems such as TILDE [4], Progol [33], and Aleph [45] are naturally

built to withstand noise to varying degrees, they struggle with building recursive pro-
grams, often vital for constructing optimal solutions. Others such as ILASP [25] and

Metagol [35] are inherently incapable of generalizing with noisy data - noise being

commonly ignored by ILP systems in exchange for soundness or optimality. However

in exchange, ILASP and Metagol are both capable of generating recursive programs

CHAPTER 1.

INTRODUCTION

4

and possess varying levels of predicate invention wherein new predicates are created

by the system and deﬁned using BK. These both are useful in constructing compact

and often optimal solutions. Handling noise is a fundamental issue in machine learn-
ing as real-world data is typically endowed with misclassiﬁcations and human-errors.

As such, machine learning approaches should be able to handle this noise as well as

possible, though this problem is never trivial to solve.

Popper Popper [13] is an ILP system which takes a unique approach of learning

from failures (LFF). Popper uses a three stage loop: generate, test, and constrain.

However, unlike other three stage approaches [24], Popper uses theta-subsumption

[38] in conjunction with failed hypotheses to constraint the hypothesis space. Rather
than building up a solution to the problem, Popper essentially narrows down the

possible solutions signiﬁcantly enough that the hypothesis space can be eﬃciently

searched.

In the generate stage, Popper generates a program or hypothesis from the hypoth-

esis space which satisﬁes a set of hypothesis constraints and subsequently tests the

hypothesis against training examples in the test stage. Successful hypotheses are re-
turn as solution programs while failed ones are used to generate further hypothesis

constraints for subsequent iterations.

While Popper’s approach has notable strengths over some existing techniques such

as ease of generating recursive programs, it is completely unable to handle noise.

Programs generated by Popper necessarily entail all positive examples and no negative

ones, clearly overﬁtting in the presence of any missclassiﬁed data. It is the objective
of this project to modify Popper’s approach to allow it to generalize better to noisy

datasets without compromising its overall performance capabilities.

1.2 Contribution

The main contribution of this project is an extension to Popper which can handle

noise in exchange for the optimality guarantees of the system. For simplicity, this

paper will refer to the original version of Popper as Normal Popper and the novel noise
handling version of Popper as Noisy Popper. The main hypotheses of this project are:

• Noisy Popper typically generalizes better than Normal Popper with noisy datasets,
being able to return more accurate hypothesis as solutions than Normal Popper

which may be incapable of returning any program at all.

CHAPTER 1.

INTRODUCTION

5

• Noisy Popper does not lose out in ability to generalize well with noiseless
datasets, however, it performs less eﬃciently than Normal Popper in these en-

vironments.

Noise Handling Approach Noisy Popper makes several modiﬁcations and algo-

rithmic changes to Normal Popper to allow it to better generalize to noise. These

changes are as follows:

• The ﬁrst contribution is to alter the LFF framework to one which handles
noise. This altered setup is called the Relaxed LFF Framework and it relaxes

deﬁnitions for solutions and optimally thus also changing how the strictly the
hypothesis space is searched.

• The second contribution is to introduce theoretically sound constraints in this
framework. These sound constraints are used to prune suboptimal hypotheses

under the new setting. Some of these use the minimum description length

principle to help the system avoid hypotheses which overﬁt the data. These

constraints are described and proved in Propositions 4-14 in Chapter 4.

• The third contribution is Noisy Popper, which implements this relaxed LFF
framework in addition to some enhancements to improve noise handling capacity

including any anytime algorithm approach and eﬃcient minimal constraints.

• The ﬁnal contribution is an experimental evaluation of Noisy Popper which
demonstrates that Noisy Popper on average generalizes better to noisy datasets
than Normal Popper, that Noisy Popper generalizes as well as Normal Popper to

noiseless datasets, and how each enhancement used to construct Noisy Popper

eﬀects its overall performance.

1.3 Paper Organization

This dissertation consists of seven chapters including this brief introduction to the

project. The subsequent chapters will be as follows:

• Chapter 2 will review related works in the ﬁeld of ILP including systems which
are unable to handle noise, systems which can handle noise, their methods, and

how eﬀective they are in practice.

CHAPTER 1.

INTRODUCTION

6

• Chapter 3 will cover background information on the LFF framework including

a brief review of logic notation and ILP.

• Chapter 4 will begin the novel contributions of this paper and discuss the Re-
laxed LFF framework including the theoretical claims used to justify the frame-

work setup.

• Chapter 5 will cover the implementation of Noisy Popper and touch on the

implementation of Normal Popper out of necessity.

• Chapter 6 will discuss the experiments and empirical results and analysis of the

Noisy Popper system.

• Chapter 7 is the conclusion and will summarise the paper, its claims and ﬁnd-
ings, and discuss limitations of the work as well as future work to be considered.

Chapter 2

Related Work

In this chapter, we give a brief overview of the current state of ILP by discussing

several systems, their approaches, and their noise handling capacities.

2.1 No Noise Handling

ILP has been a machine learning area of great interest for over three decades [11].

Naturally, many varied approaches to solving ILP problems have been introduced,
each with varying degrees of success at handling noisy data, though many take no

attempt at all. A common approach to ILP is through the use of metarules [15] which

are logical statements deﬁning the syntactic form logic programs may take within the

hypothesis space, thus restricting said space. Metagol [35] is a popular ILP system
under this Meta-Interpretive Learning (MIL) setting. Because of the strict nature of

these metarules, MIL systems like Metagol often possess higher inductive bias when

compared to predicate declarations which simply deﬁne which predicates may appear

in the head or body of a clause. These predicate declarations are what Popper uses
as its language bias (restrictions which deﬁne the initial hypothesis space). Metagol

additionally allows for automatic predicate invention wherein novel predicates are

created using existing predicates and can be used to simplify hypothesis construction.

The major drawback of such an approach however is the need for domain expertise as
a user typically needs to deﬁne the metarules to be used by the system. Additionally,

like Popper, Metagol only returns solutions which entail all positive examples and no

negative examples, meaning that the system is naturally incapable of generalizing to

noise.

ILASP [25] is another ILP system which cannot handle noise, but takes an Answer
Set Programming (ASP) approach. With ASP approaches, the problem itself is en-

7

CHAPTER 2. RELATED WORK

8

coded as an ASP problem using logical statements or rules. These rules form a

type of constraint satisfaction problem which is then solved by a state of the art ASP

solver, generating an answer set solution which satisﬁes the given problem constraints.
While eﬀective, these methods carry drawback as all machine learning approaches do.

ILASP works in a similar loop to Popper, generating hypothesis and using them to

construct ASP constraints to improve the search in subsequent iterations. These

constraints are in the form of boolean formulas over the given set of rules. ILASP
pre-computes all of these rules using an ASP encoding for the given ILP problem,

constructs additional ASP constraints from these encodings, and ﬁnally solves an ad-

ditional ASP problem with these new constraints. Pre-computing all rules is not only

computationally expensive, but the system also struggles to learn rules with many
body literals. Additionally, ILASP does not typically scale as well as other systems

as it requires a large amount of grounding with the programs it generates. With

noise, a similar issue to Metagol and Popper exists where the system continues to

constrain hypotheses until a solution is found which covers all positive examples and
no negative examples.

HEXMIL [20] is an approach which combines the MIL and ASP settings using a HEX-

formalism to encode MIL with external sources, reducing the bottleneck produced by

the need to ground all atoms. Like the others, this approach fundamentally cannot

handle noise as returned hypotheses must entail all positive examples and no negative
ones. Contrasting to these approaches, in this project we introduce Noisy Popper

which is capable of generalizing to sets of noisy examples as returned solutions may

not perfectly entail all positive examples and no negative ones.

2.2 Set Covering

One popular approach to ILP is to use set covering algorithms which progressively

learn hypotheses by adding one logical clause at a time, covering a number of positive

examples with each. Perhaps the most inﬂuential ILP system and one which imple-
ments a set covering technique is Progol [33]. Its intuitive approach selects a positive

example that has not yet been entailed by the program and generates the bottom

clause or the most speciﬁc clause that entails that example using the minimal Her-

brand model of the set of atoms. It then attempts to make this clause as general as
possible so that when added to the program constructed so far, it entails as many new

positive examples as possible while avoiding entailing negative examples. However,

CHAPTER 2. RELATED WORK

9

the model contains a noise parameter which controls the quantity of negative exam-

ples that are allowed to be entailed. In this way, the system may avoid overﬁtting

the data. However, this hyperparameter leaves much of the noise handling procedure
on the user and is not a default mechanism of the system. Signiﬁcant ﬁne tuning is

required for Progol to adequately generalize to noisy datasets, a noticeable burden

on the user. Aleph [45] is a popular system based on Progol but built in Prolog.

Aleph uses a scoring metric to determine how general to make the bottom clauses.
This score can be user deﬁned. As such, it can be selected so that the system is

adaptable to noise - a returned solution may not perfectly entail all positive example

and no negative examples thus avoiding overﬁtting. However, like Progol, setting up

the hyperparameter environment to accurately learn from noisy data is cumbersome.

TILDE [4] uses an approach of top-down induction of decision trees [39] combined

with ﬁrst-order logic to construct a solution as a decision tree. As with traditional bi-
nary decision trees, they are constructed to correctly classify the given set of training

examples with the left and right splits corresponding to conjunctions in the logi-

cal statements constructed, though the model produced is not required to cover all

given examples. A tree construction where each example corresponds to a single
leaf node/classiﬁcation is entirely possible and would constitute a form of overﬁtting,

so the system takes steps to avoid this as in a traditional machine learning setting.

However, this method again requires ﬁne tuning. Under-pruning the tree can lead

to signiﬁcant overﬁtting while over-pruning results in small decision trees which do
not ﬁt the data well. Noisy Popper however requires no such noise parameters and

naturally generalizes to noisy datasets without requiring ﬁne tuning.

2.3 Sampling Examples

MetagolN T [34] is a noise tolerant extension of the Metagol system, simply acting as a
wrapper around the original Metagol algorithm. MetagolN T ﬁrst generates a random
subset of the training examples. It then learns a program which perfectly ﬁts this data
using the original Metagol system and ﬁnds the accuracy of the resulting program

on the remaining unused training data. The system repeats this loop several times

and simply returns the program which obtained the highest accuracy. In this way,

the returned program will not always perfectly ﬁt all training data as Metagol would,
but will often better generalize to noisy datasets. This approach has shown decent

results having even been used accurately for some image classiﬁcation problems [34].

However, in that same work, the authors address how the approach has limited grasp

CHAPTER 2. RELATED WORK

10

on noise handling and often fails if noise concentration is too high. The system is

largely dependent on a number of factors including the number of training examples,

size of the random subsets, and number of candidate programs generated. If too much
noise is present in each subset, no program will capture the true underlying data

pattern. If the subsets are too small to ensure at least some contain relatively little

noise, a similar issues may occur where there are not enough examples to generalize

from. Tuning these hyperparameters is not always trivial. Like systems such as
Aleph, MetagfolN T also requires a diﬃcult to use noise parameter which determines
how many negative examples are permitted to be entailed. Like with most systems,

these hyperparameters are diﬃcult to eﬀectively tune, though Noisy Popper lacks

them entirely.

2.4 Branch and Bound

ILASP3 [26] is a noise tolerant version of ILASP taking the form of a branch and

bound approach to ILP. Like ILASP, ILASP3 uses an ASP encoding to constrain the
search space in a similar generate, test, constrain loop as Popper. With each hypoth-

esis generated, the system tests which positive examples are not covered, determines

why, and uses these failed examples to generate additional ASP constrains for the

next iteration, pruning the search space. Unlike ILASP, ILASP3 assigns weights to
each example. The system then searches for an optimal program which entails the

highest sum of weights as possible, rather than simply trying to entail all positive

examples and no negative examples. In this way, ILASP3 is designed to handle noisy

data. Weights can also be used to correct imbalances in the ratio between positive
and negative examples. For example, if there are twice as many positive examples

as negative, the negative examples may be weighted twice as much to avoid being

ignored as noise by the system, i.e., the system would only focus on entailing as many

positive examples as possible since the negative weights are negligible. However, like
the original ILASP system, ILASP3 still pre-computes all ASP rules at each itera-

tion leading to large computational cost, causing it to struggle when scaling to large

datasets.

2.5 Neural Networks

An alternative to these previous ILP approaches is to take a continuous rather than

discrete approach to the problem through the use of neural networks. The ∂ILP [17]

CHAPTER 2. RELATED WORK

11

system uses continuous weight values to determine probability distributions of atoms

over clauses. The system uses stochastic gradient descent to alter these weights and

minimize the cross entropy loss of each classiﬁed example. Like with most standard
neural network approaches, the system can be tuned with hyperparameters such as

a learning rate and initial clause weights.

In this way, the system can be trained

to handle some amount of noise as the returned program may not have zero loss.

As with the previous systems however, this hyperparameter tuning is not always
intuitive. Additionally, ∂ILP requires program templates to constrain the set of

programs searched which is another user deﬁned parameter, requiring some amount

of brute-force work in order to generate an eﬃcient search space.

2.6 Applications to Popper

While the noise handling approaches for these ILP systems are worth studying in

their own rights, the unique LFF framework of Popper means that we cannot apply

many of these techniques directly. The general concept of scoring hypotheses used
in ILASP3 is a concept which can be applied to Popper as a means to compare

more than just the accuracy of hypotheses in order to prevent overﬁtting, e.g., we

may want to score a short and highly accurate hypothesis higher than a massive

but perfectly accurate one. Ultimately however, a novel approach to noise handling
must be taken with Popper, though we aim to show that the theoretical results

used can be extended to other systems in the future regardless of whether they fall

under the LFF framework. Additionally, many of these noise tolerant systems do

so through the use of hyperparameters which often make them cumbersome to use
and ineﬀective under default conditions, i.e., signiﬁcant tuning is usually required to

allow the systems to eﬀectively generalize to noisy data. As such, a goal of the Noisy

Popper implementation is to make it as natural of an extension of Normal Popper as

possible which requires little to no hyperparameters.

Chapter 3

Learning from Failures (LFF)
Framework

This chapter will provide an overview of the LFF framework used by Normal Popper
and modiﬁed by Noisy Popper. First, we will brieﬂy cover logic programming pre-

liminaries and notation necessary for the rest of the paper, though we will assume

some prior knowledge of boolean logic on the part of the reader. Using this notation,

we will formally deﬁne the ILP problem setting. We will conclude by explaining the
LFF framework and its deﬁnitions.

3.1 Logic Programming Preliminaries

To understand the LFF framework, it is necessary to review the framework of logic
programming. This section will brieﬂy cover necessary deﬁnitions based on those

found in [8, 11]. We will assume some familiarity with the topic, though for a com-

prehensive overview, interested readers are encouraged to reference [37].

3.1.1 First Order Logic

We will refer to the following deﬁnitions from [11] throughout the paper:

• A variable is a character string which starts with an uppercase letter, e.g., A,

B, Var.

• A function symbol is a character string which starts with a lowercase letter,

e.g., f, eastbound, last, head.

• A predicate symbol is a character string which starts with a lowercase, like
a function symbol. The arity n of a predicate symbol represents the number

12

CHAPTER 3. LEARNING FROM FAILURES (LFF) FRAMEWORK

13

of arguments that it takes and is denoted as p/n, e.g., f/1, eastbound/1,
last/1, head/2.

• A constant symbol is a function or predicate symbol which has arity 0.

• A term is a variable or constant symbol, or a function or predicate symbol with

arity n that is immediately followed by a tuple of n terms.

• We call a term ground if it contains no variables.

• An atom is a logical formula p(t1, t2, ..., tn), where p is a predicate symbol of
arity n and ti is a term for i ∈ {1, 2, ..., n}, e.g., eastbound(train) where
eastbound is a predicate symbol of artiy 1 and train is a constant symbol.

• An atom is ground is all of its terms are ground, like the example in the deﬁnition

above.

• We represent the negation symbol as ¬.

• A literal is an atom A (a positive literal) or its negation ¬A (a negative literal),

e.g., eastbound(train1) is both an atom and a literal while ¬eastbound(train1)
is only a literal as atoms do not contain the negation symbol.

Clauses We can use these previous deﬁnitions as building blocks to construct the

logic programs and constraints we will be using.

Deﬁnition 1 (Clause) A clause if a ﬁnite (possibly empty) disjunction of literals.

For instance, this following set of literals constitutes a clause:

{eastbound(A), ¬has car(A,B), ¬two wheels(B), ¬roof closed(B)}

We assume all variables in a clause are universally quantiﬁed, so explicit quantiﬁers

are omitted. As with terms and atoms, clauses are ground if they contain no variables,

so the example above is not ground. In logic programming, clauses are typically in
reverse implication form:

h ← b1 ∧ b2 ∧ ... ∧ bn.

CHAPTER 3. LEARNING FROM FAILURES (LFF) FRAMEWORK

14

Put verbally, the above clause states that the literal h, known as the head literal,
is true only if all literals bi, known as body literals, are all true. All the bi literals
together are the body of the clause. Note, the head literal must always be a positive
literal. We often use shorthand replacing ← with :- and ∧ with , to ease writing
clauses and make them similar to actual Prolog notation, e.g., the above clause we

would write as:

h :- b1, b2, ..., bn.

For simplicity, we will use this Prolog notation throughout this paper. We deﬁne a

clausal theory as a set of clauses. In the LFF setting, we restrict clauses to those

which contain no function symbols and where every variable which appears in the
head of a clause also appears in its body. These clauses are known as Datalog clauses

and a set of Datalog clauses constitute a Datalog theory. We also deﬁne a Horn clause

as a clause with at most one positive literal, as is the case with the above example.

We restrict our setting to only Horn clauses and Horn theories which are sets of Horn
clauses. Deﬁnite clauses are Horn clauses with exactly one positive literal while a

Deﬁnite logic program is a set of deﬁnite clauses. The logic programs which form our

ILP hypothesis spaces will consist of only Datalog deﬁnite logic programs.

Substitution Substitution is an essential logic programming concept and is simply
the act of replacing variables v0, v1, ..., vn with terms t0, t1, ..., tn. Such a substitu-
tion is denoted by: θ = {v0/t0, v1/t1, ..., vn/tn}. For instance, the substitution θ =
{A/train} to eastbound(A) :- has car(A,B), two wheels(B), roof closed(B)
yields eastbound(train) :- has car(train,B), two wheels(B), roof closed(B).
In this example, eastbound(train) would be true if train possess some car B such
that B has three wheels and its roof is opened. A substitution θ uniﬁes atoms A and
B if Aθ = Bθ, i.e., using substitution θ on atoms A and B obtains equivalent results.

3.2 LFF Problem Setting

This section formally introduces deﬁnitions to the LFF framework problem. Most of

the deﬁnitions are taken from [13]. Interested readers should refer to this paper for a

more thorough explanation.

CHAPTER 3. LEARNING FROM FAILURES (LFF) FRAMEWORK

15

3.2.1 Declaration Bias

The LFF problem setting is based oﬀ of the ILP learning from entailment setting
[40] whose goal, as stated in the ﬁrst chapter, is to take as input sets of positive

and negative examples, BK, and a target predicate and return a hypothesis or logic

program which in conjunction with the BK entails all positive examples and no neg-

ative examples. All ILP approaches boil down to searching a hypothesis space for
such a program. For each ILP problem, the hypothesis space is restricted by a lan-

guage bias. Though several language biases exist in ILP, our LFF framework uses

predicate declarations which declare which predicates are permitted to appear in the

head of a clause in a hypothesis and which are permitted to appear in the body. The
declarations are deﬁned as follows:

Deﬁnition 2 (Head Declaration) A head declaration is a ground atom of the form

head pred(p, a) where p is a predicate symbol of arity a [13].

For example, for our running trains problem, we would have head pred(eastbound,1).

Deﬁnition 3 (Body Declaration) A body declaration is a ground atom of the form

body pred(p, a) where p is a predicate symbol of arity a [13].

For example, for the trains example, we would have body pred(has car,2),
body pred(two wheels,1) and body pred(roof closed,1) among others.

We can then deﬁne a declaration bias D as a pair (Dh, Db) where Dh is a set of
head declatations and Db is a set of body declarations. The LFF hypothesis space
then must only be comprised of programs whose clauses conform to these declaration
biases. We deﬁne the notion of a declaration consistent clause:

Deﬁnition 4 (Declaration Consistent Clause) Let D = (Dh, Db) be a declara-
tion bias and C = h ← b1, b2, ..., bn be a deﬁnite clause. We say that C is declaration
consistent with D if and only if:

• h is an atom of the form p(X1, X2, ..., Xn) such that head pred(p, n) ∈ Dh.

• every bi is a literal of the form p(X1, X2, ..., Xm) such that body pred(p, m) ∈ Db.

• every Xi is a ﬁrst-order variable.

CHAPTER 3. LEARNING FROM FAILURES (LFF) FRAMEWORK

16

[13]

Example 2 (Clause Declaration Consistency) Let
D = ({head pred(eastbound,1)}, {body pred(has car,2), body pred(two wheels,1),
body pred(roof closed,1)}) be a declaration bias. The following clauses would be
declaration consistent with D:

eastbound(A) :- has car(A,B).
eastbound(A) :- has car(A,B), two wheels(A).
eastbound(A) :- has car(A,B), roof closed(B).

Conversely, the following clauses are declaration inconsistent with D:

eastbound(A, B) :- has car(A,B).
eastbound(A) :- has car(A,B), eastbound(A).
eastbound(A) :- has car(A,B), has load(B,C).

With this deﬁnition, we can fully deﬁne Declaration consistent hypotheses which pop-

ulate our hypothesis space:

Deﬁnition 5 (Declaration Consistent Hypothesis) Let D = (Dh, Db) be a
declaration bias. A declaration consistent hypothesis H is a set of deﬁnite clauses

where each clause C ∈ H is declaration consistent with D [13].

Example 3 (Hypothesis Declaration Consistency) Again, let D be the same

declaration bias as in the example above. Then the following hypotheses are decla-
ration consistent:

h1 = (cid:8) eastbound(A) :- has car(A,B), two wheels(B). (cid:9)
(cid:26) eastbound(A) :- has car(A,B), two wheels(B).
eastbound(A) :- has car(A,B), roof closed(B).

h2 =

(cid:27)

3.2.2 Hypothesis Contraints

While declaration biases are how we restrict the initial hypothesis space, the LFF
framework revolves around pruning the hypothesis space through hypothesis con-

straints which we deﬁne as in [13]. We ﬁrst precisely deﬁne a constraint:

Deﬁnition 6 (Constraint) A constraint is a Horn clause without a head, i.e., a
denial. We say that a constraint is violated if all of its body literals are true [13].

CHAPTER 3. LEARNING FROM FAILURES (LFF) FRAMEWORK

17

We can proceed with a general deﬁnition of a hypothesis constraint:

Deﬁnition 7 (Hypothesis Constraint) Let L be a language that deﬁnes hypothe-
ses, i.e., a meta-language. Then a hypothesis constraint is a constraint expressed in

L [13].

Example 4 (Hypothesis Constraints) In both Normal and Noisy Popper, the

meta-language used to encode programs takes a form like this:

head literal(Clause,Pred,Arity,Vars)

which denotes that the clause Clause possesses a head literal with predicate symbol
Pred which has an arity of Arity and whose arguments are deﬁned by Vars (note:
Vars would be represented by a tuple of size equal to Arity). The following atom:

body literal(Clause,Pred,Arity,Vars)

analogously deﬁnes a body literal appearing in Clause. We can then construct an
example of a hypothesis constraint:

:- head literal(C,p,1, ), body literal(C,p,1, )

where ’ ’s represent wildcards. This constraint simply states that clause C cannot
contain a predicate symbol p which appears both in the head and the body of the
clause, e.g., the clause C = p(A) :- p1(A,B), p(B).

Like with declaration consistent hypotheses, we can now deﬁne a hypothesis which is

consistent with all hypothesis constraints:

Deﬁnition 8 (Constrain Consistent Hypothesis) Let C be a set of hypothesis
constraints written in a language L . A set of deﬁnite clauses H is consistent with C

if, when written in L , H does not violate any constraint in C. [13]

CHAPTER 3. LEARNING FROM FAILURES (LFF) FRAMEWORK

18

3.2.3 Problem Setting

Now that we have deﬁned declaration bias and hypothesis constraints, we can fully
deﬁne the LFF hypothesis space which takes a similar form to most ILP hypothesis

spaces:

Deﬁnition 9 (Hypothesis Space) Let D be a declaration bias and C be a set of
hypothesis constraints. Then, the hypothesis space HD,C is the set of all declaration
and constraint consistent hypotheses. We refer to any element in HD,C as a hypothesis
[13].

We additionally can deﬁne the precise LFF problem:

Deﬁnition 10 (LFF Problem Input) Our problem input is a tuple (B, D, C, E+, E−)
where:

• B is a Horn program denoting background knowledge

• D is a declaration bias

• C is a set of hypothesis constraints

• E+ is a set of ground atoms denoting positive examples

• E− is a set of ground atoms denoting negative examples

[13]

As in [13] we will also deﬁne several hypothesis outcomes or types commonly used in

ILP literature [37] which we will refer to moving forward.

Deﬁnition 11 (Hypothesis Types) Let (B, D, C, E+, E−) be an input tuple and
H ∈ HD,C be a hypothesis. Then H is:

• Complete when ∀e ∈ E+, H ∪ B |= e

• Consistent when ∀e ∈ E−, H ∪ B (cid:54)|= e

• Incomplete when ∃e ∈ E+, H ∪ B (cid:54)|= e

• Inconsistent when ∃e ∈ E−, H ∪ B |= e

CHAPTER 3. LEARNING FROM FAILURES (LFF) FRAMEWORK

19

• Totally Incomplete when ∀e ∈ E+, H ∪ B (cid:54)|= e

• Totally Inconsistent when ∀e ∈ E−, H ∪ B |= e

[13]

This terminology also helps us deﬁne an LFF solution and LFF failed hypothesis:

Deﬁnition 12 (LFF Solution) Given an input tuple (B, D, C, E+, E−), a hypoth-
esis H ∈ HD,C is a solution when H is complete and consistent [13].

Deﬁnition 13 (LFF Failed Hypothesis) Given an input tuple
(B, D, C, E+, E−), a hypothesis H ∈ HD,C fails (or is a failed hypothesis) when H is
either incomplete or inconsistent [13].

These deﬁnition correspond to many ILP system settings we discussed in Chapter’s

1 and 2 where a solution entails all positive examples and no negative examples.

Optimality For a given LFF problem, there can naturally be several solutions. For

example, consider an east-west trains problem where all eastbound trains are those

which possess a car with two wheels and all other trains are westbound. Consider the

hypotheses:

h1 = (cid:8) eastbound(A) :- has car(A,B), two wheels(B). (cid:9)
(cid:26) eastbound(A) :- has car(A,B), two wheels(B).

h2 =

eastbound(A) :- has car(A,B), two wheels(B), roof closed(B).

(cid:27)

Both hypotheses would correctly identify all trains. Note that the second clause in
h2 will entail nothing extra from the ﬁrst clause, making it redundant. Naturally,
we would rather return hypothesis h1 as it is simpler, lacking this redudant clause.
Though deciding between two solutions is a common and non-trivial problem in ILP,
often systems deﬁne optimality in terms of length, returning the solution with fewest

clauses [35, 20] or literals [7, 25]. While many ILP systems are not guaranteed to

return optimal solutions [33, 45, 4], Normal Popper [13] is guaranteed to return op-

timal solutions with minimal number of total literals. Noisy Popper also appeals to
this description of optimality as it works closely with the minimum description length

CHAPTER 3. LEARNING FROM FAILURES (LFF) FRAMEWORK

20

(MDL) principle [42] which is used to justify several claims later in this paper. As

such, we will formally deﬁne hypothesis size and solution optimality:

Deﬁnition 14 (Hypothesis Size) The function size(H) returns the total number

of literals in the hypothesis H [13].

Deﬁnition 15 (LFF Optimal Solution) Given an input tuple (B, D, C, E+, E−),
a hypothesis H ∈ HD,C is an optimal solution when two conditions hold:

• H is a solution

• ∀H (cid:48) ∈ HD,C such that H (cid:48) is a solution, size(H) ≤ size(H (cid:48))

[13]

3.2.4 Generalizations and Specializations

In the LFF framework, the hypothesis constraints are learned from the generalizations

and specializations of failed hypotheses. In this way, large sections of the hypothesis
space can be pruned for each hypothesis generated and tested. To understand gen-

eralizations and specializations, we need to deﬁne the notion of θ-subsumption [38]

which we refer to simply as subsumption.

Deﬁnition 16 (Clausal Subsumption) A clause C1 subsumes a clause C2 if and
only if there exists a θ-subsumption such that C1θ ⊆ C2 [13].

Example 5 (Clausal Subsumption) Let C1 and C2 be deﬁned as:

C1 = eastbound(A) :- has car(A,B).
C2 = eastbound(X) :- has car(X,Y), two wheels(Y).

We say that C1 subsumes C2 since if θ = {A/B} then C1 θ ⊆ C2.

Importantly, subsumption implies entailment [37], though the converse does not nec-
essarily hold. Thus, if clause C1 subsumes C2, then C1 must entail at least everything
that C2 does. [29] extends this idea of subsumption to clausal theories:

Deﬁnition 17 (Theory Subsumption) A clausal theory T1 subsumes a clausal
theory T2, denoted T1 (cid:22) T2, if and only if ∀C2 ∈ T2, ∃C1 ∈ T1 such that C1 subsumes
C2 [13].

Example 6 (Theory Subsumption) Let h1 and h2 be deﬁned as:

CHAPTER 3. LEARNING FROM FAILURES (LFF) FRAMEWORK

21

h1 = (cid:8) eastbound(A) :- has car(A,B), two wheels(B). (cid:9)

(cid:26)

(cid:27)

h2 =

h3 =

eastbound(A) :- has car(A,B), two wheels(B), roof closed(B).

(cid:26) eastbound(A) :- has car(A,B), two wheels(B).
eastbound(A) :- has car(A,B), roof closed(B).

(cid:27)

Then we can say h1 (cid:22) h2, h3 (cid:22) h2, and h3 (cid:22) h1.

[13] also proves the following proposition regarding theory subsumption:

Proposition 1 (Subsumption implies Entailment) Let T1 and T2 be clausal
theories. If T1 (cid:22) T2 then T1 |= T2 [13].

That is, using the programs above, any example that h1 entails is also entailed by h3.
Using Deﬁnition 17 for theory subsumption, we can deﬁne the notion of a generaliza-

tion:

Deﬁnition 18 (Generalization) A clausal theory T1 is a generalization of a clausal
theory T2 if and only if T1 (cid:22) T2 [13].

For example, again using the programs above, h3 is a generalization of h1 which is a
generalizations of h2. Likewise, we can deﬁne the notion of a specialization:

Deﬁnition 19 (Specialization) A clausal theory T1 is a specialization of a clausal
theory T2 if and only if T2 (cid:22) T1 [13].

Again using the previous programs as examples, h2 is a specialization of h1 which is
a specialization of h3.

With these deﬁnitions, we can describe in the next section how Normal Popper gen-
erates hypothesis constraints using generalizations and specializations from failed

hypotheses.

3.3 Hypothesis Constraints

The Normal Popper system breaks down the ILP problem into three separate stages:

generate, test, and constrain. Unlike many ILP approaches which reﬁne a clause [40,

33, 4, 45, 1] or hypothesis [6, 3, 35], Normal Popper reﬁnes the hypothesis space itself

CHAPTER 3. LEARNING FROM FAILURES (LFF) FRAMEWORK

22

by learning hypothesis constraints. In the generate stage, Normal Popper generates

a hypothesis which satisﬁes all current hypothesis constraints. These constraints

determine the syntactic form a hypothesis may take. In the subsequent test stage,
this hypothesis is then tested against the positive and negative examples provided

to the system. Should a hypothesis fail, i.e., it is either incomplete or inconsistent,

the system continues on to the constrain stage. Here, the system learns additional

hypothesis constraints from the failed hypothesis to further prune the hypothesis
space for future hypothesis generation. There are two general types of constraints

that both Normal Popper and Noisy Popper are concerned with: generalizations and

specializations. We will discuss both here in addition to a third particular type of

constrain called elimination constraints.

3.3.1 Generalization Constraints

Consider a hypothesis H being tested against E−.
If H is inconsistent, that is it
entails some or all of the examples in E−, we can conclude that H is too general.
That is, H is entailing too many examples and not being restrictive enough. Thus,
any solution to the ILP problem is necessarily more restrictive than H, i.e., it entails

less than H. We can prune all generalizations of H as these too must be inconsistent

[13] since they only can entail additional examples from H. This leads us to the

deﬁnition of a generalization constraint:

Deﬁnition 20 (Generalization Constraint) A generalization constraint only

prunes generalizations of a hypothesis from the hypothesis space [13].

Example 7 (Generalization Constraints) Suppose we have the following deﬁned:

E− = {eastbound(train1).}
h = {eastbound :- has car(A,B), two wheels(B).}

Additionally, suppose the BK contains facts:

has car(train1,car1).
two wheels(car1).

We can see how h entails the only negative example, indicating that it is too general.
As such, all generalizations of h can be pruned, e.g., programs such as:

CHAPTER 3. LEARNING FROM FAILURES (LFF) FRAMEWORK

23

h1 =

h2 =

(cid:26) eastbound(A) :- has car(A,B), two wheels(B).
eastbound(A) :- has car(A,B), roof closed(B).
eastbound(A) :- has car(A,B), two wheels(B).
eastbound(A) :- has car(A,B), has load(B,C), circle(C).
eastbound(A) :- has car(A,B), short(B).






(cid:27)






Because h1 (cid:23) h and h2 (cid:23) h, both h1 and h2 must also entail this one negative example
and therefore cannot be LFF solutions. Note that given hypotheses h and h’, if h ⊆
h’ then h is a generalization of h’.

3.3.2 Specialization Constraints

Next, consider a hypothesis H being tested against E+. If H is incomplete, that is
it entails only some or none of the examples in E+, we can conclude that H is too
speciﬁc. That is, H is entailing too few examples and being overly restrictive. Thus,
any solution to the ILP problem is necessarily less restrictive than H, i.e., it entails

more than H. We can prune all specializations of H as these too must be incomplete

[13] since they only can entail fewer examples than H. This leads us to the deﬁnition

of a specialization constraint:

Deﬁnition 21 (Specialization Constraint) A specialization constraint only prunes
specialization of a hypothesis from the hypothesis space [13].

Example 8 (Specialization Constraints) Suppose we have the following deﬁned:

E+ = {eastbound(train2).}
h = {eastbound :- has car(A,B), two wheels(B), roof closed(B).}

Additionally, suppose the BK contains the facts:

has car(train2,car2).
two wheels(car2).

We can see how h does not entails the only positive example, since train2 only
contains a car which has three wheels, but does not have its roof closed. This indicates
that the hypothesis is too speciﬁc. As such, all specializations of h can be pruned,
e.g., programs such as:

h1 = (cid:8) eastbound(A) :- has car(A,B), two wheels(B), roof closed(B), short(B). (cid:9)
h2 = (cid:8) eastbound(A) :- has car(A,B), two wheels(B), has load(B,C), circle(C). (cid:9)

Because h (cid:23) h1 and h (cid:23) h2, both h1 and h2 must also fail to entail this positive
example and therefore cannot be LFF solutions.

CHAPTER 3. LEARNING FROM FAILURES (LFF) FRAMEWORK

24

3.3.3 Elimination Constraints

Finally, we can consider a speciﬁc case where a hypothesis H is totally incomplete.

In addition to the normal specialization constraint, we can prune a particular set of
hypotheses which contain a version of H within themselves. To precisely deﬁne these

hypotheses, we will need an additional deﬁnition:

Deﬁnition 22 (Separable) A separable hypothesis G is one where no predicate
symbol in the head of a clause in G occurs in the body of a clause in G [13].

Example 9 (Non-separable Hypotheses) Consider the following hypothesis:

h =

(cid:26) eastbound(A) :- has car(A,B), f(B).

(cid:27)

f(B) :- two wheels(B), roof closed(B)

Hypothesis h is non-separable because the predicate symbol f appears in both a
[13] shows that if a hypothesis H is
head of a clause and in the body of a clause.

totally incomplete, then neither H nor any specialization of H can appear inside any

separable optimal solution. Thus, all separable hypotheses containing a specialization

of H can be pruned. This leads us to the deﬁnition of an elimination constraint:

Deﬁnition 23 (Elimination Constraint) An elimination constraint only prunes
separable hypotheses that contain specialisations of a hypothesis from the hypothesis

space [13].

Example 10 (Elimination Constraints) Consider the set of positive examples:

E+ ={eastbound(train1)., eastbound(train2).}

and consider the candidate hypothesis h:

h = (cid:8) eastbound(A) :- has car(A,B), two wheels(B), roof closed(B). (cid:9)

Additionally, suppose the BK contains the facts:

has car(train1,car1).
two wheels(car1).
has car(train2,car2).
short(car2).

CHAPTER 3. LEARNING FROM FAILURES (LFF) FRAMEWORK

25

Clearly, h is totally incomplete and as such, Popper will add an elimination constraint
which will prune all separable hypotheses that contain h1 or any of its specializations
such as:

h1 =

h2 =

h3 =




(cid:26) eastbound(A) :- has car(A,B), two wheels(B), roof closed(B).
eastbound(A) :- has car(A,B), has load(B,C), circle(C).
eastbound(A) :- has car(A,B), two wheels(B), roof closed(B).
eastbound(A) :- has car(A,B), short(B).
eastbound(A) :- has car(A,B), two wheels(B).


(cid:26) eastbound(A) :- has car(A,B), two wheels(B), roof closed(B), short(B).

(cid:27)






(cid:27)

eastbound(A) :- has car(A,B), long(B).

Note that elimination constraints may prune solutions from the hypothesis space. If
E− is empty in the example above, the hypothesis h2 above would be a solution to the
problem as it entails all positive examples. However, this hypothesis is not optimal

as elimination constraints will never prune optimal solutions. An optimal solution to

this example for instance would instead be:

h4 =

(cid:26) eastbound(A) :- has car(A,B), short(B).

(cid:27)

eastbound(A) :- has car(A,B), two wheels(B).

These basic hypothesis constraints allow Normal Popper to perform exceptionally well

on many datasets, even those with very few examples. However, these constraints
heavily rely on the absence of noise in the example sets. As the next chapter will

discuss, incorrectly labelled examples can cause these hypothesis constraints to prune

valuable sections of the hypothesis space.

3.4 Summary

In this chapter, we summarized the LFF framework for ILP problem solving.

In

doing so, we reviewed necessary logic programming concepts and notations as well

as formalized terminology we well use frequently moving forward. We additionally

discussed the crucial concepts of subsumption, generalizations, and specialization
which both Noisy and Normal Popper base their hypothesis constraints on. Finally,

we discussed the constraints Normal Popper implements: generalization constraints,

specialization constraints, and elimination constraints giving examples of each.

In

the next chapter, we discuss the modiﬁed problem setting for Noisy Popper which we
refer to as the Relaxed LFF Framework.

Chapter 4

Relaxed LFF Framework

The LFF setting described in Chapter 3 has a limitation when handling noise in

that solutions must perfectly ﬁt the given examples. Its hypothesis constraints may
remove highly accurate hypotheses which do not overﬁt any noisy data in favor of an

LFF solution which do overﬁt. One approach to avoid this is to ignore or relax all

hypothesis constraints and take a brute force approach, enumerating all hypothesis

until one of adequate accuracy is found. This has an obvious ineﬃciency limitation.
The aim of this project if to ﬁnd a middle-ground between the two solutions which

better generalizes to noisy data.

From this chapter, we will focus on presenting the novel contributions of the project.

Here, we ﬁrst outline the altered Relaxed LFF Framework from which Noisy Popper

is built. We start by deﬁning the altered noisy problem setting. We then describe

and prove the sound hypothesis constraints within this new setting. Finally, we
describe how we can apply the MDL principle to prune overﬁtting hypotheses through

additional sound constraints which take into account hypothesis size.

4.1 Relaxed LFF Problem Setting

In contrast to the LFF problem setting, in the general relaxed setting we do not
necessarily wish to ﬁnd hypotheses that entail all positive examples and no negative

examples. Rather, we wish to ﬁnd hypotheses which optimize some other metric or

score. In this manner, we can deﬁne the relaxed LFF problem input:

Deﬁnition 24 (Relaxed LFF Problem Input) Our problem input is a tuple
(B, D, C, E+, E−, S) where:

• B is a Horn program denoting background knowledge

26

CHAPTER 4. RELAXED LFF FRAMEWORK

27

• D is a declaration bias

• C is a set of hypothesis constraints

• E+ is a set of ground atoms denoting positive examples

• E− is a set of ground atoms denoting negative examples

• S is a scoring function which takes as input B, E+, E− as well as a hypothesis

H ∈ HD,C

Note that the hypothesis space in this relaxed setting is unchanged from the LFF
setting. From here, we can deﬁne an solution in this new setting:

Deﬁnition 25 (Relaxed LFF Solution) Given an input tuple (B, D, C, E+, E−, S),
a hypothesis H ∈ HD,C is a solution when ∀H (cid:48) ∈ HD,C, S(H, B, E+, E−) ≥ S(H (cid:48), B, E+, E−).

Note that it is possible to model the LFF setting from Chapter 3 in with this deﬁnition.
To do so, the scoring function S would be deﬁned as:

S(H, B, E+, E−) =

(cid:26) 1 if H is complete and consistent

(cid:27)

0 otherwise

As in the LFF setting, we deﬁne optimality similarly using the size of a hypothesis:

Deﬁnition 26 (Relaxed LFF Optimal Solution) Given an input tuple
(B, D, C, E+, E−, S), a hypothesis H ∈ HD,C is an optimal solution when two condi-
tions hold:

• H is a solution in the relaxed LFF setting

• ∀H (cid:48) ∈ HD,C such that H (cid:48) is a solution, size(H) ≤ size(H (cid:48))

With these deﬁnition, we can lay out the theoretical contributions of this project
through new hypothesis constraints which remain sound in this new setting.

CHAPTER 4. RELAXED LFF FRAMEWORK

28

4.2 Relaxed LFF Hypothesis Constraints

The main diﬃculty Normal Popper has when dealing with noise is its strict constraints

which prune the hypothesis space. If a hypothesis does not entail even just a single
positive hypothesis, it is rejected and all of its specializations are pruned. Similarly if

a hypothesis entails just one negative example, all of its generalizations are pruned.

While this works extremely well under the normal LFF setting, in the presence of

noise, being so strict may prune relaxed LFF solutions which do not ﬁt the noisy data
and only the underlying patterns. We can illustrate this type of overpruning through

an example:

Example 11 (Overpruning) Consider Normal Popper trying to learn the program:

h = (cid:8) eastbound(A) :- has car(A,B), short(B), two wheels(B). (cid:9)

Assume that all examples are correctly labelled except one noisy example:
eastbound(train1) ∈ E+ where train1 only possess a single long car. That is, in
the BK we have among others the facts:

has car(train1,car1).
long(car1).

Suppose we generate the hypothesis:

h1 = (cid:8) eastbound(A) :- has car(A,B), short(B). (cid:9)

This program will entail all positive examples in E+ except for the single noisy ex-
ample as train1 does not possess a short car. As such, in the LFF setting, h1 is
categorized as too speciﬁc. Thus, all specializations of h1 are pruned which includes
the desired solution h.

This overly strict pruning clearly can lead Normal Popper to overﬁtting any noisy

dataset as even a single incorrectly labelled example can cause heavy pruning of

the hypothesis space, potentially eliminating relaxed LFF solutions. This can be

avoided by not applying any LFF hypothesis constraints. However, we still wish to
improve the eﬃciency of the hypothesis search by removing hypotheses which can

not be relaxed LFF solutions, thus motivating relaxed LFF hypothesis constraints.

Any hypothesis constraints used in this setting should be sound under some scoring

CHAPTER 4. RELAXED LFF FRAMEWORK

29

function. Here we deﬁne the accuracy scoring function used in this section. We ﬁrst

deﬁne the notions of true positive, true negative, false positive, and false negative.

Deﬁnition 27 (True Positive) Given an input tuple (B, D, C, E+, E−, S) and a
hypothesis H ∈ HD,C, we deﬁne the true positive function as
(cid:12){e | e ∈ E+ and H ∪ B |= e}(cid:12)
tp(H, B, E+) = (cid:12)
(cid:12).

Deﬁnition 28 (True Negative) Given an input tuple (B, D, C, E+, E−, S) and a
hypothesis H ∈ HD,C, we deﬁne the true negative function as
(cid:12){e | e ∈ E− and H ∪ B (cid:54)|= e}(cid:12)
tn(H, B, E−) = (cid:12)
(cid:12).

Deﬁnition 29 (False Positive) Given an input tuple (B, D, C, E+, E−, S) and a
hypothesis H ∈ HD,C, we deﬁne the false positive function as
(cid:12){e | e ∈ E− and H ∪ B |= e}(cid:12)
f p(H, B, E−) = (cid:12)
(cid:12).

Equivalently, we may choose to write f p(H, B, E−) = |E−| − tn(H, B, E−)

Deﬁnition 30 (False Negative) Given an input tuple (B, D, C, E+, E−, S) and a
hypothesis H ∈ HD,C, we deﬁne the false negative function as
(cid:12){e | e ∈ E+ and H ∪ B (cid:54)|= e}(cid:12)
f n(H, B, E+) = (cid:12)
(cid:12).

Equivalently, we may choose to write f n(H, B, E+) = |E+| − tp(H, B, E+)

We will use these functions throughout the remained of the paper. With these, we

can deﬁne the method with which hypotheses are scored in this section:

Deﬁnition 31 (Accuracy Score) Given an input tuple (B, D, C, E+, E−, SACC)
and a hypothesis H ∈ HD,C, the function SACC(H, E+, E−, B) = tp(H, B, E+) +
tn(H, B, E−).

Note that this scoring function measures training accuracy rather than test accuracy.

We now aim to determine situations in which certain hypotheses are known to be

suboptimal under this scoring method in the relaxed LFF setting. First, we will

consider constraints constructed by comparing two hypotheses. We motivate this
through an example:

Example 12 (Learning by Comparing Hypotheses) Consider we have relaxed
LFF input (B, D, C, E+, E−, SACC) and previously observed the following hypothesis:
h1 = (cid:8) eastbound(A) :- has car(A,B), short(B). (cid:9)

CHAPTER 4. RELAXED LFF FRAMEWORK

30

which has tp(h1, B, E+) = 5 and tn(h1, B, E−) = 3. Now, consider a generalization
of this hypothesis:

h2 =

(cid:26) eastbound(A) :- has car(A,B), short(B).

(cid:27)

eastbound(A) :- has car(A,B), two wheels(B).

which identically has tp(h2, B, E+) = 5 and tn(h2, B, E−) = 3. We can conclude that
the clause eastbound(A) :- has car(A,B), two wheels(B). is redundant as it en-
tails no additional positive examples than the clause eastbound(A) :- has car(A,B),
short(B). As such, it is not worthwhile to consider any non-recursive generalizations
of h2 which add additional clauses to h2 as we could simply add these same clauses to
h1, producing a hypothesis that scores the same but is smaller as it does not contain
the redundant clause. To illustrate, consider this generalization: of h2:

h(cid:48)
2 =






eastbound(A) :- has car(A,B), short(B).
eastbound(A) :- has car(A,B), two wheels(B).
eastbound(A) :- has car(A,B), three wheels(B).






which we assume scores tp(h(cid:48)
generalization of h1:

2, B, E+) = 8 and tn(h(cid:48)

2, B, E−) = 4. Now, consider this

h(cid:48)
1 =

(cid:26) eastbound(A) :- has car(A,B), short(B).

(cid:27)

eastbound(A) :- has car(A,B), three wheels(B).

1, B, E+) = 8 and tn(h(cid:48)

which must also score tp(h(cid:48)
the same, it is redundant to consider both of them and since size(h(cid:48)
can safely prune h(cid:48)
this form can be safely pruned from the hypothesis space.

2 as a less optimal hypothesis than h(cid:48)

2 score
2), we
1. Thus, all generalizations of

1 and h(cid:48)
1) < size(h(cid:48)

1, B, E−) = 4. Since h(cid:48)

This example illustrates how we can compare previously observed hypotheses to any
new hypotheses in order to identify hypotheses which are suboptimal under the SACC
scoring. The remainder of this section will outline and prove such suboptimal cir-

cumstances.

We start by deﬁning some useful propositions relating generalizations and specializa-
tions to scoring:

Proposition 2 (Scores of Generalizations) Given problem input (B, D, C, E+, E−, S),
let H and H (cid:48) be hypotheses in HD,C where H (cid:48) is a generalization of H. Then:

CHAPTER 4. RELAXED LFF FRAMEWORK

31

• tp(H (cid:48), B, E+) ≥ tp(H, B, E+) and

• tn(H (cid:48), B, E−) ≤ tn(H, B, E−)

Proof. Follows immediately from Proposition 1 as subsumption implies entailment.

Proposition 3 (Scores of Specializations) Given problem input (B, D, C, E+, E−, S),
let H and H (cid:48) be hypotheses in HD,C where H (cid:48) is a specialization of H. Then:

• tp(H (cid:48), B, E+) ≤ tp(H, B, E+) and

• tn(H (cid:48), B, E−) ≥ tn(H, B, E−)

Proof. Follows immediately from Proposition 1 as subsumption implies entailment.

We now prove when relaxed generalization and specialization hypothesis constraints

may be applied:

Proposition 4 Given problem input (B, D, C, E+, E−, SACC) and hypotheses H1,
H2, and H3 in HD,C where H3 is a generalization of H1, if
SACC(H2, B, E+, E−) − SACC(H1, B, E+, E−) > f n(H1, B, E+) then
SACC(H2, B, E+, E−) > SACC(H3, B, E+, E−).

Proof. The improvement in score from H1 to H2 can be quantiﬁed by:

SACC(H2, B, E+, E−) − SACC(H1, B, E+, E−) > f n(H1, B, E+)

SACC(H2, B, E+, E−) − [tp(H1, B, E+) + tn(H1, B, E−)] > |E+| − tp(H1, B, E+)
SACC(H2, B, E+, E−) > |E+| + tn(H1, B, E−)

≥ tp(H3, B, E+) + tn(H3, B, E−)
= SACC(H3, B, E+, E−)

The second to last line following from Proposition 2 and that H3 is a generalization
of H1 in addition to the fact |E+| ≥ tp(H3, B, E+).

CHAPTER 4. RELAXED LFF FRAMEWORK

32

Proposition 5 Given problem input (B, D, C, E+, E−, SACC) and hypotheses H1,
H2, and H3 in HD,C where H3 in is a specialization of H1. If
SACC(H2, B, E+, E−) − SACC(H1, B, E+, E−) > f p(H1, B, E−) then
SACC(H2, B, E+, E−) > SACC(H3, B, E+, E−).

Proof. The improvement in score from H1 to H2 can be quantiﬁed by:

SACC(H2, B, E+, E−) − SACC(H1, B, E+, E−) > f p(H1, B, E−)

SACC(H2, B, E+, E−) − [tp(H1, B, E+) + tn(H1, B, E−)] > |E−| − tp(H1, B, E+)
SACC(H2, B, E+, E−) > tp(H1, B, E+) + |E−|

≥ tp(H3, B, E+) + tn(H3, B, E−)
= SACC(H3, B, E+, E−)

The second to last line following from Proposition 3 and that H3 is a specialization
of H1 in addition to the fact |E−| ≥ tn(H3, B, E−).

Proposition 6 Given problem input (B, D, C, E+, E−, SACC) and hypotheses H1
and H2 in HD,C where H2 is a generalization of H1, if tp(H1, B, E+) = tp(H2, B, E+),
then given any non-recursive hypothesis H (cid:48)
2 = H2 ∪ C for some non-empty set of
clauses C, there exists a generalization of H1, say H (cid:48)
SACC(H (cid:48)

1, B, E+, E−) ≥ SACC(H (cid:48)

2, B, E+, E−).

1, such that

2, B, E+) − tp(H2, B, E+) and

Proof. Let H (cid:48)
1 = H1 ∪ C. Also let n = tn(H1, B, E−) − tn(H2, B, E−),
i.e., SACC(H2, B, E+, E−) = SACC(H1, B, E+, E−) − n (note, n ≥ 0 by by Proposi-
tion 2). Assume that C entails p additional positive examples from H2 and n(cid:48) more
negative examples. That is p = tp(H (cid:48)
2, B, E−) (again noting that n(cid:48) ≥ 0 and p ≥ 0 by Proposi-
n(cid:48) = tn(H2, B, E−) − tn(H (cid:48)
tion 2). Since H (cid:48)
1 contains C, it must also entail p additional positive examples from
H1 and at most n + n(cid:48) additional negative examples as it may entail the negative
examples that H2 entailed but H1 did not. Thus, we have
SACC(H (cid:48)
SACC(H (cid:48)
(note that SACC(H (cid:48)
Thus, SACC(H (cid:48)

2, B, E+, E−) = SACC(H1, B, E+, E−) + p − n − n(cid:48) and
1, B, E+, E−) ≥ SACC(H1, B, E+, E−) + p − n − n(cid:48)

1, B, E+, E−) has an upper bound of SACC(H1, B, E+, E−)+p−n).

1, B, E+, E−) ≥ SACC(H (cid:48)

2, B, E+, E−).

CHAPTER 4. RELAXED LFF FRAMEWORK

33

Proposition 7 Given problem input (B, D, C, E+, E−, SACC) and hypotheses H1
and H2 where H1 ⊆ H2, if tp(H1, B, E+) = tp(H2, B, E+), given any non-recursive
specialization of H2, H (cid:48)
1, such that
SACC(H (cid:48)

2, there exists a specialization of H1, say H (cid:48)

1, B, E+, E−) ≥ SACC(H (cid:48)

2, B, E+, E−).

2, it is also in H (cid:48)

if a clause c is in both H1 and H (cid:48)

Proof. We can write H2 = H1 ∪ C for some set of clauses C. Since tp(H1, B, E+) =
tp(H2, B, E+), the clauses in C entail no additional positive examples from H1, mak-
ing them redundant to the clauses of H1. That is, every positive example entailed
by the clauses in C is already entailed by the clauses in H1. We can construct H (cid:48)
1
1. If a clause c is in H1
as such:
but not in H (cid:48)
2 and instead has been replaced by a speciﬁed version of the clause, call
it c(cid:48), then c(cid:48) is also in H (cid:48)
1. Any clauses in C or speciﬁed versions of these clauses
are not in H (cid:48)
1 will make the same speciﬁcations as H2 did to cre-
ate H (cid:48)
1 makes the same speciﬁcations to clauses in
H1 as H (cid:48)
2 does, and since C entails no extra positive examples nor will any of its
2, B, E+). Additionally, by Propo-
1, B, E+) = tp(H (cid:48)
speciﬁc clauses in H (cid:48)
sition 2, tn(H1, B, E−) ≥ tn(H2, B, E−). Since H (cid:48)
1 makes the same speciﬁcations as
2, B, E−) noting that at best any speciﬁcations H (cid:48)
1, B, E−) ≥ tn(H (cid:48)
H (cid:48)
2
makes to clauses in C can only mean C entails no negative examples in H (cid:48)
2. Thus,
2, B, E+, E−).
SACC(H (cid:48)

1. In this way, H (cid:48)
2 on the clauses in H1. Since H (cid:48)

1, B, E+, E−) ≥ SACC(H (cid:48)

2, then tn(H (cid:48)

2, then tp(H (cid:48)

Proposition 8 Given problem input (B, D, C, E+, E−, SACC) and hypotheses H1
and H2 in HD,C where H2 is a specialization of H1, if tn(H1, B, E−) = tn(H2, B, E−),
then given any non-recursive hypothesis H (cid:48)
2 = H2 ∪ C for some set of clauses C there
exists a generalization of H1, say H (cid:48)

1, B, E+, E−) ≥ SACC(H (cid:48)

1, such that SACC(H (cid:48)

2, B, E+, E−).

1, B, E−) = tn(H (cid:48)

Proof. Let H (cid:48)
tn(H (cid:48)
tp(H1, B, E+) ≥ tp(H2, B, E+) which means that tp(H (cid:48)
Thus, SACC(H (cid:48)

1 = H1 ∪ C. Then, since tn(H1, B, E−) = tn(H2, B, E−), we know that
2, B, E−). Since H2 is a specialization of H1, by Proposition 3,
2, B, E+).

1, B, E+, E−) ≥ SACC(H (cid:48)

1, B, E+) ≥ tp(H (cid:48)

2, B, E+, E−).

Propositions 6-8 notably only demonstrate SACC suboptimality for non-recursive gen-
eralizations and specializations. Though it may appear that additional clauses or
literals are redundant and do not improve either the tp or tn scores, they may set up

CHAPTER 4. RELAXED LFF FRAMEWORK

34

base cases for an SACC-optimal recursive program. We demonstrate this through an
example:

Example 13 (Non-recursive Case Motivation) Consider trying ﬁnd a program
for the target predicate alleven/1 which when given a list of integers returns True
if all of them are even and False otherwise. For instance, alleven([2,4,10,8]).
evaluates to True and alleven([1,2,3]). evaluates to False.

Assume that all examples are noiseless and suppose that the BK B contains the
following:

head([H| ],H)., i.e., returns True only if H is the ﬁrst element of the given list
tail([ |T],T)., i.e., returns True only if T is the given list with the ﬁrst element removed
empty([])., i.e. returns True only if the given list is empty
zero(0)., i.e., returns True only if the given integer is 0
even(A) :- 0 is A mod 2, i.e. returns True only if the given integer is even

Suppose we have previously seen hypothesis h1 = {alleven(A) :- head(A,B), even(B).}
which entailed all positive examples and some negative examples. Now, consider a
second hypothesis h2:

h2 =

(cid:26) alleven(A) :- head(A,B), even(B).

(cid:27)

alleven(A) :- empty(A).

which likewise entails all positive examples and the same number of negative examples.
By Proposition 7, since h1 ⊆ h2 and tp(h1,B, E+) = tp(h2,B, E+), all non-recursive
specializations of h2 are not SACC-optimal. We must specify non-recursive as the
recursive specialization h3 where:

h3 =

(cid:26) alleven(A) :- head(A,B), even(B), tail(A,C), alleven(C).

(cid:27)

alleven(A) :- empty(A).

is a solution and would entail all positive examples and no negative examples, thus
being SACC-optimal. Similar results hold for Propositions 6 and 8 where pruning
generalizations may remove SACC-optimal recursive programs.

Additionally, we can identify some sound constraints that do not rely on comparing

hypotheses:

Proposition 9 Given problem input (B, D, C, E+, E−, SACC) and hypothesis H ∈
HD,C where tp(H, B, E+) = |E+|, if hypothesis H (cid:48) ∈ HD,C is a generalization of H,
then SACC(H, B, E+, E−) ≥ SACC(H (cid:48), B, E+, E−).

CHAPTER 4. RELAXED LFF FRAMEWORK

35

Proof. Since H (cid:48) is a generalization of H, by Proposition 2,
tp(H, B, E+) ≤ tp(H (cid:48), B, E+) ⇒ tp(H (cid:48), B, E+) = |E+| and
tn(H, B, E−) ≥ tn(H (cid:48), B, E−). Thus, SACC(H, B, E+, E−) ≥ SACC(H (cid:48), B, E+, E−).

Proposition 10 Given problem input (B, D, C, E+, E−, SACC) and hypothesis H ∈
HD,C where tn(H, B, E−) = |E−|, if hypothesis H (cid:48) ∈ HD,C is a specialization of H,
then SACC(H, B, E+, E−) ≥ SACC(H (cid:48), B, E+, E−).

Proof. Since H (cid:48) is a specialization of H, by Proposition 3,
tn(H, B, E−) ≤ tn(H (cid:48), B, E−) ⇒ tn(H (cid:48), B, E−) = |E−| and
tp(H, B, E+) ≥ tp(H (cid:48), B, E+). Thus, SACC(H, B, E+, E−) ≥ SACC(H (cid:48), B, E+, E−).

4.2.1 Hypothesis Constraints Applications

These propositions all apply in any ILP setting, however, in our relaxed LFF setting,
they determine sets of programs which should be pruned should certain conditions

hold:

• Proposition 4 implies that if a hypothesis H’s accuracy score is at least f n(H (cid:48), B, E+)
greater than that of some hypothesis H (cid:48), we may prune all generalizations of
H (cid:48) as they cannot be SACC-optimal.

• Proposition 5 implies that if a hypothesis H’s accuracy score is at least f p(H (cid:48), B, E−)
greater than that of some hypothesis H (cid:48), we may prune all specializations of H (cid:48)
as they cannot be SACC-optimal.

• Proposition 6 implies that if a hypothesis H is a generalization of a hypothesis
H (cid:48) and both have equal tp values, we may prune all non-recursive superset of
H as they cannot be SACC-optimal.

• Proposition 7 implies that if a hypothesis H is a superset of a hypothesis H (cid:48)
and both have equal tp values, we may prune all non-recursice specializations
of H as they cannot be SACC-optimal.

• Proposition 8 implies that if a hypothesis H is a specialization of a hypothesis H (cid:48)
and both have equal tn values, we may prune all non-recursive generalizations
of H as they cannot be SACC-optimal.

CHAPTER 4. RELAXED LFF FRAMEWORK

36

• Proposition 9 implies that if a hypothesis H entails all positive examples, we
may prune all larger generalizations of H as they cannot be SACC-optimal.

• Proposition 10 implies that if a hypothesis H entails no negative examples, we
may prune all larger specializations of H as they cannot be SACC-optimal.

4.3 Relaxed LFF Hypothesis Constraints with Hy-

pothesis Size

Since an LFF solution must entail all positive examples and no negative examples, it

is likely to signiﬁcantly overﬁt the data in the presence of noise. In ILP, overﬁtting

often is seen through overly large programs with extra clauses speciﬁcally covering

noisy examples, as we can illustrate:

Example 14 (Large Overﬁtting Hypotheses) Suppose our sets of examples for

a east-west trains problem are as follows:

E+ = {eastbound(train1)., eastbound(train2)., eastbound(train3)}
E− = {eastbound(train4.)}

with background knowledge:

has car(train1, car1)., two wheels(car1)., long(car1).,
has car(train2, car2)., two wheels(car2)., roof closed(car2).,
has car(train3, car3)., three wheels(car3)., short(car3).,
has car(train4, car4)., three wheels(car4).

Also suppose that the eastbound(train3). fact is noisy and should truly be a
If it had been correctly classiﬁed, we see that an LFF optimal
negative example.

solution to this problem would be:

h1 = (cid:8) eastbound(A) :- has car(A,B), two wheels(B). (cid:9)

However, because of the noisy fact, we are required to add an additional clause to

entail this fact. So, an LFF optimal solution to this noisy problem could be:

h2 =

(cid:26) eastbound(A) :- has car(A,B), two wheels(B).

(cid:27)

eastbound(A) :- has car(A,B), three wheels(B), short(B).

Note that h2 has nearly over double the literals as h1. Like in many machine learning
methods, the size or complexity of the model can be correlated with overﬁtting [30].

CHAPTER 4. RELAXED LFF FRAMEWORK

37

In the worst case for an ILP problem, we could generate a program in which there is

exactly one clause entailing a single positive fact, meaning that the entire program
would contain |E+| clauses. For instance, a naive program for the example above
would be:

h3 =






eastbound(A) :- has car(A,B), two wheels(B), long(B).
eastbound(A) :- has car(A,B), two wheels(B), roof closed(B).
eastbound(A) :- has car(A,B), three wheels(B), short(B).






or equivalently, we generate a model which simply remembers all positive examples,
storing each in its entirety. But in h3, we can clearly see how the ﬁrst two clauses
could simply be combined into eastbound(A) :- has car(A,B), two wheels(B).
which is a generalization of both. Additionally, a naive hypothesis such as this may
generate clauses which subsume each other, making the subsumed clauses redundant.

Most importantly, such a hypothesis would not generalize at all to data outside of

the training set: unless any inputs given to the program were in the training set, the

program will always return false. Though constructing such hypotheses is trivial, it
will overﬁt any noisy data, is not useful, and is impractically large given large E+.

Application of Minimum Description Length Principle This provides clear
motivation why optimality in ILP is typically tied to the size of programs. With

the presence of noise, the relaxed LFF setting has to be particularly cognizant of

overﬁtting and avoid exceptionally large programs or programs where single clauses

entail one single positive example. To this end, we look to apply the minimum
description length (MDL) principle, a common method used for machine learning

model selection [41, 2, 19]. At a high level, the MDL principle states that the optimal

hypothesis or theory is one where the sum of the theory length and the length of the

training data when encoded with that theory is a minimum. If we apply this idea
to Example 14 above, hypothesis h2 ﬁts all four examples perfectly, but has a size of
seven literals. Hypothesis h1 however ﬁts three of the examples with a size of only
three literals. Taking the sum of correctly ﬁt examples and programs length yields
totals of 11 for h1 and 6 for h1. Under the MDL setting, we would claim h1 is more
optimal than h2 for this particular encoding. An interested reader should consult [42]
and [19] for more details on MDL and its applications to machine learning.

CHAPTER 4. RELAXED LFF FRAMEWORK

38

In order to apply the MDL principle, we deﬁne an alternative hypothesis scoring

method which takes into account the hypothesis size, recalling from Deﬁnition 14

size(H) equals the number of literals in hypothesis H.

Deﬁnition 32 (MDL Score) Given an input tuple (B, D, C, E+, E−) and a hy-
pothesis H ∈ HD,C, the function SM DL = tp(H, B, E+) + tn(H, B, E−) − size(H).

Note that this deﬁnition is similar to what the Aleph refers to as its compression
evaluation function [45]. With this deﬁnition, we can deﬁne several additional cir-
cumstances when hypotheses are known to be suboptimal under SM DL scoring. First,
we consider comparing two hypotheses with one another:

Proposition 11 Given problem input (B, D, C, E+, E−, SM DL) and hypotheses H1,
H2, and H3 in HD,C where H3 is a generalization of H1, if
SM DL(H2, B, E+, E−)−SM DL(H1, B, E+, E−) > f n(H1, B, E+)−(size(H3)−size(H2)),
then SM DL(H2, B, E+, E−) > SM DL(H3, B, E+, E−).

Proof. The improvement in the MDL score from H1 to H2 can be quantiﬁed by:

SM DL(H2, B, E+, E−) − SM DL(H1, B, E+, E−) > f n(H1, B, E+) − (size(H3) − size(H1))

SM DL(H2, B, E+, E−) − [tp(H1, B, E+) + tn(H1, B, E−) − size(H1)] > |E+| − tp(H1, B, E+) − (size(H3) − size(H1))

SM DL(H2, B, E+, E−) > |E+| + tn(H1, B, E−) − size(H3)

≥ tp(H3, B, E+) + tn(H3, B, E−) − size(H3)
= SM DL(H3, B, E+, E−)

The second to last line following from Proposition 2 and that H3 is a generalization
of H1 in addition to the fact |E+| ≥ tp(H3, B, E+)

Given SM DL(H2, B, E+, E−) and SM DL(H1, B, E+, E−), we can quantify the exact
size of H3 when this holds:

SM DL(H2, B, E+, E−) > |E+| + tp(H1, B, E+) − size(H3)

SM DL(H2, B, E+, E−) − |E+| − tn(H1, B, E−) > −size(H3)
|E+| + tn(H1, B, E−) − SM DL(H2, B, E+, E−) < size(H3)

Proposition 12 Given problem input (B, D, C, E+, E−, SM DL) and hypotheses H1,
H2, and H3 in HD,C where H3 is a specialization of H1, if

CHAPTER 4. RELAXED LFF FRAMEWORK

39

SM DL(H2, B, E+, E−)−SM DL(H1, B, E+, E−) > f p(H1, B, E−)−(size(H3)−size(H2)),
then SM DL(H2, B, E+, E−) > SM DL(H3, B, E+, E−).

Proof. The improvement in the MDL score from H1 to H2 can be quantiﬁed by:

SM DL(H2, B, E+, E−) − SM DL(H1, B, E+, E−) > f p(H1, B, E−) − (size(H3) − size(H1))

SM DL(H2, B, E+, E−) − [tp(H1, B, E+) + tn(H1, B, E−) − size(H1)] > |E−| − tn(H1, B, E−) − (size(H3) − size(H1))

SM DL(H2, B, E+, E−) > |E−| + tp(H1, B, E+) − size(H3)

1, B, E−) + tp(H3, B,+ ) − size(H3)

≥ tn(H (cid:48)
= SM DL(H3, B, E+, E−)

The second to last line following from Proposition 3 and that H3 is a specialization
of H1 in addition to the fact |E−| ≥ tn(H3, B, E−).

Given SM DL(H2, B, E+, E−) and SM DL(H1, B, E+, E−), we can quantify the exact
size of H (cid:48)

1 when this holds:

SM DL(H2, B, E+, E−) > |E−| + tp(H1, B, E+) − size(H3)

SM DL(H2, B, E+, E−) − |E−| − tp(H1, B, E+) > −size(H3)
|E−| + tp(H1, B, E+) − SM DL(H2, B, E+, E−) < size(H3)

Additionally, we can identify some situations that do not rely on comparing hypothe-
ses. Recall that for a hypothesis H, we can write f n(H, B, E+) = |E+|−tp(H, B, E+)
and similarly f p(H, B, E−) = |E−| − tn(H, B, E−).

Proposition 13 Given problem input (B, D, C, E+, E−, SM DL) and hypotheses H
and H (cid:48) in HD,C where H (cid:48) is a generalization of H, if size(H (cid:48)) > f n(H, B, E+) +
size(H), then SM DL(H, B, E+, E−) > SM DL(H (cid:48), B, E+, E−).

Proof. By Propsition 2 the maximum value for SM DL(H (cid:48), B, E+, E−) is:

SM DL(H (cid:48), B, E+, E−) = |E+| + tn(H, B, E−) − size(H (cid:48))

< |E+| + tn(H, B, E−) − [|E+| − tp(H, B, E+) + size(H)]

= tp(H, B, E+) + tn(H, B, E−) − size(H)
= SM DL(H, B, E+, E−)

CHAPTER 4. RELAXED LFF FRAMEWORK

40

Proposition 14 Given problem input (B, D, C, E+, E−, SM DL) and hypotheses H
and H (cid:48) in HD,C where H (cid:48) is a specialization of H, if size(H (cid:48)) > f p(H, B, E−) +
size(H), then SM DL(H, B, E+, E−) > SM DL(H (cid:48), B, E+, E−).

Proof. By Proposition 3 the maximum value for SM DL(H (cid:48), B, E+, E−) is:

SM DL(H (cid:48), B, E+, E−) = |E−| + tp(H, B, E+) − size(H (cid:48))

< |E−| + tp(H, B, E+) − [|E−| − tn(H, B, E−) + size(H)]

= tp(H, B, E+) + tn(H, B, E−) − size(H)
= SM DL(H, B, E+, E−)

4.3.1 Hypothesis Constraints with Hypothesis Size Applica-

tions

These propositions all apply in any ILP setting, however, in our relaxed LFF setting,

they determine sets of programs of speciﬁc lengths which should be pruned should

certain conditions hold:

• Proposition 11 implies that given hypotheses H1 and H2, we may prune any
hypothesis of size greater than |E+| + tn(H1, B, E−) − SM DL(H2, B, E+, E−)
which is also a generalization of H1 as they cannot be SM DL-optimal.

• Proposition 12 implies that given hypotheses H1 and H2, we may prune any
hypothesis of size greater than |E−| + tp(H1, B, E+) − SM DL(H2, B, E+, E−)
which is also a specialization of H1 as they cannot be SM DL-optimal.

• Proposition 13 implies that given hypothesis H, we may prune all generaliza-
tions of H with size greater than f n(H, B, E+) + size(H) as they cannot be
SM DL-optimal.

• Proposition 14 implies that given hypothesis H, we may prune all specializations
of H with size greater than f p(H, B, E−) + size(H) as they cannot be SM DL-
optimal.

CHAPTER 4. RELAXED LFF FRAMEWORK

41

4.4 Summary

In this chapter, we introduced the novel contribution of the relaxed LFF setting and

its problem deﬁnitions. We explained how in order to avoid the overpruning, we relax
how hypothesis constraints should be used. We next introduced and proved the orig-

inal and sound hypothesis constraints in Propositions 4-10. We next demonstrated

how the MDL principle can be applied in this setting to avoid overﬁtting by taking

into account program length. We concluded by introducing and Propositions 11-14
which describe sound hypothesis constraints which take into account hypothesis size

under this MDL scoring. In the next chapter, we discuss Noisy Popper’s implemen-

tation of the relaxed LFF framework, describing preliminaries of Normal Popper’s

implementation as needed.

Chapter 5

Noisy Popper Implementation

In this chapter, we will discuss the implementation of Noisy Popper using the re-

laxed LFF approach. We start by discussing the implementation of Normal Popper
as is necessary to understand Noisy Popper. After this, we will explain the speciﬁc

implementation diﬀerences used by Noisy Popper including its anytime algorithm ap-

proach, its use of minimal constraints to eﬃciently prune the search space, and ﬁnally
the implementation of the sound hypothesis constraints under the SACC scoring and
sound hypothesis constraints with hypothesis size under the SM DL scoring discussed
in Chapter 4.

5.1 Normal Popper Implementation

It is necessary to discuss the Normal Popper implementation as Noisy Popper is an

extension of it and uses the same structure and functions with slight modiﬁcations.

In implementation, Normal Popper combines use of the Prolog logic programming

language with ASP in a three stage generate-test-constrain loop. Algorithm 1 [13]
below illustrates these three stages of the Normal Popper algorithm. We will discuss

the generate, test, and constrain loop implementation here and provide illustrative

examples, however an interested reader should consult [13] for full details.

Generate The generate function in the Popper algorithm takes the declaration
bias, current set of hypothesis constraints as inputs along with upper bounds on

the number of literals allowed within clauses and number of clauses allowed within a

hypothesis. The hypothesis constraints determine the syntax of each valid hypothesis,

e.g., the number of clauses allowed, which predicates can appear together, which
clauses are allowed, etc. Hypothesis constraints are constructed as ASP constraints.

A deﬁned meta-language is used to encode programs from Prolog to ASP which

42

CHAPTER 5. NOISY POPPER IMPLEMENTATION

43

Algorithm 1 Normal Popper [13]
Input: E+, E−, BK, D, C, max vars, max literals, max clauses (where D is a dec-

laration bias and C is a set of constraints)

Output: LFF Solution or Empty Set

program ← generate(D, C, max vars, num literals, max clauses)
if program = ’space exhausted’ then
num literals ← num literals + 1
continue

1: num literals ← 1
2: while num literals ≤ max literals do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12: end while
13: return {}

end if
(tp, tn) ← test(E+, E−, BK, program)
if tp = |E+| and tn = |E−| then return program
end if
C ← C + learn constraints(program, tp, tn)

are then used to construct constraints. Collectively, these constraints form an ASP

problem whose answer sets consist of programs which satisfy the given constraints,

i.e., are consistent with all declaration and hypothesis constraints. Such an approach
was discussed with other ILP systems [7, 24, 20, 25, 26, 43] in Chapter 2. The

generate function returns an answer set to the current ASP problem. This answer

set represents a candidate deﬁnite program which the system considers as a potential

solution. Normal Popper additionally removes invalid hypotheses such as recursive
programs without a base case as well as redundant hypothesis in which one clause

subsumes another. If there is no answer set to the ASP problem with the speciﬁed

number of body literals num literals, then ’space exhausted’ is returned rather than a

candidate program and the number of body literals allowed is incremented by one. In
this way, Normal Popper searches the hypothesis space in order of increasing program

size.

Test After the generate stage of the Normal Popper loop, in the test function,
Normal Popper converts the provided answer set from the generate stage into a Prolog

program. The system tests this candidate program against the positive and negative

examples with the BK provided to determine which examples are entailed and which

are not. Table 5.1 [13] below illustrates the possible outcomes of the program as well as
the constraints which will later be generated from them. Note that we are using short
hard where tp = tp(program, B, E+) and tn = tn(program, B, E−). Outcomes are

CHAPTER 5. NOISY POPPER IMPLEMENTATION

44

tuples consisting of a true positive and true negative score. For example, an outcome

of (5, 0) indicates that the given hypothesis entails only ﬁve positive examples and

does not entail any negative examples (i.e. is consistent).

Outcome

tp = |E+|
0 < tp < |E+|
tp = 0

tn = |E−|

n/a
Specialization

tn < |E−|

Generalization
Specialization, Generalization

Specialization, Elimination Specialization, Generalization, Elimination

Table 5.1: The possible outcomes of testing a hypothesis with the constraints learned
by normal popper. Note that an outcome where tp = |E+| and tn = |E−| indicates
the hypothesis is a solution.

If the hypothesis is found to be a solution, that is if tp = |E+| and tn = |E−|,
the program is returned by the system. Otherwise, Normal Popper continues onto
the constrain stage which uses the failed hypothesis outcome in order to generate

additional hypothesis constraints and further prune the hypothesis space.

Constrain In the case of a failed hypothesis, Normal Popper uses the hypothesis

outcome to determine which hypothesis constraints to apply. Table 5.1 depicts the
constraints associated with each possible outcome. We will describe how a hypothe-

sis is encoded as an ASP constraint as Noisy Popper uses modiﬁed versions of these

encodings though an interested reader should consult [13] for a more detailed explana-

tion. We will explain the general form these ASP constraints take and give examples
for of generalization, specialization, and elimination constraints based on their deﬁ-

nitions from Chapter 3 as well as banish constraints which, while less essential to the

base version of Normal Popper, are critical for Noisy Popper’s implementation.

5.1.1 Hypothesis to ASP Encoding

As mentioned in Example 4 in Section 3.2.2, the meta-language which encodes atoms
to ASP programs using either head literal(Clause,Pred,Arity,Vars) for head
literals or body literal(Clause,Pred,Arity,Vars) for body literals where Clause
denotes the clause containing the literal, Pred deﬁnes the predicate symbol of the
literal, Arity deﬁnes the arity of the predicate and Vars is a tuple containing input
variables to the predicate. To do this in Normal Popper, two functions are called:
encodeHead and encodeBody as deﬁned here as they are in [13]:

CHAPTER 5. NOISY POPPER IMPLEMENTATION

45

encodeHead(Clause,Pred(Var0,...,Vark)) :=

head literal(Clause,Pred,k + 1,(encodeVar(Var0),...,encodeVar(Vark)))

encodeBody(Clause,Pred(Var0,...,Vark)) :=

body literal(Clause,Pred,k + 1,(encodeVar(Var0),...,encodeVar(Vark)))

where encodeVar converts variables to an ASP encoding.

Example 15 (Atom Encoding) If we are trying to encode eastbound(A) as a
head literal and has car(A,B) as a body literal, both in a clause C1, we would call
encodeHead(C1,eastbound(A)) and encodeBody(C1,hascar(A,B)) which would re-
turn ASP programs head literal(C1,eastbound,1,(V0)) and
body literal(C1,has car,2,(V0,V1)) respectively.

Encoding entire clauses naturally builds from encoding literals using the function
encodeClause deﬁned as it is in [13]:

encodeClause(Clause,(head:-body1,...,bodyn)) :=

encodeHead(Clause,head),
encodeBody(Clause,body1),...,encodeBody(Clause,bodyn),
assertDistinct(vars(head) ∪ vars(body1) ∪...∪ vars(bodym))

where the assertDistinct function simply imposes a pairwise inequality constraint
on all variables. For instance, if the three encoded variables are V0, V1 and V2, this
function simply returns the constraints: V0!=V1, V0!=V2, V1!=V2. Since clauses can
appear in multiple hypotheses, Normal Popper uses the clauseIdent function which
maps clauses to ASP constraints using a unique identiﬁer. This identiﬁer is used in
the ASP literal included clause(Clause,Id) which indicates that a clause Clause
includes all literals of the clause identiﬁed by Id. This leads to the inclusionRule
function as deﬁned in [13]:

inclusionRule(head:-body1,...,bodyn) :=

included clause(Cl,clauseIdent(head:-body1,...,bodyn)):-

encodeClause(Cl,(head:-body1,...,bodyn)).

This function’s head is true if all of the literals of the provided clause appear simul-

taneously in a clause. Note that this may hold true even if additional literals not in

the provided clause are present. To ensure that a provided clause appears exactly in
a program, we deﬁne the exactClause as in [13]:

CHAPTER 5. NOISY POPPER IMPLEMENTATION

46

exactClause(Clause,(head:-body1,...,bodym)) :=

included clause(Clause,clauseIdent(head:-body1,...,bodyn)),
clause size(Clause,n)

where the function clause size(Clause,n) asserts true only if the given clause con-
tains exactly n body literals.

Example 16 (Clause Encoding) If we are trying to encode the clause
eastbound(A) :- has car(A,B), two wheels(B). and we suppose that
clauseIdent(eastbound(A) :- has car(A,B), two wheels(B).) = id1, then we
deﬁne an inclusion rule with the function
inclusionRule(eastbound(A):-has car(A,B),two wheels(B)) which returns:

included clause(Cl,id1) :-

head literal(Cl,eastbound,1,(V0)),
body literal(Cl,has car,2,(V0,V1)),
body literal(Cl,two wheels,1,(V1)),
V0!=V1.

and if we wish to ensure this clause appears exactly in a hypothesis, we use the func-
tion exactClause(C1,eastbound(A):-has car(A,B),two wheels(B)) which returns:

included clause(C1,id1),
clause size(Cl,2).

Now that we have deﬁned the ASP encoding used by Normal and Noisy Popper, we
can deﬁne the exact forms which constraints take.

5.1.2 Generalization Constraints

By Deﬁnition 18, a generalization of a hypothesis H is a program which contains ex-
actly all of H’s exact clauses [13]. Thus, the generalization constraints of Deﬁnition 20

can be deﬁned as in [13]:

generalizationConstraint({Clause1, Clause2,...,Clausen}) :=

inclusionRule(Clause1),...,
inclusionRule(Clausen).
:-exactClause(C11,Clause1),...,

exactClause(C1n,Clausen).

Example 17 (Generalization Constraint Encoding) The ASP encoding for the

inclusion rule and generalization constraint of the hypothesis
h = {eastbound(A) :- has car(A,B), two wheels(B).} would be:

CHAPTER 5. NOISY POPPER IMPLEMENTATION

47

included clause(Cl,id1) :-

head literal(Cl,eastbound,1,(V0)),
body literal(Cl,has car,2,(V0,V1)),
body literal(Cl,two wheels,1,(V1)),
V0!=V1.

:-

included clause(C10,id1),
clause size(C10,2).

5.1.3 Specialization Constraints

By Deﬁnition 19, a specialization of a hypothesis H is a program which contains all

of H’s clauses, which may be specialized, and no additional clauses [13]. Thus, the

specialization constraints of Deﬁnition 21 can be deﬁned as in [13]:

specializationConstraint({Clause1, Clause2,...,Clausen}) :=

inclusionRule(Clause1),...,
inclusionRule(Clausen).
:-included clause(C11,clauseIdent(Clause1)),...,
included clause(C1n,clauseIdent(Clausen)),
assertDistinct({Cl1,...,Cln}), not clause(n).

The not clause(n) literal is only satisﬁed if there are no more than the n distinct
clauses given in the constraint. Note, since all clauses may be specialized, we do not

require clauses to be exact as we do in generalization constraints.

Example 18 (Specialization Constraint Encoding) The ASP encoding for the

inclusion rule and specialization constraint of the hypothesis:

h =

(cid:26) eastbound(A,B) :- has car(A,B), two wheels(B).
eastbound(A,B) :- has car(A,B), short(B).

(cid:27)

would be:

CHAPTER 5. NOISY POPPER IMPLEMENTATION

48

included clause(Cl,id2) :-

head literal(Cl,eastbound,1,(V0)),
body literal(Cl,has car,2,(V0,V1)),
body literal(Cl,two wheels,1,(V1)),
V0!=V1.

included clause(Cl,id3) :-

head literal(Cl,eastbound,1,(V0)),
body literal(Cl,has car,2,(V0,V1)),
body literal(Cl,short,1,(V1)),
V0!=V1.

:-

included clause(C10,id2),
included clause(C11,id3),
C10!=C11, not clause(2).

5.1.4 Elimination Constraints

As outlined in Section 3.3.3, in the case where a hypothesis H is totally incomplete,

we wish to prune all separable hypotheses which contain all clauses of H where any

of them may be specialized. Before we can describe the ASP encoding for an such a
constraint, we require the following logic programs to determine separability [13]:

non separable :- head literal( ,P,A, ), body literal( ,P,A, ).
separable :- not non separable.

With this, we can deﬁne the encoding for an elimination constraint as is done in [13]:

eliminationConstraint({Clause1, Clause2,...,Clausen}) :=

inclusionRule(Clause1),...,
inclusionRule(Clausen).
:-included clause(C11,clauseIdent(Clause1)),...,
included clause(C1n,clauseIdent(Clausen)),
separable.

Example 19 (Elimination Constraint Encoding) The ASP encoding for the

elimination constraint for the hypothesis
h = {eastbound(A) :- has car(A,B), two wheels(B).} would be:

CHAPTER 5. NOISY POPPER IMPLEMENTATION

49

included clause(Cl,id4) :-

head literal(Cl,eastbound,1,(V0)),
body literal(Cl,has car,2,(V0,V1)),
body literal(Cl,two wheels,1,(V1)),
V0!=V1.

:-

included clause(C10,id4),
separable.

5.1.5 Banish Constraints

The ﬁnal type of hypothesis constraint Normal Popper implements is to remove a

single hypothesis and is known as a banish constraint. While Normal Popper only

used this constraint for testing purposes and not in its full implementation, Noisy
Popper makes extensive use of it since in the relaxed setting it is common that a

failed hypothesis does not generate a hypothesis constraints which prunes itself from

the hypothesis space. A banish constraint simply provides that the each clause of

a given hypothesis appears in the program, non-specialized, and with no additional
clauses. The exact encoding as seen in [13] is as follows:

banishConstraint({Clause1, Clause2,...,Clausen}) :=

inclusionRule(Clause1),...,
inclusionRule(Clausen).
:-exact clause(C11,Clause1),...,
exact clause(C1n,(Clausen),
not clause(n).

Example 20 (Banish Constraint Encoding) The ASP encoding for the banish

constraint for the hypothesis
h = {eastbound(A) :- has car(A,B), two wheels(B).} would be:

included clause(Cl,id5) :-

head literal(Cl,eastbound,1,(V0)),
body literal(Cl,has car,2,(V0,V1)),
body literal(Cl,two wheels,1,(V1)),
V0!=V1.

:-

included clause(C10,id5),
clause size(C10,2),
not clause(1).

CHAPTER 5. NOISY POPPER IMPLEMENTATION

50

5.1.6 Normal Popper Worked Example

To illustrate clearly how Normal Popper works, we will consider a modiﬁed example

from [13] using the east-west trains problem. Assume we are trying to ﬁnd the
hypothesis eastbound(A) :- has car(A,B), short(B), two wheels(B). and will
only consider a small initial hypothesis space, H1:

H1 =






h1 = (cid:8) eastbound(A) :- has car(A,B),long(B). (cid:9)
h2 = (cid:8) eastbound(A) :- has car(A,B),long(A),two wheels(B). (cid:9)
h3 = (cid:8) eastbound(A) :- has car(A,B),roof closed(B). (cid:9)
h4 = (cid:8) eastbound(A) :- has car(A,B),short(B),two wheels(B). (cid:9)
h5 = (cid:8) eastbound(A) :- has car(A,B),long(B),roof closed(B). (cid:9)

(cid:26) eastbound(A) :- has car(A,B),roof closed(B).

(cid:27)

h6 =

h7 =

h8 =

h9 =

eastbound(A) :- has car(A,B),short(B).

(cid:26) eastbound(A) :- has car(A,B),roof closed(B).

(cid:27)

eastbound(A) :- has car(A,B),long(C,D),three wheels(B).
(cid:26) eastbound(B) :- has car(A,B),short(B),three wheels(B).
eastbound(A) :- has car(A,B),has load(B,C),triangle(C).
eastbound(A) :- has car(A,B),long(B).
eastbound(A) :- has car(A,B),short(B),three wheels(D,B).
eastbound(A) :- has car(A,B),short(B),two wheels(B).






(cid:27)











We will also assume we have the following set of positive examples:

E+ = {eastbound(train 1)., eastbound(train 2).}

And we will assume the following set of negative examples:

E− = {eastbound(train 3)., eastbound(train 4).}

with the BK containing the following facts about the trains:

has car(train1, car1)., short(car1)., two wheels(car1), roof closed(car1).
has car(train2, car2)., short(car2)., two wheels(car2), jagged roof(car2).
has car(train2, car3)., three wheels(B)., roof closed(car3).
has car(train3, car4)., roof closed(car4)., three wheels(car4)., short(B)
has car(train4, car5)., has load(car5,load1)., circle(load1)., two wheels(B).

Note that train2 has 2 cars. Normal Popper will ﬁrst generate the simplest hypothesis
from the search space:

h1 = (cid:8) eastbound(A,B) :- has car(A,B),long(B). (cid:9)

CHAPTER 5. NOISY POPPER IMPLEMENTATION

51

We can see that since neither train1 nor train2 contain a long car, they both will
return false when input to this hypothesis. This makes h1 a failed hypothesis as it is
totally incomplete which implies h1 is too speciﬁc. Normal Popper will generate the
following specialization constraint:

included clause(Cl,id1) :-

head literal(Cl,eastbound,1,(V0)),
body literal(Cl,has car,2,(V0,V1)),
body literal(Cl,long,1,(V1)),
V0!=V1.

:-

included clause(C10,id1),
not clause(1).

which prunes all specializations of h1 from H1, namely h2 and h5. Since h1 is totally
incomplete, Normal Popper will also generate the following elimination constraint:

:-

included clause(C10,id1),
separable.

which prunes all separable hypotheses which contain all of the clauses of h1 where
each clause may be specialized. That means that h9 is pruned from the hypothesis
space. After this pruning, our hypothesis space is left as:

H1 =






h6 =

h7 =

h8 =

h3 = (cid:8) eastbound(A) :- has car(A,B),roof closed(B). (cid:9)
h4 = (cid:8) eastbound(A) :- has car(A,B),short(B),two wheels(B). (cid:9)

(cid:26) eastbound(A) :- has car(A,B),roof closed(B).

(cid:27)

eastbound(A) :- has car(A,B),short(B).

(cid:26) eastbound(A) :- has car(A,B),roof closed(B).

eastbound(A) :- has car(A,B),long(C,D),three wheels(B).
(cid:26) eastbound(B) :- has car(A,B),short(B),three wheels(B).
eastbound(A) :- has car(A,B),has load(B,C),triangle(C).






(cid:27)

(cid:27)

The next hypothesis Normal Popper will generate is:

h3 = (cid:8) eastbound(A) :- has car(A,B),roof closed(B). (cid:9)

When we test this hypothesis, we ﬁnd that it does entail both positive examples as
both trains contain a car with a closed roof. However, h3 also entails the negative
example train3. This implies that the hypothesis is too general and thus Normal
Popper will generate the following generalization constraint:

CHAPTER 5. NOISY POPPER IMPLEMENTATION

52

included clause(Cl,id2) :-

head literal(Cl,eastbound,1,(V0)),
body literal(Cl,has car,2,(V0,V1)),
body literal(Cl,roof closed,1,(V1)),
V0!=V1.

:-

included clause(C10,id2),
clause size(C10,2).

which prunes all generalizations of h3, namely h6 and h7. Now, our hypothesis space
is left as:

H1 =






h4 = (cid:8) eastbound(A) :- has car(A,B),short(B),two wheels(B). (cid:9)
(cid:26) eastbound(B) :- has car(A,B),short(B),three wheels(B).
eastbound(A) :- has car(A,B),has load(B,C),triangle(C).

h8 =

(cid:27)






Finally, Normal Popper will generate hypothesis h4 which successfully entails all pos-
itive examples and no negative examples, making it a solution to the problem and is

thus returned.

The following sections will discuss how these constraints are adapted into Noisy Pop-

per and the modiﬁcations Noisy Popper makes to better handle noise.

5.2 Anytime Algorithm

The ﬁrst large obstacle Normal Popper presents when trying to handle noisy can be

observed in Theorem 1 from [13]:

Theorem 1 (Optimality)

:

[Normal] Popper returns an optimal solution if one

exists [13].

Thus, given any set of examples, Normal Popper will either return a solution which
entails all examples in E+ and no examples in E−, overﬁtting if noise is present, or no
hypothesis at all which equates to an empty hypothesis, i.e., a program which always

returns true. To avoid returning no hypothesis, Noisy Popper is constructed as an

anytime algorithm in which a hypothesis can be returned by the system at any point
in its runtime, regardless of whether or not that hypothesis is an optimal solution.

The approach taken in Noisy Popper consists of maintaining the best hypothesis seen
so far. That is, each hypothesis generated by Popper is scored by the SACC function

CHAPTER 5. NOISY POPPER IMPLEMENTATION

53

from Deﬁnition 31 and the hypothesis of highest score is maintained by the system.

In the case that the Popper algorithm is halted early or the entirety of the hypothesis

space is exhausted without ﬁnding a solution, the best hypothesis so far is returned.
Otherwise, if an LFF solution is found which necessarily has maximum SACC score,
that solution is returned as it would be in Normal Popper. This change to the Normal

Popper algorithm can be see in Algorithm 2 below.

Algorithm 2 Noisy Popper
Input: E+, E−, B, D, C, t, max vars, max literals, max programs, max clauses
(where B is a set of background knowledge, D is a declaration bias, C is a set of
constraints, and t is the minimal constraint threshold)

Output: Hypothesis Constraint Consistent Logic Program or Empty Set

end if
(tp, tn) ← test(E+, E−, B, program)
if tp = |E+| and tn = |E−| then return program
end if
if SACC(program, B, E+, E−) > SACC(best hypothesis, B, E+, E−) then

1: num literals ← 1
2: num programs ← 1
3: best hypothesis ← null
4: program list ← []
5: while num literals ≤ max literals and num programs ≤ max programs do
program ← generate(D, C, max vars, num literals, max clauses)
6:
if program = ’space exhausted’ then
7:
num literals ← num literals + 1
8:
continue
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27: end while
28: return best program

end if
C ← C + learn constraints(program, tp, tn)
C ← C + learn sound constraints(program,program list, B, E+, E−, tp, tn)
C ← C + learn size constraints(program, program list, B, E+, E−, tp, tn)
append(program list, program)

end if
if tn > t|E−| then
tn ← |E−|

end if
if tp > t|E+| then
tp ← |E+|

best program ← program

Note, Noisy Popper takes a max programs parameter which essentially gives a timeout

CHAPTER 5. NOISY POPPER IMPLEMENTATION

54

to the algorithm, forcing it to return whatever the current best hypothesis is after

that many programs have been considered.

5.3 Minimal Constraints

An eﬀective strategy to apply constraints is to do so minimally and only in cases

where we intuitively know that the hypotheses considered are poor. For instance,

if a hypothesis entails no positive examples, we can conﬁdently conclude that the
hypothesis is too speciﬁc, even if a portion of those examples are noisy. Likewise,

if a hypothesis entails all negative examples, we can conclude that it is much too

general regardless of noise. To this end, given a hypothesis H, we consider applying

the typical hypothesis constraints of Normal Popper as follows:

• if H is totally incomplete (i.e., tp = 0) prune all specializations of H and all

separable hypotheses which contain a specialization of H.

• if H is totally inconsistent (i.e., tn = 0) prune all generalizations of H.

• Otherwise, only prune H from the hypothesis space

Though these constraints are not sound as they may prune SACC-optimal hypotheses
they have proven very eﬀective in practice greatly improve the eﬃciency of the system

by pruning signiﬁcant chunks of the hypothesis space.

Minimal Constraint Threshold These minimal constraints arbitrarily, though

reasonably, choose a threshold at which to prune at 0, i.e., Noisy Popper should

prune normally if the tp or tn scores equal zero. However, Noisy Popper implements

this threshold as an optional hyperparameter 0 ≤ t ≤ 1 representing the percentage
of positive (resp. negative) examples which if entailed (resp. not entailed) by a

hypothesis, no pruning will occur. More speciﬁcally, typical constraints of Normal

Popper are applied for a hypothesis H as follows:

• if tp ≤ t|E+| prune all specializations of H.
hypotheses which contain a specialization of H.

If tp = 0 prune all separable

• if tn ≤ t|E−| prune all generalizations of H.

• Otherwise, only prune H from the hypothesis space

CHAPTER 5. NOISY POPPER IMPLEMENTATION

55

This implementation can be seen in lines 17-22 of the Noisy Popper algorithm which
alters the values of tp and tn to |E+| and |E−| respectively should they exceed the
threshold amounts. These modiﬁed tp and tn values are used as arguments for the
standard learn constraints function on line 23 which produces hypothesis constraints

as it did in Algorithm 1. It is common that a program may have tp and tn altered to
|E+| and |E−| respectively, i.e., complete relaxation of Normal Popper’s constraints.
In these situations, the learn constraints function only generates a banish constraint
to ensure that that hypothesis is removed from the search space. Without this, the

algorithm may consider that same hypothesis inﬁnitely. Noisy Popper was designed

to limit the use of hyperparameters as they make many ILP systems cumbersome to

use eﬀectively. By default, t = 0 which is often the most eﬀective setting.

5.4 Sound Hypothesis Constraints

The learn sound constraints function in line 24 of the Noisy Popper algorithm is im-

plemented in Algorithm 3 seen below. The algorithm compares previously generated
programs with the newly generated one to build up a set of hypothesis constraints

which are ultimately added to the ASP constraint set C. For simplicity, we will often
refer to these speciﬁc constraints as sound constraints. Note that |E+| − 1 is used to
essentially convey ”some but not all positive examples” and |E−| − 1 likewise conveys
”some but not all negative examples”. Lines 3-5 correspond to Proposition 4 and
prunes generalizations of a previously seen hypothesis if they cannot have an SACC
score higher than the new hypothesis. Likewise, lines 6-8 correspond to Proposition 5
and prune specializations of a previously seen hypothesis if they cannot have an SACC
score higher than the new hypothesis.

Lines 9-11 correspond to Propositions 6 and 7 and prune non-recursive supersets and

non-recursive specializations of the new hypothesis which has a true positive value

equal to a previous hypothesis if the new hypothesis is a superset of the previously

seen one. Lines 14-16 pertain just to Proposition 6 as we cannot prune specializations
of the new hypothesis if it is not a superset of the previously hypothesis. Lines 18-21

correspond to Proposition 8 and prune generalizations of the new hypothesis which

has a true negative value equal to a previously seen hypothesis if the new hypothesis

is a specialization of the previously seen one.

Lastly, lines 23-28 correspond to Propositions 9 and 10 and prune all generalizations
of the new hypothesis if it entails all positive examples and all specializations of the

CHAPTER 5. NOISY POPPER IMPLEMENTATION

56

new hypothesis if it entails no negative examples.

Algorithm 3 Learn Sound Hypothesis Constraints
Input: program, program list, B, E+, E−, tp, tn (where B is a set of background
knowledge, tp = tp(program,B, E+) and tn = tn(program,N, E−) as calculated
in Algorithm 1)

Output: Set of hypothesis constraints (may be empty)

if SACC(program,B, E+, E−)−SACC(p,B, E+, E−) > |E+|−tp(p,B, E+) then

constraints ← constraints + learn constraints(p, |E+|, |E−| − 1)

end if
if SACC(program,B, E+, E−)−SACC(p,B, E+, E− > |E−|−tn(p,B, E−) then

constraints ← constraints + learn constraints(p, |E+| − 1, E−)

end if
if is generalization(program, p) and tp(p, B, E+) = tp then

if p ⊆ program then

constraints ← constraints +

learn constraints non rec(program, |E+| − 1, |E−| − 1)

constraints ← constraints +

learn constraints non rec(program, |E+| − 1, |E−|)

end if
if is specialization(program, p) and tn(p,B, E−) = tn then

constraints ← constraints +

learn constraints non rec(program, |E+|, |E−| − 1))

else

1: constraints ← {}
2: for p in program list do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22: end for
23: if tp = |E+| then
24:
25: end if
26: if tn = |E−| then
27:
28: end if
29: return constraints

end if

end if

constraints ← constraints + learn constraints(program, |E+|, |E−| − 1)

constraints ← constraints + learn constraints(program, |E+| − 1, |E−|)

In Algorithm 3, while the standard learn constraints function is used to generate

ASP constraints as normal, a speciﬁc variant function learn constraints non rec is

also used to learn constraints which speciﬁcally do not prune recursive hypotheses.
In the ASP encoding, this simply requires adding not recursive to the constraint.
So, a generalization constraint which does not prune recursive hypotheses would be

deﬁned as:

CHAPTER 5. NOISY POPPER IMPLEMENTATION

57

generalizationConstraintNonRec({Clause1, Clause2,...,Clausen}) :=

inclusionRule(Clause1),...,
inclusionRule(Clausen).
:-exactClause(C11,Clause1),...,

exactClause(C1n,Clausen),
not recursive.

and likewise, an analogous specialization constraint would be deﬁned as:

specializationConstraintNonRec({Clause1, Clause2,...,Clausen}) :=

inclusionRule(Clause1),...,
inclusionRule(Clausen).
:-included clause(C11,clauseIdent(Clause1)),...,
included clause(C1n,clauseIdent(Clausen)),
assertDistinct({Cl1,...,Cln}),
not clause(n), not recursive.

Additionally, the is generalization and is specialization functions are specially im-

plemented for Noisy Popper and check for a version of subsumption as Deﬁnitions 18
and 19 outline, i.e., if every clause in H1 is subsumed by some clause in H2, then H2 is
a generalization of H1 and H1 is a specialization of H2. Notably, checking subsump-
tion is NP-complete [21] and these functions do not check for variable substitutions.

5.5 Sound Constraints with Hypothesis Size

The learn size constraints function in line 25 of the Noisy Popper algorithm is im-

plemented below in Algorithm 4. Like with Algorithm 3, this algorithm builds a list of

hypothesis constraints using hypothesis size by comparing previously generated pro-

grams with the newly generated one. We will often refer to these constraints as simply
size constraints. These constraints are ultimately added to the ASP constraint set C

in the Noisy Popper algorithm and used to prune the hypothesis space of particularly

large hypotheses.

Lines 3-5 correspond to Proposition 11 and prune all generalizations of a previ-
ously seen hypotheses of particular size as they cannot have an SM DL score greater
than that of the newly generated program and are thus not SM DL-optimal. Like-
wise, lines 6-8 correspond to Proposition 12 and prune all specializations of a previ-
ously seen hypothesis of particular size as they cannot be SM DL-optimal. Note that
even in the case where the new hypothesis performs exceptionally poorly, these will

CHAPTER 5. NOISY POPPER IMPLEMENTATION

58

Algorithm 4 Learn Sound Hypothesis Constraints with Hypothesis Size
Input: program, program list, B, E+, E−, tp, tn (where B is a set of background
knowledge, tp = tp(program,B, E+) and tn = tn(program,N, E−) as calculated
in Algorithm 1)

Output: Set of hypothesis constraints (may be empty)

gen size1 ← |E+| + tn(p,B, E−) − SM DL(program,B, E+, E−)
constraints ← constraints +

learn constraints with size(p, |E+|, |E−| − 1 gen size1)
spec size1 ← |E−| + tp(p,B, E+) − SM DL(program,B, E+, E−)
constraints ← constraints +

learn constraints with size(p, |E+| − 1, |E−|, spec size1)

1: constraints ← {}
2: for p in program list do
3:
4:
5:
6:
7:
8:
9: end for
10: gen size2 ← |E+|− tp + size(program)
11: constraints ← constraints +
12:
13: spec size2 ← |E−|− tn + size(program)
14: constraints ← constraints +
15:
16: return constraints

learn constraints with size(program, |E+|, |E−| − 1, gen size2)

learn constraints with size(program, |E+| − 1, |E−|, spec size2)

still generate hypothesis constraints and remove exceptionally large generalizations

and specializations from the hypothesis space. Lines 9-11 correspond to Proposi-

tion 13 and prunes all generalizations of the new hypothesis H with size greater
than f n(H, B, E+) + size(H) as these can never have a greater SM DL score than
H and are thus not SM DL-optimal. Likewise, lines 12-14 correspond to Proposi-
tion 14 and prunes all specializations of the new hypothesis H with size greater than
f p(H, B, E−) + size(H) as these cannot be SM DL-optimal.

Like with Algorithm 3, Algorithm 4 also introduces a new modiﬁed version of the
learn constraints called learn constraints with size which generates variants of the

typical ASP constraints, taking an additional size argument. These ASP constraints

only prune hypothesis which have a size greater than the given size argument. A

generalization constraint which only prunes hypotheses of particular size would be
deﬁned as:

CHAPTER 5. NOISY POPPER IMPLEMENTATION

59

generalizationConstraintWithSize({Clause1, Clause2,...,Clausen}, size) :=

inclusionRule(Clause1),...,
inclusionRule(Clausen).
:-exactClause(C11,Clause1),...,

exactClause(C1n,Clausen),
program size(N), size < N.

where program size(N) holds true only if the number of body literals in the given
program equals N. Likewise, an analogous specialization constraint would be deﬁned
as:

specializationConstraintWithSize({Clause1, Clause2,...,Clausen}, size) :=

inclusionRule(Clause1),...,
inclusionRule(Clausen).
:-included clause(C11,clauseIdent(Clause1)),...,
included clause(C1n,clauseIdent(Clausen)),
assertDistinct({Cl1,...,Cln}),
not clause(n), program size(N), size < N.

Learning from All Previous Hypotheses The learn sound constraints and

learn size constraints algorithms takes a list of programs with which to compare

the most recently generated hypothesis with. From line 26 in the Noisy Popper
algorithm, we can see that this list is composed of all previously seen hypotheses

that the system has generated to that point. While the original motivation behind

the sound constraints was to learn from the best hypothesis as it was continuously

being maintained, Propositions 4-14 all hold when comparing any two hypotheses.
Thus, we can generate additional constraints by comparing any new hypothesis with

a running list of all previously encountered and scored hypotheses. We motivate this

through an example:

Example 21 (Comparing to All Previous Hypotheses) Consider an east-west
trains problem with BK B and |E+| = |E−| = 5. Assume we have already observed
the following hypotheses:

h1 = (cid:8) eastbound(A) :- has car(A,B),short(B). (cid:9)
h2 = (cid:8) eastbound(A) :- has car(A,B),long(B). (cid:9)

where tp(h1,B, E+) = 2, tn(h1,B, E−) = 0, tp(h2,B, E+) = 0, and tn(h2,B, E−) = 2.
Now, consider the next hypothesis generated is h3 = {eastbound(A) :- has car(A,B),
three wheels(B).} which has SACC(h3,B, E+, E−) = 6.

CHAPTER 5. NOISY POPPER IMPLEMENTATION

60

Since SACC(h3,B, E+, E−) − SACC(h1,B, E+, E−) > |E+| − tp(h1,B, E+), by Propo-
sition 4 we may prune all generalizations of h1. Similarly, since
SACC(h3,B, E+, E−) − SACC(h2,B, E+, E−) > |E−| − tn(h2,B, E−), by Proposition 5
we may prune all specializations of h1. If we had not maintained both h1 and h2, we
would have only been able to identify one of these hypothesis constraints. Thus, the

number of hypothesis constraints we can generate can increase as more hypotheses

are maintained for comparison.

Comparing to all previous hypotheses provides the system with noticeable improve-

ment in practice though at the cost of signiﬁcant ineﬃciencies as repeated subsump-

tion checks are taxing. Steps are taken in the Noisy Popper implementation to avoid
redundant constraint generation as much as possible. Lists are maintained to keep

track of programs which have had their generalizations and specializations pruned.

Should a program have its generalizations pruned, it is removed from the respective

list to ensure it is not checked again and similar actions are taken for specializa-
tions. Programs removed from the generalizations list additionally cannot generate

any generalization constraints with hypothesis size as these would be similarly redun-

dant and likewise for specialization constraints with hypothesis size. Even with these

changes, we still may loop over every previously seen hypothesis with each generate-
test-constrain loop of Algorithm 2. Thus, given N hypothesis in the hypothesis space,
we may make O(N 2) total hypotheses comparisons in both Algorithm 3 and Algorithm
4. Each comparison may additional check for incomplete subsumption as speciﬁed

previously which, if we assume each program has at most C clauses each with at
most L literals takes O((CL)2). Thus, the additional code used to modify Normal
Popper to Noisy Popper has a worst-case runtime of O((N CL)2). Again, checking
subsumption in full is NP-complete [21] and we are using incomplete subsumption

checks here.

5.6 Noisy Popper Worked Example

To illustrate how Noisy Popper works, we will consider another east-west trains
problem. Again assume we are trying to ﬁnd the hypothesis eastbound(A) :-
has car(A,B), short(B), two wheels(B). We will also assume that t = 0 is the
minimal constraint threshold used and consider only a small initial hypothesis space,
H2:

CHAPTER 5. NOISY POPPER IMPLEMENTATION

61

H2 =






h1 = (cid:8) eastbound(A) :- has car(A,B),long(B). (cid:9)
h2 = (cid:8) eastbound(A) :- has car(A,B),long(A),two wheels(B). (cid:9)
h3 = (cid:8) eastbound(A) :- has car(A,B),short(B). (cid:9)
h4 = (cid:8) eastbound(A) :- has car(A,B),short(B),two wheels(B). (cid:9)
h5 = (cid:8) eastbound(A) :- has car(A,B),long(B),roof closed(B). (cid:9)

h6 =

h7 =

h8 =

(cid:26) eastbound(A) :- has car(A,B),roof closed(B),three wheels(B).

eastbound(A) :- has car(A,B),short(B).

(cid:26) eastbound(A) :- has car(A,B),short(B),two wheels(B).

(cid:27)

eastbound(A) :- has car(A,B),roof closed(B),two wheels(B).
eastbound(A) :- has car(A,B),long(B).
eastbound(A) :- has car(A,B),short(B),three wheels(D,B).
eastbound(A) :- has car(A,B),short(B),two wheels(B).











(cid:27)






We will also assume similar sets of examples as in the worked example in Section 5.1.6,
but with the addition of a noisy positive examples eastbound(train5).:

E+ = {eastbound(train 1)., eastbound(train 2).,eastbound(train5).}
E− = {eastbound(train 3)., eastbound(train 4).}

where the BK is again the same as in Section 5.1.6 but with the added facts for
train5

has car(train5, car6)., two wheels(car6), roof closed(car6).

Noisy Popper will ﬁrst generate hypothesis h1 = {eastbound(A):-has car(A,B),long(B).}
from the hypothesis space. Since no train contains a long car, no example is entailed,
positive or negative, giving SACC = 2. Being the ﬁrst hypothesis considered, this is
saved as the best hypothesis. Because h1 entails no positive examples and no negative
examples, the minimal constraints will not alter the outcome from (tp = 0, tn = 2).

This means that a specialization constraint and elimination constraint are generated
as they are in Section 5.1.6, pruning all specializations of h1, namely h2 and h5 as
well as all separable hypotheses which contain specializations of h1, namely h8 (line
23 of Algorithm 2).

Note that the learn sound constraints function would normally produce an iden-
tical specialization constraint as above since tn(h1,B, E−) = |E−| (lines 26-27 of
Algorithm 3), but this is avoided in implementation as it would be redundant. Simi-
larly, the learn size constraints function would produce constraints pruning all gen-
eralizations of h1 with size greater than |E+| − tp(h1,B, E+) + size(h1) = 5 (lines
10-12 of Algorithm 4) and pruning all specializations of h1 with size greater than

CHAPTER 5. NOISY POPPER IMPLEMENTATION

62

|E−| − tn(h1,B, E−) + size(h1) = 2 (lines 13-15 of Algorithm 4), but this last con-
straint is also avoided in implementation as it is redundant. These functions produce
no other constraints as there are no previous programs with which to compare h1 to.
This leaves the hypothesis space as:

H2 =






h6 =

h7 =

h3 = (cid:8) eastbound(A) :- has car(A,B),short(B). (cid:9)
h4 = (cid:8) eastbound(A) :- has car(A,B),short(B),two wheels(B). (cid:9)

(cid:26) eastbound(A) :- has car(A,B),roof closed(B),three wheels(B).

eastbound(A) :- has car(A,B),short(B).

(cid:26) eastbound(A) :- has car(A,B),short(B),two wheels(B).

(cid:27)

eastbound(A) :- has car(A,B),roof closed(B),two wheels(B).

(cid:27)






Noisy Popper will next generate hypothesis h2 = {estbound(A):-has car(A,B),short(B).}.
Since train1, train2, and train3 contain short cars, two positive and one negative
example will be entailed, giving SACC(h2,B, E+, E−) = 3. Noisy Popper will replace h1
with h2 as the new best hypothesis. Since the tp(h2,B, E+) > 0 and tn(h2,B, E−) > 0,
in Normal Popper the generalizations and specializations of h2 would be pruned, in-
cluding the true solution h4. However, due to the constraint relaxation, the values of
tp and tn are changed to |E+| and |E−| respectively (lines 17-22 of Algorithm 2), so
only a banish constraint is generated by learn constraints in line 23 of Algorithm 2.
No constraints are generated by learn sound constraints and learn size constraints

as these are all again redundant or do not eﬀect our hypothesis space. The only pro-
gram removed is h2 through the banish constraint leaving the hypothesis space as:

H2 =






h6 =

h7 =

h4 = (cid:8) eastbound(A) :- has car(A,B),short(B),two wheels(B). (cid:9)

(cid:26) eastbound(A) :- has car(A,B),roof closed(B),three wheels(B).

eastbound(A) :- has car(A,B),short(B).

(cid:26) eastbound(A) :- has car(A,B),short(B),two wheels(B).

(cid:27)

eastbound(A) :- has car(A,B),roof closed(B),two wheels(B).

(cid:27)






The next hypothesis generated is h4 = {eastbound(A):-has car(A,B),
short(B),two wheels(B).} which entails no negative examples and all but the single
noisy positive example. This gives SACC(h4) = 4 and thus it will be maintained as
the new best hypothesis, though since it does not entail all positive and no negative
examples, it is not immediately returned. Again, due to the constraint relaxation, the
outcome is values for tp and tn are changed to |E+| and |E−| respectively, avoiding
pruning all specializations of h4 as would be done in Normal Popper (lines 17-22 of

CHAPTER 5. NOISY POPPER IMPLEMENTATION

63

Algorithm 2). However, all specializations of h4 are pruned regardless as it entails no
negative examples (lines 30-31 of Algorithm 3). learn size constraints will generate
a constraint which prunes all generalizations of previously seen hypothesis h3 with size
greater than |E+| + tn(h3,B, E−) − SM DL(h4,B, E+, E−) = 4 (lines 3-5 of Algorithm
4). The generalization with size constraint created would be:

included clause(Cl,id1) :-

head literal(Cl,eastbound,1,(V0)),
body literal(Cl,has car,2,(V0,V1)),
body literal(Cl,short,1,(V1)),
V0!=V1, V0!=V2, V1!=V2.

:-

included clause(C10,id2),
clause size(C10,2),
program size(N), 4 < N.

which would prune h6 from the hypothesis space. learn size constraints would also
generate a constraint which prunes all generalizations of h4 of size greater than |E+|−
tp(h4,B, E+) + size(h4) = 4 (lines 10-12 of Algorithm 4). The generalization with size
constraint generated would be similar to the one above but with one additional literal
encoded. This prunes h7 from the hypothesis space which is notable as this hypothesis
perfectly ﬁts the data, but would overﬁt as it would entail the noisy positive example.
Lastly, h4 itself is pruned from the hypothesis space via a banish constraint leaving
H2 empty. Since no solution was found, the best maintained hypothesis is returned
instead, meaning that h4 is correctly returned.

5.7 Summary

In this chapter, we discussed the implementation details of the Normal Popper system

including its generate-test-constraint loop and how it encodes programs and hypoth-

esis constraints into an ASP problem. We then discussed how Noisy Popper modiﬁes
Normal Popper to be an anytime algorithm and demonstrated how it implements

unsound minimal constraints to prune typically suboptimal hypothesis. Next, we

described the algorithm which generates sound hypothesis constraints or sound con-

straints based on Propositions 4-10 in Chapter 4 including necessary changes to the
ASP constraint encodings. We likewise described the algorithm which generates sound

hypothesis constraints which take into account hypothesis size or size constraints un-

der the MDL scoring based on Propositions 11-14. Finally, we demonstrated Noisy

CHAPTER 5. NOISY POPPER IMPLEMENTATION

64

Popper through a worked example. In the next chapter, we will describe the experi-

ments and results used to compare Noisy Popper to Normal Popper and experiments

which compare the eﬀectiveness of the individual components of Noisy Popper.

Chapter 6

Experimental Results

In this chapter we will empirically explore the capabilities of Noisy Popper. Namely,

we aim to determine the validity of the claims made in Chapter 1 which stated that
Noisy Popper is more capable of generalizing to noisy data than Normal Popper and

that without noise, Noisy Popper still generalizes well, though less eﬃciently than

Normal Popper. To this end, the following experimental questions are formulated:

Q1. How well does Noisy Popper generalize to datasets with varying levels of noise
in comparison to Normal Popper?

To answer this question, we compare the two systems directly on several problems

commonly found in the literature. We will compare purely their accuracies over

various amounts of noise including without noise. Though we will brieﬂy compare

the two systems as they are, this is a slightly unfair comparison as Normal Popper
will typically return an empty solution in the presence of noise. Thus, for most of the

experiments we will enhance Normal Popper as an anytime algorithm as we did for

Noisy Popper.

Q2. How ineﬃcient is Noisy Popper in comparison to Normal Popper?

To answer this question, we will again test the two systems against several datasets,
this time measuring the time it takes for the systems to complete.

A natural question regarding the eﬀectiveness of Noisy Popper is to what degree do

each enhancement impacts learning, both in speed and accuracy. Thus, the following

question should be posed:

Q3. How signiﬁcantly does each enhancement within Noisy Popper impact its learn-

ing capabilities and eﬃciency.

65

CHAPTER 6. EXPERIMENTAL RESULTS

66

To answer this question, we will evaluate Noisy Popper as a whole with versions

of Noisy Popper without (i) minimal constraints (ii) sound constraints and (iii) size

constraints in addition to a completely relaxed brute force version of Normal Popper,
Enumerate. The accuracies and completion times of each system will be compared

to determine the impact of each in various settings.

6.1 Noisy Popper vs. Normal Popper

The purpose of this ﬁrst set of experiments is to evaluate how well Noisy Popper gen-

eralizes to noisy and noiseless datasets in comparison to Normal Popper, measuring

both the predictive accuracy of the systems as well as the time it take both to run. We

will evaluate both systems over several diverse problem sets commonly found in the
literature: two Michalski’s east-west trains problem variants, several program synthe-

sis list transformation problems, and two inductive general game playing (IGGP [12])

problems. These datsets will also be used for experiments in the following sections.

6.1.1 Experiment 1: East-West Trains

This series of problems consists of learning eastbound target predicates for two vari-
ations on Michalski’s east-west trains problem as described in Chapter 1 and used

throughout this paper. Such problems are easy for a system to overﬁt and will help

determine how eﬀective Noisy Popper is at generalizing to noisy data.

Materials Noisy Popper and Normal Popper will be evaluated on two diﬀerent east-

west trains problems with data generated from the following ground truth hypotheses:

(cid:26) eastbound(A) :-

(cid:27)

has car(A,C),long(C),roof closed(C),has car(A,B),three wheels(B).

(cid:27)

h1 =

h2 =

(cid:26) eastbound(A) :-

has car(A,C),roof open(C),has car(A,B),roof closed(B).

Both systems are given identical BK containing the descriptions of all 999 trains via re-
lations has car/2, has load/2, short/1, long/1, two wheels/1, three wheels,
etc.

The language biases for Normal and Noisy Popper restrict hypotheses to at most six

unique variables, at most size body literals per clause, and at most three clauses. The

systems are given type and directions (i.e., input or output) for arguments of each

CHAPTER 6. EXPERIMENTAL RESULTS

67

predicate. The minimal constraint threshold for Noisy Poppper is set to its default

t = 0. For one experiment, Normal Popper will be run as is with no modiﬁcation. This

will adequately demonstrate Normal Popper’s inability to generalize at all to noisy
data as it will be unable to ﬁnd an LFF solution before the given system timeout.

Out of fairness, the rest of the experiments here and moving forward will run Normal

Popper enhanced as an anytime algorithm which, like Noisy Popper, maintains its

best seen hypothesis and returns it if no LFF solution is found. Normal Popper will
still generate constraints as normal with this enhancement.

Methods For each hypothesis above, 50 positive and 50 negative randomly selected

examples will be generated for training while 200 positive and 200 negative randomly

selected examples will be generated for testing. Example trains are selected randomly
using the two hypotheses from the pool of 999 deﬁned in the BK. Should a system

fail to return any hypothesis, we will assume that all examples are entailed giving a

default predictive accuracy of 50% in this instance. A timeout of ten minutes and

a limit of 200 generated programs is enforced per task which ensures both systems
can learn from the same number of programs. The predictive accuracy and learning

times are recorded for each task and each experiment will repeat the task ten times

with the means and standard errors plotted. Each experiment will be repeated for

training noise levels from 0% to 40% in increments of 10% with 5% additionally being
tested. The test sets will remain noiseless.

Results and Analysis Table 6.1 below shows that when compared to Normal

Popper unenhanced by an anytime algorithm, Noisy Popper far exceeds its predictive

accuracy. Normal Popper achieving 50% accuracy indicates that the was unable to
ﬁnd an LFF solution to the task in the 200 program limit allotted and thus is given the

default predictive accuracy. This is an expected and uninteresting result as Normal

Popper’s inability to return non-LFF solutions has already been discussed.

Target Program Training Noise (%) Normal Popper Noisy Popper

h1

h2

0
5
10
20

0
5
10
20

100±0
50±0
50±0
50±0

100±0
50±0
50±0
50±0

100±0
100±0
100±0
100±0

100±0
100±0
100±0
100±0

Table 6.1: East-West Trains predictive accuracy for programs h1 and h2 with Normal
Popper not enhanced as an anytime algorithm. The error is standard

CHAPTER 6. EXPERIMENTAL RESULTS

68

Figures 6.1 and 6.2 below compare Normal Popper enhanced as an anytime algorithm

with Noisy Popper. In terms of predictive accuracy, Noisy Popper outperforms Nor-

mal Popper at all noise levels greater than 0% for both problems and ties as expected
at 0% noise with perfect predictive accuracy. A McNemar’s [22] test on the Noisy

and Normal Popper predictive accuracy additionally conﬁrmed the signiﬁcance at the

p < 0.001 level for both problems. For most noise levels, Normal Popper typically

either overﬁts the data, returning a hypothesis with several extra clauses, or underﬁts
the data having pruned the correct solution from the hypothesis space early on. Noisy

Popper’s relaxed setting helps avoid this over pruning. Over 30% noise, Noisy Popper

also begins overﬁtting the data though still produces higher predictive accuracy than

Normal Popper. Normal Popper consistently ran much quicker than Noisy Popper,
running in under two seconds regardless of noise level. Noisy Popper’s learning time is

much more volatile, dependent on the number of constraints Noisy Popper generates.

Precise reasons for this ineﬃciency are discussed in the second section of experiments.

However, with no noise, both systems take roughly the same amount of time to learn
the correct solution. Overall, these results suggest that the answer to Q1 is that

Noisy Popper generalizes better than Normal Popper under and generalizes as well

as Normal Popper when no noise is present. It also suggests that the answer is Q2

is that Noisy Popper is signiﬁcantly less eﬃcient than Normal Popper and can be at
most around 20 times slower than Normal Popper with these datasets.

)

%

(

y
c
a
r
u
c
c
A
e
v
i
t
c
i
d
e
r
P

100

95

90

85

)
s
d
n
o
c
e
s
(

i

e
m
T
g
n
i
n
r
a
e
L

20

10

0

0

Normal Popper
Noisy Popper

10

20
Noise (%)

30

40

Normal Popper
Noisy Popper

0

10

20
Noise (%)

30

40

Figure 6.1: East-West Trains predictive accuracy and learning time (in seconds) for
program h1 when varying percentage of noisy training data. Standard error is depicted
by bars.

CHAPTER 6. EXPERIMENTAL RESULTS

69

)

%

(

y
c
a
r
u
c
c
A
e
v
i
t
c
i
d
e
r
P

100

90

80

)
s
d
n
o
c
e
s
(

i

e
m
T
g
n

i

n
r
a
e
L

20

10

0

0

Normal Popper
Noisy Popper

10

20
Noise (%)

30

40

Normal Popper
Noisy Popper

0

10

20
Noise (%)

30

40

Figure 6.2: East-West Trains predictive accuracy and learning time (in seconds) for
program h2 when varying percentage of noisy training data. Standard error is depicted
by bars.

6.1.2 Experiment 2: List Manipulations

This series of problems consists of learning target predicates which manipulate or

check certain properties of number lists and serves as an example of program synthesis.
These problems are diﬃcult typically requiring recursive solutions. Learning recursive

programs has often been considered a diﬃcult though important task for ILP systems

[11].

Materials Noisy Popper and Normal Popper will be evaluated on the nine list
manipulation problems seen below in Table 6.2 from [13]. These have been shown

to be challenging for many ILP systems unless strong inductive biases are provided

with the exception of Normal Popper which has demonstrated near perfect accuracy

on each task in a noiseless setting [13].

Both systems are given identical BK containing some of the monadic (i.e. one ar-
gument) relations empty, even, odd, one, and zero, dyadic (i.e., two argument)
relations decrement, head, geq, increment, member and tail, and triadic (i.e.,
three argument) relations append and prepend in order to construct solutions.

The language biases for Normal and Noisy Popper restrict hypotheses to at most ﬁve
unique variables, at most ﬁve body literals per clause, and at most two clauses. The

systems are again given type and directions for arguments of each predicate as well as

a timeout to prevent non-terminating programs from running inﬁnitely. The minimal

constraint threshold for Noisy Popper is set to its default t = 0. Normal Popper will

CHAPTER 6. EXPERIMENTAL RESULTS

70

Name

Description

Example Solution

addhead

Prepend head of list three times

addhead(A,B):-head(A,C),cons(C,A,D),cons(C,D,E),cons(C,E,B).

droplast

Drop the last element of the list

droplast(A,B):-tail(A,B),empty(B).
droplast(A,B):-tail(A,C),droplast(C,D),head(A,E),cons(E,D,B).

evens

Check all elements are even

finddup

Find duplicate elements

last

Last element of list

len

Calculates list length

member

Member of the list

evens(A):-empty(A).
evens(A):-head(A,B),even(B),tail(A,C),even(C)

finddup(A,B):-head(A,B),tail(A,C),member(B,C).
finddup(A,B):-tail(A,C),finddup(C,B).

last (A,B):-tail(A,C),empty(C),head(A,B).
last (A,B):-tail(A,C),last (C,B).

len(A,B):-empty(A),zero(B).
len(A,B):-tail(A,C),len(C,D),increment(D,B).

member(A,B):-head(A,B).
member(A,B):-tail(A,C),member(C,B).

sorted

Checks if list is sorted

sorted(A):-tail(A,B),empty(B).
sorted(A):-head(A,B),tail(A,C),head(C,D),geq(D,B),sorted(C).

threesame First three elements are identical threesame(A):-head(A,B),tail(A,C),head(C,B),tail(C,D),head(D,B).

Table 6.2: List manipulation problems with descriptions and example solutions [13].

also be enhanced with an anytime algorithm approach as is done in Experiment 1

above.

Methods For each task, 20 positive and 20 negative randomly generated exam-

ples are used for training while 1000 positive and 1000 negative randomly generated
examples are used for testing. List elements are sampled uniformly from the set

{1, 2, ..., 100}. A timeout of ten minutes and a limit of 500 generated programs is

enforced per task, even if this prevents a system from ﬁnding a more accurate so-

lution. The predictive accuracy and learning times are recorded for each task and
each experiment will repeat the task ten times with the means and standard errors

plotted. Each experiment will be repeated for training noise levels of 0%, 5%, 10%,

and 20%, though the test sets will remain noiseless.

Results and Analysis Tables 6.3 and 6.4 below depict the predictive accuracy
and learning times respectively for each of the list manipulation tasks with training

data noise levels of 0%, 5%, 10%, and 20%. For most problems, both systems were
able to ﬁnd correct solutions with the exceptions of finddup in which both systems
struggled and member in which Normal Popper struggled with added noise. This
indicates that even Normal Popper enhanced with an anytime algorithm is capable

of generalizing well to noisy data. This makes cases where Noisy Popper outperforms

Normal Popper notable. Both systems however can struggle on particular problems

CHAPTER 6. EXPERIMENTAL RESULTS

71

or datasets, though it is possible both systems could have found correct solutions to
the finddup task if given additional time.

Normal Popper consistently ran signiﬁcantly faster than Normal Popper. This is due

to the large number of constraints Noisy Popper generates which the ASP solver must

use and the number of programs compared to generate these constraints. As discussed

in Chapter 5, this number of comparisons is quadratic in total number of hypotheses
generated. For a single hypothesis generated by Normal Popper, the system may

generate at most two constraints whereas Noisy Popper can generate a multitude of

constraints for every program already seen by the system. While the Noisy Popper

implementation attempts to mitigate redundant constraints, this clearly produces a
large bottleneck for the system also eﬀected by a large grounding issue discussed in

the following experimentation section. This data indicates than an answer to Q1

is that Noisy Popper generalizes as well as Normal Popper for many noisy datasets,

though typically never performs worse. An answer to Q2 is again that Noisy Popper
is much more ineﬃcient than Normal Popper and may in fact be unusable in certain

cases due to its extreme ineﬃciencies.

6.1.3 Experiment 3: IGGP Problems

The general game playing (GGP) competition [18] measures a system’s general intel-

ligence by having giving the agent the rules to several new games described as logic
programs before having the agent play each game. The competition winner is the

agent which scores the best total over all games. The inductive general game playing

(IGGP) [12] task inverts the GGP task, providing a system with logical traces of a

game in order for the system to try and learn the rules of the game. These exper-
iments focus on the minimal decay and rock, paper, scissors (rps) tasks, aiming to
learn the target predicate next score which determines the score a player will have
given their action and the action of the other player on a given turn.

Materials Noisy Popper and Normal Popper will be evaluated on the IGGP mini-
mal decay and rps tasks. The BK for each system will contain facts about particular

gameplay traces, i.e., speciﬁc actions players took on each turn, the actual score of a

player after a turn has completed, etc. Some gameplay rules are also provided in the

rps BK such as which action beats which other, i.e., rock beats scissors.

The language biases for both systems restrict hypotheses to at most ﬁve unique

variables, at most ﬁve body literals per clause, and at most two clauses for the

CHAPTER 6. EXPERIMENTAL RESULTS

72

Name

Training Noise (%) Normal Popper Noisy Popper

addhead

droplast

evens

finddup

last

len

member

sorted

threesame

0
5
10
20

0
5
10
20

0
5
10
20

0
5
10
20

0
5
10
20

0
5
10
20

0
5
10
20

0
5
10
20

0
5
10
20

100±0
100±0
100±0
100±0

100±0
100±0
100±0
100±0

100±0
100±0
100±0
95±0

55±0
54±0
52±0
51±0

100±0
100±0
100±0
100±0

100±0
100±0
100±0
100±0

100±0
97±0
96±0
86±0

100±0
100±0
100±0
100±0

100±0
100±0
99±0
99±0

100±0
100±0
100±0
100±0

100±0
100±0
100±0
100±0

100±0
100±0
100±0
99±0

54±0
52±0
53±0
53±0

100±0
100±0
100±0
100±0

100±0
100±0
100±0
100±0

100±0
100±0
100±0
100±0

100±0
100±0
100±0
100±0

100±0
100±0
99±0
99±0

Table 6.3: Predictive accuracy for Normal and Noisy Popper on list manipulation
problems. Accuracies are rounded to the nearest integer and errors to the nearest
tenth. Errors are standard.

CHAPTER 6. EXPERIMENTAL RESULTS

73

Name

Training Noise (%) Normal Popper Noisy Popper

addhead

droplast

evens

finddup

last

len

member

sorted

threesame

0
5
10
20

0
5
10
20

0
5
10
20

0
5
10
20

0
5
10
20

0
5
10
20

0
5
10
20

0
5
10
20

0
5
10
20

0.6±0
12±3
9±2
8±3

34±8
78±9
80±10
79±8

2±0
13±3
14±1
13±1

8±1
7±0.9
8±0.9
9±1

1±0.4
3±0.4
3±0.4
3±0.4

0.5±0
3±0.4
2±0.2
2±0.1

0.4±0
10±3
18±6
20±5

4±0.4
8±0.4
8±0.4
8±0.4

0.3±0.1
0.8±0.1
1±0.1
1±0.1

2±0.1
79±4
74±2
74±3

81±39
135±45
142±33
137±36

7±0.5
38±1
45±0.5
40±3

39±2
36±1
39±2
40±2

15±4
19±0.5
21±0.3
20±0.3

2±0.1
59±6
56±2
56±2

0.7±0
23±3
23±3
23±3

26±3
42±0.1
43±1
43±0.2

0.5±0.1
2±0.1
3±0.3
4±0.2

Table 6.4: Learning times for Normal and Noisy Popper on list manipulation prob-
lems. Times are rounded to the nearest second if they are greater than 1 second and
to the tenth otherwise. Errors are standard.

CHAPTER 6. EXPERIMENTAL RESULTS

74

minimal decay task and at most seven unique variable, at most six body literals

per clause, and at most six clauses for the rps task. Again, types and directions are

given for predicate arguments. The minimal constraint threshold for Noisy Popper is
set to its default t = 0. Normal Popper will be enhanced with an anytime algorithm

approach as is done in Experiment 1 above.

Methods For the minimal decay task, 5 positive and 20 negative randomly gen-

erated examples are used for training while 5 positive and 30 negative randomly
generated examples are used of testing. This allows us to observe how well each sys-

tem generalizes when the number of positive and negative training examples are not

equal. A timeout of ten minutes and a limit of 150 generated programs is enforced per

task. For the RPS task, 20 positive and 50 negative examples are randomly generated
to train on and 100 positive and 200 negative are generated for testing. A timeout

of ten minutes and a limit of 200 generated programs is enforced per task. The pre-

dictive accuracy and learning times are recorded for each task and each experiment

will be repeat the task ten times with the means and standard errors plotted. Each
experiment will be repeated for training noise levels from 0% to 40% in increments

of 10% with 5% noise additionally being tested. Testing sets will remain noiseless.

Results and Analysis Figures 6.3 and 6.4 show the predictive accuracies and

runtimes of Normal and Noisy Popper on the IGGP minimal decay and rps tasks
respectively. These are notably diﬃcult problems and even with no noise, neither

system could generate a correct solution for either problem. For minimal decay, this

is largely attributed to the exceptionally small number of examples used to train.

Both systems typically achieved 100% training accuracy on the noiseless minimal
decay data but did not achieve 100% testing accuracy. For RPS, the small program

limit of 200 was the contributing factor for suboptimal noiseless accuracies though

larger program limits often led to blowup in the Noisy Popper runtime. Despite

the low number of programs both systems could learn from, Noisy Popper typically
achieved equal or greater predictive accuracy than Normal Popper for all noise levels,

though by only a slim margin. A McNemar’s [22] test on the Noisy and Normal

Popper predictive accuracy additionally conﬁrmed the signiﬁcance at the p < 0.001

level for both problems indicating that the performances diﬀerences were not random.
The results for both tasks suggest that the answer to Q1 is that Noisy Popper can

generalize better to noisy data than Normal Popper and as well to noiseless data,

though the diﬀerence is often marginal and Normal Popper enhanced with an anytime

CHAPTER 6. EXPERIMENTAL RESULTS

75

algorithm can often generalize well on its own. Additionally, they suggest the answer

to Q2 is that Noisy Popper is signiﬁcantly less eﬃcient than Normal Popper even

with extremely small program limits, an issue for problems which require learning
large programs.

)

%

(

y
c
a
r
u
c
c
A
e
v
i
t
c
i
d
e
r
P

95

90

85

80

Normal Popper
Noisy Popper

)
s
d
n
o
c
e
s
(

e
m
T

i

4

3

2

1

0

0

10

20
Noise (%)

30

40

Normal Popper
Noisy Popper

0

10

20
Noise (%)

30

40

Figure 6.3: IGGP Minimal Decay task predictive accuracy and time when varying
percentage of noisy training data. Standard error is depicted by bars.

)

%

(

y
c
a
r
u
c
c
A
e
v
i
t
c
i
d
e
r
P

80

70

)
s
d
n
o
c
e
s
(

e
m
T

i

40

20

Normal Popper
Noisy Popper

Normal Popper
Noisy Popper

0

10

20
Noise (%)

30

40

0

10

20
Noise (%)

30

40

Figure 6.4: IGGP RPS task predictive accuracy and time when varying percentage
of noisy training data. Standard error is depicted by bars.

6.2 Noisy Popper Enhancements

The purpose of this set of experiments is to determine how eﬀective each enhancement

of Noisy Popper is in aiding the overall system. In each of the following experiments,

the following variants of Noisy Popper will be run against one another: (i) brute

force method described in Section 4.2 which creates no hypothesis constraints (we

CHAPTER 6. EXPERIMENTAL RESULTS

76

will refer to this variant as Enumerate, (ii) Noisy Popper in its entirety, (iii) Noisy

Popper without minimal constraints (which will be labelled as w/o minimal ), (iv)

Noisy Popper without sound constraints (which will be labelled as w/o sound ), (v)
Noisy Popper without size constraints (which will be labelled w/o size).

6.2.1 Experiment 1: East-West Trains

This set of experiments is identical to those in Section 6.1.1 using the same east-west

trains problems.

Materials Each version of Noisy Popper will be evaluated using the same hypothe-

ses as in Section 6.1.1. The language biases and BKs remain the same.

Methods The methods are the same as in Section 6.1.1.

Results and Analysis Figure 6.5 shows that all variants of Noisy Popper achieve

similar predictive accuracy for all noise levels which are higher than Enumerate’s pre-

dictive accuracy. Noisy Popper without minimal constraints performs slightly better

with 40% noise indicating that the minimal constraints used by the other systems
typically pruned a highly accurate hypothesis. However, Enumerate did not perform

as well despite pruning no hypotheses indicating that Noisy Popper without minimal

constraints was only able to ﬁnd its best hypothesis due the remaining constraints it

generated from the other enhancements, i.e. without the additional pruning, it would
not have run long enough to ﬁnd the best solution it did. Figure 6.6 shows that the

predictive accuracies for all systems were roughly the same for all noise levels, but
this may be attributed to h2 being an easier hypothesis to ﬁnd. Here at 40% noise,
Noisy Popper without minimal constraints and without sound hypothesis constraints
both perform slightly better, most likely due to the other Popper variants overﬁtting

the data and pruning too much.

The learning times from Figures 6.5 and 6.6 demonstrate that Noisy Popper actually
gains some speedup from its minimal and sound constraints. This is likely because

when any hypotheses have generalizations or specializations pruned, those programs

are essentially forgotten by the system and generate no further constraints. Notably,

they no longer generate size constraints which is where the greatest bottleneck is as
evidenced by the fast runtime of Noisy Popper without sound size hypothesis con-

straints. These size constraints create a blowup in the grounding as all possible pro-

gram sizes deﬁned in the ASP constraints must be grounded and the size constraints

CHAPTER 6. EXPERIMENTAL RESULTS

77

specify a large range of sizes in the ASP constraints. This can mean thousands of

individual programs are required to be grounded by just a single ASP constraint. In

practice, the vast majority of constraints generated by Noisy Popper are these size
constraints leading to the ineﬃciency of the system. This data suggests an answer

to Q3 is that none of the enhancements individually provide signiﬁcant beneﬁts to

the predictive accuracy of the system, but in conjunction can make the system bet-

ter than brute force enumeration. Minimal constraints however provide signiﬁcant
speedup to the system and the sound hypothesis constraints additionally contribute

to this improvement. The size constraints are the biggest bottleneck however provid-

ing little beneﬁt in return when run for such short durations. It is possible that these

size constraints would prevent the system from overﬁtting when run for extended pe-
riods, but their ineﬃciencies make running the system for too long infeasible. Further

improvements to the system and testing would be needed to draw conclusions from

this hypothesis.

100

95

90

85

)

%

(

y
c
a
r
u
c
c
A
e
v
i
t
c
i
d
e
r
P

80

0

)
s
d
n
o
c
e
s
(

i

e
m
T
g
n
i
n
r
a
e
L

50

40

30

20

10

0

0

Enumerate
Noisy Popper
w/o Min. Cons.
w/o Sound Cons.
w/o Size Cons.

10

20
Noise (%)

30

40

10

20
Noise (%)

30

40

Figure 6.5: East-West Trains predictive accuracies and learning times of Noisy Popper
variants (in seconds) for program h1 when varying percentage of noisy training data.
Standard error is depicted by bars.

6.2.2 Experiment 2: List Manipulations

This set of experiments is identical to those in Section 6.1.2.

Materials The materials are identical to those in Section 6.1.2.

Methods The methods are identical to those in Section 6.1.2.

CHAPTER 6. EXPERIMENTAL RESULTS

78

100

95

90

85

)

%

(

y
c
a
r
u
c
c
A
e
v
i
t
c
i

d
e
r
P

)
s
d
n
o
c
e
s
(

i

e
m
T
g
n

i

n
r
a
e
L

60

40

20

0

0

Enumerate
Noisy Popper
w/o Min. Cons.
w/o Sound Cons.
w/o Size Cons.

0

10

20
Noise (%)

30

40

10

20
Noise (%)

30

40

Figure 6.6: East-West Trains predictive accuracies and learning times of Noisy Popper
variants (in seconds) for program h2 when varying percentage of noisy training data.
Standard error is depicted by bars.

Results and Analysis Table 6.5 depicts the predictive accuracy for Noisy Popper
and its variants on each of the list manipulation tasks. Each performs well on most
tasks except for finddup where again no system can ﬁnd an accurate solution, sorted
where only Noisy Popper performs well, and threesame where each variant performs
slightly worse than Noisy Popper as noise increases. Enumerate notably performs
poorly on several datasets, indicating that it could not ﬁnd a correct solution in the
time allotted while Noisy Popper and its variants could. The sorted task gives the
best indication that the minimal constraints provide the biggest impact to predictive

accuracy with sound hypothesis constraints contributing to a smaller degree and size
constraints being ineﬀective except at higher noise levels where it prevents Noisy

Popper from overﬁtting.

Table 6.6 again demonstrates that typically without the size constraints, Noisy Popper

runs much more eﬃciently, though not as quickly as Enumerate on average. However,
in some tasks such as member and threesame, without noise Noisy Popper without
size constraints ﬁnds the correct solution faster than any other system. We also can

again see that typically, the minimal and sound constraints provide Noisy Popper with

considerable speedup. Figure 6.7 below depicts the predictive test accuracy of the best

hypothesis being maintained by each system versus the total number of programs each
program has generated and learned from for the evens task with 5% training noise.

This plot is exemplary of many of the list manipulation tasks over all noise levels and

demonstrates how Noisy Popper with and without size constraints typically requires

the fewest number of programs to learn from to ﬁnd an optimal solution. Noisy Popper

CHAPTER 6. EXPERIMENTAL RESULTS

79

without sound constraints typically requires generating several more hypotheses to

accomplish the same and without sound hypothesis constraints requires yet more

programs. If Enumerate ﬁnds an optimal solution, it typically requires generating
the greatest number of hypotheses. Overall, this data suggests the answer to Q3

is again that the enhancements of Noisy Popper do not individually provide large

beneﬁts to the accuracy of the system except for speciﬁc datasets. However, minimal

constraints and sound constraints do provide speedup to the system and help the
system ﬁnd correct solutions quicker than without. They additionally reduce the

total number of programs which must be generated to ﬁnd an optimal solution. This

indicates that both sets of constraints contribute are beneﬁcial to the overall system.

Size constraints can provide deterrence against overﬁtting, but only in speciﬁc cases
and at the cost of great ineﬃciency.

105

100

95

90

85

80

75

70

65

60

55

50

)

%

(

y
c
a
r
u
c
c
A
e
v
i
t
c
i
d
e
r
P

45
−50

0

50

100

Enumerate
Noisy Popper
w/o Min. Cons.
w/o Sound Cons.
w/o Size Cons.

400

450

500

550

150

200

250
Number of Programs Generated

300

350

Figure 6.7: Predictive accuracies of maintained best programs for Noisy Popper vari-
ants versus the number of programs generated by each system on evens dataset with
5% training noise. Standard error is depicted by bars.

CHAPTER 6. EXPERIMENTAL RESULTS

80

Name

Noise (%) Enumerate Noisy Popper w/o Minimal Constraints w/o Sound Constraints w/o Sound Size Constraints

addhead

droplast

evens

finddup

last

len

member

sorted

threesame

0
5
10
20

0
5
10
20

0
5
10
20

0
5
10
20

0
5
10
20

0
5
10
20

0
5
10
20

0
5
10
20

0
5
10
20

100±0
100±0
100±0
100±0

50±0
50±0
50±0
50±0

82±0
82±0
82±0
81±0

53±0
52±0
52±0
50±0

100±0
100±0
100±0
100±0

100±0
100±0
100±0
100±0

100±0
100±0
100±0
100±0

76±0
78±0.5
76±0
75±0

100±0
99±0
99±0
99±0

100±0
100±0
100±0
100±0

100±0
100±0
100±0
100±0

100±0
100±0
100±0
99±0

54±0
52±0
52±0
53±0

100±0
100±0
100±0
100±0

100±0
100±0
100±0
100±0

100±0
100±0
100±0
100±0

100±0
100±0
100±0
100±0

100±0
100±0
99±0
99±0

100±0
100±0
100±0
100±0

100±0
100±0
100±0
100±0

100±0
100±0
100±0
100±0

53±0
52±0
52±0
50±0

100±0
100±0
100±0
100±0

100±0
100±0
100±0
100±0

100±0
100±0
100±0
100±0

92±0.1
79±0.2
76±0
75±0

100±0
100±0
98±0
99±0

100±0
100±0
100±0
100±0

100±0
100±0
100±0
100±0

100±0
100±0
100±0
100±0

53±0
52±0
52±0
49±0

100±0
100±0
100±0
100±0

100±0
100±0
100±0
100±0

100±0
100±0
100±0
100±0

76±0
81±1.5
83±0.1
83±0.1

100±0
100±0
98±0
99±0

100±0
100±0
100±0
100±0

100±0
100±0
100±0
100±0

100±0
100±0
100±0
100±0

53±0
52±0
52±0
50±0

100±0
100±0
100±0
100±0

100±0
100±0
100±0
100±0

100±0
100±0
100±0
100±0

100±0
100±0
93±0.1
91±0.3

100±0
100±0
98±0
99±0

Table 6.5: Predictive accuracy for Noisy Popper variants on list manipulation prob-
lems. Accuracies are rounded to the nearest integer and errors to the nearest tenth.
Errors are standard.

CHAPTER 6. EXPERIMENTAL RESULTS

81

Name

Noise (%) Enumerate Noisy Popper w/o Minimal Constraints w/o Sound Constraints w/o Sound Size Constraints

addhead

droplast

evens

finddup

last

len

member

sorted

threesame

0
5
10
20

0
5
10
20

0
5
10
20

0
5
10
20

0
5
10
20

0
5
10
20

0
5
10
20

0
5
10
20

0
5
10
20

2±0.2
4±0.1
4±0.2
5±1

5±0
5±0
6±0.1
6±0.2

6±0.4
5±0.4
6±0.3
5±0.3

7±1
9±1
5±0.6
6±1

3±0.4
4±0.2
4±0.1
3±0.2

2±0.3
3±0.2
3±0.2
3±0.1

0.7±0.1
2±0.1
2±0.2
2±0.2

5±0.3
4±0.8
5±0.2
5±0.2

0.5±0.1
5±0.2
5±0.1
5±0.2

2±0.1
79±4
74±2
74±3

81±39
135±45
142±33
137±6

7±0.5
38±1
45±0.5
40±3

39±2
36±1
39±2
40±2

15±4
19±0.5
21±0.3
20±0.3

2±0.1
59±6
56±2
56±1

0.7±0
23±3
23±3
23±3

26±4
46±0.1
43±0.9
43±0.2

0.5±0.1
2±0.1
3±0.3
4±0.2

3±0.2
68±2
65±5
81±3

71±5
89±3
92±2
92±2

14±2
47±2
58±0
45±5

51±3
49±2
40±0.8
45±0.3

13±0.6
52±2
51±0.8
52±1

3±0.2
60±4
62±2
58±0.8

0.6±0.2
33±0.5
30±0
25±1

43±7
55±2
50±2
47±0.7

0.6±0.1
82±5
78±2
76±2

3±0.2
183±7
195±5
211±7

205±8
215±8
217±8
217±8

17±3
77±3
102±3
85±9

95±3
96±2
83±0.6
90±2

9±2
61±3
60±2
62±2

2±0.2
67±2
66±3
67±1

1±0.1
35±0.2
26±0.2
28±0.5

71±2
69±0.5
69±0.4
72±0.5

0.6±0.1
5±0
2±0.1
3±0.3

1±0.1
80±0.2
91±1
102±2

95±3
112±3
110±2
138±5

2±0.1
14±0.4
15±0.2
13±1

13±2
18±3
10±2
12±1

2±0.4
18±0.4
18±0.2
19±0.1

1±0.1
15±2
18±1
16±1

0.5±0
22±0.1
20±0.1
16±0.5

11±0.6
15±0.3
15±0.7
14±0.4

0.4±0
2±0.1
0.8±0
1±0.1

Table 6.6: Learning times for Noisy Popper variants on list manipulation problems.
Times are rounded to the nearest second if they are greater than 1 second and to the
tenth otherwise. Errors are standard.

CHAPTER 6. EXPERIMENTAL RESULTS

82

6.2.3 Experiment 3: IGGP Problems

This set of experiments is identical to those in Section 6.1.3 using the two IGGP

problems minimal decay and rps.

Materials The materials are identical to those in Section 6.1.3.

Methods The methods are identical to those in Section 6.1.3.

Results and Analysis Figures 6.8 and 6.9 both demonstrate that on both IGGP

tasks, each Noisy Popper variant obtains roughly equal predictive accuracy for these

levels of noise. For both minimal decay and rps tasks, Noisy Popper without size
constraints again performs signiﬁcantly more eﬃciently than the other variants, again

due to the grounding blowup previously discussed. This data again suggests that the

answer to Q3 is that the enhancements of Noisy Popper do not necessarily improve

its predictive accuracy over a brute force approach and that due to the grounding
blowup of the size constraints, Noisy Popper performs much less eﬃciently than this

brute force approach.

95

90

85

)

%

(

y
c
a
r
u
c
c
A
e
v
i
t
c
i
d
e
r
P

)
s
d
n
o
c
e
s
(

e
m
T

i

6

4

2

0

0

Enumerate
Noisy Popper
w/o Min. Cons.
w/o Sound Cons.
w/o Size Cons.

0

10

20
Noise (%)

30

40

10

20
Noise (%)

30

40

Figure 6.8: IGGP minimal decay task predictive accuracy and time of Noisy Popper
variants (in seconds) for when varying percentage of noisy training data. Standard
error is depicted by bars.

6.3 Summary

In this chapter, we empirically evaluated Noisy Popper’s performance against Normal
Popper’s as well as the eﬀects of the individual enhancements of the Noisy Popper

CHAPTER 6. EXPERIMENTAL RESULTS

83

)

%

(

y
c
a
r
u
c
c
A
e
v
i
t
c
i

d
e
r
P

80

78

76

74

72

70

Enumerate
Noisy Popper
w/o Min. Cons.
w/o Sound Cons.
w/o Size Cons.

60

40

20

)
s
d
n
o
c
e
s
(

e
m
T

i

0

10

20
Noise (%)

30

40

0

10

20
Noise (%)

30

40

Figure 6.9: IGGP rps task predictive accuracy and time of Noisy Popper variants (in
seconds) when varying percentage of noisy training data. Standard error is depicted
by bars.

system. Noisy Popper was shown to better generalize to noisy datasets than Nor-

mal Popper for some tasks, but many experiments suggested that Normal Popper

enhanced with an anytime algorithm approach can often generalize very well to these

datasets. The minimal constraints and sound hypothesis constraints are eﬀective at
pruning the hypothesis space and aiding the system in ﬁnding optimal solutions by

generating fewer hypotheses than the brute force Enumerate method requires. How-

ever, this comes at the cost of the expected ineﬃciency when compared to Normal

Popper and Enumerate. The grounding blowup of the size constraints makes generat-
ing several thousand programs to learn from infeasible for the system. The following

chapter will discuss work to be done in the future to mitigate these ineﬃciencies and

other additions which may be considered for the system. We will also give a brief

summary of this project, its ﬁndings and contributions as well as its limitations.

Chapter 7

Conclusions

This paper has discussed the theoretical background, implementation details, and

empirical analysis of the Noisy Popper ILP system, an extension of the Normal Popper
system [13] which is capable of generalizing to noisy datasets. The following sections

give a critical summary and analysis of the work and address future work which could

improve Noisy Popper’s capabilities.

7.1 Summary and Evaluation

Handling missclassiﬁed training examples is an important task in machine learning,

though many ILP systems are not naturally capable of doing so. We have shown

that the learning from failures (LFF) approach which Normal Popper takes to prune
its hypothesis search space is not naturally conducive to this task. The relaxed LFF

setting introduced in this paper takes a less strict approach to hypothesis search

and in doing so demonstrates better theoretical capabilities of ﬁnding hypotheses

which generalize well to noisy data. We proved several theoretical claims over how
comparing hypotheses can identify sets of hypotheses which perform suboptimally in
this relaxed setting under two scoring measures: SACC which measures the training
accuracy of a hypothesis and SM DL which weighs training accuracy against the size
of the hypothesis.

In implementation, Noisy Popper adapts the approach taken by Normal Popper,

relaxing the ASP hypothesis constraints which prune the hypothesis space and instead

typically only generating constraints which are sound in the relaxed setting. In this

way, the theoretical claims made over the relaxed LFF system are translated into
hypothesis constraints which reduce the hypothesis space during the system’s search.

Many of these constraints however create a large cost in the logical grounding required

84

CHAPTER 7. CONCLUSIONS

85

by the system which leads to signiﬁcant runtime ineﬃciencies. The experimental

work demonstrated that Noisy Popper never generalizes worse than Normal Popper

for both noisy and non-noisy datasets and is capable of exceeding the predictive
accuracy of Normal Popper on several datasets. However, enhancing Normal Popper

with an anytime algorithm approach makes the system very capable of generalizing

to noisy data on its own. The main deﬁciency of Noisy Popper is its ineﬃciency

which future work must address to make the system viable in practice. Despite
these shortcomings, Noisy Popper shows promise of being a useful ILP system which

accurately and eﬃciently generalizes to noisy datasets.

7.2 Future Work

Recusive Cases Several of the theoretical claims proved in this paper only dis-

cussed the suboptimality of non-recursive programs. Generalizing these claims or

constructing new ones which discuss the suboptimality of recursive programs would

make these claims more complete. Such claims could also be used in practice to
greatly improve the eﬃciency of hypothesis search.

Scoring Metrics
In this paper, only two scoring functions were discussed, SACC
and SM DL and all theoretical claims were derived under these two settings. Additional
theory should be explored under both of these scorings and additional scorings such
as those which measure coverage or entropy. Whether these claims manifest as new

noise handling systems or as additions to Noisy Popper itself, determining new cases

for hypothesis pruning can be used by many ILP systems moving forward and can

better improve the searching eﬃciency of such systems.

Grounding Bottleneck The largest bottleneck for Noisy Popper currently is the

need to ground all programs for all applicable sizes for the hypothesis cosntraints

with programs size. This leads to a blowup in number of required groundings which

is the expected cause of the signiﬁcant learning time diﬀerence from Normal Popper
to Noisy Popper. Changing the ASP encodings for these size constraints to eliminate

this blowup should massively improve the overall eﬃciency of the system and make

it viable for much larger problems than those tested in Chpater 6. Insuﬃcient time

led to this problem remaining unresolved upon completion of this project.

CHAPTER 7. CONCLUSIONS

86

Subsumption Checking Another large eﬃciency issue present in the implemen-

tation is the naive method in which Noisy Popper compares programs and checks for

subsumption. Rather than maintaining all previously seen programs as a list, data
structures such as subsumption lattices may be used to reduce the overall number of

subsumption checks needed. Given how expensive the subsumption is to check, or

rather the incomplete subsumption we use in implementation, this is an improvement

that would again boost the overall eﬃciency of the system and allow the system to
better handle checking exceptionally large hypotheses.

Parallelization Work to make use of multi-core machines and parallelize the Pop-

per system has been completed and has been shown to greatly improve the learning

rate of the system. Combining the noise handling approaches used in Noisy Pop-
per with this parallized approach should vastly improve the eﬃciency of the system.

Such work however is not trivial and determining new theoretical claims about such

an environment are necessary for a sound implementation.

Bibliography

[1] John Ahlgren and Shiu Yin Yuen. Eﬃcient program synthesis using constraint
satisfaction in inductive logic programming. J. Mach. Learn. Res., 14(1):3649–

3682, 2013.

[2] Alexessander Alves, Rui Camacho, and Eug´enio C. Oliveira. Improving numeri-

cal reasoning capabilities of inductive logic programming systems. In Christian

Lemaˆıtre, Carlos A. Reyes Garc´ıa, and Jes´us A. Gonz´alez, editors, Advances

in Artiﬁcial Intelligence - IBERAMIA 2004, 9th Ibero-American Conference on
AI, Puebla, Mexico, November 22-26, 2004, Proceedings, volume 3315 of Lecture

Notes in Computer Science, pages 195–204. Springer, 2004.

[3] Duangtida Athakravi, Domenico Corapi, Krysia Broda, and Alessandra Russo.
Learning through hypothesis reﬁnement using answer set programming. In Ger-

son Zaverucha, V´ıtor Santos Costa, and Aline Paes, editors, Inductive Logic

Programming - 23rd International Conference, ILP 2013, Rio de Janeiro, Brazil,

August 28-30, 2013, Revised Selected Papers, volume 8812 of Lecture Notes in
Computer Science, pages 31–46. Springer, 2013.

[4] Hendrik Blockeel and Luc De Raedt. Top-down induction of ﬁrst-order logical

decision trees. Artif. Intell., 101(1-2):285–297, 1998.

[5] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat

Flepp, Prasoon Goyal, Lawrence D. Jackel, Mathew Monfort, Urs Muller, Jiakai
Zhang, Xin Zhang, Jake Zhao, and Karol Zieba. End to end learning for self-

driving cars, 2016.

[6] Ivan Bratko. Reﬁning complete hypotheses in ILP. In Saso Dzeroski and Peter A.
Flach, editors, Inductive Logic Programming, 9th International Workshop, ILP-

99, Bled, Slovenia, June 24-27, 1999, Proceedings, volume 1634 of Lecture Notes

in Computer Science, pages 44–55. Springer, 1999.

87

BIBLIOGRAPHY

88

[7] Domenico Corapi, Alessandra Russo, and Emil Lupu. Inductive logic program-

ming in answer set programming. In Stephen Muggleton, Alireza Tamaddoni-

Nezhad, and Francesca A. Lisi, editors, Inductive Logic Programming - 21st
International Conference, ILP 2011, Windsor Great Park, UK, July 31 - August

3, 2011, Revised Selected Papers, volume 7207 of Lecture Notes in Computer

Science, pages 91–97. Springer, 2011.

[8] Andrew Cropper. Eﬃciently learning eﬃcient programs. PhD thesis, Imperial

College London, UK, 2017.

[9] Andrew Cropper. Playgol: Learning programs through play.

In Sarit Kraus,

editor, Proceedings of the Twenty-Eighth International Joint Conference on Ar-

tiﬁcial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, pages 6074–

6080. ijcai.org, 2019.

[10] Andrew Cropper. Forgetting to learn logic programs. In The Thirty-Fourth AAAI

Conference on Artiﬁcial Intelligence, AAAI 2020, The Thirty-Second Innovative

Applications of Artiﬁcial Intelligence Conference, IAAI 2020, The Tenth AAAI
Symposium on Educational Advances in Artiﬁcial Intelligence, EAAI 2020, New

York, NY, USA, February 7-12, 2020, pages 3676–3683. AAAI Press, 2020.

[11] Andrew Cropper, Sebastijan Dumancic, and Stephen H. Muggleton. Turning

30: New ideas in inductive logic programming.

In Christian Bessiere, editor,

Proceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial

Intelligence, IJCAI 2020, pages 4833–4839. ijcai.org, 2020.

[12] Andrew Cropper, Richard Evans, and Mark Law. Inductive general game play-

ing. Mach. Learn., 109(7):1393–1434, 2020.

[13] Andrew Cropper and Rolf Morel. Learning programs by learning from failures.

Mach. Learn., 110(4):801–856, 2021.

[14] Andrew Cropper, Rolf Morel, and Stephen Muggleton. Learning higher-order

logic programs. Mach. Learn., 109(7):1289–1322, 2020.

[15] Andrew Cropper and Sophie Tourret. Logical reduction of metarules. Mach.

Learn., 109(7):1323–1369, 2020.

BIBLIOGRAPHY

89

[16] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet:

A large-scale hierarchical image database. In 2009 IEEE Computer Society Con-

ference on Computer Vision and Pattern Recognition (CVPR 2009), 20-25 June
2009, Miami, Florida, USA, pages 248–255. IEEE Computer Society, 2009.

[17] Richard Evans and Edward Grefenstette. Learning explanatory rules from noisy

data. J. Artif. Intell. Res., 61:1–64, 2018.

[18] Michael R. Genesereth and Yngvi Bj¨ornsson. The international general game

playing competition. AI Mag., 34(2):107–111, 2013.

[19] Peter Gr¨unwald and Teemu Roos. Minimum description length revisited. Inter-

national Journal of Mathematics for Industry, 11(01):1930001, Dec 2019.

[20] Tobias Kaminski, Thomas Eiter, and Katsumi Inoue. Exploiting answer set

programming with external sources for meta-interpretive learning. Theory Pract.

Log. Program., 18(3-4):571–588, 2018.

[21] Deepak Kapur and Paliath Narendran. Np-completeness of the set uniﬁcation

and matching problems. In J¨org H. Siekmann, editor, 8th International Con-

ference on Automated Deduction, Oxford, England, July 27 - August 1, 1986,
Proceedings, volume 230 of Lecture Notes in Computer Science, pages 489–495.

Springer, 1986.

[22] Peter A Lachenbruch. Mcnemar test. Wiley StatsRef: Statistics Reference On-

line, 2014.

[23] Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, and Samuel J.
Gershman. Building machines that learn and think like people. CoRR,

abs/1604.00289, 2016.

[24] Mark Law.

Inductive learning of answer set programs. PhD thesis, Imperial

College London, UK, 2018.

[25] Mark Law, Alessandra Russo, and Krysia Broda. Inductive learning of answer
set programs. In Eduardo Ferm´e and Jo˜ao Leite, editors, Logics in Artiﬁcial In-

telligence - 14th European Conference, JELIA 2014, Funchal, Madeira, Portugal,

September 24-26, 2014. Proceedings, volume 8761 of Lecture Notes in Computer

Science, pages 311–325. Springer, 2014.

BIBLIOGRAPHY

90

[26] Mark Law, Alessandra Russo, and Krysia Broda. Inductive learning of answer

set programs from noisy examples. CoRR, abs/1808.08441, 2018.

[27] Dianhuan Lin, Eyal Dechter, Kevin Ellis, Joshua B. Tenenbaum, and Stephen

Muggleton. Bias reformulation for one-shot function induction.

In Torsten

Schaub, Gerhard Friedrich, and Barry O’Sullivan, editors, ECAI 2014 - 21st Eu-

ropean Conference on Artiﬁcial Intelligence, 18-22 August 2014, Prague, Czech
Republic - Including Prestigious Applications of Intelligent Systems (PAIS 2014),

volume 263 of Frontiers in Artiﬁcial Intelligence and Applications, pages 525–

530. IOS Press, 2014.

[28] Donald Michie. Machine learning in the next ﬁve years. In Derek H. Sleeman,

editor, Proceedings of the Third European Working Session on Learning, EWSL

1988, Turing Institute, Glasgow, UK, October 3-5, 1988, pages 107–122. Pitman
Publishing, 1988.

[29] Herman Midelfart. A bounded search space of clausal theories. In Saso Dzeroski

and Peter A. Flach, editors, Inductive Logic Programming, 9th International
Workshop, ILP-99, Bled, Slovenia, June 24-27, 1999, Proceedings, volume 1634

of Lecture Notes in Computer Science, pages 210–221. Springer, 1999.

[30] Tom M. Mitchell. Machine learning, International Edition. McGraw-Hill Series

in Computer Science. McGraw-Hill, 1997.

[31] Tom M. Mitchell, William W. Cohen, Estevam R. Hruschka Jr., Partha P. Taluk-
dar, Bo Yang, Justin Betteridge, Andrew Carlson, Bhavana Dalvi Mishra, Matt

Gardner, Bryan Kisiel, Jayant Krishnamurthy, Ni Lao, Kathryn Mazaitis, Thahir

Mohamed, Ndapandula Nakashole, Emmanouil A. Platanios, Alan Ritter, Mehdi

Samadi, Burr Settles, Richard C. Wang, Derry Wijaya, Abhinav Gupta, Xinlei
Chen, Abulhair Saparov, Malcolm Greaves, and Joel Welling. Never-ending

learning. Commun. ACM, 61(5):103–115, 2018.

[32] Stephen Muggleton.

Inductive logic programming. New Gener. Comput.,

8(4):295–318, 1991.

[33] Stephen Muggleton.

Inverse entailment and progol. New Gener. Comput.,

13(3&4):245–286, 1995.

BIBLIOGRAPHY

91

[34] Stephen Muggleton, Wang-Zhou Dai, Claude Sammut, Alireza Tamaddoni-

Nezhad, Jing Wen, and Zhi-Hua Zhou. Meta-interpretive learning from noisy

images. Mach. Learn., 107(7):1097–1118, 2018.

[35] Stephen H. Muggleton, Dianhuan Lin, and Alireza Tamaddoni-Nezhad. Meta-

interpretive learning of higher-order dyadic datalog: predicate invention revis-

ited. Mach. Learn., 100(1):49–73, 2015.

[36] Stephen H. Muggleton, Ute Schmid, Christina Zeller, Alireza Tamaddoni-

Nezhad, and Tarek R. Besold. Ultra-strong machine learning: comprehensibility
of programs learned with ILP. Mach. Learn., 107(7):1119–1140, 2018.

[37] Shan-Hwei Nienhuys-Cheng and Ronald De Wolf. Foundations of inductive logic

programming, volume 1228. Springer Science & Business Media, 1997.

[38] Gordon Plotkin. Automatic methods of inductive inference. PhD thesis, The

University of Edinburgh, 1972.

[39] J. Ross Quinlan. Induction of decision trees. Mach. Learn., 1(1):81–106, 1986.

[40] J. Ross Quinlan. Learning logical deﬁnitions from relations. Mach. Learn.,

5:239–266, 1990.

[41] J. Ross Quinlan and Ronald L. Rivest. Inferring decision trees using the minimum

description length principle. Inf. Comput., 80(3):227–248, 1989.

[42] Jorma Rissanen. Modeling by shortest data description. Autom., 14(5):465–471,

1978.

[43] Peter Sch¨uller and Mishal Benz. Best-eﬀort inductive logic programming via ﬁne-

grained cost-based hypothesis generation - the inspire system at the inductive

logic programming competition. Mach. Learn., 107(7):1141–1169, 2018.

[44] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George

van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneer-
shelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal

Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap, Madeleine Leach, Koray

Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of go

with deep neural networks and tree search. Nat., 529(7587):484–489, 2016.

BIBLIOGRAPHY

92

[45] Ashwin Srinivasan. The aleph manual. Machine Learning at the Computing

Laboratory, Oxford University, 2001.

[46] Lisa Torrey, Jude W. Shavlik, Trevor Walker, and Richard Maclin. Relational

macros for transfer in reinforcement learning.

In Hendrik Blockeel, Jan Ra-

mon, Jude W. Shavlik, and Prasad Tadepalli, editors, Inductive Logic Program-

ming, 17th International Conference, ILP 2007, Corvallis, OR, USA, June 19-
21, 2007, Revised Selected Papers, volume 4894 of Lecture Notes in Computer

Science, pages 254–268. Springer, 2007.

