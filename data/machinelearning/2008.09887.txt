Semi-Supervised Data Programming with Subset Selection

Ayush Maheshwari 1, Oishik Chatterjee 1, Krishnateja Killamsetty 2,
Ganesh Ramakrishnan 1 , Rishabh Iyer 2
1Department of CSE, IIT Bombay, India
2 The University of Texas at Dallas
{ayusham, oishik, ganesh}@cse.iitb.ac.in
{krishnateja.killamsetty, rishabh.iyer}@utdallas.edu

1
2
0
2

n
u
J

2
1

]

G
L
.
s
c
[

3
v
7
8
8
9
0
.
8
0
0
2
:
v
i
X
r
a

Abstract

The paradigm of data programming, which
uses weak supervision in the form of
rules/labelling functions, and semi-supervised
learning, which augments small amounts of
labelled data with a large unlabelled dataset,
have shown great promise in several text classi-
ﬁcation scenarios. In this work, we argue that
by not using any labelled data, data program-
ming based approaches can yield sub-optimal
performances, particularly when the labelling
functions are noisy. The ﬁrst contribution of
this work is an introduction of a framework,
SPEAR which is a semi-supervised data pro-
gramming paradigm that learns a joint model
that effectively uses the rules/labelling func-
tions along with semi-supervised loss func-
tions on the feature space. Next, we also study
SPEAR-SS which additionally does subset se-
lection on top of the joint semi-supervised
data programming objective and selects a set
of examples that can be used as the labelled
set by SPEAR. The goal of SPEAR-SS is
to ensure that the labelled data can comple-
ment the labelling functions, thereby beneﬁt-
ing from both data-programming as well as ap-
propriately selected data for human labelling.
We demonstrate that by effectively combining
semi-supervision, data-programming, and sub-
set selection paradigms, we signiﬁcantly out-
perform the current state-of-the-art on seven
publicly available datasets. 1

1

Introduction

Modern machine learning techniques rely on large
amounts of labelled training data for text clas-
siﬁcation tasks such as spam detection, (movie)
genre classiﬁcation, sequence labelling, etc. Super-
vised learning approaches have utilised such large
amounts of labelled data and, this has resulted in

1The

source

code

is

available

at

https://github.com/ayushbits/
Semi-Supervised-LFs-Subset-Selection

huge successes in the last decade. However, the
acquisition of labelled data, in most cases, entails
a painstaking process requiring months of human
effort. Several techniques such as active learning,
distant supervision, crowd-consensus learning, and
semi-supervised learning have been proposed to
reduce the annotation cost (Settles et al., 2008).
However, clean annotated labels continue to be crit-
ical for reliable results (Bach et al., 2019; Goh et al.,
2018).

Recently, Ratner et al. (2016) proposed a
paradigm on data-programming in which several
Labelling Functions (LF) written by humans are
used to weakly associate labels with the instances.
In data programming, users encode the weak su-
pervision in the form of labelling functions. On
the other hand, traditional semi-supervised learning
methods combine a small amount of labelled data
with large unlabelled data (Kingma et al., 2014).
In this paper, we leverage semi-supervision in the
feature space for more effective data programming
using labelling functions.

1.1 Motivating Example

We illustrate the LFs on one of the seven tasks
on which we experiment with, viz., identifying
spam/no-spam comments in the YouTube reviews.
For some applications, writing LFs is often as sim-
ple as using keyword lookups or a regex expression.
In this speciﬁc case, the users construct heuristic
patterns as LFs for classifying spam/not-spam com-
ments. Each LF takes a comment as an input and
provides a binary label as the output; +1 indicates
that the comment is spam, -1 indicates that the
comment is not spam, and 0 indicates that the LF is
unable to assert anything for the comment (referred
to as an abstain). Table 1 presents a few example
LFs for spam and non-spam classiﬁcation.

In isolation, a particular LF may neither be al-
ways correct nor complete. Furthermore, the LFs

 
 
 
 
 
 
Id
LF1

LF2

LF3

Description
If http or https in comment text, then return +1 otherwise
ABSTAIN (return 0)
If length of comment is less than 5 words, then return -1 other-
wise ABSTAIN (return 0).(Non spam comments are often short)
If comment contains my channel or my video, then return
+1 otherwise ABSTAIN (return 0).

Table 1: Three LFs based on keyword lookups or regex
expression for the YouTube spam classiﬁcation task

may also produce conﬂicting labels. In the past,
generative models such as Snorkel (Ratner et al.,
2016) and CAGE (Chatterjee et al., 2020) have
been proposed for consensus on the noisy and con-
ﬂicting labels assigned by the discrete LFs to deter-
mine the probability of the correct labels. Labels
thus obtained could be used for training any super-
vised model/classiﬁer and evaluated on a test set.
We will next highlight a challenge in doing data
programming using only LFs that we attempt to ad-
dress. For each of the following sentences S1 . . . S6
that can constitute an observed set of training in-
stances, we state the value of the true label (±1).
While the candidates in S1 and S4 are instances of
a spam comment, the ones in S2 and S3 are not. In
fact, these examples constitute one of the canonical
cases that we discovered during the analysis of our
approach in Section 4.4.
(cid:104)S1, +1(cid:105): Please help me go to college
1.
guys! Thanks from the bottom of my heart.
https://www.indiegogo.com/projects/
2. (cid:104)S2, −1(cid:105): I love this song
3. (cid:104)S3, −1(cid:105): This song is very good... but the
video makes no sense...
4. (cid:104)S4, +1(cid:105): https://www.facebook.com/teeLaLaLa
Further, let us say we have a completely unseen
set of test instances, S5 and S6, whose labels we
would also like to predict effectively:
5. (cid:104)S5, −1(cid:105): This song is prehistoric
6.
www.youtube.com/watch?v=TQ046FuAu00

(cid:104)S6, +1(cid:105): Watch Maroon 5’s latest

...

In Table 2, we present the outputs of the LFs
as well as some n-gram features F1 (‘.com’) and
F2 (‘This song’) on the observed training exam-
ples S1, S2, S3 and S4 as well as on the unseen
test examples S5 and S6. For S1, the correct con-
sensus can easily be performed to output the true
label +1, since LF1 (designed for class +1) gets
triggered, whereas LF2 (designed for class -1) is
not triggered. Similarly, for S2, LF2 gets triggered
whereas LF1 is not, making it possible to easily
perform the correct consensus. Hence, we have
treated S1 and S2 as unlabelled, indicating that we

Training data
Label
id
S1
S2
S3
S4

+1
-1
-1
+1
Test data
-1
+1

S5
S6

LF outputs

LF1(+1) LF2(-1) F1

Features
F2

1
0
0
1

0
0

0
1
0
1

1
0

1
0
0
1

0
1

0
1
1
0

1
0

Table 2: Example illustrating the insufﬁciency of using
data programming using only LFs.

could learn a model based on LFs alone without
supervision if all we observed were these two ex-
amples and the outputs of LF1 and LF2. However,
the correct consensus on S3 and S4 is challenging
since both LF1 and LF2 either ﬁre or do not. While
the (n-gram based) features F1 and F2 appear to be
informative and could potentially complement LF1
and LF2, we can easily see that correlating feature
values with LF outputs is tricky in a completely
unsupervised setup. To address this issue, we ask
the following questions:
(A) What if we are provided access to the true la-
bels of a small subset of instances - in this case,
only S3 and S4? Could the (i) correlation of fea-
tures values (eg. F1 and F2) with labels (eg. +1
and -1 respectively), modelled via a small set of
labelled instances (eg. S3 and S4), in conjunction
with (ii) the correlation of feature values (eg. F1
and F2) with LFs (eg. LF1 and LF2) modelled via
a potentially larger set of unlabelled instances (eg.
S1, S2), help improved prediction of labels for hith-
erto unseen test instances S5 and S6?
(B) Can we precisely determine the subset of the
unlabelled data that, when labelling would help
us train a model (in conjunction with the labelling
functions) that is most effective on the test set? In
other words, instead of randomly choosing the la-
belled dataset for doing semi-supervised learning
(part A), can we intelligently select the labelled
subset? In the above example, choosing the la-
belled set as S3, S4 would be much more useful
than choosing the labelled set as S1, S2.

As a solution to (A), in Section 3.3, we present
a new formulation, SPEAR, in which the parame-
ters over features and LFs are jointly trained in a
semi-supervised manner. SPEAR expands as Semi-
suPervisEd dAta pRogramming. As for (B), we
present a subset selection recipe, SPEAR-SS (in

Section 3.4), that recommends the sub-set of the
data (e.g. S3 and S4), which, after labelling, would
most beneﬁt the joint learning framework.

1.2 Our Contributions

We summarise our main contributions as follows:
To address (A), we present SPEAR (c.f., Sec-
tion 3.3), which is a novel paradigm for jointly
learning the parameters over features and labelling
functions in a semi-supervised manner. We jointly
learn a parameterized graphical model and a clas-
siﬁer model to learn our overall objective. To ad-
dress (B), we present SPEAR-SS (c.f., Section 3.4),
which is a subset selection approach to select the set
of examples which can be used as the labelled set
by SPEAR. We show, in particular, that through a
principled data selection approach, we can achieve
signiﬁcantly higher accuracies than just randomly
selecting the seed labelled set for semi-supervised
learning with labelling functions. Moreover, we
also show that the automatically selected subset
performs comparably or even better than the hand-
picked subset by humans in the work reported by
Awasthi et al. (2020), further emphasising the ben-
eﬁt of subset selection for semi-supervised data
programming. Our framework is agnostic to the
underlying network architecture and can be applied
using different underlying techniques without a
change in the meta-approach. Finally, we evalu-
ate our model on seven publicly available datasets
from domains such as spam detection, record clas-
siﬁcation, and genre prediction and demonstrate
signiﬁcant improvement over state-of-the-art tech-
niques. We also draw insights from experiments in
synthetic settings (presented in the appendix).

2 Related Work

Data Programming and Unsupervised Learn-
ing: Snorkel (Ratner et al., 2016) has been pro-
posed as a generative model to determine correct
label probability using consensus on the noisy
and conﬂicting labels assigned by the discrete
LFs. Chatterjee et al. (2020) proposed a graphi-
cal model, CAGE, that uses continuous-valued LFs
with scores obtained using soft match techniques
such as cosine similarity of word vectors, TF-IDF
score, distance among entity pairs, etc. Owing to
its generative model, Snorkel is highly sensitive to
initialisation and hyper-parameters. On the other
hand, the CAGE model employs user-controlled
quality guides that incorporate labeller intuition

into the model. However, these models completely
disregard feature information that could provide ad-
ditional information to learn the (graphical) model.
These models try to learn a combined model for
the labelling functions in an unsupervised manner.
However, in practical scenarios, some labelled data
is always available (or could be made available
by labelling a few instances); hence, a completely
unsupervised approach might not be the best solu-
tion. In this work, we augment these data program-
ming approaches by designing a semi-supervised
model that incorporates feature information and
LFs to learn the parameters jointly. Hu et al. (2016)
proposed a student-teacher model that transfers
rule information by assigning linear weight to each
rule based on an agreement objective. The model
we propose in this paper jointly learns parameters
over features and rules in a semi-supervised man-
ner rather than just weighing their outputs and can
therefore be more expressive.

Other works such as Jawanpuria et al. (2011);
Dembczy´nski et al. (2008) discovers simple and
conjuctive rules from input features and assign
weight to each rules for better generalization.
Nagesh et al. (2012) induces rules from query lan-
guage to build NER extractor while Kulkarni et al.
(2018) uses active learning to derive consensus
among labelers to label data.

Semi-Supervised Data Programming: The only
work which, to our knowledge, combines rules
with supervised learning in a joint framework is
the work by Awasthi et al. (2020). They leverage
both rules and labelled data by associating each
rule with exemplars of correct ﬁrings (i.e., instanti-
ations) of that rule. Their joint training algorithms
denoise over-generalized rules and train a classiﬁ-
cation model. Our approach differs from their work
in two ways: a) we do not have information of rule
exemplars - thus our labelled examples need not
have any correspondence to any of the LFs (and
may instead complement the LFs as illustrated
in Table 2) and b) we employ a semi-supervised
framework combined with graphical model for con-
sensus amongst the LFs to train our model. We also
study how to automatically select the seed set of
labelled data, rather than having a human provide
this seed set, as was done in (Awasthi et al., 2020).

Data Subset Selection: Finally, another approach
that has been gaining a lot of attention recently
is data subset selection. The speciﬁc application
of data subset selection depends on the goal at

hand. Data subset selection techniques have been
used to reduce end to end training time (Mirza-
soleiman et al., 2019; Kaushal et al., 2019; Killam-
setty et al., 2021) and to select unlabelled points in
an active learning manner to label (Wei et al., 2015;
Sener and Savarese, 2017) or for topic summariza-
tion (Bairi et al., 2015). In this paper, we present
a framework (SPEAR-SS) of data subset selection
for selecting a subset of unlabelled examples for
obtaining labels complementary to the labelling
functions.

3 Methodology

3.1 Problem Description

Let X and Y ∈ {1...K} be the feature space and
label space, respectively. We also have access to m
labelling functions (LF) λ1 to λm. As mentioned
in Section 1.1, each LF λj is designed to record
some class; let us denote2 by kj ∈ {1...K}, the
class associated with λj. The dataset consists of 2
components, viz.,
1. L = {(x1, y1, l1), (x2, y2, l2), . . . ,
(xN , yN , lN )}, which denotes the labelled dataset
and
2. U = {(xN +1, lN +1), (xN +2, lN +2), . . . ,
(xM , lM )}, which denotes the unlabelled dataset
wherein xi ∈ X , yi ∈ Y.
Here, the vector li = (li1, li2, . . . , lim) denotes the
ﬁrings of all the LFs on instance xi. Each lij can
be either 1 or 0. lij = 1 indicates that the LF λj
has ﬁred on the instance i and 0 indicates it has not.
All the labelling functions are discrete; hence, no
continuous scores are associated with them.

3.2 Classiﬁcation and Labelling Function

Models

SPEAR has a feature-based classiﬁcation model
fφ(x) which takes the features as input and predicts
the class label. Examples of fφ(x) we consider in
this paper are logistic regression and neural net-
work models. The output of this model is P f
φ (y|x),
i.e., the probability of the classes given the input
features. This model can be a simple classiﬁca-
tion model such as a logistic regression model or a
simple neural network model.

We also use an LF-based graphical model
Pθ(li, y) which, as speciﬁed in equation (1) for
an example xi, is a generative model on the LF

2We use the association of LF λj with some class kj only

in the quality guide component (QG) of the loss in eqn. 3

Notation Description

fφ
P f
φ
Pθ
LCE

H

g
LLs
LLu
KL
R

The feature-based Model
The label probabilities as per the feature-based model fφ
The label probabilities as per the LF-based Graphical Model
P f
Cross Entropy Loss: LCE
Entropy function : H(P f

P f
= − log
φ (y = ˆy|x) log P f
P f

(cid:17)
φ (y|x), ˜y
φ (y|x)) = − (cid:80)

(cid:16)

(cid:16)

(cid:17)

φ (y = ˜y|x)

φ (y = ˆy|x)

ˆy

Label Prediction from the LF-based graphical model
Supervised negative log likelihood
Unsupervised negative log likelihood summed over labels
KL Divergence between two probability models
Quality Guide based loss

Table 3: Summary of notation used.

outputs and class label y.

Pθ(li, y) =

ψθ(lij, y) =

m
(cid:89)

ψθ(lij, y)

1
Zθ

(cid:40)

j=1
exp(θjy)
1

if lij (cid:54)= 0
otherwise.

(1)

(2)

There are K parameters θj1, θj2...θjK for each LF
λj, where K is the number of classes. The model
makes the simple assumption that each LF λj inde-
pendently acts on an instance xi to produce outputs
li1, l1i...lim. The potentials ψθ invoked in equation
(1) are deﬁned in equation (2). Zθ is the normaliza-
tion factor. We propose a joint learning algorithm
with semi-supervision to employ both features and
LF predictions in an end-to-end manner.

Joint Learning in SPEAR

3.3
We ﬁrst specify the objective of SPEAR and there-
after explain each of its components in greater de-
tail:

min
θ,φ

(cid:88)

i∈L

LCE

(cid:16)

LCE

+

(cid:88)

i∈U

(cid:16)

P f

φ (y|xi), yi

(cid:17)

+

(cid:88)

H

(cid:16)

(cid:17)

P f

φ (y|xi)

i∈U
(cid:17)

P f

φ (y|xi), g(li)

+ LLs(θ|L)

+ LLu(θ|U) +

(cid:88)

(cid:16)

KL

P f

φ (y|xi), Pθ(y|li)

(cid:17)

i∈U ∪L

+ R(θ|{qj})

(3)

Before we proceed further, we refer the reader to
Table 3 in which we summarise the notation built
so far as well as the notation that we will soon be
introducing.
First Component (L1):
nent (L1) of the loss LCE

The ﬁrst compo-
(cid:16)
P f
=

φ (y|xi), yi

(cid:17)

(cid:16)

P f

(cid:17)
φ (y = yi|xi)

− log
is the standard cross-
entropy loss on the labelled dataset L for the model
P f
φ .
Second Component (L2): The second compo-
nent L2 is the semi-supervised loss on the unla-
belled data U. In our framework, we can use any

(cid:16)

(cid:16)

P f

(cid:17)
φ (y|xi)

unsupervised loss function. However, for this pa-
per, we use the Entropy minimisation (Grandvalet
and Bengio, 2005) approach. Thus, our second
component H
is the entropy of the
predictions on the unlabelled dataset. It acts as a
form of semi-supervision by trying to increase the
conﬁdence of the predictions made by the model
on the unlabelled dataset.
Third Component (L3): The third component
LCE
is the cross-entropy of the
classiﬁcation model using the hypothesised labels
from CAGE (Chatterjee et al., 2020) on U. Given
that li is the output vector of all labelling functions
for any xi ∈ U, we specify the predicted label for
xi using the LF-based graphical model Pθ(li, y)
from eqn. (1) as: g(li) = argmaxyPθ(li, y)
Fourth Component (L4): The fourth component
LLs(θ|L) is the (supervised) negative log likeli-
hood loss on the labelled dataset L as per eqn. (3):

φ (y|xi), g(li)

P f

(cid:17)

LLs(θ|L) = −

N
(cid:80)
i=1

log Pθ(li, yi)

Fifth Component (L5):
The ﬁfth component
LLu(θ|U) is the negative log likelihood loss for
the unlabelled dataset U as per eqn. (3). Since the
true label information is not available, the proba-
bilities need to be summed over y: LLu(θ|U) =

−

M
(cid:80)
i=N +1

log (cid:80)
y∈Y

Pθ(li, y)

Sixth Component (L6): The sixth component
KL(P f
φ (y|xi), Pθ(y|li)) is the Kullback-Leibler
(KL) divergence between the predictions of both
the models, viz., feature-based model fφ and the
LF-based graphical model Pθ summed over every
example xi ∈ U ∪ L. Through this term, we try
and make the models agree in their predictions over
the union of the labelled and unlabelled datasets.
Quality Guides (QG): As the last component in
our objective, we use quality guides R(θ|{qj}) on
LFs, which have been shown in (Chatterjee et al.,
2020) to stabilise the unsupervised likelihood train-
ing while using labelling functions. Let qj be the
fraction of cases where λj correctly triggered, and
let qt
j be the user’s belief on the fraction of exam-
ples xi where yi and lij agree. If the user’s beliefs
were not available, we consider the precision of the
LFs on the validation set as the user’s beliefs. Ex-
cept for the SMS dataset, we take the precision of
the LFs on the validation set as the quality guides.
If Pθ(yi = kj|lij = 1) is the model-based preci-
sion over the LFs, the quality guide based loss can
be expressed as R(θ|{qt
j log Pθ(yi =

j}) = (cid:80)

j qt

kj|lij = 1)+(1−qt
j) log(1−Pθ(yi = kj|lij = 1)).
Throughout the paper, we consider QG always in
conjunction with Loss L5.

In summary, the ﬁrst three components (L1, L2
and L3) invoke losses on the supervised model
fφ. While L1 compares the output fφ against the
ground truth in the labelled set L, L2 and L3 op-
erate on the unlabelled data U by minimizing the
entropy of fφ (L2) and by calibrating the fφ output
against the noisy predictions g(li) of the graphical
model Pθ(li, y) for each xi ∈ U (L3). The next
two components L4 and L5 focus on maximizing
the likelihood of the parameters θ of Pθ(li, y) over
labelled xi ∈ L and unlabelled xi ∈ U datasets
respectively. Finally, in L6, we compare the prob-
abilistic outputs from the supervised model fφ
against those from the graphical model Pθ(l, y)
through a KL divergence based loss. We use the
ADAM (stochastic gradient descent) optimizer to
train the non-convex loss objective.

Previous data programming approaches (Bach
et al., 2019; Chatterjee et al., 2020) adopt a cas-
caded approach in which they ﬁrst optimise a vari-
ant of L5 to learn the θ parameters associated with
the LFs and thereafter use the noisily generated
labels using g(l) to learn the supervised model fφ
using a variant of L3. In contrast, our approach
learns the LF’s θ parameters and the model’s φ
parameters jointly in the context of the unlabelled
data U.

We present synthetic experiments to illustrate the
effect of SPEAR for data programming and semi-
supervision in a controlled setting in which (i) the
overlap between classes in the data is controlled
and (ii) the labelling functions are accurate. The
details of the synthetic experiments are provided in
the appendix.

3.4 SPEAR-SS: Subset Selection with SPEAR

Suppose we are given an unlabelled data set U and
a limited budget for data labelling because of the
costs involved in it. It is essential for us to choose
the data points that need to be labelled properly.
We explore two strategies for selecting a subset
of data points from the unlabelled set. We then
obtain the labels for this subset, and run SPEAR on
the combination of this labelled and unlabelled set.
The two approaches given are intended to maximise
diversity of the selected subset in the feature space.
We complement both the approaches with Entropy
Filtering (also described below).

y∈Y

(cid:80)

i∈Uy

Unsupervised Facility Location:
In this ap-
proach, given an unlabelled data-set U, we want
to select a subset S such that the selected subset
has maximum diversity with respect to the fea-
tures. Inherently, we are trying to maximise the
information gained by a machine learning model
when trained on the subset selected. The objec-
tive function for unsupervised facility location is
funsup(S) = (cid:80)
i∈U maxj∈S σij where σij denotes
the similarity score (in the feature space X ) be-
tween data instance xi in unlabelled set U and data
instance xj in selected subset data S. We employ
a lazy greedy strategy to select the subset. In con-
junction with Entropy Filtering described below,we
call this technique Unsupervised Subset Selection.
Supervised Facility Location:
The objec-
function for Supervised Facility Lo-
tive
is fsup(S) =
cation (Wei et al., 2015)
(cid:80)
maxj∈S∩Uy σij.
Here we as-
sume that Uy ⊆ U is the subset of data points
with hypothesised label y. Simply put, Uy forms
a partition of U based on the hypothesized labels
obtained by performing unsupervised learning with
labelling functions. In conjunction with Entropy
Filtering, we call this technique Supervised Subset
Selection.
Entropy Filtering: We also do a ﬁltering based
on entropy. In particular, we sort the examples
based on maximum entropy and select f B number
of data points3, where B is the data selection bud-
get (which was set to the size of the labelled set
|L| in all our experiments). On the ﬁltered dataset,
we perform the subset selection, using either the
supervised or unsupervised facility location as de-
scribed above. Below, we describe the optimisation
algorithm for subset selection.
Optimisation Algorithms and Submodularity:
Both funsup(S) and fsup(S) are submodular func-
tions. We select a subset S of the ﬁltered unla-
belled data, by maximising these functions under
a cardinality budget k (i.e., a labelling budget).
For cardinality constrained maximisation, a simple
greedy algorithm provides a near-optimal solution
(Nemhauser et al., 1978). Starting with S 0 = ∅,
we sequentially update

S t+1 = S t ∪ argmax
j∈U\St

f (j|S t)

(4)

where f (j|S) = f (S ∪ j) − f (S) is the gain of
adding element j to set S. We iteratively execute
the greedy step (4) until t = k and |S t| = k. It

3In our experiments, we set f = 5

is easy to see that the complexity of the greedy
algorithm is O(nkTf ), where Tf is the complexity
of evaluating the gain f (j|S) for the supervised and
unsupervised facility location functions. We then
signiﬁcantly optimize this simple greedy algorithm
via a lazy greedy algorithm (Minoux, 1978) via
memoization and precompute statistics (Iyer and
Bilmes, 2019).

Dataset

MIT-R
YouTube
SMS
Census
Ionosphere
Audit
IMDB

|L|

1842
100
69
83
73
162
284

|U|

#Rules/LFs Precision %Cover

|Test|

64888
1586
4502
10000
98
218
852

15
10
73
83
64
52
25

80.7
78.6
97.3
84.1
65
87.5
80

14
87
40
100
100
100
58.1

14256
250
500
16281
106
233
500

Table 4: Statistics of datasets and their rules/LFs. Pre-
cision refers to micro precision of rules. %Cover is the
fraction of instances in U covered by at least one LF.
Size of Validation set is equal to |L|.

4 Experiments

In this section, we (1) evaluate our joint learning
against state-of-the-art approaches and (2) demon-
strate the importance of subset selection over ran-
dom subset selection. We present evaluations on
seven datasets on tasks such as text classiﬁcation,
record classiﬁcation and sequence labelling.

4.1 Datasets

We adopt the same experimental setting as in
Awasthi et al. (2020) for the dataset split and the
labelling functions. However (for the sake of fair-
ness), we set the validation data size to be equal
to the size of the labelled data-set unlike Awasthi
et al. (2020) in which the size of the validation set
was assumed to be much larger.

We use the following datasets: (1) YouTube:
A spam classiﬁcation on YouTube comments; (2)
SMS Spam Classiﬁcation (Almeida et al., 2011),
which is a binary spam classiﬁcation dataset con-
taining 5574 documents; (3) MIT-R (Liu et al.,
2013), is a sequence labelling task on each token
with following labels: Amenity, Prices, Cuisine,
Dish, Location, Hours, Others; (4) IMDB, which
is a plot summary based movie genre binary clas-
siﬁcation dataset, and the LFs (and the labelled
set) are obtained from the approach followed by
Varma and R´e (2018); (5) Census (Dua and Graff,
2017), (6) Ionosphere, and (7) Audit, which are
all UCI datasets. The task in the Census is to pre-
dict whether a person earns more than $50K or not.

Ionosphere is radar binary classiﬁcation task given
a list of 32 features. The task in the Audit is to
classify suspicious ﬁrms based on the present and
historical risk factors.

Statistics pertaining to these datasets are pre-
sented in Table 4. Since we compare performances
against models that adopt different terminology, we
refer to rules and labelling functions interchange-
ably. For fairness, we restrict the size of the vali-
dation set and keep it equal to the size |L| of the
labelled set. For all experiments involving com-
parison with previous approaches, we used code
and hyperparameters from (Awasthi et al., 2020)
but with our smaller-sized validation set. Note that
we mostly outperform them even with their larger-
sized validation set as can be seen in Table 5. More
details on training and validation set size are given
in the appendix.

4.2 Baselines

In Table 5, we compare SPEAR and SPEAR-SS
against other following standard methods on seven
datasets.
Only-L: We train the classiﬁer Pθ(y|x) only on
the labelled data L using loss component L1. As
explained earlier, following (Awasthi et al., 2020),
we observe that a 2-layered neural network trained
with the small amount of labelled data is capable
of achieving competitive accuracy. We choose this
method as a baseline and report gains over it.
L + Umaj: We train the baseline classiﬁer
Pθ(y|x) on the labelled data L along with Umaj
where labels on the U instances are obtained by
majority voting on the rules/LFs. The training
loss is obtained by weighing instances labelled
(xi,li)∈L − log Pθ(li|xi) +
by rules as minθ
γ (cid:80)
Learning to Reweight (L2R) (Ren et al., 2018):
This method trains the classiﬁer by an online train-
ing algorithm that assigns importance to examples
based on the gradient direction.
L+USnorkel (Ratner et al., 2016): Snorkel’s gen-
erative model that models class probabilities based
on discrete LFs for consensus on the noisy and con-
ﬂicting labels.
Posterior Regularization (PR) (Hu et al., 2016):
This is a method for joint learning of a rule and
feature network in a teacher-student setup.
Imply Loss (Awasthi et al., 2020): This approach
uses additional information in the form of labelled
rule exemplars and trains with denoised rule-label

(xi,yi)∈L − log Pθ(yi|xi).

(cid:80)

loss. Since it uses information in addition to what
we assume, Imply Loss can be considered as a
skyline for our proposed approaches.

4.3 Results with SPEAR

SPEAR uses the ‘best’ combination of the loss com-
ponents L1, L2, L3, L4, L5, L6. To determine
the ‘best’ combination, we perform a grid search
over various combinations of losses using valida-
tion accuracy/f1-score as the criteria for selecting
the most appropriate loss combination. Imply Loss
uses a larger-sized validation set to tune their mod-
els. In our experiments, we maintained a validation
set size equal to the size of the labelled data. In
Table 5, we observe that SPEAR performs signiﬁ-
cantly better than all other approaches on all but
the MIT-R data-set. Please note that all results are
based on the same hand-picked labelled data subset
as was chosen in prior work (Awasthi et al., 2020;
Varma and R´e, 2018), except for Audit and Iono-
sphere. Even though we do not have rule-exemplar
information in our model, SPEAR achieves better
gains than even ImplyLoss. Recall that the use of
ImplyLoss can be viewed as a skyline approach
owing to the additional exemplar information that
associates labelling functions with speciﬁc labelled
examples. The slightly lower performance of the
‘best’ SPEAR on the MIT-R data-set can be par-
tially explained by the fact that there are no LFs
corresponding to the ‘0’ class label, owing to which
our graphical model is not trained for all classes.
However, as we will show in the next section, by
suitably determining a subset of the data-set that
can be labelled (using the facility location represen-
tation function), we achieve improved performance
even on the MIT-R data-set (see Table 5). Also,
note that in Table 5, we present results on two ver-
sions of Audit, one in which both the train and
test set are balanced, and the other where the la-
belled training set is imbalanced. In the imbalanced
case (where the number of positives are only 10%),
we were unable to successfully run the ImplyLoss
and Posterior-Reg models (and hence the ‘-’), de-
spite communication with the authors. We see
that SPEAR and similarly, SPEAR-SS (discussed
below) signiﬁcantly outperform the baselines by al-
most 40% in the imbalanced case. In the balanced
case, the gains are similar to what we observe on
the other datasets.

Methods

Datasets

Only-L (Handpicked)
L+Umaj (Handpicked)
L2R (Ren et al., 2018) (Handpicked)
L+USnorkel (Ratner et al., 2016) (Handpicked)
Posterior Reg (Hu et al., 2016) (Handpicked)
ImplyLoss (Awasthi et al., 2020) (Handpicked)
SPEAR (Handpicked)
SPEAR-SS (Random Subset Selection)
SPEAR-SS (Unsupervised Subset Selection)
SPEAR-SS (Supervised Subset Selection)

YouTube
[Accuracy]

SMS
[F1]

90.7 (1.2)
+1.9 (1.1)
-3.7 (5.1)
+0.9 (2.6)
-1.9(1.6)
+0.4 (0.5)
+3.7(0.5)
+3.5
+3.9
+4.2

90.0 (3.7)
-0.3 (1.4)
+0.7 (2.9)
+0.3 (4.5)
-3.3 (1.9)
+0.9(0.9)
+3.4(0.9)
+1.8
+1.9
+3.2

MIT-R
[F1]

74.1 (0.4)
+0.1(0.2)
-20.2(0.9)
-0.3(0.2)
-0.2 (0.2)
0.9(0.4)
-0.8 (0.5)
-2.9
+2.6
+2.9

IMDB
[F1]

72.2 (3.1)
+1.2 (0.3)
+4.5(0.2)
+0.6(1.8)
+1.1 (0.7)
+4.3 (1.5)
+4.9(0.3)
+4.0
-0.6
+6.3

Census
[Accuracy]

Ionosphere
[F1]

Audit (Imb)
[F1]

Audit (Bal)
[F1]

78.3 (0.3)
-0.9 (0.4)
+3.6(0.3)
+1.7 (0.2)
-1.9 (0)
+3.4 (0.1)
+3.7 (0.3)
-5.2
+2.5
+2.5

92.7 (0.5)
+0.4 (0.7)
-18.8 (0.3)
-0.6 (0.5)
-0.1(0.7)
-3.9(2.4)
+5.4 (0.3)
+4.7
+4.8
+5.1

24.7(2.6)
-4.8 (6)
-1.2(3.2)
-7.4(3.4)
-
-
+44 (0.9)
+41.7
+43.5
+44.5

87.3 (0.9)
-1.4(4.2)
-3.0(4.9)
-0.6(4.2)
+0.1 (1.4)
+0.5(1)
+4.3 (0.9)
+2.0
+3.3
+3.5

Table 5: Performance of SPEAR and SPEAR-SS for three subset selection schemes on seven data-sets. All numbers
reported are gains over the baseline method (Only-L). All results are averaged over 5 runs. Numbers in brackets
‘()’ represent standard deviation of the original score. Handpicked instances refers to instances selected from the
dataset for designing LFs. These instances are taken directly from (Awasthi et al., 2020) to ensure fair comparison.

4.4 Results with SPEAR-SS

Recall that all results discussed so far (including
those for SPEAR) on the Youtube, SMS, MIT-R,
IMDB and Census datasets were based on the
same ‘hand-picked’ labelled data subset as in prior
work (Awasthi et al., 2020; Varma and R´e, 2018).
In the case of Audit and Ionosphere, the labelled
subset was randomly picked. In Table 5, we sum-
marise the results obtained by employing super-
vised and unsupervised subset selection schemes
for picking the labelled data-set and present com-
parisons against results obtained using (i) ‘hand-
picked’ labelled data-sets, and (ii) random selec-
tion of the labelled set. In each case, the size of
the subset is the same, which we set to be the
size of the hand-picked labelled set. Our data se-
lection schemes are applied to the ‘best’ SPEAR
model obtained across various loss components.
We observe that the best-performing model for
the supervised and unsupervised data selection
tends to outperform the best model based on ran-
dom selection. Secondly, we observe that between
the supervised and unsupervised data selection ap-
proaches, the supervised one tends to perform the
best, which means that using the hypothesised la-
bels does help. Thirdly, we observe that YouTube,
MIT-R, IMDB and Audit using the selected subset
outperform prior work that employ hand picked
data-set, whereas, in the case of SMS, Census and
Ionosphere, we come close. Finally, our approach
is more stable than other approaches as the stan-
dard deviation of SPEAR is low for 5 different runs
across all the datasets.

As an illustration, the examples such as S3
and S4 referred to in Section 1.1 were precisely
obtained through supervised subset selection in

SPEAR-SS, to form part of the labelled dataset. As
previously observed in Table 2, S3 and S4 comple-
ment (via n-grams features such as F1 and F2) the
effect of the labelling functions LF1 and LF2 on
the unlabelled examples such as S1 and S2, when
included in the labelled set. Further detailed results
with subset selection, etc. can be found in the ap-
pendix. In general, we observe that when the subset
of instances selected for labelling is complemen-
tary to the labelling functions (as in our case), the
performance is higher than when the labelled exam-
ples (exemplars) are inspired by labelling functions
themselves as done in the work by Awasthi et al.
(2020).

4.5 Signiﬁcance Test

We employ the Wilcoxon signed-rank test
(Wilcoxon, 1992) to determine whether there is
a signiﬁcant difference between SPEAR and Imply
Loss (current state-of-the-art). Our null hypothesis
is that there is not signiﬁcant difference between
SPEAR and Imply loss. For n = 7 instances, we ob-
serve that the one-tailed hypothesis is signiﬁcant at
p < .05, so we reject the null hypothesis. Clearly,
SPEAR signiﬁcantly outperforms Imply loss and,
therefore, all previous baselines.

Similarly, we perform the signiﬁcance test to
assess the difference between SPEAR-SS and Im-
ply Loss. As expected, the one-tailed hypothesis
is signiﬁcant at p < 0.05, which implies that our
SPEAR-SS approach signiﬁcantly outperforms Im-
ply Loss, and thus all other approaches.

5 Conclusion

We study how data programming can beneﬁt from
labelled data by learning a model (SPEAR) that

Methods

et

al.,

(Awasthi

ImplyLoss
2020)
SPEAR (Handpicked)
SPEAR-SS (Supervised Sub-
set Selection)

Datasets

YouTube
[Accuracy]
94.1

+0.3
+0.8

SMS
[F1]
93.2

+0.2
0.0

MIT-R
[F1]
74.3

-0.9
+1.7

Census
[Accuracy]
81.1

+0.9
-0.3

Table 6: Comparison of SPEAR and SPEAR-SS against
ImplyLoss on subset of datasets from Table 5 for which
ImplyLoss used a much larger validation set than |L|.
JL uses a validation set sizes equal to |L|.

jointly optimises the consensus obtained from la-
belling functions in an unsupervised manner, along
with semi-supervised loss functions designed in
the feature space. We empirically assess the per-
formance of the different components of our joint
loss function. As another contribution, we also
study some subset selection approaches to guide
the selection of the labelled subset of examples. We
present the performance of our models and present
insights on both synthetic and real datasets. While
outperforming previous approaches, our approach
is often better than an exemplar-based (skyline) ap-
proach that uses the additional information of the
association of rules with speciﬁc labelled exam-
ples.

Acknowledgements

We thank anonymous reviewers for providing con-
structive feedback. Ayush Maheshwari is sup-
ported by a Fellowship from Ekal Foundation
(www.ekal.org). We are also grateful to IBM Re-
search, India (speciﬁcally the IBM AI Horizon Net-
works - IIT Bombay initiative) for their support
and sponsorship.

References

Tiago A Almeida, Jos´e Mar´ıa G Hidalgo, and Akebo
Yamakami. 2011. Contributions to the study of sms
In Pro-
spam ﬁltering: new collection and results.
ceedings of the 11th ACM symposium on Document
engineering, pages 259–262.

Abhijeet Awasthi, Sabyasachi Ghosh, Rasna Goyal,
and Sunita Sarawagi. 2020. Learning from rules
generalizing labeled exemplars. In 8th International
Conference on Learning Representations,
ICLR
2020, Addis Ababa, Ethiopia, April 26-30, 2020.
OpenReview.net.

Stephen H Bach, Daniel Rodriguez, Yintao Liu, Chong
Luo, Haidong Shao, Cassandra Xia, Souvik Sen,
Alex Ratner, Braden Hancock, Houman Alborzi,

et al. 2019. Snorkel drybell: A case study in deploy-
ing weak supervision at industrial scale. In Proceed-
ings of the 2019 International Conference on Man-
agement of Data, pages 362–375.

Ramakrishna Bairi, Rishabh Iyer, Ganesh Ramakrish-
nan, and Jeff Bilmes. 2015. Summarization of multi-
document topic hierarchies using submodular mix-
In Proceedings of the 53rd Annual Meet-
tures.
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers),
pages 553–563.

Oishik Chatterjee, Ganesh Ramakrishnan, and Sunita
Sarawagi. 2020. Robust data programming with
precision-guided labeling functions. In AAAI.

Krzysztof Dembczy´nski, Wojciech Kotłowski, and Ro-
man Słowi´nski. 2008. Maximum likelihood rule en-
In Proceedings of the 25th international
sembles.
conference on Machine learning, pages 224–231.

Dheeru Dua and Casey Graff. 2017. UCI machine

learning repository.

Garrett B Goh, Charles Siegel, Abhinav Vishnu, and
Nathan Hodas. 2018. Using rule-based labels for
weak supervised learning: a chemnet for transfer-
In Proceedings
able chemical property prediction.
of the 24th ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining, pages
302–310.

Yves Grandvalet and Yoshua Bengio. 2005. Semi-
In
supervised learning by entropy minimization.
Advances in neural information processing systems,
pages 529–536.

Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard H.
Harnessing
CoRR,

Hovy, and Eric P. Xing. 2016.
deep neural networks with logic rules.
abs/1603.06318.

Rishabh Iyer and Jeff Bilmes. 2019. A memoiza-
tion framework for scaling submodular optimiza-
arXiv preprint
tion to large scale problems.
arXiv:1902.10176.

Pratik Jawanpuria, Saketha N Jagarlapudi, and Ganesh
Ramakrishnan. 2011. Efﬁcient rule ensemble learn-
ing using hierarchical kernels. In Proceedings of the
28th International Conference on Machine Learning
(ICML-11), pages 161–168.

Vishal Kaushal, Rishabh Iyer, Suraj Kothawade, Rohan
Mahadev, Khoshrav Doctor, and Ganesh Ramakrish-
nan. 2019. Learning from less data: A uniﬁed data
subset selection and active learning framework for
computer vision. In 2019 IEEE Winter Conference
on Applications of Computer Vision (WACV), pages
1289–1299. IEEE.

Krishnateja Killamsetty, Durga Sivasubramanian,
Ganesh Ramakrishnan, and Rishabh Iyer. 2021.
Glister: Generalization based data subset selection
for efﬁcient and robust learning. In AAAI.

Durk P Kingma, Shakir Mohamed, Danilo Jimenez
Rezende, and Max Welling. 2014. Semi-supervised
learning with deep generative models. In Advances
in neural information processing systems, pages
3581–3589.

Ashish Kulkarni, Narasimha Raju Uppalapati, Pankaj
Singh, and Ganesh Ramakrishnan. 2018. An interac-
tive multi-label consensus labeling model for multi-
ple labeler judgments. In Proceedings of the Thirty-
Second AAAI Conference on Artiﬁcial Intelligence,
(AAAI), 2018, pages 1479–1486. AAAI Press.

Jingjing Liu, Panupong Pasupat, Yining Wang, Scott
Cyphers, and Jim Glass. 2013. Query understanding
enhanced by hierarchical parsing structures. In 2013
IEEE Workshop on Automatic Speech Recognition
and Understanding, pages 72–77. IEEE.

Michel Minoux. 1978. Accelerated greedy algorithms
for maximizing submodular set functions. In Opti-
mization techniques, pages 234–243. Springer.

Baharan Mirzasoleiman,

and Jure
Leskovec. 2019. Data sketching for faster train-
arXiv preprint
ing of machine learning models.
arXiv:1906.01827.

Jeff Bilmes,

Kai Wei, Rishabh Iyer, and Jeff Bilmes. 2015. Submod-
ularity in data subset selection and active learning.
In International Conference on Machine Learning,
pages 1954–1963.

Frank Wilcoxon. 1992.
ranking methods.
pages 196–202. Springer.

Individual comparisons by
In Breakthroughs in statistics,

A Illustration of SPEAR on a synthetic

setting

Through a synthetic example, we illustrate the ef-
fectiveness of our formulation of combining semi-
supervised learning with labelling functions (i.e.,
combined Losses 1-6) to achieve superior perfor-
mance. Consider a 3-class classiﬁcation problem
with overlap in the feature space as depicted in
Figure 1. The classes are A, B and C. Though we
illustrate the synthetic setting in 2 dimensions, in
reality, we performed similar experiments in three
dimensions (and results were similar). We ran-

Ajay Nagesh, Ganesh Ramakrishnan, Laura Chiti-
cariu, Rajasekar Krishnamurthy, Ankush Dharkar,
and Pushpak Bhattacharyya. 2012. Towards efﬁ-
cient named-entity rule induction for customizabil-
In Proceedings of the 2012 Joint Conference
ity.
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 128–138, Jeju Island, Korea. Association for
Computational Linguistics.

George L Nemhauser, Laurence A Wolsey, and Mar-
shall L Fisher. 1978. An analysis of approximations
for maximizing submodular set functions—i. Math-
ematical programming, 14(1):265–294.

Alexander J Ratner, Christopher M De Sa, Sen Wu,
Daniel Selsam, and Christopher R´e. 2016. Data pro-
gramming: Creating large training sets, quickly. In
Advances in neural information processing systems,
pages 3567–3575.

Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel
Urtasun. 2018. Learning to reweight examples for
In International Conference
robust deep learning.
on Machine Learning, pages 4334–4343.

Ozan Sener and Silvio Savarese. 2017. Active learn-
ing for convolutional neural networks: A core-set
approach. arXiv preprint arXiv:1708.00489.

Burr Settles, Mark Craven, and Lewis Friedland. 2008.
In Pro-
Active learning with real annotation costs.
ceedings of the NIPS workshop on cost-sensitive
learning, pages 1–10. Vancouver, CA:.

Paroma Varma and Christopher R´e. 2018. Snuba: Au-
tomating weak supervision to label training data.
Proc. VLDB Endow., 12(3):223–236.

Figure 1: Synthetic data

domly pick 5 points from each class i ∈ {a, b, c},
and corresponding to each such point (xi, yi) we
create a labelling function based on its coordinates:

• LFa: Consider the point (xa, ya). The corre-
sponding LF will be: if y ≥ ya return 1 (i.e.
classify as class A) else return 0 (abstain).

• LFb: Similarly for (xb, yb) the LF will return

1 if x ≤ xb and else will return 0.

• LFc: The LF corresponding to (xc, yc) will

return 1 if x ≥ xc and else will return 0.

These seemingly 15 weak labelling functions (5
for each class) actually aid in classiﬁcation when
the labelled example set is extremely small and
the classiﬁer is unable to get a good estimate of
the class boundaries. This can be observed in Ta-
ble 7 wherein we report the F1 score on a held
out test dataset for models obtained by training
on the different loss components. The results are
reported in the case of three dimensions, wherein
each circle was obtained as a 3-dimensional gaus-
sian. The means for the three classes A, B and
C were respectively, (0, 0, 0), (0, 1, 0) and (0, 0, 1)

and the variance for each class was set to (1, 1, 1).
The training and test sets had 1000 examples each,
with roughly equal number of samples randomly
generated from each class (gaussian). In the ﬁrst
experiment (result in the ﬁrst row of Table 7), the
training was performed on the L1 loss by treating
the entire data-set of 1000 examples as labelled.
In all the other experiments only 1% (10 exam-
ples with almost uniform distribution across the 3
classes) of the training set was considered to be
labelled and the remaining (990 examples) were
treated as unlabelled.

Loss component used for training
L1 (on entire dataset as labelled)
L1 (1% labelled)
L1 (1% labelled) +L2
L1 (1% labelled)+L2+L3+L4 (1% labelled)+L5+L6
L4 (1% labelled)+L5

F1 Score
0.584
0.349
0.352
0.440
0.28

Table 7: F1 scores on test data in the synthetic setting

We make the following important observations
with respect to Table 7: (1) Skyline: When the en-
tire training data is treated as labelled and loss func-
tion L1 is minimized, we obtain a skyline model
with F1 score of 0.584. (2) With just 1% labelled
data on L1, we achieve 0.349 F1 score (using only
the labelled data). (3) We obtain an F1 score of 0.28
using the labelling functions on the unlabelled data
(for L5) in conjunction with the 1% labelled data
(for L4). (4) When the 1% labelled data (for L1)
and the remaining observed unlabelled data (for
L2) are used to train the semi-supervised model
using L1+L2, an F1 score of 0.352 is obtained. (5)
However, by jointly learning on all the loss compo-
nents, we observe an F1 score of 0.44. This is far
better than the numbers obtained using only (semi)-
supervised learning and those obtained using only
the labelling functions. Understandably, this num-
ber is lower than the skyline of 0.584 mentioned
on the ﬁrst row of Table 7.

B Network Architecture

To train our model on the supervised data L, we use
a neural network architecture having two hidden
layers with ReLU activation. We chose our classi-
ﬁcation network to be the same as (Awasthi et al.,
2020). In the case of MIT-R and SMS, the classi-
ﬁcation network contain 512 units in each hidden
layer whereas the classiﬁcation network for Census
has 256 units in its hidden layers. For the YouTube
dataset, we used a simple logistic regression as a

Datasets
SMS
MIT-R
Census
Youtube
Ionosphere
Audit
IMDB

lr (f network)
0.0001
0.0003
0.0003
0.0003
0.003
0.0003
0.0003

lr (g network) Batch Size

0.01
0.001
0.001
0.001
0.01
0.01
0.01

256
512
256
32
32
32
32

Table 8: Hyper parameter details for the different
datasets

classiﬁer network, again as followed in (Awasthi
et al., 2020). The features as well as the labelling
functions for each dataset are also directly obtained
from Snorkel (Ratner et al., 2016) and (Awasthi
et al., 2020). Please note that all experiments (bar-
ring those on subset selection) are based on the
same hand-picked labelled data subset as was cho-
sen in (Awasthi et al., 2020).

In each experiment, we train our model for 100
epochs and early stopping was performed based on
the validation set. We use Adam optimizer with the
dropout probability set to 0.8. The learning rate for
f and g network are set to 0.0003 and 0.001 respec-
tively for YouTube, Census and MIT-R datasets.
For SMS dataset, learning rate is set to 0.0001 and
0.01 for f and g network. For Ionosphere dataset,
learning rate for f is set to 0.003. For each ex-
periment, the numbers are obtained by averaging
over ﬁve runs, each with a different random ini-
tialisation. The model with the best performance
on the validation set was chosen for evaluation on
the test set. As mentioned previously, the experi-
mental setup in (Awasthi et al., 2020) surprisingly
employed a large validation set. For fairness, we re-
strict the size of the validation set and keep it equal
to the size of the labelled set. For all experiments
involving comparison with previous approaches,
we used code and hyperparameters from (Awasthi
et al., 2020) but with our smaller sized validation
set.

Following (Awasthi et al., 2020), we used binary-
F1 as an evaluation measure for the SMS, macro-F1
for MIT-R datasets, and accuracy for the YouTube
and Census datasets.

C Optimisation Algorithms and

Submodularity: Lazy Greedy and
Memoization

Both funsup(X) and fsup(X) are submodular func-
tions, and for data selection, we select a subset

Loss Combination

L1+L2+L3+L4
L1+L2+L4+L6
L1+L3+L4+L6
L1+L2+L3+L4+L6
L1+L3+L4+L5+L6
L1+L2+L3+L4+L5+L6

YouTube
(Accuracy)
94.6
92.0
94.4
94.4
94.6
94.5

SMS
(F1)
93.1
91.9
93.2
92.3
93.4
93.0

Datasets
MIT-R
(F1)
72.5
69.7
29.8
29.5
73.2
72.8

IMDB
(F1)
73.6
73.3
74.4
64.4
77.1
76.9

Census
(Accuracy)
82.0
81.3
81.0
80.9
82.0
81.9

Table 9: Performance on the test data, of various loss combinations from our objective function in equation (3). For
each dataset, the numbers in bold refer to the ‘best’ performing combination, determined based on performance
on the validation data-set. In general, we observe that all the loss components (barring L2) contribute to the best
model. Note that all combinations includes QG (Component 7).

X of the unlabelled data, which maximises these
functions under a cardinality budget (i.e. a la-
belling budget). For cardinality constrained max-
imisation, a simple greedy algorithm provides a
near optimal solution (Nemhauser et al., 1978).
Starting with X 0 = ∅, we sequentially update
X t+1 = X t ∪ argmaxj∈V \X tf (j|X t), where
f (j|X) = f (X ∪ j) − f (X) is the gain of adding
element j to set X. We run this till t = k and
|X t| = k, where k is the budget constraint.
It
is easy to see that the complexity of the greedy
algorithm is O(nkTf ) where Tf is the complex-
ity of evaluating the gain f (j|X) for the super-
vised and unsupervised facility location functions.
This simple greedy algorithm can be signiﬁcantly
optimized via a lazy greedy algorithm (Minoux,
1978). The idea is that instead of recomputing
f (j|X t), ∀j /∈t, we maintain a priority queue of
sorted gains ρ(j), ∀j ∈ V . Initially ρ(j) is set to
f (j), ∀j ∈ V . The algorithm selects an element
j /∈ X t, if ρ(j) ≥ f (j|X t), we add j to X t (thanks
to submodularity). If ρ(j) ≤ f (j|X t), we update
ρ(j) to f (j|X t) and re-sort the priority queue. The
complexity of this algorithm is roughly O(knRTf ),
where nR is the average number of re-sorts in each
iteration. Note that nR ≤ n, while in practice, it is
a constant thus offering almost a factor n speedup
compared to the simple greedy algorithm. One of
the parameters in the lazy greedy algorithms is Tf ,
which involves evaluating f (X ∪ j) − f (X). One
option is to do a na¨ıve implementation of comput-
ing f (X ∪j) and then f (X) and take the difference.
However, due to the greedy nature of algorithms,
we can use memoization and maintain a precom-
pute statistics pf (X) at a set X, using which the
gain can be evaluated much more efﬁciently (Iyer
and Bilmes, 2019). At every iteration, we evaluate
f (j|X) using pf (X), which we call f (j|X, pf ).
We then update pf (X ∪ j) after adding element j

to X. Both the supervised and unsupervised facil-
ity location functions admit precompute statistics
thereby enabling further speedups.

D Role of different components in the

loss function

Given that our loss function has seven components
(including the quality guides), a natural question
is ‘how do we choose among the different compo-
nents for joint learning (JL)?’ Another question we
attempt to answer is ‘whether all the components
are necessary for JL?’ For our ﬁnal model (i.e., the
results presented in Tables 6 and 7 of the main pa-
per), we attempt to choose the best performing JL
combination of the 7 loss components, viz. L1, L2,
L3, L4, L5, L6. To choose the ‘best’ JL combina-
tion, we evaluate the performance on the validation
set of the different JL combinations. Since we gen-
erally observe considerably weaker performance
by selecting lesser than 3 loss terms, we restrict
ourselves to 3 or more loss terms in our search.
We report performance on the test data, of various
JL combinations from our objective function for
each of the four data-sets. For each data-set, the
numbers in bold refer to the ‘best’ performing JL
combination, determined based on performance on
the validation data-set.

The observations on the results are as follows.
Firstly, we observe that all the loss components
(barring L2 for three datasets) contribute to the best
model. Furthermore, we observe that the best JL
combination (picked on the basis of the validation
set) either achieves the best performance or close
to best among the different JL combinations as
measured on the test dataset. Secondly, we observe
that QGs do not cause signiﬁcant improvement in
the performance during training.

