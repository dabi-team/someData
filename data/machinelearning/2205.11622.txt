2
2
0
2

y
a
M
3
2

]
L
P
.
s
c
[

1
v
2
2
6
1
1
.
5
0
2
2
:
v
i
X
r
a

SparseLNR: Accelerating Sparse Tensor Computations
Using Loop Nest Restructuring

Adhitha Dias
Electrical and Computer Engineering
Purdue University
West Lafayette, IN, USA
kadhitha@purdue.edu

Charitha Saumya
Electrical and Computer Engineering
Purdue University
West Lafayette, IN, USA
cgusthin@purdue.edu

Kirshanthan Sundararajah
Electrical and Computer Engineering
Purdue University
West Lafayette, IN, USA
ksundar@purdue.edu

Milind Kulkarni
Electrical and Computer Engineering
Purdue University
West Lafayette, IN, USA
milind@purdue.edu

Abstract
Sparse tensor algebra computations have become important
in many real-world applications like machine learning, sci-
entific simulations, and data mining. Hence, automated code
generation and performance optimizations for tensor alge-
bra kernels are paramount. Recent advancements such as
the Tensor Algebra Compiler (TACO) greatly generalize and
automate the code generation for tensor algebra expressions.
However, the code generated by TACO for many important
tensor computations remains suboptimal due to the absence
of a scheduling directive to support transformations such as
distribution/fusion.

This paper extends TACOâ€™s scheduling space to support
kernel distribution/loop fusion in order to reduce asymptotic
time complexity and improve locality of complex tensor
algebra computations. We develop an intermediate repre-
sentation (IR) for tensor operations called branched iteration
graph which specifies breakdown of the computation into
smaller ones (kernel distribution) and then fuse (loop fusion)
outermost dimensions of the loop nests, while the inner-
most dimensions are distributed, to increase data locality.
We describe exchanges of intermediate results between space
iteration spaces, transformation in the IR, and its program-
matic invocation. Finally, we show that the transformation
can be used to optimize sparse tensor kernels. Our results
show that this new transformation significantly improves
the performance of several real-world tensor algebra com-
putations compared to TACO-generated code.

Permission to make digital or hard copies of part or all of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. Copyrights for third-
party components of this work must be honored. For all other uses, contact
the owner/author(s).
ICS â€™22, June 28â€“30, 2022, Virtual Event, USA
Â© 2022 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-9281-5/22/06.
https://doi.org/10.1145/3524059.3532386

CCS Concepts: â€¢ Software and its engineering â†’ Source
code generation; Domain specific languages.

Keywords: Sparse Tensor Algebra, Loop Transformations,
Kernel Distribution, Loop Fusion

ACM Reference Format:
Adhitha Dias, Kirshanthan Sundararajah, Charitha Saumya, and Milind
Kulkarni. 2022. SparseLNR: Accelerating Sparse Tensor Computa-
tions Using Loop Nest Restructuring. In 2022 International Confer-
ence on Supercomputing (ICS â€™22), June 28â€“30, 2022, Virtual Event,
USA. ACM, New York, NY, USA, 14 pages. https://doi.org/10.1145/
3524059.3532386

1 Introduction
Sparse tensor algebra is used in many machine learning do-
mains such as graph neural networks [13, 14, 27]. Tensors
are a generalization of matrices and are typically represented
using n-dimensional arrays. However, when used to repre-
sent large graph-like structures, representing a tensor with a
dense array is wasteful, as most values in the tensor are zero.
In such cases, programmers use compressed representations
of these sparse tensors.

The problem of compiler optimizations for sparse codes
is well known [5, 17, 18, 34, 35], and there are several chal-
lenges that compilers face: (1) tensor computations have
to deal with specific data formats; (2) load imbalance can
arise due to irregular structure; and (3) data locality issues
arise due to the sparsity of the tensors. TACO provides a
compiler for automatically generating kernels for dense and
sparse tensor algebra operations [17]. The tensor application
is expressed in terms of three languages: a tensor algebra
language for expressing the computation (Section 2.1), a data
representation language for specifying how sparse tensors
are compressed, and a scheduling language that specifies the
schedule of the computation (Section 2.3).

The scheduling language provides the ability to define
different schedules for computations depending on tensorsâ€™
dimensionality and sparsity patterns, because one schedule

 
 
 
 
 
 
ICS â€™22, June 28â€“30, 2022, Virtual Event, USA

Adhitha Dias, Kirshanthan Sundararajah, Charitha Saumya, and Milind Kulkarni

may not fit all data formats and datasets. This allows the
separation of algorithmic specification from the scheduling
details of the computation. Once both are specified, code
can be generated to implement the desired algorithm and
schedule.

One important consequence of TACOâ€™s code generation is
that the asymptotic complexity of the kernels grows with the
number of index variables in the tensor index notation [2].
For example, the complexity of Ağ‘– ğ‘— = (cid:205)ğ‘˜ Bğ‘– ğ‘— Â· ğ¶ğ‘–ğ‘˜ Â· ğ· ğ‘—ğ‘˜ 1 is
ğ‘‚ (ğ‘›ğ‘›ğ‘§ (ğµğ¼ ğ½ )ğ¾)2, where ğµ is sparse. If this example is extended
with an additional computation, as in ğ´ğ‘–ğ‘™ = (cid:205)ğ‘˜ ğ‘— Bğ‘– ğ‘— Â· ğ¶ğ‘–ğ‘˜ Â·
ğ· ğ‘—ğ‘˜ Â· ğ¸ ğ‘—ğ‘™ , then the complexity is ğ‘‚ (ğ‘›ğ‘›ğ‘§ (ğµğ¼ ğ½ )ğ¾ğ¿) 3â€”and this
complexity increases with each additional index variable.
Hence, with increasing terms in the tensor expression, the
asymptotic complexity of the resulting code blows up.

Interestingly, this asymptotic blowup is a consequence
of doing multiple tensor operations in a single kernel. The
computation could instead be expressed as two separate
kernels, with the result of the first computation stored in a
temporary tensor: T ğ‘– ğ‘— = (cid:205)ğ‘˜ Bğ‘– ğ‘— Â· ğ¶ğ‘–ğ‘˜ Â· ğ· ğ‘—ğ‘˜ ; ğ´ğ‘–ğ‘™ = (cid:205)ğ‘— T ğ‘– ğ‘— Â·
ğ¸ ğ‘—ğ‘™ . This computation has a complexity of ğ‘‚ (ğ‘›ğ‘›ğ‘§ (ğµğ¼ ğ½ )(ğ¾ +
ğ¿)). However, writing complex computations as separate
TACO expressions has two downsides. First, it is no longer
possible to apply schedule transformations, such as outer-
loop parallelization, across the entire computation. Second,
if the computations require large temporaries, materializing
them results in performance degradation due to exhaustion
of the last-level cache.

The correct schedule looks like neither the single-kernel
approach nor the separate-kernels approach. Instead, it per-
forms a single outer loop over the ğ‘– and ğ‘— indices and then
performs the inner loop of the first kernel, stores the re-
sults in a temporary, then uses those results in the inner
loop of the second kernel. This approach has an asymptotic
complexity of ğ‘‚ (ğ‘›ğ‘›ğ‘§ (ğµğ¼ ğ½ )(ğ¾ + ğ¿)), comparable to the sepa-
rate kernel approach, but because the temporary is only live
within the inner loops, it is much smaller and hence can fit
in cache. Moreover, the overall computation is a single loop
nest, allowing for the outer loops to be parallelized, tiled, etc.
The above schedule transformation is analogous to ones in
dense tensor contraction that combine loop distribution and
fusion to create imperfectly-nested loops [4]. But it is less
clear how to use this technique on sparse loops for several
reasons: (i) analysis is harder, because of the sparse tensor
accesses and non-affine bounds, as polyhedral techniques do
not work due to the use of dynamic array bounds in loops;
(ii) producing good schedules is harder because performance
can degrade by forcing a sparse tensor to be processed using
dense iteration; and (iii) code generation is harder, as you

1Highlighted tensors denote sparse tensors.
2ğ‘›ğ‘›ğ‘§ (ğµğ¼ ğ½ ) denotes the nonzero values of the sparse tensor B bounded by
the hierarchical accesses ğ‘– and ğ‘—.
3ğ¾ and ğ¿ denote the number of iterations or the dimensionality of ğ‘˜ and ğ‘™
dimensions respectively.

need to deal with storage format-specific iteration machin-
ery. For example, a sparse matrix and dense matrix multi-
plication (SpMM) may be performed with a sparse matrix
of Compressed Sparse Row format (CSR), Coordinate for-
mat (COO), etc. [8]. Hence, the compiler needs to tackle
format-specific access patterns to generate code for SpMM
for different storage formats.

Our insight for tackling the complex scheduling trans-
formations needed to avoid asymptotic blowup while pre-
serving locality, is to use dense temporaries and introduce
Sparse Loop Nest Restructuring (SparseLNR)4 for tensor com-
putations. Crucially, these transformations can co-exist with
TACOâ€™s other scheduling primitives [30].

This paper introduces a new representation called branched
iteration graphs that support imperfect nesting of sparse it-
eration. Given this representation, our compiler can restruc-
ture sparse tensor computations to remove the asymptotic
blowup in sparse tensor algebra code generation while de-
livering good locality. Our specific contributions are;
Branched iteration graph for tensor multiplications We

generalize the iteration graph intermediate representa-
tion (IR) of TACO to support imperfectly nested loop
structures.

Branch IR transformation We design a sparse tensor trans-

formation that transforms iteration graphs to express
fusion and distribution.

New scheduling primitives We introduce a new schedul-
ing primitive that lets programmers integrate fusion
and distribution into TACO schedules.

For several real-world tensor algebra computations (De-
scribed in Section 6.2) on various datasets (Shown in Ta-
ble 1), using our new representation and transformations,
we show that SparseLNR can achieve 1.23â€“1997x (single-
thread) and 0.86â€“1263x (multi-thread) speedup over base-
line TACO schedules, and 0.27â€“3.21x (single-thread) and
0.51â€“3.16x (multi-thread) speedup over TACO schedules of
manually separated computations.

2 Background
This section provides the necessary background to under-
stand sparse tensor algebra computations and different ways
to schedule those computations.

2.1 Tensor Index Notation

Tensor index notation is a high-level representation used for
describing tensor algebra expressions [17]. Throughout the
paper we will be using both the standard notation and tensor
index notation to denote tensor operations. For instance, the
tensor computation ğ´ğ‘–ğ‘˜ = (cid:205)ğ‘— Bğ‘– ğ‘—ğ¶ ğ‘—ğ‘˜ written in standard
notation is equivalent to ğ´(ğ‘–, ğ‘˜) = B(ğ‘–, ğ‘—) âˆ— ğ¶ ( ğ‘—, ğ‘˜), written
in tensor index notation.5 Here, all the tensors are matrices

4https://github.com/adhithadias/SparseLNR
5This computation is classic matrix-matrix multiply.

SparseLNR

ICS â€™22, June 28â€“30, 2022, Virtual Event, USA

and indices ğ‘–, ğ‘—, and ğ‘˜ are used to iterate over matrices ğ´, ğµ,
and ğ¶. In this computation, index ğ‘— must be iterated over the
intersection of second dimension coordinates of ğµ and first
dimension coordinates of ğ¶, whereas index ğ‘– and ğ‘˜ must be
iterated over the first and second dimension coordinates of
ğµ and ğ¶ respectively.

2.2 Iteration Graph

We first summarize TACOâ€™s iteration graph representation,
which Kjolstad et al. describes in great detail [17]. When
computing the tensor expression Ağ‘– ğ‘— = (cid:205)ğ‘˜ Bğ‘– ğ‘—ğ¶ğ‘–ğ‘˜ğ· ğ‘—ğ‘˜ , co-
ordinates (ğ‘–, ğ‘—) of B, coordinates (ğ‘–, ğ‘˜) of C, and ( ğ‘—, ğ‘˜) of D
need to be iterated. An iterator on indices (ğ‘–, ğ‘—, ğ‘˜) can it-
erate through all the coordinates of ğµ, ğ¶, and ğ· and store
the results in ğ´. TACO represents the iteration space of a
tensor expression using an iteration graph, an intermediate
representation that defines tensor access patterns of indices.
Figure 1 shows a few examples of iteration graphs. For
example, a tensor expression Ağ‘– ğ‘— = (cid:205)ğ‘˜ Bğ‘– ğ‘—ğ¶ğ‘–ğ‘˜ğ· ğ‘—ğ‘˜ results
in an iteration graph as shown in Figure 1a such that the
indices lay in ğ‘–, ğ‘—, ğ‘˜ order. Here, the order of ğ‘— and ğ‘˜ is not
strict if C and D are dense. Figures 1b and 1c are the iteration
graphs of tensor expressions ğ´ğ‘–ğ‘˜ = (cid:205)ğ‘˜ğ‘™ Bğ‘–ğ‘˜ğ‘™ğ¶ğ‘™ ğ‘— ğ·ğ‘˜ ğ‘— and ğ‘¦ğ‘– =
(cid:205)ğ‘—ğ‘˜ Bğ‘– ğ‘— C ğ‘—ğ‘˜ğ‘£ğ‘˜ respectively.

Nodes in the iteration graph represent indices of tensor
index notation. In other words, the iteration graph is a di-
rected graph of these indices. These indices of the graph are
topologically sorted such that it imposes sparse iteration con-
straints (i.e.,constraints that define the sparse tensor access
patterns of indices due to lack of random access in general).
Each index in the iteration graph can be expressed as a loop
to iterate through a tensor. Therefore, a given tensor multi-
plication can be computed using nested loops, where each
loop corresponds to an index variable in the iteration graph.

Definition 2.1. An iteration graph is a directed graph ğº =
(ğ‘‰ , ğ‘ƒ) where ğ‘‰ = ğ‘£1, ğ‘£2, ..., ğ‘£ğ‘› defines the set of index vari-
ables in the tensor index notation, and ğ‘ƒ = ğ‘1, ğ‘2, ..., ğ‘ğ‘› de-
fines the set of tensor paths, a tensor path is a tuple of index
variables associated with a particular tensor variable.

2.3 Scheduling Primitives

A tensor expression can have multiple valid schedules of
computation as there are different valid orders of iterating
through indices and multiple parallelization strategies. Kjol-
stad et al. [17] and Senanayake et al. [30] have introduced
scheduling primitives for tensor computations, with which
the user can describe schedules to execute a given tensor
computation. The scheduling primitives in TACO are the
split directive to split a loop into two loops for tiling, col-
lapse directive to collapse doubly nested loops into a single
loop for balancing load among threads, reorder directive6

6Also referred to as permute directive in the literature.

(a)

(b)
Figure 1. Iteration graphs (a) SDDMM kernel Ağ‘– ğ‘— =
(cid:205)ğ‘˜ Bğ‘– ğ‘—ğ¶ğ‘–ğ‘˜ğ· ğ‘—ğ‘˜ (b) Khatri-Rao product (MTTKRP) kernel
Ağ‘–ğ‘˜ = (cid:205)ğ‘˜ğ‘™ Bğ‘–ğ‘˜ğ‘™ğ¶ğ‘™ ğ‘— ğ·ğ‘˜ ğ‘— (c) Sparse matrix vector multipli-
cation (SpMV) kernel preceded by another SpMV kernel
ğ‘¦ğ‘– = (cid:205)ğ‘—ğ‘˜ Bğ‘– ğ‘— (C ğ‘—ğ‘˜ğ‘£ğ‘˜ )

(c)

to reorder loops, unroll directive to perform loop unrolling,
parallelize directive to parallelize loops with OpenMP-based
multithreaded execution (for outer loops) or vectorized ex-
ecution (for inner loops). Furthermore, Kjolstad et al. [16]
added precompute scheduling directive to use intermediate
dense workspaces to remove sparse accesses when storing
data values to output tensors.

3 Overview
There are a number of factors taken into account when decid-
ing whether to apply transformations across kernels. If the
working sets are small, running the kernels separately with
good schedules defined on each individual kernel maybe
faster than a fused kernel. But if the working sets are large
resulting in large temporaries that do not fit in caches, it is
better to fuse two kernels and try to maximize the data reuse
by using the results produced by the first kernel and execute
part of the second kernel without waiting for the completion
of the first kernel.

3.1 Motivating Example
Consider the computation, ğ´ = ğ‘†ğ‘ğ‘ğ‘Ÿğ‘ ğ‘’ ğµ âŠ™ (ğ¶ğ·) Â· ğ¸ that is
used in graph embedding and graph neural networks [27, 36].
The Hadamard product, or element-wise product, is denoted
by âŠ™ and matrix multiplication is denoted by Â·. We can per-
form the above computation in the following order with
fine-grained smaller tensor operations. ğ‘‡ = ğ‘”ğ‘’ğ‘šğ‘š(ğ¶, ğ·),
ğ‘†ğ‘ğ‘ğ‘Ÿğ‘ ğ‘’ ğ‘ˆ = ğ‘ ğ‘ğ‘’ğ‘™ğ‘šğ‘š(ğ‘†ğ‘ğ‘ğ‘Ÿğ‘ ğ‘’ ğµ,ğ‘‡ ), ğ´ = ğ‘ ğ‘ğ‘šğ‘š(ğ‘†ğ‘ğ‘ğ‘Ÿğ‘ ğ‘’ ğ‘ˆ , ğ¸).
Here, ğ‘”ğ‘’ğ‘šğ‘š stands for the generalized matrix multiplication,
ğ‘ ğ‘ğ‘’ğ‘™ğ‘šğ‘š stands for sparse element-wise multiplication, and
ğ‘ ğ‘ğ‘šğ‘š stands for sparse matrix multiplication. Materializa-
tion of these intermediate tensors leads to multiple issues:

1. Dense matrix multiplication results in redundant calcula-
tions and unnecessary increase in asymptotic complexity,
because later it is sampled by the Sparse B matrix.

ICS â€™22, June 28â€“30, 2022, Virtual Event, USA

Adhitha Dias, Kirshanthan Sundararajah, Charitha Saumya, and Milind Kulkarni

for (int32_t jB = B2_pos[i]; jB < B2_pos[(i + 1)]; jB++) {

int32_t j = B2_crd[jB];
double tkY_val = 0.0;
for (int32_t k = 0; k < D2_dimension; k++) {

1 int32_t jY = 0;
2 for (int32_t i = 0; i < C1_dimension; i++) {
3
4
5
6
7
8
9
10
11
12 }

}
Y_vals[jY] = tkY_val;
jY++;

}

(a) Y ğ‘– ğ‘— = (cid:205)ğ‘˜ Bğ‘– ğ‘—ğ¶ ğ‘—ğ‘˜ ğ·ğ‘˜ ğ‘—

tkY_val += B_vals[jB] âˆ— C_vals[i,k] âˆ— D_vals[j,k];

for (int32_t jY = Y2_pos[i]; jY < Y2_pos[(i + 1)]; jY++) {

int32_t j = Y2_crd[jY];
for (int32_t l = 0; l < E2_dimension; l++) {

1 for (int32_t i = 0; i < Y1_dimension; i++) {
2
3
4
5
6
7
8 }

}

}

(b) ğ´ğ‘–ğ‘™ = (cid:205)ğ‘— Y ğ‘– ğ‘— ğ¸ ğ‘—ğ‘™

A_vals[i,l] = A_vals[i,l] + Y_vals[jY] âˆ— E_vals[j,l];

for (int32_t jB = B2_pos[i]; jB < B2_pos[(i + 1)]; jB++) {

double tkA = 0.0;
for (int32_t k = 0; k < D2_dimension; k++) {

int32_t j = B2_crd[jB];
for (int32_t l = 0; l < E2_dimension; l++) {

1 for (int32_t i = 0; i < C1_dimension; i++) {
2
3
4
5
6
7
8
9
10
11
12 }

}
A_vals[i,l] = A_vals[i,l] + tkA;

}

}

(c) ğ´ğ‘–ğ‘™ = (cid:205)ğ‘—ğ‘˜ Bğ‘– ğ‘—ğ¶ ğ‘—ğ‘˜ ğ·ğ‘˜ ğ‘— ğ¸ ğ‘—ğ‘™

tkA += B_vals[jB]âˆ— C_vals[i,k]âˆ— D_vals[j,k]âˆ— E_vals[j,l];

for (int32_t jB = B2_pos[i]; jB < B2_pos[(i + 1)]; jB++) {

int32_t j = B2_crd[jB];
double Y_val = 0.0;
for (int32_t k = 0; k < D2_dimension; k++) {

1 for (int32_t i = 0; i < C1_dimension; i++) {
2
3
4
5
6
7
8
9
10
11
12 }

}
for (int32_t l = 0; l < E2_dimension; l++) {

}

}

A_vals[i,l] = A_vals[i,l] + Y_val âˆ— E_vals[j,l];

Y_val += B_vals[jB] âˆ— C_vals[i,k] âˆ— D_vals[j,k];

(d) ğ´ğ‘–ğ‘™ = (cid:205)ğ‘— ((cid:205)ğ‘˜ Bğ‘– ğ‘—ğ¶ ğ‘—ğ‘˜ ğ·ğ‘˜ ğ‘— )ğ¸ ğ‘—ğ‘™

Figure 2. Different schedules of executing ğ´ğ‘–ğ‘™ = (cid:205) Bğ‘– ğ‘— Â· ğ¶ ğ‘—ğ‘˜ Â· ğ·ğ‘˜ ğ‘— Â· ğ¸ ğ‘—ğ‘™ . The code snippet 2b executed immediately after the code
snippet 2a computes the same result as fused operations explained in the code snippets 2c and 2d. Here, the code snippet 2c
has a perfectly nested loop structure while the code 2d describes a nested loop structure for the same computation.

2. Values are produced long before they are consumed, which

may cause them to be evicted from caches.

3. Having intermediate tensors is justifiable if intermediate
results are needed for some other computation, neverthe-
less a single kernel maybe needed for faster operation.

Introducing kernel fusion to tensor computations can re-
duce these issues [27]. In this section, we discuss different
schedules for performing the computation ğ´ = ğ‘†ğ‘ğ‘ğ‘Ÿğ‘ ğ‘’ ğµ Â·
(ğ¶ğ·) âˆ— ğ¸, and motivate the need for supporting loop fusion
for sparse tensor computations.

First, we discuss the opportunities for distribution in the
running example using a fused kernel with high asymptotic
complexity (Section 3.1.1). Next, we discuss opportunities for
fusion when the computation is split into two smaller kernels
(Section 3.1.2). Finally, in Section 3.1.3 we discuss how we can
exploit these different scenarios to construct a distributed
(versus the fused kernel in Section 3.1.1) and then fused (as
compared to the kernel in Section 3.1.2) implementation.

3.1.1 Asymptotic expensive fused kernel. The compu-
tation ğ´ğ‘–ğ‘™ = (cid:205) Bğ‘– ğ‘— Â· ğ¶ğ‘–ğ‘˜ Â· ğ· ğ‘—ğ‘˜ Â· ğ¸ ğ‘—ğ‘™ can be fully realized using
a nested loop iterator defined by all indices ğ‘–, ğ‘—, ğ‘˜, and ğ‘™. The
generalized way of producing kernels for a tensor multipli-
cation of this kind in TACO is by generating an iteration
graph (see Section 2.2). Since the iteration graph contains all
the indices in a linear tree pattern, TACO generates a kernel

as in Figure 2c, with time complexity of ğ‘‚ (ğ‘›ğ‘›ğ‘§ (ğµğ¼ ğ½ )ğ¾ğ¿) due
to the quadruple linearly nested loops (lines 1â€“6).

3.1.2 Asymptotically inexpensive distributed kernels.
However, the computation ğ´ğ‘–ğ‘™ = (cid:205)ğ‘˜ ğ‘— Bğ‘– ğ‘— Â·ğ¶ğ‘–ğ‘˜ Â·ğ· ğ‘—ğ‘˜ Â·ğ¸ ğ‘—ğ‘™ can be
performed by evaluating two smaller kernels: sampled dense-
dense matrix multiplication (SDDMM): Y ğ‘– ğ‘— = (cid:205)ğ‘˜ Bğ‘– ğ‘— Â·ğ¶ğ‘–ğ‘˜ Â·ğ· ğ‘—ğ‘˜
followed by SpMM: ğ´ğ‘–ğ‘™ = (cid:205)ğ‘— Y ğ‘– ğ‘— Â· ğ¸ ğ‘—ğ‘™ . As these separate ker-
nels are triply nested loops (lines 2â€“6 in Figure 2a and 1â€“3
in Figure 2b), they have lower asymptotic complexity.

Here, the Hadamard product, in SDDMM, results in Y ğ‘– ğ‘—
matrixâ€™s sparse structure to be same as Bğ‘– ğ‘— . Therefore, the
asymptotic complexity of performing two tensor computa-
tions with an intermediary matrix Y ğ‘– ğ‘— is ğ‘‚ (ğ‘›ğ‘›ğ‘§ (ğµğ¼ ğ½ )(ğ¾ +ğ¿)).
These separate kernels can be realized through loop distri-
bution of the kernel from Section 3.1.1.

Although we achieve a lower asymptotic complexity, we
are using an intermediary tensor to pass values between SD-
DMM and SpMM, Hence, we miss the opportunity to exploit
the temporal locality of the operation. The tensor contrac-
tion computed using linearly nested loops in Section 3.1.1.
is expensive because of the high degree of nesting in the
computation and the redundant duplicate computations, but
may still be good for memory-constrained systems because
the computation does not require any memory for storing
intermediate results.

SparseLNR

ICS â€™22, June 28â€“30, 2022, Virtual Event, USA

Using a temporary tensor to hold the result of the SDDMM
operation is acceptable as long as the dimensionality of the
index variables ğ‘– and ğ‘—, and the density of the temporary
tensor, are small. The code generation algorithm in TACO
is limited to generating sequential code when the output
tensor is of sparse format, (see ğ‘—ğ‘Œ variable in Figure 2a). The
kernel is sequential because the data format used to store
the results of the computation limits random accesses. Here,
the output of SDDMM operation is sparse (and the output
from SpMM is dense) in which case we cannot parallelize
the outermost loop of the SDDMM operation in separate
kernel execution whereas the kernel in Figure 2c can be
parallelized because the output of the combined kernel is
dense. This is another valid reason to prefer the single kernel
implementation despite its high asymptotic complexity.

3.1.3 Fused kernel with low asymptotic complexity.
Since both the kernels Y ğ‘– ğ‘— = (cid:205) Bğ‘– ğ‘— Â·ğ¶ğ‘–ğ‘˜ Â· ğ· ğ‘—ğ‘˜ in Figure 2a and
ğ´ğ‘–ğ‘™ = (cid:205) Y ğ‘– ğ‘— Â· ğ¸ ğ‘—ğ‘™ in Figure 2b have the same access patterns
in their two outer-most loops, we can fuse them as shown
in Figure 2d, removing the use of the intermediary tensor
to pass the values between the two separate kernels as ex-
plained in Section 3.1.2. This execution has a time complexity
of ğ‘‚ (ğ‘›ğ‘›ğ‘§ (ğµğ¼ ğ½ )(ğ¾ + ğ¿)), and at the same time removes the
usage of a large tensor temporary by using an imperfectly
nested loop structure (Lines 1â€“2,5 and 8 in Figure 2d).

Note that this partially-fused kernel provides the best of
both worlds. Like the separate kernel approach, it has low
asymptotic complexity. Like the fused kernel approach, it
has good locality (since the temporaries only need to store
data from the inner loops, their sizes much smaller and the
reuse distances are reduced). Furthermore, because the outer
loops of the partially fused are shared between both compu-
tations, and there is no longer a loop-carried dependence for
SDDMM, the overall kernel can be parallelized in the same
way as the kernel of Figure 2c.

3.2 Our approach: SparseLNR

While the schedule of computation in Figure 2d provides
both good asymptotic complexity and good locality, no exist-
ing system can automatically generate this type of schedule
when generating code for sparse computations. TACO only
handles â€œlinearâ€ iteration graphs that yield perfectly-nested
loops, and hence cannot handle the partially-fused, imper-
fectly nested loop structure needed by our example. On the
other hand, prior work on distribution and fusion for tensor
computations [4], can support this type of code structure
only for operations on dense tensors.

SparseLNR provides mechanisms for generating the code
in Figure 2d from a high level representation of the computa-
tion as well as scheduling directives that inform the structure
of the code. We introduce several components to perform
this code generation and Section 4 discuss them in detail.

1. We introduce a new representation called a branched itera-
tion graph that allows the representation of partially-fused
iteration structures, where some loops in a loop nest are
common between computations and others are separate.
Hence, this graph represents imperfect nesting. We care-
fully place constraints on these graphs to ensure that the
requirements of nested iteration over sparse structures are
met. The branched iteration graph is described in more
detail in Section 4.2.

2. We introduce new scheduling primitives for loop distri-
bution and fusion that allow programmers to generate the
branched iteration graph by applying scheduling trans-
formations to linear TACO iteration graph. We describe
the primitives and describe how they systematically trans-
form a branched iteration graph in Section 4.3.1.

3. We adapt TACOâ€™s code generation strategies to the branched
iteration graph, allowing SparseLNR to generate sparse
iteration code for tensor kernels that have had our distri-
bution and fusion transformations applied to them. We
discuss code generation in Section 4.4.

4 Detailed Design
This section describes the key components of SparseLNR.
Section 4.1 describes SparseLNRâ€™s new branched iteration
graph representation. Section 4.2 shows how partial fusion
is represented through iteration graph transformations. Sec-
tion 4.3.1 explains how scheduling directives can guide par-
tial fusion while still composing with TACOâ€™s existing sched-
uling language. Finally, Section 4.4 explains how SparseLNR
generates code.

4.1 Representation

SparseLNR uses a branched iteration graph to represent sparse
tensor algebra kernels, which is an extension to the concrete
index notation described in [16]. A branched iteration graph
can be understood as an iteration graph with branches in
index access patterns. By transforming the linear index tree
iteration graph generated by TACO to a branched iteration
graph in the context of tensor multiplication, we try to re-
move the asymptotic complexity that arises from perfectly
linearly nested loops in dense/sparse iterations.

Definition 4.1. A branched iteration graph is a directed
graph ğº = (ğ‘‰ , ğºğ‘, ğºğ‘, ğ‘ƒ), where ğ‘‰ is a set of unbranched
indices, organized as a sequence starting from the root of
the iteration graph that then has two children graphs, ğºğ‘
(producer) and ğºğ‘ (consumer), that define the two branches
of ğº, where ğºğ‘ and ğºğ‘ themselves are branched iteration
graphs, such that there is a dependence edge from ğºğ‘ to ğºğ‘
and a boundary between ğ‘‰ and (ğºğ‘, ğºğ‘ ). The dependence
edge tracks the common set of indices in ğºğ‘ and ğºğ‘ . ğ‘ƒ =
ğ‘1, ğ‘2, . . . , ğ‘ğ‘› defines the set of tensor paths, a tuple of indices
associated with a particular tensor variable.

ICS â€™22, June 28â€“30, 2022, Virtual Event, USA

Adhitha Dias, Kirshanthan Sundararajah, Charitha Saumya, and Milind Kulkarni

(a) Original kernel

(b) SDDMM

(c) SpMM

(d) Producer/Consumer kernels

(e) Fused kernel

Figure 3. loopfuse transformation performed on ğ´ğ‘–ğ‘™ = (cid:205)ğ‘—ğ‘˜ Bğ‘– ğ‘—ğ¶ğ‘–ğ‘˜ğ· ğ‘—ğ‘˜ğ¸ ğ‘—ğ‘™

Intuitively, where a TACO iteration graph corresponds to
a perfectly-nested loop where the order of the vertices in
the graph corresponds to the nesting order of the loops, a
branched iteration graph represents an imperfectly nested
loop. ğ‘‰ corresponds to the common outer loops, just as in a
TACO graph, while ğºğ‘ and ğºğ‘ correspond to the inner loop
nests (which can themselves be imperfectly nested). For ex-
ample, in Figure 3e, ğ‘‰ refers to the set of indices {ğ‘–, ğ‘—}, and ğºğ‘ ,
ğºğ‘ refer to the boxes Producer and Consumer, respectively.

4.2 Branched Iteration Graph Transformation

In Section 3.1 we saw how we could perform loop fusion
or distribution for a sparse tensor algebra computation. We
recognize this pattern in index traversal and exploit it to
generate the branched iteration graph. We name this pat-
tern recognition algorithm fusion after distribution because
it proceeds in two steps as described in Algorithm 1: (i) dis-
tributing the perfectly-nested indices in the iteration graph,
and then (ii) fusing the common indices.

Topologically sorted iteration graph. The iteration graph

in Figure 3a relates to the index expression ğ´(ğ‘–, ğ‘™) = B(ğ‘–, ğ‘—) âˆ—
ğ¶ (ğ‘–, ğ‘˜)âˆ—ğ· ( ğ‘—, ğ‘˜)âˆ—ğ¸ ( ğ‘—, ğ‘™), where B is sparse. We denote this ker-
nel as <SDDMM, SpMM>. The indices here are topologically
ordered such that the ordering of the indices are constrained
by the sparsity patterns of the sparse tensors. The ordering
ğ‘– â†’ ğ‘— â†’ ğ‘˜ â†’ ğ‘™ would be consistent with the access pat-
terns of all the tensors ğ‘– â†’ ğ‘™ in A, ğ‘– â†’ ğ‘— in B, ğ‘– â†’ ğ‘˜ in C,
ğ‘— â†’ ğ‘˜ in D, and ğ‘— â†’ ğ‘™ in E. However, there should be a
hard ordering imposed on ğ‘– and ğ‘— index variables because
ğ‘— cannot be accessed without accessing ğ‘– first. The index
access patterns of the tensor access variables are marked in
the graph as paths. ğ´1 denotes the first access dimension
of the A and ğ´2 denotes the second access dimension of
the A tensor. The fusion algorithm requires the iteration
graph in Figure 3a and the tensor index notation expression

Algorithm 1 Loop fusion after distribution
Input: Topologically Ordered Iteration Graph ğºğ¼ = (ğ¼ğº, ğ‘ƒ)
Input: Index Expression ğ¸ğ‘¥ğ‘ğ‘Ÿ : ğ´ğ‘œğ‘¢ğ‘¡ = ğ´1 âˆ— ğ´2 âˆ— ... âˆ— ğ´ğ‘›
Input: Bool ğ‘Ÿğ‘’ğ‘ğ‘¢ğ‘Ÿğ‘ ğ‘–ğ‘£ğ‘’
Output: Branched Iteration Graph ğº â€²
ğ¼

1: fusible = isFusible(ğºğ¼ )
2: if !fusible then return ğºğ¼
3: ğ‘ƒğ‘‡ â€² = ğ‘ƒğ´ğ‘œğ‘¢ğ‘¡ âˆ’ ğ‘ƒğ´ğ‘› âŠ² index path for temporary tensor ğ‘‡ â€²
4: ğºğ¼ âˆ’ğ‘ƒğ‘Ÿğ‘œğ‘‘ğ‘¢ğ‘ğ‘’ğ‘Ÿ , ğ‘ƒğ‘Ÿğ‘œğ‘‘ğ‘¢ğ‘ğ‘’ğ‘Ÿğ¸ğ‘¥ğ‘ğ‘Ÿğ‘¡ğ‘’ğ‘šğ‘ := ğ‘‡ â€²(ğ‘ƒğ‘‡ â€²) = ğ¸ğ‘¥ğ‘ğ‘Ÿ \ ğ´ğ‘›
5: ğºğ¼ âˆ’ğ¶ğ‘œğ‘›ğ‘ ğ‘¢ğ‘šğ‘’ğ‘Ÿ , ğ‘ƒğ‘Ÿğ‘œğ‘‘ğ‘¢ğ‘ğ‘’ğ‘Ÿğ¸ğ‘¥ğ‘ğ‘Ÿğ‘¡ğ‘’ğ‘šğ‘ := ğ´ğ‘œğ‘¢ğ‘¡ = ğ‘‡ â€²(ğ‘ƒğ‘‡ â€²) âˆ— ğ´ğ‘›
6: if recursive then
7:

ğºğ¼ âˆ’ğ‘ƒğ‘Ÿğ‘œğ‘‘ğ‘¢ğ‘ğ‘’ğ‘Ÿ = ğ‘Ÿğ‘’ğ‘ğ‘¢ğ‘Ÿğ‘ ğ‘–ğ‘£ğ‘’ğ¶ğ‘ğ‘™ğ‘™ (ğºğ¼ âˆ’ğ‘ƒğ‘Ÿğ‘œğ‘‘ğ‘¢ğ‘ğ‘’ğ‘Ÿ ,
â†©â†’ ğ‘ƒğ‘Ÿğ‘œğ‘‘ğ‘¢ğ‘ğ‘’ğ‘Ÿğ¸ğ‘¥ğ‘ğ‘Ÿğ‘¡ğ‘’ğ‘šğ‘, ğ‘Ÿğ‘’ğ‘ğ‘¢ğ‘Ÿğ‘ ğ‘–ğ‘£ğ‘’)

8: ğ¿ğ‘–ğ‘ ğ‘¡ğ¼ âˆ’ğ‘ƒğ‘Ÿğ‘œğ‘‘ğ‘¢ğ‘ğ‘’ğ‘Ÿ = ğºğ‘’ğ‘¡ğ¼ğ‘›ğ‘‘ğ‘–ğ‘ğ‘’ğ‘  (ğºğ¼ âˆ’ğ‘ƒğ‘Ÿğ‘œğ‘‘ğ‘¢ğ‘ğ‘’ğ‘Ÿ )
9: ğ¿ğ‘–ğ‘ ğ‘¡ğ¼ âˆ’ğ¶ğ‘œğ‘›ğ‘ ğ‘¢ğ‘šğ‘’ğ‘Ÿ = ğºğ‘’ğ‘¡ğ¼ğ‘›ğ‘‘ğ‘–ğ‘ğ‘’ğ‘  (ğºğ¼ âˆ’ğ¶ğ‘œğ‘›ğ‘ ğ‘¢ğ‘šğ‘’ğ‘Ÿ )
10: Define: ğ¼ğ‘ â„ğ‘ğ‘Ÿğ‘ğ‘ğ‘™ğ‘’ = âˆ…
11: for Each ğ‘– âˆˆ ğ¼ğº do
12:

if ğ‘– âˆˆ ğ¿ğ‘–ğ‘ ğ‘¡ğ¼ âˆ’ğ‘ƒğ‘Ÿğ‘œğ‘‘ğ‘¢ğ‘ğ‘’ğ‘Ÿ and ğ‘– âˆˆ ğ¿ğ‘–ğ‘ ğ‘¡ğ¼ âˆ’ğ¶ğ‘œğ‘›ğ‘ ğ‘¢ğ‘šğ‘’ğ‘Ÿ then

ğ¼ğ‘ â„ğ‘ğ‘Ÿğ‘ğ‘ğ‘™ğ‘’ = ğ¼ğ‘ â„ğ‘ğ‘Ÿğ‘ğ‘ğ‘™ğ‘’ âˆª ğ‘–

else

break;

13:
14:
15: Define: ğ¼ğ‘“ ğ‘¢ğ‘ ğ‘ğ‘ğ‘™ğ‘’ = âˆ…
16: for ğ‘– â† 1 to ğ‘ do
17:

if ğ‘– âˆ‰ ğ¼ğ‘ â„ğ‘ğ‘Ÿğ‘ğ‘ğ‘™ğ‘’ and ğ‘– âˆˆ ğ¿ğ‘–ğ‘ ğ‘¡ğ¼ âˆ’ğ‘ƒğ‘Ÿğ‘œğ‘‘ğ‘¢ğ‘ğ‘’ğ‘Ÿ and
â†©â†’ ğ‘– âˆˆ ğ¿ğ‘–ğ‘ ğ‘¡ğ¼ âˆ’ğ¶ğ‘œğ‘›ğ‘ ğ‘¢ğ‘šğ‘’ğ‘Ÿ then

else

break;

ğ¼ğ‘“ ğ‘¢ğ‘ ğ‘ğ‘ğ‘™ğ‘’ = ğ¼ğ‘“ ğ‘¢ğ‘ ğ‘ğ‘ğ‘™ğ‘’ âˆª ğ‘–

18:
19:
20: Define: ğ‘‡ (ğ‘ƒğ¼ğ‘“ ğ‘¢ğ‘ ğ‘ğ‘ğ‘™ğ‘’ )
21: ğ‘ƒğ‘Ÿğ‘œğ‘‘ğ‘¢ğ‘ğ‘’ğ‘Ÿğ¸ğ‘¥ğ‘ğ‘Ÿ := ğ‘‡ (ğ‘ƒğ¼ğ‘“ ğ‘¢ğ‘ ğ‘ğ‘ğ‘™ğ‘’ ) = ğ‘‡ â€²(ğ‘ƒğ‘‡ â€²)
22: ğ¶ğ‘œğ‘›ğ‘ ğ‘¢ğ‘šğ‘’ğ‘Ÿğ¸ğ‘¥ğ‘ğ‘Ÿ := ğ´ğ‘œğ‘¢ğ‘¡ = ğ‘‡ (ğ‘ƒğ¼ğ‘“ ğ‘¢ğ‘ ğ‘ğ‘ğ‘™ğ‘’ ) âˆ— ğ´ğ‘
23: return ğºğ‘Ÿğ‘ğ‘â„ğ‘…ğ‘’ğ‘¤ğ‘Ÿğ‘–ğ‘¡ğ‘’ (ğºğ¼ , ğ¼ğ‘ â„ğ‘ğ‘Ÿğ‘ğ‘ğ‘™ğ‘’,

â†©â†’ ğ‘ƒğ‘Ÿğ‘œğ‘‘ğ‘¢ğ‘ğ‘’ğ‘Ÿğ¸ğ‘¥ğ‘ğ‘Ÿ, ğ¶ğ‘œğ‘›ğ‘ ğ‘¢ğ‘šğ‘’ğ‘Ÿğ¸ğ‘¥ğ‘ğ‘Ÿ )

ğ´(ğ‘–, ğ‘™) = B(ğ‘–, ğ‘—) âˆ— ğ¶ (ğ‘–, ğ‘˜) âˆ— ğ· ( ğ‘—, ğ‘˜) âˆ— ğ¸ ( ğ‘—, ğ‘™). We identify the
iteration graph as fusible if there are indices that are only

SparseLNR

ICS â€™22, June 28â€“30, 2022, Virtual Event, USA

present in the last tensor and the output tensor in the tensor
expression (line 1 of the Algorithm 1).

Distribution into two kernels. The description of the
tensor kernel above captures all the information of perform-
ing kernel executions SDDMM: T â€²(ğ‘–, ğ‘—) = B(ğ‘–, ğ‘—) âˆ— ğ¶ (ğ‘–, ğ‘˜) âˆ—
ğ· ( ğ‘—, ğ‘˜) and SpMM: ğ´(ğ‘–, ğ‘™) = T â€²(ğ‘–, ğ‘—) âˆ— ğ¸ ( ğ‘—, ğ‘™) sequentially.
(Notice that separation of kernels requires a temporary ma-
trix ğ‘‡ â€²) Therefore, we can recover the separate 2 smaller
kernels that would yield the same result given the larger
tensor expression. We denote the first kernel as the producer
and the second kernel as the consumer. To find these sepa-
rate smaller kernels, we need to remove the last tensor ğ¸ ( ğ‘—, ğ‘™)
from the original expression. Line 8 of the Algorithm 1 cre-
ates the producer index expression and iteration graph for
the tensor computation performed first (SDDMM in our run-
ning example) by removing the last tensor from the original
expression, and then line 9 of the Algorithm 1 creates the
consumer index expression and iteration graph for the tensor
computation that is performed second (SpMM in our run-
ning example) by adding it back to the producerâ€™s expression.
These 2 separate kernels would have iteration graphs shown
in Figures 3b and 3c respectively. We perform this recovery
of the two separate operations in order to identify the fusible
and shared indices between two separate tensor operations
as we will further explain in a next paragraph.

Fusing common loops. Once we have the iteration graphs
for the separate kernels we reason about them together
(See 3d). We reason that both the sparse iterations need
to iterate through the space using index variables ğ‘– and ğ‘—.
Also, iteration space defined by the index ğ‘˜ is iterated only
by the SDDMM operation, and the iteration space defined
by the index ğ‘™ is only iterated by the SpMM operation. But
those iterations over index ğ‘˜ and ğ‘™ need to happen one af-
ter the other. The producer-consumer dependence must be
satisfied such that the values consumed by the consumer
must have been produced by the producer before its use.
The values shared between the producer and consumer can
be stored in an intermediate scratch memory. Furthermore,
the comparison of the two graphs, the producer graph and
the consumer graph, helps identify the indices that can and
cannot be shared among the iterations.

The producer graph in Figure 3b and the consumer graph
in Figure 3b have a common prefix defined by some indices
in their iteration graphs. We run a prefix match to identify
the shared indices by the two kernels (lines 8â€“14 of the Al-
gorithm 1), in Figure 3d. We see that both ğ‘– and ğ‘— indices are
shared, and the other variables are not shared. The addition
of indices ğ‘– and ğ‘— to the set of sharable indices is described
in lines 10â€“14 of the Algorithm 1. We identify this point as
a nest boundary in the iteration graph 3d, and denote the
indices above the nest boundary as fusible. The final out-
put of executing the fusion after distribution algorithm is a

branched iteration graph. Therefore, if the algorithm is ap-
plied recursively (see the kernel <SDDMM, SpMM, GEMM>
in the benchmark Section 6.2) on the producer (lines 6â€“7 of
the Algorithm 1), our algorithm can still match the prefix
even if the producer graph is already branched.

Materializing temporary variables. The next step of
the Algorithm is to identify the indices that cannot be fused
as outermost loops but are common to the producer and the
consumer. In Figure 3d we see that there are no common
variables below the nest boundary. The variables that are
below the nest boundary line and common to both the pro-
ducer and consumer define the dimensions of the temporary
variable that is shared between them. For the case of <SD-
DMM, SpMM> described in Figure 3d, since no indices are
common below the nest boundary line, we can define the
temporary as a scalar. However, for the same case of <SD-
DMM, SpMM> described in Section 4.3.1, where transpose of
ğ· is used to define the computation, we can see that index ğ‘—
is a common index below the nest boundary line. Therefore,
the algorithm defines a temporary vector bounded by the
size of the index ğ‘—, Lines 15-19 of the Algorithm 1 explain
how we perform the identification of the common indices
below the nest boundary, and line 20 defines this temporary
variable.

Rewrite the iteration graph. After we find the fusible
indices, shared indices and define the temporary variable,
we define the producer expression and consumer expression
using the temporary variable that is shared between the
producer and the consumer (lines 21, 22 of the Algorithm 1).
Then, we rewrite the iteration graph to model this behavior
with the temporary variable, the producer and the consumer
(see Figure 3d) which would eventually generate the code
shown in Figure 2d for our running example.

4.3 Scheduling

In this section we describe, (1) the invocation of scheduling
transformation and (2) the impact it has on the space of
possible schedules.

4.3.1 Scheduling Directive. SparseLNR introduces a new
scheduling directive to TACO. The user can call the loopfuse
scheduling transformation as shown in Figure 4b with other
scheduling directives. Here, 1 refers to applying the algo-
rithm once. By passing 2 or a higher number, the algorithm
can be applied recursively.

Sometimes it is necessary to combine loopfuse with other
TACO scheduling directives. Hence, it is important that our
new directive compose with the existing scheduling lan-
guage. For example, applying Algorithm 1 to the tensor ex-
pression ğ´(ğ‘–, ğ‘™) = B(ğ‘–, ğ‘—) âˆ—ğ¶ (ğ‘–, ğ‘˜) âˆ— ğ· (ğ‘˜, ğ‘—) âˆ— ğ¸ ( ğ‘—, ğ‘™) would not
yield the code in Figure 2d by default because now the access
pattern of the D matrix is different since we are using the
transpose of ğ· for this example. This difference results in a

ICS â€™22, June 28â€“30, 2022, Virtual Event, USA

Adhitha Dias, Kirshanthan Sundararajah, Charitha Saumya, and Milind Kulkarni

(b) Scheduling directives

(a) Original kernel

(c) Transformed kernel

Figure 4. The loopfuse transformation performed on ğ´ğ‘–ğ‘™ =
(cid:205)ğ‘—ğ‘˜ ğµğ‘– ğ‘—ğ¶ğ‘–ğ‘˜ğ·ğ‘˜ ğ‘— ğ¸ ğ‘—ğ‘™ .
different iteration graph as shown in Figure 4a, because now
the iteration graph needs to preserve the ordering of ğ‘– â†’ ğ‘—
for B, ğ‘– â†’ ğ‘˜ for D, ğ‘˜ â†’ ğ‘— for D, ğ‘— â†’ ğ‘™ for E, and ğ‘– â†’ ğ‘™ for A,
with a hard ordering of ğ‘– â†’ ğ‘— because B is a sparse matrix.
Applying the fusion after distribution algorithm would result
in an iteration graph as depicted in Figure 4c.

However, since D is dense, there is no hard constraint on
the ordering of indices ğ‘˜ and ğ‘—. Therefore, to arrive at the
code in Figure 2d, a loop reordering can be performed before
the loopfuse scheduling directive (See Figure 4b).

The nest boundary between the branching point in the
iteration graph constrains loop reordering between the nest
boundary. But loop reordering can be still allowed in indices
within nest boundaries.

4.3.2 Scheduling Space. In our current implementation,
if two kernels have ğ‘› common outer loops, we fuse them all.
However, if loop reordering is possible, and we choose only
certain loops to be fused, then there are 2ğ‘› possible sched-
ules to start with, given that there are ğ‘› number of fusible
loops (i.e. ğ‘› number of common iterators in the two kernels).
This is an upper bound without considering any constraints
of sparse access patterns. Reordering of inner loops can be
performed after fusion, and other scheduling directives (split,
parallelization, etc.) can be applied separately, giving more
scheduling opportunities with imperfect nesting. This sched-
uling space is obviously very large, so smart strategies for
searching that space is a promising avenue for future work.

4.4 Code Generation

We carefully redesigned intermediate representation (IR) in
TACO to support the branched iteration graph and manage
temporaries such that code generation backend does not re-
quire any changes. We rewrite the graph loop structure with

where statements defining a producer-consumer relationship.
This placement of temporaries for the producer-consumer
relationship and the change of iteration graph explained in
Section 4.2 preserves all the attributes that are necessary for
TACO code generation backend.

In TACO each index in the iteration graph is converted
to one or more loops to iterate through dense loops or co-
iterate over the levels of sparse data formats. An iteration
lattice [17] is used to co-iterate through the intersections
of the sparse dimensions which results in a single for-loop,
single while-loop or multiple while-loops.

5 Implementation
We implement the branch iteration graph transformation
described in Section 4 on top of the TACO [17] intermedi-
ate representation (IR). Furthermore, we introduce a new
scheduling directive to separate it from the algorithmic lan-
guage and to provide the scheduling language with more
opportunities to generate more (performant) schedules.

We change the iteration graph [17] and use the concrete
index notation [16] to introduce intermediate temporaries
that are shared between the producer and the consumer. We
implement a nest boundary between the fused loops and
shared index loops to constrain performing loop reordering
transformations between them. In our running example, the
user cannot interchange loops with an outer level, once the
distribution operation is performed.

This new transformation can be used in the context of
tensor multiplication. Hence, it does not generalize to ten-
sor expressions with tensor additions. We limit the number
of tensors and index variables removed from the index ex-
pression, to identify the producer and consumer graphs, per
iteration to one. We believe that the algorithm could be gen-
eralized to support fusion of indices shared between multiple
tensors which would be able to support high order tensors
and complex tensor contractions.

6 Evaluation
We compare SparseLNR to two other techniques:
TACO Original. Given a large combined index expression
containing multiple smaller index expressions, the code gen-
erated by TACO has a perfectly nested loop structure with at
least one loop per each index variable in the index expression.
We refer to this version as TACO Original.
TACO Separate. In some cases, the asymptotic complexity
of TACO Original can be reduced by manually separating a
larger index expression into multiple smaller index expres-
sions by using temporary tensors to store the intermediate
results. We refer to this version as TACO Separate. When
there are multiple ways to break down the computation into
smaller kernels, we evaluate all those combinations and re-
port the best execution time.

SparseLNR

ICS â€™22, June 28â€“30, 2022, Virtual Event, USA

(a) Single-threaded <SDDMM, SpMM>

(b) Multi-threaded <SDDMM, SPMM>

(c) Single-threaded <SpMM, GeMM>

(d) Multi-threaded <SpMM, GeMM>

(e) Single-threaded <SpMMH, GeMM>

(f) Multi-threaded <SpMMH, GeMM>

(g) Single-threaded <SDDMM, SpMM, GeMM>

(h) Multi-threaded <SDDMM, SpMM, GeMM>

Figure 5. Performance Comparison with TACO for benchmarks with 2-D matrices.

6.1 Experimental Setup

All experiments run on a single socket 64-Core AMD Ryzen
Threadripper 3990X at 2.2 GHz, with 32KB L1 data cache,
512KB shared L2 cache, and 16MB shared L3 cache. We com-
pile the code using GCC 7.5.0 with -O3 -ffast-math. We
use -fopenmp for parallel versions with OpenMP version 4.5.
All parallel versions use 64 threads which is the number of
physical cores available in the machine.
Datasets. We use sparse tensors from four sources: SuiteS-
parse [12]; Network Repository [28]; Formidable Repository
of Open Sparse Tensors and Tools (FROSTT) [31]; and the
1998 DARPA Intrusion Detection Evaluation [15]. Dense ten-
sors in kernels are randomly generated. Table 1 gives the
details of the sparse tensors.

6.2 Benchmarks
<SDDMM, SpMM>. SDDMM computation followed by the
SpMM operation, ğ´ğ‘–ğ‘™ = (cid:205) Bğ‘– ğ‘— Â· ğ¶ğ‘–ğ‘˜ Â· ğ· ğ‘—ğ‘˜ Â· ğ¸ ğ‘—ğ‘™ . This operation
is used in graph neural networks [27]. We set the inner
dimensions ğ‘˜ and ğ‘™ to <64, 64>. Fusion of SDDMM with
SpMM results in a scalar intermediate to share the results
between the fused loops as shown in Figure 2d.
<SpMMH, GEMM>. SpMMH here is pre-multiplying the
Hadamard product of two dense matrices by a sparse matrix.
The combined kernel we evaluate is ğ´ğ‘–ğ‘™ = (cid:205) Bğ‘–ğ‘˜ Â·ğ¶ğ‘˜ ğ‘— Â·ğ·ğ‘˜ ğ‘— Â·ğ¸ ğ‘—ğ‘™
with inner dimensions ğ‘— and ğ‘™ set to <128, 128>.
<SpMM, GEMM>. SpMM kernel followed by another GEMM
kernel, ğ´ğ‘–ğ‘™ = (cid:205) Bğ‘– ğ‘— Â· ğ¶ ğ‘—ğ‘˜ Â· ğ·ğ‘˜ğ‘™ . The inner dimensions ğ‘˜, ğ‘š

corabcsstk17pdb1HYSrma10cantconsphcop20k_Ashipsec1scircuitmac_econamazonwebbase-1Mcircuit5M0.00.51.01.52.02.5Normalized Execution Time16.324.933.432.432.732.517.732.115.118.510.716.526.7SparseLNRTACO-SeparateTACO-Originalcorabcsstk17pdb1HYSrma10cantconsphcop20k_Ashipsec1scircuitmac_econamazonwebbase-1Mcircuit5M0.00.51.01.52.02.5Normalized Execution Time16.05.112.77.68.29.53.68.54.03.93.35.79.3SparseLNRTACO-SeparateTACO-Originalcorabcsstk17pdb1HYSrma10cantconsphcop20k_Ashipsec1scircuitmac_econamazonwebbase-1Mcircuit5M0.00.51.01.52.02.5Normalized Execution Time3.32.63.27.229.079.954.857.959.214.950.89.07.99.26.931.2SparseLNRTACO-SeparateTACO-Originalcorabcsstk17pdb1HYSrma10cantconsphcop20k_Ashipsec1scircuitmac_econamazonwebbase-1Mcircuit5M0.00.51.01.52.02.5Normalized Execution Time2.93.03.22.885.5139.999.1112.1118.149.0102.518.320.64.611.432.4SparseLNRTACO-SeparateTACO-Originalcorabcsstk17pdb1HYSrma10cantconsphcop20k_Ashipsec1scircuitmac_econamazonwebbase-1Mcircuit5M0.00.51.01.52.02.5Normalized Execution Time21.950.627.133.935.912.429.93.84.37.0SparseLNRTACO-SeparateTACO-Originalcorabcsstk17pdb1HYSrma10cantconsphcop20k_Ashipsec1scircuitmac_econamazonwebbase-1Mcircuit5M0.00.51.01.52.02.5Normalized Execution Time11.836.518.420.521.25.217.83.13.110.2SparseLNRTACO-SeparateTACO-Originalcorabcsstk17pdb1HYSrma10cantconsphcop20k_Ashipsec1scircuitmac_econamazonwebbase-1Mcircuit5M0.00.51.01.52.02.5Normalized Execution Time93163719971555173017718001636351405154236632SparseLNRTACO-SeparateTACO-Originalcorabcsstk17pdb1HYSrma10cantconsphcop20k_Ashipsec1scircuitmac_econamazonwebbase-1Mcircuit5M0.00.51.01.52.02.5Normalized Execution Time19426126478883390022979712512065114475SparseLNRTACO-SeparateTACO-OriginalICS â€™22, June 28â€“30, 2022, Virtual Event, USA

Adhitha Dias, Kirshanthan Sundararajah, Charitha Saumya, and Milind Kulkarni

(a) Single-threaded <MTTKRP, GeMM>

(b) Multi-threaded <MTTKRP, GEMM>

(c) Single-threaded <SpTTM, SpTTM>

(d) Multi-threaded <SpTTM, SpTTM>

Figure 6. Performance Comparison with TACO for benchmarks with 3-D tensors.

Table 1. Test tensors used in the evaluation from various
matrix and tensor collections mentioned in the Section 6.1

Tensor
cora
bcsstk17
pdb1HYS
rma10
cant
consph
cop20k_A
shipsec1
scircuit
mac_econ_fwd500
amazon
webbase-1M
circuit5M
flickr-3d
nell-2
nell-1
vast-2015-mc1-3d
darpa1998

Dimensions Non-zeros
5.4ğ¾
2.7ğ¾ Ã— 2.7ğ¾
429ğ¾
11ğ¾ Ã— 11ğ¾
4.34ğ‘€
36ğ¾ Ã— 36ğ¾
2.37ğ‘€
47ğ¾ Ã— 47ğ¾
4.01ğ‘€
62ğ¾ Ã— 62ğ¾
6.01ğ‘€
83ğ¾ Ã— 83ğ¾
2.62ğ‘€
12ğ¾ Ã— 12ğ¾
7.81ğ‘€
140ğ¾ Ã— 140ğ¾
959ğ¾
171ğ¾ Ã— 171ğ¾
1.27ğ‘€
207ğ¾ Ã— 207ğ¾
1.85ğ‘€
334ğ¾ Ã— 334ğ¾
1.00ğ‘€ Ã— 1.00ğ‘€
3.11ğ‘€
59.52ğ‘€
5.56ğ‘€ Ã— 5.56ğ‘€
112.89ğ‘€
320ğ¾ Ã— 2.82ğ‘€ Ã— 1.60ğ‘€
76.88ğ‘€
12ğ¾ Ã— 9ğ¾ Ã— 288ğ¾
143.6ğ‘€
2.9ğ‘€ Ã— 2.14ğ‘€ Ã— 25.5ğ‘€
26.02ğ‘€
165ğ¾ Ã— 11ğ¾ Ã— 2
28.42ğ‘€
22ğ¾ Ã— 22ğ¾ Ã— 23.7ğ‘€

are set to <128, 64>. This calculation is commonly used for
updating the hidden state in GNNs [37].
<SDDMM, SpMM, GEMM>. We combine two of the prior
kernels to show the recursive applicability of the algorithm.
ğ´ğ‘–ğ‘š = (cid:205) Bğ‘– ğ‘— Â·ğ¶ğ‘–ğ‘˜ Â·ğ· ğ‘—ğ‘˜ Â·ğ¹ ğ‘—ğ‘™ Â·ğ‘Šğ‘™ğ‘š We could relate this execution
to performing SDDMM operation to get the attention values
along the edges of a graph, multiplying the feature matrix of
the graph with a weight matrix to get the new feature set of
the graph and then doing a neighbor sum of the graph. The
inner dimensions ğ‘˜, ğ‘™ and ğ‘š are set to <64,64,64>.
<MTTKRP, GEMM>. Khatri-Rao product (MTTKRP) fol-
lowed by GEMM operation, ğ´ğ‘–ğ‘š = (cid:205) Bğ‘–ğ‘˜ğ‘™ Â· ğ¶ğ‘™ ğ‘— Â· ğ·ğ‘˜ ğ‘— Â· ğ¸ ğ‘—ğ‘š.
MTTKRP kernel is used in various sparse computations like
signal processing and computer vision [7]. We set the inner
dimensions ğ‘— and ğ‘š to <32, 64>.

<SpTTM, SpTTM>. Sparse Tensor Times Matrix (SpTTM)
operation followed by another SpTTM operation, Ağ‘– ğ‘—ğ‘š =
(cid:205) Bğ‘– ğ‘—ğ‘˜ Â· ğ¶ğ‘˜ğ‘™ Â· ğ·ğ‘™ğ‘š. SpTTM is a computational kernel used
in data analytics and data mining applications such as the
popular Tucker decomposition [25]. The inner dimensions ğ‘™
and ğ‘š are set to <32,64>.
Sparse Formats. SpMM, SDDMM kernels use standard com-
pressed sparse row (CSR) format for their sparse matrices
whereas SpTTM, MTTKRP kernels use compressed sparse
fiber (CSF) format.

For the SDDMM, SpMM, MTTKRP kernels in TACO sepa-
rate we use the versions provided in Senanayake et al. [30].
For the rest of kernels we evaluate multiple schedules and
select the best performing one. TACO does not generate
multi-threaded code when the output tensor is sparse. Prior
work has evaluated against single-threaded code in such
situations [16, 34]. Following the strategy of Senanayeke et
al. [30], we modified the TACO generated code manually to
add multithreading

In general, the speedups we see compared to the TACO
original comes from the reduction in asymptotic complexity
while the speedups we see compared to the TACO separate
comes from the reduction in cache reuse distances by remov-
ing large tensors used to store intermediate results.

We see speedups for our approach of 0.90â€“1.23x compared
TACO Separate and 3.31â€“16.05 compared to TACO Original
in <SDDMM, SpMM> kernelâ€™s multi-threaded execution. For
single-threaded execution, we get 0.91â€“1.50x compared to
TACO separate and 10.75â€“33.39x compared to TACO origi-
nal. In multithreaded execution the fused kernel performs
better on circuit5M, shipsec1, consph, pdf1HYS, and cant,

flickr-3dnell-2nell-1vast-2015-mc1-3ddarpa19980.00.51.01.52.02.5Normalized Execution Time27.089.141.562.745.3SparseLNRTACO-SeparateTACO-Originalflickr-3dnell-2nell-1vast-2015-mc1-3ddarpa19980.00.51.01.52.02.5Normalized Execution Time9.672.834.242.955.9SparseLNRTACO-SeparateTACO-Originalflickr-3dnell-2nell-1vast-2015-mc1-3ddarpa19980.00.51.01.52.02.5Normalized Execution Time4.725.66.366.25.130.3SparseLNRTACO-Separate1TACO-Separate2TACO-Originalflickr-3dnell-2nell-1vast-2015-mc1-3ddarpa19980.00.51.01.52.02.5Normalized Execution Time2.73.910.329.06.3SparseLNRTACO-Separate1TACO-Separate2TACO-OriginalSparseLNR

ICS â€™22, June 28â€“30, 2022, Virtual Event, USA

of ğ‘‚ (ğ‘›ğ‘›ğ‘§ (ğµğ¼ ğ½ ğ¾ )ğ¿ğ‘€). But separate1 (T ğ‘– ğ‘—ğ‘™ = (cid:205) Bğ‘– ğ‘—ğ‘˜ Â· ğ¶ğ‘˜ğ‘™ fol-
lowed by Ağ‘– ğ‘—ğ‘™ = (cid:205) T ğ‘– ğ‘—ğ‘™ Â·ğ¶ğ‘˜ğ‘™ ) and separate2 (ğ‘‡ğ‘˜ğ‘š = (cid:205) ğ¶ğ‘˜ğ‘™ Â·ğ·ğ‘™ğ‘š
followed by Ağ‘– ğ‘—ğ‘š = (cid:205) Bğ‘– ğ‘—ğ‘˜ Â· ğ‘‡ğ‘˜ğ‘š) versions have complexities
of ğ‘‚ (ğ‘›ğ‘›ğ‘§ (ğµğ¼ ğ½ ğ¾ )ğ¿ +ğ‘›ğ‘›ğ‘§ (ğµğ¼ ğ½ ğ¿)ğ‘€) and ğ‘‚ (ğ¾ğ¿ğ‘€ +ğ‘›ğ‘›ğ‘§ (ğµğ¼ ğ½ ğ¾ )ğ‘€),
respectively.

When ğ‘˜ is small (for instance, dimension ğ‘˜ of the dataset
vast2015-mc1-3d in Table 1 is bounded by 2), the asymp-
totic complexity of our approach is comparable to TACOâ€™s
baseline approach, and the size of the working set is small.
Therefore, there are no cache misses for the TACO sepa-
rate approach; the re-associated schedule hence has the best
performance. In the other datasets, where ğ‘˜ is large (for
instance, the dimension ğ‘˜ of the dataset darpa1998 in Ta-
ble 1 is bounded by 28.42M), these effects vanish, and our
approach is considerably faster than any competing version.
We note that SparseLNRâ€™s representation could support re-
association scheduling directives, that would allow it to use
the better schedules of TACO separate, but we leave that for
future work.

(a) Circuit5M Scaling

(b) Webbase Scaling
Figure 8. Speedup change with respect to the number of
threads for <SDDMM, SpMM> benchmark.

The scaling results for webbase and circuit5M datasets,
on <SDDMM, SpMM> benchmark are shown in Figure 8;
SparseLNR delivers comparable scaling to the best alterna-
tive approach. We observed similar scaling for other bench-
marks and datasets.

6.3 Case Study: Performance with different inner

dimensions

We chose the inner dimensions of the benchmarks explained
in Section 6.2 arbitrarily. In this case study we consider
change of performance wrt. varying inner dimension sizes
(ğ‘˜ and ğ‘™) in the benchmark <SpMM, GEMM> (ğ´ğ‘–ğ‘™ = (cid:205) Bğ‘– ğ‘— Â·
ğ¶ ğ‘—ğ‘˜ Â· ğ·ğ‘˜ğ‘™ ) in Section 6.2. Here, the dimensions ğ‘– and ğ‘— are

(a)

(b)

(c)

(d)

Figure 7. Basic loop structure of different schedules for
<SpTTM, SpTTM> kernel.

the matrices with most non-zeros, from the tested datasets 1
compared against the separate execution.

We observe speedups of 1.60â€“1.99x against TACO separate
and 1.29â€“50.55x against TACO original in single-threaded
execution for <SpMMH, GEMM> kernel. For the same ker-
nel in multi-threaded execution, we observed speedups of
1.28â€“2.40x against TACO separate and 1.66â€“36.50x against
TACO original. For the <SpMM, GEMM> kernel in single-
threaded execution, speedups of 1.23â€“3.27x, and 6.91â€“79.86x
are observed for TACO original and TACO separate, respec-
tively. For the same kernel in multi-threaded execution, we
observed speedups of 0.86â€“3.16x, and 2.44â€“139x for TACO
original and TACO separate, respectively.

We see substantial speedups in <SDDMM, SpMM, GEMM>
due to the kernel presenting two opportunities for fusion:
<SDDMM, SpMM> and <SpMM, GEMM>. We see speedups
ranging from 1.20â€“2.26x for single-threaded execution against
TACO separate, 93â€“1997x over single-threaded TACO orig-
inal, 1.06â€“2.09x for multithreaded TACO separate and 19â€“
1263x for multithreaded TACO original.

We see that our approach under-performed in datasets
nell-2 and darpa1998 against the TACO separate for the <MT-
TKRP, GEMM> benchmark. In these datasets, the first di-
mensions of the tensors are bounded by 12092, and 22476.
Therefore, the intermediate matrix in the TACO separate
execution has sizes that fit within the last level cache. Fur-
thermore, executing kernels separately sometimes offer more
opportunities to optimize smaller kernels individually. Due
to these reasons, there may be datasets that the separate
kernel execution performs better than the fused version. But
we see considerable speedups versus TACO Separate when
the intermediate tensors are large.

For <SpTTM, SpTTM>, there are two different schedules
due to different associativity choices. We see varying re-
sults based on which choice is made, so we report both of
them as separate1 and separate2 (see Figures 6c and 6d). Fig-
ure 7 shows the basic loop structure for different versions of
<SpTTM, SpTTM>. The fused version has asymptotic com-
plexity of ğ‘‚ (ğ‘›ğ‘›ğ‘§ (ğµğ¼ ğ½ ğ¾ )ğ¿ + ğ‘›ğ‘›ğ‘§ (ğµğ¼ ğ½ )ğ¿ğ‘€). The TACO original
version Ağ‘– ğ‘—ğ‘š = (cid:205) Bğ‘– ğ‘—ğ‘˜ Â· ğ¶ğ‘˜ğ‘™ Â· ğ·ğ‘™ğ‘š has asymptotic complexity

1248163264Number of Threads020406080100120Speedup over TACO-Originalsingle-thread executionSparseLNRTACO-SeparateTACO-Original1248163264Number of Threads0102030405060Speedup over TACO-Originalsingle-thread executionSparseLNRTACO-SeparateTACO-OriginalICS â€™22, June 28â€“30, 2022, Virtual Event, USA

Adhitha Dias, Kirshanthan Sundararajah, Charitha Saumya, and Milind Kulkarni

computation. Ahrens et al. [2] proposed splitting large ten-
sor expressions into smaller kernels to minimize the time-
complexity. The Sparse Polyhedral Framework [21, 32, 33]
employs an inspector-executor strategy to transform the
data layout and schedule of sparse computations to achieve
locality and parallelism. Kurt et al. [20] improved SpMM and
SDDMM kernels by optimizing their tile sizes using a spar-
sity signature. However, these methods do not consider loop
nest restructuring transformations to improve data locality
across kernels.

Athena [23], Sparta [24], and HiParTi [22] are techniques
which provide highly optimized kernels for sparse tensor
operations and contraction sequences that shows significant
performance improvements. Kernel fusion has been used
in FusedMM [27] to accelerate SDDMM and SpMM used in
graph neural network applications. Their transformation is
structurally analogous to SparseLNR, but is specific to graph
embeddings, and further performs kernel-specific optimiza-
tions. None of these prior techniques handle arbitrary sparse
tensor expressions supporting a variety of input formats.

7.2 Dense Tensor Algebra

Optimizations for computations over dense tensors have
been well-studied for decades. Numerous loop optimizations
for dense tensor contractions [3, 4, 9â€“11, 19, 29] for CPUs
and tensor contractions for GPUs [1, 26] have been proposed
that exhibits superior performance. However, these transfor-
mations are not directly applicable to sparse tensor algebra
since there data access restrictions for sparse tensors and
the non-affine nature of loop nests.

8 Conclusion
We presented SparseLNR, a loop restructuring framework
for sparse tensor algebra programs. SparseLNR improves
the performance of sparse computations by reducing time
complexity and enhancing data locality. SparseLNR enables
kernel distribution and loop fusion and achieves significant
performance improvements for real-world benchmarks. The
new scheduling transformations introduced by SparseLNR
expands the scheduling space of sparse tensor applications
and facilitates fine-grained tuning.

Acknowledgments
We appreciate the feedback from the anonymous reviewers
for their suggestions and comments that helped to improve
this paper. We would also like to thank Fredrik Kjolstad for
the valuable discussions we had regarding the SparseLNR
transformation. This work was supported in part by the
National Science Foundation awards CCF-1908504 and CCF-
1919197. Any opinions, findings, and conclusions or recom-
mendations expressed in this paper are those of the authors
and do not necessarily reflect the views of the National Sci-
ence Foundation.

(a) TACO-Sep. vs. SparseLNR (b) TACO-Orig. vs. SparseLNR
Figure 9. Speedup variation wrt. inner dimension sizes (ğ‘˜
and ğ‘™) on the benchmark <SpMM,GEMM>. The Figures (a)
and (b) correspond to multithreaded execution of SparseLNR
against TACO-Separate and TACO-Original, respectively.

determined by size of the graphs read into the sparse matrix
Bğ‘– ğ‘— and cannot be arbitrarily changed. However, the dimen-
sions ğ‘˜ and ğ‘™ can change in size since the matrices ğ¶ ğ‘—ğ‘˜ and
ğ·ğ‘˜ğ‘™ are dense. Usually, in GNN literature, these dimensions
correspond to feature sizes in hidden layers.

Performing the loop fusion after distribution in the bench-
mark <SpMM, GEMM> results in a temporary vector of
the length of the size of dimension ğ‘˜, and ğ‘˜ and ğ‘™ dimen-
sions completely define the size of the matrix ğ·ğ‘˜ğ‘™ . When the
size of the dimension ğ‘™ is small, ğ·ğ‘˜ğ‘™ can completely fit in
higher level caches for the ğ‘˜ values considered. Therefore,
for smaller ğ‘™ values, the speedup increases with the size of
ğ‘˜ (See Figure 9). Because with increasing ğ‘˜, the temporary
tensor gets larger and it decreases the performance of TACO-
Separate. We see this behavior for ğ‘™ dimension sizes of 16-128
in Figure 9a. But with increasing ğ‘˜, when the size of the ğ‘™
dimension gets larger, the sizes of ğ·ğ‘˜ğ‘™ and temporary vector
get larger. As a result, they keep getting evicted from the
higher level caches in SparseLNR. Therefore, as shown in
Figure 9a, the peak performance in columns 256 and 512 of
the dimension ğ‘™ occurs not when ğ‘˜ is 512, but when size of ğ‘˜
takes values in the range 64â€“256. Increasing sizes of ğ‘˜ and ğ‘™
results in higher time complexity for TACO-Original. Hence,
speedup of SparseLNR increases with the sizes of ğ‘˜ and ğ‘™ in
the Figure 9b.

7 Related Work
Code generation for tensor algebra has been extensively
researched. This area of research can be primarily divided
into two subareas â€” sparse and dense. First, we discuss the
related work on code generation and optimization techniques
for sparse tensor algebra and then move to the ones for dense.

7.1 Sparse Tensor Algebra

Automated sparse code generation [5, 6, 17, 34] is a heavily-
researched topic. Even though these methods are highly
effective, they lack fine-grained optimizations like ours that
applies across kernels to reduce the time-complexity of the

163264128256512L Dimension512256128643216K Dimension1.01.52.02.53.03.5Speedup163264128256512L Dimension512256128643216K Dimension2.55.07.510.012.515.017.520.0SpeedupSparseLNR

ICS â€™22, June 28â€“30, 2022, Virtual Event, USA

References
[1] A. Abdelfattah, M. Baboulin, V. Dobrev, J. Dongarra, C. Earl, J. Fal-
cou, A. Haidar, I. Karlin, Tz. Kolev, I. Masliah, and S. Tomov. 2016.
High-performance Tensor Contractions for GPUs. Procedia Computer
Science 80 (2016), 108â€“118. https://doi.org/10.1016/j.procs.2016.05.302
International Conference on Computational Science 2016, ICCS 2016,
6-8 June 2016, San Diego, California, USA.

[2] Peter Ahrens, Fredrik Kjolstad, and Saman Amarasinghe. 2021. An
Asymptotic Cost Model for Autoscheduling Sparse Tensor Programs.
arXiv:2111.14947

[3] A. Allam, J. Ramanujam, G. Baumgartner, and P. Sadayappan. 2006.
Memory minimization for tensor contractions using integer linear pro-
gramming. In Proceedings 20th IEEE International Parallel Distributed
Processing Symposium. 8 pp.â€“. https://doi.org/10.1109/IPDPS.2006.
1639717

[4] Alina Bibireata, Sandhya Krishnan, Gerald Baumgartner, Daniel Co-
ciorva, Chi-chung Lam, P. Sadayappan, J. Ramanujam, David E. Bern-
holdt, and Venketesh Choppella. 2004. Memory-Constrained Data
Locality Optimization. In in 16th International Workshop on Languages
and Compilers for Parallel Computing (LCPCâ€™03. SpringerVerlag, 93â€“
108.

[5] Aart J. C. Bik and Harry A. G. Wijshoff. 1993. Compilation Techniques
for Sparse Matrix Computations. In Proceedings of the 7th International
Conference on Supercomputing (Tokyo, Japan) (ICS â€™93). Association
for Computing Machinery, New York, NY, USA, 416â€“424. https://doi.
org/10.1145/165939.166023

[6] Aart J. C. Bik and Harry A. G. Wijshoff. 1993. On Automatic Data
Structure Selection and Code Generation for Sparse Computations.
In Proceedings of the 6th International Workshop on Languages and
Compilers for Parallel Computing. Springer-Verlag, Berlin, Heidelberg,
57â€“75.

[7] Jee Choi, Xing Liu, Shaden Smith, and Tyler Simon. 2018. Blocking
Optimization Techniques for Sparse Tensor Computation. In 2018 IEEE
International Parallel and Distributed Processing Symposium (IPDPS).
568â€“577. https://doi.org/10.1109/IPDPS.2018.00066

[8] Stephen Chou, Fredrik Kjolstad, and Saman Amarasinghe. 2018. For-
mat Abstraction for Sparse Tensor Algebra Compilers. Proc. ACM
Program. Lang. 2, OOPSLA, Article 123 (oct 2018), 30 pages. https:
//doi.org/10.1145/3276493

[9] Daniel Cociorva, Gerald Baumgartner, Chi-Chung Lam, P. Sadayap-
pan, J. Ramanujam, Marcel Nooijen, David E. Bernholdt, and Robert
Harrison. 2002. Space-Time Trade-off Optimization for a Class of
Electronic Structure Calculations. In Proceedings of the ACM SIGPLAN
2002 Conference on Programming Language Design and Implementation
(Berlin, Germany) (PLDI â€™02). Association for Computing Machinery,
New York, NY, USA, 177â€“186. https://doi.org/10.1145/512529.512551
[10] D. Cociorva, Xiaoyang Gao, S. Krishnan, G. Baumgartner, Chi-Chung
Lam, P. Sadayappan, and J. Ramanujam. 2003. Global communication
optimization for tensor contraction expressions under memory con-
straints. In Proceedings International Parallel and Distributed Processing
Symposium. 8 pp.â€“. https://doi.org/10.1109/IPDPS.2003.1213121
[11] D. Cociorva, J. W. Wilkins, C. Lam, G. Baumgartner, J. Ramanujam,
and P. Sadayappan. 2001. Loop Optimization for a Class of Memory-
Constrained Computations. In Proceedings of the 15th International
Conference on Supercomputing (Sorrento, Italy) (ICS â€™01). Association
for Computing Machinery, New York, NY, USA, 103â€“113. https://doi.
org/10.1145/377792.377814

[12] Timothy A. Davis and Yifan Hu. 2011. The University of Florida Sparse
Matrix Collection. ACM Trans. Math. Softw. 38, 1, Article 1 (dec 2011),
25 pages. https://doi.org/10.1145/2049662.2049663

[13] William L. Hamilton, Rex Ying, and Jure Leskovec. 2018. Inductive

Representation Learning on Large Graphs. arXiv:1706.02216 [cs.SI]

[14] Yuwei Hu, Zihao Ye, Minjie Wang, Jiali Yu, Da Zheng, Mu Li, Zheng
Zhang, Zhiru Zhang, and Yida Wang. 2020. FeatGraph: A Flexible and

Efficient Backend for Graph Neural Network Systems. In Proceedings
of the International Conference for High Performance Computing, Net-
working, Storage and Analysis (Atlanta, Georgia) (SC â€™20). IEEE Press,
Article 71, 13 pages.

[15] Inah Jeon, Evangelos E. Papalexakis, U Kang, and Christos Faloutsos.
2015. HaTen2: Billion-scale tensor decompositions. In 2015 IEEE 31st
International Conference on Data Engineering. 1047â€“1058. https://doi.
org/10.1109/ICDE.2015.7113355

[16] Fredrik Kjolstad, Peter Ahrens, Shoaib Kamil, and Saman Amarasinghe.
2019. Tensor Algebra Compilation with Workspaces. (2019), 180â€“192.
http://dl.acm.org/citation.cfm?id=3314872.3314894

[17] Fredrik Kjolstad, Shoaib Kamil, Stephen Chou, David Lugato, and
Saman Amarasinghe. 2017. The Tensor Algebra Compiler. Proc. ACM
Program. Lang. 1, OOPSLA, Article 77 (Oct. 2017), 29 pages. https:
//doi.org/10.1145/3133901

[18] Vladimir Kotlyar, Keshav Pingali, and Paul Stodghill. 1997. A Rela-
tional Approach to the Compilation of Sparse Matrix Programs. In
Proceedings of the Third International Euro-Par Conference on Parallel
Processing (Euro-Par â€™97). Springer-Verlag, Berlin, Heidelberg, 318â€“327.
[19] Sandhya Krishnan, Sriram Krishnamoorthy, Gerald Baumgartner,
Daniel Cociorva, Chi-Chung Lam, P. Sadayappan, J. Ramanujam,
David E. Bernholdt, and Venkatesh Choppella. 2003. Data Locality
Optimization for Synthesis of Efficient Out-of-Core Algorithms. In
HiPC.

[20] SÃ¼reyya Emre Kurt, Aravind Sukumaran-Rajam, Fabrice Rastello, and
P. Sadayyapan. 2020. Efficient Tiled Sparse Matrix Multiplication
through Matrix Signatures. In SC20: International Conference for High
Performance Computing, Networking, Storage and Analysis. 1â€“14. https:
//doi.org/10.1109/SC41405.2020.00091

[21] Alan LaMielle and Michelle Mills Strout. 2010. Enabling Code Genera-

tion within the Sparse Polyhedral Framework.

[22] Jiajia Li. [n.d.]. A Hierarchical Parallel Tensor Infrastructure (HiParTI).

https://github.com/pnnl/HiParTI [Accessed 02-Feb-2022].

[23] Jiawen Liu, Dong Li, Roberto Gioiosa, and Jiajia Li. 2021. Athena:
High-Performance Sparse Tensor Contraction Sequence on Hetero-
geneous Memory. In Proceedings of the ACM International Confer-
ence on Supercomputing (Virtual Event, USA) (ICS â€™21). Association
for Computing Machinery, New York, NY, USA, 190â€“202.
https:
//doi.org/10.1145/3447818.3460355

[24] Jiawen Liu, Jie Ren, Roberto Gioiosa, Dong Li, and Jiajia Li. 2021.
Sparta: High-Performance, Element-Wise Sparse Tensor Contraction on
Heterogeneous Memory. Association for Computing Machinery, New
York, NY, USA, 318â€“333. https://doi.org/10.1145/3437801.3441581
[25] Yuchen Ma, Jiajia Li, Xiaolong Wu, Chenggang Yan, Jimeng Sun, and
Richard Vuduc. 2019. Optimizing sparse tensor times matrix on GPUs.
J. Parallel and Distrib. Comput. 129 (2019), 99â€“109. https://doi.org/10.
1016/j.jpdc.2018.07.018

[26] Thomas Nelson, Axel Rivera, Prasanna Balaprakash, Mary Hall, Paul D.
Hovland, Elizabeth Jessup, and Boyana Norris. 2015. Generating Effi-
cient Tensor Contractions for GPUs. In 2015 44th International Con-
ference on Parallel Processing. 969â€“978. https://doi.org/10.1109/ICPP.
2015.106

[27] Md. Khaledur Rahman, Majedul Haque Sujon, and Ariful Azad. 2020.
FusedMM: A Unified SDDMM-SpMM Kernel for Graph Embedding and
Graph Neural Networks. arXiv:2011.06391

[28] Ryan A. Rossi and Nesreen K. Ahmed. 2016. An Interactive Data
Repository with Visual Analytics. SIGKDD Explor. 17, 2 (2016), 37â€“41.
http://networkrepository.com

[29] S.K. Sahoo, S. Krishnamoorthy, R. Panuganti, and P. Sadayappan. 2005.
Integrated Loop Optimizations for Data Locality Enhancement of
Tensor Contraction Expressions. In SC â€™05: Proceedings of the 2005
ACM/IEEE Conference on Supercomputing. 13â€“13. https://doi.org/10.
1109/SC.2005.35

ICS â€™22, June 28â€“30, 2022, Virtual Event, USA

Adhitha Dias, Kirshanthan Sundararajah, Charitha Saumya, and Milind Kulkarni

[30] Ryan Senanayake, Changwan Hong, Ziheng Wang, Amalee Wilson,
Stephen Chou, Shoaib Kamil, Saman Amarasinghe, and Fredrik Kjol-
stad. 2020. A Sparse Iteration Space Transformation Framework for
Sparse Tensor Algebra. Proc. ACM Program. Lang. 4, OOPSLA, Article
158 (Nov. 2020), 30 pages. https://doi.org/10.1145/3428226

[31] Shaden Smith, Jee W. Choi, Jiajia Li, Richard Vuduc, Jongsoo Park, Xing
Liu, and George Karypis. 2017. FROSTT: The Formidable Repository of
Open Sparse Tensors and Tools. New York, NY, USA. https://doi.org/
10.1145/2049662.2049663

[32] Michelle Mills Strout, Mary Hall, and Catherine Olschanowsky. 2018.
The Sparse Polyhedral Framework: Composing Compiler-Generated
Inspector-Executor Code. Proc. IEEE 106, 11 (2018), 1921â€“1934. https:
//doi.org/10.1109/JPROC.2018.2857721

[33] Michelle Mills Strout, Alan LaMielle, Larry Carter, Jeanne Ferrante,
Barbara Kreaseck, and Catherine Olschanowsky. 2016. An Approach
for Code Generation in the Sparse Polyhedral Framework. Parallel
Comput. 53, C (apr 2016), 32â€“57. https://doi.org/10.1016/j.parco.2016.
02.004

[34] Ruiqin Tian, Luanzheng Guo, Jiajia Li, Bin Ren, and Gokcen Kestor.
2021. A High Performance Sparse Tensor Algebra Compiler in MLIR.
In 2021 IEEE/ACM 7th Workshop on the LLVM Compiler Infrastructure
in HPC (LLVM-HPC). 27â€“38. https://doi.org/10.1109/LLVMHPC54804.
2021.00009

[35] Anand Venkat, Mary Hall, and Michelle Strout. 2015. Loop and Data
Transformations for Sparse Matrix Code. In Proceedings of the 36th
ACM SIGPLAN Conference on Programming Language Design and Imple-
mentation (Portland, OR, USA) (PLDI â€™15). Association for Computing
Machinery, New York, NY, USA, 521â€“532. https://doi.org/10.1145/
2737924.2738003

[36] Mengjia Xu. 2020. Understanding graph embedding methods and
their applications. CoRR abs/2012.08019 (2020). arXiv:2012.08019
https://arxiv.org/abs/2012.08019

[37] Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang,
Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. 2021.
Graph Neural Networks: A Review of Methods and Applications.
arXiv:1812.08434 [cs.LG]

