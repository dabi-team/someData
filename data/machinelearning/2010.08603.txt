A Sequential Framework Towards an Exact SDP
Veriﬁcation of Neural Networks

1st Ziye Ma
Electrical Engineering and Computer Sciences
UC Berkeley
Berkeley, CA, USA
ziyema@berkeley.edu

2nd Somayeh Sojoudi
Electrical Engineering and Computer Sciences
Mechanical Engineering
UC Berkeley
Berkeley, CA, USA
sojoudi@berkeley.edu

1
2
0
2

p
e
S
7
2

]

G
L
.
s
c
[

2
v
3
0
6
8
0
.
0
1
0
2
:
v
i
X
r
a

Abstract—Although neural networks have been applied to
several systems in recent years, they still cannot be used in
safety-critical systems due to the lack of efﬁcient techniques to
certify their robustness. A number of techniques based on convex
optimization have been proposed in the literature to study the
robustness of neural networks, and the semideﬁnite programming
(SDP) approach has emerged as a leading contender for the
robust certiﬁcation of neural networks. The major challenge to
the SDP approach is that it is prone to a large relaxation gap.
In this work, we address this issue by developing a sequential
framework to shrink this gap to zero by adding non-convex cuts
to the optimization problem via disjunctive programming. We
analyze the performance of this sequential SDP method both
theoretically and empirically, and show that it bridges the gap
as the number of cuts increases.

Index Terms—neural networks, robustness, safety, convex op-
timization, semideﬁnite programming, disjunctive programming

I. INTRODUCTION

the problem, several

The nonlinearity of activation functions in neural net-
works is the key enabler of making neural networks act
as universal function approximators, offering great expres-
sive power [Sonoda and Murata, 2017]. However, this also
poses major challenges for the veriﬁcation against adversar-
ial attacks, due to the non-convexity that activation func-
tions induce. To circumvent
tech-
niques based on convex relaxations of ReLU constraints
have been proposed. In particular, Linear Programming (LP)
relaxation [Wong and Kolter, 2017], Semideﬁnite Program-
ming (SDP) relaxation [Raghunathan et al., 2018], and mixed-
linear programming relaxation [Tjeng et al., 2017]
integer
[Anderson et al., 2020],
[Anderson et al., 2021] have been
proposed. LP-based techniques relax each ReLU function
individually, thus introducing a relatively large relaxation gap.
Although some recent works, such as [Singh et al., 2019],
have developed k-ReLU relaxations to consider multiple ReLU
relaxations jointly, the relaxation gap is still large. On the
contrary, SDP-based relaxations naturally couple ReLU relax-
ations together without any additional effort via a semideﬁnite
constraint. Therefore, as the number of hidden layers of the
network under veriﬁcation grows, the relative reduction in
the relaxation gap also grows when compared to LP-based

This work was supported by grants from AFOSR, ONR and NSF.

methods. However, even with the power of SDP relaxation,
the relaxation gap is still signiﬁcant under most settings. In
this work, we address the issue with the SDP relaxation and
shrink the relaxation gap.

The contribution of this paper is four-fold:
• A technique to introduce non-convex cuts into the SDP
relaxation via secant approximation of non-convex con-
straints;

• An iterative algorithm that certiﬁes a given network with
nonnegative reduction in relaxation gap every step until
a certiﬁcate can be produced;

• Theoretical and empirical analyses of the efﬁcacy of sev-
eral other cut-based techniques for comparison purposes;
• Geometrical analysis of the proposed technique using
to provide insights into the current

non-convex cuts,
approach and future improvements.

A. Notations

We denote the set of real-valued m×m symmetric matrices
as Sm, and the notation X (cid:23) 0 means that X is a symmetric
positive semideﬁnite matrix. ”·” denotes the usual vector dot
product. The Hadamard (element-wise) product between X
and Y is denoted as X (cid:12) Y . Operator diag(·) coverts its
vector argument to a diagonal matrix. idx(A, a) denotes the
position/index of element a in vector or matrix A (e.g.,
idx([1, 2, 3], 3) = 3). † denotes the Penrose-Moore generalized
inverse, and (cid:104)A, B(cid:105) denotes tr(A(cid:62)B) for square matrices
A, B.

II. PROBLEM STATEMENT

Consider a K-layer ReLU neural network deﬁned by

x[0] = x, x[k] = ReLU(W [k−1]x[k−1])

(1)

for all k ∈ {1, 2, . . . , K}, where x ∈ Rnx is the input to the
neural network, z (cid:44) x[K] ∈ Rnz is the output, and ˆx[k] =
W [k−1]x[k−1] + b[k−1] ∈ Rnk is the preactivation of the kth
layer. The parameters W [k] ∈ Rnk+1×nk and b[k] ∈ Rnk+1 are
the weight matrix and bias vector applied to the kth layer’s
activation x[k] ∈ Rnk , respectively. Without loss of generality,
assume that the bias terms are accounted for in the activations
x[k], thereby setting b[k] = 0 for all layers k. Let the function
f : Rnx → Rnz denote the mapping x (cid:55)→ z deﬁned by (1).

 
 
 
 
 
 
the

spirits

Following

[Wong and Kolter, 2017],
of
[Raghunathan et al., 2018], deﬁne the input uncertainty set
X ⊆ Rnx using l∞ norms: X = {x ∈ Rnx : (cid:107)x − ¯x(cid:107)∞ ≤ (cid:15)}.
Similarly, deﬁne S ⊆ Rnz as the safe set. For classiﬁcation
networks,
(possibly unbounded) usually
polyhedral sets deﬁned as the intersection of a ﬁnite number
of half-spaces:

safe

sets

are

S = {z ∈ Rnz : Cz ≤ 0},

where C ∈ RnS ×nz is given. Any output z ∈ S is said to be
safe. The safe set is assumed to be polyhedral in the rest of
this paper.

The goal of certiﬁcation is to ensure that f (x) ∈ S for all
x ∈ X , which is equivalent to checking the satisfaction of the
following inequality:

max
i∈{1,...,nS }

{f ∗

i (X )} ≤ 0

(2)

i (X ) = sup{c(cid:62)

where f ∗
i z : z = f (x), x ∈ X }. This is a
non-convex optimization problem, since the constraint z =
f (x) is nonlinear (activation functions are designed to make
f nonlinear).

A. SDP Relaxation

In this section, we present

the SDP relaxation used
for robustness certiﬁcation. The details can be found in
[Raghunathan et al., 2018]. The main idea is to convert the
ReLU constraints to quadratic constraints and then reformulate
the non-convex certiﬁcation problem (2) as a quadratically-
constrained quadratic program (QCQP). Then, the standard
SDP relaxation of the resulting QCQP leads to checking
whether the following inequality is satisﬁed:

max
i∈{1,...,nS }

{ ˆf ∗

i (X )} ≤ 0

(3)

i (X ) = sup{c(cid:62)

where ˆf ∗
The notation (x, z) ∈ NSDP means that
Rn˜x and ˜X ∈ Sn˜x with z (cid:44) ˜x[x[K]] such that

i z : (x, z) ∈ NSDP, x ∈ X }.
there exist ˜x ∈

˜x[x[0]] = x, ˜X (cid:23) ˜x˜x(cid:62), ( ˜X, ˜x) ∈ N [k]

SDP ∀k = {1 . . . K} (4)

where the membership condition ( ˜X, ˜x) ∈ N [k]
the following conditions:

SDP is deﬁned by

˜x[x[k]] ≥ 0,
˜x[x[k]] ≥ W [k−1] ˜x[x[k−1]],
diag( ˜X[x[k](x[k])T ]) = diag(W ˜X[x[k−1](x[k])T ]),
diag( ˜X[x[k−1](x[k−1])T ]) ≤ (l[k−1] + u[k−1])(cid:12)
˜X[x[k−1]] − l[k−1] (cid:12) u[k−1],

(5)

where n˜x = ΣK
j=0nj. l[k] and u[k] are the lower and upper
bounds on x[k], respectively. Given X , only l[0] and u[0] are
known. The indexing notation in this paper is inherited from
[Raghunathan et al., 2018] to promote consistency; namely,
j=0nj] for any vector a ∈ Rn˜x
a[x[k]] = a[Σk−1
j=0 nj + 1 : Σk
and A[x[k](x[l])T ] = A[Σk−1
j=0nj, Σl−1
j=0nj + 1 :
Σl

j=0nj] for any matrix A ∈ Sn˜x .

j=0 nj + 1 : Σk

For the sake of brevity, henceforth we use the shorthand
notations ˜x[k] (cid:44) ˜x[x[k]], ˜X[Ak] (cid:44) ˜X[x[k−1](x[k−1])T ],
˜X[Bk] (cid:44) ˜X[x[k−1](x[k])T ], ˜X[Ck] (cid:44) ˜X[x[k](x[k])T ], and:

˜X[k] (cid:44)

(cid:20) ˜X[Ak]
˜X[B(cid:62)
k ]

(cid:21)
˜X[Bk]
˜X[Ck]

Furthermore, deﬁne:

Ξ (cid:44)

(cid:20)1
˜x

(cid:21)

˜x(cid:62)
˜X

(6)

(7)

Note that ˜X (cid:23) ˜x˜x(cid:62) if and only if Ξ (cid:23) 0.
The above relaxation implies that if ˆf ∗

guaranteed that f ∗
impossible to conclude whether f ∗
is loose.

i (X ) ≤ 0. However, if ˆf ∗

i (X ) ≤ 0, then it is
i (X ) ≥ 0, it is
i (X ) ≥ 0 or the relaxation

B. Tightness of SDP Relaxation

Compared to previous convex relaxation schemes (such
as LP), SDP indeed yields
lower bound
[Raghunathan et al., 2018]. However, according to the above
paper and the recent results in [Zhang, 2020], SDP relaxations
of Multi-Layer Perceptron (MLP) ReLU networks are not tight
even for single-layer instances. This issue will be elaborated
below.

tighter

a

Since Ξ is positive-semideﬁnite, it can be decomposed as:

Ξ = V V (cid:62), where V =










∈ R(n˜x+1)×r

(8)










(cid:126)e
(cid:126)x(cid:62)
1
(cid:126)x(cid:62)
2
...
(cid:126)x(cid:62)
n˜x

Each vector (cid:126)xi has dimension r, which is the rank of Ξ. Since
(cid:126)e · (cid:126)e = 1, (cid:126)e is a unit vector in Rr. Furthermore, we have
˜xi = (cid:126)e · (cid:126)xi for i ∈ {1, . . . , ˜nx}. The constraints in N [k]
SDP can
be broken down into 2 parts: Input constraints, and ReLU
constraints.

2 (li + ui)(cid:126)e with radius 1

Input constraints can be regarded as restricting each vector
in the set {(cid:126)xi|i ∈ idx(˜x, ˜x[k − 1])} to lie in a circle centered
at 1
2 (ui − li). ReLU constraints, as a
generalization to the analysis performed in the aforementioned
papers, can be interpreted as (cid:126)χj lying on the circle with (cid:126)xj
as its diameter for all {j|j ∈ idx(˜x, ˜x[k])}. Here, (cid:126)χj =
(cid:80)|x[k−1]|
is the jth row of W [k].
i=1
i=1
ReLU constraints also constrain (cid:126)xj to have a nonnegative dot
product with (cid:126)e, and a longer projection on (cid:126)e than (cid:126)χj does.

wij(cid:126)xi, where {wij}|x[k−1]|

As pointed out in Lemma 6.1 of [Zhang, 2020], the SDP
relaxation is tight if and only if (cid:126)e and all the vectors (cid:126)xi’s are
collinear. Since there is no constraint on the angle between (cid:126)xi
and (cid:126)e, the resulting spherical cap (as termed in Section 4 of
the paper) always exists, and the height of this cap will be an
upper bound on the relaxation gap of the corresponding entry
in ˜x.

III. CONVEX CUTS

IV. NON-CONVEX CUTS

As a well-known technique in nonlinear and mixed-integer
optimization, adding valid convex constraints (convex cuts)
could potentially reduce the relaxation gap of the problem.
The most effective cut for SDP relaxation is based on the
reformulation-linearization technique (RLT), which is obtained
by multiplying linear constraints and relaxing the product into
linear matrix constraints [Sherali and Adams, 2013].

In the formulation of N [k]

SDP, there are only 2 linear con-
straints: ˜x[k] ≥ 0 and ˜x[k] ≥ W [k−1] ˜x[k − 1]. The RLT cuts
associated with these linear constraints exist in the original
constraint set N [k]
SDP, except for the one obtained by the
following multiplication:

(˜x[k] − W [k−1] ˜x[k − 1])(˜x[k] − W [k−1] ˜x[k])(cid:62) ≥ 0

(9)

A. Source of Relaxation Gap

The SDP relaxation is obtained by replacing the equality
constraint ˜X −˜x˜x(cid:62) = 0 with the convex inequality ˜X −˜x˜x(cid:62) (cid:23)
0. Hence, the non-convex constraint ˜X − ˜x˜x(cid:62) (cid:22) 0 excluded
in this formulation is the source of the relaxation gap.

One necessary condition for ˜X − ˜x˜x(cid:62) (cid:22) 0 is:
i ( ˜X − ˜x˜x(cid:62))φi ≤ 0 ∀i such that
φ(cid:62)
{φ1, . . . , φn˜x }forms a basis in Rn˜x

(14)

However, since −(cid:107)φ(cid:62)
add this constraint under a convex optimization framework.

i ˜x(cid:107)2 is concave in ˜x, it is impossible to

A non-convex cut is deﬁned to be a valid constraint that is
non-convex. In this case, any constraint in the form of (14) is
a non-convex cut.

which leads to the relaxed cut:

B. A Penalization Approach

diag( ˜X[Ck] − W [k−1]X[Bk] − X[B(cid:62)
+ W [k−1]X[Ak]W [k−1](cid:62)

) ≥ 0

k ]W [k−1](cid:62)

(10)

Since directly incorporating any non-convex cut makes the
resulting problem non-convex, one may resort to penalization
techniques. Explicitly, the objective of (3) can be modiﬁed as:

Note that only the diagonal entries are of interest because
the other terms do not appear in the original QCQP formula-
tion of the problem.

c(cid:62)
i z +

n˜x(cid:88)

i=1

i (˜x˜x(cid:62) − ˜X)φi
φ(cid:62)

A. Deﬁciency of RLT

Although the above RLT reformulation seems to add new
information to the SDP relaxation, a closer examination reveals
otherwise.

Proposition 1. Constraint set (5) implies the RLT inequality
(10) for all k ∈ {1, . . . , K}.
Proof. Since ˜X (cid:23) 0, all principle submatrices ˜X[k] are
positive semideﬁnite for k ∈ {1, . . . , K}. Then, ˜X[k] (cid:23) 0
can be restated in terms of the general Schur’s complement:

˜X[k] (cid:23) 0 ⇐⇒ { ˜X[Ck] (cid:23) 0,
˜X[Ak] − ˜X[Bk]( ˜X[Ck])† ˜X[B(cid:62)
(I − ˜X[Ck]( ˜X[Ck])†) ˜X[B(cid:62)

k ] = 0}

k ] (cid:23) 0,

i=1 φ(cid:62)

the constraint ˜X − ˜x˜x(cid:62) (cid:23) 0 implies that
Nevertheless,
i z+(cid:80)n˜x
i (˜x˜x(cid:62)− ˜X)φi ≤ c(cid:62)
c(cid:62)
i z, therefore making the new
problem not necessarily a relaxation of the original problem,
i.e. the case ˆf ∗
i (X ) is possible. Moreover, this
penalization approach adds a convex term ((cid:107)φ(cid:62)
i ˜x(cid:107)2) to the
maximizing objective, which destroys the convexity of the
problem.

i (X ) ≤ f ∗

To avoid this issue, one may use the technique proposed in
[Luo et al., 2019]. That work penalizes a given QCQP with a
linear objective such that the problem remains a relaxation of
the original problem after penalization. The authors proposed
a sequential SDP procedure to approximate f ∗
i (X ) for any i
in (2) using the modiﬁed objective p∗

i (X ), deﬁned as:

(11)

p∗
i =

sup
(x,z)∈NSDP, x∈X

c(cid:62)
i z +

(cid:113)

i (˜x˜x(cid:62) − ˜X)ci
c(cid:62)

(15)

Now, one can write:

diag(W [k−1] ˜X[Ak]W [k−1](cid:62)
−
k ]W [k−1](cid:62)
W [k−1] ˜X[Bk]( ˜X[Ck])† ˜X[B(cid:62)

) ≥ 0

(12)

After noticing that diag( ˜X[Ck]) = diag(W [k−1] ˜X[Bk]) and
(I − ˜X[Ck]( ˜X[Ck])†) ˜X[B(cid:62)

k ] = 0, we obtain:

diag(W [k−1] ˜X[Ak]W [k−1](cid:62)

− ˜X[B(cid:62)

k ]W [k−1](cid:62)

) ≥ 0 (13)

By adding diag( ˜X[Ck] − W [k−1] ˜X[Bk]) = 0 to both sides of
equation, the above inequality yields (10).

Therefore, adding convex cuts to (3) using RLT will only
increase computation time without reducing the relaxation gap.

it can be shown that p∗
problem (2), i.e. p∗

i (X ) remains a relaxation of the original

i (X ) ≥ f ∗

i (X ).
1) Deﬁciency of Penalty Methods: The problem with the
approach proposed in (15) is that the vector ci
inside the
square root must match that of the original linear objective,
making it practically limited. Arguing under the framework
the method in
of adding constraints in the form of (14),
i ( ˜X −
[Luo et al., 2019] only enforces one of them, namely c(cid:62)
i ( ˜X − ˜x˜x(cid:62))ci is as
˜x˜x(cid:62))ci ≤ 0. Objective (15) ensures that c(cid:62)
small as possible.

Although interior point methods are known to converge to
maximum rank solutions, the relaxed matrix is low rank in
i ( ˜X −
general. Therefore, it is likely that the constraint c(cid:62)
˜x˜x(cid:62))ci ≤ 0 is already satisﬁed for the unmodiﬁed solution.
Simulations found in Section 6 corroborate this fact.

C. Secant Approximation

the

To address

above-mentioned issues, we

lever-
inspired by
age the method of
[Saxena et al., 2010]. Note that the constraint (14) can be
written as:

secant approximation,

−(φ(cid:62)

i ˜x)2 ≤ −(cid:104) ˜X, φiφ(cid:62)
i (cid:105)

−(cid:104) ˜X, φiφ(cid:62)
i (cid:105)

Proof. As Q → ∞, ξq = φ(cid:62)
Therefore, ξqξq+1 −(φ(cid:62)
−(φ(cid:62)

i ˜x)2, resulting in −(φ(cid:62)

i ˜x = ξq+1 for q = {0, . . . , Q+1}.
i ˜x)2 =

i ˜x)(ξq +ξq+1) = (φ(cid:62)

i ˜x)2 −2(φ(cid:62)

i ˜x)2 ≤ −(cid:104) ˜X, φiφ(cid:62)
i (cid:105)

Of course, an inﬁnite number of divisions is impractical
from both a complexity standpoint and a numerical standpoint.
Thus, selecting ξq’s intelligently to minimize the red area in
Figure 1 is critical.

Proposition 2. Given l and u for {φ(cid:62)
secant approximation with Q division points {ξq}Q
the best approximation when [l, u] is equally partitioned.

i ˜x| (˜x, ˜X) ∈ NSDP}, a
q=1 achieves

Proof. We optimize for the red area in Figure 1:

l

ξ1

ξ2

u

φ(cid:62)
i ˜x

min
{ξq}Q

q=1

Q+1
(cid:88)

(cid:90) ξq

q=1

ξq−1

−x2 − (ξq−1ξq − x(ξq−1 + ξq))dx

i ˜x| (˜x, ˜X) ∈
Fig. 1: l and u are lower and upper bounds of {φ(cid:62)
NSDP}, respectively. Red areas indicate false feasible space
induced by the secant approximation.

Graphically, the grey area in Figure 1 indicates the orig-
inal feasible space of (˜x, ˜X) constrained by −(φ(cid:62)
i ˜x)2 ≤
−(cid:104) ˜X, φiφ(cid:62)
i (cid:105). Since the set is non-convex, we use secant lines
to approximate it. The approximated feasible space contains
every point above the 3 secant lines, namely the grey area plus
the red area. ξ1, ξ2 are the points by which the feasible space
is divided. For each part of the space, we use a separate line
to lower-bound it. In other words, for each divided space, a
necessary condition for (˜x, ˜X) to lie in the original feasible
space is given. As observed in Figure 1, since secants are
used for approximation, it is important that l and u be known
in advance. For this certiﬁcation problem, those bounds can
be calculated with auxiliary SDPs. More importantly, for any
{(˜x, ˜X) ∈ NSDP| ˜X = ˜x˜x(cid:62)}, (˜x, ˜X) lies above exactly one
secant line.

In mathematical language, this means that
(cid:95)

(˜x, ˜X) ∈ Γq(φi)

q∈{0,...,Q+1}

where Γq(φi) is deﬁned as
(cid:20)
ξqξq+1 − (φ(cid:62)

(cid:95)

ξq ≤ φ(cid:62)

i ˜x ≤ ξq+1, and

q∈{0,...,Q+1}

i ˜x)(ξq + ξq+1) ≤ −(cid:104) ˜X, φiφ(cid:62)
i (cid:105)
(17)
and (cid:87) is a logical OR symbol commonly used in boolean
algebra and ξ0 (cid:44) l, ξq+1 (cid:44) u, with Q being the number
of dividing points. This means that every pair (˜x, ˜X) in
{(˜x, ˜X) ∈ NSDP| ˜X = ˜x˜x(cid:62)} belongs to only one Γq(·) for
some q, except when φ(cid:62)

i ˜x = ξq for q = {0, . . . , Q + 1}.

Lemma 1. The secant approximations (16) exactly recovers
the constraint −(φ(cid:62)

i ˜x)2 ≤ −(cid:104) ˜X, φiφ(cid:62)

i (cid:105) when Q → ∞.

(16)

(cid:21)

Since the objective f (·) is convex in {ξq}Q
0 implies that ξq = 1
This further implies an equal partitioning of [l, u].

q=1) =
2 (ξq−1 + ξq+1) for q = {0, . . . , Q + 1}.

q=1, ∇f ({ξq}Q

V. SEQUENTIAL CONSTRUCTION OF VALID CUTS

Based on the discussion of the pros and cons of various
strengthening techniques in the previous section, the method
of secant approximation offers a signiﬁcant beneﬁt over the
existing techniques. However, we have only discussed how to
i ( ˜X − ˜x˜x(cid:62))φi > 0 for a par-
approximate a single constraint φ(cid:62)
ticular φi vector, but the deployment of secant approximation
requires an in-depth analysis and careful design.

There are 2 main questions arising from this approach:
1) Which φi vector should be chosen? How do we know if
i ( ˜X −˜x˜x(cid:62))φi > 0 is already satisﬁed?

the constraint of φ(cid:62)

2) How many such constraints shall be added?

The ﬁrst question can be addressed via a technique called
Cut Generating Linear Programming (CGLP) and the second
one can be solved via a sequential procedure. The two above-
mentioned solutions will be elaborated in the following two
subsections.

A. Constructing Valid Cuts
To study whether φ(cid:62)

i ( ˜X − ˜x˜x(cid:62))φi > 0 is already sat-
isﬁed, a candidate solution (˜x∗, ˜X ∗) is needed, which can
be easily obtained by running the original SDP problem.
By direct substitution, one can verify if the given constraint
i ( ˜X ∗ − ˜x∗˜(x∗)(cid:62))φi > 0 is redundant. More importantly,
φ(cid:62)
with the candidate solution, one can ﬁnd the eigenvectors cor-
responding to the largest positive eigenvalues of ˜x∗(˜x∗)(cid:62)− ˜X ∗
in order to ﬁnd φis that violate the negative semideﬁnite
constraint the most.

However, this is not enough, because secant approximation
induces a false feasible space (colored in red in Figure 1). As
a result, it could happen that while a given constraint is not
redundant, its secant approximation is. This phenomenon is
well illustrated by Example 1 in [Saxena et al., 2010].

Therefore, it is necessary to actively search for a non-
redundant secant approximation, termed a valid cut, given

a candidate φi vector, which is usually an eigenvector
of ˜x∗(˜x∗)(cid:62) − ˜X ∗ corresponding to a strictly positive
this can be accomplished via
eigenvalue. Mathematically,
the Cut Generating Linear Programming (CGLP) problem
[Saxena et al., 2010]:

min
α,β,{µq},{νq}

α(cid:62)χ∗ − β

q νq ≤ α q ∈ {1, . . . , Q}
q νq ≥ β q ∈ {1, . . . , Q}

q ∈ {1, . . . , Q}

s.t. A(cid:62)µq + D(cid:62)
b(cid:62)µq + d(cid:62)
µq, νq ≥ 0
Q
(cid:88)

(κ(cid:62)µq + η(cid:62)

q νq) = 1

q=1

(CGLP)
where χ is a vectorized version of the variables (˜x, ˜X)
with χ∗ = [˜x∗(cid:62)
, vec( ˜X ∗)(cid:62)](cid:62), {µq}, {νq} being vectors of
nonnegative entries, and κ, {ηq} being normalizing constants.
κ, {ηq} are some constants to help with numerical stabilities,
and are not of theoretical interest.

Most importantly, Aχ ≥ b (Aχ is just matrix multiplication)
is a reformulation of the constraint (˜x, ˜X) ∈ NSDP, and
q χ ≥ dq is a reformulation of the constraint (˜x, ˜X) ∈
D(cid:62)
Γq(φi) for any φi. This is possible because every constraint
is afﬁne after lifting (i.e. introducing ˜X in the place of ˜x˜x(cid:62)).
That is, different CGLPs exist for different φis. Moreover,
note that given Q ∈ Z, {ξq}Q
q=1 are calculated according to
Proposition 2.

To illustrate how CGLP can help, we introduce the fol-
lowing theorem, which is a modiﬁcation of Theorem 1 in
[Saxena et al., 2010]:

Theorem 1. If the optimal value of (CGLP) is negative, then a
valid cut αχ ≥ β is found that cuts off the candidate solution
(˜x∗, ˜X ∗). Conversely, if the optimal value is nonnegative, then
no such cut exists for {Γq(φi)}Q+1
q=0 .

Proof. According to Theorem 3.1 of [Balas, 1998], αχ ≥ β
is a valid constraint for (3) with the secant approximations
(16) if and only if the ﬁrst three lines of the CGLP constraints
hold true. All alphas and betas for which αχ ≥ β is a valid
constraint is in the polyhedral search space of CGLP. If there
exist α∗ and β∗ such that α∗χ∗ < β∗, the optimal value of
CGLP must be negative. On the other hand, if the optimal
value of CGLP is negative, such α∗ and β∗ must exist. In
this case, α∗ and β∗ must also meet the condition α∗χ ≥ β∗
due to the convexity of polyhedra. Then, χ∗ contradicts the
requirements of χ, making the constraint αχ ≥ β a valid cut.
Conversely, if no such α∗ and β∗ exist, then χ∗ already
satisﬁes all valid constraints with respect to {Γq(φi)}Q+1
q=0
since the polyhedral search space is convex, and convexity
guarantees an exhaustive search. Thus, no valid cut can be
found.

Theorem 1 states that if the CGLP problem for a particular
φi vector returns a negative optimum, then αχ ≥ β is a valid

4

5

6

7

8

9

10

11

12

13

14

15

cut, and can be added to the SDP problem to further strengthen
the problem.

Theorem 1 and Lemma 1 lead to the following result:

Corollary 1. If D(cid:62)
k χ ≥ dk corresponds to a set of constraints
(cid:87)
q∈{0,...,Q+1}(˜x, ˜X) ∈ Γq(φi) with φi being an eigenvector
of ˜x∗ ˜x∗(cid:62) − ˜X ∗ associated with a positive eigenvalue, then a
valid cut αχ ≥ β always exists if Q → ∞.

B. A Sequential Algorithm

To systematically generate constraints of the form φ(cid:62)

i ( ˜X −
˜x˜x(cid:62))φi > 0, we propose Algorithm 1 and study its perfor-
mance in this part.

Algorithm 1: Sequential SDP veriﬁcation of NN Ro-
bustness by adding secant-approximated non-convex
cuts
1 veriﬁcation (X , S, {W 0 . . . W K−1}, Q, maxiter, γ);

Input

: Input uncertainty set X , safe set S, trained

network weights {W 0 . . . W K−1}, number of
partitions for the secant approximation,
maximum number of iterations maxiter, and
the eigenvalue threshold γ.

Output: τr = max C (cid:62)

r z for r ∈ {1, . . . , R}, where Cr

is the rth row of C, and R is the number of
rows of C, as speciﬁed by the safe set S

2 for c = C1, . . . , CR do
3

(˜x∗, ˜X ∗) ← arg max{c(cid:62)z : (x, z) ∈ NSDP, x ∈ X }
;
˜α = [], ˜β = [] ;
for i = 1, . . . , maxiter do

− ˜X ∗ bigger than γ, and with

(λ ∈ Rm, V ∈ Rn˜x×m) ← eigenvalues of
˜x∗ ˜x∗(cid:62)
eigenvector corresponding to the ith
eigenvalue being ith column of V ;
for φ = V1, . . . , Vm do

(l, u) ← {min, max}{φ(cid:62)z : (x, z) ∈
NSDP, x ∈ X } ;
(α, β) ← CGLP(Q, φ, u, l);
if α(cid:62)χ∗ < β then

˜α = [˜α; α(cid:62)], ˜β = [ ˜β; β]

end

end
(˜x∗, ˜X ∗) ← arg max{c(cid:62)z : (x, z) ∈
NSDP, ˜αχ ≥ ˜β, x ∈ X } ;

end
τr = c(cid:62) ˜x∗[K];

16
17 end

The main result of this paper is stated below.

Theorem 2. For every iteration i ∈ {2, . . . , maxiter} in
Algorithm 1, it holds that

f ∗(X ) ≤ c(cid:62) ˜x∗

i [K] ≤ c(cid:62) ˜x∗

i−1[K] ≤ ˆf ∗(X )

(18)

i , ˜X ∗

i , and by extention Ξ∗

with ˜x∗
i being the optimizers of the
ith iteration. The middle inequality becomes strict if at least
one of the optimal values of CGLPs in iteration i is negative
and either of the following holds:

• ˆc lies

in the null

[01×(n˜x−nK ) c(cid:62)](cid:62)

• Q → ∞

space of Ξi−1, where ˆc (cid:44)

i [K] ≤ c(cid:62) ˜x∗

i−1[K] ≤ ˆf ∗(X ) are
Proof. The inequalities c(cid:62) ˜x∗
due to the fact that a strict reduction in the search space will
not yield a higher optimal value. f ∗(X ) serves as a lower
bound since the constraints are valid, according to Theorem
1.

Now, since the objective is linear, ˆc(cid:62) ˜x = γ must be
a supporting hyperplane for the spectrahedral (convex body
arising from linear matrix inequalities) feasible set at ˜x∗
i−1,
where γ denotes as the constant c(cid:62) ˜x∗
i−1[K]. By Proposition
2 in [Roshchina, 2017], the intersection of a closed convex
set (e.g., this feasible set) and the supporting hyperplane is an
exposed face. Furthermore, by [Ramana and Goldman, 1995],
it is known that any exposed face of a spectraheron is a proper
face, and the null space of Ξ will stay constant over the relative
interior of this face. Thus, the intersection will consist of points
(˜x, ˜X) such that ˆc(cid:62) ˜x = γ and N (Ξ) = N (Ξi−1).

j ](cid:62). Therefore, ˆψ(cid:62)
ˆψ(cid:62)

Let a basis of N (Ξi−1) be denoted as {ψj}, where each
vector is partitioned as ψj (cid:44) [ωj
j ˜x =
−ωj for j ∈ {1, . . . , n˜x + 1}. If ˆc ∈ N (Ξi−1), the supporting
hyperplane will intersect the spectrahedron at exactly one point
(the face is of afﬁne dimension 0) because ˜x is already ﬁxed
in { ˆψj} coordinates, and γ can only take on one value. If
the intersection only consists of one point, then a valid cut
will invalidate this point, and the objective value will strictly
decrease.

On the other hand,

if Q → ∞,
the original negative
semideﬁnite constraint is recovered, as per Lemma 1. Then,
after using the valid cut, Ξi will have a strictly lower rank
than Ξi−1. Since the intersection consists of points such that
N (Ξ) = N (Ξi−1), all those points will not be in the feasible
space anymore under the presence of this valid cut. This leads
to a strict decrease in objective value.

Since n˜x is usually very large for multilayer networks, the
ﬁrst condition is often satisﬁed. However, it is important to
note that those 2 conditions are only sufﬁcient conditions,
and that in practice there is normally a non-zero reduction in
relaxation gap whenever a valid cut is calculated. Moreover,
if a valid cut is given, the algorithm will always reach new
optimal points, thus avoiding the situation of repeating the
same search again and again.

This algorithm is asymptotically exact if an inﬁnite number

of iterations is taken, as explained below.

Lemma 2. The relation c(cid:62) ˜x∗
provided that γ = 0 and Q → ∞

i [K] = f ∗(X ) holds as i → ∞,

Proof. The proof follows directly from Theorems 1, 2, and
Corollary 1.

VI. EXPERIMENTS
We certify the robustness of the IRIS dataset1 with pre-
trained MLP classiﬁcation network with 99% accuracy on test
data using the original SDP approach, our Algorithm 1, and
the penalization approach in Section IV-B. For all experiments,
maxiter is set to 10. Moreover, the l∞ radius of X is set to
0.15 for networks with 5 and 10 hidden layers, and set to 0.075
for the other 2 networks. This is because larger networks are
naturally more sensitive to perturbation.

A. Certiﬁcation Percentage

TABLE I: Certiﬁcation Percentage (First Row) and Average
Trace Gap (Second Row).

H-Layers/Q
5, Q=20
(Trace Gap)
10, Q=5
(Trace Gap)
15, Q=5
(Trace Gap)
15, Q=20
(Trace Gap)

SDP

100%
4.73 × 10−7
0%
31.15
0%
491.71
0%
513.71

Algorithm 1
100%
4.73 × 10−7
80%
27.09
60%
61.1
100%
70.05

Penalized
100%
0.48
0%
20.42
0%
228.68
0%
198.79

In Table I, certiﬁcation percentage and average trace gaps
(tr( ˜X) − ˜x(cid:62) ˜x) for the original SDP procedure, our Algorithm
1, and the penalized approach are all calculated for classiﬁca-
tion networks with different numbers of hidden layers. Trace
gap is an alternative measure for the rank of ˜X, since many
non-zero eigenvalues can be really small, and it is not obvious
whether we should count them when calculating rank.

It can be observed that when the network is small, the
original SDP procedure is already sufﬁcient. However, as the
number of hidden layers grows, Algorithm 1 shows its strength
by certifying more data points. The penalization approach
remains ineffective although sometimes it decreases the trace
gap.

It should be noted that when a small Q cannot generate
enough valid cuts (in this case Q=5), increasing that partition
number (Q=20) will generally improve the performance, as
can be noticed in the 2 cases with 15 hidden layers. This is
also an empirical veriﬁcation of Lemma 2.

The objective ˆf ∗

i (X ) of (3) for the aforementioned ap-

proaches is shown in Figure 2 for 5 data points each.

B. Running time

The running time of Algorithm 1 is problem-speciﬁc, be-
cause the number of constraints that needs to be added is
dependent on how loose the candidate solution is, and it is
apparent from Figure 2 that even for similar points in a small-
scale dataset, the differences are large.

Most importantly, the most time consuming part of this
algorithm is CGLP, because after linearization into vectors,
the dimension of the LP is large. However, since the LP is
sparse, one may use specialized methods to handle the LP in
a more time-efﬁcient fashion.

1https://archive.ics.uci.edu/ml/datasets/iris

equation (19):

(cid:88)

(φiφj)(cid:107)(cid:126)xi(cid:107)(cid:107)(cid:126)xj(cid:107)cos(θij) ≤

i,j∈{n˜x}×{n˜x},i(cid:54)=j
(cid:88)

(φiφj)(cid:107)(cid:126)xi(cid:107)(cid:107)(cid:126)xj(cid:107)cos(θi)cos(θj) + (cid:15)

(20)

i,j∈{n˜x}×{n˜x},i(cid:54)=j

after substituting equation (8). Here, φi is the ith entry of φ
(to be distinguished from φi, which is a vector in the set of
basis), and θij is the angle between the vectors (cid:126)xi, (cid:126)xj, and θi
denotes the angle between (cid:126)e and (cid:126)xi for i ∈ {1, . . . , n˜x}.
Therefore, there must exist a pair (i, j) such that

cos(θij) ≤ cos(θi)cos(θj) +

(cid:15)
n˜x

(21)

Since n˜x is usually large in multi-layer networks and (cid:15) is
can be neglected for practical
chosen to be very small,
purposes.

(cid:15)
n˜x

Recall from Section II-B that θi = 0 ∀i ∈ {1, . . . , n˜x} is
both sufﬁcient and necessary for the tightness of the relaxation.
Furthermore, in the following lemma we will show that any
decrease in θi for any i can contribute to a tighter relaxation.

Lemma 3. For any row vector (cid:126)x in V , as part of
the
factorization of ˜X, any increase in the lower bound for (cid:126)x·(cid:126)e
(cid:107)(cid:126)x(cid:107)
will decrease the upper bound on the relaxation gap for at
least one entry in ˜x.

Proof. Extending the results from Section II-B,
it can be
veriﬁed that for a ﬁxed θ, the angle between (cid:126)e and (cid:126)χ, a
decrease in (cid:107)(cid:126)χ(cid:107) will lead to a smaller spherical cap. Now,
consider a ﬁxed (cid:107)(cid:126)χ(cid:107). It can be shown that the height of the
spherical cap is (cid:107)(cid:126)χ(cid:107)
2 (1 − cos(θ)). Therefore, any increase in
cos(θ) will lead to a smaller cap. Since (cid:126)χ is just a linear
combination of different (cid:126)x, increasing the lower bound for
(cid:107)(cid:126)x(cid:107) will also increase the lower bound for (cid:126)χ·(cid:126)e
(cid:126)x·(cid:126)e
(cid:107)(cid:126)χ(cid:107) .

Given any such pair (i, j), if cos(θij) is large, namely possi-
bly close to one, the term cos(θi)cos(θj) will have to be larger
under the constraint cos(θij) ≤ cos(θi)cos(θj) + (cid:15)
. This
n˜x
means that cos(θi) ≈ 1 and cos(θj) ≈ 1, making both angles
very small. Without this constraint, it is possible for (cid:126)xj, (cid:126)xi
to be collinear, but not collinear with (cid:126)e, thus making them
have large angles θi and θj. Figure 3 graphically showcases
this difference. In particular, since ReLU only outputs positive
values, we have θi ∈ [0, π
2 ] for all i ∈ {1, . . . , n˜x}. However,
the vectors (cid:126)xi that correspond to the input layer are exceptions.
Without loss of generality, it is possible to also assume these
to be nonnegative by changing the weight matrix of the ﬁrst
layer. Therefore, cos(θi) ≥ 0 ∀i ∈ {1, . . . , n˜x}. Thus, when
cos(θi)cos(θj) ≈ 1, we have θi ≈ 0 and θj ≈ 0. Without
this constraint, it is possible that θj ≈ π and θi ≈ π. In other
words, with the ReLU constraints in place, the non-convex
cuts will push the vectors towards (cid:126)e instead of −(cid:126)e.

If cos(θij) is small, then inequality (21) will not be binding,
and the original semideﬁnite constraints will work satisfac-
torily. Ideally, inequality (21) will hold for all pairs (i, j),
but this is not guaranteed. However, under the assumption

(a) 5 H-Layers, Q=20

(b) 10 H-Layers, Q=5

(c) 15 H-Layers, Q=5

(d) 15 H-Layers, Q=20

Fig. 2: ˆf ∗
networks

i (X ) for different certiﬁcation methods and different

Below is an illustration of the runtime of Algorithm 1 for
the 5 datapoints shown in Figure 2b. It is apparent that the
CGLP procedure takes well over half of the time in the entire
algorithm.

However, since LP naturally scales much better than SDP
and the complexity can be as low as linear in the size of
the problem in presence of favorable sparsity, this algorithm
can be easily scaled to larger datasets if specialized solvers of
CGLP can be developed to handle the special structures.

Although Algorithm 1 is an exponential time algorithm in
the worst case, as most of the proofs only work when Q → ∞,
the previous section has demonstrated that we can achieve
satisfactory results even with small values of Q, making it
also empirically appealing.

TABLE II: Running time of Algorithm 1 with 10 H-Layers.

Datapoint
1
2
3
4
5

Algorithm 1 runtime (s)
166
149
143
149
165

CGLP runtime (s)
100
88
85
90
99

VII. GEOMETRIC ANALYSIS OF NON-CONVEX CUTS

In this section, we offer a geometric intuition into the

success of non-convex cuts. We revisit (16) and write it as:

ξqξq+1 − (φ(cid:62) ˜x)(ξq + ξq+1) ≤ −(cid:104) ˜X, φφ(cid:62)(cid:105)
⇔ (cid:104) ˜X, φφ(cid:62)(cid:105) ≤ (φ(cid:62) ˜x − ξq)(ξq+1 − φ(cid:62) ˜x) + (φ(cid:62) ˜x)2

(19)

Since there always exists a partition number Q large enough
such that (φ(cid:62) ˜x−ξq)(ξq+1 −φ(cid:62) ˜x) ≤ (cid:15), it is possible to rewrite

(cid:126)xi

(cid:126)xj

(cid:126)e

θij

θj

θi

(a) cos(θij) ≤ cos(θi)cos(θj)

(cid:126)e

(cid:126)xj

θi

θj

θij

(cid:126)xi

(b) cos(θij) ≥ cos(θi)cos(θj)

Fig. 3: Illustration of possible conﬁgurations of (cid:126)xi, (cid:126)xj given
a ﬁxed θij

that the weight matrices in the neural network are relatively
well conditioned (no large condition number) and that (cid:15) is
negligible, a large number of the pairs (i, j) are expected to
satisfy or approximately satisfy inequality (21).

As explained, only those pairs (i, j) with a relatively large
cos(θij) are of interest. Denote the threshold as ρ ∈ [0.5, 1)
such that cos(θij) ≥ ρ. We call a pair (i, j) to be valid if it
satisﬁes inequality cos(θij) ≤ cos(θi)cos(θj)+ (cid:15)
and invalid
n˜x
otherwise. In the worst case, cos(θi)cos(θj)−cos(θij) = 1−ρ
for all valid pairs and cos(θi)cos(θj) − cos(θij) = −(cid:37) < 0 for
all invalid pairs. This leads to the case where the number of
valid pairs is the smallest (denoted as ϑ1) and the number
of invalid pairs is the largest (denoted as ϑ2). The other
threshold (cid:37) exists because a number cos(θij) too close to
cos(θi)cos(θj) approximately satisﬁes the inequality. Under
the assumption that weight matrices are well conditioned, it
implies that (cid:107)(cid:126)xi(cid:107)(cid:107)(cid:126)xj(cid:107)s are approximately the same amongst
all (i, j) pairs. Thus, if (cid:15) is negligible:

(cid:88)

(φiφj)(cid:107)(cid:126)xi(cid:107)(cid:107)(cid:126)xj(cid:107)(cos(θi)cos(θj) − cos(θij)) ≥ 0

i,j

=⇒ ϑ1(1 − ρ) − ϑ2((cid:37)) ≈ 0 =⇒

(22)

ϑ1
ϑ2

≈

(cid:37)
1 − ρ

If ρ = 0.8 and (cid:37) = 0.2, then at least half of all pairs such
that cos(θij) ≥ ρ will be valid. As ρ increases, so does ϑ1
.
ϑ2
This means that almost all of the most important pairs (i, j)
are valid.

Furthermore, the nature of a multi-layer setup means that
if (cid:126)xi and (cid:126)xj are from different layers of the network, they
depend on each other. Figure 3 shows that the non-convex cut
tends to make (cid:126)xi and (cid:126)xj lie on different sides of (cid:126)e in the
event of a large cos(θij). This means that as vectors build on
each other (see Figure 1 in [Raghunathan et al., 2018]), they

will revolve around (cid:126)e, instead of branching out into a certain
direction. This further implies a small angle with (cid:126)e for all (cid:126)x.

VIII. CONCLUSION

to provide meaningful

This paper studies the problem of neural network veriﬁca-
tion, for which the existing cut based techniques to reduce
in
relaxation gap fail
accuracy. We leverage the fact that loose candidate optimum
points in an SDP relaxation of a QCQP reformulation of
the problem can be invalidated by simple linear constraints.
Using this property, a SDP-based method is developed, which
reduces the relaxation gap to zero as the number of iterations
increases. This algorithm is proven to be theoretically and
empirically effective.

improvement

By actively constructing constraints using CGLP, we obtain
provably valid cuts. More importantly, a negative optimal value
in CGLP also guarantees a nonzero reduction in the relaxation
gap. It is veriﬁed that the relaxation gap for the existing
methods is large when tested on large-scale networks, while
the proposed algorithm offers a satisfactory performance.

REFERENCES

[Anderson et al., 2020] Anderson, B. G., Ma, Z., Li, J., and Sojoudi, S.
Tightened convex relaxations for neural network robustness

(2020).
certiﬁcation. arXiv preprint arXiv:2004.00570.

[Anderson et al., 2021] Anderson, B. G., Ma, Z., Li, J., and Sojoudi, S.
(2021). Partition-based convex relaxations for certifying the robustness
of relu neural networks. arXiv preprint arXiv:2101.09306.

[Balas, 1998] Balas, E. (1998). Disjunctive programming: Properties of the
convex hull of feasible points. Discrete Applied Mathematics, 89(1-3):3–
44.

[Luo et al., 2019] Luo, H., Bai, X., and Peng, J. (2019). Enhancing semidef-
inite relaxation for quadratically constrained quadratic programming via
Journal of Optimization Theory and Applications,
penalty methods.
180(3):964–992.

[Raghunathan et al., 2018] Raghunathan, A., Steinhardt, J., and Liang, P. S.
(2018). Semideﬁnite relaxations for certifying robustness to adversarial
examples. In Advances in Neural Information Processing Systems, pages
10877–10887.

[Ramana and Goldman, 1995] Ramana, M. and Goldman, A. J. (1995).
Some geometric results in semideﬁnite programming. Journal of Global
Optimization, 7(1):33–50.

[Roshchina, 2017] Roshchina, V. (2017). Face of convex sets.
[Saxena et al., 2010] Saxena, A., Bonami, P., and Lee, J. (2010). Convex re-
laxations of non-convex mixed integer quadratically constrained programs:
extended formulations. Mathematical programming, 124(1-2):383–411.
[Sherali and Adams, 2013] Sherali, H. D. and Adams, W. P. (2013). A
reformulation-linearization technique for solving discrete and continuous
nonconvex problems, volume 31. Springer Science & Business Media.
[Singh et al., 2019] Singh, G., Ganvir, R., P¨uschel, M., and Vechev, M.
(2019). Beyond the single neuron convex barrier for neural network
certiﬁcation. In Advances in Neural Information Processing Systems, pages
15098–15109.

[Sonoda and Murata, 2017] Sonoda, S. and Murata, N. (2017). Neural
network with unbounded activation functions is universal approximator.
Applied and Computational Harmonic Analysis, 43(2):233–268.

[Tjeng et al., 2017] Tjeng, V., Xiao, K., and Tedrake, R. (2017). Evaluating
robustness of neural networks with mixed integer programming. arXiv
preprint arXiv:1711.07356.

[Wong and Kolter, 2017] Wong, E. and Kolter, J. Z. (2017).

Provable
defenses against adversarial examples via the convex outer adversarial
polytope. arXiv preprint arXiv:1711.00851.

[Zhang, 2020] Zhang, R. Y. (2020). On the tightness of semideﬁnite
relaxations for certifying robustness to adversarial examples. arXiv preprint
arXiv:2006.06759.

