0
2
0
2

y
a
M
4

]

G
L
.
s
c
[

1
v
0
4
6
0
1
.
5
0
0
2
:
v
i
X
r
a

DETECT: A Hierarchical Clustering Algorithm for
Behavioural Trends in Temporal Educational Data

Jessica McBroom, Kalina Yacef and Irena Koprinska
School of Computer Science, University of Sydney, Sydney, Australia
{jmcb6755, kalina.yacef, irena.koprinska}@sydney.edu.au

May 4, 2020

Abstract

Techniques for clustering student behaviour oﬀer many opportunities to im-
prove educational outcomes by providing insight into student learning. However,
one important aspect of student behaviour, namely its evolution over time, can
often be challenging to identify using existing methods. This is because the
objective functions used by these methods do not explicitly aim to ﬁnd cluster
trends in time, so these trends may not be clearly represented in the results.
This paper presents ‘DETECT’ (Detection of Educational Trends Elicited by
Clustering Time-series data), a novel divisive hierarchical clustering algorithm
that incorporates temporal information into its objective function to prioritise
the detection of behavioural trends. The resulting clusters are similar in struc-
ture to a decision tree, with a hierarchy of clusters deﬁned by decision rules on
features. DETECT is easy to apply, highly customisable, applicable to a wide
range of educational datasets and yields easily interpretable results. Through
a case study of two online programming courses (N > 600), this paper demon-
strates two example applications of DETECT: 1) to identify how cohort be-
haviour develops over time and 2) to identify student behaviours that charac-
terise exercises where many students give up.

1

Introduction

In recent decades, educational datasets have become increasingly rich and complex,
oﬀering many opportunities for analysing student behaviour to improve educational
outcomes. The analysis of student behaviour, particularly temporal trends in this
behaviour, has played a major role in many recent studies in areas including automated
feedback provision [8, 12, 16, 19], dropout analysis [17, 18, 22], collaborative learning
[4, 21, 23] and student equity [6, 9, 13].

However, a signiﬁcant challenge in analysing student behaviour is its complexity
and diversity. As such, clustering techniques [24], which organise complex data into
simpler subsets, are an important resource for analysing student behaviour, and have
been employed in many recent studies [5]. For example, in [1] and [3] K-means clus-
tering and a self-organising map, respectively, are used to group students based on

1

 
 
 
 
 
 
their interactions with an educational system. In addition, in [7] student programs
are clustered to identify common misconceptions.

One limitation of standard clustering techniques is that they are not well-suited
to detecting behavioural trends in time. One solution is to use time-series clustering
techniques, which typically combine standard techniques with extra processing steps
[2]. For example, [15] uses dynamic time warping in conjunction with K-means clus-
tering to cluster time series’ of student Moodle activity data. Alternatively, temporal
information is often considered only after all student work samples or behaviours have
been clustered. For example, in [14] student work is clustered to allow an interaction
network over time to be built and in [10] clusters of student behaviour over time are
used in a second round of clustering.

Although it is possible to gain insight into student behavioural changes using
these techniques, one important limitation is that temporal trend detection is not
explicitly incorporated into the objective function when clustering. For example,
consider the case where K-means is ﬁrst used to cluster student behaviours, and
then cluster changes over time are observed, as in [10]. Since the objective of K-
means is to minimise the distance between points (which in this case represent student
behaviours), the process will prioritise grouping behaviours that match on as many
features as possible. However, this may obscure important trends, especially if many
of the features are unrelated to these trends.

The contribution of this paper is ‘DETECT’ (Detection of Educational Trends
Elicited by Clustering Time-series data), a novel divisive hierarchical clustering algo-
rithm that incorporates trend detection into its objective function in order to identify
interesting patterns in student behaviour over time. DETECT is highly general and
can be applied to many educational datasets with temporal data (for example, from
In addition, it can be customised
regular homework tasks or repeated activities).
to detect a variety of trends and produces clusters that are well-deﬁned and easy to
understand. Moreover, it does not require that the features be independent, or that
the objective function be diﬀerentiable.

Broadly, DETECT has similar properties to the classiﬁcation technique of decision
trees [20]. In particular, it produces a hierarchy of clusters distinguished by decision
rules. However, whereas decision trees are a supervised technique requiring the exis-
tence of classes in order to calculate entropy, DETECT is unsupervised and uses an
objective function completely unrelated to this measure.

This paper is set out as follows: Section 2 describes the DETECT algorithm,
including the input it takes, its ﬂexibility and how the output is interpreted. Section
3 then shows example usage of the algorithm through a case study and Section 4
concludes with a summary of the main ideas of the paper.

2 DETECT Algorithm

2.1 Overview

DETECT produces clusters of student behaviour that reveal cohort behavioural trends
in educational datasets. Such trends can include changes in behaviour over time,

2

Table 1: Structure of input data, where the number of cells in the table is equal to
S (number of students) × T (number of time periods) × M (number of features).
F1,...,FM are diﬀerent feature names.

Student Time F1
v111
1
1
v121
2
1
...
...
...
v1T 1
T
1
...
...
...
vN 11
1
S
...
...
...
vN T 1
T
S

F2
v112
v122
...
v1T 2
...
vN 12
...
vN T 2

... FM
v11M
...
v12M
...
...
...
v1T M
...
...
...
vN 1M
...
...
...
vN T M
...

anomalous behaviours at speciﬁc points in the course or a variety of other customisable
trends. This is achieved by iteratively dividing student behaviours into clusters that
maximise a time-based objective function. The clusters found can then be interpreted
by teachers and course designers to better understand student behaviour during the
course.

DETECT can be applied to a wide range of datasets, of the form described in
Table 1.
In particular, the data should be temporal - that is, able to be divided
into a series of comparable time steps. For example, a series of homework tasks or
ﬁxed time periods during an intervention could be considered as comparable time
steps. In addition, for each student and time period, there should be a set of features
describing the behaviour of the student during that time period. These features could
be numeric, such as the number of exercise attempts, or categorical, such as a label
for the style of their work. Note that features are not required to be independent or
equally important, since the objective function can determine the quality of features
and penalise less useful ones.

DETECT outputs clusters of student behaviour explicitly deﬁned by rules on
feature values. For example, a cluster may be deﬁned as all rows of the input data
where ‘num submissions’ ≤ 7 and ‘completed’ == ‘yes’. These clusters are organised
into a hierarchical structure where, in each successive level, an additional condition
is added, similarly to a decision tree. Examples of this are given in Section 3, as part
of the case study.

It is important to note that the clusters are not clusters of students but rather
clusters of the input data rows (which each represent the behaviour of one student
at one time period). This means students are in many clusters - one for each time
period. By observing changes to the distributions of clusters over time, trends in
student behaviour can be identiﬁed (see Section 3).

2.2 Cluster Formation

Clusters are formed divisively through an iterative process with four main steps, as
summarised in Figure 1. Initially, all examples are placed in the same cluster. Then,

3

during each iteration, a search is performed to ﬁnd the best feature and value to split
this cluster on. If this split would result in new clusters that are larger in size than
a speciﬁed lower-limit (e.g. at least 100 examples each), then the split it performed,
creating two new clusters, and the process is repeated recursively on the new clusters.
Otherwise, the split is not performed. The algorithm terminates when no cluster can
be split further.

Using the given objective functions and assuming the cluster size threshold scales
proportionally with the number of examples (which places a constant upper bound
on the number of clusters), the time complexity of this process is O(nm log n), where
m is the number of features and n the number of examples.

Figure 1: A summary of the steps involved in DETECT.

2.2.1 Feature and Value Search.

Before a cluster is divided, a search is performed to ﬁnd the best feature and value to
split on. For each feature, this can be performed in O(n log n) time (where n is the
cluster size) using the given objective functions in the next subsection. For numeric
features, the process is as follows:

1. Sort the examples in ascending order based on the feature value.

2. Create two new clusters, one containing no examples (Ca) and the other con-

taining all (Cb).

3. Set a threshold, t, that is lower than the smallest feature value.

4. While there are still examples in Cb, increase t to the next smallest feature value
(or larger) and add all examples ≤ t from Cb to Ca, each time checking if this
improves the objective function value (and, if so, remembering t).

The best feature and split will then be the one that optimises the function values.
Similarly, for categorical features, each category can be iterated through to ﬁnd the
best one to split oﬀ from the rest. Note that we recommend minimising the amount
of missing data (e.g. by selecting subsets of students or making time periods relative

4

to students as in [11]). However, if required, missing values can be treated as another
category if the feature is categorical, or, if the feature is numeric, the process can be
repeated twice – once where the missing value examples are always in Ca, and once
where they are always in Cb.

2.2.2 Objective Function.

The objective function is used to determine the quality of potential cluster divisions
using temporal information, thereby controlling the types of trends detected by the
algorithm. More speciﬁcally, this function maps the distributions of Ca and Cb over
time, along with optional additional parameters, to a score. It can be customised to
suit diﬀerent purposes and there are no constraints such as diﬀerentiability on the
function. Two examples of an objective function are deﬁned here:

Let n = [na(ti), nb(ti)] be the number of students in clusters Ca and Cb respectively
at time i and T be the number of time steps in total. In addition, for f2, let x be a
time step of interest. Then:

f1(n(t1), n(t2), ..., n(tT )) =

f2(n(t1), n(t2), ..., n(tT ), x) =

na(t1) + na(t2)
2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
|na(tx) − na(tx+1)| + |na(tx) − na(tx−1)|
2

na(tT ) + na(tT −1)
2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

The ﬁrst function, f1, compares the average number of students in Ca at the
beginning of the course to the average at the end. As such, it is a measure of how
many students change cluster from the start to the end of the course, and will be
maximised when there is a large shift in behaviour. In contrast, the second function,
f2, compares the number of students in Ca at time x to the adjacent time periods
and ﬁnds the average diﬀerence. As such, it is maximised when behaviours at time x
vary greatly from those at neighbouring times.

3 Case Study

This section demonstrates two example applications of DETECT using the two ob-
jective functions introduced in Sec. 2.2.2. Speciﬁcally, in the ﬁrst example we apply
DETECT to an intermediate course using f1 to ﬁnd behavioural trends over time.
In the second example, we then apply DETECT to a beginner course using f2 to
ﬁnd behaviours that characterise an exercise where many students give up. While the
data come from programming courses, we only use general features not speciﬁc to this
domain to demonstrate the generality of the approach.

3.1 Data

Our data come from school students participating in two online Python programming
courses of diﬀerent diﬃculty levels: intermediate (N=4213) and beginners (N=7164).1

1 These refer to the number of students who attempted at least the ﬁrst exercise.

5

Table 2: Clusters formed by applying f1 to the intermediate course, using a minimum
cluster size threshold of 400 - i.e. an average of 20 students per time period. The
clusters are deﬁned by the number of autosaves (level 1) and how long before the
deadline the exercise was completed (level 2).

Level 1

autosaves ≤ 9

autosaves > 9

Level 2
completed 7.25 days or more before deadline C11
C12
completed within 7.25 days of deadline
C2

Label

These courses were held over a 5-week period during 2018 as part of a programming
challenge held primarily in Australia. The courses involved weekly notes, which intro-
duced students to new concepts, and programming exercises to practice these skills.
Students received automated feedback on their work from test cases and were able to
improve and resubmit their work.

From this data, we extracted 10 features per student per exercise: 1-3) the number
of times the student viewed, failed and passed the exercise, 4) the number of times
their work was automatically saved (triggered when unsaved work was left for 10
seconds without being edited), 5-8) the time of the ﬁrst view, autosave, fail and pass
relative to the deadline, 9) the average time between successive fails and 10) the
time between the ﬁrst fail and passing. Note that these features did not need to be
independent (see Section 2.1).

3.2 Example 1: Using f1 to Detect Changes Over Time

When analysing student behaviour during a course, one important question is how
this behaviour changes in time. To answer this, f1 was applied to the data from the
intermediate course. Since exercises from the last week diﬀered in structure from the
others (i.e. students were given signiﬁcantly less time to complete them), these were
excluded, leaving a total of 20 exercises. Each of the remaining exercises were then
considered as a time period. The resulting behavioural clusters are given in Table 2
and the number of students in each of the ﬁnal clusters at each time period is shown
in Figure 2. Note that only data from completing students (N=658) was used to
minimise the amount of missing data.

From Figure 2 and Table 2, the most important diﬀerence in behaviour between
the beginning and end of the course was the number of autosaves, which increased
over time. In particular, Figure 2 shows that most students began in C11 (with ≤ 9
autosaves) and ended in C2 (with > 9 autosaves). Since these autosaves were triggered
when a student paused for 10 or more seconds, this could indicate increased diﬃculty
(if the students were pausing to read notes or think) or increased disengagement (if
they were frequently stopping to do other activities).

Interestingly, even the students who had a smaller number of autosaves changed in
behaviour over time, with an increasing proportion completing the exercises closer to
the deadline as the course progressed. This can be seen by the increasing proportion
of students in C12 compared to C11 over time. For the importance of this change to

6

Figure 2: The distribution of ﬁnal clusters over time when f1 is applied to data from
all students who completed the course. Most students begin in C11, but transition to
other clusters over time.

be apparent, note that passing these exercises within 7 days of the deadline actually
indicates that a student is behind schedule. This is because each week of exercises is
intended to take one week, but the deadlines for the ﬁrst four weeks allow two weeks.
If students were falling behind over time, this may suggest that the course content
was too dense, and perhaps reducing the amount of content or spreading the course
over a larger time period could be beneﬁcial for students.

Furthermore, the distribution of clusters does not change smoothly over time. In
particular, the plot lines are jagged, indicating that student behaviour varies a lot even
between adjacent exercises. This is particularly interesting considering the features
the clusters are deﬁned by. For example, the fact that the number of autosaves
varies a lot between adjacent exercises indicates that some exercises may be more
interesting or diﬃcult than similar exercises. For instance, the number of students in
C11 (where there are ≤ 9 autosaves) drops by almost 100 from Exercise 3 to Exercise
4, then increases again at Exercise 5, even though all three exercises involve if-else
statements. This could indicate that Exercise 4 is more diﬃcult or less interesting
than the others, since students pause more here (either because they are thinking or
doing something else).

Another interesting observation is that there are three general and overarching
changes to the cluster distributions over the course. In particular, from around Ex-
ercises 1 to 7, C11 is most dominant. Upon inspection, these exercises are primarily
revision exercises (e.g. printing, variables and if-statements). After this period, there
is an immediate shift in cluster distributions, with C11 and C2 becoming similar in
size, as students begin to learn about string slicing and loops. This general change
suggests that students may ﬁnd these topics more challenging than the previous ones.
After Exercise 13, C2 becomes dominant and C12 overtakes C11 in size as students
learn about list operations, dictionaries and ﬁles. Since these general changes in stu-
dent behaviour seem to occur as the topics become increasingly complex, perhaps the
course could be improved by condensing the large revision period and expanding the

7

1234567891011121314151617181920Exercise Number0100200300400500600Number of StudentsC11C12C2Table 3: Clusters formed by applying f2 to data from completing beginner students
(N = 635) with parameter x = 29 and minimum size threshold of 660 (i.e. an average
of 20 students per time period). The clusters are deﬁned by the number of autosaves
and the time between a student’s ﬁrst failure and completion of the exercise.

Level 1

autosaves ≤ 8

autosaves > 8

Level 2
time from ﬁrst fail to completion ≤ 48 secs, or no
fails
time from ﬁrst fail to completion > 48 secs

Label
C11

C12
C2

other topics to allow for a more gradual diﬃculty change.

In summary, even by using DETECT with a simple objective function, f1, and a
highly general set of features, distinct and interpretable clusters can be found that
coherently represent changes in student behaviour over time. By observing how the
distributions of these clusters change at diﬀerent scales (i.e. over the whole course, over
groups of exercises or between individual exercises), important insights into student
behaviour can be easily gained, and then used for purposes such as informing course
development.

3.3 Example 2: Using f2 to Analyse Behaviour Where Many

Students Quit

Another topic of interest when analysing a course is the exercises that students have
diﬃculty completing. In particular, if students attempt an exercise but cannot com-
plete it, this can discourage them from continuing and lead to increased disengage-
ment. This is particularly concerning in a beginner course, where students may not yet
have conﬁdence and could be dissuaded from pursuing further study in the area. This
section provides an example of how DETECT could be used with objective function
f2 to explore such issues.

During the beginner course, 761 students attempted but could not complete Ex-
ercise 29 - the highest out of any exercise during the ﬁrst four weeks. To understand
how student behaviour diﬀered during this exercise compared to others, we applied
DETECT to the data using f2 (setting x = 29), which identiﬁed clusters that distin-
guished this exercise from adjacent ones. The clusters formed and their distributions
over time are shown in Table 3 and Figure 3 respectively.

By comparing the cluster distribution at Exercise 29 to the adjacent exercises in
Figure 3, three general diﬀerences can be observed. Firstly, the proportion of students
in C2 (> 8 autosaves) is much higher for Exercise 29, indicating that students paused
more often.
In addition, the proportion of students in C11 is much lower. Since
this cluster describes behaviour where students quickly solve the task (i.e. with few
pauses, and either no failed submissions or a short time from their ﬁrst fail to passing),
a decrease in its frequency suggests this exercise is especially challenging compared to
the adjacent tasks. The slight increase in the frequency of C12 (where the time from
ﬁrst failing to passing is > 48 secs) supports this, suggesting students take longer to

8

Figure 3: The distribution of ﬁnal clusters over time when f2 is applied to data
from all students who completed the course using Exercise 29 (marked in grey) as a
parameter.

Figure 4: The number of students who attempt but do not complete each of the
beginner exercises from the ﬁrst four weeks. Exercise 29, used for clustering, is marked
in grey.

correct their work after failing.

Interestingly, the pattern of C2 sharply increasing and C11 sharply decreasing is
not limited to Exercise 29. For example, this change also occurs at Exercises 17 and
20. From Figure 4, which shows the number of students who unsuccessfully attempted
each exercise, Exercises 17 and 20 also appear to have resulted in a large number of
students giving up, especially relative to the adjacent exercises. Since information
about these exercises was not used in the clustering, this is a strong indication that
the cluster changes are not simply noise, but meaningful behaviour associated with
times when students give up.2

2Indeed, regression analysis ﬁnds that the correlation between the percentage of students in C2
and the number of students unsuccessfully attempting each exercise is statistically signiﬁcant with a
p-value of 0.008.

9

13579111315171921232527293133Exercise Number0100200300400500600Number of StudentsC11C12C213579111315171921232527293133Exercise Number0200400600800Number of StudentsSince the clusters are distinguished by the number of autosaves and also the time
between a student’s ﬁrst fail and completion, one potential use of this information
could be to improve interventions. For example, additional feedback or support mes-
sages could be triggered if a student pauses too many times or is unsuccessful in
correcting their work for too long after their ﬁrst fail. In addition, since students al-
ready receive automated feedback after failing an exercise, perhaps longer correction
times could indicate that this feedback is unclear and could be revised. Finally, per-
haps the information could be a useful tool when testing future courses. For example,
senior students or a teacher could test-run a course, and the relative diﬀerences in the
number of autosaves or time taken after failing could be used to highlight potential
issues in advance.

In summary, this example has demonstrated how DETECT can be used to ﬁnd
diﬀerent kinds of trends in educational data by changing the objective function. This
customisable feature allows for great ﬂexibility so that DETECT can be used for a
range of interesting purposes.

4 Conclusion

This paper has presented a novel hierarchical clustering algorithm, DETECT, for
identifying behavioural trends in temporal educational data. In contrast to current
clustering approaches, DETECT incorporates temporal information into its objective
function to prioritise the detection of behavioural trends. It can be applied to a wide
range of educational datasets, produces easily interpretable results and is easy to ap-
ply, since the input features do not need to be independent. Two examples of objective
functions have been provided, but these can be customised to identify diﬀerent trends
with few constraints (e.g. the functions do not need to be diﬀerentiable).

Through a case study, this paper has shown how DETECT can be used to identify
interesting behavioural trends in educational data, even when the features are sim-
ple and not domain-speciﬁc. In particular, it can detect general changes in student
behaviour over time or highlight behaviours characterising exercises where students
give up. Such information is invaluable to teachers, course designers and researchers,
who can use it to understand student behaviour, stimulate further investigation and
ultimately improve educational outcomes.

In future, we hope to further develop DETECT by considering a greater range of
objective functions and stopping conditions, and exploring the impact of additional
domain-speciﬁc features and missing data on trend detection. In addition, it would
be interesting to consider how DETECT could be used in conjunction with other
techniques to, for example, analyse individual student trajectories. Ultimately, in a
time when educational data are becoming increasingly abundant, this work aims to
contribute to better-understanding student behaviour in order to improve educational
outcomes.

10

References

[1] Seth Adjei, Korinn Ostrow, Erik Erickson, and Neil T Heﬀernan. Clustering
students in assistments: Exploring system-and school-level traits to advance per-
sonalization. In The 10th International Conference on Educational Data Mining,
pages 340–341, 2017.

[2] Saeed Aghabozorgi, Ali Seyed Shirkhorshidi, and Teh Ying Wah. Time-series

clustering–a decade review. Information Systems, 53:16–38, 2015.

[3] Umi Farhana Alias, Nor Bahiah Ahmad, and Shafaatunnur Hasan. Mining of
e-learning behavior using som clustering. In 2017 6th ICT International Student
Project Conference (ICT-ISPC), pages 1–4. IEEE, 2017.

[4] Beatriz Barros and M Felisa Verdejo. Analysing student interaction processes in
order to improve collaboration. the degree approach. International Journal of
Artiﬁcial Intelligence in Education, 11(3):221–241, 2000.

[5] Ashish Dutt, Saeed Aghabozrgi, Maizatul Akmal Binti Ismail, and Hamidreza
Mahroeian. Clustering algorithms applied in educational data mining. Interna-
tional Journal of Information and Electronics Engineering, 5(2):112–116, 2015.

[6] Samantha Finkelstein, Evelyn Yarzebinski, Callie Vaughn, Amy Ogan, and Jus-
tine Cassell. The eﬀects of culturally congruent educational technologies on
In International Conference on Artiﬁcial Intelligence in
student achievement.
Education, pages 493–502. Springer, 2013.

[7] David Joyner, Ryan Arrison, Mehnaz Ruksana, Evi Salguero, Zida Wang, Ben
Wellington, and Kevin Yin. From clusters to content: Using code clustering for
course improvement. In Proceedings of the 50th ACM Technical Symposium on
Computer Science Education, pages 780–786, 2019.

[8] Hieke Keuning, Johan Jeuring, and Bastiaan Heeren. A systematic literature re-
view of automated feedback generation for programming exercises. ACM Trans-
actions on Computing Education (TOCE), 19(1):1–43, 2018.

[9] Anupam Makhija, Deborah Richards, Jesse de Haan, Frank Dignum, and
Michael J Jacobson. The inﬂuence of gender, personality, cognitive and aﬀective
student engagement on academic engagement in educational virtual worlds. In
International Conference on Artiﬁcial Intelligence in Education, pages 297–310.
Springer, 2018.

[10] Jessica McBroom, Bryn Jeﬀries, Irena Koprinska, and Kalina Yacef. Mining
behaviours of students in autograding submission system logs. In The 9th Inter-
national Conference on Educational Data Mining, pages 159–166. International
Educational Data Mining Society, 2016.

[11] Jessica Mcbroom, Irena Koprinska, and Kalina Yacef. How does student be-
haviour change approaching dropout? A study of gender and school year diﬀer-
ences. In the 13th International Conference on Educational Data Mining (Up-
coming).

11

[12] Jessica McBroom, Irena Koprinska, and Kalina Yacef. A survey of auto-
arXiv preprint

mated programming hint generation–the hints framework.
arXiv:1908.11566, 2019.

[13] Jessica McBroom, Irena Koprinska, and Kalina Yacef. Understanding gender
diﬀerences to improve equity in computer programming education. In Proceedings
of the Twenty-Second Australasian Computing Education Conference, pages 185–
194, 2020.

[14] Jessica McBroom, Kalina Yacef, Irena Koprinska, and James R Curran. A data-
driven method for helping teachers improve feedback in computer programming
automated tutors. In International Conference on Artiﬁcial Intelligence in Edu-
cation, pages 324–337. Springer, 2018.

[15] Ewa M(cid:32)lynarska, Derek Greene, and Padraig Cunningham. Time series clustering
of moodle activity data. In Proceedings of the 24th Irish Conference on Artiﬁcial
Intelligence and Cognitive Science (AICS’16), University College Dublin, Dublin,
Ireland, 20-21 September 2016, pages 104–115.

[16] Benjamin Paassen, Bassam Mokbel, and Barbara Hammer. Adaptive structure
metrics for automated feedback provision in intelligent tutoring systems. Neuro-
computing, 192:3–13, 2016.

[17] Saurabh Pal. Mining educational data to reduce dropout rates of engineering
students. International Journal of Information Engineering and Electronic Busi-
ness, 4(2):1–7, 2012.

[18] Filipe D Pereira, Elaine Oliveira, Alexandra Cristea, David Fernandes, Luciano
Silva, Gene Aguiar, Ahmed Alamri, and Mohammad Alshehri. Early dropout
prediction for programming courses supported by online judges. In International
Conference on Artiﬁcial Intelligence in Education, pages 67–72. Springer, 2019.

[19] Thomas W Price, Yihuan Dong, Rui Zhi, Benjamin Paaßen, Nicholas Lytle,
Veronica Catet´e, and Tiﬀany Barnes. A comparison of the quality of data-driven
programming hint generation algorithms. International Journal of Artiﬁcial In-
telligence in Education, 29(3):368–395, 2019.

[20] S Rasoul Safavian and David Landgrebe. A survey of decision tree classiﬁer
methodology. IEEE transactions on systems, man, and cybernetics, 21(3):660–
674, 1991.

[21] Luis Talavera and Elena Gaudioso. Mining student data to characterize similar
behavior groups in unstructured collaboration spaces. In Workshop on artiﬁcial
intelligence in CSCL. 16th European conference on artiﬁcial intelligence, pages
17–23, 2004.

[22] Cui Tang, Yuanxin Ouyang, Wenge Rong, Jingshuai Zhang, and Zhang Xiong.
Time series model for predicting dropout in massive open online courses.
In
International Conference on Artiﬁcial Intelligence in Education, pages 353–357.
Springer, 2018.

12

[23] Maureen Villamor, Yancy Vance Paredes, Japheth Duane Samaco, Joanna Feliz
Cortez, Joshua Martinez, and Ma Mercedes Rodrigo. Assessing the collaboration
quality in the pair program tracing and debugging eye-tracking experiment. In
International Conference on Artiﬁcial Intelligence in Education, pages 574–577.
Springer, 2017.

[24] Dongkuan Xu and Yingjie Tian. A comprehensive survey of clustering algorithms.

Annals of Data Science, 2(2):165–193, 2015.

13

