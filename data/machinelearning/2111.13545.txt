µNCA: Texture Generation with Ultra-Compact Neural Cellular Automata

Alexander Mordvintsev*

Eyvind Niklasson*

Google Research
{moralex, eyvind}@google.com

1
2
0
2

v
o
N
6
2

]

G
L
.
s
c
[

1
v
5
4
5
3
1
.
1
1
1
2
:
v
i
X
r
a

Abstract

We study the problem of example-based procedural tex-
ture synthesis using highly compact models. Given a sam-
ple image, we use differentiable programming to train a
generative process, parameterised by a recurrent Neural
Cellular Automata (NCA) rule. Contrary to the common
belief that neural networks should be signiﬁcantly over-
parameterised, we demonstrate that our model architecture
and training procedure allows for representing complex tex-
ture patterns using just a few hundred learned parameters,
making their expressivity comparable to hand-engineered
procedural texture generating programs. The smallest mod-
els from the proposed µNCA family scale down to 68 pa-
rameters. When using quantisation to one byte per parame-
ter, proposed models can be shrunk to a size range between
588 and 68 bytes. Implementation of a texture generator
that uses these parameters to produce images is possible
with just a few lines of GLSL1 or C code.

1. Introduction

Finding the shortest program to implement a desired be-
haviour has been of interest both in academic literature and

* Contributed equally.
1Interactive WebGL implementations of our NCA rules are available

on ShaderToy: NCA-68, NCA-588

in general computing for almost as long as general pur-
pose computing has been available to the public. Algo-
rithmic information theory deﬁnes Kolmogorov complex-
ity [17, 42] as the shortest computer program generating
a certain output. There exist parallels to the neighbour-
ing ﬁelds of compression and information theory, which
handle the more speciﬁc instance of transmitting informa-
tion using the fewest number of bits. Outside academia,
work in the demo-scene [41] has established a long tradi-
tion of encoding complex geometries and rendering algo-
rithms into extremely short programmatic code and small
compiled binaries. Likewise, the practice of code-golf - im-
plementing a speciﬁed algorithm in the shortest length of
source code - is a well recognized form of competition, with
worldwide tournaments often involving very creative uses
of language constructs and compiler features [40]. How-
ever, the interest in expressing an algorithm in the shortest
form possible is at odds with deep learning approaches du
jour, where the trend is towards ever larger models and over-
parameterisation [6, 25].

Texture synthesis is usually described as algorithmic
generation of new images that express perceptual similar-
ity to a provided texture sample. The most successful non-
parametric methods rely on copying target image patches
In re-
or pixels into the image being generated [11, 18].
cent years, methods relying on gradient-based optimiza-
tion of image pixels to match target image statistics in a

1

 
 
 
 
 
 
pre-trained neural network have become common [12, 14].
These methods have been extended to optimising the pa-
rameters of a feed-forward neural network acting as a gen-
erator, transforming a noise vector into images similar to
the target texture [20, 38]. Such feed-forward models can
be seen as learned parametric procedural texture generators,
typically having 105 − 106 parameters. Such a large param-
eter space reﬂects the observation that neural network train-
ing achieves best results in the overparameterised regime
[4, 25].

In parallel, the computer graphics community has made
great progress in building tools for procedural texture gen-
eration, allowing artists and designers to creatively combine
various primitives into pipelines producing high ﬁdelity tex-
tures and materials. Procedural materials have a number of
attractive properties, such as efﬁcient sampling and com-
pactness of the underlying representations. The expres-
siveness of algorithmic texture generators is hard to match:
sometimes it takes an experienced programmer just a few
hundred characters of OpenGL Shading Language (GLSL)
code to create a complex world on a computer screen [9,10]

2. Related Work

2.1. Texture Synthesis

There exist a few works that attempt to automatically de-
sign compact image generators given an image sample. A
notable example is work [31] by Craig Reynolds, who used
genetic algorithms (based on the work by Karl Sims [34])
driven by human evaluators to produce texture patches that
would be unnoticeable on a given background. Some suc-
cess was also achieved with describing a texture by a rela-
tively small number of statistical parameters (e.g. [30] uses
710), and synthesising images that match these parameters.
Later these ideas were combined with neural network based
features to differentiably tune parameters of predeﬁned pro-
cedural texture generators to match the provided texture
samples [32].

2.2. Neural Cellular Automata

Cellular Automata (CA) have been long known to pos-
sess the capacity to act as powerful texture generators
[37, 43]. There is even evidence for cellular automata-
like processes generating the skin textures of certain ani-
mals [22].

Recent work [26] demonstrated a method to learn
texture-generating CA given individual image samples. In
this method, the CA update rule is parameterised with a
comparatively small neural network. The output of the
CA is fed into a pre-trained discriminative network. The
rules are then differentiably optimised so as to induce fea-
ture statistics in certain layers in the discriminative network
matching the target statistics of the desired sample image.

This model was termed Texture Neural Cellular Automata
(TNCA).

In this work we extend the TNCA line of research
through reduction - we study the limits of expressivity of
TNCA as they are made more compact. We demonstrate
that TNCA having just a few hundred learned parameters
(our smallest model has just 68), instead of several thou-
sands, can be trained to generate a wide range of textures.
The parameters of the NCA update rule can be seen as a
very compact texture representation, and training such mod-
els is a form of texture compression.

2.3. Optimal Transport

Optimal transport describes the task of transforming
one distribution to another, while minimising the so-called
”earth movers distance”. Optimal transport has its roots in
the problem setting of ﬁnding the optimal redistribution of
troops from their current locations to front-line locations,
but has since been generalised to transporting a given distri-
bution of points in any space to another distribution under
an arbitrary transport cost-matrix.

Traditionally the problem has been computationally
complex, O(N 4) in the number of points. However, re-
cent work [8] by Cuturi et. al have derived a more efﬁcient,
and differentiable, algorithm based on the Sinkhorn algo-
rithm, making feasible a whole new set of applications for
optimal transport theory [29]. The problem setting of com-
puting divergence between two distributions, and differenti-
ating through such a divergence appears in many sub-ﬁelds
of machine learning [27] [36].

3. Training Ultra-Compact Neural CAs

3.1. Ultra-Compact Neural CA models (µNCA)

Our texture generator model is inspired by the Neural
CA model from [24,26]. It operates on a regular grid where
each cell is represented using a vector of scalar values, the
ﬁrst 3 of which are interpreted as the R, G and B color chan-
nels, respectively. Similarly, we use differentiable optimiza-
tion to ﬁnd a CA rule whose repeated application in each
cell on the grid produces the desired texture. The inputs to
the update rule on a given cell are only the state of the cell
itself and information about its immediate neighbourhood,
obtained by convolving a set of ﬁxed ﬁlters over the states of
surrounding cells. At ﬁrst glance, this approach may seem
contrived or artiﬁcially constrained, but it enables surprising
and noteworthy learned procedural behaviour. The models
used in the works [24, 26] are already relatively small in
size as compared to contemporary work in deep learning.
Comprised of just 5856 parameters they are capable of pro-
ducing a large variety of textures. Authors also demonstrate
these models to be tolerant to 8-bit quantization of weights
and activations.

2

We propose a modiﬁed version of the texture NCA
model from [26], which we term µNCA. This model can
be summarized in a following set of equations:

s = [s0 = R, s1 = G, s2 = B, s3, ..., sC−1]
K = [Klap · · ·
(cid:124) (cid:123)(cid:122) (cid:125)
×Nlap

, Ky · · ·
(cid:124) (cid:123)(cid:122) (cid:125)
×Ny

, Kx · · ·
(cid:124) (cid:123)(cid:122) (cid:125)
×Nx

]

p = concat(s, [Ki ∗ si]i∈0..C−1)
y = concat(p, abs(p))
snext = s + yWC·4×C + b

(1)

(2)

(3)

(4)

(5)

Like TNCA, our µNCA operates on a uniform grid,
is represented by a C-
where the state of each cell
dimentional vector s, and its ﬁrst three components are vis-
ible as RGB colors (Eq. (1)), p represents the perception
vector and snext is the updated cell state after one itera-
tion. The following modiﬁcations were made to compress
the model, making its memory footprint smaller and evalu-
ation more computationally efﬁcient:

• The original texture NCA computed a ”perception
vector” p by applying a set of convolutional ﬁlters
(Klap, Kx, Ky) to each channel of the cell state. We
use only a single ﬁlter per channel (repeating ﬁlters
to cover all channels, see table 1), thus reducing the
dimensionality of p to be the same as the number of
channels. We also concatenate the ﬁltered values with
s so that cells continue to have full access to their own
state (Eqs. (2) and (3)).

2


1
2 −12

1

2
Klap


1
2

1





−1
−2
−1





1
2
1

0
0
0
Kx









−1 −2 −1
0
0
0
2
1
1
Ky

• We introduce non-linearity into the model by expand-
ing the perception vector, concatenating it with a vec-
tor consisting of the corresponding absolute values of
p (Eq. (4)). This can be seen as variant of PReLU
non-linearity [13], where both slopes for negative and
positive input values can be learned 2.

• We remove the latent 1x1 convolution layer from the
model. The cell state updates are instead directly com-
puted with a single matrix multiplication (in practice
implemented as 1x1 convolution). All learned param-
eters are represented by the weight matrix W and the
bias vector b. Total number of parameters equals to
4 · C 2 + C, where C = Nlap + Nx + Ny. Varying
N{lap,x,y} we deﬁne a family NCA of models of dif-
ferent expressive power. Please refer to the Tab. 1 for
the speciﬁc values used in our experiments.

2for instance, consider ReLU, which can be represented using the linear

combination: relu(x) = (x + |x|)/2

3

• Texture NCAs as described in [26] use stochastic up-
dates to introduce randomness into the system as well
as to break symmetry between cells. In this work we
removed stochastic updates from the CA rule for sim-
plicity. In order to break the symmetry, and to intro-
duce stochasticity in the procedural generation, we ini-
tialize the grid with uniformly distributed random val-
ues in [−0.5, 0.5].

3.2. Optimal Transport as a Differentiable Texture

Similarity Function

In order to use differentiable optimisation to synthesize
textures or to train texture generators, one must deﬁne an
objective function that would measure the perceptual prox-
imity of the current solution to the target example. The
naive pixel-wise difference is not appropriate because by
the nature of the problem we are looking to produce novel
samples of the target texture, not pixel-perfect copies. Over
time, a number of differentiable texture similarity functions
have been proposed in literature.

Most contemporary texture generation methods rely on
pre-trained convolutional neural networks to extract target
image features [12] (sometimes referred to as ”statistics”),
and try to match the distribution of these features between
the image under generation and the target texture. A promi-
nent example is the work [12], which used an ImageNet-
pretrained VGG feature extractor [33] and the difference
between the feature channel covariance matrices as the loss.
Subsequent works have explored other approaches to com-
puting the discrepancy between feature distributions, more
recently based on optimal transportation theory [14, 15].

There are signiﬁcant drawbacks to VGG-based feature
extractors that constrain their applicability. Models using
them as feature extractors are unlikely to work well on out-
of-distribution data (with regards to the distribution of data
the feature extractor was trained on). Similarly, models are
primarily constrained to RGB images, preventing the syn-
thesis of textures with additional data encoded in the other
channels, such as surface normals, roughness, or opacity.
Pre-deep learning methods, such as using texture image
patches partially offset many of these issues and may be
used in the place of features extracted from deep neural net-
works [15, 39].

In this work we propose a differentiable texture simi-
larity function we term OTT-Loss, combining ideas from
[15, 39]. Our proposed method represents a target texture
as a collection of image patches extracted from the image
at various scales. A procedure that allows matching the
distribution of patches between a source and a target in a
differentiable fashion allows for any differentiable model to
be used a generator. In practice, we use recent advances in
optimal transport theory - speciﬁcally the sinkhorn iteration
algorithm [8] to calculate the cost of optimal re-allocation

of patches between the output of the µNCA and the distri-
bution of patches from the chosen target texture.

In this work we extract all possible K × K patches from
the levels of the Gaussian pyramid, computed for the tar-
get image. We observed that local contrast enhancement
is beneﬁcial for the µNCA training convergence speed. To
achieve it we do the sharpening of the pyramid levels before
extracting the patches using the formula:

sharpen(I) = I + 2(I − G5×5 ∗ I)

where G5×5 is a Gaussian blur kernel.

We ﬂatten each patch into a one-dimensional vector, ef-
fectively placing it in a feature space of all possible K sized
patches. Our feature extraction procedure is shown in Algo-
rithm 1. We then match the induced distributions of patches
in this space to patches sampled from the chosen target tex-
ture. This must be done separately at each pyramid level:

textureLoss(F, Ftarget) =

(cid:88)

level

OT(cid:15)(S(F level), S(F level

target))

where OT(cid:15) denotes the optimal transport divergence, and
the function S(F ) stochastically subsamples a number of
rows (we use 2048) from a given matrix to make the OT
computation tractable.

Algorithm 1: extractF eatures function
Data: Image I, Nlevels, patch size K
Result: [F l]l∈1..Nlevels
I1 ← I;
for l = 1 to Nlevels do

Isharp ← sharpen(Il) ;
F l ← list of all K × K overlapping patches
extracted from Isharp ;
Il+1 ← downscale Il by a factor of 2

end

• Overﬂow loss, Loverf low (see 2), to keep latent chan-
nels in [−1, 1]. This stabilises training by preventing
drift in latent channels and aids in post-training quan-
tisation.

We keep other augmentations as introduced in [26], in-
cluding gradient normalisation prior to taking the optimiza-
tion step, and using a sample pool to achieve longer-time
stabilisation.

Algorithm 2: Texture NCA Training
Data: Target texture image Itarget
Result: ﬁnal CA parameters θ
θ ← init CA parameters ;
opt ← init Adam optimizer;
pool ← init state pool with Npool seed states;
Ftarget ← extractF eatures(Itarget) ;
for step = 1 to Ntrain do

x ← sample Nbatch states from pool ;
if step mod seed rate = 0 then
x[0] ← random seed state

end
for i = 1 to Nsteps do

x = ca(x) ;
x = norm grad(x)

end
Igen ← extract RGB-channels from x;
Fgen ← extractF eatures(Igen) ;
Ltexture ← textureLoss(Fgen, Ftarget) ;
Loverf low ← (cid:80) |x − clip[−1,1](x)| ;
L = Ltexture + Loverf low ;
∇Lθ ← use back-propagation through CA steps
to computes the gradient of L;
Gnormed ← ∇Lθ/||∇Lθ||2 ;
θ ← opt.update(Gnormed) ;
place ﬁnal grid states x back to the pool;

3.3. Training procedure

Our training procedure (Algorithm 2) is based on the one

proposed in Mordvintsev et. al with a few modiﬁcations:

• On back-propagation we apply gradient normalization
after every CA step. This may be seen as a variant
gradient clipping [28], a standard practice to prevent
exploding or vanishing gradients in RNN training.

• We inject seed states into training batches every
seed rate training steps, rather than at every step. This
allows CA to witness longer CA evolution sequences
during training, which results in better long-term pat-
tern quality and stability. The expected number of CA
steps seen during training becomes Nbatch ∗ Nsteps ∗
seed rate.

end

4. Experiments

4.1. Datasets

We demonstrate qualitative performance on an assort-
ment of textures sourced from several datasets. Most of the
datasets were released in tandem with texture-generation
methods, and have over time been re-used and mixed across
different publications, making proper attribution difﬁcult.
The textures we use in this work are curated from the fol-
lowing datasets:

• Images collected by Portilla and Simoncelli for their
work on texture synthesis [30].
In turn, these in-
clude ’personal photographs’ scanned by the authors,

4

as well as the MIT Vision Texture database [3] and
scanned images from Textures: A Photographic Al-
bum for Artists and Designers [5].

TNCA
VGG-loss
5856 params

µNCA
VGG-loss
588 params

Ours
OTT-loss
588 params

Target

• Images release together with Appearance-space tex-

ture synthesis [19].

• The Describable Textures Dataset [7].

4.2. Results

The opening ﬁgure shows curated samples of the µNCA
trained on various textures, some natural and some artiﬁcial,
for different model sizes of the compact NCA. The number
of parameters and ﬁlter conﬁguration for each model size
is listed in 1. Notice that the largest model is 588 parame-
ters, or about 2,352 bytes in size, while the smallest model
is just 272 bytes. Thorough experimentation with quantisa-
tion of the models to just one byte per parameter showed no
evidence of degradation in quality. We release a reference
shader-based implementation of µNCA inference which op-
erates on the quantised models. The quantisation brings the
size of models down to between 588 bytes and 68 bytes. For
complex textures involving lots of ﬁne detail without much
regularity, the model degrades gracefully with size, in most
cases continuing to capture macroscopic detail (colour dis-
tributions and larger features). Simpler and more regular
patterns, such as the zebra stripes, are captured equally well
by both the small and large models.

4.2.1 Quantitative evaluation

Quantitative comparison of a synthesized texture with the
intended target is an ongoing area of research [21], in no
small part due to the open-ended deﬁnition of the texture
synthesis task itself. Approaches from the family of Fr´echet
inception distances (”FID score”), commonly used in other
image-generation settings, are not applicable, in part be-
cause the objective of µNCA is not to produce a competitive
natural image generator, and also because most baselines
for texture-synthesis, such as TNCA, utilize Gatys et. al ap-
proach of matching feature statistics during training. Thus,
FID would make the optimization objective and evaluation
criteria one and the same (albeit with a different convolu-
tional architecture). Other works [15, 19, 30] in the ﬁeld
likewise present scant quantitative evaluation, and instead
focus on qualitative samples and analysis.

4.3. Ablations

We qualitatively study the ablation of two elements of
our proposed training procedure - gradient normalisation
and overﬂow loss. Samples of two patterns under these ab-
lations can be seen in 3. In addition to the perceptual degra-
dation in quality, the training process exhibited instabilities
under both ablations, with frequent explosions in state space

Figure 1. Comparison with original TNCA loss and architecture
(VGG-loss and 5856 parameters) and with µNCA trained using
VGG-loss (588 parameters). A persistent problem with matching
image statistics in a pre-trained convolutional network is drift in
the colour channels and inaccurate colour reproduction, as seen
here. In TNCA, it appears this problem is somewhat alleviated by
the relatively higher parameter count - applying VGG-loss to the
588 parameter µNCA degrades the quality of the produced pat-
terns in most cases. OTT-loss is unbiased in colour space, as each
primary colour channel is simply a dimension in the patch feature
space and treated equally under optimal transport divergence. We
note that even spatially complex patterns, such as the weave, are
still reproduced well by the µNCA, despite having far fewer pa-
rameters than TNCA.

or in gradient space. Such instabilities are exacerbated by
use of the training sample pool, where corruption of the
states involved in a one training iteration may be re-sampled
at later training iterations.

4.4. Optimal Transport Loss Control

The use of patches and an Optimal Transport based loss
affords more ﬁne grained control over the texture similar-
ity metric than traditional feature-statistic matching based
methods. The target texture is encoded as a distribution of
image patches; tuning the size and nature of these patches
provides intuitive ”control knobs” for the training process.
For instance, the effective size of the largest patch at the
highest level of the pyramid (K ∗ 2Nlevels−1) approximately
determines the size of the largest features in the target image
that will synthesized in the source image under generation.
Note that this is only an approximate limit; the overlap be-
tween patches can still result in larger visual features being
correctly synthesized, but more often than not patches will
be recombined in a locally-consistent, globally-inconsistent

5

)
a
(

)
b
(

)
c
(

)
d
(

Ours
588 params

Target

(Nlap, Nx, Ny)

C Nparams

(2, 1, 1)
(2, 2, 2)
(4, 2, 2)
(4, 4, 4)

4
6
8
12

68
150
264
588

Table 1. µNCA ﬁlter conﬁgurations used in the experiments.

Ours

w/o
gradientNorm

w/o
gradientNorm
overﬂowLoss

Figure 3. Qualitative ablation results for training augmentations
introduced. In addition to degradation in the quality of the con-
verged pattern, lack of these augmentations often leads to explod-
ing states, or exploding gradients.

4.5. µNCA Dynamics

The inherent nature of µNCA as iterative models pre-
vents static visualisations of their states from fully captur-
ing their behaviour. We suggest readers interact with sample
µNCA models, implemented as shader programs, available
here: NCA-68, NCA-588. Additionally, we provide code to
train µNCA in the supplementary materials.

5. Discussion

5.1. Procedural Generation

We hypothesize that the extreme constraint on the num-
ber of parameters prevents overﬁtting or even learning any
signiﬁcant section of the bitmap verbatim (indeed, the tar-
get sample image bitmaps themselves are approximately
128 ∗ 128 ∗ 3 = 49152 bytes, as compared to 68 bytes in our
smallest model). Similarly, see ﬁgure 5 to see a compari-
son to a traditional image encoding algorithm, JPEG, with
the lowest quality setting at approximately 500 bytes ver-
sus 264 bytes in our smallest quantised model, albeit with
some overhead for the JPEG ﬁle format. Other formats,
such as JPEG-XL (JXL), allow for far more compact rep-
resentations, and as a result there is a ﬁeld of ”JXL art” [1]
focusing on writing minimal valid JXL ﬁles on the order of
tens of bytes. At ﬁrst glance, these hand-engineered JXL

Figure 2. Comparison of µNCA with different well-known tex-
ture synthesis approaches. In order, a) (710 params) Portilla and
Simoncelli’s [30] approach based on matching hand-deﬁned pixel
statistics, b) (parameter-free) Hugues Hoppe’s [19] work on con-
structing and utilizing an ”appearance space” to aid in the selec-
tion of patches from the target image, c) (approx. 65k params)
Ulyanov et al’s work [38] on using pre-trained convolutional net-
works as feature-extractors to differentiably train a generator net-
work, d) (approx. 65k params) Houdard et al’s work [16] on using
optimal transport to match feature statistics in images, in order to
backpropagate into a generator network (as in (c)). We note they
describe their generator network, TexNet, as ”small”, at 65k pa-
rameters.

manner. See ﬁgure 4 to see the effects of different patch
sizes.

6

target

K = 7, Nlevels = 5

K = 5, Nlevels = 5

K = 3, Nlevels = 5

K = 3, Nlevels = 3

Figure 4. Pixel-optimisation using OTT-Loss using Adam with
1e − 3 learning rate for 200 iterations. Note that µNCA are unable
to learn this target image; we intend this ﬁgure to demonstrate
the ”control knobs” made available by OTT-Loss. Larger patches
and more levels result in consistent macroscopic detail, such as
the structure of the body, while smaller patches and fewer levels
produce an image with consistent microscopic detail but lacking
all larger-scale coherence, resulting in an abundance of lizard eyes
and other immediately local features, but no discernible structure
to their composition.

ﬁles appear to match the expressivity of µNCA. However,
the underlying model of µNCA could conceivable be imple-
mented in at most hundreds of lines of standard C code [23],
while the JPEG-XL reference library [2] is composed of
hundreds of thousands of lines.

We posit that the learned parameters represent a near-
minimal procedural program that satisﬁes the desired be-
haviour (to stochastically generate a perceptually similar
texture to a given sample image), and that the model must
learn to exploit any regularities in the pattern to achieve this
task. Hand-engineered procedural texture generators are of-
ten written in an extremely compact fashion, but still rely on
extensive graphics libraries. We emphasize that most of the
generative process, or the procedural program, for a learned
texture is effectively entirely encoded in the parameters; the
underlying architecture used for µNCA is not complex.

7

JPEG Q = 4%, 500 bytes

µNCA, 264 parameters

Figure 5. Comparison of a JPEG encoded image and the output
of a µNCA rule with fewer than half the number of bytes. µNCA
make use of regularity in patterns to learn procedural generation.

t

t + 10

t + 20

t + 30

Figure 6. Four samples, each 10 steps apart, after pattern conver-
gence, from a µNCA trained on a polka dot pattern. Occasionally,
the µNCA converges to the remarkable procedural solution of syn-
thesizing a number of dots, then having each dot oscillate through
the target colours, while ensuring they are not in sync in order to
produce the desired target distribution of colours among the dots
at any given time-step. Closer inspection reveals the dots oscillate
across colours in the same order.

We ﬁnd further evidence for this hypothesis when qual-
itatively observing the model’s behaviour during iteration.
When the model is trained on a ”polka dot” pattern 6, which
features regularly spaced dots of different colours against a
white background, we observe that on occasion the µNCA
with 8 channels (280 parameters) will converge to a solu-
tion where the coloured dots oscillate in colour, as opposed
to simply generating an image with several multi-coloured
dots spatially distributed across the image in different, ﬁxed,
colours. We believe this behaviour is indicative that the so-
lution space of µNCA is the space of possible procedural
programs, in contrast to some parameterisation of differ-
ent statistics of the image. Designing a program where the
output is a polka dot pattern in different colours, with the
constraint that the program’s output must be a valid ”polka
dot” pattern at any given step after some period of conver-
gence, certainly admits the non-intuitive solution of produc-
ing bubbles of oscillating colour.

5.2. Limitations

The method outlined here, like the originally proposed
NCA, learns a complete generative model per target texture.
This is comparable to single-image CPPNs and other work

that uses neural networks to encode single data points [35]
[38]. It is conceivable that knowledge could be shared be-
tween different models operating on the same space of nat-
ural images, and thus a current limitation which may need
to be explored further is the sharing of parameters between
models. However, the focus of this work on ultra-compact
models narrowed the scope of experiments to those involv-
ing single images per model.

The texture samples produced by µNCA and the under-
lying work TNCA do not produce images indistinguishable
from images from the natural world. The models perform
signiﬁcantly better on regular patterns (especially artiﬁcally
produces ones, such as grids or other repeated features).
The distinguishing features of µNCA are their extremely
compact nature and robustness inherent to their architecture
(embarrasingly parallel and highly fault-tolerant).

References

[1] JPEG XL art. https://sneyers.info/jxl/art/.

Accessed: 2021-11-17. 6

[2] libjxl: JPEG XL image format reference implementation. 7
[3] MIT vision texture database. 5
[4] Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Man-
dal. Reconciling modern machine-learning practice and the
classical bias-variance trade-off. Proc. Natl. Acad. Sci. U. S.
A., 116(32):15849–15854, Aug. 2019. 2

[5] Phil Brodatz. Textures: A Photographic Album for Artists
and Designers. Peter Smith Publisher, Incorporated, June
1981. 5

[6] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-
hini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom
Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler,
Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish, Alec Rad-
ford, Ilya Sutskever, and Dario Amodei. Language models
are Few-Shot learners. May 2020. 1

[7] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A.
Vedaldi. Describing textures in the wild. In Proceedings of
the IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR), 2014. 5

[8] Marco Cuturi. Sinkhorn distances: Lightspeed computation

of optimal transportation distances. June 2013. 2, 3

[9] Junyu Dong, Jun Liu, Kang Yao, Mike Chantler, Lin Qi, Hui
Yu, and Muwei Jian. Survey of procedural methods for Two-
Dimensional texture generation. Sensors, 20(4), Feb. 2020.
2

[10] D Ebert, F K Musgrave, D Peachey, K Perlin, and Steven
Worley. Texturing and modeling: a procedural approach.
Choice, 32(09):32–5129–32–5129, May 1995. 2

[11] Alexei A Efros and Thomas K Leung. Texture synthesis by

non-parametric sampling. 1

[12] Leon A Gatys, Alexander S Ecker, and Matthias Bethge.
Texture synthesis using convolutional neural networks. May
2015. 2, 3

[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Delving deep into rectiﬁers: Surpassing Human-Level per-
formance on ImageNet classiﬁcation. Feb. 2015. 3

[14] Heitz, Vanhoey, Chambon, and Belcour. A sliced wasserstein
loss for neural texture synthesis. In 2021 IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
volume 0, pages 9407–9415, June 2021. 2, 3

[15] Antoine Houdard, Arthur Leclaire, Nicolas Papadakis, and
Julien Rabin. A generative model for texture synthesis based
on optimal transport between feature distributions.
June
2020. 3, 5

[16] Antoine Houdard, Arthur Leclaire, Nicolas Papadakis, and
Julien Rabin. A generative model for texture synthesis based
on optimal transport between feature distributions.
June
2020. 6

[17] A N Kolmogorov. On tables of random numbers. Theor.

Comput. Sci., 207(2):387–395, Nov. 1998. 1

[18] Sylvain Lefebvre and Hugues Hoppe. Appearance-space tex-
ture synthesis. In ACM SIGGRAPH 2006 Papers on - SIG-
GRAPH ’06, New York, New York, USA, 2006. ACM Press.
1

[19] Sylvain Lefebvre and Hugues Hoppe. Appearance-space tex-
ture synthesis. ACM Trans. Graph., 25(3):541–548, July
2006. 5, 6

[20] Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu,
and Ming-Hsuan Yang. Diversiﬁed texture synthesis with
feed-forward networks. In 2017 IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR). IEEE, July
2017. 2

[21] Wen-Chieh Lin, J Hays, Chenyu Wu, Yanxi Liu, and V Kwa-
tra. Quantitative evaluation of near regular texture synthesis
algorithms. In 2006 IEEE Computer Society Conference on
Computer Vision and Pattern Recognition (CVPR’06), vol-
ume 1, pages 427–434, June 2006. 5

[22] Liana Manukyan, Sophie A Montandon, Anamarija Fofon-
jka, Stanislav Smirnov, and Michel C Milinkovitch. A living
mesoscopic cellular automaton made of skin scales. Nature,
544(7649):173–179, Apr. 2017. 2

[23] Alexander Mordvintsev. deepdream c: Minimalistic Deep-

Dream with C89. 7

[24] Alexander Mordvintsev, Eyvind Niklasson, and Ettore Ran-
dazzo. Texture generation with neural cellular automata.
May 2021. 2

[25] Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan
Yang, Boaz Barak, and Ilya Sutskever. Deep double descent:
Where bigger models and more data hurt. Dec. 2019. 1, 2

[26] Eyvind Niklasson, Alexander Mordvintsev, Ettore Ran-
dazzo, and Michael Levin. Self-Organising textures. Distill,
6(2), Feb. 2021. 2, 3, 4

[27] Derek Onken, Samy Wu Fung, Xingjian Li, and Lars
Ruthotto. OT-Flow: Fast and accurate continuous normal-
izing ﬂows via optimal transport. May 2020. 2

[28] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On
the difﬁculty of training recurrent neural networks. Nov.
2012. 4

8

[29] Gabriel Peyr´e and Marco Cuturi. Computational optimal

transport. Mar. 2018. 2

[30] Javier Portilla and Eero P Simoncelli. A parametric tex-
ture model based on joint statistics of complex wavelet co-
efﬁcients. http://www.cns.nyu.edu/pub/eero/
portilla99- reprint.pdf, 2000. Accessed: 2021-
11-11. 2, 4, 5, 6

[31] Craig Reynolds. Interactive evolution of camouﬂage. Artif.

Life, 17(2):123–136, Mar. 2011. 2

[32] Liang Shi, Beichen Li, Miloˇs Haˇsan, Kalyan Sunkavalli,
Tamy Boubekeur, Radomir Mech, and Wojciech Matusik.
Match: Differentiable material graphs for procedural mate-
rial capture. ACM Trans. Graph., 39(6):1–15, Dec. 2020. 2
[33] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for Large-Scale image recognition. Sept.
2014. 3

[34] Karl Sims. Artiﬁcial evolution for computer graphics. SIG-
GRAPH Comput. Graph., 25(4):319–328, July 1991. 2
[35] Kenneth O Stanley. Compositional pattern producing net-
works: A novel abstraction of development. Genet. Program.
Evolvable Mach., 8(2):131–162, June 2007. 8
[36] Luis Caicedo Torres, Luiz Manella Pereira,

and M
Hadi Amini. A survey on optimal transport for machine
learning: Theory and applications. June 2021. 2

[37] Greg Turk. Generating textures on arbitrary surfaces using
reaction-diffusion. In Proceedings of the 18th annual con-
ference on Computer graphics and interactive techniques -
SIGGRAPH ’91, New York, New York, USA, 1991. ACM
Press. 2

[38] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Im-
proved texture networks: Maximizing quality and diversity
In 2017
in feed-forward stylization and texture synthesis.
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR). IEEE, July 2017. 2, 6, 8

[39] Ryan Webster. Innovative non-parametric texture synthesis

via patch permutations. Jan. 2018. 3

[40] Wikipedia contributors.

https : / / en .
wikipedia.org/wiki/Code_golf, Sept. 2021. Ac-
cessed: 2021-11-16. 1

Code golf.

[41] Wikipedia contributors. Demoscene.

https: //en .
wikipedia.org/wiki/Demoscene, Nov. 2021. Ac-
cessed: 2021-11-16. 1

contributors.

[42] Wikipedia
ity.
Kolmogorov _ complexity, Nov. 2021.
2021-11-16. 1

complex-
https : / / en . wikipedia . org / wiki /
Accessed:

Kolmogorov

[43] Andrew Witkin and Michael Kass. Reaction-diffusion tex-
tures. In Proceedings of the 18th annual conference on Com-
puter graphics and interactive techniques - SIGGRAPH ’91,
New York, New York, USA, 1991. ACM Press. 2

9

