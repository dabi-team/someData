HEX and Neurodynamic Programming

Debangshu Banerjee,

1

0
2
0
2

g
u
A
1
1

]

G
L
.
s
c
[

1
v
9
5
3
6
0
.
8
0
0
2
:
v
i
X
r
a

Abstract—Hex is a complex game with a high branching factor.
For the ﬁrst time Hex is being attempted to be solved without the
use of game tree structures and associated methods of pruning.
We also are abstaining from any heuristic information about
Virtual Connections or Semi Virtual Connections which were
previously used in all previous known computer versions of the
game. The H-search algorithm which was the basis of ﬁnding
such connections and had been used with success in previous
Hex playing agents has been forgone. Instead what we use is rein-
forcement learning through self play and approximations through
neural networks to by pass the problem of high branching factor
and maintaining large tables for state-action evaluations. Our
code is based primarily on NeuroHex. The inspiration is drawn
from the recent success of AlphaGo Zero.

Keywords—Hex, Convolution Neural Networks, Recurrent Neu-
ral Networks, Reinforcement Learning, Projected Bellman Error

I.

INTRODUCTION TO THE GAME OF HEX

HEX is a two-person board game invented by Danish
mathematician Piet Hien in 1942 at Neils Bohr Institute
and independently by Nobel-Laureate John Nash in 1948 at
Princeton University [1]. Hex is a strategy game, that gives
high signiﬁcance to the players’ decision-tree style thinking
and situational awareness in determining the outcome [1]. The
board consists of hexagonal grids called cells arranged to form
a rhombus. Typically n×n boards are used with championships
being held on 11×11 boards. Players alternate placing markers
on unoccupied cells in an attempt to link the opposite sides of
their board in a continuous chain.

A. Hex and Mathematics

The current area of research related to Hex can be found in
areas of topology, graph and matroid theory, combinatorics,
game theory, computer heuristics, artiﬁcial intelligence [1].
Hex is a connection game [1] in which players try to connect
the opposite edges using their pieces by employing strategical
decisions. It is a Maker-Breaker type positional game [1] which
is described as : the Maker wins by capturing all positions
that lead to a connection, while if the game ends with all
positions claimed but the Maker not having won, it implies the
Breaker has won. This brings to the ﬁrst theorem about Hex
that Hex cannot end in a draw [1], [2]. Though John Nash is
credited for the proof [2] there is no supposting material. The
proof is credited to David Gale, American Mathematician and
Economist, University of California, Berkley [2].

Hex is a perfect information game [1], which means that
each player while making a decision is perfectly informed

D. Banerjee is with the Stochastic Systems Lab , Indian Institute of Science,

Bangalore, KA, 560012

about all events that have previously happened. The condition
of perfect information and no-draw makes Hex a determined
game which means that for all instances of the game there is
a winning strategy for one of the players [1].

This brings us to the most famous theorem about Hex, that
the ﬁrst player has the winning strategy [2]. This theorem was
proved by John Nash using a strategy-stealing argument [2].
David Gale’s proof that Hex cannot end in a draw uses a
graph theory lemma [2]. A simple graph with nodes having
degree at most 2 can be shown to be a union of disjoint
sets of either isolated nodes, simple cycles or simple paths.
By constructing a subgraph from the original Hex graph by
including only special edges, the nodes of the subgraph is
shown to have degree at most two. The lemma is then applied
to show existence of simple paths between opposite edges [2].
That Hex cannot end in a draw has been shown equivalent to
the existence of the two dimensional Brouwer ﬁxed-point by
David Gale [2]. The Brouwer ﬁxed-point theorem was key to
proving the existence of Nash Equilibrium [2].

B. HEX and Computers

With Nash’s theorem about ﬁrst person having the winning
strategy, people were interested in solving the game of Hex by
hand as well as by computers [3] .

The ﬁrst hex playing machine was constructed in 1950
by Claude Shannon and E. F. Moore [4]. It was an analog
machine. A two-dimensional potential ﬁeld was set up corre-
sponding to the playing board. With black trying to connect
the top and bottom edges, black pieces and the top and bottom
edges were given negative charge and white pieces were given
positive charge along with the two sides. The move to be made
was speciﬁed by a certain saddle point in the potential ﬁeld.
The machine performed reasonably well and won about 70
percent of the games with opening moves.

However, until recently, machines had never been able to
challenge human capability [3]. To understand the complexity
the game-tree search techniques that are
of Hex,note that
applicable in chess becomes less useful in Hex because of
the large branching factor. A standard 11 × 11 board in Hex
has on an average 100 legal moves compared to 38 for chess
[5].

Another blow to the hopes of ﬁnding good algorithms to
solve Hex came in 1976, when Shimon Even and Robert
Tarjan showed the problem of determining which player has
a winning strategy in Shannon’s Switching Game is PSPACE
complete [6] and ﬁnally in 1981 Stefan Reisch proved PSC-
PACE completeness for N × N Hex [7]. These results indicate
that there is little chance of having a polynomial time algorithm
which could ﬁnd a winning strategy.

Still there are many results about the solution of Hex which

deserve a mention :

 
 
 
 
 
 
Smaller games upto 4 × 4 have been completely solved by
hand. Piet Hein, one of the inventors, commented that interest-
ing positions start to arise from 5×5 boards. Martin Gardner, a
popular mathematics writer brieﬂy discussed winning opening
moves on board sizes of 5 × 5. In 1995, Bert Enderton solved
all 6 × 6 openings which was later veriﬁed in 2000 by Van
Rijswijck [3] the founder of Queenbee, a HEX program that
won silver at London 2000 Computer Games Olympiad [8].

In 2001 Yang solved the 7×7 board by hand for a number of
openings [9], [10]. He used a decomposition technique where
each board conﬁguration of Hex can be viewed as a sum of
local games. Each local game can be solved using a particular
strategy. By decomposing any Hex board into a sum of local
games and using particular sets of strategies on each local
game a win is ensured. Yang used 41 such local game patterns
in his 12 page case analysis.

However the major breakthrough in computer Hex came in
2000 by Anshelevich. V.V. Anshelevich, a Russian American
physicist, gave the ﬁrst Hex playing algorithm [5], [11]. The
computer program developed was called HEXY, winner of
the gold medal at the 2000 Computer Olympiad, London.
Anshelevich’s contribution was phenomenal in that he built the
foundation on which all later strong Hex playing algorithms
would be built.

Anshelevich et al used the concept of ”sub games” following
the decomposition idea introduced by Yang. They proposed an
algorithm called H-Search. In their program of HEXY an alpha
beta search is used on a game-tree with each node, represent-
ing a board conﬁguration, has its value estimated using an
evaluator function based upon the H-Search algorithm.

Though the H-Search algorithm is theoretically insufﬁcient,
the method has been used in all major Hex playing agents
with success [12], [13], [14], [15]. In 2003 Hayward et al ,
University of Alberta, Computer Hex Research Group, solved
all 49 1st place openings on 7 × 7 boards. Since then the
success of Hex playing agents has steadily increased with
machines like SIX, WOLVE, MONGOOSE, SOLVER etc,
all developed by the Computer Hex Research Group at the
University of Alberta [16]. Each of these has won medals
in the Computer Game Olympiads along with solving for all
1st place opening positions for boards up to sizes 9 × 9. The
strongest Hex playing engine till date is known as MOHEX
[17], [18], [19]. MOHEX was written by Ryan Hayward and
Phillip Henderson, Computer Hex Research Group, University
of Alberta in 2007-10. It uses Monte Carlo Tree Searches
with Upper Conﬁdence Bounds to balance exploration versus
exploitation and prunes nodes via the Virtual Connection
Method discussed along with other improvements and heuris-
tics. MOHEX has recently won the gold medal on 11 × 11
and 13 × 13 boards in the 2015 International Computer Games
Association, Computer Games Olympiad held at Netherlands.
Till date centre opening winning strategy for 10×10 board has
been shown [16]. Currently the Computer Hex Research Group
at the University of Alberta is associated with maintaining all
these engines and carrying forward their research in the game
of Hex.

In a completely different setting, David Silver et al, at
Google DeepMind in 2016 presented ALPHAGO [20] which

2

beat a human world champion, Ke Jie, three times out of
three in the ancient Chinese board game of GO. Their success
was contributed to the use of a large data set of GrandMaster
moves which were used for supervised training of a neural
network followed by self-play and reinforcement learning for
improvement. Inspired by this, Kenny Young, Ryan Hayward,
Computer Hex Research Group, University of Alberta devel-
oped Neurohex [21] , a reinforcement learning agent trained
for Hex. Since our work is highly based on this paper we will
explain this in detail.

In 2017, [22] the same team of ALPHAGO, developed
ALPHAGO ZERO, a Hex playing engine which is architec-
turally simpler than ALPHAGO. Also it is completely devoid
of any supervision and relies only upon reinforcement learning.
ALPHAGO ZERO has been reported to beat ALPHAGO 100
to 0, calling into question human strategies employed in HEX
used to train ALPHAGO.

II. OUR CONTRIBUTION
Our contribution is mostly experimental, focusing on vari-
ous Reinforcement Learning algorithms. We focus mainly on
algorithms that aim to decrease the Projected Bellman Error.
We also compare these algorithms with those which tend to
minimize the traditionally used Value Error. We focus on both
Critic Methods as well as Actor-Critic-methods to solve control
problems of Reinforcement Learning. We compare results both
with and without the use of supervised pre-training of network
weights using a database of optimal moves. We also use a
Recurrent Neural Network based architecture and compare the
same with a Convolutional Neural Network based architecture
used in Neurohex [21]. Our ﬁnal results indicate that using
a Recurrent Neural Network might not be better suited in
comparison to the existing Convolution Neural Networks when
used in the contexts of positional games like Hex [21]. We
also provide a new optimization technique that can be used in
conjunction with function approximation and Reinforcement
Learning. We hope that our optimization technique aims to
reduce the Projected Bellman Error, though theoretical analysis
remains to be done. Empirical results indicate that our method
of optimaization gives better results than traditional algorithms
like Q-learning and SARSA, which aim to decrease the Value
Error.

Our paper is organized as follows : Section III introduces
ideas from Reinforcement Learning, Section IV deal with
Experiments and Section V deals with Results. Finally Section
VI and VII discuss Conclusion and Future Work respectively.

III. REINFORCEMENT LEARNING AND NEURO-DYNAMIC
PROGRAMMING [23]
Reinforcement Learning as the name suggests, focuses on
learning through reinforcement of positive actions. Actions
are termed positive if they are beneﬁcial. Actions have con-
sequences and beneﬁcial actions are those which have better
consequences. At a particular time instance the learning agent
encounters a state of the environment. The environment is
the particular problem setting which we aim to solve. In our
case the environment is the Hex problem. Having encountered

an environmental-state the agent takes an action. Actions are
dependent on the state encountered. Not all actions are legal
in every state. The action changes the environmental state
from one to another. The transitions can be deterministic or
stochastic. The transition also results in either a reward or a
loss signal depending on the environment. For example leaning
slightly on a bicycle will cause it to turn but leaning too much
will result in a fall. Normally we can provide a very simple
algorithm which does not involve any learning : choose actions
which give the highest rewards. But what happens usually is
that we are more interested in looking for long term goals.
For example if I paddle fast on a bicycle I will not fall so I
can choose to paddle fast, but I will also get tired quickly so
I may not reach my destination! Here my goal is reaching the
destination without falling and that would mean I optimize my
pedalling.

Hence, we have an objective: to maximize the cumulative re-
ward one receives, starting from an initial state and upon taking
a sequence of actions. Most reinforcement learning algorithms
try to achieve this, i.e. assess how good a state (in our case
a board conﬁguration) is in the long term by calculating the
expected cumulative reward received by taking a sequence of
actions. For example we try to maximize the chance of winning
while having encountered a particular board-conﬁguration by
trying different action sequences. This can be easily done by
simulation and maintaining a list of all environmental states.
We choose a state and run a simulation based on a model
of the environment following a particular predeﬁned set of
actions. The model produces the state transitions and reward
signals. We add up the rewards to get a return. This return is
a sample of the value function of that state following that
particular set of actions. (If state evolution is random we
would need to take the average of the returns based on a
certain number of simulations). We might also have different
action sequences. We can calculate the value function by
following each and every action sequence to get the value of
the state corresponding to different action sequences. Finally
we can choose the one action sequence which gives the highest
return for that state. We can carry this out for every state of
the environment to identify which action sequence gives the
highest return when in a particular state.

Instead of running an entire simulation we can also use
backed up values of the states themselves to assess the values
of other states.

vπ(s) = Eπ

T
(cid:88)

(cid:2)

t=0

Rt+1|s0 = s(cid:3)

= Eπ

T −1
(cid:88)

(cid:2)

t=0

Rt+1 + vπ(sT )|s0 = s(cid:3)

= Es(cid:48)∼π(s(cid:48)|s,π(s))

(cid:2)R1 + vπ(s1)|s0 = s(cid:3)

where π is the particular action sequence a0, a1, ...aT −1. One
can initialize vπ to some value for all states and apply the
above recursive formula iteratively to converge to a solution
of the above equation (Much like a Gauss-Seidel approach to
solving a linear system of equations).

3

With this we understand the dynamic portion of the Neuro-
dynamic Programming name. To understand the ﬁrst part we
have to look from the neural network function approximation
perspective. In problems where there are many states it is
reasonably difﬁcult
to assess the value function of all of
these states mainly because of storage constraints and general
inefﬁciency. So we try to build a parameterized function with
the number of parameters much less than the number of states.
Our hope is that with a state as input to the function, we will
get vπ(s). Obviously, because of the large difference in the
number of inputs and the number of parameters we will not
get a perfect vπ(s). But, we are not looking for perfection.
What we are looking for is an approximation, because what
we really want to ﬁnd is an optimal action sequence or policy.
This will be found as that action at a given state which gives
the maximum state-action value out of all actions applicable
in that particular state. Thus relative correct comparisons of
state-action values for different actions in a state would do
our job. So we can apply function approximation with some
conﬁdence. We optimize the network parameters so as to
decrease the error (cid:107)ˆv(s, w) − vπ(s)(cid:107). This is similar to all
supervised learning techniques [23]. The only difference that
arises in reinforcement learning is that the target vπ is also
unknown. So we need to approximate vπ(s) either as a sample
(cid:80)T
t=0 Rt+1 obtained from simulation of model following a
policy π or using bootstrapped values Rt+1 + vπ(st+1).

With this we understand the meaning of the term Neuro-
dynamic Programming and also get the basic idea of policy
evaluation, which is estimating vπ(s) given a policy or an
action sequence π. Policy Evaluation is one step of the
Policy Iteration Algorithm. The 2nd step is of course the
Policy Improvement algorithm. This is what we are primarily
interested in : To ﬁnd a policy better than what we have now
[23].

To summarize :
• Estimate vπ(s) for each state s using a policy π
• For each s recompute π(s) as the action a which

maximizes vπ(s)(s)

• Continue until π(s) does not change for any state s.
This is the general Reinforcement Learning Technique
known as Policy Iteration. One question that arises immedi-
ately is what policy should we begin with? We can start with
any random policy [23]. In our work we always start with a
random policy.

1) Assign π0(s) as : For state s choose any action a with
uniform probability over all legal actions in state s.

2) Repeat for all states s.
However, when we don’t have a model of the environment
(cid:2)R1 +
to explicitly calculate the expectation Es(cid:48)∼π(s(cid:48)|s,π(s))
vπ(s1)|s0 = s(cid:3) and we have to rely only on samples of
simulated trajectory we run into the problem of exploration
vs exploitation [?], which more often than not results in bad
estimates of the optimal policy. To see clearly why this is
so let us maintain returns not only for the state but also on
a particular action taken in that state. This does not greatly
change our notation. Instead of estimating vπ(s), where in
state s we took an action π(s), we estimate va(s), the return

followed by taking a particular action a in state s and following
policy π for all other states. So, va(st) = Rt+1 + vπ(st+1).
Note that if π(s) = a, this would have exactly corresponded
to vπ(s). Usually Pr{π(s) = a} > 0, a ∈ A(s). This slight
deviation is not of great signiﬁcance in our problem because
for games such as Hex, checkers or GO the separateness of
state and state-action pair is not that extreme and we can
combine both in an after-state [23]. Maintaining returns for
a state-action pair has its advantages when using samples to
estimate optimal policy[23].

Imagine that we take the initial policy to be random and run
the Policy Iteration Algorithm. We also assume that we do not
calculate the true expected returns of each state but instead rely
on sample trajectories. We also assume the number of states
and actions corresponding to each state are small and can be
stored within a table itself.

1)

Initialize va to zero as a table of the size corresponding
to the total number of states and actions a legal in each
state s, and π0 is random.
2) Run a simulated trajectory st

T based on policy

0, st
πt which is as we have deﬁned.

1, ....st

3)

4)

For each state s encountered in the trajectory update
va(st) = Rt+1 +vb(st+1) where a and b are the actions
chosen according to πt(st) and πt(st+1).

For each s recompute πt(s) as the action a which

maximizes va(s)

5) Go to step 2. Continue till πt(s) = πt−1(s) for all states

s.

With the above procedure, actions which are not picked at the
a−→ st+1 which
ﬁrst policy, that is transitions of the nature st
are not encountered, will not result in a computation of va(s)
where a is a valid action that could have been taken in state s.
Therefore, even if a was the optimal action to be taken in state
s, it would not be included during the policy improvement step
of πt+1(s) = arg maxa(cid:48)∈A(s)/a va(cid:48)(s).

To circumvent this problem we note that had we allowed
in step 2 the policy π0 to visit all SA actions for all states,
we could have repeated step 3 for a large number of iterations
before reaching step 4, we would not have had this problem.
For then, va(s) would have been estimated quite nicely for
each state-action pair. However in a true online algorithm we
a−→ st+1 transition. To
want to do step 3 and step 4 after each st
do this one trick is to use (cid:15)-greedy policy [23]. An (cid:15)-greedy
policy is a stochastic policy all the way. Which is to say it
never converges to a deterministic policy, but the policy does
improve and reaches an (cid:15)-optimality. Here while choosing an
action based on the policy we don’t always choose the greedy
action with respect to the state-action value. Instead we do
it randomly, that is with probability
|A(s)| we choose a non-
greedy, random action while w.p 1 − (cid:15) + (cid:15)
|A(s)| we choose the
greedy action w.r.t the current state-action value [23]. We do
this every time we simulate a trajectory and update according
to the state-action-reward-state-action tuple encountered. This
way one can be assured that the optimal policy is achieved at
the end of the procedure [23].

(cid:15)

Another method to by pass this constraint is to use Policy
techniques [23]. Unlike (cid:15)-greedy policies, where

Gradient

4

there is no explicit policy without the value function, policy
gradient techniques focus on an explicit description of the
policy itself. Policy Gradient techniques use a parameterized
function approximation of a policy and improve the policy by
optimizing the parameters so as to maximize the return from a
state following that policy.Thus, given an environmental state
as input to the function the output is a probability distribu-
tion over all actions allowable in that state and parameters
are updated in a direction that increases vπθ (s), which is
∇vπθ (s). Parameters of the policy-network are updated based
on gradient-ascent techniques. More often than not actor-critic
methods are used, where not only value functions of states
are updated, but also the policy is improving based on newer
estimates of the value functions. Policy Gradient techniques
can converge to a local optimum unlike (cid:15)-greedy policies
where there is always an (cid:15)-probability of choosing a random
action.

The ﬁnal topic of interest is the error used in conjunc-
tion with function approximation [24]. As noted earlier,
(cid:107)ˆv(s, w) − vπ(s)(cid:107) is the error one tries to reduce. But there are
a number of facets to this problem. First, the error that we are
(cid:2)vπ(s) − ˆv(s, w)(cid:3)2
trying to reduce is Eπ
, where it is assumed
that each state is distributed in the steady-state according to
the on-policy distribution π which is being used to interact
with the environment. If vπ(s) was known we could have used
samples instead of the expectation and applied gradient descent
techniques to optimize the parameters. On the other hand,
if vπ(s) is unknown, we can use estimates from simulated
trajectories. One such estimate is (cid:80)T
t=0 Rt+1, that is the entire
reward obtained through the length of the trajectory starting
from state s. Using this estimate for vπ(s) also leads to good
use of stochastic gradient techniques to optimize parameters.
But what happens when we use bootstrapped estimates, like
Rt+1 + vπ(st+1)? Here, one can get vπ(st+1) by the ap-
proximation ˆv(st+1, w). The update would be proportional to
the error, Rt+1 + ˆv(st+1, w) − ˆv(st, w).Note that, for a true
gradient descent, the direction should be along the negative of
∇w ˆv(st+1, w) − ∇w ˆv(st, w). However, what one usually does
is what is known as semi-gradient descent, which is to say
ignore the effects of the changing weight vector on the target
[23], i.e., use the direction as −∇w ˆv(st, w). Semi-Gradient
techniques have also been used successfully with on-policy
bootstrapped targets as estimates for learning through function
approximation.

The assumption that the states are distributed according to
the transition probabilities of the Markov Chain does not nec-
essarily hold true as in most cases of control through (cid:15)-greedy
policies. These form the basis of off-policy control in Markov-
Decision-Processes. Here, states are distributed according to
a behaviour policy, while the transitions about which we are
interested are those due to a target policy. The return from a
state as simulated due to a behaviour policy is therefore not a
true estimate of the return one would have obtained if the target
policy would have been followed. One can get around this
problem by applying importance sampling ratio to the returns
under the behaviour policy. Thus vπ(s) can be approximated
as ρ (cid:80)T
t=0 Rt+1 with ρ as the importance-sampling ratio, and

then apply stochastic gradient as earlier. The problem arises
when one tries to use bootstrapped values along with semi-
gradient techniques. There are counter examples [25], [24] and
a work by Richard Sutton and Mahmood [26] which show that
such techniques would diverge when used in conjunction with
function approximation. To address this problem, the target had
to be examined with its relation to function approximation.

It can be assumed that the class of functions deﬁned by a
given architecture cannot exactly represent vπ(s) for any s ∈ S
[23]. Thus the best we can do is ﬁnd parameters so that the
error, (cid:2)vπ(s) − ˆv(s, w)(cid:3)2
, is a minimum on average. When
vπ(s) is estimated as the cumulative reward over the entire
trajectory, then the solution obtained is the best solution one
could arrive at for approximating vπ(s). However, when we use
bootstrapped values the solution is often a bad approximation.
To understand this observe that when we apply the Bellman
Operator, T π [27] on a value function ˆv representable by
(cid:2)Rt+1 + ˆv(st+1, w)(cid:3),
the function architecture, the result, Eπ
is a new value function, generally not representable by the
architecture. On repeated application of the Bellman-Operator,
the value-functions converge to the true vπ. However, with
function approximation, each intermediate value function is
projected back into the function space, before applying the
Bellman Operator. This leads to different estimates for the
true value function vπ. Since, vπ is the only ﬁxed point of
the Bellman Operator [27], that is to say, it is the only point
(cid:2)Rt+1 + vπ(st+1) − vπ(st))(cid:3)
where the Bellman Error, Eπ
is zero, one could very easily see that the Bellman Error
can never be reduced to 0 while using function approxima-
tion unless the true value function is representable via the
parameterized class of value functions (obtained from using
function approximation) itself. However, if we were to project
the Bellman Error in the function space one could ﬁnd a value
function where the Projected-Bellman Error is 0 [23]. These
areas were studied by Richard Sutton, Hamid Maei, Doina
Precup, Shalabh Bhatnagar, David Silver, Csaba Szepesvari,
Eric Wiewiora and they have given algorithms which are based
on actual gradient techniques which tend to minimize the
Projected Bellman Error [28], [29], [30], [31], [32].

There is an additional point we would like to make, that
of eligibility traces. We will not go into the full details here,
except to note that there exists a parameter λ which can be used
to tune how much of the returns are to be accumulated before
using bootstrapped values of states encountered in sample
paths. λ = 0 corresponds to just a single transition, whereas
λ = 1 implies the entire cumulative reward one obtains at the
end of the trajectory. One can choose any λ between 0 and 1
[23].

IV. EXPERIMENTS

We run all experiments on a 3×3 Hex board. The reason for
doing this is we know what the optimal game play should be in
3 × 3 and moreover we believe that once the key elements for
ensuring optimal play have been identiﬁed, scaling to larger
systems would be comparatively simpler than without having
done so. Large scales might impose other problems like that
of exploration, but we defer the issue to future work.

5

Each board conﬁguration has been modelled as a 6 × 3 ×
3 Boolean 3d array [21]. The 6 channels correspond to the
following

0 : a 3 × 3 array with positions for red stones marked as

true.

1 : a 3 × 3 array with positions for blue stones marked as

true.

2 : a 3 × 3 array with positions for which red stones form
a continuous unbroken chain attached to the east wall marked
as true.

3 : a 3 × 3 array with positions for which red stones form
a continuous unbroken chain attached to the west wall marked
as true.

4 : a 3 × 3 array with positions for which blue stones form
a continuous unbroken chain attached to the north wall marked
as true.

5 : a 3 × 3 array with positions for which blue stones form a
continuous unbroken chain attached to the south wall marked
as true.

Along with this for each of the 6 channels, each side of the
3 × 3 array is augmented by 2 units to form the red and blue
edges, thus resulting in a 7 × 7 array. The 0th channel is thus
augmented with 2 columns at the beginning and end marked
as true and 2 rows at the top and bottom marked as false, the
inner 3×3 board remains as it is. The 1st channel is augmented
with 2 columns at the beginning and end marked as false and
2 rows at the top and bottom marked as true, the inner 3 × 3
board remains as it is. The 2nd channel is augmented with 2
columns at the beginning marked as true and 2 columns at
the end marked as false and 2 rows at the top and bottom
marked as false, the inner 3 × 3 board remains as it is. The
3rd channel is augmented with 2 columns at the beginning
marked as false and 2 columns at the end marked as true and
2 rows at the top and bottom marked as false, the inner 3 × 3
board remains as it is. The 4th channel is augmented with 2
columns at the beginning and at the end marked as false and
2 rows at the top marked as true and 2 rows at the bottom
marked as false, the inner 3 × 3 board remains as it is. The
5th channel is augmented with 2 columns at the beginning and
at the end marked as false, 2 rows at the top marked as false
and 2 rows at the bottom marked as true, the inner 3 × 3 board
remains as it is. Thus the input is pre-processed into a 6×7×7
three-dimensional array [21].

The neural network architecture used has 4 layers [21]. The
1st layer is a 2d convolution layer acting on the pre-processed
input, the 2nd layer is another 2d convolution layer acting
on the output of the 1st layer. The 3rd layer is also a 2d
convolution layer which acts on the 2nd layer output and the
4th layer is a sigmoid layer acting on the product of a matrix
and the output of the 3rd layer to produce a 9 × 1 vector of
after-state values squashed between -1 and 1. Each convolution
layer has 2 types of ﬁlters [21] : A 1 × 1 size ﬁlter initialized
randomly and a 2d ﬁlter of 3 × 3 with last 2 rows of 1st
column, the entire 2nd column and 1st 2 rows of last column
initialized randomly depicting the 6 neighbouring cells of a
centre Hex cell with 2 remaining cells initialized to zero. The
ﬁlter width is set to 6. The ﬁrst layer has 2 ﬁlters of type I
and 3 ﬁlters of type II. The second layer has 3 ﬁlters of type

6

Fig. 2. Flow-Model of training Network Parameters along with self play

form to represent error ﬂow and self play simultaneously.
As it is evident we have used online training of network, by
updating parameters with every transition. However, this sort of
training could result in high variance in parameter estimation,
as the network parameters can get too ﬁne tuned with a few
states which are encountered too often. We therefore want
to use batch training, where we train the network parameters
simultaneously for a number of inputs [21].

The updates, as shown in the ﬁgure, correspond to those of

TD(0) [24] :

θ := θ + α[Rt+1 + ˆv(st+1, b; θ) − ˆv(st, θ)]∇ˆv(st, a; θ). In
literature this is deﬁned as SARSA (state-action-reward-state-
action) and is an on-policy TD-method.

This update based on reducing the Value Error Eπ

ˆv(s, θ)(cid:3)2

using bootstrap estimates of vπ.

We have also used Q-learning, which is an off-policy based

TD-control procedure with the update rule

θ := θ+α[Rt+1+ max
b∈A(s)

ˆv(st+1, b; θ)−ˆv(st, a; θ)]∇θ ˆv(st, a; θ)

(cid:3)2

(cid:2)Vθ − ΠT πVθ

based on reducing the error deﬁned as above. We have also
used updates based on reducing the Projected Bellman Error
Eµ
, where Π is a projection operator, µ the
state distribution assumed under a behaviour policy b, Vθ the
current value function estimate with parameters θ and T π the
Bellman Operator acting on the value function with target
policy π [28]. Based on the initial works of Sutton et al we had
two prominent algorithms GTD 2 and TDC [29]. Their work
primarily focused on linear function approximators where the
projection operator could be represented as a matrix. Shalabh
Bhatnagar et al also argued that under assumptions of small
learning rate and smooth value-functions, the function-space
represented by a neural network could be approximated to be
linear around a small neighbourhood of the current parameter
estimate. The authors then extended their work to general
non-linear function approximators such as neural networks

(cid:2)vπ(s) −

Fig. 1. Self-Play as used in algorithm

I and 2 ﬁlters of type II. The third layer has only 5 ﬁlters of
type I. The fourth layer has a 245 × 9 matrix and 9 × 1 vector
initialized randomly.

The basic algorithm followed is the following [23]:
1) Start at a random state s0.

2) process state through the neural network to choose
an action a (cid:15)-greedily. Get ˆv(st, w)
3) play action on the state.
4) get reward Rt+1 and next state st+1.
5) process the new state through the neural network
to choose an action b (cid:15)-greedily. Get ˆv(st+1, w)
6) Update w as a function of Rt+1 + ˆv(st+1, w) −
ˆv(st, w)

The game is a 2-person game, however we do not train 2
networks representing 2 players. Instead we just train for one
player, and assume that second player at best can copy the
same strategy. This is sensible in such a game where the 1st
player has the winning strategy and if the ﬁrst player makes a
random choice in the middle, the second player can continue
to play the 1st player strategy and win. The self-play is a bit
different than what one might use in zero-sum games.

We make 2 copies of a board, in which one is the transpose
of the other. In one copy, white moves ﬁrst while in the other
black moves ﬁrst. The moves are mirror images of each other.
Which one is to start at ﬁrst is chosen at random. Once it’s
chosen, games are played on both copies of the board with
maintaining the mirror image property of the moves. Two
sequential states belong to different copies of the board (see
ﬁgure 1). The mirror property of both copies and the use of
convolution maintains that positions are evaluated equally on
both copies of the game. This methodology is in the spirit of
Hex, where the ﬁrst person always has the winning strategy
and by making the second player copy it ensures that the
best policy learned will be used by both players to the same
advantage. Another advantage is that this method takes into
account the symmetricity of the Hex board, and thus ensures
further exploration as well as exploitation [21].

In Figure 2 we show the basic algorithm in a pipeline

7

algorithm to control settings which they term as Greedy GQ(λ)
algorithm [32]. This is along the same lines of using a random
policy such as (cid:15)-greedy as a behaviour policy which ensures
exploration and a greedy policy with respect to the current
estimate of action values. We used the same ideas in our
eligibility trace extended version of the GTD2 algorithm
to extend to control problems. For completeness we have also
implemented the Greedy GQ(λ) algorithm separately though
we assume its performance would be similar to that of the TDC
algorithm used alongside an (cid:15)-greedy policy as used in off-
policy control. Our extensions are not documented because it
is similar to the extensions carried out for the TDC algorithm.
We have implemented the three algorithms along with
SARSA which includes eligibility traces as well and Q-
learning in the procedural architecture depicted in Fig 2.
That is, to update network parameters manually using update
equations following single state transitions. However it is well
known in the Deep-Learning Literature that single input-output
training of neural networks might result in high variance in
the parameter updates. So we want to use a batch of inputs
and outputs to train our neural network. In THEANO, neural
network training is done by giving a loss function, for exam-
ple (cid:2)vπ(s) − ˆv(s, w)(cid:3)2
and the network parameters are then
updated to minimize the loss using a variety of optimization
techniques. We have estimated vπ(s) by Q-learning and have
updated network-parameters after every 50 state transitions to
optimize (cid:2)vπ(s) − ˆv(s, w)(cid:3)2
using a single step of RMSPROP
[33] for a 50 state-batch as depicted in Figure 3. However,
minimizing the projected Bellman Error in this way presents a
problem. The projection operator for curved general surfaces
is difﬁcult to depict and hence the loss function could not
be represented with known parameters. The way we have
bypassed this problem is optimizing two objective functions
simultaneously:

ΠT vˆθ = min

θ
θ∗ = arg min

[T vθ − vθ]2,

[ΠT vˆθ − vθ]2,

Fig. 3. Flow-Model of training Network Parameters in Batch

[30], [32]. The updates for GTD2 and TDC algorithm using
non-linear function approximators are used for updating our
network parameters.

(cid:16)

δ = Rt+1 + ˆv(st+1, b; θ) − ˆv(st, a; θ),
δ − ∇θ ˆv(st, a; θ)T w
w := β
(cid:17)
(cid:16)
δ − ∇θ ˆv(st, a; θ)T w

h =

∇2

(cid:17)

∇θ ˆv(st, a; θ),

θ ˆv(st, a; θ)w,
(cid:26)(cid:16)

θ

:=

α

∇θ ˆv(st, a; θ)

−

∇θ ˆv(st+1, b; θ)

(cid:17)(cid:16)

∇θ ˆv(st, a; θ)T w

(cid:17)

(cid:27)

− h

.

(cid:16)

δ = Rt+1 + ˆv(st+1, b; θ) − ˆv(st, a; θ),
δ − ∇θ ˆv(st, a; θ)T w
w := β
(cid:17)
(cid:16)
δ − ∇θ ˆv(st, a; θ)T w

h =

∇2

(cid:17)

∇θ ˆv(st, a; θ),

θ ˆv(st, a; θ)w,
(cid:26)

θ

:=

α

δ∇θ ˆv(st, a; θ)

−

θ

∇θ ˆv(st+1, b; θ)

(cid:16)

∇θ ˆv(st, a; θ)T w

(cid:17)

(cid:27)

− h

.

As one works through the derivation of the above algorithms
one will notice that the starting point is the same [28], [29],
[32]. One could conclude that the algorithms would behave
almost similarly. Here α and β are the two time scales.
We have experimented by varying the time-scale ratios of
both these algorithms. Hamid Maei and Richard Sutton have
extended the TDC algorithm to include eligibility traces and
even further to include prediction of action-values as well,
which they call as the GQ(λ) algorithm [31]. By following
the same procedure we have come up with the GTD2
algorithm which includes eligibility traces. For the action-
value prediction portion, we note that in our problem there
is no distinct separation between state-value and action-value.
We have used our algorithms in the setting of what Richard
Sutton calls as after-state-values [23].

Hamid Maei has also particularly extended the GQ(λ)

where T vθ is approximated by Q-learning. After 50 state
transitions, we have an estimate of T vθ by Q-learning and
we run a single step of RMSPROP to get an approximation
of ΠT vˆθ followed by a single step of RMSPROP to update
network parameters to be used again in the next 50 state-
transitions. Though we have not proved that
this will be
equivalent to minimizing the Projected Bellman Error, we shall
nevertheless call it as the PBE.

Apart from using (cid:15)-greedy policy to control we have also
used policy-gradient techniques to explicitly optimize for an
optimal policy. Speciﬁcally we used actor-critic algorithms,
where a separate value or critic network is used to estimate
the value function and a policy or actor network is used to
optimize the policy parameters. We have included eligibility
traces during policy parameter updates as well [23]. The policy
network is similar to the value network [21] except that in the
4th layer instead of a sigmoid function we have used a softmax
function which acts on the product of a matrix and the output
of the 3rd layer. The general algorithm is suggested as follows:

1)
2)

update parameters of value network.
update parameters of policy network.

We have used gradient ascent as well as Natural Gradient
ascent [34], [35], [36] to update policy network parameters.
We have used the setting of Fig. 2. that is updating parameters
online and not used batch updates. The value function has been
updated using SARSA, TDC, GTD 2, Greedy GQ(λ) and Q-
learning.

To view how our present techniques do with older method-
ologies, we have resorted to use the H-search algorithm to
score human played board moves [21]. We constructed our own
data-set of human moves on a 3 × 3 board and scored them
via the H-search algorithm as discussed in Kenny Young et
al. Using these scores we have pre-trained the neural network
parameters, before using self-play and reinforcement learning
algorithms (this methodology is used also in [20]). We have
compared these results with those where there had been no
pre-training of neural networks and parameters were learned
solely by self-play and reinforcement learning algorithms. This
experiment was inspired by the recent success of Alpha Go
Zero [22]. Our results indicate no visible deviations with
regards to the learned values and we conclude that self-play
and reinforcement learning are sufﬁcient for learning.

Finally, we have experimented with a separate architecture
itself. The ﬁrst 4 layers are similar to the old architecture
on top of which we have applied a recurrent neural network
of depth 10. A sequential batch of 10 board conﬁgurations
is fed through the neural network. The output of the ﬁrst 3
convolution layers followed by the sigmoid layer( See Fig 3)
results in 10 vectors of size 9. We then apply the Recurrent
Layer as shown in Fig. 4. Here, U,V and W are three 9 × 9
matrices. As activation function we use a sigmoid function.
Our intuition is, by making a board conﬁguration at a later
stage dependent on board conﬁgurations encountered earlier,
one might get improvements. The target updates for each input
are estimated using Q-learning as well as SARSA. The loss
functions that the THEANO environment tries to minimize
are both the Mean Squared Error and the Projected Bellman
Error between the target and the current estimate. We use the
RMSPROP algorithm to optimize.

V. RESULTS
We plot after-state-values averaged over each trajectory as
a function of the number of iterations. Since a reward of 1 is
obtained only upon termination after a win and 0 everywhere
else, one could quickly see that optimal after-state values
would be 1.

In Figures 5, 6 and 7 we plot the after-state values for 5
two-time scale ratios and 4 value-network parameter learning
rates α in a network shown in Fig. 2 using the GTD2,
TDC and Greedy GQ(λ) update algorithms following online
implementation.

In Fig. 8 we plot the after-state values for 4 learning rates
of the value-network parameter where the network is the one
shown in Fig 2 using SARSA and Q-learning updates of targets
implemented online.

In Fig. 9 we plot the after-state values for ﬁve (cid:15) values, the
probability with which greedy actions are chosen over random

8

Fig. 4. Flow-Model of training a Recurrent Neural Network in batch.

(a)

(b)

(c)

(d)

(e)

Fig. 5. After state values using GTD 2 algorithm for α values 0.0001, 0.001,
0.01 and 0.1 and β/α ratios at: (a) β/α = 0.01 (b) β/α = 0.1 (c) β/α = 1
(d) β/α = 10 (e) β/α = 100

(a)

(b)

(c)

(d)

(e)

Fig. 6. After state values using TDC algorithm for α values 0.0001, 0.001,
0.01 and 0.1 and β/α ratios at : (a) β/α = 0.01 (b) β/α = 0.1 (c) β/α = 1
(d) β/α = 10 (e) β/α = 100

9

(a)

(b)

(c)

(a)

(b)

(c)

(d)

(e)

Fig. 7. After state values using Greedy GQ(λ algorithm for α values 0.0001,
0.001, 0.01 and 0.1 and β/α ratios at : (a) β/α = 0.01 (b) β/α = 0.1 (c)
β/α = 1 (d) β/α = 10 (e) β/α = 100

(d)

Fig. 10. After State Values with λ values at 0, 0.01, 0.1, 0.5 and 1 for: (a)
Greedy GQλ algorithm (b) TDC algorithm (c) GTD 2 algorithm (e) SARSA
algorithm

(a)

(b)

(a) After state values using SARSA algorithm for α values 0.0001,
Fig. 8.
0.001, 0.01 and 0.1 (b) After state values using Q learning algorithm for α
values 0.0001, 0.001, 0.01 and 0.1

actions, using value-network updates, in a network shown in
Fig 2 done online, given by the Greedy GQ(λ), TDC, GTD2,
Q-learning and SARSA algorithms respectively.

In Fig. 10 we show the variation of after-state values for 5
eligibility trace parameters λ using updates given by Greedy
GQ(λ), TDC, GTD2, and SARSA algorithms in the network
shown in Fig. 2 and implemented online.

In Fig. 11 we plot after-state values for a network, shown in
ﬁgure 2, where the parameters have been updated online using
the TDC algorithm, but instead of using stochastic gradient to
optimize we use Natural Gradients. The results are shown for
5 different ratios of the 2 time-scales and 5 different learning
rates. We call this method as TDC with NG.

(a)

(b)

(c)

(d)

(e)

Fig. 11. After state values using TDC algorithm with Natural Gradient for
optimizing value-network parameters with α values 0.0001, 0.001, 0.01 and
0.1 and β/α ratios at: (a) β/α = 0.01 (b) β/α = 0.1 (c) β/α = 1 (d)
β/α = 10 (e) β/α = 100

(a)

(b)

(c)

(a)

(b)

(c)

(d)

(e)

Fig. 9. After state values with (cid:15) values at 0.0001, 0.01, 0.1, 0.5 and 0.99
for: (a) Greedy GQλ algorithm (b) TDC algorithm (c) GTD 2 algorithm (d)
Q-learning algorithm (e) SARSA algorithm

(d)

(e)

Fig. 12. After-State Values using Policy Gradient to optimize a separate policy
network with learning rates α at 0.001, 0.01, 0.1, 1 and 10 for : (a) TDC
algorithm (b) TDC algorithm with Natural Gradient optimization of policy
network (c) GTD 2 algorithm (d) Greedy GQ(λ) algorithm (e) Q-learning
algorithm.

10

(a)

(b)

(c)

(a)

(b)

Fig. 15. After state values implemented using Q-learning and batch updates
for (a) CNN model with learning rates α at 0.001, 0.01, 0.1, 1 and 10 (b)
RNN model with learning rates α at 0.001, 0.01, 0.1, 1 and 10

(d)

(e)

Fig. 13. After-state values with policy parameters using eligibility traces λ
at 0, 0.1, 0.5 and 1 for : (a) TDC algorithm (b) TDC algorithm with Natural
Gradient optimization of policy network (c) GTD 2 algorithm (d) Greedy
GQ(λ) algorithm (e) Q-learning algorithm.

(a)

(b)

(c)

(d)

(e)

(f)

Fig. 14. After state values with and without pretraining using human dataset
: (a) TDC algorithm (b) TDC algorithm with Natural Gradient optimization
of policy network (c) GTD 2 algorithm (d) Greedy GQ(λ) algorithm (e) Q-
learning algorithm. (f) SARSA algorithm.

In Fig. 12 we plot after-state values generated while using a
separate policy network to learn an explicit policy.The policy
network is trained using stochastic gradient and results are
shown for ﬁve learning rates α with value-network updated
using TDC, GTD2, Greedy GQ(λ) and Q-learning. We also
have one ﬁgure where Natural Gradient optimization is used
for updating Policy Network parameters while the Value Func-
tion is updated using the TDC algorithm. The network used is
shown in Fig. 2 and updates are done online.

In Fig. 13 we plot after-state values when a separate policy
network has been used. The policy parameters were updated in
a network depicted in Fig. 2 implemented online, by stochastic
gradient using ﬁve eligibility traces λ in the policy parameters
themselves. Results are shown for each case when the value
network is updated using TDC, GTD2, Greedy GQ(λ) and Q-
learning. We also present a ﬁgure when the policy parameters
were updated using Natural Gradient instead of Stochastic
Gradient.

In Fig. 14 we compare the after state values with a super-
vised pre-training of network (Fig. 2) with parameters updated
online, using constructed data of played games and scored
according to the idea of H-search algorithm followed by self-

(a)

(b)

Fig. 16. After state values implemented using SARSA and batch training for
(a) CNN model with learning rates α at 0.001, 0.01, 0.1, 1 and 10 (b) RNN
model with learning rates α at 0.001, 0.01, 0.1, 1 and 10

play and reinforcement learning of network parameters, with-
out any human data relying only on self play and reinforcement
learning algorithms.

In Fig. 15 we plot after-state values for the CNN architecture
(Fig.3) and the RNN architecture (Fig.4) with batch training.
The CNN model takes 50 randomly sampled states that it
has encountered while the RNN model uses sequences of 10
states. The Q-learning algorithm is used for target updates and
RMSPROP algorithm is used to optimize with the network
parameters. We show results for 5 learning rate parameters α.
In Fig. 16 we use the SARSA algorithm instead of Q-
learning to compare the after-state values using the two differ-
ent architectures of the CNN (Fig. 3) and RNN (Fig. 4) using
batch training similar to the one used to generate Fig. 15 for
ﬁve learning rate parameters α.

In Fig. 17 we plot the After-State values learned using
Mean Squared Error and Projected Bellman Error (as we

(a)

(b)

Fig. 17. After-state values using 2 different objective functions for (a) CNN
architecture (b) RNN architecture

(a)

(b)

Fig. 18. Training Errors using 2 different objective functions for (a) CNN
architecture (b) RNN architecture

Fig. 19. Comparisons of the Projection Error using RNN and CNN

Fig. 20. Comparisons of the different algorithms

have implemented) as objective functions and targets estimated
by Q-learning and SARSA respectively for both the CNN
and RNN architectures. For the MSE error we have kept the
learning rate at 0.001 and for the PBE error we have used two
different learning rates. For the projection operation we have
kept the learning rate at 0.1 and for the parameter update we
have kept the learning rate at 0.001. To ensure that network
parameters are kept independent for both the updates we have
used 2 identical networks, one for performing the projection
operation and the other for value function estimation. Both
networks are updated based on the two error functions as
described. In Fig. 18 we plot the network-training errors using
Mean Squared Error and Projected Bellman Error (as we have
implemented) as objective functions for both CNN and RNN
architecture.

In Fig. 19 we plot the projection error that is incurred
between the target and the best representable function for both
architectures.

In Fig. 20 we compare between all 6 algorithms using a
network depicted in Fig. 2 using online updates and using
best hyperparameters found in our work.

To determine how well these algorithms perform in actual
game playing, we created four agents. CNN-MSE is a 4-layer
convolution architecture and uses the Mean Square Error as
the objective function that the network tries to minimize and
Q-learning to update the after-state value targets. CNN-PBE
is a 4-layer convolution architecture and uses the Projected
Bellman Error as the objective function that the network tries
to minimize and uses SARSA to update the after-state value
targets. RNN-MSE is a 3-layer convolution followed by a
RNN layer architecture and uses the Mean Square Error as
the objective function that the network tries to minimize and
Q-learning to update the after-state value targets. RNN-PBE
is a 3-layer convolution followed by a RNN layer architecture
and uses the Projected Bellman Error as the objective function
that the network tries to minimize and uses SARSA to update
the after-state value targets. We play for 2 seasons, with each
season holding 10 tournaments and each tournament having

11

a total of 12 games with the 4 players playing against each
other. In each tournament a single player plays 3 games as the
ﬁrst player and 3 games as the second player. In each season a
single player plays 30 games as the ﬁrst player and 30 games
as the second player. The results are shown in Table I.

VI. CONCLUSION

By glancing through the results in Table I one immediate
conclusion that can be drawn is using an extra Convolution
Layer instead of a RNN layer improves overall performance
thus leading us to believe our original intuition about better
improvements was wrong. This is however not surprising as
Hex is a complete information game. At any instance of a
board one has a winning strategy which is independent of
how the game was played before [1]. RNN would perhaps
be better suited in games where one needs to remember the
moves played in sequence like many card games. The next
obvious result is that our implemented error function which
we call PBE performs better than the MSE in both models.
Our hypothesis is that the HEX game can be better modeled
with a CNN architecture than using an RNN architecture as
can be seen from the projection error graph in Fig. 19. Further
we would like to hypothesize that with improved function
the Projected Bellman
approximators (in our case CNN),
Error performs signiﬁcantly better than when applied on poor
function approximators. Also that our error function is very
similar to the GTD2 and TDC algorithms as derived by Sutton
et al, can be noticed in how we also use two learning rates and
the faster convergence exhibited by both ﬁgures where one is
updated manually using the GTD2 and TDC updates and the
other through an explicit given error function that we have
proposed (see for example Figs. 20 and 17). Though we have
not formally proved the similarity, we leave it for future work.

VII. FUTURE WORK

A formal investigation into calculating the actual gradients
of the objective function we have proposed and studying the
convergence guarantees is still to be done. Also as suggested
in [37] the idea of eligibility traces as applied has to be
further investigated. Finally the performance in a large 13×13
board against strong human players has to be conducted before
making further comments.

VIII. ACKNOWLEDGEMENT

Heartfelt gratitude towards Prof. Shalabh Bhatnagar, Prasen-
jit Karmakar, Chandramouli Kamanchi, Abhik Singla and the
Stochastic Systems Lab. team. Special token of respect towards
Prof Sastry and the EECS faculty of Indian Institute of Science.
Shib Shankar Dasgupta, Kisnsuk Sircar, Swayambhu Nath Ray
have been indispensable during the hard times. A very general
gratitude towards Indian Institute of Science and the great
nation of India. A special note towards Priyaam Roy for her
help and support. A ﬁnal note of thank you towards my parents
whose constant well wishes were with me always.

TABLE I.

FINAL RESULS

Season

CNN-MSE

CNN-PBE

RNN-MSE

RNN-PBE

1
2
TOTAL WINS

First Player
20
23

Second Player
7
8

First Player
24
23

Second Player
14
12

First Player
20
21

Second Player
8
5

First Player
19
22

Second Player
8
7

58

73

54

56

12

REFERENCES

[1]
[2]

[Online]. Available: https://en.wikipedia.org/wiki/Hex_(board_game)
J.
http://web.mit.edu/sp.268/www/hex-notes.pdf

Available:

[Online].

“Hex,”

Spring

2011.

Li,

[3] B. Arneson, R. B. Hayward, and P. Henderson, “Solving hex: be-
yond humans,” in International Conference on Computers and Games.
Springer, 2010, pp. 1–10.

[4] C. E. Shannon, “Computers and automata,” vol. 41, pp. 1234 – 1241,

11 1953.

[5] V. V. Anshelevich, “A hierarchical approach to computer hex,” Artiﬁcial
Intelligence, vol. 134, no. 1, pp. 101 – 120, 2002. [Online]. Available:
http://www.sciencedirect.com/science/article/pii/S0004370201001540

[6] S. Even and R. E. Tarjan, “A combinatorial problem which is complete
in polynomial space,” Journal of the ACM (JACM), vol. 23, no. 4, pp.
710–719, 1976.

[7] S. Reisch, “Hex ist pspace-vollst¨andig,” Acta Informatica, vol. 15, no. 2,

[8]

[9]

pp. 167–191, 1981.
J. van Rijswijck, “Are bees better than fruitﬂies?” in Conference of the
Canadian Society for Computational Studies of Intelligence. Springer,
2000, pp. 13–25.
J. Yang, S. Liao, and M. Pawlak, “On a decomposition method
for ﬁnding winning strategy in hex game,” in Proceedings ADCOG:
Internat. Conf. Application and Development of Computer Games (ALW
Sing, WH Man and W. Wai, eds.), City University of Honkong, 2001,
pp. 96–111.

[10] ——, “A new solution for 7x7 hex game,” 01 2002.
[11] V. V. Anshelevich, “The game of hex: the hierarchical approach,” in
More Games of No Chance, Proc. MSRI Workshop on Combinatorial
Games. Citeseer, 2000, pp. 151–165.
J. Van Rijswijck, “Search and evaluation in hex,” Master of science,
University of Alberta, 2002.

[12]

[13] R. Hayward, Y. Bj¨ornsson, M. Johanson, M. Kan, N. Po, and J. van
Rijswijck, “Solving 7× 7 hex: Virtual connections and game-state
reduction,” in Advances in Computer Games.
Springer, 2004, pp.
261–278.

[14] R. Hayward, Y. Bj¨ornsson, M. Johanson, M. Kan, N. Po, and J. Van Ri-
jswijck, “Solving 7× 7 hex with domination, ﬁll-in, and virtual con-
nections,” Theoretical Computer Science, vol. 349, no. 2, pp. 123–139,
2005.

[15] P. Henderson, B. Arneson, and R. B. Hayward, “Solving 8x8 hex.” in

IJCAI, vol. 9, 2009, pp. 505–510.
[16]
[Online]. Available: http://webdocs.cs.ualberta.ca/ hayward/hex/
[17] B. Arneson, R. B. Hayward, and P. Henderson, “Monte carlo tree search
in hex,” IEEE Transactions on Computational Intelligence and AI in
Games, vol. 2, no. 4, pp. 251–258, 2010.

[18] T. Cazenave and A. Safﬁdine, “Monte-carlo hex,” in Proc. Board Games

Studies Colloq., Paris, France, 2010.

[19] S.-C. Huang, B. Arneson, R. B. Hayward, M. M¨uller, and J. Pawlewicz,
“Mohex 2.0: a pattern-based mcts hex player,” in International Confer-
ence on Computers and Games. Springer, 2013, pp. 60–71.

[20] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van
Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam,
M. Lanctot et al., “Mastering the game of go with deep neural networks
and tree search,” nature, vol. 529, no. 7587, pp. 484–489, 2016.

[21] K. Young, G. Vasan, and R. Hayward, “Neurohex: A deep q-learning

hex agent,” in Computer Games. Springer, 2016, pp. 3–18.

[22] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang,
A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton et al., “Mastering
the game of go without human knowledge,” Nature, vol. 550, no. 7676,
p. 354, 2017.

[23] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.

[24]

MIT press Cambridge, 1998, vol. 1, no. 1.
J. Tsitsiklis, B. Van Roy, I. of Technology. Laboratory for Information,
and M. Decision Systems, “An analysis of temporal-difference learning
with function approximation,” vol. 42, 06 1997.

[25] L. Baird, “Residual algorithms: Reinforcement learning with function
Elsevier,

approximation,” in Machine Learning Proceedings 1995.
1995, pp. 30–37.

[26] R. S. Sutton, A. R. Mahmood, and M. White, “An emphatic
temporal-difference
[Online]. Available:

approach
off-policy
learning,” CoRR, vol. abs/1503.04269, 2015.
http://arxiv.org/abs/1503.04269

problem of

the

to

[27] D. P. Bertsekas and J. N. Tsitsiklis, Neuro-Dynamic Programming,

1st ed. Athena Scientiﬁc, 1996.

[28] R. S. Sutton, H. R. Maei, and C. Szepesv´ari, “A convergent
o(n)
temporal-difference algorithm for off-policy learning with
linear function approximation,” in Advances in Neural Information
Processing Systems 21, D. Koller, D. Schuurmans, Y. Bengio, and
L. Bottou, Eds. Curran Associates,
Inc., 2009, pp. 1609–1616.
http://papers.nips.cc/paper/3626-a-convergent-
[Online]. Available:
on-temporal-difference-algorithm-for-off-policy-learning-with-linear-
function-approximation.pdf

[29] R. S. Sutton, H. R. Maei, D. Precup, S. Bhatnagar, D. Silver,
C. Szepesv´ari, and E. Wiewiora, “Fast gradient-descent methods for
temporal-difference learning with linear function approximation,” in
Proceedings of the 26th Annual International Conference on Machine
Learning, ser. ICML ’09. New York, NY, USA: ACM, 2009, pp. 993–
1000. [Online]. Available: http://doi.acm.org/10.1145/1553374.1553501
[30] S. Bhatnagar, D. Precup, D. Silver, R. S. Sutton, H. R. Maei, and
C. Szepesv´ari, “Convergent temporal-difference learning with arbitrary
smooth function approximation,” in Advances in Neural Information
Processing Systems, 2009, pp. 1204–1212.

[31] H. R. Maei and R. S. Sutton, “Gq (λ): A general gradient algorithm
for temporal-difference prediction learning with eligibility traces,” in
Proceedings of the Third Conference on Artiﬁcial General Intelligence,
vol. 1, 2010, pp. 91–96.

[32] H. R. Maei, “Gradient temporal-difference learning algorithms,” 2011.
[33] T. Tieleman and G. Hinton, “Lecture 6.5-rmsprop: Divide the gradient
by a running average of its recent magnitude,” COURSERA: Neural
networks for machine learning, vol. 4, no. 2, pp. 26–31, 2012.
[34] S. Bhatnagar, R. S. Sutton, M. Ghavamzadeh, and M. Lee, “Natural
actor–critic algorithms,” Automatica, vol. 45, no. 11, pp. 2471–2482,
2009.

[35] S.-I. Amari, “Natural gradient works efﬁciently in learning,” Neural

computation, vol. 10, no. 2, pp. 251–276, 1998.

[36] R. Pascanu and Y. Bengio, “Revisiting natural gradient for deep

networks,” arXiv preprint arXiv:1301.3584, 2013.

[37] H. Seijen and R. Sutton, “True online td (lambda),” in International

Conference on Machine Learning, 2014, pp. 692–700.

