0
2
0
2

b
e
F
3
1

]

G
L
.
s
c
[

1
v
0
1
8
5
0
.
2
0
0
2
:
v
i
X
r
a

Published as a conference paper at ICLR 2020

RNA SECONDARY STRUCTURE PREDICTION
BY LEARNING UNROLLED ALGORITHMS

Xinshi Chen1∗, Yu Li2 ∗, Ramzan Umarov2, Xin Gao2,†, Le Song1,3,†
1Georgia Tech 2KAUST 3Ant Financial
xinshi.chen@gatech.edu
{yu.li;ramzan.umarov;xin.gao}@kaust.edu.sa
lsong@cc.gatech.edu

ABSTRACT

In this paper, we propose an end-to-end deep learning model, called E2Efold, for
RNA secondary structure prediction which can effectively take into account the
inherent constraints in the problem. The key idea of E2Efold is to directly pre-
dict the RNA base-pairing matrix, and use an unrolled algorithm for constrained
programming as the template for deep architectures to enforce constraints. With
comprehensive experiments on benchmark datasets, we demonstrate the superior
performance of E2Efold: it predicts signiﬁcantly better structures compared to
previous SOTA (especially for pseudoknotted structures), while being as efﬁcient
as the fastest algorithms in terms of inference time.

1

INTRODUCTION

Ribonucleic acid (RNA) is a molecule playing essential roles
in numerous cellular processes and regulating expression of
genes (Crick, 1970). It consists of an ordered sequence of nu-
cleotides, with each nucleotide containing one of four bases:
Adenine (A), Guanine (G), Cytosine (C) and Uracile (U). This
sequence of bases can be represented as

x := (x1, . . . , xL) where xi ∈ {A, G, C, U },

which is known as the primary structure of RNA. The bases
can bond with one another to form a set of base-pairs, which
deﬁnes the secondary structure. A secondary structure can be
represented by a binary matrix A∗ where A∗
ij = 1 if the i, j-th
bases are paired (Fig 1). Discovering the secondary structure of RNA is important for understanding
functions of RNA since the structure essentially affects the interaction and reaction between RNA
and other cellular components. Although secondary structure can be determined by experimental
assays (e.g. X-ray diffraction), it is slow, expensive and technically challenging. Therefore, compu-
tational prediction of RNA secondary structure becomes an important task in RNA research and is
useful in many applications such as drug design (Iorns et al., 2007).

Figure 1: Graph and matrix represen-
tations of RNA secondary structure.

Research on computational prediction of RNA secondary
structure from knowledge of primary structure has been
carried out for decades. Most existing methods assume
the secondary structure is a result of energy minimiza-
tion, i.e., A∗ = arg minA Ex(A). The energy function
is either estimated by physics-based thermodynamic ex-
periments (Lorenz et al., 2011; Bellaousov et al., 2013;
Markham & Zuker, 2008) or learned from data (Do et al.,
2006). These approaches are faced with a common problem that the search space of all valid sec-
ondary structures is exponentially-large with respect to the length L of the sequence. To make the
minimization tractable, it is often assumed the base-pairing has a nested structure (Fig 2 left), and

Figure 2: Nested and non-nested structures.

∗Equal contribution. †Co-corresponding.

1

GGGAAACGUUCCGGG1G1A1AACGUU1C1C1Gmatrix representation(ii) Pseudo-knot(i) Nested Structure 
 
 
 
 
 
Published as a conference paper at ICLR 2020

the energy function factorizes pairwisely. With this assumption, dynamic programming (DP) based
algorithms can iteratively ﬁnd the optimal structure for subsequences and thus consider an enormous
number of structures in time O(L3).

Although DP-based algorithms have dominated RNA structure prediction, it is notable that they
restrict the search space to nested structures, which excludes some valid yet biologically important
RNA secondary structures that contain ‘pseudoknots’, i.e., elements with at least two non-nested
base-pairs (Fig 2 right). Pseudoknots make up roughly 1.4% of base-pairs (Mathews & Turner,
2006), and are overrepresented in functionally important regions (Hajdin et al., 2013; Staple &
Butcher, 2005). Furthermore, pseudoknots are present in around 40% of the RNAs. They also assist
folding into 3D structures (Fechter et al., 2001) and thus should not be ignored. To predict RNA
structures with pseudoknots, energy-based methods need to run more computationally intensive
algorithms to decode the structures.

In summary, in the presence of more complex structured output (i.e., pseudoknots), it is challenging
for energy-based approaches to simultaneously take into account the complex constraints while be-
ing efﬁcient. In this paper, we adopt a different viewpoint by assuming that the secondary structure
is the output of a feed-forward function, i.e., A∗ = Fθ(x), and propose to learn θ from data in
an end-to-end fashion. It avoids the second minimization step needed in energy function based ap-
proach, and does not require the output structure to be nested. Furthermore, the feed-forward model
can be ﬁtted by directly optimizing the loss that one is interested in.

Despite the above advantages of using a feed-forward model, the architecture design is challenging.
To be more concrete, in the RNA case, Fθ is difﬁcult to design for the following reasons:

(i) RNA secondary structure needs to obey certain hard constraints (see details in Section 3),
which means certain kinds of pairings cannot occur at all (Steeg, 1993). Ideally, the output of
Fθ needs to satisfy these constraints.

(ii) The number of RNA data points is limited, so we cannot expect that a naive fully connected
network can learn the predictive information and constraints directly from data. Thus, inductive
biases need to be encoded into the network architecture.

(iii) One may take a two-step approach, where a post-processing step can be carried out to enforce
the constraints when Fθ predicts an invalid structure. However, in this design, the deep network
trained in the ﬁrst stage is unaware of the post-processing stage, making less effective use of
the potential prior knowledge encoded in the constraints.

In this paper, we present an end-to-end deep learning solution
which integrates the two stages. The ﬁrst part of the archi-
tecture is a transformer-based deep model called Deep Score
Network which represents sequence information useful for
structure prediction. The second part is a multilayer network
called Post-Processing Network which gradually enforces the
constraints and restrict the output space. It is designed based
on an unrolled algorithm for solving a constrained optimiza-
tion. These two networks are coupled together and learned
jointly in an end-to-end fashion. Therefore, we call our model E2Efold.

Figure 3: Output space of E2Efold.

By using an unrolled algorithm as the inductive bias to design Post-Processing Network, the output
space of E2Efold is constrained (illustrated in Fig 3), which makes it easier to learn a good model
in the case of limited data and also reduces the overﬁtting issue. Yet, the constraints encoded in
E2Efold are ﬂexible enough such that pseudoknots are not excluded. In summary, E2Efold strikes a
nice balance between model biases for learning and expressiveness for valid RNA structures.

We conduct extensive experiments to compare E2Efold with state-of-the-art (SOTA) methods on
several RNA benchmark datasets, showing superior performance of E2Efold including:

• being able to predict valid RNA secondary structures including pseudoknots;
• running as efﬁcient as the fastest algorithm in terms of inference time;
• producing structures that are visually close to the true structure;
• better than previous SOTA in terms of F1 score, precision and recall.

Although in this paper we focus on RNA secondary structure prediction, which presents an impor-
tant and concrete problem where E2Efold leads to signiﬁcant improvements, our method is generic

2

All Binary StructuresOutput Space of E2Efold*All Valid Structures*NestedStructureswith constraints(DP applicable)Published as a conference paper at ICLR 2020

and can be applied to other problems where constraints need to be enforced or prior knowledge is
provided. We imagine that our design idea of learning unrolled algorithm to enforce constraints can
also be transferred to problems such as protein folding and natural language understanding problems
(e.g., building correspondence structure between different parts in a document).

2 RELATED WORK

Classical RNA folding methods identify candidate structures for an RNA sequence energy min-
imization through DP and rely on thousands of experimentally-measured thermodynamic parame-
ters. A few widely used methods such as RNAstructure (Bellaousov et al., 2013), Vienna RNAfold
(Lorenz et al., 2011) and UNAFold (Markham & Zuker, 2008) adpoted this approach. These meth-
ods typically scale as O(L3) in time and O(L2) in storage (Mathews, 2006), making them slow for
long sequences. A recent advance called LinearFold (Huang et al., 2019) achieved linear run time
O(L) by applying beam search, but it can not handle pseudoknots in RNA structures. The prediction
of lowest free energy structures with pseudoknots is NP-complete (Lyngsø & Pedersen, 2000), so
pseudoknots are not considered in most algorithms. Heuristic algorithms such as HotKnots (An-
dronescu et al., 2010) and Probknots (Bellaousov & Mathews, 2010) have been made to predict
structures with pseudoknots, but the predictive accuracy and efﬁciency still need to be improved.

Learning-based RNA folding methods such as ContraFold (Do et al., 2006) and ContextFold (Za-
kov et al., 2011) have been proposed for energy parameters estimation due to the increasing avail-
ability of known RNA structures, resulting in higher prediction accuracies, but these methods still
rely on the above DP-based algorithms for energy minimization. A recent deep learning model,
CDPfold (Zhang et al., 2019), applied convolutional neural networks to predict base-pairings, but it
adopts the dot-bracket representation for RNA secondary structure, which can not represent pseu-
doknotted structures. Moreover, it requires a DP-based post-processing step whose computational
complexity is prohibitive for sequences longer than a few hundreds.

Learning with differentiable algorithms is a useful idea that inspires a series of works (Hershey
et al., 2014; Belanger et al., 2017; Ingraham et al., 2018; Chen et al., 2018; Shrivastava et al., 2019),
which shared similar idea of using differentiable unrolled algorithms as a building block in neural
architectures. Some models are also applied to structured prediction problems (Hershey et al., 2014;
Pillutla et al., 2018; Ingraham et al., 2018), but they did not consider the challenging RNA sec-
ondary structure problem or discuss how to properly incorporating constraints into the architecture.
OptNet (Amos & Kolter, 2017) integrates constraints by differentiating KKT conditions, but it has
cubic complexity in the number of variables and constraints, which is prohibitive for the RNA case.

Dependency parsing in NLP is a different but related problem to RNA folding. It predicts the de-
pendency between the words in a sentence. Similar to nested/non-nested structures, the correspond-
ing terms in NLP are projective/non-projective parsing, where most works focus on the former and
DP-based inference algorithms are commonly used (McDonald et al., 2005). Deep learning mod-
els (Dozat & Manning, 2016; Kiperwasser & Goldberg, 2016) are proposed to proposed to score the
dependency between words, which has a similar ﬂavor to the Deep Score Network in our work.

3 RNA SECONDARY STRUCTURE PREDICTION PROBLEM

In the RNA secondary structure prediction problem, the input is the ordered sequence of bases
x = (x1, . . . , xL) and the output is the RNA secondary structure represented by a matrix A∗ ∈
{0, 1}L×L. Hard constraints on the forming of an RNA secondary structure dictate that certain
kinds of pairings cannot occur at all (Steeg, 1993). Formally, these constraints are:

(i) Only three types of nucleotides combinations, B := {AU, U A}∪

{GC, CG} ∪ {GU, U G}, can form base-pairs.

(ii) No sharp loops are allowed.

(iii) There is no overlap of pairs, i.e., it is a matching.

/∈ B,

∀i, j,
if xixj
then Aij = 0.
∀|i − j| < 4, Aij = 0.
∀i, (cid:80)L
j=1 Aij ≤ 1.

(i) and (ii) prevent pairing of certain base-pairs based on their types and relative locations. Incorpo-
rating these two constraints can help the model exclude lots of illegal pairs. (iii) is a global constraint
among the entries of A∗.

3

Published as a conference paper at ICLR 2020

The space of all valid secondary structures contains all symmetric matrices A ∈ {0, 1}L×L that
satisfy the above three constraints. This space is much smaller than the space of all binary matrices
{0, 1}L×L. Therefore, if we could incorporate these constraints in our deep model, the reduced
output space could help us train a better predictive model with less training data. We do this by
using an unrolled algorithm as the inductive bias to design deep architecture.

4 E2EFOLD: DEEP LEARNING MODEL BASED ON UNROLLED ALGORITHM

In the literature on feed-forward networks for structured prediction, most models are designed using
traditional deep learning architectures. However, for RNA secondary structure prediction, directly
using these architectures does not work well due to the limited amount of RNA data points and the
hard constraints on forming an RNA secondary structure. These challenges motivate the design of
our E2Efold deep model, which combines a Deep Score Network with a Post-Processing Network
based on an unrolled algorithm for solving a constrained optimization problem.

4.1 DEEP SCORE NETWORK

The ﬁrst part of E2Efold is a Deep Score Network Uθ(x) whose output is an L × L symmetric
matrix. Each entry of this matrix, i.e., Uθ(x)ij, indicates the score of nucleotides xi and xj being
paired. The x input to the network here is the L × 4 dimensional one-hot embedding. The speciﬁc
architecture of Uθ is shown in Fig 4. It mainly consists of
• a position embedding matrix P which distinguishes {xi}L

i=1 by their exact and relative positions:
Pi = MLP(cid:0)ψ1(i), . . . , ψ(cid:96)(i), ψ(cid:96)+1(i/L), . . . , ψn(i/L)(cid:1), where {ψj} is a set of n feature maps
such as sin(·), poly(·), sigmoid(·), etc, and MLP(·) denotes multi-layer perceptions. Such posi-
tion embedding idea has been used in natural language modeling such as BERT (Devlin et al.,
2018), but we adapted for RNA sequence representation;

• a stack of Transformer Encoders (Vaswani et al., 2017) which encode the sequence information

and the global dependency between nucleotides;

• a 2D Convolution layers (Wang et al., 2017) for outputting the pairwise scores.

With the representation power of neural networks, the
hope is that we can learn an informative Uθ such that
higher scoring entries in Uθ(x) correspond well to ac-
tual paired bases in RNA structure. Once the score
matrix Uθ(x) is computed, a naive approach to use
it is to choose an offset term s ∈ R (e.g., s = 0)
and let Aij = 1 if Uθ(x)ij > s. However, such
entry-wise independent predictions of Aij may re-
sult in a matrix A that violates the constraints for a
valid RNA secondary structure. Therefore, a post-
processing step is needed to make sure the predicted
A is valid. This step could be carried out separately
after Uθ is learned. But such decoupling of base-pair
scoring and post-processing for constraints may lead
to sub-optimal results, where the errors in these two
stages can not be considered together and tuned to-
gether. Instead, we will introduce a Post-Processing
Network which can be trained end-to-end together
with Uθ to enforce the constraints.

4.2 POST-PROCESSING NETWORK

The second part of E2Efold is a Post-Processing Net-
work PPφ which is an unrolled and parameterized al-
gorithm for solving a constrained optimization prob-
lem. We ﬁrst present how we formulate the post-processing step as a constrained optimization prob-
lem and the algorithm for solving it. After that, we show how we use the algorithm as a template to
design deep architecture PPφ.

Figure 4: Architecture of Deep Score Network.

4

input   𝐿×4position 𝐿×2𝐿×𝑑	Multiply by  W∈ℝ*×+Position Embedding0100100010001000001001000001000100100010…GAAACGUUCC…0100100010001000001001000001000100100010…GAAACGUUCC…12345…1/L2/L3/L4/L5/L…feature map  𝜓-,…,𝜓0𝐿×𝑛	MLP𝐿×𝑑	Transformer EncoderTransformer EncoderTransformer Encoder𝐿×2𝑑	𝐿×2𝑑	Sequence Encoderconcat𝐿×3𝑑	pairwise concat𝐿×𝐿×6𝑑	2D Convolution2D Convolutionconcat𝐿×𝐿×1Output Layersscores UsymmetrizationPublished as a conference paper at ICLR 2020

4.2.1 POST-PROCESSING WITH CONSTRAINED OPTIMIZATION

(cid:80)

Formulation of constrained optimization. Given the scores predicted by Uθ(x), we deﬁne the
total score 1
i,j(Uθ(x)ij − s)Aij as the objective to maximize, where s is an offset term. Clearly,
2
without structure constraints, the optimal solution is to take Aij = 1 when Uθ(x)ij > s. Intu-
itively, the objective measures the covariation between the entries in the scoring matrix and the
A matrix. With constraints, the exact maximization becomes intractable. To make it tractable,
we consider a convex relaxation of this discrete optimization to a continuous one by allowing
Aij ∈ [0, 1]. Consequently, the solution space that we consider to optimize over is A(x) :=
(cid:8)A ∈ [0, 1]L×L | A is symmetric and satisﬁes constraints (i)-(iii) in Section 3(cid:9) .

To further simplify the search space, we deﬁne a nonlinear transformation T on RL×L as T ( ˆA) :=
(cid:0) ˆA ◦ ˆA + ( ˆA ◦ ˆA)(cid:62)(cid:1) ◦ M (x), where ◦ denotes element-wise multiplication. Matrix M is deﬁned as
1
2
M (x)ij := 1 if xixj ∈ B and also |i − j| ≥ 4, and M (x)ij := 0 otherwise. From this deﬁnition we
can see that M (x) encodes both constraint (i) and (ii). With transformation T , the resulting matrix
is non-negative, symmetric, and satisﬁes constraint (i) and (ii). Hence, by deﬁning A := T ( ˆA), the
solution space is simpliﬁed as A(x) = {A = T ( ˆA) | ˆA ∈ RL×L, A1 ≤ 1}.
Finally, we introduce a (cid:96)1 penalty term (cid:107) ˆA(cid:107)1 := (cid:80)
i,j | ˆAij| to make A sparse and formulate the
post-processing step as: ((cid:104)·, ·(cid:105) denotes matrix inner product, i.e., sum of entry-wise multiplication)

max ˆA∈RL×L

1
2

(cid:69)
(cid:68)
Uθ(x) − s, A := T ( ˆA)

− ρ(cid:107) ˆA(cid:107)1

s.t. A1 ≤ 1

(1)

The advantages of this formulation are that the variables ˆAij are free variables in R and there are
only L inequality constraints A1 ≤ 1. This system of linear inequalities can be replaced by a set
of nonlinear equalities relu(A1 − 1) = 0 so that the constrained problem can be easily transformed
into an unconstrained problem by introducing a Lagrange multiplier λ ∈ RL
+:

min
λ≥0

max
ˆA∈RL×L

1
2 (cid:104)Uθ(x) − s, A(cid:105) − (cid:104)λ, relu(A1 − 1)(cid:105)
(cid:123)(cid:122)
(cid:125)
(cid:124)
f

−ρ(cid:107) ˆA(cid:107)1.

(2)

Algorithm for solving it. We use a primal-dual method for solving Eq. 2 (derived in Appendix B).
In each iteration, ˆA and λ are updated alternatively by:

(primal) gradient step:

˙At+1 ← ˆAt + α · γt
(cid:26)∂f /∂At = 1

(cid:16)

∂f /∂At + (∂f /∂At)(cid:62)(cid:17)
α · ˆAt ◦ M (x) ◦
2 (Uθ(x) − s) − (λ ◦ sign(At1 − 1)) 1(cid:62),

where

sign(c) := 1 when c > 0 and 0 otherwise,

(primal) soft threshold:

ˆAt+1 ← relu(| ˙At+1| − ρ · α · γt

α), At+1 ← T ( ˆAt+1),

(dual) gradient step: λt+1 ← λt+1 + β · γt

β · relu(At+11 − 1),

,

(3)

(4)

(5)

(6)

where α, β are step sizes and γα, γβ are decaying coefﬁcients. When it converges at T , an approx-
imate solution Round(cid:0)AT = T ( ˆAT )(cid:1) is obtained. With this algorithm operated on the learned
Uθ(x), even if this step is disconnected to the training phase of Uθ(x), the ﬁnal prediction works
much better than many other existing methods (as reported in Section 6). Next, we introduce how
to couple this post-processing step with the training of Uθ(x) to further improve the performance.

4.2.2 POST-PROCESSING NETWORK VIA AN UNROLLED ALGORITHM

We design a Post-Processing Network, denoted by PPφ, based on the above algorithm. After it is
deﬁned, we can connect it with the deep score network Uθ and train them jointly in an end-to-end
fashion, so that the training phase of Uθ(x) is aware of the post-processing step.

5

Published as a conference paper at ICLR 2020

Algorithm 1: Post-Processing Network PPφ(U, M )
Parameters φ := {w, s, α, β, γα, γβ, ρ}
U ← softsign(U − s) ◦ U
ˆA0 ← softsign(U − s) ◦ sigmoid(U )
A0 ← T ( ˆA0); λ0 ← w · relu(A01 − 1)
For t = 0, . . . , T − 1 do

λt+1, At+1, ˆAt+1 = PPcellφ(U, M, λt, At, ˆAt, t)

return {At}T

t=1

Algorithm 2: Neural Cell PPcellφ
Function PPcellφ(U, M, λ, A, ˆA, t):

2 U − (λ ◦ softsign(A1 − 1)) 1(cid:62)
t · ˆA ◦ M ◦ (G + G(cid:62))
t)

G ← 1
˙A ← ˆA + α · γα
ˆA ← relu(| ˙A| − ρ · α · γα
ˆA ← 1 − relu(1 − ˆA) [i.e.,min( ˆA, 1)]
A ← T ( ˆA); λ ← λ+β·γβ
return λ, A, ˆA

t·relu(A1−1)

The speciﬁc computation graph of PPφ is given in Algorithm 1, whose main component is a recurrent
cell which we call PPcellφ. The computation graph is almost the same as the iterative update from
Eq. 3 to Eq. 6, except for several modiﬁcations:

• (learnable hyperparameters) The hyperparameters including step sizes α, β, decaying rate γα, γβ,
sparsity coefﬁcient ρ and the offset term s are treated as learnable parameters in φ, so that there
is no need to tune the hyperparameters by hand but automatically learn them from data instead.
• (ﬁxed # iterations) Instead of running the iterative updates until convergence, PPcellφ is applied
recursively for T iterations where T is a manually ﬁxed number. This is why in Fig 3 the output
space of E2Efold is slightly larger than the true solution space.

• (smoothed sign function) Resulted from the gradient of relu(·), the update step in Eq. 4 contains a
sign(·) function. However, to push gradient through PPφ, we require a differentiable update step.
Therefore, we use a smoothed sign function deﬁned as softsign(c) := 1/(1 + exp(−kc)), where
k is a temperature.

• (clip ˆA) An additional step, ˆA ← min( ˆA, 1), is included to make the output At at each itera-
tion stay in the range [0, 1]L×L. This is useful for computing the loss over intermediate results
{At}T

t=1, for which we will explain more in Section 5.

With these modiﬁcations, the Post-Processing Network PPφ is a tuning-free and differentiable un-
rolled algorithm with meaningful intermediate outputs. Combining it with the deep score network,
the ﬁnal deep model is

E2Efold :

{At}T

t=1 =

(cid:122)
PPφ(

Post-Process Network
(cid:125)(cid:124)
Uθ(x)
(cid:124) (cid:123)(cid:122) (cid:125)
Deep Score Network

(cid:123)
, M (x)) .

(7)

5 END-TO-END TRAINING ALGORITHM

Given a dataset D containing examples of input-output pairs (x, A∗), the training procedure of
E2Efold is similar to standard gradient-based supervised learning. However, for RNA secondary
structure prediction problems, commonly used metrics for evaluating predictive performances are
F1 score, precision and recall, which are non-differentiable.

Differentiable F1 Loss. To directly optimize these metrics, we mimic true positive (TP), false posi-
tive (FP), true negative (TN) and false negative (FN) by deﬁning continuous functions on [0, 1]L×L:

TP = (cid:104)A, A∗(cid:105), FP = (cid:104)A, 1 − A∗(cid:105), FN = (cid:104)1 − A, A∗(cid:105), TN = (cid:104)1 − A, 1 − A∗(cid:105).

Since F1 = 2TP/(2TP + FP + FN), we deﬁne a loss function to mimic the negative of F1 score as:
L−F1(A, A∗) := −2(cid:104)A, A∗(cid:105)/ (2(cid:104)A, A∗(cid:105) + (cid:104)A, 1 − A∗(cid:105) + (cid:104)1 − A, A∗(cid:105)) .

(8)

Assuming that (cid:80)
recall losses can be deﬁned in a similar way, but we optimize F1 score in this paper.

ij (cid:54)= 0, this loss is well-deﬁned and differentiable on [0, 1]L×L. Precision and

ij A∗

It is notable that this F1 loss takes advantages over other differentiable losses including (cid:96)2 and
cross-entropy losses, because there are much more negative samples (i.e. Aij = 0) than positive
samples (i.e. Aij = 1). A hand-tuned weight is needed to balance them while using (cid:96)2 or cross-
entropy losses, but F1 loss handles this issue automatically, which can be useful for a number of
problems (Wang et al., 2016; Li et al., 2017).

6

Published as a conference paper at ICLR 2020

Overall Loss Function. As noted earlier, E2Efold outputs a matrix At ∈ [0, 1]L×L in each itera-
tion. This allows us to add auxiliary losses to regularize the intermediate results, guiding it to learn
parameters which can generate a smooth solution trajectory. More speciﬁcally, we use an objective
that depends on the entire trajectory of optimization:

min
θ,φ

1
|D|

(cid:88)

(x,A∗)∈D

1
T

T
(cid:88)

t=1

γT −tL−F1(At, A∗),

(9)

where {At}T
t=1 = PPφ(Uθ(x), M (x)) and γ ≤ 1 is a discounting factor. Empirically, we ﬁnd it
very useful to pre-train Uθ using logistic regression loss. Also, it is helpful to add this additional
loss to Eq. 9 as a regularization.

6 EXPERIMENTS

We compare E2Efold with the SOTA and also the most commonly used methods in the RNA sec-
ondary structure prediction ﬁeld on two benchmark datasets. It is revealed from the experimental
results that E2Efold achieves 29.7% improvement in terms of F1 score on RNAstralign dataset and
it infers the RNA secondary structure as fast as the most efﬁcient algorithm (LinearFold) among ex-
isting ones. An ablation study is also conducted to show the necessity of pushing gradient through
the post-processing step. The codes for reproducing the experimental results are released.1

Dataset. We use two benchmark datasets: (i) ArchiveII
(Sloma & Mathews, 2016), containing 3975 RNA struc-
tures from 10 RNA types, is a widely used benchmark
dataset for classical RNA folding methods. (ii) RNAS-
tralign (Tan et al., 2017), composed of 37149 structures
from 8 RNA types, is one of the most comprehensive col-
lections of RNA structures in the market. After removing
redundant sequences and structures, 30451 structures re-
main. See Table 1 for statistics about these two datasets.

Table 1: Dataset Statistics

Type

ArchiveII

RNAStralign

length #samples

length #samples

28∼2968
All
16SrRNA 73∼1995
5SrRNA 102∼135
54∼93
tRNA
210∼736
grp1
28∼533
SRP
102∼437
tmRNA
120∼486
RNaseP
telomerase 382∼559
23SrRNA 242∼2968
619∼780
grp2

3975
110
1283
557
98
928
462
454
37
35
11

30∼1851 30451
54∼1851 11620
104∼132 9385
59∼95
6443
163∼615 1502
30∼553
468
102∼437
572
189∼486
434
382∼559
37
-
-
-
-

Experiments On RNAStralign. We divide RNAStralign
dataset into training, testing and validation sets by strat-
iﬁed sampling (see details in Table 7 and Fig 6), so that
each set contains all RNA types. We compare the performance of E2Efold to six methods includ-
ing CDPfold, LinearFold, Mfold, RNAstructure (ProbKnot), RNAfold and CONTRAfold. Both
E2Efold and CDPfold are learned from the same training/validation sets. For other methods, we
directly use the provided packages or web-servers to generate predicted structures. We evaluate the
F1 score, Precision and Recall for each sequence in the test set. Averaged values are reported in
Table 2. As suggested by Mathews (2019), for a base pair (i, j), the following predictions are also
considered as correct: (i + 1, j), (i − 1, j), (i, j + 1), (i, j − 1), so we also reported the metrics when
one-position shift is allowed.

Table 2: Results on RNAStralign test set. “(S)” indi-
cates the results when one-position shift is allowed.

Method

Prec Rec

F1

Prec(S) Rec(S) F1(S)

E2Efold
CDPfold
LinearFold
Mfold

0.866 0.788 0.821 0.880 0.798 0.833
0.633 0.597 0.614 0.720 0.677 0.697
0.620 0.606 0.609 0.635 0.622 0.624
0.450 0.398 0.420 0.463 0.409 0.433
RNAstructure 0.537 0.568 0.550 0.559 0.592 0.573
0.516 0.568 0.540 0.533 0.587 0.558
CONTRAfold 0.608 0.663 0.633 0.624 0.681 0.650

RNAfold

Figure 5: Distribution of F1 score.

As shown in Table 2, traditional methods can achieve a F1 score ranging from 0.433 to 0.624,
which is consistent with the performance reported with their original papers. The two learning-based
methods, CONTRAfold and CDPfold, can outperform classical methods with reasonable margin on

1The codes for reproducing the experimental results are released at https://github.com/ml4bio/e2efold.

7

Published as a conference paper at ICLR 2020

some criteria. E2Efold, on the other hand, signiﬁcantly outperforms all previous methods across all
criteria, with at least 20% improvement. Notice that, for almost all the other methods, the recall is
usually higher than precision, while for E2Efold, the precision is higher than recall. That can be the
result of incorporating constraints during neural network training. Fig 5 shows the distributions of
F1 scores for each method. It suggests that E2Efold has consistently good performance.

To estimate the performance of E2Efold on long sequences, we also compute the F1 scores weighted
by the length of sequences, such that the results are more dominated by longer sequences. Detailed
results are given in Appendix D.3.

Table 3: Performance comparison on ArchiveII

Table 4: Inference time on RNAStralign

Method

Prec Rec

F1

Prec(S) Rec(S) F1(S)

Method

total run time time per seq

E2Efold
CDPfold
LinearFold
Mfold

0.734 0.66 0.686 0.758 0.676 0.704
0.557 0.535 0.545 0.612 0.585 0.597
0.641 0.617 0.621 0.668 0.644 0.647
0.428 0.383 0.401 0.450 0.403 0.421
RNAstructure 0.563 0.615 0.585 0.590 0.645 0.613
0.565 0.627 0.592 0.586 0.652 0.615
CONTRAfold 0.607 0.679 0.638 0.629 0.705 0.662

RNAfold

0.40s

19m (GPU)

E2Efold (Pytorch)
CDPfold (Pytorch) 440m*32 threads 300.107s
(C)
LinearFold
Mfold
(C)
RNAstructure (C)
RNAfold
(C)
CONTRAfold (C)

0.43s
7.65s
142.02s
0.55s
30.58s

20m
360m
3 days
26m
1 day

Test On ArchiveII Without Re-training. To mimic the real world scenario where the users want to
predict newly discovered RNA’s structures which may have a distribution different from the training
dataset, we directly test the model learned from RNAStralign training set on the ArchiveII dataset,
without re-training the model. To make the comparison fair, we exclude sequences that are over-
lapped with the RNAStralign dataset. We then test the model on sequences in ArchiveII that have
overlapping RNA types (5SrRNA, 16SrRNA, etc) with the RNAStralign dataset. Results are shown
in Table 3. It is understandable that the performances of classical methods which are not learning-
based are consistent with that on RNAStralign. The performance of E2Efold, though is not as good
as that on RNAStralign, is still better than all the other methods across different evaluation crite-
ria. In addition, since the original ArchiveII dataset contains domain sequences (subsequences), we
remove the domains and report the results in Appendix D.4, which are similar to results in Table 3.

Inference Time Comparison. We record the running time of all algorithms for predicting RNA
secondary structures on the RNAStralign test set, which is summarized in Table 4. LinearFold is the
most efﬁcient among baselines because it uses beam pruning heuristic to accelerate DP. CDPfold,
which achieves higher F1 score than other baselines, however, is extremely slow due to its DP
post-processing step. Since we use a gradient-based algorithm which is simple to design the Post-
Processing Network, E2Efold is fast. On GPU, E2Efold has similar inference time as LinearFold.

Table 5: Evaluation of pseudoknot prediction

Pseudoknot Prediction. Even though E2Efold does
not exclude pseudoknots, it is not sure whether it ac-
tually generates pseudoknotted structures. Therefore,
we pick all sequences containing pseudoknots and com-
pute the averaged F1 score only on this set. Besides, we
count the number of pseudoknotted sequences that are
predicted as pseudoknotted and report this count as true positive (TP). Similarly we report TN, FP
and FN in Table 5 along with the F1 score. Most tools exclude pseudoknots while RNAstructure is
the most famous one that can predict pseudoknots, so we choose it for comparison.

RNAstructure 0.472 1248 307 983 286

Set F1 TP FP TN FN

0.710 1312 242 1271 0

E2Efold

Method

Visualization. We visualize predicted structures of three
RNA sequences in the main text. More examples are
provided in appendix (Fig 8 to 14).
In these ﬁgures,
purple lines indicate edges of pseudoknotted elements.
Although CDPfold has higher F1 score than other base-
lines, its predictions are visually far from the ground-
truth. Instead, RNAstructure and CONTRAfold produce
comparatively more reasonable visualizations among all baselines, so we compare with them. These

8

E2EfoldRNAstructureCONTRAfoldtrue structureRNAstructureCONTRAfoldE2Efoldtrue structuretrue structureE2Efoldtrue structureE2EfoldPublished as a conference paper at ICLR 2020

two methods can capture a rough sketch of the structure, but not good enough. For most cases,
E2Efold produces structures most similar to the ground-truths. Moreover, it works surprisingly well
for some RNA sequences that are long and very difﬁcult to predict.

Ablation Study. To exam whether integrating the
two stages by pushing gradient through the post-
process is necessary for performance of E2Efold, we
conduct an ablation study (Table 6). We test the per-
formance when the post-processing step is discon-
nected with the training of Deep Score Network Uθ. We apply the post-processing step (i.e., for
solving augmented Lagrangian) after Uθ is learned (thus the notation “Uθ + PP” in Table 6). Al-
though “Uθ + PP” performs decently well, with constraints incorporated into training, E2Efold still
has signiﬁcant advantages over it.

Table 6: Ablation study (RNAStralign test set)
F1 Prec(S) Rec(S) F1(S)
Method Prec Rec

E2Efold 0.866 0.788 0.821 0.880 0.798 0.833
Uθ+PP 0.755 0.712 0.721 0.782 0.737 0.752

Discussion. To better estimate the performance of E2Efold on different RNA types, we include the
per-family F1 scores in Appendix D.5. E2Efold performs signiﬁcantly better than other methods
in 16S rRNA, tRNA, 5S RNA, tmRNA, and telomerase. These results are from a single model. In
the future, we can view it as multi-task learning and further improve the performance by learning
multiple models for different RNA families and learning an additional classiﬁer to predict which
model to use for the input sequence.

7 CONCLUSION

We propose a novel DL model, E2Efold, for RNA secondary structure prediction, which incorpo-
rates hard constraints in its architecture design. Comprehensive experiments are conducted to show
the superior performance of E2Efold, no matter on quantitative criteria, running time, or visualiza-
tion. Further studies need to be conducted to deal with the RNA types with less samples. Finally, we
believe the idea of unrolling constrained programming and pushing gradient through post-processing
can be generic and useful for other constrained structured prediction problems.

ACKNOWLEDGEMENT

We would like to thank anonymous reviewers for providing constructive feedbacks. This work is
supported in part by NSF grants CDS&E-1900017 D3SC, CCF-1836936 FMitF, IIS-1841351, CA-
REER IIS-1350983 to L.S. and grants from King Abdullah University of Science and Technology,
under award numbers BAS/1/1624-01, FCC/1/1976-18-01, FCC/1/1976-23-01, FCC/1/1976-25-01,
FCC/1/1976-26-01, REI/1/0018-01-01, and URF/1/4098-01-01.

REFERENCES

Brandon Amos and J Zico Kolter. Optnet: Differentiable optimization as a layer in neural networks.
In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 136–
145. JMLR. org, 2017.

Mirela S Andronescu, Cristina Pop, and Anne E Condon. Improved free energy parameters for RNA

pseudoknotted secondary structure prediction. RNA, 16(1):26–42, 2010.

David Belanger, Bishan Yang, and Andrew McCallum. End-to-end learning for structured prediction
In Proceedings of the 34th International Conference on Machine Learning-

energy networks.
Volume 70, pp. 429–439. JMLR. org, 2017.

Stanislav Bellaousov and David H Mathews. Probknot: fast prediction of RNA secondary structure

including pseudoknots. RNA, 16(10):1870–1880, 2010.

Stanislav Bellaousov, Jessica S Reuter, Matthew G Seetin, and David H Mathews. RNAstructure:
web servers for RNA secondary structure prediction and analysis. Nucleic acids research, 41
(W1):W471–W474, 2013.

Xiaohan Chen, Jialin Liu, Zhangyang Wang, and Wotao Yin. Theoretical linear convergence of un-
folded ista and its practical weights and thresholds. In Advances in Neural Information Processing
Systems, pp. 9061–9071, 2018.

9

Published as a conference paper at ICLR 2020

Francis Crick. Central dogma of molecular biology. Nature, 227(5258):561, 1970.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

Chuong B Do, Daniel A Woods, and Seraﬁm Batzoglou. Contrafold: RNA secondary structure

prediction without physics-based models. Bioinformatics, 22(14):e90–e98, 2006.

Timothy Dozat and Christopher D Manning. Deep biafﬁne attention for neural dependency parsing.

arXiv preprint arXiv:1611.01734, 2016.

P Fechter, J Rudinger-Thirion, C Florentz, and R Giege. Novel features in the tRNA-like world of

plant viral RNAs. Cellular and Molecular Life Sciences CMLS, 58(11):1547–1561, 2001.

Christine E Hajdin, Stanislav Bellaousov, Wayne Huggins, Christopher W Leonard, David H Math-
ews, and Kevin M Weeks. Accurate shape-directed RNA secondary structure modeling, including
pseudoknots. Proceedings of the National Academy of Sciences, 110(14):5498–5503, 2013.

John R Hershey, Jonathan Le Roux, and Felix Weninger. Deep unfolding: Model-based inspiration

of novel deep architectures. arXiv preprint arXiv:1409.2574, 2014.

Liang Huang, He Zhang, Dezhong Deng, Kai Zhao, Kaibo Liu, David A Hendrix, and David H
Mathews. Linearfold: linear-time approximate RNA folding by 5’-to-3’dynamic programming
and beam search. Bioinformatics, 35(14):i295–i304, 2019.

John Ingraham, Adam Riesselman, Chris Sander, and Debora Marks. Learning protein structure

with a differentiable simulator. 2018.

Elizabeth Iorns, Christopher J Lord, Nicholas Turner, and Alan Ashworth. Utilizing RNA interfer-

ence to enhance cancer drug discovery. Nature reviews Drug discovery, 6(7):556, 2007.

Eliyahu Kiperwasser and Yoav Goldberg. Simple and accurate dependency parsing using bidirec-
tional lstm feature representations. Transactions of the Association for Computational Linguistics,
4:313–327, 2016.

Yu Li, Sheng Wang, Ramzan Umarov, Bingqing Xie, Ming Fan, Lihua Li, and Xin Gao. Deepre:
sequence-based enzyme ec number prediction by deep learning. Bioinformatics, 34(5):760–769,
2017.

Ronny Lorenz, Stephan H Bernhart, Christian H¨oner Zu Siederdissen, Hakim Tafer, Christoph
Flamm, Peter F Stadler, and Ivo L Hofacker. ViennaRNA package 2.0. Algorithms for molecular
biology, 6(1):26, 2011.

Rune B Lyngsø and Christian NS Pedersen. RNA pseudoknot prediction in energy-based models.

Journal of computational biology, 7(3-4):409–427, 2000.

NR Markham and M Zuker. Unafold: software for nucleic acid folding and hybridization in: Keith

jm, editor.(ed.) bioinformatics methods in molecular biology, vol. 453, 2008.

David H Mathews. Predicting RNA secondary structure by free energy minimization. Theoretical

Chemistry Accounts, 116(1-3):160–168, 2006.

David H Mathews. How to benchmark RNA secondary structure prediction accuracy. Methods,

2019.

David H Mathews and Douglas H Turner. Prediction of RNA secondary structure by free energy

minimization. Current opinion in structural biology, 16(3):270–278, 2006.

Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajiˇc. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language Processing, pp. 523–530. Association for
Computational Linguistics, 2005.

10

Published as a conference paper at ICLR 2020

Venkata Krishna Pillutla, Vincent Roulet, Sham M Kakade, and Zaid Harchaoui. A smoother way
to train structured prediction models. In Advances in Neural Information Processing Systems, pp.
4766–4778, 2018.

Harsh Shrivastava, Xinshi Chen, Binghong Chen, Guanghui Lan, Srinvas Aluru, and Le Song. Glad:

Learning sparse graph recovery. arXiv preprint arXiv:1906.00271, 2019.

Michael F Sloma and David H Mathews. Exact calculation of loop formation probability identiﬁes

folding motifs in RNA secondary structures. RNA, 22(12):1808–1818, 2016.

David W Staple and Samuel E Butcher. Pseudoknots: RNA structures with diverse functions. PLoS

biology, 3(6):e213, 2005.

Evan W Steeg. Neural networks, adaptive optimization, and RNA secondary structure prediction.

Artiﬁcial intelligence and molecular biology, pp. 121–160, 1993.

Zhen Tan, Yinghan Fu, Gaurav Sharma, and David H Mathews. Turbofold ii: RNA structural
alignment and secondary structure prediction informed by multiple homologs. Nucleic acids
research, 45(20):11570–11581, 2017.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998–6008, 2017.

Sheng Wang, Siqi Sun, and Jinbo Xu. Auc-maximized deep convolutional neural ﬁelds for protein
sequence labeling. In Joint European Conference on Machine Learning and Knowledge Discovery
in Databases, pp. 1–16. Springer, 2016.

Sheng Wang, Siqi Sun, Zhen Li, Renyu Zhang, and Jinbo Xu. Accurate de novo prediction of protein
contact map by ultra-deep learning model. PLoS computational biology, 13(1):e1005324, 2017.

Shay Zakov, Yoav Goldberg, Michael Elhadad, and Michal Ziv-Ukelson. Rich parameterization
improves RNA structure prediction. Journal of Computational Biology, 18(11):1525–1542, 2011.

Hao Zhang, Chunhe Zhang, Zhi Li, Cong Li, Xu Wei, Borui Zhang, and Yuanning Liu. A new
method of RNA secondary structure prediction based on convolutional neural network and dy-
namic programming. Frontiers in genetics, 10, 2019.

11

Published as a conference paper at ICLR 2020

A MORE DISCUSSION ON RELATED WORKS

Here we explain the difference between our approach and other works on unrolling optimization
problems.

First, our view of incorporating constraints to reduce output space and to reduce sample complexity
is novel. Previous works (Hershey et al., 2014; Belanger et al., 2017; Ingraham et al., 2018) did not
discuss these aspects. The most related work which also integrates constraints is OptNet (Amos &
Kolter, 2017), but its very expensive and can not scale to the RNA problem. Therefore, our proposed
approach is a simple and effective one.

Second, compared to (Chen et al., 2018; Shrivastava et al., 2019), our approach has a different
purpose of using the algorithm. Their goal is to learn a better algorithm, so they commonly make
their architecture more ﬂexible than the original algorithm for the room of improvement. However,
we aim at enforcing constraints. To ensure that constraints are nicely incorporated, we keep the
original structure of the algorithm and only make the hyperparameters learnable.

Finally, although all works consider end-to-end training, none of them can directly optimize the F1
score. We proposed a differentiable loss function to mimic the F1 score/precision/recall, which is
effective and also very useful when negative samples are much fewer than positive samples (or the
inverse).

B DERIVATION OF THE PROXIMAL GRADIENT STEP

The maximization step in Eq. 1 can be written as the following minimization:

min
ˆA∈RL×L

− 1
(cid:124)

2 (cid:104)Uθ(x) − s, A(cid:105) + (cid:104)λ, relu(A1 − 1)(cid:105)
(cid:125)
(cid:123)(cid:122)
−f ( ˆA)

+ρ(cid:107) ˆA(cid:107)1.

Consider the quadratic approximation of −f ( ˆA) centered at ˆAt:

− ˜fα( ˆA) := − f ( ˆAt) + (cid:104)−

= − f ( ˆAt) +

1
2α

∂f
∂ ˆAt
(cid:13)
ˆA −
(cid:13)
(cid:13)

, ˆA − ˆAt(cid:105) +

(cid:18)

ˆAt + α

1
2α
∂f
∂ ˆAt

(cid:107) ˆA − ˆAt(cid:107)2
F

(cid:19) (cid:13)
2
(cid:13)
(cid:13)

F

,

and rewrite the optimization in Eq. 10 as

min
ˆA∈RL×L

− f ( ˆAt) +

≡ min

ˆA∈RL×L

1
2α

where

(cid:13)
ˆA − ˙At+1
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

F

+ ρ(cid:107) ˆA(cid:107)1

1
2α
(cid:13)
2
(cid:13)
(cid:13)

F

(cid:13)
ˆA − ˙At+1
(cid:13)
(cid:13)

+ ρ(cid:107) ˆA(cid:107)1,

Next, we deﬁne proximal mapping as a function depending on α as follows:

˙At+1 := ˆAt + α

∂f
∂ ˆAt

.

proxα( ˙At+1) = arg min
ˆA∈RL×L

= arg min
ˆA∈RL×L

1
2α

(cid:13)
ˆA − ˙At+1
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

F

+ ρ(cid:107) ˆA(cid:107)1

1
2

(cid:13)
ˆA − ˙At+1
(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

F

+ αρ(cid:107) ˆA(cid:107)1

= sign( ˙At+1) max(| ˙At+1| − αρ, 0)
= sign( ˙At+1)relu(| ˙At+1| − αρ).

(10)

(11)

(12)

(13)

(14)

(15)

(16)

(17)

(18)

(19)

Since we always use ˆA ◦ ˆA instead of ˆA in our problem, we can take the absolute value
|proxα( ˙At+1)| = relu(| ˙At+1| − αρ) without loss of generality. Therefore, the proximal gradient

12

Published as a conference paper at ICLR 2020

step is

˙At+1 ← ˆAt + α

∂f
∂ ˆAt

(correspond to Eq. 3)

ˆAt+1 ← relu(| ˙At+1| − αρ)

(correspond to Eq. 5).

More speciﬁcally, in the main text, we write ∂f
∂ ˆAt

as

∂f
∂ ˆAt

=

=

=

=

(cid:32)

1
2

∂f
∂At

+

(cid:18) 1
2

∂At
∂ ˆAt

(cid:19)

◦

∂f
∂At
(cid:32)

(cid:62)(cid:33)

◦

∂At
∂ ˆAt

∂f
∂At

(cid:62)(cid:33)

+

∂f
∂At

(cid:18) 1
22 ◦ M ◦ (2 ˆAt + 2 ˆA(cid:62)
t )
(cid:18) 1
22 ◦ M ◦ (2 ˆAt + 2 ˆA(cid:62)
t )
(cid:32)

= M ◦ ˆAt ◦

∂f
∂At

+

∂f
∂At

(cid:62)(cid:33)

.

(cid:32)

(cid:19)

◦

◦

(cid:19)

(cid:32)

∂f
∂At

∂f
∂At

+

+

∂f
∂At

∂f
∂At

(cid:62)(cid:33)

(cid:62)(cid:33)

(20)

(21)

(22)

(23)

(24)

(25)

(26)

The last equation holds since ˆAt will remain symmetric in our algorithm if the initial ˆA0 is symmet-
ric. Moreover, in the main text, α is replaced by α · γt
α.

C IMPLEMENTATION AND TRAINING DETAILS

We used Pytorch to implement the whole package of E2Efold.

Deep Score Network. In the deep score network, we used a hyper-parameter, d, which was set as
10 in the ﬁnal model, to control the model capacity. In the transformer encoder layers, we set the
number of heads as 2, the dimension of the feed-forward network as 2048, the dropout rate as 0.1.
As for the position encoding, we used 58 base functions to form the position feature map, which
goes through a 3-layer fully-connected neural network (the number of hidden neurons is 5 ∗ d) to
generate the ﬁnal position embedding, whose dimension is L by d. In the ﬁnal output layer, the
pairwise concatenation is carried out in the following way: Let X ∈ RL×3d be the input to the
ﬁnal output layers in Figure 4 (which is the concatenation of the sequence embedding and position
embedding). The pairwise concatenation results in a tensor Y ∈ RL×L×6d deﬁned as

Y (i, j, :) = [X(i, :), X(j, :)],

(27)

where Y (i, j, :) ∈ R6d, X(i, :) ∈ R3d, and X(j, :) ∈ R3d.

In the 2D convolution layers, the the channel of the feature map gradually change from 6∗d to d , and
ﬁnally to 1. We set the kernel size as 1 to translate the feature map into the ﬁnal score matrix. Each
2D convolution layer is followed by a batch normalization layer. We used ReLU as the activation
function within the whole score network.

Post-Processing Network. In the PP network, we initialized w as 1, s as log(9), α as 0.01, β as 0.1,
γα as 0.99, γβ as 0.99, and ρ as 1. We set T as 20.

Training details. During training, we ﬁrst pre-trained a deep score network and then ﬁne-tuned the
score network and the PP network together. To pre-train the score network, we used binary cross-
entropy loss and Adam optimizer. Since, in the contact map, most entries are 0, we used weighted
loss and set the positive sample weight as 300. The batch size was set to fully use the GPU memory,
which was 20 for the Titan Xp card. We pre-train the score network for 100 epochs. As for the
ﬁne-tuning, we used binary cross-entropy loss for the score network and F1 loss for the PP network
and summed up these two losses as the ﬁnal loss. The user can also choose to only use the F1 loss or

13

Published as a conference paper at ICLR 2020

use another coefﬁcient to weight the loss estimated on the score network Uθ. Due to the limitation of
the GPU memory, we set the batch size as 8. However, we updated the model’s parameters every 30
steps to stabilize the training process. We ﬁne-tuned the whole model for 20 epochs. Also, since the
data for different RNA families are imbalanced, we up-sampled the data in the small RNA families
based on their size. For the training of the score network Uθ in the ablation study, it is exactly the
same as the training of the above mentioned process. Except that during the ﬁne-tune process, there
is the unrolled number of iterations is set to be 0.

D MORE EXPERIMENTAL DETAILS

D.1 DATASET STATISTICS

Figure 6: The RNAStralign length distribution.

Table 7: RNAStralign dataset splits statistics

RNA type
16SrRNA
5SrRNA
tRNA
grp1
SRP
tmRNA
RNaseP
telomerase
RNAStralign

All
11620
9385
6443
1502
468
572
434
37
30451

Training Validation Testing
1150
1145
879
819
504
527
136
123
53
36
61
50
37
37
5
4
2854
2702

9325
7687
5412
1243
379
461
360
28
24895

D.2 TWO-SAMPLE HYPOTHESIS TESTING

To better understand the data distribution in different datasets, we provide statistical hypothesis test
results in this section.

We can assume that

(i) Samples in RNAStralign training set are i.i.d. from the distribution P(RNAStrtrain);

(ii) Samples in RNAStralign testing set are i.i.d. from the distribution P(RNAStrtest);

(iii) Samples in ArchiveII dataset are i.i.d. from the distribution P(ArcII).

To compare the differences among these data distributions, we can test the following hypothesis:

(a) P(RNAStrtrain) = P(RNAStrtest)

(b) P(RNAStrtrain) = P(ArchiveII)

14

Published as a conference paper at ICLR 2020

The approach that we adopted is the permutation test on the unbiased empirical Maximum Mean
Discrepancy (MMD) estimator:

MMDu(X, Y ) :=

(cid:16) N
(cid:88)

N
(cid:88)

i=1

j(cid:54)=i

k(xi, xj) +

M
(cid:88)

M
(cid:88)

i=1

j(cid:54)=i

k(yi, yj) −

2
mn

N
(cid:88)

M
(cid:88)

i=1

j=1

(cid:17) 1

2

k(xi, yj)

,

(28)

where X = {xi}N
i.i.d. samples from a distribution P2, and k(·, ·) is a string kernel.

i=1 contains N i.i.d. samples from a distribution P1, Y = {yi}M

i=1 contains M

Since we conduct stratiﬁed sampling to split the training and testing dataset, when we perform
permutation test, we use stratiﬁed re-sampling as well (for both Hypothese (a) and (b)). The result
of the permutation test (permuted 1000 times) is reported in Figure 7.

Figure 7: Left: Distribution of MMDu under Hypothesis P(RNAStrtrain) = P(RNAStrtest). Right:
Distribution of MMDu under Hypothesis P(RNAStrtrain) = P(ArchiveII).

The result shows

(a) Hypothesis P(RNAStrtrain) = P(RNAStrtest) can be accepted with signiﬁcance level 0.1.

(b) Hypothesis P(RNAStrtrain) = P(ArchiveII) is rejected since the p-value is 0.

Therefore, the data distribution in ArchiveII is very different from the RNAStralign training set. A
good performance on ArchiveII shows a signiﬁcant generalization power of E2Efold.

D.3 PERFORMANCE ON LONG SEQUENCES: WEIGHTED F1 SCORE

For long sequences, E2Efold still performs better than other methods. We compute F1 scores
weighted by the length of sequences (Table 8), such that the results are more dominated by longer
sequences.

Table 8: RNAStralign: F1 after a weighted average by sequence length.

Method

E2Efold CDPfold LinearFold Mfold

RNAstructure RNAfold CONTRAfold

non-weighted
weighted
change

0.614
0.821
0.720
0.691
-12.3% +12.5%

0.609
0.509
-16.4%

0.420
0.366
-12.8%

0.550
0.471
-14.3%

0.540
0.444
-17.7%

0.633
0.542
-14.3%

The third row reports how much F1 score drops after reweighting.

D.4 ARCHIVEII RESULTS AFTER DOMAIN SEQUENCES ARE REMOVED

Since domain sequence (subsequences) in ArchiveII are explicitly labeled, we ﬁlter them out in
ArchiveII and recompute the F1 scores (Table 9).

The results do not change too much before or after ﬁltering out subsequences.

15

Published as a conference paper at ICLR 2020

Table 9: ArchiveII: F1 after subsequences are ﬁltered out.

Method E2Efold CDPfold LinearFold Mfold RNAstructure RNAfold CONTRAfold

original
ﬁltered

0.704
0.723

0.597
0.605

0.647
0.645

0.421
0.419

0.613
0.611

0.615
0.615

0.662
0.659

D.5 PER-FAMILY PERFORMANCES

To balance the performance among different families, during the training phase we conducted
weighted sampling of the data based on their family size. With weighted sampling, the overall
F1 score (S) is 0.83, which is the same as when we did equal-weighted sampling. The per-family
results are shown in Table 10.

Table 10: RNAStralign: per-family performances

16S rRNA
F1

F1(S)

E2Efold
LinearFold
Mfold
RNAstructure
RNAfold
CONTRAfold

0.783
0.493
0.362
0.464
0.430
0.529

0.795
0.504
0.373
0.485
0.449
0.546

tmRNA

F1

0.588
0.393
0.290
0.400
0.411
0.463

F1(S)

0.653
0.412
0.308
0.423
0.430
0.482

E2Efold
LinearFold
Mfold
RNAstructure
RNAfold
CONTRAfold

tRNA

5S RNA

SRP

F1

F1(S)

F1

F1(S)

0.936
0.738
0.367
0.597
0.612
0.740

F1

0.550
0.618
0.350
0.579
0.617
0.563

F1(S)

0.614
0.648
0.378
0.617
0.651
0.596

0.906
0.713
0.356
0.578
0.592
0.717

RNaseP

F1

0.565
0.567
0.562
0.589
0.544
0.645

F1(S)

0.604
0.578
0.579
0.616
0.563
0.662

telomerase
F1

F1(S)

0.954
0.515
0.403
0.512
0.471
0.529

0.961
0.531
0.531
0.545
0.496
0.548

0.939
0.917
0.739
0.734
0.675
0.662
0.736
0.709
0.706
0.695
0.758
0.765
Group I intron
F1(S)

F1

0.387
0.565
0.483
0.566
0.589
0.603

0.428
0.579
0.498
0.599
0.599
0.620

16

Published as a conference paper at ICLR 2020

D.6 MORE VISUALIZATION RESULTS

Figure 8: Visualization of 5S rRNA, B01865.

Figure 9: Visualization of 16S rRNA, DQ170870.

17

E2EfoldRNAstructureCDPfoldtrue structureLinearFoldMfoldCONTRAfoldRNAfoldE2EfoldRNAstructureCDPfoldtrue structureLinearFoldMfoldCONTRAfoldRNAfoldPublished as a conference paper at ICLR 2020

Figure 10: Visualization of Group I intron, IC3, Kaf.c.trnL.

Figure 11: Visualization of RNaseP, A.salinestris-184.

Figure 12: Visualization of SRP, Homo.sapi. BU56690.

18

E2EfoldRNAstructureCDPfoldtrue structureLinearFoldMfoldCONTRAfoldRNAfoldE2EfoldRNAstructureCDPfoldtrue structureLinearFoldMfoldCONTRAfoldRNAfoldE2EfoldRNAstructureCDPfoldtrue structureLinearFoldMfoldCONTRAfoldRNAfoldPublished as a conference paper at ICLR 2020

Figure 13: Visualization of tmRNA, uncu.bact. AF389956.

Figure 14: Visualization of tRNA, tdbD00012019.

19

E2EfoldRNAstructureCDPfoldtrue structureLinearFoldMfoldCONTRAfoldRNAfoldE2EfoldRNAstructureCDPfoldtrue structureLinearFoldMfoldCONTRAfoldRNAfold