UniGPS: A Uniﬁed Programming Framework for
Distributed Graph Processing

Zhaokang Wang∗†, Junhong Li∗†, Yifan Qi∗†, Guanghui Zhu∗†, Chunfeng Yuan∗†, Yihua Huang∗†
∗State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, P. R. China
†Department of Computer Science and Technology, Nanjing University, Nanjing, P. R. China
{wangzhaokang,mg1833036,mg20330046}@smail.nju.edu.cn, {zgh,cfyuan,yhuang}@nju.edu.cn

1
2
0
2

g
u
A
4

]

C
D
.
s
c
[

1
v
2
2
9
1
0
.
8
0
1
2
:
v
i
X
r
a

Abstract—The industry and academia have proposed many
distributed graph processing systems. However,
the existing
systems are not friendly enough for users like data analysts and
algorithm engineers. On the one hand, the programing models
and interfaces differ a lot in the existing systems, leading to
high learning costs and program migration costs. On the other
hand, these graph processing systems are tightly bound to the
underlying distributed computing platforms, requiring users to
be familiar with distributed computing. To improve the usability
of distributed graph processing, we propose a uniﬁed distributed
graph programming framework UniGPS. Firstly, we propose a
uniﬁed cross-platform graph programming model VCProg for
UniGPS. VCProg hides details of distributed computing from
users. It is compatible with the popular graph programming
models Pregel, GAS, and Push-Pull. VCProg programs can be
executed by compatible distributed graph processing systems
without modiﬁcation, reducing the learning overheads of users.
Secondly, UniGPS supports Python as the programming lan-
guage. We propose an interprocess-communication-based exe-
cution environment isolation mechanism to enable Java/C++-
based systems to call user-deﬁned methods written in Python.
The experimental results show that UniGPS enables users to
process big graphs beyond the memory capacity of a single
machine without sacriﬁcing usability. UniGPS shows near-linear
data scalability and machine scalability.

Index Terms—programming framework, distributed graph

processing, graph processing systems, Python

I. INTRODUCTION

A. Background

The graph is a useful data structure that can model relation-
ships between multiple entities in the real world, such as social
networks and transaction graphs in e-commerce platforms. The
industry and academia have proposed many single-machine
graph processing systems [1] to facilitate graph analysis. They
provide users with friendly programming models and applica-
tion programming interfaces (API). However, the computing
power and memory space of a single machine limit their
performance. As graph scales in real-world applications grow
rapidly, their performance on big graphs becomes more and
more unsatisfactory.

In order to process big graphs efﬁciently, many distributed
graph processing systems are proposed [1]. They simplify
distributed graph processing by providing users with high-
level graph programming models and APIs. However, they
are still not easy to use for users not familiar with distributed
computing (like data analysts and algorithm engineers). They
have the following shortcomings in usability.

• The programming interfaces of the existing systems lack
the cross-platform feature. Each system has its unique
programming model and APIs. The programs written for
a certain system can only run on that system. When
a new system with higher performance appears, users
need manually migrate the existing programs to the
new system, introducing additional learning costs and
programming costs.

• The programming interfaces of the mainstream systems
(like Giraph [2] and GraphX [3]) are tightly bound to
the underlying distributed computing platforms. Users
have to learn how to use distributed computing platforms
before using the distributed graph processing systems,
creating extra barriers. For example, Giraph requires users
to be familiar with Hadoop’s APIs such as Writable and
FileInputFormat to write correct Giraph programs.

• Few mainstream systems support Python as the pro-
gramming language. The mainstream systems (Giraph,
GraphX, and Gemini) only support compiled languages
(Java, Scala, and C++), but the data analysts or algorithm
engineers prefer using Python [4] [5]. Python, along
with the interactive development environment Jupyter
Notebook, is more suitable for data exploration than the
compiled languages.

In order to make graph programming easier, a distributed
graph programming framework should satisfy three usability
criteria: 1) provide a cross-platform uniﬁed programming
interface, 2) make distributed computing details transparent
to users, and 3) support Python as the programming language.
Unfortunately, as shown in Table I, the mainstream dis-
tributed graph processing systems hardly meet the three cri-
teria simultaneously. Although KDT [9] satisﬁes the three
criteria, the expression power of its linear algebraic program-
ming model is limited, only supporting user-deﬁned functions
in several semiring methods like SpGEMM. TinkerPop [10]
is a cross-platform graph programming framework, but
it
only supports Pregel as the programming model for graph
processing. It cannot integrate with systems that adopt other
programming models (like GraphX and Gemini). Furthermore,
it only supports Java for graph processing.

B. Contributions

To enhance the usability of distributed graph processing, we
propose a cross-platform uniﬁed distributed graph program-

 
 
 
 
 
 
TABLE I
COMPARISON OF DISTRIBUTED GRAPH PROCESSING SYSTEMS/FRAMEWORKS

System/Framework

Giraph [2]
GraphX [3]
Gemini [6]
PowerGraph [7]
PowerLyra [8]
KDT [9]
TinkerPop [10]

Programming
Model

Pregel
GAS
Push-Pull
GAS
GAS
Linear Algebra
Pregel

UniGPS

VCProg

Underlying
Platform

Programming
Language

Distributed
Transparency

Interactive
Execution

Hadoop
Spark
MPI
MPI
MPI
MPI
Multiple

Multiple

Java
Scala
C++
C++
C++
Python
Java

Python

×
×
×
×
×
(cid:88)
(cid:88)

(cid:88)

×
(cid:88)
×
×
×
(cid:88)
×

(cid:88)

Development
Environment

IDE
IDE + Notebook
IDE
IDE
IDE
IDE + Notebook
IDE

IDE + Notebook

ming model VCProg. We summarize the common features of
the typical graph programming models Pregel, GAS, and Push-
Pull and further propose the VCProg programming model
to unify them. VCProg is vertex-centric. It regards graph
processing as an iterative update of vertex properties. Each
iteration consists of three phases: merging messages, updating
vertex properties, and sending messages. VCProg is compati-
ble with Pregel, GAS, and Push-Pull models. Programs written
with VCProg can be executed by the compatible distributed
graph processing systems (like Giraph, GraphX, and Gemini)
without modiﬁcation, achieving the goal of “Write Once, Run
Anywhere.”

Based on the uniﬁed programming model VCProg, we
further design a uniﬁed distributed graph programming frame-
work UniGPS that satisﬁes the three usability criteria.

1) UniGPS provides users with uniﬁed programming inter-
faces. Programs written with UniGPS can be executed
by the mainstream distributed graph processing systems
Giraph, GraphX, and Gemini without modiﬁcation.
2) The programming interfaces are platform-independent,
hiding details of the underlying distributed computing
from users.

3) UniGPS adopts Python as the programming language.
To enable the Java/C++-based distributed graph pro-
cessing systems to call user-deﬁned methods written in
Python, we propose an execution isolation mechanism
based on interprocess communication. We propose the
zero-copy optimization technique based on memory-
mapped buffers to reduce data copying overheads during
interprocess communication.

We evaluate the usability and performance of UniGPS in
a cluster with nine nodes. UniGPS enables users to conduct
distributed graph processing in a user-friendly interactive
Python development environment like Jupyter Notebook. With
the help of distributed computing, the graph scale that UniGPS
can handle is an order of magnitude higher than that of the
serial graph processing library NetworkX [11]. For the same
graph, the processing time of UniGPS is much shorter than
NetworkX. UniGPS achieves near-linear data and machine
scalability for typical graph algorithms.

II. RELATED WORK

A. Distributed Graph Processing Systems

A series of distributed graph programming models and
processing systems have been proposed to reduce the difﬁculty
of distributed graph processing. Reference [1] presents a
comprehensive overview of distributed graph programming
frameworks. According to the granularity of parallel comput-
ing, the existing graph programming models can be divided
into three categories: vertex-centric [12], block-centric [13],
and subgraph-centric [14] [15]. The vertex-centric models are
the most thoroughly studied, typical models including Pregel
[16], GAS [7], and Push-Pull [6]. However, even the same
programming model has different APIs in different systems,
lacking the cross-platform feature.

TinkerPop [10] is a graph programming framework that in-
tegrates multiple graph databases and processing systems with
uniﬁed programming interfaces. TinkerPop proposes Gremlin
[17] as its platform-independent query language. However,
TinkerPop’s graph processing framework GraphComputer only
supports Java as the programming language and adopts Pregel
as the graph programming model, not compatible with GAS,
Push-Pull, and other programming models. It can only inte-
grate the Pregel-based graph processing systems.

B. Graph Processing Libraries for Python

Python is popular among data analysts [4] [5] due to
its high usability and development efﬁciency. Many Python
graph processing libraries have been developed for the single-
machine environment [18]. NetworkX [11] and graph-tool [19]
are two popular libraries [20]. However, their performance is
limited by the computing power and memory capacity of a
single machine. They can hardly process large-scale graphs.

A possible way to process big graphs in Python is to use
general-purpose distributed computing systems like Dask and
PySpark. Due to the lack of specialized encapsulation and
optimization for graph processing, users have to manually
manage graph data and message exchange between vertices,
increasing the programming difﬁculty.

III. UNIFIED GRAPH PROGRAMMING MODEL

Among the existing distributed graph programming models,
the vertex-centric models (like Pregel [16] and GAS [7])

Fig. 1. Three phases of an iteration in vertex-centric programming models.

are the most thoroughly studied [1]. However, different pro-
gramming models have different APIs, increasing the learning
burden and bringing extra programming migration costs. In
order to overcome the drawback, we summarize the common
features of three typical vertex-centric graph programming
models Pregel [16], GAS [7], and Push-Pull [6]. We design
a vertex-centric uniﬁed graph programming model VCProg
based on the common features.

A. Common Features of Vertex-Centric Programming Models

Pregel, GAS, and Push-Pull have two salient common
iterative calculation and three-phase update. The
features:
programming models express graph processing as an iterative
update process of vertex properties. Vertices exchange data by
sending messages between iterations.

The whole process consists of several rounds of iterations.
In each iteration, every vertex updates its property based
on the properties of itself, its neighbors, and the received
messages. The iterations continue until convergence. There are
two popular convergence conditions: all vertices are inactive,
or the iteration reaches the maximum number of rounds.

In each iteration, the process of every vertex consists of
three phases: merging messages, updating vertex properties,
and sending messages, as shown in Fig. 1. Firstly, each vertex
receives messages from its incoming neighbors and merges the
messages into a single message. Secondly, each vertex updates
its property based on the merged message and its current
property. Finally, each vertex sends messages to its outgoing
neighbors based on its updated property and the properties of
its adjacent outgoing edges.

Inspired by the features, we propose a vertex-centric uniﬁed
graph programming model VCProg that is compatible with
Pregel, GAS, and Push-Pull at the same time.

B. Data Model of VCProg

VCProg adopts the property graph as its data model. Each
vertex (edge) has an attached property that is a record con-
taining several ﬁelds. All vertex (edge) properties have the
same schema. Messages exchanged between vertices are also
records. All messages have the same schema.

Before iterations begin, each vertex (edge) initializes its
property based on the input data. During iterations, the ver-
tex properties are updated while the edge properties remain
unchanged. After iterations, the vertex properties store the

Fig. 2. Application programming interface of VCProg in Python.

processing results. The vertex properties are output to ﬁles
in a tabular form.

C. Application Programming Interface of VCProg

VCProg provides its Python API in the form of an abstract
base class VCProg as shown in Fig. 2. All methods of the
base class are abstract. To write distributed graph processing
programs, users need to inherit the base class and implement
the abstract methods according to algorithmic logic. Users de-
ﬁne the behavior of the three phases in each iteration with the
mergeMessage, vertexCompute, and emitMessage
method, respectively.

The mergeMessage method combines two message
records m1 and m2 into a single message record msg. The mes-
sage order should be interchangeable: mergeMessage(m1,
m2) = mergeMessage(m2, m1).

The vertexCompute method generates the updated ver-
tex property for a vertex in each iteration. It receives the
vertex property prop from the previous iteration, the merged
message msg, and the current iteration number iter as the
parameters. The method returns the updated vertex property
new_attr and a ﬂag is_active to indicate whether the
vertex will be active in the next iteration.

The emitMessage method determines whether to send a
message is_emit and the content of the message msg for
the edge (src, dst), based on the source vertex’s property
src_prop and the edge’s property edge_prop.

VCProg further uses the initVertexAttr method to
initialize the property of each vertex before iterations, based
on the vertex ID id, the property in the input prop and
the outgoing degree out_degree. The emptyMessage
method returns a global read-only empty message record
empty_msg. The empty message is a special message that is
idempotent for merging. For any message m, it should satisfy
mergeMessage(m, empty_msg) = m.

Fig. 3 shows a demo program that uses VCProg to imple-
ment the Bellman-Ford single-source shortest path calculation
(UniSSSP). The API of VCProg is easy to use and indepen-
dent from distributed graph processing systems, ensuring the
transparency of distributed processing to users.

v.value ←VP.initVertexAttr(v.ID, len(v.out edges), v.value);

Algorithm 1 Workﬂow of the VCProg Programming Model
Input: G = (V, E), MAX ITER, VP; Output: G.
1: empty msg ← VP.emptyMessage();
2: for all v ∈ V do in parallel
3:
4: for iter ← 1 to MAX ITER do
num active ← 0;
(cid:46) Number of active vertices
5:
for all v ∈ V that v is active or v receives any message do in parallel
6:
7:
8:
9:
10:

msg ← empty msg;
for every message m received by v do
msg ← VP.mergeMsg(msg, m);

v.value, v.is active ← VP.vertexCompute(v.value, msg,

(cid:46) Global empty message
(cid:46) Initialize vertex properties

iter);

11:
12:
13:
14:

15:
16:
17:
18:

if v.is active = TRUE then

(cid:46) Only for active vertices

num active ← num active + 1;
for all e ∈ v.out edges do

(cid:46) For outgoing adjacent edges
is emit, msg ← VP.emitMessage(v.ID, e.target ID,

v.value, e.value);

if is emit = TRUE then

SENDMESSAGE(e.target ID, msg)

if num active = 0 then

break;

(cid:46) The iteration converges and terminates early

E. Compatibility with Other Programming Models

VCProg is compatible with the typical vertex-centric graph
programming models Pregel [16], GAS [7] and Push-Pull [6].
The workﬂow of VCProg can be equivalently expressed by
these models. Fig. 4 shows how to use Pregel, GAS, and
Push-Pull to achieve the same execution semantics of VProg
given a user-deﬁned VCProg instance object VP. According to
the conversion, the corresponding distributed graph processing
system can execute a VCProg program VP to get the desired
output. By this way, VCProg provides the capability of cross-
platform execution.

IV. UNIFIED GRAPH PROGRAMMING FRAMEWORK

Based on the uniﬁed graph programming model VCProg,
we design and implement a uniﬁed graph programming frame-
work UniGPS. Users can use UniGPS in interactive Python
development environments like Jupyter Notebook. To further
support cross-platform program execution, we propose an
interprocess-communication-based mechanism to isolate the
execution of user programs (written in Python) from the
underlying distributed graph processing systems.

A. System Architecture

Fig. 5 shows the system architecture of UniGPS. UniGPS
consists of four modules: VCProg programming model, native
operators, backend engine, and the uniﬁed graph I/O format.
The VCProg programming model module provides the
VCProg API and runs custom user programs. Considering
Java/C++-based distributed graph processing systems cannot
execute Python-based user programs directly, we propose an
interprocess-communication-based mechanism to isolate the
execution environment of the two parts. We adopt a row-based
serialization format to serialize property/message records dur-
ing communication. We will elaborate on the mechanism later
in Section IV-C.

The native operator module contains some frequently-used
pre-compiled graph operators. UniGPS natively implements

Fig. 3. Demo program of UniGPS to calculate the single-source shortest path
(SSSP) in Python.

D. Execution Semantics of VCProg

Algorithm 1 shows the workﬂow of VCProg. VCProg
receives a graph G = (V, E) as the input, where V and E
are the vertex and edge set of G, respectively. Each vertex
v (edge e) in G has an attached property v.value (e.value).
The user needs to specify the maximum number of iterations
MAX ITER as the hyper-parameter. The user provides its
program in the form of an instance object VP of the VCProg
base class. VP implements all the abstract methods. VP is
read-only and shared by all vertices.

VCProg iterates for at most MAX ITER rounds (line 10). In
each round, every vertex v will be either active or inactive. v
is active if and only if v is set as active in the previous round
or v receives any message. Only active vertices participate in
the current iteration (Line 7 to 16). The vertexCompute
method determines whether the vertex remains active in the
next round. If all vertices are inactive in a round (Line 17),
the iteration converges early. VCProg triggers a global barrier
implicitly at the end of each round.

(a) Pregel

Fig. 5. System architecture of UniGPS.

every operator for every distributed graph processing system in
advance. UniGPS provides a uniﬁed platform-independent API
for each operator. Every API contains an engine parameter
to select the preferred graph processing system.

The backend engine module integrates VCProg-compatible
distributed graph processing systems into UniGPS. Currently,
our prototype of UniGPS supports Giraph, GraphX, and Gem-
ini. UniGPS can integrate with other systems that are compat-
ible with the VCProg programming model. Each system is
regarded as a backend engine to run VCProg programs and
native operators.

The uniﬁed graph I/O format module uses a uniﬁed graph
serialization format (like GraphSON [10]) to decouple external
data sources and distributed graph processing systems. If we
want to support M systems reading/writing N data sources,
we have to implement M ∗ N I/O formats without the uniﬁed
format. Using the uniﬁed format as an intermediate format,
we only need to implement M + N I/O formats, signiﬁcantly
reducing the development and maintenance costs of UniGPS.

B. Application Programming Interface of UniGPS

UniGPS appears as a library in Python. Fig. 3 demonstrates
how to conduct the single-source shortest path calculation in
UniGPS. Users need to import the UniGPS library ﬁrst and
initialize a handle of UniGPS unigps with the conﬁguration
ﬁle. UniGPS supports loading/saving graphs from/to external
data sources like HDFS and graph databases.

UniGPS provides two kinds of APIs to conduct graph pro-
cessing: VCProg APIs and native operator APIs. The VCProg
APIs allow users to develop customized processing programs.
The native operator APIs allow users to call
the built-in
native implementation of frequently-used processing operators
like PageRank and single-source shortest path (SSSP). All
UniGPS’s APIs are platform-independent. Users can execute
UniGPS programs with different distributed graph processing
systems by just changing the engine parameter in each API.

(b) GAS

(c) Push-Pull (Dense Mode)

Fig. 4. Express the workﬂow of the VCProg programming model (VP) with
the typical vertex-centric graph programming models.

Fig. 6. Execution environment isolation with interprocess communication.

C. Interprocess Communication Based Execution Environ-
ment Isolation

The distributed graph processing systems written in Java,
Scala, or C++ call
the user-deﬁned functions via func-
tion/method invocation provided by the programming language
itself. It requires users to develop their programs in the same
programming language as the systems.

To enable the existing Java/Scala/C++-based systems to call
the user-deﬁned methods of VCProg written in Python, we
propose an execution environment isolation mechanism based
on interprocess communication (IPC). As shown in Fig. 5, the
mechanism contains three modules: IPC server, IPC interface,
and IPC client. The IPC server is a Python process that
contains the user-given VCProg instance object. The IPC client
embeds itself in the worker processes of distributed graph
processing systems. The IPC client and server communicate
through the IPC interface. The IPC interface allows the IPC
client to remotely call the methods of the VCProg object in
the IPC server.

1) Workﬂow of VCProg Jobs: With the execution environ-
ment isolation mechanism, UniGPS adds several extra steps
in the workﬂow of a VCProg-based graph processing job, as
shown in Fig. 6.

Before a job starts, UniGPS serializes the VCProg instance
object that the user submits to a ﬁle, and UniGPS uploads the
ﬁle to HDFS.

UniGPS starts a distributed graph processing job based
on the user-speciﬁed backend engine speciﬁed through the
command line. The job starts its driver process on the main
node of the cluster. The driver process further starts several
worker processes on the worker nodes of the cluster. Taking
GraphX as an example, the Spark driver process starts Spark
executors through the cluster resource manager YARN.

Fig. 7. Memory-mapping-based interprocess communication.

Every worker process launches a dual VCProg runner
process. The VCProg runner process deserializes the VCProg
object from the ﬁle stored on HDFS and creates an IPC server
based on the VCProg object. The worker process creates an
IPC client and establishes the connection with the IPC server.
After the initialization steps above accomplish, the driver
process starts running the distributed graph processing job
according to the workﬂow of the backend engine system. For
example, the GAS-model-based systems follow the workﬂow
in Fig. 4b to run the job. During the job execution, when a
worker process needs to call a method of the VCProg instance
object, it triggers a remote procedure call through the IPC
client to the VCProg object in the IPC server.

The execution isolation mechanism enables UniGPS to
support Python for any distributed graph processing system.
It hides the details of distributed processing from users and
supports cross-platform execution of user programs.

2) Interprocess Communication Optimization: The perfor-
mance of the execution isolation mechanism signiﬁcantly
affects the efﬁciency of UniGPS. We can implement
the
mechanism with any general-purpose remote procedure call
(RPC) framework. However, the popular RPC frameworks like
gRPC are based on the network stack. They trigger system
calls and copy data buffers between the user and kernel space
multiple times during a remote method invocation.

In order to reduce the overhead of data copies, we adopt a
zero-copy IPC technique to implement the execution environ-
ment isolation mechanism. UniGPS creates a mapped buffer
on both sides of the IPC client and server, as shown in Fig. 7.
We map the two buffers to the same region in the physical
memory through the memory mapping ﬁle mechanism pro-
vided by Linux. The reading/writing of the mapped buffers
directly operates on the corresponding region in the physical
memory. Changes to one of the buffers immediately reﬂect
in the other buffer without any data copy. In other words,
the communication between the IPC client and server does
not involve any data copy (i.e., zero copy), and the commu-
nication occurs in the user space, avoiding the overheads of
kernel-space switching. With the mapped buffers, the problem
of implementing RPC becomes how to organize concurrent

reading/writing of the mapped buffers on both sides.

UniGPS uses the memory layout in Fig. 7 to manage the
mapped buffers. The client ﬂag indicates whether the client
is ready to prepare the IPC method index and the request ar-
guments. The server ﬂag indicates whether the server ﬁnishes
the method invocation. The IPC method index indicates which
method of the VCProg object the IPC client calls.

Since RPC invocations usually ﬁnish quickly in VCProg,
we use the busy waiting mechanism to synchronize between
the IPC client and server. When the client starts an RPC
invocation, it ﬁrst sets the client ﬂag and then repeatedly
checks whether the server ﬂag becomes ready. The server side
also repeatedly checks the client ﬂag. Once the client ﬂag
becomes ready, the server processes the RPC immediately.
When the server ﬁnishes, it sets the server ﬂag to notify the
client. Compared with the lock-based mechanism, the busy
waiting avoids triggering system calls. UniGPS uses the thread
yield mechanism in busy waiting to actively give up invalid
CPU time slices, reducing the waste in CPU cycles.

V. PERFORMANCE EVALUATION

The section evaluates the usability and scalability of
UniGPS and compares its computational performance with the
stand-alone Python graph processing library NetworkX.

A. Experimental Environment

All experiments were conducted in a cluster with nine nodes
(1 main + 8 workers) connected via 1Gbps ethernet. Each node
was equipped with eight physical cores, 40 GB memory, and
1.8 TB HDD hard disk. We launched a Ubuntu 20.04 container
for each node to provide an isolated experimental environment.
We used CPython 3.7.4 as the Python interpreter.

UniGPS integrated three distributed graph processing sys-
tems as the backend engines: Giraph [2] (version 1.3.0 with
Hadoop 2.7.7 and OpenJDK 1.8), GraphX [3] (with Spark
2.1.2 and Scala 2.11.8), and Gemini [6] (compiled with GCC
9.3.0). For Giraph, UniGPS launched eight workers for each
node and allocated 4 GB memory for each worker. For
GraphX, UniGPS launched eight Spark executors for each
node and allocated 4 GB memory for each executor. For
Gemini, each node ran eight single-thread worker processes.

B. Usability Comparison

Table I compares the usability of UniGPS and the exist-
ing distributed graph processing systems/frameworks. UniGPS
supports the Python language and can be used in an interactive
development environment like Jupyter Notebook, improving
the productivity of program development and debugging. The
APIs of UniGPS are platform-independent, hiding details of
distributed computing from users. UniGPS and KDT are the
only two frameworks that satisfy the three usability criteria
discussed in Section I-A. However, the customizability of KDT
is more limited than UniGPS. KDT only supports using user-
deﬁned functions for several semiring methods. UniGPS can
customize the methods of all three phases in each iteration.

TABLE II
OVERVIEW OF REAL-WORLD GRAPH DATASETS

Dataset

|V |

|E|

Directed

Source

as-skitter (as) [21]
soc-livejournal (lj) [21]
com-orkut (ok) [21]
uk-2002 (uk) [22]

22.2M
1.7M
69.0M
4.8M
3.1M
234.4M
18.5M 298.1M

No
Yes
No
Yes

Computer Network
Social Network
Social Network
WWW

C. Performance Comparison

We compared the execution time of UniGPS with the serial
Python graph processing library NetworkX [11] (version 2.5)
on four real-world graph datasets (as shown in Table II).
Fig. 8a shows the experimental results on several
typical
graph algorithms. For UniGPS, we implemented the typical
algorithms with the VCProg API. For NetworkX, we call the
built-in algorithm operators. If the execution time exceeded
three hours (i.e., timeout) or the program crashed, the test
case was not shown.

The experimental results indicated that UniGPS could han-
dle much larger datasets than NetworkX. NetworkX could
not process the large datasets ok and uk due to the out-
of-memory exception. UniGPS with the Giraph engine could
handle all datasets with the help of distributed processing. On
the lj dataset, the execution time of UniGPS with the Giraph
engine was 21.76% (PageRank), 31.07% (SSSP), and 70.23%
(connected components) of NetworkX. UniGPS met timeout
with the GraphX and Gemini engines. The edge-parallel design
of GraphX and Gemini made IPC overheads more obvious.
UniGPS was more suitable to work with Giraph.

D. Data Scalability

To evaluate the data scalability of UniGPS, we used the
logNormalGraph generator of GraphX to generate random
graphs of similar topological characteristics but with different
scales. Fig. 8b shows the execution time of UniGPS (VCProg
API with the Giraph engine) and NetworkX on different scales.
The execution time of UniGPS and NetworkX increased
near linearly with the number of edges of the graph. The
execution time of UniGPS was much shorter than that of
NetworkX. The advantages became more obvious as the graph
scale increased. UniGPS showed near-linear data scalability
while NetworkX crashed due to the out-of-memory exception
on big graphs. The graph scale that UniGPS could handle was
an order of magnitude higher than NetworkX. UniGPS could
help users process big graphs far beyond the memory capacity
of a single machine with high usability.

E. Machine Scalability

We measured the execution time of UniGPS (VCProg API
with the Giraph engine) with different numbers of CPU cores.
Fig. 8c converts the execution time into the speedup relative
to the case of 16 cores. The experimental results indicated
that UniGPS showed near-linear machine scalability. The
scalability of CC and PR was better than that of SSSP because
the two algorithms were more computationally intensive.

(a)

(c)

(d)

(b)

Fig. 8. Performance evaluation of UniGPS on typical graph algorithms: PageRank (PR), single-source shortest path (SSSP), and connected components (CC).
(a) Performance comparison between UniGPS with different engines and NetworkX. (b) Data scalability of UniGPS and NetworkX. (c) Machine scalability
of UniGPS. (d) Effects of IPC optimization.

F. Effects of IPC Optimization

Fig. 8d compares the execution time of UniGPS with two
different RPC implementations in the execution environment
isolation mechanism: the network-based gRPC and the pro-
posed zero-copy IPC. The execution time of the zero-copy
IPC was much shorter than that of gRPC. System calls and
data copies of gRPC bring high overheads. Since UniGPS
frequently triggers remote procedure calls during execution,
adopting the zero-copy IPC can signiﬁcantly reduce the exe-
cution time.

VI. CONCLUSION AND FUTURE WORK

The existing distributed graph processing systems are not
friendly enough for data analysts and algorithm engineers.
To increase the usability of distributed graph processing, we
proposed a uniﬁed graph programming framework UniGPS.
UniGPS provides cross-platform uniﬁed programming inter-
faces, hides distributed computing details from users, and
supports Python as the programming language. UniGPS adopts
a vertex-centric uniﬁed graph programming model VCProg
compatible with the Pregel, GAS, and Push-Pull model.
UniGPS uses an IPC-based execution environment isolation
mechanism to enable Java/C++-based systems to call user-
deﬁned methods written in Python.

In the future, we plan to propose more techniques to
improve the performance of UniGPS. One possible technique
is to organize RPC invocations in a pipeline manner to overlap
computing and communication. High-performance graph I/O
is also an interesting topic to investigate.

ACKNOWLEDGMENT

This work was supported in part by the National Natu-
ral Science Foundation of China (#U1811461), the National

Key R&D Program of China (#2019YFC1711000), the Open
Project of State Key Laboratory for Novel Software Tech-
nology (#KFKT2021B33), and the Collaborative Innovation
Center of Novel Software Technology and Industrialization,
Jiangsu, China. Guanghui Zhu and Yihua Huang are corre-
sponding authors with equal contributions.

REFERENCES

[1] S. Heidari, Y. Simmhan, R. N. Calheiros, and R. Buyya, “Scalable
graph processing frameworks: A taxonomy and open challenges,” ACM
Computing Surveys, vol. 51, no. 3, pp. 60:1 – 60:53, 2018.

[2] A. Ching, S. Edunov, M. Kabiljo, D. Logothetis, and S. Muthukrishnan,
“One trillion edges: Graph processing at facebook-scale,” Proceedings
of the VLDB Endowment, vol. 8, no. 12, p. 1804–1815, 2015. [Online].
Available: http://giraph.apache.org/

[3] J. E. Gonzalez, R. S. Xin, A. Dave, D. Crankshaw, M.

J.
Franklin, and I. Stoica, “GraphX: Graph processing in a distributed
dataﬂow framework,” in Proceedings of the 11th USENIX Symposium
on Operating Systems Design and Implementation (OSDI 14).
[Online]. Available:
USENIX Association, 2014, pp. 599–613.
https://spark.apache.org/graphx/

[4] S. Cass, “The top programming languages: Our latest rankings put
python on top-again - [careers],” IEEE Spectrum, vol. 57, no. 8, pp.
22–22, 2020.

[5] G.

(2019)

Piatetsky.
science, machine
ysis.
poll-top-data-science-machine-learning-platforms.html

leads
the
data
anal-
Trends
platforms:
https://www.kdnuggets.com/2019/05/

learning
Available:

top
and

[Online].

Python

11

[6] X. Zhu, W. Chen, W. Zheng, and X. Ma, “Gemini: A computation-
centric distributed graph processing system,” in Proceedings of the 12th
USENIX Symposium on Operating Systems Design and Implementation
(OSDI 16). USENIX Association, 2016, pp. 301–316.
[Online].
Available: https://github.com/thu-pacman/GeminiGraph/

[7] J. E. Gonzalez, Y. Low, H. Gu, D. Bickson, and C. Guestrin, “Pow-
erGraph: Distributed graph-parallel computation on natural graphs,” in
Proceedings of the 10th USENIX Symposium on Operating Systems
Design and Implementation (OSDI 12). USENIX Association, 2012,
pp. 17–30.

[8] R. Chen, J. Shi, Y. Chen, B. Zang, H. Guan, and H. Chen, “PowerLyra:
Differentiated graph computation and partitioning on skewed graphs,”

ACM Transactions on Parallel Computing, vol. 5, no. 3, pp. 13:1 –
13:39, 2019.

[9] A. Lugowski, D. Alber, A. Buluc¸, J. R. Gilbert, S. Reinhardt, Y. Teng,
and A. Waranis, “A ﬂexible open-source toolbox for scalable complex
graph analysis,” in Proceedings of
the 2012 SIAM International
Conference on Data Mining (SDM), 2012, pp. 930–941. [Online].
Available: http://kdt.sourceforge.net/

[10] Apache Software Foundation. Apache tinkerpop. [Online]. Available:

https://tinkerpop.apache.org/

[11] NetworkX: Network analysis in python. [Online]. Available: https:

//networkx.org

[12] R. R. McCune, T. Weninger, and G. Madey, “Thinking like a vertex:
A survey of vertex-centric frameworks for large-scale distributed graph
processing,” ACM Computing Surveys, vol. 48, no. 2, pp. 25:1 – 25:39,
2015.

[13] D. Yan, J. Cheng, Y. Lu, and W. Ng, “Blogel: A block-centric framework
for distributed computation on real-world graphs,” Proceedings of the
VLDB Endowment, vol. 7, no. 14, pp. 1981–1992, 2014.

[14] A. Quamar, A. Deshpande, and J. Lin, “Nscale: neighborhood-centric
large-scale graph analytics in the cloud,” The VLDB Journal, vol. 23,
pp. 125 – 150, 2016.

[15] D. Yan, G. Guo, M. M. Rahman Chowdhury, M. Tamer ¨Ozsu, W.-S.
Ku, and J. C. S. Lui, “G-thinker: A distributed framework for mining
subgraphs in a big graph,” in Proceedings of the 2020 IEEE 36th
International Conference on Data Engineering (ICDE), 2020, pp. 1369–
1380.

[16] G. Malewicz, M. H. Austern, A. J. Bik, J. C. Dehnert, I. Horn, N. Leiser,
and G. Czajkowski, “Pregel: A system for large-scale graph processing,”
in Proceedings of the 2010 ACM SIGMOD International Conference on
Management of Data. ACM, 2010, pp. 135–146.

[17] M. A. Rodriguez, “The gremlin graph traversal machine and language
(invited talk),” in Proceedings of the 15th Symposium on Database
Programming Languages. ACM, 2015, pp. 1–10.

[18] V. Batagelj, “Python packages for networks,” in Encyclopedia of Social
Network Analysis and Mining, R. Alhajj and J. Rokne, Eds. Springer
New York, 2018, pp. 1943–1952.

[19] T. P. Peixoto. graph-tool: Efﬁcent network analysis with python.

[Online]. Available: https://graph-tool.skewed.de/

[20] S. Sahu, A. Mhedhbi, S. Salihoglu, J. Lin, and M. T.

¨Ozsu, “The
ubiquity of large graphs and surprising challenges of graph processing,”
Proceedings of the VLDB Endowment, vol. 11, no. 4, pp. 420–431, 2017.
[21] J. Leskovec and A. Krevl, “SNAP Datasets: Stanford large network

dataset collection,” http://snap.stanford.edu/data, Jun. 2014.

[22] P. Boldi and S. Vigna, “The WebGraph framework I: Compression
techniques,” in Proceedings of the Thirteenth International World Wide
Web Conference (WWW 2004). ACM, 2004, pp. 595–601. [Online].
Available: http://law.di.unimi.it/datasets.php

