0
2
0
2

p
e
S
7

]

G
L
.
s
c
[

4
v
9
5
2
7
0
.
2
0
0
2
:
v
i
X
r
a

Identifying Critical Neurons in ANN Architectures
using Mixed Integer Programming

Mostafa ElAraby
Dept. of Comp. Sci. & Oper. Res.,
Mila, Université de Montréal,
Quebec, Canada

Guy Wolf∗
Dept. of Math. & Stat.,
Mila, Université de Montréal,
Quebec, Canada
guy.wolf@umontreal.ca

Margarida Carvalho∗
Dept. of Comp. Sci. & Oper. Res.,
CIRRELT, Université de Montréal,
Quebec, Canada
carvalho@iro.umontreal.ca

Abstract

We introduce a mixed integer program (MIP) for assigning importance scores to
each neuron in deep neural network architectures which is guided by the impact of
their simultaneous pruning on the main learning task of the network. By carefully
devising the objective function of the MIP, we drive the solver to minimize the
number of critical neurons (i.e., with high importance score) that need to be kept
for maintaining the overall accuracy of the trained neural network. Further, the pro-
posed formulation generalizes the recently considered lottery ticket optimization
by identifying multiple “lucky” sub-networks resulting in optimized architecture
that not only performs well on a single dataset, but also generalizes across multiple
ones upon retraining of network weights. Finally, we present a scalable imple-
mentation of our method by decoupling the importance scores across layers using
auxiliary networks. We demonstrate the ability of our formulation to prune neural
networks with marginal loss in accuracy and generalizability on popular datasets
and architectures.

1

Introduction

Deep learning has proven its power to solve complex tasks and to achieve state-of-the-art results
in various domains such as image classiﬁcation, speech recognition, machine translation, robotics
and control [1, 2]. Over-parameterized deep neural models with more parameters than the training
samples can be used to achieve state-of-the art results on various tasks [3, 4]. However, the large
number of parameters comes at the expense of computational cost in terms of memory footprint,
training time and inference time on resource-limited IOT devices [5, 6].

In this context, pruning neurons from an over-parameterized neural model has been an active
research area. This remains a challenging open problem whose solution has the potential to increase
computational efﬁciency and to uncover potential sub-networks that can be trained effectively. Neural
Network pruning techniques [7–16] have been introduced to sparsify models without loss of accuracy.
Most existing work focus on identifying redundant parameters and non-critical neurons to achieve a
lossless sparsiﬁcation of the neural model. The typical sparsiﬁcation procedure includes training a

∗equal contribution

Preprint. Under review.

 
 
 
 
 
 
neural model, then computing parameters importance and pruning existing ones using certain criteria,
and ﬁne-tuning the neural model to regain its lost accuracy. Existing pruning and ranking procedures
are computationally expensive, requiring iterations of ﬁne-tuning on the sparsiﬁed model and no
experiments were conducted to check the generalization of sparsiﬁed models across different datasets.

We remark that sparse neuron connectivity is often used by modern network architectures, and
perhaps most notably in convolutional layers. Indeed, the limited size of the parameter space in
such cases increases the effectiveness of network training and enables the learning of meaningful
semantic features from the input images [17]. Inspired by the beneﬁts of sparsity in such architecture
designs, we aim to leverage the neuron sparsity achieved by our framework to attain optimized neural
architectures that can generalize well across different datasets.

Figure 1: The generic ﬂow of our proposed framework used to remove neurons having an importance
score less than certain threshold.

Contributions.
In our proposed framework, illustrated in Figure 1, we formalize the notation of
neuron importance as a score between 0 and 1 for each neuron in a neural network and the associated
dataset. The neuron importance score reﬂects how much activity decrease can be inﬂicted in it,
while controlling the loss on the neural network model accuracy. Concretely, we propose a mixed
integer programming formulation (MIP) that allows the computation of each fully connected’s neuron
and convolutional feature map importance score and that takes into account the error propagation
between the different layers. The motivation to use such approach comes from the existence of
powerful techniques to solve MIPs efﬁciently in practice, and consequently, to allow the scalability
of this procedure to large ReLU neural models. In addition, we extend the proposed formulation to
support convolutional layers computed as matrices multiplication using toeplitz format [18] with an
importance score associated with each feature map [19].

Once neuron importance scores have been determined, a threshold is established to allow the
identiﬁcation of non-critical neurons and their consequent removal from the neural network. Since
it is intractable to compute neuron scores using full datasets, in our experiments, we approximate
their value by using subsets. In fact, we provide empirical evidence showing that our pruning process
results in a marginal loss of accuracy (without ﬁne-tuning) when the scores are approximated by
a small balanced subset of data points, or even by parallelizing the scores’ computation per class
and averaging the obtained values. Furthermore, we enhance our approach such that the importance
scores computation is also efﬁcient for very deep neural models, like VGG-16 [20]. This stresses the
scalability of it to datasets with many classes and large deep networks. To add to our contribution, we
show that the computed neuron importance scores from a speciﬁc dataset generalize to other datasets
by retraining on the pruned sub-network, using the same initialization.

Organization of the paper.
In Sec. 1.1, we review relevant literature on neural networks sparsiﬁ-
cation and the use of mixed integer programming to model them. Sec. 1.2 provides background on
the formulation of ReLU neural networks as MIPs. In Sec. 2, we introduce the neuron importance
score and its incorporation in the mathematical programing model, while Sec. 3 discusses the ob-
jective function that optimizes sparsiﬁcation and balances accuracy. Sec. 4 provides computational
experiments, and Sec. 5 summarizes our ﬁndings.

1.1 Related Work

Classical weight pruning methods. LeCun et al. [7] proposed the optimal brain damage that
theoretically prunes weights having a small saliency by computing its second derivatives with
respect to the objective. The objective being optimized was the model’s complexity and training

2

error. Hassibi and Stork [21] introduced the optimal brain surgeon that aims at removing non-
critical weights determined by the Hessian computation. Another approach is presented by Chauvin
[22], Weigend et al. [23], where a penalizing term is added to the loss function during the model
training (e.g. L0 or L1 norm) as a regularizer. The model is sparsiﬁed during backpropagation of the
loss function. Since these classical methods depend i) on the scale of the weights, ii) are incorporated
during the learning process, and, some of them, iii) rely on computing the Hessian with respect to
some objective, they turn out to be slow, requiring iterations of pruning and ﬁne-tuning to avoid loss
of accuracy. On the other hand, our approach identiﬁes a set of non-critical neurons that when pruned
simultaneously results in a marginal loss of accuracy without the need of ﬁne-tuning or re-training.

Weight pruning methods. Molchanov et al. [19] devised a greedy criteria-based pruning with ﬁne-
tuning by backpropagation. The criteria devised is given by the absolute difference between dense
and sparse neural model loss (ranker). This cost function ensures that the model will not signiﬁcantly
decrease its predictive capacity. The drawback of this approach is in requiring a retraining after each
pruning step. Shrikumar et al. [24] developed a framework that computes the neurons’ importance at
each layer through a single backward pass. This technique compares the activation values among
the neurons and assigns a contribution score to each of them based on each input data point. Other
related techniques, using different objectives and interpretations of neurons importance, have been
presented [25–29, 10, 30, 31]. They all demand intensive computation, while our approach aims at
efﬁciently computing neurons’ importance. Lee et al. [13] investigates the pruning of connections,
instead of entire neurons. The connections’ sensitivity is studied through the model’s initialization
and a batch of input data. The sensitivity of the connections are computed using the magnitude of the
derivatives of the mini-batch with respect to the loss function. Connections having a sensitivity score
lower than a certain threshold are removed. This proposed technique is the current state-of-the-art in
deep networks’ compression.

Lottery ticket. Frankle and Carbin [32] introduced the lottery ticket theory that shows the existence
of a lucky pruned sub-network, a winning ticket. This winning ticket can be trained effectively with
fewer parameters, while achieving a marginal loss in accuracy. Morcos et al. [33] proposed a technique
for sparsifying n over-parameterized trained neural model based on the lottery hypothesis. Their
technique involves pruning the model and disabling some of its sub-networks. The pruned model can
be ﬁne-tuned on a different dataset achieving good results. To this end, the dataset used for on the
pruning phase needs to be large. The lucky sub-network is found by iteratively pruning the lowest
magnitude weights and retraining. Another phenomenon discovered by [34, 35], was the existence of
smaller high-accuracy models that resides within larger random networks. This phenomenon is called
strong lottery ticket hypothesis and was proved by [36] on ReLU fully connected layers. Furthermore,
Wang et al. [37] proposed a technique of selecting the winning ticket at initialization before training
the ANN by computing an importance score, based on the gradient ﬂow in each unit.

Mixed-integer programming Fischetti and Jo [38] and Anderson et al. [39] represent a ReLU
ANN using a MIP. Fischetti and Jo [38] presented a big-M formulation to represent trained ReLU
neural networks. Later, Anderson et al. [39] introduced the strongest possible tightening to the big-M
formulation by adding strengthening separation constraints when needed2, which reduced the solving
time by orders of magnitude. All the proposed formulations, are designed to represent trained ReLU
ANNs with ﬁxed parameters. In our framework, we used the formulation from [38] because its
performance was good due to our tight local variable bounds, and its polynomial number of constraints
(while, Anderson et al. [39]’s model has an exponential number of constraints). Representing ANN as
a MIP can be used to evaluate robustness, compress networks and create adversarial examples for the
trained neural network. Tjeng et al. [40] used a big-M formulation to evaluate the robustness of neural
models against adversarial attacks. In their proposed technique, they assessed the ANN’s sensitivity to
perturbations in input images. The MIP solver tries to ﬁnd a perturbed image (adversarial attack) that
would get misclassiﬁed by the ANN. Serra et al. [16] also used a MIP to maximize the compression
of an existing neural network without any loss of accuracy. Different ways of compressing (removing
neurons, folding layers, etc) are presented. However, the reported computational experiments lead
only to the removal of inactive neurons. Our method has the capability to identify such neurons, as
well as to identify other units that would not signiﬁcantly compromise accuracy.

2 The cut callbacks in Gurobi were used to inject separated inequalities into the cut loop.

3

Huang et al. [41] used also mathematical programming models to check neural models’ robustness in
the domain of natural language processing. In their proposed technique, the bounds computed for
each layer would get shifted by an epsilon value for each input data point for the MIP. This epsilon is
the amount of expected perturbation in the input adversarial data.

1.2 Background and Preliminaries

Integer programs are combinatorial optimization problems restricted to discrete variables, linear
constraints and linear objective function. These problems are NP-hard, even when variables are
restricted to be binary [42]. The difﬁculty comes from ensuring integer solutions, and thus, the
impossibility of using gradient methods. When continuous variables are included, they are designated
by mixed integer programs. Advances in combinatorial optimization such as branching techniques,
bounds tightening, valid inequalities, decomposition and heuristics, to name few, have resulted in
powerful solvers that can in practice solve MIPs of large size in seconds. See [43] for an introduction
to integer programming.
Consider layer l of a trained ReLU neural network with W l as the weight matrix, wl
i row i of W l ,
and bl the bias vector. For each input data point x, let hl be a decision vector denoting the output
value of layer l, i.e. hl = ReLU (W l hl−1 + bl) for l > 0 and h0 = x, and zl
i be a binary variable
ihl−1 + bl
taking value 1 if the unit i is active, i.e. wl
i and
U l
i be constants indicating a valid lower and upper bound for the input of each neuron i in layer l.
We discuss the computation of these bounds in Sec. 2.2. For now, we assume that Ll
i are
sufﬁciently small and large numbers, respectively, i.e., the so-called Big-M values. Next, we provide
the standard constraint representation of ReLU neural networks. For sake of simplicity, we describe
the formulation for one layer l of the model at neuron i and one input data point x:

i ≥ 0, and 0 otherwise. Finally, let Ll

i and U l

hl
i + (1 − zl

i)Ll

h0
i = xi
hl
i ≥ 0,
i ≤ wl
ihl−1 + bl
i,
iU l
hl
i ≤ zl
i ,
ihl−1 + bl
i ≥ wl
hl
i,
zl
i ∈ {0, 1}.

if l = 0, otherwise

(1a)

(1b)

(1c)

(1d)

(1e)

(1f)

In (1a), the initial decision vector h0 is forced to be equal to the input x of the ﬁrst layer. When zl
i
i to be zero, reﬂecting a non-active neuron. If an entry of zl
is 0, constraints (1b) and (1d) force hl
i
is 1, then constraints (1c) and (1e) enforce hl
ihl−1 + bl
i. See [38, 39] for details.
After formulating the ReLU, if we relax the binary constraint (1f) on zl
i to [0, 1], we obtain a linear
programming problem which is easier and faster to solve. Furthermore, the quality (tightness) of
such relaxation highly depends on the choice of tight upper and lower bounds, U l
i. In fact, the
determination of tight bounds reduces the search space and hence, the solving time.

i to be equal to wl

i , Ll

2 MIP Constraints

In what follows, we adapt the MIP constraints (1) to quantify neuron importance, and we describe
the computation of the bounds Ll
i . Our goal is to compute importance scores for all layers in
the model in an integrated fashion, as Yu et al. [28] have shown to lead to better predictive accuracy
than layer by layer.

i and U l

2.1 ReLU Layers

In ReLU activated layers, we keep the previously introduced binary variables zl
i, and continuous
i. Additionally, we create the continuous decision variables sl
variables hl
i ∈ [0, 1] representing neuron
i importance score in layer l. In this way, we modiﬁed the ReLU constraints (1) by adding the neuron
importance decision variable sl

i to constraints (1c) and (1e):

i + (1 − zl
hl

i)Ll

i ≤ wl

ihl−1 + bl

i − (1 − sl

i) max (U l

i , 0)

(2a)

4

i ≥ wl
hl

ihl−1 + bl

i − (1 − sl

i) max (U l

i , 0).

(2b)

i = 1, hl

In (2), when neuron i is activated due to the input hl−1, i.e. zl
i is equal to the right-hand-side
of those constraints. This value can be directly decreased by reducing the neuron importance sl
i.
When neuron i is non-active, i.e. zl
i = 0, constraint (2b) becomes irrelevant as its right-hand-side is
negative. This fact together with constraints (1b) and (1d), imply that hl
i is zero. Now, we claim that
constraint (2a) allows sl
i to be zero if that neuron is indeed non-important, i.e., for all possible input
data points, neuron i is not activated. This claim can be shown through the following observations.
Note that decisions h and z must be replicated for each input data point x as they present the
propagation of x over the neural network. On the other hand, s evaluates the importance of each
neuron for the main learning task and thus, it must be the same for all data input points. Thus, the key
ingredients are the bounds Ll
i that are computed for each input data point, as explained in
Sec. 2.2. In this way, if U l
i can be zero without interfering with the constraints (2).
The latter is enforced by the objective function derived in Sec. 3. We note that this MIP formulation
can naturally be extended to convolutional layers converted to matrix multiplication using toeplitz
matrix [18] and with an importance score associated with each feature map. We refer the reader to
the appendix for a detailed explanation.

i and U l
i is non-positive, sl

2.2 Bounds Propagation

In the previous MIP formulation, we assumed a large upper bound U l
i and a small lower bound Ll
i.
However, using large bounds may lead to long computational times and a lost on the freedom to
reduce the importance score as discussed above. In order to overcome these issues, we tailor these
bounds accordingly with their respective input point x by considering small perturbations on its value:

L0 = x − (cid:15)
U 0 = x + (cid:15)
Ll = W (l −)U l−1 + W (l +)Ll−1
U l = W (l +)U l−1 + W (l −)Ll−1

W (l −) (cid:44) min (W (l ), 0)
W (l +) (cid:44) max (W (l ), 0).

(3a)

(3b)

(3c)

(3d)

(3e)

(3f)

Propagating the initial bounds of the input data points throughout the trained model will create the
desired bound using simple arithmetic interval [44]. The obtained bounds are tight, narrowing the
space of feasible solutions.

3 MIP Objectives

The aim for the proposed framework is to sparsify non-critical neurons without reducing the predictive
accuracy of the pruned ANN To this end, we combine two optimization objectives.

Our ﬁrst objective is to maximize the set of neurons sparsiﬁed from the trained ANN. Let n be the
number of layers, N l the number of neurons at layer l, and I l = (cid:80)N l
i − 2) be the sum of neuron
importance scores at layer l with sl
i scaled down to the range [−2, −1]. We refer the reader to the
appendix B.4 for re-scaling experiments.

i=1(sl

In order to create a relation between neurons’ importance score in different layers, our objective
becomes the maximization on the amount of neurons sparsiﬁed from the n − 1 layers with higher
score I l. Hence, we denote A = {I l : l = 1, . . . , n} and formulate the sparsity loss as

sparsity =

max
A(cid:48) ⊂A,|A(cid:48) |=(n−1)

(cid:88)

I

I∈A(cid:48)

(cid:80)n

l=1 |N l|

.

(4)

Here, the objective is to maximize the number of non-critical neurons at each layer compared to other
layers in the trained neural model. Note that only the n − 1 layers with the largest importance score

5

will weight in the objective, allowing to reduce the pruning effort on some layer that will naturally
have low scores. The sparsity quantiﬁcation is then normalized by the total number of neurons.

Our second objective is to minimize the loss of important information due to the sparsiﬁcation of the
trained neural model. Additionally, we aim for this minimization to be done without relying on the
values of the logits, which are closely correlated with neurons pruned at each layer. Otherwise, this
would drive the MIP to simply give a full score of 1 to all neurons in order to keep the same output
logit value. Instead, we formulate this optimization objective using the marginal softmax as proposed
in [45]. Using marginal softmax allows the solver to focus on minimizing the misclassiﬁcation error
without relying on logit values. Marginal softmax loss avoids putting a large weight on logits coming
from the trained neural network and predicted logits from decision vector hn computed by the MIP.
On the other hand, in the proposed marginal softmax loss, the label having the highest logit value is
the one optimized regardless its value. Formally, we write the objective

softmax =

N n
(cid:88)

i=1

log

(cid:34)

(cid:88)

c

(cid:35)

exp(hn

i,c)

−

N n
(cid:88)

(cid:88)

i=1

c

Yi,chn

i,c,

(5)

where index c stands for the class label. The used marginal softmax objective keeps the correct
predictions of the trained model for the input batch of images x having one hot encoded labels Y
without considering the logit value.

Finally, we combine the two objectives to formulate the multi-objective loss

loss = sparsity + λ · softmax

(6)

as a weighted sum of sparsiﬁcation regularizer and marginal softmax, as proposed by Ehrgott [46].
Our experiments revealed that λ = 5 generally provides the right trade-off between our two objectives;
see the appendix for experiments with value of λ.

4 Empirical Results

We ﬁrst show in Sec. 4.2 the robustness of our proposed formulation to different input data points
and different convergence levels of a neural network. Next, in Sec. 4.3, we validate empirically
that the computed neuron importance scores are meaningful, i.e. it is crucial to guide the pruning
accordingly with the determined scores. In Sec. 4.4, we proceed with experiments to show that
sub-networks generated by our approach on a speciﬁc initialization can be transferred to another
dataset with marginal loss in accuracy (lottery hypothesis). Finally, in Sec. 4.5, we compare our
masking methodology to [28], a framework used to compute connections sensitivity, and to create a
sparsiﬁed sub-network based on the input dataset and model initialization. Before advancing to our
results, we detail our experimental settings3.

4.1 Experimental Setting

Architectures and Training We used a simple fully connected 3-layer ANN (FC-3) model, with
300+100 hidden units, from [47], and another simple fully connected 4-layer ANN (FC-4) model,
with 200+100+100 hidden units. In addition, we used convolutional LeNet-5 [47] consisting of two
sets of convolutional and average pooling layers, followed by a ﬂattening convolutional layer, then
two fully-connected layers. The largest architecture investigated was VGG-16 [20] consisting of
a stack of convolutional (conv.) layers with a very small receptive ﬁeld: 3 × 3. The VGG-16 was
adapted for CIFAR-10 [48] having 2 fully connected layers of size 512 and average pooling instead
of max pooling. Each of these models was trained 3 times with different initialization.

All models were trained for 30 epochs using RMSprop [49] optimizer with 1e-3 learning rate for
MNIST and Fashion MNIST. Lenet 5 [47] on CIFAR-10 was trained using SGD optimizer with
learning rate 1e-2 and 256 epochs. VGG-16 [20] on CIFAR-10 was trained using Adam [50] with
1e-2 learning rate for 30 epochs. Decoupled greedy learning [51] was used to train each VGG-16’s
layer using a small auxiliary network, and the neuron importance score was computed independently
on each auxiliary network; then we ﬁne-tuned the generated masks for 1 epoch to propagate error
across them. Decoupled training of each layer allowed us to represent deep models using the MIP

3The code can be found here: https://github.com/chair-dsgt/mip-for-ann.

6

formulation and to parallelize the computation per layer; see appendix for details about decoupled
greedy learning. The hyper parameters were tuned on the validation set’s accuracy. All images were
resized to 32 by 32 and converted to 3 channels to generalize the pruned network across different
datasets.

MIP and Pruning Policy Using all the training set as input to the MIP solver is intractable. Hence,
we only use a subset of the data points to approximate the neuron importance score. Representing
classes with a subset of the data points would give us an under estimation of the score, i.e., neurons
will look less critical than they really are. To that extent, the selected subset of data points must be
carefully chosen. Whenever we computed neuron scores for a trained model, we fed the MIP with a
balanced set of images, each representing a class of the classiﬁcation task. The aim was to avoid that
the determined importance scores lead to pruning neurons (features) critical to a class represented by
fewer images as input to the MIP. We used λ = 5 in the MIP objective function (6); see appendix for
experiments with the value of λ. The proposed framework, recall Figure 1, computes the importance
score of each neuron, and with a small tuned threshold based on the network’s architecture, we
masked (pruned) non-critical neurons with a score lower than it.

Computational Environment The experiments were performed in an Intel(R) Xeon(R) CPU @
2.30GHz with 12 GB RAM and Tesla k80 using Mosek 9.1.11 [52] solver on top of CVXPY [53, 54]
and PyTorch 1.3.1 [55].

4.2 MIP Robustness

(a) Effect of changing validation set of input images. (b) Evolution of the computed masked subnetwork

during model training.

Table 1: Comparing test accuracy of LeNet-5 on
imbalanced independent class by class (IMIDP.),
balanced independent class by class (IDP.) and si-
multaneously all classes (SIM) with 0.01 threshold,
and λ = 1.

We examine the robustness of our formulation
against different batches of input images fed into
the MIP. Namely, we used 25 randomly sam-
pled balanced images from the validation set.
Figure 2a shows that changing the input images
used by the MIP to compute neuron importance
scores resulted in marginal changes in the test
accuracy between different batches. We remark
that the input batches may contain images that
were misclassiﬁed by the neural network. In
this case, the MIP tries to use the score s to
achieve the true label, explaining the variations
on the pruning percentage. Indeed, as discussed
in appendix for the choice of λ, the marginal
ﬂuctuations of these results depend on the ac-
curacy of the input batch used in the MIP. Additionally, we demonstrate empirically that we can
parallelize the computation of neuron scores per class as it shows comparable results to feeding
all data points to the MIP at once; see Table 1 and appendix for extensive experiments. For those
experiments, we sampled a random number of images per class, and then we took the average of the
computed neuron importance scores from solving the MIP on each class. The obtained sub-networks
were compared to solving the MIP with 1 image per class. We achieved comparable results in terms
of test accuracy and pruning percentage. In brief, our method is empirically shown to be scalable and
that class contribution can be decoupled without deteriorating the approximation of neuron scores
and thus, the performance of our methodology.

FASHION-MNIST
89.5% ± 0.3
87.3% ± 0.3
21.8% ± 0.5
88% ± 0.1
18.1% ± 0.3
87.9% ± 0.1
18.8% ± 1.3

MNIST
98.8% ± 0.09
98.6% ± 0.15
19.8% ± 0.18
98.6% ± 0.1
15% ± 0.1
98.4% ± 0.3
13.2% ± 0.42

REF.
IDP.
PRUNE (%)
IMIDP.
PRUNE (%)
SIM.
PRUNE (%)

7

To conclude on the robustness of the scores computed based on the input points used in the MIP, we
show in Table 1 that our formulation is robust even when an imbalanced number of data points per
class is used in the MIP.

Finally, we also tested the robustness of our approach along the evolution of neuron importance
scores during training between epochs. To this end, we computed neuron importance scores after
each epoch jointly with the respective pruning. As shown in Figure 2b, our proposed formulation can
identify non-critical neurons in the network before the model’s convergence.

4.3 Comparison to Random and Critical Pruning

We started by training a reference model (REF.) using the training parameters in Sec. 4.1. After
training and evaluating the reference model on the test set, we fed an input batch of images from
the validation set to the MIP. Then, the MIP solver computed the neuron importance scores based
on those input images. In our experimental setup, by taking advantage of the conclusions from the
previous section, we used 10 images, each representing a class.

Table 2: Pruning results on fully connected (FC-3, FC-4) and convolutional (Lenet-5, VGG-16)
network architectures using three different datasets. We compare the test accuracy between the
unpruned reference network (REF.), randomly pruned model (RP.), model pruned based on critical
neurons selected by the MIP (CP.) and our non-critical pruning approach with (OURS + FT) and
without (OURS) ﬁne-tuning for 1 epoch.

REF.

RP.

CP.

OURS

OURS + FT

PRUNE (%)

THRESHOLD

MNIST

FASHION-MNIST

CIFAR-10

FC-3
FC-4
LENET-5

FC-3
FC-4
LENET-5

LENET-5
VGG-16

98.1% ± 0.1
97.9% ± 0.1
98.9% ± 0.1

87.7% ± 0.6
88.9% ± 0.1
89.7% ± 0.2

83.6% ± 4.6
77.1% ± 4.8
56.9% ± 36.2

35.3% ± 6.9
38.3% ± 4.7
33% ± 24.3

44.5% ± 7.2
50% ± 15.8
38.6% ± 40.8

11.7% ± 1.2
16.6% ± 4.1
28.6% ± 26.3

95.9% ± 0.87
96.6% ± 0.4
98.7% ± 0.1

97.8 ± 0.2
97.6% ± 0.01
98.9% ± 0.04

80% ± 2.7
86.9% ± 0.7
87.7% ± 2.2

88.1% ± 0.2
88% ± 0.03
89.8% ± 0.4

44.5% ± 7.2
42.9% ± 4.5
17.2% ± 2.4

68% ± 1.4
60.8% ± 3.2
17.8% ± 2.1

72.2% ± 0.2
83.9% ± 0.4

50.1% ± 5.6
85% ± 0.4

27.5% ± 1.7
83.3% ± 0.3

67.7% ± 2.2
N/A

68.6% ± 1.4
85.3% ± 0.2

9.9% ± 1.4
36% ± 1.1

0.1
0.1
0.2

0.1
0.1
0.2

0.3
0.3

In order to validate our pruning policy guided by the computed importance scores, we created different
sub-networks of the reference model, where the same number of neurons is removed in each layer,
thus allowing a fair comparison among them. These sub-networks were obtained through different
procedures: non-critical (our methodology), critical and randomly pruned neurons. For VGG-16
experiments, an extra ﬁne-tuning step for 1 epoch is performed on all generated sub-networks.
Although we pruned the same number of neurons, which accordingly with [56] should result in
similar performances, Table 2 shows that pruning non-critical neurons results in marginal loss and
gives better performance. On the other hand, we observe a signiﬁcant drop on the test accuracy when
critical or a random set of neurons are removed compared with the reference model. If we ﬁne-tune
for just 1 epoch the sub-network obtained through our method, the model’s accuracy can surpass the
reference model. This is due to the fact that the MIP, while computing neuron scores, is solving its
marginal softmax (5) on true labels.

4.4 Generalization Between Different Datasets

In this experiment, we train
the model on a dataset d1,
and we create a masked
neural model using our ap-
proach. After creating the
masked model, we restart it
to its original initialization.
Finally,
the new masked
model is re-trained on an-
other dataset d2, and its gen-
eralization is analyzed.

Table 3: Cross-dataset generalization: sub-network masking is com-
puted on source dataset (d1) and then applied to target dataset (d2)
by retraining with the same early initialization. Test accuracies are
presented for masked and unmasked (REF.) networks on d2, as well as
pruning percentage.

MODEL

SOURCE DATASET d1

TARGET DATASET d2

REF. ACC. MASKED ACC.

PRUNING (%)

LENET-5

MNIST

VGG-16

CIFAR-10

FASHION MNIST
CIFAR-10

MNIST
FASHION-MNIST

89.7% ± 0.3
72.2% ± 0.2

99.1% ± 0.1
92.3% ± 0.4

89.2% ± 0.5
68.1% ± 2.5

99.4% ± 0.1
92.1% ± 0.6

16.2% ± 0.2

36% ± 1.1

Table 3 displays our experiments and respective results. When we compare generalization results
to pruning using our approach on Fashion-MNIST and CIFAR-10, we discover that computing the

8

critical sub-network LeNet-5 architecture on MNIST, is creating a more sparse sub-network with
test accuracy better than zero-shot pruning without ﬁne-tuning using our approach, and comparable
accuracy with the original ANN. This behavior is happening because the solver is optimizing
on a batch of images that are classiﬁed correctly with high conﬁdence from the trained model.
Furthermore, computing the critical VGG-16 sub-network architecture on CIFAR-10 using decoupled
greedy learning [51] generalizes well to Fashion-MNIST and MNIST.

4.5 Comparison to SNIP

Our proposed framework can be viewed as a compression technique of over-parameterized neural
models. In what follows, we compare it to the state-of-the-art framework: SNIP [13]. SNIP creates
the sparse model before training the neural model by computing the sensitivity of connections. This
allows the identiﬁcation of the important connections. In our methodology, we exclusively identify
the importance of neurons and prune all the connections of non-important ones. On the other hand,
SNIP only focus on pruning neurons’ connections. Moreover, we highlight that SNIP can only
compute connection’s sensitivity on ANN’s initialization. As for a trained ANN, the magnitude of the
derivatives with respect to the loss function was optimized during the training, making SNIP more
keen to keep all the parameters. On the other hand, our framework can work on different convergence
levels as shown in Sec. 4.2. Furthermore, the connection’s sensitivity computed is only network and
dataset speciﬁc, thus the computed connection sensitivity for a single connection does not give a
meaningful signal about its importance to the task at hand, but needs to be compared to the sensitivity
of other connections.

In order to bridge the differences between the two methods, and provide a fair comparison in
equivalent settings, we make slight adjustments. We compute neuron importance scores on the
model’s initialization4. We used only 10 images as an input to the MIP corresponding to 10 different
classes, and 128 images as input to SNIP, as in the associated paper [13]. Our algorithm was able to
prune neurons from fully connected and convolutional layers of LeNet-5. After creating the sparse
network using both SNIP and our methodology, we trained them on Fashion-MNIST dataset. The
difference between SNIP (88.8% ± 0.6) and our approach (88.7% ± 0.5) was marginal in terms of
test accuracy. SNIP pruned 55% of the ANN’s parameters and our approach 58.4%.

In brief, we remark that the adjustments made to SNIP and our framework in the previous experiments
are for the purpose of comparison, while the main purpose of our method is to allow optimization
at any stage (before, during, or after training). In the speciﬁc case of optimizing over initialization
and discarding entire neurons based on connection sensitivity, the SNIP approach may have some
advantages, notably in scalability for deep architectures. However, it also has some limitations, as
discussed before.

5 Conclusion

We proposed a mixed integer program to compute neuron importance scores in ReLU-based deep
neural networks. Our contributions focus here on providing scalable computation of importance
scores in fully connected and convolutional layers. We presented results showing these scores can be
effectively used to prune unimportant parts of the network without signiﬁcantly affecting its main
task (e.g., showing small or negligible drop in classiﬁcation accuracy). Further, our results indicate
this approach allows automatic construction of efﬁcient sub-networks that can be transferred and
retrained on different datasets. The presented model introduces one of the ﬁrst steps in understanding
which components in a neural network are critical for its model capacity to perform a given task,
which can have further impact in future work beyond the pruning applications presented here.

Acknowledgments and Disclosure of Funding

This work was partially funded by: IVADO (l’institut de valorisation des données) [G.W., M.C.]; NIH
grant R01GM135929 [G.W.]; FRQ-IVADO Research Chair in Data Science for Combinatorial Game
Theory, and NSERC grant 2019-04557 [M.C.].

4Remark: we used λ = 1 and pruning threshold 0.2 and kept ratio 0.45 for SNIP. Training procedures as in

Section 4.1.

9

References

[1] Yoshua Bengio, Ian Goodfellow, and Aaron Courville. Deep learning, volume 1. Citeseer,

2017.

[2] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444,

2015.

[3] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.

[4] Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro.
Towards understanding the role of over-parametrization in generalization of neural networks.
arXiv preprint arXiv:1805.12076, 2018.

[5] Nicholas D Lane, Sourav Bhattacharya, Petko Georgiev, Claudio Forlivesi, and Fahim Kawsar.
An early resource characterization of deep learning on wearables, smartphones and internet-of-
things devices. In Proceedings of the 2015 international workshop on internet of things towards
applications, pages 7–12, 2015.

[6] He Li, Kaoru Ota, and Mianxiong Dong. Learning iot in edge: Deep learning for the internet of

things with edge computing. IEEE network, 32(1):96–101, 2018.

[7] Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Advances in neural

information processing systems, pages 598–605, 1990.

[8] Babak Hassibi, David G Stork, and Gregory J Wolff. Optimal brain surgeon and general network
pruning. In IEEE international conference on neural networks, pages 293–299. IEEE, 1993.

[9] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections
for efﬁcient neural network. In Advances in neural information processing systems, pages
1135–1143, 2015.

[10] Suraj Srinivas and R Venkatesh Babu. Data-free parameter pruning for deep neural networks.

arXiv preprint arXiv:1507.06149, 2015.

[11] Xin Dong, Shangyu Chen, and Sinno Pan. Learning to prune deep neural networks via layer-
wise optimal brain surgeon. In Advances in Neural Information Processing Systems, pages
4857–4867, 2017.

[12] Wenyuan Zeng and Raquel Urtasun. Mlprune: Multi-layer pruning for automated neural
network compression. In International Conference on Learning Representations (ICLR), 2018.

[13] Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. Snip: Single-shot network

pruning based on connection sensitivity. arXiv preprint arXiv:1810.02340, 2018.

[14] Chaoqi Wang, Roger Grosse, Sanja Fidler, and Guodong Zhang. Eigendamage: Structured
pruning in the kronecker-factored eigenbasis. arXiv preprint arXiv:1905.05934, 2019.

[15] Abdullah Salama, Oleksiy Ostapenko, Tassilo Klein, and Moin Nabi. Pruning at a glance:
Global neural pruning for model compression. arXiv preprint arXiv:1912.00200, 2019.

[16] Thiago Serra, Abhinav Kumar, and Srikumar Ramalingam. Lossless compression of deep neural

networks. arXiv preprint arXiv:2001.00218, 2020.

[17] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.

[18] Robert M Gray. Toeplitz and circulant matrices: A review, 2002. URL http://ee. stanford. edu/˜

gray/toeplitz. pdf, 2000.

[19] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional
neural networks for resource efﬁcient inference. arXiv preprint arXiv:1611.06440, 2016.

[20] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale

image recognition. arXiv preprint arXiv:1409.1556, 2014.

10

[21] Babak Hassibi and David G Stork. Second order derivatives for network pruning: Optimal brain
surgeon. In Advances in neural information processing systems, pages 164–171, 1993.

[22] Yves Chauvin. A back-propagation algorithm with optimal use of hidden units. In Advances in

neural information processing systems, pages 519–526, 1989.

[23] Andreas S Weigend, David E Rumelhart, and Bernardo A Huberman. Generalization by weight-
elimination with application to forecasting. In Advances in neural information processing
systems, pages 875–882, 1991.

[24] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through
propagating activation differences. In Proceedings of the 34th International Conference on
Machine Learning-Volume 70, pages 3145–3153. JMLR. org, 2017.

[25] Mathias Berglund, Tapani Raiko, and Kyunghyun Cho. Measuring the usefulness of hidden
units in boltzmann machines with mutual information. Neural Networks, 64:12–18, 2015.

[26] LF Barros and B Weber. Crosstalk proposal: an important astrocyte-to-neuron lactate shuttle
couples neuronal activity to glucose utilisation in the brain. The Journal of physiology, 596(3):
347–350, 2018.

[27] Kairen Liu, Rana Ali Amjad, and Bernhard C Geiger. Understanding individual neuron

importance using information theory. arXiv preprint arXiv:1804.06679, 2018.

[28] Ruichi Yu, Ang Li, Chun-Fu Chen, Jui-Hsin Lai, Vlad I Morariu, Xintong Han, Mingfei
Gao, Ching-Yung Lin, and Larry S Davis. Nisp: Pruning networks using neuron importance
score propagation. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 9194–9203, 2018.

[29] Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. A benchmark for inter-
pretability methods in deep neural networks. In Advances in Neural Information Processing
Systems, pages 9734–9745, 2019.

[30] Artur Jordao, Ricardo Kloss, Fernando Yamada, and William Robson Schwartz. Pruning deep

neural networks using partial least squares. arXiv preprint arXiv:1810.07610, 2018.

[31] Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi Yang. Soft ﬁlter pruning for
accelerating deep convolutional neural networks. arXiv preprint arXiv:1808.06866, 2018.

[32] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable

neural networks. arXiv preprint arXiv:1803.03635, 2018.

[33] Ari S Morcos, Haonan Yu, Michela Paganini, and Yuandong Tian. One ticket to win them
all: generalizing lottery ticket initializations across datasets and optimizers. arXiv preprint
arXiv:1906.02773, 2019.

[34] Vivek Ramanujan, Mitchell Wortsman, Aniruddha Kembhavi, Ali Farhadi, and Mohammad
Rastegari. What’s hidden in a randomly weighted neural network? In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11893–11902, 2020.

[35] Yulong Wang, Xiaolu Zhang, Lingxi Xie, Jun Zhou, Hang Su, Bo Zhang, and Xiaolin Hu.

Pruning from scratch. In AAAI, pages 12273–12280, 2020.

[36] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir. Proving the lottery ticket

hypothesis: Pruning is all you need. arXiv preprint arXiv:2002.00585, 2020.

[37] Chaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before training by

preserving gradient ﬂow. arXiv preprint arXiv:2002.07376, 2020.

[38] Matteo Fischetti and Jason Jo. Deep neural networks and mixed integer linear optimization.

Constraints, 23(3):296–309, 2018.

[39] Ross Anderson, Joey Huchette, Christian Tjandraatmadja, and Juan Pablo Vielma. Strong mixed-
integer programming formulations for trained neural networks. In International Conference on
Integer Programming and Combinatorial Optimization, pages 27–42. Springer, 2019.

11

[40] Vincent Tjeng, Kai Xiao, and Russ Tedrake. Evaluating robustness of neural networks with

mixed integer programming. arXiv preprint arXiv:1711.07356, 2017.

[41] Po-Sen Huang, Robert Stanforth, Johannes Welbl, Chris Dyer, Dani Yogatama, Sven Gowal,
Krishnamurthy Dvijotham, and Pushmeet Kohli. Achieving veriﬁed robustness to symbol
substitutions via interval bound propagation. arXiv preprint arXiv:1909.01492, 2019.

[42] M.l R. Garey and D. S. Johnson. Computers and Intractability: A Guide to the Theory of
NP-Completeness. W. H. Freeman & Co., New York, NY, USA, 1979. ISBN 0716710447.

[43] G. L. Nemhauser and L. A. Wolsey.

Integer and Combinatorial Optimization. Wiley-

Interscience, New York, NY, USA, 1988. ISBN 0-471-82819-X.

[44] Ramon E Moore, R Baker Kearfott, and Michael J Cloud. Introduction to interval analysis,

volume 110. Siam, 2009.

[45] Kevin Gimpel and Noah A Smith. Softmax-margin crfs: Training log-linear models with
cost functions. In Human Language Technologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computational Linguistics, pages 733–736. Association
for Computational Linguistics, 2010.

[46] Matthias Ehrgott. Multicriteria optimization, volume 491. Springer Science & Business Media,

2005.

[47] Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning

applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

[48] Alex Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis,

University of Toronto, 2009.

[49] Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):
26–31, 2012.

[50] D Kingma and J Ba. Adam: A method for stochastic optimization in: Proceedings of the 3rd

international conference for learning representations (iclr’15). San Diego, 2015.

[51] Eugene Belilovsky, Michael Eickenberg, and Edouard Oyallon. Decoupled greedy learning of

CNNs. arXiv preprint arXiv:1901.08164, 2019.

[52] APS Mosek. The mosek optimization software. Online at http://www. mosek. com, 54(2-1):5,

2010.

[53] Akshay Agrawal, Robin Verschueren, Steven Diamond, and Stephen Boyd. A rewriting system
for convex optimization problems. Journal of Control and Decision, 5(1):42–60, 2018.

[54] Steven Diamond and Stephen Boyd. CVXPY: A Python-embedded modeling language for

convex optimization. Journal of Machine Learning Research, 17(83):1–5, 2016.

[55] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative
style, high-performance deep learning library. In Advances in Neural Information Processing
Systems, pages 8024–8035, 2019.

[56] Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value

of network pruning. In International Conference on Learning Representations, 2018.

[57] Tom Brosch and Roger Tam. Efﬁcient training of convolutional deep belief networks in the
frequency domain for application to high-resolution 2D and 3D images. Neural computation,
27(1):211–227, 2015.

[58] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. In International Conference on Machine Learning, pages
448–456, 2015.

12

A Appendix

B MIP formulations

In Appendix B.1, details on the MIP constraints for convolutional layers are provided. Appendix B.2
explains the formulation used to represent pooling layers. Appendix B.3 discusses the parameter λ in
the objective function (6) guiding the computation of neuron importance scores.

B.1 MIP for convolutional layers

We convert the convolutional feature map to a toeplitz matrix and the input image to a vector. This
allow us to use simple matrix multiplication which is computationally efﬁcient. Moreover, we can
represent the convolutional layer using the same formulation of fully connected layers presented in
Sec. 2.

Toeplitz Matrix is a matrix in which each value is along the main diagonal and sub diagonals are
constant. So given a sequence an, we can create a Toeplitz matrix by putting the sequence in the ﬁrst
column of the matrix and then shifting it by one entry in the following columns:





















a0

a1

a2
...
...
...
...
a(N −1)

a−1 a−2

· · ·

· · ·

· · ·

· · ·

a0

a1

a2

a−1 a−2

a0
. . .
. . .

a−1
. . .
. . .
. . .

· · ·

· · ·

· · ·

. . .
. . .
. . .

a1

a2
· · ·

. . .
. . .

a0

a1
a2

a−2

a−1

a0
a1





















.

a−(N −1)
...
...
...
...
a−2

a−1
a0

(7)

Feature maps are ﬂipped and then converted to a matrix. The computed matrix when multiplied by
the vectorized input image will compute the fully convolutional output. For padded convolution we
use only parts of the output of the full convolution, for strided convolutions we used sum of 1 strided
convolution as proposed by Brosch and Tam [57]. First, we pad zeros to the top and right of the input
feature map to become same size as the output of the full convolution. Next, we create a toeplitz
matrix for each row of the zero padded feature map. Finally, we arrange these small toeplitz matrices
in a big doubly blocked toeplitz matrix. Each small toeplitz matrix is arranged in the doubly toeplitz
matrix in the same way a toeplitz matrix is created from input sequence with each small matrix as an
element of the sequence.

B.2 Pooling Layers

We represent both average and max pooling on multi-input units in our MIP formulation. Pooling
layers are used to reduce spatial representation of input image by applying an arithmetic operation on
each feature map of the previous layer.

Avg Pooling layer applies the average operation on each feature map of the previous layer. This
operation is linear and thus, it can directly be included in the MIP constraints:

hl+1 = AvgPool(hl

1, · · · , hl

N l ) =

1
N l

N l
(cid:88)

i=1

hl
i.

Max Pooling takes the maximum of each feature map of the previous layer:

hl+1 = MaxPool(hl

1, · · · , hl

N l ) = max{hl

1, · · · , hl

N l }.

13

(8)

(9)

This operation can be expressed by introducing a set of binary variables m1, · · · , mN l , where mi = 1
implies x = MaxPool(hl

1, · · · , hl

N l):

N l
(cid:88)

mi = 1

i=1
x ≥ hl
i,
imi + Ui(1 − mi)
mi ∈ {0, 1}

x ≤ hl






i = 1, · · · , N l.

(10a)

(10b)

B.3 Choice of Lambda

Note that our objective func-
tion (6) is implicitly using a La-
grangian relaxation, where λ ≥ 0
is the Lagrange multiplier. In fact,
one would like to control the loss
on accuracy (5) by imposing the
constraint softmax(h) ≤ (cid:15) for a
very small )(cid:15), or even to avoid
any loss via (cid:15) = 0. However,
this would introduce a nonlinear
constraint which would be hard to
handle. Thus, for tractability pur-
poses we follow a Lagrangian relaxation on this constraint, and penalize the objective whenever
softmax(h) is positive. Accordingly with the weak (Lagrangian) duality theorem, the objective (6) is
always a lower bound to the problem where we minimize sparsity and a bound on the accuracy loss is
imposed. Furthermore, the problem of ﬁnding the best λ for the Lagrangian relaxation, formulated as

Figure 3: Effect of changing value of λ when pruning LeNet
model trained on Fashion MNIST.

max
λ≥0

min{sparsity + λ · softmax},

(11)

has the well-known property of being concave, which in our experiments revealed to be easily
determined5. We note that the value of λ introduces a trade off between pruning more neurons and
the predictive capacity of the model. For example, increasing the value of λ would result on pruning
fewer neurons, as shown in Figure 3, while the accuracy on the test set would increase.

B.4 Re-scaling of MIP Sparsiﬁcation Objective

Table 4: Importance of re-scaling sparsiﬁcation objective to prune more neurons shown empirically
on LeNet-5 model using threshold 0.05, by comparing accuracy on test set between reference model
(Ref.), and pruned model (Masked).

DATASET

MNIST

FASHION-MNIST

OBJECTIVE
sl
i − 2
sl
i − 1
sl
i
sl
i − 2
sl
i − 1
sl
i

REF. ACC.

98.9% ± 0.1

MASKED ACC.
98.7% ± 0.1
98.8% ± 0.1
98.9% ± 0.2

PRUNING PERCENTAGE (%)
13.2% ± 2.9
9.6% ± 1.1
8% ± 1.6

89.9% ± 0.2

89.1% ± 0.3
89.2% ± 0.1
89% ± 0.4

17.1% ± 1.2
17% ± 3.4
10.8% ± 2.1

In Table 4, we compare re-scaling the neuron importance score in the objective function to [−2, −1],
to [−1, 0] and no re-scaling [0, 1]. This comparison shows empirically the importance of re-scaling
the neuron importance score to optimize sparsiﬁcation through neuron pruning.

5We remark that if the trained model has misclassiﬁcations, there is no guarantee that problem (11) is

concave.

14

C Generalization comparison between SNIP and our approach

Table 5: Cross-dataset generalization comparison between SNIP, with neurons having the lowest
sum of connections’ sensitivity pruned, and our framework (Ours), both applied on initialization, see
Section 4.4 for the generalization experiment description.

SOURCE DATASET d1

TARGET DATASET d2

REF. ACC. METHOD MASKED ACC.

PRUNING (%)

MNIST

FASHION-MNIST

89.7% ± 0.3

CIFAR-10

72.2% ± 0.2

SNIP
OURS

SNIP
OURS

85.8% ± 1.1
88.5% ± 0.3

53.5% ± 1.8
59.1% ± 0.8

53.5% ± 3.3
63.6% ± 1.4

53.5% ± 1.8
59.1% ± 0.8

In Table 5, we show that our framework outperforms SNIP in terms of generalization. We adjusted
SNIP to prune entire neurons based on the value of the sum of its connections’ sensitivity, and
our framework was also applied on ANN’s initialization. When our framework is applied on the
initialization, more neurons are pruned as the marginal softmax part of the objective discussed in
Section 3 is weighting less (λ = 1), driving the optimization to focus on model sparsiﬁcation.

D Scalability improvements

Appendix D.1 provides computational evidence on the parallelization of the importance scores
computation. In Appendix D.2, we describe a methodology that aims at speed-up the computation of
neuron importance scores by relying on decoupled greedy learning.

D.1 MIP Class by Class

In this experiment, we show that the neuron importance scores can be approximated by 1) solving for
each class the MIP with only one data point from it, and 2) taking the average of the computed scores
for each neuron. Such procedure would speed-up our methodology for problems with a large number
of classes. We compare the subnetworks obtained through this independent class by class approach
(IDP.) and by feeding at once the same data points from all the classes to the MIP (SIM.) on Mnist
and Fashion-Mnist using LeNet-5.

Table 6: Comparing balanced independent class by class (IDP.) and simultaneously all classes (SIM.)
with different thresholds using LeNet-5.

REF.
IDP.
PRUNE (%)
SIM.
PRUNE (%)
IDP.
PRUNE (%)
SIM.
PRUNE (%)

MNIST
98.8% ± 0.09
96.6 ± 2.4%
28.4 ± 1.5%
98.5 ± 0.28%
16.5 ± 0.5%
98.6% ± 0.15
19.8% ± 0.18
98.4% ± 0.3
13.2% ± 0.42

FASHION-MNIST
89.5% ± 0.3
86.81 ± 1.2%
29.6 ± 1.8%
88.7 ± 0.4%
18.9 ± 1.4%
87.3% ± 0.3
21.8% ± 0.5
87.9% ± 0.1
18.8% ± 1.3

THRESHOLD

0.1

0.1

0.01

0.01

Table 6 expands the results presented in Sec. 4.2, where we had discussed the comparable results
between IDP. and SIM. when we use a small threshold 0.01. However, we can notice a difference
between both of them when we use a threshold of 0.1. This difference comes from the fact that
computing neuron importance scores on each class independently zeros out more neuron scores
resulting in an average that leads more neurons to be pruned.

D.2 Decoupled Greedy Learning

We use decoupled greedy learning [51] to parallelize learning of each layer by computing its gradients
and using an auxiliary network attached to it. By using this procedure, we have auxiliary networks
of the deep neural network that represent subsets of layers thus allowing us to solve the MIP in
sub-representations of the neural network.

15

Training procedure We start by constructing auxiliary networks for each convolutional layer
except the last convolutional layer that will be attached to the classiﬁer part of the model. During the
training each auxiliary network is optimized with a separate optimizer and the auxiliary network’s
output is used to predict the back-propagated gradients. Each sub-network’s input is the output of the
previous sub-network and the gradients will ﬂow through only the current sub-network. In order to
parallelize this operation a replay buffer of previous representations should be used to avoid waiting
for output from previous sub-networks during training.

Auxiliary Network Architecture We use a spatial averaging operation to construct a scalable
auxiliary network applied to the output of the trained layer and to reduce the spatial resolution by
a factor of 4, then applying one 1 × 1 convolution with batchnorm [58] followed by a reduction to
2 × 2 and a one-layer MLP. The architecture used for the auxiliary network is smaller than the one
mentioned in the paper leading to speed up the MIP solving time per layer.

MIP representation After training each sub-network, we create a separate MIP formulation for
each auxiliary network using its trained parameters and taking as input the output of the previous sub-
network. This operation can be easily parallelized and each sub-network can be solved independently.
Then, we take the computed neuron importance scores per layer and apply them to the main deep
neural network. Since these layers were solved independently, we ﬁne tune the network for one epoch
to back-propagate the error across the network. The created sub-network can be generalized across
different datasets and yields marginal loss.

16

