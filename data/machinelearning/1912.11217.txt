Safe Sample Screening for Robust Support Vector Machine

Zhou Zhai1, Bin Gu1,2*, Xiang Li3, Heng Huang4
1 School of Computer & Software, Nanjing University of Information Science & Technology, P.R.China
2JD Finance America Corporation
3 Computer Science Department, University of Western Ontario, Canada
4Department of Electrical & Computer Engineering, University of Pittsburgh, USA
zhouzhai@nuist.edu.cn, jsgubin@gmail.com, lxiang2@uwo.ca, henghuanghh@gmail.com

9
1
0
2

c
e
D
4
2

]

G
L
.
s
c
[

1
v
7
1
2
1
1
.
2
1
9
1
:
v
i
X
r
a

Abstract

Robust support vector machine (RSVM) has been shown to
perform remarkably well to improve the generalization per-
formance of support vector machine under the noisy envi-
ronment. Unfortunately, in order to handle the non-convexity
induced by ramp loss in RSVM, existing RSVM solvers of-
ten adopt the DC programming framework which is computa-
tionally inefﬁcient for running multiple outer loops. This hin-
ders the application of RSVM to large-scale problems. Safe
sample screening that allows for the exclusion of training
samples prior to or early in the training process is an effective
method to greatly reduce computational time. However, ex-
isting safe sample screening algorithms are limited to convex
optimization problems while RSVM is a non-convex prob-
lem. To address this challenge, in this paper, we propose two
safe sample screening rules for RSVM based on the frame-
work of concave-convex procedure (CCCP). Speciﬁcally, we
provide screening rule for the inner solver of CCCP and an-
other rule for propagating screened samples between two suc-
cessive solvers of CCCP. To the best of our knowledge, this is
the ﬁrst work of safe sample screening to a non-convex opti-
mization problem. More importantly, we provide the security
guarantee to our sample screening rules to RSVM. Experi-
mental results on a variety of benchmark datasets verify that
our safe sample screening rules can signiﬁcantly reduce the
computational time.

Introduction
In supervised learning, support vector machine (SVM)
(Chang and Lin 2011; Platt 1998; Gu and Sheng 2016;
Li et al. 2015; Gu et al. 2018a; Gu et al. 2015; Gu et al. 2016;
Gu, Sheng, and Li 2015; Gu 2018) is a powerful classiﬁca-
tion method that is widely used to separate data by max-
imizing the margin between two classes. However, real-
world data tend to be massive in quantity but with quite a
few unreliable outliers. Traditional SVM usually use con-
vex hinge loss function to calculate the loss of misclassi-
ﬁed samples. Since the convex function is unbounded and
puts an extremely large penalty on outliers, traditional SVM
is unstable in the presence of outliers. Robust support vec-
tor machine (RSVM) (Wu and Liu 2007; Shen et al. 2003;

*Contact Author

Copyright © 2020, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

Xu, Crammer, and Schuurmans 2006) suppresses the inﬂu-
ence of outliers on the decision function through clipping
the convex hinge loss to the non-convex ramp loss, and has
been shown to perform remarkably well under the noisy en-
vironment.

The non-convex objective function of RSVM can be
viewed as a difference of convex (DC) (Sriperumbudur and
Lanckriet 2009) programming problem which is normally
solved by the concave-convex procedure (CCCP) (Yuille and
Rangarajan 2003; Collobert et al. 2006; Gu et al. 2018c;
Yu et al. 2019) algorithm. The CCCP algorithm iteratively
solves a sequence of constrained convex optimization prob-
lems. For each loop of CCCP algorithm, it solves a surro-
gate convex optimization problem which linearizes the con-
cave part of the original DC programming problem. The in-
ner surrogate convex optimization problem is very similar
to the problem of SVM, and is normally solved by the se-
quential minimal optimization (SMO) algorithm (Platt 1998;
Vapnik 2013). As pointed out in (Chang and Lin 2011),
the time complexity of SMO algorithm is O(nκ), where
1 < κ < 2.3, n is the number of the training samples. Thus,
the time complexity of CCCP algorithm to solve RSVM is
O(tnκ), where t is the number of loops of the CCCP al-
gorithm. The high computational cost severely hinders the
implementation of RSVM and its application to big data.

To address the above challenging problem, one promis-
ing approach is safe screening. Ghaoui et al. (2010) ﬁrst
exploited safe screening rules to discard inactive features
prior to starting a Lasso solver. They exploited the geometric
quantities of the feature space to bound the Lasso dual solu-
tion to be within a compact region and only need to solve a
smaller optimization problem on the reduced datasets which
leads to huge savings in the computational cost and memory
usage. Since then, the concept of safe screening has been ex-
panded in two main directions. The ﬁrst direction is called
sequential screening, which performs screening along the
entire regularization path which is the sequence of optimal
solutions w.r.t. different values of regularization parameter.
Sequential screening relies on an additional feasible or opti-
mal solution obtained in advance, which can provide a warm
start of the screening process. This direction has been pur-
sued in (Wang et al. 2013; Wang et al. 2014; Liu et al. 2013;

 
 
 
 
 
 
Table 1: Representative safe screening algorithm. (“Type” represents the algorithm screening samples or features).

Problem
SVM
SVM
SVM
Logistic Regression
Lasso

Reference
Zimmert et al. (2015)
Ogawa et al. (2014)
Ogawa et al. (2013)
Wang et al. (2014)
Liu et al. (2013)

Proximal Weighted Lasso Rakotomamonjy et al. (2019)

RSVM

Our

Type
Samples
Samples
Samples
Features
Features
Features
Samples

Type of screening Warm-start

Dynamic
Sequential
Sequential
Sequential
Dynamic
Dynamic
Dynamic

No
Yes
Yes
Yes
No
Yes
Yes

Type of optimization problems
Convex
Convex
Convex
Convex
Convex
Non-convex
Non-convex

Xu and Ramadge 2013; Xiang, Wang, and Ramadge 2016;
El Ghaoui, Viallon, and Rabbani 2011). However, they are
only applicable to algorithms that also compute the regular-
ization paths. The second direction is called dynamic screen-
ing (Bonnefoy et al. 2014; Bonnefoy et al. 2015), which
performs the screening throughout the optimization algo-
rithm itself. For example, Fercoq et al. (2015) proposed a
duality gap based safe feature screening algorithm for lasso.
Although dynamic screening might be useless early in the
training process, it might become efﬁcient as the algorithm
proceeds towards the optimal solution. Further, Rakotoma-
monjy et al. (2019) expanded safe feature screening rule
to lasso with non-convex sparse regularizers. They handled
the non-convexity of the objective through the majorization-
minimization (MM) principle and provided a warm-start
process that allows to propagate screened features from one
MM iteration to the next.

Recently, Ogawa et al. (2013) ﬁrst proposed a safe screen-
ing to indentify non-support vectors for SVM. They ex-
tended the existing feature-screening methods to sample-
screening. On this basis, Ogawa et al. (2014) and Wang et al.
(2014) improved its ability to screen inactive samples. How-
ever, as sequential screening algorithms, they relies on an
additional feasible or optimal solution obtained in advance,
which can be very time consuming. To overcome this dif-
ﬁcult, Zimmert et al. (2015) proposed a dynamic screening
rule using a duality gap function in the primal variables of
hinge loss kernel SVM. We summarized several representa-
tive safe screening algorithms in table 1. It shows that exist-
ing safe feature screening algorithms have been widely used
in convex and non-convex problems while existing safe sam-
ples algorithms are limited to convex problems. Dynamic
screening algorithm for SVM can not provide a warm-start
for training the model, so it only works during the training
the model. It is obvious that the dynamic samples screening
rule for RSVM is still an open problem.

In this paper, we propose two safe sample screening rules
for RSVM based on the framework of concave-convex pro-
cedure (CCCP). Speciﬁcally, we ﬁrst provide a screening
rule for the inner solver of CCCP. Secondly, we provide a
new rule for propagating screened samples between two suc-
cessive solvers of CCCP. To the best of our knowledge, this
is the ﬁrst work of safe sample screening to a non-convex op-
timization problem. More importantly, we provide the secu-
rity guarantee to our sample screening rules to RSVM. Ex-
perimental results on a variety of benchmark datasets verify
that our safe sample screening rules can signiﬁcantly reduce

the computational time.
Contributions. The main contributions of this paper are
summarized as follows:
1. To the best of our knowledge, we are the ﬁrst to propose a
safe samples screening rule for the non-convex problem.
2. By utilizing an iterative CCCP strategy to solve RSVM,
we proposed a safe samples screening rule for propagat-
ing screened samples between two successive solvers of
CCCP.

Preliminaries of Robust Support Vector
Machine
In this section, we ﬁrst give a brief review of RSVM. Then,
we give the primal and dual form of RSVM. At last, we give
the screening set in the RSVM.

Robust Support Vector Machine
We consider a training set D = {(x1, y1), · · · , (xn, yn)}
constituted with n samples , where xi ∈ Rd and yi ∈
{−1, +1}. SVM has a discrimination hyperplane in the fol-
lowing form:

fθ(xi) = wT φ(xi) + b,

(1)
where θ = (w, b) are the parameters of the model, and φ(·)
is a transformation function from an input space to a high-
dimensional reproducing kernel Hilbert space. SVM solves
the following minimization problem:

min
θ

1
2

(cid:107)w(cid:107)2 + C

n
(cid:88)

H1(yifθ(xi))

(2)

i=1
where the function Hs(z) = max(0, s − z) is the hinge
loss. Since the convex hinge loss function is unbounded and
puts an extremely large penalty on outliers, traditional SVM
is unstable in the presence of outliers. We clip the hinge
loss to get the ramp loss Rs(z) = min(s − z, H1(z)) =
H1(z) − Hs(z), where s ≤ 0. The ramp loss is bounded,
meaning that noisy samples cannot inﬂuence the solution be-
yond that of any other misclassiﬁed point. Thus, RSVM can
effectively suppress the inﬂuence of outliers and it solves the
following minimization problem:

min
θ

1
2
(cid:124)

(cid:107)w(cid:107)2 +C

n
(cid:88)

H1(yifθ(xi))

n
(cid:88)

Hs(yifθ(xi))

−C

(3)

i=1
(cid:123)(cid:122)
o(θ)

i=l

(cid:125)

(cid:124)

(cid:123)(cid:122)
v(θ)

(cid:125)

where o and v are real-valued convex functions. It is easy to
see that the objective function (3) is a form of DC.

Primal and Dual Problem
The non-convex objective function of RSVM can be viewed
as a DC programming problem which is normally solved
by the CCCP algorithm. The main mechanism of CCCP al-
gorithm is to iteratively construct an optimized surrogate
objective function which linearizes the concave part of the
original DC programming problem. In order to apply the
CCCP algorithm to solve the problem (3), we ﬁrst have to
calculate derivative of the concave part with respect to θ:
n
(cid:88)

θ · ∇v(θ) = −

µiyifθ(xi)

(4)

where µi =

i=1

(cid:26) C if yifθ(xi) < s
otherwise.

0

(5)

The primal problem can be transformed into the following
optimized surrogate objective:

P (θ) = min

θ

1
2

(cid:107)w(cid:107)2 +C

n
(cid:88)

H1(yifθ(xi))+

n
(cid:88)

µiyifθ(xi)

(6)

i=1

i=1

In this paper, we call the formulation (6) as convex in-
ner loop (CIL) problem. Using Lagrange multiplier method
(Bertsekas 2014), we directly give the dual form of the pri-
mal problem as follows:

αT Hα − yT α

(7)

min
α

s.t.

1
2
n
(cid:88)

αi = 0; C i < αi < C i

i=1

where H is a positive semideﬁnite matrix with Hij =
K(xi, xj) = (cid:104)φ(xi), φ(xj)(cid:105) for all 1 ≤ i, j ≤ n, K(xi, xj)
is the kernel function, C i = min(0, Cyi) − µiy, C i =
max(0, Cyi) − µiy, αi = yi(βi − µi) and βi is the Lagrange
multiplier.

According to the convex optimization theory (Boyd and
Vandenberghe 2004), the dual CIL problem (7) can be trans-
formed into the following min-max form:

D(θ(cid:48)) = min
α

max
b(cid:48)∈R

1
2

αT Hα − yT α + b(cid:48)

(cid:33)

αi

(8)

(cid:32) n
(cid:88)

i=1

where θ(cid:48) = (α, b(cid:48)) are the parameters of the dual CIL prob-
lem and b(cid:48) is the Lagrangian multiplier. Further, from the
KKT theorem (Bazaraa and Shetty 2012), the ﬁrst-order
derivative of D(θ(cid:48)) with respect to α leads to the following
KKT conditions:

∇D(θ(cid:48))i

def=

∂D(θ(cid:48))
∂αi

=

n
(cid:88)

j=1

αjHij + b(cid:48) − yi

(9)

Screening Set
Safe sample screening is built on the KKT optimality condi-
tion (Bertsekas 1997). According to the gradient ∇D(θ(cid:48))i,
we can categorize the n training samples into three cases:

Switching to the primal problem, (10) leads to the following
four cases:

yifθ(xi) > 1 ⇒ αi = 0;
yifθ(xi) = 1 ⇒ αi ∈ [C i, C i];

s ≤ yifθ(xi) < 1 ⇒ αi = yiC;
yifθ(xi) < s ⇒ αi = 0

(11)

(12)
(13)
(14)

If some of the training samples are known to satisfy the case
(11) or (14) in advance, we can throw away those samples
prior to the training stage. Similarly, if we know that some
samples satisfy case (13), we can ﬁx the corresponding αi
at the following training process. Namely, if some knowl-
edge on these ﬁve cases are known a-priori, our training task
would be extremely easy. The samples satisfy the case (11)
or (14) are often called non-support vectors because they
have no inﬂuence on the resulting classiﬁer.

In this paper, we show that, through our safe sample
screening rule, some of the non-SVs and some of the sam-
ples satisfying case (13) or (14) can be screened out prior
to the training process. Then, in the latter training process,
we can train the model with fewer samples to reduce com-
putation time while ensuring consistent results. Suppose that
we obtain an active set A (a subset of D) after applying our
safe sample screening rule, correspondingly, we can deﬁne
an inactive set ¯A = D − A that the variables α ¯A are ﬁxed.
The original optimization (7) can be reduced into a smaller
optimization problem as follows.

AHAAαA − (yA − HA ¯Aα ¯A)T αA (15)
αT

α = 0; C i < αi < C i

i=1

which is a smaller optimization problem. Notice that differ-
ent from existing approximate shrinking heuristics (Chang
and Lin 2011; Joachims 1999b; Gu et al. 2018b; Joachims
1999a; Fan, Chen, and Lin 2005) which sample through a
boundary without well theory guarantees, the active set ob-
tained by our sample screening rule is safe and reliable.

Safe Screening Rule for Single CIL Problem
In this section, we ﬁrst provide the safe screening rule for
single CIL problem. Then, we give the implementation of
single CIL problem. Finally, we analyze the security analy-
sis of sample screening rule.

Safe Screening Rule
As stated in the duality theory, the dual problem D(θ(cid:48)) is
a lower bound on the primal problem P (θ). When strong
duality theorem (Boyd and Vandenberghe 2004) is satisﬁed,
the optimal solution of the dual problem is equal to the op-
timal solution of the primal problem. We deﬁne the duality
gap functions GP (θ) as follows:

min
α

s.t.

1
2
n
(cid:88)

∇D(θ(cid:48))i






> 0 αi = C i
= 0 αi ∈ [C i, C i]
< 0 αi = C i

(10)

GP (θ) = min
θ=θ(θ(cid:48))

GD(θ(θ(cid:48)))

= P (θ) − max
θ=θ(θ(cid:48))

D(θ(cid:48))

(16)

= (cid:107)w(cid:107)2 +C

n
(cid:88)

i=1

H1(yifθ(xi))

+

n
(cid:88)

i=1

µiyifθ(xi) − max

α

(cid:33)

(yi − b(cid:48))αi

(cid:32) n
(cid:88)

i=1

and GD(θ(cid:48)) = P (θ(θ(cid:48))) − D(θ(cid:48)) respectively. The weak
duality theorem (Boyd and Vandenberghe 2004) guarantees
that duality gap is always greater than 0. In the following, we
ﬁrst show that the duality gap is a strong convex function.
Property 1. The duality gap GP (θ) is strongly convex with
parameter θ. Then
GP (θ1) ≥ GP (θ2) + (cid:104)∇GP (θ2), θ1 − θ2(cid:105) + (cid:107)θ1 − θ2(cid:107)2
H

We provide the detailed proof in the appendix. Accord-
ing to the strongly convex property of the duality gap, we
can easily get that the euclidean distance between arbitrarily
feasible solution and the optimal solution is always less than
the current duality gap.
Corollary 1. Let θ∗ = (w∗, b∗) be the optimal solution of
the primal problem. Then we have

(cid:107)θ − θ∗(cid:107) ≤ (cid:112)GD(θ(cid:48))

(17)
We provide the detailed proof in the appendix. Accord-
ing to (9), we can obtain the relation between the feasible
solution and the optimal solution:

∇D(θ(cid:48)∗)i =

=

n
(cid:88)

j=1
n
(cid:88)

j Hij + b(cid:48)∗ − yi
α∗

(18)

(α∗

j − αj)Hij +

n
(cid:88)

j=1

αjHij − yi

j=1
+b(cid:48)∗ − b(cid:48) + b(cid:48)
n
(cid:88)

= ∇D(θ(cid:48))i +

j=1

(α∗

j − αj)Hij + b(cid:48)∗ − b(cid:48)

= ∇D(θ(cid:48))i + (cid:104)θ∗ − θ(cid:48), φ(xi)(cid:105)
Based on Corollary 1, we further obtain the inequality rela-
tion between the euclidean distance from the feasible solu-
tion to the optimal solution of any sample and the current
duality gap.
Corollary 2. Let θ(cid:48)∗ = (α∗, b(cid:48)∗) be the optimal solution of
the dual problem. Denote Kii the entries of the associated
kernel matrix, then for all i = 1, · · · , n we have:
|∇D(θ(cid:48)∗)i − ∇D(θ(cid:48))i| ≤ (cid:112)Kii · GD(θ(cid:48))

(19)
We provide the detailed proof in the appendix. According
to Corollary 2, we know that the optimal solution ∇D(θ(cid:48)∗)i
is always in a circle with the feasible solution ∇D(θ(cid:48))i
as the center of the circle and r = (cid:112)KiiGD(θ(cid:48)) as the
radius. Thus, when this circle does not contain the point
∇D(θ(cid:48)∗)i = 0, we can screen out this sample. The safe sam-
ple screening rule is summarized as follows:

∇D(θ(cid:48))i > (cid:112)KiiGD(θ(cid:48)) ⇒ α∗
∇D(θ(cid:48))i < −(cid:112)KiiGD(θ(cid:48)) ⇒ α∗

i = C i
i = C i

(20)

(21)

Figure 1: Illustration of safe sample screening rule. The
i.e.
points O in the ﬁgure represents support vector
∇D(θ(cid:48)∗)i = 0. During the training process, if a certain sam-
ple i is a support vector, the feasible solution ∇D(θ(cid:48))i must
be in a circle centered at O and radius r = (cid:112)KiiGD(θ(cid:48))
in any iteration. Correspondingly, if the feasible solution
∇D(θ(cid:48))i is at point B, its optimal solution must be in a cir-
cle of the same radius r. At this time, the optimal solution
∇D(θ(cid:48)∗)i must be greater than 0. On the contrary, when the
feasible solution ∇D(θ(cid:48))i is at point A, we are not sure that
the optimal solution ∇D(θ(cid:48))i is equal to 0.

We give the illustration of safe sample screening rule in Fig-
ure 1.

Interpretation
We use a SMO algorithm to solve the CIL problem in its
dual form (7). The core idea of SMO algorithm is to heuris-
tically select two samples that violate the KKT condition to
the largest extent to update. Then, we will update the gradi-
ent of all the samples and the parameter b. SMO algorithm
repeat the process until it converges. The gradient of all the
samples gi is deﬁned as follows:

gi =

n
(cid:88)

j=1

αjHij − yi

(22)

During the training process, in order to use safe sample
screening rule, we need to compute the duality gap. Major
time-consuming of duality gap is compute (cid:107)w(cid:107)2. In the fol-
lowing, we will show that how to use the gradient in the
update process to compute the duality gap easily. According
to the KKT conditions, the (cid:107)w(cid:107)2 is deﬁned as follows:

(cid:107)w(cid:107)2 =

n
(cid:88)

n
(cid:88)

i=1

j=1

αiαjHij =

n
(cid:88)

i=1

(gi + yi)αi

(23)

and the yif (xi) can be easily solved as:

yif (xi) = yi(gi + b) − 1

(24)

Thus, we can avoid recalculating kernel by maintaining the
gradient in each iteration of SMO algorithm. The duality gap
in the early stage can always be large, which makes the dual
and primal estimations inaccurate and ﬁnally results in inef-
fective screening rules. We typically start sample screening
after 50 iterations and screen the samples every 10 iterations.
We summarized the safe sample screening rule for single
CIL problem in Algorithm 1.

Algorithm 1 Safe sample screening for single CIL problem

Input: Training set D, optimization precision (cid:15) .
Output: The optimal solution of α.
1: Initialize α = 0.
2: while optimaliy conditions are not satisﬁed with (cid:15) do
3:
4:
5:
6: end while

Select two samples points update.
Compute the ∇D(θ(cid:48))i and duality gap.
Screening the samples that satisfy (20)-(21).

Theoretical Analysis
The main advantage of our safe sample screening algorithm
is its theoretic guarantee. In the following, we prove that
all inactive samples would be detected and screened by our
screening rule after a ﬁnite number of iterations of the algo-
rithm.
Property 2. Deﬁne the screening set of the CIL problem
as R∗ = {i ∈ D |∇D(θ(cid:48)∗)i| = 0} and Rk = {i ∈
D |∇D(θ(cid:48)k)i| ≤ (cid:112)KiiGD(θ(cid:48)k)} obtained at iteration k
of an algorithm solving the CIL problem. Then, there exists
k0 ∈ N s.t. ∀k ≥ k0, Rk = R∗.

We provide the detailed proof in the appendix.

Safe Screening Rule for Successive CIL
Problems
In this section, we give the propagation behavior of the
screened samples.

Propagating Screening Rule
Prior to this we have introduced the inner solver for single
CIL problem and its safe screening rule, we are going to
analyze how this rule can be improved into solving succes-
sive CIL problems. Each iteration of the CCCP algorithm
approximates the concave part by its tangent and minimizes
the resulting convex function, and the its tangent is always
an upper bound of the concave part:
−C ∗H0(yi(w∗φ(xi) + b∗)) ≤ µ∗
For each inner problem of CCCP, we can perform screening
by using µi as deﬁned in (5), which improves the efﬁciency
of CCCP. However, since the µi’s are also expected to vary
for different iterations, we do not know whether the a screen
sample of iteration k can be safely screened in iteration k+1.
Thus, in the next iteration, we usually need to solve a new
CIL problem due to the change of µi. In the following, we
derive conditions that could be used to propagate screened
samples from one iteration to the next in a CCCP algorithm.
First of all, we give the relation of feasible solutions of

i yi(w∗φ(xi) + b∗)

(25)

any two subproblems:

∇D(θ(cid:48)k+1) =

n
(cid:88)

(αk+1

j − αk

j )Hij + (b(cid:48)k+1 − b(cid:48)k) + ∇D(θ(cid:48)k)

j=1

Then, we consider the relation of duality gap of any two
subproblems:
(cid:113)

(cid:113)

GD(θ(cid:48)k+1) ≤

|GD(θ(cid:48)k+1) − GD(θ(cid:48)k)| + GD(θ(cid:48)k)

(cid:113)

|GD(θ(cid:48)k+1) − GD(θ(cid:48)k)| +

(cid:113)

GD(θ(cid:48)k)

≤

By utilizing the relation from one iteration to the next in a
CCCP algorithm, we can obtain the Property 3.

Property 3. Consider a CIL problem with µk and its fea-
sible solutions θ(cid:48)k allowing to screen samples according to
(20)-(21). Suppose that we have a new set of weight of µk+1
deﬁning a new CIL problem. Given a primal-dual feasible
solution θ(cid:48)k+1 for the latter problem, a safe sample screen-
ing rule for sample i reads

∇D(θ(cid:48)k)i −

∇D(θ(cid:48)k)i +

(cid:113)

KiiGD(θ(cid:48)k) + m(cid:107)Ii(cid:107) + n − q > 0 (26)

(cid:113)

KiiGD(θ(cid:48)k) + m(cid:107)Ii(cid:107) + n + q < 0 (27)

where m, n and q are constants such that (cid:107)αk+1
i (cid:107)2 <
m, |b(cid:48)k+1 − b(cid:48)k| < n and (cid:112)Kii|GD(θ(cid:48)k+1) − GD(θ(cid:48)k)| <
q, Ii denote vector [Hi1, Hi2, · · · , Hi(n)] for all 1 ≤ i ≤ n.

i − αk

Algorithm 2 Safe sample screening for successive CIL
problem

Input: The training set D.
Output: The optimal solution of RSVM.
1: Initialize the µ0.
2: Solve a CIL problem with µ0.
3: k ← 1 .
4: Compute the µk according to (5).
5: while µk are not convergence do
6:
7:
8:
9:
10: end while

Screening the samples that satisfy (26)-(27).
Solve a CIL problem with µk.
k ← k + 1.
Compute the µk according to (5).

We provide the detailed proof in the appendix. To make
this safe sample screening rule tractable, we ﬁrst need a fea-
sible solution θ(cid:48)k+1 of the dual CIL problem in iteration
k + 1, then we can obtain an upper bound on the norm of
(cid:107)α(cid:48)k+1 − α(cid:48)k(cid:107) and a bound on the difference of the dual-
ity gap |GD(θ(cid:48)k+1) − GD(θ(cid:48)k)| and threshold |b(cid:48)k+1 − b(cid:48)k|
respectively. Interestingly, when using the primal solution
αk = (βk − µk+1)y as our feasible solution in iteration
k + 1, the safe sample screening rule given in Property 3
does not involve any additional dot product and is cheaper
to compute. The propagation of screened samples provide
a warm-start process for the next iteration in a CCCP algo-
rithm. We summarized the safe sample screening rule for
successive CIL problems in Algorithm 2.

Experiments

In this section, we ﬁrst present the experimental setup, and
then provide the experimental results and discussions.

(a) CodRNA, κ = 0.05

(b) a9a, κ = 0.05

(c) ijcnn1, κ = 0.05

(d) letter, κ = 0.05

(e) CodRNA, κ = 0.5

(f) a9a, κ = 0.5

(g) ijcnn1, κ = 0.5

(h) letter, κ = 0.5

(i) CodRNA, κ = 5

(j) a9a, κ = 5

(k) ijcnn1, κ = 5

(l) letter, κ = 5

Figure 2: Average computational time of four contrast algorithms under different setting.

Experimental Setup
Design of Experiments: In the experiments, we compare
the computation time of different algorithms for comput-
ing the optimization problem (3) to verify the effective-
ness of our algorithm. The active set technique (also called
shrinking technique) is used by two of the most commonly
used state-of-the-art SVM solvers, LIBSVM (Chang and Lin
2011) and SVMLight (Joachims 1999b). Similarly, the ac-
tive set technique solves a smaller optimization problem (15)
by screening inactive samples to reduce the computational
time. However, these methods do not have theoretic guaran-
tee w.r.t. whether a training sample can be safely remove.
In the experiments, we combine our safe sample screening
rules with active set technology to reduce the computational
time. Speciﬁcally, we use our safe sample screening rules at
the beginning of the experiment during which the screening
operation is invoked every 10 iterations until the duality gap
is smaller than 10−4. Then, we use active set technique for
the rest of the training process.

In addition, we compared our safe sample screening algo-

rithm with traditional RSVM algorithm, which uses all the
samples to train the model during the whole training process.
The compared algorithms are summarized as follows.

1. Safe: Our proposed safe sample screening algorithm.

2. Shrink: The active set technique without safe screening

guarantee (Chang and Lin 2011; Joachims 1999b).

3. Shrink+Safe: our safe sample screening rules combined

with active set technique.

4. Non screening: The traditional RSVM algorithm with

CCCP (Collobert et al. 2006).

Implementation: We implement our algorithm in MAT-
the linear kernel and Gaussian ker-
LAB. For kernel,
nel K(x1, x2) = exp(−κ(cid:107)x1 − x2(cid:107)2) are used in all
experiments. The parameter C is selected from the set
{0.1, 1, 10, 100}. The Gaussian kernel parameter κ is se-
lected from the set {0.05, 0.5, 5}. The ramp loss function
parameter s is ﬁxed at 0. The optimization precision (cid:15) is set
to be 10−8. For each dataset, we randomly selected 20000

0.1110100C020004000600080001000012000Time(s)0.1110100C101102103104105Time(s)0.1110100C10-410-2100102104Time(s)0.1110100C0500100015002000Time(s)0.1110100C00.511.52Time(s)1040.1110100C102103104Time(s)0.1110100C10-2100102104106Time(s)0.1110100C0200400600800Time(s)0.1110100C01000200030004000500060007000Time(s)0.1110100C10-410-2100102104Time(s)0.1110100C100101102103104Time(s)0.1110100C0100200300400500Time(s)(a) CodRNA, C = 1, κ = 0.05

(b) CodRNA, C = 10, κ = 0.05

(c) CodRNA, C = 10, κ = 0.5

(d) CodRNA, C = 100, κ = 5

(e) a9a, C = 1, κ = 0.05

(f) a9a, C = 10, κ = 0.05

(g) a9a, C = 10, κ = 0.5

(h) a9a, C = 100, κ = 05

(i) letter, C = 1, κ = 0.05

(j) letter, C = 10, κ = 0.05

(k) letter, C = 10, κ = 0.5

(l) letter, C = 100, κ = 5

(m) ijcnn1, C = 1, κ = 0.05

(n) ijcnn1, C = 10, κ = 0.05

(o) ijcnn1, C = 10, κ = 0.5

(p) ijcnn1, C = 100, κ = 5

Figure 3: The screening rate of different datasets.

samples for training.
Datasets: Table 2 summarizes the four benchmark datasets
(CodRNA, ijcnn1, a9a, letter) used in the experiments. There
are from LIBSVM 1sources. Originally, the Letter is a 26-
class dataset (i.e., the alphabet “A”-“Z”). We created a bi-
nary version of Letter dataset by classifying alphabet A to
M versus N to Z.

Table 2: The benchmark datasets used in the experiments.

Dateset

Dimensionality

Samples

Source

CodRNA
a9a
letter
ijcnn1

8
123
16
22

59535
32561
20000
49990

LIBSVM
LIBSVM
LIBSVM
LIBSVM

Results and Discussions
Figure 2 presents the average computational time of four
competing algorithms under different setting. Compared
with Non screening, our safe sample screening rule can ef-
fectively reduce almost 50% computational time in most set-
tings. The result clearly demonstrate that our safe sample
screening rule combined with active set technology is the
most efﬁcient method for reducing computational time, sig-
niﬁcantly outperforming the standalone active set technique.
This is because the active set technique is not safe, meaning
that, when it erroneously screens useful samples, it needs
to repeat the training after correcting those mistakes. Our
safe sample screening rule can safely screen samples until
close to the optimal solution. Then, when using the activity

1https://www.csie.ntu.edu.tw/∼cjlin/libsvmtools/datasets/.

set technique, we can try to avoid screening active samples
by mistake as much as possible. Even if some samples are
wrongly screened, the retrained process only requires fewer
samples.

Figure 3 presents the screening rate of different datasets.
The results clearly demonstrate that when the Gaussiam ker-
nel parameter is small, our safe sample screening rule can ef-
fectively screen half of the inactive samples at the beginning
of training process. As the number of iterations increases,
our safe sample screening rule can screen almost all inac-
tive samples. In Figure 3 (d), (k), (I), we only screen a few
inactive samples because the SVM model contains a lot of
support vectors and is not sparse in samples at this setting.

Conclusion

In this paper, we propose two safe sample screening rules
for RSVM based on the CCCP algorithm. Speciﬁcally, we
ﬁrst provide a screening rule for the inner solver of CCCP.
Secondly, we provide a new rule for propagating screened
samples between two successive solvers of CCCP. We also
provide the security guarantee to our sample screening rules
to RSVM. To the best of our knowledge, this is the ﬁrst work
of safe sample screening to a non-convex optimization prob-
lem. Experimental results on a variety of benchmark datasets
verify that our safe sample screening rules can signiﬁcantly
reduce the computational time.

Acknowledgments

This work was supported by Six talent peaks project (No.
XYDXX-042) and the 333 Project (No. BRA2017455) in
Jiangsu Province and the National Natural Science Founda-
tion of China (No: 61573191).

00.511.522.5Iterations10600.51Screening rate00.511.522.5Iterations10600.51Screening rate00.511.522.5Iterations10600.20.40.6Screening rate02468101214Iterations10500.10.20.3Screening rate0100020003000400050006000Iterations00.51Screening rate012345Iterations10400.51Screening rate00.511.522.53Iterations10600.20.40.6Screening rate02468Iterations10400.51Screening rate01234567Iterations10400.51Screening rate051015Iterations10400.51Screening rate0246810Iterations10400.20.40.6Screening rate0246810Iterations10400.20.4Screening rate0100200300400500600700Iterations00.51Screening rate050001000015000Iterations00.51Screening rate02468Iterations10400.51Screening rate00.511.522.53Iterations10500.51Screening rate2004.

Convex optimization.

1997. Nonlinear pro-
Journal of the Operational Research Society

References
[Bazaraa and Shetty 2012] Bazaraa, M. S., and Shetty, C. M.
2012. Foundations of optimization, volume 122. Springer
Science & Business Media.
[Bertsekas 1997] Bertsekas, D. P.
gramming.
48(3):334–334.
[Bertsekas 2014] Bertsekas, D. P. 2014. Constrained opti-
mization and Lagrange multiplier methods. Academic press.
[Bonnefoy et al. 2014] Bonnefoy, A.; Emiya, V.; Ralaivola,
L.; and Gribonval, R. 2014. A dynamic screening principle
In 2014 22nd European Signal Processing
for the lasso.
Conference (EUSIPCO), 6–10. IEEE.
[Bonnefoy et al. 2015] Bonnefoy, A.; Emiya, V.; Ralaivola,
L.; and Gribonval, R. 2015. Dynamic screening: Accel-
erating ﬁrst-order algorithms for the lasso and group-lasso.
IEEE Transactions on Signal Processing 63(19):5121–5132.
and Vanden-
[Boyd and Vandenberghe 2004] Boyd, S.,
berghe, L.
Cambridge
university press.
[Chang and Lin 2011] Chang, C.-C., and Lin, C.-J. 2011.
ACM
Libsvm: A library for support vector machines.
transactions on intelligent systems and technology (TIST)
2(3):27.
[Collobert et al. 2006] Collobert, R.; Sinz, F.; Weston, J.; and
Bottou, L. 2006. Large scale transductive svms. Journal of
Machine Learning Research 7(Aug):1687–1712.
[El Ghaoui, Viallon, and Rabbani 2011] El Ghaoui, L.; Vial-
lon, V.; and Rabbani, T. 2011. Safe feature elimination for
the lasso. Submitted, April.
[Fan, Chen, and Lin 2005] Fan, R.-E.; Chen, P.-H.; and Lin,
C.-J. 2005. Working set selection using second order in-
formation for training support vector machines. Journal of
machine learning research 6(Dec):1889–1918.
[Fercoq, Gramfort, and Salmon 2015] Fercoq, O.; Gramfort,
A.; and Salmon, J. 2015. Mind the duality gap: safer rules
for the lasso. arXiv preprint arXiv:1505.03410.
[Ghaoui, Viallon, and Rabbani 2010] Ghaoui, L. E.; Viallon,
V.; and Rabbani, T. 2010. Safe feature elimination for
the lasso and sparse supervised learning problems. arXiv
preprint arXiv:1009.4219.
[Gu and Sheng 2016] Gu, B., and Sheng, V. S. 2016. A ro-
bust regularization path algorithm for ν-support vector clas-
siﬁcation. IEEE Transactions on neural networks and learn-
ing systems 28(5):1241–1248.
[Gu et al. 2015] Gu, B.; Sheng, V. S.; Wang, Z.; Ho, D.; Os-
man, S.; and Li, S. 2015. Incremental learning for ν-support
vector regression. Neural Networks 67:140–150.
[Gu et al. 2016] Gu, B.; Sheng, V. S.; Tay, K. Y.; Romano,
W.; and Li, S.
2016. Cross validation through two-
dimensional solution surface for cost-sensitive svm. IEEE
transactions on pattern analysis and machine intelligence
39(6):1103–1121.
[Gu et al. 2018a] Gu, B.; Quan, X.; Gu, Y.; Sheng, V. S.; and
Zheng, G. 2018a. Chunk incremental learning for cost-

1999b.

sensitive hinge loss support vector machine. Pattern Recog-
nition 83:196–208.
[Gu et al. 2018b] Gu, B.; Shan, Y.; Geng, X.; and Zheng, G.
2018b. Accelerated asynchronous greedy coordinate descent
algorithm for svms. In IJCAI, 2170–2176.
[Gu et al. 2018c] Gu, B.; Yuan, X.-T.; Chen, S.; and Huang,
H. 2018c. New incremental learning algorithm for semi-
In Proceedings of the
supervised support vector machine.
24th ACM SIGKDD International Conference on Knowl-
edge Discovery & Data Mining, 1475–1484. ACM.
[Gu, Sheng, and Li 2015] Gu, B.; Sheng, V. S.; and Li, S.
2015. Bi-parameter space partition for cost-sensitive svm.
In Twenty-Fourth International Joint Conference on Artiﬁ-
cial Intelligence.
[Gu 2018] Gu, B. 2018. A regularization path algorithm for
support vector ordinal regression. Neural Networks 98:114–
121.
[Joachims 1999a] Joachims, T. 1999a. Making large-scale
support vector machine learning practical, advances in ker-
nel methods. Support vector learning.
Svmlight: Sup-
[Joachims 1999b] Joachims, T.
SVM-Light Support Vector Ma-
port vector machine.
chine http://svmlight. joachims. org/, University of Dort-
mund 19(4).
[Li et al. 2015] Li, X.; Wang, H.; Gu, B.; and Ling, C. X.
2015. Data sparseness in linear svm. In Twenty-Fourth In-
ternational Joint Conference on Artiﬁcial Intelligence.
[Liu et al. 2013] Liu, J.; Zhao, Z.; Wang, J.; and Ye, J. 2013.
Safe screening with variational inequalities and its applica-
tion to lasso. arXiv preprint arXiv:1307.7577.
[Ogawa et al. 2014] Ogawa, K.; Suzuki, Y.; Suzumura, S.;
and Takeuchi, I. 2014. Safe sample screening for support
vector machines. arXiv preprint arXiv:1401.6740.
[Ogawa, Suzuki, and Takeuchi 2013] Ogawa, K.; Suzuki, Y.;
and Takeuchi, I. 2013. Safe screening of non-support vectors
in pathwise svm computation. In International conference
on machine learning, 1382–1390.
[Platt 1998] Platt, J. 1998. Sequential minimal optimization:
A fast algorithm for training support vector machines.
[Rakotomamonjy, Gasso, and Salmon 2019]
2019.
Rakotomamonjy, A.; Gasso, G.; and Salmon, J.
Screening rules for lasso with non-convex sparse regulariz-
ers. arXiv preprint arXiv:1902.06125.
[Shen et al. 2003] Shen, X.; Tseng, G. C.; Zhang, X.; and
Wong, W. H. 2003. On ψ-learning. Journal of the American
Statistical Association 98(463):724–734.
[Sriperumbudur and Lanckriet 2009] Sriperumbudur, B. K.,
and Lanckriet, G. R. 2009. On the convergence of the
concave-convex procedure. In Proceedings of the 22nd In-
ternational Conference on Neural Information Processing
Systems, 1759–1767. Curran Associates Inc.
[Vapnik 2013] Vapnik, V. 2013. The nature of statistical
learning theory. Springer science & business media.
[Wang et al. 2013] Wang, J.; Zhou, J.; Wonka, P.; and Ye, J.
2013. Lasso screening rules via dual polytope projection. In

Advances in Neural Information Processing Systems, 1070–
1078.
[Wang et al. 2014] Wang, J.; Zhou, J.; Liu, J.; Wonka, P.; and
Ye, J. 2014. A safe screening rule for sparse logistic regres-
sion. In Advances in neural information processing systems,
1053–1061.
[Wang, Wonka, and Ye 2014] Wang, J.; Wonka, P.; and Ye,
J. 2014. Scaling svm and least absolute deviations via ex-
act data reduction. In International conference on machine
learning, 523–531.
[Wu and Liu 2007] Wu, Y., and Liu, Y. 2007. Robust trun-
cated hinge loss support vector machines. Journal of the
American Statistical Association 102(479):974–983.
[Xiang, Wang, and Ramadge 2016] Xiang, Z. J.; Wang, Y.;
and Ramadge, P. J. 2016. Screening tests for lasso prob-
lems. IEEE transactions on pattern analysis and machine
intelligence 39(5):1008–1027.
[Xu and Ramadge 2013] Xu, P., and Ramadge, P. J. 2013.
Three structural results on the lasso problem. In 2013 IEEE
International Conference on Acoustics, Speech and Signal
Processing, 3392–3396. IEEE.
[Xu, Crammer, and Schuurmans 2006] Xu, L.; Crammer, K.;
and Schuurmans, D. 2006. Robust support vector machine
In AAAI, volume 6,
training via convex outlier ablation.
536–542.
[Yu et al. 2019] Yu, S.; Gu, B.; Ning, K.; Chen, H.; Pei, J.;
and Huang, H. 2019. Tackle balancing constraint for incre-
mental semi-supervised support vector learning. In Proceed-
ings of the 25th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining, 1587–1595. ACM.
[Yuille and Rangarajan 2003] Yuille, A. L., and Rangarajan,
A. 2003. The concave-convex procedure. Neural computa-
tion 15(4):915–936.
[Zimmert et al. 2015] Zimmert, J.; de Witt, C. S.; Kerg, G.;
and Kloft, M. 2015. Safe screening for support vector ma-
chines. In NIPS 2015 Workshop on Optimization in Machine
Learning (OPT).

