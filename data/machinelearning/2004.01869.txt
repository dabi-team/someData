Learning with Semi-Deﬁnite Programming: new statistical bounds based
on ﬁxed point analysis and excess risk curvature

St´ephane Chr´etien, Mihai Cucuringu, Guillaume Lecu´e, Lucie Neirac

email: stephane.chretien@npl.co.uk, mihai.cucuringu@stats.ox.ac.uk, guillaume.lecue@ensae.fr,
Lucie.neirac@ensae.fr,
Universit´e Lyon 2, Oxford University, The Alan Turing Institute, CREST, ENSAE, IPParis.

0
2
0
2

r
p
A
4

]
T
S
.
h
t
a
m

[

1
v
9
6
8
1
0
.
4
0
0
2
:
v
i
X
r
a

Abstract

Many statistical learning problems have recently been shown to be amenable to Semi-Deﬁnite Pro-
gramming (SDP), with community detection and clustering in Gaussian mixture models as the most
striking instances [60]. Given the growing range of applications of SDP-based techniques to machine
learning problems, and the rapid progress in the design of eﬃcient algorithms for solving SDPs, an
intriguing question is to understand how the recent advances from empirical process theory can be put
to work in order to provide a precise statistical analysis of SDP estimators.

In the present paper, we borrow cutting edge techniques and concepts from the learning theory liter-
ature, such as ﬁxed point equations and excess risk curvature arguments, which yield general estimation
and prediction results for a wide class of SDP estimators. From this perspective, we revisit some clas-
sical results in community detection from [54] and [23], and we obtain statistical guarantees for SDP
estimators used in signed clustering, group synchronization and MAXCUT.

Contents

1 Introduction

1.1 Historical background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.1.1 Early history . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.1.2 The Goemans-Williamson SDP relaxation of Max-Cut and its legacy . . . . . . . . .
1.1.3 Relaxation of machine learning and high dimensional statistical estimation problems
1.2 Mathematical formulation of the problem . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3 Goal of the paper . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 Main general results for the statistical analysis of SDP estimators

3 Revisiting two results from the community detection literature [41, 54]

4 Contributions of the paper

4.1 Application to signed clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.1.1 A Signed Stochastic Block Model (SSBM) . . . . . . . . . . . . . . . . . . . . . . . .
4.1.2 Main result for the estimation of the cluster matrix in signed clustering . . . . . . .
4.2 Application to angular group synchronization . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2.1 Main results for phase recovery in the synchronization problem . . . . . . . . . . . .
4.3 Application to the MAX-CUT problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.3.1 Main results for the MAX-CUT problem . . . . . . . . . . . . . . . . . . . . . . . . .

2
2
3
3
3
4
4

4

9

11
11
12
12
13
14
15
16

1

 
 
 
 
 
 
5 Proof of Theorem 3 (signed clustering)

5.1 Curvature equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.2 Computation of the complexity ﬁxed point r˚
Gp∆q . . . . . . . . . . . . . . . . . . . . . . . .

6 Proofs of Theorem 4 and Corollary 2 (angular synchronization)

6.1 Curvature of the objective function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.2 Computation of the complexity ﬁxed point r˚
Gp∆q . . . . . . . . . . . . . . . . . . . . . . . .
6.3 End of the proof of Theorem 4 and Corollary 2: application of Corollary 1 . . . . . . . . . .

7 Proofs of Theorem 5 and 6 (MAX-CUT)

7.1 Proof of Theorem 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.2 Proof of Theorem 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8 Annex 1: Signed clustering
8.1 Proof of Equation (4.1)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8.2 Proof of Proposition 2: control of S1pZq adapted from [41] . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
8.3 Proof of Proposition 3: control of the S2pZq term from [41]

9 Annex 2: Solving SDP’s in practice

9.1 Pierra’s method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9.1.1 Application to community detection . . . . . . . . . . . . . . . . . . . . . . . . . . .
9.1.2 Application to signed clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9.2 The Burer-Monteiro approach and the Manopt Solver . . . . . . . . . . . . . . . . . . . . .

10 Numerical experiments

10.1 Signed Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10.2 Max-Cut . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10.3 Angular Synchronization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

11 Conclusions and future work

1

Introduction

17
17
18

19
20
21
21

22
22
22

23
23
24
26

28
28
29
29
30

31
31
33
33

36

Many statistical learning problems have recently been shown to be amenable to Semi-Deﬁnite Programming
(SDP), with community detection and clustering in Gaussian mixture models as the most striking instances
where SDP performs signiﬁcantly better than other current approaches [60]. SDP is a class of convex
optimisation problems generalising linear programming to linear problems over semi-deﬁnite matrices [94],
[107], [21], and which proved an important tool in the computational approach to diﬃcult challenges in
automatic control, combinatorial optimisation, polynomial optimisation, data mining, high dimensional
statistics and the numerical solution to partial diﬀerential equations. The goal of the present paper is to
introduce a new ﬁxed point approach to the statistical analysis of SDP-based estimators and illustrate our
method on four current problems of interest, namely MAX-CUT, community detection, signed clustering,
angular group synchronization. The rest of this section gives some historical background and presents the
mathematical deﬁnition of SDP based estimators.

1.1 Historical background

SDP is a class of optimisation problems which includes linear programming as a particular case and can
be written as the set of problems over symmetric (resp. Hermitian) positive semi-deﬁnite matrix variables,
with linear cost function and aﬃne constraints, i.e. optimization problems of the form

`

max
Zľ0

(cid:10)A, Z(cid:11) : (cid:10)Bj, Z(cid:11) “ bj for j “ 1, . . . , m

2

˘

(1.1)

where A, B1, . . . , Bm are given matrices. SDPs are convex programming problems which can be solved in
polynomial time when the constraint set is compact and it plays a paramount role in a large number of
convex and nonconvex problems, for which it often appear as a convex relaxation [6].

We will sometimes use the notation Sn,` (resp. Sn,´) for the cone of positive semi-deﬁnite matrices

(resp. negative semi-deﬁnite matrices).

1.1.1 Early history

Early use of Semi-Deﬁnite programming to statistics can be traced back to [88] and [42]. In the same year,
Shapiro used SDP in factor analysis [89]. The study of the mathematical properties of SDP then gained
momentum with the introduction of Linear Matrix Inequalities (LMI) and their numerous applications in
control theory, system identiﬁcation and signal processing. The book [20] is the standard reference of these
type of results, mostly obtained in the 90’s.

1.1.2 The Goemans-Williamson SDP relaxation of Max-Cut and its legacy

A notable turning point is the publication of [49] where SDP was shown to provide a 0.87 approximation to
the NP-Hard problem known as Max-Cut. The Max-Cut problem is a clustering problem on graphs which
consists in ﬁnding two complementary subsets S and Sc of nodes such that the sum of the weights of the
edges between S and Sc is maximal. In [49], the authors approach this diﬃcult combinatorial problem by
using what is now known as the Goemans-Williamson SDP relaxation and use the Choleski factorization
of the optimal solution to this SDP in order to produce a randomized scheme achieving the .87 bound
in expectation. Moreover, this problem can be seen as a ﬁrst instance where the Laplacian of a graph is
employed in order to provide an optimal bi-clustering in a graph and certainly represents the ﬁrst chapter
of a long and fruitful relationship between clustering, embedding and Laplacians. Other SDP schemes for
approximating hard combinatorial problems are, to name a few, for the graph coloring problem [61], for
satisﬁability problem [49, 48]. These results were later surveyed in [70, 47] and [106]. The randomized
scheme introduced by Goemans and Williamson was then further improved in order to study more general
Quadratically Constrained Quadratic Programmes (QCQP) in various references, most notably [79, 110]
and further extended in [56]. Many applications to signal processing are discussed in [81], [74]; one speciﬁc
reduced complexity implementation in the form of an eigenvalue minimisation problem and its application
to binary least-squares recovery and denoising is presented in [27].

1.1.3 Relaxation of machine learning and high dimensional statistical estimation problems

Applications of SDP to problems related with machine learning is more recent and probably started with
the SDP relaxation of K-means in [83, 82] and later in [5]. This approach was then further improved
using a reﬁned statistical analysis by [87] and [46]. Similar methods have also been applied to community
detection [55, 3] and for the weak recovery viewpoint, [54]. This last approach was also re-used via the
kernel trick for the point cloud clustering [28]. Another incarnation of SDP in machine learning is the
extensive use of nuclear norm-penalized least-square costs as a surrogate for rank-penalization in low-rank
recovery problems such as matrix completion in recommender systems, matrix compressed sensing, natural
language processing and quantum state tomography; these topics are surveyed in [35].

The problem of manifold learning was also addressed using SDP and is often mentioned as one of the
most accurate approaches to the problem, let aside its computational complexity; see [103, 105, 104, 57].
Connections with the design of fast converging Markov-Chains were also exhibited in [93].

In a diﬀerent direction, A. Singer and collaborators have recently promoted the use of SDP relaxation
for estimation under group invariance, an active area with many applications [92, 8]. SDP-based relaxations
have also been considered in [30] in the context of synchronization over Z2 in signed multiplex networks
with constraints, and [31] in the setting of ranking from inconsistent and incomplete pairwise comparisons
where an SDP-based relaxation of angular synchronization over SO(2) outperformed a suite of state-of-
the-art algorithms from the literature. Phase recovery using SDP was studied in e.g. [102] and [40]. An

3

extension to multi-partite clustering based on SDP was then proposed in [61]. Other important applications
of SDP are, information theory [73], estimation in power networks [67], quantum tomography [77], [50]
and polynomial optimization via Sums-of-squares relaxations [66, 14]. Sums of squares relaxations were
recently applied to statistical problems in [38, 58, 39]. Extension to the ﬁeld of complex numbers, with
(cid:10)¨, ¨(cid:11) denoting the Hermitian inner product, has been less extensively studied but has many interesting
applications and comes with eﬃcient algorithms [49, 45].

1.2 Mathematical formulation of the problem

The general problem we want to study can be stated as follows. Let A be a random matrix in Rnˆn and
C Ă Rnˆn be a constraint. The object that we want to recover, for instance, the community membership
vector in community detection, is related to an oracle deﬁned as

Z˚ P argmax

(cid:10)EA, Z(cid:11)

ZPC

(1.2)

ř

where (cid:10)A, B(cid:11) “ TrpA ¯BJq “
Aij ¯Bij when A, B P Cnˆn where ¯z is the conjugate of z P C. We would like
to estimate Z˚, from which we can ultimately retrieve the object that really matters to us (for instance,
by considering a singular vector associated to the largest singular value of Z˚). To that end, we consider
the following estimator

ˆZ P argmax

(cid:10)A, Z(cid:11),

(1.3)

ZPC

which is simply obtained by replacing the unobserved quantity EA by the observation A.

As pointed out, in many situations, Z˚ is not the object we want to estimate, but there is a straight-
forward relation between Z˚ and this object. For instance, consider the community detection problem,
where the goal is to recover the class community vector x˚ P t´1, 1un of n nodes. Here, when C is well
chosen, there is a close relation between Z˚ and x˚, given by Z˚ “ x˚px˚qJ. We therefore need a ﬁnal
step to estimate x˚ using ˆZ, for instance, by letting ˆx denote a top eigenvector of ˆZ, and then using the
Davis-Kahan ”sin-theta” Theorem [36, 108] to control the estimation of x˚ by ˆx from the one of Z˚ by ˆZ.
(cid:11) “ bj, j “ 1, . . . , mu where
B1, . . . , Bm P Rnˆn and Z ľ 0 is notation for “ Z is positive semideﬁnite” then (1.3) is a semideﬁnite
programming (SDP) [21].

When the constraint C is of the form C “ tZ P Rnˆn : Z ľ 0, (cid:10)Z, Bj

1.3 Goal of the paper

The aim of the present work is to present a general approach to the study of the statistical properties of
SDP-based estimators deﬁned in (1.3). In particular, using our framework, one is able to obtain new (non-
asymptotic) rates of convergence or exact reconstruction properties for a wide class of estimators obtained
as a solution of a semideﬁnite program like (1.3). Speciﬁcally, our goal is to show that the solution to (1.3)
can be analyzed in a statistical way when EA is only partially and noisily observed in A. Even though the
constraint C may not necessarily be the intersection of the set of PSD (or Hermitian) matrices with linear
spaces – such as in the deﬁnition of SDP – in the following, a solution ˆZ of (1.3) will be called a SDP
estimator because in all our examples ˆZ will be solution of a SDP. But for the sake of generality we will only
assume only minimal requirement on the shape of C. We also illustrate our results on a number of speciﬁc
machine learning problems such as various forms of clustering problems and angular group synchronization.
Three out of the four examples worked out here are concerned with real-valued matrices. Only the angular
synchronization problem is approached using complex matrices.

2 Main general results for the statistical analysis of SDP estimators

From a statistical point of view, the task remains to estimate in the most eﬃcient way the oracle Z˚, and
to that end ˆZ is our candidate estimator. The point of view we will use to evaluate how far ˆZ is from

4

Z˚ is coming from the learning theory literature. We therefore see ˆZ as an empirical risk minimization
(ERM) procedure built on a single observation A, where the loss function is the linear one Z P C Ñ
(cid:96)ZpAq “ ´(cid:10)A, Z(cid:11), and the oracle Z˚ is indeed the one minimizing the risk function Z P C Ñ E(cid:96)ZpAq over
C. Having this setup in mind, we can use all the machinery developed in learning theory (see for instance
[99, 63, 76, 97]) to obtain rates of convergence for the ERM (here ˆZ) toward the oracle (here Z˚).

There is one key quantity driving the rate of convergence of the ERM: a ﬁxed point complexity param-
eter. This type of parameter carries all the statistical complexity of the problem, and even though it is
usually easy to set up, its computation can be tedious since it requires to control, with large probability,
the supremum of empirical processes indexed by “localized classes”. We now deﬁne this complexity ﬁxed
point related to the problem we are considering here.

Deﬁnition 1. Let 0 ă ∆ ă 1. The ﬁxed point complexity parameter at deviation 1 ´ ∆ is

¨

»

r˚p∆q “ inf

˝r ą 0 : P

–

sup
ZPC:(cid:10)EA,Z˚´Z(cid:11)

ďr

ﬁ

˛

(cid:10)A ´ EA, Z ´ Z˚(cid:11) ď p1{2qr

ﬂ ě 1 ´ ∆

‚.

(2.1)

Fixed point complexity parameters have been extensively used in learning theory since the introduction
of the localization argument [76, 64, 97, 13]. When they can be computed, they are preferred to the
(global) analysis developed by Chervonenkis and Vapnik [99] to study ERM, since the latter analysis
always yields slower rates given that the Vapnik-Chervonenkis bound is a high probability bound on
(cid:10)A ´ EA, Z ´ Z˚(cid:11), which is an upper bound for r˚p∆q since
the non-localized empirical process supZPC
tZ P C : (cid:10)EA, Z˚ ´ Z(cid:11) ď ru Ă C. The gap between the two global and local analysis can be important
since fast rates cannot be obtained using the VC approach, whereas the localization argument resulting
in ﬁxed points such as the one in Deﬁnition 1 may yield fast rates of convergence or even exact recovery
results.

An example of a Vapnik-Chervonenkis’s type of analysis of SDP estimators can be found in [54] for the
community detection problem. An improvement of the latter approach has been obtained in [41] thanks
to a localization argument – even though it is not stated in these words (we elaborate more on the two
approaches from [54, 41] in Section 3). Somehow, a ﬁxed point such as (2.1) is a sharp way to measure
the statistical performances of ERM estimators and in particular for the SDP estimators that we are
considering here. They can even be proved to be optimal (in a minimax sense) when the noise A ´ EA is
Gaussian [69] and under mild conditions on the structure of C.

Before stating our general result, we ﬁrst recall a deﬁnition of a minimal structural assumption on the

constraint C.

Deﬁnition 2. We say that the set C is star-shaped in Z˚ when for all Z P C, the segment rZ, Z˚s is in C.

This is a pretty mild assumption satisﬁed, for instance, when C is convex, which is the setup we
will always encounter in practical applications, given that SDP estimators are usually introduced after a
“convex relaxation” argument. Our main general statistical bound on SDP estimators is as follows.

Theorem 1. We assume that the constraint C is star-shaped in Z˚. Then, for all 0 ă ∆ ă 1, with
probability at least 1 ´ ∆, it holds true that (cid:10)EA, Z˚ ´ ˆZ(cid:11) ď r˚p∆q.

Theorem 1 applies to any type of setup where an oracle Z˚ is estimated by an estimator ˆZ such as in
(1.3). Its result shows that ˆZ is almost a maximizer of the true objective function Z Ñ (cid:10)EA, Z(cid:11) over C up
to r˚p∆q. In particular, when r˚p∆q “ 0, ˆZ is exactly a maximizer such as Z˚ and, in that case, we can
work with ˆZ as if we were working with Z˚ without any loss.

Theorem 1 may be applied in many diﬀerent settings; in the following, we study four such instances.
We will apply Theorem 1 (or one of its corollary stated below) to some popular problems in the networks
and graph signal processing literatures, namely, community detection [43] (we will mostly revisit the results
in [54] and [41] from our perspective), signed clustering [32], group synchronization [90] and MAX-CUT.

5

The proof of Theorem 1 is straightforward (mostly because the loss function is linear). Its importance
stems from the fact that it puts forward two important concepts originally introduced in Learning Theory,
namely that the complexity of the problem comes from the one of the local subset C X tZ : (cid:10)EA, Z˚ ´ Z(cid:11) ď
r˚p∆qu and that the “radius” r˚p∆q of the localization is solution of a ﬁxed point equation. For a setup
given by a random matrix A and a constraint C, we should try to understand how these two ideas apply
to obtain estimation properties of SDP estimators such as ˆZ. That is to understand the shape of the
local subsets C X tZ : (cid:10)EA, Z˚ ´ Z(cid:11) ď ru, r ą 0 and the maximal oscillations of the empirical process
Z Ñ (cid:10)A ´ EA, Z ´ Z˚(cid:11) indexed by these local subsets. We will consider this task in three distinct problem
instances. We now provide a proof for Theorem 1.

Proof of Theorem 1. Denote r˚ “ r˚p∆q. Assume ﬁrst that r˚ ą 0 (the case r˚ “ 0 is analyzed
later). Let Ω˚ be the event onto which for all Z P C if (cid:10)EA, Z˚ ´ Z(cid:11) ď r˚ then (cid:10)A ´ EA, Z ´ Z˚(cid:11) ď p1{2qr˚.
By Deﬁnition of r˚, we have PrΩ˚s ě 1 ´ ∆.

Let Z P C be such that (cid:10)EA, Z˚´Z(cid:11) ą r˚ and deﬁne Z1 such that Z1´Z˚ “

pZ´Z˚q.
We have (cid:10)EA, Z˚ ´ Z1(cid:11) “ r˚ and Z1 P C because C is star-shaped in Z˚. Therefore, on the event Ω˚,
(cid:10)A ´ EA, Z1 ´ Z˚(cid:11) ď p1{2qr˚ and so (cid:10)A ´ EA, Z ´ Z˚(cid:11) ď p1{2q(cid:10)EA, Z˚ ´ Z(cid:11). It therefore follows that on
the event Ω˚, if Z P C is such that (cid:10)EA, Z˚ ´ Z(cid:11) ą r˚ then

`
r˚{(cid:10)EA, Z˚ ´ Z(cid:11)

˘

(cid:10)A, Z ´ Z˚(cid:11) ď p´1{2q(cid:10)EA, Z˚ ´ Z(cid:11) ă ´r˚{2

which implies that (cid:10)A, Z ´ Z˚(cid:11) ă 0 and therefore Z does not maximize Z Ñ (cid:10)A, Z(cid:11) over C. As a
consequence, we necessarily have (cid:10)EA, Z˚ ´ ˆZ(cid:11) ď r˚ on the event Ω˚ (which holds with probability at least
1 ´ ∆).

Let us now assume that r˚ “ 0. There exists a decreasing sequence prnqn of positive real numbers
tending to r˚ “ 0 such that for all n ě 0, PrΩns ě 1 ´ ∆ where Ωn is the event Ωn “ tψprnq ď θ{2u where
for all r ą 0,

ψprq “

1
r

sup
ZPC:(cid:10)EA,Z˚´Z(cid:11)

ďr

(cid:10)A ´ EA, Z ´ Z˚(cid:11).

Since C is star-shapped in Z˚, ψ is a non-increasing function and so pΩnqn is a decreasing sequence (i.e.
Ωn`1 Ă Ωn for all n ě 0). It follows that PrXnΩns “ limn PrΩns ě 1 ´ ∆. Let us now place ourselves on
the event XnΩn. For all n, since Ωn holds and rn ą 0, we can use the same argument as in ﬁrst case to
conclude that (cid:10)EA, Z˚ ´ ˆZ(cid:11) ď rn. Since the latter inequality is true for all n (on the event XnΩn) and
prnqn tends to zero, we conclude that (cid:10)EA, Z˚ ´ ˆZ(cid:11) ď 0 “ r˚.

The main conclusion of Theorem 1 is that all the information for the problem of estimating Z˚ via ˆZ
is contained in the ﬁxed point r˚p∆q. We therefore have to compute or upper bound such a ﬁxed point.
This might be diﬃcult in great generality but there are some tools that can help to ﬁnd upper bounds on
r˚p∆q.

A ﬁrst approach is to understand the shape of the local sets C X tZ : (cid:10)EA, Z˚ ´ Z(cid:11) ď ru, r ą 0 and
to that end it is helpful to characterize the curvature of the excess risk Z Ñ (cid:10)EA, Z˚ ´ Z(cid:11) around its
maximizer Z˚. This type of local characterization of the excess risk is also a tool used in Learning theory
that goes back to classical conditions such as the Margin assumption [96, 75] or the Bernstein condition [11].
The latter condition was initially introduced as an upper bound of the variance term by its expectation:
for all Z P C, Ep(cid:96)ApZq ´ (cid:96)ApZ˚qq2 ď c0Ep(cid:96)ApZq ´ (cid:96)ApZ˚qq for some absolute constant c0, but it has now
been better understood as a way to discriminate the oracle from the other points in the model C. These
assumptions were global assumption in the sense that they concern all Z in C. It has been recently shown
[26] that only the local curvature of the excess risk needs to be understood. We now introduce this tool in
our setup.

We characterize the local curvature of the excess risk by some function G : Rnˆn Ñ R. Most of the
time the G function is a norm like the (cid:96)1-norm or a power of a norm such as the (cid:96)2 norm to the square.
The radius deﬁning the local subset onto which we need to understand the curvature of the excess risk is

6

also solution of a ﬁxed point equation:
«

˜

ﬀ

¸

r˚
Gp∆q “ inf

r ą 0 : P

sup
ZPC:GpZ˚´Zqďr

(cid:10)A ´ EA, Z ´ Z˚(cid:11) ď p1{2qr

ě 1 ´ ∆

.

(2.2)

The diﬀerence between the two ﬁxed points r˚p∆q and r˚
Gp∆q is that the local subsets are not deﬁned using
the same proximity function to the oracle Z˚; the ﬁrst one uses the excess risk as a proximity function
while the second one uses the G function as a proximity function. The G function should play the role of
a simple description of the curvature of the excess risk function locally around Z˚; that is formalized in
the next assumption.
Assumption 1. For all Z P C, if (cid:10)EA, Z˚ ´ Z(cid:11) ď r˚

Gp∆q then (cid:10)EA, Z˚ ´ Z(cid:11) ě GpZ˚ ´ Zq.

Typical examples of curvature function G will have the form GpZ˚ ´ Zq “ θ }Z˚ ´ Z}κ for some
κ ě 1, θ ą 0 and some norm }¨}. In that case, the parameter κ was initially called the margin parameter
[95, 75]. Even though the relation given in Assumption 1 has been typically referred to as a margin
condition or Bernstein condition in the learning theory literature, we will rather call it a local curvature
assumption, following [54] and [26], since this type of relation describes the behavior of the risk function
locally around its oracle. The main advantage for ﬁnding a local curvature function G is that r˚
Gp∆q
should be easier to compute than r˚p∆q and r˚p∆q ď r˚
Gp∆q and
tZ P C : (cid:10)EA, Z˚ ´ Z(cid:11) ď r˚
Gp∆qu (thanks to Assumption 1). We can
therefore state the following corollary.

Gp∆q because of the deﬁnition of r˚

Gp∆qu Ă tZ P C : GpZ˚ ´ Zq ď r˚

Corollary 1. We assume that the constraint C is star-shaped in Z˚ and that the “local curvature” As-
sumption 1 holds for some 0 ă ∆ ă 1. With probability at least 1 ´ ∆, it holds true that

Gp∆q ě (cid:10)EA, Z˚ ´ ˆZ(cid:11) ě GpZ˚ ´ ˆZq.
r˚
When it is possible to describe the local curvature of the excess risk around its oracle by some G
function and when some estimate of r˚
Gp∆q can be obtained, Corollary 1 applies and estimation results of
Z˚ by ˆZ (w.r.t. to both the ”excess risk” metric (cid:10)EA, Z˚ ´ ˆZ(cid:11) and the G metric) follow. If not, either
because understanding the local curvature of the excess risk or the computation of r˚
Gp∆q is diﬃcult, it is
still possible to apply Theorem 1 with the global VC approach which boils down to simply upper bound
the ﬁxed point r˚p∆q used in Theorem 1 by a global parameter that is a complexity measure of the entire
set C:

ˆ

˙



„

r˚p∆q ď inf

r ą 0 : P

(cid:10)A ´ EA, Z ´ Z˚(cid:11) ď p1{2qr

sup
ZPC

ě 1 ´ ∆

.

(2.3)

Interestingly, if the latter last resort approach is used then, following the approach form [54], Grothendieck’s
inequality [51, 85] appears to be a powerful tool to upper bound the right-hand side of (2.3) in the case of
the community detection problem such as in [53] as well as in the MAX-CUT problem. Of course, when it
is possible to avoid this ultimate global approach one has to do it because the local approach will always
provide better results.

Finally, proving a “local curvature” property such as in Assumption 1 may be diﬃcult because it
requires to understand the shape of the local subsets C X tZ : (cid:10)EA, Z˚ ´ Z(cid:11) ď ru, r ą 0. It is however
possible to simplify this assumption if getting estimation results of Z˚ only w.r.t. the G function (and not
necessarily an upper bound on the excess risk (cid:10)EA, Z˚ ´ ˆZ(cid:11)) is enough. In that case, Assumption 1 may
be replaced by the following one.

Assumption 2. For all Z P C, if GpZ˚ ´ Zq ď r˚

Gp∆q then (cid:10)EA, Z˚ ´ Z(cid:11) ě GpZ˚ ´ Zq.

Assumption 2 assumes a curvature of the excess risk function in a G neighborhood of Z˚ unlike As-
sumption 1 which grants this curvature in an “ excess risk neighborhood”. The shape of a neighborhood
deﬁned by the G function may be easier to understand (for instance when G is a norm, a neighborhood
deﬁned by G is the ball of a norm centered at Z˚ with radius r˚
Gp∆q). In general, the latter assumption
and Assumption 1 do not compare. If Assumption 2 holds then ˆZ can estimate Z˚ w.r.t. the G function.

7

Theorem 2. We assume that the constraint C is star-shaped in Z˚ and that the “local curvature” As-
sumption 2 holds for some 0 ă ∆ ă 1. We assume that the G function is continuous, Gp0q “ 0 and
GpλpZ˚ ´ Zqq ď λGpZ˚ ´ Zq for all λ P r0, 1s, Z P Z˚ ´ C. With probability at least 1 ´ ∆, it holds true
that GpZ˚ ´ ˆZq ď r˚

Gp∆q.

Proof of Theorem 2. Let r˚ “ r˚

Gp∆q. First assume that r˚ ą 0. Let Z P C be such that
GpZ˚ ´ Zq ą r˚. Let f : λ P r0, 1s Ñ GpλpZ˚ ´ Zqq. We have f p0q “ Gp0q “ 0, f p1q “ GpZ˚ ´ Zq ą r˚
and f is continuous. Therefore, there exists λ0 P p0, 1q such that f pλ0q “ r˚. We let Z1 be such that
Z1 ´ Z˚ “ λ0pZ ´ Z˚q. Since C is star-shapped in Z˚ and λ0 P r0, 1s we have Z1 P C. Moreover,
GpZ˚ ´ Z1q “ r˚. As a consequence, on the event Ω˚ such that for all Z P C if GpZ˚ ´ Zq ď r˚ then
(cid:10)A ´ EA, Z ´ Z˚(cid:11) ď p1{2qr˚, we have (cid:10)A ´ EA, Z1 ´ Z˚(cid:11) ď p1{2qr˚. The latter and Assumption 2 imply
that, on Ω˚,

p1{2qr˚ ě (cid:10)A, Z1 ´ Z˚(cid:11) ` (cid:10)EA, Z˚ ´ Z1(cid:11) ě (cid:10)A, Z1 ´ Z˚(cid:11) ` GpZ˚ ´ Z1q ě (cid:10)A, Z1 ´ Z˚(cid:11) ` r˚

and so (cid:10)A, Z1 ´ Z˚(cid:11) ď ´r˚{2. Finally, using the deﬁnition of Z1, we obtain

(cid:10)A, Z ´ Z˚(cid:11) “ p1{λ0q(cid:10)A, Z1 ´ Z˚(cid:11) ď ´r˚{p2λ0q ă 0.
In particular, Z cannot be a maximizer of Z Ñ (cid:10)A, Z(cid:11) over C and so necessarily, on the event Ω˚,
GpZ˚ ´ ˆZq ď r˚.

Let us now consider the case where r˚ “ 0. Using the same approach as in the proof of Theorem 1, we

only have to check that the function

ψ : r ą 0 Ñ

1
r

sup
ZPC:GpZ˚´Zqďr

(cid:10)A ´ EA, Z ´ Z˚(cid:11)

is non-increasing. Let 0 ă r1 ă r2. W.l.o.g. we may assume that there exists some Z2 P C such that
GpZ˚ ´Z2q ď r2 and ψpr2q “ (cid:10)A´EA, Z2 ´Z˚(cid:11){r2. If GpZ˚ ´Z2q ď r1 then ψpr2q ď pr1{r2qψpr1q ď ψpr1q.
If GpZ˚´Z2q ą r1 then there exists λ0 P p0, 1q such that for Z1 “ Z˚`λ0pZ2´Z˚q we have GpZ˚´Z1q “ r1
and Z1 P C. Moreover, r1 “ Gpλ0pZ˚ ´ Z2qq ď λ0GpZ˚ ´ Z2q ď λ0r2 and so λ0 ě r1{r2. It follows that

ψpr2q “

1
r2

(cid:10)A ´ EA, Z2 ´ Z˚(cid:11) “

1
λ0r2

(cid:10)A ´ EA, Z1 ´ Z˚(cid:11) ď

r1
λ0r2

ψpr1q ď ψpr1q

where we used that ψprq ą 0 for all r ą 0 because Z˚ P tZ P C : GpZ˚ ´ Zq ď ru for all r ą 0.

As a consequence, Theorem 1, Corollary 1 and Theorem 2 are the three tools at our disposal to study
the performance of SDP estimators depending on the deepness of understanding we have on the problem.
The best approach is given by Theorem 1 when it is possible to compute eﬃciently the complexity ﬁxed
point r˚p∆q. If the latter approach is too complicated (likely because understanding the geometry of the
local subset C X tZ : (cid:10)EA, Z˚ ´ Z(cid:11) ď ru, r ą 0 may be diﬃcult) then one may resort to ﬁnd a curvature
function G of the excess risk locally around Z˚. In that case, both Corollary 1 and Theorem 2 may apply
depending on the hardness to ﬁnd a local curvature function G on an “excess risk neighborhood” (see
Assumption 1) or a “G-neighborhood” (see Assumption 2). Finally, if no local approach can be handled
(likely because describing the curvature of the excess risk in any neighborhood of Z˚ or controlling the
maximal oscillations of the empirical process Z Ñ (cid:10)EA ´ A, Z˚ ´ Z(cid:11) locally are too diﬃcult) then one
may resort ultimately to a global approach which follows from Theorem 1 as explained in (2.3). In the
following, we will use these tools for various problems.

Results like Theorem 1, Corollary 1 and Theorem 2 appeared in many papers on ERM in learning
theory such as in [64, 11, 76, 69]. In all these results, typical loss functions such as the quadratic or logistic
loss functions were not linear one such as the one we are using here. From that point of view our problem
is easier and this can be seen by the simplicity to prove our three general results from this section. What is
much more complicated here than in other more classical problems in Learning Theory is the computation

8

of the ﬁxed point because 1) the stochastic processes Z Ñ (cid:10)A ´ EA, Z ´ Z˚(cid:11) may be far from being a
Gaussian process if the noise matrix A´EA is complicated and 2) the local sets tZ P C : (cid:10)EA, Z˚ ´Z(cid:11) ď ru
or tZ P C : GpZ˚ ´ Zq ď ru for r ą 0 maybe very hard to describe in a simple way. Fortunately, we will
rely on several previous papers such as [41] to solve such problems.

3 Revisiting two results from the community detection literature [41,

54]

The rapid growth of social networks on the Internet has lead many statisticians and computer scientists
to focus their research on data coming from graphs. One important topic that has attracted particular
interest during the last decades is that of community detection [43, 86], where the goal is to recover
mesoscopic structures in a network, the so-called called communities. A community consists of group of
nodes that are relatively densely connected to each other, but sparsely connected to other dense groups
present within the network. The motivation for this line of work stems not only from the fact that ﬁnding
communities in a network is an interesting and challenging problem of its own as it leads to understanding
structural properties of networks, but community detection is also used as a data pre-processing step for
other statistical inference tasks on large graphs, as it facilitates parallelization and allows one to distribute
time consuming processes on several smaller subgraphs (i.e., the extracted communities).

One challenging aspect of the community detection problem arises in the setting of sparse graphs.
Many of the existing algorithms, which enjoy theoretical guarantees, do so in the relatively dense regime
for the edge sampling probability, where the expected average degree is of the order Θplog nq. The prob-
lem becomes challenging in very sparse graphs with bounded average degree. To this end, Gu´edon and
Vershynin proposed a semideﬁnite relaxation for a discrete optimization problem [54], an instance of which
encompasses the community detection problem, and showed that it can recover a solution with any given
relative accuracy even in the setting of very sparse graphs with average degree of order Op1q.

A subset of the existing literature for community detection and clustering relies on spectral methods,
which consider the adjacency matrix associated to a graph, and employ its eigenvalues, and especially
eigenvectors, in the analysis process or to propose eﬃcient algorithms to solve the task at hand. Along
these lines, Can et al.
[68] proposed a general framework for optimizing a general function of the graph
adjacency matrix over discrete label assignments by projecting onto a low-dimensional subspace spanned
by vectors that approximate the top eigenvectors of the expected adjacency matrix. The authors consider
the problem of community detection with k “ 2 communities, which they frame as an instance of their
proposed framework, combined with a regularization step that shifts each entry in the adjacency matrix
by a small constant τ , which renders their methodology applicable in the sparse regime as well.

In the remainder of this section, we focus on the community detection problem on random graphs under
the general stochastic block model. We will mostly revisit the work in [54] and [41] from the perspective
given by Theorem 1, which simpliﬁes the proof in [41] since the peeling argument is no longer required, and
neither is the upper bound from [54] unlike [41], thanks to the homogeneity argument hidden in Theorem 1
(which underlies the localization argument).

We ﬁrst recall the deﬁnition of the generalized stochastic block model (SBM). We consider a set of
vertices V “ t1, ¨ ¨ ¨ , nu, and assume it is partitioned into K communities C1, ¨ ¨ ¨ , CK of arbitrary sizes
|C1| “ l1, ¨ ¨ ¨ , |CK| “ lK.

Deﬁnition 3. For any pair of nodes i, j P V , we denote by i „ j when i and j belong to the same
community (i.e., there exists k P t1, . . . , Ku) such that i, j P Ck), and we denote by i  j if i and j do not
belong to the same community.

For each pair pi, jq of nodes from V , we draw an edge between i and j with a ﬁxed probability pij
independently from the other edges. We assume that there exist numbers p and q satisfying 0 ă q ă p ă 1,

9

such that

$
&

%

pij ą p, if i „ j and i ‰ j,
pij “ 1, if i “ j,
pij ă q, otherwise.

(3.1)

We denote by A “ pAi,jq1ďi,j,ďn the observed symmetric adjacency matrix, such that, for all 1 ď i ď j ď n,
Aij is distributed according to a Bernoulli of parameter pij. The community structure of such a graph is
captured by the membership matrix ¯Z P Rnˆn, deﬁned by ¯Zij “ 1 if i „ j, and ¯Zij “ 0 otherwise. The
main goal in community detection is to reconstruct ¯Z from the observation A.

Spectral methods for community detection are very popular in the literature [54, 41, 100, 15, 29]. There
are many ways to introduce such methods, one of which being via convex relaxations of certain graph cut
problems aiming to minimize a modularity function such as the RatioCut [80]. Such relaxations often lead
to SDP estimators, such as those introduced in Section 1.

Considering a random graph distributed according to the generalized stochastic block model, and its
associated adjacency matrix A (i.e. A “ AJ and Aij „ Bernppijq for 1 ď i ď j ď n and pij as deﬁned in
(3.1)), we will estimate its membership matrix ¯Z via the following SDP estimator

ˆZ P argmax

(cid:10)A, Z(cid:11),

ZPC

K
where C “ tZ P Rnˆn, Z ľ 0, Z ě 0, diagpZq ĺ In,
k“1 |Ck|2
denotes the number of nonzero elements in the membership matrix ¯Z. The motivation for this approach
stems from the fact that the membership matrix ¯Z is actually the oracle, i.e., Z˚ “ ¯Z (see Lemma 7.1 in
[54] or Lemma 1 below), where

n
i,j“1 Zij ď λu and λ “

¯Zij “

n
i,j“1

ř

ř

ř

Z˚ P argmax

(cid:10)EA, Z(cid:11).

Following the strategy from Theorem 1 and from our point of view, the upper bound on r˚p∆q from
[54] is the one that is based on the global approach – that is, without localization. Indeed, [54] uses the
observation that, for all r ą 0, it holds true that

ZPC

sup
ZPC:(cid:10)EA,Z˚´Z(cid:11)

ďr

(cid:10)A ´ EA, Z ´ Z˚(cid:11) paq

(cid:10)A ´ EA, Z ´ Z˚(cid:11) pbq

ď 2KG }A ´ EA}cut ,

(3.2)

ď sup
ZPC

where }¨}cut is the cut-norm1 and KG is the Grothendieck constant (Grothendieck’s inequality is used in
(b), see [85, 100]). Therefore, the localization around the oracle Z˚ by the excess risk “band” B˚
r :“
tZ : (cid:10)EA, Z˚ ´ Z(cid:11) ď ru is simply removed in inequality (a). As a consequence, the resulting statis-
tical bound is based on the complexity of the entire class C whereas, in a localized approach, only the
complexity of C X B˚
r matters. Next step in the proof of [54] is a high probability upper bound on
}A ´ EA}cut which follows from Bernstein’s inequality and a union bound since one has }A ´ EA}cut “
(cid:10)A ´ EA, xyJ(cid:11), then for all t ą 0, }A ´ EA}cut ď tnpn ´ 1q{2 with probability at least
maxx,yPt´1,1un
1 ´ exp
iăj pijp1 ´ pijq. The resulting
upper bound on the ﬁxed point obtained in [54] is,

2n log 2 ´ pnpn ´ 1qt2q{p16¯p ` 8t{3q

def
“ 2{rnpn ´ 1qs

where ¯p

ř

˘

`

r˚p∆q ď p8{3qKGp2n logp2q ` logp1{∆qq.

(3.3)

Finally, under the assumption of Theorem 1 in [54] (i.e., for some some (cid:15) P p0, 1q, n ě 5.104{(cid:15)2, maxppp1 ´
pq, qp1 ´ qqq ě 20{n, p “ a{n ą b{n “ q and pa ´ bq2 ě 2.104(cid:15)´2pa ` bq), for ∆ “ e35´n we obtain (using
the general result in Theorem 1) with probability at least 1 ´ ∆, the bound (cid:10)EA, Z˚ ´ ˆZ(cid:11) ď r˚p∆q ď (cid:15)n2 “
(cid:15) }Z˚}2
2, which is the result from Theorem 1 in [54]. Finally, [54] uses a (global) curvature property of the
excess risk in its Lemma 7.2:

1The cut-norm }¨}cut of a real matrix A “ paijqiPR,jPC with a set of rows indexed by R and a set of columns indexed by
ř
iPI,jPJ aij|. It is also the operator norm of A from (cid:96)8 to (cid:96)1

C, is the maximum, over all I Ă R and J Ă C, of the quantity |
and the “injective norm” in the orginal Grothendieck “r´esum´e” [52, 85]

10

Lemma 1 (Lemma 7.2 in [54]). For all Z P C, (cid:10)EA, Z˚ ´ Z(cid:11) ě rpp ´ qq{2s }Z˚ ´ Z}1.

Therefore, a (global– that is for all Z P C) curvature assumption holds for a G function which is here the
(cid:96)nˆn
norm, a margin parameter κ “ 1 and θ “ pp ´ qq{2 for the community detection problem. However
1
this curvature property is not use to compute a “better” ﬁxed point parameter but only to have a (cid:96)nˆn
estimation bound since

1

ˆ

˙

›
›
› ˆZ ´ Z˚

›
›
›
1

ď

2
p ´ q

(cid:10)EA, Z˚ ´ ˆZ(cid:11) ď

16KGp2n logp2q ` logp1{∆qq
3pp ´ qq

.

The latter bound together with the sin-Theta theorem allow the authors from [54] to obtain estimation
bound for the community membership vector x˚.

The approach from [41] improves upon the one in [54] because it uses a localization argument: the
curvature property of the excess risk function from Lemma 1 is used to improve the upper bound in (3.3)
obtained following a global approach. Indeed, the authors from [41] obtain high probability upper bound
on the quantity

sup
ZPC:}Z˚´Z}1ďr

(cid:10)A ´ EA, Z ´ Z˚(cid:11)

depending on r. This yields to exact reconstruction result in the “dense” case and exponentially decaying
rates of convergence in the “sparse” case. This is a typical example where the localization argument shows
its advantage upon the global approach. The price to pay is usually a more technical proof for the local
approach compare with the global one. However, the argument from [41] also uses an unnecessary peeling
(which is actually the one from
argument together with an unnecessary a priori upper bound on

›
›
› ˆZ ´ Z˚

›
›
›

can be avoided
[54]). It appears that this peeling argument and this a priori upper bound on
thanks to our approach from Theorem 1. This improves the probability estimate and simpliﬁes the proofs
(since the result from [54] is not required anymore neither is the peeling argument). For the sign clustering
problem we consider below as an application of our main results, we will mostly adapt the probabilistic
tools from [41] (in the “dense” case) to the methodology associated with Theorem 1 (without this two
unnecessary arguments).

1

1

›
›
› ˆZ ´ Z˚

›
›
›

4 Contributions of the paper

4.1 Application to signed clustering

Much of the clustering literature, including both spectral and non-spectral methods, has focused on un-
signed graphs, where each edge carries a non-negative scalar weight that encodes a measure of aﬃnity
(similarity, trust) between pairs of nodes. However, in numerous instances, the above-mentioned aﬃnity
takes negative values, and encodes a measure of dissimilarity or distrust. Such applications arise in social
networks where users relationships denote trust-distrust or friendship-enmity, shopping bipartite networks
which capture like-dislike relationships between users and products [10], online news and review websites,
such as Epinions [1] and Slashdot [2], that allow users to approve or denounce others [71], and clustering
ﬁnancial or economic time series data [4]. Such applications have spurred interest in the analysis of signed
networks, which has recently become an increasingly important research topic [72], with relevant lines of
work in the context of clustering signed networks including, in chronological order, [65, 24, 32].

The second application of our proposed methodology is an extension of the community detection and
clustering problems to the setting of signed graphs, where, for simplicity, we assume that an edge connecting
two nodes can take either ´1 or `1 values.

11

4.1.1 A Signed Stochastic Block Model (SSBM)

We focus on the problem of clustering a K-weakly balanced graphs2. We consider a signed stochastic block
model (SSBM) similar to the one introduced in [32], where we are given a graph G with n nodes t1, . . . , nu
which are divided into K communities, tC1, ¨ ¨ ¨ , CKu, such that, in the noiseless setting, edges within each
community are positive and edges between communities are negative.

The only information available to the user is given by a n ˆ n sparse adjacency matrix A constructed
as follows: A is symmetric, with Aii “ 1 for all i “ 1, . . . , n, and for all 1 ď i ă j ď n, Aij “ sijp2Bij ´ 1q
where

"

Bij „

Bernppq if i „ j
Bernpqq if i  j

and sij „ Bernpδq,

for some 0 ď q ă 1{2 ă p ď 1 and δ P p0, 1q. Moreover, all the variables Bij, sij for 1 ď i ă j ď n are
independent.

We remark that this SSBM model is similar to the one considered in [32], which was governed by
two parameters, the sampling probability δ as above, and the noise level η, which may ﬂip entries of the
adjacency matrix.

Our aim is to recover the community membership matrix or cluster matrix ¯Z “ p ¯Zijqnˆnwhere ¯Zij “ 1

when i „ j and ¯Zij “ 0 when i  j using only the observed censored adjacency matrix A.

Our approach is similar in nature to the one used by spectral methods in community detection. We

ﬁrst observe that for α :“ δpp ` q ´ 1q and J “ p1qnˆn we have ¯Z “ Z˚ where

Z˚ P argmax

(cid:10)EA ´ αJ, Z(cid:11)

ZPC

(4.1)

and C “ tZ P Rnˆn : Z ľ 0, Zij P r0, 1s, Zii “ 1, i “ 1, . . . , nu. The proof of (4.1) is recalled in Section 8.

Since we do not know EA and α, we should estimate both of them. We will estimate EA with A but,

for simplicity, we will assume that α is known. The resulting estimator of the cluster matrix ¯Z is

ˆZ P argmax

(cid:10)A ´ αJ, Z(cid:11)

ZPC

(4.2)

which is indeed a SDP estimator and therefore Theorem 1 (or Corollary 1 and Theorem 2) may be used
to obtain statistical bounds for the estimation of Z˚ from (4.1) by ˆZ.

We will use the following notations: s :“ δpp´qq2, θ :“ δpp´qq, ρ :“ δ maxt1´δp2p´1q2, 1´δp2q´1q2u,
K
k“1 l2
k,
pCk ˆ Ck1q. We also use the notation c0, c1, . . . , to denote absolute

ν :“ maxt2p ´ 1, 1 ´ 2qu, rms :“ t1, ¨ ¨ ¨ , mu for all m P N, lk :“ |Ck| for all k P rKs, λ2 :“
C` :“

ř

K
Y
k“1

pCk ˆ Ckq and C´ :“ Y
k‰k1

constants whose values may change from one line to another.

4.1.2 Main result for the estimation of the cluster matrix in signed clustering

Our main result concerns the reconstitution of the K communities from the observation of the matrix A.
In order to avoid solutions with some communities of degenerated size (too small or too large) we consider
the following assumption.

Assumption 3. Up to constant, the elements of the partition C1 \ ¨ ¨ ¨ \ CK of t1, . . . , nu are of same size
(up to constants): there are absolute constant c0, c1 ą 0 such that for any k P rKs, n{pc1Kq ď |Ck| “ lk ď
c0n{K.

We are now ready to state the main result on the estimation of the cluster matrix Z˚ from (4.1) by the
SDP estimator ˆZ from (4.2).

2A signed graph is K-weakly balanced if and only if all the edges are positive, or the nodes can be partitioned into K P N

disjoint sets such that positive edges exist only within clusters, and negative edges are only present across clusters [37].

12

Theorem 3. There is an absolute positive constant c0 such that the following holds. Grant Assumption
3. Assume that

nνδ ě log n,
pp ´ qq2nδ ě c0K2ν
θ2
ρ

ď max

ˆ

,

K logp2eKnq
n

˙

.

9ρ
32

(4.3)

(4.4)

(4.5)

Then, with probability at least 1 ´ expp´δνnq ´ 3{p2eKnq, exact recovery holds true, i.e., ˆZ “ Z˚.

Therefore, we have exact reconstruction in the dense case (that is under assumption (4.3)), which
stems from condition (4.5). The latter condition is in the same spirit as the one in Theorem 1 of [41],
it measures the SNR (signal-to-noise ratio) of the model which captures the hardness of the SSBM. As
mentioned in [41], it is related to the Kesten-Stigum threshold [78]. The last condition (4.5) basically
requires that the number of clusters K is at most n{ log n. If this condition is dropped out then we do
not have anymore exact reconstruction but only a certiﬁed rate of convergence: with probability at least
1 ´ expp´δνnq ´ 3{p2eKnq,

›
›
›Z˚ ´ ˆZ

›
›
›

ď

1

n
c1δpp ´ qqK

.

This shows that there is a phase transition in the dense case: exact reconstruction is possible when
K À n{ log n and, otherwise, when K Á n{ log n we only have a control of the estimation error.

4.2 Application to angular group synchronization

In this section, we introduce the group synchronization problem as well as a stochastic model for this prob-
lem. We consider a SDP relaxation of the original problem (which is exact) and construct the associated
SDP estimator such as (1.3).

The angular synchronization problem consists of estimating n unknown angles θ1, ¨ ¨ ¨ , θn (up to a
global shift angle) given a noisy subset of their pairwise oﬀsets pθi ´ θjqr2πs, where r2πs is the modulo 2π
operation. The pairwise measurements can be realized as the edge set of a graph G, typically modeled as
an Erd¨os-Renyi random graph [90].

The aim of this section is to show that the angular synchronization problem can be analyzed using our
methodology. In order to keep the presentation as simple as possible, we assume that all pairwise oﬀsets
are observed up to some Gaussian noise: we are given pθi ´ θj ` σgijqr2πs for all 1 ď i ă j ď n where
pgij : 1 ď i ă j ď nq are npn ´ 1q{2 i.i.d. standard Gaussian variables and σ ą 0 is the noise variance. We
may rewrite the problem as follows: we observe a n ˆ n complex matrix A deﬁned by

A “ S ˝ rx˚px˚qJs where S “ pSijqnˆn, Sij “

$
&

%

eισgij
1
e´ισgij

if i ă j
if i “ j
if i ą j

,

(4.6)

i “ eιθi, i “ 1, . . . , n, ¯x denotes
ι denotes the imaginary number such that ι2 “ ´1, x˚ “ px˚
the conjugate vector of x and S ˝ rx˚px˚qJs is the element-wise product pSijxi ¯xjqnˆn. In particular, S is
a Hermitian matrix (i.e. ¯SJ “ S) and ESij “ expp´σ2{2q for i ‰ j and ESii “ 1 if i “ j. We want to
estimate pθ1, . . . , θnq (up to a global shift) from the matrix of data A. Unlike classical statistical models
the noise here is multiplicative; we show that our approach covers this type of problem as well.

i“1 P Cn, x˚

i qn

The ﬁrst step is to ﬁnd an (vectorial) optimization problem which solutions are given by pθiqn

i“1 (up to
i“1 up to global angle shift is equivalent
i“1. The latter is, up to a global rotation of its coordinates, the unique

global angle shift) or some bijective function of it. Estimating pθiqn
to estimating the vector x˚ “ peιθiqn
solution of the following maximization problem
(
¯xJ EA x

“ tpeιpθi`θ0qqn

(cid:32)

(4.7)

i“1 : θ0 P r0, 2πqu.

argmax
xPCn:|xi|“1

13

A proof of (4.7) is given in Section 6. Let us now rewrite (4.7) as a SDP problem. For all x P Cn,
we have ¯xJEAx “ trpEAXq “ (cid:10)EA, X(cid:11) where X “ x¯xJ and tZ P Cnˆn : Z “ x¯xT , |xi| “ 1u “
tZ P Hn : Z ľ 0, diagpZq “ 1n, rankpZq “ 1u where Hn is the set of all n ˆ n Hermitian matrices and
1n P Cn is the vector with all coordinates equal to 1. It is therefore straightforward to construct a SDP
It appears that this relaxation is exact since, for
relaxation of (4.7) by dropping the rank constraint.
C “ tZ P Hn : Z ľ 0, diagpZq “ 1nu,

(cid:10)EA, Z(cid:11) “ tZ˚u,

argmax
ZPC

(4.8)

where Z˚ “ x˚px˚qJ. A proof of (4.8) can be found in Section 6. Finally, as we only observe A, we consider
the following SDP estimator of Z˚

ˆZ P argmax

(cid:10)A, Z(cid:11).

(4.9)

In the next section, we use the strategy from Corollary 1 to obtain statistical guarantees for the estimation
of Z˚ by ˆZ.

Intuitively, the above maximization problem (4.8) attempts to preserve the given angle oﬀsets as best

ZPC

as possible

argmax
θ1,...,θnPr0,2πq

nÿ

i,j“1

e´ιθiAijeιθj ,

(4.10)

where the objective function is incremented by `1 whenever an assignment of angles θi and θj perfectly
satisﬁes the given edge constraint δij “ pθi ´ θjqr2πs (i.e., for a clean edge for which σ “ 0), while the
contribution of an incorrect assignment (i.e., of a very noisy edge) will be almost uniformly distributed on
the unit circle in the complex plane. Due to non-convexity of optimization in (4.10), it is diﬃcult to solve
computationally [109]; one way to overcome this problem is to consider the SDP relaxation from (4.8) but
it is also possible to consider a spectral relaxation such as the one proposed by Singer [90] which replaces
the individual constraints that all zi’s should have unit magnitude by the much weaker single constraint
ř
n
i“1 |zi|2 “ n, leading to

nÿ

argmax
ř

z1,...,znPC;

n
i“1 |zi|2“n

¯ziAijzj.

i,j“1

(4.11)

The solution to the resulting maximization problem is simply given by a top eigenvector of the Hermitian
matrix A, followed by a normalization step. We remark that the main advantage of the SDP relaxation
(4.8) is that it explicitly imposes the unit magnitude constraint for eιθi, which we cannot otherwise enforce
in the spectral relaxation solved via the eigenvector method in (4.11). The above SDP program (4.8) is
very similar to the well-known Goemans-Williamson SDP relaxation for the seminal MAX-CUT problem
of ﬁnding the maximal cut of a graph (the MAX-CUT problem is one of the four applications considered
in this work, see Section 4.3), with the main diﬀerence being that here we optimize over the cone of
complex-valued Hermitian positive semideﬁnite matrices, not just real symmetric matrices.

4.2.1 Main results for phase recovery in the synchronization problem

Our main result concerns the estimation of the matrix of oﬀsets Z˚ “ x˚px˚qJ from the observation of the
matrix A. This result is then used to estimate (up to global phase shift) the angular vector x˚.

a

logp(cid:15)n4q then, with probability at least 1 ´ expp´(cid:15)σ4npn ´ 1q{2q, it

Theorem 4. Let 0 ă (cid:15) ă 1. If σ ď
holds true that

pe´σ2{2{2q }Z˚ ´ Z}2

2 ď (cid:10)EA, Z˚ ´ Z(cid:11) ď p128{3q

?

(cid:15)σ4N.

(4.12)

Once we have an estimator ˆZ for the oracle Z˚, we can extract an estimator ˆx for the vector of phases
x˚ by considering a top eigenvector (i.e. an eigenvector associated with the largest eigenvalue) of ˆZ. It is
then possible to quantify the estimation properties of x˚ by ˆx using a sin-theta theorem and Theorem 4.

14

a

Corollary 2. Let ˆx be a top eigenvector of ˆZ with Euclidean norm }ˆx}2 “
n. Let 0 ă (cid:15) ă 1 and assume
logp(cid:15)n4q. We have the existence of a universal constant c0 ą 0 (which is the constant in the
that σ ď
Davis-Kahan Theorem for Hermitian matrices) such that, with probability at least 1´expp´(cid:15)σ4npn´1q{2q,
it holds true that

?

min
zPC:|z|“1

}ˆx ´ zx˚}2 ď 8c0

a

2{3(cid:15)1{4eσ2{4σ2?
n.

(4.13)

It follows from Corollary 2 that we can estimate x˚ (up to a global rotation z P C : |z| “ 1) with
2 -estimation error of the order of σ2?
a (cid:96)n
n, this
means that a constant proportion of the entries are well estimated. For a values of (cid:15) „ 1{n2, the rate of
estimation is like σ2, we therefore get a much better estimation of x˚ but only with constant probability.
It is important to recall that ˆZ and ˆx can be both eﬃciently computed by solving a SDP problem and
then by considering a top eigenvector of its solution (for instance, using the power method).

n with exponential deviations. Given that }x˚}2 “

?

4.3 Application to the MAX-CUT problem

Let A0 P t0, 1unˆn be the adjacency (symmetric) matrix of an undirected graph G “ pV, E0q, where
V :“ t1, . . . , nu is the set of the vertices and the set of edges is E0 :“ E Y EJ Y tpi, iq : A0
ii “ 1u where
E :“ tpi, jq P V 2
ij “ 1u and EJ “ tpj, iq : pi, jq P Eu. We assume that G has no self
loop so that A0
ii “ 0 for all i P V . A cut of G is any subset S of vertices in V . For a cut S Ă V , we
ij, that is the number of edges in E between S and its
deﬁne its weight by cutpG, Sq :“ p1{2q
complement ¯S “ V zS. The MAX-CUT problem is to ﬁnd the cut with maximal weight:

i ă j and A0

pi,jqPSˆ ¯S A0

ř

:

S˚ P argmax

cutpG, Sq.

SĂV

(4.14)

The MAX-CUT problem is a NP-complete problem but [49] constructed a 0.878 approximating solution
via a SDP relaxation. Indeed, one can write the MAX-CUT problem in the following way. For a cut S Ă V ,
we deﬁne the membership vector x P t´1, 1un associated with S by setting xi :“ 1 if i P S and xi “ ´1
ř
n
i,j“1 A0
ijp1 ´ xixjq :“ cutpG, xq and so solving (4.14) is
if i R S for all i P V . We have cutpG, Sq “ p1{4q
equivalent to solve

x˚ P argmax
xPt´1,1un

cutpG, xq.

(4.15)

Since pxixjqi,j “ xxJ, the latter problem is also equivalent to solve

˜

1
4

nÿ

i,j“1

max

A0

ijp1 ´ Zijq : rankpZq “ 1, Z ľ 0, Zii “ 1

(4.16)

¸

which admits a SDP relaxation by removing the rank 1 constraint. This yields the following SDP relaxation
problem of MAX-CUT from [49]:

Z˚ P argmin

(cid:10)A0, Z(cid:11)

(4.17)

ZPC
where C :“ tZ P Rnˆn : Z ľ 0, Zii “ 1, @i “ 1, . . . , nu.

Unlike the other examples from the previous sections, the SDP relaxation in (9.1) is not exact, except
for bipartite graphs; see [62, 44] for more details. Nevertheless, thanks to the approximation result from
[49] we can use our methodology to estimate Z˚ and then deduce some approximate optimal cut. But
ﬁrst, we introduce a stochastic model because in many situations the adjacency matrix A0 is only partially
observed but still it might be interesting to ﬁnd an approximating solution to the MAX-CUT problem.

We observe A “ S ˝ A0 “ psijA0

ijq1ďi,jďn a “masked” version of A0, where S P Rnˆn is symmetric with
upper triangular matrix ﬁlled with i.i.d. Bernoulli entries: for all i, j P V such that i ď j, Sij “ Sji “ sij
where psijqiďj is a family of i.i.d. Bernoulli random variables with parameter p P p1{2, 1q. Let B :“ ´p1{pqA

15

(cid:10)EB, Z(cid:11) and so we estimate
so that ErBs “ ´A0. We can write Z˚ as an oracle since Z˚ P argmaxZPC
(cid:10)B, Z(cid:11). Our ﬁrst aim is to quantify the cost we pay by using ˆZ
Z˚ via the SDP estimator ˆZ P argmaxZPC
instead of Z˚ in our ﬁnal choice of cut. It appears that the ﬁxed point used in Theorem 1 may be used to
quantify this loss:

¨

»

r˚p∆q “ inf

˝r ą 0 : P

–

sup
ZPC:(cid:10)EB,Z˚´Z(cid:11)

ďr

ﬁ

˛

(cid:10)B ´ EB, Z ´ Z˚(cid:11) ď p1{2qr

ﬂ ě 1 ´ ∆

‚.

(4.18)

Our second result is an explicit high probability upper bound on the latter ﬁxed point.

4.3.1 Main results for the MAX-CUT problem

In this section, we gather the two results on the estimation of Z˚ from ˆZ and on the approximate optimality
of the ﬁnal cut constructed from ˆZ. Let us now explicitly provide the construction of this cut. We consider
the same strategy as in [49]. Assume that ˆZ has been constructed. Let ˆG be a centered Gaussian vector
with covariance matrix ˆZ. Let ˆx be the sign vector of ˆG. Using the statistical properties of ˆZ, it is possible
to prove near optimality of ˆx.

We denote the optimal values of the maxcut problem and its SDP relaxation by

SDPpGq :“ p1{4q(cid:10)A0, J ´ Z˚(cid:11) “ max
ZPC

1
4

ÿ

i,j

A0

i,jp1 ´ Zijq and MAXCUTpGq :“ cutpG, S˚q

where S˚ is a solution of (4.14) and J “ p1qnˆn. Our ﬁrst result is to show how the 0.878 approximating
result from [49] is downgraded by the incomplete information we have on the graph (we only partially
observed the adjacency matrix A0 via A).

Theorem 5. For all 0 ă ∆ ă 1. With probability at least 1 ´ ∆ (with respect to the masked S),

SDPpGq ě E

”
cutpG, ˆxq| ˆZ

ı

ě 0.878SDPpGq ´

0.878r˚p∆q
4

.

To precise the notation, ˆx is the sign vector of ˆG which is a centered Gaussian variable with covariance
is the conditional expectation according to ˆG for a ﬁxed ˆZ. Moreover,

ˆZ. In that context, E
the probability at least 1 ´ ∆ that we obtain is w.r.t. the mask that is to the randomness in A.

”
cutpG, ˆxq| ˆZ

ı

Let us put Theorem 5 into some perspective. If we had known the entire adjacency matrix (which is
the case when p “ 1), then we could have use Z˚ instead of ˆZ. In that case, for x‹ the sign vector of
G‹ „ N p0, Z˚q, we know from [49] that

SDPpGq ě E rcutpG, x‹qs ě 0.878SDPpGq.

(4.19)

Therefore, Theorem 5 characterizes the price we pay for not observing the entire adjacency matrix A0 but
only a masked version A of it. It is an interesting output of Theorem 5 to observe that the ﬁxed point
r˚p∆q measures, in a quantitative way, this loss. If we were able to identify scenarii of p and E for which
r˚p∆q “ 0 that would prove that there is no loss for partially observing A0 in the MAX-CUT problem.
Unfortunately, the approach we use to control r˚p∆q is the global one which does not allow for exact
reconstruction (that is to show that r˚p∆q “ 0).

Let us now turn to an estimation result of Z˚ by ˆZ via an upper bound on r˚p∆q.

Theorem 6. With probability at least 1 ´ 4´n:

d

(cid:10)EB, Z˚ ´ ˆZ(cid:11) ď r˚p4´nq ď 2n

p2 log 4qp1 ´ pqpn ´ 1q
p

`

8n log 4
3

.

16

In particular, it follows from the approximation result from Theorem 5 and the high probability upper

bound on r˚p∆q from Theorem 6 that, with probability at least 1 ´ 4´n,

”
cutpG, ˆxq| ˆZ

ı

E

ě 0.878SDPpGq ´

˜

2n

0.878
4

p2 log 4qp1 ´ pqpn ´ 1q
p

`

8n log 4
3

¸

.

(4.20)

d

This result is none trivial only when the right-hand side term is strictly larger than p0.5qSDPpGq which
is the performance of a random cut. As a consequence, (4.20) shows that one can still do better than
randomness even in an incomplete information setup for the MAX-CUT problem when p, n and SDPpGq
are such that

d

˜

¸

0.378SDPpGq ą

0.878
4

2n

p2 log 4qp1 ´ pqpn ´ 1q
p

`

8n log 4
3

.

For instance, when p is like a constant, it requires SDPpGq to be larger than c0n3{2 (for some absolute
constant c0) and when p “ 1 ´ 1{n, it requires SDPpGq to be at least c0n (for some absolute constant c0).

5 Proof of Theorem 3 (signed clustering)

The aim of this paper is to put forward a methodology developed in learning theory for the study of SDP
estimators. In each example, we follow this methodology. For a problem, such as the signed clustering,
where it is possible to characterize the curvature of the excess risk, we start to identify this curvature
because the curvature function G, coming out of it, deﬁnes the local subsets of C driving the complexity
of the problem. Then, we turn to the stochastic part of the proof which is entirely summarized into the
complexity ﬁxed point r˚
Gp∆q from (2.2). Finally, we put the two pieces together and apply the main
general result from Corollary 1 to obtain estimation results for the SDP estimator (4.2) in the signed
clustering problem; which is Theorem 3.

5.1 Curvature equation
In this section, we show that the objective function Z P C Ñ (cid:10)Z, EA ´ αJ(cid:11) satisﬁes a curvature assumption
around its maximizer Z˚ with respect to the (cid:96)nˆn
-norm given by GpZ˚ ´ Zq “ θ }Z˚ ´ Z}1 with parameter
θ “ δpp ´ qq (and margin exponent κ “ 1).
Proposition 1. For θ “ δpp ´ qq, we have for all Z P C, (cid:10)EA ´ αJ, Z˚ ´ Z(cid:11) “ θ }Z˚ ´ Z}1.
Proof. Let Z be in C. We have

1

(cid:10)Z˚ ´ Z, EA ´ αJ(cid:11) “

nÿ

pZ˚ ´ ZqijpEAij ´ αq

i,j“1
ÿ

“

pi,jqPC`

pZ˚

ij ´ Zijqpδp2p ´ 1q ´ αq `
»

ÿ

pi,jqPC´

“ δpp ´ qq

–

ÿ

pZ˚ ´ Zqij ´

ÿ

pZ˚ ´ Zqij

pi,jqPC`

pi,jqPC´

ﬁ

ﬂ .

pZ˚

ij ´ Zijqpδp2q ´ 1q ´ αq

Moreover, for all pi, jq P C`, Z˚
pi, jq P C´, pZ˚ ´ Zqij “ ´Zij “ ´|pZ˚ ´ Zqij| because in that case Z˚

ij “ 1 and 0 ď Zij ď 1, so pZ˚ ´ Zqij “ |pZ˚ ´ Zqij|. We also have for all

ij “ 1 and 0 ď Zij ď 1. Hence,

(cid:10)Z˚ ´ Z, EA ´ αJ(cid:11) “ δpp ´ qq

»

–

ÿ

|pZ˚ ´ Zqij| `

ÿ

|pZ˚ ´ Zqij|

ﬁ
ﬂ “ θ }Z˚ ´ Z}1 .

pi,jqPC`

pi,jqPC´

17

5.2 Computation of the complexity ﬁxed point r˚

Gp∆q

Deﬁne W :“ A´EA the noise matrix of the problem. Since W is symmetric, its entries are not independent.
In order to work only with independent random variables, we deﬁne the following matrix Ψ P Rnˆn:
"

Ψij “

Wij if i ď j
0 otherwise,

(5.1)

where 0 entries are considered as independent Bernoulli variables with parameter 0 and therefore, Ψ has
independent entries, and satisﬁes the relation W “ Ψ ` ΨJ.

In order to obtain upper bounds on the ﬁxed point complexity parameter r˚

Gp∆q associated with the

signed clustering problem, we need to prove a high probability upper bound on the quantity

sup
ZPC:}Z´Z˚}1ďr

(cid:10)W, Z ´ Z˚(cid:11),

(5.2)

1

and then ﬁnd a radius r as small as possible such that the quantity in (5.2) is less than pθ{2qr. We denote
Cr :“ C X pZ˚ ` rBnˆn

q “ tZ P C : }Z ´ Z˚}1 ď ru where Bnˆn

We follow the strategy from [41] by decomposing the inner product (cid:10)W, Z ´Z˚(cid:11) into two parts according
to the SVD of Z˚. This observation is a key point in the work of [41] compared to the analysis from [54].
This allows to perform the localization argument eﬃciently. Up to a change of index of the nodes, Z˚ is a
|Ck|
block matrix with K diagonal blocks of 1’s. It therefore admits K singular vectors U‚k :“ Ipi P Ckq{
with multiplicity lk associated with the singular value lk for all k P rKs. We can therefore write

is the unit (cid:96)nˆn

-ball of Rnˆn.

a

1

1

Z˚ “

Kÿ

k“1

lkU‚k b U‚k “ U DU J,

where U P RnˆK has K column vectors given by U‚k, k “ 1, . . . , K and D “ diagpl1, . . . , lKq. We deﬁne
the following projection operator

P : M P Rnˆn Ñ U U T M ` M U U T ´ U U T M U U T

and its orthogonal projection P K by

P K : M P Rnˆn Ñ M ´ PpM q “ pIn ´ U U T qM pIn ´ U U T q “

nÿ

(cid:10)M, U‚k b U‚k

(cid:11)U‚k b U‚k

k“K`1

where U‚k P Rn, k “ K ` 1, . . . , n are such that pU‚k : k “ 1, . . . , nq is an orthonormal basis of Rn.

We use the same decomposition as in [41]: for all Z P C,

(cid:10)W, Z ´ Z˚(cid:11) “ (cid:10)W, PpZ ´ Z˚q ` P KpZ ´ Z˚q(cid:11) “ (cid:10)PpZ ´ Z˚q, W (cid:11)
looooooooomooooooooon

` (cid:10)P KpZ ´ Z˚q, W (cid:11)
.
loooooooooomoooooooooon

S1pZq

S2pZq

The next step is to control with large probability the two terms S1pZq and S2pZq uniformly for all Z P
C X pZ˚ ` rBnˆn
q. To that end we use the two following propositions where we recall that ρ “ δ maxp1 ´
δp2p ´ 1q2, 1 ´ δp2q ´ 1q2q and ν “ maxp2p ´ 1, 1 ´ 2qq. The proof of Proposition 2 and 3 can be found in
Section 8, it is based on [41].

1

Proposition 2. There are absolute positive constants c0, c1, c2 and c3 such that the following holds. If
rc1rK{ns ě 2eKn expp´p9{32qnρ{Kq then we have

»

–

P

sup
ZPCXpZ˚`rBnˆn

1

S1pZq ď c2r

q

g
f
f
e Kρ
n

log

18

˜

¸ﬁ

˜

2eKn
r c1rK
n s

ﬂ ě 1 ´ 3

¸
r

c1rK
n s

.

r c1rK
n s
2eKn

Proposition 3. There exists an absolute constant c0 ą 0 such that the following holds. When nνδ ě log n,
with probability at least 1 ´ expp´δνnq,

c

sup
ZPCXpZ˚`rBnˆn

1

q

S2pZq ď c0Kr

δν
n

.

It follows from Proposition 2 and Proposition 3 that when nνδ ě log n, for all r such that rc1rK{ns ě
c1rK
n s, with probability

2eKn expp´p9{32qnρ{Kq we have, for ∆ “ ∆prq :“ expp´δνnq ´ 3
at least 1 ´ ∆,

˘
n s{p2eKnq

r c1rK

`

r

Moreover, we have

sup
ZPCXpZ˚`rBnˆn

1

q

(cid:10)W, Z ´ Z˚(cid:11) ď c0Kr

c

c

c0Kr

δν
n

` c2r

g
f
f
e Kρ
n

log

δν
n

˜

g
f
f
e Kρ
n

˜

log

¸

.

2eKn
r c1rK
n s

` c2r

¸

2eKn
r c1rK
n s

ď

θ
2

r

(5.3)

?

?

ν À

for θ “ δpp ´ qq when K
pp ´ qq2nδ ě K2ν and 1 ě 2eKn max
0 ă r ď n{pc1Kq (5.3) is true. Therefore, one can take r˚
reconstruction of Z˚:
with probability at least 1 ´ expp´δνnq ´ 3{p2eKnq, ˆZ “ Z˚.

nδpp ´ qq and rc1rK{ns ě 2eKn expp´θ2n{pKρqq. In particular, when
, we conclude that for all
Gp∆p0qq “ 0 meaning that we have exact
if pp ´ qq2nδ ě K2ν and n Á K maxpρ{θ2 logp2eK2ρ{θ2q, p1{ρq logp2eK2{ρqq then

˘
expp´θ2n{pKρqq, expp´p9{32qnρ{Kq

`

If pp ´ qq2nδ ě K2ν and 1 ă 2eKn max

then we do not have
exact reconstruction anymore and we have to take r such that c1rK{n ě 1. In that case, (5.3) holds when
r “ n{pc1Kq and so r˚

Gp∆pn{pc1Kqqq ď n{pc1Kq. Therefore, it follows from Corollary 1 that

`

˘
expp´θ2n{pKρqq, expp´p9{32qnρ{Kq

›
›
›Z˚ ´ ˆZ

›
›
›

ď

1

n
c1δpp ´ qqK

.

6 Proofs of Theorem 4 and Corollary 2 (angular synchronization)

Proof of (4.7): For all γ1, . . . , γn P r0, 2πq we have γi ´ γj “ δij for all i ‰ j P rns if and only if
eσ2{2EAijeιγj ´ eιγi “ 0 for all i ‰ j P rns. We therefore have
#

+

|eσ2{2EAijxj ´ xi|2

“ tpeιpθi`θ0qqn

i“1 : θ0 P r0, 2πqu.

(6.1)

ÿ

argmin
xPCn:|xi|“1

i‰j

Moreover, for all x “ pxiqn

i“1 P Cn such that |xi| “ 1 for i “ 1, . . . , n, we have

ÿ

i‰j

|eσ2{2EAijxj ´ xi|2 “
˜

ÿ

i‰j

|x˚
¸

i ¯x˚

j xj ´ xi|2 “

nÿ

i,j“1

|x˚

i ¯x˚

j ´ xi ¯xj|2

“ 2n2 ´ 2(cid:60)

i ¯x˚
x˚

j xj ¯xi

“ 2n2 ´ 2|(cid:10)x˚, x(cid:11)|2

nÿ

i,j“1

where (cid:60)pzq denotes the real part of z P C. On the other side, we have
ÿ

nÿ

¯xJpeσ2{2EAqx “

¯xix˚

i ¯x˚

j xj `

¯xieσ2{2xi “ npeσ2{2 ´ 1q ` |(cid:10)x˚, x(cid:11)|2.

Hence, minimizing x Ñ
maximize x Ñ ¯xJEAx over all x “ pxiqi P Cn such that |xi| “ 1. This concludes the proof with (6.1).

i‰j |eσ2{2EAijxj ´ xi|2 over all x “ pxiqi P Cn such that |xi| “ 1 is equivalent to

n

ř

i‰j

i“1

19

Proof of (4.8): Let C1 “ tZ P Cnˆn : |Zij| ď 1, @i, j P rnsu. We ﬁrst prove that C Ă C1. Let Z P C. Since
Z ľ 0, there exists X P Cnˆn such that Z “ X ¯X J. For all i P t1, . . . , nu, denote by Xi‚ the i-th row
(cid:11) “ Zii “ 1 since diagpZq “ 1n. Moreover, for all i, j P rns, we
2 “ (cid:10)Xi‚, Xi‚
vector of X. We have }Xi‚}2
have |Zij| “ |(cid:10)Xi‚, Xj‚
`
(
Let Z1 P argmax

(cid:11)| ď }Xi‚}2 }Xj‚}2 ď 1. This proves that Z P C1 and so C Ă C1.
(cid:60)p(cid:10)EA, Z(cid:11)q : Z P C1

. Since C1 is convex and the objective function Z Ñ (cid:60)p(cid:10)EA, Z(cid:11)q
is linear (for real coeﬃcients), Z1 is one of the extreme points of C1. Extreme points of C1 are matrices
Z P Cnˆn such that |Zij| “ 1 for all i, j P rns. We can then write each entry of Z1 as Z1
ij “ eιβij for some
0 ď βij ă 2π and now we obtain
˜

¸

˜

˜

¸

¸

(cid:60)p(cid:10)EA, Z1(cid:11)q “ (cid:60)

e´σ2{2eιδij e´ιβij

` (cid:60)

eιδiie´ιβii

nÿ

i,j“1

EAijZ1
ij
ÿ

nÿ

“ (cid:60)

i‰j

i

“

ÿ

i‰j

eσ2{2 cospδij ´ βijq `

cospδii ´ βiiq ď eσ2{2pn2 ´ nq ` n.

nÿ

i“1

The maximal value eσ2{2pn2 ´ nq ` n is attained only for βij “ δij for all i, j P rns, that is for Z1 “
peιδij qi,j“1,...,n “ Z˚. But we have Z˚ P C and C Ă C1, so Z˚ is the only maximizer of Z Ñ (cid:60)p(cid:10)EA, Z(cid:11)q on
C. But for all Z P C we have (cid:10)EA, Z(cid:11) “ x˚JZx˚ P R, then Z˚ is the only maximizer of Z Ñ (cid:10)EA, Z(cid:11) over
C.

6.1 Curvature of the objective function
Proposition 4. For θ “ e´σ2{2{2, we have for all Z P C, (cid:10)EA, Z˚ ´ Z(cid:11) ě θ }Z˚ ´ Z}2
2.
Proof. Let Z “ pzijeιβij qn
all i P rns, we have, on one side, (cid:10)EA, Z˚ ´ Z(cid:11) “ e´σ2{2x˚JpZ˚ ´ Zqx˚ P R, and so
˜

i,j“1q P C where zij P R and 0 ď βij ă 2π for all i, j P rns. Since Z˚

˜

¸

ii “ Zii “ 1 for

¸

(cid:10)EA, Z˚ ´ Z(cid:11) “ (cid:60)

`

(cid:10)EA, Z˚ ´ Z(cid:11)
˜

nÿ

˘

nÿ

“ (cid:60)

i,j“1

EAijpZ˚ ´ Zqij
¸

nÿ

“ (cid:60)

i,j“1

nÿ

e´σ2{2eιδij pe´ιδij ´ zije´ιβij q

“ e´σ2{2(cid:60)

1 ´ zijeιpδij ´βij q

“ e´σ2{2

p1 ´ zij cospδij ´ βijqq.

(6.2)

i,j“1

i,j“1

On the other side, we proved in the proof of (4.8) that C Ă tZ P Cnˆn : |Zij| ď 1, @i, j P rnsu. So we have
|zij| ď 1 for all i, j P rns and
nÿ

nÿ

nÿ

}Z˚ ´ Z}2

2 “

|pZ˚ ´ Zqij|2 “

|eιδij ´ zijeιβij |2 “

|1 ´ zijeιp´δij `βij q|2

i,j“1

i,j“1

i,j“1

nÿ

“

p1 ´ zij cospβij ´ δijqq2 ` z2

ij sin2pβij ´ δijq “

i,j“1
nÿ

ď 2

p1 ´ zij cospβij ´ δijqq.

i,j“1

We conclude with (6.2) and (6.3).

nÿ

i,j“1

1 ´ 2zij cospβij ´ δijq ` z2
ij

(6.3)

In fact, it follows from the proof of Proposition 4 that we have the following equality: for all Z P C,

(cid:10)EA, Z˚ ´ Z(cid:11) “ θ

´

}Z˚ ´ Z}2

2 `

›
›|Z˚|2 ´ |Z|2

›
›
1

¯

where |Z|2 “ p|Zij|2q1ďi,jďn (in particular, |Z˚|2 “ p1qnˆn). We therefore know exactly how to characterize
the curvature of the excess risk for the angular synchronization problem in terms of the (cid:96)2 (to the square)
and the (cid:96)1 norms. Nevertheless, we will not use the extra term

›
›|Z˚|2 ´ |Z|2

›
›
1 in the following.

20

6.2 Computation of the complexity ﬁxed point r˚

Gp∆q

It follows from the (global) curvature property of the excess risk for the angular synchronization problem
obtained in Proposition 4 that for the curvature G function deﬁned by GpZ˚ ´ Zq “ θ }Z˚ ´ Z}2
2 , @Z P C,
we just have to compute the r˚
Gp∆q ﬁxed point and then apply Corollary 1 in order to obtain statistical
properties of ˆZ (w.r.t.
In this section, we compute the
complexity ﬁxed point r˚

to both the excess risk and the G function).
Gp∆q for 0 ă ∆ ă 1.

Following Proposition 4, the natural “local” subsets of C around Z˚ which drive the statistical com-
plexity of the synchronization problem are deﬁned for all r ą 0 by Cr “ tZ P C : }Z ´ Z˚}2 ď ru “
C X pZ˚ ` rBnˆn

imaginary) part of bij “ Z˚

ijZij ´ Z˚

ij for all

q.
Let Z P Cr. Denote by bR

2

ij (resp. bI

i, j P rns. Since }Z ´ Z˚}2 ď r we also have

ř

ij) the real (resp.
i,jpbR
ijq2 ` pbI
˜

(cid:10)A ´ EA, Z ´ Z˚(cid:11) “ (cid:10)pS ´ ESq ˝ Z˚, Z ´ Z˚(cid:11) “ 2(cid:60)

ÿ

“ 2

pcospσgijq ´ E cospσgijqqbR

ij ´ sinpσgijqbI

ij ď 2r

iăj
g
f
f
e1 ´ e´σ2 ` 2e´σ2{2

ď 2r

˜

ÿ

iăj

E cospσgijq ´ cospσgijq

ijq2 ď r2 and so
ÿ

pSij ´ ESijqbij

¸

iăj
dÿ

iăj

¸

pcospσgijq ´ E cospσgijqq2 ` psinpσgijqq2

where we used that E cospσgq “ (cid:60)pEeιgq “ e´σ2{2 for g „ N p0, 1q. Now its remains to get a high probability
upper bound on the sum of the centered cosinus of σgij. We use Bernstein’s inequality (see Equation 7.3
below) to get such a bound. For all t ą 0, with probability at least 1 ´ expp´tq,
?

ÿ

?

E cospσgijq ´ cospσgijq ď

2V t `

ď p1 ´ e´σ2

q

t `

2t
?
3

N

2t
?

3

N

1
?
N

iăj

for N “ npn ´ 1q{2 and V “ E cos2pσgq ´ pE cospσgqq2 “ p1{2qp1 ´ e´σ2
cosp2σgqq “ p1{2qp1 ` e´2σ2

q when g „ N p0, 1q).

We now have all the ingredients to compute the ﬁxed point r˚

q2 (because E cos2pσgq “ p1{2qEp1 `

Gp∆q for 0 ă ∆ ă 1: for θ “ e´σ2{2{2 and

t “ logp1{∆q,

r˚
Gp∆q ď

4
θ

ˆ

ˆ

1 ´ e´σ2

` 2e´σ2{2

p1 ´ e´σ2

q

?

tN `

2t
3

˙˙

“

32t
3

` 8p1 ´ e´σ2

qpeσ2{2 ` 2

?

tN q.

In particular, using 1 ´ e´σ2
eσ2{2 ď 2σ2?

(cid:15)N then r˚

Gp∆q ď p128{3qσ4N

?

(cid:15).

ď σ2 and for t “ (cid:15)σ4N (where N “ npn ´ 1q{2) for some 0 ă (cid:15) ă 1, if

?

a

6.3 End of the proof of Theorem 4 and Corollary 2: application of Corollary 1
(cid:15)σ4N when eσ2{2 ď 2

Take ∆ “ expp´(cid:15)σ4N q (for N “ npn ´ 1q{2), we have r˚
(which holds for instance when σ ď
curvature property in Section 6.1 and the computation of the ﬁxed point r˚
with probability at least 1 ´ expp´(cid:15)σ4npn ´ 1q{2q, θ(cid:10)Z˚ ´ Z(cid:11)2
is the statement of Theorem 4.

(cid:15)σ2N
logp(cid:15)N 2q) and so it follows from Corollary 1 (together with the
Gp∆q from Section 6.2), that
?
2 ď (cid:10)EA, Z˚ ´ Z(cid:11) ď p128{3q
(cid:15)σ4N , which

Gp∆q ď p128{3q

Proof of Corollary 2: The oracle Z˚ is the rank one matrix x˚x˚J which has n for largest eigenvalue
and associated eigenspace tλx˚ : λ P Cu. In particular, Z˚ has a spectral gap g “ n. Let ˆx P Cn be
a top eigenvector of ˆZ with norm }ˆx}2 “
n. It follows from Davis-Kahan Theorem (see, for example,
Theorem 4.5.5 in [100] or Theorem 4 in [101]) that there exists an universal constant c0 ą 0 such that
x˚
?
n

›
›
› ˆZ ´ Z˚

min
zPC:|z|“1

ˆx
?
n

›
›
›
2

c0
g

´ z

›
›
›
›

?

?

ď

›
›
›
›
2

21

where g “ n is the spectral gap of Z˚. We conclude the proof of Corollary 2 using the upper bound on
›
›
› ˆZ ´ Z˚

from Theorem 4.

›
›
›
2

7 Proofs of Theorem 5 and 6 (MAX-CUT)

In this section, we prove the two main results from Section 4.3 using our general methodology for Theorem 6
and the technique from [49] for Theorem 5.

7.1 Proof of Theorem 5

The proof of Theorem 5 follows the one from [49] up to a minor modiﬁcation due to the fact that we use
the SDP estimator ˆZ instead of the oracle Z˚. It is based on two tools. The ﬁrst one is Grothendieck’s
identity: let g „ N p0, Inq and u, v P S n´1

, we have:

2

Ersignp(cid:10)g, u(cid:11)qsignp(cid:10)g, u(cid:11)qs “

arcsinp(cid:10)u, v(cid:11)q

2
π

and the identity: for all t P r´1, 1s

1 ´

2
π

arcsinptq “

2
π

arccosptq ě 0.878p1 ´ tq.

(7.1)

(7.2)

We now have enough tools to prove Theorem 5. The right-hand side inequality is trivial since MAXCUTpGq ď

1 , . . . , X ˚

n) the n columns vectors in

SDPpGq. For the left-hand side, we denote by ˆX1, . . . , ˆXn (resp. X ˚
of ˆZ (resp. Z˚). We also consider the event Ω˚ onto which
S n´1
2
(cid:10)EB, Z˚ ´ ˆZ(cid:11) ď r˚p∆q

ﬀ

1
2π

nÿ

i,j“1

which hold with probability at least 1 ´ ∆ according to Theorem 1. On the event Ω˚, we have

”
cutpG, ˆxq| ˆZ

ı

E

“ E

piq
“

1
4

nÿ

i,j“1

ˆ

A0
ij

1 ´

2
π

«

1
4

nÿ

i,j“1

A0

ijp1 ´ ˆxi ˆxjq

“

˙

arcsinp(cid:10) ˆXi, ˆXj

(cid:11)q

“

1
4

nÿ

i,j“1

A0
ij

´

¯
1 ´ Ersignp(cid:10) ˆXi, g(cid:11)qsignp(cid:10) ˆXj, g(cid:11)qs

nÿ

i,j“1

ij arccosp(cid:10) ˆXi, ˆXj
A0

(cid:11)q

nÿ

ijp1 ´ (cid:10) ˆXi, ˆXj
A0

(cid:11)q “

0.878
4

piiq
ě

0.878
4

“

0.878
4

i,j“1
(cid:10)A0, J ´ Z˚(cid:11) `
0.878
4

0.878
4
r˚p∆q

ě 0.878 SDPpGq ´

ijp1 ´ (cid:10)X ˚
A0

i , X ˚
j

(cid:11)q `

0.878
4

nÿ

ijp(cid:10)X ˚
A0

i , X ˚
j

(cid:11) ´ (cid:10) ˆXi, ˆXj

(cid:11)q

(cid:10)A0, Z˚ ´ ˆZ(cid:11) “ 0.878SDPpGq ´

i,j“1
(cid:10)EB, Z˚ ´ ˆZ(cid:11)

0.878
4

where we used (7.1) in (i) and (7.2) in (ii).

7.2 Proof of Theorem 6

For the MAX-CUT problem, we do not use any localization argument; we therefore use the (likely sub-
optimal) global approach. The methodology is very close to the one used in [54] for the community detection
problem. In particular, we use both Bernstein and Grothendieck inequalities to compute high probability
upper bound for r˚p∆q. We recall theses two tools now. First Bernstein’s inequality: if Y1, . . . , YN are N
independent centered random variables such that |Yi| ď M a.s. for all i “ 1, . . . , N then for all t ą 0, with
probability at least 1 ´ expp´tq,

1
?
N

Nÿ

i“1

Yi ď σ

?

2t `

2M t
?
N
3

22

(7.3)

ř

where σ2 “ p1{N q
rem 3.4 in [54]): if C P Rnˆn then

N
i“1 varpYiq. The second tool is Grothendieck inequality [52] (see also [85] or Theo-

(cid:10)C, Z(cid:11) ď KG }C}cut “ KG max

s,tPt´1,1un

sup
ZPC

nÿ

i,j“1

Cijsitj

(7.4)

where C “ tZ ľ 0 : Zii “ 1, i “ 1, . . . , nu and KG is an absolute constant, called the Grothendieck
constant.

In order to apply Theorem 1, we just have to compute the ﬁxed point r˚p∆q. As announced, we use

the global approach and Grothendieck inequality (7.4) to get

sup
ZPC:(cid:10)EB,Z˚´Z(cid:11)

ďr

(cid:10)B ´ EB, Z ´ Z˚(cid:11) ď sup
ZPC

(cid:10)B ´ EB, Z ´ Z˚(cid:11) ď 2KG }B ´ EB}cut

(7.5)

because Z˚ P C. It follows from Bernstein’s inequality (7.3) and a union bound that for all t ą 0, with
probability at least 1 ´ 4n expp´tq,

}B ´ EB}cut “ sup

s,tPt˘1un

ÿ

pBij ´ EBijqpsitj ` sjtiq ď 2

d

1ďiăjďn

p1 ´ pqnpn ´ 1qt
p

`

4t
3

.

Therefore, for t “ 2n log 4, with probability at least 1 ´ 4´n,

d

r˚p∆q ď }B ´ EB}cut ď 2n

p2 log 4qp1 ´ pqpn ´ 1q
p

`

8n log 4
3

for ∆ “ 4´n. Then the result follows from Theorem 1.

8 Annex 1: Signed clustering

8.1 Proof of Equation (4.1)
We recall that the cluster matrix ¯Z P t0, 1unˆn is deﬁned by Zij “ 1 if i „ j and Zij “ 0 when i  j and
α “ δpp ` q ´ 1q. For all matrix Z P r0, 1snˆn, we have

(cid:10)Z, EA ´ αJ(cid:11) “

nÿ

i,j“1

ZijpEAij ´ αq “

ÿ

pi,jqPC`

ÿ

ZijpEAij ´ αq `

ÿ

ZijpEAij ´ αq

ÿ

pi,jqPC´

nÿ

“ rδp2p ´ 1q ´ αs

Yij ` rδp2q ´ 1q ´ αs

Zij ` p1 ´ αq

Zii

“ δpp ´ qq

»

–

pi,jqPC`:i‰j

ÿ

ÿ

Yij ´

pi,jqPC`

pi,jqPC´

ﬁ

ﬂ ` p1 ´ αq

Yij

pi,jqPC´

nÿ

i“1

Zii.

i“1

The latter quantity is maximal for Z P r0, 1snˆn such that Zij “ 1 for pi, jq P C` and Zij “ 0 for pi, jq P C´,
(cid:10)Z, EA ´ αJ(cid:11). Moreover, ¯Z P C Ă r0, 1snˆn
that is when Z “ ¯Z. As a consequence t ¯Zu “ argmaxZPr0,1snˆn
so we also have that ¯Z is the only solution to the problem maxZPC

(cid:10)Z, EA ´ αJ(cid:11) and so ¯Z “ Z˚.

23

8.2 Proof of Proposition 2: control of S1pZq adapted from [41]

The noise matrix W is symmetric and has been decomposed as W “ Ψ ` ΨJ where Ψ has been deﬁned in
(5.1). For all Z P C X pZ˚ ` rBnˆn

q, we have

1

S1pZq “ (cid:10)PpZ ´ Z˚q, W (cid:11) “ (cid:10)PpW q, Z ´ Z˚(cid:11)
“ (cid:10)U U JW, Z ´ Z˚(cid:11) ` (cid:10)W U U J, Z ´ Z˚(cid:11) ´ (cid:10)U U JW U U J, Z ´ Z˚(cid:11)
“ 2(cid:10)U U JW, Z ´ Z˚(cid:11) ´ (cid:10)U U JW U U J, Z ´ Z˚(cid:11)
“ 2(cid:10)U U JΨ, Z ´ Z˚(cid:11) ` 2(cid:10)U U JΨJ, Z ´ Z˚(cid:11) ´ (cid:10)U U JpΨ ` ΨJqU U J, Z ´ Z˚(cid:11)
“ 2(cid:10)U U JΨ, Z ´ Z˚(cid:11) ` 2(cid:10)U U JΨJ, Z ´ Z˚(cid:11) ´ 2(cid:10)U U JΨU U J, Z ´ Z˚(cid:11)
“ 2(cid:10)U U JΨ, Z ´ Z˚(cid:11) ` 2(cid:10)U U JΨJ, Z ´ Z˚(cid:11) ´ 2(cid:10)U U JΨ, pZ ´ Z˚qU U J(cid:11).

(8.1)

An upper bound on S1pZq follows from an upper bound on the three terms in the right side of (8.1). Let
us show how to bound the ﬁrst term. Similar arguments can be used to control the other two terms.

Let V :“ U U T Ψ. Let us ﬁnd a high probability upper bound on the term (cid:10)U U JΨ, Z´Z˚(cid:11) “ (cid:10)V, Z´Z˚(cid:11)

uniformly over Z P C X pZ˚ ` rBnˆn

1

q. For all k P rKs, i P Ck and j P rns, we have

Vij “

nÿ

t“1

pU U T qitΨtj “

ÿ

tPCk

1
lk

Ψtj “

1
lk

ÿ

tPCk

Ψtj “

1
lk

ÿ

tPCk

Ψtj.

Therefore, given j P rns the Vij’s are all equal for i P Ck. We can therefore ﬁx some arbitrary index ik P Ck
and have Vij “ Vikj for all i P Ck. Moreover, pVikj : k P rKs, j P rnsq is a family of independent random
variables. We now have
ÿ

ÿ

ÿ

ÿ

ÿ

ÿ

ÿ

ÿ

(cid:10)V, Z ´ Z˚(cid:11) “

VijpZ ´ Z˚qij “

lkVikj

kPrKs

iPCk

jPrns

kPrKs

jPrns

iPCk

pZ ´ Z˚qij
lk

“

lkVikjwkj

kPrKs

jPrns

which is a weighted sum of nK independent centered random variables Xk,j :“ lkVikj with weights wk,j “
pZ ´ Z˚qij for k P rKs, j P rns. We now idenﬁty some properties on the weights wkj.
p1{lkq

ř

iPCk

The weights are such that
ÿ

ÿ

|wk,j| ď

kPrKs

jPrns

c1K
n

ÿ

ÿ

ÿ

kPrKs

jPrns

iPCk

|pZ ´ Z˚qij| “ }Z ´ Z˚}1

c1K
n

ď

c1rK
n

which is equivalent to say that the weights vector w “ pwkj
pc1rK{nqBKn

. It is also in the unit (cid:96)Kn

8 -ball since for all k P rKs and j P rns,

1

: k P rKs, j P rnsq is in the (cid:96)Kn

1

-ball

|wk,j| ď

ÿ

iPCk

|pZ ´ Z˚qij|
lk

ď }Z ´ Z˚}8 ď 1.

We therefore have w P BKn

8 X pc1rK{nqBKn

and so

sup
ZPCXpZ˚`rBnˆn

1

1
(cid:10)V, Z ´ Z˚(cid:11) ď
q

sup

wPBKn

8 Xpc1rK{nqBKn

1

ÿ

kPrKs,jPrns

Xk,jwk,j.

(8.2)

It remains to ﬁnd a high probability upper bound on the latter term. We can use the following lemma to
that end.

ř

Lemma 1. Let Xk,j “
then with probability at least 1 ´ prRs{p2eKnqqrRs,

tPCk

Ψtj for pk, jq P rKsˆrns. For all 0 ď R ď Kn, if rRs ě 2eKn expp´p9{32qnρ{Kq

sup
8 XRBKn

1

wPBKn

ÿ

Xk,jwk,j ď 4

?

8c0R

pk,jqPrKsˆrns

24

d

nρ
K

log

ˆ

˙

.

2eKn
rRs

Proof of Lemma 1. Let N “ Kn and assume that 1 ď R ď N . We denote by X ˚

2 ě ¨ ¨ ¨ , ě X ˚
N
N ) the non-decreasing rearrangement of |Xk,j| (resp. |wk,j|) for pk, jq P rKs ˆ rns. We

1 ě ¨ ¨ ¨ ě w˚

1 ě X ˚

(resp. w˚
have

ÿ

sup
8 XRBN
1

wPBN

pk,jqPrKsˆrns

Xk,jwk,j ď

sup
8 XRBN
1

wPBN

Nÿ

i“1

ď

rRsÿ

i“1

X ˚

i ` RX ˚

rRs`1 ď 2

rRsÿ

i“1

X ˚
i .

X ˚

i w˚

i ď sup
wPBN
8

rRsÿ

i“1

X ˚

i w˚

i ` sup
wPRBN
1

Nÿ

i“rRs`1

X ˚

i w˚
i

Moreover, for all τ ą 0, using a union bound, we have

¨

˛

¨

rRsÿ

˝

P

i“1
¨

X ˚

i ą τ

‚“ P

˝DI Ă rKs ˆ rns : |I| “ rRs and

ÿ

pk,jqPI
˛

˛

‚

|Xk,j| ą τ

“ P

˝

max
IĂrKsˆrns:|I|“rRs

max
uk,j “˘1,pk,jqPI

¨

ÿ

pk,jqPI

ÿ

ď

ÿ

ÿ

˝

P

Xk,juk,j ą τ

uk,jXk,j ą τ

‚

˛

‚“

ÿ

ÿ

¨

˝

P

ÿ

ÿ

˛

‚.

Ψt,juk,j ą τ

IĂrKsˆrns:|I|“rRs

uPt˘1urRs

pk,jqPI

IĂrKsˆrns:|I|“rRs

uPt˘1urRs

pk,jqPI

tPCk

Let us now control each term of the latter sum thanks to Bernstein inequality. The random variables
pΨt,j : t, j P rnsq are independent with variances at most ρ “ δ maxp1 ´ δp2p ´ 1q2, 1 ´ δp2q ´ 1q2q since
VarpΨijq “ 0 when i ą j and VarpΨijq “ VarpAij ´ ErAijsq “ VarpAijq ď ρ for j ě i. Moreover, |Ψij| “ 0
when j ă i and |Ψij| “ |Wij| “ |Aij ´ EAij| ď 2 for j ě i because Aij P t´1, 0, 1u.
It follows from
Bernstein’s inequality that for all I Ă rKs ˆ rns satisfying |I| “ rRs and u P t˘1urRs that

¨

˛

ÿ

ÿ

˝

P

pk,jqPI

tPCk

Ψt,juk,j ą τ

‚ď exp

ˆ

´τ 2
2rRslkρ ` 4τ {3

˙

ˆ

ď exp

´τ 2
2rRsc0nρ{K ` 4τ {3

˙

ˆ

ď exp

˙

´τ 2
4rRsc0nρ{K

ř

when τ ď p3{2qrRsc0nρ{K. Therefore, supwPBN

ˆ

˙

N
rRs

2rRs exp

ˆ

8 XRBN
1
´τ 2
4rRsc0nρ{K

Xk,jwk,j ď 2τ with probability at least
˙

ˆ

˙

ě 1 ´ exp

´τ 2
8rRsc0nρ{K

1 ´

when

p3{2qrRsc0nρ{K ě τ ě

d

?

8c0rRs

nρ
K

log

˙

ˆ

2eN
rRs

which is a non vacuous condition since rRs ě 2eN expp´p9{32qnρ{Kq. The result follows, in the case
1 ď R ď N , by taking τ “
For 0 ď R ď 1, we have

pnρ{Kq log p2eN {rRsq and using that 2R ě rRs when R ě 1.

8c0rRs

a

?

ÿ

sup
8 XRBKn

1

wPBKn

Xk,jwk,j “ R

max
pk,jqPrKsˆrns

|Xk,j|

pk,jqPrKsˆrns

and using Bernstein inequality as above we get that with probability at least 1 ´ expp´Kτ 2{p8c0nρqq,
a
maxpk,jqPrKsˆrns |Xk,j| ď τ when 3c0nρ{p2Kq ě τ ě
8c0nρ logpnKq{K which is a non vacuous condition
when 9c0nρ ě 4K logpnKq. By taking τ “
8c0nρ logpnKq{K, we obtain, that for all 0 ď R ď 1, if
9c0nρ ě 4K logpnKq then with probability at least 1 ´ 1{pnKq,

a

sup
8 XRBKn

1

wPBKn

ÿ

pk,jqPrKsˆrns

c

Xk,jwk,j ď R

8c0nρ logpnKq
K

.

25

We apply Lemma 1 for R “ c1rK{n to control (8.2):

»

–

P

sup
ZPCXpZ˚`rBnˆn

1

q

(cid:10)V, Z ´ Z˚(cid:11) ď c2r

g
f
f
e Kρ
n

log

˜

¸ﬁ

˜

2eKn
r c1rK
n s

ﬂ ě 1 ´

r c1rK
n s
2eKn

¸

r

c1rK
n s

when rc1rK{ns ě 2eKn expp´p9{32qnρ{Kq.

Using the same methodology, we can prove exactly the same result for supZPCXpZ˚`rBnˆn

Z˚(cid:11). We can also use the same method to upper bound supZPCXpZ˚`rBnˆn
simply have to check that the weights vector w1 “ pw1
Z˚qU U Jsij is also in BKn
case, since we have for all i P rns, k1 P rKs and j P Ck1, rpZ ´ Z˚qU U Jsij “
ř

(cid:10)U U JΨJ, Z ´
q
(cid:10)U U JΨJ, pZ ´ Z˚qU U J(cid:11), we
rpZ ´
for any Z P C such that }Z ´ Z˚}1 ď r. This is indeed the
n
p“1pZ ´ Z˚qippU U Jqpj “

kj : k P rKs, j P rnsq where w1

8 X pc1rK{nqBKn

kj “ p1{lkq

iPCk

ř

ř

1

q

1

1

pPCk1 pZ ´ Z˚qip{lk1 which is therefore constant for all elements in j P Ck1. Therefore, we have

ÿ

ÿ

|w1

kj| “

ÿ

ÿ

ÿ

kPrKs

jPrns

ÿ

ÿ

ÿ

k1PrKs
ÿ

jPCk1
ÿ

ď

kPrKs

k1PrKs

jPCk1

iPCk

pPCk1

kPrKs
1
lklk1

ˇ
ˇ
ˇ
ˇ
ˇ
ˇ

1
lklk1

ÿ

ÿ

iPCk

pPCk1

pZ ´ Z˚qip

ˇ
ˇ
ˇ
ˇ
ˇ
ˇ

|pZ ´ Z˚qip| ď }Z ´ Z˚}1

c1K
n

ď

c1rK
n

and for all k1 P rKs and j P Ck1,

|w1

kj| “

ˇ
ˇ
ˇ
ˇ
ˇ
ˇ

1
lklk1

ÿ

ÿ

iPCk

pPCk1

pZ ´ Z˚qip

ˇ
ˇ
ˇ
ˇ
ˇ
ˇ

ď }Z ´ Z˚}8 ď 1.

Therefore, w1 P BKn
(8.1). This concludes the proof of Proposition 2.

8 X pc1rK{nqBKn

1

and we obtain exactly the same upper bound for the three terms in

8.3 Proof of Proposition 3: control of the S2pZq term from [41]

In this section, we prove Proposition 3. We follow the proof from [41] but we only consider the “dense
case” which is when nδν ě log n – we recall that ν “ maxp2p ´ 1, 1 ´ 2qq. For a similar uniform control of
S2pZq in the “sparse case ”, when c0 ď nδν ď log n for some absolute constant c0, we refer the reader to
[41].

For all Z P C, we have S2pZq “ (cid:10)P KpZ ´ Z˚q, W (cid:11) “ (cid:10)P KpZq, W (cid:11) because, by construction of the
projection operator, P KpZ˚q “ 0. Therefore, S2pZq ď
˚ }W }op where }¨}˚ denotes the nuclear
norm (i.e. the sum of singular values) and }¨}op denotes the operator norm (i.e. maximum of the singular
value). In the following Lemma 2, we prove an upper bound for
and then, we will obtain a high
probability upper bound onto }W }op.

›
›
›
›P KpZq
›
›P KpZq

›
›
˚

Lemma 2. For all Z P C X pZ˚ ` rBnˆn

1

q, we have

›
›P KpZq

›
›
˚ “ TrpP KpZqq ď

c1k
n

}Z ´ Z˚}1 ď

c1Kr
n

.

Proof. Since Z ľ 0 so it is for pIn ´ U U JqZpIn ´ U U Jq and so P KpZq “ pIn ´ U U JqZpIn ´ U U Jq ľ 0
therefore

›
›
˚ “ TrpP KpZqq. Next, we bound the trace of P KpZq.

›
›P KpZq

26

Since In ´ U U J is a projection operator, it is symmetric and pIn ´ U U Jq2 “ In ´ U U J, moreover,

TrpZq “ n “ TrpZ˚q when Z P C so

TrpP KpZqq “ TrpP KpZ ´ Z˚qq “ TrppIn ´ U U JqpZ ´ Z˚qpIn ´ U U Jqq
“ TrppIn ´ U U T q2pZ ´ Z˚qq “ TrppIn ´ U U T qpZ ´ Z˚qq “ TrpZq ´ TrpZ˚q ` TrpU U T pZ˚ ´ Zqq

“

ď

ÿ

pU U T qijpZ˚ ´ Zqij “

ÿ

ÿ

i,j
c1K
n

ÿ

ÿ

kPrKs

i,jPCk

kPrKs

|pZ˚ ´ Zqij| ď

i,jPCk
c1K
n

pZ˚ ´ Zqij

(i)
“

1
lk

ÿ

kPrKs

1
lk

ÿ

i,jPCk

|pZ˚ ´ Zqij|

}Z ´ Z˚}1

where we used in (i) that for i and j in a same community, we have Z˚
pZ˚ ´ Zqij “ |pZ˚ ´ Zqij|. Finally, when Z is in the localized set C X pZ˚ ` rBnˆn
which concludes the proof.

ij “ 1 and Zij P r0, 1s, thus
q, we have }Z ´ Z˚}1 ď r

1

Now, we obtain a high probability upper bound on }W }op. In the following, we apply this result in

the “dense case” (i.e. nδν ě log n) to get the uniform bound onto S2pZq over Z P C X pZ˚ ` rBnˆn
1
a

q.

Lemma 3 (Lemma 4 in [41]). With probability at least 1 ´ expp´δνnq, }W }op ď 16

δνn ` 168

logpnq.

?

Proof. Let A1 be an independent copy of A and R P Rnˆn be a random symmetric matrix independent
from both A and A1 whose sub-diagonal entries are independent Rademacher random variables. Using a
symmetrization argument (see Chapter 2 in [64] or Chapter 2.3 in [98]), we obtain for W “ A ´ EA,

E }W }op “ E

›
›A ´ EA1

›
›
op

(i)
ď E

›
›A ´ A1

›
›

op

(ii)
“ E

›
›
pA ´ A1q ˝ R

›
›

(iii)
ď 2E }A ˝ R}op

op

where ˝ is the entry-wise matrix multiplication operation, (i) comes from Jensen’s inequality, (ii) occurs
since A ´ A1 and pA ´ A1q ˝ R are identically distributed and (iii) is the triangle inequality. Next, we obtain
an upper bound onto E }A ˝ R}op.

We deﬁne the family of independent random variables pξij : 1 ď i ď j ď nq where for all 1 ď i ď j ď n

$
’’&

’’%

ξij “

1?

with probability

|EAij |
´ 1?
0 with probability 1 ´ EAij.

with probability

|EAij |

EAij
2
EAij
2

(8.3)

a

|EAij| for all 1 ď i ď j ď n. It is straightforward to see that pξijbij : 1 ď i ď j ď nq
We also put bij :“
and pAijRij : 1 ď i ď j ď nq have the same distribution. As a consequence, }A ˝ R}op and }X}op have
the same distribution where X P Rnˆn is a symmetric matrix with Xij “ ξijbij for 1 ď i ď j ď n. An
upper bound on E }X}op follow from the next result due to [9].
Theorem 7 (Corollary 3.6 in [9]). Let ξij, 1 ď i ď j ď n be independent symmetric random variables with
unit variance and pbij, 1 ď i ď j ď nq be a family of real numbers. Let X P Rnˆn be the random symmetric
. Then, for any
matrix deﬁned by Xij “ ξijbij for all 1 ď i ď j ď n. Let σ :“ max1ďiďn
α ě 3,

n
j“1 b2
ij

!bř

)

„

E }X}op ď e

2
α

2σ ` 14α max

1ďiďjďn

!
}ξijbij}2rα logpnqs

) a

logpnq



where, for any q ą 0, }¨}q denotes the Lq-norm.

27

Since pξij : 1 ď i ď j ď nq are independent symmetric such that Varpξijq “ Erξ2

ijs “ 1 we can apply
Lemma 7 to upper bound E }X}op “ E }A ˝ R}op. We have }ξijbij}2rα logpnqs ď 1 for any α ě 3 and
ij “ |EAij| ď δν. It therefore follows from Lemma 7 for α “ 3 that
b2

E }W }op ď 2e

”
?
2

2
3

a

ı
logpnq

nδν ` 42

?

nδν ` 168

a

logpnq.

ď 8

(8.4)

The ﬁnal step to prove Lemma 3 is a concentration argument showing that }W }op is close to its
expectation with high probability. To that end we rely on a general result for Lipschitz and separately
convex functions from [16]. We ﬁrst recall that a real-valued function f of N variables is said separately
convex when for every i “ 1, . . . , N it is a convex function of the i-th variable if the rest of the variables
are ﬁxed.
Theorem 8 (Theorem 6.10 in [16]). Let X be a convex compact set in R with diameter B. Let X1, ¨ ¨ ¨ , XN
be independent random variables taking values in X . Let f : X N Ñ R be a separately convex and 1-Lipschitz
function, w.r.t. the (cid:96)N
2 -norm (i.e. |f pxq ´ f pyq| ď }x ´ y}2 for all x, y P X N ). Then Z “ f pX1, . . . , XN q
satisﬁes, for all t ą 0, with probability at least 1 ´ expp´t2{B2q, Z ď ErZs ` t.
We apply Theorem 8 to Z :“ }W }op “ f pAij ´ EAij, 1 ď i ď j ď nq “ 1?

2 }A ´ EA}op where f is a 1-
2 -norm for N “ npn´1q{2 and separately convex function and pAij ´EAij, 1 ď i ď j ď nq
Lipschitz w.r.t. (cid:96)N
is a family of N independent random variables. Moreover, for each i ě j, pA ´ EAqij P r´1 ´ δp2p ´ 1q, 1 ´
δp2q ´ 1qs, which is a convex compact set with diameter B “ 2p1 ` δpp ´ qqq ď 4. Therefore, it follows
from Theorem 8 that for all t ą 0, with probability at least 1 ´ expp´t2{16q, }W }op ď E }W }op `
2t. In
particular, we ﬁnish the proof of Lemma 3 for t “ 4

δνn and using the bound from (8.4).

?

?

It follows from Lemma 3 that when nνδ ě log n, }W }op ď 184

nδν with probability at least 1 ´

expp´δνnq. Using this later result together with Lemma 2 concludes the proof of Proposition 3.

?

9 Annex 2: Solving SDP’s in practice

The practical implementation of our approach to the problems of synchronization, signed clustering and
MAX-CUT resort to solving a convex optimization problem. In the present section, we describe the various
algoritms we used for solving these SDP’s.

9.1 Pierra’s method

For SDP’s with constraints on the entries, we propose a simple modiﬁcation of the method initially proposed
by Pierra in [84]. Let f : Rnˆn ÞÑ R be a convex function. Let C denote a convex set which can be written
as the intersection of convex sets C “ S1 X ¨ ¨ ¨ X SJ . Let us deﬁne H “ Rnˆn ˆ ¨ ¨ ¨ ˆ Rnˆn (J times) and
let D denote the (diagonal) subspace of H of vectors of the form pZ, . . . , Zq. In this new formalism, the
problem can now be formulated as a minimisation problem over the intersection of two sets only, i.e.

Deﬁne F pZq “ 1
J

following iterations

ř

˜

1
J

Jÿ

j“1

f pZjq : Z “ pZjqJ

j“1 P pS1 ˆ ¨ ¨ ¨ ˆ SJ q X D

.

¸

min
ZPH

J
j“1 fpZjq. The algorithm proposed by Pierra in [84] consists of performing the

Zp`1 “ ProxIS1ˆ¨¨¨ˆSJ ` 1

2 εF pBpq and Bp`1 “ ProjDpZp`1q.

28

9.1.1 Application to community detection

Let us now present the computational details of Pierra’s method for the community detection problem.
We will estimate its membership matrix ¯Z via the following SDP estimator

ˆZ P argmaxZPCxA, Zy,
ř

K
where C “ tZ P Rnˆn, Z ľ 0, Z ě 0, diagpZq ĺ In,
k“1 |Ck|2 denotes the
number of nonzero elements in the membership matrix ¯Z. The motivation for this approach stems from the
fact that the membership matrix ¯Z is actually the oracle, i.e., Z˚ “ ¯Z , where Z˚ P argmaxZPCxErAs, Zy.
The function f to minimize in the Pierra algorithm is deﬁned as f pZq “ ´xA, Zy.

Zij ď λu and λ “

¯Zij “

Let us denote by S` the set of symmetric positive semi-deﬁnite matrices in Rnˆn. The set C is the

ř
n
i,j“1

ř

intersection of the sets

(cid:32)

S1 “ S` S2 “
#

Z P Rnˆn | Z ě 0

(

(cid:32)

S3 “

+

Z P Rnˆn | diagpZq ĺ I

(

and S4 “

Z P Rnˆn |

Zij ď λ

nÿ

i,j“1

We now compute for all B “ pBjq4

j“1 P pRnˆnq4 and j “ 1, . . . , 4 (J “ 4 here)

ProxIS1ˆ¨¨¨ˆS4 ` 1

2 εF pBqj “ ProxISj ` 1

2J εf pBjq

We have for J “ 4

ProxISj ` 1

2J εf pBjq “ argminZPSj ´

(cid:15)
2J

xA, Zy `

1
2

´

}Z ´ Bj}2

F “ PSj

Bj `

¯

A

(cid:15)
2J

On the other hand, the projections operators PSj , j “ 1, 2, 3, 4 are given by

PS1pZ1q “ U max tΣ, 0u U J where Z1 has SVD Z1 “ UΣUJ
PS2pZ2q “ max tZ2, 0u PS3pZ3q “ Z3 ´ diagpZ3q ` min t1, diagpZ3qu

PS4pZ4q “

λř

ij pZ4qij

Z4.

To sum up, Pierra’s method can be formulated as follows.

For all iterations k in N, compute the SVD of Bk

1 ` (cid:15)

2¨4 A “ U kΣkpU kqJ. Then compute for all j “ 1, . . . , 4

˜

1
4

Bk`1

j “

!

)

!

U k max

Σk, 0

pU kqJ ` max

Bk

2 `

)

A, 0

` Bk

3 `

(cid:15)
2 ¨ 4

(cid:15)
2 ¨ 4

A ´ diagpBk
¸

3 `

(cid:15)
2 ¨ 4

Aq

` min

!
1, diagpBk

3 `

)

Aq

`

(cid:15)
2 ¨ 4

ř

λ
4 ` (cid:15)

ij pBk

2¨4 Aqij

Bk

4 `

(cid:15)
2 ¨ 4

A

.

9.1.2 Application to signed clustering

Let us now turn to the signed clustering problem. We will estimate its membership matrix ¯Z via the
following SDP estimator ˆZ P argmaxZPC xA, Zy, where C “ tZ P Rnˆn : Z ľ 0, Zij P r0, 1s, Zii “ 1, i “
1, . . . , nu. As in the community detection case the function f to minimize in the Pierra algorithm is deﬁned
as f pZq “ ´xA, Zy.

Let us denote by S` the set of symmetric positive semi-deﬁnite matrices in Rnˆn. The set C is the inter-
section of the sets S1 “ S`, S2 “ tZ P Rnˆn | Z P r0, 1snˆnu and S3 “ tZ P Rnˆn | Zii “ 1, i “ 1, . . . , nu .

29

As before, for j “ 1, . . . , 3

ProxISj ` 1

2¨3 εf pBjq “ PSj

Bj `

´

¯

A

(cid:15)
2 ¨ 3

and the projection operators PSj , j “ 1, 2 are given by

PS1pZ1q “ U max tΣ, 0u UJ, PS2pZ2q “ min tmax tZ2, 0u , 1u and PS3pZ3q “ Z3 ´ diagpZ3q ` I

To sum up, Pierra’s method can be formulated as follows.

At each iteration k, compute the SVD of Bk

1 ` (cid:15)

2¨3 A “ U kΣkpU kqJ. Then compute for all j “ 1, . . . , 3

˜

!

)

!

!

U k max

Σk, 0

U kt

` min

max

Bk

2 `

Bk`1

j “

1
3

(cid:15)
2 ¨ 3

)

)

A, 0

, 1

` Bk

3 `

(cid:15)
2 ¨ 3

´

A ´ diag

Bk

3 `

(cid:15)
2 ¨ 3

¸

¯

A

` I

.

9.2 The Burer-Monteiro approach and the Manopt Solver

To solve the MAX-CUT and Angular Synchronization problems we rely on Manopt, a freely available
Matlab toolbox for optimization on manifolds [17]. Manopt runs the Riemannian Trust-Region method
on corresponding Burer-Monteiro non-convex problem with rank bounded by p as follows. The Burer-
Monteiro approach consists of replacing the optimization of a linear function (cid:10)A, Z(cid:11) over the convex set
Z “ tZ ľ 0 : ApZq “ bu with the optimization of the quadratic function (cid:10)AY, Y (cid:11) over the non-convex set
Y “ tY P Rnˆp : ApY Y T q “ bu.

In the context of the MAX-CUT problem, the Burer-Monteiro approach amounts to the following
steps. Denoting by Z the positive semideﬁnite matrix Z “ zzT , note that both the cost function and the
constraints lend themselves to be expressed linearly in terms of Z. Dropping the NP-hard rank-1 constraint
on Z, we arrive at the well-known convex relaxation of MAX-CUT from [49]

ˆZ P argmin

(cid:10)A, Z(cid:11)

ZPC

(9.1)

where C :“ tZ P Rnˆn : Z ľ 0, Zii “ 1, @i “ 1, . . . , nu.

If a solution ˆZ of this SDP has rank 1, then ˆZ “ z˚z˚T for some z˚, which then gives the optimal
cut. Recall that in the general case of higher rank ˆZ, Goemans and Williamson [49] introduced the
celebrated rounding scheme that extracts approximately optimal cuts within a ratio of 0.878 from ˆZ. The
corresponding Burer-Monteiro non-convex problem with rank bounded by p is given by

ˆY P argmin

(cid:10)AY, Y (cid:11)

XPB

(9.2)

where B :“ tY P Rnˆp : diagpY Y T q “ 1u. Note that the constraint diagpY Y T q “ 1 requires each row of Y
to have unit (cid:96)p
in Rp,
which is a smooth manifold. Also note that the search space of the SDP is compact, since all Z feasible
for the SDP have identical trace equal to n.

2 norm, rendering Y to be a point on the Cartesian product of n unit spheres S p´1

2

If the convex set Z is compact, and m denotes the number of constraints, it holds true that whenever
p satisﬁes ppp`1q
2 ě m, the two problems share the same global optimum [12, 22]. Building on pioneering
work of Burer and Monteiro [22], Boumal et. al [18] showed that if the set Z is compact and the set Y
is a smooth manifold, then ppp`1q
2 ě m implies that, for almost all cost matrices A, global optimality is
2ns,
achieved by any Y satisfying a second-order necessary optimality conditions. Following [18], for p “ r
for almost all matrices A, even though (9.2) is non-convex, any local optimum Y is a global optimum (and
so is Z “ Y Y T ), and all saddle points have an escape (the Hessian has a negative eigenvalues). Note that
for p ą n{2 the same statement holds true for all A, and was previously established by [19].

?

30

10 Numerical experiments

This section contains the outcome of numerical experiments on the three application problems considered:
signed clustering, MAX-CUT, and angular synchronization.

10.1 Signed Clustering

To assess the eﬀectiveness of the SDP relaxation, we consider the following experimental setup. We
generate synthetic networks following the signed stochastic block model (SSBM) previously described in
Section 4.1.1, with K “ 5 communities. To quantify the eﬀectiveness of the SDP relaxation, we compare
the accuracy of a suite of algorithms from the signed clustering literature, before (that is when we perform
these algorithms directly on A) and after the SDP relaxation (that is when we perform the very same
algorithms on ˆZ). To measure the recovery quality of the clustering results, for a given indicator set
x1, . . . , xK, we rely on the error rate consider in [24], deﬁned as

γ “

Kÿ

c“1

xT
c A´

comxc ` xT
n2

c L`

comxc

,

(10.1)

com, and L`

com denotes the combinatorial graph Laplacian corresponding to A´

where xc denotes a cluster indicator vector, Acom (“ EA) is the complete K-weakly balanced ground truth
network – with 1’s on the diagonal blocks corresponding to inter-cluster edges, and ´1 otherwise – with
com ´ A´
Acom “ A`
com. Note
that xT
c A´
comxc counts the number of violations within the clusters (since negative edges should not be
placed within clusters) and xT
comxc counts the number of violations across clusters (since positive edges
should not belong to the cut). Overall, (10.1) essentially counts the fraction of intra-cluster and inter-
cluster edge violations, with respect to the full ground truth matrix. Note that this deﬁnition can also be
easily adjusted to work on real data sets, where the ground truth matrix Acom is not available, which one
can replace with the empirical observation A.

c L`

In terms of the signed clustering algorithms compared, we consider the following algorithms from the
literature. One straightforward approach is to simply rely on the spectrum of the observed adjacency
matrix A. Kunegis et al.
[65] proposed spectral tools for clustering, link prediction, and visualization of
signed graphs, by solving a 2-way “signed” ratio-cut problem based on the combinatorial Signed Laplacian
ř
[59] ¯L “ ¯D ´ A, where ¯D is a diagonal matrix with ¯Dii “
n
i“1 |Aij|. The same authors proposed signed
extensions for the case of the random-walk Laplacian ¯Lrw “ I ´ ¯D´1A, and the symmetric graph Laplacian
¯Lsym “ I´ ¯D´1{2A ¯D´1{2, the latter of which is particularly suitable for skewed degree distributions. Finally,
the last algorithm we considered is BNC of Chiang et al. [25], who introduced a formulation based on the
Balanced Normalized Cut objective

mintx1,...,xK uPI

˜

Kÿ

c“1

xT
c pD` ´ Aqxc
¯Dxc

xT
c

¸

.

(10.2)

which, in light of the decomposition D` ´ A “ D` ´ pA` ´ A´q “ D` ´ A` ` A´ “ L` ` A´, is eﬀectively
minimizing the number of violations in the clustering procedure.

In our experiments, we ﬁrst compute the error rate γbef ore of all algorithms on the original SSBM
graph (shown in Column 1 of Figure 1), and then we repeat the procedure but with the input to all signed
clustering algorithms being given by the output of the SDP relaxation, and denote the resulting recovery
error by γaf ter. The third column of the same Figure 1 shows the diﬀerence in errors γδ “ γbef ore ´ γaf ter
between the ﬁrst and second columns, while the fourth column contains a histogram of the error diﬀerences
γδ. This altogether illustrates the fact that the SDP relaxation does improve the performance of all signed
clustering algorithms, except ¯L, and could eﬀectively be used as a denoising pre-processing step.

31

Before

After

Delta

Histogram

A

¯L

¯Lrw

¯Lsym

BNC

Figure 1: Summary of results for the Signed Clustering problem. The ﬁrst column denotes the recovery
error before the SDP relaxation step, meaning that we consider a number of signed clustering algorithms
from the literature which we apply directly the initial adjacency matrix A. The second column contains
the results when applying the same suite of algorithms after the SDP relaxation. The third column
shows the diﬀerence in errors between the ﬁrst and second columns, while the fourth column contains a
histogram of the delta errors. This altogether illustrates the fact the SDP relaxation does improve the
performance of all signed clustering algorithms except ¯L.

32

10.2 Max-Cut

For the MAX-CUT problem, we consider two sets of numerical experiments. First, we consider a version
of the stochastic block model which essentially perturbs a complete bipartite graph

B “

(cid:12)
(cid:12)
(cid:12)
(cid:12)

0n1ˆn1 1n1ˆn2
1n2ˆn1 0n2ˆn2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

(10.3)

where 1n1ˆn2 (respectively, 0n1ˆn2) denotes an n1 ˆ n2 matrix of all ones, respectively, all zeros. In our
experiments, we set n1 “ n2 “ n
2 , and ﬁx n “ 500. We perturb B by deleting edges across the two
partitions, and inserting edges within each partition. More speciﬁcally, we generated the full adjacency
matrix A0 from B by adding edges independently with probability η within each partition (i.e., along the
diagonal blocks in (10.3)). Finally, we denote by A the masked version we observe, A “ A0 ˝ S, where S
denotes the adjacency matrix of an Erd˝os-R´enyi(n, δ) graph. The graph shown in Figure 2 is an instance
of the above generative model. Note that for small values of η we expect the maximum cut to occur across

Figure 2: Illustration of Max-Cut in the setting of a perturbation of a complete bipartite graph.

the initial partition PB in the clean bipartite graph B, which we aim to recover as we sparsify the observed
graph A. The heatmap in the left of Figure 3 shows the Adjusted Rand Index (ARI) between the initial
partition PB and the partition of the Max-Cut SDP relaxation in (9.1), as we vary the noise parameter η
and the sparsity δ. As expected, for a ﬁx level of noise η, we are able to recover the hypothetically optimal
Max-Cut, for suitable levels of the sparsity parameter. The heatmap in the right of Figure 3 shows the
computational running time, as we vary the two parameter, showing that the Manopt solver takes the
longest to solve dense noisy problems, as one would expect.

In the second set of experiments shown in Figure 4, we consider a graph A0 chosen at random from the
collection3 of graphs known in the literature as the Gset, where we vary the sparsity level δ, and show
the Max-Cut value attained on the original full graph A0, but using the Max-Cut partition computed by
the SDP relaxation (9.1) on the sparsiﬁed graph A.

10.3 Angular Synchronization

For the angular synchronization problem, we consider the following experimental setup, by assessing the
quality of the recovered angular solution from the SDP relaxation, as we vary the two parameters of
interest. In the x-axis in the plots from Figures 5 and 6 we vary the noise level σ, under two diﬀerent noise
models, Gaussian and outliers. On the y-axis, we vary the sparsity of the sampling graph.

We measure the quality of the recovered angles via the Mean Squared Error (MSE), deﬁned as follows.
Since a solution can only be recovered up to a global shift, one needs an MSE error that mods out such

3http://web.stanford.edu/~yyye/yyye/Gset/

33

(a) Adjusted Rand Index.

(b) Running times (MANOPT).

Figure 3: Numerical results for MAX-CUT on a perturbed complete bipartite graph, as we vary the noise
level η and the sampling sparsity δ. Results are averaged over 20 runs.

Figure 4: Max-Cut results for the G53 benchmark graph (from the Gset collection) with n “ 1000 nodes
and average degree « 12. Results are averaged over 20 runs.

a degree of freedom. The following MSE is also more broadly applicable for the case when the underlying
group is the orthogonal group Opdq, as opposed to SOp2q, as in the present work, where one can replace the
unknown angles θ1, . . . , θn with their respective representation as 2 ˆ 2 rotation matrices h1, . . . , hn P Op2q.
To that end, we look for an optimal orthogonal transformation ˆO P Op2q that minimizes the sum of squared
distances between the estimated orthogonal transformations and the ground truth measurements

ˆO “ argmin
OPOp2q

nÿ

}hi ´ Oˆhi}2
F ,

(10.4)

i“1
where ˆh1, . . . , ˆhn P Op2q denote the 2 ˆ 2 rotation matrix representation of the estimated angles ˆθ1, . . . , ˆθn.
In other words, ˆO is the optimal solution to the alignment problem between two sets of orthogonal trans-
formations, in the least-squares sense. Following the analysis of [91], and making use of properties of the
trace, one arrives at

nÿ

i“1

}hi ´ Oˆhi}2

F “

“

nÿ

i“1
nÿ

i“1

If we let Q denote the 2 ˆ 2 matrix

„´

¯ ´

Trace

hi ´ Oˆhi

hi ´ Oˆhi



¯
T

”
2I ´ 2OˆhihT
i

ı

Trace

«

“ 4n ´ 2 Trace

O

ﬀ

ˆhihT
i

.

nÿ

i“1

ˆhihT
i ,

Q “

1
n

nÿ

i“1

34

(10.5)

(10.6)

it follows from (10.5) that the MSE is given by minimizing

1
n

nÿ

i“1

}hi ´ Oˆhi}2

F “ 4 ´ 2T rpOQq.

(10.7)

In [7] it is proven that T rpOQq ď T rpV U T Qq, for all O P Op3q, where Q “ U ΣV T is the singular value
decomposition of Q. Therefore, the MSE is minimized by the orthogonal matrix ˆO “ V U T and is given by

MSE def“

1
n

nÿ

i“1

}hi ´ ˆOˆhi}2

F “ 4 ´ 2 TracepV U T U ΣV T q “ 4 ´ 2pσ1 ` σ2q,

(10.8)

where σ1, σ2 are the singular values of Q. Therefore, whenever Q is an orthogonal matrix for which
σ1 “ σ2 “ 1, the MSE vanishes. Indeed, the numerical experiments (on a log scale) in Figures 5 and 6
conﬁrm that for noiseless data, the MSE is very close to zero. Furthermore, as one would expected, under
favorable noise regimes and sparsity levels, we have almost perfect recovery, both by the SDP and the
spectral relaxations, under both noise models.

(a) Spectral relaxation.

(b) SDP relaxation (solved via MANOPT).

Figure 5: Recovery rates (MSE (10.8) - the lower the better) for angular synchronization with n “ 500,
under the Gaussian noise model, as we vary the noise level σ and the sparsity p of the measurement graph.
Averaged over 20 runs.

(a) Spectral relaxation.

(b) SDP relaxation (solved via MANOPT).

Figure 6: Recovery rates (MSE (10.8) - the lower the better) for angular synchronization with n “ 500,
under the Outlier noise model, as we vary the noise level γ and the sparsity p of the measurement graph.
Averaged over 20 runs.

35

11 Conclusions and future work

There are a number of other graph-based problems amenable to SDP relaxations, for which a similar
theoretical analysis of their SDP-based estimators could be suitable. For example, the recent work of
[34] considered a problem motivated by geosciences and engineering applications of recovering a smooth
unknown function f : G Ñ R (where G “ ra, bs is known) from noisy observations of its mod 1 values,
which is also amenable to a solution based on an SDP relaxation solved via a Burer-Monteiro approach.
Another potential application concerns the problem of clustering directed graphs, as in the very recent work
of [33] that proposed a spectral algorithm based on Hermitian matrices; this problem is also amenable to
an SDP relaxation.

Our theoretical and practical ﬁndings show that running algorithms (such as spectral methods) directly
on A may be improved by using ﬁrst a SDP estimator such as ˆZ and running the very same algorithms
on ˆZ (instead of A). Somehow, ˆZ performs a pre-processing de-noising step which improve the recovery
of the hidden signal such as community vectors.

Acknowledgements

Mihai Cucuringu acknowledges support from the EPSRC grant EP/N510129/1 at The Alan Turing Insti-
tute. Guillaume Lecu´e acknowledges support from a grant of the French National Research Agency (ANR),
“Investissements d’Avenir” (LabEx Ecodec/ANR-11-LABX-0047).

References

[1] Epinions data set. https://snap.stanford.edu/data/soc-Epinions1.html. Accessed: 2010-09-30.

[2] Slashdot data set. https://snap.stanford.edu/data/soc-Slashdot0902.html. Accessed: 2010-09-30.

[3] Emmanuel Abbe, Afonso S Bandeira, and Georgina Hall. Exact recovery in the stochastic block model. IEEE Transac-

tions on Information Theory, 62(1):471–487, 2015.

[4] Saeed Aghabozorgi, Ali Seyed Shirkhorshidi, and Teh Ying Wah. Time-series clustering–a decade review. Information

Systems, 53:16–38, 2015.

[5] Brendan PW Ames. Guaranteed clustering and biclustering via semideﬁnite programming. Mathematical Programming,

147(1-2):429–465, 2014.

[6] Miguel F Anjos and Jean B Lasserre. Handbook on semideﬁnite, conic and polynomial optimization, volume 166. Springer

Science & Business Media, 2011.

[7] K. Arun, T. Huang, and S. Bolstein. Least-squares ﬁtting of two 3-D point sets. IEEE Transactions on Pattern Analysis

and Machine Intelligence, 9(5):698–700, 1987.

[8] Afonso S Bandeira, Moses Charikar, Amit Singer, and Andy Zhu. Multireference alignment using semideﬁnite pro-
gramming. In Proceedings of the 5th conference on Innovations in theoretical computer science, pages 459–470. ACM,
2014.

[9] Afonso S Bandeira, Ramon Van Handel, et al. Sharp nonasymptotic bounds on the norm of random matrices with

independent entries. The Annals of Probability, 44(4):2479–2506, 2016.

[10] Sujogya Banerjee, Kaushik Sarkar, Sedat Gokalp, Arunabha Sen, and Hasan Davulcu. Partitioning signed bipartite
graphs for classiﬁcation of individuals and organizations. In International Conference on Social Computing, Behavioral-
Cultural Modeling, and Prediction, pages 196–204. Springer, 2012.

[11] Peter L. Bartlett and Shahar Mendelson. Empirical minimization. Probab. Theory Related Fields, 135(3):311–334, 2006.

[12] A. I. Barvinok. Problems of distance geometry and convex properties of quadratic maps. Discrete & Computational

Geometry, 13(2):189–202, Mar 1995.

[13] Lucien Birg´e and Pascal Massart. Rates of convergence for minimum contrast estimators. Probab. Theory Related Fields,

97(1-2):113–150, 1993.

[14] Grigoriy Blekherman, Pablo A Parrilo, and Rekha R Thomas. Semideﬁnite optimization and convex algebraic geometry.

SIAM, 2012.

[15] Vincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefebvre. Fast unfolding of communities in

large networks. Journal of statistical mechanics: theory and experiment, 2008(10):P10008, 2008.

36

[16] St´ephane Boucheron, G´abor Lugosi, and Pascal Massart. Concentration Inequalities: A Nonasymptotic Theory of

Independence. Oxford University Press, 2013. ISBN 978-0-19-953525-5.

[17] N. Boumal, B. Mishra, P.-A. Absil, and R. Sepulchre. Manopt, a Matlab toolbox for optimization on manifolds. Journal

of Machine Learning Research, 15:1455–1459, 2014.

[18] N. Boumal, V. Voroninski, and A.S. Bandeira. The non-convex Burer-Monteiro approach works on smooth semideﬁnite

programs. In Advances in Neural Information Processing Systems 29, pages 2757–2765. 2016.

[19] Nicolas Boumal. A riemannian low-rank method for optimization over semideﬁnite matrices with block-diagonal con-

straints. arXiv preprint arXiv:1506.00575, 2015.

[20] Stephen Boyd, Laurent El Ghaoui, Eric Feron, and Venkataramanan Balakrishnan. Linear matrix inequalities in system

and control theory, volume 15. Siam, 1994.

[21] Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.

[22] S. Burer and R.D.C. Monteiro. Local minima and convergence in low-rank semideﬁnite programming. Mathematical

Programming, 103(3):427–444, 2005.

[23] Yudong Chen and Jiaming Xu. Statistical-computational tradeoﬀs in planted problems and submatrix localization with
a growing number of clusters and submatrices. The Journal of Machine Learning Research, 17(1):882–938, 2016.

[24] K. Chiang, J. Whang, and I. Dhillon. Scalable clustering of signed networks using Balance Normalized Cut. CIKM,

2012.

[25] Kai-Yang Chiang, Joyce Whang, and Inderjit S. Dhillon. Scalable Clustering of Signed Networks using Balance Nor-

malized Cut. In ACM Conference on Information and Knowledge Management (CIKM), oct 2012.

[26] Geoﬀrey Chinot, Lecu´e Guillaume, and Lerasle Matthieu. Statistical learning with lipschitz and convex loss functions.

arXiv preprint arXiv:1810.01090, 2018.

[27] St´ephane Chr´etien and Franck Corset. Using the eigenvalue relaxation for binary least-squares estimation problems.

Signal Processing, 89(11):2079–2091, 2009.

[28] St´ephane Chr´etien, Cl´ement Dombry, and Adrien Faivre. A semi-deﬁnite programming approach to low dimensional

embedding for unsupervised clustering. Frontiers in Applied Mathematics and Statistics, to appear.

[29] Aaron Clauset, Mark EJ Newman, and Cristopher Moore. Finding community structure in very large networks. Physical

review E, 70(6):066111, 2004.

[30] M. Cucuringu. Synchronization over Z2 and community detection in multiplex networks with constraints. Journal of

Complex Networks, 3:469–506, 2015.

[31] M. Cucuringu. Sync-Rank: Robust Ranking, Constrained Ranking and Rank Aggregation via Eigenvector and Semidef-
inite Programming Synchronization. IEEE Transactions on Network Science and Engineering, 3(1):58–79, 2016.

[32] M. Cucuringu, P. Davies, A. Glielmo, and H. Tyagi. SPONGE: A generalized eigenproblem for clustering signed

networks. AISTATS 2019.

[33] M. Cucuringu, H. Li, H. Sun, and L. Zanetti. Hermitian matrices for clustering directed graphs: insights and applications.

to appear in AISTATS 2020.

[34] M. Cucuringu and H. Tyagi. Provably robust estimation of modulo 1 samples of a smooth function with applications

to phase unwrapping. to appear in Journal of Machine Learning Research (JMLR), 2020.

[35] Mark A Davenport and Justin Romberg. An overview of low-rank matrix recovery from incomplete observations. IEEE

Journal of Selected Topics in Signal Processing, 10(4):608–622, 2016.

[36] Chandler Davis and W. M. Kahan. The rotation of eigenvectors by a perturbation. iii. SIAM Journal on Numerical

Analysis, 7(1):1–46, 1970.

[37] J. A. Davis. Clustering and structural balance in graphs. Human Relations, 20(2):181–187, 1967.

[38] Yohann De Castro, Fabrice Gamboa, Didier Henrion, Roxana Hess, Jean-Bernard Lasserre, et al. Approximate optimal

designs for multivariate polynomial regression. The Annals of Statistics, 47(1):127–155, 2019.

[39] Yohann de Castro, Fabrice Gamboa, Didier Henrion, and Jean B. Lasserre. Exact solutions to super resolution on

semi-algebraic domains in higher dimensions. IEEE Trans. Information Theory, 63(1):621–630, 2017.

[40] Laurent Demanet and Paul Hand. Stable optimizationless recovery from phaseless linear measurements. Journal of

Fourier Analysis and Applications, 20(1):199–221, 2014.

[41] Yingjie Fei and Yudong Chen. Exponential error rates of SDP for block models: beyond Grothendieck’s inequality.

IEEE Trans. Inform. Theory, 65(1):551–571, 2019.

[42] Roger Fletcher. A nonlinear programming problem in statistics (educational testing). SIAM Journal on Scientiﬁc and

Statistical Computing, 2(3):257–267, 1981.

[43] Santo Fortunato. Community detection in graphs. Physics reports, 486(3-5):75–174, 2010.

37

[44] Bernd G¨artner and Jiˇr´ı Matouˇsek. Semideﬁnite programming. In Approximation Algorithms and Semideﬁnite Program-

ming, pages 15–25. Springer, 2012.

[45] Jean Charles Gilbert and C´edric Josz. Plea for a semideﬁnite optimization solver in complex numbers. 2017.

[46] Christophe Giraud and Nicolas Verzelen. Partial recovery bounds for clustering with the relaxed k means. arXiv preprint

arXiv:1807.07547, 2018.

[47] Michel X Goemans. Semideﬁnite programming in combinatorial optimization. Mathematical Programming, 79(1-3):143–

161, 1997.

[48] Michel X Goemans and David P Williamson. New 34-approximation algorithms for the maximum satisﬁability problem.

SIAM Journal on Discrete Mathematics, 7(4):656–666, 1994.

[49] Michel X Goemans and David P Williamson. Improved approximation algorithms for maximum cut and satisﬁability

problems using semideﬁnite programming. Journal of the ACM (JACM), 42(6):1115–1145, 1995.

[50] David Gross, Yi-Kai Liu, Steven T Flammia, Stephen Becker, and Jens Eisert. Quantum state tomography via com-

pressed sensing. Physical review letters, 105(15):150401, 2010.

[51] A. Grothendieck. R´esum´e de la th´eorie m´etrique des produits tensoriels topologiques. Bol. Soc. Mat. S˜ao Paulo, 8:1–79,

1953.

[52] Alexandre Grothendieck. R´esum´e de la th´eorie m´etrique des produits tensoriels topologiques. Soc. de Matem´atica de

S˜ao Paulo, 1956.

[53] Olivier Gu´edon and Roman Vershynin. Community detection in sparse networks via Grothendieck’s inequality. Probab.

Theory Related Fields, 165(3-4):1025–1049, 2016.

[54] Olivier Gu´edon and Roman Vershynin. Community detection in sparse networks via grothendieck’s inequality. Probability

Theory and Related Fields, 165(3-4):1025–1049, 2016.

[55] Bruce Hajek, Yihong Wu, and Jiaming Xu. Achieving exact cluster recovery threshold via semideﬁnite programming.

IEEE Transactions on Information Theory, 62(5):2788–2797, 2016.

[56] Simai He, Zhi-Quan Luo, Jiawang Nie, and Shuzhong Zhang. Semideﬁnite relaxation bounds for indeﬁnite homogeneous

quadratic optimization. SIAM Journal on Optimization, 19(2):503–523, 2008.

[57] Chinmay Hegde, Aswin C Sankaranarayanan, and Richard G Baraniuk. Near-isometric linear embeddings of manifolds.

In 2012 IEEE Statistical Signal Processing Workshop (SSP), pages 728–731. IEEE, 2012.

[58] Samuel B. Hopkins. Sub-gaussian mean estimation in polynomial time. CoRR, abs/1809.07425, 2018.

[59] Jao Ping Hou. Bounds for the least Laplacian eigenvalue of a signed graph. Acta Mathematica Sinica, 21(4):955–960,

2005.

[60] Adel Javanmard, Andrea Montanari, and Federico Ricci-Tersenghi. Phase transitions in semideﬁnite relaxations. Pro-

ceedings of the National Academy of Sciences, 113(16):E2218–E2223, 2016.

[61] David Karger, Rajeev Motwani, and Madhu Sudan. Approximate graph coloring by semideﬁnite programming. Journal

of the ACM (JACM), 45(2):246–265, 1998.

[62] Subhash Khot and Assaf Naor. Approximate kernel clustering. Mathematika, 55(1-2):129–165, 2009.

[63] Vladimir Koltchinskii. Local Rademacher complexities and oracle inequalities in risk minimization. Ann. Statist.,

34(6):2593–2656, 2006.

[64] Vladimir Koltchinskii. Oracle inequalities in empirical risk minimization and sparse recovery problems, volume 2033 of
Lecture Notes in Mathematics. Springer, Heidelberg, 2011. Lectures from the 38th Probability Summer School held in
Saint-Flour, 2008, ´Ecole d’´Et´e de Probabilit´es de Saint-Flour. [Saint-Flour Probability Summer School].

[65] J´erˆome Kunegis, Stephan Schmidt, Andreas Lommatzsch, J¨urgen Lerner, Ernesto William De Luca, and Sahin Albayrak.

Spectral analysis of signed graphs for clustering, prediction and visualization. SDM, 10:559–570, 2010.

[66] Jean Bernard Lasserre. An Introduction to Polynomial and Semi-Algebraic Optimization. Number 52. Cambridge

University Press, 2015.

[67] Javad Lavaei and Steven H Low. Zero duality gap in optimal power ﬂow problem. IEEE Transactions on Power Systems,

27(1):92–107, 2011.

[68] Can M. Le, Elizaveta Levina, and Roman Vershynin. Optimization via low-rank approximation for community detection

in networks. Ann. Statist., 44(1):373–400, 2016.

[69] Guillaume Lecu´e and Shahar Mendelson. Learning subgaussian classes: Upper and minimax bounds. Technical report,

CNRS, Ecole polytechnique and Technion, 2013.

[70] Claude Lemar´echal, Arkadii Nemirovskii, and Yurii Nesterov. New variants of bundle methods. Mathematical program-

ming, 69(1-3):111–147, 1995.

[71] J. Leskovec, D. Huttenlocher, and J. Kleinberg. Predicting positive and negative links in online social networks. In

WWW, pages 641–650, 2010.

38

[72] Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg. Signed Networks in Social Media. In CHI, pages 1361–1370,

2010.

[73] L´aszl´o Lov´asz. On the shannon capacity of a graph. IEEE Transactions on Information theory, 25(1):1–7, 1979.

[74] Wing-Kin Ken Ma. Semideﬁnite relaxation of quadratic optimization problems and applications. IEEE Signal Processing

Magazine, 1053(5888/10), 2010.

[75] Enno Mammen and Alexandre B. Tsybakov. Smooth discrimination analysis. Ann. Statist., 27(6):1808–1829, 1999.

[76] Pascal Massart. Concentration inequalities and model selection, volume 1896 of Lecture Notes in Mathematics. Springer,
Berlin, 2007. Lectures from the 33rd Summer School on Probability Theory held in Saint-Flour, July 6–23, 2003, With
a foreword by Jean Picard.

[77] David A Mazziotti. Large-scale semideﬁnite programming for many-electron quantum mechanics. Physical review letters,

106(8):083001, 2011.

[78] Elchanan Mossel, Joe Neeman, and Allan Sly. Reconstruction and estimation in the planted partition model. Probab.

Theory Related Fields, 162(3-4):431–461, 2015.

[79] Y Nesterov. Semideﬁnite relaxation and non-convex quadratic optimization. Optimization Methods and Software, 12:1–

20, 1997.

[80] Mark EJ Newman. Finding community structure in networks using the eigenvectors of matrices. Physical review E,

74(3):036104, 2006.

[81] Carl Olsson, Anders P Eriksson, and Fredrik Kahl. Solving large scale binary quadratic problems: Spectral methods vs.
semideﬁnite programming. In 2007 IEEE Conference on Computer Vision and Pattern Recognition, pages 1–8. IEEE,
2007.

[82] Jiming Peng and Yu Wei. Approximating k-means-type clustering via semideﬁnite programming. SIAM journal on

optimization, 18(1):186–205, 2007.

[83] Jiming Peng and Yu Xia. A new theoretical framework for k-means-type clustering. In Foundations and advances in

data mining, pages 79–96. Springer, 2005.

[84] Guy Pierra. Decomposition through formalization in a product space. Mathematical Programming, 28(1):96–115, 1984.

[85] Gilles Pisier. Grothendieck’s theorem, past and present. Bulletin of the American Mathematical Society, 49(2):237–323,

2012.

[86] Mason A Porter, Jukka-Pekka Onnela, and Peter J Mucha. Communities in networks. Notices of the AMS, 56(9):1082–

1097, 2009.

[87] Martin Royer. Adaptive clustering through semideﬁnite programming. In Advances in Neural Information Processing

Systems, pages 1795–1803, 2017.

[88] P Scobey and DG Kabe. Vector quadratic programming problems and inequality constrained least squares estimation.

J. Indust. Math. Soc., 28:37–49, 1978.

[89] Alexander Shapiro. Weighted minimum trace factor analysis. Psychometrika, 47(3):243–264, 1982.

[90] A. Singer. Angular synchronization by eigenvectors and semideﬁnite programming. Appl. Comput. Harmon. Anal.,

30(1):20–36, 2011.

[91] A. Singer and Y. Shkolnisky. Three-dimensional structure determination from common lines in Cryo-EM by eigenvectors

and semideﬁnite programming. SIAM Journal on Imaging Sciences, 4(2):543–572, 2011.

[92] Amit Singer. Angular synchronization by eigenvectors and semideﬁnite programming. Applied and computational

harmonic analysis, 30(1):20–36, 2011.

[93] Jun Sun, Stephen Boyd, Lin Xiao, and Persi Diaconis. The fastest mixing markov process on a graph and a connection

to a maximum variance unfolding problem. SIAM review, 48(4):681–699, 2006.

[94] Michael J Todd. Semideﬁnite optimization. Acta Numerica, 10:515–560, 2001.

[95] Alexandre B. Tsybakov. Optimal rate of aggregation. In Computational Learning Theory and Kernel Machines (COLT-

2003), volume 2777 of Lecture Notes in Artiﬁcial Intelligence, pages 303–313. Springer, Heidelberg, 2003.

[96] Alexandre B. Tsybakov. Optimal aggregation of classiﬁers in statistical learning. Ann. Statist., 32(1):135–166, 2004.

[97] Sara A. van de Geer. Applications of empirical process theory, volume 6 of Cambridge Series in Statistical and Proba-

bilistic Mathematics. Cambridge University Press, Cambridge, 2000.

[98] Aad W. van der Vaart and Jon A. Wellner. Weak convergence and empirical processes. Springer Series in Statistics.

Springer-Verlag, New York, 1996. With applications to statistics.

[99] Vladimir N. Vapnik. Statistical learning theory. Adaptive and Learning Systems for Signal Processing, Communications,

and Control. John Wiley & Sons, Inc., New York, 1998. A Wiley-Interscience Publication.

39

[100] Roman Vershynin. High-dimensional probability: An introduction with applications in data science, volume 47. Cam-

bridge University Press, 2018.

[101] V Vu. Singular vectors under random perturbation. Preprint available in arXiv:104.2000, 2010.

[102] Ir`ene Waldspurger, Alexandre d’Aspremont, and St´ephane Mallat. Phase recovery, maxcut and complex semideﬁnite

programming. Mathematical Programming, 149(1-2):47–81, 2015.

[103] Kilian Q Weinberger, Benjamin Packer, and Lawrence K Saul. Nonlinear dimensionality reduction by semideﬁnite

programming and kernel matrix factorization. In AISTATS, volume 2, page 6. Citeseer, 2005.

[104] Kilian Q Weinberger and Lawrence K Saul. An introduction to nonlinear dimensionality reduction by maximum variance

unfolding. In AAAI, volume 6, pages 1683–1686, 2006.

[105] Kilian Q Weinberger and Lawrence K Saul. Unsupervised learning of image manifolds by semideﬁnite programming.

International journal of computer vision, 70(1):77–90, 2006.

[106] Henry Wolkowicz. Semideﬁnite and lagrangian relaxations for hard combinatorial problems. In IFIP Conference on

System Modeling and Optimization, pages 269–309. Springer, 1999.

[107] Henry Wolkowicz, Romesh Saigal, and Lieven Vandenberghe. Handbook of semideﬁnite programming: theory, algorithms,

and applications, volume 27. Springer Science & Business Media, 2012.

[108] Y. Yu, T. Wang, and R. J. Samworth. A useful variant of the Davis–Kahan theorem for statisticians. Biometrika,

102(2):315–323, 2015.

[109] S. Zhang and Y. Huang. Complex quadratic optimization and semideﬁnite programming. SIAM Journal on Optimization,

16(3):871–890, 2006.

[110] Shuzhong Zhang. Quadratic maximization and semideﬁnite relaxation. Mathematical Programming, 87(3):453–465,

2000.

40

