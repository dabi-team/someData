Self-Triggered Markov Decision Processes

Yunhan Huang1 and Quanyan Zhu1

1
2
0
2

b
e
F
7
1

]

Y
S
.
s
s
e
e
[

1
v
1
7
5
8
0
.
2
0
1
2
:
v
i
X
r
a

Abstractâ€” In this paper, we study Markov Decision Processes
(MDPs) with self-triggered strategies, where the idea of self-
triggered control is extended to more generic MDP models.
This extension broadens the application of self-triggering poli-
cies to a broader range of systems. We study the co-design
problems of the control policy and the triggering policy to
optimize two pre-speciï¬ed cost criteria. The ï¬rst cost criterion
is introduced by incorporating a pre-speciï¬ed update penalty
into the traditional MDP cost criteria to reduce the use of
communication resources. Under this criteria, a novel dynamic
programming (DP) equation called DP equation with optimized
lookahead to proposed to solve for the self-triggering policy
under this criteria. The second self-triggering policy is to
maximize the triggering time while still guaranteeing a pre-
speciï¬ed level of sub-optimality. Theoretical underpinnings are
established for the computation and implementation of both
policies. Through a gridworld numerical example, we illustrate
the two policiesâ€™ effectiveness in reducing sources consumption
and demonstrate the trade-offs between resource consumption
and system performance.

I. INTRODUCTION

it

is desirable to limit

Recent advances in information and communication tech-
nologies have led to the implementation of
large-scale
resource-constrained networked control systems. In these
systems,
the sensor and control
communication and computation to instances when a system
needs attention [1]. As a result, the self-triggered control
paradigm is proposed to reduce the utilization of communica-
tion resources and/or actuation movements while still main-
taining desirable closed-loop behavior for these systems [2].
The self-triggered control abandons the conventional periodic
time-triggered implementations. In self-triggered control, the
self-triggering policy consists of two sub-policies: the control
policy and a triggering mechanism that pre-determines, at
an update time, when the control inputs have to be updated
the next time. Due to its efï¬ciency in resource-saving, self-
triggered control has been studied extensively in the last
decades [1]â€“[6].

The study of self-triggered has been conï¬ned to state-
space dynamical models, including either linear models [1],
[2], [4], [5] or nonlinear models [3], [6] in both(either)
continuous-time and(or) discrete-time settings. However, re-
cent developments in technologies such as wireless com-
munication, machine learning, and real-time analytics have
broadened the application of Internet of Things (IoTs) be-
yond control systems to a wide range of areas,
includ-
ing logistics and supply chain [7]â€“[9], smart cities [10],
and wearables [11]. These systems are usually large-scale,

1 Y. Huang and Q. Zhu are with the Department of Electrical and
Computer Engineering, New York University, 370 Jay St., Brooklyn, NY.
{yh.huang, qz494}@nyu.edu

1

equipped with resource-constrained devices, and difï¬cult to
be described by state-space dynamic models. Hence, there
is an urgent need to incorporate the idea of self-triggering
policy in control
into a more general dynamic model:
Markov Decision Processes (MDP). This incorporation can
lead toward a computationally and communicationally more
efï¬cient IoT-enabled system.

This paper studies a discrete-time self-triggered MDP
where the control1 policy and the triggering mecha-
nism/policy are co-designed to achieve certain cost criteria.
The differences between this work and most existing papers
in self-triggered control are three-fold. The ï¬rst is that we
study self-triggered policies for a more generic dynamic
model, i.e., an MDP model, which allows the extension of
the self-triggering policy to a wider range of applications.
Second, we address the co-design problem of jointly design-
ing the control policy and the triggering policy. Existing
self-triggering methods design the control policy and the
triggering policy in an ordered manner, i.e., the control policy
is designed ï¬rst. The triggering policy is then designed sub-
sequently while ensuring certain control performance [1], [4].
For example, in [4], the control gain is pre-set to be the ğ»âˆ
control gain, based on which a triggering policy is designed
to assure a speciï¬ed level of L2 stability. Since the control
policy is given without considering the self-triggering nature
of the whole policy, it is hard to guarantee that the given
control policy is optimal for achieving the minimum number
of updates while maintaining certain cost criteria [2]. Here,
we address a co-design problem to alleviate the concern
regarding the optimality issue. Third, in existing works [1],
the analysis of control performance under the self-triggered
control paradigm is mostly qualitative, e.g., the analysis of
whether a certain type of stability can be achieved. Control
performance is sometimes quantiï¬ed as the decay rate for
the Lyapunov function. Only few self-triggering methods
provide quantitative analysis for control performance such
as L2 gains [4], quadratic costs [12]â€“[14]. More recently,
T. Gommans et al. studies self-triggered linear-quadratic-
gaussian (LQG) control associated with quadratic costs. In
this work, we consider a generic class of cost criteria and
propose self-triggered policies that can guarantee a certain
optimality level.The contributions of this paper are summa-
rized as follows.

1) We study self-triggered MDP, which extends the idea
of self-triggered control into a more generic dynamical
model. The genericness of the MDP model enables the
application of self-triggering policies into a broader

1In this paper, we use control and action interchangeably.

 
 
 
 
 
 
range of systems.

to ï¬nd an optimal stationary Markov policy that minimizes

2) We jointly design the control policy and the triggering
policy that co-optimizes pre-speciï¬ed cost criteria.
3) We propose two frameworks that produce two co-
designed self-triggering policies. The ï¬rst is introduced
by incorporating an update penalty into the traditional
MDP cost criteria to reduce the use of communica-
tion resources. The second is a greedy reduction of
resources used while still guaranteeing any pre-given
level of sub-optimality. Theoretical underpinnings are
established for the computation and implementation of
both policies.

4) Through a gridworld example in both non-windy and
windy settings, we show that the proposed policies
are efï¬cient in reducing communication resources con-
sumed while still maintaining a high level of perfor-
mance.

A. Nomenclature

In this paper, R and N represent the set of real numbers
and natural numbers, respectively. The expectation operator
is denoted by E. And Î”ğ‘¡ âˆˆ N denotes the time steps between
two neighboring updates. The letter ğ‘™ is the index for the
ğ‘™th update and ğ‘¡ğ‘™ is the time instance when the ğ‘™th update
happens. The notation N[ğ‘¡ğ‘™ ,ğ‘¡ğ‘™+1 ] means the intersection of the
two setsN and [ğ‘¡ğ‘™, ğ‘¡ğ‘™+1]. The set of non-negative real numbers
is denoted by R+. The notation A\B denotes the set {ğ‘¥ | ğ‘¥ âˆˆ
A, ğ‘¥ âˆ‰ B}.

II. SELF-TRIGGERED MARKOV DECISION
PROCESS

In this section, we provide the problem formulation for
the self-triggered action strategy. We consider a discrete-time
MDP deï¬ned by a tuple {X, A, ğ‘ƒ, ğ‘}, where X is the state
space, A is the actions space, ğ‘ƒ is the time-homogeneous
transition probability, and ğ‘ is the state-wise cost function.
The state space X and action space A are both assumed to be
Borel subsets of Polish (Banach and separable) spaces. If an
action ğ‘ âˆˆ A is selected at a state ğ‘¥ âˆˆ X, then a cost ğ‘(ğ‘¥, ğ‘)
is incurred, where without loss of generality, we suppose
ğ‘ : X Ã— A â†’ R+. The function ğ‘ is assumed to be bounded
and Borel measurable. The transition probability ğ‘ƒ(ğµ|ğ‘¥, ğ‘)
is a Borel function on X Ã— A for each Borel subset ğµ of X,
and ğ‘ƒ(Â·|ğ‘¥, ğ‘) is a probability measure on the Borel ğœ-ï¬eld
of X for each (ğ‘¥, ğ‘) âˆˆ X Ã— A.

In classic MDP, the decision process proceeds as follows:
at time ğ‘¡ = 0, 1, Â· Â· Â· , the current state of the system, ğ‘¥ğ‘¡ ,
is observed. A decision-maker decides which action, ğ‘ğ‘¡ , to
choose, the cost ğ‘ğ‘¡ = ğ‘(ğ‘¥ğ‘¡ , ğ‘ğ‘¡ ) is incurred, the system moves
to the next state following the rule ğ‘¥ğ‘¡+1 âˆ¼ ğ‘ƒ(Â·|ğ‘¥ğ‘¡ , ğ‘), and the
process continues. The rule that the decision-maker follows
to choose an action is called policy. We consider stationary
Markov policy ğœ™ in which all decisions depend only on the
current state. A stationary Markov policy ğœ™ is deï¬ned by a
measurable mapping ğœ™ : X Ã— A. In classic MDP, the goal is

âˆ

ğ›½ğ‘¡ ğ‘(ğ‘¥ğ‘¡ , ğ‘ğ‘¡ )

ğ‘£ ğœ™ (ğ‘¥) = Eğœ™

"

Ã•ğ‘¡=0

ğ‘¥0 = ğ‘¥

,

#

(1)

where ğ›½ is a discount factor strictly less than 1, and the
expectation is based on the probability distribution on the
set of all trajectory (X Ã— A)âˆ, which is uniquely determined
by the policy ğœ™ and the initial state ğ‘¥ ( [15], pp. 140-141).
Deï¬ne the optimal cost

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ğ‘‰ (ğ‘¥) â‰” inf
ğœ™ âˆˆÎ¦

ğ‘£ ğœ™ (ğ‘¥),

where Î¦ is the set of all stationary policies. A policy ğœ™ is
called optimal if ğ‘£ ğœ™ (ğ‘¥) = ğ‘‰ (ğ‘¥) for all ğ‘¥ âˆˆ X.

A. Self-Triggered Decision Making

In classic MDP, decision making requires persistent trans-
mission of measured state and updates of actions at each
time instance ğ‘¡ âˆˆ N. In this paper, we are interested in
constructing a policy that requires less sensing demand,
lower communication rate, and less actuator movements [16],
while still maintaining certain forms of optimality.

The self-triggering policy is based on holding the current
input value for a controlled duration while still guaranteeing
certain forms of optimality. The self-triggered policy carries
the following structure

ğ‘¡ğ‘™+1 = ğ‘¡ğ‘™ + ğœ(ğ‘¥ğ‘¡ğ‘™ ),
ğ‘ğ‘¡

= ğœ‹(ğ‘¥ğ‘¡ğ‘™ ) âˆˆ A, ğ‘¡ âˆˆ N[ğ‘¡ğ‘™ .ğ‘¡ğ‘™+1) ,

(

(2)

where ğ‘™ is the index for the number of triggers, ğ‘¡0 â‰” 0,
ğœ : X â†’ T , T â‰” {1, 2, Â· Â· Â· , Â¯ğ‘‡ }, Â¯ğ‘‡ âˆˆ N, and ğœ‹ : X â†’ A.
Here, the integer Â¯ğ‘‡ is an arbitrary upper bound on the waiting
time for next update. The self-triggering policy, denoted by
ğœ‡, involves two sub-policies: the timing policy, ğœ(ğ‘¥), that
determines the next time for updating, and the control policy,
ğœ‹(ğ‘¥), that chooses a ï¬xed action to deploy for the next ğœ(ğ‘¥)
time instances. For convenience, we write ğœ‡ = (ğœ, ğœ‹) and
ğœ‡ : X â†’ T Ã— A.

B. Performance Criteria

This paper introduces two different yet related problems
associated with two cost criteria; one is constructed by
incorporating a penalty ğ‘‚ â‰¥ 0 for updating the action into the
classic cost criteria deï¬ned in eq. (1). The idea of introducing
a penalty is originated from costly measurements that have
been investigated in the context of LQG optimal control [14],
[17] and games [18], [19]. The penalty ğ‘‚ â‰¥ 0 is a scalar,
which we refer to as the update penalty. For instance, if ğ‘¡ğ‘™
and ğ‘¡ğ‘™+1 are two neighboring updating time, during the time
interval [ğ‘¡ğ‘™, ğ‘¡ğ‘™+1], the total update penalty is ğ›½ğ‘¡ğ‘™ğ‘‚ + ğ›½ğ‘¡ğ‘™+1ğ‘‚.
Now, we formulate the ï¬rst problem.

Problem 1. Find an optimal self-triggering policy ğœ‡ that
minimizes the following cost criterion over an inï¬nite horizon

ğ‘“ ğœ‡ (ğ‘¥) = Eğœ‡

âˆ

"

Ã•ğ‘¡=0

âˆ

ğ›½ğ‘¡ ğ‘(ğ‘¥ğ‘¡ , ğ‘ğ‘¡ ) +

ğ›½ğ‘¡ğ‘™ğ‘‚

Ã•ğ‘™=1

(3)

,

#

ğ‘¥0 = ğ‘¥
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

2

where the ï¬rst term is the accumulated costs in the classic
MDP, and the second term is the accumulated costs of
updating oneâ€™s action.

The other cost criteria is similar to that of [2]. That
is for a pre-speciï¬ed sub-optimal performance, we aim to
reduce the number of times the input/output is updated, while
maintaining the pre-speciï¬ed sub-optimal performance. Now,
we formulate our second problem as

Problem 2. Find a policy ğœ‡ that maximizes the next trans-
mission time ğœ(ğ‘¥) subject to the performance guarantee that

ğ‘£ ğœ‡ (ğ‘¥) â‰¤ ğ›¼ğ‘‰ (ğ‘¥), for all ğ‘¥ âˆˆ X,

(4)

where ğ›¼ â‰¥ 1 is a scalar.

Remark 1. In Problem 1, we introduce an update penalty ğ‘‚
to capture the trade-off between the degree of optimality and
the usage of sensing/communication resources. The update
penalty can be interpreted as a soft constraint on the number
of updates. In Problem 2, ğ›¼ serves as a scaling factor
that can be selected arbitrarily to balance the consumption
of sensing/communication resources and the degradation
of performance. There is a hard constraint that requires
matainting a certain degree of sub-optimality. When ğ›¼ = 1,
no degradation of performance is allowed. Solving both
problems involves the co-design of the waiting time for next
update (through ğœ) and the chosen action (through ğœ‹).

III. THEORETICAL FRAMEWORKS

In this section, by establishing theoretical underpinnings,
we pave the way for ï¬nding the self-triggering policies that
solve the problems. For Problem 1, we formulate a dynamic
programming (DP) equation, which we call a DP equation
with optimized lookahead. With this equation, we can resort
to several effective methods such as value iterations and
policy iterations to characterize an optimal self-triggering
policy. For Problem 2, we propose a greedy self-triggering
policy that aims to reduce the number of updates and show
that the proposed policy is well-deï¬ned and satisï¬es the
performance guarantee for any pre-speciï¬ed ğ›¼.

A. Dynamic Programming Equation with Optimized Looka-
head

To solve Problem 1,

the DP equation with optimized
lookahead is derived and presented in this sub-section. The
derivation idea is to form consolidated costs, states, and
actions between two update time instances, which generates
a new discrete-time MDP in the classic setting.

Let Â¯ğ‘ğ‘™ represent the consolidated costs that correspond to
the time period between ğ‘™-th update and (ğ‘™ + 1)-th update,
i.e., the time period [ğ‘¡ğ‘™, ğ‘¡ğ‘™+1). From eq. (2) and eq. (3), we
can obtain

Â¯ğ‘ğ‘™ â‰” Â¯ğ‘(ğ‘¥ğ‘¡ğ‘™ , ğ‘ğ‘¡ğ‘™ , Î”ğ‘¡ğ‘™) = E

"

Ã•ğ‘¡=0

Î”ğ‘¡ğ‘™âˆ’1

ğ›½ğ‘¡ ğ‘(ğ‘¥ğ‘¡ğ‘™ +ğ‘¡ , ğ‘ğ‘¡ğ‘™ )

where given a self-triggering policy ğœ‡ = (ğœ‹, ğœ), the ï¬xed
action ğ‘ğ‘¡ğ‘™ is produced by ğœ‹(ğ‘¥ğ‘¡ğ‘™ ) and the waiting time Î”ğ‘¡ğ‘™ is

,

#

ğ‘¥ğ‘¡ğ‘™ , ğ‘ğ‘¡ğ‘™ , Î”ğ‘¡ğ‘™
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

generated by ğœ(ğ‘¥ğ‘¡ğ‘™ ). An application of the Fubiniâ€™s theorem
(principle) and Markov property [20] yields

Î”ğ‘¡âˆ’1

Â¯ğ‘(ğ‘¥, ğ‘, Î”ğ‘¡) =

ğ›½ğ‘¡ E [ğ‘(ğ‘¥ğ‘¡ , ğ‘ğ‘¡ )|ğ‘¥0 = ğ‘¥, ğ‘ğ‘¡ = ğ‘, âˆ€ğ‘¡ < Î”ğ‘¡] .

Ã•ğ‘¡=0

Furthermore, we deï¬ne

Â¯ğ‘ƒ(ğµ|ğ‘¥, ğ‘, Î”ğ‘‡) â‰” Prob (ğ‘¥Î”ğ‘¡ âˆˆ ğµ|ğ‘¥0 = ğ‘¥, ğ‘ğ‘¡ = ğ‘, âˆ€ğ‘¡ < Î”ğ‘¡) ,

(5)
as the skip-probability that the MDP is in Borel subset ğµ of
X, after time Î”ğ‘¡, given that the initial condition is ğ‘¥0 = ğ‘¥
and that the action is ï¬xed until Î”ğ‘¡. The skip-probability
Â¯ğ‘ƒ(ğµ|ğ‘¥, ğ‘, Î”ğ‘‡) is a Borel function on X Ã— A Ã— T for each
Borel subset ğµ of X, which is determined by the one-step
transition probability ğ‘ƒ(Â·|ğ‘¥, ğ‘) deï¬ned in Section II.

With the deï¬nition of the consolidated stage-wise function
Â¯ğ‘ and the tower property of conditional expectation, the
inï¬nite-horizon cost functional in eq. (3) can be re-written
as

âˆ

ğ‘“ ğœ‡ (ğ‘¥) = E

ğ›½ğ‘¡ğ‘™

Â¯ğ‘(ğ‘¥ğ‘¡ğ‘™ , ğœ‡(ğ‘¥ğ‘¡ğ‘™ )) + ğ›½ğœ ( ğ‘¥ğ‘¡ğ‘™ )ğ‘‚)

(cid:16)
Deï¬ne the optimal cost for Problem 1 as

"

Ã•ğ‘™=0

ğ‘¥0 = ğ‘¥

#

. (6)

(cid:17)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ğ‘‰ğ‘ ğ‘¡ (ğ‘¥) â‰” inf
ğœ‡ âˆˆÎ¦ğ‘ ğ‘¡
where Î¦ğ‘ ğ‘¡ is the set of all policies taking the structure of . In
the following theorem, we state the DP equation for ğ‘‰ğ‘ ğ‘¡ (Â·).

ğ‘“ ğœ‡ (ğ‘¥),

(7)

Theorem 1. The value function deï¬ned by eq. (7) satisï¬es
the following dynamic programming equation:

ğ‘‰ğ‘ ğ‘¡ (ğ‘¥) =

inf
ğ‘ âˆˆA,Î”ğ‘¡ âˆˆT

E

"

Î”ğ‘¡âˆ’1

ğ›½ğ‘¡ ğ‘(ğ‘¥ğ‘¡ , ğ‘) + ğ›½Î”ğ‘¡ (ğ‘‰ğ‘ ğ‘¡ (ğ‘¥Î”ğ‘¡ ) + ğ‘‚)

Ã•ğ‘¡=0
ğ‘¥0 = ğ‘¥, ğ‘ğ‘¡ = ğ‘, âˆ€ğ‘¡ < Î”ğ‘¡
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

#

(8)
for all ğ‘¥ âˆˆ X. If there exists a policy ğœ‡âˆ— = (ğœâˆ—, ğœ‹âˆ—) such that

ğ‘‰ğ‘ ğ‘¡ (ğ‘¥) = E

"

ğœâˆ— ( ğ‘¥)âˆ’1

ğ›½ğ‘¡ ğ‘(ğ‘¥ğ‘¡ , ğœ‹âˆ—(ğ‘¥)) + ğ›½ğœâˆ— ( ğ‘¥)

ğ‘‰ğ‘ ğ‘¡ (ğ‘¥ ğœâˆ— ( ğ‘¥) ) + ğ‘‚

Ã•ğ‘¡=0
ğ‘¥0 = ğ‘¥, ğ‘ğ‘¡ = ğœ‹âˆ—(ğ‘¥), âˆ€ğ‘¡ < Î”ğ‘¡

(cid:0)

,

#

(cid:1)

for all ğ‘¥ âˆˆ X, then ğœ‡âˆ— is an optimal policy for Problem 1.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
Proof. See Appendix A.

(cid:3)

Ã

Remark 2. The DP equation in eq. (8) includes the consoli-
Î”ğ‘¡âˆ’1
ğ‘¡=0 ğ›½ğ‘¡ ğ‘(ğ‘¥ğ‘¡ , ğ‘), which is the accumulated
dated state costs,
costs incurred from the current update time instance to the
next update time instance, the cost-to-go after Î”ğ‘¡-steps of
lookahead, ğ›½Î”ğ‘¡ğ‘‰ (ğ‘¥Î”ğ‘¥), and the penalty for a new update
ğ›½Î”ğ‘¡ğ‘‚. Based on the current measurement ğ‘¥, the DP equation
has Î”ğ‘¡-steps of lookahead. The number of steps Î”ğ‘¡ is opti-
mized in order to balance the trade-off between the system
performance and the update penalty. Thus, we refer to eq. (8)
as the DP equation with optimized lookahead. The optimized

3

number of lookahead steps is the optimal waiting time for
the next triggering given the penalty of triggering ğ‘‚. When
ğ‘‚ = 0, the DP equations gives ğ‘‰ğ‘ ğ‘¡ (ğ‘¥) = ğ‘‰ (ğ‘¥), âˆ€ğ‘¥ âˆˆ X, i.e.,
the value function is the same as the one in classic MDPs.

Remark 3 (Computational Methods). One can resort to
methods such as the usual value iteration or the policy
iteration [21] to solve the DP equation. In the value iteration
approach, given the ğ‘˜-th estimate of the value function,
ğ‘‰ğ‘ ğ‘¡ ,ğ‘˜ (Â·), the next estimate ğ‘‰ğ‘ ğ‘¡ ,ğ‘˜+1 can be computed using
eq. (8). Repeat this process until it converges to the ï¬xed-
point of eq. (8). The convergence is guaranteed for any given
ğ‘‰ğ‘ ğ‘¡ ,0, when ğ›½ < 1, in view of the Banach ï¬xed-point theorem
(see Theorem 6.2.3. of [21]). And the convergence rate is
guaranteed to be kğ‘‰ğ‘ ğ‘¡ ,ğ‘˜ âˆ’ğ‘‰ğ‘ ğ‘¡ k â‰¤ ( ğ›½ğ‘˜/(1 âˆ’ ğ›½)) kğ‘‰ğ‘ ğ‘¡ ,0 âˆ’ğ‘‰ğ‘ ğ‘¡ ,1k.
The actual convergence speed should be faster than the
above rate depending on what the update penalty ğ‘‚ is.

With Theorem 1, we can compute the value function ğ‘‰ğ‘ ğ‘¡ (Â·)
and the optimal self-triggering policy ğœ‡âˆ—. The computation of
ğ‘‰ğ‘ ğ‘¡ (Â·) and ğœ‡âˆ— is usually off-line, and then ğœ‡âˆ— is deployed for
online implementation. In the next sub-section, we propose
a greedy policy that solves Problem 2, i.e., a policy that
reduces the number of updates while maintaining a certain
level of sub-optimality.

B. Performance Guaranteed Self-Triggering Policies

In this sub-section, we propose a greedy self-triggering
policy ğœ‡ that achieves the inequality deï¬ned in eq. (4). To
present the policy, we begin with the following lemma.

Lemma 1. If a self-triggering policy ğœ‡ = (ğœ‹, ğœ) achieves the
following inequality

ğœ ( ğ‘¥)âˆ’1

E

"

Ã•ğ‘¡=0

ğ›½ğ‘¡ ğ‘(ğ‘¥ğ‘¡ , ğœ‹(ğ‘¥)) + ğ›¼ğ›½ğœ ( ğ‘¥)ğ‘‰ (ğ‘¥ ğœ ( ğ‘¥))

for all ğ‘¥ âˆˆ X, then we have ğ‘£ ğœ‡(ğ‘¥) â‰¤ ğ›¼ğ‘‰ (ğ‘¥).

Proof. See Appendix B.

ğ‘¥0 = ğ‘¥
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

#

â‰¤ ğ›¼ğ‘‰ (ğ‘¥),

(9)

(cid:3)

Lemma 1 offers us a convenient way to ï¬nd an policy
that achieves the performance level speciï¬ed by ğ›¼ğ‘‰ (ğ‘¥) for
all ğ‘¥ âˆˆ X and for ğ›¼ â‰¥ 1. Since the agent aims to reduce
the amount of sensing/communication resources (the rate of
updating), he/she needs to ï¬nd, for each ğ‘¥ âˆˆ X, the maximum
Î”ğ‘¡ğ‘¥ âˆˆ T such that there exists at least an action ğ‘ ğ‘¥ âˆˆ A so
that eq. (9) is satisï¬ed with ğœ(ğ‘¥) and ğœ‹(ğ‘¥) replaced by Î”ğ‘¡ğ‘¥
and ğ‘ ğ‘¥ respectively. Then, Problem 2 becomes solving the
following problem for each ğ‘¥ âˆˆ X

Î”ğ‘¡ğ‘¥

max
Î”ğ‘¡ğ‘¥ âˆˆT,ğ‘ğ‘¥ âˆˆA
ğ‘ .ğ‘¡.

eq. (9),

(10)

where in eq. (9), we replace ğœ(ğ‘¥) and ğœ‹(ğ‘¥) with Î”ğ‘¡ğ‘¥ and ğ‘ ğ‘¥
respectively.

Theorem 2. If there exists an optimal policy ğœ™âˆ— for the
classic MDP such that ğ‘£ ğœ™âˆ— = ğ‘‰ (ğ‘¥), then for any ï¬xed ğ›¼ â‰¥ 1,
there always exist a feasible set for (10), i.e., the problem
(10) is well-deï¬ned.

4

Proof. See Appendix C.

(cid:3)

Remark 4 (The Greedy Choice Property). Note that the self-
triggering policy for Problem 2 follows the greedy rule. At
time ğ‘¡ğ‘™. the next update time ğ‘¡ğ‘™+1 = ğ‘¡ğ‘™ + ğœ(ğ‘¥ğ‘¡ğ‘™ ) is maximized
while ensuring eq. (9) without considering the effect of
future updates after ğ‘¡ğ‘™+1.
this choice on the number of
Different from the greedy policy, the self-triggering policy ğœ‡âˆ—
from Theorem 1 for solving Problem 1 follows the dynamic
programming rule,
i.e., current choices are made taking
into account the inï¬‚uence of current choices on the future
possibilities.

So far in this section, we have developed Theorem 1 and
Theorem 2 to help ï¬nd the self-triggering policies that can
solve Problem 1 and Problem 2. The theorems were devel-
oped without specifying the state space X, the action space
A, and the transition probabilities, except that we require
X to be Polish and ğ‘(Â·, Â·) to be bounded and non-negative
on X Ã— A. Hence, The results are applicable to a variety of
models such as LQG control [2], [5], [14], inventory control
[8], and queueing systems [9], [22]. The two theorems pave
the way for the computation and implementation of the self-
triggering policies for various Markov decision processes. In
the next section, we present a gridworld example to illus-
trate the computation and implementation of self-triggering
policies using Theorem 1 and Theorem 2.

IV. COMPUTATION AND IMPLEMENTATION: A
GRIDWORLD CASE STUDY

In this section, we consider a rectangular gridworld rep-
resentation of a simple MDP for illustration purposes. The
gridworld environment made up of 4 Ã— 6 cells is shown
in 1, where grey areas are walls. An agent lives in this
gridworld aiming to navigate from the start cell to the target
cell. The states, representing the cell the agent lives in, are
X = {1, 2, Â· Â· Â· , 19, 20}. There are four actions possible at
each state, A = {north, south, east, west}. Walls block the
agentâ€™s path. The actions that would take the agent off the
grid or into the walls in fact leave the state unchanged.
State ğ‘¥ = 20 is an absorbing state such that once the agent
reaches the target cell, he/she enters the absorbing state with
probability one (w.p.1). The agent aims to reach the target
as fast as soon. Hence, we deï¬ne

ğ‘(ğ‘¥, ğ‘) =

10,
0,

(

if ğ‘¥ âˆˆ X\{19, 20},
if ğ‘¥ âˆˆ {19, 20}.

(11)

A. A Non-Windy Gridworld

We ï¬rst consider a non-windy setting where each ac-
tion deterministically causes the agent to move one cell
in the respective direction. Let ğ‘ƒğ‘‘ denotes the transition
probabilities in a non-wind setting. For instance, we have
ğ‘ƒğ‘‘ (6|1, north) = 1. We consider the discount factor ğ›½ = 0.95,
and the bound on the waiting time for the next update is
Â¯ğ‘‡ = 6. The update penalty ğ‘‚ is subject to change.

We set the initial value function estimate to be ğ‘‰ğ‘ ğ‘¡ ,0(ğ‘¥) =
0, âˆ€ğ‘¥ âˆˆ X. We conduct value iteration using the DP equation

(15)

(16)

(17)

(18)

T (19)

(10)

(11)

(12)

(13)

(14)

(6)

(7)

(8)

(9)

S (1)

(2)

(3)

(4)

(5)

Fig. 1: A gridworld example: Grey areas represent walls, S
stands for the start cell, T denotes the target cell, and the
integers in the brackets are the indices of states.

86.24
â†“â†’

80.25
â†“â†’

73.95
â†“â†’

67.32
â†“

80.25
â†’

73.95
â†’

67.32
â†’

60.33
â†“

86.24
â†‘â†’

80.25
â†‘

52.98
â†“

0.00

10.00
â†‘

19.50
â†‘

91.93
â†‘â†’

86.24
â†‘

45.24
â†’

37.10
â†’

28.52
â†‘

Fig. 2: A non-windy gridworld: The value ğ‘‰ (ğ‘¥) (the upper
value) and the optimal action ğœ™âˆ—(ğ‘¥) (the lower pointers) in
the classic MDP for each ğ‘¥ âˆˆ ğ‘‹.

with controlled lookahead in eq. (8):

ğ‘‰ğ‘ ğ‘¡ ,ğ‘˜+1(ğ‘¥) = min

ğ‘ âˆˆA,Î”ğ‘¡ âˆˆT

E

"

Î”ğ‘¡âˆ’1

Ã•ğ‘¡=0
+ ğ‘‚

ğ›½ğ‘¡ ğ‘(ğ‘¥ğ‘¡ , ğ‘) + ğ›½Î”ğ‘¡

ğ‘‰ğ‘ ğ‘¡ ,ğ‘˜ (ğ‘¥Î”ğ‘¡ )

(cid:0)

ğ‘¥0 = ğ‘¥, ğ‘ğ‘¡ = ğ‘, âˆ€ğ‘¡ < Î”ğ‘¡
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:1)

,

#

where every term in the expectation operator can be com-
puted using transition probabilities ğ‘ƒğ‘‘. The iteration stops
when kğ‘‰ğ‘ ğ‘¡ ,ğ‘˜+1 âˆ’ ğ‘‰ğ‘ ğ‘¡ ,ğ‘˜ k â‰¤ 10âˆ’5, and the results show that
the tolerance can be achieved within 25 iterations for every
update penalties ğ‘‚ we study in this paper.

In Fig. 3, we present the optimal triggering time ğœ(ğ‘¥)âˆ—
and the optimal control policy ğœ‹âˆ—(ğ‘¥) for each state when the
update penalties are ğ‘‚ = 0, 0.1, 40, 80. As we can see from
Fig. 3 (a), when ğ‘‚ = 0, since there is no update penalty, the
optimal triggering time is to update every time, i.e., ğœâˆ— (ğ‘¥) =
1, âˆ€ğ‘¥ âˆˆ X, and the optimal control policy is the same as its
counterpart in a classic setting, i.e., ğœ‹âˆ—(ğ‘¥) = ğœ™âˆ—(ğ‘¥), âˆ€ğ‘¥ âˆˆ X.
The policy offers three paths from the start cell to the target
cell: 1 â†’ 2 â†’ 7 â†’ 11 â†’ Â· Â· Â· â†’ 19, 1 â†’ 6 â†’ 7 â†’ 11 â†’
Â· Â· Â· â†’ 19, and 1 â†’ 6 â†’ 10 â†’ 11 â†’ Â· Â· Â· â†’ 19. Each path

5

1
â†“â†’
1
â†’

1
â†“â†’
1
â†’
1
â†‘â†’
1
â†‘â†’

1
â†“â†’
1
â†’
1
â†‘
1
â†‘

1
â†“
1
â†“
1
â†“
1
â†’

0

1
â†‘
1
â†‘
1
â†‘

1
â†’

1
â†’
1
â†’

3
â†’
3
â†’
1
â†‘
2
â†‘

2
â†’
2
â†’
1
â†‘
2
â†‘

3
â†“
2
â†“
1
â†“
2
â†’

0

6
â†‘
6
â†‘
6
â†‘

1
â†’

(a) The update penalty ğ‘‚ = 0.

(b) The update penalty ğ‘‚ = 0.1.

1
â†’
1
â†’

3
â†’
3
â†’
6
â†‘
6
â†‘

2
â†’
2
â†’
6
â†‘
6
â†‘

3
â†“
2
â†“
1
â†“
2
â†’

0

6
â†‘
6
â†‘
6
â†‘

1
â†’

6
â†’
6
â†’

6
â†’
6
â†’
6
â†‘
6
â†‘

6
â†’
6
â†’
6
â†‘
6
â†‘

6
â†“
6
â†“
6
â†“
2
â†’

0

6
â†‘
6
â†‘
6
â†‘

1
â†’

(c) The update penalty ğ‘‚ = 40.

(d) The update penalty ğ‘‚ = 80.

Fig. 3: A non-windy gridworld: The optimal triggering time
policy ğœâˆ— (ğ‘¥) (the upper value) and the optimal control policy
ğœ‹âˆ—(ğ‘¥) (the lower pointers) for each ğ‘¥ âˆˆ X under different
update penalties ğ‘‚. (For Problem 1)

takes 12 steps to complete, covers 13 cells, and there are 12
updates.

Suppose a remote controller controls the agent, and the
communication between them is expensive. Each communi-
cation/update induces an update penalty ğ‘‚. When ğ‘‚ = 0.1,
as is shown in 3 (b), an update is only triggered when
there is a need to update the action. For example, when
the agent is at state ğ‘¥ = 1 at time 0, the optimal control
policy is heading north, and the optimal waiting time is 2
steps. That means at time ğ‘¡ = 0, the agent communicates
with the controller and is commanded to go north and ï¬x
this action for 2 time steps, after which a new update will
be sent. Since there is a straight path to the target cell, in
a non-windy setting, at states ğ‘¥ = 8, 9, 14, the controller
chooses the maximum allowed waiting time Â¯ğ‘‡ = 6. There
are few points worth noticing when we compare Fig. 3 (a)
and (b): First, when the update penalty ğ‘‚ = 0.1, the optimal
policy, as is shown in Fig. 3, provides one shortest path to
the target cell: 1 â†’ 6 â†’ 10 â†’ 11 â†’ Â· Â· Â· â†’ 19. The path
takes 12 time steps to complete, which is the same as when
ğ‘‚ = 1. However, the updates are only triggered when the
agent was at states ğ‘¥ = 10, 16, 3, 5. Hence, the self-triggering
policy under ğ‘‚ = 0.1 requires only 4 updates to achieve the
same shortest path as the classic optimal policy. That means
the self-triggering policy saves (12 âˆ’ 4)/12 = 66.47% of
the communication resources required in a classic policy.
Second, When the update penalty is ğ‘‚ = 0.1, at state ğ‘¥ = 1,
going west is no longer an optimal choice since going west
requires more updates (5 in this case) to achieve the shortest
path. Third, in Fig. 3 (b), the optimal triggering time and the
corresponding optimal control at each state always take the
agent to the next turning points. For instance, at ğ‘¥ = 10,

the optimal action is to go east and to ï¬x this direction
for 3 steps. This optimal action and optimal waiting time
take the agent to state 13, where the agent has to turn south
to reach the target cell. There are two reasons to explain
this phenomenon: 1. the update penalty is relatively low,
compared with the stage cost deï¬ned in eq. (11), so that
achieving the shortest path within the minimum number
of steps is still a priority. 2. In a non-windy setting, the
actions deterministically move the agent toward the desired
direction, which means the controller can anticipate the
agentâ€™s trajectory in future steps. Hence, no update is needed
between the two turning points.

The computed self-triggering policy under ğ‘‚ = 40 is
provided in Fig. 3 (c). The self-triggering policy gives a
longer path to reduce the overhead of updating: 1 â†’ Â· Â· Â· â†’
15 â†’ Â· Â· Â· â†’ 18 â†’ 3 â†’ Â· Â· Â· â†’ 19, which takes 17 time steps
(stay at state 15 for 4 time steps due to 6 time steps of going
north without update), covers 15 cells, and requires 4 updates
to complete. Even though the self-triggering policy requires
the same number of updates as the case when ğ‘‚ = 0.1, the
updates are triggered later than their counterparts in the case
of ğ‘‚ = 0.1. Hence, the updates produce less costs due to the
discount effect. As the update penalty increases to ğ‘‚ = 80
(see Fig. 3 (d)), the optimal time policy at most of the states
becomes to wait as long as possible for next update, i.e.,
ğœâˆ— (ğ‘¥) = Â¯ğ‘‡, for ğ‘¥ âˆˆ X\{3, 4}.

B. A Windy Gridworld

Next, we consider a windy gridworld where the wind takes
the agent north 10% of the chance and west 10% of the
chance. And 80% of the time, the agentâ€™s movement follows
its action. In the windy gridworld, the effect of boundaries
and walls still applies. The transition probability in a windy
setting is deï¬ned by ğ‘ƒ ğ‘¤ . For example, if the agent is at state
ğ‘¥ = 11 and chooses to go east, we have ğ‘ƒ ğ‘¤ (12|11, east) =
0.8, ğ‘ƒ ğ‘¤ (10|11, east) = 0.1, and ğ‘ƒ ğ‘¤ (16|11, east) = 0.1. We
run value iterations using the DP equation with controlled
lookahead given in eq. (8) under the transition probabilities
ğ‘ƒ ğ‘¤ in the windy environment.

The optimal timing policy and optimal control policy are
presented in Fig. 4. One difference in a windy environment
is that the control chosen will not deterministically cause the
movement of the agent. That means if there is no update, the
controller needs to estimate the agentâ€™s trajectory, and there
exists an estimation error. Hence, we hypothesize that the
agent needs to trigger the update more frequently than in a
non-windy environment to know his/her location and then
adjust his/her control.

Fig. 4 (a) presents the case when there is no update penalty,
i.e., ğ‘‚ = 0. The optimal timing policy is to observe/update
every step. The control at state ğ‘¥ = 6 becomes going east to
avoid being taken to the northwest corner by the wind. At
states ğ‘¥ = 15, 16, 17, going south is not an optimal control
anymore since if the agent goes south, there is a chance that
the wind would take the agent back to the north. When the
update penalty is small, i.e., ğ‘‚ = 0.1, the optimal policy is
listed in Fig. 4 (b). There are two points worth mentioning

1
â†’
1
â†’

1
â†’
1
â†’
1
â†’
1
â†‘

1
â†’
1
â†’
1
â†‘
1
â†‘

1
â†“
1
â†“
1
â†“
1
â†’

0

1
â†‘
1
â†‘
1
â†‘

1
â†’

3
â†’
3
â†’
1
â†’
1
â†‘

2
â†’
2
â†’
1
â†‘
2
â†‘

1
â†’
1
â†’

2
â†“
1
â†“
1
â†“
1
â†’

0

6
â†‘
6
â†‘
1
â†‘

1
â†’

(a) The update penalty ğ‘‚ = 0.

(b) The update penalty ğ‘‚ = 0.1.

6
â†’
6
â†’

6
â†’
6
â†’
6
â†‘
6
â†‘

6
â†’
6
â†’
6
â†‘
6
â†‘

6
â†“
6
â†“
3
â†“
4
â†’

0

6
â†‘
6
â†‘
6
â†‘

1
â†’

6
â†’
6
â†’
6
â†‘
6
â†‘

6
â†’
6
â†’
6
â†‘
6
â†‘

6
â†’
6
â†’

6
â†“
6
â†“
6
â†“
6
â†’

0

6
â†‘
6
â†‘
6
â†‘

6
â†’

(c) The update penalty ğ‘‚ = 40.

(d) The update penalty ğ‘‚ = 80.

Fig. 4: A windy gridworld: The optimal triggering time
policy ğœâˆ— (ğ‘¥) (the upper value) and the optimal control policy
ğœ‹âˆ—(ğ‘¥) (the lower pointers) for each ğ‘¥ âˆˆ X under different
update penalties ğ‘‚. (For Problem 1)

when we compare the windy setting and the non-windy
setting:

1) When ğ‘‚ = 0.1, the agent updates more frequently in
a windy setting. For example, at ğ‘¥ = 5, the agent will
update the next step in a windy setting, while the agent
will update 6 steps later in a non-windy setting. One
of the reasons is that in a windy setting, the agent has
to update in the next step to make sure he/she goes
to state ğ‘¥ = 9 instead of being blown by the wind to
state ğ‘¥ = 4. This result backs up our hypothesis that
the agent in a windy world needs to trigger the update
more frequently than in a non-windy environment.
2) When ğ‘‚ = 40, Fig. 4 (c) shows some interesting
and unexpected results. The agent waits longer for the
next update in a windy setting than in a non-windy
setting shown in Fig. 3 (c). This result contradicts
our hypothesis that the agent tends to update more
frequently in a noisy environment. For example, if at
time ğ‘¡, the agent is at state 11, the next time the agent
will update is ğ‘¡ +6, which is longer than its counterpart
in Fig. 3 (c). One explanation is that since the control
is to head east, and the wind pushes the agent north
or west, there is no need for the agent to update its
action. Eventually, the agent will be more likely to be
at state 18 or 13 after 6 steps of ï¬xing his/her control
of going east.

When ğ‘‚ = 80, the optimal time policy at every step increases
to the maximum allowed waiting time Â¯ğ‘‡ = 6 to reduce the
update penalties.

6

C. Performance Guaranteed Policies

In the previous subsections, we solve Problem 1 in the
context of a gridworld and obtains the optimal self-triggering
policy ğœ‡âˆ— = (ğœâˆ—, ğœ‹âˆ—). Just
to remind that we have ğœ™ :
X â†’ A, which is the policy in the classic setting, and
the self-triggering policy ğœ‡ : X â†’ T Ã— A in self-triggered
MDPs. To differentiate the self-triggering policy we obtain
for Problem 1 and the policy for Problem 2, we name them
ğœ‡âˆ—
1

2) respectively.

= (ğœâˆ—
1) and ğœ‡âˆ—
The self-triggering policy ğœ‡âˆ—

1 is optimal with respect to
a speciï¬ed update penalty ğ‘‚. However, it does not provide
an explicit performance guarantee under the original cost
criterion. Instead, the self-triggering policy ğœ‡âˆ—
2 provides a
pre-speciï¬ed level of performance guarantee.

1 , ğœ‹âˆ—

2 , ğœ‹âˆ—

= (ğœâˆ—

2

As a result of the discussions in Section III-B, the steps to
2 for Problem 2 is given

compute a self-triggering policy ğœ‡âˆ—
as follows:

1) Compute the value function {ğ‘‰ (ğ‘¥), ğ‘¥ âˆˆ X} of the MDP

in the classic setting.

2) For each ğ‘¥ âˆˆ X, select Î”ğ‘¡ğ‘¥ = Â¯ğ‘‡.
3) Compute

#

,

.

ğ‘¥0 = ğ‘¥

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
ğ‘¥0 = ğ‘¥
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Ëœğ‘‰ (ğ‘¥) = min
ğ‘ âˆˆA

E

Î”ğ‘¡ğ‘¥âˆ’1

"
Ã•ğ‘¡=0
Î”ğ‘¡ğ‘¥ âˆ’1

ğ‘âˆ—
ğ‘¥ = arg min
ğ‘ âˆˆA

E

ğ›½ğ‘¡ ğ‘(ğ‘¥ğ‘¡ , ğ‘) + ğ›¼ğ›½Î”ğ‘¡ğ‘¥ğ‘‰ (ğ‘¥Î”ğ‘¡ğ‘¥ )

ğ›½ğ‘¡ ğ‘(ğ‘¥ğ‘¡ , ğ‘) + ğ›¼ğ›½Î”ğ‘¡ğ‘¥ğ‘‰ (ğ‘¥Î”ğ‘¡ğ‘¥ )

#
(12)
4) If Ëœğ‘‰ (ğ‘¥) > ğ›¼ğ‘‰ (ğ‘¥), set Î”ğ‘¡ğ‘¥ = Î”ğ‘¡ğ‘¥ âˆ’ 1, repeat step 3).

Ã•ğ‘¡=0

"

Otherwise, ğœâˆ—

2 (ğ‘¥) = Î”ğ‘¥, ğœ‹âˆ—

2(ğ‘¥) = ğ‘âˆ—
ğ‘¥.

The optimization problem in eq. (12) admits a closed-form
solution for models such as LQG control [2] and inventory
control [8]. For the windy gridworld model, we compute
self-triggering policies following the steps for various levels
of sub-optimality. The results are presented in Fig. 5. As
we can see from Fig. 5 (a), the self-triggering policy ğœ‡âˆ—
2 can
achieve a full level of optimality, i.e., ğ‘£ ğœ‡âˆ—
2 (ğ‘¥) = ğ‘‰ (ğ‘¥), âˆ€ğ‘¥ âˆˆ X,
while requiring less communication/sensing resources. When
the level of sub-optimality is ğ›¼ = 1.1, as one can see from
Fig. 5 (b), at most states, the optimal timing policy is to
wait for two or more than two steps for the next update.
That means the self-triggering policy ğœ‡âˆ—
2 can save more than
50% communication/sensing resources while suffering only
10% of performance degradation. If one can tolerate a higher
level of degradation, one can set ğ›¼ to a higher value and
compute the corresponding self-triggering policy ğœ‡âˆ—
2. The
cases when ğ›¼ = 1.4 and ğ›¼ = 2 are presented in Fig. 5 (c)
and (d). As one expects, the higher ğ›¼ is (more performance
degradation one can tolerate), the fewer updates needed (less
communication/resources consumed).

V. CONCLUSIONS

In this paper, two self-triggering policies are obtained
by proposing two frameworks that convey two different
philosophies. Problem 1 introduces a soft constraint, i.e., a
update penalty that penalizes frequent use of communication
resources and Problem 2 applies a hard constraint on the

7

1
â†’
1
â†’

3
â†’
3
â†’
1
â†’
1
â†‘

2
â†’
2
â†’
1
â†‘
2
â†‘

1
â†“
1
â†“
1
â†“
1
â†’

0

6
â†‘
6
â†‘
1
â†‘

1
â†’

1
â†’
1
â†’

4
â†’
3
â†’
1
â†’
2
â†‘

2
â†’
2
â†’
1
â†‘
2
â†‘

3
â†“
2
â†“
1
â†“
2
â†’

0

6
â†‘
6
â†‘
4
â†‘

1
â†’

(a) The pre-speciï¬ed level of
sub-optimality penalty ğ›¼ = 1.

(b) The pre-speciï¬ed level of
sub-optimality penalty ğ›¼ = 1.1.

2
â†’
1
â†’

6
â†’
6
â†’
5
â†’
3
â†‘

5
â†’
4
â†’
1
â†‘
3
â†‘

5
â†“
3
â†“
1
â†“
3
â†’

0

6
â†‘
6
â†‘
6
â†‘

1
â†’

6
â†’
5
â†’

6
â†’
6
â†’
6
â†’
6
â†’

6
â†’
6
â†’
6
â†’
6
â†’

6
â†“
6
â†“
4
â†“
6
â†’

0

6
â†‘
6
â†‘
6
â†‘

3
â†’

(c) The pre-speciï¬ed level of
sub-optimality penalty ğ›¼ = 1.4.

(d) The pre-speciï¬ed level of
sub-optimality penalty ğ›¼ = 2.

Fig. 5: A windy gridworld: The optimal triggering time
policy ğœâˆ—
2 (ğ‘¥) (the upper value) and the optimal control policy
ğœ‹âˆ—
2(ğ‘¥) (the lower pointers) for each ğ‘¥ âˆˆ X under various sub-
optimality requirements. (For Problem 2)

.

level of sub-optimality while maximizing the triggering time
to resources consumption. Both policies are shown to be
effective in reducing the use of communication resources
in the gridworld examples. Future endeavors can focus on
developing stability guarantees of self-triggered policy for
controlled Markov chain, and learning when to trigger, i.e.,
leveraging reinforcement learning techniques for unknown
MDP models.

APPENDIX

A. Proof of Theorem 1
Proof. We prove the theorem by constructing a consolidated
Markov decision process problem. A close look at eq. (6)
shows that this is a discounted cost discrete-time MDP with
discount factor ğ›½, Markov states and Markov actions given
respectively by

ğ‘‹ğ‘™ = (ğ‘¥ğ‘¡ğ‘™ , Ëœğ‘¡ğ‘™) âˆˆ X Ã— {0, 1, 2, Â· Â· Â· },
ğ´ğ‘™ = (ğ‘ğ‘¡ğ‘™ , Î”ğ‘¡) âˆˆ A Ã— T ,

where Ëœğ‘¡ğ‘™ = ğ‘¡ğ‘™ âˆ’ ğ‘™, the state cost equal to

ğ¶ (ğ‘‹ğ‘™, ğ´ğ‘™) = ğ›½ Ëœğ‘¡ğ‘™

Â¯ğ‘(ğ‘¥ğ‘¡ğ‘™ , ğ‘ğ‘¡ğ‘™ , Î”ğ‘¡) + ğ›½Î”ğ‘¡ ğ‘‚

,

and the skip-transition probability deï¬ned in eq. (5). Hence,
the cost in eq. (6) becomes

h

i

ğ‘“ ğœ‡ (ğ‘¥) = E

âˆ

"

Ã•ğ‘™=0

ğ›½ğ‘™ğ¶ (ğ‘‹ğ‘™.ğ´ğ‘™)

ğ‘‹ğ‘™ = (ğ‘¥, 0), ğœ‡

.

#

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

The consolidated formulation can be treated as a regular
Markov decision problem. Note that the Cartesian product
of countable countably many polish spaces is still Polish.

Hence, X Ã— {0, 1, 2, Â· Â· Â· } is Polish if X is polish. Thus, the
results (mainly the results available to Polish spaces) can
be derived from current Markov decision literature [21].
Applying Theorem 6.2.5 and Theorem 6.2.12 of [21], we
(cid:3)
obtain claims in Theorem 1.

B. Proof of Lemma 1
Proof. For a given ğ¿, let ğ‘¡ğ¿+1 be the time instance of the
(ğ¿ + 1)-th update. The accumulated costs before (ğ‘¡ğ¿+1) can
be written as
ğ‘¡ğ¿+1âˆ’1

ğ›½ğ‘¡ğ‘™ Â¯ğ‘(ğ‘¥ğ‘¡ğ‘™ , ğ‘ğ‘¡ğ‘™ , ğ‘¡ğ‘™+1 âˆ’ ğ‘¡ğ‘™)

ğ›½ğ‘¡ ğ‘(ğ‘¥ğ‘¡ , ğ‘ğ‘¡ )

"

Ã•ğ‘¡=0
ğ¿

E

=E

"

Ã•ğ‘™=0
ğ¿

Ã•ğ‘™=0

=E

"

ğ‘¡=ğ‘¡ğ‘™+1âˆ’1

ğ›½ğ‘¡ğ‘™ E

"

ğ‘¡=ğ‘¡ğ‘™
Ã•

#

ğ‘¥0 = ğ‘¥
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ğ‘¥0 = ğ‘¥
(cid:12)
(cid:12)
(cid:12)
(cid:12)
ğ›½ğ‘¡âˆ’ğ‘¡ğ‘™ ğ‘(ğ‘¥ğ‘¡ , ğ‘ğ‘¡ğ‘™ )
(cid:12)

#

(13)

ğ‘¥0 = ğ‘¥

,

#

#

ğ‘¥ğ‘¡ğ‘™
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

where we use the tower property of conditional expectation
to derive the ï¬rst equality and the second equality follows
immediately after some algebraic rearrangements. Suppose
at time instance ğ‘¡ğ‘™, ğ‘™ âˆˆ N, the process is at state ğ‘¥ğ‘¡ğ‘™ . Let
ğ‘¡ğ‘™+1 = ğ‘¡ğ‘™ + ğœ(ğ‘¥ğ‘¡ğ‘™ ) and ğ‘ğ‘¡ = ğœ‹(ğ‘¥ğ‘¡ğ‘™ ) for ğ‘¡ = ğ‘¡ğ‘™, ğ‘¡ğ‘™ + 1, . . . , ğ‘¡ğ‘™+1 âˆ’ 1.
From eq. (9), we have

ğ‘¡ğ‘™+1âˆ’1

E

"

ğ‘¡=ğ‘¡ğ‘™
Ã•

ğ›½ğ‘¡âˆ’ğ‘¡ğ‘™ ğ‘(ğ‘¥ğ‘¡ , ğ‘ğ‘¡ )

ğ‘¥ğ‘¡ğ‘™

â‰¤ ğ›¼ğ‘‰ (ğ‘¥ğ‘¡ğ‘™ ) âˆ’ E

ğ›¼ğ›½ğ‘¡ğ‘™+1âˆ’ğ‘¡ğ‘™ğ‘‰ (ğ‘¥ğ‘¡ğ‘™+1)

ğ‘¥ğ‘¡ğ‘™

.

(cid:12)
(cid:12)
Applying eq. (14) into eq. (13) for every ğ‘™ â‰¤ ğ¿ yields

(cid:2)

(cid:3)
(14)

#

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E

"

â‰¤E

"
=ğ›¼E

ğ‘¡ğ¿+1âˆ’1

ğ›½ğ‘¡ ğ‘(ğ‘¥ğ‘¡ , ğ‘ğ‘¡ )

Ã•ğ‘¡=0
ğ¿

ğ›½ğ‘¡ğ‘™ ğ›¼

ğ‘¥0 = ğ‘¥

#

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ğ‘‰ (ğ‘¥ğ‘¡ğ‘™ ) âˆ’ ğ›½ğ‘¡ğ‘™+1âˆ’ğ‘¡ğ‘™ğ‘‰ (ğ‘¥ğ‘¡ğ‘™+1)

Ã•ğ‘™=0
ğ‘‰ (ğ‘¥ğ‘¡0) âˆ’ ğ›½ğ‘¡ğ¿+1ğ‘‰ (ğ‘¥ğ‘¡ğ¿+1)

(cid:0)

ğ‘¥0 = ğ‘¥

ğ‘¥0 = ğ‘¥
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

#
(cid:1)
â‰¤ ğ›¼ğ‘‰ (ğ‘¥),

(cid:2)

where we use the fact that ğ‘ : X Ã— A â†’ R+ produces a
non-negative ğ‘‰ (Â·), i.e., ğ‘‰ (ğ‘¥) â‰¥ 0, âˆ€ğ‘¥ âˆˆ X. Since ğ¿ can be
chosen arbitrarily, taking ğ¿ to inï¬nity, we have ğ‘¡ğ‘™ â†’ âˆ, and
(cid:3)
by deï¬nition of ğ‘£ ğœ‡, ğ‘£ ğœ‡ (ğ‘¥) â‰¤ ğ›¼ğ‘‰ (ğ‘¥) for every ğ‘¥ âˆˆ X.

(cid:12)
(cid:12)

(cid:3)

C. Proof of Theorem 2

Proof. To show that
there is a feasible set for problem
(10), it is sufï¬cient to show that for any ğ‘¥ âˆˆ X, when
Î”ğ‘¡ğ‘¥ = 1, there always exists an action ğ‘ ğ‘¥ âˆˆ A such that
E [ğ‘(ğ‘¥, ğ‘ ğ‘¥) + ğ›¼ğ›½ğ‘‰ (ğ‘¥1)|ğ‘¥0 = ğ‘¥, ğ‘0 = ğ‘ ğ‘¥] â‰¤ ğ›¼ğ‘‰ (ğ‘¥). Let ğœ™âˆ— be
an optimal policy of the classic MDP. Then, by Bellman
equation, we have

E [ğ‘(ğ‘¥, ğ‘) + ğ›½ğ‘‰ (ğ‘¥1)|ğ‘¥0 = ğ‘¥, ğ‘0 = ğ‘] = ğ‘‰ (ğ‘¥),

min
ğ‘ âˆˆA

where the minimum is attained at ğ‘âˆ— = ğœ™âˆ—(ğ‘¥). That means
there exists ğ‘ ğ‘¥ = ğœ™âˆ— (ğ‘¥) such that

ğ›¼E [ğ‘(ğ‘¥, ğ‘ ğ‘¥) + ğ›½ğ‘‰ (ğ‘¥1)|ğ‘¥0 = ğ‘¥, ğ‘0 = ğ‘ ğ‘¥] = ğ›¼ğ‘‰ (ğ‘¥).

Since ğ‘(Â·, Â·) is non-negative, we have

E [ğ‘(ğ‘¥, ğ‘ ğ‘¥) + ğ›¼ğ›½ğ‘‰ (ğ‘¥1)|ğ‘¥0 = ğ‘¥, ğ‘0 = ğ‘ ğ‘¥] = ğ›¼ğ‘‰ (ğ‘¥).

8

This shows that for every ğ‘¥ âˆˆ X, there always exists a Î”ğ‘¡ğ‘¥ âˆˆ
T such that we can ï¬nd an action ğ‘ ğ‘¥ âˆˆ X so that (9) is
(cid:3)
satisï¬ed.

REFERENCES

[1] W. Heemels, K. H. Johansson, and P. Tabuada, â€œAn introduction to
event-triggered and self-triggered control,â€ in 2012 ieee 51st ieee
conference on decision and control (cdc).
IEEE, 2012, pp. 3270â€“
3285.

[2] T. Gommans, D. Antunes, T. Donkers, P. Tabuada, and M. Heemels,
â€œSelf-triggered linear quadratic control,â€ Automatica, vol. 50, no. 4,
pp. 1279â€“1287, 2014.

[3] A. Anta and P. Tabuada, â€œTo sample or not to sample: Self-triggered
control for nonlinear systems,â€ IEEE Transactions on automatic con-
trol, vol. 55, no. 9, pp. 2030â€“2042, 2010.

[4] X. Wang and M. D. Lemmon, â€œSelf-triggered feedback control systems
with ï¬nite-gain l2 stability,â€ IEEE Transactions on Automatic Control,
vol. 54, no. 3, pp. 452â€“467, 2009.

[5] S. Akashi, H. Ishii, and A. Cetinkaya, â€œSelf-triggered control with
tradeoffs in communication and computation,â€ Automatica, vol. 94,
pp. 373â€“380, 2018.

[6] Y. Gao, P. Yu, D. V. Dimarogonas, K. H. Johansson, and L. Xie, â€œRo-
bust self-triggered control for time-varying and uncertain constrained
systems via reachability analysis,â€ Automatica, vol. 107, pp. 574â€“581,
2019.

[7] S. Yuvaraj and M. Sangeetha, â€œSmart supply chain management using
internet of things(iot) and low power wireless communication sys-
tems,â€ in 2016 International Conference on Wireless Communications,
Signal Processing and Networking (WiSPNET), 2016, pp. 555â€“558.

[8] E. A. Feinberg, â€œOptimality conditions for inventory control,â€ in
Optimization Challenges in Complex, Networked and Risky Systems.
INFORMS, 2016, pp. 14â€“45.

[9] P. WieÂ¸cek, E. Altman, and A. Ghosh, â€œMean-ï¬eld game approach
to admission control of an ğ‘š\ğ‘š\âˆ queue with shared service cost,â€
Dynamic Games and Applications, vol. 6, no. 4, pp. 538â€“566, 2016.
[10] A. Zanella, N. Bui, A. Castellani, L. Vangelista, and M. Zorzi,
â€œInternet of things for smart cities,â€ IEEE Internet of Things Journal,
vol. 1, no. 1, pp. 22â€“32, 2014.

[11] W. Lu, F. Fan, J. Chu, P. Jing, and S. Yuting, â€œWearable computing
for internet of things: A discriminant approach for human activity
recognition,â€ IEEE Internet of Things Journal, vol. 6, no. 2, pp. 2749â€“
2759, 2019.

[12] A. Molin and S. Hirche, â€œOn the optimality of certainty equivalence
for event-triggered control systems,â€ IEEE Transactions on Automatic
Control, vol. 58, no. 2, pp. 470â€“474, 2013.

[13] D. Maity and J. S. Baras, â€œOptimal event-triggered control of nonde-
terministic linear systems,â€ IEEE Transactions on Automatic Control,
vol. 65, no. 2, pp. 604â€“619, 2019.

[14] Y. Huang and Q. Zhu, â€œInï¬nite-horizon linear-quadratic-gaussian
control with costly measurements,â€ arXiv preprint arXiv:2012.14925,
2020.

[15] D. P. Bertsekas and S. Shreve, Stochastic optimal control: the discrete-

time case. Athena Scientiï¬c,Belmont,MA, 1996.

[16] M. Gallieri and J. M. Maciejowski, â€œlasso mpc: Smart regulation of
over-actuated systems,â€ in 2012 American Control Conference (ACC).
IEEE, 2012, pp. 1217â€“1222.

[17] C. Cooper and N. Hahi, â€œAn optimal stochastic control problem with
observation cost,â€ IEEE Transactions on Automatic Control, vol. 16,
no. 2, pp. 185â€“189, 1971.

[18] Y. Huang and Q. Zhu, â€œCross-layer coordinated attacks on cyber-
physical systems: A lqg game framework with controlled observa-
tions,â€ arXiv preprint arXiv:2012.02384, 2020.

[19] â€”â€”, â€œA pursuit-evasion differential game with strategic information

acquisition,â€ arXiv preprint arXiv:2102.05469, 2021.

[20] R. Durrett, Probability: theory and examples. Cambridge university

press, 2019, vol. 49.

[21] M. L. Puterman, Markov decision processes: discrete stochastic dy-

namic programming.

John Wiley & Sons, 2014.

[22] Y. Huang, V. Kavitha, and Q. Zhu, â€œContinuous-time markov decision
processes with controlled observations,â€ in 2019 57th Annual Allerton
Conference on Communication, Control, and Computing (Allerton).
IEEE, 2019, pp. 32â€“39.

