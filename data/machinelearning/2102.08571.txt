Self-Triggered Markov Decision Processes

Yunhan Huang1 and Quanyan Zhu1

1
2
0
2

b
e
F
7
1

]

Y
S
.
s
s
e
e
[

1
v
1
7
5
8
0
.
2
0
1
2
:
v
i
X
r
a

Abstract— In this paper, we study Markov Decision Processes
(MDPs) with self-triggered strategies, where the idea of self-
triggered control is extended to more generic MDP models.
This extension broadens the application of self-triggering poli-
cies to a broader range of systems. We study the co-design
problems of the control policy and the triggering policy to
optimize two pre-speciﬁed cost criteria. The ﬁrst cost criterion
is introduced by incorporating a pre-speciﬁed update penalty
into the traditional MDP cost criteria to reduce the use of
communication resources. Under this criteria, a novel dynamic
programming (DP) equation called DP equation with optimized
lookahead to proposed to solve for the self-triggering policy
under this criteria. The second self-triggering policy is to
maximize the triggering time while still guaranteeing a pre-
speciﬁed level of sub-optimality. Theoretical underpinnings are
established for the computation and implementation of both
policies. Through a gridworld numerical example, we illustrate
the two policies’ effectiveness in reducing sources consumption
and demonstrate the trade-offs between resource consumption
and system performance.

I. INTRODUCTION

it

is desirable to limit

Recent advances in information and communication tech-
nologies have led to the implementation of
large-scale
resource-constrained networked control systems. In these
systems,
the sensor and control
communication and computation to instances when a system
needs attention [1]. As a result, the self-triggered control
paradigm is proposed to reduce the utilization of communica-
tion resources and/or actuation movements while still main-
taining desirable closed-loop behavior for these systems [2].
The self-triggered control abandons the conventional periodic
time-triggered implementations. In self-triggered control, the
self-triggering policy consists of two sub-policies: the control
policy and a triggering mechanism that pre-determines, at
an update time, when the control inputs have to be updated
the next time. Due to its efﬁciency in resource-saving, self-
triggered control has been studied extensively in the last
decades [1]–[6].

The study of self-triggered has been conﬁned to state-
space dynamical models, including either linear models [1],
[2], [4], [5] or nonlinear models [3], [6] in both(either)
continuous-time and(or) discrete-time settings. However, re-
cent developments in technologies such as wireless com-
munication, machine learning, and real-time analytics have
broadened the application of Internet of Things (IoTs) be-
yond control systems to a wide range of areas,
includ-
ing logistics and supply chain [7]–[9], smart cities [10],
and wearables [11]. These systems are usually large-scale,

1 Y. Huang and Q. Zhu are with the Department of Electrical and
Computer Engineering, New York University, 370 Jay St., Brooklyn, NY.
{yh.huang, qz494}@nyu.edu

1

equipped with resource-constrained devices, and difﬁcult to
be described by state-space dynamic models. Hence, there
is an urgent need to incorporate the idea of self-triggering
policy in control
into a more general dynamic model:
Markov Decision Processes (MDP). This incorporation can
lead toward a computationally and communicationally more
efﬁcient IoT-enabled system.

This paper studies a discrete-time self-triggered MDP
where the control1 policy and the triggering mecha-
nism/policy are co-designed to achieve certain cost criteria.
The differences between this work and most existing papers
in self-triggered control are three-fold. The ﬁrst is that we
study self-triggered policies for a more generic dynamic
model, i.e., an MDP model, which allows the extension of
the self-triggering policy to a wider range of applications.
Second, we address the co-design problem of jointly design-
ing the control policy and the triggering policy. Existing
self-triggering methods design the control policy and the
triggering policy in an ordered manner, i.e., the control policy
is designed ﬁrst. The triggering policy is then designed sub-
sequently while ensuring certain control performance [1], [4].
For example, in [4], the control gain is pre-set to be the 𝐻∞
control gain, based on which a triggering policy is designed
to assure a speciﬁed level of L2 stability. Since the control
policy is given without considering the self-triggering nature
of the whole policy, it is hard to guarantee that the given
control policy is optimal for achieving the minimum number
of updates while maintaining certain cost criteria [2]. Here,
we address a co-design problem to alleviate the concern
regarding the optimality issue. Third, in existing works [1],
the analysis of control performance under the self-triggered
control paradigm is mostly qualitative, e.g., the analysis of
whether a certain type of stability can be achieved. Control
performance is sometimes quantiﬁed as the decay rate for
the Lyapunov function. Only few self-triggering methods
provide quantitative analysis for control performance such
as L2 gains [4], quadratic costs [12]–[14]. More recently,
T. Gommans et al. studies self-triggered linear-quadratic-
gaussian (LQG) control associated with quadratic costs. In
this work, we consider a generic class of cost criteria and
propose self-triggered policies that can guarantee a certain
optimality level.The contributions of this paper are summa-
rized as follows.

1) We study self-triggered MDP, which extends the idea
of self-triggered control into a more generic dynamical
model. The genericness of the MDP model enables the
application of self-triggering policies into a broader

1In this paper, we use control and action interchangeably.

 
 
 
 
 
 
range of systems.

to ﬁnd an optimal stationary Markov policy that minimizes

2) We jointly design the control policy and the triggering
policy that co-optimizes pre-speciﬁed cost criteria.
3) We propose two frameworks that produce two co-
designed self-triggering policies. The ﬁrst is introduced
by incorporating an update penalty into the traditional
MDP cost criteria to reduce the use of communica-
tion resources. The second is a greedy reduction of
resources used while still guaranteeing any pre-given
level of sub-optimality. Theoretical underpinnings are
established for the computation and implementation of
both policies.

4) Through a gridworld example in both non-windy and
windy settings, we show that the proposed policies
are efﬁcient in reducing communication resources con-
sumed while still maintaining a high level of perfor-
mance.

A. Nomenclature

In this paper, R and N represent the set of real numbers
and natural numbers, respectively. The expectation operator
is denoted by E. And Δ𝑡 ∈ N denotes the time steps between
two neighboring updates. The letter 𝑙 is the index for the
𝑙th update and 𝑡𝑙 is the time instance when the 𝑙th update
happens. The notation N[𝑡𝑙 ,𝑡𝑙+1 ] means the intersection of the
two setsN and [𝑡𝑙, 𝑡𝑙+1]. The set of non-negative real numbers
is denoted by R+. The notation A\B denotes the set {𝑥 | 𝑥 ∈
A, 𝑥 ∉ B}.

II. SELF-TRIGGERED MARKOV DECISION
PROCESS

In this section, we provide the problem formulation for
the self-triggered action strategy. We consider a discrete-time
MDP deﬁned by a tuple {X, A, 𝑃, 𝑐}, where X is the state
space, A is the actions space, 𝑃 is the time-homogeneous
transition probability, and 𝑐 is the state-wise cost function.
The state space X and action space A are both assumed to be
Borel subsets of Polish (Banach and separable) spaces. If an
action 𝑎 ∈ A is selected at a state 𝑥 ∈ X, then a cost 𝑐(𝑥, 𝑎)
is incurred, where without loss of generality, we suppose
𝑐 : X × A → R+. The function 𝑐 is assumed to be bounded
and Borel measurable. The transition probability 𝑃(𝐵|𝑥, 𝑎)
is a Borel function on X × A for each Borel subset 𝐵 of X,
and 𝑃(·|𝑥, 𝑎) is a probability measure on the Borel 𝜎-ﬁeld
of X for each (𝑥, 𝑎) ∈ X × A.

In classic MDP, the decision process proceeds as follows:
at time 𝑡 = 0, 1, · · · , the current state of the system, 𝑥𝑡 ,
is observed. A decision-maker decides which action, 𝑎𝑡 , to
choose, the cost 𝑐𝑡 = 𝑐(𝑥𝑡 , 𝑎𝑡 ) is incurred, the system moves
to the next state following the rule 𝑥𝑡+1 ∼ 𝑃(·|𝑥𝑡 , 𝑎), and the
process continues. The rule that the decision-maker follows
to choose an action is called policy. We consider stationary
Markov policy 𝜙 in which all decisions depend only on the
current state. A stationary Markov policy 𝜙 is deﬁned by a
measurable mapping 𝜙 : X × A. In classic MDP, the goal is

∞

𝛽𝑡 𝑐(𝑥𝑡 , 𝑎𝑡 )

𝑣 𝜙 (𝑥) = E𝜙

"

Õ𝑡=0

𝑥0 = 𝑥

,

#

(1)

where 𝛽 is a discount factor strictly less than 1, and the
expectation is based on the probability distribution on the
set of all trajectory (X × A)∞, which is uniquely determined
by the policy 𝜙 and the initial state 𝑥 ( [15], pp. 140-141).
Deﬁne the optimal cost

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

𝑉 (𝑥) ≔ inf
𝜙 ∈Φ

𝑣 𝜙 (𝑥),

where Φ is the set of all stationary policies. A policy 𝜙 is
called optimal if 𝑣 𝜙 (𝑥) = 𝑉 (𝑥) for all 𝑥 ∈ X.

A. Self-Triggered Decision Making

In classic MDP, decision making requires persistent trans-
mission of measured state and updates of actions at each
time instance 𝑡 ∈ N. In this paper, we are interested in
constructing a policy that requires less sensing demand,
lower communication rate, and less actuator movements [16],
while still maintaining certain forms of optimality.

The self-triggering policy is based on holding the current
input value for a controlled duration while still guaranteeing
certain forms of optimality. The self-triggered policy carries
the following structure

𝑡𝑙+1 = 𝑡𝑙 + 𝜏(𝑥𝑡𝑙 ),
𝑎𝑡

= 𝜋(𝑥𝑡𝑙 ) ∈ A, 𝑡 ∈ N[𝑡𝑙 .𝑡𝑙+1) ,

(

(2)

where 𝑙 is the index for the number of triggers, 𝑡0 ≔ 0,
𝜏 : X → T , T ≔ {1, 2, · · · , ¯𝑇 }, ¯𝑇 ∈ N, and 𝜋 : X → A.
Here, the integer ¯𝑇 is an arbitrary upper bound on the waiting
time for next update. The self-triggering policy, denoted by
𝜇, involves two sub-policies: the timing policy, 𝜏(𝑥), that
determines the next time for updating, and the control policy,
𝜋(𝑥), that chooses a ﬁxed action to deploy for the next 𝜏(𝑥)
time instances. For convenience, we write 𝜇 = (𝜏, 𝜋) and
𝜇 : X → T × A.

B. Performance Criteria

This paper introduces two different yet related problems
associated with two cost criteria; one is constructed by
incorporating a penalty 𝑂 ≥ 0 for updating the action into the
classic cost criteria deﬁned in eq. (1). The idea of introducing
a penalty is originated from costly measurements that have
been investigated in the context of LQG optimal control [14],
[17] and games [18], [19]. The penalty 𝑂 ≥ 0 is a scalar,
which we refer to as the update penalty. For instance, if 𝑡𝑙
and 𝑡𝑙+1 are two neighboring updating time, during the time
interval [𝑡𝑙, 𝑡𝑙+1], the total update penalty is 𝛽𝑡𝑙𝑂 + 𝛽𝑡𝑙+1𝑂.
Now, we formulate the ﬁrst problem.

Problem 1. Find an optimal self-triggering policy 𝜇 that
minimizes the following cost criterion over an inﬁnite horizon

𝑓 𝜇 (𝑥) = E𝜇

∞

"

Õ𝑡=0

∞

𝛽𝑡 𝑐(𝑥𝑡 , 𝑎𝑡 ) +

𝛽𝑡𝑙𝑂

Õ𝑙=1

(3)

,

#

𝑥0 = 𝑥
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

2

where the ﬁrst term is the accumulated costs in the classic
MDP, and the second term is the accumulated costs of
updating one’s action.

The other cost criteria is similar to that of [2]. That
is for a pre-speciﬁed sub-optimal performance, we aim to
reduce the number of times the input/output is updated, while
maintaining the pre-speciﬁed sub-optimal performance. Now,
we formulate our second problem as

Problem 2. Find a policy 𝜇 that maximizes the next trans-
mission time 𝜏(𝑥) subject to the performance guarantee that

𝑣 𝜇 (𝑥) ≤ 𝛼𝑉 (𝑥), for all 𝑥 ∈ X,

(4)

where 𝛼 ≥ 1 is a scalar.

Remark 1. In Problem 1, we introduce an update penalty 𝑂
to capture the trade-off between the degree of optimality and
the usage of sensing/communication resources. The update
penalty can be interpreted as a soft constraint on the number
of updates. In Problem 2, 𝛼 serves as a scaling factor
that can be selected arbitrarily to balance the consumption
of sensing/communication resources and the degradation
of performance. There is a hard constraint that requires
matainting a certain degree of sub-optimality. When 𝛼 = 1,
no degradation of performance is allowed. Solving both
problems involves the co-design of the waiting time for next
update (through 𝜏) and the chosen action (through 𝜋).

III. THEORETICAL FRAMEWORKS

In this section, by establishing theoretical underpinnings,
we pave the way for ﬁnding the self-triggering policies that
solve the problems. For Problem 1, we formulate a dynamic
programming (DP) equation, which we call a DP equation
with optimized lookahead. With this equation, we can resort
to several effective methods such as value iterations and
policy iterations to characterize an optimal self-triggering
policy. For Problem 2, we propose a greedy self-triggering
policy that aims to reduce the number of updates and show
that the proposed policy is well-deﬁned and satisﬁes the
performance guarantee for any pre-speciﬁed 𝛼.

A. Dynamic Programming Equation with Optimized Looka-
head

To solve Problem 1,

the DP equation with optimized
lookahead is derived and presented in this sub-section. The
derivation idea is to form consolidated costs, states, and
actions between two update time instances, which generates
a new discrete-time MDP in the classic setting.

Let ¯𝑐𝑙 represent the consolidated costs that correspond to
the time period between 𝑙-th update and (𝑙 + 1)-th update,
i.e., the time period [𝑡𝑙, 𝑡𝑙+1). From eq. (2) and eq. (3), we
can obtain

¯𝑐𝑙 ≔ ¯𝑐(𝑥𝑡𝑙 , 𝑎𝑡𝑙 , Δ𝑡𝑙) = E

"

Õ𝑡=0

Δ𝑡𝑙−1

𝛽𝑡 𝑐(𝑥𝑡𝑙 +𝑡 , 𝑎𝑡𝑙 )

where given a self-triggering policy 𝜇 = (𝜋, 𝜏), the ﬁxed
action 𝑎𝑡𝑙 is produced by 𝜋(𝑥𝑡𝑙 ) and the waiting time Δ𝑡𝑙 is

,

#

𝑥𝑡𝑙 , 𝑎𝑡𝑙 , Δ𝑡𝑙
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

generated by 𝜏(𝑥𝑡𝑙 ). An application of the Fubini’s theorem
(principle) and Markov property [20] yields

Δ𝑡−1

¯𝑐(𝑥, 𝑎, Δ𝑡) =

𝛽𝑡 E [𝑐(𝑥𝑡 , 𝑎𝑡 )|𝑥0 = 𝑥, 𝑎𝑡 = 𝑎, ∀𝑡 < Δ𝑡] .

Õ𝑡=0

Furthermore, we deﬁne

¯𝑃(𝐵|𝑥, 𝑎, Δ𝑇) ≔ Prob (𝑥Δ𝑡 ∈ 𝐵|𝑥0 = 𝑥, 𝑎𝑡 = 𝑎, ∀𝑡 < Δ𝑡) ,

(5)
as the skip-probability that the MDP is in Borel subset 𝐵 of
X, after time Δ𝑡, given that the initial condition is 𝑥0 = 𝑥
and that the action is ﬁxed until Δ𝑡. The skip-probability
¯𝑃(𝐵|𝑥, 𝑎, Δ𝑇) is a Borel function on X × A × T for each
Borel subset 𝐵 of X, which is determined by the one-step
transition probability 𝑃(·|𝑥, 𝑎) deﬁned in Section II.

With the deﬁnition of the consolidated stage-wise function
¯𝑐 and the tower property of conditional expectation, the
inﬁnite-horizon cost functional in eq. (3) can be re-written
as

∞

𝑓 𝜇 (𝑥) = E

𝛽𝑡𝑙

¯𝑐(𝑥𝑡𝑙 , 𝜇(𝑥𝑡𝑙 )) + 𝛽𝜏 ( 𝑥𝑡𝑙 )𝑂)

(cid:16)
Deﬁne the optimal cost for Problem 1 as

"

Õ𝑙=0

𝑥0 = 𝑥

#

. (6)

(cid:17)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

𝑉𝑠𝑡 (𝑥) ≔ inf
𝜇 ∈Φ𝑠𝑡
where Φ𝑠𝑡 is the set of all policies taking the structure of . In
the following theorem, we state the DP equation for 𝑉𝑠𝑡 (·).

𝑓 𝜇 (𝑥),

(7)

Theorem 1. The value function deﬁned by eq. (7) satisﬁes
the following dynamic programming equation:

𝑉𝑠𝑡 (𝑥) =

inf
𝑎 ∈A,Δ𝑡 ∈T

E

"

Δ𝑡−1

𝛽𝑡 𝑐(𝑥𝑡 , 𝑎) + 𝛽Δ𝑡 (𝑉𝑠𝑡 (𝑥Δ𝑡 ) + 𝑂)

Õ𝑡=0
𝑥0 = 𝑥, 𝑎𝑡 = 𝑎, ∀𝑡 < Δ𝑡
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

#

(8)
for all 𝑥 ∈ X. If there exists a policy 𝜇∗ = (𝜏∗, 𝜋∗) such that

𝑉𝑠𝑡 (𝑥) = E

"

𝜏∗ ( 𝑥)−1

𝛽𝑡 𝑐(𝑥𝑡 , 𝜋∗(𝑥)) + 𝛽𝜏∗ ( 𝑥)

𝑉𝑠𝑡 (𝑥 𝜏∗ ( 𝑥) ) + 𝑂

Õ𝑡=0
𝑥0 = 𝑥, 𝑎𝑡 = 𝜋∗(𝑥), ∀𝑡 < Δ𝑡

(cid:0)

,

#

(cid:1)

for all 𝑥 ∈ X, then 𝜇∗ is an optimal policy for Problem 1.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
Proof. See Appendix A.

(cid:3)

Í

Remark 2. The DP equation in eq. (8) includes the consoli-
Δ𝑡−1
𝑡=0 𝛽𝑡 𝑐(𝑥𝑡 , 𝑎), which is the accumulated
dated state costs,
costs incurred from the current update time instance to the
next update time instance, the cost-to-go after Δ𝑡-steps of
lookahead, 𝛽Δ𝑡𝑉 (𝑥Δ𝑥), and the penalty for a new update
𝛽Δ𝑡𝑂. Based on the current measurement 𝑥, the DP equation
has Δ𝑡-steps of lookahead. The number of steps Δ𝑡 is opti-
mized in order to balance the trade-off between the system
performance and the update penalty. Thus, we refer to eq. (8)
as the DP equation with optimized lookahead. The optimized

3

number of lookahead steps is the optimal waiting time for
the next triggering given the penalty of triggering 𝑂. When
𝑂 = 0, the DP equations gives 𝑉𝑠𝑡 (𝑥) = 𝑉 (𝑥), ∀𝑥 ∈ X, i.e.,
the value function is the same as the one in classic MDPs.

Remark 3 (Computational Methods). One can resort to
methods such as the usual value iteration or the policy
iteration [21] to solve the DP equation. In the value iteration
approach, given the 𝑘-th estimate of the value function,
𝑉𝑠𝑡 ,𝑘 (·), the next estimate 𝑉𝑠𝑡 ,𝑘+1 can be computed using
eq. (8). Repeat this process until it converges to the ﬁxed-
point of eq. (8). The convergence is guaranteed for any given
𝑉𝑠𝑡 ,0, when 𝛽 < 1, in view of the Banach ﬁxed-point theorem
(see Theorem 6.2.3. of [21]). And the convergence rate is
guaranteed to be k𝑉𝑠𝑡 ,𝑘 −𝑉𝑠𝑡 k ≤ ( 𝛽𝑘/(1 − 𝛽)) k𝑉𝑠𝑡 ,0 −𝑉𝑠𝑡 ,1k.
The actual convergence speed should be faster than the
above rate depending on what the update penalty 𝑂 is.

With Theorem 1, we can compute the value function 𝑉𝑠𝑡 (·)
and the optimal self-triggering policy 𝜇∗. The computation of
𝑉𝑠𝑡 (·) and 𝜇∗ is usually off-line, and then 𝜇∗ is deployed for
online implementation. In the next sub-section, we propose
a greedy policy that solves Problem 2, i.e., a policy that
reduces the number of updates while maintaining a certain
level of sub-optimality.

B. Performance Guaranteed Self-Triggering Policies

In this sub-section, we propose a greedy self-triggering
policy 𝜇 that achieves the inequality deﬁned in eq. (4). To
present the policy, we begin with the following lemma.

Lemma 1. If a self-triggering policy 𝜇 = (𝜋, 𝜏) achieves the
following inequality

𝜏 ( 𝑥)−1

E

"

Õ𝑡=0

𝛽𝑡 𝑐(𝑥𝑡 , 𝜋(𝑥)) + 𝛼𝛽𝜏 ( 𝑥)𝑉 (𝑥 𝜏 ( 𝑥))

for all 𝑥 ∈ X, then we have 𝑣 𝜇(𝑥) ≤ 𝛼𝑉 (𝑥).

Proof. See Appendix B.

𝑥0 = 𝑥
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

#

≤ 𝛼𝑉 (𝑥),

(9)

(cid:3)

Lemma 1 offers us a convenient way to ﬁnd an policy
that achieves the performance level speciﬁed by 𝛼𝑉 (𝑥) for
all 𝑥 ∈ X and for 𝛼 ≥ 1. Since the agent aims to reduce
the amount of sensing/communication resources (the rate of
updating), he/she needs to ﬁnd, for each 𝑥 ∈ X, the maximum
Δ𝑡𝑥 ∈ T such that there exists at least an action 𝑎 𝑥 ∈ A so
that eq. (9) is satisﬁed with 𝜏(𝑥) and 𝜋(𝑥) replaced by Δ𝑡𝑥
and 𝑎 𝑥 respectively. Then, Problem 2 becomes solving the
following problem for each 𝑥 ∈ X

Δ𝑡𝑥

max
Δ𝑡𝑥 ∈T,𝑎𝑥 ∈A
𝑠.𝑡.

eq. (9),

(10)

where in eq. (9), we replace 𝜏(𝑥) and 𝜋(𝑥) with Δ𝑡𝑥 and 𝑎 𝑥
respectively.

Theorem 2. If there exists an optimal policy 𝜙∗ for the
classic MDP such that 𝑣 𝜙∗ = 𝑉 (𝑥), then for any ﬁxed 𝛼 ≥ 1,
there always exist a feasible set for (10), i.e., the problem
(10) is well-deﬁned.

4

Proof. See Appendix C.

(cid:3)

Remark 4 (The Greedy Choice Property). Note that the self-
triggering policy for Problem 2 follows the greedy rule. At
time 𝑡𝑙. the next update time 𝑡𝑙+1 = 𝑡𝑙 + 𝜏(𝑥𝑡𝑙 ) is maximized
while ensuring eq. (9) without considering the effect of
future updates after 𝑡𝑙+1.
this choice on the number of
Different from the greedy policy, the self-triggering policy 𝜇∗
from Theorem 1 for solving Problem 1 follows the dynamic
programming rule,
i.e., current choices are made taking
into account the inﬂuence of current choices on the future
possibilities.

So far in this section, we have developed Theorem 1 and
Theorem 2 to help ﬁnd the self-triggering policies that can
solve Problem 1 and Problem 2. The theorems were devel-
oped without specifying the state space X, the action space
A, and the transition probabilities, except that we require
X to be Polish and 𝑐(·, ·) to be bounded and non-negative
on X × A. Hence, The results are applicable to a variety of
models such as LQG control [2], [5], [14], inventory control
[8], and queueing systems [9], [22]. The two theorems pave
the way for the computation and implementation of the self-
triggering policies for various Markov decision processes. In
the next section, we present a gridworld example to illus-
trate the computation and implementation of self-triggering
policies using Theorem 1 and Theorem 2.

IV. COMPUTATION AND IMPLEMENTATION: A
GRIDWORLD CASE STUDY

In this section, we consider a rectangular gridworld rep-
resentation of a simple MDP for illustration purposes. The
gridworld environment made up of 4 × 6 cells is shown
in 1, where grey areas are walls. An agent lives in this
gridworld aiming to navigate from the start cell to the target
cell. The states, representing the cell the agent lives in, are
X = {1, 2, · · · , 19, 20}. There are four actions possible at
each state, A = {north, south, east, west}. Walls block the
agent’s path. The actions that would take the agent off the
grid or into the walls in fact leave the state unchanged.
State 𝑥 = 20 is an absorbing state such that once the agent
reaches the target cell, he/she enters the absorbing state with
probability one (w.p.1). The agent aims to reach the target
as fast as soon. Hence, we deﬁne

𝑐(𝑥, 𝑎) =

10,
0,

(

if 𝑥 ∈ X\{19, 20},
if 𝑥 ∈ {19, 20}.

(11)

A. A Non-Windy Gridworld

We ﬁrst consider a non-windy setting where each ac-
tion deterministically causes the agent to move one cell
in the respective direction. Let 𝑃𝑑 denotes the transition
probabilities in a non-wind setting. For instance, we have
𝑃𝑑 (6|1, north) = 1. We consider the discount factor 𝛽 = 0.95,
and the bound on the waiting time for the next update is
¯𝑇 = 6. The update penalty 𝑂 is subject to change.

We set the initial value function estimate to be 𝑉𝑠𝑡 ,0(𝑥) =
0, ∀𝑥 ∈ X. We conduct value iteration using the DP equation

(15)

(16)

(17)

(18)

T (19)

(10)

(11)

(12)

(13)

(14)

(6)

(7)

(8)

(9)

S (1)

(2)

(3)

(4)

(5)

Fig. 1: A gridworld example: Grey areas represent walls, S
stands for the start cell, T denotes the target cell, and the
integers in the brackets are the indices of states.

86.24
↓→

80.25
↓→

73.95
↓→

67.32
↓

80.25
→

73.95
→

67.32
→

60.33
↓

86.24
↑→

80.25
↑

52.98
↓

0.00

10.00
↑

19.50
↑

91.93
↑→

86.24
↑

45.24
→

37.10
→

28.52
↑

Fig. 2: A non-windy gridworld: The value 𝑉 (𝑥) (the upper
value) and the optimal action 𝜙∗(𝑥) (the lower pointers) in
the classic MDP for each 𝑥 ∈ 𝑋.

with controlled lookahead in eq. (8):

𝑉𝑠𝑡 ,𝑘+1(𝑥) = min

𝑎 ∈A,Δ𝑡 ∈T

E

"

Δ𝑡−1

Õ𝑡=0
+ 𝑂

𝛽𝑡 𝑐(𝑥𝑡 , 𝑎) + 𝛽Δ𝑡

𝑉𝑠𝑡 ,𝑘 (𝑥Δ𝑡 )

(cid:0)

𝑥0 = 𝑥, 𝑎𝑡 = 𝑎, ∀𝑡 < Δ𝑡
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:1)

,

#

where every term in the expectation operator can be com-
puted using transition probabilities 𝑃𝑑. The iteration stops
when k𝑉𝑠𝑡 ,𝑘+1 − 𝑉𝑠𝑡 ,𝑘 k ≤ 10−5, and the results show that
the tolerance can be achieved within 25 iterations for every
update penalties 𝑂 we study in this paper.

In Fig. 3, we present the optimal triggering time 𝜏(𝑥)∗
and the optimal control policy 𝜋∗(𝑥) for each state when the
update penalties are 𝑂 = 0, 0.1, 40, 80. As we can see from
Fig. 3 (a), when 𝑂 = 0, since there is no update penalty, the
optimal triggering time is to update every time, i.e., 𝜏∗ (𝑥) =
1, ∀𝑥 ∈ X, and the optimal control policy is the same as its
counterpart in a classic setting, i.e., 𝜋∗(𝑥) = 𝜙∗(𝑥), ∀𝑥 ∈ X.
The policy offers three paths from the start cell to the target
cell: 1 → 2 → 7 → 11 → · · · → 19, 1 → 6 → 7 → 11 →
· · · → 19, and 1 → 6 → 10 → 11 → · · · → 19. Each path

5

1
↓→
1
→

1
↓→
1
→
1
↑→
1
↑→

1
↓→
1
→
1
↑
1
↑

1
↓
1
↓
1
↓
1
→

0

1
↑
1
↑
1
↑

1
→

1
→
1
→

3
→
3
→
1
↑
2
↑

2
→
2
→
1
↑
2
↑

3
↓
2
↓
1
↓
2
→

0

6
↑
6
↑
6
↑

1
→

(a) The update penalty 𝑂 = 0.

(b) The update penalty 𝑂 = 0.1.

1
→
1
→

3
→
3
→
6
↑
6
↑

2
→
2
→
6
↑
6
↑

3
↓
2
↓
1
↓
2
→

0

6
↑
6
↑
6
↑

1
→

6
→
6
→

6
→
6
→
6
↑
6
↑

6
→
6
→
6
↑
6
↑

6
↓
6
↓
6
↓
2
→

0

6
↑
6
↑
6
↑

1
→

(c) The update penalty 𝑂 = 40.

(d) The update penalty 𝑂 = 80.

Fig. 3: A non-windy gridworld: The optimal triggering time
policy 𝜏∗ (𝑥) (the upper value) and the optimal control policy
𝜋∗(𝑥) (the lower pointers) for each 𝑥 ∈ X under different
update penalties 𝑂. (For Problem 1)

takes 12 steps to complete, covers 13 cells, and there are 12
updates.

Suppose a remote controller controls the agent, and the
communication between them is expensive. Each communi-
cation/update induces an update penalty 𝑂. When 𝑂 = 0.1,
as is shown in 3 (b), an update is only triggered when
there is a need to update the action. For example, when
the agent is at state 𝑥 = 1 at time 0, the optimal control
policy is heading north, and the optimal waiting time is 2
steps. That means at time 𝑡 = 0, the agent communicates
with the controller and is commanded to go north and ﬁx
this action for 2 time steps, after which a new update will
be sent. Since there is a straight path to the target cell, in
a non-windy setting, at states 𝑥 = 8, 9, 14, the controller
chooses the maximum allowed waiting time ¯𝑇 = 6. There
are few points worth noticing when we compare Fig. 3 (a)
and (b): First, when the update penalty 𝑂 = 0.1, the optimal
policy, as is shown in Fig. 3, provides one shortest path to
the target cell: 1 → 6 → 10 → 11 → · · · → 19. The path
takes 12 time steps to complete, which is the same as when
𝑂 = 1. However, the updates are only triggered when the
agent was at states 𝑥 = 10, 16, 3, 5. Hence, the self-triggering
policy under 𝑂 = 0.1 requires only 4 updates to achieve the
same shortest path as the classic optimal policy. That means
the self-triggering policy saves (12 − 4)/12 = 66.47% of
the communication resources required in a classic policy.
Second, When the update penalty is 𝑂 = 0.1, at state 𝑥 = 1,
going west is no longer an optimal choice since going west
requires more updates (5 in this case) to achieve the shortest
path. Third, in Fig. 3 (b), the optimal triggering time and the
corresponding optimal control at each state always take the
agent to the next turning points. For instance, at 𝑥 = 10,

the optimal action is to go east and to ﬁx this direction
for 3 steps. This optimal action and optimal waiting time
take the agent to state 13, where the agent has to turn south
to reach the target cell. There are two reasons to explain
this phenomenon: 1. the update penalty is relatively low,
compared with the stage cost deﬁned in eq. (11), so that
achieving the shortest path within the minimum number
of steps is still a priority. 2. In a non-windy setting, the
actions deterministically move the agent toward the desired
direction, which means the controller can anticipate the
agent’s trajectory in future steps. Hence, no update is needed
between the two turning points.

The computed self-triggering policy under 𝑂 = 40 is
provided in Fig. 3 (c). The self-triggering policy gives a
longer path to reduce the overhead of updating: 1 → · · · →
15 → · · · → 18 → 3 → · · · → 19, which takes 17 time steps
(stay at state 15 for 4 time steps due to 6 time steps of going
north without update), covers 15 cells, and requires 4 updates
to complete. Even though the self-triggering policy requires
the same number of updates as the case when 𝑂 = 0.1, the
updates are triggered later than their counterparts in the case
of 𝑂 = 0.1. Hence, the updates produce less costs due to the
discount effect. As the update penalty increases to 𝑂 = 80
(see Fig. 3 (d)), the optimal time policy at most of the states
becomes to wait as long as possible for next update, i.e.,
𝜏∗ (𝑥) = ¯𝑇, for 𝑥 ∈ X\{3, 4}.

B. A Windy Gridworld

Next, we consider a windy gridworld where the wind takes
the agent north 10% of the chance and west 10% of the
chance. And 80% of the time, the agent’s movement follows
its action. In the windy gridworld, the effect of boundaries
and walls still applies. The transition probability in a windy
setting is deﬁned by 𝑃 𝑤 . For example, if the agent is at state
𝑥 = 11 and chooses to go east, we have 𝑃 𝑤 (12|11, east) =
0.8, 𝑃 𝑤 (10|11, east) = 0.1, and 𝑃 𝑤 (16|11, east) = 0.1. We
run value iterations using the DP equation with controlled
lookahead given in eq. (8) under the transition probabilities
𝑃 𝑤 in the windy environment.

The optimal timing policy and optimal control policy are
presented in Fig. 4. One difference in a windy environment
is that the control chosen will not deterministically cause the
movement of the agent. That means if there is no update, the
controller needs to estimate the agent’s trajectory, and there
exists an estimation error. Hence, we hypothesize that the
agent needs to trigger the update more frequently than in a
non-windy environment to know his/her location and then
adjust his/her control.

Fig. 4 (a) presents the case when there is no update penalty,
i.e., 𝑂 = 0. The optimal timing policy is to observe/update
every step. The control at state 𝑥 = 6 becomes going east to
avoid being taken to the northwest corner by the wind. At
states 𝑥 = 15, 16, 17, going south is not an optimal control
anymore since if the agent goes south, there is a chance that
the wind would take the agent back to the north. When the
update penalty is small, i.e., 𝑂 = 0.1, the optimal policy is
listed in Fig. 4 (b). There are two points worth mentioning

1
→
1
→

1
→
1
→
1
→
1
↑

1
→
1
→
1
↑
1
↑

1
↓
1
↓
1
↓
1
→

0

1
↑
1
↑
1
↑

1
→

3
→
3
→
1
→
1
↑

2
→
2
→
1
↑
2
↑

1
→
1
→

2
↓
1
↓
1
↓
1
→

0

6
↑
6
↑
1
↑

1
→

(a) The update penalty 𝑂 = 0.

(b) The update penalty 𝑂 = 0.1.

6
→
6
→

6
→
6
→
6
↑
6
↑

6
→
6
→
6
↑
6
↑

6
↓
6
↓
3
↓
4
→

0

6
↑
6
↑
6
↑

1
→

6
→
6
→
6
↑
6
↑

6
→
6
→
6
↑
6
↑

6
→
6
→

6
↓
6
↓
6
↓
6
→

0

6
↑
6
↑
6
↑

6
→

(c) The update penalty 𝑂 = 40.

(d) The update penalty 𝑂 = 80.

Fig. 4: A windy gridworld: The optimal triggering time
policy 𝜏∗ (𝑥) (the upper value) and the optimal control policy
𝜋∗(𝑥) (the lower pointers) for each 𝑥 ∈ X under different
update penalties 𝑂. (For Problem 1)

when we compare the windy setting and the non-windy
setting:

1) When 𝑂 = 0.1, the agent updates more frequently in
a windy setting. For example, at 𝑥 = 5, the agent will
update the next step in a windy setting, while the agent
will update 6 steps later in a non-windy setting. One
of the reasons is that in a windy setting, the agent has
to update in the next step to make sure he/she goes
to state 𝑥 = 9 instead of being blown by the wind to
state 𝑥 = 4. This result backs up our hypothesis that
the agent in a windy world needs to trigger the update
more frequently than in a non-windy environment.
2) When 𝑂 = 40, Fig. 4 (c) shows some interesting
and unexpected results. The agent waits longer for the
next update in a windy setting than in a non-windy
setting shown in Fig. 3 (c). This result contradicts
our hypothesis that the agent tends to update more
frequently in a noisy environment. For example, if at
time 𝑡, the agent is at state 11, the next time the agent
will update is 𝑡 +6, which is longer than its counterpart
in Fig. 3 (c). One explanation is that since the control
is to head east, and the wind pushes the agent north
or west, there is no need for the agent to update its
action. Eventually, the agent will be more likely to be
at state 18 or 13 after 6 steps of ﬁxing his/her control
of going east.

When 𝑂 = 80, the optimal time policy at every step increases
to the maximum allowed waiting time ¯𝑇 = 6 to reduce the
update penalties.

6

C. Performance Guaranteed Policies

In the previous subsections, we solve Problem 1 in the
context of a gridworld and obtains the optimal self-triggering
policy 𝜇∗ = (𝜏∗, 𝜋∗). Just
to remind that we have 𝜙 :
X → A, which is the policy in the classic setting, and
the self-triggering policy 𝜇 : X → T × A in self-triggered
MDPs. To differentiate the self-triggering policy we obtain
for Problem 1 and the policy for Problem 2, we name them
𝜇∗
1

2) respectively.

= (𝜏∗
1) and 𝜇∗
The self-triggering policy 𝜇∗

1 is optimal with respect to
a speciﬁed update penalty 𝑂. However, it does not provide
an explicit performance guarantee under the original cost
criterion. Instead, the self-triggering policy 𝜇∗
2 provides a
pre-speciﬁed level of performance guarantee.

1 , 𝜋∗

2 , 𝜋∗

= (𝜏∗

2

As a result of the discussions in Section III-B, the steps to
2 for Problem 2 is given

compute a self-triggering policy 𝜇∗
as follows:

1) Compute the value function {𝑉 (𝑥), 𝑥 ∈ X} of the MDP

in the classic setting.

2) For each 𝑥 ∈ X, select Δ𝑡𝑥 = ¯𝑇.
3) Compute

#

,

.

𝑥0 = 𝑥

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
𝑥0 = 𝑥
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

˜𝑉 (𝑥) = min
𝑎 ∈A

E

Δ𝑡𝑥−1

"
Õ𝑡=0
Δ𝑡𝑥 −1

𝑎∗
𝑥 = arg min
𝑎 ∈A

E

𝛽𝑡 𝑐(𝑥𝑡 , 𝑎) + 𝛼𝛽Δ𝑡𝑥𝑉 (𝑥Δ𝑡𝑥 )

𝛽𝑡 𝑐(𝑥𝑡 , 𝑎) + 𝛼𝛽Δ𝑡𝑥𝑉 (𝑥Δ𝑡𝑥 )

#
(12)
4) If ˜𝑉 (𝑥) > 𝛼𝑉 (𝑥), set Δ𝑡𝑥 = Δ𝑡𝑥 − 1, repeat step 3).

Õ𝑡=0

"

Otherwise, 𝜏∗

2 (𝑥) = Δ𝑥, 𝜋∗

2(𝑥) = 𝑎∗
𝑥.

The optimization problem in eq. (12) admits a closed-form
solution for models such as LQG control [2] and inventory
control [8]. For the windy gridworld model, we compute
self-triggering policies following the steps for various levels
of sub-optimality. The results are presented in Fig. 5. As
we can see from Fig. 5 (a), the self-triggering policy 𝜇∗
2 can
achieve a full level of optimality, i.e., 𝑣 𝜇∗
2 (𝑥) = 𝑉 (𝑥), ∀𝑥 ∈ X,
while requiring less communication/sensing resources. When
the level of sub-optimality is 𝛼 = 1.1, as one can see from
Fig. 5 (b), at most states, the optimal timing policy is to
wait for two or more than two steps for the next update.
That means the self-triggering policy 𝜇∗
2 can save more than
50% communication/sensing resources while suffering only
10% of performance degradation. If one can tolerate a higher
level of degradation, one can set 𝛼 to a higher value and
compute the corresponding self-triggering policy 𝜇∗
2. The
cases when 𝛼 = 1.4 and 𝛼 = 2 are presented in Fig. 5 (c)
and (d). As one expects, the higher 𝛼 is (more performance
degradation one can tolerate), the fewer updates needed (less
communication/resources consumed).

V. CONCLUSIONS

In this paper, two self-triggering policies are obtained
by proposing two frameworks that convey two different
philosophies. Problem 1 introduces a soft constraint, i.e., a
update penalty that penalizes frequent use of communication
resources and Problem 2 applies a hard constraint on the

7

1
→
1
→

3
→
3
→
1
→
1
↑

2
→
2
→
1
↑
2
↑

1
↓
1
↓
1
↓
1
→

0

6
↑
6
↑
1
↑

1
→

1
→
1
→

4
→
3
→
1
→
2
↑

2
→
2
→
1
↑
2
↑

3
↓
2
↓
1
↓
2
→

0

6
↑
6
↑
4
↑

1
→

(a) The pre-speciﬁed level of
sub-optimality penalty 𝛼 = 1.

(b) The pre-speciﬁed level of
sub-optimality penalty 𝛼 = 1.1.

2
→
1
→

6
→
6
→
5
→
3
↑

5
→
4
→
1
↑
3
↑

5
↓
3
↓
1
↓
3
→

0

6
↑
6
↑
6
↑

1
→

6
→
5
→

6
→
6
→
6
→
6
→

6
→
6
→
6
→
6
→

6
↓
6
↓
4
↓
6
→

0

6
↑
6
↑
6
↑

3
→

(c) The pre-speciﬁed level of
sub-optimality penalty 𝛼 = 1.4.

(d) The pre-speciﬁed level of
sub-optimality penalty 𝛼 = 2.

Fig. 5: A windy gridworld: The optimal triggering time
policy 𝜏∗
2 (𝑥) (the upper value) and the optimal control policy
𝜋∗
2(𝑥) (the lower pointers) for each 𝑥 ∈ X under various sub-
optimality requirements. (For Problem 2)

.

level of sub-optimality while maximizing the triggering time
to resources consumption. Both policies are shown to be
effective in reducing the use of communication resources
in the gridworld examples. Future endeavors can focus on
developing stability guarantees of self-triggered policy for
controlled Markov chain, and learning when to trigger, i.e.,
leveraging reinforcement learning techniques for unknown
MDP models.

APPENDIX

A. Proof of Theorem 1
Proof. We prove the theorem by constructing a consolidated
Markov decision process problem. A close look at eq. (6)
shows that this is a discounted cost discrete-time MDP with
discount factor 𝛽, Markov states and Markov actions given
respectively by

𝑋𝑙 = (𝑥𝑡𝑙 , ˜𝑡𝑙) ∈ X × {0, 1, 2, · · · },
𝐴𝑙 = (𝑎𝑡𝑙 , Δ𝑡) ∈ A × T ,

where ˜𝑡𝑙 = 𝑡𝑙 − 𝑙, the state cost equal to

𝐶 (𝑋𝑙, 𝐴𝑙) = 𝛽 ˜𝑡𝑙

¯𝑐(𝑥𝑡𝑙 , 𝑎𝑡𝑙 , Δ𝑡) + 𝛽Δ𝑡 𝑂

,

and the skip-transition probability deﬁned in eq. (5). Hence,
the cost in eq. (6) becomes

h

i

𝑓 𝜇 (𝑥) = E

∞

"

Õ𝑙=0

𝛽𝑙𝐶 (𝑋𝑙.𝐴𝑙)

𝑋𝑙 = (𝑥, 0), 𝜇

.

#

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

The consolidated formulation can be treated as a regular
Markov decision problem. Note that the Cartesian product
of countable countably many polish spaces is still Polish.

Hence, X × {0, 1, 2, · · · } is Polish if X is polish. Thus, the
results (mainly the results available to Polish spaces) can
be derived from current Markov decision literature [21].
Applying Theorem 6.2.5 and Theorem 6.2.12 of [21], we
(cid:3)
obtain claims in Theorem 1.

B. Proof of Lemma 1
Proof. For a given 𝐿, let 𝑡𝐿+1 be the time instance of the
(𝐿 + 1)-th update. The accumulated costs before (𝑡𝐿+1) can
be written as
𝑡𝐿+1−1

𝛽𝑡𝑙 ¯𝑐(𝑥𝑡𝑙 , 𝑎𝑡𝑙 , 𝑡𝑙+1 − 𝑡𝑙)

𝛽𝑡 𝑐(𝑥𝑡 , 𝑎𝑡 )

"

Õ𝑡=0
𝐿

E

=E

"

Õ𝑙=0
𝐿

Õ𝑙=0

=E

"

𝑡=𝑡𝑙+1−1

𝛽𝑡𝑙 E

"

𝑡=𝑡𝑙
Õ

#

𝑥0 = 𝑥
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

𝑥0 = 𝑥
(cid:12)
(cid:12)
(cid:12)
(cid:12)
𝛽𝑡−𝑡𝑙 𝑐(𝑥𝑡 , 𝑎𝑡𝑙 )
(cid:12)

#

(13)

𝑥0 = 𝑥

,

#

#

𝑥𝑡𝑙
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

where we use the tower property of conditional expectation
to derive the ﬁrst equality and the second equality follows
immediately after some algebraic rearrangements. Suppose
at time instance 𝑡𝑙, 𝑙 ∈ N, the process is at state 𝑥𝑡𝑙 . Let
𝑡𝑙+1 = 𝑡𝑙 + 𝜏(𝑥𝑡𝑙 ) and 𝑎𝑡 = 𝜋(𝑥𝑡𝑙 ) for 𝑡 = 𝑡𝑙, 𝑡𝑙 + 1, . . . , 𝑡𝑙+1 − 1.
From eq. (9), we have

𝑡𝑙+1−1

E

"

𝑡=𝑡𝑙
Õ

𝛽𝑡−𝑡𝑙 𝑐(𝑥𝑡 , 𝑎𝑡 )

𝑥𝑡𝑙

≤ 𝛼𝑉 (𝑥𝑡𝑙 ) − E

𝛼𝛽𝑡𝑙+1−𝑡𝑙𝑉 (𝑥𝑡𝑙+1)

𝑥𝑡𝑙

.

(cid:12)
(cid:12)
Applying eq. (14) into eq. (13) for every 𝑙 ≤ 𝐿 yields

(cid:2)

(cid:3)
(14)

#

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E

"

≤E

"
=𝛼E

𝑡𝐿+1−1

𝛽𝑡 𝑐(𝑥𝑡 , 𝑎𝑡 )

Õ𝑡=0
𝐿

𝛽𝑡𝑙 𝛼

𝑥0 = 𝑥

#

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

𝑉 (𝑥𝑡𝑙 ) − 𝛽𝑡𝑙+1−𝑡𝑙𝑉 (𝑥𝑡𝑙+1)

Õ𝑙=0
𝑉 (𝑥𝑡0) − 𝛽𝑡𝐿+1𝑉 (𝑥𝑡𝐿+1)

(cid:0)

𝑥0 = 𝑥

𝑥0 = 𝑥
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

#
(cid:1)
≤ 𝛼𝑉 (𝑥),

(cid:2)

where we use the fact that 𝑐 : X × A → R+ produces a
non-negative 𝑉 (·), i.e., 𝑉 (𝑥) ≥ 0, ∀𝑥 ∈ X. Since 𝐿 can be
chosen arbitrarily, taking 𝐿 to inﬁnity, we have 𝑡𝑙 → ∞, and
(cid:3)
by deﬁnition of 𝑣 𝜇, 𝑣 𝜇 (𝑥) ≤ 𝛼𝑉 (𝑥) for every 𝑥 ∈ X.

(cid:12)
(cid:12)

(cid:3)

C. Proof of Theorem 2

Proof. To show that
there is a feasible set for problem
(10), it is sufﬁcient to show that for any 𝑥 ∈ X, when
Δ𝑡𝑥 = 1, there always exists an action 𝑎 𝑥 ∈ A such that
E [𝑐(𝑥, 𝑎 𝑥) + 𝛼𝛽𝑉 (𝑥1)|𝑥0 = 𝑥, 𝑎0 = 𝑎 𝑥] ≤ 𝛼𝑉 (𝑥). Let 𝜙∗ be
an optimal policy of the classic MDP. Then, by Bellman
equation, we have

E [𝑐(𝑥, 𝑎) + 𝛽𝑉 (𝑥1)|𝑥0 = 𝑥, 𝑎0 = 𝑎] = 𝑉 (𝑥),

min
𝑎 ∈A

where the minimum is attained at 𝑎∗ = 𝜙∗(𝑥). That means
there exists 𝑎 𝑥 = 𝜙∗ (𝑥) such that

𝛼E [𝑐(𝑥, 𝑎 𝑥) + 𝛽𝑉 (𝑥1)|𝑥0 = 𝑥, 𝑎0 = 𝑎 𝑥] = 𝛼𝑉 (𝑥).

Since 𝑐(·, ·) is non-negative, we have

E [𝑐(𝑥, 𝑎 𝑥) + 𝛼𝛽𝑉 (𝑥1)|𝑥0 = 𝑥, 𝑎0 = 𝑎 𝑥] = 𝛼𝑉 (𝑥).

8

This shows that for every 𝑥 ∈ X, there always exists a Δ𝑡𝑥 ∈
T such that we can ﬁnd an action 𝑎 𝑥 ∈ X so that (9) is
(cid:3)
satisﬁed.

REFERENCES

[1] W. Heemels, K. H. Johansson, and P. Tabuada, “An introduction to
event-triggered and self-triggered control,” in 2012 ieee 51st ieee
conference on decision and control (cdc).
IEEE, 2012, pp. 3270–
3285.

[2] T. Gommans, D. Antunes, T. Donkers, P. Tabuada, and M. Heemels,
“Self-triggered linear quadratic control,” Automatica, vol. 50, no. 4,
pp. 1279–1287, 2014.

[3] A. Anta and P. Tabuada, “To sample or not to sample: Self-triggered
control for nonlinear systems,” IEEE Transactions on automatic con-
trol, vol. 55, no. 9, pp. 2030–2042, 2010.

[4] X. Wang and M. D. Lemmon, “Self-triggered feedback control systems
with ﬁnite-gain l2 stability,” IEEE Transactions on Automatic Control,
vol. 54, no. 3, pp. 452–467, 2009.

[5] S. Akashi, H. Ishii, and A. Cetinkaya, “Self-triggered control with
tradeoffs in communication and computation,” Automatica, vol. 94,
pp. 373–380, 2018.

[6] Y. Gao, P. Yu, D. V. Dimarogonas, K. H. Johansson, and L. Xie, “Ro-
bust self-triggered control for time-varying and uncertain constrained
systems via reachability analysis,” Automatica, vol. 107, pp. 574–581,
2019.

[7] S. Yuvaraj and M. Sangeetha, “Smart supply chain management using
internet of things(iot) and low power wireless communication sys-
tems,” in 2016 International Conference on Wireless Communications,
Signal Processing and Networking (WiSPNET), 2016, pp. 555–558.

[8] E. A. Feinberg, “Optimality conditions for inventory control,” in
Optimization Challenges in Complex, Networked and Risky Systems.
INFORMS, 2016, pp. 14–45.

[9] P. Wie¸cek, E. Altman, and A. Ghosh, “Mean-ﬁeld game approach
to admission control of an 𝑚\𝑚\∞ queue with shared service cost,”
Dynamic Games and Applications, vol. 6, no. 4, pp. 538–566, 2016.
[10] A. Zanella, N. Bui, A. Castellani, L. Vangelista, and M. Zorzi,
“Internet of things for smart cities,” IEEE Internet of Things Journal,
vol. 1, no. 1, pp. 22–32, 2014.

[11] W. Lu, F. Fan, J. Chu, P. Jing, and S. Yuting, “Wearable computing
for internet of things: A discriminant approach for human activity
recognition,” IEEE Internet of Things Journal, vol. 6, no. 2, pp. 2749–
2759, 2019.

[12] A. Molin and S. Hirche, “On the optimality of certainty equivalence
for event-triggered control systems,” IEEE Transactions on Automatic
Control, vol. 58, no. 2, pp. 470–474, 2013.

[13] D. Maity and J. S. Baras, “Optimal event-triggered control of nonde-
terministic linear systems,” IEEE Transactions on Automatic Control,
vol. 65, no. 2, pp. 604–619, 2019.

[14] Y. Huang and Q. Zhu, “Inﬁnite-horizon linear-quadratic-gaussian
control with costly measurements,” arXiv preprint arXiv:2012.14925,
2020.

[15] D. P. Bertsekas and S. Shreve, Stochastic optimal control: the discrete-

time case. Athena Scientiﬁc,Belmont,MA, 1996.

[16] M. Gallieri and J. M. Maciejowski, “lasso mpc: Smart regulation of
over-actuated systems,” in 2012 American Control Conference (ACC).
IEEE, 2012, pp. 1217–1222.

[17] C. Cooper and N. Hahi, “An optimal stochastic control problem with
observation cost,” IEEE Transactions on Automatic Control, vol. 16,
no. 2, pp. 185–189, 1971.

[18] Y. Huang and Q. Zhu, “Cross-layer coordinated attacks on cyber-
physical systems: A lqg game framework with controlled observa-
tions,” arXiv preprint arXiv:2012.02384, 2020.

[19] ——, “A pursuit-evasion differential game with strategic information

acquisition,” arXiv preprint arXiv:2102.05469, 2021.

[20] R. Durrett, Probability: theory and examples. Cambridge university

press, 2019, vol. 49.

[21] M. L. Puterman, Markov decision processes: discrete stochastic dy-

namic programming.

John Wiley & Sons, 2014.

[22] Y. Huang, V. Kavitha, and Q. Zhu, “Continuous-time markov decision
processes with controlled observations,” in 2019 57th Annual Allerton
Conference on Communication, Control, and Computing (Allerton).
IEEE, 2019, pp. 32–39.

