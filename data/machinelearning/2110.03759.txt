1
2
0
2

t
c
O
7

]
I

A
.
s
c
[

1
v
9
5
7
3
0
.
0
1
1
2
:
v
i
X
r
a

Explanation as a process: user-centric
construction of multi-level and multi-modal
explanations ⋆

Bettina Finzel, David E. Taﬂer, Stephan Scheele, and Ute Schmid

Cognitive Systems, University of Bamberg https://www.uni-bamberg.de/en/cogsys
{bettina.finzel,stephan.scheele,ute.schmid}@uni-bamberg.de,
david-elias.tafler@stud.uni-bamberg.de

Abstract. In the last years, XAI research has mainly been concerned
with developing new technical approaches to explain deep learning mod-
els. Just recent research has started to acknowledge the need to tailor
explanations to diﬀerent contexts and requirements of stakeholders. Ex-
planations must not only suit developers of models, but also domain
experts as well as end users. Thus, in order to satisfy diﬀerent stake-
holders, explanation methods need to be combined. While multi-modal
explanations have been used to make model predictions more transpar-
ent, less research has focused on treating explanation as a process, where
users can ask for information according to the level of understanding
gained at a certain point in time. Consequently, an opportunity to ex-
plore explanations on diﬀerent levels of abstraction should be provided
besides multi-modal explanations. We present a process-based approach
that combines multi-level and multi-modal explanations. The user can
ask for textual explanations or visualizations through conversational in-
teraction in a drill-down manner. We use Inductive Logic Programming,
an interpretable machine learning approach, to learn a comprehensible
model. Further, we present an algorithm that creates an explanatory tree
for each example for which a classiﬁer decision is to be explained. The
explanatory tree can be navigated by the user to get answers of diﬀer-
ent levels of detail. We provide a proof-of-concept implementation for
concepts induced from a semantic net about living beings.

Keywords: Multi-level Explanations · Multi-modal Explanations · Ex-
planatory Processes · Semantic Net · Inductive Logic Programming

1

Introduction

In order to develop artiﬁcial intelligence that serves the human user to perform
better at tasks, it is crucial to make an intelligent system comprehensible to the
human user [20,24]. This requires giving the user an understanding of how an
underlying algorithm works (mechanistic understanding) on the one hand and

⋆ The work presented in this paper is part of the BMBF ML-3 project Transparent

Medical Expert Companion (TraMeExCo), FKZ 01IS18056 B, 2018-2021.

 
 
 
 
 
 
2

B. Finzel, D. Taﬂer, S. Scheele, U. Schmid

whether the intelligent system fulﬁlls its purpose (functional understanding) on
the other hand [26]. Explanations can foster both types of understanding. For ex-
ample, developers and experts can gain insights into the decision making process
and validate the system. Users who have no in depth technical understanding in
a ﬁeld could use explainable systems for training, for instance as implemented
in intelligent tutoring systems [27].

Methods to generate explanations are developed in Explainable Artiﬁcial
Intelligence (XAI) research [10]. A variety of techniques have been proposed,
namely approaches that generate post-hoc explanations for deep learned mod-
els [1], solutions based on interpretable symbolic approaches [30] as well as a
combination of both in hybrid systems [5]. As existing methods are reﬁned and
new methods are developed, voices are growing louder about the need for more
user-centric solutions (see for example [17,26,30]). Explanations need to ﬁt into
the context of the user, meaning the task and level of expertise. However, there
is no one-size-ﬁts-all explanation method, thus approaches have to be combined.
Current research proposes multi-modal explanations to serve the user’s vary-
ing need for information [15,8,2], in particular a combination of diﬀerent expla-
nation strategies (inductive, deductive, contrastive) with explanation modalities
(text, image, audio) to represent information accordingly and involve cognitive
processes adequately for establishing eﬀective XAI methods. Recent studies show
that presenting explanations with diﬀerent modalities can have a positive inﬂu-
ence on the comprehensibility of a decision. Existing approaches combine visual,
textual or auditory explanations in multi-modal settings [39,12].

Less focus is given to explanation as a process. Substantial work exists in the
area of argumentation, machine learning and explanation [20,38,22,13], where
conversations between systems and users follow certain patterns. However, we
found that in the current literature not enough attention is given to the fact
that functional or mechanistic understanding is developed over a period of time
and that users may need diﬀerent depths and types of information depending
on where in the process they are and which level of expertise they have [8].
Template-based explanation approaches that allow humans to drill down into
an explanation and to explore its sub-problems in terms of a hierarchical struc-
ture have previously been applied to assist ontology engineers in understanding
inferences and to correct modelling ﬂaws in formal ontologies [18] as well to
justify results from semantic search [29]. Studies have shown that users prefer
abstract and simple explanations over complex ones [19], but may ask for more
detailed and complex information [40] as well, which should ideally be presented
through the best possible and comprehensible explanation strategy. Therefore,
involving them in a dialogue, where users can get more detailed explanations
if needed, and providing multi-modal explanations at the same time to reveal
diﬀerent aspects of a prediction and its context, seems to be a promising step
towards more comprehensible decision-making in human-AI-partnerships.

Our contribution, presented in this paper, is an approach that allows users to
understand the classiﬁcation of a system in a multi-modal way and to explore a
system’s decision step by step through multi-level explanations. Multi-modality

Explanation as a process

3

is implemented in a way such that explanations can be requested by the user as
textual statements and pictures that illustrate concepts and examples from the
domain of interest. Multiple levels of explanation are implemented in such a way
that the user can pose three types of requests to the system, which then returns
explanations with three diﬀerent levels of detail: a global explanation to explain
a target class, a local explanation to explain the classiﬁcation of an example
with respect to the learned classiﬁcation model and drill-down explanations that
reveal more in-depth reasons for the classiﬁcation of an example. Thus, users
can ask for diﬀerent explanation modalities and details at any point in time in
the explanation process in accordance with their understanding of the system’s
decision that was gained until this point in time.

In a proof-of-concept implementation of our proposed approach, we represent
a domain of interest as a semantic net with additional rules to reason about
the domain. We train a model based on Inductive Logic Programming (ILP),
since it is a method that allows for generating interpretable models [30] for
relational, potentially highly complex, domains, where it may be beneﬁcial for
the understanding of a user to explain them in a step-by-step manner. In contrast
to other interpretable approaches, ILP allows for integrating knowledge and
learning relations. It has already been applied to concrete application ﬁelds, such
as medicine [31] or more abstract examples, like family trees [9], but not yet with
a combination of multi-level and multi-modal explanations. In order to generate
these explanations, we produce a so-called explanatory tree for each example
that is to be explained. Each tree is derived from a proof for the classiﬁcation
of an example and extended by references to images that illustrate concepts
in the hierarchy. Finally, we allow for interaction in natural language with the
explanatory tree through a dialogue interface. Thus, we contribute a concept
for multi-level and multi-modal explanations, present an algorithm to construct
such explanations and provide a proof-of-concept implementation to realize the
explanation process accordingly.

The sections of our paper are organized as follows. In Section 2, we ﬁrst
describe our running example, i.e., the data basis from a relational domain,
which consists of a semantic net, reasoning rules and examples. In Section 3, we
show how the chosen ILP approach is used to generate an interpretable model
by generalizing from the data basis. We proceed with describing our approach
to multi-level and multi-modal explanations in Section 4. How to generate an
explanatory tree for an example to be explained, is introduced and formalized in
Subsection 4.1. The proof-of-concept implementation is presented in Subsection
4.2 and discussed in Subsection 4.3. Finally, we show which research aspects and
extensions of the system appear to be of interest for future work in Section 5.

2 A Relational Knowledge Domain

For simple domains, where the model consists just of conjunctions of feature
values, it might be suﬃcient to give a single explanation in one modality. For
instance, the prediction that Aldo plays tennis, if the outlook is sunny and the hu-

4

B. Finzel, D. Taﬂer, S. Scheele, U. Schmid

midity is normal (see PlayTennis example from [21]) can be plausibly explained
by presenting a verbal statement of the learned constraints. In contrast, if a
model is more complex, for instance involving relational structures, multi-level
and multi-modal explanations might be helpful to foster understanding.

In the following we introduce a running example that we will refer to through-
out the paper to illustrate our approach. We assume the reader is familiar with
ﬁrst-order logic and Prolog (see [35]), but we will restate the key terminology.
The main constructs in Prolog are facts, rules and queries. Basic Prolog pro-
grams consist of a ﬁnite set of Horn clauses, where each is a ﬁnite conjunction
of literals with at most one positive literal, written in the form

A0 ← A1 ∧ A2 ∧ . . . ∧ An, where n ≥ 0.

Each Ai is an atomic formula of the form p(t1, . . . , tm), consisting of a predicate
p and terms ti, that are either a constant symbol, a variable or a composite
expression. The atom A0 is called the head and the conjunction of elements
n
i=1 Ai is called the body of a clause. Horn clauses with an empty body are
⋀
denoted as facts and express unconditional statements, otherwise they are called
rules and express conditional statements. Semantically, we can read a clause as
“the conclusion (or head ) A0 is true if every Ai in the body is true”. Facts are
literals with constant terms. Rules express a logical implication to describe that
a condition holds if a combination of literals holds, supported by given facts.
Queries can be used to retrieve information from a Prolog program. Queries
can be either facts, to check for their truth or falsity or they can be composite
expressions to retrieve terms that make those expressions true. Note that Prolog
uses :- to denote ←, “,” for conjunction ∧ and every clause ends in a full stop.
Semantic nets are a well-known formalism to represent relational and hi-
erarchical knowledge. They are constructed from a set of nodes and a set of
directed, labeled edges, where nodes represent concepts and edges denote the
semantic relationships between concepts [6,11]. A semantic network serves as a
schema to represent facts in a declarative way and can therefore be implemented,
for example in Prolog using predicates to represent relations between concepts.
Fig. 1a represents knowledge about living beings and their relations to each
other. The nodes in the semantic net represent concepts and their subset hier-
archy is expressed through edges via the is relationship, e.g., birds belong to
class animal. The has relation denotes properties of a concept, e.g., bird has
feathers. Both relations are transitive, e.g., a carnivore is an animal, because
it is a mammal which is an animal. Fig. 1b shows the corresponding Prolog en-
coding, where facts consist of a predicate (is_a or has_property) with one or
more constants as terms, e.g., is_a(plant,being) denotes that plant is a being.
Reasoning over a Prolog program P is based on the inference rule modus
ponens, i.e., from B and A ← B one can deduce A, and the ﬁrst-order resolution
principle and uniﬁcation (the reader may refer to [3,35] for more details). For a
query q it veriﬁes whether logical variables can be successfully substituted with
constants from existing facts or some previously implied conditions from P . That
means, an existentially quantiﬁed query q is a logical consequence of a program

Explanation as a process

5

is_a(plant,being).
is_a(animal,being).
is_a(flower,plant).
is_a(clover,flower).
is_a(dandelion,flower).
is_a(herb,plant).
is_a(parsley,herb).
is_a(rosemary,herb).
is_a(fish,animal).
is_a(bird,animal).
is_a(mammal,animal).
is_a(herbivore,mammal).
is_a(carnivore,mammal).
is_a(mouse,herbivore).
is_a(rabbit,herbivore).
is_a(fox,carnivore).
is_a(dog,carnivore).
is_a(stomach,organ).

has_p(being,metabolism).
has_p(animal,stomach).
has_p(fish,gills).
has_p(bird,feathers).
has_p(mammal,fur).

(a) Semantic net

(b) Prolog representation

Fig. 1: Semantic net of living beings and its representation in Prolog.

P if there is a clause in P with a ground instance A ← B1, ..., Bn, n ≥ 0 such
that B1, ..., Bn are logical consequences of P , and A is an instance of q. Thus, to
n
answer query A, the conjunctive query ⋀
i=1 Bi is answered ﬁrst, since A follows
from it. For instance, the ﬁrst reasoning rule from our running example, denoting
that is(A,B) ← is_a(A,B) can be used to query the semantic net. If we pose
the query is(animal,being) accordingly, it will be true, since the predicate
is_a(animal,being) is present in the facts. With the second reasoning rule
is(A,B) (see below) and the query is(fox,being) we could, for example, ﬁnd
out that a fox is a living being, due to transitivity.

Additionally, we introduce inheritance and generalization for both relation-
ships, e.g., to express that concept rabbit inherits has relationships from mam-
mal. For instance, a rabbit has fur, since it is a mammal that has fur. Gener-
alisation, on the other hand, allows us to express properties in a more abstract
way, e.g., that an animal has a stomach and since the more general concept of
stomach is organ, it follows that animal has organ. Transitivity and inheritance
as well as generalisation are expressed by the following reasoning rules:

:-
is(A,B)
has(A,X) :-
has(A,X) :-

is_a(A,B).
has_p(A,X).
is(A,B), has(B,X).

is(A,B) :-
has(X,Z) :-
has(A,X) :-

is_a(A,C), is(C,B).

has_p(X,Y), has(Y,Z).
has_p(A,Y), is(Y,X).

ishasbeingismetabolismisplantanimalstomachhasorganisisflowerisfishgillshasisismammalbirdfurisisrabbitfoxhasfeathershascarnivoreherbivoreisisiscloverdandelionisdogmouseisisherbisisparsleyrosemary6

B. Finzel, D. Taﬂer, S. Scheele, U. Schmid

is_a(bobby,rabbit).
is_a(fluffy,rabbit).
is_a(bella,fox).
is_a(samson,dog).
is_a(argo,dog).
is_a(tipsie,mouse).
is_a(dandelion,flower).
is_a(clover,flower).
is_a(parsley,herb).
is_a(rosemary,herb).

Fig. 2: Background knowledge T with corresponding example images.

tracks_down(bobby,dandelion).
tracks_down(fluffy,clover).
tracks_down(samson,fluffy).
tracks_down(bella,bobby).
tracks_down(bella,tipsie).
tracks_down(bobby,parsley).
tracks_down(fluffy,rosemary).
tracks_down(tipsie,rosemary).

tracks_down(argo,argo).
tracks_down(dandelion,bobby).
tracks_down(bobby,bobby).
tracks_down(blubbly,samson).
tracks_down(fluffy,argo).
tracks_down(clover,clover).
tracks_down(tipsie,bella).
tracks_down(rosemary,tipsie).

Fig. 3: Positive (E+) and negative (E−) training examples.

We will show later how successful uniﬁcations resulting from this step-wise rea-
soning procedure can be stored in a data structure, which we call explanatory
trees, to generate explanations with diﬀerent levels of detail.

The deﬁnition of the semantic net and the reasoning rules represent a so-
called background theory T [7], about living beings in this case, but it does not
include examples yet. In order to represent knowledge about examples, entries
for each example can be added to T by including respective predicates such as
shown in Figure 2, e.g., to express that bobby is an instance of the concept
rabbit.

Having the background theory complemented by knowledge about concrete
examples, which describes relational and hierarchical knowledge, we proceed to
take this as input to learn a classiﬁcation model for a given target class. From
a user’s perspective, the target class can be understood as a concept, which we
want to explain to the user. For our running example, we deﬁne a relational
classiﬁcation task, where the model has to learn if and under which conditions
a living being would track down another living being.

Consider the set of training examples in Figure 3. First, we deﬁne a set E+
of positive examples that belong to the target class. For example, the rabbit
bobby would track down dandelion. Secondly, we deﬁne a set E− of negative
examples that do not belong to the target class, i.e., we deﬁne cases that exclude
reﬂexivity, inverse transitivity within species and inverses among all species. For
instance, consider the dog argo that would not track down itself and obviously
also the ﬂower dandelion would not track down the rabbit bobby.

BobbyFluffyBellaSamsonArgoTipsiedandelioncloverparsleyrosemaryExplanation as a process

7

3 Learning an Interpretable Model with ILP

Inductive Logic Programming (ILP) will be applied to induce a classiﬁcation
model in terms of inferred rules from the given examples and background theory
as introduced before. For our running example, the model shall learn whether one
living being would track down another living being. ILP is capable of learning
a model over hierarchical and non-hierarchical relations that separates positive
from negative examples. The basic learning task of ILP (see e.g., [3] and [23]) is
described in Algorithm 1 and deﬁned as follows:
An ILP problem is a tuple (T, E+, E−

) of ground atoms, where the back-
ground theory T is a ﬁnite set of Horn clauses, and E+ and E− are disjoint
ﬁnite sets of positive and negative example instances. The goal is to construct a
ﬁrst-order clausal theory M we call model, which is a set of deﬁnite clauses that
consistently explains the examples w.r.t. the background theory. Speciﬁcally, it
must hold that the model M is complete and consistent, i.e.,

∀p ∈ E+

∶ T ∪ M ⊧ p, and ∀n ∈ E−

∶ T ∪ M /⊧ n,

where symbol ⊧ denotes the semantic entailment relation and we assume the
examples to be noise-free, i.e., this excludes false positives and false negatives.

Algorithm 1: Basic ILP algorithm

Input:
(T, E+, E−): an ILP problem
Output:
M : a classiﬁcation model
Begin:

M ← ∅, initialise the model
C ← ∅, temporary clause
P os ← E+, the set of positive training examples

While P os ≠ ∅ do

C ← GenerateN ewClause(P os, E−, T )
such that ∃p ∈ E+ ∶ T ∪ C ⊧ p
and ∀n ∈ E− ∶ T ∪ C /⊧ n
and C is optimal w.r.t. quality criterion A(C)

P os ← P os ∖ {p}
M ← M ∪ {C}

End
Return M

End

In particular, it can be seen from Algorithm 1 that it learns in a top-down
approach a set of clauses which cover the positive examples while not covering
the negative ones. A model is induced by iteratively generating new clauses C
using the function GenerateN ewClause [9] with T , E+ and E−, such that for

8

B. Finzel, D. Taﬂer, S. Scheele, U. Schmid

tracks_down(A,B) :- is(A,carnivore), is(B,herbivore).
tracks_down(A,B) :- is(A,herbivore), is(B,plant).

Fig. 4: Learned model

each C there exists a positive example p that is entailed by C ∪ T , i.e., C ∪ T ⊧ p,
while no negative example n is entailed by C ∪ T . Furthermore, it is of interest
to ﬁnd clauses such that each C is optimal with respect to some quality criterion
A(C), such as the total number of entailed positive examples. Such best clauses
are added to model M .

In our approach we use the Aleph framework with default settings for the
quality criterion [34] to generalize clauses (rules) from (T, E+, E−
). A well-
structured overview on ILP and Aleph in particular is given in Gromowski et
al. [9]. Given the examples as well as the background theory for our running
example, Aleph learns a model consisting of two rules (see Figure 4). According
to these rules, a living being A tracks down another living being B either if A
is a carnivore and B is a herbivore, or A is a herbivore and B is a plant.

Note that in our running example, we disregarded carnivorous plants as well
as cannibals. Since ILP works based on the closed world assumption, these cases
do not hold given T .

4 Multi-level and Multi-modal Explanations

Having the model induced from a relational background theory and training ex-
amples, we can now proceed to explain the model’s prediction for an individual
example with our proposed multi-level and multi-modal approach that is pre-
sented and discussed in the following subsections. We deﬁne and explain how
our proposed approach generates explanations and how the user can enter into
a dialogue with the system to receive them.

4.1 Explanation Generation

Explanations produced by our approach cover three diﬀerent levels, which can
be described as follows.

– The ﬁrst level reveals a global explanation, thus an explanation for the
target class (e.g., What does tracks down mean?). This level of detail gives
the most shallow explanation for a target class of positive examples.

– The second level gives a local explanation for the classiﬁcation of an ex-

ample by instantiating the global model.

– The third level allows for a theoretically endless amount of detail/drill-
down. The drill-down follows each clause’s body, which can consist of literals
that constitute heads of further clauses. Thus producing explanations con-
tinues as long as the user asks follow-up questions. However, if the dialogue

Explanation as a process

9

reaches a fact, the drill-down ends. The user can then ask for an image in
order to receive a visual explanation.

This means, the user can ask for the class of an example, for an explanation
for the class decision as well as ask for explanations for the underlying relational
concepts and features in a step-by-step manner. We deﬁne the terminology and
diﬀerent kinds of explanations more formally in the following paragraphs.

Let P = M ∪ T be a Prolog program, which we can query to explain the
classiﬁcation of a positive example p ∈ E+. We can create a tree ε for each
clause in C ∈ M with C ⊧ p, which we call an explanatory tree. This tree is
created as part of performing a proof for example p, given a clause C ∈ M and
the corresponding background theory T such as introduced in [3]. We extend
the proof computation by storing ε consisting of successful query and sub-query
uniﬁcations. We check, whether an example p ∈ E+ can be proven to be entailed
by T ∪ C, where C ∈ M , all generated by Algorithm 1.

The tree can be traversed to yield explanations of diﬀerent levels of detail for
our proposed dialogue approach. In order to generate a global explanation for
a target class c (see Def. 1), the set of all clauses from model M of that target
class are presented. In order to explain a positive example p globally, only the
clauses from M that entail p are presented. The global explanation corresponds
to the body of each clause C from M , while the target class is encoded in the
heads of all clauses in M . In order to generate a local explanation (see Def. 2), a
ground clause Cθ has to be found for C taken from M or T , under substitution
θ of the terms in C. A local explanation is derived from a successful proof of q
initialized to the head of Cθ, where the body of the clause Cθ is entailed by M
and T . The drill-down is applied to a local explanation Cθ (see Def. 3), and is
deﬁned as the creation of a subsequent local explanation for some ground literal
Biθ from the set of all literals from the body of Cθ, given that the body of Cθ is
not empty. If the head of Cθ is a fact and consequently the body of Cθ is empty,
the drill-down stops.

Deﬁnition 1 (Global explanation). A global explanation for a target class
c is given by the set M of all clauses in the learned model.

Deﬁnition 2 (Local explanation). A local explanation for a query q is a
ground clause Cθ where C ∈ M ∪T such that q = head(Cθ) and M ∪T ⊧ body(Cθ).

Deﬁnition 3 (Drill down of a local explanation). A drill down for a local
explanation Cθ is given by some literal in body(Cθ) = B1, B2, . . . , Bn where n ≥ 0
such that either head(Cθ) is a fact, i.e., head(Cθ) ⊢ true (body(Cθ) = ∅); or
otherwise body(Cθ) ≠ ∅ and we create a local explanation for some Biθ, where
1 > i ≤ n.

Accordingly, the explanatory tree ε is constructed such that, for each suc-
cessful uniﬁcation of a query q, ground q is the parent node of ground clauses
resulting from the proof of q. The explanatory tree ε can be traversed up and
down in a dialogue to get explanations at all deﬁned levels of detail in the form
of natural language expressions or images at leaf nodes.

10

B. Finzel, D. Taﬂer, S. Scheele, U. Schmid

4.2 Explanatory Dialogue

As presented above, our multi-level and multi-modal explanation approach al-
lows the user to enter into a dialogue with the system and ask for explanations
on three diﬀerent levels. Accordingly, users can pose various types of questions,
depending on the need for information and detail.

The input, internal program structure as well as the output of an ILP pro-
gram is represented in expressive ﬁrst-order predicate logic, in Prolog. Never-
theless, although its output is readable for humans, it is not necessarily com-
prehensible, due to factors like the complexity of the domain of interest and
the unusual syntax. We therefore realize the explanatory dialogue by generating
natural language expressions.

Template-based generation is commonly used to generate natural language
explanations for logic-based models, e.g., see Siebers et al. [32] and Musto et
al. [25] or [18]. Our template consists of domain-speciﬁc as well as domain-
independent transformation rules. For example the has a and is a relations can
be easily translated for any kind of data set, which is structured as a semantic
net. For binary predicates we connect the ﬁrst argument with the second argu-
ment through a transformed version of the predicate. In case of a n-ary predicate,
arguments are added by and. Each parent node from the explanatory tree is con-
nected to its direct children through the word because, since giving a reason is at
the core of implication. Consider the ﬁrst rule of our learned model, presented in
section 3. Transforming this rule to formulate for example a global explanation
in natural language results in the sentence:“A tracks down B, because A is a car-
nivore and B is a herbivore.”. For a local explanation, the sentence is, e.g.:“Bella
tracks down Bobby, because Bella is a carnivore and Bobby is a herbivore.”.
Beyond this template-based expression generation, our dialogue provides some
static content, such as introductory content, advice and an epilogue, when the
user quits the program.

An example of an explanatory dialogue for the classiﬁcation of the relation-
ship between Bobby and dandelion is presented in Figure 5. The diﬀerent levels
of explanations correspond to the levels of the previously generated explanatory
tree. The user can explore the whole tree, following the paths up and down,
depending on the type of request (see Figure 5). Due to space restrictions we
are only presenting the dialogue for local explanations and drill-down requests.
The user could ask for a global explanation by posing the question:“What does
tracks down mean?”. The system would return an answer based on the verbaliza-
tion of the predicate is and the constants carnivore and herbivore to explain
the ﬁrst rule of the learned model from Section 3.

4.3 Proof-of-Concept Implementation

The dialogue presented in Fig. 4.2 illustrates the proof-of-concept implementa-
tion of our approach. The code is accessible via gitlab1. In general, the approach

1 Gitlab repository of the proof-of-concept implementation: https://gitlab.rz.

uni-bamberg.de/cogsys/public/multi-level-multi-modal-explanation

Explanation as a process

11

Fig. 5: An explanatory tree for tracks_down(bobby,dandelion), that can be
queried by the user to get a local explanation why Bobby tracks down dandelion
(steps A and B). A dialogue is realized by diﬀerent drill-down questions, either
to get more detailed verbal explanations or visual explanations (steps C.a and
C.b)). Furthermore, the user can return to the last explanation (step D).

is not only applicable to the domain presented here. Any Prolog program in-
cluding a model learned with ILP can be explained with our approach. The
only precondition is, that the template-based verbalization of the nodes from
the explanatory tree can be performed by the domain-independent transfor-
mation rules. Otherwise, domain-speciﬁc transformation rules must be deﬁned
ﬁrst. Furthermore, we want to point out that the introduced algorithm fulﬁlls
correctness, since it is complete and consistent with respect to the ILP problem
solution. Finally, ﬁrst empirical investigations show that humans perform better
in decision-making and trust more into the decisions of an intelligent system, if
they are presented with visual as well as verbal explanations [37].

5 Conclusion and Outlook

Explanations of decisions made by intelligent systems need to be tailored to the
needs of diﬀerent stakeholders. At the same time there exists no one-size-ﬁts-
all explanation method. Consequently, an approach that combines explanation
modalities and that provides explanation as a step-by-step process, is promising
to satisfy various users. We presented an approach that combines textual and
visual explanations, such that the user can explore diﬀerent kinds of explanations
by posing requests to the system through a dialogue. We introduced an algorithm
as well as our proof-of-concept implementation of it.

In comparison to other explainable state-of-the-art systems that present ex-
planations rather as static content [1], our approach allows for step-by-step ex-
ploration of the reasons behind a decision of an intelligent system. Since our
approach is interpretable, it could help users in the future to uncover causal-

tracks_down(bobby,dandelion).tracks_down(bobby,dandelion) :- is(bobby,herbivore), is(dandelion,plant).is(bobby,herbivore).is(dandelion,plant).parent ofis_a(rabbit,herbivore).is(rabbit,herbivore).is_a(bobby,rabbit).is_a(dandelion,flower).is(flower,plant).bobby tracksdown dandelion.In what relationare bobby anddandelion?bobby is aherbivore anddandelion is aplant.Explain why bobbytracks downdandelion!Explain furtherwhy bobby is aherbivore!bobby is arabbit and arabbit is aherbivore.Show mebobby!Go back to lastexplanation!ABbobby tracksdown dandelion.bobby is aherbivore anddandelion is aplant.DC.aC.bis_a(flower,plant).12

B. Finzel, D. Taﬂer, S. Scheele, U. Schmid

ities between data and a system’s prediction. This is especially important in
decision-critical areas, such as medicine [14,4,31].

Other recent interactive systems enable the user to perform corrections on
labels and to act upon wrong explanations, such as implemented in the CAIPI
approach [36], they allow for re-weighting of features for explanatory debugging,
like the EluciDebug system [16] and correcting generated verbal explanations and
the underlying model through user-deﬁned constraints, such as implemented in
the medical-decision support system LearnWithME [31]. Our approach could
be extended by correction capabilities in the future, in addition to requesting
information from the system to better understand its operation or purpose. In
that way, explanations would be bi-directional.

We presented a proof-of-concept implementation of our approach that could
be technically extended in the future by combining explanations with linked
data, e.g., to integrate formal knowledge from ontologies combined with media
from open data repositories, which would allow for more ﬂexibility in the pre-
sentation of content based on semantic search2. Furthermore, we envisage to
explain decisions of deep neural networks using ILP, as presented in [28].

In future work, we aim to systematically evaluate our approach with em-
pirical user studies, recognizing design dimensions of XAI put forth by Sperrle
et al. [33]. In this context, our multi-modal and multi-level approach allows for
precise control and manipulation of experimental conditions. For instance, we
are currently preparing an empirical study to evaluate the eﬀectiveness of the
combination of multi-level and multi-modal explanations with respect to user
performance and understanding of a model. It is planned to apply the proposed
approach to a decision-critical scenario, in particular to asses pain and emotions
in human facial expressions for a medical use case.

In order to meet a user-centric explanation design and to close the semantic
gap between data, model and explanation representation, the use of hybrid ap-
proaches seems promising as a future step. Especially in the ﬁeld of image-based
classiﬁcation, it is an exciting task to consider sub-symbolic neural networks as
sensory components and to combine them with symbolic approaches as drivers
of knowledge integration. In this way, models can be validated and corrected
with the help of expert knowledge. Furthermore, hybrid approaches allow for
modeling the context of an application domain in a more expressive and com-
plex manner and, in addition, taking into account users’ context and individual
diﬀerences. Our approach is a ﬁrst step towards this vision.

Acknowledgements: The authors would like to thank the anonymous referees,
who provided useful comments on the submission version of the paper.

References

1. Arrieta, A.B., D´ıaz-Rodr´ıguez, N., Del Ser, J., Bennetot, A., Tabik, S., Barbado,
A., Garc´ıa, S., Gil-L´opez, S., Molina, D., Benjamins, R., et al.: Explainable artiﬁ-

2 Retrieval through semantic search can be performed for example over semantic me-

diawiki: https://www.semantic-mediawiki.org/wiki/Help:Semantic search

Explanation as a process

13

cial intelligence (xai): Concepts, taxonomies, opportunities and challenges toward
responsible ai. Information Fusion 58, 82–115 (2020)

2. Baniecki, H., Biecek, P.: The grammar of interactive explanatory model analysis.

CoRR abs/2005.00497 (2020)

3. Bratko, I.: Prolog Programming for Artiﬁcial Intelligence. Addison-Wesley Long-

man Publishing Co., Inc., Boston, MA, USA (1986)

4. Bruckert, S., Finzel, B., Schmid, U.: The next generation of medical decision sup-
port: A roadmap toward transparent expert companions. Frontiers in Artiﬁcial
Intelligence 3, 75 (2020)

5. Calegari, R., Ciatto, G., Omicini, A.: On the integration of symbolic and sub-
symbolic techniques for XAI: A survey. Intelligenza Artiﬁciale 14(1), 7–32 (2020)
6. Chein, M., Mugnier, M.L.: Graph-based knowledge representation: computational
foundations of conceptual graphs. Springer Science & Business Media (2008)
7. De Raedt, L., Lavraˇc, N.: The many faces of inductive logic programming. In:
Komorowski, J., Ra´s, Z.W. (eds.) Methodologies for Intelligent Systems. pp. 435–
449. Lecture Notes in Computer Science, Springer Berlin Heidelberg (1993)

8. El-Assady, M., Jentner, W., Kehlbeck, R., Schlegel, U., Sevastjanova, R., Sperrle,
F., Spinner, T., Keim, D.: Towards XAI: Structuring the Processes of Explana-
tions. In: Proceedings of the ACM CHI Conference Workshop on Human-Centered
Machine Learning Perspectives at CHI’19. p. 13 (2019)

9. Gromowski, M., Siebers, M., Schmid, U.: A process framework for inducing and

explaining datalog theories. ADAC 14(4), 821–835 (2020)

10. Gunning, D., Aha, D.: DARPA’s Explainable Artiﬁcial Intelligence (XAI) Program.

AI Magazine 40(2), 44–58 (2019)

11. Hartley, R.T., Barnden, J.A.: Semantic networks: visualizations of knowledge.

Trends in Cognitive Sciences 1(5), 169–175 (1997)

12. Hendricks, L.A., Hu, R., Darrell, T., Akata, Z.: Grounding visual explanations. In:
Proceedings of the European Conference on Computer Vision (ECCV) (2018)
13. Hilton, D.J.: A Conversational Model of Causal Explanation. European Review of

Social Psychology 2(1), 51–81 (1991)

14. Holzinger, A., Langs, G., Denk, H., Zatloukal, K., M¨uller, H.: Causability and ex-
plainability of artiﬁcial intelligence in medicine. WIREs Data Mining and Knowl-
edge Discovery 9(4) (2019)

15. Holzinger, A., Malle, B., Saranti, A., Pfeifer, B.: Towards multi-modal causabil-
ity with Graph Neural Networks enabling information fusion for explainable AI.
Information Fusion 71, 28–37 (2021)

16. Kulesza, T., Stumpf, S., Burnett, M., Wong, W.K., Riche, Y., Moore, T., Oberst, I.,
Shinsel, A., McIntosh, K.: Explanatory debugging: Supporting end-user debugging
of machine-learned programs. In: 2010 IEEE Symposium on Visual Languages and
Human-Centric Computing. pp. 41–48. IEEE (2010)

17. Langer, M., Oster, D., Speith, T., Heckrmanns, H., K¨astner, L., Schmidt, E.,
Sesing, A., Baum, K.: What Do We Want From Explainable Artiﬁcial Intelligence
(XAI)? – A Stakeholder Perspective on XAI and a Conceptual Model Guiding
Interdisciplinary XAI Research. Artiﬁcial Intelligence p. 103473 (2021)

18. Liebig, T., Scheele, S.: Explaining entailments and patching modelling ﬂaws.

K¨unstliche Intell. 22(2), 25–27 (2008)

19. Lombrozo, T.: Simplicity and probability in causal explanation. Cognitive Psychol-

ogy 55(3), 232–257 (2007)

20. Miller, T.: Explanation in artiﬁcial intelligence: Insights from the social sciences.

Artiﬁcial intelligence 267, 1–38 (2019)

14

B. Finzel, D. Taﬂer, S. Scheele, U. Schmid

21. Mitchell, T.M.: Machine Learning. McGraw-Hill, New York (1997)
22. Moˇzina, M., ˇZabkar, J., Bratko, I.: Argument based machine learning. Artiﬁcial

Intelligence 171(10), 922–937 (2007)

23. Muggleton, S., de Raedt, L.: Inductive Logic Programming: Theory and methods.

The Journal of Logic Programming 19-20, 629–679 (1994)

24. Muggleton, S.H., Schmid, U., Zeller, C., Tamaddoni-Nezhad, A., Besold, T.: Ultra-
Strong Machine Learning: comprehensibility of programs learned with ILP. Ma-
chine Learning 107(7), 1119–1140 (2018)

25. Musto, C., Narducci, F., Lops, P., De Gemmis, M., Semeraro, G.: ExpLOD: A
Framework for Explaining Recommendations based on the Linked Open Data
Cloud. In: Proceedings of the 10th ACM Conference on Recommender Systems.
pp. 151–154. ACM, Boston Massachusetts USA (2016)

26. P´aez, A.: The Pragmatic Turn in Explainable Artiﬁcial Intelligence (XAI). Minds

and Machines 29(3), 441–459 (2019)

27. Putnam, V., Conati, C.: Exploring the Need for Explainable Artiﬁcial Intelligence

(XAI) in Intelligent Tutoring Systems (ITS). Los Angeles p. 7 (2019)

28. Rabold, J., Siebers, M., Schmid, U.: Explaining black-box classiﬁers with ILP –
empowering LIME with Aleph to approximate non-linear decisions with relational
rules. In: Proc. Int. Conf. Inductive Logic Programming. pp. 105–117. Lecture
Notes in Computer Science, Springer International Publishing (2018)

29. Roth-Berghofer, T., Forcher, B.: Improving understandability of semantic search

explanations. Int. J. Knowl. Eng. Data Min. 1(3), 216–234 (2011)

30. Rudin, C.: Stop explaining black box machine learning models for high stakes
decisions and use interpretable models instead. Nat. Mach. Intell. 1(5) (2019)
31. Schmid, U., Finzel, B.: Mutual explanations for cooperative decision making in

medicine. KI-K¨unstliche Intelligenz pp. 1–7 (2020)

32. Siebers, M., Schmid, U.: Please delete that! Why should I? KI - K¨unstliche Intel-

ligenz 33(1), 35–44 (2019)

33. Sperrle, F., El-Assady, M., Guo, G., Chau, D.H., Endert, A., Keim, D.: Should
We Trust (X)AI? Design Dimensions for Structured Experimental Evaluations.
arXiv:2009.06433 [cs] (2020), arXiv: 2009.06433

34. Srinivasan, A.: The Aleph Manual. http://www.cs.ox.ac.uk/activities/

machinelearning/Aleph/

35. Sterling, L., Shapiro, E.: The Art of Prolog: Advanced Programming Techniques.

MIT Press, Cambridge, MA, USA (1986)

36. Teso, S., Kersting, K.: Explanatory interactive machine learning. In: Proceedings
of the 2019 AAAI/ACM Conference on AI, Ethics, and Society. pp. 239–245 (2019)
37. Thaler, A., Schmid, U.: Explaining machine learned relational concepts in visual
domains – eﬀects of perceived accuracy on joint performance and trust. In: Proceed-
ings of the 43rd Annual Conference of the Cognitive Science Society (CogSci’21,
Vienna). Cognitive Science Society ((to appear))

38. Walton, D.: A Dialogue System for Evaluating Explanations. In: Walton, D. (ed.)
Argument Evaluation and Evidence, pp. 69–116. Law, Governance and Technology
Series, Springer International Publishing, Cham (2016)

39. Weitz, K., Schiller, D., Schlagowski, R., Huber, T., Andr´e, E.: “Let me explain!”:
exploring the potential of virtual agents in explainable AI interaction design. Jour-
nal on Multimodal User Interfaces (2020)

40. Zemla, J.C., Sloman, S., Bechlivanidis, C., Lagnado, D.A.: Evaluating everyday

explanations. Psychonomic Bulletin & Review 24(5), 1488–1500 (2017)

