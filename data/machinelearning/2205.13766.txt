2
2
0
2

y
a
M
7
2

]

G
L
.
s
c
[

1
v
6
6
7
3
1
.
5
0
2
2
:
v
i
X
r
a

Block-coordinate Frank-Wolfe algorithm and convergence analysis
for semi-relaxed optimal transport problem

Takumi Fukunaga ∗

Hiroyuki Kasai †

May 30, 2022

Abstract

The optimal transport (OT) problem has been used widely for machine learning. It is neces-
sary for computation of an OT problem to solve linear programming with tight mass-conservation
constraints. These constraints prevent its application to large-scale problems. To address this
issue, loosening such constraints enables us to propose the relaxed-OT method using a faster
algorithm. This approach has demonstrated its eﬀectiveness for applications. However, it re-
mains slow. As a superior alternative, we propose a fast block-coordinate Frank–Wolfe (BCFW)
algorithm for a convex semi-relaxed OT. Speciﬁcally, we prove their upper bounds of the worst
convergence iterations, and equivalence between the linearization duality gap and the Lagrangian
duality gap. Additionally, we develop two fast variants of the proposed BCFW. Numerical ex-
periments have demonstrated that our proposed algorithms are eﬀective for color transfer and
surpass state-of-the-art algorithms. This report presents a short version of [1]. The source code
is available at https://github.com/hiroyuki-kasai/srot.

Published in IEEE ICASSP2022: https://doi.org/10.1109/ICASSP43922.2022.9746032 [2].

1 Introduction

The optimal transport (OT) problem seeks an optimal transport plan or transport matrix by solv-
ing the total minimum transport cost from sources to destinations.
It can express the distance
between two probability distributions, which is known as Wasserstein distance [3]. This problem
has been applied to widely diverse machine learning problems [4, 5, 6, 7]. Among the OT problem
formulations, the Kantorovich formulation is represented as convex linear programming (LP) [8].
Thereby, many dedicated solvers such as an interior-point method can obtain the solutions.
It
is, nevertheless challenging to solve large-scale problems eﬃciently because its computational cost
increases cubically in terms of the data size.

To address this issue, the Sinkhorn algorithm [9], an entropy-regularized approach, has been
proposed because it functions eﬀectively and enables a parallel implementation. This faster compu-
tation derives from a diﬀerentiable and unconstrained convex optimization. Moreover, some stabler
variants address overﬂow caused by small values of the regularizer, but they become slower [10].
Along another avenue of development, some works have reported that the strict constraints in the
OT problem lead to performance degradation in some areas where mass is not necessarily preserved.

∗Department of Communications and Computer Engineering, School of Fundamental Science and Engineering,

WASEDA University, 3-4-1 Okubo, Shinjuku-ku, Tokyo 169-8555, Japan (e-mail: f takumi1997@suou.waseda.jp)

†Department of Communications and Computer Engineering, School of Fundamental Science and Engineering,

WASEDA University, 3-4-1 Okubo, Shinjuku-ku, Tokyo 169-8555, Japan (e-mail: hiroyuki.kasai@waseda.jp)

1

 
 
 
 
 
 
This speciﬁc problem has been tackled recently by relaxing such tight constraints. A constraint-
relaxed approach has attracted attention for use in various areas such as color transfer [11] and
multi-label learning [12]. Nevertheless, it remains slow.

To develop a faster solver generating sparser solutions for the OT problem and to work on its
convex semi-relaxed formulation, this report is the ﬁrst to describe a block-coordinate Frank–Wolfe
(BCFW) algorithm with theoretical analysis. The FW algorithm is a variant of projection-free
linear convex programming using a linear optimization oracle [13]. Thereby, an FW algorithm
appropriate to large datasets has spread recently for the OT problem [14, 15, 16, 17]. Furthermore,
it can produce sparser solutions. However, because this algorithm requires solutions of all columns
of the transport matrix at every iteration, its computational cost is not negligible for the very large
matrix size. Therefore, we further incorporate a coordinate descent approach [18], which updates
one column randomly every iteration. The approach can achieve much smaller computational costs
and faster convergence [19]. Several papers have already described this method in the OT literature
[20, 21], but its concrete theoretical properties for the relaxed OT problem have not been revealed.
Consequently, the analyses presented in this paper can provide several fundamentally important
theoretical contributions. The ﬁrst contribution is to yield an upper-bound of the curvature constant
without using a linear oracle, as in [18]. Then, we can obtain iteration complexities for ǫ-optimality
with FW and BCFW algorithms for the semi-relaxed OT problem. The second contribution is to
show equivalence between linearization duality gap, a special case of the Fenchel duality gap and the
Lagrangian duality gap. It becomes a certiﬁcation of the current approximation values to monitor
the convergence. We can use this property for the stopping condition in the proposed algorithm.
The third contribution is to develop two fast variants of BCFW, i.e., the BCFW algorithms with
pairwise-steps and away-steps. The ﬁnal contribution is to demonstrate numerical evaluations of the
color transfer problem, detailed analysis of the proposed BCFW, and its eﬀectiveness including its
faster variants in the semi-relaxed OT problem. Hereinafter, we designate vectors as bold lower-case
letters a, b, c, · · · and present matrices as bold-face upper-case letters A, B, C, · · · .

2 BCFW for semi-relaxed OT problem

The evaluation in this paper speciﬁcally examines the semi-relaxed problem [22] with Φ(x, y) =
1
2λ kx − yk2

2 because it is not only smooth but also convex. This is formulated as

f (T) := hT, Ci +

min
T ≥ 0,
TT 1m=b

(cid:26)

1
2λ

kT1n − ak2

2(cid:27)

,

(1)

where λ is a relaxation parameter. The domain is transformed into M = b1∆m ×b2∆m ×· · ·×bn∆m,
where bi∆m represents the simplex of the summation bi.

2.1 Proposed BCFW algorithm (Baseline)

We ﬁrst consider the FW algorithm for this problem; then we propose a faster block-coordinate
Frank–Wolfe algorithm.
Frank–Wolfe (FW) algorithm. The gradient ∇f (T) ∈ Rmn is given as (∇1f (T)T , . . . , ∇nf (T)T )T
where ∇if (T) ∈ Rm stands for the gradient on the i-th variable block bi∆m. The linear subproblem
is equal to

si = biej = bi arg min

hek, ∇if (T(k))i,

ek∈∆m,k∈[m]

(2)

2

Algorithm 1 BCFW for semi-relaxed OT
Require: T(0) = (t(0)
1: for k = 0 . . . K do
2:

1 , . . . , t(0)

Select index i ∈ [n] randomly
Compute si = bi arg min

3:

4:

5:
6: end for

ek∈∆m,k∈[m]

Compute stepsize γ
Update t(k+1)

= (1 − γ)tk

i

i + γsi

n ) ∈ b1∆m × · · · × bn∆m

hek, ∇if (T(k))i

where ej represents the extreme point on probability simplex [23]. The computational complexity
of the subproblem can be improved greatly (2). Its minute analysis is discussed in Section 2.4.
After solving the solutions S = (s1, s2, . . . , sn) ∈ Rm×n, one must search an optimal step size γ.
Classically, a decay stepsize (DEC) is often used in the FW algorithm, where γ = 2/(k + 2) with
the iteration number k. In this issue, a line-search algorithm can also be developed. Concretely,
the quadraticity of the objective enables us to solve minγ∈[0,1] f ((1 − γ)x + γs), and to calculate γ
directly.
Block-coordinate Frank–Wolfe (BCFW) algorithm. We propose the block-coordinate Frank–
Wolfe algorithm (BCFW) for the semi-relaxed problem converting the feasible set M into a Carte-
sian product. The algorithm procedure is shown in Algorithm 1. It requires, at every iteration,
solutions of the subproblem on the variable block selected randomly. Speciﬁcally, it is equivalent to
(2), but we solve the subproblem only for the i-th column, which is selected randomly. We adopt
γ = 2n/(k + 2n) as the stepsize, which is important for the convergence guarantee, as in Theorem
2.1. Similarly to the FW algorithm, an exact line-search (ELS) algorithm can also be applied. The
optimal stepsize γLS is calculated as

γLS =

λht(k)

i − si, cii + ht(k)

i − si, T(k)1n − ai

kt(k)

i − sik2

,

(3)

where ti represents the i-th column of T and si denotes the solution of the i-th subproblem in
(2). As in Theorem 2.2, the duality gap can be adopted as the stopping criterion. In a practical
implementation, one can monitor the value of the duality gap because the subproblem is solved at
every iteration. It is noteworthy that the BCFW algorithm can not compute the duality gap value
from the solution of the value of (2) because one must solve the subproblems on all the variable
blocks. Therefore, computing the duality gap, if attempted every iteration, increases the runtime
heavily. As a result, the beneﬁt of the cheaper iteration complexity in BCFW is lost. Consequently,
we monitor the duality gap every n iteration, of which period is equal to that of the FW algorithm.
Finally, we adopt two rules for sampling a column at each iteration: uniform random sampling
and random permutation sampling. The former randomly selects i ∈ [n], of which convergence
analysis is given in Section 2.2. The latter runs a cyclic order on a permuted index. Those
algorithms are represented, respectively, as BCFW-U and BCFW-P. Another sampling scheme
using the duality gap is explained in [1].

2.2 Theoretical results of the baseline BCFW algorithm

Convergence analysis. We provide the worst convergence iteration of the proposed algorithm.

Theorem 2.1. Let T∗ be the optimal solution of the semi-relaxed OT problem in (1). Consider
Algorithm 1 under the arbitrary initial point T(0) with a decay stepsize rule k = 2n
k+2n . Then,

3

one obtains E[f (T(k))] − f (T∗) ≤ 2n
is the curvature constant with ≤ 4
kCk∞ ≤ 2
it requires an additional number of 2nh0

λ , then Algorithm 1 requires at most the number of O( n

k+2n (C ⊗

f + h0), where h0 = f (T(0)) − f (T∗), and where C ⊗
f
λ . Additionally, given an approximation precision constant ǫ, if
λǫ ) for its convergence. Otherwise,

ǫ ≤ 2nkCk∞

ǫ

.

For its proof, we ﬁrst upper-bound the curvature constant C ⊗

f , in consideration of the twice
diﬀerentiability of f (T) and the simplex structure. For arbitrary T(0), we can also upper-bound
g(T(0)) by kCk∞. Finally, the upper-bound of the complexity is derived. It is particularly interesting
that our proposed algorithm requires additional iterations when kCk∞ > 2
λ . The full proof is given
in the [1].

Linearization duality gap and stopping criterion. Linearization duality is a special case of the
Fenchel duality in the FW algorithm [18, 24]. Its duality gap can be calculated at the points x as
g(x) = maxs′∈M hx−s′, ∇f (x)i = hx−s, ∇f (x)i, where M is convex. It is noteworthy that adding
f (x) onto the linearization duality is equivalent to the Wolfe duality [23]. For the semi-relaxed OT
problem, we speciﬁcally give the equivalence between the linearization gap, denoted as g(T), and
the Lagrangian duality gap.

Theorem 2.2. Consider the semi-relaxed problem in (1). The linearization duality gap is provided
as g(T) = hT − S, Ci + 1
λ hT1n − S1n, T1n − ai, where S is the solution of the subproblem (2).
Then, the linearization duality gap g(T) is equivalent to the Lagrangian duality gap of (1).

The full proof is presented in [1], but its proof sketch is the following: We calculate the La-
grangian duality gap gL(T) directly as the diﬀerence the primal and dual function for the semi-
relaxed problem. Finally, we prove the equivalence between gL(T) and g(T) deﬁned in this theorem.
This theorem enables us to use g(T) as both the linearization duality gap and the Lagrangian duality
gap. Therefore, g(T) is appropriate to the stopping criterion of the proposed algorithms.

2.3 Fast variants of BCFW algorithm

In accordance with reports of earlier work [25, 26, 27], this paper introduces the away-steps and
the pairwise-steps into the proposed BCFW algorithm in an attempt to raise the convergence rate.
First, we deﬁne the active set on the i-th (i ∈ [n]) variable block Si as Si = {ej ∈ ∆m : αej > 0},
where αej represents the coeﬃcient of the j-th extreme point ej. To remove the atoms, a new
subproblem is deﬁned as vi = arg minv′∈Si hv′, ci + 1
λ (T1n − a)i. Similarly to the subproblem
(2), we can also solve this problem. Hereinafter, we deﬁne two directions as the FW direction
dFW = si − t(k)
i − vi. Of those two directions, we adopt the
one which decreases the value of the function. Subsequently, we calculate the stepsize γ satisfying
minγ∈[0,γmax] f ((1 − γ)x + γs). This stepsize γLS is calculated by replacing t(k)
i − si in (3) with d,
where d is dFW or dAway. Then, it is necessary to update both the selected column vector t(k)
and
the selected active set S (k)
. Similarly, we can construct the block-coordinate Pairwise Frank–Wolfe
(BCPFW). In the BCPFW, only two atoms si and vi are used. Setting the direction dPair = si − vi
yields the movement between only two atoms and improvement of the convergence rate. Additional
details are presented in [1].

and the Away direction dAway = t(k)

i

i

i

2.4 Computational complexity analysis

We present a complexity analysis of the semi-relaxed OT problem. The column vector of the
transport matrix T is independent of other columns because the domain of the subproblem (2)

4

represents the Cartesian product of the probability simplex. Therefore, we can solve (2) easily
[24] without solving LP directly. Thus, the total computational complexities O((mn)3 log(mn))
are reduced to O(mn), which speeds up the time. Although the FW algorithm must call linear
oracle for the n variable block, the BCFW algorithm calls oracle only for one variable block selected
randomly at every iteration. The convergence rate of the BCFW algorithm is equal to that of the
FW algorithm. Results show that the computational complexities of the BCFW algorithm arise
more quickly than those of the FW algorithm. Finally, as for the two fast variants, the complexity
of the away-steps and that of pairwise-steps is O(|Si|) because their procedure is the same as that
of the subproblem (2). Because these algorithms also require the solution of the subproblem (2),
the complexity of those algorithms is equal to O(m + |Si|) at every iteration. Furthermore, because
the inequality 1 ≤ |Si| ≤ m holds against its cardinality, the total computational complexities of
BCAFW and BCPFW can be approximated by O(m).

3 Numerical Evaluation

Conﬁguration. This section ﬁrst evaluates convergence behaviors of the baseline of the proposed
BCFW in Section 2.1. Then, the comparisons among the fast variants of BCFW proposed in
Section 2.3 are performed. Finally, we evaluate the color-transferred images visually using the
baseline algorithm of BCFW. Note that, hereinafter, this section uses BCFW-U for the baseline
algorithm unless stated otherwise. Algorithms are conﬁgured from the same initialization point T(0),
of which the ﬁrst row is set b. The algorithms are stopped when the iteration count reaches 1000
epochs unless stated otherwise. We selected λ = {10−7, 10−9} from our preliminary evaluations.
All the experiments are executed on a 3.7 GHz Intel Core i5 CPU with 64 GB RAM. Finally,
this experiment addresses the OT-based color transfer problem [28] and uses two images, which
are source image “Gangshan District” by Boris Smokrovic, and the reference image “Minnesota
landscape arboretum” by Shannon Kunkle.
Evaluations of baseline BCFW. We compare our proposed algorithms with the projected gra-
dient descent (PGD) method and the fast iterative shrinkage-thresholding algorithm (FISTA) [29]
for the semi-relaxed OT problem deﬁned in (1). We denote the corresponding FW and BCFW algo-
rithms respectively as FW-DEC and FW-ELS, and BCFW-U-DEC and BCFW-U-ELS. We explain
convergence performances in one speciﬁc case with λ = 10−7 and setting m = n = 4096. Fig. 1
presents the results. As the objective value and duality gap in Fig. 1.(a)–(c) show, the BCFW-U
algorithms are faster than the FW algorithm. In terms of iteration, the exact line-search stepsize
rules outperform the decay stepsize rules in both the FW and BCFW-U algorithms. However, the
decay stepsize is slightly more eﬀective than the exact line-search in terms of computational time.
Finally, because of the constructibility of the algorithms, we can be assured that our proposed
algorithms output sparser solutions.

Evaluations of fast variants. This subsection presents evaluation of the performance improve-
ments by two fast variants discussed in Section 2.3, i.e., the BCFW with away-steps (BCAFW)
and the BCFW with pairwise-steps (BCPFW). The experimental initializations are the same as
the evaluation above. We explain the convergence performances when using the exact line-search
stepsize rule (ELS), λ = 10−7 and m = n = 4096. Fig. 2 presents the results. From the duality gap
in Fig. 2.(a), one can understand that both the two fast variants BCAFW and BCPFW achieve
faster convergence in terms of iteration. Also, BCPFW provides much better performances in terms
of computational time.

Evaluations of color-transformed images. This subsection presents speciﬁc evaluation of the
eﬀects of the relaxation parameter λ on the color-transferred image using real-world images. A

5

FW-DEC
FW-ELS
BCFW-U-DEC
BCFW-U-ELS
PGD
FISTA

400

350

300

250

t
s
o
C

200

150

100

400

350

300

250

t
s
o
C

200

150

100

FW-DEC
FW-ELS
BCFW-U-DEC
BCFW-U-ELS
PGD
FISTA

0

200

400

600

800

1000

0

100

200

300

400

500

Iteration

Time [sec]

(a) objective value : f (T)

(b) objective value (time): f (T)

108

106

104

102

p
a
g
y
t
i
l
a
u
D

100

0

FW-DEC
FW-ELS
BCFW-U-DEC
BCFW-U-ELS

0.9
0.8
0.7
0.6
0.5

0.4

0.3

0.2

y
t
i
s
r
a
p
S

FW-DEC
FW-ELS
BCFW-U-DEC
BCFW-U-ELS
PGD
FISTA

200

400

600

800

1000

0

200

400

600

800

1000

Iteration

(c) duality gap : g(T)

Iteration

(d) sparsity

Figure 1: Convergence of baseline BCFW and others.

suitable setting is discussed to obtain images that are visually natural. We consider the case of
m = n = 32. The baseline BCFW with the decay stepsize rule is used: BCFW-U-DEC. Fig. 3
presents the results. The single color approaches the b-weighed averaged color of the centroids
of the reference image. Next, the single color is scattered. Naturally color-transferred images are
generated. However, as the optimization process proceeds, the image at k = 106 shows some artiﬁcial
gray pixels in the background. Fig. 3.(b) presents a heat map of the obtained transport matrices
at the k-th iteration, T(k). As a result of T(3000), the matrix includes similar values along columns.
In other words, the vertical blocks with similar colors can be recognized. However, this vertical
structure disappears as the iteration proceeds.

6

 
BCFW-U-ELS
BCAFW-U-ELS
BCPFW-U-ELS

100

98

96

94

92

90

88

t
s
o
C

0

100

200

300

400

500

Time [sec]

102

p
a
g
y
t
i
l

a
u
D

101

100

0

BCFW-U-ELS
BCAFW-U-ELS
BCPFW-U-ELS

200

400

600

800

1000

Iteration

(a) objective value (time): f (T)

(b) duality gap : g(T)

Figure 2: Convergence of fast variants: BCAFW and BCPFW.

4 Conclusion

After proposing BCFW for a convex semi-relaxed OT problem, we proved its upper-bounds of the
worst convergence iterations, in addition to the equivalence between the linearization duality gap
and the Lagrangian duality gap. Numerical evaluations demonstrated that our proposed algorithms
outperform state-of-the-art algorithms across diﬀerent settings.

7

 
k = 3300

k = 106

(a) transition of color-transferred images (λ = 10−9)

k = 3000

k = 106

(b) transition of the heat map of row-wise normalized transport matrices T(k) (λ = 10−9)

Figure 3: Transition of color-transferred images and heat-map of row-wise normalized transport
matrices (m = n = 32).

8

References

[1] T. Fukunaga and H. Kasai. Fast block-coordinate Frank-Wolfe algorithm for semi-relaxed

optimal transport. arXiv preprint arXiv:2103.05857, 2021.

[2] T. Fukunaga and H. Kasai. Block-coordinate Frank–Wolfe algorithm and convergence analysis

for semi-relaxed optimal transport problem. In ICASSP, 2022.

[3] G. Peyre and M. Cuturi. Computational optimal transport. Found. Trends Mach. Learn.,

11(5-6):355–607, 2019.

[4] L. Chen, Z. Gan, Y. Cheng, L. Li, L. Carin, and J. Liu. Graph optimal transport for cross-

domain alignment. In ICML, 2020.

[5] J. Huang, Z. Fang, and H. Kasai. LCS graph kernel based on Wasserstein distance in longest

common subsequence metric space. Digit. Signal Process., 189:108281, 2021.

[6] H. Kasai. Multi-view Wasserstein discriminant analysis with entropic regularized Wasserstein

distance. In ICASSP, 2020.

[7] T. Fukunaga and H. Kasai. Wasserstein k-means with sparse simplex projection. In ICPR,

2020.

[8] L. Kantorovich. On the transfer of masses (in russian). Dokl. Akad. Nauk, 37(2):227–229, 1942.

[9] M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transportation distances.

In NeurIPS, 2013.

[10] L. Chizat, G. Peyr´e, B. Schmitzer, and F.-X. Vialard. Scaling algorithms for unbalanced

optimal transport problems. Math. Comput., 87:2563–2609, 2018.

[11] J. Rabin, S. Ferradans, and N. Papadakis. Adaptive color transfer with relaxed optimal trans-

port. ICIP, 2014.

[12] C. Frogner, C. Zhang, H. Mobahi, M. Araya-Polo, and T. Poggio. Learning with a Wasserstein

loss. In NeurIPS, 2015.

[13] M. Frank and P. Wolfe. An algorithm for quadratic programming. Nav. Res. Logist. Q.,

3:95–110, 1956.

[14] S. Ferradans, N. Papadakis, G. Peyr´e, and J.-F. Aujol. Regularized discrete optimal transport.

SIAM J. Imaging Sci., 7(3):1853–1882, 2013.

[15] A. Rakotomamonjy, R. Flamary, and N. Courty. Generalized conditional gradient: analysis of

convergence and applications. arXiv preprint arXiv:1510.06567, 2015.

[16] N. Courty, R. Flamary, D. Tuia, and A. Rakotomamonjy. Optimal transport for domain

adaptation. IEEE Trans. Pattern Anal. Mach. Intell., 39(9):1853–1865, 2017.

[17] F.-P. Paty and M. Cuturi. Subspace robust Wasserstein distances. In ICML, 2019.

[18] S. Lacoste-Julien, M. Jaggi, M. Schmidt, and P. Pletscher. Block-coordinate Frank-Wolfe

optimization for structural SVMs. In ICML, 2013.

9

[19] S. J. Wright. Coordinate descent algorithms. Math. Program., 151:3–34, 2015.

[20] M. Perrot, N. Courty, R. Flamary, and A. Habrard. Mapping estimation for discrete optimal

transport. In NeurIPS, 2016.

[21] V. Titouan, I. Redko, R. Flamary, and N. Courty. CO-optimal transport. In NeurIPS, 2020.

[22] M. Blondel, V. Seguy, and A. Rolet. Smooth and sparse optimal transport. In AISTATS, 2018.

[23] K. L. Clarkson. Coresets, sparse greedy approximation, and the Frank-Wolfe algorithm. ACM

Trans. Algorithms, 6(4):1–30, 2010.

[24] M. Jaggi. Revisiting Frank-Wolfe: Projection-free sparse convex optimization. In ICML, 2013.

[25] P. Wolfe. Integer and nonlinear programming. Amsterdam : North-Holland Pub. Co., 1970.

[26] B. F. Mitchell, V. F. Dem’yanov, and V. N. Malozemov. Finding the point of a polyhedron

closest to the origin. SIAM J. Control., 12(1):19–26, 1974.

[27] A. Osokin, J.-B. Alayrac, I. Lukasewitz, P. Dokania, and S. Lacoste-Julien. Minding the gaps

for block Frank-Wolfe optimization of structured SVMs. In ICML, 2016.

[28] F. Pitie and A. Kokaram. The linear Monge-Kantorovitch linear colour mapping for example-

based colour transfer. In CVMP, 2007.

[29] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse

problems. SIAM J. Imaging Sci., 2(1):182–202, 2009.

10

