2
2
0
2

g
u
A
8
1

]
L
M

.
t
a
t
s
[

1
v
6
2
6
8
0
.
8
0
2
2
:
v
i
X
r
a

CP-PINNS: CHANGEPOINTS DETECTION IN PDES USING
PHYSICS INFORMED NEURAL NETWORKS WITH
TOTAL-VARIATION PENALTY

Zhikang Dong

Paweł Polak

Department of Applied Mathematics and Statistics
Stony Brook University
Stony Brook, NY, 11794, USA
{zhikang.dong.1, pawel.polak}@stonybrook.edu

ABSTRACT

We consider the inverse problem for the Partial Differential Equations (PDEs) such that the parameters
of the dependency structure can exhibit random changepoints over time. This can arise, for example,
when the physical system is either under malicious attack (e.g., hacker attacks on power grids and
internet networks) or subject to extreme external conditions (e.g., weather conditions impacting
electricity grids or large market movements impacting valuations of derivative contracts). For that
purpose, we employ Physics Informed Neural Networks (PINNs)—universal approximators that
can incorporate prior information from any physical law described by a system of PDEs. This prior
knowledge acts in the training of the neural network as a regularization that limits the space of
admissible solutions and increases the correctness of the function approximation. We show that
when the true data generating process exhibits changepoints in the PDE dynamics, this regularization
can lead to a complete miss-calibration and a failure of the model. Therefore, we propose an
extension of PINNs using a Total-Variation penalty which accommodates (multiple) changepoints
in the PDE dynamics. These changepoints can occur at random locations over time, and they
are estimated together with the solutions. We propose an additional reﬁnement algorithm that
combines changepoints detection with a reduced dynamic programming method that is feasible for
the computationally intensive PINNs methods, and we demonstrate the beneﬁts of the proposed
model empirically using examples of different equations with changes in the parameters. In case of
no changepoints in the data, the proposed model reduces to the original PINNs model. In the presence
of changepoints, it leads to improvements in parameter estimation, better model ﬁtting, and a lower
training error compared to the original PINNs model.

1

Introduction

Deep learning, and machine learning methods are widely studied and used in academia and industry, e.g., [1, 2, 3].
They perform successfully in tasks such as dimensionality reduction [4], computer vision [5], and time series analysis
[6]. More recently, deep learning models have shown the ability to approximate a majority of nonlinear functions in
the context of complex dynamical systems [7, 8, 9]. [10] introduced Physics Informed Neural Networks (PINNs)—an
artiﬁcial intelligence approach that is suitable for solving PDEs and for solving inverse problems via data-driven
discovery of the system dynamics for a given class of PDEs. Compared with traditional numerical methods [11, 12, 13],
PINNs allow to solve supervised learning tasks while respecting any given law of physics described by general nonlinear
PDEs. They utilize neural networks as general estimators and encode any underlying physical laws as prior information.
PINNs have been widely applied in many ﬁelds, such as optics [14], cardiology [15], and power system [16].

There are many different formulations of PINNs, e.g., Physics-informed generative adversarial networks [17] which
have stochastic differential equations induced generators to tackle very high dimensional problems; [18] rewrite PDEs

 
 
 
 
 
 
as backward stochastic differential equations and designs the gradient of the solution as policy function, which is
approximated by deep neural networks. In reinforcement learning, [19] propose a Bayesian neural network as the prior
for PDEs and use Hamiltonian Monte Carlo and variational inference as the estimator of the posterior, resulting in more
accurate prediction and no overﬁtting.

Although PINNs are considered universal approximators, they cannot properly capture dynamics with steep slopes,
singularities, and sharpe changes. Based on Petrov-Galerkin method, the hp-variational PINNs (hp-VPINNs) [20] is
an extension of PINNs method that allows for localized parameters estimation with given test functions via domain
decomposition. The hp-VPINNs generate a global approximation to the weak solution of the PDE with local learning
algorithm that uses a domain decomposition which is preselected by the user. This leads to high accuracy of the solution.

For dynamical nonlinear system analysis, even if the law is explicitly known, there are possible discrepancies (nonlinear
frictions, wind resistance, etc.) between the model and the real-world dynamics which can lead to signiﬁcant deviations
from the true, measured behavior. [21] propose an algorithm to identify and eliminate these discrepancies by using a
sparse identiﬁcation algorithm that includes an (cid:96)1 penalty term in the PINNs loss function.

The hp-VPINNs accommodate local domain decomposition but the solution is still described by a given PDE system
and the user needs to know where is the rapid change in the dynamics in order to optimally decompose the domain
prior to estimation. One open problem is how to use PINNs methods in the presence of unknown changepoints in
a dynamical system. Many ﬁelds have taken advantage of changepoints detection methods, e.g., [22] show how
changepoint detection algorithms work on video-based surveillance. [23, 24] propose changepoints detection methods
to distinguish gaps between silence, sentences, and noise in speech recognition. In medicine, changepoints detection
methods are used to ﬁnd heart issues in real-time monitoring [25, 26]. These methods can also detect disturbances and
determine their geographic location in large-scale power grids [27]. [28] show how changepoints detection methods can
analyze anomalies and attacks in the computer networks.

We introduced a changepoints detection method for the PINNs model based on total variation penalty as in [29, 30].
This technique extends PINNs to the case of continuous time dynamical systems with breakpoints by endowing loss
function with an additonal regularization term similar to [21]. The proposed two-step algorithm, ﬁrst detects candidates
for the changepoints locations, then a reduced dynamic programming step searches through the list of these candidates
and estimates PINNs between them to provide the best domain decomposition and the corresponding domain speciﬁc
solutions. Our CP-PINNs model, in case of no changepoints in the data, reduces to the original PINNs model, and
as shown in the empirical results below, when changepoints are present in the data, it substantially outperforms the
original PINNs approach.

The rest of the paper is organized as following. Section 2 gives an overview of our methodology to detect changepoints
locations and estimate the unknown parameters. In Section 3 we consider the 1D advection-diffusion Equations and 2D
Navier-Stokes Equations as examples to illustrate our approach. Section 4 gives extensions and ideas for future work.
Section 5 concludes.

2 Model and Estimation

We consider a general form of parameterized and nonlinear continuous partial equations with breakpoints

ut + N [u; λ(t)] = 0,
(1)
where (x, t) ∈ Ω × (0, T ], Ω ⊂ Rd is the bounded domain, N [·; λ(t)] is a nonlinear operator parameterized by a
function λ(t), and u(x, t) denotes the latent solution to the above equation. Regarding the function λ(t), we make the
following assumption
Assumption 2.1. The function λ(t) : [0, T ] → R is piecewise-constant, bounded, with a ﬁnite but unknown number
of discontinuities k∗, located at some of the observations, i.e., λ(t) = (cid:80)k∗
i=1 λi1[ti−1,ti)(t), for 0 = t0 < t1 < . . . <
tk∗ < T .

2.1 Physics-Informed Neural Network

Following [10], in order to approximate the true solution of the PDE in (1) we use the neural network approximation
uN N (x, t; W, b) given by

uN N (x, t; W, b) = g ◦ T ((cid:96)) ◦ T ((cid:96)−1) ◦ · · · ◦ T (1)(x).
For each hidden layer, the nonlinear operator T (i)(x) = σ (Wix + bi) with weights Wi ∈ RNi×Ni−1 and biases
bi ∈ RNi, where N0 = d is the input dimension and in the output layer, the operator g : RN(cid:96) → R is a linear activation

2

function. Next, deﬁne a feature function

and the corresponding boundary and initial residual functions

f (x, t) = ut + N [u; λ(t)],

rb(uN N , x, t) = uN N − u,
∀(x, t) ∈ ∂Ω × (0, T ]
r0(uN N , x, t) = uN N − u0, ∀(x, t) ∈ Ω × {t = 0}.

Averaging over observations on their domains, we obtain the ﬁtting loss function for the training of the model

Lf =

1
Nb

Nb(cid:88)

i=1

(cid:12)
(cid:12)rb

(cid:0)xi

b, ti
b

(cid:1)(cid:12)
2
(cid:12)

+

1
N0

N0(cid:88)

i=1

(cid:12)
(cid:12)r0

(cid:0)xi

0

(cid:1)(cid:12)
2
(cid:12)

.

Similarly, for the observations inside the domain, the structure loss is deﬁned as

Ls =

1
N

N
(cid:88)

i=1

(cid:12)fN N (xi, ti)(cid:12)
(cid:12)
(cid:12)

2

.

(2)

(3)

Gathering together these two loss functions we obtain the total loss for the training of the model.

Standard PINNs model assumes λ(t) ≡ λ for all t ∈ [0, T ]. In order to accommodate Assumption 2.1 in the estimation
process we need to introduce additional regularization for the changes in λ(t). For that purpose we use a total variation
penalty on the ﬁrst difference in λ(t).

2.2 Total Variation Regularization

Let

V λ =

T −1
(cid:88)

i=1

δ(ti) (cid:12)

(cid:12)∆λ(ti)(cid:12)
(cid:12) ,

(4)

where ∆λ(ti) = λ(ti+1) − λ(ti), and δ(t) is a U-shape linear function of t which increases the penalty strength closer
to the edges in order to avoid estimation instabilities around the edge (in the experiments below we use δ(t) =
T −t ).

(cid:113) T

We deﬁne our loss function

L = Lf + Ls + V λ,

(5)

where the last term is a sparsity inducing penalty similar to one used in [29] for changepoints detection in linear models.
The third term in (5) is zero whenever the corresponding λ is constant, and it has positive values for i corresponding to
changepoint location.

Training of the PINNs model uses a backpropagation algorithm on the computational graph of the problem, and it
requires a differentiable loss function. In our case V λ is a nondifferentiable function, and it cannot be included into the
objective function in a current form. Hence, we rewrite (4) as a differentiable function which is a sum of two variables.
In particular, we apply rectiﬁed linear activation (ReLU) function to ∆λ(t) and −∆λ(t), respectively, and we call them
a positive part ∆λ+(t), and a negative part ∆λ−(t), such that

∆λ+(ti) =

∆λ−(ti) =

(cid:26)∆λ(ti),

0,
(cid:26)0,

for ∆λ(ti) > 0
otherwise

for ∆λ(ti) ≥ 0

−∆λ(ti), otherwise.

So, the regularization term in the loss function (5) can be written as

V λ =

T −1
(cid:88)

i=1

δ(i) (cid:2)∆λ+(ti) + ∆λ−(ti)(cid:3) .

3

(6)

2.3 Two-Step Estimation Algorithm

Given a maximum number of changepoints k∗, a standard method of changepoints detection involves the analysis of
all possible breakpoints locations that minimize the loss computed on the corresponding segments of the data. In the
context of our model, such a problem would correspond to the objective function of the form

arg min

t0<...<t∗
k

k∗
(cid:88)

(Lf + Ls)((cid:8)xi, ti(cid:9)tk

i=tk−1+1),

k=1

(7)

where t0, . . . , t∗
k are changepoints locations selected from all possible partitions of the data and for each of the partitions
the corresponding PINNs model would have to be estimated to evaluate the loss function (7). If the computational
complexity of the PINNs method is P, a naive approach to the problem above would require a O(2nP) computations.
Dynamic programming (DP) provides a computationally more efﬁcient way of solving this minimization problem, and
it has a time complexity of O(k∗n2P)). This is still too large for the PINNs algorithm to run on a realistic dataset.

Our CP-PINNs approach is an enhanced version of the DP method which combines two steps. First, we run PINNs with
total variation penalty included in the loss function (5), where V λ is given in (6). This gives us a set of candidates for
the changepoints locations. Second, we run a DP step but instead of considering all possible segmentations of the data,
we consider only the segmentations based on the changepoint candidates from the ﬁrst step candidates. This greatly
reduces the computational time to O((1 + k∗3)P) which is much lower than the original DP method because k∗ << n,
and it makes the two-step CP-PINNs approach feasible—especially with small k∗.

In the ﬁrst step, in order to select changepoints candidates we need to detect large changes in the estimated λ(t). In
order to do so, we plot log |∆ˆλ(t)|−1 estimated using PINNs model with total variation penalty as in the loss function
(5) and select the largest negative peaks in the series as our candidates for the changepoints.

3 Experiment Results

All of the experiments below are trained 100, 000 times with Adam [31] optimizer and 50, 000 times with Limited-
memory BFGS [32] optimizer. The model is deployed on a single NVIDIA GeForce RTX 3080 Ti graphics card.

3.1 Advection-Diffusion Equation

As the ﬁrst example, we discuss the advection-diffusion equation, which is widely used in heat transfer [33], mass
transfer [34], and ﬂuid dynamics problems [35]. The equation of one-dimensional form is given by

+ v

= κ

∂u
∂x

∂2u
∂x2

∂u
∂t
u(t, −1) = u(t, 1) = 0
u(x, 0) = − sin(πx),

where v is the advection coefﬁcient and κ is the diffusivity coefﬁcient, u(x, t) : Ω → R, and Ω = [−1, 1] × [0, 1]. [36]
has given the analytical solution of the advection-diffusion equations by using inﬁnite series summation. In our cases,
we assume that advection coefﬁcient v = 1 is ﬁxed, and changepoints are only associated with diffusivity coefﬁcients.

Without loss of generality, we use the same number of observations on the boundary and for the initial conditions in the
two terms (2) and (3), respectively.

Our neural network has 3 hidden layers, each of them has ﬁve neurons and use tanh() activation function. We
generate 500 points over space interval [−1, 1], and set 101 equally spaced time steps in the [0, 1] interval, so the total
observations are 50500. The initial value of diffusivity coefﬁcients κ (t) is sampled from a normal distribution with
mean 0 and standard deviation 1.

3.1.1 One breakpoint with balanced time intervals

In this case, the velocity coefﬁcient is ﬁxed to be a constant v = 1 and the diffusion coefﬁcient κ(t) is a piecewise
constant function of time with one breakpoint, where κ1 = 1 on [0, 0.5) and κ2 = 0.05 on [0.5, 1].

The results are shown in Figure 1. The model ﬁnds the true breakpoint which is the middle point of the time interval.
And estimation of the edge is stable.

4

Figure 1: One breakpoint with balanced time intervals for 1D advection-diffusion equation. The true breakpoint is at
interval 50 and the detected one is also at interval 50. The x axis is time and y axis is log

|∆ˆκ(t)|−1(cid:17)
(cid:16)

.

(a) The convergence rate over all possible subsets.

(b) Total loss values against training iterations.

Figure 2: One breakpoint with balanced time intervals. The parameter estimation on interval [0, 50), [0, 100] and
(50, 100] are 1.0004, −0.1316 and 0.0498. The training errors are 0.0391, 1453.6112, and 0.3420 respectively.

For the detected change point t∗ = 0.5, we train PINNs on each candidate and compare estimation results. The results
are shown in Figure 2. We ﬁnd that the parts before and after the breakpoint converge to their corresponding diffusion
coefﬁcient very quickly. We exclude the negative estimation results because it does not make sense in this PDE.

We display the exact solution of the dynamic system and estimated solution of each subset in Figure 3. The estimated
solution is very close to the true functions. When compared with the PINNs model estimated without any changepoints
on the whole time domain. The PINNs ﬁt is much worst—see Figure 2 (b) for the corresponding total loss in the
training iterations.

3.1.2 Two breakpoints with balanced time intervals

In general, we might have multiple breakpoints in the dynamics. The same technique can be applied in this situation.
We use the same neural network structure and settings as in the example above. We assume there are two breakpoints,
where κ1 = κ3 = 1 on [0, 0.33) and [0.66, 1], and κ2 = 0.05 on [0.33, 0.66). We also adjust the height of peaks to
control the number of candidates. The results are shown in Figure 4. The estimation of edges is still stable.

The two detected positions are 0.33 and 0.68, which are very close to the real change points locations. Subsequently, we
train distinct neural networks on all possible subsets. Figure 5 shows that the models training on [0, 0.33), [0.33, 0.68)
and [0.68, 1] yield the best estimation results. On the contrary, those training on any intervals containing breakpoints
are unable to produce the correct diffusion coefﬁcient and have much higher loss function values.

In Figure 6, we show the real solution of the whole dynamic system and the estimated solution of all possible subsets.

5

020406080100Time00101020203030log((|(t)|)1)Detection Exact breakpoint050000100000150000iteration0.20.20.00.00.20.20.40.40.60.60.80.81.01.0diffusioncoeff.(1)[0, 50][0, 101][50, 101]050000100000150000iteration101101100100101101102102103103104104lossvalues[0, 50][0, 101][50, 101](a) Exact solution

(b) Estimated solution

(c) Point-wise error

Figure 3: The solution of dynamical systems with one balanced breakpoint.

Figure 4: Two breakpoints with balanced time intervals for 1D advection-diffusion equation. The true breakpoints are
at interval 33 and 66, and the candidate points are at interval 33 and 68.

3.1.3 One breakpoint with imbalanced time intervals

In the case of imbalanced data, where the breakpoint appears closer to the edge, our model still can detect it. We set
κ1 = 1 on [0, 0.2) and κ2 = 0.05 on [0.2, 1]. The model detects the changepoint at 0.19.

The results are shown in Figure 7. Figure 8 shows the training process on each potential candidate. And the comparison
between estimated solutions and exact solutions are shown in Figure 9.

3.2

2D Navier-Stokes Equations

In another experiment, we detect changepoints among the parameters of 2D Navier-Stokes equations. Namely,

ut + (uux + vuy) = −px + λ(t) (uxx + uyy)
vt + (uvx + vvy) = −py + λ(t) (vxx + vyy) ,

(8)

for u(x, y, t), v(x, y, t) and p(x, y, t) : Ω → R, where Ω = [−5, 10] × [−5, 5] × [0, T ]. The two functions u(x, y, t)
and v(x, y, t) denote the stream-wise and transverse velocity components, respectively, and the third function p(t, x, y)
is the measurement of ﬂuid pressure. In our experiment, we focus on incompressible ﬂuid ﬂow passing through the 2D
circular cylinder with radius = 0.5. λ is the viscosity coefﬁcient, where λ = 1
Re . Re is the Reynolds number, where
Re = U D/ν. U is the velocity scale, D is the length scale and ν is the kinematic viscosity.

Following [10], we expect that solutions of (8) are divergence free functions, thus we have

∂u
∂x

+

∂v
∂y

= 0.

6

(9)

020406080100Time55101015152020252530303535log((|(t)|)1)Detection Exact breakpoint(a) Parameters estimation on all possible subsets.

(b) Total loss values against training iterations.

Figure 5: Two breakpoints with balanced time intervals. The parameter estimation on interval [0, 0.33), [0.33, 0.68)
and [0.68, 1] are 1.0003, 0.0620, 1.0000. Training errors are 0.0451, 68.8005, and 2.27 × 10−6.

(a) Exact solution

(b) Estimated solution

(c) Point-wise error

Figure 6: The solution of 1D advection-equation with two balanced breakpoints.

In order to satisfy (9), one would assume hidden functions ψ(x, y, t) , where

u =

∂ψ
∂y

,

v = −

∂ψ
∂x

.

Moreover, one needs to deﬁne feature functions:

f =

g =

∂u
∂t
∂v
∂t

(cid:18)

+

u

(cid:18)

+

u

∂u
∂x
∂v
∂x

+ v

+ v

(cid:19)

(cid:19)

∂u
∂y
∂v
∂y

+

+

∂p
∂x
∂p
∂y

− λ(t)

− λ(t)

(cid:18) ∂2u

∂x2 +

(cid:18) ∂2v

∂x2 +

7

(cid:19)

(cid:19)

.

∂2u
∂y2
∂2v
∂y2

050000100000150000iteration1.01.00.50.50.00.00.50.51.01.0diffusioncoeff.()[0, 33][0, 68][0, 101][33, 68][33, 101][68, 101]Exact 1,3Exact 2050000100000150000iteration105105103103101101101101103103lossvalues[0, 33][0, 68][0, 101][33, 68][33, 101][68, 101]Figure 7: One breakpoint with imbalanced time intervals for 1D advection-diffusion equation. The true breakpoint is at
interval 20 and the detected one is at interval 19.

(a) Parameters estimation on all possible subsets.

(b) Total loss values against training iterations.

Figure 8: One breakpoint with imbalanced time intervals. The parameter estimation on interval [0, 19), [0, 101] and
(19, 101] are 1.0003, −0.0001 and 0.0415. The training errors are 0.0346, 853.0608, and 130.1135 respectively.

Compared with the 1D case, the two terms in the loss function, (2) and (3), have two components each

Lf

(u) =

Lf

(v) =

Ls

(f ) =

Ls

(g) =

1
N

1
N

1
N

1
N

N
(cid:88)

i=1

N
(cid:88)

i=1

N
(cid:88)

i=1

N
(cid:88)

i=1

|uN N (xi, yi, ti) − u (xi, yi, ti)|2

|vN N (xi, yi, ti) − v (xi, yi, ti)|2

|fN N (xi, yi, ti)|2

|gN N (xi, yi, ti)|2 .

Thus the corresponding loss function is deﬁned as

L = Lf

(u) + Lf

(v) + Ls

(f ) + Ls

(g) + V λ.

Following the experiment in [37], the data for our analysis is generated using the following boundary conditions: no-slip
conditions on the cylinder surface; far-ﬁeld with Dirichlet boundary conditions: (u, v) = (1, 0) at the bottom and top
boundaries; the Dirichlet boundary condition: (u, v) = (1, 0) at the left boundary for corresponding inﬂow; and the
ﬂow moves out of the right boundary with ﬁxed pressure p = 0.

8

020406080100Time005510101515202025253030log((|(t)|)1)Detection Exact breakpoint050000100000150000iteration0.00.00.20.20.40.40.60.60.80.81.01.0diffusioncoeff.()[0, 19][0, 101][19, 101]Exact 1Exact 2050000100000150000iteration101101100100101101102102103103104104lossvalues[0, 19][0, 101][19, 101](a) Exact solution

(b) Estimated solution

(c) Point-wise error

Figure 9: The solution of dynamical systems with one imbalanced breakpoint.

Figure 10: One breakpoint with balanced time intervals for 2D Navier-Stokes equations. The true breakpoint is at

interval 100 and the detected one is at interval 95.The x axis is time and y axis is log

(cid:12)
(cid:12)
(cid:12)∆ˆλ(t)
(cid:12)
(cid:12)
(cid:12)

−1

.

For this experiment, our neural network structure is composed of 8 hidden layers with 20 neurons in each layer and the
tanh() as activation function. We have created a training dataset by randomly sub-sampling the high-resolution dataset,
which is generated by four order piecewise polynomial ﬁnite element solver as in [38]. In order to show the ability of
our method to detect changepoints and learn parameters from dynamic systems with sparse training data, we choose
700 points from the 2D surface, corresponding to a mere 9% of the total available.

3.2.1 One breakpoint with balanced time intervals

We consider the 2D Navier-Stokes equations as in (8) with the viscosity coefﬁcient λ(t) as a piecewise constant function
of time with one breakpoint, where λ1 = 0.5 on [0, 2) and λ2 = 0.02 on [2, 4]. The results are shown in Figure 10. In
order to shorten training time, we set an early stopping tolerance: the difference between 1.0 and the next smallest
representable ﬂoat larger than 1.0. For 64-bit binary ﬂoats in the IEEE-754 standard, this value is 2−52. Consequently,
not all training would reach the maximum 150,000 iterations. The corresponding results are shown in Figure 11.

Mean square error and parameter estimation error on each candidate interval are shown in Table 1. Again, our CP-PINNs
model provides much more accurate estimates than the original PINNs model estimated on the entire time interval.

3.2.2 Two breakpoints with balanced time intervals

We also consider the example that there are two breakpoints in the dynamical system, and their locations are evenly
distributed. In this example, t ∈ [0, 6], and we have 300 equally spaced time steps in total. For time intervals [0, 100)
and [200, 300], we generate data by using λ1 = λ3 = 0.5, and we use λ2 = 0.01 to generate data on time interval
[100, 200).

9

0255075100125150175200Time44668810101212141416161818log((|(t)|)1)Detection Exact breakpoint(a) The convergence rate over all possible subsets.

(b) Total loss values against training iterations.

Figure 11: One breakpoint with balanced time intervals.The parameter estimation on [0, 95), [0, 200] and (95, 200]
are 0.4989, 0.0719 and 0.0208, respectively. The training errors on these intervals are 0.1061, 4.3442, and 0.1520
respectively.

Interval MSE of u MSE of v Error of λ

[0, 95)
[0, 200]
(95, 200]

0.000006
0.000396
0.000016

0.000004
0.000263
0.000009

0.214571%
172.489518%
8.420763%

Table 1: Mean squared error of stream-wise velocity, traverse velocity and parameter estimation error on each potential
candidates with one balanced breakpoint.

The training results are shown in Figure 12. For the DP step, we have 3 changepoints candidates with 10 possible
partitions. We estimate the viscosity parameter on each subinterval. All the training results are shown in Figure 13.
Table 2 shows the mean square errors and parameter estimation errors on all potential intervals.

3.2.3 One breakpoint with imbalanced time intervals

We also give the example of one change point with imbalanced time intervals for Navier-Stokes equations. Here
t ∈ [0, 2.7], and we have 135 equally spaced time steps in this case. We use λ1 = 0.5 and λ2 = 0.02 to generate
training data, and the change happens at time step 100. We estimate the breakpoint location at 93 which is very close
to real breakpoint. The results are shown in Figure 14. We also estimate parameters in each time interval, the results
are shown in Figure 15. We show the mean square error of solutions and the estimation error of λ on every potential
interval. The results are shown in Table 3.

Interval

MSE of u MSE of v Error of λ

[0, 92)
[0, 98)
[0, 201)
[0, 300]
[92, 98)
[92, 201)
[92, 300]
[98, 201)
[98, 300]
[201, 300]

0.000015
0.000007
0.000657
0.000625
0.000017
0.000028
0.000742
0.000007
0.000808
0.000015

0.000007
0.000004
0.000375
0.000359
0.000013
0.000020
0.000473
0.000003
0.000406
0.000008

0.467032%
0.202596%
415.000422%
879.713949%
100.384215%
18.714501%
952.496209%
4.879582%
1080.164707%
0.859374%

Table 2: Mean squared error of stream-wise velocity, traverse velocity and parameter estimation error on each potential
candidates with two balanced breakpoints.

10

050000100000150000iteration0.00.00.10.10.20.20.30.30.40.40.50.5diffusioncoeff.()[0, 95][0, 200][95, 200]Exact 1Exact 2050000100000150000iteration101101100100101101102102103103lossvalues[0, 95][0, 200][95, 200]Figure 12: Two breakpoints with balanced time intervals for 2D Navier-Stokes equations. The true breakpoints are at
interval 100 and 200. And the potential candidates are found at 92, 98 and 201.

(a) The convergence rate over all possible subsets.

(b) Total loss values against training iterations.

Figure 13: Two breakpoints with balanced time intervals. The parameter estimation for interval [0, 98), [98, 201)
and [201, 300] are 0.4990, 0.0102 and 0.4957. The corresponding training errors are 0.1153, 0.0745 and 0.2097,
respectively.

4 Discussion

Assumption 2.1 can be modiﬁed to allow, e.g., for piecewise linear λ(t) function. In that case, similarly to [39] one
should replace the total variation penalty (4) with the second order difference penalty (cid:12)
(cid:12)∆λ(ti+1) − ∆λ(ti)(cid:12)
(cid:12)
to induce sparsity in the changes of the slope of the estimated λ(t) function. One can also consider (cid:96)2 regularization on
the differences in λ(t) to capture smooth changes in the function.

(cid:12)∆2λ(ti)(cid:12)

(cid:12) = (cid:12)

Another assumption that we made in our analysis so far is that the observations are equally spaced. We can extend our
method to data which is irregularly spaced. First, consider the estimation of the underlying continuous function λ(t)

Interval MSE of u MSE of v Error of λ

[0, 93)
[0, 135]
[93, 135]

0.000013
0.000169
0.000015

0.000008
0.000117
0.000010

0.250596%
516.275559%
34.152066%

Table 3: Mean squared error of stream-wise velocity, traverse velocity and parameter estimation error on each potential
candidates with one imbalanced breakpoints.

11

050100150200250300Time44668810101212141416161818log((|(t)|)1)Detection Exact breakpoint Exact breakpoint050000100000150000iteration0.10.10.00.00.10.10.20.20.30.30.40.40.50.5diffusioncoeff.()[0, 92][0, 98][0, 201][0, 300][92, 98][92, 201][92, 300][98, 201][98, 300][201, 300]Exact 1,3Exact 2050000100000150000iteration101101100100101101102102103103lossvalues[0, 92][0, 98][0, 201][0, 300][92, 98][92, 201][92, 300][98, 201][98, 300][201, 300]Figure 14: One breakpoint with imbalanced time intervals for 2D Navier-Stokes equations. The true breakpoint is at
interval 100 and the detected one is at interval 93.

(a) Parameters estimation on all possible subsets.

(b) Total loss values against training iterations.

Figure 15: One breakpoint with imbalanced time intervals. The parameter estimation on interval [0, 93), [0, 135] and
[93, 135] are 0.4987, 0.4077 and 0.0244, respectively.

from a ﬁnite number of data points. The total variation regularization term in a continuous form can be written as

This can be simpliﬁed by using the standard interpolation problem as explained in [39]. In particular, we can write

(cid:90) T

0

δ(t) |∆λ(t)| dt.

n−1
(cid:88)

i=1

δ(ti)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

λ(ti+1) − λ(ti)
ti+1 − ti

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

From the optimal points λ(ti), i = 1, . . . , n, we can recover the solution to the original piecewise constant problem:
the piecewise-constant function, for λ∗(ti), given by

λ∗(t) =

ti+1 − t
ti+1 − ti

λ∗(ti),

for i = 1, . . . , n, is the optimal piecewise constant λ(t).

Finally, one can consider a more general problem with changepoints in the spatio-temporal domain, i.e., λ(t, x) as a
function of time and space. The regularization term then would include increments across time and the neighboring
observations. Such a regularization would be considerably more difﬁcult to incorporate in the estimation process and so
far it has been only considered for the linear models, see [40].

12

020406080100120Time66881010121214141616log((|(t)|)1)Detection Exact breakpoint050000100000150000iteration0.00.00.10.10.20.20.30.30.40.40.50.5diffusioncoeff.()[0, 93][0, 135][93, 135]Exact 1,3Exact 2050000100000150000iteration101101100100101101102102103103lossvalues[0, 93][0, 135][93, 135]5 Conclusion

We propose a new method to detect changepoints in the dynamic systems described by general PDE dynamics. Our
algorithm consists of two-steps: The ﬁrst step is to allow for a general model with time-varying parameters and total
variation regularization on the ﬁrst order difference of unknown piecewise constant parameters and detect candidates
for the changepoints. The second step is a reduced DP step which trains PINNs on all of the potential subdomains from
the ﬁrst step. In case of no changepoints in the data, the method reduces to the original PINNs appraoch. In case of
changepoints present in the data, we show that we can detect changepoints accurately and capture the patterns of the
dynamic system much better than applying PINNs on the whole domain.

References

[1] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.
[2] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444, 2015.
[3] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural

networks. Advances in neural information processing systems, 25, 2012.

[4] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
[5] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception
In Proceedings of the IEEE conference on computer vision and pattern

architecture for computer vision.
recognition, pages 2818–2826, 2016.

[6] Klaus Greff, Rupesh K Srivastava, Jan Koutník, Bas R Steunebrink, and Jürgen Schmidhuber. Lstm: A search

space odyssey. IEEE transactions on neural networks and learning systems, 28(10):2222–2232, 2016.

[7] Hrushikesh N Mhaskar and Tomaso Poggio. Function approximation by deep networks. arXiv preprint

arXiv:1905.12882, 2019.

[8] Bethany Lusch, J Nathan Kutz, and Steven L Brunton. Deep learning for universal linear embeddings of nonlinear

dynamics. Nature communications, 9(1):1–10, 2018.

[9] Ingrid Daubechies, Ronald DeVore, Simon Foucart, Boris Hanin, and Guergana Petrova. Nonlinear approximation

and (deep) ReLU networks. Constructive Approximation, 55(1):127–172, 2022.

[10] Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learning
framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of
Computational Physics, 378:686–707, 2019.

[11] Markus Bär, Rainer Hegger, and Holger Kantz. Fitting partial differential equations to space-time dynamics.

Physical Review E, 59(1):337, 1999.

[12] Thorsten G Müller and Jens Timmer. Fitting parameters in partial differential equations from partially observed

noisy data. Physica D: Nonlinear Phenomena, 171(1-2):1–7, 2002.

[13] Xiaolei Xun, Jiguo Cao, Bani Mallick, Arnab Maity, and Raymond J Carroll. Parameter estimation of partial
differential equation models. Journal of the American Statistical Association, 108(503):1009–1020, 2013.
[14] Yuyao Chen, Lu Lu, George Em Karniadakis, and Luca Dal Negro. Physics-informed neural networks for inverse

problems in nano-optics and metamaterials. Optics express, 28(8):11618–11633, 2020.

[15] Francisco Sahli Costabal, Yibo Yang, Paris Perdikaris, Daniel E Hurtado, and Ellen Kuhl. Physics-informed

neural networks for cardiac activation mapping. Frontiers in Physics, 8:42, 2020.

[16] George S Misyris, Andreas Venzke, and Spyros Chatzivasileiadis. Physics-informed neural networks for power

systems. In 2020 IEEE Power & Energy Society General Meeting (PESGM), pages 1–5. IEEE, 2020.

[17] Liu Yang, Dongkun Zhang, and George Em Karniadakis. Physics-informed generative adversarial networks for

stochastic differential equations. SIAM Journal on Scientiﬁc Computing, 42(1):A292–A317, 2020.

[18] Jiequn Han, Arnulf Jentzen, and Weinan E. Solving high-dimensional partial differential equations using deep

learning. Proceedings of the National Academy of Sciences, 115(34):8505–8510, 2018.

[19] Liu Yang, Xuhui Meng, and George Em Karniadakis. B-pinns: Bayesian physics-informed neural networks for
forward and inverse pde problems with noisy data. Journal of Computational Physics, 425:109913, 2021.
[20] Ehsan Kharazmi, Zhongqiang Zhang, and George Em Karniadakis. hp-vpinns: Variational physics-informed neural
networks with domain decomposition. Computer Methods in Applied Mechanics and Engineering, 374:113547,
2021.

13

[21] Kadierdan Kaheman, Eurika Kaiser, Benjamin Strom, J Nathan Kutz, and Steven L Brunton. Learning discrepancy

models from experimental data. arXiv preprint arXiv:1909.08574, 2019.

[22] Richard J Radke, Srinivas Andra, Omar Al-Kofahi, and Badrinath Roysam. Image change detection algorithms: a

systematic survey. IEEE transactions on image processing, 14(3):294–307, 2005.

[23] Md Foezur Rahman Chowdhury, S-A Selouani, and D O’Shaughnessy. Bayesian on-line spectral change point
detection: a soft computing approach for on-line asr. International Journal of Speech Technology, 15(1):5–23,
2012.

[24] David Rybach, Christian Gollan, Ralf Schluter, and Hermann Ney. Audio segmentation for speech recognition
using segment features. In 2009 IEEE International Conference on Acoustics, Speech and Signal Processing,
pages 4197–4200. IEEE, 2009.

[25] M Staudacher, S Telser, A Amann, H Hinterhuber, and M Ritsch-Marte. A new method for change-point detection
developed for on-line analysis of the heart beat variability during sleep. Physica A: Statistical Mechanics and its
Applications, 349(3-4):582–596, 2005.

[26] Ping Yang, Guy Dumont, and John Mark Ansermino. Adaptive change detection in heart rate trend monitoring in

anesthetized children. IEEE transactions on biomedical engineering, 53(11):2211–2219, 2006.

[27] Zekun Yang, Yu Chen, Ning Zhou, Aleksey Polunchenko, and Yilu Liu. Data-driven online distributed disturbance

location for large-scale power grids. IET Smart Grid, 2(3):381–390, 2019.

[28] Alexander G Tartakovsky. Rapid detection of attacks in computer networks by quickest changepoint detection

methods. In Data analysis for network cyber-security, pages 33–70. World Scientiﬁc, 2014.

[29] Zaıd Harchaoui and Céline Lévy-Leduc. Multiple change-point estimation with a total variation penalty. Journal

of the American Statistical Association, 105(492):1480–1493, 2010.

[30] Zaid Harchaoui, Eric Moulines, and Francis Bach. Kernel change-point analysis. Advances in neural information

processing systems, 21, 2008.

[31] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,

2014.

[32] Dong C Liu and Jorge Nocedal. On the limited memory bfgs method for large scale optimization. Mathematical

programming, 45(1):503–528, 1989.

[33] Seyed Ali Hosseini, Nasser Darabiha, and Dominique Thévenin. Lattice boltzmann advection-diffusion model for
conjugate heat transfer in heterogeneous media. International Journal of Heat and Mass Transfer, 132:906–919,
2019.

[34] Ronbanchob Apiratikul. Application of analytical solution of advection-dispersion-reaction model to predict the
breakthrough curve and mass transfer zone for the biosorption of heavy metal ion in a ﬁxed bed column. Process
Safety and Environmental Protection, 137:58–65, 2020.

[35] Ailian Chang, HongGuang Sun, Chunmiao Zheng, Bingqing Lu, Chengpeng Lu, Rui Ma, and Yong Zhang. A time
fractional convection–diffusion equation to model gas transport through heterogeneous soil and gas reservoirs.
Physica A: Statistical Mechanics and its Applications, 502:356–369, 2018.

[36] Abdelkader Mojtabi and Michel O Deville. One-dimensional linear advection–diffusion equation: Analytical and

ﬁnite element solutions. Computers & Fluids, 107:189–195, 2015.

[37] Chris D Cantwell, David Moxey, Andrew Comerford, Alessandro Bolis, Gabriele Rocco, Gianmarco Mengaldo,
Daniele De Grazia, Sergey Yakovlev, J-E Lombard, Dirk Ekelschot, et al. Nektar++: An open-source spectral/hp
element framework. Computer physics communications, 192:205–219, 2015.

[38] David Moxey, Chris D Cantwell, Yan Bao, Andrea Cassinelli, Giacomo Castiglioni, Sehun Chun, Emilia Juda,
Ehsan Kazemi, Kilian Lackhove, Julian Marcon, et al. Nektar++: Enhancing the capability and application of
high-ﬁdelity spectral/hp element methods. Computer Physics Communications, 249:107110, 2020.

[39] Seung-Jean Kim, Kwangmoo Koh, Stephen Boyd, and Dimitry Gorinevsky. (cid:96)1 trend ﬁltering. SIAM review,

51(2):339–360, 2009.

[40] Yu-Xiang Wang, James Sharpnack, Alex Smola, and Ryan Tibshirani. Trend Filtering on Graphs. In Guy
Lebanon and S. V. N. Vishwanathan, editors, Proceedings of the Eighteenth International Conference on Artiﬁcial
Intelligence and Statistics, volume 38 of Proceedings of Machine Learning Research, pages 1042–1050, San
Diego, California, USA, 09–12 May 2015. PMLR.

14

