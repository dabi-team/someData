Fairness-guided SMT-based Rectiﬁcation of
Decision Trees and Random Forests

Jiang Zhang
National University of Singapore
zhangj@comp.nus.edu.sg

Ivan Beschastnikh
University of British Columbia
bestchai@cs.ubc.ca

Sergey Mechtaev
University College London
s.mechtaev@ucl.ac.uk

Abhik Roychoudhury
National University of Singapore
abhik@comp.nus.edu.sg

0
2
0
2

v
o
N
2
2

]

G
L
.
s
c
[

1
v
1
0
0
1
1
.
1
1
0
2
:
v
i
X
r
a

Abstract—Data-driven decision making is gaining prominence
with the popularity of various machine learning models. Unfor-
tunately, real-life data used in machine learning training may
capture human biases, and as a result the learned models may
lead to unfair decision making. In this paper, we provide a
solution to this problem for decision trees and random forests.
Our approach converts any decision tree or random forest into
a fair one with respect to a speciﬁc data set, fairness criteria,
and sensitive attributes. The FairRepair tool, built based on our
approach, is inspired by automated program repair techniques
for traditional programs. It uses an SMT solver to decide which
paths in the decision tree could have their outcomes ﬂipped to
improve the fairness of the model. Our experiments on the well-
known adult dataset from UC Irvine demonstrate that FairRepair
scales to realistic decision trees and random forests. Furthermore,
FairRepair provides formal guarantees about soundness and
completeness of ﬁnding a repair. Since our fairness-guided repair
technique repairs decision trees and random forests obtained
from a given (unfair) data-set, it can help to identify and rectify
biases in decision-making in an organisation.

I. INTRODUCTION

Due to the advent of data-driven decision-making, many
important societal decisions are increasingly being made by
programs representing learning models. It is increasingly com-
mon to have decision making for resource allocation, such
as approval of bank loans,
to be conducted by programs
representing learning models. Such programs are constructed
based on training data. So, for approval of loans, the training
data contain relevant information about past loan applications
and approval decisions. Unfortunately, past decisions could be
inﬂuenced by human biases. Therefore, the training data could
be biased, and the decision making programs obtained from
such data could be unfair.

Testing, analysis, and veriﬁcation of decision-making pro-
grams in relation to fairness properties have been studied in the
last ﬁve years. However, previous work has not studied repair-
ing decision trees and random forests which may be unfair. We
focus on these two models, since they are commonly used for
capturing decision making in software systems in various ﬁelds
including industrial operations research. The notion of fairness
in this paper combines the group fairness
that we adopt
criteria used by Galhotra et al. [15] and Albarghouthi et al.

Fig. 1. Our technique repairs an unfair decision tree into a fair one relative
to a dataset and fairness requirements.

[3]. In simple words, group fairness requires the ratio of the
probability of a minority group member getting a resource
to the probability of a majority group member getting the
resource, to be bounded from both below and above.

In this paper, we study the task of automatically repairing
decision-making programs, possibly derived from unfair train-
ing data, to achieve fairness. To the best of our knowledge, the
only existing approach for fairness guided repair is DIGITS
[3]. DIGITS relies on probability distributions to represent in-
puts, and to give (probabilistic) guarantees of completeness. In
practice, it is non-trivial to compute input distributions. Unlike
DIGITS, our technique relies on a set of input-output pairs.
We assume that a (possibly unfair) dataset of past human-made
decisions is available. Instead of directly returning a classiﬁer
trained on this dataset, our technique generates a fair classiﬁer
by repairing an unfair classiﬁer (Figure 1).

Our FairRepair approach is intended to support different
use-cases than a tool like DIGITS [3]. The DIGITS approach
can potentially help in formulating high level policies for fair
decision making in a population based on a known population
distribution. In contrast, FairRepair can identify and rectify
unfairness in decision making based on past decisions’ data;
the population distribution may be unknown. We elaborate
on the distinction between the two methodologies in Section
IX. The output from FairRepair can be used in at least two

1

DatasetFairDecision tree1.2Semantic diﬀ. bound0.9Fairness threshold[Sex, Race]Sensitive attributesFairRepairUnfairDecision tree 
 
 
 
 
 
ways: (1) the repairs may help the users understand implicit or
unconscious biases in manual decisions, and can explain how
to improve organizational decision-making workﬂows, or (2)
FairRepair can help to migrate from manual decision making
to a fair AI-based process by training classiﬁers on historical
decision data and correcting them using fairness requirements.
Contributions: The main contribution of this paper is a
solution to fairness-guided automated repair of decision trees
and random forests. Our approach automatically repairs a
given unfair decision tree (or random forest), and minimizes
the semantic difference between the original and the repaired
tree (or forest) by relying on a partial MaxSMT solver. Our
technique is sound and complete, i.e., it will ﬁnd a repair if
one exists and the repair is guaranteed to satisfy the fairness
semantic difference requirements.

We evaluate FairRepair on two well-known publicly avail-
able datasets (German data-set and Adult data-set from UC
Irvine). We show that our implementation achieves the claimed
fairness guarantees for different fairness thresholds and sensi-
tive attributes: (1) FairRepair outputs decision trees that satisfy
group fairness, and (2) FairRepair adheres to a bound on the
number of input points whose classiﬁcation result differs in the
output decision tree or random forest model. We investigate
the classiﬁcation accuracy of our repaired models, and observe
that our repairs preserve the accuracy of the original model.
We also study FairRepair’s scalability and ﬁnd that it is able
to ﬁnd a repair for decision trees on a dataset with 48,842
data points in under 5 minutes (average time). We make our
tool, FairRepair1, available online, and we plan to make it
open-source.

II. OVERVIEW

Consider the problem of approving loan applications. The
decision is either TRUE (approve application) or FALSE (reject
application). Suppose we know that the existing decision-
making program is unfair (possibly due to being derived from
an unfair data-set) for some pre-deﬁned fairness requirements.
Our goal is to produce a repair that is also a decision-making
program, but one that
is fair and with bounded semantic
difference from the unfair program.

Let D be a dataset of applications which includes three
attributes: an applicant’s sex, education level, and age, which
are discrete attributes. Examples of possible attribute values
are listed below:

sex
male, female

education
low, med, high

age
{1, . . . , 99, 100+}

Let sex be a sensitive attribute. Each sensitive attribute
partitions the input space into disjoint subsets, which we
call the sensitive groups. Continuous attributes could also be
sensitive, which would create partitions in the form of disjoint
intervals over the value range. For multiple sensitive attributes,
sensitive groups are the cross product of the partitions for
each sensitive attribute. In our example, fairness requirements
are probabilistic inequalities of proportions of applicants in a

1https://github.com/fairrepair/fair-repair

sensitive group that receive loans. We call these proportions
passing rates. In our example, the fairness requirements are
deﬁned as follows:

P(get loan|female)
P(get loan|male)

> 0.8 and

P(get loan|male)
P(get loan|female)

> 0.8.

The above inequalities restrict the ratios of the proportions
of applicants receiving loans among males and females to be
bounded by a fairness threshold c, 0.8. In general, fairness
thresholds for each inequality are not required to be the same.
For illustration purpose, we will use 0.8 for both inequalities.
The input distribution on the non-sensitive attributes may
vary among different sensitive groups. Let edu1 and edu2 be
two distinct valuations of education. Suppose 80% of male
applicants have edu1 and 80% of female applicants have edu2 .
If the decision making procedure uses only education to decide
whom to give loans, to meet the fairness requirements, either
both groups of applicants receive loans, or both groups do not
receive loans. In general, when the decision making process
only uses the non-sensitive attributes, the resulting program
may experience over-ﬁtting. Hence, to achieve group fairness,
it is reasonable to set disparate decision making schemes for
each sensitive group, in order to bound the ratios of the passing
rates. We next explain how we repair an unfair decision tree.
a) Pre-processing: Let T be an unfair decision tree
trained using a dataset D. The ﬁrst step of our approach is
to collect paths with respect to the sensitive groups. Each
path is interpreted as a region (or a hypercube) in the input
space with a coloring (its return value). In our example, there
are two colorings, TRUE and FALSE. To avoid ambiguity, we
shall use the phrase path hypercube throughout the paper. An
example path is a constraint of the form (sex=female)∧(age >
50) ∧ (edu=high) → TRUE. It can be naturally represented as
a hypercube: {female} × (50, ∞) × {high}. To account for
sensitive attributes explicitly, we collect the intersections of
these path hypercubes with the sensitive groups. Each path
hypercube will be split into disjoint subsets with the same
coloring inherited. The constraints of a path hypercube may or
may not contain sensitive attributes. The above-mentioned path
hypercube will have an empty intersection with sensitive group
male. All such subsets of path hypercubes will be collected
based on their sensitive groups, and form two sets, Sm for
male and Sf for female.

b) Calculating Lower Bound of Minimal Change: For
each path hypercube i, we denote its return value as Ii,
and calculate its path probability pi (by counting the number
datapoints residing in it), and passing rate ri by considering
the data points that receive the desired outcome (getting a
loan). Similarly, we compute the proportions of each sensitive
group in the entire population. We use linear optimisation to
calculate the minimum change in these proportions to satisfy
the fairness requirement. Let the path probability and passing
rate for male (resp. female) be pm and rm (resp pf and rf).
Let xm and xf be real variables for passing rates of male and

2

female. We are to solve
xm
xf
xm
xf
minimise pm · |xm − rm| + pf · |xf − rf |.

≥ 0.8 and,

≥ 0.8,






These minimum changes are used to calculate the theoretical
lower bound of semantic difference sd , i.e., the proportion of
the population receiving different outcomes before and after
the change. However, the lower bound might not always be
achievable in practice. For real solutions of xm and xf , the
minimal changes computed in the numbers of data points for
males and females may be non-integers, but these numbers
must be integers as we are counting people. Nonetheless, sd
serves as a good starting point for our search for a repair.

c) Calculating Patches with an SMT Solver: With the
minimal theoretical semantic difference sd , we encode the
fairness requirements as SMT formulas, and encode the return
values of the path hypercube subsets in Sm and Sf as
variables. We ﬁrst check if it is possible to repair T without
modifying its tree structure, i.e., by only ﬂipping the return
values of some path hypercubes. We assign a pseudo-Boolean
variable Xi for each path hypercube. For our example, the
fairness requirements are encoded as the following hard con-
straints:






(cid:80)
i∈Sm
(cid:80)
i∈Sf

Xi · pi/pm ≥ 0.8 (cid:80)
i∈Sf
Xi · pi/pf ≥ 0.8 (cid:80)
i∈Sm

Xi · pi/pf and,

Xi · pi/pm.

Since sd might not be achieved, yet we want the repair to have
a small semantic difference, we adopt a multiplicative factor
α as follows (Ii is the return value of path hypercube i):
(cid:88)

(cid:88)

pi · Xor(Xi, Ii) +

pi · Xor(Xi, Ij) ≤ α · sd .

i∈Sm

j∈Sm

The left hand side is the actual semantic difference, and α
is used to bound its difference with sd. If the solver returns
UNSAT, we reﬁne the path hypercubes by splitting them on
an attribute value, and re-run the query. We reﬁne repeatedly
until we ﬁnd a repair.

density function deﬁned on the input space. It is the distri-
bution of the population. An input space is not necessarily
associated with an input distribution. An element in the input
space is called a candidate.

c) Data Set: A data set D is a ﬁnite set of data points,
associated with an n-dimensional input space S. Each data
point contains an n-tuple (a1, a2, . . . , an) ∈ S and an outcome
b. The outcome b is a boolean value, i.e., TRUE or FALSE. If
the input space is not associated with an input distribution, we
assume the data set represents the input distribution perfectly.
The frequencies of n-tuples in the data set are interpreted to
be the density function p.

d) Sensitive Attributes and Sensitive Groups: Each input
space has a set of attributes. Some attributes are considered
sensitive, e.g., sex and race. A sensitive attribute’s attribute
space is partitioned into a ﬁnite number of disjoint subsets. For
discrete attributes, A is partitioned into |A| subsets, where each
partition contains exactly one element of A. For continuous
attributes, A is partitioned based on some thresholds. These
thresholds divides A into ﬁnitely many intervals whose union
is A. Sensitive groups Si’s are subsets of the input space,
partitioned by combinations of different values of sensitive
attributes. For sensitive attributes {A1, . . . , Am}, the input
space is partitioned into M := (cid:81)

m |Ai| sensitive groups.

e) Decision Making Program and Passing Rates: A
decision making program P is deﬁned on an input space S
and a data set D, such that ∀x ∈ D.P (x) ∈ {TRUE, FALSE}.
For program P , passing rate r of a sensitive group Si is the
probability of a candidate receiving a pre-deﬁned outcome
conditioned on being in the sensitive group Si. If not speciﬁed,
the pre-deﬁned outcome is TRUE. Formally, r := P(P (x) =
B|x ∈ Si), where B = TRUE.

f) Group Fairness: For an input space and a decision
making program, if for every two sensitive groups, the dif-
ference in their passing rates is bounded by a multiplicative
factor, 0 < c < 1, then we say that the decision making
program attains group fairness. This deﬁnition is adapted from
[15]. Formally, we require

∀1 ≤ i, j ≤ M, c ri ≤ rj ≤

ri
c

.

III. PRELIMINARIES

We now formally deﬁne the fairness repair problem.

a) Attributes: An attribute A is a label that takes some
value(s) from its attribute space. An attribute can be discrete or
continuous. If A is discrete, the attribute space of A is a ﬁnite
unordered discrete space. Examples of discrete attributes are
gender, education level, and occupation. If A is continuous,
the attribute space of A is a one-dimensional real space, or
R. Examples of continuous attributes are height and salary.
For simplicity, we use A to denote both the attribute and its
attribute space when no ambiguity arises.

b) Input Space and Input Distribution: An input space
S is the Cartesian product of one or more attribute spaces.
We call an input space n-dimensional if its total number of
attribute spaces is n. The input distribution p is a probability

g) Semantic Difference: For an input space S and two
decision making programs Prog 1, Prog 2 deﬁned on S, their
semantic difference SD(Prog 1, Prog 2) is deﬁned as the pro-
portion in the population that receives different outcomes in
the two programs. Formally,

SD(Prog1 , Prog) := P(Prog1 (x) (cid:54)= Prog2 (x)|x ∈ S)

Deﬁnition III.1 (Fairness Repair Problem). Given an input
space S and a decision making program Prog that does not
satisfy group fairness, a repair is a decision making program
that attains group fairness. An optimal repair is a repair that
minimises the semantic difference between itself and Prog.

To solve a fairness repair problem is to ﬁnd a repair R with
a small semantic difference with Prog, i.e., if there exists an
optimal repair Rop, R should be bounded by a multiplicative

3

factor α > 1 compared to that of the optimal repair. Formally,
SD(R, Prog) ≤ α · SD(Rop, Prog).

h) Optimal Repair: We now discuss the existence of a
theoretical optimal solution for decision tree repair. In this
context, an optimal solution is a decision tree T (cid:48) that satisﬁes
the fairness requirement and has a minimal semantic difference
compared to the original decision tree T .

We shall still use M to denote the number of sensitive
groups. These sensitive groups are shared by T and T (cid:48). Let
T (d) denote the predicted outcome for d ∈ D. We abuse
the notation to let Si, 1 ≤ i ≤ M also denote the set of
path hypercubes in the sensitive group Si. Let Pi,j’s and
pi,j’s denote the path hypercubes and path probabilities, where
1 ≤ i ≤ M , 1 ≤ j ≤ |Si|. For a path hypercube Pi,j,
we use Xi,j to denote desired return value (variable) of the
path hypercube. Each Xi,j is a pseudo-Boolean variable. Let
M : D → {Xi,j : 1 ≤ i ≤ M, 1 ≤ j ≤ |Si|} be the function
that maps the datapoint to the pseudo-Boolean variable which
corresponds to the path hypercube that the point resides in.
Since the data set (by assumption) correctly represents the
input distribution, if there exists a mapping of data points to
return values, and it satisﬁes the fairness requirement, then
such a mapping is by deﬁnition a repair. In addition, we would
like to minimise the semantic difference. This is equivalent to
minimise the number of data points that have different return
values before and after the repair, and can be formulated as
SMT formulas and directly solved by the solver, as follows:






|Si|
(cid:80)
k=1
|Sj |
(cid:80)
l=1

pi,k·Xi,k

pj,l·Xj,l

≥ c,

∀ 1 ≤ i, j ≤ M, k, l,

minimise (cid:80)
d∈D

Xor(Xi,j, T (d)).

The ﬁrst line is the fairness requirement. For any two sensitive
groups, the difference in their passing rates should be bounded
by a multiplicative factor. The second line captures the seman-
tic difference requirement: minimise the number of data points
that have a different classiﬁcation outcome after the repair.
This set of formulas always has a solution, as the fairness
requirement constraint has a trivial solution of Xi,j = 1 for all
i, j. Since the solution space is non-empty, an optimal solution
exists. We omit the deﬁnition of optimal solution for a random
forest as it can be derived by generalising the above.

IV. FAIRREPAIR FOR DECISION TRESS

The goal of our approach is to output a decision tree or a
random forest, such that it satisﬁes the fairness requirement,
and when compared to the optimal solution, the semantic
difference with the output tree or forest is bounded by a
multiplicative coefﬁcient α. Our approach will be able to
output a solution for any α > 1.

A. Top-level Algorithm

Fairness criteria are inequalities over path probabilities. A
probability triple is (Ω, F, P ), where Ω is the sample space,
F is the event space and P is the probability function. As

4

deﬁned in Section III, Ω is the Cartesian product of discrete
spaces (for discrete attributes) and 1-dimensional continuous
spaces (deﬁned on R, for continuous attributes).

Algorithm 1 outlines the repair algorithm. We ﬁrst collect all
decision tree path hypercubes, and group them based on their
sensitive attributes. Each path hypercube represents a region
in the input space that is speciﬁed by its path constraints.
The next step is to calculate the path probabilities, which
are deﬁned to be the proportion of inputs that enter the path
hypercube. We assume that each path hypercube has a 1/0
outcome, depending on whether the resource is allocated. We
specify 1 as the desired return value when we compute passing
rates, which are part of the fairness inequalities as described
in Section III. In Algorithm 1, we use linear optimisation to
calculate how much we want to change the passing rates to
meet the fairness requirement. The constraints are sent to a
partial MaxSMT solver. For some path hypercubes, we ﬂip
the outcomes to meet the fairness requirement. For some path
hypercubes, we reﬁne them (by inserting additional conditions)
to meet the desired passing rates. The SMT procedure termi-
nates when a solution is found. Finally, we modify the decision
tree based on the solution from the SMT solver.

Algorithm 1 Top-level algorithm in FairRepair for Decision
Trees
Input: D, D0 = datasets, c = fairness threshold.
Output: Solution for Repaired Tree.

train T0 on D0 {T0 is a decision tree trained on D0.}
T ← Collectpath hypercubes(T0) {Decisions on sensitive
attributes.}
[H] ← Tree2HCubes(T ) {Path constraints.}
[P ] ← PathRateCalculator(D, H) {Path probabilities.}
[R] ← LinearOpt([P ]) {Desired passing rates.}
while isReﬁnable([H]) == TRUE do

if MaxSMT([H], [P], [R]) == UNSAT then
Reﬁne([H]) {Reﬁne path hypercubes.}

else

return MaxSMT([H], [P], [R])

while TRUE do

RelaxSemDiffConstraint()
if MaxSMT([H], [P], [R]) == SAT then

return MaxSMT([H], [P], [R])

B. Collecting Decisions on Sensitive Attributes

Given a dataset D0, we ﬁrst train a decision tree T on D0
with an external tool. The algorithm starts by collecting the
path hypercubes and grouping them based on the sensitive
attributes in their path constraints. Let {A1, . . . , Am} be the
set of sensitive attributes. There are in total M := |A1|×|A2|×
· · · × |Am| sensitive groups, where each |Ai| is the number
of partitions of the valuations of Ai. For each sensitive group,
create an empty set Si. We use ti, 1 ≤ i ≤ M , to denote
the sensitive attribute values of a sensitive group. This does
not necessarily require the sensitive attributes to have discrete

domains, we only need a discrete partitioning of the set of
valuations of any sensitive attribute.

A path hypercube of the decision tree T corresponds to a
region in the input space. Similarly, each sensitive group is
also a region in the input space. For each path hypercube, its
intersection with the sensitive group ti will be recorded to the
corresponding set Si. Thus, each set Si, 1 ≤ i ≤ M , contains
all path hypercubes that an input with sensitive attributes (ti)
could possibly traverse. Modifying the outcome of a path
hypercube in set Si has no effect on the path hypercubes in
other sets, as the regions in the input space that these sets
cover are mutually disjoint.

C. Path Probability Calculation

To conduct

the repair, we ﬁrst evaluate the tree with
respect to the fairness requirements deﬁned in Section III. In
particular, ∀1 ≤ i, j ≤ M , ri/rj > c, i.e., the passing rates
of any two sensitive groups should be similar (up to threshold
c). Let D be a dataset. Note that we do not require D = D0.
We assume that the data set represents the input distribution,
and deﬁne path probability to be the number of data points
that satisfy the path. Each ri
is then obtained by adding
up the path probabilities of those path hypercubes returning
the required outcome in set Si. Also, we deﬁne the quantity
pi = (cid:80)
prob(π) where prob(π) is the path probability
of path π. Thus, for each path hypercube π in Si, we count
the number of data points in π. This number divided by the
size of the data set is the path probability prob(π). Using
these path probabilities, we can calculate the passing rates ri
and proportions pi. In our example, rmale is the proportion of
male applicants getting a loan. This value is the sum of the
path probabilities of all path hypercubes in Smale that returns
TRUE. If we sum up all path probabilities in Smale, we obtain
the probability of pmale = P(sex = male).

π∈Si

D. Calculating theoretical optimal passing rates

ri
rj

We now have the passing rates ri for each set Si, 1 ≤ i ≤
M , i.e., each pair of sensitive attributes. Recall the fairness
> c, where ri and rj are
requirement: ∀1 ≤ i, j ≤ M ,

passing rates of subtrees. To meet this requirement, we need
to modify the decision tree such that for each 1 ≤ i ≤ M , the
passing rate of Ti changes from ri to xi.

In Section III we directly solved for the return values of each
data points that achieves an optimal repair. By contrast, here
we only compute the optimal passing rate change constrained
by the fairness requirements. Additionally, we aim to ﬁnd
a solution with the minimal semantic difference from the
original, unfair, decision tree. To ﬁnd the xis, we solve the
following linear optimization problem:






∀ 1 ≤ i, j ≤ M,

xi
xj

≥ c,

∀ 1 ≤ i ≤ M, 0 ≤ xi ≤ 1,
M
(cid:80)
i=1

pi · |xi − ri|.

minimise

The ﬁrst line is the fairness requirement. Since passing rates
are probabilities, the second line requires them to be bounded
by 0 and 1. The third line accounts for the semantic difference.
Each pi · |xi − ri| is a lower bound for the proportion of data
points being affected in set Si. It is possible that the passing
rates xi and ri corresponds to different groups of data points,
and the actual number of data points being affected is not
|xi − ri|, but xi + ri. Thus, pi · |xi − ri| is only a lower bound
on the proportion of data points affected in Si.

In our example, suppose for male applicants, initially only
those who have high education receive a loan. When this is
changed to only those who have medium education receive a
loan, the difference in passing rate is only |P(male, high) −
P(male, med)|, but the proportion of data points being affected
is P(male, high) + P(male, med).

The optimisation problem always has a solution, giving us
the “desired passing rates” x1, x2, . . . , xM which are used to
modify the path hypercubes in the decision tree, as discussed
in the following.

E. Calculating patches with MaxSMT

Our goal

is to output a decision tree, such that when
compared to the optimal solution, the semantic difference of
our tree is bounded by a multiplicative coefﬁcient α. This
can be achieved by using a partial MaxSMT solver. Let Pi,j’s
and pi,j’s denote the path hypercubes and path probabilities
of set Si. Let Ii,j be a pseudo-Boolean value that indicates if
the return value of Pi,j is TRUE or FALSE. For each Ii,j, let
Xi,j be a pseudo-Boolean SMT variable. The solver will solve
for these Xi,j’s. The idea is that, to meet minimum semantic
difference, most Xi,j should be the same as Ii,j, but the
solver could produce a few Xi,j which are different from Ii,j;
these are the path hypercubes in the decision tree where the
classiﬁcation outcome of the path is ﬂipped. We now discuss
the partial MaxSMT problem formulation which produces
the Xi,j values. Our MaxSMT problem has hard constraints,
which must be met, and soft constraints the maximal number
of which should be met.

a) Hard Constraints: The ﬁrst set of hard constraints are
the fairness requirements. For every pair of subtrees, the ratio
of their passing rates must be bounded. Recall that Si is the
set of path hypercubes of sensitive group i.

∀ 1 ≤ i, j ≤ M, k, l,

|Si|
(cid:80)
k=1
|Sj |
(cid:80)
l=1

pi,k · Xi,k

pj,l · Xj,l

≥ c.

We account for semantic difference in the hard constraints.

M
(cid:88)

|Si|
(cid:88)

i=1

j=1

pi,j · Xor(Xi,j, Ii,j) ≤ α

M
(cid:88)

i=1

pi · |xi − ri|.

The left hand side is the semantic difference between our
modiﬁed tree and the original tree. The right hand side is
the minimal possible semantic difference, multiplied by α.

5

b) Soft Constraints: The soft constraints put Xi,j to be
same as Ii,j. The solver should satisfy the maximal number
of these soft constraints, which means that we should ﬂip the
return value of as few path hypercubes as possible.

inputs in Pi,j that have the attribute al. Then, we formulate
the reﬁnement problem as the following SMT query:

∀ i, j, Xi,j = Ii,j.

∀ 1 ≤ i, j ≤ M, k, l,

|Si|
(cid:80)
k=1
|Sj |
(cid:80)
k=1

|Dom(A)|
(cid:80)
l=1
|Dom(A)|
(cid:80)
l=1

p(Pi,k, al) · Yi,k,l

≥ c.

p(Pj,k, al) · Yj,k,l

The solver returns UNSAT whenever our semantic difference
is too large. Note that the above set of formulas are not
guaranteed to have a solution, because the path probabilities
might be too large. If the solver returns UNSAT for the partial
maxSMT problem, then we cannot make the decision tree fair
by ﬂipping path hypercube return values. In that case, our
algorithm moves on to reﬁne the path hypercubes, a process
we describe next.

F. Reﬁning Path Hypercubes

If the fairness criteria cannot be satisﬁed by ﬂipping the
outcome of the path hypercubes (i.e., the hard constraints
are unsatisﬁable), we proceed to reﬁne the path hypercubes.
In particular, we split one path hypercube into two path
hypercubes based on a single attribute. There is no restriction
whether this attribute should be discrete or continuous. When
the solver outputs UNSAT, we reﬁne a single path hypercube
and re-run the solver. Since the total number of path hyper-
cubes is bounded by the size of the dataset, this procedure
always terminates after a ﬁnite number of steps.

Reﬁnement requires choosing a path hypercube and a hy-
percube constraint on an attribute. FairRepair delegates this
process to the SMT solver. The changes in passing rates for all
possible constraints to reﬁne the hypercube are pre-calculated.
Then, we iteratively reﬁne path hypercubes by encoding each
reﬁnement as an optimisation problem: at each iteration we
select a reﬁnement that maximally improves fairness. After
a reﬁnement, we check if the new decision tree meets the
fairness criteria. If not, we reﬁne again. To decide on a
constraint to reﬁne, we consider the following two cases:
attributes with discrete domains and attributes with ordered
domains.

Discrete attributes: For attributes with discrete domains,
such as boolean attributes, we consider splitting path hyper-
cubes by choosing arbitrary subsets of values. Speciﬁcally, let
Pi,j be a path hypercube to reﬁne, and let A be an attribute,
and Dom(A) := {a1, a2, ..., an} be the domain of A. Our
goal is to divide the domain Dom(A) into two disjoint sets
AT and AF that would corresponds to the reﬁnements of the
path hypercube Pi,j into P T
i,j, so that the resulting
decision tree returns True for all inputs with the value of A
in AT , and returns False for all inputs with A values in AF .
To realize this, we encode path probabilities for all subsets of
attribute values. For each value al in Dom(A) we introduce
a boolean variable Yi,j,l that indicates if this value should be
in the set AT (i.e., if the decision for ai should be True).
We also assume that p(Pi,j, al) indicates the passing rate for

i,j and P F

The semantic difference hard constraint in Section IV-E re-
mains unchanged. We also add soft constraints indicating that
we should reﬁne as few hypercubes as possible. That
is,
for each path hypercube Pi,j, either (cid:86)
l ¬Yi,j,l
depending on the original decision. Given a solution to the
above constraints, we can reconstruct the sets AT and AF
from the values of Yi,j,l, and reﬁne the corresponding path
hypercubes.

l Yi,j,l or (cid:86)

Ordered domains: Ordered domains, such as integers
and real numbers, require a reﬁnement approach based on
an inequality predicate. For each path hypercube Pi,j, we
generate Ei,j, the set of all possible expressions of the form
variable > const or variable ≤ const, where each variable
is a data attribute, and const is a value of this attribute
corresponding to a data point that lies in the path hypercube.
For each expression ei,j,l ∈ Ei,j, we compute p(Pi,j, ei,j,l) the
passing rate of inputs in Pi,j that satisfy the predicate ei,j,l.
Then, we solve the same formula as for the unordered case:

∀ 1 ≤ i, j ≤ M, k, l,

|Si|
(cid:80)
k=1
|Sj |
(cid:80)
k=1

|Ei,j |
(cid:80)
l=1
|Ei,j |
(cid:80)
l=1

p(Pi,k, ei,j,l) · Yi,k,l

≥ c.

p(Pj,k, ei,j,l) · Yj,k,l

The semantic difference hard constraint in Section IV-E re-
mains unchanged. We also ensure that at most one expression
is used in reﬁning each hypercube by including a cardinality
constraints atMostOne(ei,j,1, ..., ei,j,n) for each path hyper-
cube.

G. Relaxing the Semantic Distance Constraint

When all the path hypercubes are fully reﬁned, i.e., all data
points residing in a path hypercube have exactly the same
attribute values, reﬁnement stops. At this point we adopt an
alternative method. We relax the semantic distance constraint
gradually, until a repair that meets the fairness requirement is
found. The SMT formula we use here remains the same as in
Section IV-E, except for the soft constraints below.

a) Hard Constraints: The ﬁrst set of hard constraint are

the fairness requirements.

∀ 1 ≤ i, j ≤ M, k, l,

|Si|
(cid:80)
k=1
|Sj |
(cid:80)
l=1

pi,k · Xi,k

pj,l · Xj,l

≥ c.

The second hard constraint is the semantic difference require-
ment. Note that the soft constraints in Section IV-E have now
been merged into this second hard constraint, because now the

6

path hypercubes are data points themselves, and minimising
the number of change to path hypercubes is the same as
minimising the number of change to data points.

M
(cid:88)

|Si|
(cid:88)

i=1

j=1

pi,j · Xor(Xi,j, Ii,j) ≤ α · SemDiﬀ.

M
(cid:80)
i=1

pi ·

Initially, SemDiﬀ, the semantic difference, is set to

|xi−ri|, the theoretical minimal semantic difference computed
in Section IV-D. If the solver returns UNSAT, we relax the
semantic difference constraint by updating SemDiﬀ to be
SemDiﬀ

= α · SemDiﬀ. That is,

(cid:48)

M
(cid:88)

|Si|
(cid:88)

i=1

j=1

pi,j · Xor(Xi,j, Ii,j) ≤ α · SemDiﬀ

(cid:48)

= α · α · SemDiﬀ.

Next, we re-run the solver. We iteratively update SemDiﬀ until
the solver ﬁnds a solution of Xi,j’s. To see that our algorithm
always terminates, note that there exists a trivial solution for
the fairness constraints, i.e., all Xi,j’s are TRUE. The left hand
side of the above inequality represents the actual semantic
change in fraction, so it is bounded by 1. Meanwhile, since
α > 1, as we repeatedly relax SemDiﬀ, the right hand side
of the inequality is unbounded and will exceed 1 after ﬁnitely
many iterations. Thus the solver is guaranteed to ﬁnd a solution
after a ﬁnite number of steps. This concludes our algorithm.

V. FAIRREPAIR FOR RANDOM FOREST

The algorithm in Section IV can be extended to repair
random forests. A random forest is an ensemble of decision
trees, For a given input, its outcome is the majority vote (for
classiﬁcation) or average (for regression) of the outcomes from
the decision trees. In our case, the outcome is binary, so we
use the majority vote deﬁnition. Let F := {T1, T2, . . . , Tn}
be a random forest with n decision trees. For input x,

where

F (x) = Maj
1≤i≤n

Ti(x),

Maj
1≤i≤n

Xi =

(cid:40)

1, if (cid:80) Xi >
0, otherwise.

n
2

Given a training dataset D0, we ﬁrst train a random forest F
with an external tool. We extract the decision trees T1, . . . , Tn
from F , and collect the path hypercubes as in Section IV-B.
Let {A1, . . . , Am} and M be deﬁned as earlier. For each
decision tree Ti, we collect its path hypercubes and group
them based on the sensitive attributes in their path constraints.
Let tj denote the sensitive attribute values of a sensitive group,
and let Si,j denote the set of all path hypercubes that an input
with sensitive attributes tj could possibly traverse in tree Ti.
Each path hypercube is given by a tuple (i, j, hid), indicating
the tree Ti it belongs to, the sensitive group tj it resides in,
and unique index “hid” to identify the path hypercube from
Si,j. We next calculate the passing rates of each sensitive
group in the random forest, by recording the return value of

every input data point. Since the optimal passing rates are
purely determined by the classiﬁcation results, we have the
same linear optimisation problem for a random forest as for
a decision tree. Solving the linear optimisation problems, we
obtain the theoretical optimal passing rates xi’s.

A. Computing Patches for the Initial Decision Trees

We now extend our SMT formulas to random forests. Let
D denote a dataset. Note that we do not require D = D0.
The outcome for an input d ∈ D is the majority vote of the
outcomes of path hypercubes where point d resides, across all
decision trees. The point d resides in the intersection of these
path hypercubes. If we compute the intersections of all path
hypercubes over all decision trees (for a random forest with
30 decision trees each with 6,000 paths in our case) the total
number of intersections quickly explodes. We overcome this
by only considering the intersections containing at least one
datapoint from the dataset. This follows from our assumption
that the dataset D represents the input distribution. Instead of
ﬁnding intersections that contains datapoints, we start from
the datapoints to ﬁnd the intersections. For each datapoint
d ∈ D, we identify the path hypercube where d resides in
each decision tree. For decision tree Ti, let the index tuple
of the path hypercube that contains d be (i, jd, hidi,d), the
intersection Id is

(cid:92)

Id :=

Pi,jd,hidi,d ,

1≤i≤n

where Pi,jd,hidi,d is the path hypercube in Si,j with jd being
the sensitive group d belongs to, and hidi,d being the unique
index of the path hypercube. For all d ∈ D, we compute Id
and record (i, jd, hidi,d) for each 1 ≤ i ≤ n. Note that, it is
possible that Id1 = Id2 for d1 (cid:54)= d2. Recall that tj’s denote the
sensitive groups, and sj denotes the set of datapoints in the
sensitive group tj. We shall classify datapoints in each sj into
equivalence classes based on the intersection they reside in.
We denote d1 ∼ d2 if Id1 = Id2, and we use d1 to denote
the equivalence class without loss of generality. We abuse
the notation to let sj also denote the equivalence classes. To
indicate the number of datapoints within an intersection Id1 ,
we assign a frequency ﬁeld fd1 to each intersection, denoting
the number of datapoints in the equivalence class.

Next, we construct the SMT formulas. For each path hyper-
cube Pi,j,hid in the random forest F , assign an SMT Boolean
variable Xi,j,hid as the return value of the path hypercube. We
use d’s to denote the equivalence classes in sj, and use |sj|
to denote the number of datapoints in sj. We then formulate
the fairness requirement as follows:

∀ 1 ≤ k, l ≤ M,

(cid:80)
d∈sk
(cid:80)
d∈sl

Maj
1≤i≤n
Maj
1≤i≤n

(Xi,k,hidi,d ) · fd/|sj|

(Xi,l,hidi,d ) · fd/|sk|

≥ c.

The semantic hard constraint is:

(cid:88)

(cid:88)

sk∈D

d∈sk

fd·Xor( Maj
1≤i≤n

(Xi,k,hidi,d ), F (d)) ≤ α

M
(cid:88)

i=1

pi·|xi−ri|,

7

where sk’s partition the dataset D. Note that for all d within
an equivalence class, they belong to the same path hypercube
in each decision tree. Hence they agree on F (d).

B. Computing Patches for the Intersections

If the result of the above SMT constraints is UNSAT, we
treat each intersection path hypercube separately. Let Sj :=
{Id : d ∈ sj} for 1 ≤ j ≤ M . Assign a pseudo-Boolean SMT
variable for each Id for d ∈ D. We abuse notation by letting
Id also represent the initial outcome of the intersection. Let
fd still denote the path probability of Id. Now it is almost the
same as repairing a decision tree, and the following captures
the fairness requirement.

∀ 1 ≤ i, j ≤ M,

(cid:80)
d∈si
(cid:80)
d(cid:48) ∈sj

fd · Xd

fd(cid:48) · Xd(cid:48)

≥ c.

The semantic hard constraint is:

M
(cid:88)

(cid:88)

j=1

Id∈Sj

fd · Xor(Id, Xd) ≤ α

M
(cid:88)

i=1

pi · |xi − ri|.

If the above SMT constraints are SAT, we update the
changes in the intersections in the initial decision tree path
hypercubes. If Id = Xd, we do not change anything. If
Id (cid:54)= Xd, the majority vote in the intersection is changed
and we compute the minimum number of path hypercubes
whose outcomes need to be ﬂipped to alter the majority
the forest F has n trees. Let Ii,jd,hidi,d de-
vote. Recall
note the outcomes of the path hypercubes Pi,jd,hidi,d for
(Ii,jd,hidi,d ) = Id. To
1 ≤ i ≤ M , d ∈ D. Then Maj
1≤i≤M

ﬂip Id, randomly select minimal number of decision trees Ti
with Ii,jd,hidi,d (cid:54)= Xd and add the path hypercube Id with
(Ii,jd,hidi,d ) is ﬂipped. If
prediction Xd to Ti, such that Maj
1≤i≤M
(Ii,jd,hidi,d )−

Id = TRUE, minimally (cid:80)

decision trees

(cid:107)

(cid:106) n
2

with Ii,jd,hidi,d = TRUE will have added a new path hypercube
Id (the intersection path hypercube) with a False prediction.
(cid:109)
(Ii,jd,hidi,d ) decision
If Id = FALSE, minimally

− (cid:80)

1≤i≤M

(cid:108) n
2

1≤i≤M

trees with Ii,jd,hidi,d = FALSE will have added a new path
hypercube Id with a TRUE prediction.

If the above SMT constraints are still UNSAT, we proceed
to reﬁne the intersections in the same manner as we reﬁned
the decision trees in Section IV-F.

VI. RELATIVE SOUNDNESS AND COMPLETENESS PROOF

In this section, we present a proof of relative soundness
and completeness of our repair technique. This completeness
property distinguishes our work from existing decision tree
repair tool DIGITS [3]. The only assumption we make is the
soundness and completeness of the SMT solver: if the solver
is sound and complete, then our algorithm is also sound and
complete. The proof for decision trees and random forests are
similar. Here we omit the proof for random forests.

8

The soundness of our approach partly follows from the
soundness of the SMT solver. If the solver returns a solution,
it must satisfy the hard constraints of the fairness requirement
and the (possibly relaxed) semantic change constraint. We
explained how our algorithm always provide a solution for
a repaired model in IV-G. We now prove that whenever our
algorithm produces a repaired model, its fairness and semantic
difference is always guaranteed. In practice, we also compute
the ratios of the passing rates to check if they have passed
the fairness threshold. We refer our readers to VIII for the
experimental results.

Lemma VI.1. For a given decision tree T obtained via
training on a given dataset D, for any α > 1, if the group
fairness requirement is satisﬁable and there exists an optimal
repair Rop, our algorithm (Section IV) is able to ﬁnd a
repair Rsubop satisfying the fairness requirement, such that
SD(Rsubop, T ) ≤ αSD(Rop, T ). Here Rsubop refers to a
solution (assignment) of the variables Xi,j’s, and SD is the
semantic distance function deﬁned in Section III. For any
assignment R of Xi,j’s,

SD(R, T ) =

M
(cid:88)

|Sj |
(cid:88)

i=1

j=1

pi,j · Xor(Xi,j, Ii,j),

where the symbols are deﬁned in Section IV.

Proof. In Section IV-G, we have shown that our algorithm
always terminates with an output repair whose fairness is hard-
constrained by the SMT solver. It sufﬁces to show that repair
satisﬁes the semantic requirement even after relaxation of α.
Our algorithm ﬁrst checks if the group fairness requirement
can be achieved by only ﬂipping the existing path hypercubes
as described in Section IV-E. If the solver returns SAT, our
algorithm outputs the assignments of Xi,j’s and terminates. If
the fairness requirement is UNSAT, our algorithm proceeds to
reﬁne the path hypercubes, checks if the fairness requirement
is SAT after the reﬁnement, until all the path hypercubes are
fully reﬁned (Section IV-F). Once all the hypercubes are fully
reﬁned, the path hypercubes are equivalent to the input data
points, meaning each path hypercube denotes a single data
point, with its duplicates recorded as path probability. Our
algorithm then relaxes the semantic distance constraints until
the solver returns SAT (Section IV-G), outputs the solution,
and terminates.

A solution could be produced in any of the above three
stages. We now show that whenever our algorithm out-
puts a solution Rsubop, Rsubop satisﬁes SD(Rsubop, T ) ≤
αSD(Rop, T ). To distinguish Rsubop from Rop, let Rop =
{xi,j : 1 ≤ i ≤ M, 1 ≤ j ≤ |Si|}, and Rsubop = {yi,j : 1 ≤
i ≤ M, 1 ≤ j ≤ |Si|} such that both are assignments of the
variables Xi,j’s.

a) Before Reﬁnements: If the fairness requirement can
be achieved by only changing the outcomes of some path
hypercubes, without splitting any of them, as a hard constraint

(Section IV-E), we have

M
(cid:88)

|Sj |
(cid:88)

i=1

j=1

pi,j · Xor(yi,j, Ii,j) ≤ α

M
(cid:88)

i=1

pi · |xi − ri|.

The left hand side is SD(Rsubop, T ), and the right hand side
is the product of α and the theoretical minimal semantic
difference, which is a lower bound for the actual semantic
difference. That is,

M
(cid:88)

|Sj |
(cid:88)

i=1

j=1

Therefore,

pi,j · Xor(xi,j, Ii,j) ≥

M
(cid:88)

i=1

pi · |xi − ri|.

SD(Rsubop,T ) =

M
(cid:88)

|Sj |
(cid:88)

i=1

j=1

pi,j · Xor(yi,j, Ii,j)

≤ α

M
(cid:88)

|Sj |
(cid:88)

i=1

j=1

pi,j · Xorxi,j, Ii,j) = αSD(Rop, T ).

b) During Reﬁnements: If the fairness requirement is not
achieved by only changing the outcomes of the hypercubes, we
reﬁne the path hypercubes and call the solver again. Note that
Xi,j’s are now different, since |Si| increases as the number of
path hypercubes increases due to reﬁnement. We use I
i,j’s to
denote the outcomes of the new path hypercubes, to distinguish
the path hypercubes after reﬁnement from those before the
reﬁnements. Nonetheless, the outcomes of the path hypercubes
before and after a reﬁnement do not conﬂict with each other,
as each path hypercube subset inherits the outcome from the
original one. Since in the SMT formula construction,
the
same semantic difference hard constraint is inherited from the
previous step, we also have

(cid:48)

SD(Rsubop,T ) =

M
(cid:88)

|Sj |
(cid:88)

i=1

j=1

pi,j · Xor(yi,j, I

(cid:48)

i,j)

≤ α

M
(cid:88)

|Sj |
(cid:88)

i=1

j=1

pi,j · Xor(xi,j, I

i,j) = αSD(Rop, T ).

(cid:48)

c) After Reﬁnements: When all the path hypercubes are
fully reﬁned, Xi,j’s effectively represent the classiﬁcations of
each individual point. We shall still use I
i,j’s to represent
the original return values for each data point. After the path
hypercubes have been fully reﬁned, if the SMT solver still
returns UNSAT, it means that the theoretical minimal semantic
difference cannot be achieved. Here we use the completeness
of the solver. Thus, we know that

(cid:48)

M
(cid:88)

|Sj |
(cid:88)

i=1

j=1

pi,j · Xor(Xi,j, I

i,j) > α · SemDiﬀ,

(cid:48)

for all assignments for Xi,j’s that satisfy the fairness require-
ment. In particular,

SD(Rop, T ) =

M
(cid:88)

|Sj |
(cid:88)

i=1

j=1

pi,j · Xor(xi,j, I

i,j) > α · SemDiﬀ.

(cid:48)

We use pi,j’s to denote the frequency of repeated data points.
As explained in Section IV-G, the relaxing procedure always
terminates. Let SemDiﬀ0 be the SemDiﬀ when the solver
return UNSAT at the last time. Then we have the following:

SD(Rop, T ) =

M
(cid:88)

|Sj |
(cid:88)

i=1

j=1

pi,j·Xor(xi,j, I

i,j) > α·SemDiﬀ0, and

(cid:48)

SD(Rsubop, T ) =

M
(cid:88)

|Sj |
(cid:88)

i=1

j=1

pi,j·Xor(yi,j, I

i,j) ≤ α·α·SemDiﬀ0.

(cid:48)

The above inequalities imply

SD(Rsubop, T ) ≤ α · SD(Rop, T ).

VII. IMPLEMENTATION

We developed FairRepair in 2,500 lines of Python code.
FairRepair repairs decision trees trained using scikit-learn [29]
DecisionTreeClassiﬁer. In general, our algorithm is not re-
stricted to any speciﬁc decision tree or random forest training
tool.FairRepair changes the structure of the decision tree,
but
it does not change the output range of classiﬁcations
or assume new input features. We now brieﬂy describe the
implementation.

a) Tree to Hypercubes Conversion: Decision trees and
random forests trained by scikit-learn tool do not support
categorical attributes directly. By default, it assumes all at-
tributes are continuous. This structure cannot be used for
MaxSMT solving directly, and we need to convert it into path
hypercubes. We use OneHot Encoding to generate n = |A|
attribute names A1, . . . , An for a categorical attribute A. The
OneHot threshold is set to be 0.5 and the data points’ valuation
on these attributes are 0 or 1. Note that,

∀ discrete A, if A = {A1, . . . , An}, then ∀d ∈ D.∃ i, 1 ≤ i ≤ n.

d[Ai] = 1 ∧ (∀A(cid:48) ∈ {A1, . . . , An}\Ai. d[A(cid:48)] = 0),

as each data point has a unique valuation for the attribute A.
b) Hypercube Mapping on Sensitive Attributes: To obtain
the intersections of path hypercubes with sensitive groups
of multiple sensitive attributes, we implemented the cross
product of path hypercube sets. For two sensitive attributes
A = {a1, . . . , am}, B = {b1, . . . , bn}, we ﬁrst collect path
hypercube set SA = {Sa1 , . . . , Sam} and SB = {Sb1, . . . Sbn }
as described in Section IV-B. Then we compute the cross
product SA × SB := {Sai,bj : 1 ≤ i ≤ m, 1 ≤ j ≤ n}, where
each Sai,bj := {h∩h
∈ Sbj }. This cross product
computation can be easily generalised to multiple attributes.
c) Linear Optimisation: With the path hypercubes we
can calculate the path probabilities and passing rates for each
sensitive group. We use the Python library pulp to calculate
the theoretical minimal changes in passing rates using lin-
ear optimisation. Fairness requirements are implemented as
inequalities, where each variable denotes the passing rate of
a sensitive group. We added additional variables to account

: h ∈ Sai, h

(cid:48)

(cid:48)

9

for absolute values, which is not supported by pulp directly,
and omitted the sensitive groups with too few data points (less
than ten).

d) SMT Formulas Construction: We use the Z3 SMT
solver. All fairness requirements are constructed as linear
arithmetic formulas. To improve performance, rational fairness
threshold c is represented as the ratio of two integers (a, b). For
each path hypercube, we use (n+1) variables X, X1, . . . , Xn,
where n is the number of data points residing in the path
hypercube. Variable X (path variable) denotes the return value
of the path hypercube, and Xi’s (point variables) denote the
data points in the path hypercube. Each Xi is encoded to have
the same value as the X. Each passing rate is summed over
the path variables in the sensitive group. We also used Z3’s
in-built pseudo-Boolean functions PbLe (PbGe) and AtMost
(AtLeast). These cardinality functions are optimised to handle
linear arithmetic expressions with pseudo-Boolean variables.
As an example, the majority vote on point x in Section V can
be expressed as

Maj
1≤i≤n

Ti(x) = AtLeast({Ti(x) : 1 ≤ i ≤ n},

(cid:109)

),

(cid:108) n
2

where the right hand side expression returns a pseudo-Boolean
value for if at least

decision trees output TRUE on x.

(cid:109)

(cid:108) n
2

VIII. EVALUATION

FairRepair’s goal is to repair an arbitrary decision tree to
be fair while bounding the semantic change. In our evaluation,
we study the following research questions.
RQ 1: Does our implementation of FairRepair produce a
fair decision tree (random forest) given an unfair decision
tree (random forest) input? We run FairRepair with different
fairness thresholds and observe the resulting fairness.
RQ 2: How do the semantics of the fair decision trees (random
forest) produced by FairRepair compare to the input unfair
decision trees (random forest)? We compare the semantic
difference between the input and output models produced by
FairRepair.
RQ 3: How scalable is FairRepair? We evaluate FairRepair’s
runtime for different input datasets sizes.
RQ 4: How much does the decision model change in struc-
ture? In particular, we measured the number of new paths
produced during the reﬁnement.
RQ 5:How much does the prediction accuracy change due to
the repair? We measured the precision and recall values to
compare the accuracy of the models before and after repair.

A. Methodology

For our evaluation we use two datasets: the German Credit
card dataset2 and the Adult dataset3, both from the UC Irvine
Machine Learning Repository. These datasets have several
sensitive attributes and come from domains in which fairness
is important.

Sex4

Adult Dataset
Race

Min Max Min Max Min
0.17
0.43
0.36

0.36

0.99

Sex and Race
Max
0.99

Sex

German Dataset
F. Worker4

Sex and F. Worker

Min Max Min Max Min
0.12
0.45
0.74

0.94

0.45
TABLE I
MIN, MAX INITIAL FAIRNESS (INITIAL PAIRWISE RATIOS OF THE PASSING
RATES ACROSS SENSITIVE GROUPS) IN ADULT AND GERMAN DATASET.

Max
0.99

Dataset

Points Classes

Features

German

1,000

Adult

48,842

2

2

20

14

Sensitive
attributes
Sex (4), Foreign-
(2), Sex
Worker
and
Foreign-
Worker (8)
Sex (2), Race (5),
Sex and Race (10)

TABLE II
DATASETS USED IN OUR EVALUATION. NUMBERS IN PARENTHESIS
INDICATE THE NUMBER OF GROUPS THAT MUST BE FAIR RELATIVE TO
EACH OTHER. FOR EXAMPLE, THERE ARE 4 GROUPS, FOR THE Sex
(”Personal status and sex”) ATTRIBUTE IN THE GERMAN DATASET.

Table II shows the size and sensitive attributes of the
datasets. The German dataset classiﬁes a person as a good or
a bad credit risk. The dataset includes 1,000 persons’ records
and each record ranges over 20 features, such as the person’s
age and employment status. The Adult dataset
is used to
predict whether a person will earn over 50K dollars a year. The
dataset has about 48K records and each record ranges over 14
features. We select attributes that are commonly recognised
as sensitive features to form the sensitive groups. For both
datasets, sex is selected to be sensitive attribute. For German
dataset, we selected whether a person is a foreigner worker
and for adult dataset, we selected race as the second sensitive
attribute.

Tables I list

the minimum and maximum initial group
fairness values (between 0 and 1) we evaluate in the Adult and
German datasets. For each dataset, these fairness values are
pairwise ratios of the initial passing rates across their sensitive
groups. The (group) fairness inequalities deﬁned in Section III
require all these ratios to be greater than fairness threshold c
after the repair procedure.

All experiments were run with Python 3.5.2, on a Linux
Ubuntu 16.04 server with 28-core 2.0 GHz Intel(R) Xeon(R)
CPU and 62GB RAM. When generating a scikit-learn De-
cisionTreeClassiﬁer and RandomForestClassiﬁer, we used an
80/20 training-test split of the dataset. In the experiments, we
used the same datasets for training and repairing the models.
However, we do not require them to be the same in general.
For both decision tree and random forest, we collect results
on the two datasets respectively, over 6 fairness thresholds, 6
semantic bounds and 3 choices of sensitive attribute(s) (in total

2https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)
3https://archive.ics.uci.edu/ml/datasets/Adult

4With two groups, the min and max are just inverses of one another.

10

Fig. 2. Classiﬁcation Error Rate vs. Forest Size (Adult).

Fig. 3. Achieved output fairness on the Adult dataset for different fairness
thresholds (decision tree).

108 sets of parameters). In the following, if not speciﬁed, the
plots show runs on all varying parameters. For experiments
on random forests, we select the default forest size to be
30 trees. We measured the classiﬁcation accuracy for random
forests of different sizes. Figure 2 shows the plots for both
datasets. The differences in classiﬁcation error rate is less than
1% for forests with more than 30 trees We also collected
results on different forest sizes for scalability and syntactic
change analysis. To reduce contingencies in the results, each
experiment was run with 5 different random seeds5.

RQ1: Fairness. FairRepair must achieve the stated fairness
guarantee. We used the two datasets to validate that our
approach is able to achieve the fairness requirement. For
decision trees, ﬁgures 3 and 4 show scatter plots of the fair-
ness achieved for different input fairness threshold values for
multiple runs on the Adult and German datasets, respectively.
The plots show results for repairs of all the sensitive attribute
combinations in Table II. For random forest, ﬁgure 5 shows
data for all 540 FairRepair runs on the Adult dataset, with the
tree size set to 30. We do not show the corresponding plot for
the German dataset (which is smaller), but the achieved output
tree fairness is similar to Figure 4.

As expected, the fairness of the output model is bounded
below by the fairness threshold (the y = x line) and it is
bounded above by the inverse of threshold (y = 1/x line).
The reason for the inverse upper bound is because we show
an achieved fairness for every two sensitive groups per exper-
iment. For example, for experiments with Sex as the sensitive
attribute, we plot the fairness of the Sex=Female group against
the Sex=Male group, and also the inverse fairness, as well.
The fairness of all the points falls in the expected range, thus
empirically validating our guarantee about fairness.

RQ2: Semantic difference. A fairness requirement could
be trivially achieved by changing the classiﬁcation output of
path hypercubes in the tree that impact many data points.
Achieving the minimal semantic difference (changing the
classiﬁcation output on the fewest number of points) is chal-
lenging. FairRepair makes a semantic difference guarantee –
given an α parameter, FairRepair will produce a repair that is
within a α multiplicative constant of the minimal semantic
repair. We collected results on various α values, ranging

5Randomness is used by scikit-learn’s decision tree training procedure.

Fig. 4. Achieved output fairness on the German dataset for different fairness
thresholds (decision tree).

from 1.05 to 2.0. For decision trees, ﬁgure 6 and 7 plot the
semantic difference between the input and output decision
trees for varying α and ﬁxed c = 0.8 for the Adult and
German datasets, respectively. The y-axis shows the number of
datapoints ﬂipped during repair. Figure 6 shows results for Sex
and Race as the sensitive attributes. Figure 7 shows results for
Sex and Foreign Worker as the sensitive attributes. FairRepair
points in both ﬁgures fall into the region that is bounded below
by the theoretical minimum of the semantic difference, and
is bounded above by the multiplicative constant α. We also
checked that these results hold for other fairness thresholds
and sensitive attributes.

The result for random forest is similar. Figure 8 shows
results for the Adult dataset with c = 0.8, Sex and Race as
the sensitive attributes. The semantic difference (in percentage)
drops from 10% to 5% as α decreases. All semantic difference
are bounded above by α for all settings of fairness thresholds
and sensitive attributes.

RQ3: Scalability in size of dataset. FairRepair uses an
input dataset to compute the fairness and semantic differences
as it executes. For large datasets, these operations may be
expensive. To investigate scalability we only use the Adult
dataset since the German dataset is relatively small. Figure 9
plots the total running time of FairRepair on datasets of
varying sizes for decision trees. For this, we created partitions
of the Adult dataset by sampling uniform random subsets
of increasing size (from 5K to 45K points). The trained
decision trees for these datasets varied in size, from 577
path hypercubes to 5,521 path hypercubes. In the ﬁgure we
also label the runs according to their fairness threshold. We
found that the choice of fairness threshold and α made little

11

0.40.50.60.70.80.91.0Target fairness0.51.01.52.0Fairness achievedy=1/xy=x0.40.50.60.70.80.91.0Target fairness0.51.01.52.0Fairness achievedy=1/xy=xFig. 5. Achieved output fairness on the German dataset for different fairness
thresholds (random forest).

Fig. 7. The semantic difference between input and output decision trees for
different α bounds, in the German dataset.

Fig. 6. The semantic difference between input and output random forests for
different α bounds, in the Adult dataset.

Fig. 8. The semantic difference between input and output random forests for
different α bounds, in the Adult dataset.

difference to the running time. There is no clear relationship
between the fairness threshold and α, and time to repair.
These experiments used Sex as the sensitive attribute. The
larger datasets take longer to repair.
ﬁgure illustrates that
For example, on an input of 45,000 points and a tree with
about 5,000 path hypercubes FairRepair completes the repair
in under 5 minutes.

For random forest, the running time is shown in ﬁgure 10.
Since fairness and α does not signiﬁcantly affect the running
time, we ﬁxed fairness threshold to be 0.8 and α to be 1.2.
As the size of the dataset increases, it takes up to 30 minutes
(on average) to repair a biased random forest on the full Adult
dataset.

We also measured the scalability against various forest size.
Figure 11 shows the running time for various forest sizes
ranging from 10 trees to 100 trees, with ﬁxed fairness threshold
0.8 and α = 1.2. We observe a proportional increase in the
amount of repair time as the forest size increases. For random
forest on the Adult dataset with 100 trees, FairRepair takes up
to 90 minutes to ﬁnd a repair.

We also note that our code is in Python and we believe it
could be further optimized to achieve much better scalability.
RQ4: Syntactic Difference. In our algorithm, no path
hypercubes are removed or merged during the repair. Hence
the syntactic change can simply be deﬁned as the (average)
increase in the number of path hypercubes of the decision
tree (or over the decision trees in the forest). In the results of
decision trees for both datasets, we observe that no runs require
reﬁnement, i.e., the number of path hypercubes is unchanged.
Hence we only discuss the results on random forests. We
ﬁxed the fairness threshold to be 0.8. The results for syntactic
change against initial number of path hypercubes are shown in

Figure 12. As the initial number of path hypercubes increases,
the syntactic change increases proportionally. For a random
forest with average 30K path hypercubes in each decision tree,
our repair increases the average number by less than 1,400
(less than 5% change).

RQ5: Accuracy. While achieving the fairness requirement,
the change in the classiﬁcation accuracy of the repaired models
is limited. Figure 13 shows the classiﬁcation accuracy against
various fairness thresholds on decision trees trained on the
Adult dataset, with α = 1.05 and sensitive attribute set to
sex and race respectively. The classiﬁcation accuracy before
repair was (on average) 96.3% over 5 decision trees trained
with different random seeds. For sensitive attribute race, the
accuracy at c = 0.95 is 94.8%, which only decreases by 1.5%
after the repair. For sensitive attribute sex, the accuracy at
c = 0.95 is 90.5%. The initial passing rates computed from
the predicted results are (on average) 31.0% and 11.3%. The
initial model has a high accuracy, hence the unfairness in the
dataset is highly preserved in the initial model. The passing
rates computed directly from the Adult dataset are 30.9%
for male and 11.2% for female, which is only one-third of
males. To balance the passing rates, in particular, to increase
the passing rate for females (and to decrease the passing rate
for males), some female applicants (data points) who were
initially classiﬁed with a FALSE prediction have to be given
TRUE prediction now (and some male applicants who were
initially classiﬁed with TRUE prediction are now given FALSE
prediction). This causes the loss in accuracy. However, such
loss is still bounded as shown by our results. FairRepair is able
to repair the decision trees, bounding the ratio of the passing
rates within a fairness threshold of c = 0.95, while maintaining

12

0.40.50.60.70.80.91.0Target fairness0.51.01.52.0Fairness achievedy=1/xy=x1.21.41.61.82.0Alpha30004000Semantic differenceAlpha boundFairRepairMin diff possible1.21.41.61.82.0Alpha101520Semantic differenceAlpha boundFairRepairMin diff possibleFig. 9. The total running time of FairRepair for datasets with increasing
number of points and different sensitive groups (decision tree, adult dataset).

Fig. 11. The total running time of FairRepair for random forests on the Adult
dataset with increasing forest size.

Fig. 10. The total running time of FairRepair for datasets with increasing
number of points and different fairness thresholds (random forest, adult
dataset).

Fig. 12. Syntactic change due to FairRepair for input random forests

a 90.5% accuracy. The ﬁnal passing rates are 30.9% and 29.3%
for male and female applicants respectively.

The results for decision trees on German dataset is shown
in Figure 14, where for sensitive attribute foreign worker,
the accuracy is 91.7%, and for sensitive attribute sex and
marital status, the accuracy is 93.3% at fairness threshold
0.95, compared to the initial accuracy 93.9%. Such results
show that besides keeping a small semantic distance from the
original decision model, our algorithm is able to produce a
repair that does not damage the classiﬁcation accuracy. Results
on random forests show a great similarity to Figure 13 and 14,
hence are not displayed.

IX. RELATED WORK

a) Fairness and its Testing / Analysis: There has been
an increased research effort on fairness in machine learning in
recent years. Various fairness deﬁnitions have been proposed
[6], [17], [20], [22]. In our paper, we used group fairness,
which is used in the fairness testing work of [15]. Other
recent works on fairness testing includes black-box fairness
testing [1] and individual fairness testing [33]. There have
also been attempts to verify fairness properties,
including
calculating integration with measure theoretical results [4],
and using concentration inequalities to bound the error [5].
Many approaches to train fairness classiﬁers have also been
studied [9], [10], [13], including modifying dataset fragments
to remove sensitive attribute correlations [11], and encoding
fairness requirements into classiﬁer training process [17].
Fairness is a special form of probabilistic property. The study
of probabilistic properties involves software engineering tech-
niques, as studied in past works, including probabilistic sym-
bolic execution [16] [26] [12], probabilistic model checking

[32] [23] [7], and probabilistic program analysis / veriﬁcation
[4] [31] [5].

b) Automated Program Repair: There is a large literature
in the ﬁeld of automated program repair (e.g. [24] is a recent
review of the research area). Automated repair techniques
aim to produce patches to meet certain speciﬁcations such
as passing a given test-suite. Heuristic repair, or search-based
repair, iterates over a predeﬁned search space of ﬁxes, and
checks the validity of the ﬁxes (e.g. see [34]). To introduce
higher ﬂexibility than simple mutations, heuristics like plastic
surgery hypothesis [18] or competent programmer hypothesis
are used to assume the patch can be produced from (i) other
parts of the code [30], or (ii) guided by templates obtained
from human patches [21], or (iii) via the use of learning
to prioritize patch candidates [25]. In contrast
to search-
based or template-based works on program repair, our work is
more related to constraint-based or semantic repair techniques
[27], [28], [35]. In these techniques a repair constraint is
inferred as a speciﬁcation via symbolic execution, and this
speciﬁcation is used to drive a program synthesis step which
generates the patch. Previous works on semantic repair are
mostly test-driven, where the repair attempts to pass a given
test-suite. In contrast, fairness is a hyperproperty [8] which
is an accumulative property over collection of traces, and
thus fairness repair is a fundamentally different problem from
repairing a program based on given tests/assertions.

c) Fairness in Decision Tree Learning: Other than re-
pairing a trained decision tree or random forest, attempts
have been made to integrate fairness into the model training
procedure [2] [14]. Among them, Kimaran et al. [19] also
employ the technique of relabeling leaf nodes in decision
trees to achieve fairness requirements. They adopted a different

13

10000200003000040000# of data points0100200Total time (s)Fairness thresh0.90.80.70.60.550001000015000200002500030000Average initial number of path hypercubes indecision trees for random forests (adult dataset)50010001500Syntactic changey = 0.05xpopulation, compared to just the past decision results.

The completeness guarantee of DIGITS is relative to as-
sumptions such as a manually provided sketch (describing
repair model), which is not needed in FairRepair. While its
algorithm is designed for binary sensitive attributes, ours is
able to handle multiple sensitive groups. Our results are not
directly comparable with DIGITS, although they adopted sim-
ilar fairness criteria and bench-marked on the Adult dataset.
Their decision trees were relatively small (less than 100 lines
code) and employed at most three features. In contrast, our
decision trees employ at least 10 features and scale to real-
life applications, with 10K leaves (path hypercubes) before
splits on sensitive groups. With sensitive attribute being sex
and fairness threshold set to 0.85, their algorithm was able
to produce a repair in 10 minutes with semantic difference
of 9.8%. In contrast, FairRepair is able to achieve a semantic
difference of 6.2% with a higher fairness threshold 0.9 within
5 minutes. According to DIGITS synthesis algorithm, longer
training time allows better convergence to the optimal repair,
so it might ﬁnd a repair that satisﬁes the fairness requirements
at an early stage but far from the optimal. But the comparison
does show the efﬁciency of our tool on both achieving fairness
criteria and bounding semantic difference.

X. DISCUSSION

In this paper we described FairRepair, a process to repair an
unfair decision tree to be fair relative to a dataset. Compared to
existing approaches, FairRepair is more data-driven, does not
require population distributions, and makes fewer assumptions
about the input dataset and the decision tree. At the same
time, FairRepair provides useful guarantees about relative
completeness and semantic difference. We provide proofs of
these guarantees, validate them experimentally, and evaluate
FairRepair on two datasets that are used in the fairness liter-
ature. Our tool FairRepair has been submitted as conﬁdential
supplementary material, along with this paper submission.
The tool will be released open-source for the wider research
community, if and when our paper is published.

a) Decision Tree Learning Algorithm: In this paper, we
used scikit-learn DecisionTreeClassiﬁer to train the decision
trees and random forests. However, our algorithm is generic
and not limited to any particular tree learning algorithms. We
extract the path hypercubes of the decision trees at the very
beginning of the repair procedure, and did not use the tree
structure in the later steps. Since the decision tree paths and
path hypercubes have a one-to-one correspondence, there is
not restriction on the choice of the tree classiﬁer.

b) Accuracy of Models: The main task of our algorithm
is to repair a decision tree or a random forest such that it
satisﬁes a probabilistic group fairness requirement. In this
procedure, accuracy of the models would be affected, since we
modiﬁed the classiﬁcation of the nodes. It is desirable to have a
repaired model that is also accurate. However, it is not our ﬁrst
priority to maintain the accuracy, but to repair the model with
minimal change. Instead, we proposed the notion of semantic
change to measure the difference between the original and the

Fig. 13. Classiﬁcation accuracy of decision trees on Adult dataset on two
sensitive attributes

Fig. 14. Classiﬁcation accuracy of decision trees on German dataset on two
sensitive attributes

fairness deﬁnition that requires the difference (instead of the
ratio) of the passing rates to be bounded. They compute the
contribution to minimising the difference of ﬂipping each leaf
and use a greedy algorithm to ﬂip the leaves accumulatively.
Such deﬁnition restricts their algorithm to be applied to
binary sensitive attributes only. Aghaei et al. [2] attempted to
construct an optimization framework for learning optimal and
fair decision trees, which aims to mitigate unfairness caused by
both biased dataset and machine mis-classiﬁcation. However,
these works do not provide a solution for repairing an already
trained unfair decision tree.

d) DIGITS [3]: The work of [3] performs fairness repair
guided by input population distributions. In contrast, our
work is focused on rectifying an unfair decision tree from
an unfair dataset given as the input. A dataset appear to be
similar to an input probability distribution, but they represents
two different methodologies. Our approach is able to cure
unconscious biases in a decision making entity. With a dataset
as input, we ﬁrst construct a decision tree/random forest with
a good ﬁt,
then produce a minimal repair to the model.
The original models should closely capture the biases in the
dataset. This allows us to detect and to cure the implicit and
unconscious biases in the previous decision making procedure.
The repaired model could be used as a guideline for making
future decisions, and it also works as an explanation for the
previous unfairness. In contrast, DIGITS repairs a model that
already exists. The model is repaired regardless of its accuracy
or whether it captures the original biases in the training dataset.
Assuming the model closely resemble the original biases,
the DIGITS method requires continuing sampling data points
from the input distribution to achieve better convergence to
the optimal repair. This requires more information of the

14

repair model. Similar to maximising accuracy, our algorithm
aims to bound the semantic difference.

There are accuracy-based fairness deﬁnitions, like equal
opportunity which requires the false negative rates of the
sensitive groups to be similar, and overall accuracy equality
which requires the accuracy over all sensitive groups to be
similar. Such fairness notions should be considered indepen-
dently from group fairness.

c) Beyond Group Fairness: Our approach focuses on
group fairness [6], which requires members of the sensitive
groups to be classiﬁed at approximately the same rate. There
are, however, many other fairness notions such as causal
fairness or equal opportunity fairness [17]. We provide the
full algorithms including pre-processing and SMT encoding
of repairing decision trees with respect to these two fairness
deﬁnitions in Appendix A.

One can thus look into domain speciﬁc language (DSL)
designs that can capture different notions of fairness, and
integrate them with the repair process in FairRepair.

d) Qualiﬁcation Fairness: In our algorithm, we repaired
the models with respect to group fairness (or statistical parity),
which requires the passing rates of each sensitive group to be
similar, without considering the features of the datapoints that
received the desired outcome. In conditional statistical parity,
the passing rates are computed over controlled subgroups of
the sensitive groups. One typical control is qualiﬁcation. In
our loan example, qualiﬁed group fairness would require the
proportions of candidates that received a loan among qualiﬁed
males and females be similar. Our problem formulation does
not consider qualiﬁcation, since this requires specifying the
attributes that are legitimate to be used in making decisions.
This is not the main focus of our research, which makes
it possible that more qualiﬁed applicants do not receive the
desired outcome, while the less qualiﬁed do.

e) Decision Trees and Beyond: We believe that our
repair approach is applicable to other types of models whose
behavior on an input can be symbolically expressed in a closed
form, such as a set of constraints for a decision tree, random
forest, or an SVM model. We implemented our approach,
however, only for decision trees. Additionally, by relying on
sci-kit learn, we adopted several limitations, such as the need
for a OneHot encoding of categorical attributes. We hope
to explore other model variants in our future work. One
interesting feature of FairRepair is that it explicitly uses the
sensitive attributes in the output decision tree. This is partly
a dependency of our approach on decision trees, which needs
to be investigated further for other models.

f) Beyond fairness.: Fairness is an example of a hyper-
traces [8]. There are other
property over collections of
types of hyper-properties of interest such as robustness, side-
channel, non-interference. These can all be expressed as hyper-
properties over a set of program traces. At a high level,
our approach indicates the promise of hyper-property driven
automated repair. This raises the prospect of future generation
program repair systems being systematically supported by
SMT solver back-ends with a hyper-property plugin.

APPENDIX A
OTHER FAIRNESS DEFINITIONS

The algorithm in Section IV explains how we can repair a
decision tree or a random forest with respect to group fairness,
or statistical parity. There are some other fairness notions
raised in the community. In particular, we are interested in
fairness deﬁnitions based on similarity, and fairness deﬁnitions
based on precision.

A. Causal Discrimination

We shall still explain the deﬁnitions in the context of
a binary decision making problem. One representative of
similarity-based fairness measure is causal fairness, which is
adapted from causal discrimination score deﬁned in [15]. It
requires applicants who have similar attributes to receive the
same outcome. In particular, if two applicants only differ in the
sensitive attributes, they should both be classiﬁed as 0 or 1. If
they receive different outcomes, we refer them as experienced
individual unfairness. The causal discrimination score S is
measured by the size of the proportion (as a ratio between 0
and 1) among input population, which contains individuals that
by only changing its sensitive attributes, the decision outcome
also changes. A fairness threshold c for causal fairness is a
real number between 0 and 1, such that the decision making
model is called causally fair if S ≤ c.

To repair a decision tree T with respect to causal fairness,
we start with pre-processing T . We ﬁrst collect
the path
hypercubes of T . Let H := {H1, . . . , Hk} denote the set of
path hypercubes of T , and let D denote the input dataset. Note
that H is a partition of the input space. For each datapoint
d ∈ D, there exists a unique Hi ∈ H such that d ∈ Hi. Let
M : D → {1, 2, . . . , k} be a mapping such that HM(d) is
this unique path hypercube. We use HM(d)(d) to denote the
predicted outcome of d, i.e., HM(d)(d) = T (d). Let A be the
sensitive attribute. There can be multiple sensitive attributes.
Here we use a single sensitive attribute for illustration. Note
that we force A to be categorical. Let M := {A1, . . . , Am}
be the set of sensitive groups, where each Ai is a valuation
of A. We group datapoints in D based on their non-sensitive
attributes. Two datapoints d1 and d2 are in the same group D
if and only if they differ only in the sensitive attribute. We
abuse the notation by letting D = {D1, . . . , Dl}. For each
D ∈ D, if

F(D) =

(cid:95)

HM(d)(d) (cid:54)= HM(d(cid:48) )(d

(cid:48)

)

d,d(cid:48) ∈D
(cid:48)
= ∃d, d

∈ D.HM(d)(d) (cid:54)= HM(d(cid:48) )(d

(cid:48)

)

holds, then D contributes to the causal discrimination. In
particular,

(cid:80)

|D|

S =

D∈D,D|=F

|D|

.

We next construct the SMT formulas. For each path hyper-
cube Hi, assign a pseudo-Boolean SMT variable Xi (Xi ∈
{0, 1}, representing a Boolean value). For points d ∈ D,

15

we use XM(d)
to denote the Boolean variables assigned
to path hypercube HM(d). Note that XM(d) and XM(d(cid:48) )
may refer to the same Boolean variable Xi for d (cid:54)= d
if
) = i. Substituting Xi’s into expression F(D),
M(d) = M(d
we get (cid:87)
(cid:54)= XM(d(cid:48) ), which represents that at
XM(d)

(cid:48)

(cid:48)

d,d(cid:48) ∈D

(cid:48)

∈ D have different outcomes. Thus the

least two points d, d
fairness requirement is formulated as:
(cid:87)
d,d(cid:48) ∈D

(cid:80)
D∈D

XM(d) (cid:54)= XM(d(cid:48) ) · |D|

|D|

≤ c,

where the numerator of the left hand side represents datapoints
that experience individual unfairness. By dividing the size
of the dataset, the fraction must be bounded by the fairness
threshold c.

Similar to group fairness, we want

to account for the
semantic change as well. We ﬁrst compute the initial causal
discrimination score S. If S > c, then the minimal (theoretical)
semantic change is S −c. This value is the minimal proportion
in the input population that have to be assigned an opposite
outcome to achieve causal fairness. (If S ≤ c then the decision
tree is causally fair and needs no repair.) We abuse the notation
the number of datapoints reside in Hi,
|Hi|
and use Hi to also represent the initial return value of the
hypercube. With α being the semantic difference bound, the
semantic change is encoded as follows:

to represent

(cid:80)
Hi∈H

Xor(Hi, Xi) · |Hi|

|D|

≤ α · (S − c)

B. Fairness by comparing predicted and actual outcomes

The fairness deﬁnitions listed in this section compare the
predicted outcome and actual outcome of datapoints. Before
we proceed to the deﬁnitions, we denote T (d) as the predicted
outcome of datapoint d, and denote Y (d) as the actual outcome
of d. We shall omit d and only write T and Y when d
is clear or not speciﬁed. For illustration, we consider the
sensitive groups to be M = {male, female}. In general,
M = {A1, . . . , Am}, where m is the number of sensitive
groups in the decision making scenario.

The deﬁnitions in this section uses statistical metrics like
false negative and positive predictive value. We assume the
readers are familiar with these terms.

Deﬁnition A.1 (Equal Opportunity). A decision making model
satisﬁes equal opportunity if each sensitive group has similar
false negative rate. In particular, it requires that P (T = 0|Y =
1, male) = P (T = 0|Y = 1, female). The fractions of input
population having actual outcome 1 that are classiﬁed to be
0 should be similar among each sensitive groups.

In practice, it is unlikely that the false negative rates of
each sensitive group are all equal. Hence we deﬁne a fairness
threshold c to be a real value between 0 and 1, such that if for
any two sensitive groups Ai and Aj, P (T = 0|Y = 1, Ai) ≥
c · P (T = 0|Y = 1, Aj), then the decision making model
satisﬁes equal opportunity.

Note that the measurement of fairness for equal opportunity
is similar to group fairness, with passing rates replaced with
false negative rates. We shall not go through the detailed pre-
processing steps again. Assume we have obtained the path
hypercubes split based on the sensitive groups. Let Si, 1 ≤
i ≤ m be the set of path hypercubes for each sensitive group
Ai. Each Si contains path hypercubes Hi,j’s where j indexes
over the path hypercubes in the sensitive group Ai. The setting
is the same as that of group fairness.

To repair a decision tree T with respect to predictive parity,
we ﬁrst compute the false negative rates for each sensitive
group, and compute the minimal (theoretical) semantic change
required to repair the tree. For sensitive group Ai, the false
negative rate Fi is as follows:

Fi = P (T = 0|Y = 1, Ai)

=

|{d ∈ D : d is in Ai, T (d) = 0 and Y (d) = 1}|
|{d ∈ D : d is in Ai and Y (d) = 1}|

.

Let ri denote |{d ∈ D : d is in Ai, T (d) = Y (d) = 1}|. We
want to compute xi for each ri
that is the false negative
rate after repair We construct the following linear optimisation
problem to compute the minimal semantic change, where m
is the number of sensitive groups.


xi
|{d ∈ D : d is in Ai and Y (d) = 1}|

,

∀ 1 ≤ i, j ≤ m, Fi =




Fi
Fj

∀ 1 ≤ i, j ≤ m,

≥ c,

∀ 1 ≤ i ≤ m, 0 ≤ Fi ≤ 1,
M
(cid:80)
i=1

|xi − ri|.

minimise

We next proceed to construct the SMT formulas for fairness

and semantic requirement. For simplicity, denote

Di := |{d ∈ D : d is in Ai and Y (d) = 1}|

to be the set of datapoints in sensitive group Ai that have
outcome 1. Similar to that in causal fairness repair, let Mi :
Di → {1, . . . , |Si|} be the function that maps each point d ∈
Di to the index of the unique path hypercube it resides in,
i.e., d ∈ Hi,Mi(d). Assign a pseudo-Boolean SMT variable
Xi,j to each Hi,j. We abuse the notation to also allow Hi,j
to represent the Boolean outcome of the path hypercube Hi,j.

The false negative rate Fi for Ai is represented by:

(cid:80)
d∈Di

¬Xi,j

|Di|

.

Fi =

Note that, Xi,j’s are pseudo-Boolean and (cid:80)
d∈Di
negative integer. The fairness requirement is thus encoded as:

¬Xi,j is a non-

∀1 ≤ i, j ≤ m, Fi ≥ Fj.

The semantic difference requirement is as follows:

(cid:88)

(cid:88)

1≤i≤m

d∈Di

Xor(Xi,M(d), Hi,M(d)) ≤ α ·

M
(cid:88)

i=1

|xi − ri|.

16

[20] KILBERTUS, N., ROJAS-CARULLA, M., PARASCANDOLO, G., HARDT,
M., JANZING, D., AND SCH ¨OLKOPF, B. Avoiding discrimination
In Proceedings of the 31st International
through causal reasoning.
Conference on Neural Information Processing Systems (NIPS) (2017),
Curran Associates Inc., p. 656–666.

[21] KIM, D., NAM, J., AND SONG, J. Automatic patch generation learned

from human-written patches. pp. 802–811.

[22] KUSNER, M., LOFTUS, J., RUSSELL, C., AND SILVA, R. Counterfac-
In Proceedings of the 31st International Conference on
tual fairness.
Neural Information Processing Systems (NIPS) (2017), Curran Asso-
ciates Inc., p. 4069–4079.

[23] KWIATKOWSKA, M., NORMAN, G., AND PARKER, D. Advances and
challenges of probabilistic model checking. In 2010 48th Annual Aller-
ton Conference on Communication, Control, and Computing (Allerton)
(2010), pp. 1691–1698.

[24] LE GOUES, C., PRADEL, M., AND ROYCHOUDHURY, A. Automated

program repair. Communications of the ACM 62, 12 (2019).

[25] LONG, F., AND RINARD, M. Automatic patch generation by learning
correct code. In ACM SIGPLAN Symposium on Principles of Program-
ming Languages (POPL) (2016).

[26] LUCKOW, K., P ˘AS ˘AREANU, C., DWYER, M., FILIERI, A., AND
VISSER, W. Exact and approximate probabilistic symbolic execution
for nondeterministic programs. In Proceedings of the 29th ACM/IEEE
International Conference on Automated Software Engineering (ASE)
(2014), pp. 575–586.

[27] MECHTAEV, S., YI, J., AND ROYCHOUDHURY, A. Angelix: Scalable
multiline program patch synthesis via symbolic analysis. In Proceedings
of the 38th International Conference on Software Engineering (ICSE)
(2016), Association for Computing Machinery, p. 691–701.

[28] NGUYEN, H., QI, D., ROYCHOUDHURY, A., AND CHANDRA, S.
In Proceedings of
Semﬁx: Program repair via semantic analysis.
the International Conference on Software Engineering (ICSE) (2013),
pp. 772–781.

[29] PEDREGOSA, F., VAROQUAUX, G., GRAMFORT, A., MICHEL, V.,
THIRION, B., GRISEL, O., BLONDEL, M., PRETTENHOFER, P.,
WEISS, R., DUBOURG, V., VANDERPLAS, J., PASSOS, A., COURNA-
PEAU, D., BRUCHER, M., PERROT, M., AND DUCHESNAY, E. Scikit-
Journal of Machine Learning
learn: Machine learning in Python.
Research 12 (2011), 2825–2830.

[30] RAY, B., HELLENDOORN, V., GODHANE, S., TU, Z., BACCHELLI, A.,
AND DEVANBU, P. On the ”naturalness” of buggy code. pp. 428–439.
[31] SAMPSON, A., PANCHEKHA, P., MYTKOWICZ, T., MCKINLEY, K.,
GROSSMAN, D., AND CEZE, L. Expressing and verifying probabilistic
assertions. ACM SIGPLAN Notices 49 (2014).

[32] SHMATIKOV, V. Probabilistic model checking of an anonymity system.

Journal of Computer Security 12 (2004).

[33] UDESHI, S., ARORA, P., AND CHATTOPADHYAY, S. Automated di-
In International Conference on Automated

rected fairness testing.
Software Engineering (ASE) (2018).

[34] WEIMER, W., NGUYEN, T., LE GOUES, C., AND FORREST, S. Auto-
matically ﬁnding patches using genetic programming. In Proceedings
of the 31st International Conference on Software Engineering (ICSE)
(2009), IEEE Computer Society, p. 364–374.

[35] XUAN, J., MARTINEZ, M., DEMARCO, F., CLEMENT, M., MARCOTE,
S. L., DURIEUX, T., BERRE, D. L., AND MONPERRUS, M. Nopol:
Automatic repair of conditional statement bugs in java programs.

The above is only one of the fairness deﬁnitions based on
precision. Here we do not list all of them. The readers may
ﬁnd more for their own interests.

REFERENCES

[1] AGGARWAL, A., LOHIA, P., NAGAR, S., DEY, K., AND SAHA, D.
Blackbox fairness testing of machine learning models. In International
Symposium on Foundations of Software Engineering (FSE) (2019).
[2] AGHAEI, S., AZIZI, M. J., AND VAYANOS, P. Learning optimal and fair
In The Thirty-
decision trees for non-discriminative decision-making.
Third AAAI Conference on Artiﬁcial Intelligence, AAAI (2019), AAAI
Press, pp. 1418–1426.

[3] ALBARGHOUTHI, A., D’ANTONI, L., AND DREWS, S. Repairing
decision-making programs under uncertainty. In International Confer-
ence on Computer Aided Veriﬁcation (CAV) (2017).

[4] ALBARGHOUTHI, A., D’ANTONI, L., DREWS, S., AND NORI, A.
Fairsquare: Probabilistic veriﬁcation for program fairness. In Interna-
tional Symposium on Object-oriented Programming Systems, Languages
and Applications (OOPSLA) (2017).

[5] BASTANI, O., ZHANG, X., AND SOLAR-LEZAMA, A. Probabilistic
In International
veriﬁcation of fairness properties via concentration.
Symposium on Object-oriented Programming Systems, Languages and
Applications (OOPSLA) (2019).

[6] CALDERS, T., KAMIRAN, F., AND PECHENIZKIY, M. Building classi-
ﬁers with independency constraints. In IEEE International Conference
on Data Mining Workshops (2009), pp. 13–18.

[7] CLARKE, E., AND ZULIANI, P. Statistical model checking for cyber-
physical systems. In Automated Technology for Veriﬁcation and Analysis
(ATVA) (2011), Springer, pp. 1–12.

[8] CLARKSON, M. R., AND SCHNEIDER, F. B. Hyperproperties.

J.

Comput. Secur. 18, 6 (2010), 1157–1210.

[9] CORBETT-DAVIES, S., PIERSON, E., FELLER, A., GOEL, S., AND
HUQ, A. Algorithmic decision making and the cost of fairness.
In
Proceedings of the 23rd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining (KDD) (2017), Association for
Computing Machinery, p. 797–806.

[10] DWORK, C., HARDT, M., PITASSI, T., REINGOLD, O., AND ZEMEL,
R. Fairness through awareness. In Proceedings of the 3rd Innovations
in Theoretical Computer Science Conference (ITCS) (2012), Association
for Computing Machinery, p. 214–226.

[11] FELDMAN, M., FRIEDLER, S. A., MOELLER, J., SCHEIDEGGER, C.,
AND VENKATASUBRAMANIAN, S. Certifying and removing disparate
the 21th ACM SIGKDD International
impact.
Conference on Knowledge Discovery and Data Mining (KDD) (2015),
Association for Computing Machinery, p. 259–268.

In Proceedings of

[12] FILIERI, A., P ˘AS ˘AREANU, C. S., AND VISSER, W. Reliability analysis
in symbolic pathﬁnder. In Proceedings of the 2013 International Confer-
ence on Software Engineering (ICSE) (2013), IEEE Press, p. 622–631.
[13] FISH, B., KUN, J., AND LELKES, ´A. D. A conﬁdence-based approach
for balancing fairness and accuracy. In Proceedings of the 2016 SIAM
International Conference on Data Mining (2016), SIAM, pp. 144–152.
[14] FRIEDLER, S. A., SCHEIDEGGER, C., VENKATASUBRAMANIAN, S.,
CHOUDHARY, S., HAMILTON, E. P., AND ROTH, D. A compar-
ative study of fairness-enhancing interventions in machine learning.
In Proceedings of
the Conference on Fairness, Accountability, and
Transparency (FAT) (2019), Association for Computing Machinery,
p. 329–338.

[15] GALHOTRA, S., BRUN, Y., AND MELIOU, A. Fairness testing: Testing
software for discrimination. In Proceedings of the 11th Joint Meeting on
Foundations of Software Engineering (ESEC/FSE) (2017), p. 498–510.
[16] GELDENHUYS, J., DWYER, M., AND VISSER, W. Probabilistic sym-
International Symposium on Software Testing and

bolic execution.
Analysis (ISSTA) (2012), 166–176.

[17] HARDT, M., PRICE, E., AND SREBRO, N. Equality of opportunity
the 30th International
in supervised learning.
Conference on Neural Information Processing Systems (NIPS) (2016),
Curran Associates Inc., p. 3323–3331.

In Proceedings of

[18] HARMAN, M. Automated patching techniques: the ﬁx is in: technical

perspective. Commun. ACM 53 (01 2010), 108.

[19] KAMIRAN, F., CALDERS, T., AND PECHENIZKIY, M. Discrimination
aware decision tree learning. In Proceedings of the IEEE International
Conference on Data Mining (ICDM) (2010), IEEE Computer Society,
p. 869–874.

17

