Reinforcement Learning for Linear Quadratic Control is Vulnerable
Under Cost Manipulation

Yunhan Huang1 and Quanyan Zhu1

timeliness, and consistency of the feedback information are
unlikely to be guaranteed in practice, especially in the
presence of adversarial interventions. Without consistent or
accurate feedback from the environment, the agent can either
fail to learn an implementable control policy or be tricked
into a â€˜nefariousâ€™ control policy favored by the adversaries.
In the last ï¬ve years, there has been a surge in the number
of research studies that focus on the security threats faced
by RL with discrete state space [13], [16]â€“[23]. Very few,
if not none, of these studies have investigated the potential
security threats that can impede RL-enabled control systems.
In this paper, we take the initiative to look into the security
threats faced by RL-enabled control systems. In particular,
we are interested in the deception of a Linear-Quadratic-
Gaussian (LQG) agent by manipulating the cost signals. The
attacker falsiï¬es the cost signals received by the LQG agent
to trick the agent into learning some â€˜nefariousâ€™ policies
favored by the attacker. In RL-enabled systems, the agent
obtains cost signals from user feedback, as is illustrated in 1.
For example, in recommendation systems, the costs are often
related to the number of user clicks, purchases, or add-to-cart
rate [24]; Cost signals are represented by user sentiment or
engagement in RL-enabled dialogue generation [23], [25]; in
human-robot interaction, cost signals are generated according
to the level of dissatisfaction the human has while interacting
with the robot [26], [27]. Such occasions open a back
door for the attacker to inï¬‚uence the RL-enabled system
by manipulating the cost signals. Beyond that, in networked
control systems where the agent is remote to the plant, cost
signals can be falsiï¬ed or jammed by the attacker when
transmitting from the plant to the agent [18], [19].

2
2
0
2

r
p
A
7

]

Y
S
.
s
s
e
e
[

2
v
4
7
7
5
0
.
3
0
2
2
:
v
i
X
r
a

Abstractâ€” In this work, we study the deception of a Linear-
Quadratic-Gaussian (LQG) agent by manipulating the cost
signals. We show that a small falsiï¬cation of the cost parameters
will only lead to a bounded change in the optimal policy. The
bound is linear on the amount of falsiï¬cation the attacker
can apply to the cost parameters. We propose an attack
model where the attacker aims to mislead the agent into
learning a â€˜nefariousâ€™ policy by intentionally falsifying the cost
parameters. We formulate the attackâ€™s problem as a convex
optimization problem and develop necessary and sufï¬cient
conditions to check the achievability of the attackerâ€™s goal.

We showcase the adversarial manipulation on two types of
LQG learners: the batch RL learner and the other is the
adaptive dynamic programming (ADP) learner. Our results
demonstrate that with only 2.296% of falsiï¬cation on the cost
data, the attacker misleads the batch RL into learning the
â€™nefariousâ€™ policy that leads the vehicle to a dangerous position.
The attacker can also gradually trick the ADP learner into
learning the same â€˜nefariousâ€™ policy by consistently feeding
the learner a falsiï¬ed cost signal that stays close to the actual
cost signal. The paper aims to raise peopleâ€™s awareness of the
security threats faced by RL-enabled control systems.

I. INTRODUCTION

The adoption of machine learning (ML), especially rein-
forcement learning (RL), in control theory and engineering
enables the agent
to learn a high-quality control policy
without knowing the model or the cost criteria [1]â€“[6].
The agent learns either by interacting with the environment
through communication channels [1], [3] or by processing
existing datasets from previous experience [2]. However, the
incorporation of learning into control enlarges the attack
surface of the underlying system and creates opportunities
for the adversaries. Adversarial parties can launch attacks
such as Denial-of-Service attacks (DoS) [7], [8], False Data
Injection (FDI) attacks [9], [10], spooï¬ng attacks [11], [12]
on the communication channels, and data poisoning attacks
[13], [14] on the existing dataset to mislead the agent and
sabotage the underlying system. If not dealt with properly,
such attacks can lead to a catastrophe in the underlying
system. For example, self-driving platooning vehicles can
collide with each other when measurement data is manip-
ulated [15]. It is, hence, critical to study security threats to
learning algorithms and design effective defense mechanisms
to safeguard the learning-enabled control system.

The successful application of RL in control systems relies
on accurate, timely, and consistent feedback from the envi-
ronment or reliable datasets from experience. The accuracy,

1 Y. Huang and Q. Zhu are with the Department of Electrical and
Computer Engineering, New York University, 370 Jay St., Brooklyn, NY.
{yh.huang, qz494}@nyu.edu

Fig. 1: Reinforcement learning for control under adversarial
manipulation on cost signals.

1

 
 
 
 
 
 
We characterize the fundamental limits of cost manipu-
lation, i.e., how much the attacker needs to falsify the cost
signals to steer the learned control policy from an optimal
one to a â€˜nefariousâ€™ one favored by the attacker. Given a
â€˜nefariousâ€™ control policy the attacker aims for, we formulate
the attackâ€™s problem as an optimization problem, which is
proved to be convex. We show that the attacker cannot
deceive the agent into learning some policies no matter how
the attacker falsiï¬es the cost signals. Hence, the optimization
problem is infeasible for these policies. We develop a set of
conditions in the frequency domain under which the attacker
can mislead the agent into learning the â€˜nefariousâ€™ policy.

We showcase the deception of two types of LQG learners:
one is the batch RL learner [2] and the other is the adaptive
dynamic programming (ADP) learner [1]. The batch RL
learner ï¬rst estimates the system and cost parameters using
a pre-collected dataset. The dataset includes the data points
that record the state and control trajectories and correspond-
ing cost signals. Then the agent computes the learned policy
using the estimated system and cost parameters. Suppose the
attacker can falsify the cost signals in the dataset. We show
that by only falsifying 2% of the cost signals (in magnitude),
the attacker can trick the agent into a policy that steers a
vehicle into a dangerous position.

Another is the ADP learner [1]. The ADP learner updates
its estimates about the ğ‘„-function adaptively by interact-
ing with the environment (plant) online. The ADP learner
receives cost signals and state measurements from the en-
vironment and applies controls to excite the environment.
Suppose the attacker can falsify the cost signals transmitted
through some communication channels. We show that the
attacker can craft attacks on cost signals by simply solving
a convex optimization problem. The crafted attacks falsify
the cost signals in a way such that the falsiï¬ed cost signals
and the true cost signals remain close when the system
becomes stabilized. The experiment shows that the attacker
can gradually mislead the agent into learning a â€˜nefariousâ€™
policy favored by the attacker.

While most studies focus on improving the performance of
RL algorithms in control (e.g., convergence rate, robustness,
data efï¬ciency, computational complexity), very few studies
pay attention to the security threats faced by RL algorithms
in control. The theories and examples presented in this
paper demonstrate the vulnerabilities of RL-enabled control
systems, which shows the necessity of investigating the
potential security challenges faced by RL in control.

Related works: There is a recent

trend in studying
security threats faced by RL algorithms [13], [16]â€“[23], [28],
[29].We can taxonomize these studies by the three types of
attacks targeting at three different signals: attacks on the
reward or cost signals [13], [18], [19], [21], [23], attacks
on the state sensing [15], [20], [22], attacks on the action
execution [29]. One can refer to Section 5 of [30] for a
brief review of this topic. Most of these studies focus on
RL with discrete state and action spaces, and very few
consider the security threats faced by RL-enabled control
systems. Ma et al. [13] studies data poisoning attacks on

2

the reward data on batch RL. Beyond the case of discrete
state space, the authors also demonstrate the effectiveness of
the attacks on an LQR learner using batch data. Our paper
goes beyond batch RL and focuses on the deception of LQG
agents through manipulating cost signals in a general setting.
Hence, the results in this paper can be extended to different
learning schemes and will not be limited to batch RL. In
addition to demonstrating the effectiveness of the attacks
through numerical experiments, we also develop theoretical
underpinnings to understand the fundamental limits of such
attacks, e.g., whether the attackerâ€™s goal can be achieved or
not, how much falsiï¬cation is needed to achieve such goals.
Notation: Denote the set of non-negative real numbers by
R
. Let C be the complex plane. Let Sğ‘› the set of all real
+
symmetric matrices of order ğ‘›. Denote the set of all posi-
tive semi-deï¬nite (respectively, positive deï¬nite) symmetric
matrices by S
). Given ğ‘€, ğ‘
ğ‘
+
(respectively, ğ‘€
ğ‘ is positive semi-deï¬nite
(respectively, positive deï¬nite). We denote ğ¼ğ‘› the identity
matrix of order ğ‘›, and ğ¼ is the identity matrix whose order
depends on the context. Throughout the paper, prime denotes
the transpose.

(respectively, S
++
ğ‘) means ğ‘€
âˆ’

S, ğ‘€

(cid:23)

â‰»

âˆˆ

For a matrix ğ‘€
âˆˆ
vectorization of ğ‘€:

S of order ğ‘›, deï¬ne Î˜

as the half-

ğ‘€

)

(

âˆˆ

â‰”

Î˜

ğ‘€

)

(

ğ‘š1,1,

Â· Â· Â·

, ğ‘š1,ğ‘›, ğ‘š2,2,

, ğ‘š2,ğ‘›,

, ğ‘šğ‘›
âˆ’

1

1,ğ‘›

âˆ’

â€² .

Â· Â· Â·

Â· Â· Â·

Here, ğ‘šğ‘–, ğ‘— is the element in the ğ‘–-th row and the ğ‘—-th column
(cid:2)
Rğ‘›, deï¬ne
of ğ‘€. For a vector ğ‘¥

(cid:3)

Â¯ğ‘¥ â‰”

ğ‘¥2
1, 2ğ‘¥1ğ‘¥2

, 2ğ‘¥1ğ‘¥ğ‘›, ğ‘¥2
2,

, 2ğ‘¥2ğ‘¥ğ‘›,

, ğ‘¥2
ğ‘›

â€² .

ğ‘˜

(

)

(

(cid:2)

k

ğœŒ

0.

Â· Â· Â·

Â· Â· Â·

k â‰¤

k/
ğ‘€

k Â· k

N
âˆˆ
ğœ

ğ‘€
(
ğœŒ

Â· Â· Â·
The Frobenius norm of a matrix is denoted by

ğ¹ . The
(cid:3)
k Â· k
refers to the Euclidean norm for vectors and the
norm
spectral norm for matrices unless speciï¬ed otherwise. For a
ğ‘›, the spectral radius of ğ‘€ is denoted
real matrix ğ‘€
ğ‘€ ğ‘˜
by ğœŒ
ğ‘€
ğ‘€
as the
}
{k
)
ğ‘˜ for all
ğ‘€, ğœŒ
smallest value such that
)
ğ‘˜

Rğ‘›
Ã—
âˆˆ
. Deï¬ne ğœ

â‰” supğ‘˜
ğ‘€ ğ‘˜

â‰¥
Organization of the paper: In Section II, we introduce
the LQG problem with general quadratic cost and present
some preliminary results regarding the LQG problem. Sec-
tion III proposes the problem of adversarial manipulation
on the cost parameters, the fundamental limits of what the
attacker can or cannot achieve. Section III lays a theoretical
foundation for the attack models on two popular learning
methods: the Batch RL method and the ADP method, which
are introduced in IV and V. We demonstrate the results using
numerical examples in Section VI.

)
ğ‘€

))

(

(

(

II. LQG WITH GENERAL QUADRATIC COST:
PRELIMINARIES AND BACKGROUND

Consider the discrete-time, multi-variable system

ğ‘¥ğ‘¡ , ğ‘¢ğ‘¡ , ğ‘¤ğ‘¡

ğ‘¥ğ‘¡

1 = ğ‘”
(
+
Rğ‘›, ğ‘¡ = 0, 1,

â‰” ğ´ğ‘¥ğ‘¡

ğµğ‘¢ğ‘¡

ğ¶ğ‘¤ğ‘¡ ,

)
+
is the system state, ğ‘¢ğ‘¡

+

(1)

âˆˆ
is the control input, ğ‘¤ğ‘¡

where ğ‘¥ğ‘¡
0, 1,
âˆˆ
the standard Gaussian distribution
ğµ

ğ‘š, and ğ¶

Â· Â· Â·
Rğ‘›
Ã—

Rğ‘›
Ã—

Â· Â· Â·

ğ‘.

âˆˆ

âˆˆ

Rğ‘š, ğ‘¡ =
Rğ‘ is drawn ğ‘–.ğ‘–.ğ‘‘. from
ğ‘›,
, ğ´

0, ğœ2ğ¼ğ‘

Rğ‘›
Ã—

âˆˆ

N (

)

âˆˆ

Consider a stationary control policy ğœ‹ : Rğ‘›

the form

Rğ‘š taking

â†’

ğ‘¢ğ‘¡ = ğœ‹

(

ğ‘¥ğ‘¡

= ğ¾ğ‘¥ğ‘¡

ğ‘˜,

)
Rğ‘š. The stage-wise cost of the

+

(2)

where ğ¾
âˆˆ
system is quadratic

Rğ‘š
Ã—

ğ‘› and ğ‘˜

âˆˆ

ğ‘ğ‘¡ = ğ‘

ğ‘¥, ğ‘¢

= ğ‘¥ â€²ğ·ğ‘¥

+
for some positive semi-deï¬nite ğ·
Rğ‘›, and scalar ğ‘Ÿ.
S

, vector ğ‘‘

(

)

++

âˆˆ
Assumption 1. Assume that
ğ´, ğ·1
/

is observable.

2

(
Assumption 2. ğµ has full column rank.

)

ğ‘‘ â€²ğ‘¥

ğ‘Ÿ

ğ‘¢â€²ğ¸ğ‘¢,

+

+
, positive deï¬nite ğ¸

S
+

âˆˆ

(3)

âˆˆ

ğ´, ğµ

)

(

is controllable and

Deï¬nition 1. We say ğ¾
ğ´

Rğ‘š
Ã—
ğµğ¾ is Schur stable, i.e., ğœŒ

âˆˆ

ğ‘› is stabilizing if the matrix
ğ´

< 1.

ğµğ¾

)

+

+
(
We consider the total cost as the discounted accumulated
costs over an inï¬nite horizon. Starting at state ğ‘¥ğ‘¡ under
control policy characterized by ğ¾ and ğ‘˜, the total cost is
âˆğ‘–=0 ğ›¾ğ‘–ğ‘ğ‘¡
ğ‘‰ğ¾ ,ğ‘˜
1 is the dis-
, where 0
.
count factor and the expectation is over
Â· Â· Â· }
It is well known that if ğ¾ is stabilizing, ğ‘‰ğ¾ ,ğ‘˜ takes the

â‰¤
ğ‘¤ğ‘¡ , ğ‘¡ = 0, 1, 2,
{

= E

ğ‘–
+

Ã

ğ‘¥ğ‘¡

ğ‘¥ğ‘¡

â‰¤

ğ›¾

)

]

[

(

|

form [31]:

ğ‘‰ğ¾ ,ğ‘˜

ğ‘¥ğ‘¡

(

)

= ğ‘¥ â€²ğ‘¡ ğ‘ƒğ¾ ğ‘¥ğ‘¡

â„â€²ğ¾ ,ğ‘˜ğ‘¥

+

+

ğ‘™ğ¾ ,ğ‘˜,

Note that the subscript of ğ‘ƒğ¾ is ğ¾ instead of ğ¾, ğ‘˜ because
ğ‘ƒğ¾ only depends on ğ¾. Let ğ¾âˆ— and ğ‘˜ âˆ— characterize the policy
which is optimal in a sense that the total discount cost of
every state is minimized. The value function is deï¬ned by

ğ‘‰ âˆ—

ğ‘¥

(

S

= ğ‘¥ â€²ğ‘ƒâˆ—ğ‘¥

â‰” ğ‘‰ğ¾ âˆ—,ğ‘˜âˆ— (
)
, â„âˆ— âˆˆ

ğ‘¥
)
Rğ‘›, and ğ‘™âˆ— âˆˆ

where ğ‘ƒ
the optimal policy can be characterized by [31]

++

âˆˆ

â„âˆ—â€²ğ‘¥

ğ‘™âˆ—,

+

+

R. It is well known that

ğ‘¢âˆ—ğ‘¡

= ğ¾âˆ—ğ‘¥ğ‘¡

ğ‘˜ âˆ—,

+

ğ¾âˆ— =
ğ‘˜ âˆ— =

ğ›¾

âˆ’

âˆ’(

(
ğ›¾

ğ¸

+
2
/

) (

ğ›¾ğµâ€²ğ‘ƒâˆ—ğµ
ğ¸

âˆ’
)
ğ›¾ğµâ€²ğ‘ƒâˆ—ğµ

+

1ğµâ€²ğ‘ƒâˆ— ğ´,

1ğµâ€²â„âˆ—,

âˆ’

)

ğ›¾ ğ´â€²ğ‘ƒâˆ— ğ´
ğ´
ğ›¾

ğ›¾2 ğ´â€²ğ‘ƒâˆ—ğµ
âˆ’
â€²â„âˆ—,
ğµğ¾âˆ—
)
+
Î£ğ‘¤ğ¶ â€²ğ‘ƒâˆ—ğ¶

(
ğ›¾
2 Tr

) âˆ’

(

ğ¸

(

+

ğ›¾ğµâ€²ğ‘ƒâˆ—ğµ

)

1ğµâ€²ğ‘ƒâˆ— ğ´,

âˆ’

ğ¸

(

+

ğ›¾ğµâ€²ğ‘ƒâˆ—ğµ

1ğµâ€²â„âˆ—

)âˆ’

.

ğ›¾2
2 â„âˆ—â€²ğµ
ğ›¾
1

âˆ’

Lemma 1. Under Assumption 1,
uniquely decided by the system parameters
ğ·, ğ¸, ğ‘‘
cost parameters

.

the pair

(

(
ğ´, ğµ

is
and the

ğ¾âˆ—, ğ‘˜ âˆ—)
)

(

)

The proof of Lemma 1 is presented in Appendix VII-B.
ğ¾âˆ—, ğ‘˜ âˆ—)
is uniquely determined by
Since the optimal policy
ğ´, ğµ
ğ´, ğµ
satisfying
, for a given system
(
Assumption 1, we can deï¬ne the an auxiliary notation for
the solution

to the discrete-time LQG as

ğ·, ğ¸, ğ‘‘

ğ¾, ğ‘˜

and

)

(

(

)

(

)

(

)
ğ¾, ğ‘˜

(

= DLQG

(

)

ğ·, ğ¸, ğ‘‘, ğ´, ğµ

,

)

3

where

where

ğ‘ƒâˆ— = ğ·
â„âˆ— = ğ‘‘

ğ‘Ÿ

ğ‘™âˆ— =

+

+

+

(4)

(5)

(6)

ğ‘›

Sğ‘š
where the mapping DLQG : Sğ‘›
â†’
Rğ‘š
Rğ‘š is well-deï¬ned and characterized by the relations
Ã—
(4)-(6). Since we are particularly interested in the manipula-
ğ·, ğ¸, ğ‘‘
tion of the cost signals, we write
for simplicity.

= DLQG

Rğ‘›
Ã—

Rğ‘›
Ã—

ğ¾, ğ‘˜

++ Ã—

+ Ã—

Rğ‘›

Ã—

Ã—

Ã—

ğ‘š

)

(

(

)

ğ‘›

A. Q-Functions and Policy Improvement

Deï¬ne the ğ‘„-function for a stabilizing policy ğ¾, ğ‘˜ as

ğ‘”

ğ‘¥, ğ‘¢, ğ‘¤

ğ‘¥

(7)

ğ‘„ğ¾ ,ğ‘˜

ğ‘¥, ğ‘¢

= ğ‘

ğ‘¥, ğ‘¢

ğ›¾Eğ‘¤

ğ‘‰ğ¾ ,ğ‘˜
[

(

(

(

) +

)
ğ‘¥, ğ‘¢

(
(
the value ğ‘„ğ¾ ,ğ‘˜
is the immediate cost of taking control
ğ‘¢ from state ğ‘¥ plus the expected cost-to-go starting at the
R is
next state ğ´ğ‘¥
deï¬ned for all states ğ‘¥ and all admissible controls ğ‘¢. The
expression can be written as

ğ¶ğ‘¤. And ğ‘„ ğ¾ ,ğ‘˜ : Rğ‘›

Rğ‘š

ğµğ‘¢

â†’

))|

Ã—

+

+

]

)

ğ‘„ ğ¾ ,ğ‘˜
=ğ‘¥ â€²ğ·ğ‘¥

ğ‘¥, ğ‘¢
)
ğ‘‘ â€²ğ‘¥

(
+

+

=

ğ‘¥ â€²

ğ‘¢â€²

1

,

(8)

ğ‘¥
ğ‘¢
1ï£¹
ï£®
ï£º
ï£¯
ï£º
ï£¯
ï£º
ï£¯
ï£º
ï£¯
ï£»
ï£°

+

ğ‘Ÿ

ğ‘¢â€²ğ¸ğ‘¢

+
ğ»ğ¾
ğ»ğ¾
(
ğ»ğ¾ ,ğ‘˜

(

ğ‘¥ ğ‘¥

ğ‘¢ ğ‘¥

)

ğ›¾Eğ‘¤
ğ‘‰ğ¾ ,ğ‘˜
[
ğ»ğ¾
ğ»ğ¾
(
ğ»ğ¾ ,ğ‘˜

ğ‘¢ğ‘¢

ğ‘¥ğ‘¢

)

(

(

ğ‘”
(
ğ»ğ¾ ,ğ‘˜
ğ»ğ¾ ,ğ‘˜
ğ»ğ¾ ,ğ‘˜

(

ğ‘¥1
)

ğ‘¢1
)

ğ‘¥, ğ‘¢, ğ‘¤

))]

(

)

(

)

(cid:2)

(cid:3)

=

ğ‘¥ â€²

)
1ğ‘¢

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ğ»ğ¾ ,ğ‘˜
ï£°

)
1ğ‘¥
(
ğ‘¥
ğ‘¢
1ï£¹
ï£®
ï£º
ï£¯
ï£º
ï£¯
where ğ»ğ¾ ,ğ‘˜ is a symmetric positive deï¬nite matrix.
ï£º
ï£¯
ï£º
ï£¯
ï£»
ï£°

ï£¹
ï£º
ï£º
ï£º
ï£º
ï£»

11
)

ğ‘¢â€²

1

(cid:2)

(cid:3)

,

(

Given the policy ğ¾, ğ‘˜, we can compute the ğ‘„-function
characterized by ğ»ğ¾ ,ğ‘˜. We can ï¬nd an improved

,

Â·)

(Â·

ğ‘„ğ¾ ,ğ‘˜
policy Ëœğ¾ and Ëœğ‘˜ based on ğ‘„ ğ‘˜,ğ‘˜
Ëœğ‘˜ = min
ğ‘¢

Ëœğ¾ğ‘¥

)
+
Solving the minimization problem yields:

(

,

, by solving

(Â·
Â·)
ğ‘„ğ¾ ,ğ‘˜

ğ‘¥, ğ‘¢

.

Ëœğ¾ =
Ëœğ‘˜ =

1
ğ»âˆ’
ğ¾
(
1
ğ»âˆ’
ğ¾

(

âˆ’

âˆ’

ğ‘¢ğ‘¢

ğ‘¢ğ‘¢

)

)

ğ‘¢ ğ‘¥

,

)

ğ»ğ¾

(
ğ»ğ¾ ,ğ‘˜

.

ğ‘¢1
)

(

If the policy characterized by ğ¾ and ğ‘˜ is stabilizing, the
feedback policy characterized by Ëœğ¾ and Ëœğ‘˜ is per deï¬nite a
stabilizing policy â€” it has no higher cost than ğ¾ and ğ‘˜ [1],
[31]. A new ğ‘„-function then be assigned to this improved
policy and the policy improvement procedure can be repeated
ad inï¬nitum. Algorithms following this policy improvement
is called policy iteration.

III. LQG WITH MANIPULATED COST PARAMETERS

)

}

, ğ‘¡

D

1
+

data

rely
, ğ‘‡

algorithms
= 1, 2,

RL learning
ğ‘¥ğ‘¡ , ğ‘¢ğ‘¡ , ğ‘ğ‘¡ , ğ‘¥ğ‘¡

=
on
to solve the LQG
{(
Â· Â· Â·
problem and ï¬nd a good control policy with certain
performance guarantee. For some model-based learning
algorithms, the agent ï¬rst estimates the system parameters
ğ·, ğ¸, ğ‘‘
ğ´, ğµ
and then computes the â€˜optimalâ€™
(
Ë†ğ·, Ë†ğ¸, Ë†ğ‘‘
policy based on the estimates
:
)
)
Ë†ğ¾, Ë†ğ‘˜
. For RL learning algorithms based
(
on value iteration or policy iteration which will not conduct
system identiï¬cation, the attacker can rely on the mapping
DLQG to craft its attacking strategy and understand the
limitations of the attack.

)
Ë†ğ·, Ë†ğ¸, Ë†ğ‘‘
)

= DLQG

Ë†ğ´, Ë†ğµ

and

and

(

(

)

(

)

(

Hence, to understand the security threats of RL-based
LQG problems, it is essential to investigate some fundamen-
tal properties regarding the mapping DLQG:

(

, does there exist a trio

1) To trick the agent into the learning a â€˜nefariousâ€™ control
ğ·â€ , ğ¸ â€ , ğ‘‘â€ )
ğ´, ğµ
)
(
=
ğ¾â€ , ğ‘˜ â€ )

ğ¾â€ , ğ‘˜ â€ )
policy
ğ¾â€ , ğ‘˜ â€ )
such that
(
and cost parameters
ğ·â€ , ğ¸ â€ , ğ‘‘â€ )
?
DLQG

2) How much falsiï¬cation the attacker needs to make on
to trick the agent into learning

is optimal under system

ğ·â€ , ğ¸ â€ , ğ‘‘â€ )

ğ·, ğ¸, ğ‘‘

i.e.,

?

(

(

(

(

,

(

)

3) Will a small change in

ğ·, ğ¸, ğ‘‘

(
ğ¾, ğ‘˜
ğ¾, ğ‘˜
change in
)
DLQG Lipschitz continuous?

for

)

(

(

(

ğ¾â€ , ğ‘˜ â€ )
cause a signiï¬cant
ğ·, ğ¸, ğ‘‘
? Is

)

= DLQG

(

)

(

Sğ‘š
++

, ğ¸ â€  âˆˆ

ğ¾â€ , ğ‘˜ â€ )

Since the goal of the attacker is to mislead the agent into
, we assume that ğ·â€  âˆˆ
learning a stabilizing policy
Sğ‘›
which result into a stabilizing policy. In the
++
following subsection, we answer these questions. We show
that the mapping DLQG is locally Lipchitz continuous and
derives an upper bound regarding how much falsiï¬cation
on
is needed to deceive the agent into learning
(
ğ¾â€ , ğ‘˜ â€ )
(
A. Fundamental Limits

ğ¾âˆ—, ğ‘˜ âˆ—)

instead of

ğ·, ğ¸, ğ‘‘

(

)

.

)

Note that ğ¾ is uniquely determined by ğ· and ğ¸ given
and solving for ğ¾ involves solving the Riccati equa-
ğ´, ğµ
(
tion (5). The following theorem presents a perturbation anal-
ysis on the Riccati equation (5) to see how small falsiï¬cation
ğ·, ğ¸
on
induces changes in the solution of the Riccati
equation ğ‘ƒâˆ—.

)

(

(

+

ğ´âˆ—ğ‘)

be the spectral radius of ğ´âˆ—ğ‘

â‰”
Proposition 1. Let ğœŒ
ğµğ¾âˆ— under the true cost parameters. Let ğ·âˆ— and ğ¸ âˆ— be
ğ´
the true cost parameters and ğ·â€  and ğ¸ â€  are the falsiï¬ed cost
ğœ–.
parameters. Suppose
Denote by ğ‘ƒâˆ— (respectively, ğ‘ƒâ€ ) the solution to the Riccati
equation (5) under the true cost parameters (respectively, the
falsiï¬ed parameters). Then, we have

ğ·âˆ— k â‰¤

ğ¸ âˆ— k â‰¤

ğ·â€  âˆ’

ğ¸ â€  âˆ’

ğœ– and

k

k

ğ‘ƒâ€ 

k
as long as

ğ‘ƒâˆ—

âˆ’

k â‰¤

Î“1

k

ğ·â€ 

âˆ’

ğ·âˆ—

k +

Î“2

ğ¸ â€ 

k

âˆ’

ğ¸ âˆ—

k

1

ğ¸ âˆ—âˆ’
ğ›¾ğœŒ

ğ‘ƒâˆ—

ğœ
k)
(
ğ´âˆ—ğ‘)
(
1
)
k +

âˆ’ k
1
âˆ’
2

âˆ’

k

(k

ğ´âˆ—ğ‘)

2

2

Â·

2

ğ¸ âˆ—âˆ’

2

âˆ’

k

min

1
ğ›¾2ğœ
(
2

ğµ

{
2

k

k

k

2

ğ›¾ğœŒ
âˆ’
ğ´âˆ—ğ‘) k

ğ´âˆ—ğ‘)
(
ğ´âˆ—ğ‘ kk

, 1

}

,

ğ‘†âˆ— k

1

4ğ›¾2 (

ğœ–

â‰¤

ğ´

Â·
where

k

(cid:16)

and

Applying binomial inverse theorem, we can write ğ¹
as

(

ğ‘‹, ğ·, ğ¸

)

ğ¹

ğ‘‹, ğ·, ğ¸

= ğ‘‹

ğ›¾ ğ´â€²ğ‘‹

ğ¼

(

)

âˆ’

+

ğ›¾ğµğ¸ âˆ’

1ğµâ€² ğ‘‹

1

âˆ’

ğ´

ğ·.

âˆ’

(

(cid:17)

. Hence, ğ¹

(cid:16)
Let ğ‘ƒâˆ— be the solution to the Riccati equation (5) under the
and ğ‘ƒâ€  be the solution to the
ğ·âˆ—, ğ¸ âˆ—)
true cost parameters
(
same Riccati equation under the falsiï¬ed cost parameters
= ğ¹
ğ·â€ , ğ¸ â€ )
ğ‘ƒâˆ—, ğ·âˆ—, ğ¸ âˆ—)
(
1ğµâ€², and
For simplicity, deï¬ne ğ‘†âˆ— â‰” ğµğ¸ âˆ—âˆ’
= ğ´
ğ´âˆ—ğ‘
ğµğ¾âˆ—. Here, ğ¾âˆ— is the optimal feedback control gain
ğ·âˆ—, ğ¸ âˆ—)
under the true cost parameters
. By inspection, for
(
ğ›¾ğ‘†âˆ—(
ğ‘‹
ğ‘ƒâˆ— +
any ğ‘‹ such that ğ¼
is invertible, we can write
)
= ğ¹
ğ‘‹, ğ·âˆ—, ğ¸ âˆ—
ğ‘ƒâˆ—
+
(
= ğ¹ğ‘‹
,
ğ‘‹
ğ‘‹

ğ‘ƒâ€ , ğ·â€ , ğ¸ â€ )
(
1ğµâ€², ğ‘†â€  = ğµğ¸ â€ âˆ’

+
ğ‘‹, ğ·âˆ—, ğ¸ âˆ—

ğ‘ƒâˆ—, ğ·âˆ—, ğ¸ âˆ—

= 0.

) âˆ’

ğ‘ƒâˆ—

ğ¹

ğ¹

+

+

)

)

(

(

(

) + H (

)

(9)

where

and

ğ¹ğ‘‹

ğ‘‹

)

(

= ğ‘‹

âˆ’

ğ›¾ ğ´âˆ—ğ‘ â€²ğ‘‹ ğ´âˆ—ğ‘,

= ğ›¾2 ğ´âˆ—ğ‘ ğ‘‹

)

ğ¼

ğ‘‹

H (

ğ›¾ğ‘†âˆ—
(
Denote by Î”ğ‘ƒ the difference between ğ‘ƒâˆ— and ğ‘ƒâ€ , i.e., Î”ğ‘ƒ â‰”
ğ‘ƒâ€  âˆ’
, according to (9), the
equation

ğ·â€ , ğ¸ â€ )

ğ·âˆ—, ğ¸ âˆ—)

ğ‘ƒâˆ—. Given

(10)

and

ğ‘ƒâˆ—

)]

ğ‘‹

+

+

âˆ’

[

(

(

1ğ‘†âˆ— ğ‘‹ ğ´âˆ—ğ‘.

= ğ¹ğ‘‹

ğ‘‹

ğ‘‹

(11)

ğ¹

ğ‘ƒâˆ—

ğ‘‹, ğ·âˆ—, ğ¸ âˆ—

ğ¹

ğ‘ƒâˆ—

ğ‘‹, ğ·â€ , ğ¸ â€ 

(

)

(

(

+

)âˆ’

+
)+H (
admits a unique symmetric solution ğ‘‹ such that ğ‘ƒâˆ— +
ğ‘ƒâˆ— +
which also solves ğ¹
is ğ‘‹ = Î”ğ‘ƒ. The eigenvalues of the operator ğ¹ğ‘‹ : Rğ‘›
Ã—
â†’
Rğ‘›
ğœ†ğ‘–ğœ† ğ‘— , where the eigenvalues ğœ†ğ‘– of ğ´âˆ—ğ‘
Ã—
lies inside the unit circle in the complex plane. Hence 0 <
< 2, the operator ğ¹ğ‘‹ is invertible. In view of (9) and
ğœ‡ğ‘– ğ‘—
|
(11), we construct an operator

0,
= 0. Hence, the solution

ğ‘› are ğœ‡ğ‘– ğ‘— = 1

ğ‘‹, ğ·â€ , ğ¸ â€ )

)
ğ‘‹

â‰¥

âˆ’

(

ğ‘›

|

(

)

â‰”

ğ‘

ğ‘

ğ¹

ğ¹

Î¦

âˆ’

)+

ğ‘ƒâˆ—

1
ğ¹âˆ’
ğ‘‹ (H (

ğ‘, ğ·â€ , ğ¸ â€ 

.
))
+
(
(12)
Next, we show that under certain conditions on Î”ğ· â‰” ğ·â€  âˆ’
ğ·âˆ— and Î”ğ¸ â‰” ğ¸ â€  âˆ’
ğ¸ âˆ—, there exists ğœŒ = ğ‘“
for
R such that Î¦ is constractive and maps
some ğ‘“ : R
â†’
the set

ğ‘, ğ·âˆ—, ğ¸ âˆ—

Î”ğ·

Î”ğ¸

ğ‘ƒâˆ—

)âˆ’

(k

k)

R

Ã—

+

k

k

(

,

Î©ğœŒ =

ğ‘ :

{

ğ‘

k

k â‰¤

ğœŒ, ğ‘ = ğ‘ â€², ğ‘ƒ

ğ‘

0

}

â‰¥

+

into itself. In view of (12), we obtain

Î“1 = 4ğ›¾2

1

2

ğœ
ğ´âˆ—ğ‘)
(
ğ›¾ğœŒ
(

âˆ’

ğ´âˆ—ğ‘)

(cid:17)

2

Î¦

ğ‘

(

k

1
ğ¹âˆ’
ğ‘‹ k

) k â‰¤ k

(cid:16)

kH (
ğ¹

âˆ’

(

ğ‘

ğ¹

ğ‘ƒâˆ—

) k + k
ğ‘ƒâˆ—

+
(
ğ‘, ğ·âˆ—, ğ¸ âˆ—

+

ğ‘, ğ·â€ , ğ¸ â€ 

)

(13)

.

) k

(cid:17)

,

2

(14)

From Lemma 3, we know

1
ğ¹âˆ’
ğ‘‹ k â‰¤

k

1

2

ğœ
ğ´âˆ—ğ‘)
(
ğ›¾ğœŒ
(

âˆ’

ğ´âˆ—ğ‘)

is the spectral radius of ğ´ğ‘ and ğœ

where ğœŒ
ğ´ğ‘
as ğœ

ğ´ğ‘
)
â‰” supğ‘˜
ğ´ğ‘˜
ğ‘ k/
By Lemma 2 and (10),

{k

(
)

N

(

âˆˆ

ğ´ğ‘

ğœŒ

(

)

ğ‘˜

.

}

ğ´ğ‘

)

(

is deï¬ned

ğ´

2

k

(k

ğ‘ƒâˆ—

k +

1

2 k

)

2 k

ğ·, ğ¸

)

, let ğ¹

(

1

ğ¸ âˆ—âˆ’
1

âˆ’ k
ğ‘‹, ğ·, ğ¸

)

2

2

.

ğµ
k
k
1
ğ¸ âˆ—âˆ’

k
k
be the ma-

ğ›¾2 ğ´â€²ğ‘‹ ğµ

ğ›¾ğµâ€²ğ‘‹ âˆ—ğµ

ğ¸

(

+

)

1ğµâ€²ğ‘‹ ğ´

âˆ’

ğ·

âˆ’

ğ‘

kH (

) k â‰¤

ğ›¾2

2

ğ´âˆ—ğ‘ k

k

k

ğ‘†âˆ—

ğ‘

k

kk

2.

(15)

4

2

ğœ
ğ´âˆ—ğ‘)
(
ğ›¾ğœŒ
(

Î“2 = 4ğ›¾2

1

ğ´âˆ—ğ‘)
Proof. Given parameters
trix expression

âˆ’

ğ‘‹, ğ·, ğ¸

ğ¹

(

= ğ‘‹

âˆ’

)

ğ›¾ ğ´â€²ğ‘‹ ğ´

(

+

A straight calculation yields

ğ‘, ğ·â€ , ğ¸ â€ 

ğ¹

(

âˆ’

=

ğ‘ƒâˆ—

+
ğ›¾ ğ´â€²

(

ğ‘ƒâ€ 

ğ‘

+

ğ¹

ğ‘ƒâˆ—

) âˆ’
ğ¼

(
ğ›¾ğ‘†â€ 

+
ğ‘ƒâ€ 

+

(

)

)

(cid:16)
ğ¼

(cid:16)
ğ¼

+

+
=ğ›¾

ğ›¾ ğ´â€²

ğ‘ƒâ€ 

ğ‘

(
ğ‘ƒâ€ 

+
ğ‘

+

) (

ğ´â€²
(
Î”ğ·.
(cid:2)
âˆ’

Then, by Lemma 2,

)
ğ´

ğ‘, ğ·âˆ—, ğ¸ âˆ—

ğ‘

)

+

1

âˆ’

1

(cid:17)
âˆ’

ğ›¾ğ‘†âˆ—

ğ‘ƒâ€ 

+
ğ›¾ğ‘†âˆ—

(
ğ‘ƒâ€ 

ğ‘

+
ğ‘

)
(cid:17)
ğ›¾Î”ğ‘†

(

+

))

ğ´

ğ¼

(

âˆ’

+

Î”ğ·

ğ›¾ğ‘†â€ 

ğ‘ƒâˆ—

(

+

ğ‘

))

1 ğ´

âˆ’

k

k

â‰¤

k
Assume ğœŒ < 1. Since ğ‘
Note that Î”ğ‘† = ğ‘†â€  âˆ’
4, we derive that if

ğ¹
k
ğ›¾2

ğ‘ƒâ€ , ğ·â€ , ğ¸ â€ 
) âˆ’
(
2
ğ‘
ğ‘ƒâˆ—
ğ´

2

) k
.

k

ğ¹
(
Î”ğ‘†

ğ‘ƒâ€ , ğ·âˆ—, ğ¸ âˆ—
Î”ğ·
k
ğ‘ƒâˆ—+
k
ğ¸ âˆ—}âˆ’

+
k + k
k
Î©ğœŒ, we obtain
1ğµâ€² âˆ’

ğµ

{

< 1,

Î”ğ¸

.

k

k

ğ¸ âˆ—âˆ’
1

2

ğµ
k
k
1
ğ¸ âˆ—âˆ’

âˆ’ k
13

(

) âˆ’ (

2

k
k
16
)

âˆˆ
ğ‘†âˆ— = ğµğ¸ â€ âˆ’
1Î”ğ¸
ğ¸ âˆ—âˆ’

k

k
1

Î”ğ‘†

k

k â‰¤

k

Combining the results from

, we have, for ğ‘

Î©ğœŒ

âˆˆ

ğ‘
ğ‘ƒâˆ— k+
1.
1ğµâ€². By Lemma

k â‰¤ k

(cid:3)

(16)

Î¦

ğ‘

(

k

) k â‰¤

1

2

ğœ
ğ´âˆ—ğ‘)
(
ğ›¾ğœŒ
(

âˆ’

ğ´âˆ—ğ‘)

ğ›¾2

2

ğ´âˆ—ğ‘ k

k

k

ğ‘†âˆ—

k

ğœŒ2

2

"

ğ›¾2

ğ´

k

+

2

k

(k

ğ‘ƒâˆ—

k +

2 k

1

)

1

ğ¸ âˆ—âˆ’
1

âˆ’ k

2

ğµ
k
k
1
ğ¸ âˆ—âˆ’

2

k
k

Î”ğ¸

k

k + k

Î¦

ğ‘1

(

k

) âˆ’

Î¦

ğ‘2

(

) k

Î”ğ·

k#
(17)
for

Similarly, we derive a bound for
ğ‘1, ğ‘2

Î©ğœŒ:

âˆˆ

2

ğœ
ğ´âˆ—ğ‘)
(
ğ›¾ğœŒ
(
ğ´âˆ—ğ‘ k

Î¦

ğ‘1

Î¦

ğ‘2

2ğ›¾2

2

2

ğ´

ğ‘ƒâˆ—

2

1

Î”ğ‘†

(

(

k

k

k

1

âˆ’

(k

) âˆ’

k +

) k â‰¤

ğ´âˆ—ğ‘)
ğ‘†âˆ—
k
(18)
Due to (17) and (18), the operator Î¦ is a contraction and
maps the compact set Î©ğœŒ into itself if there exists ğœŒ > 0
such that

+ (k

ğ‘1

ğ‘2

k)

âˆ’

ğœŒ

k

k

k

i

h

)

2

k

ğœŒ

â‰¥

1

2

ğœ
ğ´âˆ—ğ‘)
(
ğ›¾ğœŒ
(

âˆ’

ğ´âˆ—ğ‘)

ğ›¾2

2

ğ´âˆ—ğ‘ k

k

k

ğ‘†âˆ—

k

ğœŒ2

2

"

ğ›¾2

ğ´

k

+

2

k

(k

ğ‘ƒâˆ—

k +

2 k

1

)

1

ğ¸ âˆ—âˆ’
1

âˆ’ k

2

ğµ
k
k
1
ğ¸ âˆ—âˆ’

2

k
k

Î”ğ¸

k

k + k

Î”ğ·

,

k#

and

1 >

1

+ k
Choose

2

ğœ
ğ´âˆ—ğ‘)
(
ğ›¾ğœŒ
(
2
ğ´âˆ—ğ‘ k

2

ğ´âˆ—ğ‘)
ğ‘†âˆ—
k

âˆ’

2ğ›¾2

ğœŒ.

k

i

ğ´

2

k

(k

ğ‘ƒâˆ—

k +

2 k

1

)

k

h

1

ğ¸ âˆ—âˆ’
1

âˆ’ k

2

ğœ
ğ´âˆ—ğ‘)
(
ğ›¾ğœŒ
(

ğ´âˆ—ğ‘)

ğ´

2

k

(k

ğ‘ƒâˆ—

k +

2 k

1

)

2

k

(cid:16)

1

ğ¸ âˆ—âˆ’
1

âˆ’ k

2

ğµ
k
k
1
ğ¸ âˆ—âˆ’

2

k
k

Î”ğ¸

k

k

2

ğµ
k
k
1
ğ¸ âˆ—âˆ’

2

k
k

Î”ğ¸

k

k

min

, the operator Î¦ is a contrac-
, 1
If ğœŒ
}
tion and maps the compact set Î©ğœŒ into itself. Then, Î¦ admits

ğ‘†âˆ— k

(
) k

)
k k

â‰¤

{

(

ğ›¾ğœŒ

1
âˆ’
ğ´âˆ—ğ‘

ğ´âˆ—ğ‘
ğ´âˆ—ğ‘

2

ğœŒ =4ğ›¾2

+ k

1
âˆ’
Î”ğ·

.

k

(cid:17)
ğ›¾2 ğœ

a unique ï¬xed-point solution in Î©ğœŒ. Therefore, Î”ğ‘ƒ
Î”ğ‘ƒ

Î©ğœŒ and

âˆˆ

k

k â‰¤

ğ‘ƒâ€ 

k

âˆ’

ğ‘ƒâˆ—

ğœŒ. Hence, we have
ğœ
ğ´âˆ—ğ‘)
(
ğ´âˆ—ğ‘)
ğ›¾ğœŒ
(
ğ‘ƒâˆ—

4ğ›¾2

k â‰¤

âˆ’
2

ğ´

1

2

+ k

k

(k

k +

Î”ğ·

2

k

(cid:16)
1

)

2 k

1

k
ğ¸ âˆ—âˆ’
1

âˆ’ k

2

ğµ
k
k
1
ğ¸ âˆ—âˆ’

2

k
k

Î”ğ¸

k

.

k

(cid:17)

(cid:3)

k

ğ·âˆ— k â‰¤
ğ·â€  âˆ’
ğ¾âˆ—
ğ¾â€ 

âˆ’

R
+
ğ¸ âˆ—)/

+ â†’
ğœ†min
(
Î“4ğœ–,

â‰¤
ğœ–

(
ğµ

) +
2

Î“2

Proposition 1 presents some preliminary results showing
the change in the solution of Riccati equation is bounded,
Î“1
ğ‘ƒâˆ— âˆ’
ğ¸ â€  âˆ’
ğ‘ƒâˆ— k â‰¤
ğ·â€  âˆ’
ğ¸ âˆ—k
ğ·âˆ—k +
i.e.,
for small
k
k
ğ¸ âˆ— k
ğ¸ â€  âˆ’
ğ·âˆ—k
ğ·â€  âˆ’
. Our ultimate goal is to see how
and
k
k
ğ·, ğ¸
leads to changes in ğ¾.
small fasiï¬cation on
(
Theorem 1. Suppose
ğ‘ƒâˆ— k â‰¤
ğ‘ƒâ€  âˆ’
ğ¸ âˆ— k â‰¤
ğ‘“
, where ğ‘ƒâˆ—(respectively, ğ‘ƒâ€ ) is the solution to the Riccati
equation (5) under the true cost parameters (respectively, the
falsiï¬ed cost parameters.), and ğ‘“ : R
is some function
on ğœ– given
2, then

)
ğ¸ â€  âˆ’

ğœ– and

ğœ–. If ğœ–

k

k

k

ğœ–

)

(

Î“3 ğ‘“

k â‰¤
ğ´

{k

(19)

and Î“4 â‰”

k
where Î“3 â‰” 2ğ›¾
ğœ†min (
ğ¾âˆ— k
.

max

ğ¸ âˆ—)

,

k

k

k}

(k

ğ¾âˆ— k +

1

)

ğ¸ âˆ—) k

2ğ›¾
ğœ†min (
Proof. Note that the optimal ğ‘„-function of the LQG problem
takes the form
= 1
2

ğ‘¢â€²ğ¸ğ‘¢

ğ‘¥, ğ‘¢

ğ‘‘ â€²ğ‘¥

ğ‘„

ğ‘Ÿ

+

+

)

(

ğ‘¥ â€²ğ·ğ‘¥
ğ›¾Eğ‘¤

+

ğ´ğ‘¥

[(

+

1
2
ğ¶ğ‘¤

+

+
ğµğ‘¢

ğ´ğ‘¥

ğ‘ƒ

(

)

+

ğµğ‘¢

ğ¶ğ‘¤

,

)]

+

where ğ‘ƒ the solution of the Riccati equation (5) given ğ´,
ğµ, ğ·, ğ¸. We know that the arg minğ‘¢ ğ‘„
takes the form
ğ‘˜ for every ğ‘¥. Indeed, ğ¾ğ‘¥ is also the optimal point
ğ¾ğ‘¥
2 ğ‘¢â€²ğ¸ğ‘¢
of the function ğ‘“
.
)
Applying Lemma 7 and letting ğ¸1 = ğ¸ âˆ—, ğ‘ƒ1 = ğ‘ƒâˆ—, ğ¸2 = ğ¸ â€ ,
(cid:3)
and ğ‘ƒ2 = ğ‘ƒâ€ , we have (19).

= 1

ğ‘¥, ğ‘¢

ğ‘¥, ğ‘¢

ğµğ‘¢

ğµğ‘¢

ğ´ğ‘¥

ğ´ğ‘¥

ğ‘ƒ

+

+

+

+

ğ›¾

(

(

)

)

)

(

(

From Proposition 1 and Theorem 1, we know that the
change of the policy ğ¾ is bounded by the falsiï¬cation in
ğ·, ğ¸

. Indeed,

(

)
ğ¾â€ 

k

âˆ’
for small
bound on

ğ¾âˆ—

k â‰¤
ğ·â€  âˆ’
k
ğ‘˜ â€  âˆ’
k

ğ·â€ 

k
and

Î“3Î“1

ğ·âˆ— k
ğ‘˜ âˆ— k
.

ğ·âˆ—

âˆ’
ğ¸ â€  âˆ’

k + (
ğ¸ âˆ— k

k

Î“3Î“2

Î“4

+

ğ¸ â€ 

ğ¸ âˆ—

,

k

âˆ’

) k

. Now, letâ€™s discuss the

Proposition 2. Let â„âˆ— (respectively, â„â€ ) be the solution of
(respec-
(4)-(6) under the true cost parameters
tively, the falsiï¬ed cost parameters
). Suppose
ğ¾â€  âˆ’
ğµ
kâˆ’

ğ·âˆ—, ğ¸ âˆ—, ğ‘‘âˆ—)
(
ğ·â€ , ğ¸ â€ , ğ‘‘â€ )

(
. We have

k

ğ¼

1

ğ¾âˆ— k â‰¤
â„â€ 

1
2 k
â„âˆ—

ğ›¾ ğ´âˆ—ğ‘ k
ğ‘‘âˆ—
ğ‘‘â€ 

k
âˆ’
Î“5

Î“6

ğ¾â€ 

ğ¾âˆ—

,

k +

k

âˆ’
and Î“6 = 2ğ›¾

k

ğ¼

k(

âˆ’

k

âˆ’

k â‰¤
ğ¼

k
where Î“5 = 2
ğ›¾ ğ´âˆ—ğ‘)âˆ’
.
k
Proof. From (6), we know that

âˆ’
ğ›¾ ğ´âˆ—ğ‘)âˆ’

ğ‘‘âˆ— kk

kk

k(

âˆ’

ğµ

1

1

k

â„âˆ— = ğ‘‘âˆ—

ğ´

ğ›¾

(

+

+

ğµğ¾âˆ—

)

â€²â„âˆ— and â„â€  = ğ‘‘â€ 

ğ´

ğ›¾

(

+

+

ğµğ¾â€ 

â€²â„â€ .

)

5

= ğ´
Let ğ´âˆ—ğ‘
know that ğ¼
have

+
âˆ’

ğµğ¾âˆ— and ğ´â€ ğ‘ = ğ´
ğ›¾ ğ´âˆ—ğ‘ and ğ¼

ğµğ¾â€ . From Lemma 5, we
ğ›¾ ğ´â€ ğ‘ are invertible. Hence, we

+

âˆ’

â„âˆ— =

ğ›¾ ğ´âˆ—ğ‘)
âˆ’
Applying Lemma 8 proves the Theorem.

1ğ‘‘âˆ— and â„â€  =

âˆ’

âˆ’

(

ğ¼

(

ğ¼

1ğ‘‘â€ 

âˆ’

ğ›¾ ğ´â€ ğ‘)

(cid:3)

(

(

ğœ†min

). Suppose

ğ·âˆ—, ğ¸ âˆ—, ğ‘‘âˆ—)

(
ğ·â€ , ğ¸ â€ , ğ‘‘â€ )

Theorem 2. Let ğ‘˜ âˆ— (respectively, ğ‘˜ â€ ) be the solution of (4)-
(respectively,
(6) under the true cost parameters
ğ¸ â€  âˆ’
the falsiï¬ed cost parameters
ğ¸ âˆ—)/
ğ¸ âˆ— k â‰¤
2, we have
Î“7
ğ‘˜ â€ 
ğ‘˜ âˆ—
k â‰¤
âˆ’
k
where Î“7 =
4
ğœ†min (
ğµ

Î“8
ğ¸ âˆ—
âˆ’
k +
, Î“8 =
ğ‘˜ âˆ— k

â„âˆ—
â„â€ 
k
âˆ’
2, and Î“9 =

4ğ›¾
ğœ†min (
Proof. Note that From (4), we know that ğ‘˜ =
ğ›¾ğµâ€²ğ‘ƒğµ
)âˆ’
problem

+
1ğµâ€²â„ is the solution of the following minimization

ğ‘ƒâˆ—
k +
ğ‘˜ âˆ— kk

ğ‘ƒâ€ 
k
4ğ›¾
ğœ†min (

k
ğ¸ âˆ—) k

âˆ’
ğ¸ âˆ—) k

ğ¸ âˆ—) k

Î“9

ğ¸ â€ 

âˆ’

ğµ

ğ¸

ğ›¾

k

k

k

k

(

.

,

â‰” ğ‘˜ â€²

ğ¸

ğ›¾ğµâ€²ğ‘ƒğµ

ğ‘˜

ğ›¾â„â€²ğµğ‘˜,

)

(
(
ğ›¾ğµâ€²ğ‘ƒğµ is positive deï¬nite. Let

+

+

)

ğ‘“

ğ‘˜; ğ¸, ğ‘ƒ, â„

min
Rğ‘š
ğ‘˜
âˆˆ
where ğ¸
ğ‘“
ğ‘˜
(
)
be the solution of minğ‘˜ ğ‘“ âˆ—(
ğ‘˜
ğ‘“ âˆ—
kâˆ‡

ğ‘˜; ğ¸ âˆ—, ğ‘ƒâˆ—, â„âˆ—)

and ğ‘“ â€  (

) âˆ’ âˆ‡

) k â‰¤

ğ‘“ â€ 

+

2

ğ‘˜

ğ‘˜

k

(

(

= ğ‘“
ğ‘˜

)

k(k
ğµ

ğ›¾

+

k

kk

From Lemma 6, we know that if

)

ğ‘˜

=
ğ‘“ âˆ—(
. Let ğ‘˜ âˆ— and ğ‘˜ â€ 
ğ‘˜; ğ¸ â€ , ğ‘ƒâ€ , â„â€ )
(
ğ‘˜
and minğ‘˜ ğ‘“ â€ (
respectively.
2
ğ¸ âˆ—
ğ›¾
ğ¸ â€ 
k +
k
.
â„âˆ—
âˆ’
k
ğ¸ âˆ— k â‰¤
ğ¸ â€  âˆ’

ğ¸ âˆ—)/
2,

âˆ’
â„â€ 

ğœ†ğ‘šğ‘–ğ‘›

)
ğµ

ğ‘ƒâ€ 

ğ‘ƒâˆ—

k)

âˆ’

k

k

k

(

ğ‘˜ â€ 

k

âˆ’

ğ‘˜ âˆ—

k â‰¤

â‰¤

2
ğœ†min
(
2
ğœ†min

(
2ğ›¾

+

ğ¸ âˆ—)
ğ¸ âˆ—) (cid:16)
ğ‘˜ âˆ—
kk
k

ğ‘“ â€ 

ğ‘˜ âˆ—

(

) k

kâˆ‡

2

ğ‘˜ âˆ—

k

kk

ğ¸ â€ 

âˆ’

ğ¸ âˆ—

k

ğµ

2

k

k

ğ‘ƒâ€ 

âˆ’

ğ‘ƒâˆ—

2ğ›¾

ğµ

k

kk

â„â€ 

âˆ’

â„âˆ—

k

k +

Combining the results from Proposition 1, Theorem 1,

Proposition 2, and Theorem 2, we obtain

k

ğ¾â€ 
ğ‘˜ â€ 

k

ğ¾âˆ—
ğ‘˜ âˆ—

âˆ’

âˆ’

k â‰¤

k â‰¤

ğ¸ âˆ—
ğ¸ âˆ—

,

k

k

âˆ’

âˆ’

Î“3Î“1
Î“1Î“8

k

ğ·â€ 
ğ·â€ 

k

k
Î“5Î“9
+
Î“1Î“8
Î“7
+
Î“5Î“9

+ [

+

âˆ’

k + (

k + (

ğ·âˆ—
ğ·âˆ—
âˆ’
ğ‘‘â€ 
ğ‘‘âˆ—
k +
âˆ’
Î“1Î“3Î“6Î“9
) k
Î“4
Î“2Î“8
,
ğ‘‘â€ 

+ (
ğ‘‘âˆ—

Î“3Î“2
Î“2Î“8
Î“6Î“9
ğ·â€ 

+

Î“4
Î“7
+
ğ¾â€ 
ğ·âˆ—

k

) k

) k

ğ¸ â€ 
ğ¸ â€ 
ğ¾âˆ—

âˆ’

k

âˆ’
Î“2Î“3

k
Î“6Î“9

+

)

â‰¤ (

ğ¸ â€ 

ğ¸ âˆ—

k

âˆ’

] k

,

k

k

, and

ğ‘‘â€  âˆ’

+
âˆ’
k
k
(20)
ğ¸ âˆ— k
ğ·â€  âˆ’
ğ·âˆ—k
ğ‘‘âˆ—k
ğ¸ â€  âˆ’
for small
and for every
k
Rğ‘›. The bound above indicates
Sğ‘š
Sğ‘›
ğ·âˆ—, ğ¸ âˆ—, ğ‘‘âˆ—) âˆˆ
++ Ã—
+ Ã—
(
that the mapping DLQG that solves the discrete-time LQG
problem is locally Lipschitz over the set Sğ‘›
Rğ‘›.
Hence, the continuity of the mapping DLQG holds and a
tiny falsiï¬cation on cost parameters by the attacker will only
cause a bounded change in the computed policy. If the at-
tacker aims to mislead the agent choosing into the â€˜nefariousâ€™
ğ¾âˆ—, ğ‘˜ âˆ—)
policy
,
(20) gives an upper bound on how much falsiï¬cation the
attacker needs on the cost parameters.

from the original optimal policy

ğ¾â€ , ğ‘˜ â€ )

++ Ã—

+ +

Ã—

S

ğ‘š

(

(

B. Attacks on Cost Function Parameters

Suppose the attacker wants to deceive the agent

into
learning the policy characterized by ğ¾â€  and ğ‘˜ â€  by altering
the cost parameters. Results in Section III-A give a rough
bound on the region around the original cost parameters
ğ·, ğ¸, ğ‘‘
ğ·â€ , ğ¸ â€ , ğ‘‘â€ )
that
(
. To minimize the
leads to the â€˜nefariousâ€™ policy
cost of attacking, the attacker attempts to deviate the cost
parameters from the original ones as small as possible:

in which there might exist some
ğ¾â€ , ğ‘˜ â€ )

(

(

)

Ëœğ·

ğ·

ğ¹

Ëœğ‘‘

ğ‘‘

2

Ëœğ¸

min
Ëœğ·, Ëœğ‘‘, Ëœğ¸ , ğ‘ƒ,â„

âˆ’
k
ğ‘ .ğ‘¡. ğ‘ƒ = Ëœğ·

âˆ’

k
+ k
ğ›¾ ğ´â€²ğ‘ƒğ´

ğ¸

ğ¹ ,

k

âˆ’
ğ›¾ğµâ€²ğ‘ƒğµ

+
ğ›¾ğµâ€²ğ‘ƒğµ
ğ´
ğ›¾

)

+
+
(
ğ›¾ğµâ€²ğ‘ƒğµ
Ëœğ·

âˆ’
ğ¾â€  =
ğµğ¾â€ 
ğ‘˜ â€  =
)
Ëœğ¸
0,

Ëœğ¸
(
+
â„ = Ëœğ‘‘
Ëœğ¸
2
(
ğ‘ƒ

+
0,

(cid:23)

â‰»

(

k
ğ¾â€  â€²

+ k
Ëœğ¸
+
ğ›¾ğµâ€²ğ‘ƒğ´,
âˆ’
â€²â„,
ğ›¾ğµâ€²â„,
0.

âˆ’

)

â‰»

ğ¾â€ ,

)

(21)

Proposition 3. The optimization deï¬ned in (21) is convex.

(

ğ·1, ğ‘‘1, ğ¸1, ğ‘ƒ1, â„1

Proof. It easy to see that the objective function of (21) is
ğ·2, ğ‘‘2, ğ¸2, ğ‘ƒ2, â„2
convex. Suppose
)
satisfy the constraints in (21). We need to show that any
also satisfy the constraints in
1,
0
â‰¤
â‰¤
Ëœğ‘‘ = ğ›½ğ‘‘1
(21), where Ëœğ· = ğ›½ğ·1
ğ‘‘2,
1
+ (
ğ‘ƒ2, and Ëœâ„ = ğ›½â„1. We
Ëœğ¸ = ğ›½ğ¸1
ğ›½
1
know that

Ëœğ·, Ëœğ‘‘, Ëœğ¸, Ëœğ‘ƒ, Ëœâ„
)
1

âˆ’
+ (
ğ¸2, Ëœğ‘ƒ = ğ›½ğ‘ƒ1

ğ·2,
1

)
+ (

and

+ (

âˆ’

âˆ’

âˆ’

ğ›½

ğ›½

ğ›½

ğ›½

)

)

)

(

(

)

ğ‘ƒ1 = ğ·1
ğ‘ƒ2 = ğ·2

ğ›¾ ğ´â€²ğ‘ƒ1 ğ´
ğ›¾ ğ´â€²ğ‘ƒ2 ğ´

ğ¾â€  â€²

ğ¾â€  â€²

ğ¸1
ğ¸2

(

(

âˆ’

âˆ’

+

+

ğ›¾ğµâ€²ğ‘ƒ1ğµ
ğ›¾ğµâ€²ğ‘ƒ2ğµ

ğ¾â€ ,

ğ¾â€ .

)

)

+

+

Multiplying both sides of the ï¬rst equality by ğ›½ and both
sides of the second equality by

yields

ğ›½

1

(

âˆ’

)

.

(cid:3)

(cid:17)

ğ›½ğ‘ƒ1
=ğ›½ğ·1

+ (
+ (

1
1

âˆ’
âˆ’
ğ›½ğ¸1

ğ›½
)
ğ›½
)

ğ‘ƒ2
ğ·2
1

ğ¾â€  â€²

âˆ’

)
This equality shows that

+ (

âˆ’

[

ğ›¾ ğ´â€²
[
ğ¸2

+
ğ›½

ğ›½ğ‘ƒ1

1

+ (

âˆ’
ğ›½ğ‘ƒ1

ğ›¾ğµâ€²

+

[

ğ›½

)

+ (

ğ‘ƒ2
1

]

âˆ’

ğ´

ğ›½

)

ğ‘ƒ2

ğµ

]

]

ğ¾â€ .

Ëœğ‘ƒ = Ëœğ·

ğ›¾ ğ´â€² Ëœğ‘ƒğ´

ğ¾â€  â€²

Ëœğ¸

(

+

âˆ’

ğ›¾ğµâ€² Ëœğ‘ƒğµ

ğ¾â€ .

)

+

Similarly, we can show the rest of the constraints also form
(cid:3)
a convex set.

(

(

(

ğ´, ğµ

such that the constraints in

Since the objective function of (21) is strictly convex, there
always exist a unique solution to the optimization problem
ğ¾â€ , ğ‘˜ â€ )
(21) if the constraints in (21) are feasible for a given
.
The feasibility of problem (21) is not always guaranteed. An
, under what conditions
interesting question is given
ğ¾â€ , ğ‘˜ â€ )
are feasible.
21
on
(
)
ğ¾â€ , ğ‘˜ â€ )
whether
This question is equivalent to asking given
Ëœğ·, Ëœğ¸, Ëœğ‘‘
= DLQG
ğ¾â€ , ğ‘˜ â€ )
there exists
.
such that
)
(
If ğ¾â€  is not stabilizing, ğ¾â€  cannot be optimal for any ğ·
0,
0, and ğ‘‘â€ . Note that the goal of the attacker is not
ğ¸
to unstablize the system but to mislead the agent into a
with ğ¾â€  being stabilizing. But even
ğ¾â€ , ğ‘˜ â€ )
nefarious policy
if ğ¾â€  is stabilizing, there might not exist ğ·
0
such that ğ¾â€  is optimal. Hence, our discussion will focus on

Ëœğ·, Ëœğ¸, Ëœğ‘‘
)

0 and ğ¸

â‰»

â‰»

â‰»

(cid:23)

(

(

(

)

(

6

(cid:23)

â‰»

under what conditions, a stabilizing
some ğ·

0, and ğ‘‘.

0, ğ¸

ğ¾â€ , ğ‘˜ â€ )

(

is optimal for

The same question was ï¬rstly raised by Kalman in the ï¬eld
of inverse optimal control [32]. He has showed that for the
single-input case, the circle criterion (see Theorem 6 of [32])
is a necessary and sufï¬cient condition for a control policy to
be optimal in a continuous-time system. Fujii and Narazaki
have given a complete solution for the continuous-time multi-
input case under the assumption that the cost parameters on
the control input ğ¸ is ï¬xed [33]. Sugimoto and Yamamoto
have showed the sufï¬cient and necessary conditions for ğ¾â€ 
to be optimal for some ğ· and ğ¸ under the stage cost ğ‘¥ â€²ğ‘¡ ğ·ğ‘¥ğ‘¡
+
ğ‘¢â€²ğ‘¡ ğ¸ğ‘¢ğ‘¡ for a discrete-time system [34]. We consider a more
general cost function than [34] did. Hence, we extend the
under which
results in [34] to give the conditions on
the constraints in (21) are feasible.

ğ¾â€ , ğ‘˜ â€ )

(

In general, conditions of optimality can be expressed
most conveniently using frequency-domain formulas [32].
For system
, consider the following right coprime
factorization by two polynomial matrices ğ‘€

and ğ‘

ğ´, ğµ

ğ‘§

ğ‘§

(

)

:

âˆšğ›¾

ğ‘§ğ¼

(

ğ´

)

âˆ’

1ğµğ¸ âˆ’

1
/

2 = ğ‘€

âˆ’

ğ‘

ğ‘§

)

(

(

(

)

)
1.

âˆ’

(
ğ‘§

)

The feedback system âˆšğ›¾
factorization

(

ğ´

+

ğµğ¾

)

induces a right coprime

âˆšğ›¾

ğ‘§ğ¼

(
= ğ‘

âˆ’
ğ‘§

1ğµğ¸ âˆ’

âˆ’

ğ´ğ‘

)

1
/

2 = ğ‘€

(
. Deï¬ne

ğ‘§

)
(
â‰” ğ¼ğ‘š

ğ¾ ğ‘€

ğ‘§

) âˆ’

(
ğ¾

ğ‘§ğ¼ğ‘›

(

âˆ’

+

)

(
ğ´

ğ‘ğ‘

ğ‘§

)

ğ‘§

)

(

1,

âˆ’

1ğµğ¸ âˆ’

1
/

2 = ğ‘ğ‘

ğ‘

ğ‘§

)

(

ğ‘§

)

(

1.

âˆ’

âˆ’

)

where ğ‘ğ‘

ğ‘Š

ğ‘§

)

(

Deï¬ne

Î¨

ğ‘§

â‰” ğ‘ â€²ğ‘ (

1

ğ‘§âˆ’

ğ‘Š

0

ğ‘ğ‘

ğ‘§

ğ‘ â€²

1

ğ‘§âˆ’

ğ‘

ğ‘§

.

)

(

(

(

)

(

)

) âˆ’

)
The following theorem states the conditions under which
ğ¾, ğ‘˜

is optimal for some ğ·

(
Theorem 3. Let
be controllable and suppose that ğ´
(
is invertible and ğµ has rank ğ‘š. Let the stabilizing control
ğ‘˜ be given. Transform Î¨
policy ğ‘¢ = ğ¾ğ‘¥

into the form

0, and ğ‘‘.

0, ğ¸

ğ´, ğµ

(cid:23)

â‰»

ğ‘§

)

)

(

)

+

Î¨

ğ‘ˆ

ğ‘§

)

(

ğ‘§

)

(

=

ËœÎ¨

ğ‘§

)

(
ğ‘§

(

)

0

(cid:2)
by some unimodular matrix ğ‘ˆ
is a
rational function matrix with full rank. Then, the constraints
in (21) is feasible under the given control policy
if
and only if the conditions

and, where ËœÎ¨

ğ¾, ğ‘˜

ğ‘§

)

(

)

(

)

(

(cid:3)

is positive deï¬nite,
is positive semi-deï¬nite for all ğ‘§

1) ğ‘Š
2) Î¨
ğ‘§

0
)
(
ğ‘§
(
)
= 1,
|

|

3) and there exists no pair of ğœ†
ğœ†
(
ğ‘§

, ğ‘£ğ‘š
ğ‘£1,
(
ğ‘£ğ‘š = 0, where U+ â‰”

) â€² such that ğ‘
(
ğ‘§
âˆˆ

ğœ†
ğ‘ˆ
)
C :

Â· Â· Â·

)
| â‰¥

{

|

1

,

}

hold for some ğ¸

0.

â‰»

C such that

âˆˆ
U+ and ğ‘£ =
=

âˆˆ

ğ‘£ = 0 and ğ‘£1 =

Â· Â· Â·

Proof. Note that whether ğ¾ is optimal depends only on ğ·,
ğ¸, ğ´, and ğµ. From (4) and (5), we know that ğ¾ is optimal
to ğ·, ğ¸, ğ´, and ğµ if

ğ¾ =

ğ¸

ğ›¾

(

âˆ’

+

ğ›¾ğµâ€²ğ‘ƒâˆ—ğµ

)

1ğµâ€²ğ‘ƒâˆ— ğ´,

âˆ’

7

where ğ‘ƒâˆ— solves the Riccati equation
5
. By Theorem
)
ğ´, ğµ
5.1 of [34], we know that given
controllable and
ğ´ invertible, for a given stabilizing ğ¾, there always exist
ğ·, ğ‘ƒ

0 such that

0 and ğ¸

(
)

(

(cid:23)
ğ‘ƒ = ğ·
ğ¸

â‰»
ğ›¾ ğ´â€²ğ‘ƒğ´
âˆ’
ğ¾ =

+
ğ›¾ğµâ€²ğ‘ƒğµ

(

+

)

âˆ’

ğ›¾2 ğ´â€²ğ‘ƒğµ
ğ›¾ğµâ€²ğ‘ƒğ´

ğ¸

(

+

ğ›¾ğµâ€²ğ‘ƒğµ

)

1ğµâ€²ğ‘ƒğ´,

âˆ’

if and only if conditions 1), 2), and 3) are satisï¬ed. For ğ‘˜,
we have constraints
ğ‘˜ =
â„ =

ğ›¾ğµâ€²ğ‘ƒğµ
1ğ‘‘.

1ğµâ€²â„,

âˆ’
ğ¼

+
ğ›¾ ğ´ğ‘

ğ¸

ğ›¾

âˆ’

âˆ’

)

(

(

âˆ’

)

Note that ğµ has full rank (rank ğ‘š) and ğ¸
ğ›¾ ğ´ğ‘
is invertible. Hence, for any ğ‘˜, as long as conditions 1), 2),
3) are satisï¬ed for a given ğ¾, we can always ï¬nd ğ‘‘ such
(cid:3)
that ğ‘˜ is optimal. We, hence, completes the proof.

ğ›¾ğµâ€²ğ‘ƒğµ and ğ¼

âˆ’

+

Î”ğ‘¡
0
âˆ«

Remark 1. The assumption that ğµ has full column rank
is reasonable. The assumption indicates that there is no
redundant control inputs. As for the assumption that ğ´ is
invertible, consider a discrete-time system sampled from a
Ë†ğµğ‘¢ with a small sample
continuous-time system
period Î”ğ‘¡. For the discretized linear system, we have ğ´ =
ğ‘’ Ë†ğ´ğœ Ë†ğµğ‘‘ğœ. Apparently, ğ´ = ğ‘’ Ë†ğ´Î”ğ‘¡ is invertible.
ğ‘’ Ë†ğ´Î”ğ‘¡ , ğµ =

ğ‘¥ = Ë†ğ´ğ‘¥
Â¤

+

Remark 2. The conditions in Theorem 3 provide a quick
way to check the feasibility of the optimization problem (21)
before solving it. If the optimization problem (21) is not
feasible, then the attacker cannot trick the agent into learning
the â€˜nefariousâ€™ policy
no matter how the attacker
falsiï¬es the cost parameters. The feasible set of (21) cannot
be singleton. Once there exists ğ·
0, and ğ‘‘ such
ğ¾â€ , ğ‘˜ â€ )
is optimal, the same control
that the control policy
policy is optimal for ğ›¼ğ·, ğ›¼ğ¸, and ğ›¼ğ‘‘ for any ğ›¼ > 0.

ğ¾â€ , ğ‘˜ â€ )

0, ğ¸

(cid:23)

â‰»

(

(

Remark 3. The conditions in Theorem 3 are stated in the
frequency domain. Iracleous and Alexandridis have devel-
oped a set of necessary and sufï¬cient conditions based on the
state space representation in the time-domain, under which
the control policy ğ‘¢ = ğ¾ğ‘¥ is optimal for some ğ·
0,
ğ¸
ğ´, ğµ
[35] with stage cost
â‰»
ğ‘¥ â€²ğ‘¡ ğ·ğ‘¥ğ‘¡
ğ‘¢â€²ğ‘¡ ğ¸ğ‘¢ğ‘¡ . One can extend their results to check the
feasibility of (21) using conditions in the time-domain.

0 under the linear system

(cid:23)

+

)

(

IV. DECEIVING BATCH RL LEARNER

A. The Batch RL Learner

Consider a model-based LQG learner who learns a model
from a training dataset batch ï¬rst and then plans over
the estimated model. The LQG learner implements system
identiï¬cation from a dataset and then computes the optimal
controller based on the identiï¬ed model [2]. Consider the
ğ‘¥ğ‘¡ , ğ‘¢ğ‘¡ , ğ‘ğ‘¡ , ğ‘¥ğ‘¡
training dataset
1
}
generated from past experience of interacting with the LQR
system. The learner implements system identiï¬cation by
solving the following least-square problems:

: ğ‘¡ = 0, 1,

Â· Â· Â·

1
+

, ğ‘‡

D

{(

âˆ’

=

)

Ë†ğ´, Ë†ğµ

(

arg min
ğ´,ğµ

) âˆˆ

1
2 k

ğ‘‡

1

âˆ’

Ã•ğ‘¡=0

ğ´ğ‘¥ğ‘¡

ğµğ‘¢ğ‘¡

+

ğ‘¥ğ‘¡

1
+

k

âˆ’

2
2 ,

(22)

arg min
0,ğ¸

ğœ– ğ¼ ,ğ‘‘,ğ‘Ÿ

(cid:23)

â‰»

Ë†ğ·, Ë†ğ¸, Ë†ğ‘‘, Ë†ğ‘Ÿ

(

) âˆˆ

ğ·

Deï¬ne

ğ‘‡

1

âˆ’

ğ‘¥ â€²ğ‘¡ ğ·ğ‘¥ğ‘¡
Ã•ğ‘¡=0 (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ğ‘‘ â€²ğ‘¥ğ‘¡

ğ‘Ÿ

+

+

ğ‘¢â€²ğ‘¡ ğ¸ğ‘¢ğ‘¡

+

ğ‘ğ‘¡

âˆ’

.

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ğ‘¥ â€²1
ğ‘¥ â€²2
...
ğ‘¥ â€²ğ‘‡

ğ‘§â€²0
ğ‘§â€²1
...
ğ‘§â€²ğ‘‡

,

ğ‘‹ğ‘‡ â‰”

, ğ‘ğ‘‡ â‰”

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
where ğ‘§ğ‘¡ â‰”
ğ‘¢â€²ğ‘¡
â€². The least-square estimator for
ï£°
is (assuming the invertibility of ğ‘ â€²ğ‘)

ï£¹
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£»

ï£¹
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£»

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

ğ‘¥ â€²ğ‘¡

âˆ’

1

(cid:2)

(cid:3)

ğ´, ğµ

)

(

Note that the optimization problem (23) is convex. We can
write ğ‘¥ â€²ğ‘¡ ğ·ğ‘¥ğ‘¡ as ğ‘¥ğ‘¡ â€²Î˜

and ğ‘¢â€²ğ‘¡ ğ¸ğ‘¢ğ‘¡ as ğ‘¢ğ‘¡ â€²Î˜

. Deï¬ne

ğ·

ğ¸

Ë†ğ´

Ë†ğµ

â€² =

ğ‘ â€²ğ‘‡ ğ‘ğ‘‡

(

)

1ğ‘ğ‘‡ ğ‘‹ğ‘‡ .

âˆ’

(cid:2)

(cid:3)

ğ» â‰”

ğ‘¢0â€²
ğ‘¢1â€²
...

ğ‘¥ â€²0
ğ‘¥ â€²1
...
ğ‘¥ â€²ğ‘‡

âˆ’

1

1â€²

âˆ’

ğ‘¢ğ‘‡

1â€²

âˆ’

(

)

ğ‘¥0 â€²
ğ‘¥1 â€²
...

ï£®
ï£¯
ï£¯
ğ‘¥ğ‘‡
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

(

)

.

1
1
...
ï£¹
ï£º
ï£º
1
ï£º
ï£º
ï£º
ï£º
ï£º
ï£»

The optimization problem
admits a unique solution
23
)
(
if the objective function is strictly convex, i.e., ğ» â€²ğ» is
positive deï¬nite, or equivalently ğ» is column independent.
With the model identiï¬ed, the learner computes the control
policy based on (4), (5), and (6) using the estimated system
Ë†ğ·, Ë†ğ¸, Ë†ğ‘‘, Ë†ğ‘Ÿ
parameters

Ë†ğ´, Ë†ğµ

and

.

(

)

(

)

attacker needs to solve

(23)

min
Ëœğ·, Ëœğ¸ , Ëœğ‘‘, Ëœğ‘Ÿ , ğ‘ƒ,â„

ğ‘‡

1

âˆ’

Ã•ğ‘¡=0

câ€ 

k

c

2

âˆ’
k
ğ›¾ Ë†ğ´â€²ğ‘ƒ Ë†ğ´

ğ‘ .ğ‘¡. ğ‘ƒ = Ëœğ·

ğ›¾ Ë†ğµâ€²ğ‘ƒ Ë†ğµ

)

+
ğ›¾ Ë†ğµâ€²ğ‘ƒ Ë†ğµ
Ë†ğ´
ğ›¾
+
+
(
ğ›¾ Ë†ğµâ€²ğ‘ƒ Ë†ğµ
Ëœğ·
Ëœğ·ğ‘¥ğ‘¡

â‰¥

ğ¾â€  â€²

Ëœğ¸
(
+
âˆ’
ğ›¾ Ë†ğµâ€²ğ‘ƒ Ë†ğ´,
ğ¾â€  =
âˆ’
Ë†ğµğ¾â€ 
â€²â„,
ğ›¾ Ë†ğµâ€²â„,
ğ‘˜ â€  =
)
0,
Ëœğ‘‘ â€²ğ‘¥ğ‘¡

âˆ’
Ëœğ¸ > 0.

ğ‘Ÿ

ğ‘¢â€²ğ‘¡

)

Ëœğ¸
+
(
â„ = Ëœğ‘‘
Ëœğ¸
2
(
ğ‘ƒ

+
0,
â‰¥
= ğ‘¥ â€²ğ‘¡

ğ‘â€ ğ‘¡

+

+

+

Ëœğ¸ğ‘¢ğ‘¡ ,

ğ‘¡
âˆ€

ğ¾â€ ,

)

(24)

(25)

(26)

(27)

(28)

(29)

(30)

)

(

ğ´, ğµ

Ë†ğ´, Ë†ğµ

replaced by

The attackerâ€™s problem deï¬ned in (24)-(29) is convex,
whose feasibility is aligned with the feasibility of problem
. The solution of the
21 with
problem can then be used to falsify the original dataset
=
into the â€˜badâ€™
D
{(
dataset
. In
1
+
Section VI, we will demonstrate the effectiveness of the
batch RL attack model and the vulnerabilities of batch RL-
enabled LQG systems.

, ğ‘‡
1
}
Â· Â· Â·
: ğ‘¡ = 0, 1,
Â· Â· Â·

ğ‘¥ğ‘¡ , ğ‘¢ğ‘¡ , ğ‘ğ‘¡ , ğ‘¥ğ‘¡
Dâ€  =

(
: ğ‘¡ = 0, 1,

ğ‘¥ğ‘¡ , ğ‘¢ğ‘¡ , ğ‘â€ ğ‘¡ , ğ‘¥ğ‘¡

1
+

, ğ‘‡

{(

âˆ’

âˆ’

1

}

)

)

)

+

D

ğ‘¢â€²ğ‘¡

= ğ‘¥ â€²ğ‘¡

Ë†ğ´, Ë†ğµ
(
Ëœğ‘‘ â€²ğ‘¥ğ‘¡

Ëœğ·ğ‘¥ğ‘¡
+
Ëœğ‘‘, Ëœğ‘Ÿ and

Ë†ğ´ and Ë†ğµ because he only
Remark 4. The attacker uses
. Even if he knows ğ´ and ğµ,
knows the original dataset
D
to mimic the learnerâ€™s problem.
it is better to use
)
Ëœğ¸ğ‘¢ğ‘¡ câ€  is completely decided
Ëœğ‘Ÿ
Since ğ‘â€ ğ‘¡
+
by Ëœğ·,
Ëœğ¸,
. Indeed, the attacker only needs to
optimize over Ëœğ·, Ëœğ¸, Ëœğ‘‘, and Ëœğ‘Ÿ even though we include câ€  in the
objective function for simplicity. [13] also considered data
poisoning attacks on cost data. However, our formulation of
the optimization problem differs from theirs in two aspects:
1. We require the cost data to be consistent and compatible
with the state and the control to make the falsiï¬cation less
likely to be detected. 2. Our problem is convex. Hence, we
can solve the problem to obtain the optimal solution instead
of solving a surrogate problem which produces a sub-optimal
value.

B. The Batch RL Attacker

We deï¬ne the batch RL attack model by specifying the
attackerâ€™s capability, objective, and the information he has:

A. The ADP Learner

V. DECEIVING ADP LEARNER

1
+

1) The attacker is able to falsify the costs in the training
. The falsiï¬ed training dataset is denoted by
dataset
D
, ğ‘¡ = 0, 1,
Dâ€  =
ğ‘¥ğ‘¡ , ğ‘¢ğ‘¡ , ğ‘â€ ğ‘¡ , ğ‘¥ğ‘¡
. Suppose
{(
the falsiï¬cation of the cost data is consistent and
compatible with the state and the control data, i.e.,
ğ‘â€ ğ‘¡

+
2) The objective of the attacker is to trick the learner
by feeding

into learning the control policy
the learner the falsiï¬ed training dataset

= ğ‘¥ â€²ğ‘¡ ğ·â€ ğ‘¥ğ‘¡

ğ¾â€ , ğ‘˜ â€ )

ğ‘¢â€²ğ‘¡ ğ¸ â€ ğ‘¢ğ‘¡ .

ğ‘‘â€ â€²ğ‘¥ğ‘¡

Â· Â· Â·

, ğ‘‡

âˆ’

+

+

1

}

ğ‘Ÿ

)

(

3) The attacker only has the knowledge of the original

Dâ€ .

training dataset

.

D

[

, ğ‘ğ‘‡

ğ‘0, ğ‘1,

Deï¬ne c =
cost signals from the original dataset
ğ‘â€ 0, ğ‘â€ 1,
, ğ‘â€ ğ‘‡
[
in the poisoned dataset

Rğ‘‡ as a vector of
Â· Â· Â·
. Let câ€  =
Rğ‘‡ be a vector of falsiï¬ed cost signals
Dâ€ . To achieve his objective, the

] â€² âˆˆ

1] â€² âˆˆ

Â· Â· Â·

D

âˆ’

âˆ’

1

The policy iteration algorithms presented in Section II-A
will converge for the LQG problem [31], [36]. However,
the policy iteration algorithms required exact knowledge
of the system model (1) and the stage cost function (3).
Bradtke et al. proposed an ADP algorithm that allows the
agent
to perform an approximate version of the policy
iteration algorithm using merely the observed data points
ğ‘¥ğ‘¡ , ğ‘¢ğ‘¡ , ğ‘ğ‘¡ , ğ‘¥ğ‘¡

, ğ‘¡ = 0, 1, 2,

[1].

The ADP learner leverages Recursive Least Square (RLS)
to directly estimate the function ğ‘„ğ¾ ,ğ‘˜ in (8). To see how the
adaptive learner learns the optimal policy, we rearrange (7)
and (8) to obtain

1
+

)

Â· Â· Â·

(

ğ‘¥ğ‘¡ , ğ‘¢ğ‘¡
Î˜

= ğ‘„ğ¾ ,ğ‘˜
(
=
ğ‘¥ğ‘¡ , ğ‘¢ğ‘¡ , 1
= ğœ™â€²ğ‘¡ ğœƒğ¾ ,ğ‘˜,

[

]

ğ‘¥ğ‘¡ , ğ‘¢ğ‘¡

ğ‘

(

)

8

ğ›¾ğ‘„ğ¾ ,ğ‘˜

) âˆ’
ğ»ğ¾ ,ğ‘˜

(

) âˆ’

(
ğ›¾

ğ‘¥ğ‘¡

1, ğ‘¢ğ‘¡
+
ğ‘¥ğ‘¡

1
+
1, ğ‘¢ğ‘¡
+

)
1, 1
+

[

Î˜

(

]

ğ»ğ¾ ,ğ‘˜

,

)

ğ‘›

(

[

[

]

ğ‘š

ğ›¾
ğ‘¥ğ‘¡
[
ğ»ğ¾ ,ğ‘˜

ğ‘˜, 1
1, ğ¾ğ‘¥ğ‘¡
1
+
+
+
R(
ğ‘›
ğ‘š
ğ‘›
1
) (
+
+
) âˆˆ

=
where ğœ™ğ‘¡
ğ‘¥ğ‘¡ , ğ‘¢ğ‘¡ , 1
âˆˆ
] âˆ’
2 , and ğœƒğ¾ ,ğ‘˜ = Î˜
R(
ğ‘›
ğ‘š
2
1
2 ,
)/
) (
+
+
+
+
is the column vector concatenation of ğ‘¥, ğ‘¢,
ğ‘¥, ğ‘¢, 1
where
can
and scalar 1. Through the lens of modern RL,
be viewed as a feature vector of the original state and action
space and ğœƒğ¾ ,ğ‘˜ can be viewed as a vector of weights that
the agent needs to tune to approximate the true ğ‘„-function.
RLS can now be used to estimate ğœƒğ¾ ,ğ‘˜. The recurrence

ğ‘¥, ğ‘¢, 1

]
ğ‘š
+

2
)
+

[

]

relations for RLS are given by

ğ‘–

ğ‘–

= Ë†ğœƒğ‘§

1

ğ‘–

(

âˆ’

) +

)

ğ‘†ğ‘§

ğ‘–

(

1

)

âˆ’

Ë†ğœƒğ‘§

(

ğ‘†ğ‘§

(

)

= ğ‘†ğ‘§

ğ‘–

(

1

âˆ’

) âˆ’

ğ‘ğ‘¡

ğœ™â€²ğ‘¡
âˆ’
ğœ™â€²ğ‘¡ ğ‘†ğ‘§
ğ‘–

ğœ™ğ‘¡
1
(cid:0)
+
ğœ™ğ‘¡ ğœ™â€²ğ‘¡ ğ‘†ğ‘§
(
ğ‘–
1
)

âˆ’

(

âˆ’
ğœ™ğ‘¡

1
)
ğœ™â€²ğ‘¡ ğ‘†ğ‘§

ğ‘†ğ‘§

ğ‘–
(
1

âˆ’
+

Ë†ğœƒğ‘§

1

ğ‘–

(

âˆ’

)

,

1

)

, ğ‘†ğ‘§

= ğ‘†0.

(cid:1)

(

0

)

(

(31)
Here, ğ‘ƒ0 = ğ›½ğ¼ for some large positive constant ğ›½. ğœƒğ‘§ =
Î˜
ğ»ğ¾ğ‘§ ,ğ‘˜ğ‘§ )
is the true parameter vector for the function
ğ‘„ğ¾ğ‘§ ,ğ‘˜ğ‘§ . Ë†ğœƒğ‘§
is the ğ‘–th estimate of ğœƒğ‘§. The subscript ğ‘¡ and
ğ‘–
the index ğ‘– are both incremented at each time step. And ğ‘§
is the index that counts the number of policy updates the
algorithm made.

(

)

The LQG agent follow the ğ‘„-function based ADP algo-
rithms (i.e., Algorithm 1) to ï¬nd the optimal policy. In line
7, the agent adds an appropriate probing noise ğ‘’ğ‘¡ to make
sure ğœ™ğ‘¡ is persistently excited over time [1]. In line 9, when
1 = ğ¾ğ‘§ ğ‘¥ğ‘¡
computing ğœ™ğ‘¡ , we let ğ‘¢ğ‘¡
ğ‘˜ ğ‘§. In line 13 and 14,
+
Ë†ğ»
are sub-matrices of the matrix Ë†ğ»
ğ‘¢1
ğ‘¢ ğ‘¥
)
following the same notation for the matrix ğ»ğ¾ ,ğ‘˜ in (8).
The convergence of Algorithm 1 is guaranteed if
is
controllable (or at least stabilizable), ğ¾0 is stabilizing, and ğœ™ğ‘¡
is persistently excited. One can refer to [1] for more details
about the ğ‘„-function based ADP algorithm.

, and Ë†ğ»

ğ´, ğµ

Ë†ğ»

ğ‘¢ğ‘¢

+

)

(

,

(

(

)

)

(

Algorithm 1 The ğ‘„-Function Based ADP Algorithm

1: Initialize: Stabilizing policy

ğ¾0, ğ‘˜0

, tolerance ğœ–1 > 0,

)

(
= 0, ğ‘§ = 0, and ğ‘¡ = 0

ğœ–2 > 0
2: Set Ë†ğœƒ0
(
3: repeat
4:

0

)

Set ğ‘†ğ‘§
(
repeat

5:

6:
7:

8:

9:
10:

11:
12:

13:

14:

= ğ‘†0 â‰” ğ›½ğ¼ and ğ‘– = 0

0

)

Measure ğ‘¥ğ‘¡
Compute ğ‘¢ğ‘¡ = ğ¾ğ‘§ ğ‘¥ğ‘¡
+
Receive ğ‘ğ‘¡ and measure ğ‘¥ğ‘¡
Compute Ë†ğœƒğ‘§
ğ‘–
Set ğ‘¡ = ğ‘¡
Ë†ğœƒğ‘§
ğ‘–

1
+
(
1 and ğ‘– = ğ‘–
Ë†ğœƒğ‘§

1
+
2 < ğœ–2

)

+
) âˆ’

+

ğ‘˜ ğ‘§

ğ‘’ğ‘¡ and apply ğ‘¢ğ‘¡
1
+
using RLS (31)

(

(

k

1

) k

until
ğ‘–
âˆ’
Find the matrix Ë†ğ» corresponding to Ë†ğœƒğ‘§
Ë†ğ»
Ë†ğ»âˆ’
1
Compute ğ¾ğ‘§
ğ‘¢ğ‘¢
âˆ’
(
)
Ë†ğ»
Ë†ğ»âˆ’
1
Compute ğ‘˜ ğ‘§
ğ‘¢ğ‘¢
âˆ’
(
(
Set Ë†ğœƒğ‘§
ğ‘–
and z = z+1
(
)
ğ‘˜ ğ‘§
ğ‘˜ ğ‘§
ğ¾ğ‘§
+ k

1 =
+
1 =
+
= Ë†ğœƒğ‘§
ğ¹
1

ğ¹ < ğœ–2

0
(
)
ğ¾ğ‘§

(
ğ‘¢1
)

1
+
âˆ’

âˆ’

ğ‘¢ ğ‘¥

k

k

k

âˆ’

âˆ’

1

)

)

ğ‘–

(

)

15:
16: until

B. The ADP Attacker

We deï¬ne the ADP attack model by specifying the at-

tackerâ€™s capability, objective, and the information he has:

1) The attacker can falsify the cost signals ğ‘ğ‘¡ and ğ‘â€ ğ‘¡
received by the agent in line 8 of Algorithm 1.
2) The attackerâ€™s objective is to trick the agent

into

is

learning the â€˜nefariousâ€™ policy

ğ¾â€ , ğ‘˜ â€ )

.

(

3) The attacker knows the LQG system and receives the
same information as the agent during the learning
process.

The problem for the attacker is how to craft such an attack
so that the agent will ï¬nally learn the â€˜nefariousâ€™ policy and
the falsiï¬ed cost ğ‘â€ ğ‘¡ deviate insigniï¬cantly from the original
cost signals ğ‘ğ‘¡ . Hence, the attacker can ï¬rst solve the cost
parameters falsiï¬cation problem (21). Let
be
the solution of problem (21). The falsiï¬ed cost signal ğ‘â€ ğ‘¡
then can be crafted using ğ‘â€ ğ‘¡
ğ‘¢â€²ğ‘¡ ğ¸ğ‘¢ğ‘¡
+
+
for ğ‘¡ = 0, 1, 2,
. The agent receives ğ‘â€ ğ‘¡
in Algorithm 1
and other information received remain correct. Following
the same arguments as in Theorem 1 of [1], we know
that if problem (1) is feasible, the agent will eventually
be tricked into learning the â€˜nefariousâ€™ policy
. In
Section VI, we will demonstrate the effectiveness of the ADP
attack model and the vulnerabilities of the ADP based LQG
systems.

ğ·â€ , ğ¸ â€ , ğ‘‘â€ )
ğ‘Ÿ

ğ¾â€ , ğ‘˜ â€ )

= ğ‘¥ğ‘¡ ğ·â€ ğ‘¥ğ‘¡

ğ‘‘â€ ğ‘¥ğ‘¡

Â· Â· Â·

+

(

(

VI. NUMERICAL STUDIES

We now use an LQG system to demonstrate the bounds
we obtain in Section III as well as the effectiveness of the
two attack models against the Batch RL learner in Section IV
and the ADP learner in Section V. Consider a linear system
with 6-dimensional state consisting of its 3-dimensional (3D)
position and 3-dimensional (3D) velocity:

ğ´ =

ğ¼3
0

0.1ğ¼3
0.95ğ¼3

R6,

ğµ =

R6
Ã—

3.

0
0.1ğ¼3

)

(cid:20)

(cid:21)

âˆˆ

ğ‘¡ , ğ‘£ ğœ

(cid:20)
(cid:21)
0, 0.01ğ¼ğ‘›

âˆ¼ N (
ğœ’ğ‘¡ , ğœ‚ğ‘¡ , ğœğ‘¡
(

âˆˆ
Suppose ğ¶ = ğ¼ğ‘› and ğ‘¤ğ‘¡
. Let ğ‘¥ğ‘¡ =
)
ğ‘¡ , ğ‘£ ğœ‚
ğœ’ğ‘¡ , ğœ‚ğ‘¡ , ğœğ‘¡ , ğ‘£ ğœ’
ğ‘¡ ] â€². Here,
is the position of
[
ğ‘£ ğœ’
ğ‘¡ , ğ‘£ ğœ‚
the vehicle in the 3D space at time ğ‘¡ and
is
the velocity at time ğ‘¡. The vehicle starts from the initial
=
position
)
1,
. The LQG aims to stabilize the vehicle to the
(âˆ’
origin. The true cost parameters are ğ· = ğ¼ğ‘›, ğ¸ = 0.5ğ¼ğ‘š,
ğ‘‘ = 0, and ğ‘Ÿ = 0 and the cost signals the agent receives is
ğ‘ğ‘¡ = ğ‘¥ â€²ğ‘¡ ğ·ğ‘¥ğ‘¡
ğ‘¢â€²ğ‘¡ ğ¸ğ‘¢ğ‘¡ . The discount factor ğ›¾ is set to
be ğ›¾ = 0.9. The optimal policy, which stabilizes the vehicle
to the origin, can be computed according to (4)-(6):

ğ‘¡ , ğ‘£ ğœ
ğ‘¡ )
0 , ğ‘£ ğœ
0 )

ğœ’0, ğœ‚0, ğœ0
1

with velocity

ğ‘£ ğœ’
0 , ğ‘£ ğœ‚

1, 1, 0.5

(
0.5,

ğ‘‘ â€²ğ‘¥ğ‘¡

âˆ’

âˆ’

=

+

+

+

ğ‘Ÿ

)

)

(

(

(

0.5316
0.0000
0.0000

âˆ’
âˆ’
âˆ’
0.0000
0.0000
0.0000#

ğ¾ âˆ— =

"

ğ‘˜âˆ— =

"

âˆ’
âˆ’
âˆ’

0.0000
0.5316
0.0000

âˆ’
âˆ’
âˆ’

0.0000
0.0000
0.5315

âˆ’
âˆ’
âˆ’

0.9700
0.0000
0.0000

âˆ’
âˆ’
âˆ’

0.0000
0.9700
0.0000

âˆ’
âˆ’
âˆ’

0.0000
0.0000
0.9700#

,

âˆ’
âˆ’
âˆ’

.

(32)
Suppose the attacker aims to trick the agent into learning

the following policy:

ğ¾ â€  =

âˆ’

0.5316
0
0

"

0
0.5316
0

âˆ’

ğ‘˜â€  =

"

0.5316
0
0.5316#

âˆ’

,

9

0
0
0.5316

âˆ’

âˆ’

0.9700
0.
0

0
0.9700
0

âˆ’

0
0
0.9700#

âˆ’

(33)

(

âˆ’

)

which steer the vehicle to a dangerous position at
1, 0,

and stay there with zero velocity.

1

ğœ’, ğœ‚, ğœ

(

=

)

ğ¾â€ , ğ‘˜ â€ )

To see whether he can successfully mislead the agent
into learning the â€˜nefariousâ€™ policy
by falsifying
the cost parameters, the attacker can check the conditions
in Theorem 3. Indeed, the conditions 1), 2), and 3) are
held when choosing ğ¸ = ğ¼ğ‘š meaning the cost parameter
falsiï¬cation problem (21) is feasible. Solving problem
21
)
using CVXPY [37] yields (we write numerical values that
are less than 1.0e

10 as â€˜0â€™ due to space limitation.)

(

(

0.7163
0
0.2837
0.1218
âˆ’
0
0.1218

0.1448

âˆ’
0.2904
0
2.095

ğ·â€  = ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£°
(cid:2)

ğ‘‘â€  =

ğ¸ â€  =

"

âˆ’
0
1.000
0
0
0
0

0.2837
0
0.7163
0.1218
0
0.1219

0.1218
âˆ’
0
0.1218
0.5687
0
0.4313

0
0
0
0
1.000
0

0.1218
0
0.1218
âˆ’
0.4313
0
âˆ’
0.5687

âˆ’
0.1448

0

1.6084

0
0.5000
0

âˆ’
0.2096
0
0.2904#

.

0

0.1608

â€² ,

(cid:3)

,

ï£¹
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£»

(34)

ğ¹

k

ğ·

ğ·â€  âˆ’

ğ‘‘â€  âˆ’
ğ‘‘
The optimal value
ğ¹ of
k
Ë†ğ¾â€ , Ë†ğ‘˜ â€ )
the optimization problem (21) is 1.8137. The policy
(
computed using the falsiï¬ed cost parameters ğ·â€ , ğ¸ â€ , ğ‘‘â€ 
ğ¾â€ , ğ‘˜ â€ )
aligns well with the attackerâ€™s target policy
stated
in (33):

ğ¸ â€  âˆ’

+ k

+ k

ğ¸

k

k

ğ¹

(

True Cost Data c
Falsified Cost Data câ€ 

0

50

100

150

200
t

250

300

350

400

câ€  âˆ’ c

6

4

2

0.0

âˆ’ 0.5

0

100

200
t
(a) True cost data, falsiï¬ed cost data, and their differences.

300

400

Normal Trajectory
Trajectory After Attack
Dangerous Point

0.50
0.25
0.00
âˆ’ 0.25
âˆ’ 0.50
âˆ’ 0.75
âˆ’ 1.00

(cid:2)

0.5315
0.0000
0.0000

âˆ’
âˆ’
âˆ’
0.5316
0.0000
0.5316#

Ë†ğ¾ â€  =

"

Ë†ğ‘˜â€  =

"

âˆ’

0.0000
0.5315
0.0000

âˆ’
âˆ’
âˆ’

0.0000
0.0000
0.5315

âˆ’
âˆ’
âˆ’

0.9699
0.0000
0.0000

âˆ’
âˆ’
âˆ’

0.0000
0.9699
0.0000

âˆ’
âˆ’
âˆ’

0.0000
0.0000
0.9699#

,

âˆ’
âˆ’
âˆ’

.

âˆ’ 0.20.0 0.2 0.4 0.6 0.8 1.0 1.2

(cid:0)

1.0

0.8

0.6

0.4

(cid:1)

0.2

0.0

âˆ’ 0.2

A. Attacking Batch RL Learner

1

âˆ’

, ğ‘‡

Â· Â· Â·

ğ‘¥ğ‘¡ , ğ‘¢ğ‘¡ , ğ‘ğ‘¡ , ğ‘¥ğ‘¡

The original training data set

, ğ‘¡ =
=
0, 1,
is generated by running the LQG system
}
with uniformly distributed random controls and receiving the
accurate cost signals ğ‘ğ‘¡ , ğ‘¡ = 0, 1,
1. Here, ğ‘‡ = 400
is the number of time steps (i.e., the number of data tuple
collected).

Â· Â· Â·

1
+

, ğ‘‡

D

{(

âˆ’

)

D

Based on the dataset

, the Batch RL learner estimates
the system parameters and the cost parameters using (23)
and (23). With the estimates, the computed optimal policy
under the clean data is
0.0148
0.5613
âˆ’
0.5232
0.0268
0.0145
0.0047

0.0063
0.0189
0.9630#

0.9878
âˆ’
0.0265
0.0171

0.0234
0.0045
0.5465

0.0188
0.9337
0.0111

Ë†ğ¾ âˆ—batch

=

"

,

âˆ’
âˆ’
âˆ’
06

âˆ’
âˆ’
3.9701e

Ë†ğ‘˜âˆ—batch

=

1.5601e

âˆ’

âˆ’

âˆ’

âˆ’

âˆ’
âˆ’
â€² .

âˆ’
âˆ’

07

âˆ’

1.6453e

06

âˆ’

(cid:2)

(cid:3)

We can see that the batch RL learner learns a decent policy
Ë†ğ¾âˆ—batch, Ë†ğ‘˜ âˆ—batch)
that only differs slightly from the optimal
(
policy (32).
The attacker aims to trick the agent into learning the
â€˜nefariousâ€™ policy (33) by falsifying the cost data ğ‘ğ‘¡ into ğ‘â€ ğ‘¡ .
First, the attacker estimates the system parameters using the
clean dataset
. Then, the attacker solves the optimization
problem (24) to generate the falsiï¬ed cost data ğ‘â€ ğ‘¡ , ğ‘¡ =
0, 1,
1. The LQG agent learns from the falsiï¬ed
Â· Â· Â·
, ğ‘¡ = 0, 1, 2,
and is
data set
1
)
{(
+
misled into learning the policy

, ğ‘‡
âˆ’
Dâ€  =

ğ‘¥ğ‘¡ , ğ‘¢ğ‘¡ , ğ‘â€ ğ‘¡ , ğ‘¥ğ‘¡

Â· Â· Â·

, ğ‘‡

D

âˆ’

1

}

(b) Trajectories generated from the policy learned from the clean
data and the policy learned from the falsiï¬ed data respectively.

Fig. 2: Demonstration of attacks against LQG agent using
batch RL.

which is close to the attacker target policy (33).

The results of the attack model are demonstrated in Fig.
2. Fig. 2a presents the true cost data, the falsiï¬ed cost data,
and how much falsiï¬cation is imposed. We can see that the
attacker only applied small falsiï¬cation on each cost signal
(i.e., the cost data and the falsiï¬ed cost data stays close).
Fig. 2b shows the trajectories of the vehicle under the policy
Ë†ğ¾âˆ—batch, Ë†ğ‘˜ âˆ—batch)
learned from the true data
and the policy
(
Ë†ğ¾â€ batch, Ë†ğ‘˜ â€ batch)
Dâ€ . We can
learned from the falsiï¬ed data
(
see that the attack successfully tricks the LQG agent into
learning a policy that steers the vehicle into a dangerous
position. The attackâ€™s success is indeed built on a very small
câ€  âˆ’
falsiï¬cation of the cost data. To see this, we have
96.2731 = 2.296%. The results show
c
k
/
the attack modelâ€™s effectiveness and exhibit the LQG agentâ€™s
vulnerabilities using batch RL.

2 = 2.2107

/k

D

c

k

k

2

B. Attacking ADP Learner

0.5613
âˆ’
0.0268
0.0047

âˆ’
0.5812

0.0148
0.5232
0.0145

âˆ’
âˆ’
âˆ’
0.0382

âˆ’

âˆ’

Ë†ğ¾ â€ batch

=

Ë†ğ‘˜â€ batch

=

"

(cid:2)

0.0234
0.0045
0.5465

âˆ’
âˆ’
0.5310

â€² ,

(cid:3)

0.9878
âˆ’
0.0265
0.0171

0.0188
0.9337
0.0111

âˆ’
âˆ’

0.0063
0.0189
0.9630#

âˆ’
âˆ’

,

The LQG agent implements Algorithm 1 to learn the
optimal policy to control the vehicle by interacting with the
5. Set ğ›½ = 10.
environment. Set the tolerance ğœ–1 = ğœ–2 = 1e

âˆ’

10

600

400

200

600

400

200

0

0

500

1000

500

1000

(a) True cost signals, falsiï¬ed cost signals, and their differences.

100

10-1

10-2

5

10

15

20

(b) The policy updates ğ¾ğ‘§ , ğ‘§ = 0, 1, 2,
, 22 during the ADP
learning process under the falsiï¬ed cost signals and the policy
sequences converge to the attackerâ€™s target policy
rather
than the optimal policy

ğ¾ â€ , ğ‘˜â€ )

Â· Â· Â·

(

.

ğ¾ âˆ—, ğ‘˜âˆ—)

(

Fig. 3: Demonstration of attacks against LQG agent using
ADP.

Let the initial control policy be

ğ¾0 =

0.0300
0
0

"

0
0.5600
0

0
0
0.5000

âˆ’

0.1000
0
0

0
1.0000
0

0
0
1.0000#

,

ğ‘˜0 =

0

0

0

â€² .

(cid:3)

(cid:2)

The attacker aims to trick the ADP learner into learning the
â€˜nefariousâ€™ policy (33) by falsifying the cost signals received
by the LQG agent. As we discussed in Section V, the attacker
can use (34) to falsify cost signals into ğ‘â€ ğ‘¡
ğ‘¢â€²ğ‘¡ ğ¸ â€ ğ‘¢ğ‘¡

+
ğ‘Ÿ. The numerical computation indicates that

= ğ‘¥ â€²ğ‘¡ ğ·â€ ğ‘¥ğ‘¡

ğ‘‘â€ â€²ğ‘¥ğ‘¡

+

|

ğ‘ğ‘¡

1.088

2
2 +
k
If the LQG agent receives the cost signals,
algorithms ï¬nally lead the LQG agent to the policy

0.3060

2
2 +

0.419

| â‰¤

ğ‘¢ğ‘¡

ğ‘¥ğ‘¡

k

k

k

k

ğ‘¥ğ‘¡

2.

k

the ADP

+
ğ‘â€ ğ‘¡ âˆ’

Ë†ğ¾ âˆ—ADP

=

Ë†ğ‘˜âˆ—ADP

=

0.5280
âˆ’
0.0021
âˆ’
0.0065

"

0.0007

âˆ’

(cid:2)

0.0058
0.5409
0.0340

âˆ’
âˆ’
âˆ’
0.0054

âˆ’

0.0064
0.0201
0.5231

âˆ’
âˆ’
0.0093

â€² ,

(cid:3)

0.9673
âˆ’
0.0015
0.0033

0.0001
0.9742
0.0177

âˆ’
âˆ’

0.0035
0.0166
0.9632#

âˆ’
âˆ’

,

11

which only differs slightly from the optimal policy (32).

If the LQG agent receives the falsiï¬ed cost signals, the

agent will be tricked into learning the policy

Ë†ğ¾ â€ ADP

=

"

0.5253
0.0009
0.0093

âˆ’
âˆ’
âˆ’
0.5229

0.0473
0.5274
0.0675

âˆ’
âˆ’
0.0076

0.0284
0.0050
âˆ’
0.5754.
âˆ’
0.5144

0.9685
0.0028
0.0054

âˆ’
âˆ’
âˆ’

0.0145
0.9696
0.0234

âˆ’
âˆ’

0.0242
0.0055
1.0034#

âˆ’
âˆ’

,

=

Ë†ğ‘˜â€ ADP
which is close the the attackerâ€™s target policy (33).

â€² ,

âˆ’

(cid:2)

(cid:3)

The results of the attack model on ADP are demonstrated
in Fig. 3. In Fig. 3a, we present the true cost signals, the fal-
siï¬ed cost signals, and the absolute value of their differences
over time. As we can see, the falsiï¬ed cost signals stay close
to the true cost signals. The falsiï¬cation grows as
and
ğ‘¢ğ‘¡
increases because the falsiï¬ed cost signals is generated
k
using ğ‘â€  = ğ‘¥ â€²ğ‘¡ ğ·â€ ğ‘¥ğ‘¡
ğ‘‘â€ ğ‘¥ğ‘¡ . Fig. 3b shows the
ğ‘Ÿ
how the policy
(in line 13-14 Algorithm 1) iterates
during the learning process. The attacker gradually misleads
ğ¾â€ , ğ‘˜ â€ )
the LQG agent into learning the â€˜nefariousâ€™ policy
.
The results show the attack modelâ€™s effectiveness and exhibit
the LQG agentâ€™s vulnerabilities using ADP approaches.

+
ğ¾ğ‘§ , ğ‘˜ ğ‘§

ğ‘¢â€²ğ‘¡ ğ¸ â€ ğ‘¢ğ‘¡

ğ‘¥ğ‘¡

+

+

k

k

k

(

)

(

VII. CONCLUSION

In this work, we have studied the vulnerabilities of the
RL-enabled LQG control systems under cost signal falsiï¬-
cation. We have shown that a small falsiï¬cation of the cost
parameters will only lead to a bounded change in the optimal
policy. The bound is linear on the amount of falsiï¬cation
the attacker can apply to the cost parameters. This result
shows a certain degree of robustness of the optimal policy
to small unintended changes in the cost parameters. We
have proposed an attack model where the attacker conducts
intentional falsiï¬cation on the cost parameters to mislead the
agent into learning a â€˜nefariousâ€™ policy. We have formulated
the attackâ€™s problem as an optimization problem, which is
proved to be convex, and developed necessary and sufï¬cient
conditions to check the feasibility of the attackerâ€™s problem.
Based on the attack model on cost parameters, we dis-
cussed two attack models on the batch RL and the ADP
learners. Numerical
results have shown that with only
2.296% of falsiï¬cation on the cost data, the attacker can
achieve his goal â€” misleading the batch RL learner into
learning the â€™nefariousâ€™ policy that
leads the vehicle to
a dangerous position. An ADP learner updates the policy
iteratively and learns in an online manner. The results have
demonstrated that the attacker can gradually trick the learner
into learning the â€˜nefariousâ€™ policy, even if the falsiï¬ed cost
signals stay close to true cost signals.

The paper has exhibited the effectiveness of these at-
tack models and revealed vulnerabilities of the RL-enabled
control systems. The authors hope this paper can bring
more attention from the community to the potential security
threats faced by RL-enabled control systems. Future works
can focus on more effective attack models on cost signals,
state measurements, and control commands. With a better
understanding of these attack models, we can take further
steps to develop reliable and effective detection and defensive
mechanisms.

APPENDIX

Hence,

A. Lemmas

Lemma 2. Given any two positive semi-deï¬nite matrices of
the same dimension ğ‘‹ and ğ‘,

ğ¸ âˆ’

1
2 âˆ’

ğ¸ âˆ’

1
1 k â‰¤

k

1

k

ğ¸ âˆ’

2

1
1 k
ğ¸ âˆ’

1
1 k

âˆ’ k

Î”ğ¸

.

k

k

(cid:3)

ğ‘‹

ğ¼

(

k

+

ğ‘† ğ‘‹

1

âˆ’

)

ğ‘‹

.

k â‰¤ k
ğ‘‹ âˆ’

k
1

Proof. Assume ğ‘‹ > 0. Note that
ğ‘†
.
)
+
)
1 =
Hence, the inverse of them are equal, i.e., ğ‘‹ âˆ’
1 =
ğ‘‹.
ğ¼
(
By a continuity argument, the inequality also holds when ğ‘‹
(cid:3)
is only positive semi-deï¬nite.

1. Then, we obtain ğ‘‹

ğ‘† ğ‘‹
)âˆ’
) â‰¤

+
ğ‘†
+
ğ‘†

(
ğ‘‹ âˆ’

ğ‘‹ =
1
ğ‘‹ âˆ’
1

ğ‘† ğ‘‹

ğ‘† ğ‘‹

)âˆ’

)âˆ’

+

+

+

(
1

ğ¼

(

(

(

ğ¼

â†’
= max

Lemma 3. Let
ators Rğ‘›
Ã—

ğ‘›

L (
Rğ‘›
Ã—

ğ‘›

ğ‘›, Rğ‘›
Ã—

Rğ‘›
Ã—
ğ‘› with the following operator norm

be the space of linear oper-

)

T (
2

ğ´ğ‘

ğœ
(
ğ›¾ğœŒ

)
ğ´ğ‘

2

ğ‘‹

:

ğ‘‹

= 1

,

for

{kT (
kT k L
= ğ‘‹
Deï¬ne
Schur stable. There exists a ï¬nite ğœ

) k
}
ğ›¾ ğ´â€²ğ‘ ğ‘‹ ğ´ğ‘ with 0 < ğ›¾

âˆ’

ğ‘‹

k

k

)

â‰¤

such that

T âˆˆ L (

ğ´ğ‘

)

(

ğ‘›

Rğ‘›
Ã—

ğ‘›, Rğ‘›
Ã—

)
1 and ğ´ğ‘ is
1

kT âˆ’

k L â‰¤

)

(

1
âˆ’
Proof. Note that ğ´ğ‘ is Schur stable. There exists ğ¿ such
that ğ‘€ = ğ‘‹
ğ›¾ ğ´â€²ğ‘ ğ‘‹ ğ´ğ‘, which is a discrete-time Lyapunov
equation given ğ‘€ and ğ´ğ‘. It is easy to verify that ğ‘‹ =
ğ‘˜ by pluging it back into the Lyapunov

âˆğ‘˜=0 ğ›¾ ğ‘˜

âˆ’
ğ‘˜ ğ‘€
ğ´â€²ğ‘)
equation. Hence, we have
Ã

ğ´ğ‘

(

)

(

1

âˆ’

ğ‘€

)

(

T

(cid:13)
(cid:13)

âˆ

=

(cid:13)
(cid:13)

â‰¤

âˆ

(cid:13)
Ã•ğ‘˜=0
(cid:13)
(cid:13)
(cid:13)
(cid:13)
Ã•ğ‘˜=0

ğ›¾ ğ‘˜

ğ´â€²ğ‘)

(

ğ‘˜ ğ‘€

ğ´ğ‘

ğ‘˜

)

(

ğ›¾ ğ‘˜

2

ğ´ğ‘˜
ğ‘ k

k

k

ğ‘€

.

k

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ğœ

ğ´ğ‘

ğ´ğ‘

ğ‘€
By Gelfandâ€™s formula, there exists a ï¬nite ğœ
(
ğ‘˜ for every ğ‘˜
ğ´ğ‘˜
0. Here, ğœŒ
ğœŒ
ğ‘ k â‰¤
â‰¥
(
k
)
is deï¬ned as ğœ
ğ´ğ‘
the spectral radius of ğ´ğ‘ and ğœ
)
(
âˆğ‘˜=1 ğ›¾ ğ‘˜
ğ´ğ‘˜
< 1,
ğ´ğ‘
. Since ğœŒ
ğœŒ
supğ‘˜
ğ‘ k/
)
(
{k
ğ´ğ‘
ğœ
ğ´ğ‘˜
âˆğ‘˜=0 ğ›¾ ğ‘˜
2
convergent and
(
ğ‘ k
ğ›¾ğœŒ
k

)
(
1
kT âˆ’

}
k L â‰¤

such that
ğ´ğ‘
is
)
(
â‰”
ğ´ğ‘
)
(
ğ‘˜ is
ğ´ğ‘
k
k
2
(cid:3)
2 .
)
ğ´ğ‘

ğ´ğ‘

Ã
â‰¤

N

)

(

)

âˆˆ

ğ‘˜

Lemma 4. Let Î”ğ¸ = ğ¸2
Ã
two square matrices ğ¸1 and ğ¸2. Assume
ğ¸ âˆ’
1

< 1. We have

Î”ğ¸

âˆ’

1

k

k

1
âˆ’
ğ¸1, where ğ¸1 and ğ¸2 are
Î”ğ¸
< 1 and

)

(

k

k

ğ¸ âˆ’

1
2 âˆ’

ğ¸ âˆ’

1
1 k â‰¤

k

k

1
1 k

ğ¸ âˆ’
1

2

Î”ğ¸
k
1
ğ¸ âˆ’
1 k

k

.

âˆ’ k
Proof. A straight calculation gives ğ¸ âˆ’
It follows immediately that

1
1 âˆ’

1

ğ¸ âˆ’
2

= ğ¸ âˆ’
1

1

Î”ğ¸ ğ¸ âˆ’

1
2 .

ğ¸ âˆ’

1
1 âˆ’

ğ¸ âˆ’

1
2 k

k

=

k

1

ğ¸ âˆ’
1

Î”ğ¸ ğ¸ âˆ’

1
2 k â‰¤ k

1

Î”ğ¸

ğ¸ âˆ’
1

ğ¸ âˆ’

1
2 k

.

(35)

Applying the triangle inequality, we obtain

kk
1
ğ¸ âˆ’
1 k +
. Plugging it back into (35)

1
2 k â‰¤ k

ğ¸ âˆ’

k

Î”ğ¸

1

ğ¸ âˆ’
1
k
yields

ğ¸ âˆ’

1
2 k â‰¤

kk

1
ğ¸ âˆ’
1 k
k
1
ğ¸ âˆ’
1
1
âˆ’ k

Î”ğ¸

k

ğ¸ âˆ’

1
2 âˆ’

ğ¸ âˆ’

1
1 k â‰¤

k

k
1

ğ¸ âˆ’

1
1 k

2

Î”ğ¸
k
Î”ğ¸
1
ğ¸ âˆ’
1

âˆ’ k

k

.

k

Î”ğ¸

Î”ğ¸

k

k
1
ğ¸ âˆ’
1

k

k â‰¤ k

Lemma 5. Given a real-valued Schur stable matrix ğ´ğ‘ and
a scalar 0 < ğ›¾

ğ›¾ ğ´ğ‘ is invertible.

1, ğ¼

â‰¤

âˆ’

.

Proof. Suppose that ğœ† is an eigenvalue of ğ´ğ‘ and ğ‘£ is its
associated vector. Hence, we have

ğ´ğ‘ğ‘£ = ğœ†ğ‘£.

Therefore, we have

ğ¼

(

âˆ’

ğ›¾ ğ´ğ‘

)

ğ‘£ = ğ‘£

âˆ’

ğ›¾ğœ†ğ‘£ =

1

(

âˆ’

ğ›¾ğœ†

ğ‘£.

)

|

âˆ’

âˆ’

Hence, for every eigenvalue ğœ† of matrix ğ´ğ‘, 1
eigenvalue of the matrix ğ¼
ğœ†
|
part of 1
âˆ’
ğ´ğ‘. Thus, ğ¼

ğ›¾ğœ† is an
ğ›¾ ğ´ğ‘. Since ğ´ğ‘ is Schur stable,
< 1 for every eigenvalue ğœ† of matrix ğ´ğ‘. Hence, the real
ğ›¾ğœ† is non-zero for every eigenvalue ğœ† of matrix
(cid:3)

ğ›¾ ğ´ğ‘ is invertible.

âˆ’
Lemma 6. Suppose ğ‘“1, ğ‘“2
âˆˆ
ğ›¼ğ¼ and
and ğ›¼-strongly convex, i.e.,
â‰¤
and ğ‘¢2 = arg minğ‘¢ ğ‘“2
Let ğ‘¢1 = arg minğ‘¢ ğ‘“1
ğ‘¢2
ğ‘“1

)
k
âˆ’
Proof. An application of Taylor expansion yields

R are twice differentiable
2 ğ‘“2
ğ›¼ğ¼.
â‰¤
. Suppose

ğœ–, then

ğ‘¢
(
ğ‘¢1

â†’
2 ğ‘“1

âˆ‡
ğ‘¢
(

) k â‰¤

k â‰¤

ğœ–
ğ›¼ .

Rğ‘›

ğ‘¢2

âˆ‡

k

)

(

ğ‘“1

ğ‘¢2

=

ğ‘“1

ğ‘¢1

2 ğ‘“1

Â¯ğ‘¢

ğ‘¢2

ğ‘¢1

,

(

âˆ‡
with Â¯ğ‘¢ = ğ›½ğ‘¢1
stationary point of ğ‘“1, i.e.,

) + âˆ‡
(
ğ‘¢2 for some 0
ğ‘“1

âˆ‡
ğ›½

)
1

+ (

ğ‘¢1

âˆ’

(

)

) (
ğ›½

)

âˆ’
1. Since ğ‘¢1 is a

â‰¤

â‰¤
= 0, we have

ğ‘¢2

ğ›¼ğ¼

ğ‘¢1

k

(
âˆ’
which gives

) k â‰¤ kâˆ‡
ğ‘¢1

ğ‘¢2

(
ğœ–
ğ›¼ .

2 ğ‘“1

(
ğ‘¢1

âˆ‡
Â¯ğ‘¢

) (

)
ğ‘¢2

âˆ’

=

) k

ğ‘¢2

ğ‘“1

(

kâˆ‡

) k â‰¤

ğœ–,

(cid:3)

k

k â‰¤

âˆ’
Lemma 7. Deï¬ne the functions ğ‘“ğ‘–
ğµğ‘¢
ğ´ğ‘¥
)
(
deï¬nite. Suppose
k â‰¤
ğœ–
(Â·)
a matrix. Let ğ¾ğ‘–ğ‘¥ = arg minğ‘¢ ğ‘“ğ‘–

ğ¸1
2, where ğœ†min

+
)
for ğ‘– = 1, 2. Suppose ğ¸1 and ğ¸2 are positive
ğœ– and
ğ‘ƒ1
ğ‘ƒ2
with
is the smallest eigenvalue of
ğ‘¥, ğ‘¢
. Then, we have

) â€²ğ‘ƒğ‘–
ğœ†min

2 ğ‘¢â€²ğ¸ğ‘–ğ‘¢

ğ›¾ 1
2 (

ğ‘¥, ğ‘¢

k â‰¤

= 1

ğµğ‘¢

ğ¸2

ğ¸1

ğ´ğ‘¥

)/

âˆ’

âˆ’

â‰¤

+

+

k

k

ğœ–

)

(

(

(

ğ‘“

(

)

ğ¾1

k

âˆ’

ğ¾2

2ğ›¾

k â‰¤

ğœ†min

ğ¸1
)
2ğ›¾

(

max

ğµ

ğ´

,

k

k

{k

2

k}

(k

ğ¾1

1

)

ğ‘“

ğœ–

(

)

k +

+

ğœ†min

ğ¸1

k

)

(

ğ¾1

ğœ–.

k

(36)

Proof. According to the deï¬nition of strong convexity, ğ‘“1, ğ‘“2
are ğ›¼-strongly convex in ğ‘¢ with

ğ›¼ = min

ğœ†min

{

ğ¸1

)

(

, ğœ†min

ğ¸2

.

)}

(

We can calculate the gradient of ğ‘“ğ‘–

ğ‘¥, ğ‘¢

(

ğ‘¥, ğ‘¢

ğ‘“ğ‘–

(

âˆ‡

)

=

(

ğ›¾ğµâ€²ğ‘ƒğ‘– ğµ

ğ¸ğ‘–

ğ‘¢

)

+

+

as

)
ğ›¾ğµâ€²ğ‘ƒğ‘– ğ´ğ‘¥.

Note that given

< 1,

It is obvious that

ğ¸ âˆ’

1
1 kk

Î”ğ¸

ğ¸ âˆ’

1
1 k

.

k â‰¤ k

ğ›¾ğµâ€²ğ‘ƒ1ğµ

k

ğ¸1

+

âˆ’

ğ›¾ğµâ€²ğ‘ƒ2ğµ

ğ¸2

âˆ’

k â‰¤

ğµ

ğ›¾

k

2

k

k

ğ‘ƒ1

âˆ’

ğ‘ƒ2

ğ¸1

ğ¸2

,

k

âˆ’

k + k

12

and

ğ›¾ğµâ€²ğ‘ƒ1 ğ´

k

âˆ’

ğ›¾ğµâ€²ğ‘ƒ2 ğ´

Hence, for any ğ‘¥ such that

ğ›¾

ğµ

ğ´

kk

kk

k
1, we have

ğ‘ƒ1

ğ‘ƒ2

.

k

âˆ’

k â‰¤
ğ‘¥

k

k â‰¤

kâˆ‡
ğ›¾

â‰¤

ğ‘¥, ğ‘¢
2

ğ‘“1
(
ğµ

k

k

k
ğµ

ğ›¾

) k
ğ¸1

) âˆ’ âˆ‡
ğ‘ƒ1
âˆ’
ğ´

kk
ğ´

,

kk
ğµ

ğ‘¥, ğ‘¢

(

ğ‘“2
ğ‘ƒ2
ğ‘ƒ1
âˆ’
2

k + k
ğ‘ƒ2
ğ‘ƒ1

k]

(cid:2)
k
+ [
ğ›¾ max

{k

âˆ’
k
â‰¤
From Lemma 6, we know

k}

k

k

ğ¸2

ğ‘¢

k

k

k

âˆ’

(cid:3)
ğ‘¢

ğ‘ƒ2

k(k

1

) +

ğ¸1

ğ›¾

k

ğ¸2

ğ‘¢

k

kk

âˆ’

k +

ğ¾1ğ‘¥

ğ›¼

Note that

k

k

ğ‘¥

ğ¾2ğ‘¥

âˆ’
k
1 and

= ğ›¼

ğ‘¢1

k

ğ‘¢2

âˆ’
ğ¾1ğ‘¥

k â‰¤ kâˆ‡
ğ¾1

k â‰¤ k

k

ğ‘¢1

k

k â‰¤ k

ğ‘“2

ğ‘¢1

.

(
) k
. We have

k â‰¤
ğ›¾
ğ›¼

ğ¾1

k

âˆ’

ğ¾2

k â‰¤

ğµ

,

k

2

k}

(k

ğ¾1

ğ‘ƒ1

1

) k

âˆ’

ğ‘ƒ2

k

k +

(37)

max
ğ›¾
ğ›¼ k

{k
ğ¾1

k

ğ´

k
ğœ–.

+

Note
ğœ†min

(

that
2
)/

ğ¸1

ğœ–

â‰¤

ğœ†min

ğ¸1

2. By Weylâ€™s

â‰¤

)/
ğ›¼, we can show (36) from (37).

(

inequality
(cid:3)

Lemma 8. Let ğ‘€
âˆˆ
Î”ğ‘€
Rğ‘›
ğ‘›. Assume
Ã—
Rğ‘› be the solution of
â„

Rğ‘›
Ã—
Î”ğ‘€

âˆˆ
Î”â„

k

k â‰¤

1
1
ğ‘€ âˆ’

k

2

k

ğ‘› be a non-singular matrix, and
Rğ‘› and

. Let â„

âˆˆ

+

âˆˆ

ğ‘€ â„ = ğ‘‘,

ğ‘€

(

for some ğ‘‘
and

âˆˆ

Rğ‘› and Î”ğ‘‘

+

âˆˆ

Î”â„

= ğ‘‘

)

+

Î”ğ‘‘,

Î”ğ‘€

â„

+

) (
Rğ‘›. Then, ğ‘€

Î”ğ‘€ is non-singular

+

Î”â„

k

k â‰¤

2

k

1

ğ‘€ âˆ’

k (k

Î”ğ‘‘

Î”ğ‘€

.

â„

k)

kk

k + k

Proof. It is easy to see that

ğ‘€

+

Î”ğ‘€ = ğ‘€

(

ğ‘€ âˆ’

1Î”ğ‘€

.

)

ğ¼

+
ğ‘€ âˆ’

Since ğ‘€ is non-singular, if ğ¼
Î”ğ‘€ is non-singular. Indeed, for every non-zero ğ‘¥

+

1Î”ğ‘€ is non-singular, ğ‘€

+

ğ¼

k(

+

ğ‘€ âˆ’

1Î”ğ‘€

ğ‘¥

)

ğ‘¥

k â‰¥ k
=
1
(
0,

â‰¥

ğ‘€ âˆ’

1Î”ğ‘€

k âˆ’ k

âˆ’ k

ğ‘€ âˆ’

1Î”ğ‘€

ğ‘¥

kk
ğ‘¥

k) k

k

Rğ‘›,

âˆˆ

k

which shows the non-singularity of ğ‘€
solve for Î”â„:

+

Î”ğ‘€. Now, we can

Î”â„ =
=

(

(

ğ‘€

ğ¼

+

1

Î”ğ‘€
+
ğ‘€ âˆ’

âˆ’
)
1Î”ğ‘€

Î”ğ‘‘
âˆ’
1ğ‘€ âˆ’

âˆ’

Î”ğ‘€ â„
1
Î”ğ‘‘

(

(

)

)

âˆ’

Î”ğ‘€ â„

)

Hence, we have

Î”â„

k

k â‰¤ k(

ğ¼

+

ğ‘€ âˆ’
ğ‘€ âˆ’
k
1
ğ‘€ âˆ’
1

k ( k

1Î”ğ‘€
1

1

âˆ’

)

kk

1

ğ‘€ âˆ’

k(k

Î”ğ‘‘

Î”ğ‘€

k
kk
Î”ğ‘‘

k
k + k

Î”ğ‘‘

(k
Î”ğ‘€

k + k
â„

kk

k)

.

k + k
â„

kk

Î”ğ‘€

1
2

â‰¤

â‰¤

âˆ’ k
ğ‘€ âˆ’
k

Î”ğ‘€

â„

k)

kk

B. Proof of Lemma 1

)

)

(

(
(

ğ´, ğµ

ğ´, ğ·1
/
5
)

Proof. From Theorem 4 of [38], we know that if
is
2
is observable, the solution to the
controllable and
is positive deï¬nite and unique and ğ¾ is
Riccati equation
stabilizing. Then from (4), we know ğ¾âˆ— is uniquely decided
ğ´, ğµ
and the cost parameters
by the system parameters
ğ·, ğ¸
Since ğ¾ is stabilizing, from Lemma 5, ğ¼ğ‘›

is
invertible. Hence, the solution â„âˆ— of (6) is unique and depend
and ğ¾âˆ—. Then, (4)
ğ´, ğµ
only on the control parameters
ğ´, ğµ
shows that ğ‘˜ âˆ— is uniquely decided by
.
)
(cid:3)

ğ·, ğ¸, ğ‘‘

and

ğµğ¾

)
(

âˆ’

ğ´

+

ğ›¾

(

)

(

(

)

(

)

(

)

.

REFERENCES

[1] S. J. Bradtke, B. E. Ydstie, and A. G. Barto, â€œAdaptive linear quadratic
control using policy iteration,â€ in Proceedings of 1994 American
Control Conference-ACCâ€™94, vol. 3.

IEEE, 1994, pp. 3475â€“3479.

[2] H. Mania, S. Tu, and B. Recht, â€œCertainty equivalence is efï¬cient for
linear quadratic control,â€ Advances in Neural Information Processing
Systems, vol. 32, 2019.

[3] F. L. Lewis and D. Vrabie, â€œReinforcement learning and adaptive dy-
namic programming for feedback control,â€ IEEE circuits and systems
magazine, vol. 9, no. 3, pp. 32â€“50, 2009.

[4] Y. Jiang and Z.-P. Jiang, â€œComputational adaptive optimal control for
continuous-time linear systems with completely unknown dynamics,â€
Automatica, vol. 48, no. 10, pp. 2699â€“2704, 2012.

[5] B. Pang, T. Bian, and Z.-P. Jiang, â€œAdaptive dynamic programming
for ï¬nite-horizon optimal control of linear time-varying discrete-time
systems,â€ Control theory and technology, vol. 17, no. 1, pp. 73â€“84,
2019.

[6] D. GÂ¨orges, â€œDistributed adaptive linear quadratic control using dis-
tributed reinforcement learning,â€ IFAC-PapersOnLine, vol. 52, no. 11,
pp. 218â€“223, 2019.

[7] A. Cetinkaya, H. Ishii, and T. Hayakawa, â€œAn overview on denial-
of-service attacks in control systems: Attack models and security
analyses,â€ Entropy, vol. 21, no. 2, p. 210, 2019.

[8] Y. Huang, Z. Xiong, and Q. Zhu, â€œCross-layer coordinated attacks
on cyber-physical systems: A lqg game framework with controlled
observations,â€ in 2021 European Control Conference (ECC).
IEEE,
2021, pp. 521â€“528.

[9] Y. Mo and B. Sinopoli, â€œFalse data injection attacks in control
systems,â€ in Preprints of the 1st workshop on Secure Control Systems,
2010, pp. 1â€“6.

[10] F. Miao, Q. Zhu, M. Pajic, and G. J. Pappas, â€œCoding schemes
for securing cyber-physical systems against stealthy data injection
attacks,â€ IEEE Transactions on Control of Network Systems, vol. 4,
no. 1, pp. 106â€“117, 2016.

[11] T. Zhang and Q. Zhu, â€œStrategic defense against deceptive civilian gps
spooï¬ng of unmanned aerial vehicles,â€ in International Conference on
Decision and Game Theory for Security. Springer, 2017, pp. 213â€“233.
[12] Y.-C. Liu, G. Bianchin, and F. Pasqualetti, â€œSecure trajectory plan-
ning against undetectable spooï¬ng attacks,â€ Automatica, vol. 112, p.
108655, 2020.

[13] Y. Ma, X. Zhang, W. Sun, and J. Zhu, â€œPolicy poisoning in batch
reinforcement learning and control,â€ Advances in Neural Information
Processing Systems, vol. 32, 2019.

[15] V. Behzadan and A. Munir, â€œAdversarial

[14] X. Zhang, X. Zhu, and L. Lessard, â€œOnline data poisoning attacks,â€
in Learning for Dynamics and Control. PMLR, 2020, pp. 201â€“210.
learning
framework for benchmarking collision avoidance mechanisms in au-
tonomous vehicles,â€ IEEE Intelligent Transportation Systems Maga-
zine, vol. 13, no. 2, pp. 236â€“241, 2019.

reinforcement

[16] Y.-Y. Chen, C.-T. Chen, C.-Y. Sang, Y.-C. Yang, and S.-H. Huang,
â€œAdversarial attacks against reinforcement
learning-based portfolio
management strategy,â€ IEEE Access, vol. 9, pp. 50 667â€“50 685, 2021.
[17] M. Figura, K. C. Kosaraju, and V. Gupta, â€œAdversarial attacks in
consensus-based multi-agent reinforcement learning,â€ in 2021 Ameri-
can Control Conference (ACC).

IEEE, 2021, pp. 3050â€“3055.

k)

(cid:3)

13

[18] Y. Huang and Q. Zhu, â€œDeceptive reinforcement

learning under
adversarial manipulations on cost signals,â€ in International Conference
on Decision and Game Theory for Security. Springer, 2019, pp. 217â€“
237.

[19] â€”â€”, â€œManipulating reinforcement learning: Stealthy attacks on cost
signals,â€ Game Theory and Machine Learning for Cyber Security, pp.
367â€“388, 2021.

[20] A. Rakhsha, G. Radanovic, R. Devidze, X. Zhu, and A. Singla,
â€œPolicy teaching via environment poisoning: Training-time adversarial
attacks against reinforcement learning,â€ in International Conference on
Machine Learning. PMLR, 2020, pp. 7974â€“7984.

[21] J. Wang, Y. Liu, and B. Li, â€œReinforcement learning with perturbed
rewards,â€ in Proceedings of the AAAI Conference on Artiï¬cial Intelli-
gence, vol. 34, no. 04, 2020, pp. 6202â€“6209.

[22] H. Xu, R. Wang, L. Raizman, and Z. Rabinovich, â€œTransferable envi-
ronment poisoning: Training-time attack on reinforcement learning,â€
in Proceedings of the 20th International Conference on Autonomous
Agents and MultiAgent Systems, 2021, pp. 1398â€“1406.

[23] X. Zhang, Y. Ma, A. Singla, and X. Zhu, â€œAdaptive reward-poisoning
attacks against reinforcement learning,â€ in International Conference
on Machine Learning. PMLR, 2020, pp. 11 225â€“11 234.

[24] X. Wang, Y. Wang, D. Hsu, and Y. Wang, â€œExploration in inter-
active personalized music recommendation: a reinforcement learning
approach,â€ ACM Transactions on Multimedia Computing, Communi-
cations, and Applications (TOMM), vol. 11, no. 1, pp. 1â€“22, 2014.

and
Jurafsky, M. Galley,
[25] J. Li, W. Monroe, A. Ritter, D.
learning for dialogue generation,â€
J. Gao, â€œDeep reinforcement
in Proceedings of
the 2016 Conference on Empirical Methods
in Natural Language Processing. Austin, Texas: Association for
Computational Linguistics, Nov. 2016, pp. 1192â€“1202.
[Online].
Available: https://aclanthology.org/D16-1127

[26] H. Modares, I. Ranatunga, F. L. Lewis, and D. O. Popa, â€œOptimized
assistive humanâ€“robot interaction using reinforcement learning,â€ IEEE
transactions on cybernetics, vol. 46, no. 3, pp. 655â€“667, 2015.
[27] M. C. Priess, R. Conway, J. Choi, J. M. Popovich, and C. Radcliffe,
â€œSolutions to the inverse lqr problem with application to biological
systems analysis,â€ IEEE Transactions on control systems technology,
vol. 23, no. 2, pp. 770â€“777, 2014.

[28] H. Zhang, H. Chen, C. Xiao, B. Li, M. Liu, D. Boning, and C.-J. Hsieh,
â€œRobust deep reinforcement learning against adversarial perturbations
on state observations,â€ Advances in Neural Information Processing
Systems, vol. 33, pp. 21 024â€“21 037, 2020.

[29] G. Liu and L. Lai, â€œProvably efï¬cient black-box action poisoning at-
tacks against reinforcement learning,â€ Advances in Neural Information
Processing Systems, vol. 34, 2021.

[30] Y. Huang, L. Huang, and Q. Zhu, â€œReinforcement

learning for
feedback-enabled cyber resilience,â€ Annual Reviews in Control, 2022.
[31] D. P. Bertsekas, Dynamic programming: deterministic and stochastic

models. Prentice-Hall, Inc., 1987.

[32] R. E. Kalman, â€œWhen Is a Linear Control System Optimal?â€ Journal
of Basic Engineering, vol. 86, no. 1, pp. 51â€“60, 03 1964. [Online].
Available: https://doi.org/10.1115/1.3653115

[33] T. Fujii and M. Narazaki, â€œA complete optimality condition in the
inverse problem of optimal control,â€ SIAM journal on control and
optimization, vol. 22, no. 2, pp. 327â€“341, 1984.

[34] K. Sugimoto and Y. Yamamoto, â€œSolution to the inverse regulator
problem for discrete-time systems,â€ International Journal of Control,
vol. 48, no. 3, pp. 1285â€“1300, 1988.

[35] D. Iracleous and A. Alexandridis, â€œNew results to the inverse optimal
control problem for discrete-time linear systems,â€ Applied Mathemat-
ics and Computer Science, vol. 8, no. 3, pp. 517â€“528, 1998.

[36] D. Kleinman, â€œOn an iterative technique for riccati equation compu-
tations,â€ IEEE Transactions on Automatic Control, vol. 13, no. 1, pp.
114â€“115, 1968.

[37] S. Diamond and S. Boyd, â€œCvxpy: A python-embedded modeling
language for convex optimization,â€ The Journal of Machine Learning
Research, vol. 17, no. 1, pp. 2909â€“2913, 2016.

[38] H. J. H. J. Kushner, Introduction to stochastic control. New York:

Holt, Rinehart and Winston, 1971.

14

This figure "Illustration.png" is available in "png"(cid:10) format from:

http://arxiv.org/ps/2203.05774v2

