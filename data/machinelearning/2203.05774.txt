Reinforcement Learning for Linear Quadratic Control is Vulnerable
Under Cost Manipulation

Yunhan Huang1 and Quanyan Zhu1

timeliness, and consistency of the feedback information are
unlikely to be guaranteed in practice, especially in the
presence of adversarial interventions. Without consistent or
accurate feedback from the environment, the agent can either
fail to learn an implementable control policy or be tricked
into a ‘nefarious’ control policy favored by the adversaries.
In the last ﬁve years, there has been a surge in the number
of research studies that focus on the security threats faced
by RL with discrete state space [13], [16]–[23]. Very few,
if not none, of these studies have investigated the potential
security threats that can impede RL-enabled control systems.
In this paper, we take the initiative to look into the security
threats faced by RL-enabled control systems. In particular,
we are interested in the deception of a Linear-Quadratic-
Gaussian (LQG) agent by manipulating the cost signals. The
attacker falsiﬁes the cost signals received by the LQG agent
to trick the agent into learning some ‘nefarious’ policies
favored by the attacker. In RL-enabled systems, the agent
obtains cost signals from user feedback, as is illustrated in 1.
For example, in recommendation systems, the costs are often
related to the number of user clicks, purchases, or add-to-cart
rate [24]; Cost signals are represented by user sentiment or
engagement in RL-enabled dialogue generation [23], [25]; in
human-robot interaction, cost signals are generated according
to the level of dissatisfaction the human has while interacting
with the robot [26], [27]. Such occasions open a back
door for the attacker to inﬂuence the RL-enabled system
by manipulating the cost signals. Beyond that, in networked
control systems where the agent is remote to the plant, cost
signals can be falsiﬁed or jammed by the attacker when
transmitting from the plant to the agent [18], [19].

2
2
0
2

r
p
A
7

]

Y
S
.
s
s
e
e
[

2
v
4
7
7
5
0
.
3
0
2
2
:
v
i
X
r
a

Abstract— In this work, we study the deception of a Linear-
Quadratic-Gaussian (LQG) agent by manipulating the cost
signals. We show that a small falsiﬁcation of the cost parameters
will only lead to a bounded change in the optimal policy. The
bound is linear on the amount of falsiﬁcation the attacker
can apply to the cost parameters. We propose an attack
model where the attacker aims to mislead the agent into
learning a ‘nefarious’ policy by intentionally falsifying the cost
parameters. We formulate the attack’s problem as a convex
optimization problem and develop necessary and sufﬁcient
conditions to check the achievability of the attacker’s goal.

We showcase the adversarial manipulation on two types of
LQG learners: the batch RL learner and the other is the
adaptive dynamic programming (ADP) learner. Our results
demonstrate that with only 2.296% of falsiﬁcation on the cost
data, the attacker misleads the batch RL into learning the
’nefarious’ policy that leads the vehicle to a dangerous position.
The attacker can also gradually trick the ADP learner into
learning the same ‘nefarious’ policy by consistently feeding
the learner a falsiﬁed cost signal that stays close to the actual
cost signal. The paper aims to raise people’s awareness of the
security threats faced by RL-enabled control systems.

I. INTRODUCTION

The adoption of machine learning (ML), especially rein-
forcement learning (RL), in control theory and engineering
enables the agent
to learn a high-quality control policy
without knowing the model or the cost criteria [1]–[6].
The agent learns either by interacting with the environment
through communication channels [1], [3] or by processing
existing datasets from previous experience [2]. However, the
incorporation of learning into control enlarges the attack
surface of the underlying system and creates opportunities
for the adversaries. Adversarial parties can launch attacks
such as Denial-of-Service attacks (DoS) [7], [8], False Data
Injection (FDI) attacks [9], [10], spooﬁng attacks [11], [12]
on the communication channels, and data poisoning attacks
[13], [14] on the existing dataset to mislead the agent and
sabotage the underlying system. If not dealt with properly,
such attacks can lead to a catastrophe in the underlying
system. For example, self-driving platooning vehicles can
collide with each other when measurement data is manip-
ulated [15]. It is, hence, critical to study security threats to
learning algorithms and design effective defense mechanisms
to safeguard the learning-enabled control system.

The successful application of RL in control systems relies
on accurate, timely, and consistent feedback from the envi-
ronment or reliable datasets from experience. The accuracy,

1 Y. Huang and Q. Zhu are with the Department of Electrical and
Computer Engineering, New York University, 370 Jay St., Brooklyn, NY.
{yh.huang, qz494}@nyu.edu

Fig. 1: Reinforcement learning for control under adversarial
manipulation on cost signals.

1

 
 
 
 
 
 
We characterize the fundamental limits of cost manipu-
lation, i.e., how much the attacker needs to falsify the cost
signals to steer the learned control policy from an optimal
one to a ‘nefarious’ one favored by the attacker. Given a
‘nefarious’ control policy the attacker aims for, we formulate
the attack’s problem as an optimization problem, which is
proved to be convex. We show that the attacker cannot
deceive the agent into learning some policies no matter how
the attacker falsiﬁes the cost signals. Hence, the optimization
problem is infeasible for these policies. We develop a set of
conditions in the frequency domain under which the attacker
can mislead the agent into learning the ‘nefarious’ policy.

We showcase the deception of two types of LQG learners:
one is the batch RL learner [2] and the other is the adaptive
dynamic programming (ADP) learner [1]. The batch RL
learner ﬁrst estimates the system and cost parameters using
a pre-collected dataset. The dataset includes the data points
that record the state and control trajectories and correspond-
ing cost signals. Then the agent computes the learned policy
using the estimated system and cost parameters. Suppose the
attacker can falsify the cost signals in the dataset. We show
that by only falsifying 2% of the cost signals (in magnitude),
the attacker can trick the agent into a policy that steers a
vehicle into a dangerous position.

Another is the ADP learner [1]. The ADP learner updates
its estimates about the 𝑄-function adaptively by interact-
ing with the environment (plant) online. The ADP learner
receives cost signals and state measurements from the en-
vironment and applies controls to excite the environment.
Suppose the attacker can falsify the cost signals transmitted
through some communication channels. We show that the
attacker can craft attacks on cost signals by simply solving
a convex optimization problem. The crafted attacks falsify
the cost signals in a way such that the falsiﬁed cost signals
and the true cost signals remain close when the system
becomes stabilized. The experiment shows that the attacker
can gradually mislead the agent into learning a ‘nefarious’
policy favored by the attacker.

While most studies focus on improving the performance of
RL algorithms in control (e.g., convergence rate, robustness,
data efﬁciency, computational complexity), very few studies
pay attention to the security threats faced by RL algorithms
in control. The theories and examples presented in this
paper demonstrate the vulnerabilities of RL-enabled control
systems, which shows the necessity of investigating the
potential security challenges faced by RL in control.

Related works: There is a recent

trend in studying
security threats faced by RL algorithms [13], [16]–[23], [28],
[29].We can taxonomize these studies by the three types of
attacks targeting at three different signals: attacks on the
reward or cost signals [13], [18], [19], [21], [23], attacks
on the state sensing [15], [20], [22], attacks on the action
execution [29]. One can refer to Section 5 of [30] for a
brief review of this topic. Most of these studies focus on
RL with discrete state and action spaces, and very few
consider the security threats faced by RL-enabled control
systems. Ma et al. [13] studies data poisoning attacks on

2

the reward data on batch RL. Beyond the case of discrete
state space, the authors also demonstrate the effectiveness of
the attacks on an LQR learner using batch data. Our paper
goes beyond batch RL and focuses on the deception of LQG
agents through manipulating cost signals in a general setting.
Hence, the results in this paper can be extended to different
learning schemes and will not be limited to batch RL. In
addition to demonstrating the effectiveness of the attacks
through numerical experiments, we also develop theoretical
underpinnings to understand the fundamental limits of such
attacks, e.g., whether the attacker’s goal can be achieved or
not, how much falsiﬁcation is needed to achieve such goals.
Notation: Denote the set of non-negative real numbers by
R
. Let C be the complex plane. Let S𝑛 the set of all real
+
symmetric matrices of order 𝑛. Denote the set of all posi-
tive semi-deﬁnite (respectively, positive deﬁnite) symmetric
matrices by S
). Given 𝑀, 𝑁
𝑁
+
(respectively, 𝑀
𝑁 is positive semi-deﬁnite
(respectively, positive deﬁnite). We denote 𝐼𝑛 the identity
matrix of order 𝑛, and 𝐼 is the identity matrix whose order
depends on the context. Throughout the paper, prime denotes
the transpose.

(respectively, S
++
𝑁) means 𝑀
−

S, 𝑀

(cid:23)

≻

∈

For a matrix 𝑀
∈
vectorization of 𝑀:

S of order 𝑛, deﬁne Θ

as the half-

𝑀

)

(

∈

≔

Θ

𝑀

)

(

𝑚1,1,

· · ·

, 𝑚1,𝑛, 𝑚2,2,

, 𝑚2,𝑛,

, 𝑚𝑛
−

1

1,𝑛

−

′ .

· · ·

· · ·

Here, 𝑚𝑖, 𝑗 is the element in the 𝑖-th row and the 𝑗-th column
(cid:2)
R𝑛, deﬁne
of 𝑀. For a vector 𝑥

(cid:3)

¯𝑥 ≔

𝑥2
1, 2𝑥1𝑥2

, 2𝑥1𝑥𝑛, 𝑥2
2,

, 2𝑥2𝑥𝑛,

, 𝑥2
𝑛

′ .

𝑘

(

)

(

(cid:2)

k

𝜌

0.

· · ·

· · ·

k ≤

k/
𝑀

k · k

N
∈
𝜏

𝑀
(
𝜌

· · ·
The Frobenius norm of a matrix is denoted by

𝐹 . The
(cid:3)
k · k
refers to the Euclidean norm for vectors and the
norm
spectral norm for matrices unless speciﬁed otherwise. For a
𝑛, the spectral radius of 𝑀 is denoted
real matrix 𝑀
𝑀 𝑘
by 𝜌
𝑀
𝑀
as the
}
{k
)
𝑘 for all
𝑀, 𝜌
smallest value such that
)
𝑘

R𝑛
×
∈
. Deﬁne 𝜏

≔ sup𝑘
𝑀 𝑘

≥
Organization of the paper: In Section II, we introduce
the LQG problem with general quadratic cost and present
some preliminary results regarding the LQG problem. Sec-
tion III proposes the problem of adversarial manipulation
on the cost parameters, the fundamental limits of what the
attacker can or cannot achieve. Section III lays a theoretical
foundation for the attack models on two popular learning
methods: the Batch RL method and the ADP method, which
are introduced in IV and V. We demonstrate the results using
numerical examples in Section VI.

)
𝑀

))

(

(

(

II. LQG WITH GENERAL QUADRATIC COST:
PRELIMINARIES AND BACKGROUND

Consider the discrete-time, multi-variable system

𝑥𝑡 , 𝑢𝑡 , 𝑤𝑡

𝑥𝑡

1 = 𝑔
(
+
R𝑛, 𝑡 = 0, 1,

≔ 𝐴𝑥𝑡

𝐵𝑢𝑡

𝐶𝑤𝑡 ,

)
+
is the system state, 𝑢𝑡

+

(1)

∈
is the control input, 𝑤𝑡

where 𝑥𝑡
0, 1,
∈
the standard Gaussian distribution
𝐵

𝑚, and 𝐶

· · ·
R𝑛
×

R𝑛
×

· · ·

𝑞.

∈

∈

R𝑚, 𝑡 =
R𝑞 is drawn 𝑖.𝑖.𝑑. from
𝑛,
, 𝐴

0, 𝜎2𝐼𝑞

R𝑛
×

∈

N (

)

∈

Consider a stationary control policy 𝜋 : R𝑛

the form

R𝑚 taking

→

𝑢𝑡 = 𝜋

(

𝑥𝑡

= 𝐾𝑥𝑡

𝑘,

)
R𝑚. The stage-wise cost of the

+

(2)

where 𝐾
∈
system is quadratic

R𝑚
×

𝑛 and 𝑘

∈

𝑐𝑡 = 𝑐

𝑥, 𝑢

= 𝑥 ′𝐷𝑥

+
for some positive semi-deﬁnite 𝐷
R𝑛, and scalar 𝑟.
S

, vector 𝑑

(

)

++

∈
Assumption 1. Assume that
𝐴, 𝐷1
/

is observable.

2

(
Assumption 2. 𝐵 has full column rank.

)

𝑑 ′𝑥

𝑟

𝑢′𝐸𝑢,

+

+
, positive deﬁnite 𝐸

S
+

∈

(3)

∈

𝐴, 𝐵

)

(

is controllable and

Deﬁnition 1. We say 𝐾
𝐴

R𝑚
×
𝐵𝐾 is Schur stable, i.e., 𝜌

∈

𝑛 is stabilizing if the matrix
𝐴

< 1.

𝐵𝐾

)

+

+
(
We consider the total cost as the discounted accumulated
costs over an inﬁnite horizon. Starting at state 𝑥𝑡 under
control policy characterized by 𝐾 and 𝑘, the total cost is
∞𝑖=0 𝛾𝑖𝑐𝑡
𝑉𝐾 ,𝑘
1 is the dis-
, where 0
.
count factor and the expectation is over
· · · }
It is well known that if 𝐾 is stabilizing, 𝑉𝐾 ,𝑘 takes the

≤
𝑤𝑡 , 𝑡 = 0, 1, 2,
{

= E

𝑖
+

Í

𝑥𝑡

𝑥𝑡

≤

𝛾

)

]

[

(

|

form [31]:

𝑉𝐾 ,𝑘

𝑥𝑡

(

)

= 𝑥 ′𝑡 𝑃𝐾 𝑥𝑡

ℎ′𝐾 ,𝑘𝑥

+

+

𝑙𝐾 ,𝑘,

Note that the subscript of 𝑃𝐾 is 𝐾 instead of 𝐾, 𝑘 because
𝑃𝐾 only depends on 𝐾. Let 𝐾∗ and 𝑘 ∗ characterize the policy
which is optimal in a sense that the total discount cost of
every state is minimized. The value function is deﬁned by

𝑉 ∗

𝑥

(

S

= 𝑥 ′𝑃∗𝑥

≔ 𝑉𝐾 ∗,𝑘∗ (
)
, ℎ∗ ∈

𝑥
)
R𝑛, and 𝑙∗ ∈

where 𝑃
the optimal policy can be characterized by [31]

++

∈

ℎ∗′𝑥

𝑙∗,

+

+

R. It is well known that

𝑢∗𝑡

= 𝐾∗𝑥𝑡

𝑘 ∗,

+

𝐾∗ =
𝑘 ∗ =

𝛾

−

−(

(
𝛾

𝐸

+
2
/

) (

𝛾𝐵′𝑃∗𝐵
𝐸

−
)
𝛾𝐵′𝑃∗𝐵

+

1𝐵′𝑃∗ 𝐴,

1𝐵′ℎ∗,

−

)

𝛾 𝐴′𝑃∗ 𝐴
𝐴
𝛾

𝛾2 𝐴′𝑃∗𝐵
−
′ℎ∗,
𝐵𝐾∗
)
+
Σ𝑤𝐶 ′𝑃∗𝐶

(
𝛾
2 Tr

) −

(

𝐸

(

+

𝛾𝐵′𝑃∗𝐵

)

1𝐵′𝑃∗ 𝐴,

−

𝐸

(

+

𝛾𝐵′𝑃∗𝐵

1𝐵′ℎ∗

)−

.

𝛾2
2 ℎ∗′𝐵
𝛾
1

−

Lemma 1. Under Assumption 1,
uniquely decided by the system parameters
𝐷, 𝐸, 𝑑
cost parameters

.

the pair

(

(
𝐴, 𝐵

is
and the

𝐾∗, 𝑘 ∗)
)

(

)

The proof of Lemma 1 is presented in Appendix VII-B.
𝐾∗, 𝑘 ∗)
is uniquely determined by
Since the optimal policy
𝐴, 𝐵
𝐴, 𝐵
satisfying
, for a given system
(
Assumption 1, we can deﬁne the an auxiliary notation for
the solution

to the discrete-time LQG as

𝐷, 𝐸, 𝑑

𝐾, 𝑘

and

)

(

(

)

(

)

(

)
𝐾, 𝑘

(

= DLQG

(

)

𝐷, 𝐸, 𝑑, 𝐴, 𝐵

,

)

3

where

where

𝑃∗ = 𝐷
ℎ∗ = 𝑑

𝑟

𝑙∗ =

+

+

+

(4)

(5)

(6)

𝑛

S𝑚
where the mapping DLQG : S𝑛
→
R𝑚
R𝑚 is well-deﬁned and characterized by the relations
×
(4)-(6). Since we are particularly interested in the manipula-
𝐷, 𝐸, 𝑑
tion of the cost signals, we write
for simplicity.

= DLQG

R𝑛
×

R𝑛
×

𝐾, 𝑘

++ ×

+ ×

R𝑛

×

×

×

𝑚

)

(

(

)

𝑛

A. Q-Functions and Policy Improvement

Deﬁne the 𝑄-function for a stabilizing policy 𝐾, 𝑘 as

𝑔

𝑥, 𝑢, 𝑤

𝑥

(7)

𝑄𝐾 ,𝑘

𝑥, 𝑢

= 𝑐

𝑥, 𝑢

𝛾E𝑤

𝑉𝐾 ,𝑘
[

(

(

(

) +

)
𝑥, 𝑢

(
(
the value 𝑄𝐾 ,𝑘
is the immediate cost of taking control
𝑢 from state 𝑥 plus the expected cost-to-go starting at the
R is
next state 𝐴𝑥
deﬁned for all states 𝑥 and all admissible controls 𝑢. The
expression can be written as

𝐶𝑤. And 𝑄 𝐾 ,𝑘 : R𝑛

R𝑚

𝐵𝑢

→

))|

×

+

+

]

)

𝑄 𝐾 ,𝑘
=𝑥 ′𝐷𝑥

𝑥, 𝑢
)
𝑑 ′𝑥

(
+

+

=

𝑥 ′

𝑢′

1

,

(8)

𝑥
𝑢
1












+

𝑟

𝑢′𝐸𝑢

+
𝐻𝐾
𝐻𝐾
(
𝐻𝐾 ,𝑘

(

𝑥 𝑥

𝑢 𝑥

)

𝛾E𝑤
𝑉𝐾 ,𝑘
[
𝐻𝐾
𝐻𝐾
(
𝐻𝐾 ,𝑘

𝑢𝑢

𝑥𝑢

)

(

(

𝑔
(
𝐻𝐾 ,𝑘
𝐻𝐾 ,𝑘
𝐻𝐾 ,𝑘

(

𝑥1
)

𝑢1
)

𝑥, 𝑢, 𝑤

))]

(

)

(

)

(cid:2)

(cid:3)

=

𝑥 ′

)
1𝑢






𝐻𝐾 ,𝑘


)
1𝑥
(
𝑥
𝑢
1





where 𝐻𝐾 ,𝑘 is a symmetric positive deﬁnite matrix.














11
)

𝑢′

1

(cid:2)

(cid:3)

,

(

Given the policy 𝐾, 𝑘, we can compute the 𝑄-function
characterized by 𝐻𝐾 ,𝑘. We can ﬁnd an improved

,

·)

(·

𝑄𝐾 ,𝑘
policy ˜𝐾 and ˜𝑘 based on 𝑄 𝑘,𝑘
˜𝑘 = min
𝑢

˜𝐾𝑥

)
+
Solving the minimization problem yields:

(

,

, by solving

(·
·)
𝑄𝐾 ,𝑘

𝑥, 𝑢

.

˜𝐾 =
˜𝑘 =

1
𝐻−
𝐾
(
1
𝐻−
𝐾

(

−

−

𝑢𝑢

𝑢𝑢

)

)

𝑢 𝑥

,

)

𝐻𝐾

(
𝐻𝐾 ,𝑘

.

𝑢1
)

(

If the policy characterized by 𝐾 and 𝑘 is stabilizing, the
feedback policy characterized by ˜𝐾 and ˜𝑘 is per deﬁnite a
stabilizing policy — it has no higher cost than 𝐾 and 𝑘 [1],
[31]. A new 𝑄-function then be assigned to this improved
policy and the policy improvement procedure can be repeated
ad inﬁnitum. Algorithms following this policy improvement
is called policy iteration.

III. LQG WITH MANIPULATED COST PARAMETERS

)

}

, 𝑡

D

1
+

data

rely
, 𝑇

algorithms
= 1, 2,

RL learning
𝑥𝑡 , 𝑢𝑡 , 𝑐𝑡 , 𝑥𝑡

=
on
to solve the LQG
{(
· · ·
problem and ﬁnd a good control policy with certain
performance guarantee. For some model-based learning
algorithms, the agent ﬁrst estimates the system parameters
𝐷, 𝐸, 𝑑
𝐴, 𝐵
and then computes the ‘optimal’
(
ˆ𝐷, ˆ𝐸, ˆ𝑑
policy based on the estimates
:
)
)
ˆ𝐾, ˆ𝑘
. For RL learning algorithms based
(
on value iteration or policy iteration which will not conduct
system identiﬁcation, the attacker can rely on the mapping
DLQG to craft its attacking strategy and understand the
limitations of the attack.

)
ˆ𝐷, ˆ𝐸, ˆ𝑑
)

= DLQG

ˆ𝐴, ˆ𝐵

and

and

(

(

)

(

)

(

Hence, to understand the security threats of RL-based
LQG problems, it is essential to investigate some fundamen-
tal properties regarding the mapping DLQG:

(

, does there exist a trio

1) To trick the agent into the learning a ‘nefarious’ control
𝐷†, 𝐸 †, 𝑑†)
𝐴, 𝐵
)
(
=
𝐾†, 𝑘 †)

𝐾†, 𝑘 †)
policy
𝐾†, 𝑘 †)
such that
(
and cost parameters
𝐷†, 𝐸 †, 𝑑†)
?
DLQG

2) How much falsiﬁcation the attacker needs to make on
to trick the agent into learning

is optimal under system

𝐷†, 𝐸 †, 𝑑†)

𝐷, 𝐸, 𝑑

i.e.,

?

(

(

(

(

,

(

)

3) Will a small change in

𝐷, 𝐸, 𝑑

(
𝐾, 𝑘
𝐾, 𝑘
change in
)
DLQG Lipschitz continuous?

for

)

(

(

(

𝐾†, 𝑘 †)
cause a signiﬁcant
𝐷, 𝐸, 𝑑
? Is

)

= DLQG

(

)

(

S𝑚
++

, 𝐸 † ∈

𝐾†, 𝑘 †)

Since the goal of the attacker is to mislead the agent into
, we assume that 𝐷† ∈
learning a stabilizing policy
S𝑛
which result into a stabilizing policy. In the
++
following subsection, we answer these questions. We show
that the mapping DLQG is locally Lipchitz continuous and
derives an upper bound regarding how much falsiﬁcation
on
is needed to deceive the agent into learning
(
𝐾†, 𝑘 †)
(
A. Fundamental Limits

𝐾∗, 𝑘 ∗)

instead of

𝐷, 𝐸, 𝑑

(

)

.

)

Note that 𝐾 is uniquely determined by 𝐷 and 𝐸 given
and solving for 𝐾 involves solving the Riccati equa-
𝐴, 𝐵
(
tion (5). The following theorem presents a perturbation anal-
ysis on the Riccati equation (5) to see how small falsiﬁcation
𝐷, 𝐸
on
induces changes in the solution of the Riccati
equation 𝑃∗.

)

(

(

+

𝐴∗𝑐)

be the spectral radius of 𝐴∗𝑐

≔
Proposition 1. Let 𝜌
𝐵𝐾∗ under the true cost parameters. Let 𝐷∗ and 𝐸 ∗ be
𝐴
the true cost parameters and 𝐷† and 𝐸 † are the falsiﬁed cost
𝜖.
parameters. Suppose
Denote by 𝑃∗ (respectively, 𝑃†) the solution to the Riccati
equation (5) under the true cost parameters (respectively, the
falsiﬁed parameters). Then, we have

𝐷∗ k ≤

𝐸 ∗ k ≤

𝐷† −

𝐸 † −

𝜖 and

k

k

𝑃†

k
as long as

𝑃∗

−

k ≤

Γ1

k

𝐷†

−

𝐷∗

k +

Γ2

𝐸 †

k

−

𝐸 ∗

k

1

𝐸 ∗−
𝛾𝜌

𝑃∗

𝜏
k)
(
𝐴∗𝑐)
(
1
)
k +

− k
1
−
2

−

k

(k

𝐴∗𝑐)

2

2

·

2

𝐸 ∗−

2

−

k

min

1
𝛾2𝜏
(
2

𝐵

{
2

k

k

k

2

𝛾𝜌
−
𝐴∗𝑐) k

𝐴∗𝑐)
(
𝐴∗𝑐 kk

, 1

}

,

𝑆∗ k

1

4𝛾2 (

𝜖

≤

𝐴

·
where

k

(cid:16)

and

Applying binomial inverse theorem, we can write 𝐹
as

(

𝑋, 𝐷, 𝐸

)

𝐹

𝑋, 𝐷, 𝐸

= 𝑋

𝛾 𝐴′𝑋

𝐼

(

)

−

+

𝛾𝐵𝐸 −

1𝐵′ 𝑋

1

−

𝐴

𝐷.

−

(

(cid:17)

. Hence, 𝐹

(cid:16)
Let 𝑃∗ be the solution to the Riccati equation (5) under the
and 𝑃† be the solution to the
𝐷∗, 𝐸 ∗)
true cost parameters
(
same Riccati equation under the falsiﬁed cost parameters
= 𝐹
𝐷†, 𝐸 †)
𝑃∗, 𝐷∗, 𝐸 ∗)
(
1𝐵′, and
For simplicity, deﬁne 𝑆∗ ≔ 𝐵𝐸 ∗−
= 𝐴
𝐴∗𝑐
𝐵𝐾∗. Here, 𝐾∗ is the optimal feedback control gain
𝐷∗, 𝐸 ∗)
under the true cost parameters
. By inspection, for
(
𝛾𝑆∗(
𝑋
𝑃∗ +
any 𝑋 such that 𝐼
is invertible, we can write
)
= 𝐹
𝑋, 𝐷∗, 𝐸 ∗
𝑃∗
+
(
= 𝐹𝑋
,
𝑋
𝑋

𝑃†, 𝐷†, 𝐸 †)
(
1𝐵′, 𝑆† = 𝐵𝐸 †−

+
𝑋, 𝐷∗, 𝐸 ∗

𝑃∗, 𝐷∗, 𝐸 ∗

= 0.

) −

𝑃∗

𝐹

𝐹

+

+

)

)

(

(

(

) + H (

)

(9)

where

and

𝐹𝑋

𝑋

)

(

= 𝑋

−

𝛾 𝐴∗𝑐 ′𝑋 𝐴∗𝑐,

= 𝛾2 𝐴∗𝑐 𝑋

)

𝐼

𝑋

H (

𝛾𝑆∗
(
Denote by Δ𝑃 the difference between 𝑃∗ and 𝑃†, i.e., Δ𝑃 ≔
𝑃† −
, according to (9), the
equation

𝐷†, 𝐸 †)

𝐷∗, 𝐸 ∗)

𝑃∗. Given

(10)

and

𝑃∗

)]

𝑋

+

+

−

[

(

(

1𝑆∗ 𝑋 𝐴∗𝑐.

= 𝐹𝑋

𝑋

𝑋

(11)

𝐹

𝑃∗

𝑋, 𝐷∗, 𝐸 ∗

𝐹

𝑃∗

𝑋, 𝐷†, 𝐸 †

(

)

(

(

+

)−

+
)+H (
admits a unique symmetric solution 𝑋 such that 𝑃∗ +
𝑃∗ +
which also solves 𝐹
is 𝑋 = Δ𝑃. The eigenvalues of the operator 𝐹𝑋 : R𝑛
×
→
R𝑛
𝜆𝑖𝜆 𝑗 , where the eigenvalues 𝜆𝑖 of 𝐴∗𝑐
×
lies inside the unit circle in the complex plane. Hence 0 <
< 2, the operator 𝐹𝑋 is invertible. In view of (9) and
𝜇𝑖 𝑗
|
(11), we construct an operator

0,
= 0. Hence, the solution

𝑛 are 𝜇𝑖 𝑗 = 1

𝑋, 𝐷†, 𝐸 †)

)
𝑋

≥

−

(

𝑛

|

(

)

≔

𝑍

𝑍

𝐹

𝐹

Φ

−

)+

𝑃∗

1
𝐹−
𝑋 (H (

𝑍, 𝐷†, 𝐸 †

.
))
+
(
(12)
Next, we show that under certain conditions on Δ𝐷 ≔ 𝐷† −
𝐷∗ and Δ𝐸 ≔ 𝐸 † −
𝐸 ∗, there exists 𝜌 = 𝑓
for
R such that Φ is constractive and maps
some 𝑓 : R
→
the set

𝑍, 𝐷∗, 𝐸 ∗

Δ𝐷

Δ𝐸

𝑃∗

)−

(k

k)

R

×

+

k

k

(

,

Ω𝜌 =

𝑍 :

{

𝑍

k

k ≤

𝜌, 𝑍 = 𝑍 ′, 𝑃

𝑍

0

}

≥

+

into itself. In view of (12), we obtain

Γ1 = 4𝛾2

1

2

𝜏
𝐴∗𝑐)
(
𝛾𝜌
(

−

𝐴∗𝑐)

(cid:17)

2

Φ

𝑍

(

k

1
𝐹−
𝑋 k

) k ≤ k

(cid:16)

kH (
𝐹

−

(

𝑍

𝐹

𝑃∗

) k + k
𝑃∗

+
(
𝑍, 𝐷∗, 𝐸 ∗

+

𝑍, 𝐷†, 𝐸 †

)

(13)

.

) k

(cid:17)

,

2

(14)

From Lemma 3, we know

1
𝐹−
𝑋 k ≤

k

1

2

𝜏
𝐴∗𝑐)
(
𝛾𝜌
(

−

𝐴∗𝑐)

is the spectral radius of 𝐴𝑐 and 𝜏

where 𝜌
𝐴𝑐
as 𝜏

𝐴𝑐
)
≔ sup𝑘
𝐴𝑘
𝑐 k/
By Lemma 2 and (10),

{k

(
)

N

(

∈

𝐴𝑐

𝜌

(

)

𝑘

.

}

𝐴𝑐

)

(

is deﬁned

𝐴

2

k

(k

𝑃∗

k +

1

2 k

)

2 k

𝐷, 𝐸

)

, let 𝐹

(

1

𝐸 ∗−
1

− k
𝑋, 𝐷, 𝐸

)

2

2

.

𝐵
k
k
1
𝐸 ∗−

k
k
be the ma-

𝛾2 𝐴′𝑋 𝐵

𝛾𝐵′𝑋 ∗𝐵

𝐸

(

+

)

1𝐵′𝑋 𝐴

−

𝐷

−

𝑍

kH (

) k ≤

𝛾2

2

𝐴∗𝑐 k

k

k

𝑆∗

𝑍

k

kk

2.

(15)

4

2

𝜏
𝐴∗𝑐)
(
𝛾𝜌
(

Γ2 = 4𝛾2

1

𝐴∗𝑐)
Proof. Given parameters
trix expression

−

𝑋, 𝐷, 𝐸

𝐹

(

= 𝑋

−

)

𝛾 𝐴′𝑋 𝐴

(

+

A straight calculation yields

𝑍, 𝐷†, 𝐸 †

𝐹

(

−

=

𝑃∗

+
𝛾 𝐴′

(

𝑃†

𝑍

+

𝐹

𝑃∗

) −
𝐼

(
𝛾𝑆†

+
𝑃†

+

(

)

)

(cid:16)
𝐼

(cid:16)
𝐼

+

+
=𝛾

𝛾 𝐴′

𝑃†

𝑍

(
𝑃†

+
𝑍

+

) (

𝐴′
(
Δ𝐷.
(cid:2)
−

Then, by Lemma 2,

)
𝐴

𝑍, 𝐷∗, 𝐸 ∗

𝑍

)

+

1

−

1

(cid:17)
−

𝛾𝑆∗

𝑃†

+
𝛾𝑆∗

(
𝑃†

𝑍

+
𝑍

)
(cid:17)
𝛾Δ𝑆

(

+

))

𝐴

𝐼

(

−

+

Δ𝐷

𝛾𝑆†

𝑃∗

(

+

𝑍

))

1 𝐴

−

k

k

≤

k
Assume 𝜌 < 1. Since 𝑍
Note that Δ𝑆 = 𝑆† −
4, we derive that if

𝐹
k
𝛾2

𝑃†, 𝐷†, 𝐸 †
) −
(
2
𝑍
𝑃∗
𝐴

2

) k
.

k

𝐹
(
Δ𝑆

𝑃†, 𝐷∗, 𝐸 ∗
Δ𝐷
k
𝑃∗+
k
𝐸 ∗}−

+
k + k
k
Ω𝜌, we obtain
1𝐵′ −

𝐵

{

< 1,

Δ𝐸

.

k

k

𝐸 ∗−
1

2

𝐵
k
k
1
𝐸 ∗−

− k
13

(

) − (

2

k
k
16
)

∈
𝑆∗ = 𝐵𝐸 †−
1Δ𝐸
𝐸 ∗−

k

k
1

Δ𝑆

k

k ≤

k

Combining the results from

, we have, for 𝑍

Ω𝜌

∈

𝑍
𝑃∗ k+
1.
1𝐵′. By Lemma

k ≤ k

(cid:3)

(16)

Φ

𝑍

(

k

) k ≤

1

2

𝜏
𝐴∗𝑐)
(
𝛾𝜌
(

−

𝐴∗𝑐)

𝛾2

2

𝐴∗𝑐 k

k

k

𝑆∗

k

𝜌2

2

"

𝛾2

𝐴

k

+

2

k

(k

𝑃∗

k +

2 k

1

)

1

𝐸 ∗−
1

− k

2

𝐵
k
k
1
𝐸 ∗−

2

k
k

Δ𝐸

k

k + k

Φ

𝑍1

(

k

) −

Φ

𝑍2

(

) k

Δ𝐷

k#
(17)
for

Similarly, we derive a bound for
𝑍1, 𝑍2

Ω𝜌:

∈

2

𝜏
𝐴∗𝑐)
(
𝛾𝜌
(
𝐴∗𝑐 k

Φ

𝑍1

Φ

𝑍2

2𝛾2

2

2

𝐴

𝑃∗

2

1

Δ𝑆

(

(

k

k

k

1

−

(k

) −

k +

) k ≤

𝐴∗𝑐)
𝑆∗
k
(18)
Due to (17) and (18), the operator Φ is a contraction and
maps the compact set Ω𝜌 into itself if there exists 𝜌 > 0
such that

+ (k

𝑍1

𝑍2

k)

−

𝜌

k

k

k

i

h

)

2

k

𝜌

≥

1

2

𝜏
𝐴∗𝑐)
(
𝛾𝜌
(

−

𝐴∗𝑐)

𝛾2

2

𝐴∗𝑐 k

k

k

𝑆∗

k

𝜌2

2

"

𝛾2

𝐴

k

+

2

k

(k

𝑃∗

k +

2 k

1

)

1

𝐸 ∗−
1

− k

2

𝐵
k
k
1
𝐸 ∗−

2

k
k

Δ𝐸

k

k + k

Δ𝐷

,

k#

and

1 >

1

+ k
Choose

2

𝜏
𝐴∗𝑐)
(
𝛾𝜌
(
2
𝐴∗𝑐 k

2

𝐴∗𝑐)
𝑆∗
k

−

2𝛾2

𝜌.

k

i

𝐴

2

k

(k

𝑃∗

k +

2 k

1

)

k

h

1

𝐸 ∗−
1

− k

2

𝜏
𝐴∗𝑐)
(
𝛾𝜌
(

𝐴∗𝑐)

𝐴

2

k

(k

𝑃∗

k +

2 k

1

)

2

k

(cid:16)

1

𝐸 ∗−
1

− k

2

𝐵
k
k
1
𝐸 ∗−

2

k
k

Δ𝐸

k

k

2

𝐵
k
k
1
𝐸 ∗−

2

k
k

Δ𝐸

k

k

min

, the operator Φ is a contrac-
, 1
If 𝜌
}
tion and maps the compact set Ω𝜌 into itself. Then, Φ admits

𝑆∗ k

(
) k

)
k k

≤

{

(

𝛾𝜌

1
−
𝐴∗𝑐

𝐴∗𝑐
𝐴∗𝑐

2

𝜌 =4𝛾2

+ k

1
−
Δ𝐷

.

k

(cid:17)
𝛾2 𝜏

a unique ﬁxed-point solution in Ω𝜌. Therefore, Δ𝑃
Δ𝑃

Ω𝜌 and

∈

k

k ≤

𝑃†

k

−

𝑃∗

𝜌. Hence, we have
𝜏
𝐴∗𝑐)
(
𝐴∗𝑐)
𝛾𝜌
(
𝑃∗

4𝛾2

k ≤

−
2

𝐴

1

2

+ k

k

(k

k +

Δ𝐷

2

k

(cid:16)
1

)

2 k

1

k
𝐸 ∗−
1

− k

2

𝐵
k
k
1
𝐸 ∗−

2

k
k

Δ𝐸

k

.

k

(cid:17)

(cid:3)

k

𝐷∗ k ≤
𝐷† −
𝐾∗
𝐾†

−

R
+
𝐸 ∗)/

+ →
𝜆min
(
Γ4𝜖,

≤
𝜖

(
𝐵

) +
2

Γ2

Proposition 1 presents some preliminary results showing
the change in the solution of Riccati equation is bounded,
Γ1
𝑃∗ −
𝐸 † −
𝑃∗ k ≤
𝐷† −
𝐸 ∗k
𝐷∗k +
i.e.,
for small
k
k
𝐸 ∗ k
𝐸 † −
𝐷∗k
𝐷† −
. Our ultimate goal is to see how
and
k
k
𝐷, 𝐸
leads to changes in 𝐾.
small fasiﬁcation on
(
Theorem 1. Suppose
𝑃∗ k ≤
𝑃† −
𝐸 ∗ k ≤
𝑓
, where 𝑃∗(respectively, 𝑃†) is the solution to the Riccati
equation (5) under the true cost parameters (respectively, the
falsiﬁed cost parameters.), and 𝑓 : R
is some function
on 𝜖 given
2, then

)
𝐸 † −

𝜖 and

𝜖. If 𝜖

k

k

k

𝜖

)

(

Γ3 𝑓

k ≤
𝐴

{k

(19)

and Γ4 ≔

k
where Γ3 ≔ 2𝛾
𝜆min (
𝐾∗ k
.

max

𝐸 ∗)

,

k

k

k}

(k

𝐾∗ k +

1

)

𝐸 ∗) k

2𝛾
𝜆min (
Proof. Note that the optimal 𝑄-function of the LQG problem
takes the form
= 1
2

𝑢′𝐸𝑢

𝑥, 𝑢

𝑑 ′𝑥

𝑄

𝑟

+

+

)

(

𝑥 ′𝐷𝑥
𝛾E𝑤

+

𝐴𝑥

[(

+

1
2
𝐶𝑤

+

+
𝐵𝑢

𝐴𝑥

𝑃

(

)

+

𝐵𝑢

𝐶𝑤

,

)]

+

where 𝑃 the solution of the Riccati equation (5) given 𝐴,
𝐵, 𝐷, 𝐸. We know that the arg min𝑢 𝑄
takes the form
𝑘 for every 𝑥. Indeed, 𝐾𝑥 is also the optimal point
𝐾𝑥
2 𝑢′𝐸𝑢
of the function 𝑓
.
)
Applying Lemma 7 and letting 𝐸1 = 𝐸 ∗, 𝑃1 = 𝑃∗, 𝐸2 = 𝐸 †,
(cid:3)
and 𝑃2 = 𝑃†, we have (19).

= 1

𝑥, 𝑢

𝑥, 𝑢

𝐵𝑢

𝐵𝑢

𝐴𝑥

𝐴𝑥

𝑃

+

+

+

+

𝛾

(

(

)

)

)

(

(

From Proposition 1 and Theorem 1, we know that the
change of the policy 𝐾 is bounded by the falsiﬁcation in
𝐷, 𝐸

. Indeed,

(

)
𝐾†

k

−
for small
bound on

𝐾∗

k ≤
𝐷† −
k
𝑘 † −
k

𝐷†

k
and

Γ3Γ1

𝐷∗ k
𝑘 ∗ k
.

𝐷∗

−
𝐸 † −

k + (
𝐸 ∗ k

k

Γ3Γ2

Γ4

+

𝐸 †

𝐸 ∗

,

k

−

) k

. Now, let’s discuss the

Proposition 2. Let ℎ∗ (respectively, ℎ†) be the solution of
(respec-
(4)-(6) under the true cost parameters
tively, the falsiﬁed cost parameters
). Suppose
𝐾† −
𝐵
k−

𝐷∗, 𝐸 ∗, 𝑑∗)
(
𝐷†, 𝐸 †, 𝑑†)

(
. We have

k

𝐼

1

𝐾∗ k ≤
ℎ†

1
2 k
ℎ∗

𝛾 𝐴∗𝑐 k
𝑑∗
𝑑†

k
−
Γ5

Γ6

𝐾†

𝐾∗

,

k +

k

−
and Γ6 = 2𝛾

k

𝐼

k(

−

k

−

k ≤
𝐼

k
where Γ5 = 2
𝛾 𝐴∗𝑐)−
.
k
Proof. From (6), we know that

−
𝛾 𝐴∗𝑐)−

𝑑∗ kk

kk

k(

−

𝐵

1

1

k

ℎ∗ = 𝑑∗

𝐴

𝛾

(

+

+

𝐵𝐾∗

)

′ℎ∗ and ℎ† = 𝑑†

𝐴

𝛾

(

+

+

𝐵𝐾†

′ℎ†.

)

5

= 𝐴
Let 𝐴∗𝑐
know that 𝐼
have

+
−

𝐵𝐾∗ and 𝐴†𝑐 = 𝐴
𝛾 𝐴∗𝑐 and 𝐼

𝐵𝐾†. From Lemma 5, we
𝛾 𝐴†𝑐 are invertible. Hence, we

+

−

ℎ∗ =

𝛾 𝐴∗𝑐)
−
Applying Lemma 8 proves the Theorem.

1𝑑∗ and ℎ† =

−

−

(

𝐼

(

𝐼

1𝑑†

−

𝛾 𝐴†𝑐)

(cid:3)

(

(

𝜆min

). Suppose

𝐷∗, 𝐸 ∗, 𝑑∗)

(
𝐷†, 𝐸 †, 𝑑†)

Theorem 2. Let 𝑘 ∗ (respectively, 𝑘 †) be the solution of (4)-
(respectively,
(6) under the true cost parameters
𝐸 † −
the falsiﬁed cost parameters
𝐸 ∗)/
𝐸 ∗ k ≤
2, we have
Γ7
𝑘 †
𝑘 ∗
k ≤
−
k
where Γ7 =
4
𝜆min (
𝐵

Γ8
𝐸 ∗
−
k +
, Γ8 =
𝑘 ∗ k

ℎ∗
ℎ†
k
−
2, and Γ9 =

4𝛾
𝜆min (
Proof. Note that From (4), we know that 𝑘 =
𝛾𝐵′𝑃𝐵
)−
problem

+
1𝐵′ℎ is the solution of the following minimization

𝑃∗
k +
𝑘 ∗ kk

𝑃†
k
4𝛾
𝜆min (

k
𝐸 ∗) k

−
𝐸 ∗) k

𝐸 ∗) k

Γ9

𝐸 †

−

𝐵

𝐸

𝛾

k

k

k

k

(

.

,

≔ 𝑘 ′

𝐸

𝛾𝐵′𝑃𝐵

𝑘

𝛾ℎ′𝐵𝑘,

)

(
(
𝛾𝐵′𝑃𝐵 is positive deﬁnite. Let

+

+

)

𝑓

𝑘; 𝐸, 𝑃, ℎ

min
R𝑚
𝑘
∈
where 𝐸
𝑓
𝑘
(
)
be the solution of min𝑘 𝑓 ∗(
𝑘
𝑓 ∗
k∇

𝑘; 𝐸 ∗, 𝑃∗, ℎ∗)

and 𝑓 † (

) − ∇

) k ≤

𝑓 †

+

2

𝑘

𝑘

k

(

(

= 𝑓
𝑘

)

k(k
𝐵

𝛾

+

k

kk

From Lemma 6, we know that if

)

𝑘

=
𝑓 ∗(
. Let 𝑘 ∗ and 𝑘 †
𝑘; 𝐸 †, 𝑃†, ℎ†)
(
𝑘
and min𝑘 𝑓 †(
respectively.
2
𝐸 ∗
𝛾
𝐸 †
k +
k
.
ℎ∗
−
k
𝐸 ∗ k ≤
𝐸 † −

𝐸 ∗)/
2,

−
ℎ†

𝜆𝑚𝑖𝑛

)
𝐵

𝑃†

𝑃∗

k)

−

k

k

k

(

𝑘 †

k

−

𝑘 ∗

k ≤

≤

2
𝜆min
(
2
𝜆min

(
2𝛾

+

𝐸 ∗)
𝐸 ∗) (cid:16)
𝑘 ∗
kk
k

𝑓 †

𝑘 ∗

(

) k

k∇

2

𝑘 ∗

k

kk

𝐸 †

−

𝐸 ∗

k

𝐵

2

k

k

𝑃†

−

𝑃∗

2𝛾

𝐵

k

kk

ℎ†

−

ℎ∗

k

k +

Combining the results from Proposition 1, Theorem 1,

Proposition 2, and Theorem 2, we obtain

k

𝐾†
𝑘 †

k

𝐾∗
𝑘 ∗

−

−

k ≤

k ≤

𝐸 ∗
𝐸 ∗

,

k

k

−

−

Γ3Γ1
Γ1Γ8

k

𝐷†
𝐷†

k

k
Γ5Γ9
+
Γ1Γ8
Γ7
+
Γ5Γ9

+ [

+

−

k + (

k + (

𝐷∗
𝐷∗
−
𝑑†
𝑑∗
k +
−
Γ1Γ3Γ6Γ9
) k
Γ4
Γ2Γ8
,
𝑑†

+ (
𝑑∗

Γ3Γ2
Γ2Γ8
Γ6Γ9
𝐷†

+

Γ4
Γ7
+
𝐾†
𝐷∗

k

) k

) k

𝐸 †
𝐸 †
𝐾∗

−

k

−
Γ2Γ3

k
Γ6Γ9

+

)

≤ (

𝐸 †

𝐸 ∗

k

−

] k

,

k

k

, and

𝑑† −

+
−
k
k
(20)
𝐸 ∗ k
𝐷† −
𝐷∗k
𝑑∗k
𝐸 † −
for small
and for every
k
R𝑛. The bound above indicates
S𝑚
S𝑛
𝐷∗, 𝐸 ∗, 𝑑∗) ∈
++ ×
+ ×
(
that the mapping DLQG that solves the discrete-time LQG
problem is locally Lipschitz over the set S𝑛
R𝑛.
Hence, the continuity of the mapping DLQG holds and a
tiny falsiﬁcation on cost parameters by the attacker will only
cause a bounded change in the computed policy. If the at-
tacker aims to mislead the agent choosing into the ‘nefarious’
𝐾∗, 𝑘 ∗)
policy
,
(20) gives an upper bound on how much falsiﬁcation the
attacker needs on the cost parameters.

from the original optimal policy

𝐾†, 𝑘 †)

++ ×

+ +

×

S

𝑚

(

(

B. Attacks on Cost Function Parameters

Suppose the attacker wants to deceive the agent

into
learning the policy characterized by 𝐾† and 𝑘 † by altering
the cost parameters. Results in Section III-A give a rough
bound on the region around the original cost parameters
𝐷, 𝐸, 𝑑
𝐷†, 𝐸 †, 𝑑†)
that
(
. To minimize the
leads to the ‘nefarious’ policy
cost of attacking, the attacker attempts to deviate the cost
parameters from the original ones as small as possible:

in which there might exist some
𝐾†, 𝑘 †)

(

(

)

˜𝐷

𝐷

𝐹

˜𝑑

𝑑

2

˜𝐸

min
˜𝐷, ˜𝑑, ˜𝐸 , 𝑃,ℎ

−
k
𝑠.𝑡. 𝑃 = ˜𝐷

−

k
+ k
𝛾 𝐴′𝑃𝐴

𝐸

𝐹 ,

k

−
𝛾𝐵′𝑃𝐵

+
𝛾𝐵′𝑃𝐵
𝐴
𝛾

)

+
+
(
𝛾𝐵′𝑃𝐵
˜𝐷

−
𝐾† =
𝐵𝐾†
𝑘 † =
)
˜𝐸
0,

˜𝐸
(
+
ℎ = ˜𝑑
˜𝐸
2
(
𝑃

+
0,

(cid:23)

≻

(

k
𝐾† ′

+ k
˜𝐸
+
𝛾𝐵′𝑃𝐴,
−
′ℎ,
𝛾𝐵′ℎ,
0.

−

)

≻

𝐾†,

)

(21)

Proposition 3. The optimization deﬁned in (21) is convex.

(

𝐷1, 𝑑1, 𝐸1, 𝑃1, ℎ1

Proof. It easy to see that the objective function of (21) is
𝐷2, 𝑑2, 𝐸2, 𝑃2, ℎ2
convex. Suppose
)
satisfy the constraints in (21). We need to show that any
also satisfy the constraints in
1,
0
≤
≤
˜𝑑 = 𝛽𝑑1
(21), where ˜𝐷 = 𝛽𝐷1
𝑑2,
1
+ (
𝑃2, and ˜ℎ = 𝛽ℎ1. We
˜𝐸 = 𝛽𝐸1
𝛽
1
know that

˜𝐷, ˜𝑑, ˜𝐸, ˜𝑃, ˜ℎ
)
1

−
+ (
𝐸2, ˜𝑃 = 𝛽𝑃1

𝐷2,
1

)
+ (

and

+ (

−

−

−

𝛽

𝛽

𝛽

𝛽

)

)

)

(

(

)

𝑃1 = 𝐷1
𝑃2 = 𝐷2

𝛾 𝐴′𝑃1 𝐴
𝛾 𝐴′𝑃2 𝐴

𝐾† ′

𝐾† ′

𝐸1
𝐸2

(

(

−

−

+

+

𝛾𝐵′𝑃1𝐵
𝛾𝐵′𝑃2𝐵

𝐾†,

𝐾†.

)

)

+

+

Multiplying both sides of the ﬁrst equality by 𝛽 and both
sides of the second equality by

yields

𝛽

1

(

−

)

.

(cid:3)

(cid:17)

𝛽𝑃1
=𝛽𝐷1

+ (
+ (

1
1

−
−
𝛽𝐸1

𝛽
)
𝛽
)

𝑃2
𝐷2
1

𝐾† ′

−

)
This equality shows that

+ (

−

[

𝛾 𝐴′
[
𝐸2

+
𝛽

𝛽𝑃1

1

+ (

−
𝛽𝑃1

𝛾𝐵′

+

[

𝛽

)

+ (

𝑃2
1

]

−

𝐴

𝛽

)

𝑃2

𝐵

]

]

𝐾†.

˜𝑃 = ˜𝐷

𝛾 𝐴′ ˜𝑃𝐴

𝐾† ′

˜𝐸

(

+

−

𝛾𝐵′ ˜𝑃𝐵

𝐾†.

)

+

Similarly, we can show the rest of the constraints also form
(cid:3)
a convex set.

(

(

(

𝐴, 𝐵

such that the constraints in

Since the objective function of (21) is strictly convex, there
always exist a unique solution to the optimization problem
𝐾†, 𝑘 †)
(21) if the constraints in (21) are feasible for a given
.
The feasibility of problem (21) is not always guaranteed. An
, under what conditions
interesting question is given
𝐾†, 𝑘 †)
are feasible.
21
on
(
)
𝐾†, 𝑘 †)
whether
This question is equivalent to asking given
˜𝐷, ˜𝐸, ˜𝑑
= DLQG
𝐾†, 𝑘 †)
there exists
.
such that
)
(
If 𝐾† is not stabilizing, 𝐾† cannot be optimal for any 𝐷
0,
0, and 𝑑†. Note that the goal of the attacker is not
𝐸
to unstablize the system but to mislead the agent into a
with 𝐾† being stabilizing. But even
𝐾†, 𝑘 †)
nefarious policy
if 𝐾† is stabilizing, there might not exist 𝐷
0
such that 𝐾† is optimal. Hence, our discussion will focus on

˜𝐷, ˜𝐸, ˜𝑑
)

0 and 𝐸

≻

≻

≻

(cid:23)

(

(

(

)

(

6

(cid:23)

≻

under what conditions, a stabilizing
some 𝐷

0, and 𝑑.

0, 𝐸

𝐾†, 𝑘 †)

(

is optimal for

The same question was ﬁrstly raised by Kalman in the ﬁeld
of inverse optimal control [32]. He has showed that for the
single-input case, the circle criterion (see Theorem 6 of [32])
is a necessary and sufﬁcient condition for a control policy to
be optimal in a continuous-time system. Fujii and Narazaki
have given a complete solution for the continuous-time multi-
input case under the assumption that the cost parameters on
the control input 𝐸 is ﬁxed [33]. Sugimoto and Yamamoto
have showed the sufﬁcient and necessary conditions for 𝐾†
to be optimal for some 𝐷 and 𝐸 under the stage cost 𝑥 ′𝑡 𝐷𝑥𝑡
+
𝑢′𝑡 𝐸𝑢𝑡 for a discrete-time system [34]. We consider a more
general cost function than [34] did. Hence, we extend the
under which
results in [34] to give the conditions on
the constraints in (21) are feasible.

𝐾†, 𝑘 †)

(

In general, conditions of optimality can be expressed
most conveniently using frequency-domain formulas [32].
For system
, consider the following right coprime
factorization by two polynomial matrices 𝑀

and 𝑁

𝐴, 𝐵

𝑧

𝑧

(

)

:

√𝛾

𝑧𝐼

(

𝐴

)

−

1𝐵𝐸 −

1
/

2 = 𝑀

−

𝑁

𝑧

)

(

(

(

)

)
1.

−

(
𝑧

)

The feedback system √𝛾
factorization

(

𝐴

+

𝐵𝐾

)

induces a right coprime

√𝛾

𝑧𝐼

(
= 𝑁

−
𝑧

1𝐵𝐸 −

−

𝐴𝑐

)

1
/

2 = 𝑀

(
. Deﬁne

𝑧

)
(
≔ 𝐼𝑚

𝐾 𝑀

𝑧

) −

(
𝐾

𝑧𝐼𝑛

(

−

+

)

(
𝐴

𝑁𝑐

𝑧

)

𝑧

)

(

1,

−

1𝐵𝐸 −

1
/

2 = 𝑁𝑐

𝑁

𝑧

)

(

𝑧

)

(

1.

−

−

)

where 𝑁𝑐

𝑊

𝑧

)

(

Deﬁne

Ψ

𝑧

≔ 𝑁 ′𝑐 (

1

𝑧−

𝑊

0

𝑁𝑐

𝑧

𝑁 ′

1

𝑧−

𝑁

𝑧

.

)

(

(

(

)

(

)

) −

)
The following theorem states the conditions under which
𝐾, 𝑘

is optimal for some 𝐷

(
Theorem 3. Let
be controllable and suppose that 𝐴
(
is invertible and 𝐵 has rank 𝑚. Let the stabilizing control
𝑘 be given. Transform Ψ
policy 𝑢 = 𝐾𝑥

into the form

0, and 𝑑.

0, 𝐸

𝐴, 𝐵

(cid:23)

≻

𝑧

)

)

(

)

+

Ψ

𝑈

𝑧

)

(

𝑧

)

(

=

˜Ψ

𝑧

)

(
𝑧

(

)

0

(cid:2)
by some unimodular matrix 𝑈
is a
rational function matrix with full rank. Then, the constraints
in (21) is feasible under the given control policy
if
and only if the conditions

and, where ˜Ψ

𝐾, 𝑘

𝑧

)

(

)

(

)

(

(cid:3)

is positive deﬁnite,
is positive semi-deﬁnite for all 𝑧

1) 𝑊
2) Ψ
𝑧

0
)
(
𝑧
(
)
= 1,
|

|

3) and there exists no pair of 𝜆
𝜆
(
𝑧

, 𝑣𝑚
𝑣1,
(
𝑣𝑚 = 0, where U+ ≔

) ′ such that 𝑁
(
𝑧
∈

𝜆
𝑈
)
C :

· · ·

)
| ≥

{

|

1

,

}

hold for some 𝐸

0.

≻

C such that

∈
U+ and 𝑣 =
=

∈

𝑣 = 0 and 𝑣1 =

· · ·

Proof. Note that whether 𝐾 is optimal depends only on 𝐷,
𝐸, 𝐴, and 𝐵. From (4) and (5), we know that 𝐾 is optimal
to 𝐷, 𝐸, 𝐴, and 𝐵 if

𝐾 =

𝐸

𝛾

(

−

+

𝛾𝐵′𝑃∗𝐵

)

1𝐵′𝑃∗ 𝐴,

−

7

where 𝑃∗ solves the Riccati equation
5
. By Theorem
)
𝐴, 𝐵
5.1 of [34], we know that given
controllable and
𝐴 invertible, for a given stabilizing 𝐾, there always exist
𝐷, 𝑃

0 such that

0 and 𝐸

(
)

(

(cid:23)
𝑃 = 𝐷
𝐸

≻
𝛾 𝐴′𝑃𝐴
−
𝐾 =

+
𝛾𝐵′𝑃𝐵

(

+

)

−

𝛾2 𝐴′𝑃𝐵
𝛾𝐵′𝑃𝐴

𝐸

(

+

𝛾𝐵′𝑃𝐵

)

1𝐵′𝑃𝐴,

−

if and only if conditions 1), 2), and 3) are satisﬁed. For 𝑘,
we have constraints
𝑘 =
ℎ =

𝛾𝐵′𝑃𝐵
1𝑑.

1𝐵′ℎ,

−
𝐼

+
𝛾 𝐴𝑐

𝐸

𝛾

−

−

)

(

(

−

)

Note that 𝐵 has full rank (rank 𝑚) and 𝐸
𝛾 𝐴𝑐
is invertible. Hence, for any 𝑘, as long as conditions 1), 2),
3) are satisﬁed for a given 𝐾, we can always ﬁnd 𝑑 such
(cid:3)
that 𝑘 is optimal. We, hence, completes the proof.

𝛾𝐵′𝑃𝐵 and 𝐼

−

+

Δ𝑡
0
∫

Remark 1. The assumption that 𝐵 has full column rank
is reasonable. The assumption indicates that there is no
redundant control inputs. As for the assumption that 𝐴 is
invertible, consider a discrete-time system sampled from a
ˆ𝐵𝑢 with a small sample
continuous-time system
period Δ𝑡. For the discretized linear system, we have 𝐴 =
𝑒 ˆ𝐴𝜏 ˆ𝐵𝑑𝜏. Apparently, 𝐴 = 𝑒 ˆ𝐴Δ𝑡 is invertible.
𝑒 ˆ𝐴Δ𝑡 , 𝐵 =

𝑥 = ˆ𝐴𝑥
¤

+

Remark 2. The conditions in Theorem 3 provide a quick
way to check the feasibility of the optimization problem (21)
before solving it. If the optimization problem (21) is not
feasible, then the attacker cannot trick the agent into learning
the ‘nefarious’ policy
no matter how the attacker
falsiﬁes the cost parameters. The feasible set of (21) cannot
be singleton. Once there exists 𝐷
0, and 𝑑 such
𝐾†, 𝑘 †)
is optimal, the same control
that the control policy
policy is optimal for 𝛼𝐷, 𝛼𝐸, and 𝛼𝑑 for any 𝛼 > 0.

𝐾†, 𝑘 †)

0, 𝐸

(cid:23)

≻

(

(

Remark 3. The conditions in Theorem 3 are stated in the
frequency domain. Iracleous and Alexandridis have devel-
oped a set of necessary and sufﬁcient conditions based on the
state space representation in the time-domain, under which
the control policy 𝑢 = 𝐾𝑥 is optimal for some 𝐷
0,
𝐸
𝐴, 𝐵
[35] with stage cost
≻
𝑥 ′𝑡 𝐷𝑥𝑡
𝑢′𝑡 𝐸𝑢𝑡 . One can extend their results to check the
feasibility of (21) using conditions in the time-domain.

0 under the linear system

(cid:23)

+

)

(

IV. DECEIVING BATCH RL LEARNER

A. The Batch RL Learner

Consider a model-based LQG learner who learns a model
from a training dataset batch ﬁrst and then plans over
the estimated model. The LQG learner implements system
identiﬁcation from a dataset and then computes the optimal
controller based on the identiﬁed model [2]. Consider the
𝑥𝑡 , 𝑢𝑡 , 𝑐𝑡 , 𝑥𝑡
training dataset
1
}
generated from past experience of interacting with the LQR
system. The learner implements system identiﬁcation by
solving the following least-square problems:

: 𝑡 = 0, 1,

· · ·

1
+

, 𝑇

D

{(

−

=

)

ˆ𝐴, ˆ𝐵

(

arg min
𝐴,𝐵

) ∈

1
2 k

𝑇

1

−

Õ𝑡=0

𝐴𝑥𝑡

𝐵𝑢𝑡

+

𝑥𝑡

1
+

k

−

2
2 ,

(22)

arg min
0,𝐸

𝜖 𝐼 ,𝑑,𝑟

(cid:23)

≻

ˆ𝐷, ˆ𝐸, ˆ𝑑, ˆ𝑟

(

) ∈

𝐷

Deﬁne

𝑇

1

−

𝑥 ′𝑡 𝐷𝑥𝑡
Õ𝑡=0 (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

𝑑 ′𝑥𝑡

𝑟

+

+

𝑢′𝑡 𝐸𝑢𝑡

+

𝑐𝑡

−

.

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

𝑥 ′1
𝑥 ′2
...
𝑥 ′𝑇

𝑧′0
𝑧′1
...
𝑧′𝑇

,

𝑋𝑇 ≔

, 𝑍𝑇 ≔









where 𝑧𝑡 ≔
𝑢′𝑡
′. The least-square estimator for

is (assuming the invertibility of 𝑍 ′𝑍)































𝑥 ′𝑡

−

1

(cid:2)

(cid:3)

𝐴, 𝐵

)

(

Note that the optimization problem (23) is convex. We can
write 𝑥 ′𝑡 𝐷𝑥𝑡 as 𝑥𝑡 ′Θ

and 𝑢′𝑡 𝐸𝑢𝑡 as 𝑢𝑡 ′Θ

. Deﬁne

𝐷

𝐸

ˆ𝐴

ˆ𝐵

′ =

𝑍 ′𝑇 𝑍𝑇

(

)

1𝑍𝑇 𝑋𝑇 .

−

(cid:2)

(cid:3)

𝐻 ≔

𝑢0′
𝑢1′
...

𝑥 ′0
𝑥 ′1
...
𝑥 ′𝑇

−

1

1′

−

𝑢𝑇

1′

−

(

)

𝑥0 ′
𝑥1 ′
...




𝑥𝑇







(

)

.

1
1
...



1







The optimization problem
admits a unique solution
23
)
(
if the objective function is strictly convex, i.e., 𝐻 ′𝐻 is
positive deﬁnite, or equivalently 𝐻 is column independent.
With the model identiﬁed, the learner computes the control
policy based on (4), (5), and (6) using the estimated system
ˆ𝐷, ˆ𝐸, ˆ𝑑, ˆ𝑟
parameters

ˆ𝐴, ˆ𝐵

and

.

(

)

(

)

attacker needs to solve

(23)

min
˜𝐷, ˜𝐸 , ˜𝑑, ˜𝑟 , 𝑃,ℎ

𝑇

1

−

Õ𝑡=0

c†

k

c

2

−
k
𝛾 ˆ𝐴′𝑃 ˆ𝐴

𝑠.𝑡. 𝑃 = ˜𝐷

𝛾 ˆ𝐵′𝑃 ˆ𝐵

)

+
𝛾 ˆ𝐵′𝑃 ˆ𝐵
ˆ𝐴
𝛾
+
+
(
𝛾 ˆ𝐵′𝑃 ˆ𝐵
˜𝐷
˜𝐷𝑥𝑡

≥

𝐾† ′

˜𝐸
(
+
−
𝛾 ˆ𝐵′𝑃 ˆ𝐴,
𝐾† =
−
ˆ𝐵𝐾†
′ℎ,
𝛾 ˆ𝐵′ℎ,
𝑘 † =
)
0,
˜𝑑 ′𝑥𝑡

−
˜𝐸 > 0.

𝑟

𝑢′𝑡

)

˜𝐸
+
(
ℎ = ˜𝑑
˜𝐸
2
(
𝑃

+
0,
≥
= 𝑥 ′𝑡

𝑐†𝑡

+

+

+

˜𝐸𝑢𝑡 ,

𝑡
∀

𝐾†,

)

(24)

(25)

(26)

(27)

(28)

(29)

(30)

)

(

𝐴, 𝐵

ˆ𝐴, ˆ𝐵

replaced by

The attacker’s problem deﬁned in (24)-(29) is convex,
whose feasibility is aligned with the feasibility of problem
. The solution of the
21 with
problem can then be used to falsify the original dataset
=
into the ‘bad’
D
{(
dataset
. In
1
+
Section VI, we will demonstrate the effectiveness of the
batch RL attack model and the vulnerabilities of batch RL-
enabled LQG systems.

, 𝑇
1
}
· · ·
: 𝑡 = 0, 1,
· · ·

𝑥𝑡 , 𝑢𝑡 , 𝑐𝑡 , 𝑥𝑡
D† =

(
: 𝑡 = 0, 1,

𝑥𝑡 , 𝑢𝑡 , 𝑐†𝑡 , 𝑥𝑡

1
+

, 𝑇

{(

−

−

1

}

)

)

)

+

D

𝑢′𝑡

= 𝑥 ′𝑡

ˆ𝐴, ˆ𝐵
(
˜𝑑 ′𝑥𝑡

˜𝐷𝑥𝑡
+
˜𝑑, ˜𝑟 and

ˆ𝐴 and ˆ𝐵 because he only
Remark 4. The attacker uses
. Even if he knows 𝐴 and 𝐵,
knows the original dataset
D
to mimic the learner’s problem.
it is better to use
)
˜𝐸𝑢𝑡 c† is completely decided
˜𝑟
Since 𝑐†𝑡
+
by ˜𝐷,
˜𝐸,
. Indeed, the attacker only needs to
optimize over ˜𝐷, ˜𝐸, ˜𝑑, and ˜𝑟 even though we include c† in the
objective function for simplicity. [13] also considered data
poisoning attacks on cost data. However, our formulation of
the optimization problem differs from theirs in two aspects:
1. We require the cost data to be consistent and compatible
with the state and the control to make the falsiﬁcation less
likely to be detected. 2. Our problem is convex. Hence, we
can solve the problem to obtain the optimal solution instead
of solving a surrogate problem which produces a sub-optimal
value.

B. The Batch RL Attacker

We deﬁne the batch RL attack model by specifying the
attacker’s capability, objective, and the information he has:

A. The ADP Learner

V. DECEIVING ADP LEARNER

1
+

1) The attacker is able to falsify the costs in the training
. The falsiﬁed training dataset is denoted by
dataset
D
, 𝑡 = 0, 1,
D† =
𝑥𝑡 , 𝑢𝑡 , 𝑐†𝑡 , 𝑥𝑡
. Suppose
{(
the falsiﬁcation of the cost data is consistent and
compatible with the state and the control data, i.e.,
𝑐†𝑡

+
2) The objective of the attacker is to trick the learner
by feeding

into learning the control policy
the learner the falsiﬁed training dataset

= 𝑥 ′𝑡 𝐷†𝑥𝑡

𝐾†, 𝑘 †)

𝑢′𝑡 𝐸 †𝑢𝑡 .

𝑑†′𝑥𝑡

· · ·

, 𝑇

−

+

+

1

}

𝑟

)

(

3) The attacker only has the knowledge of the original

D†.

training dataset

.

D

[

, 𝑐𝑇

𝑐0, 𝑐1,

Deﬁne c =
cost signals from the original dataset
𝑐†0, 𝑐†1,
, 𝑐†𝑇
[
in the poisoned dataset

R𝑇 as a vector of
· · ·
. Let c† =
R𝑇 be a vector of falsiﬁed cost signals
D†. To achieve his objective, the

] ′ ∈

1] ′ ∈

· · ·

D

−

−

1

The policy iteration algorithms presented in Section II-A
will converge for the LQG problem [31], [36]. However,
the policy iteration algorithms required exact knowledge
of the system model (1) and the stage cost function (3).
Bradtke et al. proposed an ADP algorithm that allows the
agent
to perform an approximate version of the policy
iteration algorithm using merely the observed data points
𝑥𝑡 , 𝑢𝑡 , 𝑐𝑡 , 𝑥𝑡

, 𝑡 = 0, 1, 2,

[1].

The ADP learner leverages Recursive Least Square (RLS)
to directly estimate the function 𝑄𝐾 ,𝑘 in (8). To see how the
adaptive learner learns the optimal policy, we rearrange (7)
and (8) to obtain

1
+

)

· · ·

(

𝑥𝑡 , 𝑢𝑡
Θ

= 𝑄𝐾 ,𝑘
(
=
𝑥𝑡 , 𝑢𝑡 , 1
= 𝜙′𝑡 𝜃𝐾 ,𝑘,

[

]

𝑥𝑡 , 𝑢𝑡

𝑐

(

)

8

𝛾𝑄𝐾 ,𝑘

) −
𝐻𝐾 ,𝑘

(

) −

(
𝛾

𝑥𝑡

1, 𝑢𝑡
+
𝑥𝑡

1
+
1, 𝑢𝑡
+

)
1, 1
+

[

Θ

(

]

𝐻𝐾 ,𝑘

,

)

𝑛

(

[

[

]

𝑚

𝛾
𝑥𝑡
[
𝐻𝐾 ,𝑘

𝑘, 1
1, 𝐾𝑥𝑡
1
+
+
+
R(
𝑛
𝑚
𝑛
1
) (
+
+
) ∈

=
where 𝜙𝑡
𝑥𝑡 , 𝑢𝑡 , 1
∈
] −
2 , and 𝜃𝐾 ,𝑘 = Θ
R(
𝑛
𝑚
2
1
2 ,
)/
) (
+
+
+
+
is the column vector concatenation of 𝑥, 𝑢,
𝑥, 𝑢, 1
where
can
and scalar 1. Through the lens of modern RL,
be viewed as a feature vector of the original state and action
space and 𝜃𝐾 ,𝑘 can be viewed as a vector of weights that
the agent needs to tune to approximate the true 𝑄-function.
RLS can now be used to estimate 𝜃𝐾 ,𝑘. The recurrence

𝑥, 𝑢, 1

]
𝑚
+

2
)
+

[

]

relations for RLS are given by

𝑖

𝑖

= ˆ𝜃𝑧

1

𝑖

(

−

) +

)

𝑆𝑧

𝑖

(

1

)

−

ˆ𝜃𝑧

(

𝑆𝑧

(

)

= 𝑆𝑧

𝑖

(

1

−

) −

𝑐𝑡

𝜙′𝑡
−
𝜙′𝑡 𝑆𝑧
𝑖

𝜙𝑡
1
(cid:0)
+
𝜙𝑡 𝜙′𝑡 𝑆𝑧
(
𝑖
1
)

−

(

−
𝜙𝑡

1
)
𝜙′𝑡 𝑆𝑧

𝑆𝑧

𝑖
(
1

−
+

ˆ𝜃𝑧

1

𝑖

(

−

)

,

1

)

, 𝑆𝑧

= 𝑆0.

(cid:1)

(

0

)

(

(31)
Here, 𝑃0 = 𝛽𝐼 for some large positive constant 𝛽. 𝜃𝑧 =
Θ
𝐻𝐾𝑧 ,𝑘𝑧 )
is the true parameter vector for the function
𝑄𝐾𝑧 ,𝑘𝑧 . ˆ𝜃𝑧
is the 𝑖th estimate of 𝜃𝑧. The subscript 𝑡 and
𝑖
the index 𝑖 are both incremented at each time step. And 𝑧
is the index that counts the number of policy updates the
algorithm made.

(

)

The LQG agent follow the 𝑄-function based ADP algo-
rithms (i.e., Algorithm 1) to ﬁnd the optimal policy. In line
7, the agent adds an appropriate probing noise 𝑒𝑡 to make
sure 𝜙𝑡 is persistently excited over time [1]. In line 9, when
1 = 𝐾𝑧 𝑥𝑡
computing 𝜙𝑡 , we let 𝑢𝑡
𝑘 𝑧. In line 13 and 14,
+
ˆ𝐻
are sub-matrices of the matrix ˆ𝐻
𝑢1
𝑢 𝑥
)
following the same notation for the matrix 𝐻𝐾 ,𝑘 in (8).
The convergence of Algorithm 1 is guaranteed if
is
controllable (or at least stabilizable), 𝐾0 is stabilizing, and 𝜙𝑡
is persistently excited. One can refer to [1] for more details
about the 𝑄-function based ADP algorithm.

, and ˆ𝐻

𝐴, 𝐵

ˆ𝐻

𝑢𝑢

+

)

(

,

(

(

)

)

(

Algorithm 1 The 𝑄-Function Based ADP Algorithm

1: Initialize: Stabilizing policy

𝐾0, 𝑘0

, tolerance 𝜖1 > 0,

)

(
= 0, 𝑧 = 0, and 𝑡 = 0

𝜖2 > 0
2: Set ˆ𝜃0
(
3: repeat
4:

0

)

Set 𝑆𝑧
(
repeat

5:

6:
7:

8:

9:
10:

11:
12:

13:

14:

= 𝑆0 ≔ 𝛽𝐼 and 𝑖 = 0

0

)

Measure 𝑥𝑡
Compute 𝑢𝑡 = 𝐾𝑧 𝑥𝑡
+
Receive 𝑐𝑡 and measure 𝑥𝑡
Compute ˆ𝜃𝑧
𝑖
Set 𝑡 = 𝑡
ˆ𝜃𝑧
𝑖

1
+
(
1 and 𝑖 = 𝑖
ˆ𝜃𝑧

1
+
2 < 𝜖2

)

+
) −

+

𝑘 𝑧

𝑒𝑡 and apply 𝑢𝑡
1
+
using RLS (31)

(

(

k

1

) k

until
𝑖
−
Find the matrix ˆ𝐻 corresponding to ˆ𝜃𝑧
ˆ𝐻
ˆ𝐻−
1
Compute 𝐾𝑧
𝑢𝑢
−
(
)
ˆ𝐻
ˆ𝐻−
1
Compute 𝑘 𝑧
𝑢𝑢
−
(
(
Set ˆ𝜃𝑧
𝑖
and z = z+1
(
)
𝑘 𝑧
𝑘 𝑧
𝐾𝑧
+ k

1 =
+
1 =
+
= ˆ𝜃𝑧
𝐹
1

𝐹 < 𝜖2

0
(
)
𝐾𝑧

(
𝑢1
)

1
+
−

−

𝑢 𝑥

k

k

k

−

−

1

)

)

𝑖

(

)

15:
16: until

B. The ADP Attacker

We deﬁne the ADP attack model by specifying the at-

tacker’s capability, objective, and the information he has:

1) The attacker can falsify the cost signals 𝑐𝑡 and 𝑐†𝑡
received by the agent in line 8 of Algorithm 1.
2) The attacker’s objective is to trick the agent

into

is

learning the ‘nefarious’ policy

𝐾†, 𝑘 †)

.

(

3) The attacker knows the LQG system and receives the
same information as the agent during the learning
process.

The problem for the attacker is how to craft such an attack
so that the agent will ﬁnally learn the ‘nefarious’ policy and
the falsiﬁed cost 𝑐†𝑡 deviate insigniﬁcantly from the original
cost signals 𝑐𝑡 . Hence, the attacker can ﬁrst solve the cost
parameters falsiﬁcation problem (21). Let
be
the solution of problem (21). The falsiﬁed cost signal 𝑐†𝑡
then can be crafted using 𝑐†𝑡
𝑢′𝑡 𝐸𝑢𝑡
+
+
for 𝑡 = 0, 1, 2,
. The agent receives 𝑐†𝑡
in Algorithm 1
and other information received remain correct. Following
the same arguments as in Theorem 1 of [1], we know
that if problem (1) is feasible, the agent will eventually
be tricked into learning the ‘nefarious’ policy
. In
Section VI, we will demonstrate the effectiveness of the ADP
attack model and the vulnerabilities of the ADP based LQG
systems.

𝐷†, 𝐸 †, 𝑑†)
𝑟

𝐾†, 𝑘 †)

= 𝑥𝑡 𝐷†𝑥𝑡

𝑑†𝑥𝑡

· · ·

+

(

(

VI. NUMERICAL STUDIES

We now use an LQG system to demonstrate the bounds
we obtain in Section III as well as the effectiveness of the
two attack models against the Batch RL learner in Section IV
and the ADP learner in Section V. Consider a linear system
with 6-dimensional state consisting of its 3-dimensional (3D)
position and 3-dimensional (3D) velocity:

𝐴 =

𝐼3
0

0.1𝐼3
0.95𝐼3

R6,

𝐵 =

R6
×

3.

0
0.1𝐼3

)

(cid:20)

(cid:21)

∈

𝑡 , 𝑣 𝜁

(cid:20)
(cid:21)
0, 0.01𝐼𝑛

∼ N (
𝜒𝑡 , 𝜂𝑡 , 𝜁𝑡
(

∈
Suppose 𝐶 = 𝐼𝑛 and 𝑤𝑡
. Let 𝑥𝑡 =
)
𝑡 , 𝑣 𝜂
𝜒𝑡 , 𝜂𝑡 , 𝜁𝑡 , 𝑣 𝜒
𝑡 ] ′. Here,
is the position of
[
𝑣 𝜒
𝑡 , 𝑣 𝜂
the vehicle in the 3D space at time 𝑡 and
is
the velocity at time 𝑡. The vehicle starts from the initial
=
position
)
1,
. The LQG aims to stabilize the vehicle to the
(−
origin. The true cost parameters are 𝐷 = 𝐼𝑛, 𝐸 = 0.5𝐼𝑚,
𝑑 = 0, and 𝑟 = 0 and the cost signals the agent receives is
𝑐𝑡 = 𝑥 ′𝑡 𝐷𝑥𝑡
𝑢′𝑡 𝐸𝑢𝑡 . The discount factor 𝛾 is set to
be 𝛾 = 0.9. The optimal policy, which stabilizes the vehicle
to the origin, can be computed according to (4)-(6):

𝑡 , 𝑣 𝜁
𝑡 )
0 , 𝑣 𝜁
0 )

𝜒0, 𝜂0, 𝜁0
1

with velocity

𝑣 𝜒
0 , 𝑣 𝜂

1, 1, 0.5

(
0.5,

𝑑 ′𝑥𝑡

−

−

=

+

+

+

𝑟

)

)

(

(

(

0.5316
0.0000
0.0000

−
−
−
0.0000
0.0000
0.0000#

𝐾 ∗ =

"

𝑘∗ =

"

−
−
−

0.0000
0.5316
0.0000

−
−
−

0.0000
0.0000
0.5315

−
−
−

0.9700
0.0000
0.0000

−
−
−

0.0000
0.9700
0.0000

−
−
−

0.0000
0.0000
0.9700#

,

−
−
−

.

(32)
Suppose the attacker aims to trick the agent into learning

the following policy:

𝐾 † =

−

0.5316
0
0

"

0
0.5316
0

−

𝑘† =

"

0.5316
0
0.5316#

−

,

9

0
0
0.5316

−

−

0.9700
0.
0

0
0.9700
0

−

0
0
0.9700#

−

(33)

(

−

)

which steer the vehicle to a dangerous position at
1, 0,

and stay there with zero velocity.

1

𝜒, 𝜂, 𝜁

(

=

)

𝐾†, 𝑘 †)

To see whether he can successfully mislead the agent
into learning the ‘nefarious’ policy
by falsifying
the cost parameters, the attacker can check the conditions
in Theorem 3. Indeed, the conditions 1), 2), and 3) are
held when choosing 𝐸 = 𝐼𝑚 meaning the cost parameter
falsiﬁcation problem (21) is feasible. Solving problem
21
)
using CVXPY [37] yields (we write numerical values that
are less than 1.0e

10 as ‘0’ due to space limitation.)

(

(

0.7163
0
0.2837
0.1218
−
0
0.1218

0.1448

−
0.2904
0
2.095

𝐷† = 









(cid:2)

𝑑† =

𝐸 † =

"

−
0
1.000
0
0
0
0

0.2837
0
0.7163
0.1218
0
0.1219

0.1218
−
0
0.1218
0.5687
0
0.4313

0
0
0
0
1.000
0

0.1218
0
0.1218
−
0.4313
0
−
0.5687

−
0.1448

0

1.6084

0
0.5000
0

−
0.2096
0
0.2904#

.

0

0.1608

′ ,

(cid:3)

,












(34)

𝐹

k

𝐷

𝐷† −

𝑑† −
𝑑
The optimal value
𝐹 of
k
ˆ𝐾†, ˆ𝑘 †)
the optimization problem (21) is 1.8137. The policy
(
computed using the falsiﬁed cost parameters 𝐷†, 𝐸 †, 𝑑†
𝐾†, 𝑘 †)
aligns well with the attacker’s target policy
stated
in (33):

𝐸 † −

+ k

+ k

𝐸

k

k

𝐹

(

True Cost Data c
Falsified Cost Data c†

0

50

100

150

200
t

250

300

350

400

c† − c

6

4

2

0.0

− 0.5

0

100

200
t
(a) True cost data, falsiﬁed cost data, and their differences.

300

400

Normal Trajectory
Trajectory After Attack
Dangerous Point

0.50
0.25
0.00
− 0.25
− 0.50
− 0.75
− 1.00

(cid:2)

0.5315
0.0000
0.0000

−
−
−
0.5316
0.0000
0.5316#

ˆ𝐾 † =

"

ˆ𝑘† =

"

−

0.0000
0.5315
0.0000

−
−
−

0.0000
0.0000
0.5315

−
−
−

0.9699
0.0000
0.0000

−
−
−

0.0000
0.9699
0.0000

−
−
−

0.0000
0.0000
0.9699#

,

−
−
−

.

− 0.20.0 0.2 0.4 0.6 0.8 1.0 1.2

(cid:0)

1.0

0.8

0.6

0.4

(cid:1)

0.2

0.0

− 0.2

A. Attacking Batch RL Learner

1

−

, 𝑇

· · ·

𝑥𝑡 , 𝑢𝑡 , 𝑐𝑡 , 𝑥𝑡

The original training data set

, 𝑡 =
=
0, 1,
is generated by running the LQG system
}
with uniformly distributed random controls and receiving the
accurate cost signals 𝑐𝑡 , 𝑡 = 0, 1,
1. Here, 𝑇 = 400
is the number of time steps (i.e., the number of data tuple
collected).

· · ·

1
+

, 𝑇

D

{(

−

)

D

Based on the dataset

, the Batch RL learner estimates
the system parameters and the cost parameters using (23)
and (23). With the estimates, the computed optimal policy
under the clean data is
0.0148
0.5613
−
0.5232
0.0268
0.0145
0.0047

0.0063
0.0189
0.9630#

0.9878
−
0.0265
0.0171

0.0234
0.0045
0.5465

0.0188
0.9337
0.0111

ˆ𝐾 ∗batch

=

"

,

−
−
−
06

−
−
3.9701e

ˆ𝑘∗batch

=

1.5601e

−

−

−

−

−
−
′ .

−
−

07

−

1.6453e

06

−

(cid:2)

(cid:3)

We can see that the batch RL learner learns a decent policy
ˆ𝐾∗batch, ˆ𝑘 ∗batch)
that only differs slightly from the optimal
(
policy (32).
The attacker aims to trick the agent into learning the
‘nefarious’ policy (33) by falsifying the cost data 𝑐𝑡 into 𝑐†𝑡 .
First, the attacker estimates the system parameters using the
clean dataset
. Then, the attacker solves the optimization
problem (24) to generate the falsiﬁed cost data 𝑐†𝑡 , 𝑡 =
0, 1,
1. The LQG agent learns from the falsiﬁed
· · ·
, 𝑡 = 0, 1, 2,
and is
data set
1
)
{(
+
misled into learning the policy

, 𝑇
−
D† =

𝑥𝑡 , 𝑢𝑡 , 𝑐†𝑡 , 𝑥𝑡

· · ·

, 𝑇

D

−

1

}

(b) Trajectories generated from the policy learned from the clean
data and the policy learned from the falsiﬁed data respectively.

Fig. 2: Demonstration of attacks against LQG agent using
batch RL.

which is close to the attacker target policy (33).

The results of the attack model are demonstrated in Fig.
2. Fig. 2a presents the true cost data, the falsiﬁed cost data,
and how much falsiﬁcation is imposed. We can see that the
attacker only applied small falsiﬁcation on each cost signal
(i.e., the cost data and the falsiﬁed cost data stays close).
Fig. 2b shows the trajectories of the vehicle under the policy
ˆ𝐾∗batch, ˆ𝑘 ∗batch)
learned from the true data
and the policy
(
ˆ𝐾†batch, ˆ𝑘 †batch)
D†. We can
learned from the falsiﬁed data
(
see that the attack successfully tricks the LQG agent into
learning a policy that steers the vehicle into a dangerous
position. The attack’s success is indeed built on a very small
c† −
falsiﬁcation of the cost data. To see this, we have
96.2731 = 2.296%. The results show
c
k
/
the attack model’s effectiveness and exhibit the LQG agent’s
vulnerabilities using batch RL.

2 = 2.2107

/k

D

c

k

k

2

B. Attacking ADP Learner

0.5613
−
0.0268
0.0047

−
0.5812

0.0148
0.5232
0.0145

−
−
−
0.0382

−

−

ˆ𝐾 †batch

=

ˆ𝑘†batch

=

"

(cid:2)

0.0234
0.0045
0.5465

−
−
0.5310

′ ,

(cid:3)

0.9878
−
0.0265
0.0171

0.0188
0.9337
0.0111

−
−

0.0063
0.0189
0.9630#

−
−

,

The LQG agent implements Algorithm 1 to learn the
optimal policy to control the vehicle by interacting with the
5. Set 𝛽 = 10.
environment. Set the tolerance 𝜖1 = 𝜖2 = 1e

−

10

600

400

200

600

400

200

0

0

500

1000

500

1000

(a) True cost signals, falsiﬁed cost signals, and their differences.

100

10-1

10-2

5

10

15

20

(b) The policy updates 𝐾𝑧 , 𝑧 = 0, 1, 2,
, 22 during the ADP
learning process under the falsiﬁed cost signals and the policy
sequences converge to the attacker’s target policy
rather
than the optimal policy

𝐾 †, 𝑘†)

· · ·

(

.

𝐾 ∗, 𝑘∗)

(

Fig. 3: Demonstration of attacks against LQG agent using
ADP.

Let the initial control policy be

𝐾0 =

0.0300
0
0

"

0
0.5600
0

0
0
0.5000

−

0.1000
0
0

0
1.0000
0

0
0
1.0000#

,

𝑘0 =

0

0

0

′ .

(cid:3)

(cid:2)

The attacker aims to trick the ADP learner into learning the
‘nefarious’ policy (33) by falsifying the cost signals received
by the LQG agent. As we discussed in Section V, the attacker
can use (34) to falsify cost signals into 𝑐†𝑡
𝑢′𝑡 𝐸 †𝑢𝑡

+
𝑟. The numerical computation indicates that

= 𝑥 ′𝑡 𝐷†𝑥𝑡

𝑑†′𝑥𝑡

+

|

𝑐𝑡

1.088

2
2 +
k
If the LQG agent receives the cost signals,
algorithms ﬁnally lead the LQG agent to the policy

0.3060

2
2 +

0.419

| ≤

𝑢𝑡

𝑥𝑡

k

k

k

k

𝑥𝑡

2.

k

the ADP

+
𝑐†𝑡 −

ˆ𝐾 ∗ADP

=

ˆ𝑘∗ADP

=

0.5280
−
0.0021
−
0.0065

"

0.0007

−

(cid:2)

0.0058
0.5409
0.0340

−
−
−
0.0054

−

0.0064
0.0201
0.5231

−
−
0.0093

′ ,

(cid:3)

0.9673
−
0.0015
0.0033

0.0001
0.9742
0.0177

−
−

0.0035
0.0166
0.9632#

−
−

,

11

which only differs slightly from the optimal policy (32).

If the LQG agent receives the falsiﬁed cost signals, the

agent will be tricked into learning the policy

ˆ𝐾 †ADP

=

"

0.5253
0.0009
0.0093

−
−
−
0.5229

0.0473
0.5274
0.0675

−
−
0.0076

0.0284
0.0050
−
0.5754.
−
0.5144

0.9685
0.0028
0.0054

−
−
−

0.0145
0.9696
0.0234

−
−

0.0242
0.0055
1.0034#

−
−

,

=

ˆ𝑘†ADP
which is close the the attacker’s target policy (33).

′ ,

−

(cid:2)

(cid:3)

The results of the attack model on ADP are demonstrated
in Fig. 3. In Fig. 3a, we present the true cost signals, the fal-
siﬁed cost signals, and the absolute value of their differences
over time. As we can see, the falsiﬁed cost signals stay close
to the true cost signals. The falsiﬁcation grows as
and
𝑢𝑡
increases because the falsiﬁed cost signals is generated
k
using 𝑐† = 𝑥 ′𝑡 𝐷†𝑥𝑡
𝑑†𝑥𝑡 . Fig. 3b shows the
𝑟
how the policy
(in line 13-14 Algorithm 1) iterates
during the learning process. The attacker gradually misleads
𝐾†, 𝑘 †)
the LQG agent into learning the ‘nefarious’ policy
.
The results show the attack model’s effectiveness and exhibit
the LQG agent’s vulnerabilities using ADP approaches.

+
𝐾𝑧 , 𝑘 𝑧

𝑢′𝑡 𝐸 †𝑢𝑡

𝑥𝑡

+

+

k

k

k

(

)

(

VII. CONCLUSION

In this work, we have studied the vulnerabilities of the
RL-enabled LQG control systems under cost signal falsiﬁ-
cation. We have shown that a small falsiﬁcation of the cost
parameters will only lead to a bounded change in the optimal
policy. The bound is linear on the amount of falsiﬁcation
the attacker can apply to the cost parameters. This result
shows a certain degree of robustness of the optimal policy
to small unintended changes in the cost parameters. We
have proposed an attack model where the attacker conducts
intentional falsiﬁcation on the cost parameters to mislead the
agent into learning a ‘nefarious’ policy. We have formulated
the attack’s problem as an optimization problem, which is
proved to be convex, and developed necessary and sufﬁcient
conditions to check the feasibility of the attacker’s problem.
Based on the attack model on cost parameters, we dis-
cussed two attack models on the batch RL and the ADP
learners. Numerical
results have shown that with only
2.296% of falsiﬁcation on the cost data, the attacker can
achieve his goal — misleading the batch RL learner into
learning the ’nefarious’ policy that
leads the vehicle to
a dangerous position. An ADP learner updates the policy
iteratively and learns in an online manner. The results have
demonstrated that the attacker can gradually trick the learner
into learning the ‘nefarious’ policy, even if the falsiﬁed cost
signals stay close to true cost signals.

The paper has exhibited the effectiveness of these at-
tack models and revealed vulnerabilities of the RL-enabled
control systems. The authors hope this paper can bring
more attention from the community to the potential security
threats faced by RL-enabled control systems. Future works
can focus on more effective attack models on cost signals,
state measurements, and control commands. With a better
understanding of these attack models, we can take further
steps to develop reliable and effective detection and defensive
mechanisms.

APPENDIX

Hence,

A. Lemmas

Lemma 2. Given any two positive semi-deﬁnite matrices of
the same dimension 𝑋 and 𝑍,

𝐸 −

1
2 −

𝐸 −

1
1 k ≤

k

1

k

𝐸 −

2

1
1 k
𝐸 −

1
1 k

− k

Δ𝐸

.

k

k

(cid:3)

𝑋

𝐼

(

k

+

𝑆 𝑋

1

−

)

𝑋

.

k ≤ k
𝑋 −

k
1

Proof. Assume 𝑋 > 0. Note that
𝑆
.
)
+
)
1 =
Hence, the inverse of them are equal, i.e., 𝑋 −
1 =
𝑋.
𝐼
(
By a continuity argument, the inequality also holds when 𝑋
(cid:3)
is only positive semi-deﬁnite.

1. Then, we obtain 𝑋

𝑆 𝑋
)−
) ≤

+
𝑆
+
𝑆

(
𝑋 −

𝑋 =
1
𝑋 −
1

𝑆 𝑋

𝑆 𝑋

)−

)−

+

+

+

(
1

𝐼

(

(

(

𝐼

→
= max

Lemma 3. Let
ators R𝑛
×

𝑛

L (
R𝑛
×

𝑛

𝑛, R𝑛
×

R𝑛
×
𝑛 with the following operator norm

be the space of linear oper-

)

T (
2

𝐴𝑐

𝜏
(
𝛾𝜌

)
𝐴𝑐

2

𝑋

:

𝑋

= 1

,

for

{kT (
kT k L
= 𝑋
Deﬁne
Schur stable. There exists a ﬁnite 𝜏

) k
}
𝛾 𝐴′𝑐 𝑋 𝐴𝑐 with 0 < 𝛾

−

𝑋

k

k

)

≤

such that

T ∈ L (

𝐴𝑐

)

(

𝑛

R𝑛
×

𝑛, R𝑛
×

)
1 and 𝐴𝑐 is
1

kT −

k L ≤

)

(

1
−
Proof. Note that 𝐴𝑐 is Schur stable. There exists 𝐿 such
that 𝑀 = 𝑋
𝛾 𝐴′𝑐 𝑋 𝐴𝑐, which is a discrete-time Lyapunov
equation given 𝑀 and 𝐴𝑐. It is easy to verify that 𝑋 =
𝑘 by pluging it back into the Lyapunov

∞𝑘=0 𝛾 𝑘

−
𝑘 𝑀
𝐴′𝑐)
equation. Hence, we have
Í

𝐴𝑐

(

)

(

1

−

𝑀

)

(

T

(cid:13)
(cid:13)

∞

=

(cid:13)
(cid:13)

≤

∞

(cid:13)
Õ𝑘=0
(cid:13)
(cid:13)
(cid:13)
(cid:13)
Õ𝑘=0

𝛾 𝑘

𝐴′𝑐)

(

𝑘 𝑀

𝐴𝑐

𝑘

)

(

𝛾 𝑘

2

𝐴𝑘
𝑐 k

k

k

𝑀

.

k

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

𝜏

𝐴𝑐

𝐴𝑐

𝑀
By Gelfand’s formula, there exists a ﬁnite 𝜏
(
𝑘 for every 𝑘
𝐴𝑘
0. Here, 𝜌
𝜌
𝑐 k ≤
≥
(
k
)
is deﬁned as 𝜏
𝐴𝑐
the spectral radius of 𝐴𝑐 and 𝜏
)
(
∞𝑘=1 𝛾 𝑘
𝐴𝑘
< 1,
𝐴𝑐
. Since 𝜌
𝜌
sup𝑘
𝑐 k/
)
(
{k
𝐴𝑐
𝜏
𝐴𝑘
∞𝑘=0 𝛾 𝑘
2
convergent and
(
𝑐 k
𝛾𝜌
k

)
(
1
kT −

}
k L ≤

such that
𝐴𝑐
is
)
(
≔
𝐴𝑐
)
(
𝑘 is
𝐴𝑐
k
k
2
(cid:3)
2 .
)
𝐴𝑐

𝐴𝑐

Í
≤

N

)

(

)

∈

𝑘

Lemma 4. Let Δ𝐸 = 𝐸2
Í
two square matrices 𝐸1 and 𝐸2. Assume
𝐸 −
1

< 1. We have

Δ𝐸

−

1

k

k

1
−
𝐸1, where 𝐸1 and 𝐸2 are
Δ𝐸
< 1 and

)

(

k

k

𝐸 −

1
2 −

𝐸 −

1
1 k ≤

k

k

1
1 k

𝐸 −
1

2

Δ𝐸
k
1
𝐸 −
1 k

k

.

− k
Proof. A straight calculation gives 𝐸 −
It follows immediately that

1
1 −

1

𝐸 −
2

= 𝐸 −
1

1

Δ𝐸 𝐸 −

1
2 .

𝐸 −

1
1 −

𝐸 −

1
2 k

k

=

k

1

𝐸 −
1

Δ𝐸 𝐸 −

1
2 k ≤ k

1

Δ𝐸

𝐸 −
1

𝐸 −

1
2 k

.

(35)

Applying the triangle inequality, we obtain

kk
1
𝐸 −
1 k +
. Plugging it back into (35)

1
2 k ≤ k

𝐸 −

k

Δ𝐸

1

𝐸 −
1
k
yields

𝐸 −

1
2 k ≤

kk

1
𝐸 −
1 k
k
1
𝐸 −
1
1
− k

Δ𝐸

k

𝐸 −

1
2 −

𝐸 −

1
1 k ≤

k

k
1

𝐸 −

1
1 k

2

Δ𝐸
k
Δ𝐸
1
𝐸 −
1

− k

k

.

k

Δ𝐸

Δ𝐸

k

k
1
𝐸 −
1

k

k ≤ k

Lemma 5. Given a real-valued Schur stable matrix 𝐴𝑐 and
a scalar 0 < 𝛾

𝛾 𝐴𝑐 is invertible.

1, 𝐼

≤

−

.

Proof. Suppose that 𝜆 is an eigenvalue of 𝐴𝑐 and 𝑣 is its
associated vector. Hence, we have

𝐴𝑐𝑣 = 𝜆𝑣.

Therefore, we have

𝐼

(

−

𝛾 𝐴𝑐

)

𝑣 = 𝑣

−

𝛾𝜆𝑣 =

1

(

−

𝛾𝜆

𝑣.

)

|

−

−

Hence, for every eigenvalue 𝜆 of matrix 𝐴𝑐, 1
eigenvalue of the matrix 𝐼
𝜆
|
part of 1
−
𝐴𝑐. Thus, 𝐼

𝛾𝜆 is an
𝛾 𝐴𝑐. Since 𝐴𝑐 is Schur stable,
< 1 for every eigenvalue 𝜆 of matrix 𝐴𝑐. Hence, the real
𝛾𝜆 is non-zero for every eigenvalue 𝜆 of matrix
(cid:3)

𝛾 𝐴𝑐 is invertible.

−
Lemma 6. Suppose 𝑓1, 𝑓2
∈
𝛼𝐼 and
and 𝛼-strongly convex, i.e.,
≤
and 𝑢2 = arg min𝑢 𝑓2
Let 𝑢1 = arg min𝑢 𝑓1
𝑢2
𝑓1

)
k
−
Proof. An application of Taylor expansion yields

R are twice differentiable
2 𝑓2
𝛼𝐼.
≤
. Suppose

𝜖, then

𝑢
(
𝑢1

→
2 𝑓1

∇
𝑢
(

) k ≤

k ≤

𝜖
𝛼 .

R𝑛

𝑢2

∇

k

)

(

𝑓1

𝑢2

=

𝑓1

𝑢1

2 𝑓1

¯𝑢

𝑢2

𝑢1

,

(

∇
with ¯𝑢 = 𝛽𝑢1
stationary point of 𝑓1, i.e.,

) + ∇
(
𝑢2 for some 0
𝑓1

∇
𝛽

)
1

+ (

𝑢1

−

(

)

) (
𝛽

)

−
1. Since 𝑢1 is a

≤

≤
= 0, we have

𝑢2

𝛼𝐼

𝑢1

k

(
−
which gives

) k ≤ k∇
𝑢1

𝑢2

(
𝜖
𝛼 .

2 𝑓1

(
𝑢1

∇
¯𝑢

) (

)
𝑢2

−

=

) k

𝑢2

𝑓1

(

k∇

) k ≤

𝜖,

(cid:3)

k

k ≤

−
Lemma 7. Deﬁne the functions 𝑓𝑖
𝐵𝑢
𝐴𝑥
)
(
deﬁnite. Suppose
k ≤
𝜖
(·)
a matrix. Let 𝐾𝑖𝑥 = arg min𝑢 𝑓𝑖

𝐸1
2, where 𝜆min

+
)
for 𝑖 = 1, 2. Suppose 𝐸1 and 𝐸2 are positive
𝜖 and
𝑃1
𝑃2
with
is the smallest eigenvalue of
𝑥, 𝑢
. Then, we have

) ′𝑃𝑖
𝜆min

2 𝑢′𝐸𝑖𝑢

𝛾 1
2 (

𝑥, 𝑢

k ≤

= 1

𝐵𝑢

𝐸2

𝐸1

𝐴𝑥

)/

−

−

≤

+

+

k

k

𝜖

)

(

(

(

𝑓

(

)

𝐾1

k

−

𝐾2

2𝛾

k ≤

𝜆min

𝐸1
)
2𝛾

(

max

𝐵

𝐴

,

k

k

{k

2

k}

(k

𝐾1

1

)

𝑓

𝜖

(

)

k +

+

𝜆min

𝐸1

k

)

(

𝐾1

𝜖.

k

(36)

Proof. According to the deﬁnition of strong convexity, 𝑓1, 𝑓2
are 𝛼-strongly convex in 𝑢 with

𝛼 = min

𝜆min

{

𝐸1

)

(

, 𝜆min

𝐸2

.

)}

(

We can calculate the gradient of 𝑓𝑖

𝑥, 𝑢

(

𝑥, 𝑢

𝑓𝑖

(

∇

)

=

(

𝛾𝐵′𝑃𝑖 𝐵

𝐸𝑖

𝑢

)

+

+

as

)
𝛾𝐵′𝑃𝑖 𝐴𝑥.

Note that given

< 1,

It is obvious that

𝐸 −

1
1 kk

Δ𝐸

𝐸 −

1
1 k

.

k ≤ k

𝛾𝐵′𝑃1𝐵

k

𝐸1

+

−

𝛾𝐵′𝑃2𝐵

𝐸2

−

k ≤

𝐵

𝛾

k

2

k

k

𝑃1

−

𝑃2

𝐸1

𝐸2

,

k

−

k + k

12

and

𝛾𝐵′𝑃1 𝐴

k

−

𝛾𝐵′𝑃2 𝐴

Hence, for any 𝑥 such that

𝛾

𝐵

𝐴

kk

kk

k
1, we have

𝑃1

𝑃2

.

k

−

k ≤
𝑥

k

k ≤

k∇
𝛾

≤

𝑥, 𝑢
2

𝑓1
(
𝐵

k

k

k
𝐵

𝛾

) k
𝐸1

) − ∇
𝑃1
−
𝐴

kk
𝐴

,

kk
𝐵

𝑥, 𝑢

(

𝑓2
𝑃2
𝑃1
−
2

k + k
𝑃2
𝑃1

k]

(cid:2)
k
+ [
𝛾 max

{k

−
k
≤
From Lemma 6, we know

k}

k

k

𝐸2

𝑢

k

k

k

−

(cid:3)
𝑢

𝑃2

k(k

1

) +

𝐸1

𝛾

k

𝐸2

𝑢

k

kk

−

k +

𝐾1𝑥

𝛼

Note that

k

k

𝑥

𝐾2𝑥

−
k
1 and

= 𝛼

𝑢1

k

𝑢2

−
𝐾1𝑥

k ≤ k∇
𝐾1

k ≤ k

k

𝑢1

k

k ≤ k

𝑓2

𝑢1

.

(
) k
. We have

k ≤
𝛾
𝛼

𝐾1

k

−

𝐾2

k ≤

𝐵

,

k

2

k}

(k

𝐾1

𝑃1

1

) k

−

𝑃2

k

k +

(37)

max
𝛾
𝛼 k

{k
𝐾1

k

𝐴

k
𝜖.

+

Note
𝜆min

(

that
2
)/

𝐸1

𝜖

≤

𝜆min

𝐸1

2. By Weyl’s

≤

)/
𝛼, we can show (36) from (37).

(

inequality
(cid:3)

Lemma 8. Let 𝑀
∈
Δ𝑀
R𝑛
𝑛. Assume
×
R𝑛 be the solution of
ℎ

R𝑛
×
Δ𝑀

∈
Δℎ

k

k ≤

1
1
𝑀 −

k

2

k

𝑛 be a non-singular matrix, and
R𝑛 and

. Let ℎ

∈

+

∈

𝑀 ℎ = 𝑑,

𝑀

(

for some 𝑑
and

∈

R𝑛 and Δ𝑑

+

∈

Δℎ

= 𝑑

)

+

Δ𝑑,

Δ𝑀

ℎ

+

) (
R𝑛. Then, 𝑀

Δ𝑀 is non-singular

+

Δℎ

k

k ≤

2

k

1

𝑀 −

k (k

Δ𝑑

Δ𝑀

.

ℎ

k)

kk

k + k

Proof. It is easy to see that

𝑀

+

Δ𝑀 = 𝑀

(

𝑀 −

1Δ𝑀

.

)

𝐼

+
𝑀 −

Since 𝑀 is non-singular, if 𝐼
Δ𝑀 is non-singular. Indeed, for every non-zero 𝑥

+

1Δ𝑀 is non-singular, 𝑀

+

𝐼

k(

+

𝑀 −

1Δ𝑀

𝑥

)

𝑥

k ≥ k
=
1
(
0,

≥

𝑀 −

1Δ𝑀

k − k

− k

𝑀 −

1Δ𝑀

𝑥

kk
𝑥

k) k

k

R𝑛,

∈

k

which shows the non-singularity of 𝑀
solve for Δℎ:

+

Δ𝑀. Now, we can

Δℎ =
=

(

(

𝑀

𝐼

+

1

Δ𝑀
+
𝑀 −

−
)
1Δ𝑀

Δ𝑑
−
1𝑀 −

−

Δ𝑀 ℎ
1
Δ𝑑

(

(

)

)

−

Δ𝑀 ℎ

)

Hence, we have

Δℎ

k

k ≤ k(

𝐼

+

𝑀 −
𝑀 −
k
1
𝑀 −
1

k ( k

1Δ𝑀
1

1

−

)

kk

1

𝑀 −

k(k

Δ𝑑

Δ𝑀

k
kk
Δ𝑑

k
k + k

Δ𝑑

(k
Δ𝑀

k + k
ℎ

kk

k)

.

k + k
ℎ

kk

Δ𝑀

1
2

≤

≤

− k
𝑀 −
k

Δ𝑀

ℎ

k)

kk

B. Proof of Lemma 1

)

)

(

(
(

𝐴, 𝐵

𝐴, 𝐷1
/
5
)

Proof. From Theorem 4 of [38], we know that if
is
2
is observable, the solution to the
controllable and
is positive deﬁnite and unique and 𝐾 is
Riccati equation
stabilizing. Then from (4), we know 𝐾∗ is uniquely decided
𝐴, 𝐵
and the cost parameters
by the system parameters
𝐷, 𝐸
Since 𝐾 is stabilizing, from Lemma 5, 𝐼𝑛

is
invertible. Hence, the solution ℎ∗ of (6) is unique and depend
and 𝐾∗. Then, (4)
𝐴, 𝐵
only on the control parameters
𝐴, 𝐵
shows that 𝑘 ∗ is uniquely decided by
.
)
(cid:3)

𝐷, 𝐸, 𝑑

and

𝐵𝐾

)
(

−

𝐴

+

𝛾

(

)

(

(

)

(

)

(

)

.

REFERENCES

[1] S. J. Bradtke, B. E. Ydstie, and A. G. Barto, “Adaptive linear quadratic
control using policy iteration,” in Proceedings of 1994 American
Control Conference-ACC’94, vol. 3.

IEEE, 1994, pp. 3475–3479.

[2] H. Mania, S. Tu, and B. Recht, “Certainty equivalence is efﬁcient for
linear quadratic control,” Advances in Neural Information Processing
Systems, vol. 32, 2019.

[3] F. L. Lewis and D. Vrabie, “Reinforcement learning and adaptive dy-
namic programming for feedback control,” IEEE circuits and systems
magazine, vol. 9, no. 3, pp. 32–50, 2009.

[4] Y. Jiang and Z.-P. Jiang, “Computational adaptive optimal control for
continuous-time linear systems with completely unknown dynamics,”
Automatica, vol. 48, no. 10, pp. 2699–2704, 2012.

[5] B. Pang, T. Bian, and Z.-P. Jiang, “Adaptive dynamic programming
for ﬁnite-horizon optimal control of linear time-varying discrete-time
systems,” Control theory and technology, vol. 17, no. 1, pp. 73–84,
2019.

[6] D. G¨orges, “Distributed adaptive linear quadratic control using dis-
tributed reinforcement learning,” IFAC-PapersOnLine, vol. 52, no. 11,
pp. 218–223, 2019.

[7] A. Cetinkaya, H. Ishii, and T. Hayakawa, “An overview on denial-
of-service attacks in control systems: Attack models and security
analyses,” Entropy, vol. 21, no. 2, p. 210, 2019.

[8] Y. Huang, Z. Xiong, and Q. Zhu, “Cross-layer coordinated attacks
on cyber-physical systems: A lqg game framework with controlled
observations,” in 2021 European Control Conference (ECC).
IEEE,
2021, pp. 521–528.

[9] Y. Mo and B. Sinopoli, “False data injection attacks in control
systems,” in Preprints of the 1st workshop on Secure Control Systems,
2010, pp. 1–6.

[10] F. Miao, Q. Zhu, M. Pajic, and G. J. Pappas, “Coding schemes
for securing cyber-physical systems against stealthy data injection
attacks,” IEEE Transactions on Control of Network Systems, vol. 4,
no. 1, pp. 106–117, 2016.

[11] T. Zhang and Q. Zhu, “Strategic defense against deceptive civilian gps
spooﬁng of unmanned aerial vehicles,” in International Conference on
Decision and Game Theory for Security. Springer, 2017, pp. 213–233.
[12] Y.-C. Liu, G. Bianchin, and F. Pasqualetti, “Secure trajectory plan-
ning against undetectable spooﬁng attacks,” Automatica, vol. 112, p.
108655, 2020.

[13] Y. Ma, X. Zhang, W. Sun, and J. Zhu, “Policy poisoning in batch
reinforcement learning and control,” Advances in Neural Information
Processing Systems, vol. 32, 2019.

[15] V. Behzadan and A. Munir, “Adversarial

[14] X. Zhang, X. Zhu, and L. Lessard, “Online data poisoning attacks,”
in Learning for Dynamics and Control. PMLR, 2020, pp. 201–210.
learning
framework for benchmarking collision avoidance mechanisms in au-
tonomous vehicles,” IEEE Intelligent Transportation Systems Maga-
zine, vol. 13, no. 2, pp. 236–241, 2019.

reinforcement

[16] Y.-Y. Chen, C.-T. Chen, C.-Y. Sang, Y.-C. Yang, and S.-H. Huang,
“Adversarial attacks against reinforcement
learning-based portfolio
management strategy,” IEEE Access, vol. 9, pp. 50 667–50 685, 2021.
[17] M. Figura, K. C. Kosaraju, and V. Gupta, “Adversarial attacks in
consensus-based multi-agent reinforcement learning,” in 2021 Ameri-
can Control Conference (ACC).

IEEE, 2021, pp. 3050–3055.

k)

(cid:3)

13

[18] Y. Huang and Q. Zhu, “Deceptive reinforcement

learning under
adversarial manipulations on cost signals,” in International Conference
on Decision and Game Theory for Security. Springer, 2019, pp. 217–
237.

[19] ——, “Manipulating reinforcement learning: Stealthy attacks on cost
signals,” Game Theory and Machine Learning for Cyber Security, pp.
367–388, 2021.

[20] A. Rakhsha, G. Radanovic, R. Devidze, X. Zhu, and A. Singla,
“Policy teaching via environment poisoning: Training-time adversarial
attacks against reinforcement learning,” in International Conference on
Machine Learning. PMLR, 2020, pp. 7974–7984.

[21] J. Wang, Y. Liu, and B. Li, “Reinforcement learning with perturbed
rewards,” in Proceedings of the AAAI Conference on Artiﬁcial Intelli-
gence, vol. 34, no. 04, 2020, pp. 6202–6209.

[22] H. Xu, R. Wang, L. Raizman, and Z. Rabinovich, “Transferable envi-
ronment poisoning: Training-time attack on reinforcement learning,”
in Proceedings of the 20th International Conference on Autonomous
Agents and MultiAgent Systems, 2021, pp. 1398–1406.

[23] X. Zhang, Y. Ma, A. Singla, and X. Zhu, “Adaptive reward-poisoning
attacks against reinforcement learning,” in International Conference
on Machine Learning. PMLR, 2020, pp. 11 225–11 234.

[24] X. Wang, Y. Wang, D. Hsu, and Y. Wang, “Exploration in inter-
active personalized music recommendation: a reinforcement learning
approach,” ACM Transactions on Multimedia Computing, Communi-
cations, and Applications (TOMM), vol. 11, no. 1, pp. 1–22, 2014.

and
Jurafsky, M. Galley,
[25] J. Li, W. Monroe, A. Ritter, D.
learning for dialogue generation,”
J. Gao, “Deep reinforcement
in Proceedings of
the 2016 Conference on Empirical Methods
in Natural Language Processing. Austin, Texas: Association for
Computational Linguistics, Nov. 2016, pp. 1192–1202.
[Online].
Available: https://aclanthology.org/D16-1127

[26] H. Modares, I. Ranatunga, F. L. Lewis, and D. O. Popa, “Optimized
assistive human–robot interaction using reinforcement learning,” IEEE
transactions on cybernetics, vol. 46, no. 3, pp. 655–667, 2015.
[27] M. C. Priess, R. Conway, J. Choi, J. M. Popovich, and C. Radcliffe,
“Solutions to the inverse lqr problem with application to biological
systems analysis,” IEEE Transactions on control systems technology,
vol. 23, no. 2, pp. 770–777, 2014.

[28] H. Zhang, H. Chen, C. Xiao, B. Li, M. Liu, D. Boning, and C.-J. Hsieh,
“Robust deep reinforcement learning against adversarial perturbations
on state observations,” Advances in Neural Information Processing
Systems, vol. 33, pp. 21 024–21 037, 2020.

[29] G. Liu and L. Lai, “Provably efﬁcient black-box action poisoning at-
tacks against reinforcement learning,” Advances in Neural Information
Processing Systems, vol. 34, 2021.

[30] Y. Huang, L. Huang, and Q. Zhu, “Reinforcement

learning for
feedback-enabled cyber resilience,” Annual Reviews in Control, 2022.
[31] D. P. Bertsekas, Dynamic programming: deterministic and stochastic

models. Prentice-Hall, Inc., 1987.

[32] R. E. Kalman, “When Is a Linear Control System Optimal?” Journal
of Basic Engineering, vol. 86, no. 1, pp. 51–60, 03 1964. [Online].
Available: https://doi.org/10.1115/1.3653115

[33] T. Fujii and M. Narazaki, “A complete optimality condition in the
inverse problem of optimal control,” SIAM journal on control and
optimization, vol. 22, no. 2, pp. 327–341, 1984.

[34] K. Sugimoto and Y. Yamamoto, “Solution to the inverse regulator
problem for discrete-time systems,” International Journal of Control,
vol. 48, no. 3, pp. 1285–1300, 1988.

[35] D. Iracleous and A. Alexandridis, “New results to the inverse optimal
control problem for discrete-time linear systems,” Applied Mathemat-
ics and Computer Science, vol. 8, no. 3, pp. 517–528, 1998.

[36] D. Kleinman, “On an iterative technique for riccati equation compu-
tations,” IEEE Transactions on Automatic Control, vol. 13, no. 1, pp.
114–115, 1968.

[37] S. Diamond and S. Boyd, “Cvxpy: A python-embedded modeling
language for convex optimization,” The Journal of Machine Learning
Research, vol. 17, no. 1, pp. 2909–2913, 2016.

[38] H. J. H. J. Kushner, Introduction to stochastic control. New York:

Holt, Rinehart and Winston, 1971.

14

This figure "Illustration.png" is available in "png"(cid:10) format from:

http://arxiv.org/ps/2203.05774v2

