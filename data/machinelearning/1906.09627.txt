Machine learning manuscript No.
(will be inserted by the editor)

Inductive general game playing

Andrew Cropper · Richard Evans · Mark Law

9
1
0
2

n
u
J

3
2

]
I

A
.
s
c
[

1
v
7
2
6
9
0
.
6
0
9
1
:
v
i
X
r
a

the date of receipt and acceptance should be inserted later

Abstract General game playing (GGP) is a framework for evaluating an agent’s gen-
eral intelligence across a wide range of tasks. In the GGP competition, an agent is given
the rules of a game (described as a logic program) that it has never seen before. The
task is for the agent to play the game, thus generating game traces. The winner of the
GGP competition is the agent that gets the best total score over all the games. In this
paper, we invert this task: a learner is given game traces and the task is to learn the
rules that could produce the traces. This problem is central to inductive general game
playing (IGGP). We introduce a technique that automatically generates IGGP tasks from
GGP games. We introduce an IGGP dataset which contains traces from 50 diverse games,
such as Sudoku, Sokoban, and Checkers. We claim that IGGP is difﬁcult for existing in-
ductive logic programming (ILP) approaches. To support this claim, we evaluate existing
ILP systems on our dataset. Our empirical results show that most of the games cannot
be correctly learned by existing systems. The best performing system solves only 40%
of the tasks perfectly. Our results suggest that IGGP poses many challenges to existing
approaches. Furthermore, because we can automatically generate IGGP tasks from GGP
games, our dataset will continue to grow with the GGP competition, as new games are
added every year. We therefore think that the IGGP problem and dataset will be valuable
for motivating and evaluating future research.

We are very grateful to the following for feedback and guidance throughout this project: Alessandra
Russo, David Pfau, Edward Grefenstette, Krysia Broda, Marc Lanctot, Marek Sergot, and Pushmeet Kohli.

A. Cropper
University of Oxford, UK
E-mail: andrew.cropper@cs.ox.ac.uk

R. Evans
Imperial College London, UK
E-mail: richardevans@google.com

M. Law
Imperial College London, UK
E-mail: mark.law09@imperial.ac.uk

 
 
 
 
 
 
2

1 Introduction

Andrew Cropper et al.

General game playing (GGP) [28] is a framework for evaluating an agent’s general in-
telligence across a wide variety of games. In the GGP competition, an agent is given
the rules of a game that it has never seen before. The rules are described in a ﬁrst-order
logic-based language called the game description language (GDL) [54]. The rules specify
the initial game state, what constitutes legal moves, how moves update the game state,
and how the game terminates [4]. Before the game begins, the agent is given a few sec-
onds to think, to process the rules, and devise a game-speciﬁc strategy. The agent then
starts playing the game, thus generating game traces. The winner of the competition is
the agent that gets the best total score over all the games. Figure 1 shows six example
GGP games. Figure 2 shows a selection of rules, written in GDL, for the game Rock Paper
Scissors.

Fig. 1 Sample GGP games described in clockwise order starting from the top left: Alquerque, Chinese
Checkers, Eight Puzzle, Farming Quandries, Knights Tour, and Tic Tac Toe.

In this paper, we invert the GGP competition task: the learner (a machine learning
system) is given game traces and the task is to induce (learn) the rules that could have
produced the traces. In other words, the learner must learn the rules of a game by ob-
serving others play. This problem is a core part of inductive general game playing (IGGP)
[29], the task of jointly learning the rules of a game and playing the game successfully.
We focus exclusively on the ﬁrst task. Once the rules of the game have been learned then
existing GGP techniques [25,40,41] can be used to play the games.

Figure 3 shows an example IGGP task, described as a logic program, for the game
Rock Paper Scissors. In this task, a learner is given a set of ground atoms representing
background knowledge (BK) and sets of disjoint ground atoms representing positive
(E+) and negative (E−) examples of target concepts. The task is for the learner to induce
a set of general rules (a logic program) that explains all of the positive but none of the
negative examples. In this scenario, the examples are observations of the next_score
and next_step predicates, and the task is to learn the rules for these predicates, such as
the rules shown in Figure 4.

Inductive general game playing

3

1)
(succ 0
2)
(succ 1
3)
(succ 2
(beats scissors paper)
(beats paper stone)
(beats stone scissors)
(<= (next (step ?n)) (true (step ?m)) (succ ?m ?n))
(<= (next (score ?p ?n)) (true (score ?p ?n)) (draws ?p))
(<= (next (score ?p ?n)) (true (score ?p ?n)) (loses ?p))
(<= (next (score ?p ?n)) (true (score ?p ?n2)) (succ ?n2 ?n) (wins ?p))
(<= (draws ?p) (does ?p ?a) (does ?q ?a) (distinct ?p ?q))
(<= (wins ?p) (does ?p ?a1) (does ?q ?a2) (distinct ?p ?q) (beats ?a1 ?a2))
(<= (loses ?p) (does ?p ?a1) (does ?q ?a2) (distinct ?p ?q) (beats ?a2 ?a1))

Fig. 2 A selection of rules for the game Rock Paper Scissors. The rules are written in the game
description language, a variant Datalog which is usually described in preﬁx notation. The relation
(succ
1) means succ(0,1), i.e. 1 is the successor of 0. Variables begin with "?". The rela-
tion (<= (next (step ?n)) (true (step ?m)) (succ ?m ?n)) can be rewritten in Prolog notation as
next(step(N)):- true(step(M)),succ(M,N).

0

BK

E+

E−

next_score(p1,0).
next_score(p2,1).
next_step(1).

next_score(p2,0).
next_score(p1,1).
next_score(p1,2).
next_score(p2,2).
next_score(p1,3).
next_score(p2,3).
next_step(0).
next_step(2).
next_step(3).

beats(paper,stone).
beats(scissors,paper).
beats(stone,scissors).
player(p1).
player(p2).
succ(0,1).
succ(1,2).
succ(2,3).
does(p1,stone).
does(p2,paper).
true_score(p1,0).
true_score(p2,0).
true_step(0).

Fig. 3 An example learning task for the game Rock Paper Scissors. The input is a set of ground atoms
representing background knowledge (BK) and sets of ground atoms representing positive (E+
) and neg-
atives (E−
) examples. In this task, the examples are observations of the next_score and next_step
predicates. The task is to learn the rules for these predicates, such as the rules shown in Figure 4.

In this paper, we expand on the idea proposed by Genesereth [29] and we introduce
the IGGP problem (Section 3.2). Our main claim is that IGGP is difﬁcult for existing in-
ductive logic programming (ILP) techniques, and in Section 2 we outline the reasons why
we think IGGP is difﬁcult, such as the lack of task-speciﬁc language biases. To support
our claim, we make three key contributions.

Our main contribution is a new IGGP dataset1. The dataset is based on game traces
from 50 games from the GGP competition. The games vary across a number of dimen-
sions, including the number of players (1-4), the number of spatial dimensions (0-2), the
reward structure (whether the rewards are zero-sum, cooperative, or orthogonal), and

1 The dataset is available at https://github.com/andrewcropper/iggp

4

Andrew Cropper et al.

next_step(N):-

true_step(M),
succ(M,N).
next_score(P,N):-

true_score(P,N),
draws(P).
next_score(P,N):-

true_score(P,N),
loses(P).
next_score(P,N2):-

true_score(P,N1),
succ(N2,N1),
wins(P).

draws(P):-

does(P,A),
does(Q,A),
distinct(P,Q).

loses(P):-

does(P,A1),
does(Q,A2),
distinct(P,Q),
beats(A2,A1).

wins(P):-

does(P,A1),
does(Q,A2),
distinct(P,Q),
beats(A1,A2).

Fig. 4 The GGP reference solution for the Rock Paper Scissors game described as a logic program. Note
that the predicates draws, loses, and wins are not given as background knowledge and the learner must
discover these.

complexity. Some of the games are turn-taking (Alquerque) while others (Rock Paper Scis-
sors) are simultaneous. Some of the games are classic board games (Checkers and Hex);
some are puzzles (Sokoban and Sudoku); some are dilemmas from game theory (Prison-
ner’s Dilemma and Chicken); others are simple implementations of classic video games
(Centipede and Tron). Figure 5 lists the 50 games and also shows for each game the num-
ber of dimensions, the number of players, and as an estimate of the game’s complexity
the number of rules and literals in the GGP reference solution. Each game is described
as four relational learning tasks goal, next, legal, and terminal with varying arities,
although ﬂattening the dataset to remove function symbols leads to more relations as
illustrated in Figure 3 where the next predicate is ﬂattened to relations next_score/2
and next_step/2. For each game, we provide (1) training/validate/test data composed
of sets of ground atoms in a 4:1:1 split, (2) a type signature ﬁle describing the arities
of the predicates and types of the arguments, and (3) a reference solution in GDL. It
is important to note that we have not designed these games: the games were designed
independently from our IGGP problem without this induction task in mind.

Our second contribution is a mechanism to continually expand the dataset. The GGP
competition produces new games each year, which provides a continual rich source of
challenges to the GGP participants. Our technical contribution allows us to easily add
these new games to our dataset. We implemented an automatic procedure for producing
a new learning task from a game. When a new game is added to the GGP competition,
our system can read the GDL description, generate traces of sample play, and extract
an IGGP task from those traces (see Section 4.3 for technical details). This automatic
procedure means that our dataset can expand each year as new games are added to
the GGP competition. We again stress that the GGP games were not designed with this
induction task in mind. The games were designed to be challenging for GGP systems.
Thus, this induction task is based on a challenging “real world” problem, not a task that
was designed to be the appropriate level of difﬁculty for current ILP systems.

Our third contribution is an empirical evaluation of existing ILP approaches, to test
our claim that IGGP is difﬁcult for current ILP approaches. We evaluate the classical ILP
system Aleph [70] and the more recent systems ASPAL [8], Metagol [14], and ILASP
[44]. Although non-exhaustive, these systems cover a breadth of ILP approaches and

Inductive general game playing

5

techniques. We also compare non-ILP approaches in the form of simple baselines and
clustering (KNN) approaches. Figure 6 summarises the results. Although some systems
can solve some of the simpler games, most of the games cannot be solved by existing
approaches. In terms of balanced accuracy (Section 6.1.1), the best performing system,
ILASP, achieves 86%. However, in terms of our perfectly solved metric (Section 6.1.2), the
best performing system, ILASP, achieves only 40%. Our empirical results suggest that our
current IGGP dataset poses many challenges to existing ILP approaches. Furthermore,
because of our second contribution, our dataset will continue to grow with the GGP com-
petition, as new games are added every year. We therefore think that the IGGP problem
and dataset will be valuable for motivating and evaluating future research.

Game
Minimal Decay
Minimal Even
Rainbow
Rock Paper Scissors
GT Chicken
GT Attrition
Coins
Buttons and Lights
Leafy
GT Prisoner
Eight Puzzle
Lightboard
Knights Tour
Sukoshi
Walkabout
Horseshoe
GT Ultimatum
Tron
9x Buttons and Lights
Hunter
GT Centipede
Fizz Buzz
Untwisty Corridor
Don’t Touch
Tiger vs Dogs

R
2
8
10
12
16
16
16
16
17
17
17
18
18
19
22
22
22
23
24
24
24
25
27
29
30

L
6
19
48
36
78
60
45
44
80
75
60
69
46
49
66
59
67
76
77
69
69
74
68
84
88

D
0
0
0
0
0
0
0
1
2
0
2
2
2
1
2
2
0
2
2
2
0
0
0
2
2

P
1
1
1
1
2
2
1
1
2
2
1
2
1
2
2
2
2
2
1
1
2
1
1
2
2

Game
Sheep and Wolf
Duikoshi
TicTacToe
HexForThree
Connect 4
Breakthrough
Centipede
Forager
Sudoku
Sokoban
9x TicTacToe
Switches
Battle of Numbers
Free For All
Alquerque
Kono
Checkers
Pentago
Platform Jumpers
Pilgrimage
Firesheep
Farming Quandries
TTCC4
Frogs and Toads
Asylum

R
30
31
32
35
36
36
37
40
41
41
42
44
44
46
49
50
52
53
62
80
85
88
94
97
101

L
89
76
92
130
124
126
134
106
101
172
149
183
134
130
134
134
167
188
168
240
290
451
301
431
273

D
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2

P
2
2
2
3
4
2
1
1
1
1
2
1
2
2
2
2
2
2
2
2
2
2
2
2
2

Fig. 5 The IGGP dataset. We list the number of rules (clauses) R, the number of literals L, number of
dimensions D, and the number of players P.

Metric
Balanced accuracy (%)
Perfectly solved (%)

Baseline
48
4

KNN5
80
19

Aleph
66
18

ASPAL Metagol

55
10

69
34

ILASP∗
86
40

Fig. 6 Results summary. The baseline represents accepting everything. The results show that all of the
approaches struggle in terms of the perfectly solved metric (which represents how many tasks were solved
with 100% accuracy).

The rest of the paper is organised as follows. Section 2 describes related work and
further motivates this new problem and dataset. Section 3 describes the IGGP problem,

6

Andrew Cropper et al.

the GDL, in which GGP games are described, and how IGGP games are Markov games.
Section 4 introduces a technique to produce a IGGP task from a GGP game and provides
speciﬁc details on how we generated our initial IGGP dataset. Section 5 describes the
baselines and ILP systems used in the evaluation of current ILP techniques. Section 6
details the results of the evaluation and also describes why IGGP is so challenging for
existing approaches. Finally, Section 6 concludes the paper and details future work.

2 Related work

2.1 General game playing

As Björnsson states [4], from the inception of AI games have played a signiﬁcant role
as a test-bed for advancing the ﬁeld. Although the early focus was on developing gen-
eral problem-solving approaches, the focus shifted towards developing problem-speciﬁc
approaches, such as approaches to play chess [6] or checkers [68] very well. One motiva-
tion of the GGP competition is to reverse this shift, as to encourage work on developing
general AI approaches that can solve a variety of problems.

Our motivation for introducing the IGGP problem and dataset is similar. As we will
discuss in the next section, there is much work in ILP on learning rules for speciﬁc games,
or for speciﬁc patterns in games. However, there is little work on demonstrating general
techniques for learning rules for a wide variety of games (i.e. the IGGP problem). We
want to encourage such work by showing that current ILP systems struggle on this prob-
lem.

2.2 Inducing game rules

Inducing game rules has a long history in ILP, where chess has often been the focus.
Bain [2] studied inducing ﬁrst-order Horn rules to determine the legality of moves in
the chess KRK (king-rook-king) endgame, which is similar to the problem of learning
the legal predicate in the IGGP games. Bain also studied inducing rules to optimally
play the KRK endgame. Other works on chess include Goodacre [30], Morales [55],
who induced rules to play the KRK endgame and rules to describe the fork pattern, and
Muggleton et al. [58].

Besides chess, Castillo and Wrobel [7] used a top-down ILP system and active learn-
ing to induce a rule for when a square is safe in the game minesweeper. Law et al. [44]
used an ASP-based ILP approach to induce the rules for Sudoku and showed that this
more expressive formalism allows for game rules to be expressed more compactly.

Kaiser [37] learned the legal moves and the win condition (but not the state tran-
sition function) for a variety of boardgames (breakthrough, connect4, gomuku, pawn
whopping, and tictactoe). This system represents game rules as formulas of ﬁrst-order
logic augmented with a transitive closure operator T C; it learns by enumerative search,
starting with the guarded fragment before proceeding to full ﬁrst-order logic with T C.
Unusually, their system learns the game rules from videos of correct and incorrect play:
before it can start learning the rules, it has to parse the video, converting a sequence of
pixel arrays into a sequence of sets of ground atoms.

Relatedly, Grohe and Ritzert [32] also use enumerative search, searching through the
space of ﬁrst-order formulas. They exploit Gaifman’s locality theorem to search through

Inductive general game playing

7

a restricted set of local formulas. They show, remarkably, that if the max degree of the
Gaifman graph is polylogarithmic in the number n of objects, then the running time of
their enumerative learning algorithm is also polylogarithmic in n. This intriguing result
does not, however, suggest a practical algorithm as the constants involved are very large.
GRL [31] builds on SGRL [4] and LOCM [10] to learn game dynamics from traces.
In these systems, the game dynamics are modelled as as ﬁnite deterministic automata.
They do not learn the legal predicate (determining which subset of the possible moves
are available in the current state) or the goal predicate.

As is clear from these works, there is little work in ILP demonstrating general tech-
niques for learning rules for a wide variety of games. This limitation partially motivates
the introduction of the IGGP problem and dataset.

2.3 Existing datasets

One of our main contributions is the introduction of a IGGP dataset. In contrast to the
existing datasets, our dataset introduces many new challenges.

2.3.1 Size and diversity

Our dataset is larger and more diverse than most existing ILP datasets, especially on
learning game rules. Commonly used ILP datasets, such as kinship data [34], Michaslki
trains [42], Mutagenesis [21], Carcinogenesis [71], string transformations [52], and
chess positions [57], typically contain a single predicate to be learned, such as eastbound/1
or westbound/1 in the Michaslki trains dataset or active/1 in the Mutagenesis dataset.
By contrast, our dataset contains 50 distinct games, each described by at least four target
predicates, where ﬂattening leads to more relations as illustrated in Figure 3. In addi-
tion, whereas some datasets use only dyadic concepts, such as kinship or string trans-
formations, our dataset also requires learning programs with a mixture of predicates
arities, such as input_jump/8 in Checkers and next_cell/4 predicate in Sudoku. Learn-
ing programs with high-arity predicates is a challenge for some ILP approaches [14,38,
24]. Moreover, because of our second main contribution, we can continually and auto-
matically expand the dataset as new games are introduced into the GGP competition.
Therefore, our IGGP dataset will continue to expand to include more games.

2.3.2 Inductive bias

Our IGGP games come from the GGP competition. As stated in the introduction, the
games were not designed with this induction task in mind. One key challenge pro-
posed by the IGGP problem is the lack of inductive bias provided. Most existing work
on inducing game rules has assumed as input a set of high-level concepts. For instance,
Morales [55] assumed as input a predicate to determine when a chess piece is in check.
Likewise, Law [44] assumed high-level concepts such as same_row/2 and same_col/2
as background knowledge when learning whether a Sudoku board was valid. Moreover,
most existing ILP work on game learning rules (and learning in general) involves the de-
signers of the system designing the appropriate representation of the problem for their
system. By contrast, in our IGGP problem the representation is ﬁxed: it is the GDL pro-
vided by the GGP.

8

Andrew Cropper et al.

Many existing ILP techniques assume a task-speciﬁc language bias, expressing a hy-
pothesis space which contains at least one correct representation of the target concept.
When available, language biases are extremely useful as a smaller hypothesis space can
mean fewer examples and less computational resources are needed by the ILP systems.
In many practical situations, however, task-speciﬁc language biases are either not avail-
able, or are extremely wide, as very little information is known about the structure of
the target concept.

In our IGGP dataset we only provide the most simple (or primitive) low-level con-
cepts, which come directly from the GGP competition, i.e. our IGGP dataset does not
provide any task-speciﬁc language biases. For each game, the only language bias given
is the type schema of each predicate in the language of the background knowledge. For
instance, in Sudoku the higher-level concepts of same row and same col are not given.
Likewise, to learn the terminal predicate in Connect Four, a learner must learn the con-
cept of a line, which in turn requires learning rules for vertical, horizontal, and diagonal
lines. This means that for an approach to solve the IGGP problem in general (and to
be able to accept future games without changing their method), it must be able to learn
without a game-speciﬁc bias, or be able to generate this game-speciﬁc bias from the type-
schemas in the task. In addition, a learner must learn concepts from only primitive low-
level background predicates, such as cell(X,Y,Filled). Should these high-level con-
cepts be reusable then it would be advantageous to perform predicate invention, which
has long been a key challenge in ILP [59,60]. Popular ILP systems, such as FOIL [64]
and Progol [56], do not support predicate invention, and although recent work [35,61,
13] has tackled this challenge, predicate invention is still a difﬁcult problem.

2.3.3 Large programs

Many reference solutions for IGGP games are large, both in terms of the number of lit-
erals and the clauses in them. For instance, the GGP reference solution for the goal
predicate for Connect Four uses 14 clauses and a total of 72 literals. This solution uses
predicate invention to essentially compress the solution, where the auxillary predicates
include the concept of a line, which in turn uses the auxillary predicates for the concepts
of columns, rows, and diagonals. If we unfold the reference solution as to remove auxil-
lary predicates then the total number of literals required to learn a solution for this single
predicate easily exceeds 400. However, learning large programs is a challenge for most
ILP systems [11] which typically struggle to learn programs with hundreds of clauses or
literals.

2.3.4 ILP2016 competition

The closest work similar to ours is the ILP 2016 Competition [50]. The ILP 2016 compe-
tition was based on a single type of task (with various hand crafted target hypotheses)
aimed at learning the valid moves of an agent as it moved through a grid. In some ways
this is similar to our legal tasks, although many tasks required learning invented pred-
icates representing changes in state, similar to our next tasks. By contrast, our IGGP
problem and dataset is based on a variety of real games, which we did not design. Fur-
thermore, the ILP 2016 dataset provides restricted inductive biases to aid the ILP systems,
whereas we (deliberately) do not give such help.

Inductive general game playing

9

2.4 Model learning

AlphaZero [69] has shown the power of combining tree search with a deep neural net-
work for distilling search policy into a neural net. But this technique presupposes that we
have been given a model of the game dynamics: we must already know the state tran-
sition function and the reward function. Suppose we want to extend AlphaZero-style
techniques to domains where we are not given an explicit model of the environment. We
would need some way of learning a model of the environment from traces. Ideally, we
would like to learn data-efﬁciently, without needing hundreds of thousands of traces.

Model-free reinforcement learning agents have high sample complexity: they of-
ten require millions of episodes before they can learn a reasonable policy. Model-based
agents, by contrast, are able to use their understanding of the dynamics of the environ-
ment to learn much more efﬁciently [23,22,33]. Whether, and to what extent, model-
based methods are more sample efﬁcient than model-free methods depends on the com-
plexity of the particular MDP. Sometimes, in simple environments, one needs fewer data
to learn a policy than to learn a model. It has also been shown that, for Q learning, the
worst-case asymptotics for model-based and model-free are the same [39]. But these
qualiﬁcations do not, of course, undermine the claim that in complex environments that
require anticipation or planning, a model-based agent will be signiﬁcantly more sample-
efﬁcient than its model-free counterpart.

The GGP dataset was designed to test an agent’s ability to learn a model that can be
useful in planning. The most successful GGP algorithms, e.g. Cadiaplayer [25], Sancho
[40], and WoodStock [41], use Monte Carlo Tree Search (MCTS) to search. MCTS relies
on an accurate forward model of the Markov Decision Process. The further into the
future we search, the more important it is that our forward model is accurate, as errors
compound. In order to avoid having to give our MCTS agents a hand-coded model of
the game dynamics, they must be able to learn an accurate model of the dynamics from
a handful of behavior traces.

Two things make the GGP dataset an appealing task for model learning. First, hun-
dreds of games have already been designed for the GGP competition, with more being
added each year. Second, each game comes with ‘ground truth’: a set of rules that com-
pletely describe the game. From these rules, we know the learning problem is solvable,
and we have a good measure of how hard it is (by measuring the complexity of the
ground-truth program2).

3 IGGP dataset

In this section, we describe the Game Description Language (GDL) in which GGP games
are described, the IGGP problem setting, and ﬁnally an illustrative example of a typical
IGGP task.

2 This measure of complexity assumes, of course, that the length of the ground-truth program is rea-
sonably close to the shortest GDL description of the game. In other words, this assumes the actual program
length is a reasonable estimate of the Kolmogorov complexity.

10

Andrew Cropper et al.

3.1 Game description language

GGP games are described using GDL. This language describes the state of a game as a
set of facts and the game mechanics as logical rules. GDL is a variant of Datalog with
two syntactic extensions (stratiﬁed negation and restricted function symbols) and with a
small set of distinguished predicates that have a special meaning [54] (shown in Figure
7).

The ﬁrst syntactic extension is stratiﬁed negation. Standard Datalog (lacking nega-
tion altogether) has the useful property that there is a unique minimal model [18]. If we
add unrestricted negation, we lose this attractive property: now there can be multiple
distinct minimal models. To maintain the property of having a unique minimal model,
GDL adds a restricted form of negation called stratiﬁed negation [1]. The dependency
graph of a set of rules is formed by creating an edge from predicate p to predicate q
whenever there is a rule whose head is p(...) and that contains an atom q(...) in the
body. The edge is labelled with a negation if the body atom is negated. A set of rules is
stratiﬁed if the dependency graph contains no cycle that includes a negated edge.

GDL’s second syntactic extension to Datalog is restricted function symbols. The Her-
brand base of a standard Datalog program is always ﬁnite. If we add unrestricted function
symbols, the Herbrand base can be inﬁnite. To maintain the property of having a ﬁnite
Herbrand base, GDL restricts the use of function symbols in recursive rules [54].

The two syntactic extensions of GDL, stratiﬁed negation and restricted function sym-
bols, mean we extend the expressive power of Datalog without essentially changing its
key attractive property: there is always a single, ﬁnite minimal model [54].

Predicate
distinct(?x,?y)
does(?r,?m)
goal(?r,?n)
init(?f)
legal(?r,?m)
next(?f)
role(?n)
terminal
true(?f)

Description
Two terms are syntactically different
Player ?r performs action ?m in the current game state
Player ?r has reward ?n (usually a natural number) in the current state
Atom ?f is true in the initial game state
Action ?m is a legal move for player ?r in the current state
Atom ?f will be true in the next game state
Constant ?n denotes a player
The current state is terminal
Atom ?f is true in the current game state

Fig. 7 Main predicates in GDL where variables begin with a "?" symbol.

3.2 Problem setting

We now deﬁne the IGGP problem. Our problem setting is based on the ILP learning
from entailment setting [65], where an example corresponds to an observation about
the truth or falsity of a formula F and a hypothesis H covers F if H entails F . We assume
languages of background knowledge (cid:66) and examples (cid:69) each formed of function-free
ground atoms. The atoms are function-free because we ﬂatten the GDL atoms. For exam-
ple, in Figure 9, the atom true(count(9)) has been ﬂattened into true_count(p9). We
ﬂatten atoms because some ILP systems do not support function symbols. We likewise
assume a language of hypotheses (cid:72) formed of datalog programs with stratiﬁed nega-
tion. Stratiﬁed negation is not necessary but in practice allows signiﬁcantly more concise

Inductive general game playing

11

programs, and thus often makes the learning task computationally easier. Note that the
GDL also supports recursion but in practice most GGP games do not use recursion. In
future work we intend to contribute recursive games to the GGP competition.

We now deﬁne the IGGP input:

Deﬁnition 1 (IGGP input) An IGGP input ∆ is a set of m triples {(Bi, E+

i , E−

i

)}m

i=1 where

– Bi
– E+
i

⊂ (cid:66) represents background knowledge
⊆ (cid:69) and E−
i

⊆ (cid:69) represent positive and negative examples respectively

An IGGP input forms the IGGP problem:

Deﬁnition 2 (IGGP problem) Given an IGGP input ∆, the IGGP problem is to return
|= E+ and
a hypothesis H ∈ (cid:72) such that for all (Bi, E+
H ∪ Bi

) ∈ ∆ it holds that H ∪ Bi

i , E−

i

(cid:54)|= E−
i .

Note that a single hypothesis should be consistent with all given triples.

3.2.1 Illustrating example: Fizz Buzz

To give the reader an intuition for the IGGP problem and the GGP games, we now de-
scribe example scenarios for the game Fizz Buzz. Although typically a multi-player game,
in our IGGP dataset Fizz Buzz is a single-player game. The aim of the game is for the
player to replace any number divisible by three with the word ﬁzz, any number divis-
ible by ﬁve with the word buzz, and any number divisible by both three and ﬁve with
ﬁzzbuzz. For example, a game of Fizz Buzz up to the number 17 would go: 1, 2, ﬁzz, 4,
buzz, ﬁzz, 7, 8, ﬁzz, buzz, 11, ﬁzz, 13, 14, ﬁzz buzz, 16, 17.

Figures 9, 10, 11, and 12 show example IGGP problems and solutions for the target
predicates legal, next, goal, and terminal respectively. For simplicity each example
is a single (B, E+, E−) triple, although in the dataset each learning task is often a set of
multiple triples, where a single hypothesis should explain all the triples. In all cases the
BK shown in Figure 8 holds, so we omit it from the individual examples for brevity. Note
that the game only runs to the number 31.

4 Generating the GGP Dataset

In this section, we describe our procedure to automatically generate IGGP tasks from
GGP game descriptions. We ﬁrst explain how GGP games ﬁt inside the framework of
multi-agent Markov decision processes. We also explain the need for a type-signature
for each game.

4.1 Preliminaries: Markov games

GGP games are Markov games [53], a strict superset of multi-agent Markov decision
process (MDP)s that allow simultaneous moves3. The four components (S, A, T, R) of the
MDP are:

3 There are variants in which some games are stochastic, and some have imperfect information. But

in the core GGP framework all games are deterministic and have perfect information.

12

Andrew Cropper et al.

divisible(12,1).
divisible(12,2).
...
divisible(12,12).
input_say(player,1).
input_say(player,2).
...
input_say(player,30).
input_say(player,fizz).
input_say(player,buzz).
input_say(player,fizzbuzz).
role(player).
int(0).
int(1).
...
int(31).

less_than(0,1).
less_than(0,2).
...
less_than(30, 31).
minus(1,1,0).
minus(2,1,1).
...
minus(31,31,0).
positive_int(1).
positive_int(2).
...
positive_int(31).
succ(0,1).
succ(0,2).
...
succ(30,31).

Fig. 8 Common BK for Fizz Buzz.

B

E+

E−

H

true_count(9).
true_success(6).

legal_say(player,9).
legal_say(player,buzz).
legal_say(player,fizz).
legal_say(player,fizzbuzz).

legal_say(player,0).
legal_say(player,1).
...
legal_say(player,8).
legal_say(player,10).
...
legal_say(player,31).

legal_say(player,N):-
true_count(N).
legal_say(player,fizz).
legal_say(player,buzz).
legal_say(player,fizzbuzz).

Fig. 9 In this Fizz Buzz scenario the learner is given four positive examples of the legal_say/2 predicate
and many negative examples. This predicate represents what legal moves a player can make in the game.
The column H shows the reference GGP solution described as a logic program. In Fizz Buzz, the player
can always make three legal moves in any state, saying ﬁzz, buzz, or ﬁzzbuzz. The player can additionally
say the current number (the counter).

– S is a ﬁnite set of states
– A is a ﬁnite set of actions
– T is transition function T : S × A → S
– R is a reward function

We describe these elements in turn for a GGP game.

4.1.1 States

Each state s ∈ S is a set of ground atoms representing ﬂuents (propositions whose truth-
value can change from one state to another). The true predicate indicates which ﬂuents
are true in the current state. For instance, one state of a best-of-three game of Rock Paper
Scissors is:

true(score(p1,0)).
true(score(p2,2)).
true(step(2)).

Inductive general game playing

13

B

E+

E−

H

does_say(player,buzz).
true_count(12).
true_success(3).

next_count(13).
next_success(3).

next_count(0).
next_count(1).
...
next_count(12).
next_count(14).
...
next_count(31).
next_success(0).
next_success(1).
next_success(2).
next_success(4).
...
next_success(31).

next_count(After):-

true_count(Before),
succ(Before,after).

next_success(After):-

correct,
true_success(Before),
succ(Before,After).

next_success(A):-
\+ correct,
true_success(A).

correct:-

true_count(N),
divisible(N,15),
does_player_say(fizzbuzz).

correct:-

true_count(N),
divisible(N,3),
\+ divisible(N,5),
does_player_say(fizz).

correct:-

true_count(N),
divisible(N,5),
\+ divisible(N,3),
does_player_say(buzz).

correct:-

true_count(N),
\+ divisible(N,5),
\+ divisible(N,3),
does_player_say(N).

Fig. 10 In this Fizz Buzz scenario, the learner is given one positive example of the next_count/1 predi-
cate, one positive example of the next_success/1 predicate, and many negative examples of both predi-
cates. These predicates represent the change of game state. The column H shows the reference GGP solu-
tion described as a logic program, which may not necessarily be the most textually compact solution. The
next_count/1 relation represents the count in the game. The this relation has a single clause two literal
deﬁnition, which says that the count increases by one after each step in the game. The next_success/1
relation requires two clauses with many literals. This relation counts how many times a player says the
correct output. The reference GGP solution for this relation includes the correct/0 predicate which is
not provided as BK but which is reused in both clauses of next_success/1. For an ILP system to learn
the reference solution it would need to invent this predicate. Also note that this solution uses negation
in the body, including the negation of the invented predicate correct/0.

This state represents that the current score is 0 to 2 in favour of player p2, and 2 time-
steps have been performed.

4.1.2 Actions

Each action a ∈ A is a set of ground atoms representing the set of all joint actions for
agents 1..n. The does predicate indicates which agents perform which actions. For in-
stance, one set of joint actions for Rock Paper Scissors is:

does(p1,paper).
does(p2,stone).

14

B

E+

E−

H

Andrew Cropper et al.

true_count(31).
true_success(20).

goal(player,50).

goal(player,0).
goal(player,100).
goal(player,25).
goal(player,75).

goal(player,100):-

true_success(30).

goal(player,75):-

true_success(S),
less_than(S,30),
less_than(24,S).

goal(player,50):-

true_success(S),
less_than(S,25),
less_than(19,S).

goal(player,25):-

true_success(S),
less_than(S,20),
less_than(14,S).

goal(player,0):-

true_success(S),
less_than(S,15).

Fig. 11 In this Fizz Buzz scenario the learner is given one example of the goal/2 predicate and four
negative examples. This predicate represents the reward for a move. In Fizz Buzz the reward is based
on the value of true_success/1. The column H shows the reference GGP solution described as a logic
program. The reference solution requires ﬁve clauses, which means that it would be difﬁcult for ILP
systems that only support learning single-clause programs [56,64].

B

E+

E−

H

true_count(27).
true_success(8).

terminal.

terminal:-

true_count(31).

Fig. 12 In this Fizz Buzz scenario the learner is given a single negative example of the terminal/0 pred-
icate. This predicate indicates when the game has ﬁnished. In this scenario the game has not terminated.
In the dataset the Fizz Buzz game runs until the count is 31, so the learner must learn a rule such as the
one shown in column H.

4.1.3 Transition function

In a stochastic MDP, the transition function T has the signature T : S × A × S → {0, 1}.
By contrast, in a deterministic MDP, such as a GGP game, the transition function is T :
S × A → S. Given a current state s and a set of actions a, the next predicate indicates
which ﬂuents are true in the (unique) next state s(cid:48). For instance, in Rock Paper Scissors,
given the current state s and actions a above, the next state s(cid:48) is:

next(score(p1,1)).
next(score(p2,2)).
next(step(3)).

The transition function is a set of deﬁnite clauses deﬁning next in terms of true. For
instance, the following two clauses deﬁne part of the transition function for Rock Paper
Scissors:

Inductive general game playing

15

next(score(P,N2)):-
does(P,paper),
does(Q,stone),
true(score(P,N1)),
succ(N1,N2).
next(step(N2)):-

true(step(N1)),
succ(N1,N2).

4.1.4 Reward function

In a continuous multi-agent MDP, the reward function has the signature4 R : S → (cid:82)n. In
a discrete MDP, such as a GGP game, we assume a small ﬁxed set of k discrete rewards
}, where ri is not necessarily numeric. Let G[i] be the set of atoms representing
{r1, . . . , rk
that player i has one of the k rewards G[i] = {g oal(i, r j
) | j = 1..k}. Let G = G[1] ×
... × G[n] be the joint rewards for agents 1..n. In our GGP dataset, the reward function
has the signature R : S → G. Note that, in this framework, learning the reward function
becomes a classiﬁcation problem rather than a regression problem. For example, in the
Rock Paper Scissors state above, the reward for state s(cid:48) depends only on the score and is:

goal(p1,1).
goal(p2,2).

4.1.5 Legal

In the GGP framework, actions are sometimes unavailable. It is not the case that all
possible actions from A can be performed, but some of them have no effect – but rather
that only a subset of actions are available in a particular state.

The legal function L determines which actions are available in which states: L : S →
2A. Recall that an element of A is not an individual action performed by a single player, but
rather a set of simultaneous joint actions, one for each player. For example, one element
of A is {does(p1,paper)., does(p2,stone).}. Note that the availability of an action for
one agent does not depend on what other actions are being performed concurrently by
other agents; it only depends on the state S.

4.1.6 Terminal

The GDL language contains a distinguished predicate, the nullary terminal predicate,
that indicates when an episode has terminated (i.e. when the game is over).

4.2 Preliminaries: the type-signature for a GGP game

In order to calculate the complete set of ground atoms for a game5, we use a type signa-
ture Σ. The type signature deﬁnes the types of constants, functions, and predicates used

4 Sometimes, alternatively, the reward function has the slightly more expressive form R : S×A×S → (cid:82)n.
5 We could dispense with the type signature, and generate all possible untyped ground atoms. Naively
generating all possible untyped ground atoms would signiﬁcantly increase the size of the dataset. We use
the type signature as a space optimisation to keep the dataset manageable.

16

Andrew Cropper et al.

in the GDL description. Our type signatures include a simple subtyping mechanism for
inclusion polymorphism. For example:

true, next :: prop -> bool.
at :: pos -> pos -> cell -> prop.
red, black :: agent.
1, 2, 3, 4, 5 :: pos.
blank :: cell.
agent :> cell.

In this example, true and next are predicates, at is a function that takes an (x, y)
coordinate and a cell-type and returns a ﬂuent (prop). A cell is either blank or one of
the agents. The expression agent :> cell means that an agent is a subtype of cell.

Let (cid:118) be the reﬂexive transitive closure of :>. Let Σ( f ) be the type assigned to ele-

ment f by signature Σ. Then f (k1, ..., kn
– Σ( f ) = (t1, ..., tn
)
– Σ(ki

) (cid:118) ti for all i = 1..n

) is a well-formed term of type t if:

Predicates are functions that return a bool and constants are functions with no argu-
ments. For example, using the type signature above, true(at(3, 4, black)) is a well-
formed term of type bool, i.e. a well-formed ground atom.

4.3 Automatically generating induction tasks for a GGP game

Given a GGP game Γ written in GDL, and a type signature Σ for that game, our system
automatically generates an IGGP induction task. Before presenting the details, we sum-
marise the general approach. To generate the GGP dataset, we built a simple forward-
chaining GDL interpreter. We used the GDL interpreter to calculate the initial state, the
currently valid moves, the transition function, and the reward. When generating traces,
we ﬁrst calculate the actions that are currently available for each player. Then we let
each player choose actions uniform randomly. We record the state trace (s1, ..., sn
), and
extract a set of (Bi, E+
) triples from each trace. The target predicates we wish to
learn are legal, next, goal, and terminal. The (Bi, E+
i , E−
) triples for the predicates
legal, goal, and terminal are calculated from a single state, while the triples for next
are calculated from a pair of consecutive states (si, si+1

i , E−

).

i

i

We generated multiple traces for each game: 1000 episodes with a maximum of 100
time-steps. However, we chose these numbers somewhat arbitrarily because there is a
complex tradeoff on how much data to generate. We want to generate enough data to
capture the diversity of a game, so that a learner can (in theory) learn the correct game
rules. However, we do not want to generate too much data as to provide every game
state, as this would mean that a learner would not need to learn anything, and could
instead simply memorise game situations. We also we do not want to generate too much
data that it becomes expensive to compute or store. It is, however, unclear where the
boundary is between too little and too much data. Whether such a boundary even exists
itself is unclear because by imposing different biases, different learners may need more
or less information on the same task. In future work we would like to expand the dataset.
We then intend to repeat the experiments with different amounts of training data.

Our approach is presented in Algorithm 1. This procedure generates a number of
traces. Each trace is a sequence of game states, and each game state is represented by a

Inductive general game playing

17

input : Γ , a GGP game written in the GDL language
input : Σ, a type signature for Γ
input : maxtraces, the number of traces to generate
input : maxtime, the max number of time-steps in a trace
output: a set of triples of the form {(Bi , E+

)}m

i , E−

i

i=1

Λ ← {}
for i = 1..maxtraces do
s ← initial(Γ )
t ← (s)
for j = 2..maxtime do
s ← next(Γ , s)
append(t, s)
if terminal(Γ , s) then

break

end
Λ ← Λ ∪ extract(t, Σ)

end

end
return Λ

Algorithm 1: Automatically generating induction tasks from GGP games

i

i , E−

set of ground atoms. We use the extract function (described in Section 4.3.1) to produce
a set of (Bi, E+
) triples from a trace. We add this set of triples to Λ. At the end,
when we have ﬁnished all the traces, we return Λ, the set of triples. The variable s
stores the current state (a set of ground atoms). Initially, s is set to the initial state:
initial(Γ ) produces the initial state from the GDL description. Then for each time-step,
we calculate the next state via next(Γ , s). This function next(Γ , s) involves three steps.
First, we calculate the available actions for each player. Second, we let each player take
a (uniform) random move. Third, we use the transition function T to calculate the next
state from the current state s and the actions of the players. Once we have calculated the
new state, we append it to the end of t. Here, t is a trace i.e. a sequence of states. Then
we check if the new state is terminal. If it is terminal, we ﬁnish the episode; otherwise,
we continue for another time-step. Once the episode is ﬁnished, we extract the set of
(Bi, E+
) triples from the sequence of states, and continue to the next trace. Note that
we need the type signature Σ to extract the triples from the trace, but we do not need
it to generate the trace itself. For our experiments, we generated 1000 traces for each
game, and ran for a maximum of 100 time-steps per game.

i , E−

i

4.3.1 The extract function

) (a sequence of sets
The extract(t, Σ) function in Algorithm 1 takes a trace t = (s1, ..., sn
of ground atoms), and a type signature Σ and produces a set of (Bi, E+
) triples. This
set of triples represents a set of induction tasks for the distinguished predicates legal,
goal, terminal, and next. It is deﬁned as:

i , E−

i

extract((s1, ..., sn

), Σ) = Λ

1

∪ Λ

∪ Λ

3

2

∪ Λ

4

18

where:

Andrew Cropper et al.

Λ

Λ

Λ

Λ

1

2

3

4

= {triple1
= {triple1
= {triple1
= {triple2

(si, legal, Σ) | i = 1..n}
(si, goal, Σ) | i = 1..n}
(si, terminal, Σ) | i = 1..n}
(si, si+1, Σ) | i = 1..n − 1}

Before we deﬁne the triple1 and triple2 functions, we introduce the relevant notation. If
s is a set of ground atoms and p is a predicate, let sp be the subset of atoms in s that use
the predicate p. If Σ is a type signature and p is a predicate, then ground(Σ, p) is the set
of all ground atoms generated by Σ that use predicate p. Given this notation, we deﬁne
triple1

(s, p, Σ) = (B, E+, E−) where:

B = s − sp
E+ = sp
E− = ground(Σ, p) − E+

To calculate the negative instances E−
i , we use the closed-world assumption: all p-atoms
not known to be true in E+ are assumed to be false in E−. Given a type signature Σ,
we generate the set ground(Σ, p) of all possible ground atoms whose predicate is the
distinguished predicate p. For example, in a one player game, if ground(Σ, legal) =
{legal(p1, up), legal(p1, down), legal(p1, left), and legal(p1, right)}, and
slegal only contains legal(p1, up) and legal(p1, down), then:

E+
i
E−
i

= {legal(p1, up), legal(p1, down)}
= ground(Σ, legal) − E+
i

= {legal(p1, left), legal(p1, right)}

We deﬁne triple2

(si, si+1, Σ) = (B, E+, E−) where:

B = si
E+ = si+1
E− = ground(Σ, next) − E+

[true/next]

When learning next, we use the facts at the earlier time-step si as background facts, we
use the facts at the later time-step si+1 as the positive facts E+ to be learned (with the
predicate true replaced by next), and we use all the rest of the ground atoms involving
next as the negative facts E−. Note, again, the use of the closed-world assumption: we
assume all next atoms not known to be in E+ to be in E−.

5 Baselines and ILP systems

We claim that IGGP is challenging for existing ILP approaches. To support this claim we
evaluate existing ILP systems on our IGGP dataset. We compare the ILP systems against
simple baselines. We ﬁrst describe the baselines and then each ILP system.

Inductive general game playing

19

Baselines
True(B, a) = (cid:62)
I ner t ia(B, a) = a[nex t/t rue] ∈ B
M ean(B, a) = {(Bi, E+
(B, a) = |{(B(cid:48), E+(cid:48)
K N Nk
Fig. 13 Baselines where ∆ = {(Bi , E+
to replace the predicate symbol next with true in the atom a.

) ∈ ∆ | a ∈ E+
i

, E−(cid:48) ) ∈ κ

i , E−

i , E−

)}m

k

i

i

} ≥ |∆|
2
(∆, B) | a ∈ E+(cid:48) }| ≥ κ

k

(∆,B)
2

i=1 represents training data. The syntax a[next/true] means

5.1 Baselines

Figure 13 shows the four baselines. Each baseline is a Boolean function f : 2(cid:66) × (cid:69) →
{(cid:62), ⊥}, i.e. a function that takes background knowledge and an example and returns
true ((cid:62)) or false (⊥). We describe these baselines in detail.
Our ﬁrst two baselines ignore the training data:

– True deems that every atom is true:

True(B, a) = (cid:62)

– Inertia is the same as True for atoms with the target predicates goal, legal, and
terminal, but for the next predicate an atom is true if and only if the corresponding
true atom is in B. For instance, the atom next(at(1,4,x)) is true if and only if
true(at(1,4,x)) is in B:

I ner t ia(B, a) = a[nex t/t rue] ∈ B

The intuition behind this baseline is the empirical observation that in most of the
games, most ground atoms retain their truth value from one time-step to the next,
more often than not. Of course, it is possible to design games in which most or all of
the atoms change their truth value each time-step; but in typical games, such radical
changes are unusual.

Our next two baselines consider the training data ∆ = {(Bi, E+

i , E−

i

)}m

i=1:

– Mean deems that a testing atom a is true if and only if a is true more often than not

in the positive training examples:

M ean(B, a) = |{(Bi, E+

i , E−

i

) ∈ ∆ | a ∈ E+
i

}| ≥

|∆|

2

– KNNk is based on clustering the data. In K N Nk

(B, a) we ﬁnd the k triples in ∆,
denoted as κ
(∆, B), whose backgrounds are most ‘similar’ to the background B. To
assess the similarity of two sets A and B of ground atoms, we look at the size of the
symmetric difference6 between A and B:

k

d(A, B) = |A − B| + |B − A|

It is straightforward to show that the d function satisﬁes the conditions for a distance
metric:

6 For efﬁciency, we calculate this difference by converting the sets into bit vectors, applying xor, and

counting the number of set bits.

20

Andrew Cropper et al.

– d(A, B) ≥ 0
– d(A, B) = d(B, A)
– d(A, B) = 0 iff A = B
– d(A, C) ≤ d(A, B) + d(B, C)
(∆, B) to be the k triples {(Bi, E+
We set the closest k triples κ
smallest d distance between Bi and B. Given the k closest triples κ
baseline outputs (cid:62) if a appears in E+(cid:48)
formally:

)}k
i=1 with the
i
(∆, B) the KNN
in at least half of the closest k triples. More

i , E−

k

k

K N Nk

(B, a) = |{(B(cid:48)

, E+(cid:48)

, E−(cid:48) ) ∈ κ

(∆, B) | a ∈ E+(cid:48) }| ≥ k
2

k

One potential limitation of the KNN approach is that, in contrast to the ILP approaches,
the KNN approaches learn at the propositional level and are unable to learn general
ﬁrst-order rules. To illustrate this limitation, suppose we are trying to learn the target
predicate p/1 given the background predicate q/1 and that the underlying target rule is
p(X ) ← q(X ). Suppose there are only two training triples of the form (B, E+, E−):

T1
T2

= ({q(a)}, {p(a)}, {p(b), p(c)})
= ({q(b)}, {p(b)}, {p(a), p(c)})

Given the test triple ({q(c)}, {p(c)}, {p(a), p(b)}), a KNN approach will deem that p(c)
is false because it has not seen a positive instance of this particular ground atom and has
no representational resources for generalising.

5.2 ILP systems

We evaluate four ILP systems on our dataset. It is important to note that we are not try-
ing to directly compare the ILP systems, or demonstrate that any particular ILP system is
better than another. We are instead trying to show that the IGGP problem is challenging
for existing systems, and that it (and the dataset) will provide a challenging problem for
evaluating future research. Indeed, a direct comparison of ILP systems is often difﬁcult
[11], largely because different systems excel at certain classes of problems. For instance,
directly comparing the Prolog-based Metagol against ASP-based systems, such as ILASP
and HEXMIL [38] is difﬁcult because Metagol is often used to learn recursive list manip-
ulation programs, including string transformations and sorting algorithms [15]. By con-
trast, many ASP solvers disallow explicit lists, such as the popular Clingo system [26],
and thus a direct comparison is difﬁcult. Likewise, ASP-based systems can be used to
learn non-deterministic speciﬁcations represented through choice rules and preferences
modeled as weak constraints [48], which is not necessarily the case for Prolog-based
systems. In addition, because many of the systems have learning parameters, it is often
possible to show that there exist some parameter settings for which system X can perform
better than Algorithm Y on a particular dataset. Therefore, the relative performances of
the systems should largely be ignored.

We compare the ILP systems Aleph, ASPAL, Metagol, and ILASP. We describe these

systems in turn.

Inductive general game playing

21

5.2.1 Aleph

Aleph is an ILP system written in Prolog based on Progol [56]. Aleph uses the following
procedure to induce a logic program hypothesis (paraphrased from the Aleph website7):

1. Select an example to be generalised. If none exist, stop, otherwise proceed to the

next step.

2. Construct the most speciﬁc clause (also known as the bottom clause [56]) that entails

the example selected and is within language restrictions provided.

3. Search for a clause more general than the bottom clause. This step is done by search-
ing for some subset of the literals in the bottom clause that has the ‘best’ score.
4. The clause with the best score is added to the current theory and all the examples

made redundant are removed. Return to step 1.

To restrict the hypothesis space (mainly at step 2), Aleph uses both mode declarations
[56] and determinations to denote how and when a literal can appear in a clause. In
the mode language, modeh are declarations for head literals and modeb are declarations
for body literals. An example modeb declaration is modeb(2,mult(+int,+int,-int)).
The ﬁrst argument of a mode declaration is an integer denoting how often a literal may
appear in a clause. The second argument denotes that the literal mult/3 may appear
in the body of a clause and speciﬁes the type of its arguments. The symbols + and −
denote whether the arguments are input or output arguments respectively. Determina-
tions declare what predicates can be used to construct a hypothesis and are the form
of determination(TargetName/Arity,BackgroundName/Arity). The ﬁrst argument is
the name and arity of the target predicate. The second argument is the name and arity of
a predicate that can appear in the body of such clauses. Typically there will be many de-
termination declarations for a target predicate, corresponding to the predicates thought
to be relevant in constructing hypotheses. If no determinations are present Aleph does
not construct any clauses.

Aleph assumes that modes will be declared by the user. For the IGGP tasks this is
quite a burden because it requires that we create them for each game, and also requires
some knowledge of the target hypothesis we want to learn. Fortunately, however, Aleph
can extract mode declarations from determinations, where determinations are straight-
forward to supply because we can supply for each target predicate and each background
predicate a determination. Therefore, for each game, we allow Aleph to use all the pred-
icates available for that game as determinations and allow Aleph to induce the necessary
mode declarations.

There are many parameters in Aleph which greatly inﬂuence the output, such as pa-
rameters that change the search strategy when generalising a bottom clause (step 3) and
parameters that change the structure of learnable programs (such as limiting the number
of literals in the bottom clause). We run Aleph using the default parameters. Therefore,
there will most likely exist some parameter settings for which Aleph will perform better
than we present.

We use Aleph 5 with YAP 6.2.2 [9].

7 https://www.cs.ox.ac.uk/activities/programinduction/Aleph/

22

5.2.2 ASPAL

Andrew Cropper et al.

ASPAL [8] is a system for brave induction under the answer set programming (ASP) [51]
semantics. Brave induction systems aim to ﬁnd a hypothesis H such that there is at least
one answer set of B ∪ H that covers the examples8.

ASPAL works by transforming a brave induction task T into a meta-level ASP program
(cid:77) (T ) such that the answer sets of (cid:77) (T ) correspond to the inductive solutions of T . The
ﬁrst step of state-of-the-art ASP solvers, such as clingo [27], is to compute the grounding
of the program. Systems which follow this approach therefore have scalability issues
with respect to the size of the hypothesis space, as every ground instance of every rule
in the hypothesis space – i.e. the ground instances of every rule that has the potential to
be learned – is computed when the ASP solver solves (cid:77) (T ).

Similarly to Aleph, ASPAL has several input parameters, which inﬂuence the size of
the hypothesis space, such as the maximum number of body literals. For most of these,
we used the default value, but we increased the maximum number of body literals from
3 to 5 and the maximum number of rules in the hypothesis space from 3 to 15. Our
initial experiments showed that the maximum number of rules had very little effect on
the feasibility of the ASPAL approach (as the size of the grounding of (cid:77) (T ) is unaffected
by this change), whereas the maximum number of body literals can make a signiﬁcant
difference to the size of the grounding of (cid:77) (T ). It is possible that there is a set of
parameters for ASPAL that performs better than those we have chosen.

Predicate invention is supported in ASPAL by allowing new predicates (which do not
occur in the rest of the task) to appear in the mode declarations. This predicate invention
is prescriptive rather than automatic, as the schema of the new predicates (i.e. the arity,
and argument types) must be speciﬁed in the mode declarations. As how to guess the
structure of predicates which should be invented is unclear for this problem setting, we
did not allow ASPAL to use predicate invention on this dataset. It should be noted that
when programs are stratiﬁed, hypotheses containing predicate invention can always be
translated into equivalent hypotheses with no predicate invention. Of course, as such
hypotheses may be signiﬁcantly longer than the compact hypotheses which are possible
through predicate invention, they may require more examples to be learned accurately
by ASPAL.

Similarly, although ASPAL does enable learning recursive hypotheses, we did not per-
mit recursion in these experiments. Recursive hypotheses can also be translated into non-
recursive hypotheses over ﬁnite domains. Our initial experiments using ASPAL showed
that in addition to increasing the size of the hypothesis space, allowing recursion also
signiﬁcantly increased the grounding of ASPAL’s meta program, (cid:77) (T ).

5.2.3 Metagol

Metagol [61,13, 14] is an ILP system based on a Prolog meta-interpreter. The key differ-
ence between Metagol and a standard Prolog meta-interpreter is that whereas a stan-
dard Prolog meta-interpreter attempts to prove a goal by repeatedly fetching ﬁrst-order
clauses whose heads unify with a given goal, Metagol additionally attempts to prove
a goal by fetching higher-order metarules (Figure 14), supplied as background knowl-
edge, whose heads unify with the goal. The resulting meta-substitutions are saved and

8 As the programs in this paper are guaranteed to be stratiﬁed – recursion through negation is not
allowed in this dataset – all programs have exactly one answer set and so the brave and cautious settings
for ILP under the answer set semantics coincide.

Inductive general game playing

23

can be reused in later proofs. Following the proof of a set of goals, Metagol forms a
logic program by projecting the meta-substitutions onto their corresponding metarules.
Metagol is notable for its support for (non-prescriptive) predicate invention and learning
recursive programs.

Metarules deﬁne the structure of learnable programs, which in turn deﬁnes the hy-
pothesis space. Deciding which metarules to use for a given task is an unsolved prob-
lem [11,17]. To compute the benchmark, we set Metagol to use the same metarules for
all games and tasks. This set is composed of 9 derivationally irreducible metarules [16,
17], a set of metarules to allow for constants in a program, and a set of nullary metarules
(to learn the terminal predicates). Full details on the metarules used can be found in
the code repository.

i , E−

For each game, we allow Metagol to use all the predicates available for that game.
We also allow Metagol to support a primitive form of negation by additionally using
the negation of predicates. For instance, in Firesheep we allow Metagol to use the rule
not_does_kill(A,B) :- not(does_kill(A,B)). To allow Metagol to induce a program
given all (Bi, E+
) triples, we preﬁx each atom with an extra argument to denote
which triple each atom belongs to. For instance, in the ﬁrst minimal even triple, the
atom does_choose(player,1) becomes does_choose(triple1,player,1), and in the
second triple the same atom becomes does_choose(triple2,player,1). To account
for this extra argument, we also add extra argument to each literal in a metarule. For
instance, the ident metarule becomes P(I, A) ← Q(I, A) and the chain metarule becomes
P(I, A, B) ← Q(I, A, C), R(I, C, B).

i

We use Metagol 2.2.3 with YAP 6.2.2.

Name
ident
curry
precon
chain

Metarule
P(A, B) ← Q(A, B)
P(A, B) ← Q(A, B, R)
P(A, B) ← Q(A), R(A, B)
P(A, B) ← Q(A, C), R(C, B)

Fig. 14 Example metarules. The letters P, Q, R denote existentially quantiﬁed variables. The letters A,
B, and C denote universally quantiﬁed variables.

5.2.4 ILASP

ILASP (Inductive Learning of Answer Set Programs) [44,45,46] is a collection of ILP
systems, which are capable of learning ASP programs consisting of normal rules, choice
rules, hard and weak constraints. Unlike many other ILP approaches, ILASP guarantees
the computation of an optimal inductive solution (where optimality is deﬁned in terms of
the length of a hypothesis). Similarly to ASPAL, early ILASP systems, such as ILASP1 [44]
and ILASP2 [46], work by representing an ILP task (i.e. every example and every rule
in the hypothesis space) as a meta-level ASP program whose optimal answer sets cor-
respond to the optimal inductive solutions of the task. The ILASP systems each target
learning unstratiﬁed ASP programs with normal rules, choice rules and both hard and
weak constraints. Therefore, the stratiﬁed normal logic programs which are targeted in
this paper do not require the full generality of ILASP; in fact, on this dataset, the meta-
level ASP programs used by both ILASP1 and ILASP2 are isomorphic to the meta-level
program used by ASPAL.

24

Andrew Cropper et al.

ILASP2i [47] addresses the scalability with respect to the number of examples by
iteratively computing a subset of the examples, called relevant examples, and only rep-
resenting the relevant examples in the ASP program. In each iteration, ILASP2i uses
ILASP2 to ﬁnd a hypothesis H that covers the set of relevant examples and then searches
for a new relevant example which is not covered by H. When no further relevant exam-
ples exist, the computed H is guaranteed to be an optimal inductive solution of the full
task.

Although ILASP2i makes signiﬁcant improves on the scalability of ILASP1 and ILASP2
with respect to the examples, on tasks with large hypothesis spaces ILASP2i still suffers
from the same grounding bottleneck as ASPAL, ILASP1 and ILASP2. As the size of the
hypothesis spaces are one of the major challenges of the dataset in this paper, ILASP2i
would likely not perform signiﬁcantly better than ASPAL. To scale up the application of
the ILASP framework to the GGP dataset, we used an extended version of ILASP2i, which
computes, at each iteration, a relevant hypothesis space using the type signature and the
current set of relevant examples, and then uses ILASP2 to solve a learning task with the
current relevant examples and relevant hypothesis space. Through the rest of the paper,
we refer to this extended ILASP algorithm as ILASP∗. Speciﬁcally, rules that entail neg-
ative examples or do not cover at least one relevant positive example are omitted from
the relevant hypothesis space. Also, a rule is omitted if there is another rule which is
shorter and covers the same (or more) relevant positive examples. Similarly to ASPAL,
ILASP∗ takes a parameter for the maximum number of literals in the body. Our prelimi-
nary experiments showed that the method for computing the relevant hypothesis space
performed best with this parameter set to 5, so this value was used for the experiments.
The construction of a relevant hypothesis space was made signiﬁcantly easier by for-
bidding recursion and predicate invention in ILASP∗. Although the standard ILASP algo-
rithms do support recursion and (prescriptive) predicate invention, these two features
mean that the usefulness of a rule in covering examples cannot be evaluated indepen-
dently, and thus constructing the relevant hypothesis space is much more challenging. In
future work, we hope to generalise the method of relevant hypothesis space construction
to relax these two constraints.

6 Results

We now describe the results of running the baselines and ILP systems on our dataset.
All the experimental data is available at https://github.com/andrewcropper/mlj19-iggp.
When running the ILP systems, we allowed each system the same amount of time to learn
each target predicate. We allowed each system 30 minutes to learn each target predicate.

6.1 Evaluation metrics

We use two evaluation metrics: balanced accuracy and perfectly solved.

6.1.1 Balanced accuracy

In our dataset the majority of examples are negative. To account for this class imbalance,
we use balanced accuracy [5] to evaluate the approaches. Given background knowledge
B, disjoint sets of positive E+ and negative E− testing examples, and a logic program H,

Inductive general game playing

25

we deﬁne the number of positive examples as p = |E+|, the number of negative examples
as n = |E−|, the number of true positives as t p = |{e ∈ E+|B∪H |= e}|, the number of true
negatives as t n = |{e ∈ E−|B∪H (cid:54)|= e}|, and the balanced accuracy ba = (t p/p+t n/n)/2.

6.1.2 Perfectly solved

We also consider a perfectly solved metric, which is the number (or percentage) of tasks
that an approach solves with 100% accuracy. The perfectly solved metric is important
in IGGP because we know that every game has at least one perfect solution: the GDL
description from which the traces were generated is a perfectly accurate model of the
deterministic MDP. Perfect accuracy is important because even slightly inaccurate models
compound their errors as the game progresses.

6.2 Results summary

Figure 15 summarises the results and shows for each approach the balanced accuracy
and percentage of perfectly solved tasks. The full results are in the appendix. As the
results show, the ILP and KNN approaches perform better than simple baselines (True,
I ner t ia, and M ean). In terms of balanced accuracy, the KNN approaches often perform
better than the ILP systems. However, in terms of the important perfectly solved metric,
the ILP methods easily outperform the baselines and the KNN approaches. The most
successful system ILASP∗ perfectly solves 40% of the tasks. It should be noted that 4% of
test cases have no positive instances in either the training set nor the test set, meaning
that a perfect score can be achieved with the empty hypothesis. Each of our ILP systems
achieved a perfect score on these tasks. Without these trivial cases, the score of each
system on the perfectly solved metric would be even lower.

As Figure 16 shows, in terms of balanced accuracies, the most difﬁcult task is the
terminal predicate, although the margin of difference between the predicates is small.
As Figure 17 shows, in terms of the important perfectly solved metric, the most difﬁcult
task is the next predicate. The mean number of perfectly solved tasks is a measly 3%.
Even if we exclude the baselines and only consider the ILP systems then the mean is
still only 10%. Figure 18 shows the balanced accuracies for the next predicate on the
alphabetically ﬁrst ten games. This predicate corresponds to the state transition function
(Section 4.1). The next atoms are the most difﬁcult to learn and there is only one out
of the ﬁrst ten games, Buttons and Lights, for which any of the methods ﬁnd a perfect
solution. The next predicate is the most difﬁcult to learn because it has the highest
mean complexity in terms of the number of dependent predicates in the dependency
graph (Section 3.1) in the reference GDL game deﬁnitions.

Metric
BA (%)
PS (%)

Baseline
48
4

Inertia Mean

56
4

64
15

KNN1
80
16

KNN5
80
19

Aleph
66
18

ASPAL Metagol

55
10

69
34

ILASP∗
86
40

Fig. 15 Results summary. The baseline represents accepting everything. The results show that all of
the approaches struggle in terms of the perfectly solved metric (which represents how many tasks were
solved with 100% accuracy).

26

Andrew Cropper et al.

Approach
True
Inertia
Mean
KNN1
KNN5
Aleph
ASPAL
Metagol
ILASP
Mean

∗

goal
47
47
82
92
92
83
52
74
92
73

legal
56
56
61
78
79
60
59
66
86
67

next
47
80
62
86
86
59
50
60
88
69

Fig. 16 Balanced accuracy results for each target predicate.

Approach
True
Inertia
Mean
KNN1
KNN5
Aleph
ASPAL
Metagol
ILASP
Mean

∗

goal
0
0
32
34
34
32
4
48
46
26

legal
16
16
16
16
22
18
18
28
44
22

next
0
0
0
0
0
4
0
6
18
3

terminal
42
42
53
63
64
60
59
77
80
60

terminal
0
0
12
12
18
16
18
52
52
20

Fig. 17 Perfectly solved percentage for each target predicate.

Game
Alquerque
Asylum
Battle of Numbers
Breakthrough
Buttons and Lights
Centipede
Checkers
Coins
Connect 4 (Team)
Don’t Touch

Inertia Mean

90
97
88
96
54
67
91
79
93
89

73
74
52
70
50
57
66
50
50
76

KNN1
87
97
87
95
82
88
90
88
92
86

KNN5
90
97
86
96
81
85
90
81
92
90

Aleph
53
69
58
52
58
57
55
63
50
64

ASPAL Metagol

50
50
50
50
50
50
50
50
50
50

54
51
54
51
50
50
55
60
50
53

ILASP∗
74
84
67
97
100
92
95
93
96
89

Fig. 18 Balanced accuracies for the next target predicate for the alphabetically ﬁrst ten games.

.

In the following sections we analyse the results for each system and discuss the relative
limitations of the respective systems on this dataset.

6.2.1 KNN

As Figure 15 shows, the KNN approaches perform well in terms of balanced accuracy but
poorly in terms of perfectly solved. Note that KNN1 occasionally scores higher than KNN5,
which is to be expected because sometimes looking at additional triples gives misleading
information. As already mentioned, the KNN approaches learn at the propositional level.
This limitation is evident when analysing the results which show that the KNN1 and KNN5
approaches only perform well when the target predicate can be learned by memorizing
particular atoms. For some of the simpler games (e.g. Coins), the KNN approach is often
able to learn the goal predicate because the reward can be extracted directly from the

Inductive general game playing

27

value of an internal state variable representing the score. Similarly, the KNN approach
sometimes learns the legal predicate when the set of legally valid actions is static and
does not depend on the current state. But the KNN approach is not able to perfectly
learn any of the next rules for any of the games in our dataset. In addition, the KNN
approaches are expensive to compute. To get these results it took 3 days on a 3.6 GHz
machine.

6.2.2 Aleph

As Figure 15 shows, Aleph performs reasonably well, and outperforms most of the base-
lines in terms of the perfectly solved metric. However, after inspecting the learned pro-
grams, we found that Aleph was rarely learning general rules for the games, and instead
typically learned facts to explain the speciﬁc examples. In other words, on this task,
Aleph tends to learn overly speciﬁc programs. There are several potential explanations
for this limitation. First, as we stated in Section 5.2.1, we did not provide mode dec-
larations to Aleph, and instead allowed Aleph to infer them from the determinations.
Second, we ran Aleph with the default parameters. However, as stated in Section 5.2.1,
Aleph has many learning parameters which greatly inﬂuence the learning performance.
It is reasonable to assume that Aleph could perform even better with a different set of
parameters. Third, to learn a program Aleph must ﬁrst construct the most speciﬁc clause
(the bottom clause) that entails an example. However, constructing the bottom clause
requires exponential time in the depth of variables in the target theory [56]. Therefore,
learning large and complex clauses is intractable.

6.2.3 ASPAL

As Figure 15 shows, ASPAL performs quite poorly on this dataset. It is outperformed by
the mean baseline, both in terms of the perfectly solved metric, and the average balanced
accuracy. ASPAL timed out on the majority of the test problems, which was caused by
the size of the hypothesis space, and therefore the grounding of ASPAL’s meta-level ASP
program. It is possible that by using different parameters to control the size of the hypoth-
esis space, or using a different representation of the problem, with a smaller grounding,
ASPAL could perform better.

The results of ASPAL are also interesting to explain the need to create a specialised
version of the ILASP algorithm for this dataset. On this constrained problem domain,
where we are only aiming to learn stratiﬁed programs (which are guaranteed to have
a single answer set), ILASP2 and ASPAL are almost identical in their approaches. Both
map the input ILP task into a meta-level ASP program, and use the Clingo ASP solver to
ﬁnd an optimal answer set, corresponding to an optimal inductive solution of the input
task. The specialised ILASP∗ algorithm presented in Section 5.2.4 can overcome this
problem in some cases, by reducing the size of the hypothesis space being considered,
and thus reducing the size of the grounding of the meta-level program. In principle,
this specialisation (along with ILASP2i’s relevant example method) could be applied to
ASPAL, to create ASPAL∗, which would likely have performed better.

6.2.4 Metagol

Although Metagol outperforms the baselines in the perfectly correct metric (34%), it is
outperformed in terms of balanced accuracy.

28

Andrew Cropper et al.

One of the main limitations of Metagol in this dataset is that it will only return a
program if that program covers all of the positive examples and none of the negative
examples. However, in some of the games, Metagol could learn a single simple rule that
explains 99% of the training examples (and perhaps 99% of the testing examples) but
may need an additional complex rule to cover the remaining 1%. If this extra rule is too
complex to learn, then Metagol will not learn anything. To explore this limitation we ran
a modiﬁed version of Metagol that relaxes this constraint. This modiﬁed version sim-
ply samples training examples, rather than learn from all the examples. This stochastic
version of Metagol improved balanced accuracy from 69% to 76%. In future work we
intend to develop more sophisticated versions of stochastic Metagol.

Metagol can generalise from few examples because of the strong inductive bias en-
forced by the metarules. However, this strong bias is also a key reason why Metagol
struggles to learn programs for many of the games. Given insufﬁcient metarules, Metagol
cannot induce the target program. For instance, given only monadic metarules, Metagol
can only learn monadic programs. Although there is work studying which metarules
to use for monadic and dyadic logics [12,16,17], there is no work on determining
which metarules to use for higher-arity logic. Therefore, when computing the bench-
marks, Metagol could not learn some of the higher-arity target predicates, such as the
next_cell/4 predicate in Sudoku. Similarly Metagol could often not use higher-arity
predicates, such as does_move/5 and triplet/6 in Alquerque.

Another issue with the metarules is in that, as described in Section 5.2.3, we used
the same set of metarules for all games. This approach is inefﬁcient because in almost
all cases this approach meant that we were using irrelevant metarules, which added
unnecessary search to the learning task. We expect that a simple preprocessing step to
remove unusable metarules would improve learning performance, although probably
not by any considerable margin.

Another reason why Metagol struggles to solve certain games is because, as with most
ILP systems, it struggles to learn large and complex programs. For Metagol the bottleneck
is in the size of the target program because the search space grows exponentially with the
number of clauses in the target program [17]. Although there is work in trying to mitigate
this issue [13], developing approaches that can learn large and complex programs is a
major challenge for MIL and ILP in general [11].

6.2.5 ILASP∗

The system with the highest percentage of completely accurate models (see Figure 15)
is ILASP∗, with 40% of the tasks completely solved. In most of the cases where ILASP∗
terminated with a solution in the time limit of 30 minutes, a perfect solution was re-
turned. On the rare occasion that ILASP∗ terminated but learned an imperfect solution,
it did cover the training examples, but performed imperfectly on the test set; for exam-
ple, in the terminal training set for Untwisty Corridor there are no positive examples,
meaning that ILASP∗ returns the empty hypothesis (which covers the set of negative ex-
amples); however, there is a positive instance of terminal in the test set, meaning that
ILASP∗ (and all other approaches) score a balanced accuracy of 50 on this problem.

In some cases, the restriction on the number of body literals meant that the task
had no solutions. In these unsatisﬁable cases, the hypothesis in the last satisﬁable itera-
tion was returned by ILASP∗. In principle, the maximum number of body literals could
have been iteratively increased until the task became satisﬁable, but our initial exper-
iments showed that this made little or no difference to the number of perfectly solved

Inductive general game playing

29

cases. Some of the unsatisﬁable cases may have been caused by the restriction forbidding
predicate invention for ILASP∗ on this dataset – although there will always by an equiva-
lent hypothesis that does not contain predicate invention, the equivalent hypothesis may
have rules with more than 5 body literals.

Similarly to the unsatisﬁable cases, in the timeout cases, the hypothesis found in
the ILASP∗’s ﬁnal iteration was used to compute the accuracy. Returning the hypothesis
found in the last iteration explains ILASP∗’s much higher average balanced accuracy
compared to Metagol, which either returns a perfect solution over the test set or no
solution at all.

ILASP∗ is able to perfectly solve some tasks that are not perfectly solved by any
of the baselines or other ILP systems. One example is the next learning task for Rock
Paper Scissors. In this case, the raw hypothesis returned by ILASP∗ is shown in Figure 19,
which is equivalent to the (more readable) hypothesis shown in Figure 20. Note that this
hypothesis is slightly more complicated than necessary. If ILASP∗ had been permitted to
use ! = to check that two player variables did not represent the same player, it is possible
that the last three rules would have been replaced with:

next_score(Player1, Score) :-

true_score(Player1, Score), does(Player1, Move1), does(Player2, Move2),
not beats(Move1, Move2), Player1 != Player2.

It is possible to learn hypotheses with ! = (and other binary comparison operators)
in ILASP, but this would have increased the size of the hypothesis space, so in these
experiments, we only allowed ILASP∗ to construct hypothesis spaces using the language
of the input task. In future work, we may consider extending the relevant hypothesis
space construction method to allow binary comparison operators. The increase in the
size of the hypothesis space may be outweighed by the fact that the ﬁnal hypothesis can
be shorter – shorter hypotheses tend to need fewer iterations to learn.

next_step(V0) :- succ(V2, V0), true_step(V2), int(V0), int(V2).

next_score(V0, V1) :-

succ(V3, V1), beats(V8, V6), true_score(V0, V3), does(V5, V6), does(V0, V8),
agent(V0), int(V1), int(V3), agent(V5), action(V6), action(V8).

next_score(V0, V1) :-

true_score(V0, V1), does(V5, V7), does(V0, V7), V0 = p1, V5 = p2,
agent(V0), int(V1), agent(V5), action(V7).

next_score(V0, V1) :-

beats(V7, V8), true_score(V0, V1), does(V0, V8),
does(V5, V7), agent(V0),
int(V1), agent(V5), action(V7), action(V8).

next_score(V0, V1) :-

true_score(V0, V1), does(V0, V8), does(V5, V8),
V0 = p2, V5 = p1, agent(V0),
int(V1), agent(V5), action(V8).

∗
Fig. 19 The raw hypothesis returned by ILASP

for the next learning task for Rock Paper Scissors.

30

Andrew Cropper et al.

next_step(NewStep) :- succ(CurrentStep, NewStep), true_step(CurrentStep).

next_score(Player1, NewScore) :-

succ(Score, NewScore), true_score(Player1, Score), does(Player2, Move2),
does(Player1, Move1), beats(Move1, Move2).

next_score(Player1, Score) :-

true_score(Player1, Score), does(Player1, Move1), does(Player2, Move2),
beats(Move2, Move1).

next_score(p1, Score) :-

true_score(p1, Score), does(p2, Action), does(p1, Action).

next_score(p2, Score) :-

true_score(p2, Score), does(p2, Action), does(p1, Action).

Fig. 20 A more readable version of the hypothesis returned by ILASP
Paper Scissors.

∗

for the next learning task for Rock

6.3 Discussion

As Figure 15 shows, most of the IGGP tasks cannot be perfectly learned by existing ILP
systems. The best performing system (ILASP∗) solves only 40% of the tasks perfectly. Our
results suggest that the IGGP problem poses many challenges to existing approaches.

As mentioned in Section 4.3, we are unsure whether the dataset contains sufﬁcient
training examples for each approach to perfectly solve all of the tasks. Moreover, de-
termining whether there is sufﬁcient data is especially difﬁcult because the different
systems employ different biases. However, in most cases the ILP systems simply timed
out, rather than learning an incorrect solution. The key issue is that the ILP systems we
have considered do not scale to the large problems in the IGGP dataset. In the previous
section we discussed limitations of each system. We now summarise the limitations to
help explain what makes IGGP difﬁcult for existing approaches.

Large programs As discussed in Section 2, many reference solutions for IGGP games are
large, both in terms of the number of literals and the clauses in them. For instance, the
GGP reference solution for the goal predicate for Connect Four uses 14 clauses and a total
of 72 literals. However, learning large programs is a challenge for most ILP systems [11]
which typically struggle to learn programs with hundreds of clauses or literals. Metagol,
for instance, struggles to learn programs with more than 8 clauses.

Predicate invention The reference solution for goal in Connect four uses auxiliary pred-
icates (goal is deﬁned in terms of lines, which are deﬁned in terms of columns, rows
and diagonals). These auxiliary predicates are not strictly required, as any stratiﬁed def-
inition with auxiliary predicates can be translated into an equivalent program with no
auxiliary predicates; however, such equivalent programs are often signiﬁcantly longer. If
we unfold the reference solution to remove auxiliary predicates, the resulting equivalent
unfolded program contains over 400 literals. For ILP approaches that do not support the
learning of programs containing auxiliary predicates (such as Progol, Aleph, and FOIL),
it is infeasible to learn such a large program. More modern ILP approaches support predi-
cate invention, enabling the learning of auxiliary predicates which are not in the language

Inductive general game playing

31

of the background knowledge or the examples; however, predicate invention is far from
easy, and there are signiﬁcant challenges associated with it, even for state of the art ILP
systems. ASPAL and ILASP support prescriptive predicate invention, where the schema
of the auxiliary predicates (i.e. the arity, and argument types) must be speciﬁed in the
mode declarations [43]. By contrast, Metagol supports automatic predicate invention,
where Metagol invents auxiliary predicates without the need for user-supplied arities or
type information. However, Metagol’s approach can still often lead to inefﬁciencies in
the search, especially when multiple new predicate symbols are introduced.

7 Conclusion

In this paper, we have expanded on the Inductive General Game Playing task proposed by
Genesereth. We claimed that learning the rules of the GGP games is difﬁcult for existing
ILP techniques. To support this claim, we introduced a IGGP dataset based on 50 games
from the GGP competition and we evaluated existing ILP systems on the dataset. Our
empirical results show that most of the games cannot be perfectly learned by existing
systems. The best performing system (ILASP∗) solves only 40% of the tasks perfectly. Our
results suggest that the IGGP problem poses many challenges to existing approaches. We
think that the IGGP problem and dataset will provide an exciting challenge for future
research, especially as we have introduced techniques to continually expand the dataset
with new games.

7.1 Limitations and future work

Better ILP systems Our primary motivation for introducing this dataset is to encourage
future research in ILP, especially on general ILP systems able to learn rules for a diverse
set of tasks. In fact, we have already demonstrated two advancements in this paper: (1)
a stochastic version of Metagol (6.2.4), and (2) ILASP∗ (Section 5.2.4), which scales up
ILASP2 for the GGP dataset. In future work we intend to develop better ILP systems.

More games One of the main advantages of the IGGP problem is that the games are
based on the GGP competition. As mentioned in the introduction, the GGP competition
produces new games each year. These games are introduced independently from our
dataset without any particular ILP system in mind. Therefore, because of our second
contribution, we can continually expand the IGGP dataset with these new games. In
future work we intend to automate this whole process and to ensure that all the data is
publicly available.

More systems We have evaluated four ILP systems (Aleph, ASPAL, Metagol, and ILASP).
In future work we would like to evaluate more ILP systems. We could also like to consider
non-ILP systems (i.e. systems that may not necessarily learn explicit human-readable
rules).

More evaluation metrics We have evaluated ILP systems according to two metrics: bal-
anced accuracy and perfect solved. However, there are other dimensions on which to
evaluate the systems. We have not, for instance, considered the learning times of the
systems (although they all had the same maximum time to learn during the evaluation).

32

Andrew Cropper et al.

Nor have we considered the sample complexity of the approaches. In future work it
would be valuable to evaluate approaches when varying the number of game traces (i.e.
observations) available, as to identify the most data-efﬁcient approaches.

More challenges The main challenge in using existing systems on this dataset is the delib-
erate lack of game-speciﬁc language biases, meaning that for many games the hypothesis
space that each system must consider is extremely large. This reﬂects a major current
issue in ILP, where systems are often given well crafted language biases to ensure fea-
sibility; however, this is not the only current challenge in ILP. For example, some ILP
approaches target challenges such as learning from noisy data [62,24,49], probabilis-
tic reasoning [19,20,66,3,67], non-determinism expressed through unstratiﬁed nega-
tion [63,48], and preference learning [46]. Future versions of this dataset could be ex-
tended to contain these features.

Competitions SAT competitions have been held since 1992 with the aim of providing
an objective evaluation of contemporary SAT solvers [36]. The competitions have sig-
niﬁcantly contributed to the progress of developing ever more efﬁcient SAT techniques
[36]. In addition, the competitions have motivated the SAT community to develop more
robust, reliable, and general purposes SAT solvers (i.e implementations). We believe that
the ILP community stands to beneﬁt from an equivalent competition, to focus and mo-
tivate research. We hope that this new IGGP problem and dataset will become a central
component in this new competition.

Inductive general game playing

A Appendix: Full Results

33

This appendix includes the full results for our dataset of 50 games. We use balanced accuracy as the
evaluation metric (see Section 6.1.1).

Game
Alquerque
Alquerque
Alquerque
Alquerque
Asylum
Asylum
Asylum
Asylum
Battle of Numbers
Battle of Numbers
Battle of Numbers
Battle of Numbers
Breakthrough
Breakthrough
Breakthrough
Breakthrough
Buttons and Lights
Buttons and Lights
Buttons and Lights
Buttons and Lights
Centipede
Centipede
Centipede
Centipede
Checkers
Checkers
Checkers
Checkers
Coins
Coins
Coins
Coins
Connect 4 (Team)
Connect 4 (Team)
Connect 4 (Team)
Connect 4 (Team)
Don’t Touch
Don’t Touch
Don’t Touch
Don’t Touch
Duikoshi
Duikoshi
Duikoshi
Duikoshi
Eight Puzzle
Eight Puzzle
Eight Puzzle
Eight Puzzle
Farming
Farming
Farming

Predicate
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next

True
50
50
50
50
50
50
50
0
50
50
50
50
50
50
50
50
50
100
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50

Inertia Mean
50
50
90
50
50
50
97
0
50
50
88
50
50
50
96
50
50
100
54
50
50
50
67
50
50
50
91
50
50
50
79
50
50
50
93
50
50
50
89
50
50
50
93
50
50
50
84
50
50
50
87

50
52
73
50
50
52
74
100
74
56
52
50
99
51
70
50
83
100
50
50
98
86
56
50
50
54
66
50
100
50
50
50
98
62
50
50
80
50
76
50
94
51
59
50
100
50
52
50
98
52
61

KNN(1)
97
62
87
51
96
73
97
99
98
68
87
53
99
73
95
50
83
100
82
100
99
73
88
89
94
64
90
50
100
66
88
83
97
50
92
49
73
68
86
47
92
73
92
49
100
92
89
50
100
66
85

KNN(5)
95
63
90
50
84
69
97
100
97
67
86
50
99
70
96
50
83
100
81
100
99
95
85
82
88
62
90
60
100
50
81
92
98
66
92
50
80
89
90
49
94
79
92
50
100
82
88
50
99
82
83

Aleph
100
50
53
49
59
50
68
98
50
50
58
48
98
50
51
49
100
100
57
75
96
78
56
52
59
50
55
48
93
49
63
68
96
55
50
49
67
49
64
51
90
49
52
52
50
51
49
51
100
49
57

ASPAL Metagol
50
50
50
50
50
50
50
100
50
50
50
50
50
50
50
50
50
100
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50

100
50
53
100
100
50
51
100
50
50
53
50
50
50
51
50
50
100
50
100
50
50
50
50
100
50
55
50
100
50
59
50
50
50
50
50
50
73
53
50
50
50
52
50
50
50
55
100
100
50
50

ILASP∗
100
63
74
100
100
62
84
100
73
78
67
39
99
73
97
52
90
100
100
100
88
91
92
75
50
75
95
74
100
56
93
95
94
92
96
58
78
100
89
100
90
70
94
57
99
100
86
100
100
100
86

34

Andrew Cropper et al.

Farming
Firesheep
Firesheep
Firesheep
Firesheep
Fizz-Buzz
Fizz-Buzz
Fizz-Buzz
Fizz-Buzz
Forager
Forager
Forager
Forager
Free For All
Free For All
Free For All
Free For All
Frogs and Toads
Frogs and Toads
Frogs and Toads
Frogs and Toads
GT Attrition
GT Attrition
GT Attrition
GT Attrition
GT Centipede
GT Centipede
GT Centipede
GT Centipede
GT Chicken
GT Chicken
GT Chicken
GT Chicken
GT Prisoner
GT Prisoner
GT Prisoner
GT Prisoner
GT Ultimatum
GT Ultimatum
GT Ultimatum
GT Ultimatum
Hex (Three)
Hex (Three)
Hex (Three)
Hex (Three)
Horseshoe
Horseshoe
Horseshoe
Horseshoe
Hunter
Hunter
Hunter
Hunter
Knights Tour
Knights Tour
Knights Tour

terminal
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next

50
50
50
50
50
50
50
50
50
50
100
50
50
50
50
50
50
0
50
50
0
50
50
38
50
50
50
43
0
50
50
50
50
50
50
50
50
50
50
45
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50

50
50
50
82
50
50
50
69
50
50
100
92
50
50
50
86
50
0
50
95
0
50
50
60
50
50
50
61
0
50
50
59
50
50
50
69
50
50
50
61
50
50
50
96
50
50
50
64
50
50
50
88
50
50
50
83

50
63
78
69
50
100
88
50
50
50
100
87
50
77
52
65
50
100
50
93
100
48
0
54
50
75
50
53
0
50
50
50
50
50
50
63
50
50
61
68
50
100
53
62
50
98
55
50
50
50
50
77
50
50
50
64

79
97
86
82
90
100
90
72
50
97
100
94
50
99
76
86
61
100
95
97
100
48
50
64
50
61
75
69
50
91
75
79
57
83
93
82
80
91
95
68
75
100
47
97
50
100
94
87
78
91
90
88
62
82
73
87

85
97
84
80
92
100
89
71
50
100
100
93
47
98
72
84
54
100
87
97
100
48
0
62
50
100
100
64
0
85
86
71
70
78
100
76
94
89
100
71
78
100
56
95
50
98
77
83
67
90
83
90
59
72
63
84

49
100
49
51
48
100
86
53
48
55
100
50
46
81
50
59
52
100
50
51
100
97
100
57
100
82
0
59
100
54
49
50
46
56
49
63
46
56
69
57
52
99
50
50
49
96
57
69
55
58
53
69
46
53
51
63

50
50
50
50
50
50
50
50
50
50
100
50
50
50
50
50
50
100
50
50
100
50
50
50
50
50
50
50
100
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50
50

100
100
50
51
50
50
100
50
100
100
100
53
100
100
50
58
100
100
50
51
100
100
100
78
100
50
100
71
100
100
100
67
100
100
100
75
100
100
69
61
100
50
50
66
50
50
50
65
50
100
50
52
100
100
50
50

100
100
97
70
38
100
100
79
100
100
100
95
61
77
96
63
100
100
74
85
100
100
100
86
100
99
100
100
100
100
100
68
100
100
100
76
100
80
100
84
100
99
52
59
45
98
100
90
77
100
100
87
100
100
77
94

Inductive general game playing

35

Knights Tour
Kono
Kono
Kono
Kono
Leafy
Leafy
Leafy
Leafy
Lightboard
Lightboard
Lightboard
Lightboard
Minimal Decay
Minimal Decay
Minimal Decay
Minimal Decay
Minimal Even
Minimal Even
Minimal Even
Minimal Even
Multiple Buttons and Lights
Multiple Buttons and Lights
Multiple Buttons and Lights
Multiple Buttons and Lights
Nine Board TicTacToe
Nine Board TicTacToe
Nine Board TicTacToe
Nine Board TicTacToe
Pentago
Pentago
Pentago
Pentago
Pilgrimage
Pilgrimage
Pilgrimage
Pilgrimage
Platform Jumpers
Platform Jumpers
Platform Jumpers
Platform Jumpers
Rainbow
Rainbow
Rainbow
Rainbow
Rock Paper Scissors
Rock Paper Scissors
Rock Paper Scissors
Rock Paper Scissors
Sheep and Wolf
Sheep and Wolf
Sheep and Wolf
Sheep and Wolf
Sokoban
Sokoban
Sokoban

terminal
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next

50
50
50
50
50
50
50
50
50
50
100
50
50
0
100
0
50
50
100
50
50
50
100
50
50
50
6
5
50
50
50
50
50
50
50
49
0
50
50
50
50
50
50
50
50
50
100
50
50
50
41
38
0
50
50
50

50
50
50
88
50
50
50
97
50
50
100
81
50
0
100
0
50
50
100
89
50
50
100
72
50
50
6
54
50
50
50
91
50
50
50
92
0
50
50
98
50
50
50
91
50
50
100
56
50
50
41
94
0
50
50
93

50
50
53
54
50
96
50
90
50
100
100
50
50
100
100
38
100
100
100
50
50
100
100
50
50
98
54
55
50
99
52
50
50
100
50
54
0
98
52
76
50
99
50
50
50
50
100
50
50
100
55
91
0
50
50
50

45
97
61
84
52
94
56
96
51
100
100
73
45
100
100
50
100
82
100
84
69
100
100
82
98
97
67
98
49
99
67
87
50
100
65
93
0
98
66
99
50
99
81
89
46
75
100
73
100
100
65
98
0
50
72
90

63
95
65
87
53
96
56
97
50
100
100
73
50
100
100
50
100
86
100
87
75
100
100
81
100
98
60
97
50
99
65
84
50
100
65
92
0
98
62
99
74
99
86
87
50
79
100
74
100
100
66
95
0
50
75
92

52
100
50
54
51
91
50
49
49
100
100
49
48
100
100
68
100
85
100
100
100
100
100
55
48
97
50
52
49
98
50
52
49
99
49
55
100
97
50
56
48
97
50
100
57
100
100
52
0
50
50
50
98
49
53
50

50
50
50
50
50
50
50
50
50
50
100
50
50
100
100
50
100
50
100
50
50
50
100
50
50
50
50
50
50
50
50
50
50
50
50
50
100
50
50
50
50
50
50
50
50
50
100
50
50
50
50
50
100
50
50
50

50
100
50
55
97
50
50
100
50
50
100
59
100
100
100
50
100
100
100
100
100
100
100
70
100
50
50
97
50
50
56
53
50
50
50
52
100
50
55
50
50
50
100
100
50
100
100
66
100
100
50
50
100
50
50
65

54
100
82
93
97
90
100
92
47
98
100
98
100
100
100
100
100
100
100
100
100
100
100
99
100
97
85
94
52
99
85
94
64
70
70
72
100
98
83
73
30
95
47
100
80
100
100
100
100
56
54
96
100
72
71
95

36

Andrew Cropper et al.

Sokoban
Sudoku
Sudoku
Sudoku
Sudoku
Sukoshi
Sukoshi
Sukoshi
Sukoshi
Switches
Switches
Switches
Switches
TicTacToe
TicTacToe
TicTacToe
TicTacToe
Tiger vs Dogs
Tiger vs Dogs
Tiger vs Dogs
Tiger vs Dogs
Tron
Tron
Tron
Tron
TTCC4
TTCC4
TTCC4
TTCC4
Untwisty Corridor
Untwisty Corridor
Untwisty Corridor
Untwisty Corridor
Walkabout
Walkabout
Walkabout
Walkabout

References

terminal
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next
terminal
goal
legal
next
terminal

50
50
50
50
50
50
50
50
50
0
50
50
0
50
50
50
50
50
50
50
0
50
50
50
50
50
23
32
0
50
100
50
50
50
50
50
50

50
50
50
99
50
50
50
93
50
0
50
94
0
50
50
85
50
50
50
91
0
50
50
81
50
50
23
73
0
50
100
76
50
50
50
59
50

50
100
53
84
50
100
50
65
50
100
50
85
100
93
53
51
50
72
50
72
100
50
50
70
50
100
52
53
100
100
100
80
50
95
50
50
50

50
100
98
98
49
100
87
90
44
100
84
95
100
88
72
83
64
88
57
89
100
75
80
89
70
100
75
89
98
100
100
92
50
95
71
74
50

50
100
97
99
50
100
91
93
49
100
85
94
100
93
91
91
57
88
64
92
100
71
84
84
77
100
66
90
100
100
100
91
50
95
82
74
50

49
100
50
50
48
100
60
69
43
100
52
86
100
78
48
54
45
62
50
51
100
29
54
70
56
100
50
60
97
100
100
75
50
92
51
50
50

50
50
50
50
50
50
50
50
50
50
100
50
100
50
50
50
50
50
50
50
100
50
50
50
50
50
50
50
100
50
100
50
50
50
50
50
50

50
100
50
50
50
50
50
50
50
100
50
60
100
50
72
55
50
50
50
51
100
50
50
92
50
50
50
57
100
100
100
61
50
50
50
50
100

50
100
55
86
48
100
91
93
43
100
89
99
100
51
100
89
71
59
79
54
100
91
85
100
100
100
74
61
71
100
100
100
50
93
100
100
100

1. Krzysztof R. Apt, Howard A. Blair, and Adrian Walker. Towards a theory of declarative knowledge.
In Jack Minker, editor, Foundations of Deductive Databases and Logic Programming, pages 89–148.
Morgan Kaufmann, 1988.

2. Michael Bain. Learning logical exceptions in chess. PhD thesis, University of Strathclyde, 1994.
3. Elena Bellodi and Fabrizio Riguzzi. Structure learning of probabilistic logic programs by searching

the clause space. Theory and Practice of Logic Programming, 15(02):169–212, 2015.

4. Yngvi Björnsson. Learning rules of simpliﬁed boardgames by observing. In ECAI, pages 175–180,

2012.

5. Kay Henning Brodersen, Cheng Soon Ong, Klaas Enno Stephan, and Joachim M. Buhmann. The
balanced accuracy and its posterior distribution. In 20th International Conference on Pattern Recog-
nition, ICPR 2010, Istanbul, Turkey, 23-26 August 2010, pages 3121–3124. IEEE Computer Society,
2010.

6. Murray Campbell, A. Joseph Hoane Jr., and Feng-hsiung Hsu. Deep blue. Artif. Intell., 134(1-2):57–

83, 2002.

7. Lourdes Peña Castillo and Stefan Wrobel. Learning minesweeper with multirelational learning. In

IJCAI, pages 533–540. Morgan Kaufmann, 2003.

Inductive general game playing

37

8. Domenico Corapi, Alessandra Russo, and Emil Lupu.

Inductive logic programming in answer set
programming. In International Conference on Inductive Logic Programming, pages 91–97. Springer,
2011.

9. Vítor Santos Costa, Ricardo Rocha, and Luís Damas. The YAP prolog system. TPLP, 12(1-2):5–34,

2012.

10. Stephen Cresswell, Thomas Leo McCluskey, and Margaret Mary West. Acquisition of object-centred

domain models from planning examples. In ICAPS, 2009.

11. Andrew Cropper. Efﬁciently learning efﬁcient programs. PhD thesis, Imperial College London, UK,

2017.

12. Andrew Cropper and Stephen H. Muggleton. Logical minimisation of meta-rules within meta-
interpretive learning. In Jesse Davis and Jan Ramon, editors, Inductive Logic Programming - 24th
International Conference, ILP 2014, Nancy, France, September 14-16, 2014, Revised Selected Papers,
volume 9046 of Lecture Notes in Computer Science, pages 62–75. Springer, 2014.

13. Andrew Cropper and Stephen H. Muggleton. Learning higher-order logic programs through abstrac-
tion and invention. In Subbarao Kambhampati, editor, Proceedings of the Twenty-Fifth International
Joint Conference on Artiﬁcial Intelligence, IJCAI 2016, New York, NY, USA, 9-15 July 2016, pages
1418–1424. IJCAI/AAAI Press, 2016.

14. Andrew

Cropper

and

Stephen

H.

Muggleton.

Metagol

system.

https://github.com/metagol/metagol, 2016.

15. Andrew Cropper and Stephen H. Muggleton. Learning efﬁcient logic programs. Machine Learning,

108(7):1063–1083, Jul 2019.

16. Andrew Cropper and Sophie Tourret. Derivation reduction of metarules in meta-interpretive learn-

ing. In ILP, volume 11105 of Lecture Notes in Computer Science, pages 1–21. Springer, 2018.

17. Andrew Cropper and Sophie Tourret. Logical minimisation of metarules. Machine Learning, 2019.

To appear.

18. Evgeny Dantsin, Thomas Eiter, Georg Gottlob, and Andrei Voronkov. Complexity and expressive

power of logic programming. ACM Computing Surveys (CSUR), 33(3):374–425, 2001.

19. Luc De Raedt, Angelika Kimmig, and Hannu Toivonen. Problog: A probabilistic prolog and its appli-

cation in link discovery. In IJCAI, volume 7, pages 2462–2467, 2007.

20. Luc De Raedt and Ingo Thon. Probabilistic rule learning. In International Conference on Inductive

Logic Programming, pages 47–58. Springer, 2010.

21. Asim Kumar Debnath, Rosa L Lopez de Compadre, Gargi Debnath, Alan J Shusterman, and Cor-
win Hansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro com-
pounds. correlation with molecular orbital energies and hydrophobicity. Journal of medicinal chem-
istry, 34(2):786–797, 1991.

22. Michael O’Gordon Duff and Andrew Barto. Optimal Learning: Computational procedures for Bayes-
adaptive Markov decision processes. PhD thesis, University of Massachusetts at Amherst, 2002.
23. Sašo Džeroski, Luc De Raedt, and Kurt Driessens. Relational reinforcement learning. Machine learn-

ing, 43(1-2):7–52, 2001.

24. Richard Evans and Edward Grefenstette. Learning explanatory rules from noisy data. J. Artif. Intell.

Res., 61:1–64, 2018.

25. Hilmar Finnsson et al. Simulation-based general game playing. Doctor of philosophy, School of

Computer Science, Reykjavík University, 2012.

26. Martin Gebser, Roland Kaminski, Benjamin Kaufmann, and Torsten Schaub. Clingo = ASP + control:

Preliminary report. CoRR, abs/1405.3694, 2014.

27. Martin Gebser, Benjamin Kaufmann, Roland Kaminski, Max Ostrowski, Torsten Schaub, and Marius
Schneider. Potassco: The potsdam answer set solving collection. Ai Communications, 24(2):107–124,
2011.

28. Michael Genesereth and Michael Thielscher. General game playing. Synthesis Lectures on Artiﬁcial

Intelligence and Machine Learning, 8(2):1–229, 2014.

29. Michael R. Genesereth and Yngvi Björnsson. The international general game playing competition.

AI Magazine, 34(2):107–111, 2013.

30. John Goodacre. Inductive learning of chess rules using Progol. PhD thesis, University of Oxford, 1996.
31. Peter Gregory, Henrique Coli Schumann, Yngvi Björnsson, and Stephan Schiffel. The grl system:
In Computer Games, pages 130–148.

learning board game rules with piece-move interactions.
Springer, 2015.

32. Martin Grohe and Martin Ritzert. Learning ﬁrst-order deﬁnable concepts over structures of small
In Logic in Computer Science (LICS), 2017 32nd Annual ACM/IEEE Symposium on, pages

degree.
1–12. IEEE, 2017.

33. Arthur Guez, David Silver, and Peter Dayan. Efﬁcient bayes-adaptive reinforcement learning using
sample-based search. In Advances in Neural Information Processing Systems, pages 1025–1033, 2012.

38

Andrew Cropper et al.

34. Geoffrey E Hinton. Learning distributed representations of concepts. In Proceedings of the eighth

annual conference of the cognitive science society, volume 1, page 12. Amherst, MA, 1986.

35. Katsumi Inoue, Andrei Doncescu, and Hidetomo Nabeshima. Completing causal networks by meta-

level abduction. Machine learning, 91(2):239–277, 2013.

36. Matti Järvisalo, Daniel Le Berre, Olivier Roussel, and Laurent Simon. The international SAT solver

competitions. AI Magazine, 33(1), 2012.

37. Lukasz Kaiser. Learning games from videos guided by descriptive complexity. In AAAI, 2012.
38. Tobias Kaminski, Thomas Eiter, and Katsumi Inoue. Exploiting answer set programming with exter-

nal sources for meta-interpretive learning. TPLP, 18(3-4):571–588, 2018.

39. Michael J Kearns and Satinder P Singh. Finite-sample convergence rates for q-learning and indirect

algorithms. In Advances in neural information processing systems, pages 996–1002, 1999.

40. Frédéric Koriche, Sylvain Lagrue, Éric Piette, and Sébastien Tabary. Stochastic constraint program-
ming for general game playing with imperfect information. In General Intelligence in Game-Playing
Agents (GIGA?16) at the 25th International Joint Conference on Artiﬁcial Intelligence (IJCAI?16),
pages, 2016.

41. Frédéric Koriche, Sylvain Lagrue, Éric Piette, and Sébastien Tabary. Woodstock: un programme-
Revue d?intelligence artiﬁcielle–no,

joueur générique dirigé par les contraintes stochastiques.
307:336, 2017.

42. J. Larson and Ryszard S. Michalski. Inductive inference of VL decision rules. SIGART Newsletter,

63:38–44, 1977.

43. Mark Law.
2018.

Inductive Learning of Answer Set Programs. PhD thesis, Imperial College London, UK,

44. Mark Law, Alessandra Russo, and Krysia Broda.

In
Logics in Artiﬁcial Intelligence - 14th European Conference, JELIA 2014, Funchal, Madeira, Portugal,
September 24-26, 2014. Proceedings, pages 311–325, 2014.

Inductive learning of answer set programs.

45. Mark Law, Alessandra Russo, and Krysia Broda. The ILASP system for learning answer set programs.

https://www.doc.ic.ac.uk/~ml1909/ILASP, 2015.

46. Mark Law, Alessandra Russo, and Krysia Broda. Learning weak constraints in answer set program-

ming. Theory and Practice of Logic Programming, 15(4-5):511–525, 2015.

47. Mark Law, Alessandra Russo, and Krysia Broda.

Iterative learning of answer set programs from

context dependent examples. Theory and Practice of Logic Programming, 16(5-6):834–848, 2016.

48. Mark Law, Alessandra Russo, and Krysia Broda. The complexity and generality of learning answer

set programs. Artiﬁcial Intelligence, 259:110–146, 2018.

49. Mark Law, Alessandra Russo, and Krysia Broda.

Inductive learning of answer set programs from

noisy examples. Advances in Cognitive Systems, 2018.

50. Mark Law, Alessandra Russo, James Cussens, and Krysia Broda. The 2016 competition on Inductive

Logic Programming. http://ilp16.doc.ic.ac.uk/competition/, 2016.

51. Vladimir Lifschitz. What is answer set programming?. In AAAI, volume 8, pages 1594–1597, 2008.
52. Dianhuan Lin, Eyal Dechter, Kevin Ellis, Joshua B. Tenenbaum, and Stephen Muggleton. Bias re-
formulation for one-shot function induction.
In Torsten Schaub, Gerhard Friedrich, and Barry
O’Sullivan, editors, ECAI 2014 - 21st European Conference on Artiﬁcial Intelligence, 18-22 August
2014, Prague, Czech Republic - Including Prestigious Applications of Intelligent Systems (PAIS 2014),
volume 263 of Frontiers in Artiﬁcial Intelligence and Applications, pages 525–530. IOS Press, 2014.
53. Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In Machine

Learning Proceedings 1994, pages 157–163. Elsevier, 1994.

54. Nathaniel Love, Timothy Hinrichs, David Haley, Eric Schkufza, and Michael Genesereth. General
game playing: Game description language speciﬁcation. Technical report, Stanford Logic Group
Computer Science Department Stanford University, Technical Report LG-2006-01, 2008.

55. Eduardo M. Morales. Learning playing strategies in chess. Computational Intelligence, 12:65–87,

1996.

56. Stephen Muggleton. Inverse entailment and progol. New Generation Comput., 13(3&4):245–286,

1995.

57. Stephen Muggleton, Michael Bain, Jean Hayes Michie, and Donald Michie. An experimental compar-
ison of human and machine learning formalisms. In Alberto Maria Segre, editor, Proceedings of the
Sixth International Workshop on Machine Learning (ML 1989), Cornell University, Ithaca, New York,
USA, June 26-27, 1989, pages 113–118. Morgan Kaufmann, 1989.

58. Stephen Muggleton, Aline Paes, Vítor Santos Costa, and Gerson Zaverucha. Chess revision: Acquiring
the rules of chess variants through FOL theory revision from examples.
In Luc De Raedt, editor,
Inductive Logic Programming, 19th International Conference, ILP 2009, Leuven, Belgium, July 02-04,
2009. Revised Papers, volume 5989 of Lecture Notes in Computer Science, pages 123–130. Springer,
2009.

Inductive general game playing

39

59. Stephen Muggleton, Luc De Raedt, David Poole, Ivan Bratko, Peter A. Flach, Katsumi Inoue, and
Ashwin Srinivasan. ILP turns 20 - biography and future challenges. Machine Learning, 86(1):3–23,
2012.

60. Stephen H. Muggleton, Dianhuan Lin, Niels Pahlavi, and Alireza Tamaddoni-Nezhad. Meta-
interpretive learning: application to grammatical inference. Machine Learning, 94(1):25–49, 2014.
61. Stephen H. Muggleton, Dianhuan Lin, and Alireza Tamaddoni-Nezhad. Meta-interpretive learning of
higher-order dyadic datalog: predicate invention revisited. Machine Learning, 100(1):49–73, 2015.
62. Andrej Oblak and Ivan Bratko. Learning from noisy data using a non-covering ILP algorithm. In

International Conference on Inductive Logic Programming, pages 190–197. Springer, 2010.

63. Ramón P Otero.

Induction of stable models.

In Inductive Logic Programming, pages 193–205.

Springer, 2001.

64. J. Ross Quinlan. Learning logical deﬁnitions from relations. Machine Learning, 5:239–266, 1990.
65. Luc De Raedt. Logical and relational learning. Cognitive Technologies. Springer, 2008.
66. Fabrizio Riguzzi, Elena Bellodi, and Riccardo Zese. A history of probabilistic inductive logic pro-

gramming. Frontiers in Robotics and AI, 1:6, 2014.

67. Fabrizio Riguzzi, Elena Bellodi, Riccardo Zese, Giuseppe Cota, and Evelina Lamma. Scaling struc-
In European Conference on Artiﬁcial

ture learning of probabilistic logic programs by mapreduce.
Intelligence, 2016.

68. Jonathan Schaeffer, Robert Lake, Paul Lu, and Martin Bryant. CHINOOK: the world man-machine

checkers champion. AI Magazine, 17(1):21–29, 1996.

69. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,
Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi by
self-play with a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815, 2017.
70. A. Srinivasan. The ALEPH manual. Machine Learning at the Computing Laboratory, Oxford University,

2001.

71. Ashwin Srinivasan, Ross Donald King, S. H Muggleton, and M.J.E. Sternberg. Carcinogenesis pre-

dictions using ILP. Inductive Logic Programming, 1297:273–287, 1997.

