9
1
0
2

n
u
J

2
1

]

G
L
.
s
c
[

1
v
8
0
9
4
0
.
6
0
9
1
:
v
i
X
r
a

SPoC: Search-based Pseudocode to Code

Sumith Kulal∗ , Panupong Pasupat∗, Kartik Chandra, Mina Lee,
Oded Padon, Alex Aiken, Percy Liang
Department of Computer Science
Stanford University
{sumith,ppasupat,kach,minalee,padon,aaiken,pliang}@cs.stanford.edu

Abstract

We consider the task of mapping pseudocode to long programs that are functionally
correct. Given test cases as a mechanism to validate programs, we search over the
space of possible translations of the pseudocode to ﬁnd a program that passes the
validation. However, without proper credit assignment to localize the sources of
program failures, it is difﬁcult to guide search toward more promising programs.
We propose to perform credit assignment based on signals from compilation errors,
which constitute 88.7% of program failures. Concretely, we treat the translation
of each pseudocode line as a discrete portion of the program, and whenever a
synthesized program fails to compile, an error localization method tries to identify
the portion of the program responsible for the failure. We then focus search over
alternative translations of the pseudocode for those portions. For evaluation, we
collected the SPoC dataset (Search-based Pseudocode to Code) containing 18,356
programs with human-authored pseudocode and test cases. Under a budget of 100
program compilations, performing search improves the synthesis success rate over
using the top-one translation of the pseudocode from 25.6% to 44.7%.

1

Introduction

We consider the task of mapping natural language descriptions to functionally correct computer
programs that are long enough to have signiﬁcant intermediate state (e.g., 10–20 lines) and perform
non-trivial computations. Previous work on executable semantic parsing mainly focuses on translating
short text descriptions to a one-line program [42, 35, 43, 44, 21, 8], and while recent work explored
generating longer programs from text descriptions [22, 40, 29, 15, 16, 14], these programs are mostly
evaluated on syntactic metrics (e.g., exact match and BLEU score) rather than functional correctness.
In contrast, program synthesis in the programming languages community emphasizes producing
programs with the correct semantics, typically captured by a set of input-output test cases that the
program must compute correctly [11, 10]. However, input-output pairs usually give little information
about the intermediate states of the program, making it difﬁcult to synthesize long programs.

Synthesizing a general class of programs of signiﬁcant length and internal complexity is extremely
challenging without some description of the steps of computation. To that end, we propose a
framework for synthesizing programs from natural language pseudocode and test cases. The test
cases provide the semantic speciﬁcation, while the pseudocode provides guidance for the intermediate
computations the program should perform.

To synthesize a functionally correct program, instead of relying on the top-one translation of the
pseudocode, we search over the space of possible translations to ﬁnd one that passes the test cases. In
this work, we treat the translation of each pseudocode line as a discrete portion of the program. As
illustrated in Figure 1, each pseudocode line translates to a code line with approximately one or two

∗Equal contributions.

Preprint. Under review.

 
 
 
 
 
 
xi

yi

int main() {

i

1
2
3
4
5
6
7
8
9
10
11
12

in function main
let n be integer
read n
let A be vector of integers
set size of A = n
read n elements into A
for all elements in A
set min_i to i
for j = i + 1 to size of A exclusive
set min_i to j if A[min_i] > A[j]

swap A[i], A[min_i]
print all elements of A

}

int n;
cin >> n;
vector<int> A;
A.resize(n);
for(int i = 0; i < A.size(); i++) cin >> A[i];
for(int i = 0; i < A.size(); i++) {

int min_i = i;
for(int j = i+1; j < A.size(); j++) {
if(A[min_i] > A[j]) { min_i = j; }

swap(A[i], A[min_i]);

for(int i=0; i<A.size(); i++) cout<<A[i]<<" ";

Public test case 1 (out of 5):
Hidden test case 1 (out of 8):

5 3 2 4 1 5
8 9 2 4 5 6 2 7 1 → 1 2 2 4 5 6 7 9

→ 1 2 3 4 5

Figure 1: Given L pseudocode lines x1:L (with indentation levels (cid:96)1:L) and public test cases, our task
is to synthesize a program with code lines y1:L. The program is evaluated against both public and
hidden test cases.

atomic statements, and a program can be synthesized by choosing a candidate translation for each
pseudocode line.

However, common search methods for machine translation, such as beam search over the possible
sequences of code tokens [41], only use the sparse reward of whether the program succeeds. Without
a proper credit assignment to pinpoint the causes of program failures, it is difﬁcult to guide search
toward more promising programs. Since empirically 88.7% of failures during search are due to com-
pilation errors, we propose to perform credit assignment based on signals extracted from compilation
results. At a high level, when a program fails to compile, we use an error localization method to
identify which portion of the program is responsible for the failure, and then focus the search on
alternative translations of the pseudocode for that portion.

We propose two error localization methods. The ﬁrst method uses a multiclass classiﬁer to pick one
of the code lines as the offending line, which is then down-weighted in subsequent search iterations.
In contrast to previous error correction models [12], our model also uses the error message and
pseudocode for prediction. This is crucial when the compilation error can be ﬁxed in multiple ways,
but only some of which are consistent with the pseudocode. The second method, preﬁx-based pruning,
uses additional compilations to ﬁnd a code preﬁx that causes the error. Unlike the classiﬁcation
model, the identiﬁed code preﬁx is guaranteed to be erroneous and can be blacklisted entirely.
For evaluation, we collected and release a new dataset, SPoC (Search-based Pseudocode to Code)2,
containing 18,356 C++ programs (14.7 lines on average). In contrast to other language-to-code
datasets [22, 27, 16], all programs contain multiple test cases for validation. And in contrast to
the closely related NAPS dataset [41], which also contains test cases but only 6% human-authored
pseudocode, all programs in SPoC are associated with human-authored pseudocode of a consistent
annotation granularity. Section 3 details the comparison between SPoC and related datasets.

Using the top-one translation of the pseudocode yields a success rate of 24.6% on the test set. Under
a limited budget of 100 synthesis trials (i.e., 100 code compilations and executions), our best method
achieves a success rate of 44.7%. The multiclass error localization model reduces the number of
synthesis trials needed in 15.5% of the programs, with a median absolute reduction of 26 trails and
a median relative reduction of 42%. On the other hand, preﬁx-based pruning slightly increases the
number of compilations for easier problems, but is more effective on harder programs, making it
outperform the multiclass classiﬁer under larger budgets.

2 The dataset can be downloaded at https://cs.stanford.edu/~sumith/spoc/.

2

2 Problem statement

Figure 1 illustrates the setup of our synthesis task. The system is given (a) a sequence x of L
pseudocode lines x1, x2, . . . , xL, where each xi is a string with indentation level (cid:96)i; and (b) k
public test cases in the form of input-output string pairs (T in
). The task is
to synthesize a program y consisting of L code lines y1, y2, . . . , yL. The program is accepted if it
successfully compiles and passes all public test cases (i.e., the compiled binary prints the string T out
i
k(cid:48) , ˜T out
i ) as well as k(cid:48) additional hidden test cases ( ˜T in
after reading the input T in
k(cid:48) ).
At training time, the system has access to a training dataset where each example contains pseudocode
x, a gold program y, and both public and hidden test cases.

), . . . , ( ˜T in

), . . . , (T in

1 , ˜T out

k , T out
k

1 , T out
1

1

At test time, the system has access to pseudocode x, public test cases (not hidden ones), and a
computation budget. For a fair comparison under different computing environments, we use the
number of synthesis trials as the budget, where in each trial, the system can issue a single call to the
compiler and execute the compiled program on public test cases. The system must output a single
ﬁnal program, which will be validated on both public and hidden test cases.

3 Dataset

Recall that our goal is to synthesize programs of signiﬁcant length and complexity. To this end, we
argue that it is important to have both description of the intermediate computation and a functional
speciﬁcation. Table 1 shows that most existing datasets [22, 27, 16] have some varying levels of
description, but lack mechanisms to validate the correctness of programs. This inevitably leads
previous work to resort to proxy metrics, such as exact match accuracy, BLEU score, and tree node
F1 score, which only measure syntactic similarity rather than functional correctness [22, 40, 29, 15,
16, 14].

One notable exception and the inspiration for our work is the NAPS dataset [41] which contains
both description (pseudocode) and a functional speciﬁcation (test cases) of competitive programming
problems. However, most of their pseudocode is generated by heuristic rule-based templates, which
in turn has a low information content compared to the human-authored pseudocode. Furthermore, the
dataset suffers from the inconsistent granularity of text description, as the artiﬁcial pseudocode is
ﬁne-grained (e.g., “increase var0 by 1”) whereas the human-written pseudocode tends to be abstract
(e.g., “compute factorial”) as the annotators were encouraged to provide high-level descriptions. This
discrepancy is reﬂected on the ratio of the length of pseudocode to that of code, which is 1:1.82 in
their synthetic dataset, and 1:3.26 in their human-authored dataset.

As no existing dataset contains both high-quality human-authored description with a consistent level
of granularity and a mechanism to validate functional correctness, we created a new dataset called
SPoC (Search-based Pseudocode to Code), which consists of programs, pseudocode, and test cases.
The programs are non-trivial solutions to competitive programming problems, and each program is
paired with public and hidden test cases. We collected natural language pseudocode for each code
line from curated crowdworkers, which by design, ensures the consistent granularity of description.

3.1 Data collection

Programs and test cases. Similar to the NAPS dataset [41], we scraped competitive programming
problems and their test cases from codeforces.com. Each problem has multiple programs submitted
by participants as solutions to the problem. We collected accepted C++ programs from problems
marked as the easiest level based on their metric. Based on our pilot study, we ﬁltered out programs
with constructs that are difﬁcult to consistently annotate with pseudocode (i.e., programs with
#define macros, classes, structs, templates, switch statements, and mallocs).

Decomposition. We decompose each program into code lines. To obtain slightly higher-level
descriptions for common constructs, we group any block with only one statement with the preceding

1We counted the number of programs in the released dataset. Since the programs are provided as a sequence

of tokens, the number of lines per program is approximated based on the number of ;, {, and }.

2We excluded partial programs (smaller pieces of full programs) in the dataset when counting.

3

Table 1: Datasets for natural language to code. In contrast to other datasets, our SPoC dataset contains
human-authored pseudocode with a consistent granularity of description and test cases.

Programming language
Number of programs (total)
Lines per program (average)

Type of natural language input
Additional input
Granularity of text description

Fraction of human-annotated text
Number of annotators (total)

Test cases
Number of test cases (average)

MTG
[22]

Java
13,297
30.4

HS
[22]

Python
665
7.7

— card text —
— card metadata —
program program
(class)
(class)
100%
100%
n/a
n/a
(cid:55)
(cid:55)
-
-

Python
18,805
1

comment
-
line

100%
1
(cid:55)
-

DJANGO
[27, 22]

CONCODE1
[16]

NAPS2
[41]

UAST
17,477
21.7

SPoC

C++
18,356
14.7

Java
2,184,310
4.4

-
line

-
varies

documentation — pseudocode —
class context
program
(method)
100%
n/a
(cid:55)
-

100%
59
(cid:51)
38.6

6%
n/a
(cid:51)
7.5

control statement (e.g., the one-line for loop “for (int i = 0; i < n; i++) cin >> x[i];”
allows a high-level description “read n values into x”).

Pseudocode. We recruited 59 crowdworkers on Amazon Mechanical Turk to write pseudocode
for each line of code. To our surprise, we were able to identify the workers (rather than curated
specialists) who are capable of annotating C++ code by using a qualiﬁcation round, in which we
manually inspected their initial annotations.

Statistics. Our dataset contains 18,356 programs submitted for 677 programming problems. Each
problem has roughly 27 programs, which are likely to have similar semantics yet different code
syntax. Excluding closing braces and the common “int main()” line, each program contains an
average of 14.7 lines (with the minimum of 1 and maximum of 457 lines of code). The average length
of code lines is 9.08 tokens, while the average length of pseudocode lines is 7.86 tokens.

Training and test sets. To evaluate the generalization on unseen problems and annotation styles,
we created two test sets. We generated the ﬁrst test set TESTP by splitting based on problems: we
held out 158 problems (23% out of 677 problems), which is equivalent to 1,820 programs (10.1%
of all programs). The second test set TESTW is split by workers: we held out 7 workers (12% out
of 59 workers), which is equivalent to 1,752 programs (9.7% of all programs, with 186 programs
overlapping with TESTP). We used the remaining data for training and development (90:10 split).

4 Base approach

As illustrated in Figure 2, our base approach to synthesizing a program y1:L from pseudocode x1:L
and public test cases involves two steps. First, a translation model encodes each pseudocode line
xi and generates M candidate code lines ci1, . . . , ciM to be used as the ith code line. Then, we
search over the possible combinations of candidate translations until we ﬁnd a program ˆy1:L that
successfully compiles and passes all public test cases.

Translation. To generate candidate code lines, we use a standard seq2seq translation model with an
LSTM encoder and decoder [19], attention-based copying mechanism [24, 37], and coverage vector
[34]. After encoding the pseudocode line xi, we apply beam search with beam size M to produce a
ranked list of candidates translations Ci = (ci1, . . . , ciM ), where each code line cij is a sequence
of string tokens. (We use M = 100 for our experiments.) The model also assigns a probability
pij = p(cij | xi) for each candidate cij. The translation model is trained on pairs (xi, yi) from the
training data using the standard log-likelihood objective.

Best-ﬁrst search. We now describe a basic approach for searching over the space of possible
programs. Given the candidate lists C1, . . . , CL, we can synthesize a program ˆy by picking a
candidate cij[i] from each Ci (where j[i] ∈ {1, . . . , M }) and then concatenate them into a program.

4

Figure 2: Illustration of best-ﬁrst search and error localization model. In this example, (c11, c22, c32)
satisﬁes the test cases. Best-ﬁrst search iterates in the order of decreasing probabilities and succeeds
in four compiler calls. The error localization method down-weights c21, leading to an earlier success.

In our search algorithm, we iterate through programs ˆy in the descending order of probability
p(ˆy) = (cid:81) pij[i]. To do so, we maintain a heap of the combinations ˆy = (c1j[1], . . . , cLj[L]) indexed
by p(ˆy). The heap initially contains the program (c11, . . . , cL1), which is the top-one translation of
the pseudocode. In each iteration, we pop a program (c1j[1], . . . , cLj[L]) from the heap and test it.
If the program fails (either from a compilation error, a runtime error, or a mismatch between the
actual and expected test outputs), we push modiﬁed programs (c1j[1], . . . , ci(j[i]+1), . . . , cLj[L]) for
all i ∈ {1, . . . , L} that have not been explored to the heap. We continue searching until we either
ﬁnd a program that passes all public test cases or fully utilize the computation budget.

5 Error localization

So far, we have been treating program compilation and execution as a black box that only tells
whether a program passes its test cases. This sparse signal makes the search process less effective.
For instance, best-ﬁrst search will keep using an erroneous candidate cij if its probability pij is high.

To speed up search, we unpack the black box and extract more detailed search signals. In this work,
we focus on compilation errors, which constitute 88.7% of the failure cases in best-ﬁrst search. When
a program ˆy = (c1j[1], . . . , cLj[L]) fails to compile, the compiler will report error messages along
with the line numbers where the errors occur. Unfortunately, the reported line numbers do not always
correspond to the actual location of the mistake (e.g., the error “‘n’ was not declared in this scope”
can occur long after the line where n should be declared according to the pseudocode). Empirically,
the reported line number does not match the actual incorrect line 21.7% of the time.

Therefore, we treat the compilation error message as a noisy signal, and propose to use an error
localization method to infer the actual portion of the code that causes the error. As illustrated in
Figure 2, the error localization method has access to the pseudocode x, the synthesized code ˆy,
and the ﬁrst error message (ierr, merr) from the compiler, where ierr is a line number and merr is
a message string. It can then either detect the offending code lines or abstain. Depending on the
method, we either down-weight or blacklist the translation candidates in the offending code lines.

We now introduce two error localization methods: multiclass classiﬁcation, which uses a neural
model to predict a single offending line; and preﬁx-based pruning, which uses additional calls to the
compiler for detecting an erroneous code preﬁx.

Multiclass classiﬁcation. We train a neural multiclass classiﬁer to predict the offending line i∗
among the L lines. Our model is similar to the error correction model in [12]. For each line i, we
embed the tokens of xi, yi, and merr, and then use three separate LSTMs to encode the sequences.
We concatenate the ﬁnal LSTM hidden states with the positional encoding [36] of the line offset

5

x1x2x3translatec11(p11= 0.7)c12(p12= 0.2)c13(p13= 0.15)c21(p21= 0.4)c22(p22= 0.3)c23(p23= 0.05)c31(p31= 0.3)c32(p32= 0.2)c33(p33= 0.05)best-first search1.........C1:C2:C3:234→ compilation error→ runtime error→ compilation error→ success123→ runtime error→ successi* = 2down-weight c21model3:error: ‘n’ was not declaredbest-first search with error localization(a) Line-level accuracy (%)
at ranks 1, 5, 10, and 100

(b) Number of lines where
the top candidate is incorrect

(c) Number of lines where
no candidate is correct

Rank

TESTP

TESTW

1
5
10
100

84.0
89.1
90.0
92.0

87.0
91.8
92.9
94.7

TESTP

0

1

2

3

4+

TESTW

0

1

2

3

4+

0

0

1

3+2

1

2

0

20

40

60

80 100

0

20

40

60

80 100

% of programs

% of programs

Figure 3: (a) While the translation accuracy is high at the line level, we need to consider the result at
the program level. For each program, we count the number of lines i where (b) the top candidate ci1
is incorrect, and (c) none of the candidates cij ∈ Ci is correct.

∆i = ierr − i, and then apply a feedforward network to produce the line embedding of line i. The L
line embeddings are then passed through another LSTM, and the hidden state of each cell i is passed
through a feedforward network to compute the logit for line i. We return the line i∗ with the highest
probability (softmax over logits) if that probability exceeds a threshold βmul and abstain otherwise.
We use βmul = 0.95 for the experiments.
Given i∗, we down-weight the current translation candidate of the line i∗ so that it is used less often in
subsequent search iterations. Concretely, we multiply the probability pi∗j[i∗] of the current candidate
ci∗j[i∗] in line i∗ with a constant factor α < 1. As this affects the heap indices, we rebuild the heap
from scratch (which takes negligible time) and continue the search, skipping any program that has
already been explored before the heap rebuild.

To construct a dataset for training the model, we consider each program y = y1:L in the synthesis
training dataset, substitute a single line yi∗ with a candidate ci∗j ∈ Ci∗ generated from pseudocode
line xi∗ , and then collect any modiﬁed program y(cid:48) that produces a compilation error with an error
message (ierr, merr). The model is trained to maximize the log-likelihood of the offending lines i∗.

Preﬁx-based pruning. The multiclass classiﬁcation method does not guarantee that the predicted
line i∗ is actually an offending line. Furthermore, a candidate code line might be offending in some
contexts but not others (e.g., a variable re-declaration is no longer offending if the previous declaration
no longer exists). To address these issues, we propose an alternative that uses additional compiler
calls to ﬁnd an offending preﬁx of the program. Concretely, when a compilation error occurs, we use
the compiler to to ﬁnd the minimum i∗ such that the preﬁx (c1j[1], . . . , ci∗j[i∗]), plus closing braces
to complete the program, fails to compile. Since programs containing that preﬁx will also always fail
(with very rare exceptions), we can safely blacklist the preﬁx from future search iterations.

Each additional compiler call is counted as one trial toward the synthesis budget. To save the budget,
we only test i∗ = ierr − ∆i where ∆i ∈ {0, 1, 2} corresponds to the three most frequent offsets. If
we fail to ﬁnd an offending preﬁx, we simply abstain.

6 Experiments

Our main evaluation metric is success rate at B: the fraction of test examples where the system
generates an accepted program under the budget of B trials. For error localization methods, we also
consider the reduction in the number of trials used compared to normal best-ﬁrst search.

Translation accuracy. When evaluating the translation model, surface-form metrics such as exact
sequence match and BLEU scores fail to account for functional correctness of the code. For instance,
a prediction “if (b)” is functionally equivalent to the gold code “if (b == true)” when b is
a boolean. Hence, we instead evaluate the functional correctness of the translation. To check if a
predicted code line cij ∈ Ci is functionally correct, we replace the code line yi in the gold program
with cij and then verify whether the program still passes both public and hidden test cases.

The results in Figure 3(a) shows that when the lines are considered independently, the translation
model achieves a high accuracy of 84–87% under this notion of functional correctness. However,
the picture is grimmer when we consider the statistics at the program level, which is what matters
for synthesis. For each program, we count the number of lines i where the top candidate ci1 is

6

TESTP

success rate (%)

budget B

40

38

36

34

32

TESTW

success rate (%)

budget B

60

58

56

54

52

0

1,000

2,000

3,000

0

1,000

2,000

3,000

B =

10

100

1000

3000

B =

10

100

1000

3000

no localization
i∗ = ierr
multiclass
preﬁx-based

26.5
28.0
28.4
25.3

32.5
32.5
34.2
33.0

37.5
34.3
38.3
37.9

39.1
34.7
39.2
40.3

no localization
i∗ = ierr
multiclass
preﬁx-based

42.5
43.5
44.4
41.0

51.0
50.0
53.7
50.5

57.3
52.6
58.6
57.9

59.4
53.1
60.3
60.7

top-one (B = 1): 17.8

oracle (B = ∞): 55.2

top-one (B = 1): 30.7

oracle (B = ∞): 71.4

Figure 4: Success rates at budgets B of best-ﬁrst search with different error localization methods.

Table 2: Effects of using error localization methods on all test examples.

number of trials:

absolute difference

relative difference

method

effect compared to best-ﬁrst

count

mean median

geo.mean median

multiclass

improves number of trials
failed to synthesize → succeeds

13.5 % –199.5
2.0 %

–26.0

×0.39

×0.58

worsens number of trials
succeeded → fails to synthesize

0.4 % +407.5
1.6 %

+123.0

×6.70

×7.04

preﬁx-based

improves number of trials
failed to synthesize → succeeds

4.1 % –272.6
1.5 %

–91.0

×0.45

×0.57

worsens number of trials
succeeded → fails to synthesize

15.7 %
0.3 %

+68.4

+12.0

×1.65

×1.63

not functionally correct. Figure 3(b) shows that only 18.2% of programs in TESTP and 32.0% of
programs in TESTW have the top candidate correct in every line. As code lines that are functionally
correct in isolation may be incompatible one another, the programs formed by combining the top
candidate of each line have an even lower success rates of 17.8% on TESTP and 30.7% on TESTW.

Oracle success rate. To compute the maximum achievable success rate given the lists of candidates,
for each program, we count the number of lines i where the candidate list Ci does not have any correct
candidate. Figure 3(c) shows that 44.8% of programs in TESTP and 28.6% of programs in TESTW
have least one difﬁcult line where the translation model does not produce a correct prediction among
the top M = 100 candidates. This means a synthesizer with an inﬁnite search budget would achieve
a maximum success rate of 55.2% on TESTP and 71.4% on TESTW given our lists of candidates
(assuming that incorrect candidates do not give a correct behavior when combined together).

Synthesis results. Figure 4 compares the success rates of best-ﬁrst search with and without error
localization. As a baseline, we try down-weighting the reported error line (i∗ = ierr) whenever a
compilation error occurs. Due to the mismatch between the actual offending line and the reported line,
the synthesis result deteriorates. Up to the compilation budget of around B = 1500, the multiclass
classiﬁer improves the success rate the most. Preﬁx-based pruning achieves better success rates for
higher budgets, but since it uses additional compilation calls, it performs worse under tighter budgets.

Table 2 details how the error localization methods affect the synthesis outcome. The multiclass model
decreases the number of trials in 15.5% of all examples, but since its predictions are not veriﬁed, the
model is also more prone to catastrophic failures. Preﬁx-based pruning uses additional compilation

7

(1)

. . .
let s be a string
string s ;
read s
cin >> s ;
if s is half
if ( s / 2 == 0 )
. . .

7

8

9

(2)

. . .
create int l, p and q
int a , p , q ;
read l, p and q
cin >> l >> p >> q ;
print l * p / (p + q)
cout << l * p / ( p + q ) << endl ;
. . .

2

3

4

Figure 5: Examples of programs synthesized during search. In Program 1, preﬁx-based pruning
detects that the preﬁx up to line 9 is offending. In Program 2, the multiclass model incorrectly predicts
line 3 as the offending line, which ultimately leads to a failure.

calls to verify its verdict, and thus slightly worsens the number of compilations needed in a large
number of examples. However, for more difﬁcult programs, the beneﬁt outweighs the cost.

Error analysis. To understand the behavior of error localization methods, we analyzed several
examples from the development data. Some prototypical examples are shown in Figure 5. Program
1 shows how error localization can improve search. The condition “s is half” in line 9 should be
translated as “s == "half"”, but was instead interpreted as “s / 2 == 0” and “s % 2 == 0”
with high probability, and hence best-ﬁrst search spends a signiﬁcant amount of budget (1511 trails)
using these incorrect candidates in the combination. In contrast, preﬁx-based pruning detects them as
offending candidates and succeeds earlier (413 trials).

In contrast, Program 2 shows how an incorrect error localization can lead to a catastrophic failure.
Here, the multiclass model reads the error message merr = “‘l’ was not declared in this scope” with
line number ierr = 3, and incorrectly predicts that line 3 is an offending line. This causes the search
to ultimately fail whereas best-ﬁrst search ﬁnds a correct program in 80 search iterations.

7 Related work and discussion

Program synthesis. Program synthesis using test cases has been extensively studied in the pro-
gramming languages literature. The most focused and effective method is to formulate synthesis as a
constraint satisfaction problem [32, 33], which requires that the synthesis problem can be translated
to a theory with effective constraint solvers. For other problems, brute force enumeration of programs
(with some optimization) works surprisingly well [26, 3], but when the search space is too large
for enumeration, randomized search guided by a cost function can be effective [30]. Some works
combine aspects of multiple approaches (e.g., [18]). For program speciﬁcations, the norm is to use
input-output pairs. However, most synthesized programs are relatively short, and works that consis-
tently synthesize longer programs are in the domains where the intermediate computation is easier to
recover from input and output, such as string [11, 28, 6] and data structure transformations [10, 38].
For other domains, while input-output examples are precise in evaluating functional correctness, they
provide mostly global signals and inform very little about the intermediate computation, thereby
requiring other forms of speciﬁcation along with input-output examples (e.g., [9, 31]).

Semantic parsing. Works on translating natural language speciﬁcations to executable programs, as
discussed in Section 3, are closely related to semantic parsing whose goal is to map natural language
utterances to formal representation. One of its traditional tasks is to parse a given question (usually a
single sentence) into an executable database query [42, 43, 20, 4, 44, 39]. Instead of a single query,
some work aims to parse a sequence of utterances into queries that can be sequentially executed
[2, 17, 23]. However, the sequences are still relatively short (e.g., maximum 5 sentences).

Error localization. Error localization in the context of automated program repair has been an
active topic of research. Many recent work that uses neural models to localize errors has focused on
localizing and correcting syntax errors [12] or a class of well-deﬁned semantic errors such as variable
misuse and variable replace [1, 7, 25]. Other work identiﬁes error locations by interpreting compiler
error messages [13, 5]. Likewise, our multiclass error localization model uses compilation errors to
locate offending code; however, since the code is tied to pseudocode, we also use the signal from
pseudocode to distinguish ambiguous cases (e.g., in Program 2 of Figure 5, while changing either line

8

2 or line 3 can ﬁx the error, a correct model should choose line 2 as the offending line with respect to
the pseudocode.)

Acknowledgements

We thank Shivam Garg, Jason Koenig, Nadia Polikarpova, Alex Polozov and Rishabh Singh for
valuable feedback at different stages of the project. This work was supported by NSF grant CCF-
1409813, NSF CAREER Award IIS-1552635 and a grant from Amazon.

References

[1] M. Allamanis, M. Brockschmidt, and M. Khademi. Learning to represent programs with graphs.

In International Conference on Learning Representations (ICLR), 2018.

[2] J. Andreas and D. Klein. Alignment-based compositional semantics for instruction following.

In Empirical Methods in Natural Language Processing (EMNLP), 2015.

[3] S. Bansal and A. Aiken. Automatic generation of peephole superoptimizers. In Architectural

Support for Programming Languages and Operating Systems (ASPLOS), 2006.

[4] J. Berant, A. Chou, R. Frostig, and P. Liang. Semantic parsing on Freebase from question-answer

pairs. In Empirical Methods in Natural Language Processing (EMNLP), 2013.

[5] S. Bhatia, P. Kohli, and R. Singh. Neuro-symbolic program corrector for introductory program-

ming assignments. In International Conference on Software Engineering (ICSE), 2018.
[6] J. Devlin, J. Uesato, S. Bhupatiraju, R. Singh, A. Mohamed, and P. Kohli. Robustﬁll: Neural
program learning under noisy i/o. In International Conference on Machine Learning (ICML),
2017.

[7] J. Devlin, J. Uesato, R. Singh, and P. Kohli. Semantic code repair using neuro-symbolic

transformation networks. arXiv preprint arXiv:1710.11054, 2017.

[8] L. Dong and M. Lapata. Coarse-to-ﬁne decoding for neural semantic parsing. In Association

for Computational Linguistics (ACL), 2018.

[9] Y. Feng, R. Martins, Y. Wang, I. Dillig, and T. W. Reps. Component-based synthesis for complex

apis. In Principles of Programming Languages (POPL), 2017.

[10] J. K. Feser, S. Chaudhuri, and I. Dillig. Synthesizing data structure transformations from
input-output examples. In Programming Language Design and Implementation (PLDI), 2015.
[11] S. Gulwani. Automating string processing in spreadsheets using input-output examples. ACM

SIGPLAN Notices, 46(1):317–330, 2011.

[12] R. Gupta, S. Pal, A. Kanade, and S. K. Shevade. Deepﬁx: Fixing common C language errors by
deep learning. In Association for the Advancement of Artiﬁcial Intelligence (AAAI), 2017.
[13] B. Hartmann, D. MacDougall, J. Brandt, and S. R. Klemmer. What would other programmers
do: suggesting solutions to error messages. In Conference on Human Factors in Computing
Systems (CHI), 2010.

[14] T. Hashimoto, K. Guu, Y. Oren, and P. Liang. A retrieve-and-edit framework for predicting

structured outputs. In Advances in Neural Information Processing Systems (NeurIPS), 2018.

[15] S. A. Hayati, R. Olivier, P. Avvaru, P. Yin, A. Tomasic, and G. Neubig. Retrieval-based neural
code generation. In Empirical Methods in Natural Language Processing (EMNLP), 2018.
[16] S. Iyer, I. Konstas, A. Cheung, and L. S. Zettlemoyer. Mapping language to code in program-
matic context. In Empirical Methods in Natural Language Processing (EMNLP), 2018.
[17] M. Iyyer, W. Yih, and M. Chang. Answering complicated question intents expressed in

decomposed question sequences. CoRR, 0, 2016.

[18] S. Jha, S. Gulwani, S. A. Seshia, and A. Tiwari. Oracle-guided component-based program

synthesis. In International Conference on Software Engineering (ICSE), 2010.

[19] G. Klein, Y. Kim, Y. Deng, J. Senellart, and A. M. Rush. Opennmt: Open-source toolkit for

neural machine translation. arXiv preprint arXiv:1701.02810, 2017.

[20] P. Liang, M. I. Jordan, and D. Klein. Learning dependency-based compositional semantics. In

Association for Computational Linguistics (ACL), pages 590–599, 2011.

9

[21] X. V. Lin, C. Wang, L. S. Zettlemoyer, and M. D. Ernst. Nl2bash: A corpus and semantic
parser for natural language interface to the linux operating system. In Language Resources and
Evaluation Conference (LREC), 2018.

[22] W. Ling, E. Grefenstette, K. M. Hermann, T. Koˇciský, A. Senior, F. Wang, and P. Blunsom.
Latent predictor networks for code generation. In Association for Computational Linguistics
(ACL), pages 599–609, 2016.

[23] R. Long, P. Pasupat, and P. Liang. Simpler context-dependent logical forms via model projec-

tions. In Association for Computational Linguistics (ACL), 2016.

[24] M. Luong, H. Pham, and C. D. Manning. Effective approaches to attention-based neural
machine translation. In Empirical Methods in Natural Language Processing (EMNLP), pages
1412–1421, 2015.

[25] V. Marko, K. Aditya, M. Petros, B. David, and S. Rishabh. Neural program repair by jointly
learning to localize and repair. In International Conference on Learning Representations (ICLR),
2019.

[26] H. Massalin. Superoptimizer – a look at the smallest program. In Architectural Support for

Programming Languages and Operating Systems (ASPLOS), 1987.

[27] Y. Oda, H. Fudaba, G. Neubig, H. Hata, S. Sakti, T. Toda, and S. Nakamura. Learning to
IEEE/ACM

generate pseudo-code from source code using statistical machine translation.
International Conference on Automated Software Engineering (ASE), 30:574–584, 2015.

[28] E. Parisotto, A. Mohamed, R. Singh, L. Li, D. Zhou, and P. Kohli. Neuro-symbolic program

synthesis. In International Conference on Learning Representations (ICLR), 2017.

[29] M. Rabinovich, M. Stern, and D. Klein. Abstract syntax networks for code generation and

semantic parsing. In Association for Computational Linguistics (ACL), 2017.

[30] E. Schkufza, R. Sharma, and A. Aiken. Stochastic superoptimization. In Architectural Support

for Programming Languages and Operating Systems (ASPLOS), 2013.

[31] K. Shi, J. Steinhardt, and P. Liang. FrAngel: Component-based synthesis with control structures.

In Principles of Programming Languages (POPL), 2019.

[32] A. Solar-Lezama, L. Tancau, R. Bodik, V. Saraswat, and S. Seshia. Combinatorial sketching for
ﬁnite programs. In Architectural Support for Programming Languages and Operating Systems
(ASPLOS), 2006.

[33] R. Tate, M. Stepp, Z. Tatlock, and S. Lerner. Equality saturation: a new approach to optimization.

In Principles of Programming Languages (POPL), 2009.

[34] Z. Tu, Z. Lu, Y. Liu, X. Liu, and H. Li. Modeling coverage for neural machine translation. In

Association for Computational Linguistics (ACL), 2016.

[35] D. Vadas and J. R. Curran. Programming with unrestricted natural language. In Australasian

Language Technology Workshop (ALTA), 2005.

[36] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and

I. Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.

[37] O. Vinyals, M. Fortunato, and N. Jaitly. Pointer networks. In Advances in Neural Information

Processing Systems (NeurIPS), pages 2674–2682, 2015.

[38] N. Yaghmazadeh, C. Klinger, I. Dillig, and S. Chaudhuri. Synthesizing transformations on
hierarchically structured data. In Programming Language Design and Implementation (PLDI),
2016.

[39] N. Yaghmazadeh, Y. Wang, I. Dillig, and T. Dillig. Sqlizer: Query synthesis from natural
language. In Object-Oriented Programming, Systems, Languages, and Applications (OOPSLA),
2017.

[40] P. Yin and G. Neubig. A syntactic neural model for general-purpose code generation. In

Association for Computational Linguistics (ACL), pages 440–450, 2017.

[41] M. Zavershynskyi, A. Skidanov, and I. Polosukhin. NAPS: Natural program synthesis dataset.

In Workshop on Neural Abstract Machines & Program Induction (NAMPI), 2018.

10

[42] M. Zelle and R. J. Mooney. Learning to parse database queries using inductive logic program-
ming. In Association for the Advancement of Artiﬁcial Intelligence (AAAI), pages 1050–1055,
1996.

[43] L. S. Zettlemoyer and M. Collins. Online learning of relaxed CCG grammars for parsing
to logical form. In Empirical Methods in Natural Language Processing and Computational
Natural Language Learning (EMNLP/CoNLL), pages 678–687, 2007.

[44] V. Zhong, C. Xiong, and R. Socher. Seq2SQL: Generating structured queries from natural

language using reinforcement learning. arXiv preprint arXiv:1709.00103, 2017.

11

