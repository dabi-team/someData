1
2
0
2

y
a
M
3

]

G
L
.
s
c
[

2
v
3
4
2
3
0
.
1
1
0
2
:
v
i
X
r
a

A fast learning algorithm for One-Class Slab
Support Vector Machines

Bagesh Kumar, Ayush Sinha, Sourin Chakrabarti, and Prof. O.P.Vyas

IIIT Allahabad, India

Abstract. One Class Slab Support Vector Machines (OCSSVM) have
turned out to be better in terms of accuracy in certain classes of classiﬁ-
cation problems than the traditional SVMs and One Class SVMs or even
other One class classiﬁers. This paper proposes fast training method for
One Class Slab SVMs using an updated Sequential Minimal Optimiza-
tion (SMO) which divides the multi variable optimization problem to
smaller sub problems of size two that can then be solved analytically.
The results indicate that this training method scales better to large sets
of training data than other Quadratic Programming (QP) solvers.

Keywords: Support Vector Machine · One Class Slab Support Vector
Machine · Sequential Minimal Optimization.

1

Introduction

With the exponential growth of data in today’s world, the need to analyze,
classify and act upon the results in a constrained time is a great challenge.
Pattern recognition and classiﬁcation are arguably the most used operations on
real world data. The data present for training our models are not always suﬃ-
cient and accurate. Closed-set problems deal with classiﬁcation problems where
training data consists of samples spread over all classes which might appear for
classiﬁcation whereas open-set recognition problems deal with training data con-
taining samples from only a fraction of all the classes. Scheirer [6] discusses the
various open-set recognition techniques and compares them. Scheirer [12] also
introduced a novel ”1-vs-set machine” which carves out a decision space from
the output of traditional SVMs. Best ﬁtting hyperplane [7] approach is also one
of the widely known methods for open-set recognition an object detection. [28]
presents the recent developments in support vector machines. It also gives a
comparative review of all the variants of SVMs and the limitations are drawn
out.

Support Vector Machine (SVM) [10] is a supervised machine learning algo-
rithm that has been proven to be highly eﬀective and eﬃcient in solving classiﬁ-
cation and regression problems. The algorithm creates a hyperplane separating
the data given into classes. It can deal with cases where the data is not perfectly
separable by introduction of slack variables in the optimization problem [8].
Problems with high dimensional feature spaces can be tackled by using suitable
Kernels which obey Mercer’s theorem [11].

 
 
 
 
 
 
2

Bagesh Kumar, Ayush Sinha, Sourin Chakrabarti, and Prof. O.P.Vyas

However, the traditional SVM needs to label the classes so that they can be
classiﬁed during the testing and training phases. Hence, they are eﬀective only for
closed-set problems and does not perform very well for the open-set problems.
Scholkopf [9] has proposed a variation of the traditional SVM algorithm that
would work with samples from only one class. The variation is known as the
One Class SVM (OCSVM) which essentially works the same way as traditional
SVMs except that it ﬁnds the optimal hyperplane considering all the training
samples to be of one class and only the origin to be of the other class [13].
One Class SVMs have been successfully employed in various image processing
tasks [15] and anomaly detection problems [14].

Despite the research on the one-class classiﬁers, they still fail to perform
well for the open-set problems. To tackle this, another classiﬁer the One Class
Slab SVM (OCSSVM) [1] was proposed in 2016. It uses OCSVM as the base
because it scales well for both linear and non-linear problems. However, the
OCSVM keeps the target class on one side and doesn’t account for the instances
of the negative class to be present on that side of the hyperplane, which can
cause high false classiﬁcations. Unlike this, the OCSSVM encloses the target
class between two hyperplanes (which act as a slab). If the an instance lies
inside the slab, it is classiﬁes as a member of the target class otherwise not. The
OCSSVM showed better results than other SVMs. It performed comparable to
other classiﬁers, such as support vector data description (SVDD) [5] and Kernel
density estimation methods [16]. The proposed OCSSVM is seen to be applied
in various ﬁeld such as the anomaly identiﬁcation in gas-turbines [17], ﬁnding
disciplined medical states and false records in biometry [18].

Support Vector Machines are basically trained by solving a complex con-
vex optimization problem with certain constraints which boils down to large
quadratic programming problems [19]. There are several methods to solve QP
problems such as Newton based QP solvers [22], augmented Lagrangian meth-
ods [20] and primal-dual interior methods [21]. These methods do not scale well
to large datasets while training SVMs due to their high memory requirements
and numerous amount of computations. SMO [3] as proposed by John C. Platt
provide a faster method for training SVMs by Dividing the problem into smaller
sub problems of size two. SMO basically involved two basic steps: selecting the
variables to be optimized according to some heuristic and solving the two vari-
able optimization problem analytically. The SMO algorithm was modiﬁed for use
with One Class SVMs [2] and provided great results. In this paper, we modify
the SMO algorithm which will enable it to work with OCSSVM and improve
its training time. Parallel implementations of the SMO algorithm for training of
SVMs in order to provide higher scalability for large datasets have shown great
results [4].

There has been several advancements in ﬁeld of training of support vector
machines. [34] gives a modiﬁed SMO approach by computation of the working
set by constraining the bounds on results of successive optimization problem
computations by an eﬃcient loss function. An SMO algorithm which uses Con-
jugate gradient descent which results in much lesser number of iterations for

A fast learning algorithm for One-Class Slab Support Vector Machines

3

convergence [35]. [36] proposed faster convergence times by using SSGD with
SMO and achieved promising results. [31] compares the various SMO modiﬁ-
cations which revolve around parallel processing to speed up the process. Ma-
jor modiﬁcations have been done around kernel function, division of dataset,
hardware architecture etc. One of the implementations as suggested by [30] in-
volves hardware acceleration and parallelism of FPGA to fasten the processing
times. [4] proposed parallelizing the SMO algorithm by dividing the working set
with more processors. Designing SMO on VLSI is another way to quicken the
process. [29] gives an overview of the recent advances in the ﬁelds of training
of support vector machines. It compares the various techniques used to improve
SVM performance such as chunking, data reduction, decomposition etc. They
also compare the above optimizations on a number of use cases. [32] proposed a
GPU implementation of SMO for eﬃcient training in case of high dimensional
hyper-spectral images. [33] gives an implementation of PDCO for solving Sup-
port Vector Machines. The approach was compared against the various datasets
used for benchmarking SVM training algorithms. The results showed that PDCO
scaled better with dataset size on a number of occasions. [37] suggested using
LFU caching strategy to to improve SVM training times.

2 One-Class Slab Support Vector Machines

The OCSSVM ﬁnds two parallel hyperplanes (which acts as a slab) that sep-
arate the positive and negative classes. The optimization problem to ﬁnd the
hyperplanes primarily has the normal to the hyperplanes w and the distances
of the upper and lower hyperplanes from the origin ρ1 and ρ2 respectively. The
problem is given as:

min
w,ρ1,ρ2,ξ, ¯ξ

subject to

1
2

(cid:107)w(cid:107)2

2 +

1
ν1m

m
(cid:88)

i=1

ξi − ρ1 +

ε
ν2m

m
(cid:88)

i=1

¯ξi + ερ2

(cid:104)w, Φ(xi)(cid:105) ≥ ρ1 − ξi, ξi ≥ 0,
(cid:104)w, Φ(xi)(cid:105) ≤ ρ2 − ¯ξi, ¯ξi ≥ 0, ∀i = 1, ..., m.

(1)

(2)

(3)

where m is the size of the feature space, ξ and ¯ξ are the slack variables for the
upper and lower hyperplanes respectively which handle imperfectly separable
cases, ν1 and ν2 control the width of the slab by using the ratio of expected
anomalies in the data and ε determines the impact of the slack variables. Φ()
represents the mapping of the kernel function.

The decision function for each point x is given by :

f (x) = sgn{((cid:104)w, Φ(x)(cid:105) − ρ1)(ρ2 − (cid:104)w, Φ(x)(cid:105))}.

(4)

We now aim to ﬁnd the Lagrangian for the above convex optimization prob-
lem. The Lagrange multipliers αi, βi, ¯αi, ¯βi ≥ 0 are introduced and the equation

4

Bagesh Kumar, Ayush Sinha, Sourin Chakrabarti, and Prof. O.P.Vyas

is formulated as follows:

L(w, ξ, ¯ξ, α, ¯α, β, ¯β, ρ1, ρ2) =

1
2

(cid:107)w(cid:107)2

2 +

1
ν1m

m
(cid:88)

i=1

ξi − ρ1+

ε
ν2m

m
(cid:88)

i=1

¯ξi + ερ2 −

m
(cid:88)

i=1

αi((cid:104)w, Φ(xi)(cid:105) − ρ1 + ξi)−

m
(cid:88)

i=1

¯αi(ρ2 + ¯ξi − (cid:104)w, Φ(xi)(cid:105)) −

m
(cid:88)

i=1

βiξi −

m
(cid:88)

i=1

¯βi

¯ξi

(5)

Now we ﬁnd the derivative of the Lagrangian with respect to w, ξ, ¯ξ, ρ1 and ρ2,
which will give us:

dL(w, ξ, ¯ξ, α, ¯α, β, ¯β, ρ1, ρ2)
dw
dL(w, ξ, ¯ξ, α, ¯α, β, ¯β, ρ1, ρ2)
dξ
dL(w, ξ, ¯ξ, α, ¯α, β, ¯β, ρ1, ρ2)
d ¯ξ
dL(w, ξ, ¯ξ, α, ¯α, β, ¯β, ρ1, ρ2)
dρ1
dL(w, ξ, ¯ξ, α, ¯α, β, ¯β, ρ1, ρ2)
dρ2

= w −

m
(cid:88)

i=1

(αi − ¯αi)Φ(xi) = 0

=

=

1
ν1m
ε
ν2m

= −1 +

= −ε +

− αi − βi = 0

− ¯αi − ¯βi = 0

m
(cid:88)

i=1
m
(cid:88)

i=1

αi = 0

¯αi = 0

From the above equations, we could deduce the following results:

m
(cid:88)

w =

(αi − ¯αi)Φ(xi) = 0

i=1
1
ν1m
ε
ν2m

βi =

¯βi =

− αi = 0

− ¯αi = 0

m
(cid:88)

i=1
m
(cid:88)

i=1

αi = 1

¯αi = ε

(6)

(7)

(8)

(9)

(10)

(11)

(12)

(13)

(14)

(15)

A fast learning algorithm for One-Class Slab Support Vector Machines

5

Substituting equations (11), (12) and (13) into (5), we get the dual problem:

min
α, ¯α

1
2

(α − ¯α)T K T (α − ¯α)

subject to

0 ≤ αi ≤

0 ≤ ¯αi ≤

1
ν1m

ε
ν2m

,

,

m
(cid:88)

j=1
m
(cid:88)

j=1

αj = 1,

(16)

(17)

¯αj = ε, ∀i = 1, ..., m.

(18)

The decision function in terms of α and ¯α is:

f (x) = sgn{(

m
(cid:88)

(αi − ¯αi)k(xi, x) − ρ1)(ρ2 −

i=1

m
(cid:88)

i=1

(αi − ¯αi)k(xi, x))}

(19)

We can recover ρ1 and ρ2 in terms of the dual variables α and ¯α as:

ρ1 =

ρ2 =

1
n1

1
n2

(cid:88)

m
(cid:88)

i:0<αi< 1

ν1m

j=1

(αj − ¯αj)k(xi, xj)

(cid:88)

m
(cid:88)

(αj − ¯αj)k(xi, xj)

i:0< ¯αi< ε

ν2m

j=1

(20)

(21)

where n1 and n2 are the number of support vectors of the lower and upper
hyperplanes respectively.

3 SMO Algorithm for OCSSVM

3.1 Solving reduced optimization problem

SMO works by breaking a large QP problem into smaller QP Problems, thus
decreasing the time required to solve the problem. For SMO, the sub-problems
of size 2 can be solved analytically.
We have four variables(αa, αb, ¯αa, ¯αb) instead of two as in traditional SMO. Let
kij denote k(xi, xj). Hence, the QP problem (16) now boils down to:

L(αa, ¯αa, αb, ¯αb) =

(αa − ¯αa)2kaa+

1
2
(αb − ¯αb)2kbb + (αa − ¯αa)(αb − ¯αb)kab+

1
2

(cid:88)

((αi − ¯αi)

m
(cid:88)

(αj − ¯αj)kij) + L(cid:48)

(22)

i=a,b

j=1,j(cid:54)=a,b

6

Bagesh Kumar, Ayush Sinha, Sourin Chakrabarti, and Prof. O.P.Vyas

where L(cid:48) is constant with respect to the variables in the above equation. The
equality constraints (14) & (15), give us:
a + α∗
∗ + ¯αb

b = αa + αb
∗ = ¯αa + ¯αb

¯s∗ = ¯αa

s∗ = α∗

(23)

(24)

The superscript * used here is to specify the values computed using the old
parameter values. From (23) & (24), we have:
αa = s∗ − αb
¯αa = ¯s∗ − ¯αb

(25)

(26)

Therefore, (22) now becomes:

L(αb, ¯αb) =

(s∗ − ¯s∗ − αb + ¯αb)2kaa+

1
2
1
(αb − ¯αb)2kbb + (s∗ − ¯s∗ − αb + ¯αb)(αb − ¯αb)kab+
2

(αb − ¯αb)

m
(cid:88)

j=1,j(cid:54)=a,b

(αj − ¯αj)kbj+

(s∗ − ¯s∗ − αb + ¯αb)

m
(cid:88)

j=1,j(cid:54)=a,b

(αj − ¯αj)kaj + L(cid:48)

(27)

To ﬁnd optimal value for the above equation we need to compute α∗
The partial derivative of equation (27) with respect to αb and ¯αb gives us:

b and ¯αb

∗.

∂L(αb, ¯αb)
∂αb

= −1 ∗

∂L(αb, ¯αb)
∂ ¯αb

= (αb + ¯s∗ − s∗ − ¯αb)kaa+

(αb − ¯αb)kbb + (s∗ − ¯s∗ − 2αb + 2 ¯αb)kab+

(cid:88)

(αj − ¯αb)(kbj − kaj)

(28)

j=1,j(cid:54)=a,b

We know that if ∂g(x,y)
Hence, L(αb, ¯αb) = L(αb − ¯αb). Let γ = α − ¯α. Hence, equation (22) becomes:

, then g(x, y) can be expressed as g(x − y, 0).

∂x = − ∂g(x,y)

∂y

L(γa, γb) =

1
2

γ2
akaa +

1
2

γ2
b kbb + γaγbkab +

(cid:88)

(γi

m
(cid:88)

γjkij) + L(cid:48)

(29)

i=a,b

j=1,j(cid:54)=a,b

The dual problem (16) and the constraints (17) and (18) in terms of γ can be
written as:

min
γ

subject to

γT K T γ

1
2
−ε
ν2m
m
(cid:88)

j=1

γj = 1 − ε.

≤ γi ≤

1
ν1m

, ∀i = 1, ..., m,

(30)

(31)

(32)

A fast learning algorithm for One-Class Slab Support Vector Machines

7

Equation (29) can be rewritten in terms of only one variable, in the same way
as discussed above. Let t∗ = γ∗
b . Hence,

a + γ∗

L(γb) =

1
2

(t∗ − γb)2kaa +

γb

1
2

b kbb + (t∗ − γb)γbkab+
γ2
m
(cid:88)

γjkbj + (t∗ − γb)

m
(cid:88)

γjkaj + L(cid:48)

(33)

j=1,j(cid:54)=a,b

j=1,j(cid:54)=a,b

Partial derivative of L(γb) with respect to γb gives us:

∂L(γb)
∂γb

= (γb − t∗)kaa + γbkbb + (t∗ − 2γb)kab +

(cid:88)

j=1,j(cid:54)=a,b

γj(kbj − kaj)

(34)

Putting ∂L(γb)

∂γb

= 0, we get:

γb = γ∗

b + η

m
(cid:88)

γj(kaj − kbj)

j=1
1
kaa + kbb − 2kab

η =

(35)

(36)

Equation (35) gives us the update rule to update γb in terms of the old value
γ∗
b . γa can be updated using the old values of the variables as:

γa = t∗ − γb

(37)

To ensure the constraints (31), (32) and (37) for both γb and γa,we need to deﬁne
new upper and lower bounds for γb.

L = max(t∗ −

1
ν1m

,

H = min(

1
ν1m

, t∗ +

−ε
ν2m
ε
ν2m

)

)

(38)

(39)

3.2 Variable pair selection heuristic

SMO uses a heuristic to determine which pair to choose for optimisation. We use
KKT conditions [23] to check if the current solution is optimal. We determine
before each update, if the update to γi is needed by evaluating KKT conditions.

αi((cid:104)w, Φ(xi)(cid:105) − ρ1 + ξi) = 0
βiξi = 0
¯αi(ρ2 + ¯ξi − (cid:104)w, Φ(xi)(cid:105)) = 0
¯ξi = 0

¯βi

(40)

(41)

(42)

(43)

From analysis done in [1], we can see that a plane contains a sample if 0 <
αi < 1
ν2m for lower and upper hyperplanes respectively. Since

ν1m or 0 < ¯αi < (cid:15)

8

Bagesh Kumar, Ayush Sinha, Sourin Chakrabarti, and Prof. O.P.Vyas

a sample can only lie on either upper hyperplane or lower hyperplane, only one
of these conditions can be true at a time. Thus, αi > 0 and ¯αi > 0 always occur
exclusively.The overall analysis consists of 9 cases. As discussed above, in cases
where both αi, ¯αi > 0 indicates that ρ2 − ρ1 ≤ 0 which by the construction of
the primal problem should not happen because the slabs overlap and no such
slab appears in the feature space. Removing the extra invalid cases, we see that
the problem boils down to only 5 cases:

αi = 0 and ¯αi = 0 and f (x) > 0

0 < αi <

αi =

1
ν1m
1
ν1m

and ¯αi = 0 and f (x) = 0

and ¯αi = 0 and f (x) < 0

αi = 0 and 0 < ¯αi <

αi = 0 and ¯αi =

ε
ν2m
ε
ν2m

and f (x) = 0

and f (x) < 0

These equations can be written in terms of γi = (αi − ¯αi) as follows:

γi = 0 ∩ f (x) > 0

< γi < 0 ∩ f (x) = 0

−ε
ν2m

γi =

0 < γi <

γi =

−ε
ν2m
1
ν1m
1
ν1m

∩ f (x) < 0

∩ f (x) = 0

∩ f (x) < 0

(44)

(45)

(46)

(47)

(48)

(49)

(50)

(51)

(52)

(53)

We get the following Lagrange function, considering only the equality con-

straints from the dual problem in (30), (31) and (32):

¯L =

1
2

m
(cid:88)

i,j=1

γiγjk(xi, xj) + λ(1 − ε −

m
(cid:88)

i=1

γi)

The gradient of ¯L with respect to γb gives us:

∂ ¯L
∂γb

=

m
(cid:88)

i=1

γikib − λ

(54)

(55)

It is known that a function will show maximum variation on change on the
variable which has the maximum value of the gradient. According to paper [24],
the Lagrange multiplier takes the value of the oﬀset ρ in case of One class SVMs.
This function basically denotes the distance of the point from the hyper-plane

A fast learning algorithm for One-Class Slab Support Vector Machines

9

in case of OCSVMs. In our case, the analogue can be deﬁned as the minimum
of distances from both hyper-planes. Hence, we deﬁne the term ¯f (xb) as:

¯f (xb) = min(

m
(cid:88)

i=1

γikib − ρ1, ρ2 −

m
(cid:88)

i=1

γikib)

(56)

This term would aid us in ﬁnding the variable which optimizes the objective
function most. Hence, the γb which gives us max(| ¯f (xb)|). The second variable
is chosen as suggested by Scholkopf in [9], that is, select γa which gives us
max(| ¯f (xb) − ¯f (xa)|). After selecting the two variables, we use the update rule
deﬁned in (35) and (37) and constraints deﬁned in (38) and (39) to get the
new values of the variables. We then compute the new values of ρ1 and ρ2. We
continue this process until atmost one variable doesn’t satisfy the optimality
conditions deﬁned in (49), (50), (51), (52) and (53). The algorithm is given in
Algorithm 1.

Algorithm 1 Sequential Minimal Optimization for One Class Slab Support
Vector Machines
Input: Training data set Output: Hyper-plane parametres for classiﬁcation

1: while More than one variables don’t satisfy KKT conditions do
2:
3:
4:
5:
6:
7:
8:

Initialize γi ∀ i in 1, ..., m
Choose γb which maximizes | ¯f (xb)|
Choose γa which maximizes | ¯f (xb) − ¯f (xa)|
Compute new γb using Equation (35)
Use Equation (38) and (39) to set bounds for γb
Compute new γa using Equation (37)
Compute ρ1 and ρ2

4 Experimentation and Results

The above algorithm was tested against a toy dataset using a linear kernel
k(xi, xj) = xi · yi and constant values ν1 = 0.5, ν2 = 0.01 and ε = 2
3 . The
training times of the algorithm for various sizes of the training dataset was
recorded. To ﬁnd the quality of classiﬁcation, we recorded the Mathews Correla-
tion Coeﬃcient(MCC) [27]. The MCC scales well in cases of open set recognition
problem datasets. The simulation data in provided in Table 1. Fig. 1 and Fig. 2
shows the ﬁnal hyperplanes for 1000 and 2000 data samples respectively.

10

Bagesh Kumar, Ayush Sinha, Sourin Chakrabarti, and Prof. O.P.Vyas

Fig. 1. The ﬁnal plot for 1000 data samples of a toy dataset with the data points in
blue, the lower and upper hyper-planes in red and green respectively. The horizontal
and vertical axes are the x and y axis respectively. The value of constants used were
ν1 = 0.5, ν2 = 0.01 and ε = 2
3 .

Fig. 2. The ﬁnal plot for 2000 data samples of a toy dataset with the data points in
blue, the lower and upper hyper-planes in red and green respectively. The horizontal
and vertical axes are the x and y axis respectively. The value of constants used were
ν1 = 0.2, ν2 = 0.08 and ε = 1
2 .

A fast learning algorithm for One-Class Slab Support Vector Machines

11

Table 1. Training Times and MCC against number of samples

Size

Time(in s) 0.35 0.67

500 1000 2000 5000
5.91
MCC 0.07 0.13 0.26 0.33

2.1

5 Conclusion and Future Scope

This paper introduces an SMO algorithm for training the One Class Slab SVM
(OCSSVM). A brief architecture of the OCCSVM was given and the complete
derivation inspired by the SMO algrotihm for OCSVMs was proposed. First a
reduced optimization problem was derived mathematically followed by devising
a suitable working pair selection strategy. Finally, experiments were conducted
on the training dataset with variable values of the parameters. The algorithm
preformed quite well in terms of accuracy. But the achieved training time was
much lower than that of traditional QP solvers which work in complexities which
are weakly polynomial [25].

Further extensions in this ﬁeld might include attempts to solve the OCSSVM
using other faster methods of training SVMs such as PDCO [26] or applying
parallel SMO [4]. Improvements in SVM training times can be achieved by im-
plementing hardware level optimizations such as parallelism and caching and by
devising a better method for working set selection strategy. Optimizations in loss
functions or reducing number of iterations by combining SMO with algorithms
like SGD might prove to be of great signiﬁcance.

Further, SMO-like algorithms which break down a larger problem into smaller
ones should be thought of as the primary solution for any convex optimization
problem due to their great advantage of faster training and comparable accura-
cies over normal quadratic programming solving techniques. Eﬀorts can also be
directed towards integrating the algorithm with the popularly available libraries
for SVM training.

References

1. Fragoso, V., Scheirer, W., Hespanha, J. and Turk, M., 2016, December. One-class
slab support vector machine. In 2016 23rd International Conference on Pattern
Recognition (ICPR) (pp. 420-425). IEEE.

2. Jiong, J. and Hao-ran, Z., 2007, August. A fast learning algorithm for One-Class
Support vector machine. In Third International Conference on Natural Computation
(ICNC 2007) (Vol. 1, pp. 19-23). IEEE.

3. Platt, J., 1998. Sequential minimal optimization: A fast algorithm for training sup-

port vector machines.

4. Chang, P., Bi, Z. and Feng, Y., 2014, July. Parallel SMO algorithm implementation
based on OpenMP. In 2014 IEEE International Conference on System Science and
Engineering (ICSSE) (pp. 236-240). IEEE.

12

Bagesh Kumar, Ayush Sinha, Sourin Chakrabarti, and Prof. O.P.Vyas

5. Tax, D.M. and Duin, R.P., 2004. Support vector data description. Machine learning,

54(1), pp.45-66.

6. Scheirer, W.J., de Rezende Rocha, A., Sapkota, A. and Boult, T.E., 2012. Toward
open set recognition. IEEE transactions on pattern analysis and machine intelli-
gence, 35(7), pp.1757-1772.

7. Cevikalp, H., 2016. Best ﬁtting hyperplanes for classiﬁcation. IEEE transactions on

pattern analysis and machine intelligence, 39(6), pp.1076-1088.

8. Cortes, C. and Vapnik, V., 1995. Support-vector networks. Machine learning, 20(3),

pp.273-297.

9. Sch¨olkopf, B., Platt, J.C., Shawe-Taylor, J., Smola, A.J. and Williamson, R.C., 2001.
Estimating the support of a high-dimensional distribution. Neural computation,
13(7), pp.1443-1471.

10. Vapnik, VN, 1995. The Nature Of Statistical Learning. Theory.
11. Minh, H.Q., Niyogi, P. and Yao, Y., 2006, June. Mercer’s theorem, feature maps,
and smoothing. In International Conference on Computational Learning Theory
(pp. 154-168). Springer, Berlin, Heidelberg.

12. W. J. Scheirer, A. Rocha, A. Sapkota, and T. E. Boult. Towards Open Set Recog-
nition. IEEE Trans. on Pattern Analysis and Machine Intelligence, 36, July 2013.
13. Sch¨olkopf, B., Williamson, R.C., Smola, A.J., Shawe-Taylor, J. and Platt, J.C.,
2000. Support vector method for novelty detection. In Advances in neural informa-
tion processing systems (pp. 582-588).

14. Stolfo, S. and Wang, K., One class training for masquerade detection. Florida, 19
November 2003. In 3rd IEEE Conference Data Mining Workshop on Data Mining
for Computer Security.

15. Yunqiang, C.H., Xiang, Z.H., and Thomas, S., “One-class svm for learning in im-
age retrieval”, Proceedings of IEEE International Conference on Image Processing,
Thessaloniki, Greece, 2001.

16. Hoﬀmann, H., 2007. Kernel PCA for novelty detection. Pattern recognition, 40(3),

pp.863-874.

17. Clifton, D.A., Tarassenko, L., McGrogan, N., King, D., King, S. and Anuzis, P.,
2008, March. Bayesian extreme value statistics for novelty detection in gas-turbine
engines. In 2008 IEEE Aerospace Conference (pp. 1-11). IEEE.

18. H.-j. Lee and S. Cho. Retraining a Novelty Detector with Impostor Patterns for
Keystroke Dynamics-Based Authentication. In Advances in Biometrics, volume 3832
of Lecture Notes in Computer Science, pages 633–639. Springer Berlin Heidelberg,
2005.

19. Boyd, S. and Vandenberghe, L., 2004. Convex optimization. Cambridge university

press.

20. Delbos, F. and Gilbert, J.C., 2003. Global linear convergence of an augmented

Lagrangian algorithm for solving convex quadratic optimization problems.

21. Chambolle, A. and Pock, T., 2011. A ﬁrst-order primal-dual algorithm for convex
problems with applications to imaging. Journal of mathematical imaging and vision,
40(1), pp.120-145.

22. ˇSantin, O. and Havlena, V., 2011. Combined gradient and newton projection
quadratic programming solver for MPC. IFAC Proceedings Volumes, 44(1), pp.5567-
5572.

23. Kuhn, H.W. and Tucker, A.W., 1951. Nonlinear programming, in (J. Neyman,
ed.) Proceedings of the Second Berkeley Symposium on Mathematical Statistics
and Probability.

A fast learning algorithm for One-Class Slab Support Vector Machines

13

24. Martin,M., “On-line Support Vector Machines for Function Approximation”,
Technical Report LSI-02-11-R, Software Department, Universitat Politecnica de
Catalunya, 2002

25. Kozlov, M.K., Tarasov, S.P. and Khachiyan, L.G., 1979. Polynomial solvability of
convex quadratic programming. In Doklady Akademii Nauk (Vol. 248, No. 5, pp.
1049-1051). Russian Academy of Sciences.

26. Ma, D. and Saunders, M., SVM using SMO vs PDCO: Support Vector Machines
using Sequential Minimal Optimization vs Primal-Dual interior method for Convex
Objectives.

27. Powers, D.M., 2011. Evaluation: from precision, recall and F-measure to ROC,

informedness, markedness and correlation.

28. Kumar, B., Vyas, O.P. and Vyas, R., 2019. A comprehensive review on the variants

of support vector machines. Modern Physics Letters B, 33(25), p.1950303.

29. Cervantes, J., Garcia-Lamont, F., Rodr´ıguez-Mazahua, L. and Lopez, A., 2020. A
comprehensive survey on support vector machine classiﬁcation: applications, chal-
lenges and trends. Neurocomputing.

30. Noronha, D.H., Torquato, M.F. and Fernandes, M.A., 2019. A parallel implemen-
tation of sequential minimal optimization on FPGA. Microprocessors and Microsys-
tems, 69, pp.138-151.

31. Tavara, S., 2019. Parallel computing of support vector machines: a survey. ACM

Computing Surveys (CSUR), 51(6), pp.1-38.

32. Paoletti, M.E., Haut, J.M., Tao, X., Miguel, J.P. and Plaza, A., 2020. A new GPU
implementation of support vector machines for fast hyperspectral image classiﬁca-
tion. Remote Sensing, 12(8), p.1257.

33. Ma, D. and Saunders, M., 2018. SVM using SMO vs PDCO: Support Vector Ma-
chines using Sequential Minimal Optimization vs Primal-Dual interior method for
Convex Objectives.

34. Kocao˘glu, A., 2019. An Eﬃcient SMO Algorithm for Solving Non-smooth Problem
Arising in ε-Insensitive Support Vector Regression. Neural Processing Letters, 50(1),
pp.933-955.

35. Torres-Barr´an, A., Ala´ız, C. and Dorronsoro, J.R., 2020. Faster SVM Training via

Conjugate SMO. arXiv preprint arXiv:2003.08719.

36. Gu, B., Shan, Y., Quan, X. and Zheng, G., 2019. Accelerating Sequential Minimal
Optimization via Stochastic Subgradient Descent. IEEE Transactions on Cybernet-
ics.

37. Li, Q., Wen, Z. and He, B., 2019. Adaptive Kernel Value Caching for SVM Training.

IEEE transactions on neural networks and learning systems.

