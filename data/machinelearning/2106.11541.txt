Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.

Digital Object Identiﬁer 00.0000/ACCESS.0000.DOI

Kernel Clustering with Sigmoid
Regularization for Efﬁcient
Segmentation of Sequential Data

TUNG DOAN1 and ATSUHIRO TAKASU2,3 (Member, IEEE)
1Hanoi University of Science and Technology, No. 1, Dai Co Viet road, Hanoi, Vietnam
2The Graduate University for Advanced Studies, SOKENDAI, Kanagawa 240-0193 Japan
3National Institute of Informatics, Tokyo 101-8430, Japan

Corresponding author: Tung Doan (e-mail: tungdp@soict.hust.edu.vn).

2
2
0
2

n
u
J

2
2

]

G
L
.
s
c
[

2
v
1
4
5
1
1
.
6
0
1
2
:
v
i
X
r
a

ABSTRACT The segmentation of sequential data can be formulated as a clustering problem, where
the data samples are grouped into non-overlapping clusters with the constraint that all members of each
cluster are in a successive order. A popular algorithm for optimally solving this problem is dynamic
programming (DP), which has quadratic computation and memory requirements. Given that sequences
in practice are too long, this algorithm is not a practical approach. Although many heuristic algorithms
have been proposed to approximate the optimal segmentation, they have no guarantee on the quality of
their solutions. In this paper, we take a differentiable approach to alleviate the aforementioned issues. First,
we introduce a novel sigmoid-based regularization to smoothly approximate the constraints. Combining
it with objective of the balanced kernel clustering, we formulate a differentiable model termed Kernel
clustering with sigmoid-based regularization (KCSR), where the gradient-based algorithm can be exploited
to obtain the optimal segmentation. Second, we develop a stochastic variant of the proposed model. By
using the stochastic gradient descent algorithm, which has much lower time and space complexities,
for optimization, the second model can perform segmentation on overlong data sequences. Finally, for
simultaneously segmenting multiple data sequences, we slightly modify the sigmoid-based regularization
to further introduce an extended variant of the proposed model. Through extensive experiments on various
types of data sequences performances of our models are evaluated and compared with those of the existing
methods. The experimental results validate advantages of the proposed models. Our Matlab source code is
available on github.

INDEX TERMS Sequence segmentation, sequential data, differentiable approximation, stochastic opti-
mization, change point detection, temporal clustering

I. INTRODUCTION

Recently, there has been an increasing interest in developing
machine learning and data mining methods for sequential
data. This is due to the exponential growing in number of
collected data sequences from applications in wide range of
ﬁelds, including computer vision [1]–[3], speech processing
[4]–[6] , ﬁnance [7]–[9], bio-informatics [10], [11], clima-
tology [12]–[15] and trafﬁc monitoring [16]–[18]. The main
problem associated with analysis of these sequences is that
they consists of a huge number of data samples. Therefore,
it is desirable to summarize the whole sequences by a much
smaller number of the data representatives, alleviating burden
for the subsequent tasks.

Such compressed and concise summarization can be ob-
tained via sequence segmentation. More speciﬁcally, this
aims at partitioning the data sequences into several non-
overlapping and homogeneous segments of variable dura-
tions, in which some characteristics remain approximately
constant. It is widely recognized in the literature that the
segmentation of sequential data can be considered as a clus-
tering problem. The difference is that all data samples of each
cluster, which represents a segment, are constrained to be in a
successive order. Thus, in this paper, we focus on clustering-
based methods for segmentation of data sequences.

In practice, sequential data are often composed of non-
linear and complex segments. Therefore, kernel methods are

VOLUME XX, XXXX

1

 
 
 
 
 
 
often applied to map data samples into a new feature space
before segmenting. Due to the constraint imposed on the data
samples in each cluster, traditional algorithms for clustering
are inapplicable to the segmentation problem. [19] proposed
an optimal algorithm based on dynamic programming (DP)
for segmenting data sequence in the features space, which is
associated with a pre-speciﬁed kernel and mapping functions.
In general, DP has quadratic time and memory complexities.
It even induces running time of order O(n4) 1, where n is the
length of the sequence, in practice. Therefore, it is intractable
to perform segmentation on long data sequence using DP-
based algorithms. To alleviate this issue, many attempts have
been made to create approximations to the optimal algorithm.
Although a considerable amount of the computational costs
are reduced, there are still critical drawbacks remained in
the approximation algorithms. Taking pruned DP [20] and
greedy algorithm [21] as representatives. These methods se-
quentially partition the data sequence, returning one segment
boundary (a.k.a, change point) at each iteration. This strategy
offers a reduction in the computational time. However, its
expense is that errors might occur at the earlier steps and they
would inﬂuence on the subsequent iterations, inducing a huge
bias in the ﬁnal results. Massive memory complexity is also
a vital drawback of almost kernel-based methods. They need
store the kernel matrix, which requires order of O(n2) space.
Therefore, they are prohibited by themselves from handling
extensively long data sequences.

In this paper, we take a different approach to alleviate the
aforementioned issues. More precisely, we introduce a novel
sigmoid-based regularization, which smoothly approximates
the constraints of the segmentation problem. It is then inte-
grated with balanced kernel clustering to perform segmen-
tation on sequential data. Our method owns several prefer-
able characteristics. First, because objective of the proposed
model is differentiable w.r.t unconstrained and continuous
variables we can easily optimize it using gradient descent GD
algorithm. Different from the existing methods, which are
just heuristic approximations of the optimal segmentation al-
gorithm, our model has a guarantee on quality of the solutions
as convergence of the GD algorithm was theoretically proved
[22]. Second, the proposed model offers the applicability of
a more efﬁcient optimization algorithm based on stochastic
gradient – the gradient that is estimated from a subsequence
(mini-batch), which is randomly sampled from the original
data sequence at each iteration. Therefore, the stochastic
variant of our model has much lower time and space com-
plexities, making segmentation of extensively long data se-
quences possible. Finally, the proposed model is ﬂexible. We
can easily modify the sigmoid-based regularization to further
form a new extended variant that can simultaneously segment
multiple data sequences. Through extensive experiments on
various types of sequential data, our models are evaluated and
compared with baseline methods. The results validate advan-
tages of the proposed models. In summary, contributions of

1including time for computing the cost matrix in the feature space [20]

this paper are as follows

• Introduction of sigmoid-based regularization that en-
ables kernel clustering to partition sequential data. Ob-
jective of the proposed method called Kernel clustering
with sigmoid regularization (KCSR) is smooth and can
be effectively solved using gradient-based algorithm.
• Development of a stochastic variant of KCSR to reduce
the memory complexity, which is prominent in almost
kernel-based methods that prohibits them from handling
large-scale datasets.

• Extension of KCSR for simultaneously segmentation of

multiple data sequences.

• Extensively empirical evaluation of the proposed meth-
ods on widely public datasets shows theirs superiorities
over the existing methods.

The rest of this paper is organized as follows: In Section
II, we review related works that perform segmentation based
on clustering methods. Next, we brieﬂy presents some back-
ground for our proposed models in Section III, . Section IV
introduces the proposed model KCSR and its stochastic ver-
sion. This section also describes how to modify the sigmoid-
based regularization to form an extension of KCSR that
can simultaneously segment multiple data sequences. After
illustrating and discussing experimental results in Section V,
we conclude the paper in Section VI.

II. RELATED WORKS
In this paper, we focus on clustering-based methods for
nonlinear segmentation of sequential data. Thus, we will
review related works in the literature of kernel segmentation,
which sometime is referred to as ofﬂine kernel change point
detection (CPD) [23]. Here, the change points indicate the
boundaries between the segments. In addition, we also re-
view temporal clustering methods. They have recently gained
more and more popularity in the computer vision ﬁeld, where
clustering-based algorithms are employed to segment videos
of human motions.

Ofﬂine kernel change point detection. According to [23],
almost all ofﬂine kernel CPD methods attempt to optimize
the objective function as deﬁned in (2). This is also the
objective of the kernel k-means clustering. Based on the
search scheme for the segment boundaries, existing methods
can be divided into local group, which uses sliding window
and global group, which bases on dynamic programming.

The local methods [24]–[28] slide a window with a large
enough width over the data sequence. They then detect, in
the window, a single change point, at which the difference
between the preceding and succeeding samples is maximal.
Although having low computational cost, these methods is
sub-optimal as the whole sequence is not considered when
detecting the changes. Our approach is more similar to the
global methods, which take all data samples into account
for change detection. [19], [29] employed dynamic pro-
gramming (DP) algorithm to optimally obtain the segment
boundaries. However, because DP have time complexity of
order O(n4) (including computational time of the cost matrix

2

VOLUME XX, XXXX

[20] in the feature space), it is impractical for handling long
data sequences. To reduce the time complexity, [21] proposed
a greedy algorithm that sequentially detects change points
one at an iteration. [20] further reduce the space requirement
by introducing pruned DP, which combines low-rank approx-
imation of the kernel matrix and binary segmentation algo-
rithm. Our approach is different from these two methods as
it searches for all the segment boundaries simultaneously. In
addition, quality of its solutions is guaranteed as convergence
to optimum of the gradient descent algorithm employed in
our model is theoretically proved [22]. Both pruned DP and
the greedy algorithm are heuristic approximations of the
original DP. Since sequentially detect the changes, errors at
the early iterations are propagated and can not be corrected
at the subsequent iterations.

Temporal clustering refers to the factorization of data
sequences into a set of non-overlapping segments, each
of which belongs to one of k clusters. Maximum margin
temporal clustering (MMTC) [30] and Aligned clustering
analysis (ACA) [31] divide data sequences into a set of non-
overlapping short segments. These subsequences are then
partitioned into k classes using unsupervised support vector
machine [30] or kernel k-means clustering [31]. Recently,
a branch of methods based on subspace clustering has been
proposed. These methods often include two steps. First, given
a data sequences X = [x1, . . . , xn], they learn a new
representation (coding matrix) Z = [z1, . . . , zn] that char-
acterizes the underlying subspaces structures and sequential
(a.k.a. temporal) information of the original data. Second,
the normalized cut algorithm (Ncut) [32] is then utilized for
segmentation of Z.

To preserve the sequential information in the new repre-
sentation, [33], [34] proposed a linear regularization of the
form ||ZR||1,2, where R ∈ Rn×(n−1) is a lower triangular
matrix with −1 on the diagonal and 1 on the second diagonal.
By minimizing this regularization jointly with the subspace
learning objective, the new representation zj and zj+1 of
the two consecutive samples xj and xj+1, respectively, are
forced to be similar. [1] further integrated a weight matrix
into the linear regularization to avoid equally constraining
on every pair of consecutive samples. Nevertheless, since the
regularization is linear, it is ineffective for handling complex
data structure. To leverage this issue, [35], [36] proposed
manifold-based regularization that preserves the sequential
information for the local neighborhood data samples. This
type of regularization is more preferable [2] as it often out-
performs the linear one in most tests [37]. Our approach also
employs regularization to model sequential characteristics of
the data. However, the sequential information is both globally
and locally preserved in the proposed methods, thanks to
the smoothness of the sigmoid functions. In addition, since
the temporal regularization makes representation of consec-
utive samples become similar, boundaries of the segments
become difﬁcult to be identiﬁed. Our methods, in contrast,
approximate the boundaries by midpoints in the summation
of sigmoid functions with high steepness. Therefore, our

VOLUME XX, XXXX

FIGURE 1. An example of sequence segmentation: (top) an example
sequence of length 23 and (bottom) the corresponding indicator matrix with
number of segments k = 7.

models are expected to obtain better segmentation accuracy.
Both temporal clustering and ofﬂine kernel CPD ap-
proaches have to store an afﬁnity graph matrix and/or a kernel
matrix, which require memory of order O(n2). This is also
a vital reason that inhibits them from handling long data
sequence. Stochastic variant of our method has signiﬁcantly
lower space requirement. At each iteration, it approximates
the gradient based on a partial kernel matrix, which corre-
sponds to data samples in the current minibatch. Therefore,
memory complexity of Stochastic KCSR is only O(b2),
where b (cid:28) n is the minibatch size. Among the existing
methods, only pruned DP in [20] is capable of handling
large-scale data because it employs low-rank approximation
of the kernel matrix, which only requires space of order
O(r2), where r (cid:28) n is the rank of the approximation.
Comparison between performances of Stochastic KCSR and
this algorithm on large datasets will be given in Section V.

III. NOTATIONS AND BACKGROUND
A. NOTATIONS
Throughout this paper, we denote vectors and matrices by
bold lower-case and bold uppercase letters, respectively. For
a particular matrix A, its ith column is denoted as ai and its
element at position (j, i) is expressed by aj,i or Aj,i. The
transpose matrix of A is denoted by A(cid:62). If A is a square
matrix of size n then its trace is expressed as Tr(A) =
(cid:80)n
i=1 Ai,i. If A ∈ {0, 1}k×n then for any given element Aj,i
we have Aj,i = 0 or Aj,i = 1 (A is a binary matrix). By
a (cid:28) b, we mean that a is very small in comparison with b.

B. KERNEL SEGMENTATION
The goal of the segmentation task is to partition a data se-
quence into several non-overlapping and homogeneous seg-
ments of variable durations. Let X = [x1, ..., xn] ∈ Rd×n
denotes the given sequence of length n and dimension d. For
the number of segments k that is speciﬁed in advance, a valid
solution of the k−segmentation problem can be represented
by an sample-to-segment indicator matrix G ∈ {0, 1}k×n,
whose each element is as follows

Gi,j =

(cid:40)

1 xj ∈ segment i,
otherwise.
0

(1)

3

1234567891011121314151617181920212223051015200000001000000100000010000010000001000000100000010000010000001000000100000100000010000001000001000000100000010000001000001000000100000010000010000001000000100000012345678910111213141516171819202122237654321including i) Boundary:
G must satisfy two constraints,
G1,1 = 1 and Gk,n = 1 and ii) Monotonicity: for any
given Gi,j = 1 then for the next column Gi,j+1 = 1 or
Gi+1,j+1 = 1. An example of the indicator matrix is given
in Figure 1.

To discover segments with complex and nonlinear struc-
tures, kernelization is often applied. More speciﬁcally, the
data sequence X is mapped onto some high dimensional
space (a.k.a. feature space) associated with a pre-speciﬁed
kernel function κ(·, ·) : Rd ×Rd → R. The mapping function
φ(·) is implicitly deﬁned by φ(xi) = κ(xi, ·), resulting the
inner-product φ(xi)φ(xj) = κ(xi, xj). A common objec-
tive for segmentation is to minimize the total summation
of the intra-segment variances [23]. Thus, the optimization
problem is often formulated as follows

argmin
G ∈ G

k
(cid:88)

n
(cid:88)

j=1

i=1

Gj,i ||φ(xi) − µj||2
2,

(2)

where G is the set of all valid sample-to-segment indicator
matrices and µj is the mean of the jth segment in the feature
space. We can observe that the objective of this problem
is similar to that of the kernel k−means and it is difﬁcult
to be minimized because G is the discrete variables with
combinatorial constraints.

C. BALANCED KERNEL K−MEANS
As mentioned above, kernel segmentation is closely related
to kernel k−means due to the similarity between their ob-
jectives. In fact, this objective can be rewritten in matrix
form. More speciﬁcally, we can compute the corresponding
kernel matrix K ∈ Rn×n, where each element Ki,j =
φ(xi)φ(xj) = κ(xi, xj) represents how likely the two
samples are assigned to the same class. Let G ∈ {0, 1}k×n
denotes the associated (unknown) sample-to-class indicator
matrix of X, where Gi,j = 1 if xj is assigned to the ith class
and zero otherwise. Here, different from the segmentation
task, there is no constraint on the indicator matrix G. Then
the objective function of kernel k−means [38]–[40] can be
expressed as follows:

JKKM (G) = Tr (LK) ,

(3)

where L = In − G(cid:62) (cid:0)GG(cid:62)(cid:1)−1

G.

Kernel k-means is a strong approach for identifying clus-
ters that are non-linearly separable in the original space.
However, similar to its linear counterpart, kernel k-means is
sensitive to outliers. More speciﬁcally, it often outputs unbal-
anced results that consists of too big and/or too small clusters
under presents of anomaly data samples [41]. To alleviate this
issue, recently [42] has proposed a simple regularization on
the indicator matrix of the form Tr(G11(cid:62)G(cid:62)), where 1 is a
vector, whose all elements equal to one. By minimizing this
regularization jointly with the clustering objective, we can
prevent a too small or too great number of data samples from
being partitioned into a cluster. We now can combine (3) and

(a) Clustering task

(b) Segmentation task

FIGURE 2. Toy examples of (a) Clustering task and (b) Segmentation task,
where the given data and the corresponding indicator matrix are depicted.
Data samples from the same cluster or segment have identical symbol and
color. Segmentation is different from clustering in that data samples of the
same segment must be in a successive order.

the regularization to form a new objective of balanced kernel
k-means

JBKKM (G) = Tr (LK) + λ Tr(G11(cid:62)G(cid:62)),

(4)

where λ is a positive parameter that controls the balanced
regularization.

IV. THE PROPOSED METHOD
A. KERNEL CLUSTERING WITH SIGMOID-BASED
REGULARIZATION (KCSR)
Our intuitive idea is to reuse the robust objective of bal-
anced kernel k−means (4) for segmentation of data sequence
X = [x1, . . . , xn] ∈ Rd×n. However, the challenge is
that the sample-to-segment indicator matrix must satisfy
two constraints, including boundary and monotonicity, while
the indicator matrix for clustering does not. This difference
is illustrated Figure 2. To close this gap and enable the
clustering approach to segment data sequences, we intro-
duce a novel regularization that smoothly approximates the
two above constraints. The new regularization changes the
variables from a discrete to continuous domains. Therefore,
our problem can be solved using gradient descent (GD)
algorithm. Since, the convergence of GD was already proved
[22], quality of the proposed models’ solutions is guaranteed.
The proposed regularization is based on the sigmoid func-

tion. A basic sigmoid function is deﬁned as

fsigmoid(x) =

1
1 + e−α(x−β)

,

(5)

where β speciﬁes the midpoint and α controls the steepness
of the function curve at the midpoint. Figure 3 depicts a
sigmoid function, where the midpoint β is ﬁxed at 11.5 and
the parameter α varies from 0.1 to 10.

4

VOLUME XX, XXXX

1234567891011121314151617181920212223100000000000100010000000000100100000001000000001000001001000000000100000001000100000000001000000010001000001000000001000000010001000001000000000001010000010000001234567891011121314151617181920212223765432112345678910111213141516171819202122230000001000000100000010000010000001000000100000010000010000001000000100000100000010000001000001000000100000010000001000001000000100000010000010000001000000100000012345678910111213141516171819202122237654321FIGURE 3. Sigmoid function with different values of the parameter α.

We can observer that the higher α is the steeper function
curve at the midpoint becomes. In addition, the sigmoid
function is monotonic and almost piecewise constant. There-
fore, it allows us to roughly partition a sequence into two
segments, where the parameter β approximates the segment
boundary. If we denote τj ∈ [1, 2] (continuously valued) as
segment label of sample xj, then

τj ≈ 1 + fsigmoid(j, α, β).

(6)

For instance, if α = 10 and β = 11.5, then τj ≈ 1 for
j < 11.5 and 2 otherwise. To generalize for cases, where the
number of segments k > 2, we propose to use a summation
of k − 1 sigmoid functions with different parameters βi for
1 ≤ i ≤ k − 1.

τj ≈ 1 +

k−1
(cid:88)

i=1

fsigmoid(j, α, βi).

(7)

Figure 4 illustrates an example of a summation of sigmoid
functions deﬁned in (7). Here, the steepness parameter α is
shared among the sigmoid functions within the summation.
k − 1 midpoint parameters β = [β1, . . . , βk−1] approximate
the segment boundaries between the k segments. Note that
the midpoints must satisfy 1 ≤ β1 < . . . < βk−1 ≤ n to
guarantee the summation of sigmoid functions monotonically
increasing. Thus, we regularize the β by further introducing
k parameters γ1, . . . , γk such that
(cid:33)

i(cid:48)=1 eγi(cid:48)
i(cid:48)=1 eγi(cid:48)
(cid:80)i
is in the range [0, 1].
In equation (8), the ratio
(cid:80)k
Therefore, βi always satisﬁes 1 ≤ βi ≤ n. In addition,
the ratio becomes larger as i increases. This guarantees that
βi(cid:48) < βi for 1 ≤ i(cid:48) < i ≤ k − 1.

i(cid:48)=1 eγi(cid:48)
i(cid:48)=1 eγi(cid:48)

i(cid:48)=1 eγ
i(cid:48)
i(cid:48)=1 eγ
i(cid:48)

+ n ×

βi =

1 −

(cid:80)k

(cid:80)k

(cid:80)i

(cid:80)i

(8)

(cid:32)

.

It is notable that the summation of sigmoid functions in
Figure 4 smoothly approximates the indicator matrix G of
segmentation example in Figure 2(b). To make the observa-
tion more clear, we introduce the following approximation to
each element of G

Gi,j ≈ max (0, 1 − |τj − i|) .

(9)

This equation map the segment label τj from the range [1, k]
to the range [0, 1] for approximating the sample-to-segment
indicator matrix.

FIGURE 4. An example of the summation of sigmoid functions with a shared
parameter α = 10 and k − 1 different midpoint parameters β1, . . . , βk−1,
where k = 7.

Algorithm 1 : Gradient descent algorithm for KCSR
Input: Kernel matrix K, number of segments k, steepness

parameter α, tolerance (cid:15).

Output: Optimal parameters γ∗ = [γ∗

1 , . . . , γ∗

k](cid:62).

1: repeat
2:
3:

compute gradient ∇γ = ∂J
∂γ ;
compute stepsize η using Armijo-Goldstein line
search [22], [43];
update γ(t+1) = γ(t) − η∇γ(t);

4:
5: until |J(γ(t+1)) − J(γ(t))| ≤ (cid:15)

We now can formulate an optimization problem that
combines objective of the balanced kernel clustering with
sigmoid-based regularization for segmentation. Let K ∈
Rn×n be the kernel matrix of the data sequence X then our
kernel-based segmentation optimization problem is

argmin
γ1,...,γk

Tr (LK) + λ Tr(G11(cid:62)G(cid:62))

(10)

s.t. L = In − G(cid:62) (cid:0)GG(cid:62)(cid:1)−1

G,
Gi,j = max (0, 1 − |τj − i|)

∀i, j,

k−1
(cid:88)

i=1

τj = 1 +

(cid:32)

βi =

1 −

fsigmoid(j, α, βi) ∀j,

(cid:80)i

(cid:80)k

i(cid:48)=1 eγi(cid:48)
i(cid:48)=1 eγi(cid:48)

(cid:33)

+ n ×

(cid:80)i

(cid:80)k

i(cid:48)=1 eγi(cid:48)
i(cid:48)=1 eγi(cid:48)

∀i.

Since γ = [γ1, . . . , γk] are unconstrained and continuous pa-
rameters, we can optimize objective function in (10) using the
gradient descent algorithm. Let J(γ) denotes the objective
function in (10), then the gradient w.r.t parameters γ can be
computed using chain rule.

∇γ =

∂J(γ)
∂γ

=

∂J(γ)
∂G

×

∂G
∂τ

×

∂τ
∂β

×

∂β
∂γ

,

(11)

where τ = [τ1, . . . , τn]. More details on derivation of the
gradient w.r.t γ is given in Appendix A. We call the proposed
model Kernel clustering with sigmoid regularization (KCSR)
and its optimization algorithm is given in Algorithm 1.

VOLUME XX, XXXX

5

123456789101112131415161718192021222301 = 10.,  = 11.5 = 5.0,  = 11.5 = 1.0,  = 11.5 = 0.5,  = 11.5 = 0.1,  = 11.51234567891011121314151617181920212223sample index12345671 = 3.5, 2 = 7.5, 3 = 10.5, 4 = 13.5, 5 = 17.5, 6 = 20.5Method

Time
Space

SSC

TSC

ACA

AKS

GKS

KCSR

SKCSR

O(n2dt + n2) O(n2dt + n2)

O(n2)

O(n2)

0(n2nmaxt) O(r2n + r log(k)n) O(kn + n2) O(n2kt + n2) O(nbkt + b2)
O(n2nmax)

O((k + r)n)

O(n2)

O(n2)

O(b2)

TABLE 1. Time and space complexities of different segmentation methods. Here, n denotes length of the data sequence and k is the number of segments. nmax
denotes the maximum length of divided subsequences in ACA, d is dimension of the new representation Z in OSC and TSC. t denotes number of total iterations.
The rank of the approximation of the kernel matrix in AKS is denoted by r and b is the mini-batch size in SKCSR. Note that b (cid:28) n.

B. STOCHASTIC KCSR
Kernel segmentation allow us to capture nonlinear structure
in the data. However, this advantage is achieved at the ex-
pense of much higher complexities in both terms of computa-
tional time and storage requirement. More speciﬁcally, given
a sequence of n data samples, existing kernel-based methods
compute the kernel matrix K, whose both time and memory
complexities are of order O(n2). Note that this is also true for
temporal clustering methods, where the afﬁnity graph matrix
of size O(n2) is computed and stored while performing
Ncut algorithm. When n is large, these methods become
computationally difﬁcult. For example, average length of the
acceleration data for activity recognition in the experimental
section is about 125K. The corresponding kernel matrix K
requires up to approximately 116.4 GB for storage, which is
deﬁnitely out of memory for a regular computer.

Our method is also based on the kernel matrix. Especially,
at each iteration, our method computes the gradient using the
kernel matrix, which makes it very slow and even impossible
due to the large memory requirement for handling long data
sequences. Fortunately, since objective function of KCSR is
differentiable, we can reduce the complexities by using the
stochastic gradient descent (SGD) [44]–[46]. SGD estimates
the gradient from a randomly sampled subsequence2 (a mini-
batch), which consists of a much smaller number of samples,
from the original sequence. Let b (cid:28) n denotes length of the
randomly sampled subsequence X(t), where t expresses the
iteration index. Then, the stochastic gradient is estimated as
follows

∇γ =

∂J(γ)
∂G(t)

×

∂G(t)
∂τ

×

∂τ
∂β

×

∂β
∂γ

.

(12)

In equation (12), ∂J(γ)
is only associated with a partial ker-
∂G(t)
nel matrix K(t) ∈ Rb×b, which corresponds to the samples
in X(t). Therefore, it is much more efﬁcient in terms of
both running time and memory consumption than computing
the full-batch gradient as in equation (11). Details of the
algorithm is given in Algorithm 2 and complexity compar-
ison between the proposed methods and several baselines
are given in Table 1. We note that convergences of both
gradient with step size found by Armijo-Goldstein line search
[22], [43] and stochastic gradient descent algorithms with
vanishing step size are theoretically proven. In fact, it is well-
known [47], [48] that gradient descent (GD) after T iterations

2By sub-sequence, we mean that order and indexes of samples in the

original sequence are preserved in the randomly sampled mini-batch.

Algorithm 2 : Stochastic gradient descent algorithm for
KCSR
Input: Data sequence X, number of segments k, steepness
parameter α, number of iterations T , minibatch size b,
initial learning rate η0, momentum µ ∈ [0, 1), weight
decay ρ ∈ (0, 1].

Output: Optimal parameters γ∗ = [γ∗

1 , . . . , γ∗

k](cid:62).

3:
4:

1: for t = 1, . . . , T do
η = η0 × ρt;
2:
randomly sample a sub-sequence X(t) of length b;
compute the partial kernel matrix K(t);
compute the stochastic gradient ∇γ = ∂J
K(t) and original indexes of samples in X(t);

5:

∂γ based on

6: ∆γ(t) = η∇γ − µ∆γ(t−1);
γ(t) = γ(t−1) + ∆γ(t);
7:
8: end for

can ﬁnd a solution with error O(T −1) and stochastic gradient
descent (SGD) after T iterations can ﬁnd a solution with error
O(T −0.5). Thus, both KCSR and SKCSR can obtain good
solutions for problem deﬁned in (10) with enough loops.

C. MULTIPLE KCSR

In practice, at some particular circumstances, we need to
perform segmentation on multiple data sequences. If these
sequences are not in relation, the problem is effortless since
segmentation algorithms can be applied on each sequence
independently. However, when the sequences are related to
each other, performing multiple segmentation without con-
sidering relation among the sequences would induces infe-
rior results. We take sequential segmentation and matching
(SSM) problem as a study case. Given m ≥ 2 data sequences,
SSM aims at partitioning each sequence into several homo-
geneous segments and then establishing the correspondences
between these segments from different sequences. A popular
application of SSM is human action analysis. Speciﬁcally,
the human action videos are segmented into primitive actions
and the resulted sequences of the action segments are then
aligned [49]–[51].

To solve the SSM problem, in this work, we introduce
an extension of the proposed model termed Multiple ker-
nel clustering with sigmoid-based regularization (MKSSR).
MKSSR jointly partitions each data sequences into k seg-
ments such that the cth segments of all the m sequences

6

VOLUME XX, XXXX

(a)

(b)

Illustration of the cut-off summation of sigmoid functions. (a) A toy example of a concatenation of two sequences (m = 2, n1 = 23, n2 = 30) and its

FIGURE 5.
corresponding indicator matrix (k = 7). (b) The cut-off summation of sigmoid functions, whose two components are depicts in the two ﬁrst subﬁgures, can smoothly
approximate the indicator matrix in the toy example.

are matched 3. Let Xp ∈ Rd×np for 1 ≤ p ≤ m
denotes the pth data sequence and Gp ∈ Rk×np be its
corresponding sample-to-segment indicator matrix. MKSSR
ﬁrstly concatenates all the sequences to form a single long
sequence X = [x1, . . . , xm] ∈ Rd×n, where n = (cid:80)m
i=1 np.
Then G = [G1, . . . , Gm] ∈ Rk×n is the corresponding
indicator matrix of X. Similar to the original KCSR, each
element of G is deﬁned as in (9). However, in MKCSR, the
segment label τj is computed as following

The function (13), which we call as cut-off summation of
sigmoid functions, consists of two components. The ﬁrst
component is the summation of sigmoid functions. It plays a
similar role as (7) in KCSR. The second component presents
the cut-off points (a.k.a junction points), at which two among
the m original data sequences are connected. It will reset
the segment label from k to 1 after passing the ﬁnal sample
of one sequence and reaching a new sample from the next
sequence. The cut-off summation of sigmoid functions and
its components are illustrated in Figure 5.

τj = 1 +

m(k−1)
(cid:88)

i=1

fsigmoid(j, α, βi)

+ (1 − k)

m−1
(cid:88)

p=1

fsigmoid(j, α,

p
(cid:88)

q=1

nq + 0.5).

(13)

3Data samples of the cth segments from different sequences belong to the

cth class for 1 ≤ c ≤ k.

The formulation (13) has m(k−1) midpoint parameters, in
which β(p−1)(k−1)+1, . . . , βp(k−1) approximate the segment
boundaries within the range [1+(cid:80)p−1
q=1 nq] for 1 ≤
p ≤ m. Therefore, we introduce mk parameters γ1, . . . , γmk

q=1 nq, (cid:80)p

VOLUME XX, XXXX

7

12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152530000001000000100000010000010000001000000100000010000010000001000000100000100000010000001000001000000100000010000001000001000000100000010000010000001000000100000000000010000001000000100000010000010000001000000100000010000001000001000000100000010000001000001000000100000010000010000001000000100000100000010000001000000100000010000010000001000000100000010000001000000100000012345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152537654321123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525312345678910111213The first component1 = 3.5, 2 = 7.5, 3 = 10.5, 4 = 13.5, 5 = 17.5, 6 = 20.5, 7 = 27.5, 8 = 32.5, 9 = 36.5, 10 = 39.5, 11 = 42.5, 12 = 47.51234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253-6-5-4-3-2-10The second component1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253sample index1234567The segment label formula of MKCSRsuch that for (p − 1)(k − 1) + 1 ≤ j ≤ p(k − 1)

(cid:32)

βi =

1 +

(cid:33) (cid:32)

nq

1 −

p−1
(cid:88)

q=1

(cid:33)

(cid:80)i

(cid:80)pk

i(cid:48)=(p−1)k+1 eγi(cid:48)
i(cid:48)=(p−1)k+1 eγi(cid:48)
(cid:80)i
p
(cid:88)

+

nq

i(cid:48)=(p−1)k+1 eγi(cid:48)
i(cid:48)=(p−1)k+1 eγi(cid:48)

(cid:80)pk

.

q=1

(14)

By replacing the last two constraints in (10) with (13) and
(14) we can obtain the optimization problem of MKCSR.
The objective function is then minimized w.r.t mk parameters
γ1, . . . , γmk using the stochastic gradient descent algorithm.

V. EXPERIMENTS
A. BASELINES
We compare KCSR and its stochastic variant SKCSR with
the following baselines

• Aligned clustering analysis (ACA) [31] – a temporal
clustering method that combines k−means with Dy-
namic time alignment kernel [52].

• Sequential subspace clustering (SSC) [1] – a tempo-
ral clustering method that combines subspace cluster-
ing with linearly temporal regularization weighted by
(cid:96)1−norm sequential graph.

• Temporal subspace clustering (TSC) [2] – a temporal
clustering method that combines subspace clustering
with manifold-based temporal regularization and low-
rank constraint.

• Approximate kernel segmentation (AKS) [20] – a
heuristic approximation of the optimal kernel segmen-
tation, where the solution is obtained by pruned DP al-
gorithm that combines a low-rank approximation of the
kernel matrix and the binary segmentation algorithm.
• Greedy kernel segmentation (GKS) [21] – another
heuristic approximation of the optimal kernel segmen-
tation that detects the segment boundaries sequentially
using greedy algorithm.

B. DATASETS
To evaluate performances of the above methods, we use
a synthetic dataset and ﬁve real-world and widely public
datasets.

Synthetic data. We ﬁrst generate 2D data samples that
form four circles of different diameters. They are illustrated
in Figure 8(a). The number of data samples of each circle is
randomly selected in range [500, 1500] and also constrained
to be different. For instance, in our case, the numbers of
data samples of the circles from low to high diameters are
832, 1018, 1174 and 843, respectively. We then rearrange the
generated data samples in contiguous order, i.e. data samples
of one circle do not mix to the other circles. By doing so, each
circle in the original 2D space corresponds to a segment in
the new sequential data. See Figure 8(b) for illustration.

Weizmann data. The Weizmann dataset [53] consists of
90 videos of nine subjects, each performing ten actions:

(a) Weizmann data

(b) Ordered MNIST data

FIGURE 6. (a) Concatenated action videos of subject 1 in Weizmann dataset
and (b) the rearranged digit images sequence in MNIST dataset. Each data
sequence consists of 10 non-overlapping segments and only one
representative frame of each segment is depicted.

bend, run, jump-in-place (pjump), walk, jack, wave-one-
hand (wave1), side, jump-forward (jump), wave-two-hand
(wave2), and skip. Similar to [54], videos of the same sub-
jects are concatenated into a long video sequence following
the presented order of the actions. We then subtract back-
ground from each frame of these video sequences and rescale
them to the size 70×35. For each 70−by−35 rescaled frame,
we compute the binary feature as shown in Figure 6(a). To
reduce the dimensions of the feature space (2450), the top
123 principal components that preserve 99% of the total
energy are kept for experiments.

MMI Facial action units. We exploit the MMI Facial
Expression dataset [55], which contains more than 2900
videos of 75 different subjects, each performing a particular
combination of Action Unit (AU). In this paper, we focus
on videos of AU12, which corresponds to a smile. Although,
these videos consist of different number of frames, they
are composed of exactly ﬁve segments with the following
order: neutral, onset, apex, offset, neutral, where neutral is
when facial muscle is inactive, apex is when facial muscle
intensity is strongest, and onset is when facial muscle starts
to activate or offset is when facial muscle begins to relax.
Following the same pre-processing procedure as in [56], we
cropped and aligned the face using dlib-ml [57]. The results
are depicted in Figure 7. We then convert them to grayscale
and reduce their dimension to 400 using whitening PCA.
We ﬁnally selected videos of ﬁve subjects 2, 3, 6, 14 and
17 for experiments. Their ground-truth frames-to-segment
lables are already given in the original dataset.

Google spoken digits. Google’s Speech Commands
(GSC) [58], [59] is a large audio dataset that consists of more
than 30 categories of spoken terms. For each category that
relates to digits from “one” to “nine”, we randomly select
a clean recording. These recordings are then concatenated,
forming a long audio sequence with 19 segments (9 segments
of active voice and 10 silent segments) (see Fig. 10). We
further add white noise, which is also provided in the GSC
dataset, to make the segmentation problem more challeng-
ing. Finally, a sequence of acoustic features, which are 13-
dimensional mel–frequency cepstral coefﬁcients (MFCCs)
[60] for every 10ms of a 25ms window, is computed from the
noisy audio sequence. The annotation is manually obtained
based on the log ﬁlter-bank energies of the clean audio.

8

VOLUME XX, XXXX

FIGURE 7. Videos of ﬁve subjects (S002, S003, S006, S014 and S017) performing an action unit 12 that corresponds to smile taken from MMI Facial action units
dataset. The representative facial images of the segments are depicted. The bottom of each video shows duration of the corresponding ground truth
frame-to-segment labels along with the total number of frames.

Ordered MNIST data. the MNIST dataset [61] con-
sists of 28 × 28 grayscale digit [0, 9] images divided into
60K/10K for training/testing. Since all the compared meth-
ods are unsupervised and require no training phase, we use all
70K images to perform segmentation. Note that the original
data is not exact suited to the sequential assumption. Follow-
ing the same setting of [2], we rearrange order of the images
such that those of the same digit form a contiguous segment
and the ten segments are concatenated into a very long
images sequence (see Figure 6(b)). Different from [2], where
only 2K images were selected for experiment, our ordered
MNIST data consists of the whole 70K images. To handle
this large-scale data, temporal clustering and kernel CPD
methods requires up to 36.5 GB to store the kernel and/or
afﬁnity graph matrices, which is impractical for implementa-
tion on a single personal PC. Among the compared methods,
only SKCSR and AKS with low memory complexities can
perform segmentation on this dataset.

Acceleration data.4 The acceleration data [62] are ac-
quired from a triaxial accelerometer mounted on the chests
of 15 subjects, each performing a sequence of activities such
as working at computer, standing, walking, going updown
stairs and talking. The aims of our experiments is to partition
the data sequences into segments that correspond to the
activities. Thus, we ﬁrstly pre-process the data. For each
subject, we add squares of signals from the three axles.
An example is depicted in Figure 11. We then transform
the obtained summation signal using wavelet transform with
scale factor 64 and the Morlet wavelet as the mother wavelet
function. The resulting 2D wavelet coefﬁcient matrix C is of
the size 64-by-nacc, where nacc is the length of the original
acceleration signal. Note that the wavelet coefﬁcients C are
complex numbers. Thus we take its modulus as input for the
methods in our experiments. Similar to the Ordered MNIST
data, this dataset consist of long acceleration sequences. The

4The acceleration dataset is available in UCI repositories and can be
retrieved from https://archive.ics.uci.edu/ml/machine-learning-databases/
00287/ .

VOLUME XX, XXXX

9

apexoffsetneutralonsetneutralapexneutralneutralonsetapexoffsetoffsetNeutralOnsetApexOffsetNeutralneutral( 151 frames )( 93 frames )( 71 frames )S002 S014 S017 onsetneutralapexoffsetneutralonsetneutral( 103 frames )apexoffsetneutralonsetneutral( 73 frames )S003 S006 (a) Data in 2D

(b) Data in 3D

(c) SSC

(d) TSC

(e) ACA

(f) AKS

(g) GKS

(h) KCSR

(i) SKCSR

FIGURE 8. Synthetic experiment: (a) data generated in 2D space, (b) the data after contiguously rearranging and visualization of segmentation results returned by
all the compared methods. Different colors represent different clusters.

average nacc is 125K. Therefore, the methods with memory
complexities of order O(n2) will require up to approximately
116.4 GB, which is unaffordable in our case, for storage.
In our experiments, only SKCSR and AKS can handle this
dataset.

C. EVALUATION MEASURES
Given a speciﬁc value k, while KCSR, SKCSR, AKS and
GKS return exactly k non-overlapping segments, temporal
clustering-based methods partition samples of the data se-
quence into k clusters that maybe dispersed in discontiguous
segments. Since, all the compared methods base on clustering
scheme, we use accuracy and normalized mutual information
[63] as evaluation metrics to assess the segmentation results.
Let (cid:98)L = [ˆl1, . . . , ˆln] and L = [l1, . . . , ln] be the obtained
labels and ground-truth labels of a given data sequence X =
[x1, . . . , xn]. ˆlj = i (similar for lj) for 1 ≤ i ≤ k indicates
that xj belongs to cluster (segment) ˆci. The accuracy (ACC)

is deﬁned as follows:

ACC =

(cid:80)n

j=1 δ(lj, map(ˆlj))
n

,

(15)

where δ(a, b) is the delta function that equals one if a = b
and zero otherwise and map(ˆlj) is the permutation mapping
function that maps label ˆlj to the equivalent ground truth
label. In this work, we use Kuhn-Munkres algorithm [64] to
ﬁnd the mapping.

Let (cid:98)C = [ˆc1, . . . , ˆck] and C = [c1, . . . , ck] be the obtained
clusters and the ground-truth clusters. Their mutual informa-
tion (MI) is

M I(C, (cid:98)C) =

(cid:88)

p(ci, ˆci(cid:48)) log2

ci∈C,ˆci(cid:48) ∈ (cid:98)C

p(ci, ˆci(cid:48))
p(ci)p(ˆci(cid:48))

,

(16)

where p(ci) and p(ˆci(cid:48)) are the probabilities that a data sample
arbitrarily selected from the sequence belongs to the clusters
ci and ˆci(cid:48), respectively, and p(ci, ˆci(cid:48)) is the joint probability

10

VOLUME XX, XXXX

-10-50510x-axis-10-50510y-axis-10-5051010y-axis0temporal-axis-1040003000200010000x-axis-10-5051010y-axis0temporal-axis-1040003000200010000x-axis-10-5051010y-axis0temporal-axis-1040003000200010000x-axis-10-5051010y-axis0temporal-axis-1040003000200010000x-axis-10-5051010y-axis0temporal-axis-1040003000200010000x-axis-10-5051010y-axis0temporal-axis-1040003000200010000x-axis-10-5051010y-axis0temporal-axis-1040003000200010000x-axis-10-5051010y-axis0temporal-axis-1040003000200010000x-axisDataset

Synthetic data

Weizmann

MMI Facial AU

Google

Ordered MNIST

Acceleration

SSC
0.0965 (0.0736)
0.1070 (0.0649)
0.4856 (0.0269)
0.4157 (0.0387)
0.6453 (0.0238)
0.6514 (0.0412)
0.2057 (0.0275)
0.1839 (0.0256)
–
–
–
–

TSC
0.4077 (0.0795)
0.3608 (0.0758)
0.7028 (0.0430)
0.7207 (0.0501)
0.7552 (0.0392)
0.7321 (0.0437)
0.6434 (0.0313)
0.6513 (0.0330)
–
–
–
–

ACA
0.9305 (0.0224)
0.9214 (0.0375)
0.7687 (0.0146)
0.7628 (0.0196)
0.8121 (0.0157)
0.7952 (0.0205)
0.8241 (0.0182)
0.7939 (0.0196)
–
–
–
–

AKS
0.6577 (0.0671)
0.6067 (0.0751)
0.7182 (0.0188)
0.7006 (0.0205)
0.7527 (0.0204)
0.7600 (0.0214)
0.6726 (0.0257)
0.6954 (0.0266)
0.6983 (0.0282)
0.7196 (0.0209)
0.5535 (0.0434)
0.5763 (0.0398)

GKS
0.6999 (0.0255)
0.7036 (0.0325)
0.5628 (0.0218)
0.6032 (0.0262)
0.6025 (0.0249)
0.6355 (0.0298)
0.7458 (0.0294)
0.7557 (0.0305)
–
–
–
–

KCSR
0.9871 (0.0104)
0.9959 (0.0024)
0.8835 (0.0092)
0.9071 (0.0107)
0.9537 (0.0118)
0.9625 (0.0129)
0.7914 (0.0172)
0.8109 (0.0154)
–
–
–
–

SKCSR
0.9870 (0.0092)
0.9847 (0.0077)
0.8964 (0.0113)
0.9151 (0.0095)
0.9763 (0.0075)
0.9686 (0.0126)
0.8826 (0.0165)
0.9009 (0.0186)
0.9681 (0.0155)
0.9819 (0.0119)
0.8172 (0.0216)
0.8056 (0.0311)

ACC
NMI
ACC
NMI
ACC
NMI
ACC
NMI
ACC
NMI
ACC
NMI

TABLE 2. Segmentation results on six datasets, including synthetic data, Weizmann action sequences, MMI Facial smiling video, noisy Google spoken digits,
ordered MNIST data and Acceleration sequences, returned by different methods. The mean score of each methods over ﬁve random runs along with its variance
are reported. The symbol “–” means that there is no result due to the shortage of memory resources.

that the selected data sample belongs to both ci and ˆci(cid:48). This
metric is normalized to the range [0, 1] as follows:

N M I(C, (cid:98)C) =

M I(C, (cid:98)C)
max(H(C), H( (cid:98)C))

,

(17)

where H(C) and H( (cid:98)C) are the entropies of C and (cid:98)C, respec-
tively.

D. PARAMETER SETTINGS
We select the optimal parameters for each method to achieve
the best performance. The number of clusters k of all the
compared methods is set to the number of segments available
in the datasets. For ACA, its parameters nM a and nM i
that specify the maximum and minimum lengths of each
divided subsequence, respectively, are data-dependent. Let n
be the sequence length, we select nM a from a rounded set
{0.01n, 0.02n, 0.04n, 0.06n, 0.08n, 0.1n} and set nM i =
nM a
. For temporal subspace clustering methods, including
2
SSC and TSC, the most important parameter is that controls
the sequential regularization for the new representation Z.
We select this parameter from the set {1, 5, 10, 15, 20, 25}
and the other parameters are set according to the original
papers. For the proposed methods, we ﬁx the parameter that
controls the steepness of the summation of sigmoid functions
at the midpoints α = 10. The tolerance (cid:15) for convergence
veriﬁcation in KCSR is ﬁxed at 10−6. For all the datasets, we
use the Radial Basis Function (RBF) Kernel 5 with proper
width σ for AKS, GKS and the proposed methods. The mini-
batch size b of SKCSR and the rank r of the approximation
of the kernel matrix in AKS are kept equal. Their values
are selected from a set {64, 128, 256, 512, 1024, 2048}. Note
that, SKCSR terminates after processing T minibatches. We
set T such that T × b ≥ 50n (passing through the data
sequence at least 50 times).

E. RESULTS DISCUSSION
1) Evaluation of KCSR and SKCSR
Figure 8 visualizes the segmentation results on synthetic data
and the evaluation scores are given in the ﬁrst rows of Table

5RBF kernel: κ(xi, xj ) = e−

||xi−xj ||2
2
2σ2

.

VOLUME XX, XXXX

FIGURE 9. Visualization of segmentation results returned by the proposed
methods and baselines on Weizmann dataset. Different colors represent
different clusters.

2. We can observe that each segment of the generated data
sequence has a circular structure. Therefore, the nonlinear
regularization in sequential representation learning of SSC is
ineffective on the synthetic data. TSC performed signiﬁcantly
better. The manifold-based regularization allows it to be able
to capture the nonlinear structure in the data. Our methods
also perform segmentation based on regularization. However,
different from SSC and TSC, where the regularization is
just local6, the summation of sigmoid functions of KCSR
and SKCSR globally regularizes the whole data sequences
and the locality is ensured by its smooth nature. Therefore,
the proposed methods obtained the best performance on the
synthetic dataset.

On the real-world data, including Weizmann action videos,
MMI Facial smiling videos and Google spoken digits audio,
the proposed models also outperformed the baselines. Eval-
uation scores of the corresponding segmentation results are
shown in the second, third and fourth rows of Table 2. We
can observe that ACA also had good performances on these
datasets. Although ACA also performs segmentation based
on clustering as our methods do, it cannot guarantee to ﬁnd
exact k non-overlapping segments. Therefore, its evaluation
scores are slightly lower than those of the proposed models.
In comparison with heuristic approximations AKS and GKS,
our models also had better performances. Similar to AKS
and GKS, our models also search for segment boundaries.
They approximate the boundaries by midpoints β of the

6The regularization only preserves the local relationship on representation

of consecutive samples.

11

ground truthSSCTSCACAAKSGKSKCSRSKCSRFIGURE 10. From the top to the bottom: clean audio of spoken digits [1,9], the
audio contaminated by white noise, log ﬁlter-bank energies of the clean audio
used for manual annotation (blue lines depict ground truth segment
boundaries) and Mel-frequency cepstrum of the noisy audio (vertical lines
show the midpoints β of the summation of sigmoid functions returned by
SKCSR).

summation of sigmoid functions. However, different from
these heuristic approximations that search for the segment
boundaries sequentially, the proposed models simultaneously
obtain all the β via gradient-based algorithm. As conver-
gence of this optimization algorithm is theoretically proved,
optimality of the solutions is guaranteed. To qualitatively
assess the performances of the compared methods, we also
visualized the segmentation results on Weizmann video and
Google audio datasets in Figure 9 and Figure 10, respec-
tively. These visualization further validate the superior per-
formances of our methods over those of the baselines.

On these datasets, we also observe that evaluation scores
of SKCSR are greater than those of KCSR. Thus, we further
investigate convergence curves of these models. Figure 13
depicts those of SKCSR and KCSR on Weizmann action
videos and Google spoken digits audios, respectively. It is
clear that superior performances of SKCSR arise from the
exploitation of stochastic gradient descent (SGD) algorithm.
SGD allows SKCSR to update its parameter γ more fre-
quently due to fast estimation of the stochastic gradient. In
addition, SGD takes randomness of the data into account and
enjoys theoretical guarantee on convergence in an expecta-
tion sense [45]. Therefore, SKCSR is more robust to noise in
the data and able to achieve better solution than KCSR.

SKCSR also showed its superior efﬁciency over the orig-
inal KCSR and most the other baselines on the ordered

FIGURE 11. From the top to the bottom: Acceleration signal of the ﬁrst
subject, the corresponding ground-truth segment labels and the segmentation
results returned by AKS and SKCSR, respectively, on the Acceleration dataset.

that

the ordered
MNIST and Acceleration data. Recall
MNIST data consists of 70K samples. Acceleration data
contains even much more longer data sequences, where the
average length is 125K. This makes implementation of the
memory-demanding methods impossible on regular personal
PCs. Among the baselines, only AKS with memory complex-
ity of order O(r2), where r (cid:28) n is the rank of approxi-
mation of the kernel matrix, can handle the ordered MNIST
and Acceleration data. However, since AKS employs binary
segmentation to sequentially detect the segment boundaries,
its solutions are not optimally guaranteed. Visualization of
the segmentation results on the Acceleration data in Fig. 11
and the evaluation scores in the ﬁfth and sixth rows of Table
2 validate the advantages of SKCSR.

2) Evaluation of MKCSR
We next evaluate performance of MKCSR – an extension
of KCSR for handling multiple data sequences. We utilize
concatenated Weizmann action videos and MMI Facial AU
videos in this experiment. For the Weizmann data, the ﬁrst,
second and third subjects are selected and their correspond-
ing action videos are concatenated to form a long sequence

12

VOLUME XX, XXXX

2468101214Time (samples)104-101AmplitudeClean audio2468101214Time (samples)104-101AmplitudeNoisy audioClean log (mel) filterbank energies1100200300400500600700800898Time (frames)5101520Channel indexMel frequency cepstrum1100200300400500600700800898Time (frames)24681012Cepstrum index246810121416Time (samples)10434.2Amplitude103Acceleration signal246810121416Time (samples)104123456789Segment labelsGround-truth246810121416Time (samples)104123456789Segment labelsSegment result of AKS246810121416Time (samples)104123456789Segment labelsSegment result of SKCSRFIGURE 12. Visualization of segmentation results of SSC, TSC, ACA and MKCSR on three concatenated action video sequences from Weizmann dataset.

Dataset

Weizmann

MMI Facial AU

SSC
0.1496 (0.0167)
0.1439 (0.0144)
0.2315 (0.0204)
0.2432 (0.0341)

TSC
0.1967 (0.0121)
0.2159 (0.0136)
0.3890 (0.0295)
0.3952 (0.0335)

ACA
0.5743 (0.0127)
0.5655 (0.0106)
0.6757 (0.0134)
0.6656 (0.0198)

MKCSR
0.8509 (0.0139)
0.8732 (0.0150)
0.9351 (0.0103)
0.9221 (0.0095)

ACC
NMI
ACC
NMI

TABLE 3. Segmentation results on concatenated video sequences from
Weizmann and MMI Facial AU datasets returned by different methods. The
mean score of each methods over ﬁve random runs along with its variance are
reported.

(the second and third rows of Table 2), evaluation scores
of SSC, TSC and ACA on the multiple data sequences
are signiﬁcantly reduced. MKCSR, however, compared to
its original method KCSR, could preserve a great amount
segmentation accuracy. As we can see that MKCSR obtained
up to 0.8509 of ACC and 0.8732 of NMI on Weizmann data.
For MMI Facial AU data, MKCSR also achieved 0.9351 of
ACC and 0.9221 of NMI. These results validate that MKCSR
can inherit advanced properties from SKCSR to perform
efﬁciently and effectively on multiple data sequences.

VI. CONCLUSION
Approximation of segmentation for fast computational time
and low memory requirement is very important as nowadays
more and more large sequential datasets are available. Previ-
ous works for approximating optimal segmentation algorithm
are either ineffective or inefﬁcient because they still involve
in optimization over discrete variables. In this paper, we
proposed KCSR to alleviate the aforementioned issue. Our
model combines a novel regularization based on sigmoid
function with objective of balanced kernel k−means to ap-
proximate sequence segmentation. Its objective is differen-
tiable almost every where. Therefore, we can use gradient-
based algorithm to achieve the optimal segmentation. Note
that, our model update all the parameters of interest in an
uniﬁed manner. This is in contrast to existing approximation
methods that sequentially update the segment boundaries,
which has no guarantee on quality of the solutions. To further
reduce the time and memory complexities, we introduce
SKCSR – a stochastic variant of KCSR. SKCSR employs
stochastic gradient descent, where the gradient is estimated
from a randomly sampled subsequence, for updating param-
eters of the model. Thus, it can avoid storing large afﬁnity
and/or kernel matrix, which is a critical issue that inhibits
existing methods from segmenting long data sequence. Fi-
nally, we modify the sigmoid-based regularization to de-

(a) Weizmann

(b) Google

FIGURE 13. Convergence curves of SKCSR (with stochastic gradients
estimated from mini-batches b = 256) and KCSR (with gradients estimated
from full batch (the whole data sequence)) on (a) Weizmann and (b) Google
spoken digits datasets.

that consists of 30 segments, each of which belong to one
of the ten action categories. For the MMI Facial AU data,
videos of all the subjects are concatenated. The new video
sequences consists of 491 frames and 15 segments. We com-
pare MKCSR with temporal clustering methods, including
SSC, TSC and ACA. For all the compared methods, we set
the number of clusters k = 10 and k = 15 for the Weiz-
mann and MMI Facial AU data, respectively, and select the
other parameters following the same scheme as mentioned in
subsection V-D.

Fig. 12 visualizes the segmentation results on multiple
video sequences from Weizmann data and Table 3 shows
the evaluation scores on both Weizmann and MMI Facial
AU datasets. Simultaneous segmentation of multiple data
sequences is a challenging task. As we can observe that, in
comparison with segmentation results of a single sequence

VOLUME XX, XXXX

13

ground truthSSCTSCACAMKCSR05101520Running time (seconds)2468101214Objective function01020304050607080Running time (seconds)1020304050Objective functionfull-batch gradientmini-batch gradientvelop MKCSR – an extended variant of KCSR for simul-
taneous segmentation of multiple data sequences. Through
extensive experiments on various types of sequential data,
performances of all the proposed models are evaluated and
compared with those of existing methods. The experimental
results validates the claimed advantages of the proposed
models.
.

APPENDIX A DERIVATION OF THE GRADIENT
In this section, we provide derivation of the gradient w.r.t γ.
Recall that our objective function is

J(γ) = Tr

(cid:16)(cid:16)

In − G(cid:62) (cid:0)GG(cid:62)(cid:1)−1

G

(cid:17)

(cid:17)

K

+ λ Tr(G11(cid:62)G(cid:62)).

(18)

The gradient ∇γ = ∂J
We ﬁrst compute the gradient of J w.r.t G as follows:

∂γ can be computed using chain rule.

∂J
∂G

=2 (cid:0)GG(cid:62)(cid:1)−1

GKG(cid:62) (cid:0)GG(cid:62)(cid:1)−1
− 2 (cid:0)GG(cid:62)(cid:1)−1

G

GK + λG11(cid:62).

(19)

Since each entry in the jth column of G is a function of
continuously segment label τj we need to compute

∂Gi,j
∂τj

=

∂max (0, 1 − |τj − i|)
∂τj

=






−1
1
0

if i ≤ τj ≤ i + 1
if i − 1 ≤ τj < i
otherwise.

Then the the gradient of J w.r.t τ = [τ1, . . . , τn](cid:62) is

∂J
∂τj

=

k
(cid:88)

i=1

∂J
∂Gi,j

∂Gi,j
∂τj

.

(20)

(21)

The segment label τj is again computed via a mixture of k−1
sigmoid functions, each of whose parameter is βi. Thus, we
need to compute

(cid:16)

∂

1 + (cid:80)k−1
i(cid:48)=1

(cid:0)1 + e−α(j−βi(cid:48) )(cid:1)−1(cid:17)
∂βi

∂τj
∂βi

=

(cid:16)

= −α

1 + e−α(j−βi)(cid:17)−1 (cid:20)

1 −

(cid:16)

1 + e−α(j−βi)(cid:17)−1(cid:21)

.

(22)

Then the gradient of J w.r.t β = [β1, . . . , βk−1](cid:62) can be
derived as follows

∂J
∂βi

=

n
(cid:88)

j=1

∂J
∂τj

∂τj
∂βi

.

(23)

Finally, we arrive at the gradient of J w.r.t γ = [γ1, . . . , γk](cid:62)

∂J
∂γc

=

k−1
(cid:88)

i=1

∂J
∂βi

∂βi
∂γc

,

(24)

where

∂βi
∂γc

=






(cid:16)

(n−1) eγc
1 −
(cid:80)k
i(cid:48)=1 eγ
i(cid:48)
− (n−1) eγc (cid:80)i
((cid:80)k
i(cid:48)=1 eγ

i(cid:48)=1 eγ
i(cid:48)
i(cid:48)=1 eγ
i(cid:48)
i(cid:48)

(cid:80)i
(cid:80)k
i(cid:48)=1 eγ
i(cid:48) )2

(cid:17)

if c ≤ i,

if c > i.

(25)

REFERENCES

[1] Hu, Wenyu, Shenghao Li, Weidong Zheng, Yao Lu, and Gaohang Yu.
“Robust sequential subspace clustering via (cid:96)1-norm temporal graph.”
Neurocomputing 383 (2020): 380–395.

[2] Zheng, Jianwei, Ping Yang, Guojiang Shen, Shengyong Chen, and Wei
Zhang. “Enhanced low-rank constraint for temporal subspace clustering
and its acceleration scheme.” Pattern Recognition 111 (2021): 107678.
[3] Zhou, Tao, Huazhu Fu, Chen Gong, Ling Shao, Fatih Porikli, Haibin Ling,
and Jianbing Shen. “Consistency and Diversity induced Human Motion
Segmentation." IEEE Transactions on Pattern Analysis and Machine In-
telligence (2022).

[4] Harchaoui, Zaid, Félicien Vallet, Alexandre Lung-Yut-Fong, and Olivier
Cappé. “A regularized kernel-based approach to unsupervised audio seg-
mentation." In 2009 IEEE International Conference on Acoustics, Speech
and Signal Processing, pp. 1665–1668. IEEE, 2009.

[5] Seichepine, Nicolas, Slim Essid, Cédric Févotte, and Olivier Cappé.
“Piecewise constant nonnegative matrix factorization." In 2014 IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP), pp. 6721–6725. IEEE, 2014.

[6] Sakran, Alaa Ehab, Sherif Mahdy Abdou, Salah Eldeen Hamid, and
Mohsen Rashwan. “A review: Automatic speech segmentation." Interna-
tional Journal of Computer Science and Mobile Computing 6, no. 4 (2017):
308–315.

[7] Lavielle, Marc, and Gilles Teyssiere. “Adaptive detection of multiple
change-points in asset price volatility." In Long memory in economics,
pp. 129–156. Springer, Berlin, Heidelberg, 2007.

[8] Si, Yain-Whar, and Jiangling Yin.“OBST-based segmentation approach to
ﬁnancial time series." Engineering Applications of Artiﬁcial Intelligence
26, no. 10 (2013): 2581–2596.

[9] Hallac, David, Peter Nystrup, and Stephen Boyd. “Greedy Gaussian seg-
mentation of multivariate time series." Advances in Data Analysis and
Classiﬁcation 13, no. 3 (2019): 727–751.

[10] Vert, Jean-Philippe, and Kevin Bleakley.“Fast detection of multiple
change-points shared by many signals using group LARS." Advances in
neural information processing systems 23 (2010): 2343–2351.

[11] Maidstone, Robert, Toby Hocking, Guillem Rigaill, and Paul Fearnhead.
“On optimal multiple changepoint algorithms for large data." Statistics and
computing 27, no. 2 (2017): 519–533.

[12] Reeves, Jaxk, Jien Chen, Xiaolan L. Wang, Robert Lund, and Qi Qi Lu. “A
review and comparison of changepoint detection techniques for climate
data." Journal of applied meteorology and climatology 46, no. 6 (2007):
900–915.

[13] Verbesselt, Jan, Rob Hyndman, Glenn Newnham, and Darius Culvenor.
“Detecting trend and seasonal changes in satellite image time series."
Remote sensing of Environment 114, no. 1 (2010): 106–115.

[14] Jamali, Sadegh, Per Jönsson, Lars Eklundh, Jonas Ardö, and Jonathan
Seaquist. “Detecting changes in vegetation trends using time series seg-
mentation." Remote Sensing of Environment 156 (2015): 182–195.
[15] Heo, Taemin, and Lance Manuel. “Greedy copula segmentation of multi-
variate non-stationary time series for climate change adaptation." Progress
in Disaster Science (2022): 100221.

[16] Lévy-Leduc, Céline, and François Roueff. “Detection and localization
of change-points in high-dimensional network trafﬁc data." Annals of
Applied Statistics 3, no. 2 (2009): 637–662.

[17] Lung-Yut-Fong, Alexandre, Céline Lévy-Leduc, and Olivier Cappé. “Dis-
tributed detection/localization of change-points in high-dimensional net-
work trafﬁc data." Statistics and Computing 22, no. 2 (2012): 485–496.

[18] Song, Yongze, Peng Wu, Daniel Gilmore, and Qindong Li. “A spatial
heterogeneity-based segmentation model for analyzing road deterioration
network data in multi-scale infrastructure systems." IEEE Transactions on
Intelligent Transportation Systems 22, no. 11 (2020): 7073–7083.

[19] Harchaoui, Zaid, and Olivier Cappé. “Retrospective mutiple change-point
estimation with kernels.” In 2007 IEEE/SP 14th Workshop on Statistical
Signal Processing, pp. 768–772. IEEE, 2007.

14

VOLUME XX, XXXX

[44] Robbins, Herbert, and Sutton Monro. “A stochastic approximation

method.” The annals of mathematical statistics (1951): 400–407.

[45] Bottou, Léon. “Online learning and stochastic approximations." On-line

learning in neural networks 17, no. 9 (1998): 142.

[46] Spall, James C. Introduction to stochastic search and optimization: estima-

tion, simulation, and control. Vol. 65. John Wiley & Sons, 2005.

[47] Allen-Zhu, Zeyuan. “Katyusha: The ﬁrst direct acceleration of stochastic
gradient methods." The Journal of Machine Learning Research 18, no. 1
(2017): 8194–8244.

[48] Schmidt, Mark, Nicolas Le Roux, and Francis Bach. “Minimizing ﬁnite
sums with the stochastic average gradient." Mathematical Programming
162, no. 1 (2017): 83–112.

[49] Qiu, Jiayan, Xinchao Wang, Pascal Fua, and Dacheng Tao. “Match-
ing seqlets: An unsupervised approach for locality preserving sequence
matching.” IEEE transactions on pattern analysis and machine intelligence
(2019).

[50] Chang, Chien-Yi, De-An Huang, Yanan Sui, Li Fei-Fei, and Juan Carlos
Niebles. “D3tw: Discriminative differentiable dynamic time warping for
weakly supervised action alignment and segmentation.” In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 3546–3555. 2019.

[51] Li, Jun, and Sinisa Todorovic. “Set-Constrained Viterbi for Set-Supervised
Action Segmentation." In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 10820–10829. 2020.
[52] Shimodaira, Hiroshi, Ken-ichi Noma, Mitsuru Nakai, and Shigeki
Sagayama. “Dynamic time-alignment kernel in support vector machine.”
Advances in neural information processing systems 14 (2001): 921–928.

[53] Gorelick, Lena, Moshe Blank, Eli Shechtman, Michal Irani, and Ronen
Basri. “Actions as space-time shapes.” IEEE transactions on pattern anal-
ysis and machine intelligence 29, no. 12 (2007): 2247–2253.

[54] Hoai, Minh, and Fernando De la Torre. “Max-margin early event detec-
tors” International Journal of Computer Vision 107, no. 2 (2014): 191–
202.

[55] Pantic, Maja, Michel Valstar, Ron Rademaker, and Ludo Maat. "Web-
based database for facial expression analysis." In 2005 IEEE international
conference on multimedia and Expo, pp. 5–pp. IEEE, 2005.

[56] Tung, Doan Phong, and Atsuhiro Takasu. “Deep Multiview Learning From

Sequentially Unaligned Data." IEEE Access 8 (2020): 217928–217946.

[57] King, Davis E. "Dlib-ml: A machine learning toolkit." Journal of Machine

Learning Research 10, no. Jul (2009): 1755–1758.

[58] Warden, Pete. “Speech commands: A dataset for limited-vocabulary

speech recognition.” arXiv preprint arXiv:1804.03209 (2018).

[59] McMahan, Brian, and Delip Rao. “Listening to the world improves speech
command recognition.” In Proceedings of the AAAI Conference on Arti-
ﬁcial Intelligence, vol. 32, no. 1. 2018.

[60] Davis, Steven, and Paul Mermelstein. “Comparison of parametric rep-
resentations for monosyllabic word recognition in continuously spoken
sentences.” IEEE transactions on acoustics, speech, and signal processing
28, no. 4 (1980): 357–366.

[61] LeCun, Yann, Léon Bottou, Yoshua Bengio, and Patrick Haffner.
“Gradient-based learning applied to document recognition.” Proceedings
of the IEEE 86, no. 11 (1998): 2278–2324.

[62] Casale, Pierluigi, Oriol Pujol, and Petia Radeva. “Personalization and
user veriﬁcation in wearable systems using biometric walking patterns."
Personal and Ubiquitous Computing 16, no. 5 (2012): 563–580.

[63] Cai, Deng, Xiaofei He, and Jiawei Han. “Document clustering using
locality preserving indexing.” IEEE Transactions on Knowledge and Data
Engineering 17, no. 12 (2005): 1624–1637.

[64] L. Lovasz and M. Plummer. “Matching Theory.” North Holland, Budapest:

Akademiai Kiado, 1986.

[20] Celisse, Alain, Guillemette Marot, Morgane Pierre-Jean, and G. J. Rigaill.
“New efﬁcient algorithms for multiple change-point detection with repro-
ducing kernels.” Computational Statistics & Data Analysis 128 (2018):
200–220.

[21] Truong, Charles, Laurent Oudre, and Nicolas Vayatis. “Greedy kernel
change-point detection.” IEEE Transactions on Signal Processing 67, no.
24 (2019): 6204–6214.

[22] Nocedal, Jorge, and Stephen Wright. Numerical optimization. Springer

Science & Business Media, 2006.

[23] Truong, Charles, Laurent Oudre, and Nicolas Vayatis. “Selective review
of ofﬂine change point detection methods.” Signal Processing 167 (2020):
107299.

[24] Harchaoui, Zaid, Eric Moulines, and Francis R. Bach. “Kernel change-
point analysis.” In Advances in neural information processing systems, pp.
609–616. 2009.

[25] Harchaoui, Zaid, Félicien Vallet, Alexandre Lung-Yut-Fong, and Olivier
Cappé. “A regularized kernel-based approach to unsupervised audio seg-
mentation.” In 2009 IEEE International Conference on Acoustics, Speech
and Signal Processing, pp. 1665–1668. IEEE, 2009.

[26] Gretton, Arthur, Karsten M. Borgwardt, Malte J. Rasch, Bernhard
Schölkopf, and Alexander Smola. “A kernel two-sample test.” The Journal
of Machine Learning Research 13, no. 1 (2012): 723–773.

[27] Li, Shuang, Yao Xie, Hanjun Dai, and Le Song. “M-statistic for Kernel
change-point detection,” in Proc. Adv. Neural Inf. Process. Syst., Mon-
tréal, QC, Canada, 2015, pp. 3366—3374.

[28] Li, Shuang, Yao Xie, Hanjun Dai, and Le Song. “Scan B-statistic for kernel

change-point detection.” Sequential Analysis 38, no. 4 (2019): 503–544.

[29] Arlot, Sylvain, Alain Celisse, and Zaid Harchaoui. “A kernel multiple
change-point algorithm via model selection.” Journal of machine learning
research 20, no. 162 (2019).

[30] Hoai, Minh, and Fernando De la Torre. “Maximum margin temporal
clustering.” In Artiﬁcial Intelligence and Statistics, pp. 520–528. PMLR,
2012.

[31] Zhou, Feng, Fernando De la Torre, and Jessica K. Hodgins. “Hierarchical
Aligned Cluster Analysis for Temporal Clustering of Human Motion.”
IEEE Transactions on Pattern Analysis and Machine Intelligence 35, no. 3
(2013): 582–596.

[32] Shi, Jianbo, and Jitendra Malik. “Normalized cuts and image segmenta-
tion.” IEEE Transactions on pattern analysis and machine intelligence 22,
no. 8 (2000): 888-905.

[33] Tierney, Stephen, Junbin Gao, and Yi Guo. “Subspace clustering for
sequential data.” In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 1019–1026. 2014.

[34] Wu, Fei, Yongli Hu, Junbin Gao, Yanfeng Sun, and Baocai Yin. “Ordered
Subspace Clustering With Block-Diagonal Priors.” IEEE transactions on
cybernetics 46, no. 12 (2016): 3209–3219.

[35] Li, Sheng, Kang Li, and Yun Fu. “Temporal subspace clustering for
human motion segmentation.” In Proceedings of the IEEE International
Conference on Computer Vision, pp. 4453–4461. 2015.

[36] Liu, Haijun, Jian Cheng, and Feng Wang.“Sequential subspace clustering
via temporal smoothness for sequential data segmentation.” IEEE Trans-
actions on Image Processing 27, no. 2 (2017): 866–878.

[37] Clopton, Larissa, Effrosyni Mavroudi, Manolis Tsakiris, Haider Ali, and
René Vidal. “Temporal subspace clustering for unsupervised action seg-
mentation.” CSMR REU (2017): 1–7.

[38] Dhillon, Inderjit S., Yuqiang Guan, and Brian Kulis. “Kernel k-means:
spectral clustering and normalized cuts.” In Proceedings of the tenth
ACM SIGKDD international conference on Knowledge discovery and data
mining, pp. 551–556. 2004.

[39] De la Torre, Fernando. “A least-squares framework for component analy-
sis.” IEEE Transactions on Pattern Analysis and Machine Intelligence 34,
no. 6 (2012): 1041–1055.

[40] Zass, Ron, and Amnon Shashua. “A unifying approach to hard and proba-
bilistic clustering.” In Tenth IEEE International Conference on Computer
Vision (ICCV’05) Volume 1, vol. 1, pp. 294–301. IEEE, 2005.

[41] Zhong, Shi, and Joydeep Ghosh. “Model-based clustering with soft bal-
ancing.” In ICDM’03: Proceedings of the Third IEEE International Con-
ference on Data Mining, p. 459. 2003.

[42] Liu, Hanyang, Junwei Han, Feiping Nie, and Xuelong Li. “Balanced
clustering with least square regression.” In Proceedings of the AAAI
Conference on Artiﬁcial Intelligence, vol. 31, no. 1. 2017.

[43] Armijo, Larry. “Minimization of functions having Lipschitz continuous
ﬁrst partial derivatives.” Paciﬁc Journal of mathematics 16, no. 1 (1966):
1–3.

VOLUME XX, XXXX

15

TUNG DOAN received the B.S. degrees in Com-
puter Engineering from Hanoi University of Sci-
ence and Technology in 2014. In 2021, he com-
pleted the PhD course at National Institute of In-
formatics, Japan. He is now a staff lecturer at De-
partment of Computer Engineering, School of In-
formation and Communication Technology, Hanoi
University of Science and Technology His current
research interests include deep learning, multiview
learning, generative model and sequential data.

ATSUHIRO TAKASU received his B.E., M.E.,
and Dr.Eng. in 1984, 1986, and 1989, respectively,
from the University of Tokyo, Japan. He is a
professor at the National Institute of Informatics,
Japan. His research interests are data engineering
and data mining. He is a member of the ACM,
IEEE, IEICE, IPSJ, and JSAI.

16

VOLUME XX, XXXX

