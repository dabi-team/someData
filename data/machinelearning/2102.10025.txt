1
2
0
2

b
e
F
2
2

]

G
L
.
s
c
[

2
v
5
2
0
0
1
.
2
0
1
2
:
v
i
X
r
a

Learning to Stop with Surprisingly Few Samples

Daniel Russo
Columbia University
djr2174@gsb.columbia.edu

Assaf Zeevi
Columbia University
assaf@gsb.columbia.edu

Tianyi Zhang
Columbia University
tz2376@gsb.columbia.edu

February 23, 2021

Abstract

We consider a discounted inﬁnite horizon optimal stopping problem.

If the underlying
distribution is known a priori, the solution of this problem is obtained via dynamic programming
(DP) and is given by a well known threshold rule. When information on this distribution is
lacking, a natural (though naive) approach is “explore-then-exploit," whereby the unknown
distribution or its parameters are estimated over an initial exploration phase, and this estimate
is then used in the DP to determine actions over the residual exploitation phase. We show: (i)
with proper tuning, this approach leads to performance comparable to the full information DP
solution; and (ii) despite common wisdom on the sensitivity of such “plug in" approaches in
DP due to propagation of estimation errors, a surprisingly “short" (logarithmic in the horizon)
exploration horizon suﬃces to obtain said performance. In cases where the underlying distribution
is heavy-tailed, these observations are even more pronounced: a single sample exploration phase
suﬃces.

1

Introduction

The optimal stopping problem. The folklore of optimal stopping problems traces back to
the work of the British mathematician Arthur Cayley in the late 1900’s, with the ﬁrst rigorous
formulation appearing in Moser [1956]; for a review and historical account cf. Ferguson et al. [1989].
The common structure in most optimal stopping problems considers a sequence X1, . . . , Xn of
independent random variables that, in the simplest instance, are drawn from a common distribution
F . The values of these random variables are revealed sequentially and the player’s objective, roughly
speaking, is to stop the sequence at a point where it is perceived to have reached its maximum value,
and collect that as a reward. In this paper, we consider a discounted formulation of this problem (for
reasons that will be discussed shortly): ﬁxing a discount factor γ ∈ (0, 1), the player seeks to solve

V ∗ = sup
1≤τ ≤n

EF [γτ Xτ ],

(1)

where the supremum is taken over all stopping times τ with respect to the sequence of observations
that are bounded by the horizon length n. Further technical details are deferred to Section 2.

1

 
 
 
 
 
 
This optimal stopping problem can be solved by dynamic programming (DP). The optimal
policy is given by a sequence of ﬁxed thresholds. Speciﬁcally, τ ∗ = inf {1 ≤ i ≤ n : Xi ≤ S(n)
i }
where the thresholds S(n)
i =: An−1 are determined the backward recursions: A0 = 0; Aj =
γ · EF [max {X, Aj−1}] , j = 1, . . . , n; see, e.g., Bertsekas [1995] for further discussion and an
elaboration on the solution of the Bellman equation. Here X denotes a generic draw from the
distribution F and the expectation is index by F to make clear that the solution is directly determined
by this problem primitive.

The problem outlined above has an especially elegant solution in the inﬁnite horizon setting,
namely, in the asymptotic regime where n → ∞. The optimal stopping rule takes the form of a
stationary threshold policy:

where the threshold is the unique solution of Bellman’s ﬁxed point equation,

τ ∗ = inf {i ≥ 1 : Xi ≤ S∗},

S∗ = γ · EF [max {X, S∗}] .

(2)

(3)

It is due to this simpliﬁcation that the inﬁnite horizon formulation is better suited for highlighting
salient features of the learning problems described next.

The stopping problem under incomplete information. A voluminous literature studies the
optimal stopping problem and various variants thereof under complete information on the underlying
primitives, primarily, the distribution F . In contrast, and outside of work on so called secretary
problems where typically one only observes relative ranks of X1, . . . , Xn (cf. Ferguson et al. [1989]),
very few antecedents consider the impact of incomplete prior information on achievable performance.
A brief review of such work is deferred to the end of this section; the reader is also referred to the
recent paper by Goldenshluger and Zeevi [2021] and references therein. In particular, the latter
paper considers a ﬁnite horizon problem, and for nonparametric classes of distributions F , proposes a
rank-based policy which is proven to be asymptotically optimal (as the horizon grows large) relative
to a benchmark given by the complete information solution. Their proposed policy, which is derived
as a solution to an auxiliary ranking problem, possesses a relatively simple recursive structure but
must be solved numerically.

An open question ﬂagged in Goldenshluger and Zeevi [2021] is whether simpler families of policies
might yield competitive performance if the class of distributions F is suitably restricted. For example,
natural algorithms might mimic the threshold structure of the optimal policy in (3) by solving an
auxiliary DP that replaces the true distribution F with an estimate. A risk with such an approach
is that even relatively small estimation errors could result in policies that perform poorly. In fact, it
is widely recognized that solutions to Bellman’s equation can be quite sensitive to perturbations in
problem primitives when the discount factor is close to one; For example, see Nilim and El Ghaoui
[2005] who discuss the potential propagation of errors in dynamic programming recursions.

Main contributions. Under a reasonably broad parametric class of distributions, this paper gives
a granular analysis of the optimal stopping DP and the impact of mis-estimation of the problem
parameters on decision performance. This is used to establish the eﬃcacy of simple policy which, in
the spirit of model predictive control, uses some initial observations to estimate the distribution and
subsequently optimizes performance assuming this “plug-in" estimate were correct. We give a sharp
asymptotic analysis as the discount factor γ tends to 1, yielding several insights.

2

The ﬁrst insight pertains to the (minimal) number of samples needed to support learning in the
optimal stopping problem under incomplete information.1 An exploration phase that collects only on
the order of log(1/(1 − γ))2 observations from F suﬃces to learn enough about this distribution to
make the “plug in" approach near optimal. In contrast to the common wisdom described above that
solutions to the Bellman equation can be quite sensitive even to small mis-estimations, collecting an
amount of data that scales only logarithmically in the “eﬀective horizon” is suﬃcient to support near
optimal solutions in the case of optimal stopping. This threshold is sharp, in the sense that any
number of samples which is of lower order is catastrophic for the decision-maker, losing half of the
attainable value; see Theorem 1.

Surprisingly, the length of the exploration horizon required to support near-optimal performance
is lower in problems where the underlying distribution have tails that decrease more slowly. This is
especially pronounced when the tails are heavy: a single observation is revealing enough to make
the “plug-in” policy asymptotically optimal; see Theorem 2. This result is driven by an intrinsic
robustness of decision quality to inaccurate estimation as the time horizon grows.

The results above are supported by a detailed analysis of the scale of parameter mis-estimation
that can be tolerated without degrading decision performance. The critical scale is approximately
1/(log(1/(1 − γ))). In particular, any mis-estimation smaller than this scale induces a threshold
policy that is asymptotically optimal, and larger scale perturbations preclude that; see Theorem 3.
Moreover, the eﬀects of said perturbation are highly asymmetric: the performance of the resulting
threshold policy is far more sensitive to overestimation and it is reasonably robust to underestimation;
see Theorem 4. This asymmetry is one of the key attributes that support our sample complexity
ﬁndings.

Potential for broader relevance Our work focuses on a classic optimal stopping problem – a
problem which since its inception has served to illustrate features that pertain to broader sequential
decision making contexts. We use it to elucidate issues that arise speciﬁcally at the intersection of
sequential decision making and statistical inference: the sensitivity of DP solutions to mis-estimation
and the sample complexity required to ensure the eﬃcacy of plug-in type policies. By revealing rich
and unexpected behavior, the paper oﬀers a detailed “case-study" that motivates further investigation.
We are hopeful that this line of work will produce more general insights on the sample complexity of
learning in structured classes of dynamic optimization problems – like inventory control or queuing
control – and how this depends on key problem primitives.

Related literature. Our paper is connected to two major strands of literature. The ﬁrst revolves
around the optimal stopping problem, where there are only a few entries that study the incomplete
information setting. The second is more directly related to the learning theory literature, for example,
the eﬃcacy of explore-then-commit (ETC) policies, but more broadly, reinforcement learning and
sample complexity consideration in that space of problems. (For brevity, we omit a review of the
general principles of model predictive control and the like, which underlie the explore-then-commit
and plug-in policies.) These two strands are reviewed below in that order.

The initial motivation to consider optimal stopping with partial information originates with
the secretary problem. Stewart [1978] is among the ﬁrst studies in this direction, and considers a

1With slight abuse of terminology, we will refer to this in what follows as sample complexity. As will become
evident shortly, the way we deﬁne this property does not directly conform with the traditional ((cid:15), δ) PAC-learning
interpretation, but it does capture the essence of the latter in our setting.

3

uniform distribution F with unknown mean and variance. Petruccelli et al. [1980] considers an even
more restricted setting where only the mean is unknown. Samuels [1981] considered minimizing the
expected quantile of the observation selected based on Stewart’s work and constructs a minimax
policy. Petruccelli [1985] considers normal distribution with unknown mean and variance, and
Boshuizen and Hill [1992] proposes a moment-based policy to handle a sequence of independent
uniformly bounded random variables given that only the means and/or variances are known. All
these antecedents restrict F to have simple parametric form, and the proposed policies rely heavily
on this supposition. This greatly restricts the breadth of insights one can tease out. In particular,
neither one of these studies directly focus on the regret and sample complexity of the problem, or
their implications on implementation of simple yet universal learning rules like ETC.

The more recent paper by Goldenshluger and Zeevi [2021], discussed earlier, is an exception
insofar as it studies broad nonparametric classes of distributions, and characterizes minimax regret
over said classes. The policy that is designed in that paper and shown to be minimax optimal
(in order) relies on solving an auxiliary ranking problem. The latter is fairly intuitive yet leads
to a far more complex learning algorithm compared to ETC-type policies. Moreover, that paper
actually calls into question the general prospects and eﬃcacy of “plug in" approaches in the context
of optimal stopping with incomplete information. As indicated earlier, our investigation is by and
large triggered by this question and provides some initial (somewhat surprising) evidence to the
contrary.

Our work also relates to broader research on reinforcement learning. Despite a ﬂurry of recent
work in this area, very few papers develop understanding of speciﬁc structured decision problems, as
ours does. Nearly all results on the sample complexity of online or oﬄine reinforcement learning
reveal that sample complexity scales super-linearly in the eﬀective time horizon; see for example the
recent works of Azar et al. [2017], Jin et al. [2018], Agarwal et al. [2020] and references therein. That
is, learning to make near-optimal decisions requires more interactions than the time horizon allows.
Hence, successful RL algorithms learn across initial epochs of interaction how to optimize in future
epochs. By contrast, our formulation requires optimizing within a single epoch and requires learning
at a much faster timescale. Results on learning in average cost Markov decision processes like those
of Jaksch et al. [2010] appear to learn in a single episode, but require that the problem has small
diameter, essentially meaning that any state is reachable in a small number of periods. It is unclear
how such a notion could be adapted to optimal stopping: the state of the system (the most recent
observation Xn) is continuous and unbounded, and the entire goal of the decision-maker is to reach
states (i.e., very high values of Xn) that are just barely reachable within the given time horizon.

Two very recent papers suggest that it is possible to learn a near-optimal policy in general
ﬁnite-state ﬁnite-horizon MDPs with a number of interactions that scale only logarithmically in
the problem’s time horizon [Wang et al., 2020, Zhang et al., 2020]. It appears nontrivial to adapt
their results, which depend polynomially on the number of distinct states, to our problem where
the state space is continuous and unbounded. Nevertheless, when viewed in light of their work, our
results suggest that the sample complexity of learning in a broad class of structured MDPs might
also depend only logarithmically on the eﬀective horizon. It is worth mentioning that our work
provides very sharp asymptotic sample complexity results, including tight lower bounds, in a style
that is quite diﬀerent from Wang et al. [2020] and Zhang et al. [2020].

Organization of the paper. The paper is structured as follows. In Section 2 we provide the
model, formulate the problem and deﬁne the performance metric of relative regret. Section 3 describes

4

the “plug in" based explore-then-commit policy. Section 4 contains the main results concerning the
sample complexity requirements of the plug-in policy, including the result on suﬃciency of a single
observation. Section 5 provides the interpretation of the main results based on the probability of
stopping and a non-traditional loss function, as well as a verify brief outline of the proofs. Some
numerical results are presented in Section 6. Section 7 discusses the limitations and open questions.
Proofs of all statements are given in the Appendix.

2 Problem Formulation

The model. A player observes a sequence of independent and identically distributed random
variables, (X1, X2, . . .) drawn from an unknown distribution F . At each stage n = 1, 2, . . . they may
either: stop the game and collect a reward given by γnXn, where γ ∈ (0, 1) is a discount factor; or
continue playing in which case the observed value is lost without recourse and the next observation
in the sequence is revealed. A common motivation for this set up is a seller that wishes to maximize
the expected net present value from the sale of a single indivisible item. The seller interacts with
potential buyers across the sequence of rounds and the random variables deﬁned above are the oﬀers
s/he receives at each round. An admissible policy is given by a random stopping time τ adapted
to the ﬁltration (Fn)n∈N where Fn = σ(X1, · · · , Xn). The expected net present value earned by
stopping rule τ under distribution F can be written concisely as EF [γτ Xτ ]. Optimizing this over
admissible stopping times gives the value of the game, deﬁned earlier in (1). This will be viewed as
the full information benchmark. Note that the player cannot solve this optimization problem directly
as F , the key stochastic primitive, is not known a priori.

Performance metric and objectives. Motivated by the solution structure in the full information
setting given in (2) and (3), we seek to design a single stopping rule that oﬀers competitive performance
without knowledge of F . The shortfall in performance of an admissible policy induced by τ can be
measured through its relative regret, deﬁned by

R(F, γ, τ ) :=

EF

(cid:2)γτ ∗Xτ ∗(cid:3) − EF [γτ Xτ ]
EF [γτ ∗Xτ ∗]

,

(4)

where τ ∗ is the full information optimal stopping rule under F . Speciﬁcally, it takes the form of the
threshold policy (2) whose threshold value is the solution to the Bellman equation (3) under F .

As is often the case in such problems, it will be instructive to consider the asymptotic behavior
of the relative regret, embedding it within a sequence of problems with discount factor tending to
1. In eﬀect, this means we are looking at increasingly longer “eﬀective time horizons." Consider an
algorithm that speciﬁes a stopping rule τγ for a problem instance with discount factor γ ∈ (0, 1).
We say this algorithm has universally vanishing relative regret on the distribution class D if

R(F, γ, τγ) = 0 for every F ∈ D.

lim
γ→1

(5)

We usually omit the subscript on τγ, as it is clear from context. Vanishing regret indicates that the
price of only having partial knowledge that F ∈ D is negligible when the eﬀective horizon is long.

Admissible distributions and notation. For tractability and to better elucidate key structural
properties of the learning problem, we focus our analysis on a simple parametric family of distributions.

5

We ﬁx a scale parameter α ≥ 1/2 throughout the paper2. For some location parameter θ ∈ R, we let
Fθ be the distribution whose probability density function fθ takes the form

fθ(x) = C0 exp (−|x − θ|α)

(6)

where C0 is the normalization constant. We study whether particular algorithms have universally
vanishing regret on the class distributions Dα := {Fθ : θ ∈ R+}. Such algorithms must adapt to the
unknown location parameter θ. The value of α controls both the diﬃculty of estimating the location
parameter from data and the likelihood of observing values much larger than the mean.

Some general notation that will be used in the sequel. For functions f : (0, 1) → R and
g : (0, 1) → R, we say f (γ) = o(g(γ)) if f (γ)/g(γ) → 0 as γ → 1 and f (γ) = ω(g(γ)) if
f (γ)/g(γ) → ∞ as γ → 1. We write g(γ) ∼ f (γ) if g(γ)/f (γ) → 1. To denote expectations taken
over Fθ, we sometimes write Eθ[·] in place of the more cumbersome notation EFθ [·].The notation
Pθ[·] is deﬁned similarly. We write R(θ, γ, τ ) := R(Fθ, γ, τ ) to denote relative regret for distributions
in the location family Dα.

3 The Explore-then-Commit Plug-in Policy

We study an extremely simple procedure for online learning in optimal stopping, presented in
Algorithm 1. The algorithm proceeds in three phases; for completeness its details are also summarized
below in pseudo-code. In the ﬁrst, exploration phase, it simply observes the ﬁrst N values, and
uses these to learn about the unknown distribution. N is a tuning parameter of the policy that
will be expounded upon shortly. In the second phase, a maximum likelihood estimator ˆθ is used to
estimate the distribution’s location parameter θ. Finally, the algorithm "plugs in" said estimate into
the Bellman equation, and optimizes it as if the estimated parameter value were correct. Precisely,
it employs the optimal threshold policy described in (2) only using the plugged in estimate. The
resulting stopping time can be written formally as

τN = min{n ≥ N : Xn ≥ S∗(Fˆθ, γ)}.
We call this procedure the N sample “plug in" explore-then-commit policy, or just “plug in" policy
for short.

(7)

The approach described above is, as indicated, quite naive, and there are certainly more reﬁned
learning algorithms than this family of ETC solutions. In particular, rather than terminating learning
after the ﬁrst N stages, continuous improvement of the “plug in" and the induced Bellman equation
can be achieved via updating of said estimate also over the exploitation phase. Or one can allow the
exploration phase to be determined in a fully online manner as a function of collected observations
rather being ﬁxed in advance. However, as we will show next, the very simple ETC policy described
above is remarkably eﬀective as is, without these modiﬁcations. The performance guarantees that
spell this out add to the already attractive nature of the policy due to its apparent simplicity.
This follows a vast literature, usually referred to as model predictive control, that tackles dynamic
optimization under incomplete information by “separating" statistical inference and decision-making.
The challenge is then to show when such approaches lead to provably near optimal performance.
This is what we establish in this paper in the context of our optimal stopping problem.

2The requirement that α ≥ 1
that Theorem 2 holds for any α > 0.

2 is a technical condition due to the bounds in Lemma 1 in Appendix A. We conjecture

6

Algorithm 1 Explore-then-commit policy
Input: Distribution’s shape parameter α ≥ 1/2, discount factor γ < 1, sample size N ∈ N+.
1: Observe and skip the ﬁrst N samples X1, ... XN . Compute ˆθ = arg maxθ
i=1 log fθ(Xi).
2: Solve for a threshold S∗(Fˆθ, γ) satisfying the Bellman Equation (3) when the underlying distri-

(cid:80)n

.
bution is Fˆθ

3: For each period n > N , observe the oﬀer Xn and stop if it exceeds the threshold S∗(Fˆθ, γ).

4 Main Results: Sample Complexity

Light-tailed distributions. Our main results concern the ﬁrst order asymptotic behavior of the
relative regret (4) under the ETC N sample plug-in policy, deﬁned via the stopping time τN in (7).
The ﬁrst result demonstrates a sharp phase-transition with respect to the sample size. Speciﬁcally,
the plug-in policy has universally vanishing relative regret on the class of distributions Dα if its
sample size exceeds the critical threshold identiﬁed below. When the sample size falls below this
threshold, relative regret tends to 1/2, meaning that the plug-in policy loses precisely half of the
maximal value obtained in the full information problem. Vanishing regret also requires the fairly
obvious requirement that the initial sample size N is o(1/(1 − γ)), eﬀectively meaning that the
algorithm does not forego a constant fraction of the problem’s time horizon on estimating the
distribution. For exploration sample sizes that grow more slowly than the critical threshold, attaining
the sharp constant of 1/2 requires that N = ω(1) so that our asymptotic analysis applies. (Recall
the notation an = ω(bn) means limn→∞ an/bn = ∞.) Deﬁne the critical sample size scaling

Ncritical(γ) =



(cid:16)




(cid:17)1− 1
α

log 1
1−γ
log log 1
1−γ

2





.

(8)

Theorem 1. (Sample complexity light-tailed distributions) Suppose α > 1.

(i.) If N = o (Ncritical(γ)) and N = ω(1), then

lim
γ→1

R(θ, γ, τN ) =

1
2

for any θ ∈ R.

(ii.) If N = ω (Ncritical(γ)) and N = o

(cid:17)

(cid:16) 1
1−γ

, then

R(θ, γ, τN ) = 0

for any θ ∈ R.

lim
γ→1

Discussion. Note that we always have the upper bound Ncritical(γ) = O
independent of α. The term 1/(1 − γ) can be interpreted as the length of eﬀective time horizon.
Thus Theorem 1 demonstrates that the ETC policy relies on a plug-in that is nearly independent of
the problem’s horizon.

log 1
1−γ

, which is

(cid:18)(cid:16)

(cid:17)2(cid:19)

In contrast, when insuﬃcient samples are collected, the relative regret tends to precisely 1/2,
which is less than the maximal relative regret of 1 but is still completely independent of the

7

distribution’s location and scale parameters. We show in Section 5 that this is due to an intrinsic
asymmetry in the estimation eﬀects on the regret, where the plug-in policy is far more robust to
underestimation than overestimation.

Perhaps the most surprising component of this result is that sample complexity requirements are
milder when the distribution is heavier tailed. Speciﬁcally, notice that the distributions in (6) are
heavier tailed for smaller values of α but that the critical sample size (8) is decreasing in α. Although
estimation is more diﬃcult for smaller α, the performance of plug-in policies is less sensitive to
estimation errors in such problems and this latter eﬀect dominates. At the extreme, the critical
sample size in Theorem 1 seems to vanish as α tends to 1.

Heavy-tailed setting. The next result conﬁrms this observation on heavier-tailed settings, show-
ing remarkably that for distributions with sub-exponential tails the single sample plug-in policy
has vanishing regret. (Recall that τ1 denotes the plug-in policy when N = 1.) Of course, the
single-sample plug-in estimator is inaccurate, especially when the distribution has heavy tails. This
result is driven by the intrinsic robustness of decision quality to inaccurate estimation as the time
horizon grows.

Theorem 2 (Suﬃciency of a single observation). If α ∈ (cid:2) 1

2 , 1(cid:3), then
for any θ ∈ R.

lim
γ→1

R(θ, γ, τ1) = 0

We note that the results presented above focus on pointwise convergence for the plug-in policy
that hold under any ﬁxed location parameter. A careful reading of the proof techniques in Appendix
B shows that relative regret also converges uniformly on compact sets. That is, taking N = 1 if
α ∈ [1/2, 1] and N = ω(Ncritical(γ)) if α > 1, it holds that for any real numbers θ1 ≤ θ2,

lim
γ→1

sup
θ∈[θ1,θ2]

R(θ, γ, τ1) = 0.

In the next section we explore in more detail the underlying structural properties of the DP solution
and the Bellman equation and their implications on required estimation accuracy. This will be
presented in the form of an inﬁnitesimal perturbation and sensitivity analysis.

5 Theoretical Foundations: Sensitivity and Perturbation Analysis

of the Full Information Problem

The statistics governing the plug-in policy are quite simple, as we know that under general conditions
the maximum likelihood estimator is asymptotically normal. The diﬃculty in our analysis is to
understand how estimation errors that are on the order of 1/
N impact downstream decisions
made by the algorithm. To frame this question more precisely, for given discount factor let τ ∗(θ)
denote the optimal stopping rule were the distribution known to be Fθ. Our analysis centers
around understanding the relative regret R (θ, γ, τ ∗(θ + (cid:15))) incurred by employing the optimal policy
τ ∗(θ + (cid:15)) under a location parameter that is perturbed from the truth by some (cid:15). This (cid:15) can roughly
be thought of as the error in estimating the true parameter θ, but throughout the section we take a
non-stochastic view and study the sensitivity to arbitrary perturbations.

√

8

Deﬁne the critical perturbation magnitude to be

(cid:15)critical(γ) =

1
α(cid:112)Ncritical(γ)

=

log log 1
1−γ
(cid:17)1− 1
(cid:16)
α

log 1
1−γ

.

α

(9)

The next result shows that comparing against this critical scale determines whether a plug-in policy
will be robust to overestimation of the location parameter; the case of underestimation is treated
separately for reasons that will become obvious shortly.

Theorem 3 (Phase-transition for relative regret under overestimation). If (cid:15) : [0, 1] → R+ satisﬁes
lim inf γ→1 (cid:15)(γ)/(cid:15)critical(γ) > 1, then

R(θ, γ, τ ∗(θ + (cid:15)(γ))) = 1

for any θ ∈ R.

lim
γ→1

If (cid:15) : [0, 1] → R+ satisﬁes lim supγ→1 (cid:15)(γ)/(cid:15)critical(γ) < 1, then

R(θ, γ, τ ∗(θ + (cid:15)(γ))) = 0

for any θ ∈ R.

lim
γ→1

The next result shows that performance is quite robust to underestimation, revealing a funda-
mental asymmetry. For any ﬁxed scalar (cid:15) > 0, employing the plug-in policy with underestimated
location parameter θ − (cid:15) yields vanishing relative regret. Vanishing regret is even possible when (cid:15)
grows “slowly" with the discount factor.

Theorem 4 (Robustness to underestimation). If (cid:15) : [0, 1] → R+ satisﬁes

(cid:32)(cid:18)

(cid:15)(γ) = o

log

(cid:33)

(cid:19) 1
α

1
1 − γ

as γ → 1,

(10)

then

R(θ, γ, τ ∗(θ − (cid:15)(γ))) = 0

for any θ ∈ R.

lim
γ→1

Our main results from the previous section follow relatively easily from these two theorems.
For instance, consider the claim in Theorem 1 that limγ→1 R(θ, γ, τN ) = 1
when α > 1 and N
2
grows slower than the critical sample size. This can be understood by imagining that the estimated
location parameter used by the N sample plug-in policy follows a N (θ, σ2/N ) distribution. This is
approximately the case, due to classical asymptotic theory of maximum likelihood estimation. Now,
there is roughly a 1/2 chance of underestimation, to which performance is robust. There is roughly
N . By Theorem 3, this event
a 1/2 chance of overestimation by a magnitude on the order of σ
leads to relative regret of 1 when N < Ncritical(γ).

√

Interpretation based on the probability of stopping. The next proposition helps clarify
where this critical threshold (and hence the critical sample size) arises from. Notice that when the
decision-maker underestimates the location parameter, they are willing to accept and stop on slightly
lower values than is optimal. As a result, they can be expected to earn slightly less, but stop earlier.
The results above suggest performance is relatively robust to such underestimation even if the time
horizon is extremely long. When the location parameter is overestimated, the decision-maker tends

9

to reject and not stop for values they would have accepted under an optimal policy. This may lead
to a larger ultimate value E[Xτ ], but it could entail passing on an extremely sequence of observation
before stopping. This risk is what drives the sensitivity to overestimation in Theorem 3.

The next result interprets the critical perturbation magnitude (9) in terms of the expected
stopping time. It shows that when the decision-maker overestimates the location parameter by
more than this critical magnitude, they are unlikely to stop within the problem’s eﬀective time
horizon of 1/(1 − γ). The decision-maker’s mistaken optimism about future observations causes them
to reject nearly all of them. Overestimation by less than the critical magnitude does not impact
the decision-maker as severely and they still stop well within the eﬀective time horizon. While
Proposition 1 appears to study the mean of the stopping time, it is worth noting that it eﬀectively
characterizes the full distribution because τ ∗(θ + (cid:15)) has a geometric distribution.

Proposition 1 (Phase transition for the stopping time).
If (cid:15) : [0, 1] → R+ satisﬁes lim inf γ→1 (cid:15)(γ)/(cid:15)critical(γ) > 1, then for every θ ∈ R,

Eθ [τ ∗(θ + (cid:15)(γ))] = ω

(cid:18) 1

1 − γ

(cid:19)

.

If (cid:15) : [0, 1] → R+ satisﬁes lim supγ→1 (cid:15)(γ)/(cid:15)critical(γ) < 1, then for every θ ∈ R,

Eθ [τ ∗(θ + (cid:15)(γ))] = o

(cid:18) 1

1 − γ

(cid:19)

.

Greater robustness with heavier tails. Paralleling our discussion in Section 4, (cid:15)critical(γ)
increases with α, suggesting greater robustness for heavier tailed distributions. In fact, for heavy-
tailed distribution (α ≤ 1), we ﬁnd (cid:15)critical(γ) → ∞ as γ → 1, implying robustness to increasingly
large estimation errors. This increasing robustness is what drives Theorem 2.

A rough explanation of this phenomenon is as follows. Recall the form of the probability density
function, fθ(x − θ) = C0exp(−xα). Lemma 19 shows that the optimal threshold behaves roughly as
S∗(θ, γ) ≈ θ + log (1/[1 − γ])1/α. A rough idea of this eﬀect arises from plugging this approximation
on the right hand side into the PDF, giving fθ(S∗(θ, γ) − θ) ≈ C0(1 − γ). This suggests that the
optimal threshold can be found in the tail of distribution where with density is roughly proportional
to (1 − γ). When the location parameter undergoes a positive perturbation by a some small (cid:15) > 0,
the threshold employed by the plug-in policy also increases by roughly (cid:15). Such a perturbation has
a much more signiﬁcant impact on light tailed distributions. Here the density fθ(x) decays very
rapidly around x ≈ S∗(θ, γ), meaning that it may be extremely unlikely to observe value that are
(cid:15) larger than the optimal threshold. Heavier tailed distributions are ﬂatter around their optimal
threshold so the same magnitude of overestimation may not have as dramatic an eﬀect. Flatter
tails make performance less sensitive to miss-estimation of the stopping threshold. The case where
α < 1 is precisely when the PDF is strictly log-convex, so that the logarithm of the PDF becomes
increasingly ﬂat near the optimal threshold as the horizon grows.

Loss function interpretation. Studying the regret of the plug-in policy reduces, eﬀectively, to
studying the estimation error of the location parameter in a custom loss function that captures
the downstream impact of estimations errors on actions. To describe this more carefully, write
(cid:96)γ(θ, ˆθ) = R
. A loss function like this has rich properties that are not seen in common

(cid:17)
θ, γ, τ ∗(ˆθ)

(cid:16)

10

instances like the squared or log loss. First, it is natural to consider γ not as a ﬁxed parameter, but
as one that grows with the sample size, capturing how data requirements scale with the problem’s
eﬀective time horizon. In the limit as γ → 1, (cid:96)γ(θ, ˆθ) becomes incredibly asymmetric in its second
argument: one can show that for small (cid:15) > 0, (cid:96)γ(θ, θ − (cid:15)) = O((cid:15)) as γ → 1 while (cid:96)γ(θ, θ + (cid:15)) → 1 as
γ → 1. This characteristic, where a negligible but constant overestimation leads to a catastrophic
loss for the decision-maker already suggests that the sample size must grow with the problem’s time
horizon. Of course, estimating the location parameter is more diﬃcult when the distribution is heavy
tailed (α < 1), but the loss function becomes less sensitive in its second argument in that case. This
phenomenon highlights the interaction between decision-making (stopping) and statistical inference;
namely, the loss function is derived endogenously and is not hypothesized upfront.

Outline of the proofs. We provide an overview of analysis and proofs of Theorem 3 and Theorem
4. At a high level, the analysis of these optimal stopping problems as the discount factor tends
to 1 revolves around analyzing tail behavior of the class of distributions. The challenging factor
is that the optimal threshold policy S∗(θ, γ) is only deﬁned implicitly as a solution to Bellman’s
ﬁxed point equation underlying ((1)). To develop, for example, sharp asymptotics regarding the
probability of stopping under a perturbed plug-in policy (see Prop 1), requires studying the behavior
of 1 − Fθ(S∗(θ + (cid:15)(γ), γ)) as γ → 1. This is the the inverse CDF evaluated at the plug-in stopping
threshold S∗(θ + (cid:15)(γ), γ)) under the perturbation (cid:15)(γ). This in itself requires sharp asymptotics
for the threshold S∗(θ + (cid:15)(γ), γ)) under growing discount factor and growing perturbations. These
asymptotic considerations are worked out rigorously in the Appendix.

6 Numerical Illustration

In this section, we present a simple numerical illustration. Our presentation focuses on three common
exponential family distributions that closely mimic the tail behavior of (6) with diﬀerent choices
of α: the Weibull distribution (α = 1/2); the exponential distribution (α = 1); and the Normal
distribution (α = 2). Figure 1, compares the expected discounted reward generated by the plug-in
policies against an optimal policy that knows the distribution’s mean. Under the heavier-tailed
Weibull distribution, the plug-in policy that estimates the location parameter based on N = 1
samples provides essentially perfect performance. Mirroring our theory, more samples are required
to have comparatively strong performance under the exponential or normal distributions. For the
normal experiment, the line labeled “nearly-horizon independent number of samples” refers to the

sample size N =
size in (8).

(cid:16)

log 1
1−γ

(cid:17)2

that serves as an upper bound on the scaling of the critical samples

Figure 2 studies the performance of a threshold policy τ ∗(θ + (cid:15)) computed with respect to a
location parameter that deviates from the truth of θ by some given perturbation level. Notice
that diﬀerent magnitudes of mis-estimation are reasonably likely to manifest under the Weibull,
exponential, and normal distributions, respectively. To allow for meaningful comparisons, we set
(cid:15) = zσ where σ is the standard deviation of the distribution and plot results as the multiplier z varies.
The standard error of the sample mean is proportional to σ in each case. For the heavier-tailed
Weibull distribution, performance is quite insensitive to perturbation of the location parameter, even
as the horizon grows. For the normal, performance becomes increasingly sensitive to overestimation
as the eﬀective time horizon grows. Asymmetry in the curves also becomes more pronounced as the

11

Figure 1: Performance of explore-then-plug-in policies across problems with varying discount factor
γ. The horizontal axis displays the eﬀective horizon (1 − γ)−1.

Figure 2: Performance of the threshold policy τ ∗(θ + zσ) computed with a location parameter θ
that deviates from the truth a multiple z times the standard deviation σ of the oﬀer distribution.

time horizon grows; slight overestimation results in signiﬁcant performance loss but underestimation,
in contrast, has only a mild impact. This plot seems to suggest that eﬀective optimal stopping
policies must carefully guard against mis-estimation. Given this intuition, it may be surprising
that the naive plug-in policy produces competitive performance while collecting a nearly horizon
independent number of samples.

7 Discussion and Open Problems

Throughout the paper, our goal was to state theoretical results that give a crisp illustrations of
these ﬁndings, and by that optimizing the presentation to facilitate key takeaways rather than
striving for generality. One limitation of this approach is the rather speciﬁc choice of the parametric
family of densities in (6) governing the optimal stopping problem. While knowledge of the scale
parameter was assumed ex ante, preliminary numerical experiments suggest that the plug-in policy
performs well with very little data even when the scale parameter is unknown a priori. Conﬁrming
this rigorously is an open question. Our results may not extend as gracefully to completely non-
parametric formulations – where typical observations provide very little information about the tail

12

of the oﬀer distribution. (That point is the main focus in Goldenshluger and Zeevi [2021].)

Our interest in this case-study of optimal stopping is partially driven by that problem per se, and
open questions that pertain, but more broadly in learning challenges in sequential decision problems.
While generic ﬁnite state and action Markov decision processes have served as the common baseline
in theoretical reinforcement learning, features of the optimal stopping problem may translate more
naturally to other core problems in operations research, like inventory control, queuing admission
control, dynamic pricing, and sequential auctions. In particular, in these problems the search for
eﬀective policies often reduces to carefully adjusting thresholds (much like the main characteristic of
the full information policy in optimal stopping). In those contexts, the decision-maker may use data
to estimate particular distributions, like the distribution of arrivals in a queuing system, and these
distributions are linked to state transitions through known structural equations. In such settings,
more passive forms of exploration may suﬃce, in contrast to the temporally extended periods of
active exploration required to reach poorly understood states in, say, a tabular reinforcement learning
problem. Hence, constructing customized solutions to structured problems remains an interesting
area of research in reinforcement learning.

References

Alekh Agarwal, Sham Kakade, and Lin F Yang. Model-based reinforcement learning with a generative

model is minimax optimal. In Conference on Learning Theory, pages 67–83. PMLR, 2020.

Mohammad Gheshlaghi Azar, Ian Osband, and Rémi Munos. Minimax regret bounds for rein-
forcement learning. In International Conference on Machine Learning, pages 263–272. PMLR,
2017.

Dimitri P Bertsekas. Dynamic programming and optimal control, volume 1. Athena scientiﬁc Belmont,

MA, 1995.

Frans A Boshuizen and Theodore P Hill. Moment-based minimax stopping functions for sequences

of random variables. Stochastic processes and their applications, 43(2):303–316, 1992.

Thomas S Ferguson et al. Who solved the secretary problem? Statistical science, 4(3):282–289, 1989.

A Goldenshluger and A Zeevi. Optimal stopping of a random sequence with unknown distribution.

Mathematics of Operations Research (to appear), 2021.

Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement

learning. Journal of Machine Learning Research, 11(4), 2010.

Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is Q-learning provably eﬃcient?
In Proceedings of the 32nd International Conference on Neural Information Processing Systems,
pages 4868–4878, 2018.

Leo Moser. On a problem of Cayley. Scripta Math, 22:289–292, 1956.

Arnab Nilim and Laurent El Ghaoui. Robust control of Markov decision processes with uncertain

transition matrices. Operations Research, 53(5):780–798, 2005.

13

Joseph D Petruccelli. Maximin optimal stopping for normally distributed random variables. Sankhy¯a:

The Indian Journal of Statistics, Series A, 47:36–46, 1985.

Joseph D Petruccelli et al. On a best choice problem with partial information. Annals of Statistics,

8(5):1171–1174, 1980.

Stephen M Samuels. Minimax stopping rules when the underlying distribution is uniform. Journal

of the American Statistical Association, 76(373):188–197, 1981.

Theodor J Stewart. Optimal selection from a random sequence with learning of the underlying

distribution. Journal of the American Statistical Association, 73(364):775–780, 1978.

Aad W Van der Vaart. Asymptotic statistics, volume 3. Cambridge university press, 2000.

Ruosong Wang, Simon S Du, Lin Yang, and Sham Kakade. Is long horizon RL more diﬃcult than

short horizon RL? Advances in Neural Information Processing Systems, 33, 2020.

Zihan Zhang, Xiangyang Ji, and Simon S Du. Is reinforcement learning more diﬃcult than bandits?
a near-optimal algorithm escaping the curse of horizon. arXiv preprint arXiv:2009.13503, 2020.

14

A Characterizing the Exponential-Decay Distribution

This section provides some technical results on the approximation of tailed probability, hazard rate
and other metric characterizing the level of concentration of the exponential-decay distribution (6).

Deﬁne the inverse CDF as

the conditional expectation larger than S as

Pθ(S) = 1 − Fθ(S),

µθ(S) = Eθ[X|X > S],

the hazard rate as

and

hθ(S) =

fθ(S)
Pθ(S)

Pθ(S)

gθ(S) :=

(cid:82) +∞
S

Pθ(x)dx

.

Lemma 1 (Bounds on Tailed Probability). Consider any θ ∈ R and S > θ + 1.

• If α ≥ 1, then

(cid:18)

C0 ·

1
α(S − θ)α−1 −

α − 1
α2(S − θ)2α−1

(cid:19)

e−(S−θ)α

≤ Pθ(S) ≤

C0

α(S − θ)α−1 e−(S−θ)α

.

• If α ∈ [1/2, 1), then

C0

α(S − θ)α−1 e−(S−θ)α

≤ Pθ(S) ≤ C0 ·

(cid:18)

1
α(S − θ)α−1 +

1 − α
α2(S − θ)2α−1

(cid:19)

e−(S−θ)α

.

Proof. For α ≥ 1, we have

Pθ(S) =

≤

=

(cid:90) +∞

C0

e−ydy

α−1
α

αy

(cid:90) +∞

(S−θ)α
C0
α(S − θ)α−1
C0
α(S − θ)α−1 e−(S−θ)α

(S−θ)α

.

e−ydy

On the other hand,

Pθ(S) ≥ C0 ·

(cid:90) +∞

(cid:32)

(S−θ)α

1
α−1
α

αy

−

= C0 ·

(cid:34)

α − 1
2α−1
α

α2y

−

1
α−1
α

αy

= C0 ·

(cid:18)

1
α(S − θ)α−1 −

(cid:33)

e−ydy

(α − 1)(2α − 1)

α3y

3α−1
α

(cid:35)+∞

(S−θ)α
α − 1
α2(S − θ)2α−1

(cid:19)

e−(S−θ)α

.

For 1

2 ≤ α < 1, we can obtain the same type of result by changing the directions of two inequalities

and swapping the upper and lower bounds.

15

Next we consider a metric of a distribution:

(cid:82) +∞
S
Similar to hazard rate, gθ(S) also characterizes the decay rate of the tail. This can be formalized by
Lemma 2.

Pθ(x)dx

gθ(S) =

Pθ(S)

.

Lemma 2. For any S ∈ R,

Proof. Note that for any S ∈ R,

gθ(S) =

1
µθ(S) − S

.

(cid:90) ∞

S

Pθ(x)dx =

(cid:90) ∞

(cid:18)(cid:90) ∞

(cid:19)

fθ(t)dt

dx

S
(cid:90) ∞

x
(cid:18)(cid:90) t

(cid:19)

fθ(t)dx

dt

S

(t − S)fθ(t)dt

S
(cid:90) ∞

S

=

=

= Pθ(S) · (µθ(S) − S) .

Now we can state the asymptotic approximation results on hθ(S) and gθ(S). Note that Lemma

3 holds uniformly for all location parameters θ ∈ R.

Lemma 3. For any θ ∈ R, as S → ∞,

hθ(S) ∼ α(S − θ)α−1,

gθ(S) ∼ α(S − θ)α−1.

Moreover, as T → +∞,

(cid:12)
hθ(T + θ)
(cid:12)
αT α−1 − 1
(cid:12)
(cid:12)
(cid:12)
gθ(T + θ)
(cid:12)
αT α−1 − 1
(cid:12)
(cid:12)
Proof. We ﬁrst consider the case α ≥ 1. By Lemma 1,

sup
θ∈R

sup
θ∈R

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

α(S − θ)α−1 ≤

fθ(S)
Pθ(S)

≤

(cid:16)

thus as S − θ → +∞,

−→ 0,

−→ 0.

1

1
α(S−θ)α−1 −

α−1
α2(S−θ)2α−1

Now we consider gθ(S) = Pθ(S)

(cid:82) ∞
S Pθ(x)dx
(cid:16)

C0 ·

gθ(S) ≥

hθ(S) ∼ α(S − θ)α−1.
. By Lemma 1:

α−1
α2(S−θ)2α−1

(cid:17)

e−(S−θ)α

1
α(S−θ)α−1 −
C0

(cid:82) ∞
S

α(x−θ)α−1 e−(x−θ)αdx

16

(11)

(cid:17) ,

,

and

gθ(S) ≤

(cid:16)

(cid:82) ∞
S C0 ·

C0

α(S−θ)α−1 e−(S−θ)α

1
α(x−θ)α−1 −

α−1
α2(x−θ)2α−1

(cid:17)

e−(x−θ)αdx

.

By the same type of argument presented in Lemma 1, we know that as S − θ → +∞,

(cid:90) ∞

S

C0

α(x − θ)α−1 e−(x−θ)α

dx ∼

C0

α2(S − θ)2α−2 e−(S−θ)α

,

(cid:18)

C0 ·

(cid:90) ∞

S

1
α(x − θ)α−1 −

α − 1
α2(x − θ)2α−1

(cid:19)

e−(x−θ)α

dx ∼

C0

α2(S − θ)2α−2 e−(S−θ)α

,

hence

(12)

(13)

gθ(S) ∼ α(S − θ)α−1.
2 ≤ α < 1, we can obtain the same result by similar argument.

For the case 1
The uniform convergence is a direct consequence of the fact that both hθ(S) = h0(S − θ) and

gθ(S) = g0(S − θ) are functions of (S − θ). Let T = S − θ, we ﬁnish the proof.

Based on Lemma 2 and Lemma 3, we immediately get the following corollary.

Corollary 1. For any θ ≥ 0,

lim
S→+∞

µθ(S) − S
(S − θ)1−α =

1
α

.

(14)

Remark 1. For any α > 0, the exponential-decay distribution Fθ satisﬁes µ(S) − S = o(S). The
restriction α ≥ 1
2 is just to make our proof concise. On the other hand, if F0 is extremely heavy-tailed
(polynomial-tailed), then µ(S) − S = Θ(S).

Remark 2. Consider a class of distributions sharing heavier tails than exponential-decay distribution
described in (6). For any β > 2, the heavy-tailed distribution Gβ has density function

it holds that

gβ(x) =

(cid:26) β−1

xβ , x ≥ 1
x < 1
0,

µ(S) − S =

1
β − 2

· S

for any S ≥ 1. Thus µ(S) − S = Θ(S) for any distribution Gβ, β > 2.

In the end of this section, we provide bounds on the ﬁrst and second derivatives of log Pθ(S)

with respect to S.

Lemma 4. For any θ ∈ R and S > θ,

If S > θ + 1 and α ≥ 1, then

∂ log Pθ(x)
∂x

(S) = −hθ(S).

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂2 log Pθ(x)
∂x2

(S)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

α3(α − 1)(S − θ)3α−2
(α(S − θ)α − (α − 1))2 .

(15)

17

If S > θ + 1 and α ∈ [1/2, 1), then

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂2 log Pθ(x)
∂x2

(cid:12)
(cid:12)
(S)
(cid:12)
(cid:12)

≤ α(1 − α)(S − θ)α−2.

(16)

Proof. The ﬁrst derivative satisﬁes

∂ log Pθ(x)
∂x

(S) =

−fθ(S)
Pθ(S)

= −hθ(S).

For the second derivative,

∂2 log Pθ(x)
∂x2

(S) = −

Pθ(S) · ∂fθ(x)

∂x (S) + fθ(S)2
Pθ(S)2

,

(17)

Notice that ∂fθ(x)

∂x = −α(x − θ)α−1fθ(x), when S > θ + 1, it holds that ∂fθ(x)

∂x (S) < 0. Combining

with Lemma 1, we obtain the following bound on numerator for any α ≥ 1
2

(cid:12)
(cid:12)
Pθ(S) ·
(cid:12)
(cid:12)

∂fθ(x)
∂x

(S) + fθ(S)2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= |α(S − θ)α−1Pθ(S) − fθ(S)| · fθ(S)

≤ α(S − θ)α−1 ·

|α − 1|

α2(S − θ)2α−1 · fθ(S)2

=

|α − 1|
α(S − θ)α · fθ(S)2.

The inequality in the second line is due to the fact that for any α ≥ 1
2

, it holds that

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Pθ(S)
fθ(S)

−

1
α(S − θ)α−1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

|α − 1|
α2(S − θ)2α−1 ,

which is a direct result of equation (11) and its variant version for 1

2 ≤ α < 1.

For the denominator, if α ≥ 1,

(cid:18)

Pθ(S)2 ≥

1
α(S − θ)α−1 −

α − 1
α2(S − θ)2α−1

(cid:19)2

· fθ(S)2;

if 1

2 ≤ α < 1,

Pθ(S)2 ≥

(cid:18)

1
α(S − θ)α−1

(cid:19)2

· fθ(S)2.

Plugging them into the equation (17), we have for α ≥ 1,

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂2 log Pθ(x)
∂x2

(S)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

=

α−1
α(S−θ)α

(cid:17)2

(cid:16)

α−1
α2(S−θ)2α−1

1
α(S−θ)α−1 −
α3(α − 1)(S − θ)3α−2
(α(S − θ)α − (α − 1))2 ;

18

for 1

2 ≤ α < 1,

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂2 log Pθ(x)
∂x2

(cid:12)
(cid:12)
(S)
(cid:12)
(cid:12)

≤

(cid:16)

1−α
α(S−θ)α

(cid:17)2

1
α(S−θ)α−1

= α(1 − α)(S − θ)α−2;

B Uniform Approximation of the Optimal Threshold

In this section we present the uniform approximation result on Bellman threshold value for exponential-
decay distribution, which is essential in the proof of Proposition 1 in Appendix E. Basically, the
solution to Bellman Equation (3) can be approximated by a logarithmic term dependent only on the
horizon length 1/(1 − γ) plus the location parameter θ. Moreover, this approximate decomposition
holds uniformly for all θ in an expanding parameter space. To clarify this point, we consider the case
θ is bounded by an slowly exploding function f (γ), which is a non-negative function of γ satisfying

(cid:32)(cid:18)

f (γ) = o

log

(cid:33)

(cid:19) 1
α

.

1
1 − γ

Lemma 5. If f (γ) satisﬁes (18), then for any θ0 ∈ R, as γ → 1,

sup
θ∈[θ0−f (γ), θ0+f (γ)]

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

S∗(θ, γ) − θ
(cid:17) 1
(cid:16)
α

log 1
1−γ

− 1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

−→ 0.

(18)

(19)

Proof. Without loss of generality we can assume θ0 = 0 since we only consider the asymptotic results
for ﬁxed θ0. First we consider the case α ≥ 1. We consider the integration of tailed probability
(cid:82) +∞
S

Pθ(x)dx for any S, θ. By Lemma 1, we know that:

(cid:18)

C0 ·

(cid:90) ∞

S

1
α(x − θ)α−1 −

α − 1
α2(x − θ)2α−1

(cid:19)

e−(x−θ)α

dx ≤

(cid:90) +∞

Pθ(x)dx

S
(cid:90) ∞

S

≤

C0

α(x − θ)α−1 e−(x−θ)α

dx.

According to the equation (12) and equation (13), if S − θ ≥ T (α, C0) where T (α, C0) is only
dependent on α and C0, the following bounds hold:

1
2

·

C0

α2(S − θ)2α−2 e−(S−θ)α

≤

(cid:90) +∞

S

Pθ(x)dx ≤

2C0

α2(S − θ)2α−2 e−(S−θ)α

.

For any c0 > 0, consider the unique solution S = S(cid:48)

1(θ, γ) to the equation

(cid:90) +∞

S

Pθ(x)dx = (1 −

(cid:18)

) ·

log

2c0
3

1
1 − γ

(cid:19) 1

α 1 − γ
γ

.

19

(20)

(21)

Since the left-hand side of equation (21) is a monotonically decreasing function of (S − θ), and the
right-hand side of equation (21) is a monotonically decreasing function of γ, there exists γ0(cid:48) such
that for every γ ≥ γ0(cid:48), any θ ∈ R, the solution S(cid:48)
1(θ, γ) − θ ≥ T (α, C0), hence (20)
holds. From now on we without loss of generality assume γ ≥ γ0(cid:48). Combining the left-hand side
bound in (20) and equation (21), we have

1(θ, γ) satisﬁes S(cid:48)

1
2

·

C0

α2(S(cid:48)

1(θ, γ) − θ)2α−2 e−(S(cid:48)

1(θ,γ)−θ)α

(cid:18)

≤

log

1
1 − γ

(cid:19) 1

α 1 − γ
γ

.

1(θ, γ) → +∞, and the left-hand side is dominated by the exponential term
1(θ,γ)−θ)α and right-hand side is dominated by the linear term 1 − γ. Therefore, there exists γ1

Notice that as γ → 1, S(cid:48)
e−(S(cid:48)
such that for every γ ≥ γ1,

S(cid:48)

1(θ, γ) ≥ θ + (1 −

(cid:18)

) ·

log

c0
3

1
1 − γ

(cid:19) 1
α

,

for any θ ∈ R.

According to the deﬁnition of f (γ), there exists γ2 such that for every γ ≥ γ2, it holds that

f (γ) ≤

(cid:18)

·

log

(cid:19) 1
α

,

1
1 − γ

c0
3

thus for every γ ≥ γ0 = max {γ1, γ2} and any θ ∈ [−f (γ), f (γ)], it holds that

S(cid:48)

1(θ, γ) ≥ (1 −

(cid:18)

) ·

log

2c0
3

(cid:19) 1
α

.

1
1 − γ

(22)

(23)

Also consider S1(θ, γ) := θ + (1 − c0) ·
holds that

(cid:16)

log 1
1−γ

(cid:17) 1

α . By equation (22), for any θ ∈ [−f (γ), f (γ)], it

S1(θ, γ) ≤ (1 −

(cid:18)

) ·

log

2c0
3

(cid:19) 1
α

,

1
1 − γ

(24)

thus S1(θ, γ) ≤ S(cid:48)

1(θ, γ).

Deﬁne function B(S) as B(S) := 1
S

Pθ(x)dx. Notice that the integration of tailed probability
are both monotonically decreasing, it holds that for every γ ≥ γ0 and any

(cid:82) +∞
S

(cid:82) +∞
S
θ ∈ [−f (γ), f (γ)],

Pθ(x)dx and 1
S

B(S1(θ, γ)) =

1
S1(θ, γ)

≥

(cid:90) +∞

Pθ(x)dx

S1(θ,γ)
1
(cid:16)

(cid:90) +∞

(cid:17) 1
α

S(cid:48)

1(θ,γ)

Pθ(x)dx

(1 − 2c0

3 ) ·

log 1
1−γ

the last equality holds due to equation (21).

=

1 − γ
γ

,

20

Notice that the optimal threshold S∗(θ, γ) satisﬁes the equation B(S∗(θ, γ)) = 1−γ
γ
monotonically decreasing for S > 0, thus for every γ ≥ γ0 and any θ ∈ [−f (γ), f (γ)],

, and B(S) is

S∗(θ, γ) ≥ S1(θ, γ) = θ + (1 − c0) ·

(cid:18)

log

(cid:19) 1
α

.

1
1 − γ

By an almost same argument, we obtain the bound on other side:
θ ∈ [−f (γ), f (γ)],

for every γ ≥ γ(cid:48)
0

and any

S∗(θ, γ) ≤ S2(θ, γ) = θ + (1 + c0) ·

(cid:18)

log

(cid:19) 1
α

.

1
1 − γ

By the arbitrariness of selection of c0, we obtain equation (19) as γ → 1 for α ≥ 1. The case
2 ≤ α < 1 can be proved by an almost same argument.
1

The key purpose of Lemma 5 is that we can give a uniform bound on the derivative of the
threshold with respect to the location parameter θ (Lemma 6), which can help to relate the diﬀerence
of two Bellman thresholds with that of the location parameters (Corollary 3).

Note that unlike typical uniform convergence results, our result holds uniformly for an varying

α over an expanding
universe, i.e., as γ gets closer to 1, S∗(θ, γ) can be approximated by θ +
region in which parameter θ lies. Based on that, we can further obtain an approximation result on
gθ(S∗(θ, γ)) and S∗(θ, γ)gθ(S∗(θ, γ)).

log 1
1−γ

(cid:16)

(cid:17) 1

Corollary 2. If f (γ) satisﬁes (18), then for any θ0 ∈ R, as γ → 1,

sup
θ∈[θ0−f (γ), θ0+f (γ)]

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

α

gθ(S∗(θ, γ))
(cid:16)

(cid:17)1− 1
α

log 1
1−γ

− 1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

−→ 0,

and

sup
θ∈[θ0−f (γ), θ0+f (γ)]

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

S∗(θ, γ)gθ(S∗(θ, γ))
α log 1
1−γ

− 1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

−→ 0.

(25)

(26)

Proof. Without loss of generality we assume θ0 = 0. For any (cid:15)1 > 0, by Lemma 3, there exists
T0 > 0 where T0 not dependent on γ such that once S∗(θ, γ) − θ ≥ T0 is satisﬁed, it holds that

(1 − (cid:15)1)

1
2 ≤

hθ(S∗(θ, γ))

α(S∗(θ, γ) − θ)α−1 ≤ (1 + (cid:15)1)

1
2 .

(27)

By Lemma 5, there exists γ1 such that for every γ ≥ γ1, any θ ∈ [−f (γ), f (γ)], we have S∗(θ, γ)−θ ≥
T0 hold. Thus (27) holds for any γ ≥ γ1.

On the other hand, by Lemma 5, there exists γ2 > 0 such that for every γ ≥ γ2,

sup
θ∈[−f (γ), f (γ)]








α−1

S∗(θ, γ) − θ
(cid:17) 1
(cid:16)
α

log 1
1−γ




≤ (1 + (cid:15)1)

1
2 ,

21

and





α−1

inf
θ∈[−f (γ), f (γ)]

S∗(θ, γ) − θ
(cid:17) 1
(cid:16)
α







≥ (1 − (cid:15)1)

1
2 .

log 1
1−γ
Let γ0 = max{γ1, γ2}, then for every γ ≥ γ0, by multiplying (27) and the bounds above, we have:

sup
θ∈[−f (γ), f (γ)]

inf
θ∈[−f (γ), f (γ)]

α

α

gθ(S∗(θ, γ))
(cid:16)

(cid:17)1− 1
α

log 1
1−γ

gθ(S∗(θ, γ))
(cid:16)

(cid:17)1− 1
α

log 1
1−γ

≤ 1 + (cid:15)1,

≥ 1 − (cid:15)1,

(28)

(29)

Therefore we obtain (25).

For any (cid:15)2 > 0, by Lemma 5, there exists γ(cid:48)
1

such that for every γ ≥ γ(cid:48)
1

,

sup
θ∈[−f (γ), f (γ)]

inf
θ∈[−f (γ), f (γ)]

S∗(θ, γ) − θ
(cid:17) 1
(cid:16)
α

log 1
1−γ

S∗(θ, γ) − θ
(cid:17) 1
(cid:16)
α

log 1
1−γ

≤ 1 +

≥ 1 −

(cid:15)2
2

,

(cid:15)2
2

.

Since f (γ) = o

(cid:18)(cid:16)

log 1
1−γ

(cid:19)

(cid:17) 1
α

, there exists γ(cid:48)
2

such that for every γ ≥ γ(cid:48)
2

,

f (γ) ≤

(cid:18)

·

log

(cid:19) 1
α

.

1
1 − γ

(cid:15)2
2

Thus for every γ ≥ γ(cid:48)

0 = max{γ(cid:48)

1, γ(cid:48)

2},

sup
θ∈[−f (γ), f (γ)]

(cid:16)

inf
θ∈[−f (γ), f (γ)]

(cid:16)

S∗(θ, γ)

(cid:17) 1
α

log 1
1−γ

S∗(θ, γ)

(cid:17) 1
α

log 1
1−γ

≤ 1 + (cid:15)2,

≥ 1 − (cid:15)2.

Combining with (28) and (29), we obtain (26).

Next we reformulate Lemma 5 through the lens of derivative.

Lemma 6. If f (γ) satisﬁes (18), then for any θ0 ∈ R, as γ → 1,

sup
θ∈[θ0−f (γ), θ0+f (γ)]

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂S∗(θ, γ)
∂θ

− 1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

−→ 0.

(30)

22

Proof. Without loss of generality we assume θ0 = 0. Note that the Bellman Equation is

1
γ

S∗(θ, γ) =

1
1 − γ

(cid:90) +∞

S∗(θ,γ)

Pθ(x)dx,

By Lemma 5, there exists γ0(cid:48) such that for every γ ≥ γ0(cid:48), for any θ ∈ R, it holds that S∗(θ, γ) ≥ θ + 1.
We know that

By Leibniz integral rule, we have:

∂Pθ(x)
∂θ

= −

∂Pθ(x)
∂x

= fθ(x).

1 − γ
γ

·

∂S∗(θ, γ)
∂θ

= −

∂S∗(θ, γ)
∂θ

· Pθ(S∗(θ, γ)) +

(cid:90) +∞

S∗(θ,γ)

fθ(x)dx,

Thus we have the optimal threshold S∗(θ, γ) satisfy:

∂S∗(θ, γ)
∂θ

=

Pθ(S∗(θ, γ))
Pθ(S∗(θ, γ)) + 1−γ
γ

.

(31)

By the deﬁnition of gθ(S∗(θ, γ)), the Bellman Equation is also equivalent to

Pθ(S∗(θ, γ)) =

S∗(θ, γ)gθ(S∗(θ, γ))
γ

· (1 − γ).

By plugging it into equation (31), we have

∂S∗(θ, γ)
∂θ

= 1 −

1
S∗(θ, γ)gθ(S∗(θ, γ)) + 1

≤ 1.

For any (cid:15)0 > 0, there exists γ1 such that α log 1
γ2 such that for every γ ≥ γ2,

1−γ ≥ 2/(cid:15)0. Meanwhile by Corollary 2, there exists

inf
θ∈[−f (γ), f (γ)]

S∗(θ, γ)gθ(S∗(θ, γ))
α log 1
1−γ

≥

1
2

.

Therefore for every γ ≥ γ0 = max{γ1, γ2},

sup
θ∈[−f (γ), f (γ)]

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂S∗(θ, γ)
∂θ

− 1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ (cid:15)0.

One immediate consequence of Lemma 6 is that the perturbation of location parameter leads to
an almost same extent of perturbation on the optimal threshold as γ close to 1 enough, Moreover,
it is in the uniform sense. Thus it provides a good approximation of the diﬀerence of two optimal
thresholds.

2 and θ0 ∈ R, suppose f (γ) satisﬁes (18). Then for any constant
Corollary 3. For ﬁxed α ≥ 1
c0 > 0, there exist γ0 such that for every γ ≥ γ0, the following bound holds uniformly for any
θ ∈ [θ0 − f (γ), θ0 + f (γ)]:

(1 − c0) · (θ − θ0) < S∗(θ, γ) − S∗(θ0, γ) < (1 + c0) · (θ − θ0).

(32)

23

Proof. Without loss of generality we assume θ0 = 0. For any θ ∈ [−f (γ), f (γ)] and γ, by Mean
Value Theorem, we have

S∗(θ, γ) − S∗(θ0, γ) =

∂S∗(θ, γ)
∂θ

(˜θ) · (θ − θ0),

where ˜θ ∈ [−f (γ), f (γ)]. Meanwhile, by Lemma 6, for any constant c0 > 0, there exists γ0 such that
for every γ ≥ γ0, it holds that

sup
˜θ∈[−f (γ), f (γ)]

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂S∗(θ, γ)
∂θ

(˜θ) − 1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ c0.

Thus we ﬁnish the proof.

C Proofs of Theorem 3 and Theorem 4

Besides Proposition 1, the proofs of Theorem 3 and Theorem 4 rely on the following Proposition 2.
This result actually holds on under a broader set of distributions than those in (6). In particular, we
will later show that (33) and (34) hold for our class under the exponential decay distributions in (6).
The more abstract presentation here can be thought of as identifying properties that seem critical.
To emphasize that tail probability and conditional expectation are evaluated under the ground truth
F0, we denote EF0[X|X > S] as µ0(S) and PF0[X > S] as P0(S). For any distribution F , we denote
the solution to Bellman Equation (3) as S∗(F, γ) in Proposition 2.

Proposition 2. Suppose F0 and {Fγ}0<γ<1 satisﬁes

as S → +∞ and Bellman thresholds S∗(Fγ, γ) satisfy

µ0(S) ∼ S

S∗(Fγ, γ) ∼ S∗(F0, γ)

as γ → 1.

• If P0(S∗(Fγ, γ)) = ω(1 − γ), then

• If P0(S∗(Fγ, γ)) = o(1 − γ), then

R(F0, γ, τ ∗(Fγ)) = 0.

lim
γ→1

R(F0, γ, τ ∗(Fγ)) = 1.

lim
γ→1

(33)

(34)

The proofs of the two propositions will be given in Appendix F and Appendix E respectively.

Now we recall Theorem 3:

24

Theorem 3 (Phase-transition for relative regret under overestimation). If (cid:15) : [0, 1] → R+ satisﬁes
lim inf γ→1 (cid:15)(γ)/(cid:15)critical(γ) > 1, then

R(θ, γ, τ ∗(θ + (cid:15)(γ))) = 1

for any θ ∈ R.

lim
γ→1

If (cid:15) : [0, 1] → R+ satisﬁes lim supγ→1 (cid:15)(γ)/(cid:15)critical(γ) < 1, then

R(θ, γ, τ ∗(θ + (cid:15)(γ))) = 0

for any θ ∈ R.

lim
γ→1

Proof. By Corollary 1, (33) holds for Fθ. Meanwhile by Lemma 5, (34) holds for Fθ. According to
Proposition 1 and Proposition 2, we obtain the result.

Recall Theorem 4:

Theorem 4 (Robustness to underestimation). If (cid:15) : [0, 1] → R+ satisﬁes

(cid:32)(cid:18)

(cid:15)(γ) = o

log

(cid:33)

(cid:19) 1
α

1
1 − γ

as γ → 1,

(10)

then

R(θ, γ, τ ∗(θ − (cid:15)(γ))) = 0

for any θ ∈ R.

lim
γ→1

Proof. Notice that for negative perturbation (cid:15)(γ) < 0, S∗(θ + (cid:15)(γ), γ) < S∗(θ, γ). Meanwhile by
Corollary 4, it holds that Pθ(S∗(θ, γ)) = ω(1 − γ), therefore

By Proposition 2, we obtain limγ→1 R(Fθ, γ, τ ∗(θ + (cid:15)(γ))) = 0.

Pθ(S∗(θ + (cid:15)(γ), γ)) = ω(1 − γ).

D Proof of Main Theorems

In this section we prove Theorem 1 and Theorem 2.

We ﬁrst introduce a lemma to show that the maximum likelihood estimator is asymptotically
normal. We use the following result on the asymptotic normality of M-estimators. In the notation of
Van der Vaart [2000], P denotes the population distribution from which data is drawn and, viewing
this as a linear functional, he use writes P g = EX∼P [g(X)]. The important feature of this result, for
our purposes, is that he requires a Taylor expansion of P mθ rather than mθ itself. This will allow
us to smooth out the discontinuity of the log-likelihood log fθ(·) through integration. We use the
notation P→ or d→ to denote convergence in probability and convergence in distribution, respectively.

Theorem (Theorem 5.23 of Van der Vaart [2000]). For each θ in an open subset of Euclidean space
let x (cid:55)→ mθ(x) be a measurable function such that θ (cid:55)→ mθ(x) is diﬀerentiable at θ0 for P −almost
every x with derivative ˙mθ0(x) and such that, for every θ1 and θ2 in a neighborhood of θ0 and a
measurable function ˙m with P ˙m2 < ∞

|mθ1(x) − mθ2(x)| ≤ ˙m(x) (cid:107)θ1 − θ2(cid:107) .

(35)

25

Furthermore, assume that the map θ (cid:55)→ P mθ admits a second-order Taylor expansion at a point of
maximum θ0, meaning

P mθ = P mθ0 +

1
2
with nonsingular symmetric second derivative matrix Vθ0. If ˆθn ∈ arg maxθ n−1 (cid:80)n
X1, X2 · · · i.i.d∼ P satisﬁes ˆθn

(θ − θ0)(cid:62)Vθ0(θ − θ0) + o((cid:107)θ − θ0(cid:107)2),

P→ θ0, then

(36)

i=1 mθ(Xi) where

√

(cid:16)ˆθn − θ0

(cid:17)

n

= −V −1
θ0

1
√
n

n
(cid:88)

i=1

˙mθ0 (Xi) + oP (1).

In particular, the sequence
matrix V −1
θ0

P ˙mθ0 ˙mT
θ0

V −1
θ0

.

√

(cid:16)ˆθn − θ0

(cid:17)

n

is asymptotically normal with mean zero and covariance

Lemma 7. Suppose α > 1 and ﬁx any θ0 ∈ R. If X1, X2 · · · are drawn i.i.d from Fθ0 and for any
n ∈ N, ˆθn = arg maxθ

i=1 log fθ(Xi), then there exists some σ > 0 such that

(cid:80)n

√

(cid:16)ˆθn − θ0

n

(cid:17) d−→ N (cid:0)0, σ2(cid:1) .

Proof. Throughout the proof, let E[g(X)] = (cid:82) g(x)fθ0(x)dx denote the expectation under the
true parameter θ0 and put En[g(X)] = 1
i=1 g(Xi) denote the expectation under the empirical
n
distribution. Deﬁne mθ(x) = log fθ(x)−C0 = −|x−θ|α, Mn(θ) = En [mθ(X)] and M (θ) = E[mθ(X)].
(In the notation of Van der Vaart, we have Mn(θ) = P mθ.) The maximum likelihood estimator is
the unique maximizer of Mn(·), the true parameter is the unique maximizer of M (·) and by the law
of the large number Mn(θ) → M (θ) almost surely for each ﬁxed θ.

(cid:80)n

It is clear that mθ(·) is inﬁtely diﬀerentiable except at the singe point θ. We check the remaining

conditions needed

• Step 1: ˆθn

P→ θ0 .

First, we show that there is a compact interval Θ containing θ0 such that P(ˆθn ∈ Θ) → 1. Let
¯Xn = 1
n

1 Xi denote the empirical mean. By concavity, we have

(cid:80)n

Mn(θ) = En [mθ(X)] ≤ mθ( ¯Xn) = − (cid:12)

(cid:12) ¯Xn − θ(cid:12)
α .
(cid:12)

For given (cid:15) > 0 we can take (random) N suﬃciently large such that for every n ≥ N ,
| ¯Xn − θ0| ≤ (cid:15) and Mn(θ0) ≥ M (θ0) − (cid:15). Then, it is clear that there exists a bounded interval
Θ containing θ0 (whose width is dependent on (cid:15)) such that Mn(θ) < Mn(θ0) for every θ /∈ Θ.
Take ¯θn = arg maxθ∈Θ Mn(θ) to be the maximum likelihood estimator restricted to this
compact set. By the argument above, ˆθn − ¯θn → 0 almost surely. Hence, it suﬃces to verify
the consistency of the ¯θn. This follows by Theorem 5.7 of Van der Vaart [2000]. To apply this
theorem, we use that (i) The collection (Mn(θ))θ∈Θ obeys a uniform law of large numbers,
p
meaning supθ∈Θ |Mn(θ) − M (θ)|
→ 0 and (ii) the population criterion M (·) is strictly concave,
so supθ:|θ−θ0|≥(cid:15) M (θ) < M (θ0).

26

• Step 2: The local Liphshitz condition (35) is satisﬁed.

For any θ1, θ2 ∈ [θ0 − h, θ0 + h], we have the following condition holds:

|mθ1(x) − mθ2(x)| = (cid:107)x − θ1|α − |x − θ2|α| ≤ α max{|x − θ0 − h|α, |x − θ0 + h|α} |θ1 − θ2| .

• Step 3: The Taylor expansion in (36) applies.

It is clear that M (cid:48)(θ0) = 0, since θ0 is the maximizer. We will show the second derivative
M (cid:48)(cid:48)(·) exists and is continuous in a neighborhood of θ0. To see why this suﬃces, observe that
by the mean-value form of Taylor’s theorem, if θ > θ0 is another point in the neighborhood of
θ0 then there exists ˜θ ∈ [θ0, θ] such that

M (θ) = M (θ0) +

1
2

M (cid:48)(cid:48)(˜θ)(θ − θ0)2 = M (θ0) +

1
2

M (cid:48)(cid:48)(θ0)(θ − θ0)2 + o ((cid:107)θ − θ0(cid:107)) .

We now show M (cid:48)(cid:48)(θ) exits and is continuous. To do this, we argue that mθ is twice diﬀerentiable
(cid:105)
almost everywhere and justify the change of limit and integral so that M (cid:48)(cid:48)(θ) = E
.
By symmetry, we only need to consider θ > θ0.
Notice that for any x, ∂mθ(x)
θ < x, then ∂mθ(x)
−α(θ − x)α−1, ∂2mθ(x)
Meanwhile notice that ∂2mθ(x)

exist and are continuous except for θ = x.
∂θ2 = −α(α − 1)(x − θ)α−2. If θ > x, then ∂mθ(x)

and ∂2mθ(x)
∂θ = α(x − θ)α−1, ∂2mθ(x)

∂θ2 = −α(α − 1)(θ − x)α−2.

(cid:104) ∂2mθ(x)
∂θ2

is integrable:

∂θ =

∂θ2

If

∂θ

∂θ2

E

(cid:20)(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂2mθ(x)
∂θ2

(cid:12)
(cid:21)
(cid:12)
(cid:12)
(cid:12)

=

(cid:90) +∞

−∞

α(α − 1)|x − θ|α−2e−|x−θ0|α

dx < +∞,

where the fact that integral is ﬁnite depends on the fact that α > 1 and can be veriﬁed carefully
by integration by parts (similar to the argument below).

Using the leibniz rule,

∂2M (θ)
∂θ2 = E

(cid:21)

(cid:20) ∂2mθ(X)
∂θ2

= E

(cid:21)

(cid:20) ∂2mθ(X)
∂X 2

(cid:90) +∞

= −

α2(x − θ)α−1(x − θ0)α−1e−(x−θ0)α

dx

(cid:90) θ

θ
α2(θ − x)α−1(x − θ0)α−1e−(x−θ0)α

dx

.

θ0
(cid:90) θ0

−∞

α2(θ − x)α−1(θ0 − x)α−1e−(θ0−x)α

dx

+

−

The last equality is due to integration by parts. We can see that ∂2M (θ)
continuous. Meanwhile, notice that ∂2M (θ)
thus it is nonzero, which implies the existence of ﬁnite variance σ2.

|θ=θ0 = −2 (cid:82) +∞

is bounded and
α2(x − θ0)2α−2e−(x−θ0)αdx < 0,

∂θ2

∂θ2

θ0

27

Now we can prove Theorem 1:

Theorem 1. (Sample complexity light-tailed distributions) Suppose α > 1.

(i.) If N = o (Ncritical(γ)) and N = ω(1), then

lim
γ→1

R(θ, γ, τN ) =

1
2

for any θ ∈ R.

(ii.) If N = ω (Ncritical(γ)) and N = o

(cid:17)

(cid:16) 1
1−γ

, then

R(θ, γ, τN ) = 0

for any θ ∈ R.

lim
γ→1

Proof. We condition on the ﬁrst N samples which is used for estimating θ and take expectation.
According to the deﬁnition of plug-in policy τN , we have

Eθ[γτN XτN ] = EθEθ [γτN XτN |X1, X2, ... , XN ] = γN EθEθ

(cid:104)

γτ ∗(θ+ˆ(cid:15))Xτ ∗(θ+ˆ(cid:15))|X1, X2, ... , XN

(cid:105)

,

where ˆ(cid:15) is the error of plug-in estimator for θ based on the ﬁrst N samples:

ˆ(cid:15) = ˆθ − θ.

Thus we have

Notice that

R(Fθ, γ, τN ) = 1 − γN (1 − Eθ [R(Fθ, γ, τ ∗(θ + ˆ(cid:15)))|X1, X2, ... , XN ]).

Eθ [R(Fθ, γ, τ ∗(θ + ˆ(cid:15)))|X1, X2, ... , XN ] = P(ˆ(cid:15) > 0)Eθ [R(Fθ, γ, τ ∗(θ + ˆ(cid:15)))|ˆ(cid:15) > 0]
+ P(ˆ(cid:15) ≤ 0)Eθ [R(Fθ, γ, τ ∗(θ + ˆ(cid:15)))|ˆ(cid:15) ≤ 0]

(37)

(38)

The ﬁrst term and second term in the right-hand side of equation (38) correspond to the contribution
of overestimation and underestimation to the expected regret, respectively.

For the overestimation term, we continue to decompose it. Based on the sample size, we use

diﬀerent formulas.

1. α > 1 and N = o





(cid:16)





(cid:17)1− 1
α

log 1
1−γ
log log 1
1−γ



2





In this case, we decompose the overestimation term as follows:

P(ˆ(cid:15) > 0)Eθ [R(Fθ, γ, τ ∗(θ + ˆ(cid:15)))|ˆ(cid:15) > 0] = P(ˆ(cid:15) ≥ (cid:15)1(γ))Eθ [R(Fθ, γ, τ ∗(θ + ˆ(cid:15)))|ˆ(cid:15) ≥ (cid:15)1(γ)]

+P(0 < ˆ(cid:15) < (cid:15)1(γ))Eθ [R(Fθ, γ, τ ∗(θ + ˆ(cid:15)))|0 < ˆ(cid:15) < (cid:15)1(γ)]

(39)

where

(cid:15)1(γ) :=

(cid:18)

log

2
α

1
1 − γ

(cid:19) 1

α −1

log log

1
1 − γ

.

28

(cid:18)

Note that N (γ) → +∞ as γ → 1; by Lemma 7,

d−→ N (cid:0)0, σ2(cid:1), where σ2 > 0.
Also (cid:15)1(γ) = o
and P(0 < ˆ(cid:15) <
(cid:15)1(γ)) → 0. Meanwhile, by Theorem 3 and bounded convergence theorem, we know that
Eθ [R(Fθ, γ, τ ∗(θ + ˆ(cid:15)))|ˆ(cid:15) ≥ (cid:15)1(γ)] → 1. Therefore as γ → 1, it holds that

. Thus as γ → 1, we have P(ˆ(cid:15) ≥ (cid:15)1(γ)) → 1
2

1√

N ˆ(cid:15)

N (γ)

(cid:19)

√

P(ˆ(cid:15) > 0)Eθ [R(Fθ, γ, τ ∗(θ + ˆ(cid:15)))|ˆ(cid:15) > 0] −→

1
2

.

2. α > 1 and N = ω





(cid:16)





(cid:17)1− 1
α

log 1
1−γ
log log 1
1−γ



2





In this case, we decompose the overestimation term as follows:

P(ˆ(cid:15) > 0)Eθ [R(Fθ, γ, τ ∗(θ + ˆ(cid:15)))|ˆ(cid:15) > 0] = P(ˆ(cid:15) ≥ (cid:15)2(γ))Eθ [R(Fθ, γ, τ ∗(θ + ˆ(cid:15)))|ˆ(cid:15) ≥ (cid:15)2(γ)]

+P(0 < ˆ(cid:15) < (cid:15)2(γ))Eθ [R(Fθ, γ, τ ∗(θ + ˆ(cid:15)))|0 < ˆ(cid:15) < (cid:15)2(γ)]

(40)

where

(cid:15)2(γ) :=

(cid:18)

log

1
2α

1
1 − γ

(cid:19) 1

α −1

log log

1
1 − γ

.

(cid:18)

(cid:19)

1√

, thus as γ → 1, we have P(ˆ(cid:15) ≥ (cid:15)2(γ)) → 0 and P(0 < ˆ(cid:15) <
Note that (cid:15)2(γ) = ω
. Notice that Eθ [R(Fθ, γ, τ ∗(θ + ˆ(cid:15)))|ˆ(cid:15) ≥ (cid:15)2(γ)] is bounded by 1. Meanwhile, by The-
(cid:15)2(γ)) → 1
2
orem 3 and bounded convergence theorem, we know that Eθ [R(Fθ, γ, τ ∗(θ + ˆ(cid:15)))|0 < ˆ(cid:15) < (cid:15)2(γ)] →
0. Therefore as γ → 1, it holds that

N (γ)

P(ˆ(cid:15) > 0)Eθ [R(Fθ, γ, τ ∗(θ + ˆ(cid:15)))|ˆ(cid:15) > 0] −→ 0.

For the underestimation term in the right-hand side of equation (38), we can decompose it in a

uniﬁed way regardless of diﬀerent cases:

P(ˆ(cid:15) ≤ 0)Eθ [R(Fθ, γ, τ ∗(θ + ˆ(cid:15)))|ˆ(cid:15) ≤ 0] = P(ˆ(cid:15) ≤ −(cid:15)3(γ))Eθ [R(Fθ, γ, τ ∗(θ + ˆ(cid:15)))|ˆ(cid:15) ≤ −(cid:15)3(γ)]

+P((cid:15)3(γ) < ˆ(cid:15) ≤ 0)Eθ [R(Fθ, γ, τ ∗(θ + ˆ(cid:15)))|(cid:15)3(γ) < ˆ(cid:15) ≤ 0]

where

(cid:18)

(cid:15)3(γ) :=

log

(cid:19) 1
2α

.

1
1 − γ

As γ → 1, we have (cid:15)3(γ) → +∞. By the similar argument as in the case 2 for computing
overestimation term, we obtain the result that as γ → 1, it holds that

P(ˆ(cid:15) ≤ 0)Eθ [R(Fθ, γ, τ ∗(θ + ˆ(cid:15)))|ˆ(cid:15) ≤ 0] −→ 0.

(41)

Notice that if N = o

(cid:17)

(cid:16) 1
1−γ

, then γN → 1 as γ → 1. Combining the computation for diﬀerent

cases and equation (37), we prove the result stated in Theorem 1.

We recall Theorem 2:

29

Theorem 2 (Suﬃciency of a single observation). If α ∈ (cid:2) 1

2 , 1(cid:3), then
for any θ ∈ R.

lim
γ→1

R(θ, γ, τ1) = 0

Proof. The proof of Theorem 2 shares exactly the same spirit with the proof of Theorem 1. Because
there is only single sample, we have ˆθ = X1, thus

ˆ(cid:15) = X1 − θ.

We still consider the equation (38) and decompose the overestimation term as equation (40). Note
that α ≤ 1 implies (cid:15)2(γ) → +∞ as γ → 1, thus we have

lim
γ→1

P{|ˆ(cid:15)| ≥ (cid:15)2(γ)} = lim
(cid:15)→+∞

P{|X1 − θ| > (cid:15)} = 0.

By the similar argument as in the case 2 for computing overestimation term, we obtain the same
result that as γ → 1, it holds that

P(ˆ(cid:15) > 0)Eθ [R(Fθ, γ, τ ∗(θ + ˆ(cid:15)))|ˆ(cid:15) > 0] −→ 0.

Combining with the same underestimation result (41), we ﬁnish the proof of Theorem 2.

E Proof of Proposition 1

In this section, we prove the Proposition 1. We use ground truth stopping probability P0(S∗(F0, γ))
as an intermediate, and transfer the comparison of P0(S∗(Fγ, γ)) with 1 − γ to that of P0(S∗(Fγ, γ))
with P0(S∗(F0, γ)). Since we from now on we only consider the exponential-decay distribution Fθ
speciﬁed in (6), we slightly change the notation without ambiguity.

We ﬁrst introduce a lemma focusing on the ﬁrst-order approximation of the tailed probability

ratio

Pθ0 (S∗(θ0,γ))

Pθ0 (S∗(θ0+(cid:15)(γ),γ)) . To simplify our notation, we denote S∗(θ0, γ) as S∗

0 (γ).

Lemma 8. Suppose ∆(γ) is a non-negative function satisfying ∆(γ) = o

(cid:18)(cid:16)

(cid:17) 1
α

log 1
1−γ

(cid:19)

, then for

any constant c0, c0 > 0, there exists γ0 such that for every γ ≥ γ0, it holds that

exp ((1 − c0)hθ0(S∗

0 (γ)) · ∆(γ)) ≤

Pθ0(S∗

0 (γ))
0 (γ) + ∆(γ))

Pθ0(S∗

≤ exp ((1 + c0)hθ0(S∗

0 (γ)) · ∆(γ)).

(42)

Proof. Without loss of generality we assume S∗
exists γ0(cid:48) such that for every γ ≥ γ0(cid:48), S∗

0 (γ) > θ0 + 1.) Notice that

0 (γ) > θ0 + 1. (By Lemma 5, we know that there

Pθ0(S∗

0 (γ))
0 (γ) + ∆(γ))

Pθ0(S∗

by Mean Value Theorem,

= exp (log Pθ0(S∗

0 (γ)) − log Pθ0(S∗

0 (γ) + ∆(γ))),

log Pθ0(S∗

0 (γ)) − log Pθ0(S∗

2 (γ)) = −

where ˜S ∈ [S∗

0 (γ), S∗

0 (γ) + ∆(γ)].

∂ log Pθ0(x)
∂x

(S∗

0 (γ)) · ∆(γ) −

1
2

·

∂2 log Pθ0(x)
∂x2

( ˜S)∆(γ)2,

(43)

30

By Lemma 4 and Lemma 3, we have

∂ log Pθ0(x)
∂x

(S∗

0 (γ)) = −hθ0(S∗

0 (γ)) ∼ −α(S∗

0 (γ) − θ0)α−1,

thus there exists γ1 such that for every γ ≥ γ1, it holds that

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂ log Pθ0(x)
∂x

(S∗

(cid:12)
(cid:12)
0 (γ))
(cid:12)
(cid:12)

≥

1
2

· α(S∗

0 (γ) − θ0)α−1.

Next we compute a uniform bound on the second derivative term.

• If α > 1

By Lemma 4, for ∀S ∈ [S∗

0 (γ) + ∆(γ)], it holds that

0 (γ), S∗
∂2 log Pθ0(x)
∂x2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(S)
(cid:12)
(cid:12)

≤

α3(α − 1)(S − θ0)3α−2
(α(S − θ0)α − (α − 1))2 .

(44)

(45)

– If α ≥ 2, we can pick up γ(cid:48)
2

2), the function in the right-hand
side of equation (45) is monotonically increasing. Thus for every γ > γ(cid:48)
2

such that for every S > S∗

, it holds that

0 (γ(cid:48)

max

S∈[S∗

0 (γ),S∗

0 (γ)+∆(γ)]

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂2 log Pθ0(x)
∂x2

(cid:12)
(cid:12)
(S)
(cid:12)
(cid:12)

≤

α3|α − 1|(S∗
(α(S∗

0 (γ) + ∆(γ) − θ0)3α−2
0 (γ) + ∆(γ) − θ0)α − (α − 1))2
0 (γ) + ∆(γ) − θ0)α−2.
0 (γ) + ∆(γ) ≥ S∗

∼ α|α − 1|(S∗

0 (γ) ≥ S∗

0 (γ2),

Thus there exists γ2 ≥ γ(cid:48)
2
and it holds that

such that for every γ ≥ γ2, S∗

max

S∈[S∗

0 (γ),S∗

0 (γ)+∆(γ)]

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂2 log Pθ0(x)
∂x2

(cid:12)
(cid:12)
(S)
(cid:12)
(cid:12)

≤ 2α|α − 1|(S∗

0 (γ) + ∆(γ) − θ0)α−2.

(46)

– If α ∈ (1, 2), we can pick up γ(cid:48)
2

such that for every S > S∗

right-hand side of equation (45) is monotonically decreasing. Thus for every γ > γ(cid:48)
2
holds that

0 (γ(cid:48)

2), the function in the
, it

max

S∈[S∗

0 (γ),S∗

0 (γ)+∆(γ)]

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂2 log Pθ0(x)
∂x2

(S)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

α3|α − 1|(S∗
(α(S∗

0 (γ) − θ0)3α−2
0 (γ) − θ0)α − (α − 1))2
0 (γ) − θ0)α−2.

∼ α|α − 1|(S∗

Thus there exists γ2 ≥ γ(cid:48)
2

such that for every γ ≥ γ2, S∗

0 (γ) ≥ S∗

0 (γ2), and it holds that

max

S∈[S∗

0 (γ),S∗

0 (γ)+∆(γ)]

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂2 log Pθ0(x)
∂x2

(S)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ 2α|α − 1|(S∗

0 (γ) − θ0)α−2.

(47)

• If α ∈ (cid:2) 1

2 , 1(cid:3)

By Lemma 4, for ∀S ∈ [S∗
(cid:12)
(cid:12)
(cid:12)
(cid:12)

0 (γ), S∗
∂2 log Pθ(x)
∂x2

0 (γ) + ∆(γ)], it holds that

(cid:12)
(cid:12)
(S)
(cid:12)
(cid:12)

≤ α|α − 1|(S − θ)α−2

∼ α|α − 1|(S∗

0 (γ) − θ)α−2.

Thus there exists γ2 such that for every γ ≥ γ2, S∗

0 (γ) ≥ S∗

0 (γ2), (47) holds.

31

Therefore, for any α ≥ 1
2

, there exists γ2 such that for any γ ≥ γ2, it holds that

max

S∈[S∗

0 (γ),S∗

0 (γ)+∆(γ)]

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂2 log Pθ0(x)
∂x2

(cid:12)
(cid:12)
(S)
(cid:12)
(cid:12)

≤ 2α|α − 1| max {(S∗

0 (γ) − θ0)α−2, (S∗

0 (γ) + ∆(γ) − θ0)α−2}.
(48)

As γ → 1, as long as ∆(γ) = S∗
exists γ3 such that for every γ ≥ γ3, it holds that

0 (γ) + ∆(γ) − S∗

0 (γ) = o

(cid:18)(cid:16)

log 1
1−γ

(cid:19)

(cid:17) 1
α

, according to Lemma 5, there

∆(γ) ≤

(cid:18)

log

1
2

1
1 − γ

(cid:19) 1
α

≤ S∗

0 (γ) − θ0,

hence S∗
(48), we have as γ ≥ max {γ1, γ2, γ3}, it holds that

0 (γ) + ∆(γ) − θ0 ≤ 2(S∗

0 (γ) − θ0 ≤ S∗

0 (γ) − θ0) holds as γ ≥ γ3. Combining with (44) and

maxS∈[S∗

0 (γ)+∆(γ)]

0 (γ),S∗
(cid:12)
(cid:12)
(cid:12)

∂ log Pθ0 (x)
∂x

(cid:12)
(cid:12)
(cid:12)
(S∗

∂2 log Pθ0 (x)
∂x2
(cid:12)
(cid:12)
0 (γ))
(cid:12)

(cid:12)
(cid:12)
(S)
(cid:12)

≤

max {2, 2α−1}α|α − 1|(S∗
1
2 · α(S∗

0 (γ) − θ0)α−1

0 (γ) − θ0)α−2

≤

2α+2|α − 1|
S∗
0 (γ) − θ0

.

Notice that S∗
0 (γ) − θ0 ∼
exists γ0, γ0 ≥ max {γ1, γ2, γ3} such that

log 1
1−γ

(cid:16)

(cid:17) 1

α while ∆(γ) = o

(cid:18)(cid:16)

(cid:17) 1
α

log 1
1−γ

(cid:19)

, thus for any c0, c0 > 0, there

2α|α − 1| · ∆(γ)
S∗
0 (γ) − θ0

≤ c0.

Therefore for every γ ≥ γ0, it holds that

max

S∈[S∗

0 (γ),S∗

0 (γ)+∆(γ)]

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂2 log Pθ0(x)
∂x2

(cid:12)
(cid:12)
(S)
(cid:12)
(cid:12)

∆(γ)2 < c0 ·

(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂ log Pθ0(x)
∂x

(S∗

(cid:12)
(cid:12)
0 (γ))
(cid:12)
(cid:12)

∆(γ),

which combined with Lemma 4 and the Taylor Expansion equation (43) leads to the equation
(42).

Now we are well-prepared to prove Proposition 1.

Proposition 1 (Phase transition for the stopping time).
If (cid:15) : [0, 1] → R+ satisﬁes lim inf γ→1 (cid:15)(γ)/(cid:15)critical(γ) > 1, then for every θ ∈ R,

Eθ [τ ∗(θ + (cid:15)(γ))] = ω

(cid:18) 1

1 − γ

(cid:19)

.

If (cid:15) : [0, 1] → R+ satisﬁes lim supγ→1 (cid:15)(γ)/(cid:15)critical(γ) < 1, then for every θ ∈ R,

Eθ [τ ∗(θ + (cid:15)(γ))] = o

(cid:18) 1

1 − γ

(cid:19)

.

32

Proof of ﬁrst claim. Without loss of generality, we assume that (cid:15)(γ) satisﬁes (18), i.e., (cid:15)(γ) =

. By Corollary 3, for any constant c1 > 0, there exists γ0 such that for every γ ≥ γ0,

(cid:19)

(cid:17) 1
α

(cid:18)(cid:16)

o

log 1
1−γ
it holds that

∆(γ) := S∗(θ0 + (cid:15), γ) − S∗

0 (γ) ≥

(cid:16)

1 +

(cid:17)

·

c1
2

log log 1
1−γ
(cid:17)1− 1
(cid:16)
α

log 1
1−γ

α

and

∆(γ) ≤

3
2

(cid:15)(γ).

Notice that by Lemma 3, Lemma 5 and Corollary 2, it holds that

log (S∗

0 (γ) · gθ0(S∗
hθ0(S∗
0 (γ))

0 (γ)))

∼

log log 1
1−γ
(cid:17)1− 1
(cid:16)
α

log 1
1−γ

,

α

thus there exists γ(cid:48)

0 ≥ γ0 such that for every γ ≥ γ(cid:48)
0
0 (γ) · gθ0(S∗
hθ0(S∗
0 (γ))

log (S∗

c1
4

1 +

(cid:17)

(cid:16)

·

, it holds that

0 (γ)))

≤ ∆(γ) ≤

3
2

(cid:15)(γ).

(49)

(50)

By ﬁrst-order approximation Lemma 8, there exists γ(cid:48)

1 ≥ γ(cid:48)
0

such that for every γ ≥ γ(cid:48)
1

, it holds

that

Recall that

hence

Pθ0(S∗

0 (γ))

Pθ0(S∗(θ0 + (cid:15)(γ), γ))

≥ (S∗

0 (γ) · gθ0(S∗

0 (γ)))1+ c1
8 ,

Pθ0(S∗

0 (γ)) =

S∗
0 (γ)gθ0(S∗
0 (γ)) · (1 − γ)
γ

.

Pθ0(S∗(θ0 + (cid:15)(γ), γ)) ≤

1 − γ
γ

· (S∗

0 (γ) · gθ0(S∗

0 (γ)))− c1
8 .

By Corollary 2, we know that S∗
exists γ1 ≥ γ(cid:48)
1

0 (γ)) ∼ α log 1
1−γ
such that for every γ ≥ γ1, it holds that Pθ0(S∗(θ0 + (cid:15)(γ), γ)) ≤ m0(1 − γ).

, thus for any constant m0 > 0, there

0 (γ) · gθ0(S∗

Proof of second claim. By Corollary 3, for any constant c2 > 0, there exists γ0 such that for every
γ ≥ γ0, it holds that

∆(γ) ≤

(cid:16)

1 −

(cid:17)

·

c2
2

α

log log 1
1−γ
(cid:17)1− 1
(cid:16)
α

log 1
1−γ

.

By equation (49), there exists γ(cid:48)

0 ≥ γ0 such that for every γ ≥ γ(cid:48)
0
0 (γ) · gθ0(S∗
hθ0(S∗
0 (γ))

log (S∗

c2
4

1 −

(cid:16)

(cid:17)

·

∆(γ) ≤

0 (γ)))

, it holds that

(51)

By ﬁrst-order approximation Lemma 8, there exists γ(cid:48)

2 ≥ γ(cid:48)
0

such that for every γ ≥ γ(cid:48)
2

, it holds that

Pθ0(S∗

0 (γ))

Pθ0(S∗(θ0 + (cid:15)(γ), γ))

≤ (S∗

0 (γ) · gθ0(S∗

0 (γ)))1− c2
8 ,

33

thus

Pθ0(S∗(θ0 + (cid:15)(γ), γ)) ≥

1 − γ
γ

· (S∗

0 (γ) · gθ0(S∗

0 (γ)))

c2
8 .

Thus for any M0 > 0, there exists γ2 ≥ γ(cid:48)
2
(cid:15)(γ), γ)) ≥ M0(1 − γ).

such that for every γ ≥ γ2, it holds that Pθ0(S∗(θ0 +

F Proof of Proposition 2

In this section we give the proof of Proposition 2. We ﬁrst reformulate the Bellman Equation (3)
through lens of tailed probability and conditional expectation.

Lemma 9. The optimal threshold S∗(F, γ) to the Bellman Equation (3) satisﬁes:

P0(S∗(F0, γ)) =

1 − γ
γ

·

S∗(F0, γ)
µ0(S∗(F0, γ)) − S∗(F0, γ)

.

Proof. We reformulate the Bellman Equation (3) as

1
γ

S∗(F0, γ) =

1
1 − γ

(cid:90) +∞

S∗(F0,γ)

P0(x)dx.

. By Lemma 2, we obtain the equation (52).

Recall g(S) :=

P (S)

(cid:82) +∞
S

P (x)dx

(52)

(53)

By Lemma 9, we immediately obtain the following corollary if the underlying distribution F0

satisﬁes µ0(S) ∼ S.

Corollary 4. If distribution F0 satisﬁes (33), then it holds that

P0(S∗(F0, γ)) = ω (1 − γ) .

Now we are well prepared to prove Proposition 2. We restate it as follows:

Proposition 2. Suppose F0 and {Fγ}0<γ<1 satisﬁes

as S → +∞ and Bellman thresholds S∗(Fγ, γ) satisfy

µ0(S) ∼ S

S∗(Fγ, γ) ∼ S∗(F0, γ)

as γ → 1.

• If P0(S∗(Fγ, γ)) = ω(1 − γ), then

• If P0(S∗(Fγ, γ)) = o(1 − γ), then

R(F0, γ, τ ∗(Fγ)) = 0.

lim
γ→1

R(F0, γ, τ ∗(Fγ)) = 1.

lim
γ→1

34

(54)

(33)

(34)

Proof. For any threshold value S, the expected discounted reward of the corresponding threshold
policy (2) performed on the ground truth F0 is

µ0(S) · P0(S)
1 − γ(1 − P0(S))

,

thus the regret of Bellman policy τ ∗(Fγ) is

R(F0, γ, τ ∗(Fγ)) = 1 −

µ0(S∗(Fγ, γ))
µ0(S∗(F0, γ))

·

P0(S∗(Fγ, γ))
P0(S∗(F0, γ))

·

(1 − γ) + γP0(S∗(F0, γ))
(1 − γ) + γP0(S∗(Fγ, γ))

.

By (33) and (34), it holds that

µ0(S∗(Fγ, γ))
µ0(S∗(F0, γ))

−→ 1

as γ → 1. Meanwhile, by Corollary 4, it holds that P0(S∗(F0, γ)) = ω(1 − γ), thus

P0(S∗(Fγ, γ))
P0(S∗(F0, γ))

·

(1 − γ) + γP0(S∗(F0, γ))
(1 − γ) + γP0(S∗(Fγ, γ))

∼

P0(S∗(Fγ, γ))
(1 − γ) + γP0(S∗(Fγ, γ))

.

(55)

(56)

(57)

Therefore, if P0(S∗(Fγ, γ)) = ω(1 − γ), then limγ→1 R(F0, γ, τ ∗(Fγ)) = 0; if P0(S∗(Fγ, γ)) = o(1 − γ),
then limγ→1 R(F0, γ, τ ∗(Fγ)) = 1.

35

