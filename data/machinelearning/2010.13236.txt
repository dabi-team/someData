Automatic Learning of Topological Phase Boundaries

Alexander Kerr,1, 2, ∗ Geo Jose,1, 2 Colin Riggert,1, 2 and Kieran Mullen1, 2
1Homer L. Dodge Department of Physics and Astronomy, The University of Oklahoma,
440 W. Brooks St., Norman, OK 73019, USA
2Center for Quantum Research and Technology, The University of Oklahoma,
440 W. Brooks Street, Norman, Oklahoma 73019, USA
(Dated: October 27, 2020)

Topological phase transitions, which do not adhere to Landau’s phenomenological model (i.e. a
spontaneous symmetry breaking process and vanishing local order parameters) have been actively
researched in condensed matter physics. Machine learning of topological phase transitions has gener-
ally proved diﬃcult due to the global nature of the topological indices. Only recently has the method
of diﬀusion maps been shown to be eﬀective at identifying changes in topological order. However,
previous diﬀusion map results required adjustments of two hyperparameters: a data length-scale and
the number of phase boundaries. In this article we introduce a heuristic that requires no such tun-
ing. This heuristic allows computer programs to locate appropriate hyperparameters without user
input. We demonstrate this method’s eﬃcacy by drawing remarkably accurate phase diagrams in
three physical models: the Haldane model of graphene, a generalization of the Su-Schreiﬀer-Haeger
(SSH) model, and a model for a quantum ring with tunnel junctions. These diagrams are drawn,
without human intervention, from a supplied range of model parameters.

I.

INTRODUCTION

Topological quantum phase transitions (TQPTs) are
transitions between quantum states of matter with dif-
ferent topological properties, typically indicated by a
discontinuous change in a topological invariant like the
Chern number. TQPTs diﬀer from classical and quan-
tum phase transitions in that the former are described
by Landau theory, based on the emergence of a local
order parameter that is usually associated with a bro-
ken symmetry at the phase transition [1–3]. This or-
der parameter is non-zero in the ordered phase and zero
in the disordered phase. These ideas generalize to the
case of quantum phase transitions, wherein the Landau
functional is replaced by the action of the underlying
quantum ﬁeld theory. TQPTs do not conform to this
paradigm: the distinction between diﬀerent phases is not
a broken symmetry, but rather the value of a topolog-
ical index. These indices are fundamentally associated
with global features of the quantum state and are not
naturally described by local order parameters. The gen-
eral program of study for topological phase transitions
in a given model with a ﬁnite number of tunable param-
eters involves computing the desired topological index
as a function of the parameters and identifying regions
in the parameter space that pertain to diﬀerent indices.
This requires prior knowledge of the relevant topologi-
cal invariant. Here we present an unsupervised machine
learning algorithm that eﬃciently maps the boundary of
a TQPT without any foreknowledge of the underlying
topological invariant.

Machine learning programs have been previously es-
tablished in physics using neural networks as a varia-

∗ E-mail corresponding author at: ajkerr0@gmail.com

tional ansatz for many-body systems [4–6], modeling po-
tential energy surfaces with near ab-initio accuracy [7–
10], improved Monte-Carlo sampling [11, 12], and beyond
[13, 14]. Within the ﬁeld of condensed matter physics,
the identiﬁcation of phases of matter with machine learn-
ing has become an active area of research [15–18]. Clas-
sifying phases of matter (a supervised learning process)
has been demonstrated with neural networks [15, 16].
Perhaps more interestingly, other algorithms are known
to learn order parameters in an unsupervised manner
[17, 18], meaning the identity of each phase was not ex-
plicitly labeled for the algorithm a priori. For example,
computer programs were able to identify the magnetiza-
tion of a spin lattice as the key feature deﬁning the phase
transition by simply analyzing sampled spin conﬁgura-
tions without incorporating any model Hamiltonian or
parameters into the code.

Recently, unsupervised methods have distinguished
diﬀerent topological phases of matter via manifold learn-
ing [19, 20]. The principle behind using manifold learn-
ing is that diﬀerent states sharing a global property may
have a large Euclidean separation (L2 norm of the dif-
ference between their local variables) while lying on the
same manifold in phase space. Manifold learning tools
are designed to correlate data points living on the same
manifold by a non-linear manipulation of the data.
In
this article we demonstrate the ability of a particular
manifold learning routine, diﬀusion maps, to learn the
boundaries of topological phase transitions. Previous im-
plementations of diﬀusion maps [19, 20] could only accu-
rately draw phase diagrams after the number of bound-
aries were speciﬁed. We present a heuristic in which the
number of phase boundaries in a cross-section of param-
eter space is automatically determined by a computer
program. From this, phase diagrams are drawn with-
out human input as a set of Hamiltonian parameters are
swept.

0
2
0
2

t
c
O
5
2

]
l
l
a
h
-
s
e
m

.
t
a
m
-
d
n
o
c
[

1
v
6
3
2
3
1
.

0
1
0
2
:
v
i
X
r
a

 
 
 
 
 
 
We consider three cases. The ﬁrst is an experiment
which illustrates how diﬀusion maps can identify classi-
cal XY spins of diﬀerent winding numbers. The second
is applying the new heuristic to draw two well-known
phase diagrams: an extension of the Su-Schreiﬀer-Haeger
(SSH) model [21] which exhibits a phase transition be-
tween phases labeled by diﬀerent winding numbers as the
hopping parameters are varied, and the Haldane model
of graphene [22] which involves a transition in the Chern
number as a function of the hopping parameters and
magnetic phases. The third example is to investigate
a novel topological phase transition in the ground state
of a single electron on a set of ideal, tunneling coupled
rings. As the tunneling between the rings is increased,
the winding number of the single particle ground state
can change abruptly.

Gaussian similarity measure

Pij =

Kij
j Kij

(cid:80)

(cid:32)

Kij = exp

−

(cid:33)

|xi − xj|2
p
(cid:15)

2

(1)

(2)

where xi is the ith data point in the given parameteri-
zation, (cid:15) is a resolution hyperparameter setting a length
scale, and | · |p denotes the Lp norm. Note that while
a data point xi is multi-dimensional in general, the ma-
trix Pij has dimensions N × N where N is the number
of points in the dataset. These walkers are likely to hop
between nearest-neighbors while jumps across large dis-
tances are exponentially suppressed. We introduce the
eigenvalues λk and eigenvectors ψk of Pij deﬁned by

II. AUTOMATIC PHASE DIAGRAM METHOD

Pijψk = λkψk

(3)

A. Diﬀusion Maps

Conventional clustering methods such as k-means [23]
are not enough to discriminate the global indices of data.
The objective of k-means in particular is to minimize
the squared Euclidean distance between data points and
their cluster centers which inherently depends on local
In general, entries in a dataset may be
information.
well separated in parameter space despite sharing global
properties. An example is given in Figure 1(A): a se-
ries of classical XY spins on a 1D lattice. A dataset is
constructed containing spin-arrays with winding numbers
ν ∈ {0, 1, 2}. The dot product between two spin-arrays
with the same winding number can be small (or very
negative) even though they live on the same manifold in
parameter space corresponding to their global property.
Therefore, the distance between two points is not enough
information to determine if they contain the same topo-
logical index. A naive k-means analysis (Figure 1(B))
fails to cluster the pure spin-data based on their winding
number. Meanwhile linear transformations, e.g. princi-
pal components analysis [23, 24], cannot reduce this data
down to its topological invariants. The utility of mani-
fold methods, like diﬀusion maps, is to cluster data in a
non-linear manner.

Although data points on the same manifold (i.e. a
given winding number) can be well separated, adjacent
points typically lie on the same manifold. Chains of
locally-similar data can form globally-similar structures
across large regions in the multi-dimensional phase space.
If allowed to jump across nearest-neighboring points, one
could quickly diﬀuse through a single manifold. From the
speed of this diﬀusion, a new distance metric can be for-
mulated which will separate data in a Euclidean manner.
Imagine a set of random walkers allowed to diﬀuse
through a dataset. The transition probability of a walker
from data point xi to point xj (Pij) is the row-normalized

The diﬀusion distance D(xi, xj) between any two points
xi and xj in the data set after t time steps is deﬁned by
[25]

D2

t (xi, xj) =

(cid:88)

k

k [ψk(xi) − ψk(xj)]2
λ2t

(4)

where ψk(xi) is the ith component of the kth eigenvec-
tor. Data points separated by a small diﬀusion distance
Dt are understood to belong to the same manifold be-
cause Dt is small when there are many paths comprised
of short-ranged hops connecting them. Thus, the diﬀu-
sion distance is a useful metric for the global-similarity
of data points and is used to delineate manifolds. Dif-
fusion distance is not typically calculated explicitly how-
ever. Instead, the original data set {xi} (Figure 1(A)) is
transformed:
yi = (cid:8)λt

N −1ψN −1(xi)(cid:9)

1ψ1(xi), ... , λt

0ψ0(xi), λt

(5)

where the eigenvalues are in descending order. With this
transformed data {yi}, more routine data analysis can
be applied. This is because the Euclidean distance be-
tween the transformed points corresponds to the diﬀu-
sion distance between the original entries [25]. Another
nice feature of this process is that the distance metric
in Equation 2 can be generalized. A physical picture for
why diﬀusion maps work is to interpret Pij as the dynam-
ical matrix of a set of point masses connected by springs
with a stiﬀness proportional to Kij. A data manifold
corresponds to an eigenmode that moves in phase due to
their strong interconnections.

Because the transformation in Equation 5 preserves
the manifold information of the original dataset, it is un-
derstood that only a few of the largest λk contribute to
the calculation of Dt(xi, xj). The exact number of sur-
viving eigenvalues is determined by the connectivity of
the data which in turn depends on the resolution hyper-
parameter in Equation 2. Figure 1(C) shows the largest

3

FIG. 1. (A) A dataset is composed of classical XY spins with diﬀerent winding numbers. Points on a single manifold in
parameter space can have a large pairwise separation, but a small diﬀusion separation as deﬁned in Equation 4. (B) The XY
spins projected to two-dimensions. The data is plotted as a function of the angles in the ﬁrst and last lattice sites. The data is
stratiﬁed because of the presence of spin winding. However, k-means clustering (color code) does not recognize the topological
order of the full-dimensional dataset. (C) The 20 largest eigenvalues of the transition matrix Pij for the data in Figure (A),
as a function of the resolution parameter (cid:15) in Equation 2 (note the semi-log scale). The time scale here was chosen to be
t = 500. Pij is a probability matrix, and thus its eigenvalues (λk) vary between 0 and 1. The number of eigenvalues of order
unity roughly correspond to the number of manifolds in the data. Three eigenvalues survive for a large range of (cid:15), indicating
there are truly three sets of topologically distinct data. The inset shows the eigenvalues for (cid:15) = 0.8. (D) The similarity matrix
of the data for (cid:15) = 0.8, derived from Equation 2 using the L∞ norm. Much can be learned from visual inspection: 1) there
are 3 manifolds in the data 2) the manifolds are well separated with weak inter-connections and strong intra-connections 3)
each manifold is the same size 4) the manifolds have the same density. (E) The dataset transformed to two dimensions by
diﬀusion maps. The distribution of the data along the zeroth and ﬁrst diﬀusion modes are shown. Clearly the spin data in
diﬀusion space is clustered according to their winding numbers, and k-means analysis automatically relates states of the same
topological index. The inter-cluster distance is much larger than the intra-cluster distance.

eigenvalues corresponding to the XY spin data as a func-
tion of the resolution. When (cid:15) is small, there are many
surviving eigenvalues attributed to the large eﬀective sep-
aration between the data points. When this scale is small
enough, individual data points become isolated and reg-
ister as their own manifold. As (cid:15) is increased to the inher-
ent length scale of the data, the true structure is elicited
by the number of long-surviving eigenvalues. When (cid:15) is
tuned very high, all but one eigenvalue vanish due to the
entire dataset appearing as a single tightly-bound cluster.
Indeed, just three eigenvalues remain across a large range
of resolution values hinting that the data is truly struc-
tured in three separate manifolds. The similarity matrix
for a select resolution is shown in Figure 1(D), contain-
ing even more information about the connectivity of the
data. The L∞ norm was used to derive this similarity
(Equation 2) due to its ability to encode topological in-
formation [20]. The manifold form of the data can be

visualized in diﬀusion space (Figure 1(E)). Since the dif-
fusion distance of the original data is roughly equated
to the Euclidean distance between the transformed data
points, the transformed data appears in three tightly-
bound clusters. Finally, k-means clustering can be ap-
plied to the diﬀusion modes to identify topological struc-
ture. If the data in a set is ordered, then changes in the
k-means cluster labels are interpreted as a phase bound-
ary.

B. Optimization of Resolution Hyperparameter

Before diﬀusion maps can ﬁnd phase boundaries in
physical models, the resolution (cid:15) of the data in Equa-
tion 2 must be chosen. Conventionally this is performed
by inspecting the probability eigenspectrum as a func-
tion of (cid:15) (as in Figure 1(B)) and selecting a value for (cid:15)

that captures the manifold structure of the data. The
degeneracy of eigenvalues close to unity corresponds to
the number of manifolds. However, this process is ill-
deﬁned as it is generally unclear which resolution scale
“best” illustrates the global arrangement of the data. In
addition, this process must precede every experiment.

We introduce a heuristic that allows computer pro-
grams to automatically choose a resolution hyperparam-
eter, without user input. We seek the resolution that
minimizes an adjusted mean squared distance between
the similarity matrix in Equation 2 (K((cid:15))) and the “ideal”
similarity matrix (Kideal):

MSE(n, (cid:15)) =

n − 1
n

n
(cid:88)

(cid:96)=1

1
|S(cid:96)|

(cid:88)

(cid:88)

(cid:0)Kij((cid:15)) − Kideal

ij

(cid:1)2

i∈S(cid:96)

j

Kideal

ij =




1,

i, j are contained in identical
k-means clusters



0, otherwise

(6)

(7)

where n is the number of k-means clusters and S(cid:96) is the
set of points in cluster (cid:96). We are deﬁning the ideal simi-
larity matrix as having a zero similarity between points of
diﬀerent clusters, and an intra-cluster similarity of one.
The cluster assignments for the ideal matrix are deter-
mined by a k-means clustering application [23] on the
data in diﬀusion space. With the form given in Equation
6, every cluster carries the same weight in the squared
diﬀerence between the target and functional similarity
matrices, no matter the relative sizes of the clusters. The
factor of (n − 1)/n cancels out some of the natural ad-
vantage of assigning a large number of clusters to the
MSE.

Figure 2(A) shows the adjusted MSE of the XY spin
data as a function of the resolution hyperparameter
(n = 3) and has a clearly deﬁned optimum. Minimiz-
ing the MSE as a function of (cid:15) is a routine task for a
computer. Note this value for (cid:15) in general exceeds the
value that corresponds to the expected number of de-
generate eigenvalues close to one. The objective of the
heuristic is to make the (normally sparse) similarity ma-
trix very dense. Figure 2(B) features a smeared similarity
when the MSE is minimized. The ideal similarity is dis-
played in Figure 2(C) for reference. In addition to the
resolution hyperparameter, the ideal number of k-means
clusters can be automatically determined by the smallest
optimized MSE as a function of n. Figure 2(D) features
the MSE for nclusters = {2, 3, 4}. The error is extremized
for n = 3; this is expected from the three distinct winding
numbers present in the data. Once this heuristic is es-
tablished, a computer program can eﬃciently determine
both the resolution and number of k-means clusters. This
process is aided by the fact that there are natural bounds
for the scale of (cid:15) related to the shortest and largest dis-
tance between each of the points in the original dataset.

The ﬁnal result is a fully programmatic approach to per-
form diﬀusion maps on quantum states. More details are
given in Section III.

4

III. RESULTS

A. Haldane Model

We ﬁrst test our procedure on the Haldane model of
graphene, a two dimensional array of carbon atoms in a
honeycomb lattice with nearest and next nearest neigh-
bor hopping [22]. This model realizes the integer quan-
tum Hall eﬀect in graphene in the absence of a Landau
level spectrum by adding a magnetic phase to the second
nearest neighbor hopping parameters such that the net
ﬂux per plaquette is zero. The Haldane Hamiltonian in
momentum space is [22]

H(k) = d(k) · σ
dx(k) = cos(k · a1) + cos(k · a2) + 1
dy(k) = sin(k · a1) + sin(k · a2)

dz(k) = M + 2t2 sin(φ)[ sin(k · a1) − sin(k · a2)

− sin(k · (a1 − a2))]

(8)
(9)
(10)

(11)

which is the Bloch vector form. Here a1, a2 are the 2D
lattice vectors, t2 is the next-nearest neighbor hopping
parameter, M is an onsite energy term that breaks in-
version symmetry, and φ is the magnetic phase associated
with second nearest neighbor hopping. In general there
is a nearest neighbor hopping term (t1), but here it is set
to unity. The Hamiltonian parameters of interest are M
and φ.

The topological invariant associated with the integer
quantum Hall eﬀect is the Chern number [26]. For a
Hamiltonian of the form in Equation 8 at half ﬁlling, the
Hall conductivity is given by σxy = e2
h C where C is the
Chern number deﬁned as

C =

(cid:90) (cid:90)

1
2π

BZ

dkxdkyFxy(k)

(12)

ˆdb∂j

ˆda∂i

2 (cid:15)abc

ˆdc and
with Berry curvature Fxy(k) = 1
ˆda = da/|d|. This integral is over the entire ﬁrst Bril-
louin zone (BZ) which is spanned by the reciprocal lat-
tice vectors. The Chern number can be interpreted as
the winding number of the map k → ˆd from the BZ to a
2-sphere and is always an integer. It is well understood
[22, 27–30] that as M and φ are varied, the Haldane
Hamiltonian undergoes phase transitions and can realize
Chern numbers of 0, -1, and 1 (solid line in Figure 3).

To test the heuristic’s ability to draw the Haldane
phase diagram, a dataset is composed of Bloch vectors
from Equation 8. A single data point xi is a normal-
ized Bloch vector ˆd = d/|d| deﬁned across the two-
dimensional BZ mesh. For a given value of φ we build

5

FIG. 3. The learned phase boundaries (black dots) matches
that of the known phase boundaries (black line) for the Hal-
dane model. The learned boundaries are in locations where
the k-means labels of the Bloch vectors transformed by dif-
fusion maps change. Matter phases are identiﬁed by their
Chern number and are color-coded here for extra visibility.

Bloch vectors corresponding to a vertical slice of a phase
diagram (Figure 3). The diﬀusion map resolution param-
eter and number of k-means centroids for a particular
sample is determined via the procedure detailed in Sec-
tion II B. K-means analysis automatically assigns cluster
labels to each point of the transformed data {yi}. Each
of these clusters are understood to correspond to diﬀer-
ent topological phases. Because the original dataset is
composed of Bloch vectors that were sampled in an or-
dered manner (i.e. by monotonically increasing M ), the
boundary between k-means clusters is interpreted as a
Haldane phase transition. On a phase diagram, the loca-
tion in which the k-means labels change corresponds to
the phase boundary. This process is repeated for many
choices of φ. In Figure 3 we show a comparison between
the boundary discovered by machine learning and that
predicted by theory. Without prior training, the algo-
rithm can accurately determine the boundaries in the
multi-phase system.

B. Extended SSH Model

Next we look at a generalized version of the Su-
Schrieﬀer-Heeger (SSH) model describing electrons trav-
eling across a 1D lattice with staggered lattice sites and
next-next-nearest neighbor hopping (Figure 4). Like the
Haldane model, the Hamiltonian in momentum space can
be written in Bloch vector notation [31]:

dx(k) = t1 + t2 cos(k) + t3 cos(2k)
dy(k) = t2 sin(k) + t3 sin(2k)
dz(k) = 0

(13)

FIG. 2. (A) The augmented MSE [black] from Equation 6 of
the spin data in Figure 1(A) as a function of resolution given 3
clusters. Also plotted is the ﬁve largest transition probability
eigenvalues [red] (Equation 3). The minimum of the MSE can
be found with standard computer routines, and allows a pro-
gram to automatically select (cid:15) without an experimenter choos-
ing the hyperparameter. (B) The similarity matrix when the
MSE is minimized. The resolution parameter is much larger
here than in Figure 1(C), resulting in a more uniform matrix.
(C) The ‘ideal’ similarity which the heuristic tries to match.
(D) The error for diﬀerent numbers of k-means centroids on
a log-log scale. The error reaches its smallest possible value
for n = 3 and (cid:15) = 18.2. Based on the heuristic, those are the
values with which diﬀusion maps is performed. The lack of
discontinuities in the n = 3 curve may also be evidence for
the proper number of cluster centers. The locations of the
centroids smoothly change for n = 3, suggesting the data is
inherently triply clustered in diﬀusion space.

a dataset by uniformly sweeping M ∈ [−7, 7] in 1000
points. The set {xi} is then a collection of normalized

6

FIG. 6. A schematic of the triple-ring junction, which can be
understood as a single ring wrapped into the ‘trefoil’ pattern.
At points where the wires cross there is a δ-function coupling
that allows an electron to tunnel from one loop to the next.

C. Triple Junction Quantum Ring Array - An
Esoteric Case

Lastly we investigate a system where a topologi-
cal phase transition is known only through numeri-
cal analysis:
single electrons on knotted systems of
tunneling-coupled one-dimensional quantum rings. One-
dimensional quantum wires can be wrapped to form rings
in several topologically distinct ways. One such conﬁg-
uration is a single self-connected wire wrapped into a
trefoil knot (Figure 6). If one allows δ-function tunnel
coupling where the wire crosses itself, interesting eﬀects
of frustration and topology appear [32]. These eﬀects
are made richer by applying a magnetic ﬁeld such that
the tunnel-coupling matrix elements pick up a complex
Aharonov-Bohm phase factor.

Experimentally, this phase factor can be changed by
adjusting the strength of an applied magnetic ﬁeld. As
this phase is swept from −π to π, it becomes energeti-
cally favorable for the wavefunction to accommodate this
magnetic phase “twist” by changing how the phase of the
wavefunction itself winds about the knot in real space.
This manifests in a spontaneous change in the winding
number of the ground state wavefunction, deﬁned as

where

n = −

(cid:90)

i
2π

z∗ ∂z
∂s

ds

z =

ψ
|ψ|

(15)

(16)

The exact coupling phase where this transition occurs is
dependent on the magnitude of the tunnel coupling.

In Figure 7 we show the phase boundary determined by
two numerical approaches. The ﬁrst is simply to explic-
itly calculate the phase winding in Equation 15 from the
numerically calculated wavefunctions, for diﬀerent values
of coupling strength and magnetic ﬁeld. The second is to
use the diﬀusion map heuristic where each data point xi
is the ground state wavefunction normalized so that the

FIG. 4. SSH Model with next-next-nearest neighbor hopping
enabled, corresponding to the Hamiltonian of Equation 13.

FIG. 5. A) Learned phase diagram of the standard SSH model
(t3 = 0), matching theory to high precision. (B) When next-
next-nearest hopping is allowed (t3 (cid:54)= 0, t1 = 1), the heuristic
is markedly less accurate but still close to the ground truth
diagram. Topological phases are identiﬁed even when they
occupy a small volume of parameter space. The ability of the
heuristic to draw several boundaries simultaneously is also
demonstrated.

When t3 = 0 this reduces to the standard SSH model.
Phases of the SSH model diﬀer by their winding number

ν =

1
2π

(cid:90) π

(cid:18)

ˆd ×

dk

−π

d
dk

(cid:19)

ˆd

z

(14)

Again, the machine learning heuristic is tested against
the known phase diagram (Figure 5) by building a
dataset through ordered changes in the model parame-
ters. Each data point xi is the normalized Bloch vector d
but now deﬁned across 32 points in the one-dimensional
BZ, k ∈ [−π, π]. The automatic phase diagram process is
applied by sweeping the relevant hopping parameters in
Equation 13. In one experiment (Figure 5(A)), datasets
are composed of 1000 Bloch vectors for t2 ∈ [−5, 5] on
a grid of 20 t1 values, while t3 = 0.
In another ex-
periment (Figure 5(B)), 1000 data points are generated
where t3 ∈ [−5, 5] for slices of t2. Here t1 = 1 is ﬁxed. In
replicating the phase diagram of the standard SSH model
(t3 = 0), the heuristic approach displays remarkable ac-
curacy. When long-range hopping is turned on (t3 (cid:54)= 0),
the learning method still captures the true structure of
the phase diagram despite the more complicated features
in the data. Even in regions where there are many phase
boundaries (|t2| < 2) the approach is successful. This
illustrates the heuristic’s ability to identify topological
order in small regions of parameter space.

7

ters. This process was used in models of diﬀerent nature:
two were well-established Hamiltonians deﬁned in mo-
mentum space and the other was a calculated transition
in a topological quantity in a lesser-known system. The
heuristic is successful even when many phase boundaries
are encountered. Furthermore, parts of the presented al-
gorithm can be adjusted such as the distance metric to
determine local similarity. The result is a very general
procedure to draw boundaries for TQPTs and beyond.
This approach could be used to investigate systems in
which there is currently no known topological behavior.
The heuristic presented in this paper does have some
limitations. Firstly, there is no clear way to consider the
absence of a phase transition. This manifests because
the MSE in Equation 6 does not reach a ﬁnite value
for n = 1. Because of this, the program will draw a
phase boundary somewhere simply because it is forced
to. Secondly, it is possible the heuristic chooses an ex-
ceedingly large value for the resolution hyperparameter in
cases where the similarity matrix is otherwise extremely
sparse. As a result, the true phase boundaries may not
be accurately obtained. This is a consequence of trying
to match the similarity matrix with one that has sharply
deﬁned clusters in the objective function, Equation 6.
The resolution hyperparameter is tuned very high to get
similarity values of 1 between many pairs of data points.
A simple diagnostic for this is to examine the sparse-
ness of Kij directly. This is straightforward since even
with multi-dimensional data the similarity matrix is two-
dimensional. If it is exceedingly sparse, alternative ap-
proaches may be necessary.

V. ADDITIONAL NOTE

Code for the diﬀusion map and automatic resolution
determination can be made available following the pub-
lication of this manuscript.

FIG. 7. The phase diagram of the single-electron wavefunc-
tion across a triple-ring junction in Figure 6. As one varies
the real and imaginary parts of the δ-function coupling at the
junctions, the wavefunction features stark shifts of its wind-
ing numbers as deﬁned in Equation 15. The machine learning
method (dotted line) is faithful to the explicit calculation of
the winding number (solid line).

phase at a particular position starts as pure real. For dif-
ferent values of the tunneling amplitude, wavefunctions
across the trefoil were calculated for many values of the
tunneling phase. The diﬀusion map approach identiﬁes
precisely the same phase boundaries even though it does
not have the winding number explicitly coded into its al-
gorithm. This result reemphasizes the heuristic’s ability
to identify changes in generic topological structure.

IV. CONCLUSIONS

Using the method of diﬀusion maps we have used com-
puter learning to draw several very accurate phase dia-
grams of systems undergoing change in topological order.
These diagrams were determined with no human inter-
vention beyond a supplied range of physicals parame-

[1] J. Cardy, Scaling and Renormalization in Statistical
Physics, Cambridge Lecture Notes in Physics (Cam-
bridge University Press, 1996).

[2] J. Zinn-Justin, Quantum Field Theory and Critical Phe-

nomena (Oxford University Press, 2002).

[3] S. Sachdev, Quantum Phase Transitions, 2nd ed. (Cam-

bridge University Press, 2011).

[4] G. Carleo and M. Troyer, Solving the quantum many-
body problem with artiﬁcial neural networks, Science
355, 602 (2017).

[5] H. Saito, Solving the bose–hubbard model with machine
learning, Journal of the Physical Society of Japan 86,
093001 (2017), https://doi.org/10.7566/JPSJ.86.093001.
[6] H. Saito, Method to solve quantum few-body prob-
lems with artiﬁcial neural networks, Journal
of
the Physical Society of Japan 87, 074002 (2018),
https://doi.org/10.7566/JPSJ.87.074002.

[7] V. Botu, R. Batra, J. Chapman, and R. Ramprasad, Ma-
chine learning force ﬁelds: Construction, validation, and
outlook, The Journal of Physical Chemistry C 121, 511
(2017), https://doi.org/10.1021/acs.jpcc.6b10908.

[8] T. Mueller, A. Hernandez, and C. Wang, Machine learn-
ing for interatomic potential models, The Journal of
Chemical Physics 152, 050902 (2020).

[9] H. Chan, B. Narayanan, M. J. Cherukara, F. G. Sen,
K. Sasikumar, S. K. Gray, M. K. Y. Chan, and S. K.
R. S. Sankaranarayanan, Machine learning classical in-
teratomic potentials for molecular dynamics from ﬁrst-
principles training data, The Journal of Physical Chem-
istry C 123, 6941–6957 (2019).

[10] J. Behler, Perspective: Machine learning potentials for
atomistic simulations, The Journal of Chemical Physics
145, 170901 (2016).

[11] J. Liu, Y. Qi, Z. Y. Meng, and L. Fu, Self-learning monte

carlo method, Phys. Rev. B 95, 041101 (2017).

[12] L. Huang and L. Wang, Accelerated monte carlo simula-
tions with restricted boltzmann machines, Phys. Rev. B
95, 035105 (2017).

[13] G. Carleo, I. Cirac, K. Cranmer, L. Daudet, M. Schuld,
N. Tishby, L. Vogt-Maranto, and L. Zdeborov´a, Machine
learning and the physical sciences, Rev. Mod. Phys. 91,
045002 (2019).

[14] E. Bedolla, L. C. Padierna, and R. Casta˜neda-Priego,
Machine learning for condensed matter physics, Journal
of Physics: Condensed Matter (2020).

[15] J. Carrasquilla and R. G. Melko, Machine learning phases

of matter, Nature Physics 13, 431 (2017).

[16] K. Shiina, H. Mori, Y. Okabe, and H. K. Lee, Machine-
learning studies on spin models, Scientiﬁc Reports 10,
2177 (2020).

[17] L. Wang, Discovering phase transitions with unsuper-
vised learning, Physical Review B 94, 195105 (2016).
[18] S. J. Wetzel, Unsupervised learning of phase transitions:
From principal component analysis to variational autoen-
coders, Physical Review E 96, 022140 (2017).

[19] J. F. Rodriguez-Nieva and M. S. Scheurer, Identifying
topological order through unsupervised machine learn-
ing, Nature Physics 15, 790 (2019).

[20] Y. Che, C. Gneiting, T. Liu, and F. Nori, Topologi-
cal quantum phase transitions retrieved from manifold
learning, arXiv:2002.02363 [cond-mat, physics:physics]
(2020), arXiv: 2002.02363.

[21] C. Li and A. E. Miroshnichenko, Extended SSH Model:
Non-Local Couplings and Non-Monotonous Edge States,
Physics 1, 2 (2019).

8

[22] B. A. Bernevig, Topological Insulators and Topological
Superconductors (Princeton University Press, 2013).
[23] C. M. Bishop, Pattern Recognition and Machine Learning
(Information Science and Statistics) (Springer-Verlag,
Berlin, Heidelberg, 2006).

[24] Z. Ivezic, A. J. Connolly, J. T. VanderPlas, and A. Gray,
Statistics, Data Mining, and Machine Learning in As-
tronomy: A Practical Python Guide for the Analysis of
Survey Data (Princeton University Press, USA, 2014).

[25] R. R. Coifman and S. Lafon, Diﬀusion maps, Applied and
Computational Harmonic Analysis 21, 5 (2006), special
Issue: Diﬀusion Maps and Wavelets.

[26] D. J. Thouless, M. Kohmoto, M. P. Nightingale, and
M. den Nijs, Quantized hall conductance in a two-
dimensional periodic potential, Phys. Rev. Lett. 49, 405
(1982).

[27] D. Carpentier, Topology of bands in solids : From insu-

lators to dirac matter (2014), arXiv:1408.1867.

[28] U. Bhattacharya, J. Hutchinson, and A. Dutta, Quench-
ing in chern insulators with satellite dirac points: The
fate of edge states, Phys. Rev. B 95, 144304 (2017).
[29] U. Bhattacharya and A. Dutta, Interconnections between
equilibrium topology and dynamical quantum phase
transitions in a linearly ramped haldane model, Phys.
Rev. B 95, 184307 (2017).

[30] M. D. Caio, G. M¨oller, N. R. Cooper, and M. J. Bhaseen,
Topological marker currents in chern insulators, Nature
Physics 15, 257 (2019).

[31] H.-C. Hsu and T.-W. Chen, Topological anderson insu-
lating phases in the long-range su-schrieﬀer-heeger model
(2020), arXiv:2004.10348.

[32] C. Riggert and K. Mullen, Topological eﬀects

in
tunneling-coupled systems of one-dimensional quantum
rings (2020), arXiv:2007.01967.

