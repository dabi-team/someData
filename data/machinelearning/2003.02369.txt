0
2
0
2

r
a

M
4

]

C
D
.
s
c
[

1
v
9
6
3
2
0
.
3
0
0
2
:
v
i
X
r
a

ORDERING CHAOS: MEMORY-AWARE SCHEDULING OF
IRREGULARLY WIRED NEURAL NETWORKS FOR EDGE DEVICES

Byung Hoon Ahn 1 † Jinwon Lee 2 Jamie Menjay Lin 2 Hsin-Pai Cheng 3 † Jilei Hou 2 Hadi Esmaeilzadeh 1

ABSTRACT
Recent advance on automating machine learning through Neural Architecture Search and Random Network Gener-
ators, has yielded networks that deliver higher accuracy given the same hardware resource constrains, e.g., memory
capacity, bandwidth, number of functional units. Many of these emergent networks; however, comprise of irregular
wirings (connections) that complicate their execution by deviating from the conventional regular patterns of layer,
node connectivity, and computation. The irregularity leads to a new problem space where the schedule and order
of nodes signiﬁcantly affect the activation memory footprint during inference. Concurrently, there is an increasing
general demand to deploy neural models onto resource-constrained edge devices due to efﬁciency, connectivity, and
privacy concerns. To enable such a transition from cloud to edge for the irregularly wired neural networks, we set out
to devise a compiler optimization that caps and minimizes the footprint to the limitations of the edge device. This
optimization is a search for the schedule of the nodes in an intractably large space of possible solutions. We offer
and leverage the insight that partial schedules leads to repeated subpaths for search and use the graph properties to
generate a signature for these repetition. These signatures enable the use of Dynamic Programming as a basis for the
optimization algorithm. However, due to the sheer number of neurons and connections, the search space may remain
prohibitively large. As such, we devise an Adaptive Soft Budgeting technique that during dynamic programming per-
forms a light-weight meta-search to ﬁnd the appropriate memory budget for pruning suboptimal paths. Nonetheless,
schedules from any scheduling algorithm, including ours, is still bound to the topology of the neural graph under
compilation. To alleviate this intrinsic restriction, we develop an Identity Graph Rewriting scheme that leads to
even lower memory footprint without changing the mathematical integrity of the neural network. We evaluate our
proposed algorithms and schemes using representative irregularly wired neural networks. Compared to TensorFlow
Lite, a widely used framework for edge devices, the proposed framework provides 1.86×reduction in memory
footprint and 1.76× reduction in off-chip trafﬁc with an average of less than one minute extra compilation time.

1

INTRODUCTION

Growing body of work focuses on Automating Machine
Learning (AutoML) using Neural Architecture Search
(NAS) (Zoph & Le, 2017; Cortes et al., 2017; Zoph et al.,
2018; Liu et al., 2019a; Cai et al., 2019; Real et al., 2019;
Zhang et al., 2019) and now even, Random Network Genera-
tors (Xie et al., 2019; Wortsman et al., 2019) which emit mod-
els with irregular wirings, and shows that such irregularly
wired neural networks can signiﬁcantly enhance classiﬁca-
tion performance. These networks that deviate from regular
topology can even adapt to some of the constraints of the
hardware (e.g., memory capacity, bandwidth, number of func-
tional units), rendering themselves especially useful in target-

†Work done as intern at Qualcomm AI Research. 1University of
California, San Diego 2Qualcomm AI Research 3Duke University.
Correspondence to: Byung Hoon Ahn <bhahn@eng.ucsd.edu>.

Proceedings of the 3 rd MLSys Conference, Austin, TX, USA, 2020.
Copyright 2020 by the author(s).

ing edge devices. Therefore, lifting the regularity condition
provides signiﬁcant freedom for NAS and expands the search
space (Cortes et al., 2017; Zhang et al., 2019; Xie et al., 2019).

The general objective is to enable deployment of neural in-
telligence even on stringently constrained devices by trading
off regular wiring of neurons for higher resource efﬁciency.
Importantly, pushing neural execution to edge is one way to
address the growing concerns about privacy (Mireshghallah
et al., 2020) and enable their effective use where connectivity
to cloud is restricted (Wu et al., 2019). However, the new
challenge arises regarding orchestrating execution of these
irregularly wired neural networks on the edge devices as
working memory footprint during execution frequently
surpass the strict cap on the memory capacity of these
devices. The lack of multi-level memory hierarchy in these
micro devices exacerbates the problem, because the network
cannot even be executed if the footprint exceeds the capacity.
To that end, despite the signiﬁcant potential of irregularly
wired neural networks, their complicated execution pattern,

 
 
 
 
 
 
Ordering Chaos: Memory-Aware Scheduling of Irregularly Wired Neural Networks for Edge Devices

in contrast to previously streamlined execution of models
with regular topology, renders conventional frameworks
futile in taking these networks to edge due to their large peak
memory footprint. While peak memory footprint is largely
dependent on scheduling of neurons, current deep learning
compilers (Chen et al., 2018; Vasilache et al., 2018) and
frameworks (Abadi et al., 2016; Paszke et al., 2019; Jia et al.,
2014) rely on basic topological ordering algorithms that are
oblivious to peak memory footprint and instead focus on an
orthogonal problem of tiling and kernel level optimization.
This paper is an initial step towards embedding peak memory
footprint as ﬁrst-grade constraint in deep learning schedulers
to unleash the potential of the emergent irregularly wired
neural networks. As such, this paper makes the following
contributions:

(1) Memory-aware scheduling for irregularly wired
neural networks. Scheduling for these networks is a topo-
logical ordering problem, which enumerates an intractably
large space of possible schedules. We offer and leverage the
insight that partial schedules leads to repeated subpaths for
search and use the graph properties to generate a signature
for these repetition while embedding a notion of the running
memory usage. These signatures enable the use of Dynamic
Programming as a basis for the optimization algorithm.
(2) Adaptive soft budgeting for tractable compilation
time. Even with the dynamic programming as the base, due
to the sheer number of neurons and connections, the search
space may remain too large (exponentially large) in practice.
As such, we devise an Adaptive Soft Budgeting technique
that uses a lightweight meta-search mechanism to ﬁnd the
appropriate memory budget for pruning the suboptimal
paths. This technique aims to ﬁnd an inﬂection point beyond
which tighter budgets may lead to no solution and looser
budget prolongs the scheduling substantially, putting the
optimization in a position of questionable utility.
(3) Identity graph rewriting for enabling higher poten-
tial in memory reduction. Any scheduling algorithm, in-
cluding ours, is still bound to the topology of the neural graph
under compilation. To relax this intrinsic restriction, we
devise an Identity Graph Rewriting scheme that exchanges
subgraphs leading to a lower memory footprint without
altering the mathematical integrity of the neural network.

Results show that our adaptive scheduling algorithm
improves peak memory footprint for irregularly wired neural
networks by 1.68×compared to TensorFlow Lite, the de facto
framework for edge devices. Our graph rewriting technique
provides an opportunity to lower the peak memory footprint
by an additional 10.7%. Furthermore, our framework can
even bring about 1.76× reduction in off-chip trafﬁc for de-
vices with multi-level memory hierarchy, and even eliminate
the trafﬁc in some cases by conﬁning the memory footprint
below the on-chip memory capacity. These gains come at
average of less than one minute extra compilation time.

(a) RandWire

(b) SwiftNet

Figure 1. Architecture of network models from NAS and Random
Network Generators. Topology of such networks include distinctive
irregular wirings between the nodes.

2 CHALLENGES AND OUR APPROACH

2.1

Irregularly Wired Neural Networks

Recent excitement
in Automated Machine Learning
(AutoML) (Feurer et al., 2015; Dean, 2017; He et al., 2018;
Elthakeb et al., 2018; Wang et al., 2019; Laredo et al., 2019)
aims to achieve human out of the loop in developing machine
learning systems. This includes Neural Architecture Search
(NAS) (Zoph & Le, 2017; Zoph et al., 2018; Liu et al., 2019a;
Cai et al., 2019; Real et al., 2019; Zhang et al., 2019) and
Random Network Generators (Xie et al., 2019; Wortsman
et al., 2019) that focus on automation of designing neural
architectures. Figure 1 demonstrates that networks of this
regime are characterized by their distinctive irregular graph
topology with much more irregular wirings (dataﬂow)
compared to conventional networks with regular graph
topology. This paper refers to these networks as irregularly
wired neural networks.

Figure 2. ImageNet accuracy vs number of multiply-and-
accumulate, where irregularly wired neural networks show higher
performance for same compute than regular topology neural net-
works. Plot for number of parameters also displays a similar trend.

Multiply-and-accumulate (Billions)Top-1 ImageNet Accuracy (%)8565707580200103040DPN-131Inception V1MobileNetShuﬄeNetInception V2Inception V3XceptionResNet-152SENetAmoebaNet-AReNeXt-101PolyNetInception ResNet V2Inception V4NASNet-ANASNet-BRandWireAmoebaNet-AAmoebaNet-BRandWireirregularly wired neural networksregular topology neural networksirregularly wired neural networksshow better performance forsame amount of compute thanregular topology neural networks top left means is betterOrdering Chaos: Memory-Aware Scheduling of Irregularly Wired Neural Networks for Edge Devices

s∗ = argmin

µpeak(s,G),

s

for s ∈ S

(1)

(a) SwiftNet Cell A.

(b) CDF of peak memory for
different possible schedules.

Figure 3. CDF of the peak memory footprint for the different
possible schedules of a given irregularly wired neural network.

From the performance perspective, these networks have
shown to outperform manually designed architectures
in terms of accuracy while using less resources. In fact,
majority of winning neural architectures in competitions
with primary goal of reducing resources (Gauen et al., 2017)
rely on NAS, suggesting its effectiveness in that respect.
Figure 2 plots the accuracy of different models given their
computation. The ﬁgure clearly shows that the Pareto
frontier of irregularly wired neural networks from NAS
and Random Network Generators are better than the hand
designed models with regular topology. This indicates that
the efﬁciency in terms of accuracy given ﬁxed resources are
better with the irregularly wired neural networks.

2.2 Challenges

Many existing compilers (Chen et al., 2018; Vasilache et al.,
2018) and frameworks (Paszke et al., 2019; Abadi et al., 2016;
Jia et al., 2014) rely on basic topological ordering algorithms
to schedule the graph. While the current approach may be suf-
ﬁcient to run conventional networks on server-class machines,
such scheme may be unﬁt for running irregularly wired
neural networks on resource-constrained edge devices. This
is because, unlike running networks with regular topology,
running irregular networks results in varied range of memory
footprint depending on the schedule. For instance, given
the constraints of a representative edge device (SparkFun
Edge: 250KB weight/activation memory and 60M MACs),
Figure 3(b) shows that 4.1% of the schedules barely meets the
hard memory constraint, while only 0.04% would achieve the
optimal peak memory. In reality, such limitation will prevent
further exploration regarding the diversity and innovation
of network design, and in order to allow edge computing
regime to take full advantage of the irregularly wired neural
networks, this limitation should be alleviated if not removed.

The most straightforward way to schedule is a brute force
approach which just enumerates S and picks one with the
minimum peak memory footprint. While this extreme
method may ﬁnd an optimal solution, it is too costly in terms
of time due to its immense complexity: Θ(|V |!) where |V |
denotes number of nodes in the graph. One way to improve
is to narrow down the search space to just focus on only
the topological orderings ST ⊂ S. However, this will still
suffer from a complexity with an upper bound of O(|V |!)
(takes days to schedule DAG with merely 30 nodes). In fact,
previous works (Bruno & Sethi, 1976; Bernstein et al., 1989)
already prove optimal scheduling for DAGs is NP-complete.
On another extreme are heuristics for topological ordering
such as Kahn’s algorithm (Kahn, 1962), with complexity of
O(|V |+|E|) where V and E are number of nodes and edges.
However, as demonstrated in Figure 3, such method may
yield suboptimal schedule of nodes which will not run on the
target hardware. To this end, we explore dynamic program-
ming combined with adaptive soft budgeting for scheduling
to achieve an optimal solution while keeping the graph con-
stant s∗, without adding too much overhead in terms of time.
We explain our algorithms in depth in Section 3.1 and 3.2.

Graph rewriting. Any scheduling algorithm including
ours is intrinsically bounded by the graph topology.
Therefore, we explore to transform the search space
through graph rewriting (Plump, 1999). Graph rewriting
is generally concerned with substituting a certain pattern
in the graph with a different pattern to achieve a certain
objective. For a computational dataﬂow graph, leveraging
distributive, associative, and commutative properties within
the computation of the graph, graph rewriting can maintain
the semantics while bringing signiﬁcant improvements
regarding some objective. For example, in general programs,
(cid:80)
evenilogxi
or log(cid:81)
ixi, while x+x can be translated to x×2 or x << 1.
Likewise, we bring this insight to neural networks to ﬁnd a
set of possible transformations X that can rewrite the original
graph G to a new graph G(cid:48) that would also change our search
space S to one with a lower peak memory footprint:

ilogxi can be represented as (cid:80)

oddilogxi +(cid:80)

X ∗ = argmin

X

(µpeak(s∗,X (G)))

(2)

2.3 Design Objectives

Scheduling algorithm. To address this issue, our work
aims to ﬁnd a schedule of nodes s∗ from the search space
S that would minimize peak memory footprint µpeak. S
enumerates all possible orderings of the nodes v ∈ V where
V is the set of all nodes within a graph G.

We identify a set of candidate patterns for transformation
χ : g → g(cid:48) (g ∈ G and g(cid:48) ∈ G(cid:48)), which constitutes X . While
transforming the graph, our method keeps the mathematical
integrity of the graph intact, thus not an approximation
method. We embed this systematic way to improve peak
memory footprint and the search space as identity graph
rewriting, and we address this technique in Section 3.3.

concatconvPeak Memory Footprint (KB)Cumulative Distributionof Schedules (%)250 KBconstraint1000204060803504002002503004.1% of schedulessatisfy the constraint0.04% of schedulesare optimalOrdering Chaos: Memory-Aware Scheduling of Irregularly Wired Neural Networks for Edge Devices

Figure 4. Overall workﬂow of SERENITY, memory-aware scheduling of irregularly wired neural network.

3 SERENITY: MEMORY-AWARE

SCHEDULING OF IRREGULARLY
WIRED NEURAL NETWORKS

As discussed in Section 2, the objective is reducing the
peak memory footprint while executing irregularly wired
neural networks. We propose SERENITY, memory-aware
scheduling that targets devices with restricted resources
(e.g., edge devices). Figure 4 summarizes the overall
scheduling process, highlighting the major contributions of
our approach. Input to SERENITY is a graph of irregularly
wired neural network G, which in fact acts as an intermediate
representation (IR) during the scheduling process. We
augment this IR with the metadata of the nodes such as the
operation type, input/output edges, input/output shapes,
and memory cost. Then the graph rewriter transforms
the graph G → G(cid:48) to relax the memory costs of memory
intensive patterns with the goal of reducing the peak memory
footprint µpeak of G. SERENITY schedules the graph to an
optimal schedule s∗ using the dynamic programming-based
scheduler. However, since the scheduling may be slow
due to the complexity, we scale down search space by
leveraging divide-and-conquer which partitions the graph
into multiple subgraphs. Them, we augment the scheduler
with an adaptive soft budgeting which prunes suboptimal
paths by adaptively ﬁnding a budget for thresholding through
a swift meta-search to speed up the scheduling process. This
section focuses on the innovations of SERENITY: dynamic
programming-based
divide-and-conquer,
scheduling,
adaptive soft budgeting, and graph rewriting, which are
explained in detail in Section 3.1, 3.2, and 3.3, respectively.

3.1 Dynamic Programming-based Scheduling:
Achieving Optimal Peak Memory Footprint

Our goal for the scheduling algorithm is to minimize the
peak memory footprint µpeak(s,G). As stated in Section 2.3,
recursive algorithms that covers the entire search space S
or the subspace of all topological orderings ST ⊂ S takes
impractically long time. This is primarily due to the repetitive
re-computation of subproblems that upper bounds the algo-
rithm by O(|V |!). Therefore, we leverage dynamic program-
ming (Bellman, 1961; 1966; Held & Karp, 1962) which in-
cludes a memoization scheme that has been shown to be effec-
tive in reducing the complexity of time-intensive algorithms
by reusing solutions from their subproblems, while still ﬁnd-

ing optimal solution by sweeping the entire search space.

n (s∗

Identifying signature to enable dynamic programming.
The ﬁrst step to applying dynamic programming to a
new problem is characterizing the structure of an optimal
solution: s∗ = s∗
n is an optimal solution for n number of
nodes). Then, it requires identifying a recursive relationship
between the optimal solution of a subproblem s∗
i and the
original problem s∗
i+1, and we do this by analyzing the
straightforward recursive topological ordering, which
while inefﬁcient sweeps the entire search space. In essence,
topological ordering algorithm is a repeated process of
identifying a set of nodes that are available for scheduling
and iterating the set for recursion. In graph theory such a
set of nodes available for scheduling is called zero-indegree
set z, where z is a set of nodes which all of their incoming
edges and the corresponding predecessor nodes (indegree)
have been scheduled. Figure 5 demonstrates the recursion
tree of the different topological ordering algorithms, where
the height of the tree is the search step and every path from
the root to the leaf is a topological ordering s ∈ ST . The
ﬁgure highlights the redundant z in the recursive topological
ordering in the recursion tree, then merges these z to make
them unique, identifying it as the signature for repetition, and
prevent the aforementioned re-computation. This makes the
scheduling for z into a unique subproblem, that constitutes

Figure 5. Illustration of identifying redundant zero-indegree set z
and making z unique (square) throughout the topological ordering
algorithm to reduce re-computation.

123457891011121314151617Gs* GGraphRewrittenGraphScheduleIdentityGraph RewriterDynamic Programming-based SchedulerAdaptive Soft BudgetingRewrite graph to alleviate activation memory footprint of the graphFind memory-optimal schedule given an input graphAdaptively manage soft budget to speed up schedulingGﬂag = {‘no solution’, ‘timeout’, ‘solution’}τ , Ts* ABCDEFHIJKLGGGraphRecursive Topological OrderingABCJCDJGBEFJBC…Search StepCDGCDG……zsRedundant zero-indegree set zDynamic Programming-based Topological OrderingABCJA,B,CDA,B,JGEFA,C,J…Search StepCDG…Unique zero-indegree set zScheduledSchedulableXXFor memoizationXXOrdering Chaos: Memory-Aware Scheduling of Irregularly Wired Neural Networks for Edge Devices

Algorithm 1 Dynamic Programming-based Scheduling

for ui in zi do

// iterate (schedule, current memory, peak memory)
for zi,(si,µi,µpeak) in Mi do

si+1 ← si.append(ui) // allocate
zi+1 ← zero-indegree(si+1,G)
µi+1,µpeak ← µi +(cid:81)(ui.shape)
µpeak,i+1 ← max(µpeak,i,µpeak)
for pi in ui.preds do

1: Input: graph G
2: Output: optimal schedule s∗
3: // initialize memoization
4: s0 ← [], µ0,µpeak,0 ← 0, z0 ← zero-indegree(s0,G)
5: M0[z0] ← (s0,µ0,µpeak,0)
6: // iterate search step
7: for i = 0 to n−1 do
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26: end for
27: s∗,µ∗

end if
end for
// memoize schedule with least peak memory
if µpeak,i+1 ≤Mi+1[zi+1].µpeak,i+1 then
Mi+1[zi+1] ← (si+1,µi+1,µpeak,i+1)

µi+1 ← µi+1 −(cid:81)(pi.shape) // deallocate

if pi is in zero-outdegree(si+1,G) then

peak ←M[·]n.sn,M[·]n.µpeak,n // solution

end if
end for

end for

activation memory here is recorded as µ9.

Finding schedule with optimal peak memory footprint.
After scheduling ui, we save the new signature into the
Mi+1 for next search step i+1. Since the goal of this work is
to minimize the overall µpeak, we identify the corresponding
optimal schedule s∗
i+1 for each zi+1 by only saving si+1 with
the minimum µpeak,i+1. We integrate the aforementioned
step of scheduling ui and updating Mi+1 to complete
the proposed dynamic programming-based scheduling
algorithm. Algorithm 1 summarizes the the algorithm. As a
ﬁrst step, the algorithm starts by initializing the memoization
table M0, then the algorithm iterates different search steps.
In each search step i, the algorithm performs the above
illustrated memory allocation for all ui in zi, and saving si+1,
µi+1, and µpeak,i+1. After iterating all search steps to n−1,
s∗ is saved in Mn with a unique entry, for n being number
of nodes in G. We provide the proof for the optimality of the
peak memory footprint in the supplementary material.

Complexity of the algorithm. The complexity of the
proposed dynamic programming-based scheduling is
O(|V |×2|V |), which is signiﬁcantly faster than the exhaus-
tive search of ST with an upper bound complexity of O(|V |!).

Figure 6. Visualization of scheduling the node u8 = H during the
search step i = 8. Starting from s8, µ8, and µpeak,8 the ﬁgure
shows how the algorithm calculates s9, µ9, and µpeak,9

the dynamic programming-based topological ordering.

Integrating the peak memory footprint constraint. On
top of the dynamic programming formulation that shows
potential for optimizing the search space signiﬁcantly, we
overlay the problem speciﬁc constraints to achieve the
optimal solution. In particular, we calculate the memory
footprint µi+1 and its corresponding peak µpeak,i+1 in each
search step i to select optimal path s∗
i+1 for memoization.
Here, we clarify the process of a search step, explaining
the details of calculating µpeak,i+1 and saving si+1 for each
search step i. In each search step, we start with number of
unique zero-indegree sets zi (signature), saved in ith entry
of memoization Mi. For each zi, we append the schedule up
to the point si, sum of activations in the memory µi for the
signature zi, and the peak memory footprint of the si denoted
µpeak,i. Therefore, in each search step i, we start with si, µi,
and µpeak,i for si. Then, when we iterate zi to schedule a new
node ui, its output activation is appended to si to form si+1,
and is allocated in the memory. Size of ui is product ((cid:81)) of
ui.shape, where shape is a property of the activation tensor
that includes channels, height, width, and the precision (e.g.,
byte, ﬂoat), is added to µi, so µi+1 ← µi + (cid:81)(ui.shape).
Then we use µi+1 as µpeak to update µpeak,i+1 (peak
memory footprint for si+1). Since some predecessors of
ui will not be used anymore after allocating ui, we update
the outdegrees of the node by decrementing them. Having
updated the outdegree, we will be left with a zero-outdegree
set that denotes the nodes that are ready for deallocation. We
deallocate the nodes in the set and update µi+1 accordingly.

To demonstrate scheduling of a node ui, Figure 6 simulates
scheduling a node u8 = H in i = 8. In the ﬁgure, (1) H is ap-
pended to s8 and allocated to memory as it is scheduled, and
then the scheduler records maximum of the µpeak,8 and the
sum of all activations in the memory at this point as µpeak,9.
Then, it recalculates the outdegrees of the predecessor nodes
of H : D and E ’s outdegree are decremented from one to
zero. (2) Then these nodes are deallocated and sum of the

outdegree of      : 1→ 0 outdegree of      : 1→ 0 µ 8ABCDEFHIJKLGGGraphScheduledTo Schedule/AllocateBATo DeallocateBActivation MemoryDEFIJ(1) Schedule/AllocateHDEFIJH(0) Initial State(2) DeallocateFIJHDEDEHIDEi = 8µpeakµpeak,9 = max(µpeak,8, µpeak)µ9s8 =ABCDEFIJµ peak,8 from M8z8 =HGu8 =Hs9 =ABCDEFIJHOrdering Chaos: Memory-Aware Scheduling of Irregularly Wired Neural Networks for Edge Devices

Figure 7. Illustration of divide-and-conquer, which divides the
graphs into multiple subgraphs (divide), schedules each of them
using the optimal scheduler (conquer), then concatenates the
sub-schedules to get the ﬁnal schedule (combine).

Due to the space limitation, we present the derivation of the
algorithm complexity in the supplementary material.

3.2 Optimizing Scheduling Speed: Speeding up
the Dynamic Programming-based Scheduling

While the above scheduling algorithm improves complexity
of the search, search space may still be intractable due to
the immense irregularity. Therefore, we devise divide-and-
conquer and adaptive soft budgeting to accelerate the search
by effectively shrinking and pruning the search space.

Divide-and-conquer. We can observe from Figure 1
that the topology of irregularly wired neural networks are
hourglass shaped ((cid:46)
(cid:47) ), because many NAS and Random
Network Generators design cells with single input and single
output then stack them to form an hourglass shape topology.
(Wilken et al., 2000) shows that, during general purpose code
scheduling, graphs can be partitioned (divide) into multiple
subgraphs and the corresponding solutions (conquer) can be
concatenated (combine) to form an optimal solution for the
overall problem. While the complexity of the scheduling al-
gorithm remains the same, this divide-and-conquer approach
can reduce the number of nodes in each subproblem, speed-
ing up the overall scheduling time. For instance, for a graph
that can be partitioned into N equal subgraphs, the schedul-
ing time will decrease from |V | × 2|V | to |V | × 2|V |/N that
we can speed up scheduling by multiple orders of magnitude
compared to the naive approach, depending on the size of
the graph and the number of partitions.

As such, Figure 7 shows this insight can be extended to our
problem setting, where we can ﬁrst perform scheduling on
each cell and merge those solutions together to form the
ﬁnal solution. First, stage is partitioning the original graph
G into multiple subgraphs g (divide). Then, utilizing the
independence among the subgraphs, each subgraph g can
be scheduled separately for their corresponding optimal
schedule sg (conquer). Considering that the number of
nodes in the subgraph g is much smaller than the entire graph
G, the scheduling time will decrease signiﬁcantly. Finally,
the schedules of the subgraphs are concatenated to give
optimal schedule s∗ of the entire graph (combine).

Adaptive soft budgeting. While divide-and-conquer
approach scales down the number of nodes, the algorithm
may still not be fast enough due to the exponential complexity
of the algorithm. Therefore, we explore avoiding suboptimal
solutions during the early stage of scheduling without affect-
ing the optimality of the original algorithm. Since our goal is
to ﬁnd a single solution that can run within a given memory
budget τ ∗ = µ∗ while all other solutions can be discarded, set-
ting some budget τ that is greater or equal to µ∗ and pruning
suboptimal schedules with which their µpeak exceeds τ can
focus the search to a smaller search space S (cid:48)
T ⊂ ST while still
achieving the optimal schedule s∗. On top of this, we develop
a meta-search for τ . This is inspired from engineers buying
a larger memory (increase τ ) if a program fails due to stack
overﬂow (= ’no solution’ due to an overly aggressive pruning)
and selling out excess memory (decrease τ ) if the current
budget is prohibitive (= ’timeout’ due to lack of pruning).
SERENITY takes advantage of this insight to develop an
adaptive soft budgeting scheme while scheduling to cut down
the overall number of explored schedules. Figure 8 illustrates
the overall idea by ﬁrst showing how some schedules are
pruned with regard to a given budget τ in Figure 8(a) then
implication of different τ on scheduling time in Figure 8(b).

Figure 8(a) depicts a certain point while scheduling G, where
nodes G , H , F , and J can be scheduled. In particular, the

(a) While both path s1 and s2 schedules lead to same z(cid:48), their
µ and µpeak varies and we can prune schedules that yield
higher µpeak than a given budget τ . Numbers next to box or
circle are µ and numbers next to edges are µpeak

(b) Adaptive soft budgeting starts by setting a hard budget
τmax as the maximum value for the soft budget τ . Then,
conducts a binary search for τ , higher than τ ∗ that it ﬁnds a
solution yet not too high that scheduling completes quickly.

Figure 8. Illustration of the adaptive soft budgeting. (a) shows how
schedules are pruned, and (b) illustrates how the soft budget τ
relates to the number of explored schedules.

ABCDEFGHDivideConquerCombineABCDScheduleScheduleg1g2Concatenates* sg1sg2EFGHABCDEFGH322335τ = 36353838Js1ABCDEFHIKLGGGraphScheduledSchedulableXXFor memoizationXXA,B,C,D,E,I…Search StepFHG…JD6E6F6J6I3H3G3A,B,C,D,E,I,F,H……C62332s2> τzzs335output activation sizeProhibitive Scheduling Time('timeout')Scheduling Failure('no solution')No. of Explored Schedules∝ Scheduling TimeBudgetOptimal Budget τ*Hard Budget τmaxSoft Budget τAdaptive Soft BudgetingOrdering Chaos: Memory-Aware Scheduling of Irregularly Wired Neural Networks for Edge Devices

ﬁgure compares two possible solutions s1 and s2 which
schedules H → F and F → H , respectively given τ = 36.
While s1 and s2 both starts from z with µ = 32, scheduling
H leads to µpeak = 32+3 (H) = 35, whereas scheduling F
or J leads to µpeak = 32+6 (F or J) = 38. Therefore, since
we assume τ = 36, s2 and s3 will fail because µpeak = 38
for s2 and s3 exceeds 36. So, as long as we set the budget
τ higher than µ∗, the scheduler still ﬁnds a single optimal
solution while avoiding many suboptimal paths. On the
other hand, too small a τ < µ∗ leads to no solution because
the optimal path would be pruned away.

Having established the possibility of pruning, our question
boils down to discovering τ that is greater or equal to µ∗
which we call an optimal budget τ ∗, yet close enough to
shrink the search space effectively. Figure 8(b) and Algo-
rithm 2 summarizes the proposed adaptive soft budgeting.
Since we start with no information about the approximate
range for τ , we resort to a commonly used topological
ordering algorithm called Kahn’s algorithm (Kahn, 1962)
(O(|V |+|E|)) to adaptively gain idea of the range for τ . We
use the peak memory footprint from this sequence and use
it as our hard budget τmax, and in contrast we call adaptively
changing τ as a soft budget. Since τmax ≥ µ∗, we know that
any τ ≥ τmax do not need to be explored. Having this upper
bound for the search, adaptive soft budgeting implements
a binary search to ﬁrst run the scheduling algorithm with τ
and T as input, where T is an hyperparameter that limits the
scheduling time per search step. The binary search increases
τ (τnew ← (τnew + τold)/2) if it ﬁnds ’no solution’ and
decreases τ (τnew ← τnew/2) if a search step returns ’timeout’
(search step duration exceeds T ). The binary search stops as

Algorithm 2 Adaptive Soft Budgeting

// binary search for τ : decrease τ if ’timeout’
// and increase τ if ’no solution’
if f lag is ’timeout’ then

1: Input: graph G
2: Output: optimal schedule s∗
3: τmax ← µ(Kahn’sAlgorithm(G),G) // hard budget
4: τold,τnew ← τmax
5: f lag ← ’no solution’
6: repeat
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19: until f lag is ’solution’

// simultaneous
τold ← τnew, τnew ← τnew/2
else if f lag is ’no solution’ then

// simultaneous
τold ← τnew, τnew ← (τnew +τold)/2

end if
if f lag is ’solution’ then

s∗ ← schedule // optimal schedule

end if

Figure 9. Illustration of the graph rewriting patterns: channel-wise
partitioning and kernel-wise partitioning can reduce the memory
cost of convolution and depthwise convolution respectively.

soon as it ﬁnds a schedule (’solution’), and this method using
binary search is guaranteed to work due to the monotonically
increasing number of explored schedules with τ .

3.3

Identity Graph Rewriting: Improving the
Search Space for Better Peak Memory Footprint

Reorganizing the computational graph of the irregularly
wired neural networks may lead to signiﬁcant reduction in the
peak memory footprint µpeak during computation. For exam-
ple, it is notable that large stream of NAS-based works (Liu
et al., 2019a; Zhang et al., 2019) rely on extensive use of con-
catenation as a natural approach to merge information from
multiple branches of the input activations and expand the
search space of the neural architectures. However, concatena-
tion with many incoming edges may prolong the liveness of
the input activation and increase the memory pressure, which
is unfavorable especially for resource constrained scenarios.
To address this issue, we propose identity graph rewriting
to effectively reduce µpeak around the concatenation while
keeping the arithmetic outputs identical. To this end, we
present two main examples of the graph patterns in irregularly
wired neural networks that beneﬁts from our technique:

Channel-wise partitioning (convolution). One typical
pattern in irregularly wired neural networks is concatenation
(concat: [·]) that takes multiple branches of the input prior to
a convolution (conv: ∗). While executing such pattern, peak
memory footprint µpeak occurs when the output y ∈ Rn is
being computed while concatenated branches of input x ∈ Rn
are also mandated to reside in the memory. Our objective
is to achieve the same arithmetic results and logical effect
as concat yet sidestep the corresponding seemingly exces-
sive memory cost. To this end, we channel-wise partition
the conv that follows the concat so that the partitioned conv
can be computed as soon as the input xi becomes available.

µpeak = Σsize(xi)  + size(y) µpeak = max(size(xi) + size(wixi)  )µpeak = Σsize(xi)  + size(y) µpeak = max(size(xi) + size(y)  )Channel-wisePartitioningKernel-wisePartitioning==wijconcatconvx1x2xnw1…wmyaddpartialconvw?1x1x2xnw?2w?nyconcatdepth-convx1x2xnw1…wnyconcatpartialdepth-convw1x1x2xnw2wnyyxiith InputOutput jth Channel of ith KernelxxOrdering Chaos: Memory-Aware Scheduling of Irregularly Wired Neural Networks for Edge Devices

(3)

(4)

(5)

Equation 3-6 detail the mathematical derivation of this sub-
stitution. Speciﬁcally, as shown in Equation 3, each kernel
iterates and sums up the result of convolving channels in conv.
However, using the distributive property of (cid:80)
i and ∗, these
transform to summation of channel-wise partitioned convo-
lution, which we call partial conv. This partial conv removes
concat from the graph leading to lower memory cost. As
illustrated in Figure 9, the memory cost of same computation
reduces from (cid:80)xi +y to max(w(cid:63)i ∗xi)+y, which becomes
more effective when there are more incoming edges to concat.

w1i ∗xi,...,

(cid:88)

(cid:105)

wmi ∗xi

(concat+conv)

y =

=

=

=

(cid:104)(cid:88)

i
(cid:104)
(cid:88)

i
(cid:104)
(cid:88)

i
(cid:104)
(cid:88)

i

i

w1i ∗xi,...,wmi ∗xi

(cid:105)

w1i,...,wmi

(cid:105)

∗xi

(cid:105)

w(cid:63)i ∗xi

(partial conv+add)

(6)

(depthwise

convolution).
Kernel-wise partitioning
Depthwise convolution (depthconv) (Sifre & Mallat, 2014;
Howard et al., 2017) has been shown to be effective in
reducing computation yet achieve competitive performance,
hence its wide use in networks that target extreme efﬁciency
as its primary goal. For concatenation (concat) followed
by a depthwise convolution (depthconv), similar to above
concat+conv case, peak memory footprint µpeak occurs
when the concatenated x is inside the memory and the
result y additionally gets saved to the memory before x
is deallocated. This time, we leverage the independence
among different kernels to kernel-wise partition the
depthconv that follows the concat so that each input xi
is computed to smaller feature maps without residing in
the memory too long. As such, Equation 7-8 derives this
substitution. Equation 7 shows that every component in the
y is independent (different subscript index) and is viable for
partitioning. In other words, this rewriting simply exposes
the commutative property between depthconv and concat
plus kernel-wise partitioning to reduce µpeak signiﬁcantly.

(cid:105)

(cid:104)
w1 ∗x1,...,wn ∗xn
(cid:104)
[w1 ∗x1],...,[wn ∗xn]

(cid:105)

y =

=

(partial depthconv+concat)

(8)

Implementation. Following the general practice of using
pattern matching algorithms in compilers (Lattner & Adve,
2004; Rotem et al., 2018; Jia et al., 2019), we implement
identity graph rewriting using pattern matching to identify
regions of the graph which can be substituted to an operation
with lower computational cost. Likewise, we make use of this
technique to identify regions that leads to lower memory cost.

Table 1. Speciﬁcation of the networks used for evaluation.

NETWORK

TYPE

DATASET

# MAC

# WEIGHT

DARTS
SWIFTNET
RANDWIRE
RANDWIRE

NAS
NAS
RAND
RAND

IMAGENET
HPD
CIFAR10
CIFAR100

574.0M
57.4M
111.0M
160.0M

4.7M
249.7K
1.2M
4.7M

TOP-1
ACCURACY

73.3%
95.1%
93.6%
74.5%

4 EVALUATION

We evaluate SERENITY with four representative irregularly
wired neural networks graphs. We ﬁrst compare the
peak memory footprint of SERENITY against TensorFlow
Lite (Google) while using the same linear memory allocation
scheme1 for both.
Furthermore, we also experiment
the impact of such peak memory footprint reduction on
off-chip memory communication. We also conduct an
in-depth analysis of the gains from the proposed dynamic
programming-based scheduler and graph rewriting using
SwiftNet Cell A (Zhang et al., 2019). Lastly, we study the
impact of adaptive soft budgeting on the scheduling time.

4.1 Methodology

Benchmarks and datasets. Table 1 lists the details of
the networks–representative of the irregularly wired neural
networks from Neural Architecture Search (NAS) and
Random Network Generators (RAND)–used for evaluation:
DARTS (Liu et al., 2019a) for ImageNet, SwiftNet (Zhang
et al., 2019) for a dataset comprised of human presence or ab-
sence (HPD), and RandWire (Xie et al., 2019) for CIFAR10
and CIFAR100. DARTS (Liu et al., 2019a) is a gradient-
based NAS algorithm. In particular we focus on the learned
normal cell for image classiﬁcation on ImageNet: only the
ﬁrst cell because it has the highest peak memory footprint and
the reset of the network is just repeated stacking of the same
cell following the practice in NASNet (Zoph et al., 2018).
SwiftNet (Zhang et al., 2019) is network from NAS by target-
ing human detection dataset. RandWire (Xie et al., 2019) are
from Random Network Generators for image classiﬁcation
on CIFAR10 and CIFAR100. The table also lists their dataset,
multiply-accumulate count (# MAC), number of parameters
(# WEIGHT), and top-1 accuracy on their respective dataset.

Comparison with TensorFlow Lite. Figure 10 evaluates
SERENITY over TensorFlow Lite on different cells of the
aforementioned networks in terms of reduction in memory
footprint. The ﬁgures illustrate that SERENITY’s dynamic
programming-based scheduler reduces the memory footprint
by a factor of 1.68×without any changes to the graph. In

1TensorFlow Lite implements a linear memory allocator named
simple memory arena: https://github.com/tensorf
low/tensorflow/blob/master/tensorflow/lite
/simple memory arena.cc

(concat+depthconv)

(7)

4.2 Experimental Results

Ordering Chaos: Memory-Aware Scheduling of Irregularly Wired Neural Networks for Edge Devices

Figure 10. Reduction in peak memory footprint of SERENITY
against TensorFlow Lite (no memory hierarchy).

(a) Memory footprint with the memory allocator (peak mem-
ory footprint of TensorFlow Lite = 551.0KB).

Figure 11. Reduction in off-chip memory communication of
SERENITY against TensorFlow Lite (with memory hierarchy).

addition, the proposed graph rewriting technique yields an
average of 1.86×(extra 10.7%) reduction in terms of peak
memory footprint. The results suggest that SERENITY yields
signiﬁcant reduction in terms of the peak memory footprint
for irregularly wired neural networks.

Improvement in off-chip memory communication. We
also show how SERENITY affects the off-chip memory
communication, which largely affects both power and
inference speed (Chen et al., 2016; Gao et al., 2017; Sharma
et al., 2018). To this end, Figure 11 sweeps different on-chip
memory conﬁgurations to measure the reduction in off-chip
communication on systems with multi-level memory
hierarchy. Since we know the entire schedule a priori, we
use Belady’s optimal algorithm (Belady, 1966), also referred
to as the clairvoyant algorithm for measuring the off-chip
memory communication, to distill the effects of the proposed
scheduling. The results show that SERENITY can reduce
the off-chip memory communication by 1.76× for a device
with 256KB on-chip memory. In particular, while there were
few cases where peak memory footprint was already small
enough to ﬁt on-chip (N/A in ﬁgure), there were some cases
where SERENITY eradicated the off-chip communication
by successfully containing the activations in the on-chip
memory while TensorFlow Lite failed to do so (marked in
ﬁgure). This suggests that SERENITY’s effort of reducing
memory footprint is also effective in reducing the off-chip
memory communication in systems with memory hierarchy,
hence the power consumption and inference speed.

Improvement from dynamic programming-based sched-
uler and identity graph rewriting. To demonstrate where
the improvement comes from, Figure 12 plots the memory
footprint while running Swiftnet Cell A. Figure 12(a) shows
the memory footprint of SERENITY with the memory

(b) Memory footprint without the memory allocator.

Figure 12. Memory footprint while running SwiftNet Cell A with
and without the memory allocator (red arrow denotes reduction).

allocation. The ﬁgure shows that SERENITY’s dynamic
programming-based scheduler brings signiﬁcant improve-
ment to the peak memory footprint (551.0KB→250.9KB),
and the graph rewriting further improves this by 25.1KB
(250.9KB→225.8KB) by utilizing patterns that alleviate
regions with large memory footprint. In order to focus on
the effect of the scheduler and graph rewriting, Figure 12(b)
presents the memory footprint of SERENITY without the
memory allocation: the sum of the activations while running
the network. The ﬁgure shows that the proposed scheduler
ﬁnds a schedule with the optimal (minimum) peak memory
footprint without changes to the graph. Then, it shows that
the proposed graph rewriting can further reduce the peak
memory footprint by 12.5KB (200.7KB→188.2KB). The re-
sults suggest that the signiﬁcant portion of the improvement
comes from the proposed dynamic programming-based
scheduler and the graph rewriting.

Scheduling time of SERENITY. Figure 13 summarizes
the (static) scheduling time taken for SERENITY to schedule
the networks. Results show that the average scheduling
time is 40.6 secs without the graph rewriting and 48.8 secs
with graph rewriting, which the difference comes from the
increase in the number of nodes from graph rewriting. The
results show that all the above gains of SERENITY come at
the cost of less than one minute average extra compilation
time. While the dynamic programming-based scheduling
suffers from an exponential time complexity, SERENITY
manages to make the scheduling tractable through the
proposed divide-and-conquer and adaptive soft budgeting.

Speed up from divide-and-conquer and adaptive soft
budgeting. Table 2 summarizes the scheduling time
of SwiftNet (Zhang et al., 2019) for different algorithms
to demonstrate the speed up from divide-and-conquer
and adaptive soft budgeting techniques. As such, the
table lists different combination of algorithms, number of

1.83x2.20x2.39x2.09x1.40x1.27x1.68x1.25x1.39x1.68x2.20x2.44x2.70x3.45x1.40x1.27x1.68x1.25x1.39x1.86x0.001.002.003.004.00NormalCell ACell BCell CCell ACell BCell ACell BCell CGeomeanDARTSImageNetSwiftNetVisual Wake Words DatasetRandWireCIFAR10RandWireCIFAR100Reduction in Peak MemoryTensorFow LiteDynamic Programming+Memory AllocatorDynamic Programming+Graph Rewriting+Memory AllocatorHigher the betterReduction in Peak MemoryNormalCell ACell BCell CCell ACell BCell CCell ACell BGeomeanSwiftNetHuman PresenceDARTSImageNetRandWireCIFAR10RandWireCIFAR1001.92x2.58x2.51x1.15x1.08x1.29x1.08x1.30x1.52x1.92x2.68x1.25x1.11x1.31x1.11x1.61x1.49x1.92x3.56x1.25x1.19x1.09x1.08x1.51x2.00x1.35x2.50x1.82x1.38x1.76x0.001.002.003.004.00NormalCell ACell BCell CCell ACell BCell ACell BCell CGeomeanDARTSImageNetSwiftNetVisual Wake Words DatasetRandWireCIFAR10RandWireCIFAR100Reduction in Off-chip32KB64KB128KB256KBReduction in Off-chipNormalCell ACell BCell CCell ACell BCell CCell ACell BGeomeanSwiftNetHuman PresenceDARTSImageNetRandWireCIFAR10RandWireCIFAR100Memory  Communicationonly SERENITY ﬁts on-chiponly SERENITY ﬁts on-chiponly SERENITY ﬁts on-chiponly SERENITY ﬁts on-chipSERENITY removes oﬀ-chip communicationN/AN/AN/AN/AN/AN/A050100150200250Memory Footprint (KB)TimeDynamic Programming+Memory AllocatorDynamic Programming+Graph Rewriting+Memory Allocator25.1KB reduction in peak memoryfootprint with Memory AllocatorMemory Footprint (KB)Time050100150200Memory Footprint (KB)TimeDynamic ProgrammingDynamic Programming+Graph Rewriting12.5KB reductionin peak memory footprintTimeMemory Footprint (KB)Ordering Chaos: Memory-Aware Scheduling of Irregularly Wired Neural Networks for Edge Devices

bring about. This paper, in contrast, focuses on this emergent
class that breaks the regularity convention and aims to enable
their execution on memory constrained edge devices.

Scheduling and tiling for neural networks. While
prior works on scheduling (Lee et al., 2003; Keßler &
Bednarski, 2001; Wilken et al., 2000) focus on classical
computing workloads, there have been limited study about
the implications of scheduling in the neural networks domain.
There is also a signiﬁcant body of work on scheduling
operations on hardware accelerators (Abdelfattah et al.,
2018) that also considers tiling (Chen et al., 2018; Vasilache
et al., 2018; Liu et al., 2019b; Ahn et al., 2020). However,
graph scheduling for irregularly wired neural network,
specially with memory constraints, is an emerging problem,
which is the focus of this paper.

Graph rewriting for neural networks.
It has been a
common practice to rewrite parts of the graph using rule-
based (Abadi et al., 2016; Paszke et al., 2019; Rotem et al.,
2018; Cyphers et al., 2018; NVIDIA, 2017) or systematic
approaches to expose parallelism and make models more
target-aware (Jia et al., 2018; 2019; Sch¨osser & Geiß, 2007).
While these approaches may alleviate the complexity of the
graph and reduce the peak memory footprint as a side effect,
these frameworks do not explore and are not concerned with
scheduling. Our work exclusively explores graph rewriting
in the context of improving the peak memory footprint.

Optimizing neural networks. There are different opti-
mization techniques that aim to simplify the neural network
indifferent dimensions. Sparsiﬁcation/compression (LeCun
et al., 1990; Han et al., 2015; Zhu & Gupta, 2018; Anwar
et al., 2017), quantization (Han et al., 2016b; Courbariaux
et al., 2016; Zhou et al., 2016; Mishra & Marr, 2018; Esser
et al., 2020), activation compression (Jain et al., 2018), and
kernel modiﬁcations reduce the complexity of the individual
operations or remove certain computations. However, our
focus, the problem of memory-aware graph scheduling still
remains orthogonal to these inspiring efforts.

6 CONCLUSION

As the new forms of connectivity emerges in neural networks,
there is a need for system support to enable their effective
use, specially for intelligence at the edge. This paper took
an initial step toward orchestrating such network under
stringent physical memory capacity constraints. We devised
signatures to enable dynamic programming and adaptive soft
budgeting to make the optimization tractable. Even more, an
identity graph writing was developed to further the potential
for gains. The encouraging results for a set of emergent net-
works suggest that there is signiﬁcant potential for compiler
techniques that enables new forms of intelligent workloads.

Figure 13. Scheduling time evaluation for SERENITY.

Table 2. Comparison of the scheduling time for different algorithms
to schedule SwiftNet. 1 , 2 , and 3 represent dynamic program-
ming, divide-and-conquer, and adaptive soft budgeting respectively.
N/A denotes infeasible within practical time.

GRAPH
REWRITING

ALGORITHM

# NODES AND
PARTITIONS

SCHEDULING
TIME

(cid:55)
(cid:55)
(cid:55)

(cid:51)
(cid:51)
(cid:51)

1
1 + 2
1 + 2 + 3

1
1 + 2
1 + 2 + 3

62 ={62}
62={21,19,22}
62={21,19,22}

92={92}
92={33,28,29}
92={33,28,29}

N/A
56.5 secs
37.9 secs

N/A
7.2 hours
111.9 secs

nodes, and the corresponding scheduling time. Straightfor-
ward implementation of the aforementioned 1 dynamic
programming-based scheduling leads to an immeasurably
large scheduling time regardless of the graph rewriting.
However, additional application of the 2 divide-and-
conquer ( 1 + 2 ) leads to a measurable scheduling time:
56.53 secs and 7.29 hours to schedule without and with the
graph rewriting, respectively. Furthermore, we observe
that further applying 3 adaptive soft budgeting ( 1 + 2 + 3 )
signiﬁcantly reduces the scheduling time 37.9 secs and 111.9
secs to schedule without and with the graph rewriting, re-
spectively. Above results indicate that applying the proposed
algorithms leads to a scheduling time of practical utility.

5 RELATED WORKS

The prevalence of neural networks has led to the development
of several compilation frameworks for deep learning (Abadi
et al., 2016; Paszke et al., 2019; Rotem et al., 2018;
Cyphers et al., 2018). However, even industry grade tools,
mostly focus on tiling and ﬁne-grained scheduling of
micro-operations on the conventional hardware (NVIDIA,
2017; Google) or accelerators (Chen et al., 2016; 2014; Han
et al., 2016a; Judd et al., 2016; Jouppi et al., 2017; Gao et al.,
2017; Parashar et al., 2017; Sharma et al., 2018; Fowers
et al., 2018). However, these framework are mostly designed
for the common regular patterns that have dominated
deep learning from almost its conception. As such, these
tools inherently had no incentive to deal with the form of
irregularities that the emerging NAS (Zoph & Le, 2017;
Cortes et al., 2017; Zoph et al., 2018; Liu et al., 2019a;
Cai et al., 2019; Real et al., 2019; Zhang et al., 2019) and
Random Networks (Xie et al., 2019; Wortsman et al., 2019)

3.2s5.7s4.5s27.8s118.1s15.1s28.5s74.4s87.9s40.6s3.2s42.1s30.5s39.3s118.1s15.1s28.5s74.4s87.9s48.8s1101001000NormalCell ACell BCell CCell ACell BCell ACell BCell CMeanDARTSImageNetSwiftNetVisual Wake Words DatasetRandWireCIFAR10RandWireCIFAR100Scheduling Time (seconds)Dynamic Programming+Memory AllocatorDynamic Programming+Graph Rewriting+Memory AllocatorScheduling Time (seconds)NormalCell ACell BCell CCell ACell BCell CCell ACell BMeanSwiftNetHuman PresenceDARTSImageNetRandWireCIFAR10RandWireCIFAR100Ordering Chaos: Memory-Aware Scheduling of Irregularly Wired Neural Networks for Edge Devices

ACKNOWLEDGEMENT

We thank the anonymous reviewers for their insightful
comments. We also thank Harris Teague and Jangho Kim
for the fruitful discussions and feedbacks on the manuscript,
and Parham Noorzad for his help with the mathematical
formulations to calculate the complexity of the algorithms.

REFERENCES

Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J.,
Devin, M., Ghemawat, S., Irving, G., Isard, M., et al. Tensorﬂow:
A system for large-scale machine learning. In OSDI, 2016.

Abdelfattah, M. S., Han, D., Bitar, A., DiCecco, R., O’Connell,
S., Shanker, N., Chu, J., Prins, I., Fender, J., Ling, A. C., et al.
DLA: Compiler and FPGA overlay for neural network inference
acceleration. In FPL, 2018.

Ahn, B. H., Pilligundla, P., and Esmaeilzadeh, H. Chameleon:
Adaptive code optimization for expedited deep neural
In ICLR, 2020. URL h t t p s :
network compilation.
//openreview.net/forum?id=rygG4AVFvH.

Anwar, S., Hwang, K., and Sung, W. Structured pruning of deep

convolutional neural networks. JETC, 2017.

Belady, L. A. A study of replacement algorithms for a virtual-

storage computer. IBM Systems Journal, 1966.

Bellman, R. Dynamic programming. Science, 1966.

Bellman, R. E. Dynamic programming treatment of the traveling

salesman problem. 1961.

Cyphers, S., Bansal, A. K., Bhiwandiwalla, A., Bobba, J.,
Brookhart, M., Chakraborty, A., Constable, W., Convey, C.,
Cook, L., Kanawi, O., et al. Intel nGraph: An intermediate repre-
sentation, compiler, and executor for deep learning. arXiv, 2018.
URL https://arxiv.org/pdf/1801.08058.pdf.

Dean, J. Machine learning for systems and systems for machine

learning. In NIPS Workshop on ML Systems, 2017.

Elthakeb, A. T., Pilligundla, P., Yazdanbakhsh, A., Kinzer, S., and
Esmaeilzadeh, H. Releq: A reinforcement learning approach
for deep quantization of neural networks. arXiv, 2018. URL
https://arxiv.org/pdf/1811.01704.pdf.

Esser, S. K., McKinstry, J. L., Bablani, D., Appuswamy, R., and
Modha, D. S. Learned step size quantization. In ICLR, 2020.
URL https://openreview.net/forum?id=rkgO66
VKDS.

Feurer, M., Klein, A., Eggensperger, K., Springenberg, J., Blum,
M., and Hutter, F. Efﬁcient and robust automated machine
learning. In NIPS, 2015.

Fowers, J., Ovtcharov, K., Papamichael, M., Massengill, T., Liu,
M., Lo, D., Alkalay, S., Haselman, M., Adams, L., Ghandi, M.,
et al. A conﬁgurable cloud-scale dnn processor for real-time
ai. In ISCA, 2018.

Gao, M., Pu, J., Yang, X., Horowitz, M., and Kozyrakis, C.
TETRIS: Scalable and efﬁcient neural network acceleration with
3d memory. In ASPLOS, 2017.

Gauen, K., Rangan, R., Mohan, A., Lu, Y.-H., Liu, W., and Berg,
A. C. Low-power image recognition challenge. In ASP-DAC,
2017.

Bernstein, D., Rodeh, M., and Gertner, I. On the complexity of
scheduling problems for parallel/pipelined machines. TC, 1989.

Google. TensorFlow Lite. URL https://www.tensorflow

.org/mobile/tflite.

Bruno, J. and Sethi, R. Code generation for a one-register machine.

JACM, 1976.

Han, S., Pool, J., Tran, J., and Dally, W. Learning both weights and

connections for efﬁcient neural network. In NIPS, 2015.

Cai, H., Zhu, L., and Han, S. ProxylessNAS: Direct neural architec-
ture search on target task and hardware. In ICLR, 2019. URL ht
tps://openreview.net/forum?id=HylVB3AqYm.

Han, S., Liu, X., Mao, H., Pu, J., Pedram, A., Horowitz, M. A., and
Dally, W. J. EIE: efﬁcient inference engine on compressed deep
neural network. In ISCA, 2016a.

Chen, T., Moreau, T., Jiang, Z., Zheng, L., Yan, E., Shen, H., Cowan,
M., Wang, L., Hu, Y., Ceze, L., et al. Tvm: An automated end-
to-end optimizing compiler for deep learning. In OSDI, 2018.

Han, S., Mao, H., and Dally, W. J. Deep compression: Compressing
deep neural networks with pruning, trained quantization and
huffman coding. In ICLR, 2016b.

Chen, Y., Luo, T., Liu, S., Zhang, S., He, L., Wang, J., Li, L.,
Chen, T., Xu, Z., Sun, N., et al. Dadiannao: A machine-learning
supercomputer. In MICRO, 2014.

He, Y., Lin, J., Liu, Z., Wang, H., Li, L.-J., and Han, S. AMC:
AutoML for model compression and acceleration on mobile
devices. In ECCV, 2018.

Chen, Y.-H., Krishna, T., Emer, J. S., and Sze, V. Eyeriss:
for deep

reconﬁgurable accelerator

An energy-efﬁcient
convolutional neural networks. JSSC, 2016.

Cortes, C., Gonzalvo, X., Kuznetsov, V., Mohri, M., and Yang,
S. AdaNet: Adaptive structural learning of artiﬁcial neural
networks. In ICML, 2017.

Courbariaux, M., Hubara, I., Soudry, D., El-Yaniv, R., and Bengio,
Y. Binarized neural networks: Training deep neural networks
with weights and activations constrained to +1 or -1. arXiv, 2016.
URL https://arxiv.org/pdf/1602.02830.pdf.

Held, M. and Karp, R. M. A dynamic programming approach to

sequencing problems. Journal of the SIAM, 1962.

Howard, A. G., Zhu, M., Chen, B., Kalenichenko, D.,
Wang, W., Weyand, T., Andreetto, M., and Adam, H.
convolutional neural networks
MobileNets:
arXiv, 2017.
for mobile vision applications.
URL
https://arxiv.org/pdf/1704.04861.pdf.

Efﬁcient

Jain, A., Phanishayee, A., Mars, J., Tang, L., and Pekhimenko, G.
Gist: Efﬁcient data encoding for deep neural network training.
In ISCA, 2018.

Ordering Chaos: Memory-Aware Scheduling of Irregularly Wired Neural Networks for Edge Devices

Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick,
R., Guadarrama, S., and Darrell, T. Caffe: Convolutional
architecture for fast feature embedding. In MM, 2014.

Plump, D. Term graph rewriting.

In Handbook Of Graph
Grammars And Computing By Graph Transformation: Volume
2: Applications, Languages and Tools. World Scientiﬁc, 1999.

Jia, Z., Lin, S., Qi, C. R., and Aiken, A. Exploring hidden
dimensions in parallelizing convolutional neural networks. In
ICML, 2018.

Jia, Z., Thomas, J., Warszawski, T., Gao, M., Zaharia, M., and
Aiken, A. Optimizing dnn computation with relaxed graph
substitutions. In SysML, 2019.

Jouppi, N. P., Young, C., Patil, N., Patterson, D., Agrawal, G.,
Bajwa, R., Bates, S., Bhatia, S., Boden, N., Borchers, A., et al.
In-datacenter performance analysis of a tensor processing unit.
In ISCA, 2017.

Judd, P., Albericio, J., Hetherington, T., Aamodt, T. M., and
Moshovos, A. Stripes: Bit-serial deep neural network computing.
In MICRO, 2016.

Real, E., Aggarwal, A., Huang, Y., and Le, Q. V. Regularized
evolution for image classiﬁer architecture search. In AAAI, 2019.

Rotem, N., Fix, J., Abdulrasool, S., Catron, G., Deng, S.,
Dzhabarov, R., Gibson, N., Hegeman, J., Lele, M., Lev-
Glow: Graph lowering compiler
enstein, R., et al.
techniques for neural networks.
URL
https://arxiv.org/pdf/1805.00907.pdf.

arXiv, 2018.

Sch¨osser, A. and Geiß, R. Graph rewriting for hardware dependent

program optimizations. In AGTIVE, 2007.

Sharma, H., Park, J., Suda, N., Lai, L., Chau, B., Chandra, V., and Es-
maeilzadeh, H. Bit Fusion: Bit-level dynamically composable ar-
chitecture for accelerating deep neural networks. In ISCA, 2018.

Sifre, L. and Mallat, S. Rigid-motion scattering for image

Kahn, A. B. Topological sorting of large networks. CACM, 1962.

classiﬁcation. Ph.D. dissertation, 2014.

Keßler, C. and Bednarski, A. A dynamic programming approach

to optimal integrated code generation. In LCTES, 2001.

Laredo, D., Qin, Y., Sch¨utze, O., and Sun, J.-Q. Automatic
arXiv, 2019. URL

model selection for neural networks.
https://arxiv.org/pdf/1905.06010.pdf.

Lattner, C. and Adve, V. LLVM: A compilation framework for
lifelong program analysis & transformation. In CGO, 2004.

Vasilache, N., Zinenko, O., Theodoridis, T., Goyal, P., DeVito,
Z., Moses, W. S., Verdoolaege, S., Adams, A., and Cohen,
A.
Tensor Comprehensions: Framework-agnostic high-
performance machine learning abstractions. arXiv, 2018. URL
https://arxiv.org/pdf/1802.04730.pdf.

Wang, K., Liu, Z., Lin, Y., Lin, J., and Han, S. HAQ: Hardware-
aware automated quantization with mixed precision. In CVPR,
2019.

LeCun, Y., Denker, J. S., and Solla, S. A. Optimal brain damage.

Wilken, K., Liu, J., and Heffernan, M. Optimal instruction

In NIPS, 1990.

scheduling using integer programming. In PLDI, 2000.

Lee, C., Lee, J. K., Hwang, T., and Tsai, S.-C. Compiler
optimization on vliw instruction scheduling for low power.
TODAES, 2003.

Liu, H., Simonyan, K., and Yang, Y. DARTS: Differen-
In ICLR, 2019a. URL https:

tiable architecture search.
//openreview.net/forum?id=S1eYHoC5FX.

Wortsman, M., Farhadi, A., and Rastegari, M. Discovering neural

wirings. In NeurIPS, 2019.

Wu, C.-J., Brooks, D., Chen, K., Chen, D., Choudhury, S., Dukhan,
M., Hazelwood, K., Isaac, E., Jia, Y., Jia, B., et al. Machine
learning at facebook: Understanding inference at the edge. In
HPCA, 2019.

Liu, Y., Wang, Y., Yu, R., Li, M., Sharma, V., and Wang, Y. Opti-
mizing CNN model inference on CPUs. In USENIX ATC, 2019b.

Xie, S., Kirillov, A., Girshick, R., and He, K. Exploring randomly
wired neural networks for image recognition. In ICCV, 2019.

Mireshghallah, F., Taram, M., Ramrakhyani, P., Jalali, A., Tullsen,
D., and Esmaeilzadeh, H. Shredder: Learning noise distributions
to protect inference privacy. In ASPLOS, 2020.

Mishra, A. and Marr, D.

edge distillation techniques
network accuracy.
//openreview.net/forum?id=B1ae1lZRb.

In ICLR, 2018.

Apprentice: Using knowl-
low-precision
to improve
URL h t t p s :

NVIDIA. TensorRT: Programmable inference accelerator., 2017.
URL https://developer.nvidia.com/tensorrt.

Parashar, A., Rhu, M., Mukkara, A., Puglielli, A., Venkatesan, R.,
Khailany, B., Emer, J., Keckler, S. W., and Dally, W. J. Scnn:
An accelerator for compressed-sparse convolutional neural
networks. In ISCA, 2017.

Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G.,
Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. PyTorch:
An imperative style, high-performance deep learning library. In
NeurIPS, 2019.

Zhang, T., Yang, Y., Yan, F., Li, S., Teague, H., Chen, Y., et al.
Swiftnet: Using graph propagation as meta-knowledge to search
highly representative neural architectures. arXiv, 2019. URL
https://arxiv.org/pdf/1906.08305.pdf.

Zhou, S., Wu, Y., Ni, Z., Zhou, X., Wen, H., and Zou, Y.
DoReFa-Net: Training low bitwidth convolutional neural
networks with low bitwidth gradients. arXiv, 2016. URL
https://arxiv.org/pdf/1606.06160.pdf.

Zhu, M. and Gupta, S.

ex-
ploring the efﬁcacy of pruning for model compres-
URL h t t p s :
sion.
//openreview.net/forum?id=S1lN69AT-.

In ICLR Workshop, 2018.

To prune, or not to prune:

Zoph, B. and Le, Q. V.

Neural architecture search
with reinforcement
URL
https://openreview.net/forum?id=r1Ue8Hcxg.

ICLR, 2017.

learning.

Zoph, B., Vasudevan, V., Shlens, J., and Le, Q. V. Learning transfer-
able architectures for scalable image recognition. In CVPR, 2018.

Ordering Chaos: Memory-Aware Scheduling of Irregularly Wired Neural Networks for Edge Devices

A COMPARISON BETWEEN IRREGULARLY

WIRED NEURAL NETWORKS
AND CONVENTIONAL REGULAR
TOPOLOGY NEURAL NETWORKS

(a) ImageNet accuracy vs number of multiply-and-accumulate.

(b) ImageNet accuracy vs number of parameters.

Figure 14. ImageNet accuracy vs number of multiply-and-
accumulate or parameters, where irregularly wired neural networks
show higher performance for same amount of compute or number
of parameters than regular topology neural networks.

B COMPARISON WITH TENSORFLOW LITE

In addition to the relative reductions provided in Figure 10,
Figure 15 provides the raw numbers of the peak memory foot-
print for the benchmark irregularly wired neural networks.

Figure 15. Peak memory footprint of running irregularly wired
neural networks on SERENITY and TensorFlow Lite.

C PROOF FOR OPTIMAL PEAK MEMORY
FOOTPRINT FROM THE DYNAMIC
PROGRAMMING-BASED SCHEDULING

Here we prove the optimality of the above dynamic
programming-based scheduling algorithm.
THEOREM 1. In order to ﬁnd a schedule s∗ with an optimal
peak memory consumption µ∗, it is sufﬁcient to keep just
one schedule-peak memory pair (si, zi) in ST i for each
zero-indegree set zi, and to append subsequent nodes on top
of si to get si+1 in each search step.

Proof. If i = 0, the optimal s0 is an empty sequence and µ0
must be 0. On the other hand, if i ≥ 1, assume that (subop-
i ∈ zi and achieves µ∗.
timal) vi constitutes s∗, substituting u∗
In such case, let vi be replaced with (optimal) u∗
i , which will
result in µpeak ← min(µi + (cid:81)vi.shape,µi + (cid:81)u∗
i .shape),
and µi+1 is calculated by deducting (cid:81) pi.shape, ∀pi ∈
(ui.preds∩zero-outdegree(si+1,G)). By recursively apply-
ing uk for rest of the search steps k, the algorithm should
ﬁnd an alternative sequence s∗(cid:48) with µ∗(cid:48)≤ µ∗ due to the min
operator above, contradicting the original assumption on the
optimality of s∗. Therefore, our algorithm ﬁnds a schedule
(cid:4)
with an optimal peak memory consumption.

D COMPLEXITY ANALYSIS OF

THE DYNAMIC PROGRAMMING-BASED
SCHEDULING AND PROOF

We compare the complexity of exhaustively exploring ST
and our dynamic programming-based scheduling. While
the algorithm both lists candidate schedules and calculates
their peak memory footprint, we consider the peak memory
footprint calculation as one operation while deriving the
complexity. In order to visualize the analysis, we invent G
in Figure 16 to demonstrate the upper bound complexity of
each algorithm. It has a single entry node and a single exit
node A and Z , respectively, and all other nodes constitute
independent branches between the entry and the exit node.

Figure 16. Topology of G to demonstrate the upper bound
complexity of each algorithm.

First, we demonstrate the complexity of the recursive
topological sorting that exhaustively explores ST . Since
there is a single entry node and a single exit node, there
will be |V − 2| remaining nodes and these nodes can be
scheduled independently of one another, thereby the number
of candidate schedules become (cid:104)|V − 2|!(cid:105) and the overall

Multiply-and-accumulate (Billions)Top-1 ImageNet Accuracy (%)8565707580200103040DPN-131Inception V1MobileNetShuﬄeNetInception V2Inception V3XceptionResNet-152SENetAmoebaNet-AReNeXt-101PolyNetInception ResNet V2Inception V4NASNet-ANASNet-BRandWireAmoebaNet-AAmoebaNet-BRandWireirregularly wired neural networksregular topology neural networksirregularly wired neural networksshow better performance forsame amount of compute thanregular topology neural networks top left means is betterNumber of Parameters (Millions)Top-1 ImageNet Accuracy (%)856570758080040100140DPN-131irregularly wired neural networksInception V1MobileNetShuﬄeNetInception V2Inception V3XceptionResNet-152SENetAmoebaNet-CReNeXt-101PolyNetInception ResNet V2Inception V4NASNet-ANASNet-ARandWireAmoebaNet-ARandWireregular topology neural networksirregularly wired neural networksshow better performance forsame number of parameters thanregular topology neural networks top left means is better6020120NASNet-A1,65655219470645330605350160903251823345926035928011575322672204592603592801150500100015002000NormalCell ACell BCell CCell ACell BCell ACell BCell CDARTSImageNetSwiftNetVisual Wake Words DatasetRandWireCIFAR10RandWireCIFAR100Peak Memory Footprint (KB)TensorFow LiteDynamic Programming+Memory AllocatorDynamic Programming+Graph Rewriting+Memory AllocatorSmaller the betterPeak Memory Footprint (KB)NormalCell ACell BCell CCell ACell BCell CCell ACell BSwiftNetHuman PresenceDARTSImageNetRandWireCIFAR10RandWireCIFAR100ADWZCBXYGGraph…Ordering Chaos: Memory-Aware Scheduling of Irregularly Wired Neural Networks for Edge Devices

complexity becomes O(|V |!), where |V | denotes the number
of nodes. On the other hand, for the dynamic programming
we calculate the number of candidates by utilizing the num-
ber of schedules that gets memoized. Our memoization takes
advantage of the zero-indegree sets z for each search step.

For the ﬁrst and the last search steps, we assume that we have
a single entry node and a single exit node. On the other hand,
since the number of nodes scheduled in search step i would
be i − 1, the maximum number of entries for memoization
is (cid:0)|V |−2
(cid:1). On top of this, each step would make an iteration
over the set of candidate nodes to discover the next search
step’s z. Therefore, search step 1 would explore |V | − 2
nodes and the search steps 2 to |V | − 1 would iterate over
|V |−1−i nodes. Summarizing this would yield:

i−1

1+1×(|V |−2)+

(cid:19)

(cid:18)|V |−2
1

×(|V |−3)+

= 1+

×(|V |−2)+

(cid:19)

(cid:18)|V |−2
1

×(|V |−3)+

...+

(cid:19)

(cid:18)|V |−2
|V |−2
(cid:19)

...+

(cid:19)

(cid:18)|V |−2
0
(cid:18)|V |−2
|V |−2
(cid:18)|V |−2
i

|V |−2
(cid:88)

i=0

×0+1

×0+1

(cid:19)

= 2+

×(|V |−2−i)

= 2+(|V |−2)×2|V |−3
≤ (|V |−2)×2|V |−2
≤ |V |×2|V |

, for |V | ≥ 4

As a result, we can see that our dynamic programming-based
scheduling algorithm is bounded by O(|V | × 2|V |). By
using Stirling’s approximation on the complexity of the
recursive topological sorting, we can prove that the dynamic
programming-based scheduling algorithm should be
signiﬁcantly faster than the recursive topological ordering.

