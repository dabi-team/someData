1
2
0
2

t
c
O
1
3

]

G
L
.
s
c
[

2
v
4
2
6
6
0
.
3
0
1
2
:
v
i
X
r
a

Beta-CROWN: Efﬁcient Bound Propagation with
Per-neuron Split Constraints for Neural Network
Robustness Veriﬁcation

Shiqi Wang*,1 Huan Zhang*,2 Kaidi Xu*,3

Xue Lin3

1Columbia University

Suman Jana1 Cho-Jui Hsieh4 Zico Kolter2
2CMU 3Northeastern University

4UCLA

sw3215@columbia.edu
xue.lin@northeastern.edu

huan@huan-zhang.com xu.kaid@northeastern.edu
suman@cs.columbia.edu chohsieh@cs.ucla.edu
zkolter@cs.cmu.edu

* Equal Contribution

Abstract

Bound propagation based incomplete neural network veriﬁers such as CROWN
are very efﬁcient and can signiﬁcantly accelerate branch-and-bound (BaB) based
complete veriﬁcation of neural networks. However, bound propagation cannot
fully handle the neuron split constraints introduced by BaB commonly handled by
expensive linear programming (LP) solvers, leading to loose bounds and hurting
veriﬁcation efﬁciency. In this work, we develop β-CROWN, a new bound propaga-
tion based method that can fully encode neuron splits via optimizable parameters
β constructed from either primal or dual space. When jointly optimized in interme-
diate layers, β-CROWN generally produces better bounds than typical LP veriﬁers
with neuron split constraints, while being as efﬁcient and parallelizable as CROWN
on GPUs. Applied to complete robustness veriﬁcation benchmarks, β-CROWN
with BaB is up to three orders of magnitude faster than LP-based BaB methods, and
is notably faster than all existing approaches while producing lower timeout rates.
By terminating BaB early, our method can also be used for efﬁcient incomplete
veriﬁcation. We consistently achieve higher veriﬁed accuracy in many settings
compared to powerful incomplete veriﬁers, including those based on convex barrier
breaking techniques. Compared to the typically tightest but very costly semideﬁnite
programming (SDP) based incomplete veriﬁers, we obtain higher veriﬁed accuracy
with three orders of magnitudes less veriﬁcation time. Our algorithm empowered
the α,β-CROWN (alpha-beta-CROWN) veriﬁer, the winning tool in VNN-COMP
2021. Our code is available at http://PaperCode.cc/BetaCROWN.

1

Introduction

As neural networks (NNs) are being deployed in safety-critical applications, it becomes increasingly
important to formally verify their behaviors under potentially malicious inputs. Broadly speaking,
the neural network veriﬁcation problem involves proving certain desired relationships between inputs
and outputs (often referred to as speciﬁcations), such as safety or robustness guarantees, for all inputs
inside some domain. Canonically, the problem can be cast as ﬁnding the global minima of some
functions on the network’s outputs (e.g., the difference between the predictions of the true label and
another target label), within a bounded input set as constraints. This is a challenging problem due to
the non-convexity and high dimensionality of neural networks.

35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.

 
 
 
 
 
 
We ﬁrst focus on complete veriﬁcation: the veriﬁer should give a deﬁnite “yes/no” answer given
sufﬁcient time. Many complete veriﬁers rely on the branch and bound (BaB) method [8] involving
(1) branching by recursively splitting the original veriﬁcation problem into subdomains (e.g., splitting
a ReLU neuron into positive/negative linear regions by adding split constraints) and (2) bounding
each subdomain with specialized incomplete veriﬁers. Traditional BaB-based veriﬁers use expensive
linear programming (LP) solvers [15, 23, 7] as incomplete veriﬁers which can fully encode neuron
split constraints. Meanwhile, a recent veriﬁer, Fast-and-Complete [45], demonstrates that cheap
incomplete veriﬁers can signiﬁcantly accelerate complete veriﬁcation on GPUs over LP-based
ones thanks to their efﬁciency. Many cheap incomplete veriﬁers are based on bound propagation
methods [46, 42, 41, 13, 17, 36, 44], i.e., maintaining and propagating tractable and sound bounds
through networks, and CROWN [46] is a representative which propagates a linear or quadratic bound.

However, unlike LP based veriﬁers, existing bound propagation methods lack the power to handle
neuron split constraints introduced by BaB. For instance, given inputs x, y ∈ [−1, 1], they can bound
a ReLU’s input x + y as [−2, 2] but they have no means to consider neuron split constraints such
as x − y ≥ 0 introduced by splitting another ReLU to the positive linear region. Such a problem
causes looser bounds and unnecessary branching, hurting the veriﬁcation efﬁciency. Even worse,
without considering these split constraints, bound propagation methods cannot detect many infeasible
subdomains in BaB [45], leading to incompleteness unless costly checking is performed.

In our work, we develop a new, fast bound propagation based incomplete veriﬁer, β-CROWN. It solves
an optimization problem equivalent to the expensive LP based methods with neuron split constraints
while still enjoying the efﬁciency of bound propagation methods. β-CROWN contains optimizable
parameters β which come from propagation of Lagrangian multipliers, and any valid settings of these
parameters yield sound bounds for veriﬁcation. These parameters are optimized using a few steps of
(super)gradient ascent to achieve bounds as tight as possible. Optimizing β can also eliminate many
infeasible subdomains and avoid further useless branching. Furthermore, we can jointly optimize
intermediate layer bounds similar to [44] but also with the additional parameters β, allowing β-
CROWN to tighten relaxations and outperform typical LP veriﬁers with ﬁxed intermediate layer
bounds. Unlike traditional LP-based BaB methods, β-CROWN can be efﬁciently implemented with
an automatic differentiation framework on GPUs to fully exploit the power of modern accelerators.
The combination of β-CROWN and BaB (β-CROWN BaB) produces a complete veriﬁer with GPU
acceleration, reducing the veriﬁcation time of traditional LP based BaB veriﬁers [8] by up to three
orders of magnitudes on a commonly used benchmark suite on CIFAR-10 [6, 10]. Compared to all
state-of-the-art GPU-based complete veriﬁers [7, 45, 10, 23, 6, 11], our approach is noticeably faster
with lower timeout rates. Our algorithm empowered the tool α,β-CROWN (alpha-beta-CROWN),
which won the 2nd International Veriﬁcation of Neural Networks Competition [3] (VNN-COMP
2021) with the highest total score and veriﬁed the most number of problem instances in 8 benchmarks.

Finally, by terminating our complete veriﬁer β-CROWN BaB early, our approach can also function
as a more accurate incomplete veriﬁer by returning an incomplete but sound lower bound of all
subdomains explored so far. We achieve better veriﬁed accuracy on a few benchmarking models
over powerful incomplete veriﬁers including those based on tight linear relaxations [35, 37, 26] and
semideﬁnite relaxations [9]. Compared to the typically tightest but very costly incomplete veriﬁer
SDP-FO [9] based on the semideﬁnite programming (SDP) relaxations [28, 14], our method obtains
consistently higher veriﬁed accuracy while reducing veriﬁcation time by three orders of magnitudes.

2 Background

2.1 The neural network veriﬁcation problem and its LP relaxation

We deﬁne the input of a neural network as x ∈ Rd0, and deﬁne the weights and biases of an
L-layer neural network as W(i) ∈ Rdi×di−1 and b(i) ∈ Rdi (i ∈ {1, · · · , L}) respectively. For
simplicity we assume that dL = 1 so W(L) is a vector and b(L) is a scalar. The neural network
function f : Rd0 → R is deﬁned as f (x) = z(L)(x), where z(i)(x) = W(i) ˆz(i−1)(x) + b(i),
ˆz(i)(x) = σ(z(i)(x)) and ˆz(0)(x) = x. σ is the activation function and we use ReLU throughout this
paper. When the context is clear, we omit ·(x) and use z(i)
to represent the pre-activation
j
and post-activation values of the j-th neuron in the i-th layer. Neural network veriﬁcation seeks the
solution of the optimization problem in Eq. 1:
min f (x) := z(L)(x)

s.t. z(i) = W(i) ˆz(i−1)+b(i), ˆz(i) = σ(z(i)), x ∈ C, i ∈ {1, · · · , L−1} (1)

and ˆz(i)
j

2

The set C deﬁnes the allowed input region and our aim is to ﬁnd the minimum of f (x) for x ∈ C, and
throughout this paper we consider C as an (cid:96)∞ ball around a data example x0: C = {x | (cid:107)x−x0(cid:107)∞≤ (cid:15)}
but other (cid:96)p norms can also be supported. In practical settings, we typically have “speciﬁcations” to
verify, which are (usually linear) functions of neural network outputs describing the desired behavior
of neural networks. For example, to guarantee robustness we typically investigate the margin between
logits. Because the speciﬁcation can also be seen as an output layer of NN and merged into f (x)
under veriﬁcation, we do not discuss it in detail in this work. We consider the canonical speciﬁcation
f (x) > 0: if we can prove that f (x) > 0, ∀x ∈ C, we say f (x) is veriﬁed.

When C is a convex set, Eq. 1 is still a non-convex problem because the constraints ˆz(i) = σ(z(i)) are
non-convex. Given unlimited time, complete veriﬁers can solve Eq. 1 exactly: f ∗ = min f (x), ∀x ∈
C, so we can always conclude if the speciﬁcation holds or not for any problem instance. On the other
hand, incomplete veriﬁers usually relax the non-convexity of neural networks to obtain a tractable
lower bound of the solution f ≤ f ∗. If f ≥ 0, then f ∗ > 0 so f (x) can be veriﬁed; when f < 0, we
are not able to infer the sign of f ∗ so cannot conclude if the speciﬁcation holds or not.

j ) and its intermediate layer bounds l(i)

A commonly used incomplete veriﬁcation technique is to relax non-convex ReLU constraints with
linear constraints and turn the veriﬁcation problem into a linear programming (LP) problem, which
can then be solved with linear solvers. We refer to it as the “LP veriﬁer” in this paper. Speciﬁcally,
j ) := max(0, z(i)
given ReLU(z(i)
j , each ReLU
j = z(i)
can be categorized into three cases: (1) if l(i)
; (2) if
u(i)
j ≤ 0 (ReLU in inactive region) then ˆz(i)
(ReLU is unstable) then
three linear bounds are used: ˆz(i)

j ≤ u(i)
j ≥ 0 (ReLU in linear region) then ˆz(i)
j ≤ 0 ≤ u(i)
j = 0; (3) if l(i)
j
(cid:16)
j ≤ u
, and ˆz(i)
j − l(i)
z(i)

j
referred to as the “triangle” relaxation [15, 42]. The intermediate layer bounds l(i) and u(i) are
usually obtained from a cheaper bound propagation method (see next subsection). LP veriﬁers can
provide relatively tight bounds but linear solvers are still expensive especially when the network is
large. Also, unlike our β-CROWN, they have to use ﬁxed intermediate bounds and cannot use the
joint optimization of intermediate layer bounds (Section 3.3) to tighten relaxation.

j ≥ 0, ˆz(i)

; they are often

j ≥ z(i)

j ≤ z(i)

(i)
j
(i)
(i)
j −l
j

(cid:17)

u

j

j

2.2 CROWN: efﬁcient incomplete veriﬁcation by propagating linear bounds

Another cheaper way to give a lower bound for the objective in Eq. 1 is through sound bound
propagation. CROWN [46] is a representative method that propagates a linear bound of f (x) w.r.t.
every intermediate layer in a backward manner until reaching the input x. CROWN uses two linear
constraints to relax unstable ReLU neurons: a linear upper bound ˆz(i)

and a

(cid:16)
j − l(i)
z(i)

j ≤

(cid:17)

u

j

(i)
j
(i)
j −l

u

(i)
j

j

j z(i)

(0 ≤ α(i)

j ≥ α(i)

linear lower bound ˆz(i)
j ≤ 1). We can then bound the output of a ReLU layer:
Lemma 2.1 (ReLU relaxation in CROWN). Given w, v ∈ Rd, l ≤ v ≤ u (element-wise), we have
w(cid:62)ReLU(v) ≥ w(cid:62)Dv + b(cid:48),
where D is a diagonal matrix containing free variables 0 ≤ αj ≤ 1 only when uj > 0 > lj and
wj ≥ 0, while its rest values as well as constant b(cid:48) are determined by l, u, w.

Detailed forms of each term are listed in Appendix A. Lemma 2.1 can be repeatedly applied, resulting
in an efﬁcient back-substitution procedure to derive a linear lower bound of NN output w.r.t. x:
Lemma 2.2 (CROWN bound [46]). Given an L-layer ReLU NN f (x) : Rd0 → R with weights W(i),
biases b(i), pre-ReLU bounds l(i) ≤ z(i) ≤ u(i) (1 ≤ i ≤ L) and input constraint x ∈ C. We have
a(cid:62)

f (x) ≥ min
x∈C
where aCROWN and cCROWN can be computed using W(i), b(i), l(i), u(i) in polynomial time.

CROWNx + cCROWN

min
x∈C

When C is an (cid:96)p norm ball, minimization over the linear function can be easily solved using Hölder’s
inequality. The main beneﬁt of CROWN is its efﬁciency: CROWN can be efﬁciently implemented
on machine learning accelerators such as GPUs [44] and TPUs [47], and it can be a few magnitudes
faster than an LP veriﬁer which is hard to parallelize on GPUs. CROWN was generalized to general
architectures [44, 31] while we only demonstrate it for feedforward ReLU networks for simplicity.
Additionally, Xu et al. [45] showed that it is possible to optimize the slope of the lower bound, α,
using gradient ascent, to further tighten the bound (sometimes referred to as α-CROWN).

3

2.3 Branch and Bound and Neuron Split Constraints

Branch and bound (BaB) method is widely adopted in complete veriﬁers [8]: we divide the domain of
the veriﬁcation problem C into two subdomains C1 = {x ∈ C, z(i)
j < 0}
where z(i)
is an unstable ReLU neuron in C but now becomes linear for each subdomain. Incomplete
j
veriﬁers can then estimate the lower bound of each subdomain with relaxations. If the lower bound
produced for subdomain Ci (denoted by f
) is greater than 0, Ci is veriﬁed; otherwise, we further
branch over domain Ci by splitting another unstable ReLU neuron. The process terminates when all
subdomains are veriﬁed. The completeness is guaranteed when all unstable ReLU neurons are split.

j ≥ 0} and C2 = {x ∈ C, z(i)

Ci

LP veriﬁer with neuron split constraints. A popular incomplete veriﬁer used in BaB is the LP
veriﬁer. Essentially, when we split the j-th ReLU in layer i, we can simply add z(i)
j < 0
to Eq. 1 and get a linearly relaxed lower bound to each subdomain. We denote the Z +(i) and Z −(i)
as the set of neuron indices with positive and negative split constraints in layer i. We deﬁne the split
constraints at layer i as Z (i) := {z(i) | z(i)
≥ 0, z(i)
< 0, ∀j1 ∈ Z +(i), ∀j2 ∈ Z −(i)}. We denote
j2
j1
the vector of all pre-ReLU neurons as z, and we deﬁne a set Z to represent the split constraints on z:
Z = Z (1) ∩Z (2) ∩· · ·∩Z (L−1). For convenience, we also use the shorthand ˜Z (i) := Z (1) ∩· · ·∩Z (i)
and ˜z(i) := {z(1), z(2), · · · , z(i)}. LP veriﬁers can easily handle these neuron split constraints but are
more expensive than bound propagation methods like CROWN and cannot be accelerated on GPUs.

j ≥ 0 or z(i)

Branching strategy. Branching strategies (selecting which ReLU neuron to split) are generally
agnostic to the incomplete veriﬁer used in BaB but do affect the overall BaB performance. BaBSR [7]
is a widely used strategy in complete veriﬁers, which is based on an fast estimates on objective
improvements after splitting each neuron. The neuron with highest estimated improvement is selected
for branching. Recently, Filtered Smart Branching (FSB) [11] improves BaBSR by mimicking strong
branching - it utilizes bound propagation methods to evaluate the best a few candidates proposed
by BaBSR and chooses the one with largest improvement. Graph neural network (GNN) based
branching was also proposed [23]. Our β-CROWN BaB is a general complete veriﬁcation framework
ﬁt for any potential branching strategy, and we evaluate both BaBSR and FSB in experiments.

3 β-CROWN for Complete and Incomplete Veriﬁcation

In this section, we ﬁrst give intuitions on how β-CROWN handles neuron split constraints without
costly LP solvers. Then we formally state the main theorem of β-CROWN from both primal and
dual spaces, and discuss how to tighten the bounds using free parameters α, β. Lastly, we propose
β-CROWN BaB, a complete veriﬁer that is also a strong incomplete veriﬁer when stopped early.

3.1 β-CROWN: Linear Bound Propagation with Neuron Split Constraints

The NN veriﬁcation problem under neuron split constraints can be seen as an optimization problem:

min
x∈C,z∈Z

f (x).

(2)

Bound propagation methods like CROWN can give a relatively tight lower bound for minx∈C f (x)
but they cannot handle the neuron split constraints z ∈ Z. Before we present our main theorem, we
ﬁrst show the intuition on how to apply split constraints to the bound propagation process.
To encode the neuron splits, we ﬁrst deﬁne diagonal matrix S(i) ∈ Rdi×di in Eq. 3 where i ∈
[1, · · · L − 1], j ∈ [1, · · · , di] are indices of layers and neurons, respectively:

j,j = −1(if split z(i)
S(i)

j ≥ 0); S(i)

j,j = +1(if split z(i)

j < 0); S(i)

j,j = 0(if no split z(i)
j )

(3)

We start from the last layer and derive linear bounds for each intermediate layer z(i) and ˆz(i) with
both constraints x ∈ C and z ∈ Z. We also assume that pre-ReLU bounds l(i) ≤ z(i) ≤ u(i) for each
layer i are available (see discussions in Sec. 3.3 on these intermediate layer bounds). We initially
have:

min
x∈C,z∈Z

f (x) = min

x∈C,z∈Z

W(L) ˆz(L−1) + b(L).

(4)

4

Since ˆz(L−1) = ReLU(z(L−1)), we can apply Lemma 2.1 to relax the ReLU neuron at layer L − 1,
and obtain a linear lower bound for f (x) w.r.t. z(L−1) (we omit all constant terms to avoid clutter):

min
x∈C,z∈Z

f (x) ≥ min

x∈C,z∈Z

W(L)D(L−1)z(L−1) + const.

To enforce the split neurons at layer L − 1, we use a Lagrange function with β(L−1)(cid:62)S(L−1)
multiplied on z(L−1):

min
x∈C,z∈Z

f (x) ≥

min
x∈C
˜z(L−2)∈ ˜Z(L−2)

max
β(L−1)≥0

W(L)D(L−1)z(L−1) + β(L−1)(cid:62)

S(L−1)z(L−1) + const

≥ max

β(L−1)≥0

min
x∈C
˜z(L−2)∈ ˜Z(L−2)

(cid:16)
W(L)D(L−1) + β(L−1)(cid:62)

S(L−1)(cid:17)

z(L−1) + const

(5)

The ﬁrst inequality is due to the deﬁnition of the Lagrange function: we remove the constraint
z(L−1) ∈ Z (L−1) and use a multiplier to replace this constraint. The second inequality is due to weak
≥ 0 has a negative multiplier −β(L−1)
duality. Due to the design of S(L−1), neuron split z(L−1)
and split z(L−1)
. Any β(L−1) ≥ 0 yields a lower bound for
j
the constrained optimization problem. Then we substitute z(L−1) with W(L−1) ˆz(L−2) + b(L−1) for
next layer:

j
< 0 has a positive multiplier β(L−1)

j

j

min
x∈C,z∈Z

f (x) ≥ max

β(L−1)≥0

min
x∈C
˜z(L−2)∈ ˜Z(L−2)

(cid:16)
W(L)D(L−1) + β(L−1)(cid:62)

S(L−1)(cid:17)

W(L−1) ˆz(L−2) + const

(6)

We deﬁne a matrix A(i) to represent the linear relationship between f (x) and ˆz(i), where A(L−1) =
W(L) according to Eq. 4 and A(L−2) = (A(L−1)D(L−1) + β(L−1)(cid:62)
S(L−1))W(L−1) by Eq. 6.
Considering 1-dimension output f (x), A(i) has only 1 row. With A(L−2), Eq. 6 becomes:

min
x∈C,z∈Z

f (x) ≥ max

β(L−1)≥0

min
x∈C
˜z(L−2)∈ ˜Z(L−2)

A(L−2) ˆz(L−2) + const,

which is in a form similar to Eq. 4 except for the outer maximization over β(L−1). This allows the
back-substitution process (Eq. 4, Eq. 5, and Eq. 6) to continue. In each step, we swap max and min
as in Eq. 5, so every maximization over β(i) is outside of minx∈C. Eventually, we have:

min
x∈C,z∈Z

f (x) ≥ max
β≥0

min
x∈C

A(0)x + const,

where β := (cid:2)β(1)(cid:62) β(2)(cid:62) · · · β(L−1)(cid:62)(cid:3)(cid:62)
we present the main theorem in Theorem 3.1 (proof is given in Appendix A).
Theorem 3.1 (β-CROWN bound). Given an L-layer NN f (x) : Rd0 → R with weights W(i), biases
b(i), pre-ReLU bounds l(i) ≤ z(i) ≤ u(i) (1 ≤ i ≤ L), input bounds C, split constraints Z. We have:

concatenates all β(i) vectors. Following the above idea,

min
x∈C,z∈Z
where a ∈ Rd0 ,P ∈ Rd0×((cid:80)L−1

f (x) ≥ max
β≥0

min
x∈C
i=1 di),q ∈ R(cid:80)L−1
i=1 di and c ∈ R are functions of W(i), b(i), l(i), u(i).

(a + Pβ)(cid:62)x + q(cid:62)β + c,

(7)

Detailed formulations for a, P, q and c are given in Appendix A. Theorem 3.1 shows that when
neuron split constraints exist, f (x) can still be bounded by a linear equation containing optimizable
multipliers β. Observing Eq. 5, the main difference between CROWN and β-CROWN lies in the
relaxation of each ReLU layer, where we need an extra term β(i)(cid:62)S(i) in the linear relationship
matrix (for example, W(L)D(L−1) in Eq. 5) between f (x) and z(i) to enforce neuron split constraints.
This extra term in every ReLU layer yields P and q in Eq. 7 after bound propagations.

To solve the optimization problem in Eq. 7, we note that in the (cid:96)p norm robustness setting (C =
{x | (cid:107)x − x0(cid:107)p≤ (cid:15)}), the inner minimization has a closed solution:

min
x∈C,z∈Z

f (x) ≥ max
β≥0

−(cid:107)a + Pβ(cid:107)q(cid:15) + (P(cid:62)x0 + q)(cid:62)β + a(cid:62)x0 + c := max
β≥0

g(β)

(8)

p + 1

where 1
q = 1. The maximization is concave in β (q ≥ 1), so we can simply optimize it using
projected (super)gradient ascent with gradients from an automatic differentiation library. Since any

5

β ≥ 0 yields a valid lower bound for minx∈C,z∈Z f (x), convergence is not necessary to guarantee
soundness. β-CROWN is efﬁcient - it has the same asymptotic complexity as CROWN when β
is ﬁxed. When β = 0, β-CROWN yields the same results as CROWN; however the additional
optimizable β allows us to maximize and tighten the lower bound due to neuron split constraints.
We deﬁne α(i) ∈ Rdi for free variables associated with unstable ReLU neurons in Lemma 2.1 for
layer i and deﬁne all free variables α = {α(1) · · · α(L−1)}. Since any 0 ≤ α(i)
j ≤ 1 yields a valid
bound, we can optimize it to tighten the bound, similarly as done in [45]. Formally, we rewrite Eq. 8
with α explicitly:

min
x∈C,z∈Z

f (x) ≥

max
0≤α≤1, β≥0

g(α, β).

(9)

3.2 Connections to the Dual Problem

In this subsection, we show that β-CROWN can also be derived from a dual LP problem. Based
on Eq. 1 and linear relaxations in Section 2, we ﬁrst construct an LP problem for (cid:96)∞ robustness
veriﬁcation in Eq. 10 where i ∈ {1, · · · , L − 1}.

min f (x) := z(L)(x)

s.t.

Network and Input Bounds: z(i) = W(i) ˆz(i−1) + b(i); ˆz(0) ≥ x0 − (cid:15); ˆz(0) ≤ x0 + (cid:15);
Stable ReLUs: ˆz(i)

(if l(i)

j = z(i)

j

Unstable: ˆz(i)

j ≥ 0, ˆz(i)

j ≥ z(i)

j

Neuron Split Constraints: ˆz(i)

j = z(i)

j ≤ 0);

j = 0 (if u(i)
j ≥ 0); ˆz(i)
(cid:16)
u(i)
, ˆz(i)
j
u(i)
j −l(i)
j ≥ 0 (if j ∈ Z +(i)); ˆz(i)
, z(i)

j − l(i)
z(i)

j ≤

(if l(i)

(cid:17)

j

j

j

j < 0 < u(i)

j , j /∈ Z +(i) ∪ Z −(i))

j = 0, z(i)

j < 0 (if j ∈ Z −(i))

(10)

Compared to the formulation in [42], we have neuron split constraints. Many BaB based complete
veriﬁers [8, 23] use an LP solver for Eq. 10 as the incomplete veriﬁer. We ﬁrst show that it is possible
to derive Theorem 3.1 from the dual of this LP, leading to Theorem 3.2:
Theorem 3.2. The objective dLP for the dual problem of Eq. 10 can be represented as

dLP = −(cid:107)a + Pβ(cid:107)1·(cid:15) + (P(cid:62)x0 + q)(cid:62)β + a(cid:62)x0 + c,

where a, P, q and c are deﬁned in the same way as in Theorem 3.1, and β ≥ 0 corresponds to the
dual variables of neuron split constraints in Eq. 10.

A similar connection between CROWN and dual LP based veriﬁer [42] was shown in [30], and their
results can be seen as a special case of ours when β = 0 (none of the split constraints are active). An
immediate consequence is that β-CROWN can potentially solve Eq. 10 as well as using an LP solver:
Corollary 3.2.1. When α and β are optimally set and intermediate bounds l, u are ﬁxed, β-CROWN
produces p∗

LP, the optimal objective of LP with split constraints in Eq. 10:

max
0≤α≤1,β≥0

g(α, β) = p∗

LP,

In Appendix A, we give detailed formulations for conversions between the variables α, β in β-
CROWN and their corresponding dual variables in the LP problem.

Joint Optimization of Free Variables in β-CROWN

3.3
In Eq. 9, g is also a function of l(i)
are also computed using β-CROWN. To obtain l(i)

j and u(i)

j , the intermediate layer bounds for each neuron z(i)

. They
j (x) and apply Theorem 3.1:

j

j , we set f (x) := z(i)

min
x∈C,˜z(i−1)∈ ˜Z (i−1)

z(i)
j (x) ≥

max
0≤α(cid:48)≤1, β(cid:48)≥0

g(cid:48)(α(cid:48), β(cid:48)) := l(i)
j

(11)

j we simply set f (x) := −z(i)

For computing u(i)
j (x). Importantly, during solving these intermediate
layer bounds, the α(cid:48) and β(cid:48) are independent sets of variables, not the same ones for the objective
f (x) := z(L). Since g is a function of l(i)
j , it is also a function of α(cid:48) and β(cid:48). In fact, there are a total

6

of (cid:80)L−1
i=1 di intermediate layer neurons, and each neuron is associated with a set of independent α(cid:48)
and β(cid:48) variables. Optimizing these variables allowing us to tighten the relaxations on unstable ReLU
neurons (which depend on l(i)
j ) and produce tight ﬁnal bounds, which is impossible in LP. In
j
other words, we need to optimize ˆα and ˆβ, which are two vectors concatenating α, β as well as a
large number of α(cid:48) and β(cid:48) used to compute each intermediate layer bound:

and u(i)

min
x∈C,z∈Z

f (x) ≥

max
0≤ ˆα≤1, ˆβ≥0

g( ˆα, ˆβ).

(12)

This formulation is non-convex and has a large number of variables. Since any 0 ≤ ˆα ≤ 1, ˆβ ≥ 0
leads to a valid lower bound, the non-convexity does not affect soundness. When intermediate layer
bounds are also allowed to be tightened during optimization, we can outperform the LP veriﬁer
for Eq. 10 using ﬁxed intermediate layer bounds. Typically, in many previous works [8, 23, 6],
when the LP formulation Eq. 10 is formed, intermediate layer bounds are pre-computed with bound
propagation procedures [8, 23], which are far from optimal. To estimate the dimension of this problem,
we denote the number of unstable neurons at layer i as si := Tr(|S(i)|). Each neuron in layer i is
associated with 2 × (cid:80)i−1
k=1 sk variables α(cid:48). Suppose each hidden layer has d neurons (si = O(d)),
then ˆα has 2 × (cid:80)L−1
k=1 sk = O(L2d2) variables in total. This can be too large for efﬁcient
i=1 di
optimization, so we share α(cid:48) and β(cid:48) among the intermediate neurons of the same layer, leading to a
total number of O(L2d) variables to optimize. Note that a weaker form of joint optimization was
also discussed in [45] without β, and a detailed analysis can be found in Appendix B.2.

(cid:80)i−1

3.4 β-CROWN with Branch and Bound (β-CROWN BaB)
We perform complete veriﬁcation following BaB framework [8] using β-CROWN as the incomplete
solver, and we use simple branching heuristics like BaBSR [7] or FSB [11]. To efﬁciently utilize
GPU, we also use batch splits to evaluate multiple subdomains in the same batch as in [44, 10]. We
list our full algorithm β-CROWN BaB in Appendix B and we show it is sound and complete here:
Theorem 3.3. β-CROWN with Branch and Bound on splitting ReLUs is sound and complete.

Soundness is trivial because β-CROWN is a sound veriﬁer. For completeness, it sufﬁces to show
that when all unstable ReLU neurons are split, β-CROWN gives the global minimum for Eq. 10.
In contrast, combining CROWN [46] with BaB does not yield a complete veriﬁer, as it cannot
detect infeasible splits and a slow LP solver is still needed to guarantee completeness [45]. Instead,
β-CROWN can detect infeasible subdomains - according to duality theory, an infeasible primal
problem leads to an unbounded dual objective, which can be detected (see Sec. B.3 for more details).

Additionally, we show the potential of early stopping a complete veriﬁer as an incomplete veriﬁer.
BaB approaches the exact solution of Eq. 1 by splitting the problem into multiple subdomains,
and more subdomains give a tighter lower bound for Eq. 1. Unlike traditional complete veriﬁers,
β-CROWN is efﬁcient to explore a large number of subdomains during a very short time, making
β-CROWN BaB an attractive solution for efﬁcient incomplete veriﬁcation.

4 Experimental Results
4.1 Comparison to Complete Veriﬁers

We evaluate complete veriﬁcation performance on dataset provided in [23, 10] and used in VNN-
COMP 2020 [22]. The benchmark contains three CIFAR-10 models (Base, Wide, and Deep)
with 100 examples each. Each data example is associated with an (cid:96)∞ norm (cid:15) and a target label
for veriﬁcation (referred to as a property to verify). The details of neural network structures and
experimental setups can be found in Appendix C. We compare against multiple baselines for complete
veriﬁcation: (1) BaBSR [7], a basic BaB and LP based veriﬁer; (2) MIPplanet [15], a customized
MIP solver for NN veriﬁcation where unstable ReLU neurons are randomly selected for splitting;
(3) ERAN [35, 33, 36, 34], an abstract interpretation based veriﬁer which performs well on this
benchmark in VNN-COMP 2020; (4) GNN-Online [23], a BaB and LP based veriﬁers using a
learned Graph Neural Network (GNN) to guide the ReLU splits; (5) BDD+ BaBSR [6], a veriﬁcation
framework based on Lagrangian decomposition on GPUs (BDD+) with BaBSR branching strategy; (6)
OVAL (BDD+ GNN) [6, 23], a strong veriﬁer in VNN-COMP 2020 using BDD+ with GNN guiding
the ReLU splits; (7) A.set BaBSR and (8) Big-M+A.set BaBSR [10], very recent dual-space veriﬁers
on GPUs with a tighter linear relaxation than triangle LP relaxations; (9) Fast-and-Complete [45],

7

Table 1: Average runtime and average number of branches on three CIFAR-10 models over 100 properties.

CIFAR-10 Base

CIFAR-10 Wide

CIFAR-10 Deep

Method

time(s)

branches %timeout

time(s)

branches %timeout

time(s)

branches %timeout

BaBSR [7]
MIPplanet [15]
ERAN∗[35, 33, 36, 34]
GNN-online [23]
BDD+ BaBSR [6]
OVAL (BDD+ GNN)∗[6, 23]
A.set BaBSR [10]
BigM+A.set BaBSR [10]
Fast-and-Complete [45]
BaDNB (BDD+ FSB)[11]
β-CROWN BaBSR
β-CROWN FSB

2367.78
2849.69
805.94
1794.85
807.91
662.17
381.78
390.44
695.01
309.29
226.06
118.23

1020.55
-
-
565.13
195480.14
67938.38
12004.60
11938.75
119522.65
38239.04
509608.50
208018.21

36.00
68.00
5.00
33.00
20.00
16.00
7.00
7.00
17.00
7.00
6.00
3.00

2871.14
2417.53
632.20
1367.38
505.65
280.38
165.91
172.65
495.88
165.53
118.26
78.32

812.65
-
-
372.74
74203.11
17895.94
2233.10
4050.59
80519.85
11214.44
217691.24
116912.57

49.00
46.00
9.00
15.00
10.00
6.00
3.00
3.00
9.00
4.00
3.00
2.00

2750.75
2302.25
545.72
1055.33
266.28
94.69
190.28
177.22
105.64
10.50
6.12
5.69

401.28
-
-
131.85
12722.74
1990.34
2491.55
3275.25
2455.11
368.16
204.66
41.12

39.00
40.00
0.00
4.00
4.00
1.00
2.00
2.00
1.00
0.00
0.00
0.00

* OVAL (BDD+ GNN) and ERAN results are from VNN-COMP 2020 report [22]. Other results were reported by their authors.

Figure 1: Percentage of solved properties with growing running time. β-CROWN FSB (light green) and
β-CROWN BaBSR (dark green) clearly lead in all 3 settings and solve over 90% properties within 10 seconds.

which uses CROWN (LiRPA) on GPUs as the incomplete veriﬁer in BaB without neuron split
constraints; (10) BaDNB (BDD+ FSB) [11], a concurrent state-of-the-art complete veriﬁer, using
BDD+ on GPUs with FSB branching strategy. β-CROWN BaB can use either BaBSR or FSB
branching heuristic, and we include both in evaluation. All methods use a 1 hour timeout threshold.

We report the average veriﬁcation time and branch numbers in Table 1 and plot the percentage of
solved properties over time in Figure 1. β-CROWN FSB achieves the fastest average running time
compared to all other baselines with minimal timeouts, and also clearly leads on the cactus plot.
When using a weaker branching heuristic, β-CROWN BaBSR still outperforms all literature baselines,
including very recent ones such as A.set BaBSR [10], Fast-and-Complete [45] and BaDNB [11]. Our
beneﬁts are more clearly shown in Figure 1, where we solve over 90% examples under 10s and most
other veriﬁers can verify much less or none of the properties within 10s. We see a 2 to 3 orders of
magitudes speedup in Figure 1 compared to CPU based veriﬁers such as MIPplanet and BaBSR.

4.2 Comparison to Incomplete Veriﬁers

Veriﬁed accuracy.
In Table 2, we compare against a few representative and strong incomplete
veriﬁers on 5 convolutional networks and 4 MLP networks for MNIST and CIFAR-10 under the same
set of 1000 images and perturbation (cid:15) as reported in [35, 37, 26]. Among the baselines, kPoly [35],
OptC2V [37] and PRIMA [26] utilize state-of-the-art multi-neuron linear relaxation for ReLUs and
can bypass the single-neuron convex relaxation barrier [30], and are among the strongest incomplete
veriﬁers. β-CROWN FSB achieves better veriﬁed accuracy on all models using a similar or less
amount of time. Some models, such as MNIST ConvBig and CIFAR ConvBig, are quite challenging -
the veriﬁed accuracy obtained by β-CROWN FSB is close to the upper bound found via PGD attack.

To make more comprehensive evaluations, in Table 3 we further compare against a state-of-the-art
semideﬁnite programming (SDP) based veriﬁer, SDP-FO [9], on one MNIST and six CIFAR-10
models reported in their paper. The models were trained using adversarial training, which posed a
challenge for veriﬁcation [28]. The SDP formulation can be tighter than linear relaxation based ones,
but it is computationally expensive - SDP-FO takes 2 to 3 hours to converge on one GPU for verifying
a single property, resulting 5,400 GPU hours to verify 200 testing images with 10 labels each. Due
to resource limitations, we directly quote SDP-FO results from [9] on the same set of models. We
evaluate veriﬁed accuracy on the same set of 200 test images for other baselines. We include a
concurrent work PRIMA [26], the strongest multi-neuron linear relaxation baseline in Table 2, which
generally outperforms kPoly and OptC2V. Table 3 shows that overall we are 3 orders of magnitude
faster than SDP-FO while still achieving consistently higher veriﬁed accuracy on average.
Tightness of veriﬁcation.
In Figure 2, we compare the tightness of veriﬁcation bounds against
SDP-FO on two adversarially trained networks from [9]. Speciﬁcally, we use the veriﬁcation objective

8

100101102103CIFAR-10 Base: Running time (in s)0%20%40%60%80%100%-CROWN FSB-CROWN BaBSROVAL (VNN-Comp 20')ERAN (VNN-Comp 20')BaBSRMIPplanetGNN-OnlineBaDNB(BDD+ FSB)BDD+ BaBSRA.Set BaBSRBig-M+A.Set BaBSRFast-and-Complete100101102103CIFAR-10 Wide: Running time (in s)0%20%40%60%80%100%100101102103CIFAR-10 Deep: Running time (in s)0%20%40%60%80%100%Table 2: Veriﬁed accuracy (%) and avg. time (s) of 1000 images evaluated on the ERAN models in [35, 37, 26].
kPoly, OptC2V and PRIMA are strong incomplete veriﬁers that can break the convex relaxation barrier [30].
The average time reported by us excludes examples that are classiﬁed incorrectly.

CROWN/DeepPoly∗[36]

kPoly [35]

OptC2V [37]

PRIMA† [26]

Time (s) Ver.% Time(s) Ver.% Time(s) Ver.% Time(s) Ver.% Time(s)

β-CROWN FSB Upper
bound

MNIST

Dataset
Model
(Same settings as [35, 37, 26]) Veriﬁed%
MLP 5 × 100‡
MLP 8 × 100
MLP 5 × 200
MLP 8 × 200
ConvSmall
ConvBig
ConvSmall
ConvBig
ResNet

16.0
18.2
29.2
25.9
15.8
71.1
35.9
42.1
24.1

CIFAR

0.7
1.4
2.4
5.6
3
21
4
43
1

44.1
36.9
57.4
50.6
34.7
73.6
39.9
45.9
24.5

307
171
187
464
477
40
86
346
91

137
42.9
759
38.4
403
60.1
3451
52.8
55
43.6
102
77.1
39.8
105
No public code
cannot run

51.0
42.8
69.0
62.4
59.8
77.5
44.6
48.3
24.8

159
301
224
395
42
11
13
176
1.7

69.9
62.0
77.4
73.5
72.7
79.3
46.3
51.6
24.8

102
103
86
95
7.0
3.1
6.8
15.3
1.6

84.2
82.0
90.1
91.1
73.2
80.4
48.1
55.0
24.8

* CROWN/DeepPoly evaluated on CPU. † PRIMA is a concurrent work and its results are from [26] (Oct 26, 2021 version), except that ResNet results
‡ Because these MLP models are fairly small, some
are from personal communications with the authors due to a different input normalization used.
of their intermediate layer bounds are computed by mixed integer programming (MIP) using 80% time budget before branch and bound starts and
β-CROWN FSB is used during the branch and bound process. We ﬁnd that tighter intermediate bounds by MIP is beneﬁcial for these small MLP models.

Table 3: Veriﬁed accuracy (%) and avg. per-example veriﬁcation time (s) on 7 models from SDP-FO [9].
CROWN/DeepPoly are fast but loose bound propagation based methods, and they cannot be improved with
more running time. SDP-FO uses stronger semideﬁnite relaxations, which can be very slow and sometimes has
convergence issues. PRIMA, a concurrent work, is the state-of-the-art relaxation barrier breaking method; we
did not include kPoly and OptC2V because they are weaker than PRIMA (see Table 2).

Dataset
(cid:15) = 0.3 and (cid:15) = 2/255

Model

MNIST

CIFAR

CNN-A-Adv
CNN-B-Adv
CNN-B-Adv-4
CNN-A-Adv
CNN-A-Adv-4
CNN-A-Mix
CNN-A-Mix-4

β-CROWN FSB
CROWN/DeepPoly
Veriﬁed% Time (s) Ver.% Time(s) Ver.% Time(s) Ver.% Time(s)

PRIMA [26]

SDP-FO [9]∗

1.0
21.5
43.5
35.5
41.5
23.5
38.0

0.1
0.5
0.9
0.6
0.7
0.4
0.5

43.4
32.8
46.0
39.6
40.0
39.6
47.8

>20h
>25h
>25h
>25h
>25h
>25h
>25h

44.5
38.0
53.5
41.5
45.0
37.5
48.5

135.9
343.6
43.8
4.8
4.9
34.3
7.0

70.5
46.5
54.0
44.0
46.0
41.5
50.5

21.1
32.2
11.6
5.8
5.6
49.6
5.9

Upper
bound

76.5
65.0
63.5
50.0
49.5
53.0
57.5

*

SDP-FO results are directly from their paper due to its very long running time (>20h per example).

† PRIMA experiments
were done using commit 396dc7a, released on June 4, 2021. PRIMA and β-CROWN FSB results are on the same set of 200
examples (ﬁrst 200 examples of CIFAR-10 dataset) and we don’t run veriﬁers on examples that are classiﬁed incorrectly or can
be attacked by a 200-step PGD. β-CROWN uses 1 GPU and 1 CPU; PRIMA uses 1 GPU and 20 CPUs.

y

(x) − z(L)

f (x) := z(L)
y(cid:48) (x), where z(L) is the logit layer output, y and y(cid:48) are the true label and the
runner-up label. For each test image, a 200-step PGD attack [24] provides an adversarial upper bound
f of the optimal objective: f ∗ ≤ f . Veriﬁers, on the other hand, can provide a veriﬁed lower bound
f ≤ f ∗. Bounds from tighter veriﬁcation methods lie closer to line y = x in Figure 2. Figure 2 shows
that on both PGD adversarially trained networks, β-CROWN FSB consistently outperforms SDP-FO
for all 100 random test images. Importantly, for each point on the plots, β-CROWN FSB needs 3
minutes while SDP-FO needs 178 minutes on average. LP veriﬁer with triangle relaxations produces
much looser bounds than β-CROWN FSB and SDP-FO. Additional results are in Appendix C.2.

VNN-COMP 2021 results. We encourage the readers to checkout the report of the Second Inter-
national Veriﬁcation of Neural Networks Competition (VNN-COMP 2021) [3] with 9 additional
benchmarks and 12 competing methods evaluated in a standardized testing environment on AWS. Our
entry α,β-CROWN is based on the β-CROWN algorithm in this work and uses the same codebase.

5 Related Work
Many early complete veriﬁers for neural networks relied on existing solvers such as MILP or SMT
solvers [20, 15, 19, 12, 38] and were limited to very small problem instances. Branch and bound (BaB)

(a) MNIST CNN-A-Adv, runner-up targets, (cid:15) = 0.3

(b) CIFAR CNN-B-Adv, runner-up targets, (cid:15) = 2/255

Figure 2: Veriﬁed lower bound v.s. PGD adversarial upper bound. A lower bound closer to the upper bound
(closer to the line y = x) is better. β-CROWN FSB uses 3mins while SDP-FO needs 2 to 3 hours per point.

9

20246PGD adversarial upper bound1086420246Verified lower bound-CROWNLPSDP-FOPGD (y=x)101234567PGD adversarial upper bound864202468Verified lower bound-CROWNLPSDP-FOPGD (y=x)based method was proposed to better exploit the network structure using LP-based incomplete veriﬁer
for bounding and ReLU splits for branching [8, 40, 23, 5]. Besides branching on ReLU neurons,
input domain branching was also considered in [41, 29, 1] but limited by input dimensions [8].

Recently, a few approaches have been proposed to use efﬁcient iterative solvers or bound propagation
methods on GPUs without relying on LP solvers. Bunel et al. [6] decomposed the veriﬁcation
problem layer by layer, solved each layer in a closed form on GPUs, and used Lagrangian to
enforce consistency between layers. However, their formulation only has the same power as LP
and needs many iterations to converge. De Palma et al. [10] used a dual-space veriﬁer with a linear
relaxation [2, 37] tighter than triangle LP relaxation, but in most settings the extra computational costs
and difﬁculties for an efﬁcient implementation offset its beneﬁts (more discussions in section B.2). A
concurrent work BaDNB [11] proposed a new branching strategy, ﬁltered smart branching (FSB),
combined with Lagrangian decomposition to get better veriﬁcation performance. Xu et al. [44] used
CROWN as a massively paralleled incomplete solver on GPUs for complete veriﬁcation, but it cannot
handle neuron split constraints, leading to suboptimal efﬁciency and high timeout rates.

For incomplete veriﬁcation, Salman et al. [30] shows the inherent limitation of using per-neuron
convex relaxations for veriﬁcation problems. Singh et al. [35] and Müller et al. [26] broke this barrier
by considering constraints involving multiple ReLU neurons; Tjandraatmadja et al. [37] proposed to
relax a linear layer with a ReLU neuron together using a strong mixed-integer programming formu-
lation [1]. SDP based relaxations [28, 16, 14] typically produce tight bounds but with signiﬁcantly
higher cost. The most recent GPU based SDP veriﬁer [9] is still relatively slow and can take 2 hours
to verify a single image. In this work, we impose neuron split constraints using β-CROWN and
combine it with branch and bound done in parallel on GPUs. Although for each subdomain in BaB,
β-CROWN is still subject to the convex relaxation barrier, the efﬁciency of β-CROWN BaB allows
it to quickly explore a very large number of subdomains and outperform existing convex barrier
breaking incomplete veriﬁers under many scenarios in both runtime and tightness.

Additionally, another line of works train networks to enhance veriﬁed accuracy, typically using cheap
incomplete veriﬁers at training time [42, 39, 25, 43, 18, 25, 47, 4, 32]. Traditionally only these
veriﬁcation-customized networks can have reasonable veriﬁed accuracy, while β-CROWN BaB can
also give non-trivial veriﬁed accuracy on relatively large networks agnostic to veriﬁcation.

6 Conclusion

We proposed β-CROWN, a new bound propagation method that can fully encode the neuron split
constraints introduced in BaB, which clearly leads in both complete and incomplete veriﬁcation
settings. The success of β-CROWN comes from a few factors: (1) In Section 3.1, we show that β-
CROWN is an GPU-friendly bound propagation algorithm signiﬁcantly faster than LP solvers. (2) In
Section 3.2, we show that β-CROWN is solving an equivalent problem of the LP veriﬁer with neuron
split constraints. (3) In Section 3.3, we show that β-CROWN can jointly optimize intermediate layer
bounds and achieve tighter bounds than typical LP veriﬁers using ﬁxed intermediate layer bounds.

Limitations. Our veriﬁer has several limitations which are commonly shared by most existing
BaB-based complete veriﬁers. First, we focused on ReLU which can be split into two linear cases.
For other non-piecewise linear activation functions, although it is still possible to conduct branch and
bound, it is difﬁcult to guarantee completeness. Second, we discussed only the norm perturbations
for input domains. In practice, the threat model may involve complicated and nonconvex perturbation
speciﬁcations. Third, although our GPU accelerated veriﬁer outperforms existing ones, all BaB
based veriﬁers, including ours, are still limited to relatively small models far from the ImageNet
scale. Finally, we have only demonstrated robustness veriﬁcation of image classiﬁcation tasks, and
generalizing it to give veriﬁcation guarantees for other tasks such as robust deep reinforcement
learning [27, 48, 49] is an interesting direction for future work.

Societal Impact NNs have been used in an increasingly wide range of real-world applications and
play an important role in artiﬁcial intelligence (AI). The trustworthiness and robustness of NNs have
become crucial factors since AI plays an important role in modern society. β-CROWN is a strong
neural network veriﬁer which can be used to check certain properties of neural networks, which can
be helpful for guaranteeing the robustness, correctness, and fairness of NNs in applications that can
directly or indirectly impact human life. We believe our work has overall positive societal impacts,
although it may potentially be misused to identify the weakness of NNs and guide attacks.

10

Acknowledgement

This work is supported by NSF grant CNS18-01426; an ARL Young Investigator (YIP) award; an
NSF CAREER award; a Google Faculty Fellowship; a Capital One Research Grant; and a J.P. Morgan
Faculty Award; Air Force Research Laboratory under FA8750-18-2-0058; NSF IIS-1901527, NSF
IIS-2008173 and NSF CAREER-2048280; and NSF CNS-1932351. Huan Zhang is supported by
funding from the Bosch Center for Artiﬁcial Intelligence.

References

[1] G. Anderson, S. Pailoor, I. Dillig, and S. Chaudhuri. Optimization and abstraction: A synergistic
approach for analyzing neural network robustness. In Proceedings of the 40th ACM SIGPLAN
Conference on Programming Language Design and Implementation (PLDI), 2019.

[2] R. Anderson, J. Huchette, C. Tjandraatmadja, and J. P. Vielma. Strong convex relaxations
and mixed-integer programming formulations for trained neural networks. Mathematical
Programming, 2020.

[3] S. Bak, C. Liu, and T. Johnson. The second international veriﬁcation of neural networks
competition (vnn-comp 2021): Summary and results. arXiv preprint arXiv:2109.00498, 2021.

[4] M. Balunovic and M. Vechev. Adversarial training and provable defenses: Bridging the gap. In

International Conference on Learning Representations (ICLR), 2020.

[5] E. Botoeva, P. Kouvaros, J. Kronqvist, A. Lomuscio, and R. Misener. Efﬁcient veriﬁcation
In AAAI Conference on Artiﬁcial

of relu-based neural networks via dependency analysis.
Intelligence (AAAI), 2020.

[6] R. Bunel, A. De Palma, A. Desmaison, K. Dvijotham, P. Kohli, P. H. S. Torr, and M. P. Kumar.
Lagrangian decomposition for neural network veriﬁcation. Conference on Uncertainty in
Artiﬁcial Intelligence (UAI), 2020.

[7] R. Bunel, J. Lu, I. Turkaslan, P. Kohli, P. Torr, and P. Mudigonda. Branch and bound for
piecewise linear neural network veriﬁcation. Journal of Machine Learning Research (JMLR),
2020.

[8] R. R. Bunel, I. Turkaslan, P. Torr, P. Kohli, and P. K. Mudigonda. A uniﬁed view of piecewise
linear neural network veriﬁcation. In Advances in Neural Information Processing Systems
(NeurIPS), 2018.

[9] S. Dathathri, K. Dvijotham, A. Kurakin, A. Raghunathan, J. Uesato, R. R. Bunel, S. Shankar,
J. Steinhardt, I. Goodfellow, P. S. Liang, et al. Enabling certiﬁcation of veriﬁcation-agnostic
networks via memory-efﬁcient semideﬁnite programming. Advances in Neural Information
Processing Systems (NeurIPS), 2020.

[10] A. De Palma, H. S. Behl, R. Bunel, P. H. S. Torr, and M. P. Kumar. Scaling the convex barrier
with active sets. International Conference on Learning Representations (ICLR), 2021.

[11] A. De Palma, R. Bunel, A. Desmaison, K. Dvijotham, P. Kohli, P. H. Torr, and M. P. Kumar.
Improved branch and bound for neural network veriﬁcation via lagrangian decomposition. arXiv
preprint arXiv:2104.06718, 2021.

[12] S. Dutta, S. Jha, S. Sankaranarayanan, and A. Tiwari. Output range analysis for deep feedforward

neural networks. In NASA Formal Methods Symposium, 2018.

[13] K. Dvijotham, R. Stanforth, S. Gowal, T. Mann, and P. Kohli. A dual approach to scalable
veriﬁcation of deep networks. Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 2018.

[14] K. D. Dvijotham, R. Stanforth, S. Gowal, C. Qin, S. De, and P. Kohli. Efﬁcient neural
network veriﬁcation with exactness characterization. In Conference on Uncertainty in Artiﬁcial
Intelligence (UAI), 2020.

11

[15] R. Ehlers. Formal veriﬁcation of piece-wise linear feed-forward neural networks. In Interna-
tional Symposium on Automated Technology for Veriﬁcation and Analysis (ATVA), 2017.

[16] M. Fazlyab, M. Morari, and G. J. Pappas. Safety veriﬁcation and robustness analysis of neural
IEEE Transactions on

networks via quadratic constraints and semideﬁnite programming.
Automatic Control, 2020.

[17] T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov, S. Chaudhuri, and M. Vechev. Ai2:
Safety and robustness certiﬁcation of neural networks with abstract interpretation. In 2018
IEEE Symposium on Security and Privacy (SP). IEEE, 2018.

[18] S. Gowal, K. Dvijotham, R. Stanforth, R. Bunel, C. Qin, J. Uesato, T. Mann, and P. Kohli. On the
effectiveness of interval bound propagation for training veriﬁably robust models. Proceedings
of the IEEE International Conference on Computer Vision (ICCV), 2019.

[19] X. Huang, M. Kwiatkowska, S. Wang, and M. Wu. Safety veriﬁcation of deep neural networks.

In International Conference on Computer Aided Veriﬁcation (CAV), 2017.

[20] G. Katz, C. Barrett, D. L. Dill, K. Julian, and M. J. Kochenderfer. Reluplex: An efﬁcient smt
solver for verifying deep neural networks. In International Conference on Computer Aided
Veriﬁcation (CAV), 2017.

[21] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. International Conference

on Learning Representations (ICLR), 2015.

[22] C. Liu and T. Johnson. Vnn comp 2020. URL https://sites.google.com/view/vnn20/

vnncomp.

[23] J. Lu and M. P. Kumar. Neural network branching for neural network veriﬁcation. International

Conference on Learning Representation (ICLR), 2020.

[24] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models
In International Conference on Learning Representations

resistant to adversarial attacks.
(ICLR), 2018.

[25] M. Mirman, T. Gehr, and M. Vechev. Differentiable abstract interpretation for provably robust

neural networks. In International Conference on Machine Learning (ICML), 2018.

[26] M. N. Müller, G. Makarchuk, G. Singh, M. Püschel, and M. Vechev. Precise multi-neuron

abstractions for neural network certiﬁcation. arXiv preprint arXiv:2103.03638, 2021.

[27] A. Pattanaik, Z. Tang, S. Liu, G. Bommannan, and G. Chowdhary. Robust deep reinforcement

learning with adversarial attacks. arXiv preprint arXiv:1712.03632, 2017.

[28] A. Raghunathan, J. Steinhardt, and P. S. Liang. Semideﬁnite relaxations for certifying robustness
to adversarial examples. In Advances in Neural Information Processing Systems (NeurIPS),
2018.

[29] V. R. Royo, R. Calandra, D. M. Stipanovic, and C. Tomlin. Fast neural network veriﬁcation via

shadow prices. arXiv preprint arXiv:1902.07247, 2019.

[30] H. Salman, G. Yang, H. Zhang, C.-J. Hsieh, and P. Zhang. A convex relaxation barrier to
tight robustness veriﬁcation of neural networks. In Advances in Neural Information Processing
Systems (NeurIPS), 2019.

[31] Z. Shi, H. Zhang, K.-W. Chang, M. Huang, and C.-J. Hsieh. Robustness veriﬁcation for
transformers. In International Conference on Learning Representations (ICLR), 2020.

[32] Z. Shi, Y. Wang, H. Zhang, J. Yi, and C.-J. Hsieh. Fast certiﬁed robust training via better initial-
ization and shorter warmup. Advances in Neural Information Processing Systems (NeurIPS),
2021.

[33] G. Singh, T. Gehr, M. Mirman, M. Püschel, and M. Vechev. Fast and effective robustness
certiﬁcation. In Advances in Neural Information Processing Systems (NeurIPS), 2018.

12

[34] G. Singh, T. Gehr, M. Püschel, and M. Vechev. Boosting robustness certiﬁcation of neural

networks. In International Conference on Learning Representations, 2018.

[35] G. Singh, R. Ganvir, M. Püschel, and M. Vechev. Beyond the single neuron convex barrier for
neural network certiﬁcation. In Advances in Neural Information Processing Systems (NeurIPS),
2019.

[36] G. Singh, T. Gehr, M. Püschel, and M. Vechev. An abstract domain for certifying neural

networks. Proceedings of the ACM on Programming Languages (POPL), 2019.

[37] C. Tjandraatmadja, R. Anderson, J. Huchette, W. Ma, K. Patel, and J. P. Vielma. The convex
relaxation barrier, revisited: Tightened single-neuron relaxations for neural network veriﬁcation.
Advances in Neural Information Processing Systems (NeurIPS), 2020.

[38] V. Tjeng, K. Xiao, and R. Tedrake. Evaluating robustness of neural networks with mixed integer

programming. International Conference on Learning Representations (ICLR), 2019.

[39] S. Wang, Y. Chen, A. Abdou, and S. Jana. Mixtrain: Scalable training of formally robust neural

networks. arXiv preprint arXiv:1811.02625, 2018.

[40] S. Wang, K. Pei, J. Whitehouse, J. Yang, and S. Jana. Efﬁcient formal safety analysis of neural

networks. In Advances in Neural Information Processing Systems (NeurIPS), 2018.

[41] S. Wang, K. Pei, J. Whitehouse, J. Yang, and S. Jana. Formal security analysis of neural

networks using symbolic intervals. In USENIX Security Symposium, 2018.

[42] E. Wong and Z. Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In International Conference on Machine Learning (ICML), 2018.

[43] E. Wong, F. Schmidt, J. H. Metzen, and J. Z. Kolter. Scaling provable adversarial defenses. In

Advances in Neural Information Processing Systems (NeurIPS), 2018.

[44] K. Xu, Z. Shi, H. Zhang, Y. Wang, K.-W. Chang, M. Huang, B. Kailkhura, X. Lin, and C.-J.
Hsieh. Automatic perturbation analysis for scalable certiﬁed robustness and beyond. Advances
in Neural Information Processing Systems (NeurIPS), 2020.

[45] K. Xu, H. Zhang, S. Wang, Y. Wang, S. Jana, X. Lin, and C.-J. Hsieh. Fast and complete:
Enabling complete neural network veriﬁcation with rapid and massively parallel incomplete
veriﬁers. International Conference on Learning Representations (ICLR), 2021.

[46] H. Zhang, T.-W. Weng, P.-Y. Chen, C.-J. Hsieh, and L. Daniel. Efﬁcient neural network
robustness certiﬁcation with general activation functions. In Advances in Neural Information
Processing Systems (NeurIPS), 2018.

[47] H. Zhang, H. Chen, C. Xiao, B. Li, D. Boning, and C.-J. Hsieh. Towards stable and efﬁ-
cient training of veriﬁably robust neural networks. In International Conference on Learning
Representations (ICLR), 2020.

[48] H. Zhang, H. Chen, C. Xiao, B. Li, M. Liu, D. Boning, and C.-J. Hsieh. Robust deep rein-
forcement learning against adversarial perturbations on state observations. Advances in Neural
Information Processing Systems (NeurIPS), 2020.

[49] H. Zhang, H. Chen, D. Boning, and C.-J. Hsieh. Robust reinforcement learning on state obser-
vations with learned optimal adversary. International Conference on Learning Representations
(ICLR), 2021.

13

A Proofs for β-CROWN

A.1 Proofs for deriving β-CROWN using bound propagation

Lemma 2.1 is from part of the proof of the main theorem in Zhang et al. [46]. Here we present it
separately to use it as an useful subprocedure for our later proofs.
Lemma 2.1 (Relaxation of a ReLU layer in CROWN). Given two vectors w, v ∈ Rd, l ≤ v ≤ u
(element-wise), we have

w(cid:62)ReLU(v) ≥ w(cid:62)Dv + b(cid:48),

where D is a diagonal matrix deﬁned as:

Dj,j =




0 ≤ αj ≤ 1 are free variables, b(cid:48) = w(cid:62)b and each element in b is



if lj ≥ 0
if uj ≤ 0
if uj > 0 > lj and wj ≥ 0
if uj > 0 > lj and wj < 0,

1,
0,
αj,
uj
uj −lj

,

0,
0,
− uj lj
uj −lj

,



if lj > 0 or uj ≤ 0
if uj > 0 > lj and wj ≥ 0
if uj > 0 > lj and wj < 0.

bj =

(1)

(2)

Proof. For the j-th ReLU neuron, if lj ≥ 0, then ReLU(vj) = vj; if uj < 0, then ReLU(vj) = 0.
For the case of lj < 0 < uj, the ReLU function can be linearly upper and lower bounded within this
range:

αjvj ≤ ReLU(vj) ≤

(vj − lj)

∀ lj ≤ vj ≤ uj

uj
uj − lj

where 0 ≤ αj ≤ 1 is a free variable - any value between 0 and 1 produces a valid lower bound.
To lower bound w(cid:62)ReLU(v) = (cid:80)
j wjReLU(vj), for each term in this summation, we take the
lower bound of ReLU(vj) if wj is positive and take the upper bound of ReLU(vj) if wj is negative
(reﬂected in the deﬁnitions of D and b). This conservative choice allows us to always obtain a lower
bound ∀ l ≤ v ≤ u:

(cid:88)

j

wjReLU(vj) ≥

(cid:88)

j

wj

(cid:0)Dj,jvj + bj

(cid:1) = w(cid:62)Dv + w(cid:62)b = w(cid:62)Dv + b(cid:48)

where Dj,j and bj are deﬁned in Eq. 1 and Eq. 2 representing the lower or upper bounds of ReLU.

Before proving our main theorem (Theorem 3.1), we ﬁrst deﬁne matrix Ω, which is the product of a
series of model weights W and “weights” for relaxed ReLU layers D:
Deﬁnition A.1. Given a set of matrices W(2), · · · , W(L) and D(1), · · · , D(L−1), we deﬁne a recur-
sive function Ω(k, i) for 1 ≤ i ≤ k ≤ L as

Ω(i, i) = I, Ω(k+1, i) = W(k+1)D(k)Ω(k, i)

For example, Ω(3, 1) = W(3)D(2)W(2)D(1), Ω(5, 2) = W(5)D(4)W(4)D(3)W(3)D(2). Now we
present our main theorem with each term explicitly written:
Theorem 3.1 (β-CROWN bound). Given a L-layer neural network f (x) : Rd0 → R with weights
W(i), biases b(i), pre-ReLU bounds l(i) ≤ z(i) ≤ u(i) (1 ≤ i ≤ L), input constraint C and split
constraint Z. We have

min
x∈C,z∈Z

f (x) ≥ max
β≥0

min
x∈C

(a + Pβ)(cid:62)x + q(cid:62)β + c,

(3)

where P ∈ Rd0×((cid:80)L−1
R(cid:80)L−1

i=1 di is a vector q := (cid:2)q1

i=1 di) is a matrix containing blocks P :=
(cid:62)(cid:3)(cid:62)

(cid:62) · · · qL−1

, and each term is deﬁned as:

(cid:104)
P1

(cid:62) P2

(cid:62) · · · PL−1

(cid:62)(cid:105)

, q ∈

(cid:104)

Ω(L, 1)W(1)(cid:105)(cid:62)

a =

∈ Rd0×1

(4)

14

Pi = S(i)Ω(i, 1)W(1) ∈ Rdi×d0,

∀ 1 ≤ i ≤ L − 1

(5)

qi =

i
(cid:88)

k=1

S(i)Ω(i, k)b(k) +

i
(cid:88)

k=2

S(i)Ω(i, k)W(k)b(k−1) ∈ Rdi,

∀ 1 ≤ i ≤ L − 1

(6)

c =

L
(cid:88)

i=1

Ω(L, i)b(i) +

L
(cid:88)

i=2

Ω(L, i)W(i)b(i−1)

(7)

diagonal matrices D(i) and vector b(i) are determined by the relaxation of ReLU neurons, and
A(i) ∈ R1×di represents the linear relationship between f (x) and ˆz(i). D(i) and b(i) depend on
A(i), l(i) and u(i):

D(i)

j,j =






1,
0,
αj,
uj
uj −lj

,

if l(i)
if u(i)
if u(i)
if u(i)

j ≥ 0 or j ∈ Z +(i)
j ≤ 0 or j ∈ Z −(i)
j > 0 > l(i)
j > 0 > l(i)

j and j /∈ Z +(i) ∪ Z −(i) and A(i)
j and j /∈ Z +(i) ∪ Z −(i) and A(i)

1,j ≥ 0
1,j < 0

b(i)

j =


0,

0,

−

A(i) =

if l(i)
if u(i)
if u(i)

j > 0 or u(i)
j > 0 > l(i)
j > 0 > l(i)

j ≤ 0 or j ∈ Z +(i) ∪ Z −(i)
j and j /∈ Z +(i) ∪ Z −(i) and A(i)
j and j /∈ Z +(i) ∪ Z −(i) and A(i)

1,j ≥ 0
1,j < 0

i = L − 1

j

,

j l(i)
u(i)
u(i)
j −l(i)
(cid:26)W(L),

j

(A(i+1)D(i+1) + β(i+1)(cid:62)S(i+1))W(i+1), 0 ≤ i ≤ L − 2

(8)

(9)

(10)

Proof. We prove this theorem by induction: assuming we know the bounds with respect to layer ˆz(m),
we derive bounds for ˆz(m−1) until we reach m = 0 and by deﬁnition ˆz(0) = x. We ﬁrst deﬁne a set of
matrices and vectors a(m), P(m), q(m), c(m), where P(m) ∈ Rdm×((cid:80)L−1
i=m+1 di) is a matrix containing
(cid:62)(cid:21)
(cid:104)
(cid:62) · · · q(m)
q(m)
m+1
L−1

i=m+1 di is a vector q :=

, q ∈ R(cid:80)L−1

(cid:20)
P(m)
m+1

· · · P(m)
L−1

blocks P :=

(cid:62)(cid:105)(cid:62)

, and

(cid:62)

each term is deﬁned as:

a(m) =

(cid:104)
Ω(L, m + 1)W(m+1)(cid:105)(cid:62)

∈ Rdm×1

P(m)

i = S(i)Ω(i, m + 1)W(m+1) ∈ Rdi×dm,

∀ m + 1 ≤ i ≤ L − 1

(11)

(12)

q(m)
i =

i
(cid:88)

k=m+1

S(i)Ω(i, k)b(k) +

i
(cid:88)

k=m+2

S(i)Ω(i, k)W(k)b(k−1) ∈ Rdm,

∀ m + 1 ≤ i ≤ L − 1

L
(cid:88)

c(m) =

Ω(L, i)b(i) +

L
(cid:88)

Ω(L, i)W(i)b(i−1)

i=m+1

i=m+2

and we claim that

(13)

(14)

min
x∈C
z∈Z

f (x) ≥ max

˜β(m+1)≥0

min
x∈C
˜z(m)∈ ˜Z (m)

(a(m) + P(m) ˜β(m+1))(cid:62) ˆz(m) + q(m)(cid:62) ˜β(m+1) + c(m)

(15)

where ˜β(m+1) := (cid:2)β(m+1)(cid:62) · · · β(L−1)(cid:62)(cid:3)(cid:62)

concatenating all β(i) variables up to layer m + 1.

15

For the base case m = L − 1, we simply have

min
x∈C,z∈Z

f (x) = min

x∈C,z∈Z

W(L) ˆz(L−1) + b(L).

No maximization is needed and a(m) = (cid:2)Ω(L, L)W(L)(cid:3)(cid:62)
b(L). Other terms are zero.

= W(L)(cid:62)

, c(m) = (cid:80)L

i=L Ω(L, i)b(i) =

In Section 3.1 we have shown the intuition of the proof by demonstrating how to derive the bounds
from layer ˆz(L−1) to ˆz(L−2). The case for m = L − 2 is presented in Eq. 6.

Now we show the induction from ˆz(m) to ˆz(m−1). Starting from Eq. 15, since ˆz(m) = ReLU(z(m))
a(m) + P(m) ˜β(m+1)(cid:105)(cid:62)
(cid:104)
:= A(m). It is easy to show that A(m)
we apply Lemma 2.1 by setting w =
can also be equivalently and recursively deﬁned in Eq. 10 (see Lemma A.2). Based on Lemma 2.1
we have D(m) and b(m) deﬁned as in Eq. 8 and Eq. 9, so Eq. 15 becomes

min
x∈C
z∈Z

f (x) ≥ max

˜β(m+1)≥0

min
x∈C
˜z(m)∈ ˜Z (m)

(a(m) + P(m) ˜β(m+1))(cid:62)D(m)z(m)

(16)

+ (a(m) + P(m) ˜β(m+1))(cid:62)b(m) + q(m)(cid:62) ˜β(m+1) + c(m)

Note that when we apply Lemma 2.1, for j ∈ Z +(i) (positive split) we simply treat the neuron j as if
j ≥ 0, and for j ∈ Z −(i) (negative split) we simply treat the neuron j as if u(i)
l(i)
j ≤ 0. Now we add
the multiplier β(m) to z(m) to enforce per-neuron split constraints:

min
x∈C
z∈Z

f (x) ≥ max

˜β(m+1)≥0

min
x∈C
˜z(m−1)∈ ˜Z (m−1)

max
β(m)≥0

(a(m) + P(m) ˜β(m+1))(cid:62)D(m)z(m) + β(m)(cid:62)S(m)z(m)

≥ max
˜β(m)≥0

min
x∈C
˜z(m−1)∈ ˜Z (m−1)

+(a(m) + P(m) ˜β(m+1))(cid:62)b(m) + q(m)(cid:62) ˜β(m+1) + c(m)

(a(m)(cid:62)

D(m) + ˜β(m+1)(cid:62)P(m)(cid:62)

D(m) + β(m)(cid:62)S(m))z(m)

+(a(m) + P(m) ˜β(m+1))(cid:62)b(m) + q(m)(cid:62) ˜β(m+1) + c(m)

Similar to what we did in Eq. 5, we swap the min and max in the second inequality due to weak duality,
such that every maximization on β(i) is before min. Then, we substitute ˆz(m) = W(m) ˆz(m−1) +b(m)
and obtain:

min
x∈C
z∈Z

f (x) ≥ max
˜β(m)≥0

min
x∈C
˜z(m−1)∈ ˜Z (m−1)

(a(m)(cid:62)

D(m) + ˜β(m+1)(cid:62)P(m)(cid:62)

D(m) + β(m)(cid:62)S(m))(cid:62)W(m) ˆz(m−1)

D(m) + β(m)(cid:62)S(m))(cid:62)b(m)

D(m) + ˜β(m+1)(cid:62)P(m)(cid:62)

+ (a(m)(cid:62)
+ (a(m) + P(m) ˜β(m+1))(cid:62)b(m) + q(m)(cid:62) ˜β(m+1) + c(m)

(cid:104)
a(m)(cid:62)


(cid:124)
(cid:16)
(P(m)(cid:62)

(cid:123)(cid:122)
a(cid:48)
D(m)b(m) + P(m)(cid:62)

D(m)W(m)(cid:105)(cid:62)
(cid:125)

+ ( ˜β(m+1)(cid:62)P(m)(cid:62)

(cid:123)(cid:122)
P(cid:48) ˜β(m)

+

(cid:124)

=

D(m)W(m) + β(m)(cid:62)S(m)W(m))


(cid:125)
b(m) + q(m))(cid:62) ˜β(m+1) + (S(m)b(m))(cid:62)β(m)(cid:17)
(cid:125)

(cid:123)(cid:122)
q(cid:48)(cid:62) ˜β(m)



(cid:62)

ˆz(m−1)

(cid:124)

+ a(m)(cid:62)
(cid:124)

D(m)b(m) + a(m)(cid:62)

(cid:123)(cid:122)
c(cid:48)

b(m) + c(m)
(cid:125)

16

Now we evaluate each term a(cid:48), P(cid:48), q(cid:48) and c(cid:48) and show the induction holds. For a(cid:48) and q(cid:48) we have:

a(cid:48) =

(cid:104)
a(m)(cid:62)

D(m)W(m)(cid:105)(cid:62)

(cid:104)

Ω(L, m + 1)W(m+1)D(m)W(m)(cid:105)(cid:62)

=

Ω(L, m)W(m)(cid:105)(cid:62)
(cid:104)

=

= a(m−1)

c(cid:48) = c(m) + Ω(L, m + 1)W(m+1)D(m)b(m) + Ω(L, m + 1)W(m+1)b(m)

L
(cid:88)

Ω(L, i)b(i) +

L
(cid:88)

i=m+1

i=m+2

Ω(L, i)W(i)b(i−1) + Ω(L, m)b(m) + Ω(L, m + 1)W(m+1)b(m)

=

=

i=m
= c(m−1)

L
(cid:88)

Ω(L, i)b(i) +

L
(cid:88)

i=m+1

Ω(L, i)W(i)b(i−1)

For P(cid:48) := (cid:2)P(cid:48)

m

(cid:62) · · · P(cid:48)

L−1

(cid:62)(cid:3), we have a new block P(cid:48)

m where

P(cid:48)

m = S(m)W(m) = S(m)Ω(m, m)W(m) = P(m−1)

m

for other blocks where m + 1 ≤ i ≤ L − 1,

P(cid:48)

i = P(m)

i D(m)W(m) = S(i)Ω(i, m + 1)W(m+1)D(m)W(m) = S(i)Ω(i, m)W(m) = P(m−1)

i

For q(cid:48) := (cid:2)q(cid:48)

m

(cid:62) · · · q(cid:48)

L−1

(cid:62)(cid:3), we have a new block q(cid:48)

m where

m = S(m)b(m) =
q(cid:48)

m
(cid:88)

k=m

S(i)Ω(i, k)b(i) = q(m−1)

m

for other blocks where m + 1 ≤ i ≤ L − 1,

i = q(m)
q(cid:48)

i + P(m)(cid:62)
i
(cid:88)

D(m)b(m) + P(m)(cid:62)

b(m)

S(i)Ω(i, k)b(k) +

k=m+1

i
(cid:88)

S(i)Ω(i, k)b(k) +

i
(cid:88)

k=m+2

i
(cid:88)

S(i)Ω(i, k)W(k)b(k−1) + P(m)(cid:62)

D(m)b(m) + P(m)(cid:62)

b(m)

S(i)Ω(i, k)W(k)b(k−1)

k=m+1
+ S(i)Ω(i, m + 1)W(m+1)D(m)b(m) + S(i)Ω(i, m + 1)W(m+1)b(k)

k=m+2

i
(cid:88)

S(i)Ω(i, k)b(k) +

i
(cid:88)

k=m+1
k=m+2
+ S(i)Ω(i, m + 1)W(m+1)b(m)

S(i)Ω(i, k)W(k)b(k−1) + S(i)Ω(i, m)b(m)

=

=

=

i
(cid:88)

=

S(i)Ω(i, k)b(k) +

k=m
= q(m−1)
i

i
(cid:88)

k=m+1

S(i)Ω(i, k)W(k)b(k−1)

Thus, a(cid:48) = a(m−1), P(cid:48) = P(m−1), q(cid:48) = q(m−1) and c(cid:48) = c(m−1) so the induction holds for layer
ˆz(m−1):

min
x∈C
z∈Z

f (x) ≥ max
˜β(m)≥0

min
x∈C
˜z(m−1)∈ ˜Z (m−1)

(a(m−1) + P(m−1) ˜β(m))(cid:62) ˆz(m−1) + q(m−1)(cid:62) ˜β(m) + c(m−1)

(17)

Finally, Theorem 3.1 becomes the special case where m = 0 in Eq. 11, Eq. 12, Eq. 13 and Eq. 14.

17

The next Lemma unveils the connection with CROWN [46] and is also useful for drawing connections
to the dual problem.

Lemma A.2. With D, b and A deﬁned in Eq. 8, Eq. 9 and Eq. 10, we can rewrite Eq. 3 in Theorem 3.1
as:

f (x) ≥ max
β≥0

min
x∈C

A(0)x +

min
x∈C
z∈Z

L−1
(cid:88)

i=1

A(i)(D(i)b(i) + b(i))

(18)

where A(i), 0 ≤ i ≤ L − 1 contains variables β.

Proof. To prove this lemma, we simply follow the deﬁnition of A(i) and check the resulting terms
are the same as Eq. 3. For example,

A(0) = (A(1)D(1) + β(1)(cid:62)S(1))W(1)

= A(1)D(1)W(1) + β(1)(cid:62)S(1)W(1)
= (A(2)D(2) + β(2)(cid:62)S(2))W(2)D(1)W(1) + β(1)(cid:62)S(1)W(1)
= A(2)D(2)W(2)D(1)W(1) + β(2)(cid:62)S(2)W(2)D(1)W(1) + β(1)(cid:62)S(1)W(1)
= · · ·

= Ω(L, 1)W(1) +

= [a + Pβ](cid:62)

L−1
(cid:88)

i=1

β(i)(cid:62)S(i)Ω(i, 1)W(1)

Other terms can be shown similarly.

With this deﬁnition of A, we can see Eq. 3 as a modiﬁed form of CROWN, with an extra term
β(i+1)(cid:62)S(i+1) added when computing A(i). When we set β = 0, we obtain the same bound
propagation rule for A as in CROWN. Thus, only a small change is needed to implement β-CROWN
given an existing CROWN implementation: we add β(i+1)(cid:62)S(i+1) after the linear bound propagates
backwards through a ReLU layer. We also have the same observation in the dual space, as we will
show this connection in the next subsection.

A.2 Proofs for the connection to the dual space

Theorem 3.2. The objective dLP for the dual problem of Eq. 10 can be represented as

dLP = −(cid:107)a + Pβ(cid:107)1·(cid:15) + (P(cid:62)x0 + q)(cid:62)β + a(cid:62)x0 + c,

where a, P, q and c are deﬁned in the same way as in Theorem 3.1, and β ≥ 0 corresponds to the
dual variables of neuron split constraints in Eq. 10.

Proof. To prove the Theorem 3.2, we demonstrate the detailed dual objective dLP for Eq. 10, following
a construction similar to the one in Wong and Kolter [42]. We ﬁrst associate a dual variable for each
constraint involved in Eq. 10 including dual variables β for the per-neuron split constraints introduced
by BaB. Although it is possible to directly write the dual LP for Eq. 10, for easier understanding, we

18

ﬁrst rewrite the original primal veriﬁcation problem into its Lagrangian dual form as Eq. 19, with
dual variables ν, ξ+, ξ−µ, γ, λ, β:

L(z, ˆz; ν, ξ, µ, γ, λ, β) = z(L) +

L
(cid:88)

ν(i)(cid:62)

(z(i) − W(i) ˆz(i−1) − b(i))

+ ξ+(cid:62)
L−1
(cid:88)

+

(ˆz(0) − x0 − (cid:15)) + ξ−(cid:62)

i=1
(−ˆz(0) + x0 − (cid:15))

(cid:88)

(cid:20)
µ(i)
j

(cid:62)

(−ˆz(i)

j ) + γ(i)

j

(cid:62)

(z(i)

j − ˆz(i)

j ) + λ(i)

j

(cid:62)

(−u(i)

j z(i)

j + (u(i)

j − l(i)

j )ˆz(i)

j + u(i)

j l(i)
j )

(cid:21)

i=1

j /∈Z +(i) (cid:83) Z −(i)
j <0<u(i)
l(i)

j






+

L−1
(cid:88)

i=1

(cid:88)

j z(i)
β(i)

j +

(cid:88)

−β(i)

j z(i)

j

z(i)
j ∈Z −(i)

z(i)
j ∈Z +(i)






Subject to:

ξ+ ≥ 0, ξ− ≥ 0, µ ≥ 0, γ ≥ 0, λ ≥ 0, β ≥ 0

The original minimization problem then becomes:

max
ν,ξ+,ξ−,µ,γ,λ,β

min
z,ˆz

L(z, ˆz, ν, ξ+, ξ−, µ, γ, λ, β)

(19)

Given ﬁxed intermediate bounds l, u, the inner minimization is a linear optimization problem and
we can simply transfer it to the dual form. To further simplify the formula, we introduce notations
similar to those in [42], where ˆν(i−1) = W(i)(cid:62)
be written as Eq. 20.

. Then the dual form can

ν(i) and α(i)

γ(i)
j
j +γ(i)

j =

µ(i)

j

max
0≤α≤1,β≥0

g(α, β), where

g(α, β) = −

L
(cid:88)

i=1

Subject to:

ν(i)(cid:62)b(i) − ˆν(0)(cid:62)x0 − ||ˆν(0)||1·(cid:15) +

L−1
(cid:88)

i=1

(cid:88)

j [ν(i)
l(i)

j

]+

j /∈Z +(i) (cid:83) Z −(i)
j <0<u(i)
l(i)

j

ν(i),

i ∈ {1, . . . , L}

j ≤ 0, i ∈ {1, . . . , L − 1}

j ≥ 0, i ∈ {1, . . . , L − 1}
j [ˆν(i)

]−

ν(L) = −1, ˆν(i−1) = W(i)(cid:62)
j = 0, when u(i)
ν(i)
ν(i)
j = ˆν(i)
[ν(i)
j

, when l(i)
j
]+ = u(i)
j λ(i)
j , [ν(i)
[ˆν (i)
]+
, α(i)
j
u(i)
j −l(i)
j = −β(i)
ν(i)
j ,
j + ˆν(i)
j = β(i)
ν(i)
µ ≥ 0, γ ≥ 0, λ ≥ 0, β ≥ 0, 0 ≤ α ≤ 1

]− = α(i)
γ(i)
j
j +γ(i)

j =

j =

λ(i)

µ(i)

,

j

j

j

j

j

j ∈ Z −(i), i ∈ {1, . . . , L − 1}






j ∈ Z +(i), i ∈ {1, . . . , L − 1}

when l(i)

j < 0 < u(i)

j , j /∈ Z +(i) (cid:91)

Z −(i), i ∈ {1, . . . , L − 1}

Similar to the dual form in [42] (our differences are highlighted in blue), the dual problem can be
viewed in the form of another deep network by backward propagating ν(L) to ˆν(0) following the
rules in Eq. 20. If we look closely at the conditions and coefﬁcients when backward propagating
ν(i)
for j-th ReLU at layer i in Eq. 20, we can observe that they match exactly to the propagation of
j

(20)

19

diagonal matrices D(i), S(i), and vector b(i) deﬁned in Eq. 8 and Eq. 9. Therefore, using notations
in Eq. 8 and Eq. 9 we can essentially simplify the dual LP problem in Eq. 20 to:

ν(L) = −1, ˆν(i−1) = W(i)(cid:62)

(cid:88)

j [ν(i)
l(i)

j

j <0<u(i)
l(i)
j /∈Z +(i) (cid:83) Z −(i)

j

ν(i), ν(i) = D(i) ˆν(i) − β(i)S(i), i ∈ {L, · · · , 1}
]+ = −ˆν(i)T b(i), j ∈ {1, · · · , di}, i ∈ {L − 1, · · · , 1}

(21)

Then we prove the key claim for this proof with induction where a(m) and P(m) are deﬁned in Eq. 11
and Eq. 12:

ˆν(m) = −a(m) − P(m) ˜β(m+1)

(22)

When m = L − 1, we can have ˆν(L−1) = −a(L−1) − P(L−1) ˜β(L) = − (cid:2)Ω(L, L)W(L)(cid:3)(cid:62)
−W(L)(cid:62)
Now we assume that ˆν(m) = −a(m) − P(m) ˜β(m+1) holds, and we show that ˆν(m−1) = −a(m−1) −
P(m−1) ˜β(m) will hold as well:

which is true according to Eq. 21.

− 0 =

ˆν(m−1) = W(m)(cid:62) (cid:16)
= −W(m)(cid:62)

= −a(m−1) −

D(m) ˆν(m) − β(m)S(m)(cid:17)
D(m)P(m) ˜β(m+1) − W(m)(cid:62)
D(m)a(m) − W(m)(cid:62)
D(m)W(m)(cid:17)(cid:62)(cid:21) (cid:104)
(cid:16)
S(m)W(m)(cid:17)(cid:62)

P(m)(cid:62)

(cid:20)(cid:16)

,

β(m)S(m)
β(m), ˜β(m+1)(cid:105)

= −a(m−1) − P(m−1) ˜β(m)

Therefore, the claim Eq. 22 is proved with induction. Lastly, we prove the following claim where
q(m) and c(m) are deﬁned in Eq. 13 and Eq. 14.

L
(cid:88)

−

i=m+1

ν(i)(cid:62)b(i) +

L−1
(cid:88)

i=m+1

(cid:88)

j [ν(i)
l(i)

j

]+ = q(m)(cid:62) ˜β(m+1) + c(m)

(23)

j <0<u(i)
l(i)
j /∈Z +(i) (cid:83) Z −(i)

j

This claim can be proved by applying Eq. 21 and Eq. 22.

L
(cid:88)

−

ν(i)(cid:62)b(i) +

L−1
(cid:88)

(cid:88)

j [ν(i)
l(i)

j

]+

i=m+1

i=m+1

j <0<u(i)
l(i)
j /∈Z +(i) (cid:83) Z −(i)

j

L
(cid:88)

= −

i=m+1

(cid:16)

D(i) ˆν(i) − β(i)S(i)(cid:17)(cid:62)

b(i) +

L
(cid:88)

i=m+2

(cid:16)

−ˆν(i−1)T b(i−1)(cid:17)

L
(cid:88)

=

(cid:104)(cid:16)

a(i)(cid:62)

+ ˜β(i+1)(cid:62)P(i)(cid:62)(cid:17)

D(i)b(i) + β(i)(cid:62)

S(i)b(i)(cid:105)

i=m+1

L
(cid:88)

+

i=m+2

(cid:16)

a(i−1)(cid:62)

+ ˜β(i)(cid:62)P(i−1)(cid:62)(cid:17)

b(i−1)

L
(cid:88)

=

˜β(i)(cid:62) (cid:104)

S(i), P(i)(cid:62)

D(i)(cid:105)

b(i) +

L
(cid:88)

i=m+2

˜β(i)(cid:62)P(i−1)(cid:62)

b(i−1)

i=m+1

L
(cid:88)

+

i=m+1

a(i)(cid:62)

D(i)b(i) +

L
(cid:88)

i=m+2

a(i−1)(cid:62)

b(i−1)

= q(m)(cid:62) ˜β(m+1) + c(m)

20

Finally, we apply claims Eq. 22 and Eq. 23 into the dual form solution Eq. 20 and prove the
Theorem 3.2.

g(α, β) = −

L
(cid:88)

i=1

ν(i)(cid:62)b(i) − ˆν(0)(cid:62)x0 − ||ˆν(0)||1·(cid:15) +

L−1
(cid:88)

i=1

(cid:88)

j [ν(i)
l(i)

j

]+

j <0<u(i)
l(i)
j /∈Z +(i) (cid:83) Z −(i)

j

+ ˜β(1)(cid:62)P(0)(cid:62)(cid:17)
= −||−a(0) − P(0) ˜β(1)||1·(cid:15) +
= −||a + P ˜β(1)||1·(cid:15) + (cid:0)P(cid:62)x0 + q(cid:1)(cid:62) ˜β(1) + a(cid:62)x0 + c

a(0)(cid:62)

(cid:16)

x0 + q(0)(cid:62) ˜β(1) + c(0)

A more intuitive proof. Here we provide another intuitive proof showing why the dual form solution
of veriﬁcation objective in Eq. 20 is the same as the primal one in Thereom 3.1. dLP = g(α, β) is the
dual objective for Eq. 10 with free variables α and β. We want to show that the dual problem can be
viewed in the form of backward propagating ν(L) to ˆν(0) following the same rules in β-CROWN.
Salman et al. [30] showed that CROWN computes the same solution as the dual form in Wong
and Kolter [42]: ˆν(i) is corresponding to −A(i) in CROWN (deﬁned in the same way as in Eq. 10
but with β(i+1) = 0) and ν(i) is corresponding to −A(i+1)D(i+1). When the split constraints are
introduced, extra terms for the dual variable β modify ν(i) (highlighted in blue in Eq. 20). The way
β-CROWN modiﬁes A(i+1)D(i+1) is exactly the same as the way β(i) affects ν(i): when we split
j ≥ 0, we add β(i)
z(i)
to
the ν(i)
is 0 in this case because it is set to be inactive). To make this
j
relationship more clear, we deﬁne a new variable ν(cid:48), and rewrite relevant terms involving ν, ˆν below:

in Wong and Kolter [42]; when we split z(i)

in Wong and Kolter [42] (ν(i)
j

j ≥ 0, we add −β(i)

to the ν(i)
j

j

j

j

,

j ∈ Z +(i);

j ∈ Z −(i);

ν(i)
j = 0,
ν(i)
j = ˆν(i)
ν(i)
is deﬁned in the same way as in Eq. 20 for other cases
j
(cid:48) = −β(i)
ν(i)
j
(cid:48) = β(i)
ν(i)
j
(cid:48) = ν(i)
ν(i)
j
ˆν(i−1) = W(i)(cid:62)

j + ν(i)
,

j + ν(i)

j ∈ Z −(i);

j ∈ Z +(i);

otherwise

ν(i)(cid:48);

,

,

j

j

j

(24)

It is clear that ν(cid:48) corresponds to the term −(A(i+1)D(i+1) + β(i+1)(cid:62)S(i+1)) in Eq. 10, by noting
that ν(i) in [42] is equivalent to −A(i+1)D(i+1) in CROWN and the choice of signs in S(i+1) reﬂects
neuron split constraints. Thus, the dual formulation will produce the same results as Eq. 18, and thus
also equivalent to Eq. 3.

Corollary 3.2.1. When α and β are optimally set, β-CROWN produces the same solution as LP with
split constraints when intermediate bounds l, u are ﬁxed. Formally,

where p∗

LP is the optimal objective of Eq. 10.

max
0≤α≤1,β≥0

g(α, β) = p∗
LP

Proof. Given ﬁxed intermediate layer bounds l and u, the dual form of the veriﬁcation problem
in Eq. 10 is a linear programming problem with dual variables deﬁned in Eq. 19. Suppose we use an LP
solver to obtain the optimal dual solution ν∗, ξ∗, µ∗, γ∗, λ∗, β∗. Then we can set α(i)
β = β∗ and plug them into Eq. 20 to get the optimal dual solution d∗
LP given the same α(i)

∗ ,
LP. Theorem 3.2 shows
∗ , β = β∗,

that, β-CROWN can compute the same objective d∗

γ(i)
j
∗+γ(i)

j =

j =

µ(i)
j

∗

j

∗

γ(i)
j
∗+γ(i)

j

µ(i)
j

21

thus max0≤α≤1,β≥0 g(α, β) ≥ d∗
produces the same solution g(α, β) as the rewritten dual LP in Eq. 20, so g(α, β) ≤ d∗
we have max0≤α≤1,β≥0 g(α, β) = d∗
LP = d∗
p∗
LP = max0≤α≤1,β≥0 g(α, β).

LP. On the other hand, for any setting of α and β, β-CROWN
LP. Thus,
LP. Finally, due to the strong duality in linear programming,

The variables α in β-CROWN can be translated to dual variables in LP as well. Given α in β-
CROWN, we can get the corresponding dual LP variables µ, γ given α by setting µ(i)
j = (1 −
α(i)

]− and γ(i)

]−.

j = α(i)

j [ˆν(i)

j )[ˆν(i)

j

j

A.3 Proof for soundness and completeness

Theorem 3.3. β-CROWN with branch and bound on splitting ReLU neurons is sound and complete.

Proof. Soundness. Branch and bound (BaB) with β-CROWN is sound because for each subdomain
Ci := {x ∈ C, z ∈ Zi}, we apply Theorem 3.1 to obtain a sound lower bound f
(the bound is valid
for any β ≥ 0). The ﬁnal bound returned by BaB is mini f
which represents the worst case over
all subdomains, and is a sound lower bound for x ∈ C := ∪iCi.

Ci

Ci

Completeness. To show completeness, we need to solve Eq. 1 to its global minimum. When there
are N unstable neurons, we have up to 2N subdomains, and in each subdomain we have all unstable
ReLU neurons split into one of the z(i)
j < 0 case. The ﬁnal solution obtained by BaB is
the min over these 2N subdomains. To obtain the global minimum, we must ensure that in every of
these 2N subdomain we can solve Eq. 10 exactly.

j ≥ 0 or z(i)

When all unstable neurons are split in a subdomain Ci, the network becomes a linear network and
neuron split constraints become linear constraints w.r.t. inputs. Under this case, an LP with Eq. 10
can solve the veriﬁcation problem in Ci exactly. In β-CROWN, we solve the subdomain using the
usually non-concave formulation Eq. 12; however, in this case, it becomes concave in ˆβ because
no intermediate layer bounds are used (no α(cid:48) and β(cid:48)) and no ReLU neuron is relaxed (no α), thus
the only optimizable variable is β (Eq. 12 becomes Eq. 8). Eq. 8 is concave in β so (super)gradient
ascent guarantees to converge to the global optimal β∗. To ensure convergence without relying on a
preset learning rate, a line search can be performed in this case. Then, according to Corollary 3.2.1,
this optimal β∗ corresponds to the optimal dual variable for the LP in Eq. 10 and the objective is a
global minimum of Eq. 10.

B More details on β-CROWN with branch and bound (BaB)

B.1 β-CROWN with branch and bound for complete veriﬁcation

We list our β-CROWN with branch and bound based complete veriﬁer (β-CROWN BAB) in Algo-
rithm 1. The algorithm takes a target NN function f and a domain C as inputs. The subprocedure
optimized_beta_CROWN optimizes ˆα and ˆβ (free variables for computing intermediate layer bounds
and last layer bounds) as Eq. 12 in Section 3.3. It operates in a batch and returns the lower and upper
bounds for n selected subdomains simultaneously: a lower bound is obtained by optimizing Eq. 12
using β-CROWN and an upper bound can be the network prediction f (x∗) given the x∗ that mini-
mizes Eq. 71. Initially, we don’t have any splits, so we only need to optimize ˆα to obtain f for x ∈ C
(Line 2). Then we utilize the power of GPUs to split in parallel and maintain a global set P storing all
the sub-domains which does not satisfy f
< 0 (Line 5-10). Speciﬁcally, batch_pick_out extends
branching strategy BaBSR [8] or FSB [11] in a parallel manner to select n (batch size) sub-domains
in P and determine the corresponding ReLU neuron to split for each of them. If the length of P is less
than n, then we reduce n to the length of P. batch_split splits each selected Ci to two sub-domains
Cl
i and Cu
i by forcing the selected unstable ReLU neuron to be positive and negative, respectively.
≥ 0) and we insert the remaining
Domain_Filter ﬁlters out veriﬁed sub-domains (proved with f

Ci

Ci

1We want an upper bound of the objective in Eq. 1. Since Eq. 1 is an minimization problem, any feasible x
produces an upper bound of the optimal objective. When Eq. 1 is solved exactly as f ∗ (such as in the case where
all neurons are split), we have f ∗ = f = f . See also the discussions in Section I.1 of De Palma et al. [10].

22

Algorithm 1 β-CROWN with branch and bound for complete veriﬁcation. Comments are in brown.

(cid:46) Initially there is no split, so optimization is done over ˆα
(cid:46) P is the set of all unveriﬁed sub-domains

1: Inputs: f , C, n (batch size), δ (tolerance), η (maximum length of sub-domains)
2: (f , f ) ← optimized_beta_CROWN(f, [C])
3: P ← (cid:2)(f , f , C)(cid:3)
4: while f < 0 and f ≥ 0 and f − f > δ and length(P) < η do
5:
6:

(C1, . . . , Cn) ← batch_pick_out(P, n)
(cid:2)Cl
1, Cu
(cid:104)
f

(cid:3) ← batch_split(C1, . . . , Cn) (cid:46) Each Ci splits into two sub-domains Cl
, . . . , f

(cid:46) Pick sub-domains to split and removed them from P
i and Cu
i
(cid:3))
n, Cu
n

← optimized_beta_CROWN(f, (cid:2)Cl

, f Cu
(cid:46) Compute lower and upper bounds by optimizing ˆα and ˆβ mentioned in Section 3.3 in a batch
, f Cu
1], [f

P ← P (cid:83) Domain_Filter

1 , . . . , Cl
, f
, f Cl

n, Cu
n
, f Cu

[f
Filter out veriﬁed sub-domains, insert the left domains back to P

1 , . . . , Cl

(cid:17)

(cid:46)

1 ], . . . , [f

1, Cu

, Cu
n]

, f Cu

n], [f

, f C1

, f Cl

, f Cl

, Cu

, Cl

, Cl

Cu
1

Cu
1

, f

Cu
n

Cu
n

Cl
n

Cl
n

Cl
1

Cl
1

(cid:16)

(cid:105)

n

n

n

n

1

1

1

1

f ← min{f
Ci
f ← min{f Ci

| (f

| (f

, f Ci
, f Ci

Ci

Ci

, Ci) ∈ P}, i = 1, . . . , n (cid:46) To ease notation, Ci here indicates either Cu
, Ci) ∈ P}, i = 1, . . . , n

i or Cl
i

7:

8:

9:

10:

11: Outputs: f , f

ones to P. The loop breaks if the property is proved (f ≥ 0), or a counter-example is found in any
sub-domain (f < 0), or the lower bound f and upper bound f are sufﬁciently close, or the length of
sub-domains P reaches a desired threshold η (maximum memory limit).

Note that for models evaluated in our paper, we ﬁnd that computing intermediate layer bounds in
every iteration at line 7 is too costly (although it is possible and supported) so we only compute
intermediate layer bounds once at line 2. At line 7, only the neuron with split constraints have their
intermediate layer bounds updated, and other intermediate bounds are not recomputed. This makes
the intermediate layer bounds looser but it allows us to quickly explore a large number of nodes
on the branch and bound search tree and is overall beneﬁcial for verifying most models. A similar
observation was also found in De Palma et al. [10] (Section 5.1.1).

B.2 Comparisons to other GPU based complete veriﬁers

Bunel et al. [6] proposed to reformulate the linear programming problem in Eq. 10 through Lagrangian
decomposition. Eq. 10 is decomposed layer by layer, and each layer is solved with simple closed
form solutions on GPUs. A Lagrangian is used to enforce the equality between the output of a
previous layer and the input of a later layer. This optimization formulation has the same power as a
LP (Eq. 10) under convergence. The main drawback of this approach is that it converges relatively
slowly (it typically requires hundreds of iterations to converge to a solution similar to the solution
of a LP), and it also cannot easily jointly optimize intermediate layer bounds. In Table 1 (PROX
BABSR) and Figure 1 (BDD+ BABSR, which refers to the same method) we can see that this
approach is relatively slow and has high timeout rates compared to other GPU accelerated complete
veriﬁers. Recently, De Palma et al. [11] proposed a better branching strategy, ﬁltered smart branching
(FSB), to further improved veriﬁcation performance of [6], but the Lagrangian Decomposition based
incomplete veriﬁer and the branch and bound procedure stay the same.

De Palma et al. [10] used a tighter convex relaxation [2] than the typical LP formulation in Eq. 10
for the incomplete veriﬁer. This tighter relaxation may contain exponentially many constraints,
and De Palma et al. [10] proposed to solve the veriﬁcation problem in its dual form where each
constraint becomes a dual variable. A small active set of dual variables is maintained during dual
optimization to ensure efﬁciency. This tighter relaxation allows it to outperform [6], but it also
comes with extra computational costs and difﬁculties for an efﬁcient implementation (e.g. a “masked”
forward/backward pass is needed which requires a customised low-level convolution implementation).
Additionally, De Palma et al. [10] did not optimize intermediate layer bounds jointly.

Xu et al. [45] used CROWN [46] (categorized as a linear relaxation based perturbation analysis
(LiRPA) algorithm) as the incomplete solver in BaB. Since CROWN cannot encode neural split
constraints, Xu et al. [45] essentially solve Eq. 10 without neuron split constraints (z(i)
j ≥ 0, i ∈
{1, · · · , L − 1}, j ∈ Z +(i) and z(i)
j < 0, i ∈ {1, · · · , L − 1}, j ∈ Z −(i)) in Eq. 10. The missing

23

constraints lead to looser bounds and unnecessary branches. Additionally, using CROWN as the
incomplete solver leads to incompleteness - even when all unstable ReLU neurons are split, Xu et al.
[45] still cannot solve Eq. 1 to a global minimum, so a LP solver has to be used to check inconsistent
splits and guarantee completeness. Our β-CROWN BaB overcomes these drawbacks: we consider
per-neuron split constraints in β-CROWN which reduces the number of branches and solving time
(Table 1). Most importantly, β-CROWN with branch and bound is sound and complete (Theorem 3.3)
and we do not rely on any LP solvers.

Another difference between Xu et al. [45] and our method is the joint optimization of intermediate
layer bounds (Section 3.3). Although [45] also optimized intermediate layer bounds, they only
optimize α and do not have β, and they share the same variable α for all intermediate layer bounds
and ﬁnal bounds, with a total of O(Ld) variables to optimize. Our analysis in Section 3.3 shows that
there are in fact, O(L2d2) free variables to optimize, and we share less variables as in Xu et al. [45].
This allows us to achieve tighter bounds and improve overall performance.

B.3 Detection of Infeasibility

Maximizing Eq. 8 with infeasible constraints leads to unbounded dual objective, which can be
detected by checking if this optimized lower bound becomes greater than the upper bound (which is
also maintained in BaB, see Alg.1 in Sec. B.1). For the robustness veriﬁcation problem, a subdomain
that has lower bound greater than 0 is dropped, which includes the unbounded case. Due to insufﬁcient
convergence, this cannot always detect infeasibility, but it does not affect soundness, as this infeasible
subdomain only leads to worse overall lower bound in BaB. To guarantee completeness, we show
that when all unstable neurons are split the problem is concave (see Section A.3); in this case, we
can use line search to guarantee convergence when feasible, and detect infeasibility if the objective
exceeds the upper bound (line search guarantees the objective can eventually exceed upper bound).
In most real scenarios, the veriﬁer either ﬁnishes or times out before all unstable neurons are split.

C Details on Experimental Setup and Results

C.1 Experimental Setup

We run our experiments on a machine with a single NVIDIA RTX 3090 GPU (24GB GPU memory),
a AMD Ryzen 9 5950X CPU and 64GB memory. Our β-CROWN solver uses 1 CPU and 1 GPU
only, except for the MLP models in Table 2 where 16 threads are used to compute intermediate layer
bounds with Gurobi2. We use the Adam optimizer [21] to solve both ˆα and ˆβ in Eq. 12 with 20
iterations. The learning rates are set as 0.1 and 0.05 for optimizing ˆα and ˆβ respectively. We decay
the learning rates with a factor of 0.98 per iteration. To maximize the beneﬁts of parallel computing
on GPU, we use batch sizes n =1024 for Base (CIFAR-10), Wide (CIFAR-10), Deep (CIFAR-10),
CNN-A-Adv (MNIST) and ConvSmall (MNIST), n =2048 for ConvSmall (CIFAR-10), n =4096 for
CNN-A-Adv (CIFAR-10), CNN-A-Adv-4 (CIFAR-10), CNN-A-Mix (CIFAR-10) and CNN-A-Mix-4
(CIFAR-10), n =256 for CNN-B-Adv (CIFAR-10) and CNN-B-Adv-4 (CIFAR-10), n =1024 for
ConvBig (MNIST), n =10 for ConvBig (CIFAR-10), n =8 for ResNet (CIFAR-10) respectively.
The CNN-A-Adv, CNN-A-Adv-4, CNN-A-Mix, CNN-A-Mix-4, CNN-B-Adv and CNN-B-Adv-4
models are obtained from the authors or [9] and are the same as the models used in their paper. We
summarize the model structures in both incomplete veriﬁcation and complete veriﬁcation (Base, Wide
and Deep) experiments in Table A1. Our code is available at http://PaperCode.cc/BetaCROWN.

C.2 Additional Experiments

More results on incomplete veriﬁcation In this paragraph we compare our β-CROWN FSB to
many other incomplete veriﬁers. WK [42] and CROWN [46] are simple bound propagation methods;

2Note that our β-CROWN veriﬁer does not rely on MILP/LP solvers. For these very small MLP models, we
ﬁnd that a MILP solver can actually compute intermediate layer bounds pretty quickly and using these tighter
intermediate bounds are quite helpful for β-CROWN. This also enables us to utilize both CPUs and GPUs on a
machine. For all other models, intermediate layer bounds are computed through optimizing Eq. 12. Practically,
MILP is not scalable beyond these very small MLP models and these small models are not the main focus of this
work.

24

Table A1: Model structures used in our experiments. For example, Conv(1, 16, 4) stands for a conventional
layer with 1 input channel, 16 output channels and a kernel size of 4 × 4. Linear(1568, 100) stands for a fully
connected layer with 1568 input features and 100 output features. We have ReLU activation functions between
two consecutive layers.

Model name

CNN-A-Adv (MNIST)
ConvSmall (MNIST)
ConvBig (MNIST)

ConvSmall (CIFAR-10)
ConvBig (CIFAR-10)

CNN-A-Adv/-4 (CIFAR-10)
CNN-B-Adv/-4 (CIFAR-10)
CNN-A-Mix/-4 (CIFAR-10)

Model structure

Conv(1, 16, 4) - Conv(16, 32, 4) - Linear(1568, 100) - Linear(100, 10)
Conv(1, 16, 4) - Conv(16, 32, 4) - Linear(800, 100) - Linear(100, 10)
Conv(1, 32, 3) - Conv(32, 32, 4) - Conv(32, 64, 3) - Conv(64, 64, 4) - Linear(3136, 512) -
Linear(512, 512) - Linear(512, 10)
Conv(3, 16, 4) - Conv(16, 32, 4) - Linear(1152, 100) - Linear(100, 10)
Conv(3, 32, 3) - Conv(32, 32, 4) - Conv(32, 64, 3) - Conv(64, 64, 4) - Linear(4096, 512) -
Linear(512, 512) - Linear(512, 10)
Conv(3, 16, 4) - Conv(16, 32, 4) - Linear(2048, 100) - Linear(100, 10)
Conv(3, 32, 5) - Conv(32, 128, 4) - Linear(8192, 250) - Linear(250, 10)
Conv(3, 16, 4) - Conv(16, 32, 4) - Linear(2048, 100) - Linear(100, 10)

Base (CIFAR-10)
Wide (CIFAR-10)
Deep (CIFAR-10)

Conv(3, 8, 4) - Conv(8, 16, 4) - Linear(1024, 100) - Linear(100, 10)
Conv(3, 16, 4) - Conv(16, 32, 4) - Linear(2048, 100) - Linear(100, 10)
Conv(3, 8, 4) - Conv(8, 8, 3) - Conv(8, 8, 3) - Conv(8, 8, 4) - Linear(412, 100) - Linear(100, 10)

CROWN-OPT uses the joint optimization on intermediate layer bounds (optimizing Eq. 12 with no ˆβ,
as done in [45]). We also include triangle relaxation based LP veriﬁers with intermediate layer bounds
obtained from WK, CROWN and CROWN-OPT. In our experiments in Table 1, we noticed that
BIGM+A.SET BABSR [10] and Fast-and-Complete [45] are also very competitive among existing
state-of-the-art complete veriﬁers3 - they runs fast in many cases with low timeout rates. Therefore,
we also evaluate BIGM+A.SET BABSR and Fast-and-Complete with an early stop of 3 minutes for
the incomplete veriﬁcation setting as an extension of Section 4.2. The veriﬁed accuracy obtained from
each method are reported in Table A2. BIGM+A.SET BABSR and Fast-and-Complete sometimes
produce better bounds than SDP-FO, however β-CROWN FSB consistently outperforms both of
them. Additionally, we found that intermediate layer bounds are important for LP veriﬁer on some
models, although even with the tightest possible CROWN-OPT bounds the veriﬁed accuracy gap
between LP veriﬁers and ours is still large. Additionally, LP veriﬁers need signiﬁcantly more time.

Additional results on the tightness of veriﬁcation.
In Figure A1, we include LP based veriﬁers
as baselines and compare the lower bound from veriﬁcation to the upper bound obtained by PGD.
The LP veriﬁers use the triangle relaxations described in Section 2.1, with intermediate layer bounds
from WK [42], CROWN [46] and CROWN with joint optimization on intermediate layer bounds
(denoted as CROWN-OPT). We ﬁnd that tighter intermediate layer bounds obtained by CROWN can
greatly improve the performance of the LP veriﬁer compared to those using looser ones obtained
by Wong and Kolter [42]. Furthermore, using intermediate layer bounds computed by joint opti-
mization can achieve additional improvements. However, our branch and bound with β-CROWN
can signiﬁcantly outperform these LP veriﬁers. This shows that BaB is an effective approach for
incomplete veriﬁcation, outperforming the bounds produced by a single LP.

Lower bound improvements over time
In Figure A2, we plot lower bound values vs. time for
β-CROWN BABSR and BIGM+A.SET BABSR (one of the most competitive methods in Table 1)
on the CNN-A-Adv (MNIST) model. Figure A2 shows that branch and bound can indeed quickly
improve the lower bound, and our β-CROWN BABSR is consistently faster than BIGM+A.SET
BABSR. In contrast, SDP-FO [9], which typically requires 2 to 3 hours to converge, can only provide
very loose bounds during the ﬁrst 3 minutes of optimization (out of the range on these ﬁgures).

Ablation study of running time on GPUs and CPUs We conduct the same experiments as in
Table 1 but run β-CROWN FSB on CPUs instead of GPUs. As shown in Table A3, our method is
strong even on a single CPU, showing that the good performance does not only come from GPU
acceleration; our efﬁcient algorithm also contributes to our success. On the other hand, using GPU
can boost the performance by at least 2x. Importantly, the models evaluated in this table are very
small ones. Massive parallelization on GPU will lead to more signiﬁcant acceleration on larger

3The concurrent work BaDNB (BDD+ FSB) does not have public available code when our paper was

submitted.

25

Table A2: Veriﬁed accuracy (%) and avg. per-example veriﬁcation time (s) on 7 models from SDP-FO [9].

Dataset

Model
Methods

WK [42]
CROWN [46]
CROWN-OPT [45]
LP (WK)§
LP (CROWN)
LP (CROWN-OPT)
SDP-FO [9]∗
PRIMA [26]
BigM+A.Set [10]
Fast-and-Complete§
β-CROWN FSB
Upper Bound (PGD)

§

MNIST (cid:15) = 0.3

CIFAR (cid:15) = 2/255

CNN-A-Adv

CNN-B-Adv CNN-B-Adv4 CNN-A-Adv CNN-A-Adv4 CNN-A-Mix CNN-A-Mix4
Veriﬁed% Time (s) Ver.% Time(s) Ver.% Time(s) Ver.% Time(s) Ver.% Time(s) Ver.% Time(s) Ver.% Time(s)

0
1.0
14.0
0.5
3.5
14.0
43.4
44.5
63.0
66.0
70.5
76.5

0.8
34.5
0.4
8.5
0.1
0.9
43.5
0.5
21.5
0.1
4.0
45.0
5.5
21.5
2.7
1361
41.0
612
14.5
16
1570
45.0
941
21.5
22
45.0
1451
21.5
977
40
46.0 >25h
32.8 >25h
>20h
44
53.5
344
38.0
136
117 N/A†
N/A N/A
51.5
38.5
49
54.0
46.5
21
63.0
65.0
-

0.4
32.5
0.6
35.5
2.0
36.0
114
35.0
123
36.0
36.0
122
39.6 >25h
4.8
41.5
79
N/A 41.0
14
41.5
44.0
5.8
-
50.0

21
12
-

64
32
-

0.5
39.5
0.7
41.5
1.6
42.0
140
41.5
147
41.5
42.0
152
40.0 >25h
45.0
46.0
46.0
46.0
49.5

0.3
15.0
0.4
23.5
2.3
25.0
84
19.0
119
24.0
25.0
94.8
39.6 >25h
34
122
79
50
-

4.9 37.5‡
30.0
39
33.0
4.2
41.5
5.6
53.0
-

0.4
30.0
0.5
38.0
2.0
38.5
117
36.5
126
38.5
38.5
127
47.8 >25h
7.0
48.5
71
47.0
10
46.0
50.5
5.9
-
57.5

Names in parentheses are methods to compute intermediate layer bounds for the LP veriﬁer.

* SDP-FO results are directly from their paper due to the very long running time. All other methods are tested on the same set of 200 examples.
† The implementation of BigM+A.Set BaBSR is not compatible with CNN-B-Adv and CNN-B-Adv4 models which have an convolution with

asymmetric padding.

‡ A recent version (Oct 26, 2021) of [26] reported better results on CNN-A-Mix. We found that their results were produced on a selection of
100 data points, and reruning their method using the same command on the same set of 200 random examples as used in other methods in this
table produces different results, as reported here.

§ We use our β-CROWN code and turn off β optimization to emulate the algorithm used in [45]. This in fact leads to better performance than

the original approach in [45] because we allow more α variables to be optimized and our implementation is generally better.

Figure A1: Veriﬁed lower bound on f (x) by β-CROWN FSB compared against incomplete LP veriﬁers using
different intermediate layer bounds obtained from [42] (denoted as LP (WK)), CROWN [46] (denoted as LP
(CROWN)), and jointly optimized intermediate bounds in Eq. 12 (denoted as LP (CROWN-OPT)), v.s. the
adversarial upper bound on f (x) found by PGD. LPs need much longer time to solve than β-CROWN on
CIFAR-10 models (see Table A2).

(a) MNIST CNN-A-Adv, runner-up targets, (cid:15) = 0.3

(b) CIFAR CNN-B-Adv, runner-up targets, (cid:15) = 2/255

models. The speedup on multi-core CPU is not obvious, possibly due to the limitation of underlying
implementations of PyTorch.

Ablation study on the impact of α, β, and their joint optimization We conduct the same exper-
iments as in Table 1 but turn on or turn off α and β optimization to see the contribution of each
part. As shown in Table A4, optimizing both α and β leads to optimal performance. Optimizing
beta has a greater impact than optimizing α. Joint optimization is helpful for CIFAR10-Base and
CIFAR10-Wide models, reducing the overall runtime. For simple models like CIFAR10-Deep,

Figure A2: For the CNN-A-Adv (MNIST) model, we randomly select four examples from the incomplete
time (in 180 seconds) of β-CROWN BABSR and
veriﬁcation experiment and plot the lower bound v.s.
BIGM+A.SET BABSR. Larger lower bounds are better. β-CROWN BaBSR improves bound noticeably faster
in all four situations.

26

20246PGD adversarial upper bound1086420246Verified lower bound-CROWNLP(WK)LP(CROWN)LP(CROWN-OPT)PGD (y=x)101234567PGD adversarial upper bound864202468Verified lower bound-CROWNLP(WK)LP(CROWN)LP(CROWN-OPT)PGD(y=x)050100150time(s)1.52.02.53.03.54.04.55.0lower bound-CROWN+BaBSRBig-M+A. Set+BaBSR050100150time(s)1.51.00.50.00.51.01.52.02.5lower bound-CROWN+BaBSRBig-M+A. Set+BaBSR050100150time(s)2.52.01.51.00.50.00.5lower bound-CROWN+BaBSRBig-M+A. Set+BaBSR050100150time(s)1.00.50.00.51.01.52.0lower bound-CROWN+BaBSRBig-M+A. Set+BaBSRTable A3: Average runtime and average number of branches on three CIFAR-10 models over 100 properties (the
same setting as in Table 1) by using different numbers of CPU cores, as well as using a single GPU.

CIFAR-10 Base

CIFAR-10 Wide

CIFAR-10 Deep

Hardware

time(s)

branches %timeout

time(s)

branches %timeout

time(s)

branches %timeout

1 CPU 249.49
4 CPU 228.28
16 CPU 222.71
1 GPU 118.23

7886.37
9575.52
10271.08
208018.21

4.00
4.00
4.00
3.00

178.01
172.55
172.40
78.32

2749.96
3956.17
4087.15
116912.57

4.00
4.00
4.00
2.00

47.46
45.35
43.97
5.69

41.12
41.12
41.12
41.12

0.00
0.00
0.00
0.00

disabling joint optimization can help slightly because this model is very easy to verify (within a few
seconds) and using looser bounds reduces veriﬁcation cost.

Table A4: Ablation study on the CIFAR-10 Base, Wide and Deep models (the same setting as in Table 1),
including combinations of optimizing or not optimizing α and/or β variables, and using or not using joint
optimization for intermediate layer bounds.
CIFAR-10 Base

CIFAR-10 Wide

CIFAR-10 Deep

joint opt α β

time(s)

branches %timeout

time(s)

branches %timeout

time(s)

branches %timeout

(cid:88)

233.86
(cid:88) 174.10
(cid:88) (cid:88) 139.83

(cid:88)

163.69
(cid:88) 162.95
(cid:88) (cid:88) 118.23

233233.70
163037.05
133346.44

160058.80
150631.49
208018.21

(cid:88)
(cid:88)
(cid:88)

6.00
4.00
3.00

4.00
4.00
3.00

148.46
102.65
91.01

149.00
89.22
78.32

113017.10
86571.18
73713.30

115509.71
72479.96
116912.57

4.00
2.00
2.00

4.00
2.00
2.00

5.77
5.73
5.22

8.58
8.38
5.69

260.18
134.76
100.44

65.70
52.26
41.12

0.00
0.00
0.00

0.00
0.00
0.00

27

