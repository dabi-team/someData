0
2
0
2

n
u
J

6
1

]
E
S
.
s
c
[

1
v
9
2
5
9
0
.
6
0
0
2
:
v
i
X
r
a

Quality Management of Machine Learning
Systems

P. Santhanam

IBM Research AI, T.J.Watson Research Center
Yorktown Heights, New York
pasanth@us.ibm.com

Abstract. In the past decade, Artiﬁcial Intelligence (AI) has become a
part of our daily lives due to major advances in Machine Learning (ML)
techniques. In spite of an explosive growth in the raw AI technology and
in consumer facing applications on the internet, its adoption in busi-
ness applications has conspicuously lagged behind. For business/mission-
critical systems, serious concerns about reliability and maintainability
of AI applications remain. Due to the statistical nature of the output,
software ‘defects’ are not well deﬁned. Consequently, many traditional
quality management techniques such as program debugging, static code
analysis, functional testing, etc. have to be reevaluated. Beyond the cor-
rectness of an AI model, many other new quality attributes, such as
fairness, robustness, explainability, transparency, etc. become important
in delivering an AI system. The purpose of this paper is to present a view
of a holistic quality management framework for ML applications based
on the current advances and identify new areas of software engineering
research to achieve a more trustworthy AI.

Keywords: Artiﬁcial Intelligence · Machine learning · Quality manage-
ment · AI Engineering.

1

Introduction

According to the 2019 AI Index report [1], hundreds of papers are published on
AI technology every day! In 18 months, the time required to train a large image
classiﬁcation system on the cloud infrastructure has fallen from about 3 hours to
about 88 seconds! In 2019, global private AI investment was over $70B. In 2018,
the State of California licensed testing of more than 500 autonomous vehicles,
which drove over 2 million miles. When it comes to real AI applications, many
companies on the internet use the latest machine learning techniques to perform
various consumer facing tasks, such as, answer questions, recognize images, rec-
ommend products, translate content, etc. Not a day goes by when there is not a
news report of a new application of machine learning to some new domain. All
these trends point to an explosion of AI related technology all around.

Interestingly, the adoption of AI in the enterprise for business critical ap-
plications has lagged considerably behind. Recent analysts reports from various

 
 
 
 
 
 
2

P. Santhanam

sources [2] indicate at most 20-40% success rate for the adoption of AI to create
business value. This supports the assertion that moving AI from a proof-of-
concept to real business solution is not a trivial exercise. Some common reasons
cited for this result are:

– Insuﬃcient alignment of business goals and processes to the AI technology
(akin to the challenges of introducing information technology in the 1990’s).
– Lack of data strategy (i.e. “There is no AI without IA (Information Archi-

tecture)”)

– Shortage of skilled people who can combine domain knowledge and the rel-

evant AI technology.

– Unique concerns about AI (e.g. model transparency, explainability, fair-

ness/bias, reliability, safety, maintenance, etc.)

– Need for better engineering infrastructure for data and model provenance.

As the application of AI moves to business/mission critical tasks with more
severe consequences, the need for a rigorous quality management framework
becomes critical. It is bound to be very diﬀerent from the practices and processes
that have been in place for IT projects over many decades. The goal of this paper
is to provide an overview of such a framework built upon tools and methodology
available today and identify gaps for new software engineering research. The
focus of this paper is on AI systems implemented using machine learning. A
popular ML technique is the use of Deep Neural Networks (DNN). This paper
uses AI and ML interchangeably.

2 AI is Software

In general, the use of an AI component has one of three goals. (i) automate
an existing task performed by a human e.g. Support Bots (ii) improve the eﬃ-
ciency of an existing task e.g. language translation (iii) perform a new task e.g. a
recommender system. The invocation of the AI is through traditional interfaces
(e.g. REST based microservices). In this respect, it is just another software com-
ponent, albeit with some special attributes. Thus, from the system or software
management point of view, it has all the same expectations as any other software
component. Figure 1 shows the recommended system and software quality mod-
els and attributes from the ISO/IEC 25010 process standard [3]. Reference [4]
gives an accessible overview of the relevant attributes. Even though the speciﬁc
interpretation may have to be reﬁned for the purpose of AI components, the util-
ity of the basic structure is immediately evident. The quality attributes in use (in
the left column) i.e. eﬀectiveness, eﬃciency, satisfaction, risk and context cover-
age do represent the relevant dimensions for consideration. Inclusion of ‘Trust’
under the ‘Satisfaction’ category is fortuitous in hindsight, since it has taken
a more profound meaning for AI components. The product quality attributes
on the right are essential for product owners. Notably, the common metric used
by AI algorithm owners is accuracy which relates to ‘Functional Correctness’ in
Figure 1 and it is only one of the more than two dozen attributes in the ISO

Quality Management of Machine Learning Systems

3

standard. It is important to evaluate an AI component against these attributes
to understand the requirements they place on the use of an AI component in a
software system.

Fig. 1: ISO/IEC 25010- System and Software Quality Models [3,4]

2.1 Traditional Software Quality Management

The engineering practices for managing software quality go back many decades
[5], and McConnell [6] gives a more recent view of practices and tools. A key
assumption is that expected functional behavior (i.e. linking inputs and
outputs) of components is documented (or understood) at design time,
even if design evolves during a project. Central to quality management is the
deﬁnition of a defect (aka bug) as the software behavior not meeting the ex-
pectation. Defects can be opened during any of the development activities [7],
namely, Design or Code Review, Unit Test (white box), Function Test (black
box), System Test (integration), and during DevOps or Operations.

There are seven aspects to managing quality processes in a software project.
(i) Requirements management that evaluates the incoming (and often chang-
ing) requirements and selects the requirements for inclusion in the upcoming
software release(s). (ii) Defect management which includes the process of
opening, debugging, ﬁxing, closing and counting defects across the life cycle. (iii)
Change management, which relates to versioning and tracking of changes to
code and documentation during the life cycle. (iv) Test Management, consist-
ing of Test design, Test creation, Test execution and Evaluation metrics (e.g.
test eﬀectiveness, defect density, code or functional coverage, etc.) This applies
across all levels of testing i.e. unit testing (white box), function testing (black
box), etc. (v) Dev/Op processes that manage the promotion of code from

4

P. Santhanam

development to operations with the necessary processes and automation (e.g.
regression tests) to validate deployment quality and support the run-time envi-
ronment. (vi) Operations management that collects incident reports during
operations and provides a mechanism for support teams to diagnose and resolve
them expediently, which may involve the original engineers who created the rel-
evant code. (vii) Project management that brings these six diﬀerent aspects
into a cohesive decision support system for risk management via dashboards.

2.2 Machine Learning Systems

A recent paper [8] discusses the engineering challenges in building reliable ML
systems. At a high level, any AI solution based on machine learning technology
is an interplay between three factors i.e. Data, Domain context and the AI
Algorithms (Figure 2). The solution quality is determined by the algorithms that
learn from the training data to create outputs that make sense to the humans in
the speciﬁc domain context/application area. These three factors together deﬁne
the necessary conditions. If any one is missing, the resulting ML system is likely
to fail. Understanding this is critical to assess the expected business value of an
AI system.

Fig. 2: Critical Success Factors for AI applications

In supervised machine learning, the modeling data consists of large number
of inputs and the corresponding outputs (called labels). The main goal of the
ML algorithm is to ﬁnd the best approximation to the function that maps the
inputs to the output(s). During inference, the output is the prediction that can
be a continuous variable (e.g. price of a stock or tomorrow’s temperature) or
a label/class (e.g. predicting the next word in a search engine, identifying an
image as a cat, etc.) These ML functions do not have a requirement/speciﬁcation
document at the design time; they are just derived from the modeling data.
Model outputs are statistical and not deterministic. Consequently, there is no
simple way to deﬁne a defect! As an example, an image recognition algorithm
may identify a ‘cat’ as a ‘dog’ for some of the instances and this is allowed by
the statistical uncertainties in the algorithm. Debugging is complicated since the
problem can be in the model and/or the data. No guarantees or explanations are

Quality Management of Machine Learning Systems

5

Table 1: Key perspectives in quality management of AI projects

provided on the exact functional operations of the model. Traditional testing will
not work, since there is no description of the expected behavior at design time.
In addition, ML Model behavior can drift over time during deployment due to
previously unseen data. If the model is learning continuously during deployment,
new patterns of relationships in data can emerge, unknown to the model owners.
The potential breadth of AI applications invokes serious social concerns as
evidenced by various government initiatives such as the European Commission
Ethics Guidelines for Trustworthy AI [9] and AI Ethics Principles for the US De-
partment of Defense [10]. Any AI quality management framework has to address
these concerns. Table 1 describes the changes to the quality processes discussed
in section 2.1 due to the inclusion of an AI component. It is clear that every
one of the traditional quality management activities is aﬀected. Due
to the nature of the AI applications, the quality management is a required and
never ending activity, as long as the application is in use.

3 A Quality Management Framework for ML systems

The purpose of this section is to identify the key components that are needed
to have an adequate quality management framework for successful delivery of
reliable ML systems. Some of the components have prototype technology already
available for use, but while others need more experimentation and new research.
This discussion leverages some of the concepts from references [11-14].

6

P. Santhanam

Fig. 3: Key artifacts in the creation of an application with one ML component
implemented in the model (gray rounded square). For simplicity, iterative process
steps are not shown and only the AI component properties are emphasized.

Figure 3 shows the key artifacts in the development of a ML application. For
simplicity, we emphasize AI speciﬁc artifacts. All software projects start with a
set of functional and non-functional requirements, mostly in natural language
documents. Software architecture consists of high level and detailed designs typ-
ically captured in various diagrams [6]. Detailed designs identify the speciﬁc
functions needed for the implementation. One or more of the functions can be
implemented by data scientists using ML models. Modeling data refers to the
data available for training and validation. The data scientists either reuse al-
gorithms and other components from popular AI frameworks (e.g. TensorFlow,
PyTorch, etc.) or write the modeling code for the ﬁrst time. Model gets inte-
grated with other non-AI components (e.g. user interface, traditional analytics,
reporting, etc.) and IT components (e.g. access control) to create the business
application which is subsequently deployed to operations.

3.1 Where are the bugs?

The defects in application requirements and design are nothing new in software
development, but ML introduces some twists in the assessment of the right
task for the automation with AI, based on business expectations on quality
(discussed above) and operational performance. Incorrect choice of algorithm for
the chosen task is a common source for quality concerns. The programming errors
in the implementation of the algorithm is also not a new problem. Examples are
incorrect API calls, syntax errors, incorrect model parameter, etc. These can be
addressed with additional support from the frameworks, much like Eclipse for
Java.

As in all projects that involve statistics, the quality and quantity of the
available data are major concerns. An example of raw data quality is ”expec-
tation mismatch”(i.e.incorrect data type for feature values, Non-boolean value
for boolean feature type, etc.) The more subtle data problems relate to noisy
labels in the modeling data and issues with data distributions (e.g. the fraction

Quality Management of Machine Learning Systems

7

of examples containing a feature is too small, data distribution is diﬀerent for
training and validation data sets, etc.) Data problems also resurface in produc-
tion when the operational data proﬁle does not match the data used during
model development or due to unexpected emergent behavior.

Due to the extensive use of open source machine learning frameworks (i.e.
TensorFlow, CNTK, Keras, Theano, PyTorch, etc.) and associated libraries, they
become additional sources of bugs [15,16]. Testing a ML library for a speciﬁc
algorithm (e.g. convolutional neural network, recurrent neural network, etc.)
will require implementation of the same algorithm in diﬀerent ML libraries and
luckily this is not a problem with common algorithms and popular libraries.
Examples of bugs in frameworks are: not backward compatible API versions,
unaligned tensors, pooling scheme inconsistency, etc.

Then, there are the bugs in the model itself, as evidenced by an unexpected
output for the given input (e.g. a cat’s image identiﬁed as a dog) , through var-
ious modeling errors such as overﬁtting, unoptimized hyper parameters, wrong
neural net architecture, etc. Once the model is integrated into the business ap-
plication, the defects in the ML components get mixed up with the traditional
software defects in the other components. The overall quality of the application
is obviously dependent on all the contributing software components, such as user
interface, back end management, etc.

3.2 Quality Improvement Tasks for ML systems

This section describes the suggested tasks to ﬁnd defects in the artifacts de-
scribed in Section 3.1 and resolve them. These are traditional activities modiﬁed
to reﬂect the inclusion of the ML component in the application. Due to space
limitations, reference to any speciﬁc technique or tool is meant to provide an
example, rather than an exhaustive list. Quality improvement tasks that address
the unique aspects of assessing ‘Trust’ in ML systems are described in Section
3.3.

Manual Inspection With the support of tools [17] manual inspection is still an
eﬀective way to ﬁnd defects in requirements, architecture, algorithms and code.
Techniques, such as pair programming [6] have proven very useful in practice.

Static Analysis Application of static analysis to ﬁnd defects in software pro-
grams is a very mature ﬁeld, dating back to many decades [18]. There have
been recent examples of applying this technique to machine learning code [19].
Many development environments support basic syntax checking, when the code
is being written.

White Box Testing Traditional white box testing [20] leverages the knowl-
edge of the program structure to execute the program in ways to achieve the
desired coverage e.g. branch coverage, statement coverage, etc. to locate defects.

8

P. Santhanam

Similarly, a data scientist can use the detailed knowledge of a neural network
behavior in the model building process to apply various coverage criteria to the
network to ﬁnd the defects in the model. This has led to concepts such as neuron
coverage [21], novel test criteria that are tailored to structural features of DNNs
and their semantics [22], the condition-decision relationships between adjacent
layers and the combinations of values of neurons in the same layer [23], mu-
tation techniques on source code, data and models [24] and combinatorial test
design consisting of neuron pairs in the layers and the neuron-activation con-
ﬁgurations[25]. These techniques demonstrate various ways to expose incorrect
behavior of the network while being mindful of the computational cost of test
generation itself.

Black Box Testing Traditional black box testing (or functional testing) [20]
focuses on detecting defects in the expected external behavior of the software
component by carefully manipulating the input space. For ML models, it is
important that test data represents the business requirements in terms of data
values and distributions and was not used during the model creation process.
Key goal of black box testing is to evaluate if the model generalizes adequately
for previously unseen data or suggest a model rework, if not suitable. These
considerations also apply for system integration tests.

Data Assessment & Testing There are several techniques and tools to check
the quality of the modeling data during development. Breck et al.[26] present a
highly scalable data validation system, designed to detect data anomalies (e.g.
unexpected patterns, schema-free data, etc.) in the machine learning pipelines.
Barash et al.[27] use combinatorial design methodology to deﬁne the space of
business requirements and map it to the ML solution data, and use the notion
of data slices to identify uncovered requirements, under-performing slices, or
suggest the need for additional training data. This is also an example of using
data slicing for black box testing.

Application Monitoring Application monitoring during operations is a crit-
ical activity in ML applications since the model performance can change over
time due to previously unseen pattern in the operational data or emergent be-
havior not expected in the model building process. Breck et al [26] also describe
techniques to detect feature skew by doing a key-join between corresponding
batches of training and operational data followed by a feature wise compari-
son. Distribution skew between training data and serving data is detected by
distance measures. Raz et al.[28] discuss a novel approach, solely based on a
classiﬁer suggested labels and its conﬁdence in them, for alerting on data dis-
tribution or feature space changes that are likely to cause data drift. This has
two distinct beneﬁts viz. no model input data is required and does not require
labeling of data in production. In addition to the detecting any degradation of
model performance, there need to be processes in place to correct the behavior

Quality Management of Machine Learning Systems

9

as and when it occurs. There are examples of commercial oﬀerings to perform
this task [29].

Debugging Debugging is often the most under-appreciated activity in software
development that takes considerable skill and eﬀort in reality. As noted in [7],
debugging typically happens during three diﬀerent stages in software life cycle,
and the level of granularity of the analysis required for locating the defect dif-
fers in these three. First stage is during the model building process by the data
scientist who has access to the details of the model. Here, there are two classes
of errors that need debugging. (a) raw errors resulting in the execution of the
model code in the development environment during the process of model cre-
ation. The development frameworks can provide support for debugging this class
of problems. (b) Model executes successfully, but the overall performance of the
model output is not adequate for the chosen task or if the model output does
not meet the expectation for speciﬁc input instances, it is necessary to ﬁnd the
reason for these behaviors. There could be many causes, including bad choice of
the algorithm, inadequate tuning of the parameters, quality and quantity of the
modeling data, etc. [30, 31]. Some care is also needed in providing model debug-
ging information to the user to avoid exposure of system details, susceptible for
adversarial attacks.

The second stage for debugging is during the later black box testing activities
in development when an unexpected behavior is encountered. A certain amount
of debugging of the test execution is necessary to conclude that the AI model is
the cause of the unexpected behavior. Once that is conﬁrmed, debugging of the
model follows the same process as described above. Third stage is during oper-
ations, when the application is being put to real use. Any unexpected behavior
here can be the result of changes in the computing environment relative to de-
velopment or due to new patterns in the operational data not previously seen
the modeling data. Techniques discussed in [26, 28,29] can be used to address
the model drift problems.

3.3 AI Trust Assessment

Due to the black box nature of the ML models and their behavior being decided
by modeling data, trust in model outputs has become an important consideration
in business applications. This section deals with four speciﬁc aspects trust.

Explainability In many business critical applications, the outputs of the black
box ML models also require explanations to meet the business objectives. There
are many motivations for explanations [32] and it is important to know the need
so that the appropriate approach can be used. There are examples of open source
packages for implementing explainability [33] in business applications.

Bias/Fairness Due to the potential sensitivity of the outputs of the ML models
to biases inherent in the modeling data, there is a critical question of the fairness

10

P. Santhanam

of the algorithms [34] in extracting the model from the data. There are examples
of open source packages for understanding and mitigating biases [35] in business
applications.

Robustness The owner of a ML application needs a strategy for defending
against adversarial attacks. Xu et al.[36] provide a comprehensive summary of
the adversarial attacks against ML models built using images, graphs and text
and the countermeasures available. Reference [37] describes an open-source soft-
ware library, designed to help researchers and developers in creating novel defense
techniques, and in deploying practical defenses of real-world AI systems.

Transparency Given the abundance of AI components (i.e. algorithms, ser-
vices, libraries, frameworks) available from open source and commercial oﬀer-
ings, it makes sense for a company to reuse the available software component
in its application. However, due to the concerns about the trust in the available
component, the consumer of the component needs some detailed information
about the component to manage the risk. This need for transparency requires
additional assessment. Key pieces of such information are captured in a Fact-
Sheet [38], which provides the technical and process background of the AI asset
to the consumer.

3.4 Quality Metrics

Due to the unique attributes of AI based systems (discussed in Section 2), there
is a critical need to reevaluate the metrics for their quality management. This
section discusses three aspects that highlight the need.

Defect management The lack of a clear deﬁnition of software defect in ML
applications discussed in section 2.2 is a major problem in quality management.
While the defects in the other artifacts can be captured unambiguously, the
perceived errors in the model outputs are subject to the statistical uncertainties.
As a result, until a detailed debugging is performed to diagnose the reason for
the unexpected behavior one cannot be certain that this is a bug. Hence the
defect management tools have to allow this possibility with potentially more
time assigned for the necessary investigation that may point to an inadequate
training data set.

Model Evaluation Model evaluation is an important part of the ML applica-
tion development. There are many metrics [39] that can be used and depending
on the speciﬁc application domain. They need to be chosen and used carefully.
In addition to these usual machine learning metrics, additional metrics speciﬁc
to the trust topics discussed in section 3.3 (explainability, bias, robustness and
transparency) are also necessary to support the business objectives and manage
technical & business risk. There is also recent work [40] to measure application-
level key performance indicators to provide feedback to the AI model life cycle.

Quality Management of Machine Learning Systems

11

Model Uncertainty In addition to the usual ML metrics [39], typically at the
end of a DNN pipeline is a softmax activation (usually a sigmoid function) that
estimates a conﬁdence level for each output, expressed as a probability measure
between 0 and 1. In reality, a high conﬁdence level does not necessarily mean low
uncertainty [41] and hence it is not reliable for decision support. This is because
DNN models do not have a way of calculating uncertainty by themselves. Gal
and Ghahramani [41] have proposed a new method to estimate uncertainty in
DNN outputs that approximates Bayesian models, while not requiring a high
computing cost. Lakshminarayanan et al.[42] have demonstrated an alternate
approach that is scalable and can be easily implemented. Any mission critical
ML system has to include such uncertainty measures.

4 Conclusions

The purpose of this paper is to discuss a framework for a quality management
system needed to support the inclusion of AI components in the building of
business/mission critical applications. It should be clear from the description
above, that ML applications need a diﬀerent mindset from the start of a project.
Since AI is really a software component, we need to apply the relevant software
quality attributes to it. AI comes with some special quality attributes (fairness,
explainability, etc.) which have to be integrated into the quality management
methodology. Various processes and tools to support the quality management of
the application are in their infancy, mostly as prototypes from research projects.
In addition to the raw tooling needed for each task described in Sec.3, the in-
tegration of them across the life cycle to provide a holistic system is critical
for wide scale use. Furthermore, ethical guidelines from governments [9, 10] re-
quire an engineering implementation to demonstrate adherence. This does not
exist today. In spite of an extraordinary worldwide eﬀort devoted to Machine
Learning technology, the quality management of AI systems is fragmented and
incomplete. In order to meet the needs of society, we need an AI engineering
framework that meets the rigor needed. This paper provides an early glimpse of
such a framework.

Acknowledgements

Author thanks Rachel Bellamy, Evelyn Duesterwald, Eitan Farchi, Michael Hind,
David Porter, Orna Raz and Jim Spohrer for useful comments on the paper.

References

1. 2019 AI Index Report: https://hai.stanford.edu/research/ai-index-2019
2. KPMG 2019 Report: “AI Transforming the Enterprise”; OReilly 2019 Report: “AI
Adoption in the Enterprise”; Databricks 2018 Report: “Enterprise AI Adoption”;
MIT Sloan-BCG Research Report “Winning With AI”.

12

P. Santhanam

3. ISO/IEC 25010: 2011, Systems and software engineering – Systems and software
Quality Requirements and Evaluation (SQuaRE) – System and software quality
models.

4. Codacy Blog:ISO/IEC 25010 Software Quality Model. https://blog.codacy.com/iso-

25010-software-quality-model/

5. F. P. Brooks, The Mythical Man-Month: Essays on Software Engineering, Anniver-

sary Edition, Addison-Wesley Longman, Reading, MA (1995)

6. S. McConnell, Code Complete: A Practical Handbook of Software Construction, 2nd

Edition, Microsoft Press (2004)

7. B. Hailpern and P. Santhanam, “Software Debugging, Testing and Veriﬁcation”,

IBM Systems Journal, v.41, pp. 4-12 (2002).

8. P. Santhanam, E. Farchi and V. Pankratius, “Engineering Reliable Deep Learning
Systems”, AAAI Fall Symposium Series on AI in Government & Public Sector
(2019).

9. European Commission High-Level Expert Group on AI, “Ethics Guidelines for

Trustworthy AI”
https://ec.europa.eu/futurium/en/ai-alliance-consultation

10. Defense Innovation Board: “Principles: Recommendations on the Ethical Use of

Artiﬁcial Intelligence by the Department of Defense”. (2019)

11. E. Beck, et al. “The ML test score: A rubric for ML production readiness and
technical debt reduction”, IEEE International Conference on Big Data (2017)
12. S. Amershi, et al., “Software Engineering for Machine Learning: A Case Study”,
41st International Conference on Software Engineering: Software Engineering in
Practice (ICSE-SEIP 2019)

13. J. M. Zhang et al., “Machine Learning Testing: Survey, Landscapes and Horizons”,

arXiv:1906.10742 (2019)

14. R. Akkiraju, et al., “Characterizing machine learning process: A maturity frame-

work”, arXiv:1811.04871

15. Y. Zhangy et al., “An Empirical Study on TensorFlow Program Bugs”, 27th ACM
SIGSOFT International Symposium on Software Testing and Analysis (ISSTA 2018)
16. H.V. Pham et al., “CRADLE: Cross-Backend Validation to Detect and Localize
Bugs in Deep Learning Libraries”, 41st International Conference on Software Engi-
neering (ICSE 2019)

17. F. Macdonald et al., “A review of tool support for software inspections”, Sev-
enth International Workshop on Computer-Aided Software Engineering, pp.340-
349.(1995).

18. A. Gosain and G. Sharma, “Static Analysis: A Survey of Techniques and Tools”,
First International Conference on Intelligent Computing and Application (ICICA
2014), Springer.

19. J. Dolby, et al. “Ariadne: Analysis for Machine Learning Programs”, 2nd ACM
SIGPLAN International Workshop on Machine Learning and Programming Lan-
guages (MAPL 2018)

20. S.Nidhra and J. Dondeti, “Black Box and White Box Testing Techniques-A Lit-
erature Review”, International Journal of Embedded Systems and Applications
(IJESA) Vol.2, No.2, (2012)

21. K. Pei et al., “DeepXplore: Automated Whitebox Testing of Deep Learning Sys-

tems”, 26th ACM Symposium on Operating Systems Principles (SOSP 2017)

22. Y. Sun et al.,“Testing Deep Neural Networks”, arXiv:1803.04792v4
23. J. Sekhon and C. Fleming, “Towards Improved Testing For Deep Learning”, 41st
International Conference on Software Engineering: New Ideas and Emerging Results
(ICSE-NIER) (2019)

Quality Management of Machine Learning Systems

13

24. L. Ma et al. “DeepMutation: Mutation Testing of Deep Learning Systems”, IEEE
29th International Symposium on Software Reliability Engineering (ISSRE 2018).
25. L. Ma et al. “Combinatorial Testing for Deep Learning Systems”, arXiv:1806.07723
26. E. Breck at al. “Data Validation for Machine Learning”, Second SysML Conference.

(2019)

27. G. Barash et al, “Bridging the Gap between ML Solutions and Their Business
Requirements using Feature Interactions”, 27th ACM Joint Meeting on European
Software Engineering Conference and Symposium on the Foundations of Software
Engineering, (ESEC/FSE 2019)

28. O. Raz, et al., “Automatically detecting data drift in machine learning based clas-
siﬁers”, AAAI Workshop on Engineering Dependable and Secure Machine Learning
Systems (EDSMLS 2019)

29. IBM

Watson

OpenScale-Drift:

https://www.ibm.com/cloud/watson-

openscale/drift

30. J. Zhang, et al.,“Manifold:A Model-Agnostic Framework for Interpretation and
Diagnosis of Machine Learning Models”, IEEE Transactions on Visualization and
Computer Graphics, v.25, No. 1. (2019)

31. A. Chakarov, et al., “Debugging Machine Learning Tasks”, arXiv:1603.07292v1

(2016)

32. R.Guidotti, et al. , “A Survey of Methods for Explaining Black Box Models”, ACM

Computing Surveys, Article No.: 93 (2018)

33. IBM Research Blog: “Introducing AI Explainability 360”,

https://www.ibm.com/blogs/research/2019/08/ai-explainability-360/

34. S. Verma and J. Rubin, “Fairness Deﬁnitions Explained”, IEEE/ACM Interna-

tional Workshop on Software Fairness (FairWare) (2018)

35. R. K. E. Bellamy, et al., “AI Fairness 360: An Extensible Toolkit for Detecting, Un-
derstanding, and Mitigating Unwanted Algorithmic Bias”, IBM Journal of Research
and Development, v. 63, pp. Issue: 4/5, (2019)

36. H. Xu, et al. “Adversarial Attacks and Defenses in Images, Graphs and Text: A

Review”, arXiv:1909.08072

37. IBM Research Blog: “The Adversarial Robustness Toolbox: Securing AI
Against Adversarial Threats”, https://www.ibm.com/blogs/research/2018/04/ai-
adversarial-robustness-toolbox/

38. M. Arnold, et al., “FactSheets: Increasing trust in AI services through supplier’s
declarations of conformity”, IBM Journal of Research and Development, v. 63, pp.
Issue: 4/5, (2019)

39. J. Brownlee, BLOG: “Metrics To Evaluate Machine Learning Algorithms in

Python”
https://machinelearningmastery.com/metrics-evaluate-machine-learning-
algorithms-python/

40. M. Arnold, et al., “Towards Automating the AI Operations Lifecycle”, MLOps

Workshop at MLSys (2020).

41. Y. Gal and Z. Ghahramani, “Dropout as a Bayesian approximation: Representing
model uncertainty in deep learning”, 33rd International Conference on Machine
Learning, (ICML 2016)

42. B. Lakshminarayanan, A. Pritzel & C. Blundell, “Simple and Scalable Predictive
Uncertainty Estimation using Deep Ensembles”, Advances in Neural Information
Processing Systems 30 (NIPS 2017)

