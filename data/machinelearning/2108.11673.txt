Why Adversarial Reprogramming Works, When It Fails, and How to Tell the Diﬀerence

Yang Zhenga, Xiaoyi Fenga, Zhaoqiang Xiaa, Xiaoyue Jianga, Ambra Demontisb,∗, Maura Pintorb, Battista Biggiob, Fabio Rolia,b

aNorthwestern Polytechnical University, Xi’an, China
bUniversity of Cagliari, Italy

1
2
0
2

g
u
A
1
3

]

G
L
.
s
c
[

2
v
3
7
6
1
1
.
8
0
1
2
:
v
i
X
r
a

Abstract
Adversarial reprogramming allows repurposing a machine-learning model to perform a diﬀerent task. For example, a model trained
to recognize animals can be reprogrammed to recognize digits by embedding an adversarial program in the digit images provided as
input. Recent work has shown that adversarial reprogramming may not only be used to abuse machine-learning models provided as
a service, but also beneﬁcially, to improve transfer learning when training data is scarce. However, the factors aﬀecting its success
are still largely unexplained. In this work, we develop a ﬁrst-order linear model of adversarial reprogramming to show that its
success inherently depends on the size of the average input gradient, which grows when input gradients are more aligned, and when
inputs have higher dimensionality. The results of our experimental analysis, involving fourteen distinct reprogramming tasks, show
that the above factors are correlated with the success and the failure of adversarial reprogramming.

Keywords: adversarial machine learning, adversarial reprogramming, neural networks, transfer learning

1. Introduction

Adversarial reprogramming allows repurposing a machine-
learning model to perform a chosen task.
In particular, a
model trained to recognize certain classes of samples from a
source domain (e.g., ImageNet objects) can be reprogrammed
to classify samples belonging to a diﬀerent, target domain (e.g.,
MNIST handwritten digits). To this end, one should ﬁrst estab-
lish a mapping function between the class labels of the source
domain and those of the target domain (e.g., the handwritten
digit “0” can be associated to the ImageNet class “tench”, the
handwritten digit “1” to the ImageNet class “goldﬁsh”, etc.).
Once such a class mapping is established, the target-domain
samples are modiﬁed to embed an adversarial program, i.e.,
a universal adversarial perturbation, equal for all the target-
domain samples, and optimized against the target model to have
such samples assigned to the desired source-domain classes.
An example of adversarial program for reprogramming an Im-
ageNet model to classify handwritten digits is shown in Fig. 1
(top). In this case, the adversarial program consists of a frame
surrounding the input image, but such programs, as well as
other adversarial perturbations, can also be optimized to be su-
perimposed on the input images.

Adversarial reprogramming was originally introduced to
abuse machine-learning models provided as a service and steal
computational resources, by repurposing a model to perform
a task chosen by the attacker. For instance, an online service
that uses deep networks originally trained to classify images of
objects can be reprogrammed to recognize numbers and digits

for solving CAPTCHAs1 and automating the creation of fake
accounts [3]. However, it has been recently shown that adver-
sarial reprogramming can be also used for beneﬁcial tasks like
transfer learning, in scenarios with scarce data and constrained
resources. The authors of [4] have shown that in such scenar-
ios, adversarial reprogramming can achieve better performance
than ﬁne-tuning, i.e., the classical approach to transfer learning
in deep networks. Moreover, they have shown that the adver-
sarial program can be computed even if only black-box access
is available to the machine-learning model, i.e., by repeatedly
querying the model and observing its predictions on diﬀerent
inputs, without having knowledge of its internal details, includ-
ing its architecture and learned parameters.

Notwithstanding the practical relevance of adversarial repro-
gramming, the factors aﬀecting its success are still unclear.
Even the most recently published papers have not answered the
following key questions: when and why adversarial reprogram-
ming works, but most importantly, when and why it fails. In
the original work that proposed adversarial reprogramming [3],
the authors successfully applied this technique to solve a com-
plex task, i.e., reprogramming an ImageNet model to classify
MNIST handwritten digits. However, they also showed that re-
programming did not successfully work when trying to repro-
gram untrained networks to solve the same task. While starting
investigating the reasons behind such failures, we tried to apply
reprogramming to what we thought was a simpler task: repro-
gramming a small convolutional neural network (CWNet [2],
described in Table 1), trained to recognize handwritten Chinese
digits, so that it could recognize the MNIST handwritten (Ara-
bic) digits, as depicted in Fig. 1 (bottom). Despite our eﬀorts

∗Corresponding author
Email address: ambra.demontis@unica.it (Ambra Demontis)

1Completely Automated Public Turing test to tell Computers and Humans

Apart.

Preprint submitted to Pattern Recognition

September 1, 2021

 
 
 
 
 
 
Figure 1: Adversarial reprogramming of AlexNet [1], trained on the ImageNet dataset (top), and CWNet [2], trained on Chinese digits (bottom), to recognize
MNIST handwritten digits. Each digit class is mapped to a diﬀerent class among those predicted by the target model (e.g., for AlexNet, 0 to tench, 1 to goldish,
etc.). In the example, the handwritten digit 1 is embedded in both adversarial programs expected to be classiﬁed as goldﬁsh by AlexNet and as the Chinese digit 1
by CWNet. However, while AlexNet can be successfully reprogrammed, reprogramming fails for CWNet.

in tuning the hyperparameters, we found that adversarial re-
programming surprisingly fails on this intuitively simpler task.
Our conjecture was based on the fact that both the source and
the target domain in this case are digit recognition problems.
However, as we will show later, task similarity does not help
adversarial reprogramming.

To shed light on the underlying factors aﬀecting success and
failure of adversarial reprogramming, in this work we provide a
ﬁrst-order linear analysis of adversarial reprogramming, start-
ing from the observation that adversarial programs can be in-
deed considered as universal adversarial perturbations (Sect. 2).
Our analysis shows that the success of reprogramming is in-
herently dependent on the size of the average input gradient,
which grows (i) when the input gradients (i.e., the gradients of
the classiﬁcation loss w.r.t. the input values, computed on dif-
ferent target-domain samples) are more aligned, and (ii) when
the number of input dimensions increases. To validate the pro-
posed mathematical model, we carry out an extensive empiri-
cal analysis (Sect. 3) involving three diﬀerent neural-network
models and four datasets, resulting in fourteen distinct repro-
gramming tasks. Our experimental analysis shows that our ﬁrst-
order model of adversarial reprogramming correctly highlights
the main factors behind its success and failure. We conclude
the paper by discussing related work (Sect. 4), along with the
main contributions, limitations and future developments of our
work (Sect. 5).

2. Understanding Adversarial Reprogramming

In this section we develop a ﬁrst-order mathematical model
of adversarial reprogramming, which aims to highlight the main
factors underlying its success and failure. To this end, we ﬁrst
formalize adversarial reprogramming as a loss minimization

problem (Sect. 2.1). We then show that, under linearization
of the loss function, the optimal solution can be computed in
closed form, and the loss reduction is proportional to the size
of the average input gradient (Sect. 2.2). This in turn means
that reprogramming a target model is easier when the input gra-
dients are well aligned, and inputs are high dimensional. We
conclude the section by discussing how our analysis can be
extended to adversarial programs more general than those de-
picted in Fig. 1 (Sect. 2.3).

2.1. Problem Formulation

Let us assume that we have a source-domain dataset S =
j=1 and a target-domain dataset T = (xi, yi)n
( ˜x j, ˜y j)m
i=1, consist-
ing of m and n samples, respectively, along with their labels. In
both cases, the input samples are represented as d-dimensional
vectors in X = [−1, 1]d, while the class labels belong to diﬀer-
ent sets, respectively, ˜y ∈ ˜Y and y ∈ Y for the source and the
: X (cid:55)→ ˜Y
target domain. We are also given a target model f
which makes predictions on the source domain, parameterized
by θ ∈ Rt. To reprogram this model, we ﬁrst deﬁne a class-
mapping function h : Y (cid:55)→ ˜Y that maps each label of the target
domain to a label of the source domain (e.g., the target-domain
label “0” to the source-domain label “tench”). We then need
to optimize and embed the adversarial program in the target-
domain samples.
Reprogramming Mask. Even though our model and analy-
sis can be generalized and extended to many diﬀerent kinds of
adversarial programs, as discussed in Sect. 2.3, we restrict our
focus here on adversarial programs consisting of a frame sur-
rounding the target-domain samples, as shown in Fig. 1 and
originally considered in the seminal work in [3]. This means
that the target-domain samples are assumed to be smaller than
the source-domain samples, and padded with zeros to reach the

2

(cid:3)(cid:51)(cid:85)(cid:72)(cid:71)(cid:76)(cid:70)(cid:87)(cid:72)(cid:71)(cid:3)(cid:68)(cid:86)(cid:29)(cid:3)(cid:74)(cid:82)(cid:79)(cid:71)(cid:73)(cid:76)(cid:86)(cid:75)(cid:3)(cid:70)(cid:82)(cid:85)(cid:85)(cid:72)(cid:70)(cid:87)(cid:3)(cid:83)(cid:85)(cid:72)(cid:71)(cid:76)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)(cid:36)(cid:79)(cid:72)(cid:91)(cid:49)(cid:72)(cid:87)(cid:3)(cid:51)(cid:85)(cid:72)(cid:71)(cid:76)(cid:70)(cid:87)(cid:72)(cid:71)(cid:3)(cid:68)(cid:86)(cid:29)(cid:3)(cid:14269)(cid:3)(cid:3)(cid:90)(cid:85)(cid:82)(cid:81)(cid:74)(cid:3)(cid:83)(cid:85)(cid:72)(cid:71)(cid:76)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)(cid:54)(cid:16)(cid:38)(cid:49)(cid:49)(cid:53)(cid:72)(cid:83)(cid:85)(cid:82)(cid:74)(cid:85)(cid:68)(cid:80)(cid:80)(cid:76)(cid:81)(cid:74)(cid:3)(cid:87)(cid:68)(cid:86)(cid:78)(cid:19)(cid:3)(cid:3)(cid:3)(cid:314)(cid:3)(cid:3)(cid:3)(cid:87)(cid:72)(cid:81)(cid:70)(cid:75)(cid:20)(cid:3)(cid:3)(cid:3)(cid:314)(cid:3)(cid:3)(cid:3)(cid:74)(cid:82)(cid:79)(cid:71)(cid:73)(cid:76)(cid:86)(cid:75)(cid:3)(cid:11)(cid:13)(cid:12)(cid:171)(cid:3)(cid:17)(cid:17)(cid:17)(cid:53)(cid:72)(cid:83)(cid:85)(cid:82)(cid:74)(cid:85)(cid:68)(cid:80)(cid:80)(cid:76)(cid:81)(cid:74)(cid:3)(cid:87)(cid:68)(cid:86)(cid:78)(cid:19)(cid:3)(cid:3)(cid:3)(cid:314)(cid:3)(cid:3)(cid:3)(cid:14269)(cid:20)(cid:3)(cid:3)(cid:3)(cid:314)(cid:3)(cid:3)(cid:3)(cid:2893)(cid:3)(cid:11)(cid:13)(cid:12)(cid:171)(cid:3)(cid:17)(cid:17)(cid:17)Algorithm 1 Adversarial Reprogramming via Stochastic Gra-
dient Descent
Input: the target-domain dataset T = (xi, yi)n

i=1, the class-
mapping function h, the model parameters θ, the batch size
B, the number of iterations N, the step size η, and the pro-
jection operator ΠX.

Output: the optimal adversarial program δ∗.
1: δ ← 0, δ∗ ← δ, lossδ∗ ← ∞
2: t ← 0
3: for t < N do
4:
5:
6:

Randomly shuﬄe the samples in T
b ← 0
for b < (cid:98) N
B (cid:99) do
g ← 1
B
δ ← δ − η sign(g)
δ ← ΠX(δ)
b ← b + 1

(cid:80)B·b+B−1
k=B·b

7:
8:
9:
10:
11:
12:

end for
lossδ = L(δ, T ) (compute loss as given in Eq. 1)
if lossδ < lossδ∗ then

gk (average input gradient)

δ∗ ← δ
lossδ∗ ← lossδ

13:
14:
15:
16:
17:
18: end for
19: return δ(cid:63)

end if
t ← t + 1

required input size d; for example, MNIST handwritten dig-
its consist of 28 × 28 = 784 pixels per channel, and should be
padded with more than 49,000 zeros per channel to reach the in-
put size of ImageNet models (which have 224 × 224 = 50, 176
pixels per channel). We represent reprogramming masks as a
binary vector M ∈ {0, 1}d, whose values are set to 0 in the
region occupied by the target-domain sample, and to 1 in the
surrounding frame (i.e., the portion of the image which can be
modiﬁed), as shown in Fig. 2.

Under these assumptions, the optimal adversarial program δ(cid:63)
can be obtained by solving the following optimization problem:

δ(cid:63) ∈ arg min
δ∈[−1,1]d

L(δ, T ) = 1
n

n(cid:88)

i=1

(cid:96)(xi + δ ◦ M, h(yi), θ) ,

(1)

where δ is the adversarial program being optimized within
the feasible domain X = [−1, 1]d, M ∈ {0, 1}d is the repro-
gramming mask, the ◦ operator denotes element-wise vector
multiplication, and (cid:96) is the cross-entropy loss, which is mini-
mized when the perturbed target-domain samples are assigned
to the desired source-domain classes. Note that the constraint
δ ∈ [−1, 1]d is equivalent to upper bounding the (cid:96)∞ norm of δ
as (cid:107)δ(cid:107)∞ ≤ 1.
Solution Algorithm. In this work, we use Algorithm 1 to solve
the optimization problem in Eq. (1) via stochastic gradient de-
scent. This algorithm extends the Projected Gradient Descent
(PGD) algorithm originally used in [5] to optimize adversar-
ial perturbations within an (cid:15)-sized (cid:96)∞-norm constraint. Our al-
gorithm iteratively updates the adversarial program δ to mini-

mize the expected loss on the target-domain samples (line 3). In
each iteration, the target-domain samples are randomly shuﬄed
(line 4) and subdivided in B batches. The adversarial program
is then updated by iterating over the batches (line 6). In par-
ticular, the average input gradient g is ﬁrst computed on the
batch samples (line 7), and then the adversarial program is up-
dated with an η-sized step (line 8) along the sign of the nega-
tive gradient (i.e., the steepest descent direction under the given
(cid:96)∞-norm constraint). In our case, the gradient for the ith sam-
ple is computed as gi = ∇δ(cid:96)(xi, h(yi), θ) ◦ M, i.e., including
the application of the reprogramming mask M. After updating
δ, the algorithm projects the program onto the feasible space
X = [−1, 1]d, using the box-projection operator ΠX (line 9).
The algorithm ﬁnally returns the adversarial program δ(cid:63) that
achieves the minimum classiﬁcation loss across the whole opti-
mization process (line 19).

2.2. A First-order Model of Adversarial Reprogramming

We are now ready to introduce the linear model proposed in
this work to better understand which underlying factors mostly
aﬀect the success of adversarial reprogramming.
Linearization. Reprogramming aims to minimize the loss in
Eq. (1) to have the target-domain samples classiﬁed as desired
within the source-domain classes. Let us linearize the loss in
Eq. (1) as:

L(δ) ≈

1
n

n(cid:88)

i=1

(cid:96)(xi, h(yi), θ) + 1
n

n(cid:88)

i=1

δ(cid:62) ∇x(cid:96)(xi, h(yi), θ) ◦ M
(cid:124)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:123)(cid:122)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:125)
gi

,

(cid:80)

(2)
where gi is the masked input gradient, i.e., the input gradient
computed for the ith test sample, multiplied by the reprogram-
ming mask M. This amounts to zeroing the values of the input
gradient for which the mask is zero.
Source and Target Domain Alignment. The ﬁrst term in
i (cid:96)(xi, h(yi), θ), measures how diﬃcult is
Eq. 2, i.e., the loss 1
n
to reprogram a given machine-learning model from the source
to the target domain, without applying any input perturbation.
If this loss term is small, indeed, it means that the unperturbed
samples from the target domain are already consistently as-
signed to the desired source-domain classes with high proba-
bility; thus, even small perturbations may provide high repro-
gramming accuracy. Conversely, when this one-to-one map-
ping between source and target classes is only weakly present,
this term takes on larger values, meaning that reprogramming
may be more challenging. However, our experiments in Sect. 3
show that this term does not play a signiﬁcant role for repro-
gramming accuracy, since the source-domain samples and the
target-domain samples are normally quite diﬀerent.
Loss Decrement. The second term in Eq. (2) is the loss decre-
ment that can be achieved when optimizing δ. It can be rewrit-
ten as:

∆L(δ) = δ(cid:62) 1
n

(cid:88)

i

gi = δ(cid:62) g ,

(3)

being g the average input gradient. Minimizing the scalar prod-
uct δ(cid:62) g under the constraint δ ∈ [−1, 1]d given in Eq. (1) is

3

Figure 2: Reprogramming mask M used to restrict the adversarial perturbation to the frame surrounding the target-domain input image, initially padded with zeros.

equivalent to minimizing an inner product over a unit-norm (cid:96)∞
ball. It is not diﬃcult to see that, in this case, the optimal per-
turbation is given as δ(cid:63) = −sign( g), i.e., it is found by setting
j , for j = 1, . . . , d, equal to the negative sign
each component δ(cid:63)
of the corresponding element in g. Substituting δ(cid:63) into Eq. (3),
we obtain that:

∆L(δ(cid:63)) = −sign( g)(cid:62) g = −

d(cid:88)

j=1

sign(g j)g j = −

d(cid:88)

j=1

|g j| = −(cid:107) g(cid:107)1 .

(4)
This result highlights that reprogramming is more successful
when the (cid:96)1-norm of the average input gradient g is larger,
which in turn means that reprogramming is expected to work
better when:

• the input gradients gi are more aligned, as this would in-

crease the (cid:96)1 norm of the average input gradient g;

• the number of input dimensions d is higher, as the (cid:96)1 norm
of g scales linearly with the number of input dimensions.

2.2.1. Gradient Alignment

To measure the alignment of the input gradients gi w.r.t. their

average g, we introduce the gradient-alignment metric r:

r =

(cid:107) g(cid:107)1
(cid:80)
i (cid:107)gi(cid:107)1

1
n

∈ [0, 1] .

(5)

This metric equals zero only when the average vector is g = 0,
while it equals one when the (cid:96)1 norm of g is equal to the aver-
age of the (cid:96)1 norms of the input gradients. Note that the latter
case does not require the input gradients to be all equal to the
average g, conversely to what one may intuitively think. To en-
sure that r = 1, indeed, it suﬃces that the input gradients gi
are orthogonal. For example, consider a simple case in which
g1 = (0, 0, 1) and g2 = (1, 0, 0), with g = (0.5, 0, 0.5). It is not
diﬃcult to see that the average (cid:96)1 norm of g1 and g2 equals 1
as well as the (cid:96)1 norm of g. This means that gradient alignment
is maximized even if the input gradients are nearly orthogonal,
which is quite likely to happen especially in high-dimensional
input spaces (at least under the assumption that they are inde-
pendent and randomly generated).

2.2.2. Reprogramming Mask Size

The other main factor aﬀecting the success of adversarial re-
programming, as anticipated before, is the number of input di-
mensions d. In particular, not only gradient alignment is ex-
pected to increase when the input space is high dimensional,
but also the (cid:96)1 norm of the average input gradient is expected
to grow linearly with the number of input dimensions d. Note
however that, when using a reprogramming mask, the number
of dimensions (i.e., pixels) used to optimize the adversarial pro-
gram is not equal to d, but rather to the size of the reprogram-
ming frame surrounding the input image. We will thus control
the actual size of the adversarial program in our experiments
by varying the number of pixels that are used by the adversar-
ial program, i.e., by changing the size of the reprogramming
mask. The size of the reprogramming mask can be measured
by simply counting the number of ones in M, e.g., by comput-
ing (cid:107)M(cid:107)1.

2.3. Extension to Other Perturbation Models

As discussed in Sect. 2.2, the success of reprogramming
mostly depends on the minimization of the second term of
Eq. (1), whose minimum, under linearization,
is given as
∆L(δ(cid:63)) = δ(cid:62) g = −(cid:107) g(cid:107)1.

Interestingly, it is not diﬃcult to see that the (cid:96)1 norm of g
naturally appears here as it corresponds to the dual norm of the
(cid:96)∞ norm used in Eq. 1 to bound the adversarial perturbation δ.
While this result may not be surprising at ﬁrst sight, as the dual
norm is obtained by deﬁnition as the maximization of a scalar
product over a unit ball [6], it allows us to extend our model
to adversarial programs optimized with diﬀerent perturbation
constraints. For example, one may use a generic (cid:96)p-norm con-
straint (with p = 1, 2, ∞) bounded by a small perturbation size
(cid:15), i.e., (cid:107)δ(cid:107)p ≤ (cid:15), to superimpose the adversarial program in an
imperceptible manner on the input image, and found that the
optimal ∆L(δ(cid:63)) is simply given as ∆L(δ(cid:63)) = −(cid:15)(cid:107)g(cid:107)q, being q
the dual norm of p.

We conclude this section by pointing out that, while similar
ﬁndings have been derived in [7] to study the behavior of ad-
versarial examples in high-dimensional spaces, our model ex-
tends the analysis in [7] to account for adversarial perturbations
which are optimized over diﬀerent input samples (and not just
on a single input image). Such perturbations do not only include
adversarial programs, but they also encompass universal adver-
sarial perturbations [8] and robust adversarial examples [9, 10],

4

after optimization!input image (with padding)M!=0M!=1&+(⋆(&reprogramming mask making our model readily applicable to study and quantify also
the eﬀectiveness of such attacks.

3. Experimental Analysis

We report here an extensive experimental analysis aimed to
evaluate the impact of the factors identiﬁed by the mathematical
model presented in Sect. 2 on reprogramming accuracy.

3.1. Experimental Setup

We consider 14 diﬀerent reprogramming tasks and 3 model
architectures, focusing on deep neural networks trained to per-
form computer-vision tasks. In the following, we provide the
details required to reproduce our empirical analysis.
Datasets. Our experimental analysis considers four diﬀerent
computer-vision datasets: two object recognition datasets, i.e.,
ImageNet and CIFAR-10, and two datasets containing images
of handwritten Arabic (MNIST) and Chinese (HCL2000) dig-
its.
ImageNet2 is one of the largest publicly-available computer-
It contains images belonging to 1, 000 cate-
vision datasets.
gories subdivided in around 1.2 million training images and
150,000 test images. The images are collected from Internet
by search engines and labeled by humans via crowdsourcing.
We use this dataset only as a source-domain dataset, as many
pretrained models on ImageNet are readily available.
CIFAR-103 is a ten-class image-classiﬁcation dataset made up
of small resolution (32 × 32) color images, subdivided into
50,000 training and 10,000 test images.
MNIST 4 is a ten-class dataset containing images of handwritten
Arabic digits. It consists of grayscale images of size 28 × 28,
subdivided in 60,000 training and 10,000 test images.
HCL20005 is a large-scale handwritten Chinese character
database [11], containing grayscale images of size 64 × 64. We
consider only the ten classes corresponding to the Chinese dig-
its, which result into 7,000 training and 3,000 test images.
Preprocessing. We rescale the input images in X = [−1, 1]d,
with d = 224×224×3, to match the input size of the considered
models. This requires padding input images with zeros if they
are smaller in size, and replicating the image content on each
color channel for single-channel grayscale images (like in the
case of MNIST and Chinese digits).
Classiﬁers. We consider three diﬀerent neural-network archi-
tectures: the CWNet network proposed in [2] (Table 1); and the
pretrained ImageNet models AlexNet [1] and EﬃcientNet [12].
All the considered models have input size d = 224×224×3, and
apply z-score batch normalization before processing the input
samples. CWNet is only trained using MNIST and HCL2000
as the source-domain data. AlexNet and EﬃcientNet are in-
stead used with ImageNet, MNIST and HCL2000 as source-
domain datasets. For AlexNet and EﬃcientNet, we also con-
sider a setting in which their weights are set to random values,

2https://www.image-net.org/
3https://www.cs.toronto.edu/~kriz/cifar.html
4http://yann.lecun.com/exdb/mnist/
5http://www.pris.net.cn/introduction/teacher/lichunguang

Table 1: Architecture of the CWNet network trained on digit images in [2].

Dimension

Layer Type
Conv. + ReLU
32 ﬁlters (3 × 3)
Conv. + ReLU
32 ﬁlters (3 × 3)
2 × 2
Max Pooling
Conv. + ReLU
64 ﬁlters (3 × 3)
Conv. + Dropout (0.5%) + ReLU 64 ﬁlters (3 × 3)
Max Pooling
Fully Connected + ReLU
Fully Connected + ReLU
Softmax

2 × 2
200 units
200 units
10 units

which amounts to considering their untrained versions as done
in [3]. When training the considered models on MNIST and
HCL2000, we run stochastic gradient descent for 10 epochs,
with step size, momentum, and batch size respectively equal to
0.001, 0.9, and 10.

Adversarial Programs. To optimize the adversarial program
δ, we use Algorithm 1. Before optimizing it, we ﬁx the class-
mapping h as a function that maps the ﬁrst ten classes of the
source dataset to the ﬁrst ten classes of the target dataset, as
done in [3]. The only dataset having more than 10 classes is Im-
ageNet, from which we only consider the ﬁrst ten classes (i.e.,
tench, goldﬁsh, white shark, tiger, shark, hammerhead, electric
ray, stingray, cock, hen, ostrich). We set the step size η = 0.005,
N = 100 epochs, and we use batches of B = 50 samples, sam-
pled from a larger set of 5, 000 images randomly drawn from the
training set of the target-domain dataset T . Moreover, as the ad-
versarial program is expected to generalize to unseen samples,
we do not compute the loss in Eq. (1) using the same samples
used to optimize the perturbation, but rather use a separate set
of 5, 000 images sampled again from the target-domain train-
ing set. This should provide a better, unbiased evaluation of the
eﬀectiveness of the adversarial program.
Performance Metrics. We consider diﬀerent metrics to evalu-
ate the performance of reprogramming along with the underly-
ing factors aﬀecting its success, evaluated on a separate set of
1, 000 samples randomly drawn from the target-domain test set.
In particular, in addition to reprogramming accuracy (RA), we
consider: domain alignment (DA), evaluated as the accuracy
of the model on the target-domain samples padded with zeros
(i.e., before optimizing the adversarial program δ); and gradi-
ent alignment (Eq. 5) before (r0) and after (rN) reprogramming,
i.e., during the ﬁrst and the last iteration of Algorithm 1.

3.2. Experimental Results

The experimental results on the given 14 reprogramming
tasks are reported in Table 2, while the corresponding adver-
sarial programs are shown in Fig. 3. As highlighted in Table 2
with diﬀerent colors, adversarial reprogramming may exhibit
diﬀerent reprogramming accuracy (RA) values: it may work re-
markably well (RA ≥ 90%), it may work poorly (RA ≈ 50%),
or it may even completely fail (RA ≤ 30%). In the remainder

5

Table 2: Results of reprogramming AlexNet, EﬃcientNet, and CWNet from diﬀerent source (S) to target (T ) domains. For each reprogramming task, the table
reports domain alignment (DA), reprogramming accuracy (RA), and gradient alignment (Eq. 5) computed before (r0) and after (rN ) optimizing the adversarial
program δ. The cases in which reprogramming works well/poorly/badly are highlighted in green/yellow/red.

S

T

Model
EﬃcientNet
AlexNet
EﬃcientNet
AlexNet

ImageNet HCL2000
ImageNet HCL2000
ImageNet MNIST
ImageNet MNIST
ImageNet CIFAR-10 EﬃcientNet
ImageNet CIFAR-10
HCL2000
MNIST
MNIST
HCL2000
HCL2000 MNIST
HCL2000 MNIST
MNIST
HCL2000
MNIST
HCL2000

AlexNet
AlexNet
CWNet
AlexNet
CWNet
AlexNet
AlexNet
EﬃcientNet
EﬃcientNet

U
U
U
U

r0

rN

RA

DA
5.3% 98.1% 18.4% 21.14%
2.5% 97.2% 19.3% 19.35%
14.3% 90.6% 17.5% 20.33%
11.6% 90.1% 29.0% 18.75%
10.2% 51.1% 13.5% 10.54%
9.3% 46.0% 13.6% 11.28%
12.8% 49.1% 14.5% 9.11%
9.5% 45.3% 16.4% 13.59%
9.9% 21.8% 92.1% 5.27%
9.9% 19.1% 92.8% 4.82%
6.0%
11.8% 20.5% 5.0%
6.36%
9.0% 28.7% 6.7%
3.38%
11.8% 11.8% 4.0%
5.72%
6.9%
9.0%
9.0%

of this section, we analyze the impact on reprogramming accu-
racy of the main factors identiﬁed by our mathematical model
in Sect. 2, i.e., (i) the alignment between source and target do-
main, (ii) the alignment of the input gradients, and (iii) the size
of the reprogramming mask.

Source and Target Domain Alignment. Let us now analyze the
inﬂuence of the source-target domain alignment on reprogram-
ming accuracy. As we have explained in Sect. 2, reprogram-
ming accuracy also depends on the classiﬁer loss on the target-
domain dataset before reprogramming (the ﬁrst term of Eq. 2).
In Table 2, we report the accuracy of the classiﬁer on the target-
domain dataset before reprogramming, referred to as domain
alignment (DA). The table shows that DA is usually quite low
(around 10%), even when reprogramming accuracy is very high
(RA > 90%), which means that the source and target alignment
is not correlated with the reprogramming accuracy. To explain
why domain alignment is low, in Fig. 4 we report the confu-
sion matrices computed before (left plot) and after (right plot)
reprogramming. These matrices show that, before reprogram-
ming, the target-domain samples are often assigned to a single
class or a few classes. The reason is that, before reprogram-
ming, the target-domain samples are simply padded with zeros,
thus being more likely to be classiﬁed similarly.

Gradient Alignment. Here, we analyze the impact of gradient
alignment on reprogramming accuracy. For each classiﬁer and
reprogramming task, we compute the gradient-alignment met-
ric r (Eq. 5) before (r0) and after reprogramming (rN). The
reason is that, as previously explained, before reprogramming,
the target-domain samples are simply padded with zeros and
tend to be assigned to one or few classes. Accordingly, the in-
put gradients computed before optimizing the program are not
expected to be really informative, while they are expected to
be more informative when the program is optimized and repro-
gramming accuracy starts increasing.

The values of r0 and rN for the given reprogramming tasks
are reported in Table 2. In Fig. 5 we also report the correla-

tion between gradient alignment and reprogramming accuracy,
computed using Pearson (P), Spearman (S), and Kendall (K)
methods, along with the corresponding permutation tests and p-
values. The correlation between reprogramming accuracy RA
and gradient alignment before reprogramming (r0) is not sig-
niﬁcant (p > 0.05) mostly due to the presence of two outlying
observations (i.e., AlexNet HCL2000 → MNIST, and CWNet
HCL2000 → MNIST). The correlation values are much higher
and statistically signiﬁcant, instead, when considering gradient
alignment after reprogramming (rN), e.g., the correlation com-
puted with the Pearson coeﬃcient is 0.98 with p < 1e − 8.
These results show that gradient alignment, especially when
computed after reprogramming (rN), is strongly and positively
correlated with reprogramming accuracy, thus conﬁrming the
intuition provided by our mathematical model.

Reprogramming Mask Size. We ﬁnally analyze the impact of
the reprogramming mask size on reprogramming accuracy.

To this end, we consider reprogramming masks of increas-
ing sizes, using 64, 128, and 224 as their width and height val-
ues. We report reprogramming accuracy and the proposed mea-
sures computed for each classiﬁer and reprogramming task in
Table 3, and the corresponding correlation tests in Fig. 6. While
the right plot in Fig. 6 shows again that reprogramming accu-
racy is strongly correlated with the gradient alignment rN, also
for such reprogramming cases, the left plot shows that repro-
gramming accuracy is correlated also with the reprogramming
mask size, conﬁrming again the soundness of the proposed
mathematical model. From the values in Table 3, it should also
be noted that the reprogramming mask size can have a relevant
impact on reprogramming accuracy, e.g., in the case AlexNet
ImageNet → MNIST, there is an accuracy diﬀerence of 15%
between the performance obtained with the smallest and that
obtained with the largest mask size considered.

Summary of the Results. The extensive experimental analysis
that we have performed supports our mathematical model. In
particular, our work answers the question asked in the title:

6

EﬃcientNet
(ImageNet →HCL2000)

AlexNet
(ImageNet →HCL2000)

EﬃcientNet
(ImageNet →MNIST)

AlexNet
(ImageNet →MNIST)

EﬃcientNet
(ImageNet →CIFAR-10)

AlexNet
(ImageNet →CIFAR-10)

AlexNet
(MNIST →HCL2000)

CWNet
(MNIST →HCL2000)

AlexNet
(HCL2000 →MNIST)

CWNet
(HCL2000 →MNIST)

AlexNet
(U → MNIST)

AlexNet
(U → HCL2000)

EﬃcientNet
(U → MNIST)

EﬃcientNet
(U → HCL2000)

Figure 3: Adversarial programs optimized to repurpose the networks in Table 2 from diﬀerent source to target (S → T ) domains.

Table 3: Results for programs with diﬀerent reprogramming mask sizes. See the caption of Fig. 2 for further details.

Mask Size
224
128
64
224
128
64
224
128
64
224
128
64

S

Model

T
ImageNet HCL2000 EﬃcientNet
ImageNet HCL2000 EﬃcientNet
ImageNet HCL2000 EﬃcientNet
ImageNet HCL2000
ImageNet HCL2000
ImageNet HCL2000
ImageNet MNIST
ImageNet MNIST
ImageNet MNIST
ImageNet MNIST
ImageNet MNIST
ImageNet MNIST

AlexNet
AlexNet
AlexNet
EﬃcientNet
EﬃcientNet
EﬃcientNet
AlexNet
AlexNet
AlexNet

rN

RA

r0
98.1% 18.4% 21.14%
96.0% 18.5% 14.52%
89.9% 18.6% 17.81%
97.2% 19.3% 19.35%
97.1% 17.4% 16.82%
88.4% 16.1% 16.39%
90.6% 17.5% 20.33%
84.0% 18.2% 17.15%
70.6% 17.3% 8.17%
90.1% 29.0% 18.75%
84.6% 17.0% 11.36%
76.5% 16.7% 7.17%

why adversarial reprogramming works, when it fails, and how
to tell the diﬀerence. Adversarial reprogramming works when
the size of the average input gradient of the target model is large
enough. In this case, a small universal perturbation is suﬃcient
to move the target-domain samples in a region of the decision
space where they are classiﬁed as desired. As we have empiri-
cally shown, this is true even when the source and the target do-
mains are poorly aligned. Therefore, it is the size of the average
input gradient that, if not large enough, causes reprogramming
to fail. Furthermore, as we have explained, the size of the aver-

age input gradient depends on: (i) gradient alignment, which
is strongly correlated with reprogramming accuracy, playing
a key role on the success of adversarial reprogramming; and
(ii) the reprogramming mask size, which however plays a much
more marginal role as small mask sizes are usually already suf-
ﬁcient to achieve good reprogramming performance. To sum-
marize, the main ﬁndings of our extensive experimental analy-
sis are:

• source and target domain alignment, in practice, does not

aﬀect reprogramming accuracy;

7

AlexNet (ImageNet →HCL2000), RA=97.2%

AlexNet (MNIST→HCL2000), RA=49.1%

AlexNet (HCL2000→MNIST), RA=21.8%

AlexNet (U→MNIST), RA=20.5%

Figure 4: Confusion matrices (true-vs-predicted classes) on four representative cases. For each case, we report the confusion matrix before (left) and after (right)
reprogramming.

Figure 5: Correlation between the reprogramming accuracy (RA) vs the gra-
dient alignment computed before r0 (left) and after rN (right) optimizing the
program.

Figure 6: Reprogramming accuracy (RA) versus reprogramming mask size
(left) and the gradient alignment computed after reprogramming rN (right).
Considering the networks: AlexNet (A), EﬃcientNet (E), and the datasets: Im-
ageNet (I), HCL2000 (Ch), MNIST (M).

• gradient alignment plays a key role in the success of repro-

gramming;

• larger reprogramming mask sizes facilitate reprogram-
ming, but having a much more marginal impact on its suc-
cess.

Additional Comments. Here, we provide further comments on
the experimental results. On the basis of the reprogramming ac-

curacy reported in Table 2, we argue that reprogramming works
better when the reprogramming task is simple and the func-
tion learned by the target classiﬁer is complex. Table 2 also
shows that reprogramming works well when the target model is
a large model trained on a large and complex dataset and repro-
grammed to perform a simple task (such as handwritten digit
classiﬁcation). On the other hand, it does not work well: (i)

8

012345678901234567890000000000280056036153664701000241030000003201070989671831446722000010000000000000000600030000000000000070200233366074118012345678901234567899800000000009700000000008810001000091010010000000990100000002950000001100109100000000188080000001011400000000008301234567890123456789000000000000000000008727222472672748861000000000000000000000000000000000000000000000000001170767929288642106300000000000012345678901234567899324245340212187201400002224801421110065890714198726119519430290000000000000000000000000000000023106427103420000000000012345678901234567890000000000102999789105941021189110300000000000000000000000000000000000000000000000000000000000000000000000000000000012345678901234567890000000000483121223917232110000000000000000100000000000002000200020361583124105892515000000000000000000006015274656752686627701234567890123456789000000000000000000000000000000000000000000000000000000000000000000000010299978910594102118911030000000000000000000001234567890123456789000000000008831002101000000000000000000000000000000000000000000000000001021194881059410011791102000000000000000000000255075r020406080100RAP:<1e-10,p-val:0.56S:0.4,p-val:0.15K:0.3,p-val:0.165101520rN20406080100RAP:0.98,p-val:<1e-8S:0.92,p-val:<1e-5K:0.78,p-val:<1e-4100150200ReprogrammingMasksize708090RAP:0.61,p-val:0.04S:0.71,p-val:<1e-2K:0.57,p-val:0.02E(I→Ch)A(I→Ch)E(I→M)A(I→M)101520rN708090RAP:0.81,p-val:<1e-2S:0.73,p-val:<1e-2K:0.61,p-val:<1e-2when learning on the target domain represents a complex task,
(ii) when the target model is small, and (iii) when the target
model is complex but trained on a simple and small dataset.
Whereas the ﬁrst and the second case support our hypothesis,
the latter deserves further clariﬁcation. When a complex clas-
siﬁer is trained on a small and simple dataset, the representa-
tions learned by the diﬀerent network layers tend to become
very similar [13], and some of the layers thus do not alter the
classiﬁer prediction [14]. Therefore, although the classiﬁer is
complex, the function learned by the classiﬁer remains simple.

4. Related Work

In this section, we brieﬂy review related work on reprogram-
ming. We then focus on attacks that optimize adversarial per-
turbations, including universal adversarial perturbations and ro-
bust physical-world adversarial examples. Finally, we review
work which provides additional insights on the vulnerability of
machine-learning models to adversarial perturbations based on
diﬀerent ﬁrst-order linear analyses.

4.1. Adversarial Reprogramming

Adversarial reprogramming has been originally proposed
in [3]. The authors have empirically assessed the performance
of adversarial reprogramming using diﬀerent trained and un-
trained deep neural networks. They showed that reprogram-
ming usually fails when applied to untrained networks (i.e.,
neural networks with random weights), whereas it works when
the target model is trained. In the latter case, reprogramming
works even when the attacker can manipulate only a small
subset of the image pixels. However, the authors have not
explained why reprogramming works and when it fails, leav-
ing this analysis to future work. Although subsequent work
has successfully applied reprogramming in diﬀerent scenar-
ios [4, 15, 16], no work has analyzed the reasons behind its
success and failure. To the best of our knowledge, our work is
thus the ﬁrst to identify and quantify the main factors underly-
ing the success of adversarial reprogramming.

4.2. Universal and Robust Adversarial Perturbations

Gradient-based attacks on machine-learning models [17,
18, 19, 20] have been demonstrated in a variety of applica-
tion domains, including computer vision and security-related
tasks [21, 22, 23, 24, 25, 26], even before the independent
discovery of adversarial examples against deep neural net-
works [27].

While earlier work has focused on optimizing a diﬀerent ad-
versarial perturbation for each input sample, in [8] the authors
have shown that it is even possible to optimize a single, uni-
versal adversarial perturbation, i.e., a ﬁxed perturbation that
can be applied to many diﬀerent input samples to have them
misclassiﬁed as desired. The underlying idea is to optimize the
perturbation on diﬀerent input samples, similar to the idea be-
hind the optimization of robust physical-world adversarial ex-
amples [9, 10], i.e., to optimize the adversarial perturbation

9

over diﬀerent transformations of the input image (e.g., subject
to changes in pose, rotation, illumination).

In this work, we argue that the mathematical formulation of
universal perturbations, robust adversarial examples, and ad-
versarial reprogramming is essentially the same, i.e., all these
attacks require optimizing the adversarial perturbation by av-
eraging the loss function over diﬀerent input images (even
though reprogramming optimizes the perturbation to repurpose
a model, while the other attacks aim to have input samples mis-
classiﬁed). For this reason, we believe that our analysis can be
readily applied in future work to provide a better understanding
of the eﬀectiveness of both universal adversarial perturbations
and robust adversarial examples.

4.3. First-order Analysis of Adversarial Perturbations

Previous work has analyzed the vulnerability of neural net-
works against adversarial examples and universal adversar-
ial perturbations. The authors of [28] have proven the exis-
tence of small universal perturbations, attributing them to the
low curvature of the decision boundaries of deep neural net-
works [29, 30]. The work in [7] is probably the closest one
to ours, as it also builds on the previously-proposed idea of
modeling the optimization of adversarial examples as a linear
problem [31]. The same idea has also been explored to de-
velop robust methods based on regularizing the input gradi-
ents [32, 33, 34], as well as to provide deeper insights on why
adversarial examples can often transfer across diﬀerent mod-
els [35]. The main diﬀerence between our work and the work
in [7] is that our model extends their analysis to also encompass
adversarial perturbations which are optimized on diﬀerent sam-
ples, thereby not only including adversarial examples, but also
adversarial reprogramming, universal adversarial perturbations,
and robust physical-world adversarial examples.

5. Contributions, Limitations and Future Work

Adversarial reprogramming has been originally proposed as
an attack aimed to abuse machine-learning models provided as
a service. However, it has been subsequently shown that such
a technique may also be used beneﬁcially, providing a valuable
approach to transfer learning. Despite its great practical rele-
vance, no previous work has explained the main factors aﬀect-
ing the performance of adversarial reprogramming, i.e., why it
works, when it fails, and how to tell the diﬀerence.

In this work, we have overcome this limitation by provid-
ing a ﬁrst-order linear analysis of adversarial reprogramming,
which sheds light on the underlying factors inﬂuencing repro-
gramming accuracy. We have then performed an extensive ex-
perimental analysis involving diﬀerent machine-learning mod-
els and fourteen diﬀerent reprogramming tasks. Thanks to our
theoretical and empirical analyses, we have shown that the suc-
cess of reprogramming depends on the size of the average in-
put gradient, which is larger when the input gradients are more
aligned, and when inputs have higher dimensionality. Our
work thus provides a ﬁrst concrete step towards analyzing the
success and failure of adversarial reprogramming, paving the

way to future work that may enable the development of bet-
ter defenses against adversarial reprogramming, and improved
transfer-learning algorithms. An interesting future development
of this work also includes deriving guidelines to help practi-
tioners to select machine-learning models which are easier to
repurpose for a diﬀerent task, thereby facilitating the process of
transfer learning.

Two limitations currently exist in our work. The ﬁrst is that
it is not immediately clear from our analysis whether and to
which extent the number of source- and target-domain classes
may have an impact on the performance of adversarial repro-
gramming, and this aspect certainly deserves a more detailed
empirical investigation in the future. The second limitation is
that we have only considered adversarial programs optimized
within an l∞-norm constraint in this work. Nevertheless, our
analysis can be easily extended to other lp-norm perturbation
models, as discussed in Sect. 2.3, and it can also be exploited
to provide deeper insights on attacks in which the adversarial
perturbation is averaged over diﬀerent input samples (including
universal adversarial perturbations and robust physical-world
adversarial examples), as discussed in Sect. 4.3. We ﬁrmly
believe that exploring these research directions will certainly
provide a promising avenue for future work.

6. Acknowledgements

This work was partly supported by the PRIN 2017 project
RexLearn, funded by the Italian Ministry of Education, Uni-
versity and Research (grant no. 2017TWNMH2); by BMK,
BMDW, and the Province of Upper Austria in the frame of the
COMET Programme managed by FFG in the COMET Module
S3AI; and by the Key Research and Development Program of
Shaanxi (Program Nos. 2021ZDLGY15-01, 2021ZDLGY09-
the International Sci-
04, 2021GY-004 and 2020GY-050),
ence and Technology Cooperation Research Project of Shen-
zhen (GJHZ20200731095204013), the National Natural Sci-
ence Foundation of China (Grant No. 61772419).

References

[1] A. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenet classiﬁcation with
deep convolutional neural networks, F. Pereira, C. J. C. Burges, L. Bottou,
K. Q. Weinberger (Eds.), NIPS 25, 2012, pp. 1097–1105.

[2] N. Carlini, D. A. Wagner, Towards evaluating the robustness of neural

networks, IEEE Symp. on Sec. and Privacy, 2017, pp. 39–57.

[3] G. F. Elsayed, I. Goodfellow, J. Sohl-Dickstein, Adversarial Reprogram-

ming of Neural Networks, ICLR 2019.

[4] Y.-Y. Tsai, P.-Y. Chen, T.-Y. Ho, Transfer learning without knowing: Re-
programming black-box machine learning models with scarce data and
limited resources, H. D. III, A. Singh (Eds.), ICML, Vol. 119 of PMLR,
2020, pp. 9614–9624.

[5] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, A. Vladu, Towards deep

learning models resistant to adversarial attacks, ICLR, 2018.

[6] S. Boyd, L. Vandenberghe, Convex Optimization, Cambridge University

Press, 2004.

[7] C.-J. Simon-Gabriel, Y. Ollivier, L. Bottou, B. Sch¨olkopf, D. Lopez-Paz,
First-order adversarial vulnerability of neural networks and input dimen-
sion, K. Chaudhuri, R. Salakhutdinov (Eds.), ICLM, Vol. 97 of PMLR,
2019, pp. 5809–5817.

[8] S.-M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, P. Frossard, Universal ad-

versarial perturbations, CVPR, 2017.

10

[9] A. Athalye, L. Engstrom, A. Ilyas, K. Kwok, Synthesizing robust adver-

sarial examples, ICLR, 2018.

[10] K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati, C. Xiao,
A. Prakash, T. Kohno, D. Song, Robust physical-world attacks on deep
learning visual classiﬁcation, CVPR, 2018, pp. 1625–1634.

[11] H. Zhang, J. Guo, G. Chen, C. Li, HCL2000 - A Large-scale Handwritten
Chinese Character Database for Handwritten Character Recognition, Int’l
Conf. on Doc. Analysis and Recognition, 2009, pp. 286–290.

[12] M. Tan, Q. Le, EﬃcientNet: Rethinking model scaling for convolutional
neural networks, K. Chaudhuri, R. Salakhutdinov (Eds.), ICML, Vol. 97
of PMLR, 2019, pp. 6105–6114.

[13] T. Nguyen, M. Raghu, S. Kornblith, Do wide and deep networks learn the
same things? uncovering how neural network representations vary with
width and depth, ICLR, 2021.

[14] R. K. Srivastava, K. Greﬀ, J. Schmidhuber, Training very deep networks,

NIPS, Vol. 2, 2015, p. 2377–2385.

[15] P. Neekhara, S. Hussain, S. Dubnov, F. Koushanfar, Adversarial repro-
gramming of text classiﬁcation neural networks, Conf. on Empirical
Methods in NLP and the 9th Int’l Joint Conf. on NLP (EMNLP-IJCNLP),
2019, pp. 5216–5225.

[16] P. Neekhara, S. Hussain, J. Du, S. Dubnov, F. Koushanfar, J. McAuley,

Cross-modal adversarial reprogramming, 2021, ArXiv.

[17] B. Biggio, B. Nelson, P. Laskov, Poisoning attacks against support vector
machines, J. Langford, J. Pineau (Eds.), ICML, 2012, pp. 1807–1814.
[18] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. ˇSrndi´c, P. Laskov, G. Gi-
acinto, F. Roli, Evasion attacks against machine learning at test time,
H. Blockeel, K. Kersting, S. Nijssen, F. ˇZelezn´y (Eds.), ECML PKDD,
Vol. 8190 of LNCS, 2013, pp. 387–402.

[19] B. Biggio, F. Roli, Wild patterns: Ten years after the rise of adversarial

machine learning, Pattern Recognition 84 (2018) 317–331.

[20] A. D. Joseph, B. Nelson, B. I. P. Rubinstein, J. Tygar, Adversarial Ma-

chine Learning, Cambridge University Press, 2018.

[21] M. Melis, A. Demontis, B. Biggio, G. Brown, G. Fumera, F. Roli, Is deep
learning safe for robot vision? Adversarial examples against the iCub
humanoid, ICCVW (ViPAR), IEEE, 2017, pp. 751–759.

[22] A. Demontis, M. Melis, B. Biggio, D. Maiorca, D. Arp, K. Rieck,
I. Corona, G. Giacinto, F. Roli, Yes, machine learning can be more secure!
a case study on android malware detection, IEEE Trans. on Dependable
and Secure Computing 16 (4) (2019) 711–724.

[23] F. Pierazzi, F. Pendlebury, J. Cortellazzi, L. Cavallaro, Intriguing Prop-
erties of Adversarial ML Attacks in the Problem Space, IEEE Symp. on
Sec. and Privacy (SP), 2020, pp. 1332–1349.

[24] D. Maiorca, A. Demontis, B. Biggio, F. Roli, G. Giacinto, Adversarial
detection of ﬂash malware: Limitations and open issues, Computers &
Sec. Vol. 96.

[25] H. S. Anderson, A. Kharkar, B. Filar, D. Evans, P. Roth, Learning to evade
static PE machine learning malware models via reinforcement learning,
ArXiv.

[26] L. Demetrio, B. Biggio, G. Lagorio, F. Roli, A. Armando, Functionality-
preserving Black-box Optimization of Adversarial Windows Malware,
IEEE Trans. on Information Forensics and Sec.

[27] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow,

R. Fergus, Intriguing properties of neural networks, ICLR, 2014.

[28] S.-M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, P. Frossard, S. Soatto, Ro-
bustness of classiﬁers to universal perturbations: A geometric perspective,
ICLR, 2018.

[29] A. Fawzi, S.-M. Moosavi-Dezfooli, P. Frossard, S. Soatto, Empirical
study of the topology and geometry of deep networks, CVPR, 2018, pp.
3762–3770.

[30] S.-M. Moosavi-Dezfooli, A. Fawzi, J. Uesato, P. Frossard, Robustness via

curvature regularization, and vice versa, CVPR, 2019.

[31] I. J. Goodfellow, J. Shlens, C. Szegedy, Explaining and harnessing adver-

sarial examples, ICLR, 2015.

[32] C. Lyu, K. Huang, H.-N. Liang, A uniﬁed gradient regularization family
for adversarial examples, ICDM, Vol. 00, IEEE Comp. Soc., Los Alami-
tos, CA, USA, 2015, pp. 301–309.

[33] D. Varga, A. Csisz´arik, Z. Zombori, Gradient Regularization Improves

Accuracy of Discriminative Models, ArXiv.

[34] A. S. Ross, F. Doshi-Velez, Improving the adversarial robustness and in-
terpretability of deep neural networks by regularizing their input gradi-
ents, AAAI, 2018.

[35] A. Demontis, M. Melis, M. Pintor, M. Jagielski, B. Biggio, A. Oprea,
C. Nita-Rotaru, F. Roli, Why do adversarial attacks transfer? Explaining
transferability of evasion and poisoning attacks, USENIX Sec., USENIX
Ass., 2019.

Yang Zheng received his M.S. degree from the School
of Electronic Engineering, Xi’an University of Posts &
Telecommunications, China, in 2018. He is currently pur-
suing his Ph.D. in the School of Electronics and Informa-
tion, Northwestern Polytechnical University. His current re-
search interests include secure machine learning and deep

learning.

Xiaoyi Feng is a Professor with the School of Electron-
ics and Information, Northwestern Polytechnical University.
She has authored or coauthored more than 100 papers in

journals and conferences. Her current research interests in-
clude computer vision, image process, radar imagery, and
recognition.

Zhaoqiang Xia is an Associate Professor in the School of
Electronics and Information, Northwestern Polytechnical
University. He has authored or co-authored more than 60
papers in international journals and conferences. His cur-
rent research interests include visual processing, search and
recognition, and statistical machine learning.

Xiaoyue Jiang received the M.S. and Ph.D. degree from
Northwestern Polytechnical University, Xi’an, China,
in
2003 and 2006, respectively. She is an associate professor
with the School of Electronics and Information, Northwest-
ern Polytechnical University since 2012. Her research inter-
ests include image processing, computer vision and machine
learning.

Ambra Demontis is an Assistant Professor at the Uni-
versity of Cagliari, Italy. She received her M.Sc. degree
(Hons.) in Computer Science and her Ph.D. degree in
Electronic Engineering and Computer Science from the

University of Cagliari, Italy. Her research interests in-
clude secure machine learning, kernel methods, biomet-
rics, and computer security.

Maura Pintor is a Ph.D. Student at the University of
Cagliari, Italy. She received the MSc degree in Telecom-
munications Engineering with honors in 2018, from the
University of Cagliari (Italy). Her research interests in-
clude adversarial machine learning and machine learning
explainability methods, with applications in cybersecurity.

Battista Biggio (MSc 2006, PhD 2010) is Assistant Pro-
fessor at the University of Cagliari, Italy, and co-founder
of the company Pluribus One. His research interests in-
clude adversarial machine learning and cybersecurity. He
is Senior Member of the IEEE and of the ACM, and Mem-
ber of the IAPR and ELLIS.

Fabio Roli (MSc 2006, PhD 2010) is a Full Profes-
sor of Computer Science at the University of Cagliari,
Italy. He has been appointed Fellow of the IEEE

and Fellow of the International Association for Pat-
tern Recognition. He is a recipient of the Pierre De-
vijver Award for his contributions to statistical pattern
recognition.

11

