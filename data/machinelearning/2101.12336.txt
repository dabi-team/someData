1
2
0
2

y
a
M
4

]
I
S
.
s
c
[

2
v
6
3
3
2
1
.
1
0
1
2
:
v
i
X
r
a

Community Detection in the Stochastic Block Model
by Mixed Integer Programming

a Departamento de Inform´atica, Pontif´ıcia Universidade Cat´olica do Rio de Janeiro (PUC-Rio), Rua

Breno Serranoa∗, Thibaut Vidala

Marquˆes de S˜ao Vicente, 225 - G´avea, Rio de Janeiro - RJ, 22451-900, Brazil

{vidalt,bserrano}@inf.puc-rio.br

Abstract. The Degree-Corrected Stochastic Block Model (DCSBM) is a popular model to generate

random graphs with community structure given an expected degree sequence. The standard approach

of community detection based on the DCSBM is to search for the model parameters that are the most

likely to have produced the observed network data through maximum likelihood estimation (MLE).

Current techniques for the MLE problem are heuristics, and therefore do not guarantee convergence

to the optimum. We present mathematical programming formulations and exact solution methods

that can provably ﬁnd the model parameters and community assignments of maximum likelihood

given an observed graph. We compare these exact methods with classical heuristic algorithms based

on expectation-maximization (EM). The solutions given by exact methods give us a principled way

of measuring the experimental performance of classical heuristics and comparing diﬀerent variations

thereof.

Keywords. Community detection; Stochastic Block Model; Mixed Integer Programming; Machine

Learning; Unsupervised Learning; Local Search.

∗ Corresponding author

Declarations of interest: none

1. Introduction

In the community detection problem, we observe a graph G = (V, E) and aim to ﬁnd groups of

vertices (or communities) which present a similar connection pattern [22]. Some important applications

of community detection include the study of social networks [24, 32, 36] and predicting the functional

family of proteins [29], among others. One of the most popular approaches for this task consists

1

 
 
 
 
 
 
in ﬁtting a generative model (such as the DCSBM) to the observed graph G, and to search for the

parameters which maximize the likelihood of the model.

In the DCSBM, the number of edges connecting any two vertices i and j only depends on their

group memberships gi and gj and on the set of parameters θi which control the expected degree of each

vertex i. The DCSBM is characterized by a K × K aﬃnity matrix Ω = (ωrs), where K is the number

of communities in the graph. The number of edges between any two nodes i and j is drawn from a

Poisson distribution with mean θiθjωgigj . The probability that the observed network G, represented by

the adjacency matrix A, was generated from the DCSBM can be expressed as:

P (A|g, Ω, θ) =

(cid:89)

i<j

(cid:89)

i

(θiθjωgigj )Aij
Aij!

exp (−θiθjωgigj )×

(cid:0) 1
2 θ2
i ωgigi
(cid:0) 1
2 Aii

(cid:1)Aii/2
(cid:1)!

exp (cid:0)− 1

2 θ2

i ωgigi

(cid:1)

(1)

which deﬁnes the likelihood function of the DCSBM. As in Newman [30], we consider in this work the

case where θiθj = kikj

2m , where kikj

2m corresponds to the expected number of edges in the conﬁguration

model. After applying the log on both sides and grouping together constant terms, the log-likelihood

function becomes:

log P (A|g, Ω) = 1
2

n
(cid:88)

(cid:16)

i,j

Aij log ωgigj − kikj

2m ωgigj

(cid:17)

+ Const.

(2)

The MLE problem consists in ﬁnding the aﬃnity matrix ΩML and group membership assignments gML

that maximize the log-likelihood function (2).

The most common methods for this problem are heuristics, which are not guaranteed to converge

to the solution of maximum likelihood. Theoretical convergence guarantees typically focus on the

probability of recovering the true underlying communities of a graph generated by the DCSBM in the

asymptotic limit where the size of the network grows to inﬁnity. Most studies (see, e.g., [1]) adopt

2

a statistical and information-theoretic viewpoint and provide thresholds and conditions under which

diﬀerent types of algorithms recover the underlying community assignments with high probability for

diﬀerent asymptotic regimes. In this work, instead, we adopt a combinatorial optimization viewpoint,

proposing mixed integer programming (MIP) formulations and exact solution methods that can provably

ﬁnd the optimal solution of the maximum likelihood model for an observed graph.

Firstly, we propose a simple descriptive formulation, which results in a mixed integer non-linear

program (MINLP). This model can be solved to optimality with algorithms based on spatial branch-

and-bound (sBB). Building upon this ﬁrst formulation, we employ linearization techniques to produce

a mixed integer linear programming (MILP) formulation. To improve the formulation, we rely on

a dynamic generation of valid inequalities along with symmetry-breaking constraints. Moreover, we

carefully analyze the problem to derive tight bounds on the model variables, permitting signiﬁcant

reductions of computational eﬀort to ﬁnd optimal solutions.

Our solution approach is motivated by recent advances in the ﬁeld of mathematical programming.

Due to extensive research, optimization solvers have continuously improved, permitting the solution of

increasingly larger problems. Even though MILP is N P-hard in general, it is now possible to solve

instances of dimensions that were absolutely out of reach of solution methods a few decades ago. This

increase in computational eﬃciency is a consequence of methodological improvements which, coupled

with hardware improvements, have resulted in speed-up factors of the order of 1011 over two decades

(based on the ﬁgures reported in [11]).

The study of exact solution algorithms is also an important methodological step for machine learning

research. In particular, exact algorithms provide benchmark solutions that can be used to evaluate the

performance of heuristics, measuring how far they are from the optima of the maximum likelihood

model. To initiate such an analysis, we consider three basic variants of the EM algorithm, and run

computational experiments to compare the heuristic solutions with those found by the exact solution

approaches in terms of likelihood and proximity to the ground truth.

3

Therefore, this work makes a signiﬁcant step towards developing exact solution methods for the

problem of MLE of the DCSBM. Speciﬁcally, our contribution is threefold. First, we propose a MILP

formulation and an optimal solution algorithm for the problem. To the best of our knowledge, this

is the ﬁrst work to investigate exact methods for the DCSBM. Second, we perform computational

experiments on synthetic graphs to assess the performance of the exact methods, showing that the

algorithm based on the MILP formulation clearly outperforms that of the MINLP formulation. Third,

we compare the exact methods with some classical heuristics based on the EM algorithm. These

experiments show that MIP approaches are an important asset to evaluate the performance of heuristics

and highlight the importance of optimal algorithms in machine learning research. All data sets and

source codes needed to reproduce our results are available at: github.com/vidalt/Optimal-SBM.

The remainder of this paper is structured as follows. Section 2 draws an overview of related works.

Section 3 introduces our mathematical programming formulations and solution techniques. Section 4

describes the heuristic approaches and Section 5 presents the computational experiments comparing

the proposed solution methods. Finally, Section 6 concludes.

2. Related Works

A large body of literature on SBMs focuses on community recovery from an information-theoretical

and statistical viewpoint. Special cases of the SBM have regularly been considered, such as the

(balanced) Planted Partition Model (PPM). We refer the reader to the survey by Abbe [1] for a detailed

description of diﬀerent recovery requirements and consistency analyses of various algorithms.

Some studies have explored exact solution methods for community detection based on modularity

maximization [2, 34], which is known to be N P-hard [15]. Newman [30] shows that modularity

maximization is equivalent to MLE of the PPM. The problem of modularity maximization is directly

formulated as a mixed integer quadratic program (MIQP) by Xu et al. [34] and solved using a branch-

and-bound method. The authors discuss the use of symmetry-breaking constraints to improve the

4

eﬃciency of the branch-and-bound exploration. Aloise et al. [2] employ techniques based on column

generation to improve on previous works [34], reporting a reduction in computing time, and solving

larger instances with up to 512 vertices to optimality (vs. 105 vertices in previous works). Modularity

maximization (equivalently MLE of the PPM) is, however, a very specialized case of community

detection, and no exact solution algorithm has been proposed for MLE of the general SBM to date.

Several studies proposed algorithms for community recovery based on SDP relaxations of the

MLE model [16, 17, 18], leading to new results regarding the recovery of communities in SBMs with

general K under an implicit assumption of strong assortativity. Amini and Levina [4] proposed an

SDP relaxation that is tighter than previous ones and works for a broader class of SBMs, including for

disassortative structures.

Del Pia et al. [19] considered the problem of exact community recovery for the assortative planted

bisection model and discussed the theoretical performance of linear programming (LP) relaxations

of the minimum bisection problem for community recovery. They derived suﬃcient and necessary

conditions for recovery using the LP relaxation for diﬀerent asymptotic regimes.

Algorithms based on expectation-maximization for MLE have been investigated, for example, by [33]

for the SBM with two communities. However, their method is practical only for small graphs. For large

graphs, they introduce a Bayesian estimation method based on Gibbs sampling. The EM algorithm is

also used to maximize the pseudo-likelihood of the SBM parameters in Amini et al. [3]. The general

idea of pseudo-likelihood is to approximate the likelihood by ignoring some of the dependency structure

of the data to make the model more tractable.

Metaheuristics have been applied to various related clustering problems. Among others, Gribel and

Vidal [25] proposed a hybrid genetic algorithm for the minimum sum-of-squares clustering problem,

and Hansen et al. [26] proposed a variable neighborhood search heuristic for normalized cut clustering.

More broadly, several related works investigated the application of mixed integer programming to

classical machine learning models. Mixed integer optimization has been used to learn optimal decision

5

trees [9], Gaussian mixture models (GMM) [5] and ramp-loss SVMs [7], among others. We refer to

[8, 23] for surveys on mathematical programming applied to popular machine learning models.

3. Solving the DCSBM to Optimality

This section introduces mathematical programming formulations for the MLE problem given by

Equation (2). We ﬁrst present a descriptive formulation as a MINLP model. Then, we employ diﬀerent

techniques to linearize the model, leading to a MILP formulation. To further improve computational

eﬃciency, we discuss the use of bounds-tightening and symmetry-breaking techniques.

3.1. Formulation as a Mixed Integer Non-Linear Program

Let zir be a binary variable which takes value 1 if vertex i ∈ V is assigned to community r ∈ C and 0

otherwise, where C = {1, . . . , K} represent the possible communities. The continuous variables ωrs, for

r, s ∈ C, represent the elements of the connectivity matrix Ω. The MLE problem (2) can be modeled as

the following MINLP, where we minimize the negative log-likelihood (the constant term was omitted):

minimize
Z,Ω

subject to

1
2

n
(cid:88)

K
(cid:88)

i,j

r,s

fij(ωrs) zirzjs

K
(cid:88)

r=1

zir = 1

zir ∈ {0, 1}

ωrs ∈ R+

∀i ∈ V

∀i ∈ V, r ∈ C

∀r, s ∈ C

where

fij(ωrs) = −Aij log ωrs + kikj

2m ωrs.

(3)

(4)

(5)

(6)

(7)

In this model, Constraints (4) ensure that each vertex is assigned to exactly one community. This

model can be solved to optimality by global optimization solvers such as Couenne [6]. However, solution

time quickly increases with the size of the networks. In the next sections, we propose some techniques

6

to linearize Model (3)–(6) into a MILP along with additional reﬁnements that permit a signiﬁcant

reduction of solution time in comparison to the MINLP.

3.2. Formulation as a Mixed Integer Linear Program

The linearization of the MINLP formulation is done in several steps. First of all, we linearize

the function fij(ωrs) by piecewise outer-approximation. The function fij is convex everywhere in its

domain, for Aij > 0, since: ∂2fij

∂ω2 = Aij

ω2 > 0, ∀ω ∈ R+. Thus, the value of fij is always greater than or

equal to its tangent calculated at any point (cid:101)ω ∈ R+:

fij(ω) ≥ aij ˜ω ω + bij ˜ω

∀ω ∈ R+,

where the coeﬃcients aij ˜ω and bij ˜ω deﬁning the tangent line are given by:

aij ˜ω = −

Aij
(cid:101)ω

+

kikj
2m

bij ˜ω = fij((cid:101)ω) − aij ˜ω (cid:101)ω = Aij(1 − log (cid:101)ω).

(8)

(9)

(10)

By making use of this property, we introduce variables fijrs to represent the value of fij(ωrs), and

reformulate Model (3)–(6) into:

minimize
Z,Ω,F

subject to

1
2

n
(cid:88)

K
(cid:88)

i,j

r,s

fijrs zirzjs

K
(cid:88)

r=1

zir = 1

∀i ∈ V

fijrs ≥ aij ˜ω ωrs + bij ˜ω

∀i, j ∈ V, ∀r, s ∈ C, ∀(cid:101)ω ∈ R+

zir ∈ {0, 1}

∀i ∈ V, ∀r ∈ C

ωrs ∈ R+

fijrs ∈ R

∀r, s ∈ C

∀i, j ∈ V, ∀r, s ∈ C.

7

(11)

(12)

(13)

(14)

(15)

(16)

This model contains an inﬁnite number of constraints of type (13), one for every (cid:101)ω ∈ R+.

Next, we linearize the objective function. Let yijrs denote the product of the binary variables zir

and zjs in the objective function (yijrs = zirzjs). The product of two binary variables can be expressed

as a set of linear constraints:

zir − yijrs ≥ 0,

zjs − yijrs ≥ 0,

1 − zir − zjs + yijrs ≥ 0.

(17)

(18)

(19)

As a result, the objective function can be expressed as fijrs yijrs, which is a product of a continuous

and a binary variable.

To linearize the expression fijrs yijrs , we introduce continuous variables xijrs = fijrs yijrs =

fijrs zirzjs. The non-linear expression fijrs yijrs can be linearized with the big-M technique by intro-

ducing additional constraints:

xijrs ≤ M yijrs,

xijrs ≥ M yijrs,

xijrs ≥ fijrs − M (1 − yijrs).

Constraints (22) can be combined with Constraints (13), yielding:

xijrs ≥ aij ˜ω ωrs + bij ˜ω − M (1 − yijrs)

∀(cid:101)ω ∈ R+.

(20)

(21)

(22)

(23)

It is well known that formulations with big-M constants suﬀer from a weak continuous relaxation (and

therefore from larger solution times) if the lower and upper bounds (M and M ) are large in absolute

value [7, 12]. Section 3.4 proposes some natural values for these bounds. The resulting model is a

8

MILP, and it can be solved by conventional branch-and-cut algorithms.

3.3. Dynamic Constraints Generation

As mentioned previously, the MILP model has an inﬁnite number of constraints of type (23). To

solve it in practice, we initially only consider a small set of these constraints, for a set of break-points

(cid:101)ωp indexed by p ∈ B. Then, new constraints are dynamically introduced in the model during the

solution process. Whenever an integer-feasible solution is found during the branch-and-bound, the

algorithm checks if any constraint given by (23) is violated with a tolerance of (cid:15). In this case, the

solution is declared infeasible and the violated constraints are added to the model. In eﬀect, the method

iteratively reﬁnes the approximation of the function fij until the desired precision of (cid:15) is achieved.

3.4. Bounds Tightening

Given a ﬁxed assignment of vertices to communities, the optimal value of ωrs can be found by

solving a convex minimization problem by diﬀerentiation. We show that ωrs is bounded above by 2mρ:

where

ω∗

rs = 2m

(cid:32) (cid:80)
(cid:80)

i,j Aijzirzjs
i,j kikjzirzjs

(cid:33)

≤ 2mρ,

ρ := max

i,j

(cid:110) Aij
kikj

(cid:111)
.

To see that (24) holds, observe that it is equivalent to:

which is satisﬁed since:

by deﬁnition of ρ in Equation 25.

(cid:16) 1

ρ Aij − kikj

(cid:17)

zirzjs ≤ 0

(cid:88)

i,j

ρ ≥

Aij
kikj

∀i, j ∈ V

9

(24)

(25)

(26)

(27)

Let ωL

rs and ωU

rs denote the lower and upper bounds, respectively, on ωrs. We rely on these bounds

to derive bounds Mijrs ≤ fijrs ≤ Mijrs. Recall that fij(ωrs) = −Aij log ωrs + kikj

2m ωrs.

If Aij = 0, then the expression simpliﬁes to:

fij(ωrs) = kikj

2m ωrs

(28)

and therefore fijrs can be bounded by 0 ≤ fijrs ≤ kikj

2m ωU
rs.

Otherwise, if Aij (cid:54)= 0, a lower bound can be obtained by calculating the global minimum of fij(ωrs)

with respect to ωrs. The minimum can be found by solving

∂fij
∂ωrs

= −

Aij
ωrs

+

kikj
2m

= 0

ˆωrs =

2mAij
kikj

implying

and therefore

Mijrs = −Aij log 2mAij
kikj

+ Aij = Aij

(cid:16)

1 − log Aij + log kikj
2m

(cid:17)

.

(29)

(30)

(31)

Since fij(ωrs) is convex, the upper bound Mijrs can be deﬁned by calculating the function value at the

extreme points of the domain [ωL

rs, ωU

rs]:

Mijrs = max{fij(ωL

rs), fij(ωU

rs)}.

Overall, the upper and lower bounds are given by:

Mijrs =





kikj
2m ωU
rs,

max{fij(ωL

rs), fij(ωU

rs)},

if Aij = 0

if Aij (cid:54)= 0

10

(32)

(33)

Mijrs :=





0,

if Aij = 0

Aij(1 − log Aij + log kikj

2m ),

if Aij (cid:54)= 0 .

(34)

For numerical stability, we also set a lower bound ωL

rs = 10−12 since function fij is not deﬁned at

ωrs = 0.

3.5. Symmetry-breaking Constraints

In the formulations discussed above, any permutation of the group indices in the community

assignment variables Z leads to an equivalent solution. Thus, each solution is, in practice, represented

K! times in the model, leading to an ineﬃcient solution process. To circumvent this issue, Plastria [31]

proposed a set of linear constraints that limits the set of feasible solutions by eliminating solutions

which are equivalent. This is done by enforcing the model to accept only lexicographically minimal

solutions, i.e., by forcing community r to always contain the lowest numbered object (vertex) which

does not belong to any of the previous communities 1, . . . , r − 1. As shown in [31], this can be achieved

by including the following constraints:

z11 = 1

j−1
(cid:88)

r−1
(cid:88)

i=2

l=1

zil −

r
(cid:88)

l=1

zjl ≤ j − 3

∀r ∈ {2, . . . , K − 1}, ∀j ∈ {r, . . . , n}

(35)

(36)

The last cluster K is not associated with any constraint, as it will automatically contain all remaining

objects which do not belong to any of the previous clusters. These constraints eﬀectively reduce the

symmetry of the problem and speed-up the solution method.

4. Heuristics for the DCSBM

The mathematical programming approaches discussed in Section 3 permit to ﬁnd optimal solutions

of the MLE model with a certiﬁcate of global optimality. In practice, however, they are limited to

fairly small problems since their computational eﬀort quickly rises as a function of the number of nodes

11

in the graph. In contrast, heuristic methods usually solve a problem in reduced computational time

but do not provide solution-quality guarantees. We review three natural variants of the EM algorithm

based on the method proposed by Karrer and Newman [28] for community detection using the DCSBM.

These approaches are based on a local search heuristic on the space of community assignments. Since

we can now ﬁnd optimal solutions, we conduct a disciplined experimental analysis of these heuristics to

measure how far they are from the known optima of the MLE model.

4.1. Expectation-Maximization Algorithm

The EM algorithm was introduced by Dempster et al. [20] as a general iterative scheme for ﬁnding

the parameter estimates of maximum likelihood or maximum a posteriori probability (MAP) of

statistical models with (unobserved) latent variables. The EM algorithm has been applied to a variety

of models in machine learning, including Gaussian mixture models (GMM) and hidden Markov models

(HMM) [35], as well as data clustering [14, 27]. The algorithm iteratively performs an expectation step

(E-step) and a maximization step (M-step). In the context of community detection, the E-step searches

for community assignments Z that maximize the likelihood function given a connectivity matrix Ω,

whereas the M-step estimates Ω using the current assignments Z. Each step is guaranteed to increase

the log-likelihood function, and therefore the method converges towards a local optimum. We describe

the M-step in Section 4.2 and discuss three natural algorithmic variations for the E-step in Section 4.3.

4.2. Maximization Step (M-step)

The maximization step consists in estimating the parameters Ω which maximize the likelihood

function given a ﬁxed assignment of vertices to communities Z. The optimal value for ωrs can be

calculated in closed form as:

ω∗

rs = 2m

(cid:32) (cid:80)
(cid:80)

i,j Aijzirzjs
i,j kikjzirzjs

(cid:33)

=

2m · mrs
κrκs

(37)

12

where mrs = (cid:80)n

i,j Aijzirzjs is the number of edges between groups r and s, and κr = (cid:80)n

i kizir is the

sum of the degrees of the vertices in group r.

4.3. Expectation Step (E-step)

The expectation step consists in searching for community assignments Z that maximize the likelihood

given the current aﬃnity matrix Ω. This step corresponds to an N P-hard combinatorial optimization

problem [3]. There are diﬀerent possible ways to perform the E-step. We highlight three main

approaches which, combined with the M-step, result in three variations of the EM algorithm reported

in Algorithms 1, 2, and 3. In all cases, random community assignments are used as initial state.

1 Initialize community assignments Z;

1 Initialize community assignments Z;

2 repeat

3

4

5

6

7

8

9

10

11

12

Ω ← MaximizationStep(Z);

L ← log P (A|Ω, Z);

repeat

for each vertex i ∈ V and group r ∈ C do
Consider Z(cid:48) constructed from Z by

relocating vertex i to group r;

L(cid:48) ← log P (A|Ω, Z(cid:48));
if L(cid:48) > L then

Apply relocation and update

solution:

Z ← Z(cid:48); L ← L(cid:48);

until No improving relocation can be found;

13 until The likelihood function can no longer be

2 Ω ← MaximizationStep(Z);

3 L ← log P (A|Ω, Z);

4 repeat

5

6

7

8

9

10

11

12

13

repeat

for each vertex i ∈ V and group r ∈ C do
Consider Z(cid:48) constructed from Z by

relocating vertex i to group r;
Ω(cid:48) ← MaximizationStep(Z(cid:48));
L(cid:48) ← log P (A|Ω(cid:48), Z(cid:48));
if L(cid:48) > L then

Apply relocation and update

solution:

Z ← Z(cid:48); L ← L(cid:48); Ω ← Ω(cid:48);

until No improving relocation can be found;

improved;

14 until The likelihood function can no longer be

Algorithm 1: EM-LS1 algorithm

improved;

Algorithm 2: EM-LS2 algorithm

1 Initialize community assignments Z;

2 repeat

3

4

Ω ← MaximizationStep(Z);

Z ← E-exact (Ω) (Find Z by solving Model (38)–(40))

5 until The likelihood function can no longer be improved;

Algorithm 3: EM-exact algorithm

13

EM-LS1 Algorithm: Local search on the community assignment variables. The ﬁrst variant, based on

a local search approach, is described in Algorithm 1. Line 3 of the algorithm performs the M-step,

while lines 5–12 correspond to the ﬁrst variant of the E-step. For a ﬁxed value of Ω, the method

searches for community assignments Z by iterating over each vertex i of the graph and relocating

it to a diﬀerent community r whenever it leads to an improvement in the likelihood function. This

procedure is repeated until no more improving relocation exists. It is important to note that Ω stays

ﬁxed during the improvement phase based on relocation (E-step) and is only optimized in the M-step.

EM-LS2 Algorithm: Local search integrated with M-step. This variant is described in Algorithm 2. Here

the value of Ω is re-estimated (with the M-step) each time a vertex relocation is evaluated. Therefore,

the maximization step is “embedded” into the expectation step (line 8). Calculating the value of the

likelihood function can be done in O(K2n2) elementary operations from scratch. Yet, Karrer and

Newman [28] described how to ﬁnd the best relocation move more eﬃciently by instead evaluating the

change in the likelihood, exploiting the property that, when a vertex changes groups, only some terms

of the likelihood function need to be updated. Thus, ﬁnding the community relocation that produces

the maximum increase in the likelihood function can be done in time O(K(K + ¯k)) on average, where

¯k is the average degree of a vertex in G.

EM-exact Algorithm: Exact community assignments. Finally, as shown in Algorithm 3, the complete

E-step can be formulated as an integer quadratic program (IQP) of Equations (38–40) and solved using

an exact solution method.

minimize
Z

subject to

1
2

n
(cid:88)

K
(cid:88)

i,j

r,s

fij(ωrs) zirzjs

q
(cid:88)

r=1

zir = 1

zir ∈ {0, 1}

14

(38)

(39)

(40)

∀i ∈ V

∀i ∈ V, r ∈ C

This variant of EM eﬀectively applies, in turn, an optimal expectation step and an optimal maximization

step. It is, therefore, the approach that is closest to the canonical EM concept. Model (38)–(40) seeks

community assignments Z that maximize the likelihood, for a ﬁxed Ω. In contrast to Model (3)–(6),

the terms fij(ωrs) in the objective function are now constant. Despite this simpliﬁcation, the E-step

remains an N P-hard problem [3]. It can be solved to optimality using standard MIP solvers based on

branch-and-cut, such as Gurobi and CPLEX, for small and medium instances. It is less scalable, but

noteworthy as a benchmark to evaluate the impact of optimal expectation steps in EM heuristics. This

variant of the EM algorithm eﬀectively becomes a matheuristic [13], a term used to refer to methods

that combine metaheuristics with mathematical programming components.

5. Computational Experiments

The goals of our computational experiments are twofold.

1. We compare the performance of the proposed exact methods in terms of computational time and

scalability.

2. Using our knowledge of optimal solutions and bounds, we measure to which extent the heuristics

can ﬁnd the true optimum of the maximum likelihood estimation problem.

The experiments were performed on an Intel Xeon E5-2620 2.1 GHz processor machine with 128 GB

of RAM and CentOS Linux 7 (Core) operating system. The high-level programming language used in

the implementation was Julia [10], and the package JuMP [21] was used as the modeling language for the

exact methods. The underlying optimization solvers adopted for the exact methods were Couenne [6]

as the global optimization solver for the MINLP and CPLEX for the MILP. For reproducibility, we

provide our source code and all experimental data at github.com/vidalt/Optimal-SBM.

15

5.1. Instances

Synthetic graphs allow us to control the factors that might inﬂuence the diﬃculty of community

detection, such as network size and community structure (e.g., degree of separability and assortativity).

Therefore, we generated two groups of data sets, denoted S1 and S2, composed of synthetic graphs

produced by the DCSBM. These graphs contain a number of vertices n ranging from 8 to 16 and a

number of edges m ranging from 4 to 115. This problem scale allows to ﬁnd optimal solutions and, in

the largest cases, still challenges the solution capabilities of the exact methods.

For group S1, we set K = 2 and generated graphs with n ∈ {8, 10, 12, 14, 16}. Since K = 2,

the aﬃnity matrix Ω of the model has three parameters: two diagonal elements ω11, ω22 and one

oﬀ-diagonal ω12 = ω21. For each (ωin, ωout) ∈ {0.1, 0.4, 0.6, 0.9}2, such that ωin (cid:54)= ωout, we sampled

ω11, ω22 from U(ωin − 0.1, ωin + 0.1) and we sampled ω12 from U(ωout − 0.1, ωout + 0.1), where U(a, b)

is the uniform distribution in the interval [a, b]. This gives 4 × 3 = 12 combinations of values for

(ωin, ωout). Six combinations are assortative (with ωin > ωout) and six combinations are disassortative

(with ωin < ωout). For statistical signiﬁcance, we generated 10 instances for each combination, yielding

a total of 5 × 4 × 3 × 10 = 600 instances.

Data group S2 is composed of strongly assortative graphs with K ∈ {2, 3}, n ∈ {8, 10, 12, 14, 16} and

three levels of community strength: low, medium and high. For each level of community strength, we

sampled the diagonal and oﬀ-diagonal elements of Ω from a uniform distribution in the corresponding

interval given by Table 1. For each conﬁguration, we generated a total of 10 instances, leading to

2 × 3 × 5 × 10 = 300 instances.

LOW

MEDIUM HIGH

ωrr

[0.4, 1.0]

[0.6, 1.0]

[0.8, 1.0]

ωrs (r (cid:54)= s)

[0.2, 0.4]

[0.1, 0.3]

[0.0, 0.2]

Table 1: Range of possible values for the diagonal and oﬀ-diagonal elements of the aﬃnity matrix Ω

16

5.2. Performance of the exact methods

For each instance in S1 and S2, we run the two exact methods (MINLP and MILP) with a time

limit of 600 seconds. To assess the impact of the symmetry-breaking constraints (SBC) in the solution

time, we run each method twice: with (SBC) and without them (NoSBC).

General Comparison. Tables 2 and 3 present the following results for the exact methods: number

of instances solved to optimality (“Opt”), percentage gap (“Gap”), solution time in seconds (“Time”),

and number of nodes explored in the branch-and-bound tree (“Nodes”). The values reported in

both tables are averaged over the 10 instances of each type (except for “Opt”). The exact methods’

percentage gap is calculated based on the log-likelihood function (including the constant terms) as

Gap = (UB − LB)/UB, where LB and UB are the lower and upper objective bounds. A summary line

is included in the bottom of each table showing the total number of instances solved to optimality (for

column “Opt”) and average results for all other columns.

As seen in these results, the MILP is clearly faster than the MINLP for both groups of data

sets. When K = 2, the MILP can optimally solve all instances with up to n = 14 vertices, whereas

the MINLP is already unable to ﬁnd the optimum for some instances with 14 vertices. This visible

reduction in solution time is a consequence of a more eﬃcient branch-and-bound exploration. Indeed,

for the instances that are solved to optimality, the MILP visits fewer search nodes than the MINLP.

For larger instances (e.g., when K = 3 in Table 3), both methods have diﬃculties to consistently ﬁnd

optimal solutions within the time limit. However, for instances that could not be solved to optimality,

percentage gaps are generally much smaller for the MILP. These results clearly illustrate that the

DCSBM is indeed challenging to solve to optimality.

Impact of the symmetry-breaking constraints. Figure 1 illustrates the impact of using SBC

with the MINLP (on the left) and with the MILP (on the right). It shows the speed ratio between

17

MINLP

MILP

NoSBC

SBC

NoSBC

SBC

12 0.1

10 0.1

0.4

0.4

0.6

0.4

0.9

0.6

0.9

0.1

6.3
10.1
11.0
6.5
12.9
14.9
8.2
14.4
19.5
10.2
16.1
16.5
27.4
28.6
39.4
22.7
49.5

n ωin ωout Opt Gap Time Nodes Opt Gap Time Nodes Opt Gap Time Nodes Opt Gap Time Nodes
10 0.00
59.6
8
0.6
10 0.00
86.9
0.8
10 0.00
60.3
0.8
10 0.00
69.4
0.7
10 0.00
146.4
1.2
10 0.00
138.4
1.3
10 0.00
104.5
0.8
10 0.00
148.2
1.2
10 0.00
171.6
1.7
10 0.00
73.2
0.8
10 0.00
179.0
1.5
10 0.00
210.5
1.6
10 0.00
311.7
2.6
10 0.00
223.7
2.3
10 0.00
192.2
2.6
10 0.00
301.7
2.1
10 0.00
672.1
5.4
10 0.00
584.7
5.9
10 0.00
370.5
2.8
10 0.00
568.6
4.9
10 0.00
783.8
7.9
10 0.00
300.1
3.0
10 0.00
562.8
5.5
10 0.00
707.6
7.1
10 0.00
946.8
7.0
10 0.00
769.8
7.7
10 0.00
1042.3
10.1
10 0.00
1222.5
7.0
10 0.00
2786.3
23.0
10 0.00
2205.0
21.6
10 0.00
1166.3
8.1
10 0.00
2735.3
19.4
10 0.00
3003.1
27.8
10 0.00
1266.2
12.1
10 0.00
2170.4
18.9
10 0.00
2711.4
26.9
10 0.00
3263.6
22.1
10 0.00
3236.5
23.8
10 0.00
2746.2
29.2
10 0.00
28.1
4597.2
10 0.00
98.0 10378.5
10 0.00 111.4
9342.6
29.0
10 0.00
3664.9
10 0.00
75.2
7830.8
10 0.00 130.3
9842.1
3399.7
33.2
10 0.00
10 0.00 115.6 11125.2
10 0.00 122.0
9803.4
10 0.00 122.7 17711.9
9795.2
88.5
10 0.00
10 0.00
6962.4
93.0
10 0.00 140.8 17516.1
9 0.32 471.0 33429.2
8 0.87 416.9 25972.9
9 0.28 170.1 18015.5
9 0.28 417.5 35457.5
5 2.56 561.4 33329.6
9363.8
8 0.78 484.6 32075.5
4 3.29 585.3 32882.9
6346.6
78.9

167.6
3.7
0.00
10
274.7
6.6
0.00
10
216.7
7.0
0.00
10
203.8
4.2
0.00
10
349.5
7.8
0.00
10
317.6
9.7
0.00
10
266.7
6.0
0.00
10
306.8
9.2
0.00
10
326.3
11.9
0.00
10
238.5
6.9
0.00
10
333.5
10.4
0.00
10
381.1
10.8
0.00
10
915.9
15.4
0.00
10
839.2
17.7
0.00
10
999.1
22.6
0.00
10
988.8
15.9
0.00
10
1528.7
28.8
0.00
10
1433.4
31.9
0.00
10
1140.8
21.8
0.00
10
1243.0
27.2
0.00
10
1563.7
35.8
0.00
10
942.5
23.7
0.00
10
1427.1
32.3
0.00
10
1528.7
33.5
0.00
10
3280.1
48.8
0.00
10
3242.9
59.9
0.00
10
3716.4
70.3
0.00
10
3613.0
0.00
51.0
10
6625.3
0.00 102.1
10
5439.4
99.9
0.00
10
4137.3
65.4
0.00
10
6478.6
0.00
96.6
10
6667.8
0.00 109.4
10
4347.5
78.9
0.00
10
5881.0
0.00
96.7
10
6751.7
0.00 111.4
10
0.00 188.2 14498.9
10
0.00 230.4 16568.3
10
0.00 215.2 12588.9
10
0.00 233.9 17723.4
10
0.00 350.0 24600.9
10
0.00 395.0 24594.2
10
0.00 235.4 16307.2
10
0.00 353.0 25993.2
10
0.00 442.1 28049.6
10
0.00 254.7 15866.8
10
0.00 398.0 26503.8
10
0.00 426.7 25344.6
10
9.23 592.4 38779.1
1
8.27 573.8 32368.4
2
2 11.29 575.4 27781.8
1
7.31 557.7 36824.9
0 18.02 600.8 29144.8
0 21.77 600.6 24664.4
7.95 562.1 33916.7
4
0 16.84 600.9 30136.4
0 24.05 600.8 24056.4
3
8.96 558.4 28266.1
0 21.98 600.7 24294.1
0 21.64 600.6 25592.2
2.96 202.3 11376.3

306.3
0.00
10
587.4
0.00
10
569.3
0.00
10
388.5
0.00
10
730.0
0.00
10
661.3
0.00
10
459.6
0.00
10
704.2
0.00
10
796.1
0.00
10
463.8
0.00
10
786.1
0.00
10
797.9
0.00
10
2056.3
0.00
10
1676.5
0.00
10
2143.0
0.00
10
1585.0
0.00
10
3625.7
0.00
10
0.00 106.9 17598.8
9
35.8
0.00
2427.8
10
99.8 18520.7
0.00
9
3393.3
57.9
0.00
10
2126.2
41.4
0.00
10
3238.4
57.2
0.00
10
3178.2
59.1
0.00
10
6522.3
0.00
86.0
10
7322.4
0.00 105.6
10
8374.4
0.00 130.5
10
0.00
8169.0
98.2
10
0.00 194.6 14369.8
10
0.00 176.7 11083.3
10
9293.9
0.00 120.5
10
0.00 169.5 13084.0
10
0.00 206.1 14039.0
10
0.00 148.8
9853.2
10
0.00 177.3 11626.7
10
0.00 258.6 24597.3
9
0.00 362.9 30388.0
9
0.00 445.0 33400.6
10
0.40 420.9 27246.6
9
0.49 478.5 37337.6
8
4.85 574.6 37600.6
1
6.86 594.4 35976.6
1
0.63 425.6 30829.4
8
5.95 598.5 39127.7
2
9.92 596.3 33486.5
1
0.51 530.5 35931.5
8
1
7.86 598.8 36665.6
1 11.35 597.1 32256.1
0 15.88 601.0 33644.3
0 19.67 600.9 31410.4
0 23.84 600.8 26570.6
1 13.86 575.5 34141.5
0 29.13 600.8 26345.5
0 31.66 600.6 22139.2
0 15.10 600.8 31180.2
0 23.83 600.8 25580.5
0 31.91 600.6 19998.5
0 19.38 600.7 27602.0
0 27.95 600.6 21412.2
0 30.05 600.5 21792.6
5.52 267.5 15653.7

92.5
0.8
10 0.00
166.6
1.2
10 0.00
120.0
1.2
10 0.00
140.7
0.9
10 0.00
298.5
2.0
10 0.00
271.0
2.1
10 0.00
157.4
1.1
10 0.00
302.5
2.0
10 0.00
340.7
2.8
10 0.00
126.6
1.0
10 0.00
359.6
2.4
10 0.00
359.8
2.6
10 0.00
608.7
3.7
10 0.00
440.6
3.6
10 0.00
397.1
4.2
10 0.00
514.9
3.1
10 0.00
1347.8
8.5
10 0.00
1198.6
9.0
10 0.00
720.2
4.1
10 0.00
976.3
7.1
10 0.00
1561.5
12.8
10 0.00
545.2
4.6
10 0.00
1164.3
8.4
10 0.00
1610.8
12.1
10 0.00
1955.8
11.3
10 0.00
1523.8
11.1
10 0.00
1551.8
13.6
10 0.00
2138.1
10.4
10 0.00
4945.8
37.8
10 0.00
4215.0
36.0
10 0.00
2603.0
14.5
10 0.00
5318.6
35.0
10 0.00
5938.7
49.4
10 0.00
2808.6
21.0
10 0.00
4288.4
33.5
10 0.00
5334.6
46.4
10 0.00
5687.8
35.1
10 0.00
7168.1
46.8
10 0.00
45.3
4864.8
10 0.00
10 0.00
58.2 10053.6
10 0.00 180.8 19339.3
10 0.00 217.5 18671.0
10 0.00
7601.7
52.7
10 0.00 152.0 16275.0
10 0.00 252.1 21144.3
6718.1
62.8
10 0.00
10 0.00 200.2 18398.8
10 0.00 236.0 17489.5
10 0.00 262.8 35403.1
10 0.00 141.1 16431.6
10 0.00 165.7 12670.4
9 0.58 247.0 31551.6
1 6.30 579.9 39058.6
3 4.97 528.6 31429.7
8 0.81 282.0 30419.5
1 5.16 599.7 47506.8
0 7.70 600.0 32472.5
9 0.47 177.3 16306.2
1 6.59 592.3 35953.7
0 9.43 600.0 30829.3
9498.2

0.4
0.6
0.9
0.1
0.6
0.9
0.1
0.4
0.9
0.1
0.4
0.6
0.4
0.6
0.9
0.1
0.6
0.9
0.1
0.4
0.9
0.1
0.4
0.6
0.4
0.6
0.9
0.1
0.6
0.9
0.1
0.4
0.9
0.1
0.4
0.6
0.4
0.6
0.9
0.1
0.6
0.9
0.1
0.4
0.9
0.1
0.4
0.6
0.4
0.6
0.9
0.1
0.6
0.9
0.1
0.4
0.9
0.1
0.4
0.6

542 0.70 112.3

10 0.00 109.6

Aggregate

582 0.14

417

493

0.9

0.4

0.6

0.9

0.6

0.6

0.9

0.4

16 0.1

14 0.1

Table 2: General performance of the exact methods (MINLP vs MILP) for data group S1, with SBC and without them
(NoSBC)

18

MINLP

MILP

8

SBC

SBC

NoSBC

NoSBC

12.9
12.8
10.1
42.5
41.5
39.4

10
10
10
10
10
10
10
10
10
10
10
10

medium
high
16 low

medium
high
14 low

medium
high
12 low

3.4
3.3
2.2
5.9
6.0
4.3
49.9
28.8
15.2

8.3
8.6
6.4
26.1
25.1
24.7
91.9
81.5
73.1

low
medium
high
10 low

1.9
1.9
1.4
3.9
4.1
2.7
20.1
11.9
6.9
82.8
88.1
38.3

Comm.
Gap Time Nodes Opt Gap Time Nodes Opt Gap Time Nodes Opt Gap Time Nodes
K n strength Opt
122.7
10
0.00
551.1
0.00
10
2
131.8
10
0.00
726.3
0.00
10
72.1
10
0.00
432.4
0.00
10
428.7
10
0.00
2543.1
0.00
10
454.4
10
0.00
2717.2
0.00
10
270.7
10
2260.7
0.00
0.00
10
2414.6
10
0.00 163.0 11381.0
0.00
10
1395.9
10
0.00 149.8 10292.3
0.00
10
817.4
10
0.00 144.9
7602.1
0.00
10
6691.3
10
3.92 575.2 38428.5
0.00
4
7492.4
10
4.66 511.1 32873.6
0.00
4
0.00
10
0.00 457.3 30102.3
3014.3
10
1.63 474.1 25546.0
2
24.59 600.8 26048.4
0
2.24 437.8 15999.2
3
23.94 600.8 25228.8
0
9534.4
0.00 252.3
7
21.02 600.7 27075.7
0
520.3
9.3
0.00
10
9.86 486.0
8260.7
6
410.2
8.0
0.00
10
2.89 436.3 10404.9
8
372.6
6.0
0.00
10
10.93 404.7
6771.4
6
4417.9
37.1
0.00
10
33.24 600.9 23751.9
0
3193.6
23.1
0.00
10
18.32 532.7 25077.7
3
0.00
15.26 490.3 28013.6
1653.1
12.7
10
3
0.00 348.6 27153.8
60.62 601.0 16741.0
0
0.36 299.5 20088.8
58.57 601.0 15707.1
0
59.16 601.1 17991.1
9244.4
0.00 115.5
0
0 13.02 600.0 16901.7
7937.1
74.51 600.5
0
0 12.62 600.0 15150.3
7016.3
64.45 600.5
0
2.36 492.6 23093.9
7
9456.2
0
57.07 600.7
6488.1
0 24.13 600.0
1770.4
0 100.00 599.8
6196.3
0 22.48 600.0
0 100.00 600.0
2407.4
7161.1
0 19.77 600.0
2587.7
99.26 600.0
0
7214.4
3.29 192.7
28.08 410.6 13405.3
134

259.9
0.00
309.3
0.00
204.2
0.00
1147.1
0.00
1205.4
0.00
985.2
0.00
5779.5
0.00
4846.0
0.00
0.00
3672.0
0.00 346.0 24528.2
0.00 301.8 21439.0
0.00 246.4 15366.9
0 18.47 600.9 30157.5
0 16.80 600.8 28751.8
9.41 598.9 33362.2
1
1887.0
0.00
10
1668.7
0.00
10
0.00
10
1788.2
0.00 361.3 20935.7
9
0.00 284.0 18346.7
9
0.00 207.3 12059.5
10
0 39.07 600.7 18220.6
0 33.35 600.6 17730.2
0 31.70 600.6 20065.9
8061.0
0 69.01 600.2
7958.9
0 68.67 600.2
9602.9
0 64.72 600.3
3017.5
0 92.69 599.7
2509.6
0 92.67 599.6
4183.8
0 93.27 599.9
179 20.99 315.5 10668.3

250.1
0.00
268.8
0.00
142.7
0.00
835.6
0.00
878.3
0.00
529.6
0.00
5554.3
0.00
3401.1
0.00
0.00
1924.3
0.00 153.3 12810.8
0.00 162.3 12307.9
0.00
6258.7
6.37 579.9 27377.8
5.57 544.9 17520.7
2.24 376.9 12027.5
2505.8
0.00
2866.0
0.00
0.00
1827.0
0.00 225.5 22935.7
0.00 106.6 11739.5
6818.6
0.00
67.3
0 10.17 600.0 32058.7
8.06 538.6 27221.1
2
6
2.15 474.5 28412.3
0 19.77 600.0 13066.7
0 18.89 600.0 12710.3
0 11.25 600.0 17031.9
4430.8
0 29.09 600.0
4882.4
0 27.60 600.0
7191.3
0 25.58 600.0
9926.2
5.6 257.6

medium
high
low
medium
high
10 low

10
10
10
10
10
10
10
10
10
10
10
10
6
5
10
10
10
10
10
10
10
10
9
10

medium
high
Aggregate

medium
high
14 low

medium
high
12 low

medium
high
16 low

65.8
51.0
52.3

31.1
39.3
22.8

84.6

200

237

8

3

Table 3: General performance of the exact methods (MILP vs MINLP) for data group S2, with SBC and without them
(NoSBC)

19

the solution time of the method without and with SBC, depending on the number of vertices in

data group S1. The results are summarized as boxplots, with whiskers that extend to 1.5 times the

interquartile range. Points outside this range are marked as outliers and noted with a “◦”.

Figure 1: Speed ratios between between the solution time of the methods without and with SBC (MINLP on the left and
MILP on the right)

As highlighted on Figure 1, the use of SBC has a beneﬁcial impact on the MINLP and MILP

solution methods. Even in the simple case with K = 2, adding SBC clearly improves the solution time

of both exact methods for the great majority of instances. The improvement becomes more marked as

n increases, with solution times up to 2x faster.

Comparison of the formulations. Finally, Figure 2 compares the solution times of the exact

methods on data sets of group S1. As n increases, the variance in the distribution of the speed ratio

increases. The speed ratios are nonetheless always greater than 1, meaning that the MILP approach is

faster than the MINLP, regardless of the use of the SBC. In some cases, the MILP is as high as 32

times faster than the MINLP.

5.3. Performance of the heuristic methods

To evaluate the performance of the heuristic solution approaches, we compare their solutions to the

solutions found by the exact methods. For each instance in S1 and S2, we run the three EM variants

20

810121416Numberofvertices(n)12124SpeedratioMINLP810121416Numberofvertices(n)12124MILPFigure 2: Speed ratios between the solution time of the MINLP and MILP approaches (without SBC on the left, and
with SBC on the right)

for 50 trials (with diﬀerent random starts). For each instance the relative percentage gap is calculated

as:

Gap(%) =

OBJ − BKS
BKS

(41)

where OBJ is the objective value of the heuristic solution, and BKS (best-known solution) is the

objective value of the optimal or best integer-feasible solution found by the MILP with SBC.

Tables 4 and 5 present average gap values and solution times out of the 50 trials for data groups

S1 and S2, respectively. As visible from these results, the solution times of EM-LS1 and EM-LS2 are

generally close, with EM-LS1 being slightly faster on average. The solution time of EM-exact is orders

of magnitude higher than that of the other two heuristics since it involves the exact solution of a MIP

during each expectation step.

In terms of solution quality, EM-LS2 achieves a lower gap on average, compared to EM-LS1. The

comparison with EM-exact leads to more contrasted observations: for data group S1, EM-exact achieved

the lowest average gap, whereas for S2 it was outperformed by EM-LS2. For some settings, the average

gap obtained by the heuristics is small or even negative (e.g., for S2, with K = 3, n = 16 and low

community strength), meaning in the latter case that the heuristic objective value was better than the

one found by the exact method (only possible when the exact method was unable to ﬁnd the optimal

21

810121416Numberofvertices(n)12481632SpeedratioMINLP/MILPNoSBC810121416Numberofvertices(n)12481632SBCn
8

ωin
0.1

0.4

0.6

0.9

10

0.1

0.4

0.6

0.9

12

0.1

0.4

0.6

0.9

14

0.1

0.4

0.6

0.9

16

0.1

0.4

0.6

0.9

E-LS1

E-LS2

E-exact

ωout Gap (%)
3.13
0.4
4.29
0.6
7.18
0.9
3.94
0.1
1.97
0.6
3.35
0.9
3.56
0.1
2.10
0.4
2.87
0.9
6.13
0.1
1.96
0.4
2.00
0.6
2.26
0.4
3.57
0.6
6.06
0.9
2.62
0.1
1.37
0.6
2.22
0.9
3.28
0.1
1.93
0.4
1.78
0.9
4.17
0.1
2.32
0.4
2.24
0.6
2.39
0.4
4.82
0.6
5.54
0.9
2.30
0.1
1.47
0.6
1.95
0.9
2.40
0.1
1.58
0.4
1.75
0.9
3.07
0.1
1.90
0.4
1.35
0.6
2.59
0.4
3.50
0.6
5.61
0.9
2.06
0.1
1.21
0.6
1.64
0.9
3.76
0.1
1.23
0.4
1.39
0.9
4.23
0.1
1.48
0.4
1.61
0.6
1.77
0.4
3.50
0.6
5.18
0.9
1.70
0.1
1.15
0.6
1.88
0.9
2.52
0.1
1.03
0.4
1.16
0.9
3.78
0.1
1.20
0.4
0.94
0.6
2.71

Time (s) Gap (%)
1.65
2.08
3.44
2.51
1.27
1.75
2.26
1.39
1.90
3.68
1.42
1.39
1.41
1.76
4.13
2.15
1.26
1.36
2.41
1.50
1.43
3.68
1.67
1.83
1.79
2.99
3.81
2.16
1.14
1.95
2.08
1.30
1.57
2.68
1.62
1.19
2.33
2.76
4.38
1.81
1.15
1.43
3.86
1.06
1.23
3.73
1.28
1.56
1.54
3.09
4.16
1.59
1.07
1.83
2.59
1.03
1.14
4.03
1.15
0.99
2.07

1.6 × 10−4
1.3 × 10−4
1.4 × 10−4
1.2 × 10−4
1.5 × 10−4
1.6 × 10−4
1.2 × 10−4
1.4 × 10−4
1.4 × 10−4
1.3 × 10−4
1.4 × 10−4
1.4 × 10−4
4.6 × 10−4
2.5 × 10−4
2.4 × 10−4
2.3 × 10−4
2.7 × 10−4
3.0 × 10−4
2.6 × 10−4
2.9 × 10−4
2.8 × 10−4
2.6 × 10−4
3.3 × 10−4
2.9 × 10−4
5.4 × 10−4
4.5 × 10−4
4.6 × 10−4
4.0 × 10−4
5.2 × 10−4
5.0 × 10−4
4.5 × 10−4
4.2 × 10−4
6.0 × 10−4
5.2 × 10−4
5.6 × 10−4
5.5 × 10−4
7.2 × 10−4
7.9 × 10−4
8.4 × 10−4
7.1 × 10−4
7.4 × 10−4
7.6 × 10−4
6.5 × 10−4
7.8 × 10−4
8.6 × 10−4
7.9 × 10−4
8.2 × 10−4
8.4 × 10−4
1.3 × 10−3
1.1 × 10−3
1.1 × 10−3
9.9 × 10−4
1.1 × 10−3
1.4 × 10−3
1.1 × 10−3
1.2 × 10−3
1.2 × 10−3
1.1 × 10−3
1.2 × 10−3
1.2 × 10−3
5.7 × 10−4

Time (s) Gap (%)
1.76
3.09
4.33
3.29
1.65
2.89
2.70
1.83
2.12
4.46
1.56
1.67
1.45
2.44
3.98
2.11
0.81
1.61
2.29
1.32
1.23
3.72
1.69
1.57
1.30
3.06
3.63
1.69
0.80
1.83
1.94
1.22
1.25
2.45
1.28
0.88
1.49
1.95
3.85
1.25
0.69
0.98
2.73
0.75
0.94
3.12
0.98
1.24
0.98
2.42
4.09
1.15
0.45
1.28
2.04
0.52
0.71
3.80
0.85
0.59
1.92

2.5 × 10−4
2.0 × 10−4
2.4 × 10−4
1.9 × 10−4
2.1 × 10−4
2.5 × 10−4
1.9 × 10−4
2.1 × 10−4
2.4 × 10−4
2.2 × 10−4
2.2 × 10−4
2.2 × 10−4
3.7 × 10−4
3.9 × 10−4
3.9 × 10−4
3.4 × 10−4
3.8 × 10−4
4.2 × 10−4
3.8 × 10−4
4.0 × 10−4
4.0 × 10−4
3.9 × 10−4
4.5 × 10−4
3.9 × 10−4
6.9 × 10−4
7.7 × 10−4
7.3 × 10−4
6.6 × 10−4
7.2 × 10−4
7.0 × 10−4
7.4 × 10−4
6.0 × 10−4
7.7 × 10−4
7.3 × 10−4
8.2 × 10−4
7.3 × 10−4
1.1 × 10−3
1.2 × 10−3
1.1 × 10−3
10.0 × 10−4
9.7 × 10−4
1.1 × 10−3
1.1 × 10−3
1.1 × 10−3
1.2 × 10−3
1.2 × 10−3
1.1 × 10−3
1.2 × 10−3
1.7 × 10−3
1.7 × 10−3
1.8 × 10−3
1.5 × 10−3
1.7 × 10−3
1.9 × 10−3
1.7 × 10−3
1.6 × 10−3
1.6 × 10−3
1.7 × 10−3
1.7 × 10−3
1.7 × 10−3
8.3 × 10−4

Time (s)
3.9 × 10−1
4.8 × 10−1
4.3 × 10−1
4.0 × 10−1
5.4 × 10−1
5.4 × 10−1
4.9 × 10−1
6.1 × 10−1
6.3 × 10−1
4.6 × 10−1
6.0 × 10−1
6.2 × 10−1
7.3 × 10−1
7.1 × 10−1
6.7 × 10−1
6.7 × 10−1
8.5 × 10−1
8.9 × 10−1
7.1 × 10−1
9.1 × 10−1
9.2 × 10−1
8.1 × 10−1
8.6 × 10−1
9.5 × 10−1
9.5 × 10−1
9.6 × 10−1
1.0
1.1
1.4
1.2
1.1
1.2
1.3
1.1
1.2
1.3
1.4
1.4
1.6
1.6
1.7
2.0
5.0
6.2
4.1
1.6
1.8
1.9
2.1
2.1
2.1
7.6
8.4
8.2
5.7
2.7
2.8
2.2
2.6
3.0
1.8

Average

Table 4: (Average) Relative percentage gap and solution times of the heuristic methods for S1

22

K n
8
2

3

8

Community
strength
low
medium
high
low
medium
high
low
medium
high
low
medium
high
low
medium
high
low
medium
high
low
medium
high
low
medium
high
low
medium
high
low
medium
high
Average

10

12

14

16

10

12

14

16

E-LS1

E-LS2

E-exact

Gap (%)
2.54
3.14
5.81
2.45
2.40
4.85
1.47
3.09
4.12
2.18
2.21
5.16
1.61
2.27
4.51
5.35
5.90
5.95
3.81
4.90
4.87
3.25
3.98
4.55
2.59
2.67
4.17
0.84
1.36
3.32
3.51

Time (s) Gap (%)
1.69
2.49
4.33
1.62
1.94
3.63
1.29
2.94
3.56
1.85
1.88
4.33
1.58
2.03
3.51
1.15
1.64
1.90
1.94
2.01
2.35
2.10
2.21
2.43
1.69
1.46
1.96
-0.05
0.49
1.85
2.12

1.3 × 10−4
1.5 × 10−4
1.5 × 10−4
2.6 × 10−4
2.5 × 10−4
5.2 × 10−4
4.5 × 10−4
4.5 × 10−4
4.4 × 10−4
7.9 × 10−4
8.1 × 10−4
1.1 × 10−3
1.1 × 10−3
1.2 × 10−3
1.1 × 10−3
4.8 × 10−4
4.4 × 10−4
4.4 × 10−4
8.0 × 10−4
7.6 × 10−4
7.1 × 10−4
1.7 × 10−3
1.8 × 10−3
1.8 × 10−3
3.2 × 10−3
2.9 × 10−3
2.9 × 10−3
5.2 × 10−3
5.5 × 10−3
5.1 × 10−3
1.4 × 10−3

Time (s) Gap (%)
2.00
2.32
5.82
1.81
1.85
3.93
0.91
2.33
3.80
2.65
2.00
4.64
1.11
1.60
3.45
4.55
5.52
5.75
3.50
4.28
3.90
2.68
9.01
4.15
2.03
2.38
3.40
0.43
1.19
2.34
3.17

1.8 × 10−4
1.9 × 10−4
1.8 × 10−4
3.7 × 10−4
3.5 × 10−4
4.1 × 10−4
6.1 × 10−4
6.4 × 10−4
6.6 × 10−4
1.1 × 10−3
9.8 × 10−4
1.2 × 10−3
1.5 × 10−3
1.7 × 10−3
1.7 × 10−3
9.7 × 10−4
9.2 × 10−4
8.2 × 10−4
1.7 × 10−3
1.8 × 10−3
1.6 × 10−3
2.9 × 10−3
3.5 × 10−3
3.3 × 10−3
4.3 × 10−3
4.8 × 10−3
4.8 × 10−3
7.2 × 10−3
7.6 × 10−3
7.8 × 10−3
2.2 × 10−3

Time (s)
1.6
1.3
1.5
5.0
4.8
4.8
3.7
3.4
3.3
2.2 × 101
1.3 × 101
2.2 × 101
2.7
2.4
2.7
1.3
1.2
1.1
4.8
4.2
4.2
7.6
1.8 × 101
6.1
6.2
6.0
4.5
1.2 × 101
1.1 × 101
9.7
6.4

Table 5: (Average) Relative percentage gap and solution times of the heuristic methods for S2

23

solution within the time limit).

On several runs, we observed that the heuristics eﬀectively found the optimal solutions (or high-

quality solutions). This insight directly derives from our ability to ﬁnd optimal solutions with the

proposed exact algorithms, as the heuristics by themselves cannot give such a performance certiﬁcate.

It also remains an open question whether this behavior holds for larger instance, but such an analysis

would require signiﬁcant methodological advances to solve larger cases to proven optimality.

5.4. Comparison to the ground truth

We ﬁnally compare the model parameters found by the exact methods with the ground truth

parameters used in the generation of each instance. In this analysis, we calculate the agreement A( ˆZ, Z∗)

between the community assignments ˆZ of the optimal solution of maximum likelihood and the ground

truth communities Z∗ of the model. The agreement function A(·, ·) measures the maximum number of

common elements between two vectors of community assignments, considering all possible permutations

of the community labels. When the optimum is not known, the estimated communities of the BKS are

considered instead.

Figure 3 shows that the agreement between the recovered communities and the ground truth

communities of data set S1 is higher when n is larger and when the absolute diﬀerence |ωin − ωout| is

larger. This is expected since there is more information in the graph. Similarly for S2, we observe that

instances with higher community strength have a higher community agreement (Figure 4).

The solution of maximum likelihood may be far from the ground truth, in general. This is especially

true for small networks, such as those considered in this work, since there is often not enough information

to correctly recover the underlying communities.

We also compare the heuristics in their ability to recover the ground truth communities in Tables 6

and 7, respectively for S1 and S2. These tables report the average agreement (out of 50 trials) between

the recovered communities and the ground truth communities used in the generation of each instance.

24

Figure 3: (Average) agreement between the community assignments of maximum likelihood and the ground truth, as a
function of n and (ωin, ωout), for data sets in group S1

Figure 4: (Average) agreement between the community assignments of maximum likelihood and the ground truth, as a
function of n, K and the level of community strength, for data sets in group S2

We compare the performance of the heuristics with the MILP with SBC.

For some instances, the resulting community agreement is low for both exact and heuristic methods,

since there is not enough information present in the graph and it may be theoretically impossible to

recover the ground truth. Still, in the other cases, the exact approach clearly outperforms the heuristics

in almost all instances, highlighting the importance of good solutions for this task.

6. Conclusions

This study allowed us to ﬁll a signiﬁcant methodological gap: the lack of exact solution methods

for community detection in the general SBM. Exact algorithms are indeed essential for a disciplined

analysis of machine learning models and training algorithms, as they permit a precise evaluation of

25

(0.1,0.4)(0.1,0.6)(0.1,0.9)(0.4,0.1)(0.4,0.6)(0.4,0.9)(0.6,0.1)(0.6,0.4)(0.6,0.9)(0.9,0.1)(0.9,0.4)(0.9,0.6)(ωin,ωout)0.50.60.70.80.91.0Communityagreementn810121416(2,low)(2,medium)(2,high)(3,low)(3,medium)(3,high)(K,Communitystrength)0.50.60.70.80.91.0Communityagreementn810121416n
8

ωin
0.1

0.4

0.6

0.9

10

0.1

0.4

0.6

0.9

12

0.1

0.4

0.6

0.9

14

0.1

0.4

0.6

0.9

16

0.1

0.4

0.6

0.9

ωout
0.4
0.6
0.9
0.1
0.6
0.9
0.1
0.4
0.9
0.1
0.4
0.6
0.4
0.6
0.9
0.1
0.6
0.9
0.1
0.4
0.9
0.1
0.4
0.6
0.4
0.6
0.9
0.1
0.6
0.9
0.1
0.4
0.9
0.1
0.4
0.6
0.4
0.6
0.9
0.1
0.6
0.9
0.1
0.4
0.9
0.1
0.4
0.6
0.4
0.6
0.9
0.1
0.6
0.9
0.1
0.4
0.9
0.1
0.4
0.6

Average

E-LS1
0.69
0.72
0.76
0.65
0.66
0.73
0.68
0.66
0.63
0.72
0.66
0.63
0.67
0.74
0.78
0.68
0.66
0.68
0.72
0.62
0.63
0.79
0.66
0.65
0.67
0.75
0.76
0.67
0.63
0.72
0.71
0.66
0.64
0.75
0.67
0.64
0.69
0.70
0.78
0.67
0.63
0.65
0.74
0.63
0.61
0.75
0.66
0.63
0.65
0.77
0.80
0.68
0.62
0.70
0.75
0.61
0.65
0.79
0.69
0.61
0.69

Average agreement
E-LS2
0.73
0.75
0.85
0.67
0.66
0.75
0.70
0.67
0.64
0.78
0.67
0.64
0.71
0.78
0.81
0.69
0.65
0.68
0.75
0.63
0.64
0.79
0.66
0.65
0.69
0.80
0.81
0.68
0.62
0.70
0.71
0.68
0.63
0.77
0.66
0.65
0.69
0.73
0.81
0.69
0.62
0.65
0.72
0.62
0.62
0.77
0.66
0.63
0.66
0.78
0.82
0.68
0.62
0.69
0.74
0.62
0.64
0.79
0.70
0.61
0.70

E-exact MILP
0.72
0.8
0.94
0.66
0.64
0.78
0.82
0.68
0.65
0.85
0.7
0.64
0.74
0.87
0.99
0.8
0.65
0.72
0.86
0.60
0.67
0.99
0.66
0.71
0.72
0.89
0.99
0.75
0.64
0.86
0.85
0.7
0.67
0.92
0.71
0.66
0.8
0.86
0.99
0.76
0.66
0.71
0.9
0.69
0.66
0.96
0.76
0.66
0.73
0.97
1.0
0.81
0.66
0.82
0.92
0.62
0.66
0.98
0.82
0.61
0.77

0.71
0.74
0.82
0.64
0.69
0.75
0.68
0.65
0.62
0.75
0.68
0.63
0.72
0.77
0.85
0.71
0.65
0.73
0.75
0.62
0.63
0.80
0.66
0.65
0.70
0.81
0.82
0.70
0.65
0.71
0.74
0.67
0.66
0.78
0.71
0.63
0.73
0.76
0.84
0.67
0.64
0.67
0.74
0.65
0.61
0.79
0.67
0.62
0.71
0.81
0.84
0.70
0.63
0.75
0.77
0.61
0.65
0.79
0.77
0.61
0.71

Table 6: Comparison between heuristic and exact solution algorithms in recovering the ground truth communities of data
sets in group S1

26

Community

K n strength
2

8

low
medium
high
10 low

3

medium
high
12 low

medium
high
14 low

medium
high
16 low

8

medium
high
low
medium
high
10 low

medium
high
12 low

medium
high
14 low

medium
high
16 low

medium
high
Average

Average agreement
E-LS1 E-LS2 E-exact MILP
0.68
0.64
0.74
0.67
0.9
0.75
0.60
0.63
0.7
0.68
0.89
0.75
0.72
0.65
0.84
0.70
0.92
0.80
0.74
0.68
0.86
0.70
0.98
0.76
0.72
0.66
0.89
0.76
0.99
0.77
0.56
0.58
0.6
0.57
0.71
0.63
0.61
0.56
0.62
0.60
0.76
0.61
0.54
0.56
0.65
0.62
0.86
0.66
0.56
0.54
0.7
0.60
0.84
0.68
0.56
0.53
0.66
0.61
0.78
0.66
0.74
0.65

0.65
0.67
0.77
0.62
0.68
0.77
0.65
0.72
0.82
0.69
0.71
0.79
0.67
0.76
0.82
0.59
0.6
0.69
0.56
0.60
0.66
0.56
0.62
0.72
0.54
0.63
0.73
0.54
0.62
0.69
0.67

0.63
0.67
0.74
0.60
0.64
0.78
0.67
0.71
0.82
0.67
0.70
0.79
0.69
0.80
0.82
0.57
0.56
0.65
0.57
0.61
0.61
0.54
0.57
0.67
0.54
0.61
0.70
0.54
0.63
0.69
0.66

Table 7: Comparison between heuristic and exact solution algorithms in recovering the ground truth communities of data
sets in group S2

27

heuristic performance. The goal of a heuristic is to achieve an optimality gap that is systematically

close to 0% for the model at hand. As heuristics do not provide guarantees regarding solution quality,

we cannot evaluate their true optimality gap unless we have access to an eﬃcient algorithm that

produces optimal solutions (or at least good bounds on solution value).

To that end, we have introduced new mathematical programming formulations for the MLE model

of the DCSBM. We introduced a descriptive formulation based on a MINLP and employed linearization

techniques to transform it into a MILP. We also proposed bound tightening and symmetry-breaking

strategies, which lead to critical improvements to the model. The proposed solution methods can

ﬁnd optimal solutions of maximum likelihood with a certiﬁcate of global optimality. Furthermore, we

have reviewed three natural variants of the EM algorithm for this problem, and conducted extensive

numerical analyses to analyze their performance.

This work raises several interesting avenues for future research. In particular, there is still space to

improve the scalability of the exact methods. In our computational experiments, we noted that the

MILP often identiﬁes the optimal solution early in the optimization process but that it takes a much

longer time to ﬁnd good lower bounds and prove optimality. To improve this behavior, research could be

pursued on new problem formulations and valid inequalities permitting to achieve tighter lower bounds

and enhance the eﬃciency of the branch-and-bound exploration. Another alternative is to explore

mathematical decomposition techniques such as column generation, which have the potential to lead to

structurally-diﬀerent formulations and solution approaches. Finally, we generally encourage the pursuit

of a disciplined analysis of algorithms for other learning tasks, and likewise develop mathematical

programming approaches for other models of importance.

Acknowledgements

This research has been partially funded by CAPES, CNPq [grant number 308528/2018-2] and

FAPERJ [grant number E-26/202.790/2019] in Brazil, and by the Deutsche Forschungsgemeinschaft

28

(DFG, German Research Foundation) [grant number 277991500/GRK2201] in Germany. This support

is gratefully acknowledged.

References

[1] Abbe, E. 2017. Community detection and stochastic block models: recent developments. Journal

of Machine Learning Research 18(1) 6446–6531.

[2] Aloise, D., S. Caﬁeri, G. Caporossi, P. Hansen, S. Perron, L. Liberti. 2010. Column generation

algorithms for exact modularity maximization in networks. Physical Review E 82(4) 046112.

[3] Amini, A.A., A. Chen, P.J. Bickel, E. Levina. 2013. Pseudo-likelihood methods for community

detection in large sparse networks. The Annals of Statistics 41(4) 2097–2122.

[4] Amini, A.A., E. Levina. 2018. On semideﬁnite relaxations for the block model. The Annals of

Statistics 46(1) 149–179.

[5] Bandi, H., D. Bertsimas, R. Mazumder. 2019. Learning a mixture of gaussians via mixed-integer

optimization. INFORMS Journal on Optimization 1(3) 221–240.

[6] Belotti, P. 2009. Couenne: a user’s manual. Tech. rep., Lehigh University.

[7] Belotti, P., P. Bonami, M. Fischetti, A. Lodi, M. Monaci, A. Nogales-G´omez, D. Salvagnin. 2016.

On handling indicator constraints in mixed integer programming. Computational Optimization

and Applications 65(3) 545–566.

[8] Bennett, K.P., E. Parrado-Hern´andez. 2006. The interplay of optimization and machine learning

research. Journal of Machine Learning Research 7 1265–1281.

[9] Bertsimas, D., J. Dunn. 2017. Optimal classiﬁcation trees. Machine Learning 106(7) 1039–1082.

[10] Bezanson, J., A. Edelman, S. Karpinski, V.B. Shah. 2017. Julia: a fresh approach to numerical

computing. SIAM Review 59(1) 65–98.

29

[11] Bixby, R.E. 2012. A brief history of linear and mixed-integer programming computation. Documenta

Mathematica 107–121.

[12] Bonami, P., A. Lodi, A. Tramontani, S. Wiese. 2015. On mathematical programming with indicator

constraints. Mathematical Programming 151(1) 191–223.

[13] Boschetti, M.A., V. Maniezzo, M. Roﬃlli, A.B. R¨ohler. 2009. Matheuristics: optimization,

simulation and control . Springer, Berlin, Heidelberg.

[14] Bottou, L., Y. Bengio. 1995. Convergence properties of the k-means algorithms. Advances in

Neural Information Processing Systems 585–592.

[15] Brandes, U., D. Delling, M. Gaertler, R. Gorke, M. Hoefer, Z. Nikoloski, D. Wagner. 2007. On

modularity clustering. IEEE Transactions on Knowledge and Data Engineering 20(2) 172–188.

[16] Cai, T.T., X. Li. 2015. Robust and computationally feasible community detection in the presence

of arbitrary outlier nodes. The Annals of Statistics 43(3) 1027–1059.

[17] Chen, Y., S. Sanghavi, H. Xu. 2012. Clustering sparse graphs. Advances in Neural Information

Processing Systems 25 2204–2212.

[18] Chen, Y., J. Xu. 2016. Statistical-computational tradeoﬀs in planted problems and submatrix

localization with a growing number of clusters and submatrices. Journal of Machine Learning

Research 17(1) 882–938.

[19] Del Pia, A., A. Khajavirad, D. Kunisky. 2020. Linear programming and community detection.

arXiv preprint arXiv:2006.03213 .

[20] Dempster, A.P., N.M. Laird, D.B. Rubin. 1977. Maximum likelihood from incomplete data via

the EM algorithm. Journal of the Royal Statistical Society: Series B (Methodological) 39(1) 1–22.

30

[21] Dunning, I., J. Huchette, M. Lubin. 2017. JuMP: a modeling language for mathematical optimiza-

tion. SIAM Review 59(2) 295–320.

[22] Fortunato, S., D. Hric. 2016. Community detection in networks: a user guide. Physics Reports

659 1–44.

[23] Gambella, C., B. Ghaddar, J. Naoum-Sawaya. 2019. Optimization models for machine learning: a

survey. arXiv preprint arXiv:1901.05331 .

[24] Goldenberg, A., A.X. Zheng, S.E. Fienberg, E.M. Airoldi. 2010. A survey of statistical network

models. Now Publishers Inc, Hanover, MA.

[25] Gribel, D., T. Vidal. 2019. HG-means: a scalable hybrid genetic algorithm for minimum sum-of-

squares clustering. Pattern Recognition 88 569–583.

[26] Hansen, P., M. Ruiz, D. Aloise. 2012. A VNS heuristic for escaping local extrema entrapment in

normalized cut clustering. Pattern Recognition 45(12) 4337–4345.

[27] Jain, A.K. 2010. Data clustering: 50 years beyond k-means. Pattern Recognition Letters 31(8)

651–666.

[28] Karrer, B., M.E.J. Newman. 2011. Stochastic blockmodels and community structure in networks.

Physical Review E 83(1) 016107.

[29] Mallek, S., I. Boukhris, Z. Elouedi. 2015. Community detection for graph-based similarity:

application to protein binding pockets classiﬁcation. Pattern Recognition Letters 62 49–54.

[30] Newman, M.E.J. 2016. Equivalence between modularity optimization and maximum likelihood

methods for community detection. Physical Review E 94(5) 052315.

[31] Plastria, F. 2002. Formulating logical implications in combinatorial optimisation. European

Journal of Operational Research 140(2) 338–353.

31

[32] Qi, X., W. Tang, Y. Wu, G. Guo, E. Fuller, C.Q. Zhang. 2014. Optimal local community detection

in social networks based on density drop of subgraphs. Pattern Recognition Letters 36 46–53.

[33] Snijders, T.A., K. Nowicki. 1997. Estimation and prediction for stochastic blockmodels for graphs

with latent block structure. Journal of Classiﬁcation 14(1) 75–100.

[34] Xu, G., S. Tsoka, L.G. Papageorgiou. 2007. Finding community structures in complex networks

using mixed integer optimisation. The European Physical Journal B 60(2) 231–239.

[35] Xu, L., M.I. Jordan. 1996. On convergence properties of the EM algorithm for Gaussian mixtures.

Neural Computation 8(1) 129–151.

[36] Zhao, P., C.Q. Zhang. 2011. A new clustering method and its application in social networks.

Pattern Recognition Letters 32(15) 2109–2118.

32

