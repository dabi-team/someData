MLaaS4HEP: Machine Learning as a Service for HEP

Valentin Kuznetsov · Luca Giommi · Daniele Bonacorsi

0
2
0
2

c
e
D
0
1

]
x
e
-
p
e
h
[

2
v
1
8
7
4
1
.
7
0
0
2
:
v
i
X
r
a

Abstract Machine Learning (ML) will play a sig-
niﬁcant role in the success of the upcoming High-
Luminosity LHC (HL-LHC) program at CERN. An un-
precedented amount of data at the exascale will be col-
lected by LHC experiments in the next decade, and this
eﬀort will require novel approaches to train and use ML
models. In this paper, we discuss a Machine Learning as
a Service pipeline for HEP (MLaaS4HEP) which pro-
vides three independent layers: a data streaming layer
to read High-Energy Physics (HEP) data in their native
ROOT data format; a data training layer to train ML
models using distributed ROOT ﬁles; a data inference
layer to serve predictions using pre-trained ML models
via HTTP protocol. Such modular design opens up the
possibility to train data at large scale by reading ROOT
ﬁles from remote storage facilities, e.g. World-Wide
LHC Computing Grid (WLCG) infrastructure, and feed
the data to the user’s favorite ML framework. The in-
ference layer implemented as TensorFlow as a Service
(TFaaS) may provide an easy access to pre-trained ML
models in existing infrastructure and applications in-
side or outside of the HEP domain. In particular, we
demonstrate the usage of the MLaaS4HEP architecture
for a physics use-case, namely the t¯t Higgs analysis in
CMS originally performed using custom made Ntuples.

Valentin Kuznetsov
Cornell University, Ithaca, USA
E-mail: vkuznet@gmail.com
ORCiD: 0000-0003-0667-069X

Luca Giommi
University of Bologna and INFN, Bologna, Italy
E-mail: luca.giommi3@unibo.it
ORCiD: 0000-0003-3539-4313

Daniele Bonacorsi
University of Bologna and INFN, Bologna, Italy
E-mail: daniele.bonacorsi@unibo.it
ORCiD: 0000-0002-0835-9574

We provide details on the training of the ML model
using distributed ROOT ﬁles, discuss the performance
of the MLaaS and TFaaS approaches for the selected
physics analysis, and compare the results with tradi-
tional methods.

Keywords BigData
Machine Learning

·

LHC

·

Data Management

·

1 Introduction

With the CERN LHC program underway, we started
seeing an exponential acceleration of data growth in
the HEP ﬁeld. By the end of Run II, the CERN exper-
iments were already operating at the Peta-Byte (PB)
level, producing O(100) PB of data each year. The new
HL-LHC program will extend it further, to the Exa-
Byte scale, and the usage of ML in HEP will be crit-
ical [1]. ML techniques have been successfully used in
online and oﬄine reconstruction programs, and there
is a huge gain in applying them to detector simula-
tion, object reconstruction, identiﬁcation, Monte-Carlo
(MC) generation, and beyond [2]. As was pointed out
in the ML in HEP Community White Paper [1] the lack
of engagement from Computer Science experts to ad-
dress HEP ML challenges is partly due to the fact that
HEP data are stored in ROOT data-format, which is
mostly unknown outside of the HEP community. More-
over, the existing ML frameworks rely on ﬁxed-size
data representation of individual events, usually stored
in CSV [3], NumPy [4], HDF5 [5] data formats, while
in HEP the size of individual events cannot be deter-
mined a-priory1, and the data are stored in the event
tree-based data-structures used by the ROOT [6] data-

1 For instance, the number of electrons varies in each

physics event.

 
 
 
 
 
 
2

Valentin Kuznetsov et al.

format, and may require custom C++ classes to de-
code them properly. This and other reasons2 led to an
artiﬁcial gap between ML and HEP communities. For
example, in recent Kaggle challenges [7,8,9] the HEP
data was presented in CSV data-format to allow non-
HEP ML practitioners to compete. Moreover, produc-
tion workﬂows quite often require additional transfor-
mations, e.g. in CMS a Deep Neural Network (DNN)
used for a jet tagging algorithm relies on the Tensor-
Flow (TF) queue system with a custom operation kernel
for reading ROOT trees and feeding them to ML mod-
els like TensorFlow [10]. Here we discuss the Machine
Learning as a Service (MLaaS) architecture for HEP, re-
ferred to as MLaaS4HEP in this paper, which consists
of two individual parts. The ﬁrst part, the MLaaS4HEP
framework [11], provides a way to read HEP ROOT-
based data natively into the Python ML framework of
user choice. And, the second part, the TensorFlow as
a Service (TFaaS) framework [12], can be used to host
pre-trained ML models and obtain ML predictions via
HTTP protocol.

This approach can be used by physicists or experts
outside of HEP domain because it only relies on Python
libraries. It provides access to local or remote data stor-
age, and does not require any modiﬁcation or integra-
tion with the experiment’s speciﬁc framework(s). Such
modular design opens up a possibility to train ML mod-
els on PB-size datasets remotely accessible from the
WLCG sites without requiring data transformation and
data locality. Therefore, an existing gap between HEP
and ML communities can be easily closed using the dis-
cussed MLaaS architecture.

The organization of this paper is the following. Sec-
tion 2 provides a summary of related works and the key
aspects of the proposed solution. Section 3 presents the
details of the MLaaS4HEP architecture and its work-
ﬂow. Section 4 shows performance results and valida-
tion of MLaaS4HEP for a physics use-case. Section 5
summarizes possible future directions, and Section 6
presents the summary.

2 Related works and solutions

Machine Learning as a Service is a well-known concept
in industry, and major IT companies oﬀer such solu-
tions to their customers. For example, Amazon ML,
Microsoft Azure ML Studio, Google Prediction API
and ML engine, and IBM Watson are prominent imple-
mentations of this concept (see [13]). Usually, Machine

2 The event-based data structures cannot be fed directly
to existing ML frameworks and special care should be taken
either at the framework or at the data input level discussed
in this paper.

Learning as a Service is used as an umbrella of vari-
ous ML tasks such as data pre-processing, model train-
ing and evaluation, and inference through REST APIs.
Even though providers oﬀer plenty of interfaces and
APIs, most of the time these services are designed to
cover standard use-cases, e.g. natural language process-
ing, image classiﬁcations, computer vision, and speech
recognition. Even though a custom ML codebase can
be supplied to these platforms, its usage for HEP is
quite limited for several reasons. For instance, the HEP
ROOT data-format cannot be used directly in any ser-
vice provider’s APIs. Therefore, the operational cost,
e.g. data transformation from ROOT ﬁles to data-
format used by MLaaS provider APIs, data manage-
ment, and data pre-processing, can be very signiﬁcant
for large datasets. The data ﬂattening from dynamic
size event-based tree format to ﬁxed-size data repre-
sentation does not exist. Therefore, we found that out-
of-the-box commercial solutions most often are not ap-
plicable or ineﬀective for HEP use-cases (cost-wise and
functionality-wise). This might change in the future, as
various initiatives, e.g. CERN OpenLab [14], continue
to work in close cooperation with almost all aforemen-
tioned service providers.

At the same time, various R&D activities within
HEP are underway. For example, the hls4ml project [15]
targets ML inference on FPGAs, while the SonicCMS
project [16] is designed as Services for Optimal Net-
work Inference on Co-processors. Both are targeted to
the optimization of the inference phase rather than the
whole ML pipeline, i.e. from reading data to training
models and serving predictions. Another solution uses
the Spark platform for data processing and ML train-
ing [17]. Although it seems very promising, it requires
data ingestion into the CERN EOS ﬁlesystem or the
HDFS/Spark infrastructure. As such, there is no easy
way to access data located at WLCG sites or from out-
side of such dedicated infrastructure. Besides, a Spark-
based library (Analytics Zoo, BigDL) may be required
on top of Keras API, and ﬂexibility of ML framework
choice is limited on the user side. In the end, we found
that there is no ﬁnal product that can be used as Ma-
chine Learning as a Service for distributed HEP data
without additional eﬀorts which can provide transpar-
ent integration with existing Python-based ML frame-
works to perform ML training over HEP data, and this
work aims to close this gap.

2.1 Novelty of the proposed solution

The novelty of the proposed solution is the following.

MLaaS4HEP: Machine Learning as a Service for HEP

3

1. We provide a transparent access to HEP datasets
stored in the event tree-based ROOT data-format
into existing Python-based ML frameworks of user’s
choice. Usually, they are designed to operate with
row-based data structures like NumPy arrays, CSV
ﬁles and alike. The proposed solution discussed in
Sections 3.1 and 3.2 relies on the uproot library
[18] and XrootD protocol [19] for reading tree-based
ROOT ﬁles from local ﬁlesystem or remote sites.
It transforms the Jagged Arrays3 representation of
ROOT data, and fed it into ML framework via vec-
tor or matrix-based transformations applied to the
I/O stream. This opens up a possibility to use fa-
vorite ML frameworks like PyTorch [20], Tensor-
Flow [21], fast.ai [22], etc., and train ML models
using distributed HEP datasets.

2. We demonstrate in Sections 4.2 and 4.3 that the
proposed solution can work at any scale and trans-
parent to data locality. For that, we compared tra-
ditional HEP analysis based on custom ﬂat tuples
(derived from the production ROOT ﬁles) with the
MLaaS4HEP approach. We show that the latter has
several advantages such as usage of non-HEP ML
frameworks, ability to work with local or remote
storage, small or large datasets, and does not re-
quire domain knowledge and HEP software infras-
tructure.

3. We provide an independent Tensor as a Service
framework [12], developed as a part of this work,
which provides access to any kind of Tensor-based
ML models via HTTP protocol. Even though similar
solutions exist in the business world, most of them
are integrated as a part of their service stack which
may not be aﬀordable or accessible to research com-
munities where an eﬃcient, scalable open-source al-
ternative is desired to have. For instance, the TFaaS
service can be used with any programming lan-
guage, frameworks, and scripts without additional
development and modiﬁcations in existing infras-
tructure(s). It can be part of the MLaaS4HEP
pipeline or used independently to serve predictions
from non-HEP ML models.

4. Finally, we demonstrate that the proposed architec-
ture can be easily adapted among any HEP experi-
ment either as an entire pipeline or be used partially.
For example, the Data Streaming Layer (see Sect.
3.1) provides an access to distributed ROOT ﬁles.
The Data Training Layer (see Sect. 3.2) performs
proper data transformation from Jagged Arrays into
ﬂat data-format suitable for ML framework of user
choice and it can be easily integrated within exist-

3 Jagged Array is an array of arrays of which the member
arrays can be of diﬀerent sizes, see Sect. 3.2 for more details.

ing or new Python-based ML framework of user’s
choice. The Data Inference Layer (see Sect. 3.4) pro-
vides the TensorFlow as a Service (TFaaS) service
which can be used as an independent repository for
pre-trained ML models across diﬀerent experiments
or independent physics analysis groups.

3 MLaaS4HEP architecture

A typical ML workﬂow consists of several steps: acquire
the data necessary for training, use a ML framework
to train the model, and utilize the trained model for
predictions. In our Machine Learning as a Service solu-
tion, MLaaS4HEP [11], this workﬂow can be abstracted
as data streaming, data training, and inference phases,
respectively. Each of these components can be either
tightly integrated into the application design, or com-
posed and used individually. The choice is mostly driven
by particular use cases. We can deﬁne these layers as
following (see Fig. 1).

– Data Streaming Layer: it is responsible for read-
ing local and/or remote ROOT ﬁles, and streaming
data batches upstream to the Data Training Layer.
The implementation of this layer requires the ROOT
I/O layer with the support of remote I/O ﬁle access;
– Data Training Layer: it represents a thin wrapper
around standard ML libraries such as TensorFlow,
PyTorch, and others. It reads data from the Data
Streaming Layer in chunks, transforms them from
the ROOT TTree-based representation to the for-
mat suitable for the underlying ML framework, and
uses it for training purposes;

– Data Inference Layer: it refers to the inference
part of pre-trained models and can be either tightly
integrated within the underlying HEP framework,
or represented as a Service (aaS).

Even though the implementation of these layers can
diﬀer from one experiment to another (or other scien-
tiﬁc domains), it can be easily generalized and be part
of the foundation for a generic Machine Learning as
a Service framework. The MLaaS4HEP framework [11]
implements the Data Streaming and Data Training lay-
ers, and we provide their details in Sect. 3.1 and Sect.
3.2, respectively. In Sect. 3.3 we provide technical de-
tails of the ML training workﬂow implemented in the
MLaaS4HEP framework and used for our studies pre-
sented in Sect. 4. The Data Inference Layer is imple-
mented as independent TFaaS [12] framework since it
can be used outside of HEP, and its details are discussed
in Sect. 3.4.

4

Valentin Kuznetsov et al.

Fig. 1 MLaaS4HEP architecture diagram representing three independent layers: a Data Streaming Layer to read
local or remote ROOT ﬁles, a Data Training Layer to feed tree-based HEP data into ML framework, and a Data
Inference Layer via TensorFlow as a Service [23]

3.1 Data Streaming Layer

The Data Streaming Layer is responsible for streaming
data from local or remote data storage. Originally, the
reading of ROOT ﬁles was mostly possible from C++
frameworks, but the recent development of ROOT I/O
now allows to easily access ROOT data locally from
Python. The main development was done in the up-
root [18] framework supported by the DIANA-HEP
initiative [24]. The uproot library uses NumPy [4] calls
to rapidly cast data blocks in ROOT ﬁle as NumPy ar-
rays. It allows, among the implemented features, a par-
tial reading of ROOT TBranches, non-ﬂat TTrees, non
TTrees histograms, and more. It relies on data caching

and parallel processing to achieve high throughput. In
our benchmarks, we were able to read HEP events at
O(10) kHz4 from local and from remote
the level of
storages. The latter was provided via XrootD protocol
[19].

∼

In our implementation of Machine Learning as a Ser-
vice (see Sect. 3.5) this layer was composed as a Data
Generator5 which is capable of reading chunk of data ei-
ther from local or remote ﬁle(s). The output of the Data
Generator is a NumPy array with ﬂat and Jagged Ar-

4 Speed varies based on many factors, including caching,

type of storage and network bandwidth.

5 A piece of code deﬁned as Python generator to read an
appropriate chunk of data upon request from upstream code.

HDFSROOTﬁleslocalﬁlesystemRemotestorageuprootData ReaderbatchesXRootDNumPyarrayjaggedbranchesjaggeddimensionalityﬂatbranchesInput Jagged Array dataNeural Networkwith Dense Jagged  LayersData Streaming LayerData Training LayerRepositoryof NN modelsData Inference LayerMLaaS4HEP: Machine Learning as a Service for HEP

5

ray attributes. Such implementation provides eﬃcient
access to large datasets since it does not require loading
the entire dataset into the RAM of the training node.
Also, it can be used to parallelize the data ﬂow into the
ML workﬂow pipeline. The size of data chunks read by
this layer can be easily ﬁne-tuned based on the com-
plexity of events and the available bandwidth. For in-
stance, in our initial proof-of-concept implementation,
see Sect. 3.5, we used 1k events as a chunk data size,
while within performance studies discussed in Sect. 4.3
we extended chunk size to 100k events.

3.2 Data Training Layer

This layer transforms HEP ROOT data presented by
the Data Streaming Layer as Jagged Array into a ﬂat
data-format used by the application [1,10]. The Jagged
Array (see Fig. 2) is a compact representation of vari-
able size event data produced in HEP experiments.
The HEP tree-based data representation is optimized
for data storage but it is not directly suitable for ML
frameworks. Therefore a certain data transformation is
required to feed tree-based data structures into the ML
framework as a ﬂat data structure. We explored two
possible transformations: a vector representation with
padded values (see Fig. 3) and a matrix representation
of data into the phase space of user choice (see Fig. 5).
The HEP events have diﬀerent dimensionality
across event attributes. For instance, a single event may
have a diﬀerent number of physics particles. There-
fore, proper care should be done to ﬂatten and padding
ROOT events in the Jagged Array representation. For
that, we use a two-passes procedure. In the ﬁrst pass
across all the events6 we determine the dimensionality
of each attribute and its min/max values. In the sec-
ond pass we map Jagged Array attributes into a single
vector representation with proper size and padding (see
Fig. 3). In addition, we provide a proper normalization
of each attribute during this phase7. We also keep a
separate masking vector (see Fig. 4) to distinguish as-
signed padded (e.g. NaN or zeros) values from the real
values of the attributes. This may be important in cer-
tain kinds of Neural Networks, e.g. AutoEncoders (AE)
[25] where the location of padded values in the input
vector can be used in the decoding phase.

6 Even though this procedure may not be feasible at Peta-
Byte scale, it can be easily replaced by studying various
Monte-Carlo distributions of such events to ﬁnd attribute’s
boundaries.

7 This layer can be easily abstracted as a Python decora-
tor to allow multiple implementations of normalization pro-
cedure.

A matrix representation can be obtained from a
Jagged Array (see Fig. 5). For example, the spatial co-
ordinates or the attribute components are often part of
HEP datasets, and therefore it can be used for this map-
ping. This approach can resolve the ambiguity8 of vec-
tor representation (in terms of dimensionality choice)
but it has its own problem with the choice of granular-
ity of space matrix. For example, in the simplest case,
a 2D matrix representation9 (see Fig. 5) can be used
in some X-Y phase space. In this case, this matrix rep-
resents an image where X and Y refer to an arbitrary
pair of attributes. But the cell size of this image is not
known a-priory. A choice of cell size may introduce a
collision problem within an event, e.g. diﬀerent parti-
cles may have values of (X,Y) pair within the same
cell. Such ambiguity may be easily resolved either by
increasing matrix granularity or using another phase
space, e.g. via higher dimensions of the cell space. But
such changes will increase the sparsity of matrix repre-
sentation and the matrix size, and therefore will require
more computing resources at the training time.

Below we provide details of the MLaaS4HEP work-
ﬂow used in the Data Streaming and Data Training
layers using a vector representation for the results pre-
sented in Sect. 4.

3.3 ML training workﬂow implementation

We implemented the Data Streaming and Data Train-
ing layers using Python programming language and we
made them available in the MLaaS4HEP repository
[11] under MIT license. The Data Training Layer was
abstracted to support any kind of Python-based ML
frameworks: TensorFlow, PyTorch, and others10.

We used two parameters to control the data ﬂow
within the framework. The chunk size parameter con-
trols a chunk of data read by the Data Streaming Layer
from local or remote storage. And, the batch size pa-
rameter deﬁnes the number of events used by the under-
lying ML framework in each training cycle. Therefore,
further, we refer to chunk as a set of events read by the
Data Streaming Layer while batches as a set of events
used by the ML training loop.

In order to train the ML models deﬁned by the user
code (provided externally) the MLaaS4HEP framework

8 For very large datasets we may use Monte-Carlo distri-
butions to determine the ﬁnite size of the certain attributes
and cut them oﬀ at a certain level, and, therefore, reject rare
events which may exceed this threshold.

9 In the general case the matrix representation can have

any number of dimensions.
10 In all our tests we used Keras and PyTorch frameworks
to deﬁne our ML models.

6

Valentin Kuznetsov et al.

Fig. 2 Jagged Array data representation. It consists of ﬂat attributes followed by Jagged attributes whose
dimensions vary event by event [23]

Fig. 3 A vector representation of Jagged Array with padded values [23]

Fig. 4 A vector representation of Jagged Array along with corresponding mask vector [23]

uses proper data chunks with the same proportion of
events presented in the ROOT ﬁles. The schematic of

the data ﬂow used in the Data Streaming and Data
Training layers is shown in Fig. 6.

NumPyarrayjaggedbranchesjaggeddimensionalityﬂatbranchesNumPyarrayjaggedbranchesjaggeddimensionalityﬂatbranchesjaggedbranchpaddingjagged branchesrest ofjagged brancheswith paddingﬂat branchesTransform jagged NumPyarray into ﬂat onejaggedbranchpaddingjagged branchesrest ofjagged brancheswith paddingﬂat branchesdataarraymaskarraymask representingreal data valuesmask representingpadded NAN valuesMLaaS4HEP: Machine Learning as a Service for HEP

7

Fig. 5 A matrix representation of Jagged Array into certain phase space, e.g. eta-phi [23]

The ﬁrst pass (denoted by 1○ in Fig. 6) represents
the reading part of the MLaaS4HEP pipeline to create
a specs ﬁle. This part is performed by reading all the
ROOT ﬁles in chunks (which size is ﬁxed a priori by the
user) so that the information stored in the specs ﬁle is
updated chunk by chunk. The specs ﬁle contains all the
information about the ROOT ﬁles: the dimension of
Jagged branches, the minimum and the maximum for
each branch, and the number of events for each ROOT
ﬁle11.

The second part of the ﬂowchart shown as 2○ rep-
resents the following logic of the ML training phase.
In the ﬁrst loop of the cycle, when the events are not
read yet, we read N events (where N is equal to the
chunk size) from the i-th ﬁle fi that we store into the
chunk size events are taken
i-th chunk ci. Then ni/Ntot·
from it, where ni is the number of the events of the ﬁle
fi and Ntot is the whole amount of events of all the ﬁles.
These events are converted into Numpy arrays, with the
necessary ﬁx of the Jagged Arrays dimensions and nor-
malization of the values. This part is performed thanks
to the information contained in the specs ﬁle computed
in step 1○. The reading of the events and their pre-
processing is performed for all the ﬁles fi. After hav-
ing created a chunk of N events properly mixed from
the diﬀerent ﬁles, the events are used to train the ML

11 Once the specs ﬁle is produced, either through the afore-
mentioned procedure or by studying Monte-Carlo distribu-
tions (for large datasets) to determine attribute dimensions
and their min/max values, it can be reused for all ﬁles from
the given dataset during the ML training phase.

model. The training phase is performed using batches
of data taken from the created chunk, and run for a cer-
tain number of epochs. The batch size and the number
of epochs are ﬁxed a priori by the user. Then we come
back at the beginning of the cycle, and if all the events
stored in the chunk ci have been already read, we read
N events from the ﬁle fi, otherwise we read the proper
chunk size) from the chunk
amount of events (ni/Ntot ·
ci. Subsequently, the events are pre-processed. The part
of reading and pre-processing is performed for all the
ﬁles fi, in order to create the proper data chunk used
to train the ML model. Finally, the model is trained. If
the ﬁles are not completely read the entire pipeline is
restarted from the beginning of point 2○ until all events
are read, creating at each cycle a new chunk of events
that is used to train the ML model. At the end of this
cycle, all the events contained in all ﬁles are read and
the training process of the model is completed, produc-
ing a model that can be used in physics analysis.

3.4 Data Inference Layer

A data inference layer can be implemented in a variety
of ways. It can be either tightly integrated with applica-
tion frameworks (for example both CMS and ATLAS
experiments followed this approach in their CMSSW-
DNN [26] and LTNN [27] solutions respectively) or it
can be developed as a Service (aaS) solution. The for-
mer has the advantage of reducing latency of the infer-
ence step per processing event, but the latter can be eas-

NumPyarrayjaggedbranchesjaggeddimensionalityﬂatbranchesbranchvectorjagged branches representation as ﬁxed size branch vectors in some (eta-phi) spacerest of branchvectorsﬂat branchesTransform jagged NumPymatrix form (eta-phi phase)Transform matrix forminto vectorbranchvector w/ 0’sphieta8

Valentin Kuznetsov et al.

Fig. 6 Schematic representation of the steps performed in the MLaaS4HEP pipeline, in particular those inside
the Streaming and Training layers (see text for details)

ily generalized and become independent from internal
infrastructure. For instance, it can be easily integrated
into cloud platforms, it can be used as a repository of
pre-trained models, and also serve models across exper-
iment boundaries. However, the speed of the data in-
ference layer, i.e. throughput of serving predictions, can
vary based on the chosen technology. A choice of HTTP
protocol guarantees easy adaptation, while gRPC pro-
tocol can provide the best performance but will require
dedicated clients. We decided to implement the Data
Inference Layer as a TensorFlow as a Service architec-
ture [12] based on HTTP protocol12.

We evaluated several ML frameworks and we de-
cided to use TensorFlow graphs [21] for the inference
phase. The TF model represents a computational graph
in a static form, i.e. mathematical computations, graph

12 The code is available in the TFaaS repository [12] under
MIT license.

edges, and data ﬂow are well-deﬁned at run time. Read-
ing TF model can be done in diﬀerent programming
languages thanks to the support of APIs provided by
the TF library. Moreover, the TF graphs are very well
optimized for GPUs and TPUs. We opted for the Go
programming language [28] to implement the inference
part of the MLaaS4HEP framework based on the fol-
lowing factors: the Go language natively supports con-
currency via goroutines and channels; it is the language
developed and used by Google, and it is very well inte-
grated with the TF library; it provides a ﬁnal static exe-
cutable which signiﬁcantly simpliﬁes its deployment on-
premises and to various (cloud) service providers. We
also opted out in favor of the REST interface. Clients
may upload their TF models to the server and use it
for their inference needs via the same interface. Both
Python and C++ clients were developed on top of the
REST APIs (end-points) and other clients can be easily

convert into numpy arrays, ﬁx Jagged Arrays’ dimension and normalise the valuesSBBSBBBchunk of handled eventsSBBBTrain the modelAre all the ﬁles completely read?NOYESspecs.jsonRead all the ROOT ﬁlescomputespecs ﬁleload specs information12If chunk ci is empty or fully processed, readN = chunk sizeevents from the ﬁle fi{max:{key1: max_1,key_2: max_2,…},min:{key_1: min_1, key_2: min_2,…}, …}SBBBread the eventspre-process the eventsTake niNtot·chunk<latexit sha1_base64="DwC1W3VXV1UHTc/xIhGn070QReE=">AAACEnicbVA9SwNBEN3zO/ErainIYhCswl0Cahm0sZIIRoUkHHObSbJkb+/YnRPkuM6f4K+w1cpO7MQ/YOFv0Uu00OirHu/NMPNeECtpyXXfnKnpmdm5+YVCcXFpeWW1tLZ+bqPECGyKSEXmMgCLSmpskiSFl7FBCAOFF8HwaORfXKGxMtJndB1jJ4S+lj0pgHLJL221ewZEqn2ZpSd+ShFlWVt0I+JikOihXyq7FXcM/pd436RcL798hLXCYcMvvbe7kUhC1CQUWNvy3Jg6KRiSQmFWbCcWYxBD6GMrpxpCtJ10nCPjO4kFiniMhkvFxyL+3EghtPY6DPLJEGhgJ72R+J/XSqh30EmljhNCLUaHSCocH7LCyLwg5F1pkAhGnyOXmgswQIRGchAiF5O8sWLehzeZ/i85r1a8WqV66pXre+wLC2yTbbNd5rF9VmfHrMGaTLAbdsfu2YNz6zw6T87z1+iU872zwX7Bef0EteuhwQ==</latexit>niNtot·chunk<latexit sha1_base64="DwC1W3VXV1UHTc/xIhGn070QReE=">AAACEnicbVA9SwNBEN3zO/ErainIYhCswl0Cahm0sZIIRoUkHHObSbJkb+/YnRPkuM6f4K+w1cpO7MQ/YOFv0Uu00OirHu/NMPNeECtpyXXfnKnpmdm5+YVCcXFpeWW1tLZ+bqPECGyKSEXmMgCLSmpskiSFl7FBCAOFF8HwaORfXKGxMtJndB1jJ4S+lj0pgHLJL221ewZEqn2ZpSd+ShFlWVt0I+JikOihXyq7FXcM/pd436RcL798hLXCYcMvvbe7kUhC1CQUWNvy3Jg6KRiSQmFWbCcWYxBD6GMrpxpCtJ10nCPjO4kFiniMhkvFxyL+3EghtPY6DPLJEGhgJ72R+J/XSqh30EmljhNCLUaHSCocH7LCyLwg5F1pkAhGnyOXmgswQIRGchAiF5O8sWLehzeZ/i85r1a8WqV66pXre+wLC2yTbbNd5rF9VmfHrMGaTLAbdsfu2YNz6zw6T87z1+iU872zwX7Bef0EteuhwQ==</latexit> size<latexit sha1_base64="AlkJo5MfUERKU7uO2TI2e5c6PKc=">AAAB9nicbVC7TgJBFJ31ifhCLW0mEhNsyC4maieJjSUm8kiAkNnhAhNmZzczd4244Re01MrO2PoBVv6FhZ9h7+5CoeCpTs65N/fc4wZSGLTtT2thcWl5ZTWzll3f2Nzazu3s1owfag5V7ktfN1xmQAoFVRQooRFoYJ4roe4OLxK/fgPaCF9d4yiAtsf6SvQEZ5hIRtxBJ5e3i3YKOk+cKcmfvxe+Px5aR5VO7qvV9XnogUIumTFNxw6wHTGNgksYZ1uhgYDxIetDM6aKeWDaUZp1TA9Dw9CnAWgqJE1F+L0RMc+YkefGkx7DgZn1EvE/rxli76wdCRWECIonh1BISA8ZrkVcAtCu0IDIkuRAhaKcaYYIWlDGeSyGcSvZuA9n9vt5UisVneNi6crJl0/IBBmyTw5IgTjklJTJJamQKuFkQO7JI3mybq1n68V6nYwuWNOdPfIH1tsPYmWWxQ==</latexit>size<latexit sha1_base64="AlkJo5MfUERKU7uO2TI2e5c6PKc=">AAAB9nicbVC7TgJBFJ31ifhCLW0mEhNsyC4maieJjSUm8kiAkNnhAhNmZzczd4244Re01MrO2PoBVv6FhZ9h7+5CoeCpTs65N/fc4wZSGLTtT2thcWl5ZTWzll3f2Nzazu3s1owfag5V7ktfN1xmQAoFVRQooRFoYJ4roe4OLxK/fgPaCF9d4yiAtsf6SvQEZ5hIRtxBJ5e3i3YKOk+cKcmfvxe+Px5aR5VO7qvV9XnogUIumTFNxw6wHTGNgksYZ1uhgYDxIetDM6aKeWDaUZp1TA9Dw9CnAWgqJE1F+L0RMc+YkefGkx7DgZn1EvE/rxli76wdCRWECIonh1BISA8ZrkVcAtCu0IDIkuRAhaKcaYYIWlDGeSyGcSvZuA9n9vt5UisVneNi6crJl0/IBBmyTw5IgTjklJTJJamQKuFkQO7JI3mybq1n68V6nYwuWNOdPfIH1tsPYmWWxQ==</latexit>events from the chunk cilist of ﬁlesDid you go through all the ﬁles?YESNOi = i + 1i = 0MLaaS4HEP: Machine Learning as a Service for HEP

9

interaction, which gives the mass to fermions propor-
tionally to the coupling. The heaviest top quark is re-
sponsible for coupling to the Higgs boson. Direct mea-
surement of the top-Higgs coupling exploits tree-level
processes. The t¯tH production plays an important role
in the study of the top-Higgs Yukawa coupling, as other
production mechanisms (such as gluon-gluon fusion) in-
volve loop-level diagrams in which contributions from
Beyond Standard Model (BSM) physics could enter the
loops unnoticed. The highest branching ratio (
25%)
is represented by the all-hadronic decay channel with
H(b¯b) and all-hadronic t¯t. The W bosons produced by
the t¯t pair decay into a pair of light quarks while the
Higgs boson decays into a b¯b pair (see Fig. 7). In the
ﬁnal state, there are at least eight partons (more might
arise from the initial and ﬁnal state radiation) where
four of them are bottom (b) quarks. Despite the high-
est branching ratio, the all-jets ﬁnal state is very chal-
lenging. It is dominated by the large QCD multi-jet
production at LHC, and there are large uncertainties
in this channel due to the presence of many jets. At the
same time, it represents the unique possibility to fully
reconstruct the t¯tH as all decay products are observ-
able.

≈

developed thanks to HTTP protocol. The TFaaS frame-
work can be used outside of HEP to serve any kind of
TF-based models uploaded to TFaaS service via HTTP
protocol13.

3.5 MLaaS4HEP: proof-of-concept prototype

When all layers of the MLaaS4HEP framework were
developed, we successfully tested a working prototype
of the system by using ROOT ﬁles accessible through
XrootD servers. The data were read in chunks of 1k
events, where the single chunk was approximately 4 MB
in size. We tested this prototype on a local machine as
well as successfully deployed it on a GPU node. To fur-
ther validate the MLaaS4HEP framework we decided to
apply it to a real physics analysis, see Sect. 4, where we
explored local and remote data access, usage of diﬀer-
ent data chunks, random access to ﬁles, etc. All details
can be found in the next section.

4 Real case scenario

In order to validate the MLaaS4HEP approach, we de-
cided to test the infrastructure on a real physics use-
case. This allowed us to test the performances of the
MLaaS4HEP framework, and validate its results from
the physics point of view. We decided to use the t¯t
Higgs analysis (t¯tH(b¯b)) in the boosted, all-hadronic
ﬁnal state [29,30,31] due to aﬃnity with the analysis
group. In the following sub-sections we discuss:
– the t¯tH(b¯b) all-hadronic analysis strategy (Sect.

4.1);

– MLaaS4HEP validation (Sect. 4.2);
– MLaaS4HEP performance results using the physics

use-case (Sect. 4.3);

– MLaaS4HEP projected performance (Sect. 4.4);
– TFaaS performance results (Sect. 4.5).

4.1 t¯tH(b¯b) all-hadronic analysis strategy

Fig. 7 Feynman diagram for the t¯tH(b¯b) decay

The Higgs boson is considered the most relevant discov-
ery of the last few years in High Energy Physics. After
almost ﬁfty years from its prediction, it was discov-
ered by the ATLAS and CMS collaborations in 2012 at
the CERN Large-Hadron Collider (LHC) [32,33]. Since
then, many analyses have been performed in order to
measure its properties with higher precision.

In the Standard Model framework, the Higgs boson
is predicted to couple with fermions via Yukawa-like

13 For instance, we tested the TFaaS functionality using
non-HEP models such as image recognition ML models.

At the 13 TeV center-of-mass energy, top quarks
with a very high pT can be produced via t¯tH. If their
Lorentz boost is suﬃciently high, their decay prod-
ucts are very collimated into a single, wide jet, named
boosted jet. In particular, we are interested in the
t¯tH(b¯b) analysis with all-jets ﬁnal state where at least
one of the jets of the ﬁnal state is a boosted jet, and
where the Higgs boson decays in a pair of well resolved
jets identiﬁed as a result of the hadronization of bottom
quarks.

10

Valentin Kuznetsov et al.

For identiﬁcation of the t¯tH(b¯b) events containing a
resolved-Higgs decay a Machine Learning model based
on Boosted Decision Tree (BDT) was used by the CMS
Higgs Physics Analysis Group (HIG PAG) [31] and
the training was done within TMVA [34] framework.
The Monte Carlo simulation provides events used for
training, where events are selected among the t¯tH sam-
ple and the two dominant background samples, namely
QCD and t¯t, respectively. The t¯tH events with the re-
solved Higgs-boson matching to the system of two b-
tagged jets are considered as signal events. On the con-
trary, unmatched t¯tH events, and all the QCD and t¯t
events are considered as background events. Both signal
and background events are required to pass some selec-
tion criteria, such as to have at least a boosted jet, to
contain no leptons, to pass the signal trigger, etc. This
selection is aimed to select boosted, all-jets-like events.

4.2 MLaaS4HEP validation

In order to validate the MLaaS4HEP functionality
against standard BDT-based procedure, we decided to
use a set of ROOT ﬁles from the resolved-Higgs analysis
discussed in Sect. 4.1. The goal of this exercise was to
demonstrate that the MLaaS4HEP framework can pro-
vide a valuable alternative and deliver comparable re-
sults with respect to the traditional analysis based on a
pre-deﬁned set of metrics. For our purposes, we decided
to use a generic ML model and compare the results ob-
tained inside and outside MLaaS4HEP. In particular,
we explored the following approaches:

– use MLaaS4HEP to read and normalize events, and

to train the ML model;

– use MLaaS4HEP to read and normalize events, and
use a Jupyter notebook to perform the training of
the ML model outside MLaaS4HEP;

– use a Jupyter notebook to perform the entire

pipeline without using MLaaS4HEP.

Initially, we performed the analysis using the ROOT
ﬁles that passed the selection criteria discussed in Sect.
4.1. The ﬁnal dataset consisted of eight ROOT ﬁles
containing background events, and one ﬁle containing
signal events. Each ﬁle had 27 branches, with 350k
events in total, and the total size of this dataset was
28 MB. The ratio between the number of signal events
and background events was approximately 10.8%. The
dataset was split into three parts, 64% for training, 16%
for validation, and 20% for test purposes, respectively.
In particular, we used a Keras sequential Neural Net-
work with two hidden layers made by 128 and 64 neu-
rons, and with a 0.5 dropout regularization between

layers. Finally, we trained the model for 5 epochs with
a batch size of 100 events.

The results of this exercise are shown in Fig. 8, and
demonstrate that diﬀerent approaches have similar per-
formance. The AUC score of the generic ML model is
found to be comparable with the BDT-based analysis14.

4.3 MLaaS4HEP performance

In this section, we provide details of the MLaaS4HEP
performance testing: the scalability of the framework
and its benchmarks using diﬀerent storage layers. For
that purpose, we used all available ROOT ﬁles without
any physics cuts. This gave us a dataset with 28.5M
events with 74 branches (22 ﬂat and 52 Jagged), and a
total size of about 10.1 GB.

We performed all tests running the MLaaS4HEP
framework on macOS, 2.2 GHz Intel Core i7 dual-core,
8 GB of RAM, and on CentOS 7 Linux, 4 VCPU In-
tel Core Processor Haswell 2.4 GHz, 7.3 GB of RAM
CERN Virtual Machine. The ROOT ﬁles are read from
three data-centers: Bologna (BO), Pisa (PI), and Bari
(BA).

Table 1 summarizes the I/O numbers we obtained in
the ﬁrst step of the MLaaS4HEP pipeline ( 1○ in Fig. 6)
using various setups and a chunk size of 100k events. It
provides the values of time spent for reading the ﬁles,
the time spent for computing specs values, the total
time spent for completing the step 1○, and the event
throughput for the reading and specs computing step.
In Fig. 9 we show the event throughput for reading
the data as a function of chunk size for diﬀerent trials.
In all cases, we ﬁnd no signiﬁcant peaks. The larger
chunk sizes can lead to certain problems, as in the case
of the CERN VMs, where we may reach a limitation of
the underlying hardware, e.g. big memory footprint.

In the performance studies of the second step of
the MLaaS4HEP pipeline ( 2○ in Fig. 6) we are inter-
ested in the data reading part, the data pre-processing
step (which include data transformation), and the time
spent in the MLaaS4HEP training step.

As already mentioned in Sect. 3.3, there is a loop
over ﬁles that allows building the chunk used to train
the ML model with the adequate proportion of the
events. If the chunk that contains the events of the i-th
ROOT ﬁle is empty or fully processed, a new chunk of
events from the i-th ﬁle is read, and the time for read-
ing is added to the whole time spent for creating the

14 Please note, our goal was to demonstrate that the
MLaaS4HEP approach provides similar results to the BDT-
based analysis, but we did not target to reproduce and/or
match exact AUC numbers obtained in the standard physics
analysis.

MLaaS4HEP: Machine Learning as a Service for HEP

11

Fig. 8 Comparison of the AUC score for the training, validation, and test set for three diﬀerent cases: (i) using
MLaaS4HEP to read and normalize events, and to train the ML model; (ii) using MLaaS4HEP to read and
normalize events, and using a Jupyter notebook to perform the training of the ML model outside MLaaS4HEP;
(iii) using a Jupyter notebook to perform the entire pipeline without using MLaaS4HEP

macOS with local ﬁles
macOS with remote ﬁles (BO)
VM with local ﬁles
VM with remote ﬁles (BO)
VM with remote ﬁles (BA)
VM with remote ﬁles (PI)

reading time
(s)
1532
4349
1132
1919
2136
2114

specs comp. time
(s)
1031
1007
978
1017
988
996

time to complete
step 1○ (s)
2608
5453
2153
2994
3193
3171

event throughput for
reading + specs comp. (evts/s)
11006
5265
13366
9606
9027
9067

Table 1 Performances of reading and specs computing phase with chunk size ﬁxed to 100k events, using the
macOS system and the CERN VM. In local storage cases, the ﬁles are stored in a SSD 500 GB in the macOS case
and in a Virtual Disk 52 GB in the CERN VM case, respectively. Moreover, BO, BA, and PI stand for various
Italian storage facilities with diﬀerent WAN conﬁgurations (see text for more details)

chunk (see Fig. 6). In other words, the time spent for
creating a chunk is made by the sum of n reading ac-
tions, and of the time to pre-process the events. The
event throughput for creating a single data chunk and
the event throughput for pre-processing a single data
chunk are reported in Table 2. In Fig. 10 we show the
event throughput for creating a chunk as a function of
the chunk size for diﬀerent trials.

We found that the time spent for creating a chunk
was almost the same using macOS or CERN VM, and
similar using local or remote ﬁles. Obviously, for remote
ﬁles, the reading time increased consequently, and the
time for creating the chunk increased, but this diﬀer-
ence was quite negligible. For instance, we spend around
90 seconds to create a chunk of 100k events, which

translates into an event throughput of about 1.1k etvs/s
as reported in Table 2.

During the implementation of the MLaaS4HEP
framework, we resolved few bottlenecks with respect
to the results obtained in [23]. For example, we im-
proved the reading time by a factor of 10. This came
from better handling of Jagged Arrays via ﬂattening
the event arrays and computing of min/max values of
each branch. Moreover, we also obtained a factor of 2.8
improvements in the data pre-processing step by using
lists comprehensions instead of loops within the event.
We found that MLaaS4HEP took about 53 seconds to
pre-process 100k events with 42%, 44%, and 10% break-
down used for the normalization step, ﬁxing the dimen-
sions, and creating the masking vectors, respectively.

12345Epoch0.8250.8500.8750.9000.9250.950AUCusing jupyter notebookreading with MLaaS, model with jupyter notebookusing MLaaS TrainingValidationTestusing jupyter notebookreading with MLaaS, model with jupyter notebookusing MLaaS TrainingValidationTest12

Valentin Kuznetsov et al.

Fig. 9 Event throughput for reading the data as a function of the chunk size for diﬀerent trials

macOS with local ﬁles
macOS with remote ﬁles (BO)
VM with local ﬁles
VM with remote ﬁles (BO)
VM with remote ﬁles (BA)
VM with remote ﬁles (PI)

Event throughput for
creating a chunk (evts/s)
1101
1051
1081
1020
942
982

Event throughput for
pre-processing a chunk (evts/s)
1156
1188
1120
1080
1081
1060

Table 2 Event throughput for the chunk creation and for the pre-processing step with a chunk size of 100k events
computed as the ratio of the number of events over the time spent on chunk creation. The diﬀerence between the
two steps is based on the reading part, i.e. the time for creating a chunk, as the sum of times for reading events
from the ROOT ﬁles, and the time for the pre-processing step

In conclusion, for the presented physics use-case we
found comparable results between ML models inside
and outside the MLaaS4HEP framework. Using 10 GB
of data (approximately 28.5M events) we obtained the
following results:

– MLaaS4HEP framework is capable to work with lo-

cal and remote ﬁles;

– its throughput reaches about 13.4k evts/s for read-
ing local ROOT ﬁles (with specs computing), and
about 9.6k evts/s for remote ﬁles;

– the throughput of pre-processing step is peaked at

12k evts/s.

a dataset of 10 GB of data (28.5M events). Therefore,
we estimate that using the same hardware resources
the step 1○ will take about 58 hours and 58k hours for
datasets at TB and PB scale, and the time for step 2○
will be around 719 hours and 719k hours, respectively.
At this stage, our goal was mainly to prove the feasi-
bility of the MLaaS4HEP pipeline, and validate its us-
age within the context of a real physics use-case rather
than perform real ML training at TB/PB scale. In Sect.
5 we discuss further improvements which can be done.

4.5 TFaaS performance

4.4 MLaaS4HEP performance projection

Based on our studies presented in the previous section
we found that MLaaS4HEP takes about 35 minutes for
the ﬁrst step of the pipeline ( 1○ in Fig. 6), and around
7 hours for the second step ( 2○ in Fig. 6) to process

The performance testing of the TFaaS service was done
using a variety of ML models, from simple image clas-
siﬁcation to the ML model developed and discussed in
Sect. 4.2. In particular, we performed several bench-
marks using the TFaaS server running on CentOS 7
Linux, 16 cores, 30 GB of RAM. The benchmarks were

500010000150002000025000300001033501002005001000Event throughput (evts/s)Thousands events in a chunkrun in macOS with local filesrun in macOS with remote (BOLOGNA) filesrun in CERN VM with local filesrun in CERN VM with remote (BOLOGNA) filesrun in CERN VM with remote (BARI) filesrun in CERN VM with remote (PISA) filesMLaaS4HEP: Machine Learning as a Service for HEP

13

Fig. 10 Event throughput for creating a chunk as a function of the chunk size for diﬀerent trials

∼

done in two modes: using 1k calls with 100 concur-
rent clients and 5k calls with 200 concurrent clients.
We tested both JSON and ProtoBuﬀer [35] data for-
mats while sending and fetching the data to/from the
TFaaS server. In both cases, we achieved a through-
put of
500 req/sec. These numbers were obtained
by serving mid-size pre-trained model which consists
of 1024x1024 hidden layers used in the physics analy-
sis discussed in Sect. 4.1. Even though a single TFaaS
server may not be as eﬃcient as an integrated solution,
it can be easily horizontally scaled, e.g. using Kuber-
netes or other cluster orchestrated solutions, and may
provide the desired throughput for concurrent clients.
It also decouples the application layer/framework from
the inference phase which can be easily integrated into
any existing infrastructure by using the HTTP protocol
to TFaaS server for inference results. We foresee that
it can be useful in a variety of use-cases such as quick
evaluation of ML models in physics analysis, or online
applications where new models can be built periodi-
cally. The TFaaS implementation allows to use itself as
a repository of ML pre-trained models, and it can be a
valuable component in the agile ML development cycle
of any group, from small physics analysis group(s) to
cross-experiment collaborations.

5 Future directions

In the previous section, we discussed the usage of
MLaaS4HEP in the scope of a real HEP physics analy-
sis. We found the following:

– the usage of MLaaS4HEP is transparent to the cho-
sen HEP dataset, i.e. data can be read locally or
from remote storage;

– the discussed architecture is HEP experiment agnos-
tic and can be used with any existing ML (Python-
based) framework as well as easily integrated into
existing infrastructure;

– the data can be read in chunks from remote stor-
age, and this allows continuous ML training over
the large datasets, and further parallelization.
These observations open up a possibility to train ML
models over large datasets, potentially at Peta-Byte
scale, while using existing Python-based open-source
ML frameworks. Therefore, we foresee that the Machine
Learning as a Service approach can be widely applica-
ble in HEP. For example, future directions of this work
might include the exploitation of this architecture to
streamline the access to cloud and HPC resources for
training and inference tasks. It can represent an attrac-
tive option to open up HPC resources for large scale ML
training in HEP along with required security measure-
ments, resource provisioning, and remote data access
to WLCG sites. To move in this direction additional
work will be required. Below, we discuss a possible set
of improvements that can be explored.

5.1 Data Streaming Layer

To improve the Data Streaming Layer a multi-threaded
I/O layer can be implemented. This can be achieved by
wrapping up the data reader code-base into a service
that will deliver the data chunks in parallel upon re-
quests from the upstream layer. In addition, the chunks

75080085090095010001050110011501033501002005001000Event throughput (evts/s)Thousands events in a chunkrun in macOS with local filesrun in macOS with remote (BOLOGNA) filesrun in CERN VM with local filesrun in CERN VM with remote (BOLOGNA) filesrun in CERN VM with remote (BARI) filesrun in CERN VM with remote (PISA) files14

Valentin Kuznetsov et al.

can be pre-fetched from XrootD servers into a local
cache to improve the I/O throughput. In particular,
there are several R&D’s underways to demonstrate in-
telligence smart caching [36] for Dynamic On-Demand
Analysis Service (DODAS) at computer centers, such
as HPC, national Tier centers, etc. Such a DODAS fa-
cility can reduce the time spent on the Data Streaming
Layer by pre-fetching ROOT ﬁles into local cache and
use them for ML training.

5.2 Data Training Layer

If data I/O parallelism can be achieved, further im-
provements can be made via implementation of dis-
tributed training [37]. There are several R&D develop-
ments in this direction, from adapting the Dask Python
framework [38], to using MPI-Based Python framework
for distributed training [39], or using MLﬂow framework
[40] on an HDFS+Spark infrastructure, which explores
both task and data parallelism approaches.

The current landscape of ML frameworks is chang-
ing rapidly, and we should be adjusting MLaaS4HEP to
existing and future ML framework and innovations. For
instance, Open Network Exchange Format [41] opens
up the door to migration of models from one framework
into another. This may open up a possibility to use
MLaaS4HEP for the next generation of Open-Source
ML frameworks and ensure that end-users will not be
locked into a particular one. For instance, we are work-
ing on the automatic transformation of PyTorch [20]
and fast.ai [22] models into TensorFlow which later can
be uploaded and used through TFaaS service [12].

As discussed in Sect. 3.2 there are diﬀerent ap-
proaches to feed Jagged Arrays into ML framework
and R&D in this direction is in progress. For instance,
for AutoEncoder models, the vector representation with
padded values should always keep around a cast vector
which later can be used to decode back the vector rep-
resentation of the data back to Jagged Array or ROOT
TTree data-structures. We also would like to explore
matrix representation of Jagged Array data and see if
it can be applied to certain types of use-cases, e.g. in
calorimetry or tracking where image representation of
the objects can be used.

fast inference layer based on FPGAs and GPUs-based
infrastructures.

The current implementation of TFaaS can be used
as a repository of pre-trained models which can be eas-
ily shared across experiment boundaries or domains
thanks to serving ML models via HTTP protocol. For
instance, the current implementation of TFaaS allows
visual inspection of uploaded models, versioning, tag-
ging, etc. We foresee the next logical step is towards
a repository of pre-trained models with ﬂexible search
capabilities, extended model tagging, and versioning.
This can be achieved by providing a dedicated service
for ML models with proper meta-data description. For
instance, such meta-data can capture model parame-
ters, details of used software, releases, data input, and
performance output. With a proper search engine in
place, users may search for available ML models related
to their use-case.

5.4 MLaaS4HEP services

The proposed architecture allows us to develop and
deploy training and inference layers as independent
services. The separate resource providers can be used
and dynamically scaled if necessary, e.g. GPUs/TPUs
can be provisioned on-demand using the commercial
cloud(s) for training purposes of speciﬁc models, while
inference TFaaS service can reside elsewhere, e.g. on a
dedicated Kubernetes cluster at some computer center.
For instance, the continuous training of complex DL
models would be possible when data produced by the
experiment will be placed on WLCG sites. The train-
ing service will receive a set of notiﬁcations about newly
available data, and re-train speciﬁc model(s). When a
new ML model is ready it can be easily pushed to TFaaS
and be available for end-users immediately without any
intervention on the existing infrastructure as part of
CD/CI (Continuous Development and Continuous Inte-
gration) workﬂows. The TFaaS can be further adapted
to use FPGAs to speed up the inference phase. We fore-
see that such an approach may be more ﬂexible and
cost-eﬀective for HEP experiments in the HL-LHC era.
As such, we plan to perform additional R&D studies
in this direction and evaluate further MLaaS4HEP ser-
vices using available resources.

5.3 Data Inference Layer

6 Summary

On the inference side, several approaches can be used.
As discussed above, the TFaaS [12] throughput can be
further improved by switching from HTTP to a gRPC-
based solution such as SONIC [42] which can provide a

In this paper, we presented a modern approach to train
HEP ML models using the native ROOT data-format
either from local or remote storage. The MLaaS4HEP
consists of three layers: the Data Streaming and Data

MLaaS4HEP: Machine Learning as a Service for HEP

15

Training layers as part of the MLaaS4HEP framework
[11], and the Data Inference Layer implemented in the
TFaaS framework based on the TensorFlow library. All
three layers are implemented as independent compo-
nents. The Data Streaming Layer relies on the uproot
library for reading data from ROOT ﬁles (local or re-
mote) and yielding NumPy (Jagged) arrays. The Data
Training Layer transforms the input Jagged Array into
a vector representation and passes it into the ML frame-
work provided by the user. Finally, the Data Inference
Layer was implemented as an independent HTTP ser-
vice.

The ﬂexible architecture we implemented allows
performing ML training over a large set of distributed
HEP ROOT data without physically downloading data
into local storage. We demonstrated that such architec-
ture is capable of reading local and distributed datasets,
available via XrootD protocol on WLCG infrastructure.
We validate the MLaaS4HEP architecture using an of-
ﬁcial CMS t¯t Higgs analysis (t¯tH(bb)) in the boosted,
all-hadronic ﬁnal state, and we obtained comparable
ML model performance with respect to a traditional
physics analysis based on data extraction from ROOT
ﬁles into custom Ntuples.

Acknowledgements This work was done as a part of the
CMS experiment R&D program. We would like to thank Jim
Pivarski for his numerous and helpful discussions, and hard
work on uproot (and many other) packages which open up a
possibility to work on the MLaaS4HEP implementation. We
would like to thank Fabio Iemmi for the helpful discussions
we had on the aspects of the physics use-case.

7 Declarations

Funding

Not applicable

Conﬂict of interest

The authors declare that they have no conﬂict of inter-
est.

Availability of data and material

Code availability

The code is available at [11, 12] under MIT license.

References

1. Albertsson K et al (2018) Machine Learning in High En-
ergy Physics Community White Paper. arXiv:1807.02876
[physics.comp-ph]

2. A Living Review of Machine Learning for Particle Physics.

https://iml-wg.github.io/HEPML-LivingReview/

3. Comma Separated Values (CSV) data-format. https://

www.wikiwand.com/en/Comma-separated values

4. Scientiﬁc package to represent data as multi-dimensional

arrays. http://www.numpy.org

5. Hierarchical Data Format. https://www.wikiwand.com/

en/Hierarchical Data Format

6. A modular scientiﬁc toolkit used in HEP for analysis and

as a data-storage format. https://root.cern.ch

7. Higgs Boson Machine Learning Challenge used by
ATLAS experiment to identify Higgs boson. https://
www.kaggle.com/c/higgs-boson

8. High Energy Physics particle tracking in CERN detectors.
https://www.kaggle.com/c/trackml-particle-identiﬁcation
9. Flavours of Physics: Finding τ → µµµ. https://

www.kaggle.com/c/ﬂavours-of-physics/data

10. CMS Collaboration (2020) A deep neural network
to search for new long-lived particles decaying to jets.
Mach. Learn.: Sci. Technol. 1: 035012. DOI: 10.1088/2632-
2153/ab9023

11. Kuznetsov V, Giommi L (2018) MLaaS4HEP is set
for HEP. DOI: 10.5281/zen-

of MLaaS components
odo.1481785, https://github.com/vkuznet/MLaaS4HEP
12. Kuznetsov V (2018) TensorFlow as a Service. DOI:
http://github.com/vkuznet/

10.5281/zenodo.1308049,
TFaaS

13. Yao Y et al (2017) Complexity vs. performance: empirical
analysis of machine learning as a service. Proceedings of the
2017 Internet Measurement Conference, pp 384-397. DOI:
10.1145/3131365.3131372

14. CERN Openlab. https://home.cern/science/computing/

cern-openlab

15. A package for machine learning inference in FPGAs.

https://hls-fpga-machine-learning.github.io/hls4ml

16. Services for Optimal Network Inference on Coprocessors.
https://github.com/hls-fpga-machine-learning/SonicCMS
17. Machine Learning Pipelines for High Energy Physics
Using Apache Spark with BigDL and Analytics Zoo. https:
//db-blog.web.cern.ch/blog/luca-canali/machine-learning-
pipelines-high-energy-physics-using-apache-spark-bigdl

18. DIANA-HEP Scikit-hep uproot

library. Minimalist
ROOT I/O in pure Python and Numpy. https://
github.com/scikit-hep/uproot

19. A high performance, scalable fault tolerant access to data

repositories of many kinds. http://xrootd.org
20. PyTorch AI library. https://www.pytorch.org
21. Tensor Flow AI library. http://www.tensorﬂow.org
22. Fast AI library. https://www.fast.ai
23. Kuznetsov V (2018) Machine Learning as a Service for

HEP. arXiv:1811.04492 [hep-ex]

The details about datasets used in current study are
available from the corresponding author on reasonable
request. The availability of CMS dataset itself is a sub-
ject of CMS policy.

24. An umbrella organization for bringing state-of-the art for

HEP experiments. http://diana-hep.org

25. G´eron A (2019) Hands-On Machine Learning with
ISBN:

Scikit-Learn, Keras, and TensorFlow. O’Reilly,
9781492032632

16

Valentin Kuznetsov et al.

26. DNN/TensorFlow interface

for CMSSW.

https:

//github.com/mharrend/CMSSW-DNN

27. ATLAS Lightweight Trained Neural Network. DOI:
10.5281/zenodo.4299114, https://github.com/lwtnn/lwtnn

28. Go programming language. http://www.golang.org
29. CMS Collaboration (2016) Search for ttH production in
s = 13 TeV pp collisions

the H → bb decay channel with
at the CMS experiment. CMS PAS HIG-16-004

√

30. Iemmi F (2020) ttH associated production in the all-jets
ﬁnal state with the CMS experiment. Nuovo Cim. C 43, no.
2-3, 77. DOI: 10.1393/ncc/i2020-20077-4

31. Bartosik N, Castro A, Iemmi F, Kousouris K, Paspalaki
G, Tsiploitis Y (2020) ttH(bb) in boosted hadronic ﬁnal
states. Internal CMS note AN-19-053

32. ATLAS Collaboration (2012) Observation of a New Par-
ticle in the Search for the Standard Model Higgs Boson with
the ATLAS Detector at the LHC. Physics Letters B 716.1:
1–29. DOI: 10.1016/j.physletb.2012.08.020

33. CMS Collaboration (2012) Observation of a New Bo-
son at a Mass of 125 GeV with the CMS Experi-
ment at the LHC. Physics Letters B 716.1: 30–61. DOI:
10.1016/j.physletb.2012.08.021

34. Hoecker A, Speckmayer P, Stelzer J, Therhaag J, von
Toerne E, Voss H (2007) TMVA: Toolkit for Multivariate
Data Analysis. arXiv:physics/0703039 [physics.data-an]
35. ProtoBuﬀer library. https://github.com/protocolbuﬀers/

protobuf

36. Tracolli M et al (2020) Using DODAS as deployment
manager for smart caching of CMS data management sys-
tem. J. Phys.: Conf. Ser. 1525: 012057. DOI: 10.1088/1742-
6596/1525/1/012057

37. Ben-Nun T, Hoeﬂer T (2018) Demystifying Parallel
and Distributed Deep Learning: An In-Depth Concurrency
Analysis. arXiv:1802.09941 [cs.LG]

38. Scalable Analytics

framework in Python. https://

dask.org

39. Anderson D et al (2017) An MPI-Based Python Frame-
work for Distributed Training with Keras. arXiv:1712.05878
[cs.DC]

40. An open source platform for the machine learning life

cycle. https://www.mlﬂow.org

41. Open Neural Network Exchange

format. http://

www.onnx.ai

42. Duarte J et al (2019) FPGA-accelerated machine learning
inference as a service for particle physics computing. Com-
put Softw Big Sci 3, 13. DOI: 10.1007/s41781-019-0027-2

