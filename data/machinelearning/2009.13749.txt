Distributed Online Linear Quadratic Control for
Linear Time-invariant Systems

Ting-Jui Chang and Shahin Shahrampour, Senior Member, IEEE

0
2
0
2

p
e
S
9
2

]

C
O
.
h
t
a
m

[

1
v
9
4
7
3
1
.
9
0
0
2
:
v
i
X
r
a

Abstract— Classical

linear quadratic (LQ) control centers
around linear time-invariant (LTI) systems, where the control-
state pairs introduce a quadratic cost with time-invariant
parameters. Recent advancement in online optimization and
control has provided novel tools to study LQ problems that are
robust to time-varying cost parameters. Inspired by this line
of research, we study the distributed online LQ problem for
identical LTI systems. Consider a multi-agent network where
each agent is modeled as an LTI system. The LTI systems
are associated with decoupled, time-varying quadratic costs
that are revealed sequentially. The goal of the network is to
make the control sequence of all agents competitive to that
of the best centralized policy in hindsight, captured by the
notion of regret. We develop a distributed variant of the online
LQ algorithm, which runs distributed online gradient descent
with a projection to a semi-deﬁnite programming (SDP) to
generate controllers. We establish a regret bound scaling as
the square root of the ﬁnite time-horizon, implying that agents
reach consensus as time grows. We further provide numerical
experiments verifying our theoretical result.

I. INTRODUCTION

In recent years, there has been a signiﬁcant interest on
problems arising at the interface of control and machine
learning. Modern statistical and optimization algorithms have
opened new avenues to rethink classical control problems,
where linear quadratic (LQ) control ( [1]–[3]) is a prominent
point
in case. In its classical form, LQ control centers
around LTI systems, where the control-state pairs introduce
a quadratic cost with time-invariant parameters. For the
inﬁnite-horizon problem, the optimal controller has a closed-
form solution, and it can be derived by solving the algebraic
Riccati equation.

Fueled by applications in practical control problems, on-
line LQ control has received a great deal of attention [4].
In this scenario, the environment is subject to unpredictable
dynamics, making the cost functions time-varying in an
arbitrary fashion. Examples include variable-supply elec-
tricity production and building climate control with time-
varying energy costs. Motivated by the online optimization
literature, online LQ casts the time-varying problem as a
regret minimization, where the performance of the online
algorithm is compared with that of the best ﬁxed control
policy in hindsight.

In this paper, we address the distributed online LQ prob-
lem for a network of identical LTI systems. Each system is
modeled as an agent in a multi-agent network, associated

T.J. Chang

S.
’64 Department

and

Shahrampour
Industrial
of

Barnes
Texas A&M University, College
email:{tingjui.chang,shahin}@tamu.edu.

Station,

are with Wm Michael
and Systems Engineering,
TX 77843, USA.

with a decoupled,
time-varying quadratic cost. The cost
sequence for each agent can be chosen in an adversarial
fashion and the agent observes the sequence on-the-ﬂy. The
goal of the network is to make the control sequence of
all agents competitive to that of the best centralized policy
in hindsight, captured by the notion of regret. We develop
a distributed variant of the online LQ algorithm. At each
iteration, agents run distributed online gradient descent [5]
to maintain an ideal steady-state covariance matrix. To do
so, they need to perform a projection to an SDP and extract
a feasible policy to generate the controllers. We prove that
the individual regret can be bounded by O(
T ), where T
is the total number of iterations. This implies that the agents
reach consensus and collectively compete with the best ﬁxed
controller in hindsight. We ﬁnally provide simulation results
verifying this theoretical property.

√

A. Related Work

Distributed LQ Control: Distributed linear quadratic regu-
lator (LQR) has been widely studied in the control literature.
Some works focus on multi-agent systems with known,
identical decoupled dynamics. In [6], a distributed control
design is proposed by solving a single LQR problem whose
size matches the maximum vertex degree of the underlying
graph plus one. The authors of [7] derive the necessary
condition for the optimal distributed controller, resulting in a
non-convex optimization problem. The work of [8] addresses
a multi-agent network, where the dynamics of each agent
is a single integrator. The authors of [8] show that
the
computation of the optimal controller requires the knowledge
of the graph and the initial information of all agents. Given
the difﬁculty of precisely solving the optimal distributed
controller, Jiao et al. [9] provide the sufﬁcient conditions
to obtain sub-optimal controllers. All of the aforementioned
works need global information such as network topology to
compute the controllers. On the other hand, Jiao et al. [10]
propose a decentralized way to compute the controllers and
show that the system will reach consensus. For the case of
unknown dynamics, Alemzadeh et al. [11] propose a dis-
tributed Q-learning algorithm for dynamically decoupled sys-
tems. There are other works focusing on distributed control
without assuming identical decoupled sub-systems. Fattahi et
al. [12] study distributed controllers for unknown and sparse
LTI systems. Furieri et al. [13] address model-free methods
for distributed LQ problems and provide sample-complexity
bounds for problems with local gradient dominance property
(e.g., quadratically-invariant problems). The work of [14]
investigates the convergence of distributed controllers to a

 
 
 
 
 
 
global minimum for quadratically invariant problems with
ﬁrst-order methods.

Classical LQ with Unknown Dynamics: There is a recent
line of research dealing with LQ control problems with
unknown dynamics. Several techniques are proposed using
(i) gradient estimation (see e.g., [15]–[18]) (ii) the estimation
of dynamics matrices and derivation of the controller by
considering the estimation uncertainty [19], and (iii) wave-
ﬁltering [20], [21].

Online LQ Control: Recently, there has been a signiﬁcant
interest
in studying linear dynamical systems with time-
varying cost functions, where online learning techniques are
applied. This literature investigates two scenarios:

√

1) Known Systems: As mentioned before, Cohen et al.
[4] study the SDP relaxation for online LQ control
T ) for known LTI
and establish a regret bound of O(
systems with time-varying quadratic costs. Agarwal et
al. [22] propose the disturbance-action policy parame-
terization and reduce the online control problem to on-
line convex optimization with memory. They show that
for adversarial disturbances and arbitrary time-varying
T ). Agarwal et
convex functions, the regret is O(
al. [23] consider the case of time-varying strongly-
convex functions and improve the regret bound to
O(poly(logT )). Simchowitz et al. [24] further extend
the O(poly(logT )) regret bound to partially observable
systems with semi-adversarial disturbances.

√

2) Unknown Systems: For fully observable systems,
Hazan et al. [25] derive the regret of O(T 2/3) for
time-varying convex functions with adversarial noises.
the work of [24]
For partially observable systems,
addresses the cases of (i) convex functions with ad-
versarial noises and (ii) strongly-convex functions with
semi-adversarial noises, and provide regret bounds of
O(T 2/3) and O(
T ), respectively. Lale et al. [26] es-
tablish an O(poly(logT )) regret bound for the case of
stochastic perturbations, time-varying strongly-convex
functions, and partially observed states.

√

Our work lies precisely at the interface of distributed LQR

and online LQ, addressing distributed online LQ.

II. PROBLEM FORMULATION

A. Notation

We use the following notation in this work:

[n]
Tr(·)
(cid:107)·(cid:107)
E[·]
[A]ij

The set of {1, 2, . . . , n} for any integer n
The trace operator
Euclidean (spectral) norm of a vector (matrix)
The expectation operator
The entry in the i-th row and j-th column of A
Tr(A(cid:62)B)

A • B
A (cid:23) B (A − B) is positive semi-deﬁnite

where xi,t ∈ Rd and ui,t ∈ Rk represent agent i’s state and
control (or action) at time t, respectively. Furthermore, A ∈
Rd×d, B ∈ Rd×k, and wi,t is a Gaussian noise with zero
mean and covariance W (cid:31) 0. The noise sequence {wi,t} is
independent over time and agents.

Departing from the classical LQ control, we consider the
online distributed LQ problem in this work. At round t, agent
i receives the state xi,t and applies the action ui,t. Then,
positive deﬁnite cost matrices Qi,t and Ri,t are revealed,
and the agent incurs the cost x(cid:62)
i,tRi,tui,t.
Throughout this paper, we assume that Tr(Qi,t), Tr(Ri,t) ≤
C for all i, t and some C > 0. Agent i follows a policy
that selects the control ui,t based on the observed cost
matrices Qi,1, . . . , Qi,t−1 and Ri,1, . . . , Ri,t−1, as well as
the information received from the agents in its neighborhood.

i,tQi,txi,t + u(cid:62)

Centralized Benchmark: In order to gauge the performance
of any distributed LQ algorithm, we require a centralized
benchmark. In this work, we focus on the ﬁnite-horizon
problem, where for a centralized policy π, the cost after T
steps is given as

JT (π) = E

(cid:34) T

(cid:88)

t=1

xπ
t

(cid:62)Qtxπ

t + uπ
t

(cid:62)Rtuπ
t

,

(1)

(cid:35)

where Qt = (cid:80)m
i=1 Qi,t and Rt = (cid:80)m
i=1 Ri,t, and the
expectation is over the possible randomness of the policy
as well as the noise. The superscript π in uπ
t and xπ
t
alludes that the state-control pairs are chosen by the policy
π, given full access to cost matrices of all agents. Notice
in the inﬁnite-horizon version of the problem with
that
time-invariant cost matrices (Q, R), where the goal is to
minimize limT →∞ JT (π)/T , it
is well-known that for a
controllable LTI system (A, B), the optimal policy is given
by the constant linear feedback, i.e., uπ
t for a matrix
K ∈ Rk×d.

t = Kxπ

Regret Deﬁnition: The goal of a distributed LQ algorithm A
is to mimic the performance of an ideal centralized algorithm
using only local information. More formally, each agent
j locally generates the control sequence {uj,t}T
t=1, that is
competitive to the best policy among a benchmark policy
class Π. This can be formulated as minimizing the individual
regret, which is deﬁned as follows

Regretj

T (A) = J j

T (A) − min
π∈Π

JT (π),

(2)

for agent j ∈ [m], where

(cid:34) T

(cid:88)

m
(cid:88)

J j
T (A) = E

(cid:62)

xA
j,t

Qi,txA

j,t + uA
j,t

(cid:62)

Ri,tuA
j,t

(cid:35)

t=1
(cid:34) T

(cid:88)

t=1

= E

i=1

(cid:62)

xA
j,t

QtxA

j,t + uA
j,t

(cid:62)

(cid:35)

RtuA
j,t

.

B. Distributed Online LQ Control

We consider a multi-agent network of m identical LTI

systems. The dynamics of agent i is given as,

xi,t+1 = Axi,t + Bui,t + wi,t,

i ∈ [m]

A successful distributed algorithm is one that keeps the regret
sublinear with respect to T . Of course, this also depends
on the choice of the benchmark policy class Π, which is
assumed to be the set of strongly stable policies (to be
deﬁned precisely in Section II-C).

Network Structure: The agents communicate locally to
minimize the cost. The network topology is deﬁned by a
time-invariant doubly stochastic matrix P, where [P]ji > 0
if agent j communicates with agent i; otherwise [P]ji = 0.
The network is assumed to be connected, and there exists a
geometric mixing bound for P [27], such that

(cid:12)[Pk]ji − 1/m(cid:12)
(cid:12)

(cid:12) ≤

√

mβk, i ∈ [m],

m
(cid:88)

j=1

where β is the second largest singular value of P.

C. Strong Stability and Sequential Strong Stability

We consider the set of strongly stable linear (i.e., u = Kx)
controllers as the benchmark policy class. Following [4], we
deﬁne the notion of strong stability as follows.

(cid:13) ≤ κ.

(cid:13)H−1(cid:13)

Deﬁnition 1: (Strong Stability) A linear policy K is
(κ, γ)-strongly stable (for κ > 0 and 0 < γ ≤ 1) for the
LTI system (A, B), if (cid:107)K(cid:107) ≤ κ, and there exist matrices L
and H such that A + BK = HLH−1, with (cid:107)L(cid:107) ≤ 1 − γ
and (cid:107)H(cid:107) (cid:13)
Intuitively, a strongly stable policy ensures fast mixing and
exponential convergence to a steady-state distribution. In
particular, for the LTI system xt+1 = Axt + But + wt,
if a (κ, γ)-strongly stable policy K is applied (ut = Kxt),
(cid:98)Xt (the state covariance matrix of xt) converges to X (the
steady-state covariance matrix) with the following exponen-
tial rate

(cid:13)
(cid:13)
(cid:13) (cid:98)Xt − X

(cid:13)
(cid:13)

(cid:13) ≤ κ2e−2γt (cid:13)

(cid:13)
(cid:13) (cid:98)X0 − X

(cid:13)
(cid:13)
(cid:13) .

exists. For ν > 0, the SDP relaxation is formulated as [4]

minimize J(Σ) =

(cid:19)

(cid:18)Q 0
0 R

• Σ

subject to Σxx = (A B)Σ(A B)(cid:62) + W,
Σ (cid:23) 0, Tr(Σ) ≤ ν,

(3)

where Σ =

(cid:18)Σxx Σxu
Σux Σuu

(cid:19)

. Recall that in the online LQ

problem, we deal with time-varying cost matrices (Qt, Rt),
and for any t ∈ [T ], the above SDP yields different solutions.
In fact, for any feasible solution Σ of the above SDP,
a strongly stable controller K = Σ(cid:62)
xx can be extracted.
The steady-state covariance matrix induced by this controller
is also feasible for the SDP and its cost is at most that of Σ
(see Theorem 4.2 in [4]).

xuΣ−1

Moreover, for any (slowly-varying) sequence of feasible
solutions to the above SDP, the induced controller sequence
is sequentially strongly-stable. This implies that the covari-
ance matrix of the state converges to the steady-state in a
rapid sense as the following.

√

Lemma 1: (Lemma 4.4 in [4]) Assume that W (cid:23) σ2I and
ν/σ. Let {Σt} be a sequence of feasible solutions
let κ =
of the SDP, and suppose that (cid:107)Σt+1 − Σt(cid:107) ≤ η for all t
and for some η ≤ σ2/κ2. Then, the control matrix Kt =
(Σt)(cid:62)

2κ2 )-strongly stable for all t.

xx is (κ, 1

xu(Σt)−1

Furthermore, it can be shown that given the sequence
Xt = (Σt)xx, if we follow the policy sequence πt(x) =
(cid:1), the
Ktx + vt where vt ∼ N (cid:0)0, (Σt)uu − Kt(Σt)xxK(cid:62)
following relationship holds:

t

See Lemma 3.2 in [4] for details. The sequential nature of
online LQ control requires another notion of strong stability,
called sequential strong stability [4], deﬁned as follows.

Deﬁnition 2: (Sequential Strong Stability) A sequence of
t=1 is (κ, γ)-strongly stable if there
t=1 and {Lt}T
t=1 such that A + BKt =

linear policies {Kt}T
exist matrices {Ht}T
HtLtH−1

for all t with the following properties:

t

1) (cid:107)Lt(cid:107) ≤ 1 − γ and (cid:107)Kt(cid:107) ≤ κ.
2) (cid:107)Ht(cid:107) ≤ β and (cid:13)
3) (cid:13)
t+1Ht

(cid:13)
(cid:13) ≤ 1 + γ/2.

(cid:13)H−1
t

(cid:13)H−1

(cid:13)
(cid:13) ≤ 1/α with κ = β/α.

Sequential strong stability generalizes strong stability to
the time-varying scenario. On the technical level, it helps
with characterizing the convergence of the state covariance
matrices when a sequence of policies {Kt}T
t=1 is used
instead of a ﬁxed policy K, which will be the case in this
work.

D. SDP Relaxation for LQ Control

For the following dynamical system

xt+1 = Axt + But + wt, wt ∼ N (0, W),

the inﬁnite-horizon version of (1), i.e.,

minimize lim
T →∞

JT (π)/T,

with ﬁxed cost matrices Q and R can be relaxed via a
semi-deﬁnite programming when the steady-state distribution

(cid:13)
(cid:13)
(cid:13) (cid:98)Xt+1 − Xt+1

(cid:13)
(cid:13) + 4ηκ4,
(cid:13)
where (cid:98)Xt is the state covariance matrix on round t [4].

2κ2 )t (cid:13)
(cid:13)
(cid:13) (cid:98)X1 − X1

(cid:13)
(cid:13) ≤ κ2e−( 1
(cid:13)

III. ALGORITHM AND THEORETICAL RESULTS

We now lay out the distributed online LQ algorithm and

provide its theoretical regret bound.

A. Algorithm

In the distributed online LQ, each agent i at

time t
maintains an ideal steady-state covariance matrix Σi,t by
running a distributed online gradient descent on the SDP
(3). Then, a control matrix Ki,t is extracted from Σi,t and
is used to determine the action. In particular, the action ui,t
is sampled from a Gaussian distribution N (Ki,txi,t, Vi,t),
which entails E[ui,t|Ft] = Ki,txi,t, where Ft is the smallest
σ-ﬁeld containing the information about all agents up to
time t. This stochastic policy ensures the fast convergence
of the covariance matrix of xi,t and ui,t to the iterate Σi,t
generated by the algorithm. The proposed method is outlined
in Algorithm 1.

B. Theoretical Result: Regret Bound

In this section, we present our main theoretical result. By
applying algorithm 1, we show that for a multi-agent network
of identical LTI systems (with a connected communication
graph), the individual regret of an arbitrary agent is upper-
T ), which implies that the performance of
bounded by O(

√

Algorithm 1 Online Distributed LQ Control

1: Require: number of agents m, doubly stochastic matrix
P ∈ Rm×m, parameter ν, step size η, system matrices
(A, B).

for i = 1, 2, . . . , m do

2: Initialize: Σi,1 is identically initialized with a feasible
point and xi,1 is drawn from normal distribution with
mean zero for i ∈ [m].
3: for t = 1, 2, . . . , T do
4:
5:
6:

Receive xi,t
Compute Ki,t = (Σi,t)ux(Σi,t)−1
(Σi,t)uu − Ki,t(Σi,t)xxK(cid:62)
i,t
Predict ui,t ∼ N (Ki,txi,t, Vi,t) and Observe
Qi,t, Ri,t
Communicate Σi,t with agents in the neighborhood
and obtain their parameters

xx , Vi,t =

Σi,t+1 = ΠS

j PjiΣj,t − η

(cid:20)

(cid:80)

(cid:18)Qi,t

0

(cid:19)(cid:21)

,

0 Ri,t

7:

8:

9:

Σ ∈ R(d+k)×(d+k)

(cid:12)
Σ (cid:23) 0, Tr(Σ) ≤ ν,
(cid:12)
(cid:12)
Σxx = (A B)Σ(A B)(cid:62) + W
(cid:12)

(cid:27)

where
(cid:26)

S =

end for

10:
11: end for

all agents would converge to that of the best ﬁxed controller
in hindsight for large enough T .

Theorem 2: Assume that

the network is connected,
Tr(W) ≤ λ2 and W (cid:23) σ2I. Given κ > 1 and 0 ≤ γ < 1,
√
set ν = 2κ4λ2/γ and η = 1/

ρT , where

(cid:34)
4mC 2

(cid:18)

3 +

ρ =

(cid:19)

√
4
m
1 − β

+ mC(1 +

√

16
2mCν
(1 − β)σ2

(cid:35)

.

ν
σ2 )

By running Algorithm 1, the expected individual regret of
agent j with respect to any (κ, γ)-strongly stable controller
Ks is bounded as follows

for T ≥

(cid:16) 4

√

2νC

σ4(1−β)ρ1/2

(cid:17)2

.

The dependence of regret bound to the spectral gap 1 − β
is perhaps not surprising, as it has been previously observed
in distributed online algorithms (see e.g., [28] Corollary 4).

IV. NUMERICAL EXPERIMENTS

We now provide numerical simulations verifying the the-

oretical guarantee of our algorithm.

Regretj

T (A) = J j

T (A) − JT (Ks) = O

(cid:16)

(1 − β)−0.5

√

(cid:17)

,

T

P =

Fig. 1: The plot of individual regret of agent 1 vs. time shows the
sub-linearity of the regret evolvement.

√

Fig. 2: This plot shows that the individual regret of agent 1 is of
O(

T ) when T is large enough.

The resulting communication matrix is as follows









0.6
0.2
0
0
0.2

0.2
0.6
0.2
0
0

0
0.2
0.6
0.2
0

0
0
0.2
0.6
0.2









0.2
0
0
0.2
0.6

.

The other hyper-parameters are set as follows: κ = 1.5,
γ = 0.4, C = 30. We let matrices A = (1 − 2γ)I and
B = (γ/κ)I. We set the cost matrix Qi,t (respectively, Ri,t)
as a diagonal matrix with each diagonal entry sampled from
the uniform distribution over [0, C/d] (respectively, [0, C/k])
to ensure that Tr(Qi,t), Tr(Ri,t) ≤ C. The noise wi,t is
sampled from a standard Gaussian distribution, and thus
λ2 = d = 3 and σ2 = 1.

Experiment Setup: We consider a distributed network of
ﬁve agents where d = k = 3. The network topology is a
cycle, where each agent has a self-weight of 0.6, and the rest
of the weight is evenly distributed between its neighborhood.

Simulation: The total iteration number T is set as 30 times
of the theoretical lower bound in Theorem 2 in order to better
see the performance. We let Ks = (1e − 2)(−κ)I which is
(κ, γ)-strongly stable with A, B, and leads to a small enough

024681012Time (iteration)104-10123456Individual Regret of Agent 1106024681012Time (iteration)104-2000020004000600080001000012000140001600018000Individual Regret of Agent 1/Square Root of TimeAPPENDIX

Proof of Theorem 2: Recalling the deﬁnition of regret
(2), for a ﬁxed arbitrary (κ, γ)-strongly stable controller Ks
and agent j, the regret is expressed as the following:

J j
T (A) − JT (Ks)
(cid:34) T

(cid:88)

m
(cid:88)

=E

−E

t=1
(cid:34) T

(cid:88)

t=1

(x(cid:62)

j,tQi,txj,t + u(cid:62)

i=1

(xs(cid:62)

t Qtxs

t + us(cid:62)

(cid:35)
t Rtus
t )

,

(cid:35)
j,tRi,tuj,t)

(4)

Fig. 3: The averaged regrets over time for all agents converge as
time grows.

Let us denote

where us

t = Ksxs

t for all t.

cumulative cost to be the benchmark. Noting that we apply
Dykstra’s projection algorithm for the projection step, the
matrix Vi,t for action-sampling may not be positive semi-
deﬁnite (PSD) due to ﬂoating-point computations, so we do
some tuning by adding to it a small term ((1e − 25)I) to
keep it PSD. The parameters Σi,1 are identically initialized
and the initial states of all agents are sampled from normal
distribution. The entire process is repeated for 30 Monte-
Carlo simulations.

Performance: To see the sub-linearity of the individual
regret established in Theorem 2, we plot the regret of agent
1 over time. Fig. 1 shows that the regret is upper-bounded
by a linear function. To better capture this, we also plot the
regret normalized by the root-square of time in Fig. 2. we
observe that for large enough T , the slope of the plot is non-
positive, which veriﬁes that the regret is upper-bounded by
O(
T ). We also present the performance of all agents to
verify that they reach consensus asymptotically. In Fig. 3, it
can be seen that the time-averaged regrets for all agents are
decreasing and converging together.

√

V. CONCLUSION

In this paper, we considered the distributed online LQ
problem with known identical LTI systems and decoupled,
time-varying quadratic cost functions. We developed a fully
distributed algorithm to minimize the ﬁnite-horizon cost,
which can be recast as a regret minimization. We proved
that the individual regret, which is the performance of the
control sequence of any agent compared to the best (linear
and strongly stable) controller in hindsight, is upper bounded
T ). Our numerical simulations veriﬁed that the theo-
by O(
retical bound is indeed valid. For future works, it is important
to analyze the distributed online LQ under more general
settings. Possible directions include extending the setup to
unknown dynamics or assuming coupled time-varying cost
functions.

√

Li,t =

(cid:18)Qi,t

(cid:19)

0

0 Ri,t

and

Lt =

(cid:18)Qt

0
0 Rt

(cid:19)

,

where Lt = (cid:80)m

i=1 Li,t. Also let,

(cid:98)Σj,t = E (cid:2)[x(cid:62)
t = E (cid:2)[xs(cid:62)
(cid:98)Σs

j,t u(cid:62)
t us(cid:62)
t

j,t](cid:62)[x(cid:62)
](cid:62)[xs(cid:62)

j,t](cid:3)
j,t u(cid:62)
t us(cid:62)
t

](cid:3) .

We can then write (4) as

Li,t • (cid:98)Σj,t −

T
(cid:88)

t=1

Lt • (cid:98)Σs
t

Li,t • ((cid:98)Σj,t − Σj,t)

T
(cid:88)

m
(cid:88)

t=1
T
(cid:88)

i=1
m
(cid:88)

t=1
T
(cid:88)

i=1
m
(cid:88)

=

+

+

Li,t • Σj,t −

i=1

Lt • (Σs − (cid:98)Σs

t ),

t=1
T
(cid:88)

t=1

(5)

T
(cid:88)

t=1

Lt • Σs

where Σs is the steady-state covariance matrix induced by
Ks, and Σj,t is generated by Algorithm 1. Now, we show
how each term in (5) is bounded.

t=1

(cid:80)m

t=1 Lt • Σs:

xx) + Tr(Σs

i=1 Li,t • Σj,t − (cid:80)T

(I) For the term (cid:80)T
it can be shown that
Based on Lemma 3.3 in [4],
uu) ≤ 2κ4λ2/γ = ν. Then,
Tr(Σs) = Tr(Σs
by Lemma 4.1 in [4], Σs is a feasible solution to the
SDP (3). Based on the deﬁnition of the feasible set S,
the diameter supΣ,Σ(cid:48)∈S (cid:107)Σ − Σ(cid:48)(cid:107)F ≤ 2 supΣ∈S (cid:107)Σ(cid:107)F =
(cid:112)Tr(Σ)2 ≤ 2ν. And
(cid:112)Tr(Σ2) ≤ 2 supΣ∈S
2 supΣ∈S
loss function
the norm of
the linear
the gradient of
√
2C since
S → Li,t • S is upper bounded by
(cid:113)

√

Tr(Q(cid:62)

i,tQi,t) + Tr(R(cid:62)
Let Σ∗ = argminΣ∈S

i,tRi,t) ≤
(cid:80)T

2C.

t=1 Lt • Σ. Based on the regret

00.20.40.60.811.21.41.61.82Time (iteration)1048283848586878889909192Averaged Regret over Timebound of distributed online gradient descent [5], we have

Substituting (9) into (8) and summing over t ∈ [T ], we get

T
(cid:88)

m
(cid:88)

t=1
T
(cid:88)

i=1
m
(cid:88)

Li,t • Σj,t −

Li,t • Σj,t −

i=1
(cid:18)

+

3 +

t=1
mν
η

(cid:19)

√

m
4
1 − β

T
(cid:88)

t=1
T
(cid:88)

t=1

Lt • Σs

Lt • Σ∗

4mC 2ηT,

≤

≤

T
(cid:88)

t=1

Li,t • ((cid:98)Σj,t − Σj,t)

(6)

≤C(1 +

(cid:32)

ν
σ2
(cid:18) 16

(cid:13)
(cid:13)
(cid:13)((cid:98)Σj,1)xx − (Σj,1)xx
√
(cid:19)
2mCην
(1 − β)σ2 T

(cid:18) 2ν2
σ4

(cid:13)
(cid:13)
(cid:13)((cid:98)Σj,1)xx − (Σj,1)xx

(cid:13)
(cid:13)
(cid:13) +

ν
σ2 )
ν
σ2 )
ν
σ2 )

+C(1 +

≤C(1 +

(cid:33)

e− σ2

2ν (t−1)

(cid:13)
(cid:13)
(cid:13)

T
(cid:88)

t=1

√
2mCην
16
(1 − β)σ2 T

(cid:19)

,

(10)

where β ∈ [0, 1) is the second largest singular value
of P. Also, based on Lemma 3 in [5],
the variation
(cid:107)Σj,t+1 − Σj,t(cid:107)F is upper bounded as the following:

(cid:107)Σj,t+1 − Σj,t(cid:107)F ≤

.

(7)

√
4

2mCη
1 − β

(II) For the term (cid:80)T
Based on Algorithm 1, we have

(cid:80)m

t=1

i=1 Li,t • ((cid:98)Σj,t − Σj,t):

Σj,t =

=

+

(cid:18)(Σj,t)xx
(Σj,t)ux
(cid:18) (Σj,t)xx

(cid:19)

(Σj,t)xu
(Σj,t)uu

(Σj,t)xxK(cid:62)
j,t

Kj,t(Σj,t)xx Kj,t(Σj,t)xxK(cid:62)
j,t

(cid:18)0

(cid:19)

0

0 Vj,t

(cid:98)Σs

t =

(cid:19)

and

(cid:98)Σj,t
(cid:32)

=

((cid:98)Σj,t)xx

((cid:98)Σj,t)xxK(cid:62)
j,t

Kj,t((cid:98)Σj,t)xx Kj,t((cid:98)Σj,t)xxK(cid:62)
j,t

Therefore, we get

(cid:33)

+

(cid:18)0

0

0 Vj,t

(cid:19)

.

(cid:1)

(cid:1)K(cid:62)

Li,t • ((cid:98)Σj,t − Σj,t)
=Qi,t • (cid:0)((cid:98)Σj,t)xx − (Σj,t)xx
+Ri,t • Kj,t
=(Qi,t + K(cid:62)
≤Tr(Qi,t + K(cid:62)
j,tRi,tKj,t)
≤ (cid:2)Tr(Qi,t) + Tr(Ri,t) (cid:13)
(cid:13)
(cid:13)
(cid:13)((cid:98)Σj,t)xx − (Σj,t)xx

(cid:0)((cid:98)Σj,t)xx − (Σj,t)xx
j,tRi,tKj,t) • (cid:0)((cid:98)Σj,t)xx − (Σj,t)xx
(cid:1)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)((cid:98)Σj,t)xx − (Σj,t)xx
(cid:13)
(cid:3) (cid:13)
(cid:13)
(cid:13)
(cid:13)((cid:98)Σj,t)xx − (Σj,t)xx
(cid:13)
(cid:13)
(cid:13)
(cid:13) ,

(cid:13)Kj,tK(cid:62)
j,t

≤C(1 +

j,t

ν
σ2 )

(cid:13)
(cid:13)
(cid:13)

(8)

where the third inequality holds since Tr(Qi,t), Tr(Ri,t) ≤ C
2ν )-strongly stable based on Lemma 4.3
and Kj,t is (
in [4].

σ , σ2

√

ν

Choosing η such that
and Lemma 1, we have

√

4

2mCη
1−β

≤ σ4

ν , based on (7)

(cid:13)
(cid:13)
(cid:13)((cid:98)Σj,t)xx − (Σj,t)xx
2ν (t−1) (cid:13)
ν
σ2 e− σ2
(cid:13)
(cid:13)((cid:98)Σj,1)xx − (Σj,1)xx

(cid:13)
(cid:13)
(cid:13)

≤

(cid:80)T

where the second inequality comes from the fact
t=1 e−αt ≤ (cid:82) ∞

that
0 e−αtdt = 1/α for α > 0. Summing (10)

over i, the result is obtained.

(III) For the term (cid:80)T

By

denoting Σs =
(cid:32)

t=1 Lt • (Σs − (cid:98)Σs
(cid:18) Σs
xx
KsΣs

t ):

Σs

xxKs(cid:62)

xx KsΣs
(cid:33)

xxKs(cid:62)

((cid:98)Σs

t )xxKs(cid:62)

, we have

t )xx Ks((cid:98)Σs

t )xxKs(cid:62)

t )xx

((cid:98)Σs
Ks((cid:98)Σs

(cid:19)

and

Lt • (Σs − (cid:98)Σs
t )
m
(cid:88)

=

(Qi,t + KsRi,tKs(cid:62)) •

(cid:16)

Σs

xx − ((cid:98)Σs

t )xx

(cid:17)

i=1
m
(cid:88)

i=1

≤

Tr(Qi,t + KsRi,tKs(cid:62))

xx − ((cid:98)Σs

t )xx

(11)

(cid:13)
(cid:13)
(cid:13)

≤mC(1 + κ2)

xx − ((cid:98)Σs

t )xx

(cid:13)
(cid:13)Σs
(cid:13)

(cid:13)
(cid:13)Σs
(cid:13)
(cid:13)
(cid:13)
(cid:13) ,

where the second inequality comes from the fact
Tr(Qi,t), Tr(Ri,t) ≤ C and Ks is (κ, γ)-strongly stable.

that

Based on Lemma 3.2 in [4], we get
(cid:13)
(cid:13)

(cid:13) ≤ κ2e−2γ(t−1) (cid:13)

t )xx − Σs
xx

(cid:13)
(cid:13)((cid:98)Σs
(cid:13)

(cid:13)((cid:98)Σs
(cid:13)

1)xx − Σs
xx

(cid:13)
(cid:13)
(cid:13) .

(12)

Substituting (12) into (11) and summing over t, we have

T
(cid:88)

t=1

Lt • (Σs − (cid:98)Σs
t )

≤mC(1 + κ2)κ2 (cid:13)
(cid:13)((cid:98)Σs
(cid:13)

1)xx − Σs
xx

(cid:13)
(cid:13)
(cid:13)

T
(cid:88)

t=1

e−2γ(t−1)

(13)

≤

mC(κ2 + κ4)
2γ

(cid:13)
(cid:13)((cid:98)Σs
(cid:13)

1)xx − Σs
xx

(cid:13)
(cid:13)
(cid:13) .

Based on (6), (10) and (13), we have

+ mC(

J j
T (A) − JT (Ks)
mν
η
mC(κ2 + κ4)
2γ

(cid:13)
(cid:13)((cid:98)Σs
(cid:13)

2ν2(σ2 + ν)
σ6

≤

+

1)xx − Σs
xx

(cid:13)
(cid:13)
(cid:13) + ρηT,

(cid:13)
(cid:13)
(cid:13)((cid:98)Σj,1)xx − (Σj,1)xx

(cid:13)
(cid:13)
(cid:13)

)

(14)

(cid:13)
(cid:13)
(cid:13) +

√
16
2mCην
(1 − β)σ2 .

(9)

where

(cid:34)
4mC 2

(cid:18)

3 +

ρ (cid:44)

(cid:19)

√
4
m
1 − β

+ mC(1 +

√

16
2mCν
(1 − β)σ2

(cid:35)

.

ν
σ2 )

[23] N. Agarwal, E. Hazan, and K. Singh, “Logarithmic regret for online
control,” in Advances in Neural Information Processing Systems, 2019,
pp. 10 175–10 184.

[24] M. Simchowitz, K. Singh, and E. Hazan, “Improper learning for non-

stochastic control,” arXiv preprint arXiv:2001.09254, 2020.

[25] E. Hazan, S. Kakade, and K. Singh, “The nonstochastic control
problem,” in Algorithmic Learning Theory, 2020, pp. 408–421.
[26] S. Lale, K. Azizzadenesheli, B. Hassibi, and A. Anandkumar, “Loga-
rithmic regret bound in partially observable linear dynamical systems,”
arXiv preprint arXiv:2003.11227, 2020.

[27] J. S. Liu, Monte Carlo strategies in scientiﬁc computing.

Springer

Science & Business Media, 2008.

[28] S. Shahrampour and A. Jadbabaie, “Distributed online optimization in
dynamic environments using mirror descent,” IEEE Transactions on
Automatic Control, vol. 63, no. 3, pp. 714–725, 2018.

By setting η = 1/
√

√

ρT ,

ρT ). (Here it is assumed that T ≥

the upper bound in (14) is
√

(cid:16) 4

2mνC

σ4(1−β)ρ1/2

(cid:17)2

to

O(
make sure 4

√

2mCη

1−β ≤ σ4

ν and (9) holds.)

REFERENCES

[1] B. D. O. Anderson, J. B. Moore, and B. P. Molinari, “Linear optimal
control,” IEEE Transactions on Systems, Man, and Cybernetics, vol.
SMC-2, no. 4, pp. 559–559, 1972.

[2] D. P. Bertsekas, Dynamic programming and optimal control, vol. 1,

no. 2.

[3] K. Zhou, J. C. Doyle, K. Glover et al., Robust and optimal control.

Prentice hall New Jersey, 1996, vol. 40.

[4] A. Cohen, A. Hasidim, T. Koren, N. Lazic, Y. Mansour, and K. Talwar,
“Online linear quadratic control,” in International Conference on
Machine Learning, 2018, pp. 1029–1038.

[5] F. Yan, S. Sundaram, S. Vishwanathan, and Y. Qi, “Distributed
autonomous online learning: Regrets and intrinsic privacy-preserving
properties,” IEEE Transactions on Knowledge and Data Engineering,
vol. 25, no. 11, pp. 2483–2493, 2012.

[6] F. Borrelli and T. Keviczky, “Distributed lqr design for identical
dynamically decoupled systems,” IEEE Transactions on Automatic
Control, vol. 53, no. 8, pp. 1901–1912, 2008.

[7] A. Mosebach and J. Lunze, “Synchronization of autonomous agents
by an optimal networked controller,” in 2014 European Control
Conference (ECC), 2014, pp. 208–213.

[8] Y. Cao and W. Ren, “Optimal linear-consensus algorithms: An lqr
perspective,” IEEE Transactions on Systems, Man, and Cybernetics,
Part B (Cybernetics), vol. 40, no. 3, pp. 819–830, 2010.

[9] J. Jiao, H. L. Trentelman, and M. K. Camlibel, “A suboptimality
approach to distributed linear quadratic optimal control,” IEEE Trans-
actions on Automatic Control, vol. 65, no. 3, pp. 1218–1225, 2020.

[10] ——, “Distributed linear quadratic optimal control: Compute locally
and act globally,” IEEE Control Systems Letters, vol. 4, no. 1, pp.
67–72, 2020.

[11] S. Alemzadeh and M. Mesbahi, “Distributed q-learning for dynam-
ically decoupled systems,” in 2019 American Control Conference
(ACC).

IEEE, 2019, pp. 772–777.

[12] S. Fattahi, N. Matni, and S. Sojoudi, “Efﬁcient learning of distributed
linear-quadratic controllers,” arXiv preprint arXiv:1909.09895, 2019.
[13] L. Furieri, Y. Zheng, and M. Kamgarpour, “Learning the globally
optimal distributed lq regulator,” in Learning for Dynamics and
Control, 2020, pp. 287–297.

[14] L. Furieri and M. Kamgarpour, “First order methods for globally
optimal distributed controllers beyond quadratic invariance,” in 2020
American Control Conference (ACC).
IEEE, 2020, pp. 4588–4593.
[15] M. Fazel, R. Ge, S. Kakade, and M. Mesbahi, “Global convergence
of policy gradient methods for the linear quadratic regulator,” in
International Conference on Machine Learning, 2018, pp. 1467–1476.
[16] D. Malik, A. Pananjady, K. Bhatia, K. Khamaru, P. Bartlett, and
M. Wainwright, “Derivative-free methods for policy optimization:
Guarantees for linear quadratic systems,” in The 22nd International
Conference on Artiﬁcial Intelligence and Statistics.
PMLR, 2019,
pp. 2916–2925.

[17] H. Mohammadi, M. Soltanolkotabi, and M. R. Jovanovi, “On the linear
convergence of random search for discrete-time lqr,” IEEE Control
Systems Letters, vol. 5, no. 3, pp. 989–994, 2021.

[18] H. Mohammadi, M. Soltanolkotabi, and M. R. Jovanovic, “Random
search for learning the linear quadratic regulator,” in 2020 American
Control Conference (ACC), 2020, pp. 4798–4803.

[19] S. Dean, H. Mania, N. Matni, B. Recht, and S. Tu, “On the sample
complexity of the linear quadratic regulator,” Foundations of Compu-
tational Mathematics, pp. 1–47, 2019.

[20] E. Hazan, K. Singh, and C. Zhang, “Learning linear dynamical systems
via spectral ﬁltering,” in Advances in Neural Information Processing
Systems, 2017, pp. 6702–6712.

[21] S. Arora, E. Hazan, H. Lee, K. Singh, C. Zhang, and Y. Zhang,
“Towards provable control for unknown linear dynamical systems,”
2018.

[22] N. Agarwal, B. Bullins, E. Hazan, S. M. Kakade, and K. Singh,
“Online control with adversarial disturbances,” in 36th International
Conference on Machine Learning, ICML 2019.
International Machine
Learning Society (IMLS), 2019, pp. 154–165.

