2
2
0
2

r
a

M
9

]

G
L
.
s
c
[

1
v
5
2
8
4
0
.
3
0
2
2
:
v
i
X
r
a

Efﬁcient Sub-structured Knowledge Distillation

Wenye Lin1,2∗, Yangming Li1∗, Lemao Liu1, Shuming Shi1, Hai-tao Zheng2†
1Tencent AI Lab
2Tsinghua University
lwy20@mails.tsinghua.edu.cn,zheng.haitao@sz.tsinghua.edu.cn
{newmanli,redmondliu,shumingshi}@tencent.com

Abstract

Structured prediction models aim at solving a
type of problem where the output is a complex
structure, rather than a single variable. Per-
forming knowledge distillation for such mod-
els is not trivial due to their exponentially large
output space. In this work, we propose an ap-
proach that is much simpler in its formulation
and far more efﬁcient for training than exist-
ing approaches. Speciﬁcally, we transfer the
knowledge from a teacher model to its student
model by locally matching their predictions on
all sub-structures, instead of the whole output
space. In this manner, we avoid adopting some
time-consuming techniques like dynamic pro-
gramming (DP) for decoding output structures,
which permits parallel computation and makes
the training process even faster in practice. Be-
sides, it encourages the student model to bet-
ter mimic the internal behavior of the teacher
model. Experiments on two structured pre-
diction tasks demonstrate that our approach
outperforms previous methods and halves the
time cost for one training epoch.1

1

Introduction

Large-scale neural networks have achieved great
success in natural language processing (NLP) (Con-
neau et al., 2017; Devlin et al., 2019a; Lewis et al.,
2020). However, training and deploying these mod-
els are extremely computationally expensive. To
improve their efﬁciency for applications, knowl-
edge distillation is introduced by Buciluˇa et al.
(2006); Hinton et al. (2015) to transfer the knowl-
edge from a large teacher model to a small student
model, by letting the student softly mimic the pre-
dictions of its teacher.

Prior works (Sanh et al., 2019; Sun et al., 2020;
Li et al., 2021) have presented a number of effec-
tive knowledge distillation approaches. However,

∗Equal contributions.
†† Corresponding author.
1The source code for this work is publicly available at

https://github.com/Linwenye/Efﬁcient-KD.

these methods are inapplicable for structured pre-
diction models that capture the mutual dependen-
cies among output variables, because of their huge
output space. Part-of-speech (POS) tagging (Ratna-
parkhi et al., 1996) is a typical structured prediction
task where two adjacent output tags are strongly
correlated. For example, nouns usually follow ad-
jectives, but not vice versa. To incorporate such
correlations among tags, popular POS taggers such
as CRF (Lafferty et al., 2001) make predictions at
the sequence level, instead of over its single words.
Since the size of the output space exponentially in-
creases with the sequence length, directly applying
traditional knowledge distillation is computation-
ally infeasible.

A small stream of recent works called structured
knowledge distillation have studied this problem.
Kim and Rush (2016) approximate predictions of
the teacher model by generating K-best outputs as
pseudo labels. Wang et al. (2020) compute a pos-
terior marginal probability distribution for every
output variable. Wang et al. (2021) minimize the
factorized form of the knowledge distillation loss.
However, these approaches are very inefﬁcient for
training because their computations involve time-
consuming operations like K-best decoding or dy-
namic programming (DP).

In this work, we propose a highly efﬁcient ap-
proach for structured knowledge distillation. Our
idea is to locally match all sub-structured predic-
tions of the teacher model and those of its student
model, which avoids adopting time-consuming
techniques like DP to globally search for output
structures. Intuitively, similar predictions over the
sub-structures lead to an accurate approximation of
complete structures. Furthermore, mimicking the
sub-structured predictions allows the student model
to better imitate the internal behavior of the teacher
model (Ba and Caruana, 2014). For example, if a
label sequences [AA] is given score 10 and −10 by
the teacher model while its student predicts all 0

 
 
 
 
 
 
Figure 1: An example of our efﬁcient sub-structured knowledge distillation. KD is short for knowledge distillation.
Traditional KD matches output distributions directly. However, the output space of structured prediction models is
exponential in size, and thus infeasible to directly match output distributions. By matching sub-structured scores,
we efﬁciently transfer the structured knowledge from the teacher to the student.

for both of A, the student learns nothing from the
teacher if the globally structured knowledge dis-
tillation is adopted as the training objective since
they all sum up to 0. Performing sub-structured
knowledge distillation, however, enables the stu-
dent model to learn the detailed behavior of the
teacher. Besides, our simple formulation beneﬁts
in parallel matrix computation, which further re-
duces the training time.

We have performed experiments on two struc-
tured prediction tasks, named entity recognition
(NER) and syntactic chunking, showing that our
approach outperforms previous methods with far
less training time.

2 Methodology

In this section, we provide a mathematical formula-
tion of our approach. For better understanding, we
assume that the model adopts CRF for decoding,
which prevails in the literature of structured predic-
tion. Our approach is in fact applicable for almost
all structured prediction models.

2.1 CRF

CRF is widely used in structured prediction tasks.
For an input sequence x = {x1, x2, ..., xL} consist-
ing of L tokens, the corresponding ground truth is
y = {y1, y2, ..., yL}, yl ∈ {1, 2, ..., N }, where N
is the size of the label set. To get expressive seman-
tic representation for tokens, we adopt pre-trained

BERT (Devlin et al., 2019b):

h = {h1, h2, ..., hL} = BERT(x).

(1)

Then, we use a learnable matrix W to map token
representation into emission scores:

e = {e1, e2, ..., eL} = Wh,

(2)

where el is a N -dimensional vector. CRF considers
not only the emission scores but also the transition
scores tyl−1,yl, so the score function for every sub-
structure is

s (yl, yl−1, xl) = tyl−1,yl + el.

(3)

Given all sub-structure scores, the conditional prob-
ability p(y|x) takes the form:

p(y|x) =

1
Z(x)

L
(cid:89)

l=1

exp {s (yl, yl−1, xl)} ,

(4)

where Z(x) is a normalization term computed by
Viterbi algorithm.

2.2 Standard Knowledge Distillation

Knowledge distillation is a ubiquitous technique
for model compression and acceleration. It trains a
small student model to imitate the output distribu-
tion of a large teacher model via L2 distance on log-
its (Ba and Caruana, 2014) or cross-entropy (Hin-
ton et al., 2015):

LKD = −

(cid:88)

y∈Y (x)

Pt(y|x) log Ps(y|x),

(5)

where Pt and Ps are respectively the distributions
of the teacher model and the student model.

In practice, the overall training objective con-
tains the above distillation loss and a negative log
likelihood loss on the ground truth.

Lstudent = λLKD + (1 − λ)LNLL,

(6)

where λ is usually set as a dynamic weight that
starts from 1 to 0 during training.

2.3 Structured Knowledge Distillation

Conventional knowledge distillation, i.e., Eq. (5) is
intractable for structured prediction models, since
the output space Y (x) grows exponentially with
the sentence length L. To tackle this problem, Kim
and Rush (2016) utilize K-best sequences predicted
by the teacher model to approximate the teacher
distribution Pt:

pt(y|x) =

(cid:40) pt(y|x)

y∈T pt(y|x) y ∈ T
y /∈ T

(cid:80)

0

,

(7)

where T is the set of predicted sequences. Wang
et al. (2021) calculate the factorized form of the
Eq. (5) for CRF model as

LKD = −

(cid:88)

u∈U(x)

p(cid:48)(u|x)s(u, x)+log Z(x), (8)

where u denotes any possible two adjacent tags
{yl, yl−1} and p(cid:48) is a conditional probability com-
puted by the teacher model.

While these methods allow tractable knowledge
distillation, they introduce sequential calculations
with high time complexity and are not paralleliz-
able. To boost the efﬁciency, we propose to let
the student model learn from its teacher model
by locally matching their predictions on all sub-
structures. Speciﬁcally, we adopt L2 distance to
measure the sub-structure prediction difference be-
tween the two models:

LKD =

(cid:80)

u∈U(x) (cid:107)s (u, x) − s(cid:48) (u, x)(cid:107)2
|U(x)|

,

(9)

where s(cid:48) represents sub-structure scores calculated
by the teacher model and U denotes all possible
pairs of adjacent tags.

3 Experiments

3.1 Settings

We use CoNLL 2003 dataset (Sang and De Meul-
der, 2003) for experiments on NER and CoNLL
2000 dataset on text chunking. We follow standard
training/development/test splits.

Structured Prediction Model For all structured
prediction tasks, we choose the widely used BERT-
CRF model, in which pre-trained BERT (Devlin
et al., 2019b) fully exploits the knowledge from
massive unlabeled corpora and CRF captures the
interrelationship of predicted variables. For the
teacher model BERTBASE-CRF, we ﬁrst load the
pre-trained BERTBASE cased version that consists
of 12 Transformer Encoder layers and then ﬁnetune
it on target task along with the CRF layer. For the
student model BERT3L-CRF, since a pre-trained
BERT3L model is not directly available, we load
the ﬁrst three layers of pre-trained weights from
the BERTBASE model.

Training We conduct all experiments on one
NVIDIA Tesla V100 GPU device. We set the batch
size as 32 and initial learning rate as 5 × 10−5 for
the teacher model and 2 × 10−4 for the student
model. We utilize Adam (Kingma and Ba, 2014)
as the optimizer. To perform structured knowledge
distillation, we follow the following procedure: (1)
Load the BERTBASE-CRF teacher model and ﬁne-
tune it on the target task; (2) Calculate sub-structure
scores of the teacher model on the target dataset;
(3) Train the student model to minimize the L2
distance of sub-structure scores between the stu-
dent model and the teacher model using Eq. (9).
Note that for our efﬁcient knowledge distillation
method, λ is set as 1 to fully utilize the parallelism,
and is set to 0 only at the last epoch. This strategy
improves the efﬁciency of our method without hurt-
ing its performance, due to our observation that the
teacher model perfectly ﬁts the training set.

Baselines All baselines of structured knowledge
distillation (Kim and Rush, 2016; Wang et al., 2021,
2020) are re-implemented with BERT-CRF model
following the ofﬁcial implementations2 for fair
comparisons. For K-best decoding, we empirically
ﬁnd that a K value larger than 3 does not further im-
prove the performance but introduces more training
cost, so K is set to be 3 by default.

3.2 Main Results

Table 1 shows the F1 scores of baselines and our ap-
proach on NER and syntactic chunking. Although
we are focusing on improving the efﬁciency, we
observe that our proposed approach (i.e., Efﬁcient
KD) performs well and even outperforms other

2https://github.com/Alibaba-NLP/MultilangStructureKD

and https://github.com/Alibaba-NLP/StructuralKD

Table 1: Comparison of precision, recall and f1 score results on test sets. All metrics are calculated using the
CoNLL 2000 evaluation script. Reported values are the average of 5 runs with different random seeds.

Method

CoNLL 2003 NER

CoNLL 2000 Chunking

Precision Recall

F1

Precision Recall

F1

BERTBASE-CRF (teacher)
BERT3L-CRF (student)

K-best (Kim and Rush, 2016)
Struct. KD (Wang et al., 2021)

Efﬁcient KD

92.00
88.93

88.94
89.33

89.69

92.62
88.79

89.50
89.33

92.31
88.86

89.22
89.33

89.71

89.70

97.80
96.20

96.46
96.48

96.81

97.80
96.22

96.41
96.43

97.80
96.21

96.44
96.46

96.77

96.79

Table 2: The efﬁciency comparisons among baselines and our approach. We tabulate the time costs of different
computation processes in one training epoch. Tea. and Stu. represent teacher and student respectively.

Method

Tea. Forward Stu. Forward Stu. Backward Total

Vanilla Training

K-best
Struct. KD
Efﬁcient KD

-

21s
19s
16s

19s

23s
30s
5s

35s

39s
50s
20s

54s

83s
99s
41s

existing structured knowledge distillation meth-
ods. This shows that imitating the sub-structure
predictions indeed helps the student model bet-
ter mimic the internal behavior of the teacher
model. More detailedly, our observations are as
follows: (1) Thanks to the informational representa-
tion from BERT plus the ability to construct the re-
lationship of outputs from CRF, our teacher model
BERTBASE-CRF reaches new state-of-the-art per-
formance (i.e., 97.80) on CoNLL 2000 chunking
task; (2) struct. KD (Wang et al., 2021) performs
slightly better than the K-best method (Kim and
Rush, 2016) for structured prediction tasks, while
they all boost the performance of the student model;
(3) our proposed method surpasses previous base-
lines and improves the performance of the student
model by a signiﬁcant margin.

3.3 Efﬁciency Analysis

To demonstrate that our method distills the struc-
tured knowledge more efﬁciently, we measure the
training speed for different distillation methods on
the CoNLL 2003 dataset per epoch. The batch size
is 32 and the running device is a NVIDIA Tesla
V100 GPU. The running time is calculated as the
average of 5 epochs after warmup of GPU. The
results are shown in Table 2. It shows that our pro-
posed method signiﬁcantly speeds up the training
process, which is composed of three parts: teacher

forward, student forward and student backward.
We analyze the reasons as follows: The forward
speed of the teacher model is slightly quicker sim-
ply because the calculation of sub-structure scores
does not include the calculation in Eq. (4). The
time cost of the student forward and backward is
largely reduced given the credit to the computa-
tional parallelism of our method. Longer sequence,
more speed-up. As a result, our method costs less
than half of the training time for the student model
(41s compared to 83s and 99s one epoch). These
results verify the efﬁciency of our method.

4 Conclusion

While a few recent works have presented tractable
approaches for structured knowledge distillation,
they are time-consuming due to sequence-level
computation. In this work, we propose to locally
match the predictions of the student model and the
teacher model on all sub-structures. In this way, we
avoid globally searching for output structures and
help the student model better mimic the internal
behavior of its teacher. Besides, the simple formula-
tion of our approach allows parallel matrix compu-
tation on GPUs. We conduct experiments on NER
and text chunking across two datasets, showing that
our approach signiﬁcantly outperforms previous
baselines and owns very high training efﬁciency.

for pre-trained language models. In Proceedings of
the 2021 Conference on Empirical Methods in Nat-
ural Language Processing, pages 379–389, Online
and Punta Cana, Dominican Republic. Association
for Computational Linguistics.

Adwait Ratnaparkhi et al. 1996. A maximum entropy
model for part-of-speech tagging. In EMNLP, vol-
ume 1, pages 133–142. Citeseer.

Erik Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the conll-2003 shared task: Language-
independent named entity recognition. In Proceed-
ings of the Seventh Conference on Natural Language
Learning at HLT-NAACL 2003, pages 142–147.

Victor Sanh, Lysandre Debut, Julien Chaumond, and
Thomas Wolf. 2019. Distilbert, a distilled version
of bert: smaller, faster, cheaper and lighter. arXiv
preprint arXiv:1910.01108.

Haipeng Sun, Rui Wang, Kehai Chen, Masao Utiyama,
Eiichiro Sumita, and Tiejun Zhao. 2020. Knowledge
distillation for multilingual unsupervised neural ma-
chine translation. In Proceedings of the 58th Annual
Meeting of the Association for Computational Lin-
guistics, pages 3525–3535, Online. Association for
Computational Linguistics.

Xinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang,
Fei Huang, and Kewei Tu. 2020. Structure-level
knowledge distillation for multilingual sequence la-
In Proceedings of the 58th Annual Meet-
beling.
ing of the Association for Computational Linguistics,
pages 3317–3330, Online. Association for Computa-
tional Linguistics.

Xinyu Wang, Yong Jiang, Zhaohui Yan, Zixia Jia,
Nguyen Bach, Tao Wang, Zhongqiang Huang, Fei
Huang, and Kewei Tu. 2021.
Structural knowl-
edge distillation: Tractably distilling information for
structured predictor. In Proceedings of the 59th An-
nual Meeting of the Association for Computational
Linguistics and the 11th International Joint Confer-
ence on Natural Language Processing (Volume 1:
Long Papers), pages 550–564, Online. Association
for Computational Linguistics.

References

Jimmy Ba and Rich Caruana. 2014. Do deep nets really
need to be deep? Advances in Neural Information
Processing Systems, 27.

Cristian Buciluˇa, Rich Caruana,

and Alexandru
Niculescu-Mizil. 2006. Model compression. In Pro-
ceedings of the 12th ACM SIGKDD international
conference on Knowledge discovery and data min-
ing, pages 535–541.

Alexis Conneau, Holger Schwenk, Loïc Barrault, and
Yann Lecun. 2017. Very deep convolutional net-
works for text classiﬁcation. In Proceedings of the
15th Conference of the European Chapter of the As-
sociation for Computational Linguistics: Volume 1,
Long Papers, pages 1107–1116, Valencia, Spain. As-
sociation for Computational Linguistics.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019a. BERT: Pre-training of
deep bidirectional transformers for language under-
In Proceedings of the 2019 Conference
standing.
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers),
pages 4171–4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019b. Bert: Pre-training of
deep bidirectional transformers for language under-
standing. In NAACL-HLT (1).

Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.
Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531.

Yoon Kim and Alexander M. Rush. 2016. Sequence-
level knowledge distillation. In Proceedings of the
2016 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1317–1327, Austin,
Texas. Association for Computational Linguistics.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random ﬁelds: Prob-
abilistic models for segmenting and labeling se-
quence data.

Mike Lewis, Yinhan Liu, Naman Goyal, Mar-
jan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer.
2020. BART: Denoising sequence-to-sequence pre-
training for natural language generation, translation,
and comprehension. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics, pages 7871–7880, Online. Association
for Computational Linguistics.

Lei Li, Yankai Lin, Shuhuai Ren, Peng Li, Jie Zhou,
and Xu Sun. 2021. Dynamic knowledge distillation

