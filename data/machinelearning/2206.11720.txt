MEASUREMENT AND APPLICATIONS OF POSITION BIAS IN A
MARKETPLACE SEARCH ENGINE

2
2
0
2

l
u
J

3
1

]

R

I
.
s
c
[

2
v
0
2
7
1
1
.
6
0
2
2
:
v
i
X
r
a

Richard Demsyn-Jones
Thumbtack
demsynjones@thumbtack.com
rdemsynjones@gmail.com

July 14, 2022

ABSTRACT

Search engines intentionally inﬂuence user behavior by picking and ranking the list of results. Users
engage with the highest results both because of their prominent placement and because they are
typically the most relevant documents. Search engine ranking algorithms need to identify relevance
while incorporating the inﬂuence of the search engine itself. This paper describes our efforts at
Thumbtack to understand the impact of ranking, including the empirical results of a randomization
program. In the context of a consumer marketplace we discuss practical details of model choice,
experiment design, bias calculation, and machine learning model adaptation. We include a novel
discussion of how ranking bias may not only affect labels, but also model features. The randomization
program led to improved models, motivated internal scenario analysis, and enabled user-facing
scenario tooling.

Keywords position bias · search ranking · learning to rank · inverse propensity scoring · propensity estimation

1

Introduction

Free text search bars are central parts of some of our most successful consumer technologies, from informational search
engines like Google to marketplaces like Amazon. Search systems seek to present the most relevant results most visibly
to their users. By doing so they take advantage of, and entrench, top-to-bottom examination behavior. The selection
and ranking of search results is an important area of research. We veriﬁed the importance of our ranking through this
work, and we also believe that search engine ranking is non-controversially important and widely regarded as a key
determinant in the success of Google and other search-based services [Joachims et al., 2007].

There is a large literature examining and modeling consumer behavior with search engines [Chapelle and Zhang, 2009,
Dupret and Piwowarski, 2008, Yue et al., 2010, Agarwal et al., 2019b]. Often the goal is to create ideal rankings that
are not not biased by the endogenous history of prior ranking [Joachims et al., 2018, Wang et al., 2018, Aslanyan
and Porwal, 2019, Agarwal et al., 2019a, Vardasbi et al., 2020, Ovaisi et al., 2020]. In this paper we describe a path
of applied research in one marketplace, where we continually experiment with new ranking algorithms and invest in
data for subsequent iterations, such that we can improve our ranking by making it easier for customers to match with
professionals who can do their jobs. We initially built algorithms with biased features and loss functions until investing
in a randomization program to identify bias from ranking. We applied our research from the randomization program
into two improved models, ﬁrst successfully supplanting our baseline model with a new model using unbiased features,
before subsequently supplanting that model with a model using an unbiased estimator. Both of those iterations not only
showed improvements in ofﬂine model metrics, but also improved our aggregate marketplace metrics as part of a wider
ranking system. Our bias estimates also proved useful beyond improved ranking, through internal hypothesis evaluation
and external tooling.

The main contributions of this work are:

• Position bias adjustments for features, with or without adjustments for position bias in model optimization

 
 
 
 
 
 
Measurement and applications of position bias in a marketplace search engine

• Discussion of power and cost trade-offs in randomized allocation schemes for measuring position bias

• Experimental validation of RandPair position bias factors for an Inverse Propensity Scoring (IPS) model

outside of Google’s internal datasets and experiments for Gmail and Drive

• Sharing a simple calculation for position bias factors from search logs with randomized allocation

2 Thumbtack search

Thumbtack is a large marketplace for professional services. The Thumbtack marketplace has visitors (or customers or
searchers) who have projects for which they need professional help, and professionals who seek customers for their
services. A typical Thumbtack visitor might be a homeowner who uses Thumbtack to seek professional help with
ﬁxing, maintaining, or improving their home. Thumbtack hosts a wide variety of services, including house cleaning,
plumbing, wedding planning, dog walking, tax planning, and many others. The professionals on Thumbtack may be
self-employed individuals, small local businesses, or large national providers. They may specialize in speciﬁc services
or have many employees across a range of services.

Finding the right professional for a project can be daunting. Professionals are not interchangeable. They have different
skills, locations, prices, and other attributes that may matter to customers. Thumbtack aims to facilitate successful
matches in the marketplace, where a customer and a professional both want to undertake a project with each other. We
provide a search engine, allowing customers to specify their needs through text search or click navigation. Visitors see
a search results lists of applicable professionals, such as the one shown in Figure 1. They can narrow down their search
with ﬁlters on project requirements and scheduling, and they can view detailed proﬁles of individual professionals.

Figure 1: Example of search results on Thumbtack, with identifying information hidden.

Thumbtack’s search engine is central to the customer experience. While some customers can ﬁnd the right professional
through directly reaching their proﬁle or by providing project details and asking Thumbtack to ﬁnd professionals on

2

Measurement and applications of position bias in a marketplace search engine

their behalf, the majority of visitors see vertically listed search results, peruse summary information or the proﬁles of
the professionals shown, and then select from them. Sometimes we use pagination, where we show 10 professionals at
a time, while other interfaces may instead contain a continually scrolling list.

The result at the top of the ﬁrst page is the 1st position (indexing from 1), with the subsequent result the 2nd position,
and so forth. A “higher” position means higher visually to a user, with a lower numeric index, such that (for example)
position 1 is the highest position since it is the very ﬁrst listing when observed top-to-bottom.

As shown in Figure 2, we have diminishing contact rates by position, except for slight reversals at the bottom of pages.
Diminishing contact rates could be caused by a combination of customer preference for higher results and our success
at ranking better matches at the top of our search results.

Figure 2: Relative contact rate by position, for searches with 30 or more professionals available.

Thumbtack’s ideal ranking is that which facilitates the highest likelihood of the best match. Following the Probability
Ranking Principle that "documents should be ranked in order of the probability of relevance or usefulness" [Singh and
Joachims, 2018], we maximize ranking utility by ranking professionals in descending order of our predicted relevance
to the customer [Robertson, 1977]. We want pairings of customers and professionals that will choose to work together,
both better off than if they had not collaborated, and collectively better off than if any other pairing had matched. When
a customer and professional connect and the professional solves the customer’s need we call that a job done.

3 Related work

3.1 Terminology and background

The term position bias is sometimes used to refer to any causal relationship between rank and outcomes [Yue et al.,
2010, Ovaisi et al., 2020]. In other papers it is used to refer to a speciﬁc model of such causal relationships [Li, 2020,
Wang et al., 2018, Vardasbi et al., 2020]. We will use the term ranking bias to refer to any form of causal impact of
ranking on outcomes, to distinguish clearly between these two uses of position bias in the literature.

Much of the literature describes clicks as a metric of interest [Yue et al., 2010, Joachims et al., 2018]. Some search
engines observe clicks with high accuracy and have limited visibility on any further action. A click on a Thumbtack
search result does not lead away from Thumbtack to outside of our visibility. Some downstream activity stays on our
platform, creating other metrics of match success. In this paper we will focus on contact, which is when a customer
selects a professional, answers a series of standard questions about their project, and submits those answers along with
a message to the professional. The professional will then see the project listed alongside other contacts and can respond
to the customer to coordinate on the project.

Examination describes a typically unobserved process whereby customers look at and consider a search result [Joachims
et al., 2018, Wang et al., 2018, Agarwal et al., 2019b]. With pagination of search results we can know that all search

3

Measurement and applications of position bias in a marketplace search engine

results on non-visited pages were not examined. At Thumbtack we track scrolling in the user interface, so we can
further identify professionals that were not examined. When a user scrolls such that a search result is fully shown in
their interface we call that result viewed. View is only an approximation for examination, since a visitor may scroll
rapidly to see several professionals but only truly consider some of them.

The literature uses the term query to refer to the context in which a ranked search result is generated. In many search
engines, the query is a text input. The concept generalizes to other available information about the searcher. A document
is an item ranked for the visitor. The visitor has a goal of ﬁnding a relevant document that satisﬁes the purpose of their
search.

In the context of Thumbtack, the query may be text from the search bar or it may be another mechanism of reaching a
search result, such as navigation through recommended searches. Documents correspond to professionals, examination
corresponds to a serious consideration of the professional, and relevance corresponds to a professional that will lead to
a mutually beneﬁcial match between the customer and the professional.

3.1.1 The cascade model

In a cascade model visitors view the search results from top to bottom, considering each result in turn, and they stop
when they ﬁnd a relevant result [Craswell et al., 2008, Dupret and Piwowarski, 2008].

The cascade model in its most literal form is easy to refute. The model assumes users stop exactly when they ﬁnd a
relevant result or exhaust the search results. We have searches where visitors did not contact any professionals, yet
did not view the entire list of professionals we returned. A notable minority of our searches have multiple contacts,
indicating that the customer did not halt their search at the ﬁrst relevant professional. Customers do not always contact
the last viewed search result. On the contrary, even in searches with substantial scrolling we still see contacts strongly
diminish at lower positions. In the cascade model the ﬁnal examined search result is always the selected one, so even
with noise in view events we would expect an inverse relationship between position and contact when customers have
scrolled. We also observe backwards scroll behavior, where in a minority of searches we see backtracking of at least a
few positions.

3.1.2 User Browsing Model (UBM) and Dynamic Bayesian Network (DBN)

In the UBM, users scroll top-to-bottom, examine only some of the results, can select multiple results, and are more
likely to cease their search as relevant results become further apart [Dupret and Piwowarski, 2008].

In the DBN, users scroll top-to-bottom, examining each item, can click multiple items, and will stop when they click an
item that is satisfying [Chapelle and Zhang, 2009].

Those two models were published by Yahoo researchers during the same time period, and they share a very similar
framing. They articulate more complex behavior than the simpler cascade model.

In their literal formation they cannot perfectly ﬁt Thumbtack data, since they have exclusively forward movement while
Thumbtack has some reverse movement. Not only do customers change the direction of their scanning, but we also see
contacts happening in reverse order. In the case where customers make exactly two contacts from the same search result
list, about 30% of the time they contacted the lower ranked professional ﬁrst.

3.1.3 The position-based propensity model (PBM)

In the position-based propensity model (or position bias model), the probability of contact equals the probability of
relevance multiplied by the probability of examination, where the probability of examination is only a function of
position [Joachims et al., 2018, Wang et al., 2018]. Using the variables deﬁned in Table 1, the PBM is summarized in
Equations 1 and 2.

C = 1 ⇔ E = 1, R = 1

P (C = 1|q, d, k) = P (R = 1|q, d) · P (E = 1|k)

(1)

(2)

The isolation of position impact into a probability of examination is the separability hypothesis [Dupret and Piwowarski,
2008], which implies none of the quality-of-context bias from Joachims et al. [2007]. Each position has an independent
likelihood of examination, as if the visitor generates a random number for each position to decide whether to examine
the corresponding document. The PBM has been extensively used because of its simplicity and has a successful track

4

Measurement and applications of position bias in a marketplace search engine

Table 1: Symbols used in this text

Symbol

Meaning

q
d
C
y
k
E
R
θk
r
n
f
∆
λ

Query (or search)
Document (or professional)
Click (or contact)
Set of documents and positions for a query
Position
Examination, Expectation
Relevance
P(E = 1 | position = k)
Estimate of θ from logs
Sample count
Ranking algorithm
Loss function
A weight function within the loss function

record in applications [Agarwal et al., 2019a]. The PBM is a useful model, despite lacking the more realistic behavioral
aspects of the trust bias model, the UBM, or the DBN.

In the quantiﬁcation of position bias, a higher position bias means that a position is more likely to receive contacts,
holding all else equal, such that position 1 typically has the largest position bias. We can describe relative position bias
between any two positions, such that one position is given a position bias of 1 and the other position has a position bias
that indicates their interaction rate relative to the reference position. Typically position 1 is the reference position, and
we describe a full set of position biases relative to it. We assign the value of 1 to the ﬁrst position, and then the number
for position 2 is the ratio of expected rate of contacts for a professional if shown in position 2 to the expected contacts
for the same professional if shown in position 1. Position 1 could have a value of 1, position 2 a value of 0.6, position 3
a value of 0.4, and so forth. This is the common quantiﬁcation used in the position bias literature.

3.2 Extensions

The click model literature is rich with alternative models. The trust bias extension to the PBM removes the separability
hypothesis. It supports the nuance that we only observe perceived relevance, which depends not only on actual relevance
but also position [Agarwal et al., 2019a, Vardasbi et al., 2020]. Other approaches support multiple document modalities
[Chen et al., 2012, Wang et al., 2013], unify different forms of bias [Yi et al., 2021], support non-click successful
outcomes in the mobile ranking [Mao et al., 2018], add personalized bias estimations for users with substantial history
[Zhang et al., 2022], or consider iterative comparisons of neighbouring documents [Zhang et al., 2021a].

4 Modeling position bias at Thumbtack

4.1 Choice of the PBM

The literature makes the distinction between navigational queries where searchers know what document they want
and use the search engine as a tool to ﬁnd the right link, and informational queries where users seek information more
broadly and multiple results could be relevant [Dupret and Piwowarski, 2008, Chapelle and Zhang, 2009]. A third
goal that Thumbtack users could have is project fulﬁllment, where they have a speciﬁc task and seek to to hire a single
professional or compare multiple professionals. This is uncommon in the literature. Although some papers come
from retailers like eBay [Aslanyan and Porwal, 2019] or TripAdvisor [Li, 2020], many papers come from partnerships
with navigational and informational search engines like those from Google [Agarwal et al., 2019b,a, Qin et al., 2020,
Yue et al., 2010, Wang et al., 2018], Microsoft [Joachims et al., 2007, Guo et al., 2009, Ling et al., 2017], and Yahoo
[Chapelle and Zhang, 2009, Dupret and Piwowarski, 2008].

The PBM is ﬂexible enough to ﬁt most observed Thumbtack searcher behavior. Users may arrive and browse our search
results top to bottom. They could stop when they have found a sufﬁciently relevant professional, or when the results
have become demonstrably less relevant (as in the UBM), or when they have exhausted the attention they were willing
to commit to the search process. They could then review the set they have examined and contact the professional(s) with
the highest observable relevance, possibly with some preference for higher ranked results due to trust in the ranking
system. Multiple contacts can be explained by the limited relevance of individual professionals, the project’s urgency

5

Measurement and applications of position bias in a marketplace search engine

and its need for a backup plan, or informational searches. Actual user behavior could vary on any of those aspects, and
our observed data could be a mix of many types of searchers.

We assume that position bias is constant for any quality level and for any search query on Thumbtack, but varies by the
type of interaction (proﬁle view, contact, and so forth) and the type of product interface used, motivated by similar
concerns as the attribute-based propensity framing of Qin et al. [2020].

5 Measuring position bias

5.1 Alternatives to randomization

Since we have scroll data we could assume that observed scrolls represent examination. However, we don’t believe that
observation in its most literal sense explains the full extent of ranking bias in our marketplace. Consider the sharper
drop in contact rates than quality metrics even when all professionals are viewed, as shown in Figure 3 for one of our
internal estimates of quality. In the PBM, examination by position is unconditional on examination of other positions,
and when two items are both observed their click probabilities vary only on relevance.

Figure 3: Relative rates when 10 or more professionals are viewed.

We can try to measure ranking bias through regression. Conceptually, we can include all quality factors, trying to
“control” for them, and additionally include position as a feature [Ling et al., 2017, Wang et al., 2018, Haldar et al.,
2020]. The position coefﬁcients represent position bias. The concept of using position as a feature to separate its impact
extends to other model types such as decision trees and neural networks [Ling et al., 2017]. One issue with this approach
is omitted variable bias: if there exist quality factors that are correlated with position but not included in the regression,
then their effect could be misattributed to the effect of position. Similarly, the effect of position can be attributed to
other features, such that when "there is a high correlation between them, the attribution can be arbitrary" [Wang et al.,
2018]. We could also try an instrumental variables (IV) approach, using a two-stage regression where the ﬁrst stage
predicts position and the second stage uses predicted position as a coefﬁcient. Critically, the ﬁrst stage prediction needs
to accurately predict position without having the same correlation with unobserved quality factors. Another method for
unbiased ranking with non-random data is to use Expectations-Maximization (EM) algorithms [Wang et al., 2018].

Those approaches are promising but are not without risk of inaccurate position bias estimation. Position as a feature
performed poorly when evaluated in Wang et al. [2018]. The EM approach had mixed evidence when proposed in Wang
et al. [2018], performed poorly in Aslanyan and Porwal [2019], and was challenged in Agarwal et al. [2019b] with the
explanation that “Deﬁning an accurate relevance model is just as difﬁcult as the learning-to-rank problem itself, and a
misspeciﬁed relevance model can lead to biased propensity estimates”.

Thumbtack search has considerable regular variation due to ranking experiments, where we continually iterate to
help customers match with professionals more easily. This is similar to the context in Agarwal et al. [2019b]. Their

6

Measurement and applications of position bias in a marketplace search engine

approach treats variation due to ranking experiments as "virtual swap interventions", which also sounds promising for
Thumbtack’s use case.

We decided that randomization methods had little chance for error, were typically precise while requiring limited sample
relative to our scale, and would be highly convincing internally due to their simplicity and lack of modeler choice
parameters.

5.2 Randomization for position bias

Fully randomizing the entire search results for some subset of searches could lead to a direct estimate of position bias
[Wang et al., 2018]. Over enough observations, the average match quality of professionals would be constant across
positions. Measuring the advantage of the ﬁrst position over the second position would be as simple as measuring
the rate of contacts to professionals in those two positions across all searches that had at least two professionals. The
downside of any randomized algorithm is degradation of the search results, coming at a cost to successful matches. Full
randomization would lead to signiﬁcant degradation, hurting market participants during the randomization period and
possibly leading to damage to Thumbtack’s reputation due to visibly subpar search results.

Papers in the ranking bias literature typically assume a ﬁxed behavior model. We propose a behavior adaptation model
such that visitor behavior models change in response to ranking, as shown in Figure 4. In this adaptation model, visitors
choose a behavior model to maximize their utility subject to their expectation about how the ranking algorithm works.
As they observe the ranking they adjust their expectation about the ranking algorithm.

Figure 4: Behavior adaptation model.

This implies some risk that customer behavior is fundamentally different with a random list than with an intelligently
ordered list. Perhaps with an intelligently ordered list searchers may trust in our ranking to some extent, leading
to our present position bias. When viewing an entirely random list, the unintuitive ordering may be observable to
customers, and they may discard this trust. The degree of position bias may change when visitors recognize a poor
ranking algorithm. In an extreme case they may change their algorithm from a classic behavior model to one of equally
examining a ﬁxed number of professionals.

Instead of fully random search results, we implemented the RandPair algorithm described in Wang et al. [2018].
RandPair randomly swaps a single pair of adjacent professionals in each treated search result. While this still comes
at some cost, it should be much less signiﬁcant than the full randomization algorithm. To the extent that there is any
quality-of-context bias in search results [Joachims et al., 2007], such that the entire ordering matters when customers

7

Measurement and applications of position bias in a marketplace search engine

browse, RandPair involves less disturbance than full randomization. Additionally, estimates from RandPair can be used
later to validate other approaches, as in Li et al. [2020].

5.3 Power calculations

We did not know ahead of time how much sample we needed, since "sample size calculation requires assumptions that
typically cannot really be tested until the data have been collected" [Gelman and Hill, 2006]. Power calculations for
experiments are often computed using an effect size (or minimum detectable effect) that is meaningful to decision-
making based on the experiment, "the smallest difference or effect that the researcher considers to be clinically relevant"
[Pancholi et al., 2009]. For this randomization program we did not know what level of precision for bias estimates for
each position would lead to meaningful changes in the quality of our ranking or for our other use cases.

We decided to emphasize neighbouring positions rather than effects relative to the 1st position so that we could better
discern between professionals who we show near each other in search results. These are professionals that could
reasonably be reordered by more informed models, in contrast to how the quality gap between the ﬁrst listed result and
most lower results may be large enough that better measurement of position bias is unlikely to lead to a reversal in
relative ordering. We wanted to be able to measure the position bias between any neighbouring positions such that
if position bias between the two existed then we had a high likelihood of measuring bias factors with statistically
signiﬁcance differences.

We can place a likely upper bound on our effect sizes by using observed contacts rates by position. Observed rates
embed both the effects of position bias and of quality, both of which have the same sign of correlation with respect to
position. For example, position 1 has the highest position bias and (ideally) the highest average match quality, both of
which should lead to higher contact rates in position 1 than other positions. As such, the effect of position bias itself
should be smaller than observed interaction rates. We decided to compute power calculations for any neighbouring
positions under the hypothesis that half of our observed contact difference is due to position bias. We thought this was a
conservative policy for sample estimates, since the decay in contact rates exceeded the decay in estimated match quality,
as shown earlier in Figure 3.

We measured position bias separately for several interfaces on our website. For example, after customers peruse the
search results, select a professional, and contact them with project details, we then identify and present three more
professionals who speciﬁcally match those project details. The user interface is substantially different, with the three
professionals shown horizontally in a smaller grid overlapping the search results. We estimated power for position bias
estimates for this type of list and for others that vary from our typical search results.

Decays in contact rates are steep in the ﬁrst few positions, with position 1 receiving far more contacts than position 2,
position 2 still noticeably more than position 3, and so forth at diminishing rates, as shown earlier in Figure 2. Position
biases among these positions, if accounting for a meaningful proportion of the observed contact rate differences, would
be very easy to measure with statistical signiﬁcance. The position-contact curve is not always smooth, ﬁrst due to
non-monotonicity at the bottom of pages (e.g. position 10 receives more contacts than position 9), secondly due to very
steep drops in interactions at page boundaries (e.g. position 10 receiving far more interaction than position 11). Within
the second and third pages of search results we observe contact rates close enough and events sparse enough that we see
inconsistent average rates over moderate samples. Our power calculations demonstrated that we would not be able to
accurately measure position bias in a reasonable time frame between neighbouring positions this deep into the search
results.

5.4 Trafﬁc allocation

We ﬁrst isolated the randomization program to 50% of our trafﬁc. This means that 50% of trafﬁc deﬁnitely had no
randomization, while the other 50% of trafﬁc was subject to the randomization program and typically had a pair of
swapped professionals. Randomizing into 50% of trafﬁc allowed us to very easily and reliably track the aggregate cost
of the randomization program using our internal tools for A/B experiments.

We decided to have adjacent swaps for each pair up through positions 10 and 11 where we expected relative comparisons
to be statistically measurable. Additionally, we included a swap of positions 11 and 19 to give us an estimate of the
total decay over the second page, which we could attribute to various positions in a manner consistent with the tactic of
"interpolate between estimates at well-chosen ranks and/or employ smoothing" from Joachims et al. [2018]. The ﬁnal
swap is positions 11 and 19 because we expect position 20 at the bottom of the second page to beneﬁt from position
bias relative to position 19. Understanding the net impact between positions 11 and 20 would not help us understand
the slope between position 11 and the monotonically worse positions below it.

8

Measurement and applications of position bias in a marketplace search engine

Table 2: Trafﬁc allocation with 50% holdout and 11 swap permutations

Non-stochastic results
Intend to swap positions 1 and 2
...
Intend to swap positions 10 and 11
Intend to swap positions 11 and 19

50%
50% / 11 = 4.55%

50% / 11 = 4.55%
50% / 11 = 4.55%

Total Thumbtack searchers

100%

For the proportion of trafﬁc allocated to the randomization program, with uniform probability we selected a pair to
swap from our 11 total swap candidates (positions 1 and 2 through positions 11 and 19) and then performed the swap if
the search results were long enough to contain both positions. Table 2 describes our trafﬁc allocation.

5.5 Calculation from search logs

If we swapped positions k and k + 1 in 50% of our trafﬁc, the expected aggregate quality of professionals in each
position would be balanced in the limit. Instead we swap any adjacent pair much less often. Consider an example of
positions 8 and 9. Since each position is the chosen swap 4.55% of the time, then the average match quality at position 8
is still very close to the non-randomized quality of position 8 professionals, and the quality difference between position
8 and position 9 will be roughly similar to when there is was no randomization.
For comparison of positions k and k(cid:48), where k(cid:48) > k (speciﬁcally k(cid:48) = k + 1 in our program), consider a corpus of
searches that have at least k(cid:48) professionals. We can only perform a random swap if both the higher and lower position
are occupied for the particular search. A list with k professionals might have slightly different behavior than a list with
k + 1 or more. Professionals on shorter lists may have a higher likelihood of interaction (holding quality equal) due to
fewer competitors and on average more prior ﬁltering from visitors.
To construct a quality-neutral estimate for position k in its comparison with position k(cid:48), which will will call rk
k,k(cid:48), we
take an unweighted average of the interaction rates of the natural position k professionals and the professionals swapped
up from position k(cid:48) to position k. This is shown in Equation 3, with C for contacts and n for sample counts, where we
use the ﬁrst superscript to indicate the original position and the second superscript to indicate ﬁnal position. For our
position k(cid:48) quality-neutral estimate we take an unweighted average of the interaction rates for professionals naturally
ranked in position k(cid:48) and those moved down from position k to position k(cid:48), as in Equation 4.

rk
k,k(cid:48) = 0.5 ·

(cid:34)

C k,k
nk,k +

C k(cid:48),k
nk(cid:48),k

(cid:35)

rk(cid:48)
k,k(cid:48) = 0.5 ·

(cid:34)

C k(cid:48),k(cid:48)
nk(cid:48),k(cid:48) +

C k,k(cid:48)
nk,k(cid:48)

(cid:35)

(3)

(4)

Multiplying by half is for interpretability and will be omitted for convenience going forward. The progression in
Equation 5 shows how the expected value of rk
k,k(cid:48) is our position bias factor times the additive expected relevance of
the documents naturally at position k and those naturally at position k(cid:48).

E[rk

k,k(cid:48)] =

=

E[C k(cid:48),k]
nk(cid:48),k

E[C k,k]
nk,k +
(cid:80)nk,k
i=1 θkR(dk,k
nk,k

i

)

+

(cid:80)nk(cid:48) ,k

i=1 θkR(dk(cid:48),k
nk(cid:48),k

i

)

(5)

= θk[E[R(dk,k)] + E[R(dk(cid:48),k)]]

Note that relevance is only used to determine the original position and not the random selection, such that R(dk,k(cid:48)
). Substituting these equivalences and dividing by the same derivation for E[rk(cid:48)
R(dk,k) and R(dk(cid:48),k) = R(dk(cid:48),k(cid:48)

) =
k,k(cid:48)]

9

Measurement and applications of position bias in a marketplace search engine

Table 3: Cost of the randomization program relative to the non-randomized baseline sample

Metric

Performance

p-value

Rate of visitors with >= 1 contact
Rate of visitors with >= 1 match

-0.47%
-0.46%

0.09
0.14

leads us to Equation 6. This shows that the relative ratios of these quality-neutral click rates are unbiased estimators of
our relative propensity.

E[rk
E[rk(cid:48)

k,k(cid:48)]
k,k(cid:48)]

=

θk
θk(cid:48)

E[R(dk,k)] + E[R(dk(cid:48),k(cid:48)
)]
E[R(dk(cid:48),k(cid:48))] + E[R(dk,k)]

·

=

θk
θk(cid:48)

(6)

This formulation uses data from all contacts to construct position bias estimates of minimal variance. This is conceptually
similar to the description in Agarwal et al. [2019b] of weighing each observation by how often it was shown in each
position.

This gives us relative position bias estimates for every pair of adjacent positions. We place them all on the same scale,
relative to position 1, by assuming transitivity through the search results. The position 1 versus 3 ratio is calculated by
multiplying the relative position bias of 1 versus 2 with the relative position bias of 2 versus 3, as described in Wang
et al. [2018].

5.6 Outcome

We ran our randomization program over an extended time frame. Aggregate cost was tolerable, with small mean effects
on contacts and bidirectional matches as shown in Table 3. Our most impactful exchange was the ﬁrst two positions, and
that only occurred in approximately 1/11th of the sample where we had randomization. We found large and monotonic
position bias, except for where we expected non-monotonicity at the bottom of the ﬁrst page of search results. Notably,
the position bias was much less substantial than the estimates we used from earlier non-randomized data, implying
that we had been overvaluing professionals at low positions for their contacts and undervaluing professionals at high
positions for theirs.

While some papers stress the "bookkeeping overhead" of tracking randomization [Joachims et al., 2018, Aslanyan and
Porwal, 2019], we needed only minimal added tracking. We also had no difﬁculty implementing the position bias
estimates in SQL from search logs. This allowed us to have graphs of the position bias estimates updated automatically
as we accumulated more data, with the results visible to any observers in the company. This transparency helped
communicate and document the method.

Results for our most common search result interface are shown in Figure 5. We see sharply decaying relative propensities
in the ﬁrst few positions of the search results. This bias tapers off as we progress down the search results. We observe a
small bias in favour of position 10 over position 9, ﬁtting with the higher contact rates we observe at the bottom of that
page and at the bottom of subsequent pages. We see a small drop between positions 11 and 19, suggesting a very ﬂat
position bias curve between those positions.

6 Successful applications

Thumbtack ranks using predictive models based on observed historical relationships, in the class of machine learning
models where algorithms use data to determine how values for each feature should impact a probability estimate. We
call a comprehensive ranking algorithm, typically with machine learning models at its core, a ranker.

Ultimately we evaluate rankers through visitor-randomized or market-randomized experiments, where we test new
rankers ("challengers") against the existing ranker (the "baseline" or "champion"). While ofﬂine metrics for ranking are
dependent on adjustments (or non-adjustments) for ranking bias, metrics from randomized experiments are a better test
of real world performance. We use metrics of marketplace success, such as the rate of successful matches between
customers and professionals. We have many experiments, and typically allocate enough sample to power them to
support two-sided t-tests with a minimum detectable effect of a 1% improvement at signiﬁcance of 0.05 and power of
80%. While occasionally we test individual features in isolation, more commonly we incorporate multiple changes into
a single new ranker.

10

Measurement and applications of position bias in a marketplace search engine

Figure 5: Position bias estimates from the RandPair algorithm.

Following our randomization program our next two successful challengers both made substantial use of our calculated
position bias factors, with several unrelated unsuccessful challengers in between. These rankers are discussed below.

6.1 Historical rate features

We can create a ranking of professionals because those professionals differ on qualities intrinsic to each professional,
dynamic in the marketplace, and relative to the search.

A particular class of dynamic features are those based on historical aggregate customer behavior with respect to each
professional. We calculate historical contact rates for each professional. Adopting notation from Chapelle and Zhang
[2009], let there be n searches in which professional u appeared, with C as a vector of binary contact outcomes. Then
the historical contact rate for professional u is αu, as calculated in Equation 7.

αu =

(cid:80)n

i=1 Ci
n

(7)

Historical rates are powerful because they embed causal information that may be hard to fully enumerate and quantify
through other features. Customers may care about average ratings, number of reviews, enthusiasm within review text,
business names, photos, key terms in the business proﬁle, and anything else they can observe on Thumbtack. Thumbtack
may know much of this information, but at any point we will have less than perfect identiﬁcation of all possible features.
Historical contact rates could reﬂect information that Thumbtack has yet to identify and quantify. Our ranking models
use historical rates as input features, alongside many other features.

The existence of the ranking problem implies that ranking bias exists, and witnessing that bias has motivated a substantial
literature. Otherwise we would be unable to inﬂuence customer outcomes through ranking algorithms. Historical rates
depend on customer behavior and are thus directly affected by ranking bias. Consider two professionals regularly shown
in the same searches, one consistently in the ﬁrst position and the other in the 15th position. The ﬁrst professional will
almost certainly receive more contacts than the latter even if they both have similar quality.

If historical rates as in Equation 7 were signiﬁcant positive factors in our models then the rankings could be self-
perpetuating, with the top ranked professionals cementing their position and the lowest ranked professionals facing a
severe uphill battle to ever displace higher professionals. This self-perpetuating ranking results in the popularity bias
problem, with a very active literature such as recent publications by Wei et al. [2021], Zhang et al. [2021b], and Zhu
et al. [2021]. The use of historical rate features compounds the importance of estimating ranking bias.

The clicks over expected clicks (COEC) model adjusts each professional’s historical metric relative to the average
at their position [Chapelle and Zhang, 2009, Ling et al., 2017]. Each interaction can be weighed by the inverse of

11

Measurement and applications of position bias in a marketplace search engine

Table 4: Improvement of the unbiased COEC model over the baseline in a 50/50 visitor-randomized A/B test

Metric

Performance

p-value

Rate of visitors with >= 1 contact
Rate of visitors with >= 1 match

+1.6%
+1.7%

0.01
0.01

the propensity of contact at that position. For example, if the top position is contacted 10% of the time and the 15th
position 1% of the time, then we could give 10 times the credit to a contact at position 15 as a contact at position 1. Two
professionals can be ordered not by their absolute rate of contacts per search, but their rate of contacts per search relative
to their position. A professional who is above average for their position will have a higher score than a professional
who is below average at their position, regardless of what those positions are. Equation 8 shows the updated calculation,
with θ representing the vector of position weights and k the vector of positions in which the professional is shown.

αu =

(cid:80)n
(cid:80)n

i=1 Ci
i=1 θki

(8)

The issue with this approach is that if we use non-randomized data to determine our θ position weights then the formula
is not an accurate adjustment for position bias. Professionals in the highest positions are contacted more often than
professionals in lower positions not only due to the causal impact of rankings on searcher behavior, but also because
we intentionally try to rank the best matches in the highest positions. If we try to measure position bias by observed
interaction rates we conﬂate ranking bias with match quality and will overestimate the position bias. If rankings were
entirely random then contact rates by position would be an unbiased estimate of position bias, but our core goal is to
improve rankings and keep them quite different from random.

For some time we used this type of adjusted historical rates as features in our models. The position weights were derived
from our earliest ranking data, when our ranking algorithm was in its infancy and less correlated with match quality
than later iterations. We knew the feature contained a quality component, likely overstating position bias. This could
create a small countervailing reversion to the mean effect, where low-ranked professionals were over-credited for their
interactions and high ranked professionals faced an unfair challenge in maintaining their position, since our ranking
models had a monotonically positive relationship between these COEC features and predicted positive outcomes.

Whether biased COEC features are a problem depends on the impact of the bias on our ranking models. If we wanted to
predict contact as a function of current position, with position being constant, then raw historical rates could be strong
features. However, in a context where we optimize the ranking itself, bias that is not predictive of position-invariant
match quality may limit the usefulness of these features in models.

As an outcome of our randomization program we created new versions of our historical rate features, using the position
bias factors shown in Figure 5. In subsequent models we found the new features to be stronger, as measured by feature
importance metrics, and they largely displaced the older versions of rate features. We conducted a visitor-randomized
experiment on live trafﬁc comparing our baseline against a new ranking model which included these updated COEC
features. The baseline used the older (biased) version of rate features. We included several other new features in the
new model, unfortunately entangling the effects of multiple changes, but the other new features were among our most
marginal by feature importance metrics while the COEC features are among our strongest. The ranker became our
new baseline after substantial and statistically signiﬁcant improvements, as shown in Table 4. In many subsequent
experiments the updated COEC features continue to be among our strongest features and continue to have more
predictive power than their predecessors.

6.2 Ranking models under the PBM

After identifying position bias factors and transitioning to models using new historical rate features, we built a new
ranker using Inverse Propensity Scoring (IPS). Applying notation from Vardasbi et al. [2020], Equation 9 describes how
the estimated loss ( ˜∆) of a ranker (f ) is the sum across a corpus of n queries of all contacts on each document (Ci(d))
multiplied by a (typically) position-based weighting (λ) for that document divided by a position bias factor (θk). The
λ function could be the position of the document shown (creating the Average Relevance Position (ARP), as used in

12

Measurement and applications of position bias in a marketplace search engine

Table 5: Improvement of the IPS model over the unbiased COEC model in a 50/50 visitor-randomized A/B test

Metric

Performance

p-value

Rate of visitors with >= 1 contact
Rate of visitors with >= 1 match

+0.7%
+0.9%

0.08
0.05

Joachims et al. [2018]) or position could have a logarithmic penalty using Discounted Cumulative Gain (DCG), among
other options.

˜∆IP S(f ) =

1
N

n
(cid:88)

(cid:88)

i=1

(d,k)∈yi

Ci(d)
θk

· λ(d|qi, f )

(9)

In the example case of using DCG as the λ loss metric, the only modiﬁcation to traditional DCG is the division by
position bias weights (θk). IPS rewards models that would rank the contacted professionals at higher positions, weighing
observations more when they have less favourable position bias.

IPS is an ideal loss functions under the assumptions of the PBM [Joachims et al., 2018]. The separability assumption
is key, and in the presence of trust bias that varies by position the IPS loss function will not be optimal [Agarwal
et al., 2019a, Vardasbi et al., 2020]. We chose to build an IPS model even with the risk of trust bias or other model
misspeciﬁcation, because of the simplicity of model estimation, the literature of successful IPS uses, and strong existing
implementations.

We found that our IPS iteration improved our marketplace metrics in a visitor-randomized experiment by a meaningful
amount over our baseline at the time, which was the model with unbiased historical features discussed earlier. Table 5
shows the results. The IPS model became our new baseline, and we continued to use IPS in subsequent iterations.

Along with the loss function change we incorporated several new features, since we continually develop new features
for subsequent model experiments. Unfortunately this permits multiple hypotheses of improved model performance.
Improvements could be due to IPS, could be due to new features, or could be caused by model drift [Gama et al., 2014].
While we tested isolating changes in ofﬂine evaluation that indicated improvements separately from IPS and from
new features, we believe that experimenting on users is a more complete test of improvement. Given ﬁnite volume
of visitors to test new rankers on, and that earlier application of improvements leads to more cumulative value than
delayed improvements, we often use models that incorporate multiple changes.

Further observational evidence supporting the IPS approach is that it has survived challenges since, including tests of a
listwise model with LambdaMART and a trust bias model using the implementation from Vardasbi et al. [2020].

6.3

Internal and external tooling

With position bias estimates available we are better able to answer hypothetical questions about how alternative rankers
would affect our customer metrics. These scenarios helped us make decisions on product changes. The parsimony of
randomization helped to explain the results, making it easy to build trust in our applications.

In a pilot program we provided professionals with a tool to understand how their ranking and contacts could be affected
as they change some of the parameters of their business. This tool used position bias estimates to forecast how ranking
changes would likely affect their volume of contacts.

Neither of these applications were the original motivation for the randomization program. We expect that having a
better understanding of ranking bias and customer behavior will continue to help us through future new ideas.

7 Conclusion

This paper summarizes the choice of ranking bias model, measurement of position bias, and applications at a large
consumer marketplace. After due consideration of several behavior models, we chose to apply a position-based
propensity model. Applying a RandPair swap intervention program, we discuss sampling and calculation considerations.
We describe how position bias can inﬂuence historical rate features in machine learning models, improving model
performance even before adopting inverse propensity scoring as a loss function for model optimization. Our position
bias estimates directly led to improved ranking and better scenario tooling.

13

Measurement and applications of position bias in a marketplace search engine

Acknowledgements

We thank Thumbtack colleagues for creating a vibrant intellectual environment where this work could have a large
positive impact on our marketplace, and all members of the Marketplace Matching team for working on this and other
initiatives. In particular we thank Mark Andrew Yao for sharing Wang et al. [2018], Renal Khabibulin for sharing Li
[2020], and Amber Wang for proposing and eventually creating the IPS ranker.

References

Aman Agarwal, Xuanhui Wang, Cheng Li, Michael Bendersky, and Marc Najork. Addressing trust bias for unbiased

learning-to-rank. The World Wide Web Conference, 2019a.

Aman Agarwal, Ivan Zaitsev, Xuanhui Wang, Cheng Li, Marc Najork, and Thorsten Joachims. Estimating position bias
without intrusive interventions. Proceedings of the Twelfth ACM International Conference on Web Search and Data
Mining, 2019b.

G. Aslanyan and Utkarsh Porwal. Position bias estimation for unbiased learning-to-rank in ecommerce search. In

SPIRE, 2019.

Olivier Chapelle and Ya Zhang. A dynamic bayesian network click model for web search ranking. In WWW ’09, 2009.
Danqi Chen, Weizhu Chen, Haixun Wang, Zheng Chen, and Qiang Yang. Beyond ten blue links: enabling user click

modeling in federated web search. In WSDM ’12, 2012.

Nick Craswell, Onno Zoeter, Michael J. Taylor, and Bill Ramsey. An experimental comparison of click position-bias

models. In WSDM ’08, 2008.

Georges Dupret and Benjamin Piwowarski. A user browsing model to predict search engine click data from past

observations. In SIGIR ’08, 2008.

João Gama, Indr˙e Žliobait˙e, Albert Bifet, Mykola Pechenizkiy, and A. Bouchachia. A survey on concept drift adaptation.

ACM Computing Surveys (CSUR), 46:1 – 37, 2014.

Andrew Gelman and Jennifer Hill. Sample size and power calculations, page 437–456. Analytical Methods for Social

Research. Cambridge University Press, 2006. doi:10.1017/CBO9780511790942.026.

Fan Guo, Chao Liu, A. Kannan, Thomas P. Minka, Michael J. Taylor, Yi Min Wang, and Christos Faloutsos. Click

chain model in web search. In WWW ’09, 2009.

Malay Haldar, Mustafa Abdool, Prashant Ramanathan, Tyler Sax, Lanbo Zhang, Aamir Mansawala, Shulin Yang,
Bradley C. Turnbull, and Junshuo Liao. Improving deep learning for airbnb search. Proceedings of the 26th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020.

Thorsten Joachims, Laura A. Granka, Bing Pan, Helene Hembrooke, Filip Radlinski, and Geri Gay. Evaluating the
accuracy of implicit feedback from clicks and query reformulations in web search. ACM Trans. Inf. Syst., 25:7, 2007.
Thorsten Joachims, Adith Swaminathan, and Tobias Schnabel. Unbiased learning-to-rank with biased feedback. In

IJCAI, 2018.

Ruilin Li, Zhen Qin, Xuanhui Wang, Suming Jeremiah Chen, and Donald Metzler. Stabilizing neural search ranking

models. Proceedings of The Web Conference 2020, 2020.

Yinxiao Li. Handling position bias for unbiased learning to rank in hotels search. ArXiv, abs/2002.12528, 2020.
Xiaoliang Ling, Weiwei Deng, Chen Gu, Hucheng Zhou, Cui Li, and Feng Sun. Model ensemble for click prediction in

bing search ads. Proceedings of the 26th International Conference on World Wide Web Companion, 2017.

Jiaxin Mao, Cheng Luo, Min Zhang, and Shaoping Ma. Constructing click models for mobile search. The 41st

International ACM SIGIR Conference on Research & Development in Information Retrieval, 2018.

Zohreh Ovaisi, Ragib Ahsan, Yifan Zhang, Kathryn N. Vasilaky, and Elena Zheleva. Correcting for selection bias in

learning-to-rank systems. Proceedings of The Web Conference 2020, 2020.

Bhavna Pancholi, Mark Dunne, and Richard Armstrong. Sample size estimation and statistical power analyses. Optum

Today, 16, 11 2009.

Zhen Qin, Suming Jeremiah Chen, Donald Metzler, Yongwoo Noh, Jingzheng Qin, and Xuanhui Wang. Attribute-based
propensity for unbiased learning in recommender systems: Algorithm and case studies. Proceedings of the 26th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020.

Stephen Robertson. The probability ranking principle in ir. Journal of Documentation, 33:294–304, 12 1977.

doi:10.1108/eb026647.

14

Measurement and applications of position bias in a marketplace search engine

A. Singh and Thorsten Joachims. Fairness of exposure in rankings. Proceedings of the 24th ACM SIGKDD International

Conference on Knowledge Discovery & Data Mining, 2018.

Ali Vardasbi, Harrie Oosterhuis, and M. de Rijke. When inverse propensity scoring does not work: Afﬁne corrections
for unbiased learning to rank. Proceedings of the 29th ACM International Conference on Information & Knowledge
Management, 2020.

Chao Wang, Yiqun Liu, Min Zhang, Shaoping Ma, Meihong Zheng, Jing Qian, and Kuo Zhang. Incorporating vertical
results into search click models. In Proceedings of the 36th International ACM SIGIR Conference on Research and
Development in Information Retrieval, SIGIR ’13, page 503–512, 2013. doi:10.1145/2484028.2484036.

Xuanhui Wang, Nadav Golbandi, Michael Bendersky, Donald Metzler, and Marc Najork. Position bias estimation for
unbiased learning to rank in personal search. Proceedings of the Eleventh ACM International Conference on Web
Search and Data Mining, 2018.

Tianxin Wei, Fuli Feng, Jiawei Chen, Chufeng Shi, Ziwei Wu, Jinfeng Yi, and Xiangnan He. Model-agnostic
counterfactual reasoning for eliminating popularity bias in recommender system. Proceedings of the 27th ACM
SIGKDD Conference on Knowledge Discovery & Data Mining, 2021.

Jingwei Yi, Fangzhao Wu, Chuhan Wu, Qifei Li, Guang zhong Sun, and Xing Xie. Debiasedrec: Bias-aware user

modeling and click prediction for personalized news recommendation. ArXiv, abs/2104.07360, 2021.

Yisong Yue, Rajan Patel, and Hein Roehrig. Beyond position bias: examining result attractiveness as a source of

presentation bias in clickthrough data. In WWW ’10, 2010.

Junqi Zhang, Yiqun Liu, Jiaxin Mao, Xiaohui Xie, Min Zhang, Shaoping Ma, and Qi Tian. Global or local: Constructing
personalized click models for web search. In Proceedings of the ACM Web Conference 2022, WWW ’22, page
213–223, 2022. doi:10.1145/3485447.3511950.

Ruizhe Zhang, Xiaohui Xie, Jiaxin Mao, Yiqun Liu, Min Zhang, and Shaoping Ma. Constructing a comparison-based
In Proceedings of the Web Conference 2021, WWW ’21, page 270–283, 2021a.

click model for web search.
doi:10.1145/3442381.3449918.

Yang Zhang, Fuli Feng, Xiangnan He, Tianxin Wei, Chonggang Song, Guohui Ling, and Yongdong Zhang. Causal
intervention for leveraging popularity bias in recommendation. Proceedings of the 44th International ACM SIGIR
Conference on Research and Development in Information Retrieval, 2021b.

Ziwei Zhu, Yun He, Xing Zhao, and James Caverlee. Popularity bias in dynamic recommendation. Proceedings of the

27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, 2021.

15

