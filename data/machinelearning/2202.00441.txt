Few-Bit Backward: Quantized Gradients of Activation Functions for Memory
Footprint Reduction

Georgii Novikov 1 Daniel Bershatsky 1 Julia Gusak 1 Alex Shonenkov 2 Denis Dimitrov 2 3 Ivan Oseledets 1 4

2
2
0
2

b
e
F
2

]

G
L
.
s
c
[

2
v
1
4
4
0
0
.
2
0
2
2
:
v
i
X
r
a

Abstract

Memory footprint is one of the main limiting fac-
tors for large neural network training. In back-
propagation, one needs to store the input to each
operation in the computational graph. Every
modern neural network model has quite a few
pointwise nonlinearities in its architecture, and
such operation induces additional memory costs
which — as we show — can be signiﬁcantly re-
duced by quantization of the gradients. We pro-
pose a systematic approach to compute optimal
quantization of the retained gradients of the point-
wise nonlinear functions with only a few bits per
each element. We show that such approximation
can be achieved by computing optimal piecewise-
constant approximation of the derivative of the
activation function, which can be done by dy-
namic programming. The drop-in replacements
are implemented for all popular nonlinearities and
can be used in any existing pipeline. We conﬁrm
the memory reduction and the same convergence
on several open benchmarks.

1. Introduction

Modern neural networks models are getting larger and larger.
One of the main bottlenecks in the training loop is the re-
quired device memory storage (Ojika et al., 2020; Gao et al.,
2020). In this paper, we propose a universal approach that
helps to reduce the model memory footprint during back-
propagation. Note that this approach is complementary to
other memory reducing techniques such as checkpointing
(Chen et al., 2016) or ofﬂoading (Beaumont et al., 2021).
Our method can be applied to any neural network without
any additional preprocessing.

The memory consumed by the model during training (ex-
cept intermediate tensors) can be split into two groups: 1)

1Skoltech, Moscow,

3Lomonosov MSU, Moscow,

Russia
Moscow, Russia.
<georgii.novikov@skoltech.ru>.

Russia

2Sber AI, Moscow,
4AIRI,
Correspondence to: Georgii Novikov

Russia

GELU derivative
3-bit approximation

(cid:48)

)
x
(
U
L
E
G

1.0

0.8

0.6

0.4

0.2

0.0

−6

−4

−2

0
x

2

4

6

Figure 1. Optimized 3-bit piecewise-constant approximation of the
derivative of the GELU activation function.

the model weights (including additional memory for the
optimizer state), 2) activations saved for the backward pass,
over which the computation is not carried out directly at the
moment, but which will be required in the future to compute
the gradients.

Every operation in the computational graph generates a
memory footprint. It is typically overlooked, that the ap-
plication of the pointwise non-linearity (such as GELU or
sigmoid) actually results in storing the input for the back-
ward pass. We show that instead of keeping the full input
tensor, it is possible to store a low-bit representation, which
allows accurate gradients approximation. .

In this work, we propose to approximate the derivative of
the activation function in a piecewise-constant form. Such
an approximation problem has to be solved once for each
activation function, and we propose a simple technique to
do that.

The proposed approximation divides all values into several
bins and saves only its corresponding bin indices instead
of storing all values. This is a lossy compresion, but the
additional noise introduced by it is negligible as we will
show on several benchmarks 4.

Main contributions of our paper are:

• We propose new approximate backward computation

 
 
 
 
 
 
Few-Bit Backward: Quantized Gradients of Activation Functions for Memory Footprint Reduction

schemes that signiﬁcantly reduce the memory con-
sumption of neural network training.

• We benchmark our approach on several tasks. We
show that it provides up to 40% memory reduction on
various tasks while maintaining the accuracy on par
with the model trained via the standard approach

2. Quantized Gradients of Activations

Figure 2. Computation graph of both forward and backward pass.
Orange and purple parts of the graph correspond to standard and
proposed ways of saving tensors for backward, respectively. Vector
xbit stands for the tensor saved using 2-bit quantization, while x
denotes its uncompressed version.

Gradients of activations using automatic differentiation.
Modern deep learning frameworks use the reverse mode au-
tomatic differentiation to calculate the gradients of the loss
over the model parameters. Forward computation can be
associated with a directed acyclic graph, depicted in Fig. 2.
Each operation f computes the output Xl+1 given the input
Xl and has to save some information Sl that would be used
on the backward pass in order to calculate the derivative
∂L/∂Xl from ∂L/∂Xl+1 and Sl. Thus, in a typical train-
ing loop, the intermediates Sl of all operations in the graph
are stored in the memory during the whole forward pass
until they are no longer needed after the completion of the
corresponding backward operation during backward pass.
This generates an additional memory, which can be quite
signiﬁcant and be larger than the total amount of parameters
of the model.

Pointwise activations.
In this paper, we focus on a point-
wise activation function, which is ubiquitous in modern
neural network architectures. Given an input tensor Xl we
apply a function f to each of the elements of this tensor:

f (Xl) = [f (Xj1,...,jk

l

)]j1,...,jk , f : R → R.

tention when analysing computational complexity. However,
standard implementation in such a framework as PyTorch
induces not a very small memory footprint and the whole
input Xl is saved for the backward pass.

The backward pass for such a function consists of element-
wise multiplication of the propagated gradient tensor by the
derivative of the nonlinearity function at the points of the
input tensor: if Xl+1 = f (Xl), then the gradient of the loss
L with respect to Xl is computed as

∂L
∂Xl

=

∂L
∂Xl+1

f (cid:48)(Xl),

(1)

where f (cid:48)(Xl) is the tensor with elements, consisting of the
derivative of f evaluated in each element of Xl. From (1),
it follows that for the backward pass we have to store only
f (cid:48)(Xl), and Xl is not needed.

ReLU activation function. To illustrate our idea, con-
sider one of the most popular nonlinearities, f (x) =
ReLU(x) = max(0, x). Its derivative f (cid:48) takes only two
values, 0 and 1 and it only require 1 bit to store. If single
precision is used, then the compression is 32, which is quite
noticeable.

GELU activation function.
In modern transformer archi-
tectures (Vaswani et al., 2017) the GELU (Hendrycks &
Gimpel, 2016) nonlinearity is typically used. The derivative
no longer takes two values. Instead, we propose to approxi-
mate f (cid:48) by a piecewise-constant function. For example, if
we allow 8 different values, we will need only 3 bits per
each element (Fig. 1).

Quantized gradients of activations.
In stochastic opti-
mization, if the gradient for a given batch is computed
approximately, the optimization may still converge. The
GELU derivative (see Fig. 1) is quite “similar” to a
piecewise-constant approximation: for large values of x,
it is almost exactly equal to 0 or 1, and for small values
of x, a rather interesting transition from 0 to 1 occurs. In-
stead of calculating the derivative exactly on the backward
pass, we approximate it using a certain piecewise-constant
approximation:

q(x|s, y) =

(cid:88)

i

yi1[x ∈ [si; si+1]]

(2)

As noted above, if the approximation has k constant inter-
vals, instead of storing the full input tensor X, it will be
possible to save only log k bits of information (per element
of the input tensor), which, accordingly, will reduce the
memory consumption by 32/ log k times for single preci-
sion.

This operation is very cheap compared to other operations in
the deep neural network model and does not attract much at-

Fig. 1 shows an example of an optimized 3-bit piecewise-
constant approximation for the GELU activation function.

  Tensors  saved  for backwardBackward passForward passSaveQuantize and  SaveQuantized tensors saved  for backwardFew-Bit Backward: Quantized Gradients of Activation Functions for Memory Footprint Reduction

Finding the optimal approximation parameters (boundaries
of intervals and values on them) is a challenging task. We
propose to ﬁnd them by minimizing the (weighted) L2 norm
of the error.

3. Optimal Piecewise-constant Approximation

Consider function f : R → R and its derivative f (cid:48). We
will measure the quality of a piecewise constant approxima-
tion (2) with a weighted L2 norm

min
y,s

L(s, y), L(s, y) =

R

(cid:90)

(f (cid:48)(x) − q(x|s, y))2w(x)dx,

(3)
where w is some weight function reﬂecting our prior knowl-
edge of the activation function argument distribution. Prac-
tical choices of w may be either 1[x ∈ [A; B]] (with some
reasonable A and B, which should be large enough) which
makes integral (3) tractable, or maybe, e.g., standard normal
distribution.

It is easy to see that the optimal value of y for L(s, y) with
given s is:

yi(s) =

(cid:82) si+1
si

w(x)f (cid:48)(x)dx

(cid:82) si+1
si

w(x)dx

.

(4)

The gradient of L(s, y(s)) w.r.t. the vector s then can be
derived analytically:

∂L
∂si

= (2f (cid:48)(si) − yi(s) − yi−1(s))(yi(s) − yi−1(s))w(si).

(5)

Using this formula, L(s, y) can be optimized using any
gradient-based method, and optimal piecewise-constant ap-
proximations can be found for the different number of bits
using standard optimization techniques.

Dynamic programming. The minimization problem (3)
has many local minima that are far from optimal. We sug-
gest using dynamic programming to get some good initial
approximation that can be ﬁnetuned using gradient-based
methods (but also can be used as is because it is very accu-
rate on its own).

We will assume that the weighting function w is chosen
such that w(x) = 0 for x (cid:54)∈ [A; B]. Consider an auxiliary
value

val [A; t]. The recurrent formula for this value is:

DP(t, k + 1) = min

t(cid:48)

DP(t(cid:48), k)+

+

(cid:90) t

t(cid:48)

(f (cid:48)(x) − y(t(cid:48), t))2w(x)dx,

(6)

y(t(cid:48), t) =

(cid:90) t

t(cid:48)

w(x)f (cid:48)(x)dx,

since a piecewise-constant approximation of size k + 1 con-
sists of corresponding approximation of size k (ﬁrst term)
plus one constant interval (second term). Here t(cid:48) chooses the
right bound of approximation of size k, and y(t(cid:48), t) stands
for the optimal value for the interval [t(cid:48); t] (4). Then the
minimal value of L(s, y) of size k is equal to DP(B, k).

To solve the minimization problem (6), we suggest consid-
ering the discretization of t: A = t0 < t1 < · · · < tn = B
and reducing the calculation of DP(t, k) to its approxima-
tion only in the points of discretization:

DP(i, k) = min

DP(j, k) + T (j, i),

j
(cid:90) ti

T (j, i) =

y(j, i) =

tj
(cid:82) ti
tj

w(x)f (cid:48)(x)dx
(cid:82) ti
tj

w(x)dx

.

(f (cid:48)(x) − y(j, i))2w(x)dx,

(7)

Both y(j, i) and T (j, i) can be calculated in advance us-
ing analytical formulas (if possible) or numerically for the
corresponding 1-dimensional integrals. After that, the full
array of DP(i, k) can be calculated in O(n2K) time and
O(n2) space, where K is the required number of constant
intervals in the approximation (2). Please note that this opti-
mization has to be performed only once, so n can be chosen
quite large thus the result would be very close to the global
minimum.

Note that the space complexity can be reduced to O(n) by
rewriting (7) as

(cid:90) ti

A
(cid:90) ti

F 2(i) =

W (i) =

f (cid:48)2(x)w(x)dx,

w(x)dx,

F W (i) =

A
(cid:90) ti

A

f (cid:48)(x)w(x)dx,

(8)

(cid:90) t

A

DP(t, k) = min
y1:k,
s1:k+1,
s1=A,
sk+1=t

t ∈ R, k ∈ N.

(f (cid:48)(x) − q(x|y, s))2w(x)dx,

y(j, i) = (F W (j) − F W (i))/(W (j) − W (i)),
T (j, i) = F 2(i) − F 2(j) − y(j, i)2(W (i) − W (j)).

Essentially, DP(t, k) is the optimal piecewise constant ap-
proximation of size k for the given function f (cid:48) on the inter-

We can see that ultimately only O(n) one-dimensional in-
tegrals have to be stored, and everything else can be easily
evaluated in O(1) time on the spot. The one-dimensional
integrals can be calculated numerically in O(n) time and

Few-Bit Backward: Quantized Gradients of Activation Functions for Memory Footprint Reduction

Figure 3. Examples of 3-bit approximations for derivatives of popular nonlinearities: (a) Swish, (b) SELU, and (c) Sigmoid. Please note
that the derivative of the sigmoid is an even function, thus we can quantize only positive real axis [0; + inf], which essentially doubles the
memory budget.

space complexity as well:

F 2(i + 1) = F 2(i) +

(cid:90) ti+1

f (cid:48)2(x)w(x)dx,

W (i + 1) = W (i) +

ti
(cid:90) ti+1

ti

w(x)dx,

(9)

F W (i + 1) = F W (i) +

(cid:90) ti+1

ti

f (cid:48)(x)w(x)dx.

Numerical results.
In Fig. 3, we provide some 3-bit ex-
amples for popular activation functions obtained with de-
scribed method, and in Table 3, we provide numerical values
of error (3) with uniform weight on interval [−10; 10]:

w(x) =

(cid:40)

1, if x ∈ [−10; 10]
0, otherwise

.

(10)

Note that the convergence is quite fast with respect to the
number of bits: the error drops by a factor of 2 − 4 when an
additional bit is added. It would be interesting to study the
convergence rates of such approximations.

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

ReLU
GELU
Swish
Sigmoid
Tanh
SELU
Softplus

1-bit
0.0
0.1410
0.2150
0.0181
0.1584
0.2554
0.2902

2-bits
-
0.0406
0.0479
0.0038
0.0319
0.1010
0.0541

3-bits
-
0.0119
0.0170
0.0009
0.0073
0.0184
0.0121

4-bits
-
0.0031
0.0045
0.0002
0.0017
0.0039
0.0029

Table 1. Numerical values of error (3) with uniform weight on
interval [-10; 10] (10).

With precalculated piecewise-constant approximation, drop-
in replacement for activation function f is very straightfor-
ward. On the forward pass, instead of the full tensor X, we
have to save only indices of intervals to which the elements
of X belong (which would take log k bits where k is the
number of intervals), and on the backward pass, we need to
multiply gradient w.r.t. output not with the actual derivative
of f , but with values from y corresponding to stored indices.
Pseudocode is presented in Alg. 1.

# Globally stored
# piecewise-constant
# approximation parameters
s = [...]
y = [...]

def forward(X):

X_pos = sortedsearch(s, X)
save_for_backward(X_pos)
return f(X)

def backward(dLdY):

X_pos = get_saved_for_backward()
return dLdY * y[X_pos]

Listing 1. Pseudo code for quantized backward layer. Arrays s

and y are parameters of quantization (2), sortedsearch is a
binary search method.

4. Experiments

The goal of our experiments is to show that quantized gradi-
ents give memory savings with no quality degradation. We
evaluate our method on language models on several open
benchmarks. As Few-bit backward method is a drop-in
replace solution, no hyperparameter optimization was per-
formed; all hyperparameters are the same across compared

Few-Bit Backward: Quantized Gradients of Activation Functions for Memory Footprint Reduction

Figure 4. ResNet18 with ReLU replaced with either GELU (a, b, c) or Swish (d, e, f) nonlinearity trained on Imagenet. (a,d): Training
loss. (b,e): Training loss during the last third of the training process. (c,f): Final validation top-1 accuracy. All plots are averaged across
three runs with different seeds. Error bars mean minimum and maximum values.

methods and are taken from the corresponding sources.

In Table 4 we report results for RoBERTa-base model (Liu
et al., 2019) on GLUE benchmark (Wang et al., 2019) for
standard GELU and 1-, 2-, 3- and 4-bits GELU. Both task-
relevant metrics and the ﬁnal value of the loss function are
reported. 1- and 2-bits versions have minor performance
degradation, while 3- and 4-bits GELU have no visible
difference and closely match vanilla GELU performance
both in terms of performance metric and loss function while
saving 15% and 14% memory respectively 4.

To further examine the inﬂuence of backward GELU quanti-
zation on stochastic optimization, we consider the behaviour
of loss function during training, depicted in Fig. 4. We can
clearly see that our method repeats the dynamic of standard
GELU both on train and validation sets. 1- and 2-bit ver-
sions performs a little worse, while 3- and 4-bit versions
are hardly distinguishable from the standard GELU. This
allows concluding that such quantization is not noticeable
for stochastic optimization and can be applied without loss
of quality and ﬁnal properties of the model.

In Fig. 5 we present training dynamic of ruDALL-E1 Male-
vich (Ramesh et al., 2021) model on Russian Emoji dataset.
The dataset (Shonenkov et al., 2021) contains 2749 unique
emoji icons and 1611 unique texts that were collected by
web scrapping (the difference in quantities is due to the
fact that there are sets, within which emojis differ only in
color, moreover, some elements are homonyms in Russian).
ruDALL-E Malevich is a big multimodal pretrained trans-
former, which learns the conditional distribution of images
given some string of text (more precisely it autoregressively
models the text and image tokens as a single stream of
data). ruDALL-E Malevich encoder part is a 24 layer Trans-
former (Vaswani et al., 2017) model with 16 attention heads,
2048 hidden dimensions and standard GELU nonlinearity,
which in total has 1.3B parameters. It works with 128 text
tokens, which are prepared from the text input using YTTM
tokenizer2, and 1024 image tokens, which are obtained after
encoding the input image using Sber-VQGAN3. Quantized

1Implementation is taken from https://github.com/sberbank-

ai/ru-dalle

2Implementation is taken from https://github.com/

VKCOM/YouTokenToMe

3Implementation is taken from https://github.com/

Few-Bit Backward: Quantized Gradients of Activation Functions for Memory Footprint Reduction

Figure 5. Dynamic of loss values in ﬁnetuning of ruDALL-E Malevich with few-bit GELU activations. All experiments have following
setup: train size 2474, valid size 275, loss image weight 1000, frozen MLP and attention layers, batch size 40, start lr 4e-7, max lr 1e-5,
ﬁnal lr 2e-8, warmup 0.1, 8bit-Adam (Dettmers et al., 2021), weight decay 0.2, betas (0.9, 0.98), eps 1e-6, gradient checkpointing 24,
trained for 6h using 1xA100.

Table 2. RoBERTa-base on GLUE benchmark with different quantization budgets. Metric: mean accuracy/correlation (task speciﬁc).
Averaged across ﬁve runs.

1-bit GELU
0.906 (± 0.002)
stsb
mnli-mm 0.870 (± 0.001)
0.880 (± 0.009)
mrpc
0.595 (± 0.016)
cola
0.873 (± 0.001)
mnli
0.939 (± 0.003)
sst2
0.752 (± 0.021)
rte
0.914 (± 0.001)
qqp
0.925 (± 0.002)
qnli

2-bits GELU
0.907 (± 0.002)
0.870 (± 0.002)
0.884 (± 0.008)
0.580 (± 0.014)
0.872 (± 0.002)
0.938 (± 0.003)
0.756 (± 0.023)
0.915 (± 0.000)
0.925 (± 0.002)

3-bits GELU
0.910 (± 0.002)
0.871 (± 0.002)
0.884 (± 0.007)
0.596 (± 0.015)
0.874 (± 0.001)
0.941 (± 0.004)
0.780 (± 0.014)
0.916 (± 0.001)
0.926 (± 0.002)

4-bits GELU
0.909 (± 0.002)
0.870 (± 0.001)
0.885 (± 0.008)
0.607 (± 0.014)
0.874 (± 0.002)
0.941 (± 0.003)
0.771 (± 0.025)
0.916 (± 0.001)
0.927 (± 0.002)

Vanila GELU
0.909 (± 0.001)
0.871 (± 0.002)
0.882 (± 0.005)
0.604 (± 0.013)
0.874 (± 0.001)
0.943 (± 0.002)
0.771 (± 0.017)
0.916 (± 0.001)
0.927 (± 0.002)

backward for ruDALL-E Malevich shows same behaviour
as for RoBERTa-base architecture: 1- and 2-bit versions,
although coping with training perfectly ﬁne, demonstrates
minor performance degradation, while 3- and 4-bit versions
are indistinguishable from the original GELU.

ResNet Architecture. To explore the inﬂuence of back-
ward quantization on architectures other than Transformer
and nonlinearities other than GELU, we trained ResNet18
model (He et al., 2016) on ImageNet (Russakovsky et al.,
2015) benchmark (Leclerc et al., 2022) dataset with ReLU
replaced with Swish function (Swish(x) = xσ(x), where
σ is a sigmoid function) or with GELU, see Fig. 4. The
behaviour of our quantization scheme remains the same for
different network architectures and nonlinearities: 1- and 2-
bits have minor performance drop, while 3- and 4- bits are
on par with unchanged nonlinearity.

sberbank-ai/sber-vq-gan

ActNN. As a baseline, we use another quantization
scheme ActNN (Chen et al., 2021). It works in a much
wider spectrum of situations, as it can quantize not only
pointwise nonlinearity layers but also all kinds of linear
layers (convolutional and dense layers), normalization lay-
ers and pooling layers. Without going deep into details,
ActNN divides the saved tensor H into chunks hi where
each chunk is of an equal size G. Then, given the quan-
tization budget of b bits, each chunk hi is normalized:
ui = 2b(hi − min{hi})/(max{hi} − min{hi}), and its
randomly quantized version is saved ¯ui = (cid:100)ui(cid:101) with prob.
u − (cid:98)ui(cid:99), (cid:98)ui(cid:99) otherwise. Random rounding is performed
in order to guarantee that the quantization is unbiased. For
each group, two additional values min{hi} and max{hi}
are saved as well, but for the group size of G = 256 it is
only 0.125 additional bits per element, which we ignore in
our following tests. ActNN by construction does not take
into account the global behaviour of the nonlinearity deriva-
tive. We argue that for nonlinearity layers, it is very crucial,

Few-Bit Backward: Quantized Gradients of Activation Functions for Memory Footprint Reduction

Figure 6. RoBERTa-base on QQP task from GLUE benchmark. (a): Train loss. (b): Train loss during the last third of the training. (c):
Validation loss. Averaged across 10 runs.

and thus our preoptimized quantization scheme is more
preferable. To prove that, we consider ActNN behaviour
on the QQP task from the GLUE benchmark with respect
to different quantization budgets and compare it with our
method. In Fig. 8 train and validation losses are shown, and
in Table 4 we report the ﬁnal accuracy for the QQP task. In
general, our method with 1 bit less budget works the same
or better than ActNN.

ActNN
0.8880 ± 0.0008
0.9072 ± 0.0005
0.9106 ± 0.0003
0.9113 ± 0.0006

Our
0.9080 ± 0.0006
0.9097 ± 0.0006
0.9114 ± 0.0007
0.9112 ± 0.0005

1-bit
2-bit
3-bit
4-bit

Memory saving. Table 4 shows memory measurements
for different models with different number of bits per ele-
ment for backward quantization. As was shown experimen-
tally, for many tasks 3 bits is already enough for lossless
In practice, fewer bits may be chosen to even
training.
further increase the batch size and consequently speed up
training or to ﬁt a larger model on a device that did not ﬁt it
before.

5. Related Work

The reduction of the memory footprint is an important topic.
To save memory during training, in addition to working
with stored activations, the memory used to store model
parameters can be compressed. Quantization (Bondarenko
et al., 2021; Bengio et al., 2013; Banner et al., 2019; Ja-
cob et al., 2018; Nagel et al., 2021; Krishnamoorthi, 2018)
limits the admissible values of weights to some small ﬁnite
set. Thus less memory is needed for storage. The low-rank
representation of weights (Hrinchuk et al., 2020; Phan et al.,

Table 3. Peak memory usage in training time during ﬁne-tuing on
GLUE.
Task
MRPC 128

16

Batch BitWidth Mem, GiB Saving, %
0.0
13.8
14.4
0.0
8.2
8.4
0.0
15.5
16.1

Vanilla
3-bit
2-bit
Vanilla
3-bit
2-bit
Vanilla
3-bit
2-bit

11.34
9.78
9.71
11.73
10.77
10.75
13.31
11.25
11.17

256

QNLI

STS2

2020; Gusak et al., 2019; 2021; Cui et al., 2020; Novikov
et al., 2018; Lebedev et al., 2015) assumes some internal
structure of model weights and saves memory by explicitly
using this structure with low-rank methods from linear alge-
bra. Low precision learning and low precision optimizers
focus on using the lower precision ﬂoats to store weights,
optimization parameters, and model gradients. All of these
approaches are complementary to the proposed one and can
be used together.

Checkpointing (Beaumont et al., 2019; 2021; Chen et al.,
2016) methods save memory by the cost of more calcula-
tions. It stores a fewer number of activations and repeats
the calculation of the rest from the saved checkpoints. Of-
ﬂoading methods (Beaumont et al., 2020) send the saved
activations to the computer’s RAM and load them back to
the video memory on the backwards passes, which also
saves GPU memory at the cost of host-device communica-
tion time.

ActNN (Chen et al., 2021) is a framework for quantizing
stored activations adaptively on the ﬂy. In contrast to our
work, it allows quantizing not only layers of element-by-

Few-Bit Backward: Quantized Gradients of Activation Functions for Memory Footprint Reduction

Figure 7. Comparison of RoBERTa-base on QQP task from GLUE benchmark with ActNN quantization and our quantization schemes.
(a): Train loss. (b): Train loss during the last third of the training. (c): Validation loss. Averaged across ten runs.

Model
ResNet-101
256x256 size

DenseNet-121
256x256 size

Efﬁcient Net B7
256x256 size

RoBERTa-base
256 seq. len

RoBERTa-large
256 seq. len

GPT2
256 seq. len

Quant

Saving Max Batch Size

1-bit
2-bit
3-bit
4-bit

1-bit
2-bit
3-bit
4-bit

1-bit
2-bit
3-bit
4-bit

1-bit
2-bit
3-bit
4-bit

1-bit
2-bit
3-bit
4-bit

1-bit
2-bit
3-bit
4-bit

30%
29%
28%
27%

31%
30%
29%
28%

26%
25%
24%
23%

16%
15%
15%
14%

16%
16%
15%
15%

42%
41%
39%
38%

131
170 (+29.8%)
169 (+29.0%)
167 (+27.5%)
165 (+26.0%)
126
165 (+31.0%)
164 (+30.2%)
162 (+28.6%)
161 (+27.8%)
47
59 (+25.5%)
58 (+23.4%)
58 (+23.4%)
57 (+21.3%)
154
179 (+16.2%)
178 (+15.6%)
177 (+14.9%)
176 (+14.3%)
54
63 (+16.7%)
63 (+16.7%)
62 (+14.8%)
62 (+14.8%)
83
117 (+41.0%)
116 (+39.8%)
114 (+37.3%)
113 (+36.1%)

Table 4. Memory savings and maximum batch size for popular
models for different quantization budget. You can ﬁnd more de-
tailed version in Appendix B

element activations but also many others, including con-
volutional, normalization and linear layers. However, this
method depends on the distribution of elements of quan-
tizable tensors and, because of that, its performance may
degrade. Our approach, on the other hand, selects data-
agnostic optimal quantization, which in practice turns out
to be sufﬁcient and easier to use.

6. Conclusion

We have proposed a method to reduce memory consump-
tion during the training of deep neural network models by
storing less information for backward pass in the element-
wise activation functions. For effective training, there is no
need to calculate the derivative of the activation functions
precisely, but only its piecewise-constant approximation
is sufﬁcient. This makes it possible to save not the entire
input tensor at each application of the activation function,
but only the interval number in the piecewise-constant ap-
proximation. Experiments show that for a wide class of
models and problems, storing only 3 bits of information per
tensor element does not lead to degradation of the learning
quality and saves about 20 percent of memory. We have
proposed an efﬁcient algorithm for constructing an optimal
piecewise-constant approximation. The proposed drop-in re-
placements for popular activation functions (ReLU, GELU,
Swish, Sigmoid and others) do not depend on the neural net-
work model, the problem to be solved, or the peculiarities of
data distribution. The replacement of the original activation
functions by the proposed method can be performed at any
training stage (both to models trained from scratch and to
pre-trained models for subsequent ﬁne-tuning) and does not
require any changes in the training pipelines. An efﬁcient
CUDA implementation of the proposed method, together
with pre-computed piecewise-constant approximations for
many popular activation functions, is available for PyTorch
at GitHub repository4.

4Source code repository can be found at https://github.

com/SkoltechAI/fewbit.

Few-Bit Backward: Quantized Gradients of Activation Functions for Memory Footprint Reduction

7. Acknowledgements

The work was
ter under
000000D730321P5Q0002, Grant No.
02.11.2021).

supported by the Analytical cen-
(subsidy agreement
70-2021-00145

the RF Government

References

Banner, R., Nahshan, Y., and Soudry, D. Post train-
ing 4-bit quantization of convolutional networks for
rapid-deployment. In Wallach, H. M., Larochelle, H.,
Beygelzimer, A., d’Alch´e-Buc, F., Fox, E. B., and
Garnett, R. (eds.), Advances in Neural Information
Processing Systems 32: Annual Conference on Neural
Information Processing Systems 2019, NeurIPS 2019,
December 8-14, 2019, Vancouver, BC, Canada, pp. 7948–
URL https://proceedings.
7956,
neurips.cc/paper/2019/hash/
c0a62e133894cdce435bcb4a5df1db2d-Abstract.
html.

2019.

Beaumont, O., Eyraud-Dubois, L., Herrmann, J., Joly, A.,
and Shilova, A. Optimal checkpointing for heterogeneous
chains: how to train deep neural networks with limited
memory. CoRR, abs/1911.13214, 2019. URL http:
//arxiv.org/abs/1911.13214.

Beaumont, O., Eyraud-Dubois, L., and Shilova, A. Op-
timal GPU-CPU ofﬂoading strategies for deep neural
network training.
In Malawski, M. and Rzadca, K.
(eds.), Euro-Par 2020: Parallel Processing - 26th In-
ternational Conference on Parallel and Distributed Com-
puting, Warsaw, Poland, August 24-28, 2020, Proceed-
ings, volume 12247 of Lecture Notes in Computer Sci-
ence, pp. 151–166. Springer, 2020.
doi: 10.1007/
978-3-030-57675-2\ 10. URL https://doi.org/
10.1007/978-3-030-57675-2_10.

Beaumont, O., Eyraud-Dubois, L., and Shilova, A. Efﬁcient
combination of rematerialization and ofﬂoading for train-
ing dnns. Advances in Neural Information Processing
Systems, 34, 2021.

Bengio, Y., L´eonard, N., and Courville, A. C. Estimating
or propagating gradients through stochastic neurons for
conditional computation. CoRR, abs/1308.3432, 2013.
URL http://arxiv.org/abs/1308.3432.

Bondarenko, Y., Nagel, M., and Blankevoort, T. Under-
standing and overcoming the challenges of efﬁcient trans-
former quantization. In Moens, M., Huang, X., Specia, L.,
and Yih, S. W. (eds.), Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Processing,
EMNLP 2021, Virtual Event / Punta Cana, Dominican
Republic, 7-11 November, 2021, pp. 7947–7969. Associa-
tion for Computational Linguistics, 2021. doi: 10.18653/

v1/2021.emnlp-main.627. URL https://doi.org/
10.18653/v1/2021.emnlp-main.627.

Chen, J., Zheng, L., Yao, Z., Wang, D., Stoica, I., Mahoney,
M. W., and Gonzalez, J. Actnn: Reducing training mem-
ory footprint via 2-bit activation compressed training. In
Meila, M. and Zhang, T. (eds.), Proceedings of the 38th
International Conference on Machine Learning, ICML
2021, 18-24 July 2021, Virtual Event, volume 139 of
Proceedings of Machine Learning Research, pp. 1803–
1813. PMLR, 2021. URL http://proceedings.
mlr.press/v139/chen21z.html.

Chen, T., Xu, B., Zhang, C., and Guestrin, C. Train-
ing deep nets with sublinear memory cost. CoRR,
abs/1604.06174, 2016. URL http://arxiv.org/
abs/1604.06174.

Cui, C., Zhang, K., Daulbaev, T., Gusak, J., Oseledets,
I. V., and Zhang, Z. Active subspace of neural net-
works: Structural analysis and universal attacks. SIAM
J. Math. Data Sci., 2(4):1096–1122, 2020. doi: 10.1137/
19M1296070. URL https://doi.org/10.1137/
19M1296070.

Dettmers, T., Lewis, M., Shleifer, S., and Zettlemoyer,
L. 8-bit optimizers via block-wise quantization. CoRR,
abs/2110.02861, 2021. URL https://arxiv.org/
abs/2110.02861.

Gao, Y., Liu, Y., Zhang, H., Li, Z., Zhu, Y., Lin, H., and
Yang, M. Estimating gpu memory consumption of deep
learning models. In Proceedings of the 28th ACM Joint
Meeting on European Software Engineering Conference
and Symposium on the Foundations of Software Engineer-
ing, pp. 1342–1352, 2020.

Gusak, J., Kholyavchenko, M., Ponomarev, E., Mar-
keeva, L., Blagoveschensky, P., Cichocki, A., and Os-
eledets, I. V. Automated multi-stage compression of
neural networks. In 2019 IEEE/CVF International Con-
ference on Computer Vision Workshops, ICCV Work-
shops 2019, Seoul, Korea (South), October 27-28, 2019,
pp. 2501–2508. IEEE, 2019. doi: 10.1109/ICCVW.
2019.00306. URL https://doi.org/10.1109/
ICCVW.2019.00306.

Gusak, J., Daulbaev, T., Ponomarev, E., Cichocki, A., and
Oseledets, I. Reduced-order modeling of deep neural
networks. Computational Mathematics and Mathematical
Physics, 61(5):774–785, 2021.

He, K., Zhang, X., Ren, S., and Sun, J. Deep resid-
In 2016 IEEE
ual learning for image recognition.
Conference on Computer Vision and Pattern Recogni-
tion, CVPR 2016, Las Vegas, NV, USA, June 27-30,
2016, pp. 770–778. IEEE Computer Society, 2016. doi:

Few-Bit Backward: Quantized Gradients of Activation Functions for Memory Footprint Reduction

10.1109/CVPR.2016.90. URL https://doi.org/
10.1109/CVPR.2016.90.

2021.
08295.

URL https://arxiv.org/abs/2106.

Hendrycks, D. and Gimpel, K. Bridging nonlinearities and
stochastic regularizers with gaussian error linear units.
CoRR, abs/1606.08415, 2016. URL http://arxiv.
org/abs/1606.08415.

Hrinchuk, O., Khrulkov, V., Mirvakhabova, L., Orlova,
E. D., and Oseledets, I. V. Tensorized embedding layers.
In Cohn, T., He, Y., and Liu, Y. (eds.), Findings of the As-
sociation for Computational Linguistics: EMNLP 2020,
Online Event, 16-20 November 2020, volume EMNLP
2020 of Findings of ACL, pp. 4847–4860. Association
for Computational Linguistics, 2020. doi: 10.18653/v1/
2020.ﬁndings-emnlp.436. URL https://doi.org/
10.18653/v1/2020.findings-emnlp.436.

Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard,
A. G., Adam, H., and Kalenichenko, D. Quantization
and training of neural networks for efﬁcient integer-
arithmetic-only inference. In 2018 IEEE Conference on
Computer Vision and Pattern Recognition, CVPR 2018,
Salt Lake City, UT, USA, June 18-22, 2018, pp. 2704–
2713. Computer Vision Foundation / IEEE Computer
Society, 2018. doi: 10.1109/CVPR.2018.00286. URL
http://openaccess.thecvf.com/content_
cvpr_2018/html/Jacob_Quantization_
and_Training_CVPR_2018_paper.html.

Krishnamoorthi, R. Quantizing deep convolutional net-
works for efﬁcient inference: A whitepaper. CoRR,
abs/1806.08342, 2018. URL http://arxiv.org/
abs/1806.08342.

Lebedev, V., Ganin, Y., Rakhuba, M., Oseledets, I. V., and
Lempitsky, V. S. Speeding-up convolutional neural net-
works using ﬁne-tuned cp-decomposition.
In Bengio,
Y. and LeCun, Y. (eds.), 3rd International Conference
on Learning Representations, ICLR 2015, San Diego,
CA, USA, May 7-9, 2015, Conference Track Proceed-
ings, 2015. URL http://arxiv.org/abs/1412.
6553.

Leclerc, G., Ilyas, A., Engstrom, L., Park, S. M., Salman,
ffcv. https://github.com/

H., and Madry, A.
libffcv/ffcv/, 2022. commit xxxxxxx.

Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy,
O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. Roberta:
A robustly optimized BERT pretraining approach. CoRR,
abs/1907.11692, 2019. URL http://arxiv.org/
abs/1907.11692.

Nagel, M., Fournarakis, M., Amjad, R. A., Bondarenko, Y.,
van Baalen, M., and Blankevoort, T. A white paper on
neural network quantization. CoRR, abs/2106.08295,

Novikov, A., Troﬁmov, M., and Oseledets, I. Exponential
machines. Bulletin of the Polish Academy of Sciences:
Technical Sciences, pp. 789–797, 2018.

Ojika, D., Patel, B., Reina, G. A., Boyer, T., Martin, C., and
Shah, P. Addressing the memory bottleneck in AI model
training. arXiv preprint arXiv:2003.08732, 2020.

Phan, A. H., Sobolev, K., Sozykin, K., Ermilov, D., Gusak,
J., Tichavsk´y, P., Glukhov, V., Oseledets, I. V., and Ci-
chocki, A. Stable low-rank tensor decomposition for
compression of convolutional neural network. In Vedaldi,
A., Bischof, H., Brox, T., and Frahm, J. (eds.), Com-
puter Vision - ECCV 2020 - 16th European Confer-
ence, Glasgow, UK, August 23-28, 2020, Proceedings,
Part XXIX, volume 12374 of Lecture Notes in Computer
Science, pp. 522–539. Springer, 2020. doi: 10.1007/
978-3-030-58526-6\ 31. URL https://doi.org/
10.1007/978-3-030-58526-6_31.

Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C.,
Radford, A., Chen, M., and Sutskever, I. Zero-shot
text-to-image generation.
In Meila, M. and Zhang, T.
(eds.), Proceedings of the 38th International Confer-
ence on Machine Learning, ICML 2021, 18-24 July
2021, Virtual Event, volume 139 of Proceedings of Ma-
chine Learning Research, pp. 8821–8831. PMLR, 2021.
URL http://proceedings.mlr.press/v139/
ramesh21a.html.

Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh,
S., Ma, S., Huang, Z., Karpathy, A., Khosla, A.,
Ima-
Bernstein, M. S., Berg, A. C., and Fei-Fei, L.
Int. J.
genet large scale visual recognition challenge.
Comput. Vis., 115(3):211–252, 2015. doi: 10.1007/
s11263-015-0816-y. URL https://doi.org/10.
1007/s11263-015-0816-y.

Shonenkov, A., Bakshandaeva, D., Dimitrov, D., and
Nikolich, A. Emojich - zero-shot emoji generation
using russian language: a technical report. CoRR,
abs/2112.02448, 2021. URL https://arxiv.org/
abs/2112.02448.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,
Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I.
Attention is all you need. In Guyon, I., von Luxburg, U.,
Bengio, S., Wallach, H. M., Fergus, R., Vishwanathan,
S. V. N., and Garnett, R. (eds.), Advances in Neural
Information Processing Systems 30: Annual Conference
Information Processing Systems 2017,
on Neural
December 4-9, 2017, Long Beach, CA, USA, pp. 5998–
URL https://proceedings.
6008,
neurips.cc/paper/2017/hash/

2017.

Few-Bit Backward: Quantized Gradients of Activation Functions for Memory Footprint Reduction

3f5ee243547dee91fbd053c1c4a845aa-Abstract.
html.

Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and
Bowman, S. R. GLUE: A multi-task benchmark and
analysis platform for natural language understanding. In
7th International Conference on Learning Representa-
tions, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.
OpenReview.net, 2019. URL https://openreview.
net/forum?id=rJ4km2R5t7.

Few-Bit Backward: Quantized Gradients of Activation Functions for Memory Footprint Reduction

Few-Bit Backward: Quantized Gradients of Activation Functions for Memory Footprint Reduction

A. Detailed examples of few-bit approximations for popular nonlinearity layers

Figure 8. 1- to 4-bit approximations of popular nonlinearty layers.

Few-Bit Backward: Quantized Gradients of Activation Functions for Memory Footprint Reduction

B. Detailed memory measurements for different models

We provide memory measurements for different model architectures in Table B. ”Model size” is the total memory used for
storing model parameters (without model gradients and optimizator statistics). ”All activations size” is the total memory
used by tensors, saved for backward pass. ”Nonlinearity activations size” is the part of all activations used by nonlinearity
layers. Maximum batch size is calculated with the assumption, that three model copies are stored on the device (model
parameters, model gradients and optimizer statistics like weight moments in SGD with momentum).

All
Activations
Size
(Mb)

Nonlinearity
Activations
Size
(Mb)

1-bit
Saving
Max batch size

2-bit
Saving
Max batch size

3-bit
Saving
Max batch size

4-bit
Saving
Max batch size

Model
Size
(Mb)

44.6

99.2

ResNet-18

ResNet-50

40.0

156.8

11.5

47.9

73.4

ResNet-101

171.4

234.5

ResNet-152

232.3

328.2

104.9

DenseNet-121

30.9

243.8

79.1

DenseNet-161

112.4

457.2

145.3

DenseNet-169

DenseNet-201

54.7

77.4

296.3

382.2

Efﬁcient Net B0

20.4

112.4

Efﬁcient Net B3

47.5

218.6

95.3

123.9

32.4

59.5

Efﬁcient Net B7

256.3

673.5

178.9

VGG 11

507.2

100.9

VGG 16

528.2

163.8

VGG 19

548.4

178.8

RoBERTa-base

480.7

219.6

RoBERTa-large

1355.6

578.1

37.0

68.5

75.0

36.0

96.0

GPT2

491.0

331.1

144.0

28%
1010 (+27.5%)
30%
256 (+29.3%)
30%
170 (+29.8%)
31%
121 (+31.5%)
31%
165 (+31.0%)
31%
87 (+29.9%)
31%
136 (+30.8%)
31%
105 (+31.2%)
28%
360 (+27.7%)
26%
185 (+26.7%)
26%
59 (+25.5%)
36%
386 (+35.4%)
41%
237 (+40.2%)
41%
217 (+40.9%)
16%
179 (+16.2%)
16%
63 (+16.7%)
42%
117 (+41.0%)

27%
1001 (+26.4%)
29%
254 (+28.3%)
29%
169 (+29.0%)
30%
120 (+30.4%)
30%
164 (+30.2%)
30%
87 (+29.9%)
30%
134 (+28.8%)
30%
104 (+30.0%)
27%
357 (+26.6%)
26%
183 (+25.3%)
25%
58 (+23.4%)
34%
382 (+34.0%)
39%
234 (+38.5%)
39%
214 (+39.0%)
15%
178 (+15.6%)
16%
63 (+16.7%)
41%
116 (+39.8%)

26%
992 (+25.3%)
28%
252 (+27.3%)
28%
167 (+27.5%)
29%
119 (+29.3%)
29%
162 (+28.6%)
29%
86 (+28.4%)
29%
133 (+27.9%)
29%
103 (+28.8%)
26%
354 (+25.5%)
25%
182 (+24.7%)
24%
58 (+23.4%)
33%
377 (+32.3%)
38%
231 (+36.7%)
38%
211 (+37.0%)
15%
177 (+14.9%)
15%
62 (+14.8%)
39%
114 (+37.3%)

25%
984 (+24.2%)
27%
249 (+25.8%)
27%
165 (+26.0%)
28%
117 (+27.2%)
28%
161 (+27.8%)
28%
85 (+26.9%)
28%
132 (+26.9%)
28%
102 (+27.5%)
25%
351 (+24.5%)
24%
180 (+23.3%)
23%
57 (+21.3%)
32%
373 (+30.9%)
37%
228 (+34.9%)
37%
208 (+35.1%)
14%
176 (+14.3%)
15%
62 (+14.8%)
38%
113 (+36.1%)

