Mitigating Bias in Set Selection with Noisy Protected Attributes

Anay Mehrotra
Yale University

L. Elisa Celis
Yale University

February 23, 2021

Abstract

Subset selection algorithms are ubiquitous in AI-driven applications, including, online recruiting
portals and image search engines, so it is imperative that these tools are not discriminatory on the basis
of protected attributes such as gender or race. Currently, fair subset selection algorithms assume that the
protected attributes are known as part of the dataset. However, protected attributes may be noisy due
to errors during data collection or if they are imputed (as is often the case in real-world settings). While
a wide body of work addresses the eﬀect of noise on the performance of machine learning algorithms, its
eﬀect on fairness remains largely unexamined. We ﬁnd that in the presence of noisy protected attributes,
in attempting to increase fairness without considering noise, one can, in fact, decrease the fairness of the
result!

Towards addressing this, we consider an existing noise model in which there is probabilistic informa-
tion about the protected attributes (e.g., [58, 34, 20, 46]), and ask is fair selection possible under noisy
conditions? We formulate a “denoised” selection problem which functions for a large class of fairness
metrics; given the desired fairness goal, the solution to the denoised problem violates the goal by at
most a small multiplicative amount with high probability. Although this denoised problem turns out
to be NP-hard, we give a linear-programming based approximation algorithm for it. We evaluate this
approach on both synthetic and real-world datasets. Our empirical results show that this approach can
produce subsets which signiﬁcantly improve the fairness metrics despite the presence of noisy protected
attributes, and, compared to prior noise-oblivious approaches, has better Pareto-tradeoﬀs between utility
and fairness.

1
2
0
2

b
e
F
2
2

]

Y
C
.
s
c
[

2
v
9
1
2
4
0
.
1
1
0
2
:
v
i
X
r
a

1

 
 
 
 
 
 
Contents

1 Introduction

1.1 Our contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 Model

2.1 Selection problem and noise model
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 Target problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3 Denoised problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.4 Group-level noise model

3 Theoretical results

3.1 Proof overview and hardness results

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4 Empirical results

4.1 Setup and metrics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 Synthetic data with disparate error-rates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.3 Synthetic data with disparate utilities
4.4 Real-world data for candidate selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.5 Real-world data for image search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.6 Additional empirical results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5 Proofs

5.1 Proof of Theorem 3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.2 Hardness results

6 Theoretical results with multiple protected attributes

7 Limitations and future work

8 Conclusion

A Extended empirical results

3
3
4

5
5
6
6
7

7
8

10
10
11
12
14
16
18

18
19
21

24

27

27

32

2

1

Introduction

The subset selection problem arises in various contexts including online job portals (where an algorithm
shortlists candidates to show to the recruiter), university admissions (where a panel admits a subset of
students), and online search (where the platform selects a subset of the results in response to a user query)
[27, 48, 50, 66]. The basic problem is as follows: There are m items, and each item i ∈ [m] has a utility wi ≥ 0,
i.e., the value it adds to the subset. The goal is to select a subset of n (cid:28) m items which has the largest
total utility. Given the pervasiveness of subset selection tasks, it is crucial to ensure that subset selection
algorithms do not propagate social biases. Consequently, there has been extensive work on developing fair
algorithms for selection (and for the related problem of ranking); see [27, 12] for an overview. Many of these
approaches ensure that the number of individuals selected from diﬀerent socially salient groups (e.g., those
deﬁned by gender or race) satisfy some fairness constraints and/or improve along a given fairness metric.
Towards this, these algorithms assume (exact) access to the corresponding protected attributes of individuals.
However, in practice, these attributes can be erroneous, unavailable for some individuals, or missing
entirely [26, 52, 64]. For instance, in healthcare, patients’ ethnic information can be incorrectly recorded [64]
or left blank [26].1 When this data is missing, probabilistic methods based on other proxy information are
used to “impute” these protected attributes [23, 32, 29, 28]. For instance, when assessing if lenders comply
with fair lending policies, the Consumer Financial Protection Bureau uses last name and geolocation to
impute consumers’ race [8]. Similar approaches have also been used in the context of healthcare [33, 51].
Additionally, online job platforms (such as, LinkedIn) use a user’s data to infer their demographic information
based on the data they have on other users [56]. Furthermore, in some cases, such as with images on the
internet, protected attributes are missing for the entire datasets (and labeling all images is not viable).
Inferring protected attributes is bound to have errors, which can aﬀect the groups diﬀerently [7]. Thus,
using imputed attributes as a black-box in subsequent fair algorithms, without accounting for their noise,
can have an unexpected (and adverse) impact on the fairness achieved. For instance, [54, 14] observe that
(noise oblivious) fair algorithms do not satisfy their fairness guarantee in the presence of noise.

To gain some intuition, consider the setting where we are given a set of candidates and would like to
ensure proportional representation across individuals with diﬀerent skin-tones, coded as White and non-
White. Assume that the utilities of all candidates have a similar distribution, and so picking candidates with
top n utilities proportionately represents them. Further, assume that the labels have a higher amount of
noise for non-Whites than Whites.2 One can show that, any “fair algorithm” which assumes that these noisy
labels are correct, and selects a proportionate number of White and non-White candidates based on them,
would violate proportional representation. In this case, adding fairness constraints increased the disparity.
This leads us to the question addressed in this paper:

Can we develop a framework for selection which outputs an approximately fair subset despite noisy
protected attributes?

1.1 Our contributions

Building on prior work on fairness constraints [16, 66], we develop a framework for fair selection in the
presence of noisy protected attributes. This framework allows for multiple and intersectional groups, and,
given access to (unbiased3) probabilistic information about the true protected attributes, it can satisfy a
large class of fairness constraints (including, demographic parity, proportional representation, and the 80%
rule) with high probability.

Formally, we would like to solve an ideal optimization problem (Program Target) which satisﬁes the
fairness constraints for the true (and unknown) protected attributes. Such problems have been studied by
prior works, e.g., [73, 65, 66]. However, since we do not have the true protected attributes, we cannot solve
it directly using their approaches. Instead, we formulate a “denoised” problem (Program Denoised); such
that, an optimal solution of Program Denoised has the optimal utility for Program Target and violates

1Recently, this received public attention when attempting to estimate the racial disparities in COVID19 infections showed

large discrepancies [5].

2For instance, as observed in commercial image-based gender classiﬁers [7].
3Here, unbiased refers to the statistical notion of an unbiased estimator.

3

the fairness constraints of Program Target by at most a small multiplicative factor with high probability
(Lemma 3.3). Although Program Denoised turns out to be NP-hard (Theorem 3.5), we develop a linear-
programming based approximation algorithm for it. This, in turn, implies an approximation algorithm for
Program Target.

We empirically study the fairness achieved by this approach with respect to standard fairness metrics
(e.g., risk diﬀerence) on both synthetic and real-world datasets. We also study the performance of existing
fair algorithms in the presence of noise and benchmark our approach with them. We observe that our
approach achieves the highest fairness and has a Pareto-optimal tradeoﬀ between utility and fairness (on
changing the strength of constraints).
Interestingly, these observations also hold in our empirical results
where, unlike what our theoretical results assume, we have skewed probabilistic information of the noisy
attributes. Finally, our empirical results hint at potential applications of this approach, e.g., in online
recruiting portals and image search engines.

1.2 Related work

Mitigating bias. An extensive body of work strives to mitigate bias and improve diversity in subset
selection and the closely related ranking problem. We refer the reader to [27] for a comprehensive overview
of work on diverse selection, and an excellent talk [12] which discusses work on curtailing bias in rankings.
Closest to our setting, are approaches which use protected attributes to impose fairness constraints on
algorithms for selection [48, 66] and ranking [73, 18, 65, 35, 70]. However, if the attributes are noisy, these
could even increase the bias.

A diﬀerent approach is to learn “unbiased utilities” by either using a causal model to capture the relation
between attributes and utilities [53, 71] or by casting it as a multi-objective unconstrained optimization
problem [72, 74]. The former approach explicitly uses the protected attributes to generate counterfactuals, so,
it can lead to unfair outcomes in the presence of noise (also see Section 4.3). And the latter approach can lead
to sub-optimal fairness if noisy data is not accounted for, as shown by works on fair classiﬁcation [54, 4, 14].
In [37], it is empirically shown that when protected attributes are missing, proxy attributes can be used
to improve fairness in classiﬁcation. However, they do not consider how necessary noise resulting from the
proxy attributes aﬀects the fairness or accuracy.

Mitigating bias with noise. Works on curtailing bias with noisy information are relatively recent. Closest
to this paper are those which consider noise in the protected attributes. In [4], conditions on the noise under
which the popular post-processing method for fair classiﬁcation by [41] reduces bias in terms of equalized odds
are characterized. However, they only consider noise in the training samples and assume that the test samples
are not noisy, which often doesn’t hold in practice. In [54], an in-processing approach to fair classiﬁcation is
suggested; they show that applying tighter fairness constraints in existing fair classiﬁcation frameworks can
mitigate bias in terms of equalized odds and statistical parity with binary protected attributes. However,
this approach does not extend to nonbinary protected attributes and to other deﬁnitions of fairness. In [14],
an in-processing approach for fair classiﬁcation which can mitigate bias with nonbinary and noisy protected
attributes is developed. However, they assume that the noise only depends on the (unknown) underlying
protected attributes, whereas, we also allow the noise to vary with nonprotected attributes and utility.
Furthermore, [54, 4, 14] mitigate bias in classiﬁcation tasks, and it is not clear how to extend these methods
to subset selection.

In [20, 46], methods to reliably assess disparity in the setting where the protected attributes are entirely
missing are proposed. We consider a similar noise model as the one they propose; however, the problem is
fundamentally diﬀerent as their goal is assessment rather than mitigation.

Noise models in literature. Several works in the machine learning literature consider noise in the pre-
dicted labels as opposed to in attributes, protected or otherwise [3, 58, 34, 57]. In this paper, we consider a
noise model that arises from this line of work, but applied to the protected attributes rather than the label.

4

2 Model

For a natural number n ∈ N by [n] we denote the set {1, 2, . . . , n}, and for a real number x ∈ R by exp(x)
we denote ex. We use I[·] to denote the indicator function, o(1) to denote O(1/n), and U(a, b) to denote the
uniform distribution on interval [a, b]. Given a natural number p ∈ N, ∆p denotes the standard p-simplex.

2.1 Selection problem and noise model

Selection problem.
In the classical selection problem, one is given m items, where each item i ∈ [m]
has a utility wi ≥ 0. An item’s utility is the value it adds to the selection. The goal is to ﬁnd a subset of
n items which has the most total value. It is convenient to encode a subset with a binary selection vector
x ∈ {0, 1}m. Then, the classical selection problem is

maxx∈{0,1}m

(cid:88)m

i=1

wixi

s.t.,

(cid:88)m

i=1

xi = n.

(1)

Protected attributes. We consider s ∈ N protected attributes (such as, gender or race), where for k ∈ [s],
the k-th protected attribute can take pk ∈ N values (such as, diﬀerent genders or races). Let X be the domain
of all other nonprotected attributes. Fix a joint distribution over D := R≥0 × [p1] × · · · × [ps] × X . Then,
each item i ∈ [m] is represented by the tuple

(wi, z(1)

i

, . . . , z(s)

i

, ai) ∈ R≥0 × [p1] × · · · × [ps] × X ,

and is drawn independently from this joint distribution. We observe the utility wi and nonprotected at-
tributes ai, but do not observe the protected attributes (z(1)
i ). Instead, we observe a noisy version
((cid:98)z(1)

, . . . , (cid:98)z(s)
For each attribute-value pair k ∈ [s] and (cid:96) ∈ [pk], there is a (unknown) group G(k)

i ) of them (for each i ∈ [m]).

(cid:96) ⊆ [m]: items whose

, . . . , z(s)

i

i

k-th attribute has value (cid:96):

G(k)
(cid:96)

:=

(cid:110)

i ∈ [m] : z(k)

i = (cid:96)

(cid:111)

.

For example, if the k-th protected attribute is race, then for diﬀerent values of (cid:96) ∈ [pk], G(k)
is the subset
(cid:96)
candidates whose race is (cid:96). However, we only have noisy information about the protected attributes of each
item; so, only noisy information of this subset.

Intersectional groups.
In the above model, each protected attribute takes a unique value. It may appear
that this does not allow for intersectional groups, e.g., say multiracial candidates. But this is only a matter
of encoding, and is remedied by using attributes such as ‘has-raceA?’ and ‘has-raceB?’, which take Yes or
No values.
Deﬁnition 2.1 (Noise). For each item i ∈ [m] and k ∈ [s], we have a probability vector q(k)
such that, the k-th protected attribute of item i takes value (cid:96) ∈ [pk] with probability q(k)
i(cid:96)
(wi, (cid:98)z(1)

i ∈ ∆pk ,
conditioned on

, . . . , (cid:98)z(s)

, ai):

i

i

q(k)
i(cid:96)

:= Pr

(cid:104)

i ∈ G(k)

(cid:96)

| (wi, (cid:98)z(1)

i

, . . . , (cid:98)z(s)

i

, ai)

(cid:105)

.

(2)

The event that (i ∈ G(k)
Note that for all i ∈ [m] and k ∈ [s], (cid:80)

(cid:96)∈[pk] q(k)

i(cid:96) = 1.

(cid:96) ) is independent of all other items j ∈ [m]\{i} and all other attributes in [s]\{k}.

i

, . . . , (cid:98)z(s)

Discussion of the noise model. The above model says that given the utility (wi), noisy protected
attributes ((cid:98)z(1)
i ), and nonprotected attributes (ai) of an item i, there is probabilistic information
about its protected attributes. If items represent candidates for a job and the protected attribute is race,
then we can use the candidate’s last name (encoded in ai) to derive probabilistic information about their
race. This has been used in practice, e.g., by [28]. We can also consider multiple nonprotected attributes
such as both last-name and location, e.g., as used by [32, 29]. As discussed in Section 1, this could be relevant
for an online hiring platform, which may not have demographic information of some or all of its users [56],
and image search engines where the images do not have gender labels.

5

2.2 Target problem

Studies have found that, in the absence of other constraints, the selection problem (1), can overrepresent
individuals with certain protected attributes at the expense of others [47, 25]. Towards mitigating this bias,
we consider lower bounds and upper bounds on the number of items of a given protected attribute selected.
Formally, the constraints ensure that for each attribute-value pair k ∈ [s] and (cid:96) ∈ [pk], the selection has
. Then, a selection x ∈ {0, 1}m satisﬁes the (target)

at least L(k)
fairness constraints if: for all k ∈ [s] and (cid:96) ∈ [pk]

(cid:96) ≥ 0 and at most U (k)

(cid:96) ≥ 0 items from G(k)

(cid:96)

(cid:88)

L(k)

(cid:96) ≤

xi ≤ U (k)

(cid:96)

.

i∈G(k)
(cid:96)

(Fairness constraints; 3)

Constraints similar to Equation (3) have been studied by several works in algorithmic fairness [21, 18, 22],
and are rich enough to encapsulate a variety of fairness and diversity metrics (e.g., see [13]). Thus, for the
appropriate L and U , the subset satisfying constraints (3) would be fair for one from a large class of fairness
metrics.

Overall, our constrained subset selection problem is:

max
x∈{0,1}m

(cid:88)m

i=1

s.t. L(k)

(cid:96) ≤
(cid:88)m

i=1

wixi

(cid:88)

i∈G(k)
(cid:96)

xi = n.

xi ≤ U (k)

(cid:96)

,

∀ k ∈ [s], (cid:96) ∈ [pk],

(Target)

(4)

(5)

If we know the protected attributes, and in turn G(k)
(for each k ∈ [s] and (cid:96) ∈ [pk]), then we can hope
to solve Program Target directly.
Indeed, prior works consider similar problems in rankings [18], or its
generalization, to multiple Matroids constraints [22]. However, with only noisy information about protected
attributes, we can not even verify if a selection vector x is feasible for Program Target. To overcome this,
we must go beyond exact algorithms which always satisfy fairness constraints.

(cid:96)

2.3 Denoised problem
The diﬃculty in solving Program Target is that we do not know the constraints (as we do not know G(k)
(cid:96) ).
We propose to solve a diﬀerent problem, Program Denoised, which uses the noise estimates q to approximate
the constraints of Program Target. For some small δ ∈ (0, 1), we deﬁne the denoised program as the
following

(cid:88)m

i=1

wixi

max
x∈{0,1}m
s.t. L(k)
(cid:88)m

xi = n.

i=1

(cid:96) − δn ≤

(cid:88)m

i=1

i(cid:96) xi ≤ U (k)
q(k)

(cid:96) + δn, ∀ k ∈ [s], (cid:96) ∈ [pk],

(Denoised)

(6)

(7)

i=1 q(k)

Here, (cid:80)m
i(cid:96) xi is the expected number of items selected by x whose k-th protected attribute is (cid:96). Then,
intuitively, we can see Program Denoised that satisﬁes the constraints of Program Target in expectation,
where the expectation is taken over the noise.

However, just satisfying constraints in expectation is not suﬃcient. For instance, this would allow algo-
rithms that, in each use, violate the fairness constraints by a large amount, but “average out” their errors
in aggregate. Instead, our goal is to ﬁnd an algorithm which violates the constraints by at most a small
amount, almost always. Before presenting our theoretical results, we discuss an alternate noise model and
why it is not suitable in our setting.

6

2.4 Group-level noise model

Recent works on noisy fair classiﬁcation [54, 4, 14] consider a diﬀerent noise model, which adapted to our
setting, uses the following probabilities

qi(cid:96) := Pr[i ∈ G(cid:96) | ((cid:98)z(1)

i

, . . . , (cid:98)z(s)

i )].

Notice that unlike Deﬁnition 2.1, qi(cid:96) does not condition on the utility wi or nonprotected attributes ai.
Thus, its estimates are the same for all items with the same set of noisy protected attributes. We call this
the group-level noise model (GLN). In the next example, we discuss why GLN is not suﬃcient to mitigate bias
in subset selection.

Toy example. Consider a setting where there is one protected attribute which takes two values (i.e.,
s = 1 and p1 = 2), and the relevant fairness metric is equal representation. Let the two groups (unknown)
be A, B ⊆ [m], and their observed noisy versions be (cid:98)A and (cid:98)B.4

According to q, each candidate i ∈ (cid:98)B has the same probability of being in A. In this noise model, these
candidates are indistinguishable apart from their utilities, so, if one picks nb ∈ N candidates from (cid:98)B, they
would naturally be the ones with the highest utility. However, suppose that most individuals in A have a
higher utility than most individuals in B.5 In this case, the probabilities q will be “distorted” by the utilities,
such that, candidates with higher utility in (cid:98)B are more likely to be in A than those with lower utility in
(cid:98)B. In fact, if m is much larger than n, then most of the top nb candidates in (cid:98)B would, in fact, be from A.
This example can be extended to more realistic settings, with more than two groups and a smaller amount
of “bias” in the utilities. Even then, to overcome this distortion in probabilities, one needs to consider a
stronger noise model, in which the noise estimate varies with utility (as in Deﬁnition 2.1); either implicitly
through proxy nonprotected attributes (ai) or explicitly with utility (wi).

Remark 2.2. In Sections 3 and 4, for the sake of simplicity, we only consider the setting with one protected
attribute (s = 1) which takes p ≥ 1 values. We obtain analogous results for the the general case in Section 6.
When s = 1, we let p := p1 and drop superscripts (representing the protected attribute) from all variables.

3 Theoretical results

Our main algorithmic result is an eﬃcient approximation algorithm for Program Target.

Theorem 3.1 (An approximation algorithm for Program Target). There is an algorithm (Algo-
rithm 1) that given an instance of Program Target for s = 1 and noise q from Deﬁnition 2.1, outputs a
selection x ∈ {0, 1}m, such that, with probability at least 1 − 4p exp (cid:0)−δ2n/3(cid:1) over the noise in the protected
attributes of each item, the selection x

1. has a value at least as high as the optimal value of Program Target,

2. violates the cardinality constraint (5) by at most p (additive), and

3. violates the fairness constraints (4) by at most (p + 2δn) (additive).

The algorithm runs in polynomial time in the bit complexity of input.

As desired, the algorithm outputs subset which violates the constraints of Program Target by at most a
small amount, with high probability.

Note that the approximation is only in the constraints and not in the value: with high probability, x has

an higher value than the optimal solution, say x(cid:63), of Program Target, i.e.,

i=1
4Formally, A = {i : z(1)
i = 2}, (cid:98)A = {i : (cid:98)z(1)
5Such an bias in utilities is one reason why we need fairness constraints in the ﬁrst place [50, 17, 30].

i = 1} and (cid:98)B = {i : (cid:98)z(1)

i = 1}, B = {i : z(1)

i = 2}.

i=1

(cid:88)m

(cid:88)m

xiwi ≥

x(cid:63)
i wi.

7

In most real-world contexts p is a small constant. Here, Theorem 3.1 implies that x violates the fairness
constraints (Equation (4)) by a multiplicative factor of at most (1+2δ +o(1)) and the constraint Equation (5)
by a multiplicative factor of at most (1+o(1)) with high probability.6 If p is large, then x (from Theorem 3.1)
can violate the constraints by a large amount. However, in this case it is NP-hard to even check if there is
a solution to Program Denoised which violates the constraints by a constant additive factor (let alone ﬁnds
an optimal solution for Program Target); see Theorem 3.5.
Algorithm 1 crucially uses the Program Denoised:

it ﬁrst solves the linear-programming relaxation of
Program Denoised, and then, “rounds” this solution to integral coordinates. In the next section, we overview
the proof of Theorem 3.1. We defer the proof of Theorem 3.1 to Section 5.1 due to space constraints.

Remark 3.2. We can strengthen Theorem 3.1 to guarantee that Algorithm 1 ﬁnds an x ∈ {0, 1}m which does
not violate the lower bound fairness constraint (left inequality in Equation (4)) and violates the upper bound
fairness constraints by at most (p + 2δn) (without changing other conditions). In particular, this shows that,
if one places only lower bound fairness constraints, then subset found by Algorithm 1 would never violate the
fairness constraints.

Algorithm 1: Algorithm for Program Target
Input: A number n ∈ N, probability matrix q ∈ [0, 1]m×p, utility vector w ∈ Rm, constraint vectors

L, U ∈ Rp

≥0.

1. Solve x ← Find a basic feasible solution to linear-programming relaxation
.
of Program Denoised with inputs (n, q, w, L, U ).
2. Set x(cid:48)
3. Return x(cid:48).

i := (cid:100)xi(cid:101) for all i ∈ [m]. //Round solution

3.1 Proof overview and hardness results

In this section, we overview the proof of Theorem 3.1. The complete proof and an extension of Theorem 3.1
for multiple protected attributes (i.e., s ≥ 1) appear in Sections 5 and 6.

The proof of Theorem 3.1 has two broad steps: First, we show that solving Program Denoised (even
approximately) gives us a “good” solution to Program Target, and then we develop an approximation
algorithm along with matching hardness results for Program Denoised. To prove the former, we bound the
diﬀerence between the true and the expected number of candidates from any one group G(cid:96).
Lemma 3.3. For all δ ∈ (0, 1) and x ∈ [0, 1]m, s.t., (cid:80)m

i=1 xi = n:

∀ (cid:96) ∈ [p],

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

i∈G(cid:96)

(cid:88)

xi −

qi(cid:96)xi

i∈[m]

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ nδ

holds with probability at least 1 − 2p exp (cid:0)−δ2n/3(cid:1) over the noise in the protected attributes of each item.

The proof of this lemma appears in Section 5.1.1.

Using Lemma 3.3, we can show that any solution that violates the constraints of Program Denoised
by a small amount, with high probability, also violates the constraints of Program Target by at most a
small amount. Let x(cid:63) be an optimal selection for Program Target. Using Lemma 3.3, we can show that
x(cid:63) is feasible for Program Denoised with high probability. It follows any solution x which is optimal for
Program Denoised has value at least as large as x(cid:63), i.e.,

(cid:88)m

i=1

xiwi ≥

(cid:88)m

i=1

x(cid:63)
i wi.

These suﬃce to show that, solving Program Denoised gives a “good” solution for Program Target—which
satisﬁes the claims in the Theorem 3.1.

It remains to solve Program Denoised. Unfortunately, even checking if Program Denoised is feasible is
NP-hard; see Theorem 3.5 (a constant-factor approximation (in utility) to Program Denoised is also NP-
hard). We overcome this hardness by allowing solutions to violate the constraints of Program Denoised by a

6Using L(cid:96), U(cid:96) ≤ n; if not, we can set L(cid:96) to min(L(cid:96), n) and U(cid:96) to min(U(cid:96), n).

8

small additive amount (p). Towards this, consider the linear-programming relaxation of Program Denoised
(for s = 1). We show that any basic feasible solution (BFS) of LP-Denoised has a small number of fractional
entries (Lemma 3.4).

maxx∈[0,1]m

(cid:88)m

i=1

wixi

(LP-Denoised for s = 1)

s.t.

∀ (cid:96) ∈ [p],

(cid:88)m

i=1

qi(cid:96)xi ≤ U(cid:96) + δn,

L(cid:96) − δn ≤
(cid:88)m

xi = n.

i=1

(8)

(9)

Lemma 3.4 (An optimal solution with p fractional entries). Any basic feasible solution x ∈ [0, 1]m
of LP-Denoised has at most min(m, p) fractional values, i.e., (cid:80)m
i=1

I[xi ∈ (0, 1)] ≤ min(m, p).

The proof follows by specializing well-known properties of BFSs to LP-Denoised. We remark that this result
is tight; see Fact 5.1.

Proof sketch of Theorem 3.1. Using Lemma 3.3, we can show that x(cid:63) is feasible for Program Denoised
with probability at least 1 − 2p exp (cid:0)−δ2n/3(cid:1). Assume that this event happens. Then, x(cid:63) is also feasible for
LP-Denoised. Consider the basic feasible solution x to LP-Denoised from Step 1 of Algorithm 1. Since x is
optimal for LP-Denoised, it follows that x has a value at least as large as x(cid:63), i.e.,

(cid:88)m

i=1

xiwi ≥

(cid:88)m

i=1

x(cid:63)
i wi.

Further, since w ≥ 0, the rounded solution x(cid:48) from Step 2 of Algorithm 1 only increases the utility of x.
Thus,

i=1
This establishes the ﬁrst claim in Theorem 3.1.

(cid:88)m

x(cid:48)
iwi ≥

(cid:88)m

x(cid:63)
i wi.

i=1

It follows from Lemma 3.4 that x(cid:48) picks at most p more elements than x. Thus, x(cid:48) violates Equation (7), so
Equation (5) by at most p. By the same argument, x(cid:48) violates the fairness constraints of Program Denoised
by at most p (additive). Combining this with Lemma 3.3, we can show that, with probability at least
1 − 2p exp (cid:0)−δ2n/3(cid:1), x(cid:48) violates the fairness constraints of Program Target by at most 2δn + p (additive). This
establishes the last two claims in Theorem 3.1 (conditioning on the two events described above).

The run time follows since there are polynomial time algorithms to ﬁnd a basic feasible solution of a

linear program. Finally, taking a union bound over over the two events completes the proof.

3.1.1 Hardness results

Lastly, we present our hardness results; their proofs appear in Section 5.2.

Theorem 3.5 (Hardness results—Informal). Consider variants of Program Denoised for values of p.

1. If p ≥ 2, then deciding if the problem is feasible is NP-hard.

2. If p ≥ 3, then the problem is APX-hard.

3. If p = poly(m) and s > 1, then for every constant c > 0, the following violation gap variant of Pro-

gram Denoised is NP-hard.

• Output YES if the input instance is satisﬁable.
• Output NO if there is no solution which violates every upper bound constraint at most an additive

factor of c.

9

4 Empirical results(cid:63)

We evaluate our approach on utilities and noise derived from both synthetic and real-world data. We consider
the following algorithms:

Baseline.

- Blind: As a baseline, we consider the Blind algorithm which selects n candidates with the highest utility.

Note that Blind has the optimal unconstrained utility.

Noise aware.

- FairExpec is our proposed approach (see Theorem 3.1).
- FairExpecGrp is the same as FairExpec but uses the probabilities qi(cid:96) := Pr[i ∈ G(cid:96) | (cid:98)zi] from the group-

level noise model (Section 2.4).

Noise oblivious.
Impute protected attributes Bayes-optimally from q ∈ [0, 1]m×p as:

∀ i ∈ [m], (cid:96) ∈ [p],

q(cid:48)
i(cid:96) :=

(cid:40)
1
0

if (cid:96) ∈ argmaxj∈[p] qij,
otherwise.

(10)

If argmaxj qij is not unique, pick one at random. Then, we consider the following noise oblivious algorithms
which take the imputed protected attributes q(cid:48) as input:

- Thrsh solves Program Target deﬁned on q(cid:48). This is equivalent to the ranking algorithms of [18, 65]

adapted to subset selection.

- MultObj is a multi-objective optimization algorithm inspired by [72]’s approach for ranking. Let t ∈ ∆p
be the target distribution of protected attributes in the selection. For example, if the target is equal
representation, then t := (1/p, . . . , 1/p) ∈ Rp. Given a constant λ > 0, MultObj solves7

max
x∈[0,1]n : (cid:80)

i xi=n

w(cid:62)x − λ · DKL

(cid:18) (q(cid:48))(cid:62)x
n

(cid:19)

, t

·

w(cid:62)1m
m

,

where 1m ∈ Rm is the all one vector. The ﬁrst term w(cid:62)x is the value of x, (x/n) is the distribution of
noisy protected attributes in x, and entire second term is a penalty on x for being far from the target
distribution t.8

4.1 Setup and metrics

4.1.1 Setup

We consider one protected attribute (s = 1) which takes p disjoint values (we use p = 2 and p = 4). Our
simulations either target equal-representation, where t = (1/p, . . . , 1/p) ∈ Rp, or proportional representation,
where t = (|G1|/m, . . . , |Gp|/m).
In each simulation, we do the following

• FairExpec, FairExpecGrp, and Thrsh: Set L(cid:96) = 0 and U(cid:96) = n(1 − α) + nαt(cid:96), and vary α from 0 to 1. Notice
that α = 0 enforces no constraints on the subset, the constraints become tighter as α increases, and α = 1
ensures the subset chooses exactly n = t(cid:96) candidates from the (cid:96)-th group.

(cid:63)The code for the simulations is available at https://github.com/AnayMehrotra/Noisy-Fair-Subset-Selection.
7Given two vectors x, y ∈ ∆p, DKL(x, y) denotes the Kullback–Leibler divergence of x and y deﬁned as DKL(x, y) :=
i=1 xi log(xi/yi)
8We scale the second term by the average utility (cid:80)m

wi/m. This is not necessary, but ensures that λ does not (heavily)

(cid:80)p

i=1

depend on the scale of the utility.

10

• MultObj: Vary λ from 0 to a large value. Here, λ = 0 enforces no penalty on the objective, the penalty
increases as λ increases, and λ = ∞ forces MultObj to satisfy the target distribution exactly (on the noisy
attributes).

Let (αr, λr) be the r-th choice of α and λ. For each (αr, λr), we draw a set M of m individuals or items
from the dataset. For each element i ∈ M , we have qi ∈ ∆p and wi ∈ R. We give the details of drawing M
and ﬁxing qi, wi with each simulation.

4.1.2 Fairness metric

Given subset S ∈ [m] and target t ∈ [0, 1]p, let the risk diﬀerence F(S, t) ∈ [0, 1] of S for target t be

F(S, t) := 1 − min
(cid:96)∈[p]

t(cid:96) · max
(cid:96),k∈[p]

(cid:18) |S ∩ G(cid:96)|
n · t(cid:96)

−

|S ∩ Gk|
n · tk

(cid:19)

.

(11)

Here, a risk diﬀerence 1 is the most fair and 0 is the least fair. When the target is proportional representation,
F(S, t) reduces to the usual deﬁnition of risk diﬀerence (up to scaling).9 Let A(w, q) ⊆ [m] be the subset
selected by algorithm A on input (w, q). We report

FA := E [F(A(w, q), t)] ,

where the expectation is over the choices of (w, q).

4.1.3 Utility metric

Let UA to be the average utility obtained by A:

(cid:20)(cid:88)

UA := E

(cid:21)

wi

,

i∈A(w,q)

where the expectation is over the choices of (w, q). We report the utility ratio KA ∈ [0, 1] for diﬀerent
algorithms A, deﬁned as

KA :=

UA
UBlind

.

(Utility ratio)

When the algorithm A, is not important or clear from context, we drop the subscripts from FA and KA.

4.2 Synthetic data with disparate error-rates

In this simulation, we consider the setting where diﬀerent groups have diﬀerent noise levels. This has been
observed in practice, for instance, in commercial image-based gender classiﬁers [7].

4.2.1 Data

We generate a synthetic dataset with one binary protected attribute (p = 2). This attribute partitions
the (underlying) population into a minority group (40%) and a majority group (60%). We assume that
candidates in both groups have similar potentials, so, sample utilities of all candidates (independently) from
U(0, 1). Next, we sample the probabilities qi from a Gaussian mixture, such that, the resulting population has
40% minority candidates (in expectation), and the imputed attributes q(cid:48) have a higher false discovery rate
(FDR) for minority candidates (≈40%) compared to majority candidates (≈10%).10 Formally, we sample qi
as follows:

qi0 ∼

· N(0.6, 0.05) +

· N(0.05, 0.05)

and qi1 := 1 − qi0,

7
11

4
11

where N (µ, σ) is the truncated normal distribution on [0, 1] with mean µ and standard deviation σ.

9Some works also deﬁne risk diﬀerence as a measure of unfairness [11, 63], and set it equal to 1−F (S, t) with t = (1/p, . . . , 1/p)

(up to scaling).

10The diﬀerence of 30% in FDRs is comparable to 34% diﬀerence in FDRs between dark-skinned females and light-skinned

men observed by [7] for a commercial classiﬁer.

11

)
r
i
a
f

A
A
e
A
r
o
A
m
(
A
)
A
F
A
(
A
e
c
A
n
e
A
r
e
A
ﬀ
A
d
k
A
s
A
i
R
A
A
A
A
A

)
r
i
a
f

s
s
e
l
(

i

((cid:63))

(cid:63) this work

Figure 1: Synthetic data with disparate error-rate (Section 4.2): This simulation considers the setting where the
minority group (40% of total) has a higher 30% higher FDR compared to the majority group. The utilities of all
candidates are iid from the uniform distribution. The target is to ensure equal representation between the majority
and minority groups. The y-axis shows the risk diﬀerence F of diﬀerent algorithms, and the x-axis shows the
constraint parameters (α for FairExpec, FairExpecGrp, and Thrsh, and λ for MultObj); F values are averaged over 500
trials, and the error bars represent the standard error of the mean. We observe that increasing fairness constraints
to noise oblivious algorithms (Thrsh and MultObj) worsens their risk diﬀerence! Whereas, the risk diﬀerence of noise
aware algorithms improves (FairExpec and FairExpecGrp) on increasing fairness constraints.

4.2.2 Setup

In this simulation, we target equal representation between the majority group and the minority group, and
ﬁx m = 500 and n = 100.

We report the risk diﬀerence (F) of diﬀerent algorithms as a function of α (for FairExpec, FairExpecGrp,

and Thrsh) and as a function of λ (for MultObj) in Figure 1.

Remark 4.1. MultObj does not guarantee a particular fairness-level for any ﬁxed λ. Thus, one should
consider the limiting value of FMultObj in Figure 1.

4.2.3 Results

We observe that without any constraints (i.e., α = 0 and λ = 0) all algorithms have similar risk diﬀerence
≈ 0.81. However, on adding fairness constraints Thrsh and MultObj become more unfair. In fact, for the
strongest fairness constraint (i.e, α = 1 and λ = 2500) they have the lowest risk diﬀerence (< 0.7). This
is because, the imputed protected attributes have a higher FDR for the minority group (so, the algorithms
pick a higher number of candidates from the majority group).

In contrast, FairExpec and FairExpecGrp do not use the imputed protect attributes, so increasing fairness
constraints to FairExpec and FairExpecGrp improves their risk diﬀerence, and for the strongest fairness con-
straint (α = 1) they attain the highest risk diﬀerence (> 0.92). Finally, since we sample all utilities from the
same distribution, it is not surprising that FairExpec and FairExpecGrp perform similarly.

4.3 Synthetic data with disparate utilities

In this simulation, we consider the setting where diﬀerent groups have diﬀerent distributions of utilities.
In particular, we assume that the minority group (unfairly) has a lower average utility, when in fact, the
distributions of utilities should be the same for both the majority group and the minority group. (Contrast

12

0.00.20.40.60.81.00.50.60.70.80.91.0Fairness achieved ()05001000150020002500Toy experiment | (m,n)=(500,100) | iter=500 | fmetric=r_diffFairExpecFairExpecGrpMultObjThrshBlindthis with Section 4.2 where all utilities are identically drawn). Such diﬀerences in utility can manifest in the
real world for many reasons, including, the implicit biases of the committee evaluating the candidates [69,
6, 68] and structural oppression faced by diﬀerent groups [31].

Counterfactually fair approaches. One could also consider counterfactually fair approaches to mitigate
bias in selection. (We refer the reader to [53] for an overview of counterfactual fairness). At a high-level,
these approaches try to “unbias” the utilities across groups, and then use unbiased utilities in subsequent
tasks (say, selection or ranking). In this simulation, we also consider counterfactually fair algorithms by [71]:
CntrFair and CntrFairResolving. (They correspond to non-resolving and resolving algorithms in [71].)

Roughly, they assume that there is a causal model M[θ] (parameterized by θ), such that, given the
attributes (zi, ai) of an individual, their utility is wi = M[θ](zi, ai). Then, roughly, they ﬁx each individual’s
protected attributes to v and compute the “unbiased” utility as ˆwi := M[θ](v, ai); this represents the utility
of the individuals had they had the same protected attribute v.

4.3.1 Data

We consider a synthetic hiring dataset, generated with the code provided by [71]. In the data, each candidate
i has one protected attribute zi ∈ {0, 1} denoting their race (0 if the candidate is Black and 1 otherwise) and
two nonprotected attributes ai1 ∈ {0, 1} and ai2 ∈ R: ai1 is 1 if the candidate has prior work experience, and
ai2 is denotes their qualiﬁcations (the larger the better).11 We sample 2000 candidates independently from a
ﬁxed distribution deﬁned in [71], which, is such that, the utility of Black candidates is (unfairly) lower than
non-Black candidates.12 For further details, we refer the reader to Section A.1.4.

Preprocessing. We sample a training dataset D(cid:48) with m = 2000 candidates. Then, using D(cid:48) we compute
an approximation ¯θ of θ, and given a candidate i described by (wi, zi, ai1, ai2), we compute qi ∈ [0, 1]2. (Note
that the candidate i may not be in D(cid:48).) For prefer more details of the preprocessing, in Section A.1.4.

Adding noise. The dataset does not have noise to begin with. Given a noise level τ ∈ [0, 1/2], we generate
noisy race (cid:98)zi of candidate i by ﬂipping their race zi (independently) with probability τ .

4.3.2 Setup

We target proportional representation of race and vary τ over [0, 0.5]. For each noise level τ , we sample a new
instance D of the dataset with m = 2000 and add τ -noise to it. We ﬁx n = 100 and the strongest constraints
α = 1 and λ = 500 for the algorithms.13 Here, CntrFair and CntrFairResolving use M(¯θ) (calculated in
preprocessing), and FairExpec, FairExpecGrp, MultObj, and Thrsh use q (or the imputed attributes q(cid:48); both
calculated in preprocessing).

We report the risk diﬀerence (F) as a function of the noise-level (τ ) in Figure 2. We also report the

selection rates from each group in Section A.5.

4.3.3 Results

We observe that all algorithms have the highest fairness when there is no noise (τ = 0). Here, they have
a similar risk diﬀerence (lying between 0.94 to 0.98). As the noise increases, we observe that the risk
diﬀerence of FairExpecGrp and CntrFair approaches FBlind = 0.74, and risk diﬀerence of MultObj, Thrsh, and
CntrFairResolving approaches a value between [0.82, 0.87]. In contrast, FairExpec has a better risk diﬀerence,
F > 0.94, throughout. The risk diﬀerence of MultObj and Thrsh improves with τ at some values of τ — we
give a possible explanation in Remark 4.2.

Notice at τ = 0.5, for all candidates i ∈ [m], the noisy label (cid:98)zi ∈ {0, 1} is chosen uniformly at random
and provides no information about zi. CntrFair and CntrFairResolving use (cid:98)zi to compute the counterfactual

11This interpretation diﬀers from [71], who interpret both zi and ai1 as protected attributes.
12The only diﬀerence from [71] is that we increase the underlying bias (by reducing the mean utilities for Black candidates
and candidates without prior experience). We do so because, the dataset already had a high risk diﬀerence (> 0.9) without
adding any fairness constraints.

13We choose λ = 500 as MultObj’s fairness FMultObj converges before λ = 500.

13

)
r
i
a
f

AAAAAAAAAAAAAAAAAAAAA

)
F
(

e
c
n
e
r
e
ﬀ
d

e
r
o
m
A(
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A

k
s
i
R

)
r
i
a
f

s
s
e
l
(

i

((cid:63))

CntrFairRe

CntrFairRes
CntrFair

(cid:63) this work

(less noise)

Noise (τ )

(more noise)

Figure 2: Synthetic data with disparate utilities (Section 4.3): This simulation considers the setting where the utilities
of a minority group have a lower average than the majority group, and both groups have an identical amount of noise.
The target is to ensure proportional representation. The y-axis shows the risk diﬀerence F of diﬀerent algorithms,
and the x-axis shows the amount of noise added τ ∈ [0, 1/2]; F values are averaged over 200 trials, and the error bars
represent the standard error of the mean. We observe that the risk diﬀerence of all algorithms becomes poorer with
noise (τ > 0) than without it (τ = 0). Here, FairExpec has the highest risk diﬀerence for all values of noise. Finally,
unlike Section 4.2, FairExpecGrp has a lower fairness than FairExpec (since, in this simulation, diﬀerent groups have
diﬀerent distributions of utilities).

utilities, so, perform poorly at τ ≈ 0.5. Further, FairExpecGrp uses the probabilities qi which depend on (cid:98)zi,
but not on wi. Since the utility of candidates of diﬀerent races has a diﬀerent distribution, qi can be skewed
(see Section 2.4).

We note that the utility of all algorithms decreases on adding noise. In particular, while FairExpec is able
to satisfy the fairness constraints with noise, its utility decreases on adding noise; see Section A.5 for a plot
of utility ratio (K) vs noise (τ ).

Remark 4.2. The risk diﬀerence of Thrsh and MultObj is non-monotonic in the noise. This might be
because the false discovery rate (FDR) of q(cid:48) for Black candidates is non-monotonic. Speciﬁcally, the FDR
ﬁrst increases with τ (roughly, for τ ≤ 0.2), and then decreases. The decrease in FDR after τ = 0.2 comes
at the cost of fewer total positives (i.e., q(cid:48) identiﬁes fewer total Black candidates). The total number of total
positives drop below n/2 for higher values of τ . Correspondingly, the F of Thrsh and MultObj ﬁrst decreases
as FDR reduces, then increases as FDR increases until the number of total positives is larger than, roughly,
n/2, and ﬁnally, decreases as the number of total positives drops below n/2.

Remark 4.3. We do not consider counterfactual approaches in Section 4.2 because there, the utilities are al-
ready unbiased, and so, CntrFair and CntrFairResolving reduce to Blind. Further, CntrFair and CntrFairResolving
only ensure proportional representation. We ﬁnd that the datasets considered in Sections 4.4 and 4.5 are
already fair when the target is proportional representation; in both cases, FBlind > 0.9. Therefore, we omit
these algorithms from those simulations.

4.4 Real-world data for candidate selection

In this simulation, we consider the problem of selecting candidates under noisy information about their race.
Similar to what has been used in applications (e.g., [28]), we use a candidate’s last name to predict their

14

0.00.10.20.30.40.5Noise ()0.50.60.70.80.91.0Fairness achieved ()Testing causal on N dat | Testing FE on N dat with 20 eq-sz bins[[0.5,0.5],[0.5,0.5]] Causal experiment (n=100) | iter=200 | metric=r_diffFairExpecFairExpecGrpMultObjThrshCausal-ResolvingCausalBlindAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA

((cid:63))

A
A
A
)
A
K
A
(
A
o
i
A
t
a
A
r
A
y
t
A
i
l
A
i
t
A
U
A
A
A
A
A
A
A
A
A

(less fair)

Risk diﬀerence (F)

(more fair)

(cid:63) this work

Figure 3: Real-world data for candidate selection (Section 4.4): This simulation considers race as the protected
attribute which takes p = 4 values. The simulation uses last-name as a proxy to derive noisy information of race and
draws the utility of each candidate from a ﬁxed distribution depending on their race. The target is to ensure equal
representation across race. The y-axis shows the utility ratio K of diﬀerent algorithms, and the x-axis shows the risk
diﬀerence of diﬀerent algorithms; both K and F values are averaged over 100 trials, and the error bars represent the
standard error of the mean. We observe that FairExpec reaches the highest risk diﬀerence (F = 0.89), and has a
better tradeoﬀ between utility and risk diﬀerence compared to other algorithms.

race. We consider a candidate’s utility as their “previous-salary,” which we model using the race-aggregated
income dataset [10]. This dataset provides the income distribution of families from diﬀerent races (elaborated
below). This problem could be relevant in the context of an online hiring platform, which would like to display
a race-balanced set of candidates, but only has noisy information about each candidate’s race [56].

4.4.1 Data

US 2010 Census dataset [67]. The dataset contains 151,671 distinct last names which occurred at least
a 100 times in the US 2010 census (23,656 last names occur at least a 1000 times). For each last name i,
the dataset has its total occurrences per 100k people c(i) ∈ Z, and a vector ˆf = (f1(i), . . . , f6(i)) ∈ [0, 1]6
representing the fraction of individuals who are White, Black, Asian and Paciﬁc Islander (API), American
Indian and Alaskan Native only (AIAN), multiracial, or Hispanic respectively.

We do not use ‘AIAN’ and ‘two or more races’ categories (i.e., f4 and f5) as they do not occur in the
income dataset. Then, for each last name i, we deﬁne the probability vector qi as the normalized version of
the vector (f1(i), f2(i), f3(i), f6(i)).

Income dataset [10]. We use family income data aggregated by race [10]. This was compiled by the
US Census Bureau from the Current Population Survey 2018 [9]. The dataset provides income data of
83,508,000 families. It has four races (White, Black, Asian, and Hispanic), 12 age categories, and 41 income
categories.14 For each set of race, age, and income categories, the dataset has the number of families whose
reference person (see deﬁnition here) belongs to these categories.

For each race r, we consider the discrete distribution Dr of incomes of families with race r derived from

the income dataset [10]; see Figure 15 in supplementary material for some statistics of Dr.

14The age categories are: 15 to 24, 25 to 30, 30 to 35, . . . . The income categories are: [0, 5000), [5000, 104), . . . , [1.95, 2 · 105),

and (2 · 105, ∞) in USD per annum.

15

0.20.30.40.50.60.70.80.91.0Fairness achieved ()0.50.60.70.80.91.0Utility Ratio ()(m,n)=(1000,100), increase-in-white-value (multiplier)=1, iter=100,=[0.e+00 0.e+00 0.e+00 1.e+00 6.e+03]custom_size=0, custom_size_arg=0.0, and calibrate_with_util=0 f-metric=r_diff target=equal.FairExpecFairExpecGrpMultObjThrshBlindMale Female NA
106
318
568
386
1987
2635
3283
119
198
3775
2424
3401

Total
992
5008
3600
9600

Dark
Light
NA
Total

Figure 4: Statistics of the Occupations dataset [15].

4.4.2 Setup

We consider race as a protected attribute with four labels (p = 4) and target equal representation based on
race. Let m = 1000 and n = 100. For each choice of α and λ, we draw a set M of m last names uniformly
from the entire population with replacement: The i-th last name is drawn with probability proportional to
c(i). For each last name i ∈ M , we sample a ground-truth race ri (unknown to the algorithms) according to
the distribution qi, and then sample the income wi ∼ Dri.

We report the utility ratio (K) as a function of the risk diﬀerence (F) for diﬀerent algorithms in Figure 3.
We also report F as a function of α (for FairExpec, FairExpecGrp, and Thrsh) and as a function of λ (for
MultObj) in Section A.6.

4.4.3 Results

FairExpec reaches the highest risk diﬀerence of 0.89, followed by FairExpecGrp, which reaches a risk diﬀerence
of 0.84. Thrsh reaches F = 0.79, MultObj reaches F = 0.70, and Blind has F = 0.28.

We do not expect the algorithms to outperform the unconstrained utility (i.e., that of Blind). We observe
that FairExpec has a better Pareto-tradeoﬀ compared to other algorithms, i.e., for any desired level of risk
diﬀerence, it has a better utility ratio (K) than other algorithms. In contrast, while FairExpecGrp also has a
high maximum risk diﬀerence, it is not Pareto-optimal.

All algorithms lose a large fraction of the utility (up to 33%). This is because the unconstrained and
constrained optimal are very diﬀerent: without any constraints, we would roughly select 7% candidates from
some races. However, ensuring equal representation requires selecting roughly 4 times as many candidates
from these races. When the diﬀerence between unconstrained and constrained optimal is smaller, we expect
to lose a smaller fraction of the utility.

4.5 Real-world data for image search

In this simulation, we consider the problem of selecting images under noisy information about the gender
of the person depicted in the image. We derive noisy information about the gender of the person depicted
in an image using a CNN-based gender classiﬁer and use this information to select a gender-balanced set of
images. This could be relevant in mitigating gender bias in image search engines, which have been observed
to over-represent the stereotypical gender in job-related searches [47, 15]. Here, one could ﬁrst select a
balanced subset of images to display on each page, and then order this subset in decreasing order of utility
from top to bottom of each page.

4.5.1 Data

We use a recent image dataset named the Occupations Dataset by [15]. The dataset contains top 100 Google
Image Search results (from December 2019) for 96 occupations related queries. For each image, the dataset
has a gender (coded as men, women, or other), skin-tone (coded as dark or light), and the image’s position
in the search result (an integer in [100]). We present aggregate statistics from the dataset in Figure 4.

Gender classiﬁer. We use an oﬀ-the-shelf face-detector [1] to extract faces of people from the images, and
then use a CNN-based classiﬁer [62] to predict the (supposed) gender of people from their faces. For each

16

AAAAAAAAAAAAAAAAAAAAA

((cid:63))

(less fair)

AAAAAAAAAAAAAAAAAAAAA (more fair)

Risk diﬀerence (F)

(cid:63) this work

A
A
)
A
K
A
(
A
o
A
i
t
a
A
r
A
y
A
t
i
A
l
i
t
A
U
A
A
A
A
A
A
A
A
A
A

Figure 5: Real-world data for image search (Section 4.5): This simulation considers gender as the protected
attribute and uses a CNN-based classiﬁer to derive noisy information about the gender of the person depicted in the
image. The target is to ensure equal representation equal between genders. The y-axis shows the utility ratio K of
diﬀerent algorithms, and the x-axis shows the risk diﬀerence of diﬀerent algorithms; K and F values are averaged over
200 trials, and the error bars represent the standard error of the mean. We observe that FairExpec reaches the highest
risk diﬀerence (F = 0.86), and has a better tradeoﬀ between utility and fairness compared to other algorithms.

image i, the classiﬁer outputs a prediction fi ∈ [0, 1] (resp. 1 − fi) which is the (uncalibrated) likelihood
that the image is of a man (resp. women).15 We calibrate this score as described next.

As a preprocessing step, we remove all images which either have a gender label of NA and for which the
face-detector did not detect any face.16 Then, we calibrate outputs of the classiﬁer (fi) on all remaining
images. This calibration is only done once and is used to compute the noise-information (qi) for the entire
simulation. For more details of the preprocessing used, we refer the reader to Section A.1.5.

Selecting occupations. Next, we infer the occupations which have considerable gender stereotype. To-
wards this, we ﬁx a threshold ζ ∈ [0, 1] and partition the occupations into three sets:

• Styf (ζ): occupations for which at least ζ-fraction of images were labeled to appear to depict women,

• Styf (ζ): occupations for which at least ζ-fraction of images were labeled to appear to depict men, and

• all other occupations.

We ﬁx ζ = 0.8. This gives us |Styf (ζ)| = 12 and |Stym(ζ)| = 29, and 1,877 images with occupations in
Styf (ζ) ∪ Stym(ζ) (for a list of the occupations see Table 17 in Section A.7).

Remark 4.4. Note that we calibrate q on all occupations, and only consider images in a subset of occupations
(less than half ). This means that qs may not be an unbiased estimate of the protected attributes—which is a
hard case for FairExpec.

4.5.2 Setup

In this simulation, we consider gender as the protected attribute with two values (p = 2), and ﬁx m = 500
and n = 100.

15While there could be richer and nonbinary gender categories, many commercial classiﬁers, and the classiﬁer by [62] catego-

rizes images as either male or female.

16Note that we do not check if the detected faces are correct. This introduces some error, which is also expected in practice.

17

0.40.50.60.70.80.91.0Fairness achieved ()0.860.880.900.920.940.960.981.00Utility Ratio ()Image search (custom_cal=0)(m,n)=(500,100), inc-in-male-val (multiplier)=1, iter=200, target=equal, utype=DCG (100/log(r+1)), o_lists=[men_typical08,women_typical08], fmetric=r_diff.FairExpecFairExpecGrpMultObjThreshBlindWe say a particular gender is stereotypical for a given occupation if the majority of images of this
occupation are labeled to appear to depict a person with this gender. For example, men are stereotypical
for occupations in Stym(0.8) and women are stereotypical for occupations in Styf (0.8). We call an image
stereotypical if the dataset labels the person depicted in the image to appear to be of the stereotypical gender
for its occupation. We call an image anti-stereotypical if it is not stereotypical. We would like to ensure
equal representation between stereotypical and anti-stereotypical images.

For each choice of α and λ, we draw a subset M of m images uniformly from all images with occupation
in Styf (ζ) ∪ Stym(ζ). For each image i ∈ M , let its rank be ri ∈ [100]. We compute qi as discussed earlier,
and set its utility wi to wi := (log (1 + ri))−1.

We report the utility ratio (K) as a function of the risk diﬀerence (F) for diﬀerent algorithms in Figure 5.
We also report F as a function of α (for FairExpec, FairExpecGrp, and Thrsh) and as a function of λ (for
MultObj) in Section A.7.

Remark 4.5. Since the underlying application in this simulation is ranking image results, we also considered
Normalized DCG [44] as the utility metric. We observed similar results for this. For completeness, we present
the plot in Section A.7. Furthermore, we also tried other functions for utilities wi, including 100 − ri and
100/ri, and observed similar results.

4.5.3 Results

The risk diﬀerence of Blind (i.e., the risk diﬀerence without any interventions) is F = 0.48. All algorithms
reach a better risk diﬀerence than Blind. Among them, FairExpec reaches the highest risk diﬀerence of
F = 0.86. While the next best algorithm FairExpecGrp has F = 0.79.

Since the algorithms satisfy fairness constraints, we do not expect them to have a higher utility than
Blind; hence, it is not surprising that K ≤ 1. Among the algorithms we consider, we observe that FairExpec
has the Pareto-optimal tradeoﬀ between utility and fairness: it has a higher utility ratio compared to other
algorithms for a given value of risk diﬀerence.

4.6 Additional empirical results

We present additional empirical results with selection lift in Section A. We observe that, indeed, FairExpec
is able to mitigate discrimination with respect to selection lift and reaches the most-fair selection lift in all
experiments. Further, it has the Pareto-optimal tradeoﬀ between utility and fairness compared to other
algorithms in all but one case: in the simulation from Section 4.4, while FairExpec has a lower utility than
MultObj for some levels of selection lift. We believe this is because the constraint region induced by threshold
of selection lift is “diﬀerent” from the constraint region of Program Target. One could correct this, e.g.,
by using [13, Theorem 3.1] to reduce the constraint from selection lift to that of multiple lower and upper
bound constraints.17

Remark 4.6 (Risk diﬀerence on varying n/m). Diﬀerent applications could require selecting diﬀerent
fractions of results from the set of available items. For example, an image search engine might select a
small fraction of available results, whereas a job platform can select a larger fraction. Towards analyzing the
robustness of our approach to the fraction of items selected, we ﬁxed m and varied n in simulations. We ﬁnd
that on increasing n (holding m ﬁxed) the diﬀerence in algorithms’ fairness remains roughly the same. See
Figure 6 for a plot for the simulation in Section 4.4; we present the plots for simulations from Sections 4.2
and 4.5 in Section A.3.

5 Proofs

In this section, we present the proof of Theorem 3.1 (Section 5.1), and present formal statements of our
hardness results for Program Denoised and their proofs (Section 5.2).

17This reduction uses multiple lower bound and upper bound constraints to provably approximate the constraints of selection

lift.

18

AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA

)
r
i
a
f

i

e
c
n
e
r
e
ﬀ
d

e
r
o
m
A(
)
A
F
A
(
A
A
A
A
A
A
A
A
A
A
)
A
r
i
A
a
f
A
s
s
A
e
l
A
(
A
A
A

k
s
i
R

((cid:63))

AAAAAAAAAAAAAAAAAAAAA

Selection size (n)

(cid:63) this work

Figure 6: Risk diﬀerence on varying n/m (Remark 4.6): Towards analyzing the robustness of our approach to
the fraction of items selected (n/m), we ﬁx m = 1000 and vary n from 20 to 100 in Simulation 4.4. The y-axis shows
the risk diﬀerence F of diﬀerent algorithms, and the x-axis shows n; F values are averaged over 100 trials, and the
error bars represent the standard error of the mean. We ﬁnd that increasing on n the diﬀerence in the fairness of
diﬀerent algorithms remains roughly the same.

5.1 Proof of Theorem 3.1

5.1.1 Proof of Lemma 6.4

Proof of Lemma 6.4. For all i ∈ [m], deﬁne Zi ∈ {0, 1} to be the indicator random variable that (i ∈ G(cid:96)).
Notice that

From Deﬁnition 2.1 we have

Using linearity of expectation we get

(cid:88)

i∈G(cid:96)

xi =

(cid:88)

i∈[m]

xiZi.

E[Zi] = qi(cid:96).

m
(cid:88)

i=1

xiqi(cid:96) =

m
(cid:88)

i=1

E[Zi].

(12)

Now, we have

Pr

(cid:34) m
(cid:88)

i=1

xiZi >

m
(cid:88)

i=1

(cid:35)

xiqi(cid:96) + nδ

= Pr

(cid:34) m
(cid:88)

m
(cid:88)

xiZi > E[

Zi] + nδ

(cid:35)

≤ exp

i=1
(cid:18)

−

(cid:18)

≤ exp

−

(cid:18)

≤ exp

−

(δn)2
3
(δn)2
3
δ2n
3

(cid:19)

(cid:19)

i=1
1
E [(cid:80)m
i=1 xiZi]
(cid:19)
1
E [(cid:80)m
i=1 xi]

.

(Additive Chernoﬀ bound [59])

(∀i ∈ [m], Zi ≤ 1)

((cid:80)m

i=1 xi = n)

19

2030405060708090100n0.00.20.40.60.81.0Risk difference ()(m,n)=(1000,100), increase-in-white-value (multiplier)=1, iter=100,=[0.e+00 0.e+00 0.e+00 1.e+00 1.e+04]custom_size=0, custom_size_arg=0.0, and calibrate_with_util=0 f-metric=r_diff target=equal.FairExpecFairExpecGrpMultObjThrshBlindBy a similar argument, we have

(cid:34)

(cid:88)m

i=1

Pr

xiZi <

m
(cid:88)

i=1

(cid:35)

(cid:18)

xiqi(cid:96) − nδ

≤ exp

−

(cid:19)

.

δ2n
2

We get the required result by taking a union bound over all (cid:96) ∈ [p] and using Equation (12).

5.1.2 Proof of Lemma 3.4

Proof. Assume p < m, otherwise the result is trivial. Recall that LP-Denoised has m variables, p pairs
of upper and lower bound constraints, one cardinality constraint ((cid:80)m
i=1 xi = n), and 2m bounding-box
constraints of the form: (xi ≥ 0) and (xi ≤ 1) for each i ∈ [m]. If for some (cid:96) ∈ [p], L(cid:96) = U(cid:96), replace the two
inequality constraints by the equivalent equality constraint.

For any linear program, there exists a basic feasible solution [60, Theorem 2.6]. If x is a basic feasible
solution, it must satisfy m linearly-independent equalities. Notice from each of the at most p pairs of upper
and lower bound constraints18 at most one of the upper bound or the lower bound can be satisﬁed with
equality x. Thus, including the cardinality constraint ((cid:80)m
i=1 xi = n) (but, excluding the bounding-box
constraints, x satisﬁes at most (p + 1) constraints with equality. Further, note that if any of bounding-box
constraints are satisﬁed with equality, then the corresponding index in x is integral. This suﬃces to show
that x has at most (p + 1) fractional entries.

Assume that exactly (p + 1) non-bounding-box constraints are satisﬁed with equality. By the previous
discussion, one of them is the cardinality constraint, and the others must correspond to distinct values of
(cid:96) ∈ [p]. However, for each i ∈ [m] it holds that (cid:80)
(cid:96)∈[p] qi(cid:96) = 1. Further, in this case, the upper bounds or
lower bound corresponding to the p tight constraints must sum to n (otherwise, the (p+1) non-bounding-box
constraints cannot hold together). But, in this case, all the p upper or lower bound constraints imply the
cardinality constraint. Thus, at most p of the (p + 1) non-bounding-box constraints are linearly independent.
It follows that any basic feasible solution has at most p fractional constraints.

The next fact shows that Lemma 3.4 is tight.

Fact 5.1. There exists an instance of Program Denoised such that the unique optimal solution to LP-
Denoised has p fractional entries.

Proof. Fix ε > 0 and s = 1. Let p := p1 and drop the superscripts on the variables. Let n = p, m = p + 1,
and for all (cid:96) ∈ [p], L(cid:96) = 0, and U(cid:96) = 1. Let ei be the i-th standard basis vector on Rp. Deﬁne

wi =

(cid:40)

1 if i < m,
2 if i = m

and qi =

(cid:40)

ei
1
p

1p

if i < m,
if i = m.

Notice that qi ∈ ∆p for each i ∈ [m]. It is easy to see that any optimal solution x has xm = 1. Further, x
must pick n items ((cid:80)m
(cid:96)∈[p] U(cid:96) = n. So, each upper bound constraint must be tight. This
suﬃce to show the unique optimal is:

i=1 xi = n) and (cid:80)

(cid:98)x(cid:63)
i =

(cid:40)

1 − 1
p
1

if 1 ≤ i < m,
if i = m.

Note that (cid:98)x(cid:63) has p fractional values.

5.1.3 Proof of Theorem 3.1

Proof of Theorem 3.1. We claim that Algorithm 1 satisﬁes the claims in the theorem.

18If the lower bound and the upper bound are the same, we discard one of the (repeated) inequalities.

20

Run time. Step 1 ﬁnds a basic feasible solution for LP-Denoised. Since this is a linear program, one can
do so in time polynomial in the bit-complexity of the input, say, using the ellipsoid method (see, e.g.,[36]).
Step 2 is clearly linear time. Thus, the bound on the running time follows.

Correctness. Since x (in Step 1) is feasible for LP-Denoised, it satisﬁes

For all (cid:96) ∈ [p],

(cid:88)m

i=1

qi(cid:96)xi ≤ U(cid:96) + δn,

L(cid:96) − δn ≤
(cid:88)m

xi = n.

i=1

Further, since x is a basic feasible solution of LP-Denoised, x has at most p fractional values by Lemma 3.4.
Thus, x(cid:48) obtained by rounding up each nonzero coordinate to 1 (in Step 2), satisﬁes (Recall that qi(cid:96) ∈ [0, 1])

For all (cid:96) ∈ [p],

L(cid:96) − δn ≤
(cid:88)m

n ≤

i=1

(cid:88)m

i=1

qi(cid:96)x(cid:48)

i ≤ U(cid:96) + δn + p,

x(cid:48)
i ≤ n + p.

(13)

(14)

Lemma 3.3 says that with probability at least 1 − 2p exp (cid:0)−δ2n/3(cid:1) (over the noise in the protected attributes)

For all (cid:96) ∈ [p],

(cid:12)
(cid:12)
(cid:12)

(cid:88)

i∈G(cid:96)

x(cid:48)
i −

(cid:88)m

i=1

qi(cid:96)x(cid:48)
i

(cid:12)
(cid:12)
(cid:12) ≤ nδ.

Now, combining Equation (13) and Equation (15), we get that

For all (cid:96) ∈ [p],

L(cid:96) − 2δn ≤

(cid:88)m

i=1

qi(cid:96)x(cid:48)

i ≤ U(cid:96) + 2δn + p

(15)

(16)

holds with probability at least 1 − 2p exp (cid:0)−δ2n/3(cid:1) (over the noise in the protected attributes). Equation (16)
implies that x(cid:48) violates Equation (4) by at most (p + 2δn) (additive) and Equation (14) implies x(cid:48) violates
Equation (5) by at most p (additive). Let E1 be the event that Equation (16) and Equation (14) hold. From
the above discussion, we conclude that

Pr[E1] ≥ 1 − 2p exp (cid:0)−δ2n/3(cid:1) .

Let x(cid:63) be an optimal solution of Program Target. It remains to show that x(cid:48) has a value higher than x(cid:63).
Let E2 be the event that x(cid:63) is feasible for Program Target. Then, conditioning on E2 we that

(cid:88)m

i=1

wix(cid:63)

i ≤

≤

(cid:88)m

i=1

(cid:88)m

i=1

wixi

wix(cid:48)
i.

(x is optimal for Program Target)

(x(cid:48) ≥ x and w ≥ 0)

Thus, conditioning on E2 gives us that x(cid:48) has a value higher than x(cid:63). From Lemma 3.3 we get that

Taking the union bound over E1 and E2 we get the required result.

Pr[E2] ≥ 1 − 2p exp (cid:0)−δ2n/3(cid:1) .

5.2 Hardness results

In this section, we present the formal statements of our hardness results for Program Denoised and their
proofs.

5.2.1 Proof of Theorem 3.5 (1)

Theorem 5.2. For all p ≥ 2, deciding if Program Denoised is feasible is NP-hard.

Proof. We present a reduction from the subset-sum problem which is known to be NP-hard [24].

21

Subset sum problem. Given a set of m integers A = {a1, . . . , an} ∈ Zn
≥0 in binary, and a target sum
t ∈ Z≥0. The subset-sum problem is: Is there a subset of A such that the items of the subset sum to t?

.
Without loss of generality assume that ai ≤ t for all i ∈ [n] and t > 0. If not, we can remove items which
are larger than t. Since ai ≥ 0, we can solve the subset-problem for t = 0 in linear-time.

Reduction. Given an instance of the subset sum problem corresponding instance Program Target as
follows: Let the number of items to select be n and let there be m := 2n items. Consider one attribute (i.e.,
s := 1) with two disjoint values p := 2. For each item i ∈ [m], let

qi =

(cid:40)(cid:2) ai

t , 1 − ai

t

[0, 1]

(cid:3)

if i ∈ [n],
if i ∈ [n + 1, m].

Deﬁne the constraints to be U1 = 1 and U2 = n − 1. In this construction, for each i ∈ [n], the integer ai ∈ A
corresponds to the i-th item. Notice that the input to Program Target is polynomial-sized in the input of
the subset-sum problem. Further, the values qi, and constraints U can be calculated in polynomial time.

It remains to show that A has a subset S ⊆ A which sums to U if and only if Program Target is feasible.

( =⇒ ). If A has a subset S ⊆ A such that (cid:80)
|S| ≤ n items corresponding to integers in S, and any n − |S| ≤ n items from [m]\[n]. It holds

a∈S a = t, deﬁne the solution x of Program Target by choosing

(cid:88)

i∈[m]

xiqi1 =





n
(cid:88)

xiqi1 +

(cid:88)

xiqi1

 =



(cid:88)





i=1

i∈[m]\[n]

a∈S



0

 = 1 ≤ U1,

a
t

+

n−|S|
(cid:88)

i=1

(cid:88)

i∈[m]

xiqi2 =





n
(cid:88)

i=1

xiqi2 +

(cid:88)

i∈[m]\[n]





xiqi2

 =



(cid:18)

1 −

(cid:19)

a
t

+

n−|S|
(cid:88)

i=1

(cid:88)

a∈S



 = n − 1 ≤ U2.
1

Thus, Program Target is feasible if the subset-sum instance is a ‘YES’ instance.

(⇐=). For the opposite direction, notice that for any subset x, (cid:80)
i∈[m] xi(qi1 + qi2) = n, i.e., the sum of qi(cid:96)
for all items and both properties is ﬁxed. Since U1 + U2 = n, the constraints of both properties must hold
with equality:

(cid:88)

i∈[m]

xiqi1 = 1 and

(cid:88)

i∈[m]

xiqi2 = n − 1.

(17)

Deﬁne a solution S ⊆ A which has items of A corresponding to items i ∈ [n] present in x. Since qi1 = 0 for
any i ∈ [m]\[n], it follows from (17) that (cid:80)
a/t = 1 or (cid:80)
a∈S a = t. Thus, the subset sum instance is a
‘YES’ instance if Program Target is feasible.

a∈S

Combining both cases gives us that the Program Target instance is feasible iﬀ the subset sum instance
is a ‘YES’ instance. Thus, we can use an algorithm to check the feasibility of Program Target to solve an
arbitrary subset sum problem.

5.2.2 Proof of Theorem 3.5 (2)

Theorem 5.3. For all p ≥ 3, Program Denoised is APX-hard.

Proof. We present a reduction from the d-dimensional knapsack problem which is known to be APX-hard
for d ≥ 2 [49].

22

d-dimensional knapsack problem (d-KP). Given a vectors v, c ∈ Rk
dimensional knapsack (d-KP) is the problem to solve the following integer linear program.

>0, a matrix w ∈ Rk×d
≥0

the d-

max
x∈{0,1}k

s.t. ∀ (cid:96) ∈ [d],

(cid:88)

i∈[k]
(cid:88)

vixi

(18)

wi(cid:96)xi ≤ c(cid:96),

(Capacity; 19)

i∈[k]
x ∈ {0, 1}k.

(Integrality; 20)

.
Without loss of generality we assume wi(cid:96) ≤ c(cid:96) for all i ∈ [k] and (cid:96) ∈ [d]. If not, we can remove such items
in linear time.

Reduction. Given an instance of d-KP deﬁne an instance of Program Target with s := 1 as follows:

1. Set p := d + 1, n := k, m := 2k, and Up := m. For all (cid:96) ∈ [p − 1] set

U(cid:96) :=

c(cid:96)
k∈[d] ck

(cid:80)

2. For each item i ∈ [k] set wi := vi, and for all (cid:96) ∈ [p − 1] set

qi(cid:96) :=

wi(cid:96)
j∈[d] cj

(cid:80)

Finally set, qip = 1 − (cid:80)

i∈[p−1] qi(cid:96).

.

.

3. Add k additional dummy items (k + 1, k + 2, . . . , 2k). For all i ∈ [m]\[n], set qip = 1 and wi := 0. Also,

for all i ∈ [m]\[n] and (cid:96) ∈ [p − 1] set qi(cid:96) = 0.

Recall that ∆p is the p-dimensional simplex. Note that for all i ∈ [m], qi ∈ ∆p. Now, it is easy to see that
the construction is a valid instance of the Program Target.
Let x(cid:63) ∈ {0, 1}k be a solution d-KP, such that, (cid:80)m

i=1 x(cid:63) = r ≤ l. Deﬁne a solution y(cid:63) ∈ {0, 1}m to
Program Target by selecting any (k − r) dummy items. Alternatively, given a solution given a y(cid:63) ∈ {0, 1}m
to Program Target deﬁne a solution x(cid:63) ∈ {0, 1}k to d-KP, by omitting the dummy items.

( ⇐⇒ ). Consider a solution x(cid:63) to d-KP. Then for all (cid:96) ∈ [d], x(cid:63) satisﬁes

(cid:88)

i∈[k]

wi(cid:96)x(cid:63)

i ≤ c(cid:96) ⇐⇒

⇐⇒

⇐⇒

(cid:88)

i∈[k]
(cid:88)

i∈[k]
(cid:88)

(cid:32)

wi(cid:96)
j∈[d] cj

(cid:80)

(cid:33)

(cid:32)

x(cid:63)
i ≤

c(cid:96)
j∈[d] cj

(cid:80)

(cid:33)

qi(cid:96)x(cid:63)

i ≤ U(cid:96)

qi(cid:96)y(cid:63)

i ≤ U(cid:96).

(Using qi(cid:96) = 0 for i ≤ [m]\[n], (cid:96) ∈ [p − 1])

i∈[m]

Further, by construction (cid:80)m
instance of Program Target. Finally, it holds

i=1 y(cid:63)

i = n. Thus x(cid:63) is feasible for d-KP iﬀ y(cid:63) is feasible for the corresponding

(cid:88)

i∈[k]

vix(cid:63)

i =

(cid:88)

i∈[k]

wix(cid:63)

i =

(cid:88)

i∈[m]

wiy(cid:63)
i .

(Using wi = 0 for i ∈ [m]\[n])

This shows that the reduction is approximation preserving. We conclude that Program Target is APX-hard
for all p = (d + 1) ≥ 3.

23

5.2.3 Proof of Theorem 3.5 (3)

Theorem 5.4. For every constant c > 0, the following violation gap variant of Program Denoised is NP-
hard.

• Output YES if the input instance is satisﬁable.

• Output NO if there is no solution which violates every upper bound constraint at most an additive

factor of c.

Our reduction below is similar to the one used in [18, Theorem 10.4].

Proof. We use the inapproximability of the independent set [42, 75] to prove the theorem. The inapprox-
imability result states that: Given a graph G(V, E) it is NP-hard to approximate the size of the maximum
independent set to within a multiplicative factor of |V |1−ε for every constant ε > 0.

Fix any constant c > 0. Then our reduction is as follows:

Reduction. Consider an instance of the independent set problem, i.e., given a graph G(V, E) and a number n ∈
N to check whether G has an independent set of size at least n. Construct an instance of Program Denoised
with m = |V | candidates and the same n. For each clique C in G of size at least (c + 2) add a protected
attribute C which takes two values, and set an upper bound on the number of items with value 1 for attribute
C: U (k)
0 = n. Further,
(cid:1) = mc+2 = poly(m)
for each vertex v ∈ C, set q(C)
protected attributes.

v,0 = 0. Note that there are at most (cid:0) m

1 = 1. Formally, for each attribute C, set L(C)

v,1 = 1 and q(C)

1 = 0 and U (C)

0 = 0 and U (C)

1 = 1 and L(C)

c+2

We claim the following:

• If the Program Denoised is not feasible then G does not have an independent set of size n.

• If Program Denoised has a solution which violates the upper bound constraints by at most c (additive),

then G has an independent set of size at least n1/(c+1).

These claims prove the theorem: if there was an algorithm A to solve Program Denoised in polynomial time,
then we can use it to approximate independent set within |V |1−1/(c+1) factor in polynomial time,19 which is
NP-hard.

It remains to establish the claim. Notice that if G has an independent set S of size n, then this gives a
feasible solution to Program Denoised by picking the items corresponding to elements in S. This establishes
the ﬁrst claim.

Given a subset S which violates the constraints of Program Denoised by at most c (additive), consider
the subgraph GS of G induced by S. GS does not contain any (c + 2)-cliques. By [19, Lemma 4.3], GS has
an independent set of size at least n1/(c+1), so also G. This establishes the second claim.

6 Theoretical results with multiple protected attributes

In this section, we extend our theoretical results to multiple nonbinary (and intersectional) protected at-
tributes (i.e., s ≥ 1 and p ≥ 1). Like Section 3, our main algorithmic result is an approximation algorithm
for Program Target.

Theorem 6.1 (An approximation algorithm for Program Target when s ≥ 1). There is an algorithm
(Algorithm 2) that given an instance of Program Target and q from Deﬁnition 2.1, outputs a selection
x ∈ {0, 1}m, such that, with probability at least 1 − 4ps exp (cid:0)−δ2n/3(cid:1) over the noise in the protected attributes
of each item, the selection x

1. has a value at least as high as the optimal value of Program Target,

2. violates the cardinality constraint (5) (additively) by at most

19The approximation algorithm returns the same answer as the A.

1 +

(cid:88)s

k=1

(pk − 1),

24

3. and violates the fairness constraints (4) (additively) by at most

1 + 2δn +

(cid:88)s

k=1

(pk − 1).

The algorithm runs in polynomial time in the bit complexity of the input.

Remark 6.2. Note that in the special case that s = 1, we have

1 +

1 + 2δn +

(cid:88)s

k=1

(cid:88)s

k=1

(pk − 1) = p1,

(pk − 1) = 1 + 2δn + p1.

Substituting these values in Theorem 6.1, we can recover the statement of Theorem 3.1.

Compared to Theorem 3.1, the high-probability bound and approximation factors are weaker by (roughly)
a factor of s. Note that, in many real-world applications, p and s are small constants compared to the number
of items selected n. Here, Theorem 6.1 implies that x violates the fairness constraints in Equation (4) by a
multiplicative factor of at most (1 + 2δ + o(1)) and the constraint in Equation (5) by a multiplicative factor
of at most (1 + o(1)) with high probability.

Algorithm 2 is equivalent to Algorithm 1; the only diﬀerence is that Algorithm 2 solves the Program De-

noised for s ≥ 1.

Remark 6.3 (Analogous to Remark 3.2). We can strengthen Theorem 6.1 to guarantee that Algo-
rithm 2 ﬁnds an x ∈ {0, 1}m which does not violate the lower bound fairness constraint (left inequality in
Equation (4)) and violates the upper bound fairness constraints by at most (s·(p−1)+2δn) (without changing
other conditions). In particular, this shows that, if one places only lower bound fairness constraints, then
subset found by Algorithm 2 would never violate the fairness constraints.

Proof of Theorem 6.1

The proof of Theorem 6.1 follows a similar template as the proof of Theorem 3.1. Instead of repeating the
entire proof, we highlight how the proof of Theorem 6.1 diﬀers from the proof of Theorem 3.1.

We ﬁrst present a generalization of Lemma 3.3.
Lemma 6.4. For all δ ∈ (0, 1) and x ∈ [0, 1]m, s.t., (cid:80)m

i=1 xi = n:

∀ k ∈ [s], (cid:96) ∈ [pk],

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

i∈G(k)
(cid:96)

(cid:88)

xi −

q(k)
i(cid:96) xi

i∈[m]

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ nδ

holds with probability at least 1 − 2ps exp (cid:0)−δ2n/3(cid:1) over the noise in the protected attributes of each item.

Proof. The result follows by applying Lemma 3.3 for each attribute k ∈ [s] and taking the union bound over
the events.

Remark 6.5 (Hardness results). Note that the hardness results from Theorem 5.2 and Theorem 5.3
also apply when s ≥ 1. Like in Section 3, we overcome this hardness by allowing solutions to violate the
constraints of Program Denoised by an additive amount.

Consider the following linear-programming relaxation of Program Denoised

(cid:88)m

i=1

wixi

(Relaxed-Denoised; 21)

(cid:88)m

i=1

i(cid:96) xi ≤ U (k)
q(k)

(cid:96) + δn, ∀ k ∈ [s], (cid:96) ∈ [pk],

max
x∈[0,1]m

s.t.,

L(k)
(cid:96) − δn ≤
(cid:88)m

xi = n.

We can prove the following lemma.

i=1

25

Lemma 6.6 (An optimal solution with p fractional entries). Any basic feasible solution x ∈ [0, 1]m
of LP-Denoised has at most

(cid:16)

min

m, 1 +

(cid:88)s

(cid:17)

(pk − 1)

k=1

fractional values, i.e.,

(cid:88)m

i=1

I[xi ∈ (0, 1)] ≤ min

(cid:16)

m, 1 +

(cid:88)s

k=1

(pk − 1)

(cid:17)

.

Proof. In the proof of Lemma 3.4, we show that any basic feasible solution satisﬁes at most p1 linearly-
independent non-bounding-box constraints with equality. Toward this, we show that any set of p1 linear-
independent lower bound or upper bound constraints which are satisﬁed with equality, must contain the
cardinality constraint ((cid:80)m

i=1 xi = n) in their span.

Lemma 6.6 follows by using the same argument for all protected attributes and noticing that for each
protected attribute k ∈ [s], any set of pk linear-independent lower bound or upper bound constraints which
are satisﬁed with equality, must contain the cardinality constraint ((cid:80)m

i=1 xi = n) in their span.

Remark 6.7. Note that in the special case that s = 1, we have

1 +

(cid:88)s

k=1

(pk − 1) = p1,

and we recover Lemma 3.4 from Lemma 6.6. Further, using Fact 5.1, it follows that Lemma 6.6 is tight.

Proof of Theorem 6.1. The run time follows since there are polynomial time algorithms to ﬁnd a basic feasible
solution of a linear program.

The rest of proof follows a similar template to that of Theorem 3.1, and follows by using the generalizations

of the lemmas proved above and replacing the additive error of p1 (in x(cid:48) due to rounding) with

1 +

(cid:88)s

k=1

(pk − 1).

Let x(cid:63) be an optimal solution for Program Target. Using Lemma 6.4 it can be shown that x(cid:63) is feasible for
Program Denoised with probability at least 1 − 2ps exp (cid:0)−δ2n/3(cid:1). Assuming this event happens, it holds that
the rounded solution x(cid:48) (from Step 2 of Algorithm 2) has a value at least as large as x(cid:63). (This follows from
the argument as in the proof of Theorem 3.1).

Further, from Lemma 3.4 it follows that x(cid:48) picks at most

1 +

(cid:88)s

k=1

(pk − 1)

more elements than x. Thus, x(cid:48) violates Equation (7), and so Equation (5) by at most 1 + (cid:80)s
k=1(pk − 1)
(additively). Further, x(cid:48) violates the fairness constraints of Program Denoised by at most 1 + (cid:80)s
k=1(pk − 1)
(additively). Combining this with Lemma 6.4, we get that with probability at least 1 − 2ps exp (cid:0)−δ2n/3(cid:1), x(cid:48)
violates the constraints of Program Target by at most the bound claimed in Theorem 6.1.

Finally, taking a union bound over above two events completes the proof.

Algorithm 2: Algorithm for Program Target
Input: Numbers n, s ∈ N, utility vector w ∈ Rm, and for each k ∈ [s]: a probability matrix

q(k) ∈ [0, 1]m×pk and constraint vectors L(k), U (k) ∈ Rpk
≥0.

1. Solve x ← Find a basic feasible solution to linear-programming relaxation
of Program Denoised with inputs (n, s, q, w, L, U ).
.
2. Set x(cid:48)
3. Return x(cid:48).

i := (cid:100)xi(cid:101) for all i ∈ [m]. //Round solution

26

7 Limitations and future work

We consider the natural setting where the utility of a subset is the sum of utilities of its items. A useful
extension to this work could consider submodular objectives, which are relevant when the goal is to select a
subset which summarizes a collection of items [55].

Apart from this, some works also study other variants of subset selection, for example, diverse (and fair)
subset selection in the online settings [66]. Studying and mitigating bias in the presence of noise under this
variant is an interesting direction for future work.

Further, our approach assumes access to probabilistic information about the true protected attributes. If
this information is itself is skewed or incorrect, then our approach can have a poor performance. Although,
empirical results on real-world data suggest that our approach can improve fairness even when there is some
skew in the noise information (see, e.g., Remark 4.4). Still, determining probabilistic information more
reliably is an important problem, and recent works have made some progress toward this goal [43, 45].

Furthermore, while we focus on the subset selection problem, our results can also extend to the ranking
problem (where after selecting a subset, it must be ordered) by satisfying the fairness constraints in the
top-k positions, for a small number choices of k, say k1 ≤ k2 ≤ · · · ≤ kg;20 this reduces the high-probability
guarantee from 1 − 4p exp (−δ2n/3) to 1 − 4gp exp (−δ2k1/3).21 This is particularly relevant in the setting where
results are displayed one page at a time. Satisfying the constraints for a larger number of positions with high
probability might require stronger information about noise, and is an interesting direction for future work.
Empirically, we could report fairness on several other metrics, e.g., selection lift or extended diﬀerence [11,
39, 40]. We focus on risk diﬀerence as it is closer to our approach. Nevertheless, Program Target can mitigate
discrimination with respect to selection lift (and other metrics) as well (e.g., see [16, 13]). Empirically
evaluating this would be an important direction for future work.

Finally, we note that bias is a systematic issue and this work only addresses one aspect of it. Indeed, any
such approach is limited by how people and the broader system uses the subset presented to them; e.g., a
recruiter on an online hiring platform might deliberately reject minority candidates even when presented with
a representative candidate subset. Thus, it is important to complement our approach with other necessary
tools to mitigate bias and counter discrimination.

8 Conclusion

We consider the problem of mitigating bias in subset selection when the protected attributes are noisy,
or are missing for some or all of the entries and must be imputed using proxy variables. We note that
accounting for real-world noise in algorithms is important to mitigate bias, and not accounting for noise
can have unintended adverse eﬀects, e.g., adding fairness constraints in a noise oblivious fashion can even
decrease fairness when the protected attributes are noisy (Section 4.2).

We consider a model of noise where we have probabilistic information about the protected attributes, and
develop a framework to mitigate bias, which given this information, can satisfy one from a large class of fair-
ness constraints with at most a small multiplicative error with high probability (Section 3). In our empirical
study, we observe that this approach achieves a high level of fairness on standard fairness metrics (e.g., risk
diﬀerence), even when the probabilistic information about protected attributes is skewed (Remark 4.4 and
Section 7), and this approach has a better tradeoﬀ between utility and fairness compared to several prior
approaches (Section 4).

Acknowledgements

This research was supported in part by a J.P. Morgan Faculty Award. We would like to thank Nisheeth K.
Vishnoi for several useful discussions on the problem and approach.

20To do so: ﬁrst, pick k1 items and place them top-k1 positions, then from those remaining pick (k2 − k1) items and place

them in the next (k2 − k1) positions, and so on.

21This follows by using the union bound. However, as the events involved are correlated, it may be possible to get a stronger

bound using a more sophisticated analysis.

27

References

[1] OpenCV: Open Source Computer Vision Library. https://github.com/opencv/opencv_3rdparty/

raw/dnn_samples_face_detector_20170830/res10_300x300_ssd_iter_140000.caffemodel.

[2] Akshay Agrawal, Robin Verschueren, Steven Diamond, and Stephen Boyd. A rewriting system for

convex optimization problems. Journal of Control and Decision, 5(1):42–60, 2018.

[3] Dana Angluin and Philip D. Laird. Learning from noisy examples. Mach. Learn., 2(4):343–370, 1987.

[4] Pranjal Awasthi, Matth¨aus Kleindessner, and Jamie Morgenstern. Equalized odds postprocessing under
imperfect group information. In International Conference on Artiﬁcial Intelligence and Statistics, pages
1770–1780. PMLR, 2020.

[5] Tony Barboza and Joseph Serna. As coronavirus deaths surge, missing racial data worry l.a. county
oﬃcials. Los Angeles Times, April 2020. https://www.latimes.com/california/story/2020-04-06/
missing-racial-data-coronavirus-deaths-worries-los-angeles-county-officials.

[6] Marianne Bertrand and Sendhil Mullainathan. Are emily and greg more employable than lakisha and
jamal? a ﬁeld experiment on labor market discrimination. American economic review, 94(4):991–1013,
2004.

[7] Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial
gender classiﬁcation. In FAT, volume 81 of Proceedings of Machine Learning Research, pages 77–91.
PMLR, 2018.

[8] Consumer Financial Protection Bureau. Using publicly available information to proxy for uniden-
https://files.consumerfinance.gov/f/201409_cfpb_report_

2014.

tiﬁed race and ethnicity.
proxy-methodology.pdf.

[9] United States Census Bureau. Current Population Survey (CPS).

https://www.census.gov/

programs-surveys/cps.html.

[10] United States Census Bureau. FINC-02. Age of Reference Person, by Total Money Income, Type
of Family, Race and Hispanic Origin of Reference Person. https://www.census.gov/data/tables/
time-series/demo/income-poverty/cps-finc/finc-02.html.

[11] Toon Calders and Sicco Verwer. Three naive bayes approaches for discrimination-free classiﬁcation.

Data Min. Knowl. Discov., 21(2):277–292, 2010.

[12] Carlos Castillo. Fairness and transparency in ranking. SIGIR Forum, 52(2):64–71, January 2019.

[13] L. Elisa Celis, Lingxiao Huang, Vijay Keswani, and Nisheeth K. Vishnoi. Classiﬁcation with Fairness
Constraints: A Meta-Algorithm with Provable Guarantees. In FAT, pages 319–328. ACM, 2019.

[14] L. Elisa Celis, Lingxiao Huang, Vijay Keswani, and Nisheeth K. Vishnoi. Fair classiﬁcation with noisy

protected attributes: A framework with provable guarantees. CoRR, abs/2006.04778, 2020.

[15] L. Elisa Celis and Vijay Keswani. Implicit diversity in image summarization. Proc. ACM Hum. Comput.

Interact., 4(CSCW2):139:1–139:28, 2020.

[16] L. Elisa Celis, Vijay Keswani, Damian Straszak, Amit Deshpande, Tarun Kathuria, and Nisheeth K.
Vishnoi. Fair and Diverse DPP-Based Data Summarization. In ICML, volume 80 of Proceedings of
Machine Learning Research, pages 715–724. PMLR, 2018.

[17] L. Elisa Celis, Anay Mehrotra, and Nisheeth K. Vishnoi. Interventions for ranking in the presence of

implicit bias. In FAT*, pages 369–380. ACM, 2020.

[18] L. Elisa Celis, Damian Straszak, and Nisheeth K. Vishnoi. Ranking with Fairness Constraints.

In
ICALP, volume 107 of LIPIcs, pages 28:1–28:15. Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik,
2018.

28

[19] Chandra Chekuri and Sanjeev Khanna. On multidimensional packing problems. SIAM journal on

computing, 33(4):837–851, 2004.

[20] Jiahao Chen, Nathan Kallus, Xiaojie Mao, Geoﬀry Svacha, and Madeleine Udell. Fairness under un-
In FAT, pages 339–348. ACM,

awareness: Assessing disparity when protected class is unobserved.
2019.

[21] Flavio Chierichetti, Ravi Kumar, Silvio Lattanzi, and Sergei Vassilvitskii. Fair clustering through

fairlets. In NIPS, pages 5036–5044, 2017.

[22] Flavio Chierichetti, Ravi Kumar, Silvio Lattanzi, and Sergei Vassilvitskii. Matroids, matchings, and
fairness. In AISTATS, volume 89 of Proceedings of Machine Learning Research, pages 2212–2220. PMLR,
2019.

[23] Andrew J Coldman, Terry Braun, and Richard P Gallagher. The classiﬁcation of ethnic status using

name information. Journal of Epidemiology & Community Health, 42(4):390–395, 1988.

[24] Thomas H Cormen, Charles E Leiserson, Ronald L Rivest, and Cliﬀord Stein. Introduction to algorithms.

MIT press, 2009.

[25] Matthew Costello, James Hawdon, Thomas Ratliﬀ, and Tyler Grantham. Who views online extremism?

individual attributes leading to exposure. Computers in Human Behavior, 63:311–320, 2016.

[26] N.R. Council, D.B.S.S. Education, C.N. Statistics, P.D.C.R.E. Data, E. Perrin, and M.V. Ploeg. Elim-

inating Health Disparities: Measurement and Data Needs. National Academies Press, 2004.

[27] Marina Drosou, H. V. Jagadish, Evaggelia Pitoura, and Julia Stoyanovich. Diversity in big data: A

review. Big Data, 5(2):73–84, 2017.

[28] Marc Elliott, Peter Morrison, Allen Fremont, Daniel Mccaﬀrey, Philip Pantoja, and Nicole Lurie. Using
the census bureau’s surname list to improve estimates of race/ethnicity and associated disparities. Health
Services and Outcomes Research Methodology, 9:252–253, 06 2009.

[29] Marc N Elliott, Allen Fremont, Peter A Morrison, Philip Pantoja, and Nicole Lurie. A new method
for estimating race/ethnicity and associated disparities where administrative records lack self-reported
race/ethnicity. Health services research, 43(5 Pt 1):1722—1736, October 2008.

[30] Vitalii Emelianov, Nicolas Gast, Krishna P. Gummadi, and Patrick Loiseau. On fair selection in the

presence of implicit variance. In EC, pages 649–675. ACM, 2020.

[31] Erin L Faught, Patty L Williams, Noreen D Willows, Mark Asbridge, and Paul J Veugelers. The
association between food insecurity and academic achievement in canadian school-aged children. Public
health nutrition, 20(15):2778–2785, 2017.

[32] Kevin Fiscella and Allen Fremont. Use of geocoding and surname analysis to estimate race and ethnicity.

Health services research, 41:1482–500, 09 2006.

[33] Kevin Fiscella and Allen M Fremont. Use of geocoding and surname analysis to estimate race and

ethnicity. Health services research, 41(4p1):1482–1500, 2006.

[34] Benoˆıt Fr´enay and Michel Verleysen. Classiﬁcation in the presence of label noise: A survey. IEEE

Trans. Neural Networks Learn. Syst., 25(5):845–869, 2014.

[35] Sahin Cem Geyik, Stuart Ambler, and Krishnaram Kenthapadi. Fairness-aware ranking in search &
recommendation systems with application to linkedin talent search. In KDD, pages 2221–2231. ACM,
2019.

[36] Martin Gr¨otschel, L´aszl´o Lov´asz, and Alexander Schrijver. Geometric algorithms and combinatorial

optimization, volume 2. Springer Science & Business Media, 2012.

29

[37] Maya R. Gupta, Andrew Cotter, Mahdi Milani Fard, and Serena Wang. Proxy fairness. CoRR,

abs/1806.11212, 2018.

[38] LLC Gurobi Optimization. Gurobi optimizer reference manual, 2020.

[39] Sara Hajian, Josep Domingo-Ferrer, and Oriol Farr`as. Generalization-based privacy preservation and
discrimination prevention in data publishing and mining. Data Mining and Knowledge Discovery,
28(5):1158–1188, Sep 2014.

[40] Sara Hajian, Josep Domingo-Ferrer, Anna Monreale, Dino Pedreschi, and Fosca Giannotti.
Discrimination- and privacy-aware patterns. Data Mining and Knowledge Discovery, 29(6):1733–1782,
Nov 2015.

[41] Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. In Daniel D.
Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors, Advances
in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing
Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 3315–3323, 2016.

[42] Johan Hastad. Clique is hard to approximate within n1−ε.
Foundations of Computer Science, pages 627–636. IEEE, 1996.

In Proceedings of 37th Conference on

[43] ´Ursula H´ebert-Johnson, Michael P. Kim, Omer Reingold, and Guy N. Rothblum. Multicalibration:
Calibration for the (computationally-identiﬁable) masses. In ICML, volume 80 of Proceedings of Machine
Learning Research, pages 1944–1953. PMLR, 2018.

[44] Kalervo J¨arvelin and Jaana Kek¨al¨ainen. Cumulated gain-based evaluation of IR techniques. ACM

Trans. Inf. Syst., 20(4):422–446, 2002.

[45] Christopher Jung, Changhwa Lee, Mallesh M. Pai, Aaron Roth, and Rakesh Vohra. Moment multical-

ibration for uncertainty estimation. CoRR, abs/2008.08037, 2020.

[46] Nathan Kallus, Xiaojie Mao, and Angela Zhou. Assessing algorithmic fairness with unobserved protected

class using data combination. In FAT*, page 110. ACM, 2020.

[47] Matthew Kay, Cynthia Matuszek, and Sean A. Munson. Unequal representation and gender stereotypes

in image search results for occupations. In CHI, pages 3819–3828. ACM, 2015.

[48] Michael Kearns, Aaron Roth, and Zhiwei Steven Wu. Meritocratic fairness for cross-population selection.

In International Conference on Machine Learning, pages 1828–1836, 2017.

[49] Hans Kellerer, Ulrich Pferschy, and David Pisinger. Knapsack problems. Springer, 2004.

[50] Jon M. Kleinberg and Manish Raghavan. Selection problems in the presence of implicit bias. In ITCS,
volume 94 of LIPIcs, pages 33:1–33:17. Schloss Dagstuhl - Leibniz-Zentrum f¨ur Informatik, 2018.

[51] Howard K Koh, Garth Graham, and Sherry A Glied. Reducing racial and ethnic disparities: the action
plan from the department of health and human services. Health Aﬀairs, 30(10):1822–1829, 2011.

[52] Gueorgi Kossinets. Eﬀects of missing data in social networks. Social Networks, 28(3):247–268, 2006.

[53] Matt J. Kusner, Joshua R. Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. In NIPS,

pages 4066–4076, 2017.

[54] Alexandre Louis Lamy and Ziyuan Zhong. Noise-tolerant fair classiﬁcation. In NeurIPS, pages 294–305,

2019.

[55] Hui Lin and Jeﬀ Bilmes. A class of submodular functions for document summarization. In Proceed-
ings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language
Technologies - Volume 1, HLT ’11, pages 510–520, 2011.

30

[56] LinkedIn. Inferred Age or Gender on LinkedIn, February 2018. https://www.linkedin.com/help/

linkedin/answer/3566/inferred-age-or-gender-on-linkedin?lang=en.

[57] Tongliang Liu and Dacheng Tao. Classiﬁcation with noisy labels by importance reweighting. IEEE

Trans. Pattern Anal. Mach. Intell., 38(3):447–461, 2016.

[58] Naresh Manwani and P. S. Sastry. Noise tolerance under risk minimization. IEEE Trans. Cybern.,

43(3):1146–1151, 2013.

[59] Rajeev Motwani and Prabhakar Raghavan. Randomized algorithms. Cambridge university press, 1995.

[60] Christos H Papadimitriou and Kenneth Steiglitz. Combinatorial optimization: algorithms and complex-

ity. Courier Corporation, 1998.

[61] Adrian Rosebrock. Face detection with OpenCV and deep learning, February 2018. https://www.

pyimagesearch.com/2018/02/26/face-detection-with-opencv-and-deep-learning/.

[62] Rasmus Rothe, Radu Timofte, and Luc Van Gool. IMDB-WIKI – 500k+ face images with age and

gender labels. https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/.

[63] Salvatore Ruggieri. Using t-closeness anonymity to control for non-discrimination. Trans. Data Priv.,

7(2):99–129, 2014.

[64] Catherine Saunders, Gary Abel, Anas El Turabi, Faraz Ahmed, and Georgios Lyratzopoulos. Accuracy
of routinely recorded ethnic group information compared with self-reported ethnicity: Evidence from
the english cancer patient experience survey. BMJ open, 3, 06 2013.

[65] Ashudeep Singh and Thorsten Joachims. Fairness of Exposure in Rankings. In KDD, pages 2219–2228.

ACM, 2018.

[66] Julia Stoyanovich, Ke Yang, and H. V. Jagadish. Online set selection with fairness and diversity

constraints. In EDBT, pages 241–252. OpenProceedings.org, 2018.

[67] USA The Census Bureau. Frequently Occurring Surnames from the Census 2010, April 2020. https:

//www.census.gov/topics/population/genealogy/data/2010_surnames.html.

[68] Eric Luis Uhlmann and Geoﬀrey L Cohen. Constructed criteria: Redeﬁning merit to justify discrimi-

nation. Psychological Science, 16(6):474–480, 2005.

[69] Christine Wenneras and Agnes Wold. Nepotism and sexism in peer-review. Women, sience and tech-

nology: A reader in feminist science studies, pages 46–52, 2001.

[70] Ke Yang, Vasilis Gkatzelis, and Julia Stoyanovich. Balanced ranking with diversity constraints.

In

IJCAI, pages 6035–6042. ijcai.org, 2019.

[71] Ke Yang, Joshua R. Loftus, and Julia Stoyanovich. Causal intersectionality for fair ranking. CoRR,

abs/2006.08688, 2020.

[72] Ke Yang and Julia Stoyanovich. Measuring fairness in ranked outputs. In SSDBM, pages 22:1–22:6.

ACM, 2017.

[73] Meike Zehlike, Francesco Bonchi, Carlos Castillo, Sara Hajian, Mohamed Megahed, and Ricardo A.
Baeza-Yates. FA*IR: A Fair Top-k Ranking Algorithm. In CIKM, pages 1569–1578. ACM, 2017.

[74] Meike Zehlike and Carlos Castillo. Reducing disparate exposure in ranking: A learning to rank approach.

In WWW, pages 2849–2855. ACM / IW3C2, 2020.

[75] David Zuckerman. Linear degree extractors and the inapproximability of max clique and chromatic

number. In STOC 2006, pages 681–690, 2006.

31

A Extended empirical results

A.1 Implementation details

A.1.1 Optimization libraries.

We used CVXPY [2] and Gurobi [38] to implement the algorithms.

A.1.2 Rounding scheme.

The rounding scheme from our theoretical results (Step 2 of Algorithm 1) crucially uses the fact that the
intermediate solution found has a small number of fractional entries. Since the solutions of MultObj can
have a large number of fractional entries, the same rounding scheme is not useful.
Instead, one can use
randomized rounding [59], which, given a solution x ∈ [0, 1]m, outputs a subset S ⊆ [m] of size n with
probability (cid:81)

i∈S xi.

In our simulations, we use the same rounding scheme — randomized rounding [59] — for MultObj,
FairExpec, and FairExpecGrp. We also ran a version of our experiments where, we used the rounding scheme
from our theoretical results (Step 2 of Algorithm 1) for FairExpec, and FairExpecGrp, and randomized rounding
for MultObj. We observed similar results in these simulations.

A.1.3 Choice of m and n.

We set m = 500 and n = 100 in all experiments, except in Section 4.3 — where we set m = 2000 following
[71] — and in Section 4.4 — where increase m to 1000. We increase m to 1000 in Section 4.4 to ensure that
the sample of candidates has at least n/4 candidates from each of the four races. (Any algorithm requires
this to be able to satisfy the fairness constraints.)

A.1.4 Additional details of the simulation from Section 4.3

Additional details of the data. The dataset has 37% Black candidates, 37% candidates without prior
experience, and 13.7% Black candidates without prior experience. The utilities are such that the Black
candidates (zi = 0) have a lower expected utility than non-Black candidates, candidates without prior expe-
rience (ai1 = 0) have a lower expected utility than those with prior work experience, and Black candidates
without prior experience (zi = 0, ai1 = 0) have the lowest expected utility.

Preprocessing We sample a training dataset D(cid:48) with m = 2000 candidates. Then, we use the code by [71]
to get an approximation ¯θ of θ from D(cid:48). (CntrFair and CntrFairResolving use M(¯θ) to generate counterfactual
utilities). To generate the estimate of q, we partition candidates into 20 equally sized bins, (b1, . . . , b20), by
their utility. (Each bin has 100 candidates). Given a candidate i described by (wi, zi, ai1, ai2) such that
wi ∈ bk, we deﬁne qi as follows:

where the probability is over the candidates j in D(cid:48). Note that the candidate i may not be in D(cid:48).

qi0 = Prj[zj = 0 | wj ∈ bk]

and

qi1 = 1 − qi0,

(22)

A.1.5

Implementation details of simulation from Section 4.5

Speciﬁcs of cropping. For each image where the face-detector found a face, we cropped the image to
the region “around” the detected face. More precisely, we expand the bounding box returned by the face-
detector by 40% (on each side) and then crop the image to this expanded bounding box. If the expanded
bounding box exceeded the image dimensions, we ﬁlled in the empty region by copying the last pixel layer.

Face-detector. We referenced the tutorial by [61] while writing the code for our simulations.

32

Preprocessing As a preprocessing step, we remove all images with a gender label NA, this leaves us with
5,825 images. Next, we use a face-detector to extract faces from the images (5,825) and remove the ones
where the detector does not detect any face.22 This gives us 4,494 images (77% of 5,825).

We calibrate the classiﬁer by binning its values into b = 20 bins. For each bin j ∈ [20], we calculate
the classiﬁer’s accuracy a(j) ∈ [0, 1]. This initial calibration is done only once. Given image i, we compute
the classiﬁer’s output fi ∈ [0, 1]. Let fi fall in bin j. Then, we set the noise-information qi of image i to
qi := [a(j), 1−a(j)] More formally, we ran the image classiﬁer on images to get a set of predictions F := {fi}i.
Then, we binned the values in F into b = 20 equally sized bins (over [0, 1]). For all j ∈ [b], let Bj ⊆ [m] be
the set of images in the j-th bin. Further, let Gm and Gf be the set of images containing men and women,
respectively. Then, taking an uncalibrated fi as input, we output calibrated qi,(cid:96) as follows: Let fi fall in the
j-th bin, then for all (cid:96) ∈ {m, f } output

qi,(cid:96) :=

|Bj ∩ G(cid:96)|
|Bj|

.

(23)

A.2 Optimizing for Selection Lift

Given subset S ∈ [m] and target t ∈ [0, 1]p, deﬁne the selection lift FL(S, t) ∈ [0, 1] as

FL(S, t) := min
(cid:96),k∈[p]

(cid:18) |S ∩ G(cid:96)|
n · t(cid:96)

·

n · tk
|S ∩ Gk|

(cid:19)

.

(Selection lift; 24)

Note that when the target is proportional representation, then the above deﬁnition reduces to the usual
deﬁnition of selection lift [11, 39, 40]. Let A(w, q) ⊆ [m] be the subset outputted by algorithm A on input
(w, q). We report

FL,A := E [FL(A(w, q), t)] ,

where the expectation is over the choices of (w, q). We drop the subscript A when the algorithm is not
important or clear from context.

Discussion. Let selection lift of a selection x ∈ {0, 1}m, be FL(x) ∈ [0, 1]. Fix the desired level σ ∈ [0, 1]
of selection lift, and let the set of selections x ∈ {0, 1}m which have FL(x) ≥ σ selection lift be R(σ). Ideally,
we would like to ﬁnd

However, the feasible region R(cid:48)(σ), considered by FairExpec is a strict subset of R(σ). Thus, FairExpec
outputs

argmaxx∈R(σ) w(cid:62)x.

(25)

argmaxx∈R(cid:48)(σ) w(cid:62)x.

(26)

We believe this is why FairExpec is not Pareto-optimal in the simulation from Section 4.4 for FL(·). However,
a reduction from Program (25) to (multiple instances of) Program (26) should ensure that FairExpec is
Pareto-optimal for FL(·) (see, e.g., [13, Theorem 3.1]).

22Note that we do not check if the detected faces are correct. This introduces some error, which is also expected in practice.

33

A.3 Additional simulations varying n/m

)
r
i
a
f

e
r
A
o
m
A
(
A
)
A
F
A
(
A
e
c
A
n
e
A
r
A
e
ﬀ
A
i
d
A
A
k
s
A
i
R
A
A
)
r
A
i
a
A
f
s
A
s
e
A
l
(
A
A

AAAAAAAAAAAAAAAAAAAAA

Selection size (n)

Figure 7: Risk diﬀerence on varying n/m (Section 4.5): To analyze the robustness of our approach to the
fraction of items selected (n/m), we ﬁx m = 1000 and vary n from 25 to 250 in Simulation 4.5. The y-axis shows the
risk diﬀerent F of diﬀerent algorithms, and the x-axis shows n; F values are averaged over 200 trials, and the error
bars represent the std. error of the mean. We ﬁnd that on increasing n the diﬀerence between the F of diﬀerent
algorithms remains roughly the same. We refer the reader to Remark 4.6 for the details.

)
r
i
a
f

e
r
A
o
m
A
(
A
)
A
F
A
(
A
e
c
A
n
e
A
r
A
e
ﬀ
A
i
d
A
A
k
s
A
i
R
A
A
)
r
A
i
a
A
f
s
A
s
e
A
l
(
A
A

AAAAAAAAAAAAAAAAAAAAA

Selection size (n)

Figure 8: Risk diﬀerence on varying n/m (Section 4.2): To analyze the robustness of our approach to the
fraction of items selected (n/m), we ﬁx m = 1000 and vary n from 50 to 350 in Simulation 4.2. The y-axis shows the
risk diﬀerent F of diﬀerent algorithms, and the x-axis shows n; F values are averaged over 100 trials, and the error
bars represent the std. error of the mean. We ﬁnd that on increasing n, FairExpec and FairExpecGrp become slightly
fairer compared to others. We refer the reader to Remark 4.6 for the details.

34

50100150200250n0.00.20.40.60.81.0Fairness achieved ()Image search (custom_cal=0)(m,n)=(1000,250), inc-in-male-val (multiplier)=1, iter=50, target=equal, utype=DCG (100/log(r+1)), o_lists=[men_typical08,women_typical08], fmetric=r_diff.FairExpecFairExpecGrpMultObjThreshBlind50100150200250300350n0.50.60.70.80.91.0Risk difference ()Toy experiment | (m,,)=(500,1,2500.0) | iter=100 | fmetric=r_diffFairExpecFairExpecGrpMultObjThrshBlindA.4 Additional plots for Section 4.2

A
A
A
A
)
K
A
(
A
o
A
i
t
A
a
R
A
A
y
t
A
i
l
A
i
t
U
A
A
A
A

Figure 9: Additional plot for Section 4.2: This simulation considers the setting where the minority group (40% of
total) has a higher 30% higher FDR compared to the majority group. The utilities of all candidates are iid from
the uniform distribution. The target is to ensure equal representation between the majority and minority groups.
We refer the reader to Section 4.2 for the details of the simulation. The y-axis shows the utility ratio K of diﬀerent
algorithms, and the x-axis shows the constraint parameters (α for FairExpec, FairExpecGrp, and Thrsh, and λ for
MultObj); K averaged over 500 trials, and the error bars represent the standard error of the mean. We observe that
FairExpec and FairExpecGrp lose a larger fraction of the utility. Although, note that this because they have a higher
level of fairness; see Figure 1.

Remark A.1 (Reading the double x-axis). We note that both subﬁgures in Figure 9 and Figure 10 have
a double x-axis: one for α and one for λ. Therefore, one should note compared the F or K of MultObj with
other algorithms at a particular x-coordinate. It could be useful to consider the limiting their values at the
largest α and λ, respectively.

)
r
i
a
f

e
r
o
A
m
A
(
A
)
A
L
A
F
A
(
t
A
f
i
A
l
A
n
o
A
i
t
A
c
e
A
l
e
A
S
A
A
)
r
A
i
a
f

s
s
e
l
(

Figure 10: Additional plot for Section 4.2 with selection lift: This simulation considers the setting where the minority
group (40% of total) has a higher 30% higher FDR compared to the majority group. The utilities of all candidates
are iid from the uniform distribution. The target is to ensure equal representation between the majority and minority
groups. We refer the reader to Section 4.2 for the details of the simulation. The y-axis shows the selection lift FL of
diﬀerent algorithms, and the x-axis shows the constraint parameters (α for FairExpec, FairExpecGrp, and Thrsh, and
λ for MultObj); FL values are averaged over 500 trials, and the error bars represent the standard error of the mean.
We observe similar results as with risk diﬀerence (F). For a deﬁnition of selection lift see Equation (24).

35

0.00.20.40.60.81.00.900.920.940.960.981.00Utility ratio ()05001000150020002500Toy experiment | (m,n)=(500,100) | iter=500 | fmetric=r_diffFairExpecFairExpecGrpMultObjThrshBlind0.00.20.40.60.81.00.40.50.60.70.80.91.0Fairness achieved ()05001000150020002500Toy experiment | (m,n)=(500,100) | iter=500 | fmetric=selec_lftFairExpecFairExpecGrpMultObjThrshBlindA.5 Additional plots for Section 4.3

)
r
i
a
f

e
r
A
o
m
A
(
A
)
A
L
A
F
A
(
A
t
f
i
A
l
A
n
o
A
i
t
A
c
A
e
l
e
A
S
A
A
)
r
A
i
a
A
f
s
A
s
e
A
l
(
A
A

CntrFairRes
CntrFair

Figure 11: Additional results for Section 4.3 with selection lift: The y-axis shows the selection lift FL of diﬀerent
algorithms, and the x-axis shows the amount of noise added τ ∈ [0, 1/2]; FL values are averaged over 100 trials, and
the error bars represent the standard error of the mean. We refer the reader to Section 4.3 for the details of the
simulation. We observe similar results as with risk diﬀerence (F). For a deﬁnition of selection lift see Equation (24).

A
A
A
A
)
K
A
(
A
o
A
i
t
A
a
R
A
A
y
t
A
i
l
A
i
t
U
A
A
A
A

CntrFairRe

CntrFairRes
CntrFair

(less noise)

Noise (τ )

(more noise)

Figure 12: Additional results for Section 4.3: This simulation considers the setting where the utilities of a minority
group have a lower average than the majority group, and both groups have identical amount of noise. The y-axis
shows the utility ratio K of diﬀerent algorithms, and the x-axis shows the amount of noise added τ ∈ [0, 1/2]; K
values are averaged over 20 trials, and the error bars represent the standard error of the mean. We refer the reader
to Section 4.3 for details and discussion. We note that the utility ratio of all algorithms decreases on adding noise.
Finally, we note that utilities in this experiment can be negative; we observe that CntrFair has a negative utility ratio.
The caveat is that CntrFair and CntrFairResolving, try to maximize the counterfactual utilities.

Given subset S ∈ [m] and a (cid:96) ∈ [p], we deﬁne the selection rate of S with respect to group G(cid:96) as

SR(S, (cid:96)) :=

|S ∩ G(cid:96)|
n

·

m
|G(cid:96)|

(Selection rate; 27)

Let A(w, q) ⊆ [m] be the subset outputted by algorithm A on input (w, q). We report

SRA,(cid:96) := E [SR(A(w, q), (cid:96))] ,

where the expectation is over the choices of (w, q). We drop the subscript A when the algorithm is not
important or clear from context.

36

0.00.10.20.30.40.5Noise ()0.00.20.40.60.81.0Fairness achieved ()Testing causal on N dat | Testing FE on N dat with 20 eq-sz bins[[0.5,0.5],[0.5,0.5]] Causal experiment (n=100) | iter=100 | metric=selec_lftFairExpecFairExpecGrpMultObjThrshCausal-ResolvingCausalBlind0.00.10.20.30.40.5Noise ()0.30.40.50.60.70.80.91.0Utility Ratio ()Testing causal on N dat | Testing FE on N dat with 20 eq-sz bins[[0.5,0.5],[0.5,0.5]] Causal experiment (n=100) | iter=20 | metric=r_diffFairExpecFairExpecGrpMultObjThrshCausal-ResolvingCausalBlinde
t
a
r

n
o
i
t
c
e
l
e
S

A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A

e
t
a
r

n
o
i
t
c
e
l
e
S

A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A

e
t
a
r

n
o
i
t
c
e
l
e
S

A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A
A

CntrFairRes
CntrFair

CntrFairRes
CntrFair

CntrFairRes
CntrFair

Race (W: White, B: Black)

Race (W: White, B: Black)

Race (W: White, B: Black)

Figure 13: Additional plots for Section 4.3: This simulation considers the setting where the utilities of a minority
group have a lower average than the majority group, and both groups have identical amount of noise. The three
plots show three values of noise τ ∈ {0, 0.2, 0.5}. The y-axis shows the selection rate (see Equation (27)) of diﬀerent
algorithms; Selection rate values are averaged over 50 trials, and the error bars represent the standard error of the
mean. We refer the reader to Section 4.3 for details and discussion. We observe that FairExpec is the closest having
a selection rate of 1 across noise levels. We extended the code by [71] to generate this plot.

.

37

WBWBWBWBWBWBWBRace0.00.51.01.52.02.53.0Selection rateCausal exp | n=100 | iter=50 | 20 eq-sz bins | [[1,0],[0,1]].Noise: =0FairExpecFairExpecGrpMultObjThrshCausal-ResolvingCausalBlindWBWBWBWBWBWBWBRace0.00.51.01.52.02.53.0Selection rateCausal exp | n=100 | iter=50 | 20 eq-sz bins | [[0.8,0.2],[0.2,0.8]].Noise: =0.2FairExpecFairExpecGrpMultObjThrshCausal-ResolvingCausalBlindWBWBWBWBWBWBWBRace0.00.51.01.52.02.53.0Selection rateCausal exp | n=100 | iter=50 | 20 eq-sz bins | [[0.5,0.5],[0.5,0.5]].Noise: =0.5FairExpecFairExpecGrpMultObjThrshCausal-ResolvingCausalBlindA.6 Additional results for Section 4.4

A
A
A
A
)
K
A
(
A
o
A
i
t
A
a
R
A
A
y
t
A
i
l
A
i
t
U
A
A
A
A

(a) The y-axis shows the utility ratio K of diﬀerent algorithms, and the x-axis
shows the constraint parameters (α for FairExpec, FairExpecGrp, and Thrsh, and
λ for MultObj); K values are averaged over 100 trials, and the error bars represent
the standard error of the mean.

)
r
i
a
f

e
r
A
o
m
A
(
A
)
A
F
A
(
A
e
c
A
n
e
A
r
A
e
ﬀ
A
i
d
A
A
k
s
A
i
R
A
A
)
r
A
i
a
A
f
s
A
s
e
A
l
(
A
A

(b) The y-axis shows the risk diﬀerence F of diﬀerent algorithms, and the x-axis
shows the constraint parameters (α for FairExpec, FairExpecGrp, and Thrsh, and
λ for MultObj); F values are averaged over 100 trials, and the error bars represent
the standard error of the mean.

Figure 14: Additional plot for candidate selection (Section 4.4): This simulation considers race as the protected
attribute which takes p = 4 values, where an individual’s utility is drawn from diﬀerent distributions depending on
their race. The simulation uses last name as a proxy to derive noisy information of race. We refer the reader to
Section 4.4 for the details of the simulation.

Remark A.2 (Reading the double x-axis). We note that both subﬁgures in Figure 14 have a double
x-axis: one for α and one for λ. Therefore, one should note compared the F or K of MultObj with other
algorithms at a particular x-coordinate. It could be useful to consider the limiting their values at the largest
α and λ, respectively.

38

0.00.20.40.60.81.00.50.60.70.80.91.0Utility ratio ()012002400360048006000(m,n)=(1000,100), increase-in-white-value (multiplier)=1, iter=100,=[0.e+00 0.e+00 0.e+00 1.e+00 6.e+03]custom_size=0, custom_size_arg=0.0, and calibrate_with_util=0 f-metric=r_diff target=equal.FairExpecFairExpecGrpMultObjThrshBlind0.00.20.40.60.81.00.00.20.40.60.81.0Fairness achieved ()012002400360048006000(m,n)=(1000,100), increase-in-white-value (multiplier)=1, iter=100,=[0.e+00 0.e+00 0.e+00 1.e+00 6.e+03]custom_size=0, custom_size_arg=0.0, and calibrate_with_util=0 f-metric=r_diff target=equal.FairExpecFairExpecGrpMultObjThrshBlindRace
White
Black
Asian and Paciﬁc Islander (API)
Hispanic

Mean (USD per annum)
100,169
70,504
118,421
71,565

Figure 15: Statistics of Dr for diﬀerent races r; Dr is the discrete distributions of incomes of families with
race r derived from the income dataset [10].

.........................

A
A
A
A
)
K
A
(
A
o
A
i
t
A
a
r
A
y
A
t
A
i
l
i
A
t
U
A
A
A
A
A
A
A
A
A

(less fair)

Selection lift (FL)

(more fair)

Figure 16: Additional results for Section 4.4 with selection lift: The y-axis shows the selection lift FL of diﬀerent
algorithms, and the x-axis shows the risk diﬀerence of diﬀerent algorithms; both K and FL values are averaged over
200 trials, and the error bars represent the standard error of the mean. We refer the reader to Section 4.4 for the
details of the simulation. We observe that FairExpec has lower utility ratio than MultObj for some values of selection
lift; see Section 4.6 for a discussion. For a deﬁnition of selection lift see Equation (24).

A.7 Additional results for Section 4.5

Styf (0.8)
Num:
12

Stym(0.8)
Num:
29

receptionist;

Occupation name
administrative assistant; counselor; den-
tal hygienist; ﬂight attendant; hairdresser;
housekeeper; massage therapist; nurse;
nurse practitioner;
social
worker; special ed teacher
announcer; barber; bill collector; building
inspector; building painter; butcher; chief
executive oﬃcer; clergy member; construc-
tion worker; courier; crane operator; de-
tective; dishwasher; electrician; extermi-
nator; garbage collector; groundskeeper;
logistician; machinist; parking attendant;
plumber; police oﬃcer; private investiga-
tor; roofer; security guard; taxi driver;
teller; web developer; welder

Figure 17: Occupations with at least 80% of images from one gender (from the occupations dataset [15]).
We refer the reader to Section 4.5 for more details, and the deﬁnition of Stym(· and Styf ·).

Remark A.3 (Reading the double x-axis). We note that both subﬁgures in Figure 18 have a double
x-axis: one for α and one for λ. Therefore, one should note compared the F or K of MultObj with other
algorithms at a particular x-coordinate. It could be useful to consider the limiting their values at the largest
α and λ, respectively.

39

0.00.20.40.60.81.0Fairness achieved ()0.50.60.70.80.91.0Utility Ratio ()(m,n)=(1000,100), increase-in-white-value (multiplier)=1, iter=100,=[0.e+00 0.e+00 0.e+00 1.e+00 6.e+03]custom_size=0, custom_size_arg=0.0, and calibrate_with_util=0 f-metric=selec_lft target=equal.FairExpecFairExpecGrpMultObjThrshBlindA
A
A
A
)
K
A
(
A
o
A
i
t
A
a
R
A
A
y
t
A
i
l
A
i
t
U
A
A
A
A

(a) The y-axis shows the utility ratio K of diﬀerent algorithms, and the x-axis
shows the constraint parameters (α for FairExpec, FairExpecGrp, and Thrsh, and
λ for MultObj); K values are averaged over 200 trials, and the error bars represent
the standard error of the mean.

)
r
i
a
A
f
e
A
r
o
A
m
A
(
)
A
F
A
(
A
e
A
c
n
A
e
A
r
e
A
ﬀ
i
A
d
A
k
A
s
i
A
R
A
)
r
A
i
a
A
f
A
s
s
e
l
(

(b) The y-axis shows the risk diﬀerence F of diﬀerent algorithms, and the x-axis
shows the constraint parameters (α for FairExpec, FairExpecGrp, and Thrsh, and
λ for MultObj); F values are averaged over 200 trials, and the error bars represent
the standard error of the mean.

Figure 18: Additional for image selection (Section 4.5): This simulation considers gender as the protected attribute
and uses a CNN-based classiﬁer to derive noisy information about the gender of the person in the image. We refer
the reader to Section 4.5 for the details of the simulation.

.

40

0.00.20.40.60.81.00.860.880.900.920.940.960.981.00Utility ratio ()0100200300400500Image search (custom_cal=0)(m,n)=(500,100), inc-in-male-val (multiplier)=1, iter=200, target=equal, utype=DCG (100/log(r+1)), o_lists=[men_typical08,women_typical08], fmetric=r_diff.FairExpecThreshFairExpecGrpMultObjBlind0.00.20.40.60.81.00.40.50.60.70.80.91.0Fairness achieved ()0100200300400500Image search (custom_cal=0)(m,n)=(500,100), inc-in-male-val (multiplier)=1, iter=200, target=equal, utype=DCG (100/log(r+1)), o_lists=[men_typical08,women_typical08], fmetric=r_diff.FairExpecFairExpecGrpMultObjThreshBlind.........................

A
A
A
A
A
A
C
A
G
A
D
A
N
A
A
A
A
A
A
A
A
A
A
A
A

(less fair)

Selection lift (FL)

(more fair)

(a) The y-axis shows the NDCG value of diﬀerent algorithms, and the x-axis
shows selection lift (FL); both values are averaged over 100 trials.

Figure 19: Additional ﬁgure for Section 4.5: This simulation considers gender as the protected attribute and uses
a CNN-based classiﬁer to derive noisy information about the gender of the person in the image. The y-axis shows
the NDCG value of diﬀerent algorithms, and the x-axis shows selection lift (FL); both values are averaged over 100
trials. We refer the reader to Section 4.5 for details of the simulation. We observe similar results to those with utility
ratio (K) (see Figure 5).

.........................

A
A
A
A
)
K
A
(
A
o
A
i
t
A
a
r
A
y
A
t
A
i
l
i
A
t
U
A
A
A
A
A
A
A
A
A

(less fair)

Selection lift (FL)

(more fair)

Figure 20: Additional results for Section 4.5 with selection lift: The y-axis shows the utility ratio (K) of diﬀerent
algorithms, and the x-axis shows the selection lift (FL) of diﬀerent algorithms; both K and FL values are averaged
over 200 trials, and the error bars represent the standard error of the mean. We refer the reader to Section 4.5 for the
details of the simulation. We observe similar results as with risk diﬀerence (F). See Equation (24) for a deﬁnition of
selection lift (FL).

41

0.40.50.60.70.80.91.0Fairness achieved ()0.860.880.900.920.940.960.981.00Utility Ratio ()Image search (custom_cal=0)(m,n)=(500,100), inc-in-male-val (multiplier)=1, iter=200, target=equal, utype=DCG (100/log(r+1)), o_lists=[men_typical08,women_typical08], fmetric=r_diff.FairExpecFairExpecGrpMultObjThreshBlind0.40.50.60.70.80.91.0Fairness achieved ()0.860.880.900.920.940.960.981.00Utility Ratio ()Image search (custom_cal=0)(m,n)=(500,100), inc-in-male-val (multiplier)=1, iter=200, target=equal, utype=DCG (100/log(r+1)), o_lists=[men_typical08,women_typical08], fmetric=selec_lft.FairExpecFairExpecGrpMultObjThreshBlind