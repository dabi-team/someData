Dynamic Programming Encoding for
Subword Segmentation in Neural Machine Translation

Xuanli He
Monash University

Gholamreza Haffari
Monash University

Mohammad Norouzi
Google Research

{xuanli.he1,gholamreza.haffari}@monash.edu

mnorouzi@google.com

0
2
0
2

g
u
A
1

]
L
C
.
s
c
[

2
v
6
0
6
6
0
.
5
0
0
2
:
v
i
X
r
a

Abstract

This paper introduces Dynamic Programming
Encoding (DPE), a new segmentation algo-
rithm for tokenizing sentences into subword
units. We view the subword segmentation of
an output sentence as a latent variable that
should be marginalized for learning and in-
ference. A mixed character-subword trans-
former is proposed, which enables exact log
marginal likelihood estimation and exact MAP
inference to ﬁnd output segmentations with
maximum posterior probability. DPE uses
such a mixed character-subword transformer
as a means of pre-processing parallel data to
segment output sentences using dynamic pro-
gramming. Empirical results on machine trans-
lation suggest that DPE is effective for seg-
menting output sentences and can be com-
bined with BPE dropout for stochastic seg-
mentation of source sentences. DPE achieves
an average improvement of 0.9 BLEU over
BPE (Sennrich et al., 2016) and an aver-
age improvement of 0.55 BLEU over BPE
dropout (Provilkov et al., 2019) on several
WMT datasets including English ↔ (German,
Romanian, Estonian, Finnish, Hungarian).

1

Introduction

The segmentation of rare words into subword
units (Sennrich et al., 2016; Wu et al., 2016) has be-
come a critical component of neural machine trans-
lation (Vaswani et al., 2017) and natural language
understanding (Devlin et al., 2019). Subword units
enable open vocabulary text processing with a neg-
ligible pre-processing cost and help maintain a de-
sirable balance between the vocabulary size and
decoding speed. Since subword vocabularies are
built in an unsupervised manner (Sennrich et al.,
2016; Wu et al., 2016), they can be easily adopted
for any language.

Given a ﬁxed vocabulary of subword units,
rare words can be segmented into a sequence of

subword units in different ways. For instance,
“un+conscious” and “uncon+scious” are both suit-
able segmentations for the word “unconscious”.
This paper studies the impact of subword segmen-
tation on neural machine translation, given a ﬁxed
subword vocabulary, and presents a new algorithm
called Dynamic Programming Encoding (DPE).

We identify three families of subword segmenta-

tion algorithms in neural machine translation:
1. Greedy algorithms: Wu et al. (2016) segment
words by recursively selecting the longest sub-
word preﬁx. Sennrich et al. (2016) recursively
combine adjacent word fragments that co-occur
most frequently, starting from characters.
2. Stochastic algorithms (Kudo, 2018; Provilkov
et al., 2019) draw multiple segmentations for
source and target sequences resorting to ran-
domization to improve robustness and general-
ization of translation models.

3. Dynamic programming algorithms, studied
here, enable exact marginalization of subword
segmentations for certain sequence models.
We view the subword segmentation of an output
sentence in machine translation as a latent variable
that should be marginalized to obtain the proba-
bility of the output sentence given the input. On
the other hand, the segmentation of source sen-
tences can be thought of as input features and can
be randomized as a form of data augmentation
to improve translation robustness and generaliza-
tion. Unlike previous work, we recommend using
two distinct segmentation algorithms for tokeniz-
ing source and target sentences: stochastic segmen-
tation for source and dynamic programming for
target sentences.

We present a new family of mixed character-
subword transformers, for which simple dynamic
programming algorithms exist for exact marginal-
ization and MAP inference of subword segmenta-

 
 
 
 
 
 
tions. The time complexity of the dynamic pro-
gramming algorithms is O(T V ), where T is the
length of the target sentence in characters, and V
is the size of the subword vocabulary. By com-
parison, even computing the conditional probabil-
ities of subword units in an autoregressive model
requires O(T V ) to estimate the normalizing con-
stant of the categorical distributions. Thus, our
dynamic programming algorithm does not incur
additional asymptotic costs. We use a lightweight
mixed character-subword transformer as a means
of pre-processing translation datasets to segment
output sentences using DPE for MAP inference.

The performance of a standard subword trans-
former (Vaswani et al., 2017) trained on WMT
datasets tokenized using DPE is compared against
Byte Pair Encoding (BPE) (Sennrich et al., 2016)
and BPE dropout (Provilkov et al., 2019). Em-
pirical results on English ↔ (German, Romanian,
Estonian, Finnish, Hungarian) suggest that stochas-
tic subword segmentation is effective for tokeniz-
ing source sentences, whereas deterministic DPE
is superior for segmenting target sentences. DPE
achieves an average improvement of 0.9 BLEU
over greedy BPE (Sennrich et al., 2016) and an av-
erage improvement of 0.55 BLEU over stochastic
BPE dropout (Provilkov et al., 2019)1.

2 Related Work

Neural networks have revolutionized machine
translation (Sutskever et al., 2014; Bahdanau et al.,
2015; Cho et al., 2014). Early neural machine trans-
lation (NMT) systems used words as the atomic
element of sentences. They used vocabularies with
tens of thousands words, resulting in prohibitive
training and inference complexity. While learning
can be sped up using sampling techniques (Jean
et al., 2015), word based NMT models have a dif-
ﬁcult time handling rare words, especially in mor-
phologically rich languages such as Romanian, Es-
tonian, and Finnish. The size of the word vocab-
ulary should increase dramatically to capture the
compositionality of morphemes in such languages.
More recently, many NMT models have been
developed based on characters and a combination
of characters and words (Ling et al., 2015; Luong
and Manning, 2016; Vylomova et al., 2017; Lee
et al., 2017; Cherry et al., 2018). Fully character
based models (Lee et al., 2017; Cherry et al., 2018)
demonstrate a signiﬁcant improvement over word

1code and corpora: https://github.com/xlhex/dpe

based models on morphologically rich languages.
Nevertheless, owing to the lack of morphological
information, deeper models are often required to
obtain a good translation quality. Moreover, elon-
gated sequences brought by a character representa-
tion drastically increases the inference latency.

In order to maintain a good balance between the
vocabulary size and decoding speed, subword units
are introduced in NMT (Sennrich et al., 2016; Wu
et al., 2016). These segmentation approaches are
data-driven and unsupervised. Therefore, with a
negligible pre-processing overhead, subword mod-
els can be applied to any NLP task (Vaswani et al.,
2017; Devlin et al., 2019). Meanwhile, since sub-
word vocabularies are generated based on word
frequencies, only the rare words are split into sub-
word units and common words remain intact.

Previous work (Chan et al., 2016; Kudo, 2018)
has explored the idea of using stochastic subword
segmentation with multiple subword candidates to
approximate the log marginal likelihood. Kudo
(2018) observed marginal gains in translation qual-
ity at the cost of introducing additional hyper-
parameters and complex sampling procedures. We
utilize BPE dropout (Provilkov et al., 2019), a sim-
ple stochastic segmentation algorithm for tokeniz-
ing source sentences.

Dynamic programming has been used to
marginalize out latent segmentations for speech
recognition (Wang et al., 2017), showing a consis-
tent improvement over greedy segmentation meth-
ods. In addition, dynamic programming has been
successfully applied to learning sequence models
by optimizing edit distance (Sabour et al., 2018)
and aligning source and target sequences (Chan
et al., 2020; Saharia et al., 2020). We show the
effectiveness of dynamic programming for seg-
menting output sentences in NMT using a mixed
character-transformer in a pre-processing step.

Prior to our work, multiple NLP tasks, such
as word segmentation (Kawakami et al., 2019),
NMT (Kreutzer and Sokolov, 2018) and language
modeling (Grave et al., 2019), have been beneﬁt-
ing from segmentation via neural language models.
However, to the best of our knowledge, this work
is the ﬁrst call on the conditional target subword
segmentation.

3 Latent Subword Segmentation

Let x denote a source sentence and y =
(y1, . . . , yT ) denote a target sentence comprising

T characters. The goal of machine translation is
to learn a conditional distribution p(y | x) from a
large corpus of source-target sentences. State-of-
the-art neural machine translation systems make
use of a dictionary of subword units to tokenize
the target sentences in a more succinct way as a se-
quence of M ≤ T subword units. Given a subword
vocabulary, there are multiple ways to segment a
rare word into a sequence of subwords (see Fig-
ure 1). The common practice in neural machine
translation considers subword segmentation as a
pre-process and uses greedy algorithms to segment
each word across a translation corpus in a consis-
tent way. This paper aims to ﬁnd optimal subword
segmentations for the task of machine translation.
Let z = (z1, .., zM +1) denote a sequence of
character indices 0 = z1 < z2 < . . . < zM <
zM +1 = T in an ascending order, deﬁning the
boundary of M subword segments {yzi,zi+1}M
i=1.
Let ya,b ≡ [ya+1, . . . , yb] denote a subword that
spans the segment between (a + 1)th and bth char-
acters, including the boundary characters. For ex-
ample, given a subword dictionary {‘c’, ‘a’, ‘t’,
‘at’, ‘ca’}, the word ‘cat’ may be segmented using
z = (0, 1, 3) as (‘c’, ‘at’), or using z = (0, 2, 3) as
(‘ca’, ‘t’), or using z = (0, 1, 2, 3) as (‘c’, ‘a’, ‘t’).
The segmentation z = (0, 3) is not valid since ‘cat’
does not appear in the subword vocabulary.

Autoregressive language models create a cate-
gorical distribution over the subword vocabulary
at every subword position and represent the log-
probability of a subword sequence using chain rule,

log p(y, z) =

(cid:88)|z|
i=1

log p(yzi,zi+1 | yz1,z2, . . . , yzi−1,zi) .

(1)
Note that we suppress the dependence of p on x
to reduce notational clutter. Most neural machine
translation approaches assume that z is a deter-
ministic function of y and implicitly assume that
log p(y, z) ≈ log p(y).

We consider a subword segmentation z as a la-
tent variable and let each value of z ∈ Zy, in the
set of segmentations compatible with y, contribute
its share to p(y) according to p(y) = (cid:80)
z p(y, z),

log p(y) =

(cid:88)

log

exp

z∈Zy

|z|
(cid:88)

i=1

log p(yzi,zi+1 | . . . , yzi−1,zi) .

(2)

Figure 1: An illustration of marginalizing subword segmen-
tations of the word ‘unconscious’

Note that each particular subword segmentation
z ∈ Zy provides a lower bound on the log marginal
likelihood log p(y) ≥ log p(y, z). Hence, optimiz-
ing (1) for a greedily selected segmentation can
be justiﬁed as a lower bound on (2). That said,
optimizing (2) directly is more desirable. Unfor-
tunately, exact marginalization over all segmen-
tations is computationally prohibitive in a combi-
natorially large space Zy, especially because the
probability of each subword depends on the seg-
mentation of its conditioning context. In the next
section, we discuss a sequence model in which
the segmentation of the conditioning context does
not inﬂuence the probability of the next subword.
We describe an efﬁcient Dynamic Programming
algorithm to exactly marginalize out all possible
subword segmentations in this model.

4 A Mixed Character-Subword

Transformer

We propose a mixed character-subword transformer
architecture, which enables one to marginalize
out latent subword segmentations exactly using
dynamic programming (see Figure 2). Our key
insight is to let the transformer architecture pro-
cess the inputs and the conditioning context based
on characters to remain oblivious to the speciﬁc
choice of subword segmentation in the condition-
ing context and enable exact marginalization. That
said, the output of the transformer is based on
subword units and at every position it creates a
categorical distribution over the subword vocabu-
lary. More precisely, when generating a subword
yzi,zi+1, the model processes the conditioning con-
text (yz1, . . . , yzi) based solely on characters using,

(cid:88)|z|
i=1

log p(y, z) =

log p(yzi,zi+1 | yz1, ..., yzi) ,
(3)
where the dependence of p on x is suppressed to
reduce notational clutter.

Algorithm 1 Dynamic Programming (DP) for Exact Marginalization
Input: y is a sequence of T characters, V is a subword vocabulary, m is the maximum subword length
Output: log p(y) marginalizing out different subword segmentations.

1: α0 ← 0
2: for k = 1 to T do
αk ← log (cid:80)k−1
3:
4: end for
5: return αT

j=k−m

1[yj,k ∈ V ] exp

(cid:16)

αj + log Pθ(yj,k|y1, .., yj)

(cid:17)

(cid:46) the marginal probability log p(y) = log (cid:80)

z∈Zy

p(y, z)

4.1 Optimization

The training objective for our latent segmentation
translation model is (cid:80)
(x,y)∈D log Pθ(y|x) where
D is the training corpus consisting of parallel bilin-
gual sentence pairs. Maximizing the training objec-
tive requires marginalization and the computation
of the gradient of the log marginal likelihood.

Exact Marginalization. Under our model, the
probability of a subword only depends on the
character-based encoding of the conditioning con-
text and not its segmentation, as in (3). This means
that we can compute the log marginal likelihood for
a single example y, exactly, using the Dynamic Pro-
gramming algorithm shown in Algorithm 1. The
core of the algorithm is line 3, where the probabil-
ity of the preﬁx string y0,k is computed by sum-
ming terms corresponding to different segmenta-
tions. Each term consists of the product of the
probability of a subword yj,k times the probability
of its conditioning context (y1, . . . , yj). The run-
ning time of the algorithm is O(mT ), where T is
the length of the string, and m is the size of the
longest subword unit in the vocabulary.

Gradient Computation. We use automatic dif-
ferentiation in PyTorch to backpropagate through
the dynamic program in Algorithm 1 and compute
its gradient. Compared to a standard Transformer
decoder, our mixed character-subword Transformer
is 8x slower with a larger memory footprint, due
to computation involved in the DP algorithm and
large sequence length in characters. To address
these issues, we reduce the number of transformer
layers from 6 to 4, and accumulate 16 consecutive
gradients before one update.

4.2 Segmenting Target Sentences

Once the mixed character-subword transformer
is trained, it is used to segment the target side
of a bilingual corpus. We randomize the sub-
word segmentation of source sentences using BPE

Figure 2: An illustration of the mixed character-
subword Transformer. The input is a list of characters,
whereas the output is a sequence of subwords.

Given a ﬁxed subword vocabulary denoted V ,
at every character position t within y, the mixed
character-subword model induces a distribution
over the next subword w ∈ V based on,

p(w | y1, .., yt) =

exp(f (y1, .., yt)(cid:62)e(w))
w(cid:48)∈V exp(f (y1, .., yt)(cid:62)e(w(cid:48)))

(cid:80)

where f (·) processes the conditioning context us-
ing a Transformer, and e(·) represents the weights
of the softmax layer.

As depicted in in Figure 2, the mixed character-
subword Transformer consumes characters as input
generates subwords as output. This ﬁgure only
shows the decoder architecture, since as the en-
coder that processes x is a standard subword Trans-
former. Once a subword w is emitted at time step
t, the characters of the subword w are fed into the
decoder for time steps t + 1 to t + |w|, and the next
subword is generated at time step t + |w|, condi-
tioned on all of the previously generated characters.

Algorithm 2 Dynamic Programming Encoding (DPE) for Subword Segmentation
Input: y is a sequence of T characters, V is a subword vocabulary, m is the maximum subword length
Output: Segmentation z with highest posterior probability.

for k = 1 to T do

βk ← max{j∈[k−m,k−1] | yj,k∈V } βj + log Pθ(yj,k|y1, .., yj)
bk ← argmax{j∈[k−m,k−1] | yj,k∈V }βj + log Pθ(yj,k|y1, .., yj)

end for
z ← backtrace(b1, .., bT )

(cid:46) backtrace the best segmentation using b

Number of sentences Vocab
size
train

dev

test

En-Hu WMT09
En-De WMT14
En-Fi WMT15
En-Ro WMT16
En-Et WMT18

0.6M 2,051
4.2M 3000
1.7M 1,500
0.6M 1,999
1.9M 2,000

2,525
3003
1,370
1,999
2,000

32K
32K
32K
32K
32K

Table 1: Statistics of the corpora.

dropout (Provilkov et al., 2019). Conditional on
the source sentence, we use Algorithm 2, called
Dynamic Programming Encoding (DPE) to ﬁnd
a segmentation of the target sentence with high-
est posterior probability. This algorithm is similar
to the marginalization algorithm, where we use a
max operation instead of log-sum-exp. The mixed
character-subword transformer is used only for to-
kenization, and a standard subword transformer is
trained on the segmented sentences. For inference
using beam search, the mixed character-subword
transformer is not needed.

5 Experiments

Dataset We use WMT09 for En-Hu, WMT14 for
En-De, WMT15 for En-Fi, WMT16 for En-Ro and
WMT18 for En-Et. We utilize Moses toolkit2 to
pre-process all corpora, and preserve the true case
of the text. Unlike Lee et al. (2018), we retain
diacritics for En-Ro to retain the morphological
richness. We use all of the sentence pairs where
the length of either side is less than 80 tokens for.
training. Byte pair encoding (BPE) (Sennrich et al.,
2016) is applied to all language pairs to construct
a subword vocabulary and provide a baseline seg-
mentation algorithm. The statistics of all corpora
is summarized in Table 1.

2https://github.com/moses-smt/mosesdecoder

Training with BPE Dropout. We apply BPE
dropout (Provilkov et al., 2019) to each mini-batch.
For each complete word, during the BPE merge
operation, we randomly drop a particular merge
with a probability of 0.05. This value worked the
best in our experiments. A word can be split into
different segmentations at the training stage, which
helps improve the BPE baseline.

DPE Segmentation. DPE can be used for tar-
get sentences, but its use for source sentences is
not justiﬁed as source segmentations should not
be marginalized out. Accordingly, we use BPE
dropout for segmenting source sentences. That is,
we train a mixed character-subword transformer
to marginalize out the latent segmentations of a
target sentence, given a randomized segmentation
of the source sentence by BPE dropout. After the
mixed character-subword transformer is trained, it
is used to segment the target sentences as describe
in section 4.2 for tokenization.

As summarized in Figure 3, we ﬁrst train a mixed
character-subword transformer with dynamic pro-
gramming. Then, this model is frozen and used for
DPE segmentation of target sentences. Finally, a
standard subword transformer is trained on source
sentences segmented by BPE dropout and target
sentences segmented by DPE. The mixed character-
subword transformer is not needed for translation
inference.

Transformer Architectures. We use trans-
former models to train three translation models
on BPE, BPE dropout, and DPE corpora. We make
use of transformer base for all of the experiments.

5.1 Main Results

Table 2 shows the main results. First, we see that
BPE dropout consistently outperforms BPE across
language pairs. In Table 2, the column labeled to
∆1 shows the improvement of BPE dropout over
BPE. This gain can be attributed to the robustness

Method

Source segmentation
Target segmentation

En→De
En→Ro
En→Et
En→Fi
En→Hu

De→En
Ro→En
Et→En
Fi→En
Hu→En

Average

BPE

BPE
BPE

27.11
27.90
17.64
15.88
12.80

30.82
31.67
23.13
19.10
16.14

22.22

BPE dropout

BPE dropout
BPE dropout

27.27
28.07
18.20
16.18
12.94

30.85
32.56
23.65
19.34
16.61

22.57

∆1

+0.16
+0.17
+0.56
+0.30
+0.14

+0.03
+0.89
+0.52
+0.24
+0.47

+0.35

This paper

BPE dropout
DPE

27.61
28.66
18.80
16.89
13.36

31.21
32.99
24.62
19.87
17.05

23.12

∆2

+0.34
+0.59
+0.60
+0.71
+0.42

+0.36
+0.43
+0.97
+0.53
+0.44

+0.55

Table 2: Average test BLEU scores (averaged over 3 independent runs) for 3 segmentation algorithms, namely
BPE (Sennrich et al., 2016), BPE dropout (Provilkov et al., 2019), and our DPE algorithm on 10 different WMT
datasets. ∆1 shows the improvement of BPE dropout compared to BPE, and ∆2 shows further improvement DPE
compared to BPE dropout. All of the segmentation algorithms use the same subword dictionary with 32K tokens
shared between source and target languages.

BPE source:
Die G+le+is+anlage war so ausgestattet , dass dort elektr+isch betrie+bene Wagen eingesetzt
werden konnten .
DPE target:
The railway system was equipped in such a way that electrical+ly powered cart+s could be used on it .
BPE target:
The railway system was equipped in such a way that elect+r+ically powered car+ts could be used on it .

BPE source:
Normalerweise wird Kok+ain in kleineren Mengen und nicht durch Tunnel geschm+ug+gelt .
DPE target:
Normal+ly c+oca+ine is sm+ugg+led in smaller quantities and not through tunnel+s .
BPE target:
Norm+ally co+c+aine is sm+ugg+led in smaller quantities and not through tun+nels .

Table 3: Two examples of segmentation of English sentences given German inputs.

of the NMT model to the segmentation error on
the source side, as our analysis in Section 5.3 will
conﬁrm. Second, we observe further gains resulted
from DPE compared to BPE dropout. The col-
umn labeled ∆2 shows the improvement of DPE
over BPE dropout. DPE provides an average im-
provement of 0.55 BLEU over BPE dropout and
BPE dropout provides an average improvement of
0.35 BLEU over BPE. As our proposal uses BPE
dropout for segmenting the source, we attribute
our BLEU score improvements to a better segmen-
tation of the target language with DPE. Finally,
compared to BPE for segmenting the source and
target, our proposed segmentation method results
in large improvements in the translation quality, up
to 1.49 BLEU score improvements in Et→En.

5.2 Segmentation Examples

Table 3 shows examples of target sentences seg-
mented using DPE and BPE and the corresponding
source sentences. In addition, Table 4 presents the
top 50 most common English words that result in
a disagreement between BPE and DPE segmenta-
tions based on the Et→En corpus. For DPE, for
each word, we consider all segmentations produced
and show the segmentation that attains the high-
est frequency of usage in Table 4. As can be ob-
served, DPE produces more linguistically plausible
morpheme-based subwords compared to BPE. For
instance, BPE segments “carts” into “car”+“ts”,
as both “car” and “ts” are common subwords and
listed in the BPE merge table. By contrast DPE

BPE

DPE (ours)

recognises
advocates
eurozone
underlines
strengthens
entrepreneurship
acknowledges
11.30
wines
pres + ently
f + illed
endors + ement
blo + c
cru + cially
eval + uations
tre + es
tick + ets
predic + table
multilater + alism
rat + ings
predic + ted
mo + tives
reinfor + ces
pro + tocols
pro + gressively
sk + ill
preva + ils
decent + ralisation
sto + red
inﬂu + enz + a
margin + alised
12.00
sta + ying
intens + ity
rec + ast
guid + eline
emb + arked
out + lines
scen + ari + os
n + ative
preven + tative
hom + eland
bat + hing
endang + ered
cont + inen + tal
t + enth
vul + n + era + bility
realis + ing
t + ighter

recognise + s
advocate + s
euro + zone
underline + s
strengthen + s
entrepreneur + ship
acknowledge + s
11 + .30
wine + s
present + ly
ﬁll + ed
endorse + ment
bl + oc
crucial + ly
evaluation + s
tr + ees
tick + et + s
predict + able
multilateral + ism
rating + s
predict + ed
motiv + es
reinforce + s
protocol + s
progressive + ly
ski + ll
prevail + s
decent + ral + isation
stor + ed
inﬂuen + za
marginal + ised
12 + .00
stay + ing
intensi + ty
re + cast
guide + line
embark + ed
outline + s
scenario + s
na + tive
prevent + ative
home + land
bath + ing
endanger + ed
continent + al
ten + th
vul + ner + ability
real + ising
tight + er

Table 4: Word fragments obtained by BPE vs. DPE.
The most frequent words that resulted in a disagree-
ment between BPE and DPE segmentations on Et →
En are shown.

is trained.

Table 5 shows the results. It compares the un-
conditional language model (LM) DPE vs the con-
ditional DPE for segmenting the target language,
where we use BPE dropout for segmenting the
source language. We observe that without the infor-
mation from the source, LM DPE is on-par to BPE,
and is signiﬁcantly outperformed by conditional

Figure 3: The workﬂow of the proposed DPE approach.

segments “carts” into “cart”+“s”. We attribute
the linguistic characteristics of the DPE segments
to the fact that DPE conditions the segmentation
of a target word on the source sentence and the
previous tokens of the target sentence, as opposed
to BPE, which mainly makes use of frequency of
subwords, without any context.

DPE generally identiﬁes and leverages some lin-
guistic properties, e.g., plural, antonym, normal-
ization, verb tenses, etc. However, BPE tends to
deliver less linguistically plausible segmentations,
possibly due to its greedy nature and the lack of
context. We believe this phenomenon needs fur-
ther investigation, i.e., the contribution of source
vs.
target context in DPE segmentations, and a
quantitative evaluation of linguistic nature of word
fragments produced by DPE. We will leave this to
future work.

5.3 Analysis

Conditional Subword Segmentation. One of
our hypothesis for the effectiveness of subword
segmentation with DPE is that it conditions the
segmentation of the target on the source language.
To verify this hypothesis, we train mixed character-
subword Transformer solely on the target language
sentences in the bilingual training corpus using the
language model training objective. This is in con-
trast to the mixed character-subword model used in
the DPE segmentation of the main results in Table
2, where the model is conditioned on the source
language and trained on the sentence pairs using
a conditional language model training objective.
Once the mixed character-subword Transformer
language model is trained, it is then used to seg-
ment the target sentence of the bilingual corpus in
the pre-processing step before a translation model

Source
Target

BPE drop BPE drop BPE drop
LM DPE
BPE drop

DPE

Source
Target

BPE drop
DPE Fixed DPE On The Fly

BPE drop

En→Ro
En→Hu

Ro→En
Hu→En

28.07
12.94

32.56
16.61

28.07
12.87

32.57
16.41

28.66
13.36

32.99
17.05

Table 5: DPE-LM learns a segmentation of the target
based on language modelling, which is not conditioned
on the source language.

Figure 4: Disagreement of DPE segments between Et-
En and Ro-En over English vocabulary

DPE. This observation conﬁrms our hypothesis that
segmentation in NMT should be source-dependent.
We are further interested in analyzing the dif-
ferences of the target language segmentation de-
pending on the source language. For this analysis,
we ﬁltered out a multilingual parallel corpus from
WMT, which contains parallel sentences in three
languages English, Estonian and Romanian. That
is, for each English sentence we have the corre-
sponding sentences in Et and Ro. We then trained
two DPE segmentation models for the translation
tasks of Et→En and Ro→En, where English is the
target language. Figure 4 shows when conditioning
the segmentation of the target on different source
languages, DPE can lead to different segmentations
even for an identical multi-parallel corpus. The dif-
ferences are more signiﬁcant for low frequency
words.

Another aspect of DPE segmentation method is
its dependency on the segmentation of the source.
As mentioned, we segment the target sentence on
the ﬂy using our mixed character-subword model
given a randomized segmentation of the source pro-
duced by BPE dropout. That means during the
training of the NMT model where we use BPE
dropout for the source sentence, the corresponding
target sentence may get a different DPE segmen-
tation given the randomized segmentation of the

En→Ro
En→Hu
En→Et

Ro→En
Hu→En
Et→En

28.58
13.14
18.51

32.73
16.82
24.37

28.66
13.36
18.80

32.99
17.05
24.62

Table 6: “DPE Fixed” obtains a ﬁxed segmentation of
the target sentence given the BPE-segmented source
sentence, whereas “DPE On The Fly” obtain the best
segmentation of the target sentence given a randomized
segmentation of the source produced by BPE dropout.

source sentence. We are interested in the effec-
tiveness of the target segmentation if we commit
to a ﬁxed DPE segmentation conditioned on the
BPE segmentation of the input. Table 6 shows the
results. We observe that there is a marginal drop
when using the ﬁxed DPE, which indicates that
the encoder can beneﬁt from a stochastic segmen-
tation, while the decoder prefers a deterministic
segmentation corresponding to the segmentation of
the source.

Source
Target

En→Ro
En→Et

Ro→En
Et→En

BPE drop BPE drop BPE drop
BPE drop

DPE

BPE

28.04
18.09

32.40
23.52

28.07
18.20

32.56
23.65

28.66
18.80

32.99
24.62

Table 7: BLEU score of different target segmentation
methods.

DPE vs BPE. We are interested to compare
the effectiveness of DPE versus BPE for the tar-
get, given BPE dropout as the same segmentation
method for the source. Table 7 shows the results.
As observed, target segmentation with DPE consis-
tently outperforms BPE, leading to up to .9 BLEU
score improvements. We further note that using
BPE dropout on the target has a similar perfor-
mance to BPE, and it is consistently outperformed
by DPE.

We further analyze the segmentations produced
by DPE vs BPE. Figure 5 shows the percentage of
the target words which have different segmentation
with BPE and DPE, for different word frequency

bands in En→Et translation task. We observe that
for Estonian words whose occurrence is up to 5 in
the training set, the disagreement rate between DPE
and BPE is 64%. The disagreement rate decreases
as we go to words in higher frequency bands. This
may imply that the main difference between the
relatively large BLEU score difference between
BPE and DPE is due to their different segmentation
mainly for low-frequency words.

tion methods in NMT, i.e., BPE and its stochastic
variant BPE dropout.

Acknowledgment

We would like to thank the anonymous review-
ers, Taku Kudo, Colin Cherry and George Fos-
ter for their comments and suggestions on this
work. The computational resources of this work are
supported by the Google Cloud Platform (GCP),
and by the Multi-modal Australian ScienceS Imag-
ing and Visualisation Environment (MASSIVE)
(www.massive.org.au). This material is partly
based on research sponsored by Air Force Research
Laboratory and DARPA under agreement number
FA8750-19-2-0501. The U.S. Government is au-
thorized to reproduce and distribute reprints for
Governmental purposes notwithstanding any copy-
right notation thereon.

Figure 5: Disagreement of segments between BPE and
DPE over Estonian vocabulary.

References

We further plot the distribution of BLEU scores
by the length of target sentences. As shown in Fig-
ure 6, DPE demonstrates much better gains on the
longer sentences, compared with the BPE version.

Figure 6: BLEU scores of BPE vs DPE by the lengths
of sentences for En→Et.

6 Conclusion

This paper introduces Dynamic Programming En-
coding in order to incorporate the information of
the source language into subword segmentation of
the target language. Our approach utilizes dynamic
programming for marginalizing the latent segmen-
tations when training, and inferring the highest
probability segmentation when tokenizing. Our
comprehensive experiments show impressive im-
provements compared to state-of-the-art segmenta-

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. ICLR.

William Chan, Chitwan Saharia, Geoffrey Hinton, Mo-
hammad Norouzi, and Navdeep Jaitly. 2020.
Im-
puter: Sequence modelling via imputation and dy-
namic programming. arXiv:2002.08926.

William Chan, Yu Zhang, Quoc Le, and Navdeep Jaitly.
arXiv
Latent sequence decompositions.

2016.
preprint arXiv:1610.03035.

Colin Cherry, George Foster, Ankur Bapna, Orhan
Firat, and Wolfgang Macherey. 2018. Revisiting
character-based neural machine translation with ca-
pacity and compression. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 4295–4305.

Kyunghyun Cho, Bart van Merri¨enboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014.
Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1724–
1734.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. Bert: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), pages
4171–4186.

Sara Sabour, William Chan, and Mohammad Norouzi.
2018. Optimal completion distillation for sequence
learning. arXiv preprint arXiv:1810.01398.

Chitwan Saharia, William Chan, Saurabh Sax-
ena,
Non-
and Mohammad Norouzi. 2020.
autoregressive machine translation with latent align-
ments. arXiv:2004.07437.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), volume 1, pages
1715–1725.

I Sutskever, O Vinyals, and QV Le. 2014. Sequence to
sequence learning with neural networks. NIPS.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. NIPS.

Ekaterina Vylomova, Trevor Cohn, Xuanli He, and
Gholamreza Haffari. 2017. Word representation
models for morphologically rich languages in neu-
ral machine translation. Proceedings of the First
Workshop on Subword and Character Level Models
in NLP.

Chong Wang, Yining Wang, Po-Sen Huang, Abdel-
rahman Mohamed, Dengyong Zhou, and Li Deng.
2017. Sequence modeling via segmentations.
In
Proceedings of the 34th International Conference
on Machine Learning-Volume 70, pages 3674–3683.
JMLR. org.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, et al. 2016. Google’s neural machine
translation system: Bridging the gap between human
and machine translation. arXiv:1609.08144.

Edouard Grave, Sainbayar Sukhbaatar, Piotr Bo-
janowski, and Armand Joulin. 2019. Training hy-
brid language models by marginalizing over segmen-
In Proceedings of the 57th Annual Meet-
tations.
ing of the Association for Computational Linguis-
tics, pages 1477–1482, Florence, Italy. Association
for Computational Linguistics.

S´ebastien Jean, Kyunghyun Cho, Roland Memisevic,
and Yoshua Bengio. 2015. On using very large tar-
In
get vocabulary for neural machine translation.
Proceedings of the 53rd Annual Meeting of the Asso-
ciation for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing (Volume 1: Long Papers), pages 1–10.

Kazuya Kawakami, Chris Dyer, and Phil Blunsom.
2019. Learning to discover, ground and use words
with segmental neural language models. In Proceed-
ings of the 57th Annual Meeting of the Association
for Computational Linguistics, pages 6429–6441,
Florence, Italy. Association for Computational Lin-
guistics.

Julia Kreutzer and Artem Sokolov. 2018. Learning to
segment inputs for nmt favors character-level pro-
cessing.

Taku Kudo. 2018. Subword regularization: Improving
neural network translation models with multiple sub-
word candidates. In Proceedings of the 56th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 66–75.

Jason Lee, Kyunghyun Cho, and Thomas Hofmann.
2017. Fully character-level neural machine trans-
lation without explicit segmentation. Transactions
of the Association for Computational Linguistics,
5:365–378.

Jason Lee, Elman Mansimov, and Kyunghyun Cho.
2018. Deterministic non-autoregressive neural se-
In Pro-
quence modeling by iterative reﬁnement.
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1173–
1182.

Wang Ling, Chris Dyer, Alan W Black, Isabel Tran-
coso, Ram´on Fermandez, Silvio Amir, Luis Marujo,
and Tiago Lu´ıs. 2015. Finding function in form:
Compositional character models for open vocabu-
lary word representation. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1520–1530.

Minh-Thang Luong and Christopher D Manning. 2016.
Achieving open vocabulary neural machine transla-
tion with hybrid word-character models. In Proceed-
ings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 1054–1063.

Ivan Provilkov, Dmitrii Emelianenko, and Elena Voita.
2019. Bpe-dropout: Simple and effective subword
regularization. arXiv:1910.13267.

