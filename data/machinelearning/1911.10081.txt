0
2
0
2

r
a

M
3
2

]

G
L
.
s
c
[

2
v
1
8
0
0
1
.
1
1
9
1
:
v
i
X
r
a

ptype: Probabilistic Type Inference

Taha Ceritli
School of Informatics
University of Edinburgh, UK
Alan Turing Institute, London, UK
t.y.ceritli@sms.ed.ac.uk

Christopher K. I. Williams
School of Informatics
University of Edinburgh, UK
Alan Turing Institute, London, UK
ckiw@inf.ed.ac.uk

James Geddes
Alan Turing Institute, London, UK
jgeddes@turing.ac.uk

March 24, 2020

Abstract

Type inference refers to the task of inferring the data type of a given column of data. Current
approaches often fail when data contains missing data and anomalies, which are found commonly in
real-world data sets. In this paper, we propose ptype, a probabilistic robust type inference method
that allows us to detect such entries, and infer data types. We further show that the proposed
method outperforms the existing methods.

Keywords — Type inference, Robustness, Probabilistic Finite-State Machine

1 Introduction

Data analytics can be deﬁned as the process of generating useful insights from raw data sets. Central
to the entire process is the concept of data wrangling, which refers to the task of understanding,
interpreting, and preparing a raw data set, and turning it into a usable format. This often leads to
a frustrating and time-consuming process for large data sets, and even possibly for some small sized
ones (Dasu and Johnson, 2003). In order to design corresponding transformations easily, one often
desires to know characteristics of given data, e.g., data types, missing data, anomalies, etc. However,
raw data often do not contain any well-documented prior information such as meta-data.

A data column can be ascribed to a data type such as integer, string, date, ﬂoat, etc. Our focus
here is on inferring the data type for each column in a table of data. Numerous studies have attempted
to tackle type inference, including wrangling tools (Raman and Hellerstein, 2001; Kandel et al., 2011;
Guo et al., 2011; Trifacta, 2018; Fisher and Gruber, 2005; Fisher et al., 2008), software packages
(Petricek et al., 2016; Lindenberg, 2017; Stochastic Solutions, 2018; D¨ohmen et al., 2017a; Wickham
et al., 2017), and probabilistic approaches (Valera and Ghahramani, 2017; Vergari et al., 2019; Limaye
et al., 2010). However, often they do not work very well in the presence of missing and anomalous
data, which are commonly found in raw data sets due to the lack of a well-organized data collection
procedure.

One can develop a more accurate system by detecting missing and anomalous entries, and per-
forming type inference on the valid data values only. However, distinguishing such entries can be
challenging when missing data is not encoded explicitly, but as e.g., -1 or -99 in a column of integer
type. Pearson (2006) refers to such problems as the problem of disguised missing data. He further
shows how interpreting missing data as valid values can mislead the analysis. Similar problems are

This is a post-peer-review, pre-copyedit version of an article published in Data Mining and Knowledge Discovery.

The ﬁnal authenticated version is available online at: http://dx.doi.org/10.1007/s10618-020-00680-1.

1

 
 
 
 
 
 
also discussed by Qahtan et al. (2018). Existing type inference methods do not consider such problems
comprehensively. For example, they may fail when a column of integers contains string-valued missing
data encodings such as null, NA, etc. In this work, we incorporate such known missing data encodings
into our probabilistic model.

Chandola et al. (2009) deﬁne an anomaly as “a pattern that does not conform to expected nor-
mal behavior”. In our context, anomalies refer to unexpected or invalid entries for a given column,
which might result from the data collection procedure, e.g., error and warning messages generated
by servers or the use of open-ended entries while collecting data. The challenge here is mostly due
to the diﬃculty of modeling normal and anomalous data values such that the corresponding entries
become distinguishable. Moreover, one may need to consider separate strategies to model anomalies
for diﬀerent data types since their structures may vary w.r.t. the data types. For example, anomalies
in date columns might have a diﬀerent form than those in integer columns.

Up to now, too little attention has been paid to the aforementioned problems by the data mining
community. To this end, we introduce ptype, a probabilistic type inference method that can robustly
annotate a column of data. The proposed model is built upon Probabilistic Finite-State Machines
(PFSMs) that are used to model known data types, missing and anomalous data.
In contrast to
the standard use of regular expressions, PFSMs have the advantage of generating weighted posterior
predictions even when a column of data is consistent with more than one type model. Our method
generally outperforms existing type inference approaches for inferring data types, and also allows us
to identify missing and anomalous data entries.

In the rest of this paper, we ﬁrst describe PFSMs and introduce our model (Section 2). We then
present the related work (Section 3), which is followed by the experiments and the results (Section 4).
Finally, we summarize our work and discuss the possible future research directions (Section 5).

2 Methodology

This section consists of four parts. Sec. 2.1 gives background information on PFSMs used to model
regular data types, missing data, and anomalies. Sec. 2.2 introduces our model that uses a mixture
of PFSMs. Lastly, Sec. 2.3 and Sec. 2.4 describe respectively inference in and training of this model.
The data type, missing data and, anomalies can be deﬁned in broad terms as follows: The data
type is the common characteristic that is expected to be shared by entries in a column, such as integers,
strings, IP addresses, dates, etc., while missing data denotes an absence of a data value which can be
encoded in various ways, and anomalies refer to values whose types diﬀer from the given column type
or the missing type.

In order to model above types, we have developed PFSMs that can generate values from the
corresponding domains. This, in turn, allows us to calculate the probability of a given data value
being generated by a particular PFSM. We then combine these PFSMs in our model such that a data
column x can be annotated via probabilistic inference in the proposed model, i.e., given a column of
data, we can infer column type, and rows with missing and anomalous values.

2.1 Probabilistic Finite-State Machines

Finite-State Machines (FSMs) are a class of mathematical models used to represent systems consisting
of a ﬁnite number of internal states. The idea is to model a system by deﬁning its states (including
initial and ﬁnal states), transitions between the states, and external inputs/outputs. FSMs have a
long history going back at least to Rabin and Scott (1959) and Gill (1962). A more recent overview
of FSMs is given by Hopcroft et al. (2001).

In this study, we are interested in a special type of FSMs called Probabilistic Finite-State Machines
(PFSMs) in which transitions between states occur w.r.t. probability distributions (Paz, 1971; Rabin,
1963). Vidal et al. (2005) discuss various PFSMs in detail. Following a similar notation, we deﬁne a
PFSM as a tuple A = (θ, Σ, δ, I, F, T ), where θ is a ﬁnite set of states, Σ is a set of observed symbols,
δ ⊆ θ × Σ × θ is a set of transitions among states w.r.t. to observed symbols, I : θ → R+ is the

2

initial-state probabilities, F : θ → R+ is the ﬁnal-state probabilities, and T : δ → R+ is the transition
probabilities for elements of δ. During each possible transition between states, a symbol is emitted.
We denote such an event by a triple (q, α, q(cid:48)), which corresponds to a transition from a state q ∈ θ to
a state q(cid:48) ∈ θ emitting a symbol α ∈ Σ. Note that δ and T store respectively all the possible triples
and their corresponding probabilities.

A PFSM has to satisfy certain conditions. First, the sum of the initial-state probabilities has to
be equal to 1. Secondly, at each state q, it can either transition to another state q(cid:48) ∈ θ and emit a
symbol α ∈ Σ, or stop at state q without emitting any symbol. This can be expressed mathematically
as F (q) + (cid:80)
α∈Σ,q(cid:48)∈θ T (q, α, q(cid:48)) = 1 for each state q, where T (q, α, q(cid:48)) represents the probability of a
triple (q, α, q(cid:48)), and F (q) denotes the ﬁnal-state probability of state q. Based on the deﬁnition given
above, a PFSM can generate a set of strings, denoted by Σ∗1. For each string s ∈ Σ∗, we can calculate
a probability that represents how likely it is for a given PFSM to generate the corresponding string.
Note that PFSMs resemble Hidden Markov Models (HMMs) except that we now have the ﬁnal
state probabilities. Recall that each state in a HMM has a probability distribution over the possible
states that it can transition to. In PFSMs, each state also takes into account the probability of being a
ﬁnal state. Hence, the probability distribution is not only deﬁned over the possible transitions to next
states; it also includes the case of the current state being a ﬁnal state. On the other hand, emissions
are carried out similarly in PFSMs and HMMs: the observations are generated conditioned on the
hidden states in HMMs; an observation is emitted through a transition in PFSMs since each transition
is associated with a symbol. A detailed analysis of the link between PFSMs and HMMs can be found
in Dupont et al. (2005).

One can develop PFSMs to represent types described previously and calculate the probabilities for

each observed data value. We now explain the corresponding PFSMs in detail.

Representing Types with PFSMs

Here, we show how a PFSM can be used to model a data type. We divide types into 2 groups:
(i) primitive types consisting of integers, ﬂoats, Booleans and strings, and (ii) complex types such
as IP addresses, email addresses, phone numbers, dates, postcodes, genders and URLs. The details
regarding the implementation of the corresponding PFSMs can be found in Appendix A.

Consider integer numbers whose domain is {−∞, ∞}. We can represent the corresponding PFSM
as in the diagram given in Figure 1. The machine has two initial states, namely q0 and q1, and one
ﬁnal state q2. Here, q0 and q1 respectively allow us to represent integer numbers with a sign (plus or
minus), or without any sign2. The machine eventually transitions to the state q2, which stops with
a stopping probability F (q2) = Pstop. Otherwise, it transitions to itself by emitting a digit with an
equal probability, (1 − Pstop)/10. Similarly, we can develop a PFSM to represent each one of the other
column types.

start

q0

0.5(+)

0.5(−)

start

q1

0.1(0)

. . .
0.1(9)

p(0)

q2

. . .

p(9)

Figure 1: Graphical representation of a PFSM with states θ = {q0, q1, q2} and alphabet Σ =
{+, -, 0, 1, . . . , 9} where p denotes 1−Pstop

.

10

1Σ∗ denotes the set of strings a PFSM can generate by emitting multiple symbols.
2The transitions from state q1 allow the emission of a zero, which means that numbers like −007 can be emitted. If

this is not desired then one can adjust the PFSM in Figure 1 to not emit a leading 0.

3

The PFSM for missing values can be developed by using a predeﬁned set of codes such as
{-1, -9, -99, NA, NULL, N/A, -, . . . }. It assigns non-zero probabilities to each element in this set. Note
that the corresponding PFSM already supports a wide range of codes, but it can be easily extended
by the user to incorporate other known missing data encodings, leading to semi-automated operation.
In order to model anomalies, we adapt the idea of X-factor proposed by Quinn et al. (2009).
We deﬁne a machine with the widest domain among all the PFSMs that supports all of the possible
characters. This choice of PFSM lets the probabilistic model become more robust against anomalies
since it assigns probabilities to data values that do not belong to any of the PFMSs representing the
data types. Note that as this PFSM covers a wider domain, it will assign lower probabilities to known
data types than the speciﬁc models.

Constructing PFSMs for complex types might require more human engineering than the other
types. We reduce this need by building such PFSMs automatically from corresponding regular ex-
pressions. We ﬁrst convert regular expressions to FSMs by using greenery (2018) (see the function
named to fsm()). We then build their probabilistic variants, where the parameters are assigned equal
probabilities. Note that these parameters can be updated with the training.

2.2 The Proposed Model

We propose a new probabilistic mixture model with a noisy observation model, allowing us to detect
missing and anomalous data entries. Our model ﬁrst generates a column type from a set of possible
regular data types. This is followed by a “deﬁciency” process that can potentially change the data type
of each row. Consequently, each row might have a diﬀerent type rather than the generated column
type. The observation model then generates a data value for each entry according to its type. We
now introduce our notation to represent this process.
We assume that a column of data x = {xi}N

i=1 has been read in, where each xi denotes the
characters in the ith row. We propose a generative model with a set of latent variables t ∈ {1, 2, ..., K}
and z = {zi}N
i=1, where t and zi respectively denote the data type of a column and its ith row. Here, N
is the number of rows in a data column and K is the number of possible data types for a column. We
also use the additional missing and anomaly types, denoted by m and a respectively, and described
above. Note that zi can be of type m or a alongside a regular data type, i.e. zi ∈ {1, 2, ..., K, m, a}.
This noisy observation model allows a type inference procedure robustiﬁed for missing and anomalous
data values.

Hence the model has the following generative process:

column type t ∼ U(1, K),

row type zi =






t with probability πt
t ,
m with probability πm
t ,
a with probability πa
t ,

row value xi ∼ p(xi|zi),

t + πm

t + πa

where Π and p(xi|zi) are respectively the model parameter and the observation model. U denotes a
discrete Uniform distribution. Here πt
t = 1 for each column type t. Since entries are often
expected to be of a regular data type rather than missing or anomaly types, we favour regular types
during inference by using lower coeﬃcients for missing and anomaly types, i.e. πm
t < πt
t.
These weight parameters Π are assumed to be ﬁxed and known. Even though one can also learn the
weights, this may not be vital as long as the coeﬃcients of the regular types are larger than the others.
We use a uniform distribution for column types as in most cases we do not have any prior infor-
mation regarding the type of a column that would allow us to favour a particular one. To represent
the conditional distribution p(xi|zi), we have developed PFSMs as described above.

t and πa

t < πt

4

2.3

Inference

Given a data column x, our initial goal is to infer the column type t, which is cast to the problem of
calculating the posterior distribution of t given x, denoted by p(t|x). Then, we assume that each row
can be of three types: (i) the same as the column type, (ii) the missing type, and (iii) the anomaly
type. In order to identify missing or anomalous data entries, we calculate the posterior probabilities of
each row type, namely p(zi|t, x). In this section, we now brieﬂy discuss the corresponding calculations;
detailed derivations are presented in Appendix D.

Assuming that entries of a data column are conditionally independent given t, we can obtain the

posterior distribution of column type t as follows:

p(t = k|x) ∝ p(t = k)

N
(cid:89)

(cid:16)

i=1

k p(xi|zi = k) + πm
πk

k p(xi|zi = m) + πa

kp(xi|zi = a)

(cid:17)

,

(1)

which can be used to estimate the column type t, since the one with maximum posterior probability

is the most likely data type corresponding to the column x.

As per equation 1, the model estimates the column type by considering all the data rows, i.e.
having missing data or anomalies does not confuse the type inference. Note that such entries would
have similar likelihoods for each column type, which allows the model to choose the dominant data
type for regular entries.

Following the inference of column type, we can also identify entries of x which are more likely to
be missing or anomalies rather than the inferred type. For this, we compare the posterior probabilities
of each row type zi given t = k and xi, namely p(zi = j|t = k, xi), which can be written as:

p(zi = j|t = k, xi) =

Complexity Analysis

πj
kp(xi|zi = j)
(cid:96)∈{k,m,a} π(cid:96)

kp(xi|zi = (cid:96))

(cid:80)

.

(2)

The computational bottleneck in the inference is the calculation of p(x|t = k) for each type k, which
is the calculation of the probability assigned for a data column x by the kth PFSM. Note that this can
be carried out by taking into account the counts of the unique data entries, for eﬃciency. Denoting
the uth unique data value by xu, we need to consider the complexity of calculating p(xu|t = k) which
can be done via the PFSM Forward algorithm. Each iteration of the algorithm has the complexity of
k ) where Mk is the number of hidden states in the kth PFSM. As the number of iterations equals
O(M 2
to the length of xu denoted by L, the overall complexity of the inference becomes O(U KM 2L), where
U is the number of unique data entries, K is the number of types, and M is the maximum number of
hidden states in the PFSMs.

2.4 Training of the Model

The aim of the training is to tune the probabilities assigned by the PFSMs so that column types are
inferred accurately. Given a set of columns and their annotated column types, the task is to ﬁnd the
parameters of the PFSMs (i.e. the initial-state, the transition, and the ﬁnal-state probabilities) that
allow the “correct” machine to give higher probabilities to the observed entries. This is crucial as
multiple PFSMs can assign non-zero probabilities for certain strings, e.g., 1, True, etc.

We employ a discriminative training procedure on our generative model, as done in discriminative
training of HMMs (Bahl et al., 1986; Jiang, 2010; Brown, 1987; N´adas et al., 1988; Williams and
Hinton, 1991). This is shown to be generally superior to maximum likelihood estimations (Jiang,
2010), since a discriminative criterion is more consistent with the task being optimized. Moreover, it
allows us to update not only the parameters of the “correct” PFSM but also the parameters of the
other PFSMs given a column of data and its type, which in turn helps the correct one to generate the
highest probability.

5

We choose (cid:80)M

j=1 log p(tj|xj) as the objective function to maximize, where xj and tj respectively
denote the jth column of a given data matrix X and its type, and M is the number of columns. We
then apply Conjugate Gradient algorithm to ﬁnd the parameters that maximize this objective function
(please see Appendix C for detailed derivations of the gradients).

We study diﬀerent parameter settings for our model. We ﬁrst explore tuning the parameters by
hand to incorporate certain preferences over the types, e.g., Boolean over Integer for 1. Then, we learn
the parameters via the discriminative training described above where the parameters are initialized at
the hand-crafted values. Note that due to the absence of explicit labels for the missing and anomaly
types, these are not updated from the hand-crafted parameters. We have also employed the training
by initializing the parameters uniformly. However, we do not report these results as they are not
competitive with the others.

(cid:80)

i ), where p(xi

As the PFSMs are generative models it would be possible to train them unsupervised, to maximize
i,j log p(xj
j) is deﬁned as a mixture model over all types (including missing and anomaly)
for the ith row and jth column. The component PFSMs could then be updated using the Expectation-
Maximization (EM) algorithm. However, such training would be unlikely to give as good classiﬁcation
performance as supervised training.

3 Related Work

Numerous studies have attempted to tackle type inference, including wrangling tools and software
libraries. Such approaches have made limited use of probabilistic methods, while the existing proba-
bilistic approaches do not address the same type inference problem that we do. Below we discuss the
capabilities of these existing methods in the presence of missing and anomalous data.

Type inference is commonly carried out by validation functions and regular expressions. For
example, several wrangling tools, including Trifacta (2018) and its preceding versions (Raman and
Hellerstein, 2001; Kandel et al., 2011; Guo et al., 2011) apply validation functions to a sample of data
to infer types, e.g., assign the one validated for more than half of the non-missing entries as the column
type (Kandel et al., 2011). When multiple types satisfy this criterion, the more speciﬁc one is chosen
as the column type, e.g., an integer is assumed to be more speciﬁc than a ﬂoat. Trifacta supports a
comprehensive set of data types, and provides an automatic discrepancy detector to detect errors in
data (Raman and Hellerstein, 2001). However, in our experience, its performance on type inference
can be limited on messy data sets.

Fisher and Gruber (2005) and Fisher et al. (2008) have developed data description languages for
processing ad hoc data (PADS) which enables generating a human-readable description of a dataset,
based on data types inferred using regular expressions. However, their focus is on learning regular
expressions to describe a dataset, rather than classifying the data columns into known types. Unlike
the PADS library which parses a data ﬁle itself, Test-Driven Data Analytics (TDDA, Stochastic
Solutions 2018) uses the Pandas CSV reader to read the data into a data frame. It then uses the
Pandas dtypes attributes3, to determine the data type of the columns. However, this leads to a poor
type detection performance since the Pandas reader is not robust against missing data and anomalies,
where only empty string, NaN, and NULL are treated as missing data.

Petricek et al. (2016) propose another use of regular expressions with F#, where types, referred
as shapes, are inferred w.r.t. a set of preferred shape relations. Such relations are used to resolve
ambiguous cases in which a data value ﬁts multiple types. This procedure allows integrating inferred
types into the process of coding, which can be useful to interact with data. However, it does not
address the problems of missing and anomalous data comprehensively, where only three encodings of
missing data, NA, #NA, and :, are supported. This may lead to poor performance on type inference
when missing data is encoded diﬀerently, or there are anomalies such as error messages.

A set of software packages in R and Python can also infer data types. messytables (Lindenberg,
2017) determines the most probable type for a column by weighting the number of successful conver-

3obtained with the function pandas tdda type().

6

sions of its elements to each type. This can potentially help to cope with certain data errors; however,
it might be diﬃcult to ﬁnd an eﬀective conﬁguration of the weights for a good performance. Moreover,
it can not handle the disguised missing data values, e.g., -1 in a column of type integer, which can
be misleading for the analysis. D¨ohmen et al. (2017b) propose a CSV parser named hypoparsr that
treats type inference as a parsing step. It takes into account a wide range of missing data encodings;
however, it does not address anomalies, leading to poor performance on type inference. readr (Wick-
ham et al., 2017) is an R package to read tabular data, such as CSV and TSV ﬁles. However, in
contrast to hypoparsr, a limited set of values are considered as missing data, unless the user speciﬁes
otherwise. Furthermore, it employs a heuristic search procedure using a set of matching criteria for
type inference. The details regarding the criteria are given in Wickham and Grolemund (2016). The
search continues until one criterion is satisﬁed, which can be applied successfully in certain scenarios.
Whenever such conditions do not hold, the column type is assigned to string.

Valera and Ghahramani (2017) propose a model for discovery of statistical types, but this is
tackling a very diﬀerent problem than the one that we address. Their method assumes that all of
the entries in a column contain numerical data values, i.e. it cannot handle data types such as string,
date, etc. Moreover, it can only handle clean integer and ﬂoat columns that do not have any missing
data and anomalies. Given this they address the problem of making ﬁne-grained distinctions between
diﬀerent types of continuous variables (real-valued data, positive real-valued data, and interval data)
and discrete variables (categorical data, ordinal data, and count data). It would be possible to combine
their method with ours to provide more ﬁne-grained distinctions of real-valued and discrete-valued
data once the issues of detecting missing and anomalous data have been resolved.

Valera and Ghahramani (2017) is extended in Vergari et al. (2019) where the proposed method can
be used to impute missing data and detect numerical anomalies. However, they also assume that each
entry of a data column contains a numerical value or an explicit missing data indicator. Their method
cannot be used when a data column contains non-numerical missing data encodings and anomalies.
Moreover, the authors address a completely diﬀerent missing data problem, i.e. they tackle missing
data imputation rather than missing data detection. This means that the proposed method cannot
detect whether an entry is missing or not, but it can impute missing data once the location of missing
data is known. Lastly, their focus on anomaly detection is also diﬀerent than ours in the sense that
they attempt to detect numerical outliers, but do not detect string anomalies in a column.

Limaye et al. (2010) propose a log-linear model based method to annotate a table in terms of the
semantic column type, cell entity, and column relations, given an ontology of relevant information. For
example, given a table that contains information about actors such as names and ages, the task is to
ﬁnd semantic column types such as actor and age, the entities each row refers to, and the relationship
of actor-age between two columns. Even though the scope of these annotations are wider than ours,
missing data and anomalies are not considered explicitly, and an appropriate ontology is required.

4 Experiments

In this section, we ﬁrst in Sec. 4.1 describe the datasets and evaluation metrics used, and then in
Sec. 4.2 compare ptype with competitor methods on two tasks: (i) column type inference, and (ii)
type/non-type inference. Lastly, we present a qualitative evaluation of our method on challenging
cases in Sec. 4.3. The goal of our experiments is to evaluate (i) the robustness of the methods against
missing data and anomalies for column type inference and (ii) the eﬀectiveness of type/non-type
inference. These are evaluated both quantitatively (Sec. 4.2) and qualitatively (Sec. 4.3). We release
our implementation of the proposed method at https://github.com/tahaceritli/ptype-dmkd.

4.1 Experimental Setup

We have trained ptype on 25, and tested on 43 data sets obtained from various sources including UCI
ML4, data.gov.uk, ukdataservice.ac.uk, and data.gov. The data types were annotated by hand

4https://archive.ics.uci.edu/ml/datasets.html

7

for these sets. We also annotated each dataset in terms of missing data and anomalies, by using the
available meta-data, and checking the unique values.

On column type inference, we compare our method with F# (Petricek et al., 2016), hypoparsr
(D¨ohmen et al., 2017a), messytables (Lindenberg, 2017), readr (Wickham et al., 2017), TDDA (Stochas-
tic Solutions, 2018) and Trifacta (2018). Note that some of the related works are not directly applicable
for this task, and these are not included in these experiments. For example, we are not able to use
Raman and Hellerstein (2001), Kandel et al. (2011) and Guo et al. (2011) as they are not available
anymore. However, we use their latest version Trifacta in our experiments. We also exclude the PADS
library (Fisher and Gruber, 2005; Fisher et al., 2008), since it does not necessarily produce columns
and their types. The methods proposed by Valera and Ghahramani (2017) and Vergari et al. (2019)
are also not applicable for this task. First, they do not consider data types of Boolean, string, date.
Secondly, they only address integer and ﬂoat columns that do not contain any non-numerical missing
data or anomalies, which are commonly found in real-world datasets. Note that Vergari et al. (2019)
do not address missing data detection but missing data imputation, and can only handle numerical
outliers but not non-numerical outliers, whereas Valera and Ghahramani (2017) do not address these
questions. Lastly, we exclude the method presented by Limaye et al. (2010) as their goal is to infer
semantic entity types rather than syntactic data types.

On type/non-type inference, we evaluate the performance of our method in detecting missing data
and anomalies. We label such entries as non-type, and classify each entry as either type or non-type.
On this task, we compare our method with Trifacta only, as it is the leading competitor method on
column type inference, and the others do not address this task comprehensively.

Data Sets

We have conducted experiments on the data sets chosen according to two criteria: (i) coverage of the
data types, and (ii) data quality. In our experiments, we consider ﬁve common column types including
Boolean, date, ﬂoat, integer, and string. Table 1 presents the distribution of the column types found
in our data sets. Any other columns not conforming to the supported data types are omitted from the
evaluations. Secondly, we have selected messy data sets in order to evaluate the robustness against
missing data and anomalies. The fraction of the non-type entries in the test datasets can be as large
as 0.56, while the average fraction is 0.11. Note that available data sets, their descriptions and the
corresponding annotations can be accessed via https://goo.gl/v298ER.

Column Type

Boolean Date Float

Integer String Total

Training
Test

75
43

49
40

99
53

257
240

309
234

789
610

Table 1: Histogram of the column types observed in the training and the test data sets.

Evaluation Metrics

For column type inference, we ﬁrst evaluate the overall accuracy of the methods on type inference
by using the accuracy. However, this may not be informative enough due to the imbalanced data
sets. Note that the task of type inference can be seen as a multi-class classiﬁcation problem, where
each column is classiﬁed into one of the possible column types. In order to measure the performance
separately for each type, we follow a one-vs-rest approach. In such cases a common choice of metric is
the Jaccard index J (see e.g., Hand et al. (2001, sec 2.3)) deﬁned as TP/(TP + FP + FN), where TP,
FP, and FN respectively denote the number of True Positives, False Positives and False Negatives.
(Note that one-vs-rest is an asymmetric labelling, so True Negatives are not meaningful in this case.)
To measure the performance on type/non-type inference, we report Area Under Curve (AUC) of
Receiver Operating Characteristic (ROC) curves, as well as the percentages of TPs, FPs, and FNs.

8

Note that here we denote non-type and type entries as Positive and Negative respectively.

4.2 Quantitative Results

We present quantitative results on two tasks: (i) column type inference, and (ii) type/non-type de-
In column type inference, we evaluate the performance of the methods on detecting data
tection.
types of columns, and investigate their scalability. Then, in type/non-type detection we evaluate their
capability of detecting missing data and anomalies.

Column Type Inference

We present the performance of the methods in Table 2, which indicates that our method performs
better than the others for all types, except for the date type where it is slightly worse than Trifacta.
In the table ptype denotes the discriminatively trained model, and ptype-hc the version with hand-
crafted parameters. These improvements are generally due to the robustness of our method against
missing data and anomalies.

Notice that the discriminative training improves the performance, speciﬁcally for Boolean and
integer types. This shows that the corresponding confusions can be reduced by ﬁnding more optimal
parameter values, which can be diﬃcult otherwise. Note that the training has a slightly negative eﬀect
on ﬂoat and string types, but it still performs better than the other methods. These changes can be
explained by the fact that the cost function aims at decreasing the overall error over columns rather
than considering individual performances.

Method

F# messytables

readr TDDA Trifacta ptype-hc ptype

Overall
Accuracy
Boolean
Date
Float
Integer
String

0.73

0.72

0.69

0.61

0.90

0.55
0.35
0.60
0.55
0.61

0.56
0.17
0.57
0.55
0.61

0.00
0.10
0.59
0.57
0.58

0.00
0.00
0.42
0.46
0.51

0.49
0.68
0.87
0.88
0.83

0.92

0.75
0.67
0.93
0.85
0.90

0.93

0.83
0.67
0.91
0.88
0.89

Table 2: Performance of the methods using the Jaccard index and overall accuracy, for the types
Boolean, Date, Float, Integer and String.

(a) ptype

(b) Trifacta

Figure 2: Normalized confusion matrices for (a) ptype and (b) Trifacta plotted as Hinton diagrams,
where the area of a square is proportional to the magnitude of the entry.

Figure 2 shows normalized confusion matrices for ptype and Trifacta, normalized so that a column
sums to 1. This shows that they both tend to infer other column types as string even if they are not.

9

However, ptype has few of such confusions, especially when the true type is Boolean or ﬂoat.

It is noticeable from Table 2 that dates are diﬃcult to infer accurately. Detailed inspection shows
that this is due to non-standard formats used to denote dates. We could improve the PFSM for dates
in ptype to include such formats, but we have not done so, to avoid optimizing on test datasets. The
performance of Trifacta on dates can be explained by the engineering power behind the tool, and
indicates its capability to represent non-standard formats using validation functions.

To determine whether the column type predictions of ptype and Trifacta are signiﬁcantly diﬀerent,
we apply the McNemar’s test (see e.g., Dietterich 1998), which assumes that the two methods should
have the same error rate under the null hypothesis. We compute the test statistic (|n01 − n10| −
1)2)/(n01 + n10), where n01 and n10 denote the number of test columns misclassiﬁed by only Trifacta,
and by only ptype respectively. In our case, n01 and n10 are respectively equal to 19 and 6, which
results in a statistic of 5.76. If the null hypothesis is correct, then the probability that this statistic is
greater than 3.84 is less than 0.05 (Dietterich, 1998). Thus this result provides evidence to reject the
null hypothesis, and conﬁrms that the methods are statistically signiﬁcantly diﬀerent to each other.

The large performance gap for Booleans suggests that our method handles confusions with integers
and strings better. Analysis shows that such confusions occur respectively in the presence of {0, 1}, and
{Yes, No}5. We further note that F#, messytables, and readr perform similarly, especially on ﬂoats,
integers, and strings; which is most likely explained by the fact that they employ similar heuristics.

Since hypoparsr names column types diﬀerently, except for the date type, we need to rename the
annotations, and evaluate the methods again in order to compare them with hypoparsr. It refers to
Boolean and string respectively as logical and text. Moreover, integer and ﬂoat are grouped into a
single type called numeric. The resulting evaluations, which are reported in Table 6 in Appendix
F, shows a similar trend to as before in that our method performs better. However, we see some
variations, which result from the fact that we now evaluate on a smaller number of data sets since
hypoparsr, which is said to be designed for small sized ﬁles, was able to parse only 33 out of the 43 test
data sets6. This left us 358 columns including 29 date, 21 logical, 159 numeric, and 149 text. Lastly,
we observe that hypoparsr results in a higher number of confusions by inferring the type as integer
whereas the true type is text. Such cases mostly occur when the data values consist of a combination
of numeric and text, e.g., ID and address columns.

Scalability

We describe the complexity of the inference in our model in Section 2.3. Here, we demonstrate its
scaling by measuring the time it takes to infer the column types for each test dataset.

Recall that the complexity is O(U KM 2L), where U is the number of unique data entries, K is
the number of data types, M is the maximum number of hidden states in the PFSMs, and L is the
maximum length of data values. Notice that the complexity depends on data through U and L, and
does not necessarily increase with the number of rows. In fact, it grows linearly with the number of
unique values assuming L is constant. As shown in Fig. 3, the runtime for ptype is upper bounded
by a line c0 + c1U , where c0 is a small constant. The runtime thus scales linearly with the number of
unique values U , handling around 10K unique values per second. The variations can be explained by
changes in L.

We also report the size of the datasets and the times the methods take in Appendix G. We have
observed similar performance with messytables, whereas readr and TDDA seem much faster even
though they do not only predict the data types but also parse a given dataset. On the other hand,
hypoparsr takes much longer compared to the others. Lastly, we measure the processing times for
Trifacta via command line. We have observed that Trifacta takes less time than hypoparsr, but it
often takes longer compared to the other methods.

5We assume that a data column where the entries are valued as Yes or No is more likely to be a Boolean column than
a string. We have also conﬁrmed these cases with the corresponding metadata whenever available, and have carefully
annotated our datasets in terms of data types.

6This was using the default settings, but Till D¨ohmen (pers. comm.) advised against changing the limit.

10

Figure 3: The time in seconds taken to infer a column type with ptype, as a function of the number
of unique values U in the column. Also shown is the line c0 + c1U , where c0 is a small constant.

Type/Non-type Inference

Trifacta labels each entry in a data table either as type or non-type, whereas our model presents a
probability distribution over the two labels. One could apply a threshold on these probabilities in
order to assign a label to each entry. Here, we demonstrate how the methods behave under diﬀerent
thresholds. We aggregate the entries of each dataset over its columns, and compute the ROC curve
for each method.

T P

Figure 4 presents the diﬀerence AUC(ptype) - AUC(Trifacta) per dataset. Note that we exclude
ﬁve datasets as the corresponding AUCs are undeﬁned due to the deﬁnition of True Positive Rate
(
T P +F N ). This becomes undeﬁned since the denominator becomes zero when both TP and FN are
equal to zero, which occurs naturally when a dataset does not contain any missing data and anomalies.
The average of AUCs of the remaining datasets are respectively 0.77 and 0.93 for Trifacta and ptype.
To compare these two sets of AUCs, we apply a paired t-test, which results in the t-statistic of 4.59
and p-value of 0.00005. These results reject the null hypothesis that the means are equal, and conﬁrm
that they are signiﬁcantly diﬀerent.

Figure 4: AUC(ptype) - AUC(Trifacta) plotted for each test dataset.

Lastly, we compare Trifacta and ptype in terms of percentages of TPs, FPs, and FNs which are
presented in Table 3, where the labels for ptype are generated by applying a threshold of 0.5 on the
posterior distributions. Note that here we aggregate the predictions over the datasets. As per the
table, ptype results in a higher number of FPs than Trifacta, but Trifacta produces a higher number
of FNs and a lower number of TPs than ptype. Note that here we denote non-type and type entries as
Positive and Negative respectively. This indicates that ptype is more likely to identify non-types than
Trifacta, but it can also label type entries as non-types more often. However, the overall performance

11

020 k40 k60 k80 k100 k120 k140 kU05101520time (secs.)accidents_2015accident2016autocasualties_2015census_income_kddedf_stockselninoinspection_outcomesintel_labminy_vendorschool_characteristicsschool_sessionssurveyvehicles_2015data_gov_323_1data_gov_356_1data_gov_3397_1data_gov_5134_1data_gov_10012_1data_gov_10151_1data_gov_12252_1data_gov_16834_1data_gov_18386_1mass_1mass_2mass_5mass_621329809_0_5526..24036779_0_5608..24142265_0_4577..26270372_1_3774..28086084_0_3127..28154036_0_9090..28646774_0_3256..29886325_0_1448..34899692_0_6530..40534006_0_4617..41480166_0_6681..Dataset Name−0.2−0.10.00.10.20.30.40.5AUC Difference between ptype and TrifactaFigure 5: Annotating a subset of a T2D data set using ptype as normal, missing, and anomalous
entries, denoted by green, yellow, and red, respectively.

of ptype is better than Trifacta, as we have also observed in the AUCs.

Method FPs FNs TPs

Trifacta
ptype

0.67
1.13

3.96
0.20

1.57
5.34

Table 3: The percentages of FPs, FNs, and TPs for Trifacta and ptype on type/non-type detection.

We now present two examples to give insight into Table 3. Consider the “Substantial growth of
knowledge skills” column of the mass 6 dataset which consists of ﬂoating-point numbers. However,
most of the 3148 entries are non-types, i.e. empty entries, -, N/A, and NA which occur 780, 470, 1063,
and 424 times respectively. Such non-type entries are labeled correctly by ptype, whereas Trifacta
can only classify the empty entries correctly as non-type. The remainder of the non-type entries are
considered to be valid type entries, since they conform with the column type which is inferred as string
by Trifacta. Note that this confusion is high likely due to the low number of ﬂoats in the column.
Here, Trifacta results in a high percentage of FNs, and a lower percentage of TPs than ptype.

The second example illustrates the extent of cases in which ptype results in FPs. For example,
the “CAUSE NAME” column in the data gov 323 1 dataset consists of data values such as Cancer,
Stroke, and Suicide etc. Here, ptype and Trifacta infer the column type as string, and label such
entries correctly as type entries. However, Alzheimer’s disease and Parkinson’s disease are
misclassiﬁed as non-types by ptype (1,860 FPs out of 13,260 entries) as our string model does not
support the apostrophe. To handle this, we could include ’ in the corresponding alphabet, but we
also ﬁnd it helpful to detect “true” non-type entries having that character. We believe that such cases
should be left to users with domain knowledge as they can easily extend the alphabet of the string
type.

4.3 Qualitative Results

We now give some examples of predicting missing and anomalous data.

Missing Data: We support an extensive range of values that are used to denote missing data. Note
that multiple such encodings can be detected at the same time. Consider a T2Dv2 dataset7 where
missing entries are denoted by encodings such as NULL and n/a. Our method can successfully annotate
such entries as shown in Figure 5.

Next, we show how our model approaches to unseen missing data encodings which are not explic-
itly considered as missing data in our model, but can be handled with the anomaly type. For example,
“Time of Incident” column of Reported Taser 2015 data set is expected to contain date values. How-
ever, some entries have the value Unknown. Thanks to the PFSM for anomaly type, our model detects
such encodings as anomalies as long as the “correct” column type is not string, resulting in a better
performance of type inference.

Another advantage of ptype is that it can detect missing data even if their types ﬁt the column type.
Consider an integer-typed column Stake in the Rodents data set where -99 is used to denote missing

734899692 0 6530393048033763438.csv

12

entries. Our method ﬂags those entries as missing instead of treating them as regular integers since the
missing type accepts -99. Similarly, we are able to detect string-valued missing data in string-typed
columns. When the column type however is not string, our model may result in a number of false
alarms by ﬂagging normal entries as missing data. For example, integer values of -1, -99, etc. can also
represent normal data instances. We will investigate this using methods as in Qahtan et al. (2018),
and develop missing data models speciﬁc to each data type, in order to improve this issue.

Anomalies: As mentioned earlier, we can also use ptype to detect anomalies. Our model ﬂags such
entries automatically since the anomaly model covers a wide range of values including those that are
not supported by the column type.

Figure 5 shows the capability of detecting anomalies when the column type is string. As we do not
have the character & in the alphabet of the PFSM for strings, the anomaly machine allows us to detect
the anomalies in the “country” column. Similarly, the characters “refer to euro” are not supported
by the PFSM for integers, letting us detect the corresponding entry as anomalous. Moreover, we can
separately detect the anomalous and missing data as in the “selling rate” column.

Interestingly, we notice that the question mark character ? is used to express the doubt about
the collected data in the HES data set, where a data entry contains the value of 6?. We can also see
that missing data encodings not incorporated to our missing data model such as NOT AVAILABLE, ??
(double question marks), and --, are detected as anomalies.

We now discuss some other aspects of our model, such as ambiguities that can occur, and how

they can be handled; and failure cases which can be diﬃcult to avoid.

The Limitations of Our Work

Ambiguous Cases: In certain cases the posterior probability distribution over types is not heavily
weighted on a particular type. For example, consider a column of data that contains values of NULL
and 1. This could ﬁt multiple PFSMs as 1 can either be an integer, a ﬂoat, a string, or a Boolean value.
However, we have assumed that 0 and 1 are more likely to be indicators of Booleans and have thus
tuned the parameters of the corresponding machine such that the probabilities associated with 0 and 1
are slightly higher than the ones assigned by the other machines. This leads to a posterior probability
distribution with values of 0.29, 0.26, and 0.44 respectively for integers, ﬂoats, and Booleans. One can
also exploit the probabilistic nature of our model to treat such ambiguous cases diﬀerently. Instead
of directly taking the type with the highest posterior probability as the column type, one can detect
such ambiguities, and then exploit user feedback to improve the decision.

A uniformly distributed posterior distribution over types is observed when all of the entries of a
column are assigned zero probabilities by the PFSMs of the regular data types. This is not surprising
the widest alphabet
as we only support a limited set of characters in the regular machines, i.e.
among the regular data types, which is of the string type, consists of the letters, digits, and a set
of punctuations. For example, “per capitagdp (us$)[51]” column of a T2D data set8 has values such
as $2949.57. Similarly, the 23rd column of the fuel data set contains values such as “( 3.0L) ”.
Note that the anomaly type still assigns positive probabilities to these entries, as its alphabet includes
all the possible characters. However, when its weight πa
is the same regardless of the type, the
t
corresponding posterior probabilities become equal. One can handle such cases by learning diﬀerent
weight parameters for the types. However, here, to represent such unsupported/unknown types, we
have developed X-type, an anomaly-like PFSM which is used as a regular column type. This can
be further exploited to create new data types, which can be automated via a probabilistic training
procedure.

Failure Cases: We now present two cases for which ptype-hc fails to infer the column types correctly.
For example, consider a column (“BsmtHalfBath” of Housing Price data set) which denotes the number
of half bathrooms in the basement of houses, consisting of the values in {0, 1, 2}. In this case, ptype-hc
puts higher posterior probability on the Boolean type whereas the actual type is integer. This may
not be surprising, considering the fact that 0 and 1 have higher probabilities of being a Boolean, and

824036779 0 5608105867560183058.csv

13

2, occurring only twice out of 1460 entires, is treated as an anomaly. However, ptype is able to correct
this failure thanks to the discriminative training. Note that the competitor methods fail in this case.
After the evaluations, we have discovered a set of cases we have not considered in the beginning.
For example, several Boolean columns of the Census Income KDD data set have leading whitespace as
in “ No”, “ Yes”. Our model infers the types of these Boolean columns as string since such whitespace
is not considered in the Boolean type. In order to avoid optimizing the model on the test sets, we
have not addressed such cases. However, they can easily be handled by updating the corresponding
PFSMs to include the whitespace. Note that the other methods also detect the column types as string
in these cases.

There are cases of non-standard dates we do not currently handle. For example, dates are some-
times divided into multiple columns as day, month, and year. Our model detects day and month
columns separately as integers. One could develop a model that checks for this pattern, making use
of constraints on valid day, month and year values.

5 Summary

We have presented ptype, a probabilistic model for column type inference that can robustly detect the
type of each column in a given data table, and label non-type entries in each column. The proposed
model is built on PFSMs to represent regular data types (e.g., integers, strings, dates, Booleans,
etc.), missing and anomaly types. An advantage of PFSMs over regular expressions is their ability
to generate weighted posterior predictions even when a column of data is consistent with more than
one type model. We have also presented a discriminative training procedure which helps to improve
column type inference. Our experiments have demonstrated that we generally achieve better results
than competitor methods on messy data sets.

Future work includes extending the supported data types, such as categorical data, etc.; developing
subtypes, e.g., for Booleans expecting either True and False, or yes and no; and improving anomaly
detection for string-typed data by addressing semantic and syntactic errors.

Acknowledgements

TC is supported by a PhD studentship from the Alan Turing Institute, under the EPSRC grant
TU/C/000018. CW and JG would like to acknowledge the funding provided by the UK Government’s
Defence & Security Programme in support of the Alan Turing Institute. The work of CW is supported
in part by EPSRC grant EP/N510129/1 to the Alan Turing Institute. We thank the anonymous
reviewers for their comments that have helped improve the paper.

References

Bahl, L., Brown, P., De Souza, P., and Mercer, R. (1986). Maximum mutual information estimation
of hidden Markov model parameters for speech recognition. In IEEE ICASSP’86, volume 11, pages
49–52. IEEE.

Brown, P. F. (1987). The Acoustic-Modeling Problem in Automatic Speech Recognition. Ph.D. Disser-

tation, Department of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA.

Chandola, V., Banerjee, A., and Kumar, V. (2009). Anomaly detection: A survey. ACM Computing

Surveys (CSUR), 41(3):15.

Dasu, T. and Johnson, T. (2003). Exploratory data mining and data cleaning: An overview.

In
Exploratory Data Mining and Data Cleaning, volume 479, chapter 1, pages 1–16. John Wiley &
Sons, Inc., New York, NY, USA, 1 edition.

Dietterich, T. G. (1998). Approximate statistical tests for comparing supervised classiﬁcation learning

algorithms. Neural Computation, 10(7):1895–1923.

14

D¨ohmen, T., M¨uhleisen, H., and Boncz, P. (2017a). hypoparsr. https://github.com/tdoehmen/

hypoparsr [Accessed on 29/06/2018].

D¨ohmen, T., M¨uhleisen, H., and Boncz, P. (2017b). Multi-Hypothesis CSV Parsing. In Proceedings

of the 29th SSDBM.

Dupont, P., Denis, F., and Esposito, Y. (2005). Links between probabilistic automata and hid-
den Markov models: Probability distributions, learning models and induction algorithms. Pattern
Recognition, 38(9):1349–1371.

Fisher, K. and Gruber, R. (2005). PADS: A domain-speciﬁc language for processing ad hoc data. In
Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and imple-
mentation (PLDI’05), volume 40(6), pages 295–304. ACM.

Fisher, K., Walker, D., Zhu, K. Q., and White, P. (2008). From dirt to shovels: Fully automatic tool

generation from ad hoc data. In POPL ’08, volume 43(1), pages 421–434. ACM.

Gill, A. (1962). The basic model. In Introduction to the Theory of Finite-State Machines, pages 1–15.

McGraw-Hill Book Company.

greenery (2018). greenery. https://github.com/qntm/greenery/ [Accessed on 31/05/2019].

Guo, P. J., Kandel, S., Hellerstein, J. M., and Heer, J. (2011). Proactive wrangling: Mixed-initiative

end-user programming of data transformation scripts. In UIST ’11, pages 65–74. ACM.

Hand, D. J., Mannila, H., and Smyth, P. (2001). Principles of Data Mining. MIT Press.

Hopcroft, J. E., Motwani, R., and Ullman, J. D. (2001). Introduction to automata theory, languages,

and computation, 2nd edition. ACM SIGACT News, 32(1):60–65.

Jiang, H. (2010). Discriminative training of HMMs for automatic speech recognition: A survey.

Computer Speech & Language, 24(4):589–608.

Kandel, S., Paepcke, A., Hellerstein, J., and Heer, J. (2011). Wrangler: Interactive visual speciﬁcation

of data transformation scripts. In CHI ’11, pages 3363–3372. ACM.

Limaye, G., Sarawagi, S., and Chakrabarti, S. (2010). Annotating and searching web tables using

entities, types and relationships. VLDB ’10, 3(1-2):1338–1347.

Lindenberg, F. (2017). messytables Documentation Release 0.3. https://media.readthedocs.org/

pdf/messytables/latest/messytables.pdf [Accessed on 29/06/2018].

N´adas, A., Nahamoo, D., and Picheny, M. A. (1988). On a model-robust training method for speech
recognition. IEEE Transactions on Acoustics, Speech, and Signal Processing, 36(9):1432–1436.

Paz, A. (1971). Introduction to Probabilistic Automata, volume 78. Academic Press, Inc., New York,

NY, USA.

Pearson, R. K. (2006). The problem of disguised missing data. ACM SIGKDD Explorations, 8(1):83–

92.

Petricek, T., Guerra, G., and Syme, D. (2016). Types from data: Making structured data ﬁrst-class

citizens in F#. In PLDI 2016.

Qahtan, A. A., Elmagarmid, A., Castro Fernandez, R., Ouzzani, M., and Tang, N. (2018). FAHES:
A robust disguised missing values detector. In Proceedings of the 24th ACM SIGKDD, pages 2100–
2109. ACM.

Quinn, J. A., Williams, C. K. I., and McIntosh, N. (2009). Factorial switching linear dynamical
systems applied to physiological condition monitoring. IEEE Trans. Pattern Anal. Mach. Intell.,
31(9):1537–1551.

Rabin, M. O. (1963). Probabilistic automata. Information and Control, 6(3):230–245.

Rabin, M. O. and Scott, D. (1959). Finite automata and their decision problems. IBM Journal of

Research and Development, 3(2):114–125.

Raman, V. and Hellerstein, J. M. (2001). Potter’s wheel: An interactive data cleaning system. In

VLDB ’01, pages 381–390. Morgan Kaufmann Publishers Inc.

15

Stochastic Solutions (2018). Test-Driven Data Analysis. https://tdda.readthedocs.io/en/tdda-1.

0.23/constraints.html [Accessed on 08/04/2019].

Trifacta (2018). Trifacta Wrangler. https://www.trifacta.com/ [Accessed on 27/06/2018].

Valera, I. and Ghahramani, Z. (2017). Automatic discovery of the statistical types of variables in a

dataset. In Proceedings of the 34th ICML, volume 70, pages 3521–3529. PMLR.

Vergari, A., Molina, A., Peharz, R., Ghahramani, Z., Kersting, K., and Velera, I. (2019). Automatic

bayesian density analysis. In Proceedings of the 33rd AAAI.

Vidal, E., Thollard, F., de la Higuera, C., Casacuberta, F., and Carrasco, R. C. (2005). Probabilistic

ﬁnite-state machines - Part I. IEEE Trans. Pattern Anal. Mach. Intell., 27(7):1013–1025.

Wickham, H. and Grolemund, G. (2016). R for data science: Import, tidy, transform, visualize, and
model data, chapter 8, pages 137–138. O’Reilly Media, Inc., 1 edition. http://r4ds.had.co.nz/
data-import.html [Accessed on 24/07/2018].

Wickham, H., Hester, J., Francois, R., Jyl¨anki, J., and Jørgensen, M. (2017). readr 1.1.1. https:

//cran.r-project.org/web/packages/readr/readr.pdf [Accessed on 29/06/2018].

Williams, C. K. I. and Hinton, G. E. (1991). Mean ﬁeld networks that learn to discriminate temporally
distorted strings. In Proceedings of the 1990 Connectionist Models Summer School, pages 18–22.
Morgan Kaufmann Publishers, Inc.

Appendices

In this Appendices, we discuss the implementation of the Probabilistic Finite State Machines (PFSMs)
(Appendix A), describe the data sets used (Appendix B) and present the derivations for training
and inference in our model (Appendix C and D, respectively). Moreover, we present additional
experimental results (Appendix F) and report scalability of the methods (Appendix G).

A PFSMs for Data Types

In this work, we use ﬁve regular data types including integers, strings, ﬂoats, Booleans, and dates;
and two noisy data types, namely missing and anomaly.

A.1

Integers

Please see Sec. 2.1 for a detailed discussion of the PFSM used to represent integers.

A.2 Floats

A ﬂoating-point number often consists of digits and a full stop character, which is followed by another
set of digits. However, they can also be written without any fractional component, i.e. as integer
numbers. We also support the representations of ﬂoating-point numbers with e or E. Lastly, we
support the use of comma for the thousands separator in ﬂoating-point numbers, such as 1,233.15,
1,389,233.15, etc.

A.3 Strings

The string PFSM is constructed with one initial state and one ﬁnal state. Through each transition,
either a digit, an alpha character, or a punctuation character is emitted. The punctuation characters
considered here are ., ,, -,
, %, :, and ;, which are commonly found in real-world data sets to
represent columns with the string type.

16

A.4 Booleans

Our machine supports the following values by assigning them non-zero probabilities: Yes, No, True,
False, 1, 0, -1 and their variants yes, Y, y, no, true, false.

A.5 Dates

We categorize date formats into two groups, which are detailed below:

ISO-8601

We support values in YYYY-MM-DDTHH:MM::SS, where T is the time designator to indicate the
start of the representation of the time of day component. We also support other ISO-8601 formats
such as YYYYMMDD, YYYY-MM-DD, HH:MM, and HH:MM:SS.

Nonstandard Formats

We treat years in YYYY format as date type. To distinguish years and integers, we restrict this to
the range of [1000-2999]. On the other hand, we do not explicitly constrain the month (MM ) and day
columns (DD) to valid ranges, and but treat them as integers. We support ranges of years with the
formats of YYYY -YYYY, YYYY YYYY, YYYY - YYYY, YYYY -YYYY, and YYYY - YYYY.
Lastly, We support dates written as MM-DD-YYYY HH:MM:SS AM/PM, and months, e.g., January,
February, etc.

A.6 Missing

The machine for missing data assigns non-zero probabilities to the elements of this set, including Null,
NA and their variants such as NULL, null, “NA ”, NA, “N A”, N/A, “N/ A”, “N /A”, N/A, #NA, #N/A,
na, “ na”, “na ”, “n a”, n/a, N/O, NAN, NaN, nan, -NaN, and -nan; special characters such as -, !, ?,
*, and .; integers such as 0, -1, -9, -99, -999, -9999, and -99999; and characters denoting empty
cells such as “” and “ ”.

A.7 Anomaly

We use all of the Unicode characters in this machine’s alphabet, including the accented characters.
Note that the number of elements in this set is 1,114,112.

B Data Sets

We share the available data sets and the corresponding annotations at https://goo.gl/v298ER.
Here, we brieﬂy describe these data sets, and provide a list in Table 4 which denotes their sources,
and whether they are used in the training or testing phase.

• Accident 2016: information on accidents casualties across Calderdale, including location, number
of people and vehicles involved, road surface, weather conditions and severity of any casualties.

• Accidents 2015: a ﬁle from Road Safety data about the circumstances of personal injury road

accidents in GB from 1979.

• Adult: a data set extracted from the U.S. Census Bureau database to predict whether income

exceeds $50K/yr.

• Auto: a data set consisting of various characteristics of a car, its assigned insurance risk rating,

and its normalized losses in use.

• Broadband: annual survey of consumer broadband speeds in the UK.

17

• Billboard: a data set on weekly Hot 100 singles, where each row represents a song and the

corresponding position on that week’s chart.

• Boston Housing: a data which contains census tracts of Boston from the 1970 census.

• BRFSS: a subset of the 2009 survey from BRFSS, an ongoing data collection program designed

to measure behavioral risk factors for the adult population.

• Canberra Observations: weather and climate data of Canberra (Australia) in 2013.

• Casualties 2015: a ﬁle from Road Safety data about the consequential casualties.

• Census Income KDD: a data set that contains weighted census data extracted from the 1994

and 1995 current population surveys conducted by the U.S. Census Bureau.

• CleanEHR (Critical Care Health Informatics Collaborative): anonymised medical records 9.

• Cylinder Bands: a data set used in decision tree induction for mitigating process delays known

as “cylinder bands” in rotogravure printing.

• data.gov: 9 CSV ﬁles obtained from data.gov, presenting information such as the age-adjusted
death rates in the U.S., Average Daily Traﬃc counts, Web traﬃc statistics, the current mobile
licensed food vendors statistics in the City of Hartford, a history of all exhibitions held at San
Francisco International Airport by SFO Museum, etc.

• EDF Stocks: EDF stocks prices from 23/01/2017 to 10/02/2017.

• El Ni˜no: a data set containing oceanographic and surface meteorological readings taken from a

series of buoys positioned throughout the equatorial Paciﬁc.

• FACA Member List 2015: data on Federal Advisory Committee Act (FACA) Committee Member

Lists.

• French Fries: a data set collected from a sensory experiment conducted at Iowa State University

in 2004 to investigate the eﬀect of using three diﬀerent fryer oils on the taste of the fries.

• Fuel: Fuel Economy Guide data bases for 1985-1993 model.

• Geoplaces2: information about restaurants (from UCI ML Restaurant & consumer data).

• HES (Household Electricity Survey): time series measurements of the electricity use of domestic
appliances (to gain access to the data, please register at https://tinyurl.com/ybbqu3n3).

• Housing Price: a data set containing 79 explanatory variables that describe (almost) every aspect

of residential homes in Ames, Iowa.

• Inspection Outcomes: local authority childrens homes in England - inspection and outcomes as

at 30 September 2016.

• Intel Lab: a data set collected from sensors deployed in the Intel Berkeley Research lab, mea-
suring timestamped topology information, along with humidity, temperature, light and voltage.

• mass.gov: 4 CSV ﬁles obtained from mass.gov, which is the oﬃcial website of the Commonwealth

of Massachusetts.

• MINY Vendors: information on “made in New York” Vendors.

• National Characteristics:

information on the overall, authorised, unauthorised and persistent

absence rates by pupil characteristics.

9a subset is available at https://github.com/ropensci/cleanEHR/blob/master/data/sample_ccd.RData

18

• One Plus Sessions: information on the number of enrollments with one or more session of absence,

including by reason for absence.

• Pedestrian: a count data set collected in 2016, that denotes the number of pedestrians passing

within an hour.

• PHM Collection: information on the collection of Powerhouse Museum Sydney, including textual
descriptions, physical, temporal, and spatial data as well as, where possible, thumbnail images.

• Processed Cleveland: a data set concerning heart disease diagnosis, collected at Cleveland Clinic

Foundation (from the UCI ML Heart Disease Data Set).

• Sandy Related: Hurricane Sandy-related NYC 311 calls.

• Reported Taser 2015: a hand-compiled raw data set based on forms ﬁlled out by oﬃcers after
a stun gun was used in an incident, provided by CCSU’s Institute for Municipal and Regional
Policy.

• Rodents: the information collected on rodents during a survey.

• Survey: a data set from a 2014 survey that measures attitudes towards mental health and

frequency of mental health disorders in the tech workplace.

• TAO: a real-time data collected by the Tropical Atmosphere Ocean (TAO) project from moored
ocean buoys for improved detection, understanding and prediction of El Ni˜no and La Ni˜na.

• Tb: a tuberculosis dataset collected by the World Health Organisation which records the counts

of conﬁrmed tuberculosis cases by “country”, “year”, and demographic group.

• Tundra Traits: measurements of the physical characteristics of shrubs in the arctic tundra.

• T2Dv2 Gold Standard: a set of data Web tables to evaluate matching systems on the task of

matching Web tables to the DBpedia knowledge base.

• User Proﬁle: information about consumers (from UCI ML Restaurant & consumer data).

• Vehicles 2015: a ﬁle from Road Safety data about the types of vehicles involved in the accidents.

• 83492acc-1aa2-4e80-ad05-28741e06e530: a hypoparsr data set which contains information on

expenses.

Note that the data sets from mass.gov and data.gov are obtained from Abdulhakim A. Qahtan and
also used in [16].

19

name

source

training/test # columns # rows

Accidents 2015
Accident 2016
Adult
Auto
Broadband
Billboard
Boston Housing
Brfss
Canberra Observations
Casualties 2015
Census Income KDD
CleanEHR
Cylinder Bands
EDF Stocks
Elnino
FACAMemberList2015
French Fries
Fuel
Geoplaces2
HES
Housing Price
Intel Lab
Inspection Outcomes
MINY Vendor
Pedestrian
Phm
Processed Cleveland
Rodents
Sandy Related
SFR55 2017 national characteristics
SFR55 2017 one plus sessions
Survey
Tao
Tb
TundraTraits
User Proﬁle
Vehicles 2015
4 csv ﬁles
9 csv ﬁles
2015ReportedTaserData
16 csv ﬁles
83492acc-1aa2-4e80-ad05-28741e06e530.csv github.com

data.gov.uk
data.gov.uk
UCI ML
UCI ML
data.gov.uk
github.com
Kaggle
github.com
others
data.gov.uk
UCI ML
others
UCI ML
github.com
UCI ML
github.com
github.com
github.com
UCI ML
ukdataservice.ac.uk
Kaggle
others
others
data.gov
others
others
UCI ML
others
NYC OpenData
gov.uk
gov.uk
others
github.com
github.com
github.com
UCI ML
data.gov.uk
mass.gov
data.gov
github.com
T2Dv2 Gold Standard test

test
test
training
test
training
training
training
training
training
test
test
training
training
test
test
training
training
training
training
training
training
test
test
test
training
training
training
training
training
test
test
test
training
training
training
training
test
test
test
training

training

32
18
15
26
55
72
15
34
13
16
42
62
40
7
9
21
10
35
20
65
81
8
22
18
9
16
14
39
38
41
31
27
7
23
17
19
23
27 (avg.)
14 (avg.)
69
5 (avg.)
15

140,056
555
32,561
205
2,732
317
333
245
19,918
186,189
199,523
1,979
540
5,425
782
72,220
696
941
130
4,600
1,460
1,048,576
1,477
897
37,700
75,814
303
35,549
87,444
735
228,282
1,259
736
5,769
73,428
138
257,845
46,934 (avg.)
3904 (avg.)
610
127 (avg.)
886

Table 4: Information about the data sets used.

20

Figure 6: Fraction of the non-type entries in a dataset, calculated by aggregating over its columns.
Note that ‘overall’ denotes the fraction after aggregating over the datasets.

C Derivations for the Training

The task is to update the parameters of the PFSMs, given a set of columns X and their column types
t. Since the columns are assumed to be independent, the gradient can be calculated by summing the
gradient of each column. In the interest of simplicity, here we only derive the gradient for a column
x of type k. We would like to maximize the posterior probability of the correct type k given a data
column x, which can be rewritten as follows:

log p(t = k|x) = log p(t = k, x)
(cid:125)

(cid:124)

(cid:123)(cid:122)
Lc

.

− log p(x)
(cid:124) (cid:123)(cid:122) (cid:125)
Lf

(3)

We now present the derivations of the gradients w.r.t. the transition parameters where θτ
q,α,q(cid:48) denotes
the transition parameter from state q to q(cid:48) emitting the symbol α in the τ th PFSM. Note that τ ∈
{1, . . . , K} where K is the number of PFSMs.

We now diﬀerentiate these two terms, in section (4.1) and (4.2) respectively.

21

accidents_2015accident2016autocasualties_2015census_income_kddedf_stockselninoinspection_outcomesintel_labminy_vendorschool_characteristicsschool_sessionssurveyvehicles_2015data_gov_323_1data_gov_356_1data_gov_3397_1data_gov_5134_1data_gov_10012_1data_gov_10151_1data_gov_12252_1data_gov_16834_1data_gov_18386_1mass_1mass_2mass_5mass_621329809_0_5526..24036779_0_5608..24142265_0_4577..26270372_1_3774..28086084_0_3127..28154036_0_9090..28646774_0_3256..29886325_0_1448..34899692_0_6530..40534006_0_4617..41480166_0_6681..44005578_0_5400..44206774_0_3810..47709681_0_4437..78891639_0_3299..8468806_0_43824..Dataset Name0.00.20.40.60.81.0FractionFraction of the Non-type EntriesC.1 Derivative of Lc

∂ log p(t = k, x)

,

q,α,q(cid:48)

∂θτ
log p(t = k) (cid:81)N

(cid:17)
i=1 p(xi|t = k)

,

∂θτ

q,α,q(cid:48)

∂Lc

∂θτ

q,α,q(cid:48)

=

=

=

=

=

=

=

=

=

=

(cid:16)

∂

(cid:16)

∂

N
(cid:88)

i=1
N
(cid:88)

i=1
N
(cid:88)

i=1
N
(cid:88)

i=1

N
(cid:88)

i=1

N
(cid:88)

i=1
N
(cid:88)

i=1

log p(t = k) + (cid:80)N

(cid:17)
i=1 log p(xi|t = k)

∂θτ

q,α,q(cid:48)

∂ log p(xi|t = k)

∂θτ

q,α,q(cid:48)

,

,

1
p(xi|t = k)

1
p(xi|t = k)

∂p(xi|t = k)

,

q,α,q(cid:48)

∂θτ
∂(cid:0) (cid:80)

z(cid:48) p(zi = z(cid:48), xi|t = k)(cid:1)

∂θτ

q,α,q(cid:48)

,

1
p(xi|t = k)

(cid:80)

z(cid:48) ∂p(zi = z(cid:48), xi|t = k)
q,α,q(cid:48)

∂θτ

,

1
p(xi|t = k)

1
p(xi|t = k)

(cid:80)

z(cid:48) ∂

(cid:16)

(cid:17)
p(zi = z(cid:48)|t = k)p(xi|zi = z(cid:48))

∂θτ

q,α,q(cid:48)

,

(cid:16)

∂

k p(xi|zi = k) + πm
πk

k p(xi|zi = m) + πa

kp(xi|zi = a)

∂θτ

q,α,q(cid:48)

1
p(xi|t = k)

πk
k ∂p(xi|zi = k)
q,α,q(cid:48)

∂θτ

.

(cid:17)

,

(4)

When τ is not equal to k, eq. (2) becomes 0. On the other hand, if τ = k, then we would need to
calculate ∂p(xi|zi=τ )
p(xi, q0:L|zi = τ ) as xi is generated
by a PFSM. Note that q0:L denotes the states visited to generate xi. The derivative can be derived as

, where p(xi|zi = τ ) can be rewritten as (cid:80)

q,α,q(cid:48)

q0:L

∂θτ

22

follows:

∂p(xi|zi = τ )

∂θτ

q,α,q(cid:48)

=

=

=

=

=

=

=

=

=

=

=

(cid:16) (cid:80)

q0:L

∂

(cid:17)

p(xi, q0:L|zi = τ )
∂θτ

q,α,q(cid:48)

,

(cid:88)

∂p(xi, q0:L|zi = τ )

,

∂θτ

q,α,q(cid:48)

q0:L
(cid:88)

q0:L

(cid:88)

q0:L

(cid:88)

q0:L

(cid:88)

q0:L

(cid:88)

q0:L

p(xi, q0:L|zi = τ )

∂ log p(xi, q0:L|zi = τ )

∂θτ

q,α,q(cid:48)

,

p(xi, q0:L|zi = τ )

∂ log

(cid:104)
I τ (q0)

(cid:16) (cid:81)L−1

l=0 T τ (ql, xl

(cid:17)
i, ql+1)

(cid:105)
F τ (qL)

,

(cid:16)

∂ (cid:80)L−1
l=0

p(xi, q0:L|zi = τ )

∂θτ

q,α,q(cid:48)

i, ql+1)

log T τ (ql, xl
∂θτ

q,α,q(cid:48)

(cid:17)

,

p(xi, q0:L|zi = τ )

p(xi, q0:L|zi = τ )

L−1
(cid:88)

l=0

L−1
(cid:88)

l=0

∂ log T τ (ql, xl
∂θτ

q,α,q(cid:48)

i, ql+1)

,

δ(ql, q)δ(xl

i, α)δ(ql+1, q(cid:48))
i, ql+1)

,

T τ (ql, xl

(cid:88)

L−1
(cid:88)

q0:L

l=0

(cid:88)

L−1
(cid:88)

q0:L
L−1
(cid:88)

l=0

(cid:88)

l=0

q0:L

p(xi, q0:L|zi = τ )

(cid:16) δ(ql, q)δ(xl

i, α)δ(ql+1, q(cid:48))
i, ql+1)

(cid:17)

,

T τ (ql, xl

p(ql = q, ql+1 = q(cid:48), q0:L\l,l+1, xi|zi = τ )

(cid:16) δ(xl

i, α)

T τ (q, xl

i, q(cid:48))

p(ql = q, ql+1 = q(cid:48), q0:L\l,l+1, xi|zi = τ )

(cid:16) δ(xl

i, α)

T τ (q, xl

i, q(cid:48))

(cid:17)

,

(cid:17)

,

L−1
(cid:88)

l=0

δ(xl

i, α)p(ql = q, ql+1 = q(cid:48), xi|zi = τ )

T τ (q, xl

i, q(cid:48))

.

(5)

Hence, we need to evaluate the joint probability p(ql = q, ql+1 = q(cid:48), xi|zi = τ ) for each l where xl
which can be found by marginalizing out the variables q0:L\{l,l+1}:

i = α,

p(ql = q, ql+1 = q(cid:48), xi|zi = τ ) =

(cid:88)

ql(cid:48)

p(ql = q, ql+1 = q(cid:48), ql(cid:48), xi|zi = τ ),

(6)

where l(cid:48) denotes {0 : L}\{l, l+1}. This can be calculated iteratively via Forward-Backward Algorithm
where the forward and backward messages are deﬁned iteratively as follows:

vl→l+1(ql) =

λl+1→l(ql+1) =

(cid:88)

ql−1
(cid:88)

ql+2

T τ (ql−1, xl

i, ql)vl−1→l(ql−1),

T τ (ql+1, xl+2

i

, ql+2)λl+2→l+1(ql+2),

We can then rewrite p(ql = q, ql+1 = q(cid:48), xi|zi = τ ) as follows:

p(ql, ql+1, xi|zi = τ ) = (vl→l+1(ql) • λl+1→l(ql+1)) (cid:12) T τ (ql, xl+1

i

, ql+1),

where • and (cid:12) denote respectively outer and element-wise product.

(7)

(8)

23

C.2 Derivative of Lf

Let us now take the derivative of the second term Lf :

,

∂ log p(x)
∂θτ
∂ (cid:80)N

q,α,q(cid:48)
i=1 log p(xi)
∂θτ
q,α,q(cid:48)

,

∂ log p(xi)

,

∂θτ

q,α,q(cid:48)
∂(cid:0) (cid:80)

∂Lf

∂θτ

q,α,q(cid:48)

=

=

=

=

=

=

=

=

=

N
(cid:88)

i=1
N
(cid:88)

i=1
N
(cid:88)

i=1
N
(cid:88)

i=1
N
(cid:88)

i=1
N
(cid:88)

i=1
N
(cid:88)

i=1

1
p(xi)

1
p(xi)

1
p(xi)

1
p(xi)

1
p(xi)

(cid:80)

z(cid:48) p(t = t(cid:48), zi = z(cid:48), xi)(cid:1)

t(cid:48)

∂θτ

q,α,q(cid:48)

,

∂(cid:0) (cid:80)

z(cid:48) p(t = τ, zi = z(cid:48), xi)(cid:1)

∂θτ

q,α,q(cid:48)

,

∂(cid:0) (cid:80)

z(cid:48) p(t = τ )p(zi = z(cid:48)|t = τ )p(xi|zi = z(cid:48))(cid:1)

∂θτ

q,α,q(cid:48)

,

p(t = τ )∂(cid:0)πτ

τ p(xi|zi = τ ) + πm
∂θτ

q,α,q(cid:48)

τ p(xi|zi = m) + πa

τ p(xi|zi = a)(cid:1)

p(t = τ )πτ

τ ∂p(xi|zi = τ )
∂θτ

q,α,q(cid:48)

,

p(t = τ )πτ
τ
p(xi)

∂p(xi|zi = τ )

∂θτ

q,α,q(cid:48)

.

,

(9)

Let us now put all the equations together. When we are calculating the derivative of eq. (1) w.r.t.

the correct machine, i.e. τ = k, the derivative becomes the following:

∂ log p(t = k|x)

∂θτ

q,α,q(cid:48)

=

=

=

=

i=1
N
(cid:88)

i=1
N
(cid:88)

i=1
N
(cid:88)

i=1

N
(cid:88)

(cid:16)

πk
k
p(xi|t = k)

∂p(xi|zi = k)

∂θk

q,α,q(cid:48)

−

πk
k p(t = k)
p(xi)

∂p(xi|zi = k)

(cid:17)

∂θk

q,α,q(cid:48)

,

(cid:16)

πk
k

∂p(xi|zi = k)

(cid:16)

∂θk

q,α,q(cid:48)

1
p(xi|t = k)

−

p(t = k)
p(xi)

(cid:17)(cid:17)

,

πk
k
p(xi|t = k)

πk
k
p(xi|t = k)

∂p(xi|zi = k)

(cid:16)

∂θk

q,α,q(cid:48)

1 −

p(t = k)p(xi|t = k)
p(xi)

(cid:17)

,

∂p(xi|zi = k)

(cid:16)

∂θk

q,α,q(cid:48)

1 −

(cid:80)

p(t = k, xi)
k(cid:48) p(t = k(cid:48), xi)

(cid:17)

.

(10)

When we are calculating the derivative of eq. (1) w.r.t. the wrong machines, i.e. τ (cid:54)= k this becomes:

∂ log p(t = k|x)

∂θτ

q,α,q(cid:48)

= −

N
(cid:88)

i=1

(cid:16) πτ

τ p(t = τ )
p(xi)

∂p(xi|zi = τ )

(cid:17)

∂θτ

q,α,q(cid:48)

.

(11)

Lastly, we ensure the parameters remain positive and normalized using the softmax function. We

deﬁne Tτ (q, α, q(cid:48)) = exp T z
We now update these new unconstrained parameters using the new gradient calculated via the chain
rule: ∂f /∂T z

τ (q, α, q(cid:48)) = (∂f /∂Tτ (q, α, q(cid:48)))(∂Tτ (q, α, q(cid:48))/∂T z

τ (q, α(cid:48), q(cid:48)(cid:48))) and I z

τ (q, α, q(cid:48))/(exp F z

τ (q) = exp I z

α(cid:48),q(cid:48)(cid:48) exp T z

τ (q, α, q(cid:48))).

τ (q)+(cid:80)

τ (q)/ (cid:80)

q(cid:48) exp I z

τ (q(cid:48)).

24

D Derivations for Inference

The posterior distribution of column type t can be derived as follows:

p(t = k|x) ∝ p(t = k, x),
N
(cid:89)

= p(t = k)

p(xi|t = k),

= p(t = k)

i=1
N
(cid:89)

i=1

(cid:16)

k p(xi|zi = k) + πm
πk

k p(xi|zi = m) + πa

(cid:17)
kp(xi|zi = a)

.

Let us assume that t = k according to p(t|x), the posterior distribution of column type. Then we can
write the posterior distribution of row type zi given t = k and x as:

p(zi = j|t = k, x) =

=

,

p(zi = j, xi|t = k)
p(xi|t = k)
p(zi = j, xi|t = k)
zi∈{k,m,a} p(zi, xi|t = k)

(cid:80)

.

(12)

E The Outputs of the PADS Library

We have mentioned previously that the outputs generated by the PADS library do not directly address
our problem. We present a sample from an example test dataset in Table 5, and a part of the
corresponding output of the PADS library.

year

winner/2nd

NULL scores

total money (us$)

1998
fred couples
&nbsp; bruce lietzke
john cook
1997
&nbsp; mark calcavecchia
mark brooks
1996
john huston
&nbsp;
1995
kenny perry
&nbsp; david duval
1994
&nbsp;
&nbsp;
&nbsp;
1993

scott hoch
fuzzy zoeller
lennie clements
jim gallagher jr.
tom kite

1
2
1
2
1
2
1
2
1
t2
t2
t2
1

64-70-66-66-66
65-65-71-62-69
66-69-67-62-63
64-67-66-64-67
66-68-69-67-67
69-71-65-65-68
63-71-64-67-70
67-68-65-67-69
66-62-70-66-70
70-67-66-68-66
67-69-61-72-68
66-67-74-62-68
67-67-64-65-62

332
332
327
328
337
338
335
336
334
337
337
337
325

414000
248400
270000
162000
234000
140400
216000
129600
198000
82133.34
82133.33
82133.33
198000

Table 5: A sample test dataset.

The outputs are interpreted starting from the bottom. In this case, the data is deﬁned as an array
of “struct” type named Struct 194. This is further characterized as a combination of various “union”
types. For example, let us consider the ﬁrst one named Union 19 which consists of a constant string
&nbsp;, another constant string year, and and integer type. However, this can be more complicated
as in type Union 165 consisting of two struct types Struct 192 and Struct 164. Note that the former
is further divided into a union type, whereas the latter is described as a combination of some constant
strings and a ﬂoat type. As the reader can see, it can become diﬃcult and time-consuming to interpret
an output. Moreover, the output becomes more complex when delimiters are inferred correctly, as
this can prevent the types from column speciﬁc.

25

#include ”vanilla.p”
Punion Union 19 {

v stringconst 12 Pfrom(”&nbsp;”);
”year”;
Puint16 v intrange 4;

};
Pstruct Struct 76 {

”\”winner/”;
Puint8 v intconst 70 : v intconst 70 == 2;
”nd\””;

};
.
.
.
Punion Union 189 {

v stringconst 173 Pfrom(”money (us$)”);
Puint32 v intconst 169 : v intconst 169 == 72600;

};
Pstruct Struct 192 {
’\”’;
Union 189 v union 189;
’\”’;

};
Pstruct Struct 164 {
’\”’;
Pﬂoat64 v ﬂoat 156;
’\”’;

};
Punion Union 165 {

Struct 192 v struct 192;
Struct 164 v struct 164;

};
Precord Pstruct Struct 194 {

’\”’;
Union 19 v union 19;
”\”,”;
Union 62 v union 62;
”,\””;
Union 86 v union 86;
”\”,”;
Union 121 v union 121;
’,’;
Union 141 v union 141;
’,’;
Union 165 v union 165;

};
Psource Parray entries t {

Struct 194[];

};

Figure 7: A fragment of the PADS output for a given dataset.

26

F Additional Experimental Results

Table 6 presents the comparisons with hypoparsr.

Method

F# hypoparsr messytables

readr TDDA Trifacta ptype-hc ptype

Overall
Accuracy
Date
Logical
Numeric
Text

0.65

0.66

0.31
0.27
0.43
0.56

0.31
0.00
0.52
0.54

0.65

0.17
0.29
0.43
0.57

0.63

0.60

0.07
0.00
0.45
0.55

0.00
0.00
0.39
0.52

0.88

0.62
0.14
0.88
0.80

0.96

0.66
0.88
0.94
0.94

0.95

0.66
0.88
0.93
0.93

Table 6: Performance of the methods using the Jaccard index and overall accuracy, for the types Date,
Logical, Numeric and Text.

(a) F#

(b) messytables

(c) ptype

(d) readr

(e) Trifacta

(f) hypoparsr

Figure 8: Normalized confusion matrices for (a) F#, (b) messytables, (c) ptype, (d) reader, (e)
Trifacta, and (f) hypoparsr plotted as Hinton diagrams, where the area of a square is proportional to
the magnitude of the entry.

Figure 8 presents the normalized confusion matrices for the methods, discussed in the paper.

27

G Scalability of the Methods

Table 7 denotes the number of rows, columns, unique elements, and the time passed to infer column
types.

dataset

# cols # rows

U

hypoparsr messytables ptype

readr TDDA Trifacta

21329809 0 . . .
24036779 0 . . .
24142265 0 . . .
26270372 1 . . .
28086084 0 . . .
28154036 0 . . .
28646774 0 . . .
29886325 0 . . .
34899692 0 . . .
40534006 0 . . .
41480166 0 . . .
44005578 0 . . .
44206774 0 . . .
47709681 0 . . .
78891639 0 . . .
8468806 0 . . .
accident2016
accidents 2015
auto
casualties 2015
census inc. . .
data gov 10012 1
data gov 10151 1
data gov 12252 1
data gov 16834 1
data gov 18386 1
data gov 323 1
data gov 3397 1
data gov 356 1
data gov 5134 1
edf stocks
elnino
inspection ou. . .
intel lab
mass 1
mass 2
mass 5
mass 6
miny vendor
school char. . .
school sessions
survey
vehicles 2015

6
7
6
5
6
5
6
6
4
4
6
4
6
4
6
7
18
32
26
16
42
14
21
5
24
16
6
18
8
10
7
9
22
8
12
19
53
23
18
41
31
27
23

156
83
100
19
224
10
8
254
92
39
224
8
89
408
202
110
555
140056
205
186189
199523
145
99
258
15055
1238
13260
437
1279
3366
5425
782
1477
1048576
131316
44990
8282
3148
897
735
228282
1259
257845

699
490
271
71
229
39
45
1063
321
118
229
31
279
706
862
489
1835
609343
911
140158
102028
779
775
756
2911
3975
11740
534
6993
12198
4702
1032
3115
112912
803207
12452
72659
3363
6728
16832
78913
1622
141278

2.595
4.836
2.345
1.007
1.475
2.199
1.613
3.572
0.978
2.667
1.343
2.014
2.505
3.432
2.927
3.654
10.022
-
3.557
-
-
4.023
2.227
2.734
-
17.705
94.726
5.785
25.147
293.392
23.288
12.59
12.349
-
-
-
-
36.681
7.148
3.873
-
7.036
-

0.001
0.001
0.001
0.001
0.002
0.001
0.001
0.002
0.001
0.001
0.001
0.001
0.031
0.006
0.002
0.001
0.005
0.895
0.001
1.191
0.853
0.003
0.002
0.002
0.056
0.007
0.065
0.002
0.008
0.023
0.049
0.005
0.007
8.956
0.703
0.275
0.029
0.012
0.006
0.005
1.010
0.008
1.641

0.011
0.011
0.007
0.004
0.006
0.003
0.004
0.015
0.009
0.004
0.005
0.003
0.006
0.021
0.013
0.007
0.009
1.635
0.011
0.858
0.423
0.012
0.006
0.015
0.035
0.024
0.157
0.005
0.068
0.110
0.063
0.012
0.014
2.183
12.502
0.137
0.115
0.018
0.030
0.034
0.514
0.015
0.710

0.010
0.015
0.002
0.002
0.002
0.004
0.002
0.014
0.001
0.002
0.002
0.001
0.002
0.002
0.002
0.002
0.006
0.045
0.006
0.030
0.054
0.011
0.002
0.002
0.005
0.002
0.006
0.002
0.003
0.007
0.005
0.002
0.002
0.266
0.150
0.015
0.009
0.002
0.002
0.001
0.039
0.001
0.034

0.0001
0.0001
0.0001
0.0001
0.0001
0.0001
0.0001
0.0001
0.00004
0.00004
0.0001
0.00004
0.0001
0.00004
0.0001
0.0001
0.0001
0.0003
0.0002
0.0002
0.0004
0.0002
0.0002
0.00005
0.0002
0.0001
0.00005
0.0002
0.0001
0.0001
0.0001
0.0001
0.0002
0.0001
0.0001
0.0002
0.001
0.0002
0.0002
0.0003
0.0003
0.0003
0.0002

1.333
1.143
1.333
2.200
1.333
1.400
1.333
1.667
2.250
2.000
1.500
2.000
1.500
2.250
1.333
1.143
0.611
0.281
0.308
0.625
0.238
0.571
0.429
1.600
0.333
0.500
1.500
0.500
1.000
0.700
1.143
1.220
0.364
1.250
0.750
0.474
0.189
0.348
0.500
0.220
0.290
0.296
0.391

Table 7: Size of the test datasets and the times in seconds it takes to infer column types per column
(on average), where U denotes the number of unique data entries in a dataset.

28

