2
2
0
2

r
a

M
2
2

]

Y
S
.
s
s
e
e
[

1
v
2
1
8
1
1
.
3
0
2
2
:
v
i
X
r
a

Neural System Level Synthesis: Learning over All Stabilizing Policies for
Nonlinear Systems

Luca Furieri, Clara Luc´ıa Galimberti, and Giancarlo Ferrari-Trecate

Abstract— We address the problem of designing stabilizing
control policies for nonlinear systems in discrete-time, while
minimizing an arbitrary cost function. When the system is
linear and the cost is convex, the System Level Synthesis (SLS)
approach offers an exact solution based on convex programming.
Beyond this case, a globally optimal solution cannot be found
in a tractable way,
in general. In this paper, we develop a
parametrization of all and only the control policies stabilizing
a given time-varying nonlinear system in terms of the combined
effect of 1) a strongly stabilizing base controller and 2) a stable
SLS operator to be freely designed. Based on this result, we
propose a Neural SLS (Neur-SLS) approach guaranteeing closed-
loop stability during and after parameter optimization, without
requiring any constraints to be satisﬁed. We exploit recent Deep
Neural Network (DNN) models based on Recurrent Equilibrium
Networks (RENs) to learn over a rich class of nonlinear stable
operators, and demonstrate the effectiveness of the proposed
approach in numerical examples.

I. INTRODUCTION

The optimal control of nonlinear systems subject to global
asymptotic stability guarantees is one of the most important
and challenging problems in control theory. Despite a rich
body of research in nonlinear control methods [1], the general
Nonlinear Optimal Control (NOC) problem is well under-
stood only when the system dynamics are linear. Classical
approaches for dealing with continuous-time and discrete-
time NOC include dynamic programming and the associated
Bellman optimality principle, and the maximum principle
for deterministic problems [2], [3]. Unfortunately, application
of these methods to general NOC problems is hindered by
methodological and computational challenges [3].

An alternative approach is provided by receding-horizon
control schemes such as Nonlinear Model Predictive Con-
trol (NMPC) [4]. The NMPC strategy is based on real-time
optimization; a ﬁnite-horizon open-loop control problem is
solved at each time instant for deﬁning the control input at the
same time. One of the main drawbacks of NMPC is that the
resulting control policy is not available ofﬂine in explicit form.
Furthermore, several applications may not allow for sufﬁcient
computational resources to solve a mathematical program in
real-time.

More recently, Reinforcement Learning (RL) and Deep
Neural Networks (DNN) have emerged as powerful
tools
helping agents learn how to make sense of and optimally
interact with complex environments [5]. Despite the success of
DNN control policies for ﬁnite-horizon control tasks [6], [7],
RL techniques cannot provide closed-loop stability guarantees
in general. As a result, their applicability is limited to systems
that are not safety-critical.

Several methods to learn provably stabilizing DNN con-
trol policies have been proposed. We divide them into two
categories. First, constrained optimization approaches [8]–
[10] ensure global or local stability by enforcing appropri-
ate Lyapunov-like inequalities during optimization. However,
optimizing over conservative stability constraints may exces-
sively limit the set of admissible policies. Further, enforcing
constraints such as Linear Matrix Inequalities (LMIs) becomes
a computational bottleneck in large-scale applications. Second,
unconstrained optimization approaches seek to characterize
classes of control policies with built-in stability guarantees
[11], [12]. The unconstrained approach allows one to learn
over real-valued parameters through unconstrained gradient
descent, without compromising stability for arbitrarily chosen
parameter values and whilst enjoying a low computational
burden. The main challenge is to characterize stabilizing
policy classes that are broad enough to achieve optimized
performance. An original perspective on parametrizing sta-
bilizing control policies is based on the nonlinear System
Level Synthesis (SLS) [13], which proposes to directly learn
over stable nonlinear closed-loop maps — rather than over
stabilizing nonlinear control policies. Unfortunately, nonlin-
ear functional equality constraints must be satisﬁed during
optimization [13]. This aspect poses obstacles to practical
implementation beyond the well-known linear case [14].

A. Contributions

Motivated by the SLS framework, we establish an uncon-
strained parametrization of all stabilizing nonlinear control
policies for nonlinear time-varying systems in discrete-time.
Our key ﬁnding is that the intractable constraints of nonlinear
SLS [13] need not be imposed during optimization,
if a
stabilizing base control policy is known, and if this base
control policy deﬁnes itself a stable map from its inputs to
its outputs. In this case, all and only the other stabilizing
policies are parametrized in terms of a freely chosen stable
SLS operator. A main practical beneﬁt is that, no matter how
suboptimally this stable SLS operator may be chosen, closed-
loop stability is guaranteed by design. Our result extends
nonlinear variants of the Youla parametrization [15], [16] to
discrete-time system. Further, we recover the recent result of
[11] as a subcase, where the authors address the NOC problem
for linear systems with nonlinear costs. Last, we propose
using deep RENs [17] as a ﬁnite-dimensional approximation
of nonlinear stable operators, thus extending the well-known
idea of Finite Impulse Responses (FIRs) approximations [14]
which is valid for linear SLS only. The resulting Neur-SLS
optimal control method is tested on formation control prob-

 
 
 
 
 
 
lems involving vehicles with nonlinear dynamics and nonlinear
costs with collision avoidance terms.

The paper is structured as follows. Section II presents
the NOC problem under study, and reviews the nonlinear
SLS parametrization of stabilizing control policies. Section III
presents our main theoretical results. Section IV introduces
deep RENs as an unconstrained ﬁnite-dimensional approxima-
tion of nonlinear stable operators and formulates the uncon-
strained Neur-SLS learning problem. In Section V the Neur-
SLS method is tested through numerical examples.

1

P

∞
t=0 |xt|p)

p ⊂ ℓn with p ∈ N if kxkp = (

Notation: The set of all sequences x = (x0, x1, x2, . . .),
where xt ∈ Rn for all t ∈ N and N is the set of
is denoted as ℓn. Moreover, x belongs
natural numbers,
to ℓn
p < ∞,
where | · | denotes any vector norm. We say that x ∈ ℓn
∞
if supt |xt| < ∞. We use the notation xj:i
to refer
to the truncation of x to the ﬁnite-dimensional vector
(xi, xi+1, . . . , xj). An operator A : ℓn → ℓm is said to be
if A(x) = (A0(x0), A1(x1:0), . . . , At(xt:0), . . .).
causal
then A is
If
causal. Similarly, Aj:i(xj:0) =
said
strictly
(Ai(xi:0), Ai+1(xi+1:0), . . . , Aj(xj:0)).
matrix
M ∈ Rm×n, M x = (M x0, M x1, . . .) ∈ ℓm. An operator
A : ℓn → ℓm is said to be ℓp-stable if it is causal and
A(a) ∈ ℓm

in addition At(xt:0) = At(0, xt−1:0),

p . Equivalently, we write A ∈ Lp.

p for all a ∈ ℓn

For

be

to

a

II. PROBLEM STATEMENT AND NONLINEAR SYSTEM
LEVEL SYNTHESIS

We consider nonlinear discrete-time time-varying systems

xt = ft(xt−1:0, ut−1:0) + wt ,

t = 1, 2, . . . ,

(1)

where xt ∈ Rn denotes the state vector, ut ∈ Rm denotes the
control input and wt ∈ Rn stands for unknown process noise
with w0 = x0. In operator form, system (1) is equivalent to

x = F(x, u) + w ,

(2)

where F : ℓn×ℓm → ℓn is the strictly causal operator such that
F(x, u) = (0, f1(x0, u0), . . . , ft(xt−1:0, ut−1:0), . . .). Further,
is distributed
we assume that wt ∼ Dt(µt, Σt),
according to an unknown distribution Dt with expected value
µt and covariance matrix Σt ≻ 0, and that w belongs to ℓn
p .
In order to control the behavior of system (1) we consider

i.e., wt

nonlinear, dynamic, time-varying feedback control policies

u = K(x) = (K0(x0), K1(x1:0), . . . , Kt(xt:0), . . .) ,

(3)

where K is a causal operator K : ℓn → ℓm to be designed. It
is easy to see that for each sequence of disturbances w ∈ ℓn
p
the closed-loop system (1)-(3) produces unique trajectories.
Hence, the closed-loop mappings w (x, u) are well-deﬁned.
Speciﬁcally, for a system F and a controller K, we denote the
corresponding induced closed-loop maps w x and w u
as Φx[F, K] and Φu[F, K], respectively. Therefore, we have
x = Φx[F, K](w) and u = Φu[F, K](w) for all w ∈ ℓn
p .

Our goal is to synthesize a control policy K that complies

with two requirements:

(R1) The closed-loop maps Φx[F, K] and Φu[F, K] are ℓp-

stable.

(R2) A loss function

T

J = EwT :0

l(xt, ut)

,

(4)

t=0
X
is globally minimized, where l is piecewise differentiable
and l(x, u) ≥ 0 for all (x, u) ∈ Rm+n.

"

#

Requirement (R1) is a hard constraint to be satisﬁed for all the
policies we optimize over. In other words, we require fail-safe
learning, in the sense that closed-loop stability must be guaran-
teed during and after optimization of the parameters describing
the policies. Instead, we treat (R2) as a “soft” requirement. We
do not expect that a gradient-descent-based algorithm ﬁnds
the globally optimal solution for any initialization — this is
generally impossible for problems beyond Linear Quadratic
Gaussian (LQG) control, which enjoy convexity of the cost and
linearity of the optimal policies [18], [19]. Furthermore, the
expected value can only be computed empirically because the
distribution of w is unknown. Last, we note that the theoretical
results of Section III also apply to convergent inﬁnite-horizon
costs [2]; we focus on the ﬁnite-horizon cost (4) to enable the
DNN implementation of Section IV.

We are ready to formulate the NOC problem as

T

NOC: min
K(·)

EwT :0

l(xt, ut)

#

"

t=0
X

s. t. xt = ft(xt−1:0, ut−1:0) + wt , w0 = x0 ,

ut = Kt(xt:0) , ∀t = 0, 1, . . . ,
(Φx

[F, K], Φu

[F, K]) ∈ Lp .

Searching over the space of stabilizing control policies leads to
intractable optimization problems in general. The idea behind
the SLS approach [13], [14] is to circumvent the difﬁculty
of characterizing stabilizing controllers, by instead directly
designing stable closed-loop maps. Let us deﬁne the set of
all achievable closed-loop maps for system F as

CL[F] = {(Φx

[F, K], Φu

[F, K])| K is causal} ,

(5)

and the set of all achievable and stable closed-loop maps as

CLp[F] = {(Ψx

, Ψu

) ∈ CL[F]| (Ψx

, Ψu

) ∈ Lp} .

(6)

Note that, if (Ψx, Ψu) ∈ CLp[F], then x = Ψx(w) ∈ ℓn
p and
u = Ψu(w) ∈ ℓm
p . Based on Theorem III.3
of [13], and adding the requirement that the closed-loop maps
must belong to Lp, we summarize the main SLS result for
nonlinear discrete-time systems.

p for all w ∈ ℓn

Theorem 1 (Nonlinear SLS parametrization [13]):
1) The set CLp[F] of all achievable and stable closed-loop

responses admits the following characterization:

CLp[F] = {(Ψx
Ψx
(Ψx

, Ψu
= F(Ψx
, Ψu

)| (Ψx
, Ψu
) ∈ Lp} .

, Ψu
) + I ,

) are causal ,

(7)

(8)
(9)

2) For any (Ψx, Ψu) ∈ CLp[F], the causal controller

u = K(x) = Ψu

((Ψx

)−1(x)) ,

(10)

is unique, and it produces the stable closed-loop re-
sponses (Ψx, Ψu).

Theorem 1 clariﬁes that any policy K(x) achieving ℓp-
stable closed-loop maps can be described in terms of two
operators (Ψx, Ψu) ∈ Lp complying with the nonlinear
functional equality (8). Therefore, the NOC problem admits
an equivalent Nonlinear SLS (N-SLS) formulation:

T

N-SLS:

min
(Ψx,Ψu)

EwT :0

l(xt, ut)

(11)

s. t.

"
t=0
X
t (wt:0) ,

#
ut = Ψu
t (wt:0) ,
) ∈ CLp[F] , ∀t = 0, 1, . . .

xt = Ψx
, Ψu
(Ψx
According to Theorem 1, the constraint (Ψx, Ψu) ∈ CLp[F]
is equivalent to requiring that (Ψx, Ψu) comply with both
(8) and (9), which are challenging to satisfy simultaneously.
The constraint (8) simply deﬁnes the operator Ψx in terms
of Ψu and can be computed explicitly because F is strictly
causal. Instead, it is hard to select Ψu ∈ Lp such that the
corresponding Ψx satisﬁes Ψx ∈ Lp. The paper [13] suggests
directly searching over ℓp-stable operators (Ψx, Ψu) and
abandoning the goal of complying with (8) exactly. One can
then study robust stability when (8) only holds approximately.
However, this way of proceeding may result in conservative
control policies. In this work, we turn our attention to the
problem of complying with (8)-(9) by design, thus leading to
an unconstrained parametrization of all stabilizing controllers.

III. MAIN RESULT: UNCONSTRAINED PARAMETRIZATION
OF ALL STABILIZING CONTROLLERS

In this section we show that, if a single stabilizing control
policy K′(x) with K′ ∈ Lp is available for the nonlinear
system F, it is possible to parametrize all other stabilizing
control policies in terms of stable maps M ∈ Lp by applying
the control input

u = K′(x) + M(w) ,

(12)

where w is reconstructed as x−F(x, u). Furthermore, if K′ is
stabilizing, but not itself an operator in Lp, the control policy
(12) achieves ℓp-stable closed-loop maps (Ψx, Ψu) ∈ CLp[F]
for any M ∈ Lp. From now on, we consider the system

x = F(x, u) + w , u = K′(x) + v ,
v = M(w) ,

(13)
(14)

where K′ is the base controller, and the operator M : ℓn →
ℓm must be designed in order to comply with the closed-loop
stability requirement (R1) and the optimality requirement (R2).
The combined effect of K′(x) and M(w) deﬁnes an overall
control policy K such that u(x) = K(x) is equivalent to (12).
Akin to the Youla parametrization for linear systems [20],
the role of a base controller K′ is to appropriately stabilize
the system; this allows deﬁning a set of “stable coordinates”
and then freely optimize over M.

Deﬁnition 1 (ℓp-Input-State stabilizing controller): Given
p ∈ N ∪ ∞, we say that the base controller K′ is ℓp-Input-
(x, u)
State (ℓp-IS) stabilizing for F if the map (w, v)
deﬁned by (13) lies in Lp.

We remark that the notion of ℓp-IS stabilizability is linked
to those of incremental passivity [21] and input-state-stability
(ISS) [22] for discrete-time systems. For instance, if a system
is ISS-stable, then the control policy K′ = 0 is ℓ∞-IS stabi-
lizing. While this paper does not deal with the computation
of a base controller, we refer the interested reader to [21]
and references therein for modern methods to design ℓp-IS
stabilizing controllers in discrete-time. Last, note that our
results may be extended to local notions of stability, for which
a locally stabilizing base controller would be sufﬁcient [16].
Deﬁnition 2 (Strongly ℓp-IS stabilizing controller): We say
that the base controller K′ is strongly ℓp-IS stabilizing if it is
ℓp-IS stabilizing, and additionally K′ ∈ Lp.
We are ready to state our main result.
Theorem 2: Consider the closed-loop system (13)-(14).
is
1) Assume

stabilizing. Then,

that K′

(Φx[F, K], Φu[F, K]) ∈ CLp[F] for every M ∈ Lp.
2) If, in addition, K′ is strongly ℓp-IS stabilizing, then, for
any (Ψx, Ψu) ∈ CLp[F], there exists M ∈ Lp such that
the control policy (12) achieves the closed-loop maps
(Φx[F, K], Φu[F, K]) = (Ψx, Ψu).
Proof:

1) is a restatement of Deﬁnition 1 with the
substitution v = M(w). We prove 2). Let (Ψx, Ψu) ∈
CLp[F] and choose

ℓp-IS

M = −K′(Ψx

) + Ψu

.

(15)

Note that the composition of operators in Lp remains in Lp.
Then, since (Ψx, Ψu) ∈ Lp, we conclude that M ∈ Lp. It re-
mains to prove that (15) is such that the resulting control policy
(12) achieves the closed-loop maps (Φx[F, K], Φu[F, K]) =
(Ψx, Ψu). We prove this fact by induction. For the inductive
step, we assume that, for any j ∈ N, we have Φx
i = Ψx
i
for all i ∈ N with 0 ≤ i ≤ j, where we
and Φu
have dropped the notation [F, K] in the interest of readability.
Since (Φx[F, K], Φu[F, K]) are closed-loop maps induced by
K and (Ψx, Ψu) ∈ CLp[F], then
j:0, Φu

j+1=Fj+1(Ψx
Then, by (12), (15), (16), and xj+1:0 = Φx

j+1=Fj+1(Φx

j:0) +I. (16)

j:0)+ I, Ψx

i = Ψu
i

j+1:0(wj+1:0),

j:0, Ψu

Φx

j:0, Φu

Fj+1:0(Φx

Φu

j+1 =K ′

(cid:0)

(cid:0)
j+1

(17)

j+1 .

+ Ψu

j:0) + I

j+1
− K ′

j+1 = Ψx

Fj+1:0(Ψx

j:0, Ψu
i = Ψx

−
j:0) + I
(cid:1)
i = Ψu
i and Φu
By inductive assumption Φx
(cid:1)
0 ≤ i ≤ j. Hence, (17) simpliﬁes to Φu
j+1 = Ψu
(16), Φx
causality of F and achievability, we have Φx
0(Ψx
(12) and (15) imply Φu

i for every
j+1. By
j+1. For the base case when j = 0, by strict
0 = I. Then,
0 = Ψu
0 .

0(Φx
To summarize, Theorem 2 establishes that the nonlinear
functional equalities involved in the N-SLS problem (11) can
be dropped by tackling the problem in two steps. First, obtain
an ℓp-IS stabilizing, yet suboptimal control policy u = K′(x).

0 = Ψx
0) + Ψu

0 ) − K ′

0 = K ′

w

x

System

F(x, u)

u

K′(·)

Controller

M(

)·

v

F(x, u)

Fig. 1.
one freely chosen operator M ∈ Lp.

Proposed parametrization of all stabilizing controllers in terms of

Second, explore the space of operators M ∈ Lp such that u =
K′(x) +M(x − F(x, u)) optimizes the closed-loop behavior.
The corresponding closed-loop maps will lie in Lp by design.
If K′ is also strongly ℓp-IS stabilizing, then, the proposed
parametrization is complete; all the achievable closed-loop
maps — including the ones that are globally optimal for
the N-SLS problem (11) — are achieved by appropriately
selecting M ∈ Lp in (12). We illustrate the proposed control
architecture in Figure 1.

Remark 1: The assumption of strong stabilizability is piv-
otal to parametrizing all stabilizing controllers. The classical
work [15] has parametrized classes of nonlinear stabilizing
controllers in continuous-time based on strongly stabilizing
base controllers. The case of linear systems is no exception; a
linear strongly stabilizing controller yields a doubly-coprime
factorization of the plant, which enables the well-known Youla
parametrization [20] of linear stabilizing controllers. From a
theoretical point of view, the results of Theorem 2 can be
interpreted as the counterpart of [15], [16] for discrete-time
control systems, using the lens of SLS.

A. The case of linear systems with nonlinear costs

Consider a linear system of the form

xt = Axt−1 + But−1 + wt ,

(18)

and let z denote the time-shift operator. The classical Youla
parametrization [20] states that all linear stabilizing control
policies u = Kx can be written in terms of a stable
Youla parameter Q ∈ T F s, where T F s denotes the set of
stable transfer matrices. For the state-feedback case, the Youla
approach [20] simpliﬁes to ﬁnding a K ′ such that (A + BK ′)
is Schur-stable, and then expressing all other linear stabilizing
control policies as

u =

(K ′ − Q)
z

(Ax + Bu) + Qx , Q ∈ T F s .

(19)

The class of linear control policies is globally optimal for
standard LQG problems, and it allows optimizing over Q ∈
T F s using convex-programming. However, these properties
are lost when the controller is distributed [23], or the cost
function is nonlinear — even if the system dynamics are linear.
For the latter scenario, the recent paper [11] characterizes
all stabilizing (and contractive) nonlinear policies for output-
feedback linear systems by using a nonlinear and contractive
Youla parameter. We provide an alternative proof of such result

with adaptation to Lp closed-loop maps in the state-feedback
case as an immediate corollary of Theorem 2.

Corollary 1: Consider the linear system (18) and assume
that (A, B) is stabilizable. Let K ′ be such that (A + BK ′)
is Schur-stable. Then, all control policies achieving nonlinear
closed-loop maps in CLp are expressed as

u = K ′x + M

x −

where M ∈ Lp.

(cid:18)

(Ax + Bu)
z

,

(cid:19)

(20)

Proof: The linear base controller K ′ is ℓp-IS stabilizing
by assumption and it is strongly stabilizing since K ′x ∈ ℓn
p
for each x ∈ ℓn
p . The conclusion follows by applying point 2
of Theorem 2.
Note that, as expected, the linear Youla parametrization (19)
is a special case of the proposed parametrization (20) with
M = Q − K ′ and Q ∈ T F s.

IV. NEURAL SYSTEM LEVEL SYNTHESIS

In this section, we propose the Neur-SLS approach for
tackling the NOC problem. Our goal is to exploit Theorem 2 in
order to characterize highly expressive DNN control policies
which are stabilizing by design — irrespective of how “badly”
one may choose the DNN weights. Then, appropriate DNN
training leads to optimized performance.

A. Finite-dimensional approximations of Lp using RENs

An obstacle to directly applying the results of Theorem 2 in
practice is that the space Lp is inﬁnite-dimensional. Hence, a
ﬁnite-dimensional approximation of M ∈ Lp is necessary for
implementation. When linear systems are considered, the SLS
approach [14] suggests searching over FIR transfer matrices
N
i=0 M [i]z−i ∈ T F s. One can then optimize over the
M =
ﬁnitely many real matrices M [i] for every i = 0, . . . , N and
obtain near-optimal solutions by selecting the FIR order N to
be large enough.

P

However,

the FIR approach limits the search to linear
control policies. To parametrize large classes of nonlinear poli-
cies, we embrace the recently proposed RENs [17] as ﬁnite-
dimensional DNN approximations of nonlinear Lp operators.
An operator M : ℓn → ℓm is a REN if the relationship
u = M(w) is generated by the following dynamical system:

W

ξt
vt
ut



=

z




B2
A B1
C1 D11 D12
}|
C2 D21 D22

{


ξt−1
σ(vt)
wt



,



ξ−1 = 0 ,

(21)












where ξt ∈ Rq, vt ∈ Rr, and σ : R → R — the activation
function — is applied element-wise. Further, σ(·) must be
piecewise differentiable and with ﬁrst derivatives restricted to
the interval [0, 1]. As noted in [17], RENs subsume many ex-
isting DNN architectures. In general, RENs deﬁne deep equi-
librium network models [24] due to the implicit relationships
between the signals involved in (21). By restricting D11 to be
strictly lower-triangular, the outputs of (21) can be computed
explicitly, thus signiﬁcantly speeding-up computations [17].
Further, note that the nonlinear relationship deﬁned by (21) is

“arbitrarily deep”; the nonlinearity σ(·) is recursively applied
on the REN internal state ξt−1 and the input wt for r times,
where r is the chosen dimension of vt.

For an arbitrary choice of W , the map M induced by (21)
may not lie in Lp. The breakthrough of [17] is to provide an
explicit smooth mapping Θ : Rd → R(q+r+m)×(q+r+n) from
unconstrained training parameters θ ∈ Rd to a matrix W =
Θ(θ) ∈ R(q+r+m)×(q+r+n) deﬁning (21), with the property
that the corresponding operator M[θ] lies in L2 by design.1
We refer to [17] for explicit deﬁnition of the mapping Θ(·).

B. Neur-SLS

By combining the theoretical results of Theorem 2 with
REN approximations of operators in L2, we propose the
following unconstrained Neur-SLS optimization problem:

Neur-SLS

S

T

1
min
S
θ∈Rd
s=1 "
t=0
X
X
s. t. xs
t = ft(xs
ξs
t
vs
t
us
t

0
0
t:0)
t(xs
K ′

t = 0, 1, . . . ,





=









(22)

(23)

l(xs

t , us
t )

#

t−1:0, us

t−1:0) + ws

0 = xs
0 ,

t , ws
ξs
t−1
σ(vs
t )
t−1:0, us

+Θ(θ)



xs
t−ft(xs

ξ−1 = 0 .

t−1:0)


, (24)

(25)

T :0}S

In the above problem, K′ is an ℓ2-IS stabilizing base con-
troller, and {ws
s=1 is a given training set of disturbances.
The cost function (22) is deﬁned as the empiric average of the
loss evaluated over the training set, and the system dynamics
are imposed through (23) for every ws. The relationships (24)-
(25) deﬁne a control sequence us = K′(xs) + M[θ](xs −
F(xs, us)), where M[θ] ∈ L2 for every θ. As a result, each
value of θ ∈ Rd yields closed-loop maps (Ψx, Ψu) ∈ CL2.

We remark that the class of all ℓ2-stable REN operators may
be signiﬁcantly more restrictive than the class of all operators
in L2. Hence, learning over all ℓ2-stable REN operators may
not allow reaping the beneﬁts of the completeness result of
point 2) of Theorem 2. This is the reason why in the REN-
based Neur-SLS implementation (22)-(25) we allow K′ being
ℓ2-IS stabilizing, but not necessarily strongly ℓ2-IS stabilizing.
Based on the above reasoning, an important takeaway of Theo-
rem 2 is that developing ﬁnite-dimensional approximations of
Lp that are as large as possible is a crucial endeavor — and
a worthy one — towards obtaining globally optimal solutions
of NOC with the Neur-SLS.

V. NUMERICAL EXPERIMENTS

In this section, we illustrate the use of Neur-SLS to
tackle NOC problems. Further, we validate the closed-
loop stability guarantees during and after
training. We
implement
the Neur-SLS (22)-(25) using PyTorch and
train the resulting DNN with stochastic gradient descent.
The implementation details are available in Appendix I.

The code to reproduce the examples can be found at
https://github.com/DecodEPFL/neurSLS.git.

We consider point-mass vehicles with position pt ∈ R2 and
velocity qt ∈ R2 subject to nonlinear drag forces (e.g., air or
water resistance). The discrete-time model for each vehicle of
mass m ∈ R is

pt
qt

(cid:20)

(cid:21)

=

pt−1
qt−1(cid:21)

(cid:20)

+ Ts

(cid:20)

qt−1
m−1 (−C(qt−1)qt−1 + Ft−1)

, (26)

(cid:21)

where Ft ∈ R2 denotes the force control input, Ts > 0
is the sampling time and C : R2 → R is a positive
drag function. Typical scenarios include C(q) = b|q|2 and
C(q) = b for some b > 0 [25]. By comparing (26) with
(1), we remark that the disturbance sequence is given by
2. Letting p ∈ R2 and q = 02
((p0, q0, 0, 0), (0, 0, 0, 0), . . .) ∈ ℓ4
be a target position to be reached with zero velocity, we
consider a base controller ut = K ′(p − pt) with K ′ =
diag(k′
2 > 0. Note that the base controller
is strongly ℓ2-IS stabilizing if C(q) = b, and strongly ℓ∞-IS
stabilizing if C(q) includes the nonlinearity b|q|2.

2) and k′

1, k′

1, k′

We model a set of N ∈ N vehicles (26) at time t by deﬁning
an overall state xt ∈ R4N and input ut ∈ R2N . We select the
stage loss function in (22) as

l(xt, ut) = ltraj(xt, ut) + lca(xt) + lobs(xt) ,

(27)

xT
t

uT
with Q (cid:23) 0
where ltraj(xt, ut) =
t
penalizes the distance of agents from their target states and the
control energy, lca(xt) penalizes collisions between agents and
lobs(xt) penalizes collisions with obstacles in the environment.

uT
t

xT
t

Q

(cid:3)

(cid:2)

(cid:2)

(cid:3)

T

A. Results

The scenario mountains in Figure 2 involves two agents
whose goal is to coordinately pass through a narrow valley.
Both agents start from a randomly sampled initial position
marked with “◦”. The scenario swapping in Figure 3 consid-
ers twelve agents switching their positions, while avoiding all
collisions.2 Animations are available in our Github repository
We train Neur-SLS control policies to optimize the perfor-
mance (22)-(27) over a horizon of 5 seconds with sampling
time Ts = 0.05s, resulting in T = 100 time-steps. We consider
a linear drag force C(q)q = bq for swapping and a non-
linear drag force C(q)q = b1q + b2|q|2q for mountains,
with suitable b, b1, b2 > 0. The trajectories after training are
reported in Figures 2 and 3. The trained Neur-SLS control
policies avoid collisions and achieve optimized trajectories
thanks to minimizing (27).

Despite training for a ﬁnite-horizon, Theorem 2 guarantees
closed-loop stability (i.e., ℓ2-stability for swapping and ℓ∞-
stability for mountains) around the target positions for
t → ∞. To validate this result, in Figure 4 we consider the
mountains environment and report the cumulative stage loss
k, us
k) at t = 0, 1, . . . , 500 for 10 randomly sampled
initial conditions {ws
s=1 before and after the training. De-
P
spite being extremely high before training due to a randomly

t
k=0 l(xs

0}10

1Furthermore, RENs enjoy contractivity — although the theoretical results

2The mountains and swapping benchmarks are motivated by the

of this paper do not use this property.

examples in [6], [12].

research, including extension to output-feedback and appli-
cations to constrained, distributed and data-driven nonlinear
control.

ACKNOWLEDGMENTS

We thank Dr. Alessandro Bosso for helpful discussions on

stability notions.

REFERENCES

[1] S. Sastry, Nonlinear systems: analysis, stability, and control. Springer

Science & Business Media, 2013, vol. 10.

[2] D. P. Bertsekas, “Dynamic programming and optimal control: Vol. I-II,”

Belmont, MA: Athena Scientiﬁc, 2011.

[3] K. Doya, S. Ishii, A. Pouget, and R. P. Rao, Bayesian brain: Proba-

bilistic approaches to neural coding. MIT press, 2007.

[4] J. B. Rawlings, D. Q. Mayne, and M. Diehl, Model predictive control:

theory, computation, and design. Nob Hill Publishing, 2017, vol. 2.

[5] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.

MIT press, 2018.

[6] D. Onken, L. Nurbekyan, X. Li, S. W. Fung, S. Osher, and L. Ruthotto,
“A neural network approach applied to multi-agent optimal control,” in
IEEE European Control Conference (ECC), 2021, pp. 1036–1041.
[7] F. Gama and S. Sojoudi, “Graph neural networks for distributed linear-
PMLR,

quadratic control,” in Learning for Dynamics and Control.
2021, pp. 111–124.

[8] F. Berkenkamp, M. Turchetta, A. P. Schoellig, and A. Krause, “Safe
model-based reinforcement learning with stability guarantees,” Advances
in Neural Information Processing Systems 30, vol. 2, pp. 909–919, 2018.
[9] F. Gu, H. Yin, L. E. Ghaoui, M. Arcak, P. Seiler, and M. Jin, “Recur-
rent neural network controllers synthesis with stability guarantees for
partially observed systems,” arXiv:2109.03861, 2021.

[10] P. Pauli, J. K¨ohler, J. Berberich, A. Koch, and F. Allg¨ower, “Offset-
free setpoint tracking using neural network controllers,” in Learning for
Dynamics and Control. PMLR, 2021, pp. 992–1003.

[11] R. Wang, N. Barbara, M. Revay, and I. R. Manchester, “Learning over all
stabilizing nonlinear controllers for a partially-observed linear system,”
arXiv:2112.04219, 2021.

[12] L. Furieri, C. L. Galimberti, M. Zakwan, and G. Ferrari-Trecate,
“Distributed neural network control with dependability guarantees: a
compositional port-Hamiltonian approach,” PMLR, to appear, 2022.

[13] D. Ho, “A system level approach to discrete-time nonlinear systems,” in
IEEE American Control Conference (ACC), 2020, pp. 1625–1630.
[14] Y.-S. Wang, N. Matni, and J. C. Doyle, “A system level approach to

controller synthesis,” IEEE Transactions on Automatic Control, 2019.

[15] V. Anantharam and C. Desoer, “On the stabilization of nonlinear
systems,” IEEE Trans. on Aut. Contr., vol. 29, no. 6, pp. 569–572, 1984.
[16] K. Fujimoto and T. Sugie, “State-space characterization of Youla
parametrization for nonlinear systems based on input-to-state stability,”
in Proceedings of the 37th IEEE Conference on Decision and Control
(Cat. No. 98CH36171), vol. 3.

IEEE, 1998, pp. 2479–2484.

[17] M. Revay, R. Wang, and I. R. Manchester, “Recurrent equilibrium
networks: Flexible dynamic models with guaranteed stability and ro-
bustness,” arXiv:2104.05942, 2021.

[18] Y. Tang, Y. Zheng, and N. Li, “Analysis of the optimization landscape of
Linear Quadratic Gaussian (LQG) control,” in Learning for Dynamics
and Control. PMLR, 2021, pp. 599–610.

[19] L. Furieri and M. Kamgarpour, “First order methods for globally optimal
distributed controllers beyond quadratic invariance,” in IEEE American
Control Conference (ACC), 2020, pp. 4588–4593.
[20] K. Zhou and J. C. Doyle, Essentials of robust control.

Prentice hall

Upper Saddle River, NJ, 1998, vol. 104.

[21] P. J. Koelewijn, R. T´oth, and S. Weiland, “Incremental dissipativity
based control of discrete-time nonlinear systems via the LPV frame-
work,” arXiv preprint arXiv:2110.00290, 2021.

[22] Z.-P. Jiang and Y. Wang, “Input-to-state stability for discrete-time
nonlinear systems,” Automatica, vol. 37, no. 6, pp. 857–869, 2001.
[23] L. Furieri, Y. Zheng, A. Papachristodoulou, and M. Kamgarpour, “Spar-
sity invariance for convex design of distributed controllers,” IEEE Trans.
on Control of Network Systems, vol. 7, no. 4, pp. 1836–1847, 2020.

[24] S. Bai, J. Z. Kolter, and V. Koltun, “Deep equilibrium models,” Advances

in Neural Information Processing Systems, vol. 32, 2019.

[25] G. Falkovich, Fluid mechanics: A short course for physicists. Cam-

bridge University Press, 2011.

Mountains — Closed-loop trajectories after training over 500
Fig. 2.
randomly sampled initial conditions marked with ◦. Snapshots taken at times
τ1 = 0.7, τ2 = 1.1 and τ3 = 5.0 seconds. Colored (gray) lines show the
trajectories in [0, τi] ([τi, ∞)). Colored balls (and their radius) represent the
agents (and their size for collision avoidance).

Swapping — Closed-loop trajectories after training. Snapshots
Fig. 3.
taken at times τ1 = 1.25, τ2 = 1.75 and τ3 = 5.0 seconds. Colored (gray)
lines show the trajectories in [0, τi] ([τi, ∞)). Colored balls (and their radius)
represent the agents (and their size for collision avoidance).

selected control policy, the cumulative loss converges to a
constant value. Hence, the vehicles reach the target formation
with vanishing control inputs as t → ∞ irrespective of the
chosen DNN parameters. This fact validates the built-in ℓ∞-
stability of the closed-loop maps.

VI. CONCLUSIONS

As we move towards designing deep nonlinear policies for
general optimal control problems, it is crucial to guarantee
closed-loop stability during and after training. We embrace a
system-level perspective and propose a two-step procedure to
parametrize all and only the stabilizing controllers in terms
of one stable Youla-like operator. This results in a Neur-
SLS optimization problem that can be tackled by training
a DNN with unconstrained gradient descent. The proposed
system-level perspective may open up several venues for future

Fig. 4. Cumulative stage loss in mountains before and after training. The
black dashed line indicates the training horizon T = 100.

First, we deﬁne the addends of the cost function (27). We have

APPENDIX I
IMPLEMENTATION DETAILS

N

ltraj(xt, ut) =

t, qi
pi
t

T ˜Q

t, qi
pi
t

+ αu

F i
t

T

F i
t

,

lca(xt) =

(cid:0)

i=0
X
N
i=0

P

P

αca
0

(

(cid:0)
(cid:1)
j, i6=j(dij,t + ǫ)−2

(cid:1)

(cid:1)

(cid:1)

(cid:0)
(cid:0)
if dij,t ≤ D ,
otherwise ,

where pi, qi and F i denote the position, velocity and control input for agent i, ˜Q (cid:23) 0 and αu, αca ≥ 0 are hyperparameters,
dij,t = |pi
t |2 ≥ 0 denotes the distance between agent i and j and ǫ > 0 is a ﬁxed positive small constant such that the
loss remains bounded for all distance values.

t − pj

Motivated by [6], we represent the obstacles based on a Gaussian density function

η(p; µ, Σ) =

1
det(Σ)

1
2

exp

−

(cid:18)

2π

(p − µ)⊤ Σ−1 (p − µ)

,

(cid:19)

with mean µ ∈ R2 and covariance Σ ∈ R2×2 with Σ ≻ 0. The term lobs(xi

p

t) is given by

lobs(xt) = αobst

pi
t;

, 0.2 I

+ η

pi
t;

N

η
i=0 (cid:18)
X
+η

(cid:18)
pi
t;

2.5
0

(cid:21)

(cid:20)
1.5
0

(cid:21)

−2.5
0

(cid:21)

, 0.2 I

, 0.2 I

+ η

, 0.2 I

(cid:19)

(cid:18)
pi
t;

(cid:20)
−1.5
0

(cid:19)

(cid:18)

(cid:20)

(cid:21)

(cid:19)

.

(cid:19)(cid:19)

Mountains problem (2 robots)

(cid:18)

(cid:20)

The scenario mountains involves two agents whose goal is to coordinately pass through a narrow valley. The system

consists of 2 robots (26) of mass m = 1 kg and radius 0.5 m, each. We consider a drag function given by

C(q)q = b1q + b2|q|q ,

with b1 = 1 N s/m and b2 = 0.1 N s/m. The based controller has constants k′

1 = k′

2 = 1 N/m.

The REN is a deep neural network with depth r = 32 layers (v ∈ Rr). Its internal state ξ is of dimension q = 32. We use

tanh(·) as the activation function.

The initial positions of the robots are sampled from a Normal distribution centered at (±2, −2), with covariance diag(σ2, σ2).
We use stochastic gradient descent with Adam in order to minimize the loss function. We set the hyperparameters ˜Q =
diag(1, 1, 1, 1), αu = 0.1, αca = 100 and αobst = 5000, and train for 500 epochs with a learning rate of 0.001. We set
σ = 0.2 for the ﬁrst 300 epochs and then increased it to σ = 0.5 to further enhance robustness. At each epoch we simulate
ﬁve trajectories over which we calculate the corresponding loss.

Swapping problem (12 robots)

The scenario swapping considers twelve agents switching their positions, while avoiding all collisions. The system consists

of 12 robots (26) of mass m = 1 kg and radius 0.25 m each. The considered drag function is given by

C(q)q = bq ,

with b = 1 N s/m. The based controller has constants k′

1 = k′

2 = 1 N/m.

The REN is a deep neural network with depth r = 24 layers (v ∈ Rr). Its internal state ξ is of dimension q = 96. We use

tanh(·) as the activation function.

We use stochastic gradient descent with Adam in order to minimize the loss function. We set the hyperparameters ˜Q =
diag(1, 1, 1, 1), αu = 0.1 and αca = 1000, and train for 1500 epochs with a learning rate of 0.002. Since there are no ﬁxed
obstacles in the environment, we set αobst = 0.

