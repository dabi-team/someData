2
2
0
2

b
e
F
4
1

]

G
L
.
s
c
[

3
v
3
4
8
7
0
.
0
1
1
2
:
v
i
X
r
a

FOLD-R++: A Scalable Toolset for Automated
Inductive Learning of Default Theories from
Mixed Data

Huaduo Wang and Gopal Gupta

Department of Computer Science
The University of Texas at Dallas, USA
{huaduo.wang, gupta}@utdallas.edu

Abstract. FOLD-R is an automated inductive learning algorithm for
learning default rules for mixed (numerical and categorical) data. It gen-
erates an (explainable) normal logic program (NLP) rule set for classiﬁ-
cation tasks. We present an improved FOLD-R algorithm, called FOLD-
R++, that signiﬁcantly increases the eﬃciency and scalability of FOLD-
R by orders of magnitude. FOLD-R++ improves upon FOLD-R without
compromising or losing information in the input training data during the
encoding or feature selection phase. The FOLD-R++ algorithm is com-
petitive in performance with the widely-used XGBoost algorithm, how-
ever, unlike XGBoost, the FOLD-R++ algorithm produces an explain-
able model. FOLD-R++ is also competitive in performance with the
RIPPER system, however, on large datasets FOLD-R++ outperforms
RIPPER. We also create a powerful tool-set by combining FOLD-R++
with s(CASP)—a goal-directed answer set programming (ASP) execu-
tion engine—to make predictions on new data samples using the normal
logic program generated by FOLD-R++. The s(CASP) system also pro-
duces a justiﬁcation for the prediction. Experiments presented in this
paper show that our improved FOLD-R++ algorithm is a signiﬁcant
improvement over the original design and that the s(CASP) system can
make predictions in an eﬃcient manner as well.

Keywords: Inductive Logic Programming, Machine Learning, Explain-
able AI, Negation as Failure, normal logic programs, Data mining

1

Introduction

Dramatic success of machine learning has led to a torrent of Artiﬁcial Intelli-
gence (AI) applications. However, the eﬀectiveness of these systems is limited by
the machines’ current inability to explain their decisions and actions to human
users. That’s mainly because the statistical machine learning methods produce
models that are complex algebraic solutions to optimization problems such as
risk minimization or geometric margin maximization. Lack of intuitive descrip-
tions makes it hard for users to understand and verify the underlying rules that
govern the model. Also, these methods cannot produce a justiﬁcation for a pre-
diction they arrive at for a new data sample. The Explainable AI program [8]

 
 
 
 
 
 
2

Wang and Gupta

aims to create a suite of machine learning techniques that: a) Produce more
explainable models, while maintaining a high level of prediction accuracy; and
b) Enable human users to understand, appropriately trust, and eﬀectively man-
age the emerging generation of artiﬁcially intelligent systems. Inductive Logic
Programming (ILP) [14] is one Machine Learning technique where the learned
model is in the form of logic programming rules that are comprehensible to hu-
mans. It allows the background knowledge to be incrementally extended without
requiring the entire model to be re-learned. Meanwhile, the comprehensibility of
symbolic rules makes it easier for users to understand and verify induced models
and even reﬁne them.

The ILP learning problem can be regarded as a search problem for a set of
clauses that deduce the training examples. The search is performed either top
down or bottom-up. A bottom-up approach builds most-speciﬁc clauses from
the training examples and searches the hypothesis space by using generalization.
This approach is not applicable to large-scale datasets, nor it can incorporate
negation-as-failure into the hypotheses. A survey of bottom-up ILP systems
and their shortcomings can be found at [22]. In contrast, the top-down approach
starts with the most general clause and then specializes it. A top-down algorithm
guided by heuristics is better suited for large-scale and/or noisy datasets [28].

The FOIL algorithm [19] by Quinlan is a popular top-down inductive logic
programming algorithm that generates logic programs. FOIL uses weighted in-
formation gain (IG) as the heuristics to guide the search for best literals. The
FOLD algorithm by Shakerin [23, 24] is a new top-down algorithm inspired by
the FOIL algorithm. It generalizes the FOIL algorithm by learning default rules
with exceptions. It does so by ﬁrst learning the default predicate that covers pos-
itive examples while avoiding negative examples, then next it swaps the positive
and negative examples and calls itself recursively to learn the exception to the
default. Both FOIL and FOLD cannot deal with numeric features directly; an
encoding process is needed in the preparation phase of the training data that
discretizes the continuous numbers into intervals. However, this process not only
adds a huge computational overhead to the algorithm but also leads to loss of
information in the training data.

To deal with the above problems, Shakerin developed an extension of the
FOLD algorithm, called FOLD-R, to handle mixed (i.e., both numerical and
categorical) features which avoids the discretization process for numerical data
[23, 24]. However, FOLD-R still suﬀers from eﬃciency and scalability issues when
compared to other popular machine learning systems for classiﬁcation. In this
paper we report on a novel implementation method we have developed to improve
the design of the FOLD-R system. In particular, we use the preﬁx sum technique
[27] to optimize the process of calculation of information gain, the most time
consuming component of the FOLD family of algorithms [23]. Our optimization,
in fact, reduces the time complexity of the algorithm. If N is the number of
unique values from a speciﬁc feature and M is the number of training examples,
then the complexity of computing information gain for all the possible literals
of a feature is reduced from O(M ∗ N ) for FOLD-R to O(M ) in FOLD-R++.

FOLD-R++ Toolset

3

In addition to using preﬁx sum, we also improved the FOLD-R algorithm
by allowing negated literals in the default portion of the learned rules (ex-
plained later). Finally, a hyper-parameter, called exception ratio, which controls
the training process that learns exception rules, is also introduced. This hyper-
parameter helps improve eﬃciency and classiﬁcation performance. These three
changes make FOLD-R++ signiﬁcantly better than FOLD-R and competitive
with well-known algorithms such as XGBoost and RIPPER.

Our experimental results indicate that the FOLD-R++ algorithm is compa-
rable to popular machine learning algorithms such as XGBoost [3] and RIPPER
[4] wrt various metrics (accuracy, recall, precision, and F1-score) as well as in
eﬃciency and scalability. However, in addition, FOLD-R++ produces an explain-
able and interpretable model in the form of a normal logic program. A normal
logic program is a logic program extended with negation-as-failure [13]. Note that
RIPPER also generates a set of CNF formulas to explain the model, however,
as we will see later, FOLD-R++ outperforms RIPPER on large datasets.

This paper makes the following novel contribution: it presents the FOLD-
R++ algorithm that signiﬁcantly improves the eﬃciency and scalability of the
FOLD-R ILP algorithm without adding overhead during pre-processing or losing
information in the training data. As mentioned, the new approach is competitive
with popular classiﬁcation models such as the XGBoost classiﬁer [3] and the
RIPPER system [4]. The FOLD-R++ algorithm outputs a normal logic program
(NLP) [13, 7] that serves as an explainable/interpretable model. This generated
normal logic program is compatible with s(CASP) [2], a goal-directed ASP solver,
that can eﬃciently justify the prediction generated by the ASP model.1

2 Background

2.1 Inductive Logic Programming

Inductive Logic Programming (ILP) [14] is a subﬁeld of machine learning that
learns models in the form of logic programming rules that are comprehensible
to humans. This problem is formally deﬁned as:

Given

1. A background theory B, in the form of an extended logic program, i.e.,
clauses of the form h ← l1, ..., lm, not lm+1, ..., not ln, where l1, ..., ln are
positive literals and not denotes negation-as-failure (NAF) [13, 7]. We require
that B has no loops through negation, i.e., it is stratiﬁed [13].

2. Two disjoint sets of ground target predicates E+, E− known as positive and

negative examples, respectively

3. A hypothesis language of function free predicates L, and a reﬁnement oper-
ator ρ under θ-subsumption [18] that would disallow loops over negation.

1 The s(CASP) system is freely available at https://gitlab.software.imdea.org/ciao-

lang/sCASP.

4

Wang and Gupta

Find a set of clauses H such that:

– ∀e ∈ E+, B ∪ H |= e
– ∀e ∈ E−, B ∪ H (cid:54)|= e
– B ∧ H is consistent.

2.2 Default Rules

Default Logic [21, 7] is a non-monotonic logic to formalize commonsense reason-
ing. A default D is an expression of the form

A : MB
Γ
which states that the conclusion Γ can be inferred if pre-requisite A holds and
B is justiﬁed. MB stands for “it is consistent to believe B” [7]. Normal logic
programs can encode a default quite elegantly. A default of the form:

α1 ∧ α2 ∧ . . . ∧ αn : M¬β1, M¬β2 . . . M¬βm
γ

can be formalized as the following normal logic program rule:

γ :- α1, α2, . . . , αn, not β1, not β2, . . . , not βm.

where α’s and β’s are positive predicates and not represents negation-as-failure.
We call such rules default rules. Thus, the default bird(X):M ¬penguin(X)
will be
represented as the following default rule in normal logic programming:

f ly(X)

fly(X) :- bird(X), not penguin(X).

We call bird(X), the condition that allows us to jump to the default conclusion
that X can ﬂy, the default part of the rule, and not penguin(X) the exception
part of the rule.

Default rules closely represent the human thought process (commonsense
reasoning). FOLD-R and FOLD-R++ learn default rules represented as normal
logic programs. An advantage of learning default rules is that we can distin-
guish between exceptions and noise [24, 23]. Note that the programs currently
generated by the FOLD-R++ system are stratiﬁed normal logic programs [13].

3 The FOLD-R Algorithm

The FOLD algorithm [23, 24] is a top-down ILP algorithm that searches for best
literals to add to the body of the clauses for hypothesis, H, with the guidance of
an information gain-based heuristic. The FOLD-R algorithm is a numeric exten-
sion of the FOLD algorithm that adopts the approach of the well-known C4.5
algorithm [20] for ﬁnding literals. Algorithm 1 gives an overview of the FOLD-R
algorithm. The extended algorithm will directly select the best numerical literal,
in addition to selecting the categorical literals. Thus, the best numerical func-
tion (line 37 in Algorithm 1) ﬁnds the best numerical literal and adds it to the

FOLD-R++ Toolset

5

(cid:46) target, B: global vars

(cid:46) rule out already covered examples

else

while |E−| > 0 do

D ← Ø
while |E+| > 0 do

c(cid:48), ig ← add best literal(c, E+, E−)
if ig > 0 then
c ← c(cid:48)

c ← specialize(target :- true, E+, E−)
E+ ← E+ \ covers(c, E+)
D ← D ∪ {c}

Algorithm 1 FOLD-R Algorithm
Input: B: background knowledge, E+: positive example, E−: negative example
Output: D = {c1, ..., cn}: a set of defaults rules with exceptions
1: function Fold(E+, E−)
2:
3:
4:
5:
6:
7:
end while
return D
8:
9: end function
10: function Specialize(c, E+, E−)
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25: end function
26: function Exception(c, E+, E−)
AB ← fold(E−, E+)
27:
if AB is Ø then
28:
29:
c ← null
30:
31:
32:
33:
34: end function
35: function Add best literal(c, E+, E−)

end if
E+ ← covers(c, E+)
E− ← covers(c, E−)

c ← exception(c, E+, E−)
if c is null then

c ← enumerate(c, E+)

c ← set exception(c, AB)

end while
return c

end if
return c

end if

else

(cid:46) recursively call FOLD after swapping E+ and E−

(cid:46) generate clause to maximally cover E+

(cid:46) set exception part of clause c as AB

(cid:46) return the clause with the best literal added and its corresponding info gain

c1, ig1 ← best categorical(c, E+, E−)
c2, ig2 ← best numerical(c, E+, E−)
if c1 > c2 then

36:
37:
38:
39:
40:
41:
42:
43: end function

end if

else

return c1, ig1

return c2, ig2

(cid:46) FOLD-R extension

6

Wang and Gupta

clause after classifying all the training examples for each numerical split on all
the features. The other functions remain the same as the FOLD algorithm [24,
23]. We illustrate the FOLD-R algorithm through an example.

Example 1 In the FOLD-R algorithm, the target is to learn rules for fly(X).
B, E+, E− are background knowledge, positive and negative examples, respec-
tively.

B: bird(X) :- penguin(X).

bird(tweety).
cat(kitty).

E+: fly(tweety).
E-: fly(kitty).

bird(et).
penguin(polly).
fly(et).
fly(polly).

The target predicate {fly(X) :- true.} is speciﬁed when calling the spe-
cialize function at line 4 in Algorithm 1. The add best literal function selects the
literal bird(X) as a result and adds it to the clause r = fly(X) :- bird(X)
because it has the best information gain among {bird,penguin,cat} at line
12. Then, the training set gets updated to E+={tweety, et}, E−={polly} at
line 21–22 in SPECIALIZE function. The negative example polly is still falsely
implied by the generated clause. The default learning of SPECIALIZE function
is ﬁnished because the information gain of candidate literal c(cid:48) is zero. Therefore,
the exception learning starts by calling FOLD function recursively with swapped
positive and negative examples, E+={polly}, E−={tweety, et} at line 27. In
this case, an abnormal predicate {ab0(X) :- penguin(X)} is generated and re-
turned as the only exception to the previous learned clause as r = fly(X) :-
bird(X), not ab0(X). The abnormal rule {ab0(X) :- penguin(X)} is added
to the ﬁnal rule set producing the program below:

fly(X) :- bird(X), not ab0(X).
ab0(X) :- penguin(X).

4 The FOLD-R++ Algorithm

The FOLD-R++ algorithm refactors the FOLD-R algorithm. FOLD-R++ makes
three main improvements to FOLD-R: (i) it can learn and add negated literals to
the default (positive) part of the rule; in the FOLD-R algorithm negated literals
can only be in the exception part, (ii) preﬁx sum algorithm is used to speed
up computation, and (iii) a hyper parameter called ratio is introduced to con-
trol the level of nesting of exceptions. These three improvements make FOLD-R
signiﬁcantly more eﬃcient.

The FOLD-R++ algorithm is summarized in Algorithm 2. The output of
the FOLD-R++ algorithm is a set of default rules [7] coded as a normal logic
program. An example implied by any rule in the set would be classiﬁed as posi-
tive. Therefore, the FOLD-R++ algorithm rules out the already covered positive
examples at line 9 after learning a new rule. To learn a particular rule, the best

FOLD-R++ Toolset

7

literal would be repeatedly selected—and added to the default part of the rule’s
body—based on information gain using the remaining training examples (line
17). Next, only the examples that can be covered by learned default literals
would be used for further learning (specializing) of the current rule (line 20–21).
When the information gain becomes zero or the number of negative examples
drops below the ratio threshold, the learning of the default part is done. FOLD-
R++ next learns exceptions after ﬁrst learning default literals. This is done by
swapping the residual positive and negative examples and calling itself recur-
sively in line 26. The remaining positive and negative examples can be swapped
again and exceptions to exceptions learned (and then swapped further to learn
exceptions to exceptions of exceptions, and so on). The ratio parameter in Algo-
rithm 2 represents the ratio of training examples that are part of the exception
to the examples implied by only the default conclusion part of the rule. It allows
users to control the nesting level of exceptions.

Generally, avoiding falsely covering negative examples by adding literals to
the default part of a rule will reduce the number of positive examples the rule
can imply. Explicitly activating the exception learning procedure (line 26) could
increase the number of positive examples a rule can cover while reducing the
total number of rules generated. As a result, the interpretability is increased due
to fewer rules and literals being generated. For the Adult Census Income dataset,
for example, without the hyper-parameter exception ratio (equivalent to setting
the ratio to 0), the FOLD-R++ algorithm would take around 10 minutes to
ﬁnish the training and generate hundreds of rules. With the ratio parameter set
to 0.5, only 13 rules are generated in around 10 seconds.

Additionally, The FOLD and FOLD-R algorithms disabled the negated lit-
erals in the default theories to make the generated rules look more elegant (only
exceptions included negated literals). However, a negated literal sometimes is
the optimal literal with the most useful information gain. FOLD-R++ allows
for negated literals in the default part of the generated rules. We cannot make
sure that FOLD-R++ generates optimal combination of literals because it is a
greedy algorithm, however, it is an improvement over FOLD and FOLD-R.

4.1 Literal Selection

The literal selection process for Shakerin’s FOLD-R algorithm can be summa-
rized as function SPECIALIZE in Algorithm 1. The FOLD-R algorithm [23,
24] selects the best literal based on the weighted information gain for learning
defaults, similar to the original FOLD algorithm described in [24]. For numeric
features, the FOLD-R algorithm would enumerate all the possible splits. Then, it
classiﬁes the data and computes information gain for literals for each split. The
literal with the best information gain would be selected as a result. In contrast,
the FOLD-R++ algorithm uses a new, more eﬃcient method employing preﬁx
sums to calculate the information gain based on the classiﬁcation categories.
The FOLD-R++ algorithm divides features into two categories: categorical and
numerical. All the values in a categorical feature would be considered as cate-
gorical values even if some of them are numbers. Only equality and inequality

8

Wang and Gupta

Algorithm 2 FOLD-R++ Algorithm
Input: E+: positive examples, E−: negative examples

(cid:46) Global Parameters: target, B: background knowledge, ratio: exception ratio

(cid:46) Lused: used literals, initially empty

(cid:46) Etp: true positive examples implied by rule r

(cid:46) rule out the already covered examples

(cid:46) L: default literals for the result rule r

(cid:46) set default part of rule r as L

break

L ← Ø
while true do

R ← Ø
while |E+| > 0 do

end if
E+ ← E+ \ Etp
R ← R ∪ {r}

r ← learn rule(E+, E−, Lused)
Etp ← covers(r, E+)
if |Etp| = 0 then

Output: R = {r1, ..., rn}: a set of defaults rules with exceptions
1: function Fold rpp(E+, E−, Lused)
2:
3:
4:
5:
6:
7:
8:
9:
10:
end while
11:
12:
return R
13: end function
14: function Learn rule(E+, E−, Lused)
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:
33: end function

l ← find best literal(E+, E−, Lused)
L ← L ∪ {l}
r ← set default(r, L)
E+ ← covers(r, E+)
E− ← covers(r, E−)
if l is invalid or |E−| ≤ |E+| ∗ ratio then

end if
end while
return r

if l is invalid then

end if
break

else

r ← set default(r, L \ {l}) (cid:46) remove the invalid literal l from rule r

AB ← fold rpp(E−, E+, Lused + L) (cid:46) learn exception rules for r
(cid:46) set exception part of rule r as AB
r ← set exception(r, AB)

(cid:46) the head of rule r is target

literals would be generated for categorical features. For numerical features, the
FOLD-R++ algorithm would try to read each value as a number, converting
it to a categorical value if the conversion fails. Additional numerical compari-
son (≤ and >) literal candidates would be generated for numerical features. A
mixed type feature that contains both categorical and numerical values would
be treated as a numerical feature.

In FOLD-R++, information gain for a given literal is calculated as shown in
Algorithm 3. The variables tp, f n, tn, f p for ﬁnding the information gain repre-
sent the numbers of true positive, false negative, true negative, and false positive

FOLD-R++ Toolset

9

examples, respectively. With the simpliﬁed information gain function IG in Al-
gorithm 3, the new approach employs the preﬁx sum technique to speed up the
calculation. Only one round of classiﬁcation is needed for a single feature, even
with mixed types of values.

(cid:46) IG is the function that computes information gain

return −∞

if f p + f n > tp + tn then

Algorithm 3 FOLD-R++ Algorithm, Information Gain function
Input: tp, f n, tn, f p: the number of Etp, Ef n, Etn, Ef p implied by literal
Output: information gain
1: function IG(tp, f n, tn, f p)
2:
3:
4:
5:
6: end function
7: function F(a, b)
if a = 0 then
8:
9:
return 0
10:
11:
12: end function

tp+f p+tn+f n ·(F(tp, f p) + F(f p, tp) + F(tn, f n) + F(f n, tn))

end if
return a · log2( a

end if
return

a+b )

1

In the FOLD-R++ algorithm, two types of literals would be generated: equal-
ity comparison literals and numerical comparison literals. The equality (resp.
inequality) comparison is straightforward in FOLD-R++: two values are equal
if they are same type and identical, else they are unequal. However, a diﬀer-
ent assumption is made for comparisons between a numerical value and cat-
egorical value in FOLD-R++. Numerical comparisons (≤ and >) between a
numerical value and a categorical value is always false. A comparison exam-
ple is shown in Table 1 (Left), while an evaluation example for a given literal,
literal(i, ≤, 3), based on the comparison assumption is shown in Table 1 (Right).
Given E+ = {1, 2, 3, 3, 5, 6, 6, b}, E− = {2, 4, 6, 7, a}, and literal(i, ≤, 3), the true
positive example Etp, false negative examples Ef n, true negative examples Etn,
and false positive examples Ef p implied by the literal are {1, 2, 3, 3}, {5, 6, 6, b},
{4, 6, 7, a}, {2} respectively. Then, the information gain of literal(i, ≤, 3) is cal-
culated IG(i,≤,3)(4, 4, 4, 1) = −0.619 through Algorithm 3.

The new approach to ﬁnd the best literal that provides most useful informa-
tion is summarized in Algorithm 4. In line 12, pos (neg) is the dictionary that
holds the numbers of positive (negative) examples for each unique value. In line
13, xs (cs) is the list that holds the unique numerical (categorical) values. In line
14, xp (xn) is the total number of positive (negative) examples with numerical
values; cp (cn) is the total number of positive (negative) examples with cate-
gorical values. After computing the preﬁx sum at line 16, pos[x] (neg[x]) holds
the total number of positive (negative) examples that have a value less than or
equal to x. Therefore, xp − pos[x] (xn − neg[x]) represents the total number of
positive (negative) examples that have a value greater than x. In line 21, the

10

Wang and Gupta

best ig, best lit ← −∞, invalid
for i ← 1 to N do

Algorithm 4 FOLD-R++ Algorithm, Find Best Literal function
Input: E+: positive examples, E−: negative examples, Lused: used literals
Output: best lit: the best literal that provides the most information
1: function Find best literal(E+, E−, Lused)
2:
3:
4:
5:
6:
7:
8:
9:
10: end function
11: function Best info gain(E+, E−, i, Lused)
12:

ig, lit ← best info gain(E+, E−, i, Lused)
if best ig < ig then

pos, neg ← count classif ication(E+, E−, i)

end for
return best lit

best ig, best lit ← ig, lit

end if

(cid:46) N is the number of features

(cid:46) i: feature index

(cid:46) pos (neg): dicts that holds the numbers of E+ (E−) for each unique value

13:

14:

xs, cs ← collect unique values(E+, E−, i)

(cid:46) xs (cs): lists that holds the unique numerical (categorical) values

xp, xn, cp, cn ← count total(E+, E−, i)

(cid:46) xp (xn): the total number of E+ (E−) with numerical value.
(cid:46) cp (cn): the total number of E+ (E−) with categorical value.

xs ← couting sort(xs)
for j ← 1 to size(xs) do (cid:46) compute preﬁx sum for E+ & E− numerical values

pos[xsi] ← pos[xsi] + pos[xsi−1]
neg[xsi] ← neg[xsi] + neg[xsi−1]

(cid:46) compute info gain for numerical comparison literals
lit dict[literal(i, ≤, x)] ← IG(pos[x], xp−pos[x]+cp, xn−neg[x]+cn, neg[x])
lit dict[literal(i, >, x)] ← IG(xp−pos[x], pos[x]+cp, neg[x]+cn, xn−neg[x])

(cid:46) compute info gain for equality comparison literals
lit dict[literal(i, =, x)] ← IG(pos[c], cp − pos[c]+ xp, cn − neg[c]+ xn, neg[c])
lit dict[literal(i, (cid:54)=, x)] ← IG(cp − pos[c]+ xp, pos[c], neg[c], cn − neg[c]+ xn)

end for
best ig, lit ← best pair(lit dict, Lused)
return best ig, lit

(cid:46) return the best info gain and its corresponding literal

end for
for x ∈ xs do

15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30: end function

end for
for c ∈ cs do

information gain of literal(i, ≤, x) is calculated by calling Algorithm 3. Note that
pos[x] (neg[x]) is the actual value for the formal parameter tp (f p) of function
IG in Algorithm 3. Likewise, xp − pos[x] + cp (xn − neg[x] + cn) substitute for
formal parameter f n (tn) of the function IG. cp (cn) is included in the actual
parameter for formal parameter f n (tn) of function IG because of the assump-
tion that any numerical comparison between a numerical value and a categorical
value is false. The information gain calculation processes of other literals also
follow the comparison assumption mentioned above. Finally, the best info gain
function (Algorithm 4) returns the best score on information gain and the corre-

comparison evaluation

3 = ‘a’
3 (cid:54)= ‘a’
3 ≤ ‘a’
3 > ‘a’

False
True
False
False

FOLD-R++ Toolset

11

ith feature values count

E+
E−
Etp(i,≤,3)
Efn(i,≤,3)
Etn(i,≤,3)
Efp(i,≤,3)

1 2 3 3 5 6 6 b
2 4 6 7 a
1 2 3 3
5 6 6 b
4 6 7 a
2

8
5
4
4
4
1

Table 1. Left: Comparisons between a numerical value and a categorical value. Right:
Evaluation and count for literal(i, ≤, 3).

sponding literal except the literals that have been used in current rule-learning
process. For each feature, we compute the best literal, then the ﬁnd best literal
function returns the best literal among this set of best literals. FOLD-R algo-
rithm selects only positive literals in default part of rules during literal selection
even if a negative literal provides better information gain. Unlike FOLD-R, the
FOLD-R++ algorithm can also select negated literals for the default part of a
rule at line 26 in Algorithm 4.

It is easy to justify the O(M ) complexity of information gain calculation in
FOLD-R++ mentioned earlier. The time complexity of Algorithm 3 is obviously
O(1). Algorithm 3 is called in line 21, 22, 25, and 26 of Algorithm 4. Line 12–
15 in Algorithm 4 can be considered as the preparation process for calculating
information gain and has complexity O(M ), assuming that we use counting sort
(complexity O(M )) with a pre-sorted list in line 15; it is easy to see that lines
16–29 take time O(N ).

Example 2 Given positive and negative examples, E+, E−, with mixed type of
values on feature i, the target is to ﬁnd the literal with the best information gain
on the given feature. There are 8 positive examples, their values on feature i are
[1, 2, 3, 3, 5, 6, 6, b]. And, the values on feature i of the 5 negative examples are
[2, 4, 6, 7, a].

With the given examples and speciﬁed feature, the numbers of positive examples
and negative examples for each unique value are counted ﬁrst, which are shown
as pos, neg at right side of Table 2. Then, the preﬁx sum arrays are calculated
for computing the heuristic as psum+, psum−. Table 3 shows the information
gain for each literal, the literal(i, (cid:54)=, a) has been selected with the highest score.

4.2 Explainability

Explainability is very important for some tasks like loan approval, credit card
approval, and disease diagnosis system. Inductive logic programming provides
explicit rules for how a prediction is generated compared to black box mod-
els like those based on neural networks. To eﬃciently justify the prediction,
the FOLD-R++ outputs normal logic programs that are compatible with the
s(CASP) goal-directed answer set programming system [2]. The s(CASP) system

12

Wang and Gupta

ith feature values
1 2 3 3 5 6 6 b
2 4 6 7 a

E+
E−

1
value
1
pos
psum+ 1
neg
0
psum− 0

2
1
2
1
1

3
2
4
0
1

4
0
4
1
2

5
1
5
0
2

6
2
7
1
3

b
a
7
1
0
0
7 na na
1
0
1
4 na na

Table 2. Left: Examples and values on ith feature. Right: positive/negative count and
preﬁx sum on each value

Info Gain
5

4

2

3

1

a
value
≤ value −∞ −∞ -0.619 -0.661 -0.642 -0.616 -0.661
na
> value -0.664 -0.666 −∞ −∞ −∞ −∞ −∞ na
= value
(cid:54)= value

b
na
na
−∞ −∞
-0.588 -0.627

na
na

na
na

na
na

na
na

na
na

na
na

na
na

6

7

Table 3. The info gain on ith feature with given examples

executes answer set programs in a goal-directed manner [2]. Stratiﬁed normal
logic programs output by FOLD-R++ are a special case of answer set programs.

Example 3 The “Adult Census Income” is a classical classiﬁcation task that
contains 32561 records. We treat 80% of the data as training examples and
20% as testing examples. The task is to learn the income status of individu-
als (more/less than 50K/year) based on features such as gender, age, education,
marital status, etc. FOLD-R++ generates the following program that contains
only 13 rules:

(1) income(X,’=<50k’) :- not marital_status(X,’married-civ-spouse’), not ab4(X), not ab5(X).
(2) income(X,’=<50k’) :- education_num(X,N4), N4=<12.0, capital_gain(X,N10), N10=<5013.0,

not ab6(X), not ab8(X).

(3) income(X,’=<50k’) :- occupation(X,’farming-fishing’), age(X,N0), N0>62.0, N0=<63.0,
education_num(X,N4), N4>12.0, capital_gain(X,N10), N10>5013.0.

(4) income(X,’=<50k’) :- age(X,N0), N0>65.0, education_num(X,N4), N4>12.0,

capital_gain(X,N10), N10>9386.0, N10=<10566.0.

(5) income(X,’=<50k’) :- age(X,N0), N0>35.0, fnlwgt(X,N2), N2>199136.0, education_num(X,N4),

N4>12.0, capital_gain(X,N10), N10>5013.0, hours_per_week(X,N12),
N12=<20.0.
(6) ab1(X) :- age(X,N0), N0=<20.0.
(7) ab2(X) :- education_num(X,N4), N4=<10.0, capital_gain(X,N10), N10=<7978.0.
(8) ab3(X) :- capital_gain(X,N10), N10>27828.0, N10=<34095.0.
(9) ab4(X) :- capital_gain(X,N10), N10>6849.0, not ab1(X), not ab2(X), not ab3(X).
(10) ab5(X) :- age(X,N0), N0=<27.0, education_num(X,N4), N4>12.0, capital_loss(X,N11),

N11>1974.0, N11=<2258.0.

(11) ab6(X) :- not marital_status(X,’married-civ-spouse’).
(12) ab7(X) :- occupation(X,’transport-moving’), age(X,N0), N0>39.0.
(13) ab8(X) :- education_num(X,N4), N4=<8.0, capital_loss(X,N11), N11>1672.0, N11=<1977.0,

not ab7(X).

The above program achieves 0.86 accuracy, 0.88 precision, 0.95 recall, and 0.91
F1 score. Given a new data sample, the predicted answer for this data sample
using the above logic program can be eﬃciently produced by the s(CASP) system
[2]. Since s(CASP) is query driven, an example query such as ?- income(30,

FOLD-R++ Toolset

13

Y) which checks the income status of the person with ID 30, will succeed if the
income is indeed predicted as less equal to 50K by the model represented by the
logic program above.

The s(CASP) system will also produce a justiﬁcation (a proof tree) for this
prediction query. It can even generate this proof tree in English, i.e., in a more
human understandable form [1]. The justiﬁcation tree generated for the person
with ID 30 is shown below:

?- income(30,Y).
% QUERY:I would like to know if

‘income’ holds (for 30, and Y).

ANSWER: 1 (in 2.246 ms)
JUSTIFICATION_TREE:
‘income’ holds (for 30, and ‘=<50k’), because

there is no evidence that ‘marital_status’ holds (for 30, and married-civ-spouse), and
there is no evidence that ‘ab4’ holds (for 30), because

there is no evidence that ‘capital_gain’ holds (for 30, and Var1),
with Var1 not equal 0.0, and ‘capital_gain’ holds (for 30, and 0.0).

there is no evidence that ‘ab5’ holds (for 30), because

there is no evidence that ‘age’ holds (for 30, and Var2), with Var2 not equal 18.0,
and ‘age’ holds (for 30, and 18.0), and
there is no evidence that ‘education_num’ holds (for 30, and Var3),
with Var3 not equal 7.0, and ‘age’ holds (for 30, and 18.0), justified above, and
‘education_num’ holds (for 30, and 7.0).

The global constraints hold.
BINDINGS:
Y equal ‘=<50k’

With the justiﬁcation tree, the reason for the prediction can be easily un-
derstood by human beings. The generated NLP rule-set can also be understood
by a human. If there is any unreasonable logic generated in the rule set, it can
also be modiﬁed directly by the human without retraining. Thus, any bias in
the data that is captured in the generated NLP rules can be corrected by the
human user, and the updated NLP rule-set used for making new predictions.

The RIPPER system [4] is a well-known rule-induction algorithm that gener-
ates formulas in conjunctive normal form (CNF) as an explanation of the model.
RIPPER generates 53 formulas for Example 3 and achieves 0.61 accuracy, 0.98
precision, 0.50 recall, and 0.66 F1 score. A few of the ﬁfty three rules generated
by RIPPER for this dataset are shown below.

(1) marital_status=Never-married & education_num=7.0-9.0 & workclass=Private &

hours_per_week=35.0-40.0 & capital_gain=<9999.9 & sex=Female

(2) marital_status=Never-married & capital_gain=<9999.9 & education_num=7.0-9.0 &

hours_per_week=35.0-40.0 & relationship=Own-child

(3) marital_status=Never-married & capital_gain=<9999.9 & education_num=7.0-9.0 &

hours_per_week=35.0-40.0 & race=White & age=22.0-26.0

(4) marital_status=Never-married & capital_gain=<9999.9 & education_num=7.0-9.0 &

hours_per_week=24.0-35.0
... ...

(50) education_num=7.0-9.0 & age=26.0-30.0 & fnlwgt=177927.0-196123.0 & workclass=Private
(51) relationship=Not-in-family & capital_gain=<9999.9 & hours_per_week=35.0-40.0 &

sex=Female & education=Assoc-voc

(52) education_num=<7.0 & workclass=Private & fnlwgt=260549.8-329055.0
(53) relationship=Not-in-family & capital_gain=<9999.9 & hours_per_week=35.0-40.0 &

education_num=11.0-13.0 & occupation=Adm-clerical

Generally, a set of default rules is a more succinct description of a given con-
cept compared to a set of CNFs, especially when nested exceptions are allowed

14

Wang and Gupta

in the default rules. For this reason, we believe that FOLD-R++ performs better
than RIPPER on large datasets, as shown later.

5 Experiments

In this section, we present our experiments on UCI standard benchmarks [12].2
The XGBoost Classiﬁer is a popular classiﬁcation model and used as a baseline in
our experiment. We used simple settings for XGBoost classiﬁer without limiting
its performance. However, XGBoost cannot deal with mixed type (numerical
and categorical) of examples directly. One-hot encoding has been used for data
preparation. We use precision, recall, accuracy, F1 score, and execution time to
compare the results.

FOLD-R++ does not require any encoding before training. We implemented
FOLD-R++ with Python (the original FOLD-R implementation is in Java). To
make inferences using the generated rules, we developed a simple logic program-
ming interpreter for our application that is part of the FOLD-R++ system. Note
that the generated programs are stratiﬁed, so implementing an interpreter for
such a restricted class in Python is relatively easy. However, for obtaining the
justiﬁcation/proof tree, or for translating the NLP rules into equivalent English
text, one must use the s(CASP) system.

The time complexity for computing information gain on a feature is sig-
niﬁcantly reduced in FOLD-R++ due to the use of preﬁx-sum, resulting in
rather large improvements in eﬃciency. For the credit-a dataset with only 690
instances, the new FOLD-R++ algorithm is a hundred times faster than the
original FOLD-R. The hyper-parameter ratio is simply set as 0.5 for all the ex-
periments. All the learning experiments have been conducted on a desktop with
Intel i5-10400 CPU @ 2.9GHz and 32 GB ram. To measure performance met-
rics, we conducted 10-fold cross-validation on each dataset and the average of
accuracy, precision, recall, F1 score and execution time are presented (Table 4,
Table 5, Table 6). The best performer is highlighted in boldface.

Experiments reported in Table 4 are based on our re-implementation of
FOLD-R in Python. The Python re-implementation is 6 to 10 times faster
than Shakerin’s original Java implementation according to the common tested
datasets. However, the re-implementation still lacks eﬃciency on large datasets
due to the original design. The FOLD-R experiments on the Adult Census In-
come and the Credit Card Approval datasets are performed with improvements
in heuristic calculation while for other datasets the method of calculation remains
as in Shakerin’s original design. In these two cases, the eﬃciency improves sig-
niﬁcantly but the output is identical to original FOLD-R. The average execution
time of these two datasets is still quite large, however, we use polynomial regres-
sion to estimate it. The estimated average execution time of the Adult Census
Income dataset ranges from 4 to 7 days, and a random single test took 4.5 days.
The estimated execution time of the Credit Card Approval dataset ranges from

2 The FOLD-R++ system is available at https://github.com/hwd404/FOLD-R-PP.

Data Set

FOLD-R

FOLD-R++

FOLD-R++ Toolset

15

#Rows #Cols Acc. Prec. Rec. F1 T(ms) #Rules Acc. Prec. Rec. F1 T(ms) #Rules

Name
120
acute
704
autism
699
breast-w
1728
cars
690
credit-a
336
ecoli
270
heart
351
ionosphere
400
kidney
kr vs. kp
3196
mushroom 8124
435
voting
adult
32561
credit card 30000

1

0.99

0.99

7
0.98 0.99
1
18 0.95 0.97 0.97 0.96
0.95 0.96 0.96 0.96
10
0.99 0.99
7
1 0.99
0.82 0.83 0.85 0.84
16
0.93 0.92 0.92 0.91
9
0.74 0.75 0.80 0.77
14
0.89 0.90 0.93 0.91
35
0.98 0.99 0.98 0.99
25
37
0.99 0.99 0.99 0.99
23
17
15
24

2.6
24.3
10.2
12.2
10.0
11.4
11.7
12.0
5.0
18.4
8.0
0.95 0.93 0.94 0.93
10.5
0.77 0.94 0.74 0.83 4+ days 595.5 0.84 0.86 0.95 0.90 10,066 16.7
0.64 0.87 0.63 0.73 24+ days 514.9 0.82 0.83 0.96 0.89 21,349 19.1

2.3
2.0
1 0.99 0.99
62
18.4 0.93 0.96 0.95 0.95
32
0.95 0.97 0.95 0.96
11.2
17.9
50
0.97 0.98
0.97
33.4 0.85 0.92 0.79 0.85 111
34
7.7 0.94 0.95 0.92 0.93
15.9 0.79 0.80 0.83 0.80
40
5.9 0.91 0.93 0.93 0.93 385
5.7
28
16.8 0.99 0.99 0.99 0.99 319
523
8.6
16
13.7

12
321
373
134
11,316
686
888
9,297
451
1,259
1,556
96

0.95 0.92 0.95 0.93

0.98 0.99

0.99 1

1

1

1

1

1

1

1

1

Table 4. Comparison of FOLD-R and FOLD-R++ on various Datasets

24 to 55 days. For small datasets, the classiﬁcation performance are similar,
however, wrt execution time, the FOLD-R++ algorithm is order of magnitude
faster than (the re-implemented Python version of) FOLD-R. For large datasets,
FOLD-R++ signiﬁcantly improves the eﬃciency, classiﬁcation performance, and
explainability over FOLD-R. For the Adult Census Income and the Credit Card
Approval datasets, the average number of rules generated by FOLD-R are over
500 while the number for FOLD-R++ is less than 20.

Data Set

RIPPER

FOLD-R++

#Rows #Cols Acc. Prec. Rec. F1 T(ms) #Rules Acc. Prec. Rec. F1 T(ms) #Rules

Name
120
acute
704
autism
699
breast-w
1728
cars
690
credit-a
336
ecoli
270
heart
351
ionosphere
400
kidney
kr vs. kp
3196
mushroom 8124
435
voting
adult
32561
credit card 30000
rain in aus 145460

1

0.84 0.91
0.93
7
0.93 0.96 0.95 0.95
18
0.91 0.97 0.89 0.93
10
7
0.99 0.99 0.99 0.99
16 0.89 0.94 0.86 0.90
0.90 0.91 0.86 0.88
9
0.73 0.82 0.69 0.72
14
0.81 0.85 0.86 0.85
35
0.98 0.99 0.98 0.99
25
37
0.99 0.99 0.99 0.99
23
17
15
24
24

2.6
2.0 0.99 1 0.99 0.99
24.3
9.6
0.93 0.96 0.95 0.95
10.2
7.7 0.95 0.97 0.95 0.96
12.2
0.97
15.4
0.97 0.98
1
10.0
11.1
0.85 0.92 0.79 0.85
11.4
8.0 0.94 0.95 0.92 0.93
11.7
6.2 0.79 0.80 0.83 0.80
12.0
9.9 0.91 0.93 0.93 0.93
5.0
0.99 1
5.7
0.98 0.99
18.4
8.1
0.99 0.99 0.99 0.99
8.0
1
8.0
4.3 0.95 0.92 0.95 0.93
10.5
0.94 0.92 0.92 0.92
46.9 0.84 0.86 0.95 0.90 10,066 16.7
0.70 0.96 0.63 0.76
0.77 0.87 0.83 0.85
38.4 0.82 0.83 0.96 0.89 21,349 19.1
0.65 0.93 0.57 0.71 2,850,997 175.4 0.78 0.87 0.84 0.85 223,116 40.5

73
444
267
379
972
494
338
1,431
451
553
795
146
59,505
47,422

2.3
62
32
50
111
34
40
385
28
319
523
16

1

1

1

1

1

1

1

Table 5. Comparison of RIPPER and FOLD-R++ on various Datasets

The RIPPER system is another rule-induction algorithm that generates for-
mulas in conjunctive normal form as an explanation of the model. As Table 5
shows, FOLD-R++ system’s performance is comparable to RIPPER, however,
it signiﬁcantly outperforms RIPPER on large datasets (Rain in Australia [taken

16

Wang and Gupta

from Kaggle], Adult Census Income, Credit Card Approval). FOLD-R++ gen-
erates much smaller numbers of rules for these large datasets.

Performance of the XGBoost system and FOLD-R++ is compared in table
6. The XGBoost Classiﬁer employs a decision tree ensemble method for classi-
ﬁcation tasks and provides quite good performance. FOLD-R++ almost always
spends less time to ﬁnish learning compared to XGBoost classiﬁer, especially for
the (large) Adult income census dataset where numerical features have many
unique values. For most datasets, FOLD-R++ can achieve equivalent scores.
FOLD-R++ achieves higher scores on ecoli dataset. For the credit card dataset,
the baseline XGBoost model failed training due to 32 GB memory limitation,
but FOLD-R++ performed well.

Data Set

XGBoost.Classiﬁer

FOLD-R++

#Rows #Cols Acc. Prec. Rec. F1 T(ms) Acc. Prec. Rec. F1 T(ms)

Name
120
acute
704
autism
699
breast-w
1728
cars
690
credit-a
336
ecoli
270
heart
351
ionosphere
400
kidney
kr vs. kp
3196
mushroom 8124
voting
435
32561
adult
credit card 30000
rain in aus 145460

1

1

1

1

1

1

1

1

0.99
0.99 0.99
35
0.95 0.96 0.97 0.97
76
0.96 0.97 0.96 0.97
78
0.97 0.98
0.98
77
368
0.84 0.92 0.79 0.84
165 0.96 0.95 0.94 0.95
0.79 0.79 0.83 0.81
112
0.88 0.86 0.96 0.90 1,126 0.92 0.93 0.94 0.93
0.98 0.98 0.98 0.98
0.98 0.99
0.99 0.99 0.99 0.99
0.99 0.99 0.99 0.99

7
1
18 0.97 0.98 0.98 0.97
10
0.95 0.97 0.96 0.96
1
7
16 0.85 0.83 0.83 0.83
9
0.76 0.76 0.62 0.68
14 0.80 0.81 0.83 0.81
35
25
37
23
17
15 0.86 0.88 0.94 0.91 274,655 0.84 0.86 0.95 0.90 10,069
24
0.82 0.83 0.96 0.89 21,349
24 0.83 0.84 0.95 0.89 285,307 0.78 0.87 0.84 0.85 279,320

126 0.99 1
210
378
49

2.5
47
28
48
100
28
44
392
27
361
476
16

0.95 0.94 0.94 0.94

0.95 0.94 0.95 0.94

1

1

1

1

1

1

1

1

-

-

-

-

-

Table 6. Comparison of XGBoost and FOLD-R++ on various Datasets

6 Related Work and Conclusion

ALEPH [25] is one of the most popular ILP system, which induces theories by
using bottom-up generalization search. However, it cannot deal with numeric
features and its specialization step is manual, there is no automation option.
Takemura and Inoue’s method [26] relies on tree-ensembles to generate explain-
able rule sets with pattern mining techniques. Its performance depends on the
tree-ensemble model. While their algorithm advances the state of the art, it may
not be scalable as it is exponential in the number of valid rules.

A survey of ILP can be found in [16]. Rule extraction from statistical Ma-
chine Learning models has been a long-standing goal of the community. These

FOLD-R++ Toolset

17

algorithms are classiﬁed into two categories: 1) Pedagogical (i.e., learning sym-
bolic rules from black-box classiﬁers without opening them); and 2) Decomposi-
tional (i.e., to open the classiﬁer and look into the internals). TREPAN [5] is a
successful pedagogical algorithm that learns decision trees from neural networks.
SVM+Prototypes [17] is a decompositional rule extraction algorithm that makes
use of KMeans clustering to extract rules from SVM classiﬁers by focusing on
support vectors. Another rule extraction technique that is gaining attention re-
cently is “RuleFit” [6]. RuleFit learns a set of weighted rules from ensemble of
shallow decision trees combined with original features. In the ILP community
also, researchers have tried to combine statistical methods with ILP techniques.
Support Vector ILP [15] uses ILP hypotheses as the kernel in dual form of the
SVM algorithm. kFOIL [10] learns an incremental kernel for the SVM algo-
rithm using a FOIL-style specialization. nFOIL [9] integrates the Naive-Bayes
algorithm with FOIL. The advantage of our research over these is that we gener-
ate logic programs containing negation-as-failure that correspond closely to the
human thought process. Thus, the descriptions are more concise. Second, the
greedy nature of our clause search guarantees scalability. ILASP [11] is another
pioneering ILP system that learns answer set programs. ILASP can learn non-
stratiﬁed programs, however, it requires a set of rules to describe the hypothesis
space. In contrast, the FOLD-R++ algorithm only needs the target predicate’s
name.

In this paper we presented an eﬃcient and highly scalable algorithm, FOLD-
R++, to induce default theories represented as a normal logic program. The
resulting normal logic program has good performance wrt prediction and justiﬁ-
cation for the predicted classiﬁcation. In this new approach, unlike other machine
learning methods, the encoding of data is not needed anymore and no informa-
tion from training data is discarded. Compared with the popular classiﬁcation
system XGBoost, our new approach has similar performance in terms of accu-
racy, precision, recall, and F1-score, but better training eﬃciency. In addition,
the FOLD-R++ algorithm produces an explainable model. Predictions made
by this model can be computed eﬃciently and their justiﬁcation automatically
produced using the s(CASP) system.

The main advantage of the FOLD-R++ system is that it is an ILP system
that is competitive with main-stream machine learning algorithms (such as XG-
Boost). Almost all ILP systems (except RIPPER) are not competitive with main-
stream machine learning systems. However, as we showed in Section5, FOLD-
R++ signiﬁcantly outperforms the RIPPER system on really large datasets.

Acknowledgement: Authors gratefully acknowledge support from NSF grants
IIS 1718945, IIS 1910131, IIP 1916206, and from Amazon Corp, Atos Corp and
US DoD. We are grateful to Joaquin Arias and the s(CASP) team for their work
on providing facilities for generating the justiﬁcation tree and English encoding
of rules in s(CASP).

18

Wang and Gupta

References

1. Arias, J., Carro, M., Chen, Z., Gupta, G.: Justiﬁcations for goal-directed constraint
answer set programming. In: Proceedings 36th International Conference on Logic
Programming (Technical Communications). EPTCS, vol. 325, pp. 59–72 (2020)
2. Arias, J., Carro, M., Salazar, E., Marple, K., Gupta, G.: Constraint answer set pro-
gramming without grounding. Theory and Practice of Logic Programming 18(3-4),
337–354 (2018)

3. Chen, T., Guestrin, C.: XGBoost: A scalable tree boosting system. In: Proceedings

of the 22Nd ACM SIGKDD. pp. 785–794. KDD ’16 (2016)

4. Cohen, W.W.: Fast eﬀective rule induction. In: Proc. of the 12th ICML. pp. 115–
123. ICML’95, Morgan Kaufmann Publishers Inc., San Francisco, CA, USA (1995),
http://dl.acm.org/citation.cfm?id=3091622.3091637

5. Craven, M.W., Shavlik, J.W.: Extracting tree-structured representations of trained
networks. In: Proceedings of the 8th International Conference on Neural Informa-
tion Processing Systems. pp. 24–30. NIPS’95, MIT Press, Cambridge, MA, USA
(1995)

6. Friedman, J.H., Popescu, B.E., et al.: Predictive learning via rule ensembles. The

Annals of Applied Statistics 2(3), 916–954 (2008)

7. Gelfond, M., Kahl, Y.: Knowledge representation, reasoning, and the design of
intelligent agents: The answer-set programming approach. Cambridge University
Press (2014)

8. Gunning,

D.:

Explainable

artiﬁcial

intelligence

(XAI)

(2015),

https://www.darpa.mil/program/explainable-artiﬁcial-intelligence

9. Landwehr, N., Kersting, K., Raedt, L.D.: nFOIL: Integrating na¨ıve bayes and
FOIL. In: Proceedings, The Twentieth National Conference on Artiﬁcial Intelli-
gence and the Seventeenth Innovative Applications of Artiﬁcial Intelligence Con-
ference, July 9-13, 2005, Pittsburgh, Pennsylvania, USA. pp. 795–800 (2005)
10. Landwehr, N., Passerini, A., Raedt, L.D., Frasconi, P.: kFOIL: Learning simple
relational kernels. In: Proceedings, The Twenty-First National Conference on Ar-
tiﬁcial Intelligence and the Eighteenth Innovative Applications of Artiﬁcial Intel-
ligence Conference, July 16-20, 2006, MA, USA. pp. 389–394 (2006)

11. Law, M.: Inductive learning of answer set programs. Ph.D. thesis, Imperial College

London, UK (2018)

12. Lichman, M.: UCI, Machine Learning Repository, http://archive.ics.uci.edu/ml

(2013)

13. Lloyd, J.: Foundations of Logic Programming. Springer, 2nd Ext. Ed. (1987)
14. Muggleton, S.: Inductive logic programming. New Gen. Comput. 8(4) (Feb 1991)
15. Muggleton, S., Lodhi, H., Amini, A., Sternberg, M.J.E.: Support vector inductive
logic programming. In: Hoﬀmann, A., Motoda, H., Scheﬀer, T. (eds.) Discovery
Science. Springer Berlin Heidelberg, Berlin, Heidelberg (2005)

16. Muggleton, S., Raedt, L., Poole, D., Bratko, I., Flach, P., Inoue, K., Srinivasan,

A.: ILP turns 20. Mach. Learn. 86(1), 3–23 (Jan 2012)

17. N´u˜nez, H., Angulo, C., Catal`a, A.: Rule extraction from support vector machines.
In: In Proceedings of European Symposium on Artiﬁcial Neural Networks. pp.
107–112 (2002)

18. Plotkin, G.D.: A further note on inductive generalization, in machine intelligence,

volume 6, pages 101-124 (1971)

19. Quinlan, J.R.: Learning logical deﬁnitions from relations. Machine Learning 5,

239–266 (1990)

FOLD-R++ Toolset

19

20. Quinlan, J.R.: C4.5: Programs for Machine Learning. Morgan Kaufmann Publish-

ers Inc., San Francisco, CA, USA (1993)

21. Reiter, R.: A logic for default reasoning. Artiﬁcial Intelligence 13(1-2), 81–132

(1980)

22. Sakama, C.: Induction from answer sets in nonmonotonic logic programs. ACM

Trans. Comput. Log. 6(2), 203–231 (2005)

23. Shakerin, F.: Logic Programming-based Approaches in Explainable AI and Natural
Language Processing. Ph.D. thesis (2020), Department of Computer Science, The
University of Texas at Dallas

24. Shakerin, F., Salazar, E., Gupta, G.: A new algorithm to automate inductive learn-

ing of default theories. TPLP 17(5-6), 1010–1026 (2017)

25. Srinivasan, A.: The Aleph Manual,

https://www.cs.ox.ac.uk/activities/programinduction/Aleph/aleph.html (2001)
26. Takemura, A., Inoue, K.: Generating explainable rule sets from tree-ensemble learn-
ing methods by answer set programming. Electronic Proceedings in Theoretical
Computer Science 345, 127–140 (Sep 2021)

27. Wikipedia contributors: Preﬁx sum Wikipedia, the free encyclopedia (2021),
https://en.wikipedia.org/wiki/Preﬁx sum, online; accessed 5 October, 2021
28. Zeng, Q., Patel, J.M., Page, D.: Quickfoil: Scalable inductive logic programming.

Proc. VLDB Endow. 8(3), 197–208 (Nov 2014)

