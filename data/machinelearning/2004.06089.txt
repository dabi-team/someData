0
2
0
2

r
p
A
5
2

]

G
L
.
s
c
[

4
v
9
8
0
6
0
.
4
0
0
2
:
v
i
X
r
a

Published as a conference paper at ICLR 2020

THINKING WHILE MOVING: DEEP REINFORCEMENT
LEARNING WITH CONCURRENT CONTROL

Ted Xiao1, Eric Jang1, Dmitry Kalashnikov1, Sergey Levine1,2, Julian Ibarz1,
Karol Hausman1∗, Alexander Herzog3∗
1Google Brain, 2UC Berkeley, 3X
{tedxiao, ejang, dkalashnikov, slevine, julianibarz, karolhausman}@google.com,
alexherzog@x.team

ABSTRACT

We study reinforcement learning in settings where sampling an action from the
policy must be done concurrently with the time evolution of the controlled system,
such as when a robot must decide on the next action while still performing the pre-
vious action. Much like a person or an animal, the robot must think and move at
the same time, deciding on its next action before the previous one has completed.
In order to develop an algorithmic framework for such concurrent control prob-
lems, we start with a continuous-time formulation of the Bellman equations, and
then discretize them in a way that is aware of system delays. We instantiate this
new class of approximate dynamic programming methods via a simple architec-
tural extension to existing value-based deep reinforcement learning algorithms.
We evaluate our methods on simulated benchmark tasks and a large-scale robotic
grasping task where the robot must “think while moving”. Videos are available at
https://sites.google.com/view/thinkingwhilemoving.

1

INTRODUCTION

In recent years, Deep Reinforcement Learning (DRL) methods have achieved tremendous suc-
cess on a variety of diverse environments, including video games (Mnih et al., 2015), zero-sum
games (Silver et al., 2016), robotic grasping (Kalashnikov et al., 2018), and in-hand manipulation
tasks (OpenAI et al., 2018). While impressive, all of these examples use a blocking observe-think-
act paradigm: the agent assumes that the environment will remain static while it thinks, so that
its actions will be executed on the same states from which they were computed. This assumption
breaks in the concurrent real world, where the environment state evolves substantially as the agent
processes observations and plans its next actions. As an example, consider a dynamic task such as
catching a ball: it is not possible to pause the ball mid-air while waiting for the agent to decide on the
next control to command. In addition to solving dynamic tasks where blocking models would fail,
thinking and acting concurrently can provide beneﬁts such as smoother, human-like motions and the
ability to seamlessly plan for next actions while executing the current one. Despite these potential
beneﬁts, most DRL approaches are mainly evaluated in blocking simulation environments. Block-
ing environments make the assumption that the environment state will not change between when the
environment state is observed and when the action is executed. This assumption holds true in most
simulated environments, which encompass popular domains such as Atari (Mnih et al., 2013) and
Gym control benchmarks (Brockman et al., 2016). The system is treated in a sequential manner: the
agent observes a state, freezes time while computing an action, and ﬁnally applies the action and
unfreezes time. However, in dynamic real-time environments such as real-world robotics, the syn-
chronous environment assumption is no longer valid. After observing the state of the environment
and computing an action, the agent often ﬁnds that when it executes an action, the environment
state has evolved from what it had initially observed; we consider this environment a concurrent
environment.

In this paper, we introduce an algorithmic framework that can handle concurrent environments in
the context of DRL. In particular, we derive a modiﬁed Bellman operator for concurrent MDPs and

∗Indicates equal contribution.

1

 
 
 
 
 
 
Published as a conference paper at ICLR 2020

present the minimal set of information that we must augment state observations with in order to
recover blocking performance with Q-learning. We introduce experiments on different simulated
environments that incorporate concurrent actions, ranging from common simple control domains to
vision-based robotic grasping tasks. Finally, we show an agent that acts concurrently in a real-world
robotic grasping task is able to achieve comparable task success to a blocking baseline while acting
49% faster.

2 RELATED WORK

Minimizing Concurrent Effects Although real-world robotics systems are inherently concurrent,
it is sometimes possible to engineer them into approximately blocking systems. For example, using
low-latency hardware (Abbeel et al., 2006) and low-footprint controllers (Cruz et al., 2017) mini-
mizes the time spent during state capture and policy inference. Another option is to design actions
to be executed to completion via closed-loop feedback controllers and the system velocity is de-
celerated to zero before a state is recorded (Kalashnikov et al., 2018). In contrast to these works,
we tackle the concurrent action execution directly in the learning algorithm. Our approach can be
applied to tasks where it is not possible to wait for the system to come to rest between deciding new
actions.

Algorithmic Approaches Other works utilize algorithmic modiﬁcations to directly overcome the
challenges of concurrent control. Previous work in this area can be grouped into ﬁve approaches: (1)
learning policies that are robust to variable latencies (Tan et al., 2018), (2) including past history such
as frame-stacking (Haarnoja et al., 2018), (3) learning dynamics models to predict the future state at
which the action will be executed (Firoiu et al., 2018; Amiranashvili et al., 2018), (4) using a time-
delayed MDP framework (Walsh et al., 2007; Firoiu et al., 2018; Schuitema et al., 2010; Ramstedt &
Pal, 2019), and (5) temporally-aware architectures such as Spiking Neural Networks (Vasilaki et al.,
2009; Fr´emaux et al., 2013), point processes (Upadhyay et al., 2018; Li et al., 2018), and adaptive
skip intervals (Neitz et al., 2018). In contrast to these works, our approach is able to (1) optimize for
a speciﬁc latency regime as opposed to being robust to all of them, (2) consider the properties of the
source of latency as opposed to forcing the network to learn them from high-dimensional inputs, (3)
avoid learning explicit forward dynamics models in high-dimensional spaces, which can be costly
and challenging, (4) consider environments where actions are interrupted as opposed to discrete-
time time-delayed environments where multiple actions are queued and each action is executed
until completion. A recent work Ramstedt & Pal (2019) extends 1-step constant delayed MDPs to
actor-critic methods on high dimension image based tasks. The approaches in (5) show promise in
enabling asynchronous agents, but are still active areas of research that have not yet been extended
to high-dimensional, image-based robotic tasks.

Continuous-time Reinforcement Learning While previously mentioned related works largely
operate in discrete-time environments, framing concurrent environments as continuous-time sys-
tems is a natural framework to apply. In the realm of continuous-time optimal control, path integral
solutions (Kappen, 2005; Theodorou et al., 2010) are linked to different noise levels in system dy-
namics, which could potentially include latency that results in concurrent properties. Finite differ-
ences can approximate the Bellman update in continuous-time stochastic control problems (Munos
& Bourgine, 1998) and continuous-time temporal difference learning methods (Doya, 2000) can
utilize neural networks as function approximators (Coulom, 2002). The effect of time-discretization
(converting continuous-time environments to discrete-time environments) is studied in Tallec et al.
(2019), where the advantage update is scaled by the time discretization parameter. While these
approaches are promising, it is untested how these methods may apply to image-based DRL prob-
lems. Nonetheless, we build on top of many of the theoretical formulations in these works, which
motivate our applications of deep reinforcement learning methods to more complex, vision-based
robotics tasks.

2

Published as a conference paper at ICLR 2020

3 VALUE-BASED REINFORCEMENT LEARNING IN CONCURRENT

ENVIRONMENTS

In this section, we ﬁrst introduce the concept of concurrent environments, and then describe the
preliminaries necessary for discrete- and continuous-time RL formulations. We then describe the
MDP modiﬁcations sufﬁcient to represent concurrent actions and ﬁnally, present value-based RL
algorithms that can cope with concurrent environments.

The main idea behind our method is simple and can be implemented using small modiﬁcations to
standard value-based algorithms. It centers around adding additional information to the learning
algorithm (in our case, adding extra information about the previous action to a Q-function) that
allows it to cope with concurrent actions. Hereby, we provide theoretical justiﬁcation on why these
modiﬁcations are necessary and we specify the details of the algorithm in Alg. 1.

While concurrent environments affect DRL methods beyond model-free value-based RL, we focus
our scope on model-free value-based methods due to their attractive sample-efﬁciency and off-policy
properties for real-world vision-based robotic tasks.

3.1 CONCURRENT ACTION ENVIRONMENTS

In blocking environments (Figure 4a in the Appendix), actions are executed in a sequential blocking
fashion that assumes the environment state does not change between when state is observed and
when actions are executed. This can be understood as state capture and policy inference being
viewed as instantaneous from the perspective of the agent. In contrast, concurrent environments
(Figure 4b in the Appendix) do not assume a ﬁxed environment during state capture and policy
inference, but instead allow the environment to evolve during these time segments.

3.2 DISCRETE-TIME REINFORCEMENT LEARNING PRELIMINARIES

We use standard reinforcement learning formulations in both discrete-time and continuous-time
settings (Sutton & Barto, 1998). In the discrete-time case, at each time step i, the agent receives
state si from a set of possible states S and selects an action ai from some set of possible actions A
according to its policy π, where π is a mapping from S to A. The environment returns the next state
si+1 sampled from a transition distribution p(si+1|si, ai) and a reward r(si, ai). The return for a
given trajectory of states and actions is the total discounted return from time step i with discount
factor γ ∈ (0, 1]: Ri = (cid:80)∞
k=0 γkr(si+k, ai+k). The goal of the agent is to maximize the expected
return from each state si. The Q-function for a given stationary policy π gives the expected return
when selecting action a at state s: Qπ(s, a) = E[Ri|si = s, ai = a]. Similarly, the value function
gives expected return from state s: V π(s) = E[Ri|si = s].
The default blocking environment formulation is detailed in Figure 1a.

3.3 VALUE FUNCTIONS AND POLICIES IN CONTINUOUS TIME

For the continuous-time case, we start by formalizing a continuous-time MDP with the differential
equation:

ds(t) = F (s(t), a(t))dt + G(s(t), a(t))dβ

(1)

where S = Rd is a set of states, A is a set of actions, F : S × A → S and G : S × A → S
describe the stochastic dynamics of the environment, and β is a Wiener process (Ross et al., 1996).
In the continuous-time setting, ds(t) is analogous to the discrete-time p, deﬁned in Section 3.2.
Continuous-time functions s(t) and ai(t) specify the state and i-th action taken by the agent. The
agent interacts with the environment through a state-dependent, deterministic policy function π and
the return R of a trajectory τ = (s(t), a(t)) is given by (Doya, 2000):

R(τ ) =

(cid:90) ∞

t=0

γtr(s(t), a(t))dt,

3

(2)

Published as a conference paper at ICLR 2020

Figure 1: Shaded nodes represent observed variables and unshaded nodes represent unobserved
random variables. (a): In “blocking” MDPs, the environment state does not change while the agent
records the current state and selects an action. (b): In “concurrent” MDPs, state and action dynamics
are continuous-time stochastic processes s(t) and ai(t). At time t, the agent observes the state of
the world s(t), but by the time it selects an action ai(t + tAS), the previous continuous-time action
function ai−1(t − H + tAS(cid:48)(cid:48) ) has “rolled over” to an unobserved state s(t + tAS). An agent that
concurrently selects actions from old states while in motion may need to interrupt a previous action
before it has ﬁnished executing its current trajectory.

which leads to a continuous-time value function (Tallec et al., 2019):

V π(s(t)) = Eτ ∼π[R(τ )|s(t)]

= Eτ ∼π

(cid:20)(cid:90) ∞

t=0

γtr(s(t), a(t))dt

(cid:21)

,

and similarly, a continuous Q-function:

Qπ(s(t), a, t, H) = Ep

(cid:34)(cid:90) t(cid:48)=t+H

t(cid:48)=t

γt(cid:48)−tr(s(t(cid:48)), a(t(cid:48)))dt(cid:48) + γH V π(s(t + H))

(cid:35)
,

(3)

(4)

where H is the constant sampling period between state captures (i.e.
the duration of an action
trajectory) and a refers to the continuous action function that is applied between t and t + H. The
expectations are computed with respect to stochastic process p deﬁned in Eq. 1.

3.4 CONCURRENT ACTION MARKOV DECISION PROCESSES

We consider Markov Decision Processes (MDPs) with concurrent actions, where actions are not
executed to full completion. More speciﬁcally, concurrent action environments capture system state
while the previous action is still executed. After state capture, the policy selects an action that is
executed in the environment regardless of whether the previous action has completed, as shown in
Figure 4 in the Appendix. In the continuous-time MDP case, concurrent actions can be considered
as horizontally translating the action along the time dimension (Walsh et al., 2007), and the effect of
concurrent actions is illustrated in Figure 1b. Although we derive Bellman Equations for handling
delays in both continuous and discrete-time RL, our experiments extend existing DRL implementa-
tions that are based on discrete time.

3.5 VALUE-BASED CONCURRENT REINFORCEMENT LEARNING ALGORITHMS IN

CONTINUOUS AND DISCRETE-TIME

We start our derivation from this continuous-time reinforcement learning standpoint, as it allows us
to easily characterize the concurrent nature of the system. We then demonstrate that the conclusions
drawn for the continuous case also apply to the more commonly-used discrete setting that we then
use in all of our experiments.

Continuous Formulation In order to further analyze the concurrent setting, we introduce the
following notation. As shown in Figure 1b, an agent selects N action trajectories during an episode,
a1, ..., aN , where each ai(t) is a continuous function generating controls as a function of time t.
Let tAS be the time duration of state capture, policy inference and any additional communication
latencies. At time t, an agent begins computing the i-th trajectory ai(t) from state s(t), while

4

Published as a conference paper at ICLR 2020

concurrently executing the previous selected trajectory ai−1(t) over the time interval (t − H +
tAS, t + tAS). At time t + tAS, where t ≤ t + tAS ≤ t + H, the agent switches to executing actions
from ai(t). The continuous-time Q-function for the concurrent case from Eq. 4 can be expressed as
following:

Qπ(s(t), ai−1, ai, t, H) = Ep

(cid:34)(cid:90) t(cid:48)=t+tAS

t(cid:48)=t

γt(cid:48)−tr(s(t(cid:48)), ai−1(t(cid:48)))dt(cid:48)

(cid:35)

(cid:125)

(cid:124)

+ Ep

(cid:123)(cid:122)
Executing action trajectory ai−1(t) until t + tAS
(cid:35)
(cid:34)(cid:90) t(cid:48)=t+H

γt(cid:48)−tr(s(t(cid:48)), ai(t(cid:48)))dt(cid:48)

t(cid:48)=t+tAS

(cid:124)

(cid:123)(cid:122)
Executing action trajectory ai(t) until t + H

(cid:125)

+ Ep
(cid:124)

(cid:2)γH V π(s(t + H))(cid:3)
(cid:123)(cid:122)
(cid:125)
Value function at t + H

(5)

The ﬁrst two terms correspond to expected discounted returns for executing the action trajectory
ai−1(t) from time (t, t + tAS) and the trajectory ai(t) from time (t + tAS, t + tAS + H). We
can obtain a single-sample Monte Carlo estimator ˆQ by sampling random functions values p, which
simply correspond to policy rollouts:

ˆQπ(s(t), ai−1, ai, t, H) =

(cid:90) t(cid:48)=t+tAS

γt(cid:48)−tr(s(t(cid:48)), ai−1(t(cid:48)))dt(cid:48)+

t(cid:48)=t
(cid:34)(cid:90) t(cid:48)=t+H

t(cid:48)=t+tAS

γtAS

(cid:35)
γt(cid:48)−t−tAS r(s(t(cid:48)), ai(t(cid:48)))dt(cid:48) + γH−tAS V π(s(t + H))

(6)

Next, for the continuous-time case, let us deﬁne a new concurrent Bellman backup operator:

T ∗
c

ˆQ(s(t), ai−1, ai, t, tAS) =

(cid:90) t(cid:48)=t+tAS

t(cid:48)=t

γt(cid:48)−tr(s(t(cid:48)), ai−1(t(cid:48)))dt(cid:48)+

γtAS max
ai+1

Ep ˆQπ(s(t + tAS), ai, ai+1, t + tAS, H − tAS).

(7)

In addition to expanding the Bellman operator to take into account concurrent actions, we demon-
strate that this modiﬁed operator maintain its contraction properties that are crucial for Q-learning
convergence.
Lemma 3.1. The concurrent continuous-time Bellman operator is a contraction.

Proof. See Appendix A.2.

Discrete Formulation In order to simplify the notation for the discrete-time case where the dis-
tinction between the action function ai(t) and the value of that function at time step t, ai(t), is not
necessary, we refer to the current state, current action, and previous action as st, at, at−1 respec-
tively, replacing subindex i with t. Following this notation, we deﬁne the concurrent Q-function for
the discrete-time case:

Qπ(st, at−1, at, t, tAS, H) =

r(st, at−1)+γ

tAS

H Ep(st+tAS |st,at−1)Qπ(st+tAS , at, at+1, t + tAS, tAS(cid:48), H − tAS)
(8)

Where tAS(cid:48) is the “spillover duration” for action at beginning execution at time t + tAS (see Fig-
ure 1b). The concurrent Bellman operator, speciﬁed by a subscript c, is as follows:

5

Published as a conference paper at ICLR 2020

T ∗
c Q(st, at−1, at, t, tAS, H) =

r(st, at−1) + γ

tAS
H max
at+1

Ep(st+tAS |st,at−1)Qπ(st+tAS , at, at+1, t + tAS, tAS(cid:48), H − tAS).
(9)

Similarly to the continuous-time case, we demonstrate that this Bellman operator is a contraction.
Lemma 3.2. The concurrent discrete-time Bellman operator is a contraction.

Proof. See Appendix A.2.

We refer the reader to Appendix A.1 for more detailed derivations of the Q-functions and Bellman
operators. Crucially, Equation 9 implies that we can extend a conventional discrete-time Q-learning
framework to handle MDPs with concurrent actions by providing the Q function with values of tAS
and at−1, in addition to the standard inputs st, at, t.

3.6 DEEP Q-LEARNING WITH CONCURRENT KNOWLEDGE

While we have shown that knowledge of the concurrent system properties (tAS and at−1, as deﬁned
previously for the discrete-time case) is theoretically sufﬁcient, it is often hard to accurately predict
tAS during inference on a complex robotics system. In order to allow practical implementation of
our algorithm on a wide range of RL agents, we consider three additional features encapsulating
concurrent knowledge used to condition the Q-function: (1) Previous action (at−1), (2) Action
selection time (tAS), and (3) Vector-to-go (V T G), which we deﬁne as the remaining action to be
executed at the instant the state is measured. We limit our analysis to environments where at−1, tAS,
and V T G are all obtainable and H is held constant. See Appendix A.3 for details.

4 EXPERIMENTS

In our experimental evaluation we aim to study the following questions: (1) Is concurrent knowledge
deﬁned in Section 3.6, both necessary and sufﬁcient for a Q-function to recover the performance
of a blocking unconditioned Q-function, when acting in a concurrent environment? (2) Which
representations of concurrent knowledge are most useful for a Q-function to act in a concurrent
environment? (3) Can concurrent models improve smoothness and execution speed of a real-robot
policy in a realistic, vision-based manipulation task?

4.1 TOY FIRST-ORDER CONTROL PROBLEMS

First, we illustrate the effects of a concurrent control paradigm on value-based DRL methods through
an ablation study on concurrent versions of the standard Cartpole and Pendulum environments. We
use 3D MuJoCo based implementations in DeepMind Control Suite (Tassa et al., 2018) for both
tasks. For the baseline learning algorithm implementations, we use the TF-Agents (Guadarrama
et al., 2018) implementations of a Deep Q-Network agent, which utilizes a Feed-forward Neural
Network (FNN), and a Deep Q-Recurrent Neutral Network agent, which utilizes a Long Short-Term
Memory (LSTM) network. To approximate different difﬁculty levels of latency in concurrent envi-
ronments, we utilize different parameter combinations for action execution steps and action selection
steps (tAS). The number of action execution steps is selected from {0ms, 5ms, 25ms, or 50ms} once
at environment initialization. tAS is selected from {0ms, 5ms, 10ms, 25ms, or 50ms} either once at
environment initialization or repeatedly at every episode reset. In addition to environment parame-
ters, we allow trials to vary across model parameters: number of previous actions to store, number
of previous states to store, whether to use VTG, whether to use tAS, Q-network architecture, and
number of discretized actions. Further details are described in Appendix A.4.1.

To estimate the relative importance of different concurrent knowledge representations, we conduct
an analysis of the sensitivity of each type of concurrent knowledge representations to combinations
of the other hyperparameter values, shown in Figure 2a. While all combinations of concurrent
knowledge representations increase learning performance over baselines that do not leverage this

6

Published as a conference paper at ICLR 2020

(a) Cartpole

(b) Pendulum

Figure 2: In concurrent versions of Cartpole and Pendulum, we observe that providing the critic
with VTG leads to more robust performance across all hyperparameters. (a) Environment rewards
achieved by DQN with different network architectures [either a feedforward network (FNN) or a
Long Short-Term Memory (LSTM) network] and different concurrent knowledge features [Uncon-
ditioned, Vector-to-go (VTG), or previous action and tAS] on the concurrent Cartpole task for ev-
ery hyperparameter in a sweep, sorted in decreasing order. (b) Environment rewards achieved by
DQN with a FNN and different frame-stacking and concurrent knowledge parameters on the con-
current Pendulum task for every hyperparameter in a sweep, sorted in decreasing order. Larger
area-under-curve implies more robustness to hyperparameter choices. Enlarged ﬁgures provided in
Appendix A.5.

(a) Simulation

(b) Real

Figure 3: An overview of the robotic grasping task. A static manipulator arm attempts to grasp
objects placed in bins front of it. In simulation, the objects are procedurally generated.

information, the clearest difference stems from including VTG. In Figure 2b we conduct a similar
analysis but on a Pendulum environment where tAS is ﬁxed every environment; thus, we do not focus
on tAS for this analysis but instead compare the importance of VTG with frame-stacking previous
actions and observations. While frame-stacking helps nominally, the majority of the performance
increase results from utilizing information from VTG.

4.2 CONCURRENT QT-OPT ON LARGE-SCALE ROBOTIC GRASPING

Next, we evaluate scalability of our approach to a practical robotic grasping task. We simulate
a 7 DoF arm with an over-the-shoulder camera, where a bin in front of the robot is ﬁlled with

7

Published as a conference paper at ICLR 2020

Table 1: Large-Scale Simulated Robotic Grasping Results

Blocking
Actions

Timestep
Penalty

Yes
Yes
No
No
No
No
No

No
Yes
No
Yes
Yes
Yes
Yes

No
No
No
No
Yes
No
Yes

VTG Previous

Grasp Success

Episode Duration Action Completion

Action

No
No
No
No
No
Yes
Yes

132.09s ±5.70s
92.72% ± 1.10%
120.81s ±9.13s
91.53% ± 1.04%
122.15s ±14.6s
84.11% ± 7.61%
97.16s ±6.28s
83.77% ± 9.27%
82.98s ± 5.74s
92.55% ± 4.39%
92.70% ± 1.42%
87.15s ±4.80s
93.49% ± 1.04% 90.75s ±4.15s

92.33% ± 1.476%
89.53% ± 2.267%
43.4% ± 22.41%
34.69% ± 16.80%
47.28% ± 14.25%
50.09% ± 14.25%
49.19% ± 14.98%

Table 2: Real-World Robotic Grasping Results.

Blocking Actions VTG Grasp Success

Policy Duration

Yes
No

No
Yes

81.43%
68.60%

22.60s ±12.99s
11.52s ± 7.272s

procedurally generated objects to be picked up by the robot. A binary reward is assigned if an object
is lifted off a bin at the end of an episode. We train a policy with QT-Opt (Kalashnikov et al., 2018),
a deep Q-Learning method that utilizes the cross-entropy method (CEM) to support continuous
actions. In the blocking mode, a displacement action is executed until completion: the robot uses a
closed-loop controller to fully execute an action, decelerating and coming to rest before observing
the next state. In the concurrent mode, an action is triggered and executed without waiting, which
means that the next state is observed while the robot remains in motion. Further details of the
algorithm and experimental setup are shown in Figure 3 and explained in Appendix A.4.2.

Table 1 summarizes the performance for blocking and concurrent modes comparing unconditioned
models against the concurrent knowledge models described in Section 3.6. Our results indicate
that the VTG model acting in concurrent mode is able to recover baseline task performance of the
blocking execution unconditioned baseline, while the unconditioned baseline acting in concurrent
model suffers some performance loss. In addition to the success rate of the grasping policy, we also
evaluate the speed and smoothness of the learned policy behavior. Concurrent knowledge models are
able to learn faster trajectories: episode duration, which measures the total amount of wall-time used
for an episode, is reduced by 31.3% when comparing concurrent knowledge models with blocking
unconditioned models, even those that utilize a shaped timestep penalty that reward faster policies.
When switching from blocking execution mode to concurrent execution mode, we see a signiﬁcantly
lower action completion, measured as the ratio from executed gripper displacement to commanded
displacement, which expectedly indicates a switch to a concurrent environment. The concurrent
knowledge models have higher action completions than the unconditioned model in the concurrent
environment, which suggests that the concurrent knowledge models are able to utilize more efﬁcient
motions, resulting in smoother trajectories. The qualitative beneﬁts of faster, smoother trajectories
are drastically apparent when viewing video playback of learned policies1.

Real robot results
In addition, we evaluate qualitative policy behaviors of concurrent models
compared to blocking models on a real-world robot grasping task, which is shown in Figure 3b. As
seen in Table 2, the models achieve comparable grasp success, but the concurrent model is 49%
faster than the blocking model in terms of policy duration, which measures the total execution time
of the policy (this excludes the infrastructure setup and teardown times accounted for in episode
duration, which can not be optimized with concurrent actions). In addition, the concurrent VTG
model is able to execute smoother and faster trajectories than the blocking unconditioned baseline,
which is clear in video playback1.

1https://sites.google.com/view/thinkingwhilemoving

8

Published as a conference paper at ICLR 2020

5 DISCUSSION AND FUTURE WORK

We presented a theoretical framework to analyze concurrent systems where an agent must “think
while moving”. Viewing this formulation through the lens of continuous-time value-based rein-
forcement learning, we showed that by considering concurrent knowledge about the time delay
tAS and the previous action, the concurrent continuous-time and discrete-time Bellman operators
remained contractions and thus maintained Q-learning convergence guarantees. While more infor-
mation than tAS and previous action may be helpful, we showed that tAS and previous action (and
different representations of this information) are the sole theoretical requirements for good learning
performance. In addition, we introduced Vector-to-go (VTG), which incorporates the remaining
previous action to be executed, as an alternative representation for information about the concurrent
system that previous action and tAS contain.

Our theoretical ﬁndings were supported by experimental results on Q-learning models acting in
simulated control tasks that were engineered to support concurrent action execution. We conducted
large-scale ablation studies on toy task concurrent 3D Cartpole and Pendulum environments, across
model parameters as well as concurrent environment parameters. Our results indicated that VTG
is the least hyperparameter-sensitive representation, and was able to recover blocking learning per-
formance in concurrent settings. We extended these results to a complex concurrent large-scale
simulated robotic grasping task, where we showed that the concurrent models were able to recover
blocking execution baseline model success while acting 31.3% faster. We analyzed the qualitative
beneﬁts of concurrent models through a real-world robotic grasping task, where we showed that a
concurrent model with comparable grasp success as a blocking baseline was able to learn smoother
trajectories that were 49% faster.

An interesting topic to explore in future work is the possibility of increased data efﬁciency when
training on off-policy data from various latency regimes. Another natural extension of this work is
to evaluate DRL methods beyond value-based algorithms, such as on-policy learning and policy gra-
dient approaches. Finally, concurrent methods may allow robotic control in dynamic environments
where it is not possible for the robot to stop the environment before computing the action. In these
scenarios, robots must truly think and act at the same time.

REFERENCES

Pieter Abbeel, Adam Coates, Morgan Quigley, and Andrew Y. Ng. An application of rein-
In Bernhard Schlkopf, John C. Platt, and
ISBN 0-262-19568-2. URL

forcement learning to aerobatic helicopter ﬂight.
Thomas Hofmann (eds.), NIPS, pp. 1–8. MIT Press, 2006.
http://dblp.uni-trier.de/db/conf/nips/nips2006.html#AbbeelCQN06.

Artemij Amiranashvili, Alexey Dosovitskiy, Vladlen Koltun, and Thomas Brox. Motion perception
in reinforcement learning with dynamic objects. In CoRL, volume 87 of Proceedings of Machine
Learning Research, pp. 156–168. PMLR, 2018. URL http://dblp.uni-trier.de/db/
conf/corl/corl2018.html#AmiranashviliDK18.

Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym, 2016. URL http://arxiv.org/abs/1606.01540. cite
arxiv:1606.01540.

R´emi Coulom. Reinforcement learning using neural networks, with applications to motor control.

PhD thesis, Institut National Polytechnique de Grenoble-INPG, 2002.

Nicols Cruz, Kenzo Lobos-Tsunekawa, and Javier Ruiz del Solar. Using convolutional neural net-
works in robots with limited computational resources: Detecting nao robots while playing soc-
cer. CoRR, abs/1706.06702, 2017. URL http://dblp.uni-trier.de/db/journals/
corr/corr1706.html#CruzLR17.

Kenji Doya. Reinforcement learning in continuous time and space. Neural Computation, 12(1):
219–245, 2000. URL http://dblp.uni-trier.de/db/journals/neco/neco12.
html#Doya00.

Vlad Firoiu, Tina Ju, and Joshua Tenenbaum. At Human Speed: Deep Reinforcement Learning with

Action Delay. arXiv e-prints, October 2018.

9

Published as a conference paper at ICLR 2020

Nicolas Fr´emaux, Henning Sprekeler, and Wulfram Gerstner. Reinforcement learning using a
continuous time actor-critic framework with spiking neurons. PLoS computational biology, 9:
e1003024, 04 2013. doi: 10.1371/journal.pcbi.1003024.

Sergio Guadarrama, Anoop Korattikara, Oscar Ramirez, Pablo Castro, Ethan Holly, Sam Fishman,
Ke Wang, Ekaterina Gonina, Chris Harris, Vincent Vanhoucke, et al. Tf-agents: A library for
reinforcement learning in tensorﬂow, 2018.

Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash
Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Soft actor-critic algo-
rithms and applications. CoRR, abs/1812.05905, 2018. URL http://dblp.uni-trier.
de/db/journals/corr/corr1812.html#abs-1812-05905.

Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre
Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, and Sergey Levine. Qt-opt: Scal-
able deep reinforcement learning for vision-based robotic manipulation. CoRR, abs/1806.10293,
2018. URL http://dblp.uni-trier.de/db/journals/corr/corr1806.html#
abs-1806-10293.

H J Kappen. Path integrals and symmetry breaking for optimal control theory. Journal of Sta-
tistical Mechanics: Theory and Experiment, 2005(11):P11011–P11011, nov 2005. doi: 10.
1088/1742-5468/2005/11/p11011. URL https://doi.org/10.1088%2F1742-5468%
2F2005%2F11%2Fp11011.

Shuang Li, Shuai Xiao, Shixiang Zhu, Nan Du, Yao Xie, and Le Song. Learning temporal point
processes via reinforcement learning. In Proceedings of the 32Nd International Conference on
Neural Information Processing Systems, NIPS’18, pp. 10804–10814, USA, 2018. Curran Asso-
ciates Inc. URL http://dl.acm.org/citation.cfm?id=3327546.3327737.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. 2013. URL
http://arxiv.org/abs/1312.5602. cite arxiv:1312.5602Comment: NIPS Deep Learn-
ing Workshop 2013.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G.
Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Pe-
tersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran,
Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep rein-
forcement learning. Nature, 518(7540):529–533, February 2015.
ISSN 00280836. URL
http://dx.doi.org/10.1038/nature14236.

R´emi Munos
control

and Paul Bourgine.
problems.

tic
(eds.),
1035. MIT
1404-reinforcement-learning-for-continuous-stochastic-control-problems.
pdf.

continuous
stochas-
and S. A. Solla
1029–
pp.
http://papers.nips.cc/paper/

Reinforcement
Jordan, M.
I.
Information

learning for
J. Kearns,

in Neural
1998.

Processing

Advances

Systems

In M.

Press,

URL

10,

Temporal abstraction for

Alexander Neitz, Giambattista Parascandolo, Stefan Bauer,

and Bernhard Sch¨olkopf.
Adaptive skip intervals:
In
S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Gar-
pp. 9816–
nett
URL http://papers.nips.cc/paper/
9826. Curran Associates,
8188-adaptive-skip-intervals-temporal-abstraction-for-recurrent-dynamical-models.
pdf.

Information Processing Systems 31,

recurrent dynamical models.

(eds.), Advances

Inc., 2018.

in Neural

OpenAI, Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jzefowicz, Bob McGrew,
Jakub W. Pachocki, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray,
Jonas Schneider, Szymon Sidor, Josh Tobin, Peter Welinder, Lilian Weng, and Wojciech Zaremba.
Learning dexterous in-hand manipulation. CoRR, abs/1808.00177, 2018. URL http://dblp.
uni-trier.de/db/journals/corr/corr1808.html#abs-1808-00177.

10

Published as a conference paper at ICLR 2020

Simon Ramstedt and Chris Pal. Real-time reinforcement learning. In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d ´Alch´e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information
Processing Systems 32, pp. 3073–3082. Curran Associates, Inc., 2019. URL http://papers.
nips.cc/paper/8571-real-time-reinforcement-learning.pdf.

Sheldon M Ross, John J Kelly, Roger J Sullivan, William James Perry, Donald Mercer, Ruth M
Davis, Thomas Dell Washburn, Earl V Sager, Joseph B Boyce, and Vincent L Bristow. Stochastic
processes, volume 2. Wiley New York, 1996.

Erik Schuitema, Lucian Busoniu, Robert Babuka, and Pieter P. Jonker. Control delay in reinforce-
ment learning for real-time dynamic systems: A memoryless approach. 2010 IEEE/RSJ Interna-
tional Conference on Intelligent Robots and Systems, pp. 3226–3231, 2010.

David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman,
Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine
Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of go with
deep neural networks and tree search. Nature, 529:484–, January 2016. URL http://dx.
doi.org/10.1038/nature16961.

Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT
Press, March 1998. ISBN 0262193981. URL http://www.amazon.ca/exec/obidos/
redirect?tag=citeulike09-20&amp;path=ASIN/0262193981.

Correntin Tallec, Leonard Blier, and Yann Ollivier. Making Deep Q-learning Methods Robust to

Time Discretization. arXiv e-prints, January 2019.

Jie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen, Yunfei Bai, Danijar Hafner, Steven Bohez,
and Vincent Vanhoucke. Sim-to-Real: Learning Agile Locomotion For Quadruped Robots. arXiv
e-prints, April 2018.

Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Bud-
den, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy P. Lillicrap, and Martin A.
Riedmiller. Deepmind control suite. CoRR, abs/1801.00690, 2018. URL http://dblp.
uni-trier.de/db/journals/corr/corr1801.html#abs-1801-00690.

Evangelos Theodorou, Jonas Buchli, and Stefan Schaal. Reinforcement learning of motor skills in
high dimensions: A path integral approach. pp. 2397 – 2403, 06 2010. doi: 10.1109/ROBOT.
2010.5509336.

Utkarsh Upadhyay, Abir De, and Manuel Gomez-Rodrizuez. Deep reinforcement learning of
marked temporal point processes. In Proceedings of the 32Nd International Conference on Neu-
ral Information Processing Systems, NIPS’18, pp. 3172–3182, USA, 2018. Curran Associates
Inc. URL http://dl.acm.org/citation.cfm?id=3327144.3327238.

Eleni Vasilaki, Nicolas Frmaux, Robert Urbanczik, Walter Senn, and Wulfram Gerstner. Spike-based
reinforcement learning in continuous state and action space: When policy gradient methods fail.
PLoS computational biology, 5:e1000586, 12 2009. doi: 10.1371/journal.pcbi.1000586.

Thomas J. Walsh, Ali Nouri, Lihong Li, and Michael L. Littman. Planning and learning in envi-
In Joost N. Kok, Jacek Koronacki, Ramn Lpez de Mntaras,
ronments with delayed feedback.
Stan Matwin, Dunja Mladenic, and Andrzej Skowron (eds.), ECML, volume 4701 of Lecture
Notes in Computer Science, pp. 442–453. Springer, 2007.
ISBN 978-3-540-74957-8. URL
http://dblp.uni-trier.de/db/conf/ecml/ecml2007.html#WalshNLL07.

11

Published as a conference paper at ICLR 2020

A APPENDIX

A.1 DEFINING BLOCKING BELLMAN OPERATORS

As introduced in Section 3.5, we deﬁne a continuous-time Q-function estimator with concurrent
actions.

ˆQ(s(t), ai−1, ai, t, H) =

(cid:90) t(cid:48)=t+tAS

γt(cid:48)−tr(s(t(cid:48)), ai−1(t(cid:48)))dt(cid:48)+

t(cid:48)=t
(cid:90) t(cid:48)(cid:48)=t+H

t(cid:48)(cid:48)=t+tAS

γt(cid:48)(cid:48)−tr(s(t(cid:48)(cid:48)), ai(t(cid:48)(cid:48)))dt(cid:48)(cid:48) + γH V (s(t + H))

(cid:90) t(cid:48)=t+tAS

=

γt(cid:48)−tr(s(t(cid:48)), ai−1(t(cid:48)))dt(cid:48)+

(10)

(11)

(12)

t(cid:48)=t
(cid:90) t(cid:48)(cid:48)=t+H

t(cid:48)(cid:48)=t+tAS

γtAS

γt(cid:48)(cid:48)−t−tAS r(s(t(cid:48)(cid:48)), ai(t(cid:48)(cid:48)))dt(cid:48)(cid:48) + γH V (s(t + H))

(13)

(cid:90) t(cid:48)=t+tAS

=

γt(cid:48)−tr(s(t(cid:48)), ai−1(t(cid:48)))dt(cid:48)+

(14)

t(cid:48)=t
(cid:90) t(cid:48)(cid:48)=t+H

γtAS [

t(cid:48)(cid:48)=t+tAS

γt(cid:48)(cid:48)−t−tAS r(s(t(cid:48)(cid:48)), ai(t(cid:48)(cid:48)))dt(cid:48)(cid:48) + γH−tAS V (s(t + H))]

(15)

We observe that the second part of this equation (after γtAS ) is itself a Q-function at time t + tAS.
Since the future state, action, and reward values at t + tAS are not known at time t, we take the
following expectation:

Q(s(t), ai−1, ai, t, H) =

(cid:90) t(cid:48)=t+tAS

t(cid:48)=t

γt(cid:48)−tr(s(t(cid:48)), ai−1(t(cid:48)))dt(cid:48)+

(16)

(17)
which indicates that the Q-function in this setting is not just the expected sum of discounted future
rewards, but it corresponds to an expected future Q-function.

γtAS Es ˆQ(s(t), ai, ai+1, t + tAS, H − tAS)

In order to show the discrete-time version of the problem, we parameterize the discrete-time con-
current Q-function as:

ˆQ(st, at−1, at, t, tAS, H) = r(st, at−1) + γ

tAS

H Ep(st+tAS |st,at−1)r(st+tAS , at)+

H Ep(st+H |st+tAS ,at)V (st+H )
which with tAS = 0, corresponds to a synchronous environment.

γ

H

(18)

(19)

Using this parameterization, we can rewrite the discrete-time Q-function with concurrent actions as:
ˆQ(st, at−1, at, t, tAS, H) = r(st, at−1) + γ

H [Ep(st+tAS |st,at−1)r(st+tAS , at)+

(20)

tAS

γ

H−tAS

H Ep(st+H |st+tAS ,at)V (st+H )]

= r(st, at−1) + γ

tAS

H Ep(st+tAS |st,at−1)

(21)

ˆQ(st, at, at+1, t + tAS, tas(cid:48), H − tAS)

(22)

A.2 CONTRACTION PROOFS FOR THE BLOCKING BELLMAN OPERATORS

Proof of the Discrete-time Blocking Bellman Update

Lemma A.1. The traditional Bellman operator is a contraction, i.e.:

||T ∗Q∞(s, a) − T ∗Q∈(s, a)|| ≤ c||Q1(s, a) − Q2(s, a)||,

(23)

where T ∗Q(s, a) = r(s, a) + γ maxa(cid:48) EpQ(s(cid:48), a(cid:48)) and 0 ≤ c ≤ 1.

12

Published as a conference paper at ICLR 2020

Proof. In the original formulation, we can show that this is the case as following:

T ∗Q1(s, a) − T ∗Q2(s, a)
= r(s, a) + γ max

Ep[Q1(s(cid:48), a(cid:48))] − r(s, a) − γ max
a(cid:48)

a(cid:48)

= γ max

a(cid:48)
≤ γ sup
s(cid:48),a(cid:48)

Ep[Q1(s(cid:48), a(cid:48)) − Q2(s(cid:48), a(cid:48))]

[Q1(s(cid:48), a(cid:48)) − Q2(s(cid:48), a(cid:48))],

Ep[Q2(s(cid:48), a(cid:48))]

(24)

(25)

(26)

(27)

with 0 ≤ γ ≤ 1 and ||f ||∞ = supx[f (x)].

Similarly, we can show that the updated Bellman operators introduced in Section 3.5 are contractions
as well.

Proof of Lemma 3.2

Proof.
c Q1(st, ai−1, ai, t, tAS, H) − T ∗
T ∗
= r(st, ai−1) + γ

tAS
H max
ai+1

c Q2(st, ai−1, ai, t, tAS, H)

Ep(st+tAS |st,at−1)Q1(st, ai, ai+1, t + tAS, tAS(cid:48), H − tAS)

− r(st, ai−1) − γ

tAS
H max
ai+1

Ep(st+tAS |st,at−1)Q2(st, ai, ai+1, t + tAS, tAS(cid:48), H − tAS)

(28)

(29)

(30)

= γ

tAS
H max
ai+1

Ep(st+tAS |st,ai−1)[Q1(st, ai, ai+1, t + tAS, tAS(cid:48), H − tAS) − Q2(st, ai, ai+1, t + tAS, tAS(cid:48), H − tAS)]

(31)

tAS
H

≤ γ

sup
st,ai,ai+1,t+tAS ,tAS(cid:48) ,H−tAS

[Q1(st, ai, ai+1, t + tAS, tAS(cid:48), H − tAS) − Q2(st, ai, ai+1, t + tAS, tAS(cid:48), H − tAS)]

(32)

Proof of Lemma 3.1

Proof. To prove that this the continuous-time Bellman operator is a contraction, we can follow the
discrete-time proof, from which it follows:
T ∗
c Q1(s(t), ai−1, ai, t, tAS) − T ∗
= γtAS max
ai+1

Ep[Q1(s(t), ai, ai+1, t + tAS, H − tAS) − Q2(s(t), ai, ai+1, t + tAS, H − tAS)]

c Q2(s(t), ai−1, ai, t, tAS)

(33)

≤ γtAS

sup
s(t),ai,ai+1,t+tAS ,H−tAS

[Q1(s(t), ai, ai+1, t + tAS, H − tAS) − Q2(s(t), ai, ai+1, t + tAS, H − tAS)]

(35)

(34)

A.3 CONCURRENT KNOWLEDGE REPRESENTATION

We analyze 3 different representations of concurrent knowledge in discrete-time concurrent envi-
ronments, described in Section 3.6. Previous action at−1 is the action that the agent executed at
the previous timestep. Action selection time tAS is a measure of how long action selection takes,
which can be represented as either a categorical or continuous variable; in our experiments, which
take advantage of a bounded latency regime, we normalize action selection time using these known
bounds. Vector-to-go VTG is a feature that combines at−1 and st by encoding the remaining amount
of at−1 left to execute. See Figure 5 for a visual comparison.

We note that at−1 is available across the vast majority of environments and it is easy to obtain.
Using tAS, which encompasses state capture, communication latency, and policy inference, relies

13

Published as a conference paper at ICLR 2020

Figure 4: The execution order of different stages are shown relative to the sampling period H as
well as the latency tAS. (a): In “blocking” environments, state capture and policy inference are
assumed to be instantaneous. (b): In “concurrent” environments, state capture and policy inference
are assumed to proceed concurrently to action execution.

Figure 5: Concurrent knowledge representations can be visualized through an example of a 2-D
pointmass discrete-time toy task. Vector-to-go represents the remaining action that may be executed
when the current state st is observed. Previous action represents the full commanded action from
the previous timestep.

14

Published as a conference paper at ICLR 2020

on having some knowledge of the concurrent properties of the system. Calculating V T G requires
having access to some measure of action completion at the exact moment when state is observed.
When utilizing a ﬁrst-order control action space, such as joint angle or desired pose, V T G is easily
computable if proprioceptive state is measured and synchronized with state observation. In these
cases, VTG is an alternate representation of the same information encapsulated by at−1 and the
current state.

A.4 EXPERIMENT IMPLEMENTATION DETAILS

A.4.1 CARTPOLE AND PENDULUM ABLATION STUDIES

Here, we describe the implementation details of the toy task Cartpole and Pendulum experiments in
Section 4.1.

For the environments, we use the 3D MuJoCo implementations of the Cartpole-Swingup and
Pendulum-Swingup tasks in DeepMind Control Suite (Tassa et al., 2018). We use discretized
action spaces for ﬁrst-order control of joint position actuators. For the observation space of both
tasks, we use the default state space of ground truth positions and velocities.

For the baseline learning algorithms, we use the TensorFlow Agents (Guadarrama et al., 2018)
implementations of a Deep Q-Network agent, which utilizes a Feed-forward Neural Network (FNN),
and a Deep Q-Recurrent Neutral Network agent, which utilizes a Long Short-Term Memory (LSTM)
network. Learning parameters such as learning rate, lstm size, and fc layer size
were selected through hyperparameter sweeps.

To approximate different difﬁculty levels of latency in concurrent environments, we utilize different
parameter combinations for action execution steps and action selection steps (tAS). The number of
action execution steps is selected from {0ms, 5ms, 25ms, or 50ms} once at environment initializa-
tion. tAS is selected from {0ms, 5ms, 10ms, 25ms, or 50ms} either once at environment initializa-
tion or repeatedly at every episode reset. The selected tAS is implemented in the environment as
additional physics steps that update the system during simulated action selection.

Frame-stacking parameters affect the observation space by saving previous observations and actions.
The number of previous actions to store as well as the number of previous observations to store are
independently selected from the range [0, 4]. Concurrent knowledge parameters, as described in
Section 4, include whether to use VTG and whether to use tAS. Including the previous action is
already a feature implemented in the frame-stacking feature of including previous actions. Finally,
the number of actions to discretize the continuous space to is selected from the range [3, 8].

A.4.2 LARGE SCALE ROBOTIC GRASPING

Simulated Environment We simulate a 7 DoF arm with an over-the-shoulder camera (see Figure
3a). A bin in front of the robot is ﬁlled with procedurally generated objects to be picked up by the
robot and a sparse binary reward is assigned if an object is lifted off a bin at the end of an episode.
States are represented in form of RGB images and actions are continuous Cartesian displacements
of the gripper 3D positions and yaw. In addition, the policy commands discrete gripper open and
close actions and may terminate an episode. In blocking mode, a displacement action is executed
until completion: the robot uses a closed loop controller to fully execute an action, decelerating
and coming to rest before observing the next state. In concurrent mode, an action is triggered and
executed without waiting, which means that the next state is observed while the robot remains in
motion. It should be noted that in blocking mode, action completion is close to 100% unless the
gripper moves are blocked by contact with the environment or objects; this causes average blocking
mode action completion to be lower than 100%, as seen in Table 1.

Real Environment Similar to the simulated setup, we use a 7 DoF robotic arm with an over-
the-shoulder camera (see Figure 3b). The main difference in the physical setup is that objects are
selected from a set of common household objects.

Algorithm We train a policy with QT-Opt (Kalashnikov et al., 2018), a Deep Q-Learning method
that utilizes the Cross-Entropy Method (CEM) to support continuous actions. A Convolutional
Neural Network (CNN) is trained to learn the Q-function conditioned on an image input along with

15

Published as a conference paper at ICLR 2020

a CEM-sampled continuous control action. At policy inference time, the agent sends an image of
the environment and batches of CEM-sampled actions to the CNN Q-network. The highest-scoring
action is then used as the policy’s selected action. Compared to the formulation in Kalashnikov
et al. (2018), we also add a concurrent knowledge feature of VTG and/or previous action at−1 as
additional input to the Q-network. Algorithm 1 shows the modiﬁed QT-Opt procedure.

Algorithm 1: QT-Opt with Concurrent Knowledge
Initialize replay buffer D;
Initialize random start state and receive image o0;
Initialize concurrent knowledge features c0 = [V T G0 = 0, at−1 = 0, tAS = 0];
Initialize environment state st = [o0, c0];
Initialize action-value function Q(s, a) with random weights θ;
Initialize target action-value function ˆQ(s, a) with weights ˆθ = θ;
while training do
for t = 1, T do

Select random action at with probability (cid:15), else at = CEM(Q, st; θ);
Execute action in environment, receive ot+1, ct, rt;
Process necessary concurrent knowledge features ct, such as V T Gt, at−1, or tAS;
Set st+1 = [ot+1, ct];
Store transition (st, at, st+1, rt) in D;
if episode terminates then

Reset st+1 to a random reset initialization state;
Reset ct+1 to 0;

end
Sample batch of transitions from D;
for each transition (si, ai, si+1, ri) in batch do

if terminal transition then

yi = ri;

else

Select ˆai+1 = CEM( ˆQ, si; ˆθ);
yi = ri + γ ˆQ(si+1, ˆai+1);

end
Perform SGD on (yi − Q(si, ai; θ)2 with respect to θ;

end
Update target parameters ˆQ with Q and θ periodically;

end

end

For simplicity, the algorithm is described as if run synchronously on a single machine. In practice,
episode generation, Bellman updates and Q-ﬁtting are distributed across many machines and done
asynchronously; refer to (Kalashnikov et al., 2018) for more details. Standard DRL hyperparameters
such as random exploration probability ((cid:15)), reward discount (γ), and learning rate are tuned through
a hyperparameter sweep. For the time-penalized baselines in Table 1, we manually tune a timestep
penalty that returns a ﬁxed negative reward at every timestep. Empirically we ﬁnd that a timestep
penalty of −0.01, relative to a binary sparse reward of 1.0, encourages faster policies. For the
non-penalized baselines, we set a timestep penalty of −0.0.

A.5 FIGURES

See Figure 6 and Figure 7.

16

Published as a conference paper at ICLR 2020

Figure 6: Environment rewards achieved by DQN with different network architectures [either a
feedforward network (FNN) or a Long Short-Term Memory (LSTM) network] and different con-
current knowledge features [Unconditioned, vector-to-go (VTG), or previous action and tAS] on the
concurrent Cartpole task for every hyperparameter in a sweep, sorted in decreasing order. Providing
the critic with VTG information leads to more robust performance across all hyperparameters. This
ﬁgure is a larger version of 2a.

Figure 7: Environment rewards achieved by DQN with a FNN and different frame-stacking and
concurrent knowledge parameters on the concurrent Pendulum task for every hyperparameter in a
sweep, sorted in decreasing order.

17

