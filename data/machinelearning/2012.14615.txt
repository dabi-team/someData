DeePKS-kit: a package for developing machine learning-based chemically accurate
energy and density functional models

Yixiao Chena, Linfeng Zhanga, Han Wangb, Weinan Ea,c

aProgram in Applied and Computational Mathematics, Princeton University, Princeton, NJ, USA
bLaboratory of Computational Physics, Institute of Applied Physics and Computational Mathematics, Huayuan Road 6, Beijing 100088,
People’s Republic of China
cDepartment of Mathematics, Princeton University, Princeton, NJ, USA

1
2
0
2

n
u
J

1
2

]
h
p
-
m
e
h
c
.
s
c
i
s
y
h
p
[

2
v
5
1
6
4
1
.
2
1
0
2
:
v
i
X
r
a

Abstract

We introduce DeePKS-kit, an open-source software package for developing machine learning based energy and density
functional models. DeePKS-kit is interfaced with PyTorch, an open-source machine learning library, and PySCF, an ab
initio computational chemistry program that provides simple and customized tools for developing quantum chemistry
codes. It supports the DeePHF and DeePKS methods. In addition to explaining the details in the methodology and the
software, we also provide an example of developing a chemically accurate model for water clusters.

Keywords: Electronic structure, Density functional theory, Exchange-correlation functional, Deep learning

PROGRAM SUMMARY

Program Title: DeePKS-kit
Developer’s repository link:
https://github.com/deepmodeling/deepks-kit
Licensing provisions: LGPL
Programming language: Python
Nature of problem:
Modeling the energy and density functional in electronic struc-
ture problems with high accuracy by neural network models.
Solving electronic ground state energy and charge density using
the learned model.
Solution method: DeePHS and DeePKS methods are imple-
mented, interfaced with PyTorch and PySCF for neural network
training and self-consistent ﬁeld calculations. An iterative learn-
ing procedure is included to train the model self-consistently.

1. Introduction

Conventional computational methods for electronic struc-

ture problems follow a clear-cut hierarchy concerning the
trade-oﬀ between eﬃciency and accuracy. The full conﬁgu-
ration interaction (FCI) [1] method should be suﬃciently
accurate at the complete basis set limit, but it typically
scales exponentially with respect to the number of electrons
N . Coupled cluster singles, doubles and perturbative triples
(CCSD(T)) [2], the so-called golden standard of quantum
chemistry, has a scaling of O(cid:0)N 7(cid:1). The cost of Kohn-Sham
(KS) density functional theory (DFT) [3] and the Hartree-
Fock (HF) method typically scale as O(cid:0)N 3(cid:1) and O(cid:0)N 4(cid:1), re-
spectively, and some recently developed xc-functionals has

Email addresses: linfeng.zhang.zlf@gmail.com (Linfeng

Zhang), wang_han@iapcm.ac.cn (Han Wang)

1

reached great accuracy in speciﬁc applications[4]. However,
in general, chemical accuracy cannot be easily achieved for
these methods, and the design of xc-functionals can take a
lot of eﬀorts.

Recent advances in machine learning is changing the
state of aﬀairs. Signiﬁcant progress has been made by
using machine learning methods to represent a wide range
of quantities directly as functions of atomic positions and
chemical species. An incomplete list includes Refs. 5–19.
These methods generally scale linearly, yet require a large
amount of training data that is beyond the current ca-
pability of high level methods like CCSD(T). Meanwhile,
there have been eﬀorts in parametrizing many body wave-
function and using variational Monte Carlo approach to
solve the electronic Schrödinger equation directly[20–22].
These methods are generally very accurate and do not need
any training label, but the computational cost can be very
expensive (although remaining in cubic scaling) due to the
need of Monte Carlo sampling.

Lately, new machine learning models has been devel-
oped, that target at achieving chemical accuracy, for a
wide variety of atomic and molecular systems, at the cost
similar to DFT or HF methods, and requiring fewer train-
ing labels. These methods can be roughly divided into
two classes. One class is like the post-HF methods, which
use the ground-state electronic orbitals of a underlying
model (HF or DFT) as the input, and output the energy
diﬀerence between the model and the ground truth. In this
regard, machine learning based methods are used to pa-
rameterize the dependence of the energy diﬀerence on the
input orbitals, following certain physics-based principles.
Representative methods in this class include the MOB-ML
method [23, 24], the DeePHF method [25], etc. The other

 
 
 
 
 
 
class of the methods is in the spirit of DFT, in which ma-
chine learning based methods are used to parameterize the
energy functional (of the charge density or Kohn-Sham
orbitals) and can be solved to get the ground state en-
ergy in a self-consistent way. Methods in this class include
some earlier attempts[26–30] that may not be fully self-
consistent, the NeuralXC method [31], and the DeePKS
method [32], etc. There are also attempts in using dif-
ferentiable programming[33–35] to impose self-consistency
and improve sample eﬃciency, at the expense of signiﬁcant
higher computational cost in the training procedure.

With the booming of machine learning based meth-
ods for quantum chemistry problems, the community is in
urgent need of codes that can serve as a bridge between ma-
chine learning platforms and quantum chemistry softwares,
promote the transparency and reproducibility of diﬀerent
results, and better leverage the resultant models for ap-
plications. Developing such a code will not only beneﬁt
more potential users, but also avoid unnecessary eﬀorts on
reinventing the wheel. In particular, since the ﬁeld is at its
early stage, good codes should not only implement certain
methods in a user-friendly way, but also provide ﬂexible
interfaces for developing new methods or incorporating
more functionalities.

In this work, we introduce DeePKS-kit, an open-source
software package, publicly available at GitHub 1 under
the LGPL-3.0 License, for developing chemically accurate
energy and density functional models. DeePKS-kit is inter-
faced with PyTorch[36] in one end, and at the other end,
it interfaces with PySCF[37], an ab initio computational
chemistry program that provides a simple, light-weight,
and eﬃcient platform for quantum chemistry code devel-
oping and calculation. DeePKS-kit supports the DeePHF
and DeePKS methods that were developed by the authors
earlier. Furthermore, it is also designed to provide certain
ﬂexibilities for, e.g., modiﬁcation of the model construction,
changing the training scheme, interfacing other quantum
chemistry packages, etc.

The rest of the paper is organized as follows. In Sec-
tion 2, we introduce the theoretical framework of the
DeePHF and DeePKS methods as well as the notations.
In Section 3, we provide a brief introduction on how to use
DeePKS-kit to train diﬀerent quantum chemistry models
and how to use these models in production calculations. In
Section 4, we use training DeePHF and DeePKS models
for water clusters as an example to show how to use the
package. Finally, we conclude with some remarks for future
directions.

2. Methodology

We consider a many-body system with N electrons
indexed by i and M clamped ions indexed by I. The

1https://github.com/deepmodeling/deepks-kit

ground-state energy of the system can be written as

Etot = min
Ψ

= min

Ψ

E0[Ψ (x1, x2, . . . , xN )]

hΨ|T + W + Vext|Ψi ,

(1)

where Ψ (x1, x2, . . . , xN ) is the N -electron wavefunction,
T = − 1
1
|xi−xj |
are the kinetic, electron-electron interaction, and ion-electron
interaction operators, respectively.

and Vext = P

2 ∇2, W = 1

ZI
|XI −xi|

P

I,i

i,j

2

Following the (generalized) Kohn-Sham approach, we
introduce an auxiliary system that can be represented
det[ϕi(xj)], where
by a single Slater determinant Φ = 1√
N
{ϕi(x)} is the set of one-particle orbitals. We deﬁne another
energy functional E[· · · ], which takes these one-particle
orbitals as input:

Etot = min
Φ

E[Φ (x1, x2, . . . , xN )]

=

min
{ϕi},hϕi|ϕj i=δij

E[{ϕi}].

(2)

Solving this variational problem with respect to {ϕi} under
the orthonormality condition hϕi|ϕji = δij gives us the
celebrated self-consistent ﬁeld (SCF) equation,

H[{ϕj}] |ϕii = εi |ϕii

for

i = 1 . . . N.

(3)

where H = δE
denotes the eﬀective single particle Hamil-
δhϕi|
tonian that usually consists of kinetic and potential terms.
This is a non-linear equation and needs to be solved iter-
atively. The key to the Kohn-Sham DFT methods is to
ﬁnd a good approximation of E, so that the ground-state
energy and charge density obtained by solving Eq. 2 are
close to those obtained by solving Eq. 1.

We divide E into two parts,

E(cid:2){ϕi}|ω(cid:3) = Ebase[{ϕi}] + Eδ

(cid:2){ϕi}|ω(cid:3),

(4)

where Ebase is an energy functional of the baseline method,
such as the HF functional or the DFT functional with a
certain exchange correlation, and Eδ is the correction term,
whose parameters ω will be determined by a supervised
learning procedure.

We follow Ref. 25 to construct Eδ as a neural network
model that takes the “local density matrix” as input and
satisﬁes locality and symmetry requirements. The “local
density matrix” is constructed by projecting the density
matrix onto a set of atomic basis functions (cid:8)αI
(cid:9) cen-
tered on each atom I and indexed by the radial number n,
azimuthal number l, magnetic (angular) number m.

nlm

(cid:0)DI

nl

(cid:1)

mm0

= X
i

(cid:10)αI

nlm

(cid:12)
(cid:12)ϕi

(cid:11)(cid:10)ϕi

(cid:12)
(cid:12)αI

nlm0

(cid:11) .

(5)

For simplicity and locality, we only take the block diagonal
part of the full projection. In other words, the indices I,
n and l are taken to be the same on both sides, and only
angular indices m and m0 diﬀer.

2

We take the eigenvalues of those local density matrices
to ensure the resulting descriptors are rotational invariant

dI
nl

= EigenVals

mm0

(cid:2)(cid:0)DI

nl

(cid:1)

mm0

(cid:3) ,

(6)

where EigenVals
mm0 means that for given other indices,
take all possible m and m0 values, consider them as a
square matrix, and calculate the eigenvalues of it. Using
these descriptors as the direct input of a neural network
model, the correction energy Eδ is given by
Eδ = X

F NN(cid:0)dI |ω(cid:1),

(7)

I

where F NN is a fully connected neural network, parame-
terized by ω, containing skip connections[38]. dI denotes
the ﬂattened descriptors, where diﬀerent n and l indices
have been concatenated into a single vector. Results for
short alkanes in Ref. 25 show that the descriptors are able
to distinguish diﬀerent atomic species, as well as diﬀer-
ent local chemical environments (such as covalent bonds)
around atoms. Detailed description of the neural network
structures can be found in Appendix A.

We remark that both DeePHF and DeePKS schemes
adopt the same construction for the energy correction,
i.e. Eq. 7. Their diﬀerence lies in that DeePHF takes the
SCF orbitals of the baseline model, while the DeePKS takes
the SCF orbitals of the corrected energy functional Eq. 4.
The parameters in the DeePHF scheme are obtained by a
standard supervised training process. At the same time,
the DeePHF scheme can be turned to a variational model,
DeePKS, so that the energy and some electronic informa-
tion can be extracted self-consistently. In this context, the
correction energy brings an additional potential term Vδ
in the single particle Hamiltonian,

H = Hbase + Vδ,

(8)

where Hbase is the Hamiltonian corresponding to the base
model Ebase, and
Vδ = δEδ
δ hϕi|

∂Eδ
(cid:1)

∂ (cid:0)DI

(cid:11)(cid:10)αI

(cid:12)
(cid:12)αI

(9)

nlm0

= X
Inlmm0

mm0

nlm

(cid:12)
(cid:12)

nl

is the correction potential that depends on both orbitals
{ϕi} and NN parameters ω.

Similarly, the forces, deﬁned as the negative gradients
of the energy with respect to atomic positions, can be
calculated using the Hellmann-Feynman theorem. The
procedure leads to an additional term that results from the
(cid:9),
atomic position dependence of the projection basis (cid:8)αI

nlm

F (cid:2){ϕ∗

i

[ω]}|ω(cid:3) = Fbase

(cid:2){ϕ∗

(cid:2){ϕ∗
i }|ω(cid:3)
(cid:1)

mm0

i }(cid:3)
∂Eδ
∂ (cid:0)DI
(cid:12)
∂ (cid:0)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

nl
(cid:12)αI

(cid:11)(cid:10)αI

nlm

(cid:1)

(cid:12)
(cid:12)

nlm

∂X

X

−

Inlmm0
*

X

ϕ∗
i

i

(10)

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ϕ∗
i

3

where {ϕ∗
i } denotes the minimizer of the total energy func-
tional and we write out the dependence on the parameters
ω explicitly. Note that the force depends directly on NN
parameters ω, this allows us to include the force in the loss
function for the iterative training procedure.

The training of a DeePKS model requires an additional
self-consistent condition, i.e., the prediction of energy has
to go through a minimization procedure with respect to
the KS orbitals. We therefore reformulate the task into a
constrained optimization problem,[32]

min
ω

E
data,λρ

h (cid:0)Elabel − Emodel

+ λf

(cid:0)Flabel − Fmodel

(cid:2){ϕi}|ω(cid:3)(cid:1)2
(cid:2){ϕi}|ω(cid:3)(cid:1)2 i

s.t.

∃ εi ≤ µ,
(cid:16)
H(cid:2){ϕi}|ω(cid:3)

(11)

+ λρVpnt
hϕi|ϕji = δij

(cid:2)ρ[{ϕi}]|ρlabel
for

(cid:3) − εi
i, j = 1 . . . N

(cid:17)

|ϕii = 0,

where we have written the self-consistent condition as a
set of constraints. µ denotes the chemical potential. We
solve Eq. 11 using the projection method. In detail, we
ﬁrst optimize the NN parameters ω without the constraints
using the standard supervised training process. Then we
project the orbitals {ϕ∗
i } back to the subset that satis-
ﬁes the constraints by solving the SCF equations. The
procedure is repeated until we reach convergence.

(cid:2)ρ[{ϕi}]|ρlabel

The additional term λρVpnt

(cid:3) in the con-
straints is a penalty potential aiming to drive the opti-
mization procedure towards the target density ρlabel. It
can be viewed as an alternative form of the loss function
using density as labels. Since the density does not depend
explicitly on the NN parameter ω, it cannot enter a typical
loss function. We therefore use such a penalty term to uti-
lize the density information in the training process. When
λρ is zero, the term vanishes and we recover the typical
SCF equation. Otherwise, we have Vpnt = 0 if and only if
ρ[{ϕi}] = ρlabel. We can also make λρ a random variable
to reduce overﬁtting, with the expectation taken over λρ
as well.

In practice, we have to solve the SCF equations with
a ﬁnite basis set expansion. We write |ϕii = P
a cia |χai,
where |χai denotes a pre-deﬁned ﬁnite basis set. Then the
projected density matrix is given by

(cid:0)DI

nl

(cid:1)

mm0

= X
i,a,b

c∗
iacib

(cid:10)χa

(cid:12)
(cid:12)αI

nlm

(cid:11)(cid:10)αI

nlm0

(cid:12)
(cid:12)χb

(cid:11) .

(12)

When solving the SCF equations, we have

Habcib = εi

X

b

X

b

Sabcib,

(13)

where Hab = hχa|H|χbi, Sab = hχa|χbi and the correction

term from our functional is

(Vδ)

ab

= hχa|Vδ|χbi
= X
Inlmm0

∂ (cid:0)DI

∂Eδ
(cid:1)

nl

mm0

(cid:10)χa

(cid:12)
(cid:12)αI

nlm

(cid:11) (cid:10)αI

nlm0

(cid:12)
(cid:12)χb

(cid:11) .

(14)

Similarly, the contribution of our correction term in the
force (Eq. 10) is given by

Fδ = −

= −

∂Eδ
∂X
X

Inlmm0

∂Eδ
(cid:1)

nl

∂ (cid:0)DI

∂ (cid:10)χa

X

i,a,b

ciacib

mm0
(cid:12)
(cid:12)αI

nlm

(cid:11) (cid:10)αI
∂X

nlm0

(15)

(cid:11)

(cid:12)
(cid:12)χb

,

which is similar to the Pulay force and can give us the deriva-
tives with respect to neural network parameters. These
derivatives are necessary for training the DeePKS model.
We remark that it is a normal supervised learning pro-
cess for training a DeePHF model, but an iterative learning
process is required for training a DeePKS model. We put
more details in the model training and inference processes
in the next Section.

3. Software

DeePKS-kit consists of three major modules that deal
with the following tasks: (1) training a (perturbative) neu-
ral network (NN) energy functional using pre-calculated
descriptors and labels; (2) solving self-consistent ﬁeld (SCF)
equations for given systems using the energy functional
provided; (3) learning a self-consistent energy functional
by iteratively calling tasks (1) and (2). Fig. 1 provides a
schematic view of the workﬂow and architecture of DeePKS-
kit.

DeePKS-kit also provides a user-friendly interface, where
the above modules together with some auxiliary functions
are grouped into a command line tool. The format reads:

deepks CMD ARGS

Currently, CMD can be one of the following:

• iterate, which performs iterative learning by making

use of the following commands.

• train, which trains the energy model and outputs

errors during the training steps.

• test, which tests the energy model on given datasets

without considering self consistency.

• scf, which solves the SCF equation and dumps re-
sults and optional intermediate data for training.

• stats, which examines the results of the SCF calcu-

lation and prints out their statistics.

DeePKS-kit defaults to atomic units (a.u.) with an excep-
tion of using Angstrom (Å) as length unit for xyz ﬁles.

4

Figure 1: Schematic plot of the DeePKS-kit architecture and the
workﬂow. Upper: main steps of the whole iterative learning procedure.
Lower left: training of the neural network (NN) energy functional.
Descriptors are calculated from given molecular orbitals and used
as inputs of the NN model. The stochastic gradient decent (SGD)
training is implemented using the PyTorch library. Lower right:
solving generalized Kohn-Sham self-consistent ﬁeld (SCF) equations.
The XC potential is calculated from the trained NN functional. The
solver is implemented as a new class of PySCF library.

3.1. Model training

The energy functional, as shown in Eq. 4, is deﬁned to
predict the energy diﬀerence between a baseline method
like Hartree-Fock, or Kohn-Sham DFT, and a more ac-
curate method used for labeling, such as CCSD(T). The
energy functional is implemented using the PyTorch library
as a standard NN model. Since the descriptors in Eq. 6 do
not change with the neural network parameters as long as
the ground-state wavefunction is given, they are calculated
in advance. In the current implementation, the atomic
basis {αnlm} is chosen to be Gaussian type orbital (GTO)
functions, so the projection can be carried out analytically.
We ﬁnd that the basis needs to be relatively complete, and
here we use 108 basis functions per atom and azimuthal
indices l = 0, 1, 2. One might want to enlarge the set of in-
dices l, but for our testing cases, satisfactory accuracy can
already be achieved using the current setup. The detailed
coeﬃcients can be found in the Appendix of Ref. 25. The
projection of the density matrix is handled by the library
of Gaussian orbital integrals in PySCF. The calculation
of descriptors can be conducted automatically in the SCF
solving part. The NN model takes the descriptors dI
as its
nl
input and outputs the “atomic” contribution of the correc-
(cid:9)(cid:1), followed by a summation
tion energy EI
δ
to give the total correction energy Eδ in Eq. 7.

= F NN(cid:0)(cid:8)dI

Similarly, the force calculated by the NN model is the
diﬀerence between the baseline method and the labeling
method, namely the second term in Eq. 10. It can be viewed
as an analog of the Pulay force in SCF calculations using

nl

DeePKS-kitTrain modelSolve SCFDescriptorNN ModelPerturbative EnergySGD updateOrbitalXC PotentialSelf-consistent EnergySCF updatepytorchpyscfGTO basis. In the training procedure, similar to energy,
the calculation of forces is separated into the parameter-
dependent and parameter-free parts, by rewriting the force
term using the chain rule,

∂Eδ
∂X

= X
Inl

∂Eδ
∂dI
nl

∂dI
nl
∂X

.

(16)

The parameter-free part ∂dI
nl
multiplied to the NN-dependent part ∂Eδ
backward propagation to speed up the evaluation.

(cid:14)∂X is pre-calculated and
(cid:14)∂dI
given by

nl

δ

δ

The training procedure (train command) requires the
to be the input data and the reference
descriptors dI
nl
correction energies Elabel
to be the output label. If train-
ing with forces is enabled, the gradients of the descriptors
(cid:14)∂X and the reference correction forces F label
are
∂dI
nl
also needed. We call the collection of these quantities
for a single molecule conﬁguration a frame. Frames are
grouped into Numpy binary ﬁles in folders, with names and
shapes dm_eig.npy: [nframe, natom, ndest], l_e_delta.npy:
[nframe, 1], grad_vx.npy: [nframe, natom, 3, natom, ndest] and
l_f_delta.npy: [nframe, natom, 3], where ndest denote the
number of projection basis hence descriptors on each atom,
and equals to 108 in our tested examples. We call each
folder a system, which also corresponds to the system in
the SCF procedure that we will discuss later. Frames in
the same system must have the same number of atoms,
while the type of elements can be diﬀerent. These systems
can be prepared manually or generated automatically by
the scf command described later.

The training of the model follows a standard mini-batch
stochastic gradient decent (SGD) approach with the ADAM
optimizer[39] provided by PyTorch. At each training step,
a subset of training data is sampled from one or multiple
systems to form a batch. Frames in the same batch must
contain the same number of atoms. The training steps
are grouped into epoches. Each epoch corresponds to the
number of training steps for which the number of frames
sampled is equal to the size of whole training dataset.
With a user-speciﬁed interval of epoches, the square root
of averaged loss is output for both training and testing
datasets. The state of the NN model is also saved at the
output step and can be used as a restarting point. The
saved model ﬁle is also used in the SCF procedure. After
training is ﬁnished, DeePKS-kit oﬀers a test command to
examine the model’s performance as a DeePHF (non-self-
consistent) energy functional. It takes the model ﬁle and
system information with the same format in training, and
outputs the predicted correction energies for each frame in
the system, as well as averaged errors.

3.2. SCF Solving

The solver of the SCF equation is implemented as an in-
herited class from the restricted Kohn-Sham (RKS) class in
the PySCF library. Currently, DeePKS-kit only supports
restricted calculations in a non-periodic system. Neces-
sary tools for supporting unrestricted SCF and periodic

boundary conditions will be implemented in our future
work.

For each step in the SCF calculation, our module com-
putes the correction energy Eδ and the corresponding po-
tential Vδ in Eq. 14 under a given GTO basis set and adds
it to the original potential. The calculation of Eδ is the
same as in the training, by calling the PyTorch library to
evaluate the NN model, but the descriptors are generated
on the ﬂy using the projected density matrices from Eq. 12.
The overlap coeﬃcients of GTOs are pre-calculated and
saved to avoid duplicated computations. The calculation
of the potential Vδ follows a similar but reversed approach.
The gradient with respect to the projected density matrix
is computed by backward propagation via PyTorch and
then contracted with the overlap coeﬃcients. The rest of
the SCF calculation, including matrix diagonalization and
self-consistent iteration, is handled by PySCF using its
existing framework.

Force computation is also implemented as an extended
class of the corresponding Gradient class in the PySCF
library. Once the SCF calculation converges, the additional
force term can be computed by following Eq. 15, using the
converged density matrix. Adding the additional term to
the original force provided by PySCF gives us the total
force acting on an atom.

The scf command provided by DeePKS-kit is a conve-
nient interface of the module above that handles loading
and saving automatically. Similar to the training procedure,
this command also accepts systems that contains multiple
frames grouped into Numpy binary ﬁles. The data required
for each frame is the nuclear charge and position for every
atom in that conﬁguration. This should be provided in
atom.npy with shape [nframe, natom, 4]. The last axis con-
tains four elements corresponding to the nuclear charge
and three spacial coordinates respectively. For systems for
which all frames contain the same element type, one can pro-
vide atomic positions and element type in two separate ﬁles:
coord.npy: [nframe, natom, 3] and type.raw. Additionally,
energy.npy: [nframe, 1] and force.npy: [nframe, natom, 3]
can be provided as reference energies and forces to calcu-
late the corresponding labels Elabel
. DeePKS-kit
takes the name of the folder that contains the aforemen-
tioned ﬁles as the system’s name. The current implemen-
tation requires all systems to have diﬀerent names. For
convenience, the scf command also accepts a single xyz
ﬁle as a system that contains one single frame. DeePKS-kit
provides a script that converts a set of xyz ﬁles into a
normal system as well.

and F label
δ

δ

The interface takes a list of ﬁelds to be computed after
the SCF calculation, including (but not limited to) the
total energy and force, the density matrix, and all the data
needed in the training procedure. The computed ﬁelds will
also be grouped into Numpy binary ﬁles saved in the folder
with the system’s name at a speciﬁed location. The saved
folder corresponds to a training system and can be used
as the input of the train command directly. DeePKS-kit
also provides an auxiliary stats command that reads the

5

dumped SCF results and outputs the statistics, including
the convergence and averaged errors of the system.

Following Eq. 11, the SCF procedure can also accept an
additional penalty term that applies on the Hamiltonian
in order to use density labels in the iterative learning
approach. Such penalty is implemented as a hook to the
main SCF module that adds an extra potential based on
the density diﬀerence from the label. Currently, L2 and
Coulomb norms are supported as the form of the penalty.
The interface takes an optional key that speciﬁes the form
and strength of the penalty. To apply the penalty, an
additional label ﬁle dm.npy with shape [nframe, nbasis, nbasis]
is required in the systems.

3.3. Iterative learning

The iterative learning procedure is implemented using
the following strategy. First, we implement a general mod-
ule that handles the sequential execution of tasks. This
includes two main parts, (1) a scheduler that takes care
of the progress of the tasks, creates ﬁles and folders, and
restarts when necessary; (2) a dispatcher that submits
tasks to, and collects results from, speciﬁed computing re-
sources. Second, to enhance the ﬂexibility of the iteration
process, we deﬁne a set of task templates that execute of the
aforementioned DeePKS-kit commands iteratively using
user-provided systems and arguments. The detailed itera-
tion structure can be easily modiﬁed or extended. More
complicated iterations like active learning procedure can
also be implemented with ease.

The scheduler part consists of tasks and workﬂows, im-
plemented as corresponding Python classes. A task is made
up of its command to be executed, its working directory,
and the ﬁles required from previous calculation. Supported
execution methods include shell command, Python func-
tion, and the dispatcher that handles running on remote
machines or clusters. A workﬂow is a group of tasks or
sub-workﬂows that run sequentially. When been executed,
it will execute the command for each task in the speciﬁed
order, linking or creating ﬁles and folders in the process.
It will also record the task it has ﬁnished execution into a
designated ﬁle (defaults to RECORD), so that if the execution
is stopped in the middle, it can restart from its previous
location.

The dispatcher component is adapted from the DP-GEN
package[40]. It handles the execution of tasks on remote
machines or HPC clusters. Procedures like ﬁle uploading
and downloading or job submission and monitoring are
all taken care of by the dispatcher. Parallel execution of
multiple tasks using the same dispatcher is also possible,
to ensure that computing resources can be fully utilized.
Detailed implementation of the dispatcher is explained
in Ref. 40. Currently, only shell and Slurm systems are
supported in DeePKS-kit. More HPC scheduling systems
and cloud machines will be supported in the future.

With the help of the iteration module described above,
we provide a set of predeﬁned task templates that imple-
ment the iterative training procedure discussed in Section 2.

For each iteration, we deﬁne four tasks as follows. First,
we call the scf command to run the SCF calculations us-
ing the model trained in the previous iteration on given
systems. The command is executed through the dispatcher
in an embarrassingly parallelized way. The SCF procedure
will dump results and data needed for training for each
system. Second, the stats command runs directly through
Python to check the SCF results and output error statistics.
Third, the train command is executed through the dis-
patcher that trains the NN model, using the last model as
a restarting point, on the data saved by the SCF procedure.
Last, an additional test command is run through Python
to show the accuracy of the trained model. The whole
workﬂow consists of multiple number of iterations. For the
ﬁrst iteration where no initial NN model is speciﬁed, the
SCF procedure will run without any model using a base-
line method like HF. The ﬁrst training step will also start
from scratch and likely take more epochs. The number
of iterations, as well as the parameters used in the SCF
and training procedures and their dispatchers, can all be
speciﬁed by the user through a conﬁguration YAML ﬁle.

The iterate command reads the conﬁguration ﬁle, gen-
erates the workﬂow using the task templates, and executes
it. If the RECORD ﬁle exists, the command will try to restart
from the latest checkpoint recorded in the ﬁle. The re-
quired input systems used as training data are of same
format as in the scf command. Since this is a learning pro-
cedure, the reference energy in energy.npy ﬁle is required.
Force (force.npy) and density matrix (dm.npy) data are
optional and can be speciﬁed in the conﬁguration ﬁle.

4. Example

Here we provide a detailed example on generating a
DeePHF or DeePKS functional for water clusters, and
demonstrate its generalizability with tests on water hex-
amers. We use energy and force as labels. An example
of training with density labels is also provided in our git
repository 2. We take for example args.yaml as the con-
ﬁguration ﬁle. The learning procedure can be started by
the following command:

deepks iterate args.yaml

System preparation. We use randomly generated water
monomers, dimers, and trimers as training datasets. Each
dataset contains 100 near-equilibrium conﬁgurations. We
also include 50 tetramers as a validation dataset. We use
energy and force as labels. The reference values are given
by CCSD calculation with the cc-pVDZ basis. The system
conﬁgurations and corresponding labels are grouped into
diﬀerent folders by the number of atoms, following the
convention described in the previous Section. The path
to the folders can be speciﬁed in the conﬁguration ﬁle as
follows:

2https://github.com/deepmodeling/deepks-kit/tree/master/

examples/water_cluster

6

systems_train:

- ./systems/train.n[1-3]

systems_test:

- ./systems/valid.n4

Initialization of a DeePHF model. As a ﬁrst step, we
need to train an energy model as the starting point of the it-
erative learning procedure. This consists of two steps. First,
we solve the systems using the baseline method such as HF
or PBE and dump the descriptors needed for training the
energy model. Second, we conduct the training from scratch
using the previously dumped descriptors. If there is already
an existing model, this step can be skipped, by providing
the path of the model to the init_model key. The energy
model generated in this step is also a ready-to-use DeePHF
model, saved at iter.init/01.train/model.pth. If self-
consistency is not needed, the remaining iteration steps
can be ignored. We do not use force labels when training
the DeePHF energy model.

The parameters of the init SCF calculation is speciﬁed
under the init_scf key. The same set of parameters is also
accepted as a standalone ﬁle by the deepks scf command
when running SCF calculations directly. We use cc-pVDZ
as the calculation basis. The required ﬁelds to be dumped
are:

dump_fields: [conv,

e_tot, dm_eig, l_e_delta]

where dm_eig, l_e_delta, e_tot, and conv denote the
descriptors, the labels (reference correction energies), the
total energy, and the record of convergence, respectively.
Additional parameters for molecules and SCF calculations
can also be provided to mol_args and scf_args keys, and
will be directly passed to the corresponding interfaces in
PySCF.

The parameters of the initial training is speciﬁed under
the init_train key. Similarly, the parameters can also
be passed to the deepks train command as a standalone
ﬁle. In model_args, we adopted the neural network model
with three hidden layers and 100 neurons per layer, using
the GELU activation function[41] and skip connections[38].
We also scale the output correction energies by a user-
adjustable factor of 100, so that it is of order one and easier
to learn. In preprocess_args, the descriptors are set to
be preprocessed to have zero mean on the training set. A
preﬁtted ridge regression with a penalty strength 10 is also
added to the model to speed up the training process. The
batch size is set to 16 in data_args, and the the total
number of training epochs is set to 50000 in train_args.
The learning rate starts at 3e-4 and decays by a factor of
0.96 for every 500 epochs.

Iterative learning for a DeePKS model. For self-consistency,

we take the model acquired in the last step and perform
several additional iterations of SCF calculation and NN
training. The number of iterations is set to 10 in the
n_iter key. If it is set to 0, no iteration will be performed,
which gives the DeePHF model. In the iterative learning

7

procedure, we also include forces as labels to improve the
accuracy.

The SCF parameters are provided in the scf_input
key, following the same rules as the init_scf key.
In
order to use forces as labels, we add additional grad_vx for
the gradients of descriptors and l_f_delta for reference
correction forces. f_tot is also included for the total force
results.

dump_fields: [conv,

e_tot, dm_eig, l_e_delta,
f_tot, grad_vx, l_f_delta]

Due to the complexity of the neural network functional, we
use looser (but still accurate enough) convergence criteria
in scf_args, with conv_tol set to 1e-6.

The training parameters are provided in the train_input
key, similar to init_train. However, since we are restart-
ing from the existing model, no model_args is needed,
and the preprocessing procedure can be turned oﬀ.
In
addition, we add with_force: true in data_args and
force_factor: 1 in train_args to enable using forces in
training. The total number of training epochs is also re-
duced to 5000. The learning rate starts as 1e-4 and decays
by a factor of 0.5 for every 1000 steps.

Machine settings. How the SCF and training tasks are
executed is speciﬁed in scf_machine and train_machine,
respectively. Currently, both the initial and the following
iterations share the same machine settings. In this example,
we run our tasks on local computing cluster with Slurm as
the job scheduler. The platform to run the tasks is speciﬁed
under the dispatcher key, and the computing resources
assigned to each task is speciﬁed under resources. The
setting of this part diﬀers on every computing platform.
We provide here our training_machine settings as an
example:

dispatcher:

context: local
# use "shell" to run without slurm
batch: slurm
# unnecessary in local context
remote_profile: null

resources:

# resources are ignored in shell batch
time_limit: ‘24:00:00’
cpus_per_task: 4
numb_gpu: 1
mem_limit: 8 # gigabyte

python: "python" # use python in path

where we assign four CPU cores and one GPU to the train-
ing task, and set its time limit to 24 hours and memory limit
to 8GB. The detailed settings available for dispatcher and
resources can be found in the example folder of our git
repository, as well as in the document of the DP-GEN
software, with a slightly diﬀerent interface. In the case
that the Slurm scheduler is not available, we also provide

(a)

(b)

Figure 3: Energy barrier of the simultaneous proton transfer in a water
hexamer ring, calculated by diﬀerent methods. The x coordinate
corresponds to the length diﬀerence between two OH bonds connecting
to the transferred Hydrogen atom. Barrier heights during the transfer
are also shown in inset.

reaction in a water hexamer ring. We show the energy
barrier of the proton transfer path in Fig. 3. All predicted
energies from the DeePKS model fall within the chemical
accuracy range of the reference values given by the CCSD
calculation. We note that none of the training dataset
includes dissociated conﬁgurations in the proton transfer
case. Therefore, the DeePKS model trained on up to three
water molecules exhibits a fairly good transferability, even
in the OH bond breaking regime.

6

We also perform another test by calculating the binding
energies of diﬀerent isomers of the water hexamer. The
(cid:1) − 6E(cid:0)H2O(cid:1)
binding energy is given by ∆Ex = E(cid:0)(H2O)x
for a speciﬁed isomer x. The conformations of diﬀerent
water hexamers and the reference monomer are taken from
Ref. 42 with geometries optimized at the MP2 level. The
results are plotted in Fig. 4. We can see that the DeePKS
model gives chemically accurate predictions for all isomers,
outperforming the commonly used conventional functionals
like SCAN0[43, 44]. Meanwhile, the relative energies be-
tween diﬀerent isomers are also captured accurately with
the error less than 1 mH.

5. Conclusion

We have introduced the underlying theoretical frame-
work, the details of software implementation, and an exam-
ple for the readers to understand and use the DeePKS-kit
package. More capabilities, such as unrestricted SCF and
periodic boundary conditions, will be implemented in our
future work. Moreover, we hope that this and subsequent
work will help to develop an open-source community that

Figure 2: (a) The energy error during the initial training process. (b)
The energy and force error during the iterative learning procedure.
Both training and validation datasets are evaluated. Axes are plotted
in log scale.

in the repository an example input ﬁle to run the tasks in
standard shell environment.

Model testing. During each iteration of the learning pro-
cess, a brief summary of the accuracy of the SCF calculation
can be found in iter.xx/00.scf/log.data. Average en-
ergy and force (if applicable) errors are shown for both the
training and validation dataset. The corresponding results
of the SCF calculations are stored in iter.xx/00.scf/
data_test and iter.xx/00.scf/data_train, grouped by
training and validation systems. The (non-self-consistent)
error during each neural network training process can also
be found in iter.xx/01.train/log.train. We show in
Fig. 2 the energy errors during initial training process
and the energy and force errors of SCF calculation in
each iteration. At the end of the training, all errors are
much lower than the level of chemical accuracy. After 10
iterations, the resulted DeePKS model can be found at
iter.09/01.train/model.pth. The model can be used
in either a Python script creating the extended PySCF
class, or directly the deepks scf command.

As a testing example, we run the SCF calculation using
the learned DeePKS model on the collective proton transfer

8

100101102103104training epoch0.11.010energy error (mH)trainingvalidation0.10.20.40.8force MAE (mH/Å)trainingvalidation1248number of iteration0.10.31.03.0energy MAE (mH)0.80.60.40.20.00.20.40.60.8bond difference (Å)050100150200relative energy (mH)HF  224.2 mHCCSD  176.6 mHDeePKS  176.7 mHSCAN0  151.4 mHSCAN  127.7 mHThe term Lpre(cid:0)¯d(cid:1) = W pre · ¯d + bpre corresponds to the
preﬁtting procedure and ¯d = (d − µpre)/σpre is the prepro-
cessed descriptor, where W pre and bpre can be determined
through ridge regression on the training set and µpre and
σpre can be taken as the mean and standard variance of
each component of the descriptors over the training set.
Alternatively, setting W pre, bpre and µpre to 0 and σpre
to 1 can turn oﬀ the preprocessing completely. These be-
haviors are controlled by prefit, preshift and prescale
keywords under preprocess_args of train_input.
For each layer Lp in the neural network, we have

dp = Lp(cid:0)dp−1(cid:1) = ϕ(cid:0)W p · dp−1 + bp(cid:1),

(A.2)

where dp are the values of neurons in layer p = 1, 2, . . . , L
for and Mp the number of neurons controlled by the key-
word hidden_sizes under model_args. By default (and in
the example above), we use three hidden layers (L = 3) and
100 neurons for each hidden layer (Mp = 100). In particular,
d0 = ¯d is the preprocessed input of the neural network. The
weight W p ∈ RMp×Mp−1 and bias bp ∈ RMp are parame-
ters to be optimized and the activation function ϕ is applied
component-wisely and speciﬁed by actv_fn keyword, de-
faults to GELU[41]. When the keyword use_resnet is set
to true and Mp = Mp−1, skip connection is added to the
layer construction

dp = Lp(cid:0)dp−1(cid:1) = dp−1 + ϕ(cid:0)W p · dp−1 + bp(cid:1),

(A.3)

to facilitate the training process.

The output layer Lout does not have an activation func-

tion and is just a linear transformation,

Lout(cid:0)dL(cid:1) =

1
σout

(cid:0)W out · dL + bout(cid:1),

(A.4)

with weight W p ∈ R1×ML and bias bp ∈ R are parame-
ters to be optimized, and σout a scalar factor speciﬁed by
output_scale in advance to speed up training.

In the case of training a DeePKS model for a rela-
tively complicated system, the sorting of eigenvalues in
the construction of descriptors may lead to non-smooth
derivatives on speciﬁc points, making the calculation hard
to converge. In that case, we provide a smooth embed-
ding for the descriptors d0 = M(cid:0)¯d(cid:1), achieved by taking
a thermal average under diﬀerent “inverse temperatures”
βk, over the eigenvalues {dnlm}, m = −l, . . . , l in the same
shell speciﬁed by n an l,

d0
nlk

= Mk

(cid:0)(cid:8) ¯dnlm

(cid:9)(cid:1) =

P

¯dnlm exp(cid:0)βk
exp(cid:0)βk

¯dnlm

¯dnlm
(cid:1)

m

m
P

(cid:1)

,

(A.5)

where βk are trainable parameters that by default are taken
evenly between −5 and 5 with number of k equals to the
number of m. The embedding can be enabled by setting
embedding to thermal in model_args.

Figure 4: Binding energies of diﬀerent isomers of water hexamers,
calculated by diﬀerent methods. The values shown inside correspond
to the energy diﬀerence between conformations with the highest (Boat
(a)) and the lowest (Prism) energies.

will facilitate a joint and interdisciplinary eﬀort on de-
veloping universal, accurate and eﬃcient computational
quantum chemistry models.

Acknowledgement

The work of Y. C., L. Z. and W. E was supported in
part by a gift from iFlytek to Princeton University, the
ONR grant N00014-13-1-0338, and the Center Chemistry
in Solution and at Interfaces (CSI) funded by the DOE
Award DE-SC0019394. The work of H. W. is supported by
the National Science Foundation of China under Grant No.
11871110, the National Key Research and Development
Program of China under Grants No. 2016YFB0201200 and
No. 2016YFB0201203, and Beijing Academy of Artiﬁcial
Intelligence (BAAI).

Appendix A. Neural Network Structure

In Sec. 2 the neural network used to ﬁt the correc-
tion energy Eδ from descriptors dI is denoted brieﬂy by
I F NN(cid:0)dI (cid:1). Here we provide a detailed description on
P
the neural network structure we used. Since all atomic
descriptors dI share the same ﬁtting function F NN, we will
omit the index I hereafter.

The main part of F NN is constructed as a feedforward
neural network with optional skip connection. To speed
up the training process, it also supports preprocessing
and preﬁtting the input data on the training set. More
speciﬁcally, F NN has the following form:

F NN = Lpre(cid:0)¯d(cid:1) + Lout ◦ LL ◦ · · · ◦ L2 ◦ L1(cid:0)¯d(cid:1)

(A.1)

where the symbol “◦” means function composition and Lp
(p ∈ {1, 2, . . . , L}) denotes the mapping from layer p − 1 to
p in the neural network, which we will explain later.

9

PrismCageBook (a)Book (b)BagRingBoat (a)Boat (b)140130120110100908070interaction energy (mH)HF   7.9 mHCCSD   14.4 mHDeePKS   15.2 mHSCAN0   16.4 mHSCAN   19.3 mHReferences

[1] J. A. Pople, M. Head-Gordon, K. Raghavachari, Quadratic
conﬁguration interaction. A general technique for determining
electron correlation energies, J. Chem. Phys. 87 (10) (1987)
5968–5975.

[2] B. Jeziorski, H. J. Monkhorst, Coupled-cluster method for mul-
tideterminantal reference states, Phys. Rev. A 24 (4) (1981)
1668.

[3] W. Kohn, L. J. Sham, Self-consistent equations including ex-
change and correlation eﬀects, Phys. Rev. 140 (4A) (1965) A1133.
[4] L. Goerigk, A. Hansen, C. Bauer, S. Ehrlich, A. Najibi,
S. Grimme, A look at the density functional theory zoo with
the advanced gmtkn55 database for general main group thermo-
chemistry, kinetics and noncovalent interactions, Phys. Chem.
Chem. Phys. 19 (48) (2017) 32184–32215.

[5] J. Behler, M. Parrinello, Generalized neural-network representa-
tion of high-dimensional potential-energy surfaces, Phys. Rev.
Lett. 98 (14) (2007) 146401.

[6] A. P. Bartók, M. C. Payne, R. Kondor, G. Csányi, Gaussian
approximation potentials: The accuracy of quantum mechanics,
without the electrons, Phys. Rev. Lett. 104 (13) (2010) 136403.
[7] M. Rupp, A. Tkatchenko, K.-R. Müller, O. A. VonLilienfeld,
Fast and accurate modeling of molecular atomization energies
with machine learning, Phys. Rev. Lett. 108 (5) (2012) 058301.
[8] R. Ramakrishnan, P. O. Dral, M. Rupp, O. A. von Lilienfeld, Big
data meets quantum chemistry approximations: The δ-machine
learning approach, J. Chem. Theory Comput. 11 (5) (2015)
2087–2096.

[9] S. Chmiela, A. Tkatchenko, H. E. Sauceda, I. Poltavsky, K. T.
Schütt, K.-R. Müller, Machine learning of accurate energy-
conserving molecular force ﬁelds, Sci. Adv. 3 (5) (2017) e1603015.
[10] K. Schütt, P.-J. Kindermans, H. E. S. Felix, S. Chmiela,
A. Tkatchenko, K.-R. Müller, Schnet: A continuous-ﬁlter convo-
lutional neural network for modeling quantum interactions, Adv.
Neural Inf. Process. Syst. (2017) 992–1002.

[11] J. S. Smith, O. Isayev, A. E. Roitberg, ANI-1: an extensible
neural network potential with dft accuracy at force ﬁeld compu-
tational cost, Chem. Sci. 8 (4) (2017) 3192–3203.

[12] J. Han, L. Zhang, R. Car, W. E, Deep potential: a general rep-
resentation of a many-body potential energy surface, Commun.
Comput. Phys. 23 (3) (2018) 629–639.

[13] L. Zhang, J. Han, H. Wang, R. Car, W. E, Deep potential
molecular dynamics: A scalable model with the accuracy of
quantum mechanics, Phys. Rev. Lett. 120 (2018) 143001.
[14] L. Zhang, J. Han, H. Wang, W. Saidi, R. Car, W. E, End-to-end
symmetry preserving inter-atomic potential energy model for
ﬁnite and extended systems, Adv. Neural Inf. Process. Syst.
(2018) 4436–4446.

[15] F. Brockherde, L. Vogt, L. Li, M. E. Tuckerman, K. Burke,
K.-R. Müller, Bypassing the kohn-sham equations with machine
learning, Nat. Commun. 8 (1) (2017) 1–10.

[16] A. Grisaﬁ, A. Fabrizio, B. Meyer, D. M. Wilkins, C. Corminboeuf,
M. Ceriotti, Transferable machine-learning model of the electron
density, ACS Cent. Sci. 5 (1) (2018) 57–64.

[17] A. Chandrasekaran, D. Kamal, R. Batra, C. Kim, L. Chen,
R. Ramprasad, Solving the electronic structure problem with
machine learning, npj Comput. Mater. 5 (1) (2019) 1–7.

[18] L. Zepeda-Núñez, Y. Chen, J. Zhang, W. Jia, L. Zhang, L. Lin,
Deep density:
circumventing the kohn-sham equations via
symmetry preserving neural networks, arXiv preprint (2019)
1912.00775.

[19] K. Schütt, M. Gastegger, A. Tkatchenko, K.-R. Müller, R. J.
Maurer, Unifying machine learning and quantum chemistry
with a deep neural network for molecular wavefunctions, Nat.
Commun. 10 (1) (2019) 1–10.

[20] J. Han, L. Zhang, E. Weinan, Solving many-electron schrödinger
equation using deep neural networks, J. Comput. Phys. 399
(2019) 108929.

[21] J. Hermann, Z. Schätzle, F. Noé, Deep-neural-network solution
of the electronic schrödinger equation, Nat. Chem. (2020) 1–7.

[22] D. Pfau, J. S. Spencer, A. G. Matthews, W. M. C. Foulkes, Ab
initio solution of the many-electron schrödinger equation with
deep neural networks, Phys. Rev. Res. 2 (3) (2020) 033429.
[23] M. Welborn, L. Cheng, T. F. Miller III, Transferability in ma-
chine learning for electronic structure via the molecular orbital
basis, J. Chem. Theory Comput. 14 (9) (2018) 4772–4779.
[24] L. Cheng, M. Welborn, A. S. Christensen, T. F. Miller III, A
universal density matrix functional from molecular orbital-based
machine learning: Transferability across organic molecules, J.
Chem. Phys. 150 (13) (2019) 131103.

[25] Y. Chen, L. Zhang, H. Wang, W. E, Ground state energy func-
tional with hartree–fock eﬃciency and chemical accuracy, J.
Phys. Chem. A 124 (35) (2020) 7155–7165.

[26] J. C. Snyder, M. Rupp, K. Hansen, K.-R. Müller, K. Burke,
Finding density functionals with machine learning, Phys. Rev.
Lett. 108 (25) (2012) 253002.

[27] M. Bogojeski, L. Vogt-Maranto, M. E. Tuckerman, K.-R. Mueller,
K. Burke, Density functionals with quantum chemical accu-
racy: From machine learning to molecular dynamics, ChemRxiv
preprint 8079917 (2019) v1.

[28] X. Lei, A. J. Medford, Design and analysis of machine learn-
ing exchange-correlation functionals via rotationally invariant
convolutional descriptors, Phys. Rev. Mater. 3 (6) (2019) 063801.
[29] Q. Liu, J. Wang, P. Du, L. Hu, X. Zheng, G. Chen, Improving
the performance of long-range-corrected exchange-correlation
functional with an embedded neural network, J. Phys. Chem. A
121 (38) (2017) 7273–7281.

[30] R. Nagai, R. Akashi, O. Sugino, Completing density functional
theory by machine learning hidden messages from molecules, npj
Comput. Mater. 6 (1) (2020) 1–8.

[31] S. Dick, M. Fernandez-Serra, Machine learning accurate ex-
change and correlation functionals of the electronic density, Nat.
Commun. 11 (1) (2020) 1–10.

[32] Y. Chen, L. Zhang, H. Wang, W. E, Deepks: a comprehensive
data-driven approach towards chemically accurate density func-
tional theory, J. Chem. Theory Comput. 17 (1) (2020) 170–181.
[33] T. Tamayo-Mendoza, C. Kreisbeck, R. Lindh, A. Aspuru-Guzik,
Automatic diﬀerentiation in quantum chemistry with applica-
tions to fully variational hartree–fock, ACS Cent. Sci. 4 (5)
(2018) 559–566.

[34] L. Li, S. Hoyer, R. Pederson, R. Sun, E. D. Cubuk, P. Riley,
K. Burke, et al., Kohn-sham equations as regularizer: Building
prior knowledge into machine-learned physics, Phys. Rev. Lett.
126 (3) (2021) 036401.

[35] M. F. Kasim, S. M. Vinko, Learning the exchange-correlation
functional from nature with fully diﬀerentiable density functional
theory, arXiv preprint (2021) arXiv:2102.04229.

[36] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,
T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison,
A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chil-
amkurthy, B. Steiner, L. Fang, J. Bai, S. Chintala, Pytorch: An
imperative style, high-performance deep learning library, Adv.
Neural Inf. Process. Syst. (2019) 8024–8035.

[37] Q. Sun, T. C. Berkelbach, N. S. Blunt, G. H. Booth, S. Guo,
Z. Li, J. Liu, J. D. McClain, E. R. Sayfutyarova, S. Sharma, et al.,
Pyscf: the python-based simulations of chemistry framework,
Wiley Interdiscip. Rev.: Comput. Mol. Sci. 8 (1) (2018) e1340.
[38] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for
image recognition, Proc. IEEE Comput. Soc. Conf. Comput. Vis.
Pattern Recognit. (2016) 770–778.

[39] D. P. Kingma, J. Ba, Adam: A method for stochastic optimiza-

tion, arXiv preprint (2014) 1412.6980.

[40] Y. Zhang, H. Wang, W. Chen, J. Zeng, L. Zhang, H. Wang,
E. Weinan, Dp-gen: A concurrent learning platform for the gen-
eration of reliable deep learning based potential energy models,
Comput. Phys. Commun. (2020) 107206.

[41] D. Hendrycks, K. Gimpel, Gaussian error linear units (gelus),

arXiv preprint (2016) 1606.08415.

[42] E. Lambros, F. Paesani, How good are polarizable and ﬂexible
models for water: Insights from a many-body perspective, J.
Chem. Phys. 153 (6) (2020) 060901.

10

[43] J. Sun, A. Ruzsinszky, J. P. Perdew, Strongly constrained and
appropriately normed semilocal density functional, Phys. Rev.
Lett. 115 (3) (2015) 036402.

[44] K. Hui, J.-D. Chai, Scan-based hybrid and double-hybrid density
functionals from models without ﬁtted parameters, J. Chem.
Phys. 144 (4) (2016) 044114.

11

