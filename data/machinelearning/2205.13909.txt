2
2
0
2

y
a
M
7
2

]

G
L
.
s
c
[

1
v
9
0
9
3
1
.
5
0
2
2
:
v
i
X
r
a

(De-)Randomized Smoothing for
Decision Stump Ensembles

Miklós Z. Horváth∗, Mark Niklas Müller∗, Marc Fischer, Martin Vechev
Department of Computer Science
ETH Zurich, Switzerland
mihorvat@ethz.ch, {mark.mueller,marc.fischer,martin.vechev}@inf.ethz.ch

Abstract

Tree-based models are used in many high-stakes application domains such as
ﬁnance and medicine, where robustness and interpretability are of utmost impor-
tance. Yet, methods for improving and certifying their robustness are severely
under-explored, in contrast to those focusing on neural networks. Targeting this
important challenge, we propose deterministic smoothing for decision stump en-
sembles. Whereas most prior work on randomized smoothing focuses on evaluating
arbitrary base models approximately under input randomization, the key insight of
our work is that decision stump ensembles enable exact yet efﬁcient evaluation via
dynamic programming. Importantly, we obtain deterministic robustness certiﬁcates,
even jointly over numerical and categorical features, a setting ubiquitous in the real
world. Further, we derive an MLE-optimal training method for smoothed decision
stumps under randomization and propose two boosting approaches to improve
their provable robustness. An extensive experimental evaluation shows that our
approach yields signiﬁcantly higher certiﬁed accuracies than the state-of-the-art for
tree-based models. We release all code and trained models at ANONYMIZED.

1

Introduction

Tree-based models have long been a favourite for making decisions in high-stakes domains such as
medicine and ﬁnance, due to their interpretability and exceptional performance on structured data
[1]. However, recent results have highlighted that tree-based models are, similarly to other machine
learning models [2, 3], also highly susceptible to adversarial examples [4–6], raising concerns about
their use in high-stakes domains where errors can have dire consequences.

While the robustness of neural models has received considerable attention [7–21], the challenge
of obtaining robustness guarantees for ensembles of tree-based models has only been investigated
recently [4, 22, 23]. However, these initial works only consider numerical features and are based on
worst-case approximations, which do not scale well to the difﬁcult (cid:96)p-norm setting.

This Work In this work, we address this challenge and present DRS, a novel DeRandomized
Smoothing approach, for constructing robust tree-based models with deterministic (cid:96)p-norm guarantees
while supporting both categorical and numerical variables. Unlike prior work, our method is based
on Randomized Smoothing (RS) [24], an approach that obtains robustness guarantees by evaluating a
general base model under an input randomization φ(x). However, in contrast to standard applications
of RS, which use costly and imprecise approximations via sampling and only obtain probabilistic
certiﬁcates, we leverage the structure of decision stump ensembles to compute their exact output
distributions for a given input randomization scheme and thus obtain deterministic certiﬁcates.
Our key insight is that this distribution can be efﬁciently computed by aggregating independent
distributions associated with the individual features used by the ensemble.

∗Equal contribution

Preprint. Under review.

 
 
 
 
 
 
˜f1(x1)

˜f2(x2)

˜f3(x3)

(a) Meta-Stumps

(b) PDFs

(c) Ensemble PDF

(d) Ensemble CDF

Figure 1: Given an ensemble of 3 meta-stumps ˜fi (piecewise constant univariate functions), each operating on a
different feature xi of an input x, we calculate the probability of every output under input randomization (a) to
obtain a distribution over their outputs (b). We aggregate these individual PDFs via dynamic programming to
obtain the probability distribution over the ensemble’s outputs (c). We can then compute the corresponding CDF
(d) to evaluate the smoothed stump ensemble exactly.

We illustrate this idea in Fig. 1: In (a), we show an ensemble of decision stumps over three features
(x1, x2, x3), aggregated to piecewise constant functions over one feature each (discussed in Section 3)
and evaluated under the input randomization φ(x), here a Gaussian. We can compute the independent
probability density functions of their outputs (PDFs) (shown in (b)) directly, by evaluating the
(Gaussian) cumulative density function (CDF) over the constant regions. Aggregating the individual
PDFs (discussed in Section 3), we can efﬁciently compute the exact PDF (c) and CDF (d) of the
ensemble’s output. To evaluate and certify the smoothed model, we can now simply look up the
median prediction and success probability, respectively, in the CDF, without requiring sampling.

DRS combines (cid:96)p-norm certiﬁcates over numerical features, computed as described above, with
an efﬁcient worst-case analysis for (cid:96)0-perturbations of categorical features in order to, for the ﬁrst
time, provide joint certiﬁcates. To train models amenable to certiﬁcation with DRS, we propose a
robust MLE optimality criterion for training individual stumps and two boosting schemes targeting
the certiﬁed robustness of the whole ensemble. We show empirically that DRS signiﬁcantly improves
on the state-of-the-art, increasing certiﬁed accuracies on established benchmarks up to 4-fold.

Main Contributions Our key contributions are:

• DRS, a novel and efﬁcient Derandomized Smoothing approach for robustness certiﬁcation,
enabling joint deterministic certiﬁcates over numerical and categorical variables (Section 3).
• A novel MLE optimality criterion for training decision stumps robust under input random-
ization and two boosting approaches for certiﬁably robust stump ensembles (Section 4).
• An extensive empirical evaluation, demonstrating the effectiveness of our approach and

establishing a new state-of-the-art in a wide range of settings (Section 5).

∈

→

→

[C], classifying inputs to one of C

2 Background on Randomized Smoothing
Z≥2 classes, Randomized
For a given base model F : Rd
→
Smoothing (RS) is a method to construct a classiﬁer G : Rd
[C] with robustness guarantees. For a
randomization scheme φ : Rd
Rd, we deﬁne the success probability py := Px(cid:48)∼φ(x)[F (x(cid:48)) = y] and
G(x) := arg maxc∈[C] py. Depending on the choice of φ, we obtain different certiﬁcates of the form:
Theorem 2.1 (Adapted from Cohen et al. [24], Yang et al. [25]). If P(F (φ(x)) = y) := py
py
and py > 0.5, then G(x + δ) = y for all δ satisfying

δ
(cid:107)
In particular, we present two instantiations that we uti-
lize throughout this paper in Table 1, where Φ−1 is the
inverse Gaussian CDF. Similar results, yielding other
(cid:96)p-norm certiﬁcates, can be derived for a wide range
of input randomization schemes [25, 26]. Note that, by
using more information than just py, e.g., pc for the
runner-up class c, tighter certiﬁcates can be obtained
[24, 27]. Once py is computed, we can directly calculate the certiﬁable radius R := ρ(py). For a
broader overview of variants of Randomized Smoothing, please refer to Section 6.

Table 1: Randomized Smoothing guarantees.

1
2λ(py
2 )
σΦ−1(py)

p < R with R := ρ(py).

(cid:96)1 x + Unif([

−
(0, σI)

R := ρ(py)

(cid:96)2 x +

λ, λ]d)

φ(x)

N

≥

−

(cid:107)

2

0.00.51.00.00.51.00.00.51.0ρ−1(0)NaturalPredictionsuccessprobabilitypyclass0class1For most choices of f and φ, the exact success probability py can not be computed efﬁciently.
α (typically α = 10−3) using Monte Carlo
Thus a lower bound py is estimated with conﬁdence 1
sampling and the Neyman-Pearson lemma [28]. Not only is this extremely computationally expensive,
as typically 100 000 samples have to be evaluated per data point, but this also severely limits the
maximum certiﬁable radius (see Fig. 6) and only yields probabilistic guarantees. Additionally, if the
number of samples is not sufﬁcient for the statistical test, the procedure will abstain from classifying.

−

In the following, we will show how considering a speciﬁc class of models f allows us to compute
the success probability py exactly, overcoming these drawbacks, and thus invoke ρ(py) to compute
deterministic certiﬁcates over larger radii, orders of magnitude faster than RS.

3 Derandomized Smoothing for Decision Stump Ensembles

Tree-based models such as decision stump ensembles often combine exceptional performance on tabu-
lar data [1] with good interpretability, making them ideal for many real-world high-stakes applications.
Here, we propose DRS, a method to equip them with deterministic robustness guarantees. In partic-
ular, we show that their structure permits an exact evaluation under isotropic input randomization
schemes, such as those discussed in Section 2, and thus deterministic smoothing.

Stump Ensembles We deﬁne a decision stump as fm(x) = γl,m + (γr,m
leaf predictions γl,m, γr,m
ensembles, particularly suitable for Smoothing [29], of M such stumps ¯fM : Rd

γl,m)1xjm >vm, with
[0, 1], split position vm, and split variable jm. We construct unweighted

−

∈

[0, 1] as

(cid:55)→

1
M
and treat them as a binary classiﬁers 1 ¯fM (x)>0.5. While our approach is extensible to multi-class
classiﬁcation by replacing the scalar leaf predictions γ with prediction-vectors, assigning a score per
class, we focus on the binary case in this work.

¯fM (x) :=

fm(x),

m=1

(1)

M
(cid:88)

∼

Smoothed Stump Ensemble We now deﬁne a smoothed stump ensemble ¯gM along the lines of
Randomized Smoothing as discussed in Section 2, by evaluating ¯fM not only on the original input x
but rather on a whole distribution of x(cid:48)

φ(x):

∼

¯gM (x) := Px(cid:48)∼φ(x)[ ¯fM (x(cid:48)) > 0.5].
In this work, we consider randomization schemes φ(x) that are (i) isotropic, i.e., the dimensions of
x(cid:48)
φ(x) are independently distributed, and (ii) permit an efﬁcient computation of their marginal
cumulative distribution functions (CDF). This includes a wide range of distributions, e.g., the Gaussian
and Uniform distributions used in Table 1 and others commonly used for RS [25].
By denoting the model CDF as ¯
F
we can alternatively deﬁne ¯gM as ¯gM (x) := 1
which will become useful later. For a label y
¯
the success probability py =
M,x(0.5)
F
a sample from φ(x).

z],
≤
¯
M,x(0.5),
F
−
we obtain
0, 1
∈ {
}
of predicting y for
|

M,x(z) = Px(cid:48)∼φ(x)[ ¯f (x(cid:48))

y
|

−

Meta-Stumps To evaluate py exactly as illustrated in Fig. 1, we
group the stumps constituting an ensemble by their split variable
jm to obtain one meta-stump ˜fi per feature i. The key idea is that
outputs of these meta-stumps are now independently distributed
under isotropic input randomization (illustrated in Fig. 1 (b)),
allowing us to aggregate them efﬁciently later on.

We showcase this in Fig. 2, where two stumps (f1 and f2) are
combined into the meta-stump ˜fi. Formally, we have
˜fi(x) :=

fm(x),

i :=

(cid:88)

[M ]

I

m
{

∈

m∈Ii

Figure 2: A meta-stump constructed
from two stumps.

jm = i
}

,

|

(2)

(cid:80)d
deﬁne Mi =
can be represented by its split positions vi,j, sorted such that vi,j
γi,j = (cid:80)j−1

and rewrite our ensemble as ¯fM (x) := 1
M

|I
|
t=1 γr,t + (cid:80)|Ii|

t=j γl,m on each of the resulting

i

i

i=1

≤

˜fi(xi). Every meta-stump
vi,j+1, and its predictions

+ 1 regions, written as (γ, v)i.

|I

|

3

vi,1=v1f1(xi)γl,1γr,1γr,1vi,2=v2f2(xi)γl,2γl,2γr,2˜fi(xi)γi,1+++===γi,2γi,3p1p2p3Algorithm 1 Stump Ensemble PDF computation via Dynamic Programming

(Γ, v)i
function COMPUTEPDF(
{
pdf[i][t] = 0 for t
[M
pdf[0][0] = 1
for i = 1 to d do

∈

·

d
i=1, x, φ)
}
[d]
∆ + 1], i

∈

(cid:46) For 0 stumps all probability mass is on 0

for j = 1 to Mi do
for t = 0 to M

∆ + 1

Γi,j do

−
pdf[i][t + Γi,j] = pdf[i][t + Γi,j] + pdf[i

·

return pdf

1][t]

·

−

Px(cid:48)

i∼φ(x)[vi,j−1 < x(cid:48)

i ≤

vi,j]

CDF Computation Now we leverage the independence of our meta-stumps’ output distributions
under an isotropic input randomization scheme φ to compute the PDF of their ensemble efﬁciently
via dynamic programming (DP) (illustrated in Fig. 1 (c) and explained below). Given its PDF, we can
trivially compute the ensemble’s CDF ¯
M,x, allowing us to evaluate the smoothed model exactly
F
(illustrated in Fig. 1 (d)). This efﬁcient CDF computation constitutes the core of DRS.

O

((maxi

In more detail, we observe that the PDF of a stump ensemble is the convex sum of exponentially
i)d)) Dirac delta distributions. To avoid this exponential blow-up, we discretize
many (
I
all leaf predictions γ to a grid of ∆ values (typically ∆ = 100), when constructing the smoothed
such that γi,j = Γi,j
model ¯gM . For each γi,j, we deﬁne a corresponding Γi,j
∆ .
Now, we construct a DP-table, where every entry pdf[i][t] corresponds to the weight of the Dirac
t
∆M after considering the ﬁrst i meta-stumps (in any ﬁxed but arbitrary order).
delta at position
We outline the PDF computation in Algorithm 1. After initially allocating all probability mass to
t = 0, we account for the effect of the ith meta-stump, by updating the ith row in the DP-table with
pdf[i][t] = (cid:80)
vi,j]
of the randomized x(cid:48)
on the left and right,
respectively). This is illustrated in Fig. 2. After termination, the last line of the DP-table pdf[d]
contains the full PDF (see Fig. 1(c)). Formally we summarize this in the theorem below, delaying a
formal proof to App. A.1:

i lying between vi,j−1 and vi,j (padded with

Γi,j ] where pj is the probability Px(cid:48)

i∼φ(x)[vi,j−1 < x(cid:48)
and

j pj pdf[i-1][t

0, . . . , Mi

i ≤

−∞

∈ {

∞

∆

−

}

·

Theorem 3.1. For z
t=0
success probability py = Px(cid:48)∼φ(x)[ ¯fM (x(cid:48)) = y] =

M,x(z) = (cid:80)(cid:98)zM ∆(cid:99)
y

[0, 1], ¯
F

∈

|

pdf[d][t] describes the exact CDF and thus

¯
M,x(0.5)
F

−

for y

|

0, 1

∈ {

.
}

Note that the presented algorithm is slightly simpliﬁed, and we actually only have to track the range
of non-zero entries of one row of the DP-table. This allows us to compute the full PDF and thus
certiﬁcates for smoothed stump ensembles very efﬁciently, e.g., taking only around 1.2 s total for the
MNIST 2 VS. 6 task (around 2.000 data points and over 500 stumps).

Certiﬁcation Recall from Section 2 that, given the success probability py, robustness certiﬁcation
for (cid:96)p-norm bounded perturbations reduces to computing the maximal certiﬁable robustness radius
R = ρ(py). For all popular (cid:96)p-norms, ρ (and its inverse ρ−1; used shortly) can be either evaluated
symbolically [24, 25] or precomputed efﬁciently [30, 31], such that the core challenge of certiﬁcation
becomes computing (a lower bound to) py, which we solve efﬁciently via Theorem 3.1. Alternatively,
for a given target radius r, we need to check whether py

ρ−1(r) by equivalently calculating

¯gM,r(x) = ¯
F

−1
M,x(z)

z =

−
ρ−1(r)

if y = 1
if y = 0

,

(3)

≥
(cid:26)1

ρ−1(r)

and checking ¯gM,r(x) > 0.5. This corresponds to asserting that class y is predicted at least z percent
of the time. Here, the inverse CDF ¯
M,x
F
computed via Theorem 3.1. We will see in Section 4 that this view is useful when training stump
ensembles for certiﬁability. Finally, we want to highlight that this approach can be used with all
common randomization schemes yielding certiﬁcates for different (cid:96)p-norm bounded adversaries.

M,x(z) can be efﬁciently evaluated using the step-wise ¯
F

−1

Categorical Variables & Joint Certiﬁcates For practical applications, it is essential to handle both
numerical and categorical features jointly. To consider a categorical feature xi
in our
stump ensemble, we construct a di-ary stump ˜fi : [di]
[0, 1] returning a value γi,j corresponding
to each of the di categorical values and treated as a meta-stump with Mi = 1 for normalization.

1, . . . , di

∈ {

→

}

4

To provide certiﬁcates in this setting, we propose a novel scheme combining an arbitrary (cid:96)p-norm
certiﬁcate of radius rp over all numerical features, computed as discussed above, with an (cid:96)0 certiﬁcate
of radius r0 over all categorical features
, computed using an approach adapted from Wang et al. [23].
Conceptually, we compute the worst case effect of every individual categorical variable independently,
greedily aggregate these worst case effects, and account for them in our ensemble’s CDF.
Given a meta-stump’s prediction on a concrete sample qi = ˜fi(xi) as well as its maximal and
minimal output ui and li, respectively, we compute the maximum and minimum perturbation effect
to δi = ui−qi
, we can compute
the worst-case effect when perturbing at most r0 samples as

M , respectively. Given the set of categorical features

M and δi = li−qi

C

C

δr0 = max

R

(cid:88)

i∈R

δi,

s.t.

|R| ≤

r0,

R ⊆ C

by greedily picking the r0 largest δi. For δr0 we proceed anal-
ogously. Shifting the CDF, computed as above, by δ and δ for
samples with labels y = 0 and y = 1, respectively, before com-
puting the success probability py, allows us to account for the
worst-case categorical perturbations exactly. We illustrate this
for a sample with y = 0 in Fig. 3, where we show the CDFs
obtained by all possible perturbations of at most r0 categorical
variables, bounded to the right by those obtained by shifting the
original by δr0 . Note that here no smoothing over the categorical
variables is done or required, making inference trivial.

Figure 3: CDF shifted by the effect of
categorical feature perturbations.

4 Training for and with Derandomized Smoothing

To obtain large certiﬁed radii via smoothing, the base model has to be robust to the chosen randomiza-
tion scheme. To train robust decision stump ensembles, we propose a robust MLE optimality criterion
for individual stumps (Section 4.1) and two boosting schemes for whole ensembles (Section 4.2).

4.1

Independently MLE-Optimal Stumps

γl,m)1xjm >vm, its split feature jm, split
To train an individual stump fm(x) = γl,m + (γr,m
position vm, and leaf predictions γl,m, γr,m have to be determined. We choose them in an MLE-
optimal fashion with respect to the randomization scheme φ, starting with vm, as follows: We consider
the probabilities pl,i = Px(cid:48)∼φ(xi)[x(cid:48)
i lying to the left or the right
vm] and pr,i = 1
of vm, respectively, under the input randomization scheme φ. For an i.i.d. dataset with n samples
j = 1
(xi, yi)
leaf,
n
conditioned on the target label, and pj = p0
j as their sum to compute the entropy impurity
Hentropy [32] as

), we deﬁne the probabilities py

{i|yi=y} pj,i of picking the j

pl,i of x(cid:48)

j + p1

jm ≤

∈ {

l, r

(cid:80)

−

−

∼

X

Y

}

(

,

Hentropy =

−

(cid:88)

pj

(cid:88)

j∈{l,r}

y∈{0,1}

py
j
pj

(cid:32)

(cid:33)

.

py
j
pj

log

We then choose the vm approximately minimizing Hentropy via line-search. After ﬁxing vm this way,
we compute the MLE-optimal leaf predictions γφ,MLE

as:

and γφ,MLE
r

l

γφMLE
l

, γφMLE
r

= arg max
γl,γr

P[

φ(

X

Y |

), fm] = arg max

γl,γr

n
(cid:88)

i=1

Ex(cid:48)∼φ(xi) [log P[yi

x(cid:48), fm]]

|

n
(cid:88)

i∈{i|yi=0}
n
(cid:88)

= arg max
γl,γr

+

pl,i log(1

−

γl) + pr,i log(1

γr)

−

pl,i log(γl) + pr,i log(γr)

= arg max
γl,γr

i∈{i|yi=1}
p0
l log(1

−

γl) + p0

r log(1

−

5

γr) + p1

l log(γl) + p1

r log(γr),

0.30.50.7z0.00.51.0¯FM,xr0=0r0=1r0=2where the second line is obtained by splitting the sum over samples by class and explicitly computing
the expectation. We solve the maximization problem by setting the ﬁrst derivatives ∂
of
∂γl
our optimization objective to zero and checking its Hessian to conﬁrm that

and ∂
∂γr

γφMLE
l

=

p1
l
l + p0
p1
l

γφMLE
r

=

p1
r
r + p0
p1
r

(4)

are indeed maxima. We show in App. A.2 that γφ,MLE
l
when vm is chosen as the exact instead of an approximate minimizer of the entropy impurity.

, and vm are even jointly MLE-optimal,

, γφ,MLE
l

Ensembling To train an ensemble of independently MLE-optimal decision stumps, we sequentially
train one stump for every feature jm
[d] and construct an ensemble with equal weights, rejecting
∈
stumps with an entropy impurity Hentropy above a predetermined threshold.

4.2 Boosting Stump Ensembles for Certiﬁable Robustness

Decision stumps trained this way maximize the expected likeli-
hood under the chosen randomization scheme. Assuming (due to
the law of large numbers) a Gaussian output distribution, this cor-
responds to optimizing for the median output, which determines
the clean prediction. However, certiﬁed correctness at a given ra-
dius r is determined by the prediction y(cid:48)(x, r) = ¯
−1
m−1,x(z(r))
F
at the z(r) :=
percentile of the output distribution.
Where we call y(cid:48) the certiﬁable prediction, as certiﬁcation is now
equivalent to checking y = 1y(cid:48)(x,r)>0.5 (Eq. (3)). This difference
is illustrated in Fig. 4, where the clean prediction is correct (class 1) while the certiﬁable prediction
is incorrect. To align our training objective better with certiﬁed accuracy, we propose two novel
boosting schemes along the lines of the popular TREEBOOST [33] and ADABOOST [34].

Figure 4: Inverse CDF ¯F −1
M

ρ−1(r)

y
|

−

|

Gradient Boosting for Certiﬁable Robustness The key idea of gradient boosting is to compute the
gradient of a loss function with respect to an ensemble’s outputs and then add a model to the ensemble,
making a prediction along this gradient direction. Implementing this idea, we adapt TREEBOOST [33]
to propose ROBTREEBOOST: At a high level, we add stumps to the ensemble, which aim to predict
the residual between the target label and the current certiﬁable prediction. Concretely, to add the mth
stump to our ensemble, we begin by computing the current ensemble’s certiﬁable predictions y(cid:48)(r) at
y(cid:48)(r) as the residual between the target
a target radius r and then deﬁning the pseudo labels ˜y = y
labels y and the certiﬁable predictions y(cid:48)(r). This yields a regression problem, which we tackle by
choosing a feature jm and split threshold vm (approximately) minimizing MSE impurity under input
randomization before computing γl,m and γr,m as approximate minimizers of the cross-entropy loss
over the whole ensemble. Please see App. A.3 for a more detailed discussion of ROBTREEBOOST.

−

Adaptive Boosting for Certiﬁable Robustness The key idea of adaptive boosting is to build an
ensemble by iteratively training models, weighted based on their error rate, while adapting sample
weights based on whether they are classiﬁed correctly. We build on ADABOOST [34] to propose
ROBADABOOST: We construct an ensemble of K stump ensembles via hard voting, where every
ensemble is weighted based on its certiﬁable accuracy. To train a new ensemble, we increase the
weights of all samples that are currently not classiﬁed certiﬁably correct at a given radius r. We
choose stump ensembles instead of individual stumps as base classiﬁers because single stumps can
not reach the success probabilities under input randomization required for certiﬁcation. To compute
the certiﬁable radius for such an ensemble ¯FK, we compute the certiﬁable radii Rk of the individual
stump ensembles ¯f k
Rk+1 and obtain the largest
radius Rk such that the weights of the ﬁrst k ensembles sum up to more than half of the total weights.
Please see App. A.4 for a more detailed discussion of ROBADABOOST.

M , sort them in decreasing order such that Rk

≥

5 Experimental Evaluation

In this section, we empirically demonstrate the effectiveness of DRS in a wide range of settings. We
show that DRS signiﬁcantly outperforms the current state-of-the-art for certifying tree-based models
on established benchmarks, using only numerical features (Section 5.1), before highlighting its novel
ability to obtain joint certiﬁcates on a set of new benchmarks (Section 5.2). Finally, we perform an
ablation study, investigating the effect of DRS’s key components (Section 5.3).

6

0.00.10.51.0z0.00.51.0¯F−1M,xCleanPredictionCert.Predictionatr(q=0.9)class1class0Table 2: Natural accuracy (NAC) [%] and certiﬁed accuracy (CA) [%] with respect to (cid:96)1- and (cid:96)2-norm
bounded perturbations. Results for Wang et al. [23] as reported by them. Larger is better.

Perturbation

Dataset

Radius r

(cid:96)1-norm

(cid:96)2-norm

BREASTCANCER
DIABETES
FMNIST-SHOES
MNIST 1 VS. 5
MNIST 2 VS. 6

BREASTCANCER
DIABETES
FMNIST-SHOES
MNIST 1 VS. 5
MNIST 2 VS. 6

1.0
0.05
0.5
1.0
1.0

0.7
0.05
0.4
0.8
0.8

Wang et al. [23] Ours (Independent) Ours (Boosting)

NAC

98.5
72.7
87.6
95.5
92.3

91.2
-
75.5
95.6
86.3

CA

64.2
68.2
67.8
83.8
66.5

60.6
-
51.5
63.4
23.0

NAC

100.0
76.0
85.8
96.6
96.3

100.0
77.3
86.8
95.8
96.3

CA

81.0
69.5
83.3
94.1
93.9

75.2
68.2
81.2
91.6
89.6

NAC

100.0
77.9
87.2
99.3
96.6

100.0
79.9
91.0
99.2
96.3

CA

83.9
72.1
84.2
98.1
94.1

82.5
71.4
84.5
96.3
89.6

Experimental Setup We implement our approach in PyTorch [35] and evaluate it on Intel Xeon
Gold 6242 CPUs and an NVIDIA RTX 2080Ti. We compare to prior work on the DIABETES
[36], BREASTCANCER [37], FMNIST-SHOES [38], MNIST 1 VS. 5 [39], and MNIST 2 VS.
6 [39] datasets and are the ﬁrst to provide joint certiﬁcates of categorical and numerical features,
demonstrated on the ADULT [37] and CREDIT [37] datasets. For a more detailed description of the
experimental setup, please refer to App. B.

5.1 Certiﬁcation for Numerical Features

In Table 2, we compare the certiﬁed accuracies obtained via DRS on ensembles of independently
MLE optimal stumps (Independent) or boosted stump ensembles (Boosting) to the current state-of-
the-art, Wang et al. [23], using established benchmarks [23].

Independently MLE Optimal Stumps We ﬁrst consider stump ensembles trained without boosting
as described in Section 4.1 and observe that DRS obtains higher certiﬁed accuracies in all settings
and higher natural accuracies in most. For example, on MNIST 2 VS. 6, we increase the certiﬁed
accuracy at an (cid:96)2 radius of r2 = 0.8 from 23.0% to 89.6%, almost quadrupling it compared to Wang
et al. [23], while also improving natural accuracy from 86.3% to 96.3%.

Boosting for Certiﬁed Accuracy Leveraging the boosting techniques introduced in Section 4.2,
ROBTREEBOOST for BREASTCANCER and DIABETES and ROBADABOOST for FMNIST-SHOES,
MNIST 1 VS. 5, and MNIST 2 VS. 6, we increase certiﬁable and natural accuracies even further in
most settings. For example, compared to our independently trained stump ensemble, we improve
the certiﬁed accuracy for MNIST 1 VS. 5 at an (cid:96)1-radius of r1 = 1.0 from 94.1% to 98.1% and for
BREASTCANCER at an (cid:96)2-radius of r2 = 0.7 from 75.2% to 82.5%.

5.2

Joint Certiﬁcates for Categorical and Numerical Features

In Table 3, we compare models using only numerical, only cate-
gorical, or both types of features with regards to their balanced
certiﬁed accuracy (BCA) (accounting for class frequency) at
different combinations of (cid:96)2- and (cid:96)0-radii for numerical and
categorical features, respectively. We observe that models us-
ing both categorical and numerical features perform notably
better on clean data, highlighting the importance of utilizing
and thus also certifying them in combination. Moreover, cat-
egorical features make the model signiﬁcantly more robust to
(cid:96)2 perturbations, e.g., at (cid:96)2-radii > 0.75, they improve certiﬁed
accuracies, even when 2 categorical features (of only 8 and
7 for ADULT and CREDIT, respectively) are adversarially perturbed. We visualize this in Fig. 5,
showing BCA over (cid:96)2-perturbation radius and conﬁrm that the model utilizing only numerical features
(dotted line) loses accuracy much quicker with perturbation magnitude than the model leveraging
categorical variables (solid lines). As we are the ﬁrst to tackle this setting, we do not compare to
other methods but provide more detailed experiments in App. C.1.

Figure 5: Effect of (cid:96)0-perturbations on
(cid:96)2-robustness for CREDIT.

7

0.00.51.01.52.0‘2Radiusr20255075100BalancedCertiﬁedAccuracy[%]r0=0r0=1r0=2r0=3nocategoricalfeaturesTable 3: Balanced certiﬁed accuracy (BCA) [%] under joint (cid:96)0- and (cid:96)2-perturbations of categorical
and numerical features, respectively, depending on whether model uses categorical and/or numerical
features. The balanced natural accuracy is the BCA at radius r = 0.0. Larger is better.

Dataset

Categorical
Features

(cid:96)0 Radius r0

BCA without
Numerical Features

ADULT

CREDIT

no

yes

no

yes

-

0
1
2
3

-

0
1
2
3

5.3 Ablation Study

-

76.6
57.4
33.5
8.9

-

70.7
48.2
26.4
7.8

BCA with Numerical Features at (cid:96)2 Radius r2

0.00

74.9

77.5
66.0
51.4
36.7

56.1

74.1
52.7
29.3
13.6

0.25

65.7

73.9
61.7
46.2
31.4

44.5

70.3
47.7
26.0
10.3

0.50

0.75

1.00

1.25

1.50

42.4

68.1
53.9
37.5
24.1

33.3

67.3
41.7
23.8
7.8

27.4

63.3
47.4
29.3
15.4

17.7

59.7
38.3
19.2
4.9

14.5

48.7
34.3
21.5
10.3

9.7

57.1
37.1
16.8
4.4

8.9

40.7
26.6
17.1
8.1

7.2

54.9
35.1
13.5
3.9

5.1

35.2
21.8
13.4
5.7

5.0

53.4
34.7
13.0
3.4

We ﬁrst illustrate the effectiveness of our derandomization approach, before demonstrating the beneﬁt
of training with our MLE optimality criterion and investigating the effect of the noise level on DRS.

Derandomized vs Randomized Smoothing In Fig. 6, we
compare DRS, (dotted line) and sampling-based RS (solid
lines), with respect to certiﬁed accuracy over (cid:96)2 radii. We
observe that the sampling-based estimation of the success
probability in RS signiﬁcantly limits the obtained certiﬁ-
able radii. While this effect is particularly pronounced for
small sample counts n, increasing the maximum certiﬁable
radius, visible as the sudden drop in certiﬁable accuracy,
requires an exponentially increasing number of samples,
making the certiﬁcation of large radii intractable. DRS, in
contrast, can compute exact success probabilities and thus
deterministic guarantees for much larger radii, yielding a
33.1% increase in ACR compared to using n = 100 000
samples. Additionally, DRS is multiple orders of magnitude faster than RS, here, only requiring
10−4 s per sample. For more extensive experiments, please refer to App. C.2.
approximately 6.45

Figure 6: DRS vs. RS with various sample
counts n on MNIST 1 VS. 5.

·

MLE Optimality Criterion In Table 4, we evaluate our
robust MLE optimality criterion (MLE) by comparing it
to the standard entropy criterion applied to samples drawn
from the input randomization scheme (Sampling) or the
clean data (Default). We observe that the ensemble trained
on the clean data (Default) suffers from a mode collapse
when evaluated under noise. In contrast, both approaches
considering the input randomization perform much better,
with our robust MLE approach outperforming sampling
by a signiﬁcant margin, especially at large radii. For more
extensive experiments, please refer to App. C.3.

Effect of Noise Level In Fig. 7, we compare the certiﬁed
accuracy over (cid:96)1-radii for a range of different noise mag-
nitudes λ and ensembles of independently MLE optimal
stumps. We observe that at large perturbation magnitudes,
we obtain stumps that ‘think outside the (hyper-)box’, i.e.,
choose splits outside of the original data range, making
their ensembles exceptionally robust, even at large radii.
In particular, we obtain a certiﬁable accuracy of 87.3% at
radius r1 = 4.0, while the state-of-the-art achieves only
83.8% at r1 = 1.0 [23]. We provide more experiments for
varying noise magnitudes in App. C.4.

8

Table 4: Comparison of training with the
exact distribution (MLE), randomly per-
turbed data (Sampling), or clean data (De-
fault) on BREASTCANCER for σ = 1.

Method

ACR

MLE (Ours)
Sampling
Default

0.675
0.567
0.356

Certiﬁed Accuracy [%] at Radius r

0.0

100.0
99.3
26.3

0.25

97.1
95.6
25.5

0.5

86.1
75.2
25.5

0.75

30.7
8.8
25.5

Figure 7: Comparing DRS for various noise
levels λ on MNIST 1 VS. 5.

0.01.02.03.04.0‘2Radiusr20255075100CertiﬁedAccuracy[%]n=100n=1000n=10000n=100000DRS(ours)0.04.08.012.016.0‘1Radiusr10255075100CertiﬁedAccuracy[%]λ=0.5λ=1.0λ=2.0λ=4.0λ=8.0λ=16.06 Related Work

(De-)Randomized Smoothing Probabilistic certiﬁcation methods [40, 41, 24] are a popular approach
for obtaining robustness certiﬁcates for a wide range of tasks [31, 42–45] and threat models [25, 26,
46, 47, 27, 30, 31, 42, 48–52]. These methods follow the general blueprint discussed in Section 2
and consider arbitrary base classiﬁers, though specially trained [53–55], and can thus, in contrast
to our work, not leverage their structure. Speciﬁcally designed randomization schemes [50, 52]
enable efﬁcient enumeration and thus a deterministic certiﬁcate for, e.g., patch attacks or (cid:96)1-norm
perturbations. In contrast to these approaches, we permit arbitrary isotropic continuous randomization
schemes, allowing us to leverage comprehensive results on RS to obtain robustness guarantees against
a wide range of (cid:96)p-norm bounded adversaries [25].

Certiﬁcation and Training of Tree-Based Models In the setting of (cid:96)∞ robustness, where every
feature can be perturbed independently, various methods have been proposed to train [4, 22, 56, 57]
and certify [22, 58–60] robust decision trees and stumps. However, (cid:96)∞ robust models are still
vulnerable to other (cid:96)p norm perturbations [61, 62], which cover many realistic perturbations better
and are the focus of this work. There, the admissible perturbation of one feature depends on the
perturbations of all others, making the above approaches leveraging their independence not applicable.

On the other hand, Kantchelian et al. [63] discuss complete robustness certiﬁcation of tree ensembles
in the (cid:96)p-norm setting via MILP. However, this approach is intractable in most settings due its Co-
NP-complete complexity. Wang et al. [23] propose an efﬁcient but incomplete DP-based certiﬁcation
algorithm for stump ensembles based on over-approximating the maximum perturbation effect in
the (cid:96)p-norm setting. While similarly fast as our approach, we show empirically in Section 5 that
DRS obtains signiﬁcantly stronger certiﬁcates. Wang et al. [23] further introduce an incomplete
certiﬁcation algorithm for tree ensembles, which is based on computing the distance between the
pre-image of all trees’ leaves and the original sample. As they report signiﬁcantly worse results using
this approach than with stump ensembles, we omit a detailed comparison.

7 Limitations and Societal Impact

Limitations While able to handle arbitrary stump ensembles, and being extensible to arbitrary
decision trees, DRS can not handle arbitrary ensembles of decision trees. However, as these have
been shown to be signiﬁcantly more sensitive to (cid:96)p-norm perturbations than stump ensembles [23], we
believe this limitation to be of little practical relevance. Further, like all Smoothing-based approaches,
we construct a smoothed model from a base classiﬁer and only obtain robustness guarantees for the
former. In contrast to standard Randomized Smoothing approaches, we can, however, evaluate the
smoothed model exactly and efﬁciently.

Societal Impact As our contributions improve certiﬁed accuracy and certiﬁcation radii while retaining
high natural accuracy, they could help make real-world AI systems more robust and thus generally
amplify both any positive or negative societal effects. Further, while we achieve state-of-the-art
results, these may not be sufﬁcient to guarantee robustness in real-world deployment and could give
practitioners a false sense of security, leading to them relying more on our models than is justiﬁed.

8 Conclusion

We propose DRS, a (De-)Randomized Smoothing approach to robustness certiﬁcation, enabling joint
deterministic certiﬁcates over numerical and categorical variables for decision stump ensembles by
leveraging their structure to compute their exact output distributions for a given input randomization
scheme. The key insight enabling this is that this output distribution can be efﬁciently computed by
aggregating independent distributions associated with the individual features used by the ensemble.
We additionally propose a robust MLE optimality criterion for training individual decision stumps
and two boosting schemes improving an ensemble’s certiﬁable accuracy. Empirically, we demonstrate
that DRS signiﬁcantly outperforms the state-of-the-art for tree-based models in a wide range of
settings, obtaining up to 4-fold improvements in certiﬁable accuracy.

9

References

[1] R. Shwartz-Ziv and A. Armon, “Tabular data: Deep learning is not all you need,” Information

Fusion, vol. 81, 2022.

[2] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. Srndic, P. Laskov, G. Giacinto, and F. Roli,
“Evasion attacks against machine learning at test time,” in Machine Learning and Knowledge
Discovery in Databases - European Conference, ECML PKDD 2013, Prague, Czech Republic,
September 23-27, 2013, Proceedings, Part III, vol. 8190, 2013.

[3] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. J. Goodfellow, and R. Fergus,

“Intriguing properties of neural networks,” in Proc. of ICLR, 2014.

[4] H. Chen, H. Zhang, D. S. Boning, and C. Hsieh, “Robust decision trees against adversarial

examples,” in Proc. of ICML, vol. 97, 2019.

[5] F. Cartella, O. Anunciação, Y. Funabiki, D. Yamaguchi, T. Akishita, and O. Elshocht, “Ad-
versarial attacks for tabular data: Application to fraud detection and imbalanced data,” in
Proceedings of the Workshop on Artiﬁcial Intelligence Safety 2021 (SafeAI 2021) co-located
with the Thirty-Fifth AAAI Conference on Artiﬁcial Intelligence (AAAI 2021), Virtual, February
8, 2021, vol. 2808, 2021.

[6] Y. Mathov, E. Levy, Z. Katzir, A. Shabtai, and Y. Elovici, “Not all datasets are born equal: On
heterogeneous tabular data and adversarial examples,” Knowl. Based Syst., vol. 242, 2022.

[7] G. Singh, T. Gehr, M. Püschel, and M. T. Vechev, “An abstract domain for certifying neural

networks,” Proc. ACM Program. Lang., vol. 3, no. POPL, 2019.

[8] K. Xu, Z. Shi, H. Zhang, Y. Wang, K. Chang, M. Huang, B. Kailkhura, X. Lin, and C. Hsieh,
“Automatic perturbation analysis for scalable certiﬁed robustness and beyond,” in Advances
in Neural Information Processing Systems 33: Annual Conference on Neural Information
Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.

[9] T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov, S. Chaudhuri, and M. T. Vechev, “AI2:
safety and robustness certiﬁcation of neural networks with abstract interpretation,” in 2018 IEEE
Symposium on Security and Privacy, SP 2018, Proceedings, 21-23 May 2018, San Francisco,
California, USA, 2018.

[10] S. Wang, K. Pei, J. Whitehouse, J. Yang, and S. Jana, “Efﬁcient formal safety analysis of neural
networks,” in Advances in Neural Information Processing Systems 31: Annual Conference on
Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal,
Canada, 2018.

[11] T. Weng, H. Zhang, H. Chen, Z. Song, C. Hsieh, L. Daniel, D. S. Boning, and I. S. Dhillon,
“Towards fast computation of certiﬁed robustness for relu networks,” in Proc. of ICML, vol. 80,
2018.

[12] E. Wong and J. Z. Kolter, “Provable defenses against adversarial examples via the convex outer

adversarial polytope,” in Proc. of ICML, vol. 80, 2018.

[13] G. Singh, T. Gehr, M. Mirman, M. Püschel, and M. T. Vechev, “Fast and effective robustness
certiﬁcation,” in Advances in Neural Information Processing Systems 31: Annual Conference on
Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal,
Canada, 2018.

[14] M. N. Müller, G. Makarchuk, G. Singh, M. Püschel, and M. T. Vechev, “PRIMA: general
and precise neural network certiﬁcation via scalable convex hull approximations,” Proc. ACM
Program. Lang., vol. 6, no. POPL, 2022.

[15] V. Tjeng, K. Y. Xiao, and R. Tedrake, “Evaluating robustness of neural networks with mixed

integer programming,” in Proc. of ICLR, 2019.

10

[16] S. Dathathri, K. Dvijotham, A. Kurakin, A. Raghunathan, J. Uesato, R. Bunel, S. Shankar,
J. Steinhardt, I. J. Goodfellow, P. Liang, and P. Kohli, “Enabling certiﬁcation of veriﬁcation-
agnostic networks via memory-efﬁcient semideﬁnite programming,” in Advances in Neural
Information Processing Systems 33: Annual Conference on Neural Information Processing
Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.

[17] R. Ehlers, “Formal veriﬁcation of piece-wise linear feed-forward neural networks,” in Automated
Technology for Veriﬁcation and Analysis - 15th International Symposium, ATVA 2017, Pune,
India, October 3-6, 2017, Proceedings, vol. 10482, 2017.

[18] M. Mirman, T. Gehr, and M. T. Vechev, “Differentiable abstract interpretation for provably

robust neural networks,” in Proc. of ICML, vol. 80, 2018.

[19] M. Balunovic and M. T. Vechev, “Adversarial training and provable defenses: Bridging the gap,”

in Proc. of ICLR, 2020.

[20] A. Raghunathan, J. Steinhardt, and P. Liang, “Certiﬁed defenses against adversarial examples,”

in Proc. of ICLR, 2018.

[21] S. Gowal, K. Dvijotham, R. Stanforth, R. Bunel, C. Qin, J. Uesato, R. Arandjelovic, T. A. Mann,
and P. Kohli, “On the effectiveness of interval bound propagation for training veriﬁably robust
models,” ArXiv preprint, vol. abs/1810.12715, 2018.

[22] M. Andriushchenko and M. Hein, “Provably robust boosted decision stumps and trees against
adversarial attacks,” in Advances in Neural Information Processing Systems 32: Annual Confer-
ence on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,
Vancouver, BC, Canada, 2019.

[23] Y. Wang, H. Zhang, H. Chen, D. S. Boning, and C. Hsieh, “On lp-norm robustness of ensemble

decision stumps and trees,” in Proc. of ICML, vol. 119, 2020.

[24] J. M. Cohen, E. Rosenfeld, and J. Z. Kolter, “Certiﬁed adversarial robustness via randomized

smoothing,” in Proc. of ICML, vol. 97, 2019.

[25] G. Yang, T. Duan, J. E. Hu, H. Salman, I. P. Razenshteyn, and J. Li, “Randomized smoothing of

all shapes and sizes,” in Proc. of ICML, vol. 119, 2020.

[26] D. Zhang, M. Ye, C. Gong, Z. Zhu, and Q. Liu, “Black-box certiﬁcation with randomized
smoothing: A functional optimization based framework,” in Advances in Neural Information
Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020,
NeurIPS 2020, December 6-12, 2020, virtual, 2020.

[27] K. D. Dvijotham, J. Hayes, B. Balle, Z. Kolter, C. Qin, A. György, K. Xiao, S. Gowal, and
P. Kohli, “A framework for robustness certiﬁcation of smoothed classiﬁers using f-divergences,”
in Proc. of ICLR, 2020.

[28] J. Neyman and E. S. Pearson, “Ix. on the problem of the most efﬁcient tests of statistical
hypotheses,” Philosophical Transactions of the Royal Society of London. Series A, Containing
Papers of a Mathematical or Physical Character, vol. 231, no. 694-706, 1933.

[29] M. Z. Horváth, M. N. Müller, M. Fischer, and M. T. Vechev, “Boosting randomized smoothing

with variance reduced classiﬁers,” in Proc. of ICLR, 2022.

[30] G. Lee, Y. Yuan, S. Chang, and T. S. Jaakkola, “Tight certiﬁcates of adversarial robustness for
randomly smoothed classiﬁers,” in Advances in Neural Information Processing Systems 32:
Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December
8-14, 2019, Vancouver, BC, Canada, 2019.

[31] A. Bojchevski, J. Klicpera, and S. Günnemann, “Efﬁcient robustness certiﬁcates for discrete
data: Sparsity-aware randomized smoothing for graphs, images and more,” in Proc. of ICML,
vol. 119, 2020.

[32] B. Bustos, D. A. Keim, D. Saupe, T. Schreck, and D. V. Vranic, “Using entropy impurity
for improved 3d object similarity search,” in Proceedings of the 2004 IEEE International
Conference on Multimedia and Expo, ICME 2004, 27-30 June 2004, Taipei, Taiwan, 2004.

11

[33] J. H. Friedman, “Greedy function approximation: a gradient boosting machine,” Annals of

statistics, 2001.

[34] Y. Freund and R. E. Schapire, “A decision-theoretic generalization of on-line learning and an

application to boosting,” J. Comput. Syst. Sci., vol. 55, no. 1, 1997.

[35] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,
N. Gimelshein, L. Antiga, A. Desmaison, A. Köpf, E. Yang, Z. DeVito, M. Raison, A. Tejani,
S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala, “Pytorch: An imperative style,
high-performance deep learning library,” in Advances in Neural Information Processing Sys-
tems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
December 8-14, 2019, Vancouver, BC, Canada, 2019.

[36] J. W. Smith, J. E. Everhart, W. C. Dickson, W. C. Knowler, and R. S. Johannes, “Using the
adap learning algorithm to forecast the onset of diabetes mellitus,” in Annual Symposium on
Computer Application in Medical Care, 1988.

[37] D. Dua and C. Graff, “UCI machine learning repository,” 2017.

[38] H. X. an, “Fashion-mnist: a novel image dataset for benchmarking machine learnin,” ArXiv

preprint, vol. abs/1708.07747, 2017.

[39] Y. LeCun and C. Cortes, “MNIST handwritten digit database,” 1998.

[40] B. Li, C. Chen, W. Wang, and L. Carin, “Certiﬁed adversarial robustness with additive noise,”
in Advances in Neural Information Processing Systems 32: Annual Conference on Neural
Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC,
Canada, 2019.

[41] M. Lécuyer, V. Atlidakis, R. Geambasu, D. Hsu, and S. Jana, “Certiﬁed robustness to adversarial
examples with differential privacy,” in 2019 IEEE Symposium on Security and Privacy, SP 2019,
San Francisco, CA, USA, May 19-23, 2019, 2019.

[42] Z. Gao, R. Hu, and Y. Gong, “Certiﬁed robustness of graph classiﬁcation against topology attack
with randomized smoothing,” in IEEE Global Communications Conference, GLOBECOM 2020,
Virtual Event, Taiwan, December 7-11, 2020, 2020.

[43] P. Chiang, M. J. Curry, A. Abdelkader, A. Kumar, J. Dickerson, and T. Goldstein, “Detection
as regression: Certiﬁed object detection with median smoothing,” in Advances in Neural
Information Processing Systems 33: Annual Conference on Neural Information Processing
Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.

[44] M. Fischer, M. Baader, and M. T. Vechev, “Scalable certiﬁed segmentation via randomized

smoothing,” in Proc. of ICML, vol. 139, 2021.

[45] J. Jia, X. Cao, B. Wang, and N. Z. Gong, “Certiﬁed robustness for top-k predictions against

adversarial perturbations via randomized smoothing,” in Proc. of ICLR, 2020.

[46] M. Fischer, M. Baader, and M. T. Vechev, “Certiﬁed defense to image transformations via
randomized smoothing,” in Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual, 2020.

[47] L. Li, M. Weber, X. Xu, L. Rimanic, B. Kailkhura, T. Xie, C. Zhang, and B. Li, “Tss:

Transformation-speciﬁc smoothing for robustness certiﬁcation,” in ACM CCS, 2021.

[48] B. Wang, J. Jia, X. Cao, and N. Z. Gong, “Certiﬁed robustness of graph neural networks against
adversarial structural perturbation,” in KDD ’21: The 27th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining, Virtual Event, Singapore, August 14-18, 2021, 2021.

[49] J. Schuchardt, A. Bojchevski, J. Klicpera, and S. Günnemann, “Collective robustness certiﬁcates,”

in International Conference on Learning Representations, 2021.

12

[50] A. Levine and S. Feizi, “Robustness certiﬁcates for sparse adversarial attacks by randomized
ablation,” in The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence, AAAI 2020, The
Thirty-Second Innovative Applications of Artiﬁcial Intelligence Conference, IAAI 2020, The
Tenth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence, EAAI 2020, New
York, NY, USA, February 7-12, 2020, 2020.

[51] ——, “(de)randomized smoothing for certiﬁable defense against patch attacks,” in Advances
in Neural Information Processing Systems 33: Annual Conference on Neural Information
Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.

[52] ——, “Improved, deterministic smoothing for l1 certiﬁed robustness,” in Proc. of ICML, vol.

139, 2021.

[53] J. Jeong and J. Shin, “Consistency regularization for certiﬁed robustness of smoothed classiﬁers,”
in Advances in Neural Information Processing Systems 33: Annual Conference on Neural
Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.

[54] R. Zhai, C. Dan, D. He, H. Zhang, B. Gong, P. Ravikumar, C. Hsieh, and L. Wang, “MACER:
attack-free and scalable robust training via maximizing certiﬁed radius,” in Proc. of ICLR, 2020.

[55] H. Salman, J. Li, I. P. Razenshteyn, P. Zhang, H. Zhang, S. Bubeck, and G. Yang, “Provably
robust deep learning via adversarially trained smoothed classiﬁers,” in Advances in Neural
Information Processing Systems 32: Annual Conference on Neural Information Processing
Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, 2019.

[56] S. Calzavara, C. Lucchese, G. Tolomei, S. A. Abebe, and S. Orlando, “Treant: training evasion-

aware decision trees,” Data Min. Knowl. Discov., vol. 34, no. 5, 2020.

[57] D. Vos and S. Verwer, “Efﬁcient training of robust decision trees against adversarial examples,”

in Proc. of ICML, vol. 139, 2021.

[58] H. Chen, H. Zhang, S. Si, Y. Li, D. S. Boning, and C. Hsieh, “Robustness veriﬁcation of
tree-based models,” in Advances in Neural Information Processing Systems 32: Annual Confer-
ence on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,
Vancouver, BC, Canada, 2019.

[59] F. Ranzato and M. Zanella, “Abstract interpretation of decision tree ensemble classiﬁers,” in
The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence, AAAI 2020, The Thirty-Second
Innovative Applications of Artiﬁcial Intelligence Conference, IAAI 2020, The Tenth AAAI
Symposium on Educational Advances in Artiﬁcial Intelligence, EAAI 2020, New York, NY, USA,
February 7-12, 2020, 2020.

[60] J. Törnblom and S. Nadjm-Tehrani, “An abstraction-reﬁnement approach to formal veriﬁcation
of tree ensembles,” in Computer Safety, Reliability, and Security - SAFECOMP 2019 Work-
shops, ASSURE, DECSoS, SASSUR, STRIVE, and WAISE, Turku, Finland, September 10, 2019,
Proceedings, vol. 11699, 2019.

[61] L. Schott, J. Rauber, M. Bethge, and W. Brendel, “Towards the ﬁrst adversarially robust neural

network model on MNIST,” in Proc. of ICLR, 2019.

[62] F. Tramèr and D. Boneh, “Adversarial training and robustness for multiple perturbations,”
in Advances in Neural Information Processing Systems 32: Annual Conference on Neural
Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC,
Canada, 2019.

[63] A. Kantchelian, J. D. Tygar, and A. D. Joseph, “Evasion and hardening of tree ensemble

classiﬁers,” in Proc. of ICML, vol. 48, 2016.

13

A Additional Theory

In this section, we provide additional theoretical results, omitted in the main paper due to space
constraints. Concretely, in App. A.1 we provide a proof for Theorem 3.1 on the correctness of our PDF
computation. In App. A.2, we show that γl, γr and vm, computed as outlined in Section 4, are indeed
jointly MLE-optimal. Finally, we provide more details on ROBTREEBOOST and ROBADABOOST in
App. A.3 and App. A.4, respectively.

A.1 PDF Computation

Here, we provide a proof for Theorem 3.1 on the correctness of our efﬁcient PDF-computation,
restated below for convenience.
Theorem 3.1. For z
t=0
success probability py = Px(cid:48)∼φ(x)[ ¯fM (x(cid:48)) = y] =

M,x(z) = (cid:80)(cid:98)zM ∆(cid:99)
y

pdf[d][t] describes the exact CDF and thus

[0, 1], ¯
F

for y

0, 1

∈

¯
M,x(0.5)
F

|

|

−

∈ {

.
}

Proof. Let the random variable Γ(i) be the prediction of the i-th meta-stump, then we have by
deﬁnition of the meta-stump P[Γ(i) = Γi,j] = Px(cid:48)
vi,j] (see Section 3). Note
i ≤
= j. Now, we ﬁrst show by induction
that, for presentational simplicity, we assume Γi,j
that pdf[i] computes the exact PDF of (cid:80)i
l=1 Γ(l) (Lemma 1), before showing how the CDF of the
meta-stump ensemble follows.

i∼φ(x)[vi,j−1 < x(cid:48)
k
= Γi,k,
∀

Lemma 1. Algorithm 1 computes pdf[i][t] = P

(cid:104)(cid:80)i

l=1 Γ(l) = t

(cid:105)
.

Proof. We proceed by induction over i. In the base case, for i = 0, we directly have pdf[0][0] = 1.0
and pdf[0][t] = 0.0 for t > 0 by construction. Now the induction assumption is that pdf[i
1][t] =
P
d and all corresponding t. To compute the pdf[i][t], we now

for an arbitrary i

(cid:104)(cid:80)i−1

−

(cid:105)

l=1 Γ(l) = t

≤

have:

pdf[i][t] =

=

=

Mi(cid:88)

j=1

Mi(cid:88)

j=1

Mi(cid:88)

j=1

pdf[i

1][t

Γi,j]

·

−

−

Px(cid:48)

i∼φ(x)[vi,j−1 < x(cid:48)

i ≤

vi,j]

(cid:34)(cid:32)i−1
(cid:88)

l=1
(cid:34)(cid:32) i

(cid:88)

P

P

l=1

(cid:33)

Γ(l)

= t

(cid:35)

Γi,j

−

·

Px(cid:48)

i∼φ(x)[vi,j−1 < x(cid:48)

i ≤

vi,j]

(cid:33)

Γ(l)

= t

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Γ(i) = Γi,j

(cid:35)

(cid:104)

P

·

Γ(i) = Γi,j

(cid:105)

(cid:34)(cid:32) i

(cid:88)

(cid:33)

(cid:35)

Γ(l)

= t

= P

l=1

where we ﬁrst use the deﬁnition pdf[i][t] according to Algorithm 1, followed by induction assumption,
the independency of different meta-stumps and the the law of total probability over j.

Now, we show how Theorem 3.1 directly follows from Lemma 1. Recall that γi,j = Γi,j
∆ , where ∆ is
the number of discretization steps. Similarly to Γ(i), let γ(i) be the random variable describing the
prediction of the i-th meta-stump. Using Lemma 1, we obtain

¯
M,x(z) =
F

(cid:98)zM ∆(cid:99)
(cid:88)

t=0

pdf[d][t]

(cid:98)zM ∆(cid:99)
(cid:88)

(cid:34) d

(cid:88)

P

=

t=0

i=1

(cid:35)

Γ(i) = t

14

(cid:54)
(cid:54)
= P

= P

= P

(cid:34) d

(cid:88)

i=1
(cid:34) d

(cid:88)

i=1
(cid:34) d

(cid:88)

i=1

Γ(i)

zM ∆

≤ (cid:98)

(cid:35)

(cid:99)

(cid:35)

(cid:99)

M ∆

zM ∆

γ(i)
M

γ(i)
M ≤

z

≤ (cid:98)
(cid:35)

= P (cid:2) ¯fM (x)

z(cid:3) .

≤

Where the second to last step follows from the discretization of the leaf predictions leading to a
piece-wise constant CDF.

A.2 MLE-Optimal Stumps

In this section, we extend the theory from Section 4.1, showing that the vm, γl and γr computed as
outlined there, are, in fact, jointly MLE-optimal.

Recall that an individual stump operating on feature jm is characterized by three parameters: vm,
γl and γr. In Section 4.1, we show how to choose MLE-optimal γl and γr given vm. It remains to
show that if vm minimizes the entropy impurity Hentropy, then γφ,MLE
, and vm are jointly
MLE-optimal.
For an arbitrary split position vm, we have the probabilities pl,i(vm) = Px(cid:48)∼φ(xi)[x(cid:48)
pr,i(vm) = 1
−
randomization scheme φ. For an i.i.d. dataset with n samples (xi, yi)
probabilities py
j (vm) = 1
n
label, and pj(vm) = p0
[32] as

vm] and
i lying to the left or the right of vm, respectively, under the input
), we deﬁne the
leaf, conditioned on the target
l, r
j (vm) as their sum. Now, we compute the entropy impurity Hentropy

{i|yi=y} pj,i(vm) of picking the j

(cid:80)
j (vm) + p1

pl,i(vm) of x(cid:48)

, γφ,MLE
l

jm ≤

(
X

∈ {

∼

Y

}

,

l

Hentropy(vm) =

−

(cid:88)

pj(vm)

(cid:88)

j∈{l,r}

y∈{0,1}

py
j (vm)
pj(vm)

log

(cid:32)

(cid:33)

py
j (vm)
pj(vm)

=

−

(cid:88)

(cid:88)

py
j (vm) log

j∈{l,r}

y∈{0,1}

(cid:32)

py
j (vm)
pj(vm)

(cid:33)

.

(vm) be the MLE-optimal predictions given vm, as computed

l

l

(vm) and γφ,MLE

Similarly, let γφ,MLE
in Section 4.1. We formalize our statement as follows in Theorem A.1:
dataset with n samples (xi, yi)
Theorem A.1. Given an i.i.d.
arg minvm Hentropy(vm), γl(v∗
and γr are jointly MLE-optimal with respect to that dataset.

p1
l (v∗
m)
p1
m)+p0
l (v∗
l (v∗

m) and γr(v∗

m) =

m) =

Proof. Similarly to Section 4.1, but also optimizing over vm, we obtain:

let v∗
(
,
Y
X
∼
r(v∗
p1
m)
m) . Then v∗
p1
r(v∗
m)+p0
r(v∗

m :=
m, γl

),

), fm]

φ(

X

P[

Y |
n
(cid:88)

Ex(cid:48)∼φ(xi) [log P[yi

x(cid:48), fm]]

|

m , γφMLE
vφMLE

l

, γφMLE
r

= arg max
vm,γl,γr

= arg max
vm,γl,γr

= arg max
vm,γl,γr

+

i=1

n
(cid:88)

i∈{i|yi=0}
n
(cid:88)

i∈{i|yi=1}

15

pl,i(vm) log(1

−

γl) + pr,i(vm) log(1

γr)

−

pl,i(vm) log(γl) + pr,i(vm) log(γr)

= arg max
vm,γl,γr

p0
l (vm) log(1

−

γl) + p0

r(vm) log(1

γr)

−

+ p1

l (vm) log(γl) + p1

r(vm) log(γr)

As shown in Section 4.1, for a ﬁxed vm, the MLE-optimal estimates for γl and γr are γφMLE

(vm) =
r(vm) . Hence, in the following, it is enough to optimize

l

p1
l (vm)
l (vm)+p0
p1
over vm, substituting in γφMLE

l (vm) and γφMLE

r

p1
r(vm)
r(vm)+p0
p1
(vm) and γφMLE

(vm) =

r

l

(vm). We obtain:

vφMLE
m

= arg max

vm

p0
l (vm) log(1

γφ,MLE
l

−

(vm)) + p0

γφ,MLE
r

(vm))

r(vm) log(1

−
r(vm) log(γφ,MLE
(cid:19)

r

(vm))
(cid:18)

+ p0

r(vm) log

l (vm)
(cid:19)

+ p1

r(vm) log

+ p1

l (vm) log(γφ,MLE

l

= arg max

vm

p0
l (vm) log

(cid:18)

+ p1

l (vm) log

= arg max

vm

p0
l (vm) log

+ p1

l (vm) log

(cid:18)
1

−

(vm)) + p1
p1
l (vm)
l (vm) + p0
p1
p1
l (vm)
l (vm) + p0
p1
l (vm)
(cid:19)
p1
l (vm)
pl(vm)
(cid:19)

(cid:18)
1

−
(cid:18) p1
l (vm)
pl(vm)

+ p0

r(vm) log

(cid:18)

1

(cid:18) p1

−
r(vm)
pr(vm)

+ p1

r(vm) log

(cid:19)

r(vm)

1

(cid:18)

p1
r(vm)
r(vm) + p0
p1
−
(cid:19)
p1
r(vm)
r(vm) + p0
p1
(cid:19)
p1
r(vm)
pr(vm)
(cid:19)

r(vm)

Hentropy(vm)

−
Hentropy(vm)

= arg max

vm

= arg min

vm

= v∗
m

Thus, we have that the triplet v∗
p1
r(v∗
m)
r(v∗
m)+p0
r(v∗
p1

γr(v∗

m) =

m) is jointly MLE-optimal.

m := arg minvm Hentropy(vm), γl(v∗

m) =

p1
l (v∗
m)
p1
m)+p0
l (v∗
l (v∗

m) and

A.3 Gradient Boosting for Certiﬁable Robustness

Below, we describe ROBTREEBOOST, already outlined in Section 4.2, in more detail. Formally,
we aim to minimize the cross-entropy loss between the certiﬁable prediction at the qth percentile
−1
m−1,xi(q) and the one-hot target probability given by the label y, where we choose q = ρ−1(r) for
F
some target radius r. Concretely, to add the mth stump to our ensemble, we begin by computing the
certiﬁable prediction y(cid:48)
i:

y(cid:48)
i =

(cid:26) ¯
−1
m−1,xi(q)
F
¯
−1
m−1,xi(1
F

−

if y = 0
if y = 1.

q)

(5)

Now, we deﬁne the pseudo label ˜y as the residual between the target label y and the certiﬁable
prediction y(cid:48), scaled to [0, 1] as ˜yi = 1
. Subsequently, we select feature jm and split position
vm that minimize the mean squared error impurity (MSE) under the randomization scheme for these
pseudo-labels. As before, we deﬁne the mean squared error impurity HMSE in terms of the branching
probabilities pl,i = Px(cid:48)∼φ(xi)[x(cid:48)

vm] and pr,i = 1

2 + yi−y(cid:48)

pl,i:

2

i

jm ≤

µj =

(cid:80)n
i=1 pj,i ˜yi
(cid:80)n
i=1 pj,i

HMSE =

(cid:80)

j∈{l,r} pj,i(˜yi
n

µj)2

.

−

(6)

−
(cid:80)n

i=1

The optimal leaf predictions can now be computed approximately [33] to

i=1 pl,i ˜y(cid:48)
i
(1
1
i −
|
and γr analogously. We initialize this boosting process with an ensemble of individually MLE-optimal
stumps and repeat this boosting step until we have added as many stumps as desired.

2˜y(cid:48)
|

i pl,i

γl =

i −

2˜y(cid:48)

1
|

− |

(7)

(cid:80)

)

,

(cid:80)n

16

A.4 Adaptive Boosting for Certiﬁable Robustness

Below, we describe ROBADABOOST, already outlined in Section 4.2, in more detail. Our goal is to
obtain a weighted ensemble ¯FK

¯FK(x) =

1
(cid:80)K
k=1 αk

K
(cid:88)

k=1

αk1P

x(cid:48)∼φ(x)[ ¯f k

M (x(cid:48))>0.5]>0.5,

(8)

→

M , that is certiﬁably robust at a pre-determined radius r. Here,

[0, 1] is a soft-classiﬁer, that predicts class 1 for outputs > 0.5 and class 0 else.

consisting of K stump ensembles ¯f k
¯FK(x) : Rd
To train the K constituting ensembles such that the overall ensemble ¯FK is certiﬁably robust at radius
r, we proceed as follows: First, we initialize the weights of all samples xi to w1
n . Then, for
k = 1 to K, we iteratively ﬁt a new stump ensemble ¯f k
M as described in Section 4.1 using the sample
weights wk
i . Then, similar to Freund and Schapire [34] although targeting certiﬁability instead of
accuracy, we update the sample weights as follows: First, we compute whether the newly trained k-th
ensemble ¯f k

M is certiﬁably correct (ci) for each sample xi in the training set:

i = 1

ci =

(cid:40)1P
1P

x(cid:48)∼φ(xi)[ ¯f k
x(cid:48)∼φ(xi)[ ¯f k

M (x(cid:48))≤0.5]>ρ−1
M (x(cid:48))>0.5]>ρ−1

x (r)

x (r)

if y = 0

if y = 1.

(9)

Then, we determine the certiﬁable error errk, and the model weight αk of f k

m as:

errk =

(cid:80)n

i=1 wi(1
(cid:80)n

−
i=1 wi

ci)

αk = log

1

errk

−
errk

and update the sample weights for the next iteration to:

wk+1

i =

wk
i=1 wk

(cid:80)n

i exp(αk(1

−
i exp(αk(1

ci))

ci))

−

before training the next ensemble. This way, we are minimizing the overall loss for certiﬁed
predictions at radius r.
To certify ¯FK at a speciﬁc radius r, we now have to show that we can certify individual ensembles
corresponding to at least half the total weights, or more formally (here, without loss of generality
assuming a label of y = 1):

K
(cid:88)

k=1

αk1P

x(cid:48)∼φ(x)[ ¯f k

M (x(cid:48))>0.5]>ρ−1

x (r) >

(cid:80)K

αk

k=1 |
2

|

.

(10)

To compute the certiﬁable radius for ¯Fk, we compute the certiﬁable radii Rk of the individual
Rk+1 and obtain the largest radius Rk such
ensembles, sort them in decreasing order such that Rk
that (cid:80)k
. Intuitively, we need to ﬁnd a subset of models such that their weighted
predictions for class 1 reach at least half the possible weight, accounting for negative weights.

l=1 αl >

l=1 |αl|
2

(cid:80)K

≥

B Experimental Details

Here, we describe our experimental setup in greater detail. Note that we also publish all code, models,
and instructions required to reproduce our results at ANONYMIZED.

B.1 Datasets

In this section, we describe the datasets we use in detail.

Datasets with Numerical Features We conduct our experiments considering exclusively numerical
features on the same datasets as prior work [23, 22]. More concretely, we use the tabular datasets
BREASTCANCER [37] and DIABETES [36], where we follow prior work [23] in using the ﬁrst 80%
of the samples as train set and the remaining 20% as test set, normalizing the data to [0, 1], and the

17

vision datasets MNIST 1 VS. 5 [39], MNIST 2 VS. 6 [39], and FMNIST-SHOES [38], where we
use all samples of the right classes from the train and test sets.

Datasets with Numerical and Categorical Features We conduct our experiments on the joint
certiﬁcation of numerical and categorical features using the popular ADULT [37] and CREDIT [37]
datasets. For both, we use the ﬁrst 70% of the samples as the train set, and the remaining 30% as
the test set. Here, we normalize the numerical features using the mean and standard deviation of the
training data, before applying any perturbations.

The ADULT [37] dataset is a societal dataset based on the 1994 US Census database. It contains
eight categorical and six numerical variables for each individual. The cardinalities of the categorical
variables range from 2 to 42 (concretely, they are 9, 16, 7, 15, 6, 5, 2, and 42). The task is to predict
whether an individual’s salary is below or above 50k USD.

The CREDIT [37] dataset is a ﬁnancial dataset containing 13 categorical and 7 numerical fea-
tures. The cardinalities of the categorical features range from 2 to 10 (concretely, they are
4, 5, 10, 5, 5, 4, 3, 4, 3, 3, 4, 2, and 2). The task is to predict whether a customer has a low or high risk
to default on a loan.

Both datasets exhibit a signiﬁcant class imbalance, with the minority class constituting 24.6% of
the ADULT and 29.6% of the CREDIT train set. Therefore, we report balanced certiﬁed accuracy,
computed as the arithmetic mean of the per class certiﬁed accuracies.

B.2 Training Details

The key (and for independent training, the only)
hyper-parameter of our approach is the noise
magnitude, λ for (cid:96)1-certiﬁcation and σ for (cid:96)2-
certiﬁcation. In Table 5, we report the noise
levels chosen for the different datasets. We dis-
cuss the effect of different noise magnitudes in
App. C.4 and observe that results are generally
quite stable across a wide range of noise magni-
tudes. Unless otherwise stated, we determine the
split position vm via linear search using incre-
ments of size 0.01 and discretize leaf predictions
γ using 100 steps (i.e., ∆ = 100).

ROBTREEBOOST We initialize ROBTREE-
BOOST with an ensemble of independently
trained stumps and add a further nb stumps as
described in Section 4.2 using the qth percentile
to compute the certiﬁable predictions. We chose
q and nb as shown in Table 6.

Table 5: Noise levels λ for (cid:96)1 and σ for (cid:96)2-
certiﬁcation utilized for Table 2 for various
datasets.

Method

Dataset

λ (for (cid:96)1)

σ (for (cid:96)2)

Independent

Boosting

BREASTCANCER
DIABETES
MNIST 1 VS. 5
MNIST 2 VS. 6
FMNIST-SHOES

BREASTCANCER
DIABETES
MNIST 1 VS. 5
MNIST 2 VS. 6
FMNIST-SHOES

2.00
0.35
4.00
4.00
4.00

2.00
0.28
4.00
4.00
4.00

4.00
0.25
0.25
0.25
0.25

0.25
0.15
0.25
0.25
0.25

Table 6: ROBTREEBOOST parameters.
Parameter

Perturbation

BREASTCANCER DIABETES

Percentile q

Additional stumps nb

(cid:96)1
(cid:96)2

(cid:96)1
(cid:96)2

0.60
0.98

30
40

0.70
0.95

15
100

ROBADABOOST To evaluate ROBADABOOST, we consider ensembles of K = 20 individual stump
ensembles in each of our experiments. We choose the same noise magnitudes as for independently
trained stumps, described in Table 5.

Joint Certiﬁcation For joint certiﬁcation, we use ensembles of independently trained decision stumps,
one for each feature. The stump corresponding to categorical features maps a categorical value to
either 0.375 or 0.625 (which are the same distance from the decision threshold 0.5), depending on
whether the majority of the samples with this categorical value have class 0 or 1, respectively. Note
that permitting arbitrary leaf predictions slightly improves clean accuracy, but signiﬁcantly worsens
worst-case behaviour. Choosing leaf predictions further from the decision threshold gives more
emphasis to categorical variables compared to numerical ones. The stumps for the numerical features
are learned individually, as described in Section 4.1. For (cid:96)1, we used the noise magnitude λ = 2.0
and for (cid:96)2-certiﬁcation σ = 0.25.

18

Table 8: Certiﬁed accuracy (CA) [%] under joint (cid:96)0- and (cid:96)2-perturbations of categorical and numerical
features, respectively, depending on whether model uses categorical and/or numerical features. The
natural accuracy is the CA at radius r = 0.0. Larger is better.

Dataset

Categorical
Features

(cid:96)0 Radius r0

CA without
Numerical Features

ADULT

CREDIT

no

yes

no

yes

-

0
1
2
3

-

0
1
2
3

-

70.2
52.4
27.8
6.7

-

71.0
49.3
26.7
8.7

CA with Numerical Features at (cid:96)2 Radius r2

0.00

74.4

69.7
57.6
43.4
29.6

55.0

73.7
52.3
31.0
14.3

0.25

65.6

65.5
53.3
38.8
24.8

44.7

69.7
48.7
27.7
11.3

0.50

37.3

58.7
44.7
29.1
17.2

30.0

66.3
42.0
24.7
8.7

0.75

23.7

54.4
38.1
20.5
9.0

16.7

60.7
39.0
20.3
6.3

1.00

10.4

35.4
22.3
12.1
5.1

9.3

58.3
37.3
17.0
5.7

1.25

1.50

5.1

25.7
14.4
8.8
4.0

6.7

55.7
35.7
13.7
5.0

2.5

20.3
11.0
6.6
2.8

4.0

53.7
34.3
13.0
4.3

B.3 Computational Resources and Experimental Timings

In this section, we describe the computational resources required for our experiments. We run all our
experiments using 24 cores of an Intel Xeon Gold 6242 CPUs and a single NVIDIA RTX 2080Ti and
report timings for the full experiment in App. B.3. We show timings in Table 7.

(cid:96)1

Norm

Dataset

Boosting

Training Certiﬁcation

Training Certiﬁcation

Table 7: Experimental timings for whole datasets.
Independent

We observe that all certiﬁcation is extremely
quick with FMNIST-SHOES taking the longest
at 6s for the whole test set and an ensemble of
independently trained stumps in the (cid:96)1-setting,
translating to 0.003s per sample. When evaluat-
ing models in single instead of double precision,
we can, e.g., further reduce certiﬁcation times
from 3s to 1.2s for MNIST 2 VS. 6. The in-
dependently MLE-optimal training is similarly
quick, allowing us to run all core experiments in
less than 5 minutes. Only ROBADABOOST takes more than one minute for an individual experiment,
as it involves training and certifying 20 stump ensembles. For datasets combining categorical and
numerical features, the training and certiﬁcation for the categorical variables is almost instantaneous
and dominated by that for the numerical features. The latter requires 19.9s and 47.0s for the (cid:96)1
and (cid:96)2-experiment, respectively, on ADULT and 1.5s respectively 2.0s on CREDIT. We remark that
computational efﬁciency was not a main focus of this work and we did not optimize runtimes.

BREASTCANCER
DIABETES
MNIST 1 VS. 5
MNIST 2 VS. 6
FMNIST-SHOES

BREASTCANCER
DIABETES
MNIST 1 VS. 5
MNIST 2 VS. 6
FMNIST-SHOES

< 0.1s
< 0.1s
4s
3s
4s

< 0.1s
< 0.1s
5s
4s
6s

< 0.1s
< 0.1s
29s
26s
27s

< 0.1s
< 0.1s
27s
16s
40s

9s
47s
10min
9min
9min

14s
2s
13min
11min
13min

2s
2s
14s
14s
15s

2s
2s
32s
29s
31s

(cid:96)2

C Additional Experiments

In this section, we extend our experimental evaluation from Section 5. Concretely, in App. C.1, we
provide additional experiments on the joint certiﬁcation of categorical and numerical variables. In
App. C.2, we compare DRS to RS in more detail while in App. C.3, we continue our investigation
of our MLE optimality criterion. Moreover, in App. C.4, we provide additional experiments on the
effect of the noise magnitudes λ and σ for (cid:96)1- and (cid:96)2-certiﬁcation, respectively. Finally, in App. C.5,
we analyze the impact of the discretization granularity and in App. C.6, we evaluate the effect of an
approximate split position optimization.

C.1 Additional Experiments on Joint Robustness Certiﬁcates

In Table 8, we report the (unbalanced) certiﬁed accuracies corresponding to the balanced ones
reported in Table 3.

Similarly, we report the balanced and unbalanced certiﬁed accuracies at a range of (cid:96)1 radii over
the numerical features given varying perturbations of the categorical features in Tables 9 and 10
and observe similar trends as in the (cid:96)2-setting. In particular, models utilizing both categorical and
numerical features outperform those using only either one on clean data. Interestingly, the slower drop

19

Table 9: Balanced certiﬁed accuracy (BCA) [%] under joint (cid:96)0- and (cid:96)1-perturbations of categorical
and numerical features, respectively, depending on whether model uses categorical and/or numerical
features. The balanced natural accuracy is the BCA at radius r = 0.0. Larger is better.

Dataset

Categorical
Features

(cid:96)0 Radius r0

BCA without
Numerical Features

ADULT

CREDIT

no

yes

no

yes

-

0
1
2
3

-

0
1
2
3

-

76.6
57.4
33.5
8.9

-

70.7
48.2
26.4
7.8

BCA with Numerical Features at (cid:96)1 Radius r1

0.00

62.8

77.3
60.1
39.0
17.6

58.4

71.2
49.0
28.0
8.0

0.25

59.4

76.7
59.5
37.9
15.8

56.1

71.2
48.8
27.8
8.0

0.50

54.9

76.0
58.7
37.0
14.7

52.0

71.2
48.0
27.6
8.0

0.75

51.7

75.4
57.9
36.2
13.7

50.4

71.2
48.0
26.5
7.8

1.00

47.7

74.3
57.1
35.6
12.8

39.5

71.0
46.8
26.5
7.8

1.25

44.8

73.1
56.3
35.1
12.2

32.6

71.0
46.1
26.5
7.8

1.50

42.3

71.7
55.2
34.4
11.8

25.1

70.2
45.5
24.9
6.8

Table 10: Certiﬁed accuracy (CA) [%] under joint (cid:96)0- and (cid:96)1-perturbations of categorical and
numerical features, respectively, depending on whether model uses categorical and/or numerical
features. The natural accuracy is the CA at radius r = 0.0. Larger is better.

Dataset

Categorical
Features

(cid:96)0 Radius r0

CA without
Numerical Features

ADULT

CREDIT

no

yes

no

yes

-

0
1
2
3

-

0
1
2
3

-

70.2
52.4
27.8
6.7

-

71.0
49.3
26.7
8.7

CA with Numerical Features at (cid:96)1 Radius r1

0.00

80.0

70.3
53.9
32.4
12.5

68.3

71.3
49.7
27.7
9.0

0.25

77.3

69.7
53.3
31.3
11.3

66.0

71.3
49.3
27.3
9.0

0.50

72.8

68.8
52.6
30.3
10.3

60.7

71.3
48.7
27.0
9.0

0.75

69.4

67.9
51.8
29.3
9.4

59.3

71.3
48.7
26.3
8.7

1.00

64.5

66.5
50.9
28.7
8.7

46.3

71.0
47.0
26.3
8.7

1.25

61.1

65.2
50.1
28.3
8.3

39.3

71.0
46.0
26.3
8.7

1.50

58.0

63.9
49.1
27.8
8.1

31.3

70.3
45.7
25.0
7.3

in certiﬁed accuracy with increasing perturbation of the numerical features is much more pronounced
in the (cid:96)1-setting, and much higher certiﬁed accuracies are obtained even at large radii. For example,
on ADULT, just considering numerical features leads to a BCA of 62.8% at radius r1 = 0.0 dropping
to 42.3% at r1 = 1.5. In contrast, when also utilizing categorical features, the BCA at r1 = 0.0
is 77.3% dropping only to 71.7% at r1 = 1.5, when no categorical variable is perturbed (r0 = 0).
Similarly, when at most one categorical variable is perturbed, the BCA at r1 = 0.0 is 60.1% and drops
only to 55.2% radius r1 = 1.5. This highlights that, when available, utilizing categorical features in
addition to the numerical ones is essential and can make models signiﬁcantly more certiﬁably robust
to perturbations of the numerical variables, even when categorical variables can be perturbed as well.

C.2 Derandomized vs. Randomized Smoothing

Here, we compare evaluating stump ensembles deterministically via DRS (Section 3) to sampling-
based RS [24]. In Table 11, we provide quantitative results corresponding to Fig. 6, expanded
by an equivalent experiment for (cid:96)1-norm perturbations. We observe that as sampling-based RS
uses increasingly more samples, it converges towards DRS. This convergence is much faster in
the (cid:96)1-setting. However, especially in the (cid:96)2-setting, a notable gap remains even when using as
many as 100 000 samples. This is expected as sampling-based RS computes a lower conﬁdence
bound to the true success probability, which can be computed exactly with DRS. Thus the higher the
desired conﬁdence, the larger this gap will be. Further, if RS were to yield a larger radius than DRS,
this would actually be an error, occurring with probability α, as DRS computes the true maximum
certiﬁable radius. This highlights another key difference: RS provides probabilistic guarantees that
hold with conﬁdence 1
α, while DRS provides deterministic guarantees. Moreover, for RS, many
samples have to be evaluated (typically n = 100 000), while DRS can efﬁciently compute the exact
CDF. We note that the much larger improvement in certiﬁed radii observed for (cid:96)2-norm perturbations

−

20

Table 11: We compare certifying the same stump ensembles via Deterministic Smoothing (DRS) and
Randomized Smoothing (RS) with respect to the average certiﬁed radius (ACR) and the certiﬁed
accuracy [%] at numerous radii r on MNIST 1 VS. 5 for (cid:96)1 (λ = 4.0) and (cid:96)2 (σ = 0.5) norm
perturbations. Larger is better.

Norm

Method

ACR

Certiﬁed Accuracy at Radius r

0.0

0.50

1.00

1.50

2.00

2.50

3.00

3.50

(cid:96)1

(cid:96)2

RS (n = 100)
RS (n = 1000)
RS (n = 10000)
RS (n = 100000)
DRS (ours)

RS (n = 100)
RS (n = 1000)
RS (n = 10000)
RS (n = 100000)
DRS (ours)

2.809
3.337
3.430
3.456
3.467

0.680
1.102
1.403
1.627
2.161

93.0
95.6
96.0
96.1
96.6

94.8
95.6
95.9
95.9
96.0

91.2
94.4
95.3
95.5
95.6

90.1
92.5
92.9
93.0
93.0

88.6
92.8
93.7
94.0
94.1

0.0
85.0
86.9
87.3
87.5

86.2
90.6
91.6
91.9
92.1

0.0
0.0
75.0
78.1
79.0

82.9
87.8
89.4
89.9
89.9

0.0
0.0
0.0
0.0
65.3

77.0
84.7
85.8
86.3
86.5

0.0
0.0
0.0
0.0
40.5

68.8
79.5
82.1
82.9
83.1

0.0
0.0
0.0
0.0
12.3

0.0
70.4
73.8
74.6
75.1

0.0
0.0
0.0
0.0
5.9

Table 12: We compare training stump ensembles optimally via MLE-optimal criterion, training them
via noisy sampling (Sampling) and default training (Default) with respect to the average certiﬁed
radius (ACR) and the certiﬁed accuracy [%] on MNIST 2 VS. 6 at numerous radii r on various
norms for multiple noise magnitudes (λ for (cid:96)1 and σ for (cid:96)2). Larger is better.

Certiﬁed Accuracy at Radius r

Norm λ ((cid:96)1) or σ ((cid:96)2)

Method

ACR

0.0

0.5

1.0

(cid:96)1

(cid:96)2

1.0

4.0

16.0

0.25

1.0

4.0

Default
Sampling
MLE (Ours)

Default
Sampling
MLE (Ours)

Default
Sampling
MLE (Ours)

Default
Sampling
MLE (Ours)

Default
Sampling
MLE (Ours)

Default
Sampling
MLE (Ours)

0.519
0.928
0.931

2.074
3.166
3.282

8.297
6.646
8.574

0.967
1.628
1.642

3.436
1.594
1.724

12.167
1.095
1.652

51.9
96.2
96.2

51.9
96.3
96.3

51.9
96.4
96.2

51.9
96.3
96.3

51.9
95.5
95.5

51.9
89.2
95.1

51.9
93.9
94.3

51.9
95.0
95.4

51.9
95.3
95.7

51.9
92.8
93.0

51.9
89.1
90.1

51.9
72.9
88.7

51.9
64.8
66.2

51.9
93.3
93.9

51.9
94.4
95.0

51.8
85.9
86.3

51.9
76.5
79.2

51.9
50.9
76.5

1.5

0.0
0.0
0.0

51.9
90.5
91.7

51.9
93.4
94.1

48.7
71.7
73.0

51.9
57.9
62.5

51.9
32.6
59.2

2.0

0.0
0.0
0.0

51.9
87.3
88.7

51.9
91.8
93.2

0.0
0.0
0.0

51.9
33.5
40.3

51.9
15.8
36.6

2.5

0.0
0.0
0.0

51.9
81.4
84.1

51.9
90.0
91.7

0.0
0.0
0.0

51.9
11.7
18.7

51.9
4.0
16.3

3.0

0.0
0.0
0.0

51.9
72.5
76.0

51.9
87.8
90.6

0.0
0.0
0.0

51.9
2.1
5.4

51.9
0.5
4.9

3.5

0.0
0.0
0.0

51.9
56.0
62.8

51.9
84.9
88.4

0.0
0.0
0.0

51.9
0.2
1.3

51.9
0.0
1.5

is due to the signiﬁcantly higher sensitivity of the certiﬁably radius w.r.t. the success probability (see
Table 1).

C.3 MLE Optimality Criterion

In Table 12, we compare our robust MLE optimality criterion (MLE) to applying the standard
entropy criterion to samples drawn from the input randomization scheme (Sampling) or the clean
data (Default). We observe that training approaches accounting for randomness (i.e., Sampling and
MLE) consistently outperform default training. In some cases, default training even suffers from
a mode collapse, always predicting the same class. Amongst the two methods accounting for the
input randomization, our MLE optimality criterion consistently outperforms samplings at all noise
magnitudes and for both perturbation types. This effect is particularly pronounced at large noise
magnitudes, where sampling becomes less effective at capturing the input distribution.

21

Table 13: Comparison of average certiﬁed radius (ACR) and certiﬁed accuracy at various radii r with
respect to the (cid:96)1 norm for numerous datasets and noise magnitudes λ. Larger is better.

Dataset

λ

ACR

FMNIST-SHOES

MNIST 1 VS. 5

MNIST 2 VS. 6

0.5
1.0
2.0
4.0
8.0
16.0

0.5
1.0
2.0
4.0
8.0
16.0

0.5
1.0
2.0
4.0
8.0
16.0

0.407
0.766
1.463
2.780
4.755
7.975

0.476
0.934
1.808
3.467
6.472
8.957

0.477
0.931
1.780
3.282
5.617
8.574

Certiﬁed Accuracy at Radius r

1.0

0.0
55.1
74.9
80.2
80.0
81.7

0.0
77.0
92.1
94.1
95.4
88.6

0.0
66.2
92.2
93.9
94.6
95.0

2.0

0.0
0.0
47.0
73.3
75.5
77.8

0.0
0.0
62.8
89.9
93.3
86.6

0.0
0.0
43.0
88.7
91.4
93.2

3.0

0.0
0.0
0.0
60.9
70.3
75.0

0.0
0.0
0.0
83.1
91.0
83.5

0.0
0.0
0.0
76.0
87.4
90.6

4.0

0.0
0.0
0.0
21.3
63.9
71.7

0.0
0.0
0.0
39.1
87.4
80.3

0.0
0.0
0.0
3.8
80.9
86.5

5.0

0.0
0.0
0.0
0.0
56.4
67.3

0.0
0.0
0.0
0.0
82.2
77.4

0.0
0.0
0.0
0.0
71.7
82.7

6.0

0.0
0.0
0.0
0.0
46.5
63.2

0.0
0.0
0.0
0.0
75.1
72.9

0.0
0.0
0.0
0.0
56.6
77.5

7.0

0.0
0.0
0.0
0.0
32.6
57.9

0.0
0.0
0.0
0.0
60.7
67.4

0.0
0.0
0.0
0.0
31.3
70.5

8.0

0.0
0.0
0.0
0.0
1.9
52.9

0.0
0.0
0.0
0.0
4.4
61.9

0.0
0.0
0.0
0.0
0.0
62.7

0.0

84.4
83.5
83.7
85.8
83.9
84.3

96.3
96.3
96.2
96.6
97.0
90.4

96.3
96.2
96.2
96.3
96.5
96.2

9.0

0.0
0.0
0.0
0.0
0.0
47.0

0.0
0.0
0.0
0.0
0.0
56.2

0.0
0.0
0.0
0.0
0.0
53.3

10.0

0.0
0.0
0.0
0.0
0.0
41.1

0.0
0.0
0.0
0.0
0.0
49.6

0.0
0.0
0.0
0.0
0.0
41.3

Table 14: Comparison of average certiﬁed radius (ACR) and certiﬁed accuracy at various radii r with
respect to the (cid:96)2 norm for numerous datasets and noise magnitudes σ. Larger is better.

Dataset

σ

ACR

FMNIST-SHOES

MNIST 1 VS. 5

MNIST 2 VS. 6

0.25
0.5
1.0
2.0
4.0
8.0

0.25
0.5
1.0
2.0
4.0
8.0

0.25
0.5
1.0
2.0
4.0
8.0

1.361
1.723
1.699
1.681
2.136
1.518

1.737
2.161
2.044
2.012
1.875
1.808

1.642
1.824
1.724
1.688
1.652
1.718

Certiﬁed Accuracy at Radius r

0.0

86.8
86.5
86.2
86.2
57.1
83.7

95.8
96.0
96.0
95.8
94.8
96.1

96.3
95.8
95.5
95.4
95.1
74.3

0.5

79.6
78.9
78.5
78.5
52.2
74.2

93.6
93.0
92.7
92.7
87.2
90.2

93.0
91.2
90.1
89.5
88.7
61.0

1.0

70.0
70.1
69.1
68.8
49.7
64.4

89.0
87.5
86.1
85.8
71.8
80.3

86.3
81.9
79.2
78.0
76.5
53.2

1.5

58.2
56.6
55.7
55.1
47.9
51.0

82.8
79.0
75.6
74.9
48.1
62.9

73.0
66.7
62.5
60.9
59.2
49.4

2.0

0.0
42.2
41.0
40.2
46.0
35.4

0.0
65.3
57.3
56.2
34.7
36.4

0.0
46.4
40.3
38.8
36.6
46.6

2.5

0.0
27.8
25.8
25.6
43.4
21.0

0.0
40.5
28.4
26.9
29.9
20.4

0.0
23.4
18.7
17.5
16.3
40.2

3.0

0.0
17.4
16.9
16.8
39.4
10.4

0.0
12.3
11.4
10.3
23.8
13.3

0.0
7.4
5.4
4.9
4.9
30.3

3.5

0.0
8.4
8.9
8.7
35.0
4.8

0.0
5.9
6.7
6.0
15.7
7.7

0.0
1.3
1.3
1.0
1.5
17.4

C.4 Effect of Noise Level

Here, we provide additional experiments for varying
noise magnitudes, λ for (cid:96)1-certiﬁcation, and σ for (cid:96)2-
certiﬁcation. In Tables 13 and 14, we provide extensive
experiments for the (cid:96)1- and (cid:96)2-setting, respectively, which
we visualize in Figs. 7 and 8.

We observe that, in the (cid:96)1-setting, the natural accuracy
(certiﬁed accuracy at radius 0) is quite insensitive to an
increase in noise magnitude. Consequently, large λ lead to
exceptionally large ACR and certiﬁed accuracies even at
large radii, e.g., on MNIST 2 VS. 6, we obtain a certiﬁed
accuracy of 82.7% at (cid:96)1-radius r = 5.0.

22

Figure 8: Comparing DRS for various noise
levels σ on MNIST 1 VS. 5.

0.01.02.03.04.0‘2Radiusr20255075100CertiﬁedAccuracy[%]σ=0.25σ=0.5σ=1.0σ=2.0σ=4.0σ=8.0Table 15: We compare the performance of models for different number of discretization sizes with
respect to average certiﬁed radius (ACR) and given certiﬁed accuracies (CA) [%] on MNIST 1 VS.
5. We utilize λ = 4.0 for (cid:96)1 and σ = 0.5 for (cid:96)2. Larger is better.

Norm Discretizations

ACR

(cid:96)1

(cid:96)2

2
3
5
10
25
50
100
250
500
1000

2
3
5
10
25
50
100
250
500
1000

1.834
2.240
2.240
2.167
2.240
3.530
3.467
3.432
3.421
3.413

1.747
2.223
2.223
2.076
2.371
2.152
2.161
2.165
2.168
2.168

Certiﬁed Accuracy [%] at Radius r

0.00

59.1
56.0
56.0
71.4
56.0
96.9
96.6
96.1
95.9
95.8

44.0
56.0
56.0
96.6
88.5
96.2
96.0
95.9
95.8
95.8

0.50

51.6
56.0
56.0
67.4
56.0
96.1
95.6
94.9
94.8
94.6

44.0
56.0
56.0
92.6
84.0
93.0
93.0
93.1
93.1
93.1

1.00

45.1
56.0
56.0
63.1
56.0
94.8
94.1
93.5
93.2
93.0

44.0
56.0
56.0
87.3
77.1
87.7
87.5
87.5
87.3
87.3

1.50

44.0
56.0
56.0
58.0
56.0
93.4
92.1
91.5
91.5
91.5

44.0
56.0
56.0
77.2
69.6
78.9
79.0
79.1
79.1
79.1

2.00

44.0
56.0
56.0
53.4
56.0
91.6
89.9
88.9
88.6
88.4

44.0
56.0
56.0
57.5
62.5
64.3
65.3
65.4
65.6
65.7

2.50

44.0
56.0
56.0
48.1
56.0
89.0
86.5
85.6
85.5
85.2

44.0
56.0
56.0
27.8
55.0
39.1
40.5
41.5
42.0
42.0

3.00

44.0
56.0
56.0
44.6
56.0
85.2
83.1
81.5
81.0
80.5

44.0
56.0
56.0
15.3
46.4
12.0
12.3
12.5
12.4
12.4

3.50

43.9
56.0
56.0
43.9
56.0
78.5
75.1
73.9
73.5
73.3

44.0
56.0
56.0
9.1
30.9
6.4
5.9
5.8
5.6
5.6

In the (cid:96)2-setting, increasing the noise magnitude σ generally leads to a more pronounced drop in
natural and certiﬁed accuracy, and thus similar ACRs for various noise magnitudes.

∼

−

Unif([x

Thinking Outside the Box Analysing this surprising be-
haviour in the (cid:96)1-setting, we empirically ﬁnd that despite
the data being normalized to [0, 1], the MLE optimality
criterion often yields split positions vm outside of [0, 1].
Recall that there, uniformly distributed random noise is
λ, x + λ]d)).
added to the original sample (x(cid:48)
In Fig. 9, we show a histogram of the split positions (vm), il-
lustrating this behaviour. In the (cid:96)1-setting and using λ = 2,
all split positions are either smaller than
1 or larger than
1.9, which are exactly the borders of uniform distributions
with λ = 2 centered at the extremes of the image domain
([0, 1]). As all splits are outside the hyperbox constituting
the original image domain, we refer to this behaviour as
’thinking outside the box’. Intuitively, each unperturbed
data point is on the same side of vm in this case, but when
1, 2] leads to a probability mass of 0 for
the randomization scheme is applied, a split outside of [
an original feature value of 0 or 1, while for the other, the probability mass can be as high as 1
2λ .
Therefore, such splits allow the smoothed model to still separate these cases well for randomized
inputs.

Figure 9: Comparing counts for values of
vm on MNIST 2 VS. 6 for (cid:96)1 and (cid:96)2 norms
with λ = 2.0 and σ = 0.25, respectively.

−

−

While we observe this effect on all datasets in the (cid:96)1-setting given a sufﬁciently large λ, it does not
appear in the (cid:96)2-setting. There, vm’s are typically clustered closely around or inside [0, 1], as the
Gaussian randomization applied here has unbounded support and does not permit for such a clean
separation, regardless of the choice of vm.

C.5 Leaf Prediction Discretization

In the main paper, all experiments are conducted with leaf predictions discretized to ∆ = 100 values
to enable our efﬁcient CDF computation. In this section, we investigate the effect of this discretization.
Concretely, we report results on MNIST 1 VS. 5 using a range of discretization-granularities from
2 to 1000 in Table 15. While using a very coarse discretization can lead to a mode collapse and
generally degraded performance, we observe that once we use at least 50 values, a ﬁner discretization

23

−2−10123SplitPositionvm0100200300400500Countsxminxmax‘1‘2Table 16: We compare the performance of models for different binning sizes with respect to average
certiﬁed radius (ACR) and given certiﬁed accuracies (CA) [%] on MNIST 1 VS. 5. We utilize
λ = 4.0 for (cid:96)1 and σ = 0.5 for (cid:96)2. Larger is better.

Norm Binning Size

ACR

(cid:96)1

(cid:96)2

4.0
2.0
1.0
0.5
0.1
0.05
0.01
0.005
0.001
0.0005
0.0001

4.0
2.0
1.0
0.5
0.1
0.05
0.01
0.005
0.001
0.0005
0.0001

3.452
3.452
3.468
3.465
3.462
3.466
3.467
3.467
3.467
3.466
3.467

0.584
1.888
1.980
2.119
2.161
2.160
2.161
2.163
2.164
2.164
2.163

Certiﬁed Accuracy [%] at Radius r

0.00

96.2
96.2
96.5
96.6
96.6
96.5
96.6
96.6
96.5
96.5
96.5

56.0
95.4
95.7
95.8
96.0
96.0
96.0
96.0
96.0
96.0
96.0

0.50

95.2
95.2
95.6
95.5
95.5
95.6
95.6
95.6
95.6
95.5
95.5

55.9
91.3
92.0
93.0
93.1
93.1
93.0
93.1
93.0
93.0
93.0

1.00

93.6
93.6
94.1
94.2
94.2
94.1
94.1
94.1
94.1
94.1
94.2

31.5
83.6
84.1
87.2
87.5
87.5
87.5
87.5
87.5
87.5
87.5

1.50

91.7
91.7
92.0
91.9
92.0
91.9
92.1
92.1
92.1
92.1
92.1

0.0
73.9
74.0
78.5
79.1
79.1
79.0
79.0
79.0
79.0
79.0

2.00

89.4
89.4
89.9
89.9
89.9
89.9
89.9
89.9
90.0
90.0
89.9

0.0
55.7
58.8
62.6
65.2
65.3
65.3
65.3
65.3
65.3
65.3

2.50

86.3
86.3
86.5
86.6
86.4
86.5
86.5
86.5
86.5
86.5
86.5

0.0
22.4
35.2
35.7
40.8
41.4
40.5
40.9
41.1
41.2
41.1

3.00

82.7
82.7
83.1
83.1
83.0
83.1
83.1
83.1
83.1
83.0
83.1

0.0
4.0
4.8
11.5
12.3
12.4
12.3
12.4
12.5
12.5
12.5

3.50

75.3
75.3
75.2
75.0
74.8
75.1
75.1
75.1
75.1
75.1
75.1

0.0
0.8
0.6
6.5
5.9
5.8
5.9
5.9
5.8
5.8
5.8

has only a minimal effect on the model behaviour. This suggests, that using ∆ = 100 our discretized
smoothed model behaves very similar to a non-discretized one.

C.6 Split Position Search Granularity

In our main paper, all experiments are conducted using a step size of 0.01 to conduct the line search
for the optimal split position vm. In Table 16, we report results for search granularities from 4.0 to
10−4 and observe that a step size of 0.1 is sufﬁciently ﬁne and reducing it further does not improve
the performance of the obtained models. This suggest that our approximate optimization based on
line search comes very close to the ﬁnding the true optimal split position and thus jointly MLE
optimal vm and γ.

24

