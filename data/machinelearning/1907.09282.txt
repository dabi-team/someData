9
1
0
2

l
u
J

2
2

]
E
S
.
s
c
[

1
v
2
8
2
9
0
.
7
0
9
1
:
v
i
X
r
a

Learning the Relation between Code Features and Code
Transforms with Structured Prediction

ZHONGXING YU, KTH Royal Institute of Technology
MATIAS MARTINEZ, Université Polytechnique Hauts-de-France
TEGAWENDÉ F. BISSYANDÉ, SnT, University of Luxembourg
MARTIN MONPERRUS, KTH Royal Institute of Technology

We present in this paper the first approach for structurally predicting code transforms at the level of AST
nodes using conditional random fields. Our approach first learns offline a probabilistic model that captures
how certain code transforms are applied to certain AST nodes, and then uses the learned model to predict
transforms for new, unseen code snippets. We implement our approach in the context of repair transform
prediction for Java programs. Our implementation contains a set of carefully designed code features, deals with
the training data imbalance issue, and comprises transform constraints that are specific to code. We conduct a
large-scale experimental evaluation based on a dataset of 4,590,679 bug fixing commits from real-world Java
projects. The experimental results show that our approach predicts the code transforms with a success rate
varying from 37.1% to 61.1% depending on the transforms.

1 INTRODUCTION

The source code of a computer program evolves under a sequence of edits. Those edits are not
random: they capture the evolution of the specification and they reflect the semantic constraints
of the programming language. In other words, there is a probability distribution over the edit
space. This probability distribution is conditioned by two factors: the programming language and
the current version of the program. In this paper, we make a contribution to better capture this
probability distribution.

This is an important problem. It has implications in two research areas: program synthesis and
program repair. In program synthesis, capturing the probability distribution of code evolution is
one technique to steer the search in the immense synthesis search space [Gulwani et al. 2017]. In
program repair, this probability distribution can not only enable a more focused exploration of the
search space, but can also result in better patches [Long and Rinard 2016]. If one contemplates the
umbrella topic of “automated code evolution”, which also includes for example super-optimization
[Joshi et al. 2002], refactoring [Mens and Tourwé 2004], and diversification [Homescu et al. 2013],
one can consider that the probability distribution of edits over a given program is the foundation
to achieve effective search.

The problem of computing the probability distribution of edits given a program is an unsolved
problem. Yet, it is being investigated, especially with the idea to approximate the distribution with
a data-driven approach by analyzing past software evolution [Chen et al. 2018; Le et al. 2016; Wang
et al. 2018]. For instance, in the context of automated program repair, the Prophet system [Long
and Rinard 2016] analyses a set of past commits extracted from version control systems in order to
compute the likelihood of a given patch. The fundamental problem is a representation problem: we
need to identify the proper representation for both the program and the edit. If the representation
is too fine-grain (e.g., at the character or the token level), one would need tremendous amount of
data or memory to capture the probability distribution. If the representation is too coarse-grain,
the relation between code and edits becomes so vague, making it useless for driving automated
evolution.

Authors’ addresses: Zhongxing Yu, KTH Royal Institute of Technology, zhoyu@kth.edu; Matias Martinez, Université
Polytechnique Hauts-de-France, matias.martinez@univ-valenciennes.fr; Tegawendé F. Bissyandé, SnT, University of Luxem-
bourg, tegawende.bissyande@uni.lu; Martin Monperrus, KTH Royal Institute of Technology, martin.monperrus@csc.kth.se.

 
 
 
 
 
 
In this paper, we propose an approach to learn the relation between code and edits. This approach

is novel and effective. Its novelty lies in the representation of both programs and edits.

Representing programs. Programs are represented with rich combination of abstract syntax
trees and carefully engineered features. This representation has two advantages: 1) the learning
algorithm has access to the full program information (all tokens), as well as to the AST tree structure
and the AST node types; 2) the learning algorithm does not have to extract the probability from
scratch, it can leverage the human knowledge encoded in the features to better and faster identify
the signal from the noise in the learning data.

Representing edits. In our work, edits are defined as follows. An ‘edit’ is a basic operation
performed on the abstract syntaxt tree (AST) of the program before change. A ‘diff’ is the complete
set of edits done in an atomic code change, as captured by a commit in a version control system. A
code transform is an abstract view over a diff, capturing a single edit or a group of several concep-
tually related edits. In this paper, the core unit of representation for edits is code transform. We
carefully design those code transforms with two main requirements: 1) they must be automatically
extracted from commit history with high accuracy to minimize noise in the input data, and 2) they
must be precise enough to be automatically applied and to yield a well-formed program. In this
paper, we present sixteen code transforms that are specifically designed for program repair.

Learning algorithm. The learning machinery is provided by structured prediction, in particular
conditional random field (CRF) [Lafferty et al. 2001]. Structured prediction is a branch of machine
learning that is well appropriate for tree-based data, which is our case with ASTs. In recent research,
it has been successfully used on programs, including automatic deobfuscation [Raychev et al. 2015]
and automatic renaming [Alon et al. 2018]. Overall, our system takes as input ASTs annotated with
additional information, and predicts the most likely code transforms to be applied given the input
AST. This means that our learned model captures the probability distribution of edits at the level of
considered transforms.

We fully implement our approach in the context of Java programs and program repair. Our
prototype system takes as input a set of past commits and produces a probabilistic model. This
model is then used to predict the code transforms to be used for repairing a defect.

We evaluate our approach on 4,590,679 bug fixing commits. With an original experimental
methodology, we measure to what extent our approach correctly predicts the code transforms to be
applied on the program version before the commit. The core idea of the evaluation is known as ‘cross-
validation’, it works as follows: we first collect the ground-truth code transforms on all commits
and split the data into training part and testing part, we then train the model with the ground-truth
labels on the training data set, and finally we compare the predicted code transforms on the testing
data set against the ground-truth ones that were held out. We perform two series of experiments,
one on “single-transform” diffs and one on “multiple-transform” diffs. For “single-transform” diffs,
our overall best performance model achieves 61.1% accuracy. For “multiple-transform” diffs, which
is arguably a harder prediction problem, our best performance model achieves 37.1% accuracy.

To sum up, our contributions are:

• A novel approach to predict code transforms based on structured prediction.
• An implementation of the approach for repair transform prediction for Java programs. Our
implementation contains a set of carefully designed code features, deals with the training data
imbalance issue, and takes into account constraints on admissible transforms over certain
AST nodes.

• A large-scale experimental evaluation on 4,590,679 bug fixing commits. The evaluation
results show that our overall best performance model achieves 61.1% and 37.1% accuracy for
“single-transform” and “multiple-transform” cases respectively.

2

Fig. 1. An Overview of Our Approach for Predicting Code Transforms.

2 OVERVIEW

In this section, using a working example, we provide an informal overview of our approach for
predicting code transforms on AST nodes. Figure 1 gives a graphical overview. It shows the diff
of Git commit ecc184b in project Jmist1. The problem involves two wrong invocations to method
y() that should be replaced by invocations to method z(). The code transform behind this diff is
a replacement of a method invocation by another one, which is called a “Meth-RW-Meth” repair
transform in this paper. In the diff of Figure 1, there are two instances of this code transform and
they are applied to two different AST nodes.

The table at the top right hand side of Figure 1 shows the predictions of the model. The first
prediction, with the highest score 0.6, is composed of two “Meth-RW-Meth” code transforms on
nodes identified with indexes 11 and 13. Since it is the actual repair changes to be made, it means that
for this example, our approach successfully predicts the code transforms. Note that the prediction
involves the locations to apply the code transforms: “Meth-RW-Meth” points to the two AST nodes
corresponding to the invocation of y(). We now outline how our approach achieves this.

Feature Extraction. Given the buggy code snippet, our approach first parses it to construct an
AST and then extracts the following two types of features:

• The first type of feature is based on the characteristics of program elements. For instance, they
can be whether the invoked method x() has overloaded methods and whether the invocation
y() is wrapped with an if-check when called in other parts of the program. These features
are engineered, and they relate to code idioms, semantics not directly captured by the AST
(such as method overloaded), common usage.

• The second type of feature is based on the abstract syntax tree. All AST nodes are represented
with special vertices, edges, triangles that are used for structured prediction. For the specific
example in Figure 1, an excerpt of this tree structure is shown on the left hand side.

1https://github.com/bwkimmel/jmist/commit/ecc184bc08ee08159cdd79045c2ed0c4245ba59c

3

Case CYLINDER_SURFACE_BODY:      Point3 p = x. location;  --  return new Vector3(p.x() - base.x(), 0.0,                       p.y() - base.y()).unit(); ++ return new Vector3(p.x() - base.x(), 0.0,                     p.z() - base.z()).unit();   return   invo (unit) constructor call (Vector3)    binop(-)  invo(x) var(p) invo(x) var(base)   literal(0)   binop(-) invo(y) var(p) invo(y) var(base)  ParseExtract Featuresx has an overloaded methody has an if-guard when used elsewhere....Features based on analysis: Features based on AST structure: Node: {retrun, invo, binop ...}Edge: {<retrun, invo>, <binop, invo> ...}Triangle: {<binop, invo, invo> ...}Relation betweenfeatures and transformsOﬄine Learned CRF ModelPrediction1 23456789101112131411  Meth-RW-Meth 13  Meth-RW-Meth2  Meth-RW-Meth5  Wrap-Meth7  Wrap-Meth...0.600.450.38ScoreTransformsNodeIdOffline Model Learning. After extracting the features for all samples in a training dataset of
patches, our approach feeds them to a probabilistic model. The model is learned offline and captures
how certain repair transforms are applied to certain AST nodes. The model is only learned once and
then can be used to do predictions for arbitrary buggy code snippets. The model is parametrized by
a set of code transforms.

Code Transforms. The code transforms are defined in terms of their changes on the AST structure.
The AST nodes of the buggy code snippet are annotated with labels that indicate the presence
of a code transform. We then use conditional random field (CRF), to learn from these annotated
data points. The learning process makes use of the two types of features mentioned above, and
establishes the relation between the considered features and the code transforms on AST nodes.

Prediction. Finally, using the extracted features for the new, unseen buggy code snippet, the
already learned model assigns likely code transforms to AST nodes. Each assignment comes with a
score representing the probability of the transform. For the buggy code snippet shown in Figure 1,
the top-3 predictions by our trained model are shown in the table at the right hand side. The most
likely prediction, with the score of 0.6, says there is a need to apply two transforms “Meth-RW-Meth”
(replace one invocation by another one) to AST nodes with indexes 11 and 13 respectively. This
prediction is indeed correct. To repair this bug, we exactly need those two repair transforms
suggested by the most likely prediction.

Key Points. We now emphasize the key points of our approach. First, our model performs structured
prediction and predicts the repair transforms for all AST nodes of the buggy code snippet given
as input. We have a special repair transform called EMPTY which means no repair transform,
capturing the absence of change, and we call the other transforms actual repair transforms. The
EMPTY predictions are not shown in Figure 1, yet they are indeed outputs of the model. Second, our
model does prediction of transforms on specific AST nodes. For instance, there are three method
invocations involved in the buggy code snippet (i.e., unit(), x(), y()), the most likely prediction
attaches the repair transforms to the actual buggy invocation (i.e., the two calls to y()). Finally,
our model can effectively deal with the case when there need multiple actual repair transforms to
different AST nodes. Those joint transforms are learned at training time and given as outputs at
predication time, as shown in Figure 1.

3 PRELIMINARIES

Before describing our approach in detail, we first provide the necessary background and the termi-
nology that will be used throughout the paper. We do structured prediction over code transforms
on AST nodes, so we start by defining the AST.
Definition 3.1. (Abstract Syntax Tree). The abstract syntax tree (AST) for a code snippet is a
tuple < N ,T , r , δ, L, l, V , v > where N is a set of nonterminal nodes, T is a set of terminal nodes,
r ∈ N is the root node, δ : N → (N ∪ T)∗ is a function that maps a nonterminal node to its children
nodes, L is a set of node labels, l : (N ∪ T) → L is a function that maps a node to its label, V is a set
of node values, and v : (N ∪ T) → (L ∪ ϵ) is a function that maps a node to its value (can be empty).
For simplicity, we use some notations related with AST in the remaining of this paper. We use
ast to refer to a certain AST for a code snippet, n to refer to a certain AST node (either nonterminal
or terminal), n(lab, val) to refer to a certain node with label lab and value val, δ (n) to refer to
the children nodes of n, and finally l(n) and v(n) are used to denote the label and value of node n
respectively.

4

We use conditional random field (CRF) [Lafferty et al. 2001] to do the learning. Before describ-
ing CRF in detail, we first give the definition of clique and maximal clique, which are key for
understanding CRF.
Definition 3.2. (Clique and Maximal Clique). For an undirected graph G = (V, E), a clique C is
a set X of vertices of G such that every two distinct vertices are adjacent. A maximal clique is a
clique that cannot be extended by including one more adjacent vertex.

Imagine a simple undirected triangle graph, then there are three 1-vertex cliques (the vertices),
three 2-vertex cliques (the edges), and one 3-vertex clique (the triangle), and the 3-vertex clique is
the only maximal clique.

We can now give the definition of conditional random fields (CRFs). CRFs are a kind of discrimi-
native probabilistic graph model that models the conditional distribution directly, and have been
successfully used in areas such as information retrieval [Pinto et al. 2003] and computer vision [He
et al. 2004]. The formal definition of CRF is as follows [Lafferty et al. 2001].
Definition 3.3. (Conditional Random Field) Let X = {X1, ..., XN } and Y = {Y1, ..., YN } be two
sets of random variables, G = (V, E) be an undirected graph over Y such that Y is indexed by the
vertices of G, and C be the set of all cliques in G. Then (X, Y) is a conditional random field if for
any value x of X (i.e., conditioned on X), the distribution p(y|x) factorizes according to G and is
represented as:

p(y|x) = 1
Z (x)

(cid:214)

c ∈C

ψc(yc , x)

(1)

where Z(x) is a normalization function that ensures the probabilities over all possible assignments
to y sum to 1, and is defined as:

Z(x) = (cid:213)
y∈Ωx

(cid:214)

ψc(yc , x)

c ∈C
Here Ωx denotes the set of possible assignments of y for x. In the above definition, each ψc(yc , x)
is a local function that depends on the whole X but only on a subset Yc ⊆ Y which belong to the
clique c, and the value of it can be deemed as a measure of how compatible the values yc are with
each other. Like Hidden Markov Models (HMMs), each local function has the special log-linear
form over a prespecified set of feature functions {fk }K

k =1:

(2)

ψc(yc , x) = exp

(cid:27)

λk fk (yc , x)

(cid:26) K
(cid:213)

k =1

(3)

Each feature function fk : Yc × X → R is used to score assignments of subset variables Yc , and
λk is the model parameter to learn which represents the weight for feature function fk . Typically,
the same set of feature functions with the same parameters is used for every clique in the graph.

4 STRUCTURED PREDICTION OF SOURCE CODE TRANSFORMS

In this section, we introduce our approach for structured prediction of source code transforms. Our
approach works in two phases. By using the history code transform information D = {⟨ti , ni ⟩}m
i=1
for a set of m examples where ni corresponds to the AST for original code (i.e., before transform)
of example i and ti corresponds to the transforms applied to nodes of ni , we first use an offline
training phase to learn a probabilistic model that captures the conditional probability p(t | n). Once
the model is learned, we then use it to structurally predict the most likely transforms needed for
the AST nodes of a new, unseen code snippet.

5

Fig. 2. An example of input tree, output tree, and the corresponding CRF graph

We first define a conditional random field for the transform prediction task, and then discuss
how the approach can be achieved in a step-by-step manner. We give detailed specific instantiation
of the approach in the next section, but they can be instantiated in other ways.

4.1 CRF for Transform Prediction

To effectively guide the code transformation process, our aim is to precisely predict the needed
transforms at the granularity of AST node. We thus first define the following two random fields.
Definition 4.1. (Random field of AST nodes and transforms). Let P = {1, 2, 3, ..., Q} be the
set of AST nodes where the integer is a unique identifier to denote the nodes traversed in pre-
order. We associate a random field N = {N1, ..., NQ } over node symbol (a node symbol is a unique
combination of node label and node value) in each position p in the AST and another random field
T = {T1, ..., TQ } over the code transform applied to each position p in the AST.

The realizations of N (denoted by n) will be the actual nodes for a specific input AST, and the
realizations of T (denoted by t) will be the actual transforms applied to nodes of the specific input
AST. Figure 2 (a) and (b) give an example of the realizations of the two random fields where the
transforms are applied to repair the bug.

Given the two random fields N and T, we now discuss the choice of the undirected graph over
random variables in T, i.e., the choice of CRFs for transform prediction purpose. In CRF, the more
complex the graph, the more kinds of feature functions which relate all variables in a clique can be
defined, which in turn would lead to a larger class of conditional probability distributions. However,
note at the same time a complex graph will make the exact inference algorithms become intractable
[Sutton and McCallum 2012]. For the transform prediction problem, we define the undirected graph
in the following way:
Definition 4.2. (Graph for random field T) The undirected graph for random field T = {T1, ..., TQ }
is G = (V, E) such that (1) V = {T1, ..., TQ } and (2) E = {(Ti , Tj )|PC(i, j) ∨ IS(i, j)} where PC(i, j) and
IS(i, j) denote that there exist parent-child and immediate-sibling relationship between positions i
and j in the input AST tree respectively.

The undirected graph is chosen as (1) it enables to explore dependencies between transforms
applied to different AST nodes in a recursive manner both vertically and horizontally, and the
hierarchical nature of AST implies dependencies following these two recursions; (2) the maximal

6

 Assign(=)Var(Y)BinOp(+)BinOp(*)Const(3)Var(X)Var(I) A toy bug:Y = 3X + I with the correct code Y = Sin(3X) * I EmptyEmptyBinop ReplacementWrap-With-MethodEmptyEmptyEmptyT1T2T3T4T7T5T6n1n2n3n4n7n6n5t1t2t3t4t5t6t7(a) Input Tree (Random ﬁled N)(b) Output Tree (Random ﬁled T)(c) Graph for the CRFclique in the graph is triangle and exact inference algorithm is available for this kind of graph.
Figure 2 (c) shows the undirected graph for the random field T in Figure 2 (b).

The defined undirected graph contains three kinds of cliques: (1) Node clique CN which conatins
all vertices in the graph; (2) Edge clique CE which contains all connected edges in the graph; and (3)
Triangle clique CT which contains all connected triangles in the graph. For the graph in Figure 2
(c), CN = {T1, T2, T3, T4, T5, T6, T7}, CE = {(T1, T2), (T1, T3), (T2, T3), (T3, T4), (T3, T7), (T4, T7), (T4,
T5), (T4, T6), (T5, T6)}, and CT = {(T1, T2, T3), (T3, T4, T7), (T4, T5, T6)}. In the remaining of this paper,
unless explicitly specified, we refer to clique of any kinds when we talk about clique.

According to the above definition of random fields N and T and the undirected graph for random
field T, we define a CRF over the transforms t to nodes given the observable AST nodes n for the
code snippet as:

p(t|n) = 1
Z (n)

(cid:214)

c ∈C

ψc(tc , n) = 1
Z (n)

exp

(cid:26) (cid:213)

K
(cid:213)

c ∈C

k =1

(cid:27)

λk fk (tc , n)

Like our established CRF, the observable input variables and the predicted output variables in
general have the same structure for most applications of CRFs, and thus each feature function
fk (tc , n) for a certain clique c typically depends on the subset nc of n in the same clique. Considering
this, our established CRF for transform prediction is as follows:

p(t|n) = 1
Z (n)

exp

(cid:26) (cid:213)

K
(cid:213)

c ∈C

k =1

(cid:27)

λk fk (tc , nc )

(4)

where C = CN ∪ CE ∪ CT and Z(n) = (cid:205)

t∈Ωn

(cid:26)

(cid:205)

c ∈C

(cid:205)K

k =1 λk fk (tc , nc )

(cid:27)

.

The feature functions are the key components of CRF and the learned weight λk for each of
them is critical for controlling the probability of a certain assignment t given the observable n.
c for a certain clique c, a feature function fj (ts
For instance, to favor the specific assignment ts
c , nc )
can be defined with a large numerical value. With the weight λj > 0, the assignment with ts
c for
the clique c will receive a high conditional probability. We will discuss in more detail about how
feature functions can be defined for our established CRF in the next section.

4.2 Structured Prediction of Source Code Transforms Using CRF

After establishment of the CRF for transform prediction, we next describe how our approach can
be achieved. We first show how to define transforms on AST nodes, then illustrate how to define
the feature functions used in CRF, and finally describe how to train the CRF model and use the
already trained CRF to do prediction.

Transform on AST node. Our approach first needs to define a set of targeted transforms and
specify how they are attached to AST nodes. We achieve this based on the concept of basic tree
edit operations [Valiente 2002].

• Update–UPD(x, val)–Update the value of a node x with the value val.
• Add–ADD(x, y, i)–Add a new node x. If the parent y is specified, x is inserted as the ith child

of y, otherwise x is added as the new root node.

• Delete–DEL(x)–Remove the leaf node x from the tree.
• Move–MOV(x, y, i)–Move the subtree having node x as root to make it the ith child of a

parent node y.

7

To attach a certain transform t on a certain node n, we requite that there exists one of the 4
basic tree edit operations on n and (or) other basic tree edit operations on other nodes related with
n. Using repair transform as the example, the next section will give detailed examples on how
to define transforms on AST nodes. Note due to the existence of effective tree differencing tools
[Falleri et al. 2014; Fluri et al. 2007], this manner of using basic tree edit operations as the basis for
defining transforms on AST nodes also facilitates the preparation of training data.

Feature functions for CRF. Feature functions are key to controlling the likelihood predictions
in CRF. Similar to the feature functions that have been proven useful on other application areas
of CRF [He et al. 2004; Pinto et al. 2003], we can consider two types of feature functions for our
transform prediction problem.

The first type of feature function is called observation-based feature function. For a certain clique

c, observation-based feature functions typically have the form:

fk (tc , nc ) = 1tc =t,

The notation 1tc =t,

c qk (nc )
c is an indicator function of tc which takes the value 1 when tc = t,

c and 0
otherwise, q(nc ) is a function on the input nc which we call observation function. In other words, the
feature function is nonzero only for a single output configuration t,
c . But as long as the constraint
is met, then the feature value depends only on the input observation nc . Put it in another way,
we can think of observation-based features as depending only on the input nc , but we have a
separate set of weights (after learning) for each output configuration. In this case, for a certain
clique c, we can establish the relation between tc and nc by using program analysis to analyze the
characteristics of input nodes nc . For example, for a node clique c, ∈ CN which involves a node
n, whose label is invocation (i.e., method call), we can establish a function q(n, ) which analyzes
whether the called method has overloaded method(s) (return function value 1 if yes, otherwise
return function value 0) and associate the function with different possible transforms that can be
applied on this node. Suppose we are focusing transforms to repair bugs, hopefully the feature
function 1tc, =Meth−RW−Methq(n, ) will have a relatively large weight after learning from large data.
Same as mentioned in Section 2, here “Meth-RW-Meth” denotes a repair transform that replaces a
method invocation by another one, including overloaded methods.

The other type of feature function is called indicator-based feature function, which can be viewed
as a pre-processing step before the launch of the learning phase and typically have the following
form for a certain clique c:

fj (tc , nc ) = 1tc =tE

Here tE

c, and nE

c, ∧nc =nE
c,
c, denote the input and output for a clique c, observed in any of the training
data D = {⟨ti , ni ⟩}m
i=1. The basic idea behind indicator-based feature functions is for each kind
of clique cT (either node clique, edge clique or triangle clique), we collect from training data all
possible input-output tuples ⟨ncT , tcT ⟩ for it and transform each input-output tuple into a feature
function. By learning from a large representative data set, a weight then can be associated with
each input-output tuple which in turn can be used to do output predictions for new unseen inputs.
Using repair transform as the example, we will show particular instantiations of observation-

based and indicator-based feature functions in next section.
i=1 of m samples, we assume the
Learning and Prediction. Given the training data D = {⟨ti , ni ⟩}m
samples are drawn independently from the underlying joint distribution p(t, n) and are identically
distributed, i.e., the training data are IID. The training goal is to automatically compute the optimal
weights λ = {λk }K
k=1 for feature functions in a way that achieves generalization. In other words,
for the set of AST nodes n for a new code snippet drawn from the same distribution P (but not

8

contained in the training data set D), its needed transforms t are predicted correctly by the learned
model. The typical way to train CRF model is using classical penalized maximum (log)-likelihood:

λml = argmax

λλλ

m
(cid:213)

i=1

logp(t = ti |n = ni )

That is, the weights for feature functions are chosen such that the training data has highest
probability under the model.

Based on the above defined feature functions and the learned weights for each feature function,
for the AST nodes n of a new code snippet, the conditional probability of each possible transform t
can be calculated by substituting the defined feature functions and the learned weight for each
feature function into the following formula:

p(t|n) = 1
Z (n)

exp

(cid:26) (cid:213)

K
(cid:213)

c ∈C

k =1

(cid:27)

λk fk (tc , nc )

CRFs typically output the single most likely prediction by using the following query (also known

as MAP or Maximum a Posteriori query [Koller and Friedman 2009]):

t = argmax

t, ∈Ωn

p(t, |n)

For code transform prediction problems, there are some specific issues that need to be taken
into account during the learning and prediction process. First, for a code snippet, there in general
just need a few actual transforms applied to certain AST nodes. As a result, the training data
D = {⟨ti , ni ⟩}m
i=1 is skewed in turns of the number of AST nodes that are associated with actual
transforms. If the skew is too large, the learned weight λ will be biased. Second, typically there exist
constraints on the admissible transforms assigned to a certain node n. In particular, the admissible
transforms assigned to a certain node n is highly dependent of the label of itself and its neighbour
nodes.

We will show how to get the maximum likelihood estimate and how to do prediction in detail
in next section. In particular, we will present how to deal with training data imbalance issue and
constraints on admissible transforms.

5 REPAIR TRANSFORM PREDICTION

In this section, using repair transforms as example, we give a full realization of our approach for
structured prediction of source code transforms. We first give the definitions of repair transforms
on AST nodes, then describe how feature functions are constructed for the specific transform
prediction problem, and finally give the full CRF learning and prediction algorithms which take the
specific issues associated with transform prediction into consideration.

5.1 Repair Transforms on AST Nodes

Repair transforms are transforms used to change the buggy code into correct code, and are at the
heart of many repair techniques [Kim et al. 2013; Long et al. 2017; Saha et al. 2017]. The repair
transforms used by these repair techniques are not at the level of AST node and are tried in a fixed
order during repair. Our approach instead is able to predict the needed repair transforms at the
granularity of AST node. To achieve this, we first give the definitions of repair transforms on AST
nodes. Our definitions are on top of the 4 basic tree edit operations: ADD, DEL, MOV, and UPD.

We now give detailed definitions for 16 repair transforms, which cover some of the most widely
used transforms for repairing bugs. Our definition targets Java language and uses Eclipse JDT style

9

AST, but can be extended to other languages and other AST implementations. We first give some
notations and basic definitions for facilitating the definition of repair transforms.

Figure 3 presents the notations we use to define the repair transforms on AST nodes. Here the
logical expression denotes an expression made up by a set of atomic boolean expressions (through
the use of logical operator), and the logical expression cannot be extended with other atomic
boolean expressions. The function P maps a node n to its parent node, root maps a code snippet C
to the root node of its ast, d maps the value of a node v(n) to the root node of definition code for
v(n) when l(n) is Variable Access or Method Call, rl maps a node n to the root node of the related
logical expression when l(n) is Conditional If, Logical Operator, or Ternary Operator, st maps a node
n to the subtree whose root node is n, µs maps a node n to the related code block CBs when l(n) is
Conditional If or Try-Catch, τ maps an ast to all of its nodes, TE(e,n) is an indicator function about
whether there exists a specific basic tree edit operation e on node n, o maps a set of statements in a
code block CB to its subset which contains only moved statements, and finally S1O maps a code
block CB to a single statement with the smallest code line index. In addition, when the tree edit
operation is UPD, we use v(n)new to denote the new value of the node n after the edit operation.

LogicalOperator = {||, &&}
BinaryOperator = {||, &&, |, ˆ, &, ==, !=, <, >, <=, >=, «, », +, -, *, /,% }
LogicalExpression = BoolExp ||(&&) ... ||(&&) BoolExp CB = {s1, ..., sn } AO = {ADD, DEL, MOV, UPD}
rl: n → root(n −→ LogicalExpression)
P: n → npar

d: v(n) → root (cid:0)v(n) −→ Def(cid:1)

root : C → ast(r )

st: n → ast(< _, _, n, _, _, _, _, _ >)
s ∈ CB|TE(cid:16)
(cid:110)

o : CB →

MOV, root(s)

µs : n → CBs

τ : ast → (N ∪ T)

TE(e, n) : ∃ e ∈ AO, e(n, _, _)

(cid:17)(cid:111)

S1O : CB → smin

Literal = {Number, String, null}

Fig. 3. Definitions and notations used for defining repair transforms on AST nodes

Each repair transform is specified as a tuple (n, name) which denotes that a certain repair
transform name is attached to an AST node n. Note that we do not claim that our definition
is complete and covers every case of each transform, but we believe the typical case of each
transform is included according to our definition. When defining the repair transforms, we focus
on the essential tree edit operations. To make the repair transforms valid with regard to language
grammar, some other tree edit operations following up these essential tree edit operations are
implicit. For instance, when there exists the ADD operation which inserts an AST node whose label
is Conditional If, some other ADD operations which insert nodes for the condition predicate must
be accompanied. To avoid clutter, we do not explicitly show these follow-up tree edit operations.
Figure 4 gives the definitions for some repair transforms that target the inner nodes of a statement.
We first have two repair transforms which move an expression into (Wrap-Meth) and out of
(Unwrap-Meth) a method call. To avoid the case that the move operations arise because of changes
in the signature of the involved method call, we add a constraint that there are no tree edit operations
on the root node of the method definition and the children nodes of the root node. Note when our
definition explicitly involves a certain variable access or method call, we in general have constraints
about the tree edit operations on the definitions of the accessed variable or method. We then have
two repair transforms about replacing a variable access by another variable access (Var-RW-Var) or
a method call (Var-RW-Meth). Similarly, we also have repair transforms that replace a method call
by a variable access (Meth-RW-Var) or another method call (Meth-RW-Meth). In particular, there
are two sub-cases for Meth-RW-Meth repair transform, corresponding to the case that the name of
the replaced method call is different from (Case1) or same with (Case2, i.e., use another overloaded
method) that of the original method call respectively. The BinOperator-Rep and Constant-Rep
repair transforms replace a binary operator with another binary operator and replace a constant

10

literal with another constant literal respectively. Finally, we have two repair transforms which
expand (LogExp-Exp) and reduce (LogExp-Red) an existing logical expression respectively. For
these two repair transforms, we add the constraint that for the node n1 corresponding to the
inserted (deleted) logical operator, one child node n, is not subject to any tree edit operations and
for the other child n,, , the nodes of all the sub-tree rooted at n,, are subject to ADD (DEL) tree
edit operation. Besides, as logical expression can typically be expanded in different ways when it
contains several atomic boolean expressions, we thus associate both these two repair transforms to
the root node of the logical expression.

MOV(n1, n2, i1)

P(n1) = P(n3)
(cid:16)
v(n3)

(cid:17)

nd e f = d

ADD(n3, n4, i2)

n2 = n3
l(n3) = MethodCall

∀ao ∈ AO : ¬T E(ao, nd e f )

MOV(n1, n2, i1)
l(n3) = MethodCall
(cid:16)
v(n3)

nd e f = d

(cid:17)

DEL(n3)

n2 = P(n3)
n3 = P(n1)

∀ao ∈ AO : ¬T E(ao, nd e f )

∀ao ∈ AO ∀n, ∈ δ (nd e f ) : ¬T E(ao, n, )
(n1, Wrap-Meth)

∀ao ∈ AO ∀n, ∈ δ (nd e f ) : ¬T E(ao, n, )
(n1, Unwrap-Meth)

(cid:17)

UPD(n, val )
(cid:16)
v(n)

l(n) = VariableAccess
v(n)new (cid:17)
(cid:16)
nd e f = d
nd e f n = d
∀ao ∈ AO : ¬T E(ao, nd e f ), ¬T E(ao, nd e f n )
∀ao ∈ AO ∀n, ∈ δ (nd e f ) : ¬T E(ao, n, )
∀ao ∈ AO ∀n,, ∈ δ (nd e f n ) : ¬T E(ao, n,, )
(n, Var-RW-Var)

DEL(n1)

l(n2) = MethodCall
n3 = P(n1)

ADD(n2, n3, i)
nd e f 1 = d

l(n1) = VariableAccess
(cid:16)
(cid:17)
nd e f 2 = d
v(n2)
∀ao ∈ AO : ¬T E(ao, nd e f 1), ¬T E(ao, nd e f 2)

(cid:16)
v(n1)

(cid:17)

∀ao ∈ AO ∀n, ∈ δ (nd e f 1) : ¬T E(ao, n, )
∀ao ∈ AO ∀n,, ∈ δ (nd e f 2) : ¬T E(ao, n,, )
(n1, Var-RW-Meth)

(cid:17)

nd e f n = d

UPD(n, val )
(cid:16)
v(n)

l(n) = MethodCall
v(n)new (cid:17)
(cid:16)
nd e f = d
∀ao ∈ AO : ¬T E(ao, nd e f ), ¬T E(ao, nd e f n )
∀ao ∈ AO ∀n, ∈ δ (nd e f ) : ¬T E(ao, n, )
∀ao ∈ AO ∀n,, ∈ δ (nd e f n ) : ¬T E(ao, n,, )
(n, Meth-RW-Meth) Case1

n2 = P(n1)

DEL(n1)
l(n2) = MethodCall
∀ao ∈ AO : ¬T E(ao, n2), ¬T E(ao, nd e f )
∀ao ∈ AO ∀n, ∈ δ (nd e f ) : ¬T E(ao, n, )
(n1, Meth-RW-Meth) Case2

(cid:16)
v(n2)

nd e f = d

(cid:17)

DEL(n1)

l(n2) = VariableAccess
n3 = P(n1)

ADD(n2, n3, i)
nd e f 1 = d

(cid:16)
v(n2)
∀ao ∈ AO : ¬T E(ao, nd e f 1), ¬T E(ao, nd e f 2)

(cid:16)
v(n1)

l(n1) = MethodCall
nd e f 2 = d

(cid:17)

∀ao ∈ AO ∀n, ∈ δ (nd e f 1) : ¬T E(ao, n, )
∀ao ∈ AO ∀n,, ∈ δ (nd e f 2) : ¬T E(ao, n,, )
(n1, Meth-RW-Var)

(cid:17)

UPD(n, val )
l(n) = BinaryOperator
(n, BinOperator-Rep)

UPD(n, val )
l(n) = Literal
(n, Constant-Rep)

ADD(n1, n2, i1)

l(n1) = LogicalOperator

∃n, ∈ δ (n1)∀ao ∈ AO : ¬T E(ao, n, )
(cid:16)

(cid:17)

∃n,, ∈ δ (n1)∀n,,, ∈ τ

st (n,, )

: T E(ADD, n,,, )

DEL(n)

l(n) = LogicalOperator

∃n, ∈ δ (n)∀ao ∈ AO : ¬T E(ao, n, )
(cid:16)

(cid:17)

∃n,, ∈ δ (n)∀n,,, ∈ τ

st (n,, )

: T E(DEL, n,,, )

(cid:16)

r l (n1), LogExp-Exp(cid:17)

(cid:16)

r l (n), LogExp-Red(cid:17)

Fig. 4. Definitions of the repair transforms targeting inner AST nodes of a statement

Figure 5 gives the definitions for some other repair transforms that majorly target the “virtual
root” node of a statement. The “virtual root” node nvr is introduced to separate the actual root node
nr , and is inserted between nr and its parent node. For nvr , we view the label and value of it as
‘virtualroot’ and ‘null’ respectively, and we use the notation s(cid:55)→vr to denote the nvr for a statement
s. We introduce nvr as it is more reasonable to view some repair transforms are attached to it rather
than the actual root node nr . For instance, for an assignment statement whose root node is the
binary operator ‘=’, it is preferable to view the repair transform which wraps the statement with an

11

‘If’ condition check is attached to nvr . Note this also facilities the construction of the CRF model as
it typically has the single-label per time step assumption [Sutton and McCallum 2012] (i.e., single
repair transform per AST node for our problem).

We first have the repair transform which adds an ‘If’ conditional check for an existing statement,
and the added ‘If’ check does not have the ‘Else’ block. Note some other tree edit operations
on the ‘Then’ block can be accompanied by the add of the ‘If’ conditional check, and we view
the first statement in the ‘Then’ block whose actual root node is subject to ‘MOV’ operation as
the ‘old’ statement and the target of the conditional check. Depending on whether the ast of
the added logical expression has the node n(Literal, NU LL), we further split the transform into
Wrap-IF-N (as shown in Figure 5) and Wrap-IF-O (other not null related check, not shown in
Figure 5 for space reason). When the added ‘If’ check has the ‘Else’ block, we have “Wrap-IfElse”
related repair transform which has three cases: case1 for which the ‘old’ statement is in the ‘Then’
block, case2 for which the ‘old’ statement is in the ‘Else’ block, and case3 for the case of add of
a ternary expression. Similarly, we also split it into Wrap-IFELSE-N (as shown in Figure 5) and
Wrap-IFELSE-O based on the check of the existence of node n(Literal, NU LL). We then have the
Unwrap-IF repair transform which removes the conditional check, and the conditional check can
be in the form of ‘If’ expression (case1) and ternary expression (case2). Finally, the Wrap-TRY
repair transform warps an existing statement with “Try-Catch” exception handle.

(cid:16)

r l (n1)

(cid:17)

ADD(n1, n2, i1)

ast = st
l(n1) = If
n(Lit er al, N U LL) ∈ τ (ast )

µ E LS E (n1) = ∅
o
o (cid:0)µT H E N (n1)(cid:1) (cid:17)
(cid:16)

s = S1O

(cid:16)

µT H E N (n1)

(cid:17) (cid:44) ∅

(s (cid:55)→v r , Wrap-IF-N)

l(n1) = If

ADD(n1, n2, i1)
n(Lit er al, N U LL) ∈ τ (ast )
µT H E N (n1)

µ E LS E (n1) (cid:44) ∅
o
o (cid:0)µT H E N (n1)(cid:1)(cid:17)
(cid:16)

s = S1O

(cid:16)

(cid:17) (cid:44) ∅

(cid:16)

(cid:17)

ast = st
r l (n1)
µT H E N (n1) (cid:44) ∅
µ E LS E (n1)
o

(cid:16)

(cid:17) = ∅

(s (cid:55)→v r , Wrap-IFELSE-N) Case1

ADD(n1, n2, i1)
n(Lit er al, N U LL) ∈ τ (ast )
µT H E N (n1)

µ E LS E (n1) (cid:44) ∅

l(n1) = If

(cid:16)

(cid:17) = ∅

o
o (cid:0)µ E LS E (n1)(cid:1)(cid:17)
(cid:16)

s = S1O

(cid:16)

(cid:17)

ast = st
r l (n1)
µT H E N (n1) (cid:44) ∅
µ E LS E (n1)
o

(cid:16)

(cid:17) (cid:44) ∅

(s (cid:55)→v r , Wrap-IFELSE-N) Case2

MOV(n1, n2, i1)

(cid:16)

ADD(n3, n4, i2)

n2 = n3
l(n3) = TernaryOperator
n(Lit er al, N U LL) ∈ τ (ast )

P(n1) = P(n3)
r l (n3)
(n1, Wrap-IFELSE-N) Case3

(cid:17)

ast = st

DEL(n)

l(n) = If

o (cid:0)µT H E N (n)(cid:1) ∪ o (cid:0)µ E LS E (n)(cid:1)(cid:17) (cid:44) ∅
(cid:16)
(n, Unwrap-IF) Case1

MOV(n1, n2, i1)
DEL(n3)
n3 = P(n1)
n2 = P(n3)
l(n3) = TernaryOperator
(n3, Unwrap-IF) Case2

ADD(n1, n2, i1)
(cid:16)
o

µT RY (n1)

l(n1) = Try
(cid:17) (cid:44) ∅

s = S1O

o (cid:0)µT RY (n1)(cid:1)(cid:17)
(cid:16)

(s (cid:55)→v r , Wrap-TRY)

Fig. 5. Definitions of the repair transforms targeting the virtual root node of a statement

5.2 Feature Functions for CRF

We now describe how feature functions are constructed for the repair transform prediction prob-
lem. The design and choice of the feature functions is important, because it highly impacts the
performance of the CRF model [Sutton and McCallum 2012]. We consider two types of feature
functions for our established CRF model: observation-based feature functions and indicator-based
feature functions.

5.2.1 Observation-based Feature Functions. Observation-based feature functions analyze the char-
acteristics of input nodes and establish the correlation between those characteristics and the repair
transforms applied to nodes. We design observation-based feature functions related with different

12

kinds of program elements reflected in AST nodes, including variable access, method call, logical
expression, binary operator, and the whole statement. We first present the characteristics we
analyze and then present the observation-based feature functions on top of them.

Node Characteristics. Depending on the label of the AST node, we accordingly analyze different
kinds of characteristics associated with it and the characteristics can generally be classified into 3
kinds based on their nature.

Type Related. For a node n whose label is variable access, we have six characteristics related
with the type of the accessed variable var. (V1) The type of var is primitive; (V2) The type of var is
objective; (V3) var is an instance of the class it resides; (V4) There exist variables in scope (i.e., is
accessible) that are type compatible with var; (V5) There exist method definitions or method calls
in the class for which at least one of their parameters is type compatible with var; (V6) There exist
method definitions or method calls in the class whose return type is type compatible with var.

For a node n whose label is method call, we have five characteristics concerning type related
with the accessed method m. (M1) The return type of m is primitive; (M2) The return type of m is
objective; (M3) The types of some of parameters of m are compatible with the return type of m;
(M4) There exist variables in scope that are type compatible with the return type of m; (M5) There
exist method definitions or method calls in the class whose return type is type compatible with the
return type of m.

Usage Related. For the accessed variable var by an AST node n, we have eight characteristics
related with the usage of var in other statements. (V7) When var is a locale variable, it has not been
referenced in other statements before the statement that var resides; (V8) When var is a locale
variable, it has not been assigned before the statement that var resides and it does not have default
not-null expression when declaration; (V9) When var is a field, it has not been referenced in other
statements of the class besides the statement that var resides; (V10) When var is a field, it has not
been assigned in other statements of the class besides the statement that var resides and it does
not have default not-null expression when declaration; (V11) There exist other statements (besides
the statement that var resides) in the class that use some same type variables with var, but have
null check guard; (V12) There exist other statements (besides the statement that var resides) in the
class that use some same type variables with var, but have normal check guard; (V13) When var is
a parameter of a method call m1, replace var with another variable var, can get another method
call m2 used in the class; (V14) When var is a parameter of a method call m1, replace var with a
method call m, can get another method call m2 used in the class.

For the accessed method m by an AST node, we have five characteristics concerning the usage
of m in other statements. (M6) There exist other statements in the class that use a method call m,
whose signature is same with m, but have null check guard; (M7) There exist other statements
in the class that use a method call m, whose signature is same with m, but have normal check
guard; (M8) There exist other statements in the class that use a method call m, whose signature is
same with m, but are wrapped with try catch exception handle; (M9) When m is a parameter of a
method call m1, replace m with a variable var can get another method call m2 used in the class;
(M10) When m is a parameter of a method call m1, replace m with another method call m, can get
another method call m2 used in the class.

For a node n that is the root of a logical expression e, we have three characteristics concerning
usage of constituent elements of e in other statements. (LE1) There exists a variable var referenced
by an atomic boolean expression ae1 of e, and there exists another atomic boolean expression ae2
in other statements that reference variables of same type with var but ae1 and ae2 are different
after identifier substitution; (LE2) There exists a variable var in scope which is not referenced by e,

13

but there exist atomic boolean expressions in other statements which reference variables of same
type with var; (LE3) There exists a boolean variable in scope that is not referenced by e.

Syntax Related. We have two characteristics related with the syntax of the accessed variable
var by a node. (V15) There exist other variables in scope that are similar in identifier name with
var; (V16) There exist method definitions or method calls in the class that are similar in identifier
name with var. We use Levenshtein distance metric to measure the difference between two string
sequences.

For the accessed method m by a node, we have four syntax related characteristics. (M11) The
identifier name of method m starts with ‘get’; (M12) The method m has overloaded method; (M13)
There exist variables in scope that are similar in identifier name with m; (M14) There exist method
definitions or method calls in the class that are similar in identifier name with m. The similarity is
also established using Levenshtein distance.

For a statement s, we have five syntax related characteristics, and these characteristics are
attached to the virtual root node of the statement. (S1) The statement kind of s; (S2) The statement
kind of the previous statement in the same block with s; (S3) The statement kind of the next
statement in the same block with s; (S4) The statement kind of the parent statement of s; (S5) The
associated method of s throws exception or the associated class of s extends an exception type.

For a node n whose label is binary operator, we have four characteristics related with the accessed
binary operator bo. (BO1) The operator kind of bo; (BO2) When bo is a logical operator, its operands
contain the exclamation mark ! (i.e., not operator); (BO3) When bo is a logical operator, its operands
contain the literal ‘null’; (BO4) When bo is a mathematical operator, its operands contain number
‘0’ or ‘1’.

We also have three syntax related characteristics for the root node n of a logical expression e.
(LE4) There exists an atomic boolean expression that contains the exclamation mark !; (LE5) There
exists an atomic expression which is simply a boolean variable; (LE6) There exists an atomic boolean
expression ae1 of e that is null check and there also exists another atomic boolean expression ae2
of e that is not null check (i.e., there exists mix check).

For the characteristics related with statement kind (S1-S4) or binary operator kind BO1, we
enumerate all possible kinds and establish a sub-characteristic of the form “The kind is X” for each
possible kind X. Typical binary operator kinds in mainstream language include logical relation, bit
operation, equality comparison, shift operation, and mathematical operation as shown in Figure 3.
After doing this, each of the characteristics can be viewed as a boolean valued function, i.e, a
predicate on the characteristics. We use n.c to denote the boolean evaluation result of a certain
characteristic c on n.

As there exist structural dependencies between different AST nodes, the characteristic of a
certain node can possibly imply repair transforms on other nodes. For instance, the characteristic
V7 for a variable access node can be related with “Wrap-If” related transform on the virtual root
node of the statement. To take this into account, we propagate some characteristics of certain child
nodes to their parent nodes. First, we propagate the characteristics V7, V8, V9, V10, V11, and V12
for a variable access node n1 to the virtual root node of the statement and the method access node
n2 that is an ancestor of n1. Second, we propagate the characteristics M6, M7, and M8 for a method
access node to the virtual root node of the statement. When propagating a certain characteristics c
from a node n1 to another node n2, the corresponding c, for n2 is “There exists at least one child
node that has characteristic c”, and the predicate value is calculated as follows:

n2.c, = nc1.c ∨ nc2.c ∨ ... ∨ nck .c

where nc1 to nck represent k children of n2 that have characteristic c, including n1.

14

Observation-based Feature Functions. After analyzing the characteristics, the observation func-
tion q(nc ) can then be designed as indicator function of the characteristics. Let CL = {ci }n
i=1 denote
the set of characteristics we have established for node whose label is L. For an AST node n and a
certain characteristic c ∈ CL, we define the direct observation function and inverse observation
function as follows:

qdir ect (n, c) = 1l (n)=L∧n.c=t r ue

qinver se (n, c) = 1l (n)=L∧n.c=f al se
Then, we need to correlate the observation function with repair transforms. For different types
of nodes, the set of possible repair transforms on them are different. To see what transforms are
possible for a certain node, we make use of information from the training data. For repair transform
prediction problem, the training data D = {⟨ti , ni ⟩}m
i=1 consists of m buggy programs where for
each program, the repair transforms required to fix the bug are associated to appropriate AST
nodes. For an AST node n ∈ n, we use t(n) to denote the associated repair transform on it, and we
define the viable transform set for a node whose label is L as follows:

T(L) = {t |∃⟨t, n⟩ ∈ D ∃n ∈ n : l(n) = L, t(n) = t }
That is, we deem a repair transform possible for a certain type of node when we have observed

the occurrence of this at least once in the training data.

We finally define observation-based feature functions on top of the observation functions and the
viable transform set. Let γ represent the set of possible labels for a node, we define observation-based
direct feature function and observation-based inverse feature function as follows:

f dir ect (t, n) = 1L ∈γ 1l (n)=L1t ∈T(L)1c ∈CL qdir ect (n, c)
f inver se (t, n) = 1L ∈γ 1l (n)=L1t ∈T(L)1c ∈CL qinver se (n, c)
Intuitively speaking, for each possible repair transform t on a node whose label is L, we associate
it with each of the characteristics we have designed for nodes with label L to form a feature function.
Through training on big data, we can get weights for different transform-characteristic pairs. The
direct and inverse feature functions explore the correlation in different direction.

Note we in this paper define observation-based feature functions on node cliques, and it is
possible to define more complex observation-based feature functions on edge cliques and triangle
cliques by analyzing the characteristics involving all nodes in edge and triangle cliques. In addition,
as we do not analyze the characteristics of all node labels, the observation-based feature functions
accordingly do not target all types of nodes.

Indicator-based Feature Functions. The other type of feature function is called indicator-based
5.2.2
feature function, which can be viewed as a pre-processing step for the training data. Recall that l(n)
and t(n) are used to denote the label and repair transform on an AST node n respectively. Here, we
also use ni to represent the ith child node of a node n. Given the training datset D = {⟨ti , ni ⟩}m
i=1,
for each tuple ⟨t, n⟩ ∈ D, we define the observed set of transforms on nodes for different kinds of
cliques as follows:

nodetran(t, n) = {(T , L)|∃n ∈ n : l(n) = L, t(n) = T }

edдetran(t, n) = {(T1, L1,T2, L2)|∃n ∈ n, i ∈ N : l(n) = L1, t(n) = T1, l(ni ) = L2, t(ni ) = T2}

trianдletran(t, n) = {(T1, L1,T2, L2,T3, L3)|∃n ∈ n, i ∈ N : l(n) = L1, t(n) = T1, l(ni ) = L2,
t(ni ) = T2, l(ni+1) = L3, t(ni+1) = T3}

15

In other words, we enumerate possible repair transforms on nodes for different cliques observed
in the specific training example ⟨t, n⟩. For the entire training data set D, we can then obtain all
observed set of transforms on nodes as follows:

all_nodetran(D) = ∪m

i=1nodetran(ti , ni )

all_edдetran(D) = ∪m

i=1edдetran(ti , ni )

all_trianдletran(D) = ∪m

i=1trianдletran(ti , ni )

Based on the observed set of transforms on nodes, we finally define indicator-based feature

functions for different kinds of cliques as follows:

f (t1, n1) = 1(t1,l (n1))∈all _nodet r an(D)
f (t1, n1, t2, n2) = 1(t1,l (n1),t2,l (n2))∈all _edдet r an(D)
f (t1, n1, t2, n2, t3, n3) = 1(t1,l (n1),t2,l (n2),t3,l (n3))∈all _t r ianдl et r an(D)

where ti corresponds to the repair transform associated with node ni . For edge clique, n1 and n2
are parent node and child node respectively. For triangle clique, n1, n2 and n3 are parent node, left
child node and right child node respectively. Note in the remaining of this paper, we use the same
notation as here.

The above defined indicator-based feature functions do not take the value of the nodes into
account. To study the repair transforms associated with triangle cliques when the value of the
left child is the same with that of the right child (e.g., same variable access), we define another
indicator-based feature function for triangle as follows:

trianдletranspe (t, n) = {(T1, L1,T2, L2,T3, L3)|∃n ∈ n, i ∈ N : l(n) = L1, t(n) = T1, l(ni ) = L2,
t(ni ) = T2, l(ni+1) = L3, t(ni+1) = T3, L2 = L3, v(ni ) = v(ni+1)}

all_trianдletranspe (D) = ∪m

i=1trianдletranspe (ti , ni )

f , (t1, n1, t2, n2, t3, n3) = 1(t1,l (n1),t2,l (n2),t3,l (n3))∈all _t r ianдl et r anspe (D)∧v(n2)=v(n3)

In the learning phase, we can learn the corresponding weight for each of the indicator-based
feature functions defined above. Note that the weights and the indicator-based feature functions
can vary depending on the training data D, but they are independ of the buggy code snippet for
which we are trying to predict repair transforms.

5.3 Learning and Prediction
5.3.1 Learning. Recall that the learning problem is to determine the optimal weights λ = {λk }K
k=1
for feature functions from the training data D = {⟨ti , ni ⟩}m
i=1. The typical way to train CRFs is
by penalized maximum (log)-likelihood, which optimizes the following log-likelihood objective
function with respect to the model p(t | n, λ):

l(λ) =

m
(cid:213)

i=1

logp(t = ti |n = ni ; λ)

16

The above objective function treats each p(t = ti |n = ni ; λ) as equally important. However, one
significant characteristic in repair transform prediction problem (in general transform prediction
problem) is that the buggy code snippet is nearly correct, and there in general just needs a few
actual repair transforms applied to certain AST nodes. Note we attach a virtual ‘EMPTY’ repair
transform to those nodes which are not associated with any repair transforms. Consequently, the
training data D = {⟨ti , ni ⟩}m
i=1 is skewed in turns of the number of AST nodes that are associated
with actual repair transforms. If the skew is too large, the learned weight λ will be dominated by
those training examples with few repair transforms on few nodes, which in turn will predict that
few nodes need to subject certain repair transforms for most new unknown buggy code snippets.
However, correctly predicting those instances which need relatively more repair transforms on
nodes is extremely important as those bugs are much more hard to deal with. We call this issue
“transform number imbalance issue”.

Addressing the Imbalance Issue. To deal with the training data imbalance problem, there are
typically three major groups of solutions: sampling methods [Chawla et al. 2002], cost-sensitive
learning [Elkan 2001], and one-class learning [Tax and Duin 2004]. For our (repair) transform
prediction problem, inspired by the approach for dealing with training data imbalance issue in
[Song et al. 2013], we propose a method called “transform distribution aware learning”. The method
is similar to sampling methods, but does not have the disadvantage of removing important examples
in under-sampling and adding redundant examples in over-sampling (can cause overfitting). The
method analyzes the training data before the launch of training and gives more weight on those
training examples which have relatively more nodes associated with actual repair transforms.
Formally, for the training data set D = {⟨ti , ni ⟩}m
i=1, we define the set U which contains all the
observed numbers of actual repair transforms used for repairing a bug as follows:

U = {n : n ∈ Z+, ∃⟨ti , ni ⟩ ∈ D : S(t) = n}

where S(t) denotes the number of actual repair transforms in t.

We then define a distribution-aware prior χi as:

N = 1
|U |

(cid:213)

u ∈U

Nu

,

χi = (cid:16) N
NS (ti )

(cid:17)q

where Nu is the number of training examples in D that have u actual repair transforms, N is the
average number of training examples per each number in U, and q is a coefficient that controls the
magnitude of the distribution-aware prior.

We then multiply the distribution-aware prior with the log probability for each training example

⟨ti , ni ⟩ in the objective function and get a new objective function:

l(λ) =

m
(cid:213)

i=1

χi logPr (t = ti |n = ni ; λ)

Note when the training data set D has a uniform distribution (i.e., for each u ∈ U , Nu is equal) or
when the coefficient q equates to 0, the new objective function is reduced to the typical objective
function. Through the use of distribution-aware prior, more weight can be put on those training
examples which have relatively more actual repair transforms attached to nodes and are scarce in
D. The larger the coefficient q, the more weight we put on those types of training examples which
are scarce in D. Overall, by using the distribution-aware prior, all the training examples in D can be
adjusted to have a balanced impact in the learning process.

17

After substituting the CRF model into the new objective function, l(λ) becomes:

l(λ) =

m
(cid:213)

(cid:213)

K
(cid:213)

i=1

c ∈C

k =1

χi λk fk (ti

c , ni

c ) −

m
(cid:213)

i=1

χi logZ (ti )

Regularization. As our CRF contains a large number of feature functions, we use regularization
to avoid that the learned weights for feature functions are over-fitting. The regularization can be
viewed as a penalty on weight vectors whose norm is too large. We use the typical penalty based
on the Euclidean norm of λ and the strength of the penalty is determined by the regularization
parameter 1/2δ 2. The regularized objective function is then:

l(λ) =

m
(cid:213)

(cid:213)

K
(cid:213)

i=1

c ∈C

k =1

χi λk fk (ti

c , ni

c ) −

m
(cid:213)

i=1

χi logZ (ni ) −

K
(cid:213)

k =1

λ2
k
2δ 2

The regularized objective function is concave and thus every local optimum is also a global
optimum. However, l(λ) in general cannot be maximized in closed form, so numerical optimization
is used and a particularly successful method is L-BFGS [Liu and Nocedal 1989]. L-BFGS belongs
to the family of quasi-Newton methods and can be used as a black-box optimization routine by
feeding the value and first derivative of the objective function. The first derivative of the objective
function l(λ) for each parameter λk is:

m
(cid:213)

(cid:213)

=

∂l(λ)
∂λk

χi fk (ti

c , ni

c ) −

m
(cid:213)

(cid:213)

(cid:213)

+
c ∈C
ti
c ∈Φc
where Φc ranges over all assignments to t in the clique c.

c ∈C

i=1

i=1

χip(ti +

c |ni

c )fk (ti +

c , ni

c ) −

λk
δ 2

The computation of the first term is straightforward (i.e., sum the feature function values over
the training data set), but calculating the second term requires to calculate the marginal probability
p(tc |nc ), which is an inference task and we will discuss it below.

Inference. CRF typically outputs the most likely prediction using the MAP query t∗ =
5.3.2
p(t|n). As said in the section about learning, the learning process needs to calculate the
argmaxt
marginal probability p(tc |nc ) for a certain clique c. These are the two inference problems arise in
CRF, and can be seen as fundamentally the same operation on two different semirings [Sutton and
McCallum 2012]. To change the marginalization problem to the maximization problem, we just
need to substitute maximization calculation for addition calculation.

When the associated undirected graphs with CRF have cycles, typically approximate inference
algorithms have to be used. However, one advantage of our CRF model is that the maximal clique
in the undirected graph is triangle, for which efficient exact inference algorithms are available.
The process is first using junction tree algorithm to change the graph into a tree, and then belief
propagation can be used to do the inference [Sutton and McCallum 2012]. We refer readers to
[Jensen and Jensen 1994] for details about junction tree algorithm and [Pearl 1982] for details about
belief propagation algorithm.

The belief propagation algorithm is also called message-passing algorithm, and the marginal
distributions are recursively computed using messages exchanged between all the nodes in the
junction tree. When it comes to the original undirected graph for CRF, let c be a maximal clique and
the set N c be its neighbour maximal clique set, i.e., the set of maximal cliques that have common
nodes with c. One can informally interpret message passing as that the marginal distribution of c is
determined by summing over all the admissible label assignments for the nodes of each c, ∈ N c ,
and for each c, , its marginal distribution in turn relies on all the admissible label assignments for
all of its neighbour maximal cliques.

18

Constraint on Valid Repair Transforms. One important characteristic of the (repair) trans-
form prediction problem is the admissible repair transforms assigned to a certain node n are
highly dependent of the label of itself and its neighbour nodes. For instance, for our defined 16
repair transforms, a repair transform t on the virtual root node of a statement is valid only when
t ∈ {Wrap-IF-N, Wrap-IF-O, Wrap-IFELSE-N, Wrap-IFELSE-O, Wrap-TRY} . To accommodate
this, we define a set of constraints as follows:

C = {Ci (tN , n) : i ∈ Z+} ∪ {Ci (tE, n) : i ∈ Z+} ∪ {Ci (tT , n) : i ∈ Z+}

where N, E, T refers to the node clique, edge clique, and triangle clique respectively, and Ci (tc , n) :
Tk × N → {0, 1} is a boolean function indicating whether the joint assignment (tk , n) for a clique
of kind k violates i-th constraint established for this clique kind.

Constraints restrict the sets of admissible label assignments for cliques and the message-passing
algorithm can be easily modified to take the constraint into account: only admissible labeling
assignments are considered in the messages. Constraints can be established according to the
domain knowledge about what kinds of repair transforms are possible for certain nodes in a clique.
We in this paper define constraints based on the training data set D = {⟨ti , ni ⟩}m
i=1. The idea behind
is that the training data set D is large and representative enough, and for an assignment tk to be
admissible, we should have observed the occurrence of this in the training data. Using the notations
in section 5.2.2, for different types of cliques, we define the set of admissible repair transforms for
different node labels as follows:

admissible_nodetran(D) = {(L,T S )|∃L ∈ γ , ∀T ∈ T S , 1(T , L)∈all _nodet r an(D)}

admissible_edдetran(D) = {(L1, L2,T P )|∃L1, L2 ∈ γ , ∀⟨T1,T2⟩ ∈ T P , 1(T1, L1,T2, L2)∈all _edдet r an(D)}

admissible_trianдletran(D) = {(L1, L2, L3,T T )|∃L1, L2, L3 ∈ γ , ∀⟨T1,T2,T3⟩ ∈ T T ,
1(T1, L1,T2, L2,T3, L3)∈all _t r ianдl et r an(D)}

where γ represents the set of possible labels for a node, T S , T P , and T T are the sets whose elements
are 1-tuple, 2-tuple, and 3-tuple respectively.

Based on the established admissible repair transforms for different node labels, we can determine

whether the assignments for different cliques violate the constraint as follows:

V (t1, n1) = 1∃(L,T S )∈admissibl e_nodet r an(D):l (n1)=L∧t1(cid:60)T S
V (t1, n1, t2, n2) = 1∃(L1, L2,T P )∈admissibl e_edдet r an(D):l (n1)=L1∧l (n2)=L2∧(t1,t2)(cid:60)T P
V (t1, n1, t2, n2, t3, n3) = 1∃(L1, L2, L3,T T )∈admissibl e_t r ianдl et r an(D):l (n1)=L1∧l (n2)=L2∧l (n3)=L3∧(t1,t2,t3)(cid:60)T T

where the value 1 means the constraint is violated and the assignment is not admissible.

The use of constraint in (repair) transform prediction arises for two reasons. First, it can sig-
nificantly reduce the time complexity in inference as only admissible transform assignments
are considered in the messages. Second, it allows to eliminate incorrect transform assignments
according to domain knowledge, resulting in accuracy improvement.

6 EXPERIMENTAL EVALUATION

We in this section present the implementation, the experimental methodology, and the empirical
results of predicting repair transforms on Java code.

19

6.1 Implementation.
We implemented our repair transform prediction approach in a tool called TEZEN. The tool is
written in Java and it learns and preditcs code transforms for Java source code. It consists of two
parts: repair transform extraction and CRF learning and prediction. For repair transform extraction,
we use Gumtree [Falleri et al. 2014] to extract the AST tree edit script. GumTree is an off-the-shelf,
state-of-the-art tree differencing tool that computes AST-level program modifications, and outputs
them as the 4 basic tree edit operations: UPD, ADD, DEL, and MOV. We also use Spoon [Pawlak et al.
2015] to analyze the code that surrounds the AST nodes affected by tree edit operations. Besides,
PPD [Madeiral et al. 2018] is employed to facilitate the detection of certain repair transforms. For
CRF, our CRF model is implemented on top of the XCRF library [Jousse et al. 2006], which is
a framework for building CRF to label XML data. We extend XCRF to incorporate our specific
feature functions, learning and prediction algorithms. In particular, a major modification both at
the conceptual and implementation level is the support for computing the top-k predictions, i.e.,
the predictions with the k highest conditional probabilities.

6.2 Experimental Setup.

Dataset. We use the data set of Soto et al. [Soto et al. 2016] as the source to get our needed data set
with repair transforms on AST node. This data set contains 4, 590, 679 bug fixing commits. When
we use Gumtree to analyze the diffs of those bug-fixing commits, we find that when the diff is
relatively large, the edit scripts are not accurate enough to reflect real code changes in a significant
number of cases. Thus, we limit our repair transform extraction to those bug-fixing commits with
relatively small diffs. To achieve this, we first give the definition of root tree edit operation.
Definition 6.1. (Root Tree Edit Operation). A basic tree edit operation O(x, _, _) ∈ {UPD, ADD, DEL,
MOV} is a root tree edit operation if the parent node of x already exists in the AST.

For example, for the code change that inserts a method call “call(a,b)”, there are three add
operations: ADD(ncall , y, i), ADD(na, ncall , 1), and ADD(nb , ncall , 2). For the three operations, only
ADD(ncall , y, i) is a root tree edit operation as the parent node ncall for the other two add operations
does not exist in the AST before the change.

We experimented with different thresholds for root tree edit operations and find when the
threshold is set to 10, the Gumtree outputs of the 4 basic tree edit operations are accurate enough
to reflect real code changes in most cases and we can correctly attach repair transforms to nodes.
After setting the threshold to be 10, we find that the tree edit operations in a file are typically
targeting a single statement. Consequently, we use the AST of the targeted statement as the AST
of the bug code. Finally, we have 261,263 pairs of ⟨t, n⟩ extracted from the original 4,590,679 bug
fixing commits, where n is the AST of a changed statement and t is the set of repair transforms
associated with the nodes of n.

Table 1 shows the number for different repair transforms we have in the data set. The “Single”
(resp. “Multiple”) refers to the case where the number of actual (i.e., non-empty) repair transforms
for a certain ⟨t, n⟩ is one (resp. larger than one). We can see from the table that a majority of the data
involves just one actual repair transform applied to a certain node. To check the quality of the data
set, we randomly sample 200 examples for each repair transform (in particular also 200 examples
for the case of multiple transforms) and manually check whether correct repair transforms are
attached to correct AST nodes. The result is shown in the column #Correct(S). Overall, we can see
the precision is satisfactory, achieving at least 88% correctness rate.
Cross-validation. We use cross-validation to select parameters of the CRF model and evaluate
the performance of the trained CRF model.

20

Table 1. Descriptive statistics of the data set. The manual check of the extracted repair transforms verifies
the quality of the data.

Repair Transform
Wrap-Meth
Unwrap-Meth
Var-RW-Var
Var-RW-Meth
Meth-RW-Var
Meth-RW-Meth
BinOperator-Rep
Unwrap-IF
Single

#Number
6,044
2,613
28,861
3,638
3,801
43,539
5,972
6,478
250,702

#Correct (S)
95%
92%
96%
93%
91%
92%
97%
96%
—

Repair Transform #Number
LogExp-Exp
LogExp-Red
Wrap-IF-N
Wrap-IF-O
Wrap-IFELSE-N
Wrap-IFELSE-O
Constant-Rep
Wrap-TRY
Multiple

21,045
2,987
9,290
10,611
4,178
10,742
82,295
6,016
10,561

#Correct (S)
98%
97%
93%
95%
90%
92%
96%
96%
88%

First, we randomly select 300 instances for each of the 16 studied repair transforms from the
“single-transform” category and 1000 instances from the “multiple-transform” category as the
test set. For “single-transform” category, the number 300 is chosen as it is around 10 percent
of the whole instances for the studied repair transforms Unwrap-Meth, LogExp-Red, Var-RW-
Meth, and Meth-RW-Var (2,613, 2,987, 3,638, and 3,801 respectively), which have fewer number
of instances than the other 12 studied repair transforms for our data set. To enable direct and
fair comparison of the model performance for different repair transforms, we make the number
of test instances for each repair transform from the “Single-transform” category the same. For
“multiple-transform” category, the number 1000 is chosen as it is around 10 percent of the whole
instances for the “multiple-transform” category. To see the model performance, we compare the
predicted code transforms on the testing data set against the ground-truth ones that we extract
from the bug-fixing commits.

The remaining instances in the data set are used as the training data set and we use the 10-fold
cross-validation to investigate the impact of the parameters and select them accordingly. There are
three parameters involved in our CRF model: the regularization parameter δ 2, the parameter q that
indicates the magnitude of the distribution-aware prior, and finally the L-BFGS iteration parameter
G that specifies the number of gradient computations used by the optimization procedure. Higher
values of δ 2 mean larger penalty on large weights associated with feature functions and higher
values of G will result in higher execution cost for inference.

We split the training data set into 10 equal folds and evaluate the error rate on each fold by
training a model on the other 9 folds. The error rate is established using the top-3 evaluation metric
described in the next paragraph. We repeat this process by using a set of different parameters and
try to identify parameters with the lowest error rate. The procedure determines that 500 and 200
are good values for δ 2 and G respectively, and the following results are based on these two values.
The parameter q significantly impacts the prediction result for “single-transform” category and
“multiple-transform” category, and we will discuss its impact in detail in the result section.
Evaluation Metric. For each tuple ⟨t, n⟩ in the test data set, we view a prediction ⟨t, , n⟩ by our
trained CRF as correct if t = t, . That is, for each node n ∈ n, the predicted repair transform
associated to it is exactly the same as the repair transform from the ground truth data. We use
both top-1 and top-3 to evaluate the model performance. For top-3, we consider the prediction as a
success if at least one of the 3 predictions of sets of transforms exactly matches the expected ones.
Baseline. We compare the model performance with a baseline which assigns repair transforms to
nodes according to basic statistics on past commits. Using the same data set employed for training

21

the model, we establish a set of tuples <L, T, P> where L is a certain node label, T is a certain repair
transform, and P represents the probability of assigning T to nodes with label L. Basically, let N0
denote the number of nodes with label L in the training data set and N1 denote the number of nodes
with label L and are associated with repair transform T, then P = N1/N0. The baseline assumes there
needs only one actual repair transform and prioritizes the node n and repair transform T on it
according to the established probability for <l(n), T >. When there are several nodes of the same
label, the baseline breaks ties according to their orders during pre-order traversal. After all possible
single node transforms have been explored, based on the established set of tuples <L, T, P>, the
baseline gradually explores all possible two node transforms, three node transforms and so on.

In addition, to see the impact of observation-based and indicator-based feature functions, we also
compare our approach (“Full Model”) against a “Partial Model” that uses only observation-based
feature functions (denoted as ‘O’) or only uses indicator-based feature functions (denoted as ‘I’).
Model Statistics. Our full CRF model contains 12,977 feature functions, including 5,910 observation-
based feature functions and 7,067 indicator-based feature functions. We store them along with their
weights in a JSON file, which is 3.2MB.

6.3 Experimental Results

Our training is run on a cluster node with 24 cores and 512 GB RAM, and the system is Ubuntu
16.04. For the full model, the training takes 41 hours for each value of parameter q when 500 and
200 are set as the values for δ 2 and G.

Table 2 and Table 3 show the performance of the model on the test set when top-1 and top-3 are
used as the evaluation metric respectively. Recall that the test set contains 1,000 instances for the
“multiple-transform” category and 300 instances for each of the studied 16 repair transforms from
the “single-transform” category. The numbers in the cells of the tables represent the number of
instances in the test set that have been correctly predicted when a certain approach (either baseline
or different trained models) and a certain evaluation metric are used (either top-1 or top-3). The
percentages in the two rows “Overall Accuracy” and “Accuracy” summarize the performance for
“single-transform” and “multiple-transform” instances respectively. The “Full Model” refers to the
model that uses both observation-based and indicator-based feature functions, and the “Partial
Model” refers to the model that uses only observation-based feature functions (denoted as ‘O’) or
only uses indicator-based feature functions (denoted as ‘I’). For space reason, we only show the
result obtained with the overall optimal value of q (that is 0.5) for partial model.

Overall, all of our models (either full model or partial model under different values of q) perform
consistently better than the baseline when top-1 and top-3 are used as the evaluation metrics. The
baseline can just provide certain prediction accuracy for those few repair transforms that are most
widely used and is not able to correctly predict those comparably less used repair transforms, and is
not able to do “multiple-transform” prediction. Our model, however, not only keeps high prediction
accuracy for those few most widely used repair transforms, but also provides reasonable prediction
accuracy for those comparably less used repair transforms and “multiple-transform” instances.
When top-1 is used as the evaluation metric, the overall best performance model (q = 0.7) has 22.5%
and 13.0% accuracy for “single-transform” and “multiple-transform” instances respectively, and the
baseline just has 11.4% and 0% accuracy accordingly. When top-3 is used as the evaluation metric,
our best performance model (q = 0.5) achieves 61.1% and 37.1% accuracy for “single-transform” and
“multiple-transform” instances respectively, and the accuracies for the baseline are 25.2% and 0%
accordingly.

We now discuss the impact of the magnitude of the distribution-aware prior q. The distribution-
aware prior is introduced to make all training examples have a balanced impact in the learning

22

Table 2. Results for top-1 repair transform prediction.

Repair Transform

Baseline

Wrap-Meth
Unwrap-Meth
Var-RW-Var
Var-RW-Meth
Meth-RW-Var
Meth-RW-Meth
BinOperator-Rep
Constant-Rep
LogExp-Exp
LogExp-Red
Wrap-IF-N
Wrap-IF-O
Wrap-IFELSE-N
Wrap-IFELSE-O
Unwrap-IF
Wrap-TRY
Overall Accuracy

Multiple-Transform
Accuracy

0
0
0
0
0
167
0
245
136
0
0
0
0
0
0
0
11.4%

0
0.0%

q=0.0
5
28
92
40
12
166
24
231
258
6
60
11
19
6
40
11

q=0.3
4
29
91
36
11
170
29
228
259
2
58
11
13
3
37
10

Full Model
q=0.7
q=0.5
7
4
31
29
92
96
42
33
6
11
157
164
27
28
227
224
241
255
12
11
53
55
121
11
18
20
2
3
37
37
8
8

q=1.0
5
34
91
31
4
153
27
199
213
21
44
6
14
0
33
2

q=1.5 O
0
4
41
6
0
90
2
179
35
6
64
4
4
1
261
12

15
37
87
27
13
138
18
161
151
32
31
6
3
0
39
1

Partial Model
I
0
3
80
30
0
134
13
211
250
0
45
0
2
0
20
0

21.0% 20.6% 20.6% 22.5% 18.3% 15.8% 14.8% 16.4%

26
2.6%

54
5.4%

130

108
10.8% 13.0% 22.3% 27.5% 6.0% 8.6%

223

275

86

6

Table 3. Results for top-3 repair transform prediction.

Repair Transform

Baseline

Wrap-Meth
Unwrap-Meth
Var-RW-Var
Var-RW-Meth
Meth-RW-Var
Meth-RW-Meth
BinOperator-Rep
Constant-Rep
LogExp-Exp
LogExp-Red
Wrap-IF-N
Wrap-IF-O
Wrap-IFELSE-N
Wrap-IFELSE-O
Unwrap-IF
Wrap-TRY
Overall Accuracy

Multiple-Transform
Accuracy

0
0
68
0
0
262
17
280
282
0
16
62
0
0
224
0
25.2%

0
0.0%

Full Model
q=0.7
q=0.5
104
119
82
88
191
197
138
146
110
125
253
260
201
213
279
283
285
287
196
223
185
194
124
178
108
122
100
129
174
240
114
130

q=0.0
103
73
186
138
112
251
213
278
287
243
198
187
114
112
242
135
59.8% 59.9% 61.1% 55.0% 47.0% 36.0% 30.3% 43.0%

q=1.5 O
11
5
94
16
19
185
8
233
208
10
151
113
10
36
266
92

Partial Model
I
69
27
166
110
94
250
199
275
286
184
50
74
68
18
194
2

q=1.0
76
75
182
120
103
242
195
269
273
162
129
104
75
41
146
64

q=0.3
106
83
194
141
114
257
216
281
287
221
194
182
110
110
242
136

57
66
168
108
93
227
146
262
217
98
69
46
36
23
89
19

296

182
18.2% 29.6% 37.1% 40.6% 43.2% 44.0% 17.5% 31.7%

432

175

371

406

317

440

23

process. Our data is imbalanced between the “single-transform” and “multiple-transform” instances,
and it can be seen from the two tables that the use of distribution-aware prior indeed improves
the model performance on “multiple-transform” instances. By using the magnitude q = 1.5, the
model performance for “multiple-transform” instances has gone from 2.6% (when q = 0.0) to 27.5%
when top-1 is used as the evaluation metric, and the model performance for "multiple-transform"
instances has gone from 18.2% (when q = 0.0) to 44.0% when top-3 is used as the evaluation metric.
Note however when the magnitude of the distribution-aware prior q is too large, it can bring
obvious performance degrade for “single-transform” instances. In this case, “multiple-transform”
instances instead have become the dominating class in the training data. Considering both the
accuracies for “single-transform” and “multiple-transform” instances, setting the magnitude q to
0.5 or 0.7 can bring overall good performance for our data set.

We next discuss the impact of feature functions. Comparing the performance of the full model
with that of the partial model, we can see that the performance has obviously degraded when
only one kind of feature functions is used. Overall, the two kinds of feature functions, observation-
based and indicator-based, can respectively perform well for some types of repair transforms, and
complement each other to form an overall strong model. For instance, the partial model which only
uses observation-based feature functions performs much better for repair transforms Unwrap-IF
and Wrap-IF-N than the partial model which only uses indicator-based feature functions, while
the latter instead performs better for repair transforms BinOperator-Rep and Var-RW-Var.

In summary, 1) our model works on large scale real-world Java source code; 2) our model performs
well for different code transforms, as well as for both “single-transform” and “multiple-transform”
code change scenarios; 3) the use of both observation-based and indicator-based feature functions,
i.e. the mix of automatically extracted structural features and carefully engineered code analysis
features, is key to obtain good performance; 4) the distribution-aware prior q is the most important
configuration parameter in the model.

7 RELATED WORK

Big Code. Data-driven approaches for code analysis are referred to as “big code” approaches
[Raychev et al. 2015]. These approaches typically learn form the large amount of freely accessible
code in on-line repository hosting services such as GitHub, BitBucket, and SourceForge, and have
been used for code completion [Bruch et al. 2009; Raychev et al. 2014], synthesizing code [Gvero
and Kuncak 2015], name-based bug detection [Pradel and Sen 2018], deobfuscation [He et al. 2018],
inferring cryptographic API rules [Paletov et al. 2018], and adapting program analysis [Oh et al.
2015], etc. In this paper, we target a different problem: the prediction of code transforms at the level
of AST nodes. We refer the reader to [Allamanis et al. 2018a] for a recent overview of machine
learning on code.

Program Representation for Learning. Within the big code literature, some works specifically
target the representation problem. Alon et al. elaborate a path-based representation of ASTs [Alon
et al. 2018, 2019] and DeFreez et al. [DeFreez et al. 2018] use paths in the control flow graph to
embed functions. Allamanis et al. explore a graph-based representation to be given as input to a
graph neural-network [Allamanis et al. 2018b]. Henkel et al. [Henkel et al. 2018] represent programs
with traces obtained from symbolic execution. Ideally, part of the representation is automatically
inferred for these works, which is called “learned semantic feature” by Wang et al. [Wang et al.
2016].

Partial Program and Learning. Program repair [Monperrus 2017] and program sketching [Solar-
Lezama 2013] share the idea that one already has a partial solution (the buggy program or the
sketch). Recent research suggests that one can learn the sketches [Murali et al. 2018] or learn the

24

relation between code features and sketch implementations [Murali et al. 2017]. Beyond the concept
of machine learning on partial programs, the goal and techniques presented in this paper are
completely different. Long et al. [Long et al. 2017] infer code transforms from bug-fixing commits
that have fixed the same type of bugs. On the contrary, our approach does not need this tedious
data preparation phase and predicts the needed code transforms at the level of AST nodes for each
specific input code snippet.

Data-driven Program Repair. Some works in the program repair literature [Monperrus 2017]
explore data-driven program repair. Le et al. [Le et al. 2016] use the commit history to select the
most likely mutation-based patch. Long and Rinard [Long and Rinard 2016] propose an optimization
approach to select the patch that resembles the most to past human patches, aiming to get patches
that suffer less from overfitting [Yu et al. 2018]. Wang et al. [Wang et al. 2018] use data-driven
program repair for automated feedback generation. Neural networks can also be used. Hata et al.
[Hata et al. 2018] and Chen et al. [Chen et al. 2018] train a neural network based on past diffs, and
Pu et al. [Pu et al. 2016] train a neural network for fixing MOOC programs. The representation in
this work with full ASTs and carefully engineered code features, used together with structured
prediction radically departs from the existing related work.

8 CONCLUSION

A computer program evolves under a sequence of code transforms, for reasons such as bug fixing
and adaptation. These code transform activities typically represent a significant amount of the cost
of a system, calling for tools to aid them. We present in this paper the first approach for structurally
predicting code transforms at the level of AST node. Our approach leverages conditional random
filed (CRF), and first learns offline a probabilistic model that captures how certain code transforms
are applied to certain AST nodes from existing data, and then uses the learned model to predict
transforms for new, unseen code snippet. Our approach is generic and can be instantiated in
different ways. We present in this paper an implementation in the context of repair transform
prediction for Java programs. Our implementation contains a set of carefully designed code features,
and deals with the training data imbalance and transform constraint issues that arise for code
transform prediction problems. We conduct a large-scale experimental evaluation on 4,590,679
bug fixing commits, and the evaluation results show that using top-3 as the evaluation metric,
our overall best performance model achieves 61.1% and 37.1% accuracy for “single-transform” and
“multiple-transform” cases respectively.

REFERENCES
Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. 2018a. A survey of machine learning for big

code and naturalness. ACM Computing Surveys (CSUR) 51, 4 (2018), 81.

Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. 2018b. Learning to Represent Programs with Graphs. In

ICLR.

Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2018. A General Path-based Representation for Predicting Program

Properties. In PLDI. 404–419.

Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2019. code2vec: Learning Distributed Representations of Code. In

POPL.

Marcel Bruch, Martin Monperrus, and Mira Mezini. 2009. Learning from Examples to Improve Code Completion Systems.
In Proceedings of the 7th joint meeting of the European Software Engineering Conference and the ACM Symposium on the
Foundations of Software Engineering. https://doi.org/10.1145/1595696.1595728

Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. 2002. SMOTE: synthetic minority

over-sampling technique. Journal of artificial intelligence research 16 (2002), 321–357.

Zimin Chen, Steve Kommrusch, Michele Tufano, Louis-Noël Pouchet, Denys Poshyvanyk, and Martin Monperrus. 2018.

SequenceR: Sequence-to-Sequence Learning for End-to-End Program Repair. Technical Report 1901.01808. arXiv.

25

Daniel DeFreez, Aditya V Thakur, and Cindy Rubio-González. 2018. Path-Based Function Embedding and its Application to

Specification Mining. In Proceedings of ESEC/FSE.

Charles Elkan. 2001. The Foundations of Cost-sensitive Learning. In Proceedings of the 17th International Joint Conference
on Artificial Intelligence - Volume 2 (IJCAI’01). Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 973–978.
http://dl.acm.org/citation.cfm?id=1642194.1642224

Jean-Rémy Falleri, Floréal Morandat, Xavier Blanc, Matias Martinez, and Martin Monperrus. 2014. Fine-grained and accurate

source code differencing. In ASE. 313–324.

B. Fluri, M. Wuersch, M. PInzger, and H. Gall. 2007. Change Distilling:Tree Differencing for Fine-Grained Source Code

Change Extraction. IEEE Transactions on Software Engineering 33, 11 (2007), 725–743.

Sumit Gulwani, Oleksandr Polozov, Rishabh Singh, et al. 2017. Program synthesis. Foundations and Trends® in Programming

Languages 4, 1-2 (2017), 1–119.

Tihomir Gvero and Viktor Kuncak. 2015. Synthesizing Java expressions from free-form queries. In Acm Sigplan Notices,

Vol. 50. ACM, 416–432.

Hideaki Hata, Emad Shihab, and Graham Neubig. 2018. Learning to Generate Corrective Patches using Neural Machine

Translation. arXiv preprint 1812.07170 (2018).

Jingxuan He, Pesho Ivanov, Petar Tsankov, Veselin Raychev, and Martin Vechev. 2018. Debin: Predicting Debug Information

in Stripped Binaries. In CCS. 1667–1680.

Xuming He, R. S. Zemel, and M. A. Carreira-Perpinan. 2004. Multiscale conditional random fields for image labeling. In

CVPR.

Jordan Henkel, Shuvendu Lahiri, Ben Liblit, and Thomas Reps. 2018. Code Vectors: Understanding Programs Through

Embedded Abstracted Symbolic Traces. In Proceedings of ESEC/FSE.

Andrei Homescu, Steven Neisius, Per Larsen, Stefan Brunthaler, and Michael Franz. 2013. Profile-guided Automated Software
Diversity. In Proceedings of the 2013 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)
(CGO ’13). IEEE Computer Society, Washington, DC, USA, 1–11. https://doi.org/10.1109/CGO.2013.6494997

Finn V. Jensen and Frank Jensen. 1994. Optimal Junction Trees. In Proceedings of the Tenth International Conference on
Uncertainty in Artificial Intelligence (UAI’94). Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 360–366.
http://dl.acm.org/citation.cfm?id=2074394.2074440

Rajeev Joshi, Greg Nelson, and Keith Randall. 2002. Denali: A Goal-directed Superoptimizer. SIGPLAN Not. 37, 5 (May 2002),

304–314. https://doi.org/10.1145/543552.512566

Florent Jousse, Rémi Gilleron, Isabelle Tellier, and Marc Tommasi. 2006. Conditional Random Fields for XML Trees. In

Workshop on Mining and Learning in Graphs. Berlin, Germany. https://hal.inria.fr/inria-00118761

Dongsun Kim, Jaechang Nam, Jaewoo Song, and Sunghun Kim. 2013. Automatic Patch Generation Learned from Human-
written Patches. In Proceedings of the 2013 International Conference on Software Engineering (ICSE ’13). IEEE Press,
Piscataway, NJ, USA, 802–811. http://dl.acm.org/citation.cfm?id=2486788.2486893

Daphne Koller and Nir Friedman. 2009. Probabilistic Graphical Models: Principles and Techniques - Adaptive Computation and

Machine Learning. The MIT Press.

John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional Random Fields: Probabilistic Models for

Segmenting and Labeling Sequence Data. In ICML. 282–289.

X. B. D. Le, D. Lo, and C. L. Goues. 2016. History Driven Program Repair. In Proceedings of the 23rd International Conference

on Software Analysis, Evolution, and Reengineering (SANER). 213–224.

Dong C. Liu and Jorge Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Mathematical

Programming (Aug 1989), 503–528.

Fan Long, Peter Amidon, and Martin Rinard. 2017. Automatic Inference of Code Transforms for Patch Generation. In

ESEC/FSE. 727–739.

Fan Long and Martin Rinard. 2016. Automatic Patch Generation by Learning Correct Code. In POPL. 298–312.
Fernanda Madeiral, Thomas Durieux, Victor Sobreira, and Marcelo Maia. 2018. Towards an automated approach for bug fix

pattern detection. arXiv preprint arXiv:1807.11286 (2018).

Tom Mens and Tom Tourwé. 2004. A Survey of Software Refactoring. IEEE Trans. Softw. Eng. 30, 2 (Feb. 2004), 126–139.

https://doi.org/10.1109/TSE.2004.1265817

Martin Monperrus. 2017. Automatic Software Repair: a Bibliography. ACM Computing Surveys 51 (2017), 1–24. https:

//doi.org/10.1145/3105906

Vijayaraghavan Murali, Swarat Chaudhuri, and Chris Jermaine. 2018. Bayesian Sketch Learning for Program Synthesis. In

ICLR.

Vijayaraghavan Murali, Letao Qi, Swarat Chaudhuri, and Chris Jermaine. 2017. Neural sketch learning for conditional

program generation. arXiv preprint arXiv:1703.05698 (2017).

Hakjoo Oh, Hongseok Yang, and Kwangkeun Yi. 2015. Learning a strategy for adapting a program analysis via bayesian

optimisation. In ACM SIGPLAN Notices, Vol. 50. ACM, 572–588.

26

Rumen Paletov, Petar Tsankov, Veselin Raychev, and Martin Vechev. 2018. Inferring crypto api rules from code changes. In

ACM SIGPLAN Notices, Vol. 53. ACM, 450–464.

Renaud Pawlak, Martin Monperrus, Nicolas Petitprez, Carlos Noguera, and Lionel Seinturier. 2015. Spoon: A Library for
Implementing Analyses and Transformations of Java Source Code. Software: Practice and Experience 46 (2015), 1155–1179.
https://doi.org/10.1002/spe.2346

Judea Pearl. 1982. Reverend Bayes on Inference Engines: A Distributed Hierarchical Approach. In Proceedings of the Second
AAAI Conference on Artificial Intelligence (AAAI’82). AAAI Press, 133–136. http://dl.acm.org/citation.cfm?id=2876686.
2876719

David Pinto, Andrew McCallum, Xing Wei, and W. Bruce Croft. 2003. Table Extraction Using Conditional Random Fields. In

SIGIR. 235–242.

Michael Pradel and Koushik Sen. 2018. DeepBugs: A Learning Approach to Name-based Bug Detection. Proc. ACM Program.

Lang. 2, OOPSLA, Article 147 (Oct. 2018), 25 pages.

Yewen Pu, Karthik Narasimhan, Armando Solar-Lezama, and Regina Barzilay. 2016. sk_p: a neural program corrector
for MOOCs. In Companion Proceedings of the 2016 ACM SIGPLAN International Conference on Systems, Programming,
Languages and Applications: Software for Humanity. ACM, 39–40.

Veselin Raychev, Martin Vechev, and Andreas Krause. 2015. Predicting Program Properties from Big Code. In POPL. 111–124.
Veselin Raychev, Martin Vechev, and Eran Yahav. 2014. Code Completion with Statistical Language Models. In PLDI.

419–428.

Ripon K. Saha, Yingjun Lyu, Hiroaki Yoshida, and Mukul R. Prasad. 2017. ELIXIR: Effective Object Oriented Program Repair.
In Proceedings of the 32Nd IEEE/ACM International Conference on Automated Software Engineering (ASE 2017). IEEE Press,
Piscataway, NJ, USA, 648–659. http://dl.acm.org/citation.cfm?id=3155562.3155643

Armando Solar-Lezama. 2013. Program sketching. International Journal on Software Tools for Technology Transfer 15, 5-6

(2013), 475–495.

Y. Song, L. Morency, and R. Davis. 2013. Distribution-sensitive learning for imbalanced datasets. In 2013 10th IEEE International
Conference and Workshops on Automatic Face and Gesture Recognition (FG). 1–6. https://doi.org/10.1109/FG.2013.6553715
Mauricio Soto, Ferdian Thung, Chu-Pan Wong, Claire Le Goues, and David Lo. 2016. A Deeper Look into Bug Fixes:
Patterns, Replacements, Deletions, and Additions. In Proceedings of the 13th International Conference on Mining Software
Repositories (MSR ’16). ACM, New York, NY, USA, 512–515. https://doi.org/10.1145/2901739.2903495

Charles Sutton and Andrew McCallum. 2012. An Introduction to Conditional Random Fields. Found. Trends Mach. Learn.

(April 2012).

David M.J. Tax and Robert P.W. Duin. 2004. Support Vector Data Description. Machine Learning 54, 1 (01 Jan 2004), 45–66.

https://doi.org/10.1023/B:MACH.0000008084.60811.49

Gabriel Valiente. 2002. Algorithms on Trees and Graphs. Springer-Verlag, Berlin, Heidelberg.
Ke Wang, Rishabh Singh, and Zhendong Su. 2018. Search, Align, and Repair: Data-driven Feedback Generation for

Introductory Programming Exercises. In PLDI. 481–495.

Song Wang, Taiyue Liu, and Lin Tan. 2016. Automatically Learning Semantic Features for Defect Prediction. In ICSE.

297–308.

Zhongxing Yu, Matias Martinez, Benjamin Danglot, Thomas Durieux, and Martin Monperrus. 2018. Alleviating patch
overfitting with automatic test generation: a study of feasibility and effectiveness for the Nopol repair system. Empirical
Software Engineering (2018).

27

