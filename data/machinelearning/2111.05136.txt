1
2
0
2

v
o
N
5
2

]
P
A

.
t
a
t
s
[

2
v
6
3
1
5
0
.
1
1
1
2
:
v
i
X
r
a

Using sequential drift detection to test the API
economy

Samuel Ackerman ∗

Parijat Dube †

Eitan Farchi ‡

November 29, 2021

Abstract

The API economy refers to the widespread integration of API (ad-
vanced programming interface) microservices, where software applications
can communicate with each other, as a crucial element in business mod-
els and functions. The number of possible ways in which such a system
could be used is huge. It is thus desirable to monitor the usage patterns
and identify when the system is used in a way that was never used be-
fore. This provides a warning to the system analysts and they can ensure
uninterrupted operation of the system.

In this work we analyze both histograms and call graph of API usage
to determine if the usage patterns of the system has shifted. We compare
the application of nonparametric statistical and Bayesian sequential anal-
ysis to the problem. This is done in a way that overcomes the issue of
repeated statistical tests and insures statistical signiﬁcance of the alerts.
The technique was simulated and tested and proven eﬀective in detecting
the drift in various scenarios. We also mention modiﬁcations to the tech-
nique to decrease its memory so that it can respond more quickly when
the distribution drift occurs at a delay from when monitoring begins.

1

Introduction

API economy [9] is driving the digital transformation of business applications
across the cloud and edge environments. In such applications, API interfaces en-
able meeting speciﬁc business needs, such as validating the credit of a customer.
By composing the API interfaces from multiple vendors a greater business value
is obtained. For example, in an online bank, putting the APIs together allows
supporting the diﬀerent functions of the bank, such as creating accounts, pro-
viding saving programs and loans.

The composition of APIs in such a way increases the number of combinations
In fact, the number of combinations is unbounded as there is

exponentially.

∗IBM Research, Haifa; samuel.ackerman@ibm.com
†IBM Research, Yorktown Heights
‡IBM Research, Haifa

1

 
 
 
 
 
 
little limitation on how APIs can be sequenced. More importantly, APIs are
combined in unexpected ways and it is hard for the vendors providing the APIs
to anticipate and test the way the APIs will be used. This challenge exists even if
only one vendor is used, due to the exponential number of possible combinations.
In addition, teams developing for the API economy many times use continuous
delivery. In the literature survey [7], low test coverage is highlighted as one of
the challenges of continuous delivery.

This work focuses on analyzing the way APIs are composed, and identifying
new ways in which the APIs are composed that were never tested before. Such
analysis enables the system analysts to be ahead of the curve and proactive in
testing anticipated way in which APIs are composed and used. Speciﬁcally, we
use a Bayesian sequential technique from [4] to model an observed stream of
API calls by the frequency of categories (i.e., APIs), either as individual APIs
or as pairwise calls between one API microservice and another.

An alert is raised if the categorical frequency distribution of observed calls
diﬀers (i.e., has drifted) from that expected under a pre-speciﬁed baseline (typ-
ically based on a previous stable sample of calls. This Bayesian test overcomes
known challenges in sequential monitoring in that the statistical guarantee of
correctness of a decision of whether the distribution has changed is adversely
aﬀected by repeated monitoring and testing of drift vs the baseline; here, the
guarantee is maintained no matter how long the system is monitored. We prove
the eﬀectiveness of this technique through simulations based on real systems,
where we demonstrate both that we avoid detecting drift if the observations
are from the same distribution as the baseline (control of false positive error
rate) at a known rate, and also that when the distortion of the observed dis-
tribution is increased relative to the baseline, that it is correctly detected (true
positive rate) at a rate increasing with the degree of distortion, represented by
a distribution mixing parameter.

A further issue is that if distribution drift occurs at a delay after beginning
monitoring, the drift detection ability is reduced due to the memory of the test,
which takes into account the full monitoring history and not just the most recent
observations. In Section 7, we mention several modiﬁcations, which give higher
weight to recent observations, in modeling the distribution of API calls, and
thus, in making our drift decision. These modiﬁcations are left for future work.

2 Background

A recent work [2] looked at changes in confusion matrix of APIs related to
machine learning models to detect shift in API performance. They proposed
an adaptive sampling approach to estimate shifts in confusion matrix, requiring
90% less data than random sampling. [8] is a comprehensive report on designing,
monitoring, and testing of microservices based on survey results and interviews
with microservices practitioners. Monitoring and logging are integral parts of
microservices. Our solution can leverage monitoring logs for API usage.
[3]
develops a robust API tracer to protect against adversarial attacks on APIs.

2

3 Problem setup

In this work, we outline some approaches for modeling patterns of API calls
and detecting drift in them. As the problem can have many aspects (e.g.,
the parameter inputs to the APIs, timestamp of execution, etc.), we choose to
limit ourselves to modeling the patterns of which APIs are called, that is, of
modeling the frequency distribution of the API calls. As such, the problem can
be described as modeling of a random variable with a categorical distribution,
with values being the API URL labels. Though we focus on the application
to APIs, we note that this framework can be used for similar problems of a
categorical nature.

Let fi be a given API name (categorical value), such as “/home server/data/delete”,

and A = {f1, f2, . . . , f|A|} be the set of potential API names under consid-
eration. Let zt be an observation made at time index t ∈ {1, 2, . . . }, with
z = {zt}t=1,... referring to the whole stream of observations, and zt denoting a
subset {zi}t
i=1, up to a particular time index t. We will consider two observation
settings, where each zt represents either a single API or pair of APIs, as follows;
this can also be extended to combinations of more than 2 APIs. Ultimately, the
modeling strategy is essentially the same, with minor modiﬁcations:

1. Single APIs: here, each zt is a single element from the set A, which has

|A| elements.

2. Pairs of APIs: zt = (fi, fj), an ordered pair, where fi, fj ∈ A. We
also allow parent-less or child-less pairs, where ∅ denotes a missing value.
Thus, we also allow zt = (∅, fi) or (fi, ∅), where fi ∈ A, represent parent-
less or child-less calls, respectively. Denote the set of potential calls as
C = (A (cid:83){∅}) × (A (cid:83){∅}); this set has |C| = (|A| + 1)2 elements. For
simplicity, we include the pair (∅, ∅), which is not allowed, as an element
of C with probability 0. Some other pairs (fi, fj) ∈ C may not actually
be possible due to constraints of the system (e.g., some APIs may not call
others), and will have probability 0 in reality.

In this work, we assume all relevant aspects of interest of the API behaviors
can be characterized by the observed sequence of calls z1, z2, . . . . We do not
consider, for instance, the clock time gaps between observations, or associated
API input hyper-parameters, but only the relative frequencies of each category
(cid:96) observed in the sequence z. We consider such extensions in Section 8.

In our modeling of the category frequencies, we assume the observed values
zt ∈ z are mutually independent and identically distributed (iid). That is, that
any given single or set of draws do not impact the likelihood of certain values
being observed at other time indices. The identical distribution assumption
means that the distribution, in terms of the likelihood of each individual cat-
egory value being drawn, stays stable across t; this makes it more sensible to
model, since a moving target (changing distribution) is diﬃcult to model with-
out adjustments described in Section 7. Our task it to detect at some point t∗,
whether the observed stream {z1, . . . , zt∗ } appear to have drifted in distribution

3

relative to a pre-determined baseline distribution. The set A will initially have
known ﬁnite size according to the values observed in the baseline, but can be
extended if new APIs are observed over time.

Let K now, in general, be the number of unique potential category labels
(cid:96) = {(cid:96)1, . . . , (cid:96)K}. That is, in the single case each category (cid:96)i is a single API,
and in the pairs case, (cid:96)i is an ordered pair, an element of C; so K = |A| or
(|A| + 1)2, as appropriate.

. . .

In either case, each observed value zt is modeled as an iid draw from a
multinomial distribution. A multinomial distribution is denoted M(θ, n) where
(cid:3) is a K-element weight vector where 0 ≤ θi ≤ 1, ∀i =
θ = (cid:2)θ1
θK
1, . . . , K and ((cid:80)K
i=1 θi) = 1; sometimes the notation p is used instead of θ.
The multinomial distribution deﬁnes a distribution over K speciﬁed elements—
say, the category labels in the vector (cid:96)—with respective indices i = 1, . . . , K;
each θi is the probability of the ith element (cid:96)i occurring on each trial. n is a
positive integer representing a number of trials, or draws. A draw x ∼ M(θ, n)
from this distribution is a K-length vector x whose elements are non-negative
integers and sum to n. The ith element of x is the total number of occurrences
(frequency) of the element (cid:96)i, out of n trials, in this random draw. Category (cid:96)i
has an expected frequency of nθi, on average.

If n = 1, a draw x will have a single element—say, the ith—being 1, and
all others being 0. This represents a random draw of the category (cid:96)i out of the
K possible, that is, what is typically referred to as a draw from a categorical
distribution. We thus model each zt ∼ M(θ, n = 1), where the probability
vector θ is identical for all draws.

Furthermore, for zt, consisting of t total calls, let zt denote a K-length
vector of non-negative integers summing to n, where the ith element is the total
number of occurrences of the category (cid:96)i, out of K. Thus, we can model zt ∼
M(θ, n = t); the observed zt is one possible ordered sequence of realizations of
the frequency totals zt. If t = 1, then z1 = (cid:2)0
0(cid:3) is a unit vector
of 0s and a single element with 1, as above.

. . .

. . .

1

4 Drift simulation

Here, we introduce notation for our simulations, which verify the statistical
correctness of technique in detecting drift under the modeling assumptions. Our
simulations of observations are made from observed samples of calls on real API
systems. Say we observe n calls, either in the single or pairs case. Let H be a
vector or matrix whose entries are all in [0, 1] and sum to 1, and thus represent
a probability distribution over API category labels, as follows:

1. Single APIs: H = (cid:2)h1, . . . , hK

(cid:3), where hi is the relative frequency (ob-
served fraction or probability) of API label fi ∈ A being observed, out of
n.

2. Pairs of APIs: H(|A|+1)×(|A|+1) = (cid:2)hi,j

(cid:3), where hi,j is the relative fre-
quency of API pair (fi, fj) occurring, out of n. We can consider ∅ as an

4

extra (|A| + 1)th element, so then h|A|+1, |A|+1 = 0 is the probability of
(∅, ∅) occurring, which is impossible. H is typically very sparse (mostly
0s), since many theoretical potential pairs are not observed, since most
APIs may in practice not call each other, or the combination may be in-
compatible (impossible). Each hi,j corresponds to one of the K categories.

In either the single or pairwise case, let H to represent the baseline distri-
bution (based on real observed behavior), and let H∗ be another distribution
from which we simulate observations zt ∼ M(H∗, n = 1) as a single category
with probabilities given by H∗. Drift happens when the distribution H∗ of the
observed {zt}, either as single or pair calls, is determined to be signiﬁcantly
diﬀerent from the baseline H.

In our simulations, we create H∗ = (1 − πt)H + πtH(cid:48), for some 0 ≤ πt ≤ 1,
t = 1, 2, . . . as a mixture distribution of H and some other H(cid:48), so that we can
control the amount of drift by varying the mixing constants πt (see similar setup
in [1]). In the present work, instead of allowing πt to vary as in [1], we set it to
be a constant 0 ≤ c ≤ 1, ∀t. In reality (i.e., outside of simulations), H∗ will be
some unknown distribution.

In general, though, πt can vary over time, as in the scenarios in Figure 5 of [1].
If πt = 0, H∗ = H, so there is no drift. In [1], we allowed H∗ to shift gradually
from H to H(cid:48) by letting πt increase gradually from 0 to 1. Furthermore, drift
was allowed to begin at a delay of ts − 1 time points, where ts ≥ 1 is the ﬁrst
index for which πt > 0, when drift is ﬁrst introduced; drift ended at some time
te ≥ ts, at which πt = 1. That is, for t < ts, draws of zt would come from H,
and for t ≥ te, they come from H(cid:48). If ts = 1, drift begins immediately, otherwise
there is a delay while the initial draws are still from H. For ts ≤ t ≤ te, we may
increase πt in a linear or other fashion to introduce the drift gradually.

The ability to detect drift should depend on how signiﬁcantly H diﬀers from
the mixture H∗; this depends both on how H and H(cid:48) diﬀer, and on the value
of πt. For ﬁxed H(cid:48), the diﬀerence from H increases as πt grows.

5 Sequential identiﬁcation of drift of an API

stream

5.1 The importance of sequential analysis

As mentioned in Section 3, our problem consists of observing an ordered se-
quence z = {z1, z2, . . . }, each potential value of which is either a single or an
ordered pair of APIs. In either case, we can consider each potential value, even
the pairs, as a unique category label. On the basis of the observed API calls,
we would like to decide if the sequence seems signiﬁcantly anomalous relative
to what would be expected if they were drawn from the baseline H; this makes
it a sequential decision problem.

As mentioned in our prior work ([1]), performing sequential decisions is more
complicated to do in an appropriate statistical way than it is to make a single

5

non-sequential decision on a sample of values. In short, we typically want to be
able to peek at the data (e.g., the sequence {z1, . . . , zt} up to time t, or some
moving window subset of it) to make as timely a decision of drift as possible,
without waiting to observe the ‘entire’ sequence.

Traditional (non-sequential) hypothesis tests or decision problems typically
have a statistical guarantee, such as on the type-1 error (false alarm rate) that
assumes a single test or independence requirements. However, in sequential
decision-making, we may want a statistical guarantee on our single decision, if
we make it, that drift has occurred; this means we cannot naively apply methods
designed for single hypothesis tests and expect the statistical guarantees to hold.
Our work in [1] gave one example of a method (CPMs, or change point models),
designed to overcome this challenge for univariate data, without parametric
assumptions.

5.2 Multinomial sequential test and Bayes Factor (BF)

This work will directly apply a Bayesian technique from [4]. Although [4] deals
speciﬁcally with sequentially-observed categorical data, rather than numeric,
the essential background of this technique applies to any sequential data that
can be parametrically modeled.

. . .

θK

In Bayesian analysis, typically a parametric model is proposed for a given
set of observed data. In our case, we assume the frequencies of diﬀerent single
or paired API calls can be modeled by a multinomial distribution (Section 3)
with a certain probability vector θ = (cid:2)θ1
(cid:3), which is the parameter
of interest. In the Bayesian framework, this parameter of interest is modeled
itself by another distribution. In this case, typically the Dirichlet distribution
is used. The Dirichlet distribution is deﬁned by a K-length hyperparameter
vector α = (cid:2)α1
(cid:3), where αi > 0, ∀i = 1, . . . , K. A draw from the
Dirichlet distribution x ∼ D(α) is a K-length vector x = (cid:2)x1
(cid:3), where
each 0 ≤ xi ≤ 1, ∀i = 1, . . . , K and ((cid:80)K
i=1 xi) = 1, and element-wise expected
αi
. Thus, the Dirichlet distribution models a probability
value EV(xi) =
j=1 αj
weight vector, such as θ of the multinomial distribution, where each element
(weight) xi (or θi) is proportional to its respective αi; furthermore, the variance
of each xi decreases as (cid:80)

. . . xK

. . . αK

i αi increases.

Thus, the Bayesian model for θ, the probability vector (or matrix) of cate-
gories, is that the observed category frequencies are zt ∼ M(θ, n = t), and then
θ ∼ D(α). Modeling the (assumed unknown) θ which governs the frequencies of
categories zt is done by estimating the parameter α of the Dirichlet distribution
on θ.

(cid:80)K

In the Bayesian framework, the user speciﬁes initial values for this parameter
α (called the prior distribution), and the parameter values are sequentially
updated in light of the observed data (posterior distribution). For instance,
here we will specify the prior distribution (α0, where subscript ‘0’ indicates the
values at time t = 0) with respective values α0,i being proportional to the values
in the baseline H; categories zt that are observed but not in the baseline (e.g.,

6

new APIs not in A, or potential pairs in C which had 0 frequency in H) are
given very small (close to 0 but not exactly 0) prior weights.

At time t, the current posterior is αt, and both the prior and posterior
distributions are Dirichlet. The updating rules for the Dirichlet are that if
zt corresponds to category indexed i, the new posterior value is the previous
increased by 1 (e.g., αt,i = αt−1,i + 1). In this case, we say that the Dirichlet
distribution is conjugate for the multinomial, because after updating when the
data are multinomial-distributed, the posterior remains Dirichlet-distributed.

The posterior value αt takes into account both the prior α0 and updates
from the observed data zt. Given any appropriate vector value for α and set
of observed values z of length n, we can calculate how well α seems to char-
acterize z, by evaluating the likelihood function L of the Dirichlet with both z
(in the form of frequencies of counts z) and α; this applies to any parametric
distribution, not just the multinomial. The higher L(α, z), the better the ﬁt.

In particular, we can evaluate the relative ﬁts of the prior α0 and posterior
αt at any given t, to determine if drift has happened; drift in this case means the
posterior ﬁts the data much better than the prior does. This is done by forming
the ratio, or odds, L(αt, z)/L(α0, z), called the posterior odds. A user can
specify prior odds, a (subjective) guess as to how much to pre-favor the prior
α0; for instance, prior odds of 2 (i.e., 1 : 2) means the user wants to give twice
as much weight to the prior’s ﬁt over the posterior. Usually, the prior odds are
set to 1 to be agnostic. The Bayes Factor (BF) is the product of the posterior
and prior odds, and evaluates the relative ﬁt of the prior and posterior, while
taking into account the user’s conﬁdence in the initial prior. Since likelihood
functions L and odds must always be positive-valued, we have that BF > 0.

If the BF > 1, the posterior ﬁts the data better than the prior; if 0 < BF < 1,
the reverse is true, and if the BF ≈ 1, both ﬁt about the same. For a given
constant k > 1, if the BF = k (say, k = 50), this means the posterior ﬁt is
k times better than the prior; alternatively, since we can take the reciprocal
of the BF, a BF = 1/k means the prior ﬁt is k times better (or posterior is
1/k times as good). The higher the BF, the more conﬁdence we have in the
posterior, relative to the prior, and thus the distribution drift appears to be
more extreme.

As mentioned, in many statistical hypothesis decision-making setups, the
user wants to primarily control the false positive rate (i.e., false detection of
distribution change when it hasn’t really happened) to be lower than a pre-
speciﬁed level 0 < α < 1, not to be confused with the α parameters of the
Dirichlet distribution. α is usually set close to 0, with lower values corresponding
to a more conservative decision, in that the observed deviation has to be more
extreme (i.e., improbable) relative to the null hypothesis to declare drift. In
our case, a lower α means the posterior distribution over categories needs to be
more diﬀerent from the prior expected distribution than otherwise, in order to
declare the distribution has drifted. As we now see, this has a direct connection
to the measured BF.

In [4], their proposed test, which we use, has the following rule: rejecting
the posterior in favor of the prior, if BF > 1/α, has a false positive rate of α.

7

That is, say, if the BF > 100 = 1/0.01, this decision that drift happened has a
false positive rate of α = 0.01. The key aspect here is that this decision can be
made in a sequential manner without sacriﬁcing the statistical guarantee, unlike
other statistical tests whose guarantee α no longer holds once they are applied
more than once, due to the ‘peeking’ problem. That is, we can observe a stream
z1, z2, z3, . . . , and evaluate the BF for the posterior αt at each zt, versus the
prior α0 (i.e., ‘peek’). If the BF ever exceeds the threshold 1/α, the decision
can be made with false alarm guarantee α.

A test that is commonly used in non-sequential testing to detect diﬀerences
in distribution between categorical frequency or probability vectors is the chi-
squared (χ2) test (not to be confused with the chi-squared test on contingency
tables which tests independence of two diﬀerent categorical variables). The
authors of [4] contrast their sequential Bayesian test based on the BF to a
non-sequential approach of at each time t, performing the chi-squared test to
compare the prior α0 and posterior αt (normalizing each vector to sum to 1, as
is required for the test).

For a ﬁxed decision threshold α, the non-sequential test would decide drift at
the ﬁrst t where the chi-squared test yielded a p-value less than α; correspond-
ingly, the sequential test would decide drift at the ﬁrst t where the BF > 1/α.
They show that the chi-squared test results in a false positive rate that is higher
than the expected α, precisely due to the ‘peeking’ problem and that the deci-
sion process does not adjust for the fact that the test is performed sequentially;
the multinomial test, in contrast, maintains the expected α-level control over
the false positive rate. If the simpler non-sequential test performs just as well
as the more complicated multinomial test in terms of false positive rate control,
all other properties being equal, there would be no need for a more complicated
procedure, hence the need to demonstrate this. In our simulated experiments,
however, applying the chi-squared test seemed to actually give similar results
to the multinomial test.

6 Results

6.1 Simulation setup

As noted in Section 4, we simulate distribution drift from H to another H(cid:48) by
forming a mixture distribution H∗ of the two, with mixing parameter 0 ≤ πt ≤ 1
at time t. Setting πt = 0, ∀t is thus no drift, since H∗ = H, ∀t. Thus,
when applying the multinomial test from [4] (Section 5.1), at any given value
0 < α < 1, we should expect a false drift detection probability of α, if we detect
drift when the BF exceeds 1/α. This can be veriﬁed by multiple repetitions
of simulated sequences from H∗ = H, and seeing that at most (100α)% of
sequences ever have the BF exceed 1/α, at any α we choose, when the prior α0
reﬂects the baseline H. Furthermore, we should see that at any α, when we do
introduce drift (i.e., πt > 0, so H∗ (cid:54)= H), we should see that more than (100α)%
of simulated sequences are detected to have drifted, and that this proportion

8

should increase with increasing amounts of drift (larger diﬀerence between H∗
and H, which for a ﬁxed second distribution H(cid:48), means increasing πt).

In these experiments, we will illustrate only the pairwise API setting. We
begin with two frequency matrices F and F(cid:48) below, which represent n = 89
and 88 observations, respectively. These matrices represent calls on the same
system, so the sets of APIs covered are the same, and frequencies are fairly
similar. The most frequent pairs are (frontend, currencyservice) and (frontend,
productcatalogservice).

∅
adservice
cartservice
checkoutservice
currencyservice
frontend
loadgenerator
productcatalogservice
recommendationservice
shippingservice

∅
adservice
cartservice
checkoutservice
currencyservice
frontend
loadgenerator
productcatalogservice
recommendationservice
shippingservice


0
0


0


0


0


0


0


0


0
0


0
0


0


0


0


0


0


0


0
0

0
0
0
0
0
2
0
0
0
0

0
0
0
0
0
5
0
0
0
0

0
0
0
0
0
9
0
0
0
0

0
0
0
1
0
6
0
0
0
0

0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0

0
0
0
2
0
17
0
0
0
0

0
0
0
0
0
22
0
0
0
0

0
0
0
0
0
0
10
0
0
0

0
0
0
0
0
0
7
0
0
0

0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0

0
0
0
1
0
38
0
0
8
0

0
0
0
0
0
34
0
0
7
0

0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
2
0
0
0
0



0
0


0


0


0


2


0


0


0
0



0
0


0


1


0


3


0


0


0
0

F =

F(cid:48) =

In our simulations, we experiment with various drift contamination propor-
tions of π = {0.0, 0.05, 0.10, 0.20, 0.30, 1.0} on the resulting probability matrices
H and H(cid:48) (each of which is F or F(cid:48) divided by the sum of entries n). π = 0
and 1 correspond to H and H(cid:48), the baseline and alternate distributions, shown
in the upper left and lower right of Figure 1, respectively. Because the original
frequency matrices are similar, the mixture images do not seem very diﬀerent,
making our drift detection more challenging.

In the sequential multinomial test, the posterior αt of the Dirichlet distri-
bution is compared to the prior α0 (Equation 1). A prior value α0,i = 0 for the
category (cid:96)i represents a prior belief that this category is impossible. Therefore,
if this category ends up being observed (so the posterior αt,i > 0), the BF would
be inﬁnite since the denominator would be 0. Hence, we give each unobserved
category (0 frequency in F) in the baseline a small positive prior value rather
than 0. Our matrix of prior probability values α0, shown below, is calculated

9

Figure 1: Mixture probability matrices H∗ of H and H(cid:48), with varying mixing
proportions π. Note, the mixture images seem identical, but they are not; they
only appear so because the underlying distributions H (top left) and H(cid:48) (bottom
right) themselves are very similar.

10

Figure 2: Log BF for r = 500 repetitions of n = 1, 000 random draws from
H∗ = H (no drift).

by rescaling the nonzero elements of H to sum to n = 50 (the weight we give the
prior baseline sample), then setting the zero-valued frequency elements to have
a low weight of c(cid:48) = 0.00006 (the value chosen so c(cid:48)n/(# nonzero elements) ≤ c,
for c = 0.0001, to avoid giving them too much total weight.

∅
adservice
cartservice
checkoutservice
currencyservice
frontend
loadgenerator
productcatalogservice
recommendationservice
shippingservice

α0 =



















0.0
0.00006
0.00006
0.00006
0.00006
0.00006
0.00006
0.00006
0.00006
0.00006

0.00006
0.00006
0.00006
0.00006
0.00006
5.61798
0.00006
0.00006
0.00006
0.00006

0.00006
9.55056
0.00006
0.00006
0.00006
0.00006
1.1236
0.00006
0.00006
0.00006

0.00006
1.1236
0.00006
0.00006
0.00006
0.00006
0.00006
0.00006
0.00006
0.00006

0.00006
5.05618
0.00006
0.00006
0.00006
0.00006
0.00006
0.00006
0.00006
0.00006

0.00006
0.00006
0.00006
0.00006
0.00006
0.00006
0.00006
0.00006
0.00006
0.00006

0.00006
0.00006
0.00006
0.00006
0.00006
0.00006
0.00006
0.00006
0.00006
0.00006

0.00006
1.1236
0.00006
0.00006
0.00006
0.00006
0.00006
0.00006
0.00006
0.00006

0.00006
0.00006
0.00006
0.00006
0.00006
0.00006
0.00006
0.00006
0.00006
0.00006

(1)

0.00006
21.34831
0.00006
0.00006
0.00006
0.00006
0.5618
0.00006
4.49438
0.00006



















6.2 Simulation with no drift

To verify that the statistical guarantees are satisﬁed, we generate r = 500
repetitions of n = 1, 000 draws for each simulation. In the ﬁrst, we set πt = 0, ∀t.
We will test the detection with conﬁdence thresholds of α = {0.10, 0.05, 0.01}.
As noted in Section 5.1, the drift detection thresholds for the BF are 1/α.
In Figure 2, we show the natural logs of BF for each repetition (each in a
diﬀerent color), as well as the log thresholds as horizontal black dashed lines at
2.3026, 2.9957, 4.6052. The ﬁrst row of Table 1, shown in Section 6.3, shows
the false alarm rate is controlled, since at each α, the proportion of simulations
in which drift is (falsely) detected is approximately α.

11

Figure 3: Log BF for r = 500 repetitions of n = 1, 000 random draws from H∗ =
(1 − π)H + πH(cid:48) (no drift), for various values of π > 0. The detection success
should increase with π. Horizontal dashed lines show detection thresholds of
the BF at values α = {0.1, 0.05, 0.01}, from bottom to top.

6.3 Simulation with drift

Next, we simulate the same number of repetitions and time steps for each mixing
proportion π = {0.05, 0.10, 0.20, 0.30} as in Section 6.1. Again, the log BF and
thresholds are plotted. Since drift is detected for a repetition if the BF ever
passes the respective threshold, we see clearly that as π increases, the bulk of
the lines pass the thresholds at each α; at each α, this proportion should be
greater than α, and increase with π, as shown in Table 1.

6.4 Contribution of categories to drift detection

The analyses described in the following will try to determine and which cate-
gories (e.g., pairs of APIs) are responsible for the diﬀerence between the observed
and baseline distributions, and to measure either the eﬀect on the drift decision
directly, or to measure the degree of divergence from the expected frequency,
either more or less than expected. One application of drift detection to APIs
is to testing of systems. For instance, a test can be run to simulate various
scenarios of sets of APIs and their interactions. Through drift testing, we can

12

Drift proportion π

Threshold α

0.10

0.05

0.01

0.0 (no drift)

0.096

0.044

0.014

0.05

0.19

0.15

0.108

0.10

0.554

0.508

0.438

0.20

0.946

0.936

0.922

0.30

0.998

0.998

0.996

Table 1: Proportion of simulation repetitions where drift is detected. When
π > 0, this should be the correct decision. When π = 0, the proportions should
be approximately α (the desired false positive rate), showing that the false
positive rate is controlled.

see if the test has generated a diﬀerent distribution pattern of API calls (in
terms of relative frequencies) than the baseline; this can help assess if the test
has generated the expected results. If a test is known to, for instance, tend to
increase some API pair frequency, we can see if the measured divergence accords
with this anticipated change.

For an observation zt, let the operator ν(t) ∈ {1, . . . , K} denote the index
of the category labels {(cid:96)1, . . . , (cid:96)K} observed at time t by observation zt. So if
zt = (cid:96)i, ν(t) = i, so zt = (cid:96)ν(t).

In [4] (Appendix A, equation 25), the authors show that the BF at t can
be calculated recursively based on the BF at t − 1, taking into account the
new observation. In their notation, xt,i = I(zt = (cid:96)i), where I is the indicator
function; that is, xt,ν(t) = 1, while xt,i = 0 for categories (cid:96)i not observed. Note
that the posterior α is updated only for the category observed, so if i = ν(t),
αt,i = αt−1,i + 1, otherwise αt,i = αt−1,i. Simplifying the multiplier of the BF
at t − 1 (in their notation, On−1(θ0)) using the facts above, it can be written as

(cid:32)

ψt =

Γ((cid:80)K

i=1 αt−1,i)
j=1 αt−1,j) + 1)

Γ(((cid:80)K

(cid:33) (cid:32) Γ(αt−1,ν(t) + 1) (cid:81)

i(cid:54)=ν(t) Γ(αt−1,i)

(cid:33) (cid:18) 1

(cid:19)

Γ(αt−1,ν(t)) (cid:81)

j(cid:54)=ν(t) Γ(αt−1,j)

θ0,ν(t)

where Γ denotes the gamma function, and θ0,i is the null hypothesis value
. By cancellation and using the fact that Γ(a + 1) = aΓ(a),

of Pr((cid:96)i) = α0,i
j α0,j
we further have

(cid:80)

ψt =

αt−1,ν(t)
(cid:80)K

θ0,ν(t)

i=1 αt−1,i

=

αt−1,ν(t)/((cid:80)K
θ0,ν(t)

i=1 αt−1,i)

=

posterior probability of (cid:96)ν(t)
prior probability of (cid:96)ν(t)

Thus the BF at time t is the BF at t − 1 multiplied by ψt. Because of this
recursive multiplication, if we deﬁne φ0 as the prior odds, we can say that the

13

BF at time t is (cid:81)t
j=0 ψj. The denominator of ψt is the value of αt−1,ν(t) if the
null prior hypothesized probability θ0,ν(t) for observed category zt = (cid:96)ν(t) was
correct; the denominator is the actual posterior value. If ψt > 1, it means the
posterior has given more likelihood to (cid:96)ν(t) than the null (i.e., prior), which will
increase the previous time’s BF; the reverse is true if ψt < 1.

Since ln (a) = − ln (1/a), a > 0, we can consider ln (ψt) as the additive eﬀect
of zt in contributing to the BF. Only cases where the BF is large (> 1/α), that
is, when we tend to see ψt > 1, contribute to the drift detection decision. This
happens when we see categories (cid:96)i happen more often than expected, but this
also requires other categories to happen less often than expected. Thus, if drift
is detected and we stop observing at that point (i.e., the last BF value we have
is the one passing the threshold), we can consider an overall measure of the
inﬂuence of category (cid:96)i in the decision to detect drift as

∆(i) =

(cid:88)

ln (ψt) =

(cid:88)

ln (ψt)

t : ν(t)=i

t : zt=(cid:96)i

Since, by deﬁnition, categories (cid:96)i which happen more often will have ∆(i)
summed over more indices t, the overly-frequent categories will tend to have high
∆(i). However, it would be possible for a category to, say receive a high ∆(i)
for appearing still frequently, but less frequently than expected. Categories
(cid:96)i which happen around as frequently as expected under the prior, whether
frequent or not, will tend to have ψt ≈ 1, which makes their ∆(i) tend to be
close to 0. Recall that the order of observations matter, so temporal instability
in the probability of a category (cid:96)i occurring—whether by randomness, though
we assume the probabilities are iid, or representing an actual shift—can aﬀect
the detection of drift, even if on average, the category was observed at the prior
expected rate.

One weakness of the ∆ metric above is that categories (cid:96)i that are unob-
served, even though they may have received high prior weight, are not counted.
Therefore, we also below consider a log ratio metric ρ(i), as follows. Let F and
F(cid:48) be the prior (expected) and observed (posterior minus prior) and frequency
matrices, respectively, with entries summing to n0 and n1; zero-valued entries
in F(cid:48) are replaced with the same minimum constant c from the prior (see Sec-
tion 6.1). Let Fi and F (cid:48)
i be frequency values corresponding to a given category
(cid:96)i, and deﬁne a metric

ρ(i) = ln

(cid:18) max(F (cid:48)

i , 0.5)/n1
max(Fi, 0.5)/n0

(cid:19)

Since unseen categories in the prior or observations have Fi or F (cid:48)
i < 1, ratios
F (cid:48)
can be unstable if one of them is unseen. In the logarithm, we thus set them to
i
Fi
have a minimum value of 0.5 so that, say, a category that was unseen in one but
seen only once in the other would have a score of ρ(i) = ± ln(2) ≈ ±0.693, rather
than much higher, to avoid giving too anomalous scores to these low-frequency
categories. Categories (cid:96)i where ρ(i) > 0 are observed more often than expected
under the prior; the opposite is true for negative values. Thus, unlike under the

14

Figure 4: Contribution ∆(i) for various API pairs (cid:96)i ∈ C.

∆ metric, categories (cid:96)i that are not observed in one side (Fi or F (cid:48)
i ≈ 0) can
still have high (negative) anomalousness if their expected frequency Fi is high
enough. The ρ metric measures anomalousness in the ﬁnal frequencies without
regard to the order, unlike ∆, which sums the ψt anomalous scores according
to the sequence APIs were observed.

To demonstrate, we now run a single simulation with π = 0.20 and decision
threshold α = 0.01. This particular run detected drift after t = 559 obser-
vations. The resulting contribution scores {∆(i)}K
i=1 for API pairs are shown
in the left plot of Figure 4. Any grid square that is visibly shaded is con-
tributing to the decision; darker hues mean higher contribution, and blue/red
color indicates positive/negative value of the contribution ∆(i). The right plot
shows the top 3 pairs, ordered by |∆(i)|, which can be viewed as an anomaly
score, regardless of the sign of ∆(i). The third most anomalous category, (fron-
tend, recommendationservice), was observed only 2 times, but was judged to be
highly anomalous because it received the minimum weight under the prior, since
it was not observed under the baseline. Its prior probability is θ0,i, the prior
value α0,i ∈ α0 when divided by the sum of elements in α0; under the prior,
out of t = 559 draws, it would be expected to be observed with essentially 0
frequency (0.00062 ≈ 559θ0,i). The observed and expected frequencies for these
pairs are shown in Table 2

Category (cid:96)i
(frontend, currencyservice
(frontend, cartservice)
(frontend, recommendationservice)

Observed count (F (cid:48)
128
48
3

i ) Expected count (Fi)
106.77528
56.52809
0.00062

Table 2: Observed and expected counts for top 3 anomalous API pairs, by ∆
metric.

Similarly, we can rank individual parent (a) and child (b) APIs, where a, b ∈

{∅} (cid:83) A, in terms of anomalousness, by summing

(cid:88)

|∆(i)|

or

(cid:88)

|∆(i)|

i : a is the parent in (cid:96)i

i : b is the child in (cid:96)i

15

Figure 5: Contribution ∆ for various parent/child APIs for API pairs in C.

Figure 6: Anomalousness ρ(i) for various API pairs (cid:96)i ∈ C.

respectively. These are shown in Figure 5; the same calculation can be done
for ρ by substituting it for ∆ in the equation. The frontend API was the
most anomalous parent, for instance, because many API pairs having it as a
parent (Figure 4, left plot, ‘frontend’ row) were anomalous; in addition, its
anomalousness is due both to child APIs that were called either more (blue)
or less (red) often than expected, as shown the the relative lengths of blue/red
bars.

Noting the drawback of the ∆ metric above, we show the results of the
ρ metric on the same simulation, which can also measure anomalousness on
potential categories (cid:96)i which were not observed, and hence their absence is
anomalous if they were expected. These pairwise results are shown in Figure 6.
In the left plot, we see that one pair, (frontend, recommendationservice), turns
out similarly anomalous to the results in the ∆ metric (Figure 4), in that it
was more frequent than expected. The observed and expected values for the
top 3 pairs are shown in Table 3. The ρ metric, since it is calculated on ratios,
seems to emphasize diﬀerences in the relatively infrequent pairs rather than in
the more frequent ones in the ∆ metric. Figure 7 shows similar results summed
across parent and child APIs.

Though the ∆ metric has some drawbacks as compared to the ρ metric,
we note in Section 8 that we may investigate modeling the length of the clock
time gaps between pairs of APIs; for instance, the time gap to the next call
may depend on the pair just called. In this case, with a Bayesian model, this
information can be incorporated into the likelihood function, and thus in to the

16

Category (cid:96)i
(frontend, recommendationservice)
(checkoutservice, currencyservice)
(checkoutservice, productcatalogservice)

Observed count (F (cid:48)
i ) Expected count (Fi)
0.00062
3
12.56180
4
6.28090
3

Table 3: Observed and expected counts for top 3 anomalous API pairs, by ρ
metric.

Figure 7: Anomalousness ρ for various parent/child APIs for API pairs in C.

BF. The ∆ metric would thus be able to reﬂect this, but the ρ metric measures
only the deviation between frequencies. A naive approach would be to weight
the occurrence of each category (cid:96)ν(t) by, say, the average of time gaps between
it and the preceding and following calls zt−1 and zt+1, which would allow the
use of frequency-based metrics like ρ. However, particularly if API calls are
irregularly spaced in time, it is likely incorrect to simply give higher weight to
a call because it is more isolated in time. Therefore, if the model considers
aspects other than the relative frequency distribution of APIs (or associated
hyper-parameters), such as the time gaps, the ∆ metric could likely be adapted,
since it depends on the likelihood functions, but the ρ metric, since it depends
on frequencies, may no longer be appropriate.

7

Improving detection time by weighting past
observations

By their nature, Bayesian posteriors should become more certain over time. For
instance, in the test above, if the observed data is very similar to the prior (e.g.,
if πt ≈ 0), the data will reinforce the prior, and the variance of the posterior will
decrease. This is true for the Dirichlet distribution since, as noted, the larger the
sum of the α, the lower the variance, assuming the mean values of αi’s stay the
same. This means that if a long period of non-drift occurs (i.e., if drift begins
at ts >> 1), it will be more diﬃcult to detect drift if it then occurs, since the
posterior has become ‘entrenched’ into becoming more and more ‘tight’ around
the baseline. ‘More diﬃcult’ in this case means that to cause the BF to exceed
the threshold 1/α, we need to observe more instances of drift to detect it than if

17

the drift had started immediately (ts closer to 1), due to the increased certainty
(tightness) of the posterior. Since we want to be ﬂexible about detecting drift
whenever it occurs, this suggests we should make the posterior updates reﬂect
the most recent data more than old data.

There are several approaches to this. One is to add a forgetting constant
0 < w < 1 when updating. This approach, mentioned in [6] (page 6, under
equation 4), can be applied. In their example, a binary-valued variable (with
values coded as ‘success’ or ‘failure’) is modeled as Bernoulli-distributed, where
the relevant parameter ρ ∈ [0, 1] is the ‘success’ probability on each draw.
ρ is modeled as having a beta-distributed prior (which has the same domain
[0, 1], as required). The beta distribution is updated by incrementing one of
its parameters each time by 1, depending on whether the draw (rj) is a success
or failure; this is the same as the Dirichlet posterior updates in our case (see
Section 6.4), since the beta distribution is the same as the Dirichlet when the
number of categories K = 2 (i.e., success or failure).

In [6], the past observations (past total counts of success or failure) are
given a weight of w ∈ (0, 1] when updating the posterior of ρ.
If w = 1,
this is the standard posterior update with no forgetting, in that the expected
value of ρ is the fraction of successes out of the total number of draws.
If
w < 1, the procedure has forgetting, in that the past observations are given
less weight (in proportion to w) than the newest observations in the update.
1
1−w , so if w = 1 this is inﬁnite memory, since all past
The eﬃcient memory is
observations receive the same weight in the update equation. In experiments we
have performed, a constant w < 1 did not appear to signiﬁcantly reduce delay
of drift detection within a reasonable time window, when the start ts of drift
was large, where the results are essentially the same as with no forgetting (the
standard w = 1).

A similar, but more sophisticated, approach is illustrated in [5], which sim-
ilarly uses a forgetting factor w but learns it dynamically rather than having
it be ﬁxed as in [6]. Here, a model called a Hierarchical Adaptive Forgetting
Variational Filter (HAFVF) has w modeled by a beta distribution (which has
domain [0, 1], like w). At each point, a weighted sum of the initial prior and the
current raw posterior (receiving weights 1 − w and w, respectively) is formed
to represent the current best estimate of the parameter of interest (in our case,
the vector parameter α). w is dynamic; when the newest observed data ap-
pears to be signiﬁcantly diﬀerent than the prior (i.e., drift), w increases to shift
more weight to the raw posterior in the weighted update. In simulations with
repeated back-and-forth drift/no drift, the authors show the the weighted pos-
terior adjusts quickly to reﬂect the distribution of the most recent observations
(e.g., the drift distribution H(cid:48)), regardless of the time index t. That is, the
ability to adapt is not aﬀected by the delay ts, as it is in the constant forgetting
factor.

18

8 Future work

In future work, we will attempt several extensions of the work shown here.
First, we intend to adapt the HAFVF (Section 7) for the multinomial-Dirichlet
setup so drift that does not begin immediately can be detected with shorter
delay. Second, we may consider modeling API calls by a hierarchical model,
for instance not just the API names but also specifying parametric models for
API hyper-parameters—particularly if they can be discretized to multinomial
to avoid assuming a particular continuous parametric distribution—or modeling
the clock time gaps between calls. A likelihood function can be calculated by
assuming, say, independence between hyper-parameters. However, this would
make the model more complex; it may be particularly diﬃcult to adapt the
HAFVF calculations to this scenario. Also, we intend to investigate further the
particulars of the ∆ and ρ anomalousness metrics outlined in Section 6.

Furthermore, since in practice, APIs ∈ A need not necessarily be combined
only in pairs, but potentially in ordered triplets or more, the method can be
extended. In simulations where we specify a prior, we can consider all potential
permutations, or some constrained subset of them, of elements in A, up to
some maximum, give each of them a unique label in (cid:96), and assign unobserved
combinations a small positive prior constant value. This prior value may be
need to be adjusted relative to the length of the combination or the relative
likelihoods of seeing combinations of a given length. However, the notion of
an API being a parent or child, as in the anomaly scoring in Section 6.4, will
need to be generalized from the pairwise formulation, where there are only two
positions. Perhaps, for instance, this summation can be done for an API a ∈ A
regardless of its position in the API combination sequence, or perhaps with
some kind of importance weighting.

9 Conclusion

In this work we have illustrated an application of an existing method for se-
quential detection of changes in the distribution of observed categories, to the
problem of detecting a change in the patterns of API calls. The method pro-
vides a statistical guarantee on the conﬁdence of its decision that distribution
change (drift) has resulted. We also presented several metrics to explain to the
user which APIs were most anomalous, in terms of being observed much more
or less frequently than expected, relative to a baseline.

References

[1] Samuel Ackerman et al. “Detection of data drift and outliers aﬀecting ma-
chine learning model performance over time”. In: (2020), pp. 144–160. url:
https://arxiv.org/abs/2012.09258.

[2] Lingjiao Chen et al. Did the Model Change? Eﬃciently Assessing Machine

Learning API Shifts. 2021. arXiv: 2107.14203 [stat.ML].

19

[3] Daniele Cono D’Elia et al. Designing Robust API Monitoring Solutions.

2021. arXiv: 2005.00323 [cs.CR].

[4] Michael Lindon and Alan Malek. Sequential Testing of Multinomial Hy-
potheses with Applications to Detecting Implementation Errors and Missing
Data in Randomized Experiments. 2020. arXiv: 2011.03567 [stat.ME].

[5] Vincent Moens. “The Hierarchical Adaptive Forgetting Variational Filter”.
In: Proceedings of the 35th International Conference on Machine Learning
(2018). url: http://proceedings.mlr.press/v80/moens18a/moens18a.
pdf.

[6] Vincent Moens and Alexandre Zenon. “Learning and forgetting using rein-
forced Bayesian change detection”. In: PLoS Computational Biology 15.4
(2019). doi: 10.1371/journal.pcbi.1006713. url: https://doi.org/
10.1371/journal.pcbi.1006713.

[7] Mojtaba Shahin, Muhammad Ali Babar, and Liming Zhu. “Continuous In-
tegration, Delivery and Deployment: A Systematic Review on Approaches,
Tools, Challenges and Practices”. In: CoRR abs/1703.07019 (2017). arXiv:
1703.07019. url: http://arxiv.org/abs/1703.07019.

[8] Muhammad Waseem et al. Design, Monitoring, and Testing of Microser-
vices Systems: The Practitioners’ Perspective. 2021. arXiv: 2108 . 03384
[cs.SE].

[9] Markos Zachariadis and Pinar Ozcan. “The API Economy and Digital
Transformation in Financial Services: The Case of Open Banking”. In:
SSRN Electronic Journal (Jan. 2016). doi: 10.2139/ssrn.2975199.

20

