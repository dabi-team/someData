AUTOMATEDQUANTUMPROGRAMMINGVIAREINFORCEMENTLEARNINGFORCOMBINATORIALOPTIMIZATIONKeriA.McKiernan†StanfordUniversityStanford,CA94306kmckiern@stanford.eduErikDavisRigettiComputingBerkeley,CA94710erik@rigetti.comM.SohaibAlamRigettiComputingBerkeley,CA94710sohaib@rigetti.comChadRigettiRigettiComputingBerkeley,CA94710chad@rigetti.comAugust23,2019ABSTRACTWedevelopageneralmethodforincentive-basedprogrammingofhybridquantum-classicalcom-putingsystemsusingreinforcementlearning,andapplythistosolvecombinatorialoptimizationproblemsonbothsimulatedandrealgate-basedquantumcomputers.Relativetoasetofrandomlygeneratedprobleminstances,agentstrainedthroughreinforcementlearningtechniquesarecapableofproducingshortquantumprogramswhichgeneratehighqualitysolutionsonbothtypesofquantumresources.Weobservegeneralizationtoproblemsoutsideofthetrainingset,aswellasgeneralizationfromthesimulatedquantumresourcetothephysicalquantumresource.1IntroductionOneoftheearliestambitionsofartiﬁcialintelligenceresearchwastoconsiderwaysofmechanizingthetaskofcomputerprogrammingitself,viatheautomatedsynthesisofprogramsfromhighlevelspeciﬁcations.Thereisrichliteratureonsuchtechniques,includingarangeofmeta-heuristic[33,8]andmachinelearningapproaches[22](cf.[20]forarecent,broadsurveyonprogramsynthesis).Suchideasarepotentiallycompellingfortheprogrammingofquantumcomputers,duetoboththeunintuitivenatureofthesedevicesaswellastheuniquechallengespresentedbynear-termhardware.Tothisend,weexplorereinforcementlearningforautomatedprogramsynthesisinthecontextofahybridclassical-quantumcomputingarchitecture,consideringbothsimulatedandphysicalgate-basedquantumcomputers.Weexploretheapplicationofthisframeworktosolvearangeofcombinatorialoptimizationproblems(COPs).Thisclassofproblemsisofparticularinterestinthequantumcomputingdomainduetotheemergenceofnewquantumheuristicsforbothadiabatic[14]andgate-model[21,17]quantumcomputing.Fromanapplicationperspective,theseoptimizationproblemsareubiquitousandofveryhighvaluetomanyprocessesinindustry.Reinforcementlearningtechniquesappliedtoquantumcomputationhavefoundsuccessinarangeofcontexts,includingquantumerrorcorrection[18],aswellasnoisycontrolforgatedesign[2,32]andstatepreparation[11,10,5,1,39].HereweutilizereinforcementlearningdirectlyattheapplicationleveltosolveCOPprogrammingtasks.Combinatorialoptimizationhasproventobeapopulartargetdomainformachinelearningmethods.Thisworkdatesbacktoatleastthelastmachinelearningcycleofthe1980sand1990s,whereHopﬁeldnetworkswereusedtomodelavarietyofproblemtypes[35].Morerecently,stateofthearttechniquessuchasrecurrentencoder/decodernetworks[38,6],graphembeddings[24],andattentionmechanisms[28]havebeenusedtosolvearangeofCOPs.Notethat,intheseapplications,machinelearninghasbeenusedeitherend-to-endtomapdirectlyfromaprobleminstancetoasolution,oralternativelyasasubroutineofanalreadyexistingheuristic.Foracomprehensivediscussionofthisintersectionofdomains,seeRef.[7].Inwhatfollows,wedetailthedesignconsiderationsinvolvedindeﬁninganeffectivereinforcementlearningenvironment,withparticularemphasisonthedeﬁnitionofthestatespace,actionspace,andrewardfunction.Wesubsequentlyapplythisframeworktotrainreinforcementlearningagentstogeneratequantumprogramssolvingcombinatorialoptimizationproblemsonbothsimulatedandrealquantumresources.Relativetoatestsetofrandomlygeneratedprobleminstances,arXiv:1908.08054v1  [quant-ph]  21 Aug 2019weobservethatthemeanperformanceofthetrainedagentsexceedstheperformanceofbothuntrainedagentsaswellasthatoftheleadingnear-termhybridquantumalgorithmtypicallyusedtosolvecombinatorialoptimizationproblems,thequantumapproximateoptimizationalgorithm(QAOA)[17].Followingthis,webrieﬂyanalyzeanddiscussagentstrategy,considerlimitationsofthecurrentwork,andnotepromisingavenuesforward.2LearningenvironmentInthissectionwebroadlyidentifytheaspectsofthehybridclassical/quantumenvironmentrelevanttoreinforcementlearning.Inparticular,weindicatethestatespaceandobservationsofit,theavailableactions,thereward,andthelearningagent.ThesespeciﬁcationshavebeenimplementedusingthestandardenvironmentAPIproposedbyOpenAIGym[9].Thisimplementationwasusedtoconductallexperimentscontainedinthisstudy.Atahighlevel,theframeworkweproposeisasfollows.Areinforcementlearningagent,executingonaclassicalcomputingresource,incrementallyproducesaquantumprogramforexecutiononaquantumresource,withthegoalofpreparingaquantumstatewhichservestosolvesomeposedproblem.Intheexamplesweconsider,theproblemmaybedescribedbythespeciﬁcationofa‘probleminstance’(e.g.aweightedgraph)anda‘rewardfunction’(detailedbelow).ThisprocessisillustratedinFig.1.Attheoutset,theproblemrewardfunction,andgraphtobeevaluatedinthecontextofthisrewardfunctionarespeciﬁed.Giventhisinformation,theagentproducesaquantumprogrambyiteratingthefollowingsteps:i)theagentchoosesfromsomeavailablequantumgatestoappendtothecurrentprogram,ii)theupdatedprogramisevaluatedonaquantumresource,perhapsseveraltimes,andiii)theresultsoftheseevaluationsareusedtocomputearewardandan‘observation’.Theresultsofstep(iii)aresubsequentlyusedinthedecisioncriteriaofstep(i)ofthenextiteration,andsoon.Thisinteractionbetweentheagentandquantumresourceisrepeateduntiltheagent‘wins’,whereintherewardexceedssomethreshold,or‘loses’,whereintheprogramlengthexceedssomethreshold.Figure1:Interactionofalearningagentwithaquantumresourcefortheautomatedgenerationofreward-speciﬁcquantumprograms.Thequantumresourcemaybeeitheraquantumsimulationoraphysicalquantumprocessor.Thewin/losecriteriamustbedeﬁnedintheenvironment.Inthecaseofcombinatorialoptimizationproblems,onemaynaturallyidentifytherewardofaquantumstate|ψiasthecostHamiltonian’sexpectationvalue.Moregenerally,anymonotonicfunctionoftheexpectationservesasanadequatesurrogate.Fortheexperimentswhichfollow,wehavefounditconvenienttorescalethecostHamiltonian’sexpectationtotakevaluesintherange0to1.WeidentifytheactionspaceAasaﬁnitesetofquantumgates,suchasadiscretizedsetofRZandRYrotationgates.Fortheagent,wefocusexclusivelyonthePPO(ProximalPolicyOptimization,cf.[34])algorithm,appliedtoasharedactor-criticarchitecture.AreinforcementlearningproblemisformallyspeciﬁedasaMarkovDecisionProcess(MDP),forwhichthegoalofthelearningagentistoﬁndtheoptimalpolicy,i.e.theconditionalprobabilityπ∗(a|s)ofapplyingaparticularquantumgate(actiona)givenaparticularrepresentationofthequbitregister(states)thatwouldmaximizetheexpected(discounted)returnEπ[P∞k=0γkrk+1],withoutnecessarilyhavingamodeloftheenvironmentp(s0,r|s,a).DeﬁningthevalueofastatesunderapolicyπasVπ(s)=Eπ"∞Xk=0γkrt+k+1(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)St=s#(1)2weidentifytheoptimalpolicyabitmoreconcretelyasπ∗suchthatVπ∗(s)≥Vπ(s)foralls∈Sandallpoliciesπ.InpracticePPOwillﬁndsomeapproximationtothetheoreticaloptimumasafunctionofsomeparametersπ∗(a|s;θ)≈π∗(a|s)whichitwilltunetowardstheoptimumduringthelearningprocess.TheproblemweconsiderhereismorenaturallymodeledasapartiallyobservedMarkovDecisionProcess(POMDP),sincequantumstatesarenotthemselvesdirectlyobservable,andonlytheirmeasurementoutcomesare.Whiletheaction(quantumgate)thattheagentchoosestocarryoutdeterministicallyevolvesthequantumstate(intheabsenceofnoise),theobservationitreceivesfromthemeasurementsamplesareingeneralnotdeterministic.ForasingleCOPinstance,theobservationrepresentationthattheagentreceivesfromtheenvironmentisgivenbyaspeciﬁcationofthesampledbitstringswereceivefromNshotsmeasurements,givenaparticularsequenceofactionsandaﬁxedstartingstatessamples:|ψi→[N0,...,N2n−1](2)suchthatP2n−1i=0Ni=Nshots.NotethatNi/Nshots≈|αi|2,whereαidescribesthecomplexamplitudeofthecomputationalbasisstate|ii.InordertoextractquantumcircuitsfromthetrainedagentonunseenprobleminstancesofaCOP,weaugmentthestatespacewitharepresentationoftheCOPprobleminstanceitself.Forexample,inthecaseofMAXCUT(seebelow),partofthestatedescriptionisthegraphwhosemaximumcutweseek.WetraintheagentoveracollectionofseveralsuchCOPinstances,formingthetrainingset,andtestitspredictionsagainstacollectionofsimilarbutdifferentCOPinstancesthattheagenthasnotseenbefore.3QuantumheuristicforcombinatorialoptimizationInthissection,weﬁrstoutlinetheclassesofcombinatorialoptimizationproblemswhichweconsider.Followingthis,wedescribeseveralexperimentsonsimulatedandrealquantumhardware.3.1SurveyedproblemsWeconsiderthreeproblemsofincreasinggenerality.LetG=(V,E)beaweightedgraphwithverticesVandedgesE.ForconvenienceweassumeV={1,...,n},withweightsspeciﬁedbyann×nrealsymmetricmatrixw,wheretheweightwij≥0isnonzeroifthereisanedgebetweenverticesiandj.ThemaximumcutproblemasksforapartitionofVintotwosubsetssuchthatthetotaledgeweightbetweenthemismaximized.Formally,MAXCUTmaximizez∈{−1,1}n12Xi,jwij1−zizj2.ThisproblemisknowntobeNP-hard[23],althoughanumberofpolynomial-timeapproximationalgorithmsexist.Inthisregard,itisknowntobeNP-hardtoapproximateMAXCUTwithratiobetterthan16/17([4]).Thebestknownapproximationratioof0.87856isgivenbythesemideﬁniteprogrammingalgorithmofGoemansandWilliamson[19].IftheUniqueGamesConjectureholds,thisratioisoptimal[25].NotethatsolvingMAXCUTisequivalenttomaximizingtheslightlysimpliﬁedexpressionPi<j(−wij)zizj,wherethecoefﬁcients(−wij)arealwaysnegative.Anaturalgeneralizationistoallowmixedsigns.Theresultingproblem,alsoNP-hard,isMAXQPmaximizez∈{−1,1}nXi,jwijzizjwherewisarealsymmetricmatrixwithnulldiagonalentries.ItcanbeshownthattheoptimalvalueinMAXQPisnon-negative,andarandomizedΩ(1/logn)-approximationalgorithmisgivenin[13](seealso[31],[29],[27]).ItisNP-hardtoapproximatewitharatiobetterthan11/13,andquasi-NP-hardtoapproximatewitharatiobetterthanΘ(1/logγn)foraconstantγ>0[3].Althoughofgreattheoreticalinterest,MAXCUTandMAXQPdonotnecessarilyofferthemostconvenientforminwhichtoembedpracticalproblems.Inthisregard,onemayconsideraslightgeneralizationofMAXQPwherethe3quadraticisaugmentedwithanafﬁneterm.Inkeepingwiththeconventionsoftheliterature,weposethisastheQUBO("quadraticunconstrainedbinaryoptimization")problem,givenbyQUBOmaximizex∈{0,1}nXi,jwijxixjwherewisarealsymmetricn×nmatrix.TheaboveformulationissometimesabbreviatedasUQBP(“unconstrainedquadraticbinaryprogram"),see[26]forabroadsurvey,and[16]forarecentstudyoftheempiricalperformanceofvariousheuristicalgorithms.Weremarkthatunderatransformationzi=1−2xiwemayembedinstancesofMAXCUTandMAXQPasinstancesofQUBO.Inthecontextofquantumcomputingitisstandardtoformulatetheaboveoptimizationproblemsinthelanguageofoperatorsandexpectationvalues,sothattaskistomaximizetheexpectationofacertain‘problemHamiltonian’withrespecttoan-qubitquantumstate.Whenthecostfunctionisexpressedasapolynomialin{−1,+1}variables(e.g.asinPi,jwijzizj),thecorrespondingHamiltonianfollowsimmediately(e.g.Pi,jwijZi⊗ZjwhereZi⊗ZjdenotesthetensorproductofPauliZ-operatorsactingontheithandjthqubitsrespectively).3.2ExperimentsHavingﬁxedtheidentiﬁcationofthestate,observation,actionspace,reward,andlearningagent,weapplytheaforementionedframeworkinordertosolverandominstancesoftheabovethreeproblems.Inallcaseswetakethenumberofvariablesn=10ﬁxed.ForMAXCUT,weconsiderErd˝os-Renyigraphswithedgeprobability0.5.Ifthereisanedgefromvertexitovertexj,wechooseanindependentweightwij∼Uniform(0,1).Otherwise,wij=0.ForMAXQP,weconsiderrandommatriceswformedbylettingwij∼Uniform(−1,1)ifi<j,wii=0,andimposingthatwbesymmetric.ForQUBO,weconsiderrandommatriceswformedbylettingwij∼Uniform(−1,1)fori≤j,andimposingthatwissymmetric.Foreachofthethreeproblems,wegenerated50,000randominstances,whichwerethenshufﬂedintoasingledatasetfortraining.Additionalindependentvalidationandtestsetswerecreatedforeachproblemtype.Intotal,thevalidationsetcontained12000uniqueinstances,whilethetestsetcontained3000uniqueinstances.Followingamodestamountofhyperparameteroptimization(furtherdetailedinthesupplementaryinformation),wetrainedaPPOagentforapproximately1,700,000episodes(foratotalofapproximately20millionindividualsteps)onaquantumsimulator(Rigetti’s“QuantumVirtualMachine",henceforthabbreviatedasQVM[36]).Attheendofthetraining,weselectedthemodelwiththebestperformanceonthevalidationset,asindicatedbythehighestaverageepisodescore(seebelow).Werefertothismodelasthe‘QVM-trained’agent.TheQVM-trainedagentwasthenusedtoinitializetrainingofthe‘QPU-trained‘agentontheRigettiAspenQPU[15,12,30].Thismodelwastrainedfor150,000episodes.Wealsorunp=1(single-step)QAOA[17]oneachofthetestsetsasabenchmark(seethesupplementaryinformationformoreinformation).WechooseQAOAsinceitisawidelyknownandfairlygenericalgorithmforsolvingcombinatorialoptimizationproblemsonquantumcomputers.AlthoughQAOAgenerallyperformsbetterthelargerthevalueofp,orthenumberofalternatingstepsinthealgorithm,wefocusonp=1sinceforthe10-variableproblemsunderconsiderationthislimitstheprogramlengthtoapproximately100.Alargernumberofstepswouldcausetheoutputtohavesigniﬁcantcontributionsfromnoise,whichwewouldliketoavoidasacomparativebenchmark.Weadditionallybenchmarkperformancewithrespecttoan‘untrained’agent.Thisagentissubjecttothesameenvironmentasthetrainedagent,buthasnotbeenoptimizedthroughatrainingprocess.Thismeansthat,forallpossibleobservations,theuntrainedagentsamplesfromtheactionsetwithuniformprobability.Withrespecttoaﬁxedprobleminstanceandaﬁxedpolicy,onemayproduceanepisode,describedbyasequences1,a1,r1,...,sm,am,rmofstate-action-rewardtriplets.TherewardforasingleactionisformallytheexpectationvalueoftheproblemHamiltonianinthestateproducedbytheprogramsequencethusfar,normalizedtobeintherange0to1,i.e.ri=(hψi|HC|ψii−m)/(M−m)where|ψiiisthestatepreparedbytheﬁrstiactions,andm=minψhψ|HC|ψiandM=maxψhψ|HC|ψidenotetheminimumandmaximumattainableexpectationvalues.Weremarkthatthisnormalizationisconvenientforoursubsequentdataanalysis,andenablesthepossibilityofearlystoppingintraining,butisnotstrictlyspeakingnecessaryfortheuseofourproposedmethod(andrightfullyso,asthemaximumvalueusedinthenormalizationispreciselywhatwesetouttocompute).Onaphysicaldevice,therewardriisestimatedbyrepeatedpreparationandmeasurementforsomenumberofmeasurementshots(cf.thesupplementaryinformation).4Figure2:Distributionofepisodescores,withrespecttothetestdataset,foreachCOPonthequantumsimulator(top),andonthequantumprocessor(bottom),andfortheQVM-trainedagent(darkgray),theuntrainedagent(red),andtheQAOA(blue).Overall,theQVM-trainedagentyieldsthehighestexpectedtestperformanceforallproblemsonboththesimulatedandphysicalquantumresources.Figure3:ProgramlengthcorrespondingtotheepisodescoresgiveninFig.2foreachCOPonthequantumsimulator(top),andonthequantumprocessor(bottom)fortheQVM-trainedagent(darkgray),theuntrainedagent(red),andtheQAOA(blue).Throughtraining,onbothresources,shorterprogramsaregeneratedbytheagent.5Figure4:TestperformanceforeachCOPforboththeQVM-trainedagent(lightgray)andtheQPU-trainedagent(darkgray)thequantumprocessor(top),andthecorrespondingnumberofcompiledinstructionstoreacheachtestepisodescore,foreachagent,onthequantumprocessor(bottom).ThroughtrainingontheQPU,thetestscoredistributionsyieldsimilarempiricalcharacteristics,showingcomparableexpectationandvariancevalues.However,thenumberofinstructionstoreachthesescoresdecreasedoverthecourseoftheQPUtrainingprocess.Withrespecttoacompleteepisode,theepisodescoremetricisgivenbyESagent:=max1≤i≤mri.FortheQAOAansatz,whichisintendedtoruntocompletion,theQAOAepisodescoreisgivenbyESQAOA:=rm,wheremrepresentstheﬁnalansatzinstruction.Comparisonoftheperformanceonthetestproblems,foreachCOP,foreachsurveyedmethod,oneachquantumresourceisillustratedinFig.2.Ineachcircumstance,theQVM-trainedagentproducesascoredistributionwiththehighestexpectedepisodescore.InmovingfromtheQVMtotheQPU,episodescoredistributionswerefoundtoincreaseinvariance,andinthecaseoftheQAOA,decreaseinexpectedvalue.FortheQVM-trainedanduntrainedagents,expectedtestperformanceremainscomparablebetweentheQVMandQPU.Increasedperformanceofthetrainedagentcomparedtotheuntrainedagent,inallcircumstances,suggeststhatthetrainingprocessissuccessfulinbiasingtheactiondistributiontoyieldmorepromising,observation-dependent,actions.IncreasedperformanceofthetrainedagentcomparedtotheQAOA,inallcircumstances,suggeststhatthep=1QAOAansatzisalessoptimalcircuitansatzcomparedtothosegeneratedbythetrainedagent.Additionally,wenotethatthetrainedagentislesssensitivetoCOP-typecomparedtotheQAOA,resultingintestdistributionsofsimilarcharacterforeachsurveyedproblem.Interestingly,theuntrainedagentyieldsbetterperformanceontheQPUcomparedtotheQAOA.Wespeculatethatthisisinpartduetothestructure,length,andnoisesensitivityoftheQAOAansatz.GiventheepisodescoretestdistributionsgiveninFig.2,onemaycomputethenumberofinstructionscontainedwithintheprogramsgeneratingeachepisodescore.Welabelthisinstruction-metric#instructionstoreachepisodescore,anddepictthisdatainFig.3.NotethatwhenexecutingontheQVM,thereisnonoise,andtheprogramisnotcompiled.TheQVMinstruction-metricthereforedescribes,foreachepisodescore,thenumberofcorresponding(perfect)gatesspeciﬁedbytheagent(inotherwords,argmax1≤i≤mri).InthecaseoftheQPU,theagentprogramiscompiledtothechiptopologyoftheRigettiAspenQPU,andthenativegatesetgivenbyCZ,RZ(θ),andRX(±π/2)usingtheRigettiQuilccompiler.TheQPUinstruction-metricthereforedescribes,foreachepisodescore,thenumberofcompiledgatesresultingfromasequenceofactionsspeciﬁedbytheagent.Bydesign,theagentswereconstrainedto25uncompiledinstructions.FortheQAOA,theuncompiledprogramlengthisdeterminedbythesizeandconnectivityofthegraphofinteresttothep=1QAOAansatz.Throughtraining,forallproblems,theexpectedlengthofthetrainedagentprogramsisdecreasedcomparedtothoseoftheuntrainedagent.Althoughcompiled,thenumberofinstructionsrequiredtoreacheachepisodescoreisreducedforthetrained6agentontheQPUcomparedtotheQVM.NotethatontheQVM,thereisnonoise.Additionally,thereisnopenaltyforgeneratinglongerprogramsuntiltheprogramlengthconstraintof25uncompiledgates.OntheQPU,theagentexperiencesnoisethatlikelyincreasesastheprogramincreasesinlengthandcomplexity.ThiscouldcreateanincentivefortheagenttogenerateshorterprogramsontheQPU,relativetotheQVM.Inallcases,theuntrainedandtrainedagentsyieldshorterprogramscomparedtotheQAOA.OntheQPU,theQAOAprogramscompiledintomorethan102instructions.Additionally,thelatergatesoftheQAOAaregivenbythemoreexpensiveentanglinggates.Iftoomuchdecoherenceisexperiencedbythelaterstagesoftheprogram,onecouldexpectastrongdegradationinperformance,asevidencedbythedecreasedepisodescorebetweentheQVMandQPUfortheQAOA.RecallthattheQPU-trainedmodelwasinitializedusingtheparametersoftheQVM-trainedmodel,andsubsequentlytrainedforanadditional150,000episodesontheQPU.Inordertoinvestigatetheimpactoftrainingonarealquantumresource,boththeQVM-trainedmodelandQPU-trainedmodelweretestontheQPUacrossboththereward-metricandtheinstruction-metric.ThisisshowninFig.4.Inferenceonbothmodels,ontheQPU,yieldscomparablereward-metrics.ForeachCOP,theepisodescoretestdistributionshavesimilarexpectedvaluesandvariances.Althoughgeneratingcomparablereward-metricstatistics,theinstruction-metricslookmuchdifferent.ItisobservedthatthroughQPU-training,theresultantinstruction-metricvaluesbecomemuchshorter.Inthiscontext,devicenoiseactsasanindirectincentivetotheagent.Thisindirectincentivepromotesthegenerationofprogramsthatremainshortfollowingcompilation.ThisisparticularlyevidentfortheMAXCUTproblems.Inthiscase,testingtheQVM-trainedmodelontheQPUyieldscompiledprogramsoflengths,insomecases,exceeding100instructions.However,testingtheQPU-trainedmodelontheQPUyieldscompiledprogramsoflengthsshorterthan50instructions.4ActionstatisticsBroadstatisticalanalysisofagent-generatedprogramswasaccomplishedbycomputingthefrequencyofeachactionintheprogramsgeneratedbytheexperimentsdetailedinSec.3.2.ThesehistogramsareshowninFig.5.NotethatthedatabetweenFig.2,Fig.3,Fig.4,andFig.5areallconsistent,resultingfromdifferentvarietiesofanalysisonthesamesetofexperimentsoveraprecomputedtestdataset.Additionally,explicitexamplesofasetofrandomsampledprogramsareexplicitlyshowninthesupplementaryinformation.TheagentdemonstratesastrongpreferencefortheCNOT,RX(π),andRY(π)gates,occurringapproximatelyequallyforallproblems,forallmodels,onbothtypesofquantumresources.Interestingly,theRZ(π)gateisnotasconsistentlyprevalentacrossthedifferentexperiments,occurringlessfrequentlyontheQVM.OntheQVM,theremaininggatesfrequenciesaredistributedsomewhatevenly.OntheQPU,theRZ(π)gatebecomesmorecommon,eveninthecaseoftheQVM-trainedmodel.ThroughtrainingontheQPU,theQPU-trainedagentappearstolessfrequentlysamplenon-nativeRYandRXgates.Incontrast,thenativeRZ(θ)gatesshowcomparablefrequenciestotheQVM-trainedagent.Thisparallelstheinstruction-metricresultsshowninFig.4.ItappearsthatQPU-trainingaimstoreducecompiledprogramlength,whichamountstodiminishedsamplingofnon-nativegates(whicharecompiledintolongersequencesofnativegates).Tofurtherdeveloptheseobservations,moreextensiveandrobustanalysisisrequired.InthecaseoftheCOPswehaveconsideredabovewithoursimplereward(discounted)rewardstructure,wehypothesizethatthetheoreticallimitoftheoptimalprogramwouldbeaseriesofXgates.ThisissobecausetheHamiltonians(rewardfunctions)weconsiderarealldiagonalinthecomputationalbasis,andtheirsolutionscanbespeciﬁedassomebitstring,i.e.somecomputationalbasiselement,andnotnecessarilyalinearcombinationofsuchbasiselements.ThegatesIandXaresufﬁcienttoproducesuchstatesstartingfromthe|0i⊗Nstate.Hence,weexpecttheoptimalpolicytodisregardanyphaseinformationinthequantumstates.Furthermore,recallthatourdeﬁnedrepresentationoftheobservationspaceisequivalenttothespaceofprobabilisticbits.Forexample,ifthegoalwastomaximizehψ|X|ψi,itissufﬁcienttoproduceanyofthestates(1/√2)(cid:0)|0i+eiθ|1i(cid:1),whicheveroneischeapesttoproducegivenaparticulargateset.Thisisanotherreasonwethereforeexpectthattheoptimalpolicyshoulddisregardanyphaseinformation.Althoughmoreextensiveexperimentalandtheoreticalanalysisisrequired,thishypothesisisconsistentwiththegatefrequenciesobservedduringinferenceonthetestset(asshowninFig.5).Forsuchproblems,wheretheHamiltonianisdiagonalinthecomputationalbasis,theshortestsequenceofgateswecanimaginetoproducethesolutionbitstringisaseriesofXgatesontheappropriatequbits.Arotationofanyangleotherthanπaboutthex-axiswouldproducealessthanoptimalvaluefortheZiZjterm,andthereforethereward,sothatwecannotusethepolicyimprovementtheorem(Ref.[37])toimproveuponthispolicy.Similarly,ourchoiceofobservationspaceandpolicyarchitecturereﬂectthecorrespondencebetweencomputationalbasisstatesandcandidatesolutionsoftheoptimizationproblems.Inparticular,formoregeneralproblems(e.g.asmay7Figure5:ActionfrequenciesresultingfrominferenceontheQVMwiththeQVM-trainedmodel(top),andfrominferenceontheQPUwiththeQVM-trainedmodel(middle)andtheQPU-trainedmodel(bottom).Inallcases,theCNOT,RX(π),andRY(π)gatesarethemostfrequentlyimplementedactions.Theremainingactionsarechosenwithcomparablefrequencies.IntheQPU-trainedmodel,areductioninthefrequencyofnon-nativegatesisobserved.ariseinquantumchemistry)itmaybenecessaryfortheobservationtoconsistofmeasurementswithrespecttoseveralbases;thepolicyinsuchacaseshouldbemodiﬁedaccordingly.5ConclusionOverallweﬁndreinforcementlearningtobeaneffectivemethodforquantumprogrammingforcombinatorialopti-mizationfortheproblemsanddatasetsstudiedhere.Fortheevaluatedproblems,thetrainedagentswereobservedtogenerateverycompetitiveresultscomparedtoanuntrainedagentandcomparedtop=1QAOAonbothsimulatedandphysicalquantumresources.Extensionsofthisworkinclude,butarenotlimitedto:reﬁnementstothelearningenvironment,trainingandcomparisonofdifferentagenttypes,aswellasaninvestigationofhowthetrainingtimescalesasthesizeoftheproblemgrows,andapplicationofthislearningmethodtomoreprogrammingtasks.Modiﬁcationstothelearningenvironmentcouldincludeddeeperinvestigationofstateandobservationrepresentations.Thequantumandproblemobservationsareofadecidedlydistinctnature,andthusitisnaturalforapolicytotreatthemdifferently.Therehasbeenmuchrecentattentiontovectorizationsorfeaturerepresentationsofgraphsandsimilardiscretestructures,relevanttoprocessingofagivencombinatorialoptimizationproblem.Asourfocusisprimarilyonthegeneralmethodofreinforcementlearningforquantumprogramsynthesis,wehaveoptedtonotconsideranyspecializedhandlingofthestateobservationrepresentationandleavethatasanavenueforfuturework.Modiﬁcations8tothelearningenvironmentcouldalsoincludelargerorcontinuousactionspaces,aswellasmodiﬁcationsoftherewardfunction.Additionally,wechosetofocusexclusivelyonPPOlearningagents.Thisisinpartduetothebreadthandmulticom-ponentnatureinvolvediniteratingsimultaneouslyonthelearningenvironmentaswellasonthelearningagent.TherobustnessofthePPOalgorithmallowedforfastandfavorabletrainingwithoutextensivehyperparameteroptimization.PerformanceofalternativeagentssuchasdeepQ-learningisofinterest.Withrespecttoscalability,ifwelimitthearchitectureofthesharedactor-criticnetworktonevergrowmorethanpolynomiallyinthesizeoftheinputproblem,thenthecomputationsthisnetworkcarriesoutinordertoproduceanoutputactionshouldalsogrowatmostpolynomially.Wemayaskhowmanytrainingstepswerequiretoreachacertainvalidationaccuracyonsomeheldoutsetofprobleminstances.Forasufﬁcientlyhighvalidationaccuracy,thetrainednetworkmayserveasausefulheuristicifthetrainingtimegrowsatmostpolynomiallyinthesizeoftheproblem.Itisnotunreasonabletoexpectthistohappen,sinceneitherthegatesetwehaveconsideredabovenorthetrainableparametersofthenetworkgrowsuper-polynomially.However,asystematicinvestigationofhowwellthisframeworkscalesasthesizeoftheprobleminputiscurrentlylacking,andwouldserveasanimportantbarometerofhowwellthisapproachwouldworkinpractice.HerewehavechosentofocusonCOPs,forwhichthespeciﬁedHamiltonianisdiagonalinthecomputationalbasis.Weremaincuriousabouttheextensionofthisworktodifferentdomains.Wespeculatethatextendingouranalysistootherproblems,suchasthosefoundinquantumsimulationsettingswhereweexpecttoseeHamiltoniansthatarenon-diagonalinthecomputationalbasis,wouldyieldtheoreticallyoptimalpoliciesthatusenon-Cliffordoperations.Wealsoimaginethatbychangingtherewardstructure,wecouldretoolthisproceduretooptimizenotjustforshortestsequenceofgatesfromsomegivengateset,butalsotopreferentiallyutilizequantumresourcesoverclassicalones.6AcknowledgementsTheauthorswishtothankAmyBrownandtheentireRigettihardwareteamforQPUsupport.WealsowouldliketogivementiontoNimaAlidoustandDavidRivasfortheirtechnicalandlogisticalguidance,andRobertSmith,JoshuaCombes,EricPeterson,MarcusdaSilva,KirbyLinvill,andKung-ChuanHsuformanyinsightfulconversations.References[1]FranciscoAlbarran-Arriagadaetal.“Measurement-basedadaptationprotocolwithquantumreinforcementlearning”.In:PhysicalReviewA98.4(2018),p.042315.[2]ZhengAnandDLZhou.“DeepReinforcementLearningforQuantumGateControl”.In:arXivpreprintarXiv:1902.08418(2019).[3]SanjeevAroraetal.“Onnon-approximabilityforquadraticprograms”.In:46thAnnualIEEESymposiumonFoundationsofComputerScience(FOCS05).IEEE.2005,pp.206–215.[4]SanjeevAroraetal.“Proofveriﬁcationandthehardnessofapproximationproblems”.In:JournaloftheACM(JACM)45.3(1998),pp.501–555.[5]MoritzAugustandJoseMiguelHernandez-Lobato.“Takinggradientsthroughexperiments:LSTMsandmemoryproximalpolicyoptimizationforblack-boxquantumcontrol”.In:InternationalConferenceonHighPerformanceComputing.Springer.2018,pp.591–613.[6]IrwanBelloetal.“Neuralcombinatorialoptimizationwithreinforcementlearning”.In:arXivpreprintarXiv:1611.09940(2016).[7]YoshuaBengio,AndreaLodi,andAntoineProuvost.“MachineLearningforCombinatorialOptimization:aMethodologicalTourd’Horizon”.In:arXivpreprintarXiv:1811.06128(2018).[8]IlhemBoussaid,JulienLepagnot,andPatrickSiarry.“Asurveyonoptimizationmetaheuristics”.In:Informationsciences237(2013),pp.82–117.[9]GregBrockmanetal.“Openaigym”.In:arXivpreprintarXiv:1606.01540(2016).[10]MarinBukov.“ReinforcementlearningforautonomouspreparationofFloquet-engineeredstates:InvertingthequantumKapitzaoscillator”.In:PhysicalReviewB98.22(2018),p.224305.[11]MarinBukovetal.“Reinforcementlearningindifferentphasesofquantumcontrol”.In:PhysicalReviewX8.3(2018),p.031086.[12]SACaldwelletal.“Parametricallyactivatedentanglinggatesusingtransmonqubits”.In:PhysicalReviewApplied10.3(2018),p.034050.9[13]MosesCharikarandAnthonyWirth.“Maximizingquadraticprograms:extendingGrothendieck’sinequality”.In:45thAnnualIEEESymposiumonFoundationsofComputerScience.IEEE.2004,pp.54–60.[14]ArnabDasandBikasKChakrabarti.Quantumannealingandrelatedoptimizationmethods.Vol.679.SpringerScience&BusinessMedia,2005.[15]NicolasDidieretal.“ACﬂuxsweetspotsinparametrically-modulatedsuperconductingqubits”.In:arXivpreprintarXiv:1807.01310(2018).[16]IainDunning,SwatiGupta,andJohnSilberholz.“Whatworksbestwhen?AsystematicevaluationofheuristicsforMax-CutandQUBO”.In:INFORMSJournalonComputing30.3(2018),pp.608–624.[17]EdwardFarhi,JeffreyGoldstone,andSamGutmann.“Aquantumapproximateoptimizationalgorithm”.In:arXivpreprintarXiv:1411.4028(2014).[18]ThomasFöseletal.“Reinforcementlearningwithneuralnetworksforquantumfeedback”.In:PhysicalReviewX8.3(2018),p.031084.[19]MichelXGoemansandDavidPWilliamson.“Improvedapproximationalgorithmsformaximumcutandsatisﬁabilityproblemsusingsemideﬁniteprogramming”.In:JournaloftheACM(JACM)42.6(1995),pp.1115–1145.[20]SumitGulwani,OleksandrPolozov,RishabhSingh,etal.“Programsynthesis”.In:FoundationsandTrendsR(cid:13)inProgrammingLanguages4.1-2(2017),pp.1–119.[21]StuartHadﬁeldetal.“Fromthequantumapproximateoptimizationalgorithmtoaquantumalternatingoperatoransatz”.In:Algorithms12.2(2019),p.34.[22]NeelKant.“Recentadvancesinneuralprogramsynthesis”.In:arXivpreprintarXiv:1802.02353(2018).[23]RichardMKarp.“Reducibilityamongcombinatorialproblems”.In:Complexityofcomputercomputations.Springer,1972,pp.85–103.[24]EliasKhaliletal.“Learningcombinatorialoptimizationalgorithmsovergraphs”.In:AdvancesinNeuralInformationProcessingSystems.2017,pp.6348–6358.[25]SubhashKhotetal.“OptimalinapproximabilityresultsforMAX-CUTandother2-variableCSPs?”In:SIAMJournalonComputing37.1(2007),pp.319–357.[26]GaryKochenbergeretal.“Theunconstrainedbinaryquadraticprogrammingproblem:asurvey”.In:JournalofCombinatorialOptimization28.1(2014),pp.58–81.[27]AlexandreMegretski.“Relaxationsofquadraticprogramsinoperatortheoryandsystemanalysis”.In:Systems,approximation,singularintegraloperators,andrelatedtopics.Springer,2001,pp.365–392.[28]MohammadrezaNazarietal.“Reinforcementlearningforsolvingthevehicleroutingproblem”.In:AdvancesinNeuralInformationProcessingSystems.2018,pp.9839–9849.[29]ArkadiNemirovski,CornelisRoos,andTamásTerlaky.“Onmaximizationofquadraticformoverintersectionofellipsoidswithcommoncenter”.In:MathematicalProgramming86.3(1999),pp.463–473.[30]AniNersisyanetal.“Manufacturinglowdissipationsuperconductingquantumprocessors”.In:arXivpreprintarXiv:1901.08042(2019).[31]YuNesterovetal.Globalquadraticoptimizationviaconicrelaxation.Tech.rep.Citeseer,1998.[32]MurphyYuezhenNiuetal.“Universalquantumcontrolthroughdeepreinforcementlearning”.In:AIAAScitech2019Forum.2019,p.0954.[33]JuanLOlmo,JoseRRomero,andSebastianVentura.“Swarm-basedmetaheuristicsinautomaticprogramming:asurvey”.In:WileyInterdisciplinaryReviews:DataMiningandKnowledgeDiscovery4.6(2014),pp.445–469.[34]JohnSchulmanetal.“Proximalpolicyoptimizationalgorithms”.In:arXivpreprintarXiv:1707.06347(2017).[35]KateASmith.“Neuralnetworksforcombinatorialoptimization:areviewofmorethanadecadeofresearch”.In:INFORMSJournalonComputing11.1(1999),pp.15–34.[36]RobertSSmith,MichaelJCurtis,andWilliamJZeng.“Apracticalquantuminstructionsetarchitecture”.In:arXivpreprintarXiv:1608.03355(2016).[37]RichardSSuttonandAndrewGBarto.ReinforcementLearning:AnIntroduction.TheMITPress,2018.[38]OriolVinyals,MeireFortunato,andNavdeepJaitly.“Pointernetworks”.In:AdvancesinNeuralInformationProcessingSystems.2015,pp.2692–2700.[39]Xiao-MingZhangetal.“Whenreinforcementlearningstandsoutinquantumcontrol?Acomparativestudyonstatepreparation”.In:arXivpreprintarXiv:1902.02157(2019).10SupplementaryInformation:Automatedquantumprogrammingviarein-forcementlearningforcombinatorialoptimization1EnvironmentandAgentArchitectureOuroptimizationproblemsareparticularlysimpletomaptoquantumhardware,inthesensethatwemaynaturallyobtainaone-to-onecorrespondencebetweenbinaryproblemvariablesandqubits.Thusforaproblemofn=10variables,eachbasisvectorofa10-qubitsystemmaybeexpressedinketnotationas|b1...bniwherebi∈{0,1},andhenceasinglemeasurementofthissysteminthestandardbasisyieldsacandidatesolutiontotheoptimizationproblem.ActionSpaceTheactionsare•X,Y,andZrotationsoneachqubit,withdiscreteangles2πk8.•CNOTgatesoneachpairofdistinctqubits.Wherek={0,...,7}.Insummary,thereare3*10*8=240singlequbitactions,and45twoqubitactions.EachactionmaybeexpressedasasingleinstructionsintheQuilISA([2]),e.g.RX(π/2)7.ObservationSpaceThusasequenceofactionscorrespondstoaQuilprogram.Whenexecutedonaquantumdevice,acorrespondingquantumstateisprepared.Ameasurementofthisstatewithrespecttothestandardcomputationalbasisresultsinabitstringb=b1b2...bn,wherebiwasthemeasuredstateofqubiti,andwehaveﬁxedn=10.Thisprocessofpreparationandmeasurementisrepeatedforsomenumberoftimes(the“numberofshots"),resultinginasequenceofbitstrings.Inwhatfollows,wehaveﬁxedthenumberofshotstobem=10,sothattheresultingobservationofthequantumstateisa10×10binaryarrayB=[b(1);...;b(10)].Aprobleminstanceisspeciﬁedbyaspeciﬁcchoiceofweightsw.ForMAXCUTandMAXQP,the(cid:0)n2(cid:1)=45off-diagonaluppertriangularentriesoftheweightmatrixwsufﬁcetofullydescribetheprobleminstance.ForQUBO,the(cid:0)n2(cid:1)+n=55uppertriangularentriesareneeded.Thesemaybeconciselyexpressedasanumericvector˜w,representinganobservationoftheprobleminstance.Theobservationmadebytheagentconsistsofthejointquantumandproblemobservations.Inotherwords,weconsiderobs:=(B,˜w).(1)RewardWithrespecttoagivenproblemoftheformmaxx∈{0,1}nC(x),therewardassociatedwiththeobservationobsisr(obs)=110Pmi=1C(b(i))−m(M−m),wherem=minbC(b)andM=maxbC(b)aretheminimumandmaximumattainablevaluesofC.ThismaybeseentobeanestimateofthenormalizedexpectationofthecorrespondingproblemHamiltonian.Win/losecriteriaForagivenepisode,theagentissaidtohave‘won’ifthenormalizedrewardwasfoundtoexceed.8.Thiswaschosenthroughquickempiricalexperimentation.Weimaginethatmethodssuchascurriculumlearningcouldbeusedinordertodrivetheepisoderewardshigher.Notethatbywinning,theepisodeterminatesearly.Theagentissaidtohave‘lost‘iftheuncompiledprogramlengthexceeds25instructions.PolicyArchitectureFollowingobservation,(B,˜w)isconcatenatedtoasinglevectorandsubsequentlypassedthroughtwodenselayersof32neuronseach.Theoutputisavectorofactionscores.Giventhatthesolutionspaceforoneofourcombinatorialoptimizationproblemsisofsize210,wehaveintentionallyadoptedasmallandsimplearchitecture.Forlargerproblems,1arXiv:1908.08054v1  [quant-ph]  21 Aug 2019onewouldgenerallyrequirethatthenumberofweightsinthefullnetworkscalesassomesmallpolynomialinthenumberofproblemvariablesn.Forlearning,weusetheimplementationofactor-criticPPOprovidedby[1].Thesinglenetworkwedescribedaboveservesasasharedactorandcriticnetwork.Theweightsforboththedenselayersaswellasthoseformeasurementstatistics,Θ,weretrained.2AgenttrainingandhyperparameteroptimizationPPOwasfoundtoyieldreasonableresultsoff-the-shelf,butalsofoundtoyieldgreatlyimprovedresultswithamodestamountofhyperparameteroptimization.InFig.1weshowthelearningcurvesfortheagentunderarangeofagentparameterizations.Itwasfoundthatincreasingtheλparameterfrom0.95(blue)to0.999(orangeandblack)resultedinimprovedresults.Furthermore,increasingthebatchsizefrom128(blue,orange)to512(black)hadadramaticimpactonagentperformance.Theblacklearningcurverepresenttheﬁnalparameterconﬁguration(λ=.99,n_steps=512).Figure1:LearningcurvesforarangeofPPOhyperparameterconﬁgurations.Eachcurvedenotesauniquerandomseed.Itcanbeseenthattrainingperformanceiscomparableforeachrandomseed.Wenotethatthemeanepisodereward,aswellastheaverageepisodelength,donotappeartohavefullyconvergedandcouldpotentiallybeneﬁtfromadditionaltraining.Withtheexceptofthehyperparametersdiscussedabove,weusedthedefaulthyperparametersprovidedbytheimplementationin[1](Adamoptimizerwithepsilon=10−5,alinearlearningschedule,clipparameterof0.2andadvantageestimationcoefﬁcientof0.95).3GeneratedprogramsForillustrativepurposes,wequerytheQVM-trainedmodelon8randomproblemsanddisplaythecorrespondingoutputprogrambelow.Recallthattheseprogramsareiterativelyconstructed.So,foreachprogrambelow,theﬁnalprogramconsistsofthesumofinstructionsinthetable.rdenotestheevolutionoftherewardastheprogramisconstructed.2instrrRX(pi)80.497569RY(pi)10.503288RY(pi)00.691885RX(pi)90.687903RX(pi/4)20.676666RX(pi/4)20.669175RX(pi/4)20.661683RX(pi/4)20.650446RX(pi/4)20.654192RY(pi)40.713833RX(pi)50.723166RX(pi/4)20.683561CNOT590.929027instrrRY(pi)70.559380RY(pi)40.640293RX(pi)80.795450RY(pi)10.924792instrrRX(pi)70.734336RX(pi)80.710776RX(pi)20.742716RY(pi)00.751006RY(5*pi/4)30.837711instrrRX(pi)60.629431RX(pi)50.667715CNOT450.667715RZ(0)80.667715CNOT180.667715RZ(3*pi/2)10.667715RX(pi)20.760005CNOT580.904721instrrRX(pi)60.629431RX(pi)50.667715CNOT450.667715RZ(0)80.667715CNOT180.667715RZ(3*pi/2)10.667715RX(pi)20.760005CNOT580.9047213instrrRX(pi)40.674805RX(pi)20.685028RY(pi)00.763762RX(pi)80.735677RX(pi)50.792516RY(pi)30.771576RZ(pi/2)80.771576RZ(5*pi/4)40.771576RZ(pi/2)80.771576RZ(pi/2)80.771576RZ(pi/2)80.771576RZ(3*pi/2)30.771576CNOT670.771576RY(0)40.771576RZ(pi/2)80.771576CNOT670.771576RZ(pi/2)80.771576CNOT090.737719CNOT090.771576RZ(pi/2)80.771576CNOT280.801347instrrRY(pi)70.787879RX(pi)80.712911RY(pi)90.712158RZ(pi/2)20.712158RX(pi)50.938972instrrRX(pi)40.464864RX(pi)20.605566RX(pi)80.765052RX(pi)90.9127504QAOAExperimentsQAOAwithp=1consistsofoptimizingtwoclassicalparameters,conventionallydenotedγandβ,suchthattheexpectationvalueoftheproblemHamiltonianismaximizedbythequantumstatedeﬁnedby|γ,βi=e−ıβBe−ıγHC|+i⊗n(2)wherendenotesthenumberofqubits(10inourcase),|+i⊗n=H⊗n|0inistheequal-superpositionstateofallbitstrings,CdenotestheproblemHamiltonian,andBdenotesthe‘mixer’whichisconventionallytakentobeB=Pnj=1Xj.Foreveryprobleminstance,weusetheRigettiWavefunctionSimulatortocomputeoptimalvaluesofγandβ.Wedothisbydiscretizingtheinterval[0,2π)into20bins,andthenloopingovera20x20grid,correspondingtothevariousvaluesofγandβ,andcalculatingtheexpectationvaluehγ,β|C|γ,βi.Theseexpectationvaluesarecalculatedexactlyviamatrixmultiplication,andnotestimatedthroughsampling.Onceweknowtheoptimalvaluesofγandβforeachprobleminstance,werunthecorrespondingcircuitontheactualquantumprocessor,andcomputethesolutionqualityastheexpectationvalueofthecostHamiltonianforthatspeciﬁcprobleminstanceinthestateproducedbythep=1QAOAquantumcircuit,normalizedbythedifferencebetweenthemaximumandminimumexpectationvalueofthatcostHamiltonian.Themaximumcostiscomputedbysimplylooping4overallpossible210bitstrings,calculatingthevalueofthecostHamiltonianforeachofthosebitstrings,andreportingthemaximum.Concretely,wedeﬁnethesolutionqualityhγ,β|˜C|γ,βiashγ,β|˜C|γ,βi=hγ,β|C|γ,βi−min|ψihψ|C|ψimax|ψihψ|C|ψi−min|ψihψ|C|ψi(3)whichcoincideswiththedeﬁnitionoftherewardfortheRLagent.AllthetermsappearingontherighthandsideofEq.3arecomputedexactlyviaclassicalmethods,exceptfortheexpectationvaluehγ,β|C|γ,βi,whichisestimatedviasampledbitstrings.Foreachprobleminstance,wesample10bistringstoestimatethisexpectationvalue.Foreveryprobleminstance,wealsocomputethelengthofthecompiledp=1QAOAquantumprogram.References[1]AshleyHilletal.StableBaselines.https://github.com/hill-a/stable-baselines.2018.[2]RobertSSmith,MichaelJCurtis,andWilliamJZeng.“Apracticalquantuminstructionsetarchitecture”.In:arXivpreprintarXiv:1608.03355(2016).5