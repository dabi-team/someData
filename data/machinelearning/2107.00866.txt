Learning Primal Heuristics for Mixed Integer
Programs

Yunzhuang Shen
School of
Computing Technologies
RMIT University
Melbourne, Australia
s3640365@student.rmit.edu.au

Yuan Sun
School of Mathematics
Monash University
Melbourne, Australia
yuan.sun@monash.edu

Andrew Eberhard
School of Science
RMIT University
Melbourne, Australia
andy.eberhard@rmit.edu.au

Xiaodong Li
School of
Computing Technologies
RMIT University
Melbourne, Australia
xiaodong.li@rmit.edu.au

1
2
0
2

l
u
J

2

]
I

A
.
s
c
[

1
v
6
6
8
0
0
.
7
0
1
2
:
v
i
X
r
a

Abstract—This paper proposes a novel primal heuris-
tic for Mixed Integer Programs, by employing machine
learning techniques. Mixed Integer Programming is a
general technique for formulating combinatorial opti-
mization problems. Inside a solver, primal heuristics
play a critical role in ﬁnding good feasible solutions that
enable one to tighten the duality gap from the outset
of the Branch-and-Bound algorithm (B&B), greatly
improving its performance by pruning the B&B tree
aggressively. In this paper, we investigate whether
eﬀective primal heuristics can be automatically learned
via machine learning. We propose a new method to
represent an optimization problem as a graph, and train
a Graph Convolutional Network on solved problem
instances with known optimal solutions. This in turn
can predict the values of decision variables in the
optimal solution for an unseen problem instance of a
similar type. The prediction of variable solutions is
then leveraged by a novel conﬁguration of the B&B
method, Probabilistic Branching with guided Depth-
ﬁrst Search (PB-DFS) approach, aiming to ﬁnd (near-
)optimal solutions quickly. The experimental results
show that this new heuristic can ﬁnd better primal
solutions at a much earlier stage of the solving process,
compared to other state-of-the-art primal heuristics.

Index Terms—Mixed Integer Programming, Primal

Heuristics, Machine Learning

I. Introduction

Combinatorial Optimization problems can be formulated
as Mixed Integer Programs (MIPs). To solve general MIPs,
sophisticated software uses Branch-and-Bound (B&B)
framework, which recursively decomposes a problem and
enumerates over the sub-problems. A bounding function
determines whether a sub-problem can be safely discarded
by comparing the objective value of the current best
solution (incumbent) to a dual bound, generally obtained
by solving a linear programming relaxation (i.e., relaxing
integral constraints) of that sub-problem.

Inside solvers, primal heuristics play an important role in
ﬁnding good primal solutions at an early stage [1]. A good
primal solution strengthens the bounding functions which
allows one to prune suboptimal branches more aggressively
[2]. Moreover, ﬁnding good feasible solutions earlier greatly
reduces the duality gap, which is important for user

satisfaction [3]. Acknowledging the importance of primal
heuristics, a modern open-source MIP solver SCIP [4],
employs dozens of heuristics [2], including meta-heuristics
[5], heuristics supported by mathematical theory [6], and
heuristics mined by experts and veriﬁed by extensive
experimental evidence [1]. Those heuristics are triggered
to run with engineered timings during the B&B process.
In many situations, users are required to solve MIPs of
a similar structure on a regular basis [7], so it is natural
to seek Machine Learning (ML) solutions. In particular,
a number of studies leverage ML techniques to speed up
ﬁnding good primal solutions for MIP solvers. He et al. [8]
train Support Vector Machine (SVM) to decide whether to
explore or discard a certain sub-problem, aiming to devote
more of the computational budget on ones that are likely
to contain an optimal solution. Khalil et al. [9] train an
SVM model to select which primal heuristic to run at a
certain sub-problem. More related to our work here, Sun et
al. [10] and Ding et al. [11] leverage ML to predict values of
decision variables in the optimal solution, which are then
used to ﬁx a proportion of decision variables to reduce the
size of the original problem, in the hope that the reduced
space still contains the optimal solution of the original
problem.

In this work, we propose a novel B&B algorithm
guided by ML, aiming to search for high-quality primal
solutions eﬃciently. Our approach works in two steps.
Firstly, we train an ML model using a dataset formed
by optimally-solved small-scale problem instances where
the decision variables are labeled by the optimal solution
values. Speciﬁcally, we employ the Graph Convolutional
Network [12] (GCN), where an input graph associated
with a problem instance to GCN is formed by representing
each decision variable as a node and assigning an edge
between two nodes if the corresponding decision variables
appear in a constraint in the MIP formulation (see Section
II-A). Then given an unseen problem instance, the trained
GCN with the proposed graph representation method can
eﬃciently predict for each decision variable its probability
of belonging to an optimal solution in an unseen problem
instance (e.g., whether a vertex is a part of the optimal

 
 
 
 
 
 
solution for the Maximum Independent Set Problem).
The predicted probabilities are then used to guide a
novel B&B conﬁguration, called Probabilistic Branching
technique with guided Depth-ﬁrst Search (PB-DFS). PB-
DFS enumerates over the search space starting from the
region more likely to contain good primal solutions to the
region of unpromising ones, indicated by GCN.

Although both the problem-reduction approaches [10],
[11] and our proposed PB-DFS utilize solution prediction
by ML, they are inherently diﬀerent. The former can be
viewed as a pre-processing step to prune the search space
of the original problem, and the reduced problem is then
solved by a B&B algorithm. In contrast, our PB-DFS
algorithm conﬁgures the search order of the B&B method
itself and directly operates on the original problem. In this
sense, our PB-DFS is an exact method if given suﬃcient
running time. However, as the PB-DFS algorithm does not
take into account the size of the B&B search tree, it is not
good at proving optimality. Therefore, we will limit the
running time of PB-DFS and use it as a primal heuristic.

Our contributions can be summarized as follows:
1) We propose the Probabilistic Branching technique
with guided Depth-ﬁrst Search, a conﬁguration of the
B&B method specialized for boosting primal solutions
empowered by ML techniques.

2) We propose a novel graph representation method that
captures the relation between decision variables in
a problem instance. The constructed graph can be
input to GCN to make predictions eﬃciently.

3) Extensive experimental evaluation on NP-hard cover-
ing problems shows that 1) GCN with the proposed
graph representation method is very competitive in
terms of eﬃciency and eﬀectiveness as compared to
a tree-based model, a linear model, and a variant of
Graph Neural Network [11]. 2) PB-DFS can ﬁnd
(near-)optimal solutions at a much earlier stage
comparing to existing primal heuristics as well as
the problem-reduction approaches using ML.

II. Background

A. MIP and MIP solvers

MIP takes the form arg min{cT x | x ∈ F}. For an MIP
instance with n decision variables, c ∈ Rn is the objective
coeﬃcient vector. x denotes a vector of decision variables.
We consider problems where the decision variables x are
binary in this study, although our method can be easily
extended to discrete domain [13]. F is the set of feasible
solutions (search space), which is typically deﬁned by
integrality constraints and linear constraints Ax ≤ b,
where A ∈ Rm×n and b ∈ Rm are the constraint matrix
and constraint right-hand-side vector, respectively. m is
the number of constraints. The goal is to ﬁnd an optimal
solution in F that minimizes a linear objective function.
For solving MIPs, exact solvers employ B&B framework
as their backbone, as outlined in Algorithm 1. There are
two essential components in the B&B framework, branching

Algorithm 1 Branch-and-Bound Algorithm

Require: a problem instance: I;
1: the node queue: L ← {I};
2: the incumbent and its objective value: ˆx ← ∅, ˆc ← ∞;
3: while L is not empty do
4:
5:

choose Q from L; L ← L \ Q;
solve the linear relaxation QLP of Q;
if QLP is infeasible then

6:
7:
8:
9:
10:

11:
12:
13:
14:
15:
16:

go to Line 3 ;

end if
denote the LP solution ˆxLP ;
denote the LP objective ˆcLP ;
if ˆcLP ≤ ˆc then

if ˆxLP is feasible in Q then

ˆx ← ˆxLP ; ˆc ← ˆcLP ;

else

split Q into subproblems Q = Q1 ∩ ... ∩ Qn ;
L ← L ∩ {Q1..Qn};

end if

end if

17:
18:
19: end while
20: return ˆx

policy and node selection strategy. A node selection strategy
determines the next (sub-)problem (node) Q to solve from
the queue L, which maintains a list of all unexplored nodes.
B&B obtains a lower bound on the objective values of
Q by solving its Linear Programming (LP) relaxation
QLP . If the LP solution (lower bound) is larger than the
objective ˆc ≡ cT ˆx of the incumbent ˆx (upper bound),
then the sub-tree rooted at node Q can be pruned safely.
Otherwise, this sub-tree possibly contains better solutions
and should be explored further. If the LP solution ˆxLP is
feasible in Q and of better objective value, the incumbent
is updated by ˆxLP . Otherwise, Q is decomposed into
smaller problems by ﬁxing a candidate variable to a possible
integral value. The resulting sub-problems are added to the
node queue. Branching policy is an algorithm for choosing
the branching variable from a set of candidate variables,
which contains decision variables taking on fractional
values in the solution ˆxLP . Modern solvers implement
sophisticated algorithms for both components, aiming at
ﬁnding good primal solutions quickly while maintaining a
relatively small tree size. See [4] for a detailed review.

B. Heuristics in MIP solvers

During computation, primal heuristics can be executed
at any node, devoted to improve the incumbent, such
that more sub-optimal nodes can be found earlier and
thus pruned without further exploration. Berthold [2]
classiﬁes these primal heuristics into two categories: start
heuristics and improvement heuristics. Start heuristics
aim to ﬁnd feasible solutions at an early solving stage,
while improvement heuristics build upon a feasible solution
(typically the incumbent) and seek better solutions. All

Algorithm 2 An MIP Instance to Linkage Graph
Require: the constraint matrix: A ∈ Rm×n;
1: the adjacency matrix: Gadj ← 0n×n;
2: the row index of A: i ← 0;
3: the index set of variables: C ← ∅;
4: while i < m do
C ← {j | Ai,j 6= 0};
5:
for k, l ∈ C , k 6= l do
6:
Gadj
k,l ← 1, Gadj
l,k ← 1;
7:
end for
8:
i ← i + 1;
9:
10: end while
11: return Gadj

heuristics run on external memory (e.g., a copy of a sub-
problem) and do not modify the structure of the B&B tree.
For a more comprehensive description of primal heuristics,
we refer readers to [2], [3].

III. Method

In Section III-A, we describe how to train a machine
learning model to predict the probability for each binary
decision variable of its value in the optimal solution. In
Section III-B, we illustrate the proposed PB-DFS that
leverages the predicted probabilities to boost the search
for high-quality primal solutions.

A. Solution Prediction

Given a combinatorial optimization problem, we con-
struct a training dataset from multiple optimally-solved
problem instances where each instance associates with an
optimal solution. A training example corresponds to one
decision variable xi from a solved problem instance. The
label yi of xi is the solution value of xi in the optimal
solution associated with a particular problem instance. The
features fi of xi are extracted from the MIP formulation,
which describes the role of xi in that problem instance. We
describe those features in Appendix A. Given the training
data, an ML model can be trained by minimising the cross-
entropy loss function [14] to separate the training examples
with diﬀerent class labels [10], [11].

We adapt the Graph Convolutional Network (GCN)
[12] for this classiﬁcation task, a type of Graph-based
Neural Network (GNN), to take the relation between
decision variables from a particular problem instance into
account. To model the relation between decision variables,
we propose a simple method to extract information from
the constraint matrix of an MIP. Algorithm 2 outlines
the procedure. Given an MIP we represent each decision
variable as a node in a graph, and assign an edge between
two nodes if the corresponding decision variables appear in
a constraint. This graph representation method can capture
the linkage of decision variables eﬀectively, especially for
graph-based problems. For example, the constructed graph
for the Maximum Independent Set problem is exactly the

same as the graph on which the problem is deﬁned, and the
constructed graph for the Traveling Salesman Problem is
the line graph of the graph given by the problem deﬁnition.
Given the dataset containing training examples (fi, yi)
grouped by problem instances and each problem instance
associated with an adjacency matrix Gadj representing the
relation between decision variables, we can then train GCN.
Speciﬁcally for a problem instance, the adjacency matrix
Gadj is precomputed to normalize graph Laplacian by

L := I − D− 1

2 GadjD− 1
2 ,

(1)

where I and D are the identity matrix and diagonal matrix
of Gadj, respectively. The propagation rule is deﬁned as

H l+1 := σ(LH lW l + H l),

(2)

where l denotes the index of a layer. Inside the activation
function σ(·), W l denotes the weight matrix. H l
is a
matrix that contains hidden feature representations for
decision variables in that problem instance, initialized by
the feature vectors of decision variables H 0 = [f1, · · · , fn]T .
The second term is referred to as the residual connection,
which preserves information from the previous layer. For
hidden layers with arbitrary number of neurons, We adopt
ReLU (x) = max(x, 0) as the activation function [15]. For
the output layer L, there is only one neural to output
a scalar value for each decision variable and sigmoid
function is used as the activation function for prediction
HL = [ˆy1, · · · , ˆyn]. We train GCN using Stochastic Gra-
dient Descent to minimize the cross-entropy loss function
between the predicted values ˆyi of decision variables and
their optimal values y, deﬁned as

min −

1
N

N
X

i=1

(cid:0)yi × log(ˆyi) + (1 − yi) × log(1 − ˆyi)(cid:1),

(3)

where N is the total number decision variables from
multiple training problem instances.

Given an unseen problem instance at test time, we can
use the trained GCN to predict for each decision variable
xi a value ˆyi, which can be interpreted as the probability
of a decision variable taking the value of 1 in the optimal
solution pi = P (xi = 1). We refer to the array of predicted
values as the probability vector.

B. Probabilistic Branching with guided Depth-ﬁrst Search

We can then apply the predicted probability vector to
guide the search process of B&B method. The proposed
branching strategy, Probabilistic Branching (PB) attempts
to greedily select variable xi from candidate variables with
the highest score zi to branch on. The score of variable xi
can be computed by

zi ← max(pi, 1 − pi),

(4)

where pi
is the probability of xi being assigned to 1
predicted by the GCN model. This function assigns a higher
score to variables whose pi is closer to either 0 or 1. This

Figure 1: Probabilistic Branching with guided Depth-ﬁrst Search
on three decision variables. Given a node, PB branches on a
variable our prediction is more conﬁdent. The variables that
have already been branched are removed from the candidate
set. The order of node selection by guided DFS is indicated by
the arrow lines.

value tells how certain an ML model is about its prediction.
We then can branch on the decision variable xi with the
highest score,

i ← arg max

i

zi; s.t. i ∈ C.

(5)

C is the index set of candidate variables that are not ﬁxed
at the current node. In this way, our PB method prefers to
branch on the variables for which our prediction is more
“conﬁdent” at the shallow level of the search tree while
exploring the uncertain variables at the deep level of the
search tree.

We propose to use a guided Depth-ﬁrst Search (DFS) as
the node selection strategy to select the next sub-problem
to explore. After branching a node, we have a maximum of
two child nodes to explore (because the decision variables
are binary). Guided DFS selects the child node that results
from ﬁxing the decision variable to the nearest integer of
pi. When reaching a leaf node, guided DFS backtracks to
the deepest node in the search tree among all unexplored
nodes. Therefore, guided DFS always explores the node
most likely to contain optimal solutions, instructed by the
prediction of an ML model. We refer to this implementation
of B&B method as PB-DFS.

Figure 1 illustrates PB-DFS applied to a problem with
three decision variables. We note that as a conﬁguration
of the B&B framework, PB-DFS can be an exact search
method if given enough time. However, it is specialized
for aggressively seeking high-quality primal solutions while
trading oﬀ the size of the B&B tree created during the
computation. Hence, we implement and evaluate it as a
primal heuristic, which partially solves an external copy of
the (sub-)problem with a certain termination criterion.

IV. Experiments

In this section, we use numerical experiments to evaluate
the eﬃcacy of our proposed method. After describing the
experiment setup, we ﬁrst analyse diﬀerent ML models
in terms of both eﬀectiveness and eﬃciency. Then, we
evaluate the proposed PB-DFS equipped with diﬀerent
ML models against a set of primal heuristics. Further, we
show the signiﬁcance of PB-DFS with the proposed GCN
model by comparing it to the full-ﬂedged SCIP solver and
problem-reduction approaches using the SCIP solver [11].

A. Setup

a) Test Problems: We select a set of representative
NP-hard problems: Maximum Independent Set (MISP),
Dominant Set Problem (DSP), Vertex Cover Problem
(VCP), and an additional problem from Operational
Research, Combinatorial Auction Problem (CAP). For
each problem, we generate instances of three diﬀerent
scales, i.e., small, medium, and large. Small-scale problem
instances and medium-scale problem instances are solved
to optimality for training and evaluating ML models. Large-
scale problem instances are used for evaluating diﬀerent
solution approaches. Details of problem formulations and
instance generations are provided in Appendix B.

b) ML Models: We refer to the GCN that takes the
proposed graph representation of MIP as LG-GCN, where
LG stands for Linkage Graph. We compare LG-GCN
against three other machine learning (ML) models, Logistic
Regression (LR), XGBoost [16], and a variant of Graph
Neural Network (GNN) which represents the constraint
matrix of MIP as a tripartite graph [11]. We address
this GNN variant as TRIG-GCN, where TRIG stands for
Tripartite Graph. We set the number of layers to 20 for
LG-GCN and that of TRIG-GCN is set to 2 due to its high
computational cost (explained later). For these two Graph
Neural Networks, the dimension of the hidden vector of a
decision variable is set to 32. For LR and XGBoost, the
hyper-parameters are the default ones in the Scikit-learn
[17] package. For all ML models, the feature vector of a
decision variable has 57 dimensions, containing statistics
extracted from the MIP formulation of a problem instance
(listed in Appendix A). For each feature, we normalize its
values to the range [0, 1] using min-max normalization with
respect to the decision variables in a particular problem
instance. Besides, LG-GCN and TRIG-GCN are trained
with graphs of diﬀerent structures with respect to each
problem instance. For a problem, an ML model is trained
using 500 optimally-solved small-scale instances.

c) Evaluation of ML Models: We evaluate the classi-
ﬁcation performance of ML models on two test datasets,
constructed from 50 small problem instances (diﬀerent
from the training instances) and medium-sized problem
instances, respectively. We measure a model’s performance
with the Average Precision (AP) [18], deﬁned by accumulat-
ing the product of precision and the change in recall when
moving the decision threshold on a set of ranked variables.
AP is a more informative metric than the accuracy metric
in our context, because it takes the ranking of decision
variables into account. This allows AP to better measure
the classiﬁcation performance for imbalanced data that is
common in the context of solution prediction for NP-hard
problems. Further, AP can better measure the performance
of the proposed PB-DFS, because exploring a node not
containing the optimal solution in the upper level of the
B&B tree is more harmful than exploring one in the lower
level of the tree.

1234567Table I: Comparison between ML models. AP column shows the mean statistic of Average Precision values over 50 problem
instances. We conduct student’s t-test by comparing LG-GCN against other baselines. p-value less than 0.05 (a typical signiﬁcance
level) indicates that the LG-GCN is signiﬁcantly better than other ML models.

Problem Size

Model

Independent Set
p-value
AP

Dominant Set
p-value
AP

Vertex Cover
AP

p-value

Small

Medium

102

101

100

10−1

LG-GCN
TRIG-GCN
XGBoost
LR

LG-GCN
TRIG-GCN
XGBoost
LR

96.53
88.62
74.11
74.24

96.41
88.16
73.07
73.99

VCP

-
8.7E-28
3.9E-135
1.6E-128

-
4.8E-31
1.1E-125
2.0E-148

DSP

87.25
86.90
86.45
86.59

87.52
87.18
86.80
86.86

-
8.0E-02
2.6E-01
3.5E-01

-
5.1E-02
2.0E-01
2.3E-01

-
2.8E-87
1.7E-140
2.2E-137

-
2.1E-51
5.2E-67
3.3E-57

98.05
92.42
84.82
85.29

98.24
92.74
84.70
85.33

MISP

102

101

100

10−1

102

101

100

10−1

101

100

10−1

10−2

Combinatorial Auction

AP

46.10
41.77
39.05
41.31

47.07
42.67
40.85
42.35

p-value

-
4.7E-02
1.4E-03
2.3E-02

-
6.0E-04
1.2E-06
1.8E-04

CAP

small

medium

large

small

medium

large

small

medium

large

small

medium

large

LR

XGBoost

LG-GCN

TRIG-GCN

Figure 2: Increase of model prediction time when enlarging the size of problem instances: the y-axis is the prediction time in
seconds in log-scale, and the x-axis is the size of problem instances in 3 scales.

d) Evaluation of Solution Methods: We evaluate PB-
DFS equipped with the best ML model against heuristic
methods as well as problem-reduction approaches on large-
scale problem instances. In the ﬁrst part, PB-DFS is
compared with primal heuristics that do not require a
feasible solution on large problem instances. By examining
the performance of the heuristics enabled by default in
SCIP, four types of heuristics are selected as baselines:
the Feasibility Pump [19] (FP), the Relaxation Enhanced
Neighborhood Search [2] (RENS), a set of 15 diving
heuristics, and a set of 8 rounding heuristics [1]. We allow
PB-DFS to run only at the root node and terminate it
upon the ﬁrst-feasible solution is found. The compared
heuristic methods run multiple times during the B&B
process under a cutoﬀ time of 50 seconds with default
running frequency and termination criteria tuned by SCIP
developers. Generating cutting planes is disabled to best
measure the time of ﬁnding solutions by diﬀerent heuristics.
In the second part, we demonstrate the eﬀectiveness of
PB-DFS by comparing SCIP solver with only PB-DFS
as the primal heuristic against full-ﬂedged SCIP solver
where all heuristics are enabled as well as a problem-
reduction approach by Ding et al. [11]. The problem-
reduction approach splits the root node of the search
tree by a constraint generated from probability vector, and
then solved by SCIP-DEF. To alleviate the eﬀects of ML
predictions, we use the probability vector generated by LG-
GCN for both PB-DFS and ML-Split. The cutoﬀ time is
set to 1000 seconds. Note that for DSP, we employ an

alternative score function zi ← pi. The corresponding DFS
selects the child node that is the result of ﬁxing the decision
variable to one when those nodes are at the same depth.
A comparison of alternative score functions is detailed in
Appendix C.

e) Experimental Environment: We conduct experi-
ments on a cluster with 32 Intel 2.3 GHz CPUs and 128
GB RAM. PB-DFS is implemented using C-api provided by
the state-of-the-art open-source solver, SCIP, version 6.0.1.
The implementations of Logistic Regression and XGBoost
are taken from Scikit-learn [17]. Both LG-GCN and TRIG-
GCN are implemented using Tensorﬂow package [20] where
oﬄine training and online prediction are done by multiple
CPUs in parallel. All solution approaches are evaluated on
a single CPU core. Our code is available online1.

B. Results on Solution Prediction

Table I presents the ML models’ performance on solution
prediction. The small-scale test instances are of the same
size as the training instances and the medium-scale ones
are used for examining the generalization performance of
tested ML models. We cannot measure the classiﬁcation
performance of ML models on large-scale instances because
the optimal solutions for them are not available. By
comparing the mean statistic of AP values, we observe
that LG-GCN is very competitive among all problems
indicated by mean statistics. We conduct student’s t-test

1Code is available at https://github.com/Joey-Shen/pb-dfs.

Table II: Comparison between PB-DFS and primal heuristics.

Problem

Heuristic

Best Solution
Objective

Best Solution
Time

# Instances
no feasible solution

# Calls

VCP (Min.)

DSP (Min.)

MISP (Max.)

CAP (Max.)

FP
Roundings
PB-DFS-LR
PB-DFS-GCN 1629.0 (1628.2)

2137.3
1784.7
1634.0

FP
Roundings
RENS
PB-DFS-LR
PB-DFS-GCN

325.1
515.1
320.6
318.4
318.7 (316.1)

FP
Roundings
Divings
PB-DFS-LR
PB-DFS-GCN 1371.0 (1371.6)

845.8
1225.6
1260.0
1365.8

FP
Divings
Roundings
RENS
PB-DFS-LR
PB-DFS-GCN

-
3633.2
3274.4
3425.9
3018
3280.5 (3582.8)

1.6
35.2
3.8
5.7 (6.0)

4.0
47.9
19.6
9.0
10.6 (23.6)

1.3
38.0
41.5
3.9
4.5 (5.6)

-
26.1
20.4
3.2
4.1
4.5 (12.9)

19
0
0
0

2
0
17
0
0

2
0
0
0
0

30
0
0
0
0
0

1.0
252.0
1.0
1.0

1.0
135.5
1.0
1.0
1.0

1.0
681.0
9.0
1.0
1.0

1.0
36.2
990.5
1.0
1.0
1.0

Heuristic
Total Time

1.0
8.0
3.8
5.7 (21.6)

0.4
0.8
13.6
9.0
10.6 (21.6)

0.9
18.0
6.6
3.9
4.5 (21.4)

0.3
6.1
0.25
1.9
4.1
4.5 (22.3)

by comparing LG-GCN against other baselines where the
AP values for a group of test problem instances of LG-GCN
is compared with that of other ML models. In practice,
p-value less than 0.05 (a typical signiﬁcance level) indicates
that the diﬀerence between the two samples is signiﬁcant.
Therefore, we conﬁrm that the proposed LG-GCN can
predict solutions with better quality as compared to other
ML models on MISP, VCP, and CAP. The performances
of ML models on DSP are comparable. Note that on CAP
which is not originally formulated on graphs, LG-GCN’s
AP value is signiﬁcantly better than TRIG-GCN’s. This
shows that the proposed graph construction method is
more robust when extending to non-graph based problems
as compared to TRIG-GCN. LR is slightly better than
XGBoost overall. All models show their capability to
generalize to larger problem instances.

In addition to prediction accuracy, the prediction time
of an ML model is a part of the total solving time, which
should also be considered for developing eﬃcient solution
methods. Figure 2 shows the increase of the prediction
time when enlarging the problem size for the compared ML
models. Comparing graph-based models, we observe that in
practice the mean prediction time by LG-GCN is much less
than the one by TRIG-GCN. The high computational cost
of TRIG-GCN prevents us from building it with a large
number of layers. The computation time of LG-GCN is
close to those of linear models on VCP and MISP and shifts
away on DSP and CAP. This is understandable because the
complexity of LG-GCN is linear in the number of edges in
a graph [12] and is polynomial with respect to the number
of decision variables, as compared to linear growth for LR
and XGBoost. However, for MISP and VCP where the
constraint coeﬃcient matrix is sparse (i.e., the fraction
of zero values is high), in practice, the diﬀerence in the
growth of computation time with increasing problem size
is not as dramatic, but it may be signiﬁcant when an MIP
instance has a dense constraint matrix, e.g., Combinatorial
Auction Problems.

C. Results For Finding Primal Solutions

Table II shows the computational results of PB-DFS
as compared to the most eﬀective heuristic methods

used in the SCIP solver. Recall that the FP and RENS
are two standalone heuristics. Roundings refers to a set
of 8 rounding heuristics and Divings covers 15 diving
heuristics. PB-DFS-LR and PB-DFS-GCN stand for the
PB-DFS method equipped with LG-GCN and LR model,
respectively. Note that the PB-DFS method only runs at
the root node and terminates upon ﬁnding the ﬁrst feasible
solution. Besides, we analyze an additional criterion using
PB-DFS-GCN, terminating with a cutoﬀ time 20 seconds
(statistics are shown in brackets). For each problem, we run
a heuristic 30 times on diﬀerent instances. Since the SCIP
solver assigns heuristics with diﬀerent running frequencies
based on the characteristic of a problem, we only show
heuristics called at least once per instance. The column
# Instances no feasible solution reports the number of
instances that a heuristic does not ﬁnd a feasible solution.
Other columns show statistics with a geometric mean
shifted by one averaged over instances where a heuristic
ﬁnds at least one feasible solution. Note that we only
consider the best solutions found by a heuristic. Solutions
found by branching are not included. We also show the
average number of calls of a heuristic and the total running
time by a heuristic in the last columns for reference. For
those PB-DFS heuristics, mean prediction time by models
is added to the Best Solution Time and Heuristic Total
Time. Note that the primal heuristics do not meet the
running criteria by SCIP for a certain problem is excluded
from the results.

Overall, the PB-DFS methods can ﬁnd better primal
solutions much earlier on VCP, DSP, and MISP. It is
less competitive on CAP. This is understandable because
the AP value of the prediction for CAP is low (Table I),
indicating the prediction of an ML model is less eﬀective.
Comparing PB-DFS with diﬀerent ML models, PB-DFS-
GCN can ﬁnd better solutions than PB-DFS-LR on VCP,
MISP, and CAP, and they are comparable on DSP. This
means that the AP values of probability vectors for diﬀerent
models can reﬂect the performance of PB-DFS heuristics to
a certain extent. When comparing two termination criteria
using the PB-DFS-GCN model, we observe that giving the
proposed heuristic more time can lead to better solutions.
These results show that the PB-DFS method can be a
very strong primal heuristic when the ML prediction is of
high-quality (i.e., high AP values).

We further demonstrate the eﬀectiveness of PB-DFS by
comparing the use of SCIP equipped with PB-DFS only,
against both the full-ﬂedged SCIP solver (SCIP-DEF) and
a problem-reduction approach [11] using SCIP-DEF as
the solver (ML-Split). Figure 3 presents the change of the
primal bound during the solving process and the detailed
solving statistics are shown in Table III. From Figure 3,
we observe that PB-DFS ﬁnds (near-)optimal solutions
at the very beginning and outperforms other methods on
VCP and MISP. On DSP, early good solutions found by
PB-DFS are still very useful such that it can help the solver
without any primal heuristic outperform full-ﬂedged solvers

VCP (Min.)

DSP (Min.)

MISP (Max.)

CAP (Max.)

1,680

1,660

1,640

320

318

316

1,360

1,340

1,320

3,800

3,600

3,400

0

500

1,000

0

500

1,000

0

500

1,000

0

500

1,000

SCIP-DEF

ML-Split

PB-DFS

Figure 3: Change of the primal bound during computation on large-scale problem instances: the x-axis is the solving time in
seconds, and the y-axis is the objective value of the incumbent.

Table III: PB-DFS compared with SCIP-DEF and ML-Split.

Problem

Method

Best Solution
Objective

Best Solution
Time

Optimality
Gap (%)

Heuristic
Total Time

VCP (Min.)

DSP (Min.)

MISP (Max.)

CAP (Max.)

SCIP-DEF
ML-Split
PB-DFS

SCIP-DEF
ML-Split
PB-DFS

SCIP-DEF
ML-Split
PB-DFS

SCIP-DEF
ML-Split
PB-DFS

1636.4
1630.1
1629.0

315.5
315.3
316.1

1364.9
1370.4
1371.0

3824.3
3821.2
3831.4

542.8
202.7
5.7

390.8
358.9
303.4

559.9
281.6
4.5

535.0
667.5
514.6

3.97
3.62
3.46

3.0
2.95
3.15

4.67
4.34
4.17

11.62
12.01
11.35

198.5
202.8
5.7

114.2
110.1
11.5

174.5
200.4
4.5

29.2
30.8
5.5

for a while. Further, PB-DFS is computationally cheap.
Therefore, when incorporated into the SCIP solver, PB-
DFS does not introduce a large computational overhead,
and keeps more computational resources for the B&B
process. This explains why the quality of the incumbent
solution quickly catches up with other approaches on CAP.
The detailed solving statistics in Table III are consistent
with the above analysis, which conﬁrms that the PB-DFS
method is very competitive

V. Conclusion

In this work, we propose a primal heuristic based on ma-
chine learning, Probabilistic Branching with guided Depth-
First Search (PB-DFS). PB-DFS is a B&B conﬁguration
specialized for boosting the search for high-quality primal
solutions, by leveraging a predicted solution to guide the
search process of the B&B method. Results show that PB-
DFS can ﬁnd better primal solutions, and ﬁnd them much
faster on several NP-hard covering problems as compared
to other general heuristics in a state-of-the-art open-source
MIP solver, SCIP. Further, we demonstrate that PB-DFS
can make better use of high-quality predicted solutions as
compared to recent solution prediction approaches.

We would like to note several promising directions
beyond the scope of this work. Firstly, we demonstrate
that PB-DFS can be deployed as a primal heuristic that
runs only at the root node during a solver’s B&B process.
More sophisticated implementations, e.g. triggering PB-
DFS to run on diﬀerent nodes with engineered timings, can
lead to further performance improvement. Secondly, PB-
DFS relies on high-quality predicted solutions. We observe

the drop in the performances of existing ML models when
extending it to general MIP problems. We expect that
improving ML models in the context of solution prediction
for Mixed Integer Programs could be a fruitful avenue for
future research.

References

[1] T. Achterberg, T. Berthold, and G. Hendel, “Rounding and
propagation heuristics for mixed integer programming,” in
Operations research proceedings 2011. Springer, 2012, pp. 71–76.
[2] T. Berthold, “Primal heuristics for mixed integer programs,”

2006.

[3] M. Fischetti and A. Lodi, “Heuristics in mixed integer pro-
gramming,” Wiley Encyclopedia of Operations Research and
Management Science, 2010.

[4] T. Achterberg, “Scip: solving constraint integer programs,”
Mathematical Programming Computation, vol. 1, no. 1, pp. 1–41,
2009.

[5] E. Aarts, E. H. Aarts, and J. K. Lenstra, Local search in
combinatorial optimization. Princeton University Press, 2003.
[6] T. Berthold, “Rens,” Mathematical Programming Computation,

vol. 6, no. 1, pp. 33–54, 2014.

[7] M. Gasse, D. Chételat, N. Ferroni, L. Charlin, and A. Lodi,
“Exact combinatorial optimization with graph convolutional
neural networks,” in Advances in Neural Information Processing
Systems, 2019, pp. 15 554–15 566.

[8] H. He, H. Daume III, and J. M. Eisner, “Learning to search in
branch and bound algorithms,” in Advances in neural informa-
tion processing systems, 2014, pp. 3293–3301.

[9] E. B. Khalil, B. Dilkina, G. L. Nemhauser, S. Ahmed, and
Y. Shao, “Learning to run heuristics in tree search.” in IJCAI,
2017, pp. 659–666.

[10] Y. Sun, X. Li, and A. Ernst, “Using statistical measures and
machine learning for graph reduction to solve maximum weight
clique problems,” IEEE transactions on pattern analysis and
machine intelligence, 2019.

[11] J.-Y. Ding, C. Zhang, L. Shen, S. Li, B. Wang, Y. Xu, and L. Song,
“Accelerating primal solution ﬁndings for mixed integer programs
based on solution prediction,” arXiv preprint arXiv:1906.09575,
2019.

[12] T. N. Kipf and M. Welling, “Semi-supervised classiﬁcation with
graph convolutional networks,” arXiv preprint arXiv:1609.02907,
2016.

[13] V. Nair, S. Bartunov, F. Gimeno, I. von Glehn, P. Lichocki,
I. Lobov, B. O’Donoghue, N. Sonnerat, C. Tjandraatmadja,
P. Wang et al., “Solving mixed integer programs using neural
networks,” arXiv preprint arXiv:2012.13349, 2020.

[14] C. M. Bishop, Pattern recognition and machine learning.

springer, 2006.

[15] V. Nair and G. E. Hinton, “Rectiﬁed linear units improve

restricted boltzmann machines,” in ICML, 2010.

[16] T. Chen and C. Guestrin, “Xgboost: A scalable tree boosting
system,” in Proceedings of the 22nd acm sigkdd international
conference on knowledge discovery and data mining, 2016, pp.
785–794.

[17] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg
et al., “Scikit-learn: Machine learning in python,” the Journal of
machine Learning research, vol. 12, pp. 2825–2830, 2011.
[18] M. Zhu, “Recall, precision and average precision,” Department
of Statistics and Actuarial Science, University of Waterloo,
Waterloo, vol. 2, p. 30, 2004.

[19] M. Fischetti, F. Glover, and A. Lodi, “The feasibility pump,”

Mathematical Programming, vol. 104, no. 1, pp. 91–104, 2005.

[20] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean,
M. Devin, S. Ghemawat, G. Irving, M. Isard et al., “Tensorﬂow:
A system for large-scale machine learning,” in 12th {USENIX}
Symposium on Operating Systems Design and Implementation
({OSDI} 16), 2016, pp. 265–283.

[21] K. Leyton-Brown, M. Pearson, and Y. Shoham, “Towards a
universal test suite for combinatorial auction algorithms,” in
Proceedings of the 2nd ACM conference on Electronic commerce,
2000, pp. 66–76.

Appendix A

The features for decision variables are outlined as follows:

• original, positive and negative objective coeﬃcients;
• number of non-zero, positive, and negative coeﬃcients

in constraints;

• variable LP solution of the original problem ˆx; ˆx − bˆxc;
dˆxe− ˆx; a boolean indicator for whether ˆx is fractional;
• variable’s upward and downward pseudo costs; the
ratio between these pseudocosts; sum and product of
these pseudo costs; variable’s reduced cost;

• global lower bound and upper bound;
• mean, standard deviation, minimum, and maximum
degree for constraints in which the variable has a
non-zero coeﬃcient. The degree of a constraint is the
number of non-zero coeﬃcients of that constraint;
• the maximum and the minimum ratio between the left-
hand-side and right-hand-side over constraints where
the variable has a non-zero coeﬃcient;

• statistics (sum, mean, standard deviation, maximum,
minimum) for a variable’s positive and negative con-
straint coeﬃcients respectively;

• coeﬃcient statistics of all variables in the constraints
(sum, mean, standard deviation, maximum, minimum)
with respect to three weighting schemes: unit weight,
dual cost, the inverse of the sum of the coeﬃcients in
the constraint.

Appendix B

The MIP formulations for tested problems are as follows:
a) Maximum Independent Set Problem (MISP): In
an undirected graph G(V, E), a subset of nodes S is
independent iﬀ there is no edge between any pair of nodes
in S. A MISP is to ﬁnd an independent set in G of
maximum cardinality. The MIP formulation of the MISP
is: maxx
v∈V xv, subject to xu + xv ≤ 1, ∀(u, v) ∈ E and
xv ∈ 0, 1, ∀v ∈ V.

P

Table IV: Eﬀect of Score Functions

Independent Set

Dominant Set

Vertex Cover

Combinatorial Auction

score function

objective

time

objective

time

objective

time

objective

time

p
1 − p
max(p, 1 − p)

1371.0
1371.1
1371.0

2.8
3.7
3.8

318.7
322.0
321.9

9.9
11.3
11.6

1629.0
1629.1
1628.9

3.9
3.3
3.9

3226.6
3317.0
3316.0

2.3
5.2
5.2

b) Dominant Set Problem (DSP): In an undirected
graph G(V, E), a subset of nodes S ⊂ V dominates the
complementary subset V \ S iﬀ every node not in S is
adjacent to at least one node in S. The objective of a DSP
is to ﬁnd a dominant set in G of minimum cardinality. The
MIP of the DSP is as follows: minx
v∈V xv, subject to
xv + P
u∈N (v) xu ≥ 1, ∀v ∈ V and xv ∈ 0, 1, ∀v ∈ V. N (v)
denotes the set of neighborhood nodes of v.

P

c) Vertex Cover Problem (VCP): In an undirected
graph G(V, E), a subset of nodes S ⊂ V is a cover of G
iﬀ for every edge e ∈ E, there is at least one endpoint in
S. The objective of the VCP is to ﬁnd a cover set in G of
minimum cardinality. The MIP of the VCP is as follows:
v∈V xv, subject to xu + xv ≥ 1, ∀(u, v) ∈ E and
minx
xv ∈ 0, 1, ∀v ∈ V.

P

d) Combinatorial Auction Problem (CAP): A seller
faces to selectively accept oﬀers from bidders. Each oﬀer
indexed by i contains a bid pi for a set of items (bundle)
Ci by a particular bidder. The seller has limited amount
of goods and aims to allocate the goods in a way that
maximizes the total revenue. We use I and J to denote
the index set of oﬀers and the index set of items. Formally,
the MIP formulation of the problem can be expressed as
maxx
k∈{i | j∈Ci,∀i} xk ≤ 1, ∀j ∈ J
and xi ∈ 0, 1, ∀i ∈ I.

i∈I pixi, subject to P

P

For MISP, DSP, and VCP, we sample random graphs
using Erdős-Rényi generator. The aﬃnity is set to 4. The
training data consists of examples from solved small graph
instances between 500 to 1001 nodes. We form the small-
scale and medium-scale testing datasets with solved graph
instances of 1000 nodes and those of 2000 nodes. We
evaluate heuristics on large-scale graph instances of 3000
nodes. The CAP instances are generated using an arbitrary
relationship procedure. The CAP instances in small-scale
are sampled with items in the range [100, 150] and bids in
the range [500, 750]. Instances in medium-scale and large-
scale are generated with 150 items for 750 bids and 200
items for 1000 bids, respectively. The detailed parameter
settings are given by following [21].

Appendix C

We consider two alternative score functions, zi ← pi
and zi ← 1 − pi, subject to i ∈ C. When using zi ← pi as
the score function to select the decision variable with the
maximum score to branch, our guided DFS always prefers
the node that is the result of ﬁxing the decision variable to
1. The behavior of the PB-DFS due to this score function
can be interpreted as incrementally adding variables that
more likely belong to an optimal solution until a feasible
solution is obtained. When using the other score function

zi ← 1 − pi, guided DFS always prefers the node that is the
result of ﬁxing the decision variable to 0, and the resulting
behavior of the PB-DFS can be interpreted as continuously
removing variables that less likely belong to an optimal
solution until a feasible solution is obtained. In table IV,
we observe that the score functions do not signiﬁcantly
aﬀect the ﬁrst-found solution.

