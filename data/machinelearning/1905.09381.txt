Learning to Prove Theorems via Interacting with Proof Assistants

Kaiyu Yang 1 Jia Deng 1

9
1
0
2

y
a
M
1
2

]

O
L
.
s
c
[

1
v
1
8
3
9
0
.
5
0
9
1
:
v
i
X
r
a

Abstract

Humans prove theorems by relying on substan-
tial high-level reasoning and problem-speciï¬c in-
sights. Proof assistants offer a formalism that
resembles human mathematical reasoning, repre-
senting theorems in higher-order logic and proofs
as high-level tactics. However, human experts
have to construct proofs manually by entering
In this paper,
tactics into the proof assistant.
we study the problem of using machine learn-
ing to automate the interaction with proof as-
sistants. We construct CoqGym, a large-scale
dataset and learning environment containing 71K
human-written proofs from 123 projects devel-
oped with the Coq proof assistant. We develop
ASTactic, a deep learning-based model that gen-
erates tactics as programs in the form of abstract
syntax trees (ASTs). Experiments show that AS-
Tactic trained on CoqGym can generate effective
tactics and can be used to prove new theorems
not previously provable by automated methods.
Code is available at https://github.com/
princeton-vl/CoqGym.

1. Introduction

Given the statement of a theorem, simply push a button, and
the proof comes out. If this fantasy of automated theorem
proving (ATP) were true, it would impact formal mathe-
matics (McCune, 1997), software veriï¬cation (Darvas et al.,
2005), and hardware design (Kern & Greenstreet, 1999). In
reality, however, state-of-the-art theorem provers are still far
behind human experts on efï¬ciently constructing proofs in
large-scale formal systems.

Consider this theorem: 0+1+2+
. As hu-
mans, we decide to prove by induction on n. After solving
the trivial case (n = 0), we complete the proof by applying

Â· Â· Â·

+n = n(n+1)

2

1Department of Computer Science, Princeton University. Cor-
respondence to: Kaiyu Yang <kaiyuy@cs.princeton.edu>, Jia
Deng <jiadeng@cs.princeton.edu>.

Proceedings of the 36 th International Conference on Machine
Learning, Long Beach, California, PMLR 97, 2019. Copyright
2019 by the author(s).

the induction hypothesis and simplifying the resulting for-
mula. Simple as it is, the proof requires understanding the
concepts (natural numbers, addition), mastering the proof
techniques (induction), as well as problem-speciï¬c insights
to drive the decisions we made.

What a typical theorem prover does, however, is to prove by
resolution refutation: It converts the premises and the nega-
tion of the theorem into ï¬rst-order clauses in conjunctive
normal form (CNF). It then keeps generating new clauses by
applying the resolution rule until an empty clause emerges,
yielding a proof consisting of a long sequence of CNFs and
resolutions. While this provides a universal procedure, the
CNF representation of simple formulas can be long, compli-
cated and inscrutable, making it difï¬cult to beneï¬t from the
higher-level abstraction and manipulation that is common
to human mathematical reasoning.

To work around the difï¬culties of ATP in practical appli-
cations, interactive theorem proving (ITP) (Harrison et al.,
2014) incorporates humans in the loop. In ITP, human users
deï¬ne mathematical objects formally and prove theorems
semi-automatically by entering a sequence of commands
called tactics. The tactics capture high-level proof tech-
niques such as induction, leaving low-level details to the
software referred to as proof assistants. A successful se-
quence of tactics is essentially a proof written in the lan-
guage of the proof assistant. It can also be viewed as a
program that is executed by the proof assistant.

ITP relies on humans in the loop, which is labor-intensive
and hinders its wider adoption. However, at the same time,
it is a blessing in disguise, in that it opens up a route to full
automationâ€”human experts have written a large amount
of ITP code, which provides an opportunity to develop
machine learning systems to imitate humans for interacting
with the proof assistant. Indeed, some recent efforts have
attempted to learn to generate tactics from human-written
proofs and have obtained promising results (Gransden et al.,
2015; Gauthier et al., 2018; Bansal et al., 2019).

However, existing approaches to the â€œauto-ITPâ€ problem
suffer from two limitations. One is the lack of a large-scale
dataset. Prior work was trained and evaluated on no more
than a few thousands of theorems (Gransden et al., 2015;
Gauthier et al., 2018; Huang et al., 2019), likely insufï¬cient
for data-hungry approaches such as deep learning. The other

 
 
 
 
 
 
Learning to Prove Theorems via Interacting with Proof Assistants

is the limited ï¬‚exibility of the learned models in generating
tactics. A tactic can be a sophisticated line of code with
functions, arguments, and compound expressions, and the
space of possible tactics is inï¬nite. Prior work has limited
ï¬‚exibility because they generate tactics by copying from a
ï¬xed, predetermined set (Gransden et al., 2015; Gauthier
et al., 2018; Huang et al., 2019; Bansal et al., 2019), and are
thus unable to generate out-of-vocabulary tactics unseen in
the training data.

In this work we address these limitations by making two
contributions: a large-scale dataset and a new method for
tactic generation that is more ï¬‚exible and adaptive.

CoqGym: A large-scale ITP dataset and learning envi-
ronment We construct CoqGym, a dataset and learning
environment for theorem proving in proof assistants. It in-
cludes 71K human-written proofs from 123 open-source
software projects in the Coq proof assistant (Barras et al.,
1997), covering a broad spectrum of application domains,
including mathematics, computer hardware, programming
languages, etc. Our dataset is much larger and more diverse
than existing datasets, which consist of only a few thou-
sands of theorems and cover only a handful of domains such
as Peano arithmetic (Dixon & Fleuriot, 2003) or the Feitâ€“
Thompson theorem (Gonthier et al., 2013). The scale and
diversity of our dataset facilitate training machine learning
models and the evaluating cross-domain generalization.

The learning environment of CoqGym is designed for train-
ing and evaluating auto-ITP agents. The agent starts with
a set of premises and a goal (theorem) to prove; it interacts
with the proof assistant by issuing a sequence of tactics. The
proof assistant executes each tactic and reports the results
back in terms of a set of new goals. The agent succeeds
when no more goals exist.

To make the challenging task more amenable to learning, we
augment CoqGym with shorter proofs. They are synthesized
from the intermediate steps of the original human-written
proofs and may serve as additional training data.

ASTactic: A new method for tactic generation We de-
velop ASTactic, a deep learning model for tactic generation
that is more ï¬‚exible and adaptive than prior work. It gener-
ates tactics as programs by composing abstract syntax trees
(ASTs) in a predeï¬ned grammar using the tokens available
at runtime. To our knowledge, this is the ï¬rst time learning-
based AST generation is applied in the context of interactive
theorem proving.

ASTactic takes in a goal and a set of premises expressed as
terms in Coqâ€™s language. It outputs a tactic as an AST in a
subset of Coqâ€™s tactic language.

Experimental results on CoqGym show that ASTactic can
generate effective tactics. It can successfully prove 12.2%
of the theorems in the test set, signiï¬cantly outperforming

the built-in automated tactics (auto, intuition, easy,
in Coq (4.9%). More importantly, our model can
etc.)
be combined with state-of-art ATP systems (De Moura &
BjÃ¸rner, 2008; Barrett et al., 2011; KovÂ´acs & Voronkov,
2013; Schulz, 2013) and boost the success rate further to
30.0%, which shows that our model has learned effective
higher level tactics and has proved new theorems not previ-
ously provable by automatic systems.

Contributions
In summary, our contributions are two-
fold. First, we build CoqGymâ€”a large-scale dataset and
learning environment for theorem proving via interacting
with a proof assistant. Second, we develop ASTactic, a deep
learning model that learns to generate tactics as abstract
syntax trees and can be used to prove new theorems beyond
the reach of previous automatic provers.

theorem proving Modern

2. Related Work
Automated
theorem
provers (KovÂ´acs & Voronkov, 2013; Schulz, 2013)
represent theorems in ï¬rst-order logic and search for proofs
in resolution-based proof calculi. The proof search has been
signiï¬cantly improved by machine learning (Irving et al.,
2016; Wang et al., 2017; Urban et al., 2011; Bridge et al.,
2014; Loos et al., 2017; Kaliszyk et al., 2018; RocktÂ¨aschel
& Riedel, 2017). However, it remains a challenging task
due to the large search space; as a result, state-of-the-art
provers do not scale to large problems.
In contrast to
traditional ï¬rst-order provers, we focus on theorem proving
in the Coq proof assistant, which represents theorems and
manipulates proofs at a higher level, offering the unique
opportunity of learning from human proofs.

Some proof assistants allow a user to use existing ATP
systems directly. For example, Sledgehammer (Paulson &
Blanchette, 2010) translates theorems in the Isabelle proof
assistant (Paulson, 1994) to ï¬rst-order logic. It then proves
the theorems using external provers and converts the proof
back to Isabelleâ€™s tactics. Similar â€œhammersâ€ were devel-
oped for other proof assistants as well (Kaliszyk & Urban,
2014; Urban, 2004; Czajka & Kaliszyk, 2018). The hammer-
based approach essentially bypasses the proof assistant and
outsources the work to external ATPs. In contrast, our model
learns to prove theorems within the proof assistant using
native tactics without hammers.

Learning to interact with proof assistants There have
been relatively few works in learning to interact with proof
assistants. SEPIA (Gransden et al., 2015) learns from ex-
isting Coq proofs a ï¬nite-state automaton, where each tran-
sition corresponds to a tactic, and each path corresponds
to a sequence of tactics. During test time, it samples tac-
tic sequences deï¬ned by this automaton. Note that SEPIA
can only choose from a ï¬nite set of tactics, and it tries the
same tactics regardless of the theorem. That is, it will apply

Learning to Prove Theorems via Interacting with Proof Assistants

the tactic â€œapply xâ€ even when x is not a valid term in
the current context. In contrast, our model can generate an
inï¬nite number of tactics tailored to the current context.

TacticToe (Gauthier et al., 2018) generates tactics by re-
trieving from the training data a small number of candidate
tactics that have been used for theorems similar to the cur-
rent goal. Each candidate is treated as a possible action
and evaluated by a learned value function and by Monte
Carlo Tree Search. Although more adaptive than SEPIA,
the generated tactics are still chosen from a ï¬xed set with
predetermined arguments and may have difï¬culty generaliz-
ing to new domains with out-of-vocabulary terms.

FastSMT (Balunovic et al., 2018) generates tactics to inter-
act with the Z3 SMT solver (De Moura & BjÃ¸rner, 2008),
which determines the satisï¬ability of a certain class of logi-
cal formulas. Compared to our model, FastSMT uses sub-
stantially simpler tacticsâ€”all of them have only boolean
and integer arguments, whereas tactics in Coq can have
compound expressions as arguments. As a result, the ap-
proach of FastSMT does not output ASTs and is not directly
applicable to our setting.

Datasets for theorem proving Our work is related to
many previous theorem proving datasets (Sutcliffe, 2009;
Bancerek et al., 2015; Gransden et al., 2015; Gauthier et al.,
2018; Huang et al., 2019). Our work differs from prior
work in that we focus on theorems in higher-order logic
and proofs consisting of high-level tactics as opposed to
ï¬rst-order logic and low-level proofs, and we aim for larger
scale and more diverse domains.

The closest prior work is GamePad (Huang et al., 2019).
GamePad includes a tool for interacting with Coq as well
as data collected from the proof of the Feit Thompson theo-
rem (Gonthier et al., 2013). We have independently devel-
oped a similar tool in CoqGym, but we aim for a larger-scale
dataset. Our dataset contains 71K theorems, much more
than the 1,602 theorems in Feitâ€“Thompson, and our the-
orems come from a broad spectrum of 123 Coq projects.
Another difference is that we augment our dataset with syn-
thetic proofs extracted from the intermediate steps of human
proofs. Finally, in terms of tactic generation, we gener-
ate complete tactics that can be used to obtain full proofs,
whereas they group all tactics into categories and only pre-
dict the category, not the speciï¬c tactic. Also, they do not
predict the location of the arguments in the tactics. Their
method for full proof generation is speciï¬c to an algebraic
rewrite problem, which has only two possible tactics, each
with two integer arguments. Their model predicts one tactic
(out of 2) and two integers, and is not directly applicable to
our setting.

HOList (Bansal et al., 2019) is a concurrent work introduc-
ing a dataset and learning environment for the HOL Light
proof assistant (Harrison, 1996). HOList consists of 29K

proofs solely from the formalization of the Kepler conjec-
ture (Hales et al., 2017), a theorem in discrete geometry,
whereas CoqGym covers more diverse domains including
not only pure mathematics but also computer systems. Simi-
lar to ours, HOList also introduces a model for tactic genera-
tion. But unlike ours, their method does not generate tactics
in the form of ASTs.

Representing and generating programs Our model
builds upon prior work that uses deep learning to repre-
sent and generate programs. A key ingredient is using a
deep network to embed a program into a vector, by treat-
ing the program as a sequence of tokens (Allamanis et al.,
2016b) or using structured input such as ASTs (Allamanis
et al., 2016a; Alon et al., 2018) or graphs (Allamanis et al.,
2017). We use a TreeLSTM (Tai et al., 2015) on ASTs to
embed the input goal and premises.

Similarly, a program can be generated by a deep network as
a sequence of tokens (Hindle et al., 2012), an AST (Parisotto
et al., 2016) or a graph (Brockschmidt et al., 2019). We
adopt the framework of Yin & Neubig (2017) to gener-
ate tactics in the form of ASTs, conditioned on the goal
and premises. However, in our speciï¬c task, we face
the unique challenge of synthesizing the tactic arguments,
which are subject to various semantic constraints, e.g., the
H in â€œapply Hâ€ must be a valid premise in the current
context. Unlike the purely syntactic approach of Yin & Neu-
big (2017), our model utilizes the semantic constraints to
narrow down the output space.

3. Background on Coq

Coq (Barras et al., 1997) is a proof assistant with an ac-
tive community and diverse applications. It has been used
to develop certiï¬ed software and hardware (Leroy, 2009;
Paulin-Mohring, 1995), and to prove theorems in mathemat-
ics, including the Feitâ€“Thompson theorem (Gonthier et al.,
2013). Under the hood of Coq are two pieces of machinery:
a functional language for representing mathematical objects,
theorems, and proofs, and a mechanism for constructing
machine-checked proofs semi-automatically.

Coq allows us to deï¬ne mathematical objects such as sets,
functions, and relations. For example, we can deï¬ne the set
of natural numbers (nat) and the addition operation (add) in
Fig. 1 (Left). These are examples of terms in Coqâ€™s language.
The runtime environment of COq contains a set of current
terms, including both user-deï¬ned terms and predeï¬ned
terms from the Coq standard library. These terms are used
to formalize theorems. As in Fig. 1, we state the theorem
a b c : nat, (a + b) + c = a + (b + c) using nat and add.

âˆ€
Theorem proving in Coq is a backward process. The user
starts with the theorem itself as the initial goal and repeat-
edly apply tactics to decompose the goal into a list of sub-

Learning to Prove Theorems via Interacting with Proof Assistants

Figure 1. Left: A simple Coq proof for the associativity of the addition operation on natural numbers. Right: The proof tree generated by
Coq when executing this proof. A Coq proof consists of a sequences of tactics. We start with the original theorem and apply tactics to
decompose the current goal to sub-goals. This process generates a proof tree whose nodes are goals and whose edges are tactics.

goals (can be an empty list). The proof is complete when
there are no sub-goals left. Proving is a process of trial and
error; the user may try a tactic to decompose the current
goal, analyze the feedback from Coq, and backtrack to the
previous step to try a different tactic.

A successful Coq proof implicitly generates a proof tree
whose root is the original theorem and whose nodes are
goals (Fig. 1 Right). All goals share the same environment,
but have a unique local context with premises local to each
goal, such as the induction hypothesis IHaâ€™ in Fig. 1.

The edges of the proof tree are tactics; they can be simple
strings, can have arguments at various positions, and can
be combined into compound tactics. For example, simpl
simpliï¬es the current goal, â€œapply Hâ€ applies a premise H,
and â€œsimpl; apply Hâ€ performs these two operations
sequentially. The space of all valid tactics is described by
Ltac, Coqâ€™s tactic language (Delahaye, 2000).

From a machine learning perspective, theorem proving in
Coq resembles a task-oriented dialog (Bordes et al., 2016).
The agent interacts with the proof assistant for completing
the proof. At each step, the agent perceives the current goals,
their local context, and the global environment; it then gen-
erates an appropriate tactic, which is an expression in Ltac.
Methods for task-oriented dialogues have been based on
supervised learning (Bordes et al., 2016) or reinforcement
learning (Liu et al., 2017). CoqGymprovides human-written
proofs as supervision for training dialog agents. It also al-
lows reinforcement learning on this task when combined
with the tool for interacting with Coq.

4. Constructing CoqGym

CoqGym includes a large-scale dataset of 71K human-
written proofs from 123 open-source software projects in
Coq. In addition to the source ï¬les, we provide abstract
syntax trees (ASTs) and rich runtime information of the
proofs, including the environments, the goals, and the proof
trees. Furthermore, we propose a novel mechanism for turn-

ing intermediate goals into theorems and synthesizing the
corresponding proofs. These synthetic proofs may serve as
additional training data. Further details are in Appendix A.

Processing Coq projects and ï¬les The source ï¬les are
organized into projects, which contain a set of inter-related
proofs about speciï¬c domains. The projects in CoqGym
include the Coq standard library and the packages listed on
the Coq Package Index1. Some of them may not compile
because they require a speciï¬c version of Coq, or there is
a missing dependency. We only include the projects that
compile.

These projects are split into a training set, a validation set,
and a test set. This ensures that no testing proof comes from
a project that is used in training, and makes the dataset suit-
able for measuring how well the models generalize across
various domains. There are 43,844 proofs for training,
13,875 proofs for validation and 13,137 proofs for testing.

We extract the ASTs from the internals of Coqâ€™s interpreter.
They are OCaml datatypes. We serialize them into Lisp-
style S-expressions (McCarthy, 1960) and provide tools for
using them in Python.

Environments, goals, and proof trees Proofs are situ-
ated in environments containing Coq terms as premises. We
could use the source code to represent the environments, as
the code completely deï¬nes the environment. However, this
is problematic for machine learning models, as it burdens
them with learning the semantics of Coq code. Instead, we
represent the environments as a collection of kernel terms,
internal representations used by Coq stripped of syntactic
sugar. This is achieved by executing the proofs and serializ-
ing Coqâ€™s internals.

In contrast to prior work (Huang et al., 2019), CoqGym
supplies the complete environment for each proofâ€”all the
premises in the scope, including those deï¬ned in the same
source ï¬le and those imported from other libraries. Having

1https://coq.inria.fr/packages

EnvironmentTheoremProofâˆ€abc:nat,(a+b)+c=a+(b+c)Local contextGoala,b,c:nat(a+b)+c=a+(b+c)b,c:nat(0+b)+c=0+(b+c)a(cid:31),b,c:natIHa(cid:31):(a(cid:31)+b)+c=a(cid:31)+(b+c)S(a(cid:31)+(b+c))=S(a(cid:31)+(b+c))(Sa(cid:31)+b)+c=Sa(cid:31)+(b+c)a(cid:31),b,c:natIHa(cid:31):(a(cid:31)+b)+c=a(cid:31)+(b+c)introsTacticinduction a as [|aâ€™]trivialtrivialsimple; rewrite IHaâ€™Proof treeLearning to Prove Theorems via Interacting with Proof Assistants

the complete environment is important because it allows the
machine learning model to access all relevant information
in structured forms.

We represent each proof as a proof tree, where a node is
a goal along with its local context, and an edge is a tactic
decomposing the goal into sub-goals. At each step in a
proof, we serialize the current goals from Coqâ€™s interpreter
and identify the edges in the proof tree by tracking how
goals emerge and disappear during the lifetime of the proof.

Environments, goals, and proof trees together form a struc-
tured representation of Coq proofs. Compared to raw source
code, a structured representation allow machine learning
models to more easily exploit the syntactic and semantic
structures. It is worth noting that this structured represen-
tation is nontrivial to extract because Coq does not provide
APIs exposing its internals. In constructing CoqGym, we
modify Coq and use SerAPI (Gallego Arias, 2016) to se-
rialize the runtime information, without touching the core
proof-checking module of Coq so as to not compromise the
correctness of the proofs.

Synthetic proofs from intermediate goals Human-
written proofs can be long and complex, making them dif-
ï¬cult to learn from. We thus generate shorter proofs from
the intermediate goals inside a long proof. We hypothesize
that these intermediate goals are easier to prove and more
conducive to learning. This also augments the training data
with more examples.

For each intermediate goal in a human-written proof, we
generate synthetic proofs of length 1, 2, 3, and 4. We detail
the generation process in Appendix A.

Dataset statistics CoqGym has 70,856 human-written
proofs from 123 Coq projects. On average, each proof has
8.7 intermediate goals and 9.1 steps; each step has 10,350.3
premises in the environment and 5.6 premises in the local
context; each tactic has 2.0 tokens, and the height of its
AST is 1.9. Among all tactics, 53% of them contain at least
one argument. Note that these statistics vary signiï¬cantly
across different projects. For example, the average number
of premises of a theorem is 13,340 in the CompCert project,
but only 661 in the InfSeqExt project.

For the synthetic proofs, we have extracted 159,761 proofs
of 1 step; 109,602 proofs of 2 steps; 79,967 proofs of 3 steps
and 61,126 proofs of 4 steps.

5. ASTactic: generating tactics as programs

We propose a method that proves theorems by interact-
ing with Coq. At the core of our method is ASTacticâ€”
a deep learning model that generates tactics as programs.
Compared to prior work that chooses tactics from a ï¬xed
set (Huang et al., 2019; Gransden et al., 2015; Gauthier et al.,

2018; Bansal et al., 2019), we generate tactics dynamically
in the form of abstract syntax trees (ASTs) and synthesize
arguments using the available premises during runtime. At
test time, we sample multiple tactics from the model. They
are treated as possible actions to take, and we search for a
complete proof via depth-ï¬rst search (DFS).

Space of tactics The output space of our model is speci-
ï¬ed by a context-free grammar (CFG) that is ï¬xed during
training and testing. Statistics of CoqGym show that many
valid tactics (Delahaye, 2000) are seldom used in proofs.
Therefore we simplify the tactic grammar to facilitate learn-
ing, at the expense of giving up on some cases. Note that
these are design choices of the model, not the datasetâ€”We
train the model to generates tactics only in the simpliï¬ed
space, but the theorems in the testing data can require tactics
outside this space.

We only generate atomic tactics while excluding compound
ones such as â€œtac1; tac2â€. This is not a severe handicap
because all proofs can be completed without compound
tactics. For each type of tactic, we count its number of
occurrences in CoqGym and manually include the common
ones in our tactic space. When a tactic requires a Coq term
as its argument, we constrain the term to be an identiï¬er.
We also exclude user-deï¬ned tactics. The complete CFG is
in Appendix B.

Overall model architecture ASTactic has an encoder-
decoder architecture (Fig. 2). The input and output of
the model are both trees. The encoder embeds all input
Coq terms: the goal and the premises expressed in ASTs.
Conditioned on the embeddings, the decoder generates a
program-structured tactic by sequentially growing an AST.

We follow prior works (Tai et al., 2015; Yin & Neubig, 2017)
for encoding and decoding trees. The unique challenge in
our task is to synthesize tactic arguments. In the decoder,
we incorporate semantic constraints on the arguments to
narrow down the search space.

Encoding the goal and premises ASTactic encodes the
current goal and premises into vectors. We include the entire
local context and up to 10 premises in the environment, ex-
cluding a large number of premises imported from libraries.
A model could be more powerful if it is capable of selecting
relevant premises from the entire environment, but that is
left for future research.

Both the goal and the premises are Coq terms in the form of
AST (Fig. 2 Left), and we encode them using a TreeLSTM
network (Tai et al., 2015). Speciï¬cally, each node in an AST
has a symbol n indicating its syntactical role. The network
associate each node with a hidden state h and a memory
cell c which are updated by its children as follows:

(c, h) = fupdate(n, c1,

, cK,

Â· Â· Â·

K
(cid:88)

i=1

hi),

Learning to Prove Theorems via Interacting with Proof Assistants

Figure 2. The architecture of ASTactic. It generates a tactic AST conditioned on the input Coq terms by sequentially expanding a partial
tree. Here we illustrate a single expansion step of the non-terminal node in clause. The ASTs of the input terms (Left) are encoded
into feature vectors by a TreeLSTM network (Middle). A GRU controller then combines them with the information in the partial tree. It
updates the decoder state st and uses st to predict the production rule to apply. In this example, the tactic AST is complete (rewrite
IHaâ€™) after expanding the current node.

where the update function fupdate is the child-sum variant of
TreeLSTM, n is the symbol of the node in one hot encoding,
and ci and hi are the states of the children.

attention mask on the premises, which selectively attends to
the relevant premises for the current generation step. The
mask is then used to retrieve ut:

We perform this computation bottom-up and represent the
entire tree by hroot, the hidden state of the root. Finally,
we append hroot with a 3-dimensional one hot vector; it
indicates whether the term is the goal, a premise in the
environment, or a premise in the local context.

Tracking the decoder state Conditioned on the input em-
beddings, the decoder (Fig. 2 Right) follows the method in
Yin & Neubig (2017) to generate program-structured tactics
as ASTs. It begins with a single node and grows a partial
tree in the depth-ï¬rst order. At a non-terminal node, it ex-
pands the node by choosing a production rule in the CFG
of the tactic space. At a terminal node, it emits a token
corresponding to a tactic argument.

This sequential generation process is controlled by a gated
recurrent unit (GRU) (Cho et al., 2014), whose hidden state
is updated by the input embeddings and local information
in the partially generated AST.

Formally, we have learnable embeddings for all symbols
and production rules in the tactic grammar. At time step
t, let nt be the symbol of the current node; at
1 is the
production rule used to expand the previous node; pt is
the parent nodeâ€™s state concatenated with the production
rule used to expand the parent; g is the goal, which is ï¬xed
during the generation process. The state st is updated by:

âˆ’

st = fGRU (st

1, [at

1 : pt : nt : g : ut])

âˆ’

âˆ’

(1)

wi = fatt(st

1 : ri)

âˆ’
(cid:88)

ut =

wiri

(2)

(3)

i
where ri is the ith premise and wi is its weight. fatt is a
two-layer fully-connected network.

Expanding ASTs and synthesizing arguments The
state st determines how to expand the tree including which
production rules to apply and which tokens to generate.

To select a production rule, we model the probabilities for
the rules as:

pt = softmax(WR

f (st)),

(4)

Â·

where f is a linear layer followed by tanh, and WR is the
embedding matrix for production rules. We expand the node
using the applicable rule with the largest probability.

The tokens in the ASTs correspond to the tactic arguments.
Synthesizing them is challenging because the syntactic out-
put space is large: all valid identiï¬ers in Coq. However,
there are strong semantic constraints on the arguments. For
example, the tactic â€œapply Hâ€ applies a premise H to the
goal. The argument H must be a valid premise either in the
environment or in the local context.

To leverage the semantic constraints in synthesizing argu-
ments, we group arguments into categories and take different
actions for each category.

where â€œ:â€ denotes vector concatenation. The ut above is
a weighted sum of premises. We use st
1 to compute an

âˆ’

â€¢

Identiï¬ers of premises (as in â€œapply Hâ€): We score
each premise using st in the same way as computing

in_clause : â€œâ€                | â€œinâ€ LOCAL_IDENT| â€œin |-*â€| â€œin *â€EnvironmentLocal contextGoalğ‘›ğ‘ğ‘¡âˆ¶	ğ‘‡ğ‘¦ğ‘ğ‘’ğ‘‚âˆ¶ğ‘›ğ‘ğ‘¡ğ‘†âˆ¶ğ‘›ğ‘ğ‘¡	â†’ğ‘›ğ‘ğ‘¡ğ‘ğ‘‘ğ‘‘âˆ¶ğ‘›ğ‘ğ‘¡	â†’	ğ‘›ğ‘ğ‘¡	â†’ğ‘›ğ‘ğ‘¡ğ‘.,ğ‘,ğ‘âˆ¶ğ‘›ğ‘ğ‘¡ğ¼ğ»ğ‘.âˆ¶ğ‘.+ğ‘+ğ‘=ğ‘.+(ğ‘+ğ‘)ğ‘†ğ‘.+ğ‘+ğ‘=ğ‘†ğ‘.+(ğ‘+ğ‘)TreeLSTMTermParseFeature vectorInput Coq termsFeature vectorsTerm encoderAttention moduleGRU}ğ’”ğ’•:ğŸğ’–ğ’•ğ’ˆğ’”ğ’•tactic_exprrewrite_term_list1rewrite_termQUALIDin_clauseğ’‚ğ’•:ğŸğ’‘ğ’•ğ’ğ’•rewriteIHaâ€™Tactic decoderProduction rulesLearning to Prove Theorems via Interacting with Proof Assistants

the attention masks (Equation. 3). A softmax on the
scores gives us the probability for each premise.

â€¢

â€¢

Integers (as in â€œconstructor 2â€): Most integers
in the data are in the range of [1, 4]. We use a 4-way
classiï¬er to generate them.

Quantiï¬ed variables in the goal (as in â€œsimple
induction nâ€): We randomly pick a universally
quantiï¬ed variable in the goal.

Training and inference We train the model on the proof
steps extracted from CoqGym. When expanding a node
using a production rule, we apply the cross-entropy loss to
maximize the likelihood of the ground truth production rule
in Equation. 4. However, when the model emits a tactic
argument, there may be no corresponding argument in the
ground truth; because the model might have generated a
different tactic from the ground truth. For example, the
model may output â€œapply Hâ€ with an argument H, while
the ground truth may be split without any argument.

To apply a reasonable loss in this scenario, we train the
model with teacher forcing (Williams & Zipser, 1989). Dur-
ing the sequential generation of a tactic AST, the model
outputs how to expand the partial tree, but the tree grows
following the ground truth, not the modelâ€™s output. Then
the arguments generated by the model must correspond to
those in the ground truth, and we can apply losses normally.

During testing, we combine the model with depth-ï¬rst
search (DFS) for fully-automated theorem proving. At each
step, the model samples a few tactics via beam search, which
are used to search for a complete proof via DFS. We prune
the search space by backtracking when a duplicate proof
state is detected.

Implementation details We use 256-dimensional vectors
for all embeddings in ASTactic, including Coq terms (g,
ri), production rules (at
1), GRU hidden state (st), and
symbols in the tactic grammar (nt). The training data in-
cludes 190K steps from human-written proofs. We do not
train ASTactic with synthetic proofs since they only contain
tactics extracted from the human proofs. For a method to
beneï¬t from synthetic proofs, it should model the entire
sequence of tactics rather than an individual tactic.

âˆ’

10âˆ’

We train the model using RMSProp (Tieleman & Hinton,
5 and weight decay
2012) with a learning rate of 3
Ã—
6. The training goes for 5 epochs, which takes a few
of 10âˆ’
days on a single GeForce GTX 1080 GPU. During testing,
our system performs beam search with a beam width of 20
to generate the top 20 tactics at each proof step. And we set
a depth limit of 50 during DFS.

6. Experiments

Experimental setup We evaluate ASTactic on the task of
fully-automated theorem proving in Coq, using the 13,137
testing theorems in CoqGym. The agent perceives the cur-
rent goals, their local context, and the environment. It in-
teracts with Coq by executing commands, which include
tactics, backtracking to the previous step (Undo), and any
other valid Coq command. The goal for the agent is to ï¬nd
a complete proof using at most 300 tactics and within a
wall time of 10 minutes. We run all testing experiments on
machines with 16GB RAM and two Intel Xeon Silver 4114
CPU Cores. We do not use GPUs for testing since the speed
bottleneck is executing tactics rather than generating tactics.

We compare the performance of our system with several
baselines. Our ï¬rst set of baselines are Coqâ€™s built-in au-
tomated tactics, including trivial, auto, intuition,
and easy. They all try to prove the theorem via some
simple procedures such as backward chaining. The sec-
ond baseline is hammer (Czajka & Kaliszyk, 2018)â€”a
hammer-based system that proves theorems using exter-
nal ATP systems. In our particular conï¬guration, hammer
simultaneously invokes Z3 (De Moura & BjÃ¸rner, 2008),
CVC4 (Barrett et al., 2011), Vampire (KovÂ´acs & Voronkov,
2013), and E Prover (Schulz, 2013), and returns a proof as
long as one of them succeeds.

If we treat hammer as a black box tactic, it sets a default
time limit of 20 seconds to the external ATP systems. We
test hammer both in this setting and in a setting where we
extend the time limit to 10 minutes.

All of these automated tactics either prove the goal com-
pletely or leave the goal unchanged; they do not decompose
the goal into sub-goals. We combine ASTactic with them as
follows: At each step, the agent ï¬rst uses an automated tac-
tic (e.g. hammer) to see if it can solve the current goal. If
not, the agent executes a tactic from ASTactic to decompose
the goal into sub-goals.

Table 1. Percentage of theorems successfully proved. Our method
signiï¬cantly outperforms Coqâ€™s built-in automated tactics.
It
achieves the highest success rate when combined with hammer.
The default time limit for hammer is 20 seconds and the extended
time limit is 10 minutes.

Method

Success rate (%)

trivial
auto
intuition
easy
hammer (default time limit)
hammer (extended time limit)

ours
ours + auto
ours + hammer

2.4
2.9
4.4
4.9
17.8
24.8

12.2
12.8
30.0

Learning to Prove Theorems via Interacting with Proof Assistants

Success rates Table 1 shows the percentage of theo-
rems successfully proved. Our system proves 12.2% of
the theorems, while the built-in automated tactics in Coq
prove less than 4.9%. While our model underperforms
hammer (24.8% with extended time limit), its performance
is nonetheless very encouraging considering that hammer
invokes four state-of-the-art external ATP systems that took
many years to engineer whereas our model is trained from
scratch with a very limited amount of hand engineering.
When combined with hammer, ASTactic can prove 30.0%
of the theorems, a large improvement of 5.2% over using
hammer alone. This demonstrates that our system can gen-
erate effective tactics and can be used to prove theorems
previously not provable by automatic methods.

Table 2. The effect of the beam width on the success rate and the
average runtime for proving a theorem.

Beam width

Success rate (%)
Average runtime (seconds)

1

1.0
0.2

5

6.5
1.2

10

10.8
2.2

15

12.0
2.7

20

12.2
3.3

25

11.7
3.9

Efï¬ciency The beam width is the number of candidate
tactics to explore at each proof step; it thus controls the
trade-off between speed and accuracy. Table 2 shows how
the beam width affects the runtime and the number of proved
theorems. A large beam width increases the success rate as
it enables the model to explore larger search space at the
expense of longer runtime. However, when the beam width
goes above 20, the success rate drops, probably due to the
model being trapped in an unpromising branch for too long.

Fig. 3 (Left) illustrates the runtime of various methods. The
built-in automated tactics are faster, but they can prove only
a few hundred theorems. ASTactic combined with hammer
takes less than 100 seconds per theorem for proving over
3,000 theorems.

Fig. 3 (Right) shows the number of tactics tried before suc-
cessfully ï¬nding a proof. Compared to SEPIA (Gransden
et al., 2015), which uses 10,000 tactics, ASTactic is more
efï¬cient in exploring the tactic space, needing only a few
hundred tactics. When combined with hammer, it typically
ï¬nds a proof within 10 tactics.

Figure 3. The number of proved theorems increases with the run-
time (Left) and the number of allowed tactics (Right).

Generated proofs Fig. 4 shows the lengths of the gen-
erated proofs compared to all ground truth proofs in the
testing set. As expected, most generated proofs are short
(less than 10 steps). The average length of the generated

Figure 4. The lengths (the number of steps) of the generated proofs
compared to all proofs in the testing set. The dash lines are the
average lengths (6.0 and 12.5 respectively).

proofs is 6.0 while the average of the entire testing set is
12.5, which suggests that theorems with longer proofs are
much more challenging for the model.

Even for the same theorem, a generated proof can be much
shorter than the ground truth. Fig. 5 is one such example.
The generated proof calls a decision procedure (ring) to
solve the goal with fewer tactics. This also reï¬‚ects the fact
that longer proofs are harder to ï¬nd.

Figure 5. An example proof generated by our method. It is shorter
than the ground truth thanks to the ring decision procedure.

7. Conclusion

We address the problem of learning to prove theorems in
Coq. We have constructed CoqGymâ€”a large-scale dataset
and learning environment with human-written proofs from
a broad spectrum of Coq projects. We have developed
ASTactic, a deep-learning based model that generates Coq
tactics in the form of AST. Experimental results on CoqGym
conï¬rm the effectiveness of our model for synthesizing
complete proofs automatically.

Acknowledgements

This work is partially supported by the National Science
Foundation under Grant No. 1633157.

autoeasyhammerintuitionoursours+hammer010002000300040000.11.020.0600.0Time (seconds)# Theorems Provedoursours+hammer010002000300040001251030100300# Tactics# Theorems Proved0500100015002000250001020304050Length# ProofsGenerated proofGround truthLearning to Prove Theorems via Interacting with Proof Assistants

Figure F. Extracting a synthetic proof from the intermediate goal G2. Goals G3 and G4 are converted into premises in G2â€™s local context.
The synthetic proof corresponds to a trimmed sub-tree rooted at G2.

Appendix

A.3. Extracting Synthetic Proofs from Intermediate

Goals

A. Details on Constructing the Dataset

A.1. Building the Coq projects

We manually compile and install the Coq standard library
and a few projects (such as math-comp) that are frequently
required by other projects. For the rest, we try compil-
ing them automatically using simple commands such as
â€œ./configure && makeâ€, and we take whatever com-
piles, ending up with 123 projects and 3,061 Coq ï¬les (ex-
cluding the ï¬les that do not contain any proof).

A.2. Reconstructing the Proof Tree

After applying a tactic, the current goal disappears, and a
set of new goals emerge, which become the children of the
current goal in the proof tree. We can identify the edges of
the tree by tracking how goals emerge during the proof. For
example, if the list of goals changes from [2, 7] to [8, 9, 7],
we know that node 2 has two children: 8 and 9.

In certain cases, a tactic can affect more than one goal, and
it is unclear who should be the parent node. This can happen
when a tactic is applied to multiple goals using a language
feature called goal selectors (by default, a tactic is applied
only to the ï¬rst goal). However, goal selectors are rarely
used in practice. We discard all such proofs and lose only
less than 1% of our data. For the remaining data, only one
goal disappears at each step, and we can build the proof
trees unambiguously.

Given an intermediate goal, it is straightforward to treat it
as a theorem by adding its local context to the environment.
For example, in Fig. F, the goal G2 can be a theorem
(a + b) + c = a + (b + c) in the environment augmented
by a b and c. Extracting synthetic proofs for the new
theorem requires nontrivial processing. One straightforward
proof would be the sequence of tactics that follows G2 in
the original human-written proof: â€œinduction a as
[
trivial.â€. This proof corresponds to the sub-tree rooted
at G2.

simpl; rewrite IHaâ€™.

aâ€™].
|

trivial.

However, there are potentially shorter proofs for G2 using
a trimmed sub-tree. For example, if we only apply the ï¬rst
tactic to generates G3 and G4, then we can treat them as
premises H3 and H4, and complete the proof by â€œapply
apply H4.â€. Equivalently, we can also use auto
H3.
to complete the proof. This technique of converting un-
solved sub-goals into premises allows us to generate syn-
thetic proofs of controllable lengths, by taking a sequence
of tactics from the original proof and appending an auto
at the end.

We need to take extra care in converting a goal into a premise.
For example, it is easy to treat G3 as a premise, but G4 needs
some care. G4 depends on aâ€™, which is missing in G2â€™s
context. In order to convert G4 into a well-formed term in
G2â€™s context, we apply the â€œgeneralize dependentâ€
tactic to push a local premise into the statement of the goal.
When applied to G4, it generates H4 in Fig. F, which can
be added to G2â€™s local context.

H4:âˆ€a(cid:31):nat,(a(cid:31)+b)+c=a(cid:31)+(b+c)â†’(Sa(cid:31)+b)+c=Sa(cid:31)+(b+c)simple; rewrite IHaâ€™trivialtrivialintrosâˆ€abc:nat,(a+b)+c=a+(b+c)a,b,c:nat(a+b)+c=a+(b+c)b,c:nat(0+b)+c=0+(b+c)a(cid:31),b,c:natIHa(cid:31):(a(cid:31)+b)+c=a(cid:31)+(b+c)S(a(cid:31)+(b+c))=S(a(cid:31)+(b+c))(Sa(cid:31)+b)+c=Sa(cid:31)+(b+c)a(cid:31),b,c:natIHa(cid:31):(a(cid:31)+b)+c=a(cid:31)+(b+c)induction a as [|aâ€™]a,b,c:nat(a+b)+c=a+(b+c)H3:(0+b)+c=0+(b+c)Original proof trimmed sub-treeSynthetic proofG2G3G4G5G1Learning to Prove Theorems via Interacting with Proof Assistants

B. The Space of Tactics for ASTactic

Below is the context-free grammar in extended Backus-Naur form for the tactic space. The start symbol is tactic expr.

t e r m c o m m a l i s t 1 r e d u c e d i n c l a u s e

r e w r i t e t e r m l i s t 1 i n c l a u s e

t a c t i c e x p r

:

|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|

LOCAL IDENT :

l i s t

i d e n t

l o c a l

i n c l a u s e

t e r m l i s t 1

i n t r o
â€˜ a p p l y â€™
â€˜ a u t o â€™ u s i n g c l a u s e w i t h h i n t d b s
â€˜ r e w r i t e â€™
â€˜ s i m p l â€™
â€˜ u n f o l d â€™ q u a l i d l i s t 1 i n c l a u s e
d e s t r u c t
i n d u c t i o n
â€˜ e l i m â€™ QUALID
â€˜ s p l i t â€™
â€˜ a s s u m p t i o n â€™
t r i v i a l
â€˜ r e f l e x i v i t y â€™
â€˜ c a s e â€™ QUALID
c l e a r
â€˜ s u b s t â€™
â€˜ g e n e r a l i z e â€™
â€˜ e x i s t s â€™ LOCAL IDENT
i n c l a u s e
â€˜ red â€™
â€˜ omega â€™
d i s c r i m i n a t e
i n v e r s i o n
s i m p l e i n d u c t i o n
c o n s t r u c t o r
â€˜ c o n g r u e n c e â€™
â€˜ l e f t â€™
â€˜ r i g h t â€™
â€˜ r i n g â€™
â€˜ symmetry â€™
â€˜ f e q u a l â€™
â€˜ t a u t o â€™
â€˜ r e v e r t â€™
â€˜ s p e c i a l i z e â€™
â€˜ i d t a c â€™
â€˜ h n f â€™
i n c l a u s e
i n v e r s i o n c l e a r
c o n t r a d i c t i o n
â€˜ i n j e c t i o n â€™ LOCAL IDENT
â€˜ e x f a l s o â€™
â€˜ cbv â€™
â€˜ c o n t r a d i c t â€™ LOCAL IDENT
â€˜ l i a â€™
â€˜ f i e l d â€™
â€˜ e a s y â€™
â€˜ cbn â€™
â€˜ e x a c t â€™ QUALID
â€˜ i n t u i t i o n â€™
â€˜ e a u t o â€™ u s i n g c l a u s e w i t h h i n t d b s

l o c a l

i d e n t

l i s t 1

/ [ A

Za

z

] [ A

Za

z0

âˆ’

âˆ’

âˆ’

âˆ’

9 â€™ ]

âˆ’

/

âˆ—

â€˜ ( â€™ LOCAL IDENT QUALID â€˜ ) â€™

/

|

c o c

|

s e t

|

z f c /

Learning to Prove Theorems via Interacting with Proof Assistants

QUANTIFIED IDENT :

INT :

/ 1

2

3

|

|

|

4 /

/ [ A

Za

z

] [ A

Za

z0

âˆ’

âˆ’

âˆ’

âˆ’

9 â€™ ]

âˆ’

/

âˆ—

QUALID :

/ ( [ A

Za

z

] [ A

Za

z0

âˆ’

âˆ’

âˆ’

âˆ’

9 â€™ ]

âˆ’

. )

âˆ—

âˆ— \

[ A

Za

z

] [ A

Za

z0

âˆ’

âˆ’

âˆ’

âˆ’

9 â€™ ]

âˆ’

âˆ—

HINT DB :

/ a r i t h

z a r i t h

|

|

a l g e b r a

r e a l

s e t s

c o r e

b o o l

|

|

|

|

|

d a t a t y p e s

l o c a l

i d e n t

l i s t

:

|

LOCAL IDENT l o c a l

i d e n t

l i s t

l o c a l

i d e n t

l i s t 1 : LOCAL IDENT

LOCAL IDENT l o c a l

i d e n t

l i s t 1

|

q u a l i d l i s t 1 : QUALID

QUALID â€˜ , â€™ q u a l i d l i s t 1

|

t e r m l i s t 1 : QUALID

QUALID t e r m l i s t 1

|

t e r m c o m m a l i s t 1 : QUALID

QUALID â€˜ , â€™

t e r m c o m m a l i s t 1

|

h i n t d b l i s t 1 : HINT DB

HINT DB h i n t d b l i s t 1

|

r e d u c e d i n c l a u s e

:

|

â€˜ i n â€™ LOCAL IDENT

i n c l a u s e

a t c l a u s e

:

|
|
|

:

|

â€˜ i n â€™ LOCAL IDENT
â€˜ i n â€™
â€˜ i n â€™

â€˜
| âˆ’ âˆ—
â€™
â€˜
âˆ—

â€™

â€˜ a t â€™

INT

u s i n g c l a u s e

w i t h h i n t d b s

:

|

:

|
|

â€˜ u s i n g â€™ q u a l i d l i s t 1

â€˜ w i t h â€™ h i n t d b l i s t 1
â€™
â€˜ w i t h â€™

â€˜

âˆ—

i n t r o :

|

â€˜ i n t r o â€™
â€˜ i n t r o s â€™

r e w r i t e t e r m : QUALID

â€˜
â€˜

â†’
â†

|
|

â€™ QUALID
â€™ QUALID

r e w r i t e t e r m l i s t 1 :

|

r e w r i t e t e r m
r e w r i t e t e r m â€˜ , â€™

r e w r i t e t e r m l i s t 1

Learning to Prove Theorems via Interacting with Proof Assistants

d e s t r u c t

:

â€˜ d e s t r u c t â€™

t e r m c o m m a l i s t 1

i n d u c t i o n :

|

â€˜ i n d u c t i o n â€™ LOCAL IDENT
â€˜ i n d u c t i o n â€™

INT

t r i v i a l

:

â€˜ t r i v i a l â€™

c l e a r

â€˜ c l e a r â€™
â€˜ c l e a r â€™

:

|

l o c a l

i d e n t

l i s t 1

d i s c r i m i n a t e

â€˜ d i s c r i m i n a t e â€™
â€˜ d i s c r i m i n a t e â€™ LOCAL IDENT

:

|

i n v e r s i o n :

|

â€˜ i n v e r s i o n â€™ LOCAL IDENT
â€˜ i n v e r s i o n â€™

INT

s i m p l e i n d u c t i o n :

â€˜ s i m p l e

â€˜ s i m p l e i n d u c t i o n â€™

i n d u c t i o n â€™ QUANTIFIED IDENT
INT

|

c o n s t r u c t o r

â€˜ c o n s t r u c t o r â€™
â€˜ c o n s t r u c t o r â€™

INT

:

|

i n v e r s i o n c l e a r

â€˜ i n v e r s i o n c l e a r â€™ LOCAL IDENT
â€˜ i n v e r s i o n c l e a r â€™

INT

:

|

c o n t r a d i c t i o n :

|

â€˜ c o n t r a d i c t i o n â€™
â€˜ c o n t r a d i c t i o n â€™ LOCAL IDENT

Learning to Prove Theorems via Interacting with Proof Assistants

References

Allamanis, M., Chanthirasegaran, P., Kohli, P., and Sut-
ton, C. Learning continuous semantic representations of
symbolic expressions. arXiv preprint arXiv:1611.01423,
2016a.

Allamanis, M., Peng, H., and Sutton, C. A convolutional
attention network for extreme summarization of source
code. In International Conference on Machine Learning,
pp. 2091â€“2100, 2016b.

Brockschmidt, M., Allamanis, M., Gaunt, A. L., and Polo-
In
zov, O. Generative code modeling with graphs.
International Conference on Learning Representations,
2019. URL https://openreview.net/forum?
id=Bke4KsA5FX.

Cho, K., Van MerriÂ¨enboer, B., Gulcehre, C., Bahdanau,
D., Bougares, F., Schwenk, H., and Bengio, Y. Learn-
ing phrase representations using rnn encoder-decoder
arXiv preprint
for statistical machine translation.
arXiv:1406.1078, 2014.

Allamanis, M., Brockschmidt, M., and Khademi, M. Learn-
ing to represent programs with graphs. arXiv preprint
arXiv:1711.00740, 2017.

Czajka, Å. and Kaliszyk, C. Hammer for coq: Automa-
tion for dependent type theory. Journal of Automated
Reasoning, 61(1-4):423â€“453, 2018.

Alon, U., Zilberstein, M., Levy, O., and Yahav, E. code2vec:
arXiv

Learning distributed representations of code.
preprint arXiv:1803.09473, 2018.

Balunovic, M., Bielik, P., and Vechev, M. Learning to
solve smt formulas. In Advances in Neural Information
Processing Systems, pp. 10317â€“10328, 2018.

Bancerek, G., ByliÂ´nski, C., Grabowski, A., KorniÅ‚owicz, A.,
Matuszewski, R., Naumowicz, A., Pak, K., and Urban, J.
Mizar: State-of-the-art and beyond. In Conferences on In-
telligent Computer Mathematics, pp. 261â€“279. Springer,
2015.

Bansal, K., Loos, S. M., Rabe, M. N., Szegedy, C., and
Wilcox, S. Holist: An environment for machine learn-
ing of higher-order theorem proving (extended version).
arXiv preprint arXiv:1904.03241, 2019.

Barras, B., Boutin, S., Cornes, C., Courant, J., Filliatre,
J.-C., Gimenez, E., Herbelin, H., Huet, G., Munoz, C.,
Murthy, C., et al. The Coq proof assistant reference
manual: Version 6.1. PhD thesis, Inria, 1997.

Barrett, C., Conway, C. L., Deters, M., Hadarean, L., Jo-
vanoviâ€™c, D., King, T., Reynolds, A., and Tinelli, C.
CVC4. In Gopalakrishnan, G. and Qadeer, S. (eds.), Pro-
ceedings of the 23rd International Conference on Com-
puter Aided Veriï¬cation (CAV â€™11), volume 6806 of Lec-
ture Notes in Computer Science, pp. 171â€“177. Springer,
July 2011. URL http://www.cs.stanford.edu/
Ëœbarrett/pubs/BCD+11.pdf. Snowbird, Utah.

Bordes, A., Boureau, Y.-L., and Weston, J.

ing end-to-end goal-oriented dialog.
arXiv:1605.07683, 2016.

Learn-
arXiv preprint

Bridge, J. P., Holden, S. B., and Paulson, L. C. Machine
learning for ï¬rst-order theorem proving. Journal of auto-
mated reasoning, 53(2):141â€“172, 2014.

Darvas, Â´A., HÂ¨ahnle, R., and Sands, D. A theorem proving
approach to analysis of secure information ï¬‚ow. In Inter-
national Conference on Security in Pervasive Computing,
pp. 193â€“209. Springer, 2005.

De Moura, L. and BjÃ¸rner, N. Z3: An efï¬cient smt solver.
In International conference on Tools and Algorithms for
the Construction and Analysis of Systems, pp. 337â€“340.
Springer, 2008.

Delahaye, D. A tactic language for the system coq. In Inter-
national Conference on Logic for Programming Artiï¬cial
Intelligence and Reasoning, pp. 85â€“95. Springer, 2000.

Dixon, L. and Fleuriot, J. Isaplanner: A prototype proof
planner in isabelle. In International Conference on Auto-
mated Deduction, pp. 279â€“283. Springer, 2003.

J.

Gallego Arias, E.

SerAPI: Machine-Friendly,
Techni-
2016.
https://hal-mines-paristech.

Data-Centric Serialization
cal
URL
archives-ouvertes.fr/hal-01384408.

report, MINES ParisTech, October

for Coq.

Gauthier, T., Kaliszyk, C., Urban, J., Kumar, R., and Nor-
rish, M. Learning to prove with tactics. arXiv preprint
arXiv:1804.00596, 2018.

Gonthier, G., Asperti, A., Avigad, J., Bertot, Y., Cohen, C.,
Garillot, F., Le Roux, S., Mahboubi, A., OConnor, R.,
Biha, S. O., et al. A machine-checked proof of the odd
order theorem. In International Conference on Interactive
Theorem Proving, pp. 163â€“179. Springer, 2013.

Gransden, T., Walkinshaw, N., and Raman, R. Sepia: search
for proofs using inferred automata. In International Con-
ference on Automated Deduction, pp. 246â€“255. Springer,
2015.

Hales, T., Adams, M., Bauer, G., Dang, T. D., Harrison, J.,
Le Truong, H., Kaliszyk, C., Magron, V., McLaughlin, S.,

Learning to Prove Theorems via Interacting with Proof Assistants

Nguyen, T. T., et al. A formal proof of the kepler conjec-
ture. In Forum of Mathematics, Pi, volume 5. Cambridge
University Press, 2017.

McCarthy, J. Recursive functions of symbolic expressions
and their computation by machine, part i. Communica-
tions of the ACM, 3(4):184â€“195, 1960.

Harrison, J. Hol light: A tutorial introduction. In Interna-
tional Conference on Formal Methods in Computer-Aided
Design, pp. 265â€“269. Springer, 1996.

Harrison, J., Urban, J., and Wiedijk, F. History of interactive
theorem proving. In Computational Logic, volume 9, pp.
135â€“214, 2014.

Hindle, A., Barr, E. T., Su, Z., Gabel, M., and Devanbu, P.
On the naturalness of software. In Software Engineering
(ICSE), 2012 34th International Conference on, pp. 837â€“
847. IEEE, 2012.

Huang, D., Dhariwal, P., Song, D., and Sutskever, I.
Gamepad: A learning environment for theorem proving.
In International Conference on Learning Representations,
2019. URL https://openreview.net/forum?
id=r1xwKoR9Y7.

Irving, G., Szegedy, C., Alemi, A. A., EÂ´en, N., Chollet,
F., and Urban, J. Deepmath-deep sequence models for
premise selection. In Advances in Neural Information
Processing Systems, pp. 2235â€“2243, 2016.

Kaliszyk, C. and Urban, J. Learning-assisted automated rea-
soning with ï¬‚yspeck. Journal of Automated Reasoning,
53(2):173â€“213, 2014.

Kaliszyk, C., Urban, J., Michalewski, H., and OlË‡sÂ´ak, M. Re-
inforcement learning of theorem proving. arXiv preprint
arXiv:1805.07563, 2018.

Kern, C. and Greenstreet, M. R. Formal veriï¬cation in
hardware design: a survey. ACM Transactions on Design
Automation of Electronic Systems (TODAES), 4(2):123â€“
193, 1999.

KovÂ´acs, L. and Voronkov, A. First-order theorem proving
and vampire. In International Conference on Computer
Aided Veriï¬cation, pp. 1â€“35. Springer, 2013.

Leroy, X. Formal veriï¬cation of a realistic compiler. Com-

munications of the ACM, 52(7):107â€“115, 2009.

Liu, B., Tur, G., Hakkani-Tur, D., Shah, P., and Heck,
L. End-to-end optimization of task-oriented dialogue
model with deep reinforcement learning. arXiv preprint
arXiv:1711.10712, 2017.

Loos, S., Irving, G., Szegedy, C., and Kaliszyk, C.
arXiv preprint

Deep network guided proof search.
arXiv:1701.06972, 2017.

McCune, W. Solution of the robbins problem. Journal of

Automated Reasoning, 19(3):263â€“276, 1997.

Parisotto, E., Mohamed, A.-r., Singh, R., Li, L., Zhou, D.,
and Kohli, P. Neuro-symbolic program synthesis. arXiv
preprint arXiv:1611.01855, 2016.

Paulin-Mohring, C. Circuits as streams in coq: Veriï¬cation
of a sequential multiplier. In International Workshop on
Types for Proofs and Programs, pp. 216â€“230. Springer,
1995.

Paulson, L. C. Isabelle: A generic theorem prover, volume

828. Springer Science & Business Media, 1994.

Paulson, L. C. and Blanchette, J. C. Three years of ex-
perience with sledgehammer, a practical link between
automatic and interactive theorem provers. In PAAR@
IJCAR, pp. 1â€“10, 2010.

RocktÂ¨aschel, T. and Riedel, S. End-to-end differentiable
proving. In Advances in Neural Information Processing
Systems, pp. 3788â€“3800, 2017.

Schulz, S. System description: E 1.8. In International Con-
ference on Logic for Programming Artiï¬cial Intelligence
and Reasoning, pp. 735â€“743. Springer, 2013.

Sutcliffe, G. The tptp problem library and associated in-
frastructure. Journal of Automated Reasoning, 43(4):337,
2009.

Tai, K. S., Socher, R., and Manning, C. D. Improved seman-
tic representations from tree-structured long short-term
memory networks. arXiv preprint arXiv:1503.00075,
2015.

Tieleman, T. and Hinton, G. Lecture 6.5â€”RmsProp: Divide
the gradient by a running average of its recent magnitude.
COURSERA: Neural Networks for Machine Learning,
2012.

Urban, J. Mptpâ€“motivation, implementation, ï¬rst experi-
ments. Journal of Automated Reasoning, 33(3-4):319â€“
339, 2004.

Urban, J., VyskoË‡cil, J., and Ë‡StË‡epÂ´anek, P. Malecop machine
learning connection prover. In International Conference
on Automated Reasoning with Analytic Tableaux and
Related Methods, pp. 263â€“277. Springer, 2011.

Wang, M., Tang, Y., Wang, J., and Deng, J. Premise selec-
tion for theorem proving by deep graph embedding. In
Advances in Neural Information Processing Systems, pp.
2786â€“2796, 2017.

Learning to Prove Theorems via Interacting with Proof Assistants

Williams, R. J. and Zipser, D. A learning algorithm for con-
tinually running fully recurrent neural networks. Neural
computation, 1(2):270â€“280, 1989.

Yin, P. and Neubig, G.

A syntactic neural model
for general-purpose code generation. arXiv preprint
arXiv:1704.01696, 2017.

