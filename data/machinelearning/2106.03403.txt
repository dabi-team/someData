Network Inference and Inﬂuence Maximization from Samples⋆,⋆⋆

Zhijie Zhanga,b,∗, Wei Chenc, Xiaoming Suna,b, Jialin Zhanga,b

aInstitute of Computing Technology, Chinese Academy of Sciences, Beijing, 100080, China
bUniversity of Chinese Academy of Sciences, Beijing, 100080, China
cMicrosoft Research Asia, Beijing, 100080, China

2
2
0
2

b
e
F
8
1

]
I
S
.
s
c
[

2
v
3
0
4
3
0
.
6
0
1
2
:
v
i
X
r
a

Abstract

Inﬂuence maximization is the task of selecting a small number of seed nodes in a social
network to maximize the inﬂuence spread from these seeds. It has been widely investigated in
the past two decades. In the canonical setting, the social network and its diﬀusion parameters
are given as input. In this paper, we consider the more realistic sampling setting where the
network is unknown and we only have a set of passively observed cascades that record the
sets of activated nodes at each diﬀusion step. We study the task of inﬂuence maximization
from these cascade samples (IMS) and present constant approximation algorithms for it
under mild conditions on the seed set distribution. To achieve the optimization goal, we
also provide a novel solution to the network inference problem, that is, learning diﬀusion
parameters and the network structure from the cascade data. Compared with prior solutions,
our network inference algorithms require weaker assumptions and do not rely on maximum-
likelihood estimation and convex programming. Our IMS algorithms enhance the learning-
and-then-optimization approach by allowing a constant approximation ratio even when the
diﬀusion parameters are hard to learn, and we do not need any assumption related to the
network structure or diﬀusion parameters.

Keywords:
data-driven optimization, end-to-end optimization

inﬂuence maximization, network inference, optimization from samples,

1. Introduction

Maximizing the spread of inﬂuence through a social network has been widely studied
in the past two decades. It models the phenomenon in which a small set of initially active

⋆A preliminary version containing only results about the IC model appears in ICML 2021.
⋆⋆This work was supported in part by the National Natural Science Foundation of China Grants
No. 61832003, 61872334, the Strategic Priority Research Program of Chinese Academy of Sciences under
Grant No. XDA27000000, the 973 Program of China Grant No. 2016YFB1000201, K.C. Wong Education
Foundation.

∗Corresponding author
Email addresses: zhangzhijie@ict.ac.cn (Zhijie Zhang), weic@microsoft.com (Wei Chen),

sunxiaoming@ict.ac.cn (Xiaoming Sun), zhangjialin@ict.ac.cn (Jialin Zhang)

Preprint submitted to Elsevier

February 21, 2022

 
 
 
 
 
 
nodes called seeds takes some piece of information (news, ideas or opinions, etc.), and the
information spreads over the network to activate the remaining nodes. The expected number
of ﬁnal active nodes is called the inﬂuence spread of the seed set. The inﬂuence maximization
problem asks to pick at most k seeds in order to maximize the inﬂuence spread. Under
many diﬀusion models such as the discrete-time independent cascade (IC) model and linear
threshold (LT) model (Kempe et al., 2003), the problem enjoys a (1−1/e−ε)-approximation
(with small ε > 0), which is tight assuming P 6= NP (Feige, 1998). It has found applications
in many scenarios.

Traditional inﬂuence maximization problem requires as input the whole social network
(as well as its parameters), based on which one can compute or estimate the inﬂuence
spread function.
In many scenarios, however, this might be too demanding, especially
for those who do not have free access to the network. In this work, we consider inﬂuence
maximization in the sampling setting where one only has access to a set of passively observed
cascades spreading over an implicit social network. Each cascade records the sets of activated
nodes at each time step. Such sample data is available in many scenarios, especially on the
Internet where the timestamps can be recorded in principle. We are interested in whether
we can maximize the inﬂuence from such sample data. We model this problem as inﬂuence
maximization from samples below:

Inﬂuence maximization from samples (IMS). For an unknown social net-
work G with diﬀusion parameters, given t cascade samples where each seed is
independently sampled with an unknown probability, can we ﬁnd a seed set of
size at most k such that its inﬂuence spread is a constant approximation of the
optimal seed set, when t is polynomial to the size of G?

En route to solving the above problem, a natural and reasonable approach is to ﬁrst
learn the network structure as well as its parameters, and then maximize the inﬂuence over
the learned network. This leads to the well-studied network inference problem below:

Network inference. For an unknown social network G, given polynomial num-
ber of cascade samples where each seed is sampled independently with an un-
known probability, estimate all diﬀusion parameters such that with probability
at least 1 − δ, every parameter is estimated within an additive error ε.

Our contributions in this work are mainly two-fold. First, we revisit the network in-
ference problem and design brand new algorithms for it under both IC and LT models.
For the IC model, while all previous algorithms are based on the maximum likelihood esti-
mation and convex programming (Netrapalli and Sanghavi, 2012; Narasimhan et al., 2015;
Pouget-Abadie and Horel, 2015), our algorithm builds on a closed-form expression for each
edge probability in terms of quantities which can be eﬃciently estimated. As a result, our
algorithm enjoys faster implementation, lower sample complexity and weaker assumptions
comparing to previous algorithms. Our assumptions are also easy to verify from cascade
samples. We will discuss these diﬀerences further in the end of Section 3.1. For the LT

2

model, to the best of our knowledge, there is no network inference algorithm with theoret-
ical guarantees previously. To resolve the problem, we also build a closed-form expression
for each edge weight, which makes the estimation possible.

Second, we provide several end-to-end IMS algorithms with constant approximation guar-
antee under both IC and LT models. For the IC model, following the canonical learning-
and-then-optimization framework, we ﬁrst present an IMS algorithm by directly invoking
our network inference algorithm. The algorithm thus needs the assumptions used for learn-
ing. Next, we present alternative algorithms which only need two simple assumptions on
the seed distribution and impose no requirements for the underlying network. In contrast,
all the known algorithms for network inference (including ours) impose some restrictions
on the network. This result is highly non-trivial since it is impossible to resolve network
inference problem on arbitrary graphs and hence the learning-and-then-optimization frame-
work fails in this case. For instance, consider a complete graph and another graph with
one edge removed from the complete graph, where all edge probabilities are 1.
If each
node is picked as a seed independently with probability 1/2, one cannot distinguish them
within polynomially many cascade samples. For the LT model, an interesting feature of the
network inference algorithm is that the learning assumption already has no requirements
for the network. Thus, the learning-and-then-optimization framework directly leads to an
IMS algorithm that works for arbitrary networks under the LT model. Our IMS follows the
general optimization-from-samples framework (Balkanski et al., 2017b), and generalizes the
recent result on optimization from structured samples for coverage functions (Chen et al.,
2020), see Section 1.1 for details. Finally, we remark that our results not only apply to inﬂu-
ence maximization, but also to other learning and optimization settings such as probabilistic
maximum cover with application in online advertising (Chen et al., 2016).

1.1. Related Work

Inﬂuence maximization from samples follows the framework of optimization from samples
(OPS) originally proposed by Balkanski et al. (2017b): given a set of polynomial num-
ber of samples {Si, f (Si)}t
i=1 and constraint M, can we ﬁnd a set S ∈ M such that
f (S) ≥ c · maxT ∈M f (T ) for some constant c? The OPS framework is very important
for the data-driven integration of learning and optimization where the underlying model
(function f above) is not readily known. Surprisingly, Balkanski et al. (2017b) showed that
even for the maximum coverage problem, there is no constant approximation algorithm
under the OPS model, despite prior results that a coverage function f is learnable from
samples (Badanidiyuru et al., 2012) and constant optimization is available when f is known
(Nemhauser et al., 1978). Subsequently, several attempts (Balkanski et al., 2016, 2017a;
Rosenfeld et al., 2018; Chen et al., 2020) have been made to circumvent the impossibility
result of Balkanski et al. (2017b). Among them the most related one is the optimization
from structured samples (OPSS) model for coverage functions (Chen et al., 2020), where
the samples carry additional structural information in the form of {Si, N(Si)}t
i=1, where
N(Si) contains the nodes covered by Si. It was shown that if the samples are generated
from a “negatively correlated” distribution, the maximum coverage problem enjoys constant
approximation in the OPSS model. Recall that coverage functions can be regarded as IC

3

inﬂuence spread functions deﬁned over a bipartite graph with edge probabilities in {0, 1}.
Thus, our result on IMS greatly generalizes OPSS to allow general graphs and stochastic
cascades over edges.

End-to-end inﬂuence maximization from data has been explored by Goyal et al. (2011),
but they only used a heuristic method to learn inﬂuence spread functions and then used the
greedy method for inﬂuence maximization, so there was no end-to-end guarantee on IMS.
A recent work (Balkanski et al., 2017a) revisited IMS problem under the OPS model, and
provided a constant approximation algorithm when the underlying network is generated
from the stochastic block model. Our study is the ﬁrst to provide IMS algorithms with
theoretical guarantees that work on arbitrary networks.

Network inference has been extensively studied over the past decade (Gomez-Rodriguez et al.,

2010; Myers and Leskovec, 2010; Gomez-Rodriguez et al., 2011; Du et al., 2012; Netrapalli and Sanghavi,
2012; Abrahao et al., 2013; Daneshmand et al., 2014; Du et al., 2013, 2014; Narasimhan et al.,
2015; Pouget-Abadie and Horel, 2015; He et al., 2016). While most of them focused on the
continuous time diﬀusion model, there are several results under the discrete time IC model
(Netrapalli and Sanghavi, 2012; Narasimhan et al., 2015; Pouget-Abadie and Horel, 2015),
all of which build on the maximum likelihood estimation. We will compare these results
with ours after we present our approach in Section 3.1.

1.2. Organization

In Section 2, we describe the model, some concepts and notations as well as two Chernoﬀ-
type lemmas used in the analysis. In Section 3, we present network inference and IMS algo-
rithms under the IC model. In Section 4, we present network inference and IMS algorithms
under the LT model. Finally, we conclude the paper in Section 5.

2. Preliminaries

Social network, diﬀusion model and inﬂuence maximization. A social network is modeled
as a weighted directed graph G = (V, E, p), where V is the set of |V | = n nodes and E is
the set of directed edges. Each edge (u, v) ∈ E is associated with a weight or probability
puv ∈ [0, 1]. For convenience, we assume that puv = 0 if (u, v) /∈ E and puv > 0 otherwise.
We also use N(v) = N in(v) to denote the in-neighbors of node v ∈ V .

The information or inﬂuence propagates through the network in discrete time steps.
Each node v ∈ V is either active or inactive, indicating whether it receives the information.
Denote by Sτ ⊆ V the set of active nodes at time step τ . The nodes in the set S0 at time
step 0 are called seeds. The diﬀusion is assumed to be progressive, which means a node will
remain active once it is activated. Thus, for all τ ≥ 1, Sτ −1 ⊆ Sτ .

Given a set of seeds S0, the diﬀusion model describes how the information propagates and
Sτ is generated for each τ ≥ 1. In the literature, the two most well-known diﬀusion models
are the independent cascade (IC) model and the linear threshold (LT) model (Kempe et al.,
2003).

In the IC model, at time step τ , initially Sτ = Sτ −1. Next, for each node v /∈ Sτ −1,
each node u ∈ N(v) ∩ (Sτ −1 \ Sτ −2) will try to activate v independently with probability puv

4

(denote S−1 = ∅). Thus, v becomes active with probability 1 −
u∈N (v)∩(Sτ −1\Sτ −2)(1 − puv)
at this step. Once activated, v will be added into Sτ . The propagation terminates when at
the end of some time step τ , Sτ = Sτ −1. Clearly, the process proceeds in at most n − 1 time
steps and we use (S0, S1, · · · , Sn−1) to denote the random sequence of the active nodes.

Q

In the LT model, following the convention, we use w instead of p to denote the edge
weight vector. In this model, each node v ∈ V needs to satisﬁes the normalization condition
u∈N (v) wuv ≤ 1. Besides, each v ∈ V is associated with a threshold rv, which is
that
sampled independently and uniformly from [0, 1] before the diﬀusion starts. During the
diﬀusion process, at time step τ , initially Sτ = Sτ −1. Then, for each node v /∈ Sτ −1, it will
be added into Sτ if
u∈Sτ −1∩N (v) wuv ≥ rv. The diﬀusion also terminates if Sτ = Sτ −1 for
some time step τ and we use (S0, S1, · · · , Sn−1) to denote the random sequence of the active
nodes.

P

P

Let Φ(S0) = Sn−1 be the ﬁnal active set given the seed set S0. Its expected size is denoted
by E[|Φ(S0)|] and is commonly called the inﬂuence spread of S0. In general, the inﬂuence
spread function σ : 2V → R≥0 is deﬁned as σ(S) = E[|Φ(S)|] for any S ⊆ V . Sometimes,
we use σp(·) or σw(·) to specify the graph parameters explicitly. Inﬂuence maximization
(IM) asks to ﬁnd a set of at most k seeds so as to maximize the inﬂuence spread of the
chosen seed set. Formally, under a speciﬁc diﬀusion model (such as IC or LT models), given
a positive integer k ≤ n, inﬂuence maximization corresponds to the following problem:
argmaxS⊆V,|S|≤k σ(S).

The sampling setting. Standard inﬂuence maximization problem takes as input the social
network G = (V, E, p), based on which one can compute or estimate the inﬂuence spread
function σ. In this paper, we consider the problem in the sampling setting where G is not
given explicitly.

A cascade refers to a realization of the sequence of the active nodes (S0, S1, · · · , Sn−1).
By slightly abusing the notation, we still denote the cascade by (S0, S1, · · · , Sn−1). In the
sampling setting, a set of t independent cascades (Si,0, Si,1, · · · , Si,n−1)t
i=1 is given as input,
where the seed set Si,0 in cascade i is generated independently from a seed set distribution D
over the node sets, and given Si,0, the sequence (Si,1, · · · , Si,n−1) is generated according to the
speciﬁed diﬀusion rules. Throughout this work, we assume that D is a product distribution;
in other words, each node u ∈ V is drawn as a seed independently. We aim to solve the
following two problems.

1. Network inference1. Given a set of t samples (Si,0, Si,1, · · · , Si,n−1)t

i=1 deﬁned as
above, estimate the values of puv within an additive error. More formally, for some
ε ∈ (0, 1), compute a vector ˆp such that |ˆpuv − puv| ≤ ε for all u, v ∈ V .

2. Inﬂuence maximization from samples (IMS). Given a set of t samples
i=1 deﬁned as above, ﬁnd a set S A of at most k seeds such that
(Si,0, Si,1, · · · , Si,n−1)t
σ(S A) ≥ κ · σ(S ∗) for some constant κ ∈ (0, 1), where S ∗ denotes the optimal solution.

1In the literature, network inference often means to recover network structure, namely the edge set E.

Here we slightly abuse the terminology to also mean learning edge parameters.

5

Notations. Our algorithms actually only use Si,0 and Si,1 in those cascades to infer infor-
mation about the graph, and we ﬁnd it convenient to deﬁne some corresponding concepts
and notations in advance. These concepts are indeed crucial to our algorithm design. For
node v ∈ V , we use qv = PrD[v ∈ S0] to denote the probability that v is drawn as a seed.
We denote by apG,D(v) the activation probability of node v in one time step during a cas-
cade (S0, S1, · · · , Sn−1) over network G when S0 is drawn from the distribution D. Thus,
apG,D(v) = PrG,D[v ∈ S1]. Note that it contains the possibility that v itself is a seed,
namely v ∈ S0 ⊆ S1. For u, v ∈ V , we deﬁne apG,D(v | u) = PrG,D[v ∈ S1 | u ∈ S0] and
apG,D(v | ¯u) = PrG,D[v ∈ S1 | u /∈ S0], respectively, which are the corresponding probabilities
conditioned on whether u is selected as a seed. When the context is clear, we often omit the
subscripts G and D in the notation.

Chernoﬀ-type bounds. Following are Chernoﬀ-type bounds we will use in our analysis.

Lemma 1 (Multiplicative Chernoﬀ bound, Mitzenmacher and Upfal 2005). Let
X1, X2, · · · , Xn be independent random variables in {0, 1} with Pr[Xi = 1] = pi. Let
X =

n
i=1 pi. Then, for 0 < a < 1,

n
i=1 Xi and µ =

P

and

P

Pr[X ≥ (1 + a)µ] ≤ e−µa2/3,

Pr[X ≤ (1 − a)µ] ≤ e−µa2/2.

Lemma 2 (Additive Chernoﬀ bound, Alon and Spencer 2008). Let X1, · · · , Xn be
n
i=1 Xi and
independent random variables in {0, 1} with Pr[Xi = 1] = pi. Let X =
µ =

n
i=1 pi. Then for any a > 0, we have

P

P

Pr[X − µ ≥ a] ≤ exp(−a min(1/5, a/4µ)).

Moreover, for any a > 0, we have

Pr[X − µ ≤ −a] ≤ exp(−a2/2µ).

3. Algorithms under the IC Model

In this section, we solve both network inference and IMS under the IC model. In Section
3.1, we present network inference algorithms to estimate each edge probability within a small
additive error. In Section 3.2, we present several IMS algorithms.

3.1. Network Inference

In this section, we present a novel algorithm under the IC model for estimating the
edge probabilities of the underlying graph G, namely we need to ﬁnd an estimate ˆp of p
such that |ˆpuv − puv| ≤ ε for all u, v ∈ V . While all previous studies rely on the maximum
likelihood estimation to estimate ˆp (Netrapalli and Sanghavi, 2012; Narasimhan et al., 2015;
Pouget-Abadie and Horel, 2015), our algorithm is based on the following key observation on
the connection between puv and the one-step activation probabilities ap(v) and ap(v | ¯u). We
remark that our algorithm does not rely on the knowledge of edges in graph G, and in fact
it can be used to also reconstruct the edges of the graph.

6

Lemma 3. Under the IC model, for any u, v ∈ V with u 6= v,

puv =

ap(v) − ap(v | ¯u)
qu(1 − ap(v | ¯u))

.

Proof. To avoid confusion, we write the underlying graph G and the seed distribution D
explicitly in notation ap(·), namely ap(v) = apG,D(v). Consider the subgraph G′ = G \ {u}
by removing node u. Node v has two chances to be activated in one time step: either by
nodes in G′ (including the case where v itself is a seed) or by node u. Since D is a product
distribution, we have

apG,D(v) = apG′,D(v) + (1 − apG′,D(v))qupuv.

Besides, apG′,D(v) = apG,D(v | ¯u) since when considering one-step activation of v, node u not
being the seed is equivalent to removing it from the graph. Plugging the equality into the
last one, we obtain

puv =

apG,D(v) − apG,D(v | ¯u)
qu(1 − apG,D(v | ¯u))

,

which proves the lemma.

Equipped with Lemma 3, we are able to estimate puv by estimating qu, ap(v) and ap(v | ¯u)
respectively from cascade samples. Let tu = |{i ∈ [t] | u ∈ Si,0}| be the number of cascades
where u is a seed, t¯u = |{i ∈ [t] | u /∈ Si,0}| the number of cascades where u is not a seed,
tv = |{i ∈ [t] | v ∈ Si,1}| the number of cascades where v is activated in one time step
and tv
¯u = |{i ∈ [t] | u /∈ Si,0, v ∈ Si,1}| the number of cascades where u is not a seed and
¯u/t¯u are
v is activated in one time step. Then, ˆqu = tu/t,
good estimates of qu, ap(v) and ap(v | ¯u), respectively. The formal procedure is formulated
as Algorithm 1.

ap(v) = tv/t and

ap(v | ¯u) = tv

c

c

Algorithm 1 needs to work under Assumption 1 below, which ensures that all quantities
are well estimated. Assumption 1 consists of two conditions. The ﬁrst means that node
v ∈ V has a non-negligible probability of not being activated in one time step. The second
means that the probability of a node u ∈ V being selected as a seed is neither too low nor
too high.

Assumption 1 (Edge probabilities estimation under the IC model). For some pa-
rameters α ∈ (0, 1], γ ∈ (0, 1/2],

1. ap(v) ≤ 1 − α for all v ∈ V .
2. γ ≤ qu ≤ 1 − γ for all u ∈ V .

We now give an analysis of Algorithm 1. Lemma 4 below gives the number of samples

we need to estimate qu, ap(v) and ap(v | ¯u) within a small accuracy.

Lemma 4. Under Assumption 1, for any η ∈ (0, 4/5), δ ∈ (0, 1), for ˆqu,
deﬁned in Algorithm 1, if the number of samples t ≥ 16
we have

γη2 ln 12n

ap(v | ¯u)
δ , with probability at least 1 −δ,

ap(v), and

c

c

7

i=1.

Compute ˆqu = tu/t, where tu = |{i ∈ [t] | u ∈ Si,0}|.

Algorithm 1 Estimate Edge Probabilities
Require: A set of cascades (Si,0, Si,1, · · · , Si,n−1)t
Ensure: {ˆpuv}u,v∈V such that |ˆpuv − puv| ≤ ε for all u, v ∈ V .
1: for each u ∈ V do
2:
3: end for
4: for each v ∈ V do
Compute
5:
6: end for
7: for each v ∈ V do
8:
9:

ap(v) = tv/t, where tv = |{i ∈ [t] | v ∈ Si,1}|.

for each u ∈ V do

ap(v | ¯u) = tv

c

Compute
Si,0, v ∈ Si,1}|.
Let ˆpuv =

end for

10:
11:
12: end for
13: return {ˆpuv}u,v∈V .

cap(v)−cap(v | ¯u)
ˆqu(1−cap(v | ¯u)) .
c

¯u/t¯u, where t¯u = |{i ∈ [t] | u /∈ Si,0}| and tv

¯u = |{i ∈ [t] | u /∈

1. |ˆqu − qu| ≤ ηqu for all u ∈ V ,
2. |
3. |

ap(v) − ap(v)| ≤ η for all v ∈ V ,
ap(v | ¯u) − ap(v | ¯u)| ≤ η for all u, v ∈ V .
c
c

Proof. For a node u ∈ V , for i ∈ [t], let Xi be a 0-1 random variable such that Xi = 1 iﬀ
u ∈ Si,0. Thus ˆqu =

t
i=1 Xi/t. By Lemma 1,

Pr[|ˆqu−qu| ≥ ηqu] = Pr[|

P

t

i=1
X

Xi−tqu| ≥ ηtqu] ≤ 2 exp(−tquη2/3) ≤ 2 exp(−tγη2/3) ≤ δ/(3n).

The last inequality requires that t ≥ 3

γη2 ln 6n
δ .

For a node v ∈ V , for i ∈ [t], let Yi be a 0-1 random variable such that Yi = 1 iﬀ v ∈ Si,1.

Thus

ap(v) =

t
i=1 Yi/t. By Lemma 2 (a = ηt, µ = t · ap(v)),

Pr[|
c

P

ap(v) − ap(v)| ≥ η] ≤ exp(−ηt min(1/5, η/(4 · ap(v)))) + exp(−η2t/(2 · ap(v)))
≤ exp(−ηt min(1/5, η/4)) + exp(−η2t/2)
≤ δ/(6n) + δ/(6n) = δ/(3n).

c

The second inequality holds since ap(v) ≤ 1 by deﬁnition. The last inequality requires that
t ≥ 4

η2 ln 6n
δ .
For u ∈ V , let t¯u = |{i ∈ [t] | u /∈ Si,0}| be the number of cascades where u is not a seed.

Since qu ≤ 1 − γ, 8 ln(12n/δ)/η2 ≤ t(1 − qu)/2. By Lemma 1 (a = 1/2, µ = t(1 − qu)),

Pr[t¯u ≤ 8 ln(12n/δ)/η2] ≤ Pr[t¯u ≤ t(1−qu)/2] ≤ exp(−t(1−qu)/8) ≤ exp(−tγ/8) ≤ δ/(6n2).
8

The last inequality holds as long as t ≥ 16 ln(6n/δ)/γ.

Given a ﬁxed ℓ, assume that there are t¯u = ℓ cascades where u is not a seed. For
i ∈ [ℓ], let Zi be a 0-1 random variable such that Zi = 1 iﬀ v ∈ Si,1 in the i-th cascade,
ℓ
i=1 Zi/ℓ. By Lemma 2
among the ℓ cascades where u is not a seed. Then,
(a = ηℓ, µ = ℓ · ap(v | ¯u)),

ap(v | ¯u) =

ap(v | ¯u) − ap(v | ¯u)| ≥ η | tu = ℓ]

Pr[|
≤ exp(−ηℓ min(1/5, η/(4 · ap(v | ¯u)))) + exp(−η2ℓ/(2 · ap(v | ¯u)))
≤ exp(−ηℓ min(1/5, η/4)) + exp(−η2ℓ/2).

c

c

P

The last inequality holds since ap(v | u) ≤ 1 by deﬁnition. If ℓ ≥ 8 ln(12n/δ)/η2, then

Pr[|

ap(v | ¯u) − ap(v | ¯u)| ≥ η | tu = ℓ] ≤ δ/(6n2).

By law of total probability,

c

Pr[|

ap(v | ¯u) − ap(v | ¯u)| ≥ η]

≤

c

Xℓ<8 ln(12n/δ)/η2

+

Xℓ≥8 ln(12n/δ)/η2



≤ Pr[t¯u < 8 ln(12n/δ)/η2] + Pr[t¯u ≥ 8 ln(12n/δ)/η2] · δ/(6n2)
≤ δ/(6n2) + δ/(6n2) = δ/(3n2).

c



Pr[tu = ℓ] Pr[|

ap(v | ¯u) − ap(v | ¯u)| ≥ η | tu = ℓ]

The lemma follows immediately by union bound.

As stated below, Theorem 1 derives a theoretical guarantee for Algorithm 1.

Theorem 1. Under Assumption 1, for any ε, δ ∈ (0, 1), let η = εαγ/4 < 1/8, and
{ˆpuv}u,v∈V be the set of edge probabilities returned by Algorithm 1. If the number of cascades
t ≥ 16
δ , with probability at least 1 − δ, for any u, v ∈ V , |ˆpuv − puv| ≤ ε.

ε2α2γ3 ln 12n

δ = 256

γη2 ln 12n

Proof. With probability at least 1 − δ, all the events in Lemma 4 occur. We assume
that this is exactly the case in the following. Since ap(v | ¯u) ≤ ap(v) ≤ 1 − α, we have
1 − ap(v | ¯u) ≥ α. By the value of η and the assumption that qu ≥ γ, we have

η ≤

εγ
4

(1 − ap(v | ¯u)) ≤

ε
4

qu(1 − ap(v | ¯u)).

(1)

To prove ˆpuv ≤ puv + ε, we have

ˆpuv =

≤

ap(v | ¯u)
ap(v | ¯u))
c
c

ap(v) −
ˆqu(1 −
ap(v) − ap(v | ¯u) + 2η
c
(1 − η)qu(1 − ap(v | ¯u) − η)
9

≤

≤

ap(v) − ap(v | ¯u) + 2η
(1 − η)(1 − εγ/4)qu(1 − ap(v | ¯u))

puv + ε/2
(1 − η)(1 − εγ/4)

≤ puv + ε.

(2)

The ﬁrst inequality holds due to Lemma 4. The second inequality holds by applying the ﬁrst
inequality in Eq. (1). The third inequality holds due to Lemma 3 and the second inequality
in Eq. (1). To see the correctness of the last inequality, ﬁrst observe that

(puv + ε)(1 − η)(1 − εγ/4)
≥ (puv + ε)(1 − η − εγ/4)
≥ (puv + ε) − (1 + ǫ)(η + εγ/4).

Next, note that

(1 + ǫ)(η + εγ/4) = (1 + ǫ)(1 + α)εγ/4 ≤ (1 + ε)ε/4 ≤ ε/2.

The equality is due to the deﬁnition of η. The two inequalities hold since α ∈ (0, 1], γ ∈
(0, 1/2] and ε ∈ (0, 1), respectively. Combining the above two observations, we have the
desired inequality

(puv + ε)(1 − η)(1 − εγ/4) ≥ (puv + ε) − ε/2 = puv + ε/2.

On the other hand, to prove ˆpuv ≥ puv − ε, ﬁrst assume that puv ≥ ε, since otherwise the

claim would be trivial for ˆpuv ≥ 0. We now have

ˆpuv =

≥

≥

≥

ap(v | ¯u)
ap(v | ¯u))
c
c

ap(v) −
ˆqu(1 −
ap(v) − ap(v | ¯u) − 2η
c
(1 + η)qu(1 − ap(v | ¯u) + η)
apG(v) − ap(v | ¯u) − 2η
(1 + η)(1 + εγ/4)qu(1 − ap(v | ¯u))

puv − ε/2
(1 + η)(1 + εγ/4)

≥ puv − ε.

The ﬁrst inequality holds due to Lemma 4. The second inequality holds by applying the
ﬁrst inequality in Eq. (1). The third inequality holds due to Lemma 3 and the second
inequality in Eq. (1). The last inequality follows from a similar argument as the one for the
last inequality in Eq. (2), and we omit it for conciseness.

With the ability of estimating edge probabilities, we further show that we can re-
cover the graph structure by a standard threshold approach (Netrapalli and Sanghavi, 2012;
Pouget-Abadie and Horel, 2015). The formal procedure is depicted as Algorithm 2, which
estimates the edge probabilities to a prescribed accuracy and returns the edges whose esti-
mated probabilities are above a prescribed threshold. Its guarantee is shown in Theorem 2,
which shows that no “zero-probability edge” is incorrectly recognized as an edge. Besides,
only small-probability edges are omitted, which is reasonable for practical use.

10

Algorithm 2 Recover Network Structure
Require: A set of cascades (Si,0, Si,1, · · · , Si,n−1)t
Ensure: An estimated edge set ˆE.
1: {ˆpuv}u,v∈V = Estimate-Edge-Probabilities

i=1, parameter β ∈ (0, 1).

((Si,0, Si,1, · · · , Si,n−1)t

i=1). { With estimation accuracy β/2.}

2: return ˆE = {(u, v) | ˆpuv > β/2}.

Theorem 2. Under Assumption 1, if the number of cascades t ≥ 1024
δ , with proba-
bility at least 1 − δ, the edge set ˆE returned by Algorithm 2 satisﬁes (1) ˆE ⊆ E, and (2) if
puv > β, then (u, v) ∈ ˆE. As a corollary, if puv > β for all (u, v) ∈ E, then ˆE = E.

α2β2γ3 ln 4n

Proof. By Theorem 1, |ˆpuv − puv| ≤ ε = β/2 for all u, v ∈ V with probability at least 1 − δ.
If (u, v) /∈ E, then puv = 0 and hence ˆpuv ≤ β/2, which implies (u, v) /∈ ˆE. Thus, ˆE ⊆ E.
If (u, v) ∈ E and puv > β. Then, ˆpuv ≥ puv − β/2 > β/2 and hence (u, v) ∈ ˆE. Finally, if
puv > β for all (u, v) ∈ E, then E ⊆ ˆE and hence ˆE = E, which concludes the proof.

result

is worth comparing the

Discussion. It
in (Netrapalli and Sanghavi, 2012;
Narasimhan et al., 2015; Pouget-Abadie and Horel, 2015) with ours, since all of them stud-
ied network inference under the IC model. Speciﬁcally, Netrapalli and Sanghavi (2012)
initiated the study of recovering network structure and did not consider the estimation of
edge parameters. Narasimhan et al. (2015) and Pouget-Abadie and Horel (2015) studied
how to estimate edge parameters. Both of them used the Euclidean norm of the edge prob-
ability vector as the measurement of accuracy, while we use the inﬁnite norm. Besides, in
(Narasimhan et al., 2015), it was additionally assumed that the network structure is known
in advance. In (Pouget-Abadie and Horel, 2015), totally diﬀerent assumptions were used,
which seems incomparable to ours, and thus we will not further compare against it below.
There are several important diﬀerences besides the above. First, the approaches taken
are diﬀerent. All the algorithms in the previous works build on the maximum likelihood
estimation (MLE) and require to solve a convex program, while we directly ﬁnd a closed-
form expression for the edge probability puv, thus rendering fast implementation.

Second, the assumptions required are diﬀerent. The assumptions puv > β for all u, v ∈ V
and γ ≤ qu ≤ 1 − γ for all u ∈ V are also required in the previous works (though may
be presented in diﬀerent forms). The key diﬀerence is the condition ap(v) ≤ 1 − α for all
v ∈ V .
In (Netrapalli and Sanghavi, 2012), its role is replaced by the correlation decay
u∈N (v) puv ≤ 1 − α for all v ∈ V . In (Narasimhan et al.,
condition, which requires that
2015), it is instead assumed that 1 −
u∈N (v)(1 − puv) ≤ 1 − α for all v ∈ V . By observing
that ap(v) ≤ 1 −
u∈N (v)(1 − puv) ≤
u∈N (v) puv (see the appendix), it is easy to see that
our assumptions are the weakest compared with those in (Netrapalli and Sanghavi, 2012;
Narasimhan et al., 2015). Besides, Assumption 1 enjoys the advantage that it is veriﬁable,
since one can ﬁnd suitable values for α and γ by estimating ap(v) and qu from cascade
samples. However, it is impossible to verify the assumptions in (Netrapalli and Sanghavi,
2012; Narasimhan et al., 2015) based only on cascade samples. We remark that our network
11

Q
P

P

Q

Algorithm 3 IMS-IC under Assumption 1
Require: A set of cascades (Si,0, Si,1, · · · , Si,n−1)t
1: {ˆpuv}u,v∈V = Estimate-Edge-Probabilities

i=1 and k ∈ N+.

((Si,0, Si,1, · · · , Si,n−1)t

i=1). { With estimation accuracy εk/(2n3).}

2: Let ˆG = (V, E, ˆp).
3: Let S A = A( ˆG, k), where A is a κ-approximation IM algorithm.
4: return S A.

inference algorithm replies on the assumption that each seed node is independently sampled.
This assumption is also made in (Netrapalli and Sanghavi, 2012; Narasimhan et al., 2015)
for the MLE method, but conceptually it might be easier to relax this assumption with
MLE. We leave the relaxation of the independence sampling assumption of our method as
a future work.

1

lower

Finally, our algorithm has

u∈N (v)(1 − puv) ≤
α7β2γ D2 log n

sample complexity compared with those in
(Netrapalli and Sanghavi, 2012; Narasimhan et al., 2015). Assume that ap(v) ≤ 1 −
u∈N (v) puv ≤ 1 − α. Then, Netrapalli and Sanghavi (2012) needs
˜O(
δ ) samples to recover network structure, where D is the maximum in-degree
Q
of the network, while we only need O(
δ ) samples by Theorem 2. On the other hand,
assume that the network structure is known and m = |E|. Narasimhan et al. (2015) needs
˜O(
εα2γ2 m ln n
δ )
samples by achieving |ˆpuv − puv| ≤

δ ) samples to achieve kˆp − pk2

2 ≤ ε, while we only need O(

ε2α2β2γ2(1−γ)4 mn ln n

α2β2γ3 ln 4n

P

1

1

1

ε
m .

3.2. Inﬂuence Maximization from Samples

p

In this section, we present several IMS algorithms under the IC model.

In Section
3.2.1, we present an approximation-preserving algorithm under Assumption 1. In Section
3.2.2, we show that under an alternative assumption (Assumption 2), there is a constant
approximation algorithm for the problem. An attractive feature of Assumption 2 (compared
to Assumption 1) is that it has no requirement on the network. We also show that by slightly
strengthening Assumption 2, we again obtain an approximation-preserving algorithm.

3.2.1. IMS under Assumption 1

Our ﬁrst IMS algorithm is presented as Algorithm 3. It follows the canonical learning-
and-then-optimization approach by ﬁrst learning a surrogate graph ˆG = (V, E, ˆp) from the
cascades and then executing any κ-approximation algorithm A for standard inﬂuence maxi-
mization on ˆG to obtain a solution as output. The construction of ˆG builds on Algorithm 1
and is obtained by estimating all the edge probabilities to a suﬃciently small additive error.
Algorithm 3 works under Assumption 1, since Algorithm 1 does.

The correctness of Algorithm 3 relies on Lemma 5, which translates the estimation
error in edge probabilities into the learning error in the inﬂuence spread function. We
use it in Theorem 3 to prove that with high probability, Algorithm 3 almost preserves the
approximation ratio of any standard inﬂuence maximization algorithm A.

12

Lemma 5 (Narasimhan et al. 2015). Fix S ⊆ V . Under the IC model, for any two edge
probability vectors p, ˆp with kp − ˆpk1 ≤ ε/n, we have |σp(S) − σ ˆp(S)| ≤ ε.

Theorem 3. Under Assumption 1, for any ε ∈ (0, 1) and k ∈ N+, suppose that the number
of cascades t ≥ 1024
δ . Let A be a κ-approximation algorithm for inﬂuence maxi-
ε2α2γ3
mization. Let S A be the set returned by Algorithm 3 and S ∗ be the optimal solution on the
original graph. We have

n6
k2 ln 12n

Pr[σ(S A) ≥ (κ − ε)σ(S ∗)] ≥ 1 − δ.

Proof. By Theorem 1, with probability at least 1 − δ, for any u, v ∈ V , |ˆpuv − puv| ≤
εk/(2n3). Hence, kp − ˆpk1 =
u,v∈V |puv − ˆpuv| ≤ εk/(2n). Applying this condition to
Lemma 5, we have that |σp(S) − σ ˆp(S)| ≤ εk/2 for every seed set S. We thus have

P

σ(S A) ≥ σ ˆp(S A) − εk/2 ≥ κ · σ ˆp(S ∗) − εk/2
≥ κ · (σ(S ∗) − εk/2) − εk/2
= κ · σ(S ∗) − (1 + κ)εk/2 ≥ (κ − ε)σ(S ∗).

The second inequality holds since S A is a κ-approximation on ˆG. The last inequality holds
since σ(S ∗) ≥ k ≥ (1 + κ)k/2.

Compared with our learning algorithms for network inference, Algorithm 3 has an ad-
ditional overhead of n6/k2 in the number of cascades. This is because it needs to estimate
edge probabilities within an additive error of at most εk/(2n3). One can also invoke known
network inference algorithms other than ours in Algorithm 3 to obtain a similar approxi-
mate solution, but as discussed above, this only incurs higher sample complexity. We are
not aware of any approach to reduce the sample complexity and leave it as an interesting
open problem.

3.2.2. IMS under Assumptions Independent of the Network

Condition 1 of Assumption 1 depends on the diﬀusion network, and hence our Algorithm
3 may not be applicable to all networks. In this section, we show that under an alternative
assumption (Assumption 2), which is entirely independent of the diﬀusion network, there
still exists a constant approximation IMS algorithm (Algorithm 4).

Assumption 2 (IMS under the IC model, independent of the network). For
some constant c > 0 and parameter γ ∈ (0, 1/2],

u∈V qu ≤ ck.

1.
2. γ ≤ qu ≤ 1 − γ for all u ∈ V .

P

Assumption 2 consists of two conditions. The condition

u∈V qu ≤ ck replaces the
condition ap(v) ≤ 1 − α in Assumption 1. It means that a random seed set drawn from
the product distribution D has an expected size at most linear in k (but not necessarily
13

P

bounded above by k). Assumption 2 puts forward two plausible requirements for the seed
distribution D and has no requirement for the underlying network. Thus, in principle, one
can handle any social networks, as long as the seed set sampling is reasonable according to
Assumption 2.

We now describe the high-level idea of Algorithm 4. It might be surprising at ﬁrst glance
that one can remove the condition ap(v) ≤ 1 − α for all v ∈ V . After all, it is very hard
to learn information about incoming edges of v if ap(v) is very close to 1. To handle this
diﬃculty, recall that ap(v) is deﬁned as the activation probability of v in one time step.
Hence, if ap(v) is close to 1, v will be active with high probability starting from a random
seed set. The observation suggests that one can divide nodes into two parts according to
their ap(·). For the nodes with small ap(·), Assumption 1 is satisﬁed and one can ﬁnd a
good approximation for them via a similar approach as Algorithm 3. For the nodes with
large ap(·), a random seed set is already a good approximation for them. So there is no
need to learn their incoming edges. A technical issue here is that a random seed set may
not be a feasible solution for the maximization task. This is why we introduce Condition 1
of Assumption 2, by which the expected size of the seed set is at most linear in k. With the
condition, we can replace the random seed set by its random subset of size k while keeping a
constant approximation. To summarize, we ﬁnd two candidate solutions whose union must
be a good approximation over the whole network. If we choose one of them randomly, we
will ﬁnally obtain a feasible solution with constant approximation.

Following the guidance of the above idea, Algorithm 4 ﬁrst computes an estimate

ap(v)
ap(v) <
of ap(v) for all v ∈ V and partitions V into two disjoint subsets V1 = {v ∈ V |
1−δ/(4n)} and V2 = V \V1. It then estimates the probabilities of incoming edges of V1 using
c
Algorithm 1 and sets the probabilities of incoming edges of V2 to 1 directly for technical
reasons. The constructed graph is denoted by ˆG. Let T1 be a κ-approximation on ˆG and
T2 = S1,0 be the ﬁrst random seed set. Finally, Algorithm 4 selects T1 or T2 with equal
probability, and if it selects T2 while |T2| > k, it further selects a random subset of T2 with
size k as the ﬁnal output seed set S A.

c

We now give an analysis of Algorithm 4. Our analysis requires a technical lemma (Lemma
6). Informally, Lemma 6 means that for any set R ⊆ V , the inﬂuence of the seed set S when
setting the probabilities of all incoming edges of R to 1 is no larger than the inﬂuence of
S ∪ R.

Lemma 6. Let G = (V, E, p) be a directed graph and R ⊆ V . Let G′ = (V, E, p′) be a
directed graph obtained from G as follows: p′
uv = puv otherwise. Then,
for any S ⊆ V , we have σp(S ∪ R) ≥ σp′
(S).

uv = 1 if v ∈ R and p′

Proof. To prove Lemma 6, we will use live-edge graphs to interpret the IC model and
help understand the inﬂuence spread. Formally, a live-edge graph corresponding to the IC
model is a random subgraph of G such that each edge (u, v) is picked independently with
probability puv. Let σu(S) be the probability that u is reachable from S in the live-edge
graph. Then, σu(S) is also the probability that u is activated by S and hence the inﬂuence
spread function σ(S) =
u∈V σu(S). For a node u ∈ V , a ﬁxed edge set A and a seed set S,

P

14

Algorithm 4 IMS-IC under Assumption 2
Require: A set of cascades (Si,0, Si,1, · · · , Si,n−1)t

number of samples t′ ∈ [t] used to estimate ap(v)’s.

i=1, k ∈ N+, error probability δ > 0,

1: Set V1 = V and V2 = ∅.
2: for each v ∈ V do
3: Use the ﬁrst t′ samples (Si,0, Si,1, · · · , Si,n−1)t′

|{i ∈ [t′] | v ∈ Si,1}|.
if

ap(v) ≥ 1 − δ/(4n) then
4:
Set ˆpuv = 1 for all u ∈ V .
5:
V1 = V1 \ {v} and V2 = V2 ∪ {v}.
6:
c
end if
7:
8: end for
9: {ˆpuv}u∈V,v∈V1 = Estimate-Edge-Probabilities

i=1 to compute

ap(v) = tv/t′, where tv =

c

i=t′+1) on V1. {With accuracy εk/(2n3), α = δ/(6n) in Assumption

((Si,0, Si,1, · · · , Si,n−1)t
1.}

10: Let ˆG = (V, E, ˆp).
11: T1 = A( ˆG, k), where A is a κ-approximation IM algorithm.
12: T2 = S1,0.
13: Let T be a random set by picking T1 and T2 with equal probability. If T ≤ k, let S A = T ;

otherwise, let S A be a uniformly random subset of T with |S A| = k.

14: return S A.

let 1u(A, S) be the indicator variable that u is reachable from S through edges in A. Then,
σu(S) can be written as

σu(S) =

pab

(1 − pab)1u(A, S).

A⊆E
X

Y(a,b)∈A

Y(a,b)∈E\A

Let B = {(u, v) | u ∈ V, v ∈ R} be the set of incoming edges of R. By deﬁnition, p′

ab = 1
for (a, b) ∈ B and p′
ab = pab otherwise. Let B ⊆ B be a random subset of B such that for
each edge (a, b) ∈ B, (a, b) ∈ B independently with probability pab. For a ﬁxed u ∈ V , we
consider its activation probability. First, we have

σp
u(S ∪ R) =

pab

(1 − pab)E[1u(A ∪ B, S ∪ R)]

XA⊆E\B Y(a,b)∈A

Y(a,b)∈E\(A∪B)

=

pab

(1 − pab)1u(A ∪ B, S ∪ R).

XA⊆E\B Y(a,b)∈A

Y(a,b)∈E\(A∪B)
The expectation in the ﬁrst equality is taken over the randomness of B. The second equality
holds since R itself is part of the seed set, and it makes no diﬀerence whether its incoming
edges B are picked into the live-edge graph. Next, by the deﬁnition of p′,

σp′
u (S) =

pab

(1 − pab)1u(A ∪ B, S).

XA⊆E\B Y(a,b)∈A

Y(a,b)∈E\(A∪B)
15

Clearly, 1u(A ∪ B, S ∪ R) ≥ 1u(A ∪ B, S). It follows that σp
σp(S ∪ R) ≥ σp′

(S).

u(S ∪ R) ≥ σp′

u (S) and therefore

ln 12n

n8
Theorem 4. Under Assumption 2, suppose that the number of cascades t ≥ 36864
k2 ln 36n
δ +
ε2δ2γ3
δ , and the number of samples used to estimate ap(v)’s is t′ = 72n2
72n2
ln 12n
δ . Let A be
δ2
δ2
δ . Let S A
a κ-approximation algorithm for inﬂuence maximization. Assume that k ≥ 3
c ln 3
be the set returned by Algorithm 4 and S ∗ be the optimal solution on the original graph. We
have that S A is a feasible solution, and

Pr[E[σ(S A)] ≥ min

1
2c

, 1

(cid:27)

(cid:26)

κ − ε
2

σ(S ∗)] ≥ 1 − δ,

where the probability is taken over the randomness of (Si,0, Si,1, · · · , Si,n−1)t
pectation is taken over the randomness from line 13 of Algorithm 4.

i=1 and the ex-

Proof. Let G = (V, E, p) be the original graph. Let V1 = {v ∈ V |
4n } and
V2 = V \ V1, deﬁned as in Algorithm 4. Let B = {(u, v) | u ∈ V, v ∈ V2} be the set of all
edges pointing to some node in V2. Let G′ = (V, E, p′) be a directed graph obtained from G
uv = puv otherwise. Let ˆG = (V, E, ˆp) be a directed
as follows: p′
graph obtained from G′ by replacing puv with ˆpuv for any (u, v) /∈ B. Clearly, ˆG is exactly
the same graph we constructed in Algorithm 4.

uv = 1 if (u, v) ∈ B and p′

ap(v) ≤ 1 − δ

c

For any node v ∈ V , by Lemma 2, when t′ = 72n2
δ2

ln 12n
δ ,

ap(v) − ap(v)| ≥ δ/(12n)]

Pr[|
≤ exp(−t′(δ/(12n)) min(1/5, (δ/(12n)) · (1/ap(v)))
+ exp(−t′(δ/(12n))2/2ap(v))
≤ δ/(6n) + δ/(6n) = δ/(3n).

c

c
c

c
c

By union bound, with probability 1 − δ/3, for all nodes v ∈ V , |
Speciﬁcally, for a node v ∈ V1 with
1 − δ/(6n). For a node v ∈ V2 with
1 − δ/(3n).

ap(v) ≤ 1 − δ/(4n), we have ap(v) ≤
ap(v) > 1 − δ/(4n), we have ap(v) ≥

c

ap(v) − ap(v)| ≤ δ/(12n).
ap(v) + δ/(12n) ≤
ap(v) − δ/(12n) ≥

Since for any v ∈ V2, ap(v) ≥ 1 − δ/(3n), by union bound, it means that with probability
at least 1 − δ/3 in one time step all nodes in V2 are activated. Assume that indeed all nodes
in V2 are activated in one time step. Then, we have σp(T1 ∪T2) = σp(T1 ∪S1,0) ≥ σp(T1 ∪V2).
By plugging S = T1, R = V2 into Lemma 6, we obtain σp(T1 ∪ V2) ≥ σp′
(T1). Therefore, by
submodularity of σ, σp(T1) + σp(T2) ≥ σp(T1 ∪ T2) ≥ σp′

(T1).

Since {ˆp}u,v

is obtained by running Estimate-Edge-Probability with parameters
εk/(2n3), α = δ/(6n), γ, when the number of cascades t − t′ ≥ 36864
δ , with prob-
ε2δ2γ3
ability 1 − δ/3, we have |ˆpuv − puv| ≤ εk/(2n3) for any (u, v) /∈ B. Since ˆpuv = p′
uv = 1
for (u, v) ∈ B, we have kˆp − p′k1 ≤ εk/(2n). Therefore, by Lemma 5, for any S ⊆ V ,
|σ ˆp(S) − σp′

(S)| ≤ εk/2. We have

n8
k2 ln 36n

σp′

(T1) ≥ σ ˆp(T1) − εk/2 ≥ κ · σ ˆp(S ∗) − εk/2

16

≥ κ · (σp′
≥ κ · (σp(S ∗) − εk/2) − εk/2 ≥ (κ − ε)σp(S ∗).

(S ∗) − εk/2) − εk/2

The second inequality holds since T1 is a κ approximation of S ∗ on ˆG. The forth inequality
holds since σp′
uv ≥ puv for any (u, v) ∈ E. The last
inequality holds as long as (1 + κ)k/2 ≤ k ≤ σp(S ∗), which holds trivially since κ ≤ 1 and
σp(S ∗) ≥ k.

(S) ≥ σp(S) for any S ⊆ V , due to p′

Combining the previous inequalities, we have

σp(T1) + σp(T2) ≥ σp′

(T1) ≥ (κ − ε)σp(S ∗),

which implies that E[σp(T )] = 1

2 (σp(T1) + σp(T2)) ≥ 1

2(κ − ε)σp(S ∗).

Finally, since

u∈V qu ≤ ck, Pr[|S1,0| ≥ 2ck] ≤ e−ck/3 ≤ δ/3 when k ≥ 3

δ . Assume
that |T2| = |S1,0| ≤ 2ck. If T = T1 or T = T2 but |T2| ≤ k, then S A = T . If T = T2 and
|T2| > k, then S A is a uniform subset of T with size k. Since σ(·) is submodular, we have
E[σp(S A)] ≥ min

E[σp(T )] ≥ min

c ln 3

P

κ−ε

To conclude, by union bound, with probability at least 1 − δ, S A is a feasible solution
(cid:9)

(cid:8)

(cid:8)

2 σp(S∗).

1
2c , 1

1
2c , 1

and E[σp(S A)] ≥ min

(cid:9)
1
2c , 1

κ−ε

2 σp(S ∗).

(cid:8)

(cid:9)

Improving the approximation ratio. Compared with Algorithm 3, Algorithm 4 has a worse
(though still constant) approximation ratio. We show that if the constant c in Assump-
tion 2 equals to some prescribed small ε ∈ (0, 1/3), we can modify Algorithm 4 to be an
approximation-preserving algorithm as follows: let T1 = A( ˆG, (1 − 2ε)k) and returns T1 ∪ T2
It is easy to see that the modiﬁed algorithm works since T1 loses little in the
directly.
approximation ratio and T1 ∪ T2 is feasible with high probability. The formal procedure is
presented in Algorithm 5 and its guarantee is presented below.

n8
k2 ln 36n

Theorem 5. Under Assumption 2 with c = ε ∈ (0, 1/3), suppose that the number of cas-
cades t ≥ 36864
δ and the number of samples used to estimate ap(v)’s is
ε2δ2γ3
t′ = 72n2
ln 12n
δ . Let A be an κ-approximation algorithm for inﬂuence maximization. Assume
δ2
δ . Let S A be the set returned by Algorithm 5 and S ∗ be the optimal solution
that k ≥ 3
ε ln 3
on the original graph. We have

δ + 72n2
δ2

ln 12n

Pr[|S A| ≤ k ∧ σ(S A) ≥ (κ − 3ε)σ(S ∗)] ≥ 1 − δ.

Proof. By a similar analysis for Algorithm 4, σp(T1 ∪ T2) ≥ σp′
least 1 − δ/3, and for any S ⊆ V , |σ ˆp(S) − σp′
We thus have

(T1) with probability at
(S)| ≤ εk/2 with probability at least 1 − δ/3.

σp′

(T1) ≥ σ ˆp(T1) − εk/2

≥ κ(1 − 2ε) · σ ˆp(S ∗) − εk/2
≥ κ(1 − 2ε) · (σp′
(S ∗) − εk/2) − εk/2
≥ κ(1 − 2ε) · (σp(S ∗) − εk/2) − εk/2

17

Algorithm 5 IMS-IC under Assumption 2 with c = ε
Require: A set of cascades (Si,0, Si,1, · · · , Si,n−1)t

i=1 and k ∈ N+, parameter ε ∈ (0, 1/3),

error probability δ > 0, number of samples t′ ∈ [t] used to estimate ap(v)’s.

1: Set V1 = V and V2 = ∅.
2: for each v ∈ V do
3: Use the ﬁrst t′ samples (Si,0, Si,1, · · · , Si,n−1)t′

|{i ∈ [t′] | v ∈ Si,1}|.
if

ap(v) ≥ 1 − δ/(4n) then
4:
Set ˆpuv = 1 for all u ∈ V .
5:
V1 = V1 \ {v} and V2 = V2 ∪ {v}.
6:
c
end if
7:
8: end for
9: {ˆpuv}u∈V,v∈V1 = Estimate-Edge-Probabilities

i=1 to compute

ap(v) = tv/t′, where tv =

c

i=t′+1) on V1. {With accuracy εk/(2n3), α = δ/(6n) in Assumption

((Si,0, Si,1, · · · , Si,n−1)t
1.}

10: Let ˆG = (V, E, ˆp).
11: T1 = A( ˆG, (1 − 2ε)k), where A is a κ-approximation algorithm for inﬂuence maximiza-

tion.
12: T2 = S1,0.
13: return S A = T1 ∪ T2.

≥ (κ − 2ε) · σp(S ∗) − (1 + κ)εk/2
≥ (κ − 3ε) · σp(S ∗).

The second inequality holds since T1 is a κ(1 − 2ε) approximation of S ∗ on ˆG. The forth
inequality holds since σp′
(S) ≥ σp(S) for any S ⊆ V , due to p′
uv ≥ puv for any (u, v) ∈ E.
The last inequality holds as long as (1 + κ)k/2 ≤ k ≤ σp(S ∗), which holds trivially since
κ ≤ 1 and σp(S ∗) ≥ k. Therefore, we have σp(S A) = σp(T1 ∪ T2) ≥ (κ − 3ε)σp(S ∗).

k ≥ 3

Finally, since
ε ln 3
To conclude, by union bound, with probability at least 1 − δ, |S A| ≤ k and σ(S A) ≥

δ . If |S1,0| ≤ 2εk, |S A| = |T1 ∪ T2| ≤ (1 − 2ε)k + 2εk = k.

u∈V qu ≤ εk, by Lemma 1, Pr[|S1,0| ≥ 2εk] ≤ e−εk/3 ≤ δ/3 when

P

(κ − 3ε)σ(S ∗).

4. Algorithms under the LT Model

In this section, we solve both network inference and IMS problems under the LT model.
In Section 4.1, we present a network inference algorithm. In Section 4.2, we present an IMS
algorithm.

4.1. Network Inference

In this section, we present a network inference algorithm under the LT model which ﬁnds
an estimate ˆw of w such that | ˆwuv − wuv| ≤ ε for all u, v ∈ V . The algorithm is similar to

18

the one under the IC model and based on the key observation on the connection between
wuv and the one-step activation probabilities ap(v) and ap(v | ¯u). We ﬁrst give an explicit
expression of ap(v) under the LT model.

Lemma 7. Under the LT model, ap(v) = qv + (1 − qv)

u∈N (v) quwuv for any v ∈ V .

Proof. Fix v ∈ V . For v to be active in one time step, v is either picked as a seed or
activated by its in-neighbors which are picked as seeds. By the fact that D is a product
distribution, in our notations, we have

P

ap(v) = qv + (1 − qv)ap(v | ¯v).

Let S0 ∼ D and R = S0 ∩ N(v). For u ∈ N(v), let Xu ∈ {0, 1} indicate whether u ∈ S0.
Then, E[Xu] = qu. Let X =
u∈N (v) wuvXu. By the linearity of expectation, E[X] =

u∈N (v) quwuv.
On the other hand, by law of total probability, the facts that D is a product distribution

P

P
and D, θv are independent,

ap(v | ¯v) =

XR⊆N (v)

Pr
D

[R] Pr
θv

[v ∈ S1 | v /∈ S0, R] =

[R]

Pr
D

u∈R
X

XR⊆N (v)

wuv = E[X].

Therefore, we have ap(v | ¯v) =

u∈N (v) quwuv.

By plugging it back, we ﬁnally obtain ap(v) = qv + (1 − qv)

P

u∈N (v) quwuv.

Next, we derive a closed-form expression for the edge weight wuv from Lemma 7.

P

Lemma 8. Under the LT model, for any u, v ∈ V with u 6= v,

wuv =

ap(v) − ap(v | ¯u)
qu(1 − qv)

.

Proof. To avoid confusion, we write the underlying graph G and the seed distribution D
explicitly in notation ap(·), namely ap(v) = apG,D(v). Consider the subgraph G′ = G \ {u}
by removing node u. Since when considering one-step activation of v, node u not being the
seed is equivalent to removing it from the graph, we have

Next, by Lemma 7, we have

apG,D(v | ¯u) = apG′,D(v).

apG′,D(v) = qv + (1 − qv)

qu′wu′v.

apG,D(v) = qv + (1 − qv)

qu′wu′v.

Xu′∈N (v)\{u}

Xu′∈N (v)

19

We therefore obtain that

apG,D(v) − apG,D(v | ¯u) = (1 − qv)quwuv.

By rearranging the equality, we obtain

wuv =

apG,D(v) − apG,D(v | ¯u)
qu(1 − qv)

.

Equipped with the lemma, we are able to estimate wuv by estimating qu, ap(v) and
ap(v | ¯u) respectively from cascade samples. Let tu = |{i ∈ [t] | u ∈ Si,0}| be the number of
cascades where u is a seed, t¯u = |{i ∈ [t] | u /∈ Si,0}| the number of cascades where u is not
a seed, tv = |{i ∈ [t] | v ∈ Si,1}| the number of cascades where v is activated in one time
step and tv
¯u = |{i ∈ [t] | u /∈ Si,0, v ∈ Si,1}| the number of cascades where u is not a seed
and v is activated in one time step. Then, ˆqu = tu/t,
¯u/t¯u are
good estimates of qu, ap(v) and ap(v | ¯u), respectively. The formal procedure is formulated
as Algorithm 6.

ap(v) = tv/t and

ap(v | ¯u) = tv

c

c

Algorithm 6 needs to work under Assumption 3 below, which means that the probability
of a node u ∈ V being selected as a seed is neither too low nor too high. This assumption
ensures that the above quantities are well estimated. Compared with Assumption 1, As-
sumption 3 does not need the condition ap(v) ≤ 1 − α and hence imposes no requirement on
the network. This is because the condition ap(v) ≤ 1−α gives a lower bound for 1−ap(v | ¯u)
and leads to a tighter estimate of it, while in the closed-form expression of wuv, 1 − ap(v | ¯u)
does not appear in the denominator, and hence a loose estimate in the lack of the condition
still suﬃces.

Assumption 3 (Edge weights estimation under the LT model). For some parame-
ter γ ∈ (0, 1/2],

γ ≤ qu ≤ 1 − γ, ∀u ∈ V.

We now give an analysis of Algorithm 6. Lemma 9 gives the number of samples we need
to estimate qu, ap(v) and ap(v | ¯u) within a small accuracy. Its proof is exactly the same as
that of Lemma 4 and therefore omitted.

Lemma 9. Under Assumption 3, for any η ∈ (0, 4/5), δ ∈ (0, 1), for ˆqu,
deﬁned in Algorithm 6, if the number of samples t ≥ 16
we have

γη2 ln 12n

ap(v | ¯u)
δ , with probability at least 1 −δ,

ap(v), and

c

c

1. |ˆqu − qu| ≤ ηqu for all u ∈ V ,
2. |
3. |

ap(v) − ap(v)| ≤ η for all v ∈ V ,
ap(v | ¯u) − ap(v | ¯u)| ≤ η for all u, v ∈ V .
c
c

Theorem 6. Under Assumption 3, for any ε, δ ∈ (0, 1), let η = εγ2/4 < 1/8. Let
{ ˆwuv}u,v∈V be the set of edge weights returned by Algorithm 6. If the number of cascades
t ≥ 16
δ , with probability at least 1 − δ, for any u, v ∈ V , | ˆwuv − wuv| ≤ ε.

ε2γ6 ln 12n

γη2 ln 12n

δ = 256

20

i=1.

Compute ˆqu = tu/t, where tu = |{i ∈ [t] | u ∈ Si,0}|.

Algorithm 6 Estimate Edge Weights
Require: A set of cascades (Si,0, Si,1, · · · , Si,n−1)t
Ensure: { ˆwuv}u,v∈V such that | ˆwuv − wuv| ≤ ε for all u, v ∈ V .
1: for each u ∈ V do
2:
3: end for
4: for each v ∈ V do
Compute
5:
6: end for
7: for each v ∈ V do
8:
9:

ap(v) = tv/t, where tv = |{i ∈ [t] | v ∈ Si,1}|.

for each u ∈ V do

ap(v | ¯u) = tv

c

Compute
Si,0, v ∈ Si,1}|.
Let ˆwuv =

end for

10:
11:
12: end for
13: return { ˆwuv}u,v∈V .

cap(v)−cap(v | ¯u)
c
ˆqu(1−ˆqv)

.

¯u/t¯u, where t¯u = |{i ∈ [t] | u /∈ Si,0}| and tv

¯u = |{i ∈ [t] | u /∈

Proof. With probability at least 1 − δ, all the events in Lemma 9 occur. We assume
that this is exactly the case in the following. By the choice of η and the assumption that
γ ≤ qu ≤ 1 − γ, we have

εγ
4

ε
4

(1 − qv) ≤

qu(1 − qv).

(3)

To prove ˆwuv ≤ wuv + ε, we have

η ≤

ˆwuv =

ap(v) −

ap(v | u)

ˆqu(1 − ˆqv)

≤

≤

≤

c

ap(v) − ap(v | u) + 2η
c
(1 − η)qu(1 − qv − ηqv)

ap(v) − ap(v | u) + 2η
(1 − η)(1 − εγ/4)qu(1 − qv)

wuv + ε/2
(1 − η)(1 − εγ/4)

≤ wuv + ε.

The ﬁrst inequality holds due to Lemma 9. The second inequality holds by applying the ﬁrst
inequality in Eq. (3) and the fact that qv ≤ 1. The third inequality holds due to Lemma 8
and the second inequality in Eq. (3). The correctness of the last inequality follows the same
argument as Theorem 1.

On the other hand, to prove ˆwuv ≥ wuv − ε, ﬁrst assume that wuv ≥ ε, since otherwise

21

the claim would be trivial for ˆwuv ≥ 0. We now have

ˆwuv =

ap(v) −

ap(v | u)

ˆqu(1 − ˆqv)

≥

≥

≥

c

ap(v) − ap(v | u) − 2η
c
(1 + η)qu(1 − qv + ηqv)

ap(v) − ap(v | u) − 2η
(1 + η)(1 + εγ/4)qu(1 − qv)

wuv − ε/2
(1 + η)(1 + εγ/4)

≥ wuv − ε.

The ﬁrst inequality holds due to Lemma 9. The second inequality holds by applying the ﬁrst
inequality in Eq. (3) and the fact that qv ≤ 1. The third inequality holds due to Lemma 8
and the second inequality in Eq. (3). The correctness of the last inequality follows the same
argument as Theorem 1.

As in the IC model, Algorithm 6 can be adapted to recover the graph structure. For

compactness, we omit the adapted algorithm.

In general, the ˆw returned by Algorithm 6 does not necessarily satisﬁes the normalization
u∈N (v) ˆwuv ≤ 1 for all v ∈ V . The condition is crucial in deﬁning the live-
condition that
edge graph under the LT model, which helps the design of fast IM algorithm such as RR-set
(Borgs et al., 2014) and our IMS algorithm in the next subsection. For this reason, we
achieve the normalization condition by rescaling ˆw, as Corollary 1 below shows.

P

Corollary 1. For any ε, δ ∈ (0, 1), let { ˆwuv}u,v∈V be the edge weights returned by Algorithm
6 under Assumption 3 with t ≥ 1024
δ cascade samples, where D is the maximum in-
degree of the underlying graph G. Let w′
1+ε/2 . Then, with probability at least 1 − δ,

ε2γ6 D2 ln 12n

uv = ˆwuv

1.
2. |w′
P

uv ≤ 1 for all v ∈ V , and

u∈N (v) w′
uv − wuv| ≤ ε for any u, v ∈ V .

Proof. By Theorem 6, with probability at least 1 − δ, for any u, v ∈ V , | ˆwuv − wuv| ≤
ε/(2D). We assume that this is exactly the case in the following.

Fix v ∈ V , we have

w′

uv =

Xu∈N (v)

Xu∈N (v)

ˆwuv
1 + ε/2

≤

1
1 + ε/2

Xu∈N (v)

(wuv + ε/(2D)) ≤

1
1 + ε/2

(1 + ε/2) = 1.

The last inequality holds since the original w satisﬁes

u∈N (v) wuv ≤ 1.

Next, we have

|w′

uv − wuv| =

≤

1
1 + ε/2
ε/(2D)
1 + ε/2

| ˆwuv − (1 + ε/2)wuv| ≤

P
1
1 + ε/2

| ˆwuv − wuv| +

ε/2
1 + ε/2

|wuv|

+

ε/2
1 + ε/2

≤

ε
2

) ≤ ε.

1
D

(1 +

22

Algorithm 7 IMS-LT under Assumption 3
i=1 and k ∈ N+.
Require: A set of cascades (Si,0, Si,1, · · · , Si,n−1)t
1: { ˆwuv}u,v∈V = Estimate-Edge-Weights((Si,0, Si,1, · · · , Si,n−1)t

curacy εk/(2Dn3).}
uv = ˆwuv

1+ε/2 for all u, v ∈ V .

2: Let w′
3: Let G′ = (V, E, w′).
4: Let S A = A(G′, k), where A is a κ-approximation IM algorithm.
5: return S A.

i=1). { With estimation ac-

4.2. Inﬂuence Maximization from Samples

In this section, we present an IMS algorithm (Algorithm 7) under the LT model. Our
algorithm is approximation-preserving and imposes no requirement on the network. It follows
the canonical learning-and-then-optimization approach by ﬁrst learning a surrogate graph
G′ = (V, E, w′) from the cascades and then executing any κ-approximation algorithm A for
standard inﬂuence maximization on G′ to obtain a solution as output. The construction
of G′ builds on Algorithm 6 and is obtained by ﬁrst estimating all the edge weights to
a suﬃciently small additive error and then rescale them as in Corollary 1 to meet the
normalization condition. Algorithm 7 works under Assumption 3, since Algorithm 6 does.
Consequently, Algorithm 7 can handle arbitrary social networks, since Assumption 3 imposes
no requirement for the network.

As Lemma 5 for the IC model, to prove the correctness of Algorithm 7, we need to
bound the diﬀerence between two LT inﬂuence functions with diﬀerent edge parameters by
the diﬀerence of the parameters. We show this in Lemma 10 below. Note that to apply
Lemma 10, the normalization condition must hold. This explains why Algorithm 7 rescales
the estimated edge weights before it runs a standard IM algorithm.

We ﬁrst present Lemma 10 and its proof.

Lemma 10. Under the LT model, for any two edge weight vectors w, w′ such that (1) kw −
w′k1 ≤ ε/n, and (2) both w and w′ satisfy the normalization condition, we have |σw(S) −
σw′

(S)| ≤ ε for all S.

v . Clearly, σw(S) =

Proof. To prove Lemma 10, we will use live-edge graphs to interpret the LT model and
help understand the inﬂuence spread. For node v ∈ V , its (ﬁnal) activation probability
is denoted by σw
v∈V σw
v (S) for any seed set S ⊆ V . For node
b ∈ V , let Eb be the set of its incoming edges. The live-edge graph under the LT model
is generated as follows: For each node b ∈ V , among all of its incoming edges, (u, v) ∈ Eb
is selected exclusively as the single live edge with probability wuv, and no edge is selected
with probability 1 −
a∈N (b) wab. The selection is independent among all nodes b ∈ V . Let
A ⊆ E be the edge set of some realization of the live-edge graph. Then, A satisﬁes that
|A ∩ Eb| ≤ 1 for all b ∈ V . For convenience, let A = {A ⊆ E | |A ∩ Eb| ≤ 1, ∀b ∈ V } and
A ∩ Eb = {e(b)} if A ∩ Eb 6= ∅. Finally, let 1v(A, S) be the indicator variable such that
v (S)
1v(A, S) = 1 if and only if v is reachable from S via edges in A.

it is proved that σw

P

P

23

can be written as the summation of 1v(A, S) over all realizations of the live-edge graph (see
Chen et al., 2013):

σw
v (S) =

we(b)

(1 −

wab)1v(A, S).

With the above interpretation, we now bound the L∞ norm of the gradient of σw
v .

XA∈A Yb∈V :A∩Eb6=∅

Yb∈V :A∩Eb=∅

Xa∈N (b)

∂σw

v (S)
∂wcd

(cid:12)
(cid:12)
(cid:12)
=
(cid:12)

(cid:12)
(cid:12)
∂
(cid:12)
(cid:12)
∂wcd "

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
+ (1 −

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

−

≤

wcd

we(b)

(1 −

wab)1v(A ∪ {(c, d)}, S)

XA∈A,A∩Ed=∅ Yb6=d:A∩Eb6=∅

Yb6=d:A∩Eb=∅

Xa∈N (b)

wad)

we(b)

(1 −

wab)1v(A, S)

Xa∈N (d)

XA∈A,A∩Ed=∅ Yb6=d:A∩Eb6=∅

Yb6=d:A∩Eb=∅

Xa∈N (b)

we(b)

(1 −

wab)1v(A ∪ {(c, d)}, S)

#(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

XA∈A,A∩Ed=∅ Yb6=d:A∩Eb6=∅

Yb6=d:A∩Eb=∅

Xa∈N (b)

we(b)

(1 −

wab)1v(A, S)

XA∈A,A∩Ed=∅ Yb6=d:A∩Eb6=∅

Yb6=d:A∩Eb=∅

Xa∈N (b)

we(b)

(1 −

wab)

XA∈A,A∩Ed=∅ Yb6=d:A∩Eb6=∅

Yb6=d:A∩Eb=∅

Xa∈N (b)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= 1 −

wad ≤ 1.

Xa∈N (d)

The ﬁrst equality holds since when computing the partial derivative at wcd, we only need
to concern the terms where wcd appears, and wcd appears when (1) (c, d) is selected as the
single live edge of d, or (2) no incoming edges of d are selected. The inequality follows
from the fact hat 0 ≤ |1v(A ∪ {(c, d)}S) − 1v(A, S)| ≤ 1. The last equality holds since the
summation is over A ∩ Ed = ∅, which equals to the probability that no incoming edges of d
are selected. Clearly, the above inequality means that k∇wσw

v (S)k∞ ≤ 1.

By the mean-value theorem, there is a ¯w = sw + (1 − s)w′ for some s ∈ [0, 1] such that

|σw
Therefore, |σw(S) − σw′

v (S) − σw′
(S)| ≤

v (S)| = k∇ ¯wσw

v (S)k∞kw − w′k1 ≤ ε/n.

v∈V |σw

v (S) − σw′

v (S)| ≤ ε, which completes the proof.

Finally, the performance of Algorithm 7 is presented in the following theorem.

P

Theorem 7. Under Assumption 3, for any ε ∈ (0, 1) and k ∈ N+, suppose that the number
of cascades t ≥ 4096
δ . Let A be a κ-approximation algorithm for inﬂuence maxi-
ε2γ3
mization. Let S A be the set returned by Algorithm 7 and S ∗ be the optimal solution on the
original graph. Under Assumption 3, we have

D2n6
k2

ln 12n

Pr[σ(S A) ≥ (κ − ε)σ(S ∗)] ≥ 1 − δ.
24

Proof. By Corollary 1, with probability at least 1 − δ, we have 1)
v ∈ V , and 2) for any u, v ∈ V , |w′
uv| ≤ εk/(2n). Applying this condition to Lemma 10, we have that |σw(S)−σw′
w′
P
for every seed set S. We thus have

uv − wuv| ≤ εk/(2n3). Hence, kw − w′k1 =

u∈N (v) w′

P

uv ≤ 1 for all
u,v∈V |wuv −
(S)| ≤ εk/2

σ(S A) ≥ σ ˆp(S A) − εk/2 ≥ κ · σ ˆp(S ∗) − εk/2
≥ κ · (σ(S ∗) − εk/2) − εk/2
= κ · σ(S ∗) − (1 + κ)εk/2 ≥ (κ − ε)σ(S ∗).

The second inequality holds since S A is a κ-approximation on ˆG. The last inequality holds
since σ(S ∗) ≥ k ≥ (1 + κ)k/2.

5. Conclusion and Future Work

In this paper, we conduct a rigorous theoretical treatment to the inﬂuence maximization
from samples (IMS) problem under both IC and LT models, and provide several end-to-end
IMS algorithms with constant approximation guarantee. We also provide novel and eﬃcient
algorithms for network inference with weaker assumptions.

There are many future directions to extend and improve this work. First, our IMS
algorithms require a large number of samples (though polynomial) since we have to estimate
edge probabilities to a very high accuracy.
It is very interesting to investigate how to
improve the sample complexity by leveraging sparsity and diﬀerent importance of edges in
the networks. Second, our samples contain activation sets at every step. One can further
study how to do IMS when we only observe the ﬁnal activation set. Other directions include
studying IMS for other stochastic diﬀusion models (for example, the cumulative activation
model in (Shan et al., 2019)), relaxing the independent seed node sampling assumption, and
going beyond inﬂuence maximization to study other optimization tasks directly from data
samples.

Appendix A. Comparing Assumptions

We

summarize

2012;
Narasimhan et al., 2015) below and show that they are strictly stronger than our
assumptions.

(Netrapalli and Sanghavi,

assumptions

used

the

in

Assumption 4 (Assumptions in Netrapalli and Sanghavi 2012). For some parame-
ters α, β ∈ (0, 1),

1. puv ≥ β for all (u, v) ∈ E.
2. (Correlation decay)
3. qudv < 1/2 for all u, v ∈ V .

P

u∈N in(v) puv < 1 − α for all v ∈ V .

Assumption 5 (Assumptions in Narasimhan et al. 2015). For some parameters β ≥
α ∈ (0, 1/2) and γ ∈ (0, 1),

25

1. puv ≥ β for all (u, v) ∈ E.
2. 1 −
3. γ ≤ qu ≤ 1 − γ for all u ∈ V .

u∈N in(v)(1 − puv) ≤ 1 − α for all v ∈ V .

Q

Lemma 11. ap(v) ≤ 1 −

u∈N in(v)(1 − puv) ≤

u∈N in(v) puv.

Q
Proof. The ﬁrst inequality follows from ap(v) = 1−
puv), since qu ≤ 1 for all u ∈ V . The second inequality follows from the claim below.

u∈N in(v)(1−qupuv) ≤ 1−

P

u∈N in(v)(1−

Q

Q

Claim 1. For any x1, · · · , xn ∈ [0, 1],

n
i=1(1 − xi) ≥ 1 −

n
i=1 xi.

Proof of the Claim. The claim holds trivially when n = 1. Assume that the claim holds for
any k < n. Then,

P

Q

n

n−1

n−1

n

(1 − xi) =

(1 − xi)(1 − xn) ≥ (1 −

xi)(1 − xn) ≥ 1 −

xi.

i=1
Y

i=1
Y

i=1
X

i=1
X

The two inequalities both hold by induction.

References

Abrahao, B.D., Chierichetti, F., Kleinberg, R., Panconesi, A., 2013. Trace complexity of network inference,
in: Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, Chicago, IL, USA. pp. 491–499.

Alon, N., Spencer, J.H., 2008. The Probabilistic Method, Third Edition. Wiley-Interscience series in discrete

mathematics and optimization, Wiley.

Badanidiyuru, A., Dobzinski, S., Fu, H., Kleinberg, R., Nisan, N., Roughgarden, T., 2012. Sketching
valuation functions, in: Proceedings of the 23th Annual ACM-SIAM Symposium on Discrete Algorithms,
Kyoto, Japan. pp. 1025–1035.

Balkanski, E., Immorlica, N., Singer, Y., 2017a. The importance of communities for learning to inﬂuence,
in: Advances in Neural Information Processing Systems 30 (NIPS 2017), Long Beach, CA, USA. pp.
5862–5871.

Balkanski, E., Rubinstein, A., Singer, Y., 2016. The power of optimization from samples, in: Advances in

Neural Information Processing Systems 29 (NIPS 2016), Barcelona, Spain. pp. 4017–4025.

Balkanski, E., Rubinstein, A., Singer, Y., 2017b. The limitations of optimization from samples, in: Proceed-
ings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, Montreal, QC, Canada.
pp. 1016–1027.

Borgs, C., Brautbar, M., Chayes, J.T., Lucier, B., 2014. Maximizing social inﬂuence in nearly optimal time,
in: Proceedings of the 25th Annual ACM-SIAM Symposium on Discrete Algorithms, Portland, Oregon,
USA. pp. 946–957.

Chen, W., Lakshmanan, L.V.S., Castillo, C., 2013. Information and Inﬂuence Propagation in Social Net-

works. Synthesis Lectures on Data Management, Morgan & Claypool Publishers.

Chen, W., Sun, X., Zhang, J., Zhang, Z., 2020. Optimization from structured samples for coverage functions,
in: Proceedings of the 37th International Conference on Machine Learning, Virtual Event. pp. 1715–1724.
Chen, W., Wang, Y., Yuan, Y., Wang, Q., 2016. Combinatorial multi-armed bandit and its extension to
probabilistically triggered arms. J. Mach. Learn. Res. 17, 1–33. A preliminary version appeared as Chen,
Wang, and Yuan, “Combinatorial multi-armed bandit: general framework, results and applications”,
ICML 2013.

26

Daneshmand, H., Gomez-Rodriguez, M., Song, L., Sch¨olkopf, B., 2014. Estimating diﬀusion network struc-
tures: Recovery conditions, sample complexity & soft-thresholding algorithm, in: Proceedings of the 31th
International Conference on Machine Learning, Beijing, China. pp. 793–801.

Du, N., Liang, Y., Balcan, M., Song, L., 2014. Inﬂuence function learning in information diﬀusion networks,
in: Proceedings of the 31th International Conference on Machine Learning, Beijing, China. pp. 2016–2024.
Du, N., Song, L., Gomez-Rodriguez, M., Zha, H., 2013. Scalable inﬂuence estimation in continuous-time
diﬀusion networks, in: Advances in Neural Information Processing Systems 26 (NIPS 2013), Lake Tahoe,
Nevada, USA. pp. 3147–3155.

Du, N., Song, L., Smola, A.J., Yuan, M., 2012. Learning networks of heterogeneous inﬂuence, in: Advances
in Neural Information Processing Systems 25 (NIPS 2012), Lake Tahoe, Nevada, USA. pp. 2789–2797.

Feige, U., 1998. A threshold of ln n for approximating set cover. J. ACM 45, 634–652.
Gomez-Rodriguez, M., Balduzzi, D., Sch¨olkopf, B., 2011. Uncovering the temporal dynamics of diﬀusion
networks, in: Proceedings of the 28th International Conference on Machine Learning, Washington, USA.
pp. 561–568.

Gomez-Rodriguez, M., Leskovec, J., Krause, A., 2010.

Inferring networks of diﬀusion and inﬂuence, in:
Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, Washington, DC, USA. pp. 1019–1028.

Goyal, A., Bonchi, F., Lakshmanan, L.V.S., 2011. A data-based approach to social inﬂuence maximization.

Proc. VLDB Endow. 5, 73–84.

He, X., Xu, K., Kempe, D., Liu, Y., 2016. Learning inﬂuence functions from incomplete observations, in:
Advances in Neural Information Processing Systems 29 (NIPS 2016), Barcelona, Spain. pp. 2065–2073.
Kempe, D., Kleinberg, J.M., Tardos, ´E., 2003. Maximizing the spread of inﬂuence through a social network,
in: Proceedings of the 9th ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, Washington, DC, USA. pp. 137–146.

Mitzenmacher, M., Upfal, E., 2005. Probability and Computing: Randomized Algorithms and Probabilistic

Analysis. Cambridge University Press.

Myers, S.A., Leskovec, J., 2010. On the convexity of latent social network inference, in: Advances in Neural
Information Processing Systems 23 (NIPS 2010), Vancouver, British Columbia, Canada. pp. 1741–1749.
Narasimhan, H., Parkes, D.C., Singer, Y., 2015. Learnability of inﬂuence in networks, in: Advances in
Neural Information Processing Systems 28 (NIPS 2015), Montreal, Quebec, Canada. pp. 3186–3194.
Nemhauser, G.L., Wolsey, L.A., Fisher, M.L., 1978. An analysis of approximations for maximizing submod-

ular set functions - I. Math. Program. 14, 265–294.

Netrapalli, P., Sanghavi, S., 2012. Learning the graph of epidemic cascades, in: Proceedings of the 12th
ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling
of Computer Systems, London, UK. pp. 211–222.

Pouget-Abadie, J., Horel, T., 2015.

Inferring graphs from cascades: A sparse recovery framework, in:

Proceedings of the 32th International Conference on Machine Learning, Lille, France. pp. 977–986.

Rosenfeld, N., Balkanski, E., Globerson, A., Singer, Y., 2018. Learning to optimize combinatorial func-
tions, in: Proceedings of the 35th International Conference on Machine Learning, Stockholmsm¨assan,
Stockholm, Sweden. pp. 4371–4380.

Shan, X., Chen, W., Li, Q., Sun, X., Zhang, J., 2019. Cumulative activation in social networks. Sci. China

Inf. Sci. 62, 52103:1–52103:21.

27

