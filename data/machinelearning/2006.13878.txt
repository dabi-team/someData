Effective Elastic Scaling of Deep Learning
Workloads

Vaibhav Saxena
IBM Research, India

K. R. Jayaram
IBM Research, USA

Saurav Basu
Microsoft, India

Yogish Sabharwal
IBM Research, India

Ashish Verma
IBM Research, USA

0
2
0
2

n
u
J

4
2

]

C
D
.
s
c
[

1
v
8
7
8
3
1
.
6
0
0
2
:
v
i
X
r
a

Abstract—The increased use of deep learning (DL)

in
led to the
academia, government and industry has,
popularity of on-premise and cloud-hosted deep learning plat-
forms, whose goals are to enable organizations utilize expensive
resources effectively, and to share said resources among multiple
teams in a fair and effective manner.

in turn,

In this paper, we examine the elastic scaling of Deep Learning
(DL) jobs over large-scale training platforms and propose a
novel resource allocation strategy for DL training jobs, resulting
in improved job run time performance as well as increased
cluster utilization. We begin by analyzing DL workloads and
exploit the fact that DL jobs can be run with a range of
batch sizes without affecting their ﬁnal accuracy. We formulate
an optimization problem that explores a dynamic batch size
allocation to individual DL jobs based on their scaling efﬁciency,
when running on multiple nodes. We design a fast dynamic
programming based optimizer to solve this problem in real-
time to determine jobs that can be scaled up/down, and use this
optimizer in an autoscaler to dynamically change the allocated
resources and batch sizes of individual DL jobs.

We demonstrate empirically that our elastic scaling algorithm
can complete up to ≈ 2× as many jobs as compared to a strong
baseline algorithm that also scales the number of GPUs but does
not change the batch size. We also demonstrate that the average
completion time with our algorithm is up to ≈ 10× faster than
that of the baseline.

Index Terms—elasticity, deep learning, variable batch size

I. INTRODUCTION

Deep Learning (DL) models can achieve state-of-the-art
accuracy, sometimes exceeding human-level performance [37]
in image, video, speech and text recognition with applications
to driver-less cars, voice control in consumer devices, video
search, social networks, etc. The rise of easy-to-use, open-
source DL frameworks like TensorFlow [25], PyTorch [22]
and Caffe [21] have further enhanced the popularity of DL.
The increased use of DL has, in turn, led to the popularity of
on-premise and cloud-hosted DL platforms, whose goals are
to enable organizations utilize expensive resources effectively,
and to share said resources among multiple teams.

Many existing elastic scaling techniques [50], [38], [40],
[12], [5], [16] are hard to apply to DL platforms/clusters
because they focus on cluster elasticity – scaling the size of
the cluster with VMs in response to workload variations. DL
workloads and their system (hardware/software) stack present
unique challenges for cluster elasticity. DL training is gen-
erally performed on parallel systems using high performance
GPU accelerators, due to their efﬁciency in matrix computa-
tions and convolutions, and high (GPU) memory bandwidth.

1

Also, many DL frameworks and workloads beneﬁt from and
are optimized for high speed interconnection networks (like
NVLink [26], Inﬁniband [2], RDMA and 100G Ethernet).
To make effective use of such interconnects, DL workloads
need locality aware scheduling [31], i.e., learning processes
in a distributed DL job have to be located as close to each
other as possible. So, unless the entire datacenter has the same
hardware (GPUs) and interconnect, it becomes hard to quickly
scale a DL cluster. Often, one cannot scale the cluster by
acquiring VMs, even GPU enabled VMs, because they can be
provisioned anywhere in the datacenter. Cluster scaling with
support for locality aware placement and scheduling requires
manual intervention, installation and interconnect conﬁgura-
tion taking several hours [32], [31]. Second, there are often
cost-related reasons why DL cluster scaling is undesirable.

Hence, elasticity is vital to optimize resource utilization
and improve user experience of DL training clusters, which,
like other clusters, experience wide variations in workloads.
For example, sometimes the cluster may become underutilized
when the number of jobs is very low leading to idle resources.
Similarly, the cluster may become oversubscribed and some
jobs are unable to run due to shortage of resources. Hence,
in this paper, we consider job elasticity [10], [1], where jobs
themselves are scaled to ﬁt a ﬁxed size cluster. We leverage a
key characteristic of DL jobs – batch size – for job elasticity.
Batch size is the number of samples used to determine
the gradient of the parameters of the model at each update
step of the gradient descent algorithm [6]. A very small
batch size may result in noisy gradients which takes large
number of iterations to converge, while using very large batch
sizes has shown to result in poor generalization [34]. Just as
an example for training neural networks on image datasets,
researchers have typically used batch-sizes in the range from
32 to 8192 [13]. Although recently there is evidence to be
able use much larger batch-sizes (upto 32K) without affecting
the accuracy signiﬁcantly [54]. Furthermore, researchers have
shown that instead of keeping a ﬁxed batch-size over the
training period, the batch-size can be changed dynamically
without affecting the model accuracy [47], [11], [3]. We also
performed experiments demonstrating this behaviour which
are discussed in Section II-C.

Batch-size has a direct implication on using the compute
resources in the cluster and hence the time to complete the
training process. In other words, by increasing the batch-size a
higher number of GPUs can be leveraged to ﬁnish the training
faster and vice-versa.

 
 
 
 
 
 
The goal and main contribution of this paper is to use
permissible batch size ranges for performing efﬁcient resource
allocation and elastic scaling of DL jobs. To this end, this
paper makes the following technical contributions:

1) A job scalability analyzer to determine the scaling
characteristics of DL jobs and their runtime with respect
to the number of GPUs allotted and batch sizes used
(Section III-B).

2) A fast, dynamic programming based optimizer that uses
the job scalability analyzer to allocate optimal resources
and batch sizes to DL jobs in real time to maximize
cluster throughput (Section III-C).

3) An autoscaler that uses the optimizer, cluster metrics
and user input to elastically scale jobs using checkpoint-
resume (Section III-D). We demonstrate that the design
and implementation of the autoscaler is independent of
the middleware used to manage DL training jobs.
4) A detailed empirical evaluation (Section IV) of the
efﬁcacy of our elastic scaling algorithm demonstrating
that it is able to complete up to ≈ 2× the number of
jobs in comparison to the baseline algorithm that does
not consider or vary batch size. We also demonstrate that
when queueing of jobs due to resource scarcity is not
possible/desired, our elastic scaling algorithm drops up
to ≈ 3× fewer jobs, and that the average job completion
time with our algorithm is up to ≈ 10× better than that
of the baseline when job queueing is enabled.

II. BACKGROUND & ARCHITECTURE

This is a paper on effective elastic scaling mechanisms for
DL training workloads. We present our resource allocation
and elastic scaling techniques in the context of DL training
platform.

A. DL Training Platforms (DLP)

A DLP is a multi-user, multi-tenant middleware for deep
learning training. Architecturally, it resides above a cluster
manager like Kubernetes [23] or Mesos [17] and uses the clus-
ter manager for deployment, placement, and failure detection
of training jobs. The goals of a DL platform are (i) to enable
data scientists to focus only on their training application, and
not worry about any setup, security and failure handling, (ii)
to enable organizations to share expensive DL hardware, and
(iii) to serve as a backbone for commercial DL as-a-Service
offerings.

DL platforms are responsible for training jobs from start
to ﬁnish, and should be highly available, scalable and ef-
ﬁcient. For this paper, we use FfDL(Framework for Deep
Learning) [18], [30] which is an open-source DL platform.
A detailed description of design and implementation of FfDL
is beyond the scope of this paper and we refer the reader to
[18], [30]. Our autoscaler only requires the DLP to support
creation, deletion, halting (with a checkpoint) and resumption
of DL training jobs; many DLPs already do this [31]. The
actual job scaling happens through halt/resume – halting the
job that is currently running and resuming the same from

its last checkpoint. FfDL employs Kubernetes [23] (K8S) for
container orchestration and cluster management. This enables
FfDL to create replicated learners (for distributed training) and
is well suited for DL frameworks like Horovod and distributed
Tensorﬂow.

Fig. 1. High-level architecture of our techniques

B. DL Training Jobs

Data scientists typically write DL training jobs using a DL
framework like Caffe [33], PyTorch [22], TensorFlow [25],
Horovod [27], etc., which internally use different communica-
tion libraries (MPI/OpenMP/Gloo). To abstract away frame-
work/communication speciﬁc details, and simplify job man-
agement, DL platforms, including FfDL, often use Docker
containers [20]. FfDL takes user code, and automatically
instantiates Docker containers (from its library of Docker
images) containing everything necessary to run the training
job.

A DL training job typically consists of a set of learning pro-
cesses (“learners”), with each learner running in a Docker con-
tainer using one or more GPUs. Each learner container consists
of a framework Docker image (Caffe, TensorFlow, etc.) instan-
tiated with user code, job parameters, locations where training
data can be accessed and where checkpoints/logs/results have
to stored, and credentials (certiﬁcates, keys, etc.) used for
communication, data access and storing results.

Each learner comprises of a copy of the neural network
model that is being trained. The model has a set of weights that
are learned through time, and an associated model architecture
which deﬁnes the rules to update the weights. The values of
weight updates at an iteration is called the gradient. Each
learner works on a subset of the entire data at every iteration
and prepares its copy of the gradient, which is then combined
to form a single gradient through a learner-to-learner commu-
nication call such as AllReduce at every iteration. Each worker
updates its copy of the model weights with the combined
gradient at every iteration.

2

Cluster ManagerCreate/Halt/Resume APIDL PlatformAutoscalerCommunication and synchronization methods internal to
frameworks (i.e., MPI, OpenMP, use of speciﬁc parameter
servers, etc.) are preserved while the job is executed in the
DL platform and are part of the framework Docker image.

Job parameters,

including the source of

training data,
credentials to access training data, framework, number of
learners, location where results and logs should be stored,
learning rate, etc., are speciﬁed by the user/data scientist using
a manifest ﬁle.

C. Min- and Max- Batch Size

An important characteristic of deep learning jobs is the
batch size. The batch size deﬁnes the number of samples
that are processed by the network in order to collectively
determine the gradient for performing the update step in the
gradient descent algorithm. If the batch size is too large, then
the training runs very slowly whereas if the batch size is too
small, the accuracy often degrades (references). Batch size is
a very important parameter when scaling deep learning jobs.
When the number of nodes is increased, users often increase
the batch size linearly as this leads to improved resource
utilization. As mentioned earlier, prior work has not dealt with
taking into account or modifying the job parameters while
scheduling/elastically scaling the jobs.

Fig. 2. Similarity of ﬁnal accuracy in DL jobs when the batch size (BS) varies
within an allowable range. Three BS variation patterns are used. start epoch
lists the starting epoch for the corresponding BS change.

As mentioned in the Introduction, a peculiar characteristic of
a DL job is that the ﬁnal accuracy of the model remains similar
if the training is done within a certain range of batchsizes,
called the optimal batchsize range. This has been previously
noted in some recent research [47], [13], [34].

We have experimented extensively in continuance to the
hypothesis in [47], [13], [34] to verify that both increasing
and decreasing the batchsize within the tolerable range does
not affect the ﬁnal convergence. One such example is shown
in Figure 2. The model is VGG11 [46] which is trained on

3

the dataset CIFAR100 [35] for 160 epochs. The curve in
red (Fixed BS) is a usual training with a ﬁxed batchsize of
512 images throughout the training. The other three curves
(Dynamic BS) show a training where the batchsize has been
increased or decreased at random epochs, but within a range
of 128-1024 images. The comparison of the curves show that
both ﬁxed BS and dynamic (varying) BS lead to similar ﬁnal
accuracy (± 0.5%).

D. Assumptions

Focus on GPU Allocation: Most DL platforms (FfDL
included) recommend using a speciﬁc number of CPU cores
and speciﬁc amount of RAM corresponding to each GPU
type [18], [30], in order to maximize performance. For ex-
ample, the DL platform may recommend using 4 CPU cores
and 8GB RAM when 1 K80 is used by the DL job, and 6
CPU cores and 12 GB RAM when 1 V100 is used. It has
been observed [30] that data scientists do not deviate from
these recommendations. So, it is typically sufﬁcient to ﬁrst
solve GPU allocation and use the platform’s guidelines on
proportionally scaling CPUs and RAM.
Synchronous DL: Our results are presented for the typical
case of synchronous distributed DL, where all the learners
work on the exact same copy of the model weights. This is a
much more common paradigm than the sometimes used asyn-
chronous distributed DL paradigm, where the model updates
to separate learners happen independently. Synchronous DL
not only leads to generally better convergence, but the results
are easy to reproduce.
Homogeneous Distributed Jobs: We also assume that
distributed DL jobs are homogeneous, i.e., each learner in
a job uses the same type of GPU and jobs do not contain
learners using a mix of say K80 and P100 GPUs. This is quite
typical in DL training; given vast differences in compute power
between different GPU types, distributed jobs are mainly run
in homogeneous fashion for effective data parallelism. Second,
while our techniques do not require an entire datacenter to be
homogeneous with respect to the types of GPUs; we assume
that the datacenter is divided into clusters of homogeneous
GPUs, and each autoscaler only manages homogeneous GPUs.

III. DL-AWARE ELASTIC SCALING

A. Overview and Design Principles

Our mechanism is composed of three core modules - (i) a
job scalability analyzer (JSA), (ii) an optimizer and (iii) an
autoscaler. The optimizer’s goal is to allocate GPUs to the
current set of jobs in the cluster in an (optimal) manner that
maximizes cluster utilization and throughput (c.f. Section III-C
for the precise formulation of throughput; informally it cap-
tures overall progress). The role of the JSA is to determine the
scaling characteristics of the jobs in order to estimate their
runtime so that the optimizer can make optimal scheduling
decisions. The goals of the autoscaler are to (i) receive and
buffer incoming jobs, (ii) periodically consult the optimizer to
determine the set of jobs that can be allocated resources, and

The run-time for a DL training job can broadly be seg-
regated into two components: the processing time and the
communication time. DL training being data parallel, a batch
is typically divided equally amongst the GPUs and each GPU
works on the part of the batch allocated to it; we refer to the
size of the batch allocated per GPU as “batch-size-per-GPU”
(computed as batch size divided by number of GPUs). As the
processing happens in parallel over all the GPUs allotted to the
job, the processing time is simply the time required to process
batch-size-per-GPU on a single GPU. The communication
time is the time required to reduce the gradients at the end
of each iteration; this is dependent on the number of weights
in the model and the number of GPUs.

Thus it sufﬁces for the JSA to estimate: (i) for a given job,
the processing time for various batch-size-per-GPU values on
a single GPU; this is done by collecting certain job speciﬁc
scaling characteristics. (ii) time for communicating different
sizes of model weight buffers across a range of GPUs, i.e.,
the Allreduce-time for various combinations of # of weights
and # of GPUs in the cluster; this is estimated using generic
scaling characteristics.

1) Job speciﬁc scaling characteristics: To estimate the

processing time:

1) The JSA considers every new job in the temporary
holding buffer, executes it on a single GPU for a few
iterations with different values of batch-size-per-GPU
and records the average time taken per iteration. The
methodology for estimating the runtime per iteration is
similar to [53].

2) The batch-size-per-GPU values are chosen uniformly
between bmin and bmax (the maximum batch-size-per-
GPU feasible for the job).

3) These scaling characteristics (average run-time per it-
eration) are tagged to the job metadata and the job is
transferred to the temporary autoscaler buffer.

4) Note: Later,

if the optimizer requires the processing
time for a value of batch-size-per-GPU outside those
considered in Step 2, it is determined by interpolating
values computed in Steps 1-2.

Thus,

this allows us to estimate the processing time
(bgpu) for a job j when running with batch-size-per-GPU

tproc
j
of bgpu.

2) Generic scaling characteristics:

To estimate the
AllReduce-time, the JSA performs the AllReduce operation
over

• A range of GPUs {1, · · · , kmax }.
• for each number of GPUs k ∈ {1, · · · , kmax }, a range
of model weight sizes (10M, 20M, 30M, · · · , 100M
weights).

Later, if the communication time is required for other values
of number of weights, it is determined by interpolation of
the available values. We note that it sufﬁces to perform this
estimation infrequently (it
is not required to be done for
every job;
it can be performed whenever there are major
changes to the cluster). Thus, this allows us to estimate the

4

Fig. 3. Overview of our DL-aware elastic mechanism

the number of GPUs to allot to each of the jobs in the set, and
(iii) interact with the DL platform to scale jobs up or down.
Our mechanism is designed to be agnostic to the internals of
speciﬁc DL platforms. The autoscaler, in conjunction with the
runtime estimator and optimizer performs resource allocation;
placement of jobs on nodes is performed by the DL platform
and the cluster manager inside the DL platform.

In contrast to existing DL cloud scheduling systems, our
system also takes the minimum and the maximum batchsize
(bmin and bmax respectively), as additional user speciﬁed
parameters as part of the input job speciﬁcation. In addition, it
allows jobs to be scheduled up to a maximum number of kmax
GPUs. Our system varies the batch size for a job (within the
limits speciﬁed by the user) in order to maximize the system
utilization. The range of acceptable batch sizes are typically
well known for commonly used models. In many cases, users
determine the range of batch sizes based on the acceptable
time for completion of the job. In such cases, the JSA can
be used to predict the run-time for different batch-sizes. See
section V-A for additional discussion on choosing min- and
max- batchsizes.

In the autoscaler, new jobs are placed into a temporary
holding buffer on arrival, in the same order in which they
arrive. The job scalability analyzer processes the jobs from
the holding buffer. It computes the scaling characteristics of
the job and then transfers the job back to the autoscaler buffer
from where it can be processed further.

B. Job Scalability Analyzer (JSA)

JSA’s goal is to determine the scaling characteristics of each
job, so that the run time of the job can be estimated for various
(cid:104)# of GP U s, batch size (cid:105) combinations as required. Certain
scaling characteristics are job dependent whereas others are
generic, i.e., dependent on the cluster. The JSA determines
the generic scaling characteristics once at startup and repeats
it infrequently to account for changes to the cluster hardware.
Job dependent characteristics are determined whenever a new
job arrives.

AutoscalerOﬄine RuntimeEstimator (ORE)DL Platform (FfDL)OptimizerJob ParametersClusterParametersNew jobs + Existing JobsOptimal resource allocation to jobsRuntime ProﬁleHalt/Resume jobs with new allocationInfo about arriving jobscommunication time tcomm (p, k) for a job having p weights
when running on k GPUs.

The data communication time tcomm (p, k) for a particular
job principally determines how well it scales. A higher propor-
tional increase of tcomm (p, k) indicates a higher proportion of
time is spent in reducing the gradients and hence less actual
processing (or learning) per unit time; such jobs are called
communication bound and do not scale well (compared to
compute bound jobs which scale well).

3) Estimating the run-time for a job: As a batch of size
b is divided equally amongst GPUs allotted to a DL training
job, each GPU processes (cid:100) b

k (cid:101) of the input samples. Let

((cid:100) b

• tproc
j
ning the job with batch size (cid:100) b
described above in Section III-B1).

k (cid:101)) denote the processing time estimate for run-
k (cid:101) on a single GPU (as

• tcomm (pj, k) denote the communication time estimate for
performing the Allreduce operation of pj weights with k
GPUs (as described above in Section III-B2).

The per-iteration run-time for the job is then estimated as:

titer
j

(b, k) = tproc

j

((cid:100)

b
k

(cid:101)) + tcomm (pj, k)

Hence, the processing rate (number of input samples processed
per unit time) for job j with batch size b on k GPUs is:

Tj(b, k) =

b

tproc
j

((cid:100) b

k (cid:101)) + tcomm (pj, k)

In order to make optimal scheduling decisions, the optimizer
needs the processing rate estimate for a job j when scheduled
with a speciﬁc batch-size, say b, and number of GPUs, say k.
This estimate can be computed using the scaling characteristics
collected by the JSA (this estimation functionality is provided
by the JSA for use by the optimizer).

For conﬁgurations that are infeasible, for instance if batch-
size-per-GPU (cid:100)b/k(cid:101) does not ﬁt on a GPU, we take Tj(b, k) to
be a large negative number. This will ensure that the optimizer
will never consider this conﬁguration in the optimal solution.

C. Optimizer

As mentioned before,

the goal of the optimizer is to
schedule the current set of jobs on the cluster GPUs in an
optimal manner so as to maximize the net throughput (deﬁned
as the average number of jobs processed per unit time).

The autoscaler performs a periodic check (at the lapse of
every (cid:52) time interval) whether its temporary input buffer
contains some new jobs or whether some existing jobs have
completed within the time interval (or both); if any of these
conditions are met, the autoscaler adds the currently running
job IDs and the new job IDs from the input buffer (in order of
arrival) to a list (along with the associated job characteristics
of each job) and passes it to the optimizer. We next describe
in more detail the objective and algorithm of the optimizer.

5

1) Objective: The optimizer has to determine, for each job,
the number of GPUs to be allocated to the job and the batch
size it should be run with, such that the overall throughput of
the cluster is maximized. Every job must be allocated at least
1 GPU. If it is not possible to do so, the optimizer reports that
the problem is infeasible.

j

The cluster throughput is determined as follows. For a job, j,
we treat Tj(bmax
, 1) as the baseline processing rate of the job,
j
where bmax
is the maximum batch-size-per-GPU that can be
scheduled for the job. We now deﬁne the throughput scaling
factor, Tj(b, k), to be the factor increase in processing rate
obtained when the job is run with batch size b and k GPUs
in comparison to the baseline, i.e.,

Tj(b, k) =

Tj(b, k)

Tj(bmax
j

, 1)

(1)

We shall use the notation bopt
(k) to denote the optimal batch-
size-per-GPU for job j when running on k GPUs, i.e., the one
that yields the best throughout scaling factor; this can easily
be determined as follows

j

bopt
j

(k) = arg max

b

Tj(b, k)

(2)

Let J be the total number of jobs in the list obtained by the
optimizer. Then the objective of the optimizer is to determine
kj for j = 1 to J, i.e., the number of GPUs to be allocated to
each job, so as to maximise the total throughput scaling factor
of the jobs, i.e.,

maximize T =

J
(cid:88)

j=1

Tj(bopt
j

(kj), kj)

(3)

2) Algorithm: Given the objective function (3) for the opti-
mization problem, a mixed integer program can be formulated
to solve the problem. However, such programs can take very
long to solve in practice depending on the number of jobs
and GPUs. We show that the optimal solution to this problem
satisﬁes the optimal substructure property and thus admits a
dynamic program.

Proof : Optimal Substructure
The optimal solution to the Optimizer’s objective (Equa-
tion 3) satisﬁes the optimal substructure property. We shall
use the notation P(j, K) to denote the optimal throughput of
the ﬁrst j jobs when allocated a total of K GPUs.

Proof. By contradiction. Consider the optimal throughput of
the ﬁrst j jobs when allocated K GPUs, P(j, K). Then it
is not difﬁcult to see that T (cid:48) = P(j, K) − Tj(bopt
(kj), kj)
must be the optimal throughput of the ﬁrst j − 1 jobs when
scheduled on K − kj GPUs. If T (cid:48) were not optimal, then a
better solution can be constructed for the ﬁrst j jobs on K
GPUs by combining the allocation of the above solution for
the ﬁrst j−1 jobs on K−kj GPUs that yields better throughput
with an allocation of kj GPUs to the jth job. The throughput
of this solution would be T (cid:48) + Tj(bopt
(kj), kj) > P(j, K)
which would contradict the optimality of P(j, K).

j

j

Thus we can formulate a dynamic program (DP) to compute
the optimal solution every time the autoscaler consults the
optimizer by using a DP table for P(·, ·) and populating the
entries of this table iteratively using the following relation:

P(j, K) = max

1≤k≤kmax

[P(j − 1, K − k) + Tj(bopt

j

(k), k)] (4)

The optimal allocation kj to the jth job is thus given by:

kj = arg max
1≤k≤kmax

[P(j − 1, K − k) + Tj(bopt

j

(k), k)]

(5)

A practical way to solve the dynamic program (DP) in (4)
is to initialize the array P(·, ·) to a large negative number
and build the DP table progressively from j = 1, k = 1 to
j = J, k = K. Note from (4) that the optimizer might return
infeasible for such a problem setting, which will mean that
after the completion of the DP, P(J, K) ≤ 0. In such a case,
the autoscaler (next section) is responsible for ﬁguring out
a feasible solution by appropriately trimming the input job-
queue to the optimizer.

Algorithm 1 OPTIMIZER
Require: exec-queue Et, total GPUs K

return Updated Et with new GPU allocation, status
J ← LENGTH(Et)
P ← −∞J×K
SOL ← 0J×K
P(0, :) ← 0
for j = 1, · · · , J do

{initialize J × K array of −∞}
{initialize J × K array of 0s}
{no jobs mean zero utilization}

for k = 1, · · · , K do

for g = 1, · · · , kmax do
if k − g ≥ 0 then

p ← JSA.RECALL(Et(j), k) + P(j − 1, k − g + 1)
if p > P(j, k) then

P(j, k) = p ; SOL(j, k) = k
found}

{better utilization

if P(J, K) > 0 then

status ← “f easible”
while j > 0 do

;

j ← J

;

k ← K

UPDATEGPU(Et(j), SOL(j, k))
j}
j ← j − 1 ; k ← k − SOL(j, k)

{set GPU allocation for job

else

status ← “inf easible”

The implementation of the dynamic program in (4) is shown
in algorithm (1). The entries P(0, K) of the DP table are
initialized to 0 as the throughput is 0 for 0 jobs irrespective
of the number of GPUs. All other entries of the DP table are
initialized with a large negative number.

The solution array SOL(j, k) denotes the optimal GPUs
allocated to the j-th job when the total GPUs allocated to all
the j jobs is k.

Et is the input job queue that is sent to the optimizer.
The function call JSA.RECALL() looks up the value of the
throughput scaling factor Tj(bopt
(k), k) for the j-th job Et(j)
in the job-queue when run on k GPUs.

j

It is easy to see that the run-time complexity of the dynamic
program is O(JKkmax) based on the three for loops. A
problem will only be feasible if the number of jobs is no more

than the number of GPUs. Thus, we can assume that J ≤ K.
Therefore, even for 400 GPUs and kmax = 10, the complexity
is no more than an order of 2M operations (milliseconds on
modern CPUs).

If a feasible solution exists (P(J, K) > 0), each job in the
queue will have their respective GPU allocation ﬁeld updated
to the optimal number of GPUs with a call to UPDATEGPU().
In case no feasible solution exists, an ’infeasible’ status is sent
back to the autoscaler and the existing GPU allocation ﬁeld
for all the jobs in the input queue is left untouched.

D. Autoscaler

EXECUTING[]
{Jobs currently executing in DL platform}
ARRIVED[]
{Jobs that arrived since last scaling action}
FINISHED[] {Jobs that completed/failed since last scaling action}
TRIAL[]

1: init
2:
3:
4:
5:
6: upon ARRIVAL(job) do
7:
8:

{Get various values of Tj (b, k)

ARRIVED ← ENQUEUE(ARRIVED, job)
Tj[][] ← JSA.PROCESS(job)
from the JSA}
ADDTOMETADATA(job, Tj[][])

FINISHED ← FINISHED ∪ {job}

if (ARRIVED (cid:54)= {} || FINISHED (cid:54)= {}) then
EXECUTING ← EXECUTING \ FINISHED
i ← 1
while i ≤ LENGTH(ARRIVED) do

9:
10: upon DEPARTURE(job) do
11:
12: upon MAKESCALINGDECISIONS() do
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:

EXECUTING ← TRIAL
ARRIVED ← REMOVE(ARRIVED, i)

i ← i + 1

TRIAL ← EXECUTING ∪ ARRIVED[i] {Add one by one}
allocation, possible ← OPTIMIZE(TRIAL)
if possible then

SENDTODLPLATFORM(allocation, EXECUTING)

{Go to the next job in ARRIVED}

Fig. 4. High-level Pseudocode of Autoscaler

The autoscaler (pseudocode in Figure 4) maintains a list
(EXECUTING) of jobs that are currently being executed by
the DL platform. It queues all arriving jobs in another list
(ARRIVED). Each job in the queue is ﬁrst processed by the
JSA. The JSA computes the scaling characteristics of the job
as described in Section III-B and adds these characteristics to
the job metadata. The autoscaler is also notiﬁed by the DL
platform whenever a job completes (or fails due to user error)
and stores these jobs in a list (FINISHED), removing them
from the EXECUTING list before invoking the optimizer. To
be clear, the optimizer will be invoked even if no new job
arrives, but jobs leave.

Ideally, the autoscaler should invoke the optimizer every
time a job arrives or leaves. However, in practice, this can
lead to thrashing – a signiﬁcant amount of time is spent
scaling DL jobs up and down through checkpoint/resume that
it affects the progress made by the DL job. Consequently, in
practice, the autoscaler consults the optimizer periodically (say

6

every (cid:52) minutes). (cid:52) is typically based on the amount of time
a DL job can be queued without affecting user experience
and expectations. See section V-B for additional discussion
on choosing (cid:52) value.
Every (cid:52) minutes,

the autoscaler invokes the optimizer
to determine (i) whether GPUs can be allotted to as many
newly arrived jobs (from ARRIVED) and (ii) how many GPUs
are to be allotted to each job. This is done in an iterative
and incremental manner – ﬁrst by removing jobs that have
terminated, and then trying to add jobs from ARRIVED one
by one until the optimizer returns ’infeasible’. Once the set
of jobs that can be executed and the allocations to said jobs
are determined, the autoscaler interacts with the DL platform
to spawn new jobs, and scale existing jobs up or down. The
autoscaler can also be modiﬁed to drop (i.e., reject) pending
jobs (which are not feasible to accommodate) after Step 23 if
queueing is undesirable.

E. Simulator Design and Implementation

We develop a simulator in order to evaluate our techniques
on large cluster settings; given the difﬁculty in getting access
to large GPU-enabled clusters for large periods of time. The
simulator is based on the discrete event simulation (DES) [43]
methodology, wherein the simulation is driven by time based
events. An event is created in the simulator whenever a job
arrives or completes.

The input to the simulator is a job arrival ﬁle that contains
meta-information for the jobs along with their arrival time. The
meta-information includes the job details (such as maximum
batch size, minimum batch sizes, number of epochs to process,
etc.) along with the job speciﬁc scaling characteristics col-
lected by the job scalability analyzer (JSA) (see Section III-B).
Recall that the JSA runs the jobs for only a few iterations on
up to kmax GPUs to collect the job speciﬁc scaling data.

The simulator reads the job arrival information from the ﬁle
to create arrival events for the jobs based on their arrival time.
Based on the schedule returned by the optimizer, the simulator
updates the GPU allocations to the jobs and updates their
completion time based on the run-time estimates as determined
in Section III-B. It also adds a completion event for any new
job that has been scheduled. In case the optimizer returns that
no allocation is feasible, then the newly arrived job is either
dropped or put into a queue depending on the conﬁguration
of the simulator; the simulator can be run in both modes –
with and without queuing. In case of queuing, the ﬁrst job
from the queue is considered for execution on the next job
completion event; it is treated like a fresh job and the schedule
is determined accordingly. The simulator then proceeds to
process the next event until all the events are processed.

IV. EXPERIMENTAL EVALUATION

We evaluated our elastic scaling technique using a real 40
GPU cluster. Each of the machines was equipped with dual
Intel Xeon E5-2620 v4 (2.10 Ghz) with a total of 16 CPU
cores, 64 GB RAM, 10 GbE public and private uplinks and 2

P100 GPUs. The cluster was managed by Kubernetes v1.12,
and had FfDL [18] installed as the DL platform used to execute
jobs. We used PyTorch [22] for training the jobs.

Additionally, we also evaluated our elastic scaling tech-
niques using the simulator described in Section III-E over
a 400 GPU cluster. Throughout
the rest of this section,
experiments conducted on the actual cluster will be tagged
with the “Cloud” label when results are presented or plotted,
while experiments conducted through the simulator will be
tagged with the “Simulator” label.

A. Benchmarks : Job Categories and Arrival

The neural network model and the dataset of a job deter-
mines the communication vs compute characteristics of the
job. A compute bound job scales well with larger number
of GPUs compared to a communication bound job which
scales poorly due to communication overhead - this has been
captured in the throughput scaling factor calculation for a job
in Section III-C1.

Since the performance of our elastic scaling techniques
will principally depend on the distribution of the throughput
scaling factors of the jobs arriving in the arrival queue, we
focus on four categories - (1) jobs with lower throughput
scaling factors (communication bound), (2) jobs with higher
throughput scaling factors (compute bound), (3) jobs with
balanced throughput scaling factors (compute-communication
balanced) and (4) jobs with no elasticity (not intended to scale,
will run on ﬁxed number of GPUs).

We have chosen four representative jobs belonging to the
the details of which are given in Table
four categories,
I. Sampling jobs from these four categories randomly, we
reliably generate a real world DL job arrival scenario that has
a realistic distribution of jobs with different scaling potentials.
Each benchmark only runs jobs of a single job category; all
jobs have the same length (execution time) irrespective of the
category; it is set such that each job takes about half an hour
to execute on a single GPU with the maximum feasible batch
size. The jobs in a benchmark arrive in one of three patterns:
low arrival, high arrival or bursty arrival over a 240/480/720
minute period (will be clear from context).

The job arrivals have been generated by sampling from a
Poisson distribution with mean job arrival rate as the design
parameter. We deﬁne the job arrival rates as follows. A
base job arrival rate λ is deﬁned as the expected completion
rate (i.e., reciprocal of expected completion time) of a job
sampled uniformly from the 4 categories, on a single GPU
with maximum batch size per GPU. A high job arrival rate
signiﬁes that the mean arrival rate of the Poisson distribution
has been set to kmaxλ where kmax is the maximum GPUs
allocated to each job. A low arrival rate sets the mean of the
Poisson distribution to λkmax
. A bursty arrival rate alternates
the mean of the Poisson distribution between high and low
(deﬁned previously) every 60 or 120 mins as required by the
experiment.

4

7

Category
1
2
3
4

Dataset
CIFAR100
CIFAR100
CIFAR100
Food101

Model
resnet50
alexnet
vgg11 bn
alexnet

Weight Size Min BS, Max BS

24M
58M
10M
58M

32,256
16,256
16,1024
128,128

TABLE I
JOB CATEGORY INFORMATION

Characteristics
Elastic, Compute Bound
Elastic, Communication Bound
Elastic, Balanced
No Elasticity

(a) Category 1: Random-BS baseline, High Arrival

(b) Category 2: Random-BS baseline, High Arrival

(c) Category 3: Random-BS baseline, High Arrival

(d) Category 4: Fixed Batch Size, High Arrival

Fig. 5. Effect of job categories.

Batch size per GPU
Scaling Factor

8
0.86
TABLE II
THROUGHPUT SCALING FACTORS FOR CATEGORY 1 JOBS ON 2 GPUS

22
1.45

11
1.06

32
1.66

16
1.3

Furthermore, the jobs in a benchmark baseline run may be
scheduled with batch-sizes following one of three settings:
maximum batch-size (Max-BS) for that category, minimum
batch-size (Min-BS) for that category, and random batch-size
(Random-BS) which is a value picked uniformly randomly
between the Max-BS and Min-BS for that category. A bench-
mark is run for a chosen combination of job category (1, 2,
3 or 4), job arrival pattern (high, low, bursty) and batch-size
setting in case of baseline (Mas-BS, Min-BS, Random-BS).

8

B. Choosing the Baseline

A completely non-elastic scheduler can force a constant
total batch size on a ﬁxed number of GPUs through the job
lifetime; however, that is evidently a weak baseline.

For a stronger baseline, we ﬁx the total batch size of a job to
be a value selected from the range of allowed batch sizes for
that job category, but we allow the total batch of the job to be
distributed over several GPUs; this models a traditional elastic
scaler and does not take into account the characteristic (batch
size range) of the DL job. The allocation of GPUs to baseline
jobs is done through the same optimizer that is used in our
elastic scaling technique. The optimizer is invoked periodically
(every (cid:52) minutes) to allocate the relevant number of GPUs for
baseline jobs based on their throughput scaling factors such
that the total batch size remains ﬁxed across the allocated
GPUs. To reiterate, our baseline is also elastic in terms of

050100150200250Time (mins)020406080100Total Jobs CompletedJob arrivalOur method, CloudBaseline, CloudOur method, SimulatorBaseline, Simulator050100150200250Time (mins)020406080100Total Jobs CompletedJob arrivalOur method, CloudBaseline, CloudOur method, SimulatorBaseline, Simulator050100150200250Time (mins)020406080100Total Jobs CompletedJob arrivalOur method, CloudBaseline, CloudOur method, SimulatorBaseline, Simulator050100150200250Time (mins)020406080100Total Jobs CompletedJob arrivalOur method, CloudBaseline, CloudOur method, SimulatorBaseline, Simulator(a) Category 1: Random-BS Baseline - Low Job Arrival

(a) Allow Dropping of Jobs (No Queue)

(b) Category 1: Random-BS Baseline - Bursty Job Arrival

Fig. 6. Effect of Arrival Patterns.

total number of GPUs but doesn’t vary the total batch size.

C. Metrics

Unfortunately, existing metrics for evaluating elasticity [9],
[28], [19] have been designed for and apply only to cluster
elasticity. We therefore deﬁne new metrics based on:
• Optimal GPU time for all scheduled jobs

(Opt Sch Time). This is deﬁned as the sum of job
lengths (time) for all the scheduled jobs on a single GPU.
This is the minimum GPU-time needed to complete all
the scheduled jobs (GPU-time means actual runtime on
GPU, and we expect imperfect scaling).

• Actual GPU time for all the scheduled jobs

(Act Sch Time). This is deﬁned as the sum of [ (actual
number of GPUs used for running the jobs) x (Time
duration for which these GPUs were used) ]

For example, if a single scheduled job requires 10 mins to
complete on a single GPU but requires 6 mins to complete on
2 GPUs, then its Optimal GPU time will be 10 mins whereas
the Actual GPU time will be 12 mins (=6x2).

We measure and report the performance in terms of two
metrics (i) Scheduled Job Scaling (SJS) Efﬁciency, deﬁned

(b) No Dropping of Jobs (Queuing of Jobs)

Fig. 7. Effect of Queueing (Random-BS baseline)

as Opt Sch Time / Act Sch Time, and (ii) Job Drop Ratio,
deﬁned as Number of jobs dropped / Total number of jobs.
The ﬁrst metric shows the average scaling efﬁciency of all the
scheduled jobs i.e. how well the scheduled jobs scale across
GPUs. The second metric shows the proportion of jobs that
have been dropped.

D. Comparison with Max-BS and Min-BS

We ﬁrst compared our elastic scaling algorithm with two
simple strategies: one which schedules with the maximum
batch-size, Max-BS, speciﬁed by the user, while the other
schedules with the minimum batch-size, Min-BS. Note that
both these strategies can use the baseline algorithm for
scheduling as they do not dynamically change the batch-size
of the jobs.

Figures 8(a-c) compare our elastic scaling technique with
the baseline, by plotting the number of jobs completed over
time, while running Category 1 jobs with high and low arrival
rate (cf. Section IV-A & IV-A). The ﬁrst thing we observe
is how closely the simulator results match the execution on
the real 40 GPU cluster (“Cloud”). Figure 8(a) illustrates
the performance beneﬁts vis-a-vis the Max-BS baseline and
high arrival rate. It can be seen that the elastic scaling algo-
rithm performs signiﬁcantly better than the baseline algorithm,
scheduling ≈ 10 × more jobs in comparison. This is because,

9

050100150200250Time (mins)01020304050607080Total Jobs CompletedJob arrivalOur method, CloudBaseline, CloudOur method, SimulatorBaseline, Simulator050100150200250Time (mins)01020304050607080Total Jobs CompletedJob arrivalOur method, CloudBaseline, CloudOur method, SimulatorBaseline, Simulator0200400600800Time (mins)0100200300400500600700800Total Jobs CompletedJob arrivalOur method, CloudBaseline, CloudOur method, SimulatorBaseline, Simulator0200400600800100012001400Time (mins)0100200300400500600700800Total Jobs CompletedJob arrivalOur method, CloudBaseline, CloudOur method, SimulatorBaseline, Simulator(a) Jobs Completed using Baseline with Max Batch Size - High Job Arrival. Job Categories 1 and 2

(b) Jobs Completed using Baseline with Max Batch Size - High Job Arrival. Job Categories 3 and 4

(c) Jobs Completed using Baseline with Min Batch Size - High Job Arrival

Fig. 8. Comparison with Max- and Min-BS. Job arrival described in Section IV-A.

with high arrival rate, the baseline is able to schedule only
a limited number of jobs as the batch-size is high and thus
the jobs cannot be scaled down to a small number of GPUs.
The elastic scaling algorithm on the other hand can reduce the
batch size of the job itself to ﬁt on a small number of GPUs; if
the minimum batch-size allows, it can even schedule the jobs
on 1 GPU. Thus, it minimizes the number of dropped jobs.

Figures 8(b-c) illustrate the performance differential while
using the Max-BS baseline for high and low arrival rates re-
spectively. From Figure 8(b), we observe that the performance
of the baseline algorithm is quite comparable to that of the

elastic scaling algorithm as both work with small batch sizes
with high arrival rate. However, the baseline algorithm does
not perform as well with low arrival rate in Fig 8(c). We ob-
serve that the elastic scaling algorithm does marginally better
than the baseline algorithm in terms of jobs completed. The
average job completion time for the elastic scaling algorithm
is 23.04 mins whereas for the baseline algorithm, it is 27.39
mins. Thus jobs complete 16% faster using the elastic scaling
algorithm. This is explained by the fact that with low arrival
rate, both the algorithms try to maximize the GPUs used;
however, the elastic scaling algorithm is able to increase the

10

batch size as it scales to larger number of GPUs whereas the
baseline algorithm cannot. As can be expected, the scaling
behaviour is better when there is more work to be done by
every GPU, and thus our algorithm is able to achieve better
scaling resulting in quicker completion of the jobs.

To further support this argument, we conducted another
experiment where we just ran a single category 1 job on a
5 P100 GPU setup. In one experiment, we ran with the elastic
scaling algorithm and in another we ran with the baseline
algorithm using the minimum batch-size. We observed that
with the elastic algorithm, the job completed 1.6X faster than
with the baseline algorithm; the baseline algorithm used a
batchsize of 32 on 2 GPUs (16 per GPU) whereas the elastic
scaling algorithm used a batch size of 160 on 5 GPUs (32 per
GPU).

In practice, however, most users do not run with Max- or
Min- batch size due to cost and time considerations. One
notable observation through these results is that the simulator
results closely match the execution on the real GPU cluster.
In a more realistic scenario, jobs arrive with some batch size
distribution between the minimum and maximum batch sizes.
Our remaining experiments shall assume jobs with random
batch sizes.

E. Effect of different job categories

Figure 5 illustrates the total number of jobs completed
for various job categories (Section IV-A) and hence job
characteristics. We observe that the results of the simulator
is quite close to the real cluster. We also observe that all the
plots, except for Category 4, show that the elastic approach is
more effective as it demonstrates a signiﬁcant improvement in
the number of jobs completed in comparison to the baseline.
The number of jobs completed improve by 82%, 64.4% and
90% for Categories 1, 2 and 3 respectively (Figure 5(a-c)).
The reason, as discussed before, is that with high arrival rate
the baseline is able to schedule only a limited number of
jobs as the batch-size is high and thus the jobs cannot be
scaled down to a small number of GPUs. Our elastic scaling
algorithm, however, can reduce the batch size of the problem
itself to ﬁt the minimum number of GPUs feasible with the
minimum batch-size speciﬁed. Thus it minimizes the number
of dropped (rejected) jobs. To further support this argument,
we examined the best throughput scaling factors for category
1 (compute bound) and category 2 (communication bound)
jobs with their minimum batch size. We observed that the
best throughput scaling factor for the category 1 job was 1.3x
the best throughput scaling factor for the category 2 job.

For the 4th category (Figure 5(d)), results for both the
baseline and our method match with each other. This is
because in this category, the jobs are non elastic, i.e. the batch
size cannot be changed, and hence our elastic algorithm does
no better than the baseline.

F. Effect of arrival patterns

Next, we analyze the effect of elastic scaling with different
job arrival rates. Figure 6(a) and (b) plot the jobs completed

for our elastic scaling algorithm and the baseline algorithm
for micro-benchmarks wherein jobs arrive with random batch-
sizes under low and bursty arrival patterns respectively. The
bursty job arrival pattern is obtained by having a high job
arrival rate for the ﬁrst one hour followed by a low job arrival
rate for the next one hour. This pattern is repeated over the
duration of 4 hours.

For the case of low arrival rate, the number of jobs com-
pleted are more by 97% (≈ 2 ×) with the elastic approach in
comparison to the baseline. This is because both the algorithms
try to maximize the GPUs used; however, the elastic scaling
algorithm is able to increase the batch size as it scales to
larger number of GPUs whereas the baseline algorithm cannot.
As the scaling behaviour is better when there is more work
to be done by every GPU,
the scaling algorithm is able
to achieve better scaling resulting in quicker completion of
the jobs. This better scaling results in reduced drop rate.
To further support this argument, we present the throughput
scaling factors for a category 1 jobs on 2 GPUs with different
batch-size-per-GPU in Table II. It is clear that the throughput
scaling factor increases monotonically as the batch-size-per-
GPU is increased.

When the arrival pattern is bursty,

the number of jobs
completed by the elastic approach is 119% ( ≈ 2.2 ×) more
in comparison to the baseline. This is due to a mix of two
effects that have already been discussed before. During periods
of low arrival rate, the elastic scaling algorithm can increase
the batch size as it scales to larger number of GPUs to get
better scalability. On the other hand, during periods of high
arrival rate, the elastic scaling algorithm is able to reduce the
batch-size in order to ﬁt on fewer number of GPUs, thereby
accommodating more jobs. We also see that the simulator
closely match the cloud results.

G. Effect of Queuing

We next study the effectiveness of our algorithm with queu-
ing; no jobs are dropped (rejected) in this case. We consider
a longer bursty job arrival pattern wherein the jobs arrive for
720 mins (12 hours). Every new job that arrives belongs to
one of the four categories with equal probability. The job
lengths (time) for categories 1, 2, 3 and 4 when scheduled on
a single GPU are 16, 21, 41 and 27 mins respectively; these
are determined from practical settings involving incremental
training. This provides a good mix of jobs from different
categories with different run times. The bursty job arrival
pattern is obtained by having a very high job arrival rate for
the ﬁrst two hours followed by a very low job arrival rate for
the next two hours. This pattern is repeated over the duration
of 12 hours.

The results for this experiment without and with queuing
are presented in Figure 7(a) and (b) respectively. We observe
that the simulated results are consistent with the actual results
on the cloud. Our elastic approach performs much better than
the baseline; the number of jobs completed is 50% more
and almost matching the job arrival rate with queuing. As
explained before, during periods of low arrival rate, the elastic

11

Elastic-withdrop
Baseline-withdrop
Elastic-nodrop
Baseline-nodrop

Job Drop Ratio (%)
Simulator
Actual
10.09
13.59
39.57
42.4
-
-
-
-
TABLE III
PERFORMANCE METRICS FOR 40 GPUS WITH AND WITHOUT JOB DROPS (RANDOM-BS BASELINE)

Avg. Job Completion Time (mins)
Actual
24.97
34.12
33.79
351.02

SJS Efﬁciency (%)
Simulator
Actual
80.46
82.02
50.49
51.31
86.03
89.53
41.94
42.87

Simulator
22.98
28.10
27.75
320.32

slope) during periods of low arrival indicating that it is able
to schedule all the jobs. On the other hand, with queuing
the baseline curve remains straight (same slope) throughout
indicating that it is scheduling jobs that have accumulated
in the queue during the period of high arrival. In contrast,
the elastic scheduling curve ﬂattens in both settings, with and
without queuing, indicating that it is able to service all the
queued up jobs. Thus, the baseline ends up taking considerably
long to complete all the jobs. This is further reﬂected in the
average job completion time for the jobs presented in Table III.
The average job completion time increases by ∼35% (24.97
mins to 33.79 mins) for the elastic scaling algorithm whereas
it increases by nearly 10X (34.12 mins to 351.02 mins) for the
baseline algorithm. The average job completion time for the
baseline is nearly 10X that of the elastic scaling algorithm.

H. Validation of Simulator

We have seen that the simulation run curves follow the
actual run curves very closely for all the experiments presented
so far. Across all the runs presented, the jobs completed for the
simulation runs are within 7% of the actual runs. Further, we
compare other metrics between the simulator run and actual
run for the experiment conducted on 40 GPUs (discussed
above) in Table III. We can see that the simulator error is less
than 5% for Scheduled Job Scaling (SJS) Efﬁciency across
all the runs. The difference between the dropped (rejected)
jobs ratio is less than 4%. The difference in the average
job completion time are lower than the actual run by ∼10%
for two experiments and ∼17% for two other experiments.
The differences are attributed to the following reasons: (1)
the actual run needs to restart from a previous checkpoint
(therefore suffering some work loss) every time it is restarted
whereas the simulator does not, and (2) the actual run suffers
other system overheads and OS latencies.

We thus conclude that the simulator provides a very good
estimate of the number of completed jobs, SJS efﬁciency and
dropped job ratio. It also provides a reasonable estimate of the
average job completion time.

I. Large scale simulation

We ﬁnally evaluate the efﬁcacy of our algorithm on a larger
setup comprising of 400 GPUs using the simulator. In this
setting, the jobs arrive with a bursty arrival pattern for 480
mins (8 hours). Every new job that arrives belongs to one
of the four categories with equal probability. The job lengths
(time) for categories 1, 2, 3 and 4 when scheduled on a single
GPU are 16, 21, 41 and 27 mins respectively. The bursty job
arrival pattern is obtained by having a very high job arrival

(a) Allow Dropping of Jobs

(b) No Dropping of Jobs (Queuing of jobs)

Fig. 9. Simulation results for jobs Completed with bursty job arrival on 400
GPUs (Random-BS baseline)

Elastic-withdrop
Baseline-withdrop
Elastic-nodrop
Baseline-nodrop

SJS
Efﬁciency
81.00%
46.64%
81.53%
43.10%

Job Drop
Ratio
1.23%
38.28%
-
-

Avg. Job
Compl. Time
22.83
27.84
22.96
166.82

TABLE IV
PERFORMANCE METRICS FOR 400 GPUS WITH AND WITHOUT JOB
DROPS (RANDOM-BS BASELINE)

scaling algorithm can increase the batch size as it scales to
larger number of GPUs to get better scalability and during
periods of high arrival rate, the elastic scaling algorithm is
able to reduce the batch-size in order to ﬁt on fewer number
of GPUs, thereby accommodating more jobs.

Analyzing the baseline more carefully, we see that without
queuing the baseline curve tends to ﬂatten (reduction in

12

0100200300400500600Time (mins)010002000300040005000Total Jobs CompletedJob arrivalOur method, SimulatorBaseline, Simulator0200400600800Time (mins)010002000300040005000Total Jobs CompletedJob arrivalOur method, SimulatorBaseline, Simulatorrate for the ﬁrst two hours followed by a very low job arrival
rate for the next two hours. This pattern is repeated over the
duration of 8 hours.

The results for this experiment without and with queuing are
presented in Figure 9(a) and (b) respectively. The observations
are quite similar to those of the 40 GPU experiments. Our
elastic approach performs much better than the baseline; the
number of jobs completed is 60% more than the baseline
and almost matching the job arrival rate with queuing. As
in the case of 40 GPUs, we see that without queuing the
baseline curve tends to ﬂatten during periods of low arrival,
whereas with queuing, the baseline curve remains straight; this
indicates that the baseline cannot schedule all the jobs in the
queue during periods of low arrival. In contrast, the elastic
scheduling curve ﬂattens in both settings, with and without
queuing, indicating that it is able to service all the queued up
jobs.

The average job completion time for the jobs is presented
in Table IV. We see that the average job completion time is
almost the same for the elastic scaling algorithm whereas it
increases by nearly 6X (27.84 mins to 166.82 mins) for the
baseline algorithm. The average job completion time for the
baseline is nearly 6X that of the elastic scaling algorithm. Note
that though the simulator has some error in comparison to the
actual runs as discussed previously, the error is very small
compared to the order of magnitude difference in comparison
between the elastic scaling algorithm and baseline algorithm
discussed here.

V. DISCUSSION

A. Choosing Good Min-BS and Max-BS

This is a continuation of the discussion in Section II-C
and III-B. One question is whether the speciﬁcation of Min-
and Max-BS to the autoscaler is an undue burden on data
scientists. We believe that it is not.

It is well known in deep learning that each neural network
model achieves maximum accuracy when trained on a given
set of batch sizes [47]. Even when DL training is performed
on bare metal servers, without any elastic scaling or deep
learning platforms, data scientists (need to be) are aware of
this range because they need to be ready to train on new data
and the number of GPUs available to them changes either due
to budgetary issues or competition from their colleagues in the
same organization. Our work simply leverages this knowledge
for effective elastic scaling.

Furthermore, as discussed in Section III-A, the range of
Min- and Max-BS (bmin...bmax) directly impacts the execu-
tion time of the training job. By giving bmin...bmax as input
to the autoscaler, the data scientists implicitly agrees that
they are willing to wait as long as it takes for the job to
execute, even in the worst case that the cluster is extremely
overloaded and the autoscaler is only able to allocate the
GPUs corresponding to bmin. If the data scientist has time
constraints, our JSA can also be used to estimate the time
taken to execute the job for various batch sizes. This is

a straightforward extension of the following iteration time
estimation equation from Section III-B:

j

((cid:100)

titer
j

(b, k) = tproc

(cid:101)) + tcomm (pj, k)

b
k
Thus, the data scientist can then eliminate unsuitable batch
sizes according to their time constraints. The effort required
on the part of the data scientist is minimal considering the
performance beneﬁts.

B. Choosing (cid:52)

This is a continuation of the discussion in Sections III-C
and III-D. As discussed earlier in Section III-D, to prevent
thrashing, the autoscaler only calls the optimizer every (cid:52) time
interval. If (cid:52) is too small, then jobs that are autoscaled will
not have made progress or taken checkpoints beyond the last
time they were halted. If (cid:52) is too large, this can lead to bad
user experience, as some jobs may spend a lot of time waiting
in a queue before being scheduled or rejected. 10-15 minutes
seems to be a good value for (cid:52) based on our experience.
There is also the option of using a hybrid strategy, e.g., wait
10 minutes or until 5% of the jobs in the cluster terminate.

VI. RELATED WORK

The bulk of existing research (e.g., [5], [16], [19], [44], [40],
[12], [15], [28], [29], [7]) on elasticity has focused on cluster
elasticity Mechanisms for cluster elasticity do not adapt the
workload to best use a ﬁxed (or semi-ﬁxed) set of resources.
Recent research has also proposed programming elasticity
directly into the application, i.e., making the application self-
adaptive by monitoring its environment [28], [29], [44], [7].
Though programmable elasticity potentially applies to both
cluster and job elasticity, aforementioned research [28], [29],
[44], [7] is limited to cluster elasticity. Recent research on
using burstable instances [52], [51] is a form of cluster
elasticity, focusing on a combination of vertical and horizontal
scaling.

MapReduce [50], [1] and Apache Spark [10] were two of
the ﬁrst data analytics platforms to explore job elasticity in a
limited manner. However, these platforms do not handle DL
workloads natively, and do not change hyperparameters during
elastic scaling.

As mentioned earlier, Project Philly/Fiddle [32], [31] is a
DL platform internal to Microsoft intended for use in data-
center and primarily private cloud environments. It enhances
YARN’s [48] scheduler to support locality-aware scheduling
and gang scheduling. However, elastic scaling is not supported.
The Hemingway tool [41] guides the selection of appropriate
algorithms and cluster size for distributed training jobs by
using predictive models for job runtime and convergence [49].
However,
these models do not account for resource con-
tention among jobs in a cloud environment. The SLAQ frame-
work [55] explores quality-runtime tradeoffs across multiple
jobs to maximize system-wide quality by adjusting resource
allocations of all running jobs. SLAQ is a cluster scheduling

13

system for ML training jobs that aims to maximize the overall
job quality. The intuition behind SLAQ is that in the context of
approximate ML training, more resources should be allocated
to jobs that have the most potential for quality improvement.

Litz[42], similarly, is one of the ﬁrst examples of adaptive
scaling of resources applied to machine learning (ML) jobs.
The Litz environment supports API driven programming of
ML constructs, speciﬁcally elastic addition/deletion of param-
eter servers and worker shards. Its beneﬁt is that it can handle
elasticity of individual jobs seamlessly (without restarting), but
the elasticity decisions are still manual, and there is no concept
of a cluster manager/autoscaler that decides the elastic scaling
to improve cluster level throughput. Tiresias [14] proposes a
GPU cluster manager for distributed DL jobs which efﬁciently
schedules and places DL jobs to reduce their job completion
times (JCT). Their scheduling system assign priorities to jobs
in order to reduce their JCT. It doesn’t change the batch size
for reducing JCT. OASiS[4] describes scheduling in a machine
learning cluster that forecasts resource availability and uses
that in conjunction with the utility of arriving jobs to arrive
at a resource conﬁguration for a particular job. In addition,
an admission policy for the job is also prescribed in order to
prioritize most useful jobs.

Gandiva [53] exploits intra-job predictability of mini-batch
iterations to time-slice GPUs efﬁciently across multiple jobs
in order to improve cluster efﬁciency. It also dynamically
migrates a communication intensive job to preserve afﬁnities
between learners. Gandiva also supports limited elastic scaling
by allowing a learner in a DL training job to occupy all GPUs
on a machine. However, it does not alter hyperparameters
during this limited elastic scaling.

Next, we discuss commercial offerings. Many public cloud
vendors offer DL platforms with some support for elasticity.
Examples include IBM Watson Machine Learning [8], Ama-
zon AWS SageMaker [45], Google Cloud AI Platform [24],
and Microsoft Azure [39]. Amazon AWS SageMaker [45] is a
fully managed machine learning service, supporting creation,
training and deployment of machine and deep learning models.
On the training side, which is the topic of this paper, Sage-
maker only supports manual elastic scaling. It is up to the data
scientist to choose the type and number of AWS instances for
his training job, and determine appropriate hyperparameters.
training either
In the case of Google Cloud AI Platform,
happens via pre-conﬁgured VM images optimized for GPUs
and TPUs, or through Kubeﬂow [36] on Kubernetes. In the
case of VM images, elastic scaling is manual and reactive
(train, test, train...). In the case of Kubeﬂow, elastic scaling is
still user-driven but is programmable as part of the Kubeﬂow
pipeline. But, neither Kubeﬂow nor Kubernetes automatically
determines the amount of resources to allocate or the hyperpa-
rameters corresponding to the allocated resources. The elastic
scaling capabilities of Microsoft Azure [39] with respect to DL
training jobs are similar to AWS Sagemaker (other capabilities
are different, of course).

VII. CONCLUSIONS AND FUTURE WORK

In this paper, we have demonstrated that to effectively scale
deep learning jobs, we have to determine whether they are
compute or communication bound and leverage the fact that
they can be executed with a range of batch sizes in addition
to considering other factors like cluster utilization. We have
demonstrated that the exploration of a range of batch sizes
can be formulated as an optimization problem, which also
considers the scaling efﬁciency of individual DL jobs when
run on multiple resources. We have shown that this problem
admits a dynamic programming based solution which makes
it efﬁcient (for real-time use from autoscalers) and scalable
(to typical DL clusters with hundreds of GPUs). We have
also demonstrated that our elastic scaling techniques have
signiﬁcant performance beneﬁts in most scenarios (up to 2×
increase in job completions and 10× better job completion
time).

We are currently in the process of extending this work in
two directions – (i) to support job priorities during elastic
scaling and (ii) to support spot pricing models to optimize the
cost of job execution (both from the angle of reducing cost to
data scientists and increasing revenue to cloud providers who
offer DL-as-a-Service).

REFERENCES

[1] Apache Hadoop. Map Reduce, 2018.

https://hadoop.apache.org/

docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/
MapReduceTutorial.html.

[2] Inﬁniband Trade Association. The Inﬁniband Speciﬁcation. https://www.

inﬁnibandta.org/ibta-speciﬁcation/, 2019.

[3] Lukas Balles, Javier Romero, and Philipp Hennig. Coupling adaptive
batch sizes with learning rates. In Conference on Uncertainty in Artiﬁcial
Intelligence, pages 410–419, 2017.

[4] Yixin Bao, Yanghua Peng, Chuan Wu, and Zongpeng Li.

On-
line job scheduling in distributed machine learning clusters. CoRR,
abs/1801.00936, 2018.

[5] Andre Bauer, Nikolas Herbst, Simon Spinner, Ahmed Ali-Eldin, and
Samuel Kounev. Chameleon: A hybrid, proactive auto-scaling mech-
IEEE Trans. Parallel Distrib. Syst.,
anism on a level-playing ﬁeld.
30(4):800–813, April 2019.

[6] L´eon Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization methods
for large-scale machine learning. SIAM Review, 60:223–311, 2016.
[7] Wei-Chiu Chuang, Bo Sang, Sunghwan Yoo, Rui Gu, Milind Kulkarni,
and Charles Edwin Killian.
Eventwave: programming model and
runtime support for tightly-coupled elastic cloud applications. In ACM
Symposium on Cloud Computing, SOCC ’13, Santa Clara, CA, USA,
October 1-3, 2013, pages 21:1–21:16, 2013.

[8] IBM Corporation.

IBM Watson Machine Learning. https://developer.

ibm.com/clouddataservices/docs/ibm-watson-machine-learning/, 2018.

[9] Standard Performance Evaluation Corporation. SPEC Cloud IaaS 2018

Benchmark, 2018. https://www.spec.org/cloud iaas2018/.
[10] Databricks Inc. Apache Spark, 2018. https://spark.apache.org/.
[11] Aditya Devarakonda, Maxim Naumov, and Michael Garland. Adabatch:
adaptive batch sizes for training deep neural networks. arXiv preprint
arXiv:1712.02029, 2017.

[12] Anshul Gandhi, Mor Harchol-Balter, Ram Raghunathan, and Michael A.
Kozuch. Autoscale: Dynamic, robust capacity management for multi-tier
data centers. ACM Trans. Comput. Syst., 30(4):14:1–14:26, November
2012.

[13] Priya Goyal, Piotr Doll´ar, Ross B. Girshick, Pieter Noordhuis, Lukasz
Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming
He. Accurate, large minibatch SGD: training imagenet in 1 hour. CoRR,
abs/1706.02677, 2017.

14

[14] Juncheng Gu, Mosharaf Chowdhury, G. Shin Kang, Yibo Zhu, Myeong-
jae Jeon, Junjie Qian, Hongqiang Liu, and Chuanxiong Guo. Tiresias: A
gpu cluster manager for distributed deep learning. In 16th USENIX Sym-
posium on Networked Systems Design and Implementation, NSDI’19,
pages 485–500. USENIX Association, 2019.

[15] U. U. Hafeez, M. Wajahat, and A. Gandhi. Elmem: Towards an elastic
In 2018 IEEE 38th International Conference on

memcached system.
Distributed Computing Systems (ICDCS), pages 278–289, July 2018.

[16] Nikolas Herbst, Andr´e Bauer, Samuel Kounev, Giorgos Oikonomou,
Erwin Van Eyk, George Kousiouris, Athanasia Evangelinou, Rouven
Krebs, Tim Brecht, Cristina L. Abad, and Alexandru Iosup. Quantifying
cloud performance and dependability: Taxonomy, metric design, and
emerging challenges. ACM Trans. Model. Perform. Eval. Comput. Syst.,
3(4):19:1–19:36, August 2018.

[17] Benjamin Hindman, Andy Konwinski, Matei Zaharia, Ali Ghodsi,
Anthony D. Joseph, Randy Katz, Scott Shenker, and Ion Stoica. Mesos:
A platform for ﬁne-grained resource sharing in the data center.
In
Proceedings of
the 8th USENIX Conference on Networked Systems
Design and Implementation, NSDI’11, pages 295–308, Berkeley, CA,
USA, 2011. USENIX Association.

[18] IBM Inc. Ffdl: A fabric for deep learning. https://github.com/IBM/FfDL,

2018.

[19] Alexey Ilyushkin, Ahmed Ali-Eldin, Nikolas Herbst, Andr´e Bauer,
Alessandro V. Papadopoulos, Dick Epema, and Alexandru Iosup. An
experimental performance evaluation of autoscalers for complex work-
ﬂows. ACM Trans. Model. Perform. Eval. Comput. Syst., 3(2):8:1–8:32,
April 2018.

[20] Docker Inc. Docker : The Enterprise Grade Container Platform. https:

//www.docker.com/, 2019.

[21] Facebook Inc. Caffe2: A new lightweight, modular, and scalable deep

learning framework. https://caffe2.ai/, 2017.
[22] Facebook Inc. PyTorch, 2019. https://pytorch.org.
[23] Google Inc. Kubernetes: Production Grade Container Orchestration,

2017.

[24] Google

Inc.

Google Cloud Machine Learning Engine.

https://cloud.google.com/ml-engine/, 2018.

[25] Google Inc. Tensorﬂow: An open-source machine learning framework

for everyone. https://www.tensorﬂow.org/, 2018.

[26] NVIDIA Inc. NVLINK FABRIC : A FASTER, MORE SCALABLE
https://www.nvidia.com/en-us/data-center/nvlink/,

INTERCONNECT.
2019.

[27] Uber Technologies Inc. Horovod, 2019. https://eng.uber.com/horovod/.
[28] K. R. Jayaram. Elastic remote methods. In David Eyers and Karsten
Schwan, editors, Middleware 2013, pages 143–162, Berlin, Heidelberg,
2013. Springer Berlin Heidelberg.

[29] K. R. Jayaram. Exploiting causality to engineer elastic distributed
software. In 2016 IEEE 36th International Conference on Distributed
Computing Systems (ICDCS), pages 232–241, June 2016.

[30] K. R. Jayaram, Vinod Muthusamy, Parijat Dube, Vatche Ishakian, Chen
Wang, Diana Arroyo, Asser Tantawi, Benjamin Herta, Scott Boag, Archit
Verma, Falk Pollok, and Rania Khalaf. Ffdl: A ﬂexible, multi-tenant
deep learning platform. In MIDDLEWARE’19, 2019.

[31] Myeongjae Jeon, Shivaram Venkataraman, Amar Phanishayee, Junjie
Qian, Wencong Xiao, and Fan Yang. Analysis of large-scale multi-
tenant gpu clusters for dnn training workloads. In USENIX ATC 2019,
July 2019.

[32] Myeongjae Jeon, Shivaram Venkataraman, Amar Phanishayee, Junjie
Qian, Wencong Xiao, and Fan Yang. Analysis of large-scale multi-tenant
In 2019 USENIX Annual
GPU clusters for DNN training workloads.
Technical Conference (USENIX ATC 19), pages 947–960, Renton, WA,
July 2019. USENIX Association.

[33] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan
Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. Caffe:
Convolutional architecture for fast feature embedding. In Proceedings
of the 22Nd ACM International Conference on Multimedia, MM ’14,
pages 675–678, New York, NY, USA, 2014. ACM.

[34] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail
Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for
deep learning: Generalization gap and sharp minima. abs/1609.04836,
2016.

[35] Alex Krizhevsky. CIFAR datasets, 2009. https://www.cs.toronto.edu/

∼kriz/cifar.html.

[36] Kubeﬂow. The Machine Learning Toolkit for Kubernetes. https://www.

kubeﬂow.org/, 2019.

[37] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep Learning.

Nature, 521(7553):436–444, 2015.

[38] Harold C. Lim, Shivnath Babu, and Jeffrey S. Chase. Automated control
for elastic storage. In Proceedings of the 7th International Conference
on Autonomic Computing, ICAC ’10, pages 1–10, New York, NY, USA,
2010. ACM.

[39] Microsoft

Azure.

Machine

Learning

services.

https://azure.microsoft.com/en-us/services/machine-learning-services/,
2018.

[40] Hiep Nguyen, Zhiming Shen, Xiaohui Gu, Sethuraman Subbiah,
and John Wilkes. AGILE: Elastic distributed resource scaling for
In Proceedings of the 10th International
infrastructure-as-a-service.
Conference on Autonomic Computing (ICAC 13), pages 69–82, San Jose,
CA, 2013. USENIX.

[41] Xinghao Pan, Shivaram Venkataraman, Zizheng Tai, and Joseph Gonza-
lez. Hemingway: Modeling distributed optimization algorithms. arXiv
preprint arXiv:1702.05865, 2017.

[42] Aurick Qiao, Abutalib Aghayev, Weiren Yu, Haoyang Chen, Qirong Ho,
Garth A. Gibson, and Eric P. Xing. Litz: Elastic framework for high-
In 2018 USENIX Annual
performance distributed machine learning.
Technical Conference (USENIX ATC 18), pages 631–644, Boston, MA,
2018. USENIX Association.

[43] Stewart Robinson and Ebooks Corporation. Simulation : the practice
of model development and use. Chichester, West Sussex, England ;
Hoboken, NJ : John Wiley & Sons, Ltd, 2004. Includes bibliographical
references and index.

[44] Bo Sang, Gustavo Petri, Masoud Saeida Ardekani, Srivatsan Ravi, and
Patrick Eugster. Programming scalable cloud services with AEON. In
Proceedings of the 17th International Middleware Conference, Trento,
Italy, December 12 - 16, 2016, page 16, 2016.

[45] Amazon

Web

Services.

Amazon

Sagemaker.

https://aws.amazon.com/sagemaker/, 2017.

[46] K. Simonyan and A. Zisserman. Very deep convolutional networks for

large-scale image recognition. CoRR, abs/1409.1556, 2014.

[47] Samuel L. Smith, Pieter-Jan Kindermans, and Quoc V. Le. Don’t decay

the learning rate, increase the batch size. In ICLR’18, 2018.

[48] Vinod Kumar Vavilapalli, Arun C. Murthy, Chris Douglas, Sharad
Agarwal, Mahadev Konar, Robert Evans, Thomas Graves, Jason Lowe,
Hitesh Shah, Siddharth Seth, Bikas Saha, Carlo Curino, Owen O’Malley,
Sanjay Radia, Benjamin Reed, and Eric Baldeschwieler. Apache hadoop
yarn: Yet another resource negotiator. In Proceedings of the 4th Annual
Symposium on Cloud Computing, SOCC ’13, pages 5:1–5:16, New York,
NY, USA, 2013. ACM.

[49] Shivaram Venkataraman, Zongheng Yang, Michael Franklin, Benjamin
Recht, and Ion Stoica. Ernest: Efﬁcient performance prediction for large-
scale advanced analytics. In 13th Usenix Conference on Networked Sys-
tems Design and Implementation, NSDI’16, pages 363–378. USENIX
Association, 2016.

[50] Abhishek Verma, Ludmila Cherkasova, and Roy H. Campbell. Resource
provisioning framework for mapreduce jobs with performance goals. In
Proceedings of the 12th ACM/IFIP/USENIX International Conference on
Middleware, Middleware’11, pages 165–186, Berlin, Heidelberg, 2011.
Springer-Verlag.

[51] Cheng Wang, Bhuvan Urgaonkar, Aayush Gupta, George Kesidis, and
Qianlin Liang. Exploiting spot and burstable instances for improving the
cost-efﬁcacy of in-memory caches on the public cloud. In Proceedings
of the Twelfth European Conference on Computer Systems, EuroSys ’17,
pages 620–634, New York, NY, USA, 2017. ACM.

[52] Cheng Wang, Bhuvan Urgaonkar, Neda Nasiriani, and George Kesidis.
Using burstable instances in the public cloud: Why, when and how?
Proc. ACM Meas. Anal. Comput. Syst., 1(1):11:1–11:28, June 2017.

[53] Wencong Xiao, Romil Bhardwaj, Ramachandran Ramjee, Muthian Si-
vathanu, Nipun Kwatra, Zhenhua Han, Pratyush Patel, Xuan Peng,
Hanyu Zhao, Quanlu Zhang, Fan Yang, and Lidong Zhou. Gandiva:
In 13th USENIX
Introspective cluster scheduling for deep learning.
Symposium on Operating Systems Design and Implementation (OSDI
18), pages 595–610, Carlsbad, CA, 2018. USENIX Association.
[54] Yang You, Igor Gitman, and Boris Ginsburg. Large batch training of

convolutional networks. http://arxiv.org/abs/1708.03888, 2017.

[55] Haoyu Zhang, Logan Stafman, Andrew Or, and Michael J. Freedman.
SLAQ: Quality-Driven Scheduling for Distributed Machine Learning. In
ACM Symposium on Cloud Computing, pages 390–404, 2017.

15

